Title: Meshroom: An Open Source 3D Reconstruction Software
Publication date: 2020-08-24
Playlist: Open Source Days 2020
Description: 
	Meshroom: An Open Source 3D Reconstruction Software
Speakers: Fabien Castan, Benoit Maujean

For more information about the Academy Software Foundation go to: https://www.aswf.io/

We will invite users to discuss AliceVision, an open source Photogrammetric Computer Vision framework and Meshroom, that allow artists and scientists to customize 3D reconstruction and camera tracking pipelines.
We will be sharing with the community the latest evolutions of Meshroom: the improvements of the 3D reconstruction and the UI as well as the new HDRI pipeline.
alicevision.org
Captions: 
	                              okay let's start yes hi everyone                               thank you for joining us at this                               mushroom analysis vision                               session at the open source days                                     uh first of all first we would like to                               thank the academy software foundation                               for having inviting us to these panels                               of prestigious open source software                               so i'm bernard mojon and i'm here with                                uh fabian custom                                so uh to uh to give you a little bit of                                context                                uh then oh yeah the agenda of the the                                session                                so we'll start with an overview of                                mushroom analysis vision                                then we'll go to uh the recent evolution                                of mushrooms                                regarding the next release                                we'll go through also the next steps to                                come                                we figure out and then we'll talk a                                little bit about                                our community and then we'll switch to                                the q a                                that you can do with the zoom                                application                                so uh as a start uh so we                                fabian and i are working at microsimage                                so micros is a vfx studio an animation                                studio                                we are part of the technicolor group and                                mikros is positioned across the three                                service lines of                                the production services of technicolor                                on the left hand side you can see that                                we are positioned on the advertisement                                service slide under the brand name                                because npc advertising                                in the middle you can see that we are                                positioned also on the visual effects                                industry for films and on the right hand                                side you can see that we are positioned                                on the animation                                service line for feature feeds and                                cereals                                so we are located in paris but we also                                have premises                                in belgium in brussels and liege and in                                canada                                in montreal and we have some facilities                                with our friends of technicolor in                                london and in india too                                so to illustrate this uh we're gonna                                show you our demo reel                                on this screen                                [Music]                                [Music]                                [Music]                                so that that that was to give you a bit                                of context of                                the project within microsimage and now                                we're gonna have uh                                an overview of mushroom analysis vision                                so                                uh what are the motivations of the                                project uh                                the the motivation was to improve inside                                the productivity on                                                                                  for visual effects so that concerns the                                real world objects                                that need to be changed by visual                                effects                                it also deals with shooting set                                extensions or it could                                also deal with a set survey that serves                                as a reference for the camera tracking                                so in fact we can see that                                photogrammetry is                                the kind of root of all the visual                                effects uh to achieve                                this kind of uh of uh photorealistic                                matching between the real world and um                                the computer graphic worlds so you have                                these examples for the different                                activities we have at micros                                so what is the main concept of mushroom                                so meshu is the front-end application                                and                                as a reminder this is a photography                                pipeline which is made basically of uh                                three steps the first steps concern the                                analysis of the input images                                and then the second step is uh                                destination of the cameras                                that provides a sparse phone cloud with                                the                                intrinsic parameters of the camera and                                 the camera positioning                                 and the third step is a dense phone                                 cloud with                                 the estimation of the object surfaces                                 and                                 the result is a match which is textured                                 by the                                 the input images so this is the                                 classical                                 three steps of the photogrammetry                                 pipeline so mushroom again is the                                 is the front-end application                                 the user interface is made of two parts                                 the high-level ui                                 is what is used for the automatic                                 process                                 the the end user the computer graphic                                 artist can drop                                 drag and drop the images on the on the                                 left hand side and then                                 launch and submit the the process of the                                                                                   and on the lower end of the graphic the                                 user interface                                 you have the nodal pipeline for expert                                 users where you can                                 tweak your parameters or you can play                                 around with your nodes you can                                 [Music]                                 customize your pipeline and so on we we                                 give you some details on that                                 later on so what are the key points of                                 meshroom                                 so as i said there is a default workflow                                 for all the standard use case                                 but meshum is not a black box so it's a                                 it's a                                 nodal editor where you can uh                                 tweak the different nodes and also you                                 can have the results of each node                                 in a file with a caching mechanism and                                 you can also                                 view the results of each node within the                                 interface of mushroom                                 whether it is                                                            this nodal pipeline enables you also to                                 build up                                 some custom pipelines we will show you                                 some examples                                 you can also augment your construction                                 uh to iterate the scene                                 reconstruction with a couple of edited                                 image                                 uh one other key point also is that we                                 are able to submit                                 the the computing on the render farm you                                 can submit it on locally but                                 you can use a render farm because                                 photogrammetry is uh                                 needs a lot of resource uh there's also                                 in the pipeline                                 mesh post processing uh for instance we                                 can                                 simplify or we can smooth the match                                 and another cool feature also is that if                                 you do read topology in third part                                 software                                 you can retexture your your                                 simplified mesh with input images                                 and you can see the results within the                                 same                                                   in a mushroom interface                                 so here we have an example of of the                                 render farm video                                 fact what you can see is that on the on                                 the                                 right hand side that some nodes are                                 parallelized                                 onto the the render nodes uh here in                                 fact it's an implementation                                 with our dispatcher which is a tractor                                 and the other cool thing is that you can                                 follow                                 the rendering process within measuring                                 for each node                                 so regarding the the technology stack so                                 as i said the mesh measurement is the                                 frontal application                                 uh it's a it's a script mode uh ui                                 so that enables it to be more evolutive                                 and to be tested more rapidly                                 in python and qt and the back end is at                                 each vision framework with all the                                 secrets priests and                                 cuda libraries uh to perform the                                 computation with                                 the best performance                                 we also have uh we rely in fact uh                                 intensively on uh standard file formats                                 uh like alembic or obg for the for the                                                                              exr for the the                                             we also uh we have built the backbone on                                 the other open source project                                 this is uh a few of them that are being                                 used                                 within his vision uh                                 we have to mention on top of it the                                 vcpkg initiative as well                                 which is not on this list and                                 regarding the the architecture we have                                 developed also                                 a couple of plugins one is for maya                                 which you can see                                 in this video so we can export                                 the machines and the camera from                                 mushroom and                                 use them uh into maya so that                                 the cg artist can see within maya what                                 are the different parts of the pawn                                 clouds that are provided by these images                                 where are the camera located within the                                                                          so it's a great tool to help for them to                                 do the rhythmology for instance                                 in maya so                                 this so this my uh tool has been                                 developed at micros but uh what's                                 interesting is the                                 community of meshum has developed other                                 plugins for instance                                 the guy from side effects have                                 implemented an integration of                                 his vision pipeline into houdini with                                 the game development tool set uh                                 another community uh                                 add-on has been done for some guys of                                 two in blender                                 in fact these plugins uh                                 the import of mushroom and the                                 management of the data of mushroom                                 into blender or to clean the mesh or to                                 do your topology whatever so                                 we give you here two examples                                 so that was a quick overview of national                                 analysis                                 and now fadia is taking over to speak                                 about                                 the evolutions of mushroom and the new                                 version coming                                 hello so i will present the new features                                 that will                                 arrive in the new version                                 so first of all we in this new version                                 there will be a new pipeline for                                 the creation of hdr panorama                                 and so it's a support standard optics                                 but it's also a specific support for                                 full fisheye images                                 and and also it can take advantage of a                                 motorized head system and                                 so this new pipeline there we will see                                 the two parts                                 so the first part is the creation of the                                 hdr images                                 so we will have the multi-marketing ldr                                 images and we have to                                 analyze them and fuse them into hdm                                 so in our implementation in mushroom                                 it's splitted in three nodes                                 the first step is the sampling is the                                 analysis of the images and selection of                                 the most focused pixels to extract                                 information                                 uh when we have done that we can do the                                 calibration                                 and in this calibration the idea is to                                 recover the camera response function                                 from the multi-packeting and                                 and finally we have to we use this                                 calibration                                 to fuse our input images                                 into a single hdr image                                 and so it's it's quite standard and                                 but we have a specific option to be able                                 to uh                                 analyze the highlights so the idea is                                 that one of the use case                                 is the creation of images for the                                 lighting                                 and in the lighting the most important                                 aspect is to be able to make a big                                 difference between                                 the wheel source of the lighting so if                                 you have the sun                                 in your images for instance and                                 and so when you do that it's not                                 possible during the acquisition to be                                 able to recover                                 the real value of the sun you you are                                 never able to                                 make the um                                 with your camera you are not able to                                 capture the amount of light you get from                                 the sun                                 and so we detect those pixels that are                                 saturated in all images and provide some                                 post-processing stuff                                 we have also input the image viewer of                                 mushroom to be able to                                 see those images so we can uh visualize                                 them with a floating point visualization                                 and                                 interactively adjust gain and gamma to                                 be able to                                 see the values and also the color picker                                 to really                                 analyze what we get                                 and and then we have the other part of                                 this pipeline                                 for the creation of the                                            um so for that so in our implementation                                 it's splitted in four nodes so the first                                 step                                 uh is when we are using fisheye images                                 we have to detect the fisheye circle or                                 we can manually manually adjust it as                                 as we can see on the video and                                 and in the case of uh when we have a                                 motorized system with a lot of images we                                 can                                 retrieve the files from the motorized                                 head system and unuse that to initialize                                 the position of the camera                                 then the other step the estimation will                                 recover                                 the relative rotation between all the                                 cameras                                 then the warping we convert all our                                 input images into the                                                                                                   we have the compositing                                 and we use the standard multiband                                 blending algorithm to be able to                                 compensate for slight variation                                 of illumination and being able to                                 keep the pixel precision and we have                                 also implemented the                                 graph cad to be able to select the                                 best area to make the transition between                                 your different input images                                 so that's it for the hdr panorama                                 pipeline                                 and now we can switch to the features                                 regarding the photogrammetry guideline                                 so first of all for the photogrammetry                                 pipeline we have improved the texturing                                 in the context of topology so usually                                 when you have the                                 you get a really dense mesh                                 with thousands of vertices and after                                 manual                                 topology uh here in this uh really                                 extreme example we have just a few                                 triangles and it's really challenging to                                 texturate properly from the raw input                                 images                                 and so we have improved the solution to                                 be able to                                 better propagate the visibility to be                                 able to generate                                 more proper texture on these cases                                 and we have also added a new                                          on the meshing node this allows to                                 define a bonding box from your sfm so                                 uh in output of the sfm you have the                                 sparseline cloud                                 and you can use that to put a bonding                                 box and select the                                 part of your scene that you want to                                 reconstruct                                 um and it you can also                                 duplicate the mesh the machine node and                                 put different bonding box so you can                                 make a                                 kind of set survey of the full location                                 uh                                 with a certain amount of density and                                 then you are able to just                                 put another bonding box on a small                                 detail to be able to                                 generate a more precise reconstruction                                 of one object                                 in this place and this gizmo is also                                 integrated in the sfm transform to make                                 a                                 manual transformation or manual change                                 of the coordinate system in the                                 output.csf                                 in terms of visualization we have also                                 added a tool to be able to                                 look for the                                                        found                                 and we can overlay the original                                 input images and distorted images                                 to see in detail the variation                                 on the more technical side we have also                                 new visualization tools so here for                                 instance we can visualize the feature                                 points that we have extracted on the                                    images so we can see on the left                                 the blue and green                                 feature points are two two different                                 kind of features we extract on the image                                 and then we can see in orange the points                                 that have been matched to other images                                 and finally in red we can see the points                                 that are really                                 in the                                                                  really used                                 to solve the camera positions and                                 on the right if you if when we zoom in                                 detail of the                                 image we can see in blue the feature                                 point with the scale and orientation                                 and we can see the in red the                                    projection of the same point and this                                 small segment in red between the two is                                 the projection error that we get                                 there are also some widgets to                                 visualize the statistics of the                                 sfm it can we have some statistical view                                 on                                 some global statistics we have also                                 added some visualization                                 for the resource usages                                 and another point that we have in                                 photogrammetry is that we don't know the                                 coordinate system in output all our                                 geometric information is relative and                                 so there is a new option in the sfm                                 transform that allows to change the                                 coordinate system at the end                                 and we scale it so if we put marker                                 in our setup we can automatically                                 rescale it to the correct scale and this                                 allows to make measurement                                 from from that                                 and we have also added new nodes to make                                 alignment between scene and transfer                                 poses between scenes                                 so if you have an acquisition rig with                                 multi-camera system                                 you are now able to make a first                                 reconstruction with                                 with the calibration for instance and                                 and then we use that                                 as an initialization for other                                 reconstructions                                 and and it's also useful for research to                                 make                                 alignment between scene to be able to                                 align to grand truths and make                                 evaluations                                 we have also improved a little bit the                                 command line so now                                 you can create your scene in mesh room                                 save it um                                 as a standard mg file and then use that                                 as an input pipeline                                 uh for the command line and                                 and few other adjustments that could be                                 really useful there is also a new node                                 to                                 export the result directly from mesh                                 room to sketchfab                                 and and also                                 there is a new option to directly from                                 the                                           being able to visualize the intermediate                                 steps or the depth map                                 so here we can see depth map in                                        then we can also import it in                                    so here we see the depth map after                                 featuring                                 but we can select the node that we want                                 to                                 see so if we have multiple depth map                                 with different parameters etc                                 and here we can see the depth map b for                                 featuring and after filtering so we can                                 see that before filtering we                                 we have a lot of noise and a lot of                                 input candidates                                 and that are selected to get the                                 filtered version                                 and then when all the depth map are                                 fused all together                                 we get final mesh                                 and there is also a new node for image                                 processing so it's a preliminary work to                                 be able to                                 make some pre-processing of your images                                 directly within the mesh pipeline and                                 but this that can also be used on its                                 own to make some conversion                                 we have also improved the parameter                                 system so now the parameters are                                 dynamic so when you change one parameter                                 you you see only the uh                                 the relevant one for the on your nodes                                 and it's also changing validation system                                 so now                                 only the parameters that are really used                                 by the process                                 are used in the invalidation system                                 if we go more deeper in the library                                 there is a new option on the feature                                 matching to be able to                                 directly estimate                                 some distortion directly during the                                 feature matching so this allows to                                 recover more feature points when you                                 have some                                 images with large distortion like gopro                                 images for instance                                 and there is a lot of really small                                 improvement                                 in many places and                                 in particular regarding the metadata                                 support and the support of                                 more war images so i i would like to                                 take this opportunity to thank                                 the the team from openimagerio for all                                 the fixes they have made for us                                 and that's really useful                                 and so everything is uh is                                 on github and so it's under the mozilla                                 license and we provide the                                 windows and linux binaries and so                                 the next release will be uh                                 coming in the few weeks i think i'm sure                                 that that will be a question                                 that we raised when will be the next                                 release so now we're gonna take                                 talk a little bit of the further steps                                 of the project                                 as we can see it from now so                                 one of the extension could be the                                 possibility of having                                 an automatic camera tracking after the                                 shooting                                 because in fact the evaluation of the                                 live action camera                                 can take advantage of the structure from                                 motion algorithm                                 so the idea is to uh uh                                 based the first structural motion of on                                 the pawn clouds of the three-liter                                 construction of the set                                 and then to add the analysis of the                                 live-action camera                                 so this is only a premium test on our                                 production shots                                 but as you can see this we can already                                 get some interesting results so we'll be                                 working on that                                 the the the main idea behind all this is                                 to combine all the data acquisition on                                 set                                 uh within meshroom so we have the                                    reconstruction of the set                                 we have the hd panorama which is now                                 available in the next release and the                                 idea is to augment                                 this process with the live action camera                                 tracking                                 all done within the same tools with the                                 random farm possibility                                 and all the nodes that are available uh                                 within a mesh room                                 that the idea is to improve even better                                 the productivity                                 of the data acquisition on set and                                 to facilitate the integration within the                                 vfx pipeline                                 you want to talk about the test mode and                                 yeah the hub is one really                                 critical step of the pipeline because we                                 are we have                                 that's where we have most of the data to                                 process                                 so it's critical both in terms of                                 performances and in terms of quality                                 so we have made a lot of experimentation                                 during this year                                 and but it's still challenging to                                 improve the performance while keeping                                 compatibility with                                 old cab so but that's something that                                 definitely                                 needs to be continued this year                                 and on the more research side of the                                 project                                 we have finalized the phd thesis this                                 year                                 so we provide a first prototype for the                                 estimation of the lighting of the scene                                 and being able to use this lighting                                 information to refine the geometry                                 and and we will continue this work with                                 a new partnership between nicos and evie                                 to integrate this prototype into                                 mushroom                                 and and in the more long term go further                                 and uh                                 and try to analyze surface material                                 properties                                 okay now we're going to say a few words                                 on our community                                 so measure the night vision in fact is                                 being used by                                 various industries not only the vfx                                 industry                                 it's being used by the medical                                 industries on the left-hand side you can                                 see that there are some guys                                 in claremont doing some surgical                                 augmented reality                                 what they do is they do the the                                 three-year construction of the organ                                 that has been scanned                                 before in order to help the the camera                                 tracking                                 during the operation uh in the middle                                 you have the classical                                 usage of photogrammetry for cultural                                 heritage for instance we give you here                                 an example                                 or for the architecture design uh                                 with a an interesting integration of                                 machine pipeline                                 and on the right hand side we have uh                                 the people in nanter digital that we                                 know pretty well                                 we are making this digital double                                 project                                 here we can see what they are doing for                                 the the                                 face acquisition the face                                             and what's interesting here we can see                                 that they've used                                 the possibility of integrating their                                 production pipeline                                 into mesh room to be able to design                                 their custom nodes                                 regarding the interaction with                                 third-party software or acquisition or                                 whatever                                 so it's a pretty good that's a good                                 example of why we have built                                 software on the notable pipeline because                                 there is a lot of use cases for                                 photogrammetry                                 a lot of different constraints depending                                 on your acquisition setup                                 your acquisition condition also in terms                                 of lighting                                 etc and so your input can change a lot                                 so you may have to create some                                 specific processing or specific way to                                 declare the relationship and your                                 constraints you have                                 and and also in output                                 you get the first mesh from this                                 measurement you can add an extra layer                                 of interpretation to use that                                 in for specific needs so that's why                                 it's really important to have this                                 pipeline so people can build their own                                 they can adjust the input and address                                 the output                                 the community also has been for a great                                 help to build up the online                                 documentations                                 which is made around the redux framework                                 so we we have we                                 can take the opportunity today to thanks                                 again the people who have been                                 helping us for setting up this con this                                 documentation uh of course more                                 contributions are welcome and we give                                 you here the link                                 to if you want to contribute to the                                 documentation of mushroom                                 um so uh the the                                 the project is a collaborative project                                 since the beginning uh here we remind                                 you                                 the academic partners that have been                                 helping us for years                                 to start up this project but of course                                 more people are welcome whether you are                                 a researcher whether you are a developer                                 you are                                                          you are more than welcome to join us                                 because we are always interested in                                 in sharing new ideas start new                                 collaboration                                 because this is the way the project has                                 been started from the beginning                                 and today we have an announcement to to                                 do so                                 after these ten years of collaboration                                 uh with uh the                                                           we have decided to create a non-profit                                 organization                                 around mesh room builder under the                                 founding members of                                 of the project um we have seen also that                                 over the years                                 the the project has received a lot of                                 interest                                 and requests from other industries                                 that go in fact beyond the initial                                 objectives                                 that makes a lot of different use cases                                 different                                 acquisition systems like fabia mentioned                                 and                                 so this is challenging to address within                                 the same solution but                                 we think there is a great convergence                                 opportunity                                 uh regarding the needs of                                 the low-level blocks that are building                                 uh um                                 uh alice vision framework and uh                                 we think uh meshum can be a good help to                                 to build up these different pipelines                                 so that's what it's important for us to                                 continue build up                                 this open source ecosystem uh to take                                 advantage of the different use case                                 and the different data sets to improve                                 the the software                                 of course when you the objective of the                                 association is to                                 participate to the financing of                                 extra resources with donations and                                 sponsorships                                 and globally the the objective of the                                 association is to help us to build                                 a better software                                 okay i think it's time                                 we got the yeah we got we've got a                                 plenty of time to have uh                                 to answer to some of your questions uh                                 so we're gonna get through                                 the q a of the zoom                                 uh the first question is yeah yeah do                                 you plan to support aces is there a                                 roadmap to combine                                 leader and photography so that's two                                 questions so                                 for the first question so regarding acs                                 so there are some things that we have                                 looked at but                                 that's not so easy regarding dslr                                 because                                 in our case we are for photogrammetry we                                 are working from the sla                                 and and                                 there are some projects to use                                 convert dslr images to acs                                 but that's not straightforward because                                 the                                 acs rely on on the manufacturer to                                 provide the                                 [Music]                                 and so it's not provided for the seller                                 so um                                 so that's something that we would be                                 interested to discuss with uh                                 people from aces obviously and there was                                 a second question because                                 there was two issues in one regarding                                 the leader integration which has been                                 raised for a long time so we have a few                                 bits of answers                                 around this yeah we have made some                                 experimentation we have implemented a                                 different approach to make the                                 alignments and and now we need to                                 to make the proper integration the user                                 interface to uh                                 to connect all that together and make it                                 usable by a graphic artist station                                 next question is when the next version                                 build will be released                                 and it's an estimation that's a good                                 question                                 and it's always difficult to to answer                                 that because uh                                 always at the end we have all the tiny                                 things that need to be fixed                                 to provide a proper release                                 but we hope that we will be able to do                                 that in a few weeks                                 and of course we are testing this next                                 release with                                 within technical production services now                                 that's also important for us to test it                                 in production before                                 using it in open source so this is where                                 we are now                                 next question is does mission prefer                                 undistorted or distorted                                 original rectilinear images                                 so it depends on your quality of your                                 own distortion                                 but uh so basically both will work uh                                 if you have already undistorted your                                 images you you may                                 want to lock your um                                 your parameters because you have already                                 solved it and then it will try to solve                                 it again                                 okay what else i just answer live um                                 this one you already answered uh have                                 you already noticed this                                 this is one of the questions now this                                 it's a it's a facebook research uh                                 linked to consistent depth uh estimation                                 uh with the code release yeah we know                                 this paper                                 yeah we have seen the release it's                                 really interesting we have not tested it                                 yet but                                 it would be really cool to test and see                                 how it reacts                                 is there any plan planned to support                                 non-nvidia cards                                 he's asking an anonymous attendee                                 so that's so                                 non-nvidia would mean maybe nd                                 so i don't know if you mean a cpu                                 version or if you mean                                 a gpu version for the cards                                 so the point is that the depth map is                                 really                                 a critical step in terms of performance                                 so                                 with the algorithm that we have                                 implemented now                                 it would make no sense to do that on cpu                                 because it's too computational intensive                                 but it's also                                 what why it gives this robustness to the                                 results                                 so yeah so as i said we don't have any                                 plan soon to make a non-nvidia version                                 but                                 uh but obviously if there is some                                 initiative from the community we will be                                 really happy to support it and                                 uh and also what would make sense in my                                 opinion would be to                                 integrate other implementation and other                                 alternatives for cpu                                 with a completely different method and                                 and that would be also interesting to be                                 able to compare them                                 in the same system there's maybe an                                 answer in it                                 yes                                 gpu yeah no                                 short-term size right because                                 it's it's uh it's really a different way                                 to fund the problems yes open sale                                 could be a way but it's also really                                 complex to to                                 transfer it another question is do you                                 support                                                                                              freed protocol to motorize ptz camera                                 heads                                 uh yes i'm not aware of it so                                 if you can uh add an issue on the                                 repository that would be cool with the                                 link so we can look at it                                 and that would be quite easy to add we                                 can say                                 that we are supporting the the motorized                                 head used for the round shots                                 which is uh which has a specific xml                                 experience x drive or something                                 x driver yeah but not this one yeah yeah                                 this is uh if you put an issue on the                                 github with the link                                 that would be interesting to support                                 sure                                 uh does measurement follow the vfx                                 reference platform good question                                 yes we are trying to follow it                                 as much as possible so not for qt                                 because                                 we have too much                                 needs to have the latest versions but                                 for                                 all the steps for the compiler for the                                 environment                                 etc we are we are relying on it                                 okay                                 is it possible to use measuring with                                 video                                 with video inputs yeah that's rude yeah                                 so in the new version when you drop a                                 video in mushroom it will create a new                                 node                                 for extracting keyframe for your video                                 [Music]                                 so it's not a magic solution so you may                                 have to                                 make some adjustments on the keyframe                                 selection because it's really difficult                                 to find um                                 systems that work for all kind of video                                 it really depends if your video are                                 really high quality if you are um                                 if you have a lot of uncheck or or not                                 if you                                 if you are on the jaw and etc so you may                                 have to adjust some parameters but it                                 will extra keyframes                                 and and then from that you have to put                                 them back into mushroom again so it's                                 not                                 fully transparent yeah but we can say                                 that some people using drones have been                                 using mushroom already                                 even to make a rough camera tracking of                                 the drone so                                 everything is possible does measure will                                 create                                 auto photo in the future yeah that's a                                 request                                 we have a lot so that's something we                                 don't really need                                 so but that's something that would                                 really make sense                                 and i hope that will be the association                                 with the new association we will have                                 resources to do this kind of                                 features that are a bit out of our scope                                 okay uh i don't think i don't know if we                                 have                                 some other questions in the live chat                                 but because uh                                 uh we're gonna check just just in case                                 uh where                                 why fabian is doing that of course we                                 have a channel                                 in the academy software slack                                 so if anybody wants to ask for the                                 question or if we miss something                                 it's a good time for us to say that we                                 can we can catch up                                 later on in this channel of                                 aswf slack                                 another one would it be possible to                                 pre-filter input images by                                 quality as in detect noise as in                                 detect motion blur out of focus remove                                 or decrease the weight                                 of images in the solver this is a                                 question from a check yeah so that's                                 really a good question                                 so there are two things uh so it would                                 make sense to                                 uh to have a pre-filter step in input to                                 really remove the images that are really                                 too low quality but then it will also                                 make sense to                                 really take into account the blur                                 and so there are some some researchers                                 that are working on                                 on focus stacking using mushroom                                 and so i hope that with this                                 collaboration we will be able to improve                                 that                                 some people from australia yeah                                 do you have any questions if not again                                 you can use the slack channel                                 i think let me check back again                                 this one is done yeah                                 okay i think that's it thank you                                 everyone for                                 joining us yes uh again                                 good update guys thank you thank you for                                  the compliment                                  again we want to thank you the academy                                  software foundation to invite us to this                                  uh                                  open source panel uh because it's been a                                  an interesting year as everybody says                                  for everybody                                  um uh we have we have another one we can                                  take another one because                                  does it affect mushroom if the images                                  are optically or digitally stabilized on                                  the camera level that's a good one                                  yeah yeah it will uh it will affect                                  there                                  so yeah as much as possible we try to                                  get the raw images yeah yeah if you have                                  a solution to disable it that                                  that would uh help i know that on the                                  other side you will get more motion blur                                  and uh but yeah it will affect the                                  geometric estimation but uh i have not                                  made an intensive test to see                                  how far it will decrease but in                                  principle it will                                  decrease really okay                                  i think we are good uh so                                  thanks again take care of yourself take                                  care of your beloved and uh                                  we'll catch you up online uh due over                                  the the different uh channels                                  you have here the links to join us                                  and we can pick up the good work                                  bye bye thank you
YouTube URL: https://www.youtube.com/watch?v=oa_FofFYiYo


