Title: High Fidelity Unleashed with the Intel® oneAPI Rendering Toolkit
Publication date: 2020-08-24
Playlist: Open Source Days 2020
Description: 
	High Fidelity Unleashed with the Intel® oneAPI Rendering Toolkit
Speakers: Sean Mcduffee, Greg Johnson, Attila Afra, Jefferson Amstutz

For more information about the Academy Software Foundation go to: https://www.aswf.io/

Intel will present the oneAPI Rendering Toolkit product updates of their suite of open source libraries for high performance ray tracing. Hear more about their Open Volume Kernel library which provides high performance volume traversal and sampling functionality for a variety of volume types for a variety of computations optimized for the latest processors featuring SSE, AVX, AVX2, and AVX512 instructions enabling extreme performance for volume rendering engineers. Also hear about Open Image Denoise, which provides an open, high-quality, efficient and easy-to-use denoising library for path traced images. In conclusion this talk will discuss the collaboration with ANARI relating to the Analytic Rendering Interface project under the Khronos Group governance. This is a cross industry, open project aimed at cleanly separating the scene descriptions that Digital Content Creators (DCC’s) provide and the various backend rendering implementations which then generate images.
Captions: 
	                              okay i think we can start um                               welcome to the second day of open source                               days we're happy to be here with you                               today                               and we're from intel we're going to be                               sharing with you the                               open source uh intel one api rendering                               toolkit                               um just a little bit about who we are                                there's gonna be a number of speakers                                today                                we're part of the advanced rendering and                                visualization architecture group or arba                                inside of intel we're responsible for                                a number of products all fully open                                sourced some of which                                this audience may be already familiar                                with and other ones that were today                                we're going to present                                are a bit newer we're                                known for the embry open image denoise                                open bkl                                offspring open swerve and uh                                inari in part with a number of other                                partners from the cronus group                                once again like i said we're fully open                                sourced every project we work on                                um just quickly we're going to do a                                quick update for emory for people                                that may already be using embry and just                                updates over the last year                                for people that just a quick overview                                for embry embry                                is essentially a bounty volume hierarchy                                that offers ray geometry                                intersections optimized for all the                                intel                                instruction set architectures from sse                                through avx-                                   over the last year we've added a number                                of new curved geometry types such as                                capital rom                                round linear curves cone curves a new                                compact poly's                                cmake option allows double index index                                quad leaves reduces memory                                added support for multi-level instancing                                we've also added support for point                                queries so closest point nearest                                neighbors                                implemented a new min width feature for                                curves and points                                allows the tracker to increase the                                radius in a distance dependent way this                                is                                helpful for anti-aliasing techniques                                added quaternion motion blur the rtc                                commit scene                                function can now get called during                                rendering for multiple threads to lazily                                build geometry                                this improves on the previous                                implementation we had                                geometries can also now get attached to                                multiple scenes at the same time                                and we've added collision detection                                support so now embry                                supports collision detection broad phase                                collision detection for user geometries                                and there's been numerous performance                                and robustness improvements and coming                                very soon                                in the next the next release will be uh                                morton code and refilling builders for                                instance geometry                                um that's pretty much all we wanted to                                talk about with embry today if there are                                questions though                                um feel free to ask any questions on                                embry we can get to the                                to the end we want to concentrate on a                                number of other projects that we do                                uh first will be up attila afra with                                open image denoise                                and then greg johnson with openvkl                                and then jeff absence with anari so i'll                                just hand it off to                                attila for open image denoise                                thanks sean so i will quickly                                introduce openmhd noise a library for                                denoising for ray tracing                                i will briefly go into some details how                                it works                                uh how the api works as well and                                what's planned for the future so let's                                get started                                so denoising nowadays is getting more                                and more popular                                because noise is pretty much inevitable                                 in monte carlo ray tracing path tracing                                 solutions                                 um so this means essentially that in                                 many cases rendering fully converged                                 images is just too expensive                                 so the solution is basically rendering                                 partially converged images and then                                 denoising them with some kind of                                 algorithm                                 the movie industry is also already using                                 this approach                                 where it's possible to get um it seems                                 the slides are going forward without me                                 clicking on them                                 sorry about that okay so                                 um it's possible to get um two to                                     improvements uh with denoising without                                 losing                                 a significant amount of quality and it's                                 also actually crucial                                 for um real-time applications like games                                 where it's only possible to render only                                 a few samples per pixel                                 so without denoising the results will be                                 just completely unusable                                 so the solution is to use some kind of                                 denoising solution and interopen which                                 denoise is a denoising library which                                 solves this problem so this library is                                 specifically targeted for ray tracing                                 it provides a set of high quality deep                                 learning based denoising filters                                 and these filters are suitable for both                                 interactive and offline rendering                                 and it's also possible um to uh                                 train the deep learning based filters                                 shipped with the library                                 using the included uh training toolkit                                 which is based on pi torch i will                                 mention this                                 a bit later the library                                 runs uh on basically almost any modern                                 cpu                                 the only requirement is having ssc                                     and it of course takes advantage of                                 more advanced instructions such up to                                 avx                                             it runs on all three major popular                                 platforms linux windows and mac os                                 and the api was designed to be clean                                 and minimal so it can be integrated very                                 easily into                                 existing rendering solutions in many                                 cases the integration could be completed                                 in just a few hours                                 so let's see the denoising filters that                                 the library contains so the library                                 contains currently two                                 filters all retracing oriented one of                                 them is a generic                                 filter called rt which uh it aims for                                 denoising                                 um generic path raised final frames                                 and there's also dedicated uh lightmap                                 denoising filter called                                 alt-rt light map which provides higher                                 quality for                                 um ray traced light maps                                 these filters specifically the generic                                 filter                                 accepts potentially multiple input                                 buffers of course there's the color                                 buffer which could be either                                 hdr or ldr and there are also                                 ways to provide auxiliary uh feature                                 buffers                                 which are completely optional for                                 example it's possible to provide albedo                                 and normal buffers which can                                 significantly improve the denoising                                 quality                                 these buffers can be noisy as well just                                 like the color so this doesn't have to                                 be                                 noise free                                 so let's see uh the input and output                                 buffers for the filters                                 so there's um of the color buffer and                                 then the option will be the normal                                 buffers                                 so here we can see an example for a                                 scene rendered with patch tracing and                                 providing these three input buffers                                 where                                 the color is the most the noisiest one                                 but the other two buffers are also noisy                                 just they are not really visible in this                                 small size                                 providing these buffers to the library                                 will create the denoised                                 image on the right so basically this is                                 how it works now just to mention a                                 little bit about the actual algorithm                                 this is based on so                                 the algorithms in openmhd noise are                                 based on                                 uh deep learning approaches specifically                                 it uses a convolutional neural network                                 which has in this case a unit                                 architecture                                 it provides a good balance between                                 quality and performance                                 so it's it provides a quality that is                                 suitable for final frame rendering but                                 also                                 it's fast enough to use it in                                 interactive applications on many core                                 cpus                                 since this is a deep learning based                                 filter so it needs to                                 have a trained model as well for it and                                 the library ships with a set of                                 pre-trained models but                                 it's not necessary to use them users can                                 simply train these models with their own                                 data sets and these can train models can                                 be                                 specified through the api                                 so let's see a quick example of how open                                 image denoise works what kind of results                                 it can produce so here we can see a                                 frame from                                 um the spring open blender                                 open movie this was rendered with the                                 cycles render at                                                      it's quite noisy as we can see let's see                                 what's the denoising output uh if we use                                 uh i'll be the normal buffers as well so                                 with this                                 um we can achieve the ssim of                                       and just to see how well how the                                 reference looks like                                 this is this is the ground truth                                 um let's see another example so                                 this image was rendered with the corona                                 renderer                                 at                                                            see the denoised version also using                                 albedo normal buffers as well                                 so this is the denoised version as a sim                                 of                                      roughly and finally                                 this is the reference image so that                                 there is a difference of course                                 but it's still an acceptable compromise                                 in many cases despite the fact that it's                                 just                                                      of course the library works well with                                 higher sample counts as well it's just                                 more visible in presentations like this                                 to                                 see lower sample counts just a quick                                 mention about the performance                                 just to give you a rough idea of the                                 ballpark performance that                                 open which dns can achieve on a                                         machine with avx                                                  possible                                 to denoise a                                                        milliseconds a full hd image in around                                                                                  and a                                                              milliseconds so given enough cores                                 uh it's possible to use um open image                                 dns for interactive applications as well                                 i will just give a quick overview of the                                 api                                 so the api is quite similar to the embry                                 api in design                                 and philosophy it's um it has c and c                                 plus plus versions as well so it has a c                                 plus wrapper included too                                 um the api is object oriented and there                                 are basically only three                                 uh objects available in the api there's                                 a device object similar to embry                                 a buffer object and a filter object                                 these are all reference counted                                 and this means that the library is                                 is quite compact and should be quite                                 easy to use                                 so let's see a quick example how this                                 works                                 the images can be denoised with these                                 filter objects and those can be created                                 by first                                 creating a device object um after the                                 creation of the device object we can                                 just create the filter selecting what                                 kind of                                 filter would like to use in this case we                                 use the generic rt filter                                 next we have to set the filter                                 parameters which i will show in the next                                 slide                                 we have to commit the changes before                                 using the filter and we just executed                                 and then finally we clean up um so this                                 is the basic                                 um step these are the basic steps needed                                 to denoise an image                                 let's see what happens in the filter                                 setup                                 so the filter setup is also quite simple                                 sorry                                 oops                                 okay sorry about this                                 okay so first um either we need to use                                 normal buffer objects or so called                                 shared buffers where we don't create any                                 buffer objects just                                 pass pointers that the application                                 manages                                 um just like with embry we provide what                                 type of                                 buffer we specify the color albedo                                 normal or the output itself                                 and we can also specify offsets and                                 strides                                 so it's quite flexible and different                                 image layouts are possible                                 there is also a way to specify whether                                 the images hdr ldr and some other                                 parameters so this is the basic                                 idea of how to use the library                                 i would also like to mention the                                 training toolkit which was introduced                                 quite recently                                 to the library uh it um the source code                                 ships with a pytorch-based neural                                 network training toolkit which consists                                 of multiple python based scripts                                 with these the users can train the                                 filters with their own data sets which                                 has the advantage that it's possible to                                 optimize the filters                                 for custom renderers sample counts                                 content types or scenes                                 so it can potentially improve quality                                 and                                 these user trained filters can be either                                 built into the library                                 or is these can be loaded at runtime                                 through the api                                 so there are multiple ways to use these                                 custom uh                                 custom uhly trained filter models                                 um also i would like to mention uh                                 what's what is planned in the future for                                 openmhd noise                                 so we're continuously uh improving the                                 quality and performance as well                                 um one of the next features that we'll                                 introduce is                                 directional lightmap denoising so we                                 have dedicated                                 filters for not just regular lightness                                 but directional ones as well for example                                 uh using spherical harmonic basis or                                 some other bases                                 um we're also working on adding temporal                                 coherence                                 where denoising animations would result                                 in                                 much better temporal stability and                                 finally we are also working on gpu                                 support as well                                 the api is hardware agnostic so                                 uh basically adding a new type of device                                 will not require changes on the                                 application site perhaps only just                                 picking which device to use which could                                 be also                                 using a default one so it shouldn't                                 cause any disruption in the                                 applications um so as a conclusion                                 um so op intel openmhd noise is as we've                                 seen an open source denoising library                                 for retracing                                 it's suitable for both interactive and                                 final frame rendering                                 it runs on all modern cpus that support                                 ssc                                     it has a simple clean api and                                 you can download it from github and                                 multiple resources are available at                                 openimagedenoise.org where you can                                 find the detailed api documentation                                 example code                                 and a gallery as well for more example                                 images                                 thank you                                 [Music]                                 okay i am greg johnson i will be                                 presenting                                 intel open volume kernel library or                                 openvkl                                 so this is the the newest component of                                 rendering toolkit so today i'll give a                                 brief overview of openpkl                                 including describing its design goals                                 i'll talk in some detail about its                                 supported volume types as well as the                                 apis supported on those volume types                                 and then wrap up with resources where                                 you could find                                 additional information so openvkl                                 is part of intel one api rendering                                 toolkit                                 which is an open source set of libraries                                 for                                 advanced rendering and visualization in                                 terms of a typical application software                                 stack                                 uh openvkl sits at the same level as                                 embry um                                 as a low-level kernel library uh so                                 today                                 uh vkl is used extensively within osprey                                 which is our scalable                                 ray tracing based rendering engine and                                 there it provides                                 all of its volume functionality enabling                                 all of its                                 volume rendering features                                 so open vkl can also be used within                                 any renderer or any application to                                 improve volume rendering features                                 or performance it has a cmd focused                                 internal implementation with most                                 kernels written in ispc                                 and so we're able to take a very good                                 advantage of                                 cpu vector instructions uh up through uh                                 avx-                                    so intel has announced the intel xe                                 architecture                                 gpus and so we won't talk in detail                                 about that here                                 uh other than to say that open vkl and                                 the other libraries shown here                                 uh will support uh those libraries in                                 the future                                 uh so what are the goals of openvkl                                 so put very briefly open dkl intends to                                 be                                 an embry-like library for volumes and so                                 as part of this one of bkl's goals to                                 support a diverse set of volume types                                 just like embry supports a diverse set                                 of geometries                                 and so this enables applications and                                 renderers                                 to support many types of volumes through                                 a single coherent api                                 and generically written renderers and so                                 this allows                                 applications to more easily make runtime                                 decisions about uh which volume                                 representation to use                                 uh and as we'll see there are trade-offs                                 between uh between these different                                 volume types                                 so it's important to allow applications                                 uh to make those                                 switches between volumes with minimal or                                 no changes                                 to application code so open vkl does                                 provide                                 a flexible set of apis primarily                                 targeted at rendering applications                                 and so very specifically authoring and                                 volume manipulation workflows                                 are out of scope for openpkl we are                                 really laser focused on                                 supporting rendering applications in the                                 apis those need                                 uh specifically uh so we can talk about                                 open bkl's features both in terms of its                                 supported                                 volume types as well as the apis                                 supported on those volumes and so uh for                                 volumes we support uh many different                                 representations                                 ranging from dense structured uh in vdb                                 sparse structured                                 uh all the way through uh fully                                 unstructured volumes and and particle                                 volumes                                 and then for apis we support uh                                 volumetric sampling                                 uh gradient computation uh ray-based                                 interval iteration which i'll talk more                                 about                                 uh implicit iso surfacing or surface hit                                 iteration                                 uh and volume observers where i'll also                                 show an example of that                                 so the first volume type is structured                                 regular volumes                                 so here the domain is a regular grid and                                 the                                 the volume data is essentially a                                    array of dense voxels and so                                 this volume type provides extremely fast                                 access                                 but does have a relatively large                                 footprint in memory                                 and so this volume type is is very good                                 for                                 fast rendering when data fits into                                 memory                                 uh the next volume type is is very                                 similar so the domain isn't a regular                                 grid but                                 but now on spherical coordinates on                                 radius azimuth and                                 elevation and so this has similar                                 advantages and disadvantages                                 uh to the previous volume type uh but of                                 course this volume                                 is very well suited to problems with a                                 natively spherical structure                                 and so um applications or                                 simulations that use this volume type                                 can pass it to decal directly and                                 there's no need to transform the volume                                 or resample it onto a                                 a less efficient volume type                                 so we then have adaptive mesh refinement                                 volumes or more specifically                                 block structured amr so we have some                                 more flexibility here where we could                                 provide data                                 as nested structure structured regular                                 grids                                 so these nested grids must fit neatly                                 into their parent but otherwise we're                                 free to have                                 as many levels of refinement as we want                                 and so                                 a very flexible volume type the                                 traversal is somewhat more expensive                                 one important note is this is often used                                 in in large-scale                                 uh scientific simulations so again                                 uh can pass that volume type directly to                                 bkl without                                 needing to transform it                                 another volume type we support is fully                                 unstructured volumes and so here we                                 support                                 arbitrary combinations of multiple cell                                 types so                                 tetrahedra hexahedra wedges and pyramid                                 and there's really no regular structure                                 and so we can                                 place ourselves wherever we want really                                 adapt to                                 a regular forms put our resolution and                                 cell data where we need it                                 because of this um you know no regular                                 structure uh traversal is                                 uh moderately more uh expensive um                                 so this is another volume type                                 extensively used in in large-scale                                 simulations                                 uh finite element modeling and and so                                 forth                                 um and our our newest volume type uh                                 which is somewhat different from the                                 previous                                 uh cell and voxel-based volumes is our                                 particle volumes and so                                 uh here a volume is is made up of many                                 individual particles                                 where each particle has its own position                                 weight                                 and radius of influence and so when you                                 sample a volume here at a point location                                 uh any number of particles are able to                                 contribute to that sample value via                                 gaussian                                 radial basis functions so so this volume                                 type is used                                 extensively in large-scale cosmological                                 simulations                                 molecular dynamics and and other domains                                 and then last but not least we have our                                 our bdb volumes and so these                                 are really pervasive throughout vfx and                                 professional rendering                                 and so here the domain is a regular grid                                 and it supports sparse storage through                                 nested                                 uh regular grids where each level is a                                 power of two                                 in in resolution and so uh because of                                 this regular structure we have a                                 relatively fast                                 traversal and fast access and of course                                 a huge advantage                                 is an efficient use of memory                                 through this sparse representation                                 so some more detail on our vdb                                 implementation                                 so vkl does have a compile-time                                 configurable                                 topology for the different levels of the                                 tree and we do have a custom                                 implementation for                                 tree build and traversal so this is                                 implemented in isbc so we're able to do                                 a full vector-wide you know build                                 traversal sampling and so forth to make                                 that very fast                                 we support multiple reconstruction modes                                 so today that's                                 nearest filtering and tri-linear                                 filtering                                 and then we have a hierarchical dda                                 implementation which we use in                                 interval iterators which i'll talk about                                 shortly                                 the animation you're seeing here is an                                 example of                                 bkl's volume observers and so volume                                 observers                                 are a flexible component of the api that                                 allow volumes                                 uh to essentially communicate arbitrary                                 metadata um                                 about their uh usage and and statistics                                 and and so forth and so                                 in this example um the uh the the white                                 parts of the of the cloud are uh                                 internodes of the the vdb                                 tree and uh through volume observers                                 we're able to communicate back to the                                 application uh which regions of the                                 volume                                 where sampling is occurring and then                                 we're able to                                 dynamically load leaf data on demand and                                 so                                 this is a flexible mechanism for                                 application side                                 on-demand loading                                 um so with our vdb support we also ship                                 a vdb                                 utility library so this integrates                                 directly with openvdb                                 and allows for a very simple essentially                                 one line loading                                 of uh vdb data sets uh so one important                                 point                                 is that um the the leaf format that                                 openvkl uses is completely compatible                                 with                                 openvdb so you can pass those pointers                                 directly to vkl and through through our                                 shared buffer objects                                 vkl doesn't even need to make a copy of                                 those so                                 memory efficient and zero copy today we                                 are limited to                                 scalar flow fields but that will likely                                 change in the future                                 um so openvkl supports many of these                                 rendering based apis shown at the top                                 the api overall is a c                                                   is very similar                                 to the other render kit components we do                                 provide                                 ispc bindings and so if you're writing                                 code in isbc                                 and using uh those uniform and varying                                 constructs you're able to                                 also call directly into vkl we do have                                 multiple supported api modes for most of                                 the api                                 shown and so taking sampling is an                                 example                                 so we support scalar sampling where you                                 can sample a                                 single point location at a time we                                 support                                 vector-wide sampling where you could                                 sample                                 four eight or                                                          time and under the hood we'll make                                 efficient use of                                 sse avx avx-                                                            and then we also have a stream-wide                                 sampling where we can sample                                 arbitrary numbers of points at the same                                 time and under the hood that will                                 efficiently map                                 to the most appropriate vector-wide                                 interface                                 vkl does have fairly flexible data                                 interfaces for providing data to it so                                 i already mentioned shared data buffers                                 we also support                                 strided input data for which can make it                                 easier to work                                 directly with data as it exists in                                 application memory or maybe some custom                                 structs                                 and then we have a module implementation                                 that does support user extensions and so                                 if you had your own volume type you                                 wanted to implement that is possible uh                                 within vko                                 so one of the apis i wanted to focus a                                 little more in on is our                                 ray-based interval iteration or interval                                 iterators and so this allows                                 you to iterate over meaningful intervals                                 along array                                 and what i mean by meaningful is that                                 you are able to tell openvkl                                 the volume values of interest through                                 what we call a value                                 selector object and so that means that                                 when you start                                 iterating let's see                                 we accidentally went forward a slide                                 so that means when you start iterating                                 along array you're able to                                 skip any intervals that are known to to                                 not contain those values of interest so                                 that's in a very efficient form of empty                                 space skipping                                 and then the intervals you get back are                                 not necessarily boxes                                 so this is somewhat configurable per                                 volume and so                                 as an example for bdb volumes you could                                 configure                                 the the depth of the bdb tree at which                                 you intersect                                 to find these intervals and then finally                                 with the interval data you                                 do get back additional metadata so one                                 one piece of information                                 is the volume value range that you can                                 encounter when sampling along that                                 interval                                 and so this this value min max is useful                                 for rendering methods such as delta                                 tracking where a very tight bound on on                                 volume majority                                 can make your overall rendering much                                 more efficient through                                 requiring less sampling for example                                 um so that's that's the overview i                                 wanted to give uh there's more                                 information available                                 at openvcl.org uh api documentation                                 uh tutorials uh in the code base example                                 renderer                                 showing uh ways to integrate open vkl                                 we are part of render kit so you could                                 find information there                                 and then if you are attending siggraph i                                 encourage you to                                 check out our demos and so we have                                 several demos coming from our group one                                 of these is a                                 large scale cloudscape interactive demo                                 where you see images there on the right                                 so                                 this makes heavy use of openvkl                                 osprey and osprey studio and i believe                                 those those demos are going live on                                 august                                           thank you and i will turn it over to                                 jeff amsters                                 all right and i have control great                                 so i'm going to talk about anari which                                 um                                 is a chrono standard so it's it's                                 obviously not an intel                                 project it's uh it's cross cross vendor                                 um standard that's emerging it has been                                 in uh it was at first in an exploratory                                 group and then                                 um middle of this year we transitioned                                 to a full working group working on a                                 spec                                 so i'm going to just give a very brief                                 overview of what it is and what we're                                 working on                                 knowing that we just had a webinar                                 yesterday that                                 got on youtube really quickly so if you                                 search onari webinar                                 we have an entire hour length                                 presentation on it so this is going to                                 be                                 condensed even from that so hopefully                                 this what's your appetite for what an re                                 is                                 and what it isn't so first anari                                 is it stands for the analytic rendering                                 interface                                 and the the word analytic came from                                 a largely that the uh the api                                 uh that we're working on emerged from                                 scientific visualization                                 but just know that what we're working on                                 is not actually domain specific                                 so hopefully that the analytic part                                 doesn't get too distracting from                                 from what we're working on but the goal                                 is to provide a                                 a commoditized interface to talk to a                                 live software rendering system                                 and so all that means is a lot of us                                 have have data we have file i o scene                                 graphs um                                 applications that have their their scene                                 ready to go                                 but we have uh there and you'll there'll                                 be an image to describe this later but                                 we have the problem of having to talk to                                 everyone's custom api to                                 translate that data from the application                                 and share it with the renderer so the                                 renderer can make an image of it                                 so an aria is seeking to commoditize                                 specifying the what needs to be rendered                                 and then how to get an image back still                                 letting the render define all of the why                                 and the how                                 that that rendering gets gets done so                                 the the the                                 illustration at the bottom is you have                                 some input scene                                 uh you have some camera position and you                                 plumb that through inari to let                                 someone's renderer then make you an                                 image                                 that is desirable                                 so uh the this emerged from scientific                                 visualization where there were a number                                 of uh software packages                                 like vtk and paraview um that                                 every time there was a desire to use                                 like a new ray tracing                                 renderer that was shipped from a vendor                                 such as osprey or                                 there's viz rtx from nvidia there's a                                 number of great renders out in the wild                                 every single time                                 that was desired to be brought into the                                 the software package that you have to do                                 an integration                                 with that even though that that level of                                 translation to say                                 this is my application scene and i want                                 you to make me a frame                                 that that uh from the uh                                 the technical standpoint was largely                                 still saying the same thing that we had                                 to spell it with different                                 different apis over and over and over                                 again uh so to get to the                                 the lower level goods of what a renderer                                 vendor might provide                                 we're trying to commoditize that to go                                 from the left to then the right                                 where we have all of these packages                                 talking to a single                                 um a single render api and then let let                                 vendors                                 worry about the renderer with the                                 applications worry about their                                 application                                 so more concretely to put some names to                                 this uh this is the the very high level                                 stack                                 of where nre sits and nr is very thin                                 this is not trying to actually                                 implement a lot but instead standardize                                 the way the top level applications talk                                 to the lower level renderers                                 so we have some examples of renderers                                 we've been working with in cybez osprey                                 viz rtx and then we we've even had amd                                 participation                                 in the working group with radeon                                 prorender but at the end of the day                                 you could substitute your renderer                                 commercial or                                 open source uh you could insert that                                 there that takes advantage of lower                                 level apis like we've talked about embry                                 and openvkl                                 there's optics all of these things to                                 make a renderer                                 um but and then of course the hardware                                 underneath it but inari is trying to                                 not um take on the entire scene graph                                 problem                                 which deals with file i o structural                                 updates uh metadata custom to a domain                                 all kinds of different concerns we're                                 trying to let that be                                 domain specific but once you boil down                                 to want to render an image                                 that that generally looks the same                                 that's what we're trying to commoditize                                 so succinctly this ends up being a                                 viewport rendering api                                 uh so not a scene graph there's so much                                 uh both                                 um application specific domain specific                                 um uh libraries and tools out there that                                 exist                                 and uh standardizing on all of those is                                 really ambitious and difficult                                 so we're trying to make sure that what                                 remains in scope                                 is um going to actually serve us uh to                                 benefit                                 um uh to con basically manage                                 expectations is what that boils down to                                 for an re um and then also to to help                                 with this                                 uh to maintain that separation is our                                 goal is to remain                                 maximally unidirectional and what that                                 means is all that structure                                 data arrays where parameters come from                                 where they're queried remains in the                                 application and you share it with an re                                 and ari is not uh not trying to be a                                 framework to go build all of those                                 things                                 um instead it's how do you share that                                 information with a renderer so this                                 gives us lots of freedom to do                                 interesting things                                 as renderer implementers uh to be                                 divorced from                                 from all of that complexity while still                                 dealing with the complexity we have by                                 implementing renderers                                 and one cool example of this there's                                 there's a ton of                                 possibilities there what what freedom                                 helps us with as                                 implementers of rendering systems but um                                 just diverse execution topologies is                                 one that is a big deal in cybiz and one                                 we're taking very seriously with                                 with an re i think most of us probably                                 think when we                                 we want to render something we think of                                 the the left-hand side we have a single                                 application running on a single machine                                 talking to an api                                 that has a single implementation with                                 some cpus and gpus under it that's                                 absolutely                                 like a huge part of what an r is going                                 for but there are these others that                                 um that when you have a standard                                 semantic capture of what the scene is                                 and then                                 how to the desire to get frames back                                 from a certain vantage point of that                                 scene                                 you can do interesting things like an                                 implementation can farm off                                 even subparts of the same image to                                 multiple nodes in like an mpi cluster                                 or the far right side you could have an                                 existing distributed memory                                 application like an mpi simulation                                 where bits of the scene are on different                                 nodes and they all talk to the same                                 inaudi api                                 and then the underlying implementation                                 can take that and make a single image of                                 all of the data                                 these are things that we've been doing                                 in cybiz for a while now                                  but uh being able to um lower the bar to                                  be able to access those renderers is                                  is something that we're we're keeping in                                  mind as we're working on an re                                  so the current status is where we've                                  been a uh                                  a working group for just a couple of                                  months now um                                  it's it's very early on but we've got                                  some really cool examples already we                                  have                                  an early first light of vtk with some of                                  its infrastructure for uh                                  rendering uh vtk for those of you that                                  don't know is the visualization toolkit                                  is a very ubiquitous                                  package from kit where um to to deal                                  with                                  filtering and then rendering of                                  scientific data                                  or simulation data and we have a first                                  light of it                                  uh given that it was part of the                                  participating reason and cause for an                                  ari to                                  to start as an effort and then also vmd                                  is a visualization um                                  package for molecular dynamics vfd                                  and both both of those have been                                  early adopters to help drive some of the                                  the changes                                  or the the design decisions we're making                                  at inari and we have a number of                                  implementers as well                                  which has been very exciting we're still                                  exploring lots of                                  um possibilities uh we we                                  from a technical perspective we're                                  trying to write the software first                                  and then write the spec because talking                                  about a spec outside of                                  what we're going to do in practice is                                  really difficult                                  to keep the discussion focused so what                                  we what we did is we took the osprey api                                  and we have have taken it and started                                  working with that                                  as like a concrete starting point and                                  there's a number of                                  of of good changes that we've been                                  working on there to                                  to make sure it best fits all of those                                  execution topologies different renderers                                  different shading models different                                  volume types uh surface types all of                                  these things are properly handled                                  um but we're making a lot of exciting                                  progress there                                  um and so this this showcases uh                                  not that we're all trying to implement                                  the same renderer                                  but rather vmd um has                                  you know all this this scene loaded in                                  memory ready to go                                  and was able to hand it to the nr api                                  and just by virtue of                                  of selecting what back end uh anari was                                  was being                                  used um we could get a a great image                                  from a gl                                  implementation that nvidia's working on                                  called nvgl osprey                                  nrx is a optics-based implementation                                  that                                  videos was working with and of course                                  the far right is                                  all of the code that uh was hand written                                  inside of vmd what it's looking                                  it's looking like we even have uh                                  efforts from john stone who works                                  about uh who leads the vmd effort to um                                  work on his rendering code also be in                                  terms of the nra api so then there's so                                  much                                  code that can get removed from vmd to                                  dispatch to these different rendering                                  packages without having to re-implement                                  them                                  so it's it's an exciting uh gamut of                                  possibilities and this is just a first                                  light                                  but all of that being said um i just                                  wanted to wet your appetite with some                                  things that                                  when you commoditize how you talk to a                                  rendering system we're not trying to                                  standardize on                                  necessarily uh the limits of of what is                                  the perfect material model what is the                                  perfect lighting model                                  what we're trying to do is is say if you                                  have um                                  like osl or materialx or you have a                                  bunch of like the                                  like the disney principled uh brdf                                  shaders in your renderer                                  that we want to standardize the way you                                  express that to your live system and                                  then                                  uh encode that to the renderer instead                                  of um                                  saying like there's only one answer and                                  everyone should use that still a good                                  discussion but not                                  it's outside of the scope of what we're                                  doing meaning uh with                                  with the the principled brdf and car                                  paint shaders and                                  uh and osprey um an ari can can make                                  images like this                                  or make images like this where we                                  combine um                                  you know volume rendering and surfaces                                  together uh the api                                  lets us still express all kinds of of                                  details that we find in professional                                  content creation so                                  i guess the call to participation here                                  is that                                  anari is yes emerging from scientific                                  visualization but absolutely something                                  that                                  could be very relevant and useful in the                                  professional rendering space as well                                  we have a lot of people participating um                                  it's it's been exciting to see lots of                                  people want to get on board with                                  um helping craft what an re uh is                                  starting to look like                                  but we we are absolutely looking for as                                  many people who want to come participate                                  as possible                                  and so we we are a working group uh we                                  do                                  now have a um an email                                  list or an email address that we you can                                  send questions to if you're not                                  a participating work group member and of                                  course the um                                  the feedback is always welcome uh things                                  that                                  you would like to see in something like                                  inari uh and then of course                                  uh we would love it if you come                                  participate in the working group itself                                  we have lots of ideas and and would love                                  to know if they're                                  relevant and of course your ideas that                                  you have that we haven't thought of                                  and with that i'll turn it back to                                  i guess sean                                  now unfortunately we can't hear you sean                                  can you hear me now a little bit                                  well it's great thank you very much if                                  you have any questions please feel free                                  to use the question and answer box here                                  or if um have a couple minutes left                                  but if you're watching this record later                                  um please feel free to go on github for                                  any of these projects and to                                  engage with the mailing lists everybody                                  on this team is                                  sean i think we we lost you again but                                  maybe i can                                  facilitate the q a uh                                  uh so the art we do have our first                                  question that is does open vk have                                  support for motion blur volumes                                  and greg and we have johannesburg as                                  well i guess could take that                                  uh yes so today open vkl does not                                  currently                                  support motion blur volumes through                                  through our api                                  this is something we are looking into i                                  would say if you have                                  specific needs regarding motion blur on                                  specific volume types and so forth                                  you know feel free to reach out to us                                  via github and so forth and                                  that will help us prioritize our                                  development                                  great um do we have any other any other                                  questions from                                  the audience while we're on the line                                  yeah so um looks like we're getting                                  close to the end today and i don't know                                  if there's any other questions that um                                  that may be out there we do have uh                                  we have github is where you can find all                                  of our projects and always feel free to                                  reach out to us there with specific                                  concerns about                                  um each project probably using github                                  issues you can also find                                  mailing lists as well associated with                                  with those projects to contact us and                                  we'd love to hear from you about things                                  you're you're interested in                                  things you're doing uh with our projects                                  or things that uh                                  our projects aren't yet doing that are                                  interested to you um                                  cool we do have a question um we'll                                  uh the charts here and the seminar be                                  available                                  um                                  [Music]                                  and it is being recorded and so this                                  talk will be up on youtube                                  and i'm not sure about the slides                                  themselves                                  but you can at least see this talk again                                  up on youtube after after today                                  okay thanks everyone we appreciate your                                  attention being with us and                                  hope to see you in the future have a                                  good one                                  see ya
YouTube URL: https://www.youtube.com/watch?v=lK0jHoXmU9E


