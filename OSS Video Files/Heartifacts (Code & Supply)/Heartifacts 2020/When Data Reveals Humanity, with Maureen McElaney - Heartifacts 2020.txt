Title: When Data Reveals Humanity, with Maureen McElaney - Heartifacts 2020
Publication date: 2020-08-28
Playlist: Heartifacts 2020
Description: 
	Heartifacts is a Code & Supply conference that encourages intimate discussions about mental health, community building, career management, and other topics software professionals need to talk about more.

Learn more about Heartifacts at https://heartifacts.codeandsupply.co and grab some Heartifacts logo merch at https://codeandsupply.co/x/hfmerch. Learn more about Code & Supply at https://www.codeandsupply.co.

------

Technology has a deep impact on society. When you work in tech, you can see things about society that people without your background can’t see. I’d like to tell a story about my experiences teaching women in prison. I’ll illustrate the difference in my experience before and after working with a public dataset that proves people are kept in prison unjustly. This talk is about how you need to be careful about the things you build, and ask the difficult questions about what it will be used for. But this talk is also about the fact that when building new technology, you MUST make decisions based on research and data. This talk is about my experience playing with data and how that translated to deeper understanding and greater compassion for people.
Captions: 
	00:00:00,240 --> 00:00:04,640
um maureen is our final speaker of the

00:00:02,480 --> 00:00:07,600
day maureen mceldenny

00:00:04,640 --> 00:00:09,679
is a program manager at ibm quantum and

00:00:07,600 --> 00:00:10,240
cues it developer advocacy hope i said

00:00:09,679 --> 00:00:12,320
that right

00:00:10,240 --> 00:00:13,920
she's an organizer for women in machine

00:00:12,320 --> 00:00:16,160
learning and data science and on the

00:00:13,920 --> 00:00:18,240
board of the vermont technology alliance

00:00:16,160 --> 00:00:19,760
she's an experienced community builder

00:00:18,240 --> 00:00:21,119
and is passionate about building

00:00:19,760 --> 00:00:23,519
diversity of all kinds

00:00:21,119 --> 00:00:25,279
in the tech through education mentorship

00:00:23,519 --> 00:00:27,119
and advocacy

00:00:25,279 --> 00:00:28,720
marine's here to share a talk that i

00:00:27,119 --> 00:00:29,679
think is going to be the perfect closing

00:00:28,720 --> 00:00:33,359
to artifacts

00:00:29,679 --> 00:00:33,359
when data reveals humanity

00:00:34,399 --> 00:00:37,280
thank you sarah

00:00:38,480 --> 00:00:41,760
i want to thank the heart of facts

00:00:40,160 --> 00:00:44,719
community for allowing me

00:00:41,760 --> 00:00:45,120
to speak with you today it is truly an

00:00:44,719 --> 00:00:47,840
honor

00:00:45,120 --> 00:00:50,000
to be part of the speaker lineup with so

00:00:47,840 --> 00:00:52,800
many people who i know

00:00:50,000 --> 00:00:54,000
or know from the internet and greatly

00:00:52,800 --> 00:00:57,039
respect

00:00:54,000 --> 00:00:58,559
um sarah gave me a great intro but i

00:00:57,039 --> 00:01:00,079
wanted to talk a little bit more about

00:00:58,559 --> 00:01:00,559
me just because i feel like it's a it's

00:01:00,079 --> 00:01:02,640
a

00:01:00,559 --> 00:01:04,559
hard facts conference we should talk

00:01:02,640 --> 00:01:06,320
about the human side of ourselves as

00:01:04,559 --> 00:01:08,560
well so aside from just tech

00:01:06,320 --> 00:01:09,520
i'm also a mother my daughter is two and

00:01:08,560 --> 00:01:12,080
a half

00:01:09,520 --> 00:01:13,280
i live in vermont with my partner and i

00:01:12,080 --> 00:01:16,479
grew up in philadelphia

00:01:13,280 --> 00:01:20,159
i graduated from lasalle university

00:01:16,479 --> 00:01:22,720
but i didn't go to school for a techie

00:01:20,159 --> 00:01:24,640
degree i made a career switch into tech

00:01:22,720 --> 00:01:27,040
about 10 years ago

00:01:24,640 --> 00:01:28,960
and this talk focuses on the thing that

00:01:27,040 --> 00:01:31,040
i am most passionate about

00:01:28,960 --> 00:01:32,880
which is using technology to solve

00:01:31,040 --> 00:01:35,040
problems for humans

00:01:32,880 --> 00:01:37,119
and how thinking about the humans who

00:01:35,040 --> 00:01:38,159
will be impacted by the technology that

00:01:37,119 --> 00:01:42,320
you build

00:01:38,159 --> 00:01:44,399
will transform you and society

00:01:42,320 --> 00:01:45,600
so this talk is about the ways in which

00:01:44,399 --> 00:01:48,079
all of our work

00:01:45,600 --> 00:01:49,119
in tech is actually in larger service to

00:01:48,079 --> 00:01:51,200
humanity

00:01:49,119 --> 00:01:52,399
without recognizing that data is

00:01:51,200 --> 00:01:55,200
generated by human

00:01:52,399 --> 00:01:55,759
behavior aggregated and interpreted by

00:01:55,200 --> 00:01:58,719
human

00:01:55,759 --> 00:02:00,560
or by algorithms designed by humans and

00:01:58,719 --> 00:02:03,119
the outputs thereof are used to make

00:02:00,560 --> 00:02:06,479
decisions that can alter people's lives

00:02:03,119 --> 00:02:07,520
in good ways or bad and i want to make a

00:02:06,479 --> 00:02:10,479
disclaimer

00:02:07,520 --> 00:02:11,120
uh which is ironic since marianne just

00:02:10,479 --> 00:02:12,959
said

00:02:11,120 --> 00:02:15,280
uh you can't get fired for ibm i

00:02:12,959 --> 00:02:17,200
actually work at ibm

00:02:15,280 --> 00:02:18,400
uh and i will be talking about some of

00:02:17,200 --> 00:02:22,080
the work that me

00:02:18,400 --> 00:02:24,000
and the people on my teams have done

00:02:22,080 --> 00:02:26,400
but i am sharing my own personal

00:02:24,000 --> 00:02:27,760
opinions in this talk i am not here in

00:02:26,400 --> 00:02:30,400
an official capacity

00:02:27,760 --> 00:02:32,080
or as a spokesperson for ibm um so just

00:02:30,400 --> 00:02:34,239
making that disclaimer

00:02:32,080 --> 00:02:36,959
i also want to recognize that everyone

00:02:34,239 --> 00:02:41,040
here is probably tired

00:02:36,959 --> 00:02:42,560
like deeply tired not only because it's

00:02:41,040 --> 00:02:45,040
the last day of the conference i'm sure

00:02:42,560 --> 00:02:46,480
the organizers agree but

00:02:45,040 --> 00:02:48,640
you know we are weathering the impacts

00:02:46,480 --> 00:02:50,959
of a global plan den pandemic

00:02:48,640 --> 00:02:52,160
and if you live in the united states

00:02:50,959 --> 00:02:54,239
especially you are

00:02:52,160 --> 00:02:55,840
deeply impacted by the protests sparked

00:02:54,239 --> 00:02:58,879
by the murder of george

00:02:55,840 --> 00:03:00,800
george floyd in minneapolis and the many

00:02:58,879 --> 00:03:02,159
injustices that led us to

00:03:00,800 --> 00:03:04,080
the situation we're in right now

00:03:02,159 --> 00:03:06,239
together i also

00:03:04,080 --> 00:03:07,200
assume that if you're here for this talk

00:03:06,239 --> 00:03:09,200
that

00:03:07,200 --> 00:03:11,120
you are like me and that you're

00:03:09,200 --> 00:03:12,560
passionate about technology and you care

00:03:11,120 --> 00:03:15,120
about building things that make a

00:03:12,560 --> 00:03:17,360
positive impact on society

00:03:15,120 --> 00:03:18,640
so i hope i can attempt to provide you

00:03:17,360 --> 00:03:20,640
with some tools to

00:03:18,640 --> 00:03:22,400
move forward and make a difference in

00:03:20,640 --> 00:03:24,480
this space

00:03:22,400 --> 00:03:26,640
i also just want to have a content

00:03:24,480 --> 00:03:28,640
warning at the beginning as well

00:03:26,640 --> 00:03:29,680
i'm going to be using an example from

00:03:28,640 --> 00:03:31,200
the

00:03:29,680 --> 00:03:33,360
criminal justice system here in the

00:03:31,200 --> 00:03:34,080
united states i won't be describing

00:03:33,360 --> 00:03:36,400
violence

00:03:34,080 --> 00:03:38,080
in any detail but i'll talk about the

00:03:36,400 --> 00:03:40,640
inequalities that exist in the

00:03:38,080 --> 00:03:41,519
prison system including including false

00:03:40,640 --> 00:03:43,360
imprisonment

00:03:41,519 --> 00:03:46,239
and ways that technology today is

00:03:43,360 --> 00:03:48,480
automating racism and injustice

00:03:46,239 --> 00:03:50,159
i'm also going to mention sexual assault

00:03:48,480 --> 00:03:52,560
but i won't also

00:03:50,159 --> 00:03:53,680
will not be describing any detail there

00:03:52,560 --> 00:03:54,239
i just didn't want to catch you off

00:03:53,680 --> 00:03:55,840
guard

00:03:54,239 --> 00:03:58,400
with any of that and if you don't have

00:03:55,840 --> 00:04:01,599
capacity for this conversation right now

00:03:58,400 --> 00:04:03,040
feel free to log off the call i totally

00:04:01,599 --> 00:04:06,799
understand

00:04:03,040 --> 00:04:07,840
that decision so if you take one thing

00:04:06,799 --> 00:04:10,959
away from this talk

00:04:07,840 --> 00:04:12,720
let it be that you

00:04:10,959 --> 00:04:14,720
need to take time to think critically

00:04:12,720 --> 00:04:15,519
about what voices are missing from the

00:04:14,720 --> 00:04:18,959
room

00:04:15,519 --> 00:04:21,280
when you are building things in tech

00:04:18,959 --> 00:04:22,240
we all have bias it's part of what makes

00:04:21,280 --> 00:04:25,040
us human

00:04:22,240 --> 00:04:27,040
but a cognitive bias is insidious

00:04:25,040 --> 00:04:28,960
because this is the kind of bias that

00:04:27,040 --> 00:04:30,639
you don't know about

00:04:28,960 --> 00:04:32,639
because by its very nature it has

00:04:30,639 --> 00:04:35,440
convinced you that it is true

00:04:32,639 --> 00:04:36,639
when rea in reality it is not a

00:04:35,440 --> 00:04:38,880
cognitive bias

00:04:36,639 --> 00:04:40,240
is a deviation from a rationality and

00:04:38,880 --> 00:04:42,080
judgment so basically

00:04:40,240 --> 00:04:44,240
you make your own reality through your

00:04:42,080 --> 00:04:46,400
perception of something

00:04:44,240 --> 00:04:47,520
an important thing to note as i said is

00:04:46,400 --> 00:04:50,479
that you don't know

00:04:47,520 --> 00:04:51,840
about this cognitive bias so you make

00:04:50,479 --> 00:04:54,000
decisions about the world

00:04:51,840 --> 00:04:56,240
around you given the limited resources

00:04:54,000 --> 00:04:58,400
you have the only way you learn

00:04:56,240 --> 00:05:01,120
about your cognitive biases is through

00:04:58,400 --> 00:05:03,520
input and feedback from other people

00:05:01,120 --> 00:05:05,360
cult from different cultures backgrounds

00:05:03,520 --> 00:05:07,919
experiences that kind of thing

00:05:05,360 --> 00:05:10,000
and data and tech more broadly is not

00:05:07,919 --> 00:05:10,960
immune to this very human state a

00:05:10,000 --> 00:05:12,800
cognitive bias

00:05:10,960 --> 00:05:14,960
showing up in the tech that we build is

00:05:12,800 --> 00:05:15,280
the existence of irrational bias making

00:05:14,960 --> 00:05:18,400
it

00:05:15,280 --> 00:05:21,919
out into the world under the guise of

00:05:18,400 --> 00:05:24,240
tech neutrality so off too often in tech

00:05:21,919 --> 00:05:27,120
we build things using biased data

00:05:24,240 --> 00:05:29,280
or blinded by our own cognitive biases

00:05:27,120 --> 00:05:30,320
and thus we unknowingly reproduce and

00:05:29,280 --> 00:05:33,360
perpetuate

00:05:30,320 --> 00:05:35,440
and even sometimes automate as marianne

00:05:33,360 --> 00:05:39,280
highlighted inequality that exists in

00:05:35,440 --> 00:05:40,800
the world i want to share a story today

00:05:39,280 --> 00:05:43,520
about my personal experience with

00:05:40,800 --> 00:05:45,680
learning about my own cognitive biases

00:05:43,520 --> 00:05:47,600
it starts with an experience i had back

00:05:45,680 --> 00:05:49,440
in around 2014

00:05:47,600 --> 00:05:52,080
due to a bunch of community work i was

00:05:49,440 --> 00:05:53,039
doing i was teaching women to code i was

00:05:52,080 --> 00:05:55,280
teaching classes

00:05:53,039 --> 00:05:56,880
and i was invited by a local non-profit

00:05:55,280 --> 00:05:59,840
here in vermont to do a talk

00:05:56,880 --> 00:06:01,440
at my local women's prison for a career

00:05:59,840 --> 00:06:04,160
day

00:06:01,440 --> 00:06:05,520
so prior to that the last time i had set

00:06:04,160 --> 00:06:08,319
foot in a prison

00:06:05,520 --> 00:06:09,919
was to visit my half-brother who was in

00:06:08,319 --> 00:06:12,240
federal prison

00:06:09,919 --> 00:06:15,759
his name is john and i was probably

00:06:12,240 --> 00:06:17,280
around you know 13 or 14 or something

00:06:15,759 --> 00:06:18,960
i didn't leave the visitor area at the

00:06:17,280 --> 00:06:20,720
time but

00:06:18,960 --> 00:06:22,080
thinking back that memory was kind of

00:06:20,720 --> 00:06:23,600
scary so

00:06:22,080 --> 00:06:25,600
walking into the women's prison for the

00:06:23,600 --> 00:06:26,800
first time you know i distinctly

00:06:25,600 --> 00:06:29,520
remember

00:06:26,800 --> 00:06:30,800
you know the blaring fluorescent lights

00:06:29,520 --> 00:06:31,520
you know how it felt to kind of walk

00:06:30,800 --> 00:06:36,160
through this

00:06:31,520 --> 00:06:38,160
like you know uh tiled hallway

00:06:36,160 --> 00:06:39,440
down to the room where i was gonna teach

00:06:38,160 --> 00:06:43,120
um

00:06:39,440 --> 00:06:44,800
i really uh i remember expecting it to

00:06:43,120 --> 00:06:46,319
be like the movies thinking i would hear

00:06:44,800 --> 00:06:48,240
fighting or yelling or something like

00:06:46,319 --> 00:06:52,400
that

00:06:48,240 --> 00:06:55,360
but really the women just looked tired

00:06:52,400 --> 00:06:57,919
but during the class i was still really

00:06:55,360 --> 00:06:59,599
nervous i fumbled my words

00:06:57,919 --> 00:07:01,520
i hadn't even thought to do any research

00:06:59,599 --> 00:07:04,400
into whether you can get

00:07:01,520 --> 00:07:05,599
jobs in tech with a criminal record i

00:07:04,400 --> 00:07:08,560
since found out that you can

00:07:05,599 --> 00:07:10,319
obviously but um i was grateful to the

00:07:08,560 --> 00:07:12,080
host organization there because they

00:07:10,319 --> 00:07:13,919
had that information and shared it with

00:07:12,080 --> 00:07:17,039
the prisoners while i was talking

00:07:13,919 --> 00:07:17,919
but um you know at the end of the night

00:07:17,039 --> 00:07:20,160
i didn't really

00:07:17,919 --> 00:07:21,039
feel like i gave those women any more

00:07:20,160 --> 00:07:22,960
hope than

00:07:21,039 --> 00:07:24,400
what they had walked in the room with

00:07:22,960 --> 00:07:25,120
and which judging by the looks on their

00:07:24,400 --> 00:07:28,639
faces

00:07:25,120 --> 00:07:30,000
wasn't very much hope at all and so my

00:07:28,639 --> 00:07:32,639
bias against people who end

00:07:30,000 --> 00:07:34,960
up in prison was deeply personal but

00:07:32,639 --> 00:07:37,120
also fueled by society and media

00:07:34,960 --> 00:07:38,639
and the ways we perpetuate the

00:07:37,120 --> 00:07:41,680
criminalization of

00:07:38,639 --> 00:07:44,479
blackness poverty homelessness

00:07:41,680 --> 00:07:46,080
mental illness and all sorts of other

00:07:44,479 --> 00:07:47,919
crappy things in society that we

00:07:46,080 --> 00:07:50,080
criminalize

00:07:47,919 --> 00:07:52,160
and here's another example of how making

00:07:50,080 --> 00:07:53,680
decisions based on data has real world

00:07:52,160 --> 00:07:56,240
consequences

00:07:53,680 --> 00:07:56,720
in 2008 the broward county sheriff's

00:07:56,240 --> 00:07:59,039
office

00:07:56,720 --> 00:08:01,360
in fort lauderdale florida that decided

00:07:59,039 --> 00:08:04,160
that instead of opening another prison

00:08:01,360 --> 00:08:04,720
they would purchase software that would

00:08:04,160 --> 00:08:08,479
assign

00:08:04,720 --> 00:08:11,520
recidivism scores to defendants

00:08:08,479 --> 00:08:12,879
in jail awaiting their trial and so this

00:08:11,520 --> 00:08:15,840
software

00:08:12,879 --> 00:08:17,599
costs them 22 thousand dollars a year

00:08:15,840 --> 00:08:20,800
and since then everyone booked

00:08:17,599 --> 00:08:23,840
in broward county was assigned a score

00:08:20,800 --> 00:08:27,759
on a number from 1 to 10 and

00:08:23,840 --> 00:08:30,560
that score determined whether they would

00:08:27,759 --> 00:08:32,399
have a low bail or maybe be released

00:08:30,560 --> 00:08:34,000
from jail pending their trial

00:08:32,399 --> 00:08:36,240
if they were very low risk of

00:08:34,000 --> 00:08:38,959
reoffending if they were rated as

00:08:36,240 --> 00:08:40,000
close to 10 then that meant that they

00:08:38,959 --> 00:08:42,880
were very high risk

00:08:40,000 --> 00:08:43,760
and would either get a very high uh

00:08:42,880 --> 00:08:45,360
dollar

00:08:43,760 --> 00:08:47,279
bail amount or they would be released

00:08:45,360 --> 00:08:48,480
from pr or kept in prison until their

00:08:47,279 --> 00:08:50,480
trial

00:08:48,480 --> 00:08:52,320
so propublica actually obtained the risk

00:08:50,480 --> 00:08:52,720
scores assigned to more than 7 000

00:08:52,320 --> 00:08:55,920
people

00:08:52,720 --> 00:08:57,040
arrested in broward county in 2013 and

00:08:55,920 --> 00:08:58,560
00:08:57,040 --> 00:09:00,640
and they checked to see how many were

00:08:58,560 --> 00:09:01,760
charged with new crimes over the next

00:09:00,640 --> 00:09:03,680
two years

00:09:01,760 --> 00:09:06,560
using the same benchmark that was used

00:09:03,680 --> 00:09:07,839
by the creators of the algorithm

00:09:06,560 --> 00:09:10,080
and so i'm going to go through a few

00:09:07,839 --> 00:09:12,560
examples of people who committed very

00:09:10,080 --> 00:09:14,880
similar crimes but were affected

00:09:12,560 --> 00:09:16,160
by the recidivism scores in very

00:09:14,880 --> 00:09:18,560
different ways

00:09:16,160 --> 00:09:20,320
um so in this pictures in these two

00:09:18,560 --> 00:09:20,959
pictures here we have britisha borden on

00:09:20,320 --> 00:09:22,880
the right

00:09:20,959 --> 00:09:24,959
she was rated as high risk for future

00:09:22,880 --> 00:09:26,959
crime after she and a friend

00:09:24,959 --> 00:09:29,200
took a kid's bike and scooter it was

00:09:26,959 --> 00:09:32,080
valued at about 80 bucks

00:09:29,200 --> 00:09:33,279
she was 18 at the time and she did have

00:09:32,080 --> 00:09:34,959
a record but it was only for

00:09:33,279 --> 00:09:36,480
misdemeanors when she was a minor and

00:09:34,959 --> 00:09:38,480
since she's a minor it's not public

00:09:36,480 --> 00:09:39,839
record what that was but we know it was

00:09:38,480 --> 00:09:41,600
only misdemeanors

00:09:39,839 --> 00:09:43,040
vernon prater on the other hand was

00:09:41,600 --> 00:09:45,680
rated as low risk

00:09:43,040 --> 00:09:46,640
he had also shoplifted about 80 from a

00:09:45,680 --> 00:09:48,399
store

00:09:46,640 --> 00:09:50,160
he was the more seasoned criminal though

00:09:48,399 --> 00:09:50,720
he had already served five years in

00:09:50,160 --> 00:09:53,200
prison

00:09:50,720 --> 00:09:54,720
for armed robbery and attempted armed

00:09:53,200 --> 00:09:57,200
robbery

00:09:54,720 --> 00:09:58,800
um and he had another armed robbery

00:09:57,200 --> 00:10:00,720
charge on top of that

00:09:58,800 --> 00:10:02,320
and while borden did not go in on to

00:10:00,720 --> 00:10:04,560
reoffend prater

00:10:02,320 --> 00:10:06,240
is now serving an eight-year prison term

00:10:04,560 --> 00:10:08,720
for stealing thousands of dollars worth

00:10:06,240 --> 00:10:08,720
of stuff

00:10:09,600 --> 00:10:12,800
dylan fugit here on the left was rated

00:10:11,760 --> 00:10:14,560
as low risk

00:10:12,800 --> 00:10:17,360
after being arrested with cocaine and

00:10:14,560 --> 00:10:20,399
marijuana he was arrested three times on

00:10:17,360 --> 00:10:22,000
drug charges after that

00:10:20,399 --> 00:10:24,160
gregory lugo crashed his lincoln

00:10:22,000 --> 00:10:25,040
navigator into a toyota camry while he

00:10:24,160 --> 00:10:27,120
was drunk

00:10:25,040 --> 00:10:28,560
he was rated as low risk of reoffending

00:10:27,120 --> 00:10:31,519
despite the fact that it was at least

00:10:28,560 --> 00:10:33,920
his fourth dui

00:10:31,519 --> 00:10:35,839
james rivelli stole from a cvs and was

00:10:33,920 --> 00:10:37,839
caught with heroin in his car

00:10:35,839 --> 00:10:39,760
he had already spent five years in state

00:10:37,839 --> 00:10:40,480
prison in massachusetts before this

00:10:39,760 --> 00:10:42,399
crime

00:10:40,480 --> 00:10:43,519
and he was late rated as low risk even

00:10:42,399 --> 00:10:45,680
though he uh

00:10:43,519 --> 00:10:48,880
did go on to shoplift you know a couple

00:10:45,680 --> 00:10:48,880
thousand dollars worth of tools

00:10:49,760 --> 00:10:53,600
so here i'm going to show you two charts

00:10:51,519 --> 00:10:55,519
that show the average scores assigned to

00:10:53,600 --> 00:10:56,480
criminal defendants based on their skin

00:10:55,519 --> 00:10:58,240
color

00:10:56,480 --> 00:11:00,720
here you see the average scores assigned

00:10:58,240 --> 00:11:03,040
to black defendants across the 7 000

00:11:00,720 --> 00:11:05,279
people that propublica reviewed

00:11:03,040 --> 00:11:06,320
you'll notice that a fair amount of them

00:11:05,279 --> 00:11:08,959
were evaluated

00:11:06,320 --> 00:11:10,160
at a high risk score but it's kind of

00:11:08,959 --> 00:11:12,959
you know across the board

00:11:10,160 --> 00:11:14,880
where they land in terms of what their

00:11:12,959 --> 00:11:16,399
risk would be

00:11:14,880 --> 00:11:18,480
but so this is the chart for black

00:11:16,399 --> 00:11:19,839
defendants and here's the chart for

00:11:18,480 --> 00:11:22,399
white defendants

00:11:19,839 --> 00:11:24,240
and you can see between the two when i

00:11:22,399 --> 00:11:25,920
sort of switch back and forth

00:11:24,240 --> 00:11:27,279
the what the scores for white defendants

00:11:25,920 --> 00:11:30,240
were secured very

00:11:27,279 --> 00:11:31,279
heavily towards lower risk categories in

00:11:30,240 --> 00:11:33,600
fact the highest

00:11:31,279 --> 00:11:35,920
uh category that they were assigned at

00:11:33,600 --> 00:11:37,440
was uh the lowest at a one

00:11:35,920 --> 00:11:39,440
and the scores for black defendants did

00:11:37,440 --> 00:11:43,839
not uh

00:11:39,440 --> 00:11:43,839
see that that heavy of a skew

00:11:46,160 --> 00:11:49,760
and in forecasting who would reoffend

00:11:48,000 --> 00:11:51,200
the algorithm made mistakes with black

00:11:49,760 --> 00:11:53,519
and white defendants

00:11:51,200 --> 00:11:55,760
at roughly the same rate but in very

00:11:53,519 --> 00:11:58,000
different ways

00:11:55,760 --> 00:12:00,320
the formula was particularly likely to

00:11:58,000 --> 00:12:01,200
falsely flag back defendants as future

00:12:00,320 --> 00:12:03,600
criminals

00:12:01,200 --> 00:12:05,680
wrongly labeling them this way at twice

00:12:03,600 --> 00:12:08,160
the rate of white defendants

00:12:05,680 --> 00:12:10,959
white defendants were mislabeled as low

00:12:08,160 --> 00:12:13,120
risk more often than black defendants

00:12:10,959 --> 00:12:15,279
and black defendants were 77 percent

00:12:13,120 --> 00:12:19,600
more likely to be pegged as

00:12:15,279 --> 00:12:21,440
committing a violent a violent crime

00:12:19,600 --> 00:12:22,880
and 45 percent more likely to be

00:12:21,440 --> 00:12:25,680
predicted to commit

00:12:22,880 --> 00:12:26,399
a crime of any kind so the company who

00:12:25,680 --> 00:12:29,519
made this

00:12:26,399 --> 00:12:32,160
algorithm uh disputes the results by

00:12:29,519 --> 00:12:34,000
provopica on this report northpoint

00:12:32,160 --> 00:12:35,680
software continues to be among the most

00:12:34,000 --> 00:12:36,800
widely used assessment tools in the

00:12:35,680 --> 00:12:38,959
country

00:12:36,800 --> 00:12:40,720
um but it's interesting to note that

00:12:38,959 --> 00:12:42,720
north point has since changed the name

00:12:40,720 --> 00:12:44,160
of their company to the equivan

00:12:42,720 --> 00:12:45,760
so if you look for north point you won't

00:12:44,160 --> 00:12:49,760
find them anymore you have to

00:12:45,760 --> 00:12:49,760
look for the new company equivant

00:12:50,160 --> 00:12:53,920
so another common example of cognitive

00:12:52,880 --> 00:12:55,920
bias

00:12:53,920 --> 00:12:57,040
tainted data sneaking into modern

00:12:55,920 --> 00:13:00,480
society

00:12:57,040 --> 00:13:02,560
in february 2018 joy bologna a computer

00:13:00,480 --> 00:13:05,600
scientist and digital activist based

00:13:02,560 --> 00:13:06,000
at the mit media lab she ran the gender

00:13:05,600 --> 00:13:08,320
shades

00:13:06,000 --> 00:13:09,360
project so the goal of this project was

00:13:08,320 --> 00:13:11,360
to see how well

00:13:09,360 --> 00:13:13,040
different gender classification systems

00:13:11,360 --> 00:13:15,440
worked across

00:13:13,040 --> 00:13:17,360
different people's spaces and if the

00:13:15,440 --> 00:13:18,800
results changed based on someone's

00:13:17,360 --> 00:13:21,600
gender or skin type

00:13:18,800 --> 00:13:22,800
and they uh her and her team evaluated

00:13:21,600 --> 00:13:27,120
three companies

00:13:22,800 --> 00:13:30,240
ibm microsoft and face plus plus

00:13:27,120 --> 00:13:32,560
and unfortunately for my employer ibm

00:13:30,240 --> 00:13:35,200
had the largest gap in accuracy

00:13:32,560 --> 00:13:37,040
with the difference of 34.4 percent and

00:13:35,200 --> 00:13:40,240
error rates between lighter males

00:13:37,040 --> 00:13:40,240
and darker females

00:13:40,399 --> 00:13:43,519
so bola money said we have entered the

00:13:42,320 --> 00:13:46,079
age of automation

00:13:43,519 --> 00:13:47,920
over confident yet under prepared if we

00:13:46,079 --> 00:13:49,519
fail to make ethical and inclusive

00:13:47,920 --> 00:13:51,519
artificial intelligence

00:13:49,519 --> 00:13:53,680
we risk losing gains made in civil

00:13:51,519 --> 00:13:57,600
rights and gender equity

00:13:53,680 --> 00:13:57,600
under the guise of machine neutrality

00:13:58,399 --> 00:14:03,519
and in january 2019 bolo money released

00:14:01,920 --> 00:14:05,360
an update to the project where she

00:14:03,519 --> 00:14:07,040
re-tested the previous systems and then

00:14:05,360 --> 00:14:09,040
added two more

00:14:07,040 --> 00:14:10,320
amazon recognition and another company

00:14:09,040 --> 00:14:12,240
called k rose

00:14:10,320 --> 00:14:14,880
and there was good news she found that

00:14:12,240 --> 00:14:16,720
ibm face plus plus and microsoft had all

00:14:14,880 --> 00:14:19,279
seen significant improvements in their

00:14:16,720 --> 00:14:21,040
gender classification accuracy

00:14:19,279 --> 00:14:22,959
between darker skinned women and

00:14:21,040 --> 00:14:24,959
lighter-skinned men

00:14:22,959 --> 00:14:26,399
bolimony said the study really shows the

00:14:24,959 --> 00:14:28,560
need for these technologies to be

00:14:26,399 --> 00:14:31,600
externally audited in order to hold them

00:14:28,560 --> 00:14:33,839
technically accountable

00:14:31,600 --> 00:14:35,680
this is a screenshot from joy's talk at

00:14:33,839 --> 00:14:37,199
fact conference and

00:14:35,680 --> 00:14:38,880
where she talks about the follow-up from

00:14:37,199 --> 00:14:40,880
the gender shades project

00:14:38,880 --> 00:14:43,040
she said that ibm gave the quickest and

00:14:40,880 --> 00:14:44,800
most thorough response to the work

00:14:43,040 --> 00:14:46,240
ibm research actually flew her down to

00:14:44,800 --> 00:14:48,720
our headquarters in

00:14:46,240 --> 00:14:51,120
new york and worked closely with her to

00:14:48,720 --> 00:14:53,360
improve our public apis and the messa

00:14:51,120 --> 00:14:55,040
the methods that we use for classifying

00:14:53,360 --> 00:14:57,519
and scoring our tools

00:14:55,040 --> 00:14:59,199
and so we found significant improvements

00:14:57,519 --> 00:15:00,800
through hand-in-hand collaboration with

00:14:59,199 --> 00:15:02,399
her and our team

00:15:00,800 --> 00:15:04,399
and the impact of this research

00:15:02,399 --> 00:15:05,199
continues to have ripple effects on our

00:15:04,399 --> 00:15:08,320
industry

00:15:05,199 --> 00:15:09,519
in fact i think about two months ago now

00:15:08,320 --> 00:15:12,639
the new ceo

00:15:09,519 --> 00:15:13,680
of ibm took a stand in response to black

00:15:12,639 --> 00:15:15,680
lives matter

00:15:13,680 --> 00:15:17,040
and sent a letter to congress calling

00:15:15,680 --> 00:15:19,040
for police reform

00:15:17,040 --> 00:15:20,399
and responsible policies toward the use

00:15:19,040 --> 00:15:22,240
of technology

00:15:20,399 --> 00:15:24,160
this included a public stand against the

00:15:22,240 --> 00:15:25,519
sale and use of facial recognition

00:15:24,160 --> 00:15:27,680
technology

00:15:25,519 --> 00:15:29,199
um it can't be overlooked that joy and

00:15:27,680 --> 00:15:32,160
our team in my opinion

00:15:29,199 --> 00:15:33,279
um influences this decision for ibm to

00:15:32,160 --> 00:15:37,120
stop the sale of

00:15:33,279 --> 00:15:38,959
facial recognition technology and um

00:15:37,120 --> 00:15:40,320
you know it's just incredibly impactful

00:15:38,959 --> 00:15:42,560
what she did

00:15:40,320 --> 00:15:43,440
and while i'm proud that ibm took this

00:15:42,560 --> 00:15:46,800
step

00:15:43,440 --> 00:15:49,120
more needs to be done this picture is

00:15:46,800 --> 00:15:50,399
a man named robert julian borchak

00:15:49,120 --> 00:15:53,040
williams

00:15:50,399 --> 00:15:54,560
who is the first proven case of someone

00:15:53,040 --> 00:15:56,480
being falsely arrested

00:15:54,560 --> 00:15:58,000
due to the faulty results of facial

00:15:56,480 --> 00:16:00,880
recognition technology

00:15:58,000 --> 00:16:02,320
by law enforcement he was arrested in

00:16:00,880 --> 00:16:03,040
front of his wife and children outside

00:16:02,320 --> 00:16:05,440
of his house

00:16:03,040 --> 00:16:07,600
he was interrogated held in a cell a

00:16:05,440 --> 00:16:09,759
cell for 30 hours

00:16:07,600 --> 00:16:10,720
and then released on bail pending his

00:16:09,759 --> 00:16:14,000
trial

00:16:10,720 --> 00:16:14,800
his case was subsequently dropped due to

00:16:14,000 --> 00:16:17,120
unsufficient

00:16:14,800 --> 00:16:18,959
evidence and the aclu in michigan is

00:16:17,120 --> 00:16:21,600
suing

00:16:18,959 --> 00:16:24,320
the police force there to ban them from

00:16:21,600 --> 00:16:26,800
using facial recognition technology

00:16:24,320 --> 00:16:27,759
but if companies like ibm won't sell it

00:16:26,800 --> 00:16:29,920
to cops then

00:16:27,759 --> 00:16:31,839
someone else will and in this case it

00:16:29,920 --> 00:16:33,839
was a south carolina based company

00:16:31,839 --> 00:16:36,160
called dataworks plus

00:16:33,839 --> 00:16:36,880
dataworks plus has supplied technology

00:16:36,160 --> 00:16:38,800
like this

00:16:36,880 --> 00:16:39,920
to government agencies in santa barbara

00:16:38,800 --> 00:16:44,079
california

00:16:39,920 --> 00:16:44,079
chicago and philadelphia

00:16:45,600 --> 00:16:49,759
so at ibm we have a team of open source

00:16:47,839 --> 00:16:51,199
developers committed to building tools

00:16:49,759 --> 00:16:54,320
to support

00:16:51,199 --> 00:16:56,880
open source ai on the enterprise or in

00:16:54,320 --> 00:16:58,959
the enterprise not on the enterprise

00:16:56,880 --> 00:17:00,560
one of our many toolkits is called ai

00:16:58,959 --> 00:17:02,480
fairness 360

00:17:00,560 --> 00:17:04,400
and as a developer advocate working for

00:17:02,480 --> 00:17:06,799
that team i supported the maintenance of

00:17:04,400 --> 00:17:08,720
this project and advocated for it out in

00:17:06,799 --> 00:17:10,799
the community

00:17:08,720 --> 00:17:12,959
and so the air fairness 360 toolkit is

00:17:10,799 --> 00:17:14,799
an open source library of metrics that

00:17:12,959 --> 00:17:17,439
help you sort of examine

00:17:14,799 --> 00:17:18,720
report and mitigate unwanted bias in

00:17:17,439 --> 00:17:21,120
your machine learning

00:17:18,720 --> 00:17:22,959
models throughout the ai life cycle so

00:17:21,120 --> 00:17:23,919
it has metrics for data sets and machine

00:17:22,959 --> 00:17:25,679
learning models

00:17:23,919 --> 00:17:29,200
explanations for them and then

00:17:25,679 --> 00:17:30,880
algorithms to actually mitigate bias

00:17:29,200 --> 00:17:32,480
and there's a bunch of guidance material

00:17:30,880 --> 00:17:34,880
that my team made

00:17:32,480 --> 00:17:36,720
for this toolkit to help you get up to

00:17:34,880 --> 00:17:38,880
speed on what you need to use it

00:17:36,720 --> 00:17:39,840
including some demos and tutorials that

00:17:38,880 --> 00:17:43,440
were built by

00:17:39,840 --> 00:17:46,240
us and in fact one of the demos

00:17:43,440 --> 00:17:47,120
on the site utilize the data set that

00:17:46,240 --> 00:17:49,200
was provided

00:17:47,120 --> 00:17:51,200
uh obtained by propublica to do their

00:17:49,200 --> 00:17:52,240
analysis of north point's uh compass

00:17:51,200 --> 00:17:53,760
algorithm

00:17:52,240 --> 00:17:56,160
so you can actually play with that data

00:17:53,760 --> 00:17:58,320
set on the website um

00:17:56,160 --> 00:17:59,280
to find ways to recognize where the bias

00:17:58,320 --> 00:18:00,880
exists

00:17:59,280 --> 00:18:02,559
and then actually mitigate it using

00:18:00,880 --> 00:18:03,520
different algorithms from ai fairness

00:18:02,559 --> 00:18:05,039
00:18:03,520 --> 00:18:07,200
which feels amazing when you know the

00:18:05,039 --> 00:18:08,640
reality of what it's done to people

00:18:07,200 --> 00:18:11,120
but when you think about the fact that

00:18:08,640 --> 00:18:11,600
people are still sitting unjustly in

00:18:11,120 --> 00:18:14,799
jail

00:18:11,600 --> 00:18:16,960
because of compass even though there are

00:18:14,799 --> 00:18:20,160
tools available to fix the problem

00:18:16,960 --> 00:18:20,160
it is somewhat maddening

00:18:21,440 --> 00:18:25,600
so after doing this work i had a pit in

00:18:23,760 --> 00:18:27,919
my stomach

00:18:25,600 --> 00:18:28,640
i had to go back and see if i could do a

00:18:27,919 --> 00:18:32,000
better job

00:18:28,640 --> 00:18:33,840
helping the women at crcf so i reached

00:18:32,000 --> 00:18:35,520
back out to the non-profit and asked

00:18:33,840 --> 00:18:37,760
them if they would have me back

00:18:35,520 --> 00:18:39,440
but this time i would build a hands-on

00:18:37,760 --> 00:18:41,679
workshop for the women

00:18:39,440 --> 00:18:42,880
so that instead of just talking to them

00:18:41,679 --> 00:18:44,240
we could actually build something

00:18:42,880 --> 00:18:46,080
together

00:18:44,240 --> 00:18:48,559
so i scheduled this workshop for

00:18:46,080 --> 00:18:50,880
december of last year

00:18:48,559 --> 00:18:52,720
and the week before i was scheduled to

00:18:50,880 --> 00:18:54,720
visit the prison and give my workshop

00:18:52,720 --> 00:18:56,080
an investigative story broke in the news

00:18:54,720 --> 00:18:58,400
about widespread sexual

00:18:56,080 --> 00:19:00,640
assault allegations at the women's

00:18:58,400 --> 00:19:02,880
prison being ledged against multiple

00:19:00,640 --> 00:19:05,200
prison guards

00:19:02,880 --> 00:19:06,160
and i attended an emergency community

00:19:05,200 --> 00:19:08,080
forum where

00:19:06,160 --> 00:19:10,080
many officials in charge of the welfare

00:19:08,080 --> 00:19:12,000
of the prisoners in the state of vermont

00:19:10,080 --> 00:19:13,200
attended and held an open panel

00:19:12,000 --> 00:19:15,200
discussion

00:19:13,200 --> 00:19:16,960
to discuss what happened and what's

00:19:15,200 --> 00:19:18,799
being done to stop it

00:19:16,960 --> 00:19:20,320
in our breakout sessions we all sort of

00:19:18,799 --> 00:19:21,600
broke into groups and talked about what

00:19:20,320 --> 00:19:24,000
we wanted to see happen

00:19:21,600 --> 00:19:25,840
in our community and i found out that

00:19:24,000 --> 00:19:27,440
the woman sitting right next to me was

00:19:25,840 --> 00:19:28,000
actually one of the women named in the

00:19:27,440 --> 00:19:31,440
article

00:19:28,000 --> 00:19:31,840
as one of the survivors so needless to

00:19:31,440 --> 00:19:34,559
say

00:19:31,840 --> 00:19:35,760
i didn't sleep well that night thinking

00:19:34,559 --> 00:19:37,600
about

00:19:35,760 --> 00:19:39,520
you know going to the prison and looking

00:19:37,600 --> 00:19:40,559
at these women looking them in the face

00:19:39,520 --> 00:19:42,880
and

00:19:40,559 --> 00:19:43,840
you know giving them some kind of hope

00:19:42,880 --> 00:19:48,160
it just seemed

00:19:43,840 --> 00:19:50,080
indomitable so a week later i went

00:19:48,160 --> 00:19:52,400
to the prison and did the workshop the

00:19:50,080 --> 00:19:53,440
women and i decided to do a design

00:19:52,400 --> 00:19:56,640
thinking workshop

00:19:53,440 --> 00:19:59,039
together where we brainstormed

00:19:56,640 --> 00:20:01,120
we designed an application that would

00:19:59,039 --> 00:20:03,520
make prison life easier for them

00:20:01,120 --> 00:20:05,200
and help them with their rehabilitation

00:20:03,520 --> 00:20:06,080
and get them ready for when they got out

00:20:05,200 --> 00:20:07,679
of jail

00:20:06,080 --> 00:20:09,360
and then we sort of mocked up a

00:20:07,679 --> 00:20:12,240
prototype at the end

00:20:09,360 --> 00:20:14,240
so i held multiple workshops between

00:20:12,240 --> 00:20:17,280
different units of the prison

00:20:14,240 --> 00:20:19,200
and some common themes were a need for

00:20:17,280 --> 00:20:21,200
better communication avenues with their

00:20:19,200 --> 00:20:22,720
children and their families

00:20:21,200 --> 00:20:25,440
access to their lawyers and their

00:20:22,720 --> 00:20:26,960
doctors help with finding housing

00:20:25,440 --> 00:20:28,720
and employment when they get out of

00:20:26,960 --> 00:20:31,200
prison and

00:20:28,720 --> 00:20:32,880
unfortunately a lot of them talked about

00:20:31,200 --> 00:20:33,360
how to be better prepared for the next

00:20:32,880 --> 00:20:35,840
time

00:20:33,360 --> 00:20:37,919
they get in they land in prison which

00:20:35,840 --> 00:20:39,679
was eye-opening for me because

00:20:37,919 --> 00:20:41,360
they talked about their different stints

00:20:39,679 --> 00:20:42,080
and they knew that they were caught in

00:20:41,360 --> 00:20:43,840
this

00:20:42,080 --> 00:20:46,480
never-ending cycle and that when they

00:20:43,840 --> 00:20:48,880
get out they'd probably end up back in

00:20:46,480 --> 00:20:50,080
um so it was kind of heartbreaking to

00:20:48,880 --> 00:20:52,320
realize that they were just sort of

00:20:50,080 --> 00:20:53,760
resigned to that fact

00:20:52,320 --> 00:20:56,400
this time and talking with them and

00:20:53,760 --> 00:20:59,520
listening instead of talking at them

00:20:56,400 --> 00:21:02,240
i knew that they left the room with

00:20:59,520 --> 00:21:03,760
some measure of hope more than what

00:21:02,240 --> 00:21:05,600
they'd walked in with

00:21:03,760 --> 00:21:07,120
and hopefully a little more confidence

00:21:05,600 --> 00:21:08,799
that their ideas matter and their

00:21:07,120 --> 00:21:11,760
perspectives are valuable

00:21:08,799 --> 00:21:14,320
um and so it was a you know much better

00:21:11,760 --> 00:21:16,640
experience this time

00:21:14,320 --> 00:21:18,799
so back to you and what i think you can

00:21:16,640 --> 00:21:21,440
take away from this experience

00:21:18,799 --> 00:21:21,840
um i was a qa engineer for a few years

00:21:21,440 --> 00:21:24,880
and

00:21:21,840 --> 00:21:26,159
any qa will tell you the first thing we

00:21:24,880 --> 00:21:28,960
need to learn to do

00:21:26,159 --> 00:21:31,200
is fight effectively for edge cases you

00:21:28,960 --> 00:21:33,840
know you need to deliver that mvp

00:21:31,200 --> 00:21:35,280
that minimum viable product uh you need

00:21:33,840 --> 00:21:36,320
to get that to the client as quickly as

00:21:35,280 --> 00:21:38,640
possible

00:21:36,320 --> 00:21:40,240
and inevitably your your qa is always

00:21:38,640 --> 00:21:42,559
the one that finds an issue

00:21:40,240 --> 00:21:45,039
with the way is a page is rendering in a

00:21:42,559 --> 00:21:46,799
certain browser or says that a last

00:21:45,039 --> 00:21:48,400
minute color decision will render

00:21:46,799 --> 00:21:50,320
some of the features inaccessible to

00:21:48,400 --> 00:21:52,480
people who are color blind

00:21:50,320 --> 00:21:54,240
or your markup isn't optimized for

00:21:52,480 --> 00:21:56,240
screen readers

00:21:54,240 --> 00:21:57,840
as a qa you need to make the argument

00:21:56,240 --> 00:22:00,400
for this

00:21:57,840 --> 00:22:02,080
so that it becomes part of the mvp

00:22:00,400 --> 00:22:02,720
instead of a feature request that gets

00:22:02,080 --> 00:22:07,200
buried

00:22:02,720 --> 00:22:09,840
in your backlog for all eternity and

00:22:07,200 --> 00:22:11,039
you know edge cases aren't just pixels

00:22:09,840 --> 00:22:12,880
or wayward data

00:22:11,039 --> 00:22:14,159
edge cases are the ways in which the

00:22:12,880 --> 00:22:16,640
technology that you build

00:22:14,159 --> 00:22:18,559
impacts the lives of people who will be

00:22:16,640 --> 00:22:21,120
using that technology for

00:22:18,559 --> 00:22:22,720
the foreseeable future and if so if you

00:22:21,120 --> 00:22:27,280
don't consider these people

00:22:22,720 --> 00:22:29,520
you will perpetuate harm in the process

00:22:27,280 --> 00:22:31,039
so if you don't follow kim creighton yet

00:22:29,520 --> 00:22:33,360
she is a powerful advocate for

00:22:31,039 --> 00:22:35,200
underrepresented minorities in tech

00:22:33,360 --> 00:22:36,640
and the guiding principles of her

00:22:35,200 --> 00:22:39,120
kazakhstan community

00:22:36,640 --> 00:22:41,360
inform this well specifically in this

00:22:39,120 --> 00:22:43,039
case the importance of prioritizing the

00:22:41,360 --> 00:22:45,760
most vulnerable

00:22:43,039 --> 00:22:46,559
the makers of compass i believe had a

00:22:45,760 --> 00:22:48,240
good motive

00:22:46,559 --> 00:22:51,280
they wanted to make it possible for

00:22:48,240 --> 00:22:52,640
people to build less prisons

00:22:51,280 --> 00:22:54,640
but they didn't account for the fact

00:22:52,640 --> 00:22:57,039
that their data and algorithms would so

00:22:54,640 --> 00:22:59,760
easily absorb the racism that poisons

00:22:57,039 --> 00:22:59,760
our society

00:23:00,480 --> 00:23:04,960
so i am actively working to incorporate

00:23:02,960 --> 00:23:07,280
kim's guiding principles into my daily

00:23:04,960 --> 00:23:09,360
work and i suggest you do the same

00:23:07,280 --> 00:23:11,520
i recently accepted a role as program

00:23:09,360 --> 00:23:13,440
manager for ibm quantum computing's

00:23:11,520 --> 00:23:15,679
workforce development team

00:23:13,440 --> 00:23:16,480
so quantum computing is really in its

00:23:15,679 --> 00:23:18,320
infancy

00:23:16,480 --> 00:23:20,720
this is from a recent podcast we did

00:23:18,320 --> 00:23:22,159
about how to get a job in quantum

00:23:20,720 --> 00:23:24,480
we're just really starting to see the

00:23:22,159 --> 00:23:27,200
ways this technology could be

00:23:24,480 --> 00:23:29,200
possibly used in production and so my

00:23:27,200 --> 00:23:31,280
job is to build a workforce of people

00:23:29,200 --> 00:23:33,280
with quantum ready skills so when the

00:23:31,280 --> 00:23:34,799
that when the tech is up to snuff for

00:23:33,280 --> 00:23:35,280
our industry to pick it up and run with

00:23:34,799 --> 00:23:36,720
it

00:23:35,280 --> 00:23:38,720
there's already a workforce of people

00:23:36,720 --> 00:23:39,919
with the skills necessary to take with

00:23:38,720 --> 00:23:41,919
it and run with it

00:23:39,919 --> 00:23:44,000
so this means that i actually have the

00:23:41,919 --> 00:23:46,320
opportunity to rethink

00:23:44,000 --> 00:23:47,279
how this industry looks and acts with my

00:23:46,320 --> 00:23:49,360
team

00:23:47,279 --> 00:23:51,760
so we can make this industry what it

00:23:49,360 --> 00:23:53,200
should be and by prioritizing the most

00:23:51,760 --> 00:23:55,039
vulnerable from the outset

00:23:53,200 --> 00:23:56,320
and ensuring we have a clear strategy

00:23:55,039 --> 00:23:58,559
around that work

00:23:56,320 --> 00:24:00,480
we can empower underrepresented voices

00:23:58,559 --> 00:24:02,880
and make a truly equitable

00:24:00,480 --> 00:24:05,440
industry and so obviously these are

00:24:02,880 --> 00:24:08,240
lofty goals but we must strive to

00:24:05,440 --> 00:24:08,240
to achieve them

00:24:08,480 --> 00:24:11,520
so in conclusion we need to contend with

00:24:10,960 --> 00:24:14,320
the fact

00:24:11,520 --> 00:24:15,279
that data is generated based on human

00:24:14,320 --> 00:24:18,000
behavior

00:24:15,279 --> 00:24:19,840
and tech is built by humans so no matter

00:24:18,000 --> 00:24:21,840
what as technology practitioners

00:24:19,840 --> 00:24:22,880
it's our responsibility to build systems

00:24:21,840 --> 00:24:25,360
that are fair

00:24:22,880 --> 00:24:27,760
so whether this means making better

00:24:25,360 --> 00:24:29,919
decisions when building these tools

00:24:27,760 --> 00:24:32,240
pushing for legislation that protects

00:24:29,919 --> 00:24:34,159
our privacy and freedom or

00:24:32,240 --> 00:24:36,480
working to build a more inclusive work

00:24:34,159 --> 00:24:38,400
environment we all have a role in this

00:24:36,480 --> 00:24:40,720
fight for justice

00:24:38,400 --> 00:24:40,720
thank you

00:24:41,840 --> 00:24:48,159
thank you maureen that was a great talk

00:24:45,039 --> 00:24:51,440
and um i know for me i worked

00:24:48,159 --> 00:24:53,039
at a homeless food kitchen and services

00:24:51,440 --> 00:24:54,960
organization for about

00:24:53,039 --> 00:24:56,080
seven years in my previous organization

00:24:54,960 --> 00:24:59,279
in one or

00:24:56,080 --> 00:25:00,240
previous city and one of the people that

00:24:59,279 --> 00:25:02,960
helped do

00:25:00,240 --> 00:25:04,080
a lot of the like secondary executive

00:25:02,960 --> 00:25:06,559
director work

00:25:04,080 --> 00:25:08,880
um i had learned at one point he was on

00:25:06,559 --> 00:25:10,159
like an fbi watch list of somebody they

00:25:08,880 --> 00:25:13,520
were trying to hunt

00:25:10,159 --> 00:25:16,080
and he just sort of snuck into um

00:25:13,520 --> 00:25:17,760
our shelter at one point and then found

00:25:16,080 --> 00:25:21,360
out his court case got tossed

00:25:17,760 --> 00:25:24,000
out through a fluke in the system

00:25:21,360 --> 00:25:25,679
but the whole time i had known him for

00:25:24,000 --> 00:25:26,480
years and years and years he was just

00:25:25,679 --> 00:25:29,200
this

00:25:26,480 --> 00:25:29,679
pure-hearted person that just helps so

00:25:29,200 --> 00:25:31,200
many

00:25:29,679 --> 00:25:33,520
hundreds and hundreds and hundreds of

00:25:31,200 --> 00:25:35,840
people have food and services and stuff

00:25:33,520 --> 00:25:38,880
it's it's really weird to see

00:25:35,840 --> 00:25:40,640
sometimes how you know data can be

00:25:38,880 --> 00:25:42,720
important but you know how you look at

00:25:40,640 --> 00:25:46,320
it also it can just be a total twist

00:25:42,720 --> 00:25:48,400
on you know how it affects people's

00:25:46,320 --> 00:25:51,360
lives directly

00:25:48,400 --> 00:25:52,240
um i'm not actually seeing any questions

00:25:51,360 --> 00:25:54,000
and i

00:25:52,240 --> 00:25:55,520
don't know that i have any either your

00:25:54,000 --> 00:25:58,559
talk was really great and

00:25:55,520 --> 00:26:01,360
i think just really hit close to home

00:25:58,559 --> 00:26:01,360
perhaps so

00:26:02,000 --> 00:26:11,840
thank you for delivering this

00:26:31,520 --> 00:26:33,600

YouTube URL: https://www.youtube.com/watch?v=wYUqaRXwl6Q


