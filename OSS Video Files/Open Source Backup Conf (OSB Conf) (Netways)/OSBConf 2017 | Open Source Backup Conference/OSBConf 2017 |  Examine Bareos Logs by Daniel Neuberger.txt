Title: OSBConf 2017 |  Examine Bareos Logs by Daniel Neuberger
Publication date: 2017-10-09
Playlist: OSBConf 2017 | Open Source Backup Conference
Description: 
	In this presentation we will talk about examining bareos logs within the log-management tool elastic. You will hear something about how you can gather the logs in the right way.
After collecting the logs it is also handy and awesome to inspect the collected data with kibana.
After that i will also show you some enhanced filter methods to structure received debug logs of all three daemons so that it makes more sense. That makes it possible to visualize the workflow of bareos debug output in the same order as it happens.
Captions: 
	00:00:14,769 --> 00:00:24,660
all right um we are up to the last talk

00:00:18,670 --> 00:00:29,080
for today and it will be held by her

00:00:24,660 --> 00:00:33,760
longtime Paris consultant and also X

00:00:29,080 --> 00:00:36,360
Tekken lock consultant his name is

00:00:33,760 --> 00:00:39,580
Daniel Berger he's my colleague and

00:00:36,360 --> 00:00:43,860
given a warm welcome and enjoyed the

00:00:39,580 --> 00:00:43,860
last talk for today for today thank you

00:00:45,600 --> 00:00:52,989
so yeah welcome to my talk today about

00:00:49,089 --> 00:00:55,180
eczema and Barrios locks at first a few

00:00:52,989 --> 00:00:56,260
words about myself and company I works

00:00:55,180 --> 00:01:00,100
now

00:00:56,260 --> 00:01:04,799
I'm working as a senior Linux consultant

00:01:00,100 --> 00:01:08,590
and trainer at net Weiss since 2017

00:01:04,799 --> 00:01:11,049
specialized and yeah elastic stack

00:01:08,590 --> 00:01:14,409
ryedawg antebellum various consulting at

00:01:11,049 --> 00:01:17,159
least I'm also a former barrios team

00:01:14,409 --> 00:01:21,399
member and yeah I'm a private time I'm

00:01:17,159 --> 00:01:23,859
working as a beekeeper about net way is

00:01:21,399 --> 00:01:27,700
a few words we are not only is

00:01:23,859 --> 00:01:31,060
sponsoring conferences over course

00:01:27,700 --> 00:01:34,960
Germany we also providing a source

00:01:31,060 --> 00:01:38,079
consulting so we are located in noon

00:01:34,960 --> 00:01:41,740
back and now so the numbers not on a

00:01:38,079 --> 00:01:45,399
person correct em as I actually have a

00:01:41,740 --> 00:01:49,920
mind we are now up to sixteen employees

00:01:45,399 --> 00:01:53,159
now so we are specialized in open source

00:01:49,920 --> 00:01:56,409
system management and open source

00:01:53,159 --> 00:02:00,789
datacenter consulting so we are also

00:01:56,409 --> 00:02:03,880
preparing individual solutions our

00:02:00,789 --> 00:02:07,539
portfolio is covering a lot of open

00:02:03,880 --> 00:02:09,940
source tools but there are some we have

00:02:07,539 --> 00:02:13,330
special or expertise or a mouse so

00:02:09,940 --> 00:02:16,650
that's a singer mainly singer so we are

00:02:13,330 --> 00:02:19,319
also sponsoring and

00:02:16,650 --> 00:02:22,920
covering the singer and using a to Pro

00:02:19,319 --> 00:02:25,140
check we are providing public forum and

00:02:22,920 --> 00:02:29,519
consulting and we are also various

00:02:25,140 --> 00:02:32,430
partner elastic OpenNebula and some

00:02:29,519 --> 00:02:34,769
other things too so for example a chest

00:02:32,430 --> 00:02:38,450
pass off yeah I know it's not an

00:02:34,769 --> 00:02:42,299
efficient source but we needed for

00:02:38,450 --> 00:02:43,950
reporting also because most of the

00:02:42,299 --> 00:02:48,599
monitoring world there are people who

00:02:43,950 --> 00:02:51,930
needs reporting and yeah at least I can

00:02:48,599 --> 00:02:55,859
see fancy papers for representing

00:02:51,930 --> 00:02:59,370
something and our technology fields KVM

00:02:55,859 --> 00:03:03,239
so Seth post cross the epithelia word

00:02:59,370 --> 00:03:08,670
and mask I'll get as well and especially

00:03:03,239 --> 00:03:12,140
docker for example in our you know

00:03:08,670 --> 00:03:14,819
service providing housing so we have a

00:03:12,140 --> 00:03:18,739
new platform called mes where we

00:03:14,819 --> 00:03:23,299
providing micro services to customers so

00:03:18,739 --> 00:03:29,570
our customers are spraying all over the

00:03:23,299 --> 00:03:31,220
country in many different kinds of

00:03:29,570 --> 00:03:36,090
[Music]

00:03:31,220 --> 00:03:39,359
industries like Audi in GD bar so

00:03:36,090 --> 00:03:42,569
Deutsche Welle Deutsche Post and the

00:03:39,359 --> 00:03:45,780
Deutsche Bahn so most of the time we am

00:03:42,569 --> 00:03:48,180
doing using our - consulting and the

00:03:45,780 --> 00:03:53,720
last year is also the consulting for

00:03:48,180 --> 00:03:53,720
elastic stack has grown and intensively

00:03:53,810 --> 00:04:01,049
also be providing hosting in to data

00:03:57,269 --> 00:04:06,180
centers for customers like Akutan trade

00:04:01,049 --> 00:04:09,150
bite and for example Perry and we also

00:04:06,180 --> 00:04:12,780
providing events like the open source

00:04:09,150 --> 00:04:17,729
backup conference and not least but the

00:04:12,780 --> 00:04:20,639
next big conferences our own conference

00:04:17,729 --> 00:04:24,690
open source monitoring is it called so

00:04:20,639 --> 00:04:27,659
it's all about monitoring your IT world

00:04:24,690 --> 00:04:30,030
IT staff it's covering topics about the

00:04:27,659 --> 00:04:34,770
singer - I guess

00:04:30,030 --> 00:04:37,950
maybe now yes as well all topics which

00:04:34,770 --> 00:04:41,670
are related to monitoring

00:04:37,950 --> 00:04:44,280
we also yeah contribute to the community

00:04:41,670 --> 00:04:48,180
with add-ons and plugins and our own

00:04:44,280 --> 00:04:50,580
community side we providing the single

00:04:48,180 --> 00:04:54,840
community the exchanging single platform

00:04:50,580 --> 00:04:58,620
two-spirit plugins and add-ons for using

00:04:54,840 --> 00:05:01,530
and Nagios to the world and we also

00:04:58,620 --> 00:05:05,340
provide our development projects on

00:05:01,530 --> 00:05:09,330
github because what we do is 100% open

00:05:05,340 --> 00:05:11,900
source yeah and at least we covering

00:05:09,330 --> 00:05:15,420
also trainings mostly single peppered

00:05:11,900 --> 00:05:20,670
elastic stack all of our crafting so

00:05:15,420 --> 00:05:23,910
performance data in get Jenkins and data

00:05:20,670 --> 00:05:26,310
center topics like doc and Saif and at

00:05:23,910 --> 00:05:30,450
least it's missing on this and we also

00:05:26,310 --> 00:05:32,610
providing various training and and yeah

00:05:30,450 --> 00:05:36,330
discharge services we are covering so

00:05:32,610 --> 00:05:38,600
from concept and organization over

00:05:36,330 --> 00:05:41,010
workshops consulting development

00:05:38,600 --> 00:05:48,690
trainings and support and events like

00:05:41,010 --> 00:05:52,720
this so but now to my main topic

00:05:48,690 --> 00:05:54,050
it's about locks and lab management

00:05:52,720 --> 00:05:57,600
[Music]

00:05:54,050 --> 00:06:01,430
especially if it comes to barriers so

00:05:57,600 --> 00:06:08,220
what is lock in computing a log file is

00:06:01,430 --> 00:06:10,440
file where events record which occurring

00:06:08,220 --> 00:06:17,370
in our operating system which occurring

00:06:10,440 --> 00:06:20,520
in applications and can also be messages

00:06:17,370 --> 00:06:27,780
between different users so for example

00:06:20,520 --> 00:06:32,280
IRC and so that means there are a lot of

00:06:27,780 --> 00:06:34,470
different kinds of locks so transaction

00:06:32,280 --> 00:06:39,090
locks for example from databases in

00:06:34,470 --> 00:06:42,430
event locks on servers from the

00:06:39,090 --> 00:06:45,190
operating system application looks like

00:06:42,430 --> 00:06:48,850
locks from and chabe us or for example a

00:06:45,190 --> 00:06:53,710
burials we have messages on our IRC

00:06:48,850 --> 00:06:57,430
servers that are locks as well and at

00:06:53,710 --> 00:07:02,850
least so we have also debug locks which

00:06:57,430 --> 00:07:05,650
may something like a subclass of

00:07:02,850 --> 00:07:14,370
application locks but sometimes they can

00:07:05,650 --> 00:07:14,370
be also single kind or type of lock then

00:07:15,120 --> 00:07:22,210
it all needs to come together so we have

00:07:18,520 --> 00:07:23,460
locks on each of our server each our

00:07:22,210 --> 00:07:28,870
applications

00:07:23,460 --> 00:07:30,970
writing their own locks so that means we

00:07:28,870 --> 00:07:38,290
have locks read all of our systems in

00:07:30,970 --> 00:07:40,480
our environment and the goal is to to

00:07:38,290 --> 00:07:45,430
collect them to serve them on a single

00:07:40,480 --> 00:07:47,470
point in your environment so on logging

00:07:45,430 --> 00:07:49,990
a central locking server for example and

00:07:47,470 --> 00:07:54,390
what do we need for that to reach that

00:07:49,990 --> 00:07:57,520
goal so we need a sender a receiver and

00:07:54,390 --> 00:08:01,960
the data format to store it for example

00:07:57,520 --> 00:08:07,510
in how you can achieve that with system

00:08:01,960 --> 00:08:10,060
own tools as our syslog tool which can

00:08:07,510 --> 00:08:13,930
run a demon you can configure a syslog

00:08:10,060 --> 00:08:17,860
to send the local locks to another a

00:08:13,930 --> 00:08:22,450
syslog server which provides a socket

00:08:17,860 --> 00:08:26,620
where he receives the DES messages then

00:08:22,450 --> 00:08:30,660
it stores it in a fire so there we have

00:08:26,620 --> 00:08:35,970
our data format where where is it stored

00:08:30,660 --> 00:08:35,970
so yeah at the end

00:08:39,169 --> 00:08:46,880
happens with the data for example when

00:08:42,560 --> 00:08:49,160
we use air syslog we have a big file

00:08:46,880 --> 00:08:52,040
where all our other logs from our

00:08:49,160 --> 00:08:56,300
servers are application logs are written

00:08:52,040 --> 00:09:02,570
in that's it so what's the first method

00:08:56,300 --> 00:09:07,389
which comes to your mind to to to to

00:09:02,570 --> 00:09:07,389
search in this file for informations

00:09:07,660 --> 00:09:19,550
crap for example what else so crap only

00:09:14,050 --> 00:09:22,399
hmm so there are a lot of tools which I

00:09:19,550 --> 00:09:25,940
can use to search for information in

00:09:22,399 --> 00:09:29,389
that loves but most of the time one tool

00:09:25,940 --> 00:09:32,660
is not enough so you need to group them

00:09:29,389 --> 00:09:35,930
together with pipes and maybe you write

00:09:32,660 --> 00:09:38,889
your own script and so on and so on but

00:09:35,930 --> 00:09:44,930
all the time you're doing this manually

00:09:38,889 --> 00:09:47,230
yeah and it's you can do mistakes it

00:09:44,930 --> 00:09:50,600
takes a long time

00:09:47,230 --> 00:09:54,649
it brings not a lot of effort and at

00:09:50,600 --> 00:09:58,250
least you have a bunch of data which are

00:09:54,649 --> 00:10:00,589
there you can see them you can search

00:09:58,250 --> 00:10:06,140
for informations but you have two plain

00:10:00,589 --> 00:10:08,510
lines which you cannot improve so I

00:10:06,140 --> 00:10:11,060
called that you cannot enrich them so

00:10:08,510 --> 00:10:15,920
you have two plain lines we also can't

00:10:11,060 --> 00:10:18,079
call them documents and there are so if

00:10:15,920 --> 00:10:23,209
we already discussed they are searchable

00:10:18,079 --> 00:10:28,180
with system tools but that's it so they

00:10:23,209 --> 00:10:34,490
are not analyzed there are not enhanced

00:10:28,180 --> 00:10:39,290
not enriched so they have no yeah you

00:10:34,490 --> 00:10:43,699
are not really usable than to gain you

00:10:39,290 --> 00:10:47,990
know forty ways some informations so you

00:10:43,699 --> 00:10:50,520
can do not create some statistics easily

00:10:47,990 --> 00:10:55,200
you can add visualize them and

00:10:50,520 --> 00:10:58,020
yeah therefore you can use the for

00:10:55,200 --> 00:11:03,890
example the elastic stack so that means

00:10:58,020 --> 00:11:08,040
you use the stack like elastic - yeah to

00:11:03,890 --> 00:11:10,860
retrieve the enrichment of your data so

00:11:08,040 --> 00:11:13,709
I first people talked about the elastic

00:11:10,860 --> 00:11:15,360
stack so we have on the top I would

00:11:13,709 --> 00:11:19,370
start on the top we have Cabana and this

00:11:15,360 --> 00:11:23,550
is our front end where we can do you

00:11:19,370 --> 00:11:26,399
full-text search in our data we can

00:11:23,550 --> 00:11:31,620
visualize the data and different fields

00:11:26,399 --> 00:11:34,110
and we can see the matrix or right at

00:11:31,620 --> 00:11:37,020
the fields we are collecting and we can

00:11:34,110 --> 00:11:40,290
create reports from that and many other

00:11:37,020 --> 00:11:43,230
fancy stuff and underneath there is

00:11:40,290 --> 00:11:48,350
elastic search it's a patch illusion

00:11:43,230 --> 00:11:51,510
based index storage you can say at least

00:11:48,350 --> 00:11:55,529
where all the information is stored and

00:11:51,510 --> 00:11:58,860
so-called indexes so what information is

00:11:55,529 --> 00:12:01,290
stored in this indexes so at least our

00:11:58,860 --> 00:12:07,260
lock line so we can call our lock line

00:12:01,290 --> 00:12:13,220
document so each lock line we will store

00:12:07,260 --> 00:12:17,670
in an inlet see represents one document

00:12:13,220 --> 00:12:21,709
at the beginning plain document align a

00:12:17,670 --> 00:12:25,980
sentence that's it that's all and then

00:12:21,709 --> 00:12:29,160
on top we can use a lock stash for

00:12:25,980 --> 00:12:30,810
example retrieving the data not only

00:12:29,160 --> 00:12:33,120
retrieving the data are also

00:12:30,810 --> 00:12:35,820
manipulating the data or better to

00:12:33,120 --> 00:12:40,500
enrich our data so that means both locks

00:12:35,820 --> 00:12:40,950
- we are in the possibility - how can I

00:12:40,500 --> 00:12:47,610
say it

00:12:40,950 --> 00:12:50,760
need to to to establish high value for

00:12:47,610 --> 00:12:54,870
our data so we can create extra fields

00:12:50,760 --> 00:12:58,770
which we can they are combined with this

00:12:54,870 --> 00:13:02,339
document where values we are retrieving

00:12:58,770 --> 00:13:03,820
from passing this document we are

00:13:02,339 --> 00:13:05,920
considering it

00:13:03,820 --> 00:13:08,710
that means we can create fields with

00:13:05,920 --> 00:13:15,700
values from that document additionally

00:13:08,710 --> 00:13:17,590
and can store them in our index so how

00:13:15,700 --> 00:13:20,050
we can retrieve the locks with locks -

00:13:17,590 --> 00:13:23,560
we can retrieve the locks with locks -

00:13:20,050 --> 00:13:28,150
through using different shippers that

00:13:23,560 --> 00:13:30,130
means we have different inputs I already

00:13:28,150 --> 00:13:32,260
mentioned our syslog for example let's

00:13:30,130 --> 00:13:36,310
one and put we can use in combination

00:13:32,260 --> 00:13:40,570
with the TCP input and means we optimus

00:13:36,310 --> 00:13:44,440
socket in locks - which is lesson on not

00:13:40,570 --> 00:13:46,810
a public part of five one four for

00:13:44,440 --> 00:13:48,820
example because lock service locks - is

00:13:46,810 --> 00:13:52,030
running with its own user so we need a

00:13:48,820 --> 00:13:55,570
other part are we doing some port

00:13:52,030 --> 00:13:58,570
forwarding on the retrieving host but we

00:13:55,570 --> 00:14:01,300
also can use beats beats is provided by

00:13:58,570 --> 00:14:04,330
elastic and beat Springs the so-called

00:14:01,300 --> 00:14:08,530
file beat and with a file which we can

00:14:04,330 --> 00:14:14,410
read in plain files on our surface and

00:14:08,530 --> 00:14:19,840
can ship them to lakhs - to enrich the

00:14:14,410 --> 00:14:23,530
data - - to filter them to split them

00:14:19,840 --> 00:14:27,550
into several value fields and at least

00:14:23,530 --> 00:14:31,660
locks - also write our data in different

00:14:27,550 --> 00:14:33,490
outputs so now our case we'll use

00:14:31,660 --> 00:14:37,750
elastic search but there are many ways

00:14:33,490 --> 00:14:40,030
where we can route our data into so we

00:14:37,750 --> 00:14:45,040
have an output as well for a singer we

00:14:40,030 --> 00:14:47,320
have a output for a male and we can also

00:14:45,040 --> 00:14:54,190
redirect the output again back to

00:14:47,320 --> 00:14:57,150
another dish tcp output and so on so

00:14:54,190 --> 00:15:04,090
that means we have stack which covers

00:14:57,150 --> 00:15:09,100
sending data retrieving data filtering

00:15:04,090 --> 00:15:15,180
data and storing data and make it

00:15:09,100 --> 00:15:15,180
visible to the user

00:15:15,250 --> 00:15:25,160
so next we should take a look at the

00:15:22,100 --> 00:15:27,410
various locks how can we match that into

00:15:25,160 --> 00:15:31,130
that stack or how we can reach with the

00:15:27,410 --> 00:15:33,230
elastic stagger of goals so the log and

00:15:31,130 --> 00:15:37,700
its kind so what do we have we have an

00:15:33,230 --> 00:15:45,680
application lock our barrio start lock

00:15:37,700 --> 00:15:48,620
and we have additionally a debug lock if

00:15:45,680 --> 00:15:50,660
we put the demons into the into the

00:15:48,620 --> 00:15:53,779
debug mode we can write on the debug

00:15:50,660 --> 00:15:55,850
output into lock files and at least year

00:15:53,779 --> 00:16:02,000
I missed it we have also the barriers

00:15:55,850 --> 00:16:04,640
out it lock for example where we writing

00:16:02,000 --> 00:16:07,040
in a protocol for all the actions which

00:16:04,640 --> 00:16:09,260
are taking place into the the big

00:16:07,040 --> 00:16:15,380
console or interacting with the director

00:16:09,260 --> 00:16:18,860
in any kind of console connection then

00:16:15,380 --> 00:16:24,190
we have sources as well so our servers

00:16:18,860 --> 00:16:24,190
so we have different sources

00:16:24,790 --> 00:16:33,110
applications but the main application

00:16:28,580 --> 00:16:35,000
log is written and who is cinephile so

00:16:33,110 --> 00:16:38,600
the director to storage team and

00:16:35,000 --> 00:16:46,310
Philemon are writing their output into

00:16:38,600 --> 00:16:49,640
the various dialog instead of we

00:16:46,310 --> 00:16:54,200
producing debug logs then each demon is

00:16:49,640 --> 00:16:58,370
producing its own lock for this D box so

00:16:54,200 --> 00:17:02,029
that means at first okay nice we have

00:16:58,370 --> 00:17:09,280
only to take care about one file one log

00:17:02,029 --> 00:17:13,040
file during normal application

00:17:09,280 --> 00:17:15,559
processing occurrence on distributed

00:17:13,040 --> 00:17:18,920
systems so this is a problem for

00:17:15,559 --> 00:17:23,750
barriers do we need multiple shipper

00:17:18,920 --> 00:17:27,680
to send our logs to our lot management

00:17:23,750 --> 00:17:30,270
server no why

00:17:27,680 --> 00:17:32,760
what's the default of a Barrios Philemon

00:17:30,270 --> 00:17:36,210
developer Barrios Philemon is sending

00:17:32,760 --> 00:17:39,270
all your logs over the various

00:17:36,210 --> 00:17:42,120
communication to the Piraeus director so

00:17:39,270 --> 00:17:44,280
that's the that's it that's the typical

00:17:42,120 --> 00:17:48,210
or normal way if you configure it with

00:17:44,280 --> 00:17:50,450
the standards so all the logs also from

00:17:48,210 --> 00:17:54,510
the Philemon's

00:17:50,450 --> 00:18:01,350
during a begger process are written in

00:17:54,510 --> 00:18:03,780
the various server so that means we only

00:18:01,350 --> 00:18:10,020
have to take care about the lock on our

00:18:03,780 --> 00:18:13,550
director so then we need a special log

00:18:10,020 --> 00:18:13,550
in to the for mark

00:18:30,540 --> 00:18:42,060
so I have to do the same like delay this

00:18:33,570 --> 00:18:47,810
morning so you say stop raita okay okay

00:18:42,060 --> 00:18:47,810
good so we are going to the beacon Seoul

00:18:51,950 --> 00:19:10,380
and we're starting a process so that's

00:19:00,210 --> 00:19:15,870
our first run our first job so yeah you

00:19:10,380 --> 00:19:18,600
should try it like that so wait well it

00:19:15,870 --> 00:19:19,950
doesn't look like like I wanted so wait

00:19:18,600 --> 00:19:22,880
a minute we will take a look in the

00:19:19,950 --> 00:19:22,880
plane fine last

00:19:34,640 --> 00:19:42,260
so yeah here's it so so here we have a

00:19:38,550 --> 00:19:46,700
line based protocol yeah so right so

00:19:42,260 --> 00:19:52,380
line for line okay then it comes to this

00:19:46,700 --> 00:19:56,130
occurrence where the job finished so

00:19:52,380 --> 00:20:02,210
sorry for that but I have to so that it

00:19:56,130 --> 00:20:02,210
is better with Apple for you so I will

00:20:02,510 --> 00:20:13,650
try to highlight it so here so here is

00:20:07,260 --> 00:20:17,940
the scheme broken so yeah for example

00:20:13,650 --> 00:20:22,050
you can say it's still line based but

00:20:17,940 --> 00:20:24,480
now if you are trying to inspect this

00:20:22,050 --> 00:20:28,310
log file you have to deal with lines

00:20:24,480 --> 00:20:28,310
which are not starting at the beginning

00:20:28,880 --> 00:20:37,440
yeah and then there's there's one method

00:20:34,980 --> 00:20:39,180
for that that's called multi-line and

00:20:37,440 --> 00:20:42,210
you can also use that if you're doing

00:20:39,180 --> 00:20:44,520
java application monitoring yeah um if

00:20:42,210 --> 00:20:49,160
there are some developer us and us they

00:20:44,520 --> 00:20:51,870
know how a suspect race designed so and

00:20:49,160 --> 00:20:56,820
it's line base and then the exception

00:20:51,870 --> 00:21:01,200
race and then you have the the in the

00:20:56,820 --> 00:21:05,640
Indians and then your normal line you

00:21:01,200 --> 00:21:07,920
like - yeah and to to to filter em

00:21:05,640 --> 00:21:10,530
starts not at the beginning so that's

00:21:07,920 --> 00:21:13,260
the reason why we can say here a

00:21:10,530 --> 00:21:19,410
multi-line segment of a log file is

00:21:13,260 --> 00:21:23,460
starting so if the job ends that means

00:21:19,410 --> 00:21:29,100
we can yeah we can classify the format

00:21:23,460 --> 00:21:31,470
of our log and now we have to think

00:21:29,100 --> 00:21:35,160
about what can we do with that lock what

00:21:31,470 --> 00:21:37,530
informations are in our lock to

00:21:35,160 --> 00:21:40,320
visualize - to filter so we have

00:21:37,530 --> 00:21:43,200
informations about the volumes how the

00:21:40,320 --> 00:21:45,320
volumes are used we are we have

00:21:43,200 --> 00:21:48,479
informations about data size

00:21:45,320 --> 00:21:51,809
how much data is written to volume how

00:21:48,479 --> 00:21:54,539
much data is left on a volume they have

00:21:51,809 --> 00:21:57,389
the information about the storage which

00:21:54,539 --> 00:22:05,549
is used the Chop ID we are also seeing

00:21:57,389 --> 00:22:07,879
arrows for example and yeah what what is

00:22:05,549 --> 00:22:12,440
the normal way if you are looking into

00:22:07,879 --> 00:22:16,859
into into a lab for troubleshooting yeah

00:22:12,440 --> 00:22:20,460
you open the document with looking for

00:22:16,859 --> 00:22:24,299
keywords right so with Latin with cat

00:22:20,460 --> 00:22:35,330
and crab and or in the file itself with

00:22:24,299 --> 00:22:37,919
lass or so on okay that means we have a

00:22:35,330 --> 00:22:42,210
lot of content a lot of informations

00:22:37,919 --> 00:22:48,539
there but there are not splitting

00:22:42,210 --> 00:22:51,259
keywords our own fields which are very

00:22:48,539 --> 00:22:51,259
visible for us

00:23:10,040 --> 00:23:21,810
so sorry lost the track so what

00:23:18,960 --> 00:23:24,510
informations do we need we need to know

00:23:21,810 --> 00:23:28,110
how to build a stack to collect the

00:23:24,510 --> 00:23:33,590
locks humans to store them in our in

00:23:28,110 --> 00:23:33,590
details we need to enrich this documents

00:23:34,190 --> 00:23:43,590
so that it fits to our proposal so yeah

00:23:39,060 --> 00:23:47,460
to our custom means may be to gain

00:23:43,590 --> 00:23:51,510
informations first earned and better who

00:23:47,460 --> 00:23:54,270
we want we want to store the documents

00:23:51,510 --> 00:23:59,820
to make them searchable over certain

00:23:54,270 --> 00:24:04,140
amount of time and yeah we want to work

00:23:59,820 --> 00:24:07,170
with them easily so let us leave the

00:24:04,140 --> 00:24:09,470
slides and let's take a look into the

00:24:07,170 --> 00:24:09,470
stack

00:24:51,949 --> 00:25:00,509
it's big enough so here we have our our

00:24:58,139 --> 00:25:04,409
Berea server it's a typical nominal

00:25:00,509 --> 00:25:08,789
standard configuration so today I

00:25:04,409 --> 00:25:11,609
decided to show you some simple tasks so

00:25:08,789 --> 00:25:14,699
that means I have an abstract of my

00:25:11,609 --> 00:25:18,509
actual work and what I want to show you

00:25:14,699 --> 00:25:22,289
and our task is to retrieve the

00:25:18,509 --> 00:25:25,009
informations about the volume how often

00:25:22,289 --> 00:25:29,519
is it perched how often is it written

00:25:25,009 --> 00:25:36,589
how often is the volume yeah used in

00:25:29,519 --> 00:25:39,949
different ways so purged written and so

00:25:36,589 --> 00:25:46,979
how we end it so at first we need to

00:25:39,949 --> 00:25:59,669
bring the logs to our central server so

00:25:46,979 --> 00:26:01,799
therefore we use file beat so we use

00:25:59,669 --> 00:26:03,659
file beat and five it used to so-called

00:26:01,799 --> 00:26:10,529
prospectors yeah

00:26:03,659 --> 00:26:14,399
that means prospectors section where we

00:26:10,529 --> 00:26:16,919
can then find our input types and this

00:26:14,399 --> 00:26:20,009
one in this case we want to use the

00:26:16,919 --> 00:26:23,759
input type block then you specify the

00:26:20,009 --> 00:26:25,759
parts where you find the files so as you

00:26:23,759 --> 00:26:29,969
can see I not only want to trace down

00:26:25,759 --> 00:26:35,849
the usage of a volume I also want to

00:26:29,969 --> 00:26:40,499
show you how you can you make making the

00:26:35,849 --> 00:26:42,929
debugging working easier to collect the

00:26:40,499 --> 00:26:45,359
debug traces from all the servers so

00:26:42,929 --> 00:26:48,299
that's what I said at least so each

00:26:45,359 --> 00:26:52,429
demon is writing his debug clock in its

00:26:48,299 --> 00:26:52,429
own file located on the server

00:26:55,529 --> 00:27:10,059
so that means I at least I built in this

00:27:02,860 --> 00:27:12,580
case we input types so I only do this to

00:27:10,059 --> 00:27:17,679
represent to three different types we

00:27:12,580 --> 00:27:21,399
need for all demons so in this case and

00:27:17,679 --> 00:27:24,100
it's not really needed because we are on

00:27:21,399 --> 00:27:26,169
a single stack various server where file

00:27:24,100 --> 00:27:28,419
demon and director storage demon are

00:27:26,169 --> 00:27:31,179
running on and then I doing some

00:27:28,419 --> 00:27:34,529
pre-work I already classified my

00:27:31,179 --> 00:27:41,440
documents so I give them the type debug

00:27:34,529 --> 00:27:47,340
so here's the first effort again so I'm

00:27:41,440 --> 00:27:50,980
I classify them as a debug document

00:27:47,340 --> 00:27:53,370
before storing them in any database or

00:27:50,980 --> 00:27:58,240
elasticsearch

00:27:53,370 --> 00:28:01,179
and then at least you're seeing here the

00:27:58,240 --> 00:28:04,870
last log input which is our main

00:28:01,179 --> 00:28:09,250
barriers lock and I have them the class

00:28:04,870 --> 00:28:15,600
barriers lock and I say at least all

00:28:09,250 --> 00:28:20,559
lines which are starting with 4 spaces a

00:28:15,600 --> 00:28:25,870
group together to a multi-line document

00:28:20,559 --> 00:28:27,370
that means our success hours lock path

00:28:25,870 --> 00:28:28,000
which will retrieve when a job is

00:28:27,370 --> 00:28:32,110
finished

00:28:28,000 --> 00:28:36,330
will be represent one document so one

00:28:32,110 --> 00:28:41,409
line if it's stored in our entity and

00:28:36,330 --> 00:28:47,880
then I will describe the output where

00:28:41,409 --> 00:28:53,289
the file read has to then the deal ops

00:28:47,880 --> 00:29:05,460
we collect so in this case our lock -

00:28:53,289 --> 00:29:05,460
retriever running

00:29:06,760 --> 00:29:12,920
it's active and running so that means by

00:29:10,610 --> 00:29:18,880
that way it already should have sent

00:29:12,920 --> 00:29:18,880
something or at least I try to do that

00:29:30,610 --> 00:29:39,890
yep it started to send it so document

00:29:35,300 --> 00:29:43,100
type at least that's a new version yeah

00:29:39,890 --> 00:29:47,929
Alexis deck is very fast in declaring

00:29:43,100 --> 00:29:52,550
something deprecated so document type at

00:29:47,929 --> 00:29:56,150
least has a life cycle from two years

00:29:52,550 --> 00:29:58,630
okay nice that means it's possible at

00:29:56,150 --> 00:30:02,990
both six the document type is

00:29:58,630 --> 00:30:06,080
disappearing so then we see it starts

00:30:02,990 --> 00:30:08,870
harvesting and it's really in the files

00:30:06,080 --> 00:30:12,020
line by line and on the server we have a

00:30:08,870 --> 00:30:14,200
so-called registry file that no data is

00:30:12,020 --> 00:30:19,490
unnecessary

00:30:14,200 --> 00:30:23,690
sending twice it's a simple ASCII file

00:30:19,490 --> 00:30:27,550
where the macro is written on which

00:30:23,690 --> 00:30:27,550
position it stops to read and define

00:30:29,260 --> 00:30:32,260
okay

00:30:39,380 --> 00:30:53,450
so we have running one shop that means

00:30:43,610 --> 00:31:01,150
maybe at least so we just say stop is it

00:30:53,450 --> 00:31:06,980
big enough okay seven take a look if

00:31:01,150 --> 00:31:08,480
logstash is running lock search is up

00:31:06,980 --> 00:31:13,940
and running and then a list we are

00:31:08,480 --> 00:31:15,850
logging if our elastic search is running

00:31:13,940 --> 00:31:20,050
its active and running all that means if

00:31:15,850 --> 00:31:22,510
there is already data and our index

00:31:20,050 --> 00:31:26,960
[Music]

00:31:22,510 --> 00:31:37,159
let's have a swallow and don't leave the

00:31:26,960 --> 00:31:40,000
slides that's true and the page and now

00:31:37,159 --> 00:31:48,169
we are in the front end in Cabana and

00:31:40,000 --> 00:31:51,020
it's nice that I already data is acute

00:31:48,169 --> 00:31:55,059
in our India so that means in elastic

00:31:51,020 --> 00:31:55,059
search indices are already created but

00:31:55,720 --> 00:32:04,070
Cabana have not died pattern so that

00:31:59,510 --> 00:32:07,220
means we will create finally finish the

00:32:04,070 --> 00:32:11,390
creation of the index and then we should

00:32:07,220 --> 00:32:17,350
see the keywords which are needed for

00:32:11,390 --> 00:32:17,350
our case today which are already

00:32:18,700 --> 00:32:27,020
established so this in this fields are

00:32:24,140 --> 00:32:32,809
retrieved through filtering I will

00:32:27,020 --> 00:32:34,460
explain it later on and they are safe

00:32:32,809 --> 00:32:36,620
with the document you know indeed same

00:32:34,460 --> 00:32:40,370
so that means they are already created

00:32:36,620 --> 00:32:43,760
and filled so that's possibly seeing

00:32:40,370 --> 00:32:47,030
under discover and then we are seeing

00:32:43,760 --> 00:32:54,490
some data are of the last 50 minutes and

00:32:47,030 --> 00:32:54,490
there will be our chop so here we are

00:32:55,780 --> 00:33:05,530
can you enlarge this that's a good

00:32:58,730 --> 00:33:09,710
question yeah what the pros assume so

00:33:05,530 --> 00:33:11,840
yeah it's a scaling yeah this is it a

00:33:09,710 --> 00:33:20,330
scaling yeah nice

00:33:11,840 --> 00:33:22,520
I love responsive too son so the

00:33:20,330 --> 00:33:27,260
enrichment has already taken place so as

00:33:22,520 --> 00:33:31,660
you can see here if we were doing not no

00:33:27,260 --> 00:33:35,780
filtering we have only the meta types

00:33:31,660 --> 00:33:38,210
some beat informations about the name of

00:33:35,780 --> 00:33:41,420
the beat the host name of the beat the

00:33:38,210 --> 00:33:43,820
version of the bead and the emitter

00:33:41,420 --> 00:33:47,330
informations of this document index

00:33:43,820 --> 00:33:55,090
which is stored in his ID the timestamp

00:33:47,330 --> 00:33:57,350
and the type and then at least a message

00:33:55,090 --> 00:33:59,900
but in our case there are already

00:33:57,350 --> 00:34:02,950
filters there and we have additional

00:33:59,900 --> 00:34:07,220
fields like the various job ID the demon

00:34:02,950 --> 00:34:14,050
who has written this lock a wall state

00:34:07,220 --> 00:34:20,540
like new a volume name then what else

00:34:14,050 --> 00:34:22,790
and we have text so in this case only

00:34:20,540 --> 00:34:25,970
attack which flex the file that it was

00:34:22,790 --> 00:34:29,120
shipped by a beats input and that it was

00:34:25,970 --> 00:34:32,420
cracked so what is crop crop is to

00:34:29,120 --> 00:34:36,080
filter use here so I tagged during

00:34:32,420 --> 00:34:38,900
filtering this document attacked it to

00:34:36,080 --> 00:34:42,020
recognize if it went through my filter

00:34:38,900 --> 00:34:45,320
was okay or not okay and if they're

00:34:42,020 --> 00:34:48,910
standing crop line or not line fails so

00:34:45,320 --> 00:34:54,800
that means my filter has worked so

00:34:48,910 --> 00:34:59,230
before we do a deeper look in this take

00:34:54,800 --> 00:34:59,230
a look in the motor of this so

00:35:05,180 --> 00:35:10,580
at least we are providing a

00:35:07,350 --> 00:35:18,240
configuration so we providing an input

00:35:10,580 --> 00:35:21,840
this is our this or our beats input okay

00:35:18,240 --> 00:35:24,390
as well here our tcp and SNMP trap so

00:35:21,840 --> 00:35:27,810
from our testing staff so at least we

00:35:24,390 --> 00:35:33,150
have here a beats input which is listen

00:35:27,810 --> 00:35:37,050
on part five zero four four and then we

00:35:33,150 --> 00:35:39,360
have our configuration ordered by

00:35:37,050 --> 00:35:43,500
numbers why are other than by numbers so

00:35:39,360 --> 00:35:49,290
if you are writing filters it's needed

00:35:43,500 --> 00:35:51,900
at you to order them in that way you

00:35:49,290 --> 00:35:54,630
want to process your data because each

00:35:51,900 --> 00:35:57,180
unnecessary filtering of your data

00:35:54,630 --> 00:36:00,240
increase the performance you need and an

00:35:57,180 --> 00:36:03,900
amount of time for filtering it for

00:36:00,240 --> 00:36:06,930
processing it so here we have the first

00:36:03,900 --> 00:36:08,970
filter so here we are looking for soos

00:36:06,930 --> 00:36:12,630
locks but we are don't shipping source

00:36:08,970 --> 00:36:14,700
locks that means all files don't passing

00:36:12,630 --> 00:36:19,320
this field anyway because we have and

00:36:14,700 --> 00:36:21,560
yeah we have type this we try to match

00:36:19,320 --> 00:36:27,960
so okay

00:36:21,560 --> 00:36:29,790
then we will take a look in this filter

00:36:27,960 --> 00:36:35,990
so and here if you see if the type is

00:36:29,790 --> 00:36:41,430
debug yeah then we ship it through our

00:36:35,990 --> 00:36:49,220
let's roll filter so fraggin match false

00:36:41,430 --> 00:36:53,460
so normally if a conditional and match

00:36:49,220 --> 00:36:56,190
processing a message can stop at that

00:36:53,460 --> 00:36:58,500
point that we don't want it ended way

00:36:56,190 --> 00:37:02,850
because if your processing this message

00:36:58,500 --> 00:37:07,109
in one filter twice so here we can say

00:37:02,850 --> 00:37:12,110
see water water filter a crock filter

00:37:07,109 --> 00:37:15,199
does so what it does

00:37:12,110 --> 00:37:15,199
[Music]

00:37:15,450 --> 00:37:24,130
taken here so we're testing our pattern

00:37:20,620 --> 00:37:30,400
so we have our message for example a

00:37:24,130 --> 00:37:33,130
debug message so okay it's also scanning

00:37:30,400 --> 00:37:35,230
in the right way so for example let's

00:37:33,130 --> 00:37:38,050
have a - let's have a look at our debug

00:37:35,230 --> 00:37:46,560
message we have that debug message and

00:37:38,050 --> 00:37:50,980
the debug message has each line the same

00:37:46,560 --> 00:37:53,980
the same characters until the real

00:37:50,980 --> 00:37:59,040
message part begins so that means we

00:37:53,980 --> 00:38:04,290
have we have a date we have a timestamp

00:37:59,040 --> 00:38:09,400
we have to demon we have the debug level

00:38:04,290 --> 00:38:16,780
we have the file where the code comes

00:38:09,400 --> 00:38:21,130
from the line and job ID which is

00:38:16,780 --> 00:38:30,670
involved so in this case it's job ID 11

00:38:21,130 --> 00:38:38,290
and then the rest what's that know the

00:38:30,670 --> 00:38:40,830
rest is our real debug message so let's

00:38:38,290 --> 00:38:40,830
take a look

00:38:50,410 --> 00:38:53,010
not

00:38:55,460 --> 00:38:59,240
okay there is it

00:39:04,720 --> 00:39:07,380
matched

00:39:08,730 --> 00:39:17,150
crumpet and raucous kind of record style

00:39:12,720 --> 00:39:20,160
and actually I'm developing some

00:39:17,150 --> 00:39:25,200
additional pattern which we can use to

00:39:20,160 --> 00:39:29,640
filter our beret locks so in this case I

00:39:25,200 --> 00:39:31,560
established this special pattern called

00:39:29,640 --> 00:39:33,780
peruse the time stamp

00:39:31,560 --> 00:39:35,970
there are also some basic patterns but

00:39:33,780 --> 00:39:41,340
there were no pattern which already

00:39:35,970 --> 00:39:43,710
matches on that time stamp so yeah of

00:39:41,340 --> 00:39:48,000
course there are style guides and there

00:39:43,710 --> 00:39:50,970
are yeah there are rules to establish

00:39:48,000 --> 00:39:55,290
well at times themes but sometimes each

00:39:50,970 --> 00:39:57,090
developer do it on its own way so and

00:39:55,290 --> 00:40:02,040
that's the reason why here's it

00:39:57,090 --> 00:40:05,400
necessary to provide you know a custom

00:40:02,040 --> 00:40:10,710
pattern so what means that pattern so on

00:40:05,400 --> 00:40:14,010
the left side we have our zoom tax our

00:40:10,710 --> 00:40:17,490
expression which should match and on the

00:40:14,010 --> 00:40:21,410
right side we have our semantic so that

00:40:17,490 --> 00:40:31,650
means our field which this expression

00:40:21,410 --> 00:40:34,880
represent okay so far okay so and this

00:40:31,650 --> 00:40:41,250
is what happens in here we're using

00:40:34,880 --> 00:40:46,490
different crop petals to describing our

00:40:41,250 --> 00:40:52,410
message so that means at least I have to

00:40:46,490 --> 00:40:56,240
rebuild the whole document in that way

00:40:52,410 --> 00:40:56,240
that is that's it fit to my needs

00:40:57,050 --> 00:41:07,620
so I'm also using their some already

00:41:02,690 --> 00:41:10,560
existing patterns and I'm also using

00:41:07,620 --> 00:41:13,560
here an hour and a second pattern

00:41:10,560 --> 00:41:17,580
because I recognize that also the debug

00:41:13,560 --> 00:41:20,490
log format has not in every line the

00:41:17,580 --> 00:41:36,119
same format

00:41:20,490 --> 00:41:40,619
and this crap Adam just crap our finding

00:41:36,119 --> 00:41:44,660
for example here so we call first line

00:41:40,619 --> 00:41:48,680
so so now we come closer to the end so

00:41:44,660 --> 00:41:52,290
finally our first step is to define

00:41:48,680 --> 00:41:55,050
pattern that we need to filter our

00:41:52,290 --> 00:41:58,650
document and as you can see it's a lot

00:41:55,050 --> 00:42:03,690
of work and it's it's a first draft

00:41:58,650 --> 00:42:06,540
maybe there should be some adjustments

00:42:03,690 --> 00:42:09,750
but if I want to do it in the right way

00:42:06,540 --> 00:42:14,490
I think I have to rebuild the whole log

00:42:09,750 --> 00:42:18,300
as a crop pattern I think there's at

00:42:14,490 --> 00:42:22,410
least no way to do it so that it fits

00:42:18,300 --> 00:42:26,810
hundred percent so that means I also can

00:42:22,410 --> 00:42:30,330
use already existing parents to

00:42:26,810 --> 00:42:33,869
establish new parents and now we are

00:42:30,330 --> 00:42:39,839
looking into the barrios wall state for

00:42:33,869 --> 00:42:44,240
example in this line so so simple

00:42:39,839 --> 00:42:48,900
consumption so biased new volume created

00:42:44,240 --> 00:42:55,369
so knew he would and yesterday news team

00:42:48,900 --> 00:42:59,099
yeah created new volume volume named X

00:42:55,369 --> 00:43:05,960
in catalog that means him for me okay in

00:42:59,099 --> 00:43:10,670
this job a new volume was created and

00:43:05,960 --> 00:43:10,670
written to the catalog and Torah storage

00:43:11,599 --> 00:43:18,859
then we have for example

00:43:20,849 --> 00:43:26,160
ready to upend that shop starts and

00:43:23,519 --> 00:43:29,099
there is a volume to a pant on it that

00:43:26,160 --> 00:43:31,319
means for me the job starts it finds her

00:43:29,099 --> 00:43:35,849
a patentable volume that means from your

00:43:31,319 --> 00:43:38,489
volume was written once twice yeah

00:43:35,849 --> 00:43:45,119
that's for me to count our half method

00:43:38,489 --> 00:43:49,670
written that means ready to append to

00:43:45,119 --> 00:43:49,670
and a volume named X

00:43:59,550 --> 00:44:11,530
so we have to debugging stuff and then

00:44:02,680 --> 00:44:13,599
we have various stuff that means for me

00:44:11,530 --> 00:44:21,069
you have to normal various lock I have a

00:44:13,599 --> 00:44:26,440
log line which I filled which I'm

00:44:21,069 --> 00:44:28,329
filtering and I have a conditional to

00:44:26,440 --> 00:44:32,079
look if it's a multi-line log or not so

00:44:28,329 --> 00:44:37,540
I know the built OS from the file daemon

00:44:32,079 --> 00:44:42,400
is mentioned at our final Lowcountry for

00:44:37,540 --> 00:44:45,970
the chop so that means I am I place in

00:44:42,400 --> 00:44:48,579
there conditioner to to find out if it's

00:44:45,970 --> 00:44:54,000
a multi line segment or not or it is the

00:44:48,579 --> 00:44:58,390
end of the job or not and then the

00:44:54,000 --> 00:45:01,869
different filters will be used to split

00:44:58,390 --> 00:45:08,650
the information from the document into

00:45:01,869 --> 00:45:13,839
new fields so that means if it match two

00:45:08,650 --> 00:45:21,819
by two as each croc match will be

00:45:13,839 --> 00:45:29,859
applied on the message until it adds so

00:45:21,819 --> 00:45:31,420
how does this look like in Havana so we

00:45:29,859 --> 00:45:34,089
have here the full text search for

00:45:31,420 --> 00:45:42,549
example we can search for volume like

00:45:34,089 --> 00:45:44,619
this okay and last 30 minutes we can

00:45:42,549 --> 00:45:47,380
search for volume like this and we have

00:45:44,619 --> 00:45:55,210
the highlighting on and so on okay

00:45:47,380 --> 00:45:57,280
that's nice but so that's how however I

00:45:55,210 --> 00:46:01,900
wish call is wait a second

00:45:57,280 --> 00:46:06,000
so here we are seeing our field so what

00:46:01,900 --> 00:46:09,000
I can do also for example as I can write

00:46:06,000 --> 00:46:09,000
variants

00:46:10,769 --> 00:46:20,739
so you must be volume twice why volume

00:46:17,859 --> 00:46:23,680
bytes okay it's shrink because soon I

00:46:20,739 --> 00:46:29,950
had confused so various volume barrios

00:46:23,680 --> 00:46:34,539
underscore volume and okay now it's a

00:46:29,950 --> 00:46:39,880
bit easy our bad I know it's three G

00:46:34,539 --> 00:46:44,769
three zeros are and then yes so now I

00:46:39,880 --> 00:46:47,430
have all messages where exactly so why

00:46:44,769 --> 00:46:47,430
there's three

00:47:02,290 --> 00:47:06,100
okay so

00:47:09,620 --> 00:47:17,440
for them we do it faster we do it like

00:47:11,780 --> 00:47:21,980
that I don't know so at least I provided

00:47:17,440 --> 00:47:24,080
not a full-text search so you see I

00:47:21,980 --> 00:47:27,890
don't establish a valid expression in

00:47:24,080 --> 00:47:31,370
the full-text search I I placed a filter

00:47:27,890 --> 00:47:36,500
on a specific field on the various

00:47:31,370 --> 00:47:39,410
volume field and I the first volume and

00:47:36,500 --> 00:47:41,900
you automatically see all the volumes

00:47:39,410 --> 00:47:44,240
where this all the messages all the

00:47:41,900 --> 00:47:48,050
documents where this volume occurs so at

00:47:44,240 --> 00:47:54,370
least it should be also a multi-line

00:47:48,050 --> 00:47:54,370
document in it so no it's only a slug

00:47:55,000 --> 00:48:00,170
because there is no field on it so we

00:47:58,250 --> 00:48:05,240
are seeing that the volume was labeled

00:48:00,170 --> 00:48:09,920
and we are seeing that it's used and how

00:48:05,240 --> 00:48:16,220
it's used and so let's take a look so it

00:48:09,920 --> 00:48:25,700
was the first job ID I guess let's take

00:48:16,220 --> 00:48:30,530
a look for the job ID so why they are

00:48:25,700 --> 00:48:32,900
twice okay there's typos not so nice but

00:48:30,530 --> 00:48:37,250
yeah we have to deal with that I place

00:48:32,900 --> 00:48:41,710
the typos so here are all the messages

00:48:37,250 --> 00:48:41,710
from various job ID one at least

00:48:44,650 --> 00:48:57,700
and then what I didn't do is I don't

00:48:48,820 --> 00:48:59,440
throw the original message away so the

00:48:57,700 --> 00:49:03,370
main reason is in this case we are don't

00:48:59,440 --> 00:49:06,040
split the whole message so only specific

00:49:03,370 --> 00:49:16,990
sack segments and we don't leaving an

00:49:06,040 --> 00:49:20,680
unfiltered part of it and then at least

00:49:16,990 --> 00:49:29,040
we can differences the two types of our

00:49:20,680 --> 00:49:38,700
documents so here time times aging so

00:49:29,040 --> 00:49:43,050
wait we introduce another job run and

00:49:38,700 --> 00:49:43,050
then I want also some

00:50:01,230 --> 00:50:21,300
so so there are new jobs okay so it

00:50:16,560 --> 00:50:28,310
makes it easier to to represent some

00:50:21,300 --> 00:50:28,310
data so and we have here the two types

00:50:36,690 --> 00:50:40,069
No why not

00:50:46,360 --> 00:50:52,830
okay

00:50:48,000 --> 00:51:06,180
only ones leaving so the second is last

00:50:52,830 --> 00:51:07,920
okay I don't know why actually okay we

00:51:06,180 --> 00:51:10,650
are going further on so because I want

00:51:07,920 --> 00:51:13,410
to show you some visualization for a use

00:51:10,650 --> 00:51:17,070
case we want to trace down the usage of

00:51:13,410 --> 00:51:20,550
a volume so I'm already prepared that

00:51:17,070 --> 00:51:26,420
but there are still some problem fields

00:51:20,550 --> 00:51:39,090
in it so don't wondering about this okay

00:51:26,420 --> 00:51:44,270
relations and we have to at the end we

00:51:39,090 --> 00:51:49,110
have here our valid visualization few it

00:51:44,270 --> 00:51:51,450
and so this then look looking really

00:51:49,110 --> 00:51:53,520
nice so that's the visualization builder

00:51:51,450 --> 00:51:58,680
so we can do something with our later

00:51:53,520 --> 00:52:01,740
and doing a vertical bar where the bars

00:51:58,680 --> 00:52:06,560
are splitted so we were showing in a

00:52:01,740 --> 00:52:10,320
better format like this let's expand it

00:52:06,560 --> 00:52:13,740
Senate lease fears thing differential

00:52:10,320 --> 00:52:16,680
and incremental why we are only seeing

00:52:13,740 --> 00:52:20,700
this too because we have a fixed time

00:52:16,680 --> 00:52:23,780
filter so we swap the time filter to

00:52:20,700 --> 00:52:27,270
four hours and now we're seeing all our

00:52:23,780 --> 00:52:33,390
our volumes and the usage so we are

00:52:27,270 --> 00:52:36,480
seeing that yeah each volume is labeled

00:52:33,390 --> 00:52:40,800
once because it's the beginning then we

00:52:36,480 --> 00:52:48,390
are seeing each volume is written once

00:52:40,800 --> 00:52:50,460
and so we see we're seeing yeah it's not

00:52:48,390 --> 00:52:53,580
only created once and written one son

00:52:50,460 --> 00:52:55,710
and it's already written twice because

00:52:53,580 --> 00:52:59,230
they were because there was a pan for it

00:52:55,710 --> 00:53:02,080
and now we should also

00:52:59,230 --> 00:53:08,820
already have some retention time taking

00:53:02,080 --> 00:53:08,820
place and you were running a new job

00:53:13,830 --> 00:53:17,460
needed for

00:53:20,190 --> 00:53:23,970
she wet something

00:53:29,060 --> 00:53:36,150
now we are seeing the approaching

00:53:32,850 --> 00:53:41,370
actions taking place so volume full two

00:53:36,150 --> 00:53:44,190
and four one approached yeah okay so

00:53:41,370 --> 00:53:47,070
that was the case which I want to cover

00:53:44,190 --> 00:53:51,950
i want to trace down a volume and a

00:53:47,070 --> 00:53:54,150
first and yeah fast way so that's all

00:53:51,950 --> 00:53:57,450
only a small thing what you can do with

00:53:54,150 --> 00:54:03,180
the data so as we already seen I have

00:53:57,450 --> 00:54:06,600
this byte fields and file demon fields I

00:54:03,180 --> 00:54:11,010
already tracked them down from my multi

00:54:06,600 --> 00:54:13,860
log patterns so why I can can do

00:54:11,010 --> 00:54:17,250
something more with that so what we here

00:54:13,860 --> 00:54:19,290
have our strengths so the next step

00:54:17,250 --> 00:54:22,740
where to translate for example these

00:54:19,290 --> 00:54:26,190
strings into numbers so that I so I can

00:54:22,740 --> 00:54:31,070
have matrix for summer rations so that I

00:54:26,190 --> 00:54:36,870
can build sums for example and at least

00:54:31,070 --> 00:54:39,630
finally we will start some debug so the

00:54:36,870 --> 00:54:41,970
easiest way to start a debug mode for

00:54:39,630 --> 00:54:43,370
components as doing it wouldn t be

00:54:41,970 --> 00:54:46,380
console

00:54:43,370 --> 00:54:50,640
yeah so we are seeing it starts to

00:54:46,380 --> 00:54:57,260
writing the trace files and now we

00:54:50,640 --> 00:54:57,260
should take some action and yeah will do

00:55:00,579 --> 00:55:16,890
and so that it's not so heavy in

00:55:05,170 --> 00:55:22,390
operations so that's a bit 269 looks big

00:55:16,890 --> 00:55:30,189
less so we will filter it only to the

00:55:22,390 --> 00:55:32,289
debug output so that's also the reason

00:55:30,189 --> 00:55:37,150
why I choose a small set of the actually

00:55:32,289 --> 00:55:38,739
crop pattern because because it's a

00:55:37,150 --> 00:55:41,679
small farm and I have not enough

00:55:38,739 --> 00:55:46,089
resources to cover more parents and

00:55:41,679 --> 00:55:48,819
further processes so and here we see

00:55:46,089 --> 00:55:52,979
these fields are not already known that

00:55:48,819 --> 00:55:58,359
means we have to actualize our index so

00:55:52,979 --> 00:56:01,689
for the proposes it's better to so are I

00:55:58,359 --> 00:56:04,479
suggest to establish different parents

00:56:01,689 --> 00:56:08,109
where you store your data in so that

00:56:04,479 --> 00:56:13,839
means it's better to introduce debug

00:56:08,109 --> 00:56:19,809
parent to use Sandy and here we are so

00:56:13,839 --> 00:56:22,029
here we are seeing our debug output we

00:56:19,809 --> 00:56:23,729
are seeing the original message because

00:56:22,029 --> 00:56:27,340
I didn't drop it

00:56:23,729 --> 00:56:28,979
we're seeing the debug message only and

00:56:27,340 --> 00:56:32,709
[Music]

00:56:28,979 --> 00:56:34,569
we have the the job number so which I

00:56:32,709 --> 00:56:37,769
call top control number because there

00:56:34,569 --> 00:56:40,329
are also internal jobs and if there are

00:56:37,769 --> 00:56:43,589
various internal job which are not

00:56:40,329 --> 00:56:47,949
directly related to the job they are

00:56:43,589 --> 00:56:53,319
retrieving the number 0 as a job ID from

00:56:47,949 --> 00:56:58,269
the director and we see our debug levels

00:56:53,319 --> 00:57:03,009
so and why is it an effort to have this

00:56:58,269 --> 00:57:06,519
for for a debug locks so we have more

00:57:03,009 --> 00:57:09,160
than one storage demon yeah for example

00:57:06,519 --> 00:57:12,130
we have several clients and the file

00:57:09,160 --> 00:57:14,080
bead I mentioned before for shipping it

00:57:12,130 --> 00:57:17,950
is written in go

00:57:14,080 --> 00:57:21,580
it costs not a lot of performance to run

00:57:17,950 --> 00:57:24,100
it so that means you do not compromise

00:57:21,580 --> 00:57:29,220
the system which you want to analyze

00:57:24,100 --> 00:57:35,410
with there are additional performance

00:57:29,220 --> 00:57:40,900
and you can classify the document before

00:57:35,410 --> 00:57:47,560
you shipping it and now I want to open

00:57:40,900 --> 00:57:49,630
some safe search and that's D so what I

00:57:47,560 --> 00:57:52,540
can also do with searches nothing that's

00:57:49,630 --> 00:57:57,490
at least the best way to track down your

00:57:52,540 --> 00:58:04,930
various application is you can now you

00:57:57,490 --> 00:58:07,060
can file a special view of the data and

00:58:04,930 --> 00:58:10,750
it means you can select fields which are

00:58:07,060 --> 00:58:18,130
represented directly and this search

00:58:10,750 --> 00:58:22,080
view and okay okay assuming I have to

00:58:18,130 --> 00:58:25,600
scroll so that means at least we seeded

00:58:22,080 --> 00:58:30,280
the debug time we see the daemon which

00:58:25,600 --> 00:58:33,040
produces the message we are seeing our

00:58:30,280 --> 00:58:38,260
debug level we're seeing the debug

00:58:33,040 --> 00:58:41,710
source file and the line where this code

00:58:38,260 --> 00:58:48,000
to perform this action come from and at

00:58:41,710 --> 00:58:48,000
least our debug message will still aft

00:58:50,460 --> 00:58:56,380
then we are seeing that it's all

00:58:53,700 --> 00:59:01,330
internal related chops because they have

00:58:56,380 --> 00:59:05,950
to 0 so then if I want to track down a

00:59:01,330 --> 00:59:06,840
single job in a fast way is for example

00:59:05,950 --> 00:59:09,939
to

00:59:06,840 --> 00:59:09,939
[Music]

00:59:15,350 --> 00:59:18,350
talent

00:59:26,109 --> 00:59:35,440
is to use a special text so that was

00:59:30,089 --> 00:59:39,549
yeah and some kind of tricky because you

00:59:35,440 --> 00:59:41,049
not only have the dialogue of the debug

00:59:39,549 --> 00:59:43,329
informations which are related to the

00:59:41,049 --> 00:59:48,390
job you also have to internal lock

00:59:43,329 --> 00:59:52,900
messages and at first they retrieve the

00:59:48,390 --> 00:59:56,999
number zero and they have to group

00:59:52,900 --> 00:59:59,559
together with your with your job and I

00:59:56,999 --> 01:00:05,349
again that through using an additional

00:59:59,559 --> 01:00:13,390
tech where I can filter for and now I

01:00:05,349 --> 01:00:15,309
can see all kind of messages from the

01:00:13,390 --> 01:00:23,650
debugging which are related to our job

01:00:15,309 --> 01:00:26,380
ID seven at least so yeah this is how

01:00:23,650 --> 01:00:37,569
you can trace down with elastic you are

01:00:26,380 --> 01:00:40,329
debug log for example okay so on my

01:00:37,569 --> 01:00:43,299
sources are biggest form elastic stack

01:00:40,329 --> 01:00:48,400
and the gifts are nicely provided by a

01:00:43,299 --> 01:00:52,630
give EECOM so if you have for the

01:00:48,400 --> 01:00:57,249
questions now you have two possibility

01:00:52,630 --> 01:01:00,269
to bring them up and yeah there is a

01:00:57,249 --> 01:01:00,269
additional question

01:01:08,190 --> 01:01:13,569
yeah that's also possible that means you

01:01:10,930 --> 01:01:16,470
have to use another log stash filter

01:01:13,569 --> 01:01:16,470
called

01:01:22,529 --> 01:01:28,720
that's a good question I don't exactly

01:01:25,869 --> 01:01:30,910
know that if it's possible in Barrios so

01:01:28,720 --> 01:01:32,440
you can use different outputs firlock

01:01:30,910 --> 01:01:35,859
where you can write it and you can also

01:01:32,440 --> 01:01:40,210
route it to two syslog if you do that

01:01:35,859 --> 01:01:42,369
you also gain the system the FC syslog

01:01:40,210 --> 01:01:47,140
specification for the time stamp because

01:01:42,369 --> 01:01:50,020
it's put before the original message but

01:01:47,140 --> 01:01:52,029
that's something you can do but I don't

01:01:50,020 --> 01:01:53,859
know there is any other reason or maybe

01:01:52,029 --> 01:01:58,750
Philip you have an answer for that is

01:01:53,859 --> 01:02:02,970
that possible there's a command-line

01:01:58,750 --> 01:02:02,970
switch okay

01:02:07,570 --> 01:02:14,690
okay another question the photos and all

01:02:13,250 --> 01:02:18,740
the stuff you did is it somewhere wait

01:02:14,690 --> 01:02:21,620
ago so yeah good question so as I

01:02:18,740 --> 01:02:26,900
mentioned that's only a abstract from of

01:02:21,620 --> 01:02:29,990
that what I I produced the last weeks I

01:02:26,900 --> 01:02:35,030
already already started this work some

01:02:29,990 --> 01:02:36,470
years ago and then yeah so I don't do

01:02:35,030 --> 01:02:40,340
something else for that

01:02:36,470 --> 01:02:43,490
then the beginnings and now I have

01:02:40,340 --> 01:02:45,440
recognized that it's to do more or to do

01:02:43,490 --> 01:02:48,500
them nude there's a redesign needed and

01:02:45,440 --> 01:02:51,950
I have a deadline for myself so until

01:02:48,500 --> 01:02:59,450
the end of the year but it's also a good

01:02:51,950 --> 01:03:01,850
question because I have only test

01:02:59,450 --> 01:03:05,300
systems where I can produce locks so I

01:03:01,850 --> 01:03:10,700
can use mhm vtl also for tape library

01:03:05,300 --> 01:03:13,100
locks but the daters are not very

01:03:10,700 --> 01:03:15,920
representative so if there are someone

01:03:13,100 --> 01:03:19,940
in this room which can provide me for

01:03:15,920 --> 01:03:21,800
example was productive locks that would

01:03:19,940 --> 01:03:25,910
be not really nice it would help me a

01:03:21,800 --> 01:03:29,450
lot so of course please respect your own

01:03:25,910 --> 01:03:31,610
privacy and you know covered in deluxe

01:03:29,450 --> 01:03:33,290
before you send it to me so but if

01:03:31,610 --> 01:03:37,330
there's someone who want to provide me

01:03:33,290 --> 01:03:37,330
for example some productive locks with

01:03:38,440 --> 01:03:45,980
right informations it would be really

01:03:40,790 --> 01:03:51,650
nice so to do it with not only in lab

01:03:45,980 --> 01:03:53,570
produced data so until the end of the

01:03:51,650 --> 01:03:56,900
year I will announce it on the users

01:03:53,570 --> 01:04:01,130
list as well and maybe there's a

01:03:56,900 --> 01:04:05,750
conference how is it called

01:04:01,130 --> 01:04:08,510
miss fat ila and there's a conference

01:04:05,750 --> 01:04:13,220
for Tyler what is the English word for

01:04:08,510 --> 01:04:15,320
for Thailand yeah mailing there's a

01:04:13,220 --> 01:04:18,050
conference - thank you oh my god

01:04:15,320 --> 01:04:24,040
so then I can let you know that

01:04:18,050 --> 01:04:36,260
if it's finished alright some questions

01:04:24,040 --> 01:04:38,350
from the audience back the locks when

01:04:36,260 --> 01:04:46,700
you change the patterns are these

01:04:38,350 --> 01:04:51,740
completely put through locks - again so

01:04:46,700 --> 01:04:55,490
when when I have locks inside there and

01:04:51,740 --> 01:05:01,010
I change the matching patterns in what

01:04:55,490 --> 01:05:03,860
was it called croc do I have to reread

01:05:01,010 --> 01:05:07,850
all my locks to apply this hi you mean

01:05:03,860 --> 01:05:10,400
if you already index them yes so if they

01:05:07,850 --> 01:05:13,700
are written once to the index there is

01:05:10,400 --> 01:05:17,450
no possibility to modify them in an easy

01:05:13,700 --> 01:05:19,370
way so you can modify some you can yeah

01:05:17,450 --> 01:05:21,110
of course you can modify the fields but

01:05:19,370 --> 01:05:24,620
then you have to need the elasticsearch

01:05:21,110 --> 01:05:28,340
API and you have to do some API stuff so

01:05:24,620 --> 01:05:30,380
but you don't earn the position to

01:05:28,340 --> 01:05:35,420
reprocess them with locks

01:05:30,380 --> 01:05:39,410
- instead of there's one kind of way to

01:05:35,420 --> 01:05:41,630
do that so it's really nasty so it's

01:05:39,410 --> 01:05:46,040
based on how you configured your lap

01:05:41,630 --> 01:05:49,340
retention so if you ship the locks will

01:05:46,040 --> 01:06:00,830
file beat two locks - I already showed

01:05:49,340 --> 01:06:04,220
that it has its own so wait it has its

01:06:00,830 --> 01:06:09,320
own registry file okay for example so if

01:06:04,220 --> 01:06:11,540
you really sign your filters then sorry

01:06:09,320 --> 01:06:15,790
I have to say that Dennis the only way

01:06:11,540 --> 01:06:19,280
to do it in in a correct manner to

01:06:15,790 --> 01:06:23,690
delete the index because if you are

01:06:19,280 --> 01:06:25,010
changing types elasticsearch is not in

01:06:23,690 --> 01:06:27,190
the position to handle that in the right

01:06:25,010 --> 01:06:30,890
way if you have different types in one

01:06:27,190 --> 01:06:31,849
field so that's not possible in the

01:06:30,890 --> 01:06:34,309
mapping just that means

01:06:31,849 --> 01:06:36,650
you have to delete the index then you

01:06:34,309 --> 01:06:39,499
have to delete a registry file on the

01:06:36,650 --> 01:06:42,589
file bit demon then you have to start it

01:06:39,499 --> 01:06:43,249
again and it starts so it's called by

01:06:42,589 --> 01:06:48,049
five bit

01:06:43,249 --> 01:06:51,170
harvesting from the beginning the whole

01:06:48,049 --> 01:06:54,109
log so I don't know how is the lock

01:06:51,170 --> 01:06:57,289
retention how big is a lock and so then

01:06:54,109 --> 01:06:59,390
it needs that amount of time to send it

01:06:57,289 --> 01:07:01,549
to locks - then it will process the

01:06:59,390 --> 01:07:03,700
whole filter line again and then it's

01:07:01,549 --> 01:07:07,930
stored in index that's the only

01:07:03,700 --> 01:07:11,259
reasonable way so I don't know any other

01:07:07,930 --> 01:07:11,259
okay thank you

01:07:11,829 --> 01:07:24,650
so more questions please come on guys

01:07:17,920 --> 01:07:28,640
right otherwise we are like all right

01:07:24,650 --> 01:07:34,779
seven seventeen fifteen we're right to

01:07:28,640 --> 01:07:37,980
the end now Rory thank you for the

01:07:34,779 --> 01:07:44,530
presentation Daniel it was great

01:07:37,980 --> 01:07:44,530

YouTube URL: https://www.youtube.com/watch?v=hNBnrYSJL1U


