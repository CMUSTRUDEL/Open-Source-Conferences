Title: Netdev 0x12 - UDP segmentation offload
Publication date: 2018-08-01
Playlist: Netdev 0x12
Description: 
	In this talk, Boris Pismenny presented the efforts towards supporting UDP segmentation offload with existing net devices. He talked about limitations encountered and how they were overcome. Based on these experiences, Boris had some suggestions on how to improve the Linux networking stack to generalize the work and make driver development easier.

Presented at Netdev 0x12 in Montreal on July 13th, 2018.

More info:
https://www.netdevconf.org/0x12/session.html?udp-segmentation-offload
Captions: 
	00:00:00,030 --> 00:00:06,870
hi my name is Boris I'm from Mellanox

00:00:03,419 --> 00:00:12,660
and I'm going to present this work about

00:00:06,870 --> 00:00:14,429
UDP segmentation offload and so the

00:00:12,660 --> 00:00:16,020
motivation for EDP segmentation offload

00:00:14,429 --> 00:00:20,609
is the quic protocol which was presented

00:00:16,020 --> 00:00:24,330
here earlier and just a quick wreck wake

00:00:20,609 --> 00:00:27,180
up quick combines TCP T less and HTTP no

00:00:24,330 --> 00:00:29,820
single protocol it's deployed today by

00:00:27,180 --> 00:00:32,520
Google and Akamai a recent paper

00:00:29,820 --> 00:00:35,760
suggested it's about 10 percent of

00:00:32,520 --> 00:00:37,710
Internet traffic it's also a bunch of

00:00:35,760 --> 00:00:40,469
networking problems such as head of the

00:00:37,710 --> 00:00:46,379
line blocking and most importantly is

00:00:40,469 --> 00:00:49,500
that it runs over the UDP protocol so a

00:00:46,379 --> 00:00:54,300
recent patch introduced segmentation for

00:00:49,500 --> 00:00:58,199
UDP with the new user API where user

00:00:54,300 --> 00:01:02,640
sends a large UDP message with the MSS

00:00:58,199 --> 00:01:06,750
provided using the UDP segment auxiliary

00:01:02,640 --> 00:01:10,770
data as we can see in the code on the

00:01:06,750 --> 00:01:14,240
right side the kernel will Sigma segment

00:01:10,770 --> 00:01:18,170
the large message to MSS sized packets

00:01:14,240 --> 00:01:18,170
including the UDP headers

00:01:18,350 --> 00:01:24,600
however no encapsulation is supported at

00:01:21,030 --> 00:01:27,900
the moment so it's just the UDP protocol

00:01:24,600 --> 00:01:32,159
and that's it the code here on the right

00:01:27,900 --> 00:01:35,549
is taken from the testing framework

00:01:32,159 --> 00:01:38,159
provided by William when he sent in the

00:01:35,549 --> 00:01:43,710
patch so it's available for everybody

00:01:38,159 --> 00:01:46,590
and so UDP segmentation offload the

00:01:43,710 --> 00:01:48,960
problem is that current Hardware was not

00:01:46,590 --> 00:01:53,220
built with UDP segmentation offload in

00:01:48,960 --> 00:01:55,049
mind and once it was introduced we ask

00:01:53,220 --> 00:01:57,060
yourself the question whether it was

00:01:55,049 --> 00:02:00,049
possible to support it with existing

00:01:57,060 --> 00:02:05,130
hardware even though it was not designed

00:02:00,049 --> 00:02:07,860
with this purpose in mind so we started

00:02:05,130 --> 00:02:11,400
by looking at what existing hanno does

00:02:07,860 --> 00:02:12,830
when it does segmentation so at the

00:02:11,400 --> 00:02:15,170
beginning it

00:02:12,830 --> 00:02:17,180
it's a packet by duplicating headers to

00:02:15,170 --> 00:02:20,410
all packets and then segmenting the

00:02:17,180 --> 00:02:23,270
payload to multiple packets and then

00:02:20,410 --> 00:02:26,960
segmentation offload kicks in which

00:02:23,270 --> 00:02:32,000
increments the IP ID for each packet

00:02:26,960 --> 00:02:37,220
adjust the UDP length accordingly and

00:02:32,000 --> 00:02:40,850
updates the checksum however in the case

00:02:37,220 --> 00:02:43,310
of our hardware we are calculating the

00:02:40,850 --> 00:02:45,830
UDP checksum using the original UDP

00:02:43,310 --> 00:02:50,240
checksum provided the original UDP

00:02:45,830 --> 00:02:55,430
length provided from software which in

00:02:50,240 --> 00:02:59,480
the case of UDP GSO is incorrect and as

00:02:55,430 --> 00:03:02,660
a result we would get packets with the

00:02:59,480 --> 00:03:06,890
wrong checksum so we needed to find some

00:03:02,660 --> 00:03:09,370
workaround for this problem and the

00:03:06,890 --> 00:03:14,780
workload we chose to use is to split the

00:03:09,370 --> 00:03:17,030
large UDP gso packet into two Hardware

00:03:14,780 --> 00:03:21,560
descriptors one of them is sandela so

00:03:17,030 --> 00:03:24,440
it's the same that sends TCP also

00:03:21,560 --> 00:03:28,220
messages and another which is the

00:03:24,440 --> 00:03:31,510
remaining segment which has a different

00:03:28,220 --> 00:03:34,610
length it is sent using a separate

00:03:31,510 --> 00:03:37,880
descriptor so essentially two

00:03:34,610 --> 00:03:41,660
descriptors are used to send a single G

00:03:37,880 --> 00:03:47,590
so offload requests and the headers are

00:03:41,660 --> 00:03:50,270
adjusted accordingly to fit both cases

00:03:47,590 --> 00:03:52,580
however after submitting the patch

00:03:50,270 --> 00:03:55,489
receive some feedback and we discovered

00:03:52,580 --> 00:04:00,080
that exactly this functionality is

00:03:55,489 --> 00:04:02,090
provided by GS or partial thanks Alex

00:04:00,080 --> 00:04:09,320
and William for reduce or partial and

00:04:02,090 --> 00:04:12,019
UDP segmentation is this doing

00:04:09,320 --> 00:04:14,740
segmentation or fragmentation UDP

00:04:12,019 --> 00:04:17,440
segmentation it's the recent patch what

00:04:14,740 --> 00:04:20,200
the IP ID have to do with it needs to be

00:04:17,440 --> 00:04:22,630
incremented for each segment that is

00:04:20,200 --> 00:04:25,150
being sent so you know it's not

00:04:22,630 --> 00:04:26,590
fragmentation know it's you have an

00:04:25,150 --> 00:04:30,100
example of how the packets are being

00:04:26,590 --> 00:04:31,570
split ups like so we can see no I don't

00:04:30,100 --> 00:04:40,780
have an example it would in the slide

00:04:31,570 --> 00:04:48,010
but it's yeah so those are you TP

00:04:40,780 --> 00:04:50,470
packets each with 1500 bytes that's

00:04:48,010 --> 00:04:53,560
what's being sent to the network and the

00:04:50,470 --> 00:04:55,570
IP idea the IP ID is incremented even

00:04:53,560 --> 00:04:59,500
though it's probably not necessary since

00:04:55,570 --> 00:05:02,650
they don't fragment it is set but this

00:04:59,500 --> 00:05:06,570
wasn't the problem with we were facing

00:05:02,650 --> 00:05:06,570
when we were implementing this anyway

00:05:08,880 --> 00:05:16,570
so we've measured the performance of a

00:05:11,920 --> 00:05:20,160
single stream UDP between two machines

00:05:16,570 --> 00:05:23,830
connected back to back and we compared

00:05:20,160 --> 00:05:26,200
UDP UDP software segmentation and UDP

00:05:23,830 --> 00:05:28,480
with the hardware segmentation in this

00:05:26,200 --> 00:05:32,500
graph we see both throughput and SIP

00:05:28,480 --> 00:05:36,370
utilization the fruit is represented by

00:05:32,500 --> 00:05:39,910
the lines with dots so the top we have

00:05:36,370 --> 00:05:41,470
UDP segmentation with hardware and the

00:05:39,910 --> 00:05:47,440
middle UDP segmentation with software

00:05:41,470 --> 00:05:49,390
and the bottom is plain old UDP the

00:05:47,440 --> 00:05:53,770
performance numbers were measured using

00:05:49,390 --> 00:05:58,150
the self test scripts that were provided

00:05:53,770 --> 00:06:02,050
by William in his original patch and as

00:05:58,150 --> 00:06:06,010
we can see in this slide the throughput

00:06:02,050 --> 00:06:10,000
or the performance improves by 3x when

00:06:06,010 --> 00:06:13,270
everything is is equal meaning when the

00:06:10,000 --> 00:06:14,980
CPU is equal between UDP segmentation in

00:06:13,270 --> 00:06:16,720
software and UDP segmentation hard way

00:06:14,980 --> 00:06:20,380
we'll get freaks improvement in

00:06:16,720 --> 00:06:23,820
throughput as is the case with 512 bytes

00:06:20,380 --> 00:06:23,820
of MSS

00:06:24,129 --> 00:06:31,499
but with smaller packets such as 64 byte

00:06:27,399 --> 00:06:31,499
packets we can get up to 10x improvement

00:06:35,309 --> 00:06:40,809
so to summarize all the hardware can

00:06:38,739 --> 00:06:44,679
still learn new tricks and UDP

00:06:40,809 --> 00:06:47,289
segmentation offload is an example of

00:06:44,679 --> 00:06:49,479
such an offload it is supported today

00:06:47,289 --> 00:06:55,149
with connect X Pro and kinetics 5 within

00:06:49,479 --> 00:06:57,669
the ml x5 driver and a full support or

00:06:55,149 --> 00:06:59,169
not just so partial support but the

00:06:57,669 --> 00:07:01,869
support using a single descriptor

00:06:59,169 --> 00:07:05,499
requires only a relatively simple change

00:07:01,869 --> 00:07:08,679
in hardware so that could be expected in

00:07:05,499 --> 00:07:10,659
future hardware as well regarding the

00:07:08,679 --> 00:07:12,429
timeline so the patch was first

00:07:10,659 --> 00:07:16,209
introduced on April 14th

00:07:12,429 --> 00:07:21,129
Alex Anton Alif is a rental Nixon

00:07:16,209 --> 00:07:25,629
mayferd and the Mellanox Nick has its

00:07:21,129 --> 00:07:30,819
patches on June 29th and now to the

00:07:25,629 --> 00:07:35,289
discussion and so the wedges or partial

00:07:30,819 --> 00:07:38,229
works today is that it provides a sort

00:07:35,289 --> 00:07:40,899
of wait pockets down the stack and up to

00:07:38,229 --> 00:07:45,879
the driver where the IP length and the

00:07:40,899 --> 00:07:49,209
UDP length are not matching so the IP

00:07:45,879 --> 00:07:52,089
total length is 1500 bytes were well the

00:07:49,209 --> 00:07:57,339
UDP length represents the unsegmented

00:07:52,089 --> 00:08:00,459
length of say 15k for example so this

00:07:57,339 --> 00:08:04,269
mess mismatch a fix would simplify the

00:08:00,459 --> 00:08:08,019
the Analects 5 driver and maybe Alex

00:08:04,269 --> 00:08:09,550
would like to comment on that yeah so

00:08:08,019 --> 00:08:12,639
it's thinking about it and basically as

00:08:09,550 --> 00:08:15,339
far as the length field goes so our

00:08:12,639 --> 00:08:17,860
Hardware ignores that we care about the

00:08:15,339 --> 00:08:19,509
checksum so you can have the length but

00:08:17,860 --> 00:08:22,329
just leave the checksum calculated with

00:08:19,509 --> 00:08:24,550
the the partial checksum calculated with

00:08:22,329 --> 00:08:26,589
the full length and that would work for

00:08:24,550 --> 00:08:28,389
us because we have to reset the length

00:08:26,589 --> 00:08:29,439
back and so the easiest way for us to do

00:08:28,389 --> 00:08:34,509
it would be to use the same approach for

00:08:29,439 --> 00:08:36,009
both TCP for non gso partial so

00:08:34,509 --> 00:08:37,990
basically we are we have to do this for

00:08:36,009 --> 00:08:40,240
regular frames as well not just you

00:08:37,990 --> 00:08:43,360
partial frames so if we can use the same

00:08:40,240 --> 00:08:45,250
approach for standard TSO frames as geo

00:08:43,360 --> 00:08:48,220
so partial frames which is we actually

00:08:45,250 --> 00:08:52,000
just take the actual data link of the sk

00:08:48,220 --> 00:08:55,810
buff and cancel that out when we do the

00:08:52,000 --> 00:08:57,279
the checksum update in order to zero out

00:08:55,810 --> 00:09:00,550
the length so if you can leave the

00:08:57,279 --> 00:09:02,200
checksum using the longer length you

00:09:00,550 --> 00:09:03,760
could update the UDP header field with

00:09:02,200 --> 00:09:05,950
the shorter length and then I think it

00:09:03,760 --> 00:09:08,080
works out because you said your Hardware

00:09:05,950 --> 00:09:09,490
ignores the checksum whereas we care

00:09:08,080 --> 00:09:11,770
about that so just it'd be a trade off

00:09:09,490 --> 00:09:13,510
so you just have to move where the

00:09:11,770 --> 00:09:16,209
multiplication takes place I think I

00:09:13,510 --> 00:09:18,339
think this works yes yeah we can try

00:09:16,209 --> 00:09:22,709
this yep and then that should work for

00:09:18,339 --> 00:09:27,850
both drivers sounds reasonable to me and

00:09:22,709 --> 00:09:29,050
okay and so another point to consider is

00:09:27,850 --> 00:09:31,360
what other offloads

00:09:29,050 --> 00:09:34,990
could improve UDP and quick performance

00:09:31,360 --> 00:09:39,490
so we had some discussion about that in

00:09:34,990 --> 00:09:42,339
the quick session but maybe a different

00:09:39,490 --> 00:09:45,990
suggestion at least for UDP segmentation

00:09:42,339 --> 00:09:50,430
I know there's been some work with the

00:09:45,990 --> 00:09:54,810
UDP giro and I know BPF was involved

00:09:50,430 --> 00:09:59,170
what I wanted to to consider here is

00:09:54,810 --> 00:10:01,450
maybe you can add some user API or

00:09:59,170 --> 00:10:03,930
something that's similar to what we have

00:10:01,450 --> 00:10:06,970
with segmentation but on the left side

00:10:03,930 --> 00:10:10,660
maybe the user could set some socket

00:10:06,970 --> 00:10:14,860
option and provide auxiliary data that

00:10:10,660 --> 00:10:17,950
would allow it to receive large UDP

00:10:14,860 --> 00:10:22,000
messages regarding regardless of the

00:10:17,950 --> 00:10:25,390
packets that we used to build those

00:10:22,000 --> 00:10:28,360
messages and if combined with something

00:10:25,390 --> 00:10:30,940
like UDP jello as a result the user

00:10:28,360 --> 00:10:34,029
would get with one call and the same

00:10:30,940 --> 00:10:37,510
buffer that was provided within the

00:10:34,029 --> 00:10:43,750
kernel and it should improve the receive

00:10:37,510 --> 00:10:47,170
side performance of earthquake another

00:10:43,750 --> 00:10:50,370
point to consider is using the

00:10:47,170 --> 00:10:52,350
connection idea for Alice's so

00:10:50,370 --> 00:10:55,560
the quick guys mentioned that it's less

00:10:52,350 --> 00:11:00,390
than 1% so maybe it's not something of

00:10:55,560 --> 00:11:02,910
interest but in any way existing

00:11:00,390 --> 00:11:07,320
hardware today could parse albita

00:11:02,910 --> 00:11:10,070
protocols if configured to do so and the

00:11:07,320 --> 00:11:12,990
connection idea idea could be parsed if

00:11:10,070 --> 00:11:16,050
hardware is configured to do it but we

00:11:12,990 --> 00:11:20,010
don't have an interface today and to ask

00:11:16,050 --> 00:11:24,390
hardware to define those custom headers

00:11:20,010 --> 00:11:26,880
and to parse them to provide RSS so in

00:11:24,390 --> 00:11:33,050
the future this is something to consider

00:11:26,880 --> 00:11:33,050
when when such property would be desired

00:11:34,910 --> 00:11:42,150
and finally the last point is if we're

00:11:39,000 --> 00:11:45,230
interested in having some of quickly as

00:11:42,150 --> 00:11:48,570
part of the kernel specifically this is

00:11:45,230 --> 00:11:51,839
crucial to provide the crypto of load to

00:11:48,570 --> 00:12:00,839
a quick and so if anybody has any

00:11:51,839 --> 00:12:02,580
comments on that I had a question before

00:12:00,839 --> 00:12:04,980
we go into this so can you go back to

00:12:02,580 --> 00:12:08,520
your your results slide the one with the

00:12:04,980 --> 00:12:13,070
graphs so I'm trying to understand in

00:12:08,520 --> 00:12:16,550
the 256 MSS size CP utilization is

00:12:13,070 --> 00:12:20,460
midlane line utilization is middling

00:12:16,550 --> 00:12:23,130
what who's making whom wait what's

00:12:20,460 --> 00:12:25,020
happening on that system because I would

00:12:23,130 --> 00:12:26,700
have expected either you're like maxing

00:12:25,020 --> 00:12:28,710
out the CP or you're maxing or the line

00:12:26,700 --> 00:12:30,960
what you're running a benchmark here

00:12:28,710 --> 00:12:33,420
right like so you can see at the bottom

00:12:30,960 --> 00:12:35,940
the D bar charts represent the CPU

00:12:33,420 --> 00:12:37,920
utilization so which is very long no

00:12:35,940 --> 00:12:40,580
it's it's of the total of the machine

00:12:37,920 --> 00:12:45,390
and it's a single flow so this is the

00:12:40,580 --> 00:12:47,820
one house or actually 20 or 30 percent

00:12:45,390 --> 00:12:51,050
is actually a hundred percent for that

00:12:47,820 --> 00:12:51,050
for that score yeah

00:12:55,210 --> 00:13:06,040
Eric I just have one command about the

00:13:00,300 --> 00:13:08,580
UDP performance on the receive side I'm

00:13:06,040 --> 00:13:12,120
sure there is a use case or quick and

00:13:08,580 --> 00:13:16,780
high-speed role on the data center

00:13:12,120 --> 00:13:18,930
pursues TCP you know we have a proper

00:13:16,780 --> 00:13:22,480
hardware support right now with TCP

00:13:18,930 --> 00:13:25,570
floating on the transmit and receive I'm

00:13:22,480 --> 00:13:29,200
not sure there is a real case for UDP

00:13:25,570 --> 00:13:34,480
quick being offloaded on the receive

00:13:29,200 --> 00:13:37,030
side yeah I wasn't suggesting in

00:13:34,480 --> 00:13:42,990
Hardware float here it's just a user API

00:13:37,030 --> 00:13:46,150
suggestion I was talking about zero and

00:13:42,990 --> 00:13:48,130
I'm not sure we want to add stuff in

00:13:46,150 --> 00:13:51,550
here just for you scales which is

00:13:48,130 --> 00:13:58,090
basically used by benchmark are not real

00:13:51,550 --> 00:14:00,180
traffic and the data center okay thank

00:13:58,090 --> 00:14:00,180
you

00:14:00,630 --> 00:14:08,020
so in your data strikes so you said it's

00:14:04,270 --> 00:14:10,570
single thread test so my question is

00:14:08,020 --> 00:14:14,490
have you tested either performance base

00:14:10,570 --> 00:14:18,400
multiple threats and no we just have

00:14:14,490 --> 00:14:24,310
discussed but say it should be similarly

00:14:18,400 --> 00:14:28,770
scalable as TCP also so I also because

00:14:24,310 --> 00:14:34,630
you may experience testing with multiple

00:14:28,770 --> 00:14:38,920
segmented or UDP so it tends to be high

00:14:34,630 --> 00:14:41,940
number of packet loss so if we use a mod

00:14:38,920 --> 00:14:41,940
for threats

00:14:43,220 --> 00:14:49,500
yeah okay then we should probably do a

00:14:47,339 --> 00:14:58,320
more for evaluation in the future

00:14:49,500 --> 00:15:00,690
yes so I did check the ERC and so I PID

00:14:58,320 --> 00:15:03,480
should only be used in fragmentation so

00:15:00,690 --> 00:15:05,220
it probably should not be need me to

00:15:03,480 --> 00:15:08,310
even be mentioned here in ipv6 other

00:15:05,220 --> 00:15:09,510
wouldn't know hold on so I like actually

00:15:08,310 --> 00:15:11,460
explained it real quick the reason why

00:15:09,510 --> 00:15:13,500
the IP ID needs to increment is because

00:15:11,460 --> 00:15:14,910
if there's a risk of middleboxes say for

00:15:13,500 --> 00:15:18,360
example you have a protocol that doesn't

00:15:14,910 --> 00:15:20,730
set the DF bit and a middle box decides

00:15:18,360 --> 00:15:22,589
to fragment the frame then you run into

00:15:20,730 --> 00:15:26,250
the problem of collisions in terms of IP

00:15:22,589 --> 00:15:29,070
IDs well but this these are just UDP

00:15:26,250 --> 00:15:31,200
packets in the end right so for are we

00:15:29,070 --> 00:15:33,690
always fighting IP ID in all UDP packets

00:15:31,200 --> 00:15:38,880
now what for all ipv4 yes

00:15:33,690 --> 00:15:40,290
okay so then set it for ipv4 yeah it's

00:15:38,880 --> 00:15:43,230
kind of unfortunate that we went down

00:15:40,290 --> 00:15:48,200
that route so can you go to the slide

00:15:43,230 --> 00:15:48,200
where you had the options or the futures

00:15:50,180 --> 00:15:57,690
yeah that one so I think the last

00:15:54,930 --> 00:16:00,140
statement in general we could parallel

00:15:57,690 --> 00:16:02,490
out parallel that to anything that

00:16:00,140 --> 00:16:05,070
hopefully gets into the kernel to

00:16:02,490 --> 00:16:06,690
support quick would be the candidate to

00:16:05,070 --> 00:16:11,910
get into the driver so the path would be

00:16:06,690 --> 00:16:13,170
user space kernel driver device so if

00:16:11,910 --> 00:16:14,700
you stick with that path and once we

00:16:13,170 --> 00:16:17,670
start accelerating so you didn't end up

00:16:14,700 --> 00:16:19,410
accelerating to Euro for quick somehow

00:16:17,670 --> 00:16:23,040
into the kernel and then that could

00:16:19,410 --> 00:16:24,480
possibly become segue into putting it to

00:16:23,040 --> 00:16:26,820
the device as long as you maintain that

00:16:24,480 --> 00:16:28,589
path and then the accelerations kind of

00:16:26,820 --> 00:16:30,990
can be different levels and you could

00:16:28,589 --> 00:16:32,580
have an evolution towards maybe you

00:16:30,990 --> 00:16:35,100
don't need LRO today but maybe someday

00:16:32,580 --> 00:16:41,279
even want it so that that's I think that

00:16:35,100 --> 00:16:43,380
path would be good so do I understand

00:16:41,279 --> 00:16:48,390
this correctly that these all these

00:16:43,380 --> 00:16:50,550
segments will hit us consecutive packets

00:16:48,390 --> 00:16:51,959
going out of the NIC there's no

00:16:50,550 --> 00:16:53,120
possibility to paste them or anything

00:16:51,959 --> 00:16:56,200
after that

00:16:53,120 --> 00:16:57,910
if pacing is a floated to hardwood and

00:16:56,200 --> 00:17:04,270
it could definitely be pasted otherwise

00:16:57,910 --> 00:17:08,380
no yeah okay how strict is the size same

00:17:04,270 --> 00:17:10,630
size things for Ella so can you work

00:17:08,380 --> 00:17:12,550
around it so to say by I mean writing

00:17:10,630 --> 00:17:16,120
buffers of the same size but not

00:17:12,550 --> 00:17:17,020
actually filling level data I'm not sure

00:17:16,120 --> 00:17:23,199
I understand the question

00:17:17,020 --> 00:17:25,510
so different from I mean if you're

00:17:23,199 --> 00:17:28,540
generating quick packets they are not

00:17:25,510 --> 00:17:32,920
necessarily as as common our fixed size

00:17:28,540 --> 00:17:37,810
does for other usages so well you can

00:17:32,920 --> 00:17:41,310
add padding yeah yeah I know it's it's

00:17:37,810 --> 00:17:44,140
not supported with the hard way sorry

00:17:41,310 --> 00:17:46,540
Susan yeah I'm Agnes I was gonna say but

00:17:44,140 --> 00:17:48,760
um in practice especially if you can get

00:17:46,540 --> 00:17:50,290
pacing offload working or if you have a

00:17:48,760 --> 00:17:51,370
very high bandwidth connection it's

00:17:50,290 --> 00:17:53,710
pretty easy to make sure all your

00:17:51,370 --> 00:17:55,510
packets are full-sized so we we have a

00:17:53,710 --> 00:17:58,150
bunch of code that is about the land

00:17:55,510 --> 00:18:06,350
that uses the new API and it was pretty

00:17:58,150 --> 00:18:08,380
painless overall okay thank you

00:18:06,350 --> 00:18:08,380
Oh

00:18:09,760 --> 00:18:13,300
[Music]

00:18:10,360 --> 00:18:13,300

YouTube URL: https://www.youtube.com/watch?v=XveUxszErTY


