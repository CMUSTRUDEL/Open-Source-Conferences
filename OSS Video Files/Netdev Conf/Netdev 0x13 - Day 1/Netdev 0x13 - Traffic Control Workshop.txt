Title: Netdev 0x13 - Traffic Control Workshop
Publication date: 2019-05-20
Playlist: Netdev 0x13 - Day 1
Description: 
	Jamal Hadi Salim chairs the 0x13 TC workshop.

Agenda:
+Vlad - Unlocking TC rules update API: challenges and lessons learned
+Amritha/Anjali -  Intel updates and future plans
+Andy - Stats offload transfers
+Simon -  Offloading TC Rules on OvS Internal Ports
+Lucas  - tdc updates
+Roman  â€“ Quota enforcement

More info:
https://www.netdevconf.org/0x13/session.html?workshop-Traffic-Control
Captions: 
	00:00:00,030 --> 00:00:06,330
okay just I just want to talk about the

00:00:03,030 --> 00:00:07,919
agenda then you can start next slide

00:00:06,330 --> 00:00:08,790
just press the great big green button

00:00:07,919 --> 00:00:12,540
yeah thanks

00:00:08,790 --> 00:00:14,780
okay we'll get started please close the

00:00:12,540 --> 00:00:14,780
doors

00:00:16,520 --> 00:00:28,529
Hey okay we have a very tight agenda

00:00:25,619 --> 00:00:30,869
there's almost five minutes left at the

00:00:28,529 --> 00:00:33,180
end but we'll we'll wing it like the

00:00:30,869 --> 00:00:41,250
netlink guys right we'll take the break

00:00:33,180 --> 00:00:43,920
as well okay so the agenda is on the

00:00:41,250 --> 00:00:45,960
screen you can see that Vlad is going to

00:00:43,920 --> 00:00:48,719
talk about the unlocking of the TC rules

00:00:45,960 --> 00:00:51,090
a lot of effort has been putting in he

00:00:48,719 --> 00:00:53,699
presented in the last workshop and it's

00:00:51,090 --> 00:00:56,969
gonna give us an update where he's at

00:00:53,699 --> 00:01:02,489
there's I'm Rita and I think you got my

00:00:56,969 --> 00:01:04,309
old slides here I'm I'm with her and I'm

00:01:02,489 --> 00:01:09,150
charlie who's here

00:01:04,309 --> 00:01:15,780
okay we'll present on updates from Intel

00:01:09,150 --> 00:01:19,950
and II made it was I mean will present

00:01:15,780 --> 00:01:22,290
on discussion of PCI register stats

00:01:19,950 --> 00:01:28,040
trans transfers from the hardware to the

00:01:22,290 --> 00:01:30,420
kernel Simon will talk about the

00:01:28,040 --> 00:01:35,640
offloading TC roles and obvious internal

00:01:30,420 --> 00:01:37,890
parts and then Lucas I hope is here 10

00:01:35,640 --> 00:01:43,170
minutes on TDC and then five minutes for

00:01:37,890 --> 00:01:45,649
ramen okay can you put it on next slide

00:01:43,170 --> 00:01:45,649
set please

00:01:52,020 --> 00:01:58,200
yes in various tones yeah can you tell

00:01:55,579 --> 00:01:59,299
cuz i just noticed my agendas the old

00:01:58,200 --> 00:02:01,770
one yeah

00:01:59,299 --> 00:02:06,950
is this the newest one you can tell it

00:02:01,770 --> 00:02:06,950
is the latest ones

00:02:09,789 --> 00:02:13,470
hello ladies okay good

00:02:21,250 --> 00:02:25,970
hi I'm Vlad from Mellanox and today I

00:02:24,620 --> 00:02:28,150
would like to give you some update about

00:02:25,970 --> 00:02:31,640
efforts that I've been working on for

00:02:28,150 --> 00:02:34,640
more than a year now it's kinda back and

00:02:31,640 --> 00:02:37,340
forth insults about challenges and

00:02:34,640 --> 00:02:39,709
lessons that I've learned just a world

00:02:37,340 --> 00:02:42,200
of onion it might not be interesting for

00:02:39,709 --> 00:02:45,890
experienced people here to already

00:02:42,200 --> 00:02:49,010
upstream bunch of such patch stats but

00:02:45,890 --> 00:02:51,500
it would have been quite good for me to

00:02:49,010 --> 00:02:55,459
know something like this a year ago when

00:02:51,500 --> 00:02:57,140
I started so first of all I would like

00:02:55,459 --> 00:02:57,739
to give you a recap of what I've been

00:02:57,140 --> 00:03:00,620
doing

00:02:57,739 --> 00:03:03,610
I already talked about some previous TC

00:03:00,620 --> 00:03:07,510
workshop but just a quick reminder it's

00:03:03,610 --> 00:03:12,650
so today TC is using our channel

00:03:07,510 --> 00:03:14,300
organization which means also a rule

00:03:12,650 --> 00:03:16,850
update infrastructure delete and in

00:03:14,300 --> 00:03:18,950
certain even dumped in information about

00:03:16,850 --> 00:03:22,880
rules every synchronized with one single

00:03:18,950 --> 00:03:25,070
lock and even unrelated stuff like for

00:03:22,880 --> 00:03:27,500
example I didn't change in AP on your

00:03:25,070 --> 00:03:30,910
interface every sunette link is using

00:03:27,500 --> 00:03:36,320
this lock so it's quite a big problem

00:03:30,910 --> 00:03:37,820
especially especially if you want to do

00:03:36,320 --> 00:03:39,830
this in parallel because what I'm

00:03:37,820 --> 00:03:41,630
working on my company is a SAP

00:03:39,830 --> 00:03:44,510
accelerated switch non-party processing

00:03:41,630 --> 00:03:47,329
so we are using TC as a back-end for our

00:03:44,510 --> 00:03:50,120
switching for embedded switch in our

00:03:47,329 --> 00:03:53,570
Nick on kinetics Nick so it's a big

00:03:50,120 --> 00:03:56,329
problem and EPS analog can be held for

00:03:53,570 --> 00:03:58,430
seconds for some operations for example

00:03:56,329 --> 00:03:59,090
you delete the qgs with million rules on

00:03:58,430 --> 00:04:01,489
it

00:03:59,090 --> 00:04:03,860
it will just take the whole block for a

00:04:01,489 --> 00:04:06,860
second even you cannot do anything so

00:04:03,860 --> 00:04:09,170
it's very problematic and my goal is to

00:04:06,860 --> 00:04:12,590
do a lot of dates in these rules in

00:04:09,170 --> 00:04:14,720
parallel and making maybe this part of

00:04:12,590 --> 00:04:15,230
TC infrastructure completely independent

00:04:14,720 --> 00:04:19,459
from our

00:04:15,230 --> 00:04:21,620
look so I started with designs that

00:04:19,459 --> 00:04:24,889
implemented reference count in FCU

00:04:21,620 --> 00:04:27,889
fine-grained lock-in on all layers which

00:04:24,889 --> 00:04:31,340
is basically classified API action API

00:04:27,889 --> 00:04:33,380
and specific classifiers that we use in

00:04:31,340 --> 00:04:36,770
our project is flour classifier I think

00:04:33,380 --> 00:04:38,210
a lot of people are using it and it is

00:04:36,770 --> 00:04:40,550
specifically sapphire which I would like

00:04:38,210 --> 00:04:46,160
to unlock in a low parallel execution on

00:04:40,550 --> 00:04:49,520
its low on its low pass so the status is

00:04:46,160 --> 00:04:51,949
that I have six different patch sets

00:04:49,520 --> 00:04:54,110
because feature is big it's like goes

00:04:51,949 --> 00:04:57,139
200 patches can be submitted as one

00:04:54,110 --> 00:05:00,199
patch set so I split it in six patch set

00:04:57,139 --> 00:05:03,380
and four of them already up street one

00:05:00,199 --> 00:05:05,720
is only review in upstream and last one

00:05:03,380 --> 00:05:10,460
is pen genus I hope to obscure meat for

00:05:05,720 --> 00:05:13,039
next kernel okay so challenges and

00:05:10,460 --> 00:05:14,990
lesson Lords I will start with very

00:05:13,039 --> 00:05:17,720
specific challenges that applicable to

00:05:14,990 --> 00:05:20,570
GC which I faced my development and I

00:05:17,720 --> 00:05:23,060
will end with generic like suggestions

00:05:20,570 --> 00:05:25,070
for you how to make your life easier if

00:05:23,060 --> 00:05:29,930
you want to upstream some big changes to

00:05:25,070 --> 00:05:33,110
GC so first of all it's something very

00:05:29,930 --> 00:05:35,120
specific so since I'm doing working for

00:05:33,110 --> 00:05:36,919
me it's always a question of design

00:05:35,120 --> 00:05:39,139
whether to use something which

00:05:36,919 --> 00:05:41,630
lightweight like spinlock and defer

00:05:39,139 --> 00:05:45,950
everything to a synchronous context or

00:05:41,630 --> 00:05:48,349
use heavy looking like mutex so we all

00:05:45,950 --> 00:05:50,990
know that a synchronous is fast you can

00:05:48,349 --> 00:05:53,389
just do something very minimal and make

00:05:50,990 --> 00:05:55,760
work you do the work for you it's very

00:05:53,389 --> 00:05:57,479
good for user space for user space

00:05:55,760 --> 00:05:59,930
switches

00:05:57,479 --> 00:06:02,340
and with this you can also use pin locks

00:05:59,930 --> 00:06:04,229
because you don't need to sleep you just

00:06:02,340 --> 00:06:07,139
schedule something to be the key doesn't

00:06:04,229 --> 00:06:08,490
work you but it's for me and since for

00:06:07,139 --> 00:06:11,099
other people is quite hard to reason

00:06:08,490 --> 00:06:13,529
about because as soon you start doing it

00:06:11,099 --> 00:06:16,409
Kazan used after three bugs are coming

00:06:13,529 --> 00:06:18,689
for you because you play something small

00:06:16,409 --> 00:06:20,430
in work you but then it has pointer to

00:06:18,689 --> 00:06:24,029
something else which means allocate it

00:06:20,430 --> 00:06:26,400
in the mean time and then you end up

00:06:24,029 --> 00:06:28,710
sending multiple versions of your patch

00:06:26,400 --> 00:06:31,710
set and then multiple fixes for it

00:06:28,710 --> 00:06:34,620
afterwards so another approach is

00:06:31,710 --> 00:06:36,509
synchronous you just take a mutex or

00:06:34,620 --> 00:06:38,939
readwrite semaphore and you lock

00:06:36,509 --> 00:06:42,389
everything you wait for your operation

00:06:38,939 --> 00:06:44,279
to complete which is considered heavy at

00:06:42,389 --> 00:06:47,339
least in my mind we will talk about

00:06:44,279 --> 00:06:49,740
later but generally how programmers

00:06:47,339 --> 00:06:51,360
approach this but it's very easy to

00:06:49,740 --> 00:06:54,270
reason about you take the lock you'd

00:06:51,360 --> 00:06:55,979
everything in it you release the lock so

00:06:54,270 --> 00:06:58,699
what I learned is that you probably

00:06:55,979 --> 00:07:00,620
should not do a synchronous

00:06:58,699 --> 00:07:03,389
implementation a synchronous design

00:07:00,620 --> 00:07:05,339
unless you you really know you need it

00:07:03,389 --> 00:07:08,550
so you need to have some results to

00:07:05,339 --> 00:07:11,550
justify it so for my patch that I did

00:07:08,550 --> 00:07:13,620
one implementation which I did like a

00:07:11,550 --> 00:07:15,749
factory and I moved every single second

00:07:13,620 --> 00:07:17,879
a synchronous context and then my

00:07:15,749 --> 00:07:20,310
antenna just asked me why are we doing

00:07:17,879 --> 00:07:24,180
this why do I need it I said because

00:07:20,310 --> 00:07:26,219
okay it's mutex are heavy I just spin

00:07:24,180 --> 00:07:30,629
like a synchronous it's so nice and fast

00:07:26,219 --> 00:07:32,430
he said but are you sure it's fast so I

00:07:30,629 --> 00:07:34,529
had to do a proof of concept

00:07:32,430 --> 00:07:37,709
implementation with mutex and really

00:07:34,529 --> 00:07:39,959
compare the performance and I can tell

00:07:37,709 --> 00:07:41,430
you that last point and the student

00:07:39,959 --> 00:07:43,469
should not be too fancy or clever

00:07:41,430 --> 00:07:46,199
because you either need to prove that

00:07:43,469 --> 00:07:51,449
you really need it or don't do this at

00:07:46,199 --> 00:07:53,789
all so just small small discussion about

00:07:51,449 --> 00:07:55,589
in lock versus mutex so I did prove con

00:07:53,789 --> 00:08:00,180
simplement ation and it turned out that

00:07:55,589 --> 00:08:04,830
mutex is not as slow as I was expecting

00:08:00,180 --> 00:08:06,269
it to be so in its it's very specific to

00:08:04,830 --> 00:08:08,699
Jesus so it's not like general

00:08:06,269 --> 00:08:10,889
performance evaluation of musics but if

00:08:08,699 --> 00:08:13,319
you want to lock something in TCS emojis

00:08:10,889 --> 00:08:16,289
with mutex that's the numbers you can

00:08:13,319 --> 00:08:17,939
expect so without contention just an

00:08:16,289 --> 00:08:19,979
overhead of heaven mutex is very small

00:08:17,939 --> 00:08:21,599
one to three percent we are talking

00:08:19,979 --> 00:08:23,339
about slope as here right because three

00:08:21,599 --> 00:08:25,740
percent for fast pass it would be huge

00:08:23,339 --> 00:08:28,589
but for slow pass it's probably isn't

00:08:25,740 --> 00:08:31,019
worth complicating your code for so and

00:08:28,589 --> 00:08:33,209
in very congested case it's I

00:08:31,019 --> 00:08:36,240
intentionally tried to make it as slow

00:08:33,209 --> 00:08:38,219
as possible so 70 percent is as much as

00:08:36,240 --> 00:08:39,959
you will go if you like do something

00:08:38,219 --> 00:08:42,360
very stupid while holding it basically

00:08:39,959 --> 00:08:44,850
here it's linear search I had a bunch of

00:08:42,360 --> 00:08:46,740
chains to TC and then I have two linear

00:08:44,850 --> 00:08:47,370
search through them while holding the

00:08:46,740 --> 00:08:51,180
mutex

00:08:47,370 --> 00:08:54,540
so these numbers are quite good for

00:08:51,180 --> 00:08:56,730
mutex actually's so my advice is using

00:08:54,540 --> 00:08:58,410
mutants by default unless your design by

00:08:56,730 --> 00:09:00,839
design you need it like if you need lock

00:08:58,410 --> 00:09:02,850
to take lock on some atomic context like

00:09:00,839 --> 00:09:05,939
Fast Pass you have to use pin lock but

00:09:02,850 --> 00:09:08,060
don't just use it as you know premature

00:09:05,939 --> 00:09:08,060
optimization

00:09:12,640 --> 00:09:17,740
okay so I did some profiling and it's

00:09:15,820 --> 00:09:20,560
just implementation details that I did

00:09:17,740 --> 00:09:24,610
not know about mutex and Linux kernel it

00:09:20,560 --> 00:09:27,310
actually what it does is it does busy

00:09:24,610 --> 00:09:29,829
white babies a loop in same as spin lock

00:09:27,310 --> 00:09:32,890
if current holder of the mutex is

00:09:29,829 --> 00:09:35,110
executing on sip on CPU so if someone

00:09:32,890 --> 00:09:36,850
took the mutex and sleep it will also

00:09:35,110 --> 00:09:39,459
sleep but if someone who has the mutex

00:09:36,850 --> 00:09:43,029
is executing on CPU it will try to busy

00:09:39,459 --> 00:09:46,089
wait so that's why it's like relatively

00:09:43,029 --> 00:09:49,600
fast because you actually don't need to

00:09:46,089 --> 00:09:52,839
go to scheduler and you know preempt ear

00:09:49,600 --> 00:09:55,720
threads while waiting from YouTube last

00:09:52,839 --> 00:09:59,769
two lines you can see that it's busy

00:09:55,720 --> 00:10:02,980
evasion is Q walk it's optimistic spin

00:09:59,769 --> 00:10:04,870
it's kind of internal at another

00:10:02,980 --> 00:10:09,160
internal optimization which basically

00:10:04,870 --> 00:10:14,140
prevents multiple multiple tasks from

00:10:09,160 --> 00:10:16,540
spinning on exactly the same on same

00:10:14,140 --> 00:10:21,070
lock so it's kinda accused them but only

00:10:16,540 --> 00:10:24,370
one task is busy waiting on specific

00:10:21,070 --> 00:10:26,670
waiter so anyway it's quite fast so

00:10:24,370 --> 00:10:29,410
unless you have reason to use pin lock

00:10:26,670 --> 00:10:34,810
unless you have reason to use spin lock

00:10:29,410 --> 00:10:36,940
don't use it so it's more generic

00:10:34,810 --> 00:10:39,880
suggestion implement test cases because

00:10:36,940 --> 00:10:42,640
it seems obvious but a lot of people

00:10:39,880 --> 00:10:45,279
don't submit the changes with the

00:10:42,640 --> 00:10:47,740
company and test cases and some don't

00:10:45,279 --> 00:10:50,440
even run the tests for even existing

00:10:47,740 --> 00:10:52,360
tests or their changes at all so

00:10:50,440 --> 00:10:55,600
and make sure you past existing tests

00:10:52,360 --> 00:10:58,000
and if you're doing something risky like

00:10:55,600 --> 00:11:00,310
locking make sure you create tests for

00:10:58,000 --> 00:11:02,769
your stuff because this happened to me I

00:11:00,310 --> 00:11:04,750
submitted a change then I had to you

00:11:02,769 --> 00:11:06,279
know go and implement tests for it

00:11:04,750 --> 00:11:08,949
afterwards because I had my private

00:11:06,279 --> 00:11:11,740
private tests in frameworks that are

00:11:08,949 --> 00:11:13,540
used in our company but it's not good

00:11:11,740 --> 00:11:15,970
for upstream because first of all my

00:11:13,540 --> 00:11:18,639
entire cannot verify how I test if it's

00:11:15,970 --> 00:11:21,759
my private tests and another major

00:11:18,639 --> 00:11:23,860
reason is that another people who might

00:11:21,759 --> 00:11:25,930
be doing changes afterwards to your code

00:11:23,860 --> 00:11:30,360
they cannot verify that they don't break

00:11:25,930 --> 00:11:33,910
your your locking and parallelism so

00:11:30,360 --> 00:11:36,100
submit changes and it's I don't know if

00:11:33,910 --> 00:11:39,449
all of you know about this but it's not

00:11:36,100 --> 00:11:42,610
very easy to do we have TTC based

00:11:39,449 --> 00:11:45,639
basically it's Python based testing

00:11:42,610 --> 00:11:47,470
framework but you don't need to know or

00:11:45,639 --> 00:11:49,600
write any Python probably unless you

00:11:47,470 --> 00:11:52,569
implement some plugins for this for this

00:11:49,600 --> 00:11:54,279
framework you just insert like

00:11:52,569 --> 00:11:56,199
initialization commands and test

00:11:54,279 --> 00:11:59,110
commands and verification commands in

00:11:56,199 --> 00:12:01,959
JSON file and it will run everything for

00:11:59,110 --> 00:12:04,660
you to generate your test IDE to do

00:12:01,959 --> 00:12:08,470
everything so if you are not using this

00:12:04,660 --> 00:12:10,600
go and use it it's quite nice and if you

00:12:08,470 --> 00:12:14,220
need something on top just send the

00:12:10,600 --> 00:12:14,220
patches implementing plugins

00:12:15,470 --> 00:12:20,630
yeah and also not everyone is doing this

00:12:18,620 --> 00:12:24,320
please run tests viscous on and log

00:12:20,630 --> 00:12:26,450
debugging because test your code might

00:12:24,320 --> 00:12:28,850
not necessary crush the kernel but you

00:12:26,450 --> 00:12:32,450
may have some you might have some

00:12:28,850 --> 00:12:34,790
covered quite hard to find problems like

00:12:32,450 --> 00:12:37,070
use after free and you will probably not

00:12:34,790 --> 00:12:39,680
reproduce them on single test run unless

00:12:37,070 --> 00:12:42,770
you have cousin and the other kernel

00:12:39,680 --> 00:12:45,010
test infrastructure enabled so please

00:12:42,770 --> 00:12:45,010
use it

00:12:45,430 --> 00:12:50,780
another point is write descriptive cover

00:12:48,590 --> 00:12:53,480
letters so again I had this problem when

00:12:50,780 --> 00:12:56,180
I send my nice patch set which I really

00:12:53,480 --> 00:13:00,200
like but then we end up discussing my

00:12:56,180 --> 00:13:02,300
cover letter instead of my code and it's

00:13:00,200 --> 00:13:05,570
quite an foreign and unfortunate and

00:13:02,300 --> 00:13:07,580
discouraging for you but basically you

00:13:05,570 --> 00:13:09,980
have to approach your cover letter from

00:13:07,580 --> 00:13:11,630
point of view of person who did not read

00:13:09,980 --> 00:13:14,540
the code yet because when writing cover

00:13:11,630 --> 00:13:16,340
letter you me I already wrote my code I

00:13:14,540 --> 00:13:18,710
end up this cover letter it's last thing

00:13:16,340 --> 00:13:21,020
I do maybe I should start with cover

00:13:18,710 --> 00:13:23,210
letter but this colossal point so anyway

00:13:21,020 --> 00:13:25,490
when you already wrote the code it's

00:13:23,210 --> 00:13:28,370
quite obvious for you you write very dry

00:13:25,490 --> 00:13:30,380
cover letter but then people in upstream

00:13:28,370 --> 00:13:32,180
they start with cover letter and if they

00:13:30,380 --> 00:13:34,310
don't understand it they will just not

00:13:32,180 --> 00:13:36,050
review your code or we will end up

00:13:34,310 --> 00:13:38,420
having a week of discussion of your

00:13:36,050 --> 00:13:40,430
cover letter because it's confusing for

00:13:38,420 --> 00:13:43,400
them they will be asking questions so

00:13:40,430 --> 00:13:46,130
just try to proofread your cover letter

00:13:43,400 --> 00:13:49,780
before sentence it will save you a lot

00:13:46,130 --> 00:13:49,780
of time and a lot of frustration

00:13:51,319 --> 00:13:58,980
so submit early if with big patches if

00:13:56,129 --> 00:14:01,759
it's not one bar change it's better for

00:13:58,980 --> 00:14:05,720
you to submit early in the cycle because

00:14:01,759 --> 00:14:08,339
what I had instantly don't count

00:14:05,720 --> 00:14:12,509
David Miller to take your patch set when

00:14:08,339 --> 00:14:14,249
in 37 you know last Friday it it would

00:14:12,509 --> 00:14:17,610
probably not happen and you will have to

00:14:14,249 --> 00:14:19,740
resubmit because the way it goes for

00:14:17,610 --> 00:14:22,740
like patch sets with cover letter people

00:14:19,740 --> 00:14:24,749
who reviewing them they usually you know

00:14:22,740 --> 00:14:27,629
have other more important stuff to do

00:14:24,749 --> 00:14:29,759
besides removing my patch set so they

00:14:27,629 --> 00:14:31,949
just delay it until maybe they have some

00:14:29,759 --> 00:14:33,839
time Friday evening and then if they

00:14:31,949 --> 00:14:36,959
don't say delayed till next Friday

00:14:33,839 --> 00:14:38,699
evening so expected to take one two

00:14:36,959 --> 00:14:42,660
weeks just to get feedback for your

00:14:38,699 --> 00:14:44,939
initial submission you know so just you

00:14:42,660 --> 00:14:50,910
have to really plan for it if you want

00:14:44,939 --> 00:14:52,920
to get it into the current release also

00:14:50,910 --> 00:14:55,199
it's better to send early because and

00:14:52,920 --> 00:14:56,970
other people in will end up merging with

00:14:55,199 --> 00:14:58,679
your code otherwise you would have to

00:14:56,970 --> 00:15:01,290
merge with a code because if your

00:14:58,679 --> 00:15:03,540
patches do not apply clearly it's your

00:15:01,290 --> 00:15:05,339
job to to merge them to revise them on

00:15:03,540 --> 00:15:08,189
top of current not next and sometimes

00:15:05,339 --> 00:15:11,389
it's not just rebase I had to do several

00:15:08,189 --> 00:15:15,209
redesigns because you know for example

00:15:11,389 --> 00:15:18,300
user space chain API it completely

00:15:15,209 --> 00:15:22,050
changed the classifier API design for me

00:15:18,300 --> 00:15:23,519
and locking so it's also it's very time

00:15:22,050 --> 00:15:25,920
consuming you know to change the

00:15:23,519 --> 00:15:28,879
upstream try not to try to make upstream

00:15:25,920 --> 00:15:28,879
chase if it's a meter

00:15:30,100 --> 00:15:36,980
so general approach so in general what I

00:15:33,199 --> 00:15:41,660
did I wrote like 150 patches with our

00:15:36,980 --> 00:15:44,000
driver also because it's it has its own

00:15:41,660 --> 00:15:45,470
rock back and benefits so when you write

00:15:44,000 --> 00:15:47,570
hole solution first

00:15:45,470 --> 00:15:50,329
it cannot be submitted a single patch

00:15:47,570 --> 00:15:52,279
that you cannot send 100 or 150 patches

00:15:50,329 --> 00:15:54,110
because it will not be accepted you have

00:15:52,279 --> 00:15:57,829
to break it down to smaller patch sets

00:15:54,110 --> 00:16:00,589
and nice thing about it is it you can

00:15:57,829 --> 00:16:02,149
validate your whole design because you

00:16:00,589 --> 00:16:03,880
have everything you just run tests and

00:16:02,149 --> 00:16:07,850
you know if it works or it doesn't work

00:16:03,880 --> 00:16:09,350
but besides that whatever ready told you

00:16:07,850 --> 00:16:11,240
about the upkeep cost you have to

00:16:09,350 --> 00:16:13,459
constantly replace and sometimes not

00:16:11,240 --> 00:16:17,899
just the base but refactor and redesign

00:16:13,459 --> 00:16:22,550
to accommodate upstream changes so it's

00:16:17,899 --> 00:16:24,560
a very time-consuming but on the other

00:16:22,550 --> 00:16:27,380
hand if you don't do this you either

00:16:24,560 --> 00:16:29,029
have to be very knowledgeable in the

00:16:27,380 --> 00:16:31,160
part of Colonel you are changing out you

00:16:29,029 --> 00:16:33,860
will get it wrong because I know that I

00:16:31,160 --> 00:16:36,560
give away several of my initial designs

00:16:33,860 --> 00:16:38,660
because when you just look on it from

00:16:36,560 --> 00:16:40,839
high level point of view everything is

00:16:38,660 --> 00:16:43,639
easy ok just add look here just add

00:16:40,839 --> 00:16:45,380
reference counter there but when you

00:16:43,639 --> 00:16:47,690
gone down to this you have to change the

00:16:45,380 --> 00:16:50,480
api's in between different subsystems

00:16:47,690 --> 00:16:52,490
like I had to change classifier API to

00:16:50,480 --> 00:16:55,790
implement and lock classifier because

00:16:52,490 --> 00:16:58,519
current API was not not enough I need to

00:16:55,790 --> 00:17:02,420
do reference reference counting and for

00:16:58,519 --> 00:17:04,250
example it had like gate function but

00:17:02,420 --> 00:17:06,169
didn't have any kind of release function

00:17:04,250 --> 00:17:08,510
so I have to implement it and I have to

00:17:06,169 --> 00:17:11,360
modify the API to really always take the

00:17:08,510 --> 00:17:14,329
reference and release it I mean it's

00:17:11,360 --> 00:17:16,520
very easy it's only few ways to get it

00:17:14,329 --> 00:17:17,089
right and multiple a lot of ways to get

00:17:16,520 --> 00:17:20,000
it wrong

00:17:17,089 --> 00:17:22,130
so that is something you have to design

00:17:20,000 --> 00:17:25,520
for you to decide for yourself because

00:17:22,130 --> 00:17:28,030
it's both approaches has their own

00:17:25,520 --> 00:17:28,030
benefits

00:17:30,059 --> 00:17:35,100
okay questions

00:17:42,950 --> 00:17:46,660
look at this okay Mike

00:17:50,910 --> 00:17:54,810
so there's a lot of or there are

00:17:53,610 --> 00:17:57,060
concerns from time to time about the

00:17:54,810 --> 00:17:59,030
performance of TC in general I mean you

00:17:57,060 --> 00:18:01,500
said that you have a patch under review

00:17:59,030 --> 00:18:02,880
I'll admit I haven't read the change log

00:18:01,500 --> 00:18:07,440
entry for it is there a performance

00:18:02,880 --> 00:18:09,570
boost with TC TC flower so what I'm

00:18:07,440 --> 00:18:12,930
working on is to allow to call it from

00:18:09,570 --> 00:18:16,470
parallel tasks not to improve a single

00:18:12,930 --> 00:18:19,260
single thread at insertion rate so but

00:18:16,470 --> 00:18:21,240
my goal is not to degrade singles that

00:18:19,260 --> 00:18:23,880
result in certain rate because you know

00:18:21,240 --> 00:18:25,740
implemented currently just one single or

00:18:23,880 --> 00:18:28,770
taken it's a very top of the call chain

00:18:25,740 --> 00:18:30,630
and then nothing has to you don't have

00:18:28,770 --> 00:18:32,550
to worry about anything else but with my

00:18:30,630 --> 00:18:35,370
changes its bunch of you know atomic

00:18:32,550 --> 00:18:38,760
reference counters fine-grained locks so

00:18:35,370 --> 00:18:41,280
I don't expect to improve single

00:18:38,760 --> 00:18:43,790
threaded insertion rate at all okay

00:18:41,280 --> 00:18:43,790
thanks

00:18:48,840 --> 00:18:55,080
thank you yeah just a few minutes and I

00:18:53,730 --> 00:19:00,180
think they're trying to connect remote

00:18:55,080 --> 00:19:04,190
speaker but so for the record what's the

00:19:00,180 --> 00:19:09,180
number of single thread right now so

00:19:04,190 --> 00:19:10,730
long cancer militants okay so right now

00:19:09,180 --> 00:19:14,940
there is a bug

00:19:10,730 --> 00:19:19,170
related to per CPU counters usage there

00:19:14,940 --> 00:19:22,290
was like unrelated change by Eric de

00:19:19,170 --> 00:19:24,000
Magette and he just said some alignment

00:19:22,290 --> 00:19:28,290
for one structure and it degraded

00:19:24,000 --> 00:19:31,500
insertion rate from 60 60 65 euros per

00:19:28,290 --> 00:19:34,340
second to 40k rules per second okay so I

00:19:31,500 --> 00:19:36,930
like wrote on my debugging information

00:19:34,340 --> 00:19:38,730
and send it to the CPU counters

00:19:36,930 --> 00:19:42,660
maintainer I think they came up with

00:19:38,730 --> 00:19:44,940
like 12 patches patch set that fixes the

00:19:42,660 --> 00:19:48,180
issue but it's not going through net

00:19:44,940 --> 00:19:51,870
next I think they have their own okay so

00:19:48,180 --> 00:19:54,420
this is this is 65,000 updates of flour

00:19:51,870 --> 00:19:57,030
plus actions will just flower a flower

00:19:54,420 --> 00:19:59,460
perfect it's without driver every food

00:19:57,030 --> 00:20:01,470
is our driver if flour business right

00:19:59,460 --> 00:20:03,450
not all the way to the hard way yeah

00:20:01,470 --> 00:20:05,580
without harder okay and if you have 16

00:20:03,450 --> 00:20:07,590
threads if you have 16 CPUs running in

00:20:05,580 --> 00:20:10,770
parallel you can do sixteen times that

00:20:07,590 --> 00:20:12,210
or without my changes every since the

00:20:10,770 --> 00:20:13,740
niceties of channellock and with my

00:20:12,210 --> 00:20:16,020
changes it depends on whether or not

00:20:13,740 --> 00:20:19,590
you're targeting the same for example

00:20:16,020 --> 00:20:21,630
cue disk chain TP instance really

00:20:19,590 --> 00:20:23,520
depends if not targeting it you are

00:20:21,630 --> 00:20:25,410
basically embarrassed Linda parallel

00:20:23,520 --> 00:20:28,680
racing is parallel but if you are

00:20:25,410 --> 00:20:31,350
targeting them it's okay yet you get

00:20:28,680 --> 00:20:33,510
less benefit okay okay so but but in the

00:20:31,350 --> 00:20:36,180
best case scenario you're not targeting

00:20:33,510 --> 00:20:39,690
them you can get sixteen times for 16

00:20:36,180 --> 00:20:41,940
threads so close to that it's less but I

00:20:39,690 --> 00:20:44,299
think I presented my previous talk I was

00:20:41,940 --> 00:20:48,349
reaching several hundred thousands per

00:20:44,299 --> 00:20:50,299
confronted and threat okay and we're not

00:20:48,349 --> 00:20:53,179
expecting hardware to be a problem in

00:20:50,299 --> 00:20:54,440
the end you won't see once once you

00:20:53,179 --> 00:20:56,389
funnel and you're sending this to the

00:20:54,440 --> 00:20:58,999
hardware it's gonna slow down a little

00:20:56,389 --> 00:21:00,739
bit yes yes but it depends on the

00:20:58,999 --> 00:21:04,249
hardware because a lot of drivers are

00:21:00,739 --> 00:21:07,609
plugged into TC and my goal is to just

00:21:04,249 --> 00:21:09,379
to call the hardware without holding any

00:21:07,609 --> 00:21:11,239
kind of our channel lock on top and then

00:21:09,379 --> 00:21:14,239
it's up to driver implementation of the

00:21:11,239 --> 00:21:14,830
as fast as it can be okay that should be

00:21:14,239 --> 00:21:23,779
interesting

00:21:14,830 --> 00:21:26,209
Thanks okay so our next speaker we're

00:21:23,779 --> 00:21:28,209
gonna try a remote is it doable can it

00:21:26,209 --> 00:21:31,070
be done huh

00:21:28,209 --> 00:21:35,799
there is a speaker there somebody can

00:21:31,070 --> 00:21:35,799
can you give them control of the of the

00:21:38,490 --> 00:21:51,610
yeah you can try she wants to yeah she

00:21:46,210 --> 00:21:53,950
wants to present right yeah you finally

00:21:51,610 --> 00:21:56,440
put the correct agenda years ago to work

00:21:53,950 --> 00:21:58,690
around switches that can't yield us

00:21:56,440 --> 00:22:02,080
identify P addresses in different fuel

00:21:58,690 --> 00:22:04,210
and what does what does it get rid of

00:22:02,080 --> 00:22:07,419
those guys and we could say EB tables

00:22:04,210 --> 00:22:11,840
Iran and then specify some filter

00:22:07,419 --> 00:22:20,100
criteria like America's wrong wrong

00:22:11,840 --> 00:22:20,100
[Laughter]

00:22:24,910 --> 00:22:27,420
20 minutes

00:22:42,410 --> 00:22:45,289
should we just go to the next

00:22:43,940 --> 00:22:49,990
presentation while you are fixing this

00:22:45,289 --> 00:22:49,990
or yeah okay I guess

00:22:52,460 --> 00:23:02,889
Oh I mean she's there she's online right

00:22:55,129 --> 00:23:02,889
now connected to jib to Jupiter

00:23:07,090 --> 00:23:11,920
I say that again

00:23:09,780 --> 00:23:13,480
okay okay so let's do the knee can you

00:23:11,920 --> 00:23:20,920
put the agenda I don't remember the next

00:23:13,480 --> 00:23:22,550
one is I think maybe you and II I could

00:23:20,920 --> 00:23:24,880
be wrong

00:23:22,550 --> 00:23:24,880
yeah

00:23:25,620 --> 00:23:28,190
yeah

00:23:31,720 --> 00:23:38,259
oh he's there I can see it I could I

00:23:35,679 --> 00:23:44,019
could see on the screen yeah are you

00:23:38,259 --> 00:23:45,850
able to put her huh no I mean here I

00:23:44,019 --> 00:23:48,789
have I connected through blue jeans I

00:23:45,850 --> 00:23:51,970
can see can you hear me

00:23:48,789 --> 00:23:58,330
I'm written shake your head if you yes

00:23:51,970 --> 00:24:01,740
she can hear I asked her head but she

00:23:58,330 --> 00:24:01,740
didn't fall for it she went

00:24:14,910 --> 00:24:22,110
it's kidding yes okay maybe we will go

00:24:20,460 --> 00:24:22,590
to the side one can we go to number

00:24:22,110 --> 00:24:24,390
three

00:24:22,590 --> 00:24:28,110
my name is Pablo I meant to follow with

00:24:24,390 --> 00:24:30,180
a summary of updates that happened in

00:24:28,110 --> 00:24:32,930
the let's go to number three and they

00:24:30,180 --> 00:24:32,930
will come back to this

00:24:42,120 --> 00:24:44,809
okay so

00:24:47,360 --> 00:24:56,670
so this is okay

00:24:50,700 --> 00:24:59,160
yeah undie with stats yeah all right all

00:24:56,670 --> 00:25:00,720
right ready yeah you close one eye head

00:24:59,160 --> 00:25:02,430
yeah okay all right

00:25:00,720 --> 00:25:04,920
so Jamal asked me to come up and talk

00:25:02,430 --> 00:25:06,240
for a few minutes hopefully other people

00:25:04,920 --> 00:25:09,600
will fill in the gaps and talk more than

00:25:06,240 --> 00:25:12,930
me about gathering of Hardware stats and

00:25:09,600 --> 00:25:17,460
how it's not efficient kind of as we

00:25:12,930 --> 00:25:20,030
move towards the future now we got no we

00:25:17,460 --> 00:25:20,030
go to here

00:25:20,390 --> 00:25:29,179
she can't see me ok so right now if you

00:25:28,220 --> 00:25:30,650
have a small number of actions in

00:25:29,179 --> 00:25:34,480
hardware or gathering stats is not

00:25:30,650 --> 00:25:38,270
really a big deal but as we scale up to

00:25:34,480 --> 00:25:40,309
millions of rules or actions frequent

00:25:38,270 --> 00:25:41,240
updates not going to be efficient

00:25:40,309 --> 00:25:43,340
anymore

00:25:41,240 --> 00:25:46,250
it's also true for software Jamal can go

00:25:43,340 --> 00:25:48,620
on and on about some of the software

00:25:46,250 --> 00:25:51,830
things that have been done why do we

00:25:48,620 --> 00:25:52,940
care about stats billing apparently

00:25:51,830 --> 00:25:56,840
that's important to people to buy these

00:25:52,940 --> 00:25:59,450
things maintaining SLA is also important

00:25:56,840 --> 00:26:02,000
and network or action troubleshooting I

00:25:59,450 --> 00:26:05,150
know for a fact if you're doing TC rules

00:26:02,000 --> 00:26:06,320
and hardware sometimes it's good to look

00:26:05,150 --> 00:26:07,880
at the hardware and know whether or not

00:26:06,320 --> 00:26:09,110
these things are actually being hit if

00:26:07,880 --> 00:26:12,950
you can't figure out what your packets

00:26:09,110 --> 00:26:18,380
are going so let's go to the next one ok

00:26:12,950 --> 00:26:19,250
all right so here are some of the some

00:26:18,380 --> 00:26:20,630
of the reasons we need to think about

00:26:19,250 --> 00:26:23,210
this or some of the suggestions that

00:26:20,630 --> 00:26:24,620
people have had so again number of

00:26:23,210 --> 00:26:26,299
actions can make pulling all the

00:26:24,620 --> 00:26:29,660
counters in any poll period not very

00:26:26,299 --> 00:26:33,440
scalable one of the one of the 99

00:26:29,660 --> 00:26:35,240
problems so I want this to be

00:26:33,440 --> 00:26:36,890
interactive so anybody has a suggestion

00:26:35,240 --> 00:26:38,390
which I'm sure we have lots of smart

00:26:36,890 --> 00:26:40,100
folks in the crowd who can make a

00:26:38,390 --> 00:26:43,070
suggestion but here's a couple that

00:26:40,100 --> 00:26:44,450
we've come up with and it was was it

00:26:43,070 --> 00:26:46,280
Edward curry from solar flare that also

00:26:44,450 --> 00:26:47,419
had some suggestions Jamal and and

00:26:46,280 --> 00:26:51,559
started some of the discussion on the

00:26:47,419 --> 00:26:53,540
list he's not listening of course it was

00:26:51,559 --> 00:26:55,430
Edward was it Edward from solar I don't

00:26:53,540 --> 00:26:57,470
need a mic I think

00:26:55,430 --> 00:26:59,270
and I tried to invite him but he okay

00:26:57,470 --> 00:27:01,310
get right okay that's fine

00:26:59,270 --> 00:27:04,580
so you kind of started the discussion we

00:27:01,310 --> 00:27:06,290
couldn't be here so I want to maybe

00:27:04,580 --> 00:27:08,210
track some stats up to his SATs with a

00:27:06,290 --> 00:27:10,250
bit fields require some hardware support

00:27:08,210 --> 00:27:13,670
so then you could say oh only these are

00:27:10,250 --> 00:27:15,320
changed query for those you're designing

00:27:13,670 --> 00:27:18,470
hardware right now to be used in the

00:27:15,320 --> 00:27:19,610
next while that would be useful if your

00:27:18,470 --> 00:27:22,570
hardware doesn't support that you're

00:27:19,610 --> 00:27:24,770
probably not going to be a big fan you

00:27:22,570 --> 00:27:28,370
one of the things to consider to the

00:27:24,770 --> 00:27:31,550
speed of network interfaces as well as

00:27:28,370 --> 00:27:33,410
the size of the in your hardware to

00:27:31,550 --> 00:27:34,580
support the counters if it's large

00:27:33,410 --> 00:27:36,110
you're gonna see wrapping through

00:27:34,580 --> 00:27:38,080
polling periods not frequent enough this

00:27:36,110 --> 00:27:40,490
is no kind of issue to think about

00:27:38,080 --> 00:27:42,350
another one is we if we push the stats

00:27:40,490 --> 00:27:46,340
block just a host memory and have it

00:27:42,350 --> 00:27:49,640
updated automatically constantly by

00:27:46,340 --> 00:27:51,800
Hardware you could allow the kernel to

00:27:49,640 --> 00:27:52,730
have a field that's up-to-date but you

00:27:51,800 --> 00:27:53,930
also have to do some synchronization

00:27:52,730 --> 00:27:57,890
there to make sure you're not in the

00:27:53,930 --> 00:28:03,770
middle so that's kind of all I have to

00:27:57,890 --> 00:28:05,960
say surprisingly or not so the idea is

00:28:03,770 --> 00:28:09,230
look if we're gonna pull a million stats

00:28:05,960 --> 00:28:10,940
and you're doing billing you want the

00:28:09,230 --> 00:28:12,650
correct stats okay synchronize at the

00:28:10,940 --> 00:28:14,870
right time

00:28:12,650 --> 00:28:17,870
yeah so unsurprisingly we have some

00:28:14,870 --> 00:28:20,180
experience in this area yes it's a big

00:28:17,870 --> 00:28:22,100
problem so the approach that we've taken

00:28:20,180 --> 00:28:26,140
is kind of the last point there is we do

00:28:22,100 --> 00:28:28,100
it asynchronously but this kind of

00:28:26,140 --> 00:28:31,760
pushes the problem to a different place

00:28:28,100 --> 00:28:33,650
so I actually have problems there so

00:28:31,760 --> 00:28:35,900
there's kind of two approaches I suppose

00:28:33,650 --> 00:28:39,260
you could take one is to push deltas of

00:28:35,900 --> 00:28:42,260
the stats and the other one is to push

00:28:39,260 --> 00:28:43,520
absolute values of the stats and then of

00:28:42,260 --> 00:28:46,900
course you could try and be smart

00:28:43,520 --> 00:28:49,400
no they push the ones that have changed

00:28:46,900 --> 00:28:51,320
so kind of the obvious trade-offs here

00:28:49,400 --> 00:28:52,910
is like if you push your absolute values

00:28:51,320 --> 00:28:55,429
then you increasing the volume of the

00:28:52,910 --> 00:28:59,570
data because because you need a bigger

00:28:55,429 --> 00:29:03,830
value right the downside of doing the

00:28:59,570 --> 00:29:07,900
Delta is you if you miss one you drop

00:29:03,830 --> 00:29:11,320
the information right so

00:29:07,900 --> 00:29:13,240
we do the deltas but we've had to take

00:29:11,320 --> 00:29:14,830
particular care and the driver to make

00:29:13,240 --> 00:29:17,850
sure it's fast enough to keep up with

00:29:14,830 --> 00:29:21,360
these things and this has been an issue

00:29:17,850 --> 00:29:24,460
we think we're on top of it now but did

00:29:21,360 --> 00:29:26,320
so the bed but on the hop we don't see

00:29:24,460 --> 00:29:29,440
many problems on the whole side in terms

00:29:26,320 --> 00:29:31,179
of collecting the stats because of it we

00:29:29,440 --> 00:29:34,390
have them stored in memory as you're

00:29:31,179 --> 00:29:36,730
suggesting obviously there's inherent

00:29:34,390 --> 00:29:41,980
issues as per the previous presentation

00:29:36,730 --> 00:29:43,870
with collecting stuff so in general I

00:29:41,980 --> 00:29:45,610
mean I don't I just sharing a little bit

00:29:43,870 --> 00:29:48,340
experience about I agree this is a big

00:29:45,610 --> 00:29:49,659
problem with your scalability and I

00:29:48,340 --> 00:29:53,080
guess my main point is it's not just

00:29:49,659 --> 00:29:54,730
between the host and the kernel and user

00:29:53,080 --> 00:29:56,980
space it's also between the host and the

00:29:54,730 --> 00:29:58,960
cloud yeah and I think that's that's my

00:29:56,980 --> 00:30:00,850
I think the the primary interest for me

00:29:58,960 --> 00:30:04,360
is it's not just I think you're right

00:30:00,850 --> 00:30:05,590
it's the hardware and and it's all of

00:30:04,360 --> 00:30:06,850
them but I think that hardware in the

00:30:05,590 --> 00:30:08,830
kernel in particular I think it's the

00:30:06,850 --> 00:30:11,700
most so it's challenging as you look at

00:30:08,830 --> 00:30:15,190
a hardware if you have a million flows

00:30:11,700 --> 00:30:16,480
and even just once that will flow how

00:30:15,190 --> 00:30:19,659
often do you want this information

00:30:16,480 --> 00:30:22,360
synchronized in no telco guys wanted

00:30:19,659 --> 00:30:24,070
every they bill every 66 seconds for

00:30:22,360 --> 00:30:26,020
example yeah yeah so they want to be

00:30:24,070 --> 00:30:28,050
accurate within that range right so

00:30:26,020 --> 00:30:30,390
these numbers it gets

00:30:28,050 --> 00:30:34,830
it's a lot of information for the host I

00:30:30,390 --> 00:30:36,240
have to process so what is the

00:30:34,830 --> 00:30:39,510
bottleneck from there from the heresy

00:30:36,240 --> 00:30:41,760
the PCI bus or I think that's one thing

00:30:39,510 --> 00:30:43,860
you say in our experience it's it's not

00:30:41,760 --> 00:30:46,620
the PCI bus I mean so you should be able

00:30:43,860 --> 00:30:49,170
to I assume you well in our case we're

00:30:46,620 --> 00:30:51,150
using a message based passing and you

00:30:49,170 --> 00:30:54,540
can pack multiple stats into one message

00:30:51,150 --> 00:30:56,910
of course but it's it's more the host

00:30:54,540 --> 00:30:59,250
has to to store these efficiently that's

00:30:56,910 --> 00:31:04,320
very efficiently being the key word

00:30:59,250 --> 00:31:06,480
otherwise it can't keep up so the the

00:31:04,320 --> 00:31:10,530
patch we put in I wonder if that's

00:31:06,480 --> 00:31:14,910
doable in hardware as well the time stop

00:31:10,530 --> 00:31:16,530
this was back in 2018 2017 maybe we

00:31:14,910 --> 00:31:18,809
dumped actions and actions of a

00:31:16,530 --> 00:31:21,090
timestamp when it was last used so you

00:31:18,809 --> 00:31:23,400
can actually send a message I get and

00:31:21,090 --> 00:31:26,910
put a filter to say give it to me if

00:31:23,400 --> 00:31:30,500
it's been updated only put a range of

00:31:26,910 --> 00:31:30,500
time I'm familiar with that word yeah

00:31:31,220 --> 00:31:35,520
and it's always different but you do

00:31:34,410 --> 00:31:38,630
actually want to flush these out of the

00:31:35,520 --> 00:31:38,630
hard way because yeah

00:31:39,380 --> 00:31:45,420
problems but dude can you do a time

00:31:42,780 --> 00:31:49,440
stamp trick as well there's no such

00:31:45,420 --> 00:31:51,030
thing there well so I come back to Delta

00:31:49,440 --> 00:31:52,650
versus absolute value if you're only

00:31:51,030 --> 00:31:54,540
doing Delta's you have to store less

00:31:52,650 --> 00:31:56,430
information and then you keep flushing

00:31:54,540 --> 00:31:57,840
it out if you're just doing it more

00:31:56,430 --> 00:31:59,400
absolute values which I think I'd

00:31:57,840 --> 00:32:01,560
probably be required for the time stamp

00:31:59,400 --> 00:32:03,810
approach then you have to store more

00:32:01,560 --> 00:32:05,970
information in the hardware you

00:32:03,810 --> 00:32:07,860
consuming more memory basically but what

00:32:05,970 --> 00:32:10,320
does Delta mean you mean do you if it's

00:32:07,860 --> 00:32:12,840
a 32-bit value or 64-bit value do you

00:32:10,320 --> 00:32:14,880
send less information B to that to this

00:32:12,840 --> 00:32:19,530
yeah I mean say say you have a bike

00:32:14,880 --> 00:32:21,990
counter and say you plan to flush it out

00:32:19,530 --> 00:32:24,840
every second then you might be able to

00:32:21,990 --> 00:32:27,000
get away with say a 32-bit value but if

00:32:24,840 --> 00:32:30,780
you are if you want it to be cumulative

00:32:27,000 --> 00:32:31,860
you'd need a 64-bit value it'll write so

00:32:30,780 --> 00:32:33,300
you've just doubled the storage

00:32:31,860 --> 00:32:35,510
requirement and that's without the

00:32:33,300 --> 00:32:36,930
storage requirement of the time stone

00:32:35,510 --> 00:32:38,370
okay

00:32:36,930 --> 00:32:40,530
so I mean I'm not saying this it

00:32:38,370 --> 00:32:44,250
scheming is with not without merit I

00:32:40,530 --> 00:32:47,370
just it's not a clear win

00:32:44,250 --> 00:32:49,110
right and if your hardware doesn't have

00:32:47,370 --> 00:32:52,140
support for storing this time stand

00:32:49,110 --> 00:32:55,289
value or anything like that it's gonna

00:32:52,140 --> 00:33:00,210
be a while out of curiosity let's say I

00:32:55,289 --> 00:33:01,620
do DC field a flower get and it has an

00:33:00,210 --> 00:33:03,270
action that has stats how do you guys

00:33:01,620 --> 00:33:05,549
implement it does it at that point gone

00:33:03,270 --> 00:33:07,289
retrieve it from the hardware or you

00:33:05,549 --> 00:33:09,299
periodically updating the kernel

00:33:07,289 --> 00:33:13,049
it just gets it from the Turkish coffee

00:33:09,299 --> 00:33:15,030
and - colonel well not cache isn't the

00:33:13,049 --> 00:33:16,799
right word the the the counters are kept

00:33:15,030 --> 00:33:20,010
in the kernel and they periodically

00:33:16,799 --> 00:33:22,140
updated by the heart as a result of

00:33:20,010 --> 00:33:24,210
messages from the hardware what is

00:33:22,140 --> 00:33:28,820
periodically mean if I say multiple

00:33:24,210 --> 00:33:28,820
times a second is its not definable

00:33:28,940 --> 00:33:34,220
gonna still use the space so there's a

00:33:30,960 --> 00:33:36,780
clear obviously there's a lock somewhere

00:33:34,220 --> 00:33:38,070
but but when the user space just gets

00:33:36,780 --> 00:33:40,590
what the kernel thinks is the right

00:33:38,070 --> 00:33:42,360
values and only the co knows because

00:33:40,590 --> 00:33:44,220
only in the kernel because it's only

00:33:42,360 --> 00:33:45,600
sending deltas from the hardware the

00:33:44,220 --> 00:33:47,700
hardware doesn't actually know it just

00:33:45,600 --> 00:33:50,510
knows how many new packets have been in

00:33:47,700 --> 00:33:53,400
the law since the last time it's entered

00:33:50,510 --> 00:33:56,039
so in this scenario is gram could if I

00:33:53,400 --> 00:33:57,419
do a get and I dump it just it doesn't

00:33:56,039 --> 00:33:58,830
touch the hardware it doesn't touch the

00:33:57,419 --> 00:34:01,950
head over this probably a lock which

00:33:58,830 --> 00:34:04,230
stops the hardware from updating or yes

00:34:01,950 --> 00:34:05,340
something like that but you you can have

00:34:04,230 --> 00:34:07,730
ever amortize err

00:34:05,340 --> 00:34:07,730
of course

00:34:07,740 --> 00:34:12,950
Mike yeah we did the pulling from user

00:34:10,440 --> 00:34:16,950
spaces is much less of an issue for us

00:34:12,950 --> 00:34:18,900
so what other hardware vendors do sorry

00:34:16,950 --> 00:34:21,450
though would you any idea what you guys

00:34:18,900 --> 00:34:27,360
do or copy em yeah

00:34:21,450 --> 00:34:29,670
Marvel now so what's most convenient

00:34:27,360 --> 00:34:32,160
convenient for us as a hardware vendor

00:34:29,670 --> 00:34:34,410
is to maintain on the hosts updated

00:34:32,160 --> 00:34:38,750
values and actually they can be Delta's

00:34:34,410 --> 00:34:42,660
are easier for us absolute values and

00:34:38,750 --> 00:34:45,110
for house to pull them when convenient

00:34:42,660 --> 00:34:47,310
right there whatever rate you like

00:34:45,110 --> 00:34:48,960
that's the easiest thing for us as a

00:34:47,310 --> 00:34:50,220
hardware vendor to do and this can

00:34:48,960 --> 00:34:53,040
consume a lot of memory maybe if there

00:34:50,220 --> 00:34:57,180
is a lot of connections but to maintain

00:34:53,040 --> 00:34:58,770
an updated database on the host so by

00:34:57,180 --> 00:35:02,840
definition that they database in the

00:34:58,770 --> 00:35:05,040
host and this could be depending on the

00:35:02,840 --> 00:35:07,440
specific structure of the stairs that's

00:35:05,040 --> 00:35:09,270
desired but this could be without even

00:35:07,440 --> 00:35:10,470
driver intervention driver will be part

00:35:09,270 --> 00:35:11,880
of setting it up but it will not be part

00:35:10,470 --> 00:35:16,460
of every update but the hard will just

00:35:11,880 --> 00:35:16,460
push it there direct dear me

00:35:20,500 --> 00:35:24,780
that would be the fastest thing you can

00:35:22,869 --> 00:35:27,010
do right you basically DMA into

00:35:24,780 --> 00:35:28,300
application buffers and your double

00:35:27,010 --> 00:35:30,609
buffer it so you don't have to take a

00:35:28,300 --> 00:35:32,170
look and you you'll be behind for a

00:35:30,609 --> 00:35:34,690
while and hardware will eventually catch

00:35:32,170 --> 00:35:36,910
up the question I think is back to what

00:35:34,690 --> 00:35:38,470
was said earlier which is what is the

00:35:36,910 --> 00:35:39,880
application that's processing these

00:35:38,470 --> 00:35:42,790
million two million three million

00:35:39,880 --> 00:35:45,640
entries per second when it's how widely

00:35:42,790 --> 00:35:49,840
distributed is that is that chartered

00:35:45,640 --> 00:35:50,380
right yeah so the application is sitting

00:35:49,840 --> 00:35:52,720
in user space

00:35:50,380 --> 00:35:55,810
it's send it to some cloud controller

00:35:52,720 --> 00:35:58,570
it's sucking this mostly in our case for

00:35:55,810 --> 00:36:01,119
example is just billing information so

00:35:58,570 --> 00:36:06,630
happens that there's some action based

00:36:01,119 --> 00:36:09,820
on stats is used for user data counting

00:36:06,630 --> 00:36:12,609
so so the question here is right I think

00:36:09,820 --> 00:36:15,099
in the case of TC trying to do formulate

00:36:12,609 --> 00:36:17,170
our output it makes sense if you are

00:36:15,099 --> 00:36:20,770
thinking about how this would be generic

00:36:17,170 --> 00:36:24,070
data statistics off of networking it can

00:36:20,770 --> 00:36:27,339
get a very large it can be very poorly

00:36:24,070 --> 00:36:29,710
set up as in there might span cache

00:36:27,339 --> 00:36:32,440
lines in all the wrong ways because I

00:36:29,710 --> 00:36:34,420
have TX counters clumped together an RX

00:36:32,440 --> 00:36:36,099
counters clump together and blah blah

00:36:34,420 --> 00:36:38,050
blah and I think this is something that

00:36:36,099 --> 00:36:41,320
it would be a good thing for somebody to

00:36:38,050 --> 00:36:43,180
say this is a spec and this spec can do

00:36:41,320 --> 00:36:46,180
whatever 10 million ops per second or

00:36:43,180 --> 00:36:48,400
something about stats I'm probably not

00:36:46,180 --> 00:36:50,050
as excited to get them every really

00:36:48,400 --> 00:37:00,070
up-to-date Jamali you're saying you're

00:36:50,050 --> 00:37:02,250
not excited by stat may have a different

00:37:00,070 --> 00:37:02,250
approach

00:37:04,239 --> 00:37:09,279
I'm not a hardware vendor in fact I'm

00:37:06,309 --> 00:37:11,249
quite naive on hardware which leads me

00:37:09,279 --> 00:37:14,380
into my little bit of a disconnect here

00:37:11,249 --> 00:37:16,420
so you said we have millions of actions

00:37:14,380 --> 00:37:21,759
and these have to be reported every five

00:37:16,420 --> 00:37:23,229
seconds but I would point out Nicks

00:37:21,759 --> 00:37:26,650
you know we're in XDP we're dropping

00:37:23,229 --> 00:37:29,650
like 20 million packets a second and we

00:37:26,650 --> 00:37:31,150
can we can take TCP all the way up to

00:37:29,650 --> 00:37:34,589
user space and get like 10 million

00:37:31,150 --> 00:37:37,299
packets per second so I'm missing why is

00:37:34,589 --> 00:37:39,910
I'm it's fun yeah it's a simple

00:37:37,299 --> 00:37:41,890
simplistic question why is that so hard

00:37:39,910 --> 00:37:45,400
when we can do packet processing so well

00:37:41,890 --> 00:37:51,249
Oh keys you're asking why do we need

00:37:45,400 --> 00:37:53,249
hardware offload or so we don't we get

00:37:51,249 --> 00:37:56,920
good performance even without hardware

00:37:53,249 --> 00:37:58,569
offload and packet processing right so

00:37:56,920 --> 00:38:01,660
we're collecting these stats and

00:37:58,569 --> 00:38:04,599
hardware we can ship them up to the host

00:38:01,660 --> 00:38:06,519
I can't believe that's more data than

00:38:04,599 --> 00:38:07,029
packets are provided I see so what am I

00:38:06,519 --> 00:38:09,130
missing

00:38:07,029 --> 00:38:12,219
okay okay so you're saying the the

00:38:09,130 --> 00:38:14,229
bandwidth or channel 2 kernel is already

00:38:12,219 --> 00:38:16,150
there why is this if we can do it with

00:38:14,229 --> 00:38:18,160
data path why can't we use with the

00:38:16,150 --> 00:38:21,599
control path so look at it this way is

00:38:18,160 --> 00:38:23,859
is this control path for statistics a

00:38:21,599 --> 00:38:27,789
appreciable percentage of what a data

00:38:23,859 --> 00:38:31,690
path would be so am I using 5% of my PCI

00:38:27,789 --> 00:38:34,829
bus just for stats is that the problem I

00:38:31,690 --> 00:38:34,829
don't know I mean

00:38:40,180 --> 00:38:45,170
so what is the aggregate throughput I

00:38:42,830 --> 00:38:46,970
need for statistics that's a great

00:38:45,170 --> 00:38:48,410
question I mean there's we can do simple

00:38:46,970 --> 00:38:50,890
math and probably figure this out but I

00:38:48,410 --> 00:38:52,760
think to kind of get back to you

00:38:50,890 --> 00:38:56,690
everybody's probably on an answer but I

00:38:52,760 --> 00:38:58,550
think but I have this mic so I think one

00:38:56,690 --> 00:39:00,260
of the one of the things is that the

00:38:58,550 --> 00:39:02,270
control path to a lot of these devices

00:39:00,260 --> 00:39:04,640
is just not as good as the packet path

00:39:02,270 --> 00:39:07,340
and no one wants to really say that

00:39:04,640 --> 00:39:09,500
about probably out loud but I will and

00:39:07,340 --> 00:39:11,060
then we have numbers to back that up and

00:39:09,500 --> 00:39:12,109
I think everybody probably does okay

00:39:11,060 --> 00:39:14,180
here's what you should do you should

00:39:12,109 --> 00:39:16,490
create packets in the device pair in

00:39:14,180 --> 00:39:20,030
your statistics and send them up to the

00:39:16,490 --> 00:39:23,140
hosts as duly noted and problem solved

00:39:20,030 --> 00:39:25,520
duly noted and previously suggested

00:39:23,140 --> 00:39:28,369
that's exactly what we're doing right so

00:39:25,520 --> 00:39:30,800
I have come in today say certainly may

00:39:28,369 --> 00:39:32,750
work if you don't need to forward

00:39:30,800 --> 00:39:34,280
packets if you need to forward packets

00:39:32,750 --> 00:39:37,340
and you need to do it fast

00:39:34,280 --> 00:39:40,850
you cannot just do it through CP CPU it

00:39:37,340 --> 00:39:43,609
it wouldn't be fast enough so you have

00:39:40,850 --> 00:39:48,369
to gather the statistics and push it to

00:39:43,609 --> 00:39:52,400
the CPU but one one issue I see about

00:39:48,369 --> 00:39:54,890
about the asynchronous think you do and

00:39:52,400 --> 00:39:58,550
you do as well and we do in Mellanox do

00:39:54,890 --> 00:40:01,640
in annex five we do it synchronously as

00:39:58,550 --> 00:40:06,109
well is that you kind of break causality

00:40:01,640 --> 00:40:09,170
because I have a test case when I sent a

00:40:06,109 --> 00:40:13,550
packet and then I go to the counters and

00:40:09,170 --> 00:40:16,640
I don't see the packet counted so that's

00:40:13,550 --> 00:40:18,650
wrong right it's you you don't you

00:40:16,640 --> 00:40:21,750
cannot experience it when you are using

00:40:18,650 --> 00:40:23,280
software only but

00:40:21,750 --> 00:40:25,560
when you do offloading you can

00:40:23,280 --> 00:40:29,760
experience it in am alexis W in our

00:40:25,560 --> 00:40:33,690
switchdriver we we just pull whenever i

00:40:29,760 --> 00:40:35,609
i the user wants to get the statistics

00:40:33,690 --> 00:40:37,589
we go all the way down to to the

00:40:35,609 --> 00:40:40,740
hardware and get the recent statistics

00:40:37,589 --> 00:40:44,369
so that can happen but with that's a

00:40:40,740 --> 00:40:52,140
problem i think but your approach here

00:40:44,369 --> 00:40:53,940
is not very scalable I mean of course

00:40:52,140 --> 00:40:55,320
with asynchronous II there's a potential

00:40:53,940 --> 00:41:02,310
for delay but it would depend on the

00:40:55,320 --> 00:41:06,540
sampling rate of course regarding your

00:41:02,310 --> 00:41:08,339
question why why is that if you guys

00:41:06,540 --> 00:41:10,589
know to do packet processing hey how can

00:41:08,339 --> 00:41:13,560
you do can I do start processing so I

00:41:10,589 --> 00:41:17,220
was trying to explain her the problem

00:41:13,560 --> 00:41:20,550
and then you said the solution so these

00:41:17,220 --> 00:41:22,800
are control object as andy says and like

00:41:20,550 --> 00:41:24,720
in the tcp case he said you can forward

00:41:22,800 --> 00:41:26,609
these in that millions of packets per

00:41:24,720 --> 00:41:31,050
second that they let's say they belong

00:41:26,609 --> 00:41:33,270
to one one flow or a small number of

00:41:31,050 --> 00:41:34,740
flows that the number of control the

00:41:33,270 --> 00:41:37,020
hardware controller job the object is

00:41:34,740 --> 00:41:38,640
limited right but if you if you take

00:41:37,020 --> 00:41:40,950
millions flows each one of them has a

00:41:38,640 --> 00:41:43,109
control object and then you start to get

00:41:40,950 --> 00:41:46,319
into hardware cache misses and stuff

00:41:43,109 --> 00:41:48,150
like that and then the probably one of

00:41:46,319 --> 00:41:51,089
the ways to solve that as you suggest

00:41:48,150 --> 00:41:53,819
says is to apply the data pass approach

00:41:51,089 --> 00:41:56,339
to the control person so people are

00:41:53,819 --> 00:41:58,020
looking at that is you do we want to

00:41:56,339 --> 00:42:00,780
continue this discussion after I think

00:41:58,020 --> 00:42:02,819
it looks like a lot of players the

00:42:00,780 --> 00:42:06,119
players are here maybe we can have

00:42:02,819 --> 00:42:09,800
coffee somewhere and discuss I'm gonna

00:42:06,119 --> 00:42:09,800
cut it here okay thanks

00:42:11,270 --> 00:42:18,660
ready okay we're gonna switch to remote

00:42:15,060 --> 00:42:21,960
control from Amrita I don't know what

00:42:18,660 --> 00:42:23,940
time it is there right now hi Jamal hi

00:42:21,960 --> 00:42:25,980
yeah everybody else can see you not just

00:42:23,940 --> 00:42:27,830
me and I think you're displaying in

00:42:25,980 --> 00:42:32,390
multiple rooms probably which is good

00:42:27,830 --> 00:42:33,990
[Laughter]

00:42:32,390 --> 00:42:36,839
can I start

00:42:33,990 --> 00:42:43,040
yeah yes please this light house lights

00:42:36,839 --> 00:42:43,040
can if you can yeah number two

00:42:52,240 --> 00:42:59,230
so here I go I just have some very quick

00:42:56,020 --> 00:43:05,020
updates from Intel on TC Hardware

00:42:59,230 --> 00:43:09,819
offloads so the first thing we started

00:43:05,020 --> 00:43:15,609
last year was supporting matches on port

00:43:09,819 --> 00:43:18,809
ranges in the TC classifier can you

00:43:15,609 --> 00:43:20,589
switch to the next slide please

00:43:18,809 --> 00:43:24,220
yeah

00:43:20,589 --> 00:43:28,359
so our first work involved supporting

00:43:24,220 --> 00:43:31,690
matches on port Rangers in the TC

00:43:28,359 --> 00:43:36,010
classifier so we have hardware that is

00:43:31,690 --> 00:43:38,650
capable of offloading filters that match

00:43:36,010 --> 00:43:42,190
on a range of ports and this can be done

00:43:38,650 --> 00:43:45,270
as a single rule in the hardware so we

00:43:42,190 --> 00:43:48,670
first looked at the u32 classifier and

00:43:45,270 --> 00:43:52,480
the u-32 classifier comes close to

00:43:48,670 --> 00:43:56,410
offloading range based rules but had

00:43:52,480 --> 00:43:59,680
certain limitations u32 required power

00:43:56,410 --> 00:44:04,299
of two ranges and these ranges were

00:43:59,680 --> 00:44:07,660
taken as mask value pairs so it cannot

00:44:04,299 --> 00:44:11,650
just be a simple range in the form of

00:44:07,660 --> 00:44:14,109
milkman and max fields and non power of

00:44:11,650 --> 00:44:16,480
two ranges in new 32 had to be split

00:44:14,109 --> 00:44:19,390
into multiple power of two ranges so

00:44:16,480 --> 00:44:22,750
multiple rules so splitting into

00:44:19,390 --> 00:44:25,930
multiple rules even for off loads just

00:44:22,750 --> 00:44:28,270
takes away the performance benefits in

00:44:25,930 --> 00:44:31,440
being able to offload to a hardware that

00:44:28,270 --> 00:44:35,530
can implement the easiest single rules

00:44:31,440 --> 00:44:38,020
and in software there are other software

00:44:35,530 --> 00:44:42,579
infrastructure's that actually support

00:44:38,020 --> 00:44:47,170
port range rules like IP set and image

00:44:42,579 --> 00:44:49,299
trees so IP said takes these ranges as

00:44:47,170 --> 00:44:51,849
min and Max values a match tree it

00:44:49,299 --> 00:44:55,740
orders less than or greater than

00:44:51,849 --> 00:44:59,890
comparison but these do not support

00:44:55,740 --> 00:45:01,100
Hardware off loads and in obvious these

00:44:59,890 --> 00:45:05,540
are

00:45:01,100 --> 00:45:07,580
again bitwise matches require that

00:45:05,540 --> 00:45:09,650
required power of two Rangers and need

00:45:07,580 --> 00:45:13,460
to be split into multiple rules for non

00:45:09,650 --> 00:45:19,360
power of two Rangers so in the next

00:45:13,460 --> 00:45:25,010
slide our best candidate was TC flower

00:45:19,360 --> 00:45:28,910
so how the challenge is that TC flower

00:45:25,010 --> 00:45:31,910
is based on the our hash table lookup

00:45:28,910 --> 00:45:35,660
using mask based keys which is again not

00:45:31,910 --> 00:45:38,930
ideal for range matches but then we

00:45:35,660 --> 00:45:42,890
already support filter off floats

00:45:38,930 --> 00:45:46,970
through TC flower in our devices so this

00:45:42,890 --> 00:45:48,830
was the best candidate to work with so

00:45:46,970 --> 00:45:51,470
essentially we just introduced a range

00:45:48,830 --> 00:45:53,570
comparator in TC flower in addition to

00:45:51,470 --> 00:45:57,560
the already existing our hash table

00:45:53,570 --> 00:46:00,320
lookup so we could now support matching

00:45:57,560 --> 00:46:02,330
on destination and source port based

00:46:00,320 --> 00:46:05,410
range assessment and max values and

00:46:02,330 --> 00:46:08,570
these could also be combined with other

00:46:05,410 --> 00:46:12,110
existing match fields so for example you

00:46:08,570 --> 00:46:14,210
could match on IP protocol types or

00:46:12,110 --> 00:46:16,520
destination or IP source address

00:46:14,210 --> 00:46:21,070
destination IP source port range all as

00:46:16,520 --> 00:46:23,830
a single rule so the idea here is that

00:46:21,070 --> 00:46:27,620
every time a new filter is added a range

00:46:23,830 --> 00:46:30,020
when a range filter is added a new mask

00:46:27,620 --> 00:46:33,620
is created with men and max fields and a

00:46:30,020 --> 00:46:35,990
range but a certain have asked so for

00:46:33,620 --> 00:46:38,270
filter value validations like checking

00:46:35,990 --> 00:46:40,640
if the filter exists it's just the

00:46:38,270 --> 00:46:44,540
regular our hash table lookup that flour

00:46:40,640 --> 00:46:47,930
already had when the packet arrives the

00:46:44,540 --> 00:46:52,670
escape is classified using a two-step

00:46:47,930 --> 00:46:56,210
process so the first stage is the lookup

00:46:52,670 --> 00:46:59,780
in the filter list for the range so the

00:46:56,210 --> 00:47:02,750
skb port can be compared against the

00:46:59,780 --> 00:47:06,410
Mandan max values in the range filter

00:47:02,750 --> 00:47:09,560
and if the range match succeeds then

00:47:06,410 --> 00:47:11,780
proceed to the next stage which is just

00:47:09,560 --> 00:47:14,900
do the regular our hash table lookup for

00:47:11,780 --> 00:47:18,619
rest of the escapee

00:47:14,900 --> 00:47:20,720
but in software this is less performant

00:47:18,619 --> 00:47:25,400
because there's just a linear search in

00:47:20,720 --> 00:47:27,140
the filter list and software image uses

00:47:25,400 --> 00:47:31,730
tree based searches which is more

00:47:27,140 --> 00:47:36,349
performant but this method scales from a

00:47:31,730 --> 00:47:42,410
hardware offload point of view so in the

00:47:36,349 --> 00:47:45,920
next slide I have an example so here's

00:47:42,410 --> 00:47:50,510
how you could just match on destination

00:47:45,920 --> 00:47:54,800
port range with 20 and 30 as the range

00:47:50,510 --> 00:47:59,119
values and this is just a flow dump for

00:47:54,800 --> 00:48:02,020
packets that matches match this flow and

00:47:59,119 --> 00:48:07,579
in the next slide I have another example

00:48:02,020 --> 00:48:09,920
which combines destination IP based

00:48:07,579 --> 00:48:12,230
match and destination port based match

00:48:09,920 --> 00:48:17,329
as a single rule and the corresponding

00:48:12,230 --> 00:48:28,579
flow dumps for that so that's about

00:48:17,329 --> 00:48:31,369
train support in the next slide okay you

00:48:28,579 --> 00:48:33,260
say the problem with you 32 is it me you

00:48:31,369 --> 00:48:35,900
will have to use two rules yes because

00:48:33,260 --> 00:48:37,339
the first one yeah power of two and then

00:48:35,900 --> 00:48:41,780
if there's any left over I have another

00:48:37,339 --> 00:48:42,950
rule yeah yes yeah isn't that how you

00:48:41,780 --> 00:48:46,550
hardware probably works

00:48:42,950 --> 00:48:49,099
how are you implementing range so the

00:48:46,550 --> 00:48:50,070
hardware basically uses teakamp ternary

00:48:49,099 --> 00:48:53,430
can

00:48:50,070 --> 00:48:55,920
you know classifier sure you know how do

00:48:53,430 --> 00:49:00,480
you folks here familiar with it

00:48:55,920 --> 00:49:03,720
so with that you know it's it's not an

00:49:00,480 --> 00:49:07,520
exact match lookup so it can do the

00:49:03,720 --> 00:49:09,210
range rules differently like an action

00:49:07,520 --> 00:49:12,120
I'll talk to you later

00:49:09,210 --> 00:49:16,340
but we can do 10 or e matches in you 32

00:49:12,120 --> 00:49:16,340
as well right okay sorry go ahead

00:49:21,480 --> 00:49:29,580
so we further extended you 32 offloads

00:49:25,890 --> 00:49:32,130
that we already support on IX GP iooks

00:49:29,580 --> 00:49:36,410
to be so far supports you 32 our floats

00:49:32,130 --> 00:49:40,650
were matching on l3 and l4 headers and

00:49:36,410 --> 00:49:43,890
the action supported word drop or read

00:49:40,650 --> 00:49:47,520
added to a forwarding device like SR way

00:49:43,890 --> 00:49:51,960
we have sought Mac villain devices so we

00:49:47,520 --> 00:49:55,860
added extensions to support more TC

00:49:51,960 --> 00:49:59,760
actions like the action accept and

00:49:55,860 --> 00:50:02,610
action SKB edit mark X to be supports a

00:49:59,760 --> 00:50:04,680
15 bit mark value so these are

00:50:02,610 --> 00:50:08,130
essentially floor director rules in the

00:50:04,680 --> 00:50:13,440
hardware in a single flat I HTTP table

00:50:08,130 --> 00:50:15,480
and both these actions have a cue

00:50:13,440 --> 00:50:17,850
selection policy which could be one of

00:50:15,480 --> 00:50:20,280
the two so it could either be a

00:50:17,850 --> 00:50:23,520
round-robin selection implemented

00:50:20,280 --> 00:50:27,810
internally in the driver or it could be

00:50:23,520 --> 00:50:32,100
a from user specified flow ID or class

00:50:27,810 --> 00:50:34,440
ID fields the minor number in the flow

00:50:32,100 --> 00:50:37,740
ie or class ID field can be used to

00:50:34,440 --> 00:50:39,870
select the queue and this is just an

00:50:37,740 --> 00:50:44,460
ongoing work that's not available in the

00:50:39,870 --> 00:50:48,870
upstream I actually driver moving on to

00:50:44,460 --> 00:50:53,070
the next slide this is how I xt p

00:50:48,870 --> 00:50:55,560
supports offloading mark action so this

00:50:53,070 --> 00:50:59,760
first an ingress rule on the PF device

00:50:55,560 --> 00:51:02,400
which matches on say IP protocol values

00:50:59,760 --> 00:51:08,250
or destination on societies of both

00:51:02,400 --> 00:51:12,230
values and the action is marked with the

00:51:08,250 --> 00:51:15,750
metadata value so as packets arrive and

00:51:12,230 --> 00:51:19,099
finds a rule in the hardware and matches

00:51:15,750 --> 00:51:22,729
on those rule the packets get

00:51:19,099 --> 00:51:25,339
marked with a metadata value and these

00:51:22,729 --> 00:51:27,019
packets could be accepted into the cube

00:51:25,339 --> 00:51:29,869
based on one of the queue selection

00:51:27,019 --> 00:51:33,069
policies and as they're sent of the

00:51:29,869 --> 00:51:36,289
stack there's a software rule in the

00:51:33,069 --> 00:51:38,359
software rule using the firewall mock

00:51:36,289 --> 00:51:42,019
classifier matching on the metadata

00:51:38,359 --> 00:51:46,190
value and the action is to redirect it

00:51:42,019 --> 00:51:48,499
to the next PF so the performance gain

00:51:46,190 --> 00:51:51,079
here is that the first level of

00:51:48,499 --> 00:51:54,709
classification is done in the hardware

00:51:51,079 --> 00:51:56,630
and this work is also ongoing else not

00:51:54,709 --> 00:52:02,709
available in the upstream iyx to be

00:51:56,630 --> 00:52:07,640
driver moving on to the next slide a

00:52:02,709 --> 00:52:10,130
question here Anjali question C you said

00:52:07,640 --> 00:52:16,579
you're furthering for P of 0 to P f1

00:52:10,130 --> 00:52:18,309
this is what you're doing this can be

00:52:16,579 --> 00:52:22,069
done only in software

00:52:18,309 --> 00:52:26,690
we're not forwarding here to from P of 0

00:52:22,069 --> 00:52:28,219
to P f1 we are just marking never on

00:52:26,690 --> 00:52:30,880
this slide you say that you forward it

00:52:28,219 --> 00:52:30,880
to P f1

00:52:39,700 --> 00:52:46,570
yeah oh it's softer oh it's softer

00:52:53,500 --> 00:52:59,160
continued if I heard the question right

00:52:55,960 --> 00:53:02,619
the forwarding is only in the software

00:52:59,160 --> 00:53:05,260
yeah yeah you was wondering how you you

00:53:02,619 --> 00:53:07,750
got after you use sk v mark to forward

00:53:05,260 --> 00:53:13,930
to another device you do that in

00:53:07,750 --> 00:53:16,859
software in your second stage that's a

00:53:13,930 --> 00:53:19,660
good continue where you left off so

00:53:16,859 --> 00:53:23,530
here's some future work we are planning

00:53:19,660 --> 00:53:28,750
to do on in the Intel devices so we plan

00:53:23,530 --> 00:53:31,270
to extend our driver to support stats on

00:53:28,750 --> 00:53:34,510
offloaded flood filters

00:53:31,270 --> 00:53:36,849
I think Andy just completed his talk on

00:53:34,510 --> 00:53:40,540
stats so we are planning to query our

00:53:36,849 --> 00:53:44,170
hardware counters offloading the OTC

00:53:40,540 --> 00:53:48,250
flower starts command and update the

00:53:44,170 --> 00:53:51,099
stats and also support aging again based

00:53:48,250 --> 00:53:55,690
on querying discounters periodically so

00:53:51,099 --> 00:53:58,569
our big concern here is that querying

00:53:55,690 --> 00:54:02,470
the counters for every offloaded filter

00:53:58,569 --> 00:54:04,569
is kind of a performance hit so we would

00:54:02,470 --> 00:54:10,960
like to have the counters queried only

00:54:04,569 --> 00:54:14,859
for certain filters so I mean the

00:54:10,960 --> 00:54:19,349
solution we propose is a sort of a TC

00:54:14,859 --> 00:54:24,280
action counter that could associate

00:54:19,349 --> 00:54:27,040
starts with only a certain offloaded

00:54:24,280 --> 00:54:29,079
flows and not all offloaded flows it's a

00:54:27,040 --> 00:54:30,700
question for you so you have sounds like

00:54:29,079 --> 00:54:34,839
you have some timestamp that's how

00:54:30,700 --> 00:54:38,200
you're doing flow aging yes there's some

00:54:34,839 --> 00:54:41,319
timestamp in the hardware right are you

00:54:38,200 --> 00:54:44,380
able are you able therefore to dump

00:54:41,319 --> 00:54:46,270
based on timestamp let's say timestamp

00:54:44,380 --> 00:54:48,579
is has been not something that has been

00:54:46,270 --> 00:54:50,490
updated in five seconds that's the as a

00:54:48,579 --> 00:54:54,849
filter

00:54:50,490 --> 00:54:58,660
but that should be done for supporting

00:54:54,849 --> 00:55:02,230
aging right yes I think

00:54:58,660 --> 00:55:06,369
Jamal's question was can a hardware dump

00:55:02,230 --> 00:55:09,160
counter only if it was hit I mean the

00:55:06,369 --> 00:55:13,359
flow was hit in last few seconds or

00:55:09,160 --> 00:55:17,680
whatever otherwise yeah so the answer is

00:55:13,359 --> 00:55:20,020
the present hardware doesn't so this is

00:55:17,680 --> 00:55:21,490
this is more of what Amrita is talking

00:55:20,020 --> 00:55:25,750
to his slow aging from software

00:55:21,490 --> 00:55:27,670
mechanism based on counter rates and the

00:55:25,750 --> 00:55:29,920
thing that she was trying to solve is

00:55:27,670 --> 00:55:34,510
can I associate counters to certain

00:55:29,920 --> 00:55:36,400
filters or if I'm putting say of flow

00:55:34,510 --> 00:55:38,650
aggregate in the hardware not a flow

00:55:36,400 --> 00:55:42,790
it's a flow aggregate I want to counter

00:55:38,650 --> 00:55:45,790
only on that so that I I can associate

00:55:42,790 --> 00:55:50,590
an action to a flow rule which says

00:55:45,790 --> 00:55:52,900
count and I just so by reading it

00:55:50,590 --> 00:55:54,520
multiple times and if it doesn't change

00:55:52,900 --> 00:55:57,070
you know you can a get out is that the

00:55:54,520 --> 00:56:00,040
technique nobody wants to put a time

00:55:57,070 --> 00:56:03,130
stop in their hardware as I said I mean

00:56:00,040 --> 00:56:08,050
it's not there a lot of hardware vendors

00:56:03,130 --> 00:56:09,849
are solving it take somebody on to it so

00:56:08,050 --> 00:56:12,010
Anjali another question you mentioned

00:56:09,849 --> 00:56:15,700
before that somehow this can deal with

00:56:12,010 --> 00:56:17,640
broad ranges also the general case let's

00:56:15,700 --> 00:56:20,550
see for instance your hardware or

00:56:17,640 --> 00:56:22,200
can so the how we can in the same

00:56:20,550 --> 00:56:26,690
hardware in the same configuration can

00:56:22,200 --> 00:56:30,210
deal with rules that are convenient

00:56:26,690 --> 00:56:32,670
classical like key and value a Qian mask

00:56:30,210 --> 00:56:37,080
and with ranges you can do it on the

00:56:32,670 --> 00:56:45,630
same configuration we can do that girion

00:56:37,080 --> 00:56:48,230
tecum on your own spectrum as well yeah

00:56:45,630 --> 00:56:48,230
keep going please

00:56:48,800 --> 00:57:01,670
any more slides moving on to the next

00:56:54,860 --> 00:57:07,520
slide in addition to the UTC action

00:57:01,670 --> 00:57:10,820
counter we also plan to have new offload

00:57:07,520 --> 00:57:14,600
hooks for TC police action

00:57:10,820 --> 00:57:17,000
so right now TC police action is used

00:57:14,600 --> 00:57:20,180
for rate limiting inbound and outbound

00:57:17,000 --> 00:57:23,540
traffic and although for outbound

00:57:20,180 --> 00:57:26,230
traffic rate limiting we already have

00:57:23,540 --> 00:57:30,170
queuing disciplines that can offer load

00:57:26,230 --> 00:57:31,970
shaping features its the TC police

00:57:30,170 --> 00:57:35,660
action that's most used for rate

00:57:31,970 --> 00:57:39,800
limiting ingress traffic but currently

00:57:35,660 --> 00:57:45,230
there are no offload hooks for police

00:57:39,800 --> 00:57:50,650
action so that's the so next step ahead

00:57:45,230 --> 00:57:53,600
which NIC is this app which hardware

00:57:50,650 --> 00:57:57,050
yeah so this is the 900 series it has

00:57:53,600 --> 00:58:01,250
some limited police actions now this is

00:57:57,050 --> 00:58:03,670
a 900 series ice driver the 100 cake

00:58:01,250 --> 00:58:03,670
yeah

00:58:09,950 --> 00:58:15,559
um so I'll just hand over to Anjali from

00:58:13,460 --> 00:58:20,270
the next slide to talk about challenges

00:58:15,559 --> 00:58:23,589
that we faced while meeting our customer

00:58:20,270 --> 00:58:31,280
requirements and use cases

00:58:23,589 --> 00:58:34,730
Anjali would you like to go ahead yeah

00:58:31,280 --> 00:58:37,549
so I mean just I guess the problems that

00:58:34,730 --> 00:58:40,549
you know most people talk about they're

00:58:37,549 --> 00:58:44,809
very similar and we are seeing similar

00:58:40,549 --> 00:58:46,010
cases you know I'll just cover although

00:58:44,809 --> 00:58:48,859
I'm not following the slide I'm just

00:58:46,010 --> 00:58:52,059
gonna cover a few things about what we

00:58:48,859 --> 00:58:52,059
talked about counter so

00:59:00,569 --> 00:59:10,589
okay so so I mean I should say that we

00:59:06,900 --> 00:59:12,779
haven't really seen a whole lot of TC

00:59:10,589 --> 00:59:14,279
problems as you know monarchs and

00:59:12,779 --> 00:59:16,979
Broadcom have encountered but we are

00:59:14,279 --> 00:59:22,529
discovering as we are adding features as

00:59:16,979 --> 00:59:23,999
well the counters you know you know I'm

00:59:22,529 --> 00:59:27,689
gonna describe one of the problems then

00:59:23,999 --> 00:59:29,130
we have also looked at many different

00:59:27,689 --> 00:59:31,319
ways to solve it in the hardware and I

00:59:29,130 --> 00:59:33,239
think geez and a lot of people kind of

00:59:31,319 --> 00:59:36,779
covered all the different ways you can

00:59:33,239 --> 00:59:40,369
DMA the you know account rental as

00:59:36,779 --> 00:59:44,279
packets or give me two hosts memory or

00:59:40,369 --> 00:59:48,029
you know do Delta versus do absolute

00:59:44,279 --> 00:59:49,589
versus do it only when there is a change

00:59:48,029 --> 00:59:54,119
in the flow and all those things so I

00:59:49,589 --> 00:59:57,269
mean yes this is occupying a significant

00:59:54,119 --> 01:00:00,569
bandwidth when we scale it to millions

00:59:57,269 --> 01:00:05,249
of flows and counter per flow so we have

01:00:00,569 --> 01:00:08,789
to make sure that we optimize not just

01:00:05,249 --> 01:00:11,159
the hardware but also you know the the

01:00:08,789 --> 01:00:13,499
storage in the kernel retriever from

01:00:11,159 --> 01:00:15,509
kernel to the user and things like that

01:00:13,499 --> 01:00:21,269
so those are the things that we'll be

01:00:15,509 --> 01:00:25,319
looking at we also have some more work

01:00:21,269 --> 01:00:28,890
to do on the offloads which are related

01:00:25,319 --> 01:00:32,569
to two camps the longest prefix match T

01:00:28,890 --> 01:00:32,569
camps and

01:00:33,040 --> 01:00:38,140
that's available in our current

01:00:35,740 --> 01:00:40,510
generation hardware which is the 900

01:00:38,140 --> 01:00:47,230
series 100 kick hardware so we'll be

01:00:40,510 --> 01:00:50,620
looking at some more of that we are also

01:00:47,230 --> 01:00:52,960
seeing some you know usage where TC

01:00:50,620 --> 01:00:55,950
flour is limited and ever had back and

01:00:52,960 --> 01:01:00,190
forth between TC flour versus you 32 and

01:00:55,950 --> 01:01:03,460
you know some limitations because of

01:01:00,190 --> 01:01:06,550
name fields and all those you know can't

01:01:03,460 --> 01:01:10,600
use proprietary protocol types and all

01:01:06,550 --> 01:01:12,280
other issues so and and we have just

01:01:10,600 --> 01:01:15,340
started doing our performance numbers to

01:01:12,280 --> 01:01:18,130
see the flow rates you know the rule

01:01:15,340 --> 01:01:20,620
insertion rate kind of thing and she

01:01:18,130 --> 01:01:24,090
they're most likely will present our

01:01:20,620 --> 01:01:31,260
work shortly or you'll see some patches

01:01:24,090 --> 01:01:31,260
to help with whatever blood is doing so

01:01:32,150 --> 01:01:36,620
yeah and one of the things that we

01:01:33,950 --> 01:01:39,950
noticed is when we are doing the rule

01:01:36,620 --> 01:01:42,770
insertion there is multiple places where

01:01:39,950 --> 01:01:44,470
the flow rules are being maintained and

01:01:42,770 --> 01:01:46,760
so that's something that we need to

01:01:44,470 --> 01:01:49,760
definitely clean up you know the layers

01:01:46,760 --> 01:01:52,040
where it is being maintained is you know

01:01:49,760 --> 01:01:53,810
in the kernel in the dry I mean the

01:01:52,040 --> 01:01:55,310
drive itself is maintaining it and if

01:01:53,810 --> 01:01:57,860
you're offloading to hardware it's there

01:01:55,310 --> 01:02:00,080
and then there is the layer above you

01:01:57,860 --> 01:02:01,970
know if you're using any of the V

01:02:00,080 --> 01:02:05,330
switches so it's it's stored in multiple

01:02:01,970 --> 01:02:07,820
places and that's adding to the rule

01:02:05,330 --> 01:02:10,100
insertion delays as well and you know

01:02:07,820 --> 01:02:12,080
everywhere there's I'm sure Andy is

01:02:10,100 --> 01:02:17,810
shaking his head so who he's aware of

01:02:12,080 --> 01:02:19,160
that footprint what you're both it's not

01:02:17,810 --> 01:02:24,050
just the memory footprint just

01:02:19,160 --> 01:02:27,050
maintaining your status for flows it

01:02:24,050 --> 01:02:28,850
inserts you know it causes statefulness

01:02:27,050 --> 01:02:31,030
which causes your CP utilization to go

01:02:28,850 --> 01:02:31,030
higher

01:02:33,490 --> 01:02:44,980
I guess there's this one last slide

01:02:37,380 --> 01:02:49,270
veterinary yeah those are the tea camps

01:02:44,980 --> 01:02:54,460
like you could do tri-states don't care

01:02:49,270 --> 01:03:01,290
states basically yeah for that mega

01:02:54,460 --> 01:03:01,290
flows okay

01:03:03,160 --> 01:03:09,370
yeah so that's about it so yeah we rule

01:03:06,250 --> 01:03:11,350
updates as one of the area where we also

01:03:09,370 --> 01:03:16,300
will be putting a lot more work to see

01:03:11,350 --> 01:03:20,700
what can be done to optimize it Thanks

01:03:16,300 --> 01:03:26,400
thank you and someone you're next

01:03:20,700 --> 01:03:26,400
thank you I'm Rita and I'm charlie

01:03:26,770 --> 01:03:29,929
[Applause]

01:03:48,040 --> 01:03:55,260
hi we're going to talk about some work

01:03:50,740 --> 01:03:59,140
that my team at national has been doing

01:03:55,260 --> 01:04:01,810
about but the use case here is to we

01:03:59,140 --> 01:04:04,900
talk about GC offload to metronome

01:04:01,810 --> 01:04:07,180
hardware and this relates to OBS use

01:04:04,900 --> 01:04:09,730
case most of this work was done by my

01:04:07,180 --> 01:04:13,780
colleague John but he's not here today

01:04:09,730 --> 01:04:15,070
so I'll be presenting so actually we

01:04:13,780 --> 01:04:17,890
have a particular problem we're trying

01:04:15,070 --> 01:04:19,360
to solve and we have two solutions I may

01:04:17,890 --> 01:04:22,840
not have time to go through both of them

01:04:19,360 --> 01:04:25,690
they're kind of similar so obvious

01:04:22,840 --> 01:04:28,900
itself has this notion of what it calls

01:04:25,690 --> 01:04:30,760
internal ports so these are not hidden

01:04:28,900 --> 01:04:34,930
the actual net Dave's you can see them

01:04:30,760 --> 01:04:36,870
but they have some specific properties

01:04:34,930 --> 01:04:39,700
because of the way that they implemented

01:04:36,870 --> 01:04:41,380
and maybe the most common example of an

01:04:39,700 --> 01:04:46,270
internal port would be the bridge itself

01:04:41,380 --> 01:04:51,280
the OVS bridge so to explain this fairly

01:04:46,270 --> 01:04:53,740
simply when packet goes is to be

01:04:51,280 --> 01:04:56,470
egressed from one of these special ports

01:04:53,740 --> 01:04:58,570
it's kind of like a shortcut so it

01:04:56,470 --> 01:05:01,900
doesn't go through the entire stack and

01:04:58,570 --> 01:05:03,670
so in particular it doesn't go to the

01:05:01,900 --> 01:05:09,130
part of the stack where the TC hooks it

01:05:03,670 --> 01:05:12,310
so although we can we can set up a TC

01:05:09,130 --> 01:05:14,200
room sitting on this device on ingress

01:05:12,310 --> 01:05:14,650
which is where we put all our other

01:05:14,200 --> 01:05:18,090
rules

01:05:14,650 --> 01:05:18,090
it will never be hit

01:05:18,470 --> 01:05:24,819
so this creates problem why

01:05:22,520 --> 01:05:28,130
what is the use case why do we even care

01:05:24,819 --> 01:05:30,460
so we're talking about situations where

01:05:28,130 --> 01:05:34,910
we are floating

01:05:30,460 --> 01:05:38,930
we have egress to a tunnel so in this

01:05:34,910 --> 01:05:43,430
case essentially the packet will leave

01:05:38,930 --> 01:05:48,460
obvious data pause and to be egressed

01:05:43,430 --> 01:05:52,700
to a tunnel native and then magically

01:05:48,460 --> 01:05:55,190
the network stack will figure out ok so

01:05:52,700 --> 01:05:58,460
we do a neighbor look up from whatever

01:05:55,190 --> 01:06:00,260
and figure figure out ok so now they see

01:05:58,460 --> 01:06:02,990
now encapsulated package should out

01:06:00,260 --> 01:06:05,420
grace a suit should egress onto the

01:06:02,990 --> 01:06:08,329
physical network and oh the path for

01:06:05,420 --> 01:06:11,660
this the IP address the source part of

01:06:08,329 --> 01:06:13,220
the endpoint belong to the bridge and so

01:06:11,660 --> 01:06:15,319
then it sort of goes back into the

01:06:13,220 --> 01:06:18,680
bridge and then goes out and in this

01:06:15,319 --> 01:06:22,040
point we would like to apply T 0 because

01:06:18,680 --> 01:06:23,630
this is our floating mechanism and in

01:06:22,040 --> 01:06:24,980
that way allow the hardware to and know

01:06:23,630 --> 01:06:27,530
about this and process everything in

01:06:24,980 --> 01:06:30,470
hardware but as I mentioned it the TC

01:06:27,530 --> 01:06:32,859
row processing is basically skipped so

01:06:30,470 --> 01:06:32,859
it doesn't work

01:06:34,620 --> 01:06:38,910
so I apologize these diagrams are kind

01:06:36,780 --> 01:06:41,070
of complicated with diagram arrows going

01:06:38,910 --> 01:06:42,960
everywhere but the basic idea is what I

01:06:41,070 --> 01:06:45,360
just explained just now is the packets

01:06:42,960 --> 01:06:47,880
coming in and somehow it gets to the OBS

01:06:45,360 --> 01:06:51,480
kernel data path and it decides it's

01:06:47,880 --> 01:06:52,350
going to output to a VLAN so it outputs

01:06:51,480 --> 01:06:54,900
through this

01:06:52,350 --> 01:06:57,330
VLAN native which then goes to the

01:06:54,900 --> 01:07:00,330
network stack does a lookup and decides

01:06:57,330 --> 01:07:02,460
to output to the internal port and at

01:07:00,330 --> 01:07:06,720
around about point for the way that the

01:07:02,460 --> 01:07:08,070
problem start in the software case this

01:07:06,720 --> 01:07:09,390
is talking about the pure software case

01:07:08,070 --> 01:07:11,940
this is all fine everything is working

01:07:09,390 --> 01:07:14,100
perfectly no problems but we would like

01:07:11,940 --> 01:07:18,150
to offload this and as I said we like to

01:07:14,100 --> 01:07:20,520
offload it using OBS TC yes the internal

01:07:18,150 --> 01:07:23,100
port is is resides on top of eighth one

01:07:20,520 --> 01:07:28,200
or it could go to multiple each kind of

01:07:23,100 --> 01:07:30,540
devices is it so in this particular case

01:07:28,200 --> 01:07:34,500
it's just one but in the general case

01:07:30,540 --> 01:07:36,630
what we'd really like to do is so it's

01:07:34,500 --> 01:07:39,120
typical to combine a lag with this a

01:07:36,630 --> 01:07:42,180
bond and so then there would be two but

01:07:39,120 --> 01:07:43,290
that's kind of a extra complication we

01:07:42,180 --> 01:07:47,340
wanted to kind of simplify the

01:07:43,290 --> 01:07:52,700
discussion here a little but in the

01:07:47,340 --> 01:07:58,890
general case yes so what happens today

01:07:52,700 --> 01:08:00,390
if we try to set this up yeah there's a

01:07:58,890 --> 01:08:04,080
way we would have to ordinarily have to

01:08:00,390 --> 01:08:05,790
oviya so the input port would be e 0 it

01:08:04,080 --> 01:08:08,460
matches on that it's great maybe some

01:08:05,790 --> 01:08:10,830
other criteria as well and then the

01:08:08,460 --> 01:08:14,310
output the action is simply to output to

01:08:10,830 --> 01:08:18,589
the VLAN and the VLAN that there pushes

01:08:14,310 --> 01:08:22,109
on the packet sorry the encapsulation

01:08:18,589 --> 01:08:28,080
and then we need to match on this second

01:08:22,109 --> 01:08:31,380
rule in order to output it to the to the

01:08:28,080 --> 01:08:32,820
network to the physical network and so

01:08:31,380 --> 01:08:34,589
it would typically do this by matching

01:08:32,820 --> 01:08:37,260
on the input port which is in this case

01:08:34,589 --> 01:08:39,480
the bridge itself and maybe the source

01:08:37,260 --> 01:08:43,710
and destination address just to make it

01:08:39,480 --> 01:08:45,180
a little bit more fine-grained and the

01:08:43,710 --> 01:08:49,200
action would typically be output you

01:08:45,180 --> 01:08:51,390
might have other actions as well but so

01:08:49,200 --> 01:08:55,920
in the software so the problem is that

01:08:51,390 --> 01:08:59,130
the rule 2 is not being hit so proposed

01:08:55,920 --> 01:09:02,010
solution number 1 and I think this is

01:08:59,130 --> 01:09:04,500
the preferred solution at this time is

01:09:02,010 --> 01:09:07,100
to make the lines not straight to have

01:09:04,500 --> 01:09:07,100
that more curves

01:09:10,650 --> 01:09:14,470
so the fundamental problem for my point

01:09:13,180 --> 01:09:18,460
of view is that when you're processing

01:09:14,470 --> 01:09:23,560
these internal net devs the DTG hook is

01:09:18,460 --> 01:09:26,710
it's bypassed so the ingress DC hook and

01:09:23,560 --> 01:09:28,900
so may be obvious solution to this is

01:09:26,710 --> 01:09:34,090
instead of applying the filter to

01:09:28,900 --> 01:09:35,950
ingress to apply it to egress and in a

01:09:34,090 --> 01:09:38,470
sense we are egressing it's kind of

01:09:35,950 --> 01:09:40,660
confusing in my mind because the packet

01:09:38,470 --> 01:09:45,040
goes in and what's input on one side is

01:09:40,660 --> 01:09:47,530
output to another side but in a sense

01:09:45,040 --> 01:09:50,410
from the VX land- point of view we are

01:09:47,530 --> 01:09:52,300
egressing and also from a bridge point

01:09:50,410 --> 01:09:54,550
of view we're actually trying to push

01:09:52,300 --> 01:09:57,490
this packet out onto the physical

01:09:54,550 --> 01:10:00,130
network so so we're not in Italy you can

01:09:57,490 --> 01:10:04,450
kind of think of this as egress and if

01:10:00,130 --> 01:10:06,430
we do this it works things are nice what

01:10:04,450 --> 01:10:08,710
is required to achieve this what is the

01:10:06,430 --> 01:10:10,510
software changes required so the nice

01:10:08,710 --> 01:10:15,160
thing about this is it requires no

01:10:10,510 --> 01:10:17,470
changes to the kernel that it is maybe a

01:10:15,160 --> 01:10:21,340
driver change but no fundamental changes

01:10:17,470 --> 01:10:24,160
to OBS kernel data paths or DTC we just

01:10:21,340 --> 01:10:26,470
need to update user space in this case

01:10:24,160 --> 01:10:29,620
obvious user space to tell it to attach

01:10:26,470 --> 01:10:32,350
its filters to egress instead of ingress

01:10:29,620 --> 01:10:33,880
so this is kind of nice and this is kind

01:10:32,350 --> 01:10:35,740
of the way we're thinking of going and

01:10:33,880 --> 01:10:38,920
there we prototype this and it's working

01:10:35,740 --> 01:10:41,140
and now net next is open well

01:10:38,920 --> 01:10:44,020
if it's not knit Nexus doesn't it anyway

01:10:41,140 --> 01:10:47,740
we're ready to move forwards on this

01:10:44,020 --> 01:10:53,860
the other solution which so this is my

01:10:47,740 --> 01:10:55,720
colleagues idea my idea was this one so

01:10:53,860 --> 01:10:57,310
the result is that it's kind of the same

01:10:55,720 --> 01:10:59,950
it the difference is that we don't

01:10:57,310 --> 01:11:02,910
consider this to be egress we still

01:10:59,950 --> 01:11:05,860
consider it to be ingress and

01:11:02,910 --> 01:11:07,930
essentially what we do is we we teach

01:11:05,860 --> 01:11:11,350
the internal import implementation this

01:11:07,930 --> 01:11:16,000
magic function in the OVS direction to

01:11:11,350 --> 01:11:18,640
actually use the ingress hooks and again

01:11:16,000 --> 01:11:21,370
the result is pretty good both rules are

01:11:18,640 --> 01:11:24,970
processed and everything is fine the

01:11:21,370 --> 01:11:27,190
cost of this in implementation terms is

01:11:24,970 --> 01:11:29,650
we need to update the obvious kernel

01:11:27,190 --> 01:11:32,170
data path to teach it to use TC hooks

01:11:29,650 --> 01:11:34,300
which maybe is a correctness issue in

01:11:32,170 --> 01:11:37,290
moving things in a good direction but it

01:11:34,300 --> 01:11:41,050
does add extra complexity and extra cost

01:11:37,290 --> 01:11:42,880
to that piece of code only for this use

01:11:41,050 --> 01:11:45,490
case did there's no benefit to the

01:11:42,880 --> 01:11:47,680
software on the use case because they

01:11:45,490 --> 01:11:49,330
don't you wouldn't ordinarily use TC in

01:11:47,680 --> 01:11:52,360
that and I think I'm still struggling

01:11:49,330 --> 01:11:53,260
this port is just enter one side you

01:11:52,360 --> 01:11:58,030
come out the other side

01:11:53,260 --> 01:11:59,800
or you could it be a me read redirect to

01:11:58,030 --> 01:12:01,390
ingress based on a flow

01:11:59,800 --> 01:12:05,230
tag or something

01:12:01,390 --> 01:12:08,060
on the internal party of a myriad action

01:12:05,230 --> 01:12:10,900
looks at something in the in the packet

01:12:08,060 --> 01:12:13,190
metadata or data and decides which

01:12:10,900 --> 01:12:15,830
ingress hope you should go to by

01:12:13,190 --> 01:12:17,630
redirecting it there then it's just a

01:12:15,830 --> 01:12:20,740
policy issue right I mean doesn't make

01:12:17,630 --> 01:12:23,150
Intel report anything special basically

01:12:20,740 --> 01:12:26,360
or does doesn't make sense

01:12:23,150 --> 01:12:32,840
yes so the question in this approach we

01:12:26,360 --> 01:12:34,730
allow ingress a policy to be applied to

01:12:32,840 --> 01:12:41,600
packets that are received into the

01:12:34,730 --> 01:12:45,910
internal port obvious guy so I'm still

01:12:41,600 --> 01:12:48,350
struggling to understand it sounds more

01:12:45,910 --> 01:12:50,450
what you want the what you want to

01:12:48,350 --> 01:12:52,550
achieve is to redirect to some port

01:12:50,450 --> 01:12:54,230
bypassing all of TCP

01:12:52,550 --> 01:12:56,180
well that's not we want what we want to

01:12:54,230 --> 01:12:59,960
achieve that's what is implemented at

01:12:56,180 --> 01:13:03,490
this time right but you introduce this

01:12:59,960 --> 01:13:05,780
special port just for that purpose

01:13:03,490 --> 01:13:09,860
I'm not introducing these ports they

01:13:05,780 --> 01:13:11,450
exist okay so at some point the obvious

01:13:09,860 --> 01:13:16,730
developers introduced this thing right

01:13:11,450 --> 01:13:17,930
like you say yes okay and and I imagine

01:13:16,730 --> 01:13:20,650
that the motivation for the

01:13:17,930 --> 01:13:23,540
implementation by passing things is is

01:13:20,650 --> 01:13:26,480
for performance and that they had no new

01:13:23,540 --> 01:13:28,580
use case at the time this is before TC

01:13:26,480 --> 01:13:31,190
was used in conjunction with OBS they

01:13:28,580 --> 01:13:33,890
would have had no use case for this

01:13:31,190 --> 01:13:36,340
and in even though we've had obvious tc4

01:13:33,890 --> 01:13:38,330
for some years now it's only now that

01:13:36,340 --> 01:13:43,010
we're trying to implement more

01:13:38,330 --> 01:13:44,600
complicated features of obvious that

01:13:43,010 --> 01:13:46,750
we're running against these kind of

01:13:44,600 --> 01:13:46,750
problems

01:13:50,070 --> 01:13:56,460
so actually what is the use case is the

01:13:52,679 --> 01:13:58,110
host trying to talk be IBM so in this

01:13:56,460 --> 01:14:00,440
case it would be the the packet is

01:13:58,110 --> 01:14:00,440
coming

01:14:04,000 --> 01:14:09,100
typically the packet would be coming

01:14:05,949 --> 01:14:13,150
from a VM and then through the obvious

01:14:09,100 --> 01:14:14,680
and then being put out onto the wire but

01:14:13,150 --> 01:14:16,630
when it's put out into the wire its

01:14:14,680 --> 01:14:18,910
encapsulated in in in some tunneling

01:14:16,630 --> 01:14:27,300
protocol in this case v excellent but it

01:14:18,910 --> 01:14:27,300
could be GRE or anything else is outside

01:14:27,570 --> 01:14:30,750
[Music]

01:14:31,110 --> 01:14:37,989
so the internal for you

01:14:35,710 --> 01:14:39,719
it's kind of like the bridge itself so

01:14:37,989 --> 01:14:42,400
the packet is coming from the VM is

01:14:39,719 --> 01:14:45,969
traversing the bridge actually twice and

01:14:42,400 --> 01:14:47,500
then it's going out to the network and

01:14:45,969 --> 01:14:50,260
the reason it's going through twice is

01:14:47,500 --> 01:14:54,880
one is to select the fact that it's

01:14:50,260 --> 01:14:59,010
going to be going out of tunnel and then

01:14:54,880 --> 01:14:59,010
the tunnel endpoint itself is the bridge

01:15:01,050 --> 01:15:07,660
and so the you might say we'll put the

01:15:05,680 --> 01:15:09,850
tunnel endpoint somewhere else and sure

01:15:07,660 --> 01:15:12,219
you can do that but the thing is that

01:15:09,850 --> 01:15:15,630
this is kind of the way people use this

01:15:12,219 --> 01:15:15,630
software stack at this time already

01:15:20,030 --> 01:15:26,990
so I feel like I'm not understanding

01:15:22,250 --> 01:15:28,400
your question I'm just looking at and

01:15:26,990 --> 01:15:30,230
I'm saying that you know something

01:15:28,400 --> 01:15:33,890
coming off the network stacks needs to

01:15:30,230 --> 01:15:36,080
be redirected to something that may be

01:15:33,890 --> 01:15:38,180
encapsulating the exelon for itself is

01:15:36,080 --> 01:15:41,090
right and VX line is riding on top of

01:15:38,180 --> 01:15:43,760
some physical eighth device no no eh

01:15:41,090 --> 01:15:46,310
plan is riding on this is the key point

01:15:43,760 --> 01:15:48,080
the first part is correct and right this

01:15:46,310 --> 01:15:50,810
is not really a contentious problem

01:15:48,080 --> 01:15:54,320
right the thing is that the VX line is

01:15:50,810 --> 01:15:56,300
sitting not on 8th one write VX line is

01:15:54,320 --> 01:15:58,640
sitting on top of the bridge okay so the

01:15:56,300 --> 01:16:01,010
packet comes in to its zero hits VX land

01:15:58,640 --> 01:16:04,220
gets D capsulated I guess if you are the

01:16:01,010 --> 01:16:07,700
way it's encapsulated hits and there

01:16:04,220 --> 01:16:09,280
needs to be redirected to each one so

01:16:07,700 --> 01:16:12,410
this is you get run capsulated

01:16:09,280 --> 01:16:13,880
encapsulated with the px90 yep and then

01:16:12,410 --> 01:16:18,380
it needs to yes you say it needs to be

01:16:13,880 --> 01:16:20,360
redirected to 8th one okay mal to your

01:16:18,380 --> 01:16:23,270
question we only deal with short tunnel

01:16:20,360 --> 01:16:25,400
devices with TC we don't when we

01:16:23,270 --> 01:16:27,770
introduce tunnel key action we only

01:16:25,400 --> 01:16:30,650
doing that for sure tunnel devices we

01:16:27,770 --> 01:16:32,720
don't do it further all style devices

01:16:30,650 --> 01:16:35,270
this one thing yeah so what's not shown

01:16:32,720 --> 01:16:37,730
here and Simon maybe I don't know if you

01:16:35,270 --> 01:16:39,620
yeah like maybe people try here it is

01:16:37,730 --> 01:16:41,500
just hey why don't we why do we need

01:16:39,620 --> 01:16:44,630
that so if you if you can

01:16:41,500 --> 01:16:47,120
describe the use case which is not

01:16:44,630 --> 01:16:48,980
covered by today's the way we do offload

01:16:47,120 --> 01:16:51,560
today net Renault in Milan's drivers or

01:16:48,980 --> 01:16:53,870
I don't think the broad comes already or

01:16:51,560 --> 01:16:56,750
floats so oftentimes people are using

01:16:53,870 --> 01:16:59,150
OpenStack to configure obvious in

01:16:56,750 --> 01:17:02,000
production and in which case it's very

01:16:59,150 --> 01:17:04,429
common to place it's all about where the

01:17:02,000 --> 01:17:08,080
endpoint of the tunnel goes and to place

01:17:04,429 --> 01:17:11,060
the end point onto the bridge itself is

01:17:08,080 --> 01:17:13,670
like canonical OpenStack way of doing

01:17:11,060 --> 01:17:18,380
that and it's in that case that you hit

01:17:13,670 --> 01:17:20,449
this problem so it's just to cut it to

01:17:18,380 --> 01:17:21,949
existing bad habits out there with

01:17:20,449 --> 01:17:26,150
OpenStack that is one possible

01:17:21,949 --> 01:17:27,860
interpretation okay it's like we are you

01:17:26,150 --> 01:17:31,699
are P you API is right we can't take

01:17:27,860 --> 01:17:35,300
them back now is that so it's like we

01:17:31,699 --> 01:17:37,400
have a kernel which is exposing various

01:17:35,300 --> 01:17:40,219
different ways of doing things and the

01:17:37,400 --> 01:17:42,530
people over in OpenStack decided for

01:17:40,219 --> 01:17:46,250
some reason I hope to utilize a certain

01:17:42,530 --> 01:17:47,719
subset of those features and that's this

01:17:46,250 --> 01:17:50,989
subject

01:17:47,719 --> 01:17:53,689
well I I guess I looked very complex to

01:17:50,989 --> 01:17:58,969
me but I you have a use case someone

01:17:53,689 --> 01:18:00,709
wants you know it make its legit from

01:17:58,969 --> 01:18:02,840
that perspective it seems like you have

01:18:00,709 --> 01:18:04,550
a use case is I still not quite get it

01:18:02,840 --> 01:18:07,639
it looks like you know all these link

01:18:04,550 --> 01:18:12,050
lines could be replaced by two by two

01:18:07,639 --> 01:18:13,369
lines so it might have been the

01:18:12,050 --> 01:18:14,989
complication in the diagram comes

01:18:13,369 --> 01:18:17,300
because in fact the packet loops around

01:18:14,989 --> 01:18:19,610
but I've might have expressed the

01:18:17,300 --> 01:18:20,630
diagram better by drawing two separate

01:18:19,610 --> 01:18:23,889
bridges even though they're the same

01:18:20,630 --> 01:18:27,229
bridge which would be the same use case

01:18:23,889 --> 01:18:31,760
from your software implementation point

01:18:27,229 --> 01:18:33,650
of view the color patches for or what

01:18:31,760 --> 01:18:36,650
what is the suggestion so the suggestion

01:18:33,650 --> 01:18:38,809
is to go to the previous option where

01:18:36,650 --> 01:18:40,099
there's no kind of changes we have to do

01:18:38,809 --> 01:18:42,530
a little bit of work in the driver to

01:18:40,099 --> 01:18:44,619
actually offloaded there's no course

01:18:42,530 --> 01:18:48,320
tack changes which is one of the main

01:18:44,619 --> 01:18:50,539
attractions of this what's on the

01:18:48,320 --> 01:18:53,510
following slide one we were just staring

01:18:50,539 --> 01:18:56,869
at requires changes to the OBS data path

01:18:53,510 --> 01:19:01,150
to basically teach it to use the TTC

01:18:56,869 --> 01:19:02,830
ingress that will make more sense for

01:19:01,150 --> 01:19:06,190
so whatever feature you end up adding

01:19:02,830 --> 01:19:09,310
could be used by TC as well yes so the

01:19:06,190 --> 01:19:15,730
ingress idea was my so of course I think

01:19:09,310 --> 01:19:21,520
it's better but but also good but this

01:19:15,730 --> 01:19:23,760
egress idea is seems to have more

01:19:21,520 --> 01:19:23,760
traction

01:19:25,260 --> 01:19:34,379
yes or seems to be nodding in agreement

01:19:29,070 --> 01:19:39,119
or you guys must have discussed

01:19:34,379 --> 01:19:42,030
so someone submitted it and some people

01:19:39,119 --> 01:19:46,050
will review it I think we already

01:19:42,030 --> 01:19:49,760
submitted the this version it's a on

01:19:46,050 --> 01:19:49,760
obvious list I think you've seen okay

01:19:50,550 --> 01:19:57,210
okay so I think the conclusion is semi

01:19:55,530 --> 01:20:01,490
inconclusive but we will try to move

01:19:57,210 --> 01:20:06,480
forwards on on this unless someone I

01:20:01,490 --> 01:20:08,010
don't disagree violently I don't

01:20:06,480 --> 01:20:09,720
understand it enough to disagree okay

01:20:08,010 --> 01:20:13,400
right we can we can talk about this

01:20:09,720 --> 01:20:18,690
again thanks to Sun agreed because I

01:20:13,400 --> 01:20:22,140
think we're a bit o minus one Lucas is

01:20:18,690 --> 01:20:27,600
here thank you

01:20:22,140 --> 01:20:27,600
[Applause]

01:20:36,620 --> 01:20:45,400
excuse connect a laptop

01:20:40,460 --> 01:20:45,400
over there okay give HDMI yeah okay

01:20:54,569 --> 01:20:57,079
No

01:21:33,650 --> 01:21:41,180
all right so a couple of well a few new

01:21:39,230 --> 01:21:45,490
developments that's I've been working on

01:21:41,180 --> 01:21:47,870
lately I'm in the middle of pushing some

01:21:45,490 --> 01:21:50,000
quality of life improvements to TDC

01:21:47,870 --> 01:21:52,280
up to the kernel one probably about

01:21:50,000 --> 01:21:55,850
halfway through those right now a lot of

01:21:52,280 --> 01:21:57,110
them are just reporting improvements so

01:21:55,850 --> 01:21:58,700
now you know when something fails you

01:21:57,110 --> 01:22:03,850
can actually see what the commands being

01:21:58,700 --> 01:22:06,890
executed up to that point were what's

01:22:03,850 --> 01:22:10,820
I've actually gone and done and I'm

01:22:06,890 --> 01:22:12,680
going to present today and this will go

01:22:10,820 --> 01:22:15,980
in as soon as I'm finished

01:22:12,680 --> 01:22:18,620
pushing up those other changes we now

01:22:15,980 --> 01:22:22,100
actually have the ability to send

01:22:18,620 --> 01:22:25,790
traffic and validates commands that are

01:22:22,100 --> 01:22:27,620
being or validate that the commands

01:22:25,790 --> 01:22:29,660
under tests are actually working as

01:22:27,620 --> 01:22:32,300
intended so you can actually see if a

01:22:29,660 --> 01:22:35,410
rule is hits or if it's not hit based on

01:22:32,300 --> 01:22:41,470
the kind of packet you're sending so

01:22:35,410 --> 01:22:45,410
there's the method I'm doing for that is

01:22:41,470 --> 01:22:49,580
I'm actually using Scotty as of as a

01:22:45,410 --> 01:22:52,400
plugin for TDC which you can actually

01:22:49,580 --> 01:22:54,670
see here in the sample test case that I

01:22:52,400 --> 01:22:54,670
wrote

01:22:54,739 --> 01:22:57,430
so

01:22:58,219 --> 01:23:03,500
the packet will actually be fired in the

01:23:01,010 --> 01:23:07,130
if you're familiar with the general

01:23:03,500 --> 01:23:09,590
phases of TVC test cases or test case

01:23:07,130 --> 01:23:11,540
execution it's in the post execute so it

01:23:09,590 --> 01:23:14,780
will actually come right after the

01:23:11,540 --> 01:23:18,800
Commandant under test is executed so as

01:23:14,780 --> 01:23:23,330
long as that goes okay then TDC will

01:23:18,800 --> 01:23:25,010
come along and then execute what it

01:23:23,330 --> 01:23:28,160
discovers in the scapula

01:23:25,010 --> 01:23:30,680
so we're specifying an interface that we

01:23:28,160 --> 01:23:32,480
want to do it on that's actually hedging

01:23:30,680 --> 01:23:37,760
against future improvements where we

01:23:32,480 --> 01:23:39,910
have more complex setups so right now

01:23:37,760 --> 01:23:44,510
this is actually just firing out of the

01:23:39,910 --> 01:23:46,130
hostname space into the container how

01:23:44,510 --> 01:23:49,610
many times you want the packet to run

01:23:46,130 --> 01:23:51,560
and then this string so this is actually

01:23:49,610 --> 01:23:53,930
evaluated directly by SCAP ii after

01:23:51,560 --> 01:23:58,000
being passed in by the plugin so you can

01:23:53,930 --> 01:23:58,000
construct a packet with

01:23:58,300 --> 01:24:07,280
very specific configuration whatever you

01:24:05,239 --> 01:24:09,380
don't specify it actually gets filled in

01:24:07,280 --> 01:24:13,309
with defaults so you notice I'm only

01:24:09,380 --> 01:24:17,260
actually specifying at the IP level the

01:24:13,309 --> 01:24:19,639
source address it'll be filled in with a

01:24:17,260 --> 01:24:20,659
generic destination address it doesn't

01:24:19,639 --> 01:24:23,449
actually matter because we're only

01:24:20,659 --> 01:24:28,170
testing that's you know this rule is

01:24:23,449 --> 01:24:29,970
being hit okay

01:24:28,170 --> 01:24:32,880
so then that brings on to the next

01:24:29,970 --> 01:24:35,460
improvement that I made and this sort of

01:24:32,880 --> 01:24:39,330
came at the prompting of Steven Hebner

01:24:35,460 --> 01:24:41,610
and a message on the net that mailing

01:24:39,330 --> 01:24:45,450
lists a little while back and that's the

01:24:41,610 --> 01:24:48,180
ability to actually match using the JSON

01:24:45,450 --> 01:24:51,480
output from TC rather than regular

01:24:48,180 --> 01:24:53,370
expressions because they are incredibly

01:24:51,480 --> 01:24:56,040
squishy and prone to failing whenever

01:24:53,370 --> 01:24:59,070
there's a change in the change in the

01:24:56,040 --> 01:25:04,580
output whereas the JSON output is not

01:24:59,070 --> 01:25:07,680
actually going to change that much so

01:25:04,580 --> 01:25:11,040
there's a new structure in here it will

01:25:07,680 --> 01:25:12,630
actually replace I haven't deprecated

01:25:11,040 --> 01:25:14,100
the regular expression pattern-matching

01:25:12,630 --> 01:25:15,930
because one thing I discovered is that

01:25:14,100 --> 01:25:19,380
it is actually still useful in some

01:25:15,930 --> 01:25:23,400
cases but now if you provide this match

01:25:19,380 --> 01:25:28,620
JSON block instead you specify the path

01:25:23,400 --> 01:25:30,690
like where in the JSON the value that

01:25:28,620 --> 01:25:32,179
you want to look for is and then the

01:25:30,690 --> 01:25:37,250
actual value

01:25:32,179 --> 01:25:39,320
okay so now we're actually going to look

01:25:37,250 --> 01:25:45,650
with this test case we're going to see

01:25:39,320 --> 01:25:50,390
that was was this rule actually hit so

01:25:45,650 --> 01:25:55,520
did we see a packet from source IP 1661

01:25:50,390 --> 01:25:58,340
1661 okay and actually just for

01:25:55,520 --> 01:26:02,140
comparison we've got another one but

01:25:58,340 --> 01:26:04,370
it's identical but we're looking for

01:26:02,140 --> 01:26:06,860
we're firing off the packed three times

01:26:04,370 --> 01:26:09,850
as opposed to looking for just one so

01:26:06,860 --> 01:26:09,850
you can see the results there

01:26:24,199 --> 01:26:31,719
what are you doing there so I'm actually

01:26:26,929 --> 01:26:31,719
just running TVC with the test

01:26:33,030 --> 01:26:37,500
while with these two test cases okay so

01:26:35,760 --> 01:26:39,920
you can see the results so first one

01:26:37,500 --> 01:26:42,960
passed as intended

01:26:39,920 --> 01:26:44,190
second one sorry what is the first test

01:26:42,960 --> 01:26:45,450
doing I can see that

01:26:44,190 --> 01:26:47,989
oh we're just matching against the

01:26:45,450 --> 01:26:51,120
specific okay you send you send a packet

01:26:47,989 --> 01:26:54,050
specifically crafted packet or yes okay

01:26:51,120 --> 01:27:00,690
yeah what did you say into himself well

01:26:54,050 --> 01:27:02,400
what was the packet that you sent okay

01:27:00,690 --> 01:27:07,080
so if we look at TCP dump we'll see you

01:27:02,400 --> 01:27:08,700
sending that back yeah okay okay so the

01:27:07,080 --> 01:27:12,900
second test case we're actually looking

01:27:08,700 --> 01:27:14,730
to see that it only hit one time and but

01:27:12,900 --> 01:27:16,950
we actually sent it three times so you

01:27:14,730 --> 01:27:21,750
can see that this test case did indeed

01:27:16,950 --> 01:27:23,400
fail because they counted three packets

01:27:21,750 --> 01:27:29,250
matching that IP address

01:27:23,400 --> 01:27:31,260
thank you so this is still just kind of

01:27:29,250 --> 01:27:37,320
proof of concept mostly because it

01:27:31,260 --> 01:27:39,930
requires adding new things to the to the

01:27:37,320 --> 01:27:41,100
test case definition so whenever I have

01:27:39,930 --> 01:27:43,530
to add something there I get a little

01:27:41,100 --> 01:27:45,660
paranoid and worried that I may be

01:27:43,530 --> 01:27:49,290
complicated the design too much so I

01:27:45,660 --> 01:27:52,350
will be looking for feedback on whether

01:27:49,290 --> 01:27:53,820
this looks reasonable to everyone if

01:27:52,350 --> 01:27:55,440
there are changes I'm happy to make them

01:27:53,820 --> 01:28:00,510
before I push them up

01:27:55,440 --> 01:28:03,390
I can also basically put this patch out

01:28:00,510 --> 01:28:05,880
as an RFC just so people can

01:28:03,390 --> 01:28:08,460
you know check it over there will be

01:28:05,880 --> 01:28:10,080
some really weird things that you might

01:28:08,460 --> 01:28:12,030
see in the code especially for the JSON

01:28:10,080 --> 01:28:14,010
matching because I'm trying to follow

01:28:12,030 --> 01:28:16,620
the TVC philosophy of limiting the

01:28:14,010 --> 01:28:19,770
number of external Python packages that

01:28:16,620 --> 01:28:21,810
are required to run it so I had to

01:28:19,770 --> 01:28:23,850
unfortunately invent some of my own code

01:28:21,810 --> 01:28:25,350
for checking this one I know that there

01:28:23,850 --> 01:28:29,400
is already stuff in place that will do

01:28:25,350 --> 01:28:31,500
this so I do apologize but like I said

01:28:29,400 --> 01:28:38,520
at this point I am just seeking feedback

01:28:31,500 --> 01:28:40,410
for the new control options basically so

01:28:38,520 --> 01:28:43,530
I think it'd be interesting to create

01:28:40,410 --> 01:28:45,990
some sort of fuzzy testing for you for

01:28:43,530 --> 01:28:48,090
the different classifiers yeah different

01:28:45,990 --> 01:28:51,660
actions we just send a bunch of crap and

01:28:48,090 --> 01:28:54,090
see what happen alters yeah yeah so that

01:28:51,660 --> 01:28:55,620
could be done now - it could be done

01:28:54,090 --> 01:28:57,330
with this so you need another tool I

01:28:55,620 --> 01:29:01,740
don't think you would need another tool

01:28:57,330 --> 01:29:04,650
for that okay yeah now I mean it's still

01:29:01,740 --> 01:29:06,540
working under the TDC paradigm if you

01:29:04,650 --> 01:29:10,350
know like tests and that's one command

01:29:06,540 --> 01:29:12,030
at a time but you know you can maybe

01:29:10,350 --> 01:29:13,830
fire a whole bunch of different like

01:29:12,030 --> 01:29:16,080
wild and crazy packets and see if any of

01:29:13,830 --> 01:29:17,560
them actually hit a role when they're

01:29:16,080 --> 01:29:19,120
not supposed to

01:29:17,560 --> 01:29:26,370
if you have reason to believe that's the

01:29:19,120 --> 01:29:26,370
thing okay any questions questions

01:29:27,769 --> 01:29:36,079
oh wow we're gonna make it so sorry yeah

01:29:30,980 --> 01:29:38,360
oh yeah sorry glad I bit of a delay you

01:29:36,079 --> 01:29:40,670
both first off thank you for working on

01:29:38,360 --> 01:29:43,670
the test very important and then

01:29:40,670 --> 01:29:46,039
secondly we found some issues with using

01:29:43,670 --> 01:29:47,920
some of the default values in escapee

01:29:46,039 --> 01:29:52,340
for some of the tests you might want to

01:29:47,920 --> 01:29:55,039
look at look at that you you might not

01:29:52,340 --> 01:29:57,320
run into it but with with our offloads

01:29:55,039 --> 01:29:58,969
have I've run into someone so for

01:29:57,320 --> 01:30:01,909
instance the default destination back at

01:29:58,969 --> 01:30:06,050
ghost could be an issue yeah I actually

01:30:01,909 --> 01:30:09,289
did have I did have some problems with

01:30:06,050 --> 01:30:12,019
SCAP II because apparently in like a

01:30:09,289 --> 01:30:15,469
very short multi month or six month

01:30:12,019 --> 01:30:17,929
period just between the latest version

01:30:15,469 --> 01:30:20,230
that's available on gift first copy and

01:30:17,929 --> 01:30:23,480
the last version I was available through

01:30:20,230 --> 01:30:25,789
Python installer it was completely

01:30:23,480 --> 01:30:27,469
incapable of the older version was

01:30:25,789 --> 01:30:31,579
completely incapable of crafting IFE

01:30:27,469 --> 01:30:33,769
packets so it would still accept it it

01:30:31,579 --> 01:30:36,269
thought it was doing well but no it was

01:30:33,769 --> 01:30:39,420
producing garbage so yeah

01:30:36,269 --> 01:30:43,050
sort of understand and a car woman

01:30:39,420 --> 01:30:45,030
remember now but would would Scapa

01:30:43,050 --> 01:30:48,570
didn't be one of the dependencies that

01:30:45,030 --> 01:30:49,860
that you've added to the test so in

01:30:48,570 --> 01:30:50,429
other words you need to have this kept

01:30:49,860 --> 01:30:53,309
in store

01:30:50,429 --> 01:30:57,860
yes okay that's that I think that's fair

01:30:53,309 --> 01:31:01,110
we're in a Python in there so yeah yeah

01:30:57,860 --> 01:31:02,969
it's just to make it as easy as possible

01:31:01,110 --> 01:31:05,369
I do try to limits the amount of

01:31:02,969 --> 01:31:07,889
external packages because it's like you

01:31:05,369 --> 01:31:09,599
go on like say stock exchange how do I

01:31:07,889 --> 01:31:11,699
do this Python oh look there's a package

01:31:09,599 --> 01:31:15,119
for it but that it's not part of the

01:31:11,699 --> 01:31:16,980
core system so in trying to and then the

01:31:15,119 --> 01:31:20,969
last question not all of the I think not

01:31:16,980 --> 01:31:24,809
all of the the TC rules have have been

01:31:20,969 --> 01:31:28,260
Jason fired are you aware of that yes

01:31:24,809 --> 01:31:29,940
yeah I'm very aware of that I try to

01:31:28,260 --> 01:31:32,550
make them as I can but I'm still

01:31:29,940 --> 01:31:34,260
actually really depending on everyone

01:31:32,550 --> 01:31:37,739
else to help out because I don't

01:31:34,260 --> 01:31:40,829
actually code for like I don't submit

01:31:37,739 --> 01:31:42,420
code for the kernel affecting the CC

01:31:40,829 --> 01:31:44,099
subsystem so I actually depend on the

01:31:42,420 --> 01:31:45,539
people who are writing the code you may

01:31:44,099 --> 01:31:49,050
be help me come up with these tests and

01:31:45,539 --> 01:31:51,030
submit them okay so yes as Vlad was

01:31:49,050 --> 01:31:53,130
saying earlier knows people submitting

01:31:51,030 --> 01:31:58,199
patches better submit called test cases

01:31:53,130 --> 01:32:00,690
as well yeah and I mean there's a lot of

01:31:58,199 --> 01:32:02,460
test cases to be written I just thought

01:32:00,690 --> 01:32:04,620
of fuzzing for example that's a very

01:32:02,460 --> 01:32:14,880
interesting scenario

01:32:04,620 --> 01:32:14,880
next time thank you thanks Romagna next

01:32:19,949 --> 01:32:25,969
okay five minutes I think it's we have

01:32:22,860 --> 01:32:25,969
one minute before the breaks

01:32:26,390 --> 01:32:31,460
hi I'm Roman from maan gelato networks

01:32:29,219 --> 01:32:35,280
so a while back we were involved in

01:32:31,460 --> 01:32:38,460
creating designing a TC based billing

01:32:35,280 --> 01:32:41,850
system for one of North American service

01:32:38,460 --> 01:32:44,760
service providers and as part of that

01:32:41,850 --> 01:32:48,210
work we came up with a new TC action

01:32:44,760 --> 01:32:52,170
called the quota enforcement QE action

01:32:48,210 --> 01:32:56,460
so the main idea of that thing is that

01:32:52,170 --> 01:33:00,650
every subscriber has its own water limit

01:32:56,460 --> 01:33:00,650
or credit limit assigned in bytes and

01:33:01,550 --> 01:33:07,710
it's like a meter as a packet hits this

01:33:05,000 --> 01:33:12,000
action the quarter is decremented by the

01:33:07,710 --> 01:33:13,380
size of a packet if it exceeds then the

01:33:12,000 --> 01:33:16,940
packet will be dropped

01:33:13,380 --> 01:33:19,980
otherwise it passes but it can be

01:33:16,940 --> 01:33:24,540
reprogrammed in the in order to create

01:33:19,980 --> 01:33:26,580
and construct more sophisticated service

01:33:24,540 --> 01:33:30,120
graphs for example instead of simply

01:33:26,580 --> 01:33:34,050
dropping the packet we may apply a

01:33:30,120 --> 01:33:42,230
police or simply squeeze the user data

01:33:34,050 --> 01:33:45,330
pipe to some low value so this action

01:33:42,230 --> 01:33:47,280
depends on three main parameters first

01:33:45,330 --> 01:33:51,510
is the quote as I said which is defined

01:33:47,280 --> 01:33:56,100
in bytes second is the overhead overhead

01:33:51,510 --> 01:33:58,500
is essentially it's not fair to charge

01:33:56,100 --> 01:34:00,890
user for for example Ethernet header so

01:33:58,500 --> 01:34:03,750
we deduct this from the

01:34:00,890 --> 01:34:09,150
from the we do not deduct this from the

01:34:03,750 --> 01:34:11,280
quota and these third one is the

01:34:09,150 --> 01:34:15,710
multiplier multiplier is interesting

01:34:11,280 --> 01:34:19,860
thing some data plans allow for example

01:34:15,710 --> 01:34:21,480
free traffic on at night in this case

01:34:19,860 --> 01:34:23,250
multiplier will be zero so we don't

01:34:21,480 --> 01:34:27,450
charge anything the quota remains the

01:34:23,250 --> 01:34:30,060
same or a user can be charged half the

01:34:27,450 --> 01:34:33,060
price on weekends so in this case this

01:34:30,060 --> 01:34:38,490
multiplier is 0.5 sort of thing so in

01:34:33,060 --> 01:34:42,240
this case he pays just half the price in

01:34:38,490 --> 01:34:46,280
addition to TC standards stats this

01:34:42,240 --> 01:34:49,800
action also collects the starts your

01:34:46,280 --> 01:34:53,760
direction uplink and downlink because in

01:34:49,800 --> 01:34:56,820
many plans they have different quarters

01:34:53,760 --> 01:34:59,640
for the upstream of the downstream so

01:34:56,820 --> 01:35:04,370
and as a billing system poles this

01:34:59,640 --> 01:35:04,370
section it will make these calculations

01:35:07,890 --> 01:35:15,280
yes so this has been deployed in the

01:35:12,070 --> 01:35:17,950
data center or that service provider for

01:35:15,280 --> 01:35:20,430
quite a long time and we are planning to

01:35:17,950 --> 01:35:26,100
upstream this in the next few months

01:35:20,430 --> 01:35:28,210
after some cleanup and everything so I

01:35:26,100 --> 01:35:30,730
don't have time to demonstrate it now

01:35:28,210 --> 01:35:32,610
but whoever is interested can drop by my

01:35:30,730 --> 01:35:38,260
desk and I can show you on my laptop

01:35:32,610 --> 01:35:44,590
give you action in action that's it

01:35:38,260 --> 01:35:46,390
that's it thank you on time so can I

01:35:44,590 --> 01:35:47,680
make a suggestion that the stats maybe

01:35:46,390 --> 01:35:49,840
we should have a discussion outside of

01:35:47,680 --> 01:35:53,560
here at some point if everybody's around

01:35:49,840 --> 01:35:58,120
and the TDC tests I don't know if you

01:35:53,560 --> 01:36:03,820
get involved or how you I think all the

01:35:58,120 --> 01:36:05,980
players are here my cello look us maybe

01:36:03,820 --> 01:36:07,900
we can get together as well right if you

01:36:05,980 --> 01:36:12,180
guys could meet maybe in it's best to

01:36:07,900 --> 01:36:12,180
meet face-to-face thank you

01:36:13,410 --> 01:36:16,580

YouTube URL: https://www.youtube.com/watch?v=49yYdqhNgyU


