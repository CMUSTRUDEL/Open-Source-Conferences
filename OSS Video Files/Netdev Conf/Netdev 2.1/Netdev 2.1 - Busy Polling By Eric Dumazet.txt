Title: Netdev 2.1 - Busy Polling By Eric Dumazet
Publication date: 2017-05-07
Playlist: Netdev 2.1
Description: 
	In this talk given at Netdev 2.1 in Montreal on the 6th of April,
2017,  Eric Dumazet  presents the networking busy polling subsystem principles; the history of the work done and the ongoing effort to get it in better shape.

Content:
https://www.netdevconf.org/2.1/session.html?dumazet
Captions: 
	00:00:00,000 --> 00:00:07,020
hi my name is Eric Timothy I'm working

00:00:02,820 --> 00:00:09,540
at Google so I'm here to present the key

00:00:07,020 --> 00:00:13,200
what is visible in what has been done

00:00:09,540 --> 00:00:16,680
and what should be done later so first

00:00:13,200 --> 00:00:23,550
first things I want to thank everybody

00:00:16,680 --> 00:00:26,070
who work on this since 2012 just run the

00:00:23,550 --> 00:00:27,869
books that started the effort and it

00:00:26,070 --> 00:00:31,080
took like five years to get to this

00:00:27,869 --> 00:00:36,329
point so it was probably not that easy

00:00:31,080 --> 00:00:41,100
right so do people in what is that so

00:00:36,329 --> 00:00:44,850
busy polling really is the fact that the

00:00:41,100 --> 00:00:47,280
current model relied on in jobs Linux

00:00:44,850 --> 00:00:51,449
started you know with uniprocessor

00:00:47,280 --> 00:00:55,110
system 25 years ago at a time we really

00:00:51,449 --> 00:00:58,590
had to split the cpu cycles between

00:00:55,110 --> 00:01:03,210
application kernel networking disk

00:00:58,590 --> 00:01:07,619
whatever so this model has survived up

00:01:03,210 --> 00:01:10,380
to now it's still used heavily we rely

00:01:07,619 --> 00:01:12,810
on the fact that a device will generate

00:01:10,380 --> 00:01:15,270
an interrupt and we have kind of

00:01:12,810 --> 00:01:18,270
complicated way to propagate this

00:01:15,270 --> 00:01:21,600
interrupt up to the application doing a

00:01:18,270 --> 00:01:24,270
redesign code right so the idea of

00:01:21,600 --> 00:01:31,040
delivering really is to try to shortcut

00:01:24,270 --> 00:01:34,049
some of the stage so to recap this stage

00:01:31,040 --> 00:01:37,020
how it done right now you you have this

00:01:34,049 --> 00:01:38,579
in incoming Network message dni from

00:01:37,020 --> 00:01:41,759
unique to host memory and then

00:01:38,579 --> 00:01:45,119
eventually the NIC will signal insert to

00:01:41,759 --> 00:01:48,689
the host and because of some various

00:01:45,119 --> 00:01:50,790
mitigation in place we don't want to

00:01:48,689 --> 00:01:54,409
signal one interpreter packets because

00:01:50,790 --> 00:01:57,750
enjoy processing is kind of expensive so

00:01:54,409 --> 00:02:00,540
Linux has this model of hard interrupt

00:01:57,750 --> 00:02:03,630
software interrupts and all kind of

00:02:00,540 --> 00:02:06,600
loops trying to bash the bit processing

00:02:03,630 --> 00:02:09,629
so that we try to DQ multiple packets

00:02:06,600 --> 00:02:11,640
instead of a single at a time and let's

00:02:09,629 --> 00:02:12,220
follow receive but also for advanced mix

00:02:11,640 --> 00:02:17,200
so

00:02:12,220 --> 00:02:18,850
this kind of control loop where we want

00:02:17,200 --> 00:02:21,310
some batching but not too much because

00:02:18,850 --> 00:02:27,330
we had too much betting then you have

00:02:21,310 --> 00:02:27,330
extra latencies as you interrupt like a

00:02:28,080 --> 00:02:35,320
user application and thread that expect

00:02:30,640 --> 00:02:39,510
some lower returns is right so in red

00:02:35,320 --> 00:02:41,350
here that the two part of might be quite

00:02:39,510 --> 00:02:43,480
long actually

00:02:41,350 --> 00:02:47,890
so that should measure source of the

00:02:43,480 --> 00:02:52,240
jitter on the system so unbelievable try

00:02:47,890 --> 00:02:55,440
to remove these two bread paths so if

00:02:52,240 --> 00:02:58,300
you I continue the the stack here the

00:02:55,440 --> 00:03:00,130
ultimately the soft interrupt handler we

00:02:58,300 --> 00:03:02,950
call an at4 and the nappy poor

00:03:00,130 --> 00:03:05,890
eventually we see the IP and TCP UDP

00:03:02,950 --> 00:03:07,510
stack with the incoming packet and even

00:03:05,890 --> 00:03:10,450
tree wake up the application if the

00:03:07,510 --> 00:03:15,520
application is blocked on the receiver

00:03:10,450 --> 00:03:24,060
or found deep or in event to the select

00:03:15,520 --> 00:03:29,590
or EEP or aware loops so why is this

00:03:24,060 --> 00:03:31,900
traditional problematic as I said

00:03:29,590 --> 00:03:35,709
earlier this model was designed at the

00:03:31,900 --> 00:03:37,840
time we had a few CPUs and we had to be

00:03:35,709 --> 00:03:41,170
really careful how we were scraping the

00:03:37,840 --> 00:03:45,610
CPU cycles between the kernel and legal

00:03:41,170 --> 00:03:47,980
space but nowadays it this constraint is

00:03:45,610 --> 00:03:49,330
no or not really relevant anymore we

00:03:47,980 --> 00:03:52,570
have a lot of course

00:03:49,330 --> 00:03:55,060
so I'm labeling this as CPU because it's

00:03:52,570 --> 00:03:57,400
obviously a bit hard to tell what is the

00:03:55,060 --> 00:03:58,150
CPU so that's exactly an eye per thread

00:03:57,400 --> 00:04:01,180
as you want

00:03:58,150 --> 00:04:06,700
that's the CPU you you have in proxy you

00:04:01,180 --> 00:04:09,190
info okay that's a unit of execution so

00:04:06,700 --> 00:04:13,000
maybe we could try something else and

00:04:09,190 --> 00:04:18,250
maybe we want to dedicate some of this

00:04:13,000 --> 00:04:20,680
if you to control this one part of the

00:04:18,250 --> 00:04:23,830
stage of the pipeline meaning that we

00:04:20,680 --> 00:04:24,680
don't really rely on interrupts and we

00:04:23,830 --> 00:04:28,970
don't

00:04:24,680 --> 00:04:35,449
terrible injury mitigation so that's the

00:04:28,970 --> 00:04:37,820
bit appalling first I put this so that's

00:04:35,449 --> 00:04:40,490
it not waiting for the interim and just

00:04:37,820 --> 00:04:43,850
remove one source of the GTO run

00:04:40,490 --> 00:04:47,090
latencies so that's a bit controversial

00:04:43,850 --> 00:04:51,220
because yeah the reporting is actually

00:04:47,090 --> 00:04:54,500
burning cycles so yeah it's a choice

00:04:51,220 --> 00:04:58,310
maybe it's not that is a big deal if you

00:04:54,500 --> 00:05:01,780
have 100 cpu and maybe if you burn

00:04:58,310 --> 00:05:04,460
cycles on the locals to be without

00:05:01,780 --> 00:05:08,320
consuming any external resource on the

00:05:04,460 --> 00:05:12,259
bus memory bus arbors maybe it's fine

00:05:08,320 --> 00:05:19,190
just maybe some watts of power yeah

00:05:12,259 --> 00:05:22,280
that's a trade-off so now the history

00:05:19,190 --> 00:05:26,720
the story started in 2012 thanks to

00:05:22,280 --> 00:05:30,500
Jesse at least a 70 de pasiÃ³n Linux 3.11

00:05:26,720 --> 00:05:32,510
in 2013 the patch that gave good results

00:05:30,500 --> 00:05:36,289
but only for three weeks

00:05:32,510 --> 00:05:40,659
that was I GBE broad communiques weeks

00:05:36,289 --> 00:05:46,970
and many lakhs for and then later

00:05:40,659 --> 00:05:50,000
eventually auto driver wire I did so at

00:05:46,970 --> 00:05:55,419
that time this appalling started adding

00:05:50,000 --> 00:05:59,419
a new NGO operation in every driver and

00:05:55,419 --> 00:06:02,210
well it made the whole adaption of busy

00:05:59,419 --> 00:06:08,360
: quite slow because every driver read

00:06:02,210 --> 00:06:10,280
wanderer had to add this circle so this

00:06:08,360 --> 00:06:12,710
appalling was tested for TCP and

00:06:10,280 --> 00:06:14,810
connected UTP sockets and at the time

00:06:12,710 --> 00:06:19,190
only the founder received system core

00:06:14,810 --> 00:06:21,500
poland select were supported okay so of

00:06:19,190 --> 00:06:25,039
course the first results were looking

00:06:21,500 --> 00:06:29,930
very great because many unique have

00:06:25,039 --> 00:06:32,750
crazy interpret mitigation parameters

00:06:29,930 --> 00:06:35,120
like on an annex for we have 16 micro

00:06:32,750 --> 00:06:37,580
second delay logically if you can one

00:06:35,120 --> 00:06:40,849
packet than an equal signal intercepts

00:06:37,580 --> 00:06:43,370
in microseconds so if you just the RPC

00:06:40,849 --> 00:06:45,110
between two hosts in the same Iraq with

00:06:43,370 --> 00:06:48,889
so two hosts which are really close

00:06:45,110 --> 00:06:50,930
together in term of topology the resort

00:06:48,889 --> 00:06:54,310
are quite bad because we speaks in

00:06:50,930 --> 00:06:56,659
microsecond I did you have like 17,000

00:06:54,310 --> 00:06:58,939
transaction intersections without

00:06:56,659 --> 00:07:01,280
visible and with is economy

00:06:58,939 --> 00:07:06,020
you get sixty three thousand so that's a

00:07:01,280 --> 00:07:09,680
huge huge swing okay so for the API we

00:07:06,020 --> 00:07:12,289
had two ways of enabling this logic to

00:07:09,680 --> 00:07:15,259
seize control which are basically not

00:07:12,289 --> 00:07:17,210
very useful unless you want to test that

00:07:15,259 --> 00:07:20,389
your driver supports properly visible in

00:07:17,210 --> 00:07:23,900
because it's a global system for so once

00:07:20,389 --> 00:07:26,389
you enable that or a really application

00:07:23,900 --> 00:07:30,800
calling logically receive on the circuit

00:07:26,389 --> 00:07:34,819
will stop delivering and so last yeah

00:07:30,800 --> 00:07:39,409
that's that's not very good unless you

00:07:34,819 --> 00:07:45,409
have dedicated hosts and then another WA

00:07:39,409 --> 00:07:48,580
option was circuits option but for worst

00:07:45,409 --> 00:07:51,199
reason this section was limited to some

00:07:48,580 --> 00:07:53,569
capability netted in so basically if you

00:07:51,199 --> 00:07:56,360
are random user Joe a user on the system

00:07:53,569 --> 00:08:00,710
you couldn't really use this circuit

00:07:56,360 --> 00:08:02,960
option so we will see later that maybe

00:08:00,710 --> 00:08:05,330
we can remove this limitation now on the

00:08:02,960 --> 00:08:08,089
current channel because we remove the

00:08:05,330 --> 00:08:10,370
limitation that we had and the early

00:08:08,089 --> 00:08:12,379
days where the busy polling loop was

00:08:10,370 --> 00:08:17,979
done logically blocking everything else

00:08:12,379 --> 00:08:21,789
like in droves lot on - stuff like that

00:08:17,979 --> 00:08:24,770
so in Fallout 5 we did this change and

00:08:21,789 --> 00:08:29,089
basically we wrote the beautiful in loop

00:08:24,770 --> 00:08:33,349
to be better citizen by allowing bottom

00:08:29,089 --> 00:08:35,959
house under to run at the same time as

00:08:33,349 --> 00:08:38,690
you can see on the t-shirts the the

00:08:35,959 --> 00:08:41,630
result are two folds we can service

00:08:38,690 --> 00:08:44,300
bottom - blur so does it you can remove

00:08:41,630 --> 00:08:46,790
the extra licenses you had to pay at the

00:08:44,300 --> 00:08:49,899
very end of the be poor loop when you

00:08:46,790 --> 00:08:52,720
whenever button house we had to process

00:08:49,899 --> 00:08:56,160
sixteen jerk which were cured like time

00:08:52,720 --> 00:09:01,230
errors after you go back so registers so

00:08:56,160 --> 00:09:04,329
no more it's good but it's also add

00:09:01,230 --> 00:09:06,250
support for tuners like if you receive a

00:09:04,329 --> 00:09:09,040
packet then this packet needs to be

00:09:06,250 --> 00:09:11,740
reinfected in the stack using native rx

00:09:09,040 --> 00:09:15,160
it will trigger a new surfing drug so

00:09:11,740 --> 00:09:20,829
basically this change are all tuned I'll

00:09:15,160 --> 00:09:24,579
to be supported by physically and for

00:09:20,829 --> 00:09:27,310
example on some driver the transient and

00:09:24,579 --> 00:09:30,370
the receive are using to Nazis structure

00:09:27,310 --> 00:09:32,410
so basically this change or also allow

00:09:30,370 --> 00:09:34,540
the transmit compeition to happen so if

00:09:32,410 --> 00:09:36,730
you do RPC you son and receive packet

00:09:34,540 --> 00:09:38,560
it's good to be able to process the

00:09:36,730 --> 00:09:40,660
transmit compeition why we are waiting

00:09:38,560 --> 00:09:47,170
for the for the answer of the artists

00:09:40,660 --> 00:09:52,120
who just sent so the second change was

00:09:47,170 --> 00:09:56,110
to try to get rid of the driver specific

00:09:52,120 --> 00:09:58,259
change moving the visible in logic in

00:09:56,110 --> 00:10:01,509
the core layer of the networking stack

00:09:58,259 --> 00:10:03,279
you know permitted to other visible in

00:10:01,509 --> 00:10:11,680
all over a larger scale without any

00:10:03,279 --> 00:10:12,399
channel on the driver yeah why so that's

00:10:11,680 --> 00:10:15,100
the whole thing

00:10:12,399 --> 00:10:17,560
try to a row more rich for the busy

00:10:15,100 --> 00:10:21,160
polling but also avoid some extra thing

00:10:17,560 --> 00:10:24,100
that we add you use on the river because

00:10:21,160 --> 00:10:26,430
the busy pulling thread was competing

00:10:24,100 --> 00:10:29,680
with the normal next polling thread

00:10:26,430 --> 00:10:32,949
reacting to a vice enjoyed so if these

00:10:29,680 --> 00:10:35,769
to keep you would reach the net people

00:10:32,949 --> 00:10:40,060
at the same time it would be a dog so we

00:10:35,769 --> 00:10:43,300
add a digital Mina I did is logic in the

00:10:40,060 --> 00:10:46,300
rigger or path so every time the normal

00:10:43,300 --> 00:10:49,209
interval or non I'm supporting usual we

00:10:46,300 --> 00:10:51,850
were paying the extra cost of doing the

00:10:49,209 --> 00:10:54,250
spindle later options by a comparison

00:10:51,850 --> 00:10:56,300
but it was still an atomic up in the

00:10:54,250 --> 00:11:00,470
first half

00:10:56,300 --> 00:11:03,110
so more recently in Fulton so the forums

00:11:00,470 --> 00:11:07,280
and one was last year something mainly

00:11:03,110 --> 00:11:10,280
in October we did a small change or

00:11:07,280 --> 00:11:12,950
allowing UDP for delivering for

00:11:10,280 --> 00:11:15,200
unconnected visa basically that's nice

00:11:12,950 --> 00:11:18,110
for a smaller system with a single

00:11:15,200 --> 00:11:20,510
unique with a single unique you don't

00:11:18,110 --> 00:11:22,850
really care because all you GB message

00:11:20,510 --> 00:11:26,380
will eventually come to a single next

00:11:22,850 --> 00:11:29,090
bar even if the circuit is not connected

00:11:26,380 --> 00:11:33,770
then we did a small change in an attic

00:11:29,090 --> 00:11:37,480
impede an IDIQ low driver to not rearm

00:11:33,770 --> 00:11:40,070
the interrupts are in a bit boring loop

00:11:37,480 --> 00:11:43,250
meaning that we reduce the traffic

00:11:40,070 --> 00:11:45,320
between the host and the NIC enabling

00:11:43,250 --> 00:11:48,920
discipline enabling disabling interrupts

00:11:45,320 --> 00:11:51,440
or over again a year so that's important

00:11:48,920 --> 00:11:56,150
for the future step of deepening

00:11:51,440 --> 00:11:59,450
we will see that later so more recently

00:11:56,150 --> 00:12:04,400
again fold even finally got rid of all

00:11:59,450 --> 00:12:08,300
the NDB per implementation in drivers so

00:12:04,400 --> 00:12:11,300
and you can see the performance slightly

00:12:08,300 --> 00:12:14,870
increase from the early days on when an

00:12:11,300 --> 00:12:17,810
explorer we get 70 77,000 tcp our

00:12:14,870 --> 00:12:24,650
transaction I think the Mellanox 5 were

00:12:17,810 --> 00:12:30,260
above 100,000 now so for the upcoming

00:12:24,650 --> 00:12:33,860
Linux folder 12 srida and Alexander

00:12:30,260 --> 00:12:35,480
added the hippo support finally that's

00:12:33,860 --> 00:12:37,880
that's very nice because you know

00:12:35,480 --> 00:12:41,270
northern application you sleep or they

00:12:37,880 --> 00:12:47,390
don't use all or Britain system code of

00:12:41,270 --> 00:12:51,500
course so a new feature was added to the

00:12:47,390 --> 00:12:55,790
circuit API so that an application could

00:12:51,500 --> 00:12:57,800
read a natty identity here so that it

00:12:55,790 --> 00:13:00,590
could eventually

00:12:57,800 --> 00:13:03,320
classify the circuits after accept or

00:13:00,590 --> 00:13:05,360
access or connect eventually to

00:13:03,320 --> 00:13:07,260
eventually split the circuits into

00:13:05,360 --> 00:13:11,670
multiple

00:13:07,260 --> 00:13:13,500
silo if you will a silo would be under

00:13:11,670 --> 00:13:15,660
by one thread in application and one

00:13:13,500 --> 00:13:17,460
thread would be running for every

00:13:15,660 --> 00:13:20,160
received queue underneath so if you have

00:13:17,460 --> 00:13:22,800
an EP with a magic you like 16q you

00:13:20,160 --> 00:13:25,800
could speak to your application every

00:13:22,800 --> 00:13:30,090
load application with sixteen thread and

00:13:25,800 --> 00:13:32,340
each thread would under packets receive

00:13:30,090 --> 00:13:34,890
on the single received queue and so

00:13:32,340 --> 00:13:37,860
visible make sense here because of each

00:13:34,890 --> 00:13:40,890
fair with people on each on queue

00:13:37,860 --> 00:13:47,640
without hurting the road on the auto

00:13:40,890 --> 00:13:49,740
thread okay so this is there is one path

00:13:47,640 --> 00:13:55,140
missing this is the EVPs that are

00:13:49,740 --> 00:13:58,050
directly application doing lesson system

00:13:55,140 --> 00:14:03,660
collects let's say HTTP server on during

00:13:58,050 --> 00:14:06,060
million of connection we need to do the

00:14:03,660 --> 00:14:07,890
classification before the accepts so at

00:14:06,060 --> 00:14:11,610
the time we receive the same packet we

00:14:07,890 --> 00:14:14,010
want to flow this tier this in to the

00:14:11,610 --> 00:14:16,350
proper listener so we normally use a

00:14:14,010 --> 00:14:20,760
serene spot for that let's say you have

00:14:16,350 --> 00:14:24,150
a 24 listener or bond to the same port

00:14:20,760 --> 00:14:27,240
80 40 43 for example and so the syn

00:14:24,150 --> 00:14:29,460
packet ultimately should be selecting

00:14:27,240 --> 00:14:34,920
the proper circuit so that we can use

00:14:29,460 --> 00:14:37,050
the appalling for HTTP server so it's

00:14:34,920 --> 00:14:39,870
not done yet because we need this extra

00:14:37,050 --> 00:14:42,720
EVP of support but that's so trigger I'm

00:14:39,870 --> 00:14:45,030
pretty sure Daniels can cut that in a

00:14:42,720 --> 00:14:52,410
few minutes like the end of the day

00:14:45,030 --> 00:14:56,850
right waiting okay perfect okay so

00:14:52,410 --> 00:14:57,720
listen to learned Alice you know was

00:14:56,850 --> 00:15:00,720
very nice

00:14:57,720 --> 00:15:02,580
that's the name of course I mean forget

00:15:00,720 --> 00:15:04,170
to mention about the initial name of the

00:15:02,580 --> 00:15:06,600
appalling stuff and then Linnaeus

00:15:04,170 --> 00:15:13,500
complained loudly about that sound we

00:15:06,600 --> 00:15:15,240
turned out to be people so because it is

00:15:13,500 --> 00:15:18,750
for the prototypes are giving good

00:15:15,240 --> 00:15:20,480
numbers we did all these fun and after

00:15:18,750 --> 00:15:23,419
the driver

00:15:20,480 --> 00:15:27,169
I did their own stuff and so we ended it

00:15:23,419 --> 00:15:29,929
with a lot of stuff and bugs by the way

00:15:27,169 --> 00:15:35,089
in the drivers so all this is gone he

00:15:29,929 --> 00:15:37,579
took a few months so now we can start to

00:15:35,089 --> 00:15:40,419
work some on something else next

00:15:37,579 --> 00:15:43,329
generation because we don't anymore

00:15:40,419 --> 00:15:46,839
change a driver so it's much much easier

00:15:43,329 --> 00:15:51,739
for the acceptance of the new new stuff

00:15:46,839 --> 00:15:55,999
so what's next so all this started from

00:15:51,739 --> 00:15:57,549
a single email sent net desk I think it

00:15:55,999 --> 00:16:01,279
was in October last year

00:15:57,549 --> 00:16:09,230
Blackburn asked something very simple if

00:16:01,279 --> 00:16:13,069
we could get away programming an Appy in

00:16:09,230 --> 00:16:15,730
an infinite loop basically not arming

00:16:13,069 --> 00:16:18,379
and disarming interrupt anymore just

00:16:15,730 --> 00:16:22,399
saying I want to dedicate one CPU just

00:16:18,379 --> 00:16:25,369
to service dispatch and this idea kind

00:16:22,399 --> 00:16:27,829
of is similar to one idea that Powell of

00:16:25,369 --> 00:16:29,869
a beanie and Highness had few months

00:16:27,829 --> 00:16:34,609
before I think it was in May a cure

00:16:29,869 --> 00:16:37,579
where they proposed cast read to to run

00:16:34,609 --> 00:16:40,999
the net deeper logic at that time it was

00:16:37,579 --> 00:16:43,249
because of some interaction with between

00:16:40,999 --> 00:16:45,439
castle sintra daemon soft and rogue

00:16:43,249 --> 00:16:47,329
demon and user application on a single

00:16:45,439 --> 00:16:51,259
CPU system but that's a bit the same

00:16:47,329 --> 00:16:53,569
idea of course busy following a next-gen

00:16:51,259 --> 00:16:55,129
would rely having multiple CP because if

00:16:53,569 --> 00:16:56,779
you have a system within once if you

00:16:55,129 --> 00:16:59,359
don't know where you can use people in

00:16:56,779 --> 00:17:01,369
the kernel and never give up the CPU

00:16:59,359 --> 00:17:04,370
because the user application won't have

00:17:01,369 --> 00:17:06,620
a change and transferring so that's a

00:17:04,370 --> 00:17:11,899
bit different so it started from this

00:17:06,620 --> 00:17:15,589
email so you could eventually with the

00:17:11,899 --> 00:17:18,500
current color do that do this request by

00:17:15,589 --> 00:17:22,549
just coding some dummy application doing

00:17:18,500 --> 00:17:24,399
receive system core enabling faster busy

00:17:22,549 --> 00:17:26,899
polling logic and doing that in the

00:17:24,399 --> 00:17:29,659
never receive anything on this circuit

00:17:26,899 --> 00:17:31,260
life you take a random socket bond to an

00:17:29,659 --> 00:17:33,840
existing portal

00:17:31,260 --> 00:17:37,710
and by just doing that you will the

00:17:33,840 --> 00:17:40,500
thread will just read the device queue

00:17:37,710 --> 00:17:43,230
and service packets out and in

00:17:40,500 --> 00:17:46,070
so yeah it's doable but I think we can

00:17:43,230 --> 00:17:46,070
do psyche better

00:17:46,530 --> 00:17:51,900
so whatever the program here and which

00:17:49,560 --> 00:17:54,630
why very hard to optimize networking we

00:17:51,900 --> 00:17:57,150
had a lot of talks with in the past on

00:17:54,630 --> 00:17:58,890
last night days and whatever and we try

00:17:57,150 --> 00:18:01,350
to you know shave some cycles by

00:17:58,890 --> 00:18:03,390
removing the instruction world

00:18:01,350 --> 00:18:06,210
what about not really needed adding

00:18:03,390 --> 00:18:09,720
specific Camelot or a delegate or what

00:18:06,210 --> 00:18:14,280
so Tasha that's very nice like the thing

00:18:09,720 --> 00:18:18,030
is we have a huge deep pipeline before

00:18:14,280 --> 00:18:19,770
you know reaching the wire so the

00:18:18,030 --> 00:18:22,470
application does some research we need

00:18:19,770 --> 00:18:25,230
to find out the file descriptor the file

00:18:22,470 --> 00:18:28,140
structure the circuit we entered the TCP

00:18:25,230 --> 00:18:31,560
stack indoors a lot of student going on

00:18:28,140 --> 00:18:34,140
we enter the IP layer the neighbor we

00:18:31,560 --> 00:18:38,130
finally had heated cooties can do thank

00:18:34,140 --> 00:18:41,670
you and then well now we do it is now to

00:18:38,130 --> 00:18:45,000
DQ this packet again and called a device

00:18:41,670 --> 00:18:47,490
logic to send the packets and so we need

00:18:45,000 --> 00:18:49,890
all kind of locks or kind of atomic

00:18:47,490 --> 00:18:53,520
operation and then eventually reach the

00:18:49,890 --> 00:18:55,760
driver logic so just you know 10 gig

00:18:53,520 --> 00:18:59,130
driver now is a 10,000 line of code it's

00:18:55,760 --> 00:19:01,830
well so if n do stuff technically so a

00:18:59,130 --> 00:19:05,280
complex code and then eventually when

00:19:01,830 --> 00:19:07,650
finally we send this packet where we put

00:19:05,280 --> 00:19:09,840
it on the transmit ring we go back or

00:19:07,650 --> 00:19:12,660
this function all the way back to the

00:19:09,840 --> 00:19:14,880
user application so that's a DB pipeline

00:19:12,660 --> 00:19:17,280
and all the oldc

00:19:14,880 --> 00:19:18,930
are from on a single CP the cpu running

00:19:17,280 --> 00:19:21,030
in the application and maybe the

00:19:18,930 --> 00:19:23,280
application don't want us to spend all

00:19:21,030 --> 00:19:25,710
the stuff in the car maybe the vacation

00:19:23,280 --> 00:19:29,100
would like just to post this small

00:19:25,710 --> 00:19:34,200
message and continue its work in user

00:19:29,100 --> 00:19:39,900
space keeping the CPU cache hot and work

00:19:34,200 --> 00:19:41,580
with this model doesn't work and so this

00:19:39,900 --> 00:19:46,009
model doesn't work

00:19:41,580 --> 00:19:49,169
if we have a lot of CPU on the host well

00:19:46,009 --> 00:19:52,860
let's say 100 cpu they will all do this

00:19:49,169 --> 00:19:55,769
basically the same stuff so all the 100

00:19:52,860 --> 00:19:59,039
CPU cache will be loaded babies cannot

00:19:55,769 --> 00:20:02,879
good interesting that may be not very

00:19:59,039 --> 00:20:05,999
efficient so we have this pathological

00:20:02,879 --> 00:20:07,649
case where we try to increase the number

00:20:05,999 --> 00:20:09,989
of transmit you just to reduce the

00:20:07,649 --> 00:20:11,820
congestion on the 3ds clock and try to

00:20:09,989 --> 00:20:14,669
have this parallelism on a recipe

00:20:11,820 --> 00:20:20,999
the goal was all each CPU should run

00:20:14,669 --> 00:20:23,549
with with that being slowed down by

00:20:20,999 --> 00:20:26,159
order and by doing so we actually

00:20:23,549 --> 00:20:31,499
increase the slowdown because we have so

00:20:26,159 --> 00:20:35,129
many Atomics and so many cache flushing

00:20:31,499 --> 00:20:37,200
that we are arriving to this

00:20:35,129 --> 00:20:41,730
pathological case you you have

00:20:37,200 --> 00:20:44,730
experiments with 44 queues so 44 3 or 88

00:20:41,730 --> 00:20:48,450
and if you increase the load on the

00:20:44,730 --> 00:20:52,409
number of TCP such as you basically can

00:20:48,450 --> 00:20:54,929
consume maybe 30% of the CPU cycles just

00:20:52,409 --> 00:20:57,299
internal to drive this for technique

00:20:54,929 --> 00:21:00,269
while normally if you are alone one

00:20:57,299 --> 00:21:05,480
single CPU can absolutely Drive this 40

00:21:00,269 --> 00:21:08,999
gig Nick very easily so that's insane

00:21:05,480 --> 00:21:12,809
yeah so why we chose the silicon atom

00:21:08,999 --> 00:21:14,909
smaller so l1 l2 or very small just take

00:21:12,809 --> 00:21:18,409
a look at the various function in the

00:21:14,909 --> 00:21:24,570
kernel they don't even fit in that so

00:21:18,409 --> 00:21:27,239
way too small so the idea is you free

00:21:24,570 --> 00:21:29,340
like break pipes so recognize there is a

00:21:27,239 --> 00:21:34,700
pipe and just implement part of the

00:21:29,340 --> 00:21:40,970
other other pipeline in a dedicated

00:21:34,700 --> 00:21:44,980
dedicated CPU and the provision of you

00:21:40,970 --> 00:21:48,350
why it's interesting many many give the

00:21:44,980 --> 00:21:51,950
when you share server like or where with

00:21:48,350 --> 00:21:54,830
100 jobs it's very convenient to be able

00:21:51,950 --> 00:21:57,380
to dedicate CPU to job say one job

00:21:54,830 --> 00:22:01,250
should be our three TP the other should

00:21:57,380 --> 00:22:02,870
we have should have four CP but then you

00:22:01,250 --> 00:22:05,780
also hate this problem of networking

00:22:02,870 --> 00:22:09,050
workload that magically could interrupt

00:22:05,780 --> 00:22:11,420
your CP by just receiving a packet for

00:22:09,050 --> 00:22:15,110
another application and we don't like

00:22:11,420 --> 00:22:20,900
down to that ljubljana so that's that's

00:22:15,110 --> 00:22:25,000
a problem so how we could do that we

00:22:20,900 --> 00:22:27,710
could just create a visible in CP group

00:22:25,000 --> 00:22:30,710
preferably for any manner let's say you

00:22:27,710 --> 00:22:33,890
most system know how to to circuits to

00:22:30,710 --> 00:22:36,620
newman odd so in this model we will

00:22:33,890 --> 00:22:41,840
create two at least two CP group with at

00:22:36,620 --> 00:22:45,260
least one CPU on this group we should

00:22:41,840 --> 00:22:47,870
have somewhere to add in others of you

00:22:45,260 --> 00:22:51,410
and be able to schedule them or not I

00:22:47,870 --> 00:22:54,110
mean packing them and low-power mode if

00:22:51,410 --> 00:22:57,140
the road is small enough almost most

00:22:54,110 --> 00:22:59,360
several don't really know her need a lot

00:22:57,140 --> 00:23:02,480
of networking lots of networking stuff

00:22:59,360 --> 00:23:06,230
so probably one city would be enough but

00:23:02,480 --> 00:23:08,330
for highly demanding application we

00:23:06,230 --> 00:23:10,570
probably would need more CP on these

00:23:08,330 --> 00:23:10,570
groups

00:23:10,900 --> 00:23:20,770
so then after for the further how to

00:23:15,950 --> 00:23:24,530
attach an a key to the via polling group

00:23:20,770 --> 00:23:27,020
we will require for each not Ibiza MIDI

00:23:24,530 --> 00:23:30,860
board in some way in CSS or something

00:23:27,020 --> 00:23:33,680
like that so that again you can say to

00:23:30,860 --> 00:23:36,230
the system oh I want this receive queue

00:23:33,680 --> 00:23:42,460
or this transmit queue on Disney being

00:23:36,230 --> 00:23:42,460
honored by the polling CPU okay

00:23:42,619 --> 00:23:48,149
so what happens in a nappy and the

00:23:45,659 --> 00:23:51,419
Rickey paths are well in this mode the

00:23:48,149 --> 00:23:53,789
visible in CP would drain the the

00:23:51,419 --> 00:23:58,700
receive cue the ring buffer and would

00:23:53,789 --> 00:24:01,139
feed the packet to the IP TCP stack and

00:23:58,700 --> 00:24:06,389
put the packet in the received queue

00:24:01,139 --> 00:24:09,720
right and that's it if RSS for example

00:24:06,389 --> 00:24:13,320
remote received flow steering is used

00:24:09,720 --> 00:24:17,519
could be used then this discipline would

00:24:13,320 --> 00:24:20,909
stop at the moment we hit the RSS logic

00:24:17,519 --> 00:24:23,730
so our feasibility queue this packets on

00:24:20,909 --> 00:24:28,230
to another CPU backlog and some an IP I

00:24:23,730 --> 00:24:31,259
I would have is appalling it is normally

00:24:28,230 --> 00:24:34,769
in the on t RSS my name is normally with

00:24:31,259 --> 00:24:37,830
this model we should not really need RSS

00:24:34,769 --> 00:24:41,429
anymore but we see because we still have

00:24:37,830 --> 00:24:46,559
to implement that right so for the

00:24:41,429 --> 00:24:48,869
transmitters I think that what would be

00:24:46,559 --> 00:24:51,059
useful here it is we need to limit the

00:24:48,869 --> 00:24:56,369
first implementation of this diggable

00:24:51,059 --> 00:24:59,190
inner loop on the lower part from the

00:24:56,369 --> 00:25:02,850
queue disk DQ up to the device transmits

00:24:59,190 --> 00:25:05,129
logic so that also includes or the BQ

00:25:02,850 --> 00:25:08,779
limit or the transient compassion or

00:25:05,129 --> 00:25:12,990
distress right now the cutest one is a

00:25:08,779 --> 00:25:14,340
is an abomination issue and I'm

00:25:12,990 --> 00:25:16,919
surprised that nobody will complain

00:25:14,340 --> 00:25:19,350
about that because right now we do to

00:25:16,919 --> 00:25:22,669
the Qt thank you and then we see the

00:25:19,350 --> 00:25:27,480
contribute candy to the packet Andy yes

00:25:22,669 --> 00:25:30,509
well we we can lock on this secure on

00:25:27,480 --> 00:25:32,850
loop on the Crestron for 100 of packets

00:25:30,509 --> 00:25:36,029
so like if you have a real-time

00:25:32,850 --> 00:25:40,110
application using sched fifo having very

00:25:36,029 --> 00:25:43,259
strict latency requirement trapped on

00:25:40,110 --> 00:25:45,210
this loop it can spend 100 of micro

00:25:43,259 --> 00:25:49,470
second just servicing this

00:25:45,210 --> 00:25:50,669
that's pretty so in this model with the

00:25:49,470 --> 00:25:52,470
data visible

00:25:50,669 --> 00:25:55,200
you don't care anymore because you don't

00:25:52,470 --> 00:25:55,690
fat user application anymore only the

00:25:55,200 --> 00:25:58,539
visible

00:25:55,690 --> 00:26:03,519
you would do that so you don't have any

00:25:58,539 --> 00:26:06,340
more at sincere issue okay so and we

00:26:03,519 --> 00:26:08,769
must learn I think we have some

00:26:06,340 --> 00:26:12,279
challenge here but I think it's mine

00:26:08,769 --> 00:26:15,610
auditors basically this busy pulling CPU

00:26:12,279 --> 00:26:20,159
would run inside the cat thread so that

00:26:15,610 --> 00:26:25,059
we can easily tune their prior or claw

00:26:20,159 --> 00:26:28,210
schedule class on or discuss why we are

00:26:25,059 --> 00:26:31,539
so why is that we need to service soft

00:26:28,210 --> 00:26:34,120
intro you know when we see the packet

00:26:31,539 --> 00:26:35,860
the TCP stack will eventually set up a

00:26:34,120 --> 00:26:38,980
timer and this time with a phone to be

00:26:35,860 --> 00:26:41,919
fire if we are unlucky like few seconds

00:26:38,980 --> 00:26:45,279
later to do retransmits or stuff like

00:26:41,919 --> 00:26:49,210
that so an timers are using soft and

00:26:45,279 --> 00:26:54,159
wrote our super bags are to free the

00:26:49,210 --> 00:26:56,559
circuit or orestes --and-- process warp

00:26:54,159 --> 00:26:59,529
use because you know we use also walk

00:26:56,559 --> 00:27:06,629
shoes walk using them in the networking

00:26:59,529 --> 00:27:09,039
lamp yeah that's about that I think I

00:27:06,629 --> 00:27:11,409
finished so I would like to get some

00:27:09,039 --> 00:27:13,720
feedback from internal guys because I

00:27:11,409 --> 00:27:18,159
know they are working also on this busy

00:27:13,720 --> 00:27:19,899
calling new stuff with internet right

00:27:18,159 --> 00:27:25,690
are they all sitting here that's good

00:27:19,899 --> 00:27:26,919
it's that well guitars are kind of

00:27:25,690 --> 00:27:28,690
feedback if you're looking for because

00:27:26,919 --> 00:27:30,100
I'm just trying to absorb all

00:27:28,690 --> 00:27:33,309
essentially you're wanting to take the

00:27:30,100 --> 00:27:35,529
CPUs and are essentially you're wanting

00:27:33,309 --> 00:27:37,779
to bind the natty polling to a specific

00:27:35,529 --> 00:27:39,909
set of CPUs and have the busy poll just

00:27:37,779 --> 00:27:42,460
running consistently on that set of CPUs

00:27:39,909 --> 00:27:44,200
frenched installed correctly yeah so for

00:27:42,460 --> 00:27:49,210
example let's say you have an uma system

00:27:44,200 --> 00:27:51,190
with two nodes you split and normally

00:27:49,210 --> 00:27:51,970
history or from the case you have a

00:27:51,190 --> 00:27:55,389
single Nick

00:27:51,970 --> 00:27:58,210
well what you can do is placing the

00:27:55,389 --> 00:27:59,379
queues on to path of the new of the

00:27:58,210 --> 00:28:01,059
right yeah so essentially you're

00:27:59,379 --> 00:28:04,360
creating a dedicated network processor

00:28:01,059 --> 00:28:06,909
on one of the cores yeah and so normally

00:28:04,360 --> 00:28:07,850
under or light load once if you should

00:28:06,909 --> 00:28:09,409
be enough to set

00:28:07,850 --> 00:28:11,749
all the cue you have let's say you have

00:28:09,409 --> 00:28:13,759
six inches for the wrong case Wolski's

00:28:11,749 --> 00:28:17,299
being you know go out and do attack like

00:28:13,759 --> 00:28:21,080
leaders and you want to have 16 kills to

00:28:17,299 --> 00:28:23,779
be able to spread this to 16 GB right

00:28:21,080 --> 00:28:26,659
this is this is how you program your

00:28:23,779 --> 00:28:30,229
hosts you just do it as you test the

00:28:26,659 --> 00:28:31,759
sinful attack with it's a 15 min back at

00:28:30,229 --> 00:28:34,340
per second time you want your host to

00:28:31,759 --> 00:28:36,919
survive the attack so you will soon o to

00:28:34,340 --> 00:28:38,600
survive this kind of attack I need six

00:28:36,919 --> 00:28:40,700
inches let's say I don't I don't know

00:28:38,600 --> 00:28:44,059
that sorry just a random number

00:28:40,700 --> 00:28:47,509
so you speed this in 8% but for normal

00:28:44,059 --> 00:28:49,849
system not hitting this attack once if

00:28:47,509 --> 00:28:53,389
you should be enough so you don't only

00:28:49,849 --> 00:28:54,619
want to view my entire CPU group what

00:28:53,389 --> 00:28:56,419
you talking here also then have

00:28:54,619 --> 00:28:57,889
introduced number queues and down to you

00:28:56,419 --> 00:29:00,259
and so the thing is you don't reduce

00:28:57,889 --> 00:29:02,389
number of you under attack you just add

00:29:00,259 --> 00:29:03,979
a new CP and that's magical because you

00:29:02,389 --> 00:29:06,619
don't have to stop the Nick and

00:29:03,979 --> 00:29:08,599
reprogram so there is no disruption on

00:29:06,619 --> 00:29:11,330
the Prophet so you're talking about

00:29:08,599 --> 00:29:13,489
basic like changing RSS and no no no

00:29:11,330 --> 00:29:15,440
change the Nick will still still the

00:29:13,489 --> 00:29:17,899
packet on multi cube torches we don't

00:29:15,440 --> 00:29:20,389
need to have a single cube in the normal

00:29:17,899 --> 00:29:22,399
mode because if you do that you have all

00:29:20,389 --> 00:29:22,700
kind of real Dora's Union ordinary like

00:29:22,399 --> 00:29:25,249
that

00:29:22,700 --> 00:29:27,830
okay so I think it's better to just it

00:29:25,249 --> 00:29:30,229
be Nick Aziz because some leaks are a

00:29:27,830 --> 00:29:32,599
damn slow to reprogram like two seconds

00:29:30,229 --> 00:29:34,129
and you know well especially since a lot

00:29:32,599 --> 00:29:35,840
and Excel tend to do a reset if you

00:29:34,129 --> 00:29:37,879
start changing the queue and such so we

00:29:35,840 --> 00:29:39,919
we already have all these are queue

00:29:37,879 --> 00:29:45,799
balance issues when just rendering your

00:29:39,919 --> 00:29:48,679
IQ to CPU HB is disrupting so change in

00:29:45,799 --> 00:29:51,700
a number of cues and iniki's Wow yeah no

00:29:48,679 --> 00:29:55,340
no no boo

00:29:51,700 --> 00:29:58,909
yeah so basically if you keep the number

00:29:55,340 --> 00:30:01,099
of queue a constant then and then in the

00:29:58,909 --> 00:30:03,919
high stress mode you could have one

00:30:01,099 --> 00:30:06,470
superpower queue so your CPU group

00:30:03,919 --> 00:30:09,909
instead of adding one visible in CP

00:30:06,470 --> 00:30:09,909
would have eight right

00:30:15,340 --> 00:30:23,570
okay hi I didn't completely understand

00:30:20,299 --> 00:30:26,510
the takes path is it my impression was

00:30:23,570 --> 00:30:29,799
that you kind of move XPS to the socket

00:30:26,510 --> 00:30:34,360
layer so you add send message time to

00:30:29,799 --> 00:30:37,160
generate the hash and use it to identify

00:30:34,360 --> 00:30:39,650
the sending CPU is that correct or that

00:30:37,160 --> 00:30:41,510
this part is absolutely not part of the

00:30:39,650 --> 00:30:44,360
description that's it peers or the

00:30:41,510 --> 00:30:46,940
Select QLogic so I don't think it's

00:30:44,360 --> 00:30:49,429
orthogonal to the being an stuff okay

00:30:46,940 --> 00:30:52,910
the choice of the transmit q is is not

00:30:49,429 --> 00:30:56,150
part of the talk right okay what happens

00:30:52,910 --> 00:30:59,320
here just because you mentioned it a

00:30:56,150 --> 00:31:02,179
month right so I want to talk yeah so

00:30:59,320 --> 00:31:05,750
yeah the choice of the transmit you

00:31:02,179 --> 00:31:08,950
should be it's outside of the scope I

00:31:05,750 --> 00:31:13,309
mean it's the normal logic with all the

00:31:08,950 --> 00:31:15,620
only out of order logic for TCP you know

00:31:13,309 --> 00:31:19,490
sticking on the q if you have

00:31:15,620 --> 00:31:23,390
outstanding packet on the on the host so

00:31:19,490 --> 00:31:30,860
this will equal change so my goal really

00:31:23,390 --> 00:31:33,470
here is to not change too many things in

00:31:30,860 --> 00:31:35,780
the camera actually no turns would be we

00:31:33,470 --> 00:31:38,840
call on tcp/ip do at all

00:31:35,780 --> 00:31:43,490
it could be only on you know the netcode

00:31:38,840 --> 00:31:49,280
s dot C something I mean existing native

00:31:43,490 --> 00:31:52,360
or logic okay sing on try to use good

00:31:49,280 --> 00:31:52,360
not copy/paste

00:31:57,620 --> 00:32:00,810
hi Eric

00:31:59,040 --> 00:32:03,870
thanks for the interesting presentation

00:32:00,810 --> 00:32:06,150
the the one thing that could happen is

00:32:03,870 --> 00:32:08,490
that all the Knicks that support RSS can

00:32:06,150 --> 00:32:10,710
have dynamic changes made to their RSS

00:32:08,490 --> 00:32:12,300
indirection table all this by the

00:32:10,710 --> 00:32:14,460
hardware spec you have to be able to

00:32:12,300 --> 00:32:17,070
change the indirection table on the fly

00:32:14,460 --> 00:32:19,050
with no overhead really so you can just

00:32:17,070 --> 00:32:20,220
you could literally as you transition to

00:32:19,050 --> 00:32:22,020
this mode you could just change the

00:32:20,220 --> 00:32:24,000
indirection table and direct it to one

00:32:22,020 --> 00:32:25,980
or two queues as you disabled interrupts

00:32:24,000 --> 00:32:27,270
without moving interrupts or changing

00:32:25,980 --> 00:32:28,740
CPUs or anything you could just change

00:32:27,270 --> 00:32:30,840
the interaction table and instantly have

00:32:28,740 --> 00:32:32,730
all the traffic dedicated to one or two

00:32:30,840 --> 00:32:35,400
fours once that went went into polling

00:32:32,730 --> 00:32:38,280
if you wanted this emergency kind of or

00:32:35,400 --> 00:32:40,530
you know change in behavior at run time

00:32:38,280 --> 00:32:42,900
yeah but that would be probably Nick

00:32:40,530 --> 00:32:44,850
specific joins so many could be able to

00:32:42,900 --> 00:32:49,410
good well know everybody that implements

00:32:44,850 --> 00:32:50,940
RSS spec can do this yeah okay which is

00:32:49,410 --> 00:32:52,710
almost all the nicks right because they

00:32:50,940 --> 00:32:55,410
want to pass Microsoft certifications so

00:32:52,710 --> 00:32:57,180
I know it's not Microsoft you know

00:32:55,410 --> 00:32:59,870
talking here but that's what drives a

00:32:57,180 --> 00:32:59,870
lot of the changes

00:33:06,870 --> 00:33:12,640
Perik so this is great work thank you

00:33:10,720 --> 00:33:15,460
one question though

00:33:12,640 --> 00:33:16,750
on the RFS I kind of found an

00:33:15,460 --> 00:33:20,940
interesting you would think this is the

00:33:16,750 --> 00:33:23,680
anti RFS the rationale for RFS and RPS

00:33:20,940 --> 00:33:27,550
was that we wanted to specifically move

00:33:23,680 --> 00:33:32,050
the layer three layer for processing off

00:33:27,550 --> 00:33:34,360
of the CPU doing the DQ from the Nick

00:33:32,050 --> 00:33:37,090
one of the reasons we had to do that was

00:33:34,360 --> 00:33:39,640
because there could be a lot of variance

00:33:37,090 --> 00:33:41,440
in how much we have to do in layer three

00:33:39,640 --> 00:33:43,900
layer four for instance what if somebody

00:33:41,440 --> 00:33:47,920
said IPSec or some levels of

00:33:43,900 --> 00:33:50,290
encapsulation that variants we had cases

00:33:47,920 --> 00:33:53,860
where with like a millisecond blip there

00:33:50,290 --> 00:33:55,540
was something in TCP so actually I'm

00:33:53,860 --> 00:33:57,820
wondering this seems like if we're going

00:33:55,540 --> 00:34:01,450
the other direction so we went for multi

00:33:57,820 --> 00:34:03,250
cue we started you know even even before

00:34:01,450 --> 00:34:05,800
all of this we started with a single cue

00:34:03,250 --> 00:34:07,330
and everything was then on that CPU we

00:34:05,800 --> 00:34:08,980
ran that CPU to 100%

00:34:07,330 --> 00:34:11,800
that was a maximum we can get out of

00:34:08,980 --> 00:34:13,780
networking on this system multi cue came

00:34:11,800 --> 00:34:16,179
along our PS came along and we said Oh

00:34:13,780 --> 00:34:17,830
parallelism is a great thing and then we

00:34:16,179 --> 00:34:20,620
kind of went berserk on multi cue and

00:34:17,830 --> 00:34:22,570
said every every CPU needs a cue because

00:34:20,620 --> 00:34:24,850
that's perfect parallelism but had you

00:34:22,570 --> 00:34:27,280
pointed out then that kills nappy

00:34:24,850 --> 00:34:29,410
batching yeah so go in the other

00:34:27,280 --> 00:34:31,000
direction is great but then it sounds

00:34:29,410 --> 00:34:32,710
like we might get back into the state

00:34:31,000 --> 00:34:36,520
where we're trying to do too much on

00:34:32,710 --> 00:34:38,620
these key CPUs that that are now busy

00:34:36,520 --> 00:34:40,840
polling and if we're spending too much

00:34:38,620 --> 00:34:43,330
time or getting too much variance in the

00:34:40,840 --> 00:34:46,090
upper layer processing that may kind of

00:34:43,330 --> 00:34:48,550
younger a line the whole point I'm

00:34:46,090 --> 00:34:50,169
little wondering if RFS actually becomes

00:34:48,550 --> 00:34:54,790
a little more important in this context

00:34:50,169 --> 00:34:57,550
in which case my question then is is RFS

00:34:54,790 --> 00:35:00,070
adding overhead because it's like API

00:34:57,550 --> 00:35:01,870
that we could also solve by some form of

00:35:00,070 --> 00:35:04,930
busy polling on our first kiss and

00:35:01,870 --> 00:35:08,440
instance yeah so RFS really the you know

00:35:04,930 --> 00:35:10,030
we have oh no auntie family is a lot of

00:35:08,440 --> 00:35:13,810
CPU you have

00:35:10,030 --> 00:35:17,320
and a lot of risk if you you have this

00:35:13,810 --> 00:35:20,320
cross matrix between between the

00:35:17,320 --> 00:35:23,800
producer and the consumer and so with an

00:35:20,320 --> 00:35:26,020
IP I basically to a lot of CPU and so in

00:35:23,800 --> 00:35:29,200
our experience our RFS doesn't scale

00:35:26,020 --> 00:35:31,930
with a huge number of CPU so for to

00:35:29,200 --> 00:35:34,270
address your point maybe the the busy

00:35:31,930 --> 00:35:36,480
polling could be used for example source

00:35:34,270 --> 00:35:40,119
of like IPSec whatever you we could have

00:35:36,480 --> 00:35:43,660
dedicated stage of the pipeline fall for

00:35:40,119 --> 00:35:45,579
some step that we know are going to

00:35:43,660 --> 00:35:48,460
consumer at the speed let's say you need

00:35:45,579 --> 00:35:51,310
to for example do checksum computation

00:35:48,460 --> 00:35:54,220
on the receiver because unique is not

00:35:51,310 --> 00:35:56,470
about to let for our cat I'm category

00:35:54,220 --> 00:36:01,690
might have you could also have some

00:35:56,470 --> 00:36:04,750
logic fall out CPU only dedicating your

00:36:01,690 --> 00:36:07,450
cycle doing checksum running a very

00:36:04,750 --> 00:36:09,310
small code actually cheap checks on the

00:36:07,450 --> 00:36:11,640
duty to tection something like that so

00:36:09,310 --> 00:36:11,640
why not

00:36:17,530 --> 00:36:21,640
okay why is it I make the point that I

00:36:20,140 --> 00:36:23,050
think thing about understanding this

00:36:21,640 --> 00:36:25,240
right part of what you're going for is

00:36:23,050 --> 00:36:27,190
you're wanting to compress as much of

00:36:25,240 --> 00:36:28,930
the work on to us as few CPUs as

00:36:27,190 --> 00:36:31,600
possible so you get the most benefit out

00:36:28,930 --> 00:36:34,750
of the cycles that you are using

00:36:31,600 --> 00:36:36,100
essentially so by you know in the case

00:36:34,750 --> 00:36:37,990
of the busy polling if you if you're not

00:36:36,100 --> 00:36:39,190
doing anything having all the CPUs in

00:36:37,990 --> 00:36:41,200
the system busy polling you're just

00:36:39,190 --> 00:36:42,670
burning energy essentially so you want

00:36:41,200 --> 00:36:44,140
to have that one cue sitting there you

00:36:42,670 --> 00:36:46,270
know okay I'm going to be only looking

00:36:44,140 --> 00:36:48,670
in this one queue for incoming packets

00:36:46,270 --> 00:36:49,750
instead having all the CPU spin and so

00:36:48,670 --> 00:36:51,820
to some extent that makes a lot more

00:36:49,750 --> 00:36:53,620
sense especially if there's any way you

00:36:51,820 --> 00:36:57,880
can dynamically resize that then that

00:36:53,620 --> 00:36:59,980
makes a usability much better so yeah

00:36:57,880 --> 00:37:02,800
because most of the benefit of visiting

00:36:59,980 --> 00:37:07,060
really all in these two red box

00:37:02,800 --> 00:37:09,130
sure having the last part here the

00:37:07,060 --> 00:37:11,860
application busy polling on its own

00:37:09,130 --> 00:37:14,440
receive queue helps a bit because you

00:37:11,860 --> 00:37:16,420
don't have to schedule this thread but

00:37:14,440 --> 00:37:20,290
we know that it's not going to scale

00:37:16,420 --> 00:37:22,510
with like 1000 threads so and that's why

00:37:20,290 --> 00:37:24,730
the people support the Waltham important

00:37:22,510 --> 00:37:27,940
because most modern application you see

00:37:24,730 --> 00:37:33,000
for each thread hundreds thousands of

00:37:27,940 --> 00:37:33,000
circuits so people really matters here

00:37:33,510 --> 00:37:38,440
ok we'll send you to the penalty box

00:37:36,120 --> 00:37:40,330
sorry we're going to send you over there

00:37:38,440 --> 00:37:43,810
people you can come and talk to I know

00:37:40,330 --> 00:37:46,480
how much wanted to say something well

00:37:43,810 --> 00:37:52,770
you look like about to jump at him or

00:37:46,480 --> 00:37:54,840
some okay lunch location is finally

00:37:52,770 --> 00:37:58,840
sorry go ahead

00:37:54,840 --> 00:37:59,880
just I want to say thank you to again

00:37:58,840 --> 00:38:04,169
for

00:37:59,880 --> 00:38:04,169

YouTube URL: https://www.youtube.com/watch?v=X0xBCoQGUvg


