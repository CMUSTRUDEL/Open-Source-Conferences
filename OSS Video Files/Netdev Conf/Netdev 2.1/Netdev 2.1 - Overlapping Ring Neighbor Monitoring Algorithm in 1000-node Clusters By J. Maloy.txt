Title: Netdev 2.1 - Overlapping Ring Neighbor Monitoring Algorithm in 1000-node Clusters By J. Maloy
Publication date: 2017-05-09
Playlist: Netdev 2.1
Description: 
	In this talk given at Netdev 2.1 on the 7th of April, John Maloy describes the details of the architecture and algorithm of the new neighbor monitoring algorithm for TIPC which was introduced in Kernel  4.7.

content:  https://www.netdevconf.org/2.1/session.html?maloy
Captions: 
	00:00:00,030 --> 00:00:08,490
so I think we'll just start good morning

00:00:04,170 --> 00:00:12,630
everyone I'm John Malloy and I'll

00:00:08,490 --> 00:00:16,920
present a new algorithm and protocol

00:00:12,630 --> 00:00:20,520
that was introduced into tipsy in Linux

00:00:16,920 --> 00:00:21,990
4.7 that is June last year what is

00:00:20,520 --> 00:00:26,189
particularly with this algorithm is is

00:00:21,990 --> 00:00:29,369
that it allows to monitor neighboring

00:00:26,189 --> 00:00:31,529
nodes in a cluster it allows to increase

00:00:29,369 --> 00:00:35,520
that number with a factor of 10 at least

00:00:31,529 --> 00:00:38,399
from around 100 to up 2,000 maybe more

00:00:35,520 --> 00:00:40,500
we haven't tested any people I think

00:00:38,399 --> 00:00:44,100
I'll start with a brief interrupt and

00:00:40,500 --> 00:00:46,079
just explain what tips is because I'm

00:00:44,100 --> 00:00:48,690
not sure everybody here will be aware

00:00:46,079 --> 00:00:51,870
about what about what it is so tipsy is

00:00:48,690 --> 00:00:55,079
and cross nodes IPC mechanism that was

00:00:51,870 --> 00:00:57,989
introduced into Linux about ten years

00:00:55,079 --> 00:01:00,510
ago it comes from the telco telecom

00:00:57,989 --> 00:01:04,650
world and its main user base until this

00:01:00,510 --> 00:01:06,180
day has been calculations which back

00:01:04,650 --> 00:01:09,659
then were supposed to have special

00:01:06,180 --> 00:01:11,960
requirements on things and to this day

00:01:09,659 --> 00:01:14,130
the main base has been this kind of

00:01:11,960 --> 00:01:16,740
applications and telco companies in

00:01:14,130 --> 00:01:19,320
general that world I should still say

00:01:16,740 --> 00:01:21,570
that I consider this to have a much

00:01:19,320 --> 00:01:26,280
wider potential usage I'll go back to

00:01:21,570 --> 00:01:28,680
that so over the last few years I am

00:01:26,280 --> 00:01:31,610
some co-workers have spent a significant

00:01:28,680 --> 00:01:35,490
effort to improving the codebase

00:01:31,610 --> 00:01:38,009
modernizing it and make it making it

00:01:35,490 --> 00:01:42,829
more attractive for the general audience

00:01:38,009 --> 00:01:45,869
that we hope can be make use of this I

00:01:42,829 --> 00:01:48,060
should also mention that when I'm saying

00:01:45,869 --> 00:01:50,610
that using the world the word cluster

00:01:48,060 --> 00:01:53,909
throughout this presentation I don't

00:01:50,610 --> 00:01:56,549
mean anything particular with that it's

00:01:53,909 --> 00:01:58,590
this context it just means a group of

00:01:56,549 --> 00:02:05,549
nodes where all members of that group

00:01:58,590 --> 00:02:08,399
want to tightly monitor its neighbors so

00:02:05,549 --> 00:02:11,550
why do we want to monitor neighboring

00:02:08,399 --> 00:02:13,740
nodes at all well I would say in our

00:02:11,550 --> 00:02:16,950
world it's two main purposes

00:02:13,740 --> 00:02:20,100
first of all you want to typically want

00:02:16,950 --> 00:02:22,380
to have all connections to a failing

00:02:20,100 --> 00:02:24,950
node or an order who which comes becomes

00:02:22,380 --> 00:02:28,350
unavailable you want to have those

00:02:24,950 --> 00:02:30,570
aborted as soon as possible not within

00:02:28,350 --> 00:02:33,480
minutes within hours that is typical now

00:02:30,570 --> 00:02:37,650
but really within at the second level

00:02:33,480 --> 00:02:39,530
and even foster if possible and that is

00:02:37,650 --> 00:02:42,150
the first toss the second task is that

00:02:39,530 --> 00:02:44,460
typically in clusters there are cluster

00:02:42,150 --> 00:02:45,720
managers there might be group protocols

00:02:44,460 --> 00:02:49,470
there might be other users who are

00:02:45,720 --> 00:02:51,620
interested in in keeping track of which

00:02:49,470 --> 00:02:54,750
nodes are available at any moment

00:02:51,620 --> 00:02:58,170
generic service you can say and we want

00:02:54,750 --> 00:03:02,610
this to happen ASAP so within a

00:02:58,170 --> 00:03:05,460
well-defined short interval so if you

00:03:02,610 --> 00:03:07,590
look at common solutions I have seen I

00:03:05,460 --> 00:03:10,380
would rather call them naive solutions

00:03:07,590 --> 00:03:12,570
but I have seen them used one simple

00:03:10,380 --> 00:03:15,930
solution is to just you speed up the

00:03:12,570 --> 00:03:18,480
connections keep a lifetime err to some

00:03:15,930 --> 00:03:22,020
ridiculous high level second level sub

00:03:18,480 --> 00:03:23,670
second level and this works you get what

00:03:22,020 --> 00:03:26,610
he wants the connection is aborted if

00:03:23,670 --> 00:03:29,850
the tier becomes unavailable but what

00:03:26,610 --> 00:03:31,680
happens if which you in reality have

00:03:29,850 --> 00:03:34,260
thousands of such connections or tens of

00:03:31,680 --> 00:03:36,570
thousands even what happens is you the

00:03:34,260 --> 00:03:39,000
CPU load and even the network load

00:03:36,570 --> 00:03:41,970
generated by this dissipation becomes

00:03:39,000 --> 00:03:45,180
goes out of hand simply because it

00:03:41,970 --> 00:03:49,140
increases by linearly by the number of

00:03:45,180 --> 00:03:52,470
connections and so this is not a

00:03:49,140 --> 00:03:55,350
sustainable solution for the generic

00:03:52,470 --> 00:03:57,150
solution to this problem and what

00:03:55,350 --> 00:03:59,010
important that this does not provide any

00:03:57,150 --> 00:04:00,360
neighbor monitoring services was the

00:03:59,010 --> 00:04:04,830
second toss Callister in the previous

00:04:00,360 --> 00:04:07,610
slide the second solution the second the

00:04:04,830 --> 00:04:10,890
generic solution generic service is

00:04:07,610 --> 00:04:15,660
about what I've seen being done there is

00:04:10,890 --> 00:04:17,760
that people typically set up a demon on

00:04:15,660 --> 00:04:21,600
each cluster node and let each of those

00:04:17,760 --> 00:04:23,340
demons establish the connections to all

00:04:21,600 --> 00:04:27,210
the other demons in a full nest pattern

00:04:23,340 --> 00:04:29,070
full mesh pattern and then they

00:04:27,210 --> 00:04:31,050
they used to keep a lifetime there are

00:04:29,070 --> 00:04:33,090
some other monitoring mechanism to keep

00:04:31,050 --> 00:04:34,380
track of it this works it gives the

00:04:33,090 --> 00:04:37,680
service you want if you let others

00:04:34,380 --> 00:04:39,240
subscribe to the service the problem

00:04:37,680 --> 00:04:41,520
with this of course you might have false

00:04:39,240 --> 00:04:44,130
positives what if this demon crashes or

00:04:41,520 --> 00:04:46,650
becomes unresponsive while all the rest

00:04:44,130 --> 00:04:49,440
of that node is working fine so you have

00:04:46,650 --> 00:04:53,130
problems there more serious problem is

00:04:49,440 --> 00:04:54,780
that this doesn't scale much beyond

00:04:53,130 --> 00:04:57,570
around hundred notes all experience

00:04:54,780 --> 00:04:59,669
shows that then both CPU load but above

00:04:57,570 --> 00:05:05,580
all network load becomes overwhelming

00:04:59,669 --> 00:05:08,460
this is because the CPU load to maintain

00:05:05,580 --> 00:05:11,370
a number of timers that increases

00:05:08,460 --> 00:05:12,660
linearly with the number of nodes that

00:05:11,370 --> 00:05:14,820
the traffic the network traffic

00:05:12,660 --> 00:05:17,850
increases with because each node is

00:05:14,820 --> 00:05:19,889
monitoring in an end node restore n

00:05:17,850 --> 00:05:22,560
minus one neighbors it increases by

00:05:19,889 --> 00:05:24,930
square of n square of the size of the

00:05:22,560 --> 00:05:26,669
clusters so all experience shows we

00:05:24,930 --> 00:05:31,470
cannot use this pattern for more than

00:05:26,669 --> 00:05:33,960
about 200 nodes and the part of that

00:05:31,470 --> 00:05:37,289
this doesn't help to solving the source

00:05:33,960 --> 00:05:39,539
first task to aborted to somehow above

00:05:37,289 --> 00:05:42,810
automatically abort any other

00:05:39,539 --> 00:05:46,380
connections which are also there so we

00:05:42,810 --> 00:05:49,470
need something better and if we look at

00:05:46,380 --> 00:05:51,510
existing approaches to this there are

00:05:49,470 --> 00:05:56,340
several of them but I will mention three

00:05:51,510 --> 00:05:59,090
the first one is the tips a was doing

00:05:56,340 --> 00:06:02,550
this until the node 4.7

00:05:59,090 --> 00:06:05,280
which is you just to solve the neighbor

00:06:02,550 --> 00:06:08,130
monitoring problem you set up a full

00:06:05,280 --> 00:06:09,690
mesh framework according to similar

00:06:08,130 --> 00:06:11,190
little similar to the portal engine on

00:06:09,690 --> 00:06:12,930
the previous slide full mesh framework

00:06:11,190 --> 00:06:14,400
or what we call links not connections

00:06:12,930 --> 00:06:16,229
the reason we call them links is that

00:06:14,400 --> 00:06:19,020
these are running really at at the

00:06:16,229 --> 00:06:21,210
kernel level in a driver and they have

00:06:19,020 --> 00:06:25,110
been running directly across l2 and

00:06:21,210 --> 00:06:26,669
nowadays also across UDP so there are

00:06:25,110 --> 00:06:28,080
not really connections and they also use

00:06:26,669 --> 00:06:31,710
for more force than just monitoring

00:06:28,080 --> 00:06:33,690
neighbors so this provides the generic

00:06:31,710 --> 00:06:38,919
neighbor monitoring service that people

00:06:33,690 --> 00:06:41,139
want in this case also

00:06:38,919 --> 00:06:43,210
we have a sort of hierarchy so that each

00:06:41,139 --> 00:06:46,360
link end points keep track of all

00:06:43,210 --> 00:06:48,249
sockets on the local node having

00:06:46,360 --> 00:06:51,879
connections to the piano disappear node

00:06:48,249 --> 00:06:54,580
so when it discovers that the peer node

00:06:51,879 --> 00:06:58,900
is done it will immediately create a

00:06:54,580 --> 00:07:00,909
number of special messages sort of thin

00:06:58,900 --> 00:07:02,560
messages abort message it sends that

00:07:00,909 --> 00:07:04,029
pseudo message that ascends up to all

00:07:02,560 --> 00:07:06,400
these connect all these connected

00:07:04,029 --> 00:07:06,789
sockets and tell lookyou your pair is

00:07:06,400 --> 00:07:08,110
gone

00:07:06,789 --> 00:07:09,669
you have to clear down this connection

00:07:08,110 --> 00:07:12,039
and others what happens this also works

00:07:09,669 --> 00:07:18,189
fine and this part we definitely want to

00:07:12,039 --> 00:07:20,919
retain so but even this problem this

00:07:18,189 --> 00:07:22,779
solution has problems then because of

00:07:20,919 --> 00:07:25,539
the scaling it cannot scale much more

00:07:22,779 --> 00:07:28,449
bang up 200 nodes as I mentioned earlier

00:07:25,539 --> 00:07:31,529
the scalability here it goes by n a

00:07:28,449 --> 00:07:34,110
network load it grows by n square

00:07:31,529 --> 00:07:38,710
another solution that we have seen

00:07:34,110 --> 00:07:40,839
widely is a ring topology if you look at

00:07:38,710 --> 00:07:45,310
for instance chorusing which is I think

00:07:40,839 --> 00:07:47,909
a widely used cluster manager it offers

00:07:45,310 --> 00:07:50,979
two ways of monitoring neighbors either

00:07:47,909 --> 00:07:53,110
first of all it does set up a ring it

00:07:50,979 --> 00:07:57,009
organizes all the nodes in the cluster

00:07:53,110 --> 00:08:01,210
into a ring then it can either allow

00:07:57,009 --> 00:08:06,189
each node to monitor the two nearest

00:08:01,210 --> 00:08:09,069
neighbors in that ring which scales well

00:08:06,189 --> 00:08:12,069
because it increases only by by linearly

00:08:09,069 --> 00:08:14,589
by number of nodes another solution

00:08:12,069 --> 00:08:17,020
which also offering which is the

00:08:14,589 --> 00:08:20,560
principle the principle solution X

00:08:17,020 --> 00:08:22,539
recommend is to use something they they

00:08:20,560 --> 00:08:24,399
have built in which is called the totem

00:08:22,539 --> 00:08:26,349
protocol where they have an isolating

00:08:24,399 --> 00:08:31,300
token a token I to writing around the

00:08:26,349 --> 00:08:33,430
ring and each node is expecting to see

00:08:31,300 --> 00:08:35,800
this token passing by at a certain rate

00:08:33,430 --> 00:08:39,159
within a certain interval if it's

00:08:35,800 --> 00:08:40,870
missing that token it will conclude that

00:08:39,159 --> 00:08:42,820
something is wrong it was lost somewhere

00:08:40,870 --> 00:08:44,980
some no went down or there was a network

00:08:42,820 --> 00:08:46,750
problem somewhere then it has to track

00:08:44,980 --> 00:08:49,510
back and try to find out what happened

00:08:46,750 --> 00:08:52,190
where was a token lost not necessarily a

00:08:49,510 --> 00:08:55,130
trivial task but common to

00:08:52,190 --> 00:08:57,500
what the solution is it's very hard to

00:08:55,130 --> 00:08:59,170
handle accidential natural partitioning

00:08:57,500 --> 00:09:03,440
with these solutions it simply so that

00:08:59,170 --> 00:09:05,930
all two for this to work all nodes must

00:09:03,440 --> 00:09:07,910
know about what exactly what the ring

00:09:05,930 --> 00:09:09,950
looks like they must agree on that so

00:09:07,910 --> 00:09:12,890
you need a sort of consensus protocol

00:09:09,950 --> 00:09:17,300
which is not a trivial matter to to

00:09:12,890 --> 00:09:21,140
handle this so we would provide prefer

00:09:17,300 --> 00:09:24,470
something simpler and also I could say

00:09:21,140 --> 00:09:26,150
all our experience we have measured this

00:09:24,470 --> 00:09:28,310
is that this kind of solution does not

00:09:26,150 --> 00:09:30,950
scale to more than a few dozens of notes

00:09:28,310 --> 00:09:33,050
either it goes without saying that when

00:09:30,950 --> 00:09:36,200
this token is either rating the time it

00:09:33,050 --> 00:09:39,170
takes for IT rating also scales linearly

00:09:36,200 --> 00:09:43,330
with the number of nodes which means the

00:09:39,170 --> 00:09:46,610
discovery x increases third solution

00:09:43,330 --> 00:09:48,080
gossip the gossip protocol this is

00:09:46,610 --> 00:09:52,070
rather a class of protocols there are

00:09:48,080 --> 00:09:55,460
numerous varieties of it but a typical

00:09:52,070 --> 00:09:57,730
one and the most mostly used is that I

00:09:55,460 --> 00:10:00,290
think that you each node chooses to

00:09:57,730 --> 00:10:03,260
random by random it selects some

00:10:00,290 --> 00:10:04,670
neighbors to actively monitor and to

00:10:03,260 --> 00:10:09,700
exchange information with those

00:10:04,670 --> 00:10:14,120
neighbors at some regular regular

00:10:09,700 --> 00:10:16,250
interval and that information exchange

00:10:14,120 --> 00:10:21,320
can be about its its neighbors its

00:10:16,250 --> 00:10:25,070
network view for instance so this is

00:10:21,320 --> 00:10:26,750
very simple it scales very well I should

00:10:25,070 --> 00:10:30,590
first explain how it works first of all

00:10:26,750 --> 00:10:33,650
if a node X discovers node lost one of

00:10:30,590 --> 00:10:35,870
its supervise node nodes it will in step

00:10:33,650 --> 00:10:40,060
one in a middle figure you see it will

00:10:35,870 --> 00:10:42,710
it will send out this information

00:10:40,060 --> 00:10:45,020
through this periodic updates to all its

00:10:42,710 --> 00:10:47,810
no neighbors those neighbors in their

00:10:45,020 --> 00:10:51,350
turn will forward it to their own

00:10:47,810 --> 00:10:54,230
neighbors and eventually after X number

00:10:51,350 --> 00:10:56,210
of generations it will reach all nodes

00:10:54,230 --> 00:10:59,660
even in the farthest corner of the

00:10:56,210 --> 00:11:02,330
cluster and then you see the problem

00:10:59,660 --> 00:11:04,280
with this approach it's it may take

00:11:02,330 --> 00:11:05,130
quite a while until it reaches all the

00:11:04,280 --> 00:11:08,910
other nodes

00:11:05,130 --> 00:11:11,430
and the number of predict the generation

00:11:08,910 --> 00:11:14,630
is unpredictable and above all the time

00:11:11,430 --> 00:11:16,530
it takes before it before it's

00:11:14,630 --> 00:11:21,150
everybody's involved is unpredictable

00:11:16,530 --> 00:11:22,980
and also this produces quite a lot of

00:11:21,150 --> 00:11:24,780
extra network overall overhead because

00:11:22,980 --> 00:11:27,390
there will be a lot of duplicate

00:11:24,780 --> 00:11:30,150
information spreading nodes may receive

00:11:27,390 --> 00:11:31,560
the same information hopefully

00:11:30,150 --> 00:11:33,510
consistent you can't even be sure about

00:11:31,560 --> 00:11:35,850
that from different sources because

00:11:33,510 --> 00:11:37,530
anybody receiving an information it

00:11:35,850 --> 00:11:40,680
cannot really know what his neighbor has

00:11:37,530 --> 00:11:42,570
received already so this also has

00:11:40,680 --> 00:11:45,470
weaknesses although it scales extremely

00:11:42,570 --> 00:11:48,390
well and it's widely used there is a

00:11:45,470 --> 00:11:52,860
BitTorrent client called fibular that is

00:11:48,390 --> 00:11:55,890
using is so well working solution okay

00:11:52,860 --> 00:11:57,570
so the challenge we see is really we

00:11:55,890 --> 00:11:59,850
would like to find algorithm that has

00:11:57,570 --> 00:12:02,940
the scalability of gossip but with a

00:11:59,850 --> 00:12:08,820
deterministic set of peer nodes so that

00:12:02,940 --> 00:12:10,500
we know which monitor monitor and also I

00:12:08,820 --> 00:12:11,910
predict we also want to know that there

00:12:10,500 --> 00:12:14,130
is a predictable number of generations

00:12:11,910 --> 00:12:16,550
because we go before everybody all the

00:12:14,130 --> 00:12:20,970
other nodes updated about the change and

00:12:16,550 --> 00:12:23,250
this should be short we also wanted to

00:12:20,970 --> 00:12:26,220
have the lightweight properties of ring

00:12:23,250 --> 00:12:30,660
monitoring possible but less complex

00:12:26,220 --> 00:12:32,820
without the consensus protocol and still

00:12:30,660 --> 00:12:38,280
able to handle accident or accidental

00:12:32,820 --> 00:12:39,930
network partitioning so then we have the

00:12:38,280 --> 00:12:43,410
third solution the Tipsy solution which

00:12:39,930 --> 00:12:46,020
i we want to maintain the hierarchy the

00:12:43,410 --> 00:12:49,650
link to socket hierarchy because that

00:12:46,020 --> 00:12:52,770
works well we want to also to retain the

00:12:49,650 --> 00:12:55,110
full mesh link setup between all the

00:12:52,770 --> 00:12:56,970
nodes but we do not want to retain the

00:12:55,110 --> 00:12:59,220
full mesh act even monitoring of these

00:12:56,970 --> 00:12:59,880
links if you can get around that that

00:12:59,220 --> 00:13:04,410
would be nice

00:12:59,880 --> 00:13:06,600
and the answer to this challenge we have

00:13:04,410 --> 00:13:11,040
found is this protocol which I have

00:13:06,600 --> 00:13:14,820
called overlapping ring monitoring it's

00:13:11,040 --> 00:13:17,030
fairly simple if you start from the

00:13:14,820 --> 00:13:20,690
leftmost figure here

00:13:17,030 --> 00:13:22,310
each node starts with organizing all

00:13:20,690 --> 00:13:24,110
it's not always no neighbors the

00:13:22,310 --> 00:13:27,950
neighbors it discovers itself no other

00:13:24,110 --> 00:13:32,750
neighbors into a ring circular list in

00:13:27,950 --> 00:13:34,520
reality and this is based in tips on the

00:13:32,750 --> 00:13:36,380
numeric value of the of their node

00:13:34,520 --> 00:13:38,900
identity so the algorithm is the same

00:13:36,380 --> 00:13:41,780
everywhere but we still don't expect all

00:13:38,900 --> 00:13:47,350
nodes to at any time see the same ring

00:13:41,780 --> 00:13:49,580
they may differ on that that's okay then

00:13:47,350 --> 00:13:52,370
that is basically the only thing we

00:13:49,580 --> 00:13:55,130
borrow from the ring separation on top

00:13:52,370 --> 00:13:57,710
of this we apply something that looks a

00:13:55,130 --> 00:14:01,010
lot like a gossip protocol we choose a

00:13:57,710 --> 00:14:02,660
subset of the neighbors each node juices

00:14:01,010 --> 00:14:05,480
or subset of the neighbors to monitor

00:14:02,660 --> 00:14:09,230
actively monitor and there are two tasks

00:14:05,480 --> 00:14:10,790
to do there first you need to to find

00:14:09,230 --> 00:14:13,280
the optimal number of neighbors to

00:14:10,790 --> 00:14:15,200
monitor and then you have to select its

00:14:13,280 --> 00:14:16,030
members and the first task is very

00:14:15,200 --> 00:14:19,910
simple actually

00:14:16,030 --> 00:14:24,920
you take the square root of the cluster

00:14:19,910 --> 00:14:29,000
size and and then you that is the size

00:14:24,920 --> 00:14:31,250
of what we call the local domain I have

00:14:29,000 --> 00:14:33,380
chosen that that term for it because I

00:14:31,250 --> 00:14:38,450
will be using that word representation

00:14:33,380 --> 00:14:40,550
and then when it has done this it has

00:14:38,450 --> 00:14:44,270
started to chose to select the dos and

00:14:40,550 --> 00:14:46,430
minus 1x actually it since it is insert

00:14:44,270 --> 00:14:51,340
clue the included in the local domain it

00:14:46,430 --> 00:14:55,880
has two monitors the N root of n minus 1

00:14:51,340 --> 00:14:59,000
next nodes downstream in the ring add to

00:14:55,880 --> 00:15:01,070
the monitor from their own it creates

00:14:59,000 --> 00:15:03,170
what we call a domain record local

00:15:01,070 --> 00:15:05,060
domain record that it sends out to all

00:15:03,170 --> 00:15:07,250
its neighbors all nodes in the cluster

00:15:05,060 --> 00:15:11,210
are telling I am taking a responsibility

00:15:07,250 --> 00:15:13,130
for monitoring these nodes take it or

00:15:11,210 --> 00:15:20,870
leave it you don't like it don't use it

00:15:13,130 --> 00:15:22,940
if you find it useful use it then this

00:15:20,870 --> 00:15:26,639
is not enough because we still need to

00:15:22,940 --> 00:15:31,589
be able to solve the task of discovering

00:15:26,639 --> 00:15:34,410
a natural partition and here we use a

00:15:31,589 --> 00:15:36,629
second trick the node select a number of

00:15:34,410 --> 00:15:38,819
nodes outside is local domain for active

00:15:36,629 --> 00:15:42,029
monitoring and it selects them in such a

00:15:38,819 --> 00:15:44,129
way that no nodes in the in the cluster

00:15:42,029 --> 00:15:45,689
is more than two active monitoring hops

00:15:44,129 --> 00:15:50,639
away this is what you see in the mid

00:15:45,689 --> 00:15:52,939
figure here so you end up with if you

00:15:50,639 --> 00:15:56,040
analyze this you end up with another

00:15:52,939 --> 00:15:59,790
square root of n minus 1 nodes to

00:15:56,040 --> 00:16:02,160
monitor and you know that whatever

00:15:59,790 --> 00:16:03,809
happens whichever node is crashing in

00:16:02,160 --> 00:16:07,829
this cluster you will learn about it

00:16:03,809 --> 00:16:09,989
either directly or indirectly and you

00:16:07,829 --> 00:16:13,439
can take action from there so this

00:16:09,989 --> 00:16:16,799
solves the partitioning problem then we

00:16:13,439 --> 00:16:20,970
apply this on all nodes so you end up

00:16:16,799 --> 00:16:24,569
with 2 times n times square root of n

00:16:20,970 --> 00:16:27,119
minus 1 actively monitored links which

00:16:24,569 --> 00:16:30,149
may sound a lot but it's really it's it

00:16:27,119 --> 00:16:32,970
scales by n times root of n which is a

00:16:30,149 --> 00:16:34,860
lot better than N squared if you look at

00:16:32,970 --> 00:16:37,319
the figures here if you look at the

00:16:34,860 --> 00:16:44,939
frame below the last figure here you'll

00:16:37,319 --> 00:16:46,679
see that in in in a 16 oh just like the

00:16:44,939 --> 00:16:51,329
one I'm showing here instead of having

00:16:46,679 --> 00:16:54,360
15 times 16 knows the monitor or links

00:16:51,329 --> 00:16:56,939
to monitor links which is 240 you end up

00:16:54,360 --> 00:17:01,499
with 96 which is scale factor of 2 and

00:16:56,939 --> 00:17:05,299
1/2 and if you scale up further if you

00:17:01,499 --> 00:17:10,079
take an 800 node links 800 node cluster

00:17:05,299 --> 00:17:13,459
which is sustainable the result is even

00:17:10,079 --> 00:17:17,970
more more impressive instead of having

00:17:13,459 --> 00:17:22,529
800 square 640,000 links to monitor you

00:17:17,970 --> 00:17:24,510
end up with with 44,000 and this is of

00:17:22,529 --> 00:17:30,330
course a factor of 15 which makes a huge

00:17:24,510 --> 00:17:32,539
difference so then they have the special

00:17:30,330 --> 00:17:37,250
cases what happens if he loses a note

00:17:32,539 --> 00:17:37,250
what do you do with this algorithm

00:17:37,350 --> 00:17:41,400
the first there are three cases in

00:17:38,580 --> 00:17:44,070
reality to consider first is if what if

00:17:41,400 --> 00:17:47,180
you lose a node that is directly

00:17:44,070 --> 00:17:53,520
monitored within the local domain then

00:17:47,180 --> 00:17:56,180
you have to create this domain record at

00:17:53,520 --> 00:17:59,400
as percent I described earlier that is

00:17:56,180 --> 00:18:02,930
describing the whole domain

00:17:59,400 --> 00:18:07,500
it contains node identity it contains

00:18:02,930 --> 00:18:10,490
what this node it's v1 it needs only on

00:18:07,500 --> 00:18:13,350
that node is it up or is it down and

00:18:10,490 --> 00:18:15,360
then it sends out this domain record to

00:18:13,350 --> 00:18:17,220
all other nodes just like when the notes

00:18:15,360 --> 00:18:19,080
came up with now distributes a whole

00:18:17,220 --> 00:18:21,150
recognize a it goes down it went down so

00:18:19,080 --> 00:18:26,130
everybody has to learn that and it will

00:18:21,150 --> 00:18:28,980
learn that within less than a half

00:18:26,130 --> 00:18:31,620
second the thing is that when I develop

00:18:28,980 --> 00:18:33,450
this I was had the option to choose

00:18:31,620 --> 00:18:38,670
between distributing this by multicast

00:18:33,450 --> 00:18:41,220
or by some sort of of periodic update

00:18:38,670 --> 00:18:44,700
like we do typically in gossip and they

00:18:41,220 --> 00:18:47,400
opted for the latter one totally before

00:18:44,700 --> 00:18:49,200
there because the multi cost reliable

00:18:47,400 --> 00:18:51,480
multicast obviously had back then was

00:18:49,200 --> 00:18:53,580
not could not sustain this amount of or

00:18:51,480 --> 00:18:56,250
traffic now it can but I still think

00:18:53,580 --> 00:18:59,220
this is a good solution so what we are

00:18:56,250 --> 00:19:01,080
doing is since after such a change the

00:18:59,220 --> 00:19:04,680
node still has to start sending out

00:19:01,080 --> 00:19:06,390
probe messages to everybody else and

00:19:04,680 --> 00:19:08,460
update all the neighbors so you

00:19:06,390 --> 00:19:11,250
piggyback back on these probe messages

00:19:08,460 --> 00:19:14,340
you have you piggyback this domain

00:19:11,250 --> 00:19:18,930
record so everybody will learn about it

00:19:14,340 --> 00:19:20,540
within 300 default configurable value is

00:19:18,930 --> 00:19:24,570
three hundred and seventy-five

00:19:20,540 --> 00:19:28,920
milliseconds within that frame everybody

00:19:24,570 --> 00:19:30,900
will be updated also it contains a

00:19:28,920 --> 00:19:34,110
generation I and this is because this

00:19:30,900 --> 00:19:36,990
probing is going across unreliable media

00:19:34,110 --> 00:19:39,680
either UDP or l2 so it may be lost and

00:19:36,990 --> 00:19:42,870
you don't want to the recipient to

00:19:39,680 --> 00:19:46,440
reapply this record over and over again

00:19:42,870 --> 00:19:49,530
you want it to discover duplicates and

00:19:46,440 --> 00:19:51,090
just discard them and it has to act that

00:19:49,530 --> 00:19:53,129
now I received

00:19:51,090 --> 00:19:55,649
I have received number generation number

00:19:53,129 --> 00:19:59,039
X and you can stop sending and it will

00:19:55,649 --> 00:20:01,080
stop sending it so this is the first and

00:19:59,039 --> 00:20:05,100
simplest case then you have a second

00:20:01,080 --> 00:20:07,139
case what if you lose what I call the

00:20:05,100 --> 00:20:10,019
head node earlier one of these node you

00:20:07,139 --> 00:20:12,779
selected outside your own domain to

00:20:10,019 --> 00:20:16,019
monitor and you see this in the in the

00:20:12,779 --> 00:20:18,539
first figure here you detect by active

00:20:16,019 --> 00:20:21,389
monitoring failure now you have to

00:20:18,539 --> 00:20:22,980
figure out was this due to a network

00:20:21,389 --> 00:20:25,139
partition or was it just this node that

00:20:22,980 --> 00:20:27,419
went down well you know how to handle it

00:20:25,139 --> 00:20:30,149
you know which nodes this particular

00:20:27,419 --> 00:20:31,740
node is monitoring it has told you it

00:20:30,149 --> 00:20:33,629
has monitoring it is my husband it is

00:20:31,740 --> 00:20:36,179
monitoring so you just send a quick

00:20:33,629 --> 00:20:37,919
probe message to all of those and if

00:20:36,179 --> 00:20:40,169
they respond everything is fine if they

00:20:37,919 --> 00:20:43,470
don't respond you continue to probe the

00:20:40,169 --> 00:20:48,710
regular probing algorithm until you find

00:20:43,470 --> 00:20:50,669
that they all gone down and after that

00:20:48,710 --> 00:20:55,340
normally of course you'll find they're

00:20:50,669 --> 00:20:59,429
up after that you recalculate this whole

00:20:55,340 --> 00:21:02,820
gossiped monitoring algorithm monitoring

00:20:59,429 --> 00:21:05,850
topology so that you you select new

00:21:02,820 --> 00:21:07,919
heads and typically you of course after

00:21:05,850 --> 00:21:10,710
such an arrow you or such a failure you

00:21:07,919 --> 00:21:13,110
step each head in the remaining total

00:21:10,710 --> 00:21:14,090
during one step forward that's a simple

00:21:13,110 --> 00:21:16,200
as that

00:21:14,090 --> 00:21:18,389
remember this is an entirely local

00:21:16,200 --> 00:21:21,360
operation on this node everybody all

00:21:18,389 --> 00:21:23,899
nodes are doing this autonomously and

00:21:21,360 --> 00:21:26,519
their topology may look very different

00:21:23,899 --> 00:21:28,139
both the ring topology although

00:21:26,519 --> 00:21:31,919
typically those will be the same but

00:21:28,139 --> 00:21:36,570
also the Supervision topology monitoring

00:21:31,919 --> 00:21:38,279
topology third case what happens if a

00:21:36,570 --> 00:21:41,940
node that you are not directly

00:21:38,279 --> 00:21:44,850
monitoring is lost not actively

00:21:41,940 --> 00:21:46,440
monitoring well you know you can trust

00:21:44,850 --> 00:21:48,450
that there are some other nodes out

00:21:46,440 --> 00:21:51,720
there monitoring it because they are

00:21:48,450 --> 00:21:54,710
told you so those node will one after

00:21:51,720 --> 00:21:59,940
another discover that this node was lost

00:21:54,710 --> 00:22:02,789
they will send out their their local

00:21:59,940 --> 00:22:04,030
domain records so this guy he would

00:22:02,789 --> 00:22:06,940
receive first one

00:22:04,030 --> 00:22:10,000
after the first one he stores actively

00:22:06,940 --> 00:22:13,120
monitoring this note just sending out a

00:22:10,000 --> 00:22:15,940
probe then he would receive one more one

00:22:13,120 --> 00:22:18,640
more after the fourth one he takes for

00:22:15,940 --> 00:22:19,960
granted that yes the noticed gun I don't

00:22:18,640 --> 00:22:21,010
need to probe anymore I just shut it

00:22:19,960 --> 00:22:23,560
down say it's gone

00:22:21,010 --> 00:22:25,990
I think this of course means that risk

00:22:23,560 --> 00:22:29,740
slight theoretical risk for false

00:22:25,990 --> 00:22:31,660
positives it may still be up but I think

00:22:29,740 --> 00:22:33,850
that risk is worth taking it's really

00:22:31,660 --> 00:22:42,040
theoretical and normally it doesn't

00:22:33,850 --> 00:22:46,330
cause any fault fatal problems so then

00:22:42,040 --> 00:22:48,070
you have of course what the example have

00:22:46,330 --> 00:22:52,330
been working with and showing you so far

00:22:48,070 --> 00:22:56,440
all the way through is a nice 16 node

00:22:52,330 --> 00:23:00,490
cluster nice square number symmetric

00:22:56,440 --> 00:23:02,590
full mesh everything that is not reality

00:23:00,490 --> 00:23:05,440
all the time and especially not in

00:23:02,590 --> 00:23:07,210
transitional periods when when the node

00:23:05,440 --> 00:23:11,220
is coming cluster is coming up or when

00:23:07,210 --> 00:23:14,740
there are use changes going on in it so

00:23:11,220 --> 00:23:16,480
there there are numerous cases where

00:23:14,740 --> 00:23:18,910
different nodes will have different

00:23:16,480 --> 00:23:21,010
network news and we have to handle that

00:23:18,910 --> 00:23:24,660
and if you analyze this there is

00:23:21,010 --> 00:23:28,390
actually only two cases to consider

00:23:24,660 --> 00:23:31,240
first one is that a node may have

00:23:28,390 --> 00:23:33,790
discovered a peer that nobody else is

00:23:31,240 --> 00:23:37,600
monitoring what do you do then you

00:23:33,790 --> 00:23:40,180
declare the heads a domain head and act

00:23:37,600 --> 00:23:44,500
start actively monitoring it and then

00:23:40,180 --> 00:23:48,490
you go on like like normal if it gives

00:23:44,500 --> 00:23:51,940
you a local domain that it is that is it

00:23:48,490 --> 00:23:54,520
is monitoring you apply these to the

00:23:51,940 --> 00:23:57,730
extent possible if there is none you

00:23:54,520 --> 00:24:00,040
accept that there's no domain but anyway

00:23:57,730 --> 00:24:03,490
you cost to continue calculating you

00:24:00,040 --> 00:24:06,430
gossip topology and to select the next

00:24:03,490 --> 00:24:10,330
heads as the first one that is not

00:24:06,430 --> 00:24:14,250
monitored by anybody else in this case

00:24:10,330 --> 00:24:16,920
and so this is the first case

00:24:14,250 --> 00:24:19,400
second case is what if some nodes tells

00:24:16,920 --> 00:24:22,020
you that yes I'm monitoring node number

00:24:19,400 --> 00:24:23,430
seven and I haven't discovered node

00:24:22,020 --> 00:24:27,090
number seven I don't know anything about

00:24:23,430 --> 00:24:30,030
the number seven what I do then well I

00:24:27,090 --> 00:24:32,220
don't apply it to my ring it goes in

00:24:30,030 --> 00:24:34,200
this string there are only nodes that I

00:24:32,220 --> 00:24:36,930
have discovered myself I only rely my

00:24:34,200 --> 00:24:40,380
own discoveries my own knowledge what I

00:24:36,930 --> 00:24:45,180
do instead is I retain this this domain

00:24:40,380 --> 00:24:46,890
record for future use because it's quite

00:24:45,180 --> 00:24:48,600
likely during the transit error that I

00:24:46,890 --> 00:24:51,270
will discover this node later and then I

00:24:48,600 --> 00:24:54,630
can just reapply this and I'm fine and

00:24:51,270 --> 00:24:55,920
recalculate the topology so those are

00:24:54,630 --> 00:25:04,230
the two simple case you have to consider

00:24:55,920 --> 00:25:06,240
and everything is covered so what does

00:25:04,230 --> 00:25:07,530
it look like in tip see if you take a

00:25:06,240 --> 00:25:11,820
look at the example that have been

00:25:07,530 --> 00:25:16,020
showing all along now we have the 60 no

00:25:11,820 --> 00:25:18,420
clusters we have these these each node

00:25:16,020 --> 00:25:21,540
is monitoring a certain number of all

00:25:18,420 --> 00:25:26,070
the neighbors it looks like as you will

00:25:21,540 --> 00:25:28,320
see in the in the shell here I am on

00:25:26,070 --> 00:25:30,930
node number one that's what you see in

00:25:28,320 --> 00:25:34,140
the first line there and then I'm

00:25:30,930 --> 00:25:36,180
listening what each node of those nodes

00:25:34,140 --> 00:25:37,470
in the cluster have told me that that

00:25:36,180 --> 00:25:41,340
they are monitoring and what they

00:25:37,470 --> 00:25:44,190
consider about the state of this obvious

00:25:41,340 --> 00:25:47,150
node so first of all known number one is

00:25:44,190 --> 00:25:50,130
its self monitoring node number 2 T for

00:25:47,150 --> 00:25:53,700
the U to the right there means this is

00:25:50,130 --> 00:25:55,290
this an array just reflecting it we have

00:25:53,700 --> 00:25:57,660
to put this very compact because of the

00:25:55,290 --> 00:26:02,700
amount of information so the position of

00:25:57,660 --> 00:26:04,350
this use gives which node I think is up

00:26:02,700 --> 00:26:07,290
so the first node the first new means

00:26:04,350 --> 00:26:12,690
node 2 is up the second u means node 3

00:26:07,290 --> 00:26:14,400
is up third one node 4 is up then so

00:26:12,690 --> 00:26:16,650
those are directly monitored because

00:26:14,400 --> 00:26:19,590
they are part of the local domain then

00:26:16,650 --> 00:26:21,930
node number 5 that is also directly

00:26:19,590 --> 00:26:26,570
monitored the course I selected that to

00:26:21,930 --> 00:26:28,730
be a head node then I

00:26:26,570 --> 00:26:30,530
learn from the fact that note number

00:26:28,730 --> 00:26:32,570
five told me it is supervising it's

00:26:30,530 --> 00:26:36,679
supervising number six seven and eight

00:26:32,570 --> 00:26:40,100
so I don't monitor those I rely on

00:26:36,679 --> 00:26:40,690
information from node number five and so

00:26:40,100 --> 00:26:43,549
forth

00:26:40,690 --> 00:26:49,190
so this is as simple as it looks and in

00:26:43,549 --> 00:26:52,400
the 660 node example and yeah it works

00:26:49,190 --> 00:26:56,660
fine now I'll show a slightly more

00:26:52,400 --> 00:27:00,350
advanced example I have a 600 node

00:26:56,660 --> 00:27:03,590
cluster and here you see that each node

00:27:00,350 --> 00:27:06,590
now is monitoring square root of six

00:27:03,590 --> 00:27:09,950
hundred round the downloads of course

00:27:06,590 --> 00:27:11,690
so it's monitoring fifty nodes each 50

00:27:09,950 --> 00:27:15,350
neighbors each and you see that they

00:27:11,690 --> 00:27:19,580
erase here are much larger er this is 15

00:27:15,350 --> 00:27:21,169
is actually 25 sorry so just for the

00:27:19,580 --> 00:27:25,460
sake of it I took down one of the notes

00:27:21,169 --> 00:27:28,640
the 590 for all this and then you will

00:27:25,460 --> 00:27:31,100
see if you look at the at the array to

00:27:28,640 --> 00:27:33,669
the right that there is a d uppercase D

00:27:31,100 --> 00:27:36,620
going as a diagonal upwards to the right

00:27:33,669 --> 00:27:39,140
which means that all these nodes where

00:27:36,620 --> 00:27:41,660
or supervising or web or supervising

00:27:39,140 --> 00:27:45,770
node number 594 and they all think it's

00:27:41,660 --> 00:27:47,960
down so you have this information so

00:27:45,770 --> 00:27:50,150
when I took down this node did it happen

00:27:47,960 --> 00:27:52,190
did I discover it within one second well

00:27:50,150 --> 00:27:54,830
I didn't one or half one and a half in

00:27:52,190 --> 00:27:57,530
two seconds so this is where we are and

00:27:54,830 --> 00:28:00,049
once again these values are configurable

00:27:57,530 --> 00:28:01,490
you can in a smaller cluster easily turn

00:28:00,049 --> 00:28:05,809
it down to sub second levels if you

00:28:01,490 --> 00:28:10,750
wanted I have just be sticking to the

00:28:05,809 --> 00:28:13,620
default values so that's where your

00:28:10,750 --> 00:28:20,260
thank you

00:28:13,620 --> 00:28:20,260

YouTube URL: https://www.youtube.com/watch?v=ni-iNJ-njPo


