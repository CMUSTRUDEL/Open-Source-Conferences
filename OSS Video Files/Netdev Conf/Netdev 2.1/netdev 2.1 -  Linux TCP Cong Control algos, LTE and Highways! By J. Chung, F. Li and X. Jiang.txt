Title: netdev 2.1 -  Linux TCP Cong Control algos, LTE and Highways! By J. Chung, F. Li and X. Jiang
Publication date: 2017-05-10
Playlist: Netdev 2.1
Description: 
	In this talk given on the 7th of April, 2017 at Netdev 2.1 in Montreal,  Jae Won Chung discusses Verizon Labs experience in understanding the impact of the different TCP congestion control algorithms on LTE networks on moving vehicles.

Verizon wanted a deeper understanding instead of just  
putting efforts to improve throughput by fine tuning the different
system knobs.


Content at: 
https://www.netdevconf.org/2.1/session.html?chung
Captions: 
	00:00:00,000 --> 00:00:07,680
okay hi this is Jay Chung part of the

00:00:03,870 --> 00:00:09,630
Verizon law of rising labs today we I'm

00:00:07,680 --> 00:00:11,940
going to present the work that I have

00:00:09,630 --> 00:00:15,299
done with no family and child showing

00:00:11,940 --> 00:00:18,119
our team on the driving TCP congestion

00:00:15,299 --> 00:00:21,769
control algorithm on the highway okay so

00:00:18,119 --> 00:00:25,590
why are we interested in this basically

00:00:21,769 --> 00:00:27,480
you know as other the wireless carriers

00:00:25,590 --> 00:00:30,929
we want it to be a fastest the radio

00:00:27,480 --> 00:00:32,579
access network in the market so we look

00:00:30,929 --> 00:00:34,440
at like you know how can we actually

00:00:32,579 --> 00:00:38,309
improve the the user perceived

00:00:34,440 --> 00:00:42,930
performance of the network and look at

00:00:38,309 --> 00:00:46,730
the the TCP and the TCP is actually the

00:00:42,930 --> 00:00:49,920
major traffic source in the market and

00:00:46,730 --> 00:00:52,020
we know that the most of the TCP flows

00:00:49,920 --> 00:00:55,590
are using additive increase and

00:00:52,020 --> 00:00:57,539
multiplicative you know decrease of aimd

00:00:55,590 --> 00:01:02,670
based congestion control algorithm

00:00:57,539 --> 00:01:06,720
including cubic so we look at the the a

00:01:02,670 --> 00:01:09,299
I and D more closely and in the

00:01:06,720 --> 00:01:14,189
algorithms are in general not friendly

00:01:09,299 --> 00:01:15,390
to the radio access network so Angie

00:01:14,189 --> 00:01:18,090
doesn't effectively consume the

00:01:15,390 --> 00:01:20,520
available bandwidth because of the the

00:01:18,090 --> 00:01:21,990
slow ramp-up time even though the slow

00:01:20,520 --> 00:01:27,360
start

00:01:21,990 --> 00:01:29,549
says is a fast in a ramp up and also we

00:01:27,360 --> 00:01:31,710
also found that the you know you know

00:01:29,549 --> 00:01:34,619
the vendors actually implemented no

00:01:31,710 --> 00:01:36,930
active queue management not for the

00:01:34,619 --> 00:01:40,049
traffic but for their you know buffer

00:01:36,930 --> 00:01:42,720
management so they want to actually

00:01:40,049 --> 00:01:46,130
limit the you know they're you know be

00:01:42,720 --> 00:01:48,450
buffered usage by randomly dropping

00:01:46,130 --> 00:01:52,680
packets so the source connector slow

00:01:48,450 --> 00:01:57,659
down in case the no buffer is actually

00:01:52,680 --> 00:02:00,119
getting filled so how do we actually

00:01:57,659 --> 00:02:03,060
solve this problem and you know one

00:02:00,119 --> 00:02:04,920
thing that we came up with not not just

00:02:03,060 --> 00:02:07,049
us but you know the whole community

00:02:04,920 --> 00:02:09,590
actually came up with was hey can we

00:02:07,049 --> 00:02:13,560
actually do the the performance enhanced

00:02:09,590 --> 00:02:18,599
proxy to actually buffer

00:02:13,560 --> 00:02:21,930
or for packets and use like no some

00:02:18,599 --> 00:02:24,360
other you know condition control

00:02:21,930 --> 00:02:27,840
mechanics and doesn't rather than aimd

00:02:24,360 --> 00:02:31,620
to let's say you control the TS rate on

00:02:27,840 --> 00:02:34,410
the right side basically those that were

00:02:31,620 --> 00:02:38,819
the you know the objective of our

00:02:34,410 --> 00:02:43,050
research and so we wanted to actually

00:02:38,819 --> 00:02:46,850
achieve say for the small web objects we

00:02:43,050 --> 00:02:51,120
want the know fastest download time and

00:02:46,850 --> 00:02:53,310
also for the in a large objects we want

00:02:51,120 --> 00:02:57,530
to actually maximize the good bit right

00:02:53,310 --> 00:03:00,989
and also we want to maintain the low

00:02:57,530 --> 00:03:05,430
self-inflicted RTT to avoid unnecessary

00:03:00,989 --> 00:03:07,380
drops by a node B hmm so those are the

00:03:05,430 --> 00:03:12,750
no three object we are actually looking

00:03:07,380 --> 00:03:15,540
to achieve okay so look at like know

00:03:12,750 --> 00:03:17,609
then we look at like how should we

00:03:15,540 --> 00:03:19,560
actually implement the the performance

00:03:17,609 --> 00:03:22,440
enhance proxy so the technical

00:03:19,560 --> 00:03:26,280
challenges are we want fast time to

00:03:22,440 --> 00:03:29,549
market and then adapt fast adapt to the

00:03:26,280 --> 00:03:31,620
emerging technology and also we wanted

00:03:29,549 --> 00:03:35,910
to reduce the software maintenance in a

00:03:31,620 --> 00:03:37,650
headache so having all this objective

00:03:35,910 --> 00:03:41,250
like now one of the the attractive

00:03:37,650 --> 00:03:44,549
potential solution is use a transparent

00:03:41,250 --> 00:03:48,150
implement or you know purchase

00:03:44,549 --> 00:03:52,590
transparent um you know key kept using

00:03:48,150 --> 00:03:55,319
open source tcp prophesy on top of linux

00:03:52,590 --> 00:03:56,790
TCP stack and networking stack and play

00:03:55,319 --> 00:03:58,709
around with the the congestion avoidance

00:03:56,790 --> 00:04:03,750
algorithm okay

00:03:58,709 --> 00:04:05,250
so we look at existing congestion

00:04:03,750 --> 00:04:07,889
control algorithms that's actually part

00:04:05,250 --> 00:04:11,850
of the lint staff we actually look at

00:04:07,889 --> 00:04:14,180
like you know BB are like a new know TCP

00:04:11,850 --> 00:04:18,840
stack that actually is emerging from

00:04:14,180 --> 00:04:20,220
different use cases and also we look at

00:04:18,840 --> 00:04:22,680
like now what can we do to actually

00:04:20,220 --> 00:04:25,110
improve the TCP you know the performance

00:04:22,680 --> 00:04:29,610
on the wireless

00:04:25,110 --> 00:04:31,439
so um what do we know about the the TCP

00:04:29,610 --> 00:04:36,569
congestion control algorithm performance

00:04:31,439 --> 00:04:38,460
on the LTE so I actually look at all you

00:04:36,569 --> 00:04:41,189
know different papers and stuff but

00:04:38,460 --> 00:04:44,250
there is not particularly nur in the LTE

00:04:41,189 --> 00:04:46,289
network in terms of performance we look

00:04:44,250 --> 00:04:48,210
at the cubic I actually myself play

00:04:46,289 --> 00:04:52,250
around with all the cubic partners for a

00:04:48,210 --> 00:04:54,719
while but you know basically I was not

00:04:52,250 --> 00:04:58,530
the cubic didn't really impressed me and

00:04:54,719 --> 00:05:01,919
also Westwood plus they say Westwood

00:04:58,530 --> 00:05:04,409
plus is actually no more turning the the

00:05:01,919 --> 00:05:06,330
the RTT and available bandwidth and

00:05:04,409 --> 00:05:10,020
stuff like that but it didn't really

00:05:06,330 --> 00:05:12,210
work that well it all end up within a

00:05:10,020 --> 00:05:14,099
low link or irrigation even though there

00:05:12,210 --> 00:05:18,210
are tons of the wireless bandwidth

00:05:14,099 --> 00:05:22,250
available and we actually look at like

00:05:18,210 --> 00:05:24,240
no conference papers they are

00:05:22,250 --> 00:05:26,580
experimental TCP wireless link

00:05:24,240 --> 00:05:29,250
implementations but most of them

00:05:26,580 --> 00:05:31,469
actually use the UDP because maybe they

00:05:29,250 --> 00:05:34,199
don't understand the TCP that well or

00:05:31,469 --> 00:05:37,199
they want to do like you know change not

00:05:34,199 --> 00:05:39,050
only the the source I mean they want to

00:05:37,199 --> 00:05:41,819
actually implement at the both the end

00:05:39,050 --> 00:05:43,440
to actually to be really flexible so

00:05:41,819 --> 00:05:46,259
they actually implement their source and

00:05:43,440 --> 00:05:49,469
dia in our client behavior server and

00:05:46,259 --> 00:05:53,699
the client behavior over the TCP well we

00:05:49,469 --> 00:05:56,039
can't really use that and we have also

00:05:53,699 --> 00:05:57,659
looked at other you know condition

00:05:56,039 --> 00:06:03,389
control rhythm design for data centers

00:05:57,659 --> 00:06:07,020
like you know bbr and you know the new

00:06:03,389 --> 00:06:10,379
vegas so basically we are we were

00:06:07,020 --> 00:06:13,680
actually looking at no those things so

00:06:10,379 --> 00:06:16,259
but but there is not a clear winner at

00:06:13,680 --> 00:06:17,819
this point and we don't really have too

00:06:16,259 --> 00:06:20,819
much knowledge on performance and high

00:06:17,819 --> 00:06:25,020
mobility so basically almost all the

00:06:20,819 --> 00:06:28,199
test was actually you know done in what

00:06:25,020 --> 00:06:30,719
is it the stationary test environment

00:06:28,199 --> 00:06:32,969
then there are some like you know

00:06:30,719 --> 00:06:36,199
researches on on high mobility but there

00:06:32,969 --> 00:06:38,370
are not too many of them so thing

00:06:36,199 --> 00:06:39,690
actually suggested that Oh

00:06:38,370 --> 00:06:41,880
issues actually just do the drive

00:06:39,690 --> 00:06:44,940
testing you know when we are actually

00:06:41,880 --> 00:06:46,979
driving down to New Jersey New Jersey we

00:06:44,940 --> 00:06:50,040
have a headquarters so and we sometimes

00:06:46,979 --> 00:06:52,290
have a meeting there so and that was a

00:06:50,040 --> 00:06:55,860
great idea so we have to set up the

00:06:52,290 --> 00:07:00,690
scene you know test and test method that

00:06:55,860 --> 00:07:08,449
did the test you know over white LTE

00:07:00,690 --> 00:07:13,740
network okay so so let's actually get to

00:07:08,449 --> 00:07:16,470
the fun part so I'm going to talk about

00:07:13,740 --> 00:07:18,960
what we actually compared so we actually

00:07:16,470 --> 00:07:23,039
you know compared to bbr with two

00:07:18,960 --> 00:07:24,960
versions of cubic the VBR bottle net

00:07:23,039 --> 00:07:28,139
bandwidth and round-trip propagation

00:07:24,960 --> 00:07:30,960
delay that algorithm was you know made

00:07:28,139 --> 00:07:35,010
available as part of the before point

00:07:30,960 --> 00:07:37,380
eight kernel and I believe this was

00:07:35,010 --> 00:07:39,240
actually originally designed for the the

00:07:37,380 --> 00:07:41,220
data within data center communication or

00:07:39,240 --> 00:07:45,360
in between the essence of communication

00:07:41,220 --> 00:07:48,389
for server-to-server however this looks

00:07:45,360 --> 00:07:50,760
really good to me because it actually

00:07:48,389 --> 00:07:53,880
goes is actually dropped the idea of a

00:07:50,760 --> 00:07:57,150
IMD but it actually ducts to the the new

00:07:53,880 --> 00:07:59,760
you know mechanism of estimating the

00:07:57,150 --> 00:08:04,710
available bandwidth and sending at the

00:07:59,760 --> 00:08:09,900
rate of you know like that one point I

00:08:04,710 --> 00:08:13,380
think 1.25 by default X of the estimated

00:08:09,900 --> 00:08:15,990
available bandwidth so that actually

00:08:13,380 --> 00:08:20,460
looks really good to me so you know we

00:08:15,990 --> 00:08:23,280
chose that and we actually took the the

00:08:20,460 --> 00:08:25,139
two versions of cubic they're not much

00:08:23,280 --> 00:08:28,440
different I mean there are four point

00:08:25,139 --> 00:08:32,479
eight version and 3.19 kernel version of

00:08:28,440 --> 00:08:36,180
cubic the four point eight introduces

00:08:32,479 --> 00:08:38,640
you know a fix for you know when the

00:08:36,180 --> 00:08:40,430
application goes the idle and then

00:08:38,640 --> 00:08:44,730
restart transmission like you know

00:08:40,430 --> 00:08:47,700
there's a long idle time and the the

00:08:44,730 --> 00:08:50,880
event handling of the day was actually

00:08:47,700 --> 00:08:53,640
no added to three point one nine

00:08:50,880 --> 00:08:56,460
version two immediately follow dia

00:08:53,640 --> 00:08:58,140
follow the cubic curve so we actually

00:08:56,460 --> 00:09:03,210
tested like you know those three

00:08:58,140 --> 00:09:05,340
versions okay so what we did was we

00:09:03,210 --> 00:09:08,820
actually set up like you know three

00:09:05,340 --> 00:09:12,360
identical servers using C seven thousand

00:09:08,820 --> 00:09:14,760
chassis at the HPC seven thousand one we

00:09:12,360 --> 00:09:18,390
were running the cubic three point one

00:09:14,760 --> 00:09:22,830
nine one is four point eight and one

00:09:18,390 --> 00:09:25,800
with the bbr and our hands that was

00:09:22,830 --> 00:09:29,250
actually home to the Westborough

00:09:25,800 --> 00:09:32,790
Massachusetts you know P gateway and

00:09:29,250 --> 00:09:34,890
then we just start driving from you know

00:09:32,790 --> 00:09:39,680
our office in Massachusetts

00:09:34,890 --> 00:09:43,440
- you know basking ridge new jersey and

00:09:39,680 --> 00:09:50,460
this is the route arm so we have done

00:09:43,440 --> 00:09:55,520
that like no last October and so during

00:09:50,460 --> 00:09:55,520
that time we actually what we did was we

00:09:55,910 --> 00:10:03,390
we we download like 20 Meg file like you

00:10:01,050 --> 00:10:07,320
know in round robin fashion from the

00:10:03,390 --> 00:10:10,110
other these three servers okay and then

00:10:07,320 --> 00:10:13,440
that was continuously done throughout

00:10:10,110 --> 00:10:17,880
the you know the driving okay and that

00:10:13,440 --> 00:10:21,980
we collected you know 720 instance of 20

00:10:17,880 --> 00:10:25,680
mega file download and we analyzed that

00:10:21,980 --> 00:10:28,920
okay so some of the tools we actually

00:10:25,680 --> 00:10:32,810
used from the the hands that we actually

00:10:28,920 --> 00:10:36,750
use the quality pop you know the

00:10:32,810 --> 00:10:40,260
application what what would what the

00:10:36,750 --> 00:10:42,690
tool does is it actually things before

00:10:40,260 --> 00:10:46,890
each download to actually measure the

00:10:42,690 --> 00:10:50,010
kingdom ping propagation delay and it

00:10:46,890 --> 00:10:52,710
does the the download of the the file

00:10:50,010 --> 00:10:55,920
that we want to download and he also

00:10:52,710 --> 00:10:59,130
gives us like you know physical and data

00:10:55,920 --> 00:11:01,960
link layer of statistics as well so we

00:10:59,130 --> 00:11:04,690
can actually correlate the you know the

00:11:01,960 --> 00:11:07,589
the physical layer and data link layer

00:11:04,690 --> 00:11:10,839
no statistics with the other TCP

00:11:07,589 --> 00:11:15,010
performance and that was pretty fun okay

00:11:10,839 --> 00:11:19,360
I not going to go through all the

00:11:15,010 --> 00:11:22,600
details of the HPC 7000 chassis that we

00:11:19,360 --> 00:11:24,810
use that we use our party the open

00:11:22,600 --> 00:11:31,120
system that we use was a new bun to

00:11:24,810 --> 00:11:34,810
14.04 and we actually made sure the CPU

00:11:31,120 --> 00:11:38,050
usage over of the the blades was the

00:11:34,810 --> 00:11:41,440
blades were of no really low so not to

00:11:38,050 --> 00:11:44,020
be the bottom at okay the other thing

00:11:41,440 --> 00:11:47,320
that I would like to just mention is we

00:11:44,020 --> 00:11:50,080
actually tune the gue the de mobile

00:11:47,320 --> 00:11:53,680
phone to actually log on to 700 you know

00:11:50,080 --> 00:11:58,300
spectrum just because we want to you

00:11:53,680 --> 00:12:01,510
know so so before that we Verizon

00:11:58,300 --> 00:12:06,220
actually has 700 and then AWS you know

00:12:01,510 --> 00:12:09,040
radio spectrum but aw spectrums are not

00:12:06,220 --> 00:12:11,490
really you know used outside the city so

00:12:09,040 --> 00:12:13,990
there's no reason to actually you know

00:12:11,490 --> 00:12:16,450
cause like you know hard end up and

00:12:13,990 --> 00:12:18,670
stuff like that so we just tune it to a

00:12:16,450 --> 00:12:25,660
no 700 megahertz and and you know digit

00:12:18,670 --> 00:12:27,370
test okay and as you know the channel

00:12:25,660 --> 00:12:30,580
bandwidth was actually an hour 10

00:12:27,370 --> 00:12:34,570
megahertz and the modulation used in the

00:12:30,580 --> 00:12:39,600
production system is qp8 QPSK and 6 16

00:12:34,570 --> 00:12:42,100
QAM and 64 QAM so the mobile you know

00:12:39,600 --> 00:12:47,350
you know picks like no one of this like

00:12:42,100 --> 00:12:49,750
the modulation monitoring the frame I

00:12:47,350 --> 00:12:51,510
think the block generates and stuff like

00:12:49,750 --> 00:12:55,150
that and then they choose like you know

00:12:51,510 --> 00:12:57,610
you know to add more redundancy or less

00:12:55,150 --> 00:13:00,339
redundancy and go faster and stuff like

00:12:57,610 --> 00:13:01,570
that so the theoretical in a maximum

00:13:00,339 --> 00:13:04,570
throughput that you can actually get

00:13:01,570 --> 00:13:09,640
from this environment is around forty

00:13:04,570 --> 00:13:12,940
five to fifteen innovative bps ok so

00:13:09,640 --> 00:13:15,180
let's look at the the radio condition

00:13:12,940 --> 00:13:18,240
you know statistics

00:13:15,180 --> 00:13:20,310
so I'm showing this just to you know

00:13:18,240 --> 00:13:22,320
give you to the comfort level that no

00:13:20,310 --> 00:13:24,420
even though we did like to know in wrong

00:13:22,320 --> 00:13:28,740
wrong impression of downloading 20 make

00:13:24,420 --> 00:13:31,530
file all the the three conditions are

00:13:28,740 --> 00:13:34,920
great them actually experience like you

00:13:31,530 --> 00:13:37,740
know similar you know the radio

00:13:34,920 --> 00:13:42,360
condition this actually shows the signal

00:13:37,740 --> 00:13:44,880
to interference plus noise ratio and you

00:13:42,360 --> 00:13:49,380
can actually see that you know the SA

00:13:44,880 --> 00:13:53,870
and IRS are no well distributed you know

00:13:49,380 --> 00:13:57,240
from like you know in all spectrum and

00:13:53,870 --> 00:13:59,760
all three conditions are versions are

00:13:57,240 --> 00:14:06,210
actually experiencing similar RF

00:13:59,760 --> 00:14:07,860
condition okay so and if we actually

00:14:06,210 --> 00:14:09,930
look at the of the modulation and the

00:14:07,860 --> 00:14:12,230
rate adaptation I mean this is actually

00:14:09,930 --> 00:14:15,060
directly impacting you know the

00:14:12,230 --> 00:14:19,200
condition algorithm performance um when

00:14:15,060 --> 00:14:21,080
the on the lower s ir there was a lot of

00:14:19,200 --> 00:14:25,500
uh you know the

00:14:21,080 --> 00:14:28,260
QPSK was actually a lot of times QPSK

00:14:25,500 --> 00:14:31,380
was actually selected and then as the

00:14:28,260 --> 00:14:34,560
SAR are getting proved you know the

00:14:31,380 --> 00:14:38,100
handset and negotiated with the you know

00:14:34,560 --> 00:14:42,510
be to actually have where's that the

00:14:38,100 --> 00:14:46,410
tune to the 64 QAM that can potentially

00:14:42,510 --> 00:14:50,340
give you know 50 Mbps that but the these

00:14:46,410 --> 00:14:53,550
numbers 17 25 and 50 those are physical

00:14:50,340 --> 00:14:56,040
layer you know maximum throughput and so

00:14:53,550 --> 00:14:58,110
the TCP layer or the IP layer throughput

00:14:56,040 --> 00:15:00,930
will be actual less than this a little

00:14:58,110 --> 00:15:03,990
less than this okay

00:15:00,930 --> 00:15:09,660
so the two things I really want to

00:15:03,990 --> 00:15:13,560
actually you know you know the you know

00:15:09,660 --> 00:15:15,810
emphasize here is the fact that if the

00:15:13,560 --> 00:15:21,030
modulation actually changes for example

00:15:15,810 --> 00:15:24,630
from 64 QAM to 16 QAM basically suddenly

00:15:21,030 --> 00:15:26,610
the rosette the the transmission rate I

00:15:24,630 --> 00:15:29,050
mean the the bandwidth drops down like

00:15:26,610 --> 00:15:31,540
by half it's not going to meet you

00:15:29,050 --> 00:15:33,459
actually half but depends on the the

00:15:31,540 --> 00:15:36,310
real situations but you can actually

00:15:33,459 --> 00:15:39,399
drop like dramatically so what happens

00:15:36,310 --> 00:15:41,589
is the queuing delay in this case will

00:15:39,399 --> 00:15:44,680
be actually doubled so going and that

00:15:41,589 --> 00:15:46,959
can actually confuse the the the

00:15:44,680 --> 00:15:50,740
bandwidth estimation algorithms and also

00:15:46,959 --> 00:15:52,240
by doing that the you know be active

00:15:50,740 --> 00:15:54,459
queue management so are actually now

00:15:52,240 --> 00:15:56,290
certainly coming in based on their

00:15:54,459 --> 00:15:59,440
configuration and then start to drop the

00:15:56,290 --> 00:16:02,260
packet so and that will actually you

00:15:59,440 --> 00:16:06,180
know cause like no more damage to the to

00:16:02,260 --> 00:16:10,690
TCP you know if you are really doing the

00:16:06,180 --> 00:16:13,329
the a IND the you try to actually adjust

00:16:10,690 --> 00:16:15,399
the the bandwidth your transmission rate

00:16:13,329 --> 00:16:17,709
based on the packet drops on the network

00:16:15,399 --> 00:16:24,130
that can actually confuse the you know

00:16:17,709 --> 00:16:25,930
the TCP arboretum as well okay so it's

00:16:24,130 --> 00:16:30,370
going to be the heart of the this

00:16:25,930 --> 00:16:32,529
project so before we you know go into

00:16:30,370 --> 00:16:36,130
the details of the statistics I mean

00:16:32,529 --> 00:16:43,089
this is just a one no study the case

00:16:36,130 --> 00:16:45,190
study of the the bbr so the left side

00:16:43,089 --> 00:16:48,760
see last night yeah left side is

00:16:45,190 --> 00:16:52,360
actually when the SAR was actually

00:16:48,760 --> 00:16:55,660
greater than 20 mega 20db and the right

00:16:52,360 --> 00:16:57,779
side is actually when the SNR was in

00:16:55,660 --> 00:17:00,910
between 10 and 20 dB

00:16:57,779 --> 00:17:04,419
so the the upper graph actually shows 3d

00:17:00,910 --> 00:17:05,890
you know the outstanding window this you

00:17:04,419 --> 00:17:11,500
can actually generate with the TCP dump

00:17:05,890 --> 00:17:13,750
easily so so and then the the lower

00:17:11,500 --> 00:17:18,429
graph actually shows the RTT reported by

00:17:13,750 --> 00:17:21,040
D the the peak at as you can see when

00:17:18,429 --> 00:17:24,160
there were a you know high bandwidth

00:17:21,040 --> 00:17:26,500
available more bandwidth available you

00:17:24,160 --> 00:17:30,210
can actually see that the you know VBR

00:17:26,500 --> 00:17:34,470
was after after Divya the initial

00:17:30,210 --> 00:17:37,900
bandwidth program period it was able to

00:17:34,470 --> 00:17:41,380
was that the limit the outstanding

00:17:37,900 --> 00:17:42,670
window around 500 I believe this is

00:17:41,380 --> 00:17:49,690
kilobytes

00:17:42,670 --> 00:17:54,850
and that you know maintains the the RTT

00:17:49,690 --> 00:17:58,540
below hundred millisecond in Si are

00:17:54,850 --> 00:18:01,600
being between 10 and 20 DB where the

00:17:58,540 --> 00:18:04,360
lower develop that was actually low from

00:18:01,600 --> 00:18:06,490
the IP perspective it'll actually be

00:18:04,360 --> 00:18:09,130
very small I'll choose is smaller on L

00:18:06,490 --> 00:18:16,450
serine in this case the graph shows

00:18:09,130 --> 00:18:19,660
around 2200 no kilobytes to actually

00:18:16,450 --> 00:18:22,510
also limit the RTT under 100 millisecond

00:18:19,660 --> 00:18:24,850
so this actually you know shows the you

00:18:22,510 --> 00:18:30,370
know how you know bbr is actually

00:18:24,850 --> 00:18:33,690
working and you know that's basically

00:18:30,370 --> 00:18:38,230
what this case study is actually showing

00:18:33,690 --> 00:18:42,430
okay this is actually cubic so as you

00:18:38,230 --> 00:18:44,260
can see the cubic if you don't actually

00:18:42,430 --> 00:18:47,610
drop the cubic packet I mean the packet

00:18:44,260 --> 00:18:50,920
it will actually the trace was at the

00:18:47,610 --> 00:18:53,290
wind the outstanding window will

00:18:50,920 --> 00:18:55,360
actually grow up to the advertised like

00:18:53,290 --> 00:18:57,940
in the AR way and that actually kills

00:18:55,360 --> 00:18:59,860
the performance from in the wallace

00:18:57,940 --> 00:19:03,220
because you know in the left graph if

00:18:59,860 --> 00:19:05,410
you actually look at you know look at

00:19:03,220 --> 00:19:11,620
like compared the RTT graph and then oh

00:19:05,410 --> 00:19:14,760
in around three second the RTT is

00:19:11,620 --> 00:19:17,500
actually growing up and up and up and

00:19:14,760 --> 00:19:20,920
that means you don't really need to

00:19:17,500 --> 00:19:23,620
actually increase more you know the the

00:19:20,920 --> 00:19:28,800
whole wind you can actually know you are

00:19:23,620 --> 00:19:31,300
actually know injecting the RTT

00:19:28,800 --> 00:19:34,060
uselessly and that's what it shows and

00:19:31,300 --> 00:19:36,520
the the right side actually you know

00:19:34,060 --> 00:19:39,100
sometimes i don't know the reason why

00:19:36,520 --> 00:19:41,620
the packet get dropped but if there's a

00:19:39,100 --> 00:19:44,290
packet get dropped and the window

00:19:41,620 --> 00:19:45,670
adjustment actually not happen the the

00:19:44,290 --> 00:19:47,260
cut in half

00:19:45,670 --> 00:19:50,920
actually you know it takes place and

00:19:47,260 --> 00:19:52,630
that sometimes the RTT is is actually

00:19:50,920 --> 00:19:57,940
managed under 100

00:19:52,630 --> 00:20:00,070
seconds as you can see I don't know from

00:19:57,940 --> 00:20:02,800
the throughput perspective I don't know

00:20:00,070 --> 00:20:05,470
which one is better but so that actually

00:20:02,800 --> 00:20:07,810
you know gives you the idea of how cubic

00:20:05,470 --> 00:20:13,780
and no BB are actually works India on

00:20:07,810 --> 00:20:17,320
the wireless environment okay okay so so

00:20:13,780 --> 00:20:21,250
this graph actually shows the the ping

00:20:17,320 --> 00:20:23,380
RTT and TCP initial rdt the graphs of

00:20:21,250 --> 00:20:27,630
the other seven seven hundred twenty

00:20:23,380 --> 00:20:32,260
instances that we have to both the RTT

00:20:27,630 --> 00:20:35,170
of the King and TCP initial activity

00:20:32,260 --> 00:20:38,500
this is actually sing to synagogue no

00:20:35,170 --> 00:20:40,390
round-trip time are both within forty

00:20:38,500 --> 00:20:41,950
milliseconds to a hundred millisecond

00:20:40,390 --> 00:20:44,530
however the juice distribution is

00:20:41,950 --> 00:20:46,570
actually different we couldn't actually

00:20:44,530 --> 00:20:51,670
figure out why it's actually different

00:20:46,570 --> 00:20:55,750
but we were just speculating that maybe

00:20:51,670 --> 00:20:57,430
the the eat the ICMP packet was actually

00:20:55,750 --> 00:21:00,460
following you know different route than

00:20:57,430 --> 00:21:02,860
the the TCP packets the actual data

00:21:00,460 --> 00:21:07,680
packets in some of the node and that may

00:21:02,860 --> 00:21:07,680
actually cause this you know differences

00:21:09,150 --> 00:21:17,140
okay but but the the thing is like you

00:21:13,510 --> 00:21:19,870
know the RTT was actually as you can see

00:21:17,140 --> 00:21:22,090
just want to show that the initial TT

00:21:19,870 --> 00:21:24,010
measurement and if you PPR is actually

00:21:22,090 --> 00:21:26,800
doing the the minima or TT measurement

00:21:24,010 --> 00:21:31,600
you should be close to 40 in most of the

00:21:26,800 --> 00:21:38,740
case okay so this graph actually shows

00:21:31,600 --> 00:21:40,930
you the throughput of 720 instances so

00:21:38,740 --> 00:21:44,470
the the left graph actually shows the

00:21:40,930 --> 00:21:47,550
CDF cumulative density function arm over

00:21:44,470 --> 00:21:52,030
throughput and as you can see in the

00:21:47,550 --> 00:21:56,110
it's shows like you know that the the

00:21:52,030 --> 00:21:59,410
bbr was actually doing better on the the

00:21:56,110 --> 00:22:05,020
high through preside meaning it's

00:21:59,410 --> 00:22:05,830
actually so the the red line below no on

00:22:05,020 --> 00:22:08,620
the right side

00:22:05,830 --> 00:22:11,769
the blue and green line means that it

00:22:08,620 --> 00:22:13,450
was able to more instance were actually

00:22:11,769 --> 00:22:18,340
able to achieve like a high Earth orbit

00:22:13,450 --> 00:22:21,000
and and the right graph is that another

00:22:18,340 --> 00:22:23,380
representation of the same data

00:22:21,000 --> 00:22:27,220
basically we actually look at the

00:22:23,380 --> 00:22:30,690
histograms of like you know in 0 to 5 DB

00:22:27,220 --> 00:22:33,519
and no and and so on

00:22:30,690 --> 00:22:37,360
we actually shows the the mean and then

00:22:33,519 --> 00:22:43,750
no one standard deviation and just swim

00:22:37,360 --> 00:22:46,570
and actually mix Maxon mean throughput

00:22:43,750 --> 00:22:50,769
of the instances and as you can see our

00:22:46,570 --> 00:22:53,740
bbr was pretty much compatible with the

00:22:50,769 --> 00:22:56,260
the cubic to boost both versions of

00:22:53,740 --> 00:23:00,460
cubic and then it actually did dump you

00:22:56,260 --> 00:23:04,659
know pretty well on the when the SAR was

00:23:00,460 --> 00:23:06,580
actually 20 to 25 and I was actually

00:23:04,659 --> 00:23:09,880
interested that why would you would

00:23:06,580 --> 00:23:13,210
actually do that but you know if you

00:23:09,880 --> 00:23:18,070
actually remember this graph it seems

00:23:13,210 --> 00:23:19,450
like you know in 20 to 25 the the

00:23:18,070 --> 00:23:21,610
throughput was actually pretty good and

00:23:19,450 --> 00:23:23,590
then suddenly drop downs and then goes

00:23:21,610 --> 00:23:25,870
up and stuff like that right that

00:23:23,590 --> 00:23:30,279
actually probably wasn't happening more

00:23:25,870 --> 00:23:32,110
and the decay to cubic was not able to

00:23:30,279 --> 00:23:34,690
actually you know adapt to those like no

00:23:32,110 --> 00:23:43,870
weight changes and that would be the one

00:23:34,690 --> 00:23:47,679
of the reasons so okay um and then we

00:23:43,870 --> 00:23:51,570
actually look at the the hand over as

00:23:47,679 --> 00:23:53,889
well so everybody I mean not let's say

00:23:51,570 --> 00:23:56,799
many of us actually think that if you

00:23:53,889 --> 00:23:58,389
actually run the highway all my TCP will

00:23:56,799 --> 00:23:59,740
actually go through multiple hops and

00:23:58,389 --> 00:24:02,620
then there will be a lot of like you

00:23:59,740 --> 00:24:05,230
know handovers but we actually found

00:24:02,620 --> 00:24:09,130
that in our highway driving that wasn't

00:24:05,230 --> 00:24:10,570
the case that's you know the the left

00:24:09,130 --> 00:24:14,110
graph actually shows the other

00:24:10,570 --> 00:24:17,799
complimentary the CDF and it actually

00:24:14,110 --> 00:24:19,210
shows that around 65% of our you know

00:24:17,799 --> 00:24:23,220
TCP flows of

00:24:19,210 --> 00:24:27,130
pneumatic download actually only

00:24:23,220 --> 00:24:29,200
experience like no no I mean no hand

00:24:27,130 --> 00:24:30,220
over so no hand over what means like it

00:24:29,200 --> 00:24:35,799
didn't actually I do

00:24:30,220 --> 00:24:37,510
and or never happen and we actually die

00:24:35,799 --> 00:24:39,730
stay out the throughput you know

00:24:37,510 --> 00:24:43,299
statistics to the the number of

00:24:39,730 --> 00:24:45,640
handovers and obviously it actually

00:24:43,299 --> 00:24:47,289
shows that the throughput of all three

00:24:45,640 --> 00:24:50,279
of the congestion algorithm actually

00:24:47,289 --> 00:24:54,520
goes down as number of handover

00:24:50,279 --> 00:24:57,820
increases but we didn't actually see no

00:24:54,520 --> 00:25:01,299
more than two I mean three handovers

00:24:57,820 --> 00:25:06,909
actually in our test so that's probably

00:25:01,299 --> 00:25:09,100
meaning even 20 Meg download is doesn't

00:25:06,909 --> 00:25:12,159
really take long time probably four or

00:25:09,100 --> 00:25:14,529
five seconds sometimes if you actually

00:25:12,159 --> 00:25:17,740
drive through the edge of the cells then

00:25:14,529 --> 00:25:20,289
you probably see you know more handovers

00:25:17,740 --> 00:25:23,289
and also you will probably see like in a

00:25:20,289 --> 00:25:25,899
weak signals as well and so combined the

00:25:23,289 --> 00:25:32,260
effect you're probably going to have in

00:25:25,899 --> 00:25:35,200
a lowered through pit okay this graph I

00:25:32,260 --> 00:25:37,840
love this graph on this graph actually

00:25:35,200 --> 00:25:42,690
shows the the RT Landry transmission of

00:25:37,840 --> 00:25:46,960
the you know all three algorithms

00:25:42,690 --> 00:25:51,340
since EBR was actually maintaining the

00:25:46,960 --> 00:25:53,830
low RTT the RTO was actually no very

00:25:51,340 --> 00:25:57,490
impressive here because even the argue

00:25:53,830 --> 00:26:00,070
actually happens the the interval was

00:25:57,490 --> 00:26:02,740
actually pretty short and that is one of

00:26:00,070 --> 00:26:06,700
the the throughput improving factor I

00:26:02,740 --> 00:26:08,770
believe in TCP so archaea was actually

00:26:06,700 --> 00:26:11,919
know really good and if you look at the

00:26:08,770 --> 00:26:16,270
retransmission right um it was actually

00:26:11,919 --> 00:26:19,539
know pretty impressive so the percentage

00:26:16,270 --> 00:26:24,929
was actually you know much lower in most

00:26:19,539 --> 00:26:29,620
of the cases then those two the cubic

00:26:24,929 --> 00:26:32,920
cases okay so research low

00:26:29,620 --> 00:26:34,750
retransmission rate and compatible

00:26:32,920 --> 00:26:37,890
throughput actually gives a better group

00:26:34,750 --> 00:26:47,670
it for bbr and that's actually you know

00:26:37,890 --> 00:26:51,700
pretty encouraging factor for us okay so

00:26:47,670 --> 00:26:55,990
we look at the the RT 10 throughput in a

00:26:51,700 --> 00:26:59,920
slightly different way so in the the

00:26:55,990 --> 00:27:02,890
right right side graph the x axis is

00:26:59,920 --> 00:27:06,400
self-inflicted RT t and the y axis is

00:27:02,890 --> 00:27:09,160
throughput and the red dots are VB are

00:27:06,400 --> 00:27:13,840
instances and green and blue dots are

00:27:09,160 --> 00:27:20,910
cubic instances as you can see in all

00:27:13,840 --> 00:27:26,230
range of throughput the VBR was the

00:27:20,910 --> 00:27:30,880
controlling the the the RTT under around

00:27:26,230 --> 00:27:32,830
100 millisecond range and the right

00:27:30,880 --> 00:27:35,370
graph actually shows the other cdf of

00:27:32,830 --> 00:27:35,370
the same data

00:27:40,850 --> 00:27:47,910
okay in summary we just plotted the

00:27:43,620 --> 00:27:51,539
average RTT self-inflected RTP versus

00:27:47,910 --> 00:27:55,559
average throughput so you know anything

00:27:51,539 --> 00:27:58,280
goes the the left side is actually good

00:27:55,559 --> 00:28:01,799
anything goes high is actually good so

00:27:58,280 --> 00:28:11,070
actually bbr was was the best you know

00:28:01,799 --> 00:28:15,030
out of three okay all right so what I

00:28:11,070 --> 00:28:16,380
actually you know this is just a just a

00:28:15,030 --> 00:28:18,750
little discussion that I was like that

00:28:16,380 --> 00:28:21,390
so when we actually look at the the

00:28:18,750 --> 00:28:25,650
congestion control algorithm in for the

00:28:21,390 --> 00:28:28,799
wireless network the same was the Nov

00:28:25,650 --> 00:28:30,929
our the mostly bubble net I don't know

00:28:28,799 --> 00:28:34,049
how is this is going to be changing 5g

00:28:30,929 --> 00:28:37,919
but in at least for 4G to either be for

00:28:34,049 --> 00:28:39,960
exit bottleneck and actually the buffer

00:28:37,919 --> 00:28:43,130
blood was the main reason for TCP

00:28:39,960 --> 00:28:47,070
performance degradation so reducing the

00:28:43,130 --> 00:28:48,870
maximum re on ue to avoid the buffer

00:28:47,070 --> 00:28:50,789
blood is not a practical solution I mean

00:28:48,870 --> 00:28:52,890
some I saw some vendors are actually

00:28:50,789 --> 00:28:56,789
proposing this I don't think this is

00:28:52,890 --> 00:29:01,020
actually a practical solution and large

00:28:56,789 --> 00:29:06,419
buffer inside Adobe is a double-edged

00:29:01,020 --> 00:29:08,720
sword for low for performance so the

00:29:06,419 --> 00:29:11,520
interesting thing is about the fairness

00:29:08,720 --> 00:29:16,309
fairness is not really important for us

00:29:11,520 --> 00:29:18,630
because anyway we have layer to fairness

00:29:16,309 --> 00:29:24,270
algorithms are actually implemented in

00:29:18,630 --> 00:29:27,120
LTE so you know it at the best they

00:29:24,270 --> 00:29:35,070
won't be actually no fighting over with

00:29:27,120 --> 00:29:38,850
other other you ease okay so that's

00:29:35,070 --> 00:29:40,860
about it this is just conclusion so

00:29:38,850 --> 00:29:44,159
closely this was a closed layer

00:29:40,860 --> 00:29:47,070
comprehensive measurement study cubic hi

00:29:44,159 --> 00:29:49,530
start may not perform well on LTE EBR

00:29:47,070 --> 00:29:51,600
did the best we want to actually look a

00:29:49,530 --> 00:29:53,520
little bit more on the BB r to actually

00:29:51,600 --> 00:29:54,000
tune the parameters to see if it can

00:29:53,520 --> 00:29:57,420
actually

00:29:54,000 --> 00:30:04,230
achieve the goals that we have okay

00:29:57,420 --> 00:30:05,430
thank you very much questions and for

00:30:04,230 --> 00:30:07,460
sure I can probably only take one

00:30:05,430 --> 00:30:10,800
question that's it

00:30:07,460 --> 00:30:12,980
we ran out we don't have time but

00:30:10,800 --> 00:30:16,590
there'll be five minutes penalty time

00:30:12,980 --> 00:30:18,030
first that are pretty impressive results

00:30:16,590 --> 00:30:22,710
thank you very much

00:30:18,030 --> 00:30:25,020
I was or I'm basically asking why we did

00:30:22,710 --> 00:30:26,880
the scenes in a car seat is the mission

00:30:25,020 --> 00:30:32,880
because the condition control shouldn't

00:30:26,880 --> 00:30:35,550
affect this measure so so I mean well I

00:30:32,880 --> 00:30:38,690
know VBR actually uses the the metric

00:30:35,550 --> 00:30:41,070
try to measure the minimum RTT right so

00:30:38,690 --> 00:30:43,200
we could actually have done that but you

00:30:41,070 --> 00:30:45,750
know we just say okay let's just look at

00:30:43,200 --> 00:30:48,000
the Cintas University as a measure of

00:30:45,750 --> 00:30:51,480
our GT without the self-inflicted in art

00:30:48,000 --> 00:30:54,060
you know the delay so I mean that's just

00:30:51,480 --> 00:30:55,980
an optimization in terms of the the

00:30:54,060 --> 00:31:00,470
delay measurement to actually show the

00:30:55,980 --> 00:31:00,470

YouTube URL: https://www.youtube.com/watch?v=-ioi3wtl1Gc


