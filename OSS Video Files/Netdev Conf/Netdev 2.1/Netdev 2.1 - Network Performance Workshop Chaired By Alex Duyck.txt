Title: Netdev 2.1 - Network Performance Workshop Chaired By Alex Duyck
Publication date: 2017-04-27
Playlist: Netdev 2.1
Description: 
	Took place on Apr 06/2017 at Netdev 2.1 in Montreal.
The network performance workshop is a follow-up of several mailing list threads on the netdev mailing list as well as presentations at Netdev 1.2. Alex leads a  discussion on the
progress made on various efforts to enable better performance through enabling new features in the kernel, and changing existing APIs to better enable the drivers to handle the ever increasing workloads.
Slides at:  https://www.netdevconf.org/2.1/session.html?duyck
Captions: 
	00:00:00,030 --> 00:00:05,609
so ladies and gentlemen thank you for

00:00:02,100 --> 00:00:07,440
coming to the conference this is the

00:00:05,609 --> 00:00:09,750
network performance workshop and my name

00:00:07,440 --> 00:00:14,280
is Alexander Dyck I'll be acting as the

00:00:09,750 --> 00:00:15,839
chair for this present er it's not

00:00:14,280 --> 00:00:18,390
further ado I'm also changing the names

00:00:15,839 --> 00:00:21,539
for this workshop to some extent to the

00:00:18,390 --> 00:00:23,970
network performance des match at this

00:00:21,539 --> 00:00:27,359
end of the stage we have myself and John

00:00:23,970 --> 00:00:29,789
casa been representing Intel we have a

00:00:27,359 --> 00:00:34,200
yes for Dan garde Brower representing

00:00:29,789 --> 00:00:36,000
what that gesture as a sore yeah some

00:00:34,200 --> 00:00:38,010
semi mutual party although he tends to

00:00:36,000 --> 00:00:44,719
favor you know the other side a little

00:00:38,010 --> 00:00:49,700
bit and we have a say Mohammed uh

00:00:44,719 --> 00:00:52,350
actually I'll skip tarik to Khan and

00:00:49,700 --> 00:00:54,840
Anil Ansel will be representing the

00:00:52,350 --> 00:00:57,270
Mellanox side of things for the Intel

00:00:54,840 --> 00:01:00,239
stuff will be presenting page based

00:00:57,270 --> 00:01:02,820
received pass with page we use Gmail API

00:01:00,239 --> 00:01:06,659
to enable better use of build skb and

00:01:02,820 --> 00:01:10,409
xcp zero copy using AF packet to

00:01:06,659 --> 00:01:12,689
accelerate user space networking Red Hat

00:01:10,409 --> 00:01:13,830
will be presenting our yes per on behalf

00:01:12,689 --> 00:01:17,520
of Red Hat we'll be presenting the

00:01:13,830 --> 00:01:19,830
kernel memory optimizations and Amir

00:01:17,520 --> 00:01:23,310
Syed and Tariq will be resenting our

00:01:19,830 --> 00:01:25,920
extrema and bulking TX bolting bulking

00:01:23,310 --> 00:01:28,610
and multi packet TX descriptors multi

00:01:25,920 --> 00:01:31,320
packet single TX descriptor transmitted

00:01:28,610 --> 00:01:33,240
and before this gets into an outright

00:01:31,320 --> 00:01:35,490
deathmatch you know deathmatch is a

00:01:33,240 --> 00:01:37,770
little bit of an exaggeration to some

00:01:35,490 --> 00:01:42,899
extent with Md community we don't so

00:01:37,770 --> 00:01:45,329
much compete as a month it's cool as it

00:01:42,899 --> 00:01:47,130
come well not quite competition but

00:01:45,329 --> 00:01:47,670
 petition that's the word I'm

00:01:47,130 --> 00:01:49,560
looking for

00:01:47,670 --> 00:01:52,439
so to some extent what it is is the

00:01:49,560 --> 00:01:54,720
rising tide raises all ships when we

00:01:52,439 --> 00:01:56,490
implement something in our driver we're

00:01:54,720 --> 00:01:58,500
okay we implement this new memory

00:01:56,490 --> 00:02:00,719
barrier look we get all this extra

00:01:58,500 --> 00:02:02,430
performance odds are six months later

00:02:00,719 --> 00:02:04,079
Mellanox will be implementing the same

00:02:02,430 --> 00:02:05,759
barrier and they're set up a good

00:02:04,079 --> 00:02:07,649
analogy for this is for those that are

00:02:05,759 --> 00:02:09,239
familiar with the high jump back in the

00:02:07,649 --> 00:02:11,630
day you know you would be doing this you

00:02:09,239 --> 00:02:14,150
know just a vertical jump over the high

00:02:11,630 --> 00:02:17,150
our and then at some point somebody came

00:02:14,150 --> 00:02:18,890
up with the Fosbury flop you know it's

00:02:17,150 --> 00:02:21,110
this weird funky looking technique for

00:02:18,890 --> 00:02:22,370
doing it but suddenly you can get next

00:02:21,110 --> 00:02:24,820
to a couple of feet on your high jump

00:02:22,370 --> 00:02:27,350
and lo and behold a couple years later

00:02:24,820 --> 00:02:28,820
everybody's doing it and so I've kind of

00:02:27,350 --> 00:02:30,470
the goal of this session is you know

00:02:28,820 --> 00:02:31,970
this is the stuff that we're doing that

00:02:30,470 --> 00:02:40,810
makes it look really good this is how

00:02:31,970 --> 00:02:43,430
you do it okay yes it goes both ways

00:02:40,810 --> 00:02:46,400
admittedly to some extent I've done the

00:02:43,430 --> 00:02:47,870
same thing with DP D K so you see how

00:02:46,400 --> 00:02:50,840
somebody does something like oh it's a

00:02:47,870 --> 00:02:52,310
good idea I think I'll borrow it so as

00:02:50,840 --> 00:02:55,910
long as it's not licensed

00:02:52,310 --> 00:02:58,250
inappropriately anyway so that being

00:02:55,910 --> 00:03:00,140
said I'll go ahead and just cut into my

00:02:58,250 --> 00:03:03,920
bit here and this is the part where we a

00:03:00,140 --> 00:03:05,720
little bit less exciting so basic setup

00:03:03,920 --> 00:03:08,990
for implementing page base receives with

00:03:05,720 --> 00:03:10,640
page reads so I thought I'd bring this

00:03:08,990 --> 00:03:12,830
up just because you know it's come up on

00:03:10,640 --> 00:03:16,420
the list a few times so a basic received

00:03:12,830 --> 00:03:18,770
path you allocate a usually it's a page

00:03:16,420 --> 00:03:20,600
it used to be an S KB for a lot of

00:03:18,770 --> 00:03:24,440
people but SK B's include extra overhead

00:03:20,600 --> 00:03:27,050
so now just allocate the page we DMA map

00:03:24,440 --> 00:03:28,790
said page assign the page to a device

00:03:27,050 --> 00:03:31,430
usually in the using a mechanism such as

00:03:28,790 --> 00:03:33,470
a descriptor like a receive descriptor

00:03:31,430 --> 00:03:34,760
and then when you know the device

00:03:33,470 --> 00:03:37,240
performs a DMA

00:03:34,760 --> 00:03:40,220
notifies us somehow we unmask the page

00:03:37,240 --> 00:03:42,830
assign the page using other build fkb or

00:03:40,220 --> 00:03:44,990
sk b ad RX frags to get it in there

00:03:42,830 --> 00:03:46,730
preferably build skb because the ad rx

00:03:44,990 --> 00:03:48,950
frag leads to a bunch of mem copy stuff

00:03:46,730 --> 00:03:51,230
and you return to step one to the next

00:03:48,950 --> 00:03:53,480
descriptor and so on and so forth pretty

00:03:51,230 --> 00:03:55,790
simple setup except for it doesn't

00:03:53,480 --> 00:03:59,390
perform very well because step one

00:03:55,790 --> 00:04:00,950
allocates the page well as yes we can

00:03:59,390 --> 00:04:04,670
attest that's expensive

00:04:00,950 --> 00:04:06,050
step 2 that's the page well okay it

00:04:04,670 --> 00:04:08,630
depends on what you're running if it's

00:04:06,050 --> 00:04:11,330
an iommu it can be really expensive

00:04:08,630 --> 00:04:13,250
and then you know thanks device okay

00:04:11,330 --> 00:04:15,680
that's cheap then unmapped the page oh

00:04:13,250 --> 00:04:17,989
yeah I Oh mmm you that's expensive too

00:04:15,680 --> 00:04:19,850
so by the time you're done you've taken

00:04:17,989 --> 00:04:21,470
all these expensive operations to get to

00:04:19,850 --> 00:04:24,320
receive buffer in and of course your

00:04:21,470 --> 00:04:25,250
performance hurts so what do you do you

00:04:24,320 --> 00:04:28,490
do it with page

00:04:25,250 --> 00:04:30,140
which some of the older implementations

00:04:28,490 --> 00:04:32,030
this is actually kind of a little bit of

00:04:30,140 --> 00:04:34,010
a mix here so I'll explain some of it so

00:04:32,030 --> 00:04:35,660
steps 1 2 & 3 are essentially the same

00:04:34,010 --> 00:04:38,750
you still have to map it allocate a page

00:04:35,660 --> 00:04:40,670
map it and assign it to the device after

00:04:38,750 --> 00:04:42,770
you're threatened back though you can

00:04:40,670 --> 00:04:46,250
start thinking about your options in the

00:04:42,770 --> 00:04:50,030
case of most network traffic standard

00:04:46,250 --> 00:04:51,620
Ethernet uses an MTU of 1500 1500 is not

00:04:50,030 --> 00:04:53,600
all that big when compared to a fork a

00:04:51,620 --> 00:04:55,150
page it's actually quite a bit less than

00:04:53,600 --> 00:04:57,170
half you actually have enough room there

00:04:55,150 --> 00:04:58,730
that you can get away with doing things

00:04:57,170 --> 00:05:01,820
like what we do in the Intel drivers

00:04:58,730 --> 00:05:04,010
where we just used half the page and

00:05:01,820 --> 00:05:05,810
then we reuse the other half and so with

00:05:04,010 --> 00:05:07,040
that logic in mind we only need to sync

00:05:05,810 --> 00:05:08,690
the half the page that we're actually

00:05:07,040 --> 00:05:09,890
going to use and what we can do

00:05:08,690 --> 00:05:13,970
something with the else with the other

00:05:09,890 --> 00:05:15,500
half so we take that other half and well

00:05:13,970 --> 00:05:18,530
retake that half that's been dm8 into

00:05:15,500 --> 00:05:19,970
you and we have to at call skb a direct

00:05:18,530 --> 00:05:23,810
rag to go ahead and put in the page but

00:05:19,970 --> 00:05:28,400
the problem is with existing DMA API as

00:05:23,810 --> 00:05:30,979
of I think it's wet 4.1 say 4.9 410 I

00:05:28,400 --> 00:05:34,040
can't remember the exact number now but

00:05:30,979 --> 00:05:36,260
what's the legacy DMA API you couldn't

00:05:34,040 --> 00:05:38,600
actually write anything into that page

00:05:36,260 --> 00:05:41,150
because there were some cases such as

00:05:38,600 --> 00:05:43,100
software i/o TLB where you were actually

00:05:41,150 --> 00:05:45,830
using that it would invalidate the

00:05:43,100 --> 00:05:47,390
entire region on a none map so if you

00:05:45,830 --> 00:05:48,200
wrote anything in there like new header

00:05:47,390 --> 00:05:51,110
or something like that

00:05:48,200 --> 00:05:53,030
unmapped it before the the TX there

00:05:51,110 --> 00:05:55,010
before whatever was supposed to read the

00:05:53,030 --> 00:05:56,780
packet read it you could have just

00:05:55,010 --> 00:05:57,950
corrupted data you have packets going

00:05:56,780 --> 00:06:00,680
around the network that don't make sense

00:05:57,950 --> 00:06:02,120
and so for that reason it had to be left

00:06:00,680 --> 00:06:04,430
read-only so you'd end up having coffee

00:06:02,120 --> 00:06:05,450
Hedy it copied the headers out if you

00:06:04,430 --> 00:06:08,090
wanted to do something like routing

00:06:05,450 --> 00:06:10,790
which copying is expensive but on the

00:06:08,090 --> 00:06:12,740
night side of it instead of having to

00:06:10,790 --> 00:06:14,870
allocate another page you end up

00:06:12,740 --> 00:06:16,460
skipping the step 6a here which is okay

00:06:14,870 --> 00:06:18,080
don't allocate another page just

00:06:16,460 --> 00:06:20,479
increment the page count that's all you

00:06:18,080 --> 00:06:22,190
need to do and then we end up having to

00:06:20,479 --> 00:06:23,330
add a couple steps where step seven go

00:06:22,190 --> 00:06:24,710
ahead and think the other half page of

00:06:23,330 --> 00:06:28,490
the page back for the device and just

00:06:24,710 --> 00:06:31,330
give it back and as long as the whatever

00:06:28,490 --> 00:06:33,890
you're giving these SK bus to is

00:06:31,330 --> 00:06:36,169
referring them fast enough you don't

00:06:33,890 --> 00:06:37,880
have to do any more allocations any more

00:06:36,169 --> 00:06:40,250
mappings any unmapped

00:06:37,880 --> 00:06:42,350
all you end up doing is get Paige and

00:06:40,250 --> 00:06:45,440
sink just over and over and over again

00:06:42,350 --> 00:06:46,610
if it's not doing it fast enough there

00:06:45,440 --> 00:06:47,630
is a bit of a Tennessee there and that

00:06:46,610 --> 00:06:51,380
you have to fall back to the legacy

00:06:47,630 --> 00:06:52,340
approach so it's step 8 if it's not

00:06:51,380 --> 00:06:54,530
returning it fast enough you have to

00:06:52,340 --> 00:06:56,240
return to step one otherwise you just

00:06:54,530 --> 00:07:00,320
get to go to step three and so it ends

00:06:56,240 --> 00:07:04,750
up being a net win there so like I was

00:07:00,320 --> 00:07:07,730
saying legacy DMA API didn't support

00:07:04,750 --> 00:07:08,270
teammate attribute get yeah DMA

00:07:07,730 --> 00:07:11,810
attribute

00:07:08,270 --> 00:07:14,630
skip CPU sink on architectures other

00:07:11,810 --> 00:07:16,040
than I think it was I think there was an

00:07:14,630 --> 00:07:17,930
ARM architecture that has actually

00:07:16,040 --> 00:07:19,520
implemented this because for them

00:07:17,930 --> 00:07:20,960
mapping and unmasking was just way too

00:07:19,520 --> 00:07:23,060
expensive and so they were already doing

00:07:20,960 --> 00:07:24,380
sync but they didn't need the map and

00:07:23,060 --> 00:07:26,390
unmapped to do any of that they just

00:07:24,380 --> 00:07:27,560
take care of it themselves because they

00:07:26,390 --> 00:07:28,670
only had wanted to sync part of the

00:07:27,560 --> 00:07:30,250
buffer they never wanted the full thing

00:07:28,670 --> 00:07:32,510
and I looked at that it's an opportunity

00:07:30,250 --> 00:07:34,550
so I basically took the attribute I

00:07:32,510 --> 00:07:38,180
spread it out over all the architectures

00:07:34,550 --> 00:07:39,860
anything that required a caring to get

00:07:38,180 --> 00:07:44,810
in solid ated or overwrote memory as a

00:07:39,860 --> 00:07:46,990
result of DMA unmapped would end up with

00:07:44,810 --> 00:07:50,540
this not doing that so you could call

00:07:46,990 --> 00:07:54,230
the DMA on map page pass DMA attributes

00:07:50,540 --> 00:07:55,460
get CPU sync and it won't invalidate the

00:07:54,230 --> 00:07:57,980
memory so now it suddenly becomes

00:07:55,460 --> 00:07:59,000
possible for us to reuse pages and we

00:07:57,980 --> 00:08:01,430
don't have to do the mem copy because

00:07:59,000 --> 00:08:02,870
the pages are now writable because I'm

00:08:01,430 --> 00:08:05,360
mapping the page later isn't going to

00:08:02,870 --> 00:08:06,860
invalidate it

00:08:05,360 --> 00:08:09,320
the only real changes need to the driver

00:08:06,860 --> 00:08:10,760
are to add this attribute and then in

00:08:09,320 --> 00:08:13,040
addition you have to make sure you

00:08:10,760 --> 00:08:14,300
perform the synchronization yourself so

00:08:13,040 --> 00:08:16,880
basically when you pass this attribute

00:08:14,300 --> 00:08:18,170
you're telling the dma engine I'm smart

00:08:16,880 --> 00:08:20,840
enough to handle the synchronization

00:08:18,170 --> 00:08:23,000
myself you don't need to help me I'll go

00:08:20,840 --> 00:08:24,800
ahead and do the things for what I need

00:08:23,000 --> 00:08:27,590
and only what I need you don't have to

00:08:24,800 --> 00:08:30,140
do the whole page I've already got

00:08:27,590 --> 00:08:32,780
implemented code in igb and IX EBE and I

00:08:30,140 --> 00:08:37,310
believe that's in J's tree as of now I

00:08:32,780 --> 00:08:40,969
have code in Jeff cursors next q4i for

00:08:37,310 --> 00:08:43,490
TE and i40 ebf that'll probably be in

00:08:40,969 --> 00:08:45,110
Dave's tree most likely in a couple

00:08:43,490 --> 00:08:48,950
months I don't know it depends on what

00:08:45,110 --> 00:08:51,250
the timing of it all does and that's

00:08:48,950 --> 00:08:52,959
most of that

00:08:51,250 --> 00:08:56,170
actually I'm going to cut back one thing

00:08:52,959 --> 00:08:58,930
so I kind of skipped over step 7 the one

00:08:56,170 --> 00:09:00,490
other change I made to the kernel that

00:08:58,930 --> 00:09:02,740
occurs to me I didn't bring up in this

00:09:00,490 --> 00:09:06,129
so one of the downsides of page reuse is

00:09:02,740 --> 00:09:09,790
get page itself has a cost mind you is

00:09:06,129 --> 00:09:11,889
not as bad as the full page allocation

00:09:09,790 --> 00:09:14,110
was but it does add up because it's an

00:09:11,889 --> 00:09:17,170
atomic operation ends up being something

00:09:14,110 --> 00:09:18,850
like I believe is it's like 10 in a

00:09:17,170 --> 00:09:20,259
second something like that yeah

00:09:18,850 --> 00:09:23,199
something ugly district because it

00:09:20,259 --> 00:09:26,170
recycles yeah because it's an atomic

00:09:23,199 --> 00:09:28,089
operation and on x86 atomic operations

00:09:26,170 --> 00:09:29,680
caused pipeline stalls and such just

00:09:28,089 --> 00:09:34,060
because it has a cache lush cache line

00:09:29,680 --> 00:09:36,189
and all that art yeah anyway so one of

00:09:34,060 --> 00:09:38,230
the things I added was this page frag

00:09:36,189 --> 00:09:40,750
cache strain which it's kind of a

00:09:38,230 --> 00:09:42,730
mouthful to say that's due to some

00:09:40,750 --> 00:09:44,110
renaming stuff that came from renaming

00:09:42,730 --> 00:09:47,069
requests that came from the memory

00:09:44,110 --> 00:09:50,620
management guys basically the concept is

00:09:47,069 --> 00:09:52,360
a page frag cache is basically a page

00:09:50,620 --> 00:09:54,610
that can have be broken up into chunks

00:09:52,360 --> 00:09:57,189
and we're tracking it via the page count

00:09:54,610 --> 00:09:59,769
so to drain it means we're wiping out

00:09:57,189 --> 00:10:01,420
all the references we hold so one of the

00:09:59,769 --> 00:10:03,130
changes you'll find them like the IG b

00:10:01,420 --> 00:10:06,399
and IX GBE drivers is we're maintaining

00:10:03,130 --> 00:10:08,920
a per in the driver itself we maintain

00:10:06,399 --> 00:10:10,899
an extra count per page and we do this

00:10:08,920 --> 00:10:13,660
bulk update of account so I'll do a page

00:10:10,899 --> 00:10:15,160
rest ad and it's usually sixty five five

00:10:13,660 --> 00:10:17,920
thirty five just to be like

00:10:15,160 --> 00:10:19,149
safe in case there's some you know I

00:10:17,920 --> 00:10:22,569
think I was storing it in the short so

00:10:19,149 --> 00:10:24,279
that's the most I can add so by doing

00:10:22,569 --> 00:10:28,899
that I only have to take that get page

00:10:24,279 --> 00:10:30,910
hit once every 64 K worth of buffers

00:10:28,899 --> 00:10:32,589
received and so that basically gives us

00:10:30,910 --> 00:10:34,180
another gain of several cycles it's a

00:10:32,589 --> 00:10:36,279
micro optimization but something I

00:10:34,180 --> 00:10:38,649
wanted to point out and so a page frags

00:10:36,279 --> 00:10:40,870
cache train is how you free a page that

00:10:38,649 --> 00:10:46,000
has multiple references against it in

00:10:40,870 --> 00:10:48,370
one shot and the last thing I wanted to

00:10:46,000 --> 00:10:49,660
say is I just wanted to make sure

00:10:48,370 --> 00:10:51,699
everyone's aware because you know we

00:10:49,660 --> 00:10:54,399
still see the stuff popping up use

00:10:51,699 --> 00:10:55,899
memory barriers responsibly by this I

00:10:54,399 --> 00:10:57,759
mean don't go overboard and use a

00:10:55,899 --> 00:11:00,910
barrier that's way stronger than you

00:10:57,759 --> 00:11:03,490
need back in the day a lot of us driver

00:11:00,910 --> 00:11:06,820
driver driver writers we're using like

00:11:03,490 --> 00:11:08,290
WMV and R&B everywhere and that's

00:11:06,820 --> 00:11:10,089
because that was the only barrier we had

00:11:08,290 --> 00:11:12,040
that we knew would be there even if SMP

00:11:10,089 --> 00:11:13,600
was turned off a lot of that has to do

00:11:12,040 --> 00:11:14,860
with the fact the device is writing to

00:11:13,600 --> 00:11:15,730
this memory so okay we don't want to

00:11:14,860 --> 00:11:18,670
race against it

00:11:15,730 --> 00:11:22,300
and so like I believe it was what two

00:11:18,670 --> 00:11:23,649
years ago I worked on coming up with at

00:11:22,300 --> 00:11:25,420
a time I think I'd called it coherent

00:11:23,649 --> 00:11:27,970
rmv and Linda's told me I don't know how

00:11:25,420 --> 00:11:32,050
to name things so yeah it's not just

00:11:27,970 --> 00:11:34,870
even I'm lousy at naming anyway so yeah

00:11:32,050 --> 00:11:36,250
so the big issue was these memory

00:11:34,870 --> 00:11:39,220
barriers are way stronger than it needs

00:11:36,250 --> 00:11:41,529
to be on x86 for instance they played to

00:11:39,220 --> 00:11:44,290
an instruction called like a elephants

00:11:41,529 --> 00:11:46,240
or I think with the s fence and it would

00:11:44,290 --> 00:11:47,550
just outright cause pipeline stalls all

00:11:46,240 --> 00:11:48,970
sorts of performance hits and

00:11:47,550 --> 00:11:50,589
technically if you read the

00:11:48,970 --> 00:11:52,810
documentation for most things it's only

00:11:50,589 --> 00:11:55,720
needed if you're going to be you know

00:11:52,810 --> 00:11:57,310
transitioning between mm IO and coherent

00:11:55,720 --> 00:11:59,470
memories so if you go to write a device

00:11:57,310 --> 00:12:02,200
tale for instance then you need this

00:11:59,470 --> 00:12:04,690
barrier too so you can say okay write my

00:12:02,200 --> 00:12:06,490
data to my memory ring and notify the

00:12:04,690 --> 00:12:08,770
device in that situation yes you need a

00:12:06,490 --> 00:12:11,020
barrier but if it's okay I'm checking

00:12:08,770 --> 00:12:12,670
this bit on my descriptor ring and I

00:12:11,020 --> 00:12:14,140
need a barrier before I go and access

00:12:12,670 --> 00:12:15,520
the rest of data into the script ring

00:12:14,140 --> 00:12:17,740
for example a status fit that says yes

00:12:15,520 --> 00:12:19,720
devices there the device intended the

00:12:17,740 --> 00:12:24,220
memory back to me then you only need

00:12:19,720 --> 00:12:27,370
this DMA RMB a lot of it just because

00:12:24,220 --> 00:12:29,529
like in the case that x86 the ordering

00:12:27,370 --> 00:12:31,000
is strong enough ordered that you don't

00:12:29,529 --> 00:12:32,110
actually need a memory barrier you just

00:12:31,000 --> 00:12:34,149
need something that says don't reorder

00:12:32,110 --> 00:12:36,790
instructions past this point it's

00:12:34,149 --> 00:12:38,350
basically all it comes down to and so

00:12:36,790 --> 00:12:42,310
that was the main motivation behind the

00:12:38,350 --> 00:12:43,660
DMA wmb RMB the main reason I wanted to

00:12:42,310 --> 00:12:45,370
bring it up I thought everybody had

00:12:43,660 --> 00:12:46,690
already updated and then you know we

00:12:45,370 --> 00:12:48,579
still have issues coming up here and

00:12:46,690 --> 00:12:51,160
there so just something I wanted to

00:12:48,579 --> 00:12:52,690
point out the odds are that you know any

00:12:51,160 --> 00:12:55,149
new driver right out writer out there

00:12:52,690 --> 00:12:56,709
going to find like eat 1,000 or ye 1,000

00:12:55,149 --> 00:12:58,089
year or whatever and copy it it's like I

00:12:56,709 --> 00:13:00,579
don't know if that one's even been a

00:12:58,089 --> 00:13:03,010
fully updated for everything or not so

00:13:00,579 --> 00:13:05,800
it's just something to be aware of with

00:13:03,010 --> 00:13:10,270
that I think I'm done so I believed

00:13:05,800 --> 00:13:11,890
you're up next time yes / okay I'll just

00:13:10,270 --> 00:13:14,940
change this license

00:13:11,890 --> 00:13:14,940
yeah slide deck two please

00:13:17,509 --> 00:13:26,639
yep I think we begin back here could you

00:13:22,709 --> 00:13:27,089
do that I keep the mic in good I'll try

00:13:26,639 --> 00:13:31,589
this one

00:13:27,089 --> 00:13:33,629
yeah it was yeah so so what we're seeing

00:13:31,589 --> 00:13:34,859
is that the fun thing of networking for

00:13:33,629 --> 00:13:39,119
welding puddle lakes in the memory

00:13:34,859 --> 00:13:40,589
allocator so and as alex is already talk

00:13:39,119 --> 00:13:41,249
about how we try to avoid calling the

00:13:40,589 --> 00:13:44,279
memory allocator

00:13:41,249 --> 00:13:48,089
but for for I locate educating our skp

00:13:44,279 --> 00:13:50,160
data structure we we do need it and I

00:13:48,089 --> 00:13:52,319
would say it's basically done because we

00:13:50,160 --> 00:13:54,179
have to implement is the bog API so now

00:13:52,319 --> 00:13:56,549
the more fun stuffs I'm playing with the

00:13:54,179 --> 00:13:59,819
pitch a locator and what we're seeing is

00:13:56,549 --> 00:14:04,230
that it's generally limiting our our new

00:13:59,819 --> 00:14:06,119
cool feature HTTP so basically the

00:14:04,230 --> 00:14:10,019
baseline performance of the picture okay

00:14:06,119 --> 00:14:14,309
the we really cannot use and be using

00:14:10,019 --> 00:14:16,499
tricks like like pastry cycling as SMS

00:14:14,309 --> 00:14:18,179
just talked about and I'm wondering can

00:14:16,499 --> 00:14:20,160
we generalize this and can integrate

00:14:18,179 --> 00:14:22,230
some of these ideas into the wheel pH

00:14:20,160 --> 00:14:24,119
educator or the caching layer and the

00:14:22,230 --> 00:14:26,100
paycheck cater so I'm playing a lot of

00:14:24,119 --> 00:14:29,040
different things so you can I think I

00:14:26,100 --> 00:14:31,230
have a graph here but where you can see

00:14:29,040 --> 00:14:35,009
what what the cost is is which go for

00:14:31,230 --> 00:14:36,720
the different order pages so normally

00:14:35,009 --> 00:14:38,610
network people don't know what all that

00:14:36,720 --> 00:14:41,939
means and network places also have a

00:14:38,610 --> 00:14:44,970
small order to size conversions order

00:14:41,939 --> 00:14:47,759
auto zero means four kilobyte and order

00:14:44,970 --> 00:14:52,319
one is 8 kilobyte an order to 16

00:14:47,759 --> 00:14:54,360
kilobytes and so forth so we some some

00:14:52,319 --> 00:14:55,860
drivers allocates larger order pages but

00:14:54,360 --> 00:14:58,139
basically if you look at the order 0

00:14:55,860 --> 00:15:02,610
pages it is much faster than the graph

00:14:58,139 --> 00:15:05,809
and the reason for this is that the page

00:15:02,610 --> 00:15:08,519
educator implements a caching facility

00:15:05,809 --> 00:15:13,049
for for the 0 pages and up for higher

00:15:08,519 --> 00:15:15,449
order pages and so and the yellow line

00:15:13,049 --> 00:15:19,230
shows one of the tricks that some of the

00:15:15,449 --> 00:15:21,940
drivers do to avoid some of these

00:15:19,230 --> 00:15:23,740
mapping overhead for because what you

00:15:21,940 --> 00:15:26,530
could do as alexis is building out the

00:15:23,740 --> 00:15:29,020
page in several smaller segments so glad

00:15:26,530 --> 00:15:30,910
you could do you can allocate a much

00:15:29,020 --> 00:15:33,790
larger page like 32 kilobytes or

00:15:30,910 --> 00:15:36,460
something or a three page and didn't

00:15:33,790 --> 00:15:40,090
split that up and and you can sort of

00:15:36,460 --> 00:15:42,070
Komatsu size tick the cost there SS and

00:15:40,090 --> 00:15:43,720
and i think believe the trick is also on

00:15:42,070 --> 00:15:46,150
some architectures it's very expensive

00:15:43,720 --> 00:15:48,640
to do DMA mapping so you also had write

00:15:46,150 --> 00:15:51,220
a new previous an iommu yeah yeah you

00:15:48,640 --> 00:15:54,730
are but the DNA mapping costs because

00:15:51,220 --> 00:15:57,130
you only do a mailing per page so that's

00:15:54,730 --> 00:16:00,190
sort of Internet the the green line

00:15:57,130 --> 00:16:02,680
there is our 10 click line rate projects

00:16:00,190 --> 00:16:04,690
which is like two hundred cycles when we

00:16:02,680 --> 00:16:06,490
are playing with stuff like HTTP we

00:16:04,690 --> 00:16:08,710
really cannot use the page elevators

00:16:06,490 --> 00:16:11,290
directly we really need some recycle

00:16:08,710 --> 00:16:13,630
facility because we cannot have that the

00:16:11,290 --> 00:16:16,210
th allocator is taking up all our budget

00:16:13,630 --> 00:16:18,550
and this is actually the improve graph

00:16:16,210 --> 00:16:21,070
because before the base of the

00:16:18,550 --> 00:16:22,960
Patriarchate of us was over was over the

00:16:21,070 --> 00:16:25,480
green line but we recently optimize the

00:16:22,960 --> 00:16:34,930
page educator to actually get below to

00:16:25,480 --> 00:16:38,530
200 cycles cost actually basically over

00:16:34,930 --> 00:16:41,020
recover to slightly busy so we brought

00:16:38,530 --> 00:16:44,130
we allocate larger order pages and we

00:16:41,020 --> 00:16:46,930
had out this fragment but in reality we

00:16:44,130 --> 00:16:49,200
it's it's sort of problematic and Eric

00:16:46,930 --> 00:16:51,880
hits us in the head every time because

00:16:49,200 --> 00:16:55,360
what you can do like number two you can

00:16:51,880 --> 00:16:57,400
have a clever pin down of memory which

00:16:55,360 --> 00:16:59,650
is something that Google sees that you

00:16:57,400 --> 00:17:02,290
can when you have this large order page

00:16:59,650 --> 00:17:04,120
you hand out up front and have it all

00:17:02,290 --> 00:17:05,079
the fragments you can only free a page

00:17:04,120 --> 00:17:05,949
with all the fragments have been

00:17:05,079 --> 00:17:09,100
returned to you

00:17:05,949 --> 00:17:11,890
so I've smoked a clever attacker can can

00:17:09,100 --> 00:17:15,040
cause pin downing pin down of these 32

00:17:11,890 --> 00:17:17,770
kilobytes by making sure that that you

00:17:15,040 --> 00:17:19,959
are like the TCP step stack there needs

00:17:17,770 --> 00:17:23,860
to hold on to some fragments of the page

00:17:19,959 --> 00:17:26,410
and generally when you call the memory

00:17:23,860 --> 00:17:27,850
allocator directly for allocating these

00:17:26,410 --> 00:17:29,080
large order pages you're not sure you

00:17:27,850 --> 00:17:32,850
actually you can actually get one and

00:17:29,080 --> 00:17:34,679
you can also get issues with the memory

00:17:32,850 --> 00:17:38,400
all of a sudden going in to reclaim or

00:17:34,679 --> 00:17:39,720
compaction and install for a period of

00:17:38,400 --> 00:17:44,039
time which is something you definitely

00:17:39,720 --> 00:17:44,900
not one that beat speeds and it does not

00:17:44,039 --> 00:17:48,620
scale well

00:17:44,900 --> 00:17:50,850
Precure materials to put here to

00:17:48,620 --> 00:17:52,650
allocate something higher than over

00:17:50,850 --> 00:17:54,510
cereal pages because it there's a

00:17:52,650 --> 00:18:02,700
central login the Patriot locator that

00:17:54,510 --> 00:18:04,919
that everybody has to grab yeah so like

00:18:02,700 --> 00:18:06,090
I said all the basic all the high-speed

00:18:04,919 --> 00:18:08,669
drivers they they have to do page

00:18:06,090 --> 00:18:12,000
recycling because the educators just low

00:18:08,669 --> 00:18:14,580
and about it LeMay mapping so the thing

00:18:12,000 --> 00:18:17,250
is I want to generalize this and I think

00:18:14,580 --> 00:18:21,419
I proposed something I think this next

00:18:17,250 --> 00:18:23,130
slide so because I feel that every

00:18:21,419 --> 00:18:25,260
drivers reinventing that the pace

00:18:23,130 --> 00:18:26,370
recycling mechanism so far now that's

00:18:25,260 --> 00:18:28,440
the thing is nothing else to Eric

00:18:26,370 --> 00:18:31,049
stocking the Intel approach apparently

00:18:28,440 --> 00:18:32,970
so yeah we need to standardize that

00:18:31,049 --> 00:18:35,330
formalize it into an API of some sort

00:18:32,970 --> 00:18:38,159
yeah that's so that's especially the

00:18:35,330 --> 00:18:41,820
deal either if I have to implement some

00:18:38,159 --> 00:18:43,830
kind of page pool recycling which I

00:18:41,820 --> 00:18:44,970
obviously think it's better but I also

00:18:43,830 --> 00:18:48,030
which we generalize

00:18:44,970 --> 00:18:50,340
Alex's approach more so so it gets

00:18:48,030 --> 00:18:52,890
easier to useful for the drivers of this

00:18:50,340 --> 00:18:57,690
kind of the page recycling so we can

00:18:52,890 --> 00:19:00,620
avoid the DMA map and up map the driver

00:18:57,690 --> 00:19:01,919
still have handle to DMA sink pan and

00:19:00,620 --> 00:19:04,740
yeah

00:19:01,919 --> 00:19:07,710
it's let's let's see how far I get and

00:19:04,740 --> 00:19:10,380
how when alex gets its annoyed enough to

00:19:07,710 --> 00:19:11,669
kill mice since the company has some

00:19:10,380 --> 00:19:12,840
things here we just need to do

00:19:11,669 --> 00:19:16,590
rock-paper-scissors up here on stage

00:19:12,840 --> 00:19:21,750
right now it's like a continuation yeah

00:19:16,590 --> 00:19:25,289
so I think I think that's that's sort of

00:19:21,750 --> 00:19:29,130
what I have and this is a real API in

00:19:25,289 --> 00:19:31,559
tremendous keep out three famous lines I

00:19:29,130 --> 00:19:35,309
don't know you need to discuss more

00:19:31,559 --> 00:19:37,200
about fighting over their paycheck later

00:19:35,309 --> 00:19:39,330
what does it matter you know if nothing

00:19:37,200 --> 00:19:40,860
else you know all of us are fixed in

00:19:39,330 --> 00:19:42,270
time and so it's a matter of one of us

00:19:40,860 --> 00:19:44,490
having enough time to work on all that

00:19:42,270 --> 00:19:45,780
so yeah yeah somebody else out here

00:19:44,490 --> 00:19:45,960
wants to take it on you know I'm going

00:19:45,780 --> 00:19:50,039
to

00:19:45,960 --> 00:19:51,330
review patches that yeah only so many

00:19:50,039 --> 00:19:52,850
min hours I can put out as this

00:19:51,330 --> 00:19:55,260
individual and I have to work on

00:19:52,850 --> 00:19:56,669
checking and seeing what resources I

00:19:55,260 --> 00:19:59,279
have to put it towards that effort so

00:19:56,669 --> 00:20:00,990
yeah but that's the stiffness of

00:19:59,279 --> 00:20:04,620
interesting going on in this area and

00:20:00,990 --> 00:20:09,049
and and believe it yeah yes we focus I

00:20:04,620 --> 00:20:09,049
think we can change to the next slide I

00:20:09,320 --> 00:20:14,640
have a question

00:20:11,299 --> 00:20:17,000
doing it do we know like today what is

00:20:14,640 --> 00:20:19,649
the limit of the page elevator because

00:20:17,000 --> 00:20:24,809
lately we've been hitting like this 80

00:20:19,649 --> 00:20:27,080
or 70 gig barrier because of the patella

00:20:24,809 --> 00:20:27,080
Cato

00:20:28,610 --> 00:20:36,390
lately have been hitting this seventy a

00:20:32,690 --> 00:20:39,450
70 gig barrier with our performance

00:20:36,390 --> 00:20:41,850
testing and would submit rate with your

00:20:39,450 --> 00:20:46,350
letter system is a change that is still

00:20:41,850 --> 00:20:50,340
working progress we get to the what we

00:20:46,350 --> 00:20:52,380
need to the line rate of 100 Gig and do

00:20:50,340 --> 00:20:55,710
we have any plans for future work for

00:20:52,380 --> 00:20:59,010
like our preparation for 200 gig cetera

00:20:55,710 --> 00:21:01,559
yeah we have so the preparation is I we

00:20:59,010 --> 00:21:03,450
do some some faster patient tracking but

00:21:01,559 --> 00:21:06,299
and get it also optimizing the Paycheck

00:21:03,450 --> 00:21:07,860
data so we started like to cost like two

00:21:06,299 --> 00:21:09,029
hundred and seventy cycles or something

00:21:07,860 --> 00:21:14,640
now we around it

00:21:09,029 --> 00:21:17,429
it cost a hundred and something

00:21:14,640 --> 00:21:19,080
I mean ATS cycles or something and we

00:21:17,429 --> 00:21:21,899
can bring we could bring it bring it

00:21:19,080 --> 00:21:23,669
bring it further down is the matter did

00:21:21,899 --> 00:21:24,929
some really really cool taxes cutting

00:21:23,669 --> 00:21:27,059
out a lot of the stuff in the page

00:21:24,929 --> 00:21:28,649
educator for the photo whatever the

00:21:27,059 --> 00:21:30,450
SeaWorld cash it has in the page high

00:21:28,649 --> 00:21:33,029
together and I estimate we can get as

00:21:30,450 --> 00:21:35,580
low as hundred cycles for prices

00:21:33,029 --> 00:21:37,320
accessing the page educator so and

00:21:35,580 --> 00:21:39,929
that's I think I'm going to hit some

00:21:37,320 --> 00:21:42,240
limits around 100 cycles for for

00:21:39,929 --> 00:21:45,079
forgetting pages out of the page

00:21:42,240 --> 00:21:48,389
educator which is should be less and

00:21:45,079 --> 00:21:51,119
yeah and my budgets or with goggle each

00:21:48,389 --> 00:21:53,759
100 satisficing we still need some some

00:21:51,119 --> 00:21:55,499
kind of recycle caching because we don't

00:21:53,759 --> 00:21:58,379
we cannot get lower than 100 psychosis

00:21:55,499 --> 00:22:01,769
my addiction okay actually probably

00:21:58,379 --> 00:22:02,969
before we go to John thing any questions

00:22:01,769 --> 00:22:15,809
in the audience about either my

00:22:02,969 --> 00:22:19,529
presentation or yes pers so my question

00:22:15,809 --> 00:22:23,429
is about the new implementation of the

00:22:19,529 --> 00:22:26,819
page pool so the page cache or the page

00:22:23,429 --> 00:22:28,829
pool is it's down to specific receive

00:22:26,819 --> 00:22:31,319
queue is it down to specific net devices

00:22:28,829 --> 00:22:33,539
but it's bouncer specific review Q so I

00:22:31,319 --> 00:22:35,609
get some of the performance out of that

00:22:33,539 --> 00:22:38,849
that I'm bothering and creating a page

00:22:35,609 --> 00:22:41,729
pool first perceive Q actually look at

00:22:38,849 --> 00:22:44,009
for received queue yep okay that's a

00:22:41,729 --> 00:22:46,859
test that's my I see patches that is out

00:22:44,009 --> 00:22:48,299
there the other thing I think that we

00:22:46,859 --> 00:22:50,009
may want to re-examine that it may be

00:22:48,299 --> 00:22:51,569
better to do it either per node or per

00:22:50,009 --> 00:22:53,009
CPU depending on which way you won't

00:22:51,569 --> 00:22:54,569
look at because you had the perk you can

00:22:53,009 --> 00:22:56,099
end up being there's a reason why I'm

00:22:54,569 --> 00:22:58,019
hard caps at such a low of little value

00:22:56,099 --> 00:22:59,399
and that's probably because like the

00:22:58,019 --> 00:23:00,839
implementation in the Intel drivers is

00:22:59,399 --> 00:23:01,979
all per rings we basically fill the

00:23:00,839 --> 00:23:03,689
descriptor ringing in the phones of

00:23:01,979 --> 00:23:05,189
pages stay there they stay there we

00:23:03,689 --> 00:23:07,589
start looking at a pool doing it perk

00:23:05,189 --> 00:23:08,849
you that becomes something else like a

00:23:07,589 --> 00:23:12,269
lot of their measures but the pool is

00:23:08,849 --> 00:23:13,679
bone to it alright

00:23:12,269 --> 00:23:15,209
it needs to be done to the device but

00:23:13,679 --> 00:23:17,609
I'm saying it doesn't need to be you

00:23:15,209 --> 00:23:20,459
know per device for Q it could be per

00:23:17,609 --> 00:23:23,309
device per CPU or per device per Numa

00:23:20,459 --> 00:23:25,469
node that per CPU becomes an easier way

00:23:23,309 --> 00:23:28,109
to deal with it yeah but what is a given

00:23:25,469 --> 00:23:30,779
that the device would allocate 1 1 Q per

00:23:28,109 --> 00:23:33,419
see you soon so you will get that small

00:23:30,779 --> 00:23:36,599
one or more excuse for hitting on that

00:23:33,419 --> 00:23:38,639
that the I'm so ticked its while like

00:23:36,599 --> 00:23:40,109
that's a net P so well right but see

00:23:38,639 --> 00:23:41,759
that's also why I'm thinking per CPU

00:23:40,109 --> 00:23:43,499
because then you're still connected by

00:23:41,759 --> 00:23:44,969
nappy and if you're not running on

00:23:43,499 --> 00:23:47,939
certain CPUs you just don't have a page

00:23:44,969 --> 00:23:49,499
pole there or whatever so you could in

00:23:47,939 --> 00:23:50,999
theory double up your page pool than if

00:23:49,499 --> 00:23:55,739
you like to have the Q's stacked on a

00:23:50,999 --> 00:23:57,809
small set of CPUs other the thought yeah

00:23:55,739 --> 00:23:58,419
I'm still going to bounce how much we

00:23:57,809 --> 00:24:00,129
can have in

00:23:58,419 --> 00:24:02,350
cage pool to make sure that we don't

00:24:00,129 --> 00:24:04,960
work like can run make the system run up

00:24:02,350 --> 00:24:08,619
around 1/2 memory going to be use of

00:24:04,960 --> 00:24:11,799
bounded bounded page poopoo to about

00:24:08,619 --> 00:24:13,389
this thing but but what was what likely

00:24:11,799 --> 00:24:16,539
going to happen is what one of the

00:24:13,389 --> 00:24:18,730
tricks is that you've run dry and well

00:24:16,539 --> 00:24:20,769
what happens you have to actually cause

00:24:18,730 --> 00:24:23,309
a normal pitch alligator so in that

00:24:20,769 --> 00:24:25,570
situation I want to implement poking

00:24:23,309 --> 00:24:28,389
from the page alligators we have to

00:24:25,570 --> 00:24:30,129
spark APR allocating for the page

00:24:28,389 --> 00:24:31,720
educator and when we hit in situations

00:24:30,129 --> 00:24:33,070
where we run dry that's why I'm not

00:24:31,720 --> 00:24:36,129
afraid of making in a fairly small

00:24:33,070 --> 00:24:38,619
bounded queue because I'm just going to

00:24:36,129 --> 00:24:40,629
get bulk from from the page editor and

00:24:38,619 --> 00:24:43,809
then returning yeah I'm also going to do

00:24:40,629 --> 00:24:46,619
about API for the paycheck Gators then

00:24:43,809 --> 00:24:48,789
we could discuss it it's bounded if we

00:24:46,619 --> 00:24:50,739
encapsulate in something called page

00:24:48,789 --> 00:24:53,470
full of being encapsulated in something

00:24:50,739 --> 00:24:54,609
else that Mariel excuse or we open code

00:24:53,470 --> 00:24:57,989
and use these bulk

00:24:54,609 --> 00:25:00,580
API call to Ricky of the drivers but

00:24:57,989 --> 00:25:05,279
that's completely up to health name use

00:25:00,580 --> 00:25:05,279
this API and time will tell

00:25:07,379 --> 00:25:15,179
any other questions any more questions

00:25:11,259 --> 00:25:15,179
to me else we could hand it over to John

00:25:19,230 --> 00:25:26,909
okay hello I was going to talk about

00:25:22,179 --> 00:25:31,210
some quick oh good good may so just

00:25:26,909 --> 00:25:32,980
making sure the page holes are not by

00:25:31,210 --> 00:25:35,080
themselves trying to solve the the

00:25:32,980 --> 00:25:36,940
memory fragmentation issue that you

00:25:35,080 --> 00:25:38,710
mentioned right we're a bunch of

00:25:36,940 --> 00:25:40,570
applications are holding a specific page

00:25:38,710 --> 00:25:44,679
so you never really have I won't solve

00:25:40,570 --> 00:25:51,249
that okay it's you know because if you I

00:25:44,679 --> 00:25:52,840
guess if you somehow bounded the as you

00:25:51,249 --> 00:25:55,359
mentioned if you're running dry if

00:25:52,840 --> 00:25:57,519
instead of you just stop they receive

00:25:55,359 --> 00:25:58,570
cue that would kind of solve it but

00:25:57,519 --> 00:26:02,159
maybe it caused a bunch of other

00:25:58,570 --> 00:26:06,369
problems when you cannot replenish the

00:26:02,159 --> 00:26:08,109
Java video so I've talked to tested I

00:26:06,369 --> 00:26:10,299
think you're sort of hinting to some of

00:26:08,109 --> 00:26:11,260
the other benefits from I'm doing a page

00:26:10,299 --> 00:26:13,059
pool because

00:26:11,260 --> 00:26:15,669
if I'm doing fish well I have to account

00:26:13,059 --> 00:26:18,010
how many outstanding pages I have and

00:26:15,669 --> 00:26:20,110
and I could use that to say okay I want

00:26:18,010 --> 00:26:22,299
to stop because now I can see the system

00:26:20,110 --> 00:26:24,250
has used half of the pages in the system

00:26:22,299 --> 00:26:27,640
and I'm not going to allow this received

00:26:24,250 --> 00:26:30,850
you to proceed that's that that would be

00:26:27,640 --> 00:26:34,330
one of the sort of - one of the benefits

00:26:30,850 --> 00:26:37,809
from from using a page robot but I sort

00:26:34,330 --> 00:26:39,429
of had arrived benefit I won't bring

00:26:37,809 --> 00:26:41,559
into like the discussion why we should

00:26:39,429 --> 00:26:53,640
do this because it's maybe it's not that

00:26:41,559 --> 00:26:56,799
useful and more questions comments okay

00:26:53,640 --> 00:27:00,070
so I just want to talk about some early

00:26:56,799 --> 00:27:03,669
work Bjorn and myself are doing around

00:27:00,070 --> 00:27:05,169
AF packet yeah okay here we go so if

00:27:03,669 --> 00:27:07,740
you're not familiar with a a packet it's

00:27:05,169 --> 00:27:10,059
kind of what drives behind lippy cap and

00:27:07,740 --> 00:27:13,900
sericata and I know there's a bunch of

00:27:10,059 --> 00:27:16,690
other tools so it was sort of motivated

00:27:13,900 --> 00:27:19,059
to accelerate the existing data path for

00:27:16,690 --> 00:27:23,860
these applications that are running in

00:27:19,059 --> 00:27:25,929
user space and one of the kind of a

00:27:23,860 --> 00:27:29,200
common theme it seems that it would be

00:27:25,929 --> 00:27:31,059
great to remove as much copies as we can

00:27:29,200 --> 00:27:34,890
so we saw that earlier today with some

00:27:31,059 --> 00:27:34,890
of the zero copy stuff for TCP and UDP

00:27:35,160 --> 00:27:39,130
starting from that premise we have the

00:27:37,240 --> 00:27:43,450
idea that we would like to also support

00:27:39,130 --> 00:27:46,900
zero copy Rx in in these other

00:27:43,450 --> 00:27:50,440
applications as well so how are we

00:27:46,900 --> 00:27:52,210
looking at doing that first we have an

00:27:50,440 --> 00:27:54,460
application that sits at the top there

00:27:52,210 --> 00:27:57,429
it has an AF packet socket that it

00:27:54,460 --> 00:28:00,520
creates and it has some memory if we can

00:27:57,429 --> 00:28:03,490
memory map that that that block of

00:28:00,520 --> 00:28:06,820
memory and tenet or alternatively

00:28:03,490 --> 00:28:09,130
request memory from the kernel itself we

00:28:06,820 --> 00:28:11,440
can then pass the the pages into the

00:28:09,130 --> 00:28:13,480
driver and why we want to do this is so

00:28:11,440 --> 00:28:17,169
that when the driver today when it does

00:28:13,480 --> 00:28:19,210
a DMA into the into the kernel you're

00:28:17,169 --> 00:28:22,150
grabbing pages out of a page pool or a

00:28:19,210 --> 00:28:23,740
driver allocated page and these are all

00:28:22,150 --> 00:28:24,530
owned by the kernel which means then you

00:28:23,740 --> 00:28:26,510
have

00:28:24,530 --> 00:28:28,430
you in the traditional AF pocket you

00:28:26,510 --> 00:28:30,800
have to process that building that's KB

00:28:28,430 --> 00:28:33,140
send it to the AF socket socket logic

00:28:30,800 --> 00:28:35,510
and then they attack a socket socket

00:28:33,140 --> 00:28:38,900
logic will do a most likely a copy into

00:28:35,510 --> 00:28:40,490
user space so what we've sort of

00:28:38,900 --> 00:28:43,190
identified is the cost of all these

00:28:40,490 --> 00:28:46,090
things add up and and correspondingly

00:28:43,190 --> 00:28:49,190
your performance per packet goes down so

00:28:46,090 --> 00:28:51,170
by pushing the actual pages into the

00:28:49,190 --> 00:28:53,720
driver letting the driver populate the

00:28:51,170 --> 00:28:57,890
descriptors in the driver so that we can

00:28:53,720 --> 00:29:01,340
do rx DMA copy into the memory that are

00:28:57,890 --> 00:29:07,610
shared with user space we no longer have

00:29:01,340 --> 00:29:13,190
to build an skb and and also do the copy

00:29:07,610 --> 00:29:15,530
in the stack so that's great

00:29:13,190 --> 00:29:18,710
now the sort of next question that comes

00:29:15,530 --> 00:29:20,510
out of that is so you've initialized the

00:29:18,710 --> 00:29:22,810
descriptor ring it has a bunch of DMA

00:29:20,510 --> 00:29:28,730
addresses that it knows map to

00:29:22,810 --> 00:29:31,580
application program the question is how

00:29:28,730 --> 00:29:33,440
do the application then understand that

00:29:31,580 --> 00:29:34,850
the packets have already been copied

00:29:33,440 --> 00:29:36,680
into user space so what we've done is

00:29:34,850 --> 00:29:38,780
we've built a virtual descriptor ring

00:29:36,680 --> 00:29:41,300
that is shared between both the

00:29:38,780 --> 00:29:43,250
application and the driver and when the

00:29:41,300 --> 00:29:45,650
driver receives a packet which it has

00:29:43,250 --> 00:29:47,510
already DMA into memory that the

00:29:45,650 --> 00:29:49,910
application knows about it then

00:29:47,510 --> 00:29:52,970
translates its own descriptor into a

00:29:49,910 --> 00:29:54,380
common generic virtual descriptor that

00:29:52,970 --> 00:29:58,130
is shared with the application and that

00:29:54,380 --> 00:30:00,500
descriptor is actually defined by AF

00:29:58,130 --> 00:30:02,540
packet before so we did look at using

00:30:00,500 --> 00:30:07,240
other older versions of AF packet

00:30:02,540 --> 00:30:10,250
I have packet v2 v3 but trying to map a

00:30:07,240 --> 00:30:12,020
descriptor that is sort of built for

00:30:10,250 --> 00:30:14,080
hardware and a descriptor that had never

00:30:12,020 --> 00:30:16,280
been intended to use with hardware is

00:30:14,080 --> 00:30:17,780
challenging and because we're trying to

00:30:16,280 --> 00:30:19,280
get the lowest possible overhead we want

00:30:17,780 --> 00:30:23,690
to get sort of the closest mapping as

00:30:19,280 --> 00:30:26,410
possible the important or at least one

00:30:23,690 --> 00:30:28,490
important observation here is that the

00:30:26,410 --> 00:30:29,870
descriptor that the application is

00:30:28,490 --> 00:30:33,560
looking at in the virtual descriptor

00:30:29,870 --> 00:30:35,810
ring is not specific to any hardware

00:30:33,560 --> 00:30:37,660
so although we are doing this right now

00:30:35,810 --> 00:30:42,830
on Intel hardware

00:30:37,660 --> 00:30:45,410
we fully expect and will want to review

00:30:42,830 --> 00:30:47,720
with other vendors to ensure that we

00:30:45,410 --> 00:30:49,820
don't somehow encode some sort of Intel

00:30:47,720 --> 00:30:51,290
specific data into that descriptor the

00:30:49,820 --> 00:30:54,290
idea being that if you have an

00:30:51,290 --> 00:30:55,820
application that runs on your 10 gig AF

00:30:54,290 --> 00:30:57,350
package before today you should very

00:30:55,820 --> 00:31:01,760
easily be able to swap out a 40 or 100

00:30:57,350 --> 00:31:06,290
gig and run the same program and see you

00:31:01,760 --> 00:31:08,630
know the performance improve the next

00:31:06,290 --> 00:31:13,030
question that often comes up in this

00:31:08,630 --> 00:31:15,890
context is if you how do you decide what

00:31:13,030 --> 00:31:21,200
descriptor ring in the hardware to use

00:31:15,890 --> 00:31:25,340
for for this if we just sort of mapped

00:31:21,200 --> 00:31:28,600
all the dma addresses on one descriptor

00:31:25,340 --> 00:31:31,070
to to user space on an arbitrary ring

00:31:28,600 --> 00:31:32,810
you know RSS would usually kick in and

00:31:31,070 --> 00:31:34,280
you get some flows in user space and

00:31:32,810 --> 00:31:37,850
sloven flows in kernel space so you

00:31:34,280 --> 00:31:39,320
would be totally unmanageable so what

00:31:37,850 --> 00:31:41,900
we're doing is we're using the hardware

00:31:39,320 --> 00:31:44,120
features that exists on I think almost

00:31:41,900 --> 00:31:46,850
all of the 10 gig necks I believe and

00:31:44,120 --> 00:31:49,250
certainly most of the 40 gig NICs that

00:31:46,850 --> 00:31:54,410
allow you to use an interval filter to

00:31:49,250 --> 00:31:56,390
match a 5-2 pool or IP address or a MAC

00:31:54,410 --> 00:31:59,000
address even and forward that traffic to

00:31:56,390 --> 00:32:03,050
a specific queue and we expect that

00:31:59,000 --> 00:32:05,270
since you when we do the mapping from of

00:32:03,050 --> 00:32:08,240
the memory from the application to the

00:32:05,270 --> 00:32:10,790
driver we specify the ring to use then

00:32:08,240 --> 00:32:12,740
you have a five tuple or some other kind

00:32:10,790 --> 00:32:14,930
of match that says I want all of the

00:32:12,740 --> 00:32:16,700
traffic that is this IP address for

00:32:14,930 --> 00:32:19,820
example and I want to forward it to this

00:32:16,700 --> 00:32:22,550
ring I sort of a two-phase step at the

00:32:19,820 --> 00:32:25,520
moment but it seems to work pretty good

00:32:22,550 --> 00:32:27,980
and then the rest of your rings can use

00:32:25,520 --> 00:32:33,380
RSS as normal their traffic goes to X to

00:32:27,980 --> 00:32:37,340
PU the stack or wherever they go I think

00:32:33,380 --> 00:32:39,710
what we have is fairly generic now we

00:32:37,340 --> 00:32:41,330
had in a original sort of rough patch

00:32:39,710 --> 00:32:43,850
set that we sent out to the mailing list

00:32:41,330 --> 00:32:47,000
on I did a v2 translation so the idea is

00:32:43,850 --> 00:32:49,690
very similar just with the improved data

00:32:47,000 --> 00:32:52,390
structures and descriptor formats

00:32:49,690 --> 00:32:55,570
and we hope to get out some patches soon

00:32:52,390 --> 00:32:59,200
but they're not out there yet so that's

00:32:55,570 --> 00:33:00,430
the are XPath the next question is so

00:32:59,200 --> 00:33:02,260
you've received packets in your

00:33:00,430 --> 00:33:05,320
application a lot of applications also

00:33:02,260 --> 00:33:07,660
like to send packet and so how do we do

00:33:05,320 --> 00:33:11,320
the same sort of logic but in the other

00:33:07,660 --> 00:33:12,760
direction so because we have the

00:33:11,320 --> 00:33:15,010
infrastructure to share this virtual

00:33:12,760 --> 00:33:17,350
ring with the application already we can

00:33:15,010 --> 00:33:19,600
do the same thing with TX rings not them

00:33:17,350 --> 00:33:21,970
very similarly similar to how we do it

00:33:19,600 --> 00:33:26,500
in the RX case map the virtual ring to a

00:33:21,970 --> 00:33:27,760
hardware TX ring and we can have the

00:33:26,500 --> 00:33:28,840
application populate the virtual

00:33:27,760 --> 00:33:30,340
descriptor ring and then kick the

00:33:28,840 --> 00:33:33,220
hardware at which point the hardware

00:33:30,340 --> 00:33:35,110
then needs to go and read the ring set

00:33:33,220 --> 00:33:38,950
up its own descriptors and transmit the

00:33:35,110 --> 00:33:40,480
packet so right now we're you know we

00:33:38,950 --> 00:33:42,130
debate done exactly what this kick looks

00:33:40,480 --> 00:33:45,700
like it could be a system call it could

00:33:42,130 --> 00:33:47,200
be something like the always on busy

00:33:45,700 --> 00:33:49,630
pull logic that we talked about this

00:33:47,200 --> 00:33:52,180
morning something along those lines to

00:33:49,630 --> 00:33:55,870
kick it and this is sort of the rough

00:33:52,180 --> 00:33:59,140
sketch of how we're doing zero copy in

00:33:55,870 --> 00:34:01,960
the user space for our X TX in a sort of

00:33:59,140 --> 00:34:04,390
Hardware agnostic way and while trying

00:34:01,960 --> 00:34:08,590
to limit the overhead that we incur from

00:34:04,390 --> 00:34:12,570
this so it's a workshop so I felt ok

00:34:08,590 --> 00:34:15,610
showing very early patches and design

00:34:12,570 --> 00:34:17,260
but I think you know next net said we

00:34:15,610 --> 00:34:19,630
should have some performance numbers and

00:34:17,260 --> 00:34:20,950
hopefully share maybe the the

00:34:19,630 --> 00:34:22,390
interesting thing is if anybody is

00:34:20,950 --> 00:34:24,850
working on similar stuff it would be

00:34:22,390 --> 00:34:28,620
great to hear from you if anybody has

00:34:24,850 --> 00:34:30,820
any comments or questions ask now or

00:34:28,620 --> 00:34:33,669
definitely come find me later looks like

00:34:30,820 --> 00:34:39,429
there's some in the back now so anybody

00:34:33,669 --> 00:34:42,300
know where the other monkey's is behind

00:34:39,429 --> 00:34:42,300
your laptop Oh

00:34:43,249 --> 00:34:50,909
so hopefully there was someone clear a

00:34:47,299 --> 00:34:53,009
lot of details about the virtual this is

00:34:50,909 --> 00:34:55,559
very nice by the virtual descriptive

00:34:53,009 --> 00:34:58,289
format yeah it's Friday Oh suitable for

00:34:55,559 --> 00:35:00,900
this sorry oh it was the it's Froyo a

00:34:58,289 --> 00:35:04,829
suitable virtual descriptor ring for a

00:35:00,900 --> 00:35:06,930
faculty for I still missed the part of

00:35:04,829 --> 00:35:09,440
it as a question is is Verdejo a

00:35:06,930 --> 00:35:13,140
suitable descriptor format for business

00:35:09,440 --> 00:35:15,299
okay um we did look at Verdi oh and it's

00:35:13,140 --> 00:35:17,490
really close but there are some issues

00:35:15,299 --> 00:35:20,549
mapping the Verdi or Scripture into the

00:35:17,490 --> 00:35:22,680
hardware we have some hope that the

00:35:20,549 --> 00:35:24,210
Verdi oh next they're working on you

00:35:22,680 --> 00:35:27,630
expect for Verdi oh maybe the next spec

00:35:24,210 --> 00:35:29,549
might be a little bit closer it's

00:35:27,630 --> 00:35:32,430
unclear so we went off and created our

00:35:29,549 --> 00:35:35,309
own AF package before at this point yeah

00:35:32,430 --> 00:35:38,819
so we looked at this problem a lot point

00:35:35,309 --> 00:35:40,859
nine five were tyo one dot io what i/o

00:35:38,819 --> 00:35:43,249
they both have the same problem multiple

00:35:40,859 --> 00:35:45,509
in this direction not suitable for DMA

00:35:43,249 --> 00:35:47,759
if what I was going to go in a direction

00:35:45,509 --> 00:35:50,579
where it is suitable for DMA it would

00:35:47,759 --> 00:35:53,700
work out but for now there is a whole

00:35:50,579 --> 00:35:57,480
lot of optimization that is missing nine

00:35:53,700 --> 00:35:59,910
comes to a hard way okay so I'm not

00:35:57,480 --> 00:36:03,480
involved in vertigo but I think that our

00:35:59,910 --> 00:36:06,749
plans - I prefer Nia 1.1 so it may make

00:36:03,480 --> 00:36:12,599
sense to try to at least work towards a

00:36:06,749 --> 00:36:14,009
common solution here so about the RF

00:36:12,599 --> 00:36:15,989
strings I think the shadows also say the

00:36:14,009 --> 00:36:17,579
same thing and I haven't talked to you

00:36:15,989 --> 00:36:18,660
but if you have any badges you know if

00:36:17,579 --> 00:36:21,930
you want to share them for us because

00:36:18,660 --> 00:36:25,680
we're going to try but for the PX ID I

00:36:21,930 --> 00:36:29,579
know you're trying to save a copy and

00:36:25,680 --> 00:36:34,130
all but if would it mean losing checksum

00:36:29,579 --> 00:36:37,259
offload and things like that uh yeah in

00:36:34,130 --> 00:36:40,529
existing prototypes that we have we have

00:36:37,259 --> 00:36:45,390
ignored the hardware offload a piece of

00:36:40,529 --> 00:36:48,509
it that's kind of not nice I mean I

00:36:45,390 --> 00:36:49,680
scabies I get cursed out a lot but being

00:36:48,509 --> 00:36:51,420
able to offer some of these things

00:36:49,680 --> 00:36:53,930
really helps so it would be nice to keep

00:36:51,420 --> 00:36:58,790
that yeah so checksum

00:36:53,930 --> 00:37:01,400
is a good one to consider I think we are

00:36:58,790 --> 00:37:04,550
also wrestling with that in the XDP

00:37:01,400 --> 00:37:08,329
world as well so maybe we can come up

00:37:04,550 --> 00:37:11,020
with some solutions okay yeah I think

00:37:08,329 --> 00:37:11,020
it's a valid point

00:37:11,890 --> 00:37:17,569
have you left space in the descriptor

00:37:14,960 --> 00:37:20,839
rings for other use cases for example

00:37:17,569 --> 00:37:22,579
being able to push UDP and TCP packets

00:37:20,839 --> 00:37:25,910
that have been received on the device to

00:37:22,579 --> 00:37:27,800
you in your space so I mean if you're

00:37:25,910 --> 00:37:29,569
asking if like some packet is received

00:37:27,800 --> 00:37:31,579
on another descriptor that's not doing

00:37:29,569 --> 00:37:33,950
the direct kind of mapping and female

00:37:31,579 --> 00:37:37,190
justice could you also push it - well

00:37:33,950 --> 00:37:42,440
I'm thinking zero copy received both TCP

00:37:37,190 --> 00:37:45,109
packets zero copy received a TCP actress

00:37:42,440 --> 00:37:48,800
code so if a packet is on the ring at

00:37:45,109 --> 00:37:50,329
the moment we any packet that's received

00:37:48,800 --> 00:37:52,010
on that ring that has been allocated to

00:37:50,329 --> 00:37:53,780
the user spaces is zero copy Deezer

00:37:52,010 --> 00:37:55,609
space and so there's no way to like

00:37:53,780 --> 00:37:59,329
somehow get that back in the stack at

00:37:55,609 --> 00:38:00,859
that point it's more you need to leave

00:37:59,329 --> 00:38:02,829
space in the descriptor rings to

00:38:00,859 --> 00:38:06,980
identify the socket that it came in on

00:38:02,829 --> 00:38:10,510
so so the question is is the descriptor

00:38:06,980 --> 00:38:14,720
format at least extensible to a degree

00:38:10,510 --> 00:38:19,569
um the current prototype descriptor

00:38:14,720 --> 00:38:21,829
format I think could probably use some

00:38:19,569 --> 00:38:24,619
perhaps additional reviews to see if

00:38:21,829 --> 00:38:28,400
additional use cases will work in there

00:38:24,619 --> 00:38:29,930
I am definitely interested in feedback

00:38:28,400 --> 00:38:31,040
on that on that front when we actually

00:38:29,930 --> 00:38:33,740
get something mailing list I'd be really

00:38:31,040 --> 00:38:35,900
interesting to see the sort of other

00:38:33,740 --> 00:38:38,150
port I would make as though we are

00:38:35,900 --> 00:38:40,520
trying to go for sort of up maximum

00:38:38,150 --> 00:38:42,260
performance here and so if it slows it

00:38:40,520 --> 00:38:46,369
down then we have to weigh the

00:38:42,260 --> 00:38:49,030
flexibility versus the cost and my

00:38:46,369 --> 00:38:52,579
opinion is it's not terribly expensive

00:38:49,030 --> 00:38:55,819
you know you can mix and Max match AF

00:38:52,579 --> 00:38:57,319
packet before at v3 for example right it

00:38:55,819 --> 00:38:58,670
pushes the complexity into the

00:38:57,319 --> 00:39:01,190
application though which is again

00:38:58,670 --> 00:39:03,230
perhaps not nice so yeah but that

00:39:01,190 --> 00:39:05,180
complexity is worth worth it for things

00:39:03,230 --> 00:39:06,599
like messaging systems where you have

00:39:05,180 --> 00:39:08,640
lots and

00:39:06,599 --> 00:39:10,170
two packets coming in and doing a lot of

00:39:08,640 --> 00:39:11,489
fan-out well then you're also talking

00:39:10,170 --> 00:39:12,630
about an environment where you have to

00:39:11,489 --> 00:39:14,999
have shared memory between the

00:39:12,630 --> 00:39:17,249
applications and write which is easily

00:39:14,999 --> 00:39:21,440
attainable in a lot of piece dedicated

00:39:17,249 --> 00:39:23,970
use cases yeah I think it's all valid uh

00:39:21,440 --> 00:39:26,460
more questions please okay the back

00:39:23,970 --> 00:39:28,589
you've had your share for the day so I

00:39:26,460 --> 00:39:29,369
have a question actually so I think I

00:39:28,589 --> 00:39:32,400
asked you this before

00:39:29,369 --> 00:39:34,589
you can do better than RSS I can do

00:39:32,400 --> 00:39:36,989
better than Isis yeah I mean you can

00:39:34,589 --> 00:39:38,460
well RSS is the default that's all these

00:39:36,989 --> 00:39:41,729
things that something's distributing the

00:39:38,460 --> 00:39:43,589
Q's but if I just if I wanted to do TC

00:39:41,729 --> 00:39:47,069
what do I want to do it's better than

00:39:43,589 --> 00:39:49,170
vs. oh I let's say I just want to look

00:39:47,069 --> 00:39:51,390
at DNS packets that's the only thing I

00:39:49,170 --> 00:39:54,900
want to pass to use a space so user

00:39:51,390 --> 00:39:57,779
space packets are in our models are you

00:39:54,900 --> 00:40:00,900
steer them fears of usually flow

00:39:57,779 --> 00:40:02,220
director or TC or something right all

00:40:00,900 --> 00:40:05,450
right what I'm saying is that the rest

00:40:02,220 --> 00:40:07,829
of the Q's can still use RSS in parallel

00:40:05,450 --> 00:40:09,059
right so you but you'd probably want to

00:40:07,829 --> 00:40:11,160
pass all of your traffic to user space

00:40:09,059 --> 00:40:15,299
this sorry okay so I just want to pass

00:40:11,160 --> 00:40:16,799
specific flows okay doable not RSA give

00:40:15,299 --> 00:40:20,430
us a couple months we'll have coloring

00:40:16,799 --> 00:40:23,609
you can have it and RS yeah pretty sure

00:40:20,430 --> 00:40:26,249
we can do that I've got one question

00:40:23,609 --> 00:40:28,920
what if it's regarding this car together

00:40:26,249 --> 00:40:30,690
interface do you support that like it

00:40:28,920 --> 00:40:33,989
might depend one vendor to vendor how

00:40:30,690 --> 00:40:36,089
they're basically I mean how many

00:40:33,989 --> 00:40:38,789
fragments are part of basically that

00:40:36,089 --> 00:40:40,619
descriptive right so it could be say

00:40:38,789 --> 00:40:44,640
sixteen could be more depending on the

00:40:40,619 --> 00:40:47,309
vendor what your I mean targeting right

00:40:44,640 --> 00:40:48,779
so how would you generalize that

00:40:47,309 --> 00:40:51,989
particular space within the descriptor

00:40:48,779 --> 00:40:54,479
of say for example a TX so it could be

00:40:51,989 --> 00:40:56,039
number three bits which you might be in

00:40:54,479 --> 00:40:58,979
the actual descriptor

00:40:56,039 --> 00:41:00,779
it could be any number of bits right so

00:40:58,979 --> 00:41:04,789
it needs to be generalized to have some

00:41:00,779 --> 00:41:07,469
sort of a very virtual isolation and

00:41:04,789 --> 00:41:08,759
think I miss the quest also there so

00:41:07,469 --> 00:41:11,009
basically there's a way to handle

00:41:08,759 --> 00:41:13,380
scatter gather right it hasn't sort of

00:41:11,009 --> 00:41:13,920
in the frame notification or something

00:41:13,380 --> 00:41:17,969
like that

00:41:13,920 --> 00:41:19,229
so is this a single buffer only set up

00:41:17,969 --> 00:41:19,890
or is this something where you could

00:41:19,229 --> 00:41:25,619
assemble a scatter

00:41:19,890 --> 00:41:28,140
and handle like jumbo frames um maybe we

00:41:25,619 --> 00:41:30,210
could handle jumbo frames yes again the

00:41:28,140 --> 00:41:31,650
prototype is not so concerned with this

00:41:30,210 --> 00:41:32,880
because if nothing else was probably

00:41:31,650 --> 00:41:35,069
like a 1-bit field that you'd have to

00:41:32,880 --> 00:41:37,319
add your description to say like in the

00:41:35,069 --> 00:41:38,970
packet you just like the hardware does

00:41:37,319 --> 00:41:40,289
you get out right the next four

00:41:38,970 --> 00:41:42,119
descriptors are actual virtual

00:41:40,289 --> 00:41:44,750
descriptors belong to the same packet

00:41:42,119 --> 00:41:46,859
and then set the end of packet date yeah

00:41:44,750 --> 00:41:51,119
seems reasonable

00:41:46,859 --> 00:41:53,609
there's there's always a you know I

00:41:51,119 --> 00:41:55,859
think we need to get the prototype done

00:41:53,609 --> 00:41:57,690
get some original code out there get the

00:41:55,859 --> 00:41:59,460
code completed and then I think these to

00:41:57,690 --> 00:42:03,769
me are sort of features that fall out

00:41:59,460 --> 00:42:05,119
fall led to a and a subsequent patch set

00:42:03,769 --> 00:42:08,579
Thanks

00:42:05,119 --> 00:42:11,700
that's sure yeah so just making sure I

00:42:08,579 --> 00:42:13,619
understand so is this approach that the

00:42:11,700 --> 00:42:15,829
packet gets into application memory and

00:42:13,619 --> 00:42:19,109
it's kind of completely lost

00:42:15,829 --> 00:42:20,670
there's no it should never see really

00:42:19,109 --> 00:42:26,640
see the stack if I understand you right

00:42:20,670 --> 00:42:28,680
yeah yeah I mean this is the point the

00:42:26,640 --> 00:42:30,690
point is that this you blinded it you

00:42:28,680 --> 00:42:33,599
have to have a hardware filter that put

00:42:30,690 --> 00:42:36,720
is in into a specific queue and you have

00:42:33,599 --> 00:42:38,609
to in this specific you to us to solve

00:42:36,720 --> 00:42:41,069
the early Demark problem you have to

00:42:38,609 --> 00:42:43,109
have pre-mapped the memory in this queue

00:42:41,069 --> 00:42:44,910
this this memory that this pre mapped

00:42:43,109 --> 00:42:47,579
into this queue is also met in the user

00:42:44,910 --> 00:42:52,920
space so that's the only way user space

00:42:47,579 --> 00:42:54,480
can can do single copy received and and

00:42:52,920 --> 00:42:57,779
it's completely bypasses all of the

00:42:54,480 --> 00:42:59,640
criminal APIs that it cannot just place

00:42:57,779 --> 00:43:02,910
the only thing that happens in the

00:42:59,640 --> 00:43:04,670
driver is that it puts that into it also

00:43:02,910 --> 00:43:06,990
does the descriptive translation and

00:43:04,670 --> 00:43:09,150
ensures a memory is secure

00:43:06,990 --> 00:43:11,160
yeah and the only thing that viro does

00:43:09,150 --> 00:43:14,369
it flips a bit that says now this

00:43:11,160 --> 00:43:16,109
belongs to use of space and well mister

00:43:14,369 --> 00:43:17,849
space well more than that you have to

00:43:16,109 --> 00:43:18,839
actually write the virtual descriptors

00:43:17,849 --> 00:43:21,180
right because we don't want to end up in

00:43:18,839 --> 00:43:25,849
a scenario where every time I upgrade my

00:43:21,180 --> 00:43:29,369
hardware my application Baker yeah right

00:43:25,849 --> 00:43:31,710
so and again I think this will cover

00:43:29,369 --> 00:43:32,730
controller station because the Atty oh

00:43:31,710 --> 00:43:34,710
we are building in you

00:43:32,730 --> 00:43:39,859
this gift or so why not okay you can

00:43:34,710 --> 00:43:43,020
share maybe sure like say we get the mic

00:43:39,859 --> 00:43:45,329
here in maximum effect Alexei oh sorry

00:43:43,020 --> 00:43:51,630
okay last one yeah last one I likes you

00:43:45,329 --> 00:43:54,720
too Roni ask first I think I just want

00:43:51,630 --> 00:43:56,579
to comment on the dirt a urine whatever

00:43:54,720 --> 00:44:00,869
the future or descriptor will be

00:43:56,579 --> 00:44:04,770
invented 1.11 - in 22 I would say even

00:44:00,869 --> 00:44:07,140
if it fits the purpose if it will fit it

00:44:04,770 --> 00:44:10,200
should not be used the reason is that

00:44:07,140 --> 00:44:13,740
certain respect for the sound lifetime

00:44:10,200 --> 00:44:15,720
with its own multiple organizations

00:44:13,740 --> 00:44:21,750
dealing with it this is pure Quran

00:44:15,720 --> 00:44:24,109
networking here and and I think here we

00:44:21,750 --> 00:44:26,940
should not be like dictated by the

00:44:24,109 --> 00:44:30,750
standards and expects an organization

00:44:26,940 --> 00:44:32,369
community it's the colonel should be

00:44:30,750 --> 00:44:35,270
doing what makes sense from performance

00:44:32,369 --> 00:44:35,270
round earlier that's it

00:44:38,420 --> 00:44:46,339
and I and I think we also already have

00:44:43,250 --> 00:44:50,269
this kind of interface we use it in the

00:44:46,339 --> 00:44:52,490
InfiniBand called verbs so it's the same

00:44:50,269 --> 00:44:57,490
idea so you have this kind of generic

00:44:52,490 --> 00:44:59,299
descriptors you want to go this way or

00:44:57,490 --> 00:45:02,450
no not really

00:44:59,299 --> 00:45:05,559
I think I'm suggesting take a suggestion

00:45:02,450 --> 00:45:09,519
because because we did it we already

00:45:05,559 --> 00:45:11,869
struggle some to this generic descriptor

00:45:09,519 --> 00:45:16,809
mapping the memory all those kind of

00:45:11,869 --> 00:45:20,660
stuff a way to create rules what ring

00:45:16,809 --> 00:45:21,559
but we just do the creating the ring in

00:45:20,660 --> 00:45:25,910
the user space

00:45:21,559 --> 00:45:29,690
I think guilties give the same same

00:45:25,910 --> 00:45:31,579
value I think you should be like as he's

00:45:29,690 --> 00:45:33,500
looking at our DNA stuff and the web

00:45:31,579 --> 00:45:36,710
stuff to like see what what kind of

00:45:33,500 --> 00:45:38,299
mistakes did they do and right let's try

00:45:36,710 --> 00:45:40,789
to add like those but take the good

00:45:38,299 --> 00:45:44,180
things so I also think that it looks

00:45:40,789 --> 00:45:47,269
like a lot like a DML you met the user

00:45:44,180 --> 00:45:48,799
space memory in to a queue pair in oh

00:45:47,269 --> 00:45:51,470
yeah that's essentially so it's

00:45:48,799 --> 00:45:59,750
essentially a very similar tendencies

00:45:51,470 --> 00:46:02,690
also yeah and nothing are you

00:45:59,750 --> 00:46:05,000
considering what these those sockets are

00:46:02,690 --> 00:46:09,980
bypassing the kernel so TCP dump and all

00:46:05,000 --> 00:46:14,180
those kind of stuff or any filters will

00:46:09,980 --> 00:46:16,369
be bypassed yeah it probably should be

00:46:14,180 --> 00:46:17,720
the case I would think so you're asked

00:46:16,369 --> 00:46:19,490
about if you have a socket filter

00:46:17,720 --> 00:46:24,230
running on your AF packet but yes that

00:46:19,490 --> 00:46:25,549
you have well presumably then you should

00:46:24,230 --> 00:46:26,839
be if you wanted to port that

00:46:25,549 --> 00:46:28,910
application you should be other on that

00:46:26,839 --> 00:46:30,109
filter in the hardware because we're

00:46:28,910 --> 00:46:32,150
using the hardware is that is the

00:46:30,109 --> 00:46:34,130
pre-filter into the into the socket at

00:46:32,150 --> 00:46:35,690
this Mary yeah I'm asking it because we

00:46:34,130 --> 00:46:37,849
have some customers that's asking okay

00:46:35,690 --> 00:46:41,539
if you're using verbs when you're doing

00:46:37,849 --> 00:46:44,539
TCP dump I want to see those pockets so

00:46:41,539 --> 00:46:45,920
there are so you can set up your

00:46:44,539 --> 00:46:48,589
hardware to mirror it to another cue of

00:46:45,920 --> 00:46:51,260
you if you think that's valuable mother

00:46:48,589 --> 00:46:53,760
is that gonna walk out to the box I

00:46:51,260 --> 00:46:54,599
don't see why it was going down I just

00:46:53,760 --> 00:46:56,490
that's me Ringo

00:46:54,599 --> 00:46:59,010
well yeah no you just bring out TC and

00:46:56,490 --> 00:47:00,780
you say okay I instead of rewriting

00:46:59,010 --> 00:47:02,250
traffic just mirror it to that other key

00:47:00,780 --> 00:47:05,430
you know yeah

00:47:02,250 --> 00:47:07,829
makes sense maybe we should move on to

00:47:05,430 --> 00:47:09,329
the other locks people so they medicate

00:47:07,829 --> 00:47:11,400
sometimes it's on well I don't think

00:47:09,329 --> 00:47:13,710
this is decimation well actually I think

00:47:11,400 --> 00:47:16,440
the clock start at what 55 yeah I think

00:47:13,710 --> 00:47:17,760
this is not a fair dis match well yeah I

00:47:16,440 --> 00:47:19,170
think you guys will get some more time

00:47:17,760 --> 00:47:22,079
here than I think I think we're up to

00:47:19,170 --> 00:47:24,089
the tail so yeah just add 15 minutes on

00:47:22,079 --> 00:47:26,369
stare at it what half hour on the clock

00:47:24,089 --> 00:47:28,260
you guys still have 45 minutes against

00:47:26,369 --> 00:47:30,900
what we was promised yeah just fair

00:47:28,260 --> 00:47:35,730
keepin stairs - I meant for us 45

00:47:30,900 --> 00:47:38,970
minutes for you let's take out sign no

00:47:35,730 --> 00:47:41,700
is it not that we think the slides

00:47:38,970 --> 00:47:42,990
everybody to be laughed at Merliah think

00:47:41,700 --> 00:47:47,940
of the coffee's very good

00:47:42,990 --> 00:47:51,599
hey go thanks okay so I'll be talking

00:47:47,940 --> 00:47:55,020
about like some bulking mechanism we've

00:47:51,599 --> 00:47:58,740
been working on in both the device

00:47:55,020 --> 00:48:03,750
driver and or hardware I'll be

00:47:58,740 --> 00:48:05,760
presenting the arts bulking in the

00:48:03,750 --> 00:48:09,810
device driver level which is made for

00:48:05,760 --> 00:48:14,130
like more for its VP purposes with the

00:48:09,810 --> 00:48:18,720
with the huge the P programs that does

00:48:14,130 --> 00:48:21,240
intermix decisions then we will move on

00:48:18,720 --> 00:48:24,800
to our X byte streaming which is riding

00:48:21,240 --> 00:48:27,210
our cue what we have an over Hardware

00:48:24,800 --> 00:48:30,839
after that we will move to a different

00:48:27,210 --> 00:48:35,190
topic which is also bulking but 40 X + +

00:48:30,839 --> 00:48:39,329
TX Dobrin batching and device driver and

00:48:35,190 --> 00:48:43,500
cakes bulking descriptor bulking which

00:48:39,329 --> 00:48:52,170
is a multi packet descriptor in the

00:48:43,500 --> 00:48:56,910
hardware level so why are bulking why do

00:48:52,170 --> 00:48:58,680
we need this today as Alex showed we

00:48:56,910 --> 00:49:01,910
have a lot of steps we do for every

00:48:58,680 --> 00:49:03,950
every packet inside the hardware and

00:49:01,910 --> 00:49:05,990
and every packet we get from the

00:49:03,950 --> 00:49:08,180
hardware and we do lots of steps and

00:49:05,990 --> 00:49:11,030
that thing we do that to the stack and

00:49:08,180 --> 00:49:17,420
we would like to bear to those steps

00:49:11,030 --> 00:49:21,559
into into some stages and move data into

00:49:17,420 --> 00:49:24,250
a pipeline in the our expect and are

00:49:21,559 --> 00:49:29,210
then best multiple packets to the stack

00:49:24,250 --> 00:49:35,569
which should do the work for our a use

00:49:29,210 --> 00:49:38,539
case community P is loaded so the stages

00:49:35,569 --> 00:49:42,200
are going to do like first thing we will

00:49:38,539 --> 00:49:45,819
need to data and our descriptor early

00:49:42,200 --> 00:49:48,650
prefetch so we can reduce a cache method

00:49:45,819 --> 00:49:51,079
which is today what is what helps the

00:49:48,650 --> 00:49:57,069
driver the most in the first data cache

00:49:51,079 --> 00:49:59,329
miss we hit an air pocket another

00:49:57,069 --> 00:50:01,910
motivation is that we also utilize

00:49:59,329 --> 00:50:04,480
instruction cache as I said for large

00:50:01,910 --> 00:50:07,880
EBP programs

00:50:04,480 --> 00:50:10,520
it seemed theoretically because this

00:50:07,880 --> 00:50:12,559
still under development but

00:50:10,520 --> 00:50:16,250
theoretically when you have a large VP

00:50:12,559 --> 00:50:19,010
program that for example drops every

00:50:16,250 --> 00:50:23,150
second packet and passes every first

00:50:19,010 --> 00:50:24,619
packet this way that the codes receive

00:50:23,150 --> 00:50:28,789
pass will move back and forth between

00:50:24,619 --> 00:50:31,369
the colonel and xtp code and the part is

00:50:28,789 --> 00:50:33,680
that that we we have to read out

00:50:31,369 --> 00:50:37,069
instruction cast for the year yeah BPA

00:50:33,680 --> 00:50:41,089
program does run as yeah so the idea is

00:50:37,069 --> 00:50:42,770
to bulk the VP program to run and we

00:50:41,089 --> 00:50:46,609
know that exhibit program is going to

00:50:42,770 --> 00:50:49,119
drop like specific amount of packets

00:50:46,609 --> 00:50:52,609
will drop them then we'll go handle the

00:50:49,119 --> 00:50:56,990
other packets that are going into the

00:50:52,609 --> 00:51:00,349
stack so this is a motivation on today

00:50:56,990 --> 00:51:04,839
as we already said this is what we have

00:51:00,349 --> 00:51:07,960
in Napa loop we do some previous some

00:51:04,839 --> 00:51:11,060
twitching of the RF descriptor and

00:51:07,960 --> 00:51:15,810
conclusion in in case of our hardware

00:51:11,060 --> 00:51:20,520
we fit the data I think the DMA running

00:51:15,810 --> 00:51:22,920
CP program building is KB and populate

00:51:20,520 --> 00:51:26,790
the SK BCC field and then back to the

00:51:22,920 --> 00:51:32,280
stack which is God's led today and we

00:51:26,790 --> 00:51:35,010
would like to do some bulking and the

00:51:32,280 --> 00:51:38,040
problem that we have as I explained that

00:51:35,010 --> 00:51:39,390
we always had a cache miss at the

00:51:38,040 --> 00:51:41,820
beginning of the pitching of the

00:51:39,390 --> 00:51:45,840
descriptor itself and the data pointers

00:51:41,820 --> 00:51:50,190
that this descriptor points to again

00:51:45,840 --> 00:51:55,440
also instruction cache invalidation when

00:51:50,190 --> 00:51:59,280
we have like logic VP programs so the

00:51:55,440 --> 00:52:02,400
idea is to break the are expressed in

00:51:59,280 --> 00:52:06,090
terms like those in general of those

00:52:02,400 --> 00:52:10,020
three steps that we have an experimental

00:52:06,090 --> 00:52:13,230
code right now we fit in hacking a loop

00:52:10,020 --> 00:52:16,170
like we can fit a predetermined amount

00:52:13,230 --> 00:52:20,910
of packets or descriptors into some

00:52:16,170 --> 00:52:26,000
temporary array in the workspace then

00:52:20,910 --> 00:52:30,690
pre fit the data itself d may think and

00:52:26,000 --> 00:52:33,780
and feed this temporary completion

00:52:30,690 --> 00:52:36,270
element into the next pages which is the

00:52:33,780 --> 00:52:38,610
AP program first of all store the

00:52:36,270 --> 00:52:41,040
decision then run the decision like

00:52:38,610 --> 00:52:45,990
first of all we do stupid Rob and XT DT

00:52:41,040 --> 00:52:48,480
X which is still inside the driver code

00:52:45,990 --> 00:52:53,160
which is small relative to the x PP code

00:52:48,480 --> 00:52:56,370
and the kernel code and then take all

00:52:53,160 --> 00:53:04,740
those skb that we lift with and passing

00:52:56,370 --> 00:53:08,010
tests as bad and yet this is the idea so

00:53:04,740 --> 00:53:11,220
there are as I said this is like still

00:53:08,010 --> 00:53:17,450
in progress a and there are some risks

00:53:11,220 --> 00:53:19,560
and it's been also a very problematic

00:53:17,450 --> 00:53:21,960
implementation that I've been

00:53:19,560 --> 00:53:23,089
experiencing because every change to the

00:53:21,960 --> 00:53:27,799
code like

00:53:23,089 --> 00:53:29,599
today we have both known steps that you

00:53:27,799 --> 00:53:32,140
need to do and once you start tracking

00:53:29,599 --> 00:53:36,140
them start hitting new cache misses and

00:53:32,140 --> 00:53:40,099
you go back to data that you already pre

00:53:36,140 --> 00:53:44,089
fit and start experiencing new cache

00:53:40,099 --> 00:53:49,130
misses that you need to handle by fine

00:53:44,089 --> 00:53:52,279
fine tuning the code one more thing that

00:53:49,130 --> 00:53:55,069
the prefetch is without xpp program this

00:53:52,279 --> 00:53:58,519
big city program we don't experience a

00:53:55,069 --> 00:54:01,900
very huge or ever will experience a

00:53:58,519 --> 00:54:03,559
little gain and in the data path so

00:54:01,900 --> 00:54:07,099
without xvp

00:54:03,559 --> 00:54:11,900
we won't feel a theoretically we won't

00:54:07,099 --> 00:54:14,779
feel any improvement data that's because

00:54:11,900 --> 00:54:21,499
HTTP to charge the first cache line much

00:54:14,779 --> 00:54:25,880
earlier than yeah right so we we need to

00:54:21,499 --> 00:54:29,359
like Crispus code and see how how it is

00:54:25,880 --> 00:54:31,729
how it is how it works in the common

00:54:29,359 --> 00:54:34,279
case and xtp case and most of the cases

00:54:31,729 --> 00:54:37,249
that we need so and there are some

00:54:34,279 --> 00:54:40,329
questions to be be raised like how many

00:54:37,249 --> 00:54:42,619
prefix do we need because the pre-fitted

00:54:40,329 --> 00:54:45,289
but cache line dependent and

00:54:42,619 --> 00:54:47,359
architecture dependent so we need to go

00:54:45,289 --> 00:54:53,269
in at least we know in advance and to

00:54:47,359 --> 00:54:56,269
have some numbers also for the common

00:54:53,269 --> 00:55:00,640
stack do we nee need to like have

00:54:56,269 --> 00:55:04,719
different arcs best paths for the stack

00:55:00,640 --> 00:55:08,420
delivery and when you load is VP program

00:55:04,719 --> 00:55:13,119
have the staging running and when you

00:55:08,420 --> 00:55:19,849
don't have XDP program run the booklet

00:55:13,119 --> 00:55:22,009
or Express and we those are questions to

00:55:19,849 --> 00:55:25,160
be raised and this is work in progress

00:55:22,009 --> 00:55:26,569
so we are going to do a lot of testing a

00:55:25,160 --> 00:55:29,900
lot of measurements

00:55:26,569 --> 00:55:33,679
many architectures and many kinds and

00:55:29,900 --> 00:55:35,029
types of workloads I think will be like

00:55:33,679 --> 00:55:37,160
a test-driven development

00:55:35,029 --> 00:55:39,229
yeah sure that they are introduced in a

00:55:37,160 --> 00:55:40,849
progressions right so this idea this is

00:55:39,229 --> 00:55:45,499
a plan this is what we are going to do

00:55:40,849 --> 00:55:49,150
and see where it goes I want to share

00:55:45,499 --> 00:55:55,179
some early results that we have with the

00:55:49,150 --> 00:55:55,179
with the current VP benchmarks and

00:55:55,269 --> 00:56:02,269
without with my early patches I end with

00:55:59,839 --> 00:56:04,699
some fine tuning tuning to that driver

00:56:02,269 --> 00:56:09,489
to solve the cache misses that I

00:56:04,699 --> 00:56:13,849
introduced with the arcs Oryx batching

00:56:09,489 --> 00:56:16,969
we see that a 10% improvement for VP TX

00:56:13,849 --> 00:56:19,519
because with the data touch because the

00:56:16,969 --> 00:56:23,089
prefetching works slightly better than

00:56:19,519 --> 00:56:25,819
the four and one to two percent for

00:56:23,089 --> 00:56:28,009
exhibit drop and step delivery because

00:56:25,819 --> 00:56:33,949
the GP program so small that you cannot

00:56:28,009 --> 00:56:36,769
just you can't see the improvement so

00:56:33,949 --> 00:56:39,769
that's it for now as I said this is work

00:56:36,769 --> 00:56:42,499
in progress I think in next- you will

00:56:39,769 --> 00:56:46,420
have some results and some wide

00:56:42,499 --> 00:56:52,339
measurements and more performance number

00:56:46,420 --> 00:56:55,670
so any questions okay I'll pass to torak

00:56:52,339 --> 00:57:03,609
here is the the main implementer of the

00:56:55,670 --> 00:57:08,569
hardware liver Alex bulking thank you so

00:57:03,609 --> 00:57:13,309
and I will talk about are clocking

00:57:08,569 --> 00:57:19,279
capability we have in our Hardware the

00:57:13,309 --> 00:57:21,609
feature name is striding our queue so

00:57:19,279 --> 00:57:25,009
first I will start with the

00:57:21,609 --> 00:57:29,929
disadvantages of using the conventional

00:57:25,009 --> 00:57:35,059
arc showing this is our motivation so in

00:57:29,929 --> 00:57:35,920
the conventional or extreme each it's

00:57:35,059 --> 00:57:39,690
hard

00:57:35,920 --> 00:57:43,210
a packet we have a Harvard descriptor

00:57:39,690 --> 00:57:48,910
and for each harvest hundred descriptor

00:57:43,210 --> 00:57:52,330
whom we do to PCI transactions one to

00:57:48,910 --> 00:57:58,500
read the description and other to write

00:57:52,330 --> 00:58:02,910
a packet to to select in the in the memo

00:57:58,500 --> 00:58:06,700
another thing that we should be ready to

00:58:02,910 --> 00:58:08,200
receive the packets of any size so the

00:58:06,700 --> 00:58:13,180
buffer size should be the largest

00:58:08,200 --> 00:58:15,820
possible which is until and this is a

00:58:13,180 --> 00:58:23,740
very wasteful when the actual occupying

00:58:15,820 --> 00:58:29,080
packets are very smooth so the memory

00:58:23,740 --> 00:58:33,820
model of the conventional arts ring like

00:58:29,080 --> 00:58:37,740
this we have the buffers and let's say

00:58:33,820 --> 00:58:41,800
that the small packets are flying in

00:58:37,740 --> 00:58:47,010
each packet goes into the its buffer you

00:58:41,800 --> 00:58:51,670
can see for example that we use out of

00:58:47,010 --> 00:58:55,570
the large NQ allocated memory we use

00:58:51,670 --> 00:59:02,800
only small small amount and all the rest

00:58:55,570 --> 00:59:06,030
is just wasted and that is we want to

00:59:02,800 --> 00:59:06,030
want to avoid that

00:59:08,079 --> 00:59:15,179
so starting to argue with the we

00:59:11,140 --> 00:59:18,069
recalled a bike screen buffering and

00:59:15,179 --> 00:59:23,589
starting argue we do batch for the

00:59:18,069 --> 00:59:27,189
Harvard descriptors we call every every

00:59:23,589 --> 00:59:31,719
entry multi packet lookie-lookie for

00:59:27,189 --> 00:59:37,089
work you element so every element can

00:59:31,719 --> 00:59:42,069
serve multiple packets and each each

00:59:37,089 --> 00:59:45,729
buffer can serve we allocate buffers of

00:59:42,069 --> 00:59:51,880
larger sizes not into you but much much

00:59:45,729 --> 00:59:56,319
larger typically it's about 256

00:59:51,880 --> 01:00:00,130
kilobytes per descriptor the buffers

00:59:56,319 --> 01:00:04,359
themselves exist of order 0 pages so we

01:00:00,130 --> 01:00:10,319
are fine with without disadvantages of

01:00:04,359 --> 01:00:13,329
using high order pages that yester 11th

01:00:10,319 --> 01:00:18,849
and the hardware knows to write the

01:00:13,329 --> 01:00:21,579
receive packets continuously so of

01:00:18,849 --> 01:00:27,630
course with an alignment because we a

01:00:21,579 --> 01:00:33,339
bit care about performance and let's say

01:00:27,630 --> 01:00:36,699
a team of small packets are flying in so

01:00:33,339 --> 01:00:39,549
they will both the small packets will go

01:00:36,699 --> 01:00:43,059
into the same buffer one after the other

01:00:39,549 --> 01:00:48,519
and the whole buffer is still available

01:00:43,059 --> 01:00:53,709
for up coming here packets so this way

01:00:48,519 --> 01:00:56,920
we we save memory which means for the

01:00:53,709 --> 01:01:03,549
same language we ask you know for before

01:00:56,920 --> 01:01:05,890
less pages educator so actually I have a

01:01:03,549 --> 01:01:07,359
question yes so is there no padding at

01:01:05,890 --> 01:01:08,769
all being added on these or are you

01:01:07,359 --> 01:01:12,670
doing some sort of padding to at least

01:01:08,769 --> 01:01:15,999
align things we do align but they're not

01:01:12,670 --> 01:01:19,049
we don't for whom I will talk about this

01:01:15,999 --> 01:01:19,049
next we're gonna do

01:01:19,640 --> 01:01:28,110
okay just hardware element you mean

01:01:24,930 --> 01:01:29,580
you're siding with Becket's or yeah and

01:01:28,110 --> 01:01:30,930
just you know there's talking about

01:01:29,580 --> 01:01:32,670
these rights from one to the next thing

01:01:30,930 --> 01:01:34,890
okay well there's got to be some padding

01:01:32,670 --> 01:01:36,630
there otherwise no there is alignment is

01:01:34,890 --> 01:01:40,920
I don't have a charge a can

01:01:36,630 --> 01:01:43,170
okay so I write define destroy time okay

01:01:40,920 --> 01:01:47,220
right you destroy typically you have

01:01:43,170 --> 01:01:49,230
this right size is 64 bytes and I could

01:01:47,220 --> 01:01:51,210
to consume number of cash right into a

01:01:49,230 --> 01:01:53,430
packet contains a number of stride yes

01:01:51,210 --> 01:01:55,800
and the next attitude and right with the

01:01:53,430 --> 01:01:57,170
next branch line extra we have miss

01:01:55,800 --> 01:02:02,030
Lyman

01:01:57,170 --> 01:02:05,970
okay so among the advantages is that we

01:02:02,030 --> 01:02:08,310
reach that transactions are the Picard

01:02:05,970 --> 01:02:13,620
transaction Teresa article script or is

01:02:08,310 --> 01:02:21,660
now done only once for a large multi

01:02:13,620 --> 01:02:29,580
package and not packet we we waste much

01:02:21,660 --> 01:02:34,230
less memory the DMA rice are we get

01:02:29,580 --> 01:02:36,420
higher locality in the DNA right and we

01:02:34,230 --> 01:02:44,220
were not directly affected by thank you

01:02:36,420 --> 01:02:47,480
sighs so another thing is that we that

01:02:44,220 --> 01:02:51,840
the in Hardware we we have the higher

01:02:47,480 --> 01:02:56,310
higher limit for 4q for the packet rate

01:02:51,840 --> 01:03:00,450
we have a processing during 15 million

01:02:56,310 --> 01:03:08,900
packet per second while we had 30 min

01:03:00,450 --> 01:03:12,390
with the other type of party we still

01:03:08,900 --> 01:03:17,910
still have issues where we will chance

01:03:12,390 --> 01:03:20,730
to sit English with HDPE we will not fit

01:03:17,910 --> 01:03:25,100
with the requirements of page per packet

01:03:20,730 --> 01:03:27,970
or linear risky because there is

01:03:25,100 --> 01:03:33,550
nonlinear recipes

01:03:27,970 --> 01:03:36,640
we will take this as future war we are

01:03:33,550 --> 01:03:40,770
also considering if you were

01:03:36,640 --> 01:03:45,160
enhancements in hardware for example

01:03:40,770 --> 01:03:48,450
currently we have a packet here I might

01:03:45,160 --> 01:03:52,780
much cross the page in the page boundary

01:03:48,450 --> 01:03:57,040
so we would like to avoid that and then

01:03:52,780 --> 01:04:04,860
keep each packet each skb for a packet

01:03:57,040 --> 01:04:08,230
with with with my one one friend and

01:04:04,860 --> 01:04:13,090
also later would like to move to linear

01:04:08,230 --> 01:04:17,110
cv so we had the hardware capability to

01:04:13,090 --> 01:04:21,610
reserve some room for head and tail so

01:04:17,110 --> 01:04:28,690
we could build escapee over the our seed

01:04:21,610 --> 01:04:34,480
packet performance we have performed

01:04:28,690 --> 01:04:42,400
numbers we have the I was single TCP

01:04:34,480 --> 01:04:45,340
stream we so I good good game and with

01:04:42,400 --> 01:04:48,870
the multi stream we we were hitting line

01:04:45,340 --> 01:04:52,390
rate and we still a triage so also

01:04:48,870 --> 01:04:57,490
packet rate is higher and we were still

01:04:52,390 --> 01:05:02,910
optimizing this another measure we did

01:04:57,490 --> 01:05:07,260
was sending bursts of small packet and

01:05:02,910 --> 01:05:11,020
because we are our butters are by screen

01:05:07,260 --> 01:05:15,850
hey they can handle more packet without

01:05:11,020 --> 01:05:20,020
without grouping so the conventional

01:05:15,850 --> 01:05:23,710
argue you could see drops

01:05:20,020 --> 01:05:28,870
starting from even for a burst of 2k

01:05:23,710 --> 01:05:32,200
packet talking about small packets but

01:05:28,870 --> 01:05:36,670
with starting argue even for 32 kilo

01:05:32,200 --> 01:05:38,310
packets we see no jobs only starting

01:05:36,670 --> 01:05:40,020
from 64

01:05:38,310 --> 01:05:43,080
just eaten I want to comment on that

01:05:40,020 --> 01:05:45,390
that's justice because you now now you

01:05:43,080 --> 01:05:49,110
sort of compress the packet soup so you

01:05:45,390 --> 01:05:51,410
get you can handle a lot more for the

01:05:49,110 --> 01:05:53,340
same amount of memory literally yeah

01:05:51,410 --> 01:05:57,170
before you map out the rings you have

01:05:53,340 --> 01:06:00,810
that a thousand increasingly rings you

01:05:57,170 --> 01:06:02,280
right and only a thousand Tigers could

01:06:00,810 --> 01:06:04,080
be there but but now because you

01:06:02,280 --> 01:06:06,030
actually only use the space you really

01:06:04,080 --> 01:06:08,460
need you can actually handle small small

01:06:06,030 --> 01:06:10,680
plates so you can fit a lot a lot more

01:06:08,460 --> 01:06:13,050
than a thousand small packets in this

01:06:10,680 --> 01:06:15,060
type of Q which is which is really nice

01:06:13,050 --> 01:06:16,920
that near thing I'm probably a much more

01:06:15,060 --> 01:06:18,600
efficient PC I cut imagine you get the

01:06:16,920 --> 01:06:20,160
probably writing 64 byte packets

01:06:18,600 --> 01:06:22,140
probably like four or five or no or the

01:06:20,160 --> 01:06:23,610
eight at a time since if you're doing a

01:06:22,140 --> 01:06:25,590
streaming DMA it's like okay four

01:06:23,610 --> 01:06:28,890
packets in the single five a pair 256

01:06:25,590 --> 01:06:30,120
byte right on the PCI bus is going to be

01:06:28,890 --> 01:06:34,350
much more efficient than doing for

01:06:30,120 --> 01:06:36,510
single 64 byte or 65 copies so yeah

01:06:34,350 --> 01:06:39,180
that's also I'm also seeing I'm hitting

01:06:36,510 --> 01:06:41,370
limits on the PGI iPhone and so so that

01:06:39,180 --> 01:06:45,350
this looks basically also have 11 pitch

01:06:41,370 --> 01:06:45,350
is because to limit the load on them

01:06:48,380 --> 01:07:00,510
well as far as well yeah Thunder good so

01:06:58,500 --> 01:07:02,040
so how does the DMA on map even work for

01:07:00,510 --> 01:07:04,830
your buffers

01:07:02,040 --> 01:07:09,300
I'm just by the way from Intel only

01:07:04,830 --> 01:07:13,350
after the whole buffer is consumed we do

01:07:09,300 --> 01:07:18,360
the yet they have a bug I've informed

01:07:13,350 --> 01:07:20,850
them up in a while ago a bug to the

01:07:18,360 --> 01:07:24,690
starting our to forward to the PKS which

01:07:20,850 --> 01:07:26,310
is very good okay yeah so in starting

01:07:24,690 --> 01:07:33,270
our queue we have like only eight

01:07:26,310 --> 01:07:36,150
descriptors each pointing to 180 extra

01:07:33,270 --> 01:07:39,180
necessity for 50 56 kilobytes

01:07:36,150 --> 01:07:41,130
okay now having to copy and right there

01:07:39,180 --> 01:07:43,020
all mapped wants you allocate this

01:07:41,130 --> 01:07:46,890
descriptor and done map once you consume

01:07:43,020 --> 01:07:48,720
all this is kept oh right yeah but I

01:07:46,890 --> 01:07:50,400
think that that's really the same thing

01:07:48,720 --> 01:07:51,480
as regular page reuse because they can't

01:07:50,400 --> 01:07:53,430
actually use it

01:07:51,480 --> 01:07:55,380
filled skb or anything like that because

01:07:53,430 --> 01:07:57,240
they don't have any Headroom or any that

01:07:55,380 --> 01:07:58,320
or children yeah so they're just doing

01:07:57,240 --> 01:07:59,880
the same thing we were doing in the

01:07:58,320 --> 01:08:00,960
recycling so they would copy off the

01:07:59,880 --> 01:08:02,910
header and then this offended as a

01:08:00,960 --> 01:08:06,240
read-only page once another script was

01:08:02,910 --> 01:08:08,820
fully consumed we can just map it we use

01:08:06,240 --> 01:08:10,410
like reference counts for that like the

01:08:08,820 --> 01:08:16,020
one Alex talked about

01:08:10,410 --> 01:08:19,109
so once the Earth count hits back to its

01:08:16,020 --> 01:08:21,569
consumed we unknot the page or the page

01:08:19,109 --> 01:08:23,700
we don't even anybody can see everything

01:08:21,569 --> 01:08:25,739
I think what what gets done map a

01:08:23,700 --> 01:08:27,359
regular page yeah that you're using like

01:08:25,739 --> 01:08:29,670
what I'm talking about the DMA sink on

01:08:27,359 --> 01:08:31,350
the sink for CPU yeah it worked on those

01:08:29,670 --> 01:08:33,239
are the small sizes or whatever right I

01:08:31,350 --> 01:08:36,089
think I think I think Jessica's the

01:08:33,239 --> 01:08:37,830
point is that that that the page has to

01:08:36,089 --> 01:08:39,960
be considered read out in the air I

01:08:37,830 --> 01:08:41,400
think yeah you've yeah so so what what

01:08:39,960 --> 01:08:43,380
they do they I look at a separate area

01:08:41,400 --> 01:08:46,799
copy or the hitters and do all the setup

01:08:43,380 --> 01:08:49,200
right and put put it the page data which

01:08:46,799 --> 01:08:51,569
is considered read-only dominant right

01:08:49,200 --> 01:08:53,339
and then as long as the page count is

01:08:51,569 --> 01:08:55,799
greater than one exactly read over yeah

01:08:53,339 --> 01:08:57,000
yeah yeah they can unwrap it safely

01:08:55,799 --> 01:09:01,319
while they're still holding at least one

01:08:57,000 --> 01:09:03,930
reference yeah it'll be more critical

01:09:01,319 --> 01:09:06,420
because the slides you mentioned that

01:09:03,930 --> 01:09:07,790
you wanted to use build skp does not you

01:09:06,420 --> 01:09:11,130
cannot do that right now

01:09:07,790 --> 01:09:13,020
yeah does a future improve we have to do

01:09:11,130 --> 01:09:15,900
and then you have to make sure you use

01:09:13,020 --> 01:09:17,339
Alex's that the amazing API yeah that's

01:09:15,900 --> 01:09:18,750
if you get to the point where you get

01:09:17,339 --> 01:09:19,920
build skb I'm guessing that's gonna be a

01:09:18,750 --> 01:09:21,630
little while yet you have to add

01:09:19,920 --> 01:09:23,250
Headroom and kill rooms yeah yeah I

01:09:21,630 --> 01:09:24,799
think it's just everything somehow that

01:09:23,250 --> 01:09:28,109
seems just right don't you

01:09:24,799 --> 01:09:31,830
okay so I just want to mention that at

01:09:28,109 --> 01:09:33,690
the hyper-v receive area is pretty much

01:09:31,830 --> 01:09:36,000
the same way with our end s we have a

01:09:33,690 --> 01:09:38,940
big it's bigger just one big chunk of

01:09:36,000 --> 01:09:41,430
memory stuff gets popped in there and

01:09:38,940 --> 01:09:43,440
right now we have to copy it out we

01:09:41,430 --> 01:09:45,060
there's been some experimental patches

01:09:43,440 --> 01:09:46,950
to do copies receipt but we have the

01:09:45,060 --> 01:09:49,410
same issues and the same issues with

01:09:46,950 --> 01:09:51,569
supporting STP where we have a shared

01:09:49,410 --> 01:09:53,430
area where we want to write some here's

01:09:51,569 --> 01:09:56,100
some here's a packet run your program

01:09:53,430 --> 01:09:57,690
but by the way you know it's not you

01:09:56,100 --> 01:10:00,719
don't really own the whole page you

01:09:57,690 --> 01:10:02,820
write that part of the packet yeah and

01:10:00,719 --> 01:10:03,489
if we want x DP to be general it's got

01:10:02,820 --> 01:10:04,630
to do that

01:10:03,489 --> 01:10:09,160
we can all live some of those

01:10:04,630 --> 01:10:10,780
restrictions was that the thing is right

01:10:09,160 --> 01:10:11,830
now one of the big things is XDP is it's

01:10:10,780 --> 01:10:13,420
the fact that you can go in and actually

01:10:11,830 --> 01:10:14,890
modify aspects of it and that's one of

01:10:13,420 --> 01:10:16,420
the main uses well the other thing is

01:10:14,890 --> 01:10:19,719
you can let people modify as long as

01:10:16,420 --> 01:10:22,600
you're willing to say you can rot easier

01:10:19,719 --> 01:10:23,980
you say there's if your mother you if

01:10:22,600 --> 01:10:27,219
you want to shoot yourself in the foot

01:10:23,980 --> 01:10:31,080
can or if you say you could give some

01:10:27,219 --> 01:10:34,810
brains restrictions well that's so like

01:10:31,080 --> 01:10:36,280
other than so there's the DMA API issue

01:10:34,810 --> 01:10:38,410
which we if you use the correct

01:10:36,280 --> 01:10:40,000
attribute now that's not a problem so

01:10:38,410 --> 01:10:42,730
was there something else within a

01:10:40,000 --> 01:10:45,010
graffiti that was making it so that you

01:10:42,730 --> 01:10:47,530
couldn't change the region once it's

01:10:45,010 --> 01:10:50,170
been given to the driver no there's not

01:10:47,530 --> 01:10:52,000
I mean you can't you'd have to go give

01:10:50,170 --> 01:10:53,520
another region but that's another you

01:10:52,000 --> 01:10:56,230
don't want to take that it's expensive

01:10:53,520 --> 01:10:57,870
okay how about something else we can

01:10:56,230 --> 01:11:00,910
talk about offline

01:10:57,870 --> 01:11:03,010
okay just another question about the

01:11:00,910 --> 01:11:08,260
striding don't you have like a massive

01:11:03,010 --> 01:11:08,830
true side slash like memory pinning

01:11:08,260 --> 01:11:11,140
problem

01:11:08,830 --> 01:11:14,020
they say chunk it up that's if I'm not

01:11:11,140 --> 01:11:15,190
mistaken right so yeah it's the same

01:11:14,020 --> 01:11:17,440
kind of thing we do with like so right

01:11:15,190 --> 01:11:20,320
now as you take a look at the Esk bus

01:11:17,440 --> 01:11:22,480
allocator it ends up using a 32 K chunk

01:11:20,320 --> 01:11:24,280
and the only verse says okay I only use

01:11:22,480 --> 01:11:25,630
you know X number of cache lines I'm

01:11:24,280 --> 01:11:28,750
gonna report that in terms of my size

01:11:25,630 --> 01:11:30,310
for true size right that's kind of fine

01:11:28,750 --> 01:11:32,620
right but if you have like this big page

01:11:30,310 --> 01:11:35,050
and you're pinning one TTP segment in

01:11:32,620 --> 01:11:37,719
there you're still pinning like this

01:11:35,050 --> 01:11:43,930
whole memory yeah yeah but still the it

01:11:37,719 --> 01:11:46,210
has a use order zero pages so all the

01:11:43,930 --> 01:11:50,260
other pages are released no kind of town

01:11:46,210 --> 01:11:51,700
that does I yeah I still think like

01:11:50,260 --> 01:11:54,730
you're cheating a little bit on the true

01:11:51,700 --> 01:11:57,880
size matter Rwanda everyone does and

01:11:54,730 --> 01:12:04,800
every case obsess with us every time

01:11:57,880 --> 01:12:07,360
and give you a little yeah yes okay so

01:12:04,800 --> 01:12:12,040
I'm imagining that was the starting

01:12:07,360 --> 01:12:17,560
argue next we'll talk about dicks

01:12:12,040 --> 01:12:22,900
bulking the idea that is to be

01:12:17,560 --> 01:12:25,510
implemented in software this time it was

01:12:22,900 --> 01:12:29,830
actually those actually this just

01:12:25,510 --> 01:12:34,719
recently you know now in a male straight

01:12:29,830 --> 01:12:43,739
I'm wearing this so start with a

01:12:34,719 --> 01:12:46,389
motivation and it's the like extreme you

01:12:43,739 --> 01:12:48,130
you should notify the hardware whenever

01:12:46,389 --> 01:12:51,760
you have a new package ready for

01:12:48,130 --> 01:12:57,480
transmission this not fire Hardware

01:12:51,760 --> 01:13:01,270
operations called doorbells are costly

01:12:57,480 --> 01:13:04,840
these include the barriers include PCI

01:13:01,270 --> 01:13:10,360
operations so we would like to save that

01:13:04,840 --> 01:13:13,960
as much as we can and we would like to

01:13:10,360 --> 01:13:18,219
do it once per a bad chest expects and

01:13:13,960 --> 01:13:24,179
not for every year in fact it there's an

01:13:18,219 --> 01:13:28,420
existing years solution a one Smith Mort

01:13:24,179 --> 01:13:32,739
William field in the STB and it

01:13:28,420 --> 01:13:36,400
indicates where if another packet is to

01:13:32,739 --> 01:13:39,190
follow and this way the driver knows not

01:13:36,400 --> 01:13:45,520
issue adorable but wait for the next

01:13:39,190 --> 01:13:50,440
night well we we took this to this idea

01:13:45,520 --> 01:13:54,270
and we want to do a transparent Smith

01:13:50,440 --> 01:13:57,610
Morrill Act mechanism and driver level

01:13:54,270 --> 01:14:01,780
we would like to see if the Laurel if we

01:13:57,610 --> 01:14:08,350
know that that each Napa completion is

01:14:01,780 --> 01:14:10,900
expected soon and when when the Napa

01:14:08,350 --> 01:14:14,780
context

01:14:10,900 --> 01:14:19,750
run only then we issue a single doorbell

01:14:14,780 --> 01:14:19,750
for all the heat accumulated in the ring

01:14:22,480 --> 01:14:33,770
well I think a very crazy idea and the

01:14:30,800 --> 01:14:37,240
mailing list and the you also given a

01:14:33,770 --> 01:14:41,690
page for Mellanox for Iver

01:14:37,240 --> 01:14:47,000
he s also gain of about seventy five

01:14:41,690 --> 01:14:50,600
percent X I'm not mistaken so I talked

01:14:47,000 --> 01:14:57,470
to this and they wrote a patch for Mad

01:14:50,600 --> 01:15:00,650
Max 5 over the Nexus 5 and and I see as

01:14:57,470 --> 01:15:07,540
a proof of concept we see again we can

01:15:00,650 --> 01:15:11,840
can get an packet rate still we have

01:15:07,540 --> 01:15:16,010
many serious challenges first it's about

01:15:11,840 --> 01:15:16,640
synchronization who should the ring the

01:15:16,010 --> 01:15:19,790
doorbell

01:15:16,640 --> 01:15:21,710
we should synchronize between the indio

01:15:19,790 --> 01:15:26,480
context the engine starting Schmitt and

01:15:21,710 --> 01:15:31,430
an epic contest maybe do a hardware

01:15:26,480 --> 01:15:35,990
level exploring another challenge is

01:15:31,430 --> 01:15:41,480
that the waiting delays are now very

01:15:35,990 --> 01:15:43,730
violent might be less expected solutions

01:15:41,480 --> 01:15:51,310
might be adapted to interrupt moderation

01:15:43,730 --> 01:15:57,140
please or fix the polling that Rick

01:15:51,310 --> 01:16:02,630
talked about earlier today so we are

01:15:57,140 --> 01:16:05,410
this is a still under investment in

01:16:02,630 --> 01:16:05,410
destination server

01:16:09,210 --> 01:16:17,210
that's it for the tEEX walking in and

01:16:12,810 --> 01:16:21,420
driver level next I mean we talked about

01:16:17,210 --> 01:16:26,820
walking in hardwood okay that's the

01:16:21,420 --> 01:16:30,380
green one yeah maybe maybe some people

01:16:26,820 --> 01:16:33,870
have some questions about it I'm sure

01:16:30,380 --> 01:16:36,120
because it is a little bit controversial

01:16:33,870 --> 01:16:42,210
podcast we can introduce added latency

01:16:36,120 --> 01:16:45,270
right and the transmitter yes we the

01:16:42,210 --> 01:16:48,690
latency but you get two more packet

01:16:45,270 --> 01:16:51,060
right so you know yeah oh yeah that's a

01:16:48,690 --> 01:16:53,040
thing that this is firstly I cannot made

01:16:51,060 --> 01:16:54,810
something similar a few years back and

01:16:53,040 --> 01:16:56,790
so yeah that was the number one argument

01:16:54,810 --> 01:16:58,680
is it's you're adding extra latency it's

01:16:56,790 --> 01:17:00,030
going to be the fact that basically

01:16:58,680 --> 01:17:01,590
you're not transmitting anything from

01:17:00,030 --> 01:17:03,960
the time you submit that first packet

01:17:01,590 --> 01:17:08,130
until you re-enable the cutest resume

01:17:03,960 --> 01:17:10,110
transmitted again so ideally if there is

01:17:08,130 --> 01:17:12,690
a way for us to identify Windows where

01:17:10,110 --> 01:17:14,190
we start to exceed like it's nothing

01:17:12,690 --> 01:17:15,600
else you may want to look at instead of

01:17:14,190 --> 01:17:17,310
trying to find a way to just

01:17:15,600 --> 01:17:20,400
automatically do on the first packet

01:17:17,310 --> 01:17:22,050
somehow track the flow itself - okay the

01:17:20,400 --> 01:17:24,090
flow now is going to switch to bulking

01:17:22,050 --> 01:17:25,380
instead of automatically just you know

01:17:24,090 --> 01:17:29,010
first time it comes in and okay we're

01:17:25,380 --> 01:17:32,490
setting the there's a real stop and yeah

01:17:29,010 --> 01:17:34,230
it is that's to the full duration we

01:17:32,490 --> 01:17:35,990
were trying to do I think we have to

01:17:34,230 --> 01:17:40,460
play like thought about a single engine

01:17:35,990 --> 01:17:40,460
takes into a motivation where are you

01:17:40,490 --> 01:17:44,220
all right that's the thing is basically

01:17:42,630 --> 01:17:46,950
we're introducing our expectancy on top

01:17:44,220 --> 01:17:48,540
of the TX latency so yeah anyway yeah

01:17:46,950 --> 01:17:50,130
don't have a ton of time so let's go

01:17:48,540 --> 01:17:52,020
ahead and hear something that can be

01:17:50,130 --> 01:17:54,630
solved in the teks moderation where you

01:17:52,020 --> 01:17:57,240
can adaptively understand where you are

01:17:54,630 --> 01:18:01,320
in the right heart from the completion

01:17:57,240 --> 01:18:03,090
if you if you if you start in the low

01:18:01,320 --> 01:18:06,030
latency mode and then if you see that

01:18:03,090 --> 01:18:07,470
you can hear a grated many packets to be

01:18:06,030 --> 01:18:10,260
sentenced and you can increase the cakes

01:18:07,470 --> 01:18:13,670
motivation all right so then we're about

01:18:10,260 --> 01:18:13,670
it okay

01:18:13,740 --> 01:18:23,280
so dicks bulking or multiply could stick

01:18:17,730 --> 01:18:28,380
Scripture so on top this is this is

01:18:23,280 --> 01:18:33,030
comes in cases where you already have

01:18:28,380 --> 01:18:35,850
bursts of packets but Eric's work was to

01:18:33,030 --> 01:18:38,520
enforce the these bursts and this

01:18:35,850 --> 01:18:41,970
feature is when you already have bursts

01:18:38,520 --> 01:18:46,740
of packet either with Exmouth more

01:18:41,970 --> 01:18:49,200
enabled or any or with this method so

01:18:46,740 --> 01:18:52,980
that that's where it comes to to

01:18:49,200 --> 01:18:58,760
optimize today there is the transmitter

01:18:52,980 --> 01:19:04,380
rookie that takes descriptor is 64 bytes

01:18:58,760 --> 01:19:07,530
so you have each 64 bytes as a control

01:19:04,380 --> 01:19:10,020
either Ethernet order the pointer and it

01:19:07,530 --> 01:19:12,660
has a padding because it must be 64 byte

01:19:10,020 --> 01:19:16,710
so if you have three cache lines you can

01:19:12,660 --> 01:19:22,800
only use those three cache lines to post

01:19:16,710 --> 01:19:25,710
three packets 32 2.92 bytes and post

01:19:22,800 --> 01:19:31,230
three packets this is what you have

01:19:25,710 --> 01:19:35,010
today until this interconnect expired in

01:19:31,230 --> 01:19:37,980
phonetics 5 we introduced a new feature

01:19:35,010 --> 01:19:43,200
called multi packet looking for this

01:19:37,980 --> 01:19:47,310
this descriptor in this feature we don't

01:19:43,200 --> 01:19:50,700
need which we can aggregate we can hang

01:19:47,310 --> 01:19:55,680
more packets on the same transmit

01:19:50,700 --> 01:19:58,410
descriptor it can be up to 60 packets if

01:19:55,680 --> 01:20:02,970
you increase the descriptor large enough

01:19:58,410 --> 01:20:07,290
to be 1k which is the maximum so they

01:20:02,970 --> 01:20:09,870
can be no there is an obligation to have

01:20:07,290 --> 01:20:11,490
shared header between those packets it

01:20:09,870 --> 01:20:14,570
can be different MAC address different

01:20:11,490 --> 01:20:23,470
ECP different pointers

01:20:14,570 --> 01:20:27,770
at all in the memory so you can just put

01:20:23,470 --> 01:20:30,880
pointers into the descriptor and notify

01:20:27,770 --> 01:20:35,660
the hardware and it will transmit it

01:20:30,880 --> 01:20:38,270
again what the only difference in in the

01:20:35,660 --> 01:20:42,140
way that the turn takes the scripture is

01:20:38,270 --> 01:20:44,870
formatted is just the opcode and and

01:20:42,140 --> 01:20:49,910
then the ganic knows how to handle these

01:20:44,870 --> 01:20:53,420
pointers and then so what are the

01:20:49,910 --> 01:20:56,620
benefits benefits is higher packet rate

01:20:53,420 --> 01:21:00,250
because you get a better civilization

01:20:56,620 --> 01:21:05,030
you are handling the same number of

01:21:00,250 --> 01:21:07,910
cache lines in the software but sending

01:21:05,030 --> 01:21:12,320
all packets for the same cache lines so

01:21:07,910 --> 01:21:15,220
cache misses are less and that causes

01:21:12,320 --> 01:21:18,140
higher packets right

01:21:15,220 --> 01:21:20,840
second thing is better PCI Equalization

01:21:18,140 --> 01:21:25,130
if you have 64 bytes to send and yet

01:21:20,840 --> 01:21:27,590
they add 64 bytes to control to be sent

01:21:25,130 --> 01:21:30,920
over the PCI that's only 50 percent of

01:21:27,590 --> 01:21:33,250
the PCI that's being utilized and you

01:21:30,920 --> 01:21:36,710
want to get two full hundred gigabit

01:21:33,250 --> 01:21:38,840
language you can't utilize only 50% of

01:21:36,710 --> 01:21:42,710
the VCR you need to have as much as

01:21:38,840 --> 01:21:46,190
possible into ization so in this case

01:21:42,710 --> 01:21:51,470
you can you can i sighs much better

01:21:46,190 --> 01:21:55,580
and the third thing is you can use this

01:21:51,470 --> 01:21:57,560
either the same number of entries in the

01:21:55,580 --> 01:22:01,340
transmit queue and have a lot more

01:21:57,560 --> 01:22:03,860
outstanding packets or you can use

01:22:01,340 --> 01:22:05,960
smaller queues which have better memory

01:22:03,860 --> 01:22:10,720
consumption and still have the same

01:22:05,960 --> 01:22:10,720
amount of tiny packets

01:22:11,170 --> 01:22:17,410
it's just format here so it's the same

01:22:14,900 --> 01:22:20,630
format as a regular one just different

01:22:17,410 --> 01:22:24,740
opcode and this is an example of the

01:22:20,630 --> 01:22:27,240
same three cache lines we saw before but

01:22:24,740 --> 01:22:30,230
this time the same three cache lines

01:22:27,240 --> 01:22:33,240
I can contain 10 packets so the average

01:22:30,230 --> 01:22:37,440
overhead per packet instead of 64 by

01:22:33,240 --> 01:22:41,820
goes down to 19 bytes in 10 packets and

01:22:37,440 --> 01:22:47,700
if you go all the way you can get up to

01:22:41,820 --> 01:22:50,760
17 bytes per packet if you do 60 packets

01:22:47,700 --> 01:22:52,740
per burst and of course that depends in

01:22:50,760 --> 01:23:00,180
the length of the birth that the

01:22:52,740 --> 01:23:02,790
software is pushing here's an example

01:23:00,180 --> 01:23:07,260
for what you can do with the transmit

01:23:02,790 --> 01:23:12,090
cube size so for for the default mode

01:23:07,260 --> 01:23:17,490
you have 1,000 entries that takes up 64

01:23:12,090 --> 01:23:21,950
K bytes and you have a maximum of of 1k

01:23:17,490 --> 01:23:25,800
of standing packets with the new viewer

01:23:21,950 --> 01:23:30,110
format you can you can reduce the size

01:23:25,800 --> 01:23:33,600
to even a quarter of that and have an

01:23:30,110 --> 01:23:37,470
use quarter of the memory and still have

01:23:33,600 --> 01:23:46,640
the maximum of standing packets with the

01:23:37,470 --> 01:23:46,640
same number of the same ring size so

01:23:46,730 --> 01:23:54,600
that's the feature in questions we're

01:23:52,650 --> 01:23:57,260
not going to allow questions if you have

01:23:54,600 --> 01:23:57,260
a quick one

01:23:58,630 --> 01:24:05,840
so to repeat the question what about

01:24:00,980 --> 01:24:07,640
offloads because the trans made the

01:24:05,840 --> 01:24:11,180
transmitted descriptors gotten any info

01:24:07,640 --> 01:24:13,700
at that point for a TCP offload unity of

01:24:11,180 --> 01:24:16,580
the regular one but it's already

01:24:13,700 --> 01:24:18,800
optimized because you have only that's

01:24:16,580 --> 01:24:21,290
already actually using some kind of

01:24:18,800 --> 01:24:24,050
multi facet descriptor connect connector

01:24:21,290 --> 01:24:25,340
located be also in liquidation let's

01:24:24,050 --> 01:24:27,020
have this discussion the following that

01:24:25,340 --> 01:24:30,110
some barbarians is finishing the coffee

01:24:27,020 --> 01:24:32,060
and all the everything out there this

01:24:30,110 --> 01:24:37,040
was for 30 minutes now they've been just

01:24:32,060 --> 01:24:39,680
consuming that and since we're late have

01:24:37,040 --> 01:24:42,520
this discussion after you know sorry I'm

01:24:39,680 --> 01:24:42,520
sorry you got it short

01:24:45,150 --> 01:24:49,400

YouTube URL: https://www.youtube.com/watch?v=pK60pzbyUHw


