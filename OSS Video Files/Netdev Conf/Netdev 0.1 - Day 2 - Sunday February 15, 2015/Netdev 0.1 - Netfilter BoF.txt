Title: Netdev 0.1 - Netfilter BoF
Publication date: 2015-03-21
Playlist: Netdev 0.1 - Day 2 - Sunday February 15, 2015
Description: 
	Netfilter BoF
Josh Hunt, David Gervais, and Pete Bohman
February 2015

Description from netdev01.org:
 This BoF intends to bring together interested parties and stakeholders to discuss the current state of iptables, ipset, and nftables when used in a large-scale environment. The discussion will focus around the use and issues with the current netfilter tools in such an environment and what we can do to improve them.

Some examples of those topics are:

    Supported Interfaces
        The need for solid, supported development libraries for iptables, ipset, nftables allowing applications to fully take advantage of their features.
    Improvements to existing components
        Handling very large sets (1 million to 25 million entries). Discuss alternatives to ipset (such as nft set implementation).
        Limitations in existing iptables functionality.
    nftables considerations
        Performance
        Backwards compatibility
        New features


https://www.netdev01.org

This video is licensed under Creative Commons Attribution-ShareAlike 4.0 International license. Feel free to download and distribute.
Captions: 
	00:00:00,350 --> 00:00:07,440
this is the net filter bought my

00:00:05,009 --> 00:00:10,530
colleague Josh hunt and my name is Pete

00:00:07,440 --> 00:00:13,019
boom and we'll be presenting sort of how

00:00:10,530 --> 00:00:18,000
Akamai uses IP set some of the stuff

00:00:13,019 --> 00:00:21,050
we've ran into and some of the changes

00:00:18,000 --> 00:00:23,340
we're looking forward to with NF tables

00:00:21,050 --> 00:00:26,130
it's kind of an outline of what we'll be

00:00:23,340 --> 00:00:27,660
talking about first of all we have an

00:00:26,130 --> 00:00:29,369
ether pad set up that will be taking

00:00:27,660 --> 00:00:32,099
notes on so that we can kind of remember

00:00:29,369 --> 00:00:36,210
stuff that we talked about whenever we

00:00:32,099 --> 00:00:38,610
go back home then we'll go into the

00:00:36,210 --> 00:00:40,950
presentation sort of ockham eyes use of

00:00:38,610 --> 00:00:44,730
netfilter components we'll start off

00:00:40,950 --> 00:00:46,800
with describing our use of netfilter

00:00:44,730 --> 00:00:50,120
components some of the desired netfilter

00:00:46,800 --> 00:00:54,600
interfaces that we're looking forward to

00:00:50,120 --> 00:00:55,920
we'll go into the internals of some of

00:00:54,600 --> 00:00:58,320
the changes we've made to netfilter

00:00:55,920 --> 00:01:00,239
components so that we could scale them

00:00:58,320 --> 00:01:04,549
to how we use them in our environment

00:01:00,239 --> 00:01:09,360
and then we'll go ahead and talk about

00:01:04,549 --> 00:01:11,159
NF tables and our trajectory our path

00:01:09,360 --> 00:01:14,900
forward with NF tables and what we

00:01:11,159 --> 00:01:16,710
envisioned for that if anybody has any

00:01:14,900 --> 00:01:19,259
presentations they would like to give

00:01:16,710 --> 00:01:20,820
related to netfilter components we

00:01:19,259 --> 00:01:22,560
should have time at the end for that or

00:01:20,820 --> 00:01:25,470
if you have topics you'd like to discuss

00:01:22,560 --> 00:01:27,680
we'll definitely have time I think Pablo

00:01:25,470 --> 00:01:30,060
and Patrick will continue their

00:01:27,680 --> 00:01:32,400
presentation from last night their

00:01:30,060 --> 00:01:35,250
tutorial from last night and F tables

00:01:32,400 --> 00:01:36,450
during this time and then if there's

00:01:35,250 --> 00:01:43,200
still time remaining will have an open

00:01:36,450 --> 00:01:46,320
discussion right so the system that

00:01:43,200 --> 00:01:48,899
optimizes for managing the firewall and

00:01:46,320 --> 00:01:51,390
all of our machines is force field this

00:01:48,899 --> 00:01:54,210
is an internal application there's a lot

00:01:51,390 --> 00:01:56,790
of optimized specific code in it it was

00:01:54,210 --> 00:01:58,860
deployed around 2010 to help manage the

00:01:56,790 --> 00:02:02,180
firewall and Oliver on all of our

00:01:58,860 --> 00:02:06,710
machines we have 160,000 machines

00:02:02,180 --> 00:02:11,099
spanning 2600 data centers in roughly

00:02:06,710 --> 00:02:12,310
107 countries so prior to force field we

00:02:11,099 --> 00:02:14,800
had a lot of

00:02:12,310 --> 00:02:16,810
teams kind of managing the firewall in

00:02:14,800 --> 00:02:19,690
an ad hoc manner there was really no way

00:02:16,810 --> 00:02:23,110
to see what the policy was for a given

00:02:19,690 --> 00:02:24,580
machine type that we had deployed so

00:02:23,110 --> 00:02:26,319
that was one of the first goals of force

00:02:24,580 --> 00:02:28,989
field was to create a centralized place

00:02:26,319 --> 00:02:33,280
where we could view and manage the

00:02:28,989 --> 00:02:36,160
policies for all of our machines as a

00:02:33,280 --> 00:02:39,880
firewall it is our first line of defense

00:02:36,160 --> 00:02:41,920
against attacks and kind of the the

00:02:39,880 --> 00:02:44,410
usability goals that we had were for it

00:02:41,920 --> 00:02:46,959
to be to safely expose all the net

00:02:44,410 --> 00:02:49,780
filter functionality to other teams so

00:02:46,959 --> 00:02:51,430
that applications could write iptables

00:02:49,780 --> 00:02:53,799
rules netfilter rules have all the

00:02:51,430 --> 00:02:56,560
functionality available to them but we

00:02:53,799 --> 00:02:58,900
wanted to do it as safeway a way that we

00:02:56,560 --> 00:03:00,099
could monitor it report on it and make

00:02:58,900 --> 00:03:03,790
sure everything was running as we

00:03:00,099 --> 00:03:06,220
expected and then we also provided some

00:03:03,790 --> 00:03:08,290
easy to use abstractions for developers

00:03:06,220 --> 00:03:11,560
to use that covered most of the common

00:03:08,290 --> 00:03:14,620
use cases so that they didn't have to be

00:03:11,560 --> 00:03:18,880
an IP tables expert to say open a port

00:03:14,620 --> 00:03:21,840
on a machine so this diagram at the

00:03:18,880 --> 00:03:25,329
bottom kind of depicts our stack or our

00:03:21,840 --> 00:03:27,730
model that we work with so we have three

00:03:25,329 --> 00:03:29,560
our forcefield exposes three api's we

00:03:27,730 --> 00:03:32,350
have an installation API so if you think

00:03:29,560 --> 00:03:34,840
a chef or puppet this would be like an

00:03:32,350 --> 00:03:36,700
API that we expose that that layer to

00:03:34,840 --> 00:03:39,459
where people can register rules or open

00:03:36,700 --> 00:03:43,709
ports we have a runtime API where people

00:03:39,459 --> 00:03:46,269
can add rules modify things at runtime

00:03:43,709 --> 00:03:48,130
then we also have a metadata API and

00:03:46,269 --> 00:03:51,940
when I say metadata that's probably an

00:03:48,130 --> 00:03:54,940
Akamai's specific term but this is an

00:03:51,940 --> 00:03:57,790
API where operators can go through a web

00:03:54,940 --> 00:03:59,829
UI to update configuration information

00:03:57,790 --> 00:04:01,989
for machines part of that configuration

00:03:59,829 --> 00:04:05,560
information is for IP tables and I psets

00:04:01,989 --> 00:04:07,720
through force field and again this is

00:04:05,560 --> 00:04:10,690
this is going to be a brief overview of

00:04:07,720 --> 00:04:12,130
how we use netfilter in force field so

00:04:10,690 --> 00:04:14,380
that whenever josh gets up and talks

00:04:12,130 --> 00:04:17,380
about some of the stuff that we want to

00:04:14,380 --> 00:04:19,239
see an IP set in iptables that you guys

00:04:17,380 --> 00:04:21,690
have an idea of how we use it where

00:04:19,239 --> 00:04:21,690
we're coming from

00:04:22,690 --> 00:04:27,350
okay so for our install API I can

00:04:25,310 --> 00:04:30,830
mention we can open-close ports we also

00:04:27,350 --> 00:04:32,720
have a way to a policy called protect

00:04:30,830 --> 00:04:35,509
this court and what this means is

00:04:32,720 --> 00:04:37,639
basically its source IP akal will be

00:04:35,509 --> 00:04:40,340
applied to that port and we're currently

00:04:37,639 --> 00:04:42,349
using the hash net IP set type and we

00:04:40,340 --> 00:04:44,150
have roughly four hundred and sixty

00:04:42,349 --> 00:04:53,300
thousand entries in there that cover all

00:04:44,150 --> 00:04:54,860
of our Akamai machine IPS and so that's

00:04:53,300 --> 00:04:56,570
part of that's those are the easy

00:04:54,860 --> 00:04:58,900
abstractions that force field provides

00:04:56,570 --> 00:05:01,880
for users they can also install

00:04:58,900 --> 00:05:04,520
application specific rules in IP sets

00:05:01,880 --> 00:05:07,699
some of the popular use cases that we

00:05:04,520 --> 00:05:09,199
see are white and black list we also see

00:05:07,699 --> 00:05:11,210
rate limiting so a lot of people like to

00:05:09,199 --> 00:05:13,820
write limit the amount of information

00:05:11,210 --> 00:05:16,310
there the number of requests going to a

00:05:13,820 --> 00:05:20,330
certain API so they will offload that to

00:05:16,310 --> 00:05:21,740
netfilter when possible we also probably

00:05:20,330 --> 00:05:24,470
like a lot of other people use it for a

00:05:21,740 --> 00:05:27,380
taxing or put attacks signatures into

00:05:24,470 --> 00:05:28,759
our rules so any common attacks and

00:05:27,380 --> 00:05:32,330
ensures that we have a put into rules

00:05:28,759 --> 00:05:34,520
and then lastly there's a use case

00:05:32,330 --> 00:05:37,820
around random sampling so a lot of

00:05:34,520 --> 00:05:40,280
people like to log whether it be draw

00:05:37,820 --> 00:05:42,680
packets or a certain sort a certain set

00:05:40,280 --> 00:05:44,270
of packets but they don't want to have

00:05:42,680 --> 00:05:45,949
all the packets coming up to user space

00:05:44,270 --> 00:05:47,599
so then they will use a random sampling

00:05:45,949 --> 00:05:50,090
to get packets user space where they'll

00:05:47,599 --> 00:05:52,610
then bog a certain set of the packets

00:05:50,090 --> 00:05:54,139
that they're interested in this code

00:05:52,610 --> 00:05:57,590
snippet out at the bottom is an example

00:05:54,139 --> 00:06:01,159
of our installation API so this is

00:05:57,590 --> 00:06:05,780
setting a protection opening port 80 and

00:06:01,159 --> 00:06:08,120
443 on machines that we on the machine

00:06:05,780 --> 00:06:12,050
type on our global host or ghost for

00:06:08,120 --> 00:06:14,349
short so that's our install API for the

00:06:12,050 --> 00:06:17,000
runtime side we allow applications to

00:06:14,349 --> 00:06:22,219
dynamically modify the contents of IP

00:06:17,000 --> 00:06:24,919
sets so this is how they can add IPS to

00:06:22,219 --> 00:06:27,860
an IP set dynamically at runtime they

00:06:24,919 --> 00:06:29,990
also use this to toggle iptables rules

00:06:27,860 --> 00:06:32,240
so what we found is it's really easy at

00:06:29,990 --> 00:06:34,819
install time to put all of your rules

00:06:32,240 --> 00:06:37,490
into IP table so you establish what we

00:06:34,819 --> 00:06:39,319
call a playbook and then later on

00:06:37,490 --> 00:06:42,650
whenever you want to say your machine is

00:06:39,319 --> 00:06:45,590
under attack or you want to enable your

00:06:42,650 --> 00:06:47,539
rule set you can toggle an IP set so an

00:06:45,590 --> 00:06:50,599
example of this would be an IP tables

00:06:47,539 --> 00:06:54,199
rule that matches on say a whitelist

00:06:50,599 --> 00:06:56,599
toggle set with the destination IP and

00:06:54,199 --> 00:06:58,520
it jumps to the change filter and so

00:06:56,599 --> 00:07:00,590
this this can be done at install time

00:06:58,520 --> 00:07:02,870
but then later on during the run time if

00:07:00,590 --> 00:07:05,150
the machine comes under heavy load the

00:07:02,870 --> 00:07:08,090
application can go ahead and add that

00:07:05,150 --> 00:07:11,449
machine's IP to the to the whitelist

00:07:08,090 --> 00:07:13,310
toggle set and now the top of the the

00:07:11,449 --> 00:07:16,810
filtering that they wanted to do for the

00:07:13,310 --> 00:07:19,130
attack is in place so this alleviates

00:07:16,810 --> 00:07:21,349
always processing all the chains and

00:07:19,130 --> 00:07:22,759
taking the performance hit and in this

00:07:21,349 --> 00:07:26,080
case we only take the performance it

00:07:22,759 --> 00:07:26,080
when we detect that we're under attack

00:07:32,210 --> 00:07:37,740
so the last API that I've talked about

00:07:34,530 --> 00:07:40,110
is the metadata interface and this

00:07:37,740 --> 00:07:42,420
allows operators to push changes out to

00:07:40,110 --> 00:07:44,340
a set of machines so they can say all

00:07:42,420 --> 00:07:47,970
these machines i want to add this rule

00:07:44,340 --> 00:07:51,210
to it and it's a quick and safe way for

00:07:47,970 --> 00:07:53,970
us to react to any sort of attack

00:07:51,210 --> 00:07:55,770
purchasing on the network we try to have

00:07:53,970 --> 00:07:58,140
around five minute timeline to get this

00:07:55,770 --> 00:08:00,240
data al so by the time the operator

00:07:58,140 --> 00:08:02,280
pushes the button to send this out but

00:08:00,240 --> 00:08:11,370
roughly five minutes it will have

00:08:02,280 --> 00:08:13,470
propagated to all 160,000 machines so

00:08:11,370 --> 00:08:17,460
the last bit on force field is its

00:08:13,470 --> 00:08:18,570
monitoring as any service that we run on

00:08:17,460 --> 00:08:20,970
our network we want to be able to

00:08:18,570 --> 00:08:22,350
monitor it and it's kind of centralized

00:08:20,970 --> 00:08:24,120
way so that we know its operating the

00:08:22,350 --> 00:08:25,470
way we think it should some of the

00:08:24,120 --> 00:08:28,080
things that we're interested in from the

00:08:25,470 --> 00:08:29,610
net filter side is are the chains and IP

00:08:28,080 --> 00:08:33,900
sets that we've configured are they

00:08:29,610 --> 00:08:36,479
actually loaded and then we want to also

00:08:33,900 --> 00:08:38,370
get the counter type of information back

00:08:36,479 --> 00:08:41,570
from these components that we can detect

00:08:38,370 --> 00:08:43,560
if things are abnormally high is there

00:08:41,570 --> 00:08:47,160
indication that there might be a Miss

00:08:43,560 --> 00:08:49,740
configuration on the network or just for

00:08:47,160 --> 00:08:51,270
reporting some some of this gets tied

00:08:49,740 --> 00:08:53,550
back to some of our products where we

00:08:51,270 --> 00:08:55,350
can tell our customers yes we've dropped

00:08:53,550 --> 00:09:00,960
this many packets on your behalf and

00:08:55,350 --> 00:09:03,720
such part of the monitoring is also our

00:09:00,960 --> 00:09:06,900
traffic analysis our info SEC team and

00:09:03,720 --> 00:09:08,340
security team like to have access to all

00:09:06,900 --> 00:09:11,280
the packets we've dropped so that they

00:09:08,340 --> 00:09:14,310
can analyze them so we implemented this

00:09:11,280 --> 00:09:16,740
feature within forcefield we're using it

00:09:14,310 --> 00:09:19,890
the NF log utilities we log is sampling

00:09:16,740 --> 00:09:21,180
of packets that we've dropped initially

00:09:19,890 --> 00:09:23,310
we were doing some deep packet

00:09:21,180 --> 00:09:26,550
inspection on the machines themselves

00:09:23,310 --> 00:09:29,160
but we found that to be to be a security

00:09:26,550 --> 00:09:31,200
vulnerability whenever we were the

00:09:29,160 --> 00:09:33,480
libraries we were using two parts the

00:09:31,200 --> 00:09:35,790
package weren't exactly safe so now we

00:09:33,480 --> 00:09:38,820
just love the actual packet payload

00:09:35,790 --> 00:09:41,420
itself and we combine that in a message

00:09:38,820 --> 00:09:44,730
frame that has air correction codes

00:09:41,420 --> 00:09:47,130
these logs then get aggregated using our

00:09:44,730 --> 00:09:50,250
sure to a centralized place where we

00:09:47,130 --> 00:09:51,839
have some analysis tools that analyze

00:09:50,250 --> 00:09:55,050
these packets to see exactly what's

00:09:51,839 --> 00:09:57,240
going on our network example what that

00:09:55,050 --> 00:10:00,600
dashboard looks like is here where we

00:09:57,240 --> 00:10:02,160
can show the breakdown of packets that

00:10:00,600 --> 00:10:04,589
we've dropped based on where they're

00:10:02,160 --> 00:10:07,320
coming from the source country on the

00:10:04,589 --> 00:10:12,089
target country as well as the protocols

00:10:07,320 --> 00:10:14,329
and ports that are being targeted so the

00:10:12,089 --> 00:10:16,560
sort of heat map has been used to

00:10:14,329 --> 00:10:19,250
there's a specific application that

00:10:16,560 --> 00:10:21,839
you're writing it's using a protocol and

00:10:19,250 --> 00:10:24,209
things aren't working or you're seeing a

00:10:21,839 --> 00:10:26,010
performance decrease some people have

00:10:24,209 --> 00:10:28,050
come into here to see oh there's a hot

00:10:26,010 --> 00:10:30,000
spot in this one region of the world

00:10:28,050 --> 00:10:31,649
maybe something's misconfigured there

00:10:30,000 --> 00:10:38,519
maybe it's lagging behind in its

00:10:31,649 --> 00:10:40,529
configuration okay so I talked about how

00:10:38,519 --> 00:10:43,190
we've integrated with netfilter some of

00:10:40,529 --> 00:10:45,449
the pain points during this process of

00:10:43,190 --> 00:10:47,699
distributing our policies across all of

00:10:45,449 --> 00:10:50,670
our machines in an automated way has

00:10:47,699 --> 00:10:55,170
come from the textual CLI that these

00:10:50,670 --> 00:10:56,490
tools support so an example of one of

00:10:55,170 --> 00:10:58,290
the problems that we've ran into is I

00:10:56,490 --> 00:11:01,230
think whenever you create something in

00:10:58,290 --> 00:11:03,839
IP set what you create and then what you

00:11:01,230 --> 00:11:06,630
read back from my pset may not always be

00:11:03,839 --> 00:11:08,670
the same and in this example I have up

00:11:06,630 --> 00:11:12,029
here the the underlined part has been

00:11:08,670 --> 00:11:14,550
added so I do an IPSec create and then I

00:11:12,029 --> 00:11:16,920
save that same set which is the read

00:11:14,550 --> 00:11:19,680
operation and IP set I give back

00:11:16,920 --> 00:11:20,910
something that looks different and when

00:11:19,680 --> 00:11:23,040
dealing with this an automated fashion

00:11:20,910 --> 00:11:24,779
yes we could parse this and realize it's

00:11:23,040 --> 00:11:27,600
the same but it was a stumbling block

00:11:24,779 --> 00:11:29,190
initially that you know this does look

00:11:27,600 --> 00:11:31,589
different this isn't exactly what I put

00:11:29,190 --> 00:11:34,079
in did it really load correctly so

00:11:31,589 --> 00:11:38,490
that's one of the things there's also

00:11:34,079 --> 00:11:41,010
teams that want to change the IP sets in

00:11:38,490 --> 00:11:43,620
IBD tables rules at a fairly frequent

00:11:41,010 --> 00:11:48,300
pace so shelling out to these components

00:11:43,620 --> 00:11:49,559
on as a performance consideration for

00:11:48,300 --> 00:11:52,689
them

00:11:49,559 --> 00:11:54,069
one workaround fed of course is batching

00:11:52,689 --> 00:11:56,559
the operations together but that

00:11:54,069 --> 00:11:58,569
requires a little bit more thought on

00:11:56,559 --> 00:12:04,269
our side are a little bit more logic on

00:11:58,569 --> 00:12:06,639
the users face side another interesting

00:12:04,269 --> 00:12:09,939
point that we ran into was the

00:12:06,639 --> 00:12:11,350
non-atomic IP set operations that IPSec

00:12:09,939 --> 00:12:13,629
currently supports the right now if you

00:12:11,350 --> 00:12:16,959
if you restore a script that has

00:12:13,629 --> 00:12:19,329
multiple operations in it if there's a

00:12:16,959 --> 00:12:21,970
problem halfway through it half of those

00:12:19,329 --> 00:12:24,040
operations will be committed to the IP

00:12:21,970 --> 00:12:25,779
set and you'll be left in an

00:12:24,040 --> 00:12:28,299
inconsistent state or maybe something

00:12:25,779 --> 00:12:29,829
you don't expect so our work around

00:12:28,299 --> 00:12:33,519
currently for that is to create a

00:12:29,829 --> 00:12:36,579
temporary set copy the existing set to

00:12:33,519 --> 00:12:38,410
the temporary set perform our operations

00:12:36,579 --> 00:12:41,499
on the temporary set and if everything

00:12:38,410 --> 00:12:44,319
went well swap that temporary setback to

00:12:41,499 --> 00:12:46,720
the target site of course if there is a

00:12:44,319 --> 00:12:48,489
failure then we can delete the temporary

00:12:46,720 --> 00:12:52,290
set and never swap back to the target

00:12:48,489 --> 00:12:55,149
set and the target site is left unharmed

00:12:52,290 --> 00:12:57,100
one of the disadvantages of this current

00:12:55,149 --> 00:13:00,939
approach is that we lose counter

00:12:57,100 --> 00:13:02,379
information during this swab so I think

00:13:00,939 --> 00:13:04,559
it was mentioned also there I also

00:13:02,379 --> 00:13:06,639
talked about last night at the NF tables

00:13:04,559 --> 00:13:08,259
tutorial about losing counter

00:13:06,639 --> 00:13:12,850
informations during these swapping

00:13:08,259 --> 00:13:17,709
operations another feature that we'd

00:13:12,850 --> 00:13:19,869
like to see is the ability to notice

00:13:17,709 --> 00:13:21,759
when changes are being made to IP tables

00:13:19,869 --> 00:13:27,069
and IP sets that we can react

00:13:21,759 --> 00:13:29,499
accordingly so with that said a short

00:13:27,069 --> 00:13:31,389
list of if we were to develop an API

00:13:29,499 --> 00:13:34,299
program programmatic API that

00:13:31,389 --> 00:13:37,480
applications can use to sort of monitor

00:13:34,299 --> 00:13:39,730
and manage iptables I p sub we would

00:13:37,480 --> 00:13:41,730
like to create an API that has the

00:13:39,730 --> 00:13:45,459
normal create read update delete

00:13:41,730 --> 00:13:47,970
operations we're seeing this come along

00:13:45,459 --> 00:13:49,959
and enough tables obviously but the

00:13:47,970 --> 00:13:51,939
whenever you create something you would

00:13:49,959 --> 00:13:55,470
get a handle to it and perform these

00:13:51,939 --> 00:13:55,470
operations using that handle

00:13:55,650 --> 00:14:00,640
the the messages exchanged could be

00:13:58,300 --> 00:14:02,400
extensible with a well-defined field

00:14:00,640 --> 00:14:05,680
play out I think we're getting there and

00:14:02,400 --> 00:14:08,050
then lastly a way to register call backs

00:14:05,680 --> 00:14:10,990
or provide some asynchronous method for

00:14:08,050 --> 00:14:16,420
getting updates about what is changing

00:14:10,990 --> 00:14:17,980
with in iptables or IP set okay so with

00:14:16,420 --> 00:14:20,080
that we've kind of started at the top

00:14:17,980 --> 00:14:22,180
with our application layer how we

00:14:20,080 --> 00:14:24,040
integrate with NF filters josh is going

00:14:22,180 --> 00:14:26,860
to be able to talk about some of the

00:14:24,040 --> 00:14:28,990
internals of IP sets and iptables that

00:14:26,860 --> 00:14:36,130
he's been working on to help scale to

00:14:28,990 --> 00:14:37,090
our needs so yeah so Pete's kind of

00:14:36,130 --> 00:14:39,160
hover covered like you said the

00:14:37,090 --> 00:14:43,270
high-level overview of what what we've

00:14:39,160 --> 00:14:45,730
been doing with iptables and a lot of

00:14:43,270 --> 00:14:48,490
those issues revolve around lack of a

00:14:45,730 --> 00:14:51,520
fully supported library there and for

00:14:48,490 --> 00:14:53,890
the most part we're able to to leverage

00:14:51,520 --> 00:14:57,540
iptables as is we have really had to do

00:14:53,890 --> 00:14:59,620
too many modifications to to iptables

00:14:57,540 --> 00:15:01,290
basically leveraging you know the great

00:14:59,620 --> 00:15:03,010
work that the community is already done

00:15:01,290 --> 00:15:07,620
recently though we have hit some

00:15:03,010 --> 00:15:09,700
problems with hash limit matching and so

00:15:07,620 --> 00:15:12,700
this is kind of a description of the

00:15:09,700 --> 00:15:15,670
problem one of the problems so right now

00:15:12,700 --> 00:15:20,020
there's an issue where if we modify hash

00:15:15,670 --> 00:15:23,950
parameters through iptables restore with

00:15:20,020 --> 00:15:27,160
the same table name the configuration

00:15:23,950 --> 00:15:30,430
parameters don't don't change so the

00:15:27,160 --> 00:15:32,650
example here is the original rule had a

00:15:30,430 --> 00:15:34,900
rate limit up to 10 packets per second

00:15:32,650 --> 00:15:38,110
we've changed it in the new rule during

00:15:34,900 --> 00:15:39,580
a restore to a thousand per second this

00:15:38,110 --> 00:15:41,200
actually doesn't change any that

00:15:39,580 --> 00:15:46,330
configure underlying configuration for

00:15:41,200 --> 00:15:50,070
for that match and so I had proposed a

00:15:46,330 --> 00:15:52,880
change earlier this year but apparently

00:15:50,070 --> 00:15:55,760
next slide

00:15:52,880 --> 00:15:58,370
step back so the problem is that the

00:15:55,760 --> 00:15:59,930
internal function there only checks

00:15:58,370 --> 00:16:01,130
against the name and the family to see

00:15:59,930 --> 00:16:03,110
if there already exists and if it does

00:16:01,130 --> 00:16:06,920
it doesn't do anything to change the

00:16:03,110 --> 00:16:09,080
configuration the upstream the changes

00:16:06,920 --> 00:16:11,420
that I kind of proposed was to check

00:16:09,080 --> 00:16:13,430
against all of the configuration

00:16:11,420 --> 00:16:18,140
parameters for the table for the match

00:16:13,430 --> 00:16:20,660
and and then if it doesn't match create

00:16:18,140 --> 00:16:25,160
a new one apparently though there was

00:16:20,660 --> 00:16:27,530
some concern if there's already some

00:16:25,160 --> 00:16:30,800
existing file this was existing

00:16:27,530 --> 00:16:33,170
functionality and this might break some

00:16:30,800 --> 00:16:35,930
people's expectations of how hatch

00:16:33,170 --> 00:16:39,410
stomach currently operates there's also

00:16:35,930 --> 00:16:41,480
a secondary problem along the same same

00:16:39,410 --> 00:16:43,610
lines as if you have even the same rule

00:16:41,480 --> 00:16:45,380
set if you're referencing the same hash

00:16:43,610 --> 00:16:47,180
limit table with different config

00:16:45,380 --> 00:16:49,730
parameters it just silently allows you

00:16:47,180 --> 00:16:52,580
to do this but actually and in the

00:16:49,730 --> 00:16:55,190
example here the second rule actually

00:16:52,580 --> 00:16:57,710
still uses the same configuration as the

00:16:55,190 --> 00:17:02,780
first rule so there's no no warning or

00:16:57,710 --> 00:17:06,439
error out in this condition and so I

00:17:02,780 --> 00:17:09,620
guess the question here is you know it

00:17:06,439 --> 00:17:11,900
is this something that can we change in

00:17:09,620 --> 00:17:14,000
the current hash limit or is it

00:17:11,900 --> 00:17:16,579
something that would need a new

00:17:14,000 --> 00:17:22,069
implementation so I guess that's a

00:17:16,579 --> 00:17:23,990
question for the group here and then

00:17:22,069 --> 00:17:30,290
another problem we've run into recently

00:17:23,990 --> 00:17:31,730
is there's a maximum rate of rate limit

00:17:30,290 --> 00:17:33,950
right now is hard-coded to 10,000

00:17:31,730 --> 00:17:37,850
packets per second I think this is due

00:17:33,950 --> 00:17:39,440
to a couple of things but I if I

00:17:37,850 --> 00:17:43,730
remember correctly there is a

00:17:39,440 --> 00:17:46,580
expectation that the smallest frequency

00:17:43,730 --> 00:17:50,930
allows I think over well our match

00:17:46,580 --> 00:17:54,380
against like 11 event per day and then

00:17:50,930 --> 00:17:59,060
also be able to support up to 10,000

00:17:54,380 --> 00:18:01,250
packets per second wondering if we could

00:17:59,060 --> 00:18:03,980
extend this it appears that it might be

00:18:01,250 --> 00:18:05,180
based the limitation here is because if

00:18:03,980 --> 00:18:07,070
it's a 32-bit

00:18:05,180 --> 00:18:10,160
data type and we might be able to extend

00:18:07,070 --> 00:18:11,330
that to support a higher level of

00:18:10,160 --> 00:18:14,020
packets per second we're getting

00:18:11,330 --> 00:18:16,760
requests from teams to be able to do

00:18:14,020 --> 00:18:19,610
matching on you know like DNS servers

00:18:16,760 --> 00:18:22,910
and things where 10,000 packets per

00:18:19,610 --> 00:18:28,670
second isn't it that threshold for us is

00:18:22,910 --> 00:18:30,170
too low and so they think that I think

00:18:28,670 --> 00:18:34,340
we would like to go ahead and raise this

00:18:30,170 --> 00:18:40,370
this limit either in existing hash limit

00:18:34,340 --> 00:18:42,260
or in a new implementation assuming that

00:18:40,370 --> 00:18:44,390
the first problem that I talked about is

00:18:42,260 --> 00:18:47,900
it's something that we can't resolve now

00:18:44,390 --> 00:18:52,180
so I don't know is there any Pablo or I

00:18:47,900 --> 00:18:52,180
don't know if anybody sorry

00:19:05,860 --> 00:19:13,070
okay now so yes the problem is that we I

00:19:09,620 --> 00:19:16,040
mean policy has been so far the that we

00:19:13,070 --> 00:19:18,320
didn't accept this kind of second

00:19:16,040 --> 00:19:21,320
versions of existing matches and targets

00:19:18,320 --> 00:19:24,650
I mean that we had in the past patches

00:19:21,320 --> 00:19:27,410
to get quota to and similar things what

00:19:24,650 --> 00:19:29,300
we have currently said revision dervish

00:19:27,410 --> 00:19:33,770
revision infrastructure that allows us

00:19:29,300 --> 00:19:39,050
to introduce updates to two extensions

00:19:33,770 --> 00:19:41,270
the point there is that we should do it

00:19:39,050 --> 00:19:43,760
in a biocompatible fashion so should

00:19:41,270 --> 00:19:46,640
probably probably introducing a new

00:19:43,760 --> 00:19:49,190
parameter what well yet another

00:19:46,640 --> 00:19:51,170
perimeter to to hash name it already has

00:19:49,190 --> 00:19:53,450
quite a lot of them yeah but introduces

00:19:51,170 --> 00:19:57,680
a new parameters so the new revision

00:19:53,450 --> 00:20:01,450
knows that you want the new behavior and

00:19:57,680 --> 00:20:04,940
you want to use 62 bits is too big

00:20:01,450 --> 00:20:06,830
variable to store the rape and all the

00:20:04,940 --> 00:20:08,870
things that you need so i think i would

00:20:06,830 --> 00:20:11,740
go that probably that way Patrick you

00:20:08,870 --> 00:20:11,740
have some comment on that

00:20:20,880 --> 00:20:27,250
yeah my main question is the behavior

00:20:24,580 --> 00:20:29,680
you mentioned about the different

00:20:27,250 --> 00:20:31,930
American parent parameters you use and

00:20:29,680 --> 00:20:35,070
what would the behavior be you expect if

00:20:31,930 --> 00:20:39,550
you use the same table in two places and

00:20:35,070 --> 00:20:41,200
supply different sets of parameters so

00:20:39,550 --> 00:20:43,270
like you're talking about in this case

00:20:41,200 --> 00:20:45,340
here where we've used the same table in

00:20:43,270 --> 00:20:47,170
the same rule set yeah I think it

00:20:45,340 --> 00:20:50,530
relates to the restoring problem yeah I

00:20:47,170 --> 00:20:52,000
think it's off yeah I am I minded

00:20:50,530 --> 00:20:52,750
whatever out of this case right it

00:20:52,000 --> 00:20:55,600
shouldn't allow this type of

00:20:52,750 --> 00:20:57,520
configuration so just rejected yeah yeah

00:20:55,600 --> 00:20:59,260
I guess that would be fine but you would

00:20:57,520 --> 00:21:01,900
obviously have to specify that this

00:20:59,260 --> 00:21:05,590
behavior the rejection is wanted right

00:21:01,900 --> 00:21:08,429
right sure yeah okay did you have a

00:21:05,590 --> 00:21:08,429
question too sorry

00:21:15,440 --> 00:21:19,920
the first two problems you described I

00:21:17,970 --> 00:21:23,010
suspect that's a problem for a lot of

00:21:19,920 --> 00:21:25,470
people and I do not really know if we

00:21:23,010 --> 00:21:27,090
made an attempt to understand I do not

00:21:25,470 --> 00:21:31,640
know if we made an attempt to understand

00:21:27,090 --> 00:21:34,020
if anybody would be adversely affected

00:21:31,640 --> 00:21:35,670
but I guess from Project perspecto I

00:21:34,020 --> 00:21:37,470
understand the concern we don't want to

00:21:35,670 --> 00:21:40,500
break the backward compatibility in case

00:21:37,470 --> 00:21:43,200
there is somebody out there but we are a

00:21:40,500 --> 00:21:45,690
user of hash limits and we land into the

00:21:43,200 --> 00:21:48,270
same problems okay yeah and we had to

00:21:45,690 --> 00:21:52,830
work I could do something about it okay

00:21:48,270 --> 00:21:56,040
yeah I like I think the best way if you

00:21:52,830 --> 00:21:58,550
would find I mean the algorithm seems to

00:21:56,040 --> 00:22:01,310
have a problem to express this kind of

00:21:58,550 --> 00:22:04,500
different rate limits for the same

00:22:01,310 --> 00:22:06,390
stated keeps I guess it should be

00:22:04,500 --> 00:22:07,410
possible to maintain the state in a

00:22:06,390 --> 00:22:09,330
fashion that you can apply different

00:22:07,410 --> 00:22:11,550
limits against the same say it so if you

00:22:09,330 --> 00:22:14,220
would find a solution to that it would

00:22:11,550 --> 00:22:19,140
be but the most preferable way I think

00:22:14,220 --> 00:22:23,460
okay sure we can look into that so just

00:22:19,140 --> 00:22:25,950
to clarify same table name different

00:22:23,460 --> 00:22:31,530
rate specifications we should be

00:22:25,950 --> 00:22:35,430
possible I think ok alright cool let's

00:22:31,530 --> 00:22:38,040
see so that was pretty much all I had to

00:22:35,430 --> 00:22:41,520
talk about with sorry for IP tables

00:22:38,040 --> 00:22:46,040
right now we really spend more of our

00:22:41,520 --> 00:22:48,660
time in the IP set code especially

00:22:46,040 --> 00:22:50,660
recently we've gotten a lot of internal

00:22:48,660 --> 00:22:55,230
requests to use more IP sets for things

00:22:50,660 --> 00:22:59,730
as Pete mentioned through force field

00:22:55,230 --> 00:23:02,130
you know we have that the ACL where

00:22:59,730 --> 00:23:04,470
there's about 460,000 entries we also

00:23:02,130 --> 00:23:07,110
have other teams where I think they've

00:23:04,470 --> 00:23:09,360
got sets in the order of you know two

00:23:07,110 --> 00:23:11,160
million entries with the possibility of

00:23:09,360 --> 00:23:14,610
going up to 25 million in the next

00:23:11,160 --> 00:23:18,000
couple of years or maybe even sooner I

00:23:14,610 --> 00:23:20,190
think a lot of that though can at least

00:23:18,000 --> 00:23:26,150
for this particular use case that I'm

00:23:20,190 --> 00:23:29,070
thinking of the 25 million issue is more

00:23:26,150 --> 00:23:32,810
the set is generally of like an IP port

00:23:29,070 --> 00:23:35,160
match type set and you've got multiple

00:23:32,810 --> 00:23:38,340
entries for the same IP with different

00:23:35,160 --> 00:23:41,280
ports so if we could somehow do you know

00:23:38,340 --> 00:23:43,140
an interval or match against multiple

00:23:41,280 --> 00:23:45,270
ports for the same IP for the same entry

00:23:43,140 --> 00:23:49,130
I think that would drop jump this down

00:23:45,270 --> 00:23:52,050
considerably for this particular case

00:23:49,130 --> 00:23:54,000
the type of sets that we're using I

00:23:52,050 --> 00:23:57,570
think Pete mentioned hash net you know

00:23:54,000 --> 00:23:59,250
we're covering just about this is an

00:23:57,570 --> 00:24:00,810
everything that's supported but and

00:23:59,250 --> 00:24:05,040
enlist everything that we're using here

00:24:00,810 --> 00:24:08,130
but this there's a lot of different hash

00:24:05,040 --> 00:24:11,970
types being used at Akamai and we've got

00:24:08,130 --> 00:24:13,830
like I said a lot of internal teams have

00:24:11,970 --> 00:24:16,980
been coming to us looking to use IP sets

00:24:13,830 --> 00:24:18,860
and seems like they keep requesting yeah

00:24:16,980 --> 00:24:21,830
the need to block larger and larger

00:24:18,860 --> 00:24:23,760
numbers of IPs and entries that

00:24:21,830 --> 00:24:29,330
generally are using us for white and

00:24:23,760 --> 00:24:32,940
black listing and so we kind of had to

00:24:29,330 --> 00:24:35,970
two ways of working on this Akamai

00:24:32,940 --> 00:24:37,560
Pete's team as you know the firewall

00:24:35,970 --> 00:24:40,050
management layer but there's some teams

00:24:37,560 --> 00:24:42,000
where this just doesn't it's it's not it

00:24:40,050 --> 00:24:44,340
doesn't fit their needs right now and so

00:24:42,000 --> 00:24:48,690
they're just relying on live I piece set

00:24:44,340 --> 00:24:50,660
and so I understand that it's not a kind

00:24:48,690 --> 00:24:55,020
of a fully supported or stable API but

00:24:50,660 --> 00:24:56,520
it's what's available right now and we

00:24:55,020 --> 00:24:59,400
haven't had the time to kind of extend

00:24:56,520 --> 00:25:00,900
it so I think we're willing to kind of

00:24:59,400 --> 00:25:03,660
take those risk and then just handle the

00:25:00,900 --> 00:25:05,040
problems internally and also if we we

00:25:03,660 --> 00:25:07,170
have any issues there I mean we

00:25:05,040 --> 00:25:10,770
definitely try to push whatever we can

00:25:07,170 --> 00:25:13,560
back the problems with live I be set

00:25:10,770 --> 00:25:15,180
though is it doesn't really export or

00:25:13,560 --> 00:25:19,130
expose everything that we'd like to see

00:25:15,180 --> 00:25:22,110
with respect to set metadata and so

00:25:19,130 --> 00:25:24,680
we've recently started trying to push

00:25:22,110 --> 00:25:27,900
out a few patches there to to extend

00:25:24,680 --> 00:25:29,700
expose some some of the set information

00:25:27,900 --> 00:25:31,500
for example like set size info isn't

00:25:29,700 --> 00:25:33,900
something that's displayed in IP set

00:25:31,500 --> 00:25:35,660
header right now so we have teams

00:25:33,900 --> 00:25:38,460
relying on actually counting entries

00:25:35,660 --> 00:25:38,780
just to check the actual set side see

00:25:38,460 --> 00:25:42,410
how

00:25:38,780 --> 00:25:44,770
elements are present so it's just things

00:25:42,410 --> 00:25:49,250
like this where they're just not

00:25:44,770 --> 00:25:53,780
available right now and so I with all

00:25:49,250 --> 00:25:56,690
the requests for support for the knee

00:25:53,780 --> 00:25:59,660
fry psets from other other teams they

00:25:56,690 --> 00:26:01,490
also want this visibility into the this

00:25:59,660 --> 00:26:03,980
sets and to be able to to configure them

00:26:01,490 --> 00:26:06,440
as they want and also to monitor that

00:26:03,980 --> 00:26:07,850
information so and another big problem

00:26:06,440 --> 00:26:14,350
with that live ib says it's not

00:26:07,850 --> 00:26:16,430
thread-safe so just to kind of look at

00:26:14,350 --> 00:26:20,300
this isn't performance data that

00:26:16,430 --> 00:26:22,100
probably most people care about but just

00:26:20,300 --> 00:26:26,000
looking at the overhead of just adding

00:26:22,100 --> 00:26:30,430
entries to set so if we try to add entry

00:26:26,000 --> 00:26:35,510
like a million entries random IPS to a

00:26:30,430 --> 00:26:37,570
basic hash IP set here takes on the

00:26:35,510 --> 00:26:40,970
order of a little over five minutes

00:26:37,570 --> 00:26:43,520
which isn't really doable that's not

00:26:40,970 --> 00:26:45,410
usable so we've and as kind of Pete

00:26:43,520 --> 00:26:47,960
talked about earlier the way that we add

00:26:45,410 --> 00:26:50,270
entries to sets is through restore and

00:26:47,960 --> 00:26:52,280
so the restore method you can see we can

00:26:50,270 --> 00:26:54,770
restore a set of a million entries in

00:26:52,280 --> 00:26:56,360
approximately two seconds here and then

00:26:54,770 --> 00:26:58,580
the time to save them as a little less

00:26:56,360 --> 00:27:00,140
than a second so these are the cut these

00:26:58,580 --> 00:27:01,790
are the operations that we're kind of

00:27:00,140 --> 00:27:05,150
using right now to update set

00:27:01,790 --> 00:27:06,470
information that basically because of

00:27:05,150 --> 00:27:11,270
the scale that we're having to deal out

00:27:06,470 --> 00:27:13,430
with peak alluded to a lot of these

00:27:11,270 --> 00:27:16,490
earlier but you know these operations

00:27:13,430 --> 00:27:18,650
are anatomic so failure midway gets us a

00:27:16,490 --> 00:27:21,110
partial set which isn't usable usually

00:27:18,650 --> 00:27:22,550
you don't know what state you're in also

00:27:21,110 --> 00:27:26,150
sets currently grow but they don't

00:27:22,550 --> 00:27:28,700
shrink so we can resize up but we never

00:27:26,150 --> 00:27:31,640
reclaim that memory on the way back down

00:27:28,700 --> 00:27:33,730
and this separation between IP set and

00:27:31,640 --> 00:27:36,770
iptables makes it difficult to work with

00:27:33,730 --> 00:27:40,789
you know from a kind of a middle layer

00:27:36,770 --> 00:27:43,289
level you have to kind of manage both

00:27:40,789 --> 00:27:45,809
double-checking that ok when I put the

00:27:43,289 --> 00:27:48,830
set in you know actually completed and

00:27:45,809 --> 00:27:51,360
you know then I can update the rule so

00:27:48,830 --> 00:27:53,100
we wanted to take a look at NF tables

00:27:51,360 --> 00:27:56,130
because we're pretty interested in in

00:27:53,100 --> 00:28:00,000
that and wanted to see you know just

00:27:56,130 --> 00:28:05,100
kind of you know how it is compared to

00:28:00,000 --> 00:28:09,000
IP set and so I did some basic tests

00:28:05,100 --> 00:28:12,409
with with NF table sets to kind of

00:28:09,000 --> 00:28:15,600
understand how it is on par with just

00:28:12,409 --> 00:28:19,610
the simple you know creation and

00:28:15,600 --> 00:28:24,210
deletion test that I had just shown so

00:28:19,610 --> 00:28:28,980
we took the current net next and tried

00:28:24,210 --> 00:28:33,830
adding a million entries to to an NF tea

00:28:28,980 --> 00:28:36,389
set this is a type ipv4 address set and

00:28:33,830 --> 00:28:37,950
it took a really long time so I decided

00:28:36,389 --> 00:28:41,039
just to scale it back to 10,000 entries

00:28:37,950 --> 00:28:45,779
and I got around about three

00:28:41,039 --> 00:28:49,289
three-and-a-half minutes and so with IP

00:28:45,779 --> 00:28:51,029
set I can do 10,000 entries in about 20

00:28:49,289 --> 00:28:53,389
seconds which still isn't really usable

00:28:51,029 --> 00:28:59,429
for us but it just kind of gives you a

00:28:53,389 --> 00:29:00,840
data point there and so like I said

00:28:59,429 --> 00:29:04,320
before we've seen this problem with IP

00:29:00,840 --> 00:29:06,000
set so the next logical step was ok well

00:29:04,320 --> 00:29:09,330
let's try restores how do how do we do

00:29:06,000 --> 00:29:12,059
with their stores there so we tried to

00:29:09,330 --> 00:29:13,500
do a restore of 10,000 entries and it's

00:29:12,059 --> 00:29:15,210
less than a second so it was like all

00:29:13,500 --> 00:29:20,130
right there we go that's that's our

00:29:15,210 --> 00:29:22,889
route so then trying to do a restore of

00:29:20,130 --> 00:29:26,820
a million we hit a problem so the system

00:29:22,889 --> 00:29:29,370
kind of started softlock ipping on us so

00:29:26,820 --> 00:29:33,389
I reported this upstream I think there's

00:29:29,370 --> 00:29:36,750
the link here for the the thread kong

00:29:33,389 --> 00:29:39,179
replied back with a patch to insert a

00:29:36,750 --> 00:29:40,860
condrey sched and the new set element

00:29:39,179 --> 00:29:42,269
function that appears to get rid of the

00:29:40,860 --> 00:29:46,380
soft lock up but there's still some

00:29:42,269 --> 00:29:48,779
other underlying issues there so looking

00:29:46,380 --> 00:29:50,800
into it and a little more detail right

00:29:48,779 --> 00:29:54,700
now there seems to be a problem with

00:29:50,800 --> 00:29:59,290
when we do set restores the hash table

00:29:54,700 --> 00:30:01,480
isn't growing properly so we've got we

00:29:59,290 --> 00:30:04,950
start off I think nft hash sets uses

00:30:01,480 --> 00:30:07,360
initial set of size of four buckets and

00:30:04,950 --> 00:30:10,270
when we go ahead and we try to do a

00:30:07,360 --> 00:30:11,980
restore here of 10,000 elements it

00:30:10,270 --> 00:30:15,190
doesn't do the expansion until after the

00:30:11,980 --> 00:30:17,830
restore is complete so why doesn't it

00:30:15,190 --> 00:30:20,260
grow on restore so I'm not sure yet it's

00:30:17,830 --> 00:30:23,530
on my to-do list but I think after

00:30:20,260 --> 00:30:24,580
talking with Patrick yesterday and I

00:30:23,530 --> 00:30:26,740
think he mentioned that he spoke with

00:30:24,580 --> 00:30:28,060
Thomas about it that maybe they have a

00:30:26,740 --> 00:30:30,280
good idea of what's going on here I

00:30:28,060 --> 00:30:37,030
don't know did you guys have anything to

00:30:30,280 --> 00:30:39,730
add at this point her yeah I think well

00:30:37,030 --> 00:30:43,540
some recent changes to our hash table

00:30:39,730 --> 00:30:45,370
broke the grow decision so it basically

00:30:43,540 --> 00:30:47,560
doesn't decide to grow I think that

00:30:45,370 --> 00:30:49,540
should be it i'm not one hundred percent

00:30:47,560 --> 00:30:51,820
sure maybe Thomas has looked into it

00:30:49,540 --> 00:30:54,340
already the question would be have you

00:30:51,820 --> 00:30:57,670
tested with fakes that was posted or

00:30:54,340 --> 00:31:01,690
without the fix which fix our fix was

00:30:57,670 --> 00:31:03,490
posted by your post want to fix that

00:31:01,690 --> 00:31:06,460
like the growth the growth decision was

00:31:03,490 --> 00:31:09,850
broken oh yeah yesterday that's on yeah

00:31:06,460 --> 00:31:11,230
okay sorry yes so this is a yeah I'm

00:31:09,850 --> 00:31:13,090
getting to that so maybe it's a little

00:31:11,230 --> 00:31:16,300
out of order so with the grow decision

00:31:13,090 --> 00:31:21,790
fix it still has a problem growing so

00:31:16,300 --> 00:31:24,340
this was this was yet with that patch

00:31:21,790 --> 00:31:27,790
the one thing that we discussed is that

00:31:24,340 --> 00:31:29,800
in the case of nft hash growing from

00:31:27,790 --> 00:31:32,100
from from four buckets to 1 million

00:31:29,800 --> 00:31:34,930
takes a lot off expansion cycles so I

00:31:32,100 --> 00:31:36,730
talked to talk about that maybe we can

00:31:34,930 --> 00:31:38,530
grow quicker than just by a factor of

00:31:36,730 --> 00:31:39,850
two but I wonder what you doing next

00:31:38,530 --> 00:31:42,580
manager we know why it's not even

00:31:39,850 --> 00:31:44,650
kicking in at all during well that's

00:31:42,580 --> 00:31:46,540
definitely a baka yeah that's definitely

00:31:44,650 --> 00:31:49,150
need to fix but I think even beyond that

00:31:46,540 --> 00:31:52,620
right when you grow from four buckets to

00:31:49,150 --> 00:31:55,060
one Miller to maybe 500,000 pockets sure

00:31:52,620 --> 00:31:56,350
the expansion will take a while and in

00:31:55,060 --> 00:31:58,180
the meantime a lot of insurance will

00:31:56,350 --> 00:31:59,250
happen yeah that will create long chain

00:31:58,180 --> 00:32:01,350
length that will

00:31:59,250 --> 00:32:02,910
we'll make the expand take even longer I

00:32:01,350 --> 00:32:05,100
think we'll have that address after we

00:32:02,910 --> 00:32:06,810
fixed the expansion problem general will

00:32:05,100 --> 00:32:08,820
have to address that as well okay well

00:32:06,810 --> 00:32:10,260
that makes it yeah I mean it's so I'm

00:32:08,820 --> 00:32:12,720
not quite sure what's happening there a

00:32:10,260 --> 00:32:16,130
a couple of questions regarding the

00:32:12,720 --> 00:32:18,630
let's start with the insertion times the

00:32:16,130 --> 00:32:21,120
when you added the 1 million entries did

00:32:18,630 --> 00:32:23,130
you make 1,000,000 invocations of NF

00:32:21,120 --> 00:32:25,680
tables or did you add multiple entries

00:32:23,130 --> 00:32:28,830
in one invocation or how did you exactly

00:32:25,680 --> 00:32:31,230
add them so when I so I should clarify

00:32:28,830 --> 00:32:33,780
this earlier when I talked about ads I'm

00:32:31,230 --> 00:32:36,360
doing nft ad when I'm talking about

00:32:33,780 --> 00:32:39,270
restores I'm restoring it from a ruleset

00:32:36,360 --> 00:32:42,090
file yeah so what I did was I generated

00:32:39,270 --> 00:32:44,190
the million inch random IPS and then I

00:32:42,090 --> 00:32:46,500
populate that into a rule set and then I

00:32:44,190 --> 00:32:48,780
did n ft minus F to restore that right

00:32:46,500 --> 00:32:51,810
okay and so the 1 million ads was

00:32:48,780 --> 00:32:54,930
1,000,000 to execute invocations of the

00:32:51,810 --> 00:32:57,840
NF tables binaries and adding one

00:32:54,930 --> 00:33:00,630
element each time in the ad case yes

00:32:57,840 --> 00:33:02,490
yeah okay sure that's like it's expect

00:33:00,630 --> 00:33:04,140
yeah yes so that and I have a little so

00:33:02,490 --> 00:33:05,790
in a couple more slides I go into

00:33:04,140 --> 00:33:07,200
batching and things like that which

00:33:05,790 --> 00:33:09,630
seems to resolve some of those issues

00:33:07,200 --> 00:33:13,950
okay jaan regarding the expansion stuff

00:33:09,630 --> 00:33:15,540
we talked about that and it's not quite

00:33:13,950 --> 00:33:17,850
clear yet what to do but it's clear that

00:33:15,540 --> 00:33:19,920
there is some problem with which needs

00:33:17,850 --> 00:33:21,510
to be solved and I guess we'll talk

00:33:19,920 --> 00:33:26,190
about that some more and come up with

00:33:21,510 --> 00:33:28,050
something yeah sure so yeah i'll get to

00:33:26,190 --> 00:33:33,420
that patch with the expansion problem in

00:33:28,050 --> 00:33:35,700
just a second so i wanted to understand

00:33:33,420 --> 00:33:37,590
you know the underlining for our hash

00:33:35,700 --> 00:33:40,860
tables if we start at four buckets and

00:33:37,590 --> 00:33:42,510
we did an ad so nft ad and we did that

00:33:40,860 --> 00:33:44,130
at one element at a time how bit how

00:33:42,510 --> 00:33:49,770
larger the table grow too and so it gets

00:33:44,130 --> 00:33:51,720
to 16,000 elements there and so what

00:33:49,770 --> 00:33:55,170
happens if I do a restore now and I have

00:33:51,720 --> 00:33:57,270
and I'm provided with a hint of a large

00:33:55,170 --> 00:33:58,500
enough hint at the beginning to say I'm

00:33:57,270 --> 00:34:01,230
going to start with a million buckets

00:33:58,500 --> 00:34:03,330
right and if I do a restore their then I

00:34:01,230 --> 00:34:06,870
get it down to 1.3 seconds to load a

00:34:03,330 --> 00:34:09,090
million entries and so I psets on about

00:34:06,870 --> 00:34:11,379
two seconds so we're you know now we're

00:34:09,090 --> 00:34:20,259
on par with IP set restore

00:34:11,379 --> 00:34:25,629
oh yeah you exactly yeah yeah so sorry

00:34:20,259 --> 00:34:27,730
no just indicating that that in NF

00:34:25,629 --> 00:34:29,980
stables we have transaction on support

00:34:27,730 --> 00:34:32,529
so we are also solving the problem that

00:34:29,980 --> 00:34:35,859
exactly you're enrolling in case that we

00:34:32,529 --> 00:34:38,710
have probably adding an an element so so

00:34:35,859 --> 00:34:42,579
better and faster yeah quite surprising

00:34:38,710 --> 00:34:46,509
I didn't know so here goes this goes

00:34:42,579 --> 00:34:49,509
into a little more detail what Thomas

00:34:46,509 --> 00:34:51,399
was talking about so the first problem

00:34:49,509 --> 00:34:53,859
that I hit was that we couldn't actually

00:34:51,399 --> 00:34:55,929
expand the hash table so the underlying

00:34:53,859 --> 00:35:00,190
data structure for the hash table n n of

00:34:55,929 --> 00:35:03,849
T sets is using our hash tables and so

00:35:00,190 --> 00:35:05,500
as of like a week ago the net net

00:35:03,849 --> 00:35:08,019
connect next implementation for an of T

00:35:05,500 --> 00:35:10,619
hash set well that this still is true I

00:35:08,019 --> 00:35:13,539
guess but it defaults to four buckets

00:35:10,619 --> 00:35:15,759
but it doesn't define a Mac shift

00:35:13,539 --> 00:35:17,529
parameter to our hash tables and if

00:35:15,759 --> 00:35:21,730
that's not defined then we can't grow

00:35:17,529 --> 00:35:23,680
right and so there's a check effectively

00:35:21,730 --> 00:35:25,000
in our hash tables that if max shift

00:35:23,680 --> 00:35:29,700
isn't defined we don't were not allowed

00:35:25,000 --> 00:35:32,769
to to grow the hash at that point so

00:35:29,700 --> 00:35:39,309
basically i push the patch to say you

00:35:32,769 --> 00:35:40,750
know in our hash tables that if ya to

00:35:39,309 --> 00:35:42,880
resolve basically this problem that if

00:35:40,750 --> 00:35:46,299
you don't provide a Mac shift parameter

00:35:42,880 --> 00:35:49,150
and you're trying to but you have to

00:35:46,299 --> 00:35:50,680
find a grow decision function that it it

00:35:49,150 --> 00:35:54,099
won't allow you to do so so that's what

00:35:50,680 --> 00:35:55,900
this this link links to that patch and I

00:35:54,099 --> 00:35:59,259
think basically agree that that was a

00:35:55,900 --> 00:36:03,579
valid case Daniel had some changes but I

00:35:59,259 --> 00:36:05,710
think that one will be easy to do the

00:36:03,579 --> 00:36:09,700
other problem though is that right now

00:36:05,710 --> 00:36:12,339
nft set right a few sets or nf2 said

00:36:09,700 --> 00:36:14,289
hashes has no way to really set an

00:36:12,339 --> 00:36:17,769
initial number of buckets so I know you

00:36:14,289 --> 00:36:20,470
have some thoughts on this but yoga

00:36:17,769 --> 00:36:23,049
might interrupt we used to have this way

00:36:20,470 --> 00:36:24,549
and it seems that fell victim to the our

00:36:23,049 --> 00:36:26,439
hash table okay well

00:36:24,549 --> 00:36:29,199
yeah we need to restore that all right

00:36:26,439 --> 00:36:32,439
yeah yeah so right now there's a size

00:36:29,199 --> 00:36:34,959
parameter for an ft sets and it's kind

00:36:32,439 --> 00:36:36,999
of used in two places so it's right now

00:36:34,959 --> 00:36:41,259
or currently in that next it's

00:36:36,999 --> 00:36:44,229
populating the NLM a hint for our hash

00:36:41,259 --> 00:36:45,579
tables which is supposed to state how

00:36:44,229 --> 00:36:47,559
many initial buckets you want to

00:36:45,579 --> 00:36:50,799
allocate for that table but it's also

00:36:47,559 --> 00:36:54,640
being used as the ceiling on how high

00:36:50,799 --> 00:36:56,529
the table in NN ft sets is being used as

00:36:54,640 --> 00:37:01,289
the ceiling on how high the set can grow

00:36:56,529 --> 00:37:04,089
so these two really don't overlap and so

00:37:01,289 --> 00:37:06,869
basically this this seemed incorrect and

00:37:04,089 --> 00:37:10,959
so I proposed kind of two patches here

00:37:06,869 --> 00:37:14,559
one is to use the size parameter to

00:37:10,959 --> 00:37:16,959
define Mac shift for nft hash sets so

00:37:14,559 --> 00:37:20,259
that it's consistent with the other ft

00:37:16,959 --> 00:37:22,989
code and I defaulted it too if you don't

00:37:20,259 --> 00:37:24,819
define a size parameter in the ruleset I

00:37:22,989 --> 00:37:26,769
defaulted it to a thousand elements I

00:37:24,819 --> 00:37:28,809
don't know that was just arbitrary

00:37:26,769 --> 00:37:32,439
number that seemed reasonable at the

00:37:28,809 --> 00:37:35,259
time so there's a link to that patch and

00:37:32,439 --> 00:37:38,529
then also to introduce a new parameter

00:37:35,259 --> 00:37:40,779
of which I just called an its size which

00:37:38,529 --> 00:37:45,609
probably isn't the greatest but to

00:37:40,779 --> 00:37:47,499
effectively plug in to the NLM hint

00:37:45,609 --> 00:37:51,400
which would then be passed to our hash

00:37:47,499 --> 00:37:53,349
table so that's those are the two and

00:37:51,400 --> 00:37:56,969
with those in place then we kind of

00:37:53,349 --> 00:38:02,559
start seeing some better better results

00:37:56,969 --> 00:38:05,259
so again if I if I do single ads here we

00:38:02,559 --> 00:38:07,900
still see some some slowdowns are not

00:38:05,259 --> 00:38:11,880
slowed up but just it's slow these are

00:38:07,900 --> 00:38:15,699
single invocations of n ft add element

00:38:11,880 --> 00:38:19,299
so this case it's 10,000 it still takes

00:38:15,699 --> 00:38:23,920
about three minutes but if I do a

00:38:19,299 --> 00:38:26,679
restore and this also a backup I gave an

00:38:23,920 --> 00:38:28,089
initial size hit here of 16,000 so that

00:38:26,679 --> 00:38:32,890
the hash table didn't have to grow

00:38:28,089 --> 00:38:35,650
throughout the the ad operations the

00:38:32,890 --> 00:38:38,130
other the other data here is did a

00:38:35,650 --> 00:38:41,650
restore of 10,000 elements were

00:38:38,130 --> 00:38:44,769
pretty fast there and then a restorer of

00:38:41,650 --> 00:38:46,269
a million elements using two million

00:38:44,769 --> 00:38:48,339
initial buckets which i think if i did

00:38:46,269 --> 00:38:51,309
an ad that's about what it would what

00:38:48,339 --> 00:38:53,920
our hash tables would probably set the

00:38:51,309 --> 00:38:56,410
bucket sizes too so and that's about 1.6

00:38:53,920 --> 00:39:00,430
seconds which is you know that's

00:38:56,410 --> 00:39:01,809
reasonable so revisiting ads so we still

00:39:00,430 --> 00:39:05,619
kind of have a problem with just the

00:39:01,809 --> 00:39:06,819
single ad case here I don't you know why

00:39:05,619 --> 00:39:10,539
do they take so long is there anything

00:39:06,819 --> 00:39:13,210
that we can do about it so the new

00:39:10,539 --> 00:39:15,819
patches add you know the number of

00:39:13,210 --> 00:39:19,299
initial buckets at least the ability to

00:39:15,819 --> 00:39:21,039
populate that and so even doing that

00:39:19,299 --> 00:39:23,440
doesn't seem to affect the performance

00:39:21,039 --> 00:39:27,210
noticeably at least in the single ad

00:39:23,440 --> 00:39:30,309
case i'll get to batching in a second so

00:39:27,210 --> 00:39:32,140
why do they take so long still I again

00:39:30,309 --> 00:39:35,380
it's something that I want to look into

00:39:32,140 --> 00:39:40,839
is it just the system call / ahead of

00:39:35,380 --> 00:39:44,170
calling this a million times I and so

00:39:40,839 --> 00:39:45,670
what are the alternatives here nft does

00:39:44,170 --> 00:39:48,729
support batching and so we can add

00:39:45,670 --> 00:39:52,239
multiple elements in a single instance

00:39:48,729 --> 00:39:54,640
right and so if i try to batch add a

00:39:52,239 --> 00:39:58,599
million entries when i do it in chunks

00:39:54,640 --> 00:40:05,859
of 2k or 4k then were able to get down

00:39:58,599 --> 00:40:07,869
to here we see 14 to 12 seconds and in

00:40:05,859 --> 00:40:09,579
this particular case though i did

00:40:07,869 --> 00:40:11,710
provide a bucket hint at the beginning

00:40:09,579 --> 00:40:14,259
of the when I created the set of a

00:40:11,710 --> 00:40:15,849
million entries if I don't do that we

00:40:14,259 --> 00:40:17,380
still see problems so if I don't provide

00:40:15,849 --> 00:40:18,609
the hint there we start with four

00:40:17,380 --> 00:40:20,469
buckets we still can't grow so there's

00:40:18,609 --> 00:40:22,690
still the issue that Thomas was

00:40:20,469 --> 00:40:24,969
mentioning earlier okay video did you

00:40:22,690 --> 00:40:27,069
actually change min the minimum shift

00:40:24,969 --> 00:40:29,769
parameter as well I noticed yesterday

00:40:27,069 --> 00:40:33,249
that if the minimum shift parameters

00:40:29,769 --> 00:40:35,469
below your your elements hint when

00:40:33,249 --> 00:40:36,819
you're adding you will actually start

00:40:35,469 --> 00:40:40,180
shrinking because your number of

00:40:36,819 --> 00:40:42,249
elements is below okay so here that

00:40:40,180 --> 00:40:45,400
needs to be fixed either by providing

00:40:42,249 --> 00:40:47,069
out like a patch that min shift is

00:40:45,400 --> 00:40:49,019
always

00:40:47,069 --> 00:40:50,670
at least in this comments hints yeah

00:40:49,019 --> 00:40:53,489
otherwise you will start drinking right

00:40:50,670 --> 00:40:55,589
away yeah I definitely saw that I didn't

00:40:53,489 --> 00:40:57,509
notice it in this case but I definitely

00:40:55,589 --> 00:40:59,849
saw that if you provide too large of a

00:40:57,509 --> 00:41:01,769
hint and you're not using it our hash

00:40:59,849 --> 00:41:05,069
tables will start shrinking basically

00:41:01,769 --> 00:41:06,150
immediately but now I didn't update the

00:41:05,069 --> 00:41:08,640
men shift here so that's definitely

00:41:06,150 --> 00:41:11,670
something to try on that so did you have

00:41:08,640 --> 00:41:14,219
a I think that's an interesting question

00:41:11,670 --> 00:41:15,869
because the hint NF tab is used to have

00:41:14,219 --> 00:41:17,519
this hint and it was meant if you

00:41:15,869 --> 00:41:19,709
specify a size in user space it was

00:41:17,519 --> 00:41:21,630
meant that you are actually intending to

00:41:19,709 --> 00:41:24,150
use that so we should only invoke the

00:41:21,630 --> 00:41:26,430
string to sit in the swing decision if

00:41:24,150 --> 00:41:28,499
you're deleting elements not by adding

00:41:26,430 --> 00:41:30,329
elements I mean currently the our hash

00:41:28,499 --> 00:41:32,759
table implementation checks both

00:41:30,329 --> 00:41:34,859
whenever you change anything so it will

00:41:32,759 --> 00:41:37,529
shrink even if you add something which

00:41:34,859 --> 00:41:40,979
probably doesn't make too much sense we

00:41:37,529 --> 00:41:43,079
can remove that I think and it might

00:41:40,979 --> 00:41:44,880
also make sense to just say I don't want

00:41:43,079 --> 00:41:50,699
to go below just have a min shift like

00:41:44,880 --> 00:41:53,609
you suggested alright cool so what's

00:41:50,699 --> 00:41:56,489
sorry sorry why do you want why are you

00:41:53,609 --> 00:41:58,650
batching those median entries in

00:41:56,489 --> 00:42:01,619
different why are you placing why are

00:41:58,650 --> 00:42:03,569
you facing those those updates in

00:42:01,619 --> 00:42:07,739
different batches I mean in a small

00:42:03,569 --> 00:42:09,599
batches right so I wanted to see if I

00:42:07,739 --> 00:42:12,660
could take advantage of the net link

00:42:09,599 --> 00:42:15,329
messages right so no matter that is not

00:42:12,660 --> 00:42:17,069
I mean any tables internally but it

00:42:15,329 --> 00:42:20,579
happens is every time did you send a

00:42:17,069 --> 00:42:23,009
batch tip at the end I mean it's going

00:42:20,579 --> 00:42:24,539
to except two phase commit protocol

00:42:23,009 --> 00:42:26,219
right but if you see here that the time

00:42:24,539 --> 00:42:28,799
actually starts getting worse as I get

00:42:26,219 --> 00:42:30,839
larger batch sizes if i did a million

00:42:28,799 --> 00:42:32,069
entries all at once it's still it's

00:42:30,839 --> 00:42:33,359
gonna be like three minutes again I

00:42:32,069 --> 00:42:36,180
whatever it is I do you think that I

00:42:33,359 --> 00:42:38,459
think yeah but anyway I mean that double

00:42:36,180 --> 00:42:40,410
resolving several calls of synchronized

00:42:38,459 --> 00:42:42,690
our see you in a row and sure you yes

00:42:40,410 --> 00:42:44,190
that's usually very bad so yeah there's

00:42:42,690 --> 00:42:46,049
definitely something here with that and

00:42:44,190 --> 00:42:47,729
it's probably tied to why adding a

00:42:46,049 --> 00:42:50,069
million single entries takes a long time

00:42:47,729 --> 00:42:53,910
so the more investigation needs to be

00:42:50,069 --> 00:42:56,819
done into this yes so I think it's just

00:42:53,910 --> 00:42:58,880
a summary slide so four thousand entries

00:42:56,819 --> 00:43:01,830
took about 12 seconds single element

00:42:58,880 --> 00:43:04,060
takes many minutes

00:43:01,830 --> 00:43:06,460
so the batching allows you to combine

00:43:04,060 --> 00:43:09,370
multiple entries into single net link

00:43:06,460 --> 00:43:11,500
message and I guess you know reduce this

00:43:09,370 --> 00:43:14,920
is called overhead there so here's kind

00:43:11,500 --> 00:43:19,060
of a summary of where things were when I

00:43:14,920 --> 00:43:22,090
last left them you know I pset add of a

00:43:19,060 --> 00:43:24,580
single element a million times or you

00:43:22,090 --> 00:43:27,520
know a million entries just takes about

00:43:24,580 --> 00:43:31,330
five five and a half minutes a restore

00:43:27,520 --> 00:43:35,950
though is about two seconds single adds

00:43:31,330 --> 00:43:39,670
of a million entries so nft add takes a

00:43:35,950 --> 00:43:41,200
long time if I am I batch that though I

00:43:39,670 --> 00:43:45,730
can get it down to 12 seconds and

00:43:41,200 --> 00:43:47,710
they're still probably you know a way to

00:43:45,730 --> 00:43:51,160
get that maybe even further down and

00:43:47,710 --> 00:43:55,810
then the restore I can get that too with

00:43:51,160 --> 00:43:58,870
the patches to 1.6 seconds and so it's

00:43:55,810 --> 00:44:02,020
on par with and as Pablo point out

00:43:58,870 --> 00:44:07,990
better than then I pset because we have

00:44:02,020 --> 00:44:09,220
that Adam icity stuff so yeah so this is

00:44:07,990 --> 00:44:11,110
just basically kind of our initial

00:44:09,220 --> 00:44:15,400
impressions of what we played around

00:44:11,110 --> 00:44:16,900
with with enough tables sets you know

00:44:15,400 --> 00:44:18,550
with the patches everything the

00:44:16,900 --> 00:44:22,030
performance for restore definitely seems

00:44:18,550 --> 00:44:23,590
except acceptable the atomicity is

00:44:22,030 --> 00:44:26,620
definitely a big win for us I know

00:44:23,590 --> 00:44:31,120
that's a big problem for Pete's team the

00:44:26,620 --> 00:44:34,390
ability to combine v4b 6 rule sets will

00:44:31,120 --> 00:44:37,570
reduce duplicate code for us is there a

00:44:34,390 --> 00:44:42,400
way to combine v4 and v6 entries in the

00:44:37,570 --> 00:44:44,080
same set oh you can't do that on the you

00:44:42,400 --> 00:44:47,400
have to specify the type and it's going

00:44:44,080 --> 00:44:50,440
to be either before before or v6 right

00:44:47,400 --> 00:44:54,640
this isn't exactly set related but this

00:44:50,440 --> 00:44:58,830
is enough tables related do you guys

00:44:54,640 --> 00:45:01,720
mentioned last night that the inet table

00:44:58,830 --> 00:45:05,230
type or whatever there was a restriction

00:45:01,720 --> 00:45:08,950
on that can do can you repeat what the

00:45:05,230 --> 00:45:10,480
changing the inet the NH anti yeah the

00:45:08,950 --> 00:45:13,130
restrictions I mean there are no

00:45:10,480 --> 00:45:16,490
restrictions we don't have

00:45:13,130 --> 00:45:19,039
I think for the ina table so far for the

00:45:16,490 --> 00:45:21,740
high net family and we don't have the

00:45:19,039 --> 00:45:23,750
route chain property so no automatic

00:45:21,740 --> 00:45:25,460
rerouting of the packet remarking now

00:45:23,750 --> 00:45:27,109
all you feel doing it's basically only

00:45:25,460 --> 00:45:29,150
filtering what is supported currently

00:45:27,109 --> 00:45:30,650
okay your net would be easy to add I

00:45:29,150 --> 00:45:33,140
mean if you need it we can add it it's

00:45:30,650 --> 00:45:36,500
yeah but so we'd be able to combine v4

00:45:33,140 --> 00:45:38,480
and v6 rule sets into a single rule set

00:45:36,500 --> 00:45:42,200
at this point right yeah right okay

00:45:38,480 --> 00:45:44,539
that's why that you can also you can

00:45:42,200 --> 00:45:46,970
also it's bit in a way that allows you

00:45:44,539 --> 00:45:48,829
to well basically do what you would

00:45:46,970 --> 00:45:50,450
expect you can specify rules which will

00:45:48,829 --> 00:45:53,359
apply to both families like if you use

00:45:50,450 --> 00:45:56,000
just GCPD port 22 for instance we will

00:45:53,359 --> 00:45:58,880
apply to both v4 and v6 packets with

00:45:56,000 --> 00:46:01,369
roland right that's okay yeah and

00:45:58,880 --> 00:46:05,029
regarding having having sets with ipv4

00:46:01,369 --> 00:46:08,539
and ipv6 addresses the main problem is

00:46:05,029 --> 00:46:11,450
that we have the payload expression

00:46:08,539 --> 00:46:14,690
basically receives a base offset and

00:46:11,450 --> 00:46:16,940
length and in the ina in the inet family

00:46:14,690 --> 00:46:18,920
will need some kind of something similar

00:46:16,940 --> 00:46:22,579
to what we did with reject some mapping

00:46:18,920 --> 00:46:25,069
so we can specify not based on on base

00:46:22,579 --> 00:46:27,049
offset but based on attack that says I

00:46:25,069 --> 00:46:28,819
want to match this nation address IP a

00:46:27,049 --> 00:46:30,289
destination address and then well no

00:46:28,819 --> 00:46:31,910
it's two different problems I mean the

00:46:30,289 --> 00:46:35,569
one thing is loading the address from

00:46:31,910 --> 00:46:37,250
the packet this can well we know that we

00:46:35,569 --> 00:46:39,650
do it like we do before we load four

00:46:37,250 --> 00:46:42,349
bytes for before and sixteen bytes for

00:46:39,650 --> 00:46:46,250
v6 the problem is representation and

00:46:42,349 --> 00:46:47,930
comparability in the set you will I mean

00:46:46,250 --> 00:46:50,240
it's just bit strings you will need some

00:46:47,930 --> 00:46:53,059
mapping to basically make sure of before

00:46:50,240 --> 00:46:55,849
address isn't interpreted as a some kind

00:46:53,059 --> 00:46:58,130
of b6 address which is just mapped so

00:46:55,849 --> 00:47:00,259
it's not about loading from the packet I

00:46:58,130 --> 00:47:02,029
mean you loaded like always but it's

00:47:00,259 --> 00:47:03,710
about having this different data types

00:47:02,029 --> 00:47:06,559
in the set and making sure they're not

00:47:03,710 --> 00:47:08,809
mixed up while comparing we most likely

00:47:06,559 --> 00:47:10,180
can do it in some way but so far it

00:47:08,809 --> 00:47:14,720
hasn't been high on the priority list

00:47:10,180 --> 00:47:16,430
okay the next point is something that I

00:47:14,720 --> 00:47:18,500
talked to Patrick about last night so I

00:47:16,430 --> 00:47:20,000
wasn't able to get a set to choose our

00:47:18,500 --> 00:47:22,759
bee tree but it seems like it requires

00:47:20,000 --> 00:47:24,680
intervals is that yeah the RB tree I

00:47:22,759 --> 00:47:25,390
mean it's in quite inefficient set

00:47:24,680 --> 00:47:28,180
implementation

00:47:25,390 --> 00:47:31,690
it is mainly it exists because for

00:47:28,180 --> 00:47:33,820
testing interval-based sets and what you

00:47:31,690 --> 00:47:36,880
obviously can't do with hash tables so

00:47:33,820 --> 00:47:38,590
the RP tree is basically the most was

00:47:36,880 --> 00:47:40,360
the easiest way to have an

00:47:38,590 --> 00:47:41,980
implementation which actually supports

00:47:40,360 --> 00:47:43,600
that so as soon as you start using

00:47:41,980 --> 00:47:45,670
intervals the RP tree you will be

00:47:43,600 --> 00:47:47,770
automatically chosen okay because of its

00:47:45,670 --> 00:47:49,210
high memory requirements it is not

00:47:47,770 --> 00:47:53,500
chosen automatically for any different

00:47:49,210 --> 00:47:56,470
setup sure and so I one of the questions

00:47:53,500 --> 00:47:59,830
that we had going into you know the

00:47:56,470 --> 00:48:04,590
underlying supported data structures

00:47:59,830 --> 00:48:06,760
there is well I guess two things one is

00:48:04,590 --> 00:48:08,800
what are the thoughts on being able to

00:48:06,760 --> 00:48:12,240
provide a hint or to be able to force

00:48:08,800 --> 00:48:15,610
something to take a particular set type

00:48:12,240 --> 00:48:17,950
why would you use that I mean basically

00:48:15,610 --> 00:48:19,720
this means that our selection process

00:48:17,950 --> 00:48:23,320
would be broken because we promised to

00:48:19,720 --> 00:48:26,200
select something reasonable for you okay

00:48:23,320 --> 00:48:28,810
I mean you actually can do that you can

00:48:26,200 --> 00:48:32,050
load what is happening in the kernel

00:48:28,810 --> 00:48:35,320
adjust as a it loads all modules which

00:48:32,050 --> 00:48:36,970
areas to a tea set and once this has

00:48:35,320 --> 00:48:39,100
happened so you can unload the modules

00:48:36,970 --> 00:48:41,650
you don't want like if your unload nft

00:48:39,100 --> 00:48:43,720
hash it won't try to load modules again

00:48:41,650 --> 00:48:46,000
because one module providing nfg hash is

00:48:43,720 --> 00:48:47,860
present which would be rb3 at that point

00:48:46,000 --> 00:48:51,490
to go always I definitely saw that yeah

00:48:47,860 --> 00:48:54,040
yeah the other questions that we had was

00:48:51,490 --> 00:48:55,480
you know and I do you alluded to this

00:48:54,040 --> 00:48:58,630
last night as probably in your slides

00:48:55,480 --> 00:49:00,370
later but are open to adding other I

00:48:58,630 --> 00:49:02,410
mean the hooks are definitely there to

00:49:00,370 --> 00:49:04,300
provide other data structures to be used

00:49:02,410 --> 00:49:06,090
instead of a hash or RB tree or whatever

00:49:04,300 --> 00:49:08,260
I think we're interested in looking at

00:49:06,090 --> 00:49:11,560
alternatives there so yeah sure I

00:49:08,260 --> 00:49:14,980
mentioned I have it's called a remap try

00:49:11,560 --> 00:49:17,050
and shoot up for years now um it's

00:49:14,980 --> 00:49:18,280
almost finished four years now I'm going

00:49:17,050 --> 00:49:20,980
to submit it at some point but we are

00:49:18,280 --> 00:49:23,020
open to adding well this is why we have

00:49:20,980 --> 00:49:25,780
this infrastructure we would like to

00:49:23,020 --> 00:49:28,030
have different ideas I think we'd be

00:49:25,780 --> 00:49:30,430
interested in testing that it's just a

00:49:28,030 --> 00:49:34,690
comment on the user or how the don't

00:49:30,430 --> 00:49:37,190
think that makes one just

00:49:34,690 --> 00:49:40,160
alright test just to comment on how the

00:49:37,190 --> 00:49:41,980
hatch type is selected I think our users

00:49:40,160 --> 00:49:44,330
are really interested in knowing exactly

00:49:41,980 --> 00:49:46,310
which type is going to be selected so

00:49:44,330 --> 00:49:48,520
they could say beforehand I want to

00:49:46,310 --> 00:49:51,830
explicitly use this time they'd be happy

00:49:48,520 --> 00:49:53,750
absolutely they know well why I'm

00:49:51,830 --> 00:49:55,340
interested in understanding why you

00:49:53,750 --> 00:49:57,860
would be interested in having one

00:49:55,340 --> 00:49:59,420
particular implementation if from the

00:49:57,860 --> 00:50:02,930
user point of view it's basically makes

00:49:59,420 --> 00:50:04,880
no difference and we promise to select

00:50:02,930 --> 00:50:07,130
based on the policy you specify memory

00:50:04,880 --> 00:50:09,770
performance usage we promise to select

00:50:07,130 --> 00:50:12,080
one which is optimal for your data I

00:50:09,770 --> 00:50:14,570
just from the engineers that we work

00:50:12,080 --> 00:50:18,080
with they will want to know exactly what

00:50:14,570 --> 00:50:19,370
it is and have full control so that yeah

00:50:18,080 --> 00:50:21,830
that thing's not swapped out between

00:50:19,370 --> 00:50:23,210
releases or something right so I guess

00:50:21,830 --> 00:50:26,180
you could think of an example where

00:50:23,210 --> 00:50:29,570
maybe we implement something new in the

00:50:26,180 --> 00:50:33,820
next kernel and that gets chosen now for

00:50:29,570 --> 00:50:35,630
this particular set type but and yeah

00:50:33,820 --> 00:50:37,100
maybe not something that they were

00:50:35,630 --> 00:50:38,780
expecting other that they've fully

00:50:37,100 --> 00:50:39,980
tested because they were assuming you

00:50:38,780 --> 00:50:41,750
know they're making assumptions based

00:50:39,980 --> 00:50:44,060
that we would still be choosing a hash

00:50:41,750 --> 00:50:46,070
type not the array try or whatever yeah

00:50:44,060 --> 00:50:49,340
I see I mean basically what you can do

00:50:46,070 --> 00:50:51,140
is load the modules manually or

00:50:49,340 --> 00:50:53,390
blacklist modules which is basically one

00:50:51,140 --> 00:50:56,060
make a way to choose it we don't have a

00:50:53,390 --> 00:50:58,610
way to express it in the API on this

00:50:56,060 --> 00:51:01,040
also there is some reason for that we

00:50:58,610 --> 00:51:02,630
wanted to have the freedom to remove set

00:51:01,040 --> 00:51:04,130
implementations from the colonel okay

00:51:02,630 --> 00:51:06,050
replace them and basically have the

00:51:04,130 --> 00:51:07,700
colonel make the selection without any

00:51:06,050 --> 00:51:09,320
exposure to user space so when we add

00:51:07,700 --> 00:51:12,890
something new which we think is better

00:51:09,320 --> 00:51:14,570
than the previous implementation so it

00:51:12,890 --> 00:51:16,490
will automatically use it without any

00:51:14,570 --> 00:51:18,520
changes to user space and once we start

00:51:16,490 --> 00:51:21,110
exposing that stuff we are limited to

00:51:18,520 --> 00:51:25,300
changing behavior removing

00:51:21,110 --> 00:51:28,630
implementations etc so I would prefer to

00:51:25,300 --> 00:51:31,400
unstick to dealing with modules for

00:51:28,630 --> 00:51:37,040
chosing choosing which using instead of

00:51:31,400 --> 00:51:40,430
exposing it in the API okay that is

00:51:37,040 --> 00:51:42,950
still live don't don't allow don't allow

00:51:40,430 --> 00:51:44,450
people to select to caramelize I mean we

00:51:42,950 --> 00:51:48,950
are supposed to provide a very good

00:51:44,450 --> 00:51:50,030
solution but why I'm I kind of a check

00:51:48,950 --> 00:51:54,050
in the feeling that at some point we

00:51:50,030 --> 00:51:55,460
will have to to provide so details so i

00:51:54,050 --> 00:51:57,740
guess most people we just use

00:51:55,460 --> 00:52:01,430
performance or memory just the policy

00:51:57,740 --> 00:52:05,690
but i mean an for example the problem

00:52:01,430 --> 00:52:07,250
that that George had and the attribute

00:52:05,690 --> 00:52:09,530
that he needed to add is it's not

00:52:07,250 --> 00:52:10,910
overlapping actually ride with yeah I

00:52:09,530 --> 00:52:13,610
don't know if it overlaps with any other

00:52:10,910 --> 00:52:15,110
set I mean types you are specifying the

00:52:13,610 --> 00:52:17,480
number of the number of buckets and the

00:52:15,110 --> 00:52:19,160
size attribute that we have indicated

00:52:17,480 --> 00:52:21,590
maximum number of elements right well it

00:52:19,160 --> 00:52:24,230
was all sure it was meant as a hint the

00:52:21,590 --> 00:52:26,090
hint is enforced as a maximum also you

00:52:24,230 --> 00:52:28,400
specify the maximum but is it is

00:52:26,090 --> 00:52:30,710
expected to be actually used so it used

00:52:28,400 --> 00:52:32,750
to be used to actually allocate in the

00:52:30,710 --> 00:52:35,240
hash table case a set of that amount of

00:52:32,750 --> 00:52:38,180
that size it's also enforced as a

00:52:35,240 --> 00:52:40,910
maximum just in case there might be set

00:52:38,180 --> 00:52:42,530
implementations in the future which you

00:52:40,910 --> 00:52:45,080
where you have to know the size from the

00:52:42,530 --> 00:52:47,120
beginning so um because the colonel

00:52:45,080 --> 00:52:48,890
automatically chooses this transparently

00:52:47,120 --> 00:52:50,810
the user doesn't notice I didn't want to

00:52:48,890 --> 00:52:52,130
get into your situation where the

00:52:50,810 --> 00:52:54,860
colonel might choose an implementation

00:52:52,130 --> 00:52:56,680
which really has hard limits on this

00:52:54,860 --> 00:52:58,910
hint and other stones so you get

00:52:56,680 --> 00:53:00,710
inconsistent behavior so I chose to

00:52:58,910 --> 00:53:02,810
always enforce it as a maximum even if

00:53:00,710 --> 00:53:04,790
not necessary but it's meant to be as a

00:53:02,810 --> 00:53:07,850
hint it was actually meant for exactly

00:53:04,790 --> 00:53:10,700
this purpose and it's basically a bug

00:53:07,850 --> 00:53:13,700
that it's not working right now it okay

00:53:10,700 --> 00:53:15,740
well but then would you what would you

00:53:13,700 --> 00:53:17,690
use to set the ceiling on the size of

00:53:15,740 --> 00:53:18,830
the set or would you want to enforce

00:53:17,690 --> 00:53:24,590
that something and through something

00:53:18,830 --> 00:53:28,130
else well I would use the same value for

00:53:24,590 --> 00:53:30,080
both actually um I mean it specifies the

00:53:28,130 --> 00:53:32,450
size so I don't know what you want I

00:53:30,080 --> 00:53:36,020
don't know that you want to say I have a

00:53:32,450 --> 00:53:37,430
set that I'd like to cap at 25 million

00:53:36,020 --> 00:53:39,980
entries but I don't want to start the

00:53:37,430 --> 00:53:41,630
hash off it toilet or whatever buckets

00:53:39,980 --> 00:53:42,890
we could use because especially because

00:53:41,630 --> 00:53:45,830
it's going to start shrinking

00:53:42,890 --> 00:53:48,440
immediately that's also something which

00:53:45,830 --> 00:53:51,530
it shouldn't do I guess well yeah yeah

00:53:48,440 --> 00:53:51,920
so if we fix that I mean you're right it

00:53:51,530 --> 00:53:53,060
might be

00:53:51,920 --> 00:53:54,740
reasonable to add two different

00:53:53,060 --> 00:53:56,870
parameters for that because I think you

00:53:54,740 --> 00:53:58,280
lose something with our hash tables

00:53:56,870 --> 00:54:01,580
being able to shrink and expand as

00:53:58,280 --> 00:54:03,320
needed by forcing it to stay at okay I'm

00:54:01,580 --> 00:54:04,760
going to need 25 million buckets at some

00:54:03,320 --> 00:54:06,530
point so I'm going to go ahead allocate

00:54:04,760 --> 00:54:08,180
those now yeah sure the use case was

00:54:06,530 --> 00:54:10,550
actually a little bit different was

00:54:08,180 --> 00:54:12,170
meant four sets where the entire

00:54:10,550 --> 00:54:14,900
contents are known from the beginning in

00:54:12,170 --> 00:54:16,370
userspace can make this analysis we have

00:54:14,900 --> 00:54:18,320
these men and there's many elements so

00:54:16,370 --> 00:54:20,690
we're going to get a hash table of

00:54:18,320 --> 00:54:22,580
exactly the size of affray but it's

00:54:20,690 --> 00:54:26,470
different of course we can add a hidden

00:54:22,580 --> 00:54:29,090
basically um for the initial size and

00:54:26,470 --> 00:54:37,600
keep the current size specification as a

00:54:29,090 --> 00:54:37,600
cab okay let's see so where are we here

00:54:38,650 --> 00:54:42,080
another thing that we kind of came

00:54:40,370 --> 00:54:45,920
across and I think you guys talked about

00:54:42,080 --> 00:54:48,200
this yesterday we wanted to clarify so I

00:54:45,920 --> 00:54:50,720
noticed that storing like really large

00:54:48,200 --> 00:54:52,640
sets in a single file becomes cumbersome

00:54:50,720 --> 00:54:57,110
because you've got you have your rule

00:54:52,640 --> 00:54:58,580
set and then you have how many lines of

00:54:57,110 --> 00:55:00,560
a million entries and then maybe you

00:54:58,580 --> 00:55:02,240
have say you have 20 sets that have

00:55:00,560 --> 00:55:06,260
million entries each all stored in a

00:55:02,240 --> 00:55:08,540
single file so you mentioned i think the

00:55:06,260 --> 00:55:10,340
possibility to do like includes stuff

00:55:08,540 --> 00:55:12,470
like that with the rule set is that

00:55:10,340 --> 00:55:13,970
possible you have an include statement

00:55:12,470 --> 00:55:16,250
you can simply read in different files

00:55:13,970 --> 00:55:19,640
on so you could move your sets own

00:55:16,250 --> 00:55:21,440
definitions all to different files this

00:55:19,640 --> 00:55:24,530
will only work you will have to have the

00:55:21,440 --> 00:55:26,090
entire set either the commands or the

00:55:24,530 --> 00:55:27,740
entire set definition including the

00:55:26,090 --> 00:55:29,240
elements in a second flight you

00:55:27,740 --> 00:55:30,740
basically you can do it include in the

00:55:29,240 --> 00:55:33,770
middle of a statement to just read

00:55:30,740 --> 00:55:35,270
entries from a file which only contains

00:55:33,770 --> 00:55:38,630
for instance addresses and nothing else

00:55:35,270 --> 00:55:40,250
no markups or whatever okay we could

00:55:38,630 --> 00:55:42,230
also add something like that so you can

00:55:40,250 --> 00:55:43,820
just use a list to read it in and

00:55:42,230 --> 00:55:47,390
treated a single elements or something

00:55:43,820 --> 00:55:49,370
with help so I mean can you reference so

00:55:47,390 --> 00:55:51,800
instead of so I know the rule you

00:55:49,370 --> 00:55:53,840
reference to set with whatever a at set

00:55:51,800 --> 00:55:56,210
name or whatever it is and then at the

00:55:53,840 --> 00:55:59,060
top of the rule file if I could just say

00:55:56,210 --> 00:56:00,620
you know include the set nail basically

00:55:59,060 --> 00:56:02,870
what everybody what it does is you

00:56:00,620 --> 00:56:04,010
indicate include at the beginning of the

00:56:02,870 --> 00:56:06,900
file

00:56:04,010 --> 00:56:09,450
that file is going to contain a table

00:56:06,900 --> 00:56:12,000
definition and the sets that you want to

00:56:09,450 --> 00:56:15,060
load in one file so you in one side you

00:56:12,000 --> 00:56:16,740
maintain we want a lot of different file

00:56:15,060 --> 00:56:19,110
you are going to have the root the rules

00:56:16,740 --> 00:56:21,240
actually that are referencing to that to

00:56:19,110 --> 00:56:22,680
that said that you want to use so just

00:56:21,240 --> 00:56:24,540
just in case you want to maintain two

00:56:22,680 --> 00:56:25,950
different files one is going likely one

00:56:24,540 --> 00:56:28,650
is likely not going to change that

00:56:25,950 --> 00:56:30,480
contain the ruleset and the other file

00:56:28,650 --> 00:56:33,180
is going to happen to change that means

00:56:30,480 --> 00:56:36,420
they rule the pset elements by the IP

00:56:33,180 --> 00:56:39,990
addresses okay all right we'll play

00:56:36,420 --> 00:56:40,950
around with that then and I think you

00:56:39,990 --> 00:56:43,620
guys are going to talk about this later

00:56:40,950 --> 00:56:45,990
too but just wondering the plans as far

00:56:43,620 --> 00:56:49,050
as enough to set implementation if it's

00:56:45,990 --> 00:56:51,960
the plan is to reach kind of feature

00:56:49,050 --> 00:56:54,270
parity with IP sets in relation respect

00:56:51,960 --> 00:56:57,540
to you know set types and things along

00:56:54,270 --> 00:57:00,360
those lines well feature wise of course

00:56:57,540 --> 00:57:02,040
we want the dynamic updates removals

00:57:00,360 --> 00:57:03,720
timeouts basically people have been

00:57:02,040 --> 00:57:05,250
asking for that and I have patches cute

00:57:03,720 --> 00:57:08,790
i'm going to talk about that later okay

00:57:05,250 --> 00:57:10,710
regarding set types I mean n if tables

00:57:08,790 --> 00:57:13,080
is quite different from IP sets so we

00:57:10,710 --> 00:57:15,630
are not going to add millions of

00:57:13,080 --> 00:57:17,250
different types based on the data we and

00:57:15,630 --> 00:57:18,780
of course but we have as I was

00:57:17,250 --> 00:57:20,900
mentioning we're quite interested in

00:57:18,780 --> 00:57:24,000
getting different deployment ations so

00:57:20,900 --> 00:57:26,310
which are I mean there might be many

00:57:24,000 --> 00:57:29,450
possibilities to optimize for specific

00:57:26,310 --> 00:57:32,160
key lengths for small or bigger set

00:57:29,450 --> 00:57:34,560
larger set contents and so on we're

00:57:32,160 --> 00:57:36,420
quite interested in having many of these

00:57:34,560 --> 00:57:41,640
implementations which are optimized for

00:57:36,420 --> 00:57:44,550
specific cases okay so we have kind of

00:57:41,640 --> 00:57:48,270
the general types of v4 dress v6 you

00:57:44,550 --> 00:57:49,920
know ports and things like that is there

00:57:48,270 --> 00:57:51,660
is it supportive right now to combine

00:57:49,920 --> 00:57:53,760
them into a single set to have multiple

00:57:51,660 --> 00:57:56,730
types defined in assets so you even

00:57:53,760 --> 00:58:00,390
emulate like hash IP port or hash net

00:57:56,730 --> 00:58:02,600
net or something so you want to have

00:58:00,390 --> 00:58:05,700
multiple data types and one set right

00:58:02,600 --> 00:58:08,010
yeah well yeah so right now for free

00:58:05,700 --> 00:58:09,990
like I said examples you know hash IP

00:58:08,010 --> 00:58:12,390
port I need to be able to find a hash

00:58:09,990 --> 00:58:14,370
that that matches against this IP and

00:58:12,390 --> 00:58:15,950
the port number yeah I have a feature

00:58:14,370 --> 00:58:17,660
queued up which

00:58:15,950 --> 00:58:18,950
is we wanted to have this from the

00:58:17,660 --> 00:58:21,349
beginning it took a while to get it

00:58:18,950 --> 00:58:23,570
ready but I'm basically it's ready it

00:58:21,349 --> 00:58:25,490
will be submitted for the next kernel

00:58:23,570 --> 00:58:27,170
version it's called concatenations where

00:58:25,490 --> 00:58:30,770
you can simply concatenate different

00:58:27,170 --> 00:58:34,190
keys so you can say IP source address

00:58:30,770 --> 00:58:36,410
dot tcpd port for instance in the world

00:58:34,190 --> 00:58:38,450
concatenate those data values in the

00:58:36,410 --> 00:58:39,980
colonel okay and your keys will also be

00:58:38,450 --> 00:58:41,960
concatenated in the set so you can

00:58:39,980 --> 00:58:44,060
basically combine any kind of Giga

00:58:41,960 --> 00:58:46,280
create any type of you set type that you

00:58:44,060 --> 00:58:48,260
oh yes and actually it basically you can

00:58:46,280 --> 00:58:50,720
dynamically instantiate new types by

00:58:48,260 --> 00:58:53,210
specifying concatenations and you can

00:58:50,720 --> 00:58:56,359
combine any kind of key you want ok cool

00:58:53,210 --> 00:58:58,550
is there going to be I mean this is

00:58:56,359 --> 00:59:00,500
probably just an implement user

00:58:58,550 --> 00:59:02,119
implementation but that you let the list

00:59:00,500 --> 00:59:03,920
set type is that going to be something

00:59:02,119 --> 00:59:06,619
that will be I'm not sure what's with

00:59:03,920 --> 00:59:08,750
what what is it doing so right now a

00:59:06,619 --> 00:59:12,070
list set is effectively I think it's a

00:59:08,750 --> 00:59:16,820
linked list of different set right I

00:59:12,070 --> 00:59:20,119
have not considered that do you what do

00:59:16,820 --> 00:59:21,619
you use this for I know that we are

00:59:20,119 --> 00:59:27,250
using it i don't remember the use case

00:59:21,619 --> 00:59:29,540
off the top of my head but I I think

00:59:27,250 --> 00:59:30,980
yeah i'll have to check and get back to

00:59:29,540 --> 00:59:33,440
you i don't know i mean we could

00:59:30,980 --> 00:59:35,750
certainly add that i wouldn't want to

00:59:33,440 --> 00:59:38,300
add it on the top level because we would

00:59:35,750 --> 00:59:40,160
have two special case that stuff okay so

00:59:38,300 --> 00:59:42,099
we might be able to add a set type which

00:59:40,160 --> 00:59:44,660
basically just dispatch us stuff to

00:59:42,099 --> 00:59:48,339
multiple sets or should be possible i

00:59:44,660 --> 00:59:51,290
guess if there's a valid use case okay

00:59:48,339 --> 00:59:55,310
let's see oh and this kind of came up

00:59:51,290 --> 00:59:57,319
earlier to but so the more i think like

00:59:55,310 --> 00:59:58,790
p was saying from our perspective the

00:59:57,319 --> 01:00:01,609
more user configurable the sets

00:59:58,790 --> 01:00:04,940
information is kind of the more valuable

01:00:01,609 --> 01:00:08,690
it becomes for us whether it's control

01:00:04,940 --> 01:00:10,369
or whatever you know even if it's only

01:00:08,690 --> 01:00:12,440
done at the library level at the api

01:00:10,369 --> 01:00:15,349
level and not exposed maybe through the

01:00:12,440 --> 01:00:16,510
command line utility but the ability to

01:00:15,349 --> 01:00:19,750
do

01:00:16,510 --> 01:00:21,010
you know setting like we said earlier

01:00:19,750 --> 01:00:23,350
the initial number of initial hash

01:00:21,010 --> 01:00:26,170
buckets but maybe even setting grow and

01:00:23,350 --> 01:00:28,990
shrink thresholds things along those

01:00:26,170 --> 01:00:31,390
lines so that you could specify if you

01:00:28,990 --> 01:00:32,530
don't want the default of 75 30 or

01:00:31,390 --> 01:00:34,540
whatever it is we could specify

01:00:32,530 --> 01:00:36,070
something there and just put a generic

01:00:34,540 --> 01:00:39,400
function in the kernel that yeah I can

01:00:36,070 --> 01:00:41,410
see that it's useful um but I'm I'm not

01:00:39,400 --> 01:00:43,590
so sure about exposing implementation

01:00:41,410 --> 01:00:46,450
specifics in the API because I'm

01:00:43,590 --> 01:00:48,220
basically it's meant to choose an

01:00:46,450 --> 01:00:53,230
implementation which might not even have

01:00:48,220 --> 01:00:55,420
any notion of these limits so I would

01:00:53,230 --> 01:00:58,350
prefer not true actually if we can find

01:00:55,420 --> 01:01:02,920
some different way I would prefer that

01:00:58,350 --> 01:01:07,420
okay Oh probably Raleigh we can yes I

01:01:02,920 --> 01:01:10,630
mean yes hello users I mean provide both

01:01:07,420 --> 01:01:12,580
both ways to the policy way that we have

01:01:10,630 --> 01:01:15,160
to indicate memory of performance or

01:01:12,580 --> 01:01:17,170
just a low also to to explicitly

01:01:15,160 --> 01:01:18,610
indicate what kind of said they want

01:01:17,170 --> 01:01:20,140
sure sure we can do that but at that

01:01:18,610 --> 01:01:23,050
point we lose one part of the

01:01:20,140 --> 01:01:24,550
flexibility we hope we can't change too

01:01:23,050 --> 01:01:26,590
much of the behavior I mean all that

01:01:24,550 --> 01:01:28,120
stuff becomes part of the API the

01:01:26,590 --> 01:01:30,100
internal implementation becomes part of

01:01:28,120 --> 01:01:31,710
the API if we allow to specify these

01:01:30,100 --> 01:01:34,360
parameters select the exact

01:01:31,710 --> 01:01:36,070
implementation behavior and it was

01:01:34,360 --> 01:01:38,710
actually meant to exactly not do this

01:01:36,070 --> 01:01:40,390
there's no Sun on purpose so I don't

01:01:38,710 --> 01:01:42,820
want to just sure we can do it I know

01:01:40,390 --> 01:01:45,070
that but I want at least to think about

01:01:42,820 --> 01:01:50,800
that a lot before doing that okay let's

01:01:45,070 --> 01:01:52,750
wait for more use cases but this came up

01:01:50,800 --> 01:01:54,880
as part of one of the other patches the

01:01:52,750 --> 01:01:58,860
discussion it's our hash table related I

01:01:54,880 --> 01:02:03,430
don't know that it exactly fits here but

01:01:58,860 --> 01:02:05,260
we have this size value which sets the

01:02:03,430 --> 01:02:07,480
ceiling and the sets I think we've kind

01:02:05,260 --> 01:02:10,030
of decided that that's going to stay

01:02:07,480 --> 01:02:12,370
just keep the ceiling there there was a

01:02:10,030 --> 01:02:14,650
question that I had asked God when I

01:02:12,370 --> 01:02:17,080
posted that back shift patch though of

01:02:14,650 --> 01:02:18,450
whether the our hash table

01:02:17,080 --> 01:02:21,970
implementation should have some type of

01:02:18,450 --> 01:02:24,370
bounds on how many elements should be in

01:02:21,970 --> 01:02:26,230
the table at all because right now you

01:02:24,370 --> 01:02:28,390
guys are we're setting a Mac shift which

01:02:26,230 --> 01:02:29,450
allows bounds on the number of buckets

01:02:28,390 --> 01:02:30,890
that are created but

01:02:29,450 --> 01:02:32,930
not necessarily the number of elements

01:02:30,890 --> 01:02:34,849
so you can continue to grow and the

01:02:32,930 --> 01:02:36,550
chains as long as you have available

01:02:34,849 --> 01:02:39,560
memory I don't know if that's a

01:02:36,550 --> 01:02:41,150
consideration I it if it should be left

01:02:39,560 --> 01:02:42,740
up to I think what was decided as it

01:02:41,150 --> 01:02:51,230
should be left up to the user to kind of

01:02:42,740 --> 01:02:53,660
force these type of limits but yeah

01:02:51,230 --> 01:02:55,430
definitely I mean right now like the

01:02:53,660 --> 01:02:57,710
auto user is nothing soccer table which

01:02:55,430 --> 01:03:01,220
enforces its own limit that question is

01:02:57,710 --> 01:03:02,810
whether we're nft wants to do that so

01:03:01,220 --> 01:03:05,240
any side or Irish table or whether we

01:03:02,810 --> 01:03:10,369
want to enforce that limit Justin eng

01:03:05,240 --> 01:03:14,060
and have set layer I didn't really get

01:03:10,369 --> 01:03:15,380
that question I think this mike is not

01:03:14,060 --> 01:03:17,030
really working more so the question is

01:03:15,380 --> 01:03:19,400
whether we want to enforce the limit

01:03:17,030 --> 01:03:22,970
inside our hash table maximum number of

01:03:19,400 --> 01:03:25,430
entries or inside the NF tables set code

01:03:22,970 --> 01:03:26,690
well I don't really care I mean I my

01:03:25,430 --> 01:03:30,200
comment on the mailing list was just

01:03:26,690 --> 01:03:31,579
about we're doing half of half of that

01:03:30,200 --> 01:03:32,930
stuff is done internally and the other

01:03:31,579 --> 01:03:34,490
half outside so it's basically

01:03:32,930 --> 01:03:45,020
inconsistent but I mean I don't really

01:03:34,490 --> 01:03:46,339
care either way box all right okay so so

01:03:45,020 --> 01:03:49,670
I mentioned a couple of to do's

01:03:46,339 --> 01:03:50,599
throughout then basically I just ran out

01:03:49,670 --> 01:03:52,430
of time before we had to do the

01:03:50,599 --> 01:03:53,780
presentation but there's a bunch of

01:03:52,430 --> 01:03:55,130
things I still want to understand about

01:03:53,780 --> 01:03:57,440
enough table so we're going to continue

01:03:55,130 --> 01:03:59,329
to look into it play around with some

01:03:57,440 --> 01:04:00,349
things especially after a lot of the

01:03:59,329 --> 01:04:02,839
things that came up today in the

01:04:00,349 --> 01:04:05,240
discussion you know why the table

01:04:02,839 --> 01:04:07,940
doesn't grow during restore there was I

01:04:05,240 --> 01:04:10,790
wanted to do some other performance

01:04:07,940 --> 01:04:13,730
comparisons as far as if tables vs IP

01:04:10,790 --> 01:04:15,920
sets some throughput and package per

01:04:13,730 --> 01:04:19,910
second information so if when i do that

01:04:15,920 --> 01:04:21,530
i will send it to the mailing list you

01:04:19,910 --> 01:04:24,589
know why single element ads take so long

01:04:21,530 --> 01:04:27,619
and then also I'd like to add some tests

01:04:24,589 --> 01:04:29,329
of the NF tables test infrastructure to

01:04:27,619 --> 01:04:30,220
kind of exercise some of the cases that

01:04:29,329 --> 01:04:32,320
I

01:04:30,220 --> 01:04:33,550
talked about today so that we're kind of

01:04:32,320 --> 01:04:38,200
you know making sure that those are

01:04:33,550 --> 01:04:39,190
covered in the future versions I don't

01:04:38,200 --> 01:04:41,320
know if you guys didn't get to this

01:04:39,190 --> 01:04:42,970
yesterday I don't think but lib enough

01:04:41,320 --> 01:04:47,830
tables supposed to be the third party

01:04:42,970 --> 01:04:49,960
library right and so I for us in order

01:04:47,830 --> 01:04:52,030
to to build on that I think that's

01:04:49,960 --> 01:04:53,859
really important to have a fully

01:04:52,030 --> 01:04:55,300
supported API that that we can take

01:04:53,859 --> 01:04:57,250
advantage of the internet filter

01:04:55,300 --> 01:04:59,920
component so actually if i get my miter

01:04:57,250 --> 01:05:02,320
interrupt it's meant as a lowly or

01:04:59,920 --> 01:05:04,359
communication library the plan is to

01:05:02,320 --> 01:05:07,930
expose some higher layer functionality

01:05:04,359 --> 01:05:09,910
basically on what n if tables is doing

01:05:07,930 --> 01:05:12,369
the the binary the user space binary is

01:05:09,910 --> 01:05:14,680
doing internally we have the front end

01:05:12,369 --> 01:05:16,300
basically which is the Bison passer and

01:05:14,680 --> 01:05:19,450
after that we have all the internal if

01:05:16,300 --> 01:05:22,000
your evaluation steps which combines to

01:05:19,450 --> 01:05:24,460
have rearranged stuff to put I

01:05:22,000 --> 01:05:26,200
propagation all that stuff and that

01:05:24,460 --> 01:05:27,910
layer we actually want to expose in the

01:05:26,200 --> 01:05:30,880
library we are not quite there yet

01:05:27,910 --> 01:05:32,800
because it's still moving too much but

01:05:30,880 --> 01:05:35,890
we will probably start at what at some

01:05:32,800 --> 01:05:37,900
point in the maybe not too far future to

01:05:35,890 --> 01:05:40,150
start separating stuff into a library

01:05:37,900 --> 01:05:43,480
internally within and if tables so we

01:05:40,150 --> 01:05:46,990
can try to get an API right but not

01:05:43,480 --> 01:05:49,119
exported so far and just try to well

01:05:46,990 --> 01:05:51,430
split up the NSA best binary in a

01:05:49,119 --> 01:05:54,670
library which will be exported also for

01:05:51,430 --> 01:05:56,910
users and at some point we will get

01:05:54,670 --> 01:06:00,220
going in this direction at some point um

01:05:56,910 --> 01:06:02,050
release it as a separate project okay

01:06:00,220 --> 01:06:03,849
yeah that we we're definitely interested

01:06:02,050 --> 01:06:06,940
in helping out with that and testing it

01:06:03,849 --> 01:06:10,210
so there is one thing so when I tried to

01:06:06,940 --> 01:06:13,210
implement the in its size parameter 1 I

01:06:10,210 --> 01:06:15,190
was stumbling block it was there's a lot

01:06:13,210 --> 01:06:17,050
of places to update it seems if you want

01:06:15,190 --> 01:06:20,500
to add new functionality to user space

01:06:17,050 --> 01:06:25,030
so you have to update the enough tables

01:06:20,500 --> 01:06:27,550
utility live in ft NL and then possibly

01:06:25,030 --> 01:06:28,869
live maybe live enough tables I don't

01:06:27,550 --> 01:06:30,550
know or if that would need to be updated

01:06:28,869 --> 01:06:32,589
and then also if it requires a kernel

01:06:30,550 --> 01:06:34,300
update we're also having to update their

01:06:32,589 --> 01:06:35,500
maybe this is just the price we have to

01:06:34,300 --> 01:06:37,750
pay for the way things are structured

01:06:35,500 --> 01:06:39,820
but it does seem like you know updating

01:06:37,750 --> 01:06:41,859
the three or four places that wants to

01:06:39,820 --> 01:06:42,970
implement a single you know new

01:06:41,859 --> 01:06:45,780
parameter

01:06:42,970 --> 01:06:48,040
was a lot yeah it has a lot of overhead

01:06:45,780 --> 01:06:49,960
if you actually need to add something

01:06:48,040 --> 01:06:53,170
new to the comment you have to walk

01:06:49,960 --> 01:06:55,000
through all these layers and basically

01:06:53,170 --> 01:06:59,710
make the same simple adjustment and all

01:06:55,000 --> 01:07:01,630
of them kind of annoying but you in

01:06:59,710 --> 01:07:02,770
there are many cases on the upside there

01:07:01,630 --> 01:07:04,420
are many cases where you actually don't

01:07:02,770 --> 01:07:06,100
need to update the colonel to implement

01:07:04,420 --> 01:07:08,200
and change but can it you do it purely

01:07:06,100 --> 01:07:09,790
in the front end which is right out of

01:07:08,200 --> 01:07:12,790
that even in those cases you still have

01:07:09,790 --> 01:07:15,160
to update these three components right

01:07:12,790 --> 01:07:17,200
well not necessarily for instance

01:07:15,160 --> 01:07:20,020
implementing a new match can usually be

01:07:17,200 --> 01:07:21,340
done well on packet payload for instance

01:07:20,020 --> 01:07:22,630
can usually be done in the end of tape

01:07:21,340 --> 01:07:24,760
is front and completely you don't have

01:07:22,630 --> 01:07:27,220
to touch any other to touch any okay so

01:07:24,760 --> 01:07:31,390
it depends basically it's annoying short

01:07:27,220 --> 01:07:32,830
yeah sure and then some of the

01:07:31,390 --> 01:07:34,840
requirements that we would have four

01:07:32,830 --> 01:07:39,550
live in if tables would be you know

01:07:34,840 --> 01:07:43,390
obviously thread safety and the ability

01:07:39,550 --> 01:07:45,550
to kind of expose what will call kernel

01:07:43,390 --> 01:07:47,290
level a rule or set metadata more

01:07:45,550 --> 01:07:49,090
information about the the sets and

01:07:47,290 --> 01:07:50,320
things that we don't see right now and

01:07:49,090 --> 01:07:53,770
live by piece set that we're trying to

01:07:50,320 --> 01:07:55,150
expose the atomicity which i think is

01:07:53,770 --> 01:07:56,530
already kind of covered in the

01:07:55,150 --> 01:07:58,090
underlying infrastructure for enough

01:07:56,530 --> 01:08:01,660
tables that's not really a concern and

01:07:58,090 --> 01:08:03,670
then you know I think Pete didn't see

01:08:01,660 --> 01:08:05,860
you want to make sure that you know

01:08:03,670 --> 01:08:07,180
random matching they do allow

01:08:05,860 --> 01:08:09,790
statistical matching and things like

01:08:07,180 --> 01:08:11,860
that and so that was a concern that you

01:08:09,790 --> 01:08:13,630
know I don't think that's maybe not part

01:08:11,860 --> 01:08:17,830
of live in of tables but is it available

01:08:13,630 --> 01:08:20,800
in NF tables right now well not really

01:08:17,830 --> 01:08:22,390
random yeah we don't have basically we

01:08:20,800 --> 01:08:24,550
need something to provide random I mean

01:08:22,390 --> 01:08:26,440
you might be able to just use random

01:08:24,550 --> 01:08:28,570
packet data and you care this is random

01:08:26,440 --> 01:08:31,410
but it's not real random we are missing

01:08:28,570 --> 01:08:40,380
that so quite easy to implement I mean

01:08:31,410 --> 01:08:40,380
we can add it if you need it okay tis it

01:08:40,510 --> 01:08:45,470
we were also playing around with the NF

01:08:43,670 --> 01:08:49,580
tables compatibility tool that was a

01:08:45,470 --> 01:08:54,350
really cool idea but for us to be able

01:08:49,580 --> 01:08:56,029
to use it although after you after the

01:08:54,350 --> 01:08:58,520
discussion last night it seems like

01:08:56,029 --> 01:09:01,700
maybe it it doesn't work for us but if

01:08:58,520 --> 01:09:04,430
we were to use it the right now there's

01:09:01,700 --> 01:09:06,109
no IP set functionality there and so it

01:09:04,430 --> 01:09:13,069
kind of it it breaks when we're going

01:09:06,109 --> 01:09:14,660
through yeah I don't know if it's

01:09:13,069 --> 01:09:16,220
possible or if it's do I mean I'm sure

01:09:14,660 --> 01:09:18,140
it's possible but whether it's it's

01:09:16,220 --> 01:09:20,450
needed we were thinking about doing it

01:09:18,140 --> 01:09:23,870
so that we could almost you know your

01:09:20,450 --> 01:09:25,490
your example of sim linking to the

01:09:23,870 --> 01:09:29,240
compat utility for this we get sim

01:09:25,490 --> 01:09:31,310
linked to an ipsec compat layer the

01:09:29,240 --> 01:09:33,049
problem though I guess is that and we

01:09:31,310 --> 01:09:34,940
were hoping to it'd be a nice kind of

01:09:33,049 --> 01:09:36,680
Shem layer kind of drop-in replacement

01:09:34,940 --> 01:09:39,020
while we're transitioning to full enough

01:09:36,680 --> 01:09:42,980
tables and developing something on that

01:09:39,020 --> 01:09:44,660
on that end the problem that I think

01:09:42,980 --> 01:09:47,029
that was online last night is that if we

01:09:44,660 --> 01:09:49,190
do this one to one mapping from iptables

01:09:47,029 --> 01:09:51,080
rule sets to NF tables that there's

01:09:49,190 --> 01:09:54,470
possibly going to be expected

01:09:51,080 --> 01:09:57,050
performance issue with just taking the

01:09:54,470 --> 01:10:00,590
rules as they as they stand right now in

01:09:57,050 --> 01:10:01,790
iptables right and so we would need to

01:10:00,590 --> 01:10:03,200
woo I think we're playing on

01:10:01,790 --> 01:10:04,940
investigating it but if that is the case

01:10:03,200 --> 01:10:06,980
that may not be as much of a concern for

01:10:04,940 --> 01:10:09,920
us at least to be able to use something

01:10:06,980 --> 01:10:12,140
like that I mean if you were to dump

01:10:09,920 --> 01:10:16,510
your IP tables rules as enough tables

01:10:12,140 --> 01:10:18,440
rules and just you know and use that

01:10:16,510 --> 01:10:19,790
that would be great but it seems like

01:10:18,440 --> 01:10:21,260
there's a performance problem or there's

01:10:19,790 --> 01:10:27,200
going to be a performative right that's

01:10:21,260 --> 01:10:31,450
the expectation at least right yeah so I

01:10:27,200 --> 01:10:34,310
guess another possible just kind of

01:10:31,450 --> 01:10:36,140
thinking about all this is what about

01:10:34,310 --> 01:10:40,310
the ability to take your existing IP

01:10:36,140 --> 01:10:44,030
tables rules and make them more enough

01:10:40,310 --> 01:10:45,800
tables desk if taking them and combining

01:10:44,030 --> 01:10:47,620
them into something that's more

01:10:45,800 --> 01:10:49,940
palatable for him enough tables

01:10:47,620 --> 01:10:51,910
basically what we do want to have I mean

01:10:49,940 --> 01:10:55,580
first step would be to convert

01:10:51,910 --> 01:10:57,440
to convert your VIP tables rules to NF

01:10:55,580 --> 01:10:59,960
tables of course yep what we would like

01:10:57,440 --> 01:11:01,910
to have at some point is an optimizer

01:10:59,960 --> 01:11:04,190
and user space your multiple optimizes

01:11:01,910 --> 01:11:08,290
which just take your rule set try to

01:11:04,190 --> 01:11:11,150
analyze structure rearrange this up and

01:11:08,290 --> 01:11:13,250
basically this wouldn't be for iptables

01:11:11,150 --> 01:11:14,840
specifically but for NF tables of course

01:11:13,250 --> 01:11:16,670
the first step would be to convert you

01:11:14,840 --> 01:11:19,610
rules it and then take advantage of an

01:11:16,670 --> 01:11:21,650
optimizer at some point right I started

01:11:19,610 --> 01:11:24,110
the set infrastructure's actually meant

01:11:21,650 --> 01:11:26,060
to support building more complex data

01:11:24,110 --> 01:11:28,190
structures in the coronal from user

01:11:26,060 --> 01:11:29,720
space and one of the main goals was to

01:11:28,190 --> 01:11:32,480
be able to express something like the

01:11:29,720 --> 01:11:34,100
HIPEC algorithm completely user space ok

01:11:32,480 --> 01:11:37,790
I've been trying I think starting to

01:11:34,100 --> 01:11:39,800
work on that a little i stopped at some

01:11:37,790 --> 01:11:42,920
point it will happen at some point that

01:11:39,800 --> 01:11:44,720
we're going to do that ok and i guess it

01:11:42,920 --> 01:11:47,060
just takes i mean at some point when i

01:11:44,720 --> 01:11:49,040
find two three four weeks to get some

01:11:47,060 --> 01:11:51,470
basic implementation going where we can

01:11:49,040 --> 01:11:54,110
try to expand it and it has lots of

01:11:51,470 --> 01:11:55,850
difficult cases the basic optimization

01:11:54,110 --> 01:11:58,150
phase it's not too hard to do but

01:11:55,850 --> 01:12:01,010
translating it in the reverse direction

01:11:58,150 --> 01:12:02,960
trying to handle updates and at that

01:12:01,010 --> 01:12:04,670
point you need to transform one tree

01:12:02,960 --> 01:12:06,320
into a different tree and stuff like

01:12:04,670 --> 01:12:08,140
that this is quite complicated but the

01:12:06,320 --> 01:12:10,760
basic optimization optimizing

01:12:08,140 --> 01:12:14,750
optimization step is not too hard to do

01:12:10,760 --> 01:12:18,410
ok cool you know it's also some some

01:12:14,750 --> 01:12:22,160
code is still not jet in industry that

01:12:18,410 --> 01:12:25,580
shoot a low should provide the missing

01:12:22,160 --> 01:12:28,040
clue code so you can reload your IP IP

01:12:25,580 --> 01:12:30,770
table rule set with the iptables compact

01:12:28,040 --> 01:12:35,230
and currently if you try to list it with

01:12:30,770 --> 01:12:39,770
nft it breaks so with that with a

01:12:35,230 --> 01:12:42,560
missing code we will use a special

01:12:39,770 --> 01:12:44,690
syntax so you will see your IP tables

01:12:42,560 --> 01:12:47,720
rules you can start on slaton them to

01:12:44,690 --> 01:12:49,700
native if possible but if for example

01:12:47,720 --> 01:12:52,940
let's say we for example don't have cash

01:12:49,700 --> 01:12:56,720
limit we can it will be able to use the

01:12:52,940 --> 01:13:00,830
native x tables extensions mixed with a

01:12:56,720 --> 01:13:02,810
river so it's just a way to provide a an

01:13:00,830 --> 01:13:03,470
easy way to transition while we don't

01:13:02,810 --> 01:13:08,330
have

01:13:03,470 --> 01:13:12,170
native support for what x tables all

01:13:08,330 --> 01:13:14,180
right yeah cool so I think that's pretty

01:13:12,170 --> 01:13:15,470
much all we had to cover those are any

01:13:14,180 --> 01:13:20,120
other questions that didn't come up

01:13:15,470 --> 01:13:21,920
during the talker thanks for sending

01:13:20,120 --> 01:13:24,860
patches i think i told you yesterday but

01:13:21,920 --> 01:13:28,220
I mean every time that we heat a book I

01:13:24,860 --> 01:13:32,090
mean just the air for to just get

01:13:28,220 --> 01:13:36,590
diagnose it and oh yeah sure so hey

01:13:32,090 --> 01:13:37,460
hopefully they'll keep coming soon all

01:13:36,590 --> 01:13:39,020
right I think that is the end of our

01:13:37,460 --> 01:13:42,820
talk did you guys want to go ahead and

01:13:39,020 --> 01:13:46,730
continue on with the NF tables tutorial

01:13:42,820 --> 01:13:49,040
if people are interested I can I have

01:13:46,730 --> 01:13:52,070
something specific about on the NF table

01:13:49,040 --> 01:13:53,810
set some like the upcoming features and

01:13:52,070 --> 01:13:56,840
what we're planning to do short-term now

01:13:53,810 --> 01:14:00,350
on I can talk about that yeah I think we

01:13:56,840 --> 01:14:06,040
have x is usually but looks like 35

01:14:00,350 --> 01:14:06,040

YouTube URL: https://www.youtube.com/watch?v=e-U9yCE08Cg


