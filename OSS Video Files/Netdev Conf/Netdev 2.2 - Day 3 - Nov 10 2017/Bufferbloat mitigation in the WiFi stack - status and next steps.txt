Title: Bufferbloat mitigation in the WiFi stack - status and next steps
Publication date: 2018-03-15
Playlist: Netdev 2.2 - Day 3 - Nov 10 2017
Description: 
	Speaker: Toke Høiland-Jørgensen
Friday November 10th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?jorgensen-wifistack-talk
Captions: 
	00:00:02,160 --> 00:00:07,979
Fix buffer bloat in the and the Wi-Fi

00:00:04,890 --> 00:00:10,020
stack so first I would like to get a

00:00:07,979 --> 00:00:17,520
show of hands how many know what buffer

00:00:10,020 --> 00:00:19,200
bloat is good how many use Wi-Fi and how

00:00:17,520 --> 00:00:25,500
many are too busy using the Wi-Fi to

00:00:19,200 --> 00:00:27,300
listen to what I'm saying right so there

00:00:25,500 --> 00:00:30,180
should be something for almost everyone

00:00:27,300 --> 00:00:32,129
then so I'll go through a little bit

00:00:30,180 --> 00:00:36,110
about what it is we've been trying to

00:00:32,129 --> 00:00:39,630
fix and some of the peculiarities and

00:00:36,110 --> 00:00:41,850
properties of the 802 11 Mac protocol

00:00:39,630 --> 00:00:45,420
that gives some constraints on how we

00:00:41,850 --> 00:00:48,710
can fix this and then I'll show what we

00:00:45,420 --> 00:00:51,930
did and give you a few slides with some

00:00:48,710 --> 00:00:54,840
some performance results and then the

00:00:51,930 --> 00:00:57,540
most important thing is this next step

00:00:54,840 --> 00:01:00,390
thing where I'm hoping it will be a

00:00:57,540 --> 00:01:02,010
little bit of an interactive session or

00:01:00,390 --> 00:01:02,910
otherwise we will have a lot of awkward

00:01:02,010 --> 00:01:05,309
silences

00:01:02,910 --> 00:01:07,680
so I'm trying I'm hoping to get some

00:01:05,309 --> 00:01:11,070
some feedback on some of the ideas we

00:01:07,680 --> 00:01:12,720
have for the next steps whether or not

00:01:11,070 --> 00:01:15,510
they are a good idea whether or not it's

00:01:12,720 --> 00:01:18,090
completely daft what I'm proposing and

00:01:15,510 --> 00:01:24,090
maybe some ideas of how to do it as well

00:01:18,090 --> 00:01:28,680
so first up the problem buffer bloat so

00:01:24,090 --> 00:01:32,220
this has been this has been pretty much

00:01:28,680 --> 00:01:36,150
fixed in large parts of the stack but as

00:01:32,220 --> 00:01:39,150
we can see here for the this is sort of

00:01:36,150 --> 00:01:41,549
a plot of different queueing disciplines

00:01:39,150 --> 00:01:43,229
apply it to Wi-Fi link and even the best

00:01:41,549 --> 00:01:46,049
ones still have almost a hundred

00:01:43,229 --> 00:01:49,590
milliseconds of latency at the Wi-Fi

00:01:46,049 --> 00:01:52,200
link and I'll get back to why that is

00:01:49,590 --> 00:01:54,000
and and what we did to fix it but this

00:01:52,200 --> 00:01:57,509
was sort of what we had going into this

00:01:54,000 --> 00:01:59,610
and then the other thing that we did was

00:01:57,509 --> 00:02:03,990
look at this thing called airtime

00:01:59,610 --> 00:02:06,899
fairness so in the moderator 211

00:02:03,990 --> 00:02:09,840
protocol by default it will provide sort

00:02:06,899 --> 00:02:14,090
throughput fairness where the time spent

00:02:09,840 --> 00:02:14,090
transmitting to each device

00:02:14,129 --> 00:02:19,269
depends on the time this is this one

00:02:16,599 --> 00:02:22,840
that this is the default the bottom one

00:02:19,269 --> 00:02:24,670
there that it depends on how long each

00:02:22,840 --> 00:02:27,359
station actually spends transmitting

00:02:24,670 --> 00:02:31,420
where what we want is really this thing

00:02:27,359 --> 00:02:33,010
complete fairness because the the scarce

00:02:31,420 --> 00:02:36,459
resource here is the time spent

00:02:33,010 --> 00:02:39,189
transmitting on the air because when we

00:02:36,459 --> 00:02:41,290
get the other thing we end up with with

00:02:39,189 --> 00:02:43,540
rupert fairness which means that the

00:02:41,290 --> 00:02:45,129
whole network goes at the rate of the

00:02:43,540 --> 00:02:47,109
slowest station on the network so if you

00:02:45,129 --> 00:02:49,540
have one device that at the other end of

00:02:47,109 --> 00:02:51,040
the room has a bad connectivity that's

00:02:49,540 --> 00:02:53,199
going to slow down your whole network

00:02:51,040 --> 00:02:55,000
because it's taking up all the airtime

00:02:53,199 --> 00:02:59,739
trying to get a few packets through at

00:02:55,000 --> 00:03:02,290
11 megabits per second in order to fix

00:02:59,739 --> 00:03:06,519
this we had some constraints from the

00:03:02,290 --> 00:03:08,889
marketer to 11 protocol we must do

00:03:06,519 --> 00:03:11,769
aggregation so because there's a lot of

00:03:08,889 --> 00:03:14,769
overhead each time we transmit anything

00:03:11,769 --> 00:03:17,230
on Wi-Fi we attribute packets to try to

00:03:14,769 --> 00:03:19,030
be more efficient and that has to be

00:03:17,230 --> 00:03:21,159
done per traffic ID which is an an

00:03:19,030 --> 00:03:23,829
identifier that each packet is mapped to

00:03:21,159 --> 00:03:27,549
usually by by diffserv mapping so this

00:03:23,829 --> 00:03:29,709
is also how Wi-Fi direct QoS and we must

00:03:27,549 --> 00:03:32,290
also handle reinjection of packets for

00:03:29,709 --> 00:03:34,560
retransmission which means that if the

00:03:32,290 --> 00:03:37,930
driver sends out package to the hardware

00:03:34,560 --> 00:03:40,419
and the hardware fails to transmit it

00:03:37,930 --> 00:03:43,209
and that doesn't get an act back it will

00:03:40,419 --> 00:03:44,709
throw them back up to the driver and the

00:03:43,209 --> 00:03:47,680
driver will then we queue them for the

00:03:44,709 --> 00:03:49,120
next transmission we must also of course

00:03:47,680 --> 00:03:52,689
be able to keep the hardware busy

00:03:49,120 --> 00:03:55,150
especially on low-power devices this

00:03:52,689 --> 00:03:58,180
means that there are some limits to how

00:03:55,150 --> 00:04:01,359
little queuing we can have and we also

00:03:58,180 --> 00:04:03,099
wanted to do this thing a to be

00:04:01,359 --> 00:04:04,810
deployable without having a flat day

00:04:03,099 --> 00:04:07,079
where you upgrade the whole network so a

00:04:04,810 --> 00:04:08,859
lot of the the benefits we want by just

00:04:07,079 --> 00:04:12,909
upgrading the access point for example

00:04:08,859 --> 00:04:15,009
and then there's some some sensibility

00:04:12,909 --> 00:04:17,560
to reordering for some of the operations

00:04:15,009 --> 00:04:19,989
which I was one of the interesting bugs

00:04:17,560 --> 00:04:23,680
we had when doing this that turns out if

00:04:19,989 --> 00:04:25,630
you at vanish queuing and then then

00:04:23,680 --> 00:04:27,370
package can get reordered and then these

00:04:25,630 --> 00:04:29,320
sequence numbers and crypto

00:04:27,370 --> 00:04:32,050
Ivy's out of order and the receiver will

00:04:29,320 --> 00:04:33,970
just drop half of your packets but

00:04:32,050 --> 00:04:36,370
especially the the first two constraints

00:04:33,970 --> 00:04:39,460
there that we have to know more about

00:04:36,370 --> 00:04:42,040
the traffic than a q-tip does and we

00:04:39,460 --> 00:04:44,590
have to be able to reject packets means

00:04:42,040 --> 00:04:47,590
that we decided we couldn't use the the

00:04:44,590 --> 00:04:49,090
existing purist layer without adding a

00:04:47,590 --> 00:04:50,639
lot of API between there that would

00:04:49,090 --> 00:04:54,100
complicate everything

00:04:50,639 --> 00:04:55,870
so that's sort of a bit of the

00:04:54,100 --> 00:05:01,330
background so what we already did and

00:04:55,870 --> 00:05:04,570
what is already in main lines since 411

00:05:01,330 --> 00:05:06,550
I think for some of it between 49 and

00:05:04,570 --> 00:05:08,350
foil 11 this which grunion in invite

00:05:06,550 --> 00:05:11,530
bunches so we've introduced buffer bloat

00:05:08,350 --> 00:05:14,350
by an order of magnitude as you can see

00:05:11,530 --> 00:05:17,680
here like this is this is linux default

00:05:14,350 --> 00:05:20,280
before 4.9 when this thing when done so

00:05:17,680 --> 00:05:23,260
this is with a normal FIFO queue dish

00:05:20,280 --> 00:05:25,660
where you get your 500 milliseconds of

00:05:23,260 --> 00:05:30,970
latency under load and now we're down to

00:05:25,660 --> 00:05:33,070
around 20 so there's it's still a bit

00:05:30,970 --> 00:05:35,260
more than you get an Ethernet but a lot

00:05:33,070 --> 00:05:38,110
of that is like the low-hanging fruits

00:05:35,260 --> 00:05:40,720
have have been handled now we also have

00:05:38,110 --> 00:05:42,910
almost perfect airtime fairness in the

00:05:40,720 --> 00:05:45,550
driver which is reported in earth mine K

00:05:42,910 --> 00:05:49,300
F 10 K only has the the buffer bloat

00:05:45,550 --> 00:05:51,039
fixes and we're working on how to expand

00:05:49,300 --> 00:05:52,169
this to basically all drivers I'll get

00:05:51,039 --> 00:05:55,570
back to that in a bit

00:05:52,169 --> 00:05:57,250
so how did we do that do this well

00:05:55,570 --> 00:06:00,729
naturally once you have a queuing

00:05:57,250 --> 00:06:02,229
problem and you want to solve this what

00:06:00,729 --> 00:06:06,639
we did is we increase the amount of

00:06:02,229 --> 00:06:08,889
queuing by a factor of about 16 so like

00:06:06,639 --> 00:06:13,330
this is we try to do queue smarter not

00:06:08,889 --> 00:06:15,220
harder by changing the algorithm of the

00:06:13,330 --> 00:06:22,330
tune instead of just staring with it you

00:06:15,220 --> 00:06:27,910
and what this what we did in more detail

00:06:22,330 --> 00:06:30,580
was we created a perfect lecture in

00:06:27,910 --> 00:06:33,669
structure in the mag 802 11 layer which

00:06:30,580 --> 00:06:36,460
is the the common library that all the

00:06:33,669 --> 00:06:39,909
Wi-Fi drivers used to implement the Mac

00:06:36,460 --> 00:06:40,870
protocol which has like a shared pool of

00:06:39,909 --> 00:06:43,810
queues so instead

00:06:40,870 --> 00:06:47,590
of just allocating an F Q : instance to

00:06:43,810 --> 00:06:49,270
every tid that we have to send to which

00:06:47,590 --> 00:06:52,990
would take up way too much memory we

00:06:49,270 --> 00:06:55,570
sort of created a new structure that is

00:06:52,990 --> 00:06:57,940
based on FQ Carlo but she has the total

00:06:55,570 --> 00:06:59,890
number of Q's and then just assigns them

00:06:57,940 --> 00:07:02,530
as they're filled up to to the

00:06:59,890 --> 00:07:04,300
difference she IDs so it supports this

00:07:02,530 --> 00:07:06,370
party ID D chewing is you have the

00:07:04,300 --> 00:07:08,500
scheduling everything and so on top of

00:07:06,370 --> 00:07:11,260
this it was quite straightforward to

00:07:08,500 --> 00:07:13,360
build a schedule aware we just measure

00:07:11,260 --> 00:07:17,230
how much airtime are we using to

00:07:13,360 --> 00:07:18,850
transmit to each station and then shut

00:07:17,230 --> 00:07:22,110
your this in a way so that we eat were

00:07:18,850 --> 00:07:25,660
that out over time it's also aid a

00:07:22,110 --> 00:07:28,000
deficit round robin based cetera that

00:07:25,660 --> 00:07:29,590
does this it's sort of the ideas also

00:07:28,000 --> 00:07:35,530
comes from FQ Caudill but it's working

00:07:29,590 --> 00:07:38,500
in our time instead of instead of bytes

00:07:35,530 --> 00:07:39,970
and then we optimize also for sparse

00:07:38,500 --> 00:07:43,030
station so if you have a station that

00:07:39,970 --> 00:07:45,850
only transmits one packet every now and

00:07:43,030 --> 00:07:47,670
then we will put it on the front of the

00:07:45,850 --> 00:07:54,280
queue similar to what fq coddle does to

00:07:47,670 --> 00:07:57,550
sparse flows and so this is what the the

00:07:54,280 --> 00:07:59,650
queueing structure looked like in in a

00:07:57,550 --> 00:08:01,780
Wi-Fi device before we started changing

00:07:59,650 --> 00:08:04,120
things so this is for the for the f9k

00:08:01,780 --> 00:08:05,680
driver where you have the the cutest

00:08:04,120 --> 00:08:08,440
layer up here with up to thousand

00:08:05,680 --> 00:08:09,730
packets of five for curing by default or

00:08:08,440 --> 00:08:12,670
you could replace that with with

00:08:09,730 --> 00:08:15,790
anything but the problem was that down

00:08:12,670 --> 00:08:18,400
here in the driver below the the smart

00:08:15,790 --> 00:08:21,250
queue you have a whole nother layer of

00:08:18,400 --> 00:08:25,120
queuing with up to a hundred and twenty

00:08:21,250 --> 00:08:28,060
three packets and this is what added the

00:08:25,120 --> 00:08:29,950
latency that wouldn't get rid of and so

00:08:28,060 --> 00:08:32,830
what we did was we changed it to this

00:08:29,950 --> 00:08:35,350
where down here and the driver will now

00:08:32,830 --> 00:08:37,840
only have to retry queue which contains

00:08:35,350 --> 00:08:38,980
up to an accurate of packets if if

00:08:37,840 --> 00:08:40,990
transmission failed

00:08:38,980 --> 00:08:43,120
that's just priority queue and then up

00:08:40,990 --> 00:08:48,340
here in my data 211 we now have the

00:08:43,120 --> 00:08:50,200
smart queuing system which applies fq

00:08:48,340 --> 00:08:52,450
cuddle between all these queues and does

00:08:50,200 --> 00:08:53,470
all the tricks that that does for a

00:08:52,450 --> 00:08:55,779
charming and

00:08:53,470 --> 00:09:01,149
and prioritizing sparse flows and so on

00:08:55,779 --> 00:09:03,100
so we can get the nice low latency a few

00:09:01,149 --> 00:09:04,839
evaluation results we have these like

00:09:03,100 --> 00:09:07,000
the FIFO is the default before we

00:09:04,839 --> 00:09:09,579
started modifying things then we try

00:09:07,000 --> 00:09:11,889
just putting fq coddled on the on the

00:09:09,579 --> 00:09:15,910
Wi-Fi neck without changing anything and

00:09:11,889 --> 00:09:17,819
fq mac is then just the new queueing

00:09:15,910 --> 00:09:20,199
structure and then the airtime fairness

00:09:17,819 --> 00:09:24,129
obviously is for airtime fairness

00:09:20,199 --> 00:09:26,680
captioning and latency here you can see

00:09:24,129 --> 00:09:28,509
this was the the green line here was the

00:09:26,680 --> 00:09:32,579
one i showed you on the slide before

00:09:28,509 --> 00:09:36,069
where we're now down to here so like

00:09:32,579 --> 00:09:37,649
about an order of magnitude less curing

00:09:36,069 --> 00:09:42,069
to the default and even compared to

00:09:37,649 --> 00:09:45,160
putting fq coddled on on the interface

00:09:42,069 --> 00:09:49,560
we get a benefit of about a factor of

00:09:45,160 --> 00:09:54,490
two or three in latency throughput also

00:09:49,560 --> 00:09:57,160
increases so like this is the test

00:09:54,490 --> 00:09:58,630
scenario where we have two stations that

00:09:57,160 --> 00:10:00,819
are really close to the access point so

00:09:58,630 --> 00:10:04,600
have excellent poopit and then one slow

00:10:00,819 --> 00:10:06,519
station over here and this is like this

00:10:04,600 --> 00:10:09,459
is then the total of of all these so the

00:10:06,519 --> 00:10:10,810
slow station gets less and less

00:10:09,459 --> 00:10:12,339
throughput because we're we're

00:10:10,810 --> 00:10:14,110
throttling it back so doesn't take up

00:10:12,339 --> 00:10:17,079
all the airtime and that works out to

00:10:14,110 --> 00:10:20,079
really big increase in efficiency for

00:10:17,079 --> 00:10:22,870
the network and like the first part here

00:10:20,079 --> 00:10:26,439
from fq coddled and fq mac before we

00:10:22,870 --> 00:10:29,170
start scheduling our time is due to more

00:10:26,439 --> 00:10:31,930
queuing space so that you cannot this

00:10:29,170 --> 00:10:33,730
the the slow station cannot take over

00:10:31,930 --> 00:10:37,149
the whole queue and and staff the other

00:10:33,730 --> 00:10:41,470
stations and this is as you can see

00:10:37,149 --> 00:10:44,620
about a factor of three improvement

00:10:41,470 --> 00:10:47,019
total throughput for the network and

00:10:44,620 --> 00:10:49,180
measuring the the actual our time the

00:10:47,019 --> 00:10:52,629
three stations used this is where you

00:10:49,180 --> 00:10:54,759
can see the the effects of not having

00:10:52,629 --> 00:10:57,449
our time fairness where this station 1

00:10:54,759 --> 00:10:59,860
station takes up between like half and

00:10:57,449 --> 00:11:02,230
ninety percent of the total of

00:10:59,860 --> 00:11:04,120
transmission time three stations where

00:11:02,230 --> 00:11:04,750
when we share to them we achieve pretty

00:11:04,120 --> 00:11:07,840
much perfect

00:11:04,750 --> 00:11:12,010
aniss so that's sort of what we did

00:11:07,840 --> 00:11:14,590
already the next steps that we we want

00:11:12,010 --> 00:11:16,870
to try we have some ideas for further

00:11:14,590 --> 00:11:18,520
getting rid of the last 20 milliseconds

00:11:16,870 --> 00:11:22,060
or as much as it as we can of latency

00:11:18,520 --> 00:11:24,220
and then there's some some other ideas

00:11:22,060 --> 00:11:26,680
during forward for how to apply policy

00:11:24,220 --> 00:11:29,320
to the airtime fairness scheduling how

00:11:26,680 --> 00:11:31,740
to handle QoS smarter than what we're

00:11:29,320 --> 00:11:34,510
doing now and how to make all this

00:11:31,740 --> 00:11:38,020
configurable and integrate it with the

00:11:34,510 --> 00:11:41,410
existing tools and this is where I would

00:11:38,020 --> 00:11:46,210
like some feedback from you guys so the

00:11:41,410 --> 00:11:48,070
first one is sort of straightforward I

00:11:46,210 --> 00:11:49,900
guess like we want to minimize the

00:11:48,070 --> 00:11:53,260
buffering in the hardware as much as we

00:11:49,900 --> 00:11:55,390
can so when we don't control that

00:11:53,260 --> 00:11:57,310
rotation in the driver and that is done

00:11:55,390 --> 00:11:59,710
in firmware or Hardware the only thing

00:11:57,310 --> 00:12:01,450
we can do is try to minimize the amount

00:11:59,710 --> 00:12:03,550
of the amount of packet that we send

00:12:01,450 --> 00:12:05,200
down to the driver and like there's

00:12:03,550 --> 00:12:08,860
beach well to do this on Ethernet and

00:12:05,200 --> 00:12:11,280
maybe we can adapt that to Wi-Fi as well

00:12:08,860 --> 00:12:13,630
there's been some attempts for this but

00:12:11,280 --> 00:12:13,990
doesn't quite work as well as we would

00:12:13,630 --> 00:12:18,900
like

00:12:13,990 --> 00:12:22,210
yet when we do control aggregate

00:12:18,900 --> 00:12:25,180
creation in the driver we can even do

00:12:22,210 --> 00:12:27,070
better because we can actually start in

00:12:25,180 --> 00:12:30,160
theory at least we can start building

00:12:27,070 --> 00:12:32,200
the next aggregate once the previous one

00:12:30,160 --> 00:12:34,210
has started going off going out we can

00:12:32,200 --> 00:12:35,860
like it would be really helpful if the

00:12:34,210 --> 00:12:37,450
hardware can give us an interrupt not

00:12:35,860 --> 00:12:39,640
when it's finished transmitting but when

00:12:37,450 --> 00:12:41,140
it started transmitting and accurate so

00:12:39,640 --> 00:12:42,640
that we can start building the next one

00:12:41,140 --> 00:12:46,620
and then get it ready just in time for

00:12:42,640 --> 00:12:46,620
the next accurate yes

00:12:51,240 --> 00:12:55,810
so one thing I'm noticing in all of this

00:12:53,950 --> 00:12:58,029
is that it seems that logically what's

00:12:55,810 --> 00:13:00,040
what's happening is that whereas a

00:12:58,029 --> 00:13:02,950
queueing discipline would measure local

00:13:00,040 --> 00:13:06,510
cue residency time you're measuring air

00:13:02,950 --> 00:13:09,430
time as your as your cue we do both

00:13:06,510 --> 00:13:11,410
excuse me we do both we do both right so

00:13:09,430 --> 00:13:12,850
I'm saying like it's a combination of so

00:13:11,410 --> 00:13:15,370
you're externalizing to cue sort of

00:13:12,850 --> 00:13:16,959
speak right it's not just the time that

00:13:15,370 --> 00:13:18,370
it sits within the local Q and the

00:13:16,959 --> 00:13:22,839
queueing discipline it's also the time

00:13:18,370 --> 00:13:24,940
it spends on the radio right yeah so the

00:13:22,839 --> 00:13:27,279
latency is mostly the time it spends in

00:13:24,940 --> 00:13:29,680
the great but but we get the other

00:13:27,279 --> 00:13:31,120
benefits by measuring right so you want

00:13:29,680 --> 00:13:32,769
the transmitter up so that you can more

00:13:31,120 --> 00:13:35,440
accurately measure the air time

00:13:32,769 --> 00:13:37,630
component mmm

00:13:35,440 --> 00:13:40,450
no that we can actually do pretty well

00:13:37,630 --> 00:13:42,480
already by just like we get one we're a

00:13:40,450 --> 00:13:44,920
bit behind like there may be small

00:13:42,480 --> 00:13:48,510
unfairness on a small time scale because

00:13:44,920 --> 00:13:50,500
we we are sort of catching up the the

00:13:48,510 --> 00:13:52,899
get the interrupt while we're

00:13:50,500 --> 00:13:55,810
transmitting is more sort of a way to

00:13:52,899 --> 00:13:57,640
try to get fewer bytes chewed in the

00:13:55,810 --> 00:13:59,940
hardware I see what you're saying so

00:13:57,640 --> 00:14:05,410
doing what bql would have done otherwise

00:13:59,940 --> 00:14:07,810
yes so like my idea is that we may be

00:14:05,410 --> 00:14:09,820
able to do better than just beat well

00:14:07,810 --> 00:14:11,079
because we know that okay we have this

00:14:09,820 --> 00:14:13,779
one aggregate it's probably going to

00:14:11,079 --> 00:14:15,880
take four milliseconds to send out so we

00:14:13,779 --> 00:14:19,120
wait like three milliseconds before we

00:14:15,880 --> 00:14:20,769
build the next one it's sort of the

00:14:19,120 --> 00:14:22,120
register of how well this would work in

00:14:20,769 --> 00:14:23,680
practice without the interrupts and we

00:14:22,120 --> 00:14:25,449
have to sort of estimate the time so

00:14:23,680 --> 00:14:28,209
it's a more explicit mechanism whereas

00:14:25,449 --> 00:14:29,620
be kilos kind of passive yes right for

00:14:28,209 --> 00:14:31,540
the drivers where we can do this like

00:14:29,620 --> 00:14:33,130
that 9k and so for the other drivers

00:14:31,540 --> 00:14:34,449
that does like rotation and so on and

00:14:33,130 --> 00:14:40,209
firmware we probably can't do better

00:14:34,449 --> 00:14:48,730
than be sure okay thank you and there's

00:14:40,209 --> 00:14:50,050
also breach ones so at one point since

00:14:48,730 --> 00:14:52,240
you mentioned like when we do

00:14:50,050 --> 00:14:55,390
aggregation in firmware we do that on

00:14:52,240 --> 00:14:57,910
the Intel NICs and we've played in the

00:14:55,390 --> 00:15:01,060
past with limiting actually the the

00:14:57,910 --> 00:15:03,380
amount of time that that packets would

00:15:01,060 --> 00:15:05,990
spend on the queues that get aggregated

00:15:03,380 --> 00:15:08,990
so I think we can do better than bql

00:15:05,990 --> 00:15:12,500
because we did do see that when we do

00:15:08,990 --> 00:15:14,420
this that the thing is that when you

00:15:12,500 --> 00:15:16,340
when packets spend a lot of time

00:15:14,420 --> 00:15:19,310
the reason tends to be that they cannot

00:15:16,340 --> 00:15:22,220
be aggregated anyway because if you if

00:15:19,310 --> 00:15:23,780
you were able to aggregate them then you

00:15:22,220 --> 00:15:25,730
would send them out but so much quicker

00:15:23,780 --> 00:15:27,890
right if you send large aggregates they

00:15:25,730 --> 00:15:30,560
can take maybe a few milliseconds maybe

00:15:27,890 --> 00:15:33,140
one to three milliseconds at most

00:15:30,560 --> 00:15:34,700
even if they are large but if you if

00:15:33,140 --> 00:15:35,030
packets start queuing up longer than

00:15:34,700 --> 00:15:37,160
that

00:15:35,030 --> 00:15:39,440
you're really not sending the aggregates

00:15:37,160 --> 00:15:41,540
so I think you can do better than bql

00:15:39,440 --> 00:15:43,190
because you can measure how long your

00:15:41,540 --> 00:15:45,830
packets are on your queue until they get

00:15:43,190 --> 00:15:47,390
send out as aggregates so there's some

00:15:45,830 --> 00:15:49,970
work that the drivers would have to do

00:15:47,390 --> 00:15:52,400
in that area but um yeah I'm not really

00:15:49,970 --> 00:15:55,300
sure I can I would completely agree with

00:15:52,400 --> 00:15:57,980
we cannot do better than B Q oh okay

00:15:55,300 --> 00:16:00,470
yeah if we can get information from the

00:15:57,980 --> 00:16:03,170
firmware both on transmission time and

00:16:00,470 --> 00:16:06,640
queueing time before that that would be

00:16:03,170 --> 00:16:10,220
really helpful the other thing is

00:16:06,640 --> 00:16:12,920
retransmission so right now in earth 9 K

00:16:10,220 --> 00:16:16,850
every packet will be potentially reach

00:16:12,920 --> 00:16:19,190
right 30 times before it's dropped and

00:16:16,850 --> 00:16:21,590
that is a bit on the high end especially

00:16:19,190 --> 00:16:26,330
if it's being transmitted at 6 megabits

00:16:21,590 --> 00:16:28,430
per second so the way to fix this I

00:16:26,330 --> 00:16:31,040
think is to start counting how long did

00:16:28,430 --> 00:16:32,780
we actually try already to send this

00:16:31,040 --> 00:16:34,670
packet including how long it would cure

00:16:32,780 --> 00:16:37,910
it and then just drop it if we can't get

00:16:34,670 --> 00:16:39,140
it through and try the next one and I

00:16:37,910 --> 00:16:40,780
think that's straightforward to do I

00:16:39,140 --> 00:16:43,940
just haven't gotten around to it yet

00:16:40,780 --> 00:16:46,520
then there's the ad size of the Acrobat

00:16:43,940 --> 00:16:49,760
so that it may be that it would be a

00:16:46,520 --> 00:16:52,640
good idea if you have 10 20 stations

00:16:49,760 --> 00:16:54,200
with with outstanding packets you may

00:16:52,640 --> 00:16:55,760
want to start sending smaller aggregates

00:16:54,200 --> 00:16:57,830
because if you're spending 4

00:16:55,760 --> 00:16:59,750
milliseconds on each of those 10 20

00:16:57,830 --> 00:17:02,630
stations that's 50 to 100 milliseconds

00:16:59,750 --> 00:17:04,970
of delay between each transmission to

00:17:02,630 --> 00:17:08,120
each station whereas if you only send

00:17:04,970 --> 00:17:10,430
one millisecond aggregates to each it's

00:17:08,120 --> 00:17:14,000
only 10 20 milliseconds at the thought

00:17:10,430 --> 00:17:15,620
at the cost of a bit of efficiency so

00:17:14,000 --> 00:17:17,240
there's there's a latency throughputs

00:17:15,620 --> 00:17:19,700
right over here somewhere

00:17:17,240 --> 00:17:22,730
and and by doing this dynamically I'm

00:17:19,700 --> 00:17:25,070
figuring out the right trade-off is of

00:17:22,730 --> 00:17:27,320
course important here and then for again

00:17:25,070 --> 00:17:28,850
for firm we're just having a hook into

00:17:27,320 --> 00:17:33,650
the firmware that says please limit your

00:17:28,850 --> 00:17:35,900
accurate size to n milliseconds for very

00:17:33,650 --> 00:17:39,320
small numbers of n would be it would be

00:17:35,900 --> 00:17:43,280
useful here yes

00:17:39,320 --> 00:17:45,680
moving on to policies so whenever you

00:17:43,280 --> 00:17:47,600
you present this especially in academic

00:17:45,680 --> 00:17:51,190
circles people start talking about

00:17:47,600 --> 00:17:54,440
different notions of fairness and

00:17:51,190 --> 00:17:57,200
normally I would argue because airtime

00:17:54,440 --> 00:17:58,760
fairness works out to proportional

00:17:57,200 --> 00:18:01,370
fairness at the throughput level which

00:17:58,760 --> 00:18:03,500
is generally what you want but sometimes

00:18:01,370 --> 00:18:05,810
not so for example if your wireless

00:18:03,500 --> 00:18:06,980
music players in the next room at the

00:18:05,810 --> 00:18:09,830
other end of your apartment from your

00:18:06,980 --> 00:18:11,750
access point and it with airtime

00:18:09,830 --> 00:18:14,780
fairness it's now being throttle so that

00:18:11,750 --> 00:18:17,990
your mp3 streams no longer work maybe if

00:18:14,780 --> 00:18:20,750
you gave it like 1.5 times its fair

00:18:17,990 --> 00:18:22,910
share have our time you would lose a bit

00:18:20,750 --> 00:18:24,080
of throughput on the whole network but

00:18:22,910 --> 00:18:27,020
you should listen to your music screen

00:18:24,080 --> 00:18:28,940
that sort of thing the other thing is

00:18:27,020 --> 00:18:31,760
someone pointed out to me that when we

00:18:28,940 --> 00:18:33,560
have this interface we could also do

00:18:31,760 --> 00:18:35,120
things like a limited guest network

00:18:33,560 --> 00:18:37,370
where we don't put in a shaper to

00:18:35,120 --> 00:18:39,800
throttle it but we just have a work

00:18:37,370 --> 00:18:44,930
conserving scheduler that make sure that

00:18:39,800 --> 00:18:47,570
if your stations are active on your on

00:18:44,930 --> 00:18:49,580
your own network those get priority so

00:18:47,570 --> 00:18:51,830
that the guest network will never take

00:18:49,580 --> 00:18:54,620
out more than half of your airtime which

00:18:51,830 --> 00:18:56,060
is really the Stars resource so all

00:18:54,620 --> 00:18:57,470
these kinds of things could would be

00:18:56,060 --> 00:18:59,410
nice to be able to do instead of just

00:18:57,470 --> 00:19:03,710
having the strict fairness scheduler and

00:18:59,410 --> 00:19:06,460
my idea for how to do this is to instead

00:19:03,710 --> 00:19:09,980
of sheduled used individual stations

00:19:06,460 --> 00:19:12,920
group them and that way we can also have

00:19:09,980 --> 00:19:15,980
use of space to cite the grouping and

00:19:12,920 --> 00:19:17,600
then you set your airtime first between

00:19:15,980 --> 00:19:19,880
groups and then within the groups and

00:19:17,600 --> 00:19:22,100
you crawl you could also make the groups

00:19:19,880 --> 00:19:23,870
like and levels of recursive if you want

00:19:22,100 --> 00:19:25,160
to get really fancy and then you can add

00:19:23,870 --> 00:19:27,320
weights to them so there's a few

00:19:25,160 --> 00:19:30,360
examples of this so this is the I want

00:19:27,320 --> 00:19:33,030
my slow station to be a bit faster where

00:19:30,360 --> 00:19:37,140
just put each station in some group and

00:19:33,030 --> 00:19:42,450
way the the slow station for however

00:19:37,140 --> 00:19:44,340
much you want its share to be so that

00:19:42,450 --> 00:19:52,740
gets twice its fair time sharing this

00:19:44,340 --> 00:19:54,630
example so I think there's a bit of an

00:19:52,740 --> 00:19:56,880
issue here right because you if you

00:19:54,630 --> 00:19:59,040
weigh the slow station just with twice

00:19:56,880 --> 00:20:02,520
when you add more stations to the

00:19:59,040 --> 00:20:05,490
network then you still go down in

00:20:02,520 --> 00:20:08,220
airtime and your music jitter like has

00:20:05,490 --> 00:20:10,050
problems again right so it seems like

00:20:08,220 --> 00:20:12,390
you'd want to like group all of the

00:20:10,050 --> 00:20:16,770
others together in some way yeah that's

00:20:12,390 --> 00:20:18,300
the next thing okay so yeah so like this

00:20:16,770 --> 00:20:19,830
was actually my example from there just

00:20:18,300 --> 00:20:21,480
network but Matt may be a good point

00:20:19,830 --> 00:20:23,880
that you could also use this for the

00:20:21,480 --> 00:20:25,410
other if case so where you like first to

00:20:23,880 --> 00:20:27,210
divide out these two groups they get

00:20:25,410 --> 00:20:29,430
half the airtime and then within these

00:20:27,210 --> 00:20:31,740
groups these three devices share and

00:20:29,430 --> 00:20:33,360
this just get to the other half so this

00:20:31,740 --> 00:20:35,730
worked really well in this example if

00:20:33,360 --> 00:20:37,830
this is your guest network or your slow

00:20:35,730 --> 00:20:40,440
station then days will be limited but

00:20:37,830 --> 00:20:42,480
what if this is your guest network then

00:20:40,440 --> 00:20:46,380
you're suddenly giving the guests the

00:20:42,480 --> 00:20:48,390
guests more than its fair share and by

00:20:46,380 --> 00:20:51,810
the obvious way to fix this is to have

00:20:48,390 --> 00:20:54,630
like two sets of groups and then you Max

00:20:51,810 --> 00:20:57,990
and min on those and I don't necessarily

00:20:54,630 --> 00:21:01,950
think that is a good complexity thing to

00:20:57,990 --> 00:21:06,630
do so we should also just decide that oh

00:21:01,950 --> 00:21:09,390
well that's the limit of the complexity

00:21:06,630 --> 00:21:11,880
that we want to add to this and users

00:21:09,390 --> 00:21:17,910
page can do grouping to try to go around

00:21:11,880 --> 00:21:21,180
this if they want but we don't care so

00:21:17,910 --> 00:21:24,170
like the grouping policy has great

00:21:21,180 --> 00:21:27,270
expressiveness so my my alternative

00:21:24,170 --> 00:21:29,460
solution my thought was well we should

00:21:27,270 --> 00:21:32,520
also just let user space install a DPF

00:21:29,460 --> 00:21:34,710
program into the scheduler that will

00:21:32,520 --> 00:21:36,240
allowed to divide ups the add time

00:21:34,710 --> 00:21:39,390
between all the stations in whatever

00:21:36,240 --> 00:21:41,370
arbitrary way at once the problem is to

00:21:39,390 --> 00:21:44,050
do this you kind of need to loop over

00:21:41,370 --> 00:21:45,580
all the stations that are available so

00:21:44,050 --> 00:21:52,390
not sure that's doable and I'm not sure

00:21:45,580 --> 00:21:54,160
it's easier so yeah hmm also since we're

00:21:52,390 --> 00:21:56,050
doing more advanced scheduling we may

00:21:54,160 --> 00:21:58,240
need to move it out of the fast path and

00:21:56,050 --> 00:21:59,560
have some kind of internal thing it

00:21:58,240 --> 00:22:03,460
doesn't really matter that we are

00:21:59,560 --> 00:22:05,290
falling a bit behind H as I said on very

00:22:03,460 --> 00:22:07,900
small timescales you're going to be

00:22:05,290 --> 00:22:10,690
unfair pretty much no matter what you do

00:22:07,900 --> 00:22:13,030
because we we can't know in advance how

00:22:10,690 --> 00:22:14,650
long we're going to spend transmitting a

00:22:13,030 --> 00:22:16,630
packet as we can when we're counting

00:22:14,650 --> 00:22:20,260
bytes so so there's no way around that

00:22:16,630 --> 00:22:22,060
and like for our time policy the whole

00:22:20,260 --> 00:22:25,510
prerequisite of doing all of this is

00:22:22,060 --> 00:22:28,780
right now the airtime scheduler is in

00:22:25,510 --> 00:22:31,870
the driver in at 9k and I'm trying to

00:22:28,780 --> 00:22:35,220
change the API between the driver and my

00:22:31,870 --> 00:22:38,290
data to 11 so instead of the driver just

00:22:35,220 --> 00:22:40,930
get pull deciding which cue to pull from

00:22:38,290 --> 00:22:42,730
it will go and ask my data to 11 instead

00:22:40,930 --> 00:22:44,440
and say please give me the next cue to

00:22:42,730 --> 00:22:46,330
put packets from and then it will start

00:22:44,440 --> 00:22:48,250
pulling packets from there because that

00:22:46,330 --> 00:22:49,630
means it's really easy to do the

00:22:48,250 --> 00:22:51,820
scheduling and make it to 11 and the

00:22:49,630 --> 00:22:54,820
driver doesn't have to change and

00:22:51,820 --> 00:22:56,080
there's a I have a draft patch set for

00:22:54,820 --> 00:23:01,540
that it probably needs a bit more work

00:22:56,080 --> 00:23:03,730
but it's seems to work yeah any more

00:23:01,540 --> 00:23:07,020
comments on our time policies is this

00:23:03,730 --> 00:23:07,020
this is a good idea

00:23:15,610 --> 00:23:20,929
maybe you just answered this here I'm

00:23:17,990 --> 00:23:22,789
not sure but in the beginning you

00:23:20,929 --> 00:23:27,230
mentioned you've got this work in a th9

00:23:22,789 --> 00:23:29,720
ka th 10k I'm wondering even after this

00:23:27,230 --> 00:23:31,490
patch goes in how much of this niceness

00:23:29,720 --> 00:23:34,400
you've been adding to a reduced buffer

00:23:31,490 --> 00:23:37,220
blue is needs to be wired up

00:23:34,400 --> 00:23:38,390
specifically in each driver how much of

00:23:37,220 --> 00:23:40,340
this is really specific to how the

00:23:38,390 --> 00:23:42,770
hardware works and how much is this kind

00:23:40,340 --> 00:23:49,340
of generalized model of sending out

00:23:42,770 --> 00:23:52,669
airframes yeah so the what what this

00:23:49,340 --> 00:23:56,000
does is it changes the the API I think I

00:23:52,669 --> 00:23:59,299
maybe I forgot to mention this on on

00:23:56,000 --> 00:24:01,280
this slide with the structure here that

00:23:59,299 --> 00:24:04,159
as it like before we started changing

00:24:01,280 --> 00:24:06,530
this this is a push model where meg 802

00:24:04,159 --> 00:24:08,059
11 when it gets a packet from the

00:24:06,530 --> 00:24:10,130
networks that it would push it into the

00:24:08,059 --> 00:24:12,710
driver and the driver will chew it and

00:24:10,130 --> 00:24:14,570
then send it out later where this API

00:24:12,710 --> 00:24:16,039
between the drive and manga to 11 in

00:24:14,570 --> 00:24:18,559
this model has changed to a pull model

00:24:16,039 --> 00:24:20,600
where the driver just wakes up that like

00:24:18,559 --> 00:24:22,549
sorry Maggie to 11 wakes up the drive

00:24:20,600 --> 00:24:26,030
and says I have a packet for you and

00:24:22,549 --> 00:24:28,280
then the driver either on a call back

00:24:26,030 --> 00:24:31,460
later or immediately we hold that up to

00:24:28,280 --> 00:24:35,870
Magneto 211 sigh please give me a packet

00:24:31,460 --> 00:24:37,640
so the drivers need to be changed to use

00:24:35,870 --> 00:24:40,549
this API the problem is this thirty five

00:24:37,640 --> 00:24:41,510
thirty thirty five brothers so we don't

00:24:40,549 --> 00:24:44,120
really want to do that

00:24:41,510 --> 00:24:48,230
all of them oh that would take forever

00:24:44,120 --> 00:24:50,900
so Johannes has a has outlined a plan on

00:24:48,230 --> 00:24:52,850
how to move all of Maggie to eleven to

00:24:50,900 --> 00:24:54,559
just use this API and get rid of the

00:24:52,850 --> 00:24:56,750
whole the one by introducing a

00:24:54,559 --> 00:25:00,080
compatibility layer so that if the

00:24:56,750 --> 00:25:02,840
driver doesn't implement the hood itself

00:25:00,080 --> 00:25:05,000
made a 211 will do it for it and have

00:25:02,840 --> 00:25:08,179
like a shim layer between that will then

00:25:05,000 --> 00:25:10,460
push the package down and depending on

00:25:08,179 --> 00:25:12,380
how much queuing the driver does and how

00:25:10,460 --> 00:25:15,200
we can limit that you can get a lot of

00:25:12,380 --> 00:25:16,909
the benefit that way but I think like

00:25:15,200 --> 00:25:19,220
longer-term the drivers will have to

00:25:16,909 --> 00:25:21,260
change to use this API okay but at least

00:25:19,220 --> 00:25:23,120
it's just a change in the API you don't

00:25:21,260 --> 00:25:24,650
have to add coddle to every try

00:25:23,120 --> 00:25:27,890
ever no no that's that lives up here

00:25:24,650 --> 00:25:29,960
okay yeah so we want to to do as much as

00:25:27,890 --> 00:25:36,070
possible for the driver to to have this

00:25:29,960 --> 00:25:36,070
in manga to learn yes moving on

00:25:39,860 --> 00:25:52,309
show us handling what are your default

00:25:50,450 --> 00:25:55,160
assumptions of air time is not reported

00:25:52,309 --> 00:25:57,080
for whatever reason let's say the Nick

00:25:55,160 --> 00:25:58,670
crashed or you have just too much

00:25:57,080 --> 00:26:01,010
environmental noise you mentioned a case

00:25:58,670 --> 00:26:02,690
where a packet is retransmitted like

00:26:01,010 --> 00:26:05,770
thirty times before it's eventually

00:26:02,690 --> 00:26:08,960
succeeds or give up gives up I actually

00:26:05,770 --> 00:26:12,230
have a slide here how does it get ads on

00:26:08,960 --> 00:26:14,660
fairness in your driver and so like the

00:26:12,230 --> 00:26:15,590
idea is if you don't expose this at a

00:26:14,660 --> 00:26:17,630
driver level you would just get

00:26:15,590 --> 00:26:19,340
round-robin scheduling between the

00:26:17,630 --> 00:26:21,860
active stations and you will get to put

00:26:19,340 --> 00:26:24,440
fairness as is the default but what you

00:26:21,860 --> 00:26:26,450
have to do this is like the up driver up

00:26:24,440 --> 00:26:29,960
that you have to implement to switch to

00:26:26,450 --> 00:26:32,150
this API then we we have a new hardware

00:26:29,960 --> 00:26:35,120
flag which is air time accounting where

00:26:32,150 --> 00:26:37,010
you tell Merida to eleven basically I

00:26:35,120 --> 00:26:39,350
know what our time is and I will report

00:26:37,010 --> 00:26:40,970
it to you when I can and then what you

00:26:39,350 --> 00:26:44,059
have to do is this is the RX and TX

00:26:40,970 --> 00:26:45,500
status strux that you send up with each

00:26:44,059 --> 00:26:48,370
packet to Matt it to eleven there's

00:26:45,500 --> 00:26:52,720
fields in these for air time where you

00:26:48,370 --> 00:26:56,030
account how much air time you've used

00:26:52,720 --> 00:26:58,160
for the transmitted on on receive you

00:26:56,030 --> 00:27:01,820
can you can calculate how long did this

00:26:58,160 --> 00:27:03,559
packet take and and then magnitude

00:27:01,820 --> 00:27:05,660
eleven will do its thing and if these

00:27:03,559 --> 00:27:07,730
are zero or if you don't set the flag it

00:27:05,660 --> 00:27:09,110
will just fall back like we want to make

00:27:07,730 --> 00:27:10,940
sure that we just fall back to doing

00:27:09,110 --> 00:27:12,380
something sensible where you won't get

00:27:10,940 --> 00:27:13,750
the benefits but it won't hurt you

00:27:12,380 --> 00:27:22,490
either

00:27:13,750 --> 00:27:25,160
yes so QoS everyone likes QoS right yeah

00:27:22,490 --> 00:27:26,720
exactly so at least so the way Wi-Fi

00:27:25,160 --> 00:27:29,690
works normally like the standard

00:27:26,720 --> 00:27:31,880
specifies for QoS levels like the voice

00:27:29,690 --> 00:27:35,720
video best effort and background cues

00:27:31,880 --> 00:27:37,420
and those are the diff surf mappings are

00:27:35,720 --> 00:27:39,430
turned into these cues

00:27:37,420 --> 00:27:41,440
and everyone knows that diffserv is

00:27:39,430 --> 00:27:44,980
universally deployed and always works

00:27:41,440 --> 00:27:47,650
right so this is fine or not so much

00:27:44,980 --> 00:27:50,740
so apart from all the issues of how you

00:27:47,650 --> 00:27:52,840
get your right QoS things actually it is

00:27:50,740 --> 00:27:53,800
much less needed I have some more result

00:27:52,840 --> 00:27:56,230
I don't know if we will have time for

00:27:53,800 --> 00:27:58,330
them that shows that with these changes

00:27:56,230 --> 00:27:59,890
as far as latencies concern for example

00:27:58,330 --> 00:28:03,100
we can do almost as well with best

00:27:59,890 --> 00:28:05,110
effort as we could with the the BOQ

00:28:03,100 --> 00:28:07,690
before depending on the contention of

00:28:05,110 --> 00:28:10,540
the air times of course but there's also

00:28:07,690 --> 00:28:11,740
the queuing of the different us levels

00:28:10,540 --> 00:28:14,230
also has some issues and there's no

00:28:11,740 --> 00:28:16,780
admission control so for example you can

00:28:14,230 --> 00:28:19,390
it's perfectly possible to take a big

00:28:16,780 --> 00:28:21,970
fat TCP flow marked this serf package to

00:28:19,390 --> 00:28:25,140
go into the voq and completely throttle

00:28:21,970 --> 00:28:27,670
everything else going in that queue

00:28:25,140 --> 00:28:30,040
there's also the fact that when you have

00:28:27,670 --> 00:28:31,960
strict priority you get less aggregation

00:28:30,040 --> 00:28:33,670
which can also be an issue and then

00:28:31,960 --> 00:28:35,770
there's some interactions with that and

00:28:33,670 --> 00:28:38,560
fairness I'll go through these quickly

00:28:35,770 --> 00:28:40,660
now the first thing my idea was you can

00:28:38,560 --> 00:28:45,010
do a kind of soft Mission Control where

00:28:40,660 --> 00:28:47,050
you have your your vio tid over here and

00:28:45,010 --> 00:28:49,060
your best effort and then you have if

00:28:47,050 --> 00:28:51,730
you have one of these are peripheral

00:28:49,060 --> 00:28:53,410
cues right so what you can do is you can

00:28:51,730 --> 00:28:56,500
look at this crew and say AHA this flow

00:28:53,410 --> 00:28:58,330
is starting to build a queue on vio

00:28:56,500 --> 00:28:59,560
which is not good it shouldn't be doing

00:28:58,330 --> 00:29:01,450
that so what we're going to do is we

00:28:59,560 --> 00:29:03,610
just pull this whole flow out of this

00:29:01,450 --> 00:29:05,530
churn structure and move it over here to

00:29:03,610 --> 00:29:07,870
be e and remark the packets and sent

00:29:05,530 --> 00:29:09,250
them out as B while all the other flows

00:29:07,870 --> 00:29:11,860
that don't the look you they're allowed

00:29:09,250 --> 00:29:14,680
to go through vo so this sort of becomes

00:29:11,860 --> 00:29:16,510
a soft admission control in that it's

00:29:14,680 --> 00:29:18,790
not a fixed rate that you have to

00:29:16,510 --> 00:29:20,830
configure as a max rate for vo it's just

00:29:18,790 --> 00:29:22,750
if you are sending too many packets and

00:29:20,830 --> 00:29:24,460
you're building a big queue we are going

00:29:22,750 --> 00:29:27,280
to demote you to best-effort because

00:29:24,460 --> 00:29:29,860
then we can accurate the packets and you

00:29:27,280 --> 00:29:33,700
won't hurt the other well-behaved vo

00:29:29,860 --> 00:29:38,350
flows it's sort of a bit like how fq

00:29:33,700 --> 00:29:40,990
coddle will prioritize sparse flows but

00:29:38,350 --> 00:29:42,520
if you build up a queue once the

00:29:40,990 --> 00:29:45,010
round-robin scheduler comes around you

00:29:42,520 --> 00:29:48,250
will be right be shuttled as as best

00:29:45,010 --> 00:29:50,520
effort and this structure lets us do

00:29:48,250 --> 00:29:53,070
that with a bit of surgery

00:29:50,520 --> 00:29:55,590
so I haven't tried this yet but I think

00:29:53,070 --> 00:30:08,310
that could be a way to get rid of some

00:29:55,590 --> 00:30:11,700
of the the curious problems so given you

00:30:08,310 --> 00:30:13,650
have flow cues and you don't put cues

00:30:11,700 --> 00:30:15,540
and some of the cues don't even need the

00:30:13,650 --> 00:30:16,830
priority scaling anymore right I think

00:30:15,540 --> 00:30:18,180
it would give you the same result and

00:30:16,830 --> 00:30:22,050
just not having the priorities getting

00:30:18,180 --> 00:30:23,550
at all sorry if you would just use fq

00:30:22,050 --> 00:30:25,530
coddle and not have priority scheduling

00:30:23,550 --> 00:30:26,090
this might actually give you the same

00:30:25,530 --> 00:30:31,710
result

00:30:26,090 --> 00:30:35,430
yes but basically like but the thing is

00:30:31,710 --> 00:30:38,820
these also affect the Mac air time

00:30:35,430 --> 00:30:41,040
consumption app of air wave contention

00:30:38,820 --> 00:30:43,320
parameters and so if their senders vo

00:30:41,040 --> 00:30:45,860
you will get a smaller contention window

00:30:43,320 --> 00:30:48,720
so you would be more likely to grab the

00:30:45,860 --> 00:30:53,250
the the medium which is nice

00:30:48,720 --> 00:30:55,050
if it's really contended so so it's a

00:30:53,250 --> 00:30:58,620
way to preserve that but getting some of

00:30:55,050 --> 00:31:00,900
the benefits the other the other idea

00:30:58,620 --> 00:31:03,330
was Oh let's see we have this station

00:31:00,900 --> 00:31:06,540
that has one vo packet and 4b packets

00:31:03,330 --> 00:31:08,280
and what we do now is first we use a to

00:31:06,540 --> 00:31:10,680
hold transmit opportunity to sent the

00:31:08,280 --> 00:31:13,050
bun 1 vo packet without aggregation and

00:31:10,680 --> 00:31:16,730
then we do a separate transmission and

00:31:13,050 --> 00:31:19,470
sent before B Packard's so why just not

00:31:16,730 --> 00:31:21,990
combine them and send it all off in one

00:31:19,470 --> 00:31:24,870
aggregate you could even do like blog

00:31:21,990 --> 00:31:26,220
acts to do a crew acting separately and

00:31:24,870 --> 00:31:29,160
so on but I'm not I'm not sure that's

00:31:26,220 --> 00:31:31,650
needed and of course the question then

00:31:29,160 --> 00:31:36,000
becomes at which levels should this be

00:31:31,650 --> 00:31:38,430
sent as Leo as best effort you can't in

00:31:36,000 --> 00:31:40,380
vo is you can't send it you can't

00:31:38,430 --> 00:31:45,720
aggregate them do but VI for example you

00:31:40,380 --> 00:31:47,690
could could send it so that's also

00:31:45,720 --> 00:31:49,460
something that that might be worth doing

00:31:47,690 --> 00:31:54,450
mmm

00:31:49,460 --> 00:31:57,660
the next thing is how do we how - Q s

00:31:54,450 --> 00:31:59,250
intact with our time fairness so say you

00:31:57,660 --> 00:32:01,500
have these two stations with packets

00:31:59,250 --> 00:32:03,540
outstanding this one has used up all its

00:32:01,500 --> 00:32:04,050
airtime deficit so we shouldn't actually

00:32:03,540 --> 00:32:06,450
be

00:32:04,050 --> 00:32:08,250
because then we're not being fair but it

00:32:06,450 --> 00:32:10,080
has a vo packet whereas the one that

00:32:08,250 --> 00:32:12,810
should be sending according to the

00:32:10,080 --> 00:32:15,240
airtime scheduler does not and so right

00:32:12,810 --> 00:32:17,540
now we have strict Protege to link so we

00:32:15,240 --> 00:32:22,380
sent from vo first so we actually

00:32:17,540 --> 00:32:24,180
violate the fairness to get the the QoS

00:32:22,380 --> 00:32:25,560
parameters and I'm not sure this is the

00:32:24,180 --> 00:32:30,570
right thing to do I think what we should

00:32:25,560 --> 00:32:32,760
be doing is figure out which stations

00:32:30,570 --> 00:32:34,860
who sent to from the add-on funicula and

00:32:32,760 --> 00:32:37,950
then take the highest priority packet

00:32:34,860 --> 00:32:44,150
from that station and send it subject to

00:32:37,950 --> 00:32:51,570
the other stuff I was doing with GRS

00:32:44,150 --> 00:32:55,800
does anyone not agree with this well

00:32:51,570 --> 00:32:57,650
okay that's what we're doing then

00:32:55,800 --> 00:33:00,650
there's the last issue which is

00:32:57,650 --> 00:33:03,150
configurability so right now the only

00:33:00,650 --> 00:33:06,300
interface we have to all this is debug

00:33:03,150 --> 00:33:09,360
of s which is obviously not optimal and

00:33:06,300 --> 00:33:11,520
I'm gonna go downstairs and to the TC

00:33:09,360 --> 00:33:16,980
workshop and talk about this in a bit as

00:33:11,520 --> 00:33:19,410
well but we have basically three knobs

00:33:16,980 --> 00:33:22,290
that we can set the packet a memory

00:33:19,410 --> 00:33:25,020
limit and the quantum for F Q and the

00:33:22,290 --> 00:33:27,150
flag by add some fairness which tells

00:33:25,020 --> 00:33:32,330
the driver whether to account for thr X

00:33:27,150 --> 00:33:35,940
both I this one is is a debug thing I

00:33:32,330 --> 00:33:37,500
think so that's not necessary Sarah but

00:33:35,940 --> 00:33:39,300
this one corresponds to some of the

00:33:37,500 --> 00:33:41,640
curative knobs and the same thing with

00:33:39,300 --> 00:33:43,500
statistics so then idea for example

00:33:41,640 --> 00:33:46,470
should we since we have a no-shoe cure

00:33:43,500 --> 00:33:48,900
dish could we have the two dish

00:33:46,470 --> 00:33:50,460
statistics reach down into my data to 11

00:33:48,900 --> 00:33:52,560
pull out some statistics and return them

00:33:50,460 --> 00:33:54,630
to TC or should we do something

00:33:52,560 --> 00:33:58,680
different should we come up with a new

00:33:54,630 --> 00:34:00,150
interface to iw should we some of it is

00:33:58,680 --> 00:34:05,640
straightforward like our time stats we

00:34:00,150 --> 00:34:07,770
can just add 2 NL 82 11 but what what's

00:34:05,640 --> 00:34:14,630
a great way to integrate this with with

00:34:07,770 --> 00:34:14,630
tooling that we already have microphone

00:34:19,210 --> 00:34:25,990
so at what arity does the queue do two

00:34:24,490 --> 00:34:30,700
queues exist instead of on a per

00:34:25,990 --> 00:34:33,370
wireless device basis sorry do the queue

00:34:30,700 --> 00:34:38,260
instances exist on a wireless device

00:34:33,370 --> 00:34:42,669
basis yes okay so it kind of seems like

00:34:38,260 --> 00:34:45,520
some kind of net link interface through

00:34:42,669 --> 00:34:48,570
an O 802 11 to fetch these these these

00:34:45,520 --> 00:34:51,640
things seems to be the way to go right

00:34:48,570 --> 00:34:54,010
yeah but that means it would be

00:34:51,640 --> 00:34:56,080
different from any other interface it's

00:34:54,010 --> 00:34:57,520
a different interface see okay so that

00:34:56,080 --> 00:35:00,160
this is this is kind of what you buy

00:34:57,520 --> 00:35:02,350
into when you pull the queueing

00:35:00,160 --> 00:35:03,970
discipline and all this stuff out in

00:35:02,350 --> 00:35:05,830
cinema code it's your 11 you really need

00:35:03,970 --> 00:35:08,230
your own unique interfaces for this and

00:35:05,830 --> 00:35:10,450
there's no reason for the generic queue

00:35:08,230 --> 00:35:11,980
just Slayer to know about airtime and

00:35:10,450 --> 00:35:13,480
things like this people are playing with

00:35:11,980 --> 00:35:17,680
that stuff aren't interested right sure

00:35:13,480 --> 00:35:18,850
so I think a custom sort of I mean all

00:35:17,680 --> 00:35:20,470
you guys gonna use something other than

00:35:18,850 --> 00:35:24,670
FQ in the future so that's the next

00:35:20,470 --> 00:35:26,680
question don't think so probably I don't

00:35:24,670 --> 00:35:28,630
think so but but if that okay so the

00:35:26,680 --> 00:35:31,210
answer if the answer was yes that's

00:35:28,630 --> 00:35:32,380
possible then yeah we gotta think about

00:35:31,210 --> 00:35:35,560
the interface a little bit more

00:35:32,380 --> 00:35:37,660
seriously yeah so like you're right like

00:35:35,560 --> 00:35:38,800
add some stats and all this I think is

00:35:37,660 --> 00:35:40,000
pretty straightforward that should go

00:35:38,800 --> 00:35:41,560
into an L 802 11

00:35:40,000 --> 00:35:43,750
my thought was more that some of these

00:35:41,560 --> 00:35:45,760
things look a little bit like especially

00:35:43,750 --> 00:35:47,470
the statistics look a little bit like

00:35:45,760 --> 00:35:49,720
whatever you coddle is exposing for time

00:35:47,470 --> 00:35:51,160
right so should we expose that through

00:35:49,720 --> 00:35:54,910
the same channel so is that a good idea

00:35:51,160 --> 00:35:57,910
so you you could share the layout of the

00:35:54,910 --> 00:36:01,900
message that Cotto uses and encapsulate

00:35:57,910 --> 00:36:03,310
that in a nested attribute that Nano 802

00:36:01,900 --> 00:36:07,450
11 could spit back to the user

00:36:03,310 --> 00:36:09,480
that's just one idea yeah okay okay cool

00:36:07,450 --> 00:36:13,570
just as long as it's not the bug offense

00:36:09,480 --> 00:36:16,270
yes yes we're definitely moving it out

00:36:13,570 --> 00:36:18,610
of des purpose all right so in summary

00:36:16,270 --> 00:36:22,680
we've reduced Wi-Fi buffer bloat by an

00:36:18,610 --> 00:36:25,690
order of magnitude already yeah and

00:36:22,680 --> 00:36:28,270
almost perfect a time fairness in most

00:36:25,690 --> 00:36:30,070
cases in the drivers that support it

00:36:28,270 --> 00:36:32,770
and we have some ideas going forward and

00:36:30,070 --> 00:36:34,240
of course like there's this is when you

00:36:32,770 --> 00:36:37,690
get the slide this is a link you can

00:36:34,240 --> 00:36:39,130
click on to the original paper that we

00:36:37,690 --> 00:36:44,710
wrote describing these things it's in

00:36:39,130 --> 00:36:47,380
you snips ATC this summer and many

00:36:44,710 --> 00:36:49,900
thanks to all the people who have helped

00:36:47,380 --> 00:36:51,580
test this give feedback and the friendly

00:36:49,900 --> 00:36:55,300
Wireless maintainer for taking my

00:36:51,580 --> 00:36:56,860
patches and so on so any overarching

00:36:55,300 --> 00:37:03,910
questions I'm almost burning out of time

00:36:56,860 --> 00:37:06,070
actually and that's surprising so what

00:37:03,910 --> 00:37:09,850
about testing how do you test this new

00:37:06,070 --> 00:37:12,820
packet scheduler do you have a fake

00:37:09,850 --> 00:37:15,790
device to test that or because it's it

00:37:12,820 --> 00:37:20,350
seems to be I'm wondering what you

00:37:15,790 --> 00:37:22,900
didn't make it a proper scheduler back

00:37:20,350 --> 00:37:26,080
at school or with maybe some few

00:37:22,900 --> 00:37:28,990
callbacks that the driver could use to

00:37:26,080 --> 00:37:32,290
fetch a particular packet for a station

00:37:28,990 --> 00:37:34,900
whatever it seems to me that you could

00:37:32,290 --> 00:37:38,730
have just wrote a new packet scheduler

00:37:34,900 --> 00:37:42,520
that could be used by a device you know

00:37:38,730 --> 00:37:44,950
fake device to how you make sure that

00:37:42,520 --> 00:37:47,650
you you your change in this packet

00:37:44,950 --> 00:37:54,910
scheduler are well tested before

00:37:47,650 --> 00:37:58,270
reaching a Wi-Fi driver we don't have

00:37:54,910 --> 00:37:59,860
any automated testing of any of this

00:37:58,270 --> 00:38:01,630
what you mean but like when you say a

00:37:59,860 --> 00:38:05,020
packet scheduler do you mean at the

00:38:01,630 --> 00:38:07,420
cutest layer or yeah basically being

00:38:05,020 --> 00:38:11,260
able to use like something like it neti

00:38:07,420 --> 00:38:14,770
mm or you know something that could be

00:38:11,260 --> 00:38:17,830
easily tested with some existing

00:38:14,770 --> 00:38:20,230
infrared camera without having a real

00:38:17,830 --> 00:38:22,750
Wi-Fi yeah I mean you want to be able to

00:38:20,230 --> 00:38:25,650
assign it to a virtual interface first

00:38:22,750 --> 00:38:25,650
and all that so yeah

00:38:26,480 --> 00:38:32,640
so we we haven't hooked this up but we

00:38:30,000 --> 00:38:34,410
have the ability to use hardware SIM so

00:38:32,640 --> 00:38:36,150
we have underneath the McAdoo to leaven

00:38:34,410 --> 00:38:39,900
stack we have the hardware sim thing

00:38:36,150 --> 00:38:42,030
that basically pretends to be a bunch of

00:38:39,900 --> 00:38:44,460
Wi-Fi devices that are talking on a

00:38:42,030 --> 00:38:46,680
shared medium within the kernel so it's

00:38:44,460 --> 00:38:49,500
entirely virtual it has right now it has

00:38:46,680 --> 00:38:51,450
no notion of transmission time so it

00:38:49,500 --> 00:38:53,369
basically it's instantaneous right it's

00:38:51,450 --> 00:38:55,890
like the earth or something it takes the

00:38:53,369 --> 00:38:57,540
packet and it's just over here but if we

00:38:55,890 --> 00:38:59,220
add some sort of notion of transmission

00:38:57,540 --> 00:39:01,200
time between that and rate control and

00:38:59,220 --> 00:39:03,180
we have a way to pull that out into user

00:39:01,200 --> 00:39:04,830
space to effect the channel parameters

00:39:03,180 --> 00:39:07,859
and say alright this channel is really

00:39:04,830 --> 00:39:10,619
congested or this channel is really slow

00:39:07,859 --> 00:39:12,630
or you know the the RSSI between those

00:39:10,619 --> 00:39:14,220
two stations is really low so we could

00:39:12,630 --> 00:39:16,520
have a way of building we do have a way

00:39:14,220 --> 00:39:19,710
of building these fake topologies

00:39:16,520 --> 00:39:21,660
already for mesh testing in which mesh

00:39:19,710 --> 00:39:23,670
mesh paths are you taking and things

00:39:21,660 --> 00:39:25,380
like that so if we extend that just a

00:39:23,670 --> 00:39:27,210
little bit to have this notion of how

00:39:25,380 --> 00:39:29,550
long did it take to transmit this packet

00:39:27,210 --> 00:39:33,390
I think we could pretty easily build it

00:39:29,550 --> 00:39:35,609
on top of that it's not there right now

00:39:33,390 --> 00:39:37,530
they're using actual hardware for

00:39:35,609 --> 00:39:41,190
testing and that's kind of the first

00:39:37,530 --> 00:39:46,530
target for making sure it works I guess

00:39:41,190 --> 00:39:50,490
I could also add on the the FQ queuing

00:39:46,530 --> 00:39:54,540
stuff is in a library that can in like

00:39:50,490 --> 00:39:57,119
fq IMP header that you can use in other

00:39:54,540 --> 00:40:01,200
places as well so it's fairly generic if

00:39:57,119 --> 00:40:03,330
you have like I was I was talking with

00:40:01,200 --> 00:40:06,450
with Jason for example on using it for

00:40:03,330 --> 00:40:08,430
why God but we haven't decided if it's

00:40:06,450 --> 00:40:11,339
worth the effort yet if that's the right

00:40:08,430 --> 00:40:14,940
way to do it but like any anywhere that

00:40:11,339 --> 00:40:17,580
you have want to have this thing where

00:40:14,940 --> 00:40:19,200
you have vanished queueing but split up

00:40:17,580 --> 00:40:21,869
between different entities that need to

00:40:19,200 --> 00:40:29,540
be scheduled together on a couple of

00:40:21,869 --> 00:40:29,540
flows you can you can reuse this alright

00:40:29,660 --> 00:40:33,710
okay do you want to see some more slides

00:40:34,220 --> 00:40:38,520
we we already did this one so I have

00:40:37,589 --> 00:40:42,150
some more

00:40:38,520 --> 00:40:45,570
application graphs and so on so this is

00:40:42,150 --> 00:40:49,320
the HTTP page load time which like this

00:40:45,570 --> 00:40:55,320
is a large scale so this is 35 seconds

00:40:49,320 --> 00:40:57,210
down to one second far like that's both

00:40:55,320 --> 00:41:00,090
because of the the better throughput and

00:40:57,210 --> 00:41:03,000
the lower latency but maybe more

00:41:00,090 --> 00:41:05,490
interesting was this thing we did with

00:41:03,000 --> 00:41:08,400
with wipe where we tried out the

00:41:05,490 --> 00:41:11,100
different choice levels so if you send

00:41:08,400 --> 00:41:13,680
things on the BOQ if you send a voice

00:41:11,100 --> 00:41:16,350
flow on the voq before we made all these

00:41:13,680 --> 00:41:18,480
changes that gets priority obviously and

00:41:16,350 --> 00:41:19,770
you can get I'm not going to go into

00:41:18,480 --> 00:41:22,440
whether or not mass is a good measure

00:41:19,770 --> 00:41:24,330
but like four and a half is as far as a

00:41:22,440 --> 00:41:26,130
dose so this is pretty good wine is

00:41:24,330 --> 00:41:28,830
completely unusable it's a like

00:41:26,130 --> 00:41:33,180
synthetic benchmark that came out of the

00:41:28,830 --> 00:41:34,890
ITU but down here once we have the fqm

00:41:33,180 --> 00:41:37,230
act then you will notice that the best

00:41:34,890 --> 00:41:40,140
effort performance is actually better

00:41:37,230 --> 00:41:43,260
than the vo performance was before we

00:41:40,140 --> 00:41:48,020
make those changes so that's the thing

00:41:43,260 --> 00:41:50,160
where QoS becomes less important and

00:41:48,020 --> 00:41:52,140
there's still some things if the network

00:41:50,160 --> 00:41:55,260
is really contended but I guess means

00:41:52,140 --> 00:41:59,760
you can actually have usable wipe on

00:41:55,260 --> 00:42:01,290
your Wi-Fi network with like apps that

00:41:59,760 --> 00:42:04,620
can't say that they've served markings

00:42:01,290 --> 00:42:10,620
or ISPs that remove them and all these

00:42:04,620 --> 00:42:14,300
kinds of things the space station up to

00:42:10,620 --> 00:42:14,300
my session was this thing where we

00:42:16,850 --> 00:42:20,460
prioritize the station that only sends

00:42:19,080 --> 00:42:23,370
occasionally and that gives us like

00:42:20,460 --> 00:42:25,140
another few milliseconds and this test

00:42:23,370 --> 00:42:28,110
and then the last thing I want to show

00:42:25,140 --> 00:42:31,610
you is we also have someone else test

00:42:28,110 --> 00:42:35,250
this when we were when we were

00:42:31,610 --> 00:42:36,990
developing it with in a 30-second test

00:42:35,250 --> 00:42:38,670
bed where the slow station is only

00:42:36,990 --> 00:42:41,460
running at one megabit per second so

00:42:38,670 --> 00:42:45,120
it's artificially limit limited to like

00:42:41,460 --> 00:42:48,450
the 802 11 B 1 megabit per second second

00:42:45,120 --> 00:42:51,480
fire rate and there you can see even

00:42:48,450 --> 00:42:52,520
though there's 30 stations the slow

00:42:51,480 --> 00:42:55,790
station

00:42:52,520 --> 00:42:58,190
takes up two-thirds of the available our

00:42:55,790 --> 00:43:00,410
time whereas we can actually limit it

00:42:58,190 --> 00:43:02,080
even in this case with with 30 stations

00:43:00,410 --> 00:43:04,790
and then suddenly the throughput

00:43:02,080 --> 00:43:08,180
difference goes from like a factor of

00:43:04,790 --> 00:43:11,180
two or three to a factor of five six

00:43:08,180 --> 00:43:14,330
eight depending on which one you compare

00:43:11,180 --> 00:43:16,720
with and you can also see that this

00:43:14,330 --> 00:43:20,360
comes of course like the slow station

00:43:16,720 --> 00:43:27,560
will be throttled so the latency for

00:43:20,360 --> 00:43:30,110
that will of course be worse yes and

00:43:27,560 --> 00:43:32,930
this is the johannes outline for the

00:43:30,110 --> 00:43:35,410
things we need to do to to convert

00:43:32,930 --> 00:43:38,540
matter to eleven which is a lot of

00:43:35,410 --> 00:43:41,120
interesting things so I don't know

00:43:38,540 --> 00:43:44,440
that's if anyone is interested not that

00:43:41,120 --> 00:43:44,440

YouTube URL: https://www.youtube.com/watch?v=sZ-l_nBX1VQ


