Title: Netdev 2.2: Comprehensive XDP offâ€Œload-handling the edge cases
Publication date: 2018-03-15
Playlist: Netdev 2.2 - Day 3 - Nov 10 2017
Description: 
	Speakers: Jakub Kicinski and Nic Viljoen
Netdev 2.2, Seoul, Korea
Friday November 10th, 2017
URL: https://www.netdevconf.org/2.2/session.html?viljoen-xdpoffload-talk
Captions: 
	00:00:01,040 --> 00:00:06,189
systems so really it's about the work we

00:00:03,530 --> 00:00:08,780
have done our doing and will do in this

00:00:06,189 --> 00:00:11,180
so the agenda is going to be just a very

00:00:08,780 --> 00:00:13,010
quick refresher on the programming model

00:00:11,180 --> 00:00:15,440
and the architecture and how it actually

00:00:13,010 --> 00:00:17,869
fits onto the neck then I'm going to

00:00:15,440 --> 00:00:19,280
give a brief two minutes on quickly on

00:00:17,869 --> 00:00:20,780
the performance we're seeing today and

00:00:19,280 --> 00:00:21,440
the optimizations we're gonna be doing

00:00:20,780 --> 00:00:23,180
in the future

00:00:21,440 --> 00:00:25,160
that's just to give you an idea of kind

00:00:23,180 --> 00:00:26,750
of how this fits together and then

00:00:25,160 --> 00:00:28,520
beyond that we're actually going to go

00:00:26,750 --> 00:00:30,080
on to the stuff we really need to do to

00:00:28,520 --> 00:00:31,670
be able to use this in production and

00:00:30,080 --> 00:00:34,579
that's mainly what Jakob will be

00:00:31,670 --> 00:00:36,440
covering so the programming model and

00:00:34,579 --> 00:00:37,969
this is actually changed very recently

00:00:36,440 --> 00:00:40,280
slightly and Jakob will go through those

00:00:37,969 --> 00:00:43,249
changes but for now this is the model as

00:00:40,280 --> 00:00:45,050
was so really you you compile your

00:00:43,249 --> 00:00:46,749
programs you run your programs as you

00:00:45,050 --> 00:00:49,309
would do if it was not offloaded and

00:00:46,749 --> 00:00:51,350
everything runs as normal until you get

00:00:49,309 --> 00:00:55,339
down to the verifier once you get down

00:00:51,350 --> 00:00:58,039
to the verifier we run it as normal and

00:00:55,339 --> 00:00:59,449
then go down into the driver where we

00:00:58,039 --> 00:01:01,249
can detect if you actually want to

00:00:59,449 --> 00:01:04,280
offload the program so this is done via

00:01:01,249 --> 00:01:07,670
IP route or if you're using CLS BPF you

00:01:04,280 --> 00:01:10,310
can do it for RTC and at that point we

00:01:07,670 --> 00:01:13,580
then rerun the Vera file and have a few

00:01:10,310 --> 00:01:15,470
extra checks which we use for the for

00:01:13,580 --> 00:01:16,460
the JIT and for the actual offload so

00:01:15,470 --> 00:01:18,440
there are a few things like for example

00:01:16,460 --> 00:01:20,300
if you've got a map type we don't yet

00:01:18,440 --> 00:01:21,710
support in which case we won't actually

00:01:20,300 --> 00:01:24,320
offload the program and then you just

00:01:21,710 --> 00:01:25,970
run in the driver as per normal so

00:01:24,320 --> 00:01:27,440
really it's the idea is to give you the

00:01:25,970 --> 00:01:30,440
offload if you want it and not if you

00:01:27,440 --> 00:01:33,890
don't thanks a lot

00:01:30,440 --> 00:01:35,150
so this is just an I apologize it seems

00:01:33,890 --> 00:01:37,430
that our slides are cutting off at the

00:01:35,150 --> 00:01:40,190
side but what this is about is this is

00:01:37,430 --> 00:01:43,970
to show just how it fits onto the actual

00:01:40,190 --> 00:01:45,350
NIC so you've got your ten BPF registers

00:01:43,970 --> 00:01:47,960
they go into the our general-purpose

00:01:45,350 --> 00:01:50,180
registers we have about 64 of those per

00:01:47,960 --> 00:01:52,730
thread and you've got sixty cores on the

00:01:50,180 --> 00:01:54,080
device so and four threads per core

00:01:52,730 --> 00:01:56,510
running so there's actually eight

00:01:54,080 --> 00:01:58,340
threads were really useful for our BPF

00:01:56,510 --> 00:02:00,260
offload at the moment and then you put

00:01:58,340 --> 00:02:02,930
the sack into the local memory where we

00:02:00,260 --> 00:02:05,510
have about a kilobyte per thread on each

00:02:02,930 --> 00:02:08,780
core and then at the moment the maps of

00:02:05,510 --> 00:02:10,639
all sizes go down actually into DRAM so

00:02:08,780 --> 00:02:11,959
we have about two gig of DRAM there we

00:02:10,639 --> 00:02:13,219
can actually hold up to eight gig of

00:02:11,959 --> 00:02:14,750
DRAM they're on they're on they're on

00:02:13,219 --> 00:02:16,700
the NIC if we want to through

00:02:14,750 --> 00:02:18,260
chip and we have a bunch of other

00:02:16,700 --> 00:02:19,340
memories which we don't actually use at

00:02:18,260 --> 00:02:22,670
this point in time but they've become

00:02:19,340 --> 00:02:24,260
important later on so this is just to

00:02:22,670 --> 00:02:27,740
give you a quick example of performance

00:02:24,260 --> 00:02:30,200
we're seeing so I quickly hacked up a

00:02:27,740 --> 00:02:32,120
simple XDP load balancer it came to

00:02:30,200 --> 00:02:33,730
about 800 vpf instructions and there are

00:02:32,120 --> 00:02:36,140
four lookups

00:02:33,730 --> 00:02:37,459
it's based on the TC example in the

00:02:36,140 --> 00:02:42,020
kernel which you can find in self-test

00:02:37,459 --> 00:02:44,300
BPF L 4 L 4 L B dot C and then I just

00:02:42,020 --> 00:02:47,180
combined it with the IP tunneling from

00:02:44,300 --> 00:02:50,150
XD PT x IP tunnel to create a sim very

00:02:47,180 --> 00:02:51,890
simple load balancer it's you know it's

00:02:50,150 --> 00:02:54,440
not optimized big hal phoning this is

00:02:51,890 --> 00:02:56,750
not perfect the one change I had to make

00:02:54,440 --> 00:02:58,850
to offload it was there's a per CPU

00:02:56,750 --> 00:03:00,860
array for the stats I had to change that

00:02:58,850 --> 00:03:05,360
to a standard array because we don't

00:03:00,860 --> 00:03:06,620
quite hold the the actual per CPU system

00:03:05,360 --> 00:03:07,970
doesn't really make sense for us at the

00:03:06,620 --> 00:03:11,530
moment we would have to put a small map

00:03:07,970 --> 00:03:14,209
and eat every single core which would be

00:03:11,530 --> 00:03:15,410
on ideal at this point in time so we can

00:03:14,209 --> 00:03:17,780
do that in the future but at the moment

00:03:15,410 --> 00:03:20,330
that's not something we're doing so I

00:03:17,780 --> 00:03:21,980
run it as is today with the offload as

00:03:20,330 --> 00:03:24,230
we have it in our PLC at the moment and

00:03:21,980 --> 00:03:26,269
we got to about 24 million packets per

00:03:24,230 --> 00:03:27,560
second in the driver ran at about 21

00:03:26,269 --> 00:03:29,810
million packets per second which is

00:03:27,560 --> 00:03:32,930
about 2.6 million packets per second per

00:03:29,810 --> 00:03:34,820
core so you can you're getting a lot

00:03:32,930 --> 00:03:36,739
from the offload and then what I did was

00:03:34,820 --> 00:03:37,940
a few of the maps are quite small so at

00:03:36,739 --> 00:03:40,400
the moment as she saw from the first

00:03:37,940 --> 00:03:41,780
slide we put all the maps in 2d Ram now

00:03:40,400 --> 00:03:43,700
if you actually take the smaller maps

00:03:41,780 --> 00:03:45,890
and you put them higher up so you use

00:03:43,700 --> 00:03:47,720
the memories which are much closer to

00:03:45,890 --> 00:03:49,670
the cause then we were getting this

00:03:47,720 --> 00:03:51,950
running up to about 40 to 43 million

00:03:49,670 --> 00:03:53,150
packets per second it's just a rough

00:03:51,950 --> 00:03:55,430
numbers just to give you an idea of

00:03:53,150 --> 00:03:57,560
where we stand today and there's still a

00:03:55,430 --> 00:03:59,120
lot of optimization left to do so at the

00:03:57,560 --> 00:03:59,630
moment the map placements can be

00:03:59,120 --> 00:04:01,610
improved

00:03:59,630 --> 00:04:02,570
I put caching there but really you can't

00:04:01,610 --> 00:04:04,610
really use caching if you're doing

00:04:02,570 --> 00:04:07,310
things like DOS so it's really about the

00:04:04,610 --> 00:04:08,900
placement and then what I've called the

00:04:07,310 --> 00:04:10,280
packet cashiers we effectively we

00:04:08,900 --> 00:04:13,100
actually have a store of the packet on

00:04:10,280 --> 00:04:14,450
the core in each thread as it's running

00:04:13,100 --> 00:04:16,609
but we're not actually using that today

00:04:14,450 --> 00:04:17,900
today when any packet access we do we're

00:04:16,609 --> 00:04:20,239
actually going to a piece of memory

00:04:17,900 --> 00:04:21,890
which is on the island which is about 50

00:04:20,239 --> 00:04:24,110
cycles away even though we actually have

00:04:21,890 --> 00:04:25,700
a packet sitting three cycles away now

00:04:24,110 --> 00:04:28,160
that's because it was easier to do and

00:04:25,700 --> 00:04:28,610
it was quick to get done now that's an

00:04:28,160 --> 00:04:31,969
optimized

00:04:28,610 --> 00:04:33,080
we have in the future and John has in

00:04:31,969 --> 00:04:36,650
the audience has been doing a lot of

00:04:33,080 --> 00:04:38,840
work on 32-bit areas for Olivia and that

00:04:36,650 --> 00:04:41,030
obviously then also is significantly

00:04:38,840 --> 00:04:42,379
better when you have a 32-bit

00:04:41,030 --> 00:04:43,580
architecture like we have because it

00:04:42,379 --> 00:04:46,009
massively reduces the amount of

00:04:43,580 --> 00:04:47,389
instructions you're actually using and

00:04:46,009 --> 00:04:51,169
finally there's some firmware locks

00:04:47,389 --> 00:04:52,610
which we can get rid of but the

00:04:51,169 --> 00:04:54,199
important thing in what we really want

00:04:52,610 --> 00:04:56,240
to focus on today and what we want to

00:04:54,199 --> 00:04:59,270
focus on in this talk is about how we

00:04:56,240 --> 00:05:01,069
get ready to use this in production how

00:04:59,270 --> 00:05:02,509
you can use this and how you can feel

00:05:01,069 --> 00:05:04,520
safe and comfortable when you're running

00:05:02,509 --> 00:05:06,830
it in a you know in a massive massive

00:05:04,520 --> 00:05:08,750
data center so the first thing we have

00:05:06,830 --> 00:05:10,550
is and there so briefly touch on is

00:05:08,750 --> 00:05:12,469
multistage processing but we've caught

00:05:10,550 --> 00:05:14,419
it here with what that really means is

00:05:12,469 --> 00:05:16,310
that there could be situations where

00:05:14,419 --> 00:05:17,539
you've got a huge chunk of BPF programs

00:05:16,310 --> 00:05:19,550
running and you don't want to run all of

00:05:17,539 --> 00:05:20,840
them in the offload mode you actually

00:05:19,550 --> 00:05:22,849
want to offload some of them and run

00:05:20,840 --> 00:05:24,979
some of them in the driver that could be

00:05:22,849 --> 00:05:26,509
for a variety of reasons you could have

00:05:24,979 --> 00:05:27,800
new features which haven't even yet been

00:05:26,509 --> 00:05:29,750
implemented in the offload which are

00:05:27,800 --> 00:05:31,330
running in the in the driver there could

00:05:29,750 --> 00:05:34,039
be many other things going on like that

00:05:31,330 --> 00:05:37,099
or you could have you know you could

00:05:34,039 --> 00:05:38,120
have maps which are 20 30 gig in size

00:05:37,099 --> 00:05:41,000
that and that you could have some

00:05:38,120 --> 00:05:42,830
something funny going on like that so

00:05:41,000 --> 00:05:44,300
there's some reasons why you would want

00:05:42,830 --> 00:05:47,599
to run some of the programs locally and

00:05:44,300 --> 00:05:49,039
others remotely or offered and then

00:05:47,599 --> 00:05:51,889
obviously we'll talk about the debug

00:05:49,039 --> 00:05:53,419
work and finally we'll mention about the

00:05:51,889 --> 00:05:57,770
JIT architecture and the changes we've

00:05:53,419 --> 00:05:59,270
made whereby the verifier now reruns in

00:05:57,770 --> 00:06:01,400
a different place so effectively you

00:05:59,270 --> 00:06:02,539
rerun the verifier much earlier that's

00:06:01,400 --> 00:06:05,060
before you have a lot of the

00:06:02,539 --> 00:06:06,919
optimization for the host architecture

00:06:05,060 --> 00:06:08,210
happening which means it's much easier

00:06:06,919 --> 00:06:12,469
to offload it and it means we can keep

00:06:08,210 --> 00:06:13,699
feature parity much quicker so just

00:06:12,469 --> 00:06:16,610
quickly I'll touch on the multi stage

00:06:13,699 --> 00:06:18,889
processing before handling over we can

00:06:16,610 --> 00:06:20,389
obviously the education run into here is

00:06:18,889 --> 00:06:21,560
what if you have tail calls which are

00:06:20,389 --> 00:06:24,500
going to different programs you have

00:06:21,560 --> 00:06:25,939
some programs leaving the NIC having

00:06:24,500 --> 00:06:28,550
done let's say three or four PPF

00:06:25,939 --> 00:06:31,909
programs that want to then go to a

00:06:28,550 --> 00:06:33,589
particular other PPF program whereas

00:06:31,909 --> 00:06:35,750
some of the other examples could tell

00:06:33,589 --> 00:06:37,580
call into a different BBF program so you

00:06:35,750 --> 00:06:39,500
could use something like the XDP are the

00:06:37,580 --> 00:06:41,930
data meta fields which daniel Bourquin

00:06:39,500 --> 00:06:42,559
added to make sure that you actually go

00:06:41,930 --> 00:06:44,959
to the

00:06:42,559 --> 00:06:46,429
correct next program it's it's just a

00:06:44,959 --> 00:06:48,019
very simple thing which you need to be

00:06:46,429 --> 00:06:50,239
able to handle I think once you get to

00:06:48,019 --> 00:06:53,449
the stage of using this in significant

00:06:50,239 --> 00:06:55,399
in angle ready so I think that's pretty

00:06:53,449 --> 00:06:57,019
much it for me at this stage I'll hand

00:06:55,399 --> 00:06:59,269
over to Jack oh yeah coop who'll talk

00:06:57,019 --> 00:07:02,409
about most of the most of the real work

00:06:59,269 --> 00:07:05,989
we're doing thank you

00:07:02,409 --> 00:07:08,509
so updates we gave a presentation about

00:07:05,989 --> 00:07:10,459
the same subject a year ago and I will

00:07:08,509 --> 00:07:14,119
go through the items and what we've been

00:07:10,459 --> 00:07:16,519
doing for the last year so things we

00:07:14,119 --> 00:07:18,409
changed upstream there were some

00:07:16,519 --> 00:07:20,389
instruction that onion Daniel added for

00:07:18,409 --> 00:07:22,129
comparisons so he was nice enough to

00:07:20,389 --> 00:07:24,469
implement them for our Gees as well

00:07:22,129 --> 00:07:26,599
John has added a negation instruction

00:07:24,469 --> 00:07:29,269
and I've added a bite swap and I think

00:07:26,599 --> 00:07:30,619
some others so that just I think we are

00:07:29,269 --> 00:07:32,539
pretty much covering the basic

00:07:30,619 --> 00:07:36,019
instruction set for BPF or at least

00:07:32,539 --> 00:07:39,439
would LLVM is using we also added direct

00:07:36,019 --> 00:07:42,019
packet access so which is the newer way

00:07:39,439 --> 00:07:44,809
of accessing packet data in DB PF so

00:07:42,019 --> 00:07:46,729
based on packet pointers directly so

00:07:44,809 --> 00:07:49,749
that's supported it required some

00:07:46,729 --> 00:07:52,069
verifier changes and basically sparked

00:07:49,749 --> 00:07:53,779
their decision to change our

00:07:52,069 --> 00:07:56,599
architecture slightly I will talk about

00:07:53,779 --> 00:08:03,489
that in more in detail later we also

00:07:56,599 --> 00:08:03,489
have full stack support and all 512 512

00:08:04,089 --> 00:08:09,319
lat

00:08:05,389 --> 00:08:13,159
so unaligned accesses yes pretty much

00:08:09,319 --> 00:08:15,109
anything I put a just cat helper here we

00:08:13,159 --> 00:08:16,399
don't have that upstream yet but I was

00:08:15,109 --> 00:08:18,649
hoping I would be able to post the

00:08:16,399 --> 00:08:20,929
patches during the week but I wasn't but

00:08:18,649 --> 00:08:23,529
that's all ready so we will be able to

00:08:20,929 --> 00:08:28,389
support adjusted perhaps in the next

00:08:23,529 --> 00:08:31,999
match window but yeah hopefully soon and

00:08:28,389 --> 00:08:34,309
as Nick mentioned John has added a basic

00:08:31,999 --> 00:08:36,709
32-bit register support or LLVM and

00:08:34,309 --> 00:08:38,929
we'll be continuing the work both in the

00:08:36,709 --> 00:08:43,519
kernel LLVM and basically of the two

00:08:38,929 --> 00:08:45,040
chains and there's a whole bunch of

00:08:43,519 --> 00:08:48,559
stuff that we have prototyped and

00:08:45,040 --> 00:08:49,639
implemented internally to test the

00:08:48,559 --> 00:08:52,399
performance and see where our

00:08:49,639 --> 00:08:55,420
bottlenecks are and also to just get a

00:08:52,399 --> 00:08:56,560
feel of how this stuff all fits together

00:08:55,420 --> 00:08:58,149
and I don't think we will be up

00:08:56,560 --> 00:09:00,370
streaming it when in the form that it is

00:08:58,149 --> 00:09:02,920
today because there's a lot of sharp

00:09:00,370 --> 00:09:04,240
edges but we do have map of basically

00:09:02,920 --> 00:09:06,459
running and Nick was showing the

00:09:04,240 --> 00:09:09,100
performance numbers for for the

00:09:06,459 --> 00:09:12,279
implementation and we have atomic

00:09:09,100 --> 00:09:14,769
operations as well so the atomic EDB PF

00:09:12,279 --> 00:09:17,139
can support we have other atomic

00:09:14,769 --> 00:09:18,790
operations but well we can expose them

00:09:17,139 --> 00:09:21,850
in the instruction set if people need

00:09:18,790 --> 00:09:24,490
them but right the chip can can do more

00:09:21,850 --> 00:09:27,850
atomic operations and then just add also

00:09:24,490 --> 00:09:29,500
Nick mentioned the optimization so then

00:09:27,850 --> 00:09:31,899
yeah I will talk about this in detail

00:09:29,500 --> 00:09:34,510
slightly because we require some api's

00:09:31,899 --> 00:09:36,250
perhaps to be added to the kernel but we

00:09:34,510 --> 00:09:38,800
the junk I started working on some

00:09:36,250 --> 00:09:40,870
optimization energy and those

00:09:38,800 --> 00:09:42,610
optimization will probably become more

00:09:40,870 --> 00:09:45,579
and more complex so the ones that we

00:09:42,610 --> 00:09:47,110
have right now is we have some that are

00:09:45,579 --> 00:09:50,800
very simple which are already upstream

00:09:47,110 --> 00:09:52,990
but there are the the latest one was mem

00:09:50,800 --> 00:09:56,889
copy optimization that's not yet posted

00:09:52,990 --> 00:09:58,630
upstream but basically there's there are

00:09:56,889 --> 00:10:02,320
no no loops in VBF right now

00:09:58,630 --> 00:10:05,019
so mam copies are completely unrolled

00:10:02,320 --> 00:10:08,440
and that's that generates slightly an

00:10:05,019 --> 00:10:09,970
optimal IO pattern for us so John has

00:10:08,440 --> 00:10:12,160
added optimization to basically roll

00:10:09,970 --> 00:10:17,320
them back in into a single big eye

00:10:12,160 --> 00:10:20,260
operation and yes the the work and

00:10:17,320 --> 00:10:22,149
register tracking basically it comes

00:10:20,260 --> 00:10:24,910
down to trying to make sure that if

00:10:22,149 --> 00:10:27,490
there's a 32-bit value in the kept by

00:10:24,910 --> 00:10:29,470
the register we don't have to do 64-bit

00:10:27,490 --> 00:10:35,290
ALU operations on them so save some

00:10:29,470 --> 00:10:37,240
cycles and co space there the bigger

00:10:35,290 --> 00:10:39,850
changes that we have contributed are

00:10:37,240 --> 00:10:41,529
mostly about tuning obviously when you

00:10:39,850 --> 00:10:43,630
start implementing a more complex

00:10:41,529 --> 00:10:45,940
project you need to think about tooling

00:10:43,630 --> 00:10:48,910
and one of the first things that we need

00:10:45,940 --> 00:10:50,649
it is is a way of accident of performing

00:10:48,910 --> 00:10:55,029
map operations and make it flexible and

00:10:50,649 --> 00:10:56,680
easy way if you look at the BPF samples

00:10:55,029 --> 00:10:58,720
that are in the kernel they're most

00:10:56,680 --> 00:11:01,029
basically all VP of programs do they

00:10:58,720 --> 00:11:02,589
have sort of two elements to them the

00:11:01,029 --> 00:11:05,290
control plane and the data plane so a

00:11:02,589 --> 00:11:07,269
BPF is the big data plane but there's

00:11:05,290 --> 00:11:07,880
also a control plane program that

00:11:07,269 --> 00:11:09,350
understand

00:11:07,880 --> 00:11:11,200
to have the BPF program works and

00:11:09,350 --> 00:11:14,240
communicates with it through the maps

00:11:11,200 --> 00:11:16,400
and that's that that's how it usually

00:11:14,240 --> 00:11:18,230
deployed but for testing purposes and

00:11:16,400 --> 00:11:20,360
for just introspection into seeing

00:11:18,230 --> 00:11:22,160
what's running in the system it's useful

00:11:20,360 --> 00:11:24,940
to be able to just perform like single

00:11:22,160 --> 00:11:28,760
operations like map updates and lookups

00:11:24,940 --> 00:11:31,100
by hand from command line so we added

00:11:28,760 --> 00:11:34,010
the PPF - and it's in the kernel tree

00:11:31,100 --> 00:11:35,810
and will be in the 415 release it's also

00:11:34,010 --> 00:11:37,970
github but we will probably remove it

00:11:35,810 --> 00:11:40,850
from github because the the entry

00:11:37,970 --> 00:11:43,520
version is more up-to-date and it

00:11:40,850 --> 00:11:46,070
basically allows to list and read

00:11:43,520 --> 00:11:49,400
information about all the EPF objects in

00:11:46,070 --> 00:11:51,370
the system so all the stuff that that

00:11:49,400 --> 00:11:53,720
the kernel exposes this basically work

00:11:51,370 --> 00:11:56,000
leverages the the work that Martine from

00:11:53,720 --> 00:11:58,430
Facebook did to expose all the BPF

00:11:56,000 --> 00:12:00,110
information and some of the old stuff

00:11:58,430 --> 00:12:02,390
that was already there in the file

00:12:00,110 --> 00:12:04,310
descriptor info and that gathers them in

00:12:02,390 --> 00:12:07,000
all in one place that that's easy for

00:12:04,310 --> 00:12:10,550
people to access and it also allows the

00:12:07,000 --> 00:12:13,070
damping the jaded images the the host GZ

00:12:10,550 --> 00:12:15,350
images and just BPF instructions and

00:12:13,070 --> 00:12:19,010
printing them and as I said on the map

00:12:15,350 --> 00:12:21,980
operations yes and recently Quentin also

00:12:19,010 --> 00:12:23,510
added a JSON output which is extremely

00:12:21,980 --> 00:12:25,490
useful for us first testing hopefully

00:12:23,510 --> 00:12:28,670
for other people as well and there is

00:12:25,490 --> 00:12:30,830
some BBFS integration also that Quentin

00:12:28,670 --> 00:12:33,170
that didn't we had patches from pressure

00:12:30,830 --> 00:12:35,870
on Prashant lightning which is very nice

00:12:33,170 --> 00:12:41,090
to see someone else found outside the

00:12:35,870 --> 00:12:43,940
company contributing to do - already so

00:12:41,090 --> 00:12:45,980
another piece of tooling that we really

00:12:43,940 --> 00:12:49,100
needed and have put upstream with the

00:12:45,980 --> 00:12:52,240
Jencks work on LLVM machine code that's

00:12:49,100 --> 00:12:54,800
basically allows vm macro assembler and

00:12:52,240 --> 00:12:57,770
we started writing the tests basically

00:12:54,800 --> 00:13:00,500
if writing is it harder translator we

00:12:57,770 --> 00:13:02,960
have a need to test this particular BBF

00:13:00,500 --> 00:13:05,690
snippets like sequences of instructions

00:13:02,960 --> 00:13:07,010
and the way that the the test

00:13:05,690 --> 00:13:09,730
infrastructure in the kernel does it

00:13:07,010 --> 00:13:12,140
today is basically they include

00:13:09,730 --> 00:13:13,490
instructions inside the C program and

00:13:12,140 --> 00:13:16,070
there are like a static array of

00:13:13,490 --> 00:13:18,470
instructions but that's not great for

00:13:16,070 --> 00:13:21,020
testing and for adding tests in the test

00:13:18,470 --> 00:13:23,839
cases so we added

00:13:21,020 --> 00:13:25,520
VMC so basically right now we can just

00:13:23,839 --> 00:13:28,970
write the BPF programs in the same

00:13:25,520 --> 00:13:35,060
format as the verifier uses for for

00:13:28,970 --> 00:13:38,240
logging the it's log output and assemble

00:13:35,060 --> 00:13:39,770
that into elf elf files and and load

00:13:38,240 --> 00:13:43,430
that directly so that's really useful

00:13:39,770 --> 00:13:45,800
for us and that also will be useful if

00:13:43,430 --> 00:13:49,100
someone wants to add inline assembly to

00:13:45,800 --> 00:13:50,690
be PFC code in a VM we are probably not

00:13:49,100 --> 00:13:52,730
going to be too interested in this

00:13:50,690 --> 00:13:58,100
ourselves but it opens the way to

00:13:52,730 --> 00:14:01,640
implement that so now into the kernel

00:13:58,100 --> 00:14:04,430
kernel changes this is how the PPF

00:14:01,640 --> 00:14:07,940
offload sort of worked until recently

00:14:04,430 --> 00:14:11,839
and you can see on the left hand side is

00:14:07,940 --> 00:14:14,300
the BPF subsystem in from into

00:14:11,839 --> 00:14:17,360
interactions and on the right is is the

00:14:14,300 --> 00:14:19,070
networking stuff and one thing that I

00:14:17,360 --> 00:14:21,500
should point out from the start is that

00:14:19,070 --> 00:14:24,649
there is nothing no lines crossing from

00:14:21,500 --> 00:14:26,660
the one to the other so BPF programs are

00:14:24,649 --> 00:14:29,000
loaded and verified and basically

00:14:26,660 --> 00:14:32,690
prepared and then there's another system

00:14:29,000 --> 00:14:35,300
call which is attaching them to a

00:14:32,690 --> 00:14:38,050
specific place in the kernel so when the

00:14:35,300 --> 00:14:40,700
user wants to load the program the it

00:14:38,050 --> 00:14:43,339
everything is done by the vpf subsystem

00:14:40,700 --> 00:14:45,649
but if we want to offload that program

00:14:43,339 --> 00:14:48,079
later for for the network device there

00:14:45,649 --> 00:14:50,410
is no no way for us to know that the

00:14:48,079 --> 00:14:53,360
time of the first verification and

00:14:50,410 --> 00:14:56,810
basically a lot of information is lost

00:14:53,360 --> 00:15:00,920
or things are changed and that makes the

00:14:56,810 --> 00:15:04,160
offload more challenging so that this is

00:15:00,920 --> 00:15:06,110
the the way that we used to work when

00:15:04,160 --> 00:15:07,970
the EPF program was attached and the

00:15:06,110 --> 00:15:10,910
driver was informed either for the TCC

00:15:07,970 --> 00:15:13,339
India or in xdp that there will be an

00:15:10,910 --> 00:15:15,290
offload we would try to reuse the EPF

00:15:13,339 --> 00:15:16,910
infrastructure and sort of pretend that

00:15:15,290 --> 00:15:21,490
we loading the program for the second

00:15:16,910 --> 00:15:24,770
time and basically rerun the entire load

00:15:21,490 --> 00:15:26,300
part of the verification and that would

00:15:24,770 --> 00:15:29,480
allow us to gather information and then

00:15:26,300 --> 00:15:32,329
underneath the image did the program

00:15:29,480 --> 00:15:34,130
into the hardware but as I said when we

00:15:32,329 --> 00:15:35,750
start adding more

00:15:34,130 --> 00:15:40,790
advanced features this becomes

00:15:35,750 --> 00:15:42,530
problematic and the reason is as I said

00:15:40,790 --> 00:15:46,220
there is no information for which device

00:15:42,530 --> 00:15:47,930
the the program is loaded and and a big

00:15:46,220 --> 00:15:49,490
problem is that the verifier when the

00:15:47,930 --> 00:15:53,150
program's loaded will actually modify it

00:15:49,490 --> 00:15:55,250
so after the program is loaded the

00:15:53,150 --> 00:15:58,610
original image that the user has loaded

00:15:55,250 --> 00:16:01,190
is completely lost and we sort of have

00:15:58,610 --> 00:16:08,300
to make reverse the modifications to

00:16:01,190 --> 00:16:12,440
offload it so what we have done is right

00:16:08,300 --> 00:16:14,150
now we have hooked the the driver of

00:16:12,440 --> 00:16:16,430
those stuff directly into the first

00:16:14,150 --> 00:16:18,620
invocation of the EPF program loading I

00:16:16,430 --> 00:16:20,750
wonder when users are loading the

00:16:18,620 --> 00:16:24,680
program they will they can optionally

00:16:20,750 --> 00:16:26,240
specify the device index the the net the

00:16:24,680 --> 00:16:28,610
index for which the program will be

00:16:26,240 --> 00:16:30,890
loaded and then the PPF system call

00:16:28,610 --> 00:16:34,460
which find that will find that net

00:16:30,890 --> 00:16:37,250
device and basically inform it about the

00:16:34,460 --> 00:16:39,650
the the program as it goes through the

00:16:37,250 --> 00:16:44,090
verification and the cheating stages and

00:16:39,650 --> 00:16:46,670
that all will be done before before the

00:16:44,090 --> 00:16:52,000
modifications take place so basically

00:16:46,670 --> 00:16:55,850
the load system code will be invoked and

00:16:52,000 --> 00:16:58,520
the appropriate net negative is found

00:16:55,850 --> 00:17:02,360
and bound to the program and the per did

00:16:58,520 --> 00:17:04,250
the drivers an informed about program

00:17:02,360 --> 00:17:06,920
being loaded for it it can prepare all

00:17:04,250 --> 00:17:09,199
its all the data structures for

00:17:06,920 --> 00:17:11,600
translation and then on every stage of

00:17:09,199 --> 00:17:13,790
the verification it will get a callback

00:17:11,600 --> 00:17:17,350
with the current state so it can learn

00:17:13,790 --> 00:17:21,110
about what the state of the program is

00:17:17,350 --> 00:17:23,000
executed and when the JIT happens we

00:17:21,110 --> 00:17:25,490
will not perform the JIT for the hosts

00:17:23,000 --> 00:17:28,910
we will again just call the driver and

00:17:25,490 --> 00:17:31,040
the driver will generate the image and

00:17:28,910 --> 00:17:32,510
that that will be useful for the device

00:17:31,040 --> 00:17:34,780
and that this program will not be

00:17:32,510 --> 00:17:38,120
executed on the host effectively and

00:17:34,780 --> 00:17:40,700
this also allows us to inform the PPF

00:17:38,120 --> 00:17:43,250
core to place some of the offload state

00:17:40,700 --> 00:17:45,620
inside the BPF program structure which

00:17:43,250 --> 00:17:47,809
makes it easier to later report state

00:17:45,620 --> 00:17:49,249
makes it more natural to

00:17:47,809 --> 00:17:51,440
towards state about the author of the

00:17:49,249 --> 00:17:55,309
program without having to go through the

00:17:51,440 --> 00:17:56,419
native infrastructure the BPF core

00:17:55,309 --> 00:17:59,950
basically already has all the

00:17:56,419 --> 00:18:03,279
information it may want to report so

00:17:59,950 --> 00:18:06,049
right to go again for this for the

00:18:03,279 --> 00:18:09,769
challenges and why we changed the the

00:18:06,049 --> 00:18:11,450
model in more detail the modifications

00:18:09,769 --> 00:18:13,029
the first item is the modifications that

00:18:11,450 --> 00:18:16,669
are performed by the verifier

00:18:13,029 --> 00:18:18,740
it changes the the offsets of the field

00:18:16,669 --> 00:18:20,480
offsets from the one that ones that are

00:18:18,740 --> 00:18:24,470
exposed to user space to the actual

00:18:20,480 --> 00:18:26,360
kernel structure offset and that's

00:18:24,470 --> 00:18:28,999
slightly easy to reverse although not

00:18:26,360 --> 00:18:31,340
entirely clear a clean as far as the

00:18:28,999 --> 00:18:33,320
implementation goes the second problem

00:18:31,340 --> 00:18:35,749
is the functions so when the program is

00:18:33,320 --> 00:18:38,779
loaded from user space the functions are

00:18:35,749 --> 00:18:41,029
referred to by IDs and after the first

00:18:38,779 --> 00:18:43,129
verification the the the verifier will

00:18:41,029 --> 00:18:47,480
patch them and change the instructions

00:18:43,129 --> 00:18:49,940
to contain basically pointers and the

00:18:47,480 --> 00:18:53,059
third program is with the maps so when

00:18:49,940 --> 00:18:55,580
the program is loaded map load the map

00:18:53,059 --> 00:18:58,759
pointer load instructions are specially

00:18:55,580 --> 00:19:00,980
marked and that those markings are

00:18:58,759 --> 00:19:04,490
removed by the verifier so if we want to

00:19:00,980 --> 00:19:08,299
offload the device the program to a

00:19:04,490 --> 00:19:10,820
device we need to basically find all the

00:19:08,299 --> 00:19:12,919
map references and it's kind of

00:19:10,820 --> 00:19:16,220
impossible to define the pointer pointer

00:19:12,919 --> 00:19:20,330
loads when they Rev the markings already

00:19:16,220 --> 00:19:22,279
removed another thing that happens after

00:19:20,330 --> 00:19:25,190
the first verification is there may be a

00:19:22,279 --> 00:19:28,009
prologue or epilogue generated for the

00:19:25,190 --> 00:19:31,720
program which your general populates an

00:19:28,009 --> 00:19:34,399
information or does some extra checking

00:19:31,720 --> 00:19:35,899
which basically again generates some

00:19:34,399 --> 00:19:38,629
code which the verifier will not be able

00:19:35,899 --> 00:19:40,909
to parse the second time around so that

00:19:38,629 --> 00:19:43,490
is basically a showstopper so some

00:19:40,909 --> 00:19:46,039
features like direct pocket access for

00:19:43,490 --> 00:19:48,879
right would not be possible to offload

00:19:46,039 --> 00:19:52,129
in the previous previous architecture

00:19:48,879 --> 00:19:53,840
and there are optimizations that the

00:19:52,129 --> 00:19:55,210
verifier does and I think today there's

00:19:53,840 --> 00:19:59,000
I think only one

00:19:55,210 --> 00:20:00,769
alexia the inlining of lookup map with a

00:19:59,000 --> 00:20:04,049
lookup course

00:20:00,769 --> 00:20:05,399
and again the when the program's loaded

00:20:04,049 --> 00:20:08,759
from user space the map lookup course

00:20:05,399 --> 00:20:11,729
just a call and I think I did ID one but

00:20:08,759 --> 00:20:14,669
the verifier will in line some of the

00:20:11,729 --> 00:20:17,340
code basically the invocation of the map

00:20:14,669 --> 00:20:19,679
lookup is sort of an indirect call the

00:20:17,340 --> 00:20:20,879
first helper just does basic checking

00:20:19,679 --> 00:20:23,789
and then calls the actual implementation

00:20:20,879 --> 00:20:25,619
for the given map type so that the basic

00:20:23,789 --> 00:20:28,769
checks can be inlined and then we save

00:20:25,619 --> 00:20:31,679
one one the reference basically one in

00:20:28,769 --> 00:20:33,599
one color it in direction and that's

00:20:31,679 --> 00:20:36,179
great for host performance but for the

00:20:33,599 --> 00:20:37,950
device it's neither useful nor nor is it

00:20:36,179 --> 00:20:43,619
possible to get that for the verifier

00:20:37,950 --> 00:20:46,379
for the second time and the the other

00:20:43,619 --> 00:20:48,929
problem is when we load the program onto

00:20:46,379 --> 00:20:51,239
the device there is really no flexible

00:20:48,929 --> 00:20:53,639
way to report errors and the verifier

00:20:51,239 --> 00:20:55,830
log as much as some people may say it's

00:20:53,639 --> 00:20:58,940
deficient it's actually quite useful and

00:20:55,830 --> 00:21:01,559
flexible to just output errors or

00:20:58,940 --> 00:21:04,220
information basically as we translate as

00:21:01,559 --> 00:21:07,139
we verify the program and this could be

00:21:04,220 --> 00:21:08,820
just errors like just telling the users

00:21:07,139 --> 00:21:11,429
why the program couldn't be loaded or

00:21:08,820 --> 00:21:13,049
offloaded or just state about

00:21:11,429 --> 00:21:15,979
optimizations which we're done or not

00:21:13,049 --> 00:21:19,109
done for some reason so the verifier log

00:21:15,979 --> 00:21:20,789
is a really useful tool but obviously if

00:21:19,109 --> 00:21:22,019
we run the the verification for the

00:21:20,789 --> 00:21:24,149
second time inside the networking

00:21:22,019 --> 00:21:26,519
subsystem there is no way for us to

00:21:24,149 --> 00:21:29,070
report it back to user space so moving

00:21:26,519 --> 00:21:30,960
the the translation for the device to

00:21:29,070 --> 00:21:33,019
the load stage is really useful in that

00:21:30,960 --> 00:21:33,019
regard

00:21:34,609 --> 00:21:41,129
as I said also the the BBFS we can now

00:21:38,669 --> 00:21:43,859
place some of the state inside the BPF

00:21:41,129 --> 00:21:47,070
program structure itself so we are

00:21:43,859 --> 00:21:49,859
hoping to report the the translated

00:21:47,070 --> 00:21:51,989
image so basically we when we translate

00:21:49,859 --> 00:21:53,669
the the program for the device we are

00:21:51,989 --> 00:21:55,679
actually generating machine code it's

00:21:53,669 --> 00:21:56,820
not like some abstract instructions of

00:21:55,679 --> 00:21:57,389
the firm will the in translate to

00:21:56,820 --> 00:22:00,119
something else

00:21:57,389 --> 00:22:02,580
it's it's the actual NFP machine code

00:22:00,119 --> 00:22:05,729
and we can take that and disassemble

00:22:02,580 --> 00:22:08,129
that with with some libraries so we are

00:22:05,729 --> 00:22:10,440
hoping to be able to dump those images

00:22:08,129 --> 00:22:13,140
those translated images back to user

00:22:10,440 --> 00:22:16,800
space and perhaps extend VP f2 to be

00:22:13,140 --> 00:22:19,080
able to disassemble NFB code so users

00:22:16,800 --> 00:22:20,910
can see what the EPF programs were

00:22:19,080 --> 00:22:23,580
translated on the hardware to it the

00:22:20,910 --> 00:22:27,060
same way they would for be able to for

00:22:23,580 --> 00:22:29,760
x86 and that should work pretty

00:22:27,060 --> 00:22:32,730
naturally in the new model the only

00:22:29,760 --> 00:22:34,740
thing we need to add is the Machine type

00:22:32,730 --> 00:22:37,920
effectively because if we are talking

00:22:34,740 --> 00:22:40,530
about digit image for the host it's

00:22:37,920 --> 00:22:42,510
obvious that which disassembler to use

00:22:40,530 --> 00:22:45,030
basically the the host disassembler is

00:22:42,510 --> 00:22:47,130
the correct one and we can easily find

00:22:45,030 --> 00:22:49,650
out what the host we are running on is

00:22:47,130 --> 00:22:51,960
but if it's an offload device and it's

00:22:49,650 --> 00:22:54,540
an NFP or an arm core or whatever else

00:22:51,960 --> 00:22:57,150
we have to include in the user space in

00:22:54,540 --> 00:22:59,550
the program information structure which

00:22:57,150 --> 00:23:04,830
which machine type the image was

00:22:59,550 --> 00:23:06,450
generated for and another extensions

00:23:04,830 --> 00:23:08,190
which we will hopefully add soon is

00:23:06,450 --> 00:23:09,720
there are a lot of places where the

00:23:08,190 --> 00:23:12,660
extend attacks were editing the stack

00:23:09,720 --> 00:23:14,700
already but the TCF loads have been left

00:23:12,660 --> 00:23:17,820
out for now and we will hope to extend

00:23:14,700 --> 00:23:20,460
that hopefully soon we have already deck

00:23:17,820 --> 00:23:24,090
stack in xdp call back if in the XD pn

00:23:20,460 --> 00:23:26,370
do but used having it in the setup tc1

00:23:24,090 --> 00:23:30,090
it would be extremely useful as well we

00:23:26,370 --> 00:23:32,040
can actually we know ourselves that this

00:23:30,090 --> 00:23:34,110
is a very powerful tool and the number

00:23:32,040 --> 00:23:36,300
of questions i've got asked about why my

00:23:34,110 --> 00:23:38,070
xdv program does not load fell

00:23:36,300 --> 00:23:40,740
dramatically after I implemented this so

00:23:38,070 --> 00:23:44,430
I really like this and will implement

00:23:40,740 --> 00:23:46,650
this for TC as well soon and another

00:23:44,430 --> 00:23:49,080
thing that that is still missing for

00:23:46,650 --> 00:23:51,330
other people for basically production

00:23:49,080 --> 00:23:53,310
readiness is being able to report

00:23:51,330 --> 00:23:55,920
information when the program runs on the

00:23:53,310 --> 00:23:58,880
hardware so I think everybody who uses

00:23:55,920 --> 00:24:02,730
BPF programs today uses some form of

00:23:58,880 --> 00:24:05,100
print k or the perf invent output to

00:24:02,730 --> 00:24:07,170
basically trace what happens to the

00:24:05,100 --> 00:24:10,170
packet that goes to the BPF program and

00:24:07,170 --> 00:24:12,120
we have the implementation of perfect

00:24:10,170 --> 00:24:14,100
output in the farmer but we need to

00:24:12,120 --> 00:24:19,470
bring bring it out basically supports it

00:24:14,100 --> 00:24:21,690
in the indian driver as well and yes as

00:24:19,470 --> 00:24:24,190
I said the optimizations that we do in

00:24:21,690 --> 00:24:25,929
them in the driver are becoming

00:24:24,190 --> 00:24:28,210
more and more complicated and more and

00:24:25,929 --> 00:24:32,440
more complex and obviously of complexity

00:24:28,210 --> 00:24:36,490
there might some be some bugs which lurk

00:24:32,440 --> 00:24:38,500
in and also if we start optimizing the

00:24:36,490 --> 00:24:40,179
programs in non-trivial ways we may

00:24:38,500 --> 00:24:43,120
change how the program works we don't

00:24:40,179 --> 00:24:45,370
intend to break the BPF API but

00:24:43,120 --> 00:24:47,049
obviously as we do optimizations at most

00:24:45,370 --> 00:24:49,539
things may start working slightly

00:24:47,049 --> 00:24:52,029
differently on the host and done on the

00:24:49,539 --> 00:24:54,159
host and until we change or fix the

00:24:52,029 --> 00:24:56,950
optimization it would be great if we had

00:24:54,159 --> 00:24:58,450
a way of like disabling particular

00:24:56,950 --> 00:25:02,169
optimizations that we have in the driver

00:24:58,450 --> 00:25:04,179
and that would actually be useful for

00:25:02,169 --> 00:25:06,669
the host as well right now the the

00:25:04,179 --> 00:25:08,340
inlining optimizations is skied on

00:25:06,669 --> 00:25:10,600
basically jetting the program and

00:25:08,340 --> 00:25:12,700
perhaps there will be some use in being

00:25:10,600 --> 00:25:15,159
able to control that as well for a

00:25:12,700 --> 00:25:21,159
separate API instance we just using the

00:25:15,159 --> 00:25:22,870
JIT flag and last but not least I showed

00:25:21,159 --> 00:25:25,059
the way that we changed the architecture

00:25:22,870 --> 00:25:27,070
for programs we don't have map offload

00:25:25,059 --> 00:25:29,879
but we are actually thinking about doing

00:25:27,070 --> 00:25:31,870
a similar thing for maps so instead of

00:25:29,879 --> 00:25:34,600
offloading the maps when they get

00:25:31,870 --> 00:25:37,769
attached to a device create the maps

00:25:34,600 --> 00:25:40,649
from the start on the device itself and

00:25:37,769 --> 00:25:43,000
the reasoning for that is effectively

00:25:40,649 --> 00:25:45,070
exactly the same as for the program

00:25:43,000 --> 00:25:47,649
where there is no verifier to go through

00:25:45,070 --> 00:25:49,929
but I were reporting gets a lot more

00:25:47,649 --> 00:25:51,730
easier when when the error can be

00:25:49,929 --> 00:25:53,500
reported when the map is created instead

00:25:51,730 --> 00:25:58,090
of when the program is attached to the

00:25:53,500 --> 00:26:00,070
device and the complexity there is some

00:25:58,090 --> 00:26:02,379
complexity in the process that it takes

00:26:00,070 --> 00:26:04,330
for to take them up that's exists in the

00:26:02,379 --> 00:26:06,340
kernel and can be accessed wizard from

00:26:04,330 --> 00:26:09,940
user space already and put it on the

00:26:06,340 --> 00:26:13,120
device we need to basically stole all

00:26:09,940 --> 00:26:15,519
the map operations and also the the map

00:26:13,120 --> 00:26:17,889
fully and then allow the operations

00:26:15,519 --> 00:26:20,350
again it's not very hard but it's

00:26:17,889 --> 00:26:22,480
perhaps unnecessary complexity if most

00:26:20,350 --> 00:26:26,139
people will just want to offload the map

00:26:22,480 --> 00:26:28,630
from the start and there's also some

00:26:26,139 --> 00:26:31,480
trickiness around the

00:26:28,630 --> 00:26:33,280
Gregg around claiming the maps because

00:26:31,480 --> 00:26:36,340
we want to support the use case where

00:26:33,280 --> 00:26:38,020
there's two programs effectively you use

00:26:36,340 --> 00:26:39,460
the same map because for instance we

00:26:38,020 --> 00:26:42,160
want to offload both of them onto the

00:26:39,460 --> 00:26:45,280
same device or replace one by the other

00:26:42,160 --> 00:26:47,170
but we can't really allow programs that

00:26:45,280 --> 00:26:49,060
are and won't be offloaded to claim the

00:26:47,170 --> 00:26:52,330
same maps as those which were offloaded

00:26:49,060 --> 00:26:54,820
so there is some complexity and the maps

00:26:52,330 --> 00:26:57,070
also don't contain back pointers to the

00:26:54,820 --> 00:26:59,860
programs to which they're bound so it

00:26:57,070 --> 00:27:02,430
will simplify things if we just create

00:26:59,860 --> 00:27:05,110
the maps on the device from the start

00:27:02,430 --> 00:27:12,750
and that's basically it

00:27:05,110 --> 00:27:12,750

YouTube URL: https://www.youtube.com/watch?v=mDSMbyb_xlg


