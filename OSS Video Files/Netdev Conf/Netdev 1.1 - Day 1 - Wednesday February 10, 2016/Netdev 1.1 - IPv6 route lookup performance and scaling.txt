Title: Netdev 1.1 - IPv6 route lookup performance and scaling
Publication date: 2016-03-10
Playlist: Netdev 1.1 - Day 1 - Wednesday February 10, 2016
Description: 
	Michal Kubeček
February 2016
Captions: 
	00:00:00,170 --> 00:00:06,299
so one dog full of boring numbers right

00:00:04,680 --> 00:00:09,150
after the launch what could possibly go

00:00:06,299 --> 00:00:11,670
wrong so my name is Michael kubacheck

00:00:09,150 --> 00:00:14,880
I'm from Susa labs and I would like to

00:00:11,670 --> 00:00:19,080
talk about the ipv6 routing look up

00:00:14,880 --> 00:00:23,340
performance and scaling of the routine

00:00:19,080 --> 00:00:27,779
cup so let's start with some background

00:00:23,340 --> 00:00:31,769
story there is a principle that some of

00:00:27,779 --> 00:00:34,829
our customers expect which I call ipv6

00:00:31,769 --> 00:00:39,559
parity the principle is if it works with

00:00:34,829 --> 00:00:46,770
ipv4 it should work also with ipv6 oh

00:00:39,559 --> 00:00:48,989
it's better than it used to be it mostly

00:00:46,770 --> 00:00:52,170
holds these times but there are still

00:00:48,989 --> 00:00:54,930
some exceptions and in particular when

00:00:52,170 --> 00:00:57,690
it comes to performance we cannot always

00:00:54,930 --> 00:01:02,309
in general expect ipv6 to perform

00:00:57,690 --> 00:01:05,820
exactly as well as ipv4 does but it

00:01:02,309 --> 00:01:07,439
shouldn't perform much worse so the

00:01:05,820 --> 00:01:13,080
question is does it perform as well as

00:01:07,439 --> 00:01:14,790
it could if the answer was yes there

00:01:13,080 --> 00:01:18,420
would be little point in having this

00:01:14,790 --> 00:01:21,390
talk so i guess you can expect assume

00:01:18,420 --> 00:01:24,210
the answer is no but at least in some

00:01:21,390 --> 00:01:27,210
cases so let's start with some

00:01:24,210 --> 00:01:31,110
background story and experience of one

00:01:27,210 --> 00:01:36,390
of our customers who filed a bug in

00:01:31,110 --> 00:01:39,270
november two thousand twelve the name as

00:01:36,390 --> 00:01:41,850
usually is a bit misleading well the

00:01:39,270 --> 00:01:45,259
problem of theirs was they had host with

00:01:41,850 --> 00:01:47,640
money with intensive ipv6 traffic

00:01:45,259 --> 00:01:50,610
routing to about two hundred thousand

00:01:47,640 --> 00:01:55,500
different hosts and they experienced

00:01:50,610 --> 00:01:58,070
heavy contention on ipv6 fib garbage

00:01:55,500 --> 00:01:58,070
collector lock

00:01:59,310 --> 00:02:05,100
the contention was having enough to

00:02:02,130 --> 00:02:09,450
trigger the software cup detector it was

00:02:05,100 --> 00:02:11,610
really affecting their function and a

00:02:09,450 --> 00:02:14,099
similar amount of traffic or similar

00:02:11,610 --> 00:02:20,010
traffic pattern with IP be who are just

00:02:14,099 --> 00:02:22,410
fine on the same box so we suggested

00:02:20,010 --> 00:02:28,140
them to raise the garbage collector

00:02:22,410 --> 00:02:32,489
threshold and routing table limits which

00:02:28,140 --> 00:02:38,280
worked they were happy until few months

00:02:32,489 --> 00:02:42,030
later they opened a new bug this time

00:02:38,280 --> 00:02:44,010
the summary is even more receding it was

00:02:42,030 --> 00:02:47,099
the same host and the same project as

00:02:44,010 --> 00:02:52,260
before and they experienced bond

00:02:47,099 --> 00:02:54,599
flapping and it was caused by dropped

00:02:52,260 --> 00:02:58,080
packets because they were using PRP

00:02:54,599 --> 00:03:01,769
monitor for some reasons I'd want to go

00:02:58,080 --> 00:03:03,299
into and analysis showed that the

00:03:01,769 --> 00:03:05,880
network card was dropping packets

00:03:03,299 --> 00:03:08,459
because the network is tag was too slow

00:03:05,880 --> 00:03:11,480
to process them and some of those

00:03:08,459 --> 00:03:15,359
dropped packets were the arp probes or

00:03:11,480 --> 00:03:18,780
replies to the arp probes which caused

00:03:15,359 --> 00:03:23,780
the bonding driver to believe the

00:03:18,780 --> 00:03:27,690
interfaces down well the reason was that

00:03:23,780 --> 00:03:30,980
when the we suggested them to raise the

00:03:27,690 --> 00:03:35,100
garbage collector short they ended up

00:03:30,980 --> 00:03:39,900
raising it to march to the values where

00:03:35,100 --> 00:03:43,230
the routing subsystem wasn't able to

00:03:39,900 --> 00:03:46,200
process the packets fast enough so the

00:03:43,230 --> 00:03:51,769
further analysis showed that whenever

00:03:46,200 --> 00:03:55,950
the limits are all the garbage collector

00:03:51,769 --> 00:03:58,739
contention occurs as before whenever the

00:03:55,950 --> 00:04:02,489
limits are set hi the packets are

00:03:58,739 --> 00:04:04,049
dropped and bond is flopping and we

00:04:02,489 --> 00:04:07,230
found out that there was actually no

00:04:04,049 --> 00:04:09,590
value where neither of these two would

00:04:07,230 --> 00:04:09,590
occur

00:04:09,990 --> 00:04:15,850
this time we were a bit lucky because we

00:04:12,820 --> 00:04:20,050
found that the algorithm triggering the

00:04:15,850 --> 00:04:25,030
garbage collector for fib 6 had some

00:04:20,050 --> 00:04:28,210
deficiencies causing an effect where if

00:04:25,030 --> 00:04:32,350
one cpu was doing the garbage collecting

00:04:28,210 --> 00:04:36,490
any other cpu would decide to do the

00:04:32,350 --> 00:04:38,800
same would wait for the lock only to run

00:04:36,490 --> 00:04:43,390
the car bill collector again once the

00:04:38,800 --> 00:04:45,060
first cpu started finished sorry so this

00:04:43,390 --> 00:04:48,060
is this was something that was

00:04:45,060 --> 00:04:51,730
unnecessary and an easy fix was to

00:04:48,060 --> 00:04:56,490
prevent rerunning the garbage collector

00:04:51,730 --> 00:05:02,620
again and again for each cpu in a row so

00:04:56,490 --> 00:05:05,950
the solution was to only use trilok for

00:05:02,620 --> 00:05:07,990
the garbage collector lock so that if we

00:05:05,950 --> 00:05:10,390
found that some money somebody is

00:05:07,990 --> 00:05:13,510
already garbage collecting we wouldn't

00:05:10,390 --> 00:05:17,170
try to do the same again the customer

00:05:13,510 --> 00:05:23,770
was satisfied the pitch was accepted and

00:05:17,170 --> 00:05:26,110
the bag was closed well however we

00:05:23,770 --> 00:05:28,060
didn't actually solve the real problem

00:05:26,110 --> 00:05:31,540
the background problem which was that

00:05:28,060 --> 00:05:35,440
the ipv6 Luca performance was less than

00:05:31,540 --> 00:05:39,700
satisfactory and in particular that it

00:05:35,440 --> 00:05:42,640
doesn't scale to the number of

00:05:39,700 --> 00:05:48,430
simultaneous threats as well as it does

00:05:42,640 --> 00:05:50,440
in ipv4 case moreover there was another

00:05:48,430 --> 00:05:53,200
customer who claimed to encounter

00:05:50,440 --> 00:05:56,020
similar problems on the kernel which

00:05:53,200 --> 00:05:58,240
contain this fix but it son confirmed

00:05:56,020 --> 00:06:02,200
because they didn't tell us actually i

00:05:58,240 --> 00:06:05,670
only found that year or two later in

00:06:02,200 --> 00:06:05,670
some disguise mailing list

00:06:06,760 --> 00:06:14,090
okay small intimate own experience from

00:06:12,139 --> 00:06:17,720
the same customer well this is a

00:06:14,090 --> 00:06:20,930
customer who works as kind of QA for

00:06:17,720 --> 00:06:24,889
dark corners of ipv6 because they are

00:06:20,930 --> 00:06:26,960
constantly hitting strange bugs they

00:06:24,889 --> 00:06:31,010
edit Alex see containers to the picture

00:06:26,960 --> 00:06:34,310
which means namespaces and again there

00:06:31,010 --> 00:06:38,210
were hitting the contention of garbage

00:06:34,310 --> 00:06:39,949
in garbage collector rock and analysis

00:06:38,210 --> 00:06:42,229
showed that the problem this time is

00:06:39,949 --> 00:06:46,610
that while we have pear namespace

00:06:42,229 --> 00:06:51,080
routing tables and fib trees and / name

00:06:46,610 --> 00:06:52,880
a space garbage collector that this

00:06:51,080 --> 00:06:57,139
garbage collector these garbage

00:06:52,880 --> 00:07:00,620
collectors actually are protected by one

00:06:57,139 --> 00:07:04,460
shard walk and also the handling of

00:07:00,620 --> 00:07:06,680
so-called walkers is sharp and their

00:07:04,460 --> 00:07:09,560
protection is shirt which caused these

00:07:06,680 --> 00:07:11,510
contentions this is a really fresh

00:07:09,560 --> 00:07:15,520
because I only learned about this last

00:07:11,510 --> 00:07:19,160
Friday there are currently testing

00:07:15,520 --> 00:07:21,590
provisional fix and if it works for them

00:07:19,160 --> 00:07:25,760
it's going to be submitted perhaps next

00:07:21,590 --> 00:07:27,770
week so it's still ongoing process but

00:07:25,760 --> 00:07:31,160
the background is there is still a

00:07:27,770 --> 00:07:34,270
problem with the routing lookup

00:07:31,160 --> 00:07:38,479
performance so what I decided was to

00:07:34,270 --> 00:07:42,289
find out if there is a problem and how

00:07:38,479 --> 00:07:44,630
big the problem is so for this for this

00:07:42,289 --> 00:07:47,860
purpose I decided to do a micro

00:07:44,630 --> 00:07:53,030
benchmarking of the ipv6 routing look up

00:07:47,860 --> 00:07:55,910
the technique is similar to what David

00:07:53,030 --> 00:07:59,599
Miller was using when working on the

00:07:55,910 --> 00:08:02,389
routine cash remover for ipv4 so the

00:07:59,599 --> 00:08:04,070
idea is if we cannot do this we cannot

00:08:02,389 --> 00:08:06,110
micro benchmark this from user space

00:08:04,070 --> 00:08:08,090
because we would rather benchmark the

00:08:06,110 --> 00:08:12,630
net link not the routine

00:08:08,090 --> 00:08:15,479
so we do this from a carnival you the

00:08:12,630 --> 00:08:18,180
kernel module is running the benchmark

00:08:15,479 --> 00:08:23,190
is calling directly DeLuca functions

00:08:18,180 --> 00:08:28,440
when all did and because I was

00:08:23,190 --> 00:08:30,120
interested in performance for many

00:08:28,440 --> 00:08:34,229
simultaneous threats or more

00:08:30,120 --> 00:08:39,899
simultaneous threats this is run in

00:08:34,229 --> 00:08:43,250
several corner threats and each kind of

00:08:39,899 --> 00:08:47,850
threat is running given number of cups

00:08:43,250 --> 00:08:50,910
all this is remitted repeated and can be

00:08:47,850 --> 00:08:54,390
done for ipv4 or ipv6 and once finished

00:08:50,910 --> 00:08:58,020
we can read the results process them

00:08:54,390 --> 00:09:03,060
partner some technicalities this was

00:08:58,020 --> 00:09:07,350
almost performed on one reasonable good

00:09:03,060 --> 00:09:11,070
machine with 24 logical CPUs without

00:09:07,350 --> 00:09:13,620
reporting original tests were performed

00:09:11,070 --> 00:09:16,830
Annesley 12 kernel which means 312 but

00:09:13,620 --> 00:09:23,700
what I'm going to present today our

00:09:16,830 --> 00:09:26,130
results with relatively fresh for 410 ya

00:09:23,700 --> 00:09:29,270
authors were performing performed in

00:09:26,130 --> 00:09:33,690
single user system to minimalize other

00:09:29,270 --> 00:09:36,330
effects from interfering and the tests

00:09:33,690 --> 00:09:39,180
were performed with her routing table

00:09:36,330 --> 00:09:42,950
pre filled with given number of routes

00:09:39,180 --> 00:09:42,950
from one 200,000

00:09:46,840 --> 00:09:52,580
okay I also subtracted the results for a

00:09:50,450 --> 00:09:56,270
dry run when no Luca was actually

00:09:52,580 --> 00:10:00,080
performed to mitigate the influence of

00:09:56,270 --> 00:10:04,550
the overhead of the test itself so let's

00:10:00,080 --> 00:10:06,740
go to numbers so these are all numbers

00:10:04,550 --> 00:10:12,110
in all tables from now on will be in

00:10:06,740 --> 00:10:15,560
nanoseconds para wanna hook up well so

00:10:12,110 --> 00:10:20,450
let's look these numbers are for a

00:10:15,560 --> 00:10:24,380
single thread test with numbers of roots

00:10:20,450 --> 00:10:29,480
in the table from 1 to 100 thousand the

00:10:24,380 --> 00:10:31,730
ipv4 numbers loop relatively good as

00:10:29,480 --> 00:10:37,660
expected with growing size of the

00:10:31,730 --> 00:10:42,890
routing table the tests take longer but

00:10:37,660 --> 00:10:47,210
the scaling is relatively fine the ipv6

00:10:42,890 --> 00:10:49,640
numbers are much worse it's expected to

00:10:47,210 --> 00:10:53,390
be worse but this is a bit too much it's

00:10:49,640 --> 00:10:57,800
something like 50 times higher in the

00:10:53,390 --> 00:10:59,870
easiest case well the relative growth

00:10:57,800 --> 00:11:03,440
with the number of with the size of the

00:10:59,870 --> 00:11:07,490
table is not as high as unbelief ipv4

00:11:03,440 --> 00:11:12,520
but if you take the absolute growth it's

00:11:07,490 --> 00:11:12,520
actually about the same

00:11:13,740 --> 00:11:21,870
yeah so both ipv4 and ipv6 scale

00:11:18,920 --> 00:11:24,720
reasonably well with respect to the size

00:11:21,870 --> 00:11:26,760
of the routing table what about scaling

00:11:24,720 --> 00:11:33,390
to the number of threats simultaneous

00:11:26,760 --> 00:11:38,970
deaths Wow so if we take a look at abb

00:11:33,390 --> 00:11:41,820
for we can see that it scales rather

00:11:38,970 --> 00:11:45,720
well well I also calculated the mean

00:11:41,820 --> 00:11:49,410
deviations but putting them in the same

00:11:45,720 --> 00:11:51,000
table would be too confusing so you will

00:11:49,410 --> 00:11:53,640
have to wait for the proceedings for

00:11:51,000 --> 00:11:56,670
those but that is nothing nothing

00:11:53,640 --> 00:12:00,570
exceptional we can see that for 24

00:11:56,670 --> 00:12:04,790
threads simultaneously Luca is a bit

00:12:00,570 --> 00:12:09,210
slower about really not much vana

00:12:04,790 --> 00:12:14,580
49-percent and this doesn't actually

00:12:09,210 --> 00:12:17,910
change for big routing table size with

00:12:14,580 --> 00:12:21,330
ipv6 well the results are much less

00:12:17,910 --> 00:12:23,580
impressive we can see that the lookup

00:12:21,330 --> 00:12:27,000
time the average hookup time grows

00:12:23,580 --> 00:12:30,660
rather rapidly and to see how rapidly

00:12:27,000 --> 00:12:34,410
does it grow let's take a bit different

00:12:30,660 --> 00:12:38,730
view these numbers are average duration

00:12:34,410 --> 00:12:41,580
of a single look up so we take the total

00:12:38,730 --> 00:12:45,920
cpu time divided by the number of

00:12:41,580 --> 00:12:49,050
lookups across all threats in this table

00:12:45,920 --> 00:12:51,720
I'm doing the different thing I think

00:12:49,050 --> 00:12:55,790
the lips time for the road test divided

00:12:51,720 --> 00:12:55,790
by the total number number of fruit cups

00:12:56,690 --> 00:13:03,120
reciprocal value of what is here would

00:12:59,400 --> 00:13:06,780
be how many hookups can we do per unit

00:13:03,120 --> 00:13:08,930
of time but I didn't like to switch

00:13:06,780 --> 00:13:12,060
between tables where more is better and

00:13:08,930 --> 00:13:17,260
last is better so it's still in

00:13:12,060 --> 00:13:22,000
nanoseconds so as we can see

00:13:17,260 --> 00:13:24,160
in ideal world the number should be

00:13:22,000 --> 00:13:26,620
inverse proportional to the number of

00:13:24,160 --> 00:13:29,040
the threats if the root cap scaled

00:13:26,620 --> 00:13:35,470
perfectly we can see that it actually

00:13:29,040 --> 00:13:39,460
doesn't decrease this well not by far

00:13:35,470 --> 00:13:41,170
and in some of the tests not shown here

00:13:39,460 --> 00:13:43,180
but in some of the tests I could

00:13:41,170 --> 00:13:46,360
actually see that for higher number of

00:13:43,180 --> 00:13:48,010
threads like 24 the reason the final

00:13:46,360 --> 00:13:50,260
results in the last one would be

00:13:48,010 --> 00:13:53,710
actually higher than the last our school

00:13:50,260 --> 00:13:57,070
so the result would be even worse than

00:13:53,710 --> 00:14:00,370
if the lookups versi realized which is

00:13:57,070 --> 00:14:05,560
really really bad so we can see that the

00:14:00,370 --> 00:14:15,180
look up for ipv6 scales that are poorly

00:14:05,560 --> 00:14:19,460
could not higher number of pets so why

00:14:15,180 --> 00:14:22,320
the problem is that ipv6 for fib is

00:14:19,460 --> 00:14:26,870
implemented it in a different way for

00:14:22,320 --> 00:14:31,020
historical reasons than in ipv4 ah sorry

00:14:26,870 --> 00:14:34,320
you still microphone sorry so which ipv6

00:14:31,020 --> 00:14:38,310
we have an order three and one problem

00:14:34,320 --> 00:14:40,800
is / Tyler it right spin lock so

00:14:38,310 --> 00:14:43,830
essentially whenever we are writing to

00:14:40,800 --> 00:14:47,640
the routing table nobody can perform

00:14:43,830 --> 00:14:49,980
even look up there is some

00:14:47,640 --> 00:14:52,410
sophistication the sophistication OG for

00:14:49,980 --> 00:14:56,090
so-called workers which helps to

00:14:52,410 --> 00:15:00,120
minimize the duration of these workouts

00:14:56,090 --> 00:15:04,770
but the problem still is there and it's

00:15:00,120 --> 00:15:08,880
made worse by putting the cash and

00:15:04,770 --> 00:15:12,420
threes to the same data structure this

00:15:08,880 --> 00:15:16,800
defect was minimized since four to

00:15:12,420 --> 00:15:19,380
Colonel by the work of martinka Philo so

00:15:16,800 --> 00:15:21,510
that now we are cashing only the entries

00:15:19,380 --> 00:15:24,240
that we actually need to cash by for

00:15:21,510 --> 00:15:26,760
example in response to pnt you this

00:15:24,240 --> 00:15:29,790
ethic was much worse in pre for

00:15:26,760 --> 00:15:32,460
companies and because we are putting

00:15:29,790 --> 00:15:35,490
these countries to the same structure we

00:15:32,460 --> 00:15:37,770
need the garbage collector and whenever

00:15:35,490 --> 00:15:42,120
the garbage collector starts the

00:15:37,770 --> 00:15:46,230
performance goes away but on the other

00:15:42,120 --> 00:15:50,190
hand with before we hope we have so with

00:15:46,230 --> 00:15:54,900
ipv4 we have an RPC tri-level and path

00:15:50,190 --> 00:15:57,710
compressed traffic stream the rocking is

00:15:54,900 --> 00:16:00,270
based on our see you so we're eating is

00:15:57,710 --> 00:16:04,140
essentially low class on non-preemptive

00:16:00,270 --> 00:16:07,200
kernels we don't have no cash ten threes

00:16:04,140 --> 00:16:09,870
in the fib try those are handled

00:16:07,200 --> 00:16:12,780
separately in exception they below and

00:16:09,870 --> 00:16:18,520
so we don't need a garbage

00:16:12,780 --> 00:16:22,960
so as a result for this kind of test I

00:16:18,520 --> 00:16:25,720
be before performs much better well to

00:16:22,960 --> 00:16:28,570
be honest I was a bit cheeky cheating

00:16:25,720 --> 00:16:35,800
here and the tests were bit unfair to

00:16:28,570 --> 00:16:40,420
ipv6 because ipv4 test test call

00:16:35,800 --> 00:16:44,890
function f IPO cup while ipv6 test use

00:16:40,420 --> 00:16:46,780
IP 6 output which it does much more the

00:16:44,890 --> 00:16:50,200
problem is that due to design these

00:16:46,780 --> 00:16:53,850
variances it was hard to find a function

00:16:50,200 --> 00:16:58,270
which was correspond to fib hookup so

00:16:53,850 --> 00:17:03,070
let's do another round of results this

00:16:58,270 --> 00:17:06,730
time using a function I IP route output

00:17:03,070 --> 00:17:09,580
key for ib before which better

00:17:06,730 --> 00:17:14,910
corresponds to what we were using for

00:17:09,580 --> 00:17:22,390
ipv6 so it's the same results as before

00:17:14,910 --> 00:17:26,740
again in nanoseconds so now we can see

00:17:22,390 --> 00:17:34,420
that with a one root in the table IP IP

00:17:26,740 --> 00:17:39,370
v4 doesn't scale as well actuate it's

00:17:34,420 --> 00:17:43,090
girls similar similarly more as it does

00:17:39,370 --> 00:17:47,770
for ipv6 but what is interesting that it

00:17:43,090 --> 00:17:52,570
is that with a bigger table like 10,000

00:17:47,770 --> 00:17:57,250
or hundred thousand roots the numbers

00:17:52,570 --> 00:18:01,570
are actually similar sorry the scaling

00:17:57,250 --> 00:18:04,510
is almost perfect as it was before so

00:18:01,570 --> 00:18:10,960
this additional overhead we introduced

00:18:04,510 --> 00:18:13,270
in this test scales poorly for small

00:18:10,960 --> 00:18:16,750
routing tables but once the routing

00:18:13,270 --> 00:18:18,500
table is large enough which means we

00:18:16,750 --> 00:18:20,930
have less collisions

00:18:18,500 --> 00:18:25,820
for example when accessing the reference

00:18:20,930 --> 00:18:29,420
counts this effect is essentially like

00:18:25,820 --> 00:18:36,700
readable as we can see in the lower line

00:18:29,420 --> 00:18:39,200
of the table our SAV has seen with ipv6

00:18:36,700 --> 00:18:45,410
even having a hundred thousand roots

00:18:39,200 --> 00:18:48,040
doesn't actually help to make the

00:18:45,410 --> 00:18:48,040
scaling better

00:18:58,600 --> 00:19:12,519
so to summarize the problem with ipv6

00:19:07,909 --> 00:19:15,620
and cups is that we are using the same

00:19:12,519 --> 00:19:18,710
the same data structure for the table

00:19:15,620 --> 00:19:22,519
for the static table or almost static

00:19:18,710 --> 00:19:25,519
table and for dynamic entries and the

00:19:22,519 --> 00:19:27,889
locking is done in a way which harms the

00:19:25,519 --> 00:19:33,860
performance whenever we touch the data

00:19:27,889 --> 00:19:38,059
structure so there are some big points

00:19:33,860 --> 00:19:40,730
in the analysis the biggest one of

00:19:38,059 --> 00:19:45,950
course is that these tests do not really

00:19:40,730 --> 00:19:48,950
reflect real life scenario because the

00:19:45,950 --> 00:19:50,899
ryokan currency would be much lower real

00:19:48,950 --> 00:19:55,610
concurrency on the Rue cups would be

00:19:50,899 --> 00:20:00,080
much lower in four real pockets because

00:19:55,610 --> 00:20:02,179
the lookup is only one part of the

00:20:00,080 --> 00:20:03,950
packet processing and for some packets

00:20:02,179 --> 00:20:09,429
for a lot of pockets we are not doing

00:20:03,950 --> 00:20:13,460
the hook up at all or the foo look up

00:20:09,429 --> 00:20:18,440
anaerobic point is the bit completely

00:20:13,460 --> 00:20:23,090
different design it's hard to do a fair

00:20:18,440 --> 00:20:28,669
comparison which would be still

00:20:23,090 --> 00:20:30,769
available also another weak point is

00:20:28,669 --> 00:20:33,320
that for these tests the large routing

00:20:30,769 --> 00:20:36,470
tables were randomly generated so that

00:20:33,320 --> 00:20:42,260
their structure or the patterns do not

00:20:36,470 --> 00:20:47,360
necessarily reflect what do large tables

00:20:42,260 --> 00:20:49,940
from real life Reuters look like I have

00:20:47,360 --> 00:20:53,870
no idea how much this can affect the

00:20:49,940 --> 00:20:55,020
results and of course there are many

00:20:53,870 --> 00:20:58,830
other factors

00:20:55,020 --> 00:21:02,220
which can affect the performance however

00:20:58,830 --> 00:21:06,290
I believe that the problem I tried to

00:21:02,220 --> 00:21:10,470
outline does exist and the scaling of

00:21:06,290 --> 00:21:14,190
ipv6 root lookup is a program that also

00:21:10,470 --> 00:21:18,320
the customer experience indicates dust

00:21:14,190 --> 00:21:20,940
is starting to affect real life systems

00:21:18,320 --> 00:21:24,150
especially told those with high traffic

00:21:20,940 --> 00:21:27,750
directed to many different hosts and

00:21:24,150 --> 00:21:31,440
systems with many CPUs where many is

00:21:27,750 --> 00:21:38,490
already something like 34 or 32 so this

00:21:31,440 --> 00:21:43,050
is quite common these days so what can

00:21:38,490 --> 00:21:49,610
be done I think it's obvious that one

00:21:43,050 --> 00:21:54,150
idea kind of presents itself and that is

00:21:49,610 --> 00:21:56,850
what if we tried to implement ipv6 fib

00:21:54,150 --> 00:21:59,400
in the way similar to what we have for

00:21:56,850 --> 00:22:02,880
ipv4 because it works well in the works

00:21:59,400 --> 00:22:07,320
fine it's highly efficient for ipv4 so

00:22:02,880 --> 00:22:11,280
what if we tried to base fib for ipv6 on

00:22:07,320 --> 00:22:14,100
the same data structure which means the

00:22:11,280 --> 00:22:19,520
level and path compressed prefix three

00:22:14,100 --> 00:22:19,520
and the same approach to talking

00:22:20,240 --> 00:22:25,740
originally I wanted and plant to present

00:22:23,430 --> 00:22:29,030
here also some results with the

00:22:25,740 --> 00:22:31,590
proof-of-concept implementation of this

00:22:29,030 --> 00:22:35,610
where only the data structure and

00:22:31,590 --> 00:22:39,240
algorithms would be implemented in this

00:22:35,610 --> 00:22:43,070
way but last week I found that what I

00:22:39,240 --> 00:22:46,650
had was actually completely wrong so I

00:22:43,070 --> 00:22:49,830
don't have those numbers at the end so

00:22:46,650 --> 00:22:52,860
it's still on the to-do list but I'm

00:22:49,830 --> 00:22:55,070
definitely planning to work on this and

00:22:52,860 --> 00:22:57,669
to check this

00:22:55,070 --> 00:23:03,320
and to see if this does help with the

00:22:57,669 --> 00:23:06,559
performance and in the scaling of course

00:23:03,320 --> 00:23:08,149
this is the blue cup itself and the data

00:23:06,559 --> 00:23:09,590
structure and the algorithms are not

00:23:08,149 --> 00:23:13,970
enough because there are some more

00:23:09,590 --> 00:23:16,429
details like the exception table because

00:23:13,970 --> 00:23:20,919
there is a bit more exceptions for ipv6

00:23:16,429 --> 00:23:27,350
and some things behave a bit differently

00:23:20,919 --> 00:23:34,850
but I believe this has this idea and

00:23:27,350 --> 00:23:37,059
this work has some potential sorry so

00:23:34,850 --> 00:23:50,559
thank you thank you for your attention

00:23:37,059 --> 00:23:50,559
so are there any questions comments

00:24:02,169 --> 00:24:04,200
Oh

00:24:20,850 --> 00:24:27,750
pocket see in future again so

00:24:34,600 --> 00:24:41,860
how we can handle it end of service

00:24:37,670 --> 00:24:41,860
attacks against especially this

00:24:42,040 --> 00:24:51,400
because I think I sixes this is much

00:24:45,610 --> 00:24:53,530
worse than ever before they're good

00:24:51,400 --> 00:24:58,770
object in a happy b 6i can imagine that

00:24:53,530 --> 00:24:58,770
we need to be need to start wyatt action

00:25:03,030 --> 00:25:10,030
well the comment is that there is

00:25:06,100 --> 00:25:14,679
actually work on putting the exception

00:25:10,030 --> 00:25:17,290
handling in ipv4 back to the main data

00:25:14,679 --> 00:25:21,309
structure which is a surprise for me I

00:25:17,290 --> 00:25:25,059
never heard about that and the second

00:25:21,309 --> 00:25:32,410
part was that this risk of denial of

00:25:25,059 --> 00:25:34,780
service attacks attacks attacking or

00:25:32,410 --> 00:25:37,570
targeting the exception handling could

00:25:34,780 --> 00:25:39,460
be much better than ipv6 which is

00:25:37,570 --> 00:25:42,340
exactly what I meant when I talked about

00:25:39,460 --> 00:25:46,929
the devil in the details is that these

00:25:42,340 --> 00:25:51,299
exceptions like emt you are going to be

00:25:46,929 --> 00:25:56,260
more frequent with ipv6 so this is one

00:25:51,299 --> 00:26:00,820
source of possible problems well anyway

00:25:56,260 --> 00:26:03,370
are still think that even if the

00:26:00,820 --> 00:26:07,390
exceptions have to be handled in the

00:26:03,370 --> 00:26:14,320
main data structure the RCU based

00:26:07,390 --> 00:26:18,280
approach from ipv4 could still be a plus

00:26:14,320 --> 00:26:21,160
or an improvement against current state

00:26:18,280 --> 00:26:26,590
where we are using per table read write

00:26:21,160 --> 00:26:28,960
lock because that is something that also

00:26:26,590 --> 00:26:31,900
I didn't present the results from Paris

00:26:28,960 --> 00:26:35,490
but i also used pair of to analyze

00:26:31,900 --> 00:26:41,940
what's going on during these tests and

00:26:35,490 --> 00:26:46,570
indicates that one source of programs is

00:26:41,940 --> 00:26:49,900
the contention on reference counting and

00:26:46,570 --> 00:26:52,929
or not contention but the concurrency on

00:26:49,900 --> 00:26:56,290
the reference counters and one source of

00:26:52,929 --> 00:26:59,710
the problem is this perth table oak read

00:26:56,290 --> 00:27:00,570
write lock so i believe that even if we

00:26:59,710 --> 00:27:03,679
have to

00:27:00,570 --> 00:27:08,120
exceptions back to the main structure

00:27:03,679 --> 00:27:14,850
unifying the implementation would still

00:27:08,120 --> 00:27:17,210
pay out in the end more questions or

00:27:14,850 --> 00:27:17,210
comments

00:27:23,100 --> 00:27:25,940
excuse me

00:27:29,370 --> 00:27:34,330
differently

00:27:31,110 --> 00:27:36,700
you mean why the implementation is

00:27:34,330 --> 00:27:41,260
different I guess that's for historical

00:27:36,700 --> 00:27:43,390
reasons because well I don't remember

00:27:41,260 --> 00:27:47,260
that because I'm working in the kernel

00:27:43,390 --> 00:27:50,320
development only for five years but we

00:27:47,260 --> 00:27:54,600
must remember that until Colonel 36 we

00:27:50,320 --> 00:27:58,660
still had ipv4 routing cash in the old

00:27:54,600 --> 00:28:02,470
version so i guess in the beginning when

00:27:58,660 --> 00:28:04,420
ipv6 routine was implemented in this way

00:28:02,470 --> 00:28:08,140
it was actually better than what we had

00:28:04,420 --> 00:28:10,990
in ipv4 but once this was reworked to

00:28:08,140 --> 00:28:14,530
drop the routine cash from my period for

00:28:10,990 --> 00:28:17,200
the situation kind of reversed but

00:28:14,530 --> 00:28:21,360
that's only my guess because i don't

00:28:17,200 --> 00:28:21,360
remember the discussions from the time

00:28:22,230 --> 00:28:29,020
and this is also one possible argument

00:28:25,600 --> 00:28:32,380
for unifying the implementation that it

00:28:29,020 --> 00:28:35,860
would make management of the code easier

00:28:32,380 --> 00:28:39,180
if we could use the same approach and

00:28:35,860 --> 00:28:39,180
the same algorithms for both

00:28:41,410 --> 00:28:47,070

YouTube URL: https://www.youtube.com/watch?v=7SD4GKhSmxE


