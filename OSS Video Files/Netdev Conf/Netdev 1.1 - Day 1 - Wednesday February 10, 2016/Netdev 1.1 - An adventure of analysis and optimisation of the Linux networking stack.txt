Title: Netdev 1.1 - An adventure of analysis and optimisation of the Linux networking stack
Publication date: 2016-03-10
Playlist: Netdev 1.1 - Day 1 - Wednesday February 10, 2016
Description: 
	Marco Varlese, Kim-marie Jones
February 2016
Captions: 
	00:00:01,010 --> 00:00:07,529
Hey hi everyone I'm Kim and this is

00:00:04,740 --> 00:00:09,870
Marco and our talk today we'd like to

00:00:07,529 --> 00:00:12,269
tell you about our adventure of analysis

00:00:09,870 --> 00:00:17,780
and optimization of the Linux networking

00:00:12,269 --> 00:00:17,780
stack we both work for Intel in Ireland

00:00:21,450 --> 00:00:26,730
has to be done this so I'm just gonna

00:00:24,720 --> 00:00:29,280
give a bit of an introduction to our

00:00:26,730 --> 00:00:32,340
project I'll tell you our use cases and

00:00:29,280 --> 00:00:34,470
then I'll discuss the BIOS kernel and

00:00:32,340 --> 00:00:36,750
system settings that we found improved

00:00:34,470 --> 00:00:38,790
the performance for our setup and then

00:00:36,750 --> 00:00:39,840
I'll hand you over to Marco and he will

00:00:38,790 --> 00:00:46,080
take you through the benchmarking

00:00:39,840 --> 00:00:48,540
results and discuss the next steps so

00:00:46,080 --> 00:00:50,760
our main goal was simply to enhance the

00:00:48,540 --> 00:00:54,030
out-of-the-box performance of the Linux

00:00:50,760 --> 00:00:56,280
networking stack obviously there are

00:00:54,030 --> 00:00:59,010
these kernel bypass technologies like DB

00:00:56,280 --> 00:01:00,240
DK but I mean if we all wanted to bypass

00:00:59,010 --> 00:01:03,870
the kernel none of us would be here

00:01:00,240 --> 00:01:06,119
right so our main goal yes was to just

00:01:03,870 --> 00:01:09,119
improve the performance and then also to

00:01:06,119 --> 00:01:11,040
provide a set of guidelines for anyone

00:01:09,119 --> 00:01:14,369
else that wanted to do something similar

00:01:11,040 --> 00:01:16,200
and we plan to include any BIOS and

00:01:14,369 --> 00:01:19,050
kernel and system settings that we found

00:01:16,200 --> 00:01:20,759
improved our network performance all of

00:01:19,050 --> 00:01:22,860
our tests were done with an X running on

00:01:20,759 --> 00:01:27,210
bare metal but the same tweaks should

00:01:22,860 --> 00:01:29,280
apply to virtualization VMs and

00:01:27,210 --> 00:01:34,500
containers with a little bit of extra

00:01:29,280 --> 00:01:37,679
tweaking so for our tests we focused on

00:01:34,500 --> 00:01:41,160
l2 and l3 and on the TCP and UDP

00:01:37,679 --> 00:01:43,920
protocols although we do plan to extend

00:01:41,160 --> 00:01:47,220
that to s TMP in the future for some

00:01:43,920 --> 00:01:50,009
telco use cases the packet sizes that we

00:01:47,220 --> 00:01:52,619
tested with were 64 bytes up to jumbo

00:01:50,009 --> 00:01:56,039
frames and we included an IMAX profile

00:01:52,619 --> 00:01:58,580
as well to give a more realistic example

00:01:56,039 --> 00:02:01,470
of what network traffic might look like

00:01:58,580 --> 00:02:04,050
so we measured throughput latency and

00:02:01,470 --> 00:02:06,780
scalability from the networking site and

00:02:04,050 --> 00:02:09,569
then we also measured some performance

00:02:06,780 --> 00:02:12,810
metrics such as CPU utilization for

00:02:09,569 --> 00:02:15,090
example so our testing procedure

00:02:12,810 --> 00:02:17,250
basically involved first of all we clone

00:02:15,090 --> 00:02:20,040
our pull from that next and then we'd

00:02:17,250 --> 00:02:21,959
modify either by us a kernel or a system

00:02:20,040 --> 00:02:23,700
setting that we had found through or

00:02:21,959 --> 00:02:26,910
expected through research should improve

00:02:23,700 --> 00:02:28,560
the performance of the network stack and

00:02:26,910 --> 00:02:29,950
then we would depending on our use case

00:02:28,560 --> 00:02:33,460
use

00:02:29,950 --> 00:02:35,500
excites network or i pref to run our

00:02:33,460 --> 00:02:37,060
benchmarks and once we obtain the

00:02:35,500 --> 00:02:40,390
results we would then compare them to

00:02:37,060 --> 00:02:42,640
previous test iterations and continue to

00:02:40,390 --> 00:02:44,440
modify our settings until we had a best

00:02:42,640 --> 00:02:49,420
known configuration for both of our use

00:02:44,440 --> 00:02:51,040
cases so these use cases we had two the

00:02:49,420 --> 00:02:52,780
first was to treat the platform as a

00:02:51,040 --> 00:02:54,490
network forwarding node and then the

00:02:52,780 --> 00:02:56,860
second was to treat the platform as a

00:02:54,490 --> 00:03:00,190
network endpoint in the forwarding

00:02:56,860 --> 00:03:01,780
scenario we used Ixia IX network to

00:03:00,190 --> 00:03:04,000
forward traffic to a port on the

00:03:01,780 --> 00:03:06,460
platform and the traffic was then routed

00:03:04,000 --> 00:03:08,590
to another port on the same platform on

00:03:06,460 --> 00:03:10,710
a different subnet and sent back to Ixia

00:03:08,590 --> 00:03:15,430
where the throughput and latency were

00:03:10,710 --> 00:03:17,920
calculated in the endpoint scenario we

00:03:15,430 --> 00:03:22,540
had iperf client running on the platform

00:03:17,920 --> 00:03:24,780
and sending packets to 40gb port on that

00:03:22,540 --> 00:03:27,340
platform with a particular IP address

00:03:24,780 --> 00:03:29,560
iperf server was bound to that port and

00:03:27,340 --> 00:03:32,100
so when the packets arrived we were able

00:03:29,560 --> 00:03:35,140
to determine the throughput and latency

00:03:32,100 --> 00:03:37,780
so there was a large number of BIOS

00:03:35,140 --> 00:03:41,380
kernel and system settings that we found

00:03:37,780 --> 00:03:42,700
improved our performance and through the

00:03:41,380 --> 00:03:44,440
testing procedure that I previously

00:03:42,700 --> 00:03:46,150
mentioned we obtained a best known

00:03:44,440 --> 00:03:49,900
configuration for both of the use cases

00:03:46,150 --> 00:03:52,330
I just mentioned in terms of the BIOS

00:03:49,900 --> 00:03:54,700
first of all we disabled hyper threading

00:03:52,330 --> 00:03:59,620
the reason for this was that we were

00:03:54,700 --> 00:04:03,580
finit izing each or X and T export Q to

00:03:59,620 --> 00:04:05,530
one CPU so one Q one CPU what the

00:04:03,580 --> 00:04:07,450
presence of the secondary logical core

00:04:05,530 --> 00:04:09,130
when hyper shredding was enabled meant

00:04:07,450 --> 00:04:12,489
that that Q wouldn't be able to fully

00:04:09,130 --> 00:04:14,650
utilize the CPU it was bound to in terms

00:04:12,489 --> 00:04:17,109
of turbo boost we also disabled that

00:04:14,650 --> 00:04:18,669
because the erratic frequency hikes can

00:04:17,109 --> 00:04:21,790
lead to quite unstable performance

00:04:18,669 --> 00:04:23,440
results we disabled P States because

00:04:21,790 --> 00:04:24,730
obviously we didn't want the CPUs going

00:04:23,440 --> 00:04:27,820
to sleep when they weren't doing work

00:04:24,730 --> 00:04:29,470
and we disabled P States as well so that

00:04:27,820 --> 00:04:32,020
the CPUs were running at maximum

00:04:29,470 --> 00:04:34,419
frequency and voltage at all times a

00:04:32,020 --> 00:04:37,639
little bit of a disclaimer we were only

00:04:34,419 --> 00:04:39,499
interested in network performance so

00:04:37,639 --> 00:04:41,629
if you disable all of these and then you

00:04:39,499 --> 00:04:42,680
find that your CPU burns out after a

00:04:41,629 --> 00:04:47,330
year and you have a really high

00:04:42,680 --> 00:04:49,370
electricity bill don't blame me there

00:04:47,330 --> 00:04:53,569
was only one kernel setting that needed

00:04:49,370 --> 00:04:56,659
to be changed from the default this was

00:04:53,569 --> 00:04:59,150
pre-empting on so basically we disabled

00:04:56,659 --> 00:05:01,610
any kernel code preemption which is

00:04:59,150 --> 00:05:04,719
ideal for good throughput and there was

00:05:01,610 --> 00:05:07,669
a large number of network core ipv4

00:05:04,719 --> 00:05:10,520
affinities ation and nick settings that

00:05:07,669 --> 00:05:13,639
we changed using the proc file system

00:05:10,520 --> 00:05:15,650
and east to unfortunately don't have

00:05:13,639 --> 00:05:17,270
time to go through them all now but we

00:05:15,650 --> 00:05:18,800
do have them recorded in the backup

00:05:17,270 --> 00:05:21,110
slides and they'll be available online

00:05:18,800 --> 00:05:24,590
after the presentation to anyone who is

00:05:21,110 --> 00:05:25,879
interested so I'll now hand you over to

00:05:24,590 --> 00:05:27,490
Marco and he'll take you through the

00:05:25,879 --> 00:05:34,370
rest of the presentation

00:05:27,490 --> 00:05:39,499
hi everyone so and just briefly on the

00:05:34,370 --> 00:05:41,719
setup we were using a end desktop which

00:05:39,499 --> 00:05:46,610
looks like Morris's server said three

00:05:41,719 --> 00:05:49,819
gigahertz CPU was 64 gigabyte of RAM and

00:05:46,610 --> 00:05:52,490
we were using the i-40 II internal

00:05:49,819 --> 00:05:58,189
network card which is the most known as

00:05:52,490 --> 00:05:59,899
forth will we we did use two form

00:05:58,189 --> 00:06:03,560
factors of the of the network card one

00:05:59,899 --> 00:06:06,740
was the 4 by 10 gigabit and another one

00:06:03,560 --> 00:06:09,770
was the 1 by 40 the rationale behind

00:06:06,740 --> 00:06:13,189
that is in the forwarding scenario we

00:06:09,770 --> 00:06:16,520
had some logistics problem in the lab

00:06:13,189 --> 00:06:20,360
using the Axia setup so we we had to use

00:06:16,520 --> 00:06:22,580
the 1 by 40 solidi 4 by 10 while in the

00:06:20,360 --> 00:06:24,770
endpoints in ru we just because we were

00:06:22,580 --> 00:06:27,379
using to two desktops connected to each

00:06:24,770 --> 00:06:32,379
other we we were able to use the 1 by 40

00:06:27,379 --> 00:06:36,289
gigabit a little bit of the methodology

00:06:32,379 --> 00:06:38,750
each test case we ran was running for

00:06:36,289 --> 00:06:42,050
between 30 and 60 seconds depending on

00:06:38,750 --> 00:06:44,000
which test we were focusing on

00:06:42,050 --> 00:06:47,240
the true boots used for each pocket size

00:06:44,000 --> 00:06:51,410
is the average over the total run time

00:06:47,240 --> 00:06:54,530
and usually the CPU idle time was taken

00:06:51,410 --> 00:06:57,260
around 10-15 seconds in the test were

00:06:54,530 --> 00:06:59,000
basically the full test will was fully

00:06:57,260 --> 00:07:02,750
loaded and all the flaws were running

00:06:59,000 --> 00:07:05,060
and we were kinda sure that the that at

00:07:02,750 --> 00:07:10,580
that point CPU was kinda handling as

00:07:05,060 --> 00:07:13,550
much as possible again this is just like

00:07:10,580 --> 00:07:15,830
very briefly about some settings that we

00:07:13,550 --> 00:07:18,169
had asked in pointed out we have all

00:07:15,830 --> 00:07:21,650
these documented in our backup slides

00:07:18,169 --> 00:07:24,130
and in the paper so from a common line

00:07:21,650 --> 00:07:28,220
perspective from a boot grub perspective

00:07:24,130 --> 00:07:31,370
we disabled the piece States we disabled

00:07:28,220 --> 00:07:33,260
the ipv6 we had enabled the transparent

00:07:31,370 --> 00:07:39,169
huge pages together with huge page

00:07:33,260 --> 00:07:42,410
support and we isolated the CPU 0 to

00:07:39,169 --> 00:07:46,760
basically you know so that all the

00:07:42,410 --> 00:07:49,310
system interrupts all the system and

00:07:46,760 --> 00:07:52,660
alerts were basically running on CP u 0

00:07:49,310 --> 00:07:56,720
and because of that also other tweaks

00:07:52,660 --> 00:07:59,810
were done for example to use CPU 0 only

00:07:56,720 --> 00:08:01,760
- and for example the management port of

00:07:59,810 --> 00:08:04,250
the desktop

00:08:01,760 --> 00:08:06,919
another thing we did just to have much

00:08:04,250 --> 00:08:09,380
more control on how the frequency was

00:08:06,919 --> 00:08:11,750
being set we Nabal the user space

00:08:09,380 --> 00:08:14,360
governor and it's just more for

00:08:11,750 --> 00:08:17,390
convenience and we set the D CPU

00:08:14,360 --> 00:08:19,729
frequency speed to its maximum which is

00:08:17,390 --> 00:08:24,530
the maximum allowed by this CPU which

00:08:19,729 --> 00:08:28,100
was triggered again just because this

00:08:24,530 --> 00:08:30,650
will come up with benchmarks what we

00:08:28,100 --> 00:08:32,570
identify as the tcp full of load versus

00:08:30,650 --> 00:08:35,089
tcp know off load is virtually the

00:08:32,570 --> 00:08:38,089
setting that you can do through the eth

00:08:35,089 --> 00:08:39,970
tool - k command where you can enable

00:08:38,089 --> 00:08:45,220
for example

00:08:39,970 --> 00:08:48,400
Erick's hashing or DTS or dro offloading

00:08:45,220 --> 00:08:49,810
onto the NIC and similarly did not

00:08:48,400 --> 00:08:53,320
offload is literally having all those

00:08:49,810 --> 00:08:54,910
options set to OFF something that I

00:08:53,320 --> 00:08:58,060
would like to point out is that the

00:08:54,910 --> 00:09:01,120
network card we use has a lot of support

00:08:58,060 --> 00:09:03,880
for TCP offloading but it didn't have

00:09:01,120 --> 00:09:11,350
any for UDP so there was really not much

00:09:03,880 --> 00:09:14,370
benefit for the UDP use case so what we

00:09:11,350 --> 00:09:17,710
did as Kim said we stick with net next

00:09:14,370 --> 00:09:19,930
and the rationale is we wanted to make

00:09:17,710 --> 00:09:21,550
sure that any improvement made by all

00:09:19,930 --> 00:09:24,220
the contributors we could look at those

00:09:21,550 --> 00:09:26,800
and similarly if we had done anything to

00:09:24,220 --> 00:09:31,570
to change tweak or contribute we could

00:09:26,800 --> 00:09:35,170
do it very easily the the diversion is

00:09:31,570 --> 00:09:36,580
the four four zero RC 3 and in all cases

00:09:35,170 --> 00:09:38,890
we are talking about all our

00:09:36,580 --> 00:09:42,310
optimization that you can find in the

00:09:38,890 --> 00:09:44,620
paper where or turned on similar to what

00:09:42,310 --> 00:09:45,760
Jamal pointed out earlier on I would

00:09:44,620 --> 00:09:47,980
really like to discuss some of these

00:09:45,760 --> 00:09:50,380
findings later on in the buff because

00:09:47,980 --> 00:09:53,710
for example there is some discrepancy

00:09:50,380 --> 00:09:57,100
here when we look at the setting of the

00:09:53,710 --> 00:10:00,490
NTU sites on the ethernet between the

00:09:57,100 --> 00:10:04,180
1500 and the 9 9000 support jumbo frames

00:10:00,490 --> 00:10:06,790
and even for packet sizes that are

00:10:04,180 --> 00:10:09,040
actually below the 1500 mg sides we

00:10:06,790 --> 00:10:12,450
actually see some discrepancies in in

00:10:09,040 --> 00:10:19,060
the throughput in the line raid percent

00:10:12,450 --> 00:10:21,150
and we did run the stats many many times

00:10:19,060 --> 00:10:23,950
and we always end up with same results

00:10:21,150 --> 00:10:26,260
it's some weirdness with which we didn't

00:10:23,950 --> 00:10:31,690
really manage to explain just because as

00:10:26,260 --> 00:10:33,280
you can see I would I would kind of be

00:10:31,690 --> 00:10:36,310
able to explain if the packet size was

00:10:33,280 --> 00:10:37,720
between 1,500 and in 9000 but it's

00:10:36,310 --> 00:10:40,240
really hard to make for me to explain

00:10:37,720 --> 00:10:42,550
why this such a discrepancies in in

00:10:40,240 --> 00:10:45,000
performance throughput for packet sizes

00:10:42,550 --> 00:10:49,740
around 512 written

00:10:45,000 --> 00:10:51,960
it makes very little sense a similar

00:10:49,740 --> 00:10:57,180
discrepancies can be found also for the

00:10:51,960 --> 00:10:59,520
TCP use case between an MTU size of 1500

00:10:57,180 --> 00:11:05,760
vs. an empty size of 9,000 on the

00:10:59,520 --> 00:11:09,660
interface then after that we decided to

00:11:05,760 --> 00:11:13,350
actually start looking at what if we are

00:11:09,660 --> 00:11:15,600
able to handle the software queues and

00:11:13,350 --> 00:11:19,950
having a strategy that's a little bit

00:11:15,600 --> 00:11:23,070
more precise that that can reduce the

00:11:19,950 --> 00:11:24,540
jitter between operations and so we

00:11:23,070 --> 00:11:28,110
looked at the RT patch the real-time

00:11:24,540 --> 00:11:31,050
patch first of all because we were using

00:11:28,110 --> 00:11:33,540
net next we had to port the earthly

00:11:31,050 --> 00:11:37,260
patch to donate to the net next branch

00:11:33,540 --> 00:11:39,740
so there was some effort there and then

00:11:37,260 --> 00:11:43,100
we would actually start benchmarking and

00:11:39,740 --> 00:11:46,590
the findings are actually very positive

00:11:43,100 --> 00:11:48,510
so in terms of throughput for small

00:11:46,590 --> 00:11:53,070
packet sizes which are like below 700

00:11:48,510 --> 00:11:55,020
bytes the RT patch is actually providing

00:11:53,070 --> 00:11:59,460
at 3x by the performance versus a

00:11:55,020 --> 00:12:01,380
standard kernel and similarly when

00:11:59,460 --> 00:12:05,040
looking at the latency we actually

00:12:01,380 --> 00:12:08,670
observed the same type of behavior up to

00:12:05,040 --> 00:12:12,600
the thousand thousand 24 bytes with two

00:12:08,670 --> 00:12:17,610
acts by the latency performance in a

00:12:12,600 --> 00:12:19,650
similar fashion the CPU idle time of the

00:12:17,610 --> 00:12:23,610
RT patched version of the kernel

00:12:19,650 --> 00:12:27,210
actually was much better again almost

00:12:23,610 --> 00:12:31,230
double the number of three cycles versus

00:12:27,210 --> 00:12:33,870
T the standard kernel and to me wasn't

00:12:31,230 --> 00:12:37,230
like a big surprise than eventually the

00:12:33,870 --> 00:12:39,060
two the two different versions of the

00:12:37,230 --> 00:12:43,980
kernels like the vanilla one and RT

00:12:39,060 --> 00:12:46,310
patch of entually merged at a bigger

00:12:43,980 --> 00:12:49,500
packet size just because the number of

00:12:46,310 --> 00:12:51,690
interrupts number of packets that going

00:12:49,500 --> 00:12:55,290
through the system actually much lower

00:12:51,690 --> 00:13:00,780
at the bigger pocket-size this was for

00:12:55,290 --> 00:13:02,930
the UDP but the TCP use case showed

00:13:00,780 --> 00:13:05,130
exactly the same type of performance

00:13:02,930 --> 00:13:11,270
from both a throughput and latency

00:13:05,130 --> 00:13:11,270
perspective and CPU cycles perspective

00:13:12,980 --> 00:13:18,810
as I said at the beginning we just were

00:13:15,750 --> 00:13:22,380
not just interested in the forwarding

00:13:18,810 --> 00:13:25,850
sandara's but like also in using linux

00:13:22,380 --> 00:13:29,960
as an endpoint scenario so we we used

00:13:25,850 --> 00:13:32,220
iperf to basically use a standard

00:13:29,960 --> 00:13:36,270
application that can consumes packets

00:13:32,220 --> 00:13:39,540
either TCP or UDP and I I don't have

00:13:36,270 --> 00:13:42,480
that slide here but when we when we were

00:13:39,540 --> 00:13:44,880
using like one flow the throughput at 64

00:13:42,480 --> 00:13:47,970
byte packet was 1.75 gigabit per second

00:13:44,880 --> 00:13:51,180
in the user space and when we were

00:13:47,970 --> 00:13:54,270
basically moving the application to use

00:13:51,180 --> 00:13:58,400
the seven-course just because one call

00:13:54,270 --> 00:14:00,900
was just jailed seven core seven flows

00:13:58,400 --> 00:14:04,230
the scalability just proved out to be

00:14:00,900 --> 00:14:09,840
almost linear from 175 to eleven point

00:14:04,230 --> 00:14:12,630
nine what was very interesting here is

00:14:09,840 --> 00:14:14,370
that we we did play a lot with the

00:14:12,630 --> 00:14:17,940
different MTU sizes for example to be

00:14:14,370 --> 00:14:21,300
set on the Ethernet interface and as I

00:14:17,940 --> 00:14:24,720
briefly said at the beginning we have

00:14:21,300 --> 00:14:29,130
this full of loading offloading type of

00:14:24,720 --> 00:14:33,660
settings and we also like played with

00:14:29,130 --> 00:14:37,310
the window size and window scaling did

00:14:33,660 --> 00:14:40,380
the hosta graph shows here obviously the

00:14:37,310 --> 00:14:43,020
utilizing the offloading of abilities of

00:14:40,380 --> 00:14:46,290
the network card has a huge impact on

00:14:43,020 --> 00:14:48,570
performance like I think the chart

00:14:46,290 --> 00:14:51,330
speaks on its own the bottom the bottom

00:14:48,570 --> 00:14:55,110
line there is a is a benchmark done with

00:14:51,330 --> 00:14:59,440
no offloading at all on the system while

00:14:55,110 --> 00:15:01,480
the uppermost is using all the possible

00:14:59,440 --> 00:15:04,660
settings and all the applauding

00:15:01,480 --> 00:15:07,650
capabilities of the network art but

00:15:04,660 --> 00:15:09,700
what's also very interesting is again

00:15:07,650 --> 00:15:12,550
very differently than what actually

00:15:09,700 --> 00:15:16,740
happened in the forwarding scenario here

00:15:12,550 --> 00:15:20,200
the MTU set on the Internet is actually

00:15:16,740 --> 00:15:23,290
giving a different type of outcome while

00:15:20,200 --> 00:15:25,240
in the forwarding scenario smaller MTU

00:15:23,290 --> 00:15:27,370
was actually behaving better in terms of

00:15:25,240 --> 00:15:29,140
throughput in the endpoint scenario a

00:15:27,370 --> 00:15:32,710
bigger MTU is actually providing better

00:15:29,140 --> 00:15:35,800
performance and again this discrepancy

00:15:32,710 --> 00:15:37,720
and because we also did the formatting

00:15:35,800 --> 00:15:40,150
scenario we couldn't explain this and I

00:15:37,720 --> 00:15:46,660
hope that we can talk about this more in

00:15:40,150 --> 00:15:50,610
the details at the Bob so with regard to

00:15:46,660 --> 00:15:54,040
the next steps I think our end goal was

00:15:50,610 --> 00:15:58,390
was much bigger than what we thought and

00:15:54,040 --> 00:16:01,150
we we wanted to go down and using perf

00:15:58,390 --> 00:16:02,500
so looking at very much into details of

00:16:01,150 --> 00:16:05,350
what's happening to the system we didn't

00:16:02,500 --> 00:16:08,200
manage to to do that activity before

00:16:05,350 --> 00:16:11,800
today so that's definitely a con on our

00:16:08,200 --> 00:16:14,860
next step on the rider because we did

00:16:11,800 --> 00:16:16,480
manage to benchmark the benefits of the

00:16:14,860 --> 00:16:18,130
RT patch in the forwarding Solari we

00:16:16,480 --> 00:16:21,550
would like to do the same using the

00:16:18,130 --> 00:16:24,550
endpoint scenario so again using hyper

00:16:21,550 --> 00:16:27,520
on a patched version of the kernel using

00:16:24,550 --> 00:16:30,670
the real-time patch we are definitely

00:16:27,520 --> 00:16:32,310
like interested in understanding the

00:16:30,670 --> 00:16:34,750
empty wilderness that we have observed

00:16:32,310 --> 00:16:38,490
above all the discrepancies between the

00:16:34,750 --> 00:16:42,910
two different use cases and why is that

00:16:38,490 --> 00:16:44,800
just because us again tomorrow and not

00:16:42,910 --> 00:16:48,040
earlier on there's a lot of noise made

00:16:44,800 --> 00:16:50,980
by all this bypass technologies and what

00:16:48,040 --> 00:16:53,650
they can achieve we would like to look

00:16:50,980 --> 00:16:56,260
into the park and a map option on the

00:16:53,650 --> 00:17:00,070
sockets and really understand the

00:16:56,260 --> 00:17:01,000
benefit of it and when doing that we

00:17:00,070 --> 00:17:03,310
would like to create a standalone

00:17:01,000 --> 00:17:06,160
application that can actually do some l3

00:17:03,310 --> 00:17:07,990
forwarding and again investigate the

00:17:06,160 --> 00:17:09,100
benefit of the queue disk bypass which

00:17:07,990 --> 00:17:13,140
is another option

00:17:09,100 --> 00:17:16,180
we have even through the packet I'm up

00:17:13,140 --> 00:17:19,840
because this is are all these tests are

00:17:16,180 --> 00:17:22,240
really like baseline tests we have a

00:17:19,840 --> 00:17:23,830
couple of use cases customer use cases

00:17:22,240 --> 00:17:26,350
that we are really interested in

00:17:23,830 --> 00:17:30,010
benchmarking and see how do they perform

00:17:26,350 --> 00:17:33,870
so to have a real-life scenario where

00:17:30,010 --> 00:17:36,100
Linux is being used and again as I said

00:17:33,870 --> 00:17:39,520
we would like to release a working on

00:17:36,100 --> 00:17:41,140
our third phase which is using perps I'm

00:17:39,520 --> 00:17:44,380
looking into the code identifying

00:17:41,140 --> 00:17:47,380
bottlenecks which could be logs memory

00:17:44,380 --> 00:17:49,360
copies interrupt handlers anything

00:17:47,380 --> 00:17:56,080
really that can either improve or make

00:17:49,360 --> 00:17:57,820
performance worse so just because so

00:17:56,080 --> 00:18:00,880
just because when we started doing this

00:17:57,820 --> 00:18:03,730
this activity was I think was mid

00:18:00,880 --> 00:18:08,020
November late November and we had a big

00:18:03,730 --> 00:18:10,450
big goal in mind I think we missed the

00:18:08,020 --> 00:18:13,000
point of you know allowing us for a lot

00:18:10,450 --> 00:18:15,850
of ramping up there are lots of knobs in

00:18:13,000 --> 00:18:18,490
the car that can be tuned and sometimes

00:18:15,850 --> 00:18:21,400
you keep tuning things and things get

00:18:18,490 --> 00:18:22,750
better and then you hit one one value

00:18:21,400 --> 00:18:26,980
one setting that actually makes

00:18:22,750 --> 00:18:28,480
everything worse so things you need to

00:18:26,980 --> 00:18:32,530
law if you're interested in doing this

00:18:28,480 --> 00:18:35,800
sort of analysis and tuning need a long

00:18:32,530 --> 00:18:41,260
time to understand old knobs and what's

00:18:35,800 --> 00:18:44,770
really happening on the system so what

00:18:41,260 --> 00:18:48,460
we found as well out is that a lot of

00:18:44,770 --> 00:18:52,120
performance benefit can be just taken by

00:18:48,460 --> 00:18:54,670
you know changing some BIOS settings or

00:18:52,120 --> 00:18:58,870
taking advantage fully of your system so

00:18:54,670 --> 00:19:02,080
for example flow affinities ation offers

00:18:58,870 --> 00:19:05,080
offer so much offers a great opportunity

00:19:02,080 --> 00:19:09,340
for better performance better locality

00:19:05,080 --> 00:19:11,830
of packets less cache misses so just

00:19:09,340 --> 00:19:13,480
knowing your system how how the flow of

00:19:11,830 --> 00:19:17,070
servers your system actually already

00:19:13,480 --> 00:19:17,070
helps in terms of performance tuning

00:19:17,889 --> 00:19:31,659
so thanks okay then you have any

00:19:28,279 --> 00:19:31,659
question comment okay

00:19:34,210 --> 00:19:38,779
can you explain this offloading thing is

00:19:36,830 --> 00:19:40,789
that something they Nick can do is the

00:19:38,779 --> 00:19:40,970
TCP off is it at all or what what is

00:19:40,789 --> 00:19:43,940
that

00:19:40,970 --> 00:19:46,850
is it our TCP polling or is it it's a

00:19:43,940 --> 00:19:52,039
it's basically the G ro offloading TS

00:19:46,850 --> 00:19:55,129
offloading okay yeah okay that was

00:19:52,039 --> 00:19:56,750
confusing all right so it's TS or zero

00:19:55,129 --> 00:19:59,330
all right for a second there I thought

00:19:56,750 --> 00:20:06,309
you're putting at all which is a scene

00:19:59,330 --> 00:20:06,309
around here okay any question

00:20:18,320 --> 00:20:27,440
so I was wondering for these tests were

00:20:22,020 --> 00:20:31,410
you doing ipv6 ipv4 or both it was ipv4

00:20:27,440 --> 00:20:34,350
ipv4 yes so it'd be interesting to see

00:20:31,410 --> 00:20:35,790
ipv6 also you may find some differences

00:20:34,350 --> 00:20:40,800
and one thing I'm particularly concerned

00:20:35,790 --> 00:20:43,140
about is Hardware may have differences

00:20:40,800 --> 00:20:46,290
like if there's a difference in TSO with

00:20:43,140 --> 00:20:49,470
how ipv4 it could be six works that

00:20:46,290 --> 00:20:51,060
could be kind of critical yeah it's

00:20:49,470 --> 00:20:54,600
definitely not doesn't want to be an

00:20:51,060 --> 00:20:57,650
excuse but we had to trim our test cases

00:20:54,600 --> 00:21:00,840
a lot because we didn't divide so many

00:20:57,650 --> 00:21:02,940
and and on the on our agenda there's so

00:21:00,840 --> 00:21:06,930
many still to be tested in ipv6 every on

00:21:02,940 --> 00:21:10,710
the agenda well right but think of it

00:21:06,930 --> 00:21:12,630
this way so ipv6 is definitely growing

00:21:10,710 --> 00:21:15,660
exponentially and there should be no

00:21:12,630 --> 00:21:18,090
question with any vendor now that ipv6

00:21:15,660 --> 00:21:20,790
is absolutely critical so be very

00:21:18,090 --> 00:21:24,000
helpful to see ipv6 probably ipv6 and

00:21:20,790 --> 00:21:25,440
ipv4 and you may actually discover some

00:21:24,000 --> 00:21:26,070
performance differences that could be

00:21:25,440 --> 00:21:31,590
interesting

00:21:26,070 --> 00:21:39,710
thank you anything I'll call my no

00:21:31,590 --> 00:21:39,710
question okay two more good anyone else

00:21:42,700 --> 00:21:50,010
I have a question about you said flow

00:21:46,180 --> 00:21:52,720
affinity pinning do you do that using

00:21:50,010 --> 00:21:54,450
some specific thing on the device so

00:21:52,720 --> 00:21:57,910
that it actually maps to a specific car

00:21:54,450 --> 00:22:01,380
so it's it's using a feature from the

00:21:57,910 --> 00:22:01,380
networker which is called flow director

00:22:10,880 --> 00:22:15,410
right I just wanted to give a quick

00:22:12,640 --> 00:22:18,490
feedback or input since you mentioned

00:22:15,410 --> 00:22:20,540
you wanted to SCTP benchmarking as well

00:22:18,490 --> 00:22:24,880
you will be in for a big surprise

00:22:20,540 --> 00:22:29,540
because linux is known to have a very

00:22:24,880 --> 00:22:30,830
well slow and inefficient implementation

00:22:29,540 --> 00:22:34,310
of SCTP so that's going to be an

00:22:30,830 --> 00:22:37,190
interesting ride well this is why we

00:22:34,310 --> 00:22:40,390
want to look into s-http because we

00:22:37,190 --> 00:22:42,890
actually have a couple of customers that

00:22:40,390 --> 00:22:45,440
having troubles so we'd like to help

00:22:42,890 --> 00:22:49,040
yeah I mean I do a lot of work in in in

00:22:45,440 --> 00:22:50,930
Telecom and the the nobody seems to be

00:22:49,040 --> 00:22:52,820
interested in improving the mainline

00:22:50,930 --> 00:22:54,890
kernel code and everyone uses an out of

00:22:52,820 --> 00:22:57,410
tree proprietary SCTP implementation

00:22:54,890 --> 00:22:59,420
because the next one just has so many

00:22:57,410 --> 00:23:01,010
issues it's very unfortunate that nobody

00:22:59,420 --> 00:23:03,260
seems to be contributing to the effort

00:23:01,010 --> 00:23:06,470
and rather just use proprietary stacks

00:23:03,260 --> 00:23:16,180
offer this is gonna be the good excuse

00:23:06,470 --> 00:23:16,180
and prove it okay thanks okay one more

00:23:23,610 --> 00:23:29,130
so you mentioned that you used a lot you

00:23:26,970 --> 00:23:31,200
had some stuff with interrupt didn't you

00:23:29,130 --> 00:23:34,940
configure interrupt call it coalescing 3

00:23:31,200 --> 00:23:37,320
th tool didn't work well yeah we sew in

00:23:34,940 --> 00:23:40,170
network paper we played a lot with the

00:23:37,320 --> 00:23:42,480
interrupts collison as well through D th

00:23:40,170 --> 00:23:44,190
tool obviously we're using Nabi but you

00:23:42,480 --> 00:23:49,440
still have even with knob you still have

00:23:44,190 --> 00:23:50,760
all these soft Eric use going forward in

00:23:49,440 --> 00:23:53,010
case let's say you have an infinite

00:23:50,760 --> 00:23:56,060
stream of packets so at some point you

00:23:53,010 --> 00:23:58,710
don't have any interrupt anymore right

00:23:56,060 --> 00:24:01,320
but what we've also found out is but

00:23:58,710 --> 00:24:03,300
with the Alex with the extra text

00:24:01,320 --> 00:24:07,320
coalescing different values we're giving

00:24:03,300 --> 00:24:09,240
us different performance different

00:24:07,320 --> 00:24:10,980
benefits so sometimes you actually get

00:24:09,240 --> 00:24:13,290
weather troopers and other times you get

00:24:10,980 --> 00:24:14,550
better latencies so we ended up with the

00:24:13,290 --> 00:24:19,440
specific body that you could actually

00:24:14,550 --> 00:24:22,800
find in our paper to be what seems to be

00:24:19,440 --> 00:24:25,280
the best you are using stricter

00:24:22,800 --> 00:24:25,280
motivation

00:24:29,450 --> 00:24:32,200
fixed

00:24:33,970 --> 00:24:39,080
or you use the deputy Foundation or

00:24:36,500 --> 00:24:40,820
they're just not adaptive was off okay

00:24:39,080 --> 00:24:44,000
yes yes of course

00:24:40,820 --> 00:24:47,510
so so the strict one didn't work well it

00:24:44,000 --> 00:24:51,620
did with somebody's which if I remember

00:24:47,510 --> 00:24:53,030
correctly now is 25 and 75 okay so in

00:24:51,620 --> 00:24:57,070
the in the in the buff later you will

00:24:53,030 --> 00:24:57,070
speak about the RT patch and what okay

00:25:06,390 --> 00:25:13,280
okay and it'll comment/question good

00:25:08,580 --> 00:25:13,280

YouTube URL: https://www.youtube.com/watch?v=e94kSaDItZE


