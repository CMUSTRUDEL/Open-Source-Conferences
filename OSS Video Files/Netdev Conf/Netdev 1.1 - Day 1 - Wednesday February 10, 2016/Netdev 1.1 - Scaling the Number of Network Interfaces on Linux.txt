Title: Netdev 1.1 - Scaling the Number of Network Interfaces on Linux
Publication date: 2016-03-10
Playlist: Netdev 1.1 - Day 1 - Wednesday February 10, 2016
Description: 
	David Ahern, Nikolay Aleksandrov, Roopa Prabhu
February 2016
Captions: 
	00:00:00,500 --> 00:00:05,430
all right so yeah we're gonna talk about

00:00:02,970 --> 00:00:08,250
some work that Nicolai as well as part

00:00:05,430 --> 00:00:11,670
of this team that myself Rupa Nicolai

00:00:08,250 --> 00:00:13,200
did and we're looking at what happens as

00:00:11,670 --> 00:00:16,520
you start making larger and larger

00:00:13,200 --> 00:00:18,600
deployments on a Linux operating system

00:00:16,520 --> 00:00:20,820
so we'll start off with getting into

00:00:18,600 --> 00:00:23,310
some of the examples of interface

00:00:20,820 --> 00:00:26,670
stacking that causes this explosion of

00:00:23,310 --> 00:00:28,650
network interfaces and look at some of

00:00:26,670 --> 00:00:31,890
the kernel side memory impacts some

00:00:28,650 --> 00:00:34,050
suggestions on what we could do to lower

00:00:31,890 --> 00:00:36,329
that that impact and then Rupa will talk

00:00:34,050 --> 00:00:40,680
about some user space impacts with with

00:00:36,329 --> 00:00:42,840
these number of interfaces so we start

00:00:40,680 --> 00:00:44,640
talking about the deployments that the

00:00:42,840 --> 00:00:47,489
first one gets into Ethernet bridging

00:00:44,640 --> 00:00:50,010
where you're connecting hosts at layer

00:00:47,489 --> 00:00:52,699
two you could take physical interfaces

00:00:50,010 --> 00:00:56,160
or bonds or logical interfaces like

00:00:52,699 --> 00:00:59,309
VLANs and plugging them together into

00:00:56,160 --> 00:01:06,080
bridges so that's the the basis of the

00:00:59,309 --> 00:01:08,670
first set of stacking examples all right

00:01:06,080 --> 00:01:10,590
so we start at the bottom and we've got

00:01:08,670 --> 00:01:13,140
our physical switch ports so that's what

00:01:10,590 --> 00:01:16,650
the swp is switch ports and those are

00:01:13,140 --> 00:01:20,729
our physical interfaces and you know

00:01:16,650 --> 00:01:23,580
often those those switch ports are put

00:01:20,729 --> 00:01:26,189
into bonds for redundancy those bonds

00:01:23,580 --> 00:01:28,170
are then have VLANs on top of them for

00:01:26,189 --> 00:01:32,310
example in this case it's behind 0 100

00:01:28,170 --> 00:01:33,810
and then the bonds the bond interfaces

00:01:32,310 --> 00:01:36,689
are plugged into a bridge for the

00:01:33,810 --> 00:01:39,930
connectivity with a Mac VLAN on top for

00:01:36,689 --> 00:01:41,909
virtual router redundancy so when you

00:01:39,930 --> 00:01:44,369
start off with the maximum ports and a

00:01:41,909 --> 00:01:48,329
switch say for example 128 switchboards

00:01:44,369 --> 00:01:49,470
that would convert into 64 bonds when

00:01:48,329 --> 00:01:52,619
you start talking about theoretical

00:01:49,470 --> 00:01:54,860
maximums you've got 4000 94 VLANs that

00:01:52,619 --> 00:01:57,930
you could put on these interfaces and

00:01:54,860 --> 00:02:00,540
then you plugging that into 4,000 94

00:01:57,930 --> 00:02:02,820
bridges with the MACD line interfaces

00:02:00,540 --> 00:02:05,070
per bridge so if you were to try to

00:02:02,820 --> 00:02:06,750
deploy a theoretical maximum number of

00:02:05,070 --> 00:02:08,369
interfaces you're getting into the

00:02:06,750 --> 00:02:10,319
hundreds of thousands of network

00:02:08,369 --> 00:02:13,200
interfaces that the operating system

00:02:10,319 --> 00:02:15,640
would have to deal with

00:02:13,200 --> 00:02:17,770
turning into the are turning to the the

00:02:15,640 --> 00:02:19,780
Newbridge model we don't have individual

00:02:17,770 --> 00:02:22,060
VLAN devices plugging into the bridge

00:02:19,780 --> 00:02:24,400
but rather the bonds themselves one to a

00:02:22,060 --> 00:02:28,180
bridge the bridge has a VLAN interface

00:02:24,400 --> 00:02:31,240
on it with the Mac VLANs though so in

00:02:28,180 --> 00:02:33,120
this case you're only talking 8,000 then

00:02:31,240 --> 00:02:36,490
interfaces from a theoretical maximum

00:02:33,120 --> 00:02:42,070
but still 8,000 interfaces is a lot to

00:02:36,490 --> 00:02:43,450
have to deal with the X land also is

00:02:42,070 --> 00:02:46,120
another one where you can very quickly

00:02:43,450 --> 00:02:47,680
explode the number of network interfaces

00:02:46,120 --> 00:02:49,900
that you're gonna have in your operating

00:02:47,680 --> 00:02:52,360
system for example the switch ports

00:02:49,900 --> 00:02:54,640
again adding in VLANs

00:02:52,360 --> 00:02:57,370
the maximum number of inter VLAN spur

00:02:54,640 --> 00:03:00,070
interface and then in this case you know

00:02:57,370 --> 00:03:02,980
we're just talking a meter mm X land

00:03:00,070 --> 00:03:06,070
devices and you quickly get into the 500

00:03:02,980 --> 00:03:08,920
thousand range for our network

00:03:06,070 --> 00:03:11,140
interfaces so clearly these are

00:03:08,920 --> 00:03:13,210
theoretical numbers of what could happen

00:03:11,140 --> 00:03:15,010
and not what we're doing today but we

00:03:13,210 --> 00:03:17,200
want to be out in front on this from a

00:03:15,010 --> 00:03:21,850
performance impact and from a resource

00:03:17,200 --> 00:03:24,190
impact turning to two layer three again

00:03:21,850 --> 00:03:26,350
starting with the physical interface of

00:03:24,190 --> 00:03:28,630
the switch boards each one of those

00:03:26,350 --> 00:03:31,630
switch ports could have the maximum

00:03:28,630 --> 00:03:35,080
number of VLANs per interface and then

00:03:31,630 --> 00:03:37,150
turning those those VLANs into into

00:03:35,080 --> 00:03:39,580
brf's devices for virtual routing and

00:03:37,150 --> 00:03:41,680
forwarding you very quickly again could

00:03:39,580 --> 00:03:44,680
be seeing into the hundreds of thousands

00:03:41,680 --> 00:03:46,870
of network interfaces so that's the

00:03:44,680 --> 00:03:49,660
conceptual problem of what we want to

00:03:46,870 --> 00:03:51,670
make sure we're out in front time so

00:03:49,660 --> 00:03:54,489
each one of these network interfaces

00:03:51,670 --> 00:03:56,860
that you would see you know in a Linux

00:03:54,489 --> 00:03:59,800
kernel these are all represented and at

00:03:56,860 --> 00:04:01,360
the kernel level by a net device so the

00:03:59,800 --> 00:04:03,760
Ethernet interfaces the VLAN sub

00:04:01,360 --> 00:04:07,590
interfaces bridges bonds all of these

00:04:03,760 --> 00:04:07,590
are modeled as what's called a net dev

00:04:08,340 --> 00:04:14,860
so when you create an interface in in

00:04:11,890 --> 00:04:17,769
lenox each IP link ad is consuming at

00:04:14,860 --> 00:04:19,239
least 43 kilobytes of memory you haven't

00:04:17,769 --> 00:04:21,430
done anything with the interface yet you

00:04:19,239 --> 00:04:22,590
just created it and getting ready to do

00:04:21,430 --> 00:04:24,960
something like plugging

00:04:22,590 --> 00:04:27,389
into a bridge or adding an IP address on

00:04:24,960 --> 00:04:29,850
it so where is that I where is that

00:04:27,389 --> 00:04:31,590
memory going well the net device

00:04:29,850 --> 00:04:33,389
structure itself is actually not that

00:04:31,590 --> 00:04:38,010
large it's only twenty nine hundred

00:04:33,389 --> 00:04:39,410
bytes but that gets rounded up to 48 64

00:04:38,010 --> 00:04:41,790
when it when you start talking about

00:04:39,410 --> 00:04:42,870
there are the requested size getting

00:04:41,790 --> 00:04:45,600
converted to something that the

00:04:42,870 --> 00:04:48,870
allocator wants to hand out and that's

00:04:45,600 --> 00:04:50,490
48 64 it's the net device plus the

00:04:48,870 --> 00:04:55,410
hardware addressing plus the queues that

00:04:50,490 --> 00:04:58,650
are getting set up the other what's at

00:04:55,410 --> 00:05:00,690
40 40 K bytes is getting taken up by

00:04:58,650 --> 00:05:02,729
things that are eating malloc runtime so

00:05:00,690 --> 00:05:04,979
for example the fact that you have a

00:05:02,729 --> 00:05:06,780
network interface is an object in the

00:05:04,979 --> 00:05:11,040
kernel and has an appearance and slash

00:05:06,780 --> 00:05:13,470
sis and creating all those directory

00:05:11,040 --> 00:05:17,550
entries and vile entries insists that's

00:05:13,470 --> 00:05:19,380
consuming another 14k right now

00:05:17,550 --> 00:05:23,250
automatically every time you create a

00:05:19,380 --> 00:05:25,410
net device the ipv4 ipv6 initializers

00:05:23,250 --> 00:05:28,620
are run and those things are allocating

00:05:25,410 --> 00:05:32,639
data structures preparing for addresses

00:05:28,620 --> 00:05:34,320
to come along so in some as you as you

00:05:32,639 --> 00:05:38,190
were creating devices you're consuming

00:05:34,320 --> 00:05:39,930
memory at a pretty fast clip so looking

00:05:38,190 --> 00:05:41,610
back at those traditional cases of those

00:05:39,930 --> 00:05:44,910
stacking cases that I talked about

00:05:41,610 --> 00:05:47,150
earlier when you start having 270,000

00:05:44,910 --> 00:05:50,520
interfaces you're using 11 gig of memory

00:05:47,150 --> 00:05:53,310
just preparing yourself to do something

00:05:50,520 --> 00:05:55,680
with networking devices so clearly

00:05:53,310 --> 00:05:58,830
something needs to give to make this

00:05:55,680 --> 00:06:00,510
much more efficient so if you're if

00:05:58,830 --> 00:06:04,860
you're having these these allocations

00:06:00,510 --> 00:06:06,900
and in memory consumption like this what

00:06:04,860 --> 00:06:09,330
are some options to start reducing to

00:06:06,900 --> 00:06:11,070
start getting this under control so one

00:06:09,330 --> 00:06:13,050
thing that's been done already is like

00:06:11,070 --> 00:06:13,560
wait tunnel devices are lightweight with

00:06:13,050 --> 00:06:15,930
lightweight

00:06:13,560 --> 00:06:18,720
tunnels so rather than creating a net

00:06:15,930 --> 00:06:21,570
device per tunnel the glues roopa that

00:06:18,720 --> 00:06:24,060
put this implementation and for putting

00:06:21,570 --> 00:06:25,620
metadata on or out so rather than

00:06:24,060 --> 00:06:28,620
creating a net device you just say well

00:06:25,620 --> 00:06:30,419
I just need this information to tell the

00:06:28,620 --> 00:06:31,979
the packets where they're going next and

00:06:30,419 --> 00:06:34,120
so we'll just put the metadata on them

00:06:31,979 --> 00:06:39,220
so that's reduced the memory over

00:06:34,120 --> 00:06:40,600
had for certain cases some time last

00:06:39,220 --> 00:06:43,300
summer someone had thrown out of passion

00:06:40,600 --> 00:06:45,280
Florian forget the last name Florian

00:06:43,300 --> 00:06:47,650
sent out a patch for an l2 only device

00:06:45,280 --> 00:06:50,020
which essentially kind of skipped some

00:06:47,650 --> 00:06:52,540
of the l3 initializations so that's

00:06:50,020 --> 00:06:54,970
certainly a start but as we saw on the

00:06:52,540 --> 00:06:57,070
last slide it's the Sisyphus stuff that

00:06:54,970 --> 00:06:59,560
it's also bringing in some some huge

00:06:57,070 --> 00:07:02,500
memory consumptions so I played around

00:06:59,560 --> 00:07:04,660
with some patches - what if what if you

00:07:02,500 --> 00:07:07,539
decided to drop slash Sisyphus as well

00:07:04,660 --> 00:07:09,789
or what if you decided to drop the dev

00:07:07,539 --> 00:07:13,590
the dev Kampf that comes along with ipv4

00:07:09,789 --> 00:07:17,080
and ipv6 okay is these is this a way to

00:07:13,590 --> 00:07:23,560
also further reduce some of the memory

00:07:17,080 --> 00:07:25,479
footprint so playing around with this

00:07:23,560 --> 00:07:28,150
concept let's just start cutting out

00:07:25,479 --> 00:07:30,370
some of these memory allocations like

00:07:28,150 --> 00:07:30,820
dropping the Sisyphus like dropping deaf

00:07:30,370 --> 00:07:33,070
Kampf

00:07:30,820 --> 00:07:35,470
so still doing ipv4 initialization still

00:07:33,070 --> 00:07:38,139
doing ipv6 initializations but dropping

00:07:35,470 --> 00:07:41,200
the sis control entries for example that

00:07:38,139 --> 00:07:45,220
gets replicated per device you can get

00:07:41,200 --> 00:07:47,710
that memory down from 40k 45k to 13 K or

00:07:45,220 --> 00:07:49,780
I guess that relation you 43k a 13 K so

00:07:47,710 --> 00:07:53,380
that's pretty much a third of the memory

00:07:49,780 --> 00:07:54,849
cost and if you combine that with any of

00:07:53,380 --> 00:07:56,770
these devices that you know will never

00:07:54,849 --> 00:07:58,960
have an address and you could you add in

00:07:56,770 --> 00:08:03,729
the l2 only impacts of it you can get

00:07:58,960 --> 00:08:05,650
that down to 48 96 that 48 96 is still

00:08:03,729 --> 00:08:07,810
roundups because of the way that the

00:08:05,650 --> 00:08:09,460
allocations are being done if you

00:08:07,810 --> 00:08:11,199
further go into the net device struct

00:08:09,460 --> 00:08:14,169
and do some optimizations to start

00:08:11,199 --> 00:08:16,419
cutting out some some overhead there I

00:08:14,169 --> 00:08:19,660
think that number can be reduced even

00:08:16,419 --> 00:08:23,410
further potentially down to the 2k range

00:08:19,660 --> 00:08:24,220
so - 2 kilobytes per net device so now

00:08:23,410 --> 00:08:25,750
we're starting to get to something

00:08:24,220 --> 00:08:27,940
that's a little more manageable a little

00:08:25,750 --> 00:08:29,650
more acceptable because we want in that

00:08:27,940 --> 00:08:32,409
device we want to be able to add IP

00:08:29,650 --> 00:08:34,930
filter rules or the TC rules or other

00:08:32,409 --> 00:08:37,120
kinds of routes is up hanging off of

00:08:34,930 --> 00:08:38,650
these these net devices you know using

00:08:37,120 --> 00:08:41,020
that net device as an anchor for some of

00:08:38,650 --> 00:08:43,450
these other facilities so now we can get

00:08:41,020 --> 00:08:44,640
the memory under control but there's a

00:08:43,450 --> 00:08:48,089
cost to that

00:08:44,640 --> 00:08:51,660
is it acceptable to not have individual

00:08:48,089 --> 00:08:53,880
sis control entries per net device well

00:08:51,660 --> 00:08:56,010
realistically speaking most of those

00:08:53,880 --> 00:08:58,800
devices if you can say my default

00:08:56,010 --> 00:09:01,200
settings are gonna work for 90% of my

00:08:58,800 --> 00:09:03,269
interfaces then I don't need to

00:09:01,200 --> 00:09:07,680
replicate the sis control entries for

00:09:03,269 --> 00:09:10,230
every one of them similar to similar to

00:09:07,680 --> 00:09:11,790
that is the the slash sis entries if you

00:09:10,230 --> 00:09:14,490
look at what's what's visible in slash

00:09:11,790 --> 00:09:16,110
says you have networking statistics you

00:09:14,490 --> 00:09:18,750
know this ability to just read a file to

00:09:16,110 --> 00:09:22,290
get one single counter or to make a

00:09:18,750 --> 00:09:23,130
change to one particular setting so well

00:09:22,290 --> 00:09:25,320
that's nice

00:09:23,130 --> 00:09:28,440
there's an overhead to it you know it's

00:09:25,320 --> 00:09:30,750
it's a fair chunk percentage-wise of

00:09:28,440 --> 00:09:33,329
where our our memory allocations are

00:09:30,750 --> 00:09:35,670
going so if you're fine with dropping

00:09:33,329 --> 00:09:37,589
the cysts and fixing whatever tools that

00:09:35,670 --> 00:09:39,680
may or may not be accessing /this and

00:09:37,589 --> 00:09:42,660
just using the artena link interface

00:09:39,680 --> 00:09:45,540
then again we can have considerable

00:09:42,660 --> 00:09:47,910
savings and where work overhead is for

00:09:45,540 --> 00:09:50,329
these interfaces and then bringing in

00:09:47,910 --> 00:09:54,060
again the l2 combining that with the l2

00:09:50,329 --> 00:09:56,610
stopped doing i net initializations ipv4

00:09:54,060 --> 00:09:58,769
ipv6 initializations just because we

00:09:56,610 --> 00:10:03,720
created in it and that device further

00:09:58,769 --> 00:10:08,300
brings this this overhead down so now

00:10:03,720 --> 00:10:08,300
Roopa will talk about user space impacts

00:10:09,949 --> 00:10:16,620
so the next few slides I talked about I

00:10:13,800 --> 00:10:20,940
look at the native scaling aspect from

00:10:16,620 --> 00:10:25,019
user space and in this slide we talk

00:10:20,940 --> 00:10:27,839
about some of the user space

00:10:25,019 --> 00:10:29,640
applications that run on a nas or this

00:10:27,839 --> 00:10:32,490
is a problem that a container OS or it

00:10:29,640 --> 00:10:35,399
can be a hypervisor as well so you have

00:10:32,490 --> 00:10:37,920
a lot of demons or protocol demons and

00:10:35,399 --> 00:10:39,870
network interface managers or some

00:10:37,920 --> 00:10:42,510
redundancy high availability demons all

00:10:39,870 --> 00:10:44,880
listening to network interfaces or all

00:10:42,510 --> 00:10:48,829
gathering network interface data to

00:10:44,880 --> 00:10:52,949
react to some changes of the network and

00:10:48,829 --> 00:10:54,990
another example is monitoring SNMP also

00:10:52,949 --> 00:10:57,870
always it usually

00:10:54,990 --> 00:11:00,630
and if you're talking about thousands of

00:10:57,870 --> 00:11:03,330
net devices in a system all these demons

00:11:00,630 --> 00:11:07,020
trying to get thousands of interface

00:11:03,330 --> 00:11:09,779
data every five second or so is yeah it

00:11:07,020 --> 00:11:11,790
doesn't scale and we have done a few

00:11:09,779 --> 00:11:14,670
words in work in application area as

00:11:11,790 --> 00:11:17,850
well to optimize some of these one

00:11:14,670 --> 00:11:20,820
example is Leben l i-- lebanon used

00:11:17,850 --> 00:11:23,130
linear walks for interfaces initially

00:11:20,820 --> 00:11:25,470
but then we submitted some patches to

00:11:23,130 --> 00:11:29,640
add hash table support because we could

00:11:25,470 --> 00:11:36,660
not scale for these thousands of

00:11:29,640 --> 00:11:40,350
interfaces here's some data about how

00:11:36,660 --> 00:11:43,140
much user space gets for a query on an

00:11:40,350 --> 00:11:45,330
interface for example there are some

00:11:43,140 --> 00:11:48,000
numbers for a single physical device or

00:11:45,330 --> 00:11:50,160
a logical device like a bridge and what

00:11:48,000 --> 00:11:53,040
user space gets today is tons of

00:11:50,160 --> 00:11:55,350
attributes about stats about address

00:11:53,040 --> 00:11:59,480
family attributes if the interface has

00:11:55,350 --> 00:12:04,890
an address ipv4 ipv6 and def confers

00:11:59,480 --> 00:12:09,000
david mentioned this is the CCT and CCC

00:12:04,890 --> 00:12:10,500
tilde dot and the kernel does have some

00:12:09,000 --> 00:12:12,450
infrastructure today to actually filter

00:12:10,500 --> 00:12:15,330
some of this data before it's sent to

00:12:12,450 --> 00:12:18,089
user space but that filtering is at a

00:12:15,330 --> 00:12:21,060
very high level you can say that don't

00:12:18,089 --> 00:12:23,790
give me stats i I just want half of the

00:12:21,060 --> 00:12:30,209
data not stats and you can filter at a

00:12:23,790 --> 00:12:32,880
very high level so an IP route to IP

00:12:30,209 --> 00:12:36,450
route 2 also does some filtering you for

00:12:32,880 --> 00:12:38,459
example if you have the interface scale

00:12:36,450 --> 00:12:39,750
that we are talking about 4000 or 5000

00:12:38,459 --> 00:12:42,870
and if you're doing an IP link shown a

00:12:39,750 --> 00:12:45,540
system you just can't consume that data

00:12:42,870 --> 00:12:47,760
even if you're looking at from a

00:12:45,540 --> 00:12:49,890
debugging perspective so there is

00:12:47,760 --> 00:12:52,260
ability in IP route 2 to actually filter

00:12:49,890 --> 00:12:53,910
that data when it's returned to use the

00:12:52,260 --> 00:12:57,630
space but all that filtering happens

00:12:53,910 --> 00:12:58,950
today in inside IP route 2 IP route 2 is

00:12:57,630 --> 00:12:59,399
asking for all the reader from the

00:12:58,950 --> 00:13:02,210
kernel

00:12:59,399 --> 00:13:05,660
every time you query

00:13:02,210 --> 00:13:07,700
and filter it in userspace and there is

00:13:05,660 --> 00:13:10,520
also the net link notification overload

00:13:07,700 --> 00:13:13,160
problem so net link notification is a

00:13:10,520 --> 00:13:16,490
mechanism that kernel indicates to user

00:13:13,160 --> 00:13:18,440
space about the changes in a network

00:13:16,490 --> 00:13:21,560
interface State and it's very critical

00:13:18,440 --> 00:13:24,140
for applications especially protocol

00:13:21,560 --> 00:13:28,340
demons or even for network interface

00:13:24,140 --> 00:13:31,940
monitoring daemons to know about this

00:13:28,340 --> 00:13:34,910
information and react to changes and

00:13:31,940 --> 00:13:38,180
changes in the network at scale this

00:13:34,910 --> 00:13:41,180
becomes a problem so for example I was

00:13:38,180 --> 00:13:43,040
doing some a bond creation can create a

00:13:41,180 --> 00:13:45,710
can send you about 10 or 20

00:13:43,040 --> 00:13:47,540
notifications because during a bond

00:13:45,710 --> 00:13:50,030
creation a slave goes through certain

00:13:47,540 --> 00:13:51,950
transitions it gets a master and then

00:13:50,030 --> 00:13:53,900
the bond comes up and the bond goes

00:13:51,950 --> 00:13:55,280
through several transitions and all this

00:13:53,900 --> 00:13:57,740
is notified to use a space by a

00:13:55,280 --> 00:13:59,240
notifications and you can imagine when

00:13:57,740 --> 00:14:00,770
you're provisioning thousands of

00:13:59,240 --> 00:14:03,110
interfaces and they're all coming up at

00:14:00,770 --> 00:14:07,610
once you have a notification storm and

00:14:03,110 --> 00:14:12,770
if you have a box or a nurse or where

00:14:07,610 --> 00:14:15,620
you are running 10 or 20 Network daemons

00:14:12,770 --> 00:14:18,220
you can easily see how all of them get

00:14:15,620 --> 00:14:21,350
busy at once trying to process the same

00:14:18,220 --> 00:14:23,330
same kind of information so again

00:14:21,350 --> 00:14:25,580
possible solutions to these problems are

00:14:23,330 --> 00:14:27,560
definitely reducing the number of net

00:14:25,580 --> 00:14:31,520
devices because all these scale linearly

00:14:27,560 --> 00:14:34,250
with as the number of net devices or and

00:14:31,520 --> 00:14:37,700
the other option is to send a given

00:14:34,250 --> 00:14:40,520
option to use a space to yeah give less

00:14:37,700 --> 00:14:43,910
data to use a space or add more

00:14:40,520 --> 00:14:48,230
filtering mechanisms another example to

00:14:43,910 --> 00:14:49,790
the right is an example where we if you

00:14:48,230 --> 00:14:51,830
create today a bridge wheel and

00:14:49,790 --> 00:14:54,110
filtering a VLAN filtering bridge and

00:14:51,830 --> 00:14:57,920
you add ports and again this is a worst

00:14:54,110 --> 00:15:02,840
case where you create false 4:09 for

00:14:57,920 --> 00:15:05,530
VLANs you can so the bridge the bridge

00:15:02,840 --> 00:15:08,210
driver adds a local FD BMAC for every

00:15:05,530 --> 00:15:10,240
port and this is not a problem or you

00:15:08,210 --> 00:15:12,649
don't see the problem if you have ten

00:15:10,240 --> 00:15:15,980
ten bridge ports

00:15:12,649 --> 00:15:18,980
as soon as you go to 4,000 Bridgeport's

00:15:15,980 --> 00:15:20,689
you will suddenly see this explored and

00:15:18,980 --> 00:15:23,059
you will as you can see the numbers

00:15:20,689 --> 00:15:24,050
there you will get notifications for

00:15:23,059 --> 00:15:29,839
every FTP

00:15:24,050 --> 00:15:32,779
local FTP entry that is added so the

00:15:29,839 --> 00:15:35,149
options to do all these problems

00:15:32,779 --> 00:15:36,829
definitely one higher level thing is to

00:15:35,149 --> 00:15:38,899
reduce the number of net devices there

00:15:36,829 --> 00:15:42,939
are multiple things that are going on as

00:15:38,899 --> 00:15:45,529
David said to do that in the kernel and

00:15:42,939 --> 00:15:48,230
there are some outstanding patches from

00:15:45,529 --> 00:15:50,209
David as well for filtering some of this

00:15:48,230 --> 00:15:57,709
information in the kernel before sending

00:15:50,209 --> 00:15:59,509
it to user space and again this is for

00:15:57,709 --> 00:16:01,579
filtering don't send statistics there is

00:15:59,509 --> 00:16:04,490
already some ability in the colonel told

00:16:01,579 --> 00:16:07,699
filter if the user space asks for it but

00:16:04,490 --> 00:16:10,699
then we are continuously sending david

00:16:07,699 --> 00:16:15,740
has some outstanding patches to filter

00:16:10,699 --> 00:16:19,730
in kernel again and this is another set

00:16:15,740 --> 00:16:22,490
of data where with 615 interfaces if you

00:16:19,730 --> 00:16:25,249
do i peeling show and you can see how

00:16:22,490 --> 00:16:26,929
much I think this was it with an S trace

00:16:25,249 --> 00:16:28,910
you can see how many receive message

00:16:26,929 --> 00:16:31,370
calls on the socket one has to make to

00:16:28,910 --> 00:16:33,649
actually get all the data for these many

00:16:31,370 --> 00:16:35,629
interfaces if you had filtering in the

00:16:33,649 --> 00:16:37,490
kernel that you told that is if you told

00:16:35,629 --> 00:16:39,980
the kernel to send you less data then

00:16:37,490 --> 00:16:46,220
you can grab the data more quickly in

00:16:39,980 --> 00:16:49,069
user space and network interface

00:16:46,220 --> 00:16:50,629
configuration at scale you can imagine

00:16:49,069 --> 00:16:52,759
so most of the network interface

00:16:50,629 --> 00:16:55,579
managers on Linux today they are flat

00:16:52,759 --> 00:16:58,579
files and you specify a network

00:16:55,579 --> 00:17:03,230
interface configuration in text in files

00:16:58,579 --> 00:17:08,409
or multiple files and at this scale this

00:17:03,230 --> 00:17:11,260
can explode for example we we used the

00:17:08,409 --> 00:17:15,439
Debian's network interface manager and

00:17:11,260 --> 00:17:17,329
we made some extensions to it so to deal

00:17:15,439 --> 00:17:19,640
with the scale problem because once you

00:17:17,329 --> 00:17:21,799
have if you have to define all these in

00:17:19,640 --> 00:17:24,529
a flat file the flag the file can

00:17:21,799 --> 00:17:25,820
explore it just grows exponentially as

00:17:24,529 --> 00:17:27,380
the number of interfaces

00:17:25,820 --> 00:17:29,510
so one thing what we have found useful

00:17:27,380 --> 00:17:33,080
is template eyes your interface

00:17:29,510 --> 00:17:35,750
configuration specification and an

00:17:33,080 --> 00:17:38,060
example to the right is an example from

00:17:35,750 --> 00:17:43,150
iff down to which is an interface

00:17:38,060 --> 00:17:47,960
manager which understands programmatic

00:17:43,150 --> 00:17:49,960
Python templates using miko and as you

00:17:47,960 --> 00:17:53,750
can see it to the right you can create

00:17:49,960 --> 00:17:56,360
like hundred devices with a single loop

00:17:53,750 --> 00:17:58,870
bridges it creates a VLAN devices for

00:17:56,360 --> 00:18:03,500
you it creates the bridge and so on and

00:17:58,870 --> 00:18:05,930
in a real real scenario this this

00:18:03,500 --> 00:18:07,940
interface configuration example actually

00:18:05,930 --> 00:18:11,090
has only two attributes the ports and

00:18:07,940 --> 00:18:12,920
the STP state but in real sorry this can

00:18:11,090 --> 00:18:15,590
actually go to approve 10 or 20

00:18:12,920 --> 00:18:18,740
attributes when you deal with MSTP

00:18:15,590 --> 00:18:23,180
configuration or IGMP configuration and

00:18:18,740 --> 00:18:30,280
so on on the port so yeah that's about

00:18:23,180 --> 00:18:30,280
it that was my last slide any questions

00:18:36,360 --> 00:18:41,340
if you have any question please raise

00:18:38,440 --> 00:18:41,340
your hand okay

00:18:59,310 --> 00:19:06,730
regarding the metrics shown there they

00:19:03,400 --> 00:19:09,930
mainly display the metrics regarding

00:19:06,730 --> 00:19:12,580
memory allocation did you also make some

00:19:09,930 --> 00:19:16,710
benchmarking or performance measurements

00:19:12,580 --> 00:19:19,870
of how much time does it take to

00:19:16,710 --> 00:19:25,840
initialize and bring up let's say a

00:19:19,870 --> 00:19:28,060
hundred thousand or so interfaces so

00:19:25,840 --> 00:19:29,530
yeah I have looked at that I did not

00:19:28,060 --> 00:19:31,660
include the data in there ace I could

00:19:29,530 --> 00:19:33,790
have thrown some more in maybe I can add

00:19:31,660 --> 00:19:35,850
that to the paper only submit it is to

00:19:33,790 --> 00:19:37,870
show when you're cutting out the cyst

00:19:35,850 --> 00:19:39,970
initializations which tend to be one of

00:19:37,870 --> 00:19:41,020
the larger overheads timewise as it's

00:19:39,970 --> 00:19:43,660
going through and creating all these

00:19:41,020 --> 00:19:45,370
entries in the sis file system by

00:19:43,660 --> 00:19:46,840
cutting out by you know using the

00:19:45,370 --> 00:19:49,420
lightweight net device option and

00:19:46,840 --> 00:19:52,450
cutting out the slash says cutting out

00:19:49,420 --> 00:19:58,960
some of these other allocations that the

00:19:52,450 --> 00:20:01,440
time does improve significantly any

00:19:58,960 --> 00:20:01,440
other question

00:20:10,520 --> 00:20:14,630
not a question but I can actually give

00:20:12,770 --> 00:20:17,900
you some data on the answer to that

00:20:14,630 --> 00:20:21,080
which is for about 40,000 devices on a

00:20:17,900 --> 00:20:25,580
fairly low CPU we went from somewhere in

00:20:21,080 --> 00:20:27,680
the order of thing an hour 16 minutes to

00:20:25,580 --> 00:20:31,180
eight minutes by removing just the slash

00:20:27,680 --> 00:20:31,180
since references

00:20:39,760 --> 00:20:44,740
okay

00:20:41,740 --> 00:20:44,740

YouTube URL: https://www.youtube.com/watch?v=_t41_dlBO7Q


