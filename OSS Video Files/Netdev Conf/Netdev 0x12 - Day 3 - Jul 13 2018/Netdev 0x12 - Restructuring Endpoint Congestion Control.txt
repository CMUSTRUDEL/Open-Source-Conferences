Title: Netdev 0x12 - Restructuring Endpoint Congestion Control
Publication date: 2018-08-01
Playlist: Netdev 0x12 - Day 3 - Jul 13 2018
Description: 
	From the same folks at MIT who brought you the idea of Congestion Manager (Linux being able to plugin different congestion control algorithms) came an exciting idea to bring even more modularity into Linux TCP. CM concepts that were harder to put into the kernel are now possible.

On July 13th, 2018 at Netdev 0x12 in Montreal, Akshay Narayan and Frank Cangialosi discussed Congestion Control Plane (CCP).
CCP is a new way of separating sender side TCP into control (sitting in user space) and datapath (sitting in the kernel). Control state from the kernel is used by algorithms in user space. User space algorithms use this information to control the kernel’s congestion window or pacing rate.

The talk described the details of the design principles used, kernel refactoring made, libccp which exposes user API, and experimental results from the implementation.

More info:
https://www.netdevconf.org/0x12/session.html?restructuring-endpoint-congestion-control
Captions: 
	00:00:00,000 --> 00:00:05,670
okay hi I'm Akshay and this is Frank

00:00:02,669 --> 00:00:08,069
were both from MIT and we've been

00:00:05,670 --> 00:00:10,019
working on a new way to write congestion

00:00:08,069 --> 00:00:13,710
control algorithms which I'm going to

00:00:10,019 --> 00:00:16,859
tell you about right now so this is

00:00:13,710 --> 00:00:18,449
basically how applications send data to

00:00:16,859 --> 00:00:20,789
the network today so congestion control

00:00:18,449 --> 00:00:22,619
as we just heard about is a fundamental

00:00:20,789 --> 00:00:24,150
problem in networking the question it

00:00:22,619 --> 00:00:27,599
answers is when should we send the next

00:00:24,150 --> 00:00:29,970
packet so historically a natural place

00:00:27,599 --> 00:00:32,040
to implement congestion control is in

00:00:29,970 --> 00:00:34,530
the transport layer and the two remain

00:00:32,040 --> 00:00:37,200
pretty tightly interwoven today so you

00:00:34,530 --> 00:00:39,989
have your application and you send your

00:00:37,200 --> 00:00:41,250
data to TCP and completely transparent

00:00:39,989 --> 00:00:42,960
to you TCP is running some congestion

00:00:41,250 --> 00:00:46,800
control algorithm whether it's bbr or

00:00:42,960 --> 00:00:49,230
cubic or or PCC vivace and that's how it

00:00:46,800 --> 00:00:49,770
decides when to send your data to the

00:00:49,230 --> 00:00:52,879
network

00:00:49,770 --> 00:00:55,050
however this architecture where you tie

00:00:52,879 --> 00:00:59,100
congestion control to the data path has

00:00:55,050 --> 00:01:01,620
three major shortcomings first TCP is no

00:00:59,100 --> 00:01:04,049
longer the only data path where I mean

00:01:01,620 --> 00:01:06,240
the network API by which the application

00:01:04,049 --> 00:01:07,920
sends data to the network I'm not just

00:01:06,240 --> 00:01:10,439
referencing software data paths like

00:01:07,920 --> 00:01:13,590
quick but also new hardware accelerated

00:01:10,439 --> 00:01:15,570
data pads created to aid CPUs in sending

00:01:13,590 --> 00:01:20,009
packets onto the wire as a network line

00:01:15,570 --> 00:01:21,780
Lori's having have scaled and second

00:01:20,009 --> 00:01:23,369
while data paths have been getting

00:01:21,780 --> 00:01:24,659
faster congestion control algorithms

00:01:23,369 --> 00:01:27,509
have been getting more and more

00:01:24,659 --> 00:01:30,240
sophisticated it's always been an active

00:01:27,509 --> 00:01:32,159
area of development and by the way the

00:01:30,240 --> 00:01:35,100
timeline on this slide is approximate

00:01:32,159 --> 00:01:36,030
and not to scale but algorithms proposed

00:01:35,100 --> 00:01:38,159
in the last few years

00:01:36,030 --> 00:01:40,829
use increasingly complex methods to

00:01:38,159 --> 00:01:42,869
detect the link capacity or across

00:01:40,829 --> 00:01:44,509
traffic characteristics and adapt their

00:01:42,869 --> 00:01:47,850
sending rates appropriately so we have

00:01:44,509 --> 00:01:49,619
you know sprout and Remy from more than

00:01:47,850 --> 00:01:51,450
five years ago or about five years ago

00:01:49,619 --> 00:01:53,549
where they use Bayesian forecasting and

00:01:51,450 --> 00:01:56,369
offline learning and then PCC which uses

00:01:53,549 --> 00:01:57,869
online learning indigo from this year

00:01:56,369 --> 00:01:59,490
which uses reinforcement learning which

00:01:57,869 --> 00:02:03,390
is trained on on the panthéon data set

00:01:59,490 --> 00:02:05,399
and our own work at MIT which is um uses

00:02:03,390 --> 00:02:07,079
signal processing techniques to detect

00:02:05,399 --> 00:02:10,289
and respond to cross traffic

00:02:07,079 --> 00:02:12,330
characteristics and basically what this

00:02:10,289 --> 00:02:13,650
explosion of development in congestion

00:02:12,330 --> 00:02:15,000
control leads to is what

00:02:13,650 --> 00:02:18,480
like to call the cross product of

00:02:15,000 --> 00:02:20,400
sadness so you have a congestion control

00:02:18,480 --> 00:02:21,900
developer who much must learn the ins

00:02:20,400 --> 00:02:25,079
and outs of each data path like how

00:02:21,900 --> 00:02:26,760
pacing works the ins and outs of cue

00:02:25,079 --> 00:02:28,220
disks and the various layers of cueing

00:02:26,760 --> 00:02:30,150
and how they interact with each other to

00:02:28,220 --> 00:02:32,700
successfully implement their algorithm

00:02:30,150 --> 00:02:35,250
then a new data path comes along written

00:02:32,700 --> 00:02:39,510
for different trade-offs or a different

00:02:35,250 --> 00:02:41,730
type of implementation and you have to

00:02:39,510 --> 00:02:43,500
start all over again you have to learn

00:02:41,730 --> 00:02:46,139
the quirks of the new data path and so

00:02:43,500 --> 00:02:47,940
on and implement your algorithm there so

00:02:46,139 --> 00:02:49,829
for example the bbr implementation on

00:02:47,940 --> 00:02:51,329
quic was started after the Linux

00:02:49,829 --> 00:02:52,829
implementation was already stable and

00:02:51,329 --> 00:02:56,280
yet it took several months to complete

00:02:52,829 --> 00:02:59,040
and similarly we just heard how the PCC

00:02:56,280 --> 00:03:01,560
vivace implementation they had some

00:02:59,040 --> 00:03:04,739
problems moving between the two data

00:03:01,560 --> 00:03:05,819
pets that they were using and finally

00:03:04,739 --> 00:03:07,200
the third problem is that tying

00:03:05,819 --> 00:03:08,939
congestion control to the data path

00:03:07,200 --> 00:03:10,769
makes evolving new capabilities

00:03:08,939 --> 00:03:13,049
difficult so there's a really old

00:03:10,769 --> 00:03:16,560
proposal called the congestion manager

00:03:13,049 --> 00:03:18,419
where you do per user congestion control

00:03:16,560 --> 00:03:20,549
instead of flow congestion control and

00:03:18,419 --> 00:03:23,790
manage congestion control across

00:03:20,549 --> 00:03:27,379
multiple flows at the same time and this

00:03:23,790 --> 00:03:29,879
is pretty hard to implement in Linux and

00:03:27,379 --> 00:03:32,909
basically required application changes

00:03:29,879 --> 00:03:34,019
in order to implement so to summarize

00:03:32,909 --> 00:03:35,760
this is what congestion control looks

00:03:34,019 --> 00:03:38,430
like today the application sends data to

00:03:35,760 --> 00:03:40,260
the data path and as the data data path

00:03:38,430 --> 00:03:42,389
receives feedback from the network it

00:03:40,260 --> 00:03:45,569
maintains statistics and exposes them

00:03:42,389 --> 00:03:48,030
via a custom API for example in Linux is

00:03:45,569 --> 00:03:51,900
the pluggable TCP API what we're

00:03:48,030 --> 00:03:54,479
proposing is that the data path that we

00:03:51,900 --> 00:03:56,310
should encapsulate congestion control in

00:03:54,479 --> 00:03:58,650
a separate component and decouple

00:03:56,310 --> 00:04:00,629
algorithm sophistication from the

00:03:58,650 --> 00:04:02,220
complexity of the data path we call this

00:04:00,629 --> 00:04:06,690
new architecture the congestion control

00:04:02,220 --> 00:04:08,519
plane or CCP the application here is in

00:04:06,690 --> 00:04:10,769
a separate process from the CCP agent

00:04:08,519 --> 00:04:12,510
and congestion control is move out of

00:04:10,769 --> 00:04:15,209
the kernel or out of the data path and

00:04:12,510 --> 00:04:17,280
into user space there are several

00:04:15,209 --> 00:04:20,010
advantages of this design so the first

00:04:17,280 --> 00:04:21,239
is that congestion control algorithm

00:04:20,010 --> 00:04:24,180
code is now in user space and we heard

00:04:21,239 --> 00:04:25,730
earlier from the quick folks how useful

00:04:24,180 --> 00:04:28,640
this is for them and

00:04:25,730 --> 00:04:31,070
they're similar we're making similar

00:04:28,640 --> 00:04:33,260
arguments in that respect second is what

00:04:31,070 --> 00:04:34,940
we call write once run anywhere so the

00:04:33,260 --> 00:04:36,410
same implementation of a congestion

00:04:34,940 --> 00:04:39,770
control algorithm can interface with

00:04:36,410 --> 00:04:41,450
multiple data pads and run congestion

00:04:39,770 --> 00:04:43,460
control and finally this new

00:04:41,450 --> 00:04:45,110
architecture enables new capabilities

00:04:43,460 --> 00:04:47,180
such as the congestion manager to be

00:04:45,110 --> 00:04:48,500
added pretty easily however this comes

00:04:47,180 --> 00:04:51,200
with an obvious trade-off there's no

00:04:48,500 --> 00:04:53,390
latency involved with receiving

00:04:51,200 --> 00:04:56,660
information from and sending enforcement

00:04:53,390 --> 00:04:58,460
decisions to the data path and we solved

00:04:56,660 --> 00:05:00,440
the problem in the pretty standard way

00:04:58,460 --> 00:05:02,450
of batching information so it's pretty

00:05:00,440 --> 00:05:04,340
reasonable to expect uncompromised

00:05:02,450 --> 00:05:06,320
performance because congestion control

00:05:04,340 --> 00:05:08,030
is now happening asynchronously so

00:05:06,320 --> 00:05:09,950
communication or congestion control

00:05:08,030 --> 00:05:12,380
decisions might happen maybe once or

00:05:09,950 --> 00:05:14,960
twice per RTT and not upon every packet

00:05:12,380 --> 00:05:17,210
and fortunately once or twice per RTT

00:05:14,960 --> 00:05:18,800
happens to be a pretty natural time

00:05:17,210 --> 00:05:21,680
scale for congestion control since once

00:05:18,800 --> 00:05:23,170
you send a packet it takes one RTT to

00:05:21,680 --> 00:05:25,640
figure out what happened to it and

00:05:23,170 --> 00:05:28,820
before I show you some evaluation

00:05:25,640 --> 00:05:32,110
results I'm like let Frank do a quick

00:05:28,820 --> 00:05:32,110
demo of our implementation

00:05:40,360 --> 00:05:43,110
right

00:05:43,759 --> 00:05:49,939
great okay okay so just yeah it's fun to

00:05:48,020 --> 00:05:52,430
give you a quick demo of our prototype

00:05:49,939 --> 00:05:53,749
of CCP we have running so just for the

00:05:52,430 --> 00:05:58,120
purpose of this demo I'm going to show

00:05:53,749 --> 00:06:02,360
you our implementation of PBR what's up

00:05:58,120 --> 00:06:04,939
great thank you oh come on font larger

00:06:02,360 --> 00:06:09,139
yeah sure okay

00:06:04,939 --> 00:06:13,370
that better a little larger

00:06:09,139 --> 00:06:14,719
okay so bbr has a number of phases and

00:06:13,370 --> 00:06:16,370
the implementation details are pretty

00:06:14,719 --> 00:06:18,529
complicated but just for those of you

00:06:16,370 --> 00:06:20,539
who aren't familiar with even the high

00:06:18,529 --> 00:06:23,719
level just to give you an overview of

00:06:20,539 --> 00:06:25,999
what to expect here it spends most of

00:06:23,719 --> 00:06:28,490
its time in a phase called pro bandwidth

00:06:25,999 --> 00:06:30,110
which does exactly as it sounds it's

00:06:28,490 --> 00:06:32,330
probing for more bandwidth and it does

00:06:30,110 --> 00:06:34,729
this by oscillating about a target rate

00:06:32,330 --> 00:06:36,589
between three fourths and five force of

00:06:34,729 --> 00:06:39,050
that target rate and then periodically

00:06:36,589 --> 00:06:41,930
it'll go into a probe RTT phase where it

00:06:39,050 --> 00:06:44,089
drains the cues cuts its rates to drain

00:06:41,930 --> 00:06:45,349
the cues and get a better estimate of

00:06:44,089 --> 00:06:48,589
the min or TT and then it will ramp back

00:06:45,349 --> 00:06:49,639
up so I just want to show you what the

00:06:48,589 --> 00:06:51,199
kernel implementation looks like and

00:06:49,639 --> 00:06:54,319
then we'll compare that with how our

00:06:51,199 --> 00:07:00,379
implementation looks so we have an

00:06:54,319 --> 00:07:01,999
emulator here get one side and I have a

00:07:00,379 --> 00:07:06,289
little visual visualization here of the

00:07:01,999 --> 00:07:07,909
throughput and delay on the bottom and

00:07:06,289 --> 00:07:11,810
what I'm going to do is just run a

00:07:07,909 --> 00:07:13,189
single DVR flow using iperf and show you

00:07:11,810 --> 00:07:15,199
what the throughput and delay look like

00:07:13,189 --> 00:07:16,399
over time and the bottleneck link

00:07:15,199 --> 00:07:19,639
characteristics are on the bottom there

00:07:16,399 --> 00:07:22,029
48 fps 50 milliseconds rqt and one pdp-8

00:07:19,639 --> 00:07:22,029
buffering

00:07:27,030 --> 00:07:32,110
alright so the animations a little

00:07:29,770 --> 00:07:35,050
jagged but just to give you an idea so

00:07:32,110 --> 00:07:36,520
you can see that the in the blue line

00:07:35,050 --> 00:07:38,770
there that's the delay you can see it

00:07:36,520 --> 00:07:40,990
oscillating about that target rate and

00:07:38,770 --> 00:07:43,690
then that periodic drop right there was

00:07:40,990 --> 00:07:45,250
the probe RTT phase and so this just

00:07:43,690 --> 00:07:46,840
gives you a general idea of what the

00:07:45,250 --> 00:07:48,660
kernel implantation looks like now in

00:07:46,840 --> 00:07:51,520
the very simple case of a single flow

00:07:48,660 --> 00:07:55,590
and then so now I'm going to show you

00:07:51,520 --> 00:07:55,590
what it looks like when you run with TCP

00:08:12,890 --> 00:08:17,990
alright so in addition to you know

00:08:16,050 --> 00:08:20,720
getting everything set up we have this

00:08:17,990 --> 00:08:23,760
additional step simply of running our

00:08:20,720 --> 00:08:25,070
userspace module so I have this all

00:08:23,760 --> 00:08:27,210
compiled and everything already but

00:08:25,070 --> 00:08:29,520
basically as a she's gonna describe in

00:08:27,210 --> 00:08:31,140
more detail later we have a user space

00:08:29,520 --> 00:08:32,460
library implemented and rest and all you

00:08:31,140 --> 00:08:34,680
have to do to implant your algorithm is

00:08:32,460 --> 00:08:37,380
import our library and add your

00:08:34,680 --> 00:08:39,120
functionality and so I've compiled it

00:08:37,380 --> 00:08:41,430
already so I'm gonna run that here and

00:08:39,120 --> 00:08:44,340
then we'll start iperf again but this

00:08:41,430 --> 00:08:52,770
time we'll tell it to use ccp instead of

00:08:44,340 --> 00:08:55,040
PBR let's make sure you get the receiver

00:08:52,770 --> 00:08:55,040
okay

00:08:55,480 --> 00:09:00,400
it's a little delayed but so we haven't

00:08:59,140 --> 00:09:01,450
implemented the startup face so it goes

00:09:00,400 --> 00:09:03,910
through a little bit of a slow startup

00:09:01,450 --> 00:09:05,590
but now you can see that it's doing

00:09:03,910 --> 00:09:07,120
about the about the same behavior so

00:09:05,590 --> 00:09:09,040
we're oscillating about that target rate

00:09:07,120 --> 00:09:11,410
and then there you see that the probe

00:09:09,040 --> 00:09:12,580
our Tiki drop so again I'll emphasize

00:09:11,410 --> 00:09:14,560
that this is a very like simple

00:09:12,580 --> 00:09:18,000
implementation of the high-level details

00:09:14,560 --> 00:09:22,470
of a PBR but this just kind of shows the

00:09:18,000 --> 00:09:24,880
capabilities and expressiveness of CCP

00:09:22,470 --> 00:09:27,430
so I'll hand it back over to Akshay to

00:09:24,880 --> 00:09:30,000
go through more of the details of how

00:09:27,430 --> 00:09:30,000
that get works

00:09:46,250 --> 00:09:52,980
great so we just saw the live result for

00:09:50,460 --> 00:09:55,380
DVR and this is cubic

00:09:52,980 --> 00:09:58,140
the CCP implementation and the kernel

00:09:55,380 --> 00:10:00,750
implementation on the same axis you can

00:09:58,140 --> 00:10:02,820
see that the CCP implementation matches

00:10:00,750 --> 00:10:06,480
the dynamics of what the kernel does

00:10:02,820 --> 00:10:10,320
pretty closely in terms of performance

00:10:06,480 --> 00:10:12,450
this experiment was you run a like

00:10:10,320 --> 00:10:14,640
localhost connections with the

00:10:12,450 --> 00:10:18,210
increasingly parallel a number of flows

00:10:14,640 --> 00:10:21,300
and the CCP implementations are

00:10:18,210 --> 00:10:22,980
configurable so you can decide if you

00:10:21,300 --> 00:10:25,950
want to that you're going to make a

00:10:22,980 --> 00:10:27,750
decision upon every act and get get

00:10:25,950 --> 00:10:29,160
feedback sent to user space and do the

00:10:27,750 --> 00:10:31,380
context which can make a decision and

00:10:29,160 --> 00:10:34,980
send your enforcement decision back or

00:10:31,380 --> 00:10:36,750
you can decide that you're going to make

00:10:34,980 --> 00:10:39,140
a decision only every say 10

00:10:36,750 --> 00:10:42,750
milliseconds or so and you can see that

00:10:39,140 --> 00:10:47,490
there is you get closer to what the

00:10:42,750 --> 00:10:49,440
kernel does when you make decisions when

00:10:47,490 --> 00:10:51,300
you avoid contact switches context

00:10:49,440 --> 00:10:54,240
switches by making decisions less

00:10:51,300 --> 00:10:56,760
frequently and what I think are the most

00:10:54,240 --> 00:10:59,160
exciting results are what we called

00:10:56,760 --> 00:11:01,110
write once run anywhere so these results

00:10:59,160 --> 00:11:03,000
hold across data paths so this is the

00:11:01,110 --> 00:11:05,700
same code for two different algorithms

00:11:03,000 --> 00:11:07,230
on the bottom is cubic and on the top is

00:11:05,700 --> 00:11:08,970
coppa which is a new delay based

00:11:07,230 --> 00:11:12,030
algorithm which is proposes a 10 SDI

00:11:08,970 --> 00:11:16,170
this year and you can see that on the

00:11:12,030 --> 00:11:17,820
left the two algorithms run on in the

00:11:16,170 --> 00:11:19,920
kernel and then in the middle on quick

00:11:17,820 --> 00:11:24,870
and then on the right on em tcp which is

00:11:19,920 --> 00:11:27,240
a DP DK based tcp implementation and the

00:11:24,870 --> 00:11:29,430
dynamics of congestion control a

00:11:27,240 --> 00:11:36,390
congestion window evolution match across

00:11:29,430 --> 00:11:37,680
all three data paths so for the rest of

00:11:36,390 --> 00:11:39,960
this talk I'm going to discuss CCP's

00:11:37,680 --> 00:11:41,370
congestion control API or at least our

00:11:39,960 --> 00:11:44,430
proposed version of what the API should

00:11:41,370 --> 00:11:46,680
be we need to support this asynchronous

00:11:44,430 --> 00:11:49,140
operation without comprised compromising

00:11:46,680 --> 00:11:52,110
on either algorithm correctness or

00:11:49,140 --> 00:11:53,580
performance because each packet contains

00:11:52,110 --> 00:11:55,650
valuable information we have to gather

00:11:53,580 --> 00:11:57,060
measurements per packet but we still

00:11:55,650 --> 00:11:59,220
want to make most congestion control

00:11:57,060 --> 00:12:00,899
decisions asynchronously so

00:11:59,220 --> 00:12:02,279
we split algorithm implementations into

00:12:00,899 --> 00:12:04,680
two components there's a slow path

00:12:02,279 --> 00:12:06,300
component which has access to user space

00:12:04,680 --> 00:12:09,029
libraries runs in user space but

00:12:06,300 --> 00:12:10,680
operates asynchronously and a data path

00:12:09,029 --> 00:12:12,750
or fast path component which has a

00:12:10,680 --> 00:12:15,810
constrained API but operates

00:12:12,750 --> 00:12:18,449
synchronously upon every feedback that

00:12:15,810 --> 00:12:20,370
is received so first of all overview the

00:12:18,449 --> 00:12:21,720
API for this asynchronous component as

00:12:20,370 --> 00:12:24,029
we saw earlier in the diagram we've

00:12:21,720 --> 00:12:27,509
refer to this component as the ccp agent

00:12:24,029 --> 00:12:29,459
it provides a runtime and api which

00:12:27,509 --> 00:12:30,899
algorithm developers use to specify the

00:12:29,459 --> 00:12:33,060
behavior of a congestion control

00:12:30,899 --> 00:12:35,459
algorithm our implementation is written

00:12:33,060 --> 00:12:39,509
in rust and exposes python bindings so

00:12:35,459 --> 00:12:42,540
i'll use python syntax to explain the

00:12:39,509 --> 00:12:44,250
api so there are two event handlers that

00:12:42,540 --> 00:12:45,899
you have to implement one is oncreate

00:12:44,250 --> 00:12:48,360
which is what should you do when a new

00:12:45,899 --> 00:12:49,620
flow arrives and on report which is what

00:12:48,360 --> 00:12:51,769
should you do when the data path sends

00:12:49,620 --> 00:12:53,970
you some new measurements to work with

00:12:51,769 --> 00:12:56,160
so I'll walk through an example on

00:12:53,970 --> 00:12:58,350
report function which implements a

00:12:56,160 --> 00:13:03,870
congestion avoidance mode so this code

00:12:58,350 --> 00:13:06,059
runs in the context of a ccp agent so

00:13:03,870 --> 00:13:07,379
first one thing you might want to do is

00:13:06,059 --> 00:13:09,509
take the information that you're given

00:13:07,379 --> 00:13:11,220
so the number of packets that were act

00:13:09,509 --> 00:13:14,329
and perform this additive increase

00:13:11,220 --> 00:13:16,589
update to your congressional window and

00:13:14,329 --> 00:13:19,079
next you can tell the data path to

00:13:16,589 --> 00:13:25,319
update the congestion window that you

00:13:19,079 --> 00:13:27,360
just decided to use okay for the data

00:13:25,319 --> 00:13:28,920
path component there there are two parts

00:13:27,360 --> 00:13:31,589
to it so the first interacts with the

00:13:28,920 --> 00:13:33,569
data pack data path so in the case of

00:13:31,589 --> 00:13:35,550
the kernel we use the pluggable tcp api

00:13:33,569 --> 00:13:37,860
to get access to the state that we need

00:13:35,550 --> 00:13:40,230
to collect measurements and enforce

00:13:37,860 --> 00:13:42,149
systems we also provide an execution

00:13:40,230 --> 00:13:43,350
environment for what we call data path

00:13:42,149 --> 00:13:45,779
programs which are the synchronous

00:13:43,350 --> 00:13:47,879
component of a ccp algorithm the data

00:13:45,779 --> 00:13:49,230
path includes a component called Lib CCP

00:13:47,879 --> 00:13:52,529
which interprets these data path

00:13:49,230 --> 00:13:54,600
programs to produce these enforcement

00:13:52,529 --> 00:13:56,730
decisions and measurements so this

00:13:54,600 --> 00:13:58,860
second part which the red arrow is

00:13:56,730 --> 00:14:00,959
pointing to is portable across data path

00:13:58,860 --> 00:14:04,079
so we took the same code and we use it

00:14:00,959 --> 00:14:08,670
inside the Linux TCP data path inside

00:14:04,079 --> 00:14:10,529
quick and inside M TCP and the way you

00:14:08,670 --> 00:14:12,490
define a data path program is in user

00:14:10,529 --> 00:14:14,529
space you define it using it

00:14:12,490 --> 00:14:15,610
specific language which we wrote the

00:14:14,529 --> 00:14:19,660
language has a pretty straightforward

00:14:15,610 --> 00:14:22,779
simple syntax and is probably more

00:14:19,660 --> 00:14:24,610
restrictive than developing so you can't

00:14:22,779 --> 00:14:27,490
you can do simple arithmetic

00:14:24,610 --> 00:14:28,870
computations and it's intended to merely

00:14:27,490 --> 00:14:32,260
gather information from the data path

00:14:28,870 --> 00:14:37,120
and report it so these programs are sent

00:14:32,260 --> 00:14:39,160
down to the data path using IPC and we

00:14:37,120 --> 00:14:40,839
currently use netlink but you can

00:14:39,160 --> 00:14:45,040
imagine using other things as well so

00:14:40,839 --> 00:14:46,029
for quick we use UNIX domain sockets so

00:14:45,040 --> 00:14:47,470
there are a number of predefined

00:14:46,029 --> 00:14:49,839
variables that the program can access

00:14:47,470 --> 00:14:51,520
these correspond directly to congestion

00:14:49,839 --> 00:14:54,310
control state stored in the data paths

00:14:51,520 --> 00:14:56,770
flow context and exposed to the data

00:14:54,310 --> 00:14:58,450
path program by the shim layer that runs

00:14:56,770 --> 00:15:01,149
inside the data path so this example

00:14:58,450 --> 00:15:03,430
will program on the first line defines a

00:15:01,149 --> 00:15:05,770
report structure and inside that report

00:15:03,430 --> 00:15:08,290
structure a variable called act which is

00:15:05,770 --> 00:15:11,890
the number of consecutively act packets

00:15:08,290 --> 00:15:14,320
that we that we saw and on the first

00:15:11,890 --> 00:15:16,420
Clause the the when true that says that

00:15:14,320 --> 00:15:19,600
this event handler should fire every

00:15:16,420 --> 00:15:21,610
time that the we got more feedback and

00:15:19,600 --> 00:15:24,070
when we do get more feedback we

00:15:21,610 --> 00:15:26,470
increment this act counter by the number

00:15:24,070 --> 00:15:30,250
of Q middle cumulative cumulative lis

00:15:26,470 --> 00:15:33,850
act packets and in the last two clauses

00:15:30,250 --> 00:15:36,279
we see that when they're when an RTT has

00:15:33,850 --> 00:15:38,649
elapsed or we saw some lost packets we

00:15:36,279 --> 00:15:40,870
should send this predefined report

00:15:38,649 --> 00:15:46,060
structure to the ccp agent and user

00:15:40,870 --> 00:15:48,459
space the data path program can also do

00:15:46,060 --> 00:15:50,050
some pretty simple modification to the

00:15:48,459 --> 00:15:53,920
enforcement so you can set the Seawind

00:15:50,050 --> 00:15:55,570
so for example here we did a one-line

00:15:53,920 --> 00:16:00,820
addition in our when true clause and

00:15:55,570 --> 00:16:03,610
implemented slow start so it's not just

00:16:00,820 --> 00:16:06,310
act dot bytes act this is the full list

00:16:03,610 --> 00:16:07,899
of congestion control signals that we

00:16:06,310 --> 00:16:09,550
support for use in data path programs

00:16:07,899 --> 00:16:12,370
and a description of how we define them

00:16:09,550 --> 00:16:14,709
in terms of the Linux TCP data path we

00:16:12,370 --> 00:16:16,510
we view this list as sort of an evolving

00:16:14,709 --> 00:16:18,250
standard but I'd like to point out that

00:16:16,510 --> 00:16:21,130
new congestion control signals don't

00:16:18,250 --> 00:16:25,209
come along that often and all of these

00:16:21,130 --> 00:16:26,110
except for the last two were in Linux

00:16:25,209 --> 00:16:29,170
for a really long

00:16:26,110 --> 00:16:36,700
and the last two are now in net next as

00:16:29,170 --> 00:16:39,610
of two days ago so moving forward we'd

00:16:36,700 --> 00:16:42,459
like to see CCP distributed as an entry

00:16:39,610 --> 00:16:44,110
kernel module once we figure out some

00:16:42,459 --> 00:16:46,480
minor issues we've had with hardening at

00:16:44,110 --> 00:16:49,180
further scale we're more than happy to

00:16:46,480 --> 00:16:51,040
work with CC congestion control

00:16:49,180 --> 00:16:54,519
algorithm developers to implement their

00:16:51,040 --> 00:16:56,200
algorithms using our library our current

00:16:54,519 --> 00:16:59,589
data path component implementations

00:16:56,200 --> 00:17:01,750
which we saw we're used to generate the

00:16:59,589 --> 00:17:04,929
graphs on this in this talk they're all

00:17:01,750 --> 00:17:06,250
on github as is the CCP agent and a few

00:17:04,929 --> 00:17:10,919
algorithm implications we've currently

00:17:06,250 --> 00:17:14,380
implemented cubic reno bbr copa and and

00:17:10,919 --> 00:17:16,150
Nimbus which is our own one caveat is

00:17:14,380 --> 00:17:17,760
the quick data path implementation we're

00:17:16,150 --> 00:17:19,870
not sure how to distribute our patch

00:17:17,760 --> 00:17:22,449
without posting all of chromium

00:17:19,870 --> 00:17:24,250
ourselves and I'm happy to talk to

00:17:22,449 --> 00:17:26,860
people about how to the best way to

00:17:24,250 --> 00:17:37,090
distribute that afterwards thanks and

00:17:26,860 --> 00:17:40,419
happy to take questions questions when

00:17:37,090 --> 00:17:42,370
you're talking about the good boot as

00:17:40,419 --> 00:17:45,700
you increase the number of flows for was

00:17:42,370 --> 00:17:47,799
the RTT for those flows this is a local

00:17:45,700 --> 00:17:52,600
host connection so the RTT is basically

00:17:47,799 --> 00:17:56,590
circle okay so when we when we run it on

00:17:52,600 --> 00:17:59,950
a testbed cluster of 10g and measure CP

00:17:56,590 --> 00:18:02,049
utilization we find that the context

00:17:59,950 --> 00:18:04,620
switch overhead was maybe about five to

00:18:02,049 --> 00:18:07,270
eight percent of extra CPU utilization

00:18:04,620 --> 00:18:10,419
so how much would disappear you overhead

00:18:07,270 --> 00:18:12,220
sorry how much was the CPU overhead like

00:18:10,419 --> 00:18:14,350
five to eight percent to 8 percent yeah

00:18:12,220 --> 00:18:15,809
okay and I think people have already

00:18:14,350 --> 00:18:18,549
mentioned to you that you could also use

00:18:15,809 --> 00:18:20,200
BPF to have their performance

00:18:18,549 --> 00:18:23,200
antibiotics yeah and that's a great

00:18:20,200 --> 00:18:28,690
point I actually have an extra slide for

00:18:23,200 --> 00:18:30,130
this so we diverged from EBP F on in two

00:18:28,690 --> 00:18:31,630
fronts so the front end which is the

00:18:30,130 --> 00:18:33,520
language and the and the back end on the

00:18:31,630 --> 00:18:37,480
data path so I think that the data path

00:18:33,520 --> 00:18:39,850
part is is the more important part so we

00:18:37,480 --> 00:18:41,620
when we looked into EVP F we

00:18:39,850 --> 00:18:43,750
found that doing congestion control

00:18:41,620 --> 00:18:45,220
enforcement like setting pacing rates

00:18:43,750 --> 00:18:47,110
and doing indirectly changing the

00:18:45,220 --> 00:18:49,870
conduction window is a little bit hard

00:18:47,110 --> 00:18:53,740
and more importantly what we we really

00:18:49,870 --> 00:18:55,660
cared about having the datapath

00:18:53,740 --> 00:18:57,430
component of ccp be portable across the

00:18:55,660 --> 00:19:00,610
iPads so we would have had to implement

00:18:57,430 --> 00:19:03,610
a BPF runtime in you know in quick and

00:19:00,610 --> 00:19:05,710
in EM TCP and no no I mean you you could

00:19:03,610 --> 00:19:08,500
extend the kernel to have that component

00:19:05,710 --> 00:19:10,900
but rather than calling your user level

00:19:08,500 --> 00:19:13,840
program to to make decisions you were

00:19:10,900 --> 00:19:16,510
called it in BPF so that would be much

00:19:13,840 --> 00:19:17,680
less overhead for that yeah but then you

00:19:16,510 --> 00:19:19,240
lose the flexibility of running

00:19:17,680 --> 00:19:21,130
arbitrary you're making arbitrary

00:19:19,240 --> 00:19:23,350
decisions in user space right you'd have

00:19:21,130 --> 00:19:30,180
to be restricted to what ebf can do ie

00:19:23,350 --> 00:19:32,590
BPF can do just a very quick question hi

00:19:30,180 --> 00:19:36,310
thank you for the nice presentation and

00:19:32,590 --> 00:19:38,740
demo actually do you it's the only way

00:19:36,310 --> 00:19:40,270
how you manipulate the sending rate just

00:19:38,740 --> 00:19:43,590
the congestion window or do you also

00:19:40,270 --> 00:19:47,380
have some kind of time-based pacing

00:19:43,590 --> 00:19:49,300
whatever influence the mechanism what do

00:19:47,380 --> 00:19:51,250
you mean by time-based pacing like the

00:19:49,300 --> 00:19:52,660
interface I would like is telling just

00:19:51,250 --> 00:19:56,350
telling you when you should send out the

00:19:52,660 --> 00:19:58,150
next packet yeah so we currently haven't

00:19:56,350 --> 00:20:00,070
implemented this but I'm happy to talk

00:19:58,150 --> 00:20:04,050
about how we could do that I think

00:20:00,070 --> 00:20:04,050
that's important for a future use cases

00:20:06,000 --> 00:20:12,850
yeah yeah so I mean you can set like a

00:20:09,520 --> 00:20:15,790
so pacing rate and you can do that like

00:20:12,850 --> 00:20:18,070
you can say after five microseconds I

00:20:15,790 --> 00:20:19,630
want a so pacing rate to be X and after

00:20:18,070 --> 00:20:22,360
10 microseconds I want a so piercing

00:20:19,630 --> 00:20:24,070
rate to be Y and so on but controlling

00:20:22,360 --> 00:20:26,140
exactly like you can't do like plus on

00:20:24,070 --> 00:20:36,840
sending of practice we don't support

00:20:26,140 --> 00:20:39,700
that yet Erik you can go next okay so

00:20:36,840 --> 00:20:41,920
how frequent is the communication

00:20:39,700 --> 00:20:44,200
between the data and the control path is

00:20:41,920 --> 00:20:46,930
it for every a queue trigger an app call

00:20:44,200 --> 00:20:48,490
this is configurable so it depends on

00:20:46,930 --> 00:20:49,870
the implementation of the congestion

00:20:48,490 --> 00:20:51,790
control over them but you can you can

00:20:49,870 --> 00:20:52,670
make it be whatever you want okay so

00:20:51,790 --> 00:20:54,680
like

00:20:52,670 --> 00:20:57,980
can your implementation wrong se-young

00:20:54,680 --> 00:21:00,320
40 Gbps snake what kind of super so the

00:20:57,980 --> 00:21:02,510
fastest we've tested it is 10 G we have

00:21:00,320 --> 00:21:04,790
we don't have access to a 40 G neck okay

00:21:02,510 --> 00:21:07,340
so for a 10 G then what's the support

00:21:04,790 --> 00:21:10,130
you can reach yeah so we can saturate 10

00:21:07,340 --> 00:21:12,580
G okay yeah that's very good and also in

00:21:10,130 --> 00:21:15,080
your last slides there is a cluster

00:21:12,580 --> 00:21:17,300
aggregation CCP mm-hmm

00:21:15,080 --> 00:21:18,920
why is that yeah so this is the idea of

00:21:17,300 --> 00:21:22,010
the congestion manager this is something

00:21:18,920 --> 00:21:23,270
that we haven't we're like looking into

00:21:22,010 --> 00:21:27,350
an outward we're just starting to

00:21:23,270 --> 00:21:29,450
implement this is the idea of performing

00:21:27,350 --> 00:21:31,580
congestion control operations on groups

00:21:29,450 --> 00:21:35,270
of flows together okay like all the

00:21:31,580 --> 00:21:37,730
flows in inside the host yeah inside a

00:21:35,270 --> 00:21:39,170
host or even in a cluster and the

00:21:37,730 --> 00:21:41,120
congestion control is happening like

00:21:39,170 --> 00:21:42,020
remotely on another machine okay yeah

00:21:41,120 --> 00:21:49,880
very interesting

00:21:42,020 --> 00:21:51,290
thank you yeah is Eric next Rick you are

00:21:49,880 --> 00:21:55,340
go you fine

00:21:51,290 --> 00:21:58,610
yeah just sure that you are going to

00:21:55,340 --> 00:22:01,930
test stuff with 10 million TCP active

00:21:58,610 --> 00:22:05,330
flows because this is the target we are

00:22:01,930 --> 00:22:12,500
we have on Google servers but at least

00:22:05,330 --> 00:22:14,180
the GFE servers know yeah yeah making it

00:22:12,500 --> 00:22:15,260
scale to to millions of flows is

00:22:14,180 --> 00:22:20,060
something that we're working on right

00:22:15,260 --> 00:22:24,200
now thank you for president Asian I

00:22:20,060 --> 00:22:25,610
actually really like the idea but I

00:22:24,200 --> 00:22:26,690
can't squared away everything in my head

00:22:25,610 --> 00:22:28,880
so I let her think about this a little

00:22:26,690 --> 00:22:30,770
bit longer because I feel like there are

00:22:28,880 --> 00:22:33,530
a lot of missing pieces the presentation

00:22:30,770 --> 00:22:36,890
is obviously an overview but there's

00:22:33,530 --> 00:22:38,840
also something that's not clear to me a

00:22:36,890 --> 00:22:40,700
couple of things actually one it's not

00:22:38,840 --> 00:22:43,370
clear to me how the kernel recipe

00:22:40,700 --> 00:22:45,620
interacts with this with this model with

00:22:43,370 --> 00:22:47,120
this module what's the what is the API

00:22:45,620 --> 00:22:49,370
that you are using right now for kernel

00:22:47,120 --> 00:22:51,590
TCP to interact with this not the API

00:22:49,370 --> 00:22:54,260
but the what what is the mechanism you

00:22:51,590 --> 00:22:56,060
are using so we use so we implement

00:22:54,260 --> 00:22:59,630
support for kernel using the pluggable

00:22:56,060 --> 00:23:00,980
TCP API the pluggable tcp/ip are so we

00:22:59,630 --> 00:23:02,690
write we wrote a kernel module which

00:23:00,980 --> 00:23:05,040
implements a pluggable TCP congestion

00:23:02,690 --> 00:23:10,440
control algorithm which

00:23:05,040 --> 00:23:12,510
various uh you know these fields in the

00:23:10,440 --> 00:23:15,150
TCP sock structure and in the rates rate

00:23:12,510 --> 00:23:17,580
sample structure and exposes them to the

00:23:15,150 --> 00:23:21,870
data path program execution environment

00:23:17,580 --> 00:23:23,580
I see I thought

00:23:21,870 --> 00:23:26,220
Jenna's question might be how is the

00:23:23,580 --> 00:23:29,490
kernel part up calling into user space

00:23:26,220 --> 00:23:31,650
to do the yeah that also happens via IPC

00:23:29,490 --> 00:23:34,230
so we would kind of eyepiece so

00:23:31,650 --> 00:23:35,160
currently it's not linked I think we

00:23:34,230 --> 00:23:38,910
also have a character device

00:23:35,160 --> 00:23:40,590
implementation it's flexible um one of

00:23:38,910 --> 00:23:42,560
the concerns maybe I actually touched on

00:23:40,590 --> 00:23:44,730
this earlier

00:23:42,560 --> 00:23:46,410
Ian talked about in in the quick

00:23:44,730 --> 00:23:47,790
presentation we talked we touched on

00:23:46,410 --> 00:23:50,160
basically why it's difficult for us to

00:23:47,790 --> 00:23:53,730
do pacing currently with the model that

00:23:50,160 --> 00:23:58,800
FQ uses in the system so something like

00:23:53,730 --> 00:24:03,150
that is points so the problem there is

00:23:58,800 --> 00:24:05,190
that in the kernel TCP FQ assumes that a

00:24:03,150 --> 00:24:07,890
socket is one connection whereas it the

00:24:05,190 --> 00:24:10,380
the quick model one socket can map to

00:24:07,890 --> 00:24:12,090
many connections something like that

00:24:10,380 --> 00:24:14,660
points to assumptions in the underlying

00:24:12,090 --> 00:24:17,100
model that is something like if Q makes

00:24:14,660 --> 00:24:18,810
and that's what I'm trying to tease

00:24:17,100 --> 00:24:20,940
apart in my head I'm trying to figure

00:24:18,810 --> 00:24:22,590
out what are the assumptions in the

00:24:20,940 --> 00:24:23,910
underlying model in the underlying world

00:24:22,590 --> 00:24:26,550
of condition control that you're making

00:24:23,910 --> 00:24:28,790
to arrive at this API right because I

00:24:26,550 --> 00:24:32,220
wonder if this API is going to be

00:24:28,790 --> 00:24:34,110
adequate for for other condition

00:24:32,220 --> 00:24:35,520
controllers at the same time I also

00:24:34,110 --> 00:24:37,740
wonder if what you have in there is

00:24:35,520 --> 00:24:39,930
going to be adequate for like you were

00:24:37,740 --> 00:24:42,180
pointing out quick as well as TCP for

00:24:39,930 --> 00:24:45,420
now just to finish my thought if TCP

00:24:42,180 --> 00:24:48,330
uses FQ for pacing and quit doesn't use

00:24:45,420 --> 00:24:50,700
FQ for pacing I can't square away how

00:24:48,330 --> 00:24:52,710
your module is going to be able to allow

00:24:50,700 --> 00:24:54,990
for both of those things to exist so the

00:24:52,710 --> 00:24:56,580
back end of how the enforcement happens

00:24:54,990 --> 00:24:59,930
and how the measurements are collected

00:24:56,580 --> 00:25:03,240
is going to be data pat specific so in

00:24:59,930 --> 00:25:05,880
the Linux kernel we use a so pacing rate

00:25:03,240 --> 00:25:07,470
in quick we might use some other data

00:25:05,880 --> 00:25:10,350
path specific quick specific way of

00:25:07,470 --> 00:25:11,520
pacing so like obviously with different

00:25:10,350 --> 00:25:14,850
data paths there going to be different

00:25:11,520 --> 00:25:16,500
ways of doing enforcement and we're

00:25:14,850 --> 00:25:18,800
going to need a shim layer per data path

00:25:16,500 --> 00:25:20,690
that that interprets the command

00:25:18,800 --> 00:25:23,660
that their user spaces sending into the

00:25:20,690 --> 00:25:25,870
data path and and carries out those

00:25:23,660 --> 00:25:28,040
enforcement in a datapad specific way

00:25:25,870 --> 00:25:29,930
fair enough so you might have to end up

00:25:28,040 --> 00:25:31,730
implementing something like piecing yes

00:25:29,930 --> 00:25:34,520
well inside of the control yeah and I

00:25:31,730 --> 00:25:37,700
think our our quick data paths our quick

00:25:34,520 --> 00:25:39,770
patch ear uses something that's already

00:25:37,700 --> 00:25:41,150
in quick I'm not too familiar with it

00:25:39,770 --> 00:25:43,130
I'd have to check but I can get back to

00:25:41,150 --> 00:25:44,690
you just one thing to be very off is

00:25:43,130 --> 00:25:46,160
that one of the nice things about having

00:25:44,690 --> 00:25:48,530
multiple implementations of a protocol

00:25:46,160 --> 00:25:50,330
is of course the standard arguments

00:25:48,530 --> 00:25:51,860
apply that you can read out bugs you

00:25:50,330 --> 00:25:53,870
have multiple implementations so there's

00:25:51,860 --> 00:25:55,400
no single point of failure you will be

00:25:53,870 --> 00:25:57,530
creating one of those things when you

00:25:55,400 --> 00:25:58,760
have something like this which is not

00:25:57,530 --> 00:26:01,670
necessarily a bad thing but just

00:25:58,760 --> 00:26:03,290
something to be very aware of but I do

00:26:01,670 --> 00:26:05,650
like the idea quite a bit thank you

00:26:03,290 --> 00:26:07,940
thanks

00:26:05,650 --> 00:26:10,910
yeah actually Jenna left out a cool

00:26:07,940 --> 00:26:12,620
story about the benefits of diversity of

00:26:10,910 --> 00:26:15,890
congestion control where the quickfix

00:26:12,620 --> 00:26:17,810
and when they re implemented cubic they

00:26:15,890 --> 00:26:20,360
found a bug that had been in there for a

00:26:17,810 --> 00:26:23,300
decade that calls it that was a very

00:26:20,360 --> 00:26:24,920
profound bug and actually once we once

00:26:23,300 --> 00:26:27,440
those guys found it and we fixed it in

00:26:24,920 --> 00:26:29,810
the TCP in YouTube we like cut the loss

00:26:27,440 --> 00:26:31,370
rate measure like visibly and all of our

00:26:29,810 --> 00:26:34,280
graphs so that there's some cool

00:26:31,370 --> 00:26:37,160
benefits from diversity this is actually

00:26:34,280 --> 00:26:38,960
more of a minor point but um on the of

00:26:37,160 --> 00:26:42,500
the cool demo by the way and thanks for

00:26:38,960 --> 00:26:44,900
picking bbr uh-huh

00:26:42,500 --> 00:26:47,810
do you know what the pacing mechanism

00:26:44,900 --> 00:26:49,880
was on the machines that were was the FQ

00:26:47,810 --> 00:26:53,230
pacing or the internal things a demo you

00:26:49,880 --> 00:26:59,120
just in stock pacing is sacrificing okay

00:26:53,230 --> 00:27:02,870
okay cool yeah this is definitely just a

00:26:59,120 --> 00:27:05,540
minor point but I would look try to

00:27:02,870 --> 00:27:07,130
maybe update to a more recent kernel or

00:27:05,540 --> 00:27:11,870
backports and fixes we had some recent

00:27:07,130 --> 00:27:13,820
fixes that affect the internal pacing

00:27:11,870 --> 00:27:16,430
behavior of VBR I noticed that in a demo

00:27:13,820 --> 00:27:18,620
actually the latency sort of went up and

00:27:16,430 --> 00:27:20,120
hovered around a high level and with I

00:27:18,620 --> 00:27:21,650
guess that was a single flow right yeah

00:27:20,120 --> 00:27:24,440
it's just yeah for a single fo bv ours

00:27:21,650 --> 00:27:26,300
should is almost always good at sort of

00:27:24,440 --> 00:27:28,420
keeping the latency down around the the

00:27:26,300 --> 00:27:33,130
two-way propagation delay so

00:27:28,420 --> 00:27:34,360
I'll email you the patch to look at for

00:27:33,130 --> 00:27:38,830
fixing that yeah that'd be great

00:27:34,360 --> 00:27:40,420
Thanks one more comment about that it

00:27:38,830 --> 00:27:43,780
would be nice for you to have a metric

00:27:40,420 --> 00:27:45,310
where you compare the behavior of the

00:27:43,780 --> 00:27:46,960
protocols when they are in Colonel

00:27:45,310 --> 00:27:48,640
versus your image yeah you mentioned you

00:27:46,960 --> 00:27:51,490
are a synchronous so you're comparing a

00:27:48,640 --> 00:27:53,080
good book and a pro coke we had the same

00:27:51,490 --> 00:27:54,730
good good and just have like three times

00:27:53,080 --> 00:27:56,530
more glasses right but it may be enough

00:27:54,730 --> 00:27:58,030
it would be it would be great to talk

00:27:56,530 --> 00:27:59,590
about this offline we because we've

00:27:58,030 --> 00:28:01,870
thought about this a lot and we haven't

00:27:59,590 --> 00:28:03,190
really found a good metric for like a

00:28:01,870 --> 00:28:04,930
single like something that we can

00:28:03,190 --> 00:28:06,730
condense down and say like this is the

00:28:04,930 --> 00:28:08,230
score for the fidelity of this

00:28:06,730 --> 00:28:09,580
implementation other than other than

00:28:08,230 --> 00:28:10,360
looking at two graphs in parallel I

00:28:09,580 --> 00:28:16,720
think it would be great to hear

00:28:10,360 --> 00:28:18,940
suggestions sure it's not entirely clear

00:28:16,720 --> 00:28:20,560
to me what's the execution context for

00:28:18,940 --> 00:28:22,990
you do you have programs so that's

00:28:20,560 --> 00:28:25,180
something that you're you're taking this

00:28:22,990 --> 00:28:26,230
lispy looking thing and compiling it to

00:28:25,180 --> 00:28:28,840
something that you're shoving in the

00:28:26,230 --> 00:28:31,390
kernel or maybe elsewhere as well what

00:28:28,840 --> 00:28:33,580
how does can you explain a bit more how

00:28:31,390 --> 00:28:35,980
that works yeah so I mean we have like

00:28:33,580 --> 00:28:37,600
you can kind of think of it as a really

00:28:35,980 --> 00:28:40,150
high level virtual machine kind of like

00:28:37,600 --> 00:28:42,160
how EBP F works so it's you know

00:28:40,150 --> 00:28:43,840
compiled and user space into love

00:28:42,160 --> 00:28:45,300
low-level instructions and those

00:28:43,840 --> 00:28:47,830
instructions are interpreted inside

00:28:45,300 --> 00:28:50,320
inside our data path component loop CCP

00:28:47,830 --> 00:28:52,360
so you've added yet another bytecode

00:28:50,320 --> 00:28:54,970
interpreter to the kernel sure you can

00:28:52,360 --> 00:28:57,280
think of it that way okay it is your

00:28:54,970 --> 00:29:03,450
intention to also put this other places

00:28:57,280 --> 00:29:06,520
for portability year yeah so our our

00:29:03,450 --> 00:29:08,650
implementation is portable and like to

00:29:06,520 --> 00:29:11,500
implement support for quick and M TCP we

00:29:08,650 --> 00:29:13,360
took the same interpreter and linked it

00:29:11,500 --> 00:29:17,680
in there so that's that's how we

00:29:13,360 --> 00:29:20,880
implemented this cool I forgot to ask

00:29:17,680 --> 00:29:25,000
one question I'm sure you thought about

00:29:20,880 --> 00:29:27,250
reusing existing implementations but I

00:29:25,000 --> 00:29:28,510
clearly you did not do that in terms of

00:29:27,250 --> 00:29:31,780
the conditional controllers themselves

00:29:28,510 --> 00:29:33,880
did you actually try using for example

00:29:31,780 --> 00:29:35,350
the linux or the quick baby our

00:29:33,880 --> 00:29:38,890
implementation instead of writing your

00:29:35,350 --> 00:29:40,240
own because that would that if this is

00:29:38,890 --> 00:29:42,730
to go forward

00:29:40,240 --> 00:29:44,800
as as a library I would much rather not

00:29:42,730 --> 00:29:47,260
have brand-new implementations well

00:29:44,800 --> 00:29:48,760
actually let me not say that I would I

00:29:47,260 --> 00:29:50,800
would want to consider reusing existing

00:29:48,760 --> 00:29:53,559
fermentations as well as developing new

00:29:50,800 --> 00:29:55,840
ones yeah but I'd be interested in

00:29:53,559 --> 00:29:57,280
seeing did you try that what's the

00:29:55,840 --> 00:29:58,900
experience been if not would you

00:29:57,280 --> 00:30:01,030
consider trying that yeah this is a fair

00:29:58,900 --> 00:30:04,000
question I think one of the differences

00:30:01,030 --> 00:30:06,160
is that existing implementations which

00:30:04,000 --> 00:30:09,550
are data paths specific tend to be

00:30:06,160 --> 00:30:12,100
written like synchronously so there is

00:30:09,550 --> 00:30:13,750
the expectation that you're able to make

00:30:12,100 --> 00:30:16,059
decisions upon a free packet whereas

00:30:13,750 --> 00:30:19,929
this new api is asynchronous so we

00:30:16,059 --> 00:30:22,920
didn't consider sort of attempting to

00:30:19,929 --> 00:30:26,850
use the code as is because the api is

00:30:22,920 --> 00:30:29,950
sort of running in a different context

00:30:26,850 --> 00:30:32,020
you might you might look at the code it

00:30:29,950 --> 00:30:34,000
might be easier to make it asynchronous

00:30:32,020 --> 00:30:36,400
in parts that you care about okay then

00:30:34,000 --> 00:30:38,290
you then you imagine at least some I can

00:30:36,400 --> 00:30:39,490
I can speak for the for the quick

00:30:38,290 --> 00:30:42,370
implementation because we have a pretty

00:30:39,490 --> 00:30:45,700
clean interface in chromium and that

00:30:42,370 --> 00:30:48,640
interface does I again I try to think

00:30:45,700 --> 00:30:51,700
about this more but then I suspect you

00:30:48,640 --> 00:30:53,380
might be able to use that directly yeah

00:30:51,700 --> 00:30:55,650
I think we're definitely willing to look

00:30:53,380 --> 00:30:55,650
into it

00:30:59,390 --> 00:31:06,760
any more questions thanks I really

00:31:04,020 --> 00:31:10,119
enjoyed this thank you

00:31:06,760 --> 00:31:10,119

YouTube URL: https://www.youtube.com/watch?v=XZKyOZunS0c


