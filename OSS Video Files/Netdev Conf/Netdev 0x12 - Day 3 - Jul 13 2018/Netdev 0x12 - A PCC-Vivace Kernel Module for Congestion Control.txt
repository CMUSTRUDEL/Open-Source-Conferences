Title: Netdev 0x12 - A PCC-Vivace Kernel Module for Congestion Control
Publication date: 2018-08-01
Playlist: Netdev 0x12 - Day 3 - Jul 13 2018
Description: 
	On July 13th, 2018 at Netdev 0x12 in Montreal, Tomer Gilad gave a talk asserting that there is no one-size-fits-all TCP congestion control algorithm.

Different apps have different goals for their service lifetimes:
- streaming video apps are sensitive to bandwidth fluctuations,
- voice chats desire low latency and bulk transfers only care about completion time.
Varying link characteristics add another dimension to a congestion control algorithm:
- hardwired assumptions about the cause of packet loss, or
- measured latency inflation can lead to reduced application performance.

PCC-Vivace was created to address these issues.
PCC-Vivace congestion control algorithm provides an explicit utility function that allows developers to provide weights to different performance metrics like throughput, latency, packet loss and jitter. PCC-Vivace's online learning framework also allows it to adapt to a variety of network conditions, consistently delivering high performance.

The talk discussed the challenges of implementing PCC-Vivace in the Linux kernel. Tomer presented initial results comparing the performance of Vivace with other existing Linux congestion controllers (BBR etc). The talk also demonstrated the implementation's flexibility by creating and testing a variety of utility functions.

More info:
https://www.netdevconf.org/0x12/session.html?a-pcc-vivace-kernel-module-for-congestion-control
Captions: 
	00:00:04,100 --> 00:00:10,950
yeah gets turned a style yeah

00:00:07,740 --> 00:00:13,099
so hello i'm tamil Gilad from the hebrew

00:00:10,950 --> 00:00:16,260
university and i'm here to present you

00:00:13,099 --> 00:00:20,220
pcc vivace and the kernel module that

00:00:16,260 --> 00:00:22,949
tries to implement it so let's talk a

00:00:20,220 --> 00:00:27,300
bit about congestion control first of

00:00:22,949 --> 00:00:30,769
all we have the sender's that the

00:00:27,300 --> 00:00:33,930
network and the receivers the sender's

00:00:30,769 --> 00:00:36,300
choose when to send the data using

00:00:33,930 --> 00:00:39,270
either the congestion window or placing

00:00:36,300 --> 00:00:41,809
and the receivers the pretty much

00:00:39,270 --> 00:00:44,850
passive they only send acknowledgments

00:00:41,809 --> 00:00:48,539
the network tries it's best to out the

00:00:44,850 --> 00:00:52,050
packet fool but there are many different

00:00:48,539 --> 00:00:56,510
types of networks and you can have small

00:00:52,050 --> 00:00:59,340
bottles and applause competing flows etc

00:00:56,510 --> 00:01:02,120
so there are a few approaches to

00:00:59,340 --> 00:01:05,460
congestion control and it's the

00:01:02,120 --> 00:01:09,210
congestion controls job to avoid

00:01:05,460 --> 00:01:15,350
congesting the network the classical TCP

00:01:09,210 --> 00:01:19,619
approach such as cubic hard wires the

00:01:15,350 --> 00:01:21,869
response to a packet event for example

00:01:19,619 --> 00:01:25,670
cubic will back off based on a single

00:01:21,869 --> 00:01:28,950
loss which makes it really easy to

00:01:25,670 --> 00:01:33,119
implement and think about but suboptimal

00:01:28,950 --> 00:01:36,000
in cases such as random loss well

00:01:33,119 --> 00:01:42,420
obviously it will have a really low

00:01:36,000 --> 00:01:46,079
weight the other problem with cubic it

00:01:42,420 --> 00:01:50,180
feels large buffers causing buffer bloat

00:01:46,079 --> 00:01:50,180
and high self induced Agency

00:01:50,430 --> 00:01:56,820
does the BBL approach which tries to

00:01:53,940 --> 00:02:00,630
model the network as a single link it's

00:01:56,820 --> 00:02:03,800
a more of a white box approach and it it

00:02:00,630 --> 00:02:07,470
gets some really impressive performance

00:02:03,800 --> 00:02:10,979
but it still has a few problems

00:02:07,470 --> 00:02:12,930
it feels baffles to some extent and

00:02:10,979 --> 00:02:16,950
drains them all the time

00:02:12,930 --> 00:02:20,520
causing some self-induced latency and it

00:02:16,950 --> 00:02:25,620
has a loss rate when the multiple VBR

00:02:20,520 --> 00:02:28,709
flows competing with each other and the

00:02:25,620 --> 00:02:32,520
third approach we are going to talk

00:02:28,709 --> 00:02:34,200
about pcc's approach PCC stands for

00:02:32,520 --> 00:02:38,370
performance Soviet and congestion

00:02:34,200 --> 00:02:41,160
control it tries to monitor the

00:02:38,370 --> 00:02:44,940
performance of various sending grades

00:02:41,160 --> 00:02:52,080
and then adapt the rate to maximize its

00:02:44,940 --> 00:02:54,989
utility PCC uses monitor intervals each

00:02:52,080 --> 00:02:59,810
interval is about one RTT long and

00:02:54,989 --> 00:03:05,910
corresponds to a single sending rate and

00:02:59,810 --> 00:03:09,570
in each interval PCC will observe the

00:03:05,910 --> 00:03:12,810
network reaction to this sending rate it

00:03:09,570 --> 00:03:15,690
looks at these parameters the throughput

00:03:12,810 --> 00:03:18,900
latency the change in latency and the

00:03:15,690 --> 00:03:21,600
loss rate and with these statistics it

00:03:18,900 --> 00:03:27,239
calculate the it calculates the utility

00:03:21,600 --> 00:03:30,000
of this sending rate this means PCC will

00:03:27,239 --> 00:03:32,400
rely on a causal relation between the

00:03:30,000 --> 00:03:36,690
sending rate and value of the utility

00:03:32,400 --> 00:03:39,150
and it allows PCC to treat the network

00:03:36,690 --> 00:03:41,340
as a black box and make as few

00:03:39,150 --> 00:03:44,540
assumptions as possible about the

00:03:41,340 --> 00:03:44,540
structure of the network

00:03:44,599 --> 00:03:50,989
so the utility function it should

00:03:48,409 --> 00:03:54,980
reflect the applications performance

00:03:50,989 --> 00:03:59,629
objectives these can be things like high

00:03:54,980 --> 00:04:03,200
throughput low latency jitter etc it

00:03:59,629 --> 00:04:06,139
should also guarantee fairness in when

00:04:03,200 --> 00:04:11,769
facing multiple PCC senders using the

00:04:06,139 --> 00:04:16,039
same utility and here we have this

00:04:11,769 --> 00:04:19,370
example graph of a single PCC send all

00:04:16,039 --> 00:04:22,099
on a link well the utility increases as

00:04:19,370 --> 00:04:24,860
long as the rate below the link capacity

00:04:22,099 --> 00:04:28,370
and once it's above it's starting to

00:04:24,860 --> 00:04:31,520
decrease so this is the thing we want to

00:04:28,370 --> 00:04:35,470
maximize we want to have maximum utility

00:04:31,520 --> 00:04:35,470
as possible

00:04:35,990 --> 00:04:43,160
so in our implementation we're going to

00:04:39,349 --> 00:04:45,229
give two utility functions Allegro and

00:04:43,160 --> 00:04:48,830
vivace which are named after the

00:04:45,229 --> 00:04:53,570
versions of PCC that they were first

00:04:48,830 --> 00:04:59,060
introduced in Allegro being the first

00:04:53,570 --> 00:05:04,400
version of PCC from NSD i-15 and we

00:04:59,060 --> 00:05:09,470
watch from an SDI 18 so al ago is

00:05:04,400 --> 00:05:13,849
completely lost based utility it it

00:05:09,470 --> 00:05:16,580
rewards the sender for sending at a

00:05:13,849 --> 00:05:19,340
certain rate and this reward diminishes

00:05:16,580 --> 00:05:24,800
with the lost weight and it penalizes

00:05:19,340 --> 00:05:29,780
for loss we watch on the other hand it

00:05:24,800 --> 00:05:34,130
has it has still rewards and penalties

00:05:29,780 --> 00:05:37,849
based on the count rate but it also

00:05:34,130 --> 00:05:41,900
penalizes for the change in latency over

00:05:37,849 --> 00:05:45,190
time and like before you have a penalty

00:05:41,900 --> 00:05:45,190
for the loss rate

00:05:46,740 --> 00:05:50,000
but these aren't the only utility

00:05:48,509 --> 00:05:55,830
functions in mind

00:05:50,000 --> 00:05:58,169
PCC is really flexible so you can plug

00:05:55,830 --> 00:06:01,340
in different utility functions other

00:05:58,169 --> 00:06:04,860
utility functions we are thinking about

00:06:01,340 --> 00:06:07,800
things like rate scavengers which try to

00:06:04,860 --> 00:06:13,490
utilize links and back off on the first

00:06:07,800 --> 00:06:18,090
sign of competition may be using jitter

00:06:13,490 --> 00:06:23,009
could help doing that other utility

00:06:18,090 --> 00:06:24,750
functions may use latency directly

00:06:23,009 --> 00:06:29,479
instead of the latency change like the

00:06:24,750 --> 00:06:33,750
battery utility and this may give some

00:06:29,479 --> 00:06:37,800
guarantees about the latency when the

00:06:33,750 --> 00:06:41,460
network is relatively known oh you could

00:06:37,800 --> 00:06:45,599
use that to keep buffers slightly full

00:06:41,460 --> 00:06:48,120
and in Wireless scenarios where the

00:06:45,599 --> 00:06:54,300
capacity suddenly changes this could

00:06:48,120 --> 00:06:58,379
prove helpful so now that we have the

00:06:54,300 --> 00:07:02,430
utility function we want to make the

00:06:58,379 --> 00:07:07,500
next step we want to decide what will be

00:07:02,430 --> 00:07:12,050
the next rate tested so we are using two

00:07:07,500 --> 00:07:15,750
intervals and use them to calculate the

00:07:12,050 --> 00:07:19,830
utility values and then we calculate the

00:07:15,750 --> 00:07:24,199
gradient of the utility and then we use

00:07:19,830 --> 00:07:24,199
gradient and send to quickly maximize it

00:07:25,709 --> 00:07:34,789
so PCC has three states in its state

00:07:30,330 --> 00:07:39,179
machine in a rate control Pulte

00:07:34,789 --> 00:07:44,909
startup bobbing and moving so the

00:07:39,179 --> 00:07:47,789
startup is similar to tcp slow-start the

00:07:44,909 --> 00:07:52,919
goal is to quickly reach within 50

00:07:47,789 --> 00:07:56,159
percent of the link capacity it like tcp

00:07:52,919 --> 00:08:00,689
slow-start doubles sending gate on each

00:07:56,159 --> 00:08:04,169
article but it backs off when the

00:08:00,689 --> 00:08:07,259
utility starts to decrease it isn't

00:08:04,169 --> 00:08:10,709
necessarily on the first loss it can be

00:08:07,259 --> 00:08:14,009
for instance in an environment of random

00:08:10,709 --> 00:08:19,469
loss the loss rate is the same with all

00:08:14,009 --> 00:08:21,569
the rates you're testing so the startup

00:08:19,469 --> 00:08:26,339
will handle that better than tcp

00:08:21,569 --> 00:08:29,099
slow-start once the utility decrease we

00:08:26,339 --> 00:08:32,000
are moving to the probing state here we

00:08:29,099 --> 00:08:35,849
try slightly lower and slightly higher

00:08:32,000 --> 00:08:40,439
sending rates to find what is the

00:08:35,849 --> 00:08:43,919
direction we need to continue on and we

00:08:40,439 --> 00:08:48,750
do that twice to get a conclusive

00:08:43,919 --> 00:08:52,050
gradient once the two gradients agree

00:08:48,750 --> 00:08:54,779
with each other then we know with high

00:08:52,050 --> 00:09:00,389
certainty what is the direction we need

00:08:54,779 --> 00:09:02,870
to move on so to maximize our utility we

00:09:00,389 --> 00:09:08,670
then transition to the moving state

00:09:02,870 --> 00:09:11,850
which the goal here is to make quick

00:09:08,670 --> 00:09:15,059
steps towards maximizing the utility in

00:09:11,850 --> 00:09:21,029
the direction that that we found earlier

00:09:15,059 --> 00:09:23,339
and the point is on each sending gate we

00:09:21,029 --> 00:09:27,180
calculate the gradient using the last

00:09:23,339 --> 00:09:30,089
two intervals and take step depending on

00:09:27,180 --> 00:09:32,430
the size of the gradient we repeat that

00:09:30,089 --> 00:09:35,670
as long as the utility increases and

00:09:32,430 --> 00:09:39,769
once it's it's starting to decrease with

00:09:35,670 --> 00:09:39,769
go to probing state a

00:09:40,500 --> 00:09:48,780
so this was PCC PCC bachi and it works

00:09:46,110 --> 00:09:54,600
it has user space implementations that

00:09:48,780 --> 00:09:58,680
do quite well and now we started porting

00:09:54,600 --> 00:10:01,320
it to kernel we thought it was going to

00:09:58,680 --> 00:10:07,860
be easy because the user space code is

00:10:01,320 --> 00:10:10,010
written but obviously we were one so the

00:10:07,860 --> 00:10:13,370
first problem we encountered was

00:10:10,010 --> 00:10:16,860
managing the intervals associating the

00:10:13,370 --> 00:10:21,960
packet results either delivered or lost

00:10:16,860 --> 00:10:25,140
to the intervals they were sent in this

00:10:21,960 --> 00:10:28,320
is a one crucial part of the PCC flame-o

00:10:25,140 --> 00:10:30,360
because if it doesn't work well then we

00:10:28,320 --> 00:10:35,400
can't have this relation between the

00:10:30,360 --> 00:10:37,950
sending gate and the utility so the use

00:10:35,400 --> 00:10:43,940
of space implementations they were based

00:10:37,950 --> 00:10:47,180
on UDT which had unique packet IDs like

00:10:43,940 --> 00:10:52,250
something like we saw earlier with quick

00:10:47,180 --> 00:10:54,930
and we have we had packet

00:10:52,250 --> 00:10:58,650
acknowledgments which made it really

00:10:54,930 --> 00:11:03,380
easy to figure out what packet was lost

00:10:58,650 --> 00:11:07,440
and exactly at what interval was it sent

00:11:03,380 --> 00:11:13,080
the kernel implementation doesn't have

00:11:07,440 --> 00:11:15,900
this because TCP is it doesn't it just

00:11:13,080 --> 00:11:18,780
doesn't have the unique packet IDs and

00:11:15,900 --> 00:11:23,460
the packet acknowledgements so we only

00:11:18,780 --> 00:11:26,180
have an approximate environment of of

00:11:23,460 --> 00:11:30,380
each packet we only know approximately

00:11:26,180 --> 00:11:35,190
what interval it belongs to help

00:11:30,380 --> 00:11:40,650
mitigate that we introduced an

00:11:35,190 --> 00:11:43,050
uncertainty bound to PCC meaning we

00:11:40,650 --> 00:11:46,470
ignore the start and end of each

00:11:43,050 --> 00:11:48,220
interval and only count statistics from

00:11:46,470 --> 00:11:51,640
packets and

00:11:48,220 --> 00:11:54,910
in the middle of an interval and that

00:11:51,640 --> 00:12:00,190
way we won't mix up between the

00:11:54,910 --> 00:12:04,560
intervals themselves so you might ask

00:12:00,190 --> 00:12:07,480
yourself why not to use the rate samples

00:12:04,560 --> 00:12:10,210
rate samples were introduced with PBL

00:12:07,480 --> 00:12:12,070
and on the first glance they do exactly

00:12:10,210 --> 00:12:15,040
what we are trying to do with the

00:12:12,070 --> 00:12:19,420
monitor intervals they have statistics

00:12:15,040 --> 00:12:21,910
about about the packets lost the packets

00:12:19,420 --> 00:12:25,450
delivered and they sample over an

00:12:21,910 --> 00:12:29,560
interval of time so we had a few

00:12:25,450 --> 00:12:33,160
problems with that first of all they

00:12:29,560 --> 00:12:38,470
overlap which makes it kind of difficult

00:12:33,160 --> 00:12:41,830
to manage some packet could be accounted

00:12:38,470 --> 00:12:46,870
for in in a few following great samples

00:12:41,830 --> 00:12:51,490
and and you can't really configure when

00:12:46,870 --> 00:12:53,470
it they start in the end so if they were

00:12:51,490 --> 00:12:55,450
short and non overlapping we could have

00:12:53,470 --> 00:12:57,940
got them together if we could configure

00:12:55,450 --> 00:13:00,990
the length we could say start now and

00:12:57,940 --> 00:13:06,700
later and have an interval as we want it

00:13:00,990 --> 00:13:11,140
and we can do that another problem is

00:13:06,700 --> 00:13:15,860
that the missing key piece of

00:13:11,140 --> 00:13:20,370
information which is the sending gate

00:13:15,860 --> 00:13:26,130
at the time the interval was the rate

00:13:20,370 --> 00:13:30,360
sample was taken which means it's hard

00:13:26,130 --> 00:13:34,950
to associate the rate sample to the

00:13:30,360 --> 00:13:38,670
standing date another challenge we had

00:13:34,950 --> 00:13:44,459
when implementing PCC is dealing with

00:13:38,670 --> 00:13:48,839
these approximations so we had a packet

00:13:44,459 --> 00:13:54,270
to interval approximation we had earlier

00:13:48,839 --> 00:13:56,070
in associating them and we have other

00:13:54,270 --> 00:13:59,149
approximations in the current

00:13:56,070 --> 00:14:02,640
implementation as well the use of space

00:13:59,149 --> 00:14:07,649
implementation calculated the change in

00:14:02,640 --> 00:14:10,140
latency what it did was holding out the

00:14:07,649 --> 00:14:14,180
LTTE for each packet in an interval and

00:14:10,140 --> 00:14:22,290
then use some linear regression to have

00:14:14,180 --> 00:14:24,600
a more accurate altitude gradient we

00:14:22,290 --> 00:14:27,029
obviously can do that because holding

00:14:24,600 --> 00:14:31,890
such information in the kernel is is

00:14:27,029 --> 00:14:33,990
extremely costly so we used the smooth

00:14:31,890 --> 00:14:37,050
data T at the beginning and the end of

00:14:33,990 --> 00:14:40,370
an interval and try to approximate the

00:14:37,050 --> 00:14:45,930
gradient using time and was the whole

00:14:40,370 --> 00:14:50,760
while moving from a float utility to an

00:14:45,930 --> 00:14:57,230
integer one so it it works a bit a bit

00:14:50,760 --> 00:15:05,520
walls this was altered in in a lot of

00:14:57,230 --> 00:15:08,190
instability in the gradients so unstable

00:15:05,520 --> 00:15:13,770
gradients really had the performance of

00:15:08,190 --> 00:15:16,500
the algorithm so it's obviously bad what

00:15:13,770 --> 00:15:18,980
we did to help with that was setting the

00:15:16,500 --> 00:15:22,890
minimum rate change to two percent

00:15:18,980 --> 00:15:25,800
meaning every time we test slightly

00:15:22,890 --> 00:15:26,990
higher and lower rates in the poping and

00:15:25,800 --> 00:15:30,680
even the moving

00:15:26,990 --> 00:15:36,020
the states will force the rate change to

00:15:30,680 --> 00:15:40,930
be 2% to have more correct gradients

00:15:36,020 --> 00:15:49,220
because the difference won't be as noisy

00:15:40,930 --> 00:15:52,360
with against a smaller rate change so

00:15:49,220 --> 00:15:57,650
despite these problems we have some

00:15:52,360 --> 00:16:00,080
really good initial results we tested it

00:15:57,650 --> 00:16:03,890
using Pantheon and we looked at the

00:16:00,080 --> 00:16:05,810
following three metrics the loss

00:16:03,890 --> 00:16:10,160
resilience buffer bloat and loss at

00:16:05,810 --> 00:16:12,620
convergence and we compared it against

00:16:10,160 --> 00:16:15,110
the use of space versions because they

00:16:12,620 --> 00:16:19,790
are the two benchmark we are trying to

00:16:15,110 --> 00:16:23,180
implement after all and against cubic

00:16:19,790 --> 00:16:31,990
and bbl implementations this was with

00:16:23,180 --> 00:16:37,460
kernel for 16 so high loss resilience

00:16:31,990 --> 00:16:40,720
this means the amount of random loss the

00:16:37,460 --> 00:16:47,360
algorithm can handle while maintaining

00:16:40,720 --> 00:16:50,480
75% of its maximal full put so you can

00:16:47,360 --> 00:16:53,690
see that the kernel implementation does

00:16:50,480 --> 00:16:57,620
well up to 5% of random loss which is

00:16:53,690 --> 00:17:00,610
slightly higher than random loss that

00:16:57,620 --> 00:17:04,130
the use of space implementations can

00:17:00,610 --> 00:17:06,980
handle oh by the way this was for with

00:17:04,130 --> 00:17:11,839
the we watch a utility PCC channel here

00:17:06,980 --> 00:17:15,980
is Revati utility function so it is

00:17:11,839 --> 00:17:21,520
slightly mobile resilient to loss we

00:17:15,980 --> 00:17:24,290
attributed to the to the

00:17:21,520 --> 00:17:27,260
Paquette to interval association which

00:17:24,290 --> 00:17:33,620
slightly fables delivered packets over

00:17:27,260 --> 00:17:37,700
lost and we have here beabea which does

00:17:33,620 --> 00:17:47,510
extremely well with 10% loss and even at

00:17:37,700 --> 00:17:50,360
15% and cubic which yeah so the low

00:17:47,510 --> 00:17:52,510
buffer bloat meaning how much self

00:17:50,360 --> 00:18:00,800
induced latency does the algorithm

00:17:52,510 --> 00:18:03,650
causes so the all of the pcc variants do

00:18:00,800 --> 00:18:08,000
here quite good

00:18:03,650 --> 00:18:10,460
well the kernel implementation does

00:18:08,000 --> 00:18:13,970
slightly worse than both of the use of

00:18:10,460 --> 00:18:17,030
space again probably due to the bad

00:18:13,970 --> 00:18:21,950
calculation of the utility of the

00:18:17,030 --> 00:18:25,340
latency gradient and you can see that

00:18:21,950 --> 00:18:33,290
both vb island cubic here increase the

00:18:25,340 --> 00:18:37,070
the latency as the baffle goes the loss

00:18:33,290 --> 00:18:40,610
of convergence meaning how many how much

00:18:37,070 --> 00:18:43,670
random loss the algorithm causes when

00:18:40,610 --> 00:18:45,650
multiple competing flows with the same

00:18:43,670 --> 00:18:51,650
using the same algorithm at the

00:18:45,650 --> 00:18:56,180
convergence point so bbl converges to

00:18:51,650 --> 00:19:00,650
someone about 15% cubic does extremely

00:18:56,180 --> 00:19:06,470
well here it maintains really low loss

00:19:00,650 --> 00:19:10,700
rate at convergence but the pcc variants

00:19:06,470 --> 00:19:16,970
they all have this increasing loss rate

00:19:10,700 --> 00:19:22,380
as the number of flows in cases this

00:19:16,970 --> 00:19:26,549
does well up to about 25

00:19:22,380 --> 00:19:38,100
competing clothes were then bbl starts

00:19:26,549 --> 00:19:42,480
to get ahead of the PCC variants so to

00:19:38,100 --> 00:19:44,970
conclude we have these really promising

00:19:42,480 --> 00:19:48,389
initial results but we aren't done yet

00:19:44,970 --> 00:19:56,309
this is still a work in progress still

00:19:48,389 --> 00:19:59,190
in the early stages we want to make some

00:19:56,309 --> 00:20:07,129
changes in the sampling of the in the

00:19:59,190 --> 00:20:10,710
kernel to have better better statistics

00:20:07,129 --> 00:20:17,279
like now all we did was without making

00:20:10,710 --> 00:20:19,379
any change to the kernel API and we are

00:20:17,279 --> 00:20:22,830
still debating on the right way to

00:20:19,379 --> 00:20:25,500
expose the utility function to the use

00:20:22,830 --> 00:20:29,779
of space I mean the obvious way would be

00:20:25,500 --> 00:20:34,710
to have separate congestion controls

00:20:29,779 --> 00:20:37,740
one for each utility function but we

00:20:34,710 --> 00:20:40,200
might want to have a bit more power for

00:20:37,740 --> 00:20:43,470
the applications maybe having them

00:20:40,200 --> 00:20:47,669
change some constants in the utility or

00:20:43,470 --> 00:20:52,139
stuff like that and we haven't decided

00:20:47,669 --> 00:20:56,299
on the right way to do that yet the code

00:20:52,139 --> 00:21:00,000
is available at github on this link and

00:20:56,299 --> 00:21:02,220
we have a website with much more

00:21:00,000 --> 00:21:05,970
detailed information about PCC with

00:21:02,220 --> 00:21:08,850
links from to the papers and nice

00:21:05,970 --> 00:21:12,320
infographics so go ahead and check them

00:21:08,850 --> 00:21:15,500
out test the code tell us what you think

00:21:12,320 --> 00:21:15,500
thank you

00:21:22,059 --> 00:21:28,549
Erica there's anything from your side

00:21:23,989 --> 00:21:31,159
living oh yeah I was actually curious if

00:21:28,549 --> 00:21:33,980
you did a test with a plummeted traffic

00:21:31,159 --> 00:21:37,100
where these were all non admitted no

00:21:33,980 --> 00:21:40,460
these were all known up limited okay we

00:21:37,100 --> 00:21:43,070
still assume and you have what to send

00:21:40,460 --> 00:21:44,690
all the time yeah yeah because that

00:21:43,070 --> 00:21:47,860
introduces a lot of very interesting and

00:21:44,690 --> 00:21:47,860
challenging problems yeah

00:21:53,299 --> 00:22:01,429
I did a couple of quick thoughts on the

00:21:58,149 --> 00:22:04,820
surprise on the slide with latency for

00:22:01,429 --> 00:22:06,499
the various congestion controls do you

00:22:04,820 --> 00:22:10,909
happen to know if the experiments were

00:22:06,499 --> 00:22:13,639
done with fq q disk or with internal

00:22:10,909 --> 00:22:19,100
pacing for babies so these were using

00:22:13,639 --> 00:22:23,019
Pantheon so I think the BBL that does

00:22:19,100 --> 00:22:27,590
use FQ though it the way it was defined

00:22:23,019 --> 00:22:31,279
but PCC I don't remember whether it was

00:22:27,590 --> 00:22:36,259
with a few Q disk or not okay it does

00:22:31,279 --> 00:22:40,340
PCC requests pacing from the system or

00:22:36,259 --> 00:22:43,489
does it is it window based it is using X

00:22:40,340 --> 00:22:49,369
increasing okay and then I was curious

00:22:43,489 --> 00:22:52,369
if you have thoughts on or test results

00:22:49,369 --> 00:22:56,090
from how the vivace algorithm behaves

00:22:52,369 --> 00:22:58,850
when there's very small amounts of

00:22:56,090 --> 00:23:02,440
buffering so for example example like I

00:22:58,850 --> 00:23:09,230
would a bottleneck buffer that has about

00:23:02,440 --> 00:23:11,989
1% of the GDP so we don't have results

00:23:09,230 --> 00:23:13,970
from Dec and our implementation but we

00:23:11,989 --> 00:23:16,820
do have results from the use of space 1

00:23:13,970 --> 00:23:21,739
ok um hopefully the kernel

00:23:16,820 --> 00:23:27,440
implementation matches yeah so it's I

00:23:21,739 --> 00:23:32,350
think it does quite well very it I think

00:23:27,440 --> 00:23:35,720
it requires something like a buffer of

00:23:32,350 --> 00:23:42,139
nine packets well the bdp is something

00:23:35,720 --> 00:23:42,799
like eight packets oh no I don't

00:23:42,139 --> 00:23:45,350
remember

00:23:42,799 --> 00:23:50,929
I'm sorry okay yeah maybe I'll win right

00:23:45,350 --> 00:23:55,739
over email thank you so much yeah Eric

00:23:50,929 --> 00:23:57,749
yeah so thanks for this very nice talk

00:23:55,739 --> 00:24:02,190
it looks very interesting and very

00:23:57,749 --> 00:24:05,759
promising so feel free to to send the

00:24:02,190 --> 00:24:08,389
RFC patch on the death you know that's

00:24:05,759 --> 00:24:09,899
that's the eventual plan right now it's

00:24:08,389 --> 00:24:17,909
not ready yet

00:24:09,899 --> 00:24:18,809
but someday it will be mimes hi thank

00:24:17,909 --> 00:24:20,669
you for the talk

00:24:18,809 --> 00:24:24,119
did you do an experiment with furnace

00:24:20,669 --> 00:24:30,629
how do you do with multiple PCC flows

00:24:24,119 --> 00:24:36,690
how they react to each other so they do

00:24:30,629 --> 00:24:38,609
converge and that's somewhat what you

00:24:36,690 --> 00:24:44,820
saw in the with the loss rated

00:24:38,609 --> 00:24:47,369
convergence you didn't see the the the

00:24:44,820 --> 00:24:53,239
convergence itself but they converge to

00:24:47,369 --> 00:24:53,239
a fellow to FL bookshare okay thank you

00:24:58,090 --> 00:25:04,710
the the convergence brought to mind a

00:25:02,559 --> 00:25:07,059
question of can you talk about the

00:25:04,710 --> 00:25:10,779
approach that they've actually takes for

00:25:07,059 --> 00:25:16,510
coexisting with Reno or cubic flows and

00:25:10,779 --> 00:25:19,330
how it keeps its probing from causing

00:25:16,510 --> 00:25:21,640
packet loss that causes the Reno going

00:25:19,330 --> 00:25:26,470
cubic flows to tobacco often in yield

00:25:21,640 --> 00:25:33,220
yeah its bandwidth so basically since we

00:25:26,470 --> 00:25:37,080
watch a is part latency based and it it

00:25:33,220 --> 00:25:40,419
is somewhat less aggressive then then

00:25:37,080 --> 00:25:46,529
cubic or we know or any completely lost

00:25:40,419 --> 00:25:49,360
based congestion control and our

00:25:46,529 --> 00:25:52,390
experiments with the use of space prove

00:25:49,360 --> 00:25:57,990
that we haven't done extensive

00:25:52,390 --> 00:26:03,490
evaluation of that with the kernel but

00:25:57,990 --> 00:26:06,039
well imagine that vivace is alone on a

00:26:03,490 --> 00:26:12,549
link it will try to keep the buffers

00:26:06,039 --> 00:26:14,500
empty but the cubic for example it is

00:26:12,549 --> 00:26:20,760
completely lost based so it will fill

00:26:14,500 --> 00:26:30,149
the buffers meaning it will get more

00:26:20,760 --> 00:26:30,149
more bandwidth than the vivace thank you

00:26:33,060 --> 00:26:36,930

YouTube URL: https://www.youtube.com/watch?v=tx6zjv_RgMQ


