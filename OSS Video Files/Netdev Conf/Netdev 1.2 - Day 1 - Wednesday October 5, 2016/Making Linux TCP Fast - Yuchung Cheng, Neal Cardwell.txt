Title: Making Linux TCP Fast - Yuchung Cheng, Neal Cardwell
Publication date: 2016-10-06
Playlist: Netdev 1.2 - Day 1 - Wednesday October 5, 2016
Description: 
	http://netdevconf.org/1.2/session.html?yuchung-cheng
Captions: 
	00:00:21,440 --> 00:00:25,970
oh-oh I change their dislike

00:00:26,140 --> 00:00:33,270
I can't do it okay all right

00:00:33,370 --> 00:00:41,320
i I want today I'm here to present

00:00:37,740 --> 00:00:44,760
making Linux TCP fast along with my

00:00:41,320 --> 00:00:47,680
co-workers neocon well from Google and

00:00:44,760 --> 00:00:50,080
this I'll start from a story of set

00:00:47,680 --> 00:00:53,140
about when you receive an ACK

00:00:50,080 --> 00:00:55,239
what do you do in a TCP stack and over

00:00:53,140 --> 00:00:58,900
time we have changed how he process this

00:00:55,239 --> 00:01:02,980
act considerably for to eval and

00:00:58,900 --> 00:01:05,770
deploying features so first thing we do

00:01:02,980 --> 00:01:08,350
is I usually process the act information

00:01:05,770 --> 00:01:10,240
and then take out whatever packet at a

00:01:08,350 --> 00:01:13,000
speed instead of acknowledged by this is

00:01:10,240 --> 00:01:17,619
multiplied in the standard RC 793

00:01:13,000 --> 00:01:20,530
nothing new here and try to detect

00:01:17,619 --> 00:01:22,690
losses if any so that Center for the

00:01:20,530 --> 00:01:25,149
reliability part and that part we have

00:01:22,690 --> 00:01:27,190
changed and then we're going to ask the

00:01:25,149 --> 00:01:30,070
congestion control okay now we have

00:01:27,190 --> 00:01:32,800
known what to retransmit what the new

00:01:30,070 --> 00:01:36,070
that I want to send and how much do we

00:01:32,800 --> 00:01:38,979
sent and then there is this very high

00:01:36,070 --> 00:01:43,590
performance packet scheduler to sort of

00:01:38,979 --> 00:01:43,590
send out the packets at exactly the rate

00:01:48,509 --> 00:01:56,470
exactly the rate that we wanted to send

00:01:51,099 --> 00:01:59,560
to um so let me copy this thing called a

00:01:56,470 --> 00:02:01,750
dupe access hole which are essentially

00:01:59,560 --> 00:02:04,840
is that if you receive through backs

00:02:01,750 --> 00:02:06,759
then some hack is probably lost and then

00:02:04,840 --> 00:02:08,709
you need to retransmit sort of the first

00:02:06,759 --> 00:02:12,780
on a packet where often moving with that

00:02:08,709 --> 00:02:16,180
all the time this is has seen its limit

00:02:12,780 --> 00:02:19,239
primarily because for shortfalls you

00:02:16,180 --> 00:02:20,240
don't get to constantly do backs or when

00:02:19,239 --> 00:02:23,090
you have we all

00:02:20,240 --> 00:02:25,850
then this number magic van persie is

00:02:23,090 --> 00:02:28,070
kind of wrong and we have to pour this

00:02:25,850 --> 00:02:29,750
new sinkhole rock which invests of just

00:02:28,070 --> 00:02:33,020
counting the number of out of all the

00:02:29,750 --> 00:02:34,730
packets we essentially enter when the

00:02:33,020 --> 00:02:36,650
pattern was sent and you can imagine

00:02:34,730 --> 00:02:38,780
that every packet you said you put sort

00:02:36,650 --> 00:02:47,740
of the climber on it and when it expires

00:02:38,780 --> 00:03:10,670
the packets p1 and p2 and then you get

00:02:47,740 --> 00:03:13,580
so you know that we transmit time outs

00:03:10,670 --> 00:03:17,000
in the disorder state in TCP which means

00:03:13,580 --> 00:03:19,100
that delivery and so here in this

00:03:17,000 --> 00:03:20,210
disorders say it could be reorder it

00:03:19,100 --> 00:03:22,850
could be lost you couldn't quite

00:03:20,210 --> 00:03:25,070
determine what to do and a lot of time

00:03:22,850 --> 00:03:28,150
if you didn't come in active as you

00:03:25,070 --> 00:03:31,880
would resort to pan out but this

00:03:28,150 --> 00:03:34,520
approach has effectively reduce this

00:03:31,880 --> 00:03:37,850
timeout by a large percentage and we

00:03:34,520 --> 00:03:39,590
have to find our C draft there in numb

00:03:37,850 --> 00:03:42,140
those rights and you can look at the

00:03:39,590 --> 00:03:44,990
details but now I really hope you got

00:03:42,140 --> 00:03:47,210
what packets should be retransmitted and

00:03:44,990 --> 00:03:49,820
then we have to go out the congestion

00:03:47,210 --> 00:03:52,960
control how fast do I want to send all

00:03:49,820 --> 00:03:55,910
this retransmission or the new packets

00:03:52,960 --> 00:03:57,560
and that's what the jacket congestion

00:03:55,910 --> 00:03:59,930
control do and that's why we spent a lot

00:03:57,560 --> 00:04:02,210
of time decoupling the two of like a

00:03:59,930 --> 00:04:04,490
loaf recovery module and the congestion

00:04:02,210 --> 00:04:07,010
control module and people talk about

00:04:04,490 --> 00:04:10,430
contention control then the skyways

00:04:07,010 --> 00:04:13,370
since description and what is congestion

00:04:10,430 --> 00:04:14,920
right and we all start with this oh god

00:04:13,370 --> 00:04:18,269
for the simple you have to stay

00:04:14,920 --> 00:04:21,790
and the receiver consumer bottleneck and

00:04:18,269 --> 00:04:23,950
then as you sell faster and faster when

00:04:21,790 --> 00:04:26,530
you finally reach the pollen equates a

00:04:23,950 --> 00:04:29,560
hundred megabits per second the queue

00:04:26,530 --> 00:04:31,600
starts to form and in the meantime you

00:04:29,560 --> 00:04:34,030
know the ants are still are coming back

00:04:31,600 --> 00:04:36,340
and you feel that you know the spacing

00:04:34,030 --> 00:04:38,620
of the ad that kind of give you a sense

00:04:36,340 --> 00:04:40,870
of how fast the paper the packets are

00:04:38,620 --> 00:04:43,120
being delivered so this is standard

00:04:40,870 --> 00:04:46,720
maple one completely nothing fancy about

00:04:43,120 --> 00:04:49,900
it and then at this time if you actually

00:04:46,720 --> 00:04:53,020
looked at set of this anomic packets in

00:04:49,900 --> 00:04:56,979
flight and its relation to the believer

00:04:53,020 --> 00:04:59,350
rate initially when you start you know I

00:04:56,979 --> 00:05:01,300
saying you crank up the rate and then

00:04:59,350 --> 00:05:03,250
the way will keep increasing until you

00:05:01,300 --> 00:05:05,410
reach the hundred megabits per second

00:05:03,250 --> 00:05:08,530
right and then the the real kind of

00:05:05,410 --> 00:05:11,320
cocktails and everything excess you send

00:05:08,530 --> 00:05:13,900
into the networks just comes into queues

00:05:11,320 --> 00:05:17,740
so the amount of employee will continue

00:05:13,900 --> 00:05:20,020
to grow until it hits the buffer size

00:05:17,740 --> 00:05:25,360
and then you get all this you know

00:05:20,020 --> 00:05:32,380
packet losses due to overflow and in the

00:05:25,360 --> 00:05:35,260
meantime if you look at the RTT that

00:05:32,380 --> 00:05:37,450
they had to add time right the RTV

00:05:35,260 --> 00:05:40,630
initially starts when you start sending

00:05:37,450 --> 00:05:42,010
a then we max out the tonic rate

00:05:40,630 --> 00:05:44,320
the other thing will sort of be

00:05:42,010 --> 00:05:49,020
essentially the two-way propagation

00:05:44,320 --> 00:05:51,850
delay and it's an it's remains constant

00:05:49,020 --> 00:05:53,350
until the bottleneck is saturated and

00:05:51,850 --> 00:05:58,270
the queue starts to build and then the

00:05:53,350 --> 00:06:00,880
RTT just start to shoot up and that's

00:05:58,270 --> 00:06:03,310
where we start building this new

00:06:00,880 --> 00:06:06,100
position protocol where people heard

00:06:03,310 --> 00:06:08,169
about his call bbr but before we talk

00:06:06,100 --> 00:06:10,770
about VBR this look at what the current

00:06:08,169 --> 00:06:13,289
set of default cubic or

00:06:10,770 --> 00:06:17,400
we know this last base congestion

00:06:13,289 --> 00:06:20,039
control works the way they work is that

00:06:17,400 --> 00:06:23,879
they will at the regime where when you

00:06:20,039 --> 00:06:27,479
receive a packet loss as a signal and

00:06:23,879 --> 00:06:29,430
then you reacts to that right and the

00:06:27,479 --> 00:06:33,770
problem is that is that the operating

00:06:29,430 --> 00:06:36,360
point of that is on the far right edge

00:06:33,770 --> 00:06:39,180
the course I will you reach the paper

00:06:36,360 --> 00:06:43,789
size and then you take some actions to

00:06:39,180 --> 00:06:43,789
try to keep the congestion control

00:06:46,069 --> 00:06:54,750
problems sink or pocketbook

00:06:52,319 --> 00:06:57,599
right the buffer can be very much bigger

00:06:54,750 --> 00:07:00,419
than the BGP this person really drawn to

00:06:57,599 --> 00:07:03,090
the scale in reality we have seen that

00:07:00,419 --> 00:07:06,030
the paper size is hundred times bigger

00:07:03,090 --> 00:07:09,120
than the PDP and when you have that much

00:07:06,030 --> 00:07:11,940
of imply in the network sentence with

00:07:09,120 --> 00:07:14,520
your RTG that goes just into a zebra

00:07:11,940 --> 00:07:17,909
which is we're talking about tens of

00:07:14,520 --> 00:07:20,880
seconds sometimes so that one comment

00:07:17,909 --> 00:07:25,110
because is this huge amount of Q in the

00:07:20,880 --> 00:07:27,810
network and then they counter side of

00:07:25,110 --> 00:07:31,349
this problem is what in packet loss is

00:07:27,810 --> 00:07:34,710
not caused by this buffer overflow or

00:07:31,349 --> 00:07:36,990
not caused by a persistent queue but so

00:07:34,710 --> 00:07:40,050
for shallow Packers we have used it out

00:07:36,990 --> 00:07:42,569
to get that magnitude time the losses

00:07:40,050 --> 00:07:45,150
are actually cubed by coronated

00:07:42,569 --> 00:07:48,270
traffickers but lots of friends are

00:07:45,150 --> 00:07:50,400
sending a small town of say couple

00:07:48,270 --> 00:07:51,880
kilobytes for a lot of communion and

00:07:50,400 --> 00:07:54,610
then our causes

00:07:51,880 --> 00:07:57,220
this losses once this face actually

00:07:54,610 --> 00:07:59,980
flows through the network then the

00:07:57,220 --> 00:08:02,260
condition is gone and if you think that

00:07:59,980 --> 00:08:04,480
the way we thought could be works is

00:08:02,260 --> 00:08:07,530
every time he hit the loss then she cut

00:08:04,480 --> 00:08:11,290
the rate by you know either half or like

00:08:07,530 --> 00:08:16,030
20% and he only takes a few losses as a

00:08:11,290 --> 00:08:19,330
first signal to get the rate way down to

00:08:16,030 --> 00:08:22,780
less than the bottleneck link wait then

00:08:19,330 --> 00:08:25,600
this famous published by Matt Matt is

00:08:22,780 --> 00:08:28,150
saying that which we not kidding

00:08:25,600 --> 00:08:31,150
essentially the suport is sort of the

00:08:28,150 --> 00:08:34,330
inverse square root of Lovett so that

00:08:31,150 --> 00:08:38,590
means is that imagine you have a 10 Gbps

00:08:34,330 --> 00:08:41,260
make a hundred millisecond RTT you can

00:08:38,590 --> 00:08:43,960
only this one packet of 30 million

00:08:41,260 --> 00:08:46,990
packets in order to achieve that rate

00:08:43,960 --> 00:08:49,480
and it's basically very very difficult

00:08:46,990 --> 00:08:54,010
to operate a network that has such low

00:08:49,480 --> 00:08:56,590
drops so what we would like to do is to

00:08:54,010 --> 00:08:59,010
rework in this kind of graph right this

00:08:56,590 --> 00:09:02,490
applies to any network not just a

00:08:59,010 --> 00:09:05,560
particular network one will be the ideal

00:09:02,490 --> 00:09:10,660
operating point that we would like to

00:09:05,560 --> 00:09:16,000
operate at and sorry I'm getting some

00:09:10,660 --> 00:09:24,910
technical problems it's not moving it's

00:09:16,000 --> 00:09:28,560
asking me to install some update but I

00:09:24,910 --> 00:09:28,560
can change that right

00:09:36,320 --> 00:09:42,030
I woke up okay

00:09:40,110 --> 00:09:44,940
so the idea of waiting point I think

00:09:42,030 --> 00:09:47,880
it's pretty intuitive is we want to

00:09:44,940 --> 00:09:49,770
maximize the bandwidth while minimizing

00:09:47,880 --> 00:09:53,850
the kills and that's where the red

00:09:49,770 --> 00:09:56,430
circle is about and there is actually

00:09:53,850 --> 00:10:00,690
miss medical crews about that by Gail

00:09:56,430 --> 00:10:03,470
and Glenrock now in the 1980s that shows

00:10:00,690 --> 00:10:07,470
that this is called an echo power and

00:10:03,470 --> 00:10:10,380
when you have reached this side for any

00:10:07,470 --> 00:10:13,380
flow which is a local optimum then

00:10:10,380 --> 00:10:15,420
globally the network which is optimal

00:10:13,380 --> 00:10:17,820
state because everyone is operating at

00:10:15,420 --> 00:10:19,560
the maximum seeing their bottleneck

00:10:17,820 --> 00:10:21,360
penguins who are minimizing the local

00:10:19,560 --> 00:10:25,500
qubit and this can add up to a global

00:10:21,360 --> 00:10:27,570
optimum and now the question is how do

00:10:25,500 --> 00:10:31,080
we get to the optimal point without

00:10:27,570 --> 00:10:33,960
relying on say the losses and this is

00:10:31,080 --> 00:10:37,460
what my colleague Nick I will show you

00:10:33,960 --> 00:10:37,460
how Billy I will approach that

00:10:46,450 --> 00:10:53,360
see if this works yeah yeah so wrong

00:10:50,870 --> 00:10:57,530
let's see do I just hit the arrow key

00:10:53,360 --> 00:10:59,390
here yeah so let's go back one thing so

00:10:57,530 --> 00:11:02,360
then you the first thing we notice here

00:10:59,390 --> 00:11:05,360
is that the this optimal operating point

00:11:02,360 --> 00:11:06,950
here is at the point where the amount of

00:11:05,360 --> 00:11:09,140
data that you have in flight in the

00:11:06,950 --> 00:11:12,620
network is equal to something called the

00:11:09,140 --> 00:11:15,050
bdp or a bandwidth delay product now the

00:11:12,620 --> 00:11:16,640
BGP the bandwidth delay product is

00:11:15,050 --> 00:11:20,390
literally just the bottleneck bandwidth

00:11:16,640 --> 00:11:24,400
times the round-trip propagation delay

00:11:20,390 --> 00:11:27,890
of the path and so we want to talk about

00:11:24,400 --> 00:11:29,810
how we can estimate where that optimal

00:11:27,890 --> 00:11:31,370
point is we basically need to figure out

00:11:29,810 --> 00:11:33,430
what the bottleneck bandwidth is and

00:11:31,370 --> 00:11:36,650
what the round-trip propagation delay is

00:11:33,430 --> 00:11:39,350
so one straightforward way to estimate

00:11:36,650 --> 00:11:41,870
these things is basically using some

00:11:39,350 --> 00:11:44,120
simple filtering so if we first want to

00:11:41,870 --> 00:11:47,300
find out the two-way propagation delay

00:11:44,120 --> 00:11:49,280
of the network based on the graph at the

00:11:47,300 --> 00:11:50,360
top you can sort of see you know in the

00:11:49,280 --> 00:11:52,370
real world you're going to get a bunch

00:11:50,360 --> 00:11:54,080
of round-trip time samples that are

00:11:52,370 --> 00:11:56,510
these gray dots that are sort of above

00:11:54,080 --> 00:11:58,460
the constraint line and one simple way

00:11:56,510 --> 00:12:00,620
to get a pretty decent estimate of the

00:11:58,460 --> 00:12:03,589
round-trip propagation delay is just to

00:12:00,620 --> 00:12:05,450
run a min filter over the recent window

00:12:03,589 --> 00:12:07,610
of a samples that you've received over

00:12:05,450 --> 00:12:08,990
some recent time period and that'll give

00:12:07,610 --> 00:12:11,330
you a reasonable estimate for the

00:12:08,990 --> 00:12:12,440
two-way propagation delay and of course

00:12:11,330 --> 00:12:15,110
finally if you want to know the

00:12:12,440 --> 00:12:16,820
bandwidth of the path you know you can

00:12:15,110 --> 00:12:19,040
take a look at the delivery rate samples

00:12:16,820 --> 00:12:21,380
that you get based on all the

00:12:19,040 --> 00:12:22,820
acknowledgments you're receiving and you

00:12:21,380 --> 00:12:24,410
can take a look at those delivery rate

00:12:22,820 --> 00:12:26,690
samples and they're going to look like

00:12:24,410 --> 00:12:28,640
these gray dots on the bandwidth graph

00:12:26,690 --> 00:12:31,280
here and so one way to get a reasonable

00:12:28,640 --> 00:12:34,520
estimate of the available bandwidth for

00:12:31,280 --> 00:12:36,320
this flow is to take a max filter that's

00:12:34,520 --> 00:12:39,800
winded over the some recent number of

00:12:36,320 --> 00:12:41,930
round-trip times to figure out the an

00:12:39,800 --> 00:12:46,070
estimate for the bandwidth available to

00:12:41,930 --> 00:12:48,950
the flow so so that's great but one

00:12:46,070 --> 00:12:50,370
interesting problem with this is that if

00:12:48,950 --> 00:12:51,930
you want to see both the max

00:12:50,370 --> 00:12:54,210
bandwidth and the minimum round-trip

00:12:51,930 --> 00:12:56,940
time it turns out you need to spend some

00:12:54,210 --> 00:12:59,820
time on both sides of that optimal point

00:12:56,940 --> 00:13:01,380
because over on the left side only the

00:12:59,820 --> 00:13:03,270
minimum round-trip time is visible

00:13:01,380 --> 00:13:05,460
because you've only got enough because

00:13:03,270 --> 00:13:07,830
you don't really have enough packets in

00:13:05,460 --> 00:13:10,140
flight to actually fully utilize the

00:13:07,830 --> 00:13:11,820
bandwidth of the path and then

00:13:10,140 --> 00:13:14,610
interestingly on the other side of the

00:13:11,820 --> 00:13:17,370
bdp if you've got enough data in flight

00:13:14,610 --> 00:13:19,050
that you're sure you've filled the pipe

00:13:17,370 --> 00:13:20,880
then there's going to be at least a

00:13:19,050 --> 00:13:22,350
little bit of queue there that's going

00:13:20,880 --> 00:13:24,420
to be essentially hiding the two a

00:13:22,350 --> 00:13:26,339
propagation delay from you so if you

00:13:24,420 --> 00:13:28,050
want to understand both parameters you

00:13:26,339 --> 00:13:33,570
actually need to spend a little time on

00:13:28,050 --> 00:13:37,140
each side of that bdp point so so

00:13:33,570 --> 00:13:40,380
putting this all together one reasonable

00:13:37,140 --> 00:13:41,580
way to try to stay near this optimal

00:13:40,380 --> 00:13:44,520
point when are we getting maximum

00:13:41,580 --> 00:13:46,440
bandwidth and minimum delay it kind of

00:13:44,520 --> 00:13:48,390
looks like this so first we try to build

00:13:46,440 --> 00:13:50,730
an explicit model of the network path

00:13:48,390 --> 00:13:52,260
that has both important parameters that

00:13:50,730 --> 00:13:54,930
has both the borrowing bandwidth

00:13:52,260 --> 00:13:55,980
estimate and the estimate of the two-way

00:13:54,930 --> 00:13:58,529
propagation delay

00:13:55,980 --> 00:13:59,850
and we can update those on every act

00:13:58,529 --> 00:14:02,279
because when they react we're going to

00:13:59,850 --> 00:14:05,029
get an RTT sample and we've added code

00:14:02,279 --> 00:14:08,190
to also get a delivery rate simple and

00:14:05,029 --> 00:14:10,800
once you've got your model then you can

00:14:08,190 --> 00:14:13,620
use that model to control how fast you

00:14:10,800 --> 00:14:14,850
send and interestingly you need to do at

00:14:13,620 --> 00:14:18,150
least a couple different things that

00:14:14,850 --> 00:14:19,470
seems to make this work so as I just

00:14:18,150 --> 00:14:22,470
mentioned you're going to need to probe

00:14:19,470 --> 00:14:24,990
both the maximum bandwidth which with in

00:14:22,470 --> 00:14:27,480
flight above bdp and you're also going

00:14:24,990 --> 00:14:30,089
to need to probe the minimum round-trip

00:14:27,480 --> 00:14:31,740
time to get an estimate of the two-way

00:14:30,089 --> 00:14:32,910
propagation delay and that means you're

00:14:31,740 --> 00:14:36,839
going to have to spend at least a little

00:14:32,910 --> 00:14:39,209
time with your in flat below the bdp and

00:14:36,839 --> 00:14:41,730
second to get this to work well you want

00:14:39,209 --> 00:14:43,680
a pace at a rate so that you're sending

00:14:41,730 --> 00:14:45,390
packets into the network at a rate

00:14:43,680 --> 00:14:47,310
that's close to the bottom like

00:14:45,390 --> 00:14:49,170
bandwidth which is going to reduce the

00:14:47,310 --> 00:14:52,260
amount of queuing you create or do C Mon

00:14:49,170 --> 00:14:54,089
packet loss and then third to make this

00:14:52,260 --> 00:14:54,640
all fit together you're going to need to

00:14:54,089 --> 00:14:57,970
vary

00:14:54,640 --> 00:15:00,730
to keep the amount of data in flight in

00:14:57,970 --> 00:15:05,020
the network close to the BDP so that you

00:15:00,730 --> 00:15:06,850
get a full pipe but small queues so if

00:15:05,020 --> 00:15:09,580
we put all this together then that's

00:15:06,850 --> 00:15:12,670
basically bbr congestion control in a

00:15:09,580 --> 00:15:14,290
nutshell and bbr stands for a bottleneck

00:15:12,670 --> 00:15:18,070
bandwidth and round-trip propagation

00:15:14,290 --> 00:15:21,790
time so our team at Google including Ben

00:15:18,070 --> 00:15:24,580
Jacobson and Yoochun and myself we've

00:15:21,790 --> 00:15:26,920
been working on this and we've submitted

00:15:24,580 --> 00:15:30,970
the code it's in net next right now

00:15:26,920 --> 00:15:33,640
slated for 4.9 and we got a paper coming

00:15:30,970 --> 00:15:35,530
out in the next week or two in ACM queue

00:15:33,640 --> 00:15:38,620
getting some more details if you're

00:15:35,530 --> 00:15:41,080
still interested after this talk so if

00:15:38,620 --> 00:15:43,930
you had to summarize bbr in one sentence

00:15:41,080 --> 00:15:46,300
I think we could say that bbr seeks high

00:15:43,930 --> 00:15:48,940
throughput with a small Q by

00:15:46,300 --> 00:15:51,310
sequentially probing both the bottleneck

00:15:48,940 --> 00:15:54,460
bandwidth and the round-trip time to

00:15:51,310 --> 00:15:55,900
build a model so what does that look

00:15:54,460 --> 00:15:57,940
like so let's go back to our favorite

00:15:55,900 --> 00:15:59,920
graph here so we're gonna we're gonna

00:15:57,940 --> 00:16:01,750
build a model and we're gonna try to

00:15:59,920 --> 00:16:03,520
stay near the maximum bandwidth and the

00:16:01,750 --> 00:16:05,380
minimum round-trip time we're gonna try

00:16:03,520 --> 00:16:08,470
to stay as close as possible to this

00:16:05,380 --> 00:16:12,280
optimal operating point which is right

00:16:08,470 --> 00:16:15,220
around the BT P so to make this work PDR

00:16:12,280 --> 00:16:17,290
has a state machine and every connection

00:16:15,220 --> 00:16:19,390
starts out in the middle that we call

00:16:17,290 --> 00:16:22,030
start-up where it's basically trying to

00:16:19,390 --> 00:16:23,380
rapidly probe the bandwidth available in

00:16:22,030 --> 00:16:26,860
the network so that we can we can

00:16:23,380 --> 00:16:30,250
quickly reach full utilization so what

00:16:26,860 --> 00:16:32,860
bbr does is it starts with the whatever

00:16:30,250 --> 00:16:34,540
initial congestion window the TCP stack

00:16:32,860 --> 00:16:37,570
thinks is reasonable sort of an

00:16:34,540 --> 00:16:39,910
orthogonal problem and then it basically

00:16:37,570 --> 00:16:43,780
done is it's sending rate every

00:16:39,910 --> 00:16:45,850
round-trip time as long as the delivery

00:16:43,780 --> 00:16:48,000
rate is also doubling every round-trip

00:16:45,850 --> 00:16:50,070
time and then the deadly

00:16:48,000 --> 00:16:52,920
time of course is is very similar to the

00:16:50,070 --> 00:16:55,740
behavior you get with a traditional tcp

00:16:52,920 --> 00:16:57,210
slow-start where the interests were the

00:16:55,740 --> 00:17:01,860
difference where the difference is more

00:16:57,210 --> 00:17:03,420
interesting is that as bbr is doing this

00:17:01,860 --> 00:17:04,980
is looking at what's happening to the

00:17:03,420 --> 00:17:07,050
delivery rate and it's only going to

00:17:04,980 --> 00:17:09,329
double its sending rate as long as it

00:17:07,050 --> 00:17:12,120
sees that the delivery rate is also

00:17:09,329 --> 00:17:14,610
exponentially growing once it sees that

00:17:12,120 --> 00:17:16,620
the bandwidth samples that it's getting

00:17:14,610 --> 00:17:19,050
are actually they've actually stopped

00:17:16,620 --> 00:17:21,720
growing exponentially then it estimates

00:17:19,050 --> 00:17:24,000
that the pipe is probably full and it

00:17:21,720 --> 00:17:27,660
switches to the next mode in its state

00:17:24,000 --> 00:17:30,420
machine which we call drain because it's

00:17:27,660 --> 00:17:34,260
job is basically to drain the queue that

00:17:30,420 --> 00:17:37,500
is inevitably created if the first

00:17:34,260 --> 00:17:40,050
minutes is filling the pipe which it

00:17:37,500 --> 00:17:42,180
almost always does so the idea is that

00:17:40,050 --> 00:17:43,980
when this mode starts out there's going

00:17:42,180 --> 00:17:46,860
to be some amount of queue so the drain

00:17:43,980 --> 00:17:49,680
mode uses a pacing rate that's below the

00:17:46,860 --> 00:17:52,650
estimated bottleneck bandwidth so that

00:17:49,680 --> 00:17:55,350
gradually the the amount of data in fly

00:17:52,650 --> 00:17:58,350
decreases and once the amount of data in

00:17:55,350 --> 00:18:01,050
flight has decreased to the estimated

00:17:58,350 --> 00:18:03,840
bdp we estimate that the pipe is full

00:18:01,050 --> 00:18:08,010
but the queue is very small and then we

00:18:03,840 --> 00:18:11,130
can proceed on to the next state in the

00:18:08,010 --> 00:18:13,110
state machine which we call BW because

00:18:11,130 --> 00:18:15,240
it's it's got a couple of jobs here but

00:18:13,110 --> 00:18:18,000
the main job of course is to explore

00:18:15,240 --> 00:18:20,820
bandwidth to see how much more bandwidth

00:18:18,000 --> 00:18:22,500
is available if any and then after that

00:18:20,820 --> 00:18:24,570
it tries to drain the queue that might

00:18:22,500 --> 00:18:27,000
be created by that process and then it

00:18:24,570 --> 00:18:29,250
tries to cruise with high utilization

00:18:27,000 --> 00:18:33,630
and a little queue so how does this work

00:18:29,250 --> 00:18:35,550
so the first this works in a sort of

00:18:33,630 --> 00:18:37,350
cycle and it goes through a couple that

00:18:35,550 --> 00:18:39,210
goes through each of these phases that

00:18:37,350 --> 00:18:43,080
we just described so in the first phase

00:18:39,210 --> 00:18:45,780
of the cycle we start out somewhere near

00:18:43,080 --> 00:18:46,970
the BDP and then what we need to do is

00:18:45,780 --> 00:18:50,599
briefly for

00:18:46,970 --> 00:18:52,849
estimated round-trip time we use a

00:18:50,599 --> 00:18:55,639
pacing rate that's a small multiple of

00:18:52,849 --> 00:18:58,070
the bottleneck bandwidth so we use right

00:18:55,639 --> 00:19:00,259
now we're using 1.25 times the estimated

00:18:58,070 --> 00:19:02,809
bottleneck bandwidth for about one

00:19:00,259 --> 00:19:04,700
round-trip time and what that does is

00:19:02,809 --> 00:19:06,619
since we're sending faster than the

00:19:04,700 --> 00:19:10,519
network is delivering the amount of data

00:19:06,619 --> 00:19:11,779
in flight is going to increase now what

00:19:10,519 --> 00:19:13,549
the two things will happen here either

00:19:11,779 --> 00:19:16,580
there's more bandwidth that really is

00:19:13,549 --> 00:19:18,409
available in which case since were max

00:19:16,580 --> 00:19:20,269
filtering the bandwidth samples to get

00:19:18,409 --> 00:19:22,070
our bandwidth estimate we will

00:19:20,269 --> 00:19:24,739
immediately notice that and that would

00:19:22,070 --> 00:19:26,179
be our new bandwidth estimate and it

00:19:24,739 --> 00:19:28,419
will increase this from the baseline

00:19:26,179 --> 00:19:31,789
rate at which we're sending immediately

00:19:28,419 --> 00:19:34,070
on the other hand if we don't have any

00:19:31,789 --> 00:19:35,450
more bandwidth available then the fact

00:19:34,070 --> 00:19:37,460
that we sound faster than the available

00:19:35,450 --> 00:19:40,729
bandwidth will have created a little

00:19:37,460 --> 00:19:42,679
queue in the network and bbr tries to

00:19:40,729 --> 00:19:46,249
run with a low Q so the next thing we do

00:19:42,679 --> 00:19:49,159
is to dissipate that queue we spent one

00:19:46,249 --> 00:19:50,659
round-trip time sending a way that's

00:19:49,159 --> 00:19:52,359
below us and we had a bottleneck

00:19:50,659 --> 00:19:55,460
bandwidth and specifically it's

00:19:52,359 --> 00:19:57,409
correspondingly below so we send it

00:19:55,460 --> 00:20:00,409
point seven five times the bottleneck

00:19:57,409 --> 00:20:03,249
bandwidth which will then drive the

00:20:00,409 --> 00:20:06,349
in-flight down back down toward the bdp

00:20:03,249 --> 00:20:09,259
and then once we've drained that queue

00:20:06,349 --> 00:20:13,279
if any then we spend some time cruising

00:20:09,259 --> 00:20:14,899
at essentially exactly the estimated

00:20:13,279 --> 00:20:17,090
bottleneck bandwidth which is going to

00:20:14,899 --> 00:20:21,070
attend to fully utilize the pipe but

00:20:17,090 --> 00:20:24,499
keep the queue nice and small so this is

00:20:21,070 --> 00:20:26,090
actually where a typical bbr flow at

00:20:24,499 --> 00:20:27,649
least one that lasts more than a few

00:20:26,090 --> 00:20:29,779
seconds is going to spend most of its

00:20:27,649 --> 00:20:34,099
time just probing the bandwidth and

00:20:29,779 --> 00:20:35,599
keeping the queue small now what

00:20:34,099 --> 00:20:37,429
everything I've described so far is

00:20:35,599 --> 00:20:39,739
about pushing out to the right of this

00:20:37,429 --> 00:20:42,440
curve keeping in flight a little higher

00:20:39,739 --> 00:20:45,049
than bdp or at bdp so that we can probe

00:20:42,440 --> 00:20:45,920
bandwidth and you throw the pipe but of

00:20:45,049 --> 00:20:48,260
course you've got

00:20:45,920 --> 00:20:50,630
this equation which is the model of the

00:20:48,260 --> 00:20:52,100
two-way propagation delay and to see

00:20:50,630 --> 00:20:53,870
that as we've mentioned you need to

00:20:52,100 --> 00:20:55,970
actually have your in flight below the

00:20:53,870 --> 00:21:00,440
bdp so there's sort of two interesting

00:20:55,970 --> 00:21:02,450
cases here right so our our TT estimate

00:21:00,440 --> 00:21:06,260
is based on a min filter that's running

00:21:02,450 --> 00:21:08,120
over currently 10 seconds so the

00:21:06,260 --> 00:21:10,070
interesting thing is that a lot of

00:21:08,120 --> 00:21:12,650
applications actually are going to be

00:21:10,070 --> 00:21:15,170
application limited or basically run out

00:21:12,650 --> 00:21:17,420
of data to send over the course of a 10

00:21:15,170 --> 00:21:20,780
second window right so when you download

00:21:17,420 --> 00:21:23,360
a webpage if you've got RPC traffic if

00:21:20,780 --> 00:21:24,950
you've got video traffic that's cut into

00:21:23,360 --> 00:21:27,620
chunks maybe because you want to change

00:21:24,950 --> 00:21:29,960
your a bit rate potentially so in all

00:21:27,620 --> 00:21:31,490
those cases you've got traffic that's

00:21:29,960 --> 00:21:34,760
basically going to have some period of

00:21:31,490 --> 00:21:36,710
silence pretty often so that that

00:21:34,760 --> 00:21:40,550
silence itself will tend to drain the

00:21:36,710 --> 00:21:43,100
queue and give the flow or a chance when

00:21:40,550 --> 00:21:44,750
it restarts for those first few packets

00:21:43,100 --> 00:21:47,930
to make it through the network and give

00:21:44,750 --> 00:21:49,880
you a good minimum RTT sample because

00:21:47,930 --> 00:21:52,250
those first couple of packets restarting

00:21:49,880 --> 00:21:55,610
after idle are going to have an inflight

00:21:52,250 --> 00:21:57,860
that's much lower than the BGP so so for

00:21:55,610 --> 00:22:00,590
many applications you don't need to do

00:21:57,860 --> 00:22:02,750
anything special at all for the mid RTT

00:22:00,590 --> 00:22:05,300
filter to sort of opportunistically pick

00:22:02,750 --> 00:22:07,640
up a good sample now if you do have a

00:22:05,300 --> 00:22:09,320
bulk transfer application that's going

00:22:07,640 --> 00:22:12,680
to be sending it full bore for the

00:22:09,320 --> 00:22:16,190
entire 10 second window then probe ITT

00:22:12,680 --> 00:22:19,880
mode is designed to notice this and on a

00:22:16,190 --> 00:22:22,340
sort of as needed basis when it sees

00:22:19,880 --> 00:22:25,670
that the men are TT estimate has not

00:22:22,340 --> 00:22:28,160
been matched or may better over the

00:22:25,670 --> 00:22:31,520
course of ten seconds then it briefly

00:22:28,160 --> 00:22:34,190
drops into this probe RT t mode and cuts

00:22:31,520 --> 00:22:36,950
the amount of data in flight to a sort

00:22:34,190 --> 00:22:39,890
of minimum value and then gets a good

00:22:36,950 --> 00:22:42,680
bit RTT sample and then flips the in

00:22:39,890 --> 00:22:46,020
flight back up to our VDP and goes back

00:22:42,680 --> 00:22:49,620
into pro pw state and the parameters of

00:22:46,020 --> 00:22:52,530
this mode are chosen so that the typical

00:22:49,620 --> 00:22:55,320
impact for a bulk transfer flow is small

00:22:52,530 --> 00:22:56,430
so obviously since in-flight is blow bdp

00:22:55,320 --> 00:22:58,860
there's going to be some brief

00:22:56,430 --> 00:23:00,930
underutilization but the parameters are

00:22:58,860 --> 00:23:05,100
chosen to typically limit that impact to

00:23:00,930 --> 00:23:07,260
about two percent so that's basically BB

00:23:05,100 --> 00:23:09,180
are in a nutshell and that's you know

00:23:07,260 --> 00:23:11,610
one way that a congestion controller can

00:23:09,180 --> 00:23:12,720
estimate a good rate at which to send

00:23:11,610 --> 00:23:14,310
packets through the network

00:23:12,720 --> 00:23:16,560
so once the congestion control has

00:23:14,310 --> 00:23:18,480
figured out a good bait then the next

00:23:16,560 --> 00:23:20,580
part of the picture analytics TCP

00:23:18,480 --> 00:23:24,590
architecture is that we hand off the

00:23:20,580 --> 00:23:27,630
packets to the packet scheduler so in

00:23:24,590 --> 00:23:29,550
little CCPD this part of the picture

00:23:27,630 --> 00:23:31,950
kind of looks like this now it starts

00:23:29,550 --> 00:23:34,440
out at the top and the TCP layer where

00:23:31,950 --> 00:23:36,510
you've got we've got the red and the

00:23:34,440 --> 00:23:38,130
green and a blue flow and here in the

00:23:36,510 --> 00:23:41,880
TCP layer we can see their light cues

00:23:38,130 --> 00:23:44,700
which are filled with maximally sized as

00:23:41,880 --> 00:23:48,870
kebabs or SK B's and so our first job is

00:23:44,700 --> 00:23:50,310
basically to pick out a burst size that

00:23:48,870 --> 00:23:52,260
we think is appropriate for this flow

00:23:50,310 --> 00:23:54,990
and that's the job of something called

00:23:52,260 --> 00:23:57,570
TSO auto sizing which basically looks at

00:23:54,990 --> 00:23:59,610
the send rate of the flow and picks out

00:23:57,570 --> 00:24:01,740
a burst size that makes sense when is

00:23:59,610 --> 00:24:03,720
proportional to the rate of the flow so

00:24:01,740 --> 00:24:06,780
flows that are half the gigabit or bot

00:24:03,720 --> 00:24:08,610
they get nice big sq B's 64 kilobytes

00:24:06,780 --> 00:24:11,490
for smaller flows they get

00:24:08,610 --> 00:24:13,680
proportionally smaller bursts so once

00:24:11,490 --> 00:24:15,720
you chopped off then sqb of a good size

00:24:13,680 --> 00:24:17,940
then you have another question which is

00:24:15,720 --> 00:24:19,770
okay and now a good time to hand this

00:24:17,940 --> 00:24:22,860
off to the lower layers or should I wait

00:24:19,770 --> 00:24:26,130
so the part of the system that decides

00:24:22,860 --> 00:24:27,690
that is called TCP small queues which is

00:24:26,130 --> 00:24:29,280
basically like a little flow control

00:24:27,690 --> 00:24:32,060
engine to limit the amount of data

00:24:29,280 --> 00:24:34,860
that's in the queues of the sending host

00:24:32,060 --> 00:24:37,590
and when it decides that now is a good

00:24:34,860 --> 00:24:39,810
time then TCP hands off the packet to

00:24:37,590 --> 00:24:43,020
the lower layers goes through IP and

00:24:39,810 --> 00:24:45,690
then to the cutest glare now the cutest

00:24:43,020 --> 00:24:47,640
layer the most interesting case and the

00:24:45,690 --> 00:24:52,050
most full-featured kiosk is one called

00:24:47,640 --> 00:24:54,060
FQ which actually offers two services

00:24:52,050 --> 00:24:54,410
basically the first layer that you hit

00:24:54,060 --> 00:24:56,990
is

00:24:54,410 --> 00:24:59,660
pacing layer which basically decides the

00:24:56,990 --> 00:25:01,850
appropriate release time for each skb

00:24:59,660 --> 00:25:04,850
based on the pacing rate set by the

00:25:01,850 --> 00:25:06,920
congestion control module and then once

00:25:04,850 --> 00:25:08,890
we've reached that release time then it

00:25:06,920 --> 00:25:11,780
releases it to the fair queuing

00:25:08,890 --> 00:25:14,570
component of F Q which is going to mix

00:25:11,780 --> 00:25:16,280
the packets from the different flows in

00:25:14,570 --> 00:25:19,040
a way that's fair on a byte by byte

00:25:16,280 --> 00:25:21,170
basis which reduces head-of-line

00:25:19,040 --> 00:25:23,270
blocking and also gives the mice a

00:25:21,170 --> 00:25:25,430
chance a better chance

00:25:23,270 --> 00:25:27,200
so what's fair queuing has decided it's

00:25:25,430 --> 00:25:30,730
ready it hands off the packet to the NIC

00:25:27,200 --> 00:25:33,800
and then it heads onto the wire so

00:25:30,730 --> 00:25:35,990
that's basically the system so if we we

00:25:33,800 --> 00:25:38,540
think about the the Ross recovery and

00:25:35,990 --> 00:25:40,880
congestion control and packet scheduling

00:25:38,540 --> 00:25:42,500
modules all together what does that bias

00:25:40,880 --> 00:25:47,180
these days what does that look like in

00:25:42,500 --> 00:25:49,880
terms of performance so first it allows

00:25:47,180 --> 00:25:52,040
us to fully utilize the bandwidth

00:25:49,880 --> 00:25:54,920
offered by the path even when there's

00:25:52,040 --> 00:25:57,080
high packet loss so this is just a graph

00:25:54,920 --> 00:25:59,960
to kind of illustrate the proper scaling

00:25:57,080 --> 00:26:01,970
properties now so on the y-axis we have

00:25:59,960 --> 00:26:06,020
good put here in megabytes per second on

00:26:01,970 --> 00:26:08,030
the x-axis loss rate in percent we have

00:26:06,020 --> 00:26:09,620
none left we've got a thousandth of a

00:26:08,030 --> 00:26:12,320
percent we over on the right fifty

00:26:09,620 --> 00:26:16,130
percent so this is comparing bbr and

00:26:12,320 --> 00:26:19,340
cubic for a bulk test with one flow and

00:26:16,130 --> 00:26:21,380
this had a 100 megabit bottleneck with

00:26:19,340 --> 00:26:23,090
100 millisecond round-trip time but the

00:26:21,380 --> 00:26:25,790
curve is going to look pretty similar no

00:26:23,090 --> 00:26:27,880
matter what the the bandwidth so the

00:26:25,790 --> 00:26:29,720
first thing you notice here is this

00:26:27,880 --> 00:26:32,270
property of Ross based congestion

00:26:29,720 --> 00:26:33,590
control including cubic that you ton

00:26:32,270 --> 00:26:36,800
mentioned where you need a really low

00:26:33,590 --> 00:26:39,620
loss rate to get a good throughput with

00:26:36,800 --> 00:26:42,590
loss based congestion control and in

00:26:39,620 --> 00:26:45,220
fact but the other thing you notice is

00:26:42,590 --> 00:26:48,110
the PBR is able to achieve the full

00:26:45,220 --> 00:26:51,500
bandwidth offered by the path add up to

00:26:48,110 --> 00:26:53,470
about 5% and even up to about 15% you

00:26:51,500 --> 00:26:55,550
get the maximum theoretically possible

00:26:53,470 --> 00:26:57,980
given that there is some packet loss

00:26:55,550 --> 00:27:00,650
happening and above that of course they

00:26:57,980 --> 00:27:02,759
both do poorly but the the place at

00:27:00,650 --> 00:27:04,769
which PBR is to put falls off

00:27:02,759 --> 00:27:06,389
should be noted is actually a design

00:27:04,769 --> 00:27:09,629
parameter it's not a fundamental

00:27:06,389 --> 00:27:11,240
property of that algorithm so what does

00:27:09,629 --> 00:27:14,850
this look like a sort of typical

00:27:11,240 --> 00:27:18,210
realistic numbers so if we if we

00:27:14,850 --> 00:27:19,679
consider this sort of 10 gigabit biomech

00:27:18,210 --> 00:27:22,619
with a hundred millisecond round-trip

00:27:19,679 --> 00:27:24,360
time and a 1% loss rate which is kind of

00:27:22,619 --> 00:27:26,639
realistic for what we see with

00:27:24,360 --> 00:27:28,889
commodities Shama buffered switches on

00:27:26,639 --> 00:27:31,169
their backbone cubic is going to give

00:27:28,889 --> 00:27:33,840
you about three point three megabits on

00:27:31,169 --> 00:27:37,110
a path like that bbr is going to give

00:27:33,840 --> 00:27:39,240
you about 9.1 gigabits so it's about a

00:27:37,110 --> 00:27:42,179
three order of magnitude difference in

00:27:39,240 --> 00:27:45,539
the bandwidth that you can see in a case

00:27:42,179 --> 00:27:52,139
like this with shower buffers giving you

00:27:45,539 --> 00:27:54,629
these non congestive losses so so the

00:27:52,139 --> 00:27:58,799
next thing that that is interesting is

00:27:54,629 --> 00:28:02,190
the picture now on last mile style links

00:27:58,799 --> 00:28:04,350
so what this new architecture buys you

00:28:02,190 --> 00:28:06,990
is the ability to have low queuing delay

00:28:04,350 --> 00:28:07,619
despite buffers that might be really

00:28:06,990 --> 00:28:10,049
really bloated

00:28:07,619 --> 00:28:15,210
so here the graph is just showing the

00:28:10,049 --> 00:28:17,610
medium RTT latency on the y-axis and on

00:28:15,210 --> 00:28:19,710
the x-axis we have the buffer size sort

00:28:17,610 --> 00:28:22,220
of ranging from 100 kilobytes over in

00:28:19,710 --> 00:28:23,700
left to 10 megabytes over on the right

00:28:22,220 --> 00:28:25,919
again

00:28:23,700 --> 00:28:27,929
the graph is going to look the same no

00:28:25,919 --> 00:28:29,789
matter what the the bandwidth in this

00:28:27,929 --> 00:28:32,909
case it happened to be we were looking

00:28:29,789 --> 00:28:33,389
at a sort of 2g style bandwidth with 128

00:28:32,909 --> 00:28:36,749
kilobits

00:28:33,389 --> 00:28:38,519
bottleneck and here we had 8 flows

00:28:36,749 --> 00:28:40,559
running for a while to give a sense of

00:28:38,519 --> 00:28:44,159
what happens but of course what happens

00:28:40,559 --> 00:28:48,059
is as you noted while space congestion

00:28:44,159 --> 00:28:50,669
control operates by basically filling

00:28:48,059 --> 00:28:52,799
the buffers and just riding there so as

00:28:50,669 --> 00:28:54,539
you increase the size of your buffers

00:28:52,799 --> 00:28:58,139
the queuing delay that you're going to

00:28:54,539 --> 00:29:00,269
see in a FIFO queue just grows in

00:28:58,139 --> 00:29:03,149
proportion with that buffer size if

00:29:00,269 --> 00:29:06,119
you're running cubic by contrast bbr

00:29:03,149 --> 00:29:07,980
because it builds an explicit model of

00:29:06,119 --> 00:29:09,419
the amount of data in flight that you

00:29:07,980 --> 00:29:10,429
really need of course

00:29:09,419 --> 00:29:13,149
that's independent of the

00:29:10,429 --> 00:29:15,799
Versailles that has to do with the

00:29:13,149 --> 00:29:18,230
bandwidth and the round-trip propagation

00:29:15,799 --> 00:29:20,149
delay which doesn't change so the amount

00:29:18,230 --> 00:29:22,279
of data in flight doesn't change in the

00:29:20,149 --> 00:29:24,889
case of the PPR so it can give you some

00:29:22,279 --> 00:29:28,399
nice load delays so for a typical

00:29:24,889 --> 00:29:31,820
example might be a 10 megabit last mile

00:29:28,399 --> 00:29:33,919
link with a 40 millisecond trip time in

00:29:31,820 --> 00:29:36,289
a case like that PBR is going to give

00:29:33,919 --> 00:29:38,539
you a median RTT of about 43

00:29:36,289 --> 00:29:40,700
milliseconds and Kubik is going to give

00:29:38,539 --> 00:29:44,629
you an RTT median RTT of about one

00:29:40,700 --> 00:29:50,600
second so it's about a 25 X improvement

00:29:44,629 --> 00:29:53,299
in latency so those were just to

00:29:50,600 --> 00:29:56,330
illustrate the basic properties we've

00:29:53,299 --> 00:29:59,330
actually deployed bbr for all of the TCP

00:29:56,330 --> 00:30:01,669
traffic running on Google's b4 Network

00:29:59,330 --> 00:30:04,309
which is a backbone network that

00:30:01,669 --> 00:30:06,049
connects the Google Data Centers and

00:30:04,309 --> 00:30:10,070
carries the majority of traffic between

00:30:06,049 --> 00:30:11,509
Google Data Centers these days so and

00:30:10,070 --> 00:30:15,440
where's the performance look like with

00:30:11,509 --> 00:30:18,669
that so on with PBR we see about two

00:30:15,440 --> 00:30:21,259
times 220 times the performance of cubic

00:30:18,669 --> 00:30:23,419
and it turns out that actually right now

00:30:21,259 --> 00:30:26,240
the BBU our performance is basically

00:30:23,419 --> 00:30:28,159
limited by the maximum receive window

00:30:26,240 --> 00:30:32,419
that's been configured on these hosts

00:30:28,159 --> 00:30:34,220
and if you go in and and have you done

00:30:32,419 --> 00:30:37,190
enough machines and do some experiments

00:30:34,220 --> 00:30:39,200
we found that basically PBR once you've

00:30:37,190 --> 00:30:41,090
lifted that receive window limit it gets

00:30:39,200 --> 00:30:45,190
about a hundred times the debt with

00:30:41,090 --> 00:30:49,340
cubic guts on this network

00:30:45,190 --> 00:30:51,049
so in conclusion let me switch to our

00:30:49,340 --> 00:30:54,710
new conclusion sleds there we go

00:30:51,049 --> 00:30:56,570
in conclusion there been changes above

00:30:54,710 --> 00:31:00,590
the algorithmic level and the

00:30:56,570 --> 00:31:02,529
architectural level in Linux TCP as its

00:31:00,590 --> 00:31:05,179
evolved over the past couple years and

00:31:02,529 --> 00:31:06,379
in congestion control

00:31:05,179 --> 00:31:09,289
we've got a new congestion control

00:31:06,379 --> 00:31:10,790
algorithm that's available that tries to

00:31:09,289 --> 00:31:12,650
maximize bandwidth

00:31:10,790 --> 00:31:15,080
minimize the round-trip time by building

00:31:12,650 --> 00:31:17,690
an explicit model instead of being lost

00:31:15,080 --> 00:31:19,540
base and in the last recovery world

00:31:17,690 --> 00:31:23,120
we've got a new algorithm rack that

00:31:19,540 --> 00:31:25,580
basically uses time based reasoning to

00:31:23,120 --> 00:31:28,010
work more generally and to do a better

00:31:25,580 --> 00:31:30,470
job at recovering all offices or most

00:31:28,010 --> 00:31:32,540
losses in in one round-trip time and

00:31:30,470 --> 00:31:34,640
this is all of course based on a

00:31:32,540 --> 00:31:37,250
foundation of a high performance packet

00:31:34,640 --> 00:31:39,230
scheduler and we should give a shout out

00:31:37,250 --> 00:31:42,200
to Eric dmoz a who wrote basically all

00:31:39,230 --> 00:31:45,530
of these components this includes the

00:31:42,200 --> 00:31:49,310
fqq disk which offers fair queuing and

00:31:45,530 --> 00:31:53,720
pacing the TSM auto sizing subsystem and

00:31:49,310 --> 00:31:55,160
the tsq flow control system and when you

00:31:53,720 --> 00:31:57,980
put this all together you've got a

00:31:55,160 --> 00:32:00,710
system that offers orders of magnitude

00:31:57,980 --> 00:32:04,060
higher bandwidth and lower latency than

00:32:00,710 --> 00:32:07,070
what was possible before so next step on

00:32:04,060 --> 00:32:09,620
we as I said we've deployed this on our

00:32:07,070 --> 00:32:11,180
internal backbone but we are now in the

00:32:09,620 --> 00:32:15,800
process of deploying this for google.com

00:32:11,180 --> 00:32:17,420
and YouTube traffic and with a certain

00:32:15,800 --> 00:32:19,820
amount of tuning and testing we hope

00:32:17,420 --> 00:32:24,440
maybe it can be deployed more widely on

00:32:19,820 --> 00:32:26,990
the internet and these the rack and bbr

00:32:24,440 --> 00:32:29,510
both are undergoing ongoing development

00:32:26,990 --> 00:32:31,760
so we'd love to have your help both with

00:32:29,510 --> 00:32:34,550
code and testing you know help us make

00:32:31,760 --> 00:32:37,040
these better we've got a year off for a

00:32:34,550 --> 00:32:38,240
BB our dev mailing list which is the

00:32:37,040 --> 00:32:40,610
public mailing list for anyone

00:32:38,240 --> 00:32:43,880
interested in discussing or sharing test

00:32:40,610 --> 00:32:46,390
results that kind of thing so so thank

00:32:43,880 --> 00:32:46,390
you any questions

00:32:49,180 --> 00:32:54,400
flows operating at max throughput on

00:32:52,040 --> 00:32:56,840
your probing for additional bandwidth to

00:32:54,400 --> 00:32:59,450
intelligently schedule the probes that

00:32:56,840 --> 00:33:03,410
it doesn't create latency Peaks or

00:32:59,450 --> 00:33:06,230
something like that yes the gain cycling

00:33:03,410 --> 00:33:08,810
is protein for other extra bandwidth

00:33:06,230 --> 00:33:10,720
it's it's randomized so that different

00:33:08,810 --> 00:33:13,550
flows are doing in it at different times

00:33:10,720 --> 00:33:18,110
and so that tends to help out with that

00:33:13,550 --> 00:33:20,130
yeah I'm illicit almost too good to be

00:33:18,110 --> 00:33:22,799
true that's great

00:33:20,130 --> 00:33:25,559
it's incredible t speeds on here 40

00:33:22,799 --> 00:33:28,440
years ago and still were quite a couple

00:33:25,559 --> 00:33:31,200
questions actually it for a client

00:33:28,440 --> 00:33:34,100
that's potentially on a low bandwidth

00:33:31,200 --> 00:33:37,140
network it's not like before you know

00:33:34,100 --> 00:33:40,049
have you tested on such a setup

00:33:37,140 --> 00:33:42,750
do you know how all this does yes so

00:33:40,049 --> 00:33:44,179
we've done a lot of testing in the lab

00:33:42,750 --> 00:33:48,030
at low bandwidth and we've also done

00:33:44,179 --> 00:33:50,010
several years of testing on YouTube and

00:33:48,030 --> 00:33:52,830
actually on YouTube most of our test

00:33:50,010 --> 00:33:55,830
sites because we're you know we're

00:33:52,830 --> 00:33:57,450
cognizant of this as the sort of as one

00:33:55,830 --> 00:34:02,309
of the challenging areas mister our test

00:33:57,450 --> 00:34:05,039
sites are actually in cellular networks

00:34:02,309 --> 00:34:07,320
in Brazil and India and places like that

00:34:05,039 --> 00:34:11,629
with lower bandwidth links and what we

00:34:07,320 --> 00:34:15,000
see is that bbr does a nice job of

00:34:11,629 --> 00:34:17,250
sending based on the available data rate

00:34:15,000 --> 00:34:19,850
and then bounding the amount of data in

00:34:17,250 --> 00:34:22,770
flight to be proportional to the

00:34:19,850 --> 00:34:25,919
estimated bdp that it needs so we see

00:34:22,770 --> 00:34:29,010
that yeah typically VBR is able to match

00:34:25,919 --> 00:34:31,409
the the throughput that Kubik will give

00:34:29,010 --> 00:34:33,270
you but the cuts the attempts to cut the

00:34:31,409 --> 00:34:36,260
median round-trip time by about a factor

00:34:33,270 --> 00:34:40,020
of two because it keeps smaller queues

00:34:36,260 --> 00:34:42,750
okay okay that sounds good so coming

00:34:40,020 --> 00:34:47,700
back to Thomas's question I think if you

00:34:42,750 --> 00:34:51,000
were if you would do like if you didn't

00:34:47,700 --> 00:34:52,980
have you know anything smart on your

00:34:51,000 --> 00:34:55,740
client host you didn't have cuddle you

00:34:52,980 --> 00:34:58,080
didn't have this you have nothing it

00:34:55,740 --> 00:34:59,310
sounds like if you have as a low number

00:34:58,080 --> 00:35:01,770
of flows you might be able to get most

00:34:59,310 --> 00:35:06,690
of the benefits of Q cuddling in terms

00:35:01,770 --> 00:35:08,310
of reducing reducing q agency if you had

00:35:06,690 --> 00:35:09,480
something like this again if you're a

00:35:08,310 --> 00:35:14,310
client not if you're an intermediate

00:35:09,480 --> 00:35:15,869
Rooter yeah exactly so clients or

00:35:14,310 --> 00:35:17,820
servers should be able to benefit from

00:35:15,869 --> 00:35:19,320
this anybody who's doing water sending

00:35:17,820 --> 00:35:20,160
should be able to benefit from this and

00:35:19,320 --> 00:35:22,680
as you say

00:35:20,160 --> 00:35:24,690
one way to think about it is if you

00:35:22,680 --> 00:35:25,530
don't have FQ Connell in the path or

00:35:24,690 --> 00:35:28,260
something like that

00:35:25,530 --> 00:35:32,160
BB R does a nice job of keeping the

00:35:28,260 --> 00:35:37,640
bottle McHugh short you know even

00:35:32,160 --> 00:35:39,630
without active key management thanks yes

00:35:37,640 --> 00:35:44,430
yeah you didn't say anything about

00:35:39,630 --> 00:35:49,350
fairness against cubic or right so yeah

00:35:44,430 --> 00:35:50,880
so the the fairness kind of between DB

00:35:49,350 --> 00:35:52,260
are flows it kind of looks like this if

00:35:50,880 --> 00:35:54,900
you look at a picture you know the flow

00:35:52,260 --> 00:35:58,920
start out they probe and then what tends

00:35:54,900 --> 00:36:02,690
to happen is they can all basically

00:35:58,920 --> 00:36:05,610
converge toward such an estimate of the

00:36:02,690 --> 00:36:07,350
round-trip propagation delay and an

00:36:05,610 --> 00:36:10,080
estimate of their fair share of the

00:36:07,350 --> 00:36:13,010
bandwidth so that gradually over time

00:36:10,080 --> 00:36:15,810
they converge to a very nice fair

00:36:13,010 --> 00:36:18,270
distribution of the bandwidth and it

00:36:15,810 --> 00:36:20,850
turns out that the same bandwidth

00:36:18,270 --> 00:36:24,270
probing and min RTT priming mechanism

00:36:20,850 --> 00:36:26,970
works to make the system converge toward

00:36:24,270 --> 00:36:29,280
a fair share of bandwidth distribution

00:36:26,970 --> 00:36:31,860
even if some of the flows are cubic

00:36:29,280 --> 00:36:33,750
because fundamentally the pro BW is just

00:36:31,860 --> 00:36:36,270
checking to see you know is there more

00:36:33,750 --> 00:36:38,520
bandwidth available and then probe RTT

00:36:36,270 --> 00:36:42,300
is just checking to see okay you know

00:36:38,520 --> 00:36:44,010
what's the what's the round-trip time in

00:36:42,300 --> 00:36:46,520
the system without my traffic in the

00:36:44,010 --> 00:36:48,710
picture so it gives you a sort of

00:36:46,520 --> 00:36:51,420
round-trip time that you need to

00:36:48,710 --> 00:36:54,330
converge toward your fair share of the

00:36:51,420 --> 00:36:57,480
available bandwidth and of course if the

00:36:54,330 --> 00:37:00,000
bottleneck buffer is as deep enough then

00:36:57,480 --> 00:37:03,300
of course an algorithm like cubic or

00:37:00,000 --> 00:37:05,670
Reno is going to tends to put more data

00:37:03,300 --> 00:37:06,330
than is its fair share more data than is

00:37:05,670 --> 00:37:09,270
necessary

00:37:06,330 --> 00:37:10,950
inside that bottleneck link and so bbr

00:37:09,270 --> 00:37:15,570
and it's going to get proportionally

00:37:10,950 --> 00:37:17,460
less bandwidth but then the you know we

00:37:15,570 --> 00:37:20,910
it's our judgment that that's probably

00:37:17,460 --> 00:37:23,190
the best direction to go in in the

00:37:20,910 --> 00:37:25,050
meantime while they're both while both

00:37:23,190 --> 00:37:28,250
congestion control algorithms operate

00:37:25,050 --> 00:37:28,250
and simultaneously

00:37:32,589 --> 00:37:39,490
did you test your DVR with Auto TCP

00:37:36,650 --> 00:37:42,080
implementations in the mix check on

00:37:39,490 --> 00:37:44,800
environment chronica multiple streams

00:37:42,080 --> 00:37:47,980
using a different discipline

00:37:44,800 --> 00:37:53,270
yeah we tested PBR without other streams

00:37:47,980 --> 00:37:55,940
using cubic and Reno so far and we'd

00:37:53,270 --> 00:37:58,310
love to have you know data points from

00:37:55,940 --> 00:38:00,230
other folks can be testing it alongside

00:37:58,310 --> 00:38:04,670
other congestion control algorithms as

00:38:00,230 --> 00:38:06,109
well so when you say you've tested

00:38:04,670 --> 00:38:07,700
against cubic is that in a high

00:38:06,109 --> 00:38:09,800
round-trip time environment because

00:38:07,700 --> 00:38:12,589
there seems to really squeeze out cubic

00:38:09,800 --> 00:38:16,880
flows once the RTT gets up about 40

00:38:12,589 --> 00:38:20,210
milliseconds well the the dynamic in

00:38:16,880 --> 00:38:22,970
terms of how cubic and BP irish share

00:38:20,210 --> 00:38:25,730
bandwidth i should say two things about

00:38:22,970 --> 00:38:30,410
it one is it is it's largely driven by

00:38:25,730 --> 00:38:34,190
the buffer size so the the current

00:38:30,410 --> 00:38:35,570
version of bbr tends to the algorithm

00:38:34,190 --> 00:38:38,930
for calculating the congestion window

00:38:35,570 --> 00:38:41,990
and the sending pacing rate is such that

00:38:38,930 --> 00:38:45,230
it's going to try to keep about one

00:38:41,990 --> 00:38:46,820
bandwidth still a product of data in

00:38:45,230 --> 00:38:48,740
flight in the network so if the buffers

00:38:46,820 --> 00:38:50,210
are smaller than indeed that's going to

00:38:48,740 --> 00:38:55,400
be a little bit too much and that's

00:38:50,210 --> 00:38:56,750
going to tend to crowd out cubic arena

00:38:55,400 --> 00:38:59,570
flows and so this is something that

00:38:56,750 --> 00:39:02,450
we're actively working on at the moment

00:38:59,570 --> 00:39:05,420
now if the buffers are deeper what

00:39:02,450 --> 00:39:07,010
happens is that of course cubic and me

00:39:05,420 --> 00:39:09,290
know just try to fill those buffers

00:39:07,010 --> 00:39:11,930
whereas PBR tries to keep the same size

00:39:09,290 --> 00:39:16,430
amount of data in the in the network so

00:39:11,930 --> 00:39:18,349
anytime the buffer is above about twice

00:39:16,430 --> 00:39:19,970
the bandwidth delay product cubic and

00:39:18,349 --> 00:39:23,270
reno are actually going to turn to crowd

00:39:19,970 --> 00:39:25,760
out bbr in the way that they're dumping

00:39:23,270 --> 00:39:28,369
too much data into the network so it's

00:39:25,760 --> 00:39:31,160
sort of a balancing act and we're

00:39:28,369 --> 00:39:33,160
actively working on improving the queue

00:39:31,160 --> 00:39:35,570
pressure that bbr places on the network

00:39:33,160 --> 00:39:36,890
so you can there's a way to resolve that

00:39:35,570 --> 00:39:38,210
so they can kind of coexist peacefully

00:39:36,890 --> 00:39:40,270
you're not just a sort of a

00:39:38,210 --> 00:39:42,099
Thunderdome winner

00:39:40,270 --> 00:39:45,099
absolutely yeah we definitely don't want

00:39:42,099 --> 00:39:46,570
to Thunderdome scenario where you know

00:39:45,099 --> 00:39:49,300
we're not at the end of the road here we

00:39:46,570 --> 00:39:52,630
realize there's going to be some room

00:39:49,300 --> 00:39:55,450
for improvement absolutely and we we you

00:39:52,630 --> 00:40:00,490
know obviously we want bbr to codes as

00:39:55,450 --> 00:40:03,130
well with cubic in you know any buffer

00:40:00,490 --> 00:40:04,960
size that that is reasonable and and you

00:40:03,130 --> 00:40:07,090
know common out there in the world which

00:40:04,960 --> 00:40:08,890
includes shower buffer switches and you

00:40:07,090 --> 00:40:10,420
know a lot of routers have about a btp

00:40:08,890 --> 00:40:13,150
of buffering we want to make that work

00:40:10,420 --> 00:40:16,200
well and we realize that we've got some

00:40:13,150 --> 00:40:16,200

YouTube URL: https://www.youtube.com/watch?v=hIl_zXzU3DA


