Title: Netdev 1.2: Virtual networking East West Performance - Do offloads really matter ?
Publication date: 2016-10-06
Playlist: Netdev 1.2 - Day 1 - Wednesday October 5, 2016
Description: 
	Stephen Hemminger

more info  at:


http://netdevconf.org/1.2/session.html?stephen-hemminger
Captions: 
	00:00:16,130 --> 00:00:23,790
hi I'm by his team manager I work now

00:00:21,390 --> 00:00:25,410
for Microsoft but this isn't a Microsoft

00:00:23,790 --> 00:00:30,420
talk so I didn't didn't although

00:00:25,410 --> 00:00:33,330
Microsoft logos and I want to talk today

00:00:30,420 --> 00:00:38,280
about east-west virtual networking

00:00:33,330 --> 00:00:41,610
performance the one Microsoft that here

00:00:38,280 --> 00:00:43,469
was this was one of the areas that I've

00:00:41,610 --> 00:00:48,930
initially focused on trying to improve

00:00:43,469 --> 00:00:51,120
in hyper-v and lenox and so one of the

00:00:48,930 --> 00:00:53,700
things I did was investigate how we're

00:00:51,120 --> 00:00:58,680
doing today on other systems so I'll get

00:00:53,700 --> 00:01:02,640
back to that one really over is where

00:00:58,680 --> 00:01:04,830
this site where this shows up what kind

00:01:02,640 --> 00:01:08,970
of test I could do audit what I found

00:01:04,830 --> 00:01:15,660
and some crazy stupid ideas that maybe

00:01:08,970 --> 00:01:20,039
would make this faster if you have

00:01:15,660 --> 00:01:21,750
looked at any of the standard tool sets

00:01:20,039 --> 00:01:27,530
that are out there now we've got

00:01:21,750 --> 00:01:32,399
OpenStack OPN Fe docker all of them jump

00:01:27,530 --> 00:01:34,950
create crazy network charts to have

00:01:32,399 --> 00:01:39,869
different components linking together

00:01:34,950 --> 00:01:43,020
with bridges and open V switch and final

00:01:39,869 --> 00:01:46,979
routers and hardware and everything like

00:01:43,020 --> 00:01:51,149
that so this is not some crazy thing

00:01:46,979 --> 00:01:53,640
this really happens um and if you're

00:01:51,149 --> 00:01:57,330
just trying to go from one vm to another

00:01:53,640 --> 00:02:00,869
we call that east-west internet working

00:01:57,330 --> 00:02:02,700
world north south is what Alex and

00:02:00,869 --> 00:02:04,130
everybody else is talking about it

00:02:02,700 --> 00:02:08,390
earlier about getting out the door

00:02:04,130 --> 00:02:10,009
over the wire this is just inside one we

00:02:08,390 --> 00:02:14,560
have environment you going to to each

00:02:10,009 --> 00:02:18,560
other and you see OpenStack does it with

00:02:14,560 --> 00:02:20,720
bridges and top devices and opie a

00:02:18,560 --> 00:02:22,760
no-fee does it they actually change

00:02:20,720 --> 00:02:26,870
things and there's actually a benchmark

00:02:22,760 --> 00:02:29,810
which I wanted to run but it's not far

00:02:26,870 --> 00:02:32,420
enough along yet that OPN fe guys have

00:02:29,810 --> 00:02:35,750
called vs perf that is basically take

00:02:32,420 --> 00:02:38,060
packets in from one network device right

00:02:35,750 --> 00:02:40,069
through one network function goes

00:02:38,060 --> 00:02:44,239
through a bridge to another network

00:02:40,069 --> 00:02:47,390
function and then back out the reason I

00:02:44,239 --> 00:02:48,799
couldn't run it is there's no the test

00:02:47,390 --> 00:02:50,840
environment is all based on

00:02:48,799 --> 00:02:53,269
hardware-based traffic generators and

00:02:50,840 --> 00:02:56,810
they don't have the software solution

00:02:53,269 --> 00:02:58,579
for that yet but docker has the same

00:02:56,810 --> 00:03:01,640
thing that basically run through a

00:02:58,579 --> 00:03:03,980
virtual bridge so all these guys this is

00:03:01,640 --> 00:03:09,079
a common thread this is a common pain

00:03:03,980 --> 00:03:15,019
point and so the first question I asked

00:03:09,079 --> 00:03:20,750
was what offloads my rat's little rat

00:03:15,019 --> 00:03:23,060
tail here what authors are there what's

00:03:20,750 --> 00:03:26,170
supported now if you take the kind of

00:03:23,060 --> 00:03:29,780
parts Alex to talk about into LAX GBE

00:03:26,170 --> 00:03:33,440
they basically have all of it and a lot

00:03:29,780 --> 00:03:35,540
of muscle have LRO as well but if you

00:03:33,440 --> 00:03:37,100
look at the virtue of mix there's a lot

00:03:35,540 --> 00:03:39,530
of red here about things they don't

00:03:37,100 --> 00:03:42,139
support and my question over here really

00:03:39,530 --> 00:03:44,030
was I'm working on I could be now which

00:03:42,139 --> 00:03:46,489
one of these things would matter at all

00:03:44,030 --> 00:03:50,720
which ones you know what so priority

00:03:46,489 --> 00:03:54,650
should I put on a good they matter and I

00:03:50,720 --> 00:03:57,019
used KTM grid is by testbed who took

00:03:54,650 --> 00:03:59,690
cover several reasons first of all I use

00:03:57,019 --> 00:04:03,670
VMware violates the license I can't

00:03:59,690 --> 00:04:06,610
possibly quote anything if I you

00:04:03,670 --> 00:04:10,150
hyper-v and it's bad I lose my job right

00:04:06,610 --> 00:04:12,010
now so i'll just use great IL because

00:04:10,150 --> 00:04:15,550
it's a well-known thing and everybody in

00:04:12,010 --> 00:04:20,440
this room can go test on it plus i plan

00:04:15,550 --> 00:04:23,310
to fix all these that's really um so the

00:04:20,440 --> 00:04:27,580
system i used to test this was a pretty

00:04:23,310 --> 00:04:31,780
standard server system nothing really

00:04:27,580 --> 00:04:33,940
wild just a modern intel processor DVF

00:04:31,780 --> 00:04:36,850
our memory and kvm and current

00:04:33,940 --> 00:04:38,380
generation kernels on if you go run old

00:04:36,850 --> 00:04:40,720
enterprise cars you probably get worse

00:04:38,380 --> 00:04:42,700
numbers it's also important to note that

00:04:40,720 --> 00:04:45,960
you've got to have all your memory banks

00:04:42,700 --> 00:04:49,600
populate this memory bandwidth number is

00:04:45,960 --> 00:04:51,490
four times one for each memory bank if

00:04:49,600 --> 00:04:53,200
you've got a system with only one memory

00:04:51,490 --> 00:04:56,020
bank you get one quarter of the lemon

00:04:53,200 --> 00:05:01,419
bandwidth and that really shows up in

00:04:56,020 --> 00:05:03,490
this test and so first question I asked

00:05:01,419 --> 00:05:06,070
myself was what is the theoretical

00:05:03,490 --> 00:05:09,210
performance that could Maxima get for

00:05:06,070 --> 00:05:12,039
this thing what is the ball throughput I

00:05:09,210 --> 00:05:14,380
guess the PF slide doesn't build as well

00:05:12,039 --> 00:05:16,150
but basically there's three copies one

00:05:14,380 --> 00:05:20,229
from the application into the guest

00:05:16,150 --> 00:05:22,690
Colonel one from the guest Colonel into

00:05:20,229 --> 00:05:25,690
the host the group's doesn't really do a

00:05:22,690 --> 00:05:27,910
copy just moves yes kb around and then

00:05:25,690 --> 00:05:32,020
gets copied back up so you copying the

00:05:27,910 --> 00:05:34,870
packet four times and my best case

00:05:32,020 --> 00:05:37,660
scenario here is we're using TCP with

00:05:34,870 --> 00:05:40,990
TSL and GOI will get each one of those

00:05:37,660 --> 00:05:43,390
of your 64k packet so let's assume we're

00:05:40,990 --> 00:05:47,520
doing really well if we do that we have

00:05:43,390 --> 00:05:52,180
four copies we have two system calls and

00:05:47,520 --> 00:05:55,750
to vm exits actually you have for DM

00:05:52,180 --> 00:05:57,729
exits because as i was writing a paper i

00:05:55,750 --> 00:05:59,289
said well what about the act coming back

00:05:57,729 --> 00:06:02,310
but doesn't make a whole lot of

00:05:59,289 --> 00:06:05,260
difference so you got the packet size

00:06:02,310 --> 00:06:06,910
versus how fast you can mem copy so it

00:06:05,260 --> 00:06:10,890
basically takes you one point two

00:06:06,910 --> 00:06:14,890
microseconds to copy that 64k packet

00:06:10,890 --> 00:06:15,700
there's a really bad side effect when

00:06:14,890 --> 00:06:18,070
you're doing is

00:06:15,700 --> 00:06:23,170
transfers you're basically killing your

00:06:18,070 --> 00:06:25,860
CPU cache you fill your CPU cache with

00:06:23,170 --> 00:06:28,090
the aid of your transiting through so

00:06:25,860 --> 00:06:32,380
everything else gets slow down in the

00:06:28,090 --> 00:06:35,590
process the MX it's got a benchmark and

00:06:32,380 --> 00:06:37,330
ran it's about half microsecond and this

00:06:35,590 --> 00:06:39,400
is called I ran a benchmark comes out

00:06:37,330 --> 00:06:43,060
about the same basically it's not

00:06:39,400 --> 00:06:45,190
surprising on an intel cpu you all want

00:06:43,060 --> 00:06:48,010
to contact you have to save when you go

00:06:45,190 --> 00:06:50,950
between environments so if you do that

00:06:48,010 --> 00:06:54,400
comes out to around hundred forty-seven

00:06:50,950 --> 00:06:57,370
TSO packets of secret which comes out

00:06:54,400 --> 00:07:00,520
view the math to about 77 gigabits a

00:06:57,370 --> 00:07:02,530
second it's the maximum rate you can get

00:07:00,520 --> 00:07:04,270
between BMS actually doing the bathroom

00:07:02,530 --> 00:07:06,730
or calculates a little bit lower it's

00:07:04,270 --> 00:07:11,680
about 66 when you count the acts going

00:07:06,730 --> 00:07:14,620
back and i was using i play with just a

00:07:11,680 --> 00:07:19,420
single student and got 41 gigabits a

00:07:14,620 --> 00:07:24,160
second so about sixty percent of what I

00:07:19,420 --> 00:07:28,050
math said I could get rich not bad but

00:07:24,160 --> 00:07:32,110
still not really what you'd like to see

00:07:28,050 --> 00:07:35,350
so if next question I said what happens

00:07:32,110 --> 00:07:37,540
if I turn on and off some of the

00:07:35,350 --> 00:07:40,420
offloads and play with things well the

00:07:37,540 --> 00:07:47,260
first thing is turning on and off TSO

00:07:40,420 --> 00:07:50,650
had absolutely no effect ah I'll get to

00:07:47,260 --> 00:07:52,660
officiate a second digit earnings enough

00:07:50,650 --> 00:07:55,030
and the guest driver on the podium for

00:07:52,660 --> 00:07:57,730
the guests so you would bring these

00:07:55,030 --> 00:08:02,290
about with the front end with us either

00:07:57,730 --> 00:08:05,080
sender yes OMG LOL likewise i was

00:08:02,290 --> 00:08:08,410
playing with any receiver so either side

00:08:05,080 --> 00:08:12,750
and that no change my suspicion is this

00:08:08,410 --> 00:08:16,630
goes all the way back to the current tcp

00:08:12,750 --> 00:08:18,700
small Q related pacing model sizing that

00:08:16,630 --> 00:08:20,220
went on that was talked about in early

00:08:18,700 --> 00:08:23,190
talk that we may not

00:08:20,220 --> 00:08:26,490
she be sending full-size TSO frames in

00:08:23,190 --> 00:08:28,980
this environment anyway because the

00:08:26,490 --> 00:08:30,930
color may be deciding that the queue

00:08:28,980 --> 00:08:34,680
isn't big enough for the rate is in

00:08:30,930 --> 00:08:37,530
height up it just as an experiment I

00:08:34,680 --> 00:08:42,510
turned off trans with check sums and the

00:08:37,530 --> 00:08:48,590
performance drop 27 gigabits so major

00:08:42,510 --> 00:08:52,230
hit kind of as an aside I said what if

00:08:48,590 --> 00:08:55,770
instead of going from guest to guest

00:08:52,230 --> 00:08:59,970
view the house I gave each guest a

00:08:55,770 --> 00:09:03,470
virtual function device with SRV and use

00:08:59,970 --> 00:09:07,350
the internal switch in the hardware that

00:09:03,470 --> 00:09:09,210
gets 17 gigatons a separate which two

00:09:07,350 --> 00:09:12,510
things it's surprising one that's a 10

00:09:09,210 --> 00:09:15,390
gigabit card which I really expected

00:09:12,510 --> 00:09:18,060
only to get 10 gigabits out second one

00:09:15,390 --> 00:09:20,220
is I was moving where that number came

00:09:18,060 --> 00:09:22,170
from it like gives an Alex talk it's

00:09:20,220 --> 00:09:29,480
pretty much the PCI bandwidth or that

00:09:22,170 --> 00:09:32,490
got on the box so i traded what know

00:09:29,480 --> 00:09:34,680
when i'm getting yourselves the 80s are

00:09:32,490 --> 00:09:37,350
back on so is probably effective so

00:09:34,680 --> 00:09:42,210
basically that's interesting because I

00:09:37,350 --> 00:09:45,540
traded CPU cycles for PCI cycles what's

00:09:42,210 --> 00:09:48,150
also not really shown here is all the

00:09:45,540 --> 00:09:50,610
tests I ran when i get in software had

00:09:48,150 --> 00:09:54,210
like a water two gigabit standard

00:09:50,610 --> 00:09:56,250
deviation so i'd have to run a large set

00:09:54,210 --> 00:10:02,040
of tests and average them otherwise i

00:09:56,250 --> 00:10:04,710
would see this that's our Imam 17

00:10:02,040 --> 00:10:09,360
degrees every time may change any

00:10:04,710 --> 00:10:13,890
Chester on so I was particularly

00:10:09,360 --> 00:10:15,839
interesting um just to go really more

00:10:13,890 --> 00:10:17,910
exploring I said well maybe this cache

00:10:15,839 --> 00:10:21,110
effect might be a part of the thing so I

00:10:17,910 --> 00:10:24,510
turned on the TX no cache copy option

00:10:21,110 --> 00:10:27,820
performance drop the 34.4 gigabits

00:10:24,510 --> 00:10:30,010
second so I'm really wondering if you

00:10:27,820 --> 00:10:31,870
bother keeping that around maybe there

00:10:30,010 --> 00:10:36,880
is a use case it's good for that still

00:10:31,870 --> 00:10:40,360
Tom did it initially but maybe somebody

00:10:36,880 --> 00:10:44,500
and then the last one is I went ahead

00:10:40,360 --> 00:10:47,920
and turned on the RX busy pole in the

00:10:44,500 --> 00:10:51,880
guest and what happens is since you're

00:10:47,920 --> 00:10:55,540
both transferred the RX busy pole ties

00:10:51,880 --> 00:10:58,720
up the CPU so much that the this this

00:10:55,540 --> 00:11:00,190
cpu is too busy now turner in the car

00:10:58,720 --> 00:11:06,510
goes off that it thinks you've got a

00:11:00,190 --> 00:11:09,820
stuck task and kills it or crashes so

00:11:06,510 --> 00:11:13,710
for both throughput or it's busy pro

00:11:09,820 --> 00:11:13,710
east and software is a really bad idea

00:11:17,040 --> 00:11:22,660
now since I've you brought your growth

00:11:20,260 --> 00:11:24,280
stuff sometimes and it's kind of

00:11:22,660 --> 00:11:26,460
important to me i said well doing the

00:11:24,280 --> 00:11:29,380
single stream test is one bit of data

00:11:26,460 --> 00:11:30,730
but realize i do multiple streams and

00:11:29,380 --> 00:11:36,700
you want to look at what the latency of

00:11:30,730 --> 00:11:39,580
all is so first thing i did was just

00:11:36,700 --> 00:11:41,730
draw this is a stock default test

00:11:39,580 --> 00:11:44,320
there's something called the flexible

00:11:41,730 --> 00:11:47,260
network test environment East we called

00:11:44,320 --> 00:11:49,390
I rule that basically runs net / runs

00:11:47,260 --> 00:11:51,750
multiple threads sending receiving at

00:11:49,390 --> 00:11:55,990
same time in Moses pin latency and

00:11:51,750 --> 00:12:01,000
produces these nice charts the top one

00:11:55,990 --> 00:12:02,770
is the how fast the guests suggest TCP

00:12:01,000 --> 00:12:05,470
one direction is going and the other one

00:12:02,770 --> 00:12:07,120
is to tcp upload so basically these two

00:12:05,470 --> 00:12:10,510
numbers should be the same they should

00:12:07,120 --> 00:12:12,040
be flat they're not and then this is the

00:12:10,510 --> 00:12:14,860
ping time which holds pretty much

00:12:12,040 --> 00:12:18,070
constant the colors are basically it

00:12:14,860 --> 00:12:19,930
uses different type of service options

00:12:18,070 --> 00:12:23,080
which if you've got a network where

00:12:19,930 --> 00:12:24,640
you've got priority going on you can see

00:12:23,080 --> 00:12:26,980
the different colors will diverge in

00:12:24,640 --> 00:12:31,660
this case all the streams are just being

00:12:26,980 --> 00:12:34,740
noisy one thing I do with this test is I

00:12:31,660 --> 00:12:37,589
also said well what if I try different

00:12:34,740 --> 00:12:40,830
genius on the guests would it have any

00:12:37,589 --> 00:12:43,410
impact and every test I read pretty much

00:12:40,830 --> 00:12:46,070
previous the same picture so all the

00:12:43,410 --> 00:12:49,110
normal set of Q just ft you after coddle

00:12:46,070 --> 00:12:51,990
p piper past all those pretty much

00:12:49,110 --> 00:12:55,500
produce the same value so says that's

00:12:51,990 --> 00:12:58,080
not an interesting data point so just

00:12:55,500 --> 00:13:01,290
not telling to that being the RK data so

00:12:58,080 --> 00:13:04,740
why did I run oh also just said while I

00:13:01,290 --> 00:13:06,209
try running data center TCP because this

00:13:04,740 --> 00:13:09,690
carlos said your data center environment

00:13:06,209 --> 00:13:13,380
thing and this is an example where it

00:13:09,690 --> 00:13:17,010
was actually worse we kind of vibrated

00:13:13,380 --> 00:13:20,040
it got confused and CCN bouncing up and

00:13:17,010 --> 00:13:23,279
down rates and it kept the same and i

00:13:20,040 --> 00:13:25,200
also decided to have BB r and this one

00:13:23,279 --> 00:13:29,279
was this is the data point that still I

00:13:25,200 --> 00:13:31,620
owners dead the throughput one direction

00:13:29,279 --> 00:13:37,370
was this what double what i was getting

00:13:31,620 --> 00:13:42,600
before so it's up to about 55 gigabits

00:13:37,370 --> 00:13:49,050
know if you go back this one's sitting

00:13:42,600 --> 00:13:51,600
at about 3.5 this one's much faster one

00:13:49,050 --> 00:13:55,410
direction but terrible the other

00:13:51,600 --> 00:13:59,399
direction and i don't understand why

00:13:55,410 --> 00:14:02,339
this is happening it can do that my

00:13:59,399 --> 00:14:03,990
suspicion is that BB r is assuming that

00:14:02,339 --> 00:14:06,329
it's going on a real network that what

00:14:03,990 --> 00:14:09,510
it's doing is not impacting results and

00:14:06,329 --> 00:14:17,250
maybe there's some sort of you know cash

00:14:09,510 --> 00:14:20,250
effect heisenberg thing going on but i'm

00:14:17,250 --> 00:14:26,310
puzzled by it that's why i did him it's

00:14:20,250 --> 00:14:29,579
like so so after doing this test i

00:14:26,310 --> 00:14:35,300
started to ask myself what could i

00:14:29,579 --> 00:14:40,279
change to make it better first thing was

00:14:35,300 --> 00:14:40,279
why do you have to use just standard tcp

00:14:40,399 --> 00:14:47,240
everything here is with that empty or

00:14:44,400 --> 00:14:49,410
1500 now in it

00:14:47,240 --> 00:14:50,940
environment you can have bigger than T

00:14:49,410 --> 00:14:53,310
but then you have to go out the door

00:14:50,940 --> 00:14:55,470
with something else and also you know

00:14:53,310 --> 00:14:59,360
internally inside a server be nice to

00:14:55,470 --> 00:15:03,209
even just go with full-size 64k frames

00:14:59,360 --> 00:15:05,850
but the current concept of MTU is a

00:15:03,209 --> 00:15:10,709
little rough for that we have TC me

00:15:05,850 --> 00:15:13,410
metrics and so on but you really be nice

00:15:10,709 --> 00:15:14,850
to figure out a way to manage that

00:15:13,410 --> 00:15:18,240
better and come up with a better

00:15:14,850 --> 00:15:21,329
solution so that in a vm traffic would

00:15:18,240 --> 00:15:23,610
see 64k inside the same data center

00:15:21,329 --> 00:15:28,310
night you could jumbo frames and maybe

00:15:23,610 --> 00:15:33,389
you're going out the door be 1500 I

00:15:28,310 --> 00:15:36,720
question whether the assumptions of just

00:15:33,389 --> 00:15:38,550
initially jestin would though and the

00:15:36,720 --> 00:15:40,970
whole Syrian mechanisms really makes

00:15:38,550 --> 00:15:44,069
sense it does the network model of a

00:15:40,970 --> 00:15:46,470
software-based two things communicating

00:15:44,069 --> 00:15:50,310
with each other even fit the control

00:15:46,470 --> 00:15:53,910
theory mollis of TCP is there something

00:15:50,310 --> 00:15:58,139
else that needs to be added and lastly

00:15:53,910 --> 00:16:01,259
on every hypervisor environment has

00:15:58,139 --> 00:16:06,630
invented its own into vm communication

00:16:01,259 --> 00:16:10,769
socket family that the next step i want

00:16:06,630 --> 00:16:13,110
to do is test vmware's KTMs and hyper-v

00:16:10,769 --> 00:16:15,600
version of that and see what the

00:16:13,110 --> 00:16:18,480
performance of this is personally as a

00:16:15,600 --> 00:16:20,519
long time nellore person i really don't

00:16:18,480 --> 00:16:22,769
like to see custom socket families

00:16:20,519 --> 00:16:25,980
affected because it means an application

00:16:22,769 --> 00:16:30,540
change it means infrastructure change it

00:16:25,980 --> 00:16:31,889
doesn't necessarily help anything and to

00:16:30,540 --> 00:16:34,139
what you end up doing is building

00:16:31,889 --> 00:16:37,829
another abstraction on top of it like

00:16:34,139 --> 00:16:41,550
cmq or something to to do with the model

00:16:37,829 --> 00:16:43,759
but right now you're not me good data on

00:16:41,550 --> 00:16:43,759
that

00:16:44,490 --> 00:16:49,529
Michael and the guise of the world I all

00:16:47,339 --> 00:16:53,130
have done a new version called pralaya

00:16:49,529 --> 00:16:57,959
one dot one it's not on by default but

00:16:53,130 --> 00:17:01,080
you can go look at the kvm form and what

00:16:57,959 --> 00:17:04,380
it addressed tries to address is right

00:17:01,080 --> 00:17:06,780
now the verdi oh the way the Rings are

00:17:04,380 --> 00:17:09,390
laid out and software causes a lot of

00:17:06,780 --> 00:17:14,459
cache misses and doesn't get any

00:17:09,390 --> 00:17:16,709
patching related to this is the fact

00:17:14,459 --> 00:17:18,750
that the road oil driver in the way it

00:17:16,709 --> 00:17:22,319
manages transmitted frames does not

00:17:18,750 --> 00:17:24,660
implement by Q limits because it doesn't

00:17:22,319 --> 00:17:31,980
implement by Q limits it doesn't really

00:17:24,660 --> 00:17:34,050
get TCP small shoes right there's no

00:17:31,980 --> 00:17:36,840
that pressure so was it like for sure

00:17:34,050 --> 00:17:38,460
you cannot enjoy so I think that the

00:17:36,840 --> 00:17:41,490
word I hope driver really needs to

00:17:38,460 --> 00:17:43,620
change to make this environment more

00:17:41,490 --> 00:17:45,330
stable and it may not gain that

00:17:43,620 --> 00:17:48,960
performance but it will probably be more

00:17:45,330 --> 00:17:51,360
stable and the other question I was kind

00:17:48,960 --> 00:17:54,270
of like uh when they were mentioning

00:17:51,360 --> 00:17:57,090
before walk was quitas to relieve bulk

00:17:54,270 --> 00:17:59,580
receiving both transmitted in a virtual

00:17:57,090 --> 00:18:02,940
environment it'd be nice to wake up the

00:17:59,580 --> 00:18:07,020
guests and say here's 64 packets go

00:18:02,940 --> 00:18:13,080
process of because the cost of that wake

00:18:07,020 --> 00:18:14,940
up is always there well then maybe you

00:18:13,080 --> 00:18:19,860
would trigger the same build and busy

00:18:14,940 --> 00:18:22,260
port stuff yeah that's true but well

00:18:19,860 --> 00:18:25,559
with then if we want case off tire QD

00:18:22,260 --> 00:18:28,950
working again milk use much of a problem

00:18:25,559 --> 00:18:33,179
um the other one is back to the four

00:18:28,950 --> 00:18:37,530
copies do you really need to have the

00:18:33,179 --> 00:18:39,360
code user copy um the first one that's

00:18:37,530 --> 00:18:42,630
kind of an mentioned earlier in this

00:18:39,360 --> 00:18:44,610
talking because today is that there's a

00:18:42,630 --> 00:18:49,530
splicer map app where you can basically

00:18:44,610 --> 00:18:54,150
say m napa memory region spliced subnet

00:18:49,530 --> 00:18:57,330
FD to send kind of trades off

00:18:54,150 --> 00:19:00,840
before threading because that spice has

00:18:57,330 --> 00:19:02,250
to wait until that send has been act so

00:19:00,840 --> 00:19:04,950
then you'll end up with having to write

00:19:02,250 --> 00:19:06,600
a ping-pong kind of application that has

00:19:04,950 --> 00:19:10,410
one in flights and and the mother

00:19:06,600 --> 00:19:14,100
appending gets kind of messy Linux does

00:19:10,410 --> 00:19:15,570
it has barely has a io support and it

00:19:14,100 --> 00:19:19,860
was never really implemented for

00:19:15,570 --> 00:19:21,420
networking they don't know i think it's

00:19:19,860 --> 00:19:23,670
just so painful that we'll never get

00:19:21,420 --> 00:19:27,740
there but other operating systems that's

00:19:23,670 --> 00:19:27,740
how they solve that problem to reduce it

00:19:27,830 --> 00:19:32,070
another way people have attack this

00:19:30,180 --> 00:19:36,510
problem is to do the networking in user

00:19:32,070 --> 00:19:38,340
space but nitro seven sure was familiar

00:19:36,510 --> 00:19:40,080
with the DB carry which has total

00:19:38,340 --> 00:19:42,720
separate drivers and the thread model

00:19:40,080 --> 00:19:45,390
this if it pushes all the heavy lifting

00:19:42,720 --> 00:19:48,360
to application there's a new photo

00:19:45,390 --> 00:19:52,080
project called TLD k which is TCP user

00:19:48,360 --> 00:19:54,330
space I looked at it they're really off

00:19:52,080 --> 00:19:57,570
the ground yet so they weren't really

00:19:54,330 --> 00:20:00,720
testable but if you are building an

00:19:57,570 --> 00:20:02,400
application then that was your pain

00:20:00,720 --> 00:20:04,980
point you could go that direction and

00:20:02,400 --> 00:20:06,660
lastly it's worth mentioning the people

00:20:04,980 --> 00:20:09,330
that just don't want cuddles at all the

00:20:06,660 --> 00:20:11,420
guests they run unit curdles well

00:20:09,330 --> 00:20:14,160
basically everything's in chrome mode

00:20:11,420 --> 00:20:16,760
claudius the Rope curl project bunch of

00:20:14,160 --> 00:20:21,030
other things are all work that way um

00:20:16,760 --> 00:20:22,710
Steve I'm over here did you look at any

00:20:21,030 --> 00:20:24,150
ideas for I mean what you're trying to

00:20:22,710 --> 00:20:26,820
do is you're trying to get data into the

00:20:24,150 --> 00:20:29,670
application space right right well if

00:20:26,820 --> 00:20:33,330
the data was coming from an application

00:20:29,670 --> 00:20:34,770
and it was in memory like why why does

00:20:33,330 --> 00:20:36,750
the network stack even need to be there

00:20:34,770 --> 00:20:38,310
at all is a different guess in different

00:20:36,750 --> 00:20:40,740
security domain and the person the

00:20:38,310 --> 00:20:42,690
middle wants to to post some security

00:20:40,740 --> 00:20:43,980
thing and that's that the other what

00:20:42,690 --> 00:20:46,650
people always asked is why not you

00:20:43,980 --> 00:20:49,170
shared memory but in these environments

00:20:46,650 --> 00:20:51,510
everybody always wants to impose some

00:20:49,170 --> 00:20:54,030
sort of security policy about who sought

00:20:51,510 --> 00:20:55,620
to talk to him about what so if you had

00:20:54,030 --> 00:20:57,240
a security policy that you could manage

00:20:55,620 --> 00:20:58,680
shared memory away it would probably be

00:20:57,240 --> 00:21:00,510
it that would be

00:20:58,680 --> 00:21:02,730
to do it especially if you could manage

00:21:00,510 --> 00:21:04,620
that he has other thing is I'm not

00:21:02,730 --> 00:21:06,210
working for a chip vendor but chip

00:21:04,620 --> 00:21:08,640
vendors always talk about Chef

00:21:06,210 --> 00:21:12,330
extensions to WoW memory to be

00:21:08,640 --> 00:21:15,270
controlled and accessed and pastor out

00:21:12,330 --> 00:21:17,970
that i would also postulate to your

00:21:15,270 --> 00:21:20,280
earlier question about the SRV device

00:21:17,970 --> 00:21:21,960
having a good solid number all the time

00:21:20,280 --> 00:21:24,180
was probably because it had dql

00:21:21,960 --> 00:21:25,350
implemented right right guys responding

00:21:24,180 --> 00:21:29,430
to the back pressure very efficiently

00:21:25,350 --> 00:21:31,860
thanks to eric opening now i say the

00:21:29,430 --> 00:21:33,840
copy at the v search level here i

00:21:31,860 --> 00:21:39,180
mentioned that shared memory issues are

00:21:33,840 --> 00:21:40,890
secured and also in next-generation

00:21:39,180 --> 00:21:43,620
environment people are talking about

00:21:40,890 --> 00:21:45,180
encrypted memory and VMS so if you have

00:21:43,620 --> 00:21:47,850
two different VMs with two different

00:21:45,180 --> 00:21:50,270
encryption keys you have to figure out

00:21:47,850 --> 00:21:53,100
if you got at least two have one copy to

00:21:50,270 --> 00:21:55,200
somehow take what was encrypted memory

00:21:53,100 --> 00:21:59,220
and one guest to put it on the other

00:21:55,200 --> 00:22:00,480
domain and like i said less RV p

00:21:59,220 --> 00:22:02,910
switching hardware is another

00:22:00,480 --> 00:22:08,790
possibility for this or some hardware

00:22:02,910 --> 00:22:14,540
support with that tried to cut the talk

00:22:08,790 --> 00:22:14,540
down so thank you time for questions

00:22:15,290 --> 00:22:24,180
while i go round to the JK these in the

00:22:19,140 --> 00:22:27,390
middle and then we can work back so on

00:22:24,180 --> 00:22:29,130
the the TSL was video doesn't video if

00:22:27,390 --> 00:22:30,900
you have TSO enabled simply send the

00:22:29,130 --> 00:22:32,610
whole unit up is a single blah without

00:22:30,900 --> 00:22:34,560
splitting apart well it's actually i

00:22:32,610 --> 00:22:37,110
think what it does is it sends it into v

00:22:34,560 --> 00:22:40,130
host is one big blob but at some point

00:22:37,110 --> 00:22:42,780
it has to go spit it back up as

00:22:40,130 --> 00:22:45,060
individual MTU size things into the

00:22:42,780 --> 00:22:49,830
verdi or the other way where it gets grr

00:22:45,060 --> 00:22:51,600
we assembled so wouldn't it be nice if

00:22:49,830 --> 00:22:53,490
it just didn't have to chop that I wrong

00:22:51,600 --> 00:22:57,720
that's kind of lays in the empty you

00:22:53,490 --> 00:22:59,580
think if we can figure out how we wrote

00:22:57,720 --> 00:23:01,800
IL could say or whatever Roberto

00:22:59,580 --> 00:23:03,710
environment could say I can take risks

00:23:01,800 --> 00:23:10,399
eyes flamed if you only want to

00:23:03,710 --> 00:23:15,770
to me and i'll chop it you know I'm just

00:23:10,399 --> 00:23:17,960
as in those side I poppy has a another

00:23:15,770 --> 00:23:20,480
copy involved because it basically has

00:23:17,960 --> 00:23:22,039
one giant receive buffer that holds

00:23:20,480 --> 00:23:23,779
multiple frames so you have to extract

00:23:22,039 --> 00:23:30,230
right now the frames out of that buffer

00:23:23,779 --> 00:23:32,600
so but this standing did you check

00:23:30,230 --> 00:23:36,230
Observatory assumptions to a no vm to

00:23:32,600 --> 00:23:38,149
via communications and metaphysics i was

00:23:36,230 --> 00:23:41,510
just using what's in the standard

00:23:38,149 --> 00:23:43,789
upstream color with the host net and all

00:23:41,510 --> 00:23:46,159
the normal things that are labeled and

00:23:43,789 --> 00:23:48,080
every other has to have been united

00:23:46,159 --> 00:23:50,990
kingdom with I'd missish mins from

00:23:48,080 --> 00:23:52,850
VMware with emc I you can I didn't get

00:23:50,990 --> 00:23:55,399
into this I be shared memory and all

00:23:52,850 --> 00:23:57,890
that and what's if it's another made

00:23:55,399 --> 00:24:00,890
from a few moment of LD mas having some

00:23:57,890 --> 00:24:02,840
register memories yeah that's yeah

00:24:00,890 --> 00:24:04,730
that's another possibility but then any

00:24:02,840 --> 00:24:06,919
question is can you do idea man software

00:24:04,730 --> 00:24:09,580
and how much however software support

00:24:06,919 --> 00:24:09,580
there is for it

00:24:16,450 --> 00:24:24,340
so about the non temporal copies someone

00:24:21,460 --> 00:24:26,260
else on the list had mentioned that they

00:24:24,340 --> 00:24:29,290
were seeing a performance regression

00:24:26,260 --> 00:24:31,750
also using it so I think the intent is

00:24:29,290 --> 00:24:33,280
to take it out but I have to admit it's

00:24:31,750 --> 00:24:36,510
kind of disappointing maybe the Intel

00:24:33,280 --> 00:24:39,640
guys can explain this but it should

00:24:36,510 --> 00:24:41,500
should theoretically improve things if

00:24:39,640 --> 00:24:47,470
you're not using the data we're saving

00:24:41,500 --> 00:24:50,320
the cash maybe my guess is maybe there

00:24:47,470 --> 00:24:54,250
needs to be substrate off where we do a

00:24:50,320 --> 00:24:55,810
cash version to the first bulk of the

00:24:54,250 --> 00:24:57,880
first shot of the data where the headers

00:24:55,810 --> 00:25:01,030
likely to be and then do the non-cash

00:24:57,880 --> 00:25:03,190
versions of the bulk afterwards because

00:25:01,030 --> 00:25:05,640
you know the host or something on the

00:25:03,190 --> 00:25:11,350
other side is going to look at that data

00:25:05,640 --> 00:25:13,090
right away well so in this case that's

00:25:11,350 --> 00:25:16,180
true but even in the normal case where

00:25:13,090 --> 00:25:18,160
we're just copying user data adding to

00:25:16,180 --> 00:25:19,990
the kernel with the expectation that's

00:25:18,160 --> 00:25:23,290
just going to be sent as is once we have

00:25:19,990 --> 00:25:26,290
a TCP header we don't expect any of that

00:25:23,290 --> 00:25:28,030
data to be accessed in the colonel no so

00:25:26,290 --> 00:25:31,480
the expectation wasn't on temporal copy

00:25:28,030 --> 00:25:33,250
should have saved us an antenna but one

00:25:31,480 --> 00:25:35,080
of the questions so if you go back to

00:25:33,250 --> 00:25:39,010
the slide where we're trying to save the

00:25:35,080 --> 00:25:41,080
colonel user copy yep you mentioned

00:25:39,010 --> 00:25:45,450
there were two copies can we eliminate

00:25:41,080 --> 00:25:49,180
the i guess the user to user copy also

00:25:45,450 --> 00:25:54,250
there's a closer and as a kernel look

00:25:49,180 --> 00:25:57,940
there's a curve the host so that the

00:25:54,250 --> 00:26:00,550
house right now extracts the memory in d

00:25:57,940 --> 00:26:03,480
host extracts the memory from one guest

00:26:00,550 --> 00:26:05,890
puts it at its Colonel copy and then

00:26:03,480 --> 00:26:09,580
eventually puts it back in the other

00:26:05,890 --> 00:26:12,460
Colonel other guest Colonel but this was

00:26:09,580 --> 00:26:14,700
on one unlike the transmitter side I

00:26:12,460 --> 00:26:16,650
think had two copies in laying right I

00:26:14,700 --> 00:26:19,800
think that I mean was a

00:26:16,650 --> 00:26:23,040
our slide yeah that one so basically

00:26:19,800 --> 00:26:25,530
this copy is because the host net

00:26:23,040 --> 00:26:27,810
extracts the data buffers from Verdi Oh

00:26:25,530 --> 00:26:32,340
probably because when I ask you is so

00:26:27,810 --> 00:26:33,900
small it can't really be wants to be

00:26:32,340 --> 00:26:36,600
able to give the data back but will

00:26:33,900 --> 00:26:38,580
copying the data twice yeah so I mean

00:26:36,600 --> 00:26:42,180
theoretically we could do something you

00:26:38,580 --> 00:26:44,040
know I mean Xander's things with page

00:26:42,180 --> 00:26:49,700
flipping but then patiently runs into

00:26:44,040 --> 00:26:49,700
the TLB issues with multiple CPUs so

00:26:51,140 --> 00:26:57,720
over here there's a question here um ok

00:26:55,950 --> 00:26:59,700
so he's got the right there he can't go

00:26:57,720 --> 00:27:01,950
back to the slide where you need to

00:26:59,700 --> 00:27:07,920
explain me disable GRL and able to zoom

00:27:01,950 --> 00:27:10,050
ok this one so regarding the t6 an

00:27:07,920 --> 00:27:13,200
intentional for disability initially you

00:27:10,050 --> 00:27:15,000
say that vmx its coastal oaks and now

00:27:13,200 --> 00:27:21,920
terrifically should be much more vm

00:27:15,000 --> 00:27:25,560
exits unless the killer does gia so yeah

00:27:21,920 --> 00:27:29,520
yeah actually it turns out that if if

00:27:25,560 --> 00:27:31,710
you have tea or so off the software will

00:27:29,520 --> 00:27:34,260
generate a bunch of frames which have

00:27:31,710 --> 00:27:36,450
transmitted morissette which causes the

00:27:34,260 --> 00:27:38,970
same number of vm exits ok so the

00:27:36,450 --> 00:27:40,650
Victorian villains twins now actually

00:27:38,970 --> 00:27:42,390
I'd implements judgment more but it

00:27:40,650 --> 00:27:44,450
because it doesn't do bike here limits

00:27:42,390 --> 00:27:48,360
it doesn't get that for the normal case

00:27:44,450 --> 00:27:50,130
so it only gets that for 1g one tsf rave

00:27:48,360 --> 00:27:53,940
it doesn't get that for the back-to-back

00:27:50,130 --> 00:27:55,320
frames we go into the allure of this

00:27:53,940 --> 00:27:57,360
thing to do that and maybe to address

00:27:55,320 --> 00:27:59,940
this can you explain to me what's your

00:27:57,360 --> 00:28:02,880
hypothesis regarding this one ah there's

00:27:59,940 --> 00:28:06,570
an option it was added to basically in

00:28:02,880 --> 00:28:09,270
the socket layer don't copy the data

00:28:06,570 --> 00:28:11,970
with Romelu mem copy but use the CPU

00:28:09,270 --> 00:28:14,970
instructions that do not cause the data

00:28:11,970 --> 00:28:18,210
not to pollute the cash and the idea

00:28:14,970 --> 00:28:20,190
here is that the user data can just get

00:28:18,210 --> 00:28:22,470
a memory we don't need to hit the CPU

00:28:20,190 --> 00:28:24,450
cache with it because in the normal case

00:28:22,470 --> 00:28:25,920
we're done we're gonna put

00:28:24,450 --> 00:28:27,600
memory and then we're going to hand it

00:28:25,920 --> 00:28:30,930
to a hardware device and hardware device

00:28:27,600 --> 00:28:33,150
is going to send it is the idea but

00:28:30,930 --> 00:28:34,890
reality in this case it doesn't help i

00:28:33,150 --> 00:28:37,950
don't know if i didn't test the

00:28:34,890 --> 00:28:42,660
north-south case it may help here tells

00:28:37,950 --> 00:28:47,160
22e junkman he came to study for Juve

00:28:42,660 --> 00:28:51,330
right from 40 from 40 something or now

00:28:47,160 --> 00:29:01,290
so it oh you started from 40 it went

00:28:51,330 --> 00:29:03,390
from 41 down to 62 34 yeah I mean yeah

00:29:01,290 --> 00:29:15,150
topped I think Tom didn't you turned it

00:29:03,390 --> 00:29:17,070
off so for your TCP congestion control

00:29:15,150 --> 00:29:21,150
test what's the difference in the

00:29:17,070 --> 00:29:23,550
upstream setup like the downstream of

00:29:21,150 --> 00:29:25,470
three basically this is all a script

00:29:23,550 --> 00:29:27,240
wrapper around that perf and it

00:29:25,470 --> 00:29:30,900
basically starts a bunch in that purse

00:29:27,240 --> 00:29:34,530
in parallel or with multiple threads in

00:29:30,900 --> 00:29:37,080
both directions and then does pings and

00:29:34,530 --> 00:29:40,500
measure the results so the uploads

00:29:37,080 --> 00:29:42,830
should be just just so they should both

00:29:40,500 --> 00:29:46,890
mean they should go through the same

00:29:42,830 --> 00:29:48,720
matching lines maybe one in the ideal

00:29:46,890 --> 00:29:51,930
case they should both do the same both

00:29:48,720 --> 00:29:53,820
directions I see so deep deep could you

00:29:51,930 --> 00:29:56,030
share the traces at least for BB

00:29:53,820 --> 00:29:58,590
articles with Emily curious about a

00:29:56,030 --> 00:30:01,290
regression years yeah I was going to

00:29:58,590 --> 00:30:04,320
give you a qui non awkward but whichever

00:30:01,290 --> 00:30:06,120
way is the least intrusive way and the

00:30:04,320 --> 00:30:11,310
most useful to you I can get that data

00:30:06,120 --> 00:30:13,530
ok great thanks but it should be really

00:30:11,310 --> 00:30:17,100
easy test we just set up it's not a hard

00:30:13,530 --> 00:30:20,720
it's not like I'm asking you to find a

00:30:17,100 --> 00:30:20,720
hundred gigabit network carton

00:30:22,410 --> 00:30:28,410
so um about the comment about 1500 to

00:30:26,510 --> 00:30:30,480
david stevens has done that for

00:30:28,410 --> 00:30:31,920
something it for spark and they got some

00:30:30,480 --> 00:30:33,420
really good numbers from that so that

00:30:31,920 --> 00:30:37,280
might there might be something in there

00:30:33,420 --> 00:30:37,280
that I generalizable my phone repair

00:30:43,219 --> 00:30:48,989
also you mentioned the special sockets

00:30:46,919 --> 00:30:51,690
families and you said it's their info

00:30:48,989 --> 00:30:54,029
inter vm aren't they for vm to

00:30:51,690 --> 00:30:56,279
hypervisor communication no these are

00:30:54,029 --> 00:31:00,179
for interview communication as well then

00:30:56,279 --> 00:31:02,700
how does the Metro stock use it you have

00:31:00,179 --> 00:31:06,029
to open make a different version of that

00:31:02,700 --> 00:31:10,499
purp that this cause it calls those

00:31:06,029 --> 00:31:13,950
those address families basically you

00:31:10,499 --> 00:31:17,099
give a guest ID instead of a socket yes

00:31:13,950 --> 00:31:23,399
and you instead of an IP address that

00:31:17,099 --> 00:31:25,409
you just get a bite screen you mentioned

00:31:23,399 --> 00:31:28,499
that you have not yet fiddled with them

00:31:25,409 --> 00:31:30,959
to you but I was ring I mean one

00:31:28,499 --> 00:31:33,719
possibility could be although you trade

00:31:30,959 --> 00:31:37,559
off some operational complexity giving a

00:31:33,719 --> 00:31:39,239
virtual giving the vm to interfaces when

00:31:37,559 --> 00:31:40,709
with the bog standard em to you toward

00:31:39,239 --> 00:31:43,769
the internet and when toward anything

00:31:40,709 --> 00:31:46,309
off the box and one with 65k you'd have

00:31:43,769 --> 00:31:49,109
to manage which routes are reachable yes

00:31:46,309 --> 00:31:51,839
but but I was gonna do it with one and

00:31:49,109 --> 00:31:58,019
just have the rods and any sad ratings

00:31:51,839 --> 00:32:00,419
are the routes yeah but you know tests

00:31:58,019 --> 00:32:05,399
get involved in stages so you start with

00:32:00,419 --> 00:32:07,320
the simple and go porcelain system just

00:32:05,399 --> 00:32:10,739
a quick comment Elizabeth musicians and

00:32:07,320 --> 00:32:13,200
interesting sometimes humans within your

00:32:10,739 --> 00:32:15,029
course of course and in tears since then

00:32:13,200 --> 00:32:17,789
should be different peppers because know

00:32:15,029 --> 00:32:19,499
each other so get inside the circuit

00:32:17,789 --> 00:32:22,200
also cannot equal and it's like a

00:32:19,499 --> 00:32:24,599
microphone in my system yeah and that is

00:32:22,200 --> 00:32:27,539
another issue it probably relate to

00:32:24,599 --> 00:32:29,269
pinning and stuff like that I didn't I

00:32:27,539 --> 00:32:32,269
didn't play around with pinning of

00:32:29,269 --> 00:32:32,269
guessing

00:32:43,520 --> 00:32:53,590
who's up next stacked be Lance thank you

00:32:51,320 --> 00:32:53,590

YouTube URL: https://www.youtube.com/watch?v=NjlyutCppcU


