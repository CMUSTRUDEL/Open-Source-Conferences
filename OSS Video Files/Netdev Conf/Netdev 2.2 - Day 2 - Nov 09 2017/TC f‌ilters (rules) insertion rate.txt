Title: TC fâ€Œilters (rules) insertion rate
Publication date: 2018-03-15
Playlist: Netdev 2.2 - Day 2 - Nov 09 2017
Description: 
	Speaker: Rony Efraim, Guy Shattah
Friday November 09th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?brakmo-tcpbpf-talk
Captions: 
	00:00:01,380 --> 00:00:11,700
see an approving the filter insertion

00:00:05,370 --> 00:00:14,480
rate so first I want to out go to talk

00:00:11,700 --> 00:00:17,940
ye wire here while we talk about that

00:00:14,480 --> 00:00:20,220
work that has been done and what we are

00:00:17,940 --> 00:00:25,080
the next step we think we need to be

00:00:20,220 --> 00:00:28,199
done a little bit describe how is the

00:00:25,080 --> 00:00:32,579
TCC handle the lookup flow the problem

00:00:28,199 --> 00:00:36,989
the the problem and progress so far flow

00:00:32,579 --> 00:00:40,379
of TC filter request talk again also

00:00:36,989 --> 00:00:44,159
about the RT n n RK analog and to

00:00:40,379 --> 00:00:49,739
suggest you know how to to improve the

00:00:44,159 --> 00:00:57,949
performance and how to execute

00:00:49,739 --> 00:01:04,799
accumulating work so why do you need a

00:00:57,949 --> 00:01:12,090
highly insertion rate so Sdn is asking

00:01:04,799 --> 00:01:14,039
to rapidly upgrade the rules this is a

00:01:12,090 --> 00:01:16,560
growing number in the beginning we

00:01:14,039 --> 00:01:19,530
thought it was a few thousands but now

00:01:16,560 --> 00:01:23,240
we understand that it's not mm it's more

00:01:19,530 --> 00:01:25,770
than 1 million updates per second

00:01:23,240 --> 00:01:28,049
especially when you talk about flows so

00:01:25,770 --> 00:01:31,049
flows can be million or flows you want

00:01:28,049 --> 00:01:36,030
to update them in a second you don't

00:01:31,049 --> 00:01:38,460
want let's to take a lot of time and why

00:01:36,030 --> 00:01:40,590
TC because this e we use currently by

00:01:38,460 --> 00:01:50,189
all that's the way we do the obvious

00:01:40,590 --> 00:01:54,479
offload so when we start working with

00:01:50,189 --> 00:01:56,460
open V switch and we offload it to TC we

00:01:54,479 --> 00:01:59,310
try to do some measure measurement and

00:01:56,460 --> 00:02:02,819
we understand that this is not scaling

00:01:59,310 --> 00:02:05,819
well and we're trying to push a table of

00:02:02,819 --> 00:02:08,819
million rules and then Jenny that's did

00:02:05,819 --> 00:02:13,890
the testing went home it didn't finish

00:02:08,819 --> 00:02:15,280
yet it's took few hours so we understand

00:02:13,890 --> 00:02:20,080
we have a problem

00:02:15,280 --> 00:02:26,340
and we did some work and now we have can

00:02:20,080 --> 00:02:30,070
have about 50 K is rules per second and

00:02:26,340 --> 00:02:32,770
despite recent improvement we still want

00:02:30,070 --> 00:02:34,750
to to get to a higher number

00:02:32,770 --> 00:02:37,090
that's what customers are talking now

00:02:34,750 --> 00:02:41,740
right now talking about a million per

00:02:37,090 --> 00:02:46,060
second million updates per second of

00:02:41,740 --> 00:02:47,920
course to achieve this number we need to

00:02:46,060 --> 00:02:55,180
do something big and not something just

00:02:47,920 --> 00:02:58,810
not small improvements so I want to have

00:02:55,180 --> 00:03:03,490
two to talk about how is TC handle look

00:02:58,810 --> 00:03:06,490
up in the in the kernel so there is a TC

00:03:03,490 --> 00:03:09,760
flow in the TC flow they're a search for

00:03:06,490 --> 00:03:13,270
the device it's a linear search lookup

00:03:09,760 --> 00:03:16,720
for a specific you disk find the class

00:03:13,270 --> 00:03:19,680
and that's attached to the QP e to the

00:03:16,720 --> 00:03:24,239
ik you disk find the classifier and

00:03:19,680 --> 00:03:27,730
inside the classify you do a lookup to

00:03:24,239 --> 00:03:33,310
for the head for the handle with a

00:03:27,730 --> 00:03:35,620
classifier get so this is a method that

00:03:33,310 --> 00:03:40,110
search in a linear search in a linear

00:03:35,620 --> 00:03:47,049
search then you set the action with the

00:03:40,110 --> 00:03:49,959
change so that's how its work okay so

00:03:47,049 --> 00:03:53,470
the problem is that's inside the

00:03:49,959 --> 00:03:56,230
classifier the lookup rule and handle

00:03:53,470 --> 00:04:00,190
the the classifier the classifier get

00:03:56,230 --> 00:04:02,170
method is doing a linear search and when

00:04:00,190 --> 00:04:06,580
you try to do a linear search with a

00:04:02,170 --> 00:04:10,780
mallet 1 million full entry its second

00:04:06,580 --> 00:04:15,310
and over 1 million so it's a lot of time

00:04:10,780 --> 00:04:18,250
and also the set actions was implemented

00:04:15,310 --> 00:04:20,560
with a with a hash that is good start

00:04:18,250 --> 00:04:25,660
but the problem that is a hash refer

00:04:20,560 --> 00:04:27,740
buckets and hash size was 16 so again

00:04:25,660 --> 00:04:32,479
it's kind of a linear search

00:04:27,740 --> 00:04:35,810
so we try to do to think what to do if 2

00:04:32,479 --> 00:04:38,599
1 in the beginning want to use a ha a

00:04:35,810 --> 00:04:41,770
big hash and we understand that we will

00:04:38,599 --> 00:04:44,750
have issue with the memory performance

00:04:41,770 --> 00:04:50,330
that will require in advance a lot of

00:04:44,750 --> 00:04:57,199
memory and so then we we decided to use

00:04:50,330 --> 00:05:00,979
IDR we did request idea 64 that wasn't

00:04:57,199 --> 00:05:04,550
in the kernel now it is so those patches

00:05:00,979 --> 00:05:08,570
are in in the kernel and as I said

00:05:04,550 --> 00:05:10,370
before and now we can reach about 50k

00:05:08,570 --> 00:05:13,930
rules per second of course depends on

00:05:10,370 --> 00:05:17,990
the processor it could be even a hundred

00:05:13,930 --> 00:05:20,780
K rules update per second so this give a

00:05:17,990 --> 00:05:23,900
boost of performance so now it instead

00:05:20,780 --> 00:05:27,740
of waiting a few hours to fulfill 1

00:05:23,900 --> 00:05:32,199
million rules you can take it in less

00:05:27,740 --> 00:05:35,960
than less than 10 minutes 10 seconds so

00:05:32,199 --> 00:05:43,550
it's improved but we still want to to

00:05:35,960 --> 00:05:44,780
make it faster ok so I will start

00:05:43,550 --> 00:05:48,169
described a little bit higher level

00:05:44,780 --> 00:05:51,680
because every TC filter is coming from

00:05:48,169 --> 00:05:56,659
netlink so it start about a net link

00:05:51,680 --> 00:05:59,479
layer then it's called to the RTL to the

00:05:56,659 --> 00:06:03,580
RT net link sorry let's accept the

00:05:59,479 --> 00:06:08,150
message of course lock the RT analog and

00:06:03,580 --> 00:06:11,120
send it to TC in the TC layer it's used

00:06:08,150 --> 00:06:13,750
a classifier and we hope that it's also

00:06:11,120 --> 00:06:16,490
go to the hardware that's an optional

00:06:13,750 --> 00:06:19,539
but the performance that I show before

00:06:16,490 --> 00:06:24,139
they are software only it's mean skip as

00:06:19,539 --> 00:06:26,330
skip harder so it's not involved the the

00:06:24,139 --> 00:06:30,909
time that's its take the hardware to

00:06:26,330 --> 00:06:30,909
process those rules

00:06:32,100 --> 00:06:39,960
so s Florian probably you saw his

00:06:35,880 --> 00:06:44,430
lecture yesterday so the best way to

00:06:39,960 --> 00:06:47,100
resolve that is through the problem okay

00:06:44,430 --> 00:06:53,070
so the problem is the lock is the artena

00:06:47,100 --> 00:06:56,100
lock that's you you need to lock on

00:06:53,070 --> 00:07:00,720
every arcane Altran L so it's mean

00:06:56,100 --> 00:07:05,250
that's if you can't do you can't use

00:07:00,720 --> 00:07:07,140
many threads in order to use to to have

00:07:05,250 --> 00:07:10,920
multiply insertions from different

00:07:07,140 --> 00:07:13,380
threads and increase the performance so

00:07:10,920 --> 00:07:17,670
the best way to solve it if it week if

00:07:13,380 --> 00:07:19,860
we can not if we don't need to use RT

00:07:17,670 --> 00:07:24,240
analog that's the best way to do that

00:07:19,860 --> 00:07:33,330
but it's kind it's currently very hard

00:07:24,240 --> 00:07:38,120
to to resolve this issue so as I said

00:07:33,330 --> 00:07:41,190
before user process send multiple TC and

00:07:38,120 --> 00:07:43,020
filter requests in a parallel but the

00:07:41,190 --> 00:07:53,070
RNA lock is blocking us from do it in

00:07:43,020 --> 00:07:55,320
parallel so what we are suggesting so

00:07:53,070 --> 00:07:55,770
breaking the lock that's what I said

00:07:55,320 --> 00:07:58,620
before

00:07:55,770 --> 00:08:00,920
so as I mention it it's difficult a

00:07:58,620 --> 00:08:04,830
difficult task

00:08:00,920 --> 00:08:08,400
Florian started to do the job but look

00:08:04,830 --> 00:08:15,090
like it's take a lot of time and it's a

00:08:08,400 --> 00:08:18,720
hard work and the other option is that's

00:08:15,090 --> 00:08:21,930
we're suggesting is to dude to use the

00:08:18,720 --> 00:08:25,170
to do a multi thread under the lock so

00:08:21,930 --> 00:08:29,640
you you take the RTN a lock you batch a

00:08:25,170 --> 00:08:36,570
lot of rule F filters that you want to

00:08:29,640 --> 00:08:39,479
add and then you use multi thread below

00:08:36,570 --> 00:08:44,240
the lock so then you can increase the

00:08:39,479 --> 00:08:44,240
performance so

00:08:44,300 --> 00:08:53,280
we have to suggestion how to implement

00:08:46,830 --> 00:08:56,840
that the suggestion a is to use multiply

00:08:53,280 --> 00:08:59,610
multiply and netlink messages and

00:08:56,840 --> 00:09:04,650
suggestions B is to compound a netlink

00:08:59,610 --> 00:09:08,730
message of course the issues for both of

00:09:04,650 --> 00:09:12,420
them is the parallel processing implies

00:09:08,730 --> 00:09:15,990
all actions must be a massive mustn't

00:09:12,420 --> 00:09:18,720
must not have dependencies between them

00:09:15,990 --> 00:09:22,230
because if we want to do the imperil of

00:09:18,720 --> 00:09:24,870
them we can't we we don't want to try to

00:09:22,230 --> 00:09:27,960
enforce the the dependencies between

00:09:24,870 --> 00:09:32,310
them and the parallel processing force

00:09:27,960 --> 00:09:35,460
current to run multi thread so the user

00:09:32,310 --> 00:09:37,460
start from a single thread and then we

00:09:35,460 --> 00:09:41,520
are processing it in a few different

00:09:37,460 --> 00:09:44,490
course it is we're not sure this is

00:09:41,520 --> 00:09:52,380
something nice but that's the only way

00:09:44,490 --> 00:09:55,920
to solve this issue so the first

00:09:52,380 --> 00:09:58,470
suggestion that we thought about is to

00:09:55,920 --> 00:10:01,560
extend a net field the net linked

00:09:58,470 --> 00:10:03,660
messages with a begin in an end then you

00:10:01,560 --> 00:10:13,710
can understand that this is the

00:10:03,660 --> 00:10:17,280
beginning of a block of of match

00:10:13,710 --> 00:10:19,080
messages and the end then you can

00:10:17,280 --> 00:10:26,820
understand us now we can go to process

00:10:19,080 --> 00:10:28,200
them so so accumulating the message what

00:10:26,820 --> 00:10:32,190
we're doing we accumulate the message

00:10:28,200 --> 00:10:34,800
list to maintain pair users so because

00:10:32,190 --> 00:10:38,430
if it's from from the cameras

00:10:34,800 --> 00:10:41,130
perspective you get the rules one by one

00:10:38,430 --> 00:10:43,860
it could be that two processes writing

00:10:41,130 --> 00:10:45,960
to you at the same time you need to

00:10:43,860 --> 00:10:51,450
understand and have to maintain a list

00:10:45,960 --> 00:10:53,700
for each for each process ID because you

00:10:51,450 --> 00:10:56,900
can have multiple process that's writing

00:10:53,700 --> 00:10:56,900
you in the same time and

00:10:58,490 --> 00:11:06,060
so when you get the end then you can

00:11:01,680 --> 00:11:14,820
process them directly on a single on few

00:11:06,060 --> 00:11:18,150
cues of you course the other suggestion

00:11:14,820 --> 00:11:23,190
is to use another a compound netlink

00:11:18,150 --> 00:11:28,220
message to do a batching so the idea is

00:11:23,190 --> 00:11:28,220
to to encapsulate a message with

00:11:28,850 --> 00:11:35,570
multiple TC filter requests so it's a

00:11:31,860 --> 00:11:39,480
single message that can have a lot of

00:11:35,570 --> 00:11:44,100
filters that you want to processing in

00:11:39,480 --> 00:11:47,730
singleton in a batch the fetching work

00:11:44,100 --> 00:11:50,640
can be by sending all messages today to

00:11:47,730 --> 00:11:55,530
the existing TC layer in parallel at

00:11:50,640 --> 00:11:59,250
once and as you can see the idea is to

00:11:55,530 --> 00:12:03,720
have another a new TC message batch

00:11:59,250 --> 00:12:07,130
header and in the network message will

00:12:03,720 --> 00:12:11,100
be like today so in today you have a net

00:12:07,130 --> 00:12:14,520
NL message header and then a TC message

00:12:11,100 --> 00:12:16,620
header and then the attributes so in the

00:12:14,520 --> 00:12:21,930
batching we will have another batching

00:12:16,620 --> 00:12:25,830
header in each in front of every TC

00:12:21,930 --> 00:12:28,620
message in attributes and of course the

00:12:25,830 --> 00:12:39,090
last one will be an attribute of 0 size

00:12:28,620 --> 00:12:42,300
so we understand this is the last one so

00:12:39,090 --> 00:12:46,070
the way we we want to accumulate the

00:12:42,300 --> 00:12:48,600
work to execute it in its in a work you

00:12:46,070 --> 00:12:51,030
so it's not mean all the time that's

00:12:48,600 --> 00:12:54,750
we're going to use Multi multi course

00:12:51,030 --> 00:12:59,400
but we at least have multi processing

00:12:54,750 --> 00:13:03,530
that can be benefit can benefit and the

00:12:59,400 --> 00:13:03,530
performance we for sure will be better

00:13:07,630 --> 00:13:15,889
so the on the first method face to a the

00:13:13,940 --> 00:13:18,050
the first the multiply Network message

00:13:15,889 --> 00:13:21,649
multiplying Network messages with the

00:13:18,050 --> 00:13:24,470
begin with the end a result return per

00:13:21,649 --> 00:13:29,209
net link message so it's also a kind of

00:13:24,470 --> 00:13:34,430
overhead because in the other suggestion

00:13:29,209 --> 00:13:36,259
we can use a single success for every if

00:13:34,430 --> 00:13:39,079
all them they all of the rule success

00:13:36,259 --> 00:13:41,500
that's what we would like to have so

00:13:39,079 --> 00:13:51,230
there will be a single success message

00:13:41,500 --> 00:13:54,589
so we also illuminate this overhead so

00:13:51,230 --> 00:13:59,630
we did some compression the confirmation

00:13:54,589 --> 00:14:02,680
between those two suggestion so as you

00:13:59,630 --> 00:14:06,170
can see the second one is a much more

00:14:02,680 --> 00:14:10,370
efficient so the performance will be

00:14:06,170 --> 00:14:16,310
better but the first one is more is more

00:14:10,370 --> 00:14:19,760
generic so so we're have some debate and

00:14:16,310 --> 00:14:22,910
I think the reason that's we're aiming

00:14:19,760 --> 00:14:26,660
to go to 1 million look like we don't

00:14:22,910 --> 00:14:31,220
have any any other options and to go for

00:14:26,660 --> 00:14:33,800
the second option to to be that's the

00:14:31,220 --> 00:14:47,029
only one that will deliver the required

00:14:33,800 --> 00:14:53,000
performance questions or what use what

00:14:47,029 --> 00:14:56,260
you think is the best well I have some

00:14:53,000 --> 00:14:56,260
questions can you

00:14:57,390 --> 00:15:00,450
[Music]

00:15:04,470 --> 00:15:10,690
Ronnie thanks I think some some of the

00:15:08,440 --> 00:15:13,750
stuff you've been talking about for

00:15:10,690 --> 00:15:15,670
nothing they begin and end flags and so

00:15:13,750 --> 00:15:18,520
what could be interesting if in case

00:15:15,670 --> 00:15:22,420
that TC wants to support two-phase

00:15:18,520 --> 00:15:24,520
commit protocol but I think I think

00:15:22,420 --> 00:15:26,140
there is all the infrastructure is in

00:15:24,520 --> 00:15:28,150
place already to do the batching so

00:15:26,140 --> 00:15:30,610
basically what you have to do is that

00:15:28,150 --> 00:15:34,600
you have to take a big buffer and you

00:15:30,610 --> 00:15:37,240
have to start placing the netting

00:15:34,600 --> 00:15:39,520
messages one after another and then want

00:15:37,240 --> 00:15:41,170
once that buffer gets full do you get

00:15:39,520 --> 00:15:42,760
another buffer and you keep adding

00:15:41,170 --> 00:15:44,920
messages there and then we would use

00:15:42,760 --> 00:15:47,920
send message and you just pass all those

00:15:44,920 --> 00:15:50,770
buffers by RI of X and all that is going

00:15:47,920 --> 00:15:52,870
to be handled as a single batch so

00:15:50,770 --> 00:15:55,000
you're going to save all the Cisco

00:15:52,870 --> 00:15:59,080
interactions that is actually what is

00:15:55,000 --> 00:16:00,460
slowing down and tired I think and I

00:15:59,080 --> 00:16:03,130
think it's going to be something it's

00:16:00,460 --> 00:16:05,140
going to be close to do to do what you

00:16:03,130 --> 00:16:08,440
need for your requirements I remember in

00:16:05,140 --> 00:16:12,339
that in the first net that happen in

00:16:08,440 --> 00:16:17,740
Canada I think my people were reporting

00:16:12,339 --> 00:16:20,760
salt like 1 million messages basically

00:16:17,740 --> 00:16:22,750
to add elements to 2nf table sets and

00:16:20,760 --> 00:16:25,540
following this approach on insulin use

00:16:22,750 --> 00:16:28,330
if they were basically close to one

00:16:25,540 --> 00:16:30,640
second something like that so I think I

00:16:28,330 --> 00:16:32,800
think what with infrastructure in place

00:16:30,640 --> 00:16:34,870
and some changes in user space it should

00:16:32,800 --> 00:16:36,640
be it should be good enough unless you

00:16:34,870 --> 00:16:38,410
want to add two-phase commit protocol to

00:16:36,640 --> 00:16:40,080
GC where all these areas could be very

00:16:38,410 --> 00:16:42,610
useful

00:16:40,080 --> 00:16:46,360
so currently we're not looking on a

00:16:42,610 --> 00:16:47,860
two-phase commit but again I think the

00:16:46,360 --> 00:16:53,880
problem that's facing right now that's

00:16:47,860 --> 00:16:56,380
the TC insertion itself take time and

00:16:53,880 --> 00:16:59,589
we're trying to do other optimization

00:16:56,380 --> 00:17:03,910
there and we still struggling to go to

00:16:59,589 --> 00:17:07,929
the 100k rules update per second

00:17:03,910 --> 00:17:10,990
so we don't the only way to do that I

00:17:07,929 --> 00:17:16,280
think it's to have multi-core to have

00:17:10,990 --> 00:17:18,290
multi processing that's that's

00:17:16,280 --> 00:17:22,400
unless we will find you know to optimize

00:17:18,290 --> 00:17:25,510
it by 10 so this is noted not easy task

00:17:22,400 --> 00:17:29,840
to do because we already did the first

00:17:25,510 --> 00:17:32,480
optimization yeah so using multiple

00:17:29,840 --> 00:17:34,430
processor core won't help if they over

00:17:32,480 --> 00:17:39,110
content on the routine and look anyway

00:17:34,430 --> 00:17:41,360
so are you sure of that because that's

00:17:39,110 --> 00:17:43,880
the reason we do it in under the lock we

00:17:41,360 --> 00:17:47,360
batch them we get a batch and the back

00:17:43,880 --> 00:17:49,550
we check each class it's filtered under

00:17:47,360 --> 00:17:53,060
the batch and process them in parallel

00:17:49,550 --> 00:17:55,270
on different course I don't see how it

00:17:53,060 --> 00:17:55,270
can work

00:18:02,320 --> 00:18:08,000
so one thing that's I think is important

00:18:05,840 --> 00:18:09,860
to recognize and it it could help you

00:18:08,000 --> 00:18:11,720
wrap your head around this problem in a

00:18:09,860 --> 00:18:12,920
different way perhaps is that usually

00:18:11,720 --> 00:18:15,320
when you're loading a million entries

00:18:12,920 --> 00:18:17,090
and into any kind of thing like a fib

00:18:15,320 --> 00:18:20,150
table or the TC classifier they're

00:18:17,090 --> 00:18:22,250
related in some way like it's a it's a

00:18:20,150 --> 00:18:25,430
range of addresses from X to Y or

00:18:22,250 --> 00:18:28,340
there's some pattern in the entries ins

00:18:25,430 --> 00:18:30,260
in some way and if you could describe

00:18:28,340 --> 00:18:32,210
that relationship amongst the entries

00:18:30,260 --> 00:18:33,830
you can compress the representation of

00:18:32,210 --> 00:18:35,840
the message that you send down into the

00:18:33,830 --> 00:18:37,520
kernel and then expand the entries as as

00:18:35,840 --> 00:18:40,580
they get created and I think you should

00:18:37,520 --> 00:18:42,740
really think seriously about that so we

00:18:40,580 --> 00:18:45,710
thought about it but we don't want to

00:18:42,740 --> 00:18:50,720
limit that the batching only to support

00:18:45,710 --> 00:18:52,670
like a single net device or a single a

00:18:50,720 --> 00:18:55,250
single net device a single cutest

00:18:52,670 --> 00:18:55,880
because the N on a single priority most

00:18:55,250 --> 00:18:59,480
of the rules

00:18:55,880 --> 00:19:02,390
probably you push to the same table to

00:18:59,480 --> 00:19:06,950
the same QD to the same priority in a

00:19:02,390 --> 00:19:10,420
queue disk but then we are limiting to

00:19:06,950 --> 00:19:10,420
the batching to be very very specific

00:19:16,299 --> 00:19:21,799
okay so just to expand on what they've

00:19:19,429 --> 00:19:24,230
said basically representing a lot of

00:19:21,799 --> 00:19:26,360
rules is that's wild cards right so

00:19:24,230 --> 00:19:27,769
typically in the offload case we think

00:19:26,360 --> 00:19:30,860
about devices that can offload wild

00:19:27,769 --> 00:19:32,750
cards but i t:c already has the notion

00:19:30,860 --> 00:19:34,970
of representing a lot of rules you know

00:19:32,750 --> 00:19:37,820
in a compressed way which is a wild card

00:19:34,970 --> 00:19:40,850
representation let's talk about rules

00:19:37,820 --> 00:19:45,890
that could be why card so you you could

00:19:40,850 --> 00:19:49,549
implement in the sort of implement wild

00:19:45,890 --> 00:19:51,649
card by offloading all the rules all the

00:19:49,549 --> 00:19:54,529
microphones and all the simple rules but

00:19:51,649 --> 00:19:58,190
that way you could send a single command

00:19:54,529 --> 00:19:59,929
right bets the command by the wild card

00:19:58,190 --> 00:20:01,940
representation and implement them all

00:19:59,929 --> 00:20:03,679
under the lock yes so so the problem

00:20:01,940 --> 00:20:05,539
that's they're not white card if there

00:20:03,679 --> 00:20:08,360
were wild cards once it was a single

00:20:05,539 --> 00:20:13,639
rule right so we're talking about exact

00:20:08,360 --> 00:20:15,710
match on five tuples technically so it's

00:20:13,639 --> 00:20:18,200
not a wild card I understand but wild

00:20:15,710 --> 00:20:21,169
card is the like an answer to Dave right

00:20:18,200 --> 00:20:22,370
the way to represent a lot of rules

00:20:21,169 --> 00:20:24,470
together if they do have a relationship

00:20:22,370 --> 00:20:27,409
that's wild card if you're talking about

00:20:24,470 --> 00:20:28,940
a million random rules and wild card

00:20:27,409 --> 00:20:31,610
does not apply no but what Dave said

00:20:28,940 --> 00:20:35,269
that if you have won millions IP

00:20:31,610 --> 00:20:37,909
addresses so it's not a wild card it's

00:20:35,269 --> 00:20:42,260
not a range but they still on the same

00:20:37,909 --> 00:20:45,169
table so they have the same mask what

00:20:42,260 --> 00:20:46,580
they are you can group them together you

00:20:45,169 --> 00:20:49,760
don't need to have the mask you don't

00:20:46,580 --> 00:20:52,820
need there are other things that can be

00:20:49,760 --> 00:20:56,929
compressed not the rule itself not the

00:20:52,820 --> 00:21:02,570
match a row buddy do you want to say

00:20:56,929 --> 00:21:05,799
something okay so I'm just gonna build

00:21:02,570 --> 00:21:08,720
up maybe on Publicis point so publicist

00:21:05,799 --> 00:21:11,659
we can already do batching if you have

00:21:08,720 --> 00:21:14,090
multiple netic messages right is that

00:21:11,659 --> 00:21:17,630
not applicable to you because you send

00:21:14,090 --> 00:21:19,669
one big sent message but it has 50 it

00:21:17,630 --> 00:21:20,690
has a thousand little Achmed messages

00:21:19,669 --> 00:21:22,760
each with a header

00:21:20,690 --> 00:21:24,770
yes but for each of them you need to

00:21:22,760 --> 00:21:26,630
take the RT and n log no no you take one

00:21:24,770 --> 00:21:28,880
art in a lock you do a send message grab

00:21:26,630 --> 00:21:33,020
the art in a lock and you can send a

00:21:28,880 --> 00:21:36,380
and TC commands from the RT netlink

00:21:33,020 --> 00:21:38,030
space right that's one the other one is

00:21:36,380 --> 00:21:41,470
can you go back to your diagram where

00:21:38,030 --> 00:21:41,470
you're showing all this multiple threads

00:21:42,520 --> 00:21:47,480
they showing the work of threads and

00:21:45,140 --> 00:21:50,330
next one yeah so you're assuming all

00:21:47,480 --> 00:21:52,550
these CPUs are going to be accessing

00:21:50,330 --> 00:21:54,310
some hardware or what are these things

00:21:52,550 --> 00:21:56,810
so first we're talking right now

00:21:54,310 --> 00:21:58,640
software only this is software only so

00:21:56,810 --> 00:22:00,290
they're just updating and they're going

00:21:58,640 --> 00:22:02,960
to hold some continue obviously if

00:22:00,290 --> 00:22:06,920
you're holding something that like no

00:22:02,960 --> 00:22:08,960
will update the IDR lock right they're

00:22:06,920 --> 00:22:12,620
going to be competing what is the

00:22:08,960 --> 00:22:15,560
bottleneck in this case the memory okay

00:22:12,620 --> 00:22:18,380
so it's but okay and you need to have a

00:22:15,560 --> 00:22:20,840
small lock but and it's been long and

00:22:18,380 --> 00:22:22,190
not kind of to lock all the process so

00:22:20,840 --> 00:22:24,590
the only way this would work well is

00:22:22,190 --> 00:22:25,310
everything there's totally no dependency

00:22:24,590 --> 00:22:27,950
at all right

00:22:25,310 --> 00:22:30,140
is that correct because I'm not sure it

00:22:27,950 --> 00:22:31,970
was the possibility of reordering based

00:22:30,140 --> 00:22:34,400
on how it was sent from user space that

00:22:31,970 --> 00:22:35,750
doesn't matter maybe yes so that that's

00:22:34,400 --> 00:22:37,940
the reason that we said all the batch

00:22:35,750 --> 00:22:43,570
need to be without any there's no

00:22:37,940 --> 00:22:52,700
dependency no dependency right okay okay

00:22:43,570 --> 00:22:55,790
anybody else with the question so it's

00:22:52,700 --> 00:22:57,770
nice that you have a way of or an

00:22:55,790 --> 00:23:00,230
application that has no dependencies

00:22:57,770 --> 00:23:02,150
between the pieces but in order to make

00:23:00,230 --> 00:23:04,640
this a little more generic which was

00:23:02,150 --> 00:23:08,660
part of your suggestion a suggestion B

00:23:04,640 --> 00:23:10,490
problem can you find a way to put a some

00:23:08,660 --> 00:23:13,300
sort of flagging in this general

00:23:10,490 --> 00:23:16,460
facility that you want to add that says

00:23:13,300 --> 00:23:19,520
these are all independent or that we

00:23:16,460 --> 00:23:21,380
need to do here's your batch and you

00:23:19,520 --> 00:23:22,520
know set up the set up the lock but each

00:23:21,380 --> 00:23:24,620
one of these needs to be done

00:23:22,520 --> 00:23:30,860
individually instead of multiprocessing

00:23:24,620 --> 00:23:32,660
them so if they are not in you can't

00:23:30,860 --> 00:23:36,410
process them if you need to process them

00:23:32,660 --> 00:23:38,770
independently so if they need to make an

00:23:36,410 --> 00:23:41,150
order between them so you can't use a

00:23:38,770 --> 00:23:42,260
work use and distribute them on

00:23:41,150 --> 00:23:45,380
different course

00:23:42,260 --> 00:23:50,650
so you you want gain what I'm trying to

00:23:45,380 --> 00:23:56,510
push so this is the default attribute

00:23:50,650 --> 00:23:58,880
for batching so because I don't see how

00:23:56,510 --> 00:24:02,510
it's helped if you if you're trying to

00:23:58,880 --> 00:24:05,420
batch and tell us that's you we can't be

00:24:02,510 --> 00:24:07,790
paralyzed okay Ronnie loves beer you'll

00:24:05,420 --> 00:24:09,950
find him at the happy hour today come

00:24:07,790 --> 00:24:15,850
and talk to him we're gonna wrap it up

00:24:09,950 --> 00:24:19,240
here let's give him a thank you

00:24:15,850 --> 00:24:19,240

YouTube URL: https://www.youtube.com/watch?v=dL4vZmADml4


