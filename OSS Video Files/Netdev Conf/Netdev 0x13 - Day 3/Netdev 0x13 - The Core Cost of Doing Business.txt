Title: Netdev 0x13 - The Core Cost of Doing Business
Publication date: 2019-05-26
Playlist: Netdev 0x13 - Day 3
Description: 
	Don Wallwork and Andy Gospodarek present a formula for estimating the minimum number of server cores - running Linux network kernel - for a particular network service at a given rate and frame size. They cover IPv4 and IPv6 as well and VxLAN encapsulated or tunneled traffic with the goal of understanding how the utilization of these cores impacts planning and deployment of new systems.

More info:
https://netdevconf.org/0x13/session.html?talk-core-cost
Captions: 
	00:00:01,250 --> 00:00:10,530
ready all right we're getting studded

00:00:03,920 --> 00:00:13,320
old thanks Jamal all right so we get

00:00:10,530 --> 00:00:16,710
near the end of the OBS and TC track it

00:00:13,320 --> 00:00:19,529
seems like so thanks everyone for

00:00:16,710 --> 00:00:21,750
staying around for a few minutes so I'm

00:00:19,529 --> 00:00:23,789
Andy Gosford Eric this is Don wall work

00:00:21,750 --> 00:00:25,800
from Broadcom we're gonna talk about our

00:00:23,789 --> 00:00:28,800
catchy title of a core cost of doing

00:00:25,800 --> 00:00:31,140
business try to characterize what it

00:00:28,800 --> 00:00:35,270
takes to use OBS and maybe deploy OBS

00:00:31,140 --> 00:00:37,260
and look at kind of a typical use case

00:00:35,270 --> 00:00:40,260
we know that a lot of people have

00:00:37,260 --> 00:00:42,540
religion around OBS and some love it

00:00:40,260 --> 00:00:45,180
some don't people from both groups are

00:00:42,540 --> 00:00:47,370
probably in this room but the fact of

00:00:45,180 --> 00:00:50,280
the matter is it's it's extremely

00:00:47,370 --> 00:00:51,600
popular we have a lot of discussions

00:00:50,280 --> 00:00:53,039
with customers about it all the time so

00:00:51,600 --> 00:00:57,360
we thought we'd sort of take a look at

00:00:53,039 --> 00:00:58,590
it so typical you might think of a

00:00:57,360 --> 00:01:01,050
typical server I don't know why I keep

00:00:58,590 --> 00:01:03,780
looking there I can look here anyway if

00:01:01,050 --> 00:01:05,519
this were say a 24 course system if I'm

00:01:03,780 --> 00:01:08,400
doing my math right 6 times 4 there we

00:01:05,519 --> 00:01:09,510
go we've got maybe six of those or half

00:01:08,400 --> 00:01:10,470
of those cores are available for

00:01:09,510 --> 00:01:12,860
actually running virtualized

00:01:10,470 --> 00:01:15,659
applications whether it be containers

00:01:12,860 --> 00:01:17,009
VMs whatever and let's say because

00:01:15,659 --> 00:01:18,570
you're running open V switch you decide

00:01:17,009 --> 00:01:21,479
that you need to allocate 4 cores for

00:01:18,570 --> 00:01:23,130
doing Network it's the ends you might

00:01:21,479 --> 00:01:24,720
also have some storage backing that

00:01:23,130 --> 00:01:27,990
needs to be done either those for the

00:01:24,720 --> 00:01:30,390
hosts or for the applications those are

00:01:27,990 --> 00:01:32,159
the esses we picked good letters for

00:01:30,390 --> 00:01:34,170
these we feel like H would be the

00:01:32,159 --> 00:01:37,320
hypervisor management or the host

00:01:34,170 --> 00:01:38,729
management and actually H would be the

00:01:37,320 --> 00:01:39,900
hypervisor M would be the management

00:01:38,729 --> 00:01:42,030
networking because you don't need much

00:01:39,900 --> 00:01:44,939
of that we really focused on data plane

00:01:42,030 --> 00:01:46,350
so the question is you know can you

00:01:44,939 --> 00:01:47,759
migrate some of these off what if you

00:01:46,350 --> 00:01:49,409
had a bunch of cores that were available

00:01:47,759 --> 00:01:54,750
or what if you had fancy hardware

00:01:49,409 --> 00:01:57,290
offload what could you do all right so

00:01:54,750 --> 00:02:00,750
this next slide kind of goes through the

00:01:57,290 --> 00:02:03,329
test setup that we use to to do all of

00:02:00,750 --> 00:02:06,750
our characterization so we had a dual

00:02:03,329 --> 00:02:09,420
socket Intel system in this case there

00:02:06,750 --> 00:02:11,220
were 32 cores we're using a Broadcom 2

00:02:09,420 --> 00:02:14,820
by 25 gig net

00:02:11,220 --> 00:02:18,030
and what we did was we sent the traffic

00:02:14,820 --> 00:02:21,210
in on one in on both ports and then

00:02:18,030 --> 00:02:24,660
we're measuring the system load under

00:02:21,210 --> 00:02:28,710
different profiles so we're running Red

00:02:24,660 --> 00:02:30,450
Hat Linux seven six and this is as much

00:02:28,710 --> 00:02:33,480
as possible this is just straight out of

00:02:30,450 --> 00:02:36,630
the box so that's the the kernel version

00:02:33,480 --> 00:02:39,990
that ships with seven six and the the

00:02:36,630 --> 00:02:47,910
inbox open beasts which we did of course

00:02:39,990 --> 00:02:51,120
do affinity to pin the OBS processes or

00:02:47,910 --> 00:02:57,110
the really the Rings to the same Numa

00:02:51,120 --> 00:03:00,810
node so that we don't get inter Numa

00:02:57,110 --> 00:03:02,940
performance penalties and we're just

00:03:00,810 --> 00:03:06,480
doing simple layer three forwarding

00:03:02,940 --> 00:03:08,610
there's no mat and no no filtering

00:03:06,480 --> 00:03:13,620
anything like that just pretty pretty

00:03:08,610 --> 00:03:15,420
straightforward to set up so what we

00:03:13,620 --> 00:03:18,060
wanted to characterize in this first set

00:03:15,420 --> 00:03:23,610
of tests that we ran was to understand

00:03:18,060 --> 00:03:26,670
the per core performance that we get and

00:03:23,610 --> 00:03:29,820
in this case is that this is using I mix

00:03:26,670 --> 00:03:34,590
packet I mix average packet size and we

00:03:29,820 --> 00:03:36,720
wanted to see a how much performance do

00:03:34,590 --> 00:03:38,940
we get on a single core and does that

00:03:36,720 --> 00:03:41,459
performance scale linearly as we

00:03:38,940 --> 00:03:43,890
increase the number of cores and what we

00:03:41,459 --> 00:03:46,410
did what we saw was that this did scale

00:03:43,890 --> 00:03:50,550
very linearly as we increase the number

00:03:46,410 --> 00:03:52,980
of cores we're getting about 1.3 million

00:03:50,550 --> 00:03:56,100
packets per second with a single core

00:03:52,980 --> 00:03:59,970
and that number was very consistent up

00:03:56,100 --> 00:04:02,070
to 4 cores in this test setup you begin

00:03:59,970 --> 00:04:05,700
to see the performance degrades a little

00:04:02,070 --> 00:04:10,410
bit because in the setup that I'm using

00:04:05,700 --> 00:04:12,360
now I've got I'm using all 16 cores on

00:04:10,410 --> 00:04:14,850
this one Numa node so I've got one of

00:04:12,360 --> 00:04:16,739
those that's sharing with the rest of

00:04:14,850 --> 00:04:18,870
the system processes and interrupts and

00:04:16,739 --> 00:04:21,180
and things like that so there's a little

00:04:18,870 --> 00:04:24,790
bit of degradation once I scaled out to

00:04:21,180 --> 00:04:28,090
8 cores but overall

00:04:24,790 --> 00:04:32,380
pretty consistent scaling the thing

00:04:28,090 --> 00:04:34,660
that's you know not great is that we see

00:04:32,380 --> 00:04:37,420
that the performance is like nineteen

00:04:34,660 --> 00:04:40,570
hundred cycles per packet which is

00:04:37,420 --> 00:04:42,430
pretty expensive but we're just trying

00:04:40,570 --> 00:04:45,760
to establish a baseline this is what we

00:04:42,430 --> 00:04:47,410
saw when we're we're forwarding and this

00:04:45,760 --> 00:04:49,630
is what the kernel data path right this

00:04:47,410 --> 00:04:52,240
is the kernel data path we did not do

00:04:49,630 --> 00:04:57,280
these tested we're not doing any kind of

00:04:52,240 --> 00:05:00,760
hardware offload simple this is the

00:04:57,280 --> 00:05:02,620
layer three forwarding so we're coming

00:05:00,760 --> 00:05:04,600
in we're using a traffic generator

00:05:02,620 --> 00:05:11,280
coming in on one port going out on the

00:05:04,600 --> 00:05:11,280
other port just very simple test yes

00:05:11,760 --> 00:05:17,170
yeah good question the question was

00:05:15,220 --> 00:05:19,510
about hyperthreading hyperthre etting

00:05:17,170 --> 00:05:21,220
was disabled in the setup so we didn't

00:05:19,510 --> 00:05:31,230
did not do any kind of hyper threading

00:05:21,220 --> 00:05:36,490
for these tests the number of flows in

00:05:31,230 --> 00:05:40,390
this case was this is a single flow we

00:05:36,490 --> 00:05:43,900
did other tests I mean I put a flow per

00:05:40,390 --> 00:05:45,940
core right right actually let me take

00:05:43,900 --> 00:05:48,000
that back it was not a single flow

00:05:45,940 --> 00:05:50,800
because that would not scale with our SS

00:05:48,000 --> 00:05:53,170
we had multiple flows we're generating

00:05:50,800 --> 00:05:55,480
different l3 streams so that we're

00:05:53,170 --> 00:05:56,800
distributing the traffic using RSS so

00:05:55,480 --> 00:06:00,390
really the way that we're controlling

00:05:56,800 --> 00:06:03,460
the number of cores in these tests is by

00:06:00,390 --> 00:06:05,680
setting the number of rings on the neck

00:06:03,460 --> 00:06:09,160
and then setting the affinity for those

00:06:05,680 --> 00:06:11,590
rings for given cores and then the

00:06:09,160 --> 00:06:17,920
traffic is distributed with RSS to

00:06:11,590 --> 00:06:20,410
different cores so this next test we

00:06:17,920 --> 00:06:23,080
wanted to see how does the performance

00:06:20,410 --> 00:06:26,170
change as we increase the packet size so

00:06:23,080 --> 00:06:29,110
we went from the I next size of 358 and

00:06:26,170 --> 00:06:32,230
then a few different steps up to MTU

00:06:29,110 --> 00:06:36,430
size and again we're seeing pretty

00:06:32,230 --> 00:06:37,870
consistent performance in this range so

00:06:36,430 --> 00:06:44,110
we felt like this was a pretty good

00:06:37,870 --> 00:06:45,550
baseline for our additional tests all

00:06:44,110 --> 00:06:50,680
right you know and now we get to the fun

00:06:45,550 --> 00:06:54,640
right or you all right so the next test

00:06:50,680 --> 00:06:56,410
we wanted to see what happens because

00:06:54,640 --> 00:06:58,660
you have their different performance

00:06:56,410 --> 00:07:00,340
characteristics when you hit a flow

00:06:58,660 --> 00:07:03,540
that's already programmed in the data

00:07:00,340 --> 00:07:08,950
path versus a new flow that arrives and

00:07:03,540 --> 00:07:15,040
so what we did was we offered do new

00:07:08,950 --> 00:07:17,230
traffic with short duration streams so

00:07:15,040 --> 00:07:20,770
that over time you end up with an

00:07:17,230 --> 00:07:23,110
average number of a consistent average

00:07:20,770 --> 00:07:24,850
number of flows in the system but the

00:07:23,110 --> 00:07:28,540
key thing that we wanted to characterize

00:07:24,850 --> 00:07:31,240
here is what kind of forwarding rate can

00:07:28,540 --> 00:07:34,330
you get when you hit in the data path

00:07:31,240 --> 00:07:37,240
versus when you miss and you have to do

00:07:34,330 --> 00:07:39,640
an up call to user space to add the flow

00:07:37,240 --> 00:07:42,220
and then push the packet re-inject the

00:07:39,640 --> 00:07:45,220
packet into the data path via the miss

00:07:42,220 --> 00:07:49,570
path and so pretty dramatic difference

00:07:45,220 --> 00:07:51,940
here there's a 13 times more expensive

00:07:49,570 --> 00:07:55,660
to do that miss processing than to do

00:07:51,940 --> 00:08:01,330
the hit so that's a pretty dramatic

00:07:55,660 --> 00:08:03,430
impact that we saw so the next step was

00:08:01,330 --> 00:08:07,270
really this is just this is a projection

00:08:03,430 --> 00:08:11,740
of from the maximum rate per core that

00:08:07,270 --> 00:08:15,070
you can achieve down to the if you had

00:08:11,740 --> 00:08:17,230
100% miss rate what kind of performance

00:08:15,070 --> 00:08:19,210
would you get and this shows the

00:08:17,230 --> 00:08:22,840
dramatic drop-off that you get when

00:08:19,210 --> 00:08:24,730
you're you're doing more as the number

00:08:22,840 --> 00:08:27,310
of misses as a percent of your traffic

00:08:24,730 --> 00:08:30,850
increases so kind of wanted to

00:08:27,310 --> 00:08:33,160
understand that as you have more and

00:08:30,850 --> 00:08:37,410
more turn in your flows what is the

00:08:33,160 --> 00:08:37,410
impact on your overall system behavior

00:08:38,700 --> 00:08:43,150
all right so we thought you know that

00:08:40,900 --> 00:08:44,890
one of the key roles for these problems

00:08:43,150 --> 00:08:47,050
the panacea for this is what about

00:08:44,890 --> 00:08:47,540
hardware acceleration what can we do if

00:08:47,050 --> 00:08:50,060
the

00:08:47,540 --> 00:08:52,670
path is actually offloaded to hardware

00:08:50,060 --> 00:08:54,230
and so in that in that situation maximum

00:08:52,670 --> 00:08:55,720
Hardware rate would be 90 million

00:08:54,230 --> 00:08:59,870
packets per second in this particular

00:08:55,720 --> 00:09:01,519
device and if we have eight cores

00:08:59,870 --> 00:09:03,139
dedicated to processing the missus what

00:09:01,519 --> 00:09:06,500
does that look like and surprise

00:09:03,139 --> 00:09:08,690
surprise it's still pretty bad so

00:09:06,500 --> 00:09:11,060
there's it doesn't matter how great your

00:09:08,690 --> 00:09:13,759
hardware offload is if you start missing

00:09:11,060 --> 00:09:15,050
the cost is basically the same as if

00:09:13,759 --> 00:09:18,889
you're missing from the kernel data path

00:09:15,050 --> 00:09:21,380
so again new traffic that's being set up

00:09:18,889 --> 00:09:23,750
I mean you can you can see right here it

00:09:21,380 --> 00:09:25,069
just takes a huge huge hit and what

00:09:23,750 --> 00:09:27,019
really is the most important part about

00:09:25,069 --> 00:09:30,160
all this is that your hit rate is what

00:09:27,019 --> 00:09:32,449
is going to dominate your throughput so

00:09:30,160 --> 00:09:36,430
yeah the other thing I want to point out

00:09:32,449 --> 00:09:40,040
is in order to see a difference between

00:09:36,430 --> 00:09:42,139
so what we have on this chart is really

00:09:40,040 --> 00:09:46,310
two different lines one is if you have

00:09:42,139 --> 00:09:49,819
one a single core using today's today's

00:09:46,310 --> 00:09:54,620
control plane to add those flows when

00:09:49,819 --> 00:09:57,410
you have a Miss versus a theoretical 10x

00:09:54,620 --> 00:10:00,170
improvement in that control plane

00:09:57,410 --> 00:10:03,319
processing and distributing that to

00:10:00,170 --> 00:10:05,149
eight core so really in the control in

00:10:03,319 --> 00:10:09,410
the control plane in this graph that's

00:10:05,149 --> 00:10:13,040
the the orange line shows really an ad X

00:10:09,410 --> 00:10:16,100
improvement in that control plane and

00:10:13,040 --> 00:10:19,190
yet it barely moves that line so that's

00:10:16,100 --> 00:10:22,730
a pretty important point that your

00:10:19,190 --> 00:10:24,860
performance drops off and I mean if you

00:10:22,730 --> 00:10:26,360
were doing all misses then all you can

00:10:24,860 --> 00:10:27,170
do is what the control thing can do

00:10:26,360 --> 00:10:29,269
that's right

00:10:27,170 --> 00:10:31,040
so it's this was this line was a little

00:10:29,269 --> 00:10:32,029
bit shocking to us when we first we're

00:10:31,040 --> 00:10:33,680
starting to started thinking about it

00:10:32,029 --> 00:10:35,569
like what if we really what if we just

00:10:33,680 --> 00:10:37,250
made it a goal we'll make sure that the

00:10:35,569 --> 00:10:39,380
user space lookups are faster all that

00:10:37,250 --> 00:10:41,810
process is cleaned up what would happen

00:10:39,380 --> 00:10:42,500
and it basically didn't move didn't move

00:10:41,810 --> 00:10:49,779
the line at all

00:10:42,500 --> 00:10:52,339
so all right okay so this next slide is

00:10:49,779 --> 00:10:55,279
takes a look at well if we run this on

00:10:52,339 --> 00:10:58,760
DP DK how does that compare so in this

00:10:55,279 --> 00:11:01,180
case you know a DP DK it's it's

00:10:58,760 --> 00:11:06,260
basically the same kind of proof

00:11:01,180 --> 00:11:08,690
DPD k we measured about 25 little almost

00:11:06,260 --> 00:11:11,420
26 million packets per second when

00:11:08,690 --> 00:11:13,160
you're hitting in the data path but you

00:11:11,420 --> 00:11:15,680
have the same cost there I mean a

00:11:13,160 --> 00:11:18,830
control plane this is expensive this is

00:11:15,680 --> 00:11:21,170
a the bottom line you're a little bit

00:11:18,830 --> 00:11:22,310
better end in D PDK because you don't

00:11:21,170 --> 00:11:24,350
have to

00:11:22,310 --> 00:11:27,740
you're not making transitions between

00:11:24,350 --> 00:11:30,470
user space and and the kernel path to

00:11:27,740 --> 00:11:32,930
add the flows but not much I mean at

00:11:30,470 --> 00:11:37,580
this scale you end up essentially at the

00:11:32,930 --> 00:11:40,100
same place so that's that's kind of what

00:11:37,580 --> 00:11:41,630
that would show so we started thinking

00:11:40,100 --> 00:11:44,000
about this in the context of Amdahl's

00:11:41,630 --> 00:11:45,620
law so for those that may or may not

00:11:44,000 --> 00:11:48,620
have heard of this all bore you with the

00:11:45,620 --> 00:11:50,720
brief explanation of it so it's a way to

00:11:48,620 --> 00:11:52,160
calculate it's an old rule it's an old

00:11:50,720 --> 00:11:53,960
sort of law that's been created it's a

00:11:52,160 --> 00:11:55,820
way to calculate the theoretical speed

00:11:53,960 --> 00:11:59,930
up that you get from improving a part of

00:11:55,820 --> 00:12:02,420
a task so you can see these these

00:11:59,930 --> 00:12:03,860
particular things here the idea here is

00:12:02,420 --> 00:12:06,020
that the speed up of the task is

00:12:03,860 --> 00:12:08,180
improved with lowercase s and P is the

00:12:06,020 --> 00:12:09,980
proportion of that of that execution

00:12:08,180 --> 00:12:12,980
time that's improved so this is a way

00:12:09,980 --> 00:12:16,040
for us to sort of quantify our our 10x

00:12:12,980 --> 00:12:16,970
improvement if you will of of that how

00:12:16,040 --> 00:12:19,750
much is that really going to make a

00:12:16,970 --> 00:12:22,880
difference overall so if we apply this

00:12:19,750 --> 00:12:26,120
10x speed up and if we decided that in

00:12:22,880 --> 00:12:29,510
this particular case at 85% hit rate

00:12:26,120 --> 00:12:31,910
would mean that we spend 75% of our time

00:12:29,510 --> 00:12:35,120
processing misses and other time

00:12:31,910 --> 00:12:37,370
processing misses or hits and misses

00:12:35,120 --> 00:12:38,360
you do the math which hopefully is

00:12:37,370 --> 00:12:42,530
fairly readable

00:12:38,360 --> 00:12:46,760
this only results in like a three times

00:12:42,530 --> 00:12:49,040
speed up so again this miss processing

00:12:46,760 --> 00:12:52,400
really just to prove the point that it

00:12:49,040 --> 00:12:56,690
is so expensive to do this did that

00:12:52,400 --> 00:12:58,040
that's that's not not amazing so here's

00:12:56,690 --> 00:13:01,250
kind of where the conclusion that we

00:12:58,040 --> 00:13:02,720
sort of get to so if we have this

00:13:01,250 --> 00:13:04,670
compute server where we previously

00:13:02,720 --> 00:13:08,060
talked about four cores that were

00:13:04,670 --> 00:13:11,120
burning for obs the not allowed to

00:13:08,060 --> 00:13:12,920
actually use to run applications or used

00:13:11,120 --> 00:13:15,260
to rent out to whoever is going

00:13:12,920 --> 00:13:19,040
vm's from us not sure who would rent

00:13:15,260 --> 00:13:22,310
vm's for me but anyway OBS is in this

00:13:19,040 --> 00:13:24,380
case consumes four course so we of

00:13:22,310 --> 00:13:26,000
course like many people have a smart

00:13:24,380 --> 00:13:28,160
Nick that's got the capability to move

00:13:26,000 --> 00:13:29,540
load down there so you could save those

00:13:28,160 --> 00:13:30,560
four course by moving them down the

00:13:29,540 --> 00:13:32,660
stingray

00:13:30,560 --> 00:13:33,860
in this case we have eight core so we

00:13:32,660 --> 00:13:39,350
could just go ahead and burn all this

00:13:33,860 --> 00:13:42,949
course for this and this reason up the

00:13:39,350 --> 00:13:44,180
course so the other the other kind of

00:13:42,949 --> 00:13:46,339
key piece to think about

00:13:44,180 --> 00:13:48,500
it's also possible in this scenario is

00:13:46,339 --> 00:13:50,600
not only could we again move the four

00:13:48,500 --> 00:13:52,430
course down there if we really wanted to

00:13:50,600 --> 00:13:54,709
we could also then Hardware offload

00:13:52,430 --> 00:13:57,139
those we sort of shown that the hardware

00:13:54,709 --> 00:13:58,760
offload case may or may not be super

00:13:57,139 --> 00:14:00,410
helpful if you're processing a lot of

00:13:58,760 --> 00:14:02,930
missed packets you're processing a lot

00:14:00,410 --> 00:14:04,940
of packets that are previously learned

00:14:02,930 --> 00:14:06,920
you're in good shape but this is sort of

00:14:04,940 --> 00:14:08,990
the this is the the Holy Grail the

00:14:06,920 --> 00:14:10,850
Utopia that everybody wants is you know

00:14:08,990 --> 00:14:12,649
we're not doing magically not doing any

00:14:10,850 --> 00:14:15,800
work in the CPUs anymore we've offloaded

00:14:12,649 --> 00:14:18,050
everything to hardware so this is sort

00:14:15,800 --> 00:14:22,160
of the the premise here that we'd like

00:14:18,050 --> 00:14:25,010
to do right so the the the thing that

00:14:22,160 --> 00:14:27,490
really I wanted to bring out here is and

00:14:25,010 --> 00:14:31,100
II was just describing it if we take

00:14:27,490 --> 00:14:33,769
take a look at this load and different

00:14:31,100 --> 00:14:37,190
ways that you could run OBS on your

00:14:33,769 --> 00:14:39,350
server so this this chart shows four

00:14:37,190 --> 00:14:42,980
different profiles one is you're running

00:14:39,350 --> 00:14:45,260
the kernel data path on four cores that

00:14:42,980 --> 00:14:48,589
are dedicated for that server in that

00:14:45,260 --> 00:14:52,130
case you're getting 5.4 million packets

00:14:48,589 --> 00:14:54,560
per second let's say you decided to

00:14:52,130 --> 00:14:57,110
change that load or change the

00:14:54,560 --> 00:14:59,899
distribution and I want to run DP DK on

00:14:57,110 --> 00:15:02,959
that that gets you you're you're

00:14:59,899 --> 00:15:05,870
consuming those four cores doing the DPD

00:15:02,959 --> 00:15:09,350
OBS DP DK data path that gets you to

00:15:05,870 --> 00:15:11,829
almost 226 million packets per second as

00:15:09,350 --> 00:15:15,529
Andy was saying you could take that load

00:15:11,829 --> 00:15:19,339
run it on a stingray dedicate all eight

00:15:15,529 --> 00:15:22,010
cores to that and do the OVS DP D K data

00:15:19,339 --> 00:15:25,420
path on the Stingray and now that takes

00:15:22,010 --> 00:15:27,910
you to 28 million packets per second

00:15:25,420 --> 00:15:32,140
and then finally if you're running it

00:15:27,910 --> 00:15:34,330
again and the key things is zero zero

00:15:32,140 --> 00:15:39,250
server cores so now those cores are

00:15:34,330 --> 00:15:44,830
freed up to run your your customer

00:15:39,250 --> 00:15:47,650
applications so in both of the two green

00:15:44,830 --> 00:15:49,840
lines on the right you use that you're

00:15:47,650 --> 00:15:54,160
consuming zero server course for the

00:15:49,840 --> 00:15:56,620
networking load and now you've got

00:15:54,160 --> 00:15:59,320
and finally 90 million packets per

00:15:56,620 --> 00:16:01,300
second is using the true flow offload on

00:15:59,320 --> 00:16:05,020
the Stingray so you've got hardware

00:16:01,300 --> 00:16:07,090
offloaded data path and your you're

00:16:05,020 --> 00:16:09,970
fooling fully taking advantage of the

00:16:07,090 --> 00:16:11,650
capabilities of the smart neck so I see

00:16:09,970 --> 00:16:13,330
tom has a question so I'm a little

00:16:11,650 --> 00:16:15,220
confused because we just spent a lot of

00:16:13,330 --> 00:16:17,650
time saying the control path was the

00:16:15,220 --> 00:16:19,690
bottleneck and this is saying we're all

00:16:17,650 --> 00:16:28,570
floating the data path is this does this

00:16:19,690 --> 00:16:30,130
include the misses on OVS to the miss

00:16:28,570 --> 00:16:33,490
case because it sounded like the miss

00:16:30,130 --> 00:16:35,500
case was the problem right this case is

00:16:33,490 --> 00:16:38,970
a problem and it's the same problem

00:16:35,500 --> 00:16:42,130
wherever you run it it's it's gonna be

00:16:38,970 --> 00:16:44,050
that's a cost of doing business that you

00:16:42,130 --> 00:16:46,090
can't fix based on where you place the

00:16:44,050 --> 00:16:48,160
control plane unless you dramatic oh

00:16:46,090 --> 00:16:50,170
yeah okay so let me just want one

00:16:48,160 --> 00:16:57,730
comment so are we done talking about the

00:16:50,170 --> 00:16:59,700
control plane then today are you I'm

00:16:57,730 --> 00:17:02,440
interested in it where you described it

00:16:59,700 --> 00:17:04,450
so Tom can I would we ask you a question

00:17:02,440 --> 00:17:07,300
as go back to that chart your last slide

00:17:04,450 --> 00:17:10,060
what is the definitive in okay somebody

00:17:07,300 --> 00:17:11,260
get the chart up there what's the

00:17:10,060 --> 00:17:12,850
difference between the third and the

00:17:11,260 --> 00:17:16,360
fourth column I don't understand the

00:17:12,850 --> 00:17:19,870
difference the third the third column is

00:17:16,360 --> 00:17:27,340
running the OBS DP DP DP decayed data

00:17:19,870 --> 00:17:31,240
plane on the the smart neck on the

00:17:27,340 --> 00:17:33,790
server if you run OVS DP d k then DP DK

00:17:31,240 --> 00:17:36,190
implements your your forwarding plane

00:17:33,790 --> 00:17:38,720
right that's great that's you can do the

00:17:36,190 --> 00:17:42,440
same thing on the arm core

00:17:38,720 --> 00:17:45,350
on the on the SmartNet okay so okay fine

00:17:42,440 --> 00:17:49,220
that's a lie but okay you're just

00:17:45,350 --> 00:17:52,399
getting off the course that's right

00:17:49,220 --> 00:17:54,289
right yeah okay okay let me ask the

00:17:52,399 --> 00:17:56,690
obvious question then I run the control

00:17:54,289 --> 00:17:57,799
plane on the arm on the neck yes

00:17:56,690 --> 00:18:00,230
absolutely

00:17:57,799 --> 00:18:03,909
okay absolutely you have the projections

00:18:00,230 --> 00:18:06,080
for that not in this deck

00:18:03,909 --> 00:18:07,369
what does that solve though I mean

00:18:06,080 --> 00:18:10,159
you're still gonna have those me if you

00:18:07,369 --> 00:18:12,350
have misses does that help just because

00:18:10,159 --> 00:18:13,940
you're riding on the neck yeah because

00:18:12,350 --> 00:18:15,649
the misses don't hit the whole point of

00:18:13,940 --> 00:18:17,419
this so and maybe we didn't do a good

00:18:15,649 --> 00:18:19,879
job making that clear so the point of

00:18:17,419 --> 00:18:21,859
this is that all of your OBS work is off

00:18:19,879 --> 00:18:24,259
the zeon course in this case or your arm

00:18:21,859 --> 00:18:26,119
server course whichever flavor of server

00:18:24,259 --> 00:18:30,169
core you like to use so we're not

00:18:26,119 --> 00:18:35,840
running any open V switch at all on the

00:18:30,169 --> 00:18:37,519
server so even if you but isn't it's

00:18:35,840 --> 00:18:40,359
when it misses that's the problem it

00:18:37,519 --> 00:18:43,009
goes way to use a space or because well

00:18:40,359 --> 00:18:45,859
yes it goes the user space on my arm

00:18:43,009 --> 00:18:48,980
course on the amp cause or the NIC yes

00:18:45,859 --> 00:18:50,149
and what I don't know what the savings

00:18:48,980 --> 00:18:53,359
are because you're still gonna have a

00:18:50,149 --> 00:18:56,210
still gonna go to user space right it's

00:18:53,359 --> 00:18:59,779
just isn't the bottleneck right Johan

00:18:56,210 --> 00:19:02,059
let's just what can you run applications

00:18:59,779 --> 00:19:04,279
on answer right you can now run more

00:19:02,059 --> 00:19:06,889
apps on the Z that's right yes yes I

00:19:04,279 --> 00:19:08,419
understand the freeing path but it seems

00:19:06,889 --> 00:19:09,859
there's this fundamental problem proper

00:19:08,419 --> 00:19:12,460
I don't want to sound biased

00:19:09,859 --> 00:19:15,169
maybe in the obvious architecture itself

00:19:12,460 --> 00:19:16,700
well that was the other question this is

00:19:15,169 --> 00:19:19,519
an l-3 forwarding thing why are you

00:19:16,700 --> 00:19:20,899
using obvious the if you use the routing

00:19:19,519 --> 00:19:23,269
path you would not have this problem you

00:19:20,899 --> 00:19:25,249
would download out LPM would be loaded

00:19:23,269 --> 00:19:28,100
you would this whole problem goes away

00:19:25,249 --> 00:19:30,619
so you know there's a much simpler it is

00:19:28,100 --> 00:19:32,570
a particular use case that we chose to

00:19:30,619 --> 00:19:35,419
study based on there are a lot of

00:19:32,570 --> 00:19:37,639
customers that are using this so you

00:19:35,419 --> 00:19:39,320
could you could characterize it using

00:19:37,639 --> 00:19:42,289
different data planes we were looking at

00:19:39,320 --> 00:19:45,679
what you can do and how you can offload

00:19:42,289 --> 00:19:47,509
this work to a smart NIC and how that

00:19:45,679 --> 00:19:49,549
impacts the system yeah so the one

00:19:47,509 --> 00:19:50,230
comment I would have is big something

00:19:49,549 --> 00:19:52,750
other than

00:19:50,230 --> 00:19:56,460
forwarding this is almost the worst the

00:19:52,750 --> 00:19:56,460
correct answer here is don't use obvious

00:19:56,880 --> 00:20:00,840
wait wait can you repeat that

00:20:02,010 --> 00:20:07,330
sorry I don't do that it's when you

00:20:05,410 --> 00:20:09,580
couldn't figure obvious to do routing or

00:20:07,330 --> 00:20:12,190
to do something obvious use mega flows

00:20:09,580 --> 00:20:14,040
and then it's not doing it

00:20:12,190 --> 00:20:18,130
not every pocket is going to stop up

00:20:14,040 --> 00:20:21,790
it's only the first pocket and that's

00:20:18,130 --> 00:20:27,309
all this is the way oh she's using mega

00:20:21,790 --> 00:20:29,410
flows right so so this is abuse of you

00:20:27,309 --> 00:20:31,870
know you're trying to figure to say that

00:20:29,410 --> 00:20:34,540
everything is going to the user space

00:20:31,870 --> 00:20:37,799
but it's not the case if the quality

00:20:34,540 --> 00:20:40,270
that you specified was I'm using flows

00:20:37,799 --> 00:20:41,980
but you don't and you're actually using

00:20:40,270 --> 00:20:44,320
flow because as you mentioned the flows

00:20:41,980 --> 00:20:46,270
pocket has come for a single port and go

00:20:44,320 --> 00:20:49,390
out to the second floor this is the flow

00:20:46,270 --> 00:20:51,700
on this wall this is a one mega flow in

00:20:49,390 --> 00:20:53,830
this one mega flow you can run millions

00:20:51,700 --> 00:20:56,260
of connection but it is still one room

00:20:53,830 --> 00:20:59,650
that's right that's a different test

00:20:56,260 --> 00:21:01,900
case the point is in that the ability to

00:20:59,650 --> 00:21:04,390
take advantage of mega flows depends

00:21:01,900 --> 00:21:06,070
entirely on your traffic pattern if you

00:21:04,390 --> 00:21:07,900
have a traffic pattern that doesn't

00:21:06,070 --> 00:21:09,970
happen to hit the mega flow that you've

00:21:07,900 --> 00:21:13,410
added then that doesn't solve anything

00:21:09,970 --> 00:21:16,690
for you if it's not if it's if you've

00:21:13,410 --> 00:21:21,130
defined l3 rules and your your traffic

00:21:16,690 --> 00:21:30,910
doesn't match on on your particular mega

00:21:21,130 --> 00:21:33,850
flow that doesn't help okay so yeah I I

00:21:30,910 --> 00:21:38,080
agree to a certain extent but on slide

00:21:33,850 --> 00:21:40,809
12 you put that probability at 75% right

00:21:38,080 --> 00:21:47,049
the the probability of hitting I guess

00:21:40,809 --> 00:21:49,630
so 25% is missing so I mean you you can

00:21:47,049 --> 00:21:52,240
tune your OVS for your traffic pattern

00:21:49,630 --> 00:21:56,470
and you can minimize that absolutely and

00:21:52,240 --> 00:21:58,480
as a matter of fact we Andy sent an

00:21:56,470 --> 00:22:01,330
email because we wanted to talk to the

00:21:58,480 --> 00:22:03,260
OBS main Tanner's about this I mean our

00:22:01,330 --> 00:22:09,650
goal is is not to

00:22:03,260 --> 00:22:13,340
whine over yes by I think it's you can

00:22:09,650 --> 00:22:14,720
tune it but isn't the issue here where

00:22:13,340 --> 00:22:17,540
you're running I guess if you're if you

00:22:14,720 --> 00:22:19,010
said it could be a DOS attack right so

00:22:17,540 --> 00:22:20,540
this is this is always a problem

00:22:19,010 --> 00:22:22,640
throughout cache on Linux before it was

00:22:20,540 --> 00:22:25,040
removed that all these porn sites

00:22:22,640 --> 00:22:27,860
attacking each other and making sure

00:22:25,040 --> 00:22:29,360
that everything is a Miss that could

00:22:27,860 --> 00:22:32,540
happen when there was a rather

00:22:29,360 --> 00:22:35,120
interesting talk given at the open V

00:22:32,540 --> 00:22:37,430
switch conference late last year that

00:22:35,120 --> 00:22:38,780
talked about the actual extremely small

00:22:37,430 --> 00:22:43,010
number of packets that you need to send

00:22:38,780 --> 00:22:44,720
to what will be interesting is if you if

00:22:43,010 --> 00:22:46,880
if there's a study or a that it's been

00:22:44,720 --> 00:22:49,370
done everywhere that says you know on

00:22:46,880 --> 00:22:51,260
average you have ten percent miss but

00:22:49,370 --> 00:22:53,510
you can't protect against DDoS attacks

00:22:51,260 --> 00:22:56,090
and you can't and with a lot of that

00:22:53,510 --> 00:22:58,880
data is stuck it's stuck at the

00:22:56,090 --> 00:23:01,790
operators not necessarily something that

00:22:58,880 --> 00:23:04,100
everyone can't share or will share but I

00:23:01,790 --> 00:23:07,310
think the important thing to understand

00:23:04,100 --> 00:23:10,610
is control plain programming is very

00:23:07,310 --> 00:23:13,280
expensive and what we tried to do here

00:23:10,610 --> 00:23:16,250
was characterize what that expense is

00:23:13,280 --> 00:23:22,250
for a particular use case I think that's

00:23:16,250 --> 00:23:24,290
spam yep okay so maybe just receipts

00:23:22,250 --> 00:23:27,230
point you know friends don't let friends

00:23:24,290 --> 00:23:28,160
run OBS on application servers so this

00:23:27,230 --> 00:23:30,050
is the part where you're supposed to

00:23:28,160 --> 00:23:31,520
laugh in case you weren't curious in

00:23:30,050 --> 00:23:33,320
case you didn't know okay I I think I

00:23:31,520 --> 00:23:36,770
think that that conclusion is very valid

00:23:33,320 --> 00:23:39,950
the miss spot handling is very important

00:23:36,770 --> 00:23:41,810
my point that to be precise was don't

00:23:39,950 --> 00:23:43,520
you l-3 forwarding probably was the

00:23:41,810 --> 00:23:45,710
worst example you could have used well

00:23:43,520 --> 00:23:47,240
it is it isn't it isn't I mean it's the

00:23:45,710 --> 00:23:48,710
minimum number of the l3 forwarding case

00:23:47,240 --> 00:23:50,390
I'm pretty sure in the OVS data path

00:23:48,710 --> 00:23:53,030
uses the minimum number of instructions

00:23:50,390 --> 00:23:56,000
and our point was not to explicitly call

00:23:53,030 --> 00:23:58,280
out the because it doesn't do cloning it

00:23:56,000 --> 00:23:59,930
doesn't it doesn't deal with having

00:23:58,280 --> 00:24:01,520
flows that are you know currently

00:23:59,930 --> 00:24:03,410
unlearned and then rewriting it were

00:24:01,520 --> 00:24:05,450
interesting to see if that compares with

00:24:03,410 --> 00:24:06,860
the kernels l3 LPM look

00:24:05,450 --> 00:24:09,110
I think they look up right now it's

00:24:06,860 --> 00:24:11,539
going to be it's probably way better yes

00:24:09,110 --> 00:24:13,490
yeah I think as I said 30 40 I'll

00:24:11,539 --> 00:24:16,580
probably was the world was the wrong or

00:24:13,490 --> 00:24:19,399
not wrong was an example that left

00:24:16,580 --> 00:24:21,350
itself open to criticism oh yeah but we

00:24:19,399 --> 00:24:23,240
expected no different but but I think

00:24:21,350 --> 00:24:24,860
the meta point is absolutely valid right

00:24:23,240 --> 00:24:27,679
and I think it makes sense and to the

00:24:24,860 --> 00:24:29,600
point we are conversations over lunch I

00:24:27,679 --> 00:24:30,950
think this this is the stuff that needs

00:24:29,600 --> 00:24:33,620
to have a slightly more standardized

00:24:30,950 --> 00:24:36,950
answer I think Ron OVS unarmed core is

00:24:33,620 --> 00:24:38,389
an interesting strategy it's probably

00:24:36,950 --> 00:24:39,830
not practical because the people who are

00:24:38,389 --> 00:24:41,840
running obvious on the host now to

00:24:39,830 --> 00:24:44,419
figure out how to get that obvious

00:24:41,840 --> 00:24:51,919
installed and jammed into the handle

00:24:44,419 --> 00:24:53,120
that runs now but yeah but I think the

00:24:51,919 --> 00:24:55,639
conclusion is correct I think the

00:24:53,120 --> 00:24:58,220
overhead is real and and and it's now

00:24:55,639 --> 00:25:01,309
beginning to become significant portion

00:24:58,220 --> 00:25:03,740
of the compute as the rates are going up

00:25:01,309 --> 00:25:05,389
yeah I think one of the striking things

00:25:03,740 --> 00:25:07,490
to me was you know attending an OBS

00:25:05,389 --> 00:25:09,289
conference and realizing what sort of

00:25:07,490 --> 00:25:11,899
what sort of data rates we talked about

00:25:09,289 --> 00:25:13,460
here and what people see and what people

00:25:11,899 --> 00:25:17,000
expect to see as high data rates and

00:25:13,460 --> 00:25:19,700
it's a drastically different level of

00:25:17,000 --> 00:25:22,639
expectation and I think it's I mean it's

00:25:19,700 --> 00:25:24,169
extremely functional it works for a lot

00:25:22,639 --> 00:25:26,750
of use cases obviously otherwise people

00:25:24,169 --> 00:25:28,250
wouldn't use it but it's it's just a

00:25:26,750 --> 00:25:30,529
different there's a different cost

00:25:28,250 --> 00:25:32,929
associated with it then we then we see I

00:25:30,529 --> 00:25:34,789
mean I think if yes per were here he

00:25:32,929 --> 00:25:36,380
would stand up and tell me exactly how

00:25:34,789 --> 00:25:42,230
many cycles that currently takes on his

00:25:36,380 --> 00:25:44,000
system at home to do xdp redirect so so

00:25:42,230 --> 00:25:47,450
yeah we just want to sort of highlight

00:25:44,000 --> 00:25:50,840
that that cost and look at some of the

00:25:47,450 --> 00:25:53,659
options are available I think that's it

00:25:50,840 --> 00:25:56,929
yeah thank you any more questions

00:25:53,659 --> 00:25:58,549
so so okay I guess you guys don't work

00:25:56,929 --> 00:26:03,880
whenever yes is anybody works on obvious

00:25:58,549 --> 00:26:06,590
yeah yeah so it's not true that we don't

00:26:03,880 --> 00:26:08,690
know as anybody thought okay look in the

00:26:06,590 --> 00:26:10,820
Linux kernel it's considered a really

00:26:08,690 --> 00:26:13,460
bad idea to start caching at different

00:26:10,820 --> 00:26:15,840
layers right because you're said yes

00:26:13,460 --> 00:26:17,700
opening yourself up to a DOS attack

00:26:15,840 --> 00:26:20,210
right if you have your own private land

00:26:17,700 --> 00:26:23,850
that's great it works well there right

00:26:20,210 --> 00:26:26,100
but route cache was you know a great

00:26:23,850 --> 00:26:27,870
experiment in that space there are the

00:26:26,100 --> 00:26:31,860
subsystems they kind of like ARP or

00:26:27,870 --> 00:26:33,360
IPSec I think with which will cache

00:26:31,860 --> 00:26:35,570
maybe about three packets and start

00:26:33,360 --> 00:26:38,159
dropping after after some threshold

00:26:35,570 --> 00:26:40,320
because you know I have seen I've

00:26:38,159 --> 00:26:42,960
already requested for this slow path I'm

00:26:40,320 --> 00:26:44,669
not gonna start hammering user space

00:26:42,960 --> 00:26:47,520
again right so you have Ikey listening

00:26:44,669 --> 00:26:50,159
on user space and it was it is asked to

00:26:47,520 --> 00:26:57,510
resolve a role that model ever been

00:26:50,159 --> 00:27:00,480
thought of there so yeah in my opinion I

00:26:57,510 --> 00:27:04,279
I agree that it obvious wasn't designed

00:27:00,480 --> 00:27:07,830
for for this particular case right yes

00:27:04,279 --> 00:27:10,799
it does cache it so it has an open flow

00:27:07,830 --> 00:27:12,899
pipeline in user space and if you're

00:27:10,799 --> 00:27:15,720
gonna mess in the cache in the kernel

00:27:12,899 --> 00:27:19,020
you need to go via net link from kernel

00:27:15,720 --> 00:27:20,940
to user space through that table then

00:27:19,020 --> 00:27:22,409
get that cash so that it yeah obviously

00:27:20,940 --> 00:27:27,149
that that's kind of sucked performance

00:27:22,409 --> 00:27:29,940
it's gonna suck but my point and I do

00:27:27,149 --> 00:27:31,799
understand that and that is what I've

00:27:29,940 --> 00:27:34,320
been at that same same conference where

00:27:31,799 --> 00:27:35,970
that guy showed with very few packets

00:27:34,320 --> 00:27:39,120
and this this is a design flaw I agree

00:27:35,970 --> 00:27:42,049
with very few packets you can almost do

00:27:39,120 --> 00:27:47,960
s oh yes you can

00:27:42,049 --> 00:27:50,760
so no you know critique there obviously

00:27:47,960 --> 00:27:56,309
yeah and in my opinion is just not the

00:27:50,760 --> 00:27:58,649
the use case we've seen but yeah I think

00:27:56,309 --> 00:28:00,000
it's it's valid points you're you're

00:27:58,649 --> 00:28:04,919
you're basically what you're doing is

00:28:00,000 --> 00:28:06,929
you're moving that that expense down to

00:28:04,919 --> 00:28:08,309
your core that's what are you doing

00:28:06,929 --> 00:28:10,169
so that's why you're getting zero

00:28:08,309 --> 00:28:13,140
because my other question was gonna be

00:28:10,169 --> 00:28:16,320
how do you get zero zero core usage

00:28:13,140 --> 00:28:18,390
without updating stats and day one of

00:28:16,320 --> 00:28:22,230
this conference we had the whole static

00:28:18,390 --> 00:28:24,899
conversation and yeah I mean that that

00:28:22,230 --> 00:28:26,250
at least takes part of a core to do

00:28:24,899 --> 00:28:27,120
status update but yeah if you put that

00:28:26,250 --> 00:28:30,330
on the neck

00:28:27,120 --> 00:28:32,250
you you don't need yeah right right

00:28:30,330 --> 00:28:34,410
the other the other part that was a

00:28:32,250 --> 00:28:35,580
little bit shocking to me and I'll just

00:28:34,410 --> 00:28:37,290
share this with you because I thought it

00:28:35,580 --> 00:28:39,090
was really interesting stat is that at

00:28:37,290 --> 00:28:41,190
that same conference somebody commented

00:28:39,090 --> 00:28:45,690
on the the the annual revenue associated

00:28:41,190 --> 00:28:46,590
with a core and I've heard numbers go

00:28:45,690 --> 00:28:48,090
back and forth I've actually heard

00:28:46,590 --> 00:28:50,520
numbers bigger than this and smaller

00:28:48,090 --> 00:28:52,440
than this but but that was one of the

00:28:50,520 --> 00:28:57,630
things that floored me is that if you're

00:28:52,440 --> 00:28:59,730
if you're leasing out your system that

00:28:57,630 --> 00:29:01,620
on a per core basis you can maybe make

00:28:59,730 --> 00:29:03,390
as much as a thousand dollars us and

00:29:01,620 --> 00:29:05,309
revenue per year so when you start to

00:29:03,390 --> 00:29:07,140
think about and that we were actually

00:29:05,309 --> 00:29:08,910
both there and sort of looked at each

00:29:07,140 --> 00:29:13,200
other we're like I said is that legit

00:29:08,910 --> 00:29:15,270
are those people hiring yeah look I mean

00:29:13,200 --> 00:29:19,170
if somebody is making that good that I

00:29:15,270 --> 00:29:21,870
don't see you and it's it's I looked

00:29:19,170 --> 00:29:23,580
around a lot online and there's there

00:29:21,870 --> 00:29:25,380
seems to be multiple people not just

00:29:23,580 --> 00:29:27,300
this one person that are there are

00:29:25,380 --> 00:29:29,880
claiming numbers in that in that area so

00:29:27,300 --> 00:29:34,380
even if it's half that or a quarter of

00:29:29,880 --> 00:29:36,510
that it's it that's a massive amount and

00:29:34,380 --> 00:29:38,760
yeah I wonder like should I be

00:29:36,510 --> 00:29:42,570
maybe I should be hosting VMs I don't

00:29:38,760 --> 00:29:46,130
know if you look at AWS rates for

00:29:42,570 --> 00:29:50,370
example it's a VM you can get a VM for

00:29:46,130 --> 00:29:51,990
350 400 bucks for a year right so if I

00:29:50,370 --> 00:29:54,960
mean yes it's not a core right obviously

00:29:51,990 --> 00:29:56,520
I subdivide it but whatever so if you

00:29:54,960 --> 00:29:57,929
can yeah and now that's where you start

00:29:56,520 --> 00:30:05,190
to look at like what could three of them

00:29:57,929 --> 00:30:08,610
run at the same time so yeah if we had

00:30:05,190 --> 00:30:10,559
use the Linux kernel forwarding what

00:30:08,610 --> 00:30:13,590
would what would be the expectation of

00:30:10,559 --> 00:30:16,650
the CPU usage maybe CPU maybe two course

00:30:13,590 --> 00:30:19,470
read two course how many packets per

00:30:16,650 --> 00:30:21,030
second can we expect if we are deuce the

00:30:19,470 --> 00:30:26,670
Linux kernel forwarding instead of the

00:30:21,030 --> 00:30:28,230
various we didn't characterize that case

00:30:26,670 --> 00:30:29,760
I would imagine that they're probably a

00:30:28,230 --> 00:30:32,220
number of people in this room that could

00:30:29,760 --> 00:30:34,050
probably give good number I think I

00:30:32,220 --> 00:30:36,990
think it will be good because you're

00:30:34,050 --> 00:30:38,760
just doing layer 3 right you know I win

00:30:36,990 --> 00:30:41,270
a good test in my opinion do you III

00:30:38,760 --> 00:30:47,130
don't think we perform very well my

00:30:41,270 --> 00:30:52,440
but well they had a single router that

00:30:47,130 --> 00:30:55,460
is is it a single ruffed one rule no but

00:30:52,440 --> 00:30:58,500
they're all known flows Dell known right

00:30:55,460 --> 00:31:00,630
will probably do okay wait then I mean

00:30:58,500 --> 00:31:01,799
that's that's not the same I mean you

00:31:00,630 --> 00:31:02,970
can run it if you want but we know

00:31:01,799 --> 00:31:06,360
that's gonna destroy it

00:31:02,970 --> 00:31:10,080
I mean just like we would know that DP

00:31:06,360 --> 00:31:11,789
DK would destroy the kernel data path DB

00:31:10,080 --> 00:31:13,110
DK for the data path would destroy the

00:31:11,789 --> 00:31:15,149
kernel data path broken through switch

00:31:13,110 --> 00:31:19,409
so I mean if you want to do apples to

00:31:15,149 --> 00:31:20,909
apples and have the most the how would

00:31:19,409 --> 00:31:31,470
we say it most likes of the most

00:31:20,909 --> 00:31:33,750
flexible one used yeah you know and I

00:31:31,470 --> 00:31:35,130
mean we could actually we could run yeah

00:31:33,750 --> 00:31:36,750
we could run tests across the board we

00:31:35,130 --> 00:31:41,549
can do all that stuff on the smart neck

00:31:36,750 --> 00:31:43,520
so I think you could mention more this

00:31:41,549 --> 00:31:48,330
getting show this line again

00:31:43,520 --> 00:31:54,000
sure I can't so somebody yeah so I think

00:31:48,330 --> 00:31:55,830
the last the fourth the I wonder 90

00:31:54,000 --> 00:31:58,470
million bucks per second this is the

00:31:55,830 --> 00:32:01,649
important one because you're trying to

00:31:58,470 --> 00:32:04,260
sell your chorus instead of Intel course

00:32:01,649 --> 00:32:06,630
okay we'll say how much you how much

00:32:04,260 --> 00:32:10,140
cost your core how much cost into a core

00:32:06,630 --> 00:32:11,880
we can debate okay but I think the most

00:32:10,140 --> 00:32:14,789
important thing it's the last thing

00:32:11,880 --> 00:32:16,830
that's your choice because then you get

00:32:14,789 --> 00:32:20,480
much better performance this is

00:32:16,830 --> 00:32:28,830
something that you can't do with that

00:32:20,480 --> 00:32:32,549
intercourse right right so this is the

00:32:28,830 --> 00:32:36,029
important thing and this is the thing

00:32:32,549 --> 00:32:38,820
that's we're doing today of course as

00:32:36,029 --> 00:32:41,360
you do it with the kinetics arts and

00:32:38,820 --> 00:32:44,580
indeed this is the performance round of

00:32:41,360 --> 00:32:46,649
79 degrees of performance a pair by air

00:32:44,580 --> 00:32:50,120
pockets per second this is something

00:32:46,649 --> 00:32:56,190
that cannot be done today all right

00:32:50,120 --> 00:33:01,420
sorry we need a break so thank you guys

00:32:56,190 --> 00:33:01,420

YouTube URL: https://www.youtube.com/watch?v=5SjVA2jalb4


