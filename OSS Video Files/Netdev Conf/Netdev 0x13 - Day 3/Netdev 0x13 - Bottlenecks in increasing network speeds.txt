Title: Netdev 0x13 - Bottlenecks in increasing network speeds
Publication date: 2019-06-03
Playlist: Netdev 0x13 - Day 3
Description: 
	In this talk, Tariq Toukan reviews the main bottlenecks that are being addressed to support NIC speeds of 100Gbps, 200Gbps, and beyond.
Over the last few years, NIC speeds have gone up from 1G to 10G to 25G to 40G and now 100G and 200G and even 400G NICS are going to be showing up in the commodity market soon.

Every time these rates go up, new challenges come to the surface in the kernel. 
Tariq summarize several ideas and solutions ranging from traditional tricks of the trade like page recycling to innovative HW capabilities that helped to recently achieve milestone packet rate of 100 Million PPS in XDP_TX and 135Mpps in XDP_DROP.

More info:
https://netdevconf.org/0x13/session.html?talk-bottlenecks
Captions: 
	00:00:00,060 --> 00:00:07,859
and I have to go on from Mellanox and in

00:00:04,140 --> 00:00:12,830
this talk I'm gonna review with you

00:00:07,859 --> 00:00:16,529
first importance of bottleneck analysis

00:00:12,830 --> 00:00:23,250
especially with ongoing increase in NIC

00:00:16,529 --> 00:00:25,740
speed then we're gonna review two

00:00:23,250 --> 00:00:27,779
examples of bottlenecks and how we deal

00:00:25,740 --> 00:00:32,779
with this and what we've done and what

00:00:27,779 --> 00:00:32,779
is being done to overcome them

00:00:34,250 --> 00:00:43,710
yes so first on motivation then we'll

00:00:37,860 --> 00:00:52,430
review some PCI bottleneck and later

00:00:43,710 --> 00:01:00,210
what making CPU okay so Nick speeds are

00:00:52,430 --> 00:01:03,719
getting higher and higher next 100

00:01:00,210 --> 00:01:11,880
gigabit per second eggs are here for for

00:01:03,719 --> 00:01:12,360
a few years now 200 and it's not

00:01:11,880 --> 00:01:20,400
stopping

00:01:12,360 --> 00:01:25,380
next we have 400 and we deal with new

00:01:20,400 --> 00:01:29,759
challenges that this means that we will

00:01:25,380 --> 00:01:31,920
have higher packet rate which means that

00:01:29,759 --> 00:01:36,210
you have less time to spend processing

00:01:31,920 --> 00:01:40,590
every single packet the stack needs to

00:01:36,210 --> 00:01:47,180
get aligned with this new next bit if

00:01:40,590 --> 00:01:49,560
you want to keep posting these packets

00:01:47,180 --> 00:01:53,549
with the same number of course then you

00:01:49,560 --> 00:01:56,670
have a lower more time to spend if you

00:01:53,549 --> 00:02:05,210
want to spare more cores then you need

00:01:56,670 --> 00:02:05,210
to solve some scaling issues today

00:02:07,470 --> 00:02:17,260
so this does not come you can you cannot

00:02:15,580 --> 00:02:21,880
just support this new speed with the

00:02:17,260 --> 00:02:27,010
current or with all the there are some

00:02:21,880 --> 00:02:29,380
bottlenecks that deal with and your

00:02:27,010 --> 00:02:32,760
performance is limited by your weaker

00:02:29,380 --> 00:02:36,790
component so you need to analyze

00:02:32,760 --> 00:02:39,280
different components along the way and

00:02:36,790 --> 00:02:44,410
you can get as fast as this component

00:02:39,280 --> 00:02:47,890
can give you that's why I want to

00:02:44,410 --> 00:02:52,720
emphasize the importance of analyzing

00:02:47,890 --> 00:02:54,580
bottlenecks in advance do not wait until

00:02:52,720 --> 00:03:01,600
you hit this bottleneck because it could

00:02:54,580 --> 00:03:13,750
be too late you need to prepare both in

00:03:01,600 --> 00:03:18,610
hardware and software and this is the

00:03:13,750 --> 00:03:30,850
stage we need you to prepare a set of

00:03:18,610 --> 00:03:35,799
tasks to have your you know with the new

00:03:30,850 --> 00:03:42,400
problems and you probably will not keep

00:03:35,799 --> 00:03:47,580
doing it until now you're gonna have

00:03:42,400 --> 00:03:51,880
some innovation to solve the problem

00:03:47,580 --> 00:03:59,610
okay so I will start with the with a PCI

00:03:51,880 --> 00:04:03,760
bottleneck you will see a table of

00:03:59,610 --> 00:04:07,209
different generations of PCI we're going

00:04:03,760 --> 00:04:12,400
to focus on generation three you can see

00:04:07,209 --> 00:04:14,760
that the throughput about 120 bits per

00:04:12,400 --> 00:04:14,760
second

00:04:18,500 --> 00:04:26,610
now you take this 126 and you have not

00:04:23,610 --> 00:04:29,280
all of it is available for for you

00:04:26,610 --> 00:04:34,400
because you have some over it for the

00:04:29,280 --> 00:04:38,360
simple encoding you have headers per

00:04:34,400 --> 00:04:55,080
transaction the transaction is done in

00:04:38,360 --> 00:04:57,090
size which is configurable but and if we

00:04:55,080 --> 00:05:03,569
are talking about neck speed of 100

00:04:57,090 --> 00:05:07,379
gigabits per second then it leaves us no

00:05:03,569 --> 00:05:10,919
enough room to reach our line rate with

00:05:07,379 --> 00:05:14,580
our typical layouts that we used till

00:05:10,919 --> 00:05:19,409
now in our data path descriptors we're

00:05:14,580 --> 00:05:22,199
gonna revisit the layouts of our X

00:05:19,409 --> 00:05:27,389
descriptors takes descriptors and the

00:05:22,199 --> 00:05:30,900
completion descriptors the problem gets

00:05:27,389 --> 00:05:34,710
even worse with when you want to reach

00:05:30,900 --> 00:05:39,659
line rate with small packets because

00:05:34,710 --> 00:05:42,089
descriptors overhead is more dominant so

00:05:39,659 --> 00:05:45,800
again this demands rethinking about

00:05:42,089 --> 00:05:45,800
things that you used to do until now

00:05:50,270 --> 00:05:59,400
for to deal with that we had to have new

00:05:56,449 --> 00:06:02,610
new communication descriptors with the

00:05:59,400 --> 00:06:04,710
hardware software can't do it by himself

00:06:02,610 --> 00:06:10,319
it's a PCI transaction that both sides

00:06:04,710 --> 00:06:12,659
need to agree about the heart of the

00:06:10,319 --> 00:06:16,979
hardware features that I'm gonna review

00:06:12,659 --> 00:06:21,180
now the heart idea of that is that you

00:06:16,979 --> 00:06:25,050
already use some moderation you already

00:06:21,180 --> 00:06:27,230
do things in bulks then take advantage

00:06:25,050 --> 00:06:32,330
of that

00:06:27,230 --> 00:06:36,530
and reduced over this this you will see

00:06:32,330 --> 00:06:43,370
this idea repeated and the feature that

00:06:36,530 --> 00:06:47,350
I'm gonna present so first first one is

00:06:43,370 --> 00:06:47,350
the first feature that I'm gonna present

00:06:49,240 --> 00:06:56,780
reducing the overhead of our X

00:06:51,770 --> 00:07:02,930
completions then we'll see and we'll see

00:06:56,780 --> 00:07:06,230
how what's the impact of that then in

00:07:02,930 --> 00:07:14,210
the Teague side again bulking that is

00:07:06,230 --> 00:07:16,250
descriptors and what's the impact okay

00:07:14,210 --> 00:07:21,530
so get that goal is to reduce the PCI

00:07:16,250 --> 00:07:27,770
overhead to reduce the data to

00:07:21,530 --> 00:07:31,490
completion ratio so how it works until

00:07:27,770 --> 00:07:35,270
now we had a fixed sized completion for

00:07:31,490 --> 00:07:40,130
every packet no matter what was the

00:07:35,270 --> 00:07:42,920
packet size so for small packets the

00:07:40,130 --> 00:07:45,290
this fixed side was 64 bytes so for

00:07:42,920 --> 00:07:51,140
small packets let's say packets of 64

00:07:45,290 --> 00:07:54,170
bytes you actually had the same you have

00:07:51,140 --> 00:07:57,740
you had a one to one ratio over the PCI

00:07:54,170 --> 00:08:02,840
between your data your packets and your

00:07:57,740 --> 00:08:07,220
completions and this way you can your

00:08:02,840 --> 00:08:15,740
packet rate is very limited so how it

00:08:07,220 --> 00:08:17,840
works we the feature is to compress you

00:08:15,740 --> 00:08:20,300
take several completions that you

00:08:17,840 --> 00:08:24,580
already as I said you already have

00:08:20,300 --> 00:08:27,610
moderation you already work in box to

00:08:24,580 --> 00:08:33,250
take these completions and compress them

00:08:27,610 --> 00:08:33,250
so that if the completions are

00:08:33,550 --> 00:08:42,969
are almost identical take this part

00:08:37,209 --> 00:08:46,810
write it once and then the parts that

00:08:42,969 --> 00:08:50,079
are per packet that the unique

00:08:46,810 --> 00:08:55,329
information per packet which is much

00:08:50,079 --> 00:08:59,129
smaller write it write it each packet

00:08:55,329 --> 00:09:03,720
and then this way you will save you save

00:08:59,129 --> 00:09:08,889
space you will save bandwidth on the PCI

00:09:03,720 --> 00:09:11,860
and of course the more successful you

00:09:08,889 --> 00:09:14,500
are with compressing completions then

00:09:11,860 --> 00:09:20,939
your packet rate can get higher and

00:09:14,500 --> 00:09:25,209
higher okay so before the feature we had

00:09:20,939 --> 00:09:31,000
completion of 64 bytes per packet after

00:09:25,209 --> 00:09:36,430
that we have some shared information of

00:09:31,000 --> 00:09:40,889
64 bytes then many completions with

00:09:36,430 --> 00:09:46,139
unique information it's eight bytes it

00:09:40,889 --> 00:10:02,920
so you can open the session and compress

00:09:46,139 --> 00:10:06,309
you can get some numbers packet rate

00:10:02,920 --> 00:10:08,889
test of small packets with the server

00:10:06,309 --> 00:10:13,259
doing the SDP drop just with the x TP 1

00:10:08,889 --> 00:10:16,839
sample before that we had 83 million

00:10:13,259 --> 00:10:22,870
packets per second and with this feature

00:10:16,839 --> 00:10:30,279
we are able to reach 130 per second it's

00:10:22,870 --> 00:10:32,470
more than 50 percent more now the

00:10:30,279 --> 00:10:34,840
feature the compression feature was

00:10:32,470 --> 00:10:37,720
implemented and introduced back then

00:10:34,840 --> 00:10:39,520
in German for seven but it gained extra

00:10:37,720 --> 00:10:43,090
significance when the software now

00:10:39,520 --> 00:10:48,250
allows reaching this packet rate with

00:10:43,090 --> 00:10:51,820
small packets so this is thanks to today

00:10:48,250 --> 00:10:57,330
the speech is controlled the private

00:10:51,820 --> 00:11:02,380
flag actually I would I would like to

00:10:57,330 --> 00:11:14,790
discuss with you some some API I would

00:11:02,380 --> 00:11:19,450
like to hear from you later maybe I

00:11:14,790 --> 00:11:22,060
think this kind of features can can be

00:11:19,450 --> 00:11:24,940
standardized because these problems we

00:11:22,060 --> 00:11:31,839
just started yeah and mixed are getting

00:11:24,940 --> 00:11:37,000
faster and faster and we need support

00:11:31,839 --> 00:11:40,390
these features as this was 100 feature

00:11:37,000 --> 00:11:43,660
the second one is bulking of these

00:11:40,390 --> 00:11:46,150
descriptors again the goal is to reduce

00:11:43,660 --> 00:11:52,660
the overhead by combining transmit

00:11:46,150 --> 00:11:55,510
descriptors it works by similar idea

00:11:52,660 --> 00:11:59,650
take the shared info which is the

00:11:55,510 --> 00:12:03,610
control segment write it once and then

00:11:59,650 --> 00:12:08,140
write multiple data segments one per

00:12:03,610 --> 00:12:10,710
packet this can work we knew when you

00:12:08,140 --> 00:12:14,140
know that you have bulk trans

00:12:10,710 --> 00:12:17,290
transmissions for example this is good

00:12:14,140 --> 00:12:22,959
next EP redirect because there is some

00:12:17,290 --> 00:12:26,709
bulking there of sixteen rate also next

00:12:22,959 --> 00:12:35,830
des petites you can bulk up to the nappy

00:12:26,709 --> 00:12:38,550
budget before the feature we had some

00:12:35,830 --> 00:12:42,029
control segment per packet

00:12:38,550 --> 00:12:44,820
in some alignment to align for the 64

00:12:42,029 --> 00:12:48,330
bytes and then the second packet and so

00:12:44,820 --> 00:12:51,510
on with this feature we have one shared

00:12:48,330 --> 00:12:54,930
control segment and one data segment per

00:12:51,510 --> 00:13:10,320
packet we can afford that because we

00:12:54,930 --> 00:13:14,370
have a bulk one 1/4 of your overhead is

00:13:10,320 --> 00:13:22,589
1/4 of what it used to be so we see we

00:13:14,370 --> 00:13:29,250
say this is supported from connectives 5

00:13:22,589 --> 00:13:33,209
and you were starting kernel 5 0 packet

00:13:29,250 --> 00:13:35,850
rate test same transmitter on server we

00:13:33,209 --> 00:13:40,920
run next the boutique's the XDP to

00:13:35,850 --> 00:13:45,779
sample before that we had 20 million

00:13:40,920 --> 00:13:51,750
packet per second so 70 million per

00:13:45,779 --> 00:14:00,589
second after that packets per second for

00:13:51,750 --> 00:14:07,140
SDP tips yeah this is 66 more 66% more

00:14:00,589 --> 00:14:09,570
packets per second for XDP redirect on

00:14:07,140 --> 00:14:13,050
the server and again we use the sample

00:14:09,570 --> 00:14:16,170
program before that we had 64 million

00:14:13,050 --> 00:14:21,620
packets per second and with this feature

00:14:16,170 --> 00:14:21,620
we had 92 millions packet percent

00:14:22,020 --> 00:14:34,550
this is 44 percent more that was PCI

00:14:31,800 --> 00:14:38,360
section next I'm going to review some

00:14:34,550 --> 00:14:38,360
CPU scaling issue

00:14:39,200 --> 00:14:42,450
[Music]

00:14:45,719 --> 00:14:50,309
so the the next page a locator is a

00:14:48,239 --> 00:14:52,469
general component it serves all

00:14:50,309 --> 00:14:57,929
subsystems it's not dedicated to

00:14:52,469 --> 00:15:02,389
networking there is some body a locator

00:14:57,929 --> 00:15:04,739
by the system that cares to minimize the

00:15:02,389 --> 00:15:08,549
external fragmentation and combines

00:15:04,739 --> 00:15:11,809
packets so it combines pages of flow

00:15:08,549 --> 00:15:15,209
orders into pages of higher orders and

00:15:11,809 --> 00:15:21,149
this is a shared resource a different

00:15:15,209 --> 00:15:22,139
CPUs as for pages and this needs to be

00:15:21,149 --> 00:15:27,449
seen

00:15:22,139 --> 00:15:34,289
this is protected by a spin lock in this

00:15:27,449 --> 00:15:37,109
in the structure to reduce the frequency

00:15:34,289 --> 00:15:41,579
of accesses to this shared resource the

00:15:37,109 --> 00:15:48,659
camera provides a layer of local per CPU

00:15:41,579 --> 00:15:50,969
page lists PC peas and short so for for

00:15:48,659 --> 00:15:55,379
a page a location requests like general

00:15:50,969 --> 00:15:57,689
asks the local PCP for a page if the if

00:15:55,379 --> 00:16:00,599
there is some page available then we're

00:15:57,689 --> 00:16:03,779
done if not the PCP as the body a

00:16:00,599 --> 00:16:10,219
locator for a page and this is done

00:16:03,779 --> 00:16:15,499
under zone lock same thing in page free

00:16:10,219 --> 00:16:21,899
very similar kernel releases the page

00:16:15,499 --> 00:16:25,799
back to the local PCP PCP and then if

00:16:21,899 --> 00:16:29,099
this PCP is full then the PCP releases

00:16:25,799 --> 00:16:39,299
the pages back to the body system this

00:16:29,099 --> 00:16:42,119
is done under yes sorry I think that was

00:16:39,299 --> 00:16:43,649
fixed a month ago the second entry yeah

00:16:42,119 --> 00:16:46,169
the free because there was a bug there

00:16:43,649 --> 00:16:47,699
and was freed back to me yeah I mean I

00:16:46,169 --> 00:16:51,329
will mention that later no it's it's

00:16:47,699 --> 00:16:53,010
it's not fixed it's just yeah the

00:16:51,329 --> 00:16:56,910
severity was reduced but

00:16:53,010 --> 00:17:01,640
not totally fixed just still if if the

00:16:56,910 --> 00:17:04,589
PCP gets full then there's no place

00:17:01,640 --> 00:17:12,570
release the page back to the body

00:17:04,589 --> 00:17:20,010
alligator okay one body alligator many

00:17:12,570 --> 00:17:22,440
PCP is one big so when when every PCP

00:17:20,010 --> 00:17:26,880
satisfies the local page allocation and

00:17:22,440 --> 00:17:30,800
fries there is no contention and we are

00:17:26,880 --> 00:17:38,160
fine but is that always the case

00:17:30,800 --> 00:17:46,370
can you guess let's see it's a view that

00:17:38,160 --> 00:17:50,630
typical network flow we have a receiving

00:17:46,370 --> 00:17:55,350
usually that allocates pages in advance

00:17:50,630 --> 00:18:00,030
for the incoming packets it does so from

00:17:55,350 --> 00:18:03,930
the local pc p s QP is created past of

00:18:00,030 --> 00:18:09,030
the network stack and once done the

00:18:03,930 --> 00:18:15,000
pages of the s QP is released back local

00:18:09,030 --> 00:18:18,800
PCP okay all good the processing the

00:18:15,000 --> 00:18:22,650
processing has several scheduling points

00:18:18,800 --> 00:18:25,380
this means that handling and release or

00:18:22,650 --> 00:18:32,360
the ICB can be completed on a different

00:18:25,380 --> 00:18:32,360
core than the one allocated the pages

00:18:32,810 --> 00:18:44,580
okay so for example if if Nappi runs on

00:18:38,810 --> 00:18:51,180
pcp x that keeps allocate pages and s

00:18:44,580 --> 00:18:53,130
key b is continuously moved to core to

00:18:51,180 --> 00:18:58,620
core y then it would continuously

00:18:53,130 --> 00:19:00,600
release pages to another PCP and this

00:18:58,620 --> 00:19:04,650
will cause imbalance PCP

00:19:00,600 --> 00:19:05,790
yeah if this pattern is is repeated all

00:19:04,650 --> 00:19:10,560
the time

00:19:05,790 --> 00:19:13,650
then core eggs will will keep asked by

00:19:10,560 --> 00:19:17,480
the educator four pages Couric's will

00:19:13,650 --> 00:19:20,910
keep as actually pcp Y will keep

00:19:17,480 --> 00:19:25,520
releasing pages back to the body system

00:19:20,910 --> 00:19:25,520
and all is done under the same zone not

00:19:26,750 --> 00:19:36,650
ok let's let's do some tests and see how

00:19:30,840 --> 00:19:43,560
severe the problem is set up is that

00:19:36,650 --> 00:19:51,170
connected 500 gigabytes back-to-back 48

00:19:43,560 --> 00:19:55,650
course I used 12 channels receiving

00:19:51,170 --> 00:20:05,040
running 214 at perf instances TCP

00:19:55,650 --> 00:20:09,180
multi-stream kernel is very new 0r0

00:20:05,040 --> 00:20:11,760
I modified it to disable some of the

00:20:09,180 --> 00:20:17,280
page page relocation optimizations that

00:20:11,760 --> 00:20:20,430
we already have I didn't want them I

00:20:17,280 --> 00:20:26,360
reduced them to you to have the size in

00:20:20,430 --> 00:20:26,360
order to simulate the packet rate of 200

00:20:27,770 --> 00:20:36,780
so first one observation actually we

00:20:33,090 --> 00:20:39,690
have two types of course type one runs

00:20:36,780 --> 00:20:43,850
both nappy and some mid server

00:20:39,690 --> 00:20:48,870
processing so it keeps allocating pages

00:20:43,850 --> 00:20:52,380
for the receiver and it also releases

00:20:48,870 --> 00:20:56,610
some pages back the second type of

00:20:52,380 --> 00:21:02,960
course runs only net server and it just

00:20:56,610 --> 00:21:11,100
keeps releasing pages bandwidth is not

00:21:02,960 --> 00:21:13,350
only for T so when monitoring the two

00:21:11,100 --> 00:21:17,100
events of page allocation and page free

00:21:13,350 --> 00:21:17,620
in perf that is the system-wide the

00:21:17,100 --> 00:21:20,320
number

00:21:17,620 --> 00:21:27,010
are fine we see more or less the same

00:21:20,320 --> 00:21:30,550
allocation and free statistics then I

00:21:27,010 --> 00:21:34,120
looked into core number one I see that

00:21:30,550 --> 00:21:35,620
there are many page allocations few page

00:21:34,120 --> 00:21:39,070
freeze

00:21:35,620 --> 00:21:43,770
this means that it's core of type one

00:21:39,070 --> 00:21:47,309
that I described earlier and of course

00:21:43,770 --> 00:21:49,900
it's an imbalanced PCP and it's gonna

00:21:47,309 --> 00:21:54,640
take a lot indexes the body are looking

00:21:49,900 --> 00:22:00,580
to another core attribute set represents

00:21:54,640 --> 00:22:03,730
the other type of course it only frees

00:22:00,580 --> 00:22:07,470
pages there's one page allocation that I

00:22:03,730 --> 00:22:07,470
need to figure out where it come from

00:22:07,830 --> 00:22:23,500
but yeah it's an imbalanced PCP and it

00:22:11,080 --> 00:22:24,580
also needs to the flame graph this is

00:22:23,500 --> 00:22:29,610
how it looks

00:22:24,580 --> 00:22:34,510
we're spinning 55% of the time

00:22:29,610 --> 00:22:37,690
system-wide you can see that well blocks

00:22:34,510 --> 00:22:43,240
of nappa processing on the left-hand

00:22:37,690 --> 00:22:46,690
side okay so this is the nappy

00:22:43,240 --> 00:22:51,970
processing within this never seen 77% of

00:22:46,690 --> 00:22:56,860
the time we are spinning so we clearly

00:22:51,970 --> 00:23:02,110
see the bottoming here some solutions

00:22:56,860 --> 00:23:07,300
that were suggested first of all I want

00:23:02,110 --> 00:23:10,420
to introduce the page pool API by by

00:23:07,300 --> 00:23:18,490
Jesper unfortunately he had to leave

00:23:10,420 --> 00:23:20,890
yesterday he's not it's it's under the

00:23:18,490 --> 00:23:26,460
net sub system it was introduced in

00:23:20,890 --> 00:23:29,670
kernel 418 and it has several goals but

00:23:26,460 --> 00:23:35,160
in general

00:23:29,670 --> 00:23:40,530
the main goal is to deal with these kind

00:23:35,160 --> 00:23:42,180
of problems of the page a locator and

00:23:40,530 --> 00:23:46,230
many of the ideas that I will describe

00:23:42,180 --> 00:23:48,650
later are intending to be integrated

00:23:46,230 --> 00:23:52,470
into the page pool

00:23:48,650 --> 00:23:55,020
okay so first idea is page reuse make

00:23:52,470 --> 00:23:57,660
reuse of the if you're on page achieve

00:23:55,020 --> 00:24:04,260
better page utilization it's very simple

00:23:57,660 --> 00:24:07,140
very fundamental for example for 4k 4

00:24:04,260 --> 00:24:10,320
kilobyte page you can flip it twice for

00:24:07,140 --> 00:24:14,550
default MTU and this way you reduce the

00:24:10,320 --> 00:24:21,600
allocation rate by half we actually have

00:24:14,550 --> 00:24:23,670
some hardware feature and better

00:24:21,600 --> 00:24:27,990
utilization according to the actual

00:24:23,670 --> 00:24:33,570
received packet size and not according

00:24:27,990 --> 00:24:38,940
to the MTU size but I'm gonna move on to

00:24:33,570 --> 00:24:43,200
the next idea second idea is to recycle

00:24:38,940 --> 00:24:45,510
the page so you as a driver or you as a

00:24:43,200 --> 00:24:49,410
page pool maintain a waiting hue of

00:24:45,510 --> 00:24:53,060
allocated pages of the pages that you

00:24:49,410 --> 00:24:53,060
already are looking at it from the PCP

00:24:54,500 --> 00:25:00,930
for this page processed by the by the

00:24:59,100 --> 00:25:03,350
stack there is some ref counting going

00:25:00,930 --> 00:25:05,700
over there and we need when you need to

00:25:03,350 --> 00:25:08,250
allocate a new page

00:25:05,700 --> 00:25:14,430
you check internally for an available

00:25:08,250 --> 00:25:16,760
one usually you check that the ref count

00:25:14,430 --> 00:25:19,500
is back to one because you hold up a

00:25:16,760 --> 00:25:23,000
reference count and you need all the

00:25:19,500 --> 00:25:25,590
other reference counts to be released

00:25:23,000 --> 00:25:29,030
the issue here is that there is no

00:25:25,590 --> 00:25:31,920
signaling or core or callback mechanism

00:25:29,030 --> 00:25:35,370
and you need to continuously

00:25:31,920 --> 00:25:39,900
observe the ref code upon your

00:25:35,370 --> 00:25:42,730
allocations and then you are exposed to

00:25:39,900 --> 00:25:47,110
head of the queue blocks because

00:25:42,730 --> 00:25:49,590
if your first page in the queue is not

00:25:47,110 --> 00:25:52,540
released yet it's not available for you

00:25:49,590 --> 00:25:55,000
then it's unclear what is the right

00:25:52,540 --> 00:26:00,370
strategy to do here is just release it

00:25:55,000 --> 00:26:02,950
or you know pop it back to the end of

00:26:00,370 --> 00:26:05,590
the queue there is no clear policy and

00:26:02,950 --> 00:26:08,920
there's no one policy that is good for

00:26:05,590 --> 00:26:13,180
all cases this is still helpful actually

00:26:08,920 --> 00:26:15,310
if you know and expect the traffic

00:26:13,180 --> 00:26:23,620
pattern and the processing time that you

00:26:15,310 --> 00:26:28,230
your your data center and and then tune

00:26:23,620 --> 00:26:28,230
your recycle queue accordingly

00:26:29,800 --> 00:26:35,010
next idea is to page remote release

00:26:35,290 --> 00:26:38,339
[Music]

00:26:46,170 --> 00:26:59,140
break so the idea is to have some page

00:26:55,120 --> 00:27:04,750
returned directly to release the page

00:26:59,140 --> 00:27:07,960
directly to its origin meeting PCP from

00:27:04,750 --> 00:27:10,120
the remote PC from Europe your you're

00:27:07,960 --> 00:27:13,090
working PCP which is the remote one

00:27:10,120 --> 00:27:18,160
because you you probably moved to the

00:27:13,090 --> 00:27:22,120
original PCP this way you keep the PCP

00:27:18,160 --> 00:27:25,690
balanced for this you need to save some

00:27:22,120 --> 00:27:29,290
minimal amount of metadata on the STB

00:27:25,690 --> 00:27:32,770
structure and use it upon daisuki be

00:27:29,290 --> 00:27:36,390
released the recent direction is to

00:27:32,770 --> 00:27:39,940
integrate this into the page pool api

00:27:36,390 --> 00:27:44,800
and release the page back to the page

00:27:39,940 --> 00:27:48,790
pool and not to the PCP to propose

00:27:44,800 --> 00:27:52,669
places were suggested input page and in

00:27:48,790 --> 00:27:56,029
k free sqb currently the

00:27:52,669 --> 00:27:59,919
action is to do for Jeffrey Eric pointed

00:27:56,029 --> 00:28:03,980
out many problems with that area Seaver

00:27:59,919 --> 00:28:07,789
trying his best to deal with with issues

00:28:03,980 --> 00:28:11,080
but this is the the directions you can

00:28:07,789 --> 00:28:15,850
there is a pointer for the notes that

00:28:11,080 --> 00:28:21,789
yes / and ideas are taking doing their

00:28:15,850 --> 00:28:30,830
solutions you can go and and see that

00:28:21,789 --> 00:28:36,279
current status and actually no matter

00:28:30,830 --> 00:28:40,159
how how good you get in the net networks

00:28:36,279 --> 00:28:42,230
an orthogonal efforts can be also done

00:28:40,159 --> 00:28:50,720
in the memory subsystem in the memory in

00:28:42,230 --> 00:28:54,830
the mmm subsystem to improve the scaling

00:28:50,720 --> 00:28:58,940
of the zone look so there is some RC by

00:28:54,830 --> 00:29:04,780
Aaron do I don't want to get into the

00:28:58,940 --> 00:29:06,820
idea but he found some some places that

00:29:04,780 --> 00:29:11,629
[Music]

00:29:06,820 --> 00:29:13,789
reduce the critical section the number

00:29:11,629 --> 00:29:20,539
of operations and the cost of the

00:29:13,789 --> 00:29:27,379
operations that under the lock and this

00:29:20,539 --> 00:29:31,070
way he actually improves the scaling of

00:29:27,379 --> 00:29:38,289
the lock unrelated to the effort net

00:29:31,070 --> 00:29:42,260
subsystem accepted patches in the mm

00:29:38,289 --> 00:29:43,840
subsystem there were some improvements

00:29:42,260 --> 00:29:46,970
[Music]

00:29:43,840 --> 00:29:49,369
accepted to 417 again I don't want to

00:29:46,970 --> 00:29:52,070
get into this and there's some bug fixes

00:29:49,369 --> 00:29:58,100
I think this is the one that is

00:29:52,070 --> 00:30:02,239
mentioned so my earlier simulation and

00:29:58,100 --> 00:30:03,930
test was done with this fixed inside

00:30:02,239 --> 00:30:07,410
okay so

00:30:03,930 --> 00:30:15,830
this fix just reduces the severity does

00:30:07,410 --> 00:30:24,930
not solve the problem that's it

00:30:15,830 --> 00:30:30,300
questions yeah so the original idea I

00:30:24,930 --> 00:30:35,250
had one year ago was sending back to skp

00:30:30,300 --> 00:30:39,780
to the original CPU using a channel with

00:30:35,250 --> 00:30:45,240
an IP I but I had an idea last week I

00:30:39,780 --> 00:30:48,870
sent patch yesterday in TCP stack you

00:30:45,240 --> 00:30:53,220
can actually use a participe cache of

00:30:48,870 --> 00:30:56,130
one skb meaning that receive message

00:30:53,220 --> 00:30:58,110
time when you copy the data instead of

00:30:56,130 --> 00:31:00,360
doing cafes can be on behalf of the

00:30:58,110 --> 00:31:04,080
process consuming the escapee you just

00:31:00,360 --> 00:31:06,060
put it back in the TCP socket and so the

00:31:04,080 --> 00:31:08,700
next packet being received by a bottom

00:31:06,060 --> 00:31:12,060
half Wilfried ask a B so it will be

00:31:08,700 --> 00:31:15,060
freed on the behalf of the CPU feeding

00:31:12,060 --> 00:31:18,780
the packet on the cube and that gave me

00:31:15,060 --> 00:31:21,240
like ten percent improvement in DPR RPC

00:31:18,780 --> 00:31:24,390
what good so it's interesting how much

00:31:21,240 --> 00:31:27,900
will give with my test where I force

00:31:24,390 --> 00:31:28,790
some course to not do Napa locations and

00:31:27,900 --> 00:31:32,280
others

00:31:28,790 --> 00:31:36,780
I didn't have a chance to take a look at

00:31:32,280 --> 00:31:38,820
your patch but I will bring that tested

00:31:36,780 --> 00:31:43,260
that was ten percent with the page

00:31:38,820 --> 00:31:45,330
recycle in place so there is only cost

00:31:43,260 --> 00:31:48,810
of the SKF itself because the page

00:31:45,330 --> 00:31:52,950
recycling only works for the page but no

00:31:48,810 --> 00:31:56,130
desk above right but it depends on the

00:31:52,950 --> 00:31:59,060
assumption that there is some next

00:31:56,130 --> 00:31:59,060
packet or

00:31:59,570 --> 00:32:03,799
okay thanks

00:32:11,240 --> 00:32:17,700
so easier Erik's descriptor is 64-bit

00:32:14,669 --> 00:32:21,330
64-byte white each rx descriptor a

00:32:17,700 --> 00:32:27,350
64-bit white this was the completion not

00:32:21,330 --> 00:32:27,350
the RX descriptor and yeah it's 64 bytes

00:32:27,679 --> 00:32:32,130
and you mentioned about the compression

00:32:30,149 --> 00:32:35,179
of the RX descriptor completion is it

00:32:32,130 --> 00:32:39,779
compression or just a completion queue

00:32:35,179 --> 00:32:43,380
we compress that completions so that

00:32:39,779 --> 00:32:48,029
instead of a 64 bytes completion per

00:32:43,380 --> 00:32:50,669
packet we have one 64 bytes completion

00:32:48,029 --> 00:32:55,679
for the shared info and then eight bytes

00:32:50,669 --> 00:32:57,240
for each packet which yeah and the speed

00:32:55,679 --> 00:33:00,600
lock which you mentioned which shows up

00:32:57,240 --> 00:33:02,970
55 percent 75 percent would that problem

00:33:00,600 --> 00:33:05,909
go away if you just pass it and do your

00:33:02,970 --> 00:33:07,500
interrupts affinity correctly sorry can

00:33:05,909 --> 00:33:09,750
you repeat you mentioned there are

00:33:07,500 --> 00:33:13,590
various spin locks showing up in the

00:33:09,750 --> 00:33:15,539
test like 55 percent 77 percent because

00:33:13,590 --> 00:33:18,029
your nap is running somewhere then the

00:33:15,539 --> 00:33:20,100
consumer CPU well if you tossed it

00:33:18,029 --> 00:33:21,960
everything more or less the consumer

00:33:20,100 --> 00:33:24,330
producer and you're interrupting would

00:33:21,960 --> 00:33:27,450
that problem automatically come down

00:33:24,330 --> 00:33:30,960
yeah if you have if your PCB PCBs are

00:33:27,450 --> 00:33:33,450
balanced you have more or less the same

00:33:30,960 --> 00:33:35,970
allocation and free rate of your PCP

00:33:33,450 --> 00:33:39,720
then yeah there's no problem or the

00:33:35,970 --> 00:33:42,389
problem is much less severe yeah that's

00:33:39,720 --> 00:33:51,149
because I configured my setup this way I

00:33:42,389 --> 00:33:54,120
have only 12 nappy processors and 448 of

00:33:51,149 --> 00:33:56,840
course that do other stuff

00:33:54,120 --> 00:33:59,580
[Music]

00:33:56,840 --> 00:34:02,700
okay another question is the bulking

00:33:59,580 --> 00:34:05,730
works without x2p does it work for

00:34:02,700 --> 00:34:08,609
normal TCP flows with expect more kind

00:34:05,730 --> 00:34:10,770
of when do you stop the bulking 40x it's

00:34:08,609 --> 00:34:14,940
good work it's not

00:34:10,770 --> 00:34:18,780
because the bottleneck in this case is

00:34:14,940 --> 00:34:23,010
not yet the PCI the bottleneck is the

00:34:18,780 --> 00:34:26,790
CPU the the heavy net Network stack does

00:34:23,010 --> 00:34:29,369
not allow these rates yet but but yeah

00:34:26,790 --> 00:34:32,550
it's the same idea can be implemented

00:34:29,369 --> 00:34:35,730
there's no no limitation and with the

00:34:32,550 --> 00:34:40,530
bulking do you see partial rates going

00:34:35,730 --> 00:34:42,450
down substantially on the bus so since

00:34:40,530 --> 00:34:44,639
you implemented is bulking do you see

00:34:42,450 --> 00:34:51,589
the PCI partial rates going down

00:34:44,639 --> 00:34:51,589
I didn't monitor that no thank you

00:34:56,660 --> 00:35:21,760
more questions I so I just kind of

00:35:15,050 --> 00:35:24,070
puzzled I look at receive things so

00:35:21,760 --> 00:35:26,270
[Music]

00:35:24,070 --> 00:35:30,220
weird I go for example is slightly

00:35:26,270 --> 00:35:33,680
different what it's doing is storing the

00:35:30,220 --> 00:35:37,330
like any extra data linearly with a

00:35:33,680 --> 00:35:42,500
packet and then you just make

00:35:37,330 --> 00:35:44,930
descriptors much smaller right and so I

00:35:42,500 --> 00:35:48,290
understand that you transmit descriptors

00:35:44,930 --> 00:35:52,250
are also like after you compress

00:35:48,290 --> 00:35:55,970
everything you kind of get a an iPad

00:35:52,250 --> 00:36:02,020
control and a 16 byte control and eight

00:35:55,970 --> 00:36:02,020
bytes each data rate compression

00:36:05,110 --> 00:36:14,630
overhead of the control is 16 bytes and

00:36:08,330 --> 00:36:18,080
then 8 bytes per packet so I'm saying

00:36:14,630 --> 00:36:24,380
an average after compression you get 16

00:36:18,080 --> 00:36:25,850
bytes I prefer to call it bulking not

00:36:24,380 --> 00:36:30,950
compression not to mix with the other

00:36:25,850 --> 00:36:34,060
feature but yeah for for large bugs you

00:36:30,950 --> 00:36:37,160
can just ignore the first 16 bytes and

00:36:34,060 --> 00:36:42,850
then you have one data segment of 8

00:36:37,160 --> 00:36:42,850
bytes instead of 64 so it's one eight

00:36:46,780 --> 00:36:51,880
it's really basically doing what we're

00:36:48,790 --> 00:36:55,660
tires doing so yes now what we're Tigers

00:36:51,880 --> 00:36:58,540
doing is just storing metadata linearly

00:36:55,660 --> 00:37:02,050
with a packet and then each descriptor

00:36:58,540 --> 00:37:05,680
is just a pointer and so each descriptor

00:37:02,050 --> 00:37:07,320
is 16 bytes without like compression of

00:37:05,680 --> 00:37:09,790
alkene tricks

00:37:07,320 --> 00:37:12,010
yeah but might be something it yeah but

00:37:09,790 --> 00:37:14,980
but but if you if you don't have a bulk

00:37:12,010 --> 00:37:18,700
to deal with then you have to have your

00:37:14,980 --> 00:37:22,090
control Simon and you want to be aligned

00:37:18,700 --> 00:37:25,480
to the cache line so that we had before

00:37:22,090 --> 00:37:29,290
this feature the key here is like you

00:37:25,480 --> 00:37:32,170
have a bulk to - right - right down to

00:37:29,290 --> 00:37:37,030
the PCI otherwise you're not going to

00:37:32,170 --> 00:37:37,900
save opportunity to see exactly so that

00:37:37,030 --> 00:37:40,120
that's my point

00:37:37,900 --> 00:37:42,610
so yeah what I'm saying is alternative

00:37:40,120 --> 00:37:46,540
is you can you could store some data

00:37:42,610 --> 00:37:52,990
linear with a packet then you have 256

00:37:46,540 --> 00:37:55,660
bytes this Express packets anyway and so

00:37:52,990 --> 00:37:59,140
we small like if packets are smaller

00:37:55,660 --> 00:38:01,480
than 256 bytes for example your u.s.

00:37:59,140 --> 00:38:05,670
you're sending this data anyway so it

00:38:01,480 --> 00:38:10,710
would you put put extra data they're

00:38:05,670 --> 00:38:10,710
doing you're suggesting to inline the

00:38:10,740 --> 00:38:19,600
inline control with the packet so you

00:38:14,200 --> 00:38:21,370
would have yeah it's a and then we

00:38:19,600 --> 00:38:26,340
support this feature it's not just out

00:38:21,370 --> 00:38:26,340
of the scope of these slides but yeah

00:38:26,670 --> 00:38:32,850
that's a good idea and we support that

00:38:35,640 --> 00:38:40,990
one quick one

00:38:38,200 --> 00:38:43,750
one minute question so I have just a

00:38:40,990 --> 00:38:47,890
quick question which leads to your talk

00:38:43,750 --> 00:38:50,500
and the previous comment both so the

00:38:47,890 --> 00:38:53,590
data you are showing in terms of

00:38:50,500 --> 00:38:55,550
improvement is talking about a very

00:38:53,590 --> 00:38:59,120
homogeneous

00:38:55,550 --> 00:39:01,820
so those kinds of workloads can show the

00:38:59,120 --> 00:39:05,750
value of any kind of you know PCI

00:39:01,820 --> 00:39:08,540
improvements to the workload do you see

00:39:05,750 --> 00:39:12,200
the same kind of improvement or any

00:39:08,540 --> 00:39:14,810
uptick in improvement using the ideas

00:39:12,200 --> 00:39:17,000
you suggested with a mixed workload and

00:39:14,810 --> 00:39:17,570
a fully not stack that's my first

00:39:17,000 --> 00:39:20,510
question

00:39:17,570 --> 00:39:22,160
and second is relating to the comment

00:39:20,510 --> 00:39:27,140
that was made just before I started

00:39:22,160 --> 00:39:32,200
speaking was the use of you know 256

00:39:27,140 --> 00:39:36,490
byte a PCIe transaction do you see that

00:39:32,200 --> 00:39:40,310
idea also usable in a fully Mac stack

00:39:36,490 --> 00:39:42,500
with a regular workload like and I'm not

00:39:40,310 --> 00:39:45,080
going into anything more than a micro

00:39:42,500 --> 00:39:50,050
benchmark but if you used a pause with

00:39:45,080 --> 00:39:53,570
mixed sizes would you see an improvement

00:39:50,050 --> 00:39:57,590
yeah the size here is not was not part

00:39:53,570 --> 00:40:02,210
of the synthetic benchmark it was just

00:39:57,590 --> 00:40:04,850
to emphasize that we just to make things

00:40:02,210 --> 00:40:07,190
more difficult because when you work

00:40:04,850 --> 00:40:11,450
with small packet sizes when the packet

00:40:07,190 --> 00:40:17,570
rate is higher but the same idea does

00:40:11,450 --> 00:40:19,280
not rely on the packet size you can work

00:40:17,570 --> 00:40:24,380
with any packet size or with different

00:40:19,280 --> 00:40:28,490
packet sizes the heart of all these

00:40:24,380 --> 00:40:31,750
features is to reduce overhead by using

00:40:28,490 --> 00:40:34,310
bugs and it can be yeah it can be used

00:40:31,750 --> 00:40:38,720
to answer your first question it can be

00:40:34,310 --> 00:40:43,610
used along the stack where you need to

00:40:38,720 --> 00:40:47,540
find the right the right trade of the

00:40:43,610 --> 00:40:53,620
right balance point between two long

00:40:47,540 --> 00:40:59,030
bulk and because this will get you your

00:40:53,620 --> 00:41:01,760
your cash dirtied and let's take this

00:40:59,030 --> 00:41:04,520
one after the outside please Thanks

00:41:01,760 --> 00:41:06,580
let's give it up for Terry

00:41:04,520 --> 00:41:06,580

YouTube URL: https://www.youtube.com/watch?v=YgGL6V2kloI


