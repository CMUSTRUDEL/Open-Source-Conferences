Title: Netdev 0x13 - Industry Perspectives
Publication date: 2019-06-03
Playlist: Netdev 0x13 - Day 3
Description: 
	The Industry Perspective Panel is a new session which will bring
in networking industry craftsmen to discuss different topics.

The primary goal is to have developers (and ops types)
appreciate how Linux (networking) is used in the industry. We are
not going to restrict the perspectives to be Linux only i.e a
panelist does not necessarily have to use Linux in their operations,
but needs to answer as to why not.

The secondary goal of this session is to learn how we can
do better in Linux. Both users and non-users of Linux can
teach us something.

The format is:
- The panelist gives a presentation of their craft
- questions by panelist moderator(s)
- open mike to the conference

At 0x13, the theme was network operations. We have:
- a big data centre view (LinkedIn)
- a world wide cloud platform deployment view (CloudFlare)
- and an edge cloud view (Mobiledgex

Panel co-chairs: Sowmini Varadhan and Roopa Prabhu
Featuring panelists:
Artur Makutunowicz from LinkedIn,
Marek Majkowski from Cloudflare,
and Vikram Siwach from MobiledgeX.

More info:
https://netdevconf.org/0x13/news.html?panel-industry-perspectives
Captions: 
	00:00:00,000 --> 00:00:04,950
okay so we'll get started this is our

00:00:02,010 --> 00:00:08,670
first-ever industry perspective panel if

00:00:04,950 --> 00:00:13,880
you guys like it we'll do it we'll keep

00:00:08,670 --> 00:00:17,520
it as a format our theme this time is

00:00:13,880 --> 00:00:22,619
network operations we have three very

00:00:17,520 --> 00:00:25,949
different types of operators the model

00:00:22,619 --> 00:00:28,019
is going to be the panelists will

00:00:25,949 --> 00:00:30,810
present anywhere from fifty to twenty

00:00:28,019 --> 00:00:32,460
minutes about their architecture it

00:00:30,810 --> 00:00:33,780
doesn't have to be necessarily Linux but

00:00:32,460 --> 00:00:38,910
we'd like to know why they don't use

00:00:33,780 --> 00:00:42,600
Linux and that will be followed by the

00:00:38,910 --> 00:00:44,399
two panel chairs Rupa and so many who

00:00:42,600 --> 00:00:47,370
are going to ask hard questions

00:00:44,399 --> 00:00:50,149
maybe not you they'll take it easy on

00:00:47,370 --> 00:00:53,219
them because this is the first time and

00:00:50,149 --> 00:00:57,980
then it's open to the audience to ask

00:00:53,219 --> 00:01:00,030
any questions okay thanks so that I

00:00:57,980 --> 00:01:11,369
please introduce yourselves

00:01:00,030 --> 00:01:13,909
I know Bala Jax hey my name is Arthur I

00:01:11,369 --> 00:01:17,189
work as a software engineer at LinkedIn

00:01:13,909 --> 00:01:18,900
I'm replacing for Sean who had last

00:01:17,189 --> 00:01:23,580
minute business activity and had to

00:01:18,900 --> 00:01:26,369
cancel last minute hello my name is Mark

00:01:23,580 --> 00:01:29,700
I work at the culture and I'm an

00:01:26,369 --> 00:01:32,000
engineer with my biggest expertise in

00:01:29,700 --> 00:01:32,000
these applications

00:01:38,520 --> 00:01:44,830
all right so maybe this this might be

00:01:43,390 --> 00:01:47,890
interesting to you guys we will come out

00:01:44,830 --> 00:01:49,390
of Linux come a little bit and see how

00:01:47,890 --> 00:01:51,580
people are using stuff and what

00:01:49,390 --> 00:01:56,020
real-life challenges we are trying to

00:01:51,580 --> 00:01:58,180
solve so we are just young startup in

00:01:56,020 --> 00:02:01,210
Silicon Valley we were funded by Dasha

00:01:58,180 --> 00:02:03,130
Lika and basically I use cases that we

00:02:01,210 --> 00:02:04,810
can aggregate operators across the world

00:02:03,130 --> 00:02:07,990
and we'll provide that infrastructure

00:02:04,810 --> 00:02:11,710
for people like neon tape or game

00:02:07,990 --> 00:02:14,830
applications to come and run and why is

00:02:11,710 --> 00:02:17,310
it important I think the way we look at

00:02:14,830 --> 00:02:21,910
this market right now is emerging

00:02:17,310 --> 00:02:25,000
latency is becoming important people are

00:02:21,910 --> 00:02:27,670
seeing the need to run applications

00:02:25,000 --> 00:02:29,770
closest to edge now educate boundaries

00:02:27,670 --> 00:02:31,270
are still debatable people still believe

00:02:29,770 --> 00:02:33,690
that some of the it will be actually on

00:02:31,270 --> 00:02:36,310
the device on the cars in the planes and

00:02:33,690 --> 00:02:38,650
but we strongly believe the networks

00:02:36,310 --> 00:02:40,710
will have some say in how much dollops

00:02:38,650 --> 00:02:43,570
in your future

00:02:40,710 --> 00:02:48,670
but first cases we want to tackle is the

00:02:43,570 --> 00:02:50,860
AR and machine learning stuff why now

00:02:48,670 --> 00:02:52,180
because we see operators are made

00:02:50,860 --> 00:02:54,310
billion dollar investments in

00:02:52,180 --> 00:02:56,650
virtualizing their infrastructure we

00:02:54,310 --> 00:02:59,590
could actually convert them to interlock

00:02:56,650 --> 00:03:01,900
internet like structure who are

00:02:59,590 --> 00:03:04,540
customers basically whoever wants to run

00:03:01,900 --> 00:03:10,630
an application on page and the device

00:03:04,540 --> 00:03:14,440
makers so this is from the recent study

00:03:10,630 --> 00:03:15,850
we did essentially to look at it a lot

00:03:14,440 --> 00:03:18,340
of cloud service providers are pumping

00:03:15,850 --> 00:03:20,110
money into cloud and it's actually a lot

00:03:18,340 --> 00:03:23,050
of money three hundred in dollars have

00:03:20,110 --> 00:03:26,050
gone in last ten years but then equally

00:03:23,050 --> 00:03:28,180
amount of actually way more amount of

00:03:26,050 --> 00:03:30,040
money has gone into building the mobile

00:03:28,180 --> 00:03:33,070
networks to support the sustained data

00:03:30,040 --> 00:03:37,180
rate so in last ten years is like 1.7

00:03:33,070 --> 00:03:38,770
trillion dollars and it's actually the

00:03:37,180 --> 00:03:40,870
more the movement we are seeing right

00:03:38,770 --> 00:03:43,060
now across operators is that they want

00:03:40,870 --> 00:03:45,040
to virtualize their infrastructure so a

00:03:43,060 --> 00:03:48,610
lot of these assets are still

00:03:45,040 --> 00:03:50,500
running without any applications they

00:03:48,610 --> 00:03:52,900
are still oversubscribe overbuilt

00:03:50,500 --> 00:03:54,909
because that was the traditional way how

00:03:52,900 --> 00:03:59,370
operators used to build it and we are

00:03:54,909 --> 00:04:03,099
seeing huge increase in IOT devices

00:03:59,370 --> 00:04:05,019
phones cars drones everything in your

00:04:03,099 --> 00:04:07,959
home everything is getting connected so

00:04:05,019 --> 00:04:13,000
there is a steady state increase in data

00:04:07,959 --> 00:04:16,060
consumption and networks in between are

00:04:13,000 --> 00:04:17,729
just a byte pipe as of today and their

00:04:16,060 --> 00:04:21,489
model is just to connect it to internet

00:04:17,729 --> 00:04:25,150
this is what typically happens when you

00:04:21,489 --> 00:04:26,889
consume data it runs through mobile

00:04:25,150 --> 00:04:29,229
networks in the way it is organized is

00:04:26,889 --> 00:04:31,000
pretty painful but essentially it's a

00:04:29,229 --> 00:04:33,340
collection of boxes then it goes to

00:04:31,000 --> 00:04:34,840
cloud and comes back so if you're lucky

00:04:33,340 --> 00:04:37,509
you probably have a 40 millisecond delay

00:04:34,840 --> 00:04:40,509
there oh it could be worse depending on

00:04:37,509 --> 00:04:42,580
where you are but nothing is dynamic

00:04:40,509 --> 00:04:44,949
nothing is closer to edge we can't

00:04:42,580 --> 00:04:47,050
figure out where to go most of the times

00:04:44,949 --> 00:04:49,180
your sessions will be anchored on some

00:04:47,050 --> 00:04:52,630
gateway and then you will be routed back

00:04:49,180 --> 00:04:54,699
even if there's a cache by a cloud

00:04:52,630 --> 00:04:59,139
service coil so it's it's something

00:04:54,699 --> 00:05:00,820
which is the problem we want to solve so

00:04:59,139 --> 00:05:03,460
what did we what we are trying to do is

00:05:00,820 --> 00:05:05,560
essentially if you look at it if I can

00:05:03,460 --> 00:05:09,300
show this these are the new cloudiness

00:05:05,560 --> 00:05:12,880
which are emerging in the operators

00:05:09,300 --> 00:05:14,620
infrastructure and these are the

00:05:12,880 --> 00:05:16,479
configurations which we see typically

00:05:14,620 --> 00:05:19,389
are not like massive cloud services

00:05:16,479 --> 00:05:20,979
based on which side we are going to if

00:05:19,389 --> 00:05:24,639
it's a zip code level it could be like

00:05:20,979 --> 00:05:28,139
some bunch of racks sitting there

00:05:24,639 --> 00:05:32,169
we broke the model of bringing devices

00:05:28,139 --> 00:05:34,360
by having a standard SDK and also being

00:05:32,169 --> 00:05:36,490
part of subscribe device makers

00:05:34,360 --> 00:05:38,470
enablement layers and then we also

00:05:36,490 --> 00:05:41,139
provide the standard clouds consumption

00:05:38,470 --> 00:05:43,360
model to people like me and take or some

00:05:41,139 --> 00:05:48,250
other application companies to come and

00:05:43,360 --> 00:05:50,620
onboard us so these are some of the key

00:05:48,250 --> 00:05:52,930
features we actually have and I'll show

00:05:50,620 --> 00:05:55,360
you how it actually works but some of

00:05:52,930 --> 00:05:57,400
the cases-- which we are looking at and

00:05:55,360 --> 00:05:57,760
one of one of few of the cases we are

00:05:57,400 --> 00:06:01,020
solved

00:05:57,760 --> 00:06:05,380
right now from mobile gaming and

00:06:01,020 --> 00:06:07,990
augmented reality games but that's in

00:06:05,380 --> 00:06:11,170
the data thinning problem is like I have

00:06:07,990 --> 00:06:13,750
lots of data coming out of IOT devices

00:06:11,170 --> 00:06:15,370
these are like short packets people on

00:06:13,750 --> 00:06:16,870
these machines just send it across it's

00:06:15,370 --> 00:06:18,760
very expensive to run it over the

00:06:16,870 --> 00:06:20,530
operators network you could actually

00:06:18,760 --> 00:06:22,150
aggregate them and send a combined

00:06:20,530 --> 00:06:25,030
report back to the cloud so that's the

00:06:22,150 --> 00:06:27,400
data the problem which we want to solve

00:06:25,030 --> 00:06:30,370
and there's a bunch of things like don't

00:06:27,400 --> 00:06:32,020
drone swarming of course there's IOT

00:06:30,370 --> 00:06:36,610
that a lot of players in IOT who want to

00:06:32,020 --> 00:06:39,400
solve you know localized IOT spaces like

00:06:36,610 --> 00:06:41,110
industrial setting how machines that

00:06:39,400 --> 00:06:42,850
communicate to each other but they don't

00:06:41,110 --> 00:06:44,230
really necessarily need to go to an

00:06:42,850 --> 00:06:47,230
operator ecosystem they couldn't run

00:06:44,230 --> 00:06:48,370
their network there so yeah these are

00:06:47,230 --> 00:06:50,800
some of the applications we are looking

00:06:48,370 --> 00:06:55,240
at and some of them are deployed but let

00:06:50,800 --> 00:06:59,620
me dive into how does it work the

00:06:55,240 --> 00:07:02,050
somebody like game company will come and

00:06:59,620 --> 00:07:04,150
actually embed this decay in their

00:07:02,050 --> 00:07:10,320
application this could be classes this

00:07:04,150 --> 00:07:13,240
could be devices and what we look at is

00:07:10,320 --> 00:07:15,430
if the edge applicant is the application

00:07:13,240 --> 00:07:17,890
is offloaded to the edge then we have

00:07:15,430 --> 00:07:20,760
something called matching engine which

00:07:17,890 --> 00:07:24,460
is let me see if I can point that yet

00:07:20,760 --> 00:07:26,650
which will actually look at the your GPS

00:07:24,460 --> 00:07:31,090
coordinates coming out and point you to

00:07:26,650 --> 00:07:33,250
the nearest edge and then we have

00:07:31,090 --> 00:07:36,700
sophisticated mechanisms to increase the

00:07:33,250 --> 00:07:39,370
the cluster size reduce it based on the

00:07:36,700 --> 00:07:41,410
stress it is very likely that if some

00:07:39,370 --> 00:07:44,590
site is collapsing then we might

00:07:41,410 --> 00:07:46,300
actually route you to a better site the

00:07:44,590 --> 00:07:49,020
default route is always go back to the

00:07:46,300 --> 00:07:50,140
cloud but there is this layers of

00:07:49,020 --> 00:07:52,600
cloudlets

00:07:50,140 --> 00:07:54,670
within the operatory system that if you

00:07:52,600 --> 00:07:57,760
miss the zip code you can go to regional

00:07:54,670 --> 00:08:00,070
level and then to the cloud by the way a

00:07:57,760 --> 00:08:02,830
lot of latency x' across this entire

00:08:00,070 --> 00:08:04,570
chain if I look at it can be saved by

00:08:02,830 --> 00:08:06,380
doing what we are doing but then there

00:08:04,570 --> 00:08:09,200
is ran part two we will talk about

00:08:06,380 --> 00:08:11,360
a little bit later how we see it it's

00:08:09,200 --> 00:08:12,860
evolving but yeah it's actually

00:08:11,360 --> 00:08:14,600
something which is deployed and

00:08:12,860 --> 00:08:18,890
osterlich theorem and so this is how

00:08:14,600 --> 00:08:24,410
typically these things work we use cloud

00:08:18,890 --> 00:08:25,970
fair so that's I have a co-panelists now

00:08:24,410 --> 00:08:28,010
so I can attest to that so but

00:08:25,970 --> 00:08:33,200
essentially yeah when you actually try

00:08:28,010 --> 00:08:35,810
to access some URI like dot Telecom next

00:08:33,200 --> 00:08:38,479
artnet is gonna resolve and give you a

00:08:35,810 --> 00:08:41,750
public IP this public IP will be

00:08:38,479 --> 00:08:43,640
isolated into a particular DME which

00:08:41,750 --> 00:08:45,440
will be running in particular city so

00:08:43,640 --> 00:08:48,320
this is actual cloudlet and bond which

00:08:45,440 --> 00:08:50,510
actually gets this and all the HTTP

00:08:48,320 --> 00:08:52,130
requests are offered the similar

00:08:50,510 --> 00:08:56,320
mechanisms exist across different

00:08:52,130 --> 00:08:56,320
operators like telefÃ³nica and barcelona

00:08:57,670 --> 00:09:06,490
okay so this is the stack which is

00:09:00,980 --> 00:09:09,490
deployed today nothing more inside

00:09:06,490 --> 00:09:11,390
exciting about it but this is how we see

00:09:09,490 --> 00:09:13,100
typically when we engage with the

00:09:11,390 --> 00:09:16,190
operator how they are layered layered

00:09:13,100 --> 00:09:18,710
their infrastructure we see OpenStack as

00:09:16,190 --> 00:09:21,440
a common common preference across the

00:09:18,710 --> 00:09:22,970
board it's not necessary that we have to

00:09:21,440 --> 00:09:25,520
run our own virtual machines but we

00:09:22,970 --> 00:09:26,810
could we just collect this inventory we

00:09:25,520 --> 00:09:29,320
figure out how much you know and things

00:09:26,810 --> 00:09:33,290
they have and we run the workload on it

00:09:29,320 --> 00:09:34,940
we'll laugh we are working on some of

00:09:33,290 --> 00:09:39,110
the partners to actually enable Behrman

00:09:34,940 --> 00:09:42,760
rules which will be I think which will

00:09:39,110 --> 00:09:45,050
be more resource effective for us and

00:09:42,760 --> 00:09:47,300
these are our components so essentially

00:09:45,050 --> 00:09:50,270
what happens is when you onboard us

00:09:47,300 --> 00:09:51,980
today we just make it really simple the

00:09:50,270 --> 00:09:54,350
people are used to cloud environment and

00:09:51,980 --> 00:09:56,870
given it is is one of the favorite

00:09:54,350 --> 00:09:59,630
orchestration schemes they have so we

00:09:56,870 --> 00:10:04,550
spawn a cluster for a particular client

00:09:59,630 --> 00:10:06,980
and you learned about the DME which is

00:10:04,550 --> 00:10:10,460
the stateless matching engine which

00:10:06,980 --> 00:10:13,070
actually points to the edge site and the

00:10:10,460 --> 00:10:15,630
content will be served out of edge the

00:10:13,070 --> 00:10:19,170
CRM is the smart guy who's actually

00:10:15,630 --> 00:10:20,910
getting out how much to skate beware and

00:10:19,170 --> 00:10:23,940
there are some some of the things which

00:10:20,910 --> 00:10:26,490
by nature of running within a operator

00:10:23,940 --> 00:10:27,780
you get free like there's a cellular

00:10:26,490 --> 00:10:30,360
control plane we can actually verify

00:10:27,780 --> 00:10:30,810
that entity we can figure out where you

00:10:30,360 --> 00:10:34,050
are

00:10:30,810 --> 00:10:36,810
I'm just based on how this tower you are

00:10:34,050 --> 00:10:38,910
connected to and then we have specific

00:10:36,810 --> 00:10:42,420
controller policies which run across

00:10:38,910 --> 00:10:44,490
countries which we enforce let's say if

00:10:42,420 --> 00:10:48,780
it's Europe then there are some gdpr

00:10:44,490 --> 00:10:51,000
regulations we cannot the bunch of stuff

00:10:48,780 --> 00:10:54,990
which we could which we don't allow to

00:10:51,000 --> 00:10:56,280
share are but yet these mobile logics

00:10:54,990 --> 00:10:58,290
controllers which are sitting on the

00:10:56,280 --> 00:11:02,240
public cloud are mechanism for you to

00:10:58,290 --> 00:11:02,240
choose where you want to go in the world

00:11:02,810 --> 00:11:11,550
so typically how location works is that

00:11:07,200 --> 00:11:14,490
you connect to the mobile network we

00:11:11,550 --> 00:11:17,850
were just to the client we actually

00:11:14,490 --> 00:11:20,250
verify the identity then DMA also does

00:11:17,850 --> 00:11:24,300
the location verification and we

00:11:20,250 --> 00:11:26,730
actually validate it if there is an

00:11:24,300 --> 00:11:28,890
event where and this is was the this was

00:11:26,730 --> 00:11:30,960
a driver for people like me antic who

00:11:28,890 --> 00:11:33,900
who were losing money because people

00:11:30,960 --> 00:11:36,720
were actually spoofing locations do it

00:11:33,900 --> 00:11:39,590
even or some 10 12 million dollars so

00:11:36,720 --> 00:11:42,450
there this allows them to actually

00:11:39,590 --> 00:11:44,480
protect themselves by using this

00:11:42,450 --> 00:11:48,260
location information from the operator

00:11:44,480 --> 00:11:50,430
so in case if it's a if you actually

00:11:48,260 --> 00:11:52,290
spoofed the location and if you're not

00:11:50,430 --> 00:11:53,930
in that zone in that cellular tower

00:11:52,290 --> 00:11:57,480
nearby that salutare they'll actually

00:11:53,930 --> 00:12:01,950
respond that that's not where you are so

00:11:57,480 --> 00:12:06,330
yeah it helps and then what we have done

00:12:01,950 --> 00:12:10,620
typically is that when we see in the

00:12:06,330 --> 00:12:11,850
operators owns some of the the same

00:12:10,620 --> 00:12:14,730
technologies which are being used in

00:12:11,850 --> 00:12:17,430
cloud we are porting today on the cloud

00:12:14,730 --> 00:12:18,740
that doesn't mean the the future

00:12:17,430 --> 00:12:22,250
landscape will look

00:12:18,740 --> 00:12:24,080
look like the same I think the way cloud

00:12:22,250 --> 00:12:28,450
service providers do the business is

00:12:24,080 --> 00:12:34,130
based on charging you for a certain CPU

00:12:28,450 --> 00:12:37,190
IO storage the idea is that it's more

00:12:34,130 --> 00:12:39,200
usable not performance oriented our form

00:12:37,190 --> 00:12:42,290
factors a little different we cannot

00:12:39,200 --> 00:12:46,010
have massive data center in Midwest to

00:12:42,290 --> 00:12:47,570
run this at cost we actually look at a

00:12:46,010 --> 00:12:49,070
lot of performance things in

00:12:47,570 --> 00:12:53,210
improvements we want to do across these

00:12:49,070 --> 00:12:55,190
frameworks but quite simply I think what

00:12:53,210 --> 00:12:56,870
is happening is that the form factor

00:12:55,190 --> 00:12:58,240
even the clients are bringing their

00:12:56,870 --> 00:13:01,430
applications is this pretty lightweight

00:12:58,240 --> 00:13:04,070
and they selectively choose the workload

00:13:01,430 --> 00:13:06,980
they want to bring to edge and this is

00:13:04,070 --> 00:13:08,600
what we see that people have bunch of

00:13:06,980 --> 00:13:10,040
micro services running in the cloud and

00:13:08,600 --> 00:13:13,840
they bring certain things which they

00:13:10,040 --> 00:13:16,610
want to offload and we conveniently

00:13:13,840 --> 00:13:21,050
offload them and give them the latency

00:13:16,610 --> 00:13:23,900
cap sticks they need so quite pretty

00:13:21,050 --> 00:13:27,890
simple there so the big question I think

00:13:23,900 --> 00:13:32,450
the panel was asking is why why should

00:13:27,890 --> 00:13:33,950
we invest in Linux and the the model we

00:13:32,450 --> 00:13:36,830
are seeing which is emerging in the

00:13:33,950 --> 00:13:38,690
industry today is essentially you look

00:13:36,830 --> 00:13:41,020
across all cloud service providers they

00:13:38,690 --> 00:13:45,280
are actually building their it strategy

00:13:41,020 --> 00:13:48,410
they're putting their caches regions and

00:13:45,280 --> 00:13:51,500
it's gonna naturally be an environment

00:13:48,410 --> 00:13:54,020
like when you go into Phi G people will

00:13:51,500 --> 00:13:55,340
have Network slices they'll buy those

00:13:54,020 --> 00:13:59,060
slices from the operators and they'll

00:13:55,340 --> 00:14:01,310
get and put that near zip codes or near

00:13:59,060 --> 00:14:03,500
regions but the business model is

00:14:01,310 --> 00:14:06,020
basically based on taking that workload

00:14:03,500 --> 00:14:08,210
and offloading to the backend which is

00:14:06,020 --> 00:14:11,000
the cloud because that's the only way

00:14:08,210 --> 00:14:13,580
the clouds could actually service that

00:14:11,000 --> 00:14:17,030
kind of traffic and optimally a lot of

00:14:13,580 --> 00:14:18,530
investment has been there what our view

00:14:17,030 --> 00:14:21,250
of looking at this problem is that if

00:14:18,530 --> 00:14:23,480
you look at specific sets of workloads

00:14:21,250 --> 00:14:25,550
we want to serve them at the real edge

00:14:23,480 --> 00:14:29,390
we don't want to take these transactions

00:14:25,550 --> 00:14:32,310
off to the cloud and that's what we see

00:14:29,390 --> 00:14:34,800
because if if you're playing an ER game

00:14:32,310 --> 00:14:36,120
the latency is more than 25 milliseconds

00:14:34,800 --> 00:14:39,690
you might actually have an headache

00:14:36,120 --> 00:14:41,460
playing that game so these are these are

00:14:39,690 --> 00:14:42,600
interesting models because all the

00:14:41,460 --> 00:14:44,700
gaming industry might not look like

00:14:42,600 --> 00:14:46,890
mission-critical but the characteristics

00:14:44,700 --> 00:14:49,730
which we see on a game is pretty much

00:14:46,890 --> 00:14:52,470
same how we will see autonomous vehicles

00:14:49,730 --> 00:14:56,250
other industries develop it which will

00:14:52,470 --> 00:14:58,020
have similar demands so what what is our

00:14:56,250 --> 00:15:01,650
goal here I think what we are looking at

00:14:58,020 --> 00:15:04,230
is not how the H caches are being

00:15:01,650 --> 00:15:05,940
developed today by peering but we are

00:15:04,230 --> 00:15:09,150
actually looking at a way to distribute

00:15:05,940 --> 00:15:12,810
these applications across the world so

00:15:09,150 --> 00:15:15,990
that you know there's there are the edge

00:15:12,810 --> 00:15:19,279
workloads are optimally executed and

00:15:15,990 --> 00:15:21,690
also there's paths for everybody to go

00:15:19,279 --> 00:15:23,279
where they want to go to zip code level

00:15:21,690 --> 00:15:25,770
they want to go to region level they

00:15:23,279 --> 00:15:27,390
want to go to the cloud that that sort

00:15:25,770 --> 00:15:30,600
of distributed nature of this network

00:15:27,390 --> 00:15:32,610
will allow it to scale across the

00:15:30,600 --> 00:15:34,680
operator so it doesn't have to be that

00:15:32,610 --> 00:15:35,790
just because you're not let's say we're

00:15:34,680 --> 00:15:38,190
eyes and subscribe or you won't get

00:15:35,790 --> 00:15:42,300
access to the service you probably will

00:15:38,190 --> 00:15:47,280
get benefits of it so where do we see

00:15:42,300 --> 00:15:49,440
really value in in Linux I think for us

00:15:47,280 --> 00:15:53,160
the driver is that there has to be

00:15:49,440 --> 00:15:55,080
programmability inbuilt programmability

00:15:53,160 --> 00:15:57,810
inbuilt in how packets move in our

00:15:55,080 --> 00:16:00,290
fabric programmable into programming the

00:15:57,810 --> 00:16:05,070
end devices we are very interested in

00:16:00,290 --> 00:16:07,050
accelerations we don't see people at

00:16:05,070 --> 00:16:11,670
least the infrastructure providers

00:16:07,050 --> 00:16:13,980
having or buying smart necks or using

00:16:11,670 --> 00:16:16,260
more sophisticated chips what we do see

00:16:13,980 --> 00:16:18,920
is in record where racks and racks of

00:16:16,260 --> 00:16:20,790
those line they're so running

00:16:18,920 --> 00:16:23,970
high-performance stuff on a commodity

00:16:20,790 --> 00:16:28,200
hardware is our drive we have a bigger

00:16:23,970 --> 00:16:30,060
problem which is mobility because the

00:16:28,200 --> 00:16:32,510
way the sessions are anchored today they

00:16:30,060 --> 00:16:35,640
are getting routed through P gateway

00:16:32,510 --> 00:16:38,310
generally the way it used to work was

00:16:35,640 --> 00:16:40,020
lightly the people were just connecting

00:16:38,310 --> 00:16:43,380
too intimate so that's how they layered

00:16:40,020 --> 00:16:44,540
their network the mobile operators but

00:16:43,380 --> 00:16:47,150
now as we move

00:16:44,540 --> 00:16:50,600
towards the edge that scheme of anchored

00:16:47,150 --> 00:16:52,790
networks will not work so there was a

00:16:50,600 --> 00:16:54,590
talk I think about isle NP so that's a

00:16:52,790 --> 00:16:57,350
good effort but I think from a research

00:16:54,590 --> 00:16:59,750
point to actually a demo to a production

00:16:57,350 --> 00:17:01,520
it takes time but we are seriously

00:16:59,750 --> 00:17:04,880
looking at if we could solve mobility

00:17:01,520 --> 00:17:08,990
problems upfront then there are other

00:17:04,880 --> 00:17:10,520
aspects of traffic slicing for certain

00:17:08,990 --> 00:17:13,550
types of traffic so that we have better

00:17:10,520 --> 00:17:15,170
control on which package should land on

00:17:13,550 --> 00:17:18,440
which application and what should be the

00:17:15,170 --> 00:17:20,390
the path to it and the inverted security

00:17:18,440 --> 00:17:25,220
I think it's becoming more and more

00:17:20,390 --> 00:17:28,100
clear we you rarely see mobile networks

00:17:25,220 --> 00:17:33,620
getting hacked but you do see often

00:17:28,100 --> 00:17:34,820
attacks on cloud the internet is exposed

00:17:33,620 --> 00:17:37,970
in that matter maybe because of the

00:17:34,820 --> 00:17:39,770
nature day it is so popular but I think

00:17:37,970 --> 00:17:41,210
embedded security within the devices

00:17:39,770 --> 00:17:44,900
within the West layers is what we are

00:17:41,210 --> 00:17:46,540
looking to achieve hopefully with these

00:17:44,900 --> 00:17:49,040
things

00:17:46,540 --> 00:17:50,990
okay so programmability as we look at it

00:17:49,040 --> 00:17:53,450
is across the virtualization

00:17:50,990 --> 00:17:57,560
infrastructure what we see it's a common

00:17:53,450 --> 00:18:01,670
denominator we look at this problem as

00:17:57,560 --> 00:18:02,930
saying okay can we program a host the

00:18:01,670 --> 00:18:06,680
same way I want to program a container

00:18:02,930 --> 00:18:09,770
or maybe a switch the form factor of

00:18:06,680 --> 00:18:12,350
switch and server is not much different

00:18:09,770 --> 00:18:13,610
today I mean if I could basically take

00:18:12,350 --> 00:18:16,490
it generically

00:18:13,610 --> 00:18:19,460
server and convert it into a switch I

00:18:16,490 --> 00:18:21,350
have all the software features in the

00:18:19,460 --> 00:18:23,270
kernel to actually make it happen but

00:18:21,350 --> 00:18:25,790
yeah Asics will serve a purpose for me

00:18:23,270 --> 00:18:27,830
so I'll use that whenever I have that

00:18:25,790 --> 00:18:29,960
availability and then when tray so based

00:18:27,830 --> 00:18:31,490
on these combinations of applications do

00:18:29,960 --> 00:18:34,040
I new to accelerate something do I need

00:18:31,490 --> 00:18:35,480
to do something in an ASIC that

00:18:34,040 --> 00:18:41,060
programmability should be inbuilt into

00:18:35,480 --> 00:18:42,290
smaller today I haven't seen that there

00:18:41,060 --> 00:18:44,200
are a bunch of people who are solving

00:18:42,290 --> 00:18:47,150
this problem on the SDM space but I

00:18:44,200 --> 00:18:50,150
think a uniform way to program these

00:18:47,150 --> 00:18:52,190
things is what we we look for and we'll

00:18:50,150 --> 00:18:53,200
make some movements in this area as we

00:18:52,190 --> 00:18:54,240
go along

00:18:53,200 --> 00:18:56,860
will contribute back to the community

00:18:54,240 --> 00:19:00,340
but that's what our way of programming

00:18:56,860 --> 00:19:02,490
but it would be and we definitely see

00:19:00,340 --> 00:19:05,889
that we need to improve the mobility

00:19:02,490 --> 00:19:07,929
problem because as the networks are

00:19:05,889 --> 00:19:10,539
evolving right now the the range for a

00:19:07,929 --> 00:19:14,080
handoff is like five miles as we go into

00:19:10,539 --> 00:19:17,679
Phi G it'll be 0.5 miles so rapid

00:19:14,080 --> 00:19:20,529
handoffs happening and so you see the

00:19:17,679 --> 00:19:23,889
degradation and the general protocols

00:19:20,529 --> 00:19:26,649
which will work on LD will start failing

00:19:23,889 --> 00:19:28,690
so we need a way that it should be

00:19:26,649 --> 00:19:31,720
stateless it should be efficient it

00:19:28,690 --> 00:19:33,970
should be resilient to attacks and it

00:19:31,720 --> 00:19:35,830
should be scalable to billions of

00:19:33,970 --> 00:19:39,789
devices and that's what the market is

00:19:35,830 --> 00:19:42,130
evolving to be this is just a standard

00:19:39,789 --> 00:19:46,240
programmable pipeline which we generally

00:19:42,130 --> 00:19:50,139
want to energize so we do see value in

00:19:46,240 --> 00:19:52,870
doing stuff like okay can I actually do

00:19:50,139 --> 00:19:55,950
load balancing on top of rocks watch and

00:19:52,870 --> 00:19:58,750
if I have to do it how do I go about it

00:19:55,950 --> 00:20:01,149
we also see value in not running

00:19:58,750 --> 00:20:03,220
communities natively because the packets

00:20:01,149 --> 00:20:05,049
are just running across the fabric from

00:20:03,220 --> 00:20:07,269
a cube router to an overlay to a

00:20:05,049 --> 00:20:10,330
container we want to hit a container

00:20:07,269 --> 00:20:12,669
right - right away from a tar and get

00:20:10,330 --> 00:20:14,649
the response back our idea is not to

00:20:12,669 --> 00:20:16,330
overwhelm our fabric with lots of

00:20:14,649 --> 00:20:18,610
packets running around in a micro

00:20:16,330 --> 00:20:20,700
service more so if you look at it it's

00:20:18,610 --> 00:20:23,230
actually going in opposite direction

00:20:20,700 --> 00:20:24,820
people move to micro services model now

00:20:23,230 --> 00:20:28,090
we are actually trying to bring them

00:20:24,820 --> 00:20:30,389
back to say can we run it's okay to run

00:20:28,090 --> 00:20:32,799
stateful application but can we run like

00:20:30,389 --> 00:20:34,899
lightweight containers and we could hit

00:20:32,799 --> 00:20:37,000
your containers and get you the latency

00:20:34,899 --> 00:20:39,549
demands you have instead of wasting my

00:20:37,000 --> 00:20:42,909
fabric capacity so bunch of these things

00:20:39,549 --> 00:20:45,130
are evolving this is our view of how our

00:20:42,909 --> 00:20:47,110
pipeline should look like should be

00:20:45,130 --> 00:20:48,970
controlled centrally we should be able

00:20:47,110 --> 00:20:51,070
to locate resources we should be able to

00:20:48,970 --> 00:20:53,769
find inventories or devices we should be

00:20:51,070 --> 00:20:57,159
knowing where to host a particular

00:20:53,769 --> 00:21:01,419
workflow that'll allow us to move into

00:20:57,159 --> 00:21:04,059
the the real edge latency sensitive

00:21:01,419 --> 00:21:06,299
environments this is one other effort

00:21:04,059 --> 00:21:07,919
which we think is critical which is

00:21:06,299 --> 00:21:09,839
when I often talk to mobile network

00:21:07,919 --> 00:21:11,580
operators they say well okay I will

00:21:09,839 --> 00:21:13,889
you'll solve my problem from Melissa kin

00:21:11,580 --> 00:21:15,839
to microsecond or you'll feed the

00:21:13,889 --> 00:21:16,950
packets but the real problem is on the

00:21:15,839 --> 00:21:21,869
ran side which is actually the

00:21:16,950 --> 00:21:23,099
consumption where the actual latency and

00:21:21,869 --> 00:21:24,539
there are some efforts I think I haven't

00:21:23,099 --> 00:21:26,669
seen something really critical but

00:21:24,539 --> 00:21:29,309
that's something to let it to Linux but

00:21:26,669 --> 00:21:32,359
what I want to emphasize is that unless

00:21:29,309 --> 00:21:35,609
we solve the end-to-end latency problem

00:21:32,359 --> 00:21:38,359
you won't have the next generation of

00:21:35,609 --> 00:21:40,139
Internet what we want to actually build

00:21:38,359 --> 00:21:42,329
so we're going to be working with

00:21:40,139 --> 00:21:46,139
certain people to actually make these

00:21:42,329 --> 00:21:51,299
things they're real it can control the

00:21:46,139 --> 00:21:52,409
ran site scheduling yeah but this is

00:21:51,299 --> 00:21:56,489
this is our way of looking at this

00:21:52,409 --> 00:22:02,389
problem this is programmable slices any

00:21:56,489 --> 00:22:02,389
questions we can wait further yeah okay

00:22:06,889 --> 00:22:11,450
all right so after I think your next

00:22:27,850 --> 00:22:36,170
okay works so first of all I want to

00:22:33,830 --> 00:22:39,590
apologize first as as a result of this

00:22:36,170 --> 00:22:41,870
late last-second cancellation I had

00:22:39,590 --> 00:22:45,350
almost no time to prepare the offense

00:22:41,870 --> 00:22:47,570
fancy slides others have so these are

00:22:45,350 --> 00:22:50,690
more about talking points that I'm going

00:22:47,570 --> 00:22:53,540
to touch on also at the same time I'm

00:22:50,690 --> 00:22:55,190
planning to talk more about the data

00:22:53,540 --> 00:22:58,250
center perspective from the perspective

00:22:55,190 --> 00:23:03,410
of a large content provider and slightly

00:22:58,250 --> 00:23:06,080
more on a his system level and a little

00:23:03,410 --> 00:23:09,350
bit about the potential operational

00:23:06,080 --> 00:23:13,910
challenges we we have we have and we

00:23:09,350 --> 00:23:16,010
still have so first of all to build the

00:23:13,910 --> 00:23:18,170
context to you slightly understand that

00:23:16,010 --> 00:23:22,660
the size of the problem we are dealing

00:23:18,170 --> 00:23:25,970
with so as of now linkedin has roughly

00:23:22,660 --> 00:23:31,070
250 K of machines he bare metal key

00:23:25,970 --> 00:23:34,160
servers we are with with the network or

00:23:31,070 --> 00:23:38,120
with our servers in about 20 locations

00:23:34,160 --> 00:23:44,540
globally and we peer with roughly 4000

00:23:38,120 --> 00:23:47,540
of not the networks so the his size of

00:23:44,540 --> 00:23:50,990
of the network is it's one thing so

00:23:47,540 --> 00:23:53,809
let's say it would be easy to to

00:23:50,990 --> 00:23:57,559
eventually build a network but we still

00:23:53,809 --> 00:24:02,179
need to play the catch-up game so once

00:23:57,559 --> 00:24:04,309
once we get to the the size of the

00:24:02,179 --> 00:24:07,670
infrastructure we had a month or a

00:24:04,309 --> 00:24:13,100
quarter ago the demands are bigger and

00:24:07,670 --> 00:24:16,790
bigger so we roughly have 30 to 35

00:24:13,100 --> 00:24:19,520
percent of growth every year and the

00:24:16,790 --> 00:24:23,929
demands from the bandwidth they mostly

00:24:19,520 --> 00:24:29,390
come from the demands from the compute

00:24:23,929 --> 00:24:31,970
so as the days of the Moore's Law they

00:24:29,390 --> 00:24:34,760
are effectively over so we expect this

00:24:31,970 --> 00:24:36,810
to grow again faster and faster us to

00:24:34,760 --> 00:24:39,270
tackle bigger problems

00:24:36,810 --> 00:24:41,090
from the application perspective we will

00:24:39,270 --> 00:24:43,680
expect to see more and more

00:24:41,090 --> 00:24:46,890
communications between the hidden

00:24:43,680 --> 00:24:50,460
machines and our our network needs to

00:24:46,890 --> 00:24:53,430
handle that so the the organic growth is

00:24:50,460 --> 00:24:57,060
one thing so it's pretty easy having the

00:24:53,430 --> 00:25:00,920
the right models to to forecast the

00:24:57,060 --> 00:25:04,410
capacity the the real problems are with

00:25:00,920 --> 00:25:07,290
with the inorganic growth as it's very

00:25:04,410 --> 00:25:10,170
challenging to negate it right at the

00:25:07,290 --> 00:25:12,840
scale of multiple or hundreds of

00:25:10,170 --> 00:25:15,540
applications we run so even a single

00:25:12,840 --> 00:25:20,310
update key to the application my might

00:25:15,540 --> 00:25:24,350
cause a big traffic spikes and while

00:25:20,310 --> 00:25:28,530
while here from the edge perspective

00:25:24,350 --> 00:25:30,990
every single byte sent sent to us it

00:25:28,530 --> 00:25:33,990
causes explosion internally in the east

00:25:30,990 --> 00:25:37,380
East West via traffic so we roughly

00:25:33,990 --> 00:25:40,620
estimate that every single byte sent to

00:25:37,380 --> 00:25:43,860
us will cause a thousandth of bytes sent

00:25:40,620 --> 00:25:46,440
within the fabric so it includes things

00:25:43,860 --> 00:25:51,050
like the call graph clip or the main

00:25:46,440 --> 00:25:55,860
LinkedIn application or or the website

00:25:51,050 --> 00:25:58,470
the the Kafka we we use for all the of

00:25:55,860 --> 00:26:02,040
the logging and accounting things and

00:25:58,470 --> 00:26:05,190
offline analysis and dealing with things

00:26:02,040 --> 00:26:11,150
like like machine learning in the heat

00:26:05,190 --> 00:26:17,340
training phase but as as we now have

00:26:11,150 --> 00:26:20,550
2019 I guess we haven't always been in

00:26:17,340 --> 00:26:25,290
in that is slightly more comfortable his

00:26:20,550 --> 00:26:27,720
situation that we are right now over

00:26:25,290 --> 00:26:31,170
over the years here starting from from

00:26:27,720 --> 00:26:33,960
year roughly 2010 we had different main

00:26:31,170 --> 00:26:37,710
challenges or different priorities were

00:26:33,960 --> 00:26:41,850
in the best our resources so first of

00:26:37,710 --> 00:26:46,350
all around around year 2010 our main

00:26:41,850 --> 00:26:48,600
main job was to keep the cycle as the

00:26:46,350 --> 00:26:50,580
side the size of the infrastructural

00:26:48,600 --> 00:26:55,230
size of the network it was growing very

00:26:50,580 --> 00:26:58,500
rapidly and at that time it was growing

00:26:55,230 --> 00:27:01,289
faster that we had experience with both

00:26:58,500 --> 00:27:06,600
at the organizational level and the heat

00:27:01,289 --> 00:27:08,820
tuning level so once this this thing was

00:27:06,600 --> 00:27:11,220
solved and we can never get from the

00:27:08,820 --> 00:27:14,580
operational overload and and we could

00:27:11,220 --> 00:27:17,400
spend some more time on the design of

00:27:14,580 --> 00:27:20,850
things to make sure it doesn't happen

00:27:17,400 --> 00:27:25,830
again we experienced event few bigger

00:27:20,850 --> 00:27:28,789
growth so here then we we saw that the

00:27:25,830 --> 00:27:32,610
limits of the the current network design

00:27:28,789 --> 00:27:36,270
as the the services the applications

00:27:32,610 --> 00:27:40,080
they they wanted to scale scale faster

00:27:36,270 --> 00:27:44,900
and faster and we had to come up with a

00:27:40,080 --> 00:27:51,299
way to be able to deliver that capacity

00:27:44,900 --> 00:27:54,750
so eventually once once we designed the

00:27:51,299 --> 00:27:56,970
new architecture of our data centers we

00:27:54,750 --> 00:28:02,429
had to focus on innovation for the hyper

00:27:56,970 --> 00:28:05,850
scale as we call it so the unlimited

00:28:02,429 --> 00:28:09,240
bandwidth this is like a big bait here

00:28:05,850 --> 00:28:11,850
as obviously no we cannot talk about the

00:28:09,240 --> 00:28:13,940
unlimited bandwidth but sometimes from

00:28:11,850 --> 00:28:17,520
the perspective of low bandwidth

00:28:13,940 --> 00:28:20,809
application working in a in a data

00:28:17,520 --> 00:28:23,970
center do with a lot of bandwidth we can

00:28:20,809 --> 00:28:27,780
provide that kind of illusion or an

00:28:23,970 --> 00:28:30,270
abstraction layer then at the hyper

00:28:27,780 --> 00:28:35,880
scale we also need computed for demand

00:28:30,270 --> 00:28:39,330
as we see spikes in the usage so it's

00:28:35,880 --> 00:28:42,030
it's easy to provide that that compute

00:28:39,330 --> 00:28:47,309
and he for the network to be able to

00:28:42,030 --> 00:28:49,980
handle this so and then the important

00:28:47,309 --> 00:28:52,950
stuff is the data center as a whole you

00:28:49,980 --> 00:28:55,350
should be programmable so once we build

00:28:52,950 --> 00:28:57,390
the the hard word and once we build that

00:28:55,350 --> 00:28:58,730
infrastructure we don't want to deal

00:28:57,390 --> 00:29:01,789
with

00:28:58,730 --> 00:29:04,220
the physical stuff anymore we want to

00:29:01,789 --> 00:29:07,460
change as much as we can do with the

00:29:04,220 --> 00:29:11,779
software and you pretty much as as quick

00:29:07,460 --> 00:29:14,809
as we can and all these things are easy

00:29:11,779 --> 00:29:20,119
assuming you you have unlimited amount

00:29:14,809 --> 00:29:23,389
of cash but being able to scale the cost

00:29:20,119 --> 00:29:26,359
effectively and not even scale the costs

00:29:23,389 --> 00:29:31,340
but scale scale the number of people we

00:29:26,359 --> 00:29:33,619
had as people scale it terribly that

00:29:31,340 --> 00:29:36,529
thought that was the biggest challenge

00:29:33,619 --> 00:29:40,549
to get to make sure that our key growth

00:29:36,529 --> 00:29:44,299
is is efficient in energy very probably

00:29:40,549 --> 00:29:47,419
return so it brought us to the concept

00:29:44,299 --> 00:29:50,659
of owning owning the code and it's it's

00:29:47,419 --> 00:29:54,109
likely also the the answer to the

00:29:50,659 --> 00:29:56,779
question why why we use Linux the high

00:29:54,109 --> 00:29:59,210
level phrase is that it enables us to

00:29:56,779 --> 00:30:02,570
control our own destiny but what it

00:29:59,210 --> 00:30:06,109
means in in real life so actually to me

00:30:02,570 --> 00:30:11,049
it means that we can get much higher

00:30:06,109 --> 00:30:11,049
velocity owning owning the software so

00:30:11,139 --> 00:30:17,690
with with higher velocity we can roll

00:30:14,779 --> 00:30:20,600
out the new software version every every

00:30:17,690 --> 00:30:23,239
day or never week basically as soon as

00:30:20,600 --> 00:30:26,090
as soon as we want it enables us to

00:30:23,239 --> 00:30:29,929
experiment so for example if the

00:30:26,090 --> 00:30:33,230
experiment fails we can emit immediately

00:30:29,929 --> 00:30:36,230
rolled back and and continue working as

00:30:33,230 --> 00:30:38,619
we as we worked before so the the same

00:30:36,230 --> 00:30:40,970
thing applies to the bug fixes and

00:30:38,619 --> 00:30:43,460
innovation is much easier as we for

00:30:40,970 --> 00:30:46,009
example we want to change some element

00:30:43,460 --> 00:30:52,100
of the system we we don't have to wait

00:30:46,009 --> 00:30:57,320
for a vendor 12 or 18 months to to make

00:30:52,100 --> 00:30:59,210
this happen as having a lack of that

00:30:57,320 --> 00:31:01,940
many months and you're trying to play

00:30:59,210 --> 00:31:03,830
the catch-up game with with the our

00:31:01,940 --> 00:31:07,909
infrastructure growth it just fit

00:31:03,830 --> 00:31:10,309
doesn't doesn't fit well and a few words

00:31:07,909 --> 00:31:12,629
here about our key data center design

00:31:10,309 --> 00:31:15,320
the we use

00:31:12,629 --> 00:31:19,109
five p stage claws or spine and leave

00:31:15,320 --> 00:31:22,139
our control plane protocol is its beauty

00:31:19,109 --> 00:31:27,959
here you asked why bgp at that time that

00:31:22,139 --> 00:31:30,450
was the only only solution that we could

00:31:27,959 --> 00:31:34,379
use taking into account all the all the

00:31:30,450 --> 00:31:39,330
constraints we had and the data center

00:31:34,379 --> 00:31:42,570
is a single kiss q so as a basic

00:31:39,330 --> 00:31:45,719
building block we we use one hardware

00:31:42,570 --> 00:31:48,379
box that is replicated in in all the

00:31:45,719 --> 00:31:55,049
roles spine leaf and and so on so forth

00:31:48,379 --> 00:31:57,239
so it's very important to talk about the

00:31:55,049 --> 00:32:01,739
key design principles we had in mind

00:31:57,239 --> 00:32:05,339
when when he designing the data centers

00:32:01,739 --> 00:32:10,289
so you simply simplicity to me is is the

00:32:05,339 --> 00:32:12,359
key as humans the in general are our key

00:32:10,289 --> 00:32:14,609
terrible at keeping the state of the

00:32:12,359 --> 00:32:20,339
network in mind at the large scale and

00:32:14,609 --> 00:32:23,519
it leads to outages so making sure that

00:32:20,339 --> 00:32:26,899
we have very few exceptions and and the

00:32:23,519 --> 00:32:29,669
infrastructure is very simple plus

00:32:26,899 --> 00:32:31,979
engaging soft words you to make sure

00:32:29,669 --> 00:32:35,249
that this part of the state could be

00:32:31,979 --> 00:32:37,469
kept in software to me it's it's very

00:32:35,249 --> 00:32:40,469
important at the same time that we

00:32:37,469 --> 00:32:43,499
wanted to use open standards or open

00:32:40,469 --> 00:32:47,099
tools when possible obviously it was not

00:32:43,499 --> 00:32:48,899
always always possible from the business

00:32:47,099 --> 00:32:51,329
continuity perspective as well

00:32:48,899 --> 00:32:55,619
being independent so not really tied to

00:32:51,329 --> 00:32:56,279
any single vendor or or even in terms of

00:32:55,619 --> 00:32:59,789
you know

00:32:56,279 --> 00:33:02,909
ODP any single supplier was also

00:32:59,789 --> 00:33:05,789
important thing and i mentioned that you

00:33:02,909 --> 00:33:09,559
program at programmability bit before so

00:33:05,789 --> 00:33:12,749
we just don't want to deal with you with

00:33:09,559 --> 00:33:13,349
with changing the physical data center

00:33:12,749 --> 00:33:16,440
anymore

00:33:13,349 --> 00:33:19,559
we want to model this in in software and

00:33:16,440 --> 00:33:24,269
change this in software

00:33:19,559 --> 00:33:25,350
so essentially single disk you nice nice

00:33:24,269 --> 00:33:28,259
picture

00:33:25,350 --> 00:33:30,120
how you can be visualized it so every

00:33:28,259 --> 00:33:33,240
rock has a hit talk of the rock you

00:33:30,120 --> 00:33:38,070
switch with hundred gigs up links access

00:33:33,240 --> 00:33:40,350
links the top of the rock you switch as

00:33:38,070 --> 00:33:43,169
they did they connect to the aggregation

00:33:40,350 --> 00:33:47,460
his reaches or core which eventually

00:33:43,169 --> 00:33:51,630
connect to a fabric so then the going

00:33:47,460 --> 00:33:54,120
down this path we we can minimize the

00:33:51,630 --> 00:33:56,190
latency or the number number of chips

00:33:54,120 --> 00:34:00,870
that every every packet he needs to

00:33:56,190 --> 00:34:05,159
travel over the network in terms of the

00:34:00,870 --> 00:34:08,819
key building block the data centers the

00:34:05,159 --> 00:34:13,649
the new data center design uses one

00:34:08,819 --> 00:34:17,220
piece which odm so we decided to no

00:34:13,649 --> 00:34:20,069
longer use big chassis switches at the

00:34:17,220 --> 00:34:24,560
expense of having more control planes

00:34:20,069 --> 00:34:27,450
and management place each manage and

00:34:24,560 --> 00:34:31,109
also what's important as we are another

00:34:27,450 --> 00:34:34,139
vendor and we he tried to keep the

00:34:31,109 --> 00:34:37,500
architecture simple we use a very tiny

00:34:34,139 --> 00:34:42,300
tiny set of simple features so no fiber

00:34:37,500 --> 00:34:47,099
channel no key excellent no evpn or or

00:34:42,300 --> 00:34:50,429
things similar so yes so here after

00:34:47,099 --> 00:34:54,050
spending some time on analyzing and

00:34:50,429 --> 00:34:58,619
exploring we ended up using his sonic so

00:34:54,050 --> 00:35:06,060
the sonic is a network operating system

00:34:58,619 --> 00:35:08,910
based on Linux and it was the good

00:35:06,060 --> 00:35:12,270
enough it was not good enough for he for

00:35:08,910 --> 00:35:17,430
our use case as we effectively needed

00:35:12,270 --> 00:35:21,510
only very basic ipv4 and ipv6 routing we

00:35:17,430 --> 00:35:26,430
had to have a BGP and we wanted to have

00:35:21,510 --> 00:35:31,410
an option to modify it as especially I I

00:35:26,430 --> 00:35:33,540
think that the key thing to running the

00:35:31,410 --> 00:35:36,270
network with high availability and high

00:35:33,540 --> 00:35:37,730
user in satisfaction is mostly in the

00:35:36,270 --> 00:35:40,430
management plane

00:35:37,730 --> 00:35:42,020
in the data plane or a control plane now

00:35:40,430 --> 00:35:45,950
assuming that the control plane is done

00:35:42,020 --> 00:35:50,060
roughly roughly right yes I think here

00:35:45,950 --> 00:35:53,300
about the flat networks we don't want to

00:35:50,060 --> 00:35:57,080
build any form of overlay networks no

00:35:53,300 --> 00:36:01,040
middle boxes if if applications want to

00:35:57,080 --> 00:36:03,470
build some form of overlays it's fine

00:36:01,040 --> 00:36:06,520
but the network does not provide that

00:36:03,470 --> 00:36:06,520
that kind of service

00:36:06,609 --> 00:36:13,760
so super super quickly about his

00:36:10,220 --> 00:36:17,420
self-healing as he said okay selfie link

00:36:13,760 --> 00:36:21,050
is a very very good use case for he for

00:36:17,420 --> 00:36:23,210
example things like TCP analytics so the

00:36:21,050 --> 00:36:29,720
gathering the data and then acting on

00:36:23,210 --> 00:36:32,359
the data so first of all he from the he

00:36:29,720 --> 00:36:35,500
sonic perspective we he push all the

00:36:32,359 --> 00:36:40,940
older he data we have to

00:36:35,500 --> 00:36:44,090
Katka and then and then makeevka so

00:36:40,940 --> 00:36:46,550
under key sonic switches we have caca

00:36:44,090 --> 00:36:50,750
agent eventually it gets to the the calf

00:36:46,550 --> 00:36:53,830
car key broker when we were we can use

00:36:50,750 --> 00:36:57,220
the data both for the online and offline

00:36:53,830 --> 00:37:00,430
analysis so he things like a learning

00:36:57,220 --> 00:37:06,080
equality correlations logging for the

00:37:00,430 --> 00:37:09,460
analysis in the future so for the the

00:37:06,080 --> 00:37:12,080
the first step in any any form of

00:37:09,460 --> 00:37:15,740
self-healing is to make sure we can

00:37:12,080 --> 00:37:18,440
detect and isolate the fault in the

00:37:15,740 --> 00:37:21,920
network so we have an active probing

00:37:18,440 --> 00:37:25,100
tool that is installed on every every

00:37:21,920 --> 00:37:29,000
house and send active probes he sends

00:37:25,100 --> 00:37:31,540
data to to Kafka pipeline and it's used

00:37:29,000 --> 00:37:33,940
for for pinpointing the

00:37:31,540 --> 00:37:37,390
the errors in the network at the same

00:37:33,940 --> 00:37:39,760
time we use this for things like SLO

00:37:37,390 --> 00:37:45,130
measurements learning or displaying heat

00:37:39,760 --> 00:37:47,830
map and you find out a final step in the

00:37:45,130 --> 00:37:52,869
self-healing face is how to fix the

00:37:47,830 --> 00:37:55,900
problem or L to remediate them so vast

00:37:52,869 --> 00:37:58,810
majority of the alerts we have a broken

00:37:55,900 --> 00:38:01,660
power supply these are super simple and

00:37:58,810 --> 00:38:04,840
super easy to fix and actually the uncle

00:38:01,660 --> 00:38:10,810
engineer does not have to be involved so

00:38:04,840 --> 00:38:14,290
we use workflow engine that we build

00:38:10,810 --> 00:38:18,160
some rules and and it's simply files

00:38:14,290 --> 00:38:22,720
that ticket to the the technician that

00:38:18,160 --> 00:38:27,310
will eventually replace the faulty

00:38:22,720 --> 00:38:30,580
hardware so and also one thing here the

00:38:27,310 --> 00:38:34,270
very simple rules can get you very far

00:38:30,580 --> 00:38:36,670
in terms of that it's possible

00:38:34,270 --> 00:38:39,730
potentially to use some form of machine

00:38:36,670 --> 00:38:42,150
learning but I haven't seen my use case

00:38:39,730 --> 00:38:42,150
yet

00:38:57,839 --> 00:39:04,599
hello my name is mark and I represent

00:39:00,700 --> 00:39:08,470
culture so I was asked to give a short

00:39:04,599 --> 00:39:11,980
talk on how we use Linux to build

00:39:08,470 --> 00:39:14,289
culture in if you haven't heard about

00:39:11,980 --> 00:39:18,640
culture we are running a global network

00:39:14,289 --> 00:39:21,069
our servers sit between the users like

00:39:18,640 --> 00:39:24,910
the browsing the internet and the actual

00:39:21,069 --> 00:39:28,539
origin is the actual websites our

00:39:24,910 --> 00:39:33,250
network is quite large we are powering

00:39:28,539 --> 00:39:34,750
about 10% of Internet domains before we

00:39:33,250 --> 00:39:37,900
get any further I would like to make an

00:39:34,750 --> 00:39:41,440
explicit distinction here so we run two

00:39:37,900 --> 00:39:43,390
types of data centers we run servers on

00:39:41,440 --> 00:39:46,990
the edge in multiple locations around

00:39:43,390 --> 00:39:49,240
globe and these servers are used to

00:39:46,990 --> 00:39:51,880
actually serve the data but then we also

00:39:49,240 --> 00:39:53,470
have the core data centers which are

00:39:51,880 --> 00:39:56,319
used for offline processing clock

00:39:53,470 --> 00:39:57,789
analysis and things like that in this

00:39:56,319 --> 00:40:02,349
talk I'll speak mostly about the edge

00:39:57,789 --> 00:40:05,170
network that's my area of expertise okay

00:40:02,349 --> 00:40:08,769
so our edges work is quite large our

00:40:05,170 --> 00:40:12,190
servers are in about 170 locations

00:40:08,769 --> 00:40:16,329
around the world and the deployments

00:40:12,190 --> 00:40:19,690
vary from very small and very large and

00:40:16,329 --> 00:40:24,160
what makes our network unique is the use

00:40:19,690 --> 00:40:27,130
of any custom this helps us in number of

00:40:24,160 --> 00:40:29,799
ways so in any cast basically basically

00:40:27,130 --> 00:40:32,440
means that every datacenter advertises

00:40:29,799 --> 00:40:34,119
exactly the same IP ranges so it's

00:40:32,440 --> 00:40:35,440
single IP address is kind of visible all

00:40:34,119 --> 00:40:37,900
around the world even though it's not

00:40:35,440 --> 00:40:41,049
served from a single place so this

00:40:37,900 --> 00:40:42,400
helped with speed because whenever you

00:40:41,049 --> 00:40:44,579
connect to our IP address you're

00:40:42,400 --> 00:40:48,309
guaranteed to connect to the closest

00:40:44,579 --> 00:40:50,859
server to your current location and also

00:40:48,309 --> 00:40:52,900
helps us with the denial of service

00:40:50,859 --> 00:40:54,160
attacks if there's attack against a

00:40:52,900 --> 00:40:56,279
single IP address it's nicely

00:40:54,160 --> 00:41:01,359
distributed over all around the world

00:40:56,279 --> 00:41:03,089
and on the software side we are having

00:41:01,359 --> 00:41:05,190
quite a uniform

00:41:03,089 --> 00:41:07,260
each and every servers almost identical

00:41:05,190 --> 00:41:09,000
on the software side we don't do

00:41:07,260 --> 00:41:11,750
virtualization we don't run containers

00:41:09,000 --> 00:41:14,609
we run our software on on bare metal

00:41:11,750 --> 00:41:15,810
every server has attached thousands and

00:41:14,609 --> 00:41:18,530
thousands like the addresses to it

00:41:15,810 --> 00:41:22,079
that's because we are using any guest

00:41:18,530 --> 00:41:24,420
and furthermore we have quite rich stack

00:41:22,079 --> 00:41:28,230
of applications we are running a full

00:41:24,420 --> 00:41:31,980
stack of HTTP by fine and that does TLS

00:41:28,230 --> 00:41:34,829
termination SDP to quick we also run a

00:41:31,980 --> 00:41:36,960
user's code which we call workers so

00:41:34,829 --> 00:41:39,540
you're able to deploy for encode motor

00:41:36,960 --> 00:41:40,890
servers then we also have DNS

00:41:39,540 --> 00:41:43,829
applications we are running quite a

00:41:40,890 --> 00:41:46,530
large DNS alternative name server we

00:41:43,829 --> 00:41:48,720
also have a resolver project we also get

00:41:46,530 --> 00:41:50,070
a couple of smaller pieces but the the

00:41:48,720 --> 00:41:53,790
message here is that we don't run a

00:41:50,070 --> 00:41:56,190
single piece of code on hardware side

00:41:53,790 --> 00:41:58,770
our servers are quite similar to each

00:41:56,190 --> 00:42:02,369
other and this is a picture from our

00:41:58,770 --> 00:42:04,230
blog post about gen9 servers as you can

00:42:02,369 --> 00:42:07,020
see in single Sassy's we have four

00:42:04,230 --> 00:42:08,670
different x86 machines they are very

00:42:07,020 --> 00:42:10,470
similar they're identical but they are

00:42:08,670 --> 00:42:12,300
like different separate boxes so we can

00:42:10,470 --> 00:42:14,250
replace them if you want the main

00:42:12,300 --> 00:42:16,619
intention here is to pack as much

00:42:14,250 --> 00:42:19,530
capacity as much CPU power in a unit as

00:42:16,619 --> 00:42:21,150
as we can within a single data center we

00:42:19,530 --> 00:42:22,980
usually run only a couple of generations

00:42:21,150 --> 00:42:24,450
of servers so you can generally assume

00:42:22,980 --> 00:42:26,430
that each and every server is quite the

00:42:24,450 --> 00:42:29,400
same and they deliver a reasonably

00:42:26,430 --> 00:42:32,970
similar amount of amount of processing

00:42:29,400 --> 00:42:35,420
power I need to note that we are running

00:42:32,970 --> 00:42:39,450
heavy experiments with using arm servers

00:42:35,420 --> 00:42:41,480
there are strong reasons to use arm so

00:42:39,450 --> 00:42:44,760
we are plotting our software stack to

00:42:41,480 --> 00:42:47,849
use army so if you done any kernel work

00:42:44,760 --> 00:42:49,500
on getting linux ready for arm thank you

00:42:47,849 --> 00:42:53,040
very much we are definitely definitely

00:42:49,500 --> 00:42:55,829
using that and this is one of the

00:42:53,040 --> 00:42:57,569
reasons why Linux is is great we

00:42:55,829 --> 00:42:59,339
probably wouldn't able to be wouldn't be

00:42:57,569 --> 00:43:03,990
able to put off if we used some other

00:42:59,339 --> 00:43:06,119
platform our stock both software and

00:43:03,990 --> 00:43:08,910
hardware spike uniform and this allows

00:43:06,119 --> 00:43:11,520
us to think about traffic and data

00:43:08,910 --> 00:43:14,190
center about service data centers are

00:43:11,520 --> 00:43:16,000
basically shared nothing we have us

00:43:14,190 --> 00:43:17,410
pretty much everyone else

00:43:16,000 --> 00:43:20,200
north-south traffic when northeast

00:43:17,410 --> 00:43:22,240
requests from users office requests

00:43:20,200 --> 00:43:24,490
going back to the actual content to the

00:43:22,240 --> 00:43:26,670
you too big to the origin servers and

00:43:24,490 --> 00:43:30,040
then east-west traffic when the server's

00:43:26,670 --> 00:43:32,260
exchange data between themselves for

00:43:30,040 --> 00:43:33,940
example a single cached file will be on

00:43:32,260 --> 00:43:35,710
a single on a one machine not on many

00:43:33,940 --> 00:43:37,990
machines so if it's requested from under

00:43:35,710 --> 00:43:41,620
machine it there there needs to be a

00:43:37,990 --> 00:43:43,360
quite large east-west traffic and each

00:43:41,620 --> 00:43:46,420
and every server runs exactly the same

00:43:43,360 --> 00:43:48,160
software stack here I pointed to five

00:43:46,420 --> 00:43:50,770
different kind of stages which I'll go

00:43:48,160 --> 00:43:53,320
into more details in a moment but it's

00:43:50,770 --> 00:43:55,750
important to understand that the servers

00:43:53,320 --> 00:43:57,910
are identical so even though each of the

00:43:55,750 --> 00:43:59,860
lip when the request crosses the layers

00:43:57,910 --> 00:44:03,450
it may go to a different server each

00:43:59,860 --> 00:44:03,450
server strong as it runs the full stack

00:44:03,630 --> 00:44:09,610
let me just go through quickly on how do

00:44:07,780 --> 00:44:11,650
we how we actually what do we do with

00:44:09,610 --> 00:44:14,410
the requests what is our application and

00:44:11,650 --> 00:44:17,470
kind of pipeline so first a request goes

00:44:14,410 --> 00:44:20,230
yeah it's routed via ecmp so kind of

00:44:17,470 --> 00:44:23,680
hardware a lot balancing and and it hits

00:44:20,230 --> 00:44:25,540
xdp layer it's deep you layer on the

00:44:23,680 --> 00:44:27,670
lock boxes does we are using it for two

00:44:25,540 --> 00:44:29,770
things first we are using it for Jesus

00:44:27,670 --> 00:44:33,400
mitigation and then we are also using

00:44:29,770 --> 00:44:35,650
xdp for load balancing then advancing

00:44:33,400 --> 00:44:37,300
forward for worse traffic to potentially

00:44:35,650 --> 00:44:39,760
another machine within the same data

00:44:37,300 --> 00:44:41,860
center and it hits normal Linux kernel

00:44:39,760 --> 00:44:43,390
normal Linux iptables

00:44:41,860 --> 00:44:45,310
and normally knocks networking stack and

00:44:43,390 --> 00:44:47,050
it's routed to what we call protocols

00:44:45,310 --> 00:44:49,810
layer you can think about particles

00:44:47,050 --> 00:44:53,710
layer as a place when we terminate TLS

00:44:49,810 --> 00:44:56,590
quick thanks like that from now on the

00:44:53,710 --> 00:44:59,500
request is piped over a unix pipe so

00:44:56,590 --> 00:45:01,570
kind of localhost to an application or

00:44:59,500 --> 00:45:05,170
application brain which we call front

00:45:01,570 --> 00:45:06,820
line it's an engine X installation with

00:45:05,170 --> 00:45:08,830
Lua code and set inside

00:45:06,820 --> 00:45:11,890
it's quite important to understand this

00:45:08,830 --> 00:45:13,690
is IO sorry is a CPU bound this is a

00:45:11,890 --> 00:45:16,810
heavy Lua code where we do all the

00:45:13,690 --> 00:45:18,970
business logic this business logic

00:45:16,810 --> 00:45:20,740
derives the cash key and the request is

00:45:18,970 --> 00:45:23,350
denoted to potentially another machine

00:45:20,740 --> 00:45:26,380
and same data center which actually can

00:45:23,350 --> 00:45:28,030
store the asset on disk and from now on

00:45:26,380 --> 00:45:29,740
we usually return assets

00:45:28,030 --> 00:45:34,240
from disks directly if not we have to

00:45:29,740 --> 00:45:38,050
reach back to the origin I mentioned

00:45:34,240 --> 00:45:42,910
that the XDP layer we we are we are

00:45:38,050 --> 00:45:44,500
doing the litigations and so we want to

00:45:42,910 --> 00:45:46,090
do that we need to do that in front of

00:45:44,500 --> 00:45:48,100
our load balancer because we don't want

00:45:46,090 --> 00:45:52,380
to forward heavy Tito's traffic within

00:45:48,100 --> 00:45:55,900
our our datacenter and this this code is

00:45:52,380 --> 00:45:57,430
is not it basically most dropping

00:45:55,900 --> 00:45:59,800
packets if you want to hear more about

00:45:57,430 --> 00:46:01,780
what it does I think today five o'clock

00:45:59,800 --> 00:46:04,930
and my colleague Artur will speak about

00:46:01,780 --> 00:46:06,280
exactly the component but the other

00:46:04,930 --> 00:46:08,920
things we can do in this layer is to

00:46:06,280 --> 00:46:11,440
rate limit inbound traffic and this is

00:46:08,920 --> 00:46:13,900
there nice and XDP is perfect place for

00:46:11,440 --> 00:46:16,210
it with the exception that XDP doesn't

00:46:13,900 --> 00:46:19,360
do any memory synchronization barriers

00:46:16,210 --> 00:46:20,920
which makes it tricky to write simple

00:46:19,360 --> 00:46:23,260
things like token bucket so if you see

00:46:20,920 --> 00:46:25,690
any poor request from us on adding some

00:46:23,260 --> 00:46:28,090
locks maybe spin locks or or doing maybe

00:46:25,690 --> 00:46:31,600
compare exchange in the X DB does the

00:46:28,090 --> 00:46:33,720
reason basically need token buckets and

00:46:31,600 --> 00:46:36,400
then on the normal networking status

00:46:33,720 --> 00:46:38,740
networking side of Linux kernel we are

00:46:36,400 --> 00:46:43,180
running us mitigations in multiple

00:46:38,740 --> 00:46:43,900
layers we are running many modules on

00:46:43,180 --> 00:46:46,720
iptables

00:46:43,900 --> 00:46:48,640
we are running xt BPF collimate hash

00:46:46,720 --> 00:46:51,220
limits IP sets these are super useful

00:46:48,640 --> 00:46:53,590
for these mitigations if there if the

00:46:51,220 --> 00:46:54,040
packet goes through them we hit some

00:46:53,590 --> 00:46:56,770
cookies

00:46:54,040 --> 00:47:00,610
thanks for fixing single case in 318

00:46:56,770 --> 00:47:02,440
before that they were very slow and

00:47:00,610 --> 00:47:04,780
finally the application themselves can

00:47:02,440 --> 00:47:07,240
set some filters using SL filter so

00:47:04,780 --> 00:47:13,750
basil TB PF on top of sockets this is

00:47:07,240 --> 00:47:16,450
very useful okay the next layer is load

00:47:13,750 --> 00:47:18,010
balancing which we do it in X DB and I

00:47:16,450 --> 00:47:19,360
won't go into more details here but I

00:47:18,010 --> 00:47:21,790
just want to flag that dilib and

00:47:19,360 --> 00:47:23,530
Sinclair needs to be able to do a socket

00:47:21,790 --> 00:47:25,510
lookup needs to be able the way we

00:47:23,530 --> 00:47:28,570
design it is that it needs to be able to

00:47:25,510 --> 00:47:30,430
verify if they packet flying in can

00:47:28,570 --> 00:47:33,570
potentially belong to a connection that

00:47:30,430 --> 00:47:38,350
is local today to the current machine

00:47:33,570 --> 00:47:40,090
BBF xep has has helpers product so it

00:47:38,350 --> 00:47:41,440
works quite well but it's not really

00:47:40,090 --> 00:47:45,220
complete yet for exam

00:47:41,440 --> 00:47:48,130
we needed to add help her to decide if

00:47:45,220 --> 00:47:49,510
sin cookie is valid and as for as for a

00:47:48,130 --> 00:47:51,609
couple of weeks ago that was not

00:47:49,510 --> 00:47:56,980
possible so this is another area of

00:47:51,609 --> 00:47:58,690
exhibit work we are engaged in okay the

00:47:56,980 --> 00:48:01,420
dent packets are routed to kind of

00:47:58,690 --> 00:48:03,010
normal and Oaks networking stack it goes

00:48:01,420 --> 00:48:05,380
the iptables which I mentioned and then

00:48:03,010 --> 00:48:07,630
we do socket dispatch guys look at this

00:48:05,380 --> 00:48:09,940
patch I mean basically to which socket

00:48:07,630 --> 00:48:12,369
the packet should be routed to inside

00:48:09,940 --> 00:48:14,470
Linux a networking stack you may say

00:48:12,369 --> 00:48:15,030
okay it's easy you buy to port 80 and we

00:48:14,470 --> 00:48:17,290
are done

00:48:15,030 --> 00:48:19,900
unfortunately in our case it is much

00:48:17,290 --> 00:48:21,790
much more complex I'll give you a couple

00:48:19,900 --> 00:48:28,210
of use cases why this is not trial for

00:48:21,790 --> 00:48:31,720
us so for example for the DOS we decided

00:48:28,210 --> 00:48:33,880
to not have a shirt UDP receive socket

00:48:31,720 --> 00:48:35,890
the reason is that if there's an attack

00:48:33,880 --> 00:48:38,020
against the single IP address we don't

00:48:35,890 --> 00:48:40,750
want other IP addresses to suffer so

00:48:38,020 --> 00:48:43,540
historically we have about 30,000 UDP

00:48:40,750 --> 00:48:45,220
sockets just in order to be sure that

00:48:43,540 --> 00:48:47,829
the key that will overflow will not

00:48:45,220 --> 00:48:50,589
affect the others now that we can fix

00:48:47,829 --> 00:48:54,160
that with sole filter and EPF on sockets

00:48:50,589 --> 00:48:56,859
so we're working towards that and second

00:48:54,160 --> 00:48:59,740
problem our second caveat maybe an about

00:48:56,859 --> 00:49:02,530
UDP sockets is it's pretty hard to do 0

00:48:59,740 --> 00:49:04,329
downtown downtown restarts and with new

00:49:02,530 --> 00:49:06,760
work like quick it will become more and

00:49:04,329 --> 00:49:08,740
more painful so we are doing hot when we

00:49:06,760 --> 00:49:11,079
do a for people kind of connected

00:49:08,740 --> 00:49:13,180
sockets on top of unconnected sockets

00:49:11,079 --> 00:49:16,599
but perhaps there's a better way maybe

00:49:13,180 --> 00:49:18,730
we can improve the api's and finally as

00:49:16,599 --> 00:49:20,380
I mentioned we have many many many IP

00:49:18,730 --> 00:49:23,500
addresses we have thousands IP addresses

00:49:20,380 --> 00:49:25,990
and for us it's not good enough to just

00:49:23,500 --> 00:49:28,900
bind to port 80 for all the IPS we may

00:49:25,990 --> 00:49:31,680
have different applications needing to

00:49:28,900 --> 00:49:34,720
bind to port 80 on different IP ranges

00:49:31,680 --> 00:49:36,010
just to give you an example our resolver

00:49:34,720 --> 00:49:37,960
is when a couple IP addresses our

00:49:36,010 --> 00:49:41,109
alternative name servers and couple IP

00:49:37,960 --> 00:49:43,630
addresses they both of them are running

00:49:41,109 --> 00:49:46,150
on number of IP addresses so we can't

00:49:43,630 --> 00:49:48,520
bind to specific / 30 toes so we need

00:49:46,150 --> 00:49:50,589
something else for that we prepared a

00:49:48,520 --> 00:49:52,900
patch called soul bind to prefix we

00:49:50,589 --> 00:49:54,249
submit it a couple of years ago

00:49:52,900 --> 00:49:57,519
this basically allows you to bind to

00:49:54,249 --> 00:50:00,910
specific sub rate subnet and it's the

00:49:57,519 --> 00:50:02,440
reception wasn't very very in open and I

00:50:00,910 --> 00:50:04,660
think it's kind of specific to our use

00:50:02,440 --> 00:50:05,499
case but still there it needs to be we

00:50:04,660 --> 00:50:08,710
believe there needs to be some

00:50:05,499 --> 00:50:10,450
innovation around this area recently we

00:50:08,710 --> 00:50:13,869
started using T proxy when we have

00:50:10,450 --> 00:50:16,089
similar issue but not multiple sockets

00:50:13,869 --> 00:50:18,700
when we basically want a single socket

00:50:16,089 --> 00:50:22,630
to answer to traffic to multiple port

00:50:18,700 --> 00:50:25,299
numbers if you consider for example

00:50:22,630 --> 00:50:27,309
cutting traffic from all 65,000 port

00:50:25,299 --> 00:50:32,319
numbers the proxy is the solution for

00:50:27,309 --> 00:50:34,479
that unfortunately this this works

00:50:32,319 --> 00:50:36,729
reasonably for our current design but it

00:50:34,479 --> 00:50:38,229
doesn't really work with the load

00:50:36,729 --> 00:50:39,729
balancer which I mentioned as I

00:50:38,229 --> 00:50:41,170
mentioned load balancer needs to do the

00:50:39,729 --> 00:50:43,809
socket dispatch and it's completely

00:50:41,170 --> 00:50:47,170
confused about the T proxy iptables

00:50:43,809 --> 00:50:49,089
firewall rule so once again perhaps we

00:50:47,170 --> 00:50:51,729
can improve this okay dispatch in Linux

00:50:49,089 --> 00:50:53,589
perhaps we can add immediate around.i

00:50:51,729 --> 00:50:57,180
net look up or or improve it some some

00:50:53,589 --> 00:50:57,180
other way to make it more flexible on

00:50:57,960 --> 00:51:05,920
next layer inside our application stack

00:51:00,789 --> 00:51:08,349
we are using a patch from from Jason

00:51:05,920 --> 00:51:11,829
it's a fairly early fairly old patch

00:51:08,349 --> 00:51:13,359
which wasn't merged at the end I think

00:51:11,829 --> 00:51:15,339
there was logic use case but we have a

00:51:13,359 --> 00:51:19,180
use case for it so the problem with in

00:51:15,339 --> 00:51:22,569
our design or maybe in Linux is that the

00:51:19,180 --> 00:51:25,410
front line part which does which does

00:51:22,569 --> 00:51:28,509
the CPU intensive request processing

00:51:25,410 --> 00:51:31,180
it's using eople and it does except on

00:51:28,509 --> 00:51:34,509
the UNIX socket the problem is that

00:51:31,180 --> 00:51:37,359
people really doesn't load bounds the

00:51:34,509 --> 00:51:39,910
accepted sockets so we end up in a

00:51:37,359 --> 00:51:41,710
couple of workers doing plenty of work

00:51:39,910 --> 00:51:43,960
and most of the workers motor the CPUs

00:51:41,710 --> 00:51:46,239
being idle so basically there is no load

00:51:43,960 --> 00:51:48,279
balancing involved so people round-robin

00:51:46,239 --> 00:51:50,910
allows us to fix that perhaps it could

00:51:48,279 --> 00:51:52,779
be useful for also for others and

00:51:50,910 --> 00:51:56,319
finally looking to the future we are

00:51:52,779 --> 00:51:59,710
very excited about doing more putting

00:51:56,319 --> 00:52:03,160
more pushing more the data plane onto

00:51:59,710 --> 00:52:05,210
the kernel multiple play places we have

00:52:03,160 --> 00:52:08,180
a need to just wise to this B socket

00:52:05,210 --> 00:52:10,160
a good example is WebSockets we really

00:52:08,180 --> 00:52:11,840
don't need to receive data from one end

00:52:10,160 --> 00:52:14,030
and just write it to the other and we

00:52:11,840 --> 00:52:15,440
could just ask Colonel to do to splice

00:52:14,030 --> 00:52:18,320
the circuits together and do the old

00:52:15,440 --> 00:52:19,880
work inside inside kernel

00:52:18,320 --> 00:52:21,560
so sock Rob could be used for that

00:52:19,880 --> 00:52:22,820
software is not really ready for prime

00:52:21,560 --> 00:52:27,140
time yet but we are definitely looking

00:52:22,820 --> 00:52:29,390
into that similarly similarly k TLS with

00:52:27,140 --> 00:52:33,110
send file is a very powerful tool which

00:52:29,390 --> 00:52:39,170
could speed up our cache cache serving

00:52:33,110 --> 00:52:42,320
data from disk we are doing many

00:52:39,170 --> 00:52:43,640
experiments to tune stuff for for the

00:52:42,320 --> 00:52:46,420
internet so we are very excited about

00:52:43,640 --> 00:52:49,010
the EPF saqqaq and other developments I

00:52:46,420 --> 00:52:52,730
want to mention one more thing which is

00:52:49,010 --> 00:52:57,950
the the problem of introspection on the

00:52:52,730 --> 00:53:00,470
nose a static normal example is how we

00:52:57,950 --> 00:53:02,690
how we operate is our a series have an

00:53:00,470 --> 00:53:04,280
alert on listen drops metric and next at

00:53:02,690 --> 00:53:06,110
this is a very useful metric it tells

00:53:04,280 --> 00:53:07,760
you whether the new connections were

00:53:06,110 --> 00:53:09,560
dropped because the application was too

00:53:07,760 --> 00:53:11,990
slow that's fine so we want to know that

00:53:09,560 --> 00:53:14,000
the problem is the news doesn't tell you

00:53:11,990 --> 00:53:17,000
what application I had a problem so it

00:53:14,000 --> 00:53:19,010
so currently requires it requires an

00:53:17,000 --> 00:53:21,800
operator to log into machine and verify

00:53:19,010 --> 00:53:23,660
what is what is wrong so basically the

00:53:21,800 --> 00:53:27,230
granularity of Linux counters is not

00:53:23,660 --> 00:53:29,570
good enough for for our use case so this

00:53:27,230 --> 00:53:31,940
also we are using is EBP of explore turn

00:53:29,570 --> 00:53:34,100
its a Prometheus module primitive

00:53:31,940 --> 00:53:38,000
back-end which allows us to run EBP F

00:53:34,100 --> 00:53:39,680
probes on our server fleet and with with

00:53:38,000 --> 00:53:42,200
that we can extract pretty much anything

00:53:39,680 --> 00:53:43,670
we want it is super useful for many many

00:53:42,200 --> 00:53:45,440
things we found multiple but with that

00:53:43,670 --> 00:53:47,660
this is basically our way of

00:53:45,440 --> 00:53:52,100
introspecting kernel for specific needs

00:53:47,660 --> 00:53:53,870
we have a good example is disk speed if

00:53:52,100 --> 00:53:55,970
you want to have histograms not just row

00:53:53,870 --> 00:54:01,670
counter not just a single counter it can

00:53:55,970 --> 00:54:03,290
do that as always upgrading the kernels

00:54:01,670 --> 00:54:05,390
is hard especially we've had a lot large

00:54:03,290 --> 00:54:07,040
fleet I don't think I have much to say

00:54:05,390 --> 00:54:09,800
here except for the fact that we hit a

00:54:07,040 --> 00:54:11,420
couple of Hardware bugs recently and a

00:54:09,800 --> 00:54:14,060
couple of Cardinal regressions I mean

00:54:11,420 --> 00:54:15,540
there is the pain needs to be there

00:54:14,060 --> 00:54:19,230
perhaps we can improve on

00:54:15,540 --> 00:54:20,790
on how we roll out new changes in case

00:54:19,230 --> 00:54:23,130
you didn't notice I just mentioned to

00:54:20,790 --> 00:54:24,780
you six layers of EPS inside our stack

00:54:23,130 --> 00:54:26,100
and we're just starting there are at

00:54:24,780 --> 00:54:28,320
least three more layers we can

00:54:26,100 --> 00:54:29,100
potentially use so it's not soft we're

00:54:28,320 --> 00:54:32,430
eating the world

00:54:29,100 --> 00:54:34,020
it's BPF eating the kernel I guess I'm

00:54:32,430 --> 00:54:35,310
not sure if that's good or bad on one

00:54:34,020 --> 00:54:39,090
hand it's good because it's super

00:54:35,310 --> 00:54:40,950
powerful and it realises to do very

00:54:39,090 --> 00:54:42,720
exciting things on the other hand it's

00:54:40,950 --> 00:54:43,850
kind of scary that we stopped developing

00:54:42,720 --> 00:54:49,380
kernel ate the ice

00:54:43,850 --> 00:55:00,930
perhaps we can we can improve thank you

00:54:49,380 --> 00:55:03,120
very much folks in the hallway each of

00:55:00,930 --> 00:55:08,100
you said you use the minutes of use

00:55:03,120 --> 00:55:10,560
heavily invested if you're not so I

00:55:08,100 --> 00:55:12,480
think you can talk about why you choose

00:55:10,560 --> 00:55:15,180
these things one of the goals what

00:55:12,480 --> 00:55:29,270
motivates you to go bypass or not what

00:55:15,180 --> 00:55:32,520
you would like to see the Linux thanks

00:55:29,270 --> 00:55:34,830
so we are not really contributing at

00:55:32,520 --> 00:55:36,780
this stage to kernel but what we look at

00:55:34,830 --> 00:55:40,260
is the strategy can you expand into car

00:55:36,780 --> 00:55:42,030
Nova case what happens is this is the

00:55:40,260 --> 00:55:44,250
common denominator across the world

00:55:42,030 --> 00:55:47,580
where we could go and program certain

00:55:44,250 --> 00:55:52,530
things whether it's routing with it

00:55:47,580 --> 00:55:54,210
something related to slicing I what we

00:55:52,530 --> 00:55:58,110
are looking at as a kernel as the

00:55:54,210 --> 00:56:01,290
resource and I think there I love fair

00:55:58,110 --> 00:56:04,320
is heavily invested as we can see in the

00:56:01,290 --> 00:56:08,070
way they run their infrastructure for us

00:56:04,320 --> 00:56:10,530
most of the features like xvp songs for

00:56:08,070 --> 00:56:12,930
interesting EBV program in it program

00:56:10,530 --> 00:56:17,580
ability is what we look for these

00:56:12,930 --> 00:56:21,990
technologies will use so the there's

00:56:17,580 --> 00:56:24,390
certain issues with how how it's not

00:56:21,990 --> 00:56:27,450
that easy to use

00:56:24,390 --> 00:56:31,440
we either developer team for controller

00:56:27,450 --> 00:56:34,470
person house so or we basically rely on

00:56:31,440 --> 00:56:36,390
outsourcing back to someone so that's

00:56:34,470 --> 00:56:39,420
the challenge we basically struggle with

00:56:36,390 --> 00:56:42,330
but essentially if you look at the way

00:56:39,420 --> 00:56:44,700
technologies are improving thanks to

00:56:42,330 --> 00:56:49,290
some of the work the contributors here

00:56:44,700 --> 00:56:51,270
who have done I think it's getting ready

00:56:49,290 --> 00:56:56,370
for the next generation of internet what

00:56:51,270 --> 00:56:57,990
we believe should be low latency slicing

00:56:56,370 --> 00:57:00,660
and all these programmability features I

00:56:57,990 --> 00:57:04,500
think they live all pretty fast and

00:57:00,660 --> 00:57:06,090
everybody will use it I saw in Arthur's

00:57:04,500 --> 00:57:08,030
presentation the also talked about

00:57:06,090 --> 00:57:10,530
programmability and your data center so

00:57:08,030 --> 00:57:13,290
it sounds pretty exciting that you guys

00:57:10,530 --> 00:57:19,380
are working on but that's that's how I

00:57:13,290 --> 00:57:23,520
think yeah so my perspective is slightly

00:57:19,380 --> 00:57:26,850
different here as as we do not use Linux

00:57:23,520 --> 00:57:32,400
in a way that the guys at the edge are

00:57:26,850 --> 00:57:35,190
using so I don't want to focus at the at

00:57:32,400 --> 00:57:38,670
the application itself so the way I look

00:57:35,190 --> 00:57:41,640
at it is more broadly as to Linux of the

00:57:38,670 --> 00:57:44,670
kernel or GTO Linux as the entire system

00:57:41,640 --> 00:57:47,940
or even community enables us to do the

00:57:44,670 --> 00:57:51,500
things we do know with with the

00:57:47,940 --> 00:57:55,410
networking but also having that said

00:57:51,500 --> 00:57:59,250
Lincoln and a lot of large scale

00:57:55,410 --> 00:58:03,240
operators they have a luxury of not

00:57:59,250 --> 00:58:07,410
using this general paradigm of the

00:58:03,240 --> 00:58:10,500
network where people it's assumed that

00:58:07,410 --> 00:58:13,410
people own only network all these

00:58:10,500 --> 00:58:15,390
companies they own the infrastructure

00:58:13,410 --> 00:58:17,850
end-to-end so he started from the end

00:58:15,390 --> 00:58:21,750
host going through the network and

00:58:17,850 --> 00:58:24,060
ending at the end host so the good part

00:58:21,750 --> 00:58:28,350
is so we are here looking very closely

00:58:24,060 --> 00:58:32,190
at the advances in things like TCP

00:58:28,350 --> 00:58:36,270
analytics where we can use this

00:58:32,190 --> 00:58:40,829
I held at the in house to to have better

00:58:36,270 --> 00:58:44,460
network observability or to to measure

00:58:40,829 --> 00:58:47,040
or to better understand how how the

00:58:44,460 --> 00:58:53,010
applications perceive or how they

00:58:47,040 --> 00:58:57,410
experience the network so about the

00:58:53,010 --> 00:59:01,020
kernel a bus we were we were using

00:58:57,410 --> 00:59:02,970
rhetoric kernel i bypass forty dos with

00:59:01,020 --> 00:59:04,650
a simple argument that if you need to

00:59:02,970 --> 00:59:06,329
drop millions packets per second it's

00:59:04,650 --> 00:59:10,619
much more efficient to do it in the

00:59:06,329 --> 00:59:12,300
bypass case but unfortunately that's

00:59:10,619 --> 00:59:16,800
about it we don't have another use case

00:59:12,300 --> 00:59:20,609
and there are people suggesting to use a

00:59:16,800 --> 00:59:22,950
kernel bypass for speeding up tcp linux

00:59:20,609 --> 00:59:26,010
kernel there are couple problems with

00:59:22,950 --> 00:59:28,560
that first is that the multi-tenancy

00:59:26,010 --> 00:59:30,240
story is quite hard so we don't find a

00:59:28,560 --> 00:59:30,960
single application if we run a single

00:59:30,240 --> 00:59:34,020
piece of code

00:59:30,960 --> 00:59:36,630
perhaps we could offload it but they

00:59:34,020 --> 00:59:39,599
have multiple pieces second is that

00:59:36,630 --> 00:59:41,190
point is quite hard we do use things

00:59:39,599 --> 00:59:46,079
like TCP dump which generally don't work

00:59:41,190 --> 00:59:48,780
with with the auto solutions and finally

00:59:46,079 --> 00:59:50,369
our applications mostly CEO bound we

00:59:48,780 --> 00:59:54,270
don't spend too much time and doing

00:59:50,369 --> 00:59:57,780
actually actually interrupts actually

00:59:54,270 --> 01:00:00,630
doing any time in networking stack so so

00:59:57,780 --> 01:00:02,609
it was always hard for me to judge what

01:00:00,630 --> 01:00:04,200
is the goal of kernel by bus if the goal

01:00:02,609 --> 01:00:06,780
is for beta latency perhaps that

01:00:04,200 --> 01:00:08,339
safeness about use case but but it but

01:00:06,780 --> 01:00:11,180
you have but it may cost and

01:00:08,339 --> 01:00:11,180
inflexibility

01:00:16,250 --> 01:00:22,230
so one of the biggest challenges and

01:00:20,520 --> 01:00:25,230
it's pretty well known with things like

01:00:22,230 --> 01:00:27,299
the colonel is upgrades it's true for

01:00:25,230 --> 01:00:29,299
all software that upgrading is always a

01:00:27,299 --> 01:00:32,339
pain but kernels in particular seem

01:00:29,299 --> 01:00:35,220
really problematic in upgrading to the

01:00:32,339 --> 01:00:36,900
extent that there are certainly some

01:00:35,220 --> 01:00:39,569
people who are just terrified of this

01:00:36,900 --> 01:00:44,670
I'm wondering from the perspective of

01:00:39,569 --> 01:00:46,710
the major operators what what is the

01:00:44,670 --> 01:00:48,750
state of this and where do you think

01:00:46,710 --> 01:00:51,770
this could go to improve this process

01:00:48,750 --> 01:00:55,140
obviously if we're not upgrading kernels

01:00:51,770 --> 01:00:57,210
even with programmability and xdp these

01:00:55,140 --> 01:00:59,750
are going to be things we can do so give

01:00:57,210 --> 01:00:59,750
any comments on that

01:00:59,780 --> 01:01:09,119
so from our point of view we have a

01:01:06,180 --> 01:01:11,520
luxury of not storing too much state on

01:01:09,119 --> 01:01:13,589
our our servers so in the worst case we

01:01:11,520 --> 01:01:16,260
can always turn off the data center to

01:01:13,589 --> 01:01:17,940
the railroads to do whatever we need on

01:01:16,260 --> 01:01:21,690
the maintenance window and enter it on

01:01:17,940 --> 01:01:23,670
but again so it's it's is generally fine

01:01:21,690 --> 01:01:26,280
I think the tendency to move more and

01:01:23,670 --> 01:01:29,190
more things to PDF is a partial source

01:01:26,280 --> 01:01:31,170
source product xdp allows us to not

01:01:29,190 --> 01:01:34,559
write kernel modules which is a good

01:01:31,170 --> 01:01:37,799
thing but I don't think there is a there

01:01:34,559 --> 01:01:41,280
is a broader story the well another

01:01:37,799 --> 01:01:45,329
issue we're running into is kernel

01:01:41,280 --> 01:01:47,579
driver bugs so that it is beneficial for

01:01:45,329 --> 01:01:50,520
us to run older carnavalet versions

01:01:47,579 --> 01:01:52,589
where when we are sure that this is less

01:01:50,520 --> 01:01:54,210
buggy hey I think the risk is always

01:01:52,589 --> 01:02:05,250
there so I don't think there's there's

01:01:54,210 --> 01:02:06,990
enough yeah you sure so hmm I think

01:02:05,250 --> 01:02:09,900
you're very similar key challenges

01:02:06,990 --> 01:02:13,049
applied to operating the kernel on the

01:02:09,900 --> 01:02:15,210
network devices you let's say the access

01:02:13,049 --> 01:02:18,780
to switch that many of the machines are

01:02:15,210 --> 01:02:22,859
connected to so so here I I see this

01:02:18,780 --> 01:02:23,640
morisaki short game between the network

01:02:22,859 --> 01:02:26,160
and the

01:02:23,640 --> 01:02:28,320
vacation so if the applications are

01:02:26,160 --> 01:02:32,580
designed in a way that our kiss tapeless

01:02:28,320 --> 01:02:35,310
or its kind about easy to migrate away

01:02:32,580 --> 01:02:39,330
all the key traffic from the server's

01:02:35,310 --> 01:02:41,400
connected to the the switch then the

01:02:39,330 --> 01:02:42,480
this whole upgrade is getting let's say

01:02:41,400 --> 01:02:46,310
easier

01:02:42,480 --> 01:02:52,350
I'm not saying easy but are this easier

01:02:46,310 --> 01:02:55,590
but certs and obviously there is no one

01:02:52,350 --> 01:02:59,370
one good solution for that there are key

01:02:55,590 --> 01:03:02,190
solutions that are we probably there

01:02:59,370 --> 01:03:06,660
have higher chances of introducing fewer

01:03:02,190 --> 01:03:09,420
outages like the rolling upgrades you're

01:03:06,660 --> 01:03:13,940
doing calories he progressively rollouts

01:03:09,420 --> 01:03:17,310
or automatic automatic in validations if

01:03:13,940 --> 01:03:19,320
if the device after the upgrade works

01:03:17,310 --> 01:03:25,170
correctly or not and then rolling

01:03:19,320 --> 01:03:27,330
rolling back automatically I think

01:03:25,170 --> 01:03:30,300
that's the huge problem yeah a great

01:03:27,330 --> 01:03:31,890
colonel I think right now people are

01:03:30,300 --> 01:03:34,560
looking at this problem as a datacenter

01:03:31,890 --> 01:03:38,160
problem can i isolate certain machines

01:03:34,560 --> 01:03:40,950
and upgrade them and then then upgrade

01:03:38,160 --> 01:03:44,880
the red zone or the clusters but I think

01:03:40,950 --> 01:03:48,510
the way if Nick Linux has to mature to

01:03:44,880 --> 01:03:50,670
be the end device colonel I think

01:03:48,510 --> 01:03:54,600
somebody who solves this problem will be

01:03:50,670 --> 01:03:56,970
a multiple in your company because I

01:03:54,600 --> 01:03:58,980
think this is not a easy problem today

01:03:56,970 --> 01:04:01,970
every time you touch Colonel I think

01:03:58,980 --> 01:04:05,520
it's a hassle people roll back people

01:04:01,970 --> 01:04:08,850
lose connectivity and then and more and

01:04:05,520 --> 01:04:10,890
more so I think people are actually very

01:04:08,850 --> 01:04:14,610
slow in moving to the latest conversions

01:04:10,890 --> 01:04:15,720
just because of the fear these things

01:04:14,610 --> 01:04:19,380
happening in the network today

01:04:15,720 --> 01:04:21,180
um but yeah it's a real problem in the

01:04:19,380 --> 01:04:25,320
corner today I think that we also have

01:04:21,180 --> 01:04:28,170
some very good fix it yeah but if you if

01:04:25,320 --> 01:04:29,940
you rely on a distro vendor I guess the

01:04:28,170 --> 01:04:32,369
disturb render will take care of you

01:04:29,940 --> 01:04:34,289
and that's true and yeah most of the

01:04:32,369 --> 01:04:37,289
times we see that we have with other

01:04:34,289 --> 01:04:40,200
than this version we need but the

01:04:37,289 --> 01:04:42,089
operators are not you know that skill to

01:04:40,200 --> 01:04:44,819
debug the problem in real time

01:04:42,089 --> 01:04:46,380
ci some some solution exists that we

01:04:44,819 --> 01:04:49,109
could actually look at what's going on

01:04:46,380 --> 01:04:50,339
without great what's strong and then why

01:04:49,109 --> 01:04:52,470
we losing connectivity or why the

01:04:50,339 --> 01:04:54,630
machine is not giving up fighting this

01:04:52,470 --> 01:04:56,339
kind of visibility if we can get

01:04:54,630 --> 01:04:59,160
throughout the infrastructure not just

01:04:56,339 --> 01:05:01,920
actually the fact the cloudless your

01:04:59,160 --> 01:05:03,960
building but when we upgrade the devices

01:05:01,920 --> 01:05:07,140
out in the field that'll be awesome

01:05:03,960 --> 01:05:09,630
ok so both of you talked about

01:05:07,140 --> 01:05:11,490
programmable needful programmability are

01:05:09,630 --> 01:05:14,160
you guys looking at programmable

01:05:11,490 --> 01:05:16,619
hardware and better support for these

01:05:14,160 --> 01:05:21,109
hardware in Linux or like before or

01:05:16,619 --> 01:05:26,880
something or it's just at software level

01:05:21,109 --> 01:05:30,150
I know I I don't think yeah we don't see

01:05:26,880 --> 01:05:33,450
that adoption that fast ok what we are

01:05:30,150 --> 01:05:36,270
seeing is that a lot of operators are

01:05:33,450 --> 01:05:39,660
buying general hardware it's just a cost

01:05:36,270 --> 01:05:42,299
of buying this in bulk and deploying it

01:05:39,660 --> 01:05:44,549
and virtualizing all of their vnf and

01:05:42,299 --> 01:05:47,609
core networks I think the drive there is

01:05:44,549 --> 01:05:49,740
to exqueeze the maximum bang for the

01:05:47,609 --> 01:05:51,569
buck whether I'm using a standard make

01:05:49,740 --> 01:05:53,520
or whether can I do something in the

01:05:51,569 --> 01:05:57,599
kernel what can I offer to the hardware

01:05:53,520 --> 01:06:00,059
that's the drive EC we do not see people

01:05:57,599 --> 01:06:03,539
jumping on programmability that much on

01:06:00,059 --> 01:06:06,690
the hardware side but for for us to

01:06:03,539 --> 01:06:09,329
bring this to life to edge we have to

01:06:06,690 --> 01:06:10,440
scale this in a way that we could

01:06:09,329 --> 01:06:14,760
program instruction in the commo

01:06:10,440 --> 01:06:17,400
starting very simply there then actually

01:06:14,760 --> 01:06:19,740
move those systems of systems which we

01:06:17,400 --> 01:06:23,069
have mode off load into the general

01:06:19,740 --> 01:06:25,410
hardware I think the technology is there

01:06:23,069 --> 01:06:28,319
I think that's the stage we are looking

01:06:25,410 --> 01:06:31,200
at this problem not really not trying to

01:06:28,319 --> 01:06:34,579
solve by buildings and custom chips are

01:06:31,200 --> 01:06:34,579
are but not ok

01:06:35,640 --> 01:06:39,990
okay so here to me it's it's much more

01:06:37,890 --> 01:06:42,330
important that he programmability at the

01:06:39,990 --> 01:06:45,000
software level than at the hardware

01:06:42,330 --> 01:06:48,060
level as they the whole goal is to to

01:06:45,000 --> 01:06:51,660
keep the data plane as simple as

01:06:48,060 --> 01:06:54,420
possible so at every every single time

01:06:51,660 --> 01:06:59,640
we can replace the chip we can replace

01:06:54,420 --> 01:07:03,420
the vendor and at the same time I just

01:06:59,640 --> 01:07:06,270
think that the biggest value of building

01:07:03,420 --> 01:07:08,970
great networks lies more on the

01:07:06,270 --> 01:07:12,000
management plane he had done actually on

01:07:08,970 --> 01:07:14,430
the data plane so why not me I'm

01:07:12,000 --> 01:07:17,810
watching this very closely and it's very

01:07:14,430 --> 01:07:22,620
fascinating to see things like p4 and

01:07:17,810 --> 01:07:26,100
and programmable hardware and I believe

01:07:22,620 --> 01:07:32,130
the the real use case are still to

01:07:26,100 --> 01:07:35,250
emerge but as that's it now we decided

01:07:32,130 --> 01:07:38,910
to go the way of keeping the the network

01:07:35,250 --> 01:07:41,430
as as soon as possible and we try to to

01:07:38,910 --> 01:07:47,600
move a lot of complexity due to the

01:07:41,430 --> 01:07:50,370
applications ok so before the XDP was

01:07:47,600 --> 01:07:52,650
created there was and you need to

01:07:50,370 --> 01:07:54,300
offload more stuff are either to do

01:07:52,650 --> 01:07:57,420
cardio bypass or upload it to the

01:07:54,300 --> 01:08:01,440
hardware but nowadays xdp is just fast

01:07:57,420 --> 01:08:04,370
enough so I don't see us pressing our

01:08:01,440 --> 01:08:07,140
vendors to do more hardware uploads and

01:08:04,370 --> 01:08:09,720
having said that there are two things

01:08:07,140 --> 01:08:13,020
which I'm personally very excited about

01:08:09,720 --> 01:08:16,710
one is just the good old TCP offloads

01:08:13,020 --> 01:08:20,940
can someone get a little working well

01:08:16,710 --> 01:08:25,200
please for many flows these days and and

01:08:20,940 --> 01:08:27,480
the second one is magical xdp of flows

01:08:25,200 --> 01:08:30,150
we are running a typical we don't mind

01:08:27,480 --> 01:08:32,580
where it runs as long as we own it and

01:08:30,150 --> 01:08:34,380
as soon as we can modify it rapidly I

01:08:32,580 --> 01:08:40,739
don't mind the Dragon Con

01:08:34,380 --> 01:08:42,630
on hardware directed to alpha

01:08:40,739 --> 01:08:46,650
Vegas and told to our ask hard questions

01:08:42,630 --> 01:08:48,480
so so I guess as you know we've been

01:08:46,650 --> 01:08:50,969
talking about Hardware offload and

01:08:48,480 --> 01:08:54,719
you've you just mentioned simpler data

01:08:50,969 --> 01:08:57,750
planes and we have been talking about in

01:08:54,719 --> 01:09:01,020
the hardware off of workshops about for

01:08:57,750 --> 01:09:03,659
Asics which Asics especially offloading

01:09:01,020 --> 01:09:07,020
the colonel data plane to Hardware using

01:09:03,659 --> 01:09:09,239
native Linux acceleration so have you

01:09:07,020 --> 01:09:12,179
and I know sonic doesn't do that it does

01:09:09,239 --> 01:09:15,509
kind of a kernel pie pals so have you

01:09:12,179 --> 01:09:23,279
guys spent any time looking at what the

01:09:15,509 --> 01:09:25,739
kernel is doing so it works I guess so

01:09:23,279 --> 01:09:28,710
I personally think there's much more of

01:09:25,739 --> 01:09:30,690
a use case at the at the edge side of

01:09:28,710 --> 01:09:33,839
things I I mentioned about this

01:09:30,690 --> 01:09:35,819
east-west explosion so the here the

01:09:33,839 --> 01:09:38,609
amount of the traffic in the data center

01:09:35,819 --> 01:09:41,520
is much much bigger and actually it's

01:09:38,609 --> 01:09:44,699
not about being really just here we

01:09:41,520 --> 01:09:47,870
simply go with the his solution that

01:09:44,699 --> 01:09:50,819
works well for us is good enough and

01:09:47,870 --> 01:09:55,560
taking into account the environment we

01:09:50,819 --> 01:10:00,270
are in so our our resources and our

01:09:55,560 --> 01:10:02,520
constraints so simply as of now you're

01:10:00,270 --> 01:10:05,370
looking from the sonic perspective or if

01:10:02,520 --> 01:10:07,440
you're looking from the perspective of

01:10:05,370 --> 01:10:13,949
the switches in the data center we

01:10:07,440 --> 01:10:18,000
simply have no use case for it okay it's

01:10:13,949 --> 01:10:20,760
time do you guys use FRR oh yeah so I

01:10:18,000 --> 01:10:24,239
believe it's uh it's a pepper are and

01:10:20,760 --> 01:10:26,870
it's a simple BGP here so did nothing

01:10:24,239 --> 01:10:26,870
very magical

01:10:32,950 --> 01:10:37,890
I still have to respond but if you don't

01:10:40,170 --> 01:10:46,750
know I think on the contrary I think not

01:10:44,260 --> 01:10:49,720
the same we definitely want to avoid the

01:10:46,750 --> 01:10:52,060
east-west traffic we don't believe

01:10:49,720 --> 01:10:54,640
that's the way we could actually run any

01:10:52,060 --> 01:10:57,220
latency-sensitive application we gonna

01:10:54,640 --> 01:10:59,170
program tour we're gonna try to do load

01:10:57,220 --> 01:11:01,600
balancing on the door to hit the right

01:10:59,170 --> 01:11:03,400
container these are the environments

01:11:01,600 --> 01:11:06,460
where people are looking for stringent

01:11:03,400 --> 01:11:08,260
things highly likely there will be live

01:11:06,460 --> 01:11:11,430
particular applications on it so we

01:11:08,260 --> 01:11:16,330
definitely won't be doing its best

01:11:11,430 --> 01:11:18,340
explosions as microservices evolved we

01:11:16,330 --> 01:11:20,050
will try to contain that you know very

01:11:18,340 --> 01:11:23,050
lightweight format and try to program

01:11:20,050 --> 01:11:24,670
top of racks which is the fastest way we

01:11:23,050 --> 01:11:27,960
could get the packet out the best

01:11:24,670 --> 01:11:27,960
vehicles are the customers

01:11:38,810 --> 01:11:51,659
yeah so I'm just curious most of most of

01:11:47,880 --> 01:11:55,590
the worthless you ran a kind of in-house

01:11:51,659 --> 01:12:01,500
I'm just curious lots of ink has been

01:11:55,590 --> 01:12:04,560
spilled about all the all the side

01:12:01,500 --> 01:12:06,900
channels that exist in current hardware

01:12:04,560 --> 01:12:09,780
and lots of CPU cycles

01:12:06,900 --> 01:12:12,900
some people are spent and trying to

01:12:09,780 --> 01:12:17,639
mitigate this all of you kind of

01:12:12,900 --> 01:12:24,270
disabling these mitigations because you

01:12:17,639 --> 01:12:29,310
just ran in your own code or affected in

01:12:24,270 --> 01:12:31,110
any way by this and but you know working

01:12:29,310 --> 01:12:33,770
on reducing the performance impact of

01:12:31,110 --> 01:12:36,449
these mitigations be of interest if I

01:12:33,770 --> 01:12:40,829
can answer that from culture point of

01:12:36,449 --> 01:12:43,560
view we have mostly software slack

01:12:40,829 --> 01:12:46,679
written by us so it's most not a problem

01:12:43,560 --> 01:12:49,050
but we do run workers which which is

01:12:46,679 --> 01:12:53,400
basically a third party users user

01:12:49,050 --> 01:12:56,340
supplied code so we do our best to run

01:12:53,400 --> 01:12:58,889
all the investigations in case of Linux

01:12:56,340 --> 01:13:02,579
the response to the to the hardware box

01:12:58,889 --> 01:13:04,440
was quite quite excellent but it wasn't

01:13:02,579 --> 01:13:06,239
without problems we have a couple of

01:13:04,440 --> 01:13:08,790
crushes and we have one issue but we are

01:13:06,239 --> 01:13:12,690
we are doing our best and better

01:13:08,790 --> 01:13:15,510
question is how it would what are the

01:13:12,690 --> 01:13:18,150
implications to be api's and definitely

01:13:15,510 --> 01:13:21,360
in our case our task ID eyes are being

01:13:18,150 --> 01:13:23,880
trimmed in order to not leave much

01:13:21,360 --> 01:13:30,659
attack service so at least we are

01:13:23,880 --> 01:13:33,440
considering at so in in our perspective

01:13:30,659 --> 01:13:37,440
I believe it's slightly simpler as the

01:13:33,440 --> 01:13:40,620
few sonic switches are key to internal

01:13:37,440 --> 01:13:43,140
with no way of accessing them from

01:13:40,620 --> 01:13:46,680
inside or the case which is accessing

01:13:43,140 --> 01:13:51,360
the outside plus all the older software

01:13:46,680 --> 01:13:54,090
is either open source written by us so

01:13:51,360 --> 01:13:58,469
Michael is I guess we should be deterred

01:13:54,090 --> 01:14:00,960
or we aren't doing anything right now

01:13:58,469 --> 01:14:03,510
we're just trying to kill clustered or

01:14:00,960 --> 01:14:08,850
in their metal server right now nothing

01:14:03,510 --> 01:14:11,580
special I had a question on network

01:14:08,850 --> 01:14:14,850
analytics Arthur you mentioned that

01:14:11,580 --> 01:14:16,949
telemetry and we did have a couple of

01:14:14,850 --> 01:14:19,969
analytics workshops and so on at the

01:14:16,949 --> 01:14:22,710
conference do you guys have any specific

01:14:19,969 --> 01:14:28,410
requirements or specific things that you

01:14:22,710 --> 01:14:31,860
use today from the kernel start yeah so

01:14:28,410 --> 01:14:33,690
the the main challenge I guess with is

01:14:31,860 --> 01:14:37,860
with the sheer amount of data and

01:14:33,690 --> 01:14:40,920
processing it and collecting it so while

01:14:37,860 --> 01:14:43,590
we have all the all the basics that

01:14:40,920 --> 01:14:45,900
needs to be collected and you disappear

01:14:43,590 --> 01:14:49,500
info that that should be more than

01:14:45,900 --> 01:14:53,070
enough that thinks we have now the

01:14:49,500 --> 01:14:54,960
challenge is collecting this at scale or

01:14:53,070 --> 01:14:58,160
if you're collecting at scale than

01:14:54,960 --> 01:15:03,020
saving this and making them available

01:14:58,160 --> 01:15:06,300
both both for the real-time analytics so

01:15:03,020 --> 01:15:09,930
looking for example deciding whether if

01:15:06,300 --> 01:15:14,390
someone says the network is slow whether

01:15:09,930 --> 01:15:18,300
it is really dead or a core maybe it's

01:15:14,390 --> 01:15:23,370
it's a host or a nick or or the

01:15:18,300 --> 01:15:26,460
application and so and it's it's also

01:15:23,370 --> 01:15:28,949
important to perform the he TCP

01:15:26,460 --> 01:15:32,670
analytics or any form of analytics as

01:15:28,949 --> 01:15:34,890
close to the user as possible as the

01:15:32,670 --> 01:15:38,730
fundamental question is she worka does

01:15:34,890 --> 01:15:40,710
the network start so he sometimes he

01:15:38,730 --> 01:15:43,460
networking people think more than about

01:15:40,710 --> 01:15:45,920
the first access port on the switch

01:15:43,460 --> 01:15:48,290
but what what user thinks that the

01:15:45,920 --> 01:15:51,020
network for for them is between two

01:15:48,290 --> 01:15:55,660
sockets and they you do not care what

01:15:51,020 --> 01:16:01,070
fails so assuming we can we can measure

01:15:55,660 --> 01:16:04,340
some form of metrics or some form of

01:16:01,070 --> 01:16:07,250
user satisfaction with the network as

01:16:04,340 --> 01:16:09,110
close to the socket level as possible or

01:16:07,250 --> 01:16:11,360
as close to the application level as

01:16:09,110 --> 01:16:16,460
possible that opened it also allows us

01:16:11,360 --> 01:16:19,550
to to spend our time and resources

01:16:16,460 --> 01:16:23,870
activities that actually improve the

01:16:19,550 --> 01:16:26,540
things that are broken so that's an

01:16:23,870 --> 01:16:29,120
interesting comment right I was I would

01:16:26,540 --> 01:16:31,760
have thought that for you to do your

01:16:29,120 --> 01:16:34,610
telemetry at the edge where the socket

01:16:31,760 --> 01:16:36,260
lives is not as interesting as doing

01:16:34,610 --> 01:16:37,820
something like excite where you have all

01:16:36,260 --> 01:16:40,160
this data a calf car something and you

01:16:37,820 --> 01:16:43,219
might try to find out what parts of the

01:16:40,160 --> 01:16:47,420
network were underperforming is that not

01:16:43,219 --> 01:16:49,820
true it's not idea of doing yeah so I'm

01:16:47,420 --> 01:16:54,940
not be involved in the edge so that's

01:16:49,820 --> 01:16:59,030
why I mean I'm not the right person to

01:16:54,940 --> 01:17:00,050
discuss the edge but you still even

01:16:59,030 --> 01:17:02,840
having Kafka

01:17:00,050 --> 01:17:04,510
here with thousands of tens of thousands

01:17:02,840 --> 01:17:07,750
or hundreds of thousands sometimes

01:17:04,510 --> 01:17:11,690
connections per host you trying to

01:17:07,750 --> 01:17:14,390
export all the data probably it wouldn't

01:17:11,690 --> 01:17:17,330
work so we need to be much much more

01:17:14,390 --> 01:17:21,550
selective and do some form of sampling

01:17:17,330 --> 01:17:24,469
or exporting only the interesting events

01:17:21,550 --> 01:17:27,680
there's a large I would have thought

01:17:24,469 --> 01:17:29,450
that for you to monitor these things at

01:17:27,680 --> 01:17:31,810
a socket may even be possible at the

01:17:29,450 --> 01:17:31,810
circus

01:17:34,849 --> 01:17:43,929
oh so we have you burn metal servers so

01:17:39,559 --> 01:17:43,929
we don't have to deal with with the ends

01:17:47,349 --> 01:17:52,040
you said that you have a flat network so

01:17:49,969 --> 01:17:58,219
you don't have a tendency for that

01:17:52,040 --> 01:18:02,630
reason as well because so yeah if that

01:17:58,219 --> 01:18:05,809
network it's a result of the of the

01:18:02,630 --> 01:18:08,659
simplicity in the design so we want to

01:18:05,809 --> 01:18:11,989
keep the network as as simple as

01:18:08,659 --> 01:18:14,389
possible and if any any team any

01:18:11,989 --> 01:18:18,619
application they they want to build

01:18:14,389 --> 01:18:22,190
their own overlay it's fine but the the

01:18:18,619 --> 01:18:24,619
network organization or network team as

01:18:22,190 --> 01:18:31,099
a whole is not offering that that kind

01:18:24,619 --> 01:18:34,190
of things I think probably one question

01:18:31,099 --> 01:18:36,440
or more or two maybe okay well you

01:18:34,190 --> 01:18:41,239
passing the mic I'll ask a mobile a cake

01:18:36,440 --> 01:18:42,619
sky did you just did I hear you

01:18:41,239 --> 01:18:46,280
correctly saying you you don't care

01:18:42,619 --> 01:18:49,460
about the hype called kubernetes sir no

01:18:46,280 --> 01:18:54,260
we do okay you have to understand the

01:18:49,460 --> 01:18:57,650
communities genesis is that google is

01:18:54,260 --> 01:19:01,520
promoting it and people like it right so

01:18:57,650 --> 01:19:03,440
we're not gonna say no to it but does it

01:19:01,520 --> 01:19:05,270
serve the purpose of performance I think

01:19:03,440 --> 01:19:07,099
the audience here understand the

01:19:05,270 --> 01:19:09,679
technology very well it doesn't right

01:19:07,099 --> 01:19:11,510
it's meant for usability 90 question of

01:19:09,679 --> 01:19:14,690
the users and cloud service providers

01:19:11,510 --> 01:19:18,170
don't care what black black magic is

01:19:14,690 --> 01:19:20,630
happening underneath but in these late

01:19:18,170 --> 01:19:23,510
insistence that everyone would care so

01:19:20,630 --> 01:19:26,809
we'll have to change certain key things

01:19:23,510 --> 01:19:30,710
in that orchestration scheme to allow us

01:19:26,809 --> 01:19:34,849
to program that stuff right so no we do

01:19:30,710 --> 01:19:37,670
care you also care what you say I think

01:19:34,849 --> 01:19:39,619
performance is important to you and you

01:19:37,670 --> 01:19:41,090
don't think kubernetes is ready in that

01:19:39,619 --> 01:19:43,500
space

01:19:41,090 --> 01:19:45,329
I'm CUBAN it is allows you to

01:19:43,500 --> 01:19:47,909
flexibility you can plug in whatever you

01:19:45,329 --> 01:19:50,579
want there what we are seeing is that we

01:19:47,909 --> 01:19:52,980
don't see that typically that people

01:19:50,579 --> 01:19:54,540
would change that they tend to just on

01:19:52,980 --> 01:19:57,139
put an application on a cloud

01:19:54,540 --> 01:19:59,489
whatever default is available to use it

01:19:57,139 --> 01:20:00,840
but when we offer a service like that

01:19:59,489 --> 01:20:03,150
which is a little insensitive we'll have

01:20:00,840 --> 01:20:05,340
to take care of the the way we plumb the

01:20:03,150 --> 01:20:11,309
packets into the containers which are in

01:20:05,340 --> 01:20:14,219
the back so please donate so there were

01:20:11,309 --> 01:20:16,829
many comments about how latency is

01:20:14,219 --> 01:20:19,889
important I was hoping you can help put

01:20:16,829 --> 01:20:22,739
a number to both what kinds of Layton

01:20:19,889 --> 01:20:25,349
sees I expected like what you have now

01:20:22,739 --> 01:20:29,309
and where you're going and also what

01:20:25,349 --> 01:20:31,500
throughput you're looking at okay I'll

01:20:29,309 --> 01:20:32,639
start we care only for end-to-end

01:20:31,500 --> 01:20:35,250
latency

01:20:32,639 --> 01:20:38,119
that's the only metrics we believe is

01:20:35,250 --> 01:20:40,500
the right metrics we started off by

01:20:38,119 --> 01:20:44,070
taking care of our own infrastructure

01:20:40,500 --> 01:20:45,809
first the things we can control but then

01:20:44,070 --> 01:20:47,610
there's other aspects of RAM latency

01:20:45,809 --> 01:20:49,739
which we haven't figured out the right

01:20:47,610 --> 01:20:51,869
ways to do it as of now but we are

01:20:49,739 --> 01:20:54,000
working with some partners to eyeball

01:20:51,869 --> 01:20:56,760
that space so you could actually have

01:20:54,000 --> 01:20:58,460
contracts on the RAM side just say yeah

01:20:56,760 --> 01:21:00,929
I'm gonna give you a little scene

01:20:58,460 --> 01:21:03,050
optimal slice and you can actually run

01:21:00,929 --> 01:21:07,730
latency sensitive applications and there

01:21:03,050 --> 01:21:11,489
but it is the respected technology I

01:21:07,730 --> 01:21:14,159
think the shortest path is if you short

01:21:11,489 --> 01:21:17,400
circuit a packet or the orbits of right

01:21:14,159 --> 01:21:21,150
I mean that would be the ideal way to do

01:21:17,400 --> 01:21:21,960
it it's probably a low-cost way to but

01:21:21,150 --> 01:21:24,570
yeah there are certain applications

01:21:21,960 --> 01:21:26,190
which we might short-circuit that way

01:21:24,570 --> 01:21:28,409
but there are other applications which

01:21:26,190 --> 01:21:29,909
actually might go to user space so the

01:21:28,409 --> 01:21:32,730
different aspects of different types of

01:21:29,909 --> 01:21:34,980
workloads depends on where these

01:21:32,730 --> 01:21:38,460
applications are situated what kind of

01:21:34,980 --> 01:21:40,260
stuff they are running so yeah it's it's

01:21:38,460 --> 01:21:42,210
all over the place but we only care

01:21:40,260 --> 01:21:44,190
about the entire latency if we can

01:21:42,210 --> 01:21:47,070
improve it that is awesome

01:21:44,190 --> 01:21:50,429
today when we see traffic coming from

01:21:47,070 --> 01:21:53,489
client device to internet we and all

01:21:50,429 --> 01:21:54,150
operators we see more than 40

01:21:53,489 --> 01:21:56,700
milliseconds

01:21:54,150 --> 01:21:59,610
and that's the babe benchmark we have

01:21:56,700 --> 01:22:02,010
once you bring the clown light inside

01:21:59,610 --> 01:22:04,680
the operator it drops significantly to

01:22:02,010 --> 01:22:06,720
2025 milliseconds so that's good enough

01:22:04,680 --> 01:22:19,950
to run for an application but we need to

01:22:06,720 --> 01:22:22,340
do better than that what was the

01:22:19,950 --> 01:22:22,340
question again

01:22:26,450 --> 01:22:32,820
you mean after the on the public level

01:22:29,690 --> 01:22:36,270
yes so it varies greatly depending on

01:22:32,820 --> 01:22:39,000
the region and also the 40 millisecond

01:22:36,270 --> 01:22:41,280
I'm actually measuring is from client

01:22:39,000 --> 01:22:43,040
device to the Internet router before it

01:22:41,280 --> 01:22:46,770
addresses to internet exchange point

01:22:43,040 --> 01:22:50,220
yeah the minimum is 40 milliseconds or

01:22:46,770 --> 01:22:52,080
so unless somebody has adopted a

01:22:50,220 --> 01:22:59,070
breakout strategy and then also it

01:22:52,080 --> 01:23:05,180
doesn't yeah so most of the traffic we

01:22:59,070 --> 01:23:09,150
serve is HTTP so the latency is mostly

01:23:05,180 --> 01:23:11,400
incurred by the network we try to run

01:23:09,150 --> 01:23:14,910
our data centers let's say within 10

01:23:11,400 --> 01:23:16,770
milliseconds to everyone it is possible

01:23:14,910 --> 01:23:19,560
and fairly easy in the certain regions

01:23:16,770 --> 01:23:21,210
like Europe or or us it's very hard to

01:23:19,560 --> 01:23:24,120
put it in Philippines when everyone's on

01:23:21,210 --> 01:23:25,980
the wireless over multiple links so it

01:23:24,120 --> 01:23:28,830
depends in the region there's Pacific

01:23:25,980 --> 01:23:30,420
guarantees you want but this also ties

01:23:28,830 --> 01:23:32,840
back to the kind of technical discussion

01:23:30,420 --> 01:23:36,060
on kernel bypass if I can speed up the

01:23:32,840 --> 01:23:39,390
kernel packet processing time by 50

01:23:36,060 --> 01:23:43,850
micros that's great it the rule is that

01:23:39,390 --> 01:23:47,490
at all I had a second question about

01:23:43,850 --> 01:23:50,220
what to put is required in the context

01:23:47,490 --> 01:23:52,850
of the latency and would you be able to

01:23:50,220 --> 01:23:52,850
comment on that

01:23:54,440 --> 01:24:08,810
butyou can get pick a number we we are

01:24:06,680 --> 01:24:10,310
able to separate our our machines into

01:24:08,810 --> 01:24:11,330
this Oh

01:24:10,310 --> 01:24:21,470
are you asking what about the user

01:24:11,330 --> 01:24:25,100
expectations or oh we are serving plenty

01:24:21,470 --> 01:24:28,060
of data we are each server is fairly

01:24:25,100 --> 01:24:34,550
busy and it's sending gigabytes or

01:24:28,060 --> 01:24:36,830
gigabit we historically run on each

01:24:34,550 --> 01:24:38,540
server was cut to ten gigs because of

01:24:36,830 --> 01:24:40,010
that's going to you're having we are not

01:24:38,540 --> 01:24:41,990
usually going up to ten weeks but it

01:24:40,010 --> 01:24:44,990
depends on the patterns if the person is

01:24:41,990 --> 01:24:47,720
power so it can go quite high but you

01:24:44,990 --> 01:24:56,960
usually expect a couple of gigs per

01:24:47,720 --> 01:25:01,100
server thanks all right I think we will

01:24:56,960 --> 01:25:04,010
probably stop here I would like to ask

01:25:01,100 --> 01:25:06,590
how do you travel so the xdp programs I

01:25:04,010 --> 01:25:09,130
mean since your driver space and not in

01:25:06,590 --> 01:25:11,870
the cabinet space so you don't have a

01:25:09,130 --> 01:25:17,510
typical utilities that you deal with a

01:25:11,870 --> 01:25:19,100
kernel I'm not in DB expert but today

01:25:17,510 --> 01:25:24,470
five o'clock Mike Leake will tell you

01:25:19,100 --> 01:25:26,930
exactly okay a high-level question so

01:25:24,470 --> 01:25:29,210
with the with the advance of the

01:25:26,930 --> 01:25:31,760
internet services and clouds and

01:25:29,210 --> 01:25:34,460
everything privacy started to be a

01:25:31,760 --> 01:25:35,930
concern and Linux was built and deep

01:25:34,460 --> 01:25:38,510
hole open-source movement was built

01:25:35,930 --> 01:25:38,960
around freedoms and privacy and all that

01:25:38,510 --> 01:25:41,450
stuff

01:25:38,960 --> 01:25:44,260
MIT's like some the presentation here

01:25:41,450 --> 01:25:48,080
being about fetching acquiesce to

01:25:44,260 --> 01:25:50,450
precise location of people within

01:25:48,080 --> 01:25:54,970
accuracy that we needed for the service

01:25:50,450 --> 01:25:57,890
and actually fighting actively users

01:25:54,970 --> 01:26:00,140
that try to protect their privacy by

01:25:57,890 --> 01:26:03,800
hitting the altercation so my question

01:26:00,140 --> 01:26:07,460
is how can be as the news community help

01:26:03,800 --> 01:26:07,690
you to protect or care about privacy of

01:26:07,460 --> 01:26:13,780
your

01:26:07,690 --> 01:26:18,760
more very succeed on this one I'm

01:26:13,780 --> 01:26:20,650
actually gonna be okay so the problem

01:26:18,760 --> 01:26:22,600
what you saw what we were doing with the

01:26:20,650 --> 01:26:26,980
location verification is a common

01:26:22,600 --> 01:26:29,650
problem it's not something that we have

01:26:26,980 --> 01:26:31,630
any access to what your number is or

01:26:29,650 --> 01:26:35,140
what content you're viewing that's not

01:26:31,630 --> 01:26:37,210
the idea of it then you actually connect

01:26:35,140 --> 01:26:39,610
your device to mobile network by very

01:26:37,210 --> 01:26:41,830
nature of connecting that they have to

01:26:39,610 --> 01:26:43,870
provide you a service on a tower they

01:26:41,830 --> 01:26:45,670
need to have that information they do

01:26:43,870 --> 01:26:47,770
not expose us any of your location

01:26:45,670 --> 01:26:50,380
credentials there's a sophisticated

01:26:47,770 --> 01:26:52,720
mechanism that a token is exchanged is

01:26:50,380 --> 01:26:55,390
completely opaque to us who the customer

01:26:52,720 --> 01:26:57,250
is the only thing which is happening is

01:26:55,390 --> 01:27:00,420
if somebody has a leakage and their

01:26:57,250 --> 01:27:03,640
business model and people are actually

01:27:00,420 --> 01:27:06,220
exploiting it you could avoid that by

01:27:03,640 --> 01:27:08,470
saying yes this person and this is no

01:27:06,220 --> 01:27:10,330
person by the way this is this token is

01:27:08,470 --> 01:27:13,120
somewhere in this location that's all it

01:27:10,330 --> 01:27:16,150
is so there's no privacy encroachment

01:27:13,120 --> 01:27:18,910
for say here we don't know who the user

01:27:16,150 --> 01:27:22,150
is we just know the applications we

01:27:18,910 --> 01:27:23,530
offload and most of the times even

01:27:22,150 --> 01:27:26,800
though the application content is

01:27:23,530 --> 01:27:28,620
encrypted so what you're seeing in the

01:27:26,800 --> 01:27:31,270
way we are laying out our infrastructure

01:27:28,620 --> 01:27:33,640
it's actually like lower layer for load

01:27:31,270 --> 01:27:37,960
balancing it's not like typical layer 7

01:27:33,640 --> 01:27:41,440
stuff we don't we don't see any yeah ok

01:27:37,960 --> 01:27:44,230
when we're at the panelist won't respond

01:27:41,440 --> 01:27:46,330
to that others who know more questions

01:27:44,230 --> 01:27:51,700
but is looking at the data right right

01:27:46,330 --> 01:27:53,620
you're they add a each guy yes ok so no

01:27:51,700 --> 01:27:55,600
more questions please take it outside

01:27:53,620 --> 01:27:57,130
these guys are eval now what we'll do

01:27:55,600 --> 01:27:57,760
next time is maybe you should make this

01:27:57,130 --> 01:28:01,330
two hours

01:27:57,760 --> 01:28:04,000
it's a we this was an experiment we'll

01:28:01,330 --> 01:28:06,190
try to bring different types of

01:28:04,000 --> 01:28:10,540
industries here there's and talk about

01:28:06,190 --> 01:28:11,200
did people like this kind of panel say

01:28:10,540 --> 01:28:12,640
yeah

01:28:11,200 --> 01:28:14,380
well send us an email if you don't like

01:28:12,640 --> 01:28:16,070
it because otherwise we can assume you

01:28:14,380 --> 01:28:20,710
like it right

01:28:16,070 --> 01:28:23,240
or talk ah what's that

01:28:20,710 --> 01:28:26,410
okay no more questions okay let's give

01:28:23,240 --> 01:28:26,410

YouTube URL: https://www.youtube.com/watch?v=ZQsbYmdMjnw


