Title: Scaling with multiple network namespaces in a single application - PJ Waskiewicz
Publication date: 2016-10-07
Playlist: Netdev 1.2 - Day 2 - Thursday October 6, 2016
Description: 
	http://netdevconf.org/1.2/session.html?pj-waskiewicz
Captions: 
	00:00:10,670 --> 00:00:15,600
all right good morning thank you for

00:00:13,650 --> 00:00:17,120
making it out this morning it's always

00:00:15,600 --> 00:00:19,970
fun to have the first present

00:00:17,120 --> 00:00:21,290
of the after the first day of the

00:00:19,970 --> 00:00:23,300
conference to see how many people

00:00:21,290 --> 00:00:26,320
actually survived the first night so I'm

00:00:23,300 --> 00:00:29,570
glad this the good number of people here

00:00:26,320 --> 00:00:30,860
so yeah I'm PJ wants cabbage I'm a

00:00:29,570 --> 00:00:33,500
principal engineer and the platform

00:00:30,860 --> 00:00:37,489
group of solidfire's division within

00:00:33,500 --> 00:00:41,210
NetApp and today I wanted to come in and

00:00:37,489 --> 00:00:44,180
share some of our experiences with using

00:00:41,210 --> 00:00:47,300
network namespaces in a way that we

00:00:44,180 --> 00:00:49,460
found wasn't very typical at least from

00:00:47,300 --> 00:00:51,890
what we could find to solve some

00:00:49,460 --> 00:00:54,079
problems within our storage appliance

00:00:51,890 --> 00:00:56,540
and some of the challenges we came

00:00:54,079 --> 00:01:00,879
across how we worked around some of

00:00:56,540 --> 00:01:05,000
those some techniques for scaling and

00:01:00,879 --> 00:01:07,250
some areas that we found might be good

00:01:05,000 --> 00:01:11,600
areas to focus on within the community

00:01:07,250 --> 00:01:13,369
to try and improve the API and some of

00:01:11,600 --> 00:01:17,350
the plumbing that's exposed by the

00:01:13,369 --> 00:01:19,880
network namespaces but we'll get to that

00:01:17,350 --> 00:01:23,209
and if anyone has questions please feel

00:01:19,880 --> 00:01:24,380
free to ask just please raise your hands

00:01:23,209 --> 00:01:26,600
so that we don't have the whole room

00:01:24,380 --> 00:01:31,009
shouting microphone as you're speaking

00:01:26,600 --> 00:01:35,090
so we can get a mic to you all right so

00:01:31,009 --> 00:01:37,490
the obligatory agenda slide so I'm gonna

00:01:35,090 --> 00:01:41,000
cover why why did we get to using

00:01:37,490 --> 00:01:44,330
Network namespaces now for the sake of

00:01:41,000 --> 00:01:48,500
time I cut down some of that but in the

00:01:44,330 --> 00:01:51,649
paper there are some examples of some

00:01:48,500 --> 00:01:53,660
things that we tried and I'll kind of

00:01:51,649 --> 00:01:56,090
allude to them but I won't go into you

00:01:53,660 --> 00:01:58,959
know great detail but I have some fancy

00:01:56,090 --> 00:02:04,039
pictures in the in the paper as to what

00:01:58,959 --> 00:02:05,509
what got us to where we're at today so

00:02:04,039 --> 00:02:07,399
what is our application what was the

00:02:05,509 --> 00:02:10,630
need for our application to use this and

00:02:07,399 --> 00:02:13,130
then what did we end up coming up with

00:02:10,630 --> 00:02:14,870
and just in case people don't know how

00:02:13,130 --> 00:02:17,599
to programmatically use on network

00:02:14,870 --> 00:02:18,920
namespaces you might use them on the

00:02:17,599 --> 00:02:20,720
command line or you might use them

00:02:18,920 --> 00:02:23,180
within another framework such as that

00:02:20,720 --> 00:02:25,909
let's see your docker I'd like to kind

00:02:23,180 --> 00:02:27,409
of level set on how would you actually

00:02:25,909 --> 00:02:30,030
do some lifecycle management of a

00:02:27,409 --> 00:02:31,830
network namespace so Ida created how

00:02:30,030 --> 00:02:34,260
you name it how do you refer to it

00:02:31,830 --> 00:02:37,680
within an application how do you switch

00:02:34,260 --> 00:02:38,910
to it and from there how do you start

00:02:37,680 --> 00:02:40,200
doing things more efficiently when

00:02:38,910 --> 00:02:43,350
you're dealing with very large numbers

00:02:40,200 --> 00:02:44,880
of namespaces and there are some

00:02:43,350 --> 00:02:48,300
techniques that we we came up with that

00:02:44,880 --> 00:02:50,160
we'll share on this presentation with

00:02:48,300 --> 00:02:51,660
some data that's not in the presentation

00:02:50,160 --> 00:02:55,170
but I can I can go ahead and quote it

00:02:51,660 --> 00:02:57,150
it's it's very loose data that I'm not

00:02:55,170 --> 00:03:05,100
allowed to publish but I can speak about

00:02:57,150 --> 00:03:08,489
it and so we'll talk about that and for

00:03:05,100 --> 00:03:11,160
our application we are more of an

00:03:08,489 --> 00:03:13,739
appliance so we get the the bane of the

00:03:11,160 --> 00:03:16,319
existence of getting to run on what I'll

00:03:13,739 --> 00:03:18,420
call an ancient crusty kernel we're

00:03:16,319 --> 00:03:20,610
still running on a 3/8 kernel so we ran

00:03:18,420 --> 00:03:24,150
into some problems with some network

00:03:20,610 --> 00:03:25,620
namespace plumbing in the kernel that

00:03:24,150 --> 00:03:28,530
was fixed later and I wanted to call

00:03:25,620 --> 00:03:31,440
that out to show just how much of a

00:03:28,530 --> 00:03:32,430
massive impact the performance was and

00:03:31,440 --> 00:03:34,290
that'll kind of shift some of the

00:03:32,430 --> 00:03:35,850
discussion into some proposed

00:03:34,290 --> 00:03:39,420
improvements so that we can try and

00:03:35,850 --> 00:03:41,760
prevent changes in that space from

00:03:39,420 --> 00:03:48,959
causing scalability problems again in

00:03:41,760 --> 00:03:51,900
the future ok so why did we want to use

00:03:48,959 --> 00:03:55,700
network namespaces so we have cloud

00:03:51,900 --> 00:03:57,780
infrastructures such as Azure an AWS

00:03:55,700 --> 00:04:01,109
some private cloud public cloud services

00:03:57,780 --> 00:04:02,459
all over the world that can let you take

00:04:01,109 --> 00:04:05,220
your application and shove it into a

00:04:02,459 --> 00:04:06,450
container or a virtual machine and

00:04:05,220 --> 00:04:08,459
that's great so if you have a little

00:04:06,450 --> 00:04:09,989
compute node or you have a web server or

00:04:08,459 --> 00:04:12,390
a database server and you need to do

00:04:09,989 --> 00:04:14,549
some kind of containerization and get it

00:04:12,390 --> 00:04:16,739
out of a traditional data center this is

00:04:14,549 --> 00:04:19,919
something that you can easily do in a

00:04:16,739 --> 00:04:22,229
lot of these frameworks today but there

00:04:19,919 --> 00:04:24,930
are some applications that will need to

00:04:22,229 --> 00:04:27,090
run inside of a public cloud instance or

00:04:24,930 --> 00:04:28,770
a private cloud instance that maybe

00:04:27,090 --> 00:04:31,169
isn't an AWS thing and maybe it's a

00:04:28,770 --> 00:04:32,490
storage appliance or maybe it is some

00:04:31,169 --> 00:04:36,260
other database appliance that's I could

00:04:32,490 --> 00:04:38,550
distribute the database server where the

00:04:36,260 --> 00:04:40,440
application itself also needs to be

00:04:38,550 --> 00:04:42,350
aware of how to operate in the cloud

00:04:40,440 --> 00:04:45,930
meaning that it has to serve

00:04:42,350 --> 00:04:47,310
multiple tenants or multiple users so

00:04:45,930 --> 00:04:50,190
that you have some core logic with the

00:04:47,310 --> 00:04:52,770
application that manages some kind of

00:04:50,190 --> 00:04:55,080
business logic that's managing the the

00:04:52,770 --> 00:04:56,610
tenancy but then at the networking layer

00:04:55,080 --> 00:05:00,120
that's where you need to actually

00:04:56,610 --> 00:05:02,160
enforce some of the containerization if

00:05:00,120 --> 00:05:05,070
that's even a word but you need to

00:05:02,160 --> 00:05:09,919
enforce some of that strict separation

00:05:05,070 --> 00:05:12,810
of each tenant for security reasons but

00:05:09,919 --> 00:05:14,930
in our case it was actually because we

00:05:12,810 --> 00:05:17,039
needed separate layer three stacks where

00:05:14,930 --> 00:05:20,099
the problem we were trying to solve this

00:05:17,039 --> 00:05:22,710
very vrf issue and there is a vrf talk

00:05:20,099 --> 00:05:24,449
later on in this conference so I would

00:05:22,710 --> 00:05:27,750
I'm actually pretty interested in seeing

00:05:24,449 --> 00:05:30,229
that but we would we were trying to

00:05:27,750 --> 00:05:32,820
solve a problem where we had multiple

00:05:30,229 --> 00:05:35,250
tenants that had overlapping IP address

00:05:32,820 --> 00:05:36,539
faces so we needed multiple network

00:05:35,250 --> 00:05:39,509
stacks that were separated from one

00:05:36,539 --> 00:05:46,590
another and be able to have our core

00:05:39,509 --> 00:05:49,770
application does not care about that so

00:05:46,590 --> 00:05:52,050
I like pictures and you know that I drew

00:05:49,770 --> 00:05:53,909
this because it's very basic I'd like to

00:05:52,050 --> 00:05:58,530
stick figure Pig pictures because it's

00:05:53,909 --> 00:06:01,699
not from a marketing person but this is

00:05:58,530 --> 00:06:05,340
a very high-level overview of our

00:06:01,699 --> 00:06:07,080
storied appliance architecture this is

00:06:05,340 --> 00:06:08,610
very similar to something like Steph if

00:06:07,080 --> 00:06:11,970
you've seen this but this is also the

00:06:08,610 --> 00:06:13,530
SolidFire architecture and so here we

00:06:11,970 --> 00:06:15,330
have some kind of database it's

00:06:13,530 --> 00:06:17,220
distributed database within a clustered

00:06:15,330 --> 00:06:20,550
environment we have a bunch of nodes and

00:06:17,220 --> 00:06:23,190
a cluster and this contains metadata for

00:06:20,550 --> 00:06:24,930
where volumes live on which nodes in the

00:06:23,190 --> 00:06:28,259
cluster all right so this is just a

00:06:24,930 --> 00:06:29,849
loose cluster of commodity hardware with

00:06:28,259 --> 00:06:33,900
a bunch of SSDs rammed into them and

00:06:29,849 --> 00:06:36,120
each one has volumes at posts we have

00:06:33,900 --> 00:06:40,979
some cluster management software music

00:06:36,120 --> 00:06:42,360
zookeeper for this and so we have this

00:06:40,979 --> 00:06:43,889
running on all the nodes as well and

00:06:42,360 --> 00:06:45,720
then on each node we have a thread pool

00:06:43,889 --> 00:06:48,509
we have a generic set of threads that

00:06:45,720 --> 00:06:49,919
just do general-purpose work whether

00:06:48,509 --> 00:06:53,909
it's network processing whether it's our

00:06:49,919 --> 00:06:55,349
pcs intra cluster or if it's actual

00:06:53,909 --> 00:06:56,130
database transactions that we need to be

00:06:55,349 --> 00:06:59,669
profs

00:06:56,130 --> 00:07:02,009
and IO and then we have a nice cozy

00:06:59,669 --> 00:07:05,820
stack that we run in userspace currently

00:07:02,009 --> 00:07:09,090
and then down into the kernel we're

00:07:05,820 --> 00:07:12,090
listening on well-known port 30 to 60 on

00:07:09,090 --> 00:07:14,810
top of a network stack so this is pre

00:07:12,090 --> 00:07:20,630
Network namespace this is what we had

00:07:14,810 --> 00:07:25,919
and jumping to what we now have

00:07:20,630 --> 00:07:26,970
hopefully people can see that okay kind

00:07:25,919 --> 00:07:30,180
of walk through this a little bit we

00:07:26,970 --> 00:07:31,860
have the same business logic up here the

00:07:30,180 --> 00:07:34,470
core logic the vol database the cluster

00:07:31,860 --> 00:07:38,669
management the thread pool these

00:07:34,470 --> 00:07:40,740
remained unchanged some of the the

00:07:38,669 --> 00:07:42,660
starting points that we had were how do

00:07:40,740 --> 00:07:44,669
we make these separate network stacks

00:07:42,660 --> 00:07:49,080
right so each one of these is a network

00:07:44,669 --> 00:07:50,639
namespace call it one two and three if

00:07:49,080 --> 00:07:52,550
you can envision the network namespaces

00:07:50,639 --> 00:07:55,380
we've we thought well maybe we can fork

00:07:52,550 --> 00:07:56,759
one of these processes off so we can

00:07:55,380 --> 00:07:59,849
have two processes running on two

00:07:56,759 --> 00:08:02,610
network namespaces and that turned into

00:07:59,849 --> 00:08:05,150
be an IPC nightmare right because all of

00:08:02,610 --> 00:08:07,530
a sudden now your core logic is affected

00:08:05,150 --> 00:08:09,300
by having to do some kind of IPC's

00:08:07,530 --> 00:08:12,360
synchronization of the volume database

00:08:09,300 --> 00:08:13,860
the cluster management and for pretty

00:08:12,360 --> 00:08:16,130
complex applications that are trying to

00:08:13,860 --> 00:08:19,590
you know enforce a tax OS type

00:08:16,130 --> 00:08:20,880
environment for your database that just

00:08:19,590 --> 00:08:23,010
wasn't something that was going to work

00:08:20,880 --> 00:08:26,789
that would have too big of an impact on

00:08:23,010 --> 00:08:28,830
the core architecture one of the other

00:08:26,789 --> 00:08:31,320
approaches that we had was to actually

00:08:28,830 --> 00:08:33,870
spawn off a thread a new thread per

00:08:31,320 --> 00:08:35,729
namespace as a dedicated worker thread

00:08:33,870 --> 00:08:38,190
for any i/o coming through that network

00:08:35,729 --> 00:08:39,719
stack and that looked great it actually

00:08:38,190 --> 00:08:41,370
worked pretty well until we scale past

00:08:39,719 --> 00:08:43,919
like 16 namespaces and then everything

00:08:41,370 --> 00:08:46,230
fell on its face you know in one of

00:08:43,919 --> 00:08:49,560
these complex environments like a SEF or

00:08:46,230 --> 00:08:50,910
a pure storage or a or a SolidFire these

00:08:49,560 --> 00:08:52,230
thread pools are pretty massive right

00:08:50,910 --> 00:08:55,200
there's thousands of threads running at

00:08:52,230 --> 00:08:57,810
any given time on a node doing any bit

00:08:55,200 --> 00:08:59,579
of work that we have so having all these

00:08:57,810 --> 00:09:03,360
idle threads going up to hundreds of

00:08:59,579 --> 00:09:07,800
namespaces was really a big impact and a

00:09:03,360 --> 00:09:08,970
waste on system resources so instead we

00:09:07,800 --> 00:09:11,520
decided to

00:09:08,970 --> 00:09:14,730
go with this network namespace approach

00:09:11,520 --> 00:09:16,470
where we bound we created and then bound

00:09:14,730 --> 00:09:19,140
and listened on port thirty to sixty in

00:09:16,470 --> 00:09:20,640
each namespace and we kind of stumbled

00:09:19,140 --> 00:09:23,100
across this nice little thing where

00:09:20,640 --> 00:09:25,770
while the ports are separate they're in

00:09:23,100 --> 00:09:27,390
separate way or three domains the file

00:09:25,770 --> 00:09:29,970
descriptors that back them are in global

00:09:27,390 --> 00:09:32,760
to the host and I was like holy crap

00:09:29,970 --> 00:09:34,500
that's awesome so we can just shove all

00:09:32,760 --> 00:09:36,990
of these file descriptors into an e pole

00:09:34,500 --> 00:09:39,450
loop and whenever something needed to

00:09:36,990 --> 00:09:41,370
happen on that file descriptor we've got

00:09:39,450 --> 00:09:43,560
a notification said oh which namespace

00:09:41,370 --> 00:09:45,990
are you in switch to it and then we can

00:09:43,560 --> 00:09:47,880
start processing socket data and I'll

00:09:45,990 --> 00:09:50,010
get into more details about that and and

00:09:47,880 --> 00:09:54,030
how some of the the logic behind that

00:09:50,010 --> 00:09:55,770
works so that's the basic gist of how we

00:09:54,030 --> 00:09:58,080
got to the architecture that we're going

00:09:55,770 --> 00:10:01,670
after and now I'd like to kind of go

00:09:58,080 --> 00:10:04,680
into how do we kind of make this work so

00:10:01,670 --> 00:10:06,360
I don't like busy slides and I apologize

00:10:04,680 --> 00:10:07,590
for this but I thought to just kind of

00:10:06,360 --> 00:10:13,830
cram it all into one and then we'll walk

00:10:07,590 --> 00:10:18,450
through it a bit so for those of you

00:10:13,830 --> 00:10:19,650
that that use namespaces directly this

00:10:18,450 --> 00:10:21,780
is probably something you've seen before

00:10:19,650 --> 00:10:23,850
but if you haven't used Network

00:10:21,780 --> 00:10:25,380
namespaces then this is something that

00:10:23,850 --> 00:10:27,390
actually behaves differently than other

00:10:25,380 --> 00:10:29,550
namespaces like pit namespaces or mount

00:10:27,390 --> 00:10:32,300
namespaces where those are more

00:10:29,550 --> 00:10:34,980
programmatic through a system called

00:10:32,300 --> 00:10:37,500
direct system call interaction to create

00:10:34,980 --> 00:10:39,600
them when Network namespaces actually

00:10:37,500 --> 00:10:42,330
have the ability to be created from the

00:10:39,600 --> 00:10:46,170
command line with the IP route to suite

00:10:42,330 --> 00:10:47,760
so through net link so we use IP net NS

00:10:46,170 --> 00:10:49,830
add and then you can give it a name

00:10:47,760 --> 00:10:52,170
space name and that does some magical

00:10:49,830 --> 00:10:54,570
plumbing in the kernel and you have a

00:10:52,170 --> 00:10:57,570
net net name space thing that shows up

00:10:54,570 --> 00:11:00,480
on the file system and I'll get to that

00:10:57,570 --> 00:11:01,830
here in a second incidentally we were

00:11:00,480 --> 00:11:04,440
actually talking about this last night

00:11:01,830 --> 00:11:06,450
at the at the dinner about how this is

00:11:04,440 --> 00:11:08,190
actually potentially a security issue

00:11:06,450 --> 00:11:10,320
that needs to be addressed for secure

00:11:08,190 --> 00:11:14,440
containers so that should be a pretty

00:11:10,320 --> 00:11:16,990
interesting discussion in the future

00:11:14,440 --> 00:11:19,720
you can also use netlink directly so if

00:11:16,990 --> 00:11:22,330
you want to you know use eleven L or

00:11:19,720 --> 00:11:25,330
send that link commands yourself you can

00:11:22,330 --> 00:11:28,060
do that as well or if you're creating a

00:11:25,330 --> 00:11:30,040
new thread or a process using clone you

00:11:28,060 --> 00:11:31,630
use the clone unit flag and that will

00:11:30,040 --> 00:11:34,000
create the new namespace underneath and

00:11:31,630 --> 00:11:39,100
then it will place the thread that you

00:11:34,000 --> 00:11:40,390
just created into that new namespace so

00:11:39,100 --> 00:11:42,910
let me back up a little bit if you

00:11:40,390 --> 00:11:45,310
create the namespace with IP net NS or

00:11:42,910 --> 00:11:46,810
net link all that's doing is actually

00:11:45,310 --> 00:11:49,120
creating the namespace it hasn't

00:11:46,810 --> 00:11:51,190
actually associated anything any

00:11:49,120 --> 00:11:53,380
threader or process context into that

00:11:51,190 --> 00:11:54,820
new namespace so it's now just an empty

00:11:53,380 --> 00:11:57,220
container that is waiting for something

00:11:54,820 --> 00:12:00,640
to be put into it on that that's

00:11:57,220 --> 00:12:04,090
actually a pretty important thing to to

00:12:00,640 --> 00:12:06,190
distinguish between the clone so

00:12:04,090 --> 00:12:10,600
programmatically we we just use the

00:12:06,190 --> 00:12:12,220
command line mostly because of some GPL

00:12:10,600 --> 00:12:14,020
things with with lemon L that we're

00:12:12,220 --> 00:12:16,210
trying to sort out on our own for our

00:12:14,020 --> 00:12:17,950
application since it is a user space

00:12:16,210 --> 00:12:21,390
application that's currently closed

00:12:17,950 --> 00:12:25,290
sourced we have to watch what we link to

00:12:21,390 --> 00:12:28,540
our plan is to move towards the net link

00:12:25,290 --> 00:12:32,110
approach but this was a kind of a quick

00:12:28,540 --> 00:12:33,700
and dirty hack once you have actually

00:12:32,110 --> 00:12:36,400
created the namespace and this is where

00:12:33,700 --> 00:12:37,690
we started doing the aha moments a lot

00:12:36,400 --> 00:12:39,040
of the documentation that you'll find

00:12:37,690 --> 00:12:42,340
out there says well all of your

00:12:39,040 --> 00:12:46,870
namespaces live in proc did NS internet

00:12:42,340 --> 00:12:48,190
did mounts UTC but that means that you

00:12:46,870 --> 00:12:49,870
have to actually have a process in the

00:12:48,190 --> 00:12:52,090
namespace to figure out which namespace

00:12:49,870 --> 00:12:53,830
it's in and I found that pretty

00:12:52,090 --> 00:12:54,730
frustrating when I was trying to you

00:12:53,830 --> 00:12:57,850
know sort through some of this

00:12:54,730 --> 00:12:59,620
architecture in the early stages of this

00:12:57,850 --> 00:13:01,150
because I'm like I just created the

00:12:59,620 --> 00:13:03,070
namespace and I don't have anything in

00:13:01,150 --> 00:13:04,630
it so how do I get to it if it's if I be

00:13:03,070 --> 00:13:08,020
oh it's a chicken and egg and horse-cart

00:13:04,630 --> 00:13:10,480
problem or however you want to say it so

00:13:08,020 --> 00:13:13,900
we stumbled on that run that NS or a bar

00:13:10,480 --> 00:13:15,970
run that NS on other distros also has

00:13:13,900 --> 00:13:19,480
another kernel special file get created

00:13:15,970 --> 00:13:22,390
that happens to be the same kernel inode

00:13:19,480 --> 00:13:24,850
of the namespace that's referred to in

00:13:22,390 --> 00:13:29,000
proc kid and has

00:13:24,850 --> 00:13:31,209
for a bid that's in that namespace so we

00:13:29,000 --> 00:13:33,380
we had that aha moment that we now had a

00:13:31,209 --> 00:13:34,850
static handle that was a well-known

00:13:33,380 --> 00:13:36,820
handle that's on the file system that

00:13:34,850 --> 00:13:41,089
will be there in any network namespace

00:13:36,820 --> 00:13:43,100
that we can go ahead and open using the

00:13:41,089 --> 00:13:45,290
open system call get a file descriptor

00:13:43,100 --> 00:13:46,760
that we can then store long-term for the

00:13:45,290 --> 00:13:49,370
lifetime of the namespace within the

00:13:46,760 --> 00:13:51,410
application and then we can use that

00:13:49,370 --> 00:13:55,940
with the said NS call to switch in that

00:13:51,410 --> 00:13:58,010
the namespace when we need it to once

00:13:55,940 --> 00:13:59,240
you have a thread or a process that's in

00:13:58,010 --> 00:14:03,620
that namespace then you can go through

00:13:59,240 --> 00:14:06,709
the proc hid an S net and that becomes

00:14:03,620 --> 00:14:08,000
important here in a little bit for some

00:14:06,709 --> 00:14:09,950
of the more advanced lifecycle

00:14:08,000 --> 00:14:13,149
management of how do you get threads

00:14:09,950 --> 00:14:13,149
back to where they came from

00:14:17,269 --> 00:14:22,139
so the identification of a namespace is

00:14:20,639 --> 00:14:25,379
something that became also a challenge

00:14:22,139 --> 00:14:28,999
for us once you have the name and you

00:14:25,379 --> 00:14:31,499
run that NS namespace one for example

00:14:28,999 --> 00:14:33,269
you can open it get a file descriptor

00:14:31,499 --> 00:14:37,739
you know some opaque handle that you

00:14:33,269 --> 00:14:39,539
hold on to for a while and the reason

00:14:37,739 --> 00:14:41,359
that we did the opaque handle that we

00:14:39,539 --> 00:14:44,069
held on to the lifetime of the namespace

00:14:41,359 --> 00:14:46,829
was because we don't know when that

00:14:44,069 --> 00:14:48,089
namespace might go away and we also

00:14:46,829 --> 00:14:49,799
don't want to do some sort of

00:14:48,089 --> 00:14:51,749
synchronization between threads saying

00:14:49,799 --> 00:14:53,369
okay which one currently has this thing

00:14:51,749 --> 00:14:55,979
open we didn't have want to have like

00:14:53,369 --> 00:14:58,349
500 references open to this namespace

00:14:55,979 --> 00:14:59,999
that we're trying to use it while some

00:14:58,349 --> 00:15:02,189
other context is trying to tear it down

00:14:59,999 --> 00:15:03,449
so we said grab one file descriptors

00:15:02,189 --> 00:15:04,889
stored off as metadata in the

00:15:03,449 --> 00:15:09,049
application and then we'll use it later

00:15:04,889 --> 00:15:11,189
on whenever any thread needs to switch

00:15:09,049 --> 00:15:12,959
that presents a different challenge of

00:15:11,189 --> 00:15:14,639
how do you know that that's the

00:15:12,959 --> 00:15:17,299
namespace that you actually want to be

00:15:14,639 --> 00:15:19,439
in because by file descriptor a name

00:15:17,299 --> 00:15:20,970
that may not be sufficient if you

00:15:19,439 --> 00:15:22,679
destroy the namespace and then recreate

00:15:20,970 --> 00:15:23,909
a namespace of the same name you can

00:15:22,679 --> 00:15:25,979
still get the same file descriptor

00:15:23,909 --> 00:15:28,349
integer back but that will be a

00:15:25,979 --> 00:15:30,059
different handle to that namespace so

00:15:28,349 --> 00:15:32,549
you can switch into it and all sorts of

00:15:30,059 --> 00:15:34,289
things just come flying off the rails we

00:15:32,549 --> 00:15:36,599
were scratching our heads as to why are

00:15:34,289 --> 00:15:37,829
we just seeing data that is definitely

00:15:36,599 --> 00:15:40,049
not from the socket that we think it is

00:15:37,829 --> 00:15:43,889
and it turned out that we had this big

00:15:40,049 --> 00:15:45,479
you know really bad mismatch so this is

00:15:43,889 --> 00:15:48,149
one of the areas that I want to get to

00:15:45,479 --> 00:15:50,729
in some of the proposed improvements but

00:15:48,149 --> 00:15:52,379
not jumping and had too far one of the

00:15:50,729 --> 00:15:54,149
things for granularity that we came up

00:15:52,379 --> 00:15:57,659
with was we looked at the modification

00:15:54,149 --> 00:15:59,699
time or the end time from stats on run

00:15:57,659 --> 00:16:02,099
that NS namespace name so we have run

00:15:59,699 --> 00:16:03,720
that NS namespace 1 and call stat on

00:16:02,099 --> 00:16:05,749
that you get the kernel inode you get

00:16:03,720 --> 00:16:08,639
all sorts of stuff but the end time

00:16:05,749 --> 00:16:11,159
happened to be track as also the

00:16:08,639 --> 00:16:12,719
creation time so this wasn't something

00:16:11,159 --> 00:16:14,189
that changed if you made any changes to

00:16:12,719 --> 00:16:17,359
the namespace through its lifetime and

00:16:14,189 --> 00:16:21,989
the granularity of that for the actual

00:16:17,359 --> 00:16:24,149
time spec is is pretty good so if we're

00:16:21,989 --> 00:16:27,259
able to create and destroy and create

00:16:24,149 --> 00:16:30,060
namespaces you know at nanosecond levels

00:16:27,259 --> 00:16:32,220
then we'd be screwed using this approach

00:16:30,060 --> 00:16:33,780
so this goddess granularity that was

00:16:32,220 --> 00:16:37,950
good enough for what we're trying to do

00:16:33,780 --> 00:16:39,840
at this time so once we figured that out

00:16:37,950 --> 00:16:42,030
we said let's go ahead the store the end

00:16:39,840 --> 00:16:43,740
time will store the file descriptor of

00:16:42,030 --> 00:16:46,950
that that long-running opaque handle and

00:16:43,740 --> 00:16:49,530
the name and store that in some metadata

00:16:46,950 --> 00:16:51,300
for each namespace alongside the file

00:16:49,530 --> 00:16:53,940
descriptor of that TCP port that we

00:16:51,300 --> 00:16:56,370
bound in that namespace and now we know

00:16:53,940 --> 00:16:58,890
when we get an event on that that TCP

00:16:56,370 --> 00:17:00,510
socket get a file descriptor we we can

00:16:58,890 --> 00:17:02,070
look at the namespace metadata and we

00:17:00,510 --> 00:17:10,530
can know exactly which namespace we need

00:17:02,070 --> 00:17:13,040
to be in so this is pretty basic stuff

00:17:10,530 --> 00:17:15,800
but I've you know again to level set

00:17:13,040 --> 00:17:18,300
once we have all this this opaque data

00:17:15,800 --> 00:17:21,480
that that is the metadata to track the

00:17:18,300 --> 00:17:24,870
namespace switching between them is just

00:17:21,480 --> 00:17:26,760
done with a set NS system call so a very

00:17:24,870 --> 00:17:28,770
very basic example is to open file

00:17:26,760 --> 00:17:30,660
descriptor you know run that NS

00:17:28,770 --> 00:17:31,800
namespace one reopen it read-only it

00:17:30,660 --> 00:17:32,880
doesn't really matter you're not going

00:17:31,800 --> 00:17:36,060
to be able to write to it it's read-only

00:17:32,880 --> 00:17:40,050
file anyways and then you call set NS

00:17:36,060 --> 00:17:43,830
with the cloned new net flag said NS can

00:17:40,050 --> 00:17:46,140
fail you refer back to if you have a

00:17:43,830 --> 00:17:47,520
handle that was destroyed to a namespace

00:17:46,140 --> 00:17:49,200
and then you have this thing still

00:17:47,520 --> 00:17:52,980
around and you try to use that same file

00:17:49,200 --> 00:17:55,800
descriptor it will fail we ran into some

00:17:52,980 --> 00:17:58,260
other races in 3/8 that we had to work

00:17:55,800 --> 00:18:00,950
around that seemed to be fixed in at

00:17:58,260 --> 00:18:03,480
least 4.2 was the last kernel be tested

00:18:00,950 --> 00:18:05,120
we haven't searched for which change

00:18:03,480 --> 00:18:08,250
that actually fixed one of these races

00:18:05,120 --> 00:18:10,980
between creation of the namespace and

00:18:08,250 --> 00:18:12,930
being able to call set NS on it so just

00:18:10,980 --> 00:18:16,530
be aware that there might be some demons

00:18:12,930 --> 00:18:18,510
lurking but the more interesting thing

00:18:16,530 --> 00:18:20,880
is if you have an application that you

00:18:18,510 --> 00:18:22,980
have all these threads and some of these

00:18:20,880 --> 00:18:24,420
threads might be running i/o in one

00:18:22,980 --> 00:18:26,100
context and then the next time they get

00:18:24,420 --> 00:18:29,120
scheduled they might be running control

00:18:26,100 --> 00:18:32,430
plane stuff so non data path not on path

00:18:29,120 --> 00:18:34,260
so you may want to get them back to

00:18:32,430 --> 00:18:37,260
where they came from right so you might

00:18:34,260 --> 00:18:39,720
switch into namespace one but you may

00:18:37,260 --> 00:18:41,520
want them to be back in this what we

00:18:39,720 --> 00:18:43,590
call the default namespace or we refer

00:18:41,520 --> 00:18:47,010
to internally as the base namespace

00:18:43,590 --> 00:18:49,830
which is of the initial namespace when

00:18:47,010 --> 00:18:51,330
the when the system is created so the

00:18:49,830 --> 00:18:53,880
quick and dirty way has just opened up

00:18:51,330 --> 00:18:55,680
hid one's net namespace

00:18:53,880 --> 00:18:57,300
because that really should never change

00:18:55,680 --> 00:18:58,590
unless you're using pid' namespaces and

00:18:57,300 --> 00:19:00,540
you have multiple Pig ones running

00:18:58,590 --> 00:19:02,190
around and then you get to deal with the

00:19:00,540 --> 00:19:06,120
the nested hell that you created on

00:19:02,190 --> 00:19:08,310
yourself but we can store this this base

00:19:06,120 --> 00:19:09,630
namespace file descriptor as well when

00:19:08,310 --> 00:19:11,460
we're setting all of this up and you

00:19:09,630 --> 00:19:14,100
know when the application starts to say

00:19:11,460 --> 00:19:16,830
if we need to get back to this base

00:19:14,100 --> 00:19:18,990
network namespace now we have a file

00:19:16,830 --> 00:19:21,110
handle that we know how to get back to

00:19:18,990 --> 00:19:21,110
it

00:19:24,210 --> 00:19:28,840
so that's all well and good once we can

00:19:27,489 --> 00:19:31,149
identify the namespace we know how to

00:19:28,840 --> 00:19:34,989
create them and we can switch between

00:19:31,149 --> 00:19:37,090
them we started seeing some some actual

00:19:34,989 --> 00:19:41,499
data moving through the system and that

00:19:37,090 --> 00:19:44,889
was really really encouraging however

00:19:41,499 --> 00:19:46,889
and the ordering of the slides that the

00:19:44,889 --> 00:19:48,700
next one talks about some some horrible

00:19:46,889 --> 00:19:52,210
scalability problems that we ran into

00:19:48,700 --> 00:19:55,619
with within the kernel plumbing I'll get

00:19:52,210 --> 00:19:58,269
to that in a second once we solved that

00:19:55,619 --> 00:20:00,369
and now we're dealing with optimizations

00:19:58,269 --> 00:20:01,749
of using the set NS system call and

00:20:00,369 --> 00:20:04,419
using all this metadata that we've built

00:20:01,749 --> 00:20:07,090
up we ran into some problems with

00:20:04,419 --> 00:20:09,429
scalability because for every single

00:20:07,090 --> 00:20:11,409
thread in our application like I said it

00:20:09,429 --> 00:20:12,820
might be running control plane stuff it

00:20:11,409 --> 00:20:15,309
might be running database transactions

00:20:12,820 --> 00:20:17,099
might be running our pcs within nodes in

00:20:15,309 --> 00:20:19,989
the cluster or it might be running i/o

00:20:17,099 --> 00:20:22,179
we didn't know whenever any thread was

00:20:19,989 --> 00:20:23,919
going to be in that context so we just

00:20:22,179 --> 00:20:26,349
said screw it we'll call set NS on

00:20:23,919 --> 00:20:29,590
everything and it worked

00:20:26,349 --> 00:20:31,440
it worked I mean IO ran and that was

00:20:29,590 --> 00:20:34,779
fantastic

00:20:31,440 --> 00:20:37,749
but we ran into a problem where the

00:20:34,779 --> 00:20:38,830
overhead for the context switch even if

00:20:37,749 --> 00:20:41,159
we didn't have to switch in the

00:20:38,830 --> 00:20:43,929
namespace was killing our performance

00:20:41,159 --> 00:20:45,369
and this is the data that I can't put in

00:20:43,929 --> 00:20:46,599
a slide because then it's published but

00:20:45,369 --> 00:20:49,090
I can talk about it I guess it's on a

00:20:46,599 --> 00:20:54,840
video but I can just deny it it seems to

00:20:49,090 --> 00:20:58,570
work for politics so what we did was we

00:20:54,840 --> 00:21:01,749
took that same file descriptor metadata

00:20:58,570 --> 00:21:03,519
and name at the end time and put it into

00:21:01,749 --> 00:21:06,099
thread local storage for each thread in

00:21:03,519 --> 00:21:07,749
our system so whenever the thread got

00:21:06,099 --> 00:21:09,820
scheduled and we had some a poll event

00:21:07,749 --> 00:21:13,210
or we had an inline send or an inline

00:21:09,820 --> 00:21:16,299
receive that we're waiting on we can go

00:21:13,210 --> 00:21:18,039
look at the socket we can pull out the

00:21:16,299 --> 00:21:20,649
the metadata of the namespace that that

00:21:18,039 --> 00:21:21,879
socket needed to be in and then compare

00:21:20,649 --> 00:21:24,820
it with the thread local storage of

00:21:21,879 --> 00:21:27,070
where was this thread last look at them

00:21:24,820 --> 00:21:31,509
and say holy crap they match don't call

00:21:27,070 --> 00:21:34,929
set honest so to give some context

00:21:31,509 --> 00:21:35,909
around what the impact was on our

00:21:34,929 --> 00:21:38,549
systems

00:21:35,909 --> 00:21:40,679
our basis cluster the one that most of

00:21:38,549 --> 00:21:43,729
my testing was done on it's a two

00:21:40,679 --> 00:21:47,879
hundred thousand IAP by scuzzy target

00:21:43,729 --> 00:21:53,669
just for machines pretty basic Fork a

00:21:47,879 --> 00:21:56,489
lot sizes prior to doing this efficient

00:21:53,669 --> 00:21:58,289
namespace switching we were running at

00:21:56,489 --> 00:22:02,249
about anywhere from a fifteen to forty

00:21:58,289 --> 00:22:04,559
percent hit in throughput at 100% CPU

00:22:02,249 --> 00:22:06,959
utilization so it was a very noticeable

00:22:04,559 --> 00:22:09,959
hit in performance and this was with 512

00:22:06,959 --> 00:22:11,489
Network namespaces using this technique

00:22:09,959 --> 00:22:13,169
with the same exact setup with the same

00:22:11,489 --> 00:22:15,809
number of namespaces we see about a 3%

00:22:13,169 --> 00:22:19,169
hit so it's a very very significant

00:22:15,809 --> 00:22:20,940
optimization something that that when I

00:22:19,169 --> 00:22:23,069
saw the numbers I couldn't believe it so

00:22:20,940 --> 00:22:25,079
we ran the test for about a week and

00:22:23,069 --> 00:22:27,359
that's when I convinced myself that that

00:22:25,079 --> 00:22:30,709
actually work so we were pretty

00:22:27,359 --> 00:22:30,709
surprised and pretty happy about that

00:22:34,410 --> 00:22:38,100
and so yeah I put that same picture up

00:22:36,930 --> 00:22:39,990
and I was supposed to talk through that

00:22:38,100 --> 00:22:44,700
with the whole process but I think we

00:22:39,990 --> 00:22:46,080
already covered that so I'll move on so

00:22:44,700 --> 00:22:49,890
this is the the biggest thing that we

00:22:46,080 --> 00:22:51,660
ran into that we had to solve and so the

00:22:49,890 --> 00:22:55,040
earlier versions of the NS proxy

00:22:51,660 --> 00:22:58,950
handling and the task struct used to be

00:22:55,040 --> 00:23:00,450
under our CEO protection and there was a

00:22:58,950 --> 00:23:04,740
discussion on the mailing list a couple

00:23:00,450 --> 00:23:05,880
years ago again old crusty kernel that

00:23:04,740 --> 00:23:07,470
discussed that there were some

00:23:05,880 --> 00:23:08,730
performance issues well they didn't

00:23:07,470 --> 00:23:13,620
really characterize what the issues were

00:23:08,730 --> 00:23:15,240
or how how severe they were and we saw

00:23:13,620 --> 00:23:17,280
as we added more Network namespaces

00:23:15,240 --> 00:23:19,620
obviously we're going to incur more

00:23:17,280 --> 00:23:24,510
switching we started seeing massive

00:23:19,620 --> 00:23:26,310
latency spikes in our i/o and Eric

00:23:24,510 --> 00:23:30,540
Biederman did fix this I want to say it

00:23:26,310 --> 00:23:33,720
was in 317 or 319 roughly so it's it's

00:23:30,540 --> 00:23:36,930
you know still pretty long ago in in

00:23:33,720 --> 00:23:39,330
terms of kernel kernel age but we back

00:23:36,930 --> 00:23:41,120
ported the the fix this was to put

00:23:39,330 --> 00:23:45,240
another task struct

00:23:41,120 --> 00:23:48,210
task lock spin lock protection to

00:23:45,240 --> 00:23:51,150
actually switch the DNS proxy structures

00:23:48,210 --> 00:23:54,120
from one namespace to another for the

00:23:51,150 --> 00:23:58,170
task and this is where we're very

00:23:54,120 --> 00:24:00,300
concerned that if people don't

00:23:58,170 --> 00:24:01,710
understand the massive amount of impact

00:24:00,300 --> 00:24:03,870
that this had we wanted to bring it up

00:24:01,710 --> 00:24:05,790
share it and say this is an area that we

00:24:03,870 --> 00:24:08,090
really really want to keep an eye on and

00:24:05,790 --> 00:24:11,520
make sure that we don't break this again

00:24:08,090 --> 00:24:16,830
so looking at some some graphs try to

00:24:11,520 --> 00:24:19,730
highlight this so this graph the y axis

00:24:16,830 --> 00:24:23,550
is latency of how long it took to switch

00:24:19,730 --> 00:24:25,140
so basically this is not the i/o this is

00:24:23,550 --> 00:24:27,210
the amount of time that it took for the

00:24:25,140 --> 00:24:28,860
set of nests to start and then for it to

00:24:27,210 --> 00:24:32,580
return for us to be able to continue

00:24:28,860 --> 00:24:36,630
processing so this is in microseconds on

00:24:32,580 --> 00:24:37,890
the y-axis and on the x-axis this is the

00:24:36,630 --> 00:24:42,810
number of network namespaces we were

00:24:37,890 --> 00:24:46,110
running the orange-ish line is the old

00:24:42,810 --> 00:24:47,110
colonel under RC protection the teal

00:24:46,110 --> 00:24:50,230
line

00:24:47,110 --> 00:24:53,110
is where the Greenline Jeff is under

00:24:50,230 --> 00:24:55,990
task lock protection and so we see about

00:24:53,110 --> 00:24:58,650
five microseconds at one namespace at

00:24:55,990 --> 00:25:02,290
sixteen tasks walk we're still at five

00:24:58,650 --> 00:25:04,630
we go up to about twelve and at sixty

00:25:02,290 --> 00:25:08,799
four we jumped up to about 150 under the

00:25:04,630 --> 00:25:10,960
old colonel but remained at five for 64

00:25:08,799 --> 00:25:12,460
now the reason I put this graph up is

00:25:10,960 --> 00:25:14,770
because it puts it into perspective when

00:25:12,460 --> 00:25:17,140
we go out to 512 namespaces where

00:25:14,770 --> 00:25:20,950
they're actually still is a data line on

00:25:17,140 --> 00:25:22,929
the x-axis that isn't the x-axis but out

00:25:20,950 --> 00:25:25,750
at 512 under task lock it's still up

00:25:22,929 --> 00:25:28,590
around 5 microseconds and we shot up to

00:25:25,750 --> 00:25:33,010
about 550 milliseconds for half a second

00:25:28,590 --> 00:25:35,080
per switch so you can imagine we're

00:25:33,010 --> 00:25:36,700
trying to run at 200,000 I ops and all

00:25:35,080 --> 00:25:39,340
of a sudden I'm seeing AI ops at like a

00:25:36,700 --> 00:25:41,169
hundred PI ops because of the amount of

00:25:39,340 --> 00:25:43,840
latency in between each i/o where we're

00:25:41,169 --> 00:25:46,150
waiting for the kernel switch so this

00:25:43,840 --> 00:25:49,390
was a very very simple fix that had a

00:25:46,150 --> 00:25:50,650
very very massive impact and that there

00:25:49,390 --> 00:25:52,809
might have been a lot of sleep and gray

00:25:50,650 --> 00:25:59,140
hair trying to figure out what the hell

00:25:52,809 --> 00:26:00,640
was going on so this leads me to kind of

00:25:59,140 --> 00:26:03,040
the wrap-up and proposed improvements

00:26:00,640 --> 00:26:05,290
now these are two things that we could

00:26:03,040 --> 00:26:07,299
come up with throughout the course of

00:26:05,290 --> 00:26:10,410
working around a lot of the issues that

00:26:07,299 --> 00:26:12,220
we ran into these are very very loose

00:26:10,410 --> 00:26:13,780
suggestions and these are more things

00:26:12,220 --> 00:26:16,059
that I'd like to have you know a broader

00:26:13,780 --> 00:26:17,470
discussion about these are definitely

00:26:16,059 --> 00:26:20,110
not things like I'm saying we you know

00:26:17,470 --> 00:26:23,740
we must do this but we definitely want

00:26:20,110 --> 00:26:26,320
to try to add some more or any given

00:26:23,740 --> 00:26:28,000
yesterday's discussion about the lack of

00:26:26,320 --> 00:26:30,340
testing of anything in the kernel

00:26:28,000 --> 00:26:31,990
networking area right now we'd like to

00:26:30,340 --> 00:26:34,960
add some kind of torture tests unit

00:26:31,990 --> 00:26:38,250
tests maybe along the lines of RCU

00:26:34,960 --> 00:26:41,230
torture tests that can help us detect

00:26:38,250 --> 00:26:43,809
performance regressions in network

00:26:41,230 --> 00:26:45,220
namespace switching so that's one area

00:26:43,809 --> 00:26:47,740
that that we would really like to focus

00:26:45,220 --> 00:26:52,110
on and you know my team back in

00:26:47,740 --> 00:26:52,110
SolidFire is is ready to try to address

00:26:52,440 --> 00:26:58,900
that's one thing but the the bigger

00:26:54,970 --> 00:27:00,550
issue that we see is we were running

00:26:58,900 --> 00:27:01,809
into this uniqueness problem so I spend

00:27:00,550 --> 00:27:03,190
a little bit of time talking about we

00:27:01,809 --> 00:27:05,110
have the file descriptor then we have

00:27:03,190 --> 00:27:08,230
the name and then we stumbled on the end

00:27:05,110 --> 00:27:10,929
time it's kind of a good workaround to

00:27:08,230 --> 00:27:14,290
help identify one of these namespaces

00:27:10,929 --> 00:27:18,100
and within a clustered environment like

00:27:14,290 --> 00:27:20,919
ours where you know you have clocks on

00:27:18,100 --> 00:27:23,320
each machine and you know we can't

00:27:20,919 --> 00:27:27,850
really rely on the fact that you know

00:27:23,320 --> 00:27:29,919
time SKUs don't happen they do and we

00:27:27,850 --> 00:27:32,169
did actually run into some kind of

00:27:29,919 --> 00:27:35,590
Heisenberg bugs very very very difficult

00:27:32,169 --> 00:27:38,559
to reproduce but some of these torture

00:27:35,590 --> 00:27:39,970
tests that our test engineers had create

00:27:38,559 --> 00:27:42,940
namespaces destroy them create them

00:27:39,970 --> 00:27:45,760
destroy them and all of a sudden we

00:27:42,940 --> 00:27:49,419
failed the Senate actually switch into a

00:27:45,760 --> 00:27:51,309
namespace and we have IO failures and we

00:27:49,419 --> 00:27:53,919
traced it to we had a clock skew that

00:27:51,309 --> 00:27:56,380
was just enough to show that the end

00:27:53,919 --> 00:27:58,240
time was the same on a new namespace

00:27:56,380 --> 00:28:00,750
that was just created and so we had a

00:27:58,240 --> 00:28:00,750
bad handle

00:28:09,680 --> 00:28:13,420
it's right up here in the front please

00:28:16,740 --> 00:28:20,640
I'm talking about that time of the

00:28:19,380 --> 00:28:25,350
flight descriptors in the proc file

00:28:20,640 --> 00:28:27,870
system this is the end time of the file

00:28:25,350 --> 00:28:30,230
of the network namespace file descriptor

00:28:27,870 --> 00:28:33,390
that we pulled from stat of proc and s

00:28:30,230 --> 00:28:35,669
exactly so they can get they can like

00:28:33,390 --> 00:28:38,490
get renewed every time basic do you I

00:28:35,669 --> 00:28:41,220
think you can't really like thing that

00:28:38,490 --> 00:28:42,690
they are stable yeah and that's what I'm

00:28:41,220 --> 00:28:44,940
saying is that so yeah I think it's

00:28:42,690 --> 00:28:48,179
actually just like if you do an echo 3

00:28:44,940 --> 00:28:49,770
to drop caches also I think that

00:28:48,179 --> 00:28:51,840
actually the program has been rebuilt

00:28:49,770 --> 00:28:53,870
those I notes and you will see you new M

00:28:51,840 --> 00:28:57,090
times no so the end time of the

00:28:53,870 --> 00:28:58,440
namespace that exists actually doesn't

00:28:57,090 --> 00:29:00,330
change over time is what we found

00:28:58,440 --> 00:29:01,950
again if we if we destroyed the

00:29:00,330 --> 00:29:04,080
namespace and created a new one but the

00:29:01,950 --> 00:29:05,220
clock skewed the end time actually could

00:29:04,080 --> 00:29:06,929
have lined up to be the one that the

00:29:05,220 --> 00:29:12,840
previous one was okay

00:29:06,929 --> 00:29:17,850
I'm surprised as well okay thanks try

00:29:12,840 --> 00:29:20,039
using the file system notify stuff yes

00:29:17,850 --> 00:29:21,960
we we use I notify to figure out when a

00:29:20,039 --> 00:29:23,640
namespace okay we have a very

00:29:21,960 --> 00:29:25,710
asynchronous way of actually creation

00:29:23,640 --> 00:29:28,669
right of the namespaces and then

00:29:25,710 --> 00:29:30,870
actually starting up the i/o path and

00:29:28,669 --> 00:29:33,179
the problem is that the I notifies can

00:29:30,870 --> 00:29:34,289
stack up right and so we process one and

00:29:33,179 --> 00:29:35,610
then we might have four in the queue

00:29:34,289 --> 00:29:38,070
behind it that's actually what happened

00:29:35,610 --> 00:29:39,870
here by the time we tried to switch into

00:29:38,070 --> 00:29:41,100
a namespace of the new one that was

00:29:39,870 --> 00:29:45,710
there we hadn't processed the new

00:29:41,100 --> 00:29:45,710
inotify yet okay

00:29:48,100 --> 00:29:52,400
so a my first three actually when I

00:29:50,300 --> 00:29:55,670
looked at your context switching issue

00:29:52,400 --> 00:29:59,540
I'm like yo why isn't set NS doing this

00:29:55,670 --> 00:30:00,590
optimization why isn't it saying oh that

00:29:59,540 --> 00:30:02,780
names these points the same return

00:30:00,590 --> 00:30:04,010
immediately um it's it's actually

00:30:02,780 --> 00:30:05,840
because what we would have to do that

00:30:04,010 --> 00:30:08,390
that up in the C library we wouldn't

00:30:05,840 --> 00:30:09,860
want to take the actual context which to

00:30:08,390 --> 00:30:11,090
get into the system call itself I

00:30:09,860 --> 00:30:12,920
understand it should have been pretty

00:30:11,090 --> 00:30:16,370
freakin fast if the namespace is the

00:30:12,920 --> 00:30:17,630
equivalent yeah it wasn't but that could

00:30:16,370 --> 00:30:19,190
be an area that we look at and try and

00:30:17,630 --> 00:30:21,200
think so because I mean I really don't

00:30:19,190 --> 00:30:23,810
think that I applications should be have

00:30:21,200 --> 00:30:25,220
to be aware this issue if they think it

00:30:23,810 --> 00:30:27,620
should be able to execute a very simple

00:30:25,220 --> 00:30:29,150
model upset and as for the operation

00:30:27,620 --> 00:30:30,230
we're about to execute and that should

00:30:29,150 --> 00:30:31,850
be reasonably fast if there's no

00:30:30,230 --> 00:30:33,320
switches involved yeah that's great

00:30:31,850 --> 00:30:35,270
input and that's something that we can

00:30:33,320 --> 00:30:39,290
certainly look at also another issue

00:30:35,270 --> 00:30:40,670
that's come up a lot is the size of all

00:30:39,290 --> 00:30:42,830
the stuff that gets attached to a new

00:30:40,670 --> 00:30:44,750
net namespace when you create it is this

00:30:42,830 --> 00:30:47,300
an issue in your realm at the scale that

00:30:44,750 --> 00:30:50,440
you're making namespaces that's a very

00:30:47,300 --> 00:30:53,230
good question the simple answer is no

00:30:50,440 --> 00:30:55,010
the machines that we actually run are

00:30:53,230 --> 00:30:57,770
running anywhere from three hundred

00:30:55,010 --> 00:30:59,540
eighty four gigs of ram to a lot more

00:30:57,770 --> 00:31:01,370
than that it used to be ridiculous we

00:30:59,540 --> 00:31:03,110
used to hang hundreds and hundreds of

00:31:01,370 --> 00:31:05,500
kilobytes would and if not megabytes

00:31:03,110 --> 00:31:07,640
worth of hash tables and whatever and

00:31:05,500 --> 00:31:09,200
what's happened over time is has been in

00:31:07,640 --> 00:31:11,660
consolidation to the point where we're

00:31:09,200 --> 00:31:14,030
like okay so this is a hash table we'll

00:31:11,660 --> 00:31:16,160
just add a namespace cookie to the key

00:31:14,030 --> 00:31:17,810
for lookups instead of having any a hash

00:31:16,160 --> 00:31:18,800
table per namespace and so that's the

00:31:17,810 --> 00:31:22,570
kind of things that are happening in

00:31:18,800 --> 00:31:27,850
that area and how important is net space

00:31:22,570 --> 00:31:27,850
creation and destroy performance for you

00:31:30,010 --> 00:31:35,870
depends on who you ask

00:31:32,320 --> 00:31:40,580
we are we are good with how it works in

00:31:35,870 --> 00:31:41,690
3/8 right now so our our concern is how

00:31:40,580 --> 00:31:43,520
well we failover

00:31:41,690 --> 00:31:45,230
in our cluster from one one machine to

00:31:43,520 --> 00:31:46,790
another if we have a machine die or

00:31:45,230 --> 00:31:47,960
something happens and you have to bring

00:31:46,790 --> 00:31:50,000
up like and then we have to bring it up

00:31:47,960 --> 00:31:51,590
like hundreds of namespaces for a new I

00:31:50,000 --> 00:31:54,200
scuzzy targets and as long as we're

00:31:51,590 --> 00:31:55,240
within a certain time bound okay and

00:31:54,200 --> 00:32:00,250
right now where are we

00:31:55,240 --> 00:32:02,560
I think okay good I just want to warn

00:32:00,250 --> 00:32:04,480
you that I recently got box that we have

00:32:02,560 --> 00:32:06,280
like worse namespace creation in

00:32:04,480 --> 00:32:08,800
Tunisian times with newer colors and

00:32:06,280 --> 00:32:10,480
result comments see it's said to be

00:32:08,800 --> 00:32:13,420
running on crusty old kernel sometimes

00:32:10,480 --> 00:32:16,120
oh that's good

00:32:13,420 --> 00:32:18,010
so just to comment about the some of the

00:32:16,120 --> 00:32:18,850
scalability issues and architectural

00:32:18,010 --> 00:32:21,910
design issues

00:32:18,850 --> 00:32:23,890
that's why myself and then my company

00:32:21,910 --> 00:32:25,960
cumulus has been saying that a namespace

00:32:23,890 --> 00:32:28,480
is very heavyweight solution for a verb

00:32:25,960 --> 00:32:30,610
and that's why we have the vrf

00:32:28,480 --> 00:32:32,770
implementation that was accepted into

00:32:30,610 --> 00:32:34,300
the 4.3 and forward kernels is

00:32:32,770 --> 00:32:36,100
specifically for some of these design

00:32:34,300 --> 00:32:38,140
problems that that you're experiencing

00:32:36,100 --> 00:32:41,020
yeah when I when I saw that you're

00:32:38,140 --> 00:32:43,690
you're giving the talk later so I'm very

00:32:41,020 --> 00:32:45,010
interested to see that talk because we

00:32:43,690 --> 00:32:46,740
don't have that infrastructure available

00:32:45,010 --> 00:32:49,690
and the kernels that we have right now

00:32:46,740 --> 00:32:50,890
and so this is something where if we got

00:32:49,690 --> 00:32:52,480
something like this at least the

00:32:50,890 --> 00:32:54,160
plumbing in place and the application

00:32:52,480 --> 00:32:57,100
right we can move it to something a

00:32:54,160 --> 00:32:59,080
little bit more general purpose in newer

00:32:57,100 --> 00:33:05,170
kernels that's I'm very interested to

00:32:59,080 --> 00:33:06,910
see what what what's going on there so

00:33:05,170 --> 00:33:08,080
anyway so the uniqueness issue is

00:33:06,910 --> 00:33:10,660
something that I think we still need to

00:33:08,080 --> 00:33:12,550
solve I just threw out kind of a UUID

00:33:10,660 --> 00:33:15,700
like mechanism that we might expose

00:33:12,550 --> 00:33:17,920
through proc or something it's

00:33:15,700 --> 00:33:19,060
definitely something that bit us and I

00:33:17,920 --> 00:33:22,630
think it's something that's that's worth

00:33:19,060 --> 00:33:24,820
solving and so I I'm hoping to bring

00:33:22,630 --> 00:33:26,890
this to the the list and just get some

00:33:24,820 --> 00:33:28,600
discussion around and get some ideas

00:33:26,890 --> 00:33:30,610
maybe some hallway conversations here to

00:33:28,600 --> 00:33:37,990
just brainstorm and see if people have

00:33:30,610 --> 00:33:40,210
sums and thoughts on this don't you

00:33:37,990 --> 00:33:43,540
think and pass the inode out of the

00:33:40,210 --> 00:33:47,380
bracket Satan because right now we're

00:33:43,540 --> 00:33:50,080
not tracking the I nodes and we were

00:33:47,380 --> 00:33:51,910
actually seeing where they actually I

00:33:50,080 --> 00:33:54,130
want to take that back I don't remember

00:33:51,910 --> 00:33:56,050
if we were seeing the same inode show up

00:33:54,130 --> 00:33:58,030
I'm pretty sure that you do because that

00:33:56,050 --> 00:33:59,100
we use as NS installed which which

00:33:58,030 --> 00:34:01,620
increment which

00:33:59,100 --> 00:34:03,690
the global I need a locator and and it

00:34:01,620 --> 00:34:06,210
creates a I know then it's atomic and it

00:34:03,690 --> 00:34:10,859
should be like visible all over all the

00:34:06,210 --> 00:34:12,510
places so that's the idea dykes to use

00:34:10,859 --> 00:34:13,200
it also for for things and it worked out

00:34:12,510 --> 00:34:15,600
so far

00:34:13,200 --> 00:34:18,210
follow me so out of curiosity would you

00:34:15,600 --> 00:34:21,750
would you think it's a good idea to they

00:34:18,210 --> 00:34:25,350
hack for VAR get net violation s if the

00:34:21,750 --> 00:34:28,050
kernel magically by choice you have to

00:34:25,350 --> 00:34:31,470
say you want this feature magically out

00:34:28,050 --> 00:34:32,940
of by and mount the name a name of some

00:34:31,470 --> 00:34:35,700
phone all right

00:34:32,940 --> 00:34:38,609
like you say a UID so you don't have to

00:34:35,700 --> 00:34:41,850
go on scan proc PID or it's just there

00:34:38,609 --> 00:34:43,379
and you get I notify to say namespaces

00:34:41,850 --> 00:34:46,260
and create it what do you think that

00:34:43,379 --> 00:34:48,090
idea I think that'd be fantastic I mean

00:34:46,260 --> 00:34:49,800
that that that's a nice common way for

00:34:48,090 --> 00:34:51,270
anyone to go just subscribe to be

00:34:49,800 --> 00:34:54,119
notified and then boom you get this

00:34:51,270 --> 00:34:55,740
really notification and that was all one

00:34:54,119 --> 00:34:58,980
of the race problems that we saw of when

00:34:55,740 --> 00:35:00,300
the namespace was created that we can

00:34:58,980 --> 00:35:02,100
get that I notify when the kernel was

00:35:00,300 --> 00:35:03,359
actually done creating the namespace and

00:35:02,100 --> 00:35:05,250
getting all the plumbing in place and

00:35:03,359 --> 00:35:06,990
then we would know that we can actually

00:35:05,250 --> 00:35:09,869
start switching into it this lecture

00:35:06,990 --> 00:35:12,480
there's actually a PID that's it let's

00:35:09,869 --> 00:35:14,220
an opaque piece of data that gets put in

00:35:12,480 --> 00:35:17,250
the net in s you can actually take it

00:35:14,220 --> 00:35:19,200
yourself from the user space IP route to

00:35:17,250 --> 00:35:20,550
seems to have support for it I somebody

00:35:19,200 --> 00:35:23,490
seems to be using that but it's an

00:35:20,550 --> 00:35:26,640
opaque piece of trailer and and anybody

00:35:23,490 --> 00:35:28,410
can override it but yes if you could get

00:35:26,640 --> 00:35:30,720
like notification when namespaces get

00:35:28,410 --> 00:35:33,570
created so so we actually do the I

00:35:30,720 --> 00:35:35,340
notify on the run net NS directory like

00:35:33,570 --> 00:35:36,480
Steven was saying we ran into the

00:35:35,340 --> 00:35:38,550
problem that we were actually getting

00:35:36,480 --> 00:35:41,130
namespace file handle showing up and run

00:35:38,550 --> 00:35:42,690
that NS before they were ready which was

00:35:41,130 --> 00:35:44,280
a very interesting problem and we don't

00:35:42,690 --> 00:35:46,740
seem to see that newer kernel so we

00:35:44,280 --> 00:35:48,960
don't know where it we haven't spent the

00:35:46,740 --> 00:35:52,440
time to figure out where it changed okay

00:35:48,960 --> 00:35:54,810
all right but I'm here for the rest of

00:35:52,440 --> 00:35:55,800
the time I'm at my thirty five minutes

00:35:54,810 --> 00:35:58,280
so I don't want to cut into someone

00:35:55,800 --> 00:36:00,590
else's time please feel free to grab me

00:35:58,280 --> 00:36:05,310
asking questions have discussions

00:36:00,590 --> 00:36:07,109
threatening on stuff that we did but

00:36:05,310 --> 00:36:09,560
thank you for your time and enjoy the

00:36:07,109 --> 00:36:09,560

YouTube URL: https://www.youtube.com/watch?v=8Ux9id-Vf3U


