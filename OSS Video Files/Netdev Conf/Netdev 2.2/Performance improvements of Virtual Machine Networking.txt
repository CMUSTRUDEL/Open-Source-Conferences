Title: Performance improvements of Virtual Machine Networking
Publication date: 2018-03-14
Playlist: Netdev 2.2
Description: 
	Speaker: Jason Wang
Friday November 09th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?wang-vmperformance-talk
Captions: 
	00:00:01,319 --> 00:00:06,899
and I'm going to talk a little bit about

00:00:04,770 --> 00:00:09,809
how to improve the performance of

00:00:06,899 --> 00:00:13,559
virtual machine and for the performance

00:00:09,809 --> 00:00:17,099
I mean the packet per second so the goal

00:00:13,559 --> 00:00:18,739
is to try to forward package as much

00:00:17,099 --> 00:00:24,000
quickly as possible

00:00:18,739 --> 00:00:29,640
so here's a typical setup of the host

00:00:24,000 --> 00:00:32,790
kernel data path so on the left is the

00:00:29,640 --> 00:00:37,460
most popular configuration is to use a

00:00:32,790 --> 00:00:40,680
bridge and use tab as it's a port and

00:00:37,460 --> 00:00:42,750
the entire can talk with me host net

00:00:40,680 --> 00:00:45,270
which is doing the translation between

00:00:42,750 --> 00:00:49,140
the voir dire descriptors and host

00:00:45,270 --> 00:00:52,260
hello vectors and on the right figure is

00:00:49,140 --> 00:00:54,570
another popular configuration which used

00:00:52,260 --> 00:00:57,079
McVie tab the only difference is that

00:00:54,570 --> 00:00:58,829
mikvah tab is tightly coupled with

00:00:57,079 --> 00:01:01,800
underlayer harbor Nick

00:00:58,829 --> 00:01:05,100
so MATLAB can submit packet directly

00:01:01,800 --> 00:01:07,620
through the mix transmission routine or

00:01:05,100 --> 00:01:09,659
it can decide to submit to the Mac of a

00:01:07,620 --> 00:01:15,600
line layer to do something like reading

00:01:09,659 --> 00:01:17,400
or something like this so we were we

00:01:15,600 --> 00:01:20,729
know that we were slow but we don't know

00:01:17,400 --> 00:01:24,299
how slow we were we so this is a simple

00:01:20,729 --> 00:01:28,470
reference test so the yellow the the

00:01:24,299 --> 00:01:31,680
blue bar is the linux 4.5 plus missile

00:01:28,470 --> 00:01:35,700
net and the red bar is the test PMD

00:01:31,680 --> 00:01:38,490
which is PDK paste and use the poly mode

00:01:35,700 --> 00:01:41,130
the driver and we can see that the test

00:01:38,490 --> 00:01:44,820
PMD can get almost about 12 meetings per

00:01:41,130 --> 00:01:48,600
second but the linux 4.5 cap we can only

00:01:44,820 --> 00:01:50,729
get about round 1 mili so the

00:01:48,600 --> 00:01:54,390
competition is not fair because the test

00:01:50,729 --> 00:01:56,270
PMD passed by past the bridge so I just

00:01:54,390 --> 00:02:01,740
used these figures for your reference so

00:01:56,270 --> 00:02:04,320
in the in the numbers at the end of

00:02:01,740 --> 00:02:07,799
slides I will give you more fair

00:02:04,320 --> 00:02:10,529
competitions after XDP is used so this

00:02:07,799 --> 00:02:13,200
is agenda so I will first discuss about

00:02:10,529 --> 00:02:14,790
the be host rating model and then there

00:02:13,200 --> 00:02:17,400
are four major improvement

00:02:14,790 --> 00:02:20,430
he includes the busy pulling the

00:02:17,400 --> 00:02:21,870
improvements of tap and the patch worker

00:02:20,430 --> 00:02:24,810
processing and xep

00:02:21,870 --> 00:02:27,510
the view evaluate the performance and

00:02:24,810 --> 00:02:31,340
the talking about discuss about the

00:02:27,510 --> 00:02:34,049
to-do list so here is the current

00:02:31,340 --> 00:02:35,879
threading model used by B host night we

00:02:34,049 --> 00:02:39,510
use one worker thread and we switch

00:02:35,879 --> 00:02:43,890
between the TX and rx so this is not

00:02:39,510 --> 00:02:45,170
very good because it obviously a

00:02:43,890 --> 00:02:50,659
half-duplex

00:02:45,170 --> 00:02:54,060
implementation and it may not scale well

00:02:50,659 --> 00:02:57,629
for example if we have a heavy pair

00:02:54,060 --> 00:03:00,659
direction no traffic so here comes some

00:02:57,629 --> 00:03:04,049
several new models the first is Elvis

00:03:00,659 --> 00:03:06,659
which is presented by Xavier codon it

00:03:04,049 --> 00:03:10,940
tries to use dedicated course but if we

00:03:06,659 --> 00:03:14,010
opt for we host and then in several

00:03:10,940 --> 00:03:17,430
different VMs can share a single we host

00:03:14,010 --> 00:03:19,980
working worker thread and it has some

00:03:17,430 --> 00:03:25,680
optimizations in interrupt processing

00:03:19,980 --> 00:03:28,769
and it can also do pooling so the issue

00:03:25,680 --> 00:03:31,739
is that it needs something like I have

00:03:28,769 --> 00:03:35,250
scheduler so each VM needs at least

00:03:31,739 --> 00:03:38,940
something like a quarter so and it also

00:03:35,250 --> 00:03:43,440
likes the group spot so depending das

00:03:38,940 --> 00:03:44,970
has presented another new ideas which is

00:03:43,440 --> 00:03:47,639
which which based on the concurrent

00:03:44,970 --> 00:03:50,220
manage work you so with this method we

00:03:47,639 --> 00:03:53,730
can get all benefit from it for the

00:03:50,220 --> 00:03:56,849
modern uma and dynamic workers and it

00:03:53,730 --> 00:03:58,949
can also be sick group aware but it was

00:03:56,849 --> 00:04:03,319
a little bit expensive to switch between

00:03:58,949 --> 00:04:06,720
different groups for some reasons all

00:04:03,319 --> 00:04:08,639
the works has been stalled so if you

00:04:06,720 --> 00:04:13,680
have some more ideas pages are more than

00:04:08,639 --> 00:04:17,780
welcome and for simply see we are still

00:04:13,680 --> 00:04:21,299
using the traditional reading mode when

00:04:17,780 --> 00:04:24,810
workers write for all the traffic for

00:04:21,299 --> 00:04:28,650
the late so the first thing we can learn

00:04:24,810 --> 00:04:31,020
from PDK based solution is that

00:04:28,650 --> 00:04:33,449
the busy pulling model so instead

00:04:31,020 --> 00:04:35,760
instead of waiting for event to happen

00:04:33,449 --> 00:04:38,850
which we which is very expensive for

00:04:35,760 --> 00:04:41,970
example when guess want to transmit a

00:04:38,850 --> 00:04:45,780
packet it needs first to kick the what

00:04:41,970 --> 00:04:49,139
cues Roo MML rupiah so there will be a

00:04:45,780 --> 00:04:50,970
BM exit and then we will switch to the

00:04:49,139 --> 00:04:52,830
hypervisor mode hypervisor we would try

00:04:50,970 --> 00:04:55,740
to decode the instruction and pick up

00:04:52,830 --> 00:04:57,840
the because right so there's an another

00:04:55,740 --> 00:05:01,289
delay which will be caused by the

00:04:57,840 --> 00:05:04,620
scheduler and if the traffic comes from

00:05:01,289 --> 00:05:07,440
the receiving the Saltire queue we also

00:05:04,620 --> 00:05:11,840
try to pick up the because which is

00:05:07,440 --> 00:05:15,900
another delay caused by scheduler so

00:05:11,840 --> 00:05:18,479
what we do is that tries instead of wait

00:05:15,900 --> 00:05:22,310
for the event to happen we try to pull

00:05:18,479 --> 00:05:25,229
for for a while and we expect that

00:05:22,310 --> 00:05:28,800
something like the packet or something

00:05:25,229 --> 00:05:31,289
but the packet will be come soon so the

00:05:28,800 --> 00:05:33,539
overhead of the watch realization and

00:05:31,289 --> 00:05:37,550
the cups was totally emulated in this

00:05:33,539 --> 00:05:40,950
case so to Vernon pooling we we need to

00:05:37,550 --> 00:05:44,430
disable all the even source so instead

00:05:40,950 --> 00:05:48,570
of wait for them happens we will try to

00:05:44,430 --> 00:05:50,940
put them and we will also try to exceed

00:05:48,570 --> 00:05:53,160
busy polling if your signal body

00:05:50,940 --> 00:05:55,470
pending or obvious a high priority

00:05:53,160 --> 00:05:58,979
process become rainbow in the same

00:05:55,470 --> 00:06:03,000
process so during the micro benchmark we

00:05:58,979 --> 00:06:06,810
can see about 5 to 20% of improvement

00:06:03,000 --> 00:06:10,199
and it has some limitations so first

00:06:06,810 --> 00:06:13,560
first of all compared to the test PMD if

00:06:10,199 --> 00:06:15,449
it does not 100% visible in

00:06:13,560 --> 00:06:18,120
implementation if students wait for the

00:06:15,449 --> 00:06:21,930
first event to be happen and this could

00:06:18,120 --> 00:06:24,090
be done or improve on top and some user

00:06:21,930 --> 00:06:26,639
one may want balance between the latency

00:06:24,090 --> 00:06:28,830
and CPU consumption so we are still try

00:06:26,639 --> 00:06:31,500
to investigate a better method but

00:06:28,830 --> 00:06:36,770
visiting is good enough for us to

00:06:31,500 --> 00:06:40,919
evaluate the maximum performance now and

00:06:36,770 --> 00:06:42,389
so the next step is tries to improve the

00:06:40,919 --> 00:06:46,720
tab so tab

00:06:42,389 --> 00:06:50,169
yep Happy's has a long history even

00:06:46,720 --> 00:06:52,660
before the virtualization so it was not

00:06:50,169 --> 00:06:54,040
designed for high performance pop a

00:06:52,660 --> 00:06:57,729
packet processing

00:06:54,040 --> 00:07:00,220
so the first bottleneck we mentioned it

00:06:57,729 --> 00:07:02,340
we'd noticed is that the tap is still

00:07:00,220 --> 00:07:05,620
using the old stay of sake receive queue

00:07:02,340 --> 00:07:10,410
the problem is that the socket used if

00:07:05,620 --> 00:07:13,810
you is bubbling list so it was not very

00:07:10,410 --> 00:07:19,419
friendly so for example if we want to

00:07:13,810 --> 00:07:21,669
insert as skb it would list we need to

00:07:19,419 --> 00:07:24,250
touch for example the previous note the

00:07:21,669 --> 00:07:27,220
next note all the head so it touched

00:07:24,250 --> 00:07:32,680
several cache line it was not very good

00:07:27,220 --> 00:07:34,990
for us so during during our case we are

00:07:32,680 --> 00:07:38,050
in fact doing the first packet transfer

00:07:34,990 --> 00:07:40,720
between to process so usually between

00:07:38,050 --> 00:07:45,460
the case of the lqd and the B host an ID

00:07:40,720 --> 00:07:48,450
pro thread so one of my colleagues my

00:07:45,460 --> 00:07:52,660
christine event a very cache friendly

00:07:48,450 --> 00:07:57,490
data structure so basically it was just

00:07:52,660 --> 00:08:00,729
an array of pointers and the producers

00:07:57,490 --> 00:08:03,370
and consumers have their own index and

00:08:00,729 --> 00:08:05,349
the most interesting part is that for

00:08:03,370 --> 00:08:08,110
example the producer does not need to

00:08:05,349 --> 00:08:13,180
check the consumers index because it can

00:08:08,110 --> 00:08:17,050
it need only to verify against verify

00:08:13,180 --> 00:08:19,060
that the array entries against now so

00:08:17,050 --> 00:08:22,830
for example if we found the current

00:08:19,060 --> 00:08:31,210
entry is now so it can produce an entry

00:08:22,830 --> 00:08:36,219
so to prevent each science to access

00:08:31,210 --> 00:08:42,669
their own cache line and we align we

00:08:36,219 --> 00:08:46,630
align the the index in in in in in the

00:08:42,669 --> 00:08:49,209
cache of the specific side so the parser

00:08:46,630 --> 00:08:53,170
and consumer has their own cache line

00:08:49,209 --> 00:08:55,990
and the other side won't be touched

00:08:53,170 --> 00:08:59,200
and on top of the polar ring we

00:08:55,990 --> 00:09:02,200
implement a wrappers for skp ponders so

00:08:59,200 --> 00:09:06,730
we replace the socket of Scipio foot hat

00:09:02,200 --> 00:09:14,260
and we caught about 50% of improvement

00:09:06,730 --> 00:09:17,320
during the micro benchmark so it works

00:09:14,260 --> 00:09:20,440
well if the consumer speed matched to

00:09:17,320 --> 00:09:23,529
put the speed of producer but this is

00:09:20,440 --> 00:09:29,230
not the case especially in our case the

00:09:23,529 --> 00:09:32,019
consumer is really slow so in this case

00:09:29,230 --> 00:09:36,160
which means that the ring is almost full

00:09:32,019 --> 00:09:39,310
so in this case we do get very seriously

00:09:36,160 --> 00:09:41,199
cache line contingent because the

00:09:39,310 --> 00:09:44,170
consumer index and producer index me

00:09:41,199 --> 00:09:51,519
refer the date in the same cache line so

00:09:44,170 --> 00:09:54,430
we need to do something to solve this so

00:09:51,519 --> 00:09:59,649
match the ring is one of the methods of

00:09:54,430 --> 00:10:02,949
this it instead of just maintain one

00:09:59,649 --> 00:10:05,589
consumer index we are now maintained to

00:10:02,949 --> 00:10:08,230
the first is the consumer head which

00:10:05,589 --> 00:10:13,630
could be treated as the real consumer

00:10:08,230 --> 00:10:15,699
index so after each packet was consumed

00:10:13,630 --> 00:10:18,279
by a consumer we will advance the

00:10:15,699 --> 00:10:22,540
consumer head but we don't note the

00:10:18,279 --> 00:10:27,690
route entry instead we track the next

00:10:22,540 --> 00:10:27,690
entry to be invalidated through another

00:10:27,720 --> 00:10:37,540
variable called consumer tail so in this

00:10:33,519 --> 00:10:39,519
case we can see that we can keep the

00:10:37,540 --> 00:10:42,699
producer a little bit far away from the

00:10:39,519 --> 00:10:44,920
consumer and we will only start right

00:10:42,699 --> 00:10:47,529
with during the entries when the

00:10:44,920 --> 00:10:51,310
consumer is through cash lying far away

00:10:47,529 --> 00:10:56,949
from the producer and we also you to the

00:10:51,310 --> 00:10:59,380
d-ring in the reverse order so the

00:10:56,949 --> 00:11:01,269
reason that we tweet in the reversals is

00:10:59,380 --> 00:11:04,360
that we need to make sure that the

00:11:01,269 --> 00:11:05,210
producer won't make any progress and it

00:11:04,360 --> 00:11:11,150
will stay

00:11:05,210 --> 00:11:15,650
at the catch line it can avoid extra

00:11:11,150 --> 00:11:18,530
cane Koechlin bonusing so after the

00:11:15,650 --> 00:11:20,500
patch tearing we can see that at that

00:11:18,530 --> 00:11:22,970
time if the producer want to produce

00:11:20,500 --> 00:11:26,510
several new entries there will be no

00:11:22,970 --> 00:11:31,600
catch line balancing at all because the

00:11:26,510 --> 00:11:36,890
consumer is very far from the producer

00:11:31,600 --> 00:11:39,170
and on top of the batch the ring we

00:11:36,890 --> 00:11:43,160
introduced the Padgett EQE so the reason

00:11:39,170 --> 00:11:46,880
for this is that in our case it it may

00:11:43,160 --> 00:11:50,240
have more multiple consumers and

00:11:46,880 --> 00:11:53,270
producers so we need to spin lock to

00:11:50,240 --> 00:11:56,480
synchronize between them and the batch

00:11:53,270 --> 00:11:59,510
the queuing is a technology to amortize

00:11:56,480 --> 00:12:03,770
the cost on the spin lock so in this

00:11:59,510 --> 00:12:05,450
case you try to be cure several pointers

00:12:03,770 --> 00:12:08,660
he is wrong

00:12:05,450 --> 00:12:10,610
so after this batch the queuing any

00:12:08,660 --> 00:12:15,140
pointers could be accessed in a lock

00:12:10,610 --> 00:12:20,540
free way so this can also reduce that

00:12:15,140 --> 00:12:24,020
catch miss firstly because the consumer

00:12:20,540 --> 00:12:27,680
is even more far away and this needs

00:12:24,020 --> 00:12:31,820
some operations from the be denied

00:12:27,680 --> 00:12:34,970
because the current API is luckily I

00:12:31,820 --> 00:12:37,670
used by tab does not support ease so the

00:12:34,970 --> 00:12:40,040
solution is that we export the point

00:12:37,670 --> 00:12:43,010
ring through B hostnet and in this night

00:12:40,040 --> 00:12:45,980
we will try to decode all the pointers

00:12:43,010 --> 00:12:50,120
of the sake buffers in so an array and

00:12:45,980 --> 00:12:51,710
each night we will try to pass the skb

00:12:50,120 --> 00:12:54,680
back to the types through the control

00:12:51,710 --> 00:12:58,400
message so in this case when we do

00:12:54,680 --> 00:13:07,040
receiving there's no spin lock at least

00:12:58,400 --> 00:13:09,200
in the receiving of the tab and the

00:13:07,040 --> 00:13:15,310
other things we can learn from the DPD k

00:13:09,200 --> 00:13:17,800
we host PMD beckon is that traditionally

00:13:15,310 --> 00:13:18,940
the business implementing Colonel does

00:13:17,800 --> 00:13:21,970
not do better at all

00:13:18,940 --> 00:13:27,760
so it'll it can only receive or transmit

00:13:21,970 --> 00:13:30,520
a packet one pack at a time but we also

00:13:27,760 --> 00:13:35,440
PMD that does the batching very

00:13:30,520 --> 00:13:38,080
aggressively so let's see why so this is

00:13:35,440 --> 00:13:41,740
the this is the worst case for the cache

00:13:38,080 --> 00:13:47,380
misses for each what queue access so

00:13:41,740 --> 00:13:50,320
when you try to for exam how to process

00:13:47,380 --> 00:13:54,430
are expected from the point of view of

00:13:50,320 --> 00:13:57,670
the we host night it will cost at most

00:13:54,430 --> 00:14:02,260
file cache misses the first is try to

00:13:57,670 --> 00:14:05,410
read available index and the second is

00:14:02,260 --> 00:14:07,810
that we read the index from the

00:14:05,410 --> 00:14:10,480
available ring and the third is that the

00:14:07,810 --> 00:14:13,840
use this indirect as an offset of the

00:14:10,480 --> 00:14:16,270
Peacekeeper table and the force means is

00:14:13,840 --> 00:14:18,700
that we try to read back this in back in

00:14:16,270 --> 00:14:22,839
the user ring and the fifth is that we

00:14:18,700 --> 00:14:28,630
updated the use index so thio misses for

00:14:22,839 --> 00:14:32,470
one packet that is really bad so we can

00:14:28,630 --> 00:14:36,100
easily amortize the cache misses for

00:14:32,470 --> 00:14:40,210
patching so if we can batch for decade

00:14:36,100 --> 00:14:43,720
in batch what we can have so in this

00:14:40,210 --> 00:14:47,560
case we can still get one miss been

00:14:43,720 --> 00:14:51,040
reading the available index and but we

00:14:47,560 --> 00:14:54,130
can but we can only get one miss when we

00:14:51,040 --> 00:14:57,580
read patch read the index from the

00:14:54,130 --> 00:15:01,360
available ring so one is for for packing

00:14:57,580 --> 00:15:04,360
and one is for forty secure route

00:15:01,360 --> 00:15:07,690
readings from the discrete able and one

00:15:04,360 --> 00:15:10,870
miss will write the for index in the

00:15:07,690 --> 00:15:14,620
user ring and another one is when during

00:15:10,870 --> 00:15:20,020
update the app used index so after using

00:15:14,620 --> 00:15:22,089
the batching we in the best case so we

00:15:20,020 --> 00:15:27,910
can only get one point two five misses

00:15:22,089 --> 00:15:30,910
per packet so that's pretty good so the

00:15:27,910 --> 00:15:36,490
still working on in progress so I have

00:15:30,910 --> 00:15:38,920
done some so we can see that it can

00:15:36,490 --> 00:15:41,950
reduce the cache misses and it can also

00:15:38,920 --> 00:15:42,490
reduce the cache duration when the ring

00:15:41,950 --> 00:15:46,960
is almost

00:15:42,490 --> 00:15:50,310
MDNA or so the things is rather similar

00:15:46,960 --> 00:15:53,980
to what we happen or what we see in the

00:15:50,310 --> 00:15:56,320
contrary that's because that device or

00:15:53,980 --> 00:16:04,120
Trevor won't make any progress when the

00:15:56,320 --> 00:16:07,660
index changes so and another advantages

00:16:04,120 --> 00:16:09,760
is that if we patch the reading we can

00:16:07,660 --> 00:16:11,980
use something like fast stream copy

00:16:09,760 --> 00:16:15,630
function and then we can benefit from

00:16:11,980 --> 00:16:21,130
modern CPUs which has specific

00:16:15,630 --> 00:16:23,260
optimizations on this so there's a

00:16:21,130 --> 00:16:26,050
prototype which can batch reading the

00:16:23,260 --> 00:16:28,750
available index and batch update them in

00:16:26,050 --> 00:16:33,010
the user ring and update the used index

00:16:28,750 --> 00:16:36,850
in a bunch so we can collect about more

00:16:33,010 --> 00:16:39,060
than 20% of improvement in TX and more

00:16:36,850 --> 00:16:44,380
than sixty percent improvement in our X

00:16:39,060 --> 00:16:47,920
so that's pretty good and the next step

00:16:44,380 --> 00:16:53,590
is trying to implement the batch disable

00:16:47,920 --> 00:16:56,950
ring okay so after learning from the be

00:16:53,590 --> 00:17:02,830
host PMD we switch our attention to the

00:16:56,950 --> 00:17:05,589
recent xev so a city has three lots of

00:17:02,830 --> 00:17:07,689
good stuffs so from our point of view

00:17:05,589 --> 00:17:10,209
the most important part is that one of

00:17:07,689 --> 00:17:12,910
these actually encode redirect so this

00:17:10,209 --> 00:17:15,220
open a chance that we can redirect the

00:17:12,910 --> 00:17:21,550
packet between a physical hardware annex

00:17:15,220 --> 00:17:25,709
and helps in a very fast v so a typical

00:17:21,550 --> 00:17:28,780
active inflammation requires several

00:17:25,709 --> 00:17:30,040
things the first is that it requires a

00:17:28,780 --> 00:17:32,590
dedicated XQ

00:17:30,040 --> 00:17:35,620
for Lockleys transmission this is really

00:17:32,590 --> 00:17:37,390
done through a procedure to ask you or

00:17:35,620 --> 00:17:40,750
Purvis ask you

00:17:37,390 --> 00:17:45,790
so that means that we need multi-kill

00:17:40,750 --> 00:17:48,700
support and the XDP routine was really

00:17:45,790 --> 00:17:52,480
run and they're not people rooting so it

00:17:48,700 --> 00:17:56,260
was it will run before desk if it has be

00:17:52,480 --> 00:17:58,750
created and a typical active

00:17:56,260 --> 00:18:01,799
implementation you disable all the large

00:17:58,750 --> 00:18:04,030
package support for example tremor frame

00:18:01,799 --> 00:18:10,360
error or something like this

00:18:04,030 --> 00:18:13,480
so if we want to if we want to implement

00:18:10,360 --> 00:18:17,190
XDP for tab some things is a little bit

00:18:13,480 --> 00:18:22,200
different the first challenge is that

00:18:17,190 --> 00:18:25,450
the tab has multi cue support but

00:18:22,200 --> 00:18:27,520
creating or destroying nucleus was

00:18:25,450 --> 00:18:30,880
totally under the control of user space

00:18:27,520 --> 00:18:32,260
that means that we could not change

00:18:30,880 --> 00:18:35,500
something we could not implement

00:18:32,260 --> 00:18:38,860
something like per CPU clear excuse what

00:18:35,500 --> 00:18:42,820
happen without notifying the guest and

00:18:38,860 --> 00:18:47,260
unfortunately there's no ways to notify

00:18:42,820 --> 00:18:51,840
the gas about such changes now so the

00:18:47,260 --> 00:18:55,540
solution is that we will use the exist

00:18:51,840 --> 00:18:58,870
here excuse what happen when drawback is

00:18:55,540 --> 00:19:02,710
that it me requires spin lock to

00:18:58,870 --> 00:19:06,480
synchronize but I enemy it works this

00:19:02,710 --> 00:19:09,100
could be done or optimized in the future

00:19:06,480 --> 00:19:13,030
another thing another another challenge

00:19:09,100 --> 00:19:17,260
is that we could not disable the error

00:19:13,030 --> 00:19:18,700
or jumbo when cassity's run so these

00:19:17,260 --> 00:19:21,970
things that the situation is similar

00:19:18,700 --> 00:19:24,690
because we could not achieve such of

00:19:21,970 --> 00:19:27,400
configurations without notifying the gas

00:19:24,690 --> 00:19:31,090
so it will implement a hybrid mode of

00:19:27,400 --> 00:19:34,960
active implementation that means that if

00:19:31,090 --> 00:19:37,840
we see big packet they will try to build

00:19:34,960 --> 00:19:43,299
skb and use the generic hack CB helper

00:19:37,840 --> 00:19:46,150
to finish the X DB and the next

00:19:43,299 --> 00:19:49,150
challenge is that early the data copy

00:19:46,150 --> 00:19:49,860
was done with escapee allocations for

00:19:49,150 --> 00:19:52,880
that

00:19:49,860 --> 00:19:56,820
so we solve this by the code

00:19:52,880 --> 00:19:59,520
aesthetically out of the escapee

00:19:56,820 --> 00:20:01,740
allocation through the build escapee in

00:19:59,520 --> 00:20:04,950
this case we can first copy the packet

00:20:01,740 --> 00:20:07,530
and then to the ecstasy staff and if we

00:20:04,950 --> 00:20:11,670
want to be USS Kb we can try to build

00:20:07,530 --> 00:20:14,130
its route build skb and the next things

00:20:11,670 --> 00:20:18,450
is that there's really no nazi by

00:20:14,130 --> 00:20:22,410
default so we tweet so we decided to eat

00:20:18,450 --> 00:20:24,330
in the tungsten message and the last

00:20:22,410 --> 00:20:28,559
thing interesting is that the traps

00:20:24,330 --> 00:20:32,730
approach zero copy so for simply C which

00:20:28,559 --> 00:20:36,390
was to do all the activity for the rock

00:20:32,730 --> 00:20:38,309
up a decade so the generic FTP layer the

00:20:36,390 --> 00:20:40,890
reasons is that actively really requires

00:20:38,309 --> 00:20:43,470
some head rooms to do something like

00:20:40,890 --> 00:20:47,130
head advancement but for the okapi

00:20:43,470 --> 00:20:49,380
packet maybe there's no way currently

00:20:47,130 --> 00:20:52,020
forget to let gas to reserve something

00:20:49,380 --> 00:20:55,530
like hey the rooms for us so we can only

00:20:52,020 --> 00:21:01,049
tweet through the generic active layer

00:20:55,530 --> 00:21:04,559
now so this is a figure to show how the

00:21:01,049 --> 00:21:09,120
hybrid XE piece was implemented in tab

00:21:04,559 --> 00:21:11,040
so for T row or big packet which was

00:21:09,120 --> 00:21:14,340
passed it to the generic has to be

00:21:11,040 --> 00:21:16,679
Hyper's that works except for the

00:21:14,340 --> 00:21:20,490
activity wrecked so this need to be

00:21:16,679 --> 00:21:22,860
fixed and for some for small packet we

00:21:20,490 --> 00:21:26,309
were forced to the data copy and then we

00:21:22,860 --> 00:21:30,270
will implement a native active

00:21:26,309 --> 00:21:32,400
implementation and if it needs to be

00:21:30,270 --> 00:21:35,910
dropped we can drop after it right now

00:21:32,400 --> 00:21:40,290
and if we want to redirect it to a host

00:21:35,910 --> 00:21:43,890
Nick so we will use the axe CP buffer

00:21:40,290 --> 00:21:49,169
and called FCB transmission routines

00:21:43,890 --> 00:21:53,730
from the host this has been marked in

00:21:49,169 --> 00:21:56,900
upstream so we can we can now try to use

00:21:53,730 --> 00:22:02,870
a CD to accelerate the gas transmission

00:21:56,900 --> 00:22:05,120
and for the gas to receive receiving

00:22:02,870 --> 00:22:07,150
it usually requires 2 x TB transmission

00:22:05,120 --> 00:22:14,000
support foot hat

00:22:07,150 --> 00:22:16,940
so we implement a prototype which which

00:22:14,000 --> 00:22:19,160
was done through a pointer ring which is

00:22:16,940 --> 00:22:26,120
really similar to what US per deed for

00:22:19,160 --> 00:22:31,000
the CPU map and we also store the xep

00:22:26,120 --> 00:22:35,180
metadata in the head room of the packet

00:22:31,000 --> 00:22:38,840
so when we want to redirect our expert

00:22:35,180 --> 00:22:41,870
force to the head if you first build a

00:22:38,840 --> 00:22:45,740
Skippy buffer and kill it kill the

00:22:41,870 --> 00:22:48,320
pointer athlete in a pointer ring and we

00:22:45,740 --> 00:22:50,750
also need to make sure that the if we

00:22:48,320 --> 00:22:54,830
call the time receive message this

00:22:50,750 --> 00:22:57,260
packet could be received and we also

00:22:54,830 --> 00:23:01,220
want we also need to support the badge

00:22:57,260 --> 00:23:03,520
tqe since it was a requirement for VF

00:23:01,220 --> 00:23:03,520
snipe

00:23:04,300 --> 00:23:13,090
so for test part we still has actively

00:23:09,380 --> 00:23:16,670
support so focused is it looks much

00:23:13,090 --> 00:23:19,790
sweet for it was multi opposite which

00:23:16,670 --> 00:23:22,550
means that we have per CPU TX @ TB Q and

00:23:19,790 --> 00:23:28,670
we need to reserve in a few pairs during

00:23:22,550 --> 00:23:31,310
the VM LAN Chi and for offload we try it

00:23:28,670 --> 00:23:34,010
would disable the flows on demand that

00:23:31,310 --> 00:23:37,250
means if ax e be said they try to

00:23:34,010 --> 00:23:40,820
disabled through the world queue control

00:23:37,250 --> 00:23:44,360
command and the next is that there's no

00:23:40,820 --> 00:23:46,460
recite during the exhibition the reset

00:23:44,360 --> 00:23:52,430
is really required because when it were

00:23:46,460 --> 00:23:57,260
with extra Headroom but we choose to use

00:23:52,430 --> 00:24:00,200
another idea so before the actively said

00:23:57,260 --> 00:24:04,610
we don't reserve any Headroom and after

00:24:00,200 --> 00:24:06,770
taxi beside we will try to reserve had a

00:24:04,610 --> 00:24:09,410
room so there will be your small chance

00:24:06,770 --> 00:24:11,510
that we meet a packet which has been

00:24:09,410 --> 00:24:13,580
allocated before accidie was said so we

00:24:11,510 --> 00:24:14,590
did hurt this case and it would be the

00:24:13,580 --> 00:24:17,200
copy

00:24:14,590 --> 00:24:21,159
so this is a little bit slow but should

00:24:17,200 --> 00:24:22,929
be work and should be rare and what's

00:24:21,159 --> 00:24:25,899
more important is that we also support

00:24:22,929 --> 00:24:28,330
xep redirection and transmission this

00:24:25,899 --> 00:24:31,090
means that we can use to XDP to

00:24:28,330 --> 00:24:36,190
accelerate the traffic for something

00:24:31,090 --> 00:24:41,559
like an STM no page recycling was

00:24:36,190 --> 00:24:43,419
support yet so it was nice to hi oh okay

00:24:41,559 --> 00:24:47,950
so the performance away from developed

00:24:43,419 --> 00:24:51,759
patient so this is the threat to the

00:24:47,950 --> 00:24:55,090
setup of the test we try to generic

00:24:51,759 --> 00:24:57,429
generic generate the traffic so it has

00:24:55,090 --> 00:25:01,059
PMD between located on a remote host and

00:24:57,429 --> 00:25:03,249
we use another test PMD in South cast to

00:25:01,059 --> 00:25:05,950
do some to do the transmission or

00:25:03,249 --> 00:25:09,850
receiving so the reasons we used has

00:25:05,950 --> 00:25:12,190
been being guest is that we found a

00:25:09,850 --> 00:25:19,600
traffic generator is not very suitable

00:25:12,190 --> 00:25:21,940
for worst case and because it was it it

00:25:19,600 --> 00:25:25,570
was slow and the current patron and

00:25:21,940 --> 00:25:27,279
Trevor cannot cooperate this the package

00:25:25,570 --> 00:25:30,909
journey through very well

00:25:27,279 --> 00:25:35,889
since we do not have clicks interrupt by

00:25:30,909 --> 00:25:41,730
default okay here so here's the rags

00:25:35,889 --> 00:25:46,299
performance so we can see from the linux

00:25:41,730 --> 00:25:50,350
4.5 we get many interesting features

00:25:46,299 --> 00:25:51,970
which can help the performance so the

00:25:50,350 --> 00:25:55,389
one thing i want i want to mention is

00:25:51,970 --> 00:25:58,840
that let's take about let's take a look

00:25:55,389 --> 00:26:02,369
at the 4.11 the axe repeat reverse

00:25:58,840 --> 00:26:05,289
switch to use this the buda Skippy so

00:26:02,369 --> 00:26:08,169
after this commit we can also get some

00:26:05,289 --> 00:26:10,840
improvement this is an example of how we

00:26:08,169 --> 00:26:14,619
can get benefit from the improvement of

00:26:10,840 --> 00:26:17,769
the host driver and the true major

00:26:14,619 --> 00:26:20,710
improvement was all under development

00:26:17,769 --> 00:26:23,289
the first least xdb transmission so we

00:26:20,710 --> 00:26:27,159
can see that after we used it we can

00:26:23,289 --> 00:26:28,260
increase the performance from 2.4 file

00:26:27,159 --> 00:26:32,080
meaning

00:26:28,260 --> 00:26:34,150
pps43 meeting and if we do axe batching

00:26:32,080 --> 00:26:40,870
on top you can get about five meetings

00:26:34,150 --> 00:26:45,430
or exit ETS so 40 x XE p still heads

00:26:40,870 --> 00:26:52,930
loss so after using xt p the performance

00:26:45,430 --> 00:26:55,900
boost from the r11 to about 2.6 and if

00:26:52,930 --> 00:26:58,990
we do something like patching or by

00:26:55,900 --> 00:27:02,350
positive loquacious we can get about 3

00:26:58,990 --> 00:27:07,600
millions TX PPS so this looks very good

00:27:02,350 --> 00:27:12,450
so it's time to compare against the

00:27:07,600 --> 00:27:19,930
thespian so with the support of the

00:27:12,450 --> 00:27:22,870
activity rekt things it's the support of

00:27:19,930 --> 00:27:25,900
the x EPV to correct we can get more

00:27:22,870 --> 00:27:28,630
fair computation because the setup looks

00:27:25,900 --> 00:27:31,600
much more similar to what has PMD did

00:27:28,630 --> 00:27:34,060
and the only difference is that for the

00:27:31,600 --> 00:27:36,370
guest receiving all things worse than

00:27:34,060 --> 00:27:38,560
you already buy through process one is

00:27:36,370 --> 00:27:40,750
the software code daemon and another is

00:27:38,560 --> 00:27:44,880
Vince night but for transmission all

00:27:40,750 --> 00:27:46,600
things was done in the V host night so

00:27:44,880 --> 00:27:53,470
so here we are

00:27:46,600 --> 00:27:56,610
so for TX we can get three meanings PBS

00:27:53,470 --> 00:28:00,280
and for X we can get five so there's a

00:27:56,610 --> 00:28:03,850
good improvement for the past several

00:28:00,280 --> 00:28:04,210
versions plus some something something

00:28:03,850 --> 00:28:08,350
new

00:28:04,210 --> 00:28:15,430
but we are still far from what Timothy

00:28:08,350 --> 00:28:19,600
can get Thank You us ok so to do to do

00:28:15,430 --> 00:28:22,240
is to continue to optimize the code so

00:28:19,600 --> 00:28:25,120
this is the first topic for case of

00:28:22,240 --> 00:28:27,610
devaki racks so we can see that the

00:28:25,120 --> 00:28:31,870
spinlock contingent which is probably

00:28:27,610 --> 00:28:34,960
because of we need to use spin log to

00:28:31,870 --> 00:28:37,750
synchronize the users so we public and

00:28:34,960 --> 00:28:39,730
so this or amortize this through patch

00:28:37,750 --> 00:28:43,360
in QE

00:28:39,730 --> 00:28:47,169
and the next is the axe GBE arc sitting

00:28:43,360 --> 00:28:50,019
which is good so and the third is the

00:28:47,169 --> 00:28:53,380
salt the salt F Red Bull this is looks a

00:28:50,019 --> 00:28:55,659
little bit interesting so this would be

00:28:53,380 --> 00:28:59,470
they should not be there so so do you

00:28:55,659 --> 00:29:01,840
need some investigation and this is the

00:28:59,470 --> 00:29:05,110
probe top for real tonight's red Eric's

00:29:01,840 --> 00:29:08,200
so this looks normal

00:29:05,110 --> 00:29:10,960
all things related to something like

00:29:08,200 --> 00:29:14,289
data copying this will be a translator

00:29:10,960 --> 00:29:17,470
or something like this and this is the

00:29:14,289 --> 00:29:19,389
proof for we host 90 X so we can see the

00:29:17,470 --> 00:29:21,789
first is the this decrepit advanced

00:29:19,389 --> 00:29:25,210
translation so we may get some

00:29:21,789 --> 00:29:28,450
bottleneck in this this probably because

00:29:25,210 --> 00:29:30,850
nerds currently we use interval tree to

00:29:28,450 --> 00:29:35,139
store the mappings so we probably need

00:29:30,850 --> 00:29:37,690
something to accelerate this and the

00:29:35,139 --> 00:29:41,139
next is the tank at user it is normal

00:29:37,690 --> 00:29:44,260
because I need to do transmission and

00:29:41,139 --> 00:29:45,490
the third is also interesting the we

00:29:44,260 --> 00:29:47,529
host can't be killed

00:29:45,490 --> 00:29:50,799
descriptor which means that they

00:29:47,529 --> 00:29:53,470
probably still some cache thrashing then

00:29:50,799 --> 00:29:56,559
we try to access the descriptive because

00:29:53,470 --> 00:30:01,019
we don't implement the batch descriptor

00:29:56,559 --> 00:30:05,289
table recreate so it looks a nice

00:30:01,019 --> 00:30:08,789
optimizations on top so here's some raw

00:30:05,289 --> 00:30:11,409
ideas on how to improve the first is

00:30:08,789 --> 00:30:14,789
maybe we can try to integrate this

00:30:11,409 --> 00:30:19,029
something like not Ibiza poly or

00:30:14,789 --> 00:30:23,080
implement pure busy pulling node of the

00:30:19,029 --> 00:30:26,139
Usenet and for xtp will probably need a

00:30:23,080 --> 00:30:29,610
bad bad really big corporations for the

00:30:26,139 --> 00:30:31,809
hardware NIC vendors because each

00:30:29,610 --> 00:30:34,870
hardware Nick Traverse has his own

00:30:31,809 --> 00:30:38,500
optimizations for people cycling so we

00:30:34,870 --> 00:30:41,649
probably need a battery to cooperate and

00:30:38,500 --> 00:30:44,590
the next things worse to be done is

00:30:41,649 --> 00:30:47,440
build and you see as KB or actively

00:30:44,590 --> 00:30:50,940
buffers inverse night and the next is

00:30:47,440 --> 00:30:55,240
probably we can try to implement

00:30:50,940 --> 00:30:59,440
something like ax 0 copy IBM engineers

00:30:55,240 --> 00:31:01,870
is presently the an idea which use a new

00:30:59,440 --> 00:31:04,059
and the old and in this in the old can

00:31:01,870 --> 00:31:06,909
try to post and arcs buffers from

00:31:04,059 --> 00:31:10,570
Cassius's base to magnet have and

00:31:06,909 --> 00:31:15,639
magnetic try to push it in the post

00:31:10,570 --> 00:31:19,330
Eric's big buffers and the last things

00:31:15,639 --> 00:31:22,029
is that we see that patching helps a lot

00:31:19,330 --> 00:31:25,870
for we for help and we host performance

00:31:22,029 --> 00:31:27,850
so in order to address all the

00:31:25,870 --> 00:31:31,389
limitations of the current work le dot

00:31:27,850 --> 00:31:35,830
we try to invent a new ring layout

00:31:31,389 --> 00:31:39,220
called what L 1.1 so if any of you have

00:31:35,830 --> 00:31:43,360
interest in in this please feel free to

00:31:39,220 --> 00:31:47,710
comment on it ok that's it

00:31:43,360 --> 00:31:52,649
any question ok he's sitting between you

00:31:47,710 --> 00:31:52,649
and the beer right now so any questions

00:31:57,240 --> 00:32:05,230
so pointer ring works with two locks

00:32:03,129 --> 00:32:07,510
there's a consumer lock and a producer

00:32:05,230 --> 00:32:10,240
lock which I guess works perfectly for

00:32:07,510 --> 00:32:12,519
you guys because you usually want one

00:32:10,240 --> 00:32:14,830
CPU queuing packets up and the other D

00:32:12,519 --> 00:32:19,120
queuing because yes on you know

00:32:14,830 --> 00:32:21,580
different size of the VM wondering if

00:32:19,120 --> 00:32:24,309
you'd be willing to accept patches to

00:32:21,580 --> 00:32:32,080
make the whole thing lock --less yes

00:32:24,309 --> 00:32:34,480
page has more than do come ok Lockleys

00:32:32,080 --> 00:32:38,440
version so if you call the loveliest

00:32:34,480 --> 00:32:40,419
version it works the cpu map has already

00:32:38,440 --> 00:32:43,059
used locally sporting of the pony ring

00:32:40,419 --> 00:32:49,779
so it has a localized version in in

00:32:43,059 --> 00:32:52,000
point earring in red is that if you

00:32:49,779 --> 00:32:56,100
think there's no need for a spin lock

00:32:52,000 --> 00:32:56,100
you can just call the loveliest version

00:32:57,600 --> 00:33:09,970
ok we'll talk there are fine

00:33:00,460 --> 00:33:13,710
I haven't looked at it does it do lock

00:33:09,970 --> 00:33:16,300
let's compare exchanger does it do

00:33:13,710 --> 00:33:20,530
assuming that somebody else is doing the

00:33:16,300 --> 00:33:23,350
locking for it we don't we don't use the

00:33:20,530 --> 00:33:26,830
log list version but in some case for

00:33:23,350 --> 00:33:37,540
example if you sure that you are the

00:33:26,830 --> 00:33:40,680
only reader already yeah I won't

00:33:37,540 --> 00:33:40,680
applause Jason

00:33:41,120 --> 00:33:46,559

YouTube URL: https://www.youtube.com/watch?v=OsEaSEUySP8


