Title: Extending Linux network stack for persistent memory
Publication date: 2018-03-14
Playlist: Netdev 2.2
Description: 
	Speaker: Michio Honda, Giuseppe Lettieri 
Friday November 10th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?honda-extendlinuxstack-talk
Captions: 
	00:00:00,989 --> 00:00:06,810
so maybe we can start and thank you very

00:00:04,589 --> 00:00:08,610
much for the staying in this room and

00:00:06,810 --> 00:00:13,260
really surprised I was really afraid of

00:00:08,610 --> 00:00:15,090
that all of them so the by the way my

00:00:13,260 --> 00:00:17,519
name is Dimitri Honda from any celebrity

00:00:15,090 --> 00:00:20,460
Europe and the working with some former

00:00:17,519 --> 00:00:23,130
Greeks and my permanent permanent or

00:00:20,460 --> 00:00:26,489
some usual colleagues the this is

00:00:23,130 --> 00:00:30,119
basically the working photo in the world

00:00:26,489 --> 00:00:35,550
of persistent memories what implication

00:00:30,119 --> 00:00:39,809
for networking so let's start from some

00:00:35,550 --> 00:00:42,059
overview and so the position memory also

00:00:39,809 --> 00:00:44,730
known as storage class memory or

00:00:42,059 --> 00:00:48,449
non-volatile memory they imagine

00:00:44,730 --> 00:00:51,739
actually coming so H ve already ships

00:00:48,449 --> 00:00:55,230
some standardized invading n which is

00:00:51,739 --> 00:00:58,320
some dieren backed by party so that we

00:00:55,230 --> 00:01:01,350
can make data persist on top of it but

00:00:58,320 --> 00:01:04,739
with the speed of DRAM that is the one

00:01:01,350 --> 00:01:06,990
kind of say speed optimized NV RAM or

00:01:04,739 --> 00:01:11,909
persistent memory but on the other hand

00:01:06,990 --> 00:01:14,100
a lot of other the PM correct whose

00:01:11,909 --> 00:01:16,530
characteristics is different for example

00:01:14,100 --> 00:01:18,689
in the 3d crosspoint so which is

00:01:16,530 --> 00:01:22,500
slightly slower than theorem but

00:01:18,689 --> 00:01:25,350
capacity is a huge and price is expected

00:01:22,500 --> 00:01:27,420
to be cheaper than theorem so it is also

00:01:25,350 --> 00:01:31,170
still the put into u.s. abba

00:01:27,420 --> 00:01:33,479
DIMM slots so this is actually

00:01:31,170 --> 00:01:36,869
fundamental change in memory hierarchy

00:01:33,479 --> 00:01:39,360
so the CPU cache on top and disk in the

00:01:36,869 --> 00:01:41,250
disks in the bottom so it is basically

00:01:39,360 --> 00:01:45,030
two three orders of magnitude faster

00:01:41,250 --> 00:01:47,549
than 50 so disks and so more importantly

00:01:45,030 --> 00:01:51,740
or equally important trees the we can

00:01:47,549 --> 00:01:55,109
access to persistent tile data using

00:01:51,740 --> 00:02:00,149
privileged load store instructions so

00:01:55,109 --> 00:02:02,369
this is in contrast discs and SSDs which

00:02:00,149 --> 00:02:06,060
we need the system course to access

00:02:02,369 --> 00:02:08,610
persistent data using at not bite

00:02:06,060 --> 00:02:12,480
granularity but broke coolant

00:02:08,610 --> 00:02:15,030
granularity so the industry in the

00:02:12,480 --> 00:02:18,560
storage community that they wrote work

00:02:15,030 --> 00:02:21,420
work is going on for example so today so

00:02:18,560 --> 00:02:25,040
example data structure file systems are

00:02:21,420 --> 00:02:28,319
mostly designed to organize organize a

00:02:25,040 --> 00:02:30,090
Brooklynite granularity data but the on

00:02:28,319 --> 00:02:32,370
position memory the weekend shift data

00:02:30,090 --> 00:02:36,750
in patran righty so the random access

00:02:32,370 --> 00:02:38,670
very much the convenient and fast so

00:02:36,750 --> 00:02:41,790
this talk is about what I implemented

00:02:38,670 --> 00:02:43,620
implications for networking so let's

00:02:41,790 --> 00:02:46,500
take a look at the example of end-to-end

00:02:43,620 --> 00:02:49,290
transaction that's very simple data

00:02:46,500 --> 00:02:53,250
exchange between client on the server so

00:02:49,290 --> 00:02:55,620
the client say send some data to the

00:02:53,250 --> 00:02:59,220
server over the ethernet say I want to

00:02:55,620 --> 00:03:01,410
store this data persistently travel then

00:02:59,220 --> 00:03:04,050
the data arrives at the network stack

00:03:01,410 --> 00:03:07,350
and application typically runs this kind

00:03:04,050 --> 00:03:10,410
of code basically the looping around a

00:03:07,350 --> 00:03:13,590
poor weight and reading data from the

00:03:10,410 --> 00:03:18,269
socket and writing this data to some

00:03:13,590 --> 00:03:22,260
file using or some virus or the EM maps

00:03:18,269 --> 00:03:25,590
and doing mem copy then the the we have

00:03:22,260 --> 00:03:28,200
to call F sync or M sync to make sure

00:03:25,590 --> 00:03:29,489
that is actually written back to the

00:03:28,200 --> 00:03:32,910
disks ossd

00:03:29,489 --> 00:03:36,060
then the application right responds to

00:03:32,910 --> 00:03:38,160
the socket say hey your data is safe I

00:03:36,060 --> 00:03:41,070
can recover your data even after I

00:03:38,160 --> 00:03:42,360
question reboot this is the basic

00:03:41,070 --> 00:03:47,820
mechanism health

00:03:42,360 --> 00:03:50,130
today's drive rewrite happens so this

00:03:47,820 --> 00:03:52,799
process actually so this is the right

00:03:50,130 --> 00:03:55,200
and everything comes in this path takes

00:03:52,799 --> 00:03:57,630
say one more than one millisecond in the

00:03:55,200 --> 00:04:00,660
world of disks or 50 so there is nothing

00:03:57,630 --> 00:04:03,090
to do with networking okay but in the

00:04:00,660 --> 00:04:07,170
world so on the other hand that working

00:04:03,090 --> 00:04:09,390
path we don't write data then the

00:04:07,170 --> 00:04:11,970
inbound trip between client server even

00:04:09,390 --> 00:04:13,950
including some simple HTTP logic it is

00:04:11,970 --> 00:04:16,109
only 40 micro second okay

00:04:13,950 --> 00:04:19,620
so in this world we have nothing to do

00:04:16,109 --> 00:04:23,010
with networking but we have persistent

00:04:19,620 --> 00:04:25,080
memories that attach to denote the this

00:04:23,010 --> 00:04:27,030
part writing data so we don't need

00:04:25,080 --> 00:04:28,210
everything I'm using anymore we can just

00:04:27,030 --> 00:04:31,360
use your flash

00:04:28,210 --> 00:04:34,569
traction and this takes only two

00:04:31,360 --> 00:04:37,690
microseconds so now the four entertained

00:04:34,569 --> 00:04:40,690
transaction the drive rewriting data so

00:04:37,690 --> 00:04:43,120
bottleneck is networking so you may

00:04:40,690 --> 00:04:45,520
think this 2 micro second it'd be a 2

00:04:43,120 --> 00:04:48,280
micro second is nothing so the we can

00:04:45,520 --> 00:04:51,460
still we may still say nothing to do but

00:04:48,280 --> 00:04:54,250
it is actually not because in the real

00:04:51,460 --> 00:04:56,770
world the it looks like this so we have

00:04:54,250 --> 00:04:59,740
a lot of client connecting at the same

00:04:56,770 --> 00:05:03,039
time and equal weight or typical server

00:04:59,740 --> 00:05:06,250
program looks like this so on each CPU

00:05:03,039 --> 00:05:10,259
core server rounds thread ones

00:05:06,250 --> 00:05:13,389
eople weight and ribs and process each

00:05:10,259 --> 00:05:17,320
TCP connection then the reading data

00:05:13,389 --> 00:05:19,690
writing data and processed sending

00:05:17,320 --> 00:05:24,909
acknowledgement back to the client on

00:05:19,690 --> 00:05:27,849
each file descriptor and here and this

00:05:24,909 --> 00:05:30,370
process is realized so in the end we

00:05:27,849 --> 00:05:32,560
have a lot of latency throughput or

00:05:30,370 --> 00:05:35,919
transaction per second decrease and

00:05:32,560 --> 00:05:38,639
latency increase so the worry about

00:05:35,919 --> 00:05:41,349
between some light gray and dark gray

00:05:38,639 --> 00:05:45,699
pass or boxes just compared between

00:05:41,349 --> 00:05:49,479
white power boxes which is like in the

00:05:45,699 --> 00:05:52,900
case we don't pass this to data or that

00:05:49,479 --> 00:05:54,940
takes a this is that gray box or bar so

00:05:52,900 --> 00:05:58,000
we significantly reduce repeat and

00:05:54,940 --> 00:06:01,840
increased latency as we have more number

00:05:58,000 --> 00:06:07,240
of concurrent TCP connections so the

00:06:01,840 --> 00:06:10,330
problem so the these days we have seen a

00:06:07,240 --> 00:06:12,880
lot of improve the tcp/ip stack so that

00:06:10,330 --> 00:06:16,630
is an estate of school so what's going

00:06:12,880 --> 00:06:18,789
on today so the for example I can most

00:06:16,630 --> 00:06:22,930
vision be next when the message 0 copy

00:06:18,789 --> 00:06:25,659
so we can do like we can avoid data copy

00:06:22,930 --> 00:06:28,060
but we itself we cannot patch system

00:06:25,659 --> 00:06:31,680
call that well but we can still use a

00:06:28,060 --> 00:06:36,940
Linux tcp/ip but the we don't have

00:06:31,680 --> 00:06:40,479
packet be placed on name the packet area

00:06:36,940 --> 00:06:41,800
or persistent memory directory so we

00:06:40,479 --> 00:06:44,020
have this we still have

00:06:41,800 --> 00:06:47,379
copy data from the some DRAM to

00:06:44,020 --> 00:06:50,169
persistent memory so a canary question

00:06:47,379 --> 00:06:52,659
multiplexer so we can call we can batch

00:06:50,169 --> 00:06:55,150
system call across many TCP connections

00:06:52,659 --> 00:06:57,900
but the we cannot the zero copy and

00:06:55,150 --> 00:07:01,210
still the packet buffers are on Iran

00:06:57,900 --> 00:07:03,400
Saudi PDK and either space stack is the

00:07:01,210 --> 00:07:07,560
biggest problem is they don't have a

00:07:03,400 --> 00:07:10,810
good tcp/ip permutation right so what

00:07:07,560 --> 00:07:16,740
paste is our system trying to solve is

00:07:10,810 --> 00:07:21,819
we want to meet all of these requirement

00:07:16,740 --> 00:07:25,000
so this is how it works hideous than

00:07:21,819 --> 00:07:27,789
going straight to implementation so just

00:07:25,000 --> 00:07:30,699
the imagine like it a space and kind of

00:07:27,789 --> 00:07:32,680
space and we have Nick and we also have

00:07:30,699 --> 00:07:36,789
packet buffer which is statically

00:07:32,680 --> 00:07:39,280
allocated and index so the point is that

00:07:36,789 --> 00:07:42,490
we have packet buffer on the persistent

00:07:39,280 --> 00:07:45,159
memory back to fire say slash mounts

00:07:42,490 --> 00:07:48,900
number time a memory packet the box okay

00:07:45,159 --> 00:07:52,719
this is just a normal file but the XFS

00:07:48,900 --> 00:07:55,479
x4 which support attacks the direct

00:07:52,719 --> 00:08:00,460
access extension for position the memory

00:07:55,479 --> 00:08:02,680
okay so this to have to process packet

00:08:00,460 --> 00:08:05,020
against protocol stack we just the

00:08:02,680 --> 00:08:10,270
exchange packet between this special

00:08:05,020 --> 00:08:14,979
packet buffer and binax tcp/ip and but

00:08:10,270 --> 00:08:16,810
application ok widow data access data on

00:08:14,979 --> 00:08:25,569
this packet buffer without data copy

00:08:16,810 --> 00:08:29,529
using net map api in the writing data so

00:08:25,569 --> 00:08:32,020
applications and have to keep data for

00:08:29,529 --> 00:08:34,779
example i want to keep this data so to

00:08:32,020 --> 00:08:37,930
do that the application write down

00:08:34,779 --> 00:08:38,890
metadata entry to another private file

00:08:37,930 --> 00:08:42,370
ok

00:08:38,890 --> 00:08:44,260
so private this fire it looks like so

00:08:42,370 --> 00:08:46,930
say it's located the interest mount

00:08:44,260 --> 00:08:50,079
non-volatile memory my application

00:08:46,930 --> 00:08:54,880
metadata so this metadata since this is

00:08:50,079 --> 00:09:00,590
a zero indexed array so it just keeps

00:08:54,880 --> 00:09:04,700
it's death keeps behind X 0 by finding X

00:09:00,590 --> 00:09:08,510
1 3 1 0 1 3 right so it is basically

00:09:04,700 --> 00:09:10,460
specify extent of each buffer and after

00:09:08,510 --> 00:09:12,860
writing this meta data entry the

00:09:10,460 --> 00:09:14,900
application have to refresh this data

00:09:12,860 --> 00:09:19,820
buffer because many of you already know

00:09:14,900 --> 00:09:24,290
about DDI all the which DMS data not to

00:09:19,820 --> 00:09:27,140
get rampant to last level cache yes so

00:09:24,290 --> 00:09:30,470
we have to the write down data so let's

00:09:27,140 --> 00:09:33,080
go in bit more animated way so imagine

00:09:30,470 --> 00:09:35,480
that the server received 4 for requests

00:09:33,080 --> 00:09:39,080
each contained in single packet and

00:09:35,480 --> 00:09:42,470
single TCP segment and application first

00:09:39,080 --> 00:09:46,060
read this data and the tcp/ip process

00:09:42,470 --> 00:09:48,830
this packet then the applications

00:09:46,060 --> 00:09:53,150
identifies this data for example it has

00:09:48,830 --> 00:09:58,400
to write the passage every write to a

00:09:53,150 --> 00:10:01,400
file then it was a set operation for key

00:09:58,400 --> 00:10:03,830
value store then the application write

00:10:01,400 --> 00:10:07,160
this metadata entries I want to store

00:10:03,830 --> 00:10:12,250
our packet buffer packet buffer 0 from

00:10:07,160 --> 00:10:16,820
offset 100 byte and 4 lengths of 1185

00:10:12,250 --> 00:10:19,400
then it flashes this metadata and packet

00:10:16,820 --> 00:10:22,790
buffer they say the process how to drop

00:10:19,400 --> 00:10:26,000
restore data so that we process the

00:10:22,790 --> 00:10:28,250
second packet second request segment and

00:10:26,000 --> 00:10:30,740
in the same way and imagine that

00:10:28,250 --> 00:10:33,620
so the third packet was idempotent data

00:10:30,740 --> 00:10:37,040
so which is a get operation for key

00:10:33,620 --> 00:10:40,070
value store for such a data applications

00:10:37,040 --> 00:10:47,300
simply skips the metadata entry right

00:10:40,070 --> 00:10:48,860
and also it skips franching metadata so

00:10:47,300 --> 00:10:50,720
these are let's go into the

00:10:48,860 --> 00:10:53,060
implementation the cool thing is that we

00:10:50,720 --> 00:10:56,480
can do that without modifying Linux

00:10:53,060 --> 00:10:58,820
scanner that's really good so the the

00:10:56,480 --> 00:11:01,790
point is we do zero copy pocket aisles

00:10:58,820 --> 00:11:03,740
to and from the position memory back to

00:11:01,790 --> 00:11:06,680
fire and

00:11:03,740 --> 00:11:08,390
we also take all the best practices from

00:11:06,680 --> 00:11:13,400
literature about how to do

00:11:08,390 --> 00:11:17,180
high-performance Network stack so the

00:11:13,400 --> 00:11:20,570
the this is how the initialization steps

00:11:17,180 --> 00:11:28,010
happens so application first create some

00:11:20,570 --> 00:11:32,240
files as a management PNM for the actual

00:11:28,010 --> 00:11:37,630
the the data reside here and application

00:11:32,240 --> 00:11:42,110
just open and Emma that file okay and

00:11:37,630 --> 00:11:45,710
then the application the nm open which

00:11:42,110 --> 00:11:49,610
is some net maps and descriptor open

00:11:45,710 --> 00:11:51,650
operation and point is this previous M

00:11:49,610 --> 00:11:55,820
mapped mapped the virtual address of

00:11:51,650 --> 00:11:59,510
this persistent memory region so the so

00:11:55,820 --> 00:12:02,120
the using paste extension to net map the

00:11:59,510 --> 00:12:05,030
design name open also passes this

00:12:02,120 --> 00:12:08,120
validus to the cannon okay then the in

00:12:05,030 --> 00:12:11,300
the Connor that it obtains the Karner

00:12:08,120 --> 00:12:14,720
pages using get user pages from this

00:12:11,300 --> 00:12:21,320
virtual address this is exported to the

00:12:14,720 --> 00:12:23,330
outside of the maintainer and they also

00:12:21,320 --> 00:12:27,670
the initial ID is some net macerated

00:12:23,330 --> 00:12:29,660
object like rings and some buffer

00:12:27,670 --> 00:12:33,890
organization for example the splitting

00:12:29,660 --> 00:12:38,090
this region to fix a fix to the 2k

00:12:33,890 --> 00:12:41,030
buffers to terabyte of Apophis so the

00:12:38,090 --> 00:12:43,880
important thing is the net map rings

00:12:41,030 --> 00:12:48,070
really don't contain buffer it only

00:12:43,880 --> 00:12:51,110
contains the point at the buffer so the

00:12:48,070 --> 00:12:53,780
in the for example in these parts the

00:12:51,110 --> 00:12:55,430
number nine is contained number nine the

00:12:53,780 --> 00:12:57,560
content here which means that so this is

00:12:55,430 --> 00:13:01,700
by the way zero indexed array so the

00:12:57,560 --> 00:13:04,520
number nine points nice packet buffer in

00:13:01,700 --> 00:13:07,750
the same way this number one points just

00:13:04,520 --> 00:13:11,870
means the one first

00:13:07,750 --> 00:13:14,480
zero index first the packet buffer okay

00:13:11,870 --> 00:13:17,740
so these are not really packet above a

00:13:14,480 --> 00:13:17,740
packet buffers our here

00:13:18,339 --> 00:13:24,529
so to use tcp/ip application just use

00:13:22,670 --> 00:13:26,779
normal socket API like a socket

00:13:24,529 --> 00:13:31,879
boundaries accept and connect for

00:13:26,779 --> 00:13:34,399
contour paths so the point is packets

00:13:31,879 --> 00:13:37,249
are coming from the neck to the ring

00:13:34,399 --> 00:13:39,589
then that these data must be processed

00:13:37,249 --> 00:13:44,180
by the network stack in a network stack

00:13:39,589 --> 00:13:46,509
so I'll receive a pass it simply put the

00:13:44,180 --> 00:13:49,519
beard skb on receive the packet buffer

00:13:46,509 --> 00:13:53,089
with the push it into the network

00:13:49,519 --> 00:13:55,970
receive skb then we also modify socket

00:13:53,089 --> 00:14:00,499
the data radikor back so we can

00:13:55,970 --> 00:14:04,059
intercept ready to read the data say you

00:14:00,499 --> 00:14:09,350
know the TCP segment after the socket to

00:14:04,059 --> 00:14:12,350
move weekend as if that data so that we

00:14:09,350 --> 00:14:17,629
can simply move the disk data to the

00:14:12,350 --> 00:14:19,850
user space mapped ring when sending data

00:14:17,629 --> 00:14:22,939
it is just opposite the user space fear

00:14:19,850 --> 00:14:26,059
of the TX ring and the push so in the

00:14:22,939 --> 00:14:28,670
pasted it pushes each data to transient

00:14:26,059 --> 00:14:37,279
page and we can intercept packet at the

00:14:28,670 --> 00:14:41,149
bottom and do start X meet oh yeah just

00:14:37,279 --> 00:14:44,980
not but the each ring the contains

00:14:41,149 --> 00:14:48,800
packet across multiple TCP connections

00:14:44,980 --> 00:14:51,470
so now they in a bit more detail how it

00:14:48,800 --> 00:14:54,170
works so the way application executes

00:14:51,470 --> 00:14:57,259
some poor system tour on the net mofford

00:14:54,170 --> 00:15:01,040
descriptors it brings packet into this

00:14:57,259 --> 00:15:03,829
million mapped net mapping say that we

00:15:01,040 --> 00:15:06,529
just got packet for the own Paquette 4

00:15:03,829 --> 00:15:08,899
packets on from ring from buffer index 1

00:15:06,529 --> 00:15:12,559
to 4 so we have this the full package

00:15:08,899 --> 00:15:15,889
now so these packets up destroyers throw

00:15:12,559 --> 00:15:18,230
into network receive skb and we

00:15:15,889 --> 00:15:24,889
intercepted packet at sk date already

00:15:18,230 --> 00:15:29,089
ok so this is in the so the next so this

00:15:24,889 --> 00:15:32,079
is the ok packet buffer 1 2 4 identified

00:15:29,089 --> 00:15:34,939
to be you to be read by the user

00:15:32,079 --> 00:15:37,610
application so it is moved to you that's

00:15:34,939 --> 00:15:40,279
okay so the kind in the user space ring

00:15:37,610 --> 00:15:42,829
it is both head and tailored pointing

00:15:40,279 --> 00:15:45,559
this part so which means nothing to read

00:15:42,829 --> 00:15:48,230
from between head and tail okay so once

00:15:45,559 --> 00:15:50,629
the data is moves then the can also move

00:15:48,230 --> 00:15:54,410
there here so which means you that can

00:15:50,629 --> 00:15:56,720
read the data from here to here they use

00:15:54,410 --> 00:15:59,269
eyes allowed to read before one two

00:15:56,720 --> 00:16:04,759
three four okay so this is the process

00:15:59,269 --> 00:16:08,180
in this part actually so in this form we

00:16:04,759 --> 00:16:11,480
mean consuming packet from head to tail

00:16:08,180 --> 00:16:14,420
you can see in the code right so the

00:16:11,480 --> 00:16:18,319
point is that this for a purchased

00:16:14,420 --> 00:16:21,110
robust is ring and if the application

00:16:18,319 --> 00:16:23,649
identify this packet buffer translated

00:16:21,110 --> 00:16:32,860
from buffer index is a write request

00:16:23,649 --> 00:16:36,920
then it must move the data away from the

00:16:32,860 --> 00:16:40,399
ring which is the mangled by the paste

00:16:36,920 --> 00:16:45,110
on net map so we do this is a buffer

00:16:40,399 --> 00:16:47,600
index okay so I'm gonna explain detail

00:16:45,110 --> 00:16:54,439
later but it basically the moves packet

00:16:47,600 --> 00:16:59,089
away from the link so the so data is not

00:16:54,439 --> 00:17:07,549
in flashed into the buffers and so this

00:16:59,089 --> 00:17:11,270
their injured to be drop restored okay

00:17:07,549 --> 00:17:14,029
so educated how it works and it is very

00:17:11,270 --> 00:17:16,279
easy to use but I will go back to the

00:17:14,029 --> 00:17:20,089
explanation about how to his net map API

00:17:16,279 --> 00:17:23,179
later so the challenge is basically and

00:17:20,089 --> 00:17:25,279
the you see the whisker we are creating

00:17:23,179 --> 00:17:29,750
honey module and having packet buffers

00:17:25,279 --> 00:17:32,510
and the sharing that packet buffer with

00:17:29,750 --> 00:17:35,480
the Linux users really nasty GP IP stack

00:17:32,510 --> 00:17:37,580
so this is where most difficult so for

00:17:35,480 --> 00:17:40,610
example that the GP IP stack always

00:17:37,580 --> 00:17:42,320
called underscore underscore K free sk b

00:17:40,610 --> 00:17:44,750
so the

00:17:42,320 --> 00:17:47,030
but the many of you know the bumping a

00:17:44,750 --> 00:17:50,900
preference count on escape bath doesn't

00:17:47,030 --> 00:17:52,700
work so we have to explain we have the

00:17:50,900 --> 00:17:56,390
bitter or everything or we have to use

00:17:52,700 --> 00:17:58,820
we get and get paid for our own packet

00:17:56,390 --> 00:18:02,480
buffer to survive and the squanders

00:17:58,820 --> 00:18:04,490
every escapee so detecting packet buffer

00:18:02,480 --> 00:18:08,530
series by the stack is also difficult

00:18:04,490 --> 00:18:14,390
because in patch carry on Eric's case or

00:18:08,530 --> 00:18:16,430
rx case so basically the owner of the

00:18:14,390 --> 00:18:21,410
page your actual packet the buffer space

00:18:16,430 --> 00:18:24,890
is us so we have to know that for a

00:18:21,410 --> 00:18:27,680
particular buffer the nobody or a stack

00:18:24,890 --> 00:18:31,790
doesn't have refined their anymore so

00:18:27,680 --> 00:18:34,790
the but the problem is so basically we

00:18:31,790 --> 00:18:39,620
rely on s KBTX tip 0 copy which is also

00:18:34,790 --> 00:18:43,160
used by the send send message 0 copy

00:18:39,620 --> 00:18:45,980
version but we can't use that on receive

00:18:43,160 --> 00:18:48,530
a pass because skb often on network

00:18:45,980 --> 00:18:53,360
receive a stable core the already inbox

00:18:48,530 --> 00:18:57,680
this data at this callback so the so

00:18:53,360 --> 00:18:59,780
it's already execute our core box so we

00:18:57,680 --> 00:19:04,250
have no idea what happens on that buffer

00:18:59,780 --> 00:19:07,400
anymore in IPO a TCP stack in APO TCP

00:19:04,250 --> 00:19:09,080
implementation so in the end all that we

00:19:07,400 --> 00:19:11,840
ended up with using a two-level

00:19:09,080 --> 00:19:12,890
distractors so with own received pass we

00:19:11,840 --> 00:19:16,810
first set

00:19:12,890 --> 00:19:21,220
SKB destructor and that callback

00:19:16,810 --> 00:19:26,450
actually said net s KBTX tip 0 copy and

00:19:21,220 --> 00:19:30,680
the another callback ok so then so that

00:19:26,450 --> 00:19:34,850
we can know the this this new callback

00:19:30,680 --> 00:19:39,920
is fired in actually in the top of TCP

00:19:34,850 --> 00:19:46,090
implementation to Maxim packet the head

00:19:39,920 --> 00:19:46,090
estate developer for the free so

00:19:49,980 --> 00:19:53,730
so yeah this is basically the basic

00:19:51,899 --> 00:19:55,110
mankind about how to share but the point

00:19:53,730 --> 00:19:56,820
is that we use the two levels of a

00:19:55,110 --> 00:19:58,860
quarterback like using Escobar

00:19:56,820 --> 00:20:02,159
quarterback and escapee datapath

00:19:58,860 --> 00:20:07,620
quarterback which is for normally s KBTX

00:20:02,159 --> 00:20:10,049
deputy okapi so the maybe the imprints

00:20:07,620 --> 00:20:15,419
pretty pretty possible using that API is

00:20:10,049 --> 00:20:20,250
a KCM or maybe a pocket version for but

00:20:15,419 --> 00:20:22,500
the problem is I personally think the

00:20:20,250 --> 00:20:25,080
managing buffers for zero copy is very

00:20:22,500 --> 00:20:27,330
very hard so of course we can do some

00:20:25,080 --> 00:20:29,309
zero copy for normal read and write but

00:20:27,330 --> 00:20:32,370
it complicates buffer management

00:20:29,309 --> 00:20:35,100
actually so net map api is actually

00:20:32,370 --> 00:20:39,029
native shared memory api so it is very

00:20:35,100 --> 00:20:42,600
easy to move data say between unique and

00:20:39,029 --> 00:20:44,730
other parts in a simple way so look at

00:20:42,600 --> 00:20:48,029
this code so this is basically how the

00:20:44,730 --> 00:20:51,799
API works including the moving packet

00:20:48,029 --> 00:20:55,019
out of the Nick link without data copy

00:20:51,799 --> 00:20:58,940
so this is against the reading buffer

00:20:55,019 --> 00:21:02,370
from head to tail and this is the part

00:20:58,940 --> 00:21:05,519
this actually translate buffer index to

00:21:02,370 --> 00:21:07,710
the actual buffer pointer then the

00:21:05,519 --> 00:21:09,630
application examine data to see whether

00:21:07,710 --> 00:21:13,559
this is read request or write request

00:21:09,630 --> 00:21:17,970
for example then we can also claims

00:21:13,559 --> 00:21:21,029
packet buffer from net map which we are

00:21:17,970 --> 00:21:23,820
not associated with any ring so we can

00:21:21,029 --> 00:21:26,820
take that we can take that buffer ok

00:21:23,820 --> 00:21:28,860
this is actually throat but we can take

00:21:26,820 --> 00:21:32,700
that extra buffer which is out of the

00:21:28,860 --> 00:21:36,480
ring so then the we can fresh this is

00:21:32,700 --> 00:21:39,330
the data this is a sheer sheer C stand

00:21:36,480 --> 00:21:42,929
for cache line size so we can here is

00:21:39,330 --> 00:21:47,190
this is the part where we swap we move

00:21:42,929 --> 00:21:51,990
data out of the ring and set buffer

00:21:47,190 --> 00:21:56,880
index of extra buffer into the current

00:21:51,990 --> 00:22:00,120
slot so that the net map in the conure

00:21:56,880 --> 00:22:02,690
side use this rod or this para index

00:22:00,120 --> 00:22:02,690
next time

00:22:03,420 --> 00:22:08,830
right so now that we are going to maybe

00:22:07,060 --> 00:22:12,930
the most interesting part or performance

00:22:08,830 --> 00:22:15,670
so the this test the week the client

00:22:12,930 --> 00:22:17,740
continually sent one kilobyte of write

00:22:15,670 --> 00:22:22,410
requests over HTTP POST

00:22:17,740 --> 00:22:28,180
the server uses a single single CPU core

00:22:22,410 --> 00:22:30,730
using the xeon e5 cpu and we also used

00:22:28,180 --> 00:22:32,770
for position memory which is from HPE

00:22:30,730 --> 00:22:37,290
and 80 gigabyte which is the only

00:22:32,770 --> 00:22:39,970
available position memory so far so you

00:22:37,290 --> 00:22:42,730
may see something behind this is the

00:22:39,970 --> 00:22:46,780
dark gray box and Bob don't worry about

00:22:42,730 --> 00:22:49,870
that justice see that gray and light

00:22:46,780 --> 00:22:51,880
gray part so we see the like forty

00:22:49,870 --> 00:22:54,460
percent about forty percent performance

00:22:51,880 --> 00:22:57,070
improvement or depending on number of

00:22:54,460 --> 00:23:00,570
concurrent TCP connections and we also

00:22:57,070 --> 00:23:00,570
see a lot of the latency reduction

00:23:01,170 --> 00:23:07,930
pretty useful

00:23:03,690 --> 00:23:10,120
the conclusion is basically ripples the

00:23:07,930 --> 00:23:12,640
working on paste which is the extension

00:23:10,120 --> 00:23:14,800
to net map or new networking interface

00:23:12,640 --> 00:23:17,950
for high-performance message-oriented

00:23:14,800 --> 00:23:22,300
workload which involves the persistent

00:23:17,950 --> 00:23:24,970
memory support as well the point is we

00:23:22,300 --> 00:23:27,550
DMA right into position to memory back

00:23:24,970 --> 00:23:30,400
to fire or name the packet buffers so

00:23:27,550 --> 00:23:33,550
the coder is now that the the net metal

00:23:30,400 --> 00:23:35,830
sub repositories or subtree so but we

00:23:33,550 --> 00:23:38,140
also have some local update so that you

00:23:35,830 --> 00:23:43,560
want you're interested in prior to out

00:23:38,140 --> 00:23:43,560
so just let me know thanks

00:24:10,120 --> 00:24:13,170
[Music]

00:24:24,250 --> 00:24:35,059
or are they me isn't it for our DMA okay

00:24:32,720 --> 00:24:39,889
with spice you could take a socket or a

00:24:35,059 --> 00:24:41,899
file descriptor some combination of

00:24:39,889 --> 00:24:44,090
splice and a map you might be able to

00:24:41,899 --> 00:24:47,330
get a lot of the same things without one

00:24:44,090 --> 00:24:49,429
be quite so invasive but you think

00:24:47,330 --> 00:24:52,190
sprites other than their own imagine

00:24:49,429 --> 00:24:55,070
that pack recipient Atta then the house

00:24:52,190 --> 00:24:59,059
uh application gets chance to see data

00:24:55,070 --> 00:25:01,340
whether to actually keep or not right it

00:24:59,059 --> 00:25:03,889
would be more like if you could it may

00:25:01,340 --> 00:25:07,639
be like AF packet and I'm steering or

00:25:03,889 --> 00:25:09,679
something might be way I mean I'm just

00:25:07,639 --> 00:25:12,289
trying to think of doing the same thing

00:25:09,679 --> 00:25:14,389
without trying to get as invasive a set

00:25:12,289 --> 00:25:19,070
of patches and like that map is never

00:25:14,389 --> 00:25:21,380
going to go in so let's you know can we

00:25:19,070 --> 00:25:24,559
go can we get something similar without

00:25:21,380 --> 00:25:28,850
going okay quite as far that's that's

00:25:24,559 --> 00:25:31,970
basically what I'm thinking you know

00:25:28,850 --> 00:25:33,679
it's like the point is the way I view

00:25:31,970 --> 00:25:37,130
this is this is research pushing the

00:25:33,679 --> 00:25:39,380
envelope proving a point and to get from

00:25:37,130 --> 00:25:42,529
there to what shows up in a production

00:25:39,380 --> 00:25:44,980
environment we have to iterate over a

00:25:42,529 --> 00:25:48,059
couple of durations of something soon

00:25:44,980 --> 00:25:48,059

YouTube URL: https://www.youtube.com/watch?v=mAmEwm21e9Q


