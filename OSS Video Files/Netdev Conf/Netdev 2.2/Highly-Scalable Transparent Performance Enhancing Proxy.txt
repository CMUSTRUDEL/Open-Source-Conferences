Title: Highly-Scalable Transparent Performance Enhancing Proxy
Publication date: 2018-03-14
Playlist: Netdev 2.2
Description: 
	Speaker: Jae Won Chung, Xiaoxiao Jiang, Jamal Hadi Salim, Roman Mashak, Sriram Sridhar and Manish Kurup
Friday November 10th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?chung-highscalablepep-talk
Captions: 
	00:00:01,120 --> 00:00:11,800
okay um last net that I gave a talk

00:00:05,120 --> 00:00:15,410
about you know evaluating PBR against

00:00:11,800 --> 00:00:19,130
against the two cubic over you know why

00:00:15,410 --> 00:00:22,820
wife I know I'm not the wife idea to 4G

00:00:19,130 --> 00:00:25,060
network and then at that time I actually

00:00:22,820 --> 00:00:28,240
showed that there is a possibility to

00:00:25,060 --> 00:00:33,260
improve the end-to-end peace you know

00:00:28,240 --> 00:00:35,420
TCP performance by using the recent

00:00:33,260 --> 00:00:39,319
generation of TCP congestion avoidance

00:00:35,420 --> 00:00:42,589
algorithm which are based on the you

00:00:39,319 --> 00:00:44,960
know Rd T to actually avoid detect and

00:00:42,589 --> 00:00:48,109
avoid e of the congestion

00:00:44,960 --> 00:00:51,559
however it's probably gonna take some

00:00:48,109 --> 00:00:52,579
you know time for these new algorithms

00:00:51,559 --> 00:00:56,449
to be adopted

00:00:52,579 --> 00:01:00,530
you know widely in their internet but as

00:00:56,449 --> 00:01:02,959
the wireless service provider we might

00:01:00,530 --> 00:01:07,070
not want to actually wait for know at

00:01:02,959 --> 00:01:10,460
that time but do little things to

00:01:07,070 --> 00:01:12,260
actually improve though the TCP

00:01:10,460 --> 00:01:16,100
performance on the wireless so how can

00:01:12,260 --> 00:01:18,880
we do that I mean there is no potential

00:01:16,100 --> 00:01:23,960
to actually use the you know layer for

00:01:18,880 --> 00:01:26,810
performance in this proxy to me what is

00:01:23,960 --> 00:01:31,370
it the bridge the the two different

00:01:26,810 --> 00:01:35,710
conditions are rhythm and that actually

00:01:31,370 --> 00:01:41,840
would be a fairly good solution for us

00:01:35,710 --> 00:01:43,850
okay so if we are doing this then what

00:01:41,840 --> 00:01:48,110
would be the the best way to actually

00:01:43,850 --> 00:01:52,580
implement the dapat first of all I would

00:01:48,110 --> 00:01:54,380
like to you know if you know where's

00:01:52,580 --> 00:02:00,140
that the the fast time to market would

00:01:54,380 --> 00:02:03,680
be a very nice feature and I also would

00:02:00,140 --> 00:02:07,310
like to have our solution fast adapt to

00:02:03,680 --> 00:02:09,289
emerging technology and also reduce the

00:02:07,310 --> 00:02:12,859
software maintain headache so an

00:02:09,289 --> 00:02:14,660
attractive solution for us is to using

00:02:12,859 --> 00:02:17,240
transparent

00:02:14,660 --> 00:02:20,930
performance in its proxy using open

00:02:17,240 --> 00:02:25,640
source TCP proxy on top of Linux so that

00:02:20,930 --> 00:02:30,770
I can use the the Linux TCP networking

00:02:25,640 --> 00:02:32,900
stack and maybe we should I can actually

00:02:30,770 --> 00:02:36,380
use like you know congestion control

00:02:32,900 --> 00:02:39,020
algorithms like PBR or DC TCP or even

00:02:36,380 --> 00:02:40,820
let's say a homegrown you know TCP

00:02:39,020 --> 00:02:44,590
algorithm for the wireless side while

00:02:40,820 --> 00:02:54,320
still using the cubic on the server side

00:02:44,590 --> 00:02:58,340
basically okay so so the deployment

00:02:54,320 --> 00:03:01,700
model that we have been thinking of is

00:02:58,340 --> 00:03:04,970
the following so first of all the proxy

00:03:01,700 --> 00:03:07,040
servers are front ended by layer 3 at

00:03:04,970 --> 00:03:08,870
load balancers in this talk I'm not

00:03:07,040 --> 00:03:11,420
going to talk about the the routing or

00:03:08,870 --> 00:03:13,550
the load balancing design because

00:03:11,420 --> 00:03:20,770
probably you guys actually know better

00:03:13,550 --> 00:03:23,420
than me also in this picture basically

00:03:20,770 --> 00:03:25,490
you know it seems like it's a two

00:03:23,420 --> 00:03:30,130
different network with a two separate

00:03:25,490 --> 00:03:33,350
spine but in reality it could be a one

00:03:30,130 --> 00:03:37,850
this one and this one could be a same

00:03:33,350 --> 00:03:39,830
spine with the different VLAN so that is

00:03:37,850 --> 00:03:43,550
under there possibility in deployment

00:03:39,830 --> 00:03:45,560
scenario and that would be the worst

00:03:43,550 --> 00:03:48,050
scenario for me to actually handle the

00:03:45,560 --> 00:03:50,390
the pop and push the the violent text

00:03:48,050 --> 00:03:52,910
and that might actually impact the

00:03:50,390 --> 00:03:58,970
performance so that is actually under

00:03:52,910 --> 00:04:04,100
there thing that I that we assumed for

00:03:58,970 --> 00:04:06,320
the POC and then we got our deploy we

00:04:04,100 --> 00:04:10,430
are assuming we are going to deploy the

00:04:06,320 --> 00:04:14,120
l3 l4 proxies in like you know some of

00:04:10,430 --> 00:04:17,180
the servers and then have the layer

00:04:14,120 --> 00:04:21,140
three load balancers actually load

00:04:17,180 --> 00:04:24,110
balancer traffic perhaps using ecmp okay

00:04:21,140 --> 00:04:29,200
so that's basically our assumption of

00:04:24,110 --> 00:04:35,690
the deployment model okay so

00:04:29,200 --> 00:04:38,020
so this call flow just shows that there

00:04:35,690 --> 00:04:41,750
are are there will be you know two

00:04:38,020 --> 00:04:46,060
connections on the other sides of the

00:04:41,750 --> 00:04:50,180
transparent proxy and the source IP is

00:04:46,060 --> 00:04:53,630
spoofed on the the destination when we

00:04:50,180 --> 00:04:59,990
actually initiate the the TCP connection

00:04:53,630 --> 00:05:04,760
to the server for this POC we use the

00:04:59,990 --> 00:05:08,060
dexia for the to actually email like the

00:05:04,760 --> 00:05:12,740
client and server and then in the middle

00:05:08,060 --> 00:05:17,180
we use the blade server with running the

00:05:12,740 --> 00:05:19,010
h a proxy version 1.4 in transparent

00:05:17,180 --> 00:05:22,640
mode I'm gonna explain what what I mean

00:05:19,010 --> 00:05:27,020
by transparent mode next slide we end up

00:05:22,640 --> 00:05:30,550
running 20 H a proxy processes in that

00:05:27,020 --> 00:05:35,690
server so this actually summarized the

00:05:30,550 --> 00:05:39,680
the the our blade server which has two

00:05:35,690 --> 00:05:45,710
Numa circuits and the CPUs that we used

00:05:39,680 --> 00:05:52,940
is Intel Xeon e5 I believe this is Sandy

00:05:45,710 --> 00:05:56,270
Bridge has 14 course and two thread per

00:05:52,940 --> 00:05:58,280
core so basically 28 hyper threads and

00:05:56,270 --> 00:06:03,080
then Nick that we actually use for this

00:05:58,280 --> 00:06:05,330
POC was Intel I 40 ecard

00:06:03,080 --> 00:06:11,530
and the the kernel version that we

00:06:05,330 --> 00:06:14,390
actually run was 4.11 daro okay so

00:06:11,530 --> 00:06:17,420
typical transparent proxy configuration

00:06:14,390 --> 00:06:21,110
if you actually Google like how - how -

00:06:17,420 --> 00:06:23,990
you know configure a TCP transparent TCP

00:06:21,110 --> 00:06:25,970
proxy you probably want to say this so

00:06:23,990 --> 00:06:29,560
the basic thing is first of all you

00:06:25,970 --> 00:06:34,340
actually have to set the socket option

00:06:29,560 --> 00:06:37,760
IP transparent socket option so that it

00:06:34,340 --> 00:06:40,070
will you know except the the packet even

00:06:37,760 --> 00:06:41,600
though the destination IP is not you

00:06:40,070 --> 00:06:44,930
know towards me so

00:06:41,600 --> 00:06:48,580
you will only use the port number to

00:06:44,930 --> 00:06:51,530
actually accept the de packet okay so

00:06:48,580 --> 00:06:54,950
that's one thing and then basically now

00:06:51,530 --> 00:06:59,690
we actually have to also set up the

00:06:54,950 --> 00:07:04,010
filter for the the packet that needs to

00:06:59,690 --> 00:07:08,840
be a proxy service you can use IP tables

00:07:04,010 --> 00:07:12,430
to you know set up the filter so

00:07:08,840 --> 00:07:15,470
basically the first line says all TCP

00:07:12,430 --> 00:07:19,970
destination port 80 which is HTTP you

00:07:15,470 --> 00:07:25,400
know proxy HTTP traffic use the the T

00:07:19,970 --> 00:07:28,010
proxy destination and then the T proxy

00:07:25,400 --> 00:07:31,550
if it's loosening on port 1 2 3 4 that

00:07:28,010 --> 00:07:35,360
needs to be specified here and then it's

00:07:31,550 --> 00:07:40,430
also asking to use the SK be mark set to

00:07:35,360 --> 00:07:43,640
1 so okay and for the returned traffic

00:07:40,430 --> 00:07:47,270
this applies to sin and then all the

00:07:43,640 --> 00:07:49,550
returned traffic basically acts or the

00:07:47,270 --> 00:07:53,300
data packets and stuff like that the

00:07:49,550 --> 00:07:57,320
second line says TCP if we have a socket

00:07:53,300 --> 00:08:01,900
a local socket in the in basic loopback

00:07:57,320 --> 00:08:05,330
device then set the mark equals to 1 so

00:08:01,900 --> 00:08:07,190
that's basically what you what you

00:08:05,330 --> 00:08:10,510
usually do to actually set up the other

00:08:07,190 --> 00:08:14,330
filter then basically you just add a IP

00:08:10,510 --> 00:08:18,320
rule to a certain table in this case

00:08:14,330 --> 00:08:21,070
table 1 say anything any marked packet

00:08:18,320 --> 00:08:26,870
you know send the packet to the you know

00:08:21,070 --> 00:08:29,030
the lubic device basically so this is

00:08:26,870 --> 00:08:32,150
the things that you actually have to set

00:08:29,030 --> 00:08:35,240
up to actually do the transparent proxy

00:08:32,150 --> 00:08:38,090
routing however when we are actually

00:08:35,240 --> 00:08:41,840
facing this I don't we didn't really

00:08:38,090 --> 00:08:47,320
want to actually do this into host what

00:08:41,840 --> 00:08:49,910
is it the host you know machine because

00:08:47,320 --> 00:08:52,010
if you want to actually prioritize this

00:08:49,910 --> 00:08:53,810
you know the thing then you probably

00:08:52,010 --> 00:08:56,900
want to log into

00:08:53,810 --> 00:09:00,020
there was you know host and do some

00:08:56,900 --> 00:09:03,290
other things and setting all all this

00:09:00,020 --> 00:09:06,710
routing into host days actually making

00:09:03,290 --> 00:09:10,370
the the product not really good in terms

00:09:06,710 --> 00:09:14,270
of maintenance and all that stuff so our

00:09:10,370 --> 00:09:17,920
first design principle in doing this we

00:09:14,270 --> 00:09:20,779
wanted to use a network container and

00:09:17,920 --> 00:09:25,580
put the proxy in the network container

00:09:20,779 --> 00:09:28,339
okay and this is in the second design

00:09:25,580 --> 00:09:30,560
principle that we actually have was to

00:09:28,339 --> 00:09:32,089
maximize the parallel processing I mean

00:09:30,560 --> 00:09:36,380
this is not a new thing I mean you

00:09:32,089 --> 00:09:42,080
probably do this for all the the Linux

00:09:36,380 --> 00:09:46,880
based applications and also we try to

00:09:42,080 --> 00:09:48,620
minimize the you know okay things of

00:09:46,880 --> 00:09:50,690
course to have the Numa boundary

00:09:48,620 --> 00:09:54,770
basically to reduce the interrupts and

00:09:50,690 --> 00:09:56,810
context switches and also we also run

00:09:54,770 --> 00:10:00,230
all the proxy instances on the same new

00:09:56,810 --> 00:10:04,310
one they're not just not just the the

00:10:00,230 --> 00:10:07,190
fast path but all the nature proxies we

00:10:04,310 --> 00:10:09,860
end up running in the same Numa node

00:10:07,190 --> 00:10:14,380
that actually manages the PCI bus

00:10:09,860 --> 00:10:17,660
connect to the PCI bus 4d of the neck

00:10:14,380 --> 00:10:22,610
alright so this diagram actually shows

00:10:17,660 --> 00:10:27,800
the inside the the inside of DM the host

00:10:22,610 --> 00:10:30,200
so basically what happens is for um so

00:10:27,800 --> 00:10:32,870
first of all we actually use the the two

00:10:30,200 --> 00:10:36,650
different VLANs for ingress and egress

00:10:32,870 --> 00:10:40,070
sites so let's say a packet comes in to

00:10:36,650 --> 00:10:42,980
the ethernet and then basically we use

00:10:40,070 --> 00:10:46,690
it use the TC to actually you know based

00:10:42,980 --> 00:10:51,890
on the devii line ID we use TC to

00:10:46,690 --> 00:10:56,600
redirect the de packet to either C out

00:10:51,890 --> 00:10:59,440
ve or s out B's so and that will

00:10:56,600 --> 00:11:04,700
actually deliver the packet to the scene

00:10:59,440 --> 00:11:07,190
and basically the VLAN will be actually

00:11:04,700 --> 00:11:11,210
popped at this point of time

00:11:07,190 --> 00:11:13,790
and what happens within the network

00:11:11,210 --> 00:11:17,120
container is everything actually you

00:11:13,790 --> 00:11:22,370
know coming out of the this bees will be

00:11:17,120 --> 00:11:24,980
just you know directed to the diello so

00:11:22,370 --> 00:11:28,220
Lou Peck device so that I don't really

00:11:24,980 --> 00:11:32,060
have to worry anything about you know

00:11:28,220 --> 00:11:34,750
the the process routing so that's how is

00:11:32,060 --> 00:11:37,130
actually configured and we also

00:11:34,750 --> 00:11:40,160
configure the default route to the

00:11:37,130 --> 00:11:43,010
client side first of all and then have

00:11:40,160 --> 00:11:44,330
to H a proxy to actually bind the server

00:11:43,010 --> 00:11:46,670
side connection whenever you actually

00:11:44,330 --> 00:11:51,620
makes a server side condition just bound

00:11:46,670 --> 00:11:53,480
it to the know this bees so that we can

00:11:51,620 --> 00:11:58,760
actually very simplify the of the

00:11:53,480 --> 00:12:02,090
routing okay so by doing that we don't

00:11:58,760 --> 00:12:10,330
really even need you even need the a IP

00:12:02,090 --> 00:12:10,330
you know table rules basically because

00:12:10,390 --> 00:12:16,670
if you look at the IP table rules to

00:12:13,370 --> 00:12:20,480
general IP table rules

00:12:16,670 --> 00:12:22,520
it's basically IP table rules is to

00:12:20,480 --> 00:12:25,460
actually just use the of the you know

00:12:22,520 --> 00:12:27,200
filter which packet to actually service

00:12:25,460 --> 00:12:31,250
and then which packet to actually give

00:12:27,200 --> 00:12:34,700
to the loop ik device right however in

00:12:31,250 --> 00:12:38,150
this model we are assuming we will

00:12:34,700 --> 00:12:41,180
actually you know use the you know TC to

00:12:38,150 --> 00:12:44,660
do the packet routing or the packet

00:12:41,180 --> 00:12:46,790
filtering later on so assuming that

00:12:44,660 --> 00:12:49,130
anything that actually comes to this you

00:12:46,790 --> 00:12:52,070
know be it will be actually service so

00:12:49,130 --> 00:12:56,690
we don't really have to worry about you

00:12:52,070 --> 00:13:01,240
know filtering folder okay so basically

00:12:56,690 --> 00:13:01,240
that is the configuration very simple

00:13:01,690 --> 00:13:11,500
okay so so three scenarios the system

00:13:07,340 --> 00:13:14,600
configuration scenarios that we

00:13:11,500 --> 00:13:18,440
considers are the following

00:13:14,600 --> 00:13:19,220
basically we once actually just used a

00:13:18,440 --> 00:13:21,740
chronal even

00:13:19,220 --> 00:13:24,170
without the netfilter hooks and then

00:13:21,740 --> 00:13:26,750
because we don't really need iptables

00:13:24,170 --> 00:13:29,629
and then the next one was we actually

00:13:26,750 --> 00:13:32,500
used the the kernel with IP table hooks

00:13:29,629 --> 00:13:35,360
but we didn't use the IP you know

00:13:32,500 --> 00:13:37,550
netfilter hooks but no IP table rules

00:13:35,360 --> 00:13:39,920
were configured and then the third

00:13:37,550 --> 00:13:45,230
scenario was to actually you know use

00:13:39,920 --> 00:13:48,230
the IP table to actually do the what is

00:13:45,230 --> 00:13:52,730
it the sort of netting because this is

00:13:48,230 --> 00:13:55,550
actually interesting is because if you

00:13:52,730 --> 00:13:58,430
if your proxy is just the you listen to

00:13:55,550 --> 00:14:00,199
a handful of the ports destination port

00:13:58,430 --> 00:14:02,930
then you probably don't even need this

00:14:00,199 --> 00:14:05,600
but if you want to actually cover a wide

00:14:02,930 --> 00:14:08,810
range of you know pores to be terminated

00:14:05,600 --> 00:14:10,790
then maybe using the IP table might be

00:14:08,810 --> 00:14:14,360
in a better way and then do the netting

00:14:10,790 --> 00:14:16,730
might be do you know better way so we

00:14:14,360 --> 00:14:25,550
actually considered you know this three

00:14:16,730 --> 00:14:27,490
scenarios okay all right so performance

00:14:25,550 --> 00:14:31,699
tuning options that we consider

00:14:27,490 --> 00:14:34,309
obviously the NIC RSS so distributing

00:14:31,699 --> 00:14:36,949
traffic Omo multiple receiving queues

00:14:34,309 --> 00:14:39,579
will actually you know increase the

00:14:36,949 --> 00:14:46,550
parallelism in packet processing so that

00:14:39,579 --> 00:14:49,730
is a must an option to consider and then

00:14:46,550 --> 00:14:52,550
we actually consider symmetric and

00:14:49,730 --> 00:14:55,399
asymmetric RSS what I mean by that is if

00:14:52,550 --> 00:14:57,170
all did you know the TCP packets they

00:14:55,399 --> 00:15:01,220
have act packets are actually handled by

00:14:57,170 --> 00:15:03,050
a same core then basically we might

00:15:01,220 --> 00:15:06,170
actually have a little bit better you

00:15:03,050 --> 00:15:07,939
know performance so that is actually

00:15:06,170 --> 00:15:12,350
another option that we actually

00:15:07,939 --> 00:15:14,389
considering okay the next thing is fast

00:15:12,350 --> 00:15:18,889
path Numa buying binding what I mean by

00:15:14,389 --> 00:15:21,529
that is we actually bind the RSS cues to

00:15:18,889 --> 00:15:23,290
the NIC attached Numa node so that means

00:15:21,529 --> 00:15:26,509
the Numa know that's actually you know

00:15:23,290 --> 00:15:28,850
directly connected to the the NIC

00:15:26,509 --> 00:15:33,100
through the PCI bus is actually what we

00:15:28,850 --> 00:15:38,740
are binding the Doris's

00:15:33,100 --> 00:15:41,140
you know cues and in our evaluation we

00:15:38,740 --> 00:15:43,930
didn't actually show the number but we

00:15:41,140 --> 00:15:46,000
first actually we actually forget to do

00:15:43,930 --> 00:15:48,550
this and then run the test and then we

00:15:46,000 --> 00:15:50,710
actually get like you know probably

00:15:48,550 --> 00:15:53,170
quarter of the performance of what we

00:15:50,710 --> 00:15:56,950
could actually get at the end so you

00:15:53,170 --> 00:15:58,990
know I just want to mention that but I'm

00:15:56,950 --> 00:16:00,670
not gonna actually show the result in

00:15:58,990 --> 00:16:03,520
this presentation okay

00:16:00,670 --> 00:16:06,400
the other thing is actually proxy Numa

00:16:03,520 --> 00:16:08,790
binding what I mean by that is we

00:16:06,400 --> 00:16:12,610
actually run the proxy process itself

00:16:08,790 --> 00:16:14,140
themselves onto the Numa node that

00:16:12,610 --> 00:16:17,080
actually connected to the at the neck

00:16:14,140 --> 00:16:20,440
and surprisingly I mean even though we

00:16:17,080 --> 00:16:23,980
were do we were using splicing it

00:16:20,440 --> 00:16:28,570
actually impacted the performance so we

00:16:23,980 --> 00:16:32,200
were thinking that even the socket

00:16:28,570 --> 00:16:35,470
structure access through the Numa

00:16:32,200 --> 00:16:38,620
boundary is actually causing some you

00:16:35,470 --> 00:16:41,140
know performance degradation so that was

00:16:38,620 --> 00:16:43,420
actually one thing that we consider okay

00:16:41,140 --> 00:16:45,910
and obviously splicing because we are

00:16:43,420 --> 00:16:48,130
elf for the layer four I mean I don't

00:16:45,910 --> 00:16:51,850
really need to read all the packets out

00:16:48,130 --> 00:16:55,060
of the kernel so we are actually

00:16:51,850 --> 00:17:01,090
considered an O splicing option as much

00:16:55,060 --> 00:17:02,800
as possible and for for curiosity I mean

00:17:01,090 --> 00:17:06,459
I just we just wanted to understand like

00:17:02,800 --> 00:17:09,189
you know what is the performance of HTTP

00:17:06,459 --> 00:17:11,079
proxy probably you know this is more

00:17:09,189 --> 00:17:14,199
like you know the header processing and

00:17:11,079 --> 00:17:16,890
all this stuff I'm not sure if the you

00:17:14,199 --> 00:17:19,420
know H a proxy has the best

00:17:16,890 --> 00:17:22,060
implementation of the head of processing

00:17:19,420 --> 00:17:23,800
and stuff like that but anyway we

00:17:22,060 --> 00:17:29,320
actually just so you know evaluate that

00:17:23,800 --> 00:17:32,860
as well okay before we actually do the

00:17:29,320 --> 00:17:34,750
evaluation of the proxy performance the

00:17:32,860 --> 00:17:36,940
first thing we actually have to solve is

00:17:34,750 --> 00:17:39,520
l2 performance I mean we need to make

00:17:36,940 --> 00:17:42,340
sure the our test bed is actually

00:17:39,520 --> 00:17:48,180
configured right not to be the

00:17:42,340 --> 00:17:48,180
bottleneck at the l2 so what we did was

00:17:48,360 --> 00:17:55,560
we actually first of all created like

00:17:51,740 --> 00:17:57,990
120 ATC graphs because we were thinking

00:17:55,560 --> 00:18:01,340
to actually use a TC to do the no

00:17:57,990 --> 00:18:04,920
service routing and service

00:18:01,340 --> 00:18:10,350
orchestration we actually created like

00:18:04,920 --> 00:18:12,630
128 TC graphs and then and then actually

00:18:10,350 --> 00:18:15,840
use those things just a little bit

00:18:12,630 --> 00:18:19,230
probably 128 is actually not enough to

00:18:15,840 --> 00:18:22,440
do the service orchestration such as for

00:18:19,230 --> 00:18:25,410
this source IP I'm not I don't want to

00:18:22,440 --> 00:18:27,570
do the you know what is that the provide

00:18:25,410 --> 00:18:29,640
the proxy service and stuff like that if

00:18:27,570 --> 00:18:32,130
you need to actually implement that it's

00:18:29,640 --> 00:18:35,220
probably gonna need much more than 128

00:18:32,130 --> 00:18:38,460
ISA graph but to actually start up I

00:18:35,220 --> 00:18:41,310
mean we actually know configure like 128

00:18:38,460 --> 00:18:46,340
ISA graphs and and send a packet through

00:18:41,310 --> 00:18:51,420
it okay we actually used a 12 RSS cues

00:18:46,340 --> 00:18:54,720
and then the queue disk was MP mq prior

00:18:51,420 --> 00:18:57,630
and then we play around with the

00:18:54,720 --> 00:19:00,140
Nannette budget and end up like setting

00:18:57,630 --> 00:19:03,650
up setting it to you know four thousand

00:19:00,140 --> 00:19:06,240
okay and then we actually used the

00:19:03,650 --> 00:19:08,250
qualified metric of no packets per

00:19:06,240 --> 00:19:09,570
second and average latency so those are

00:19:08,250 --> 00:19:14,670
the two things that we actually looked

00:19:09,570 --> 00:19:17,760
at so one thing that we actually found

00:19:14,670 --> 00:19:19,710
before actually use the of the mq you

00:19:17,760 --> 00:19:22,080
know prior to disk we just used a

00:19:19,710 --> 00:19:24,590
private kritis and then we found that

00:19:22,080 --> 00:19:28,710
the performance is actually horrible

00:19:24,590 --> 00:19:35,390
because because of the lock so we moved

00:19:28,710 --> 00:19:39,840
to you know the p mq prior to disk but

00:19:35,390 --> 00:19:41,160
the we still had like not a very good

00:19:39,840 --> 00:19:43,800
you know performance and then we

00:19:41,160 --> 00:19:47,250
actually found that because we were

00:19:43,800 --> 00:19:49,200
actually using what is that the davila

00:19:47,250 --> 00:19:52,220
and TC actions to actually pump and push

00:19:49,200 --> 00:19:57,420
the the VLAN and that was actually using

00:19:52,220 --> 00:20:00,970
spin lock and we were hold by spin lock

00:19:57,420 --> 00:20:07,230
so we end up changing that to use the

00:20:00,970 --> 00:20:09,940
see you and Gemma I mean are we done

00:20:07,230 --> 00:20:13,840
okay so we are actually submitting to

00:20:09,940 --> 00:20:17,860
patch at this moment so after we get

00:20:13,840 --> 00:20:21,539
around with this issue then basically we

00:20:17,860 --> 00:20:24,990
were able to push a lot of you know

00:20:21,539 --> 00:20:28,390
pretty good you know traffic through

00:20:24,990 --> 00:20:32,350
through this system so basically we use

00:20:28,390 --> 00:20:35,730
the UDP packets if you look at it this

00:20:32,350 --> 00:20:38,440
one so basically instead of a ship proxy

00:20:35,730 --> 00:20:40,720
we just made sure that you know this

00:20:38,440 --> 00:20:42,610
whatever comes here do UDP packet goes

00:20:40,720 --> 00:20:44,380
here and then whatever comes this you

00:20:42,610 --> 00:20:48,159
Deepika goes here we actually figured

00:20:44,380 --> 00:20:49,780
that and then push the you know the

00:20:48,159 --> 00:20:53,909
packet from the server side and then

00:20:49,780 --> 00:20:59,140
client side the YouTube packets in at

00:20:53,909 --> 00:21:04,390
10:10 Giga bps each side so that's how

00:20:59,140 --> 00:21:08,230
we actually know did this test okay so

00:21:04,390 --> 00:21:12,330
by doing that when the de pekka size was

00:21:08,230 --> 00:21:17,320
actually a small we were able to achieve

00:21:12,330 --> 00:21:23,220
around seven million packets per second

00:21:17,320 --> 00:21:23,220
but as we actually grow deep existential

00:21:26,350 --> 00:21:32,080
where is it the throughput of 20 gig

00:21:29,890 --> 00:21:35,890
this 20 gig is actually bi-directional

00:21:32,080 --> 00:21:38,950
so sir of client to server side UDP and

00:21:35,890 --> 00:21:42,570
then server to clients our UDP was

00:21:38,950 --> 00:21:46,600
actually you know was actually 20 gate

00:21:42,570 --> 00:21:50,020
so with this all true configuration this

00:21:46,600 --> 00:21:53,260
actually shows that the NIC and the de

00:21:50,020 --> 00:22:01,289
passe is not the bottleneck so we moved

00:21:53,260 --> 00:22:05,559
on to the next test okay so first of all

00:22:01,289 --> 00:22:09,929
we use three different systems so this

00:22:05,559 --> 00:22:11,830
actually graph shows the the the

00:22:09,929 --> 00:22:13,929
transaction per second a cheap

00:22:11,830 --> 00:22:14,800
transaction per second this transaction

00:22:13,929 --> 00:22:16,990
per second

00:22:14,800 --> 00:22:19,450
is not TCP transaction per second but

00:22:16,990 --> 00:22:23,590
pair of TCP transaction per second I

00:22:19,450 --> 00:22:25,930
would say this is proxy transaction per

00:22:23,590 --> 00:22:29,830
second and that transaction per second

00:22:25,930 --> 00:22:34,830
number was let's look at the blue

00:22:29,830 --> 00:22:40,990
compared to blue lines because the blue

00:22:34,830 --> 00:22:43,060
so this is a system using new netfilter

00:22:40,990 --> 00:22:45,160
net filter was not there this is net

00:22:43,060 --> 00:22:47,550
filter compiled in this is using IP

00:22:45,160 --> 00:22:51,610
table you can actually see a slight

00:22:47,550 --> 00:22:55,030
where as the degradation in the packets

00:22:51,610 --> 00:22:59,350
words at the transaction per second but

00:22:55,030 --> 00:23:01,960
it wasn't you know that much and so

00:22:59,350 --> 00:23:05,410
basically when we were using the kernel

00:23:01,960 --> 00:23:07,900
without even you know without net filter

00:23:05,410 --> 00:23:10,810
then we were able to I mean this is with

00:23:07,900 --> 00:23:15,330
all the you know optimization we were

00:23:10,810 --> 00:23:20,950
able to you know get around 97k

00:23:15,330 --> 00:23:22,600
transaction per second and 94 when we

00:23:20,950 --> 00:23:26,100
are actually using the kernel that

00:23:22,600 --> 00:23:30,420
actually has the hooks netfilter hoops

00:23:26,100 --> 00:23:34,540
with the IP table rules we actually know

00:23:30,420 --> 00:23:37,960
got like 93 K but I believe this is

00:23:34,540 --> 00:23:41,050
actually ignoring for our cases so I

00:23:37,960 --> 00:23:43,240
would say based on our need we might

00:23:41,050 --> 00:23:49,060
choose this option or at this option

00:23:43,240 --> 00:23:57,010
okay all right and also the next thing

00:23:49,060 --> 00:23:59,470
is the Numa binding so when I I didn't

00:23:57,010 --> 00:24:02,200
so accidentally we didn't actually set

00:23:59,470 --> 00:24:04,680
the Numa binding for the proxy processes

00:24:02,200 --> 00:24:07,930
the 20 proxy processes that we have and

00:24:04,680 --> 00:24:10,660
it was all over the place I mean like

00:24:07,930 --> 00:24:14,980
you know either the Numa one or new at 2

00:24:10,660 --> 00:24:16,960
and the performance was actually going

00:24:14,980 --> 00:24:19,750
up and down I mean it was not even like

00:24:16,960 --> 00:24:22,480
you know what is it the consistent so

00:24:19,750 --> 00:24:25,690
when we actually look at the dir queue

00:24:22,480 --> 00:24:28,630
some cores actually have a very high IQ

00:24:25,690 --> 00:24:31,900
you know your allegations I said what is

00:24:28,630 --> 00:24:35,400
going on and I actually look at little

00:24:31,900 --> 00:24:39,550
father and then basically found that the

00:24:35,400 --> 00:24:45,310
the whichever was that the core that is

00:24:39,550 --> 00:24:49,090
not that actually has to has to serve

00:24:45,310 --> 00:24:52,150
the memory access data for the other or

00:24:49,090 --> 00:24:55,360
is that the proxy was actually causing a

00:24:52,150 --> 00:24:58,780
high RQ utilization CP realization so

00:24:55,360 --> 00:25:02,500
after we make sure everything comes to

00:24:58,780 --> 00:25:06,250
the pneuma one note and basically the

00:25:02,500 --> 00:25:10,630
performance actually no jumped up to 97

00:25:06,250 --> 00:25:17,140
so this is one of the things that we

00:25:10,630 --> 00:25:22,180
actually found ok alright so the next

00:25:17,140 --> 00:25:25,990
thing is the RSS symmetric or asymmetric

00:25:22,180 --> 00:25:28,900
so I was actually expecting a little

00:25:25,990 --> 00:25:31,030
more than this when we actually do the

00:25:28,900 --> 00:25:34,870
asymmetry I mean symmetric over a

00:25:31,030 --> 00:25:36,660
symmetric but the there was a small you

00:25:34,870 --> 00:25:40,090
know increase increase in performance

00:25:36,660 --> 00:25:42,880
but not too much I mean if you compare

00:25:40,090 --> 00:25:45,280
the blue and and yellow or the green is

00:25:42,880 --> 00:25:48,790
it green or yellow I think it's green

00:25:45,280 --> 00:25:56,440
there so then basically the improvement

00:25:48,790 --> 00:26:02,650
is not know that much okay alright so

00:25:56,440 --> 00:26:04,390
then this impact of splicing so I was

00:26:02,650 --> 00:26:06,340
expecting a little more than this but

00:26:04,390 --> 00:26:11,440
you know the fact that we are using

00:26:06,340 --> 00:26:14,590
aka object download basically the impact

00:26:11,440 --> 00:26:18,040
of splicing is will be not too much but

00:26:14,590 --> 00:26:20,770
know as we increase the object size I

00:26:18,040 --> 00:26:24,160
believe the impact of you know splicing

00:26:20,770 --> 00:26:27,430
the socket will will actually improve

00:26:24,160 --> 00:26:30,760
the performance however what we actually

00:26:27,430 --> 00:26:34,390
found is this is actually the case with

00:26:30,760 --> 00:26:37,120
the TCP TCP proxy and then this is when

00:26:34,390 --> 00:26:38,860
we actually said to the TCP mode of the

00:26:37,120 --> 00:26:45,340
I mean the proxy mode of the H

00:26:38,860 --> 00:26:47,110
prostitue HTTP HTTP mode of a small you

00:26:45,340 --> 00:26:50,500
know object transfer we didn't really

00:26:47,110 --> 00:26:54,760
have any benefit of splicing as expected

00:26:50,500 --> 00:26:59,140
and what's a little bit not intuitive

00:26:54,760 --> 00:27:02,290
was this is actually this graph actually

00:26:59,140 --> 00:27:06,280
shows the delay in terms of you know

00:27:02,290 --> 00:27:08,890
time to the first byte in terms of

00:27:06,280 --> 00:27:11,710
response byte was actually going going

00:27:08,890 --> 00:27:16,540
up when splicing was actually on so it's

00:27:11,710 --> 00:27:21,580
like that's a little bit awkward but

00:27:16,540 --> 00:27:28,059
what we found was it was basically big

00:27:21,580 --> 00:27:31,210
even though the splicing is on what

00:27:28,059 --> 00:27:33,280
happens is that the the receiver side I

00:27:31,210 --> 00:27:36,490
mean when they actually get received the

00:27:33,280 --> 00:27:39,460
after the header it actually read us a

00:27:36,490 --> 00:27:41,830
header to a certain extent and you know

00:27:39,460 --> 00:27:46,570
send IRISA and and actually transferred

00:27:41,830 --> 00:27:49,360
that HTTP header and write it to the

00:27:46,570 --> 00:27:52,330
decline sighs okay and then it actually

00:27:49,360 --> 00:27:54,970
calls the what is it displacing and then

00:27:52,330 --> 00:27:56,250
splicing system call itself actually

00:27:54,970 --> 00:27:59,650
delayed a little bit

00:27:56,250 --> 00:28:01,559
so even the data was actually a small

00:27:59,650 --> 00:28:04,120
number of data it was actually in the

00:28:01,559 --> 00:28:05,110
transmit queue of towards the client

00:28:04,120 --> 00:28:07,870
side

00:28:05,110 --> 00:28:10,540
it was just waiting for a little more

00:28:07,870 --> 00:28:12,010
data to actually come before you sent

00:28:10,540 --> 00:28:14,110
the first packet and that was actually

00:28:12,010 --> 00:28:19,650
you know causing little over you know

00:28:14,110 --> 00:28:19,650
more delay here okay

00:28:22,260 --> 00:28:30,429
okay and for the last one we actually

00:28:26,880 --> 00:28:34,600
when we compared the TCP proxy and HTTP

00:28:30,429 --> 00:28:36,990
proxy the mode change then as you can

00:28:34,600 --> 00:28:40,210
see we actually have a huge you know

00:28:36,990 --> 00:28:42,760
degradation in terms of the performance

00:28:40,210 --> 00:28:45,760
when we used HTTP that is actually as

00:28:42,760 --> 00:28:52,090
expected but we haven't actually looked

00:28:45,760 --> 00:28:55,570
father to what what actually caused this

00:28:52,090 --> 00:28:58,450
much of degradation I mean if our hunch

00:28:55,570 --> 00:29:02,980
was if we are actually you know parsing

00:28:58,450 --> 00:29:05,650
the what is it the the headers without

00:29:02,980 --> 00:29:09,720
copying the headers all over the place

00:29:05,650 --> 00:29:12,010
then I was expecting about you know same

00:29:09,720 --> 00:29:14,740
similar performance not like here but

00:29:12,010 --> 00:29:17,500
maybe around here but it seems like you

00:29:14,740 --> 00:29:24,550
know Asia proxy is not too much

00:29:17,500 --> 00:29:28,300
optimized in header processing okay so

00:29:24,550 --> 00:29:30,880
the conclusion of this talk is as

00:29:28,300 --> 00:29:35,230
follows so we actually build a high

00:29:30,880 --> 00:29:37,780
scalable transparent performance

00:29:35,230 --> 00:29:40,290
enhancement proxy on Linux using an

00:29:37,780 --> 00:29:43,870
open-source h-shaped proxy

00:29:40,290 --> 00:29:46,240
simple but I think it's actually um

00:29:43,870 --> 00:29:49,000
efficient I mean in order to prioritize

00:29:46,240 --> 00:29:54,429
this we probably need you know more work

00:29:49,000 --> 00:29:57,309
but as a POC I think this was this POC

00:29:54,429 --> 00:29:59,950
was actually enough to you know give the

00:29:57,309 --> 00:30:04,050
the upper limit of in a performance that

00:29:59,950 --> 00:30:08,340
we are expecting from a single card okay

00:30:04,050 --> 00:30:13,990
so our transparent path achieved around

00:30:08,340 --> 00:30:16,270
97 so close to 100k TPS only using a

00:30:13,990 --> 00:30:19,020
single new Manos so the second Numa node

00:30:16,270 --> 00:30:26,230
was actually completely doing nothing

00:30:19,020 --> 00:30:29,440
and with no c14 core CPU and hyper

00:30:26,230 --> 00:30:32,380
thread I'm 28 RSS queues using our

00:30:29,440 --> 00:30:34,630
excursion with the aka object size we

00:30:32,380 --> 00:30:41,890
were able to achieve you know this

00:30:34,630 --> 00:30:46,059
number right hundred K TPS okay so and

00:30:41,890 --> 00:30:48,159
also we show the show the the major

00:30:46,059 --> 00:30:52,299
performance tuning designs and then what

00:30:48,159 --> 00:30:57,040
they know they are impact on the this

00:30:52,299 --> 00:31:01,830
particular in a workload and that's our

00:30:57,040 --> 00:31:04,200
contribution here so the the future work

00:31:01,830 --> 00:31:07,379
so we would like to I mean

00:31:04,200 --> 00:31:10,649
we only had like you know two 10-gig

00:31:07,379 --> 00:31:13,350
axial port for this project because you

00:31:10,649 --> 00:31:17,100
know our development team was actually

00:31:13,350 --> 00:31:19,139
very actively using the exia so we

00:31:17,100 --> 00:31:23,419
didn't we couldn't actually pump the

00:31:19,139 --> 00:31:27,210
data faster than you know 10 gig but

00:31:23,419 --> 00:31:28,980
since we are since I actually have

00:31:27,210 --> 00:31:31,590
chance to get more except or I would

00:31:28,980 --> 00:31:35,419
like to push more realistic you know

00:31:31,590 --> 00:31:38,220
traffic model into the proxy and see

00:31:35,419 --> 00:31:40,440
what is the the numbers in terms of

00:31:38,220 --> 00:31:43,559
performance okay

00:31:40,440 --> 00:31:47,039
and I was really happy to actually see

00:31:43,559 --> 00:31:50,820
the multi PCI socket network device talk

00:31:47,039 --> 00:31:55,590
so if possible I would love to actually

00:31:50,820 --> 00:31:59,340
you know use that Nick to pump T of the

00:31:55,590 --> 00:32:02,389
traffic to both the de Numa note and get

00:31:59,340 --> 00:32:05,850
the best out of the you know discard

00:32:02,389 --> 00:32:08,309
okay so that is on another thing that I

00:32:05,850 --> 00:32:10,470
could do and then another thing is I

00:32:08,309 --> 00:32:14,429
would like to work on the you know the

00:32:10,470 --> 00:32:16,710
TC scaling as a tool for containerized

00:32:14,429 --> 00:32:20,639
service orchestration this is what I am

00:32:16,710 --> 00:32:23,580
you know explained as a given example of

00:32:20,639 --> 00:32:25,700
like you know bypass flow and those kind

00:32:23,580 --> 00:32:31,379
of things could be actually you know

00:32:25,700 --> 00:32:34,019
orchestrated using TC and also or maybe

00:32:31,379 --> 00:32:37,850
a little more it's hard to actually say

00:32:34,019 --> 00:32:43,289
right now but if it's more complicated

00:32:37,850 --> 00:32:46,019
routing forwarding for the terminated

00:32:43,289 --> 00:32:50,940
traffic then probably using TC might be

00:32:46,019 --> 00:32:54,809
the the better option if it's just the

00:32:50,940 --> 00:32:57,840
the bypass then probably xtp would be

00:32:54,809 --> 00:33:01,739
there right layer to you know enforce

00:32:57,840 --> 00:33:06,299
the de policy so those things are

00:33:01,739 --> 00:33:09,049
actually I left no as he a future work

00:33:06,299 --> 00:33:09,049
okay

00:33:11,490 --> 00:33:20,650
okay so I'd like to actually thank

00:33:15,010 --> 00:33:22,600
you.thank to basically my manager DJ to

00:33:20,650 --> 00:33:28,480
actually allow me to actually come to

00:33:22,600 --> 00:33:32,520
this wonderful conference and Mark who

00:33:28,480 --> 00:33:36,220
actually let me use you know the Exia

00:33:32,520 --> 00:33:39,570
and onon rekha who actually did you know

00:33:36,220 --> 00:33:43,350
run most of the tests for me okay all

00:33:39,570 --> 00:33:43,350
right thank you very much

00:33:43,630 --> 00:33:49,509
[Applause]

00:33:50,059 --> 00:33:56,250
I'm curious can you share anything about

00:33:52,799 --> 00:33:57,990
the results you had with the proxy

00:33:56,250 --> 00:34:02,540
itself in terms of improving the

00:33:57,990 --> 00:34:06,960
performance of TCP in your network - I

00:34:02,540 --> 00:34:22,109
sure I mean yeah we have data yeah I can

00:34:06,960 --> 00:34:24,090
actually share in maybe negative okay so

00:34:22,109 --> 00:34:26,520
you mentioned some performance issues

00:34:24,090 --> 00:34:30,210
with splice I think there are some bugs

00:34:26,520 --> 00:34:33,600
with splice right now we need to fix

00:34:30,210 --> 00:34:38,210
them and another issue with splice is

00:34:33,600 --> 00:34:44,310
that when you receive packets which are

00:34:38,210 --> 00:34:49,980
1500 bytes long and when we splice them

00:34:44,310 --> 00:34:54,389
to the outgoing TCP sockets and skb as

00:34:49,980 --> 00:34:56,669
only 17 fragment available by default

00:34:54,389 --> 00:35:01,619
I know share info meaning that outgoing

00:34:56,669 --> 00:35:03,600
is KB won't be 64 K long but much less

00:35:01,619 --> 00:35:08,190
than that so that might explain some

00:35:03,600 --> 00:35:11,220
performance issue so one thing you could

00:35:08,190 --> 00:35:15,270
try is to increase the max skb frag in

00:35:11,220 --> 00:35:18,630
the Linux include SK both dot H and

00:35:15,270 --> 00:35:22,200
increase that to like 45 and run your

00:35:18,630 --> 00:35:25,020
test again I guess you could get some

00:35:22,200 --> 00:35:28,190
performance improvement like that thanks

00:35:25,020 --> 00:35:28,190
for the comment I'll try that

00:35:30,000 --> 00:35:33,050
[Music]

00:35:38,080 --> 00:35:43,190
okay thank you very much

00:35:39,890 --> 00:35:43,190

YouTube URL: https://www.youtube.com/watch?v=w2hmr6ndj2k


