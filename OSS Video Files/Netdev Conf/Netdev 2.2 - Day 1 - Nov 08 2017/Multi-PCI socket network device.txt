Title: Multi-PCI socket network device
Publication date: 2018-03-15
Playlist: Netdev 2.2 - Day 1 - Nov 08 2017
Description: 
	Speaker: Achiad Shochat
Friday November 08th, 2017 
Seoul, Korea
https://www.netdevconf.org/2.2/session.html?shochat-devicemgmt-talk
Captions: 
	00:00:01,420 --> 00:00:09,940
devices I'll start with the problem

00:00:07,480 --> 00:00:14,139
description and motivation for having

00:00:09,940 --> 00:00:16,449
such Nicks and then I'll continue with

00:00:14,139 --> 00:00:19,740
suggesting a software model to represent

00:00:16,449 --> 00:00:23,910
such nicks in the kernel and finally

00:00:19,740 --> 00:00:27,460
show some very early proof of concept

00:00:23,910 --> 00:00:35,010
performance numbers okay so we'll start

00:00:27,460 --> 00:00:37,809
with the problem descriptions so so far

00:00:35,010 --> 00:00:40,089
probably all high-speed network

00:00:37,809 --> 00:00:43,089
interface cards that you know they have

00:00:40,089 --> 00:00:48,459
just a single PCIe bus connectivity to

00:00:43,089 --> 00:00:50,889
the server but lately network devices

00:00:48,459 --> 00:00:55,600
are starting starting to show up with

00:00:50,889 --> 00:00:59,289
multiple PCI bus connectivities as you

00:00:55,600 --> 00:01:02,649
can see here this is just a second this

00:00:59,289 --> 00:01:05,799
is a connectives for a NIC and as you

00:01:02,649 --> 00:01:08,350
can see it has a PCI extension here

00:01:05,799 --> 00:01:12,039
which gives it another PCI bus

00:01:08,350 --> 00:01:13,810
connectivity and the other one is that

00:01:12,039 --> 00:01:18,130
you cannot see it here but it's directly

00:01:13,810 --> 00:01:20,829
attached to the main NIC board so you

00:01:18,130 --> 00:01:25,990
have two PCI buses to connect to the

00:01:20,829 --> 00:01:27,969
server for a single NIC device so we'll

00:01:25,990 --> 00:01:32,469
start with the motivation so we have

00:01:27,969 --> 00:01:35,920
currently three use cases where such

00:01:32,469 --> 00:01:39,070
network devices are beneficial so the

00:01:35,920 --> 00:01:42,549
first case would be where the network

00:01:39,070 --> 00:01:48,429
port bandwidth is higher than the any

00:01:42,549 --> 00:01:52,590
that than the PCI bus connectivity of

00:01:48,429 --> 00:01:56,560
any PCI slot in the server for example

00:01:52,590 --> 00:01:59,499
in this slide we see a server here which

00:01:56,560 --> 00:02:04,270
has two PCIe sockets each one is the gen

00:01:59,499 --> 00:02:08,710
3 by 16 which is 100 gigabits per second

00:02:04,270 --> 00:02:12,909
speed and we see a NIC with a network

00:02:08,710 --> 00:02:14,920
port of 200 gigabits per second so

00:02:12,909 --> 00:02:17,640
obviously if the NIC has just a single

00:02:14,920 --> 00:02:20,160
PCI bus connectivity like onyx today

00:02:17,640 --> 00:02:24,850
then the server cannot enjoy the

00:02:20,160 --> 00:02:26,370
high-speed of the port and as you can

00:02:24,850 --> 00:02:30,850
see on the right if the NIC will have

00:02:26,370 --> 00:02:33,370
two PCI interfaces each one of 100

00:02:30,850 --> 00:02:35,650
gigabit per second then the server can

00:02:33,370 --> 00:02:41,920
get the full full wire speed of the NIC

00:02:35,650 --> 00:02:45,310
and this use case this example use case

00:02:41,920 --> 00:02:47,950
is going to be very real in the in the

00:02:45,310 --> 00:02:50,020
near future as network ports of two

00:02:47,950 --> 00:02:53,380
hundred gigabits per second are going to

00:02:50,020 --> 00:02:56,200
be introduced very very soon and most

00:02:53,380 --> 00:02:58,480
servers today do not have PCI sockets

00:02:56,200 --> 00:03:03,030
with that speed so this is going to be a

00:02:58,480 --> 00:03:03,030
real use case in the very near future

00:03:03,209 --> 00:03:10,920
so that's one use case well the other

00:03:07,810 --> 00:03:15,519
use case in the Neumann systems is a

00:03:10,920 --> 00:03:18,130
non-uniform memory access systems in

00:03:15,519 --> 00:03:22,720
these servers you have a multiple CPUs

00:03:18,130 --> 00:03:25,239
each one has its own course and each CPU

00:03:22,720 --> 00:03:29,650
in the system has its own local memory

00:03:25,239 --> 00:03:33,940
attached to it but each CPU can access

00:03:29,650 --> 00:03:38,200
also memory attached to other CPUs which

00:03:33,940 --> 00:03:41,829
is a remote for the DCP you and this

00:03:38,200 --> 00:03:43,450
connectivity is a server proprietary and

00:03:41,829 --> 00:03:45,700
they could the interconnect that

00:03:43,450 --> 00:03:49,180
connected the multiple memory instances

00:03:45,700 --> 00:03:56,260
of the system for example the Intel qpi

00:03:49,180 --> 00:03:59,410
bus well in these systems also each CPU

00:03:56,260 --> 00:04:03,040
has its own PCI socket the PCs sockets

00:03:59,410 --> 00:04:07,000
are not shared between the CPUs so if we

00:04:03,040 --> 00:04:10,180
have an equal just in a one PCI bus so

00:04:07,000 --> 00:04:13,660
we can it can we can connect it directly

00:04:10,180 --> 00:04:15,820
to just one CP of the system and then

00:04:13,660 --> 00:04:21,660
the NIC would be local only to one just

00:04:15,820 --> 00:04:26,110
CP of the system and this is the serious

00:04:21,660 --> 00:04:28,510
scalability issue because only only

00:04:26,110 --> 00:04:31,240
network network applications can

00:04:28,510 --> 00:04:35,920
and efficiently only on the cpu that is

00:04:31,240 --> 00:04:37,960
directly connected to the NIC for

00:04:35,920 --> 00:04:41,250
example if a network application would

00:04:37,960 --> 00:04:45,040
run on the CPU that is remote to the NIC

00:04:41,250 --> 00:04:47,550
then it's the DMA access of the NIC for

00:04:45,040 --> 00:04:51,550
this application will have to go through

00:04:47,550 --> 00:04:55,050
remote DMA access accessing remote

00:04:51,550 --> 00:04:57,670
memory from the next perspective

00:04:55,050 --> 00:05:01,900
so obviously applications running on

00:04:57,670 --> 00:05:07,540
such remote CPUs will suffer inferior

00:05:01,900 --> 00:05:08,860
performance and in addition to that in

00:05:07,540 --> 00:05:12,520
addition to the lower performance

00:05:08,860 --> 00:05:15,120
they'll get they also hurt other

00:05:12,520 --> 00:05:18,580
applications that use the qpi bus

00:05:15,120 --> 00:05:20,740
because for example if a software

00:05:18,580 --> 00:05:23,040
application application runs on CPU

00:05:20,740 --> 00:05:26,950
number one here and it wants to access

00:05:23,040 --> 00:05:29,980
it needs to access memory office that is

00:05:26,950 --> 00:05:32,740
directly attached to CPU zero then it

00:05:29,980 --> 00:05:35,770
has to go to the qpi bus and if this qpi

00:05:32,740 --> 00:05:38,820
bus is overloaded with DMA accesses of

00:05:35,770 --> 00:05:41,740
the neck then there is a contention and

00:05:38,820 --> 00:05:43,660
this application will suffer worse

00:05:41,740 --> 00:05:47,320
performance to not only the network

00:05:43,660 --> 00:05:52,900
application so this is a serious scaling

00:05:47,320 --> 00:05:56,050
problem and with having a NIC with multi

00:05:52,900 --> 00:06:02,050
PCI buses you can see how this issue can

00:05:56,050 --> 00:06:05,620
be resolved and the third use case I

00:06:02,050 --> 00:06:08,790
show here is what we call a multi host a

00:06:05,620 --> 00:06:12,400
NIC which is a single network port a

00:06:08,790 --> 00:06:15,580
single network port that serves multiple

00:06:12,400 --> 00:06:18,730
servers just different servers they have

00:06:15,580 --> 00:06:22,500
a single NIC that serves many servers in

00:06:18,730 --> 00:06:27,900
this way you can save lots of money for

00:06:22,500 --> 00:06:30,670
network ports in the intern inter system

00:06:27,900 --> 00:06:32,230
okay so these are the motivations and

00:06:30,670 --> 00:06:38,430
now I'm going to talk about how to

00:06:32,230 --> 00:06:38,430
represent it in the software model so

00:06:39,150 --> 00:06:42,960
the last use case of the multi host is

00:06:41,340 --> 00:06:45,090
not relevant for the software model

00:06:42,960 --> 00:06:48,780
since in this year's case its servers

00:06:45,090 --> 00:06:51,180
just sees a single PCI bus and the

00:06:48,780 --> 00:06:53,580
single PCI and the single network device

00:06:51,180 --> 00:06:55,979
so the software model is not applicable

00:06:53,580 --> 00:07:03,990
to this use case so we're gonna focus on

00:06:55,979 --> 00:07:06,210
the two other use cases okay so we

00:07:03,990 --> 00:07:09,990
believe that the bet the rightmost way

00:07:06,210 --> 00:07:13,560
to represent such devices in the kernel

00:07:09,990 --> 00:07:16,379
is to keep sticking to the net to the

00:07:13,560 --> 00:07:19,740
kernel convention of having a single net

00:07:16,379 --> 00:07:24,240
def representing a single network port

00:07:19,740 --> 00:07:28,069
as you can see here and this means that

00:07:24,240 --> 00:07:30,870
the you have on the PCI subsystem level

00:07:28,069 --> 00:07:35,279
you have two devices and from the

00:07:30,870 --> 00:07:40,319
networking stack perspective you see

00:07:35,279 --> 00:07:43,319
just a single network port and in this

00:07:40,319 --> 00:07:45,990
model all the aggregation of the

00:07:43,319 --> 00:07:53,159
aggregation of the PCI devices is done

00:07:45,990 --> 00:07:56,490
by the device driver and as you can see

00:07:53,159 --> 00:08:00,659
it also symmetric symmetrically modeling

00:07:56,490 --> 00:08:03,000
the physical layout of the device you

00:08:00,659 --> 00:08:05,520
have two PCI interfaces and two PCI

00:08:03,000 --> 00:08:09,599
devices in the kernel single network

00:08:05,520 --> 00:08:13,409
port and a single native and another

00:08:09,599 --> 00:08:15,569
benefit of this software model is that

00:08:13,409 --> 00:08:18,689
it works out of box you don't have to

00:08:15,569 --> 00:08:24,620
configure anything in the kernel and

00:08:18,689 --> 00:08:24,620
that's another advantage of this model

00:08:26,449 --> 00:08:34,949
okay so native creation what's

00:08:31,620 --> 00:08:37,949
problematic with these devices and the

00:08:34,949 --> 00:08:39,810
software model is that so far in the

00:08:37,949 --> 00:08:42,870
kernel and net Devdutt represents a

00:08:39,810 --> 00:08:48,060
physical poor device is being created

00:08:42,870 --> 00:08:50,450
upon its PCI probe and this this method

00:08:48,060 --> 00:08:54,050
doesn't work for such devices

00:08:50,450 --> 00:08:56,540
because you want a single native and you

00:08:54,050 --> 00:09:02,089
have multiple PCI devices will have

00:08:56,540 --> 00:09:07,540
multiple probes so it doesn't work for a

00:09:02,089 --> 00:09:10,220
multi PCI bus devices so you can have

00:09:07,540 --> 00:09:13,550
two different approaches to resolve this

00:09:10,220 --> 00:09:18,649
issue one of them would be the static

00:09:13,550 --> 00:09:20,660
approach is to create a native only when

00:09:18,649 --> 00:09:24,410
all the PCI buses of the device are

00:09:20,660 --> 00:09:27,230
probe the first probe of the of the PCI

00:09:24,410 --> 00:09:29,709
the first PCI bus of the device you do

00:09:27,230 --> 00:09:32,720
not create a native yet only when all

00:09:29,709 --> 00:09:35,660
the PCI busses of the net of the network

00:09:32,720 --> 00:09:38,120
device or probe only then you create the

00:09:35,660 --> 00:09:44,060
network so that's one approach to

00:09:38,120 --> 00:09:46,310
achieve it and the other approach is the

00:09:44,060 --> 00:09:48,980
dynamic approach would be to create a

00:09:46,310 --> 00:09:53,389
native upon the first the probe of the

00:09:48,980 --> 00:09:57,649
first PCI bus of the device and then

00:09:53,389 --> 00:10:01,880
weren't more PCI buses of the device are

00:09:57,649 --> 00:10:04,790
being probed dynamic dynamically enhance

00:10:01,880 --> 00:10:08,660
adjust the native and create more

00:10:04,790 --> 00:10:12,800
resources for the for its other PCI

00:10:08,660 --> 00:10:14,600
buses and obviously the dynamic approach

00:10:12,800 --> 00:10:23,930
is much more more complicated to

00:10:14,600 --> 00:10:26,149
implement but may work ok so in order to

00:10:23,930 --> 00:10:30,190
achieve all that you need some method to

00:10:26,149 --> 00:10:35,899
detect that the device is a multi mode

00:10:30,190 --> 00:10:40,370
devices multiple PCI buses and with this

00:10:35,899 --> 00:10:42,529
software this software model where the

00:10:40,370 --> 00:10:44,930
device driver is the one that implements

00:10:42,529 --> 00:10:48,949
all the PCI bus is aggregation it makes

00:10:44,930 --> 00:10:51,500
sense to make this detection method

00:10:48,949 --> 00:10:55,279
device specific and not make anything

00:10:51,500 --> 00:10:59,510
generic here and so no change in PCI bus

00:10:55,279 --> 00:11:03,399
in the PCI subsystem nor in than in the

00:10:59,510 --> 00:11:03,399
network stack only in device driver

00:11:06,690 --> 00:11:14,050
okay so one may I may suggest to have to

00:11:11,290 --> 00:11:17,950
resolve to represent such devices using

00:11:14,050 --> 00:11:21,700
a Linux bond or team device on top of -

00:11:17,950 --> 00:11:24,640
I mean create for each PCI bus its own

00:11:21,700 --> 00:11:28,870
network device and then instantiate and

00:11:24,640 --> 00:11:31,120
slave them into a Linux bond device well

00:11:28,870 --> 00:11:34,029
I believe this

00:11:31,120 --> 00:11:36,790
this software model is not as good as

00:11:34,029 --> 00:11:40,089
the previous one mainly because it

00:11:36,790 --> 00:11:44,980
breaks the convention of having a one

00:11:40,089 --> 00:11:46,680
native to represent we the net the

00:11:44,980 --> 00:11:52,480
convention of having a single native

00:11:46,680 --> 00:11:57,000
airport and the consequence of breaking

00:11:52,480 --> 00:12:01,480
this convention for example you get

00:11:57,000 --> 00:12:06,040
ambiguity in the port representation for

00:12:01,480 --> 00:12:09,010
instance if you want to see to show the

00:12:06,040 --> 00:12:10,660
the port speed so how would you show it

00:12:09,010 --> 00:12:15,270
if you have two natives that represent

00:12:10,660 --> 00:12:20,320
the same port to which interface it's

00:12:15,270 --> 00:12:24,330
it's a kind of an unresolvable issue in

00:12:20,320 --> 00:12:27,010
addition to that there is no

00:12:24,330 --> 00:12:30,010
straightforward to make the load

00:12:27,010 --> 00:12:33,220
balancing of incoming traffic of English

00:12:30,010 --> 00:12:36,550
traffic over the multiple PCIe buses if

00:12:33,220 --> 00:12:42,550
you have a linux bond you have no way to

00:12:36,550 --> 00:12:45,520
no straightforward way to control it in

00:12:42,550 --> 00:12:47,560
addition to that it it just shows you

00:12:45,520 --> 00:12:50,610
see you have a single network port and

00:12:47,560 --> 00:12:54,300
you see multiple natives on the system

00:12:50,610 --> 00:12:58,899
causing such cumbersome cumbersome view

00:12:54,300 --> 00:13:01,480
in the operating system and also it's

00:12:58,899 --> 00:13:04,720
not working out of box you need manual

00:13:01,480 --> 00:13:08,980
configuration to set up the bonding

00:13:04,720 --> 00:13:11,310
device on top of the net of the physical

00:13:08,980 --> 00:13:11,310
devices

00:13:15,870 --> 00:13:23,260
so one of the problem with having

00:13:19,090 --> 00:13:26,650
multiple PCI buses is ordering it's

00:13:23,260 --> 00:13:29,320
quite obvious for everyone here that

00:13:26,650 --> 00:13:32,770
each transmitted queue and each received

00:13:29,320 --> 00:13:35,950
queue of network device must skip

00:13:32,770 --> 00:13:41,260
ordering within you if you have for

00:13:35,950 --> 00:13:43,750
example a tech skill in you submit to X

00:13:41,260 --> 00:13:47,980
it calls for this text here you expect

00:13:43,750 --> 00:13:50,680
the two packets of these calls to be

00:13:47,980 --> 00:13:54,880
sent to the wire in order this is

00:13:50,680 --> 00:13:57,970
obvious but on the other end from the

00:13:54,880 --> 00:14:00,220
PCI spec space perspective there is no

00:13:57,970 --> 00:14:02,590
any ordering guarantee between two

00:14:00,220 --> 00:14:06,100
different PCI buses so you cannot have

00:14:02,590 --> 00:14:10,930
the the single txq transmit packets over

00:14:06,100 --> 00:14:13,300
to PCI buses so conclusion of that is

00:14:10,930 --> 00:14:16,330
that each txq and the same goes for the

00:14:13,300 --> 00:14:25,870
earth you must be affined with a single

00:14:16,330 --> 00:14:28,270
pci bus okay so and and usually a common

00:14:25,870 --> 00:14:31,660
practice to have in the network devices

00:14:28,270 --> 00:14:33,910
the pitch takes give and rxq to have a

00:14:31,660 --> 00:14:38,080
ticks can Eric super core they're

00:14:33,910 --> 00:14:42,670
usually a fine pair core so in the case

00:14:38,080 --> 00:14:45,220
of the Numa system it makes sense to

00:14:42,670 --> 00:14:47,920
find each ticks queue to the PCI bus

00:14:45,220 --> 00:14:52,600
that is local to the core which the

00:14:47,920 --> 00:14:57,310
queue is find with so it's a quite

00:14:52,600 --> 00:15:00,940
intuitive and in nonono systems the

00:14:57,310 --> 00:15:04,630
driver will have to just distribute the

00:15:00,940 --> 00:15:07,930
queues across the PCI buses in order to

00:15:04,630 --> 00:15:10,350
get a decent load balancing on the PCI

00:15:07,930 --> 00:15:10,350
bus

00:15:14,720 --> 00:15:23,040
okay and with such devices can comes the

00:15:19,380 --> 00:15:25,680
question of to which PCI bus I have a

00:15:23,040 --> 00:15:28,230
single network device and I want to open

00:15:25,680 --> 00:15:32,190
a resource for it so - which PCI bus

00:15:28,230 --> 00:15:37,200
will I run the command to the NIC I have

00:15:32,190 --> 00:15:41,130
multiple buses so I think this should be

00:15:37,200 --> 00:15:43,290
vendor specific decision I mean each

00:15:41,130 --> 00:15:48,030
driver can decide its driver and device

00:15:43,290 --> 00:15:50,610
I can decide how to handle it for

00:15:48,030 --> 00:15:53,120
example I show you three options they

00:15:50,610 --> 00:15:57,630
might be more I mean one option

00:15:53,120 --> 00:15:59,820
implementation option would be to assign

00:15:57,630 --> 00:16:01,500
one of the buses as the main bus for

00:15:59,820 --> 00:16:03,630
control operations to the nature and

00:16:01,500 --> 00:16:09,080
design and run all the commands to the

00:16:03,630 --> 00:16:13,770
Nick through this bus option two here is

00:16:09,080 --> 00:16:16,290
to have the resources that a five that

00:16:13,770 --> 00:16:19,410
are refined with a given PCI bus from as

00:16:16,290 --> 00:16:21,540
which as we saw that it transmitted

00:16:19,410 --> 00:16:24,090
queues and the receive queues that are

00:16:21,540 --> 00:16:28,020
refined with a bus then if the takes Q

00:16:24,090 --> 00:16:31,470
is assigned with that is affiliated with

00:16:28,020 --> 00:16:33,900
one bus then we run the creation command

00:16:31,470 --> 00:16:36,450
of this queue through the bus that it is

00:16:33,900 --> 00:16:39,810
a fine with and for resources that are

00:16:36,450 --> 00:16:42,720
that are not a fine with the PCI bus use

00:16:39,810 --> 00:16:45,990
just one of the PCI buses that may be

00:16:42,720 --> 00:16:49,020
another approach and another approach

00:16:45,990 --> 00:16:51,090
may be I don't know just issue the

00:16:49,020 --> 00:16:54,360
commands through any of the PCI bus ISM

00:16:51,090 --> 00:16:58,640
any command through any PCI bus all that

00:16:54,360 --> 00:17:03,710
is the vendor driver specific policy

00:16:58,640 --> 00:17:03,710
whatever works for the device is good

00:17:07,179 --> 00:17:17,240
okay now what does it take to run a RFS

00:17:14,240 --> 00:17:19,720
accelerated receive flow steering on

00:17:17,240 --> 00:17:23,869
such NICs

00:17:19,720 --> 00:17:27,889
so the NIC will have to support I mean

00:17:23,869 --> 00:17:30,470
with air first we insert flow steering

00:17:27,889 --> 00:17:32,629
rules to the device that direct the

00:17:30,470 --> 00:17:36,289
traffic hitting this rule to a specific

00:17:32,629 --> 00:17:38,990
receive Q so now we have received Q on

00:17:36,289 --> 00:17:41,600
multiple PCI buses and obviously the

00:17:38,990 --> 00:17:43,700
device will have to support flow

00:17:41,600 --> 00:17:46,220
steering rules that point to receive

00:17:43,700 --> 00:17:50,509
cues that are refined with different PCI

00:17:46,220 --> 00:17:55,549
buses so this is a hard work that such a

00:17:50,509 --> 00:17:58,669
device will have to support and the

00:17:55,549 --> 00:18:01,999
strong thing about having using a RFS in

00:17:58,669 --> 00:18:06,940
for example in the in the Newman system

00:18:01,999 --> 00:18:10,700
would be that it it can be used to

00:18:06,940 --> 00:18:15,590
completely eliminate the DMA traffic on

00:18:10,700 --> 00:18:24,549
the QPR in such system because if I go

00:18:15,590 --> 00:18:27,980
back to the picture as we can see here

00:18:24,549 --> 00:18:31,610
with a RFS each core will direct each

00:18:27,980 --> 00:18:33,950
traffic to the receive cues to its own

00:18:31,610 --> 00:18:36,249
receive cues and since the receive cues

00:18:33,950 --> 00:18:40,639
of this core are fine with this PCI bus

00:18:36,249 --> 00:18:43,340
then traffic each core will will receive

00:18:40,639 --> 00:18:45,769
its traffic with using error FS each

00:18:43,340 --> 00:18:48,440
core will receive its traffic directly

00:18:45,769 --> 00:18:53,149
from the PCI socket that is local to it

00:18:48,440 --> 00:18:56,059
and then we'll have zero qpi traffic for

00:18:53,149 --> 00:18:58,360
DMA for networking this is a very strong

00:18:56,059 --> 00:18:58,360
feature

00:19:03,730 --> 00:19:13,600
so this is about air first and RSS

00:19:08,300 --> 00:19:16,880
receive side scaling so with RSS we have

00:19:13,600 --> 00:19:20,030
we used to have a single indirection

00:19:16,880 --> 00:19:22,550
table in the device but now we have

00:19:20,030 --> 00:19:26,210
multiple we have received cues that are

00:19:22,550 --> 00:19:28,820
refined with multiple PCI buses so the

00:19:26,210 --> 00:19:32,450
NIC will have to support a single

00:19:28,820 --> 00:19:36,290
indirection table pointing to receive

00:19:32,450 --> 00:19:43,400
cues that reside on different PCI buses

00:19:36,290 --> 00:19:48,350
as we can see here another benefit of

00:19:43,400 --> 00:19:50,030
such a model is that the RSS indirection

00:19:48,350 --> 00:19:52,280
table can be used not only to load

00:19:50,030 --> 00:19:55,690
balance the traffic among the different

00:19:52,280 --> 00:19:58,600
cores of the system but also implicitly

00:19:55,690 --> 00:20:02,240
control the load balance over the

00:19:58,600 --> 00:20:08,590
multiple PCI buses using using the same

00:20:02,240 --> 00:20:11,410
mechanism of the indirection table and

00:20:08,590 --> 00:20:13,400
obviously in Numa system it also

00:20:11,410 --> 00:20:15,530
controls the load balance over the

00:20:13,400 --> 00:20:18,770
memory sees the different instances of

00:20:15,530 --> 00:20:22,670
the memory system so all done with a

00:20:18,770 --> 00:20:29,660
single mechanism that is already known

00:20:22,670 --> 00:20:38,110
to everybody okay what happens in

00:20:29,660 --> 00:20:42,290
virtualization in virtualization we

00:20:38,110 --> 00:20:44,150
would like to assign each VM I mean we

00:20:42,290 --> 00:20:46,370
want to give each VM access to the

00:20:44,150 --> 00:20:49,430
network port but the network Bob has

00:20:46,370 --> 00:20:55,070
been accessed through two different PCI

00:20:49,430 --> 00:20:58,400
devices so we will have to find a way to

00:20:55,070 --> 00:21:03,650
assign each VM with a virtual function

00:20:58,400 --> 00:21:07,340
of each PCI bus and then inside the VM

00:21:03,650 --> 00:21:09,470
the same driver knows how to aggregate

00:21:07,340 --> 00:21:10,140
the PCI devices into a single network

00:21:09,470 --> 00:21:12,300
device

00:21:10,140 --> 00:21:14,720
as we saw before the same will happen

00:21:12,300 --> 00:21:17,880
inside the VMS with the same driver

00:21:14,720 --> 00:21:21,240
because it will see it to PCI device the

00:21:17,880 --> 00:21:23,370
two PCI devices the driver will realize

00:21:21,240 --> 00:21:25,590
they represent a single network port and

00:21:23,370 --> 00:21:27,270
then it will instantiate a single

00:21:25,590 --> 00:21:30,060
Network device in the VM and will have

00:21:27,270 --> 00:21:34,740
the same in the VM the same view as we

00:21:30,060 --> 00:21:37,010
saw in the hypervisor but we have to

00:21:34,740 --> 00:21:40,260
figure out how to achieve that

00:21:37,010 --> 00:21:42,360
unfortunately I'm not yet so familiar

00:21:40,260 --> 00:21:45,900
with the virtualization software

00:21:42,360 --> 00:21:49,110
management I hope to get to it and see

00:21:45,900 --> 00:22:03,330
how to achieve it but it's something we

00:21:49,110 --> 00:22:06,330
need to resolve ok congestion it would

00:22:03,330 --> 00:22:09,510
be a very common use case to have the

00:22:06,330 --> 00:22:12,300
network board speed I mean this each

00:22:09,510 --> 00:22:16,950
piece that it is it's a it's a common

00:22:12,300 --> 00:22:18,720
use case that each PCI bus the speed of

00:22:16,950 --> 00:22:22,020
each PCI bus will be less than the

00:22:18,720 --> 00:22:23,730
network speed it could be it could be a

00:22:22,020 --> 00:22:27,330
valid use case especially for Numa

00:22:23,730 --> 00:22:30,330
systems to have these multiple PCI buses

00:22:27,330 --> 00:22:34,200
each one with the same speed or as the

00:22:30,330 --> 00:22:38,430
network port it would make sense to make

00:22:34,200 --> 00:22:41,730
the system more scalable but more common

00:22:38,430 --> 00:22:44,550
use case may be that the PCI bandwidth

00:22:41,730 --> 00:22:46,380
of each single PCI bus will be less than

00:22:44,550 --> 00:22:52,050
the bandwidth of the PIP of the network

00:22:46,380 --> 00:22:55,740
port and in these cases comes an issue

00:22:52,050 --> 00:22:57,360
with congestion because if we have for

00:22:55,740 --> 00:23:01,020
example here we have a network port of

00:22:57,360 --> 00:23:03,510
200 gigabit per second and 2 PCI buses

00:23:01,020 --> 00:23:07,080
each one 100 gigabit per second so it's

00:23:03,510 --> 00:23:08,850
half the speed and if we have traffic a

00:23:07,080 --> 00:23:11,610
burst of traffic that is this time

00:23:08,850 --> 00:23:12,270
defined with this PCI bus so very

00:23:11,610 --> 00:23:15,390
quickly

00:23:12,270 --> 00:23:20,040
we'll have congestion coming from this

00:23:15,390 --> 00:23:22,790
PCI bus and it will not do anything then

00:23:20,040 --> 00:23:25,880
the congestion may hurt

00:23:22,790 --> 00:23:33,560
going to other PCI buses this is a known

00:23:25,880 --> 00:23:36,740
situation in switches so in order to

00:23:33,560 --> 00:23:40,850
resolve this issue the device has to

00:23:36,740 --> 00:23:43,430
deploy some method to avoid it and for

00:23:40,850 --> 00:23:47,000
example the W read the weighted random

00:23:43,430 --> 00:23:49,640
early drop algorithm that is known

00:23:47,000 --> 00:23:54,410
algorithm in switching would be that if

00:23:49,640 --> 00:23:57,320
one of the PCI bus has starts to fill up

00:23:54,410 --> 00:24:01,670
the receive buffer here then we start

00:23:57,320 --> 00:24:03,350
dropping its its packet and this way the

00:24:01,670 --> 00:24:07,640
congestion does not affect does not

00:24:03,350 --> 00:24:10,190
propagate to other PCI buses so this is

00:24:07,640 --> 00:24:14,470
something that has to be resolved in the

00:24:10,190 --> 00:24:14,470
device level in the network device level

00:24:16,870 --> 00:24:28,130
and finally some performance number that

00:24:21,770 --> 00:24:31,850
we ran on a Numa system so this this is

00:24:28,130 --> 00:24:37,640
the system we used it was a Dell server

00:24:31,850 --> 00:24:40,610
running at 3.2 ghz four cores and then

00:24:37,640 --> 00:24:42,340
no hyper hyper threading we used the

00:24:40,610 --> 00:24:47,570
read at seven point three

00:24:42,340 --> 00:24:52,700
we used a Mellanox connected for neck we

00:24:47,570 --> 00:25:01,480
its port speed was 100 gb bit and it had

00:24:52,700 --> 00:25:09,350
two PCIe gen3 by eight a pci socket and

00:25:01,480 --> 00:25:11,090
in this test case we tested the

00:25:09,350 --> 00:25:16,120
bandwidth the throughput of ethernet

00:25:11,090 --> 00:25:19,390
traffic and in order to demonstrate the

00:25:16,120 --> 00:25:25,240
value of the multi pci bus

00:25:19,390 --> 00:25:28,850
we also loaded the inter connected qpi

00:25:25,240 --> 00:25:31,820
with software traffic I mean not a

00:25:28,850 --> 00:25:34,130
network traffic just ran application on

00:25:31,820 --> 00:25:35,779
the two sockets that access remote

00:25:34,130 --> 00:25:41,200
memory of each other

00:25:35,779 --> 00:25:46,489
and then we ran the network application

00:25:41,200 --> 00:25:48,379
and as you can see here when we ran the

00:25:46,489 --> 00:25:52,330
application on the remote CPU of the

00:25:48,379 --> 00:25:56,779
Numa system we got a bandwidth of about

00:25:52,330 --> 00:25:58,969
80 gigabits per second and when we ran

00:25:56,779 --> 00:26:02,210
it on the local CPU this is the blue

00:25:58,969 --> 00:26:06,469
line we got quite a line rate which is

00:26:02,210 --> 00:26:09,889
almost 100 gigabit and when we use the

00:26:06,469 --> 00:26:12,969
socket direct running using the two PCI

00:26:09,889 --> 00:26:17,089
buses we got close to the optimal

00:26:12,969 --> 00:26:19,460
bandwidth and probably the deviations

00:26:17,089 --> 00:26:23,379
here are due to the congestion problem

00:26:19,460 --> 00:26:29,210
that I just mentioned bursts of traffic

00:26:23,379 --> 00:26:33,169
reaching one of the PCI buses so this is

00:26:29,210 --> 00:26:38,509
one test case I ran and another in

00:26:33,169 --> 00:26:40,369
another taste test case we ran a 150 TCP

00:26:38,509 --> 00:26:45,769
streams and measure the average latency

00:26:40,369 --> 00:26:48,679
of each of which such TCP connection and

00:26:45,769 --> 00:26:54,200
here you can see even more impressive

00:26:48,679 --> 00:26:57,649
results so running on the remote CP of

00:26:54,200 --> 00:27:01,700
the system you see very high latency for

00:26:57,649 --> 00:27:05,869
the for each TCP stream and running on

00:27:01,700 --> 00:27:08,809
the local CPU obviously you see much

00:27:05,869 --> 00:27:11,269
better latency much lower and with the

00:27:08,809 --> 00:27:14,899
socket direct which means running with

00:27:11,269 --> 00:27:18,559
two PCI buses connected you can see that

00:27:14,899 --> 00:27:21,830
we got even lower latency and this is

00:27:18,559 --> 00:27:24,830
probably better than the local test

00:27:21,830 --> 00:27:29,539
running only on the local CPU as it

00:27:24,830 --> 00:27:32,149
utilizes the caches of both CPUs this is

00:27:29,539 --> 00:27:37,549
probably why the performance is even

00:27:32,149 --> 00:27:40,629
better than running on the local CPU and

00:27:37,549 --> 00:27:40,629
that's it

00:27:40,799 --> 00:27:52,360
so Jamal is not here to pass the

00:27:43,660 --> 00:27:54,720
microphone if anyone asked questions you

00:27:52,360 --> 00:27:54,720
won't ask

00:28:00,920 --> 00:28:08,690
oh this is actually real good work you

00:28:05,960 --> 00:28:10,130
know thanks for leading the way because

00:28:08,690 --> 00:28:14,350
we have been thinking of exactly the

00:28:10,130 --> 00:28:14,350
same thing in same solution thank you

00:28:14,410 --> 00:28:18,830
one of the things that I was looking at

00:28:17,180 --> 00:28:20,810
you have like three options that you

00:28:18,830 --> 00:28:24,110
talked about when at probe time you

00:28:20,810 --> 00:28:30,530
could statically or dynamically you know

00:28:24,110 --> 00:28:37,130
configure yeah dynamic and Static

00:28:30,530 --> 00:28:40,490
approach my suggestion would be to go

00:28:37,130 --> 00:28:42,830
with the dynamic one the reason is you

00:28:40,490 --> 00:28:44,840
already have built into your native

00:28:42,830 --> 00:28:47,360
model where you can say it's a real

00:28:44,840 --> 00:28:49,730
number of Q's where you come up with

00:28:47,360 --> 00:28:52,520
maybe a large number of Q's and there

00:28:49,730 --> 00:28:54,470
are only some that are active for the

00:28:52,520 --> 00:28:56,930
given PCI bus and then you go ahead and

00:28:54,470 --> 00:28:59,690
make the rest of them once active as the

00:28:56,930 --> 00:29:01,790
other PCI bus has come up so you could

00:28:59,690 --> 00:29:04,160
dynamically add those I don't think it's

00:29:01,790 --> 00:29:08,690
complicated I think it's actually pretty

00:29:04,160 --> 00:29:10,250
easy to get you know make it work

00:29:08,690 --> 00:29:13,040
the second thing it'll scales really

00:29:10,250 --> 00:29:15,500
well for us are a big case because in a

00:29:13,040 --> 00:29:17,860
serving case you're assuming that you

00:29:15,500 --> 00:29:21,050
know your administrators are actually

00:29:17,860 --> 00:29:24,380
linking both your PCIe interfaces into

00:29:21,050 --> 00:29:26,810
the VM which he or she may not do it

00:29:24,380 --> 00:29:28,400
right they might have only one piece a

00:29:26,810 --> 00:29:29,540
interface go in there right so you

00:29:28,400 --> 00:29:31,610
cannot make that static approach

00:29:29,540 --> 00:29:35,240
assumption that both interfaces are

00:29:31,610 --> 00:29:38,840
there in ovm yeah okay generally I agree

00:29:35,240 --> 00:29:41,150
the dynamic approach is more optimal but

00:29:38,840 --> 00:29:44,120
about the complexity I'm not sure it's

00:29:41,150 --> 00:29:47,600
simple because the issue you mentioned

00:29:44,120 --> 00:29:51,380
those scaling the native dynamically

00:29:47,600 --> 00:29:54,980
it's one issue another issue may be for

00:29:51,380 --> 00:29:56,840
example if you know it's kind of vendor

00:29:54,980 --> 00:30:01,880
specific depends on the vendor specific

00:29:56,840 --> 00:30:04,910
policy for example if you have in the

00:30:01,880 --> 00:30:07,970
creation in the management of the device

00:30:04,910 --> 00:30:10,730
if you create some of the resources

00:30:07,970 --> 00:30:12,050
through one of the buses and others to

00:30:10,730 --> 00:30:15,440
the

00:30:12,050 --> 00:30:19,790
so think of the case when someone who

00:30:15,440 --> 00:30:21,910
touches one PCI bus and then attaches

00:30:19,790 --> 00:30:25,580
another one and he touches the first one

00:30:21,910 --> 00:30:27,740
so you need to if you created resources

00:30:25,580 --> 00:30:31,970
through one of the PCI buses and the

00:30:27,740 --> 00:30:33,950
device does not no no them does not

00:30:31,970 --> 00:30:41,900
recognize them when access to the other

00:30:33,950 --> 00:30:44,360
PCI bus then you have a problem you can

00:30:41,900 --> 00:30:46,970
always deactivate those queues right so

00:30:44,360 --> 00:30:48,770
you always have that option of not

00:30:46,970 --> 00:30:51,080
really keeping all those queues active

00:30:48,770 --> 00:30:52,880
yeah but besides the queue there are

00:30:51,080 --> 00:30:55,100
there are other resources for example

00:30:52,880 --> 00:30:58,190
the indirection table so if it is

00:30:55,100 --> 00:31:00,350
resides just on one of the PCI buses

00:30:58,190 --> 00:31:02,570
then you have a program but I agree with

00:31:00,350 --> 00:31:06,980
you is the optimal solution but let's

00:31:02,570 --> 00:31:12,140
reveal to implement yeah so I would like

00:31:06,980 --> 00:31:13,880
to perhaps provide some well I like the

00:31:12,140 --> 00:31:15,970
other solution better than solution

00:31:13,880 --> 00:31:18,290
where you have net before every PF

00:31:15,970 --> 00:31:20,090
because for if you go this way then you

00:31:18,290 --> 00:31:21,920
need an API basically to configure

00:31:20,090 --> 00:31:23,870
everything like it's a switch

00:31:21,920 --> 00:31:25,430
effectively so if you model it as a

00:31:23,870 --> 00:31:30,770
switch as you gave the example of

00:31:25,430 --> 00:31:33,680
read/write you just like put a red in DC

00:31:30,770 --> 00:31:36,560
on the net def that's like underneath

00:31:33,680 --> 00:31:39,260
the bridge and that gives you red and if

00:31:36,560 --> 00:31:41,240
you go and pretend to the user that

00:31:39,260 --> 00:31:43,520
there's only one net def then you have

00:31:41,240 --> 00:31:45,650
to like teach everything like tell the

00:31:43,520 --> 00:31:48,170
user which queue actually lives on with

00:31:45,650 --> 00:31:50,270
PCI and like you know there's a lot of

00:31:48,170 --> 00:31:53,330
API is that you have to tell the like

00:31:50,270 --> 00:31:54,890
for very basic users one def is nice

00:31:53,330 --> 00:31:57,800
because they understand what or how it

00:31:54,890 --> 00:31:59,870
works but if you go into power users who

00:31:57,800 --> 00:32:01,700
really want to optimize things they will

00:31:59,870 --> 00:32:03,650
ask you like questions like how to

00:32:01,700 --> 00:32:05,030
actually like assign things properly and

00:32:03,650 --> 00:32:09,620
then you need to add api's for

00:32:05,030 --> 00:32:11,980
everything basically forever yeah right

00:32:09,620 --> 00:32:14,000
totally agree it's actually hiding

00:32:11,980 --> 00:32:17,540
actually there is a switch on the device

00:32:14,000 --> 00:32:20,090
but it's a switch not between physical

00:32:17,540 --> 00:32:23,360
network ports you have one network port

00:32:20,090 --> 00:32:25,060
and the other ports are actually PCI

00:32:23,360 --> 00:32:32,110
buses right

00:32:25,060 --> 00:32:35,500
so yes and serve you know I assume that

00:32:32,110 --> 00:32:39,190
there will be people who will prefer not

00:32:35,500 --> 00:32:42,370
to mess with a multi with a switch

00:32:39,190 --> 00:32:45,100
management for such devices so for such

00:32:42,370 --> 00:32:45,580
people this is this solution would be

00:32:45,100 --> 00:32:49,320
better

00:32:45,580 --> 00:32:54,430
and if sophisticated as you say powerful

00:32:49,320 --> 00:32:56,680
users will want real a manageability of

00:32:54,430 --> 00:32:58,900
the internals which are not high instead

00:32:56,680 --> 00:33:02,550
of hiding it then for them will have to

00:32:58,900 --> 00:33:06,310
have difference of the modal probably

00:33:02,550 --> 00:33:08,950
some explicit switch and expose an

00:33:06,310 --> 00:33:10,770
explicit switch to the system with three

00:33:08,950 --> 00:33:13,770
negatives and the switch between them

00:33:10,770 --> 00:33:16,390
but that doesn't fit it may fit some

00:33:13,770 --> 00:33:18,670
sophisticated customers but I believe

00:33:16,390 --> 00:33:21,430
many customers will prefer not to care

00:33:18,670 --> 00:33:26,740
about it and to have a simpler view in

00:33:21,430 --> 00:33:31,870
the system but both both models are

00:33:26,740 --> 00:33:33,820
valid yeah thank you I agree with you

00:33:31,870 --> 00:33:35,680
both vows are right but I think that you

00:33:33,820 --> 00:33:38,500
may have run into a situation where at

00:33:35,680 --> 00:33:40,690
the right answer on the host side where

00:33:38,500 --> 00:33:43,180
you've got a PF is that you want to

00:33:40,690 --> 00:33:46,930
expose everything that you want to let

00:33:43,180 --> 00:33:50,800
them manage it but in your example where

00:33:46,930 --> 00:33:53,200
you had up into the guest in that

00:33:50,800 --> 00:33:56,290
situation you don't because you really

00:33:53,200 --> 00:33:59,590
need to be able to take off-the-shelf

00:33:56,290 --> 00:34:02,440
distros and run them and you don't want

00:33:59,590 --> 00:34:04,480
to have to be doing configuration in you

00:34:02,440 --> 00:34:06,700
know god-awful you know let's say maybe

00:34:04,480 --> 00:34:10,090
it's rl6 and you know I already gone

00:34:06,700 --> 00:34:11,470
down that rat hole with Azure and you're

00:34:10,090 --> 00:34:15,190
gonna run in there that with every other

00:34:11,470 --> 00:34:17,050
but everybody else as well so the right

00:34:15,190 --> 00:34:19,480
answer maybe you want a single met dev

00:34:17,050 --> 00:34:22,840
when it shows up and a guest but you

00:34:19,480 --> 00:34:24,730
want to let you know the hosts do

00:34:22,840 --> 00:34:26,860
whatever configuration because that's

00:34:24,730 --> 00:34:29,169
that's not a you know off the shelf

00:34:26,860 --> 00:34:32,700
store situation completely agree with

00:34:29,169 --> 00:34:32,700
you that's exactly what I'm saying

00:34:33,169 --> 00:34:41,379
any other questions nope thank you for

00:34:37,940 --> 00:34:41,379

YouTube URL: https://www.youtube.com/watch?v=nYGl1o4Vn_U


