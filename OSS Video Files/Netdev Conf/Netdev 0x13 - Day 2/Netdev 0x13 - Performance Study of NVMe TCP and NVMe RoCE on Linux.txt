Title: Netdev 0x13 - Performance Study of NVMe TCP and NVMe RoCE on Linux
Publication date: 2019-05-25
Playlist: Netdev 0x13 - Day 2
Description: 
	With the addition of NVMe over Fabrics (NVMe-oF) on Linux (using TCP),
there is that elephant in the room question that needs to be answered.
Is there a performance impact when using TCP instead of an RDMA transport
such as RoCE?

Ariel Cohen believes has some answers. In his talk he describes a performance
study comparing NVMe/TCP to NVMe/RoCE on Linux. Throughput, latency
distribution, and CPU load are measured and compared. A performance
comparison of these kernel protocols to user-level protocols using SPDK
are also described.

https://netdevconf.org/0x13/session.html?talk-nvme-tcp-roce
Captions: 
	00:00:00,060 --> 00:00:17,279
I am Ariel Cohen I'm from Oracle a few

00:00:12,920 --> 00:00:20,220
months ago when the nvme TCP patches

00:00:17,279 --> 00:00:26,789
were submitted I started doing a

00:00:20,220 --> 00:00:30,480
performance study to compare nvme TCP to

00:00:26,789 --> 00:00:34,710
nvme raki and the motivation was made

00:00:30,480 --> 00:00:37,469
mostly in terms of using this for use

00:00:34,710 --> 00:00:39,379
cases at Oracle I haven't been

00:00:37,469 --> 00:00:41,940
personally involved in the

00:00:39,379 --> 00:00:45,300
standardization or the implementation of

00:00:41,940 --> 00:00:48,570
this so I take no credit for it and

00:00:45,300 --> 00:00:55,559
accept no blame for it I was focused on

00:00:48,570 --> 00:00:57,420
studying the the performance and I don't

00:00:55,559 --> 00:00:59,670
need to go through the introduction to

00:00:57,420 --> 00:01:06,119
this where he gave us a very good

00:00:59,670 --> 00:01:08,549
introduction on on on this topic so one

00:01:06,119 --> 00:01:10,549
thing I would say is that rocky you know

00:01:08,549 --> 00:01:13,500
it's already May over converged Ethernet

00:01:10,549 --> 00:01:16,590
which gives you a hardware transport

00:01:13,500 --> 00:01:20,009
zero copy memory transfer so clearly it

00:01:16,590 --> 00:01:22,140
has hardware that provides better

00:01:20,009 --> 00:01:23,840
performance than doing something like

00:01:22,140 --> 00:01:28,619
software TCP

00:01:23,840 --> 00:01:32,909
so what comparisons were run so

00:01:28,619 --> 00:01:35,420
basically I was looking at the kernel

00:01:32,909 --> 00:01:37,350
implementation and the SP DK

00:01:35,420 --> 00:01:41,220
implementation they're both available

00:01:37,350 --> 00:01:44,939
and then in each one comparing TCP to

00:01:41,220 --> 00:01:46,200
rocky and the SPD am sure most of you

00:01:44,939 --> 00:01:48,380
are familiar with ada storage

00:01:46,200 --> 00:01:50,850
performance development kit which is

00:01:48,380 --> 00:01:52,500
which provides tools and libraries for

00:01:50,850 --> 00:01:55,409
writing high-performance user-mode

00:01:52,500 --> 00:01:57,310
storage applications and it has this pod

00:01:55,409 --> 00:02:00,220
mode operation

00:01:57,310 --> 00:02:03,340
so I was interested in SP DK vs. Colonel

00:02:00,220 --> 00:02:05,890
and also in the performance tools you

00:02:03,340 --> 00:02:08,470
know there's FIO with the famous FIO and

00:02:05,890 --> 00:02:12,490
then there's perf that's part of this

00:02:08,470 --> 00:02:14,890
PDK which has lower overhead so I wanted

00:02:12,490 --> 00:02:17,200
to compare those and also see the impact

00:02:14,890 --> 00:02:19,330
of the MTU so this was a pretty

00:02:17,200 --> 00:02:22,840
extensive study and the results are in

00:02:19,330 --> 00:02:25,440
the paper that I posted there there's a

00:02:22,840 --> 00:02:28,090
lot there I can't go over all of it but

00:02:25,440 --> 00:02:31,750
go at least over some of it and then the

00:02:28,090 --> 00:02:33,670
rest you can see in the paper so this is

00:02:31,750 --> 00:02:35,470
a basic test configuration with two

00:02:33,670 --> 00:02:39,450
servers back-to-back over 100 gig

00:02:35,470 --> 00:02:43,239
Ethernet one one client one target

00:02:39,450 --> 00:02:46,690
fairly low end servers each one dual CPU

00:02:43,239 --> 00:02:49,450
with 8 cores per CPU 2.1 gigahertz 32

00:02:46,690 --> 00:02:53,500
gigabytes of memory Nick his Mellanox

00:02:49,450 --> 00:02:56,560
Connect x5 which is capable of rocky and

00:02:53,500 --> 00:02:59,500
hardware here are the software versions

00:02:56,560 --> 00:03:03,610
is 419 with the nvme patches and via me

00:02:59,500 --> 00:03:06,070
tcp patches and SPD KNF i/o versions I

00:03:03,610 --> 00:03:10,660
wanted to study the performance of the

00:03:06,070 --> 00:03:14,290
protocols in isolation from any nvme

00:03:10,660 --> 00:03:18,760
device performance aspect so it's used

00:03:14,290 --> 00:03:21,640
in all block storage device in in this

00:03:18,760 --> 00:03:24,370
in these tests as the target and the

00:03:21,640 --> 00:03:30,540
workload was a random mix reads and

00:03:24,370 --> 00:03:35,109
writes for the FIO and perf workload

00:03:30,540 --> 00:03:37,090
so let's start with it impact of MTU

00:03:35,109 --> 00:03:40,030
which is a kind of one of the first

00:03:37,090 --> 00:03:43,090
things I ran for some reason so this is

00:03:40,030 --> 00:03:45,129
with nvme TCP the numbers you see in

00:03:43,090 --> 00:03:47,470
this chart they're all in thousands of I

00:03:45,129 --> 00:03:48,939
ops so when it says you know thousand

00:03:47,470 --> 00:03:53,079
thousand eighty four you know it's a

00:03:48,939 --> 00:03:55,629
million I ops over a over a million I up

00:03:53,079 --> 00:03:58,780
so this is this ran with 32 client jobs

00:03:55,629 --> 00:04:00,939
io depth of eight and you can see the

00:03:58,780 --> 00:04:05,799
impact you know fifteen hundred nine

00:04:00,939 --> 00:04:08,769
thousand MTU and it's significant within

00:04:05,799 --> 00:04:11,439
vme TCP about twenty eight to fifty

00:04:08,769 --> 00:04:13,569
percent increase as you go with the

00:04:11,439 --> 00:04:16,570
larger MTU obviously for the larger io

00:04:13,569 --> 00:04:18,430
sizes small io size like 512 it's not

00:04:16,570 --> 00:04:22,419
going to make a difference

00:04:18,430 --> 00:04:23,979
also lower latency with SPD care and the

00:04:22,419 --> 00:04:26,110
same thing the difference is less

00:04:23,979 --> 00:04:28,990
significant but it's still significant

00:04:26,110 --> 00:04:31,000
with rocky you won't see this because

00:04:28,990 --> 00:04:33,639
it's you know obviously that's done in

00:04:31,000 --> 00:04:36,840
hardware so it is not an issue we're

00:04:33,639 --> 00:04:36,840
dealing with the smaller packets

00:04:37,110 --> 00:04:44,460
so what what what workloads did I test

00:04:40,740 --> 00:04:48,150
so I did I did the workloads with 32

00:04:44,460 --> 00:04:54,030
jobs which basically used all the cores

00:04:48,150 --> 00:04:57,419
on the on decline and I did varying i/o

00:04:54,030 --> 00:04:58,830
sizes so with i/o depth depth 8 I'm not

00:04:57,419 --> 00:05:00,539
going to show the results of that

00:04:58,830 --> 00:05:01,830
workload here we don't have the time

00:05:00,539 --> 00:05:04,169
it's in the paper

00:05:01,830 --> 00:05:07,770
but I will go over quickly the the

00:05:04,169 --> 00:05:11,250
single job case with different i/o depth

00:05:07,770 --> 00:05:13,590
and the 32 job case with different oil

00:05:11,250 --> 00:05:19,139
depth with the size of 4 kilobyte size

00:05:13,590 --> 00:05:27,000
io size so let's start with the kernel

00:05:19,139 --> 00:05:29,759
and tcp vs rocky with one job so here we

00:05:27,000 --> 00:05:33,620
see basically the eye ops on the again

00:05:29,759 --> 00:05:36,629
it's in thousands you can see that rocky

00:05:33,620 --> 00:05:38,009
so you get good performance ok

00:05:36,629 --> 00:05:42,270
throughout this whole thing I need to

00:05:38,009 --> 00:05:44,250
say that nvme TCP provides fairly good

00:05:42,270 --> 00:05:47,789
performance and in a lot of these graphs

00:05:44,250 --> 00:05:49,469
you will see rocky is much higher but it

00:05:47,789 --> 00:05:51,360
that shouldn't distract you from the

00:05:49,469 --> 00:05:54,440
fact that actually envy TCP performs

00:05:51,360 --> 00:05:56,729
well it's naturally what he has

00:05:54,440 --> 00:05:58,589
capabilities in terms of the hardware

00:05:56,729 --> 00:06:01,199
implementation and the zero copy and so

00:05:58,589 --> 00:06:04,259
on that are not there with the soft with

00:06:01,199 --> 00:06:06,779
TCP so in general with this study you

00:06:04,259 --> 00:06:09,719
know the kind of the the how the story

00:06:06,779 --> 00:06:11,729
ends is known you know the Rockies going

00:06:09,719 --> 00:06:15,029
to perform better as PD casement it

00:06:11,729 --> 00:06:17,069
perform better than the kernel and FIO

00:06:15,029 --> 00:06:19,620
and this perf performs better than FIO

00:06:17,069 --> 00:06:21,509
so that we know what the results would

00:06:19,620 --> 00:06:22,800
be and that's those were my expectations

00:06:21,509 --> 00:06:24,900
and indeed there were no there weren't

00:06:22,800 --> 00:06:27,270
really any major surprises but the

00:06:24,900 --> 00:06:29,550
interest is in kind of quantifying it I

00:06:27,270 --> 00:06:32,699
wanted to see what is the difference so

00:06:29,550 --> 00:06:36,790
here we see in this jobs one case by two

00:06:32,699 --> 00:06:40,540
two to four times higher high ops with

00:06:36,790 --> 00:06:46,140
with rocky and similarly with the

00:06:40,540 --> 00:06:48,940
latency the impact is similar the local

00:06:46,140 --> 00:06:51,910
noteblock device is 1.3 microseconds so

00:06:48,940 --> 00:06:56,290
it's very negligible in yeah the latency

00:06:51,910 --> 00:07:02,650
picture here okay so let's look at 32

00:06:56,290 --> 00:07:07,900
jobs we basically get between 28% higher

00:07:02,650 --> 00:07:13,510
to 3 times the the I ops on on rocky

00:07:07,900 --> 00:07:14,980
compared to to TCP and we're getting you

00:07:13,510 --> 00:07:16,510
know I mean if you look at the local

00:07:14,980 --> 00:07:18,760
node block device it's seven and a half

00:07:16,510 --> 00:07:21,070
million IAP so you can compare to what

00:07:18,760 --> 00:07:23,970
we're seeing here what we're getting up

00:07:21,070 --> 00:07:26,710
to a million on this on this set up with

00:07:23,970 --> 00:07:31,980
nvme TCP and we're able to get to the

00:07:26,710 --> 00:07:31,980
three million on Rocky

00:07:32,850 --> 00:07:42,210
okay similar on latency of TCP versus

00:07:38,370 --> 00:07:44,640
Rocky CPU load in general you will see

00:07:42,210 --> 00:07:46,530
that the client CPU load in this kind of

00:07:44,640 --> 00:07:48,360
test is higher than the target because

00:07:46,530 --> 00:07:50,010
it's also you know it's one client one

00:07:48,360 --> 00:07:55,350
target and the client is running the

00:07:50,010 --> 00:07:59,700
actual i/o load test utility you will

00:07:55,350 --> 00:08:02,700
see you know here that the again

00:07:59,700 --> 00:08:05,370
significantly lower CPU load to

00:08:02,700 --> 00:08:09,390
throughput ratio for Rocky again as

00:08:05,370 --> 00:08:11,430
expected but you can see both on the

00:08:09,390 --> 00:08:14,280
client and other target what the what

00:08:11,430 --> 00:08:16,320
the CPU is like for rocky versus TCP I

00:08:14,280 --> 00:08:18,270
won't go over this in detail but you can

00:08:16,320 --> 00:08:22,010
see it in the paper market look at it

00:08:18,270 --> 00:08:22,010
more carefully yep

00:08:32,090 --> 00:08:39,590
just when you say job you mean execution

00:08:38,360 --> 00:08:41,780
unit like a threader

00:08:39,590 --> 00:08:45,680
yeah I mean so this is basically job in

00:08:41,780 --> 00:08:49,670
the car like F basically FIO job so it's

00:08:45,680 --> 00:08:51,380
another and what was how many's like

00:08:49,670 --> 00:08:53,900
every time you use a certain amount of

00:08:51,380 --> 00:08:57,830
jobs you have as many as CPUs as these

00:08:53,900 --> 00:09:01,430
jobs or you I I had so I on the server I

00:08:57,830 --> 00:09:03,770
am on the so I had 16 CPUs there and I

00:09:01,430 --> 00:09:07,670
ran 32 jobs hyper-threading was enabled

00:09:03,770 --> 00:09:10,340
that was the so it's actually eight CPUs

00:09:07,670 --> 00:09:14,260
with hyper-threading it's 16 CPUs

00:09:10,340 --> 00:09:17,990
physical CPUs okay and what was your

00:09:14,260 --> 00:09:22,040
affinity strategic strategy is regarding

00:09:17,990 --> 00:09:23,930
to was it diagonal interrupts do you

00:09:22,040 --> 00:09:27,080
urinate did you run it on both Numa

00:09:23,930 --> 00:09:29,120
notes you know you can yeah I did not

00:09:27,080 --> 00:09:33,110
make it so basically I did not try to

00:09:29,120 --> 00:09:36,140
set up any kind of stitch any particular

00:09:33,110 --> 00:09:38,120
affinity there I just ran it so it

00:09:36,140 --> 00:09:40,820
whatever the driver is doing right for

00:09:38,120 --> 00:09:43,610
the case of tcp yep and also in europe

00:09:40,820 --> 00:09:45,170
you're probably running it's probably

00:09:43,610 --> 00:09:47,750
it's a modern node node that has two

00:09:45,170 --> 00:09:50,930
minutes to nuuma sockets right so

00:09:47,750 --> 00:09:52,940
correct okay mate cuz many times in

00:09:50,930 --> 00:09:55,400
network micro benchmarking people are

00:09:52,940 --> 00:09:57,530
using only the Numa node which is closer

00:09:55,400 --> 00:10:01,010
to the card so you use it like out of

00:09:57,530 --> 00:10:03,080
the box why I did write for the 32 I

00:10:01,010 --> 00:10:06,170
used it out of the box for the 32 jobs

00:10:03,080 --> 00:10:09,520
for the single job case I did set the

00:10:06,170 --> 00:10:11,960
affinity to the closer Neumann world yep

00:10:09,520 --> 00:10:13,679
okay

00:10:11,960 --> 00:10:18,929
[Music]

00:10:13,679 --> 00:10:21,939
okay okay okay let's continue so SP DK

00:10:18,929 --> 00:10:24,220
we transition from Colonel to SP D K now

00:10:21,939 --> 00:10:27,009
this is kind of an anomaly K so here

00:10:24,220 --> 00:10:29,559
that I don't fully know the reasons for

00:10:27,009 --> 00:10:31,359
it so as you can see here in the judge

00:10:29,559 --> 00:10:34,749
single job case there is a big gap

00:10:31,359 --> 00:10:37,899
between between d CP and rocky and

00:10:34,749 --> 00:10:39,999
basically TCP I didn't get on this set

00:10:37,899 --> 00:10:42,879
on the case of single job I didn't get

00:10:39,999 --> 00:10:46,629
much benefit on the higher IO tips from

00:10:42,879 --> 00:10:48,489
SP DK I'm not sure why but that's but it

00:10:46,629 --> 00:10:51,669
rocky I did get a big benefit so that's

00:10:48,489 --> 00:10:55,509
why you're seeing this big gap here in

00:10:51,669 --> 00:10:59,169
the 32 job case it looks more like what

00:10:55,509 --> 00:11:02,019
I expected it to be and in fact when I

00:10:59,169 --> 00:11:05,019
use perf as opposed to FIO also this

00:11:02,019 --> 00:11:07,239
anomaly wasn't quite there so that's

00:11:05,019 --> 00:11:11,669
something to look into why the gap is as

00:11:07,239 --> 00:11:15,009
wide as it is here similar with latency

00:11:11,669 --> 00:11:21,129
and let's look at the 32 job case again

00:11:15,009 --> 00:11:24,689
you know rocky versus pcp with SB DK you

00:11:21,129 --> 00:11:27,489
get two to seven times the I ops on on

00:11:24,689 --> 00:11:30,759
rocky and you know we're getting pretty

00:11:27,489 --> 00:11:34,239
close here actually to the you know this

00:11:30,759 --> 00:11:37,589
5,000 something I ops with rocky is it

00:11:34,239 --> 00:11:39,819
basically boils down to 170 something

00:11:37,589 --> 00:11:42,970
gigabits per second so close to the

00:11:39,819 --> 00:11:48,669
bi-directional bandwidth of the of the

00:11:42,970 --> 00:11:51,970
wire yeah similar with the latency you

00:11:48,669 --> 00:11:54,710
know lower latency for Rocky okay so how

00:11:51,970 --> 00:11:58,520
does the colonel compare to

00:11:54,710 --> 00:12:01,460
- SP DK so here's this kind of anomalous

00:11:58,520 --> 00:12:05,360
case that I told you about the jobs

00:12:01,460 --> 00:12:08,779
equals one if you compare colonel and SP

00:12:05,360 --> 00:12:12,290
DK when IO depth is one or two I see a

00:12:08,779 --> 00:12:15,410
nice increase with SP DK compared to the

00:12:12,290 --> 00:12:18,950
colonel but then the higher I adepts

00:12:15,410 --> 00:12:20,330
for some reason it's not the case so

00:12:18,950 --> 00:12:23,510
that's something to look at because

00:12:20,330 --> 00:12:27,580
that's not what I expected to see not

00:12:23,510 --> 00:12:27,580
this I'm sorry

00:12:28,510 --> 00:12:32,570
while he's getting the mic I mean this

00:12:30,620 --> 00:12:35,300
is a this is what you should expect to

00:12:32,570 --> 00:12:38,959
see because because you're getting over

00:12:35,300 --> 00:12:40,700
thirty two CPUs right so so after at

00:12:38,959 --> 00:12:42,200
around 16 to 32 you're seeing the

00:12:40,700 --> 00:12:45,470
inflection because the kernel is doing a

00:12:42,200 --> 00:12:47,779
better job of balancing your CPU right

00:12:45,470 --> 00:12:50,560
but the on this even on the single job

00:12:47,779 --> 00:12:56,180
case I would have expected to see a

00:12:50,560 --> 00:12:59,089
benefit with SP DK no no this is this is

00:12:56,180 --> 00:13:00,680
all single job what you see here on the

00:12:59,089 --> 00:13:03,260
x-axis is the number it's the I out

00:13:00,680 --> 00:13:11,810
there this is not CPU this is just our

00:13:03,260 --> 00:13:14,750
depth okay yeah yes just the question is

00:13:11,810 --> 00:13:18,190
is the kernel versus SP DK is on the

00:13:14,750 --> 00:13:21,190
target side or the initiator both of

00:13:18,190 --> 00:13:21,190
those

00:13:21,959 --> 00:13:32,079
okay I would think about it okay right

00:13:27,309 --> 00:13:34,869
okay so okay so Colonel that first is sp

00:13:32,079 --> 00:13:40,139
DK for Rocky so actually rocky gets a

00:13:34,869 --> 00:13:43,779
big boost out of SP DK bigger boost and

00:13:40,139 --> 00:13:45,369
then tcp so it makes it makes it a very

00:13:43,779 --> 00:13:48,790
significant difference when you move

00:13:45,369 --> 00:13:50,800
from Colonel to SPD K so quick

00:13:48,790 --> 00:13:54,600
clarifying question on that when you say

00:13:50,800 --> 00:13:58,240
speedy Kate TCP what does that mean so

00:13:54,600 --> 00:14:00,579
has an implementation within SPT care

00:13:58,240 --> 00:14:10,199
it's an SP DK implementation of the DCP

00:14:00,579 --> 00:14:13,209
start okay let's look at the 32 job case

00:14:10,199 --> 00:14:16,029
so here it behaves more like I expected

00:14:13,209 --> 00:14:20,470
when you go from you know Colonel to SP

00:14:16,029 --> 00:14:23,290
DK you do get a benefit throughout the

00:14:20,470 --> 00:14:26,259
different IO depths for the 32 job case

00:14:23,290 --> 00:14:30,670
unlike the one drug case so this is what

00:14:26,259 --> 00:14:34,929
it looks like for TCP nice increase as

00:14:30,670 --> 00:14:40,720
you go into the higher I adapt and this

00:14:34,929 --> 00:14:43,269
is rocky also for 3432 jobs and we're

00:14:40,720 --> 00:14:47,079
basically getting also a very nice boost

00:14:43,269 --> 00:14:51,519
with SP DK I mean a bigger boost

00:14:47,079 --> 00:14:53,679
actually okay so another interesting

00:14:51,519 --> 00:14:57,939
thing is what does the latency

00:14:53,679 --> 00:14:59,379
distribution actually look like so you

00:14:57,939 --> 00:15:01,540
know there there is a I showed some

00:14:59,379 --> 00:15:05,319
results with latency but those are

00:15:01,540 --> 00:15:07,329
that's average latency a natural

00:15:05,319 --> 00:15:09,930
question is how is the latency

00:15:07,329 --> 00:15:13,230
distributed do you get a lot of

00:15:09,930 --> 00:15:14,970
you have a long tail are there somehow

00:15:13,230 --> 00:15:18,210
the on average they may be good but

00:15:14,970 --> 00:15:20,460
there are quite a few iOS that have high

00:15:18,210 --> 00:15:23,580
latency so it's always important to

00:15:20,460 --> 00:15:28,370
study the distribution not just the the

00:15:23,580 --> 00:15:30,750
average and so FIO actually spits out

00:15:28,370 --> 00:15:33,450
latency distributions as well so it's

00:15:30,750 --> 00:15:35,970
fortunately pretty easy to to look at

00:15:33,450 --> 00:15:38,700
that so I looked at a 32 jobs io size

00:15:35,970 --> 00:15:45,480
four kilobyte and varying the i/o depth

00:15:38,700 --> 00:15:50,790
per job 8 and 16 I Oh depth per per job

00:15:45,480 --> 00:15:54,660
so here's what I got for the 34 IO depth

00:15:50,790 --> 00:15:58,380
8 so you see different latency ranges

00:15:54,660 --> 00:16:01,920
there and comparing the tcp kernel TCPS

00:15:58,380 --> 00:16:04,860
PDK rocky kernel rockiest BBK so you

00:16:01,920 --> 00:16:08,400
know the red one is TCP kernel so you

00:16:04,860 --> 00:16:11,070
see it has its more it shifted more to

00:16:08,400 --> 00:16:14,820
the right because higher latency has has

00:16:11,070 --> 00:16:17,400
more tail to it when you go from Colonel

00:16:14,820 --> 00:16:21,360
to SP DK and kind of the left a light

00:16:17,400 --> 00:16:24,360
blue light gray graph you get you know

00:16:21,360 --> 00:16:26,910
lower latency less tail and then with

00:16:24,360 --> 00:16:31,140
rocky you can see the rocky colonel in

00:16:26,910 --> 00:16:34,140
the yellow graph which is to the left of

00:16:31,140 --> 00:16:37,709
the TCP graphs because of again lower

00:16:34,140 --> 00:16:41,070
latency less tail and if you go to the

00:16:37,709 --> 00:16:44,190
rocky with SPD K then it's particularly

00:16:41,070 --> 00:16:46,260
good so you can see how these different

00:16:44,190 --> 00:16:49,440
protocols affect the distribution of the

00:16:46,260 --> 00:16:52,040
latency as you go to deeper IO depths

00:16:49,440 --> 00:16:56,270
you get more of a tail

00:16:52,040 --> 00:17:00,280
you get more high latency iOS coming in

00:16:56,270 --> 00:17:03,260
so the difference is is more pronounced

00:17:00,280 --> 00:17:07,550
so again the kernel has a longer tail

00:17:03,260 --> 00:17:11,959
than SP DK for both TCP and rocky TCP

00:17:07,550 --> 00:17:15,230
has a longer tailed and rocky and it's

00:17:11,959 --> 00:17:19,910
it's actually fairly long tail for high

00:17:15,230 --> 00:17:22,150
higher i/o depths so that gave me some

00:17:19,910 --> 00:17:25,070
idea of what what that looks like

00:17:22,150 --> 00:17:28,430
and I also mentioned that there are this

00:17:25,070 --> 00:17:30,920
other tool perf that SP DK has for

00:17:28,430 --> 00:17:35,390
performance so I actually compared the

00:17:30,920 --> 00:17:38,810
two perfect FIO to see if it's truly the

00:17:35,390 --> 00:17:42,280
case that perf is lower lower overhead

00:17:38,810 --> 00:17:46,190
and provides better better results so

00:17:42,280 --> 00:17:50,600
now in the red one you see if IO light

00:17:46,190 --> 00:17:56,690
blue is perf so indeed the eye ops this

00:17:50,600 --> 00:17:59,000
is for TCP there higher for for perf and

00:17:56,690 --> 00:18:01,340
it's you know 15 percent to 34 percent

00:17:59,000 --> 00:18:03,620
higher and this is where I you know I

00:18:01,340 --> 00:18:06,680
this is this jobs one case where I had

00:18:03,620 --> 00:18:09,290
this weird phenomenon with FIO where I

00:18:06,680 --> 00:18:13,550
wasn't seeing much benefit with SP DK

00:18:09,290 --> 00:18:16,760
but here you see that actually it does

00:18:13,550 --> 00:18:20,390
do well as you go into the i/o depths

00:18:16,760 --> 00:18:23,120
with perf versus FIO so I'm not sure why

00:18:20,390 --> 00:18:29,810
they behave differently in this case but

00:18:23,120 --> 00:18:32,740
they do and this is for the rock for I'm

00:18:29,810 --> 00:18:32,740
sorry to 32

00:18:34,350 --> 00:18:40,000
okay let me see what I did here this was

00:18:37,899 --> 00:18:43,509
oh yeah this was a 32 job the previous

00:18:40,000 --> 00:18:45,039
one was the single job so again you see

00:18:43,509 --> 00:18:48,750
the twenty twelve percent to twenty six

00:18:45,039 --> 00:18:50,980
percent higher I ops with with perf so

00:18:48,750 --> 00:18:53,620
you know so it does actually make a

00:18:50,980 --> 00:18:58,559
difference to look at perf in in

00:18:53,620 --> 00:18:58,559
addition to FIO when when running SPD k

00:18:58,769 --> 00:19:05,320
ok so let's look at the what the actual

00:19:03,009 --> 00:19:07,120
latency numbers were cuz it's a little

00:19:05,320 --> 00:19:10,389
bit hard to see from all the graphs I

00:19:07,120 --> 00:19:13,539
put it in these tables so you can see

00:19:10,389 --> 00:19:17,320
this is the jobs one and let's say IO

00:19:13,539 --> 00:19:18,940
depth one so you can see TCP Colonel I

00:19:17,320 --> 00:19:23,230
was getting 60 micro so this is

00:19:18,940 --> 00:19:27,429
microseconds DCPS PDK brought it down to

00:19:23,230 --> 00:19:31,600
33 with FIO and then with perf went down

00:19:27,429 --> 00:19:36,220
to 25 rocky colonel i got 21 and then

00:19:31,600 --> 00:19:39,190
rocky with SP DK it went down to 5 so

00:19:36,220 --> 00:19:44,649
you can see under the impact of these of

00:19:39,190 --> 00:19:49,059
these now you know i think if you look

00:19:44,649 --> 00:19:51,700
at nvme you know pci and vm e storage

00:19:49,059 --> 00:19:55,210
devices you know Layton sees they play

00:19:51,700 --> 00:19:57,909
in the tens of microseconds so it's you

00:19:55,210 --> 00:20:01,809
know these are not that big of an issue

00:19:57,909 --> 00:20:04,179
here but but you can see a comparison

00:20:01,809 --> 00:20:07,029
here of how big of a difference you know

00:20:04,179 --> 00:20:11,070
it makes if you just look at the this

00:20:07,029 --> 00:20:14,530
little article latency in isolation

00:20:11,070 --> 00:20:17,679
similarly for 32 jobs I provided the

00:20:14,530 --> 00:20:19,870
table here that shows you the how this

00:20:17,679 --> 00:20:22,840
works you know the gray part is the TCP

00:20:19,870 --> 00:20:26,290
and a white part I did not run rocky

00:20:22,840 --> 00:20:28,960
with perf I only run ran to be with her

00:20:26,290 --> 00:20:31,860
so I don't have that column with the SPD

00:20:28,960 --> 00:20:38,350
caper from the Rocky

00:20:31,860 --> 00:20:41,500
okay so summary you know they weren't

00:20:38,350 --> 00:20:45,610
really it was what I expected but I got

00:20:41,500 --> 00:20:47,380
a good handle on what the numbers are so

00:20:45,610 --> 00:20:49,570
the performance of nvme TCP is quite

00:20:47,380 --> 00:20:51,780
good so we got 1 million I ops on this

00:20:49,570 --> 00:20:55,630
you know to server setup client server

00:20:51,780 --> 00:21:00,670
with the kernel 1.4 million I ops with

00:20:55,630 --> 00:21:05,140
SP DK I was with the 32 jobs IEPs 8 4

00:21:00,670 --> 00:21:07,030
kilobyte IO size when you go to 9000 MTU

00:21:05,140 --> 00:21:09,250
versus 1500 you do get a meaningful

00:21:07,030 --> 00:21:12,490
increase with in performance with nvme

00:21:09,250 --> 00:21:15,760
tcp rocky performance is significantly

00:21:12,490 --> 00:21:18,220
higher than nvme tcp in all the tests so

00:21:15,760 --> 00:21:22,030
in the instead of 1 million I ops in the

00:21:18,220 --> 00:21:24,190
kernel we got two and a half million 5.4

00:21:22,030 --> 00:21:28,990
million I ops with SPD canes that at one

00:21:24,190 --> 00:21:30,820
point four and but SPD K increases

00:21:28,990 --> 00:21:34,000
performance significantly for both TCP

00:21:30,820 --> 00:21:35,679
and rocky more so for Rocky than the

00:21:34,000 --> 00:21:39,880
anticipate but it's significant for TCP

00:21:35,679 --> 00:21:42,670
as well longer latency tales with TCP

00:21:39,880 --> 00:21:45,010
versus rocky SP DK reduces detail for

00:21:42,670 --> 00:21:48,550
both and perf shows better performance

00:21:45,010 --> 00:21:52,150
in fi o.4 in GMA TCP I did not get the

00:21:48,550 --> 00:21:55,740
time to do it for Rocky okay that's what

00:21:52,150 --> 00:21:55,740
I had any additional questions

00:22:01,470 --> 00:22:05,530
so a

00:22:03,670 --> 00:22:07,810
the question that comes to mind is

00:22:05,530 --> 00:22:11,410
whether any of the differences for

00:22:07,810 --> 00:22:17,200
example between TCP and rocky as well as

00:22:11,410 --> 00:22:20,020
between TCP and SP decay you know a

00:22:17,200 --> 00:22:23,290
colonel and SVD a would be to do due to

00:22:20,020 --> 00:22:26,080
some of the security mechanisms that we

00:22:23,290 --> 00:22:27,670
have in the kernel so to test that one

00:22:26,080 --> 00:22:29,590
what could disable respective

00:22:27,670 --> 00:22:33,730
mitigations what could one could try to

00:22:29,590 --> 00:22:37,270
disable as map support right I have not

00:22:33,730 --> 00:22:40,600
tried to do that so I ran so this was

00:22:37,270 --> 00:22:43,830
kernel for 19 with whatever is in there

00:22:40,600 --> 00:22:46,930
so I don't know if I have not tried to

00:22:43,830 --> 00:22:49,810
disable the security mitigation say it

00:22:46,930 --> 00:22:52,200
might be interesting to to run with that

00:22:49,810 --> 00:22:52,200
as well

00:22:52,350 --> 00:23:04,740
I'm sorry I don't know or use the mic

00:23:01,169 --> 00:23:07,289
user my it for 19 with the way they ask

00:23:04,740 --> 00:23:10,080
matches with the nvme TCP T alright I

00:23:07,289 --> 00:23:12,360
repeat the question for the please for

00:23:10,080 --> 00:23:15,630
my mom okay the question was he's gonna

00:23:12,360 --> 00:23:20,039
see the video yeah the question was how

00:23:15,630 --> 00:23:23,429
it was tested in 419 when it was the

00:23:20,039 --> 00:23:26,639
patch was introduced in 5 right yeah but

00:23:23,429 --> 00:23:29,009
there's a 419 with with the patches for

00:23:26,639 --> 00:23:32,570
with nvme TCP and that's that's what I

00:23:29,009 --> 00:23:35,220
used for this it's not kernel 5 its 419

00:23:32,570 --> 00:23:38,700
is the question from Michael on or

00:23:35,220 --> 00:23:43,350
really that the kernel nvme TCP numbers

00:23:38,700 --> 00:23:44,759
are too low is that it's actually pretty

00:23:43,350 --> 00:23:48,120
in line right at a million high hopes

00:23:44,759 --> 00:23:50,340
through TCP is pretty hard is I think

00:23:48,120 --> 00:23:54,139
the earlier point that all was probably

00:23:50,340 --> 00:23:57,600
making maybe was the setup you interrupt

00:23:54,139 --> 00:24:00,570
distribution may not have been the best

00:23:57,600 --> 00:24:03,269
for the sake of TCP so it could probably

00:24:00,570 --> 00:24:05,909
have good better numbers no well like I

00:24:03,269 --> 00:24:09,830
said with this single job case I pinned

00:24:05,909 --> 00:24:12,269
the I set the affinity locally to the

00:24:09,830 --> 00:24:14,759
core for the single case okay okay

00:24:12,269 --> 00:24:19,169
seeing the case 32 I let it go where it

00:24:14,759 --> 00:24:22,610
goes we can probably afford one more

00:24:19,169 --> 00:24:25,210
question that's it anybody

00:24:22,610 --> 00:24:25,210
okay

00:24:29,000 --> 00:24:35,250
so certainly the nvme or tcp kernel

00:24:32,910 --> 00:24:37,620
number are quite low and the latency

00:24:35,250 --> 00:24:39,990
distribution is everywhere this is with

00:24:37,620 --> 00:24:42,480
the null block device right latency

00:24:39,990 --> 00:24:45,270
starts with 30 or so something like that

00:24:42,480 --> 00:24:51,690
and it goes to 1,500 did you try to use

00:24:45,270 --> 00:24:53,720
any tcp optimizations like RM w mem not

00:24:51,690 --> 00:24:57,240
changed any of the tuning parameters

00:24:53,720 --> 00:24:58,620
out-of-the-box out-of-the-box it's I

00:24:57,240 --> 00:24:59,910
mean the thing is with changing

00:24:58,620 --> 00:25:02,370
questions what it changed to and then

00:24:59,910 --> 00:25:04,230
it's like is it a wacom all where you

00:25:02,370 --> 00:25:07,830
know you improve it for some work but

00:25:04,230 --> 00:25:10,200
not another right ok thank you ok so it

00:25:07,830 --> 00:25:15,210
sounds like the TCP variant may need

00:25:10,200 --> 00:25:19,110
some retweeting and then yeah anybody

00:25:15,210 --> 00:25:23,930
wanting to tuning yeah just send him

00:25:19,110 --> 00:25:23,930

YouTube URL: https://www.youtube.com/watch?v=HLXxE5WWRf8


