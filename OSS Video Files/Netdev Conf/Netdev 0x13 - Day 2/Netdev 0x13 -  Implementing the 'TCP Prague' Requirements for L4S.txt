Title: Netdev 0x13 -  Implementing the 'TCP Prague' Requirements for L4S
Publication date: 2019-05-25
Playlist: Netdev 0x13 - Day 2
Description: 
	Bob Briscoe et al will introduce the important components of the first 'TCP Prague' implementation which runs on Linux.
A set of changes to DCTCP that would make it deployable over the public Internet are
dubbed the 'TCP Prague requirements'.

Bob provides the rationale and explanation on how the parts integrate together,
both within each system and at Internet scale; he outlines the maturity of each
component implementation, plus some points of interest and issues where decisions
are still needed.

https://netdevconf.org/0x13/session.html?talk-tcp-prague-l4s
Captions: 
	00:00:00,000 --> 00:00:03,030
coming in because we've we've been

00:00:01,439 --> 00:00:05,029
pushed back a bit and there's a lot to

00:00:03,030 --> 00:00:07,170
get through so this is about

00:00:05,029 --> 00:00:11,130
implementing the TCP product

00:00:07,170 --> 00:00:21,779
requirements for l4s actually I am kami

00:00:11,130 --> 00:00:24,840
of it on the monitor here as well it's

00:00:21,779 --> 00:00:32,850
not on the monitor down here it was a

00:00:24,840 --> 00:00:34,680
few seconds ago okay yeah actually I

00:00:32,850 --> 00:00:37,920
changed the title but I only uploaded

00:00:34,680 --> 00:00:40,710
the slides with the new title I'm trying

00:00:37,920 --> 00:00:41,910
to get get out of this TCP prog

00:00:40,710 --> 00:00:43,530
requirement and just call it the prog

00:00:41,910 --> 00:00:46,020
requirements because it also covers

00:00:43,530 --> 00:00:48,719
real-time media so anyway that's all the

00:00:46,020 --> 00:00:52,260
offers carry on next I've got the thing

00:00:48,719 --> 00:00:59,250
here what do we do green one yep all

00:00:52,260 --> 00:01:02,149
right so the point of this is to have

00:00:59,250 --> 00:01:07,799
low delay and in fact very low delay and

00:01:02,149 --> 00:01:10,110
high throughput at the same time we've

00:01:07,799 --> 00:01:12,810
got about 10 times lower delay on the

00:01:10,110 --> 00:01:16,290
state-of-the-art a QMS in all the

00:01:12,810 --> 00:01:19,770
performance tests we've done because

00:01:16,290 --> 00:01:21,689
we're doing not just in iqm it's it's a

00:01:19,770 --> 00:01:24,229
aqm that works with new congestion

00:01:21,689 --> 00:01:26,640
controls or with the congestion controls

00:01:24,229 --> 00:01:29,850
we call scaleable congestion controls

00:01:26,640 --> 00:01:31,200
like they just sent a TCP and I'll come

00:01:29,850 --> 00:01:35,009
on to all that that's the point of this

00:01:31,200 --> 00:01:40,560
talk it's about an implementation of TCP

00:01:35,009 --> 00:01:47,030
Prague which is essentially DC TCP with

00:01:40,560 --> 00:01:49,860
some changes and so the idea is to use

00:01:47,030 --> 00:01:53,009
these to get very low delay over the

00:01:49,860 --> 00:01:54,390
Internet not just in data centers but

00:01:53,009 --> 00:01:58,950
still have that capacity seeking

00:01:54,390 --> 00:02:00,869
behavior both in TCP like transports and

00:01:58,950 --> 00:02:03,420
in real-time transports and obviously

00:02:00,869 --> 00:02:07,950
and all the others like RPC DNS type

00:02:03,420 --> 00:02:10,770
things as well so basically a new way

00:02:07,950 --> 00:02:12,640
for in replacing if you like the best

00:02:10,770 --> 00:02:14,020
effort internet with

00:02:12,640 --> 00:02:15,700
and Internet that's low delay for

00:02:14,020 --> 00:02:17,500
everything and then you don't have to

00:02:15,700 --> 00:02:19,320
worry about delay anymore you don't have

00:02:17,500 --> 00:02:21,580
to have it as Kwas you just you have

00:02:19,320 --> 00:02:26,260
causes Japan birthing but you never have

00:02:21,580 --> 00:02:31,120
to worry about delay anymore so there's

00:02:26,260 --> 00:02:32,890
no low delay class it's on the ITF so

00:02:31,120 --> 00:02:35,470
experimental track yell for ice

00:02:32,890 --> 00:02:38,640
architecture and l4s stands for low

00:02:35,470 --> 00:02:42,280
latency low loss and scalable throughput

00:02:38,640 --> 00:02:45,580
so all three of those items I'm going to

00:02:42,280 --> 00:02:47,920
cover all then it's also been adopted in

00:02:45,580 --> 00:02:51,430
January the specs were released in

00:02:47,920 --> 00:02:54,940
DOCSIS 3.1 for cable networks and cable

00:02:51,430 --> 00:03:01,410
operators are currently implementing

00:02:54,940 --> 00:03:04,870
that should see products appearing

00:03:01,410 --> 00:03:09,580
certainly for operators to try out early

00:03:04,870 --> 00:03:13,800
next year so but this talk is about the

00:03:09,580 --> 00:03:13,800
linux reference implementation next

00:03:16,410 --> 00:03:20,590
right so when I say I just want to

00:03:19,269 --> 00:03:23,230
emphasize this point about for every

00:03:20,590 --> 00:03:26,650
application and and low delay we're not

00:03:23,230 --> 00:03:28,269
just talking about SSH voice gaming

00:03:26,650 --> 00:03:32,489
although of course they're very

00:03:28,269 --> 00:03:37,860
important particularly gaming and HTTP

00:03:32,489 --> 00:03:43,420
we're talking about any tcp any quick

00:03:37,860 --> 00:03:45,780
real time media WebRTC you know HD cut

00:03:43,420 --> 00:03:48,340
video conferencing interactive video

00:03:45,780 --> 00:03:53,950
cloud rendered virtual reality and in

00:03:48,340 --> 00:03:56,019
fact that oculus rift down the bottom

00:03:53,950 --> 00:04:00,549
there over I hope is in the room

00:03:56,019 --> 00:04:04,090
somewhere demonstrated l4s at multimedia

00:04:00,549 --> 00:04:07,989
systems on a 40 Meg broadband link with

00:04:04,090 --> 00:04:12,810
an oculus rift doing cloud rendered

00:04:07,989 --> 00:04:15,810
video from a 360-degree camera and also

00:04:12,810 --> 00:04:15,810
the

00:04:17,180 --> 00:04:28,880
oh how do you point on this thing which

00:04:22,250 --> 00:04:31,400
one is it yeah this one here with a HD

00:04:28,880 --> 00:04:33,289
panoramic stitched together football

00:04:31,400 --> 00:04:35,150
match and you can pan and zoom your

00:04:33,289 --> 00:04:37,060
particular view of it which is try to

00:04:35,150 --> 00:04:39,289
show there with them with white square

00:04:37,060 --> 00:04:40,400
you can pan and zoom that and it sort of

00:04:39,289 --> 00:04:44,120
sticks to your fingering though it's

00:04:40,400 --> 00:04:47,360
rendered in the cloud over a broadband

00:04:44,120 --> 00:04:51,020
link and we had four software downloads

00:04:47,360 --> 00:04:54,680
going on and a high level of synthetic

00:04:51,020 --> 00:04:56,180
web traffic and a gaming benchmark all

00:04:54,680 --> 00:04:57,919
on a 40 mega link and none of the

00:04:56,180 --> 00:05:03,380
packets worth more than one millisecond

00:04:57,919 --> 00:05:06,820
giving delay so it's it's quite a

00:05:03,380 --> 00:05:09,320
significant change and it means you can

00:05:06,820 --> 00:05:11,150
effectively forget about delay or

00:05:09,320 --> 00:05:18,139
queuing delay and only worry about

00:05:11,150 --> 00:05:20,419
propagation delay so how's it work or

00:05:18,139 --> 00:05:22,960
what what's the big trick well the big

00:05:20,419 --> 00:05:25,940
trick is scalable congestion control and

00:05:22,960 --> 00:05:29,229
as I said DC TCP is an example of that

00:05:25,940 --> 00:05:31,639
so essentially going through one to four

00:05:29,229 --> 00:05:34,570
in the early days with just the tail

00:05:31,639 --> 00:05:37,250
drop buffers your TCP sawtooth is

00:05:34,570 --> 00:05:40,070
occupying the top of the buffer I've

00:05:37,250 --> 00:05:42,680
tried to draw here two things with

00:05:40,070 --> 00:05:44,750
different units in this on the same plot

00:05:42,680 --> 00:05:49,370
there's your bandwidth in the pipe and

00:05:44,750 --> 00:05:53,449
then your buffer on top of it and your

00:05:49,370 --> 00:05:55,490
congestion window using the buffer and

00:05:53,449 --> 00:05:57,710
then as as you bring in a QAM in number

00:05:55,490 --> 00:06:01,010
two you can push down the top of the

00:05:57,710 --> 00:06:05,090
spikes conceptually so you get still get

00:06:01,010 --> 00:06:06,860
full utilization but you don't use so

00:06:05,090 --> 00:06:08,539
much of the buffer but if you try and

00:06:06,860 --> 00:06:10,849
push that down further in number three

00:06:08,539 --> 00:06:14,030
you start underutilizing the link and

00:06:10,849 --> 00:06:15,949
the problem is the unscalable TCP which

00:06:14,030 --> 00:06:20,240
I call classic sender congestion control

00:06:15,949 --> 00:06:20,570
like Reno like cubic in fact as well and

00:06:20,240 --> 00:06:23,540
our

00:06:20,570 --> 00:06:25,550
I've got a slide on that has these

00:06:23,540 --> 00:06:29,780
sorties are getting bigger as you go

00:06:25,550 --> 00:06:31,700
faster over the years and so you can't

00:06:29,780 --> 00:06:33,650
have both low delay and higher

00:06:31,700 --> 00:06:36,650
utilization unless you go for number

00:06:33,650 --> 00:06:38,570
four which is the smaller sawteeth which

00:06:36,650 --> 00:06:41,990
is what you get with datacenter TTP and

00:06:38,570 --> 00:06:46,490
the like so you can have both low delay

00:06:41,990 --> 00:06:47,990
and higher utilization and then of

00:06:46,490 --> 00:06:52,810
course you you can bring down your

00:06:47,990 --> 00:06:55,480
buffer sizing and all the rest of it so

00:06:52,810 --> 00:07:05,410
just just to give you an example of this

00:06:55,480 --> 00:07:05,410
cubic shown here running first one rate

00:07:07,210 --> 00:07:12,110
running at one rate and then running

00:07:09,980 --> 00:07:15,890
eight times higher cubic in red and

00:07:12,110 --> 00:07:22,600
datacenter TTP in blue and the sawtooth

00:07:15,890 --> 00:07:27,280
of cubic sorry the sawteeth of cubic

00:07:22,600 --> 00:07:31,130
getting the longer as you get faster so

00:07:27,280 --> 00:07:33,920
at the at the 100 mega rate there 250

00:07:31,130 --> 00:07:35,780
round-trip times long when you go eight

00:07:33,920 --> 00:07:38,480
times faster they get twice as long and

00:07:35,780 --> 00:07:42,410
that keeps on going and that was the

00:07:38,480 --> 00:07:45,260
problem with reno originally which was

00:07:42,410 --> 00:07:49,550
why cubic was introduced that cubic is

00:07:45,260 --> 00:07:52,160
still just not quite as unscalable as

00:07:49,550 --> 00:07:55,960
Reno was Reno was was growing linearly

00:07:52,160 --> 00:07:58,880
with with rate cubics sawteeth a growing

00:07:55,960 --> 00:08:01,640
bigger and bigger as you get faster but

00:07:58,880 --> 00:08:03,920
not as fast as Reno was growing and take

00:08:01,640 --> 00:08:06,860
the center TCP they don't grow at all

00:08:03,920 --> 00:08:08,630
they're invariant they stay the same

00:08:06,860 --> 00:08:11,180
size whatever your rate and that's what

00:08:08,630 --> 00:08:15,830
makes it scalable and that means and so

00:08:11,180 --> 00:08:20,240
the little sort of magnified area there

00:08:15,830 --> 00:08:22,400
shows the datacenter TCP sawtooth the

00:08:20,240 --> 00:08:24,530
reason you're not getting a larger

00:08:22,400 --> 00:08:28,310
variation delay as things get faster is

00:08:24,530 --> 00:08:31,040
they never change on average as you get

00:08:28,310 --> 00:08:35,090
faster over the years right

00:08:31,040 --> 00:08:38,150
and so they this little parameter V that

00:08:35,090 --> 00:08:40,340
tells you how many you get in a you know

00:08:38,150 --> 00:08:43,900
round-trip time and with data since the

00:08:40,340 --> 00:08:47,300
TCP on average you get two for any rate

00:08:43,900 --> 00:08:49,070
with cubic it goes up as you see from

00:08:47,300 --> 00:08:51,080
one two hundred and fiftieth to one five

00:08:49,070 --> 00:08:52,460
hundredth in other words one every two

00:08:51,080 --> 00:08:54,350
hundred fifty round-trip times or one

00:08:52,460 --> 00:08:56,690
every five hundred round-trip times so

00:08:54,350 --> 00:08:59,150
effectively you're running blind between

00:08:56,690 --> 00:09:01,190
each of those control signals

00:08:59,150 --> 00:09:02,240
whereas in data center TCP you're

00:09:01,190 --> 00:09:04,400
getting a signal every - every

00:09:02,240 --> 00:09:07,550
round-trip time it's a much richer

00:09:04,400 --> 00:09:10,610
signal so that's that's sort of the

00:09:07,550 --> 00:09:15,320
intuition that gives you why it all

00:09:10,610 --> 00:09:17,960
works but it might be scalable but it's

00:09:15,320 --> 00:09:22,280
too aggressive for the Internet if you

00:09:17,960 --> 00:09:23,420
try and put DC TCP on the Internet in

00:09:22,280 --> 00:09:26,450
these explicit congestion notification

00:09:23,420 --> 00:09:28,490
and each tip of those sawtooth raises

00:09:26,450 --> 00:09:33,940
the UC end of each one of those is

00:09:28,490 --> 00:09:37,130
another ACN mark or at least one and so

00:09:33,940 --> 00:09:39,280
if you put that alongside classic

00:09:37,130 --> 00:09:42,700
congestion controls like TCP or quick

00:09:39,280 --> 00:09:44,870
they detect a very high congestion level

00:09:42,700 --> 00:09:46,790
compared to what they think of you know

00:09:44,870 --> 00:09:49,370
every 250 round-trip times or something

00:09:46,790 --> 00:09:51,850
getting one ACN mark and so they back

00:09:49,370 --> 00:09:54,050
down to a very low rate and that's why

00:09:51,850 --> 00:09:56,270
they descent the TCP he was called

00:09:54,050 --> 00:09:59,870
datacenter TCP because it meant keep me

00:09:56,270 --> 00:10:01,520
locked up in a data center it wasn't

00:09:59,870 --> 00:10:03,440
because it only works in a data center

00:10:01,520 --> 00:10:05,150
it was because you'd need to change the

00:10:03,440 --> 00:10:07,220
whole internet all at the same time to

00:10:05,150 --> 00:10:10,280
deploy it on the internet because it

00:10:07,220 --> 00:10:11,750
doesn't coexist with existing traffic at

00:10:10,280 --> 00:10:14,780
least not until now

00:10:11,750 --> 00:10:21,200
and that's what the point of what we're

00:10:14,780 --> 00:10:23,030
doing is so I'm only gonna briefly talk

00:10:21,200 --> 00:10:25,570
about the couple take um because I want

00:10:23,030 --> 00:10:29,540
to focus this on the TCP side of it

00:10:25,570 --> 00:10:33,560
because there's a later talk in track 3

00:10:29,540 --> 00:10:36,130
I think by Olga on her implementation of

00:10:33,560 --> 00:10:42,830
the dual cue couple date um

00:10:36,130 --> 00:10:44,460
but very briefly it relies on using the

00:10:42,830 --> 00:10:47,660
remaining code point

00:10:44,460 --> 00:10:54,149
in the ecn field of the IP header east

00:10:47,660 --> 00:10:57,930
t12 identify this traffic as l4s ecn and

00:10:54,149 --> 00:11:00,510
senders setback which classifies it into

00:10:57,930 --> 00:11:02,820
a second queue called the l4 SQ or the

00:11:00,510 --> 00:11:06,420
low light and secure if you like and all

00:11:02,820 --> 00:11:07,980
all the existing classic congestion

00:11:06,420 --> 00:11:10,830
control type traffic that doesn't use

00:11:07,980 --> 00:11:14,070
that gets classified into the other

00:11:10,830 --> 00:11:15,750
queue this is not bandwidth priority

00:11:14,070 --> 00:11:17,970
it's just sort of old and new it's just

00:11:15,750 --> 00:11:19,950
an incremental deployment technique to

00:11:17,970 --> 00:11:22,260
have one queue for the for the new stuff

00:11:19,950 --> 00:11:24,750
and the other for the old stuff and the

00:11:22,260 --> 00:11:26,190
idea is that the old stuff doesn't get

00:11:24,750 --> 00:11:27,810
any worse than it does today you can

00:11:26,190 --> 00:11:31,050
yeah this is a framework you can have

00:11:27,810 --> 00:11:36,330
different ATMs in those two queues with

00:11:31,050 --> 00:11:40,920
a PI aqm in the lower queue you get the

00:11:36,330 --> 00:11:43,320
same delay characteristics as you do

00:11:40,920 --> 00:11:46,500
today with PI and the idea is that in

00:11:43,320 --> 00:11:48,930
the l4 SQ you can get this ten times

00:11:46,500 --> 00:11:52,670
better delay and because ten times

00:11:48,930 --> 00:11:56,850
better delay means queuing delay means

00:11:52,670 --> 00:11:58,800
you effectively don't need so much

00:11:56,850 --> 00:12:01,020
server infrastructure you can have

00:11:58,800 --> 00:12:03,150
larger radius is served by one server

00:12:01,020 --> 00:12:06,630
because you've got more flexibility with

00:12:03,150 --> 00:12:11,610
your with your propagation delay but

00:12:06,630 --> 00:12:14,760
anyway back back to this so ETA and code

00:12:11,610 --> 00:12:17,279
point gets you into the other queue and

00:12:14,760 --> 00:12:22,290
then you have a conditional priority

00:12:17,279 --> 00:12:24,270
scheduler which gives priority to the l4

00:12:22,290 --> 00:12:25,740
s queue but only priority in terms of

00:12:24,270 --> 00:12:28,680
delighting not in terms of bandwidth

00:12:25,740 --> 00:12:30,660
because there's this coupling going back

00:12:28,680 --> 00:12:32,579
the other way which is about on the next

00:12:30,660 --> 00:12:35,279
slide but I'll just briefly explain on

00:12:32,579 --> 00:12:37,260
this one and what that coupling does it

00:12:35,279 --> 00:12:40,890
works against the priority scheduler

00:12:37,260 --> 00:12:44,760
what it does it puts the level of

00:12:40,890 --> 00:12:47,940
congestion marking from the classic

00:12:44,760 --> 00:12:50,430
queue into the l4 SQ it couples it

00:12:47,940 --> 00:12:53,640
across so it looks as if the traffic is

00:12:50,430 --> 00:12:55,560
in the other queue so it sort of adds

00:12:53,640 --> 00:12:57,089
enough congestion in there to represent

00:12:55,560 --> 00:12:59,490
the traffic that

00:12:57,089 --> 00:13:04,410
is in the other queue and that means

00:12:59,490 --> 00:13:06,839
that the scalable senders leave space if

00:13:04,410 --> 00:13:13,379
there is traffic in the orange classic

00:13:06,839 --> 00:13:14,970
queue the the low latency flows think

00:13:13,379 --> 00:13:16,769
this traffic there they leave space for

00:13:14,970 --> 00:13:20,550
it and that means that even though

00:13:16,769 --> 00:13:22,709
they've got priority for the latency the

00:13:20,550 --> 00:13:28,800
coupling means that they don't get

00:13:22,709 --> 00:13:33,360
priority for the bandwidth and so

00:13:28,800 --> 00:13:35,850
overall you get flow rate fairness if

00:13:33,360 --> 00:13:38,160
you like you know that the flow rates of

00:13:35,850 --> 00:13:40,529
each of the flows are the same as though

00:13:38,160 --> 00:13:45,540
it's one pool of bandwidth even though

00:13:40,529 --> 00:13:46,740
it's too cute now just to explain that

00:13:45,540 --> 00:13:53,550
in a bit more depth the bandwidth

00:13:46,740 --> 00:13:58,199
pooling you've got it's slightly more

00:13:53,550 --> 00:14:00,420
interesting than that in that the I've

00:13:58,199 --> 00:14:04,230
got you know you've got maths here right

00:14:00,420 --> 00:14:06,059
and this is you know we need to explain

00:14:04,230 --> 00:14:08,309
why this squaring is important and

00:14:06,059 --> 00:14:11,550
that's because all the classic

00:14:08,309 --> 00:14:15,240
congestion controls their their packet

00:14:11,550 --> 00:14:18,389
rate runs to this formula that most TCP

00:14:15,240 --> 00:14:20,339
people know or at least participe

00:14:18,389 --> 00:14:23,449
researchers know which depends on the

00:14:20,339 --> 00:14:26,910
square root of the drop probability and

00:14:23,449 --> 00:14:31,740
so when we couple across we have to

00:14:26,910 --> 00:14:34,079
square the the probability that we're

00:14:31,740 --> 00:14:36,470
coupling across to the other queue to

00:14:34,079 --> 00:14:39,059
counterbalance that square root so that

00:14:36,470 --> 00:14:41,939
it represents those flows in the other

00:14:39,059 --> 00:14:44,579
queue and and I didn't think that was

00:14:41,939 --> 00:14:46,110
going to work that well when we thought

00:14:44,579 --> 00:14:50,189
up the idea but it actually works really

00:14:46,110 --> 00:14:55,050
well you get very similar flow rates as

00:14:50,189 --> 00:14:57,749
if you were all in the same queue so

00:14:55,050 --> 00:15:00,300
just just an example on the right there

00:14:57,749 --> 00:15:04,050
you've got two numbers so if you've got

00:15:00,300 --> 00:15:06,620
point o 9% packet drop level in the

00:15:04,050 --> 00:15:09,050
classic queue

00:15:06,620 --> 00:15:10,730
that is the square of 3% because

00:15:09,050 --> 00:15:12,410
obviously numbers are smaller than one

00:15:10,730 --> 00:15:17,300
so when you square it they get even

00:15:12,410 --> 00:15:18,800
smaller and so you're getting that 3%

00:15:17,300 --> 00:15:22,910
marking which is what I said about this

00:15:18,800 --> 00:15:26,180
more frequent marking in the earlier

00:15:22,910 --> 00:15:28,910
slide with the small sore teeth and

00:15:26,180 --> 00:15:31,400
that's what the scalable senders will

00:15:28,910 --> 00:15:42,170
induce and you see there they haven't

00:15:31,400 --> 00:15:43,340
got the square root on the don't have

00:15:42,170 --> 00:15:50,080
that square root and that's what makes

00:15:43,340 --> 00:15:50,080
them scalable that's why they're linear

00:15:50,380 --> 00:15:59,780
right so the most important requirement

00:15:55,670 --> 00:16:02,390
then is to ensure that these scalable

00:15:59,780 --> 00:16:05,890
congestion controls deal with an

00:16:02,390 --> 00:16:09,620
internet that doesn't always have that

00:16:05,890 --> 00:16:12,920
system in every cube the idea is you put

00:16:09,620 --> 00:16:16,520
that dual Q system where the bottleneck

00:16:12,920 --> 00:16:20,480
is which is why cable labs have adopted

00:16:16,520 --> 00:16:24,770
this for DOCSIS and similarly it's being

00:16:20,480 --> 00:16:34,400
worked on for DSL and for access into

00:16:24,770 --> 00:16:38,090
the XS on data centers and we need to

00:16:34,400 --> 00:16:42,230
make sure that this aggressive traffic

00:16:38,090 --> 00:16:43,910
doesn't harm other traffic so in order

00:16:42,230 --> 00:16:46,370
for scalable congestion controls to use

00:16:43,910 --> 00:16:49,910
the Internet when we first demonstrated

00:16:46,370 --> 00:16:55,670
this at the ITF in this city Prague in

00:16:49,910 --> 00:16:57,410
July 2015 the day after there was a ad

00:16:55,670 --> 00:17:01,100
hoc meeting of about 30 people working

00:16:57,410 --> 00:17:04,400
on DC TCP to try and pull together a set

00:17:01,100 --> 00:17:07,610
of mandatory requirements that would be

00:17:04,400 --> 00:17:09,320
needed to change DCTC to be to make it

00:17:07,610 --> 00:17:11,600
suitable for the public Internet and

00:17:09,320 --> 00:17:15,020
some optional performance requirements

00:17:11,600 --> 00:17:17,690
as well and and we called them at the

00:17:15,020 --> 00:17:19,620
time or Matt Mathis suggested the name

00:17:17,690 --> 00:17:21,120
TCP Prague

00:17:19,620 --> 00:17:22,439
we called them the TCP product

00:17:21,120 --> 00:17:23,699
requirements well I'm trying to get away

00:17:22,439 --> 00:17:26,130
from that and just call them the Prague

00:17:23,699 --> 00:17:28,860
l4s requirements because they're not

00:17:26,130 --> 00:17:33,720
just for TCP they apply to any transport

00:17:28,860 --> 00:17:36,029
quick real-time whatever and they have

00:17:33,720 --> 00:17:39,570
evolved into ITF conditions for setting

00:17:36,029 --> 00:17:41,760
this easy t1 code point in IP and in the

00:17:39,570 --> 00:17:51,000
last draft I've taken out the word tcp

00:17:41,760 --> 00:17:52,470
prog requirements so what is tcp Prague

00:17:51,000 --> 00:17:55,980
I'm gonna quickly try and skip through

00:17:52,470 --> 00:17:57,809
all this it's a new linux congesting a

00:17:55,980 --> 00:18:01,020
trail model and a module and i can say

00:17:57,809 --> 00:18:04,460
that because finally we got it an RFC

00:18:01,020 --> 00:18:04,460
out yesterday whew

00:18:05,700 --> 00:18:11,340
and essentially it's DC TCP with

00:18:08,970 --> 00:18:13,169
mandatory use of certain improvements to

00:18:11,340 --> 00:18:14,730
the base TCP so things that are already

00:18:13,169 --> 00:18:16,799
there it's sort of like a set of

00:18:14,730 --> 00:18:18,899
mandatory configurations of other bit

00:18:16,799 --> 00:18:21,899
it's a configuration of mandatory other

00:18:18,899 --> 00:18:27,240
bits but there are some other new parts

00:18:21,899 --> 00:18:29,490
as well and so most of it is bits that

00:18:27,240 --> 00:18:31,279
were useful for other reasons that are

00:18:29,490 --> 00:18:36,059
being pulled together into one

00:18:31,279 --> 00:18:37,350
implementation it's usable for testing

00:18:36,059 --> 00:18:40,200
but it's still working progress

00:18:37,350 --> 00:18:42,570
obviously it's available from that repo

00:18:40,200 --> 00:18:44,640
and as I say obviously submitted

00:18:42,570 --> 00:18:45,720
yesterday and they're the instructions

00:18:44,640 --> 00:18:47,340
for loading and labeling and they're

00:18:45,720 --> 00:18:49,380
also in the paper so I won't read them

00:18:47,340 --> 00:18:53,130
out so these were the product

00:18:49,380 --> 00:18:54,000
requirements seven of them I'm not gonna

00:18:53,130 --> 00:18:58,350
read them through because I'm going to

00:18:54,000 --> 00:19:00,299
quickly go through them all first is in

00:18:58,350 --> 00:19:02,850
order to set this eesti one code point

00:19:00,299 --> 00:19:06,450
the sender must have this scalable

00:19:02,850 --> 00:19:08,159
behavior and it obviously applies for v6

00:19:06,450 --> 00:19:16,679
as well as before you've got the same

00:19:08,159 --> 00:19:18,809
ACN field and we put an RFC into to

00:19:16,679 --> 00:19:23,820
change so you could also do this with DC

00:19:18,809 --> 00:19:28,649
TCP but actually I'm not sure we have

00:19:23,820 --> 00:19:30,610
put there FCM 50t TCP yet but we intend

00:19:28,649 --> 00:19:33,740
to

00:19:30,610 --> 00:19:38,750
but for TCP Prague is default East t1

00:19:33,740 --> 00:19:40,400
all right and but you can change it if

00:19:38,750 --> 00:19:43,670
you want to if you're doing testing on a

00:19:40,400 --> 00:19:45,710
network where you want to for some

00:19:43,670 --> 00:19:49,720
reason you you need to label things

00:19:45,710 --> 00:19:49,720
differently on a test bit or something

00:19:50,140 --> 00:20:00,410
the next requirement is that you need

00:19:56,270 --> 00:20:05,080
accurate feedback of ACN in TCP because

00:20:00,410 --> 00:20:07,280
originally TCP only gave you at most one

00:20:05,080 --> 00:20:09,290
signal per round trip time and it's

00:20:07,280 --> 00:20:12,770
feedback even if you had multiple and

00:20:09,290 --> 00:20:15,110
you need this fine-grained Mia gave a

00:20:12,770 --> 00:20:17,180
talk on that led to have to point to

00:20:15,110 --> 00:20:20,270
this thing called accurate ecn I don't

00:20:17,180 --> 00:20:22,790
know if any of you were there but

00:20:20,270 --> 00:20:26,330
essentially it's a you can think of it a

00:20:22,790 --> 00:20:28,970
bit like the original TCP only gave you

00:20:26,330 --> 00:20:31,520
one drop signal per round trip time and

00:20:28,970 --> 00:20:33,440
then sack was added to get to get the

00:20:31,520 --> 00:20:36,980
actual drops in a round-trip time you

00:20:33,440 --> 00:20:38,750
can think of it like that and this has

00:20:36,980 --> 00:20:40,640
been going through the ITF for many

00:20:38,750 --> 00:20:42,050
years in fact I started on that in I

00:20:40,640 --> 00:20:48,230
looked it up the other day I think it

00:20:42,050 --> 00:20:49,610
was 2003 and that's hopefully coming up

00:20:48,230 --> 00:20:52,180
for working great last call but we have

00:20:49,610 --> 00:21:00,140
been saying that for quite a while

00:20:52,180 --> 00:21:08,810
it so it gives you every ACN mark back

00:21:00,140 --> 00:21:11,990
at the sender and I think I'm gonna have

00:21:08,810 --> 00:21:13,870
to skip over some of this one one one

00:21:11,990 --> 00:21:16,280
important thing there down the bottom

00:21:13,870 --> 00:21:17,840
that obviously depends on having

00:21:16,280 --> 00:21:20,030
accurate easy and on both ends if you

00:21:17,840 --> 00:21:24,850
want to do this for testing we've added

00:21:20,030 --> 00:21:28,670
a a non-default this control option to

00:21:24,850 --> 00:21:31,430
force the other end to give you feedback

00:21:28,670 --> 00:21:33,290
even though it's it hasn't got accurate

00:21:31,430 --> 00:21:38,060
ACN on it and we could we can do that

00:21:33,290 --> 00:21:42,419
using sort of faking the acknowledgment

00:21:38,060 --> 00:21:46,019
of the other ends ACN all the time

00:21:42,419 --> 00:21:48,359
so if as soon as it sends us a signal

00:21:46,019 --> 00:21:51,149
which it would it would repeat the whole

00:21:48,359 --> 00:21:53,970
round-trip time we turn it off straight

00:21:51,149 --> 00:21:56,850
away by sending the acknowledgment all

00:21:53,970 --> 00:22:00,149
the time so it's unreliable but at least

00:21:56,850 --> 00:22:08,879
it allows you to test it and you know

00:22:00,149 --> 00:22:12,779
use it so that I think I'll leave

00:22:08,879 --> 00:22:15,239
accuracy in there right the important

00:22:12,779 --> 00:22:20,940
one falling back to Reno friendly

00:22:15,239 --> 00:22:24,749
control behavior on the sender and so if

00:22:20,940 --> 00:22:26,669
you detect a loss that means you're

00:22:24,749 --> 00:22:28,379
probably not in an elf rescue you're

00:22:26,669 --> 00:22:30,330
probably running over your bottleneck is

00:22:28,379 --> 00:22:31,919
somewhere else obviously it may be a

00:22:30,330 --> 00:22:33,029
radio loss or whatever but that's the

00:22:31,919 --> 00:22:38,940
general problem you have with the

00:22:33,029 --> 00:22:41,429
internet so we've patched DCTC and we

00:22:38,940 --> 00:22:44,999
did submit this one RFC this one anyway

00:22:41,429 --> 00:22:46,470
and Larry for good reason says he

00:22:44,999 --> 00:22:48,690
doesn't want to do it quite like that in

00:22:46,470 --> 00:22:49,799
DC TTP but we're doing it like this and

00:22:48,690 --> 00:22:53,970
T's to be proud because it's for the

00:22:49,799 --> 00:22:56,850
public Internet but this patch was

00:22:53,970 --> 00:23:02,669
actually submitted as well two years ago

00:22:56,850 --> 00:23:06,809
for DC TCP but got lost because DC TCP

00:23:02,669 --> 00:23:09,330
actually doesn't respond to a loss if

00:23:06,809 --> 00:23:11,100
it's if it's a fast retransmit loss that

00:23:09,330 --> 00:23:15,840
and that is a bug that has to be sorted

00:23:11,100 --> 00:23:17,940
out somehow and we've sorted it out in

00:23:15,840 --> 00:23:23,070
TCP prog and Larry wants to look further

00:23:17,940 --> 00:23:25,549
at it for DC TCP so this is TCP only

00:23:23,070 --> 00:23:31,139
responds at the moment to a timeout loss

00:23:25,549 --> 00:23:36,029
and so there's a bit of work there but

00:23:31,139 --> 00:23:38,190
rightly Larry didn't didn't want to do

00:23:36,029 --> 00:23:40,440
what we're doing on the Internet because

00:23:38,190 --> 00:23:43,710
if you're in control of every cue in

00:23:40,440 --> 00:23:48,029
your in your data center your ECN

00:23:43,710 --> 00:23:50,960
marking will proceed a loss and so you

00:23:48,029 --> 00:23:50,960
don't want to do a full

00:23:52,650 --> 00:23:57,990
loss reduction as well as doing all the

00:23:55,800 --> 00:24:02,300
ecn marking reduction was on the public

00:23:57,990 --> 00:24:05,640
internet you can't control that all your

00:24:02,300 --> 00:24:09,360
bottlenecks are easy and capable so it's

00:24:05,640 --> 00:24:14,100
correct to do a proper harvick when you

00:24:09,360 --> 00:24:23,670
get a loss unless your PPR of course and

00:24:14,100 --> 00:24:28,470
then you do what you want but so the

00:24:23,670 --> 00:24:31,080
other one is what if you detect classic

00:24:28,470 --> 00:24:34,790
ECN at the bottleneck well it's actually

00:24:31,080 --> 00:24:37,050
quite hard to detect classic ecn

00:24:34,790 --> 00:24:39,030
effectively what you're having to detect

00:24:37,050 --> 00:24:42,450
is an increase in delay before you get

00:24:39,030 --> 00:24:44,310
or while you're getting ACN marking

00:24:42,450 --> 00:24:46,350
because if you don't get a increase in

00:24:44,310 --> 00:24:49,680
delay you're probably at a shallow

00:24:46,350 --> 00:24:52,830
threshold queue like the L 4sq that I

00:24:49,680 --> 00:24:58,590
talked about was if you do you're at a

00:24:52,830 --> 00:25:01,650
classic queue the at the moment we

00:24:58,590 --> 00:25:05,190
haven't put that in because as far as we

00:25:01,650 --> 00:25:07,500
know all the classic ecn on the Internet

00:25:05,190 --> 00:25:10,140
is fq coddled and so you've already got

00:25:07,500 --> 00:25:11,280
that isolation from other flows so you

00:25:10,140 --> 00:25:13,230
don't need to worry about being

00:25:11,280 --> 00:25:15,270
aggressive because you're your own FQ

00:25:13,230 --> 00:25:19,740
system which is making sure that you

00:25:15,270 --> 00:25:22,530
don't use other flows bandwidth but I

00:25:19,740 --> 00:25:25,700
mean we've done millions of measurements

00:25:22,530 --> 00:25:28,800
to try and find other seee

00:25:25,700 --> 00:25:33,120
generating cm in congestion exposure

00:25:28,800 --> 00:25:37,170
sorry congestion experienced on the

00:25:33,120 --> 00:25:38,640
internet and so far not found any but if

00:25:37,170 --> 00:25:40,170
we do obviously we'll put it in but we

00:25:38,640 --> 00:25:44,190
don't want to add complexity if it's not

00:25:40,170 --> 00:25:46,920
actually there so it's a potentially a

00:25:44,190 --> 00:25:50,340
non-required but we're willing to put

00:25:46,920 --> 00:25:53,130
that work in if if someone can find some

00:25:50,340 --> 00:25:54,960
ACN ACN routers on the Internet I was

00:25:53,130 --> 00:25:57,800
talking to tallest last night he thought

00:25:54,960 --> 00:25:57,800
there were some but

00:25:58,140 --> 00:26:13,320
Yeah right okay so that so the next

00:26:09,870 --> 00:26:16,460
requirement how are we doing for time I

00:26:13,320 --> 00:26:22,020
think I'm going to skip over this one

00:26:16,460 --> 00:26:24,240
and this is the PDF isn't it so these

00:26:22,020 --> 00:26:27,480
were all hidden slides that I would jump

00:26:24,240 --> 00:26:31,640
to if people ask questions because I've

00:26:27,480 --> 00:26:37,140
got time for these the the final one I

00:26:31,640 --> 00:26:41,299
think is detecting loss in units of time

00:26:37,140 --> 00:26:44,750
and so we're taking the opportunity to

00:26:41,299 --> 00:26:48,780
require man that's really require RAC or

00:26:44,750 --> 00:26:54,299
something like it and that means that

00:26:48,780 --> 00:26:55,830
the network can reduce its intolerance

00:26:54,299 --> 00:26:57,720
to reordering so it doesn't have to

00:26:55,830 --> 00:26:58,679
re-sequence and you can do that at the

00:26:57,720 --> 00:27:01,740
entice and there's been a long

00:26:58,679 --> 00:27:06,210
discussion of this on the lists and

00:27:01,740 --> 00:27:09,570
that's a that's a great advantage by

00:27:06,210 --> 00:27:11,429
having this separate queue it allows you

00:27:09,570 --> 00:27:14,010
to know that everything is using RAC

00:27:11,429 --> 00:27:15,600
rather than just some things are using

00:27:14,010 --> 00:27:17,520
RAC and all the old stuff isn't so you

00:27:15,600 --> 00:27:19,650
can you know that everything's being

00:27:17,520 --> 00:27:22,140
less tolerant less intolerant to

00:27:19,650 --> 00:27:28,290
reordering or more tolerant to

00:27:22,140 --> 00:27:30,150
reordering so that you can you don't

00:27:28,290 --> 00:27:31,710
have to keep everything in such strict

00:27:30,150 --> 00:27:34,169
order in the network which means you can

00:27:31,710 --> 00:27:35,580
make your switches go faster you can

00:27:34,169 --> 00:27:37,380
make your links go faster and everything

00:27:35,580 --> 00:27:40,110
cuz you haven't got that you know you

00:27:37,380 --> 00:27:41,730
can you can paralyze everything more and

00:27:40,110 --> 00:27:44,040
even if you get things a bit out of

00:27:41,730 --> 00:27:46,830
order as long as you're you're you're

00:27:44,040 --> 00:27:52,200
within this this time period that rack

00:27:46,830 --> 00:27:55,049
allows your alright now now these are

00:27:52,200 --> 00:27:58,500
the performance optimizations one is to

00:27:55,049 --> 00:28:01,140
put easier non control packets which TCP

00:27:58,500 --> 00:28:06,450
doesn't allow I want to try and allow

00:28:01,140 --> 00:28:09,270
time for questions so I'll just skip

00:28:06,450 --> 00:28:10,750
that one hour yeah that's this is

00:28:09,270 --> 00:28:16,480
probably the most important aspect

00:28:10,750 --> 00:28:19,650
to it in order when you let's go back

00:28:16,480 --> 00:28:24,520
one I need to explain this

00:28:19,650 --> 00:28:26,500
TTP when when ECM was added it was said

00:28:24,520 --> 00:28:29,140
you must not put easy and capability on

00:28:26,500 --> 00:28:35,860
the scene on control packets on pure

00:28:29,140 --> 00:28:40,180
axon cynic on fins resets all the rest

00:28:35,860 --> 00:28:42,190
of it and we went through all that and

00:28:40,180 --> 00:28:44,560
knocked down all the arguments it's

00:28:42,190 --> 00:28:47,320
going through the ITF now and it and

00:28:44,560 --> 00:28:49,000
there we've got this accuracy and

00:28:47,320 --> 00:28:52,240
feedback which gives you the feedback on

00:28:49,000 --> 00:28:53,260
all these packets so you can or we

00:28:52,240 --> 00:28:56,650
should be able to turn it on on

00:28:53,260 --> 00:28:59,920
everything if you've negotiated how

00:28:56,650 --> 00:29:04,870
crazy and feedback which is the thing I

00:28:59,920 --> 00:29:10,060
mentioned me I did and um Olivier posted

00:29:04,870 --> 00:29:13,060
that as part of the TCP prog thing

00:29:10,060 --> 00:29:14,590
yesterday so there's now a version of a

00:29:13,060 --> 00:29:20,110
QT and that's up to the tip of the

00:29:14,590 --> 00:29:24,790
mainline and there is a problem though

00:29:20,110 --> 00:29:33,450
with this that there was some code back

00:29:24,790 --> 00:29:36,610
in May 2012 put into the ecn part of TCP

00:29:33,450 --> 00:29:39,520
that was decided to take Network

00:29:36,610 --> 00:29:42,130
mangling and because the old behavior

00:29:39,520 --> 00:29:45,040
said you don't put easy and capability

00:29:42,130 --> 00:29:46,480
on the scene that was used as a test to

00:29:45,040 --> 00:29:49,210
see if there might be Network mangling

00:29:46,480 --> 00:29:51,370
and it and if there was easy to see on

00:29:49,210 --> 00:29:53,140
the scene it turned off ecn so now we

00:29:51,370 --> 00:29:56,560
want to put easier ATT on the scene

00:29:53,140 --> 00:29:58,540
we've got this problem but you you you

00:29:56,560 --> 00:29:59,500
hit your ACN server and it turns it off

00:29:58,540 --> 00:30:07,090
all right

00:29:59,500 --> 00:30:10,330
so oliviers also submitted a very simple

00:30:07,090 --> 00:30:11,760
patch for for net stable for that so

00:30:10,330 --> 00:30:16,030
that it will go out to all the existing

00:30:11,760 --> 00:30:18,640
install base of ecn servers so that it

00:30:16,030 --> 00:30:21,669
tests not just for

00:30:18,640 --> 00:30:25,120
in the TCP flags it doesn't just test

00:30:21,669 --> 00:30:27,029
for the two that say I want ACN it also

00:30:25,120 --> 00:30:30,669
test that the others are zero because

00:30:27,029 --> 00:30:32,080
when you're using accuracy and one of

00:30:30,669 --> 00:30:34,000
them will be one and when you're using

00:30:32,080 --> 00:30:35,860
anything in the future you'll probably

00:30:34,000 --> 00:30:38,529
have changed one of them so it just it

00:30:35,860 --> 00:30:42,100
makes that test more specific and

00:30:38,529 --> 00:30:44,710
hopefully that will get back ported

00:30:42,100 --> 00:30:47,080
fairly quickly and get out because it's

00:30:44,710 --> 00:30:51,190
so simple and we'll start to be able to

00:30:47,080 --> 00:30:57,399
use the existing install base of of ACN

00:30:51,190 --> 00:31:00,370
servers with ACN on the scene all right

00:30:57,399 --> 00:31:02,110
and there's a the next talk is about

00:31:00,370 --> 00:31:04,840
this so I won't cover this at all I just

00:31:02,110 --> 00:31:07,990
I just was doing this yoginis not very

00:31:04,840 --> 00:31:10,000
well but I think he's still okay to do

00:31:07,990 --> 00:31:13,659
the talk and then I think he's needs to

00:31:10,000 --> 00:31:15,820
go home it's about how you get a faster

00:31:13,659 --> 00:31:19,260
flow start how you get faster than

00:31:15,820 --> 00:31:24,360
additive increase and so the summary is

00:31:19,260 --> 00:31:24,360
that these were all the requirements and

00:31:25,019 --> 00:31:29,559
for those of you that are colorblind

00:31:27,240 --> 00:31:31,389
I've put in bold the ones that are on

00:31:29,559 --> 00:31:33,309
green compared to orange so that you can

00:31:31,389 --> 00:31:35,769
tell the difference because I had that

00:31:33,309 --> 00:31:39,880
feedback from someone who couldn't tell

00:31:35,769 --> 00:31:42,070
the green from the red and as you can

00:31:39,880 --> 00:31:45,460
see about half the requirements are in

00:31:42,070 --> 00:31:47,019
the base TCP stack and we've done most

00:31:45,460 --> 00:31:50,380
of them and nearly done

00:31:47,019 --> 00:31:53,559
the other one the in progress one I've

00:31:50,380 --> 00:31:57,070
got a well he's the code is available

00:31:53,559 --> 00:32:01,139
but we're still working on it and we

00:31:57,070 --> 00:32:01,139
didn't feel it was ready to put in yet

00:32:01,260 --> 00:32:07,559
some of them can potentially go into DT

00:32:04,210 --> 00:32:11,830
TCP but our main focus is TCP Prague and

00:32:07,559 --> 00:32:16,539
there's two there you see are in orange

00:32:11,830 --> 00:32:19,120
or red the reno friendly of classic easy

00:32:16,539 --> 00:32:23,080
and bottleneck is the one i said well

00:32:19,120 --> 00:32:25,049
put in if necessary and the reduce RTT

00:32:23,080 --> 00:32:28,900
dependence was the one I skipped over

00:32:25,049 --> 00:32:33,280
because we have only simulated it so

00:32:28,900 --> 00:32:39,760
I think oh yeah I suppose I ought to

00:32:33,280 --> 00:32:42,940
show you a performance plot so that's I

00:32:39,760 --> 00:32:44,340
just do yeah sorry pressing the wrong

00:32:42,940 --> 00:32:55,110
button again

00:32:44,340 --> 00:33:00,330
that's I need to start off by explaining

00:32:55,110 --> 00:33:02,020
the plot what we've done here is that

00:33:00,330 --> 00:33:05,080
log scale

00:33:02,020 --> 00:33:07,450
it's a CDF cumulative distribution

00:33:05,080 --> 00:33:12,190
function to show you the delay and that

00:33:07,450 --> 00:33:14,050
this says 99% of the packets have better

00:33:12,190 --> 00:33:19,450
than Mach delay not nine they're better

00:33:14,050 --> 00:33:22,660
than that and so on and these are all we

00:33:19,450 --> 00:33:25,090
are the congestion controls in the

00:33:22,660 --> 00:33:27,820
interesting one is the red one which is

00:33:25,090 --> 00:33:30,309
the classic queue that's working

00:33:27,820 --> 00:33:33,220
alongside the blue one in that dual

00:33:30,309 --> 00:33:36,520
queue system and the aim of this part is

00:33:33,220 --> 00:33:40,260
to show that it's no worse than high

00:33:36,520 --> 00:33:40,260
wires I mean it's a bit worse than me

00:33:48,679 --> 00:34:27,690
for all the control because there are so

00:34:22,169 --> 00:34:29,669
there and this is a this is a real

00:34:27,690 --> 00:34:33,030
hammering load so we're getting two

00:34:29,669 --> 00:34:35,310
milliseconds of the 99% are here the

00:34:33,030 --> 00:34:38,730
median at the 50th percentile it's about

00:34:35,310 --> 00:34:40,860
100 to 200 microseconds on the public

00:34:38,730 --> 00:34:42,510
Internet this is you know this we're

00:34:40,860 --> 00:34:44,609
talking now microseconds of queuing

00:34:42,510 --> 00:34:51,240
delay on the public Internet not not

00:34:44,609 --> 00:34:52,710
even milliseconds okay pretty close two

00:34:51,240 --> 00:34:55,379
milliseconds median delighted to

00:34:52,710 --> 00:34:57,080
something like that but it's the

00:34:55,379 --> 00:35:01,590
percentiles that are important when you

00:34:57,080 --> 00:35:04,800
need to do real-time media because the

00:35:01,590 --> 00:35:06,570
buffering sort of expands to the level

00:35:04,800 --> 00:35:07,950
of delay and that's what gives you you

00:35:06,570 --> 00:35:11,130
know if you've got to do interactive

00:35:07,950 --> 00:35:12,450
real-time media and you know control

00:35:11,130 --> 00:35:16,730
using video and all the rest of it you

00:35:12,450 --> 00:35:16,730
need to get the higher percentiles down

00:35:18,230 --> 00:35:28,470
so I think yet and there's the the spec

00:35:26,160 --> 00:35:30,900
of the traffic here I can go through

00:35:28,470 --> 00:35:32,970
loads loads more traffic's and eyes

00:35:30,900 --> 00:35:38,480
we've got millions of them and I mean

00:35:32,970 --> 00:35:43,020
millions and I'm aware of it

00:35:38,480 --> 00:35:45,300
Davis is unable or feels unable to test

00:35:43,020 --> 00:35:48,470
this himself and that's that's a shame

00:35:45,300 --> 00:35:50,820
because this this does need to be tested

00:35:48,470 --> 00:35:52,140
you know by other people and all this

00:35:50,820 --> 00:35:56,570
needs to be validated by other

00:35:52,140 --> 00:36:00,540
people but I'm hoping it will prove to

00:35:56,570 --> 00:36:02,430
be as good as well I mean as such it's

00:36:00,540 --> 00:36:05,610
very difficult to get queuing delay you

00:36:02,430 --> 00:36:10,200
know it's and and I'm hoping you'll find

00:36:05,610 --> 00:36:12,000
that so the summary is you've got

00:36:10,200 --> 00:36:13,070
frequent these frequent markings are

00:36:12,000 --> 00:36:15,720
what gives you the leap in performance

00:36:13,070 --> 00:36:20,280
get your low latency your low loss and

00:36:15,720 --> 00:36:22,680
your scalable throughput and importantly

00:36:20,280 --> 00:36:23,940
it's a set of incremental changes to the

00:36:22,680 --> 00:36:27,920
network into hosts that we're not doing

00:36:23,940 --> 00:36:32,700
anything particularly radical here and

00:36:27,920 --> 00:36:37,970
we do now have a patch out for RFC

00:36:32,700 --> 00:36:44,450
finally for TTP prog and the other one

00:36:37,970 --> 00:36:44,450
okay Dave

00:36:52,390 --> 00:36:57,680
voice is going so I just have two quick

00:36:55,580 --> 00:36:59,900
questions of the audience and then a

00:36:57,680 --> 00:37:03,710
statement how many here really

00:36:59,900 --> 00:37:06,380
understand what ecn is the markings in

00:37:03,710 --> 00:37:08,930
the header the classic ecn behavior

00:37:06,380 --> 00:37:11,210
defined by RFC and I get a raising of

00:37:08,930 --> 00:37:14,480
hands that people understand how easy

00:37:11,210 --> 00:37:16,369
and supposed to work so for everybody

00:37:14,480 --> 00:37:18,800
else in this room that even the concept

00:37:16,369 --> 00:37:20,510
of ecn is foreign and that means that

00:37:18,800 --> 00:37:22,730
the debate that was been raging on the

00:37:20,510 --> 00:37:25,550
buffer bloat net bloat mailing list is

00:37:22,730 --> 00:37:27,470
incomprehensible to most people and I

00:37:25,550 --> 00:37:29,690
would really encourage people to read

00:37:27,470 --> 00:37:31,580
the elf for us and the GCP Prague and

00:37:29,690 --> 00:37:33,820
the other requirements while getting a

00:37:31,580 --> 00:37:36,020
deep understanding of how ACN works

00:37:33,820 --> 00:37:39,740
second question is how many people here

00:37:36,020 --> 00:37:40,700
have heard of buffer bloat net hey wow

00:37:39,740 --> 00:37:43,940
that's more people that have heard of

00:37:40,700 --> 00:37:45,589
ecn so we've been a group working with

00:37:43,940 --> 00:37:47,359
academic researchers all over the world

00:37:45,589 --> 00:37:49,820
so all volunteers have been some great

00:37:47,359 --> 00:37:52,970
contributors of things like to Eric

00:37:49,820 --> 00:37:55,130
Dumas A's for example like TCP he did FQ

00:37:52,970 --> 00:37:56,780
Caudill that's our highlight thing which

00:37:55,130 --> 00:37:59,930
is now deployed at a hundred percent or

00:37:56,780 --> 00:38:01,609
so so if you look at that debate and you

00:37:59,930 --> 00:38:05,540
look at this wonderful slide where you

00:38:01,609 --> 00:38:07,490
have to also get that there's three

00:38:05,540 --> 00:38:09,320
requirements to l4s that are really

00:38:07,490 --> 00:38:11,390
problematic one of which is it

00:38:09,320 --> 00:38:14,780
repurposes the last bit and the IP

00:38:11,390 --> 00:38:19,010
header for one specific congestion

00:38:14,780 --> 00:38:23,750
control DC TCP not bbr not cubic not

00:38:19,010 --> 00:38:26,780
product requires a patented under friend

00:38:23,750 --> 00:38:29,480
terms aq up which is an up until now all

00:38:26,780 --> 00:38:31,940
the benchmarks and all the source code

00:38:29,480 --> 00:38:34,190
have been proprietary and impossible for

00:38:31,940 --> 00:38:36,440
my group to reproduce I have been

00:38:34,190 --> 00:38:37,970
welcoming the fact that code is finally

00:38:36,440 --> 00:38:39,890
landing and I am looking forward to

00:38:37,970 --> 00:38:42,520
being able to do a full evaluation of it

00:38:39,890 --> 00:38:44,990
in the coming weeks

00:38:42,520 --> 00:38:47,960
I must just jump in there and say

00:38:44,990 --> 00:38:51,380
the code was released under GPL veto in

00:38:47,960 --> 00:38:53,839
July 2016 but you have never used it I

00:38:51,380 --> 00:38:55,550
have because of the family once now

00:38:53,839 --> 00:38:57,080
because of the Frant patent the fact

00:38:55,550 --> 00:38:59,150
that we are competitors I can't touch

00:38:57,080 --> 00:39:00,410
the patent I'm hoping someone else in

00:38:59,150 --> 00:39:01,910
this room is willing to go touch

00:39:00,410 --> 00:39:04,070
patented code in order to be able to

00:39:01,910 --> 00:39:07,760
give me a black box and my team can

00:39:04,070 --> 00:39:09,440
evaluate it so anyway one last thing

00:39:07,760 --> 00:39:11,390
buffer block dotnet form their own

00:39:09,440 --> 00:39:12,859
working group it's called East en Seine

00:39:11,390 --> 00:39:15,260
due to many things

00:39:12,859 --> 00:39:16,849
and that's a link all of our documents

00:39:15,260 --> 00:39:19,640
are linked to off the Buffalo net

00:39:16,849 --> 00:39:21,830
website along with our charter a means

00:39:19,640 --> 00:39:23,960
of operation and the source code we're

00:39:21,830 --> 00:39:25,910
landing to a competing proposal called

00:39:23,960 --> 00:39:28,940
some congestion experienced which we

00:39:25,910 --> 00:39:31,700
hope to discuss at the next IETF encode

00:39:28,940 --> 00:39:33,880
is available for that as well thank you

00:39:31,700 --> 00:39:33,880
Bob

00:39:52,860 --> 00:40:02,430
so one of the issue of the RFC is 3168

00:39:57,270 --> 00:40:09,400
en was that it could be abused by people

00:40:02,430 --> 00:40:15,300
pretending to use what prevents anybody

00:40:09,400 --> 00:40:18,580
to use ECT wonders oh I'm good okay so

00:40:15,300 --> 00:40:21,930
in the in the DOCSIS version and and

00:40:18,580 --> 00:40:24,700
this isn't in the linux version yet

00:40:21,930 --> 00:40:27,370
we've put a queue protection function in

00:40:24,700 --> 00:40:30,820
front of the l4s queue so that anyone

00:40:27,370 --> 00:40:32,260
causing hi delay any flow causing higher

00:40:30,820 --> 00:40:34,980
delay he gets kicked into the other

00:40:32,260 --> 00:40:40,950
queue and and that can catch packets

00:40:34,980 --> 00:40:44,740
after one or two packets of a flow but

00:40:40,950 --> 00:40:48,700
the I mean the ECM that's currently in

00:40:44,740 --> 00:40:54,130
fq coddle obviously you've got the the

00:40:48,700 --> 00:40:55,630
flow protection in there but the the

00:40:54,130 --> 00:40:59,560
other thing that you you've got

00:40:55,630 --> 00:41:01,990
protection on in the dual queue is that

00:40:59,560 --> 00:41:06,550
when you saw the the priority scheduler

00:41:01,990 --> 00:41:08,290
there it said that the it's a

00:41:06,550 --> 00:41:11,050
conditional priority scheduler so it

00:41:08,290 --> 00:41:13,720
allows the lower queue to have some

00:41:11,050 --> 00:41:15,490
priority whatever whatever the other key

00:41:13,720 --> 00:41:18,850
is and there's two different registers

00:41:15,490 --> 00:41:20,590
for doing that in the in the code as two

00:41:18,850 --> 00:41:23,290
examples

00:41:20,590 --> 00:41:24,970
that's weighted round robin with a with

00:41:23,290 --> 00:41:30,280
a high wait for the other one and I

00:41:24,970 --> 00:41:37,780
think all the time shifted FIFO now the

00:41:30,280 --> 00:41:41,800
so the other aspect of that is that any

00:41:37,780 --> 00:41:43,930
ecn flow you know that problem we did a

00:41:41,800 --> 00:41:45,880
lot of experiments with with overload

00:41:43,930 --> 00:41:47,740
you know it's like dos attacks using ACN

00:41:45,880 --> 00:41:49,320
and things like that to show that you

00:41:47,740 --> 00:41:54,190
couldn't actually get worse performance

00:41:49,320 --> 00:41:57,490
than if you were using non ecn because

00:41:54,190 --> 00:42:01,840
the queue turns off easier and when it

00:41:57,490 --> 00:42:03,700
gets when it gets overloaded and and so

00:42:01,840 --> 00:42:08,770
you you can't get it

00:42:03,700 --> 00:42:12,040
both overload and and without it turning

00:42:08,770 --> 00:42:13,930
off ecn so it's it's it's it starts

00:42:12,040 --> 00:42:17,260
ignoring AC and if you start using it an

00:42:13,930 --> 00:42:19,540
attack and we we did experiments that

00:42:17,260 --> 00:42:22,660
show they're just a very small window

00:42:19,540 --> 00:42:26,849
when you can get slightly better attack

00:42:22,660 --> 00:42:30,570
force than not using ACN before the

00:42:26,849 --> 00:42:30,570
Machine asked to turn it off

00:42:43,400 --> 00:42:50,219
how do you about to cannibalize

00:42:47,630 --> 00:42:52,589
individuals inside the high priority

00:42:50,219 --> 00:42:56,339
queue because I mean there you have

00:42:52,589 --> 00:42:59,219
knowing kind of I didn't get the verb

00:42:56,339 --> 00:43:02,759
how do you had you boy cannibalizing

00:42:59,219 --> 00:43:04,680
cannibalized resident of versus the

00:43:02,759 --> 00:43:07,859
other because it can it may happen that

00:43:04,680 --> 00:43:10,469
you have two individuals with different

00:43:07,859 --> 00:43:14,519
are today inside the inside the class a

00:43:10,469 --> 00:43:17,940
high priority class okay so I assume

00:43:14,519 --> 00:43:20,099
that one might I mean you're you're not

00:43:17,940 --> 00:43:23,069
having firmness inside the class you

00:43:20,099 --> 00:43:26,819
have fairness between two classes yeah

00:43:23,069 --> 00:43:29,670
but that um I mean inside the class it's

00:43:26,819 --> 00:43:31,259
just the same as multiple TCP flows or

00:43:29,670 --> 00:43:33,869
real time flows that like you having a

00:43:31,259 --> 00:43:36,269
FIFO at the moment I mean that that's

00:43:33,869 --> 00:43:41,329
how the internet works at the moment

00:43:36,269 --> 00:43:47,009
right okay so you have no I mean

00:43:41,329 --> 00:43:51,180
congestion has some count some somehow

00:43:47,009 --> 00:43:53,880
signaling only for I mean if if there's

00:43:51,180 --> 00:43:58,079
a individual inside the high priority

00:43:53,880 --> 00:43:59,999
class but as a smaller oddity it can

00:43:58,079 --> 00:44:03,839
take advantage of the others which are

00:43:59,999 --> 00:44:05,459
in this in the same class yeah yeah like

00:44:03,839 --> 00:44:09,239
the internet does today if you've got a

00:44:05,459 --> 00:44:11,910
smaller are TT TT P goes faster but the

00:44:09,239 --> 00:44:13,920
the round-trip time you dependence we

00:44:11,910 --> 00:44:17,519
yet we have done simulations but this

00:44:13,920 --> 00:44:19,349
isn't in the code yet of a of a TCP

00:44:17,519 --> 00:44:21,869
product is less dependent on round-trip

00:44:19,349 --> 00:44:23,969
time but but otherwise what you're

00:44:21,869 --> 00:44:25,589
saying is the same today it is it is

00:44:23,969 --> 00:44:27,119
different tonight I could go through

00:44:25,589 --> 00:44:33,619
that side but I noticed we're we're way

00:44:27,119 --> 00:44:38,489
light and the the difference is that

00:44:33,619 --> 00:44:40,739
without a queue you're you've got much

00:44:38,489 --> 00:44:42,829
less of a cushion so let me try and

00:44:40,739 --> 00:44:42,829
explain

00:44:43,460 --> 00:44:53,390
with um I'll try and explain if you've

00:44:51,110 --> 00:44:55,040
got no queue and you've got like a two

00:44:53,390 --> 00:44:56,960
millisecond and a 200 milliseconds

00:44:55,040 --> 00:44:58,280
round-trip time competing yen to each

00:44:56,960 --> 00:45:00,500
other that one will go a hundred times

00:44:58,280 --> 00:45:05,690
so this one would go 100 times faster

00:45:00,500 --> 00:45:08,000
because it's 200 over - yeah but if

00:45:05,690 --> 00:45:11,180
you've got a queue of say 100

00:45:08,000 --> 00:45:13,250
milliseconds you then got 102 against

00:45:11,180 --> 00:45:17,150
300 and so there only one two three

00:45:13,250 --> 00:45:18,350
different right and and so when you have

00:45:17,150 --> 00:45:20,600
got a queue yes you've got more of a

00:45:18,350 --> 00:45:24,290
problem and that's why we wear with

00:45:20,600 --> 00:45:27,560
being working on a an an on our TT

00:45:24,290 --> 00:45:29,870
dependent congestion control but

00:45:27,560 --> 00:45:33,310
otherwise it's pretty much like the

00:45:29,870 --> 00:45:33,310
current internet problem

00:45:40,089 --> 00:45:48,729
okay you want to cut sorry

00:45:43,989 --> 00:45:51,339
yeah we're 15 minutes over yeah yo Keem

00:45:48,729 --> 00:45:53,289
still well enough to give his talk yeah

00:45:51,339 --> 00:45:55,719
while he's coming up I just wanted to

00:45:53,289 --> 00:45:57,849
respond to Dave and say that you know

00:45:55,719 --> 00:45:59,559
we've been working on this and working

00:45:57,849 --> 00:46:06,999
through the all the itea crisis to get

00:45:59,559 --> 00:46:09,219
this code point now since 2015 and you

00:46:06,999 --> 00:46:13,410
know you you have to engage in that

00:46:09,219 --> 00:46:13,410
process no matter how painful it is but

00:46:14,219 --> 00:46:20,529
yeah and we since well not on this

00:46:18,939 --> 00:46:25,299
subject on SCE you've been shipping

00:46:20,529 --> 00:46:27,430
running codes into two days ago here

00:46:25,299 --> 00:46:30,630
we've all been shipping code you know

00:46:27,430 --> 00:46:33,630
since some year about something but not

00:46:30,630 --> 00:46:33,630

YouTube URL: https://www.youtube.com/watch?v=GyXwvRKW0QE


