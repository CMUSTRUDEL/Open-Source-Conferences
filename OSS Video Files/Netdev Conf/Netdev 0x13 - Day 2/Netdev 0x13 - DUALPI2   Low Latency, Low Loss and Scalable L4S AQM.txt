Title: Netdev 0x13 - DUALPI2   Low Latency, Low Loss and Scalable L4S AQM
Publication date: 2019-05-25
Playlist: Netdev 0x13 - Day 2
Description: 
	Olga Albisser describes the DUALPI2 AQM qdisc. DualPI2 AQM is part of the IETF L4S infrastructure standardization. The primary goal of DUALPI2 AQM is to provide ability
to deploy congestion controls like DCTCP (which is traditionaly deployed in data centres)
on the internet.

In this talk, Olga  presents the details of DUALPI2 implementation and explain problems
it solves.

More info:
https://netdevconf.org/0x13/session.html?talk-DUALPI2-AQM
Captions: 
	00:00:00,000 --> 00:00:05,960
my name is dongho I'm going to talk

00:00:02,520 --> 00:00:11,960
about two pi squared icon which is

00:00:05,960 --> 00:00:11,960
mechanism for low latency low loss and

00:00:12,180 --> 00:00:15,310
[Music]

00:00:17,340 --> 00:00:20,410
[Music]

00:00:20,510 --> 00:00:27,930
which you might have attended so we're

00:00:25,859 --> 00:00:32,940
going to start with the motivation for

00:00:27,930 --> 00:00:49,010
this work the motivation is that low

00:00:32,940 --> 00:00:52,020
latency is a critical factor so it was

00:00:49,010 --> 00:00:54,960
common to believe not a long time ago

00:00:52,020 --> 00:00:59,399
that interactive applications such as

00:00:54,960 --> 00:01:03,210
finance apps online gaming most desktop

00:00:59,399 --> 00:01:07,140
applications are that they require low

00:01:03,210 --> 00:01:10,020
latency but also low bandwidth because

00:01:07,140 --> 00:01:12,330
such applications often use compression

00:01:10,020 --> 00:01:15,930
to reduce the amount of traffic that is

00:01:12,330 --> 00:01:19,189
sound but with the latest advances and

00:01:15,930 --> 00:01:22,890
graphics for the same applications and

00:01:19,189 --> 00:01:27,380
the increased popularity of video

00:01:22,890 --> 00:01:27,380
streaming virtual reality applications

00:01:28,000 --> 00:01:34,330
we can say now that interactive low

00:01:30,970 --> 00:01:36,730
latency applications are require not

00:01:34,330 --> 00:01:45,370
only low latency but also high

00:01:36,730 --> 00:01:49,750
throughput and while it was believed

00:01:45,370 --> 00:01:53,790
that low latency and high throughput are

00:01:49,750 --> 00:01:56,200
not achievable at the same time so

00:01:53,790 --> 00:01:59,920
latency is a very complex problem that

00:01:56,200 --> 00:02:04,270
needs to be addressed both and systems

00:01:59,920 --> 00:02:07,420
and in the network and why one reason

00:02:04,270 --> 00:02:11,800
why it was believed that low latency

00:02:07,420 --> 00:02:16,720
cannot be achieved with high throughput

00:02:11,800 --> 00:02:21,130
is because classic TCP requires large

00:02:16,720 --> 00:02:22,990
queues to keep the limitation high so

00:02:21,130 --> 00:02:26,860
you can see here that the capacity

00:02:22,990 --> 00:02:31,300
seeking sorties they need some buffer to

00:02:26,860 --> 00:02:34,150
keep the linkage utilization high and if

00:02:31,300 --> 00:02:38,730
we have a network bottleneck with tell

00:02:34,150 --> 00:02:43,780
job and noachian this becomes quite bad

00:02:38,730 --> 00:02:45,530
so we have bloated buffers and quite bad

00:02:43,780 --> 00:02:49,460
performance

00:02:45,530 --> 00:02:54,920
and if we use a chiamo it gets better

00:02:49,460 --> 00:02:59,900
and then we still have some buffering

00:02:54,920 --> 00:03:03,680
but the to keep the link association

00:02:59,900 --> 00:03:05,540
high but the delay is still large so the

00:03:03,680 --> 00:03:07,490
trick is to use the scout book

00:03:05,540 --> 00:03:10,250
congestion control with finer so teeth

00:03:07,490 --> 00:03:13,459
and you can refer to Bob's previous talk

00:03:10,250 --> 00:03:16,300
about the details on that

00:03:13,459 --> 00:03:21,770
so the Scout book congestion control

00:03:16,300 --> 00:03:23,959
would achieve low latency but the

00:03:21,770 --> 00:03:27,830
problem is it's too aggressive to use to

00:03:23,959 --> 00:03:30,200
be used on the public Internet so it

00:03:27,830 --> 00:03:31,880
cannot be deployed because it will not

00:03:30,200 --> 00:03:35,900
be able to convince those classic

00:03:31,880 --> 00:03:39,680
congestion controls and I would like to

00:03:35,900 --> 00:03:42,050
give you a small preview of the results

00:03:39,680 --> 00:03:44,410
that we have achieved with to a pie

00:03:42,050 --> 00:03:44,410
squared

00:03:44,500 --> 00:03:50,260
and on this plot you can see the result

00:03:48,430 --> 00:03:52,480
for the same experiment that we did

00:03:50,260 --> 00:03:55,180
three times for each of the eight games

00:03:52,480 --> 00:04:01,380
and if you pay attention to the first

00:03:55,180 --> 00:04:04,060
plot you can see that for s traffic with

00:04:01,380 --> 00:04:07,750
which is uses callable congestion

00:04:04,060 --> 00:04:11,740
control achieves very low queuing delay

00:04:07,750 --> 00:04:13,810
i was dope pi squared so it's near zero

00:04:11,740 --> 00:04:16,510
for 50% of the packets and it's less

00:04:13,810 --> 00:04:20,230
than one millisecond for 90% of the

00:04:16,510 --> 00:04:23,320
packets so the results are quite

00:04:20,230 --> 00:04:28,180
impressive and I'll get back to a bit

00:04:23,320 --> 00:04:31,840
more detailed evaluation later so to

00:04:28,180 --> 00:04:36,570
summarize the main goals that we have

00:04:31,840 --> 00:04:40,210
with your PI squared ikm we can simplify

00:04:36,570 --> 00:04:43,300
down to two points the first point is

00:04:40,210 --> 00:04:46,810
isolation of alpha rest traffic from

00:04:43,300 --> 00:04:50,230
classic traffic and the second goal is

00:04:46,810 --> 00:04:53,080
to make the coexistence of those two

00:04:50,230 --> 00:04:56,710
types of traffic possible so to solve

00:04:53,080 --> 00:04:58,960
the first goal first problem we used two

00:04:56,710 --> 00:05:02,220
different queues and each one of them

00:04:58,960 --> 00:05:04,720
has post native and couple taken and

00:05:02,220 --> 00:05:06,910
then to make the coexistence between

00:05:04,720 --> 00:05:09,760
this two type of traffic possible we

00:05:06,910 --> 00:05:13,740
apply different signal intensity for

00:05:09,760 --> 00:05:16,930
each types of the traffic so we use

00:05:13,740 --> 00:05:19,840
higher single intensity for more

00:05:16,930 --> 00:05:21,960
aggressive traffic and lower

00:05:19,840 --> 00:05:27,460
signal intensity for the other traffic

00:05:21,960 --> 00:05:30,340
and this figure again shows in a more

00:05:27,460 --> 00:05:33,760
simplified form how the egg-cam works we

00:05:30,340 --> 00:05:36,970
have two types of traffic and we use the

00:05:33,760 --> 00:05:39,970
AC and classifier to decide which cue

00:05:36,970 --> 00:05:42,940
each type of traffic is placed into and

00:05:39,970 --> 00:05:46,540
then each of the queues has both native

00:05:42,940 --> 00:05:48,520
Cyclorama that ensures that each queue

00:05:46,540 --> 00:05:51,760
gets scheduled even if there is no

00:05:48,520 --> 00:05:53,530
traffic in the other queue and then we

00:05:51,760 --> 00:05:56,860
have a couple that p.m. which applies

00:05:53,530 --> 00:06:02,590
higher signal probability for Alfa

00:05:56,860 --> 00:06:04,919
rescue which is more aggressive more

00:06:02,590 --> 00:06:10,690
aggressive and then we have lower

00:06:04,919 --> 00:06:15,580
probability for the classic q and just

00:06:10,690 --> 00:06:20,349
to show you how it works

00:06:15,580 --> 00:06:23,949
in real-time I'm going to show a small

00:06:20,349 --> 00:06:33,580
demo and I hope this works because I'm

00:06:23,949 --> 00:06:38,380
gonna do that this remotely of course so

00:06:33,580 --> 00:06:42,430
the internet the Wi-Fi is not that fast

00:06:38,380 --> 00:06:45,870
here so I hope you can be patient with

00:06:42,430 --> 00:06:45,870
this can I get

00:06:52,840 --> 00:06:59,320
so to me to see the results of the job

00:06:57,430 --> 00:07:03,820
path quiet performance compared to other

00:06:59,320 --> 00:07:07,150
eye creams we created small graphical

00:07:03,820 --> 00:07:08,770
interface this one is running now at a

00:07:07,150 --> 00:07:13,120
spot where we have two sanders and two

00:07:08,770 --> 00:07:14,770
receivers and a gram note in between so

00:07:13,120 --> 00:07:17,110
on each of the sender and receiver

00:07:14,770 --> 00:07:20,230
players we can use different congestion

00:07:17,110 --> 00:07:22,419
controls so this is first sender and

00:07:20,230 --> 00:07:25,510
receiver player and this is second and

00:07:22,419 --> 00:07:27,790
the first one is using now DC TCP the

00:07:25,510 --> 00:07:30,820
other one is using cubic and have drop I

00:07:27,790 --> 00:07:36,900
squared now certain take am note so here

00:07:30,820 --> 00:07:36,900
I can start now some greedy flows

00:07:37,550 --> 00:07:45,659
which are just simulated by a client

00:07:42,960 --> 00:07:49,620
that we use that sounds traffic from

00:07:45,659 --> 00:07:53,659
sender to receiver and here you can see

00:07:49,620 --> 00:07:56,659
the rate for each of the flows and for

00:07:53,659 --> 00:07:59,669
the two types of traffic will focus on

00:07:56,659 --> 00:08:01,650
window fairness and not rate furnace so

00:07:59,669 --> 00:08:05,129
I'm going to switch it into window mode

00:08:01,650 --> 00:08:07,530
so now it shows the fare window which we

00:08:05,129 --> 00:08:11,430
calculate based on the link speed and

00:08:07,530 --> 00:08:13,919
the total number of flows and now you

00:08:11,430 --> 00:08:17,159
can see the proportion of the fare

00:08:13,919 --> 00:08:22,080
window which is close to hundred percent

00:08:17,159 --> 00:08:25,560
or close to one so you can see that each

00:08:22,080 --> 00:08:30,210
flow gets a rather fair share of the

00:08:25,560 --> 00:08:33,899
link and at the same time we have near

00:08:30,210 --> 00:08:36,209
zero latency for the alpha rescue so

00:08:33,899 --> 00:08:39,240
that that part shows the queuing delay

00:08:36,209 --> 00:08:40,860
and this is Q&A for the alpha rest

00:08:39,240 --> 00:08:44,219
traffic and this is that queuing delay

00:08:40,860 --> 00:08:46,620
for the classic traffic so you can see

00:08:44,219 --> 00:08:49,820
the average current array and the 99th

00:08:46,620 --> 00:08:54,329
percentile so the 99th percentile is

00:08:49,820 --> 00:08:56,579
almost at one millisecond and the

00:08:54,329 --> 00:08:59,399
average is below one millisecond so

00:08:56,579 --> 00:09:03,480
close to zero now we can switch between

00:08:59,399 --> 00:09:05,870
different accounts so we can set PI for

00:09:03,480 --> 00:09:05,870
example

00:09:05,990 --> 00:09:15,580
now why is not month work with this is B

00:09:09,920 --> 00:09:15,580
so I can switch to cubic cm on the first

00:09:16,120 --> 00:09:23,029
server and client pair and now you can

00:09:21,110 --> 00:09:27,050
compare the queuing delay which is now

00:09:23,029 --> 00:09:29,209
at roughly 20 milliseconds which seconds

00:09:27,050 --> 00:09:35,709
for both types of traffic just the

00:09:29,209 --> 00:09:41,240
target for pi and we can also try with

00:09:35,709 --> 00:09:48,350
fq cold pool that's again same setup

00:09:41,240 --> 00:09:54,110
just that I changed the second take em

00:09:48,350 --> 00:09:56,690
note now the queuing delays at roughly 7

00:09:54,110 --> 00:09:59,899
milliseconds on average and the 99th

00:09:56,690 --> 00:10:04,070
percentile is at Thomas milliseconds I

00:09:59,899 --> 00:10:09,829
can also start some short flows to see

00:10:04,070 --> 00:10:15,350
the completion times so here

00:10:09,829 --> 00:10:21,050
we can cancel - 100 requests per second

00:10:15,350 --> 00:10:25,429
on average and now I can switch back to

00:10:21,050 --> 00:10:27,679
PI so you can can see the difference

00:10:25,429 --> 00:10:35,869
together the completion times become a

00:10:27,679 --> 00:10:39,999
bit larger and now if I switch back to -

00:10:35,869 --> 00:10:39,999
all PI squared with this DCP

00:10:50,630 --> 00:10:57,440
now you can see the queuing delay has

00:10:54,019 --> 00:11:01,910
switched back to near zero and the

00:10:57,440 --> 00:11:04,699
completion times are also getting

00:11:01,910 --> 00:11:06,709
smaller it's probably a little bit hard

00:11:04,699 --> 00:11:10,490
to see but I'm I can clear that and then

00:11:06,709 --> 00:11:14,029
you can see that they on the x-axis you

00:11:10,490 --> 00:11:17,990
there is a flow sites which starts at 1

00:11:14,029 --> 00:11:21,250
byte and approximately and that 1

00:11:17,990 --> 00:11:25,130
megabyte so the completion time our

00:11:21,250 --> 00:11:29,620
completion times are approximately one

00:11:25,130 --> 00:11:33,880
second for the largest flows and 0.01

00:11:29,620 --> 00:11:39,570
plus seconds for short shorter flows

00:11:33,880 --> 00:11:42,600
this is just to show how

00:11:39,570 --> 00:11:45,930
the results look like in real time we

00:11:42,600 --> 00:11:47,520
have here also marking probability and

00:11:45,930 --> 00:11:50,070
dropping probability for each of the

00:11:47,520 --> 00:11:53,190
queues so you can see that the marking

00:11:50,070 --> 00:11:59,520
probability is significantly higher for

00:11:53,190 --> 00:12:02,640
the afresh queue and for the classic you

00:11:59,520 --> 00:12:05,700
it's quite low and you can also see the

00:12:02,640 --> 00:12:09,830
link utilization which is that 100

00:12:05,700 --> 00:12:12,909
doesn't really change at the moment

00:12:09,830 --> 00:12:12,909
[Music]

00:12:14,870 --> 00:12:21,620
switch back to one of the other ATMs and

00:12:18,620 --> 00:12:24,590
then you can see that the difference

00:12:21,620 --> 00:12:27,730
between marking and dropping probability

00:12:24,590 --> 00:12:29,870
for two types of traffic will become

00:12:27,730 --> 00:12:33,129
almost the same

00:12:29,870 --> 00:12:33,129
[Music]

00:12:36,430 --> 00:12:44,200
I think that's all for the live demo

00:12:40,240 --> 00:12:46,570
this was primarily to show you that it

00:12:44,200 --> 00:12:51,339
works and we get very low queuing delay

00:12:46,570 --> 00:12:56,520
and here we can switch things in real

00:12:51,339 --> 00:12:56,520
time and see the results immediately

00:12:58,240 --> 00:13:03,509
so maybe I can get back to the slides

00:13:12,800 --> 00:13:18,380
so I wasn't going into too much details

00:13:15,610 --> 00:13:20,600
about

00:13:18,380 --> 00:13:23,390
how do i pi squared is implemented

00:13:20,600 --> 00:13:26,000
because I think that is going to take a

00:13:23,390 --> 00:13:30,830
lot of time and I can answer questions

00:13:26,000 --> 00:13:32,870
later if there are some and the main

00:13:30,830 --> 00:13:35,150
thing that I would like to focus on is

00:13:32,870 --> 00:13:35,840
how to use to all PI squared and how to

00:13:35,150 --> 00:13:39,800
deploy it

00:13:35,840 --> 00:13:44,810
so the first deployment scenario that we

00:13:39,800 --> 00:13:48,890
target is an internet path and primarily

00:13:44,810 --> 00:13:52,520
downstream part and the last access

00:13:48,890 --> 00:13:55,850
router and other deployment scenario is

00:13:52,520 --> 00:13:58,790
a data center where for example both

00:13:55,850 --> 00:14:02,060
these TCP and classic TCP are used and

00:13:58,790 --> 00:14:04,460
they have to coexist until everything is

00:14:02,060 --> 00:14:06,470
changed so for example if only

00:14:04,460 --> 00:14:09,890
incremental changes are possible for a

00:14:06,470 --> 00:14:11,840
while there is no single admin that can

00:14:09,890 --> 00:14:18,470
allow to change everything at the same

00:14:11,840 --> 00:14:22,520
time and if duo PI squared is used with

00:14:18,470 --> 00:14:25,400
this disappea it has to use the DC tcp

00:14:22,520 --> 00:14:31,700
compatibility parameters which are this

00:14:25,400 --> 00:14:34,280
do Q and D cesium and if scalable

00:14:31,700 --> 00:14:36,260
congestion control uses d1 then it will

00:14:34,280 --> 00:14:40,130
just work with default parameters so

00:14:36,260 --> 00:14:42,770
nothing has to be changed and there is

00:14:40,130 --> 00:14:46,310
more detailed explanation of the

00:14:42,770 --> 00:14:49,160
parameters int a github repository which

00:14:46,310 --> 00:14:51,260
I insert a link to here

00:14:49,160 --> 00:14:54,500
so you can download it from there and

00:14:51,260 --> 00:14:56,630
you can read the readme file which

00:14:54,500 --> 00:15:01,280
explains what the parameters are useful

00:14:56,630 --> 00:15:03,350
but one of the parameters that is

00:15:01,280 --> 00:15:06,380
possible to experiment with is the

00:15:03,350 --> 00:15:12,490
target away for the classic cube which

00:15:06,380 --> 00:15:15,170
is set from the target parameter and to

00:15:12,490 --> 00:15:19,820
give some examples of how you can

00:15:15,170 --> 00:15:21,950
actually set it if you want to use dual

00:15:19,820 --> 00:15:24,950
PI Square to the scalable congestion

00:15:21,950 --> 00:15:28,640
control so one example is TCP prompt

00:15:24,950 --> 00:15:31,340
that you can also download from github

00:15:28,640 --> 00:15:34,430
repository and then you can load them

00:15:31,340 --> 00:15:36,890
all view and enable it and for TCP

00:15:34,430 --> 00:15:40,460
project and just use your PI squared

00:15:36,890 --> 00:15:43,280
with default parameters and if you want

00:15:40,460 --> 00:15:47,160
to use it with DC TCP as a Laurentian we

00:15:43,280 --> 00:15:51,330
already mentioned you have to use the

00:15:47,160 --> 00:15:55,230
this is your cue on TCM parameters that

00:15:51,330 --> 00:15:59,190
will use ct0

00:15:55,230 --> 00:16:07,230
as a alfred specifier and notice it e1

00:15:59,190 --> 00:16:11,190
which is default so that's actually all

00:16:07,230 --> 00:16:23,450
I wanted to show and I can answer

00:16:11,190 --> 00:16:23,450
questions yes sorry

00:16:29,330 --> 00:16:38,959
so when you try to a few cuddle in your

00:16:32,089 --> 00:16:41,300
experiment you using the I was going to

00:16:38,959 --> 00:16:44,690
mention if there's questions possible to

00:16:41,300 --> 00:16:50,180
use this version to achieve to set lower

00:16:44,690 --> 00:16:52,930
ACN threshold and set more delay which

00:16:50,180 --> 00:16:56,600
would then produce lower delay for

00:16:52,930 --> 00:16:58,370
scalable traffic but the version that I

00:16:56,600 --> 00:17:01,610
use was without that option mainly

00:16:58,370 --> 00:17:04,699
because I don't really understand why

00:17:01,610 --> 00:17:06,410
the queuing delay was that big with that

00:17:04,699 --> 00:17:08,329
queue cutter because it should be like

00:17:06,410 --> 00:17:12,490
the minimal one with multiple queues

00:17:08,329 --> 00:17:18,290
yeah so there's no way you can beat fq

00:17:12,490 --> 00:17:19,850
well it's possible so to set this option

00:17:18,290 --> 00:17:23,380
and then the queue is going to be lower

00:17:19,850 --> 00:17:23,380
but without this option is not

00:17:26,770 --> 00:17:29,339
this

00:17:30,930 --> 00:17:37,320
in yours where's a demo right I see you

00:17:34,470 --> 00:17:40,110
know you could introduce a little more

00:17:37,320 --> 00:17:41,340
delay I mean yes you did you try with

00:17:40,110 --> 00:17:49,580
the largely as well

00:17:41,340 --> 00:17:52,440
yes yep there so I'll switch back to

00:17:49,580 --> 00:17:56,970
square

00:17:52,440 --> 00:18:00,240
and then I'll set this EP again on the

00:17:56,970 --> 00:18:02,490
first sender and receiver prayer and so

00:18:00,240 --> 00:18:04,680
here we have the link speed of 40

00:18:02,490 --> 00:18:08,010
megabits per second and the base our

00:18:04,680 --> 00:18:09,660
video oh I only use Pacers do you know

00:18:08,010 --> 00:18:14,490
anyway it's seven milliseconds so I can

00:18:09,660 --> 00:18:17,010
set it let's say 250 milliseconds and we

00:18:14,490 --> 00:18:18,900
have 40 megabits link with 50

00:18:17,010 --> 00:18:22,860
milliseconds and the queuing delay still

00:18:18,900 --> 00:18:28,110
low so it's still at below 1 millisecond

00:18:22,860 --> 00:18:32,220
average and I can actually take a

00:18:28,110 --> 00:18:35,970
hireling speed let's say 120 megabits

00:18:32,220 --> 00:18:38,190
now when switches you will see some

00:18:35,970 --> 00:18:41,400
hiccups but that's mostly because was

00:18:38,190 --> 00:18:44,040
that thick yeah yeah yeah I see I see

00:18:41,400 --> 00:18:46,200
the other the gyro like I mean the

00:18:44,040 --> 00:18:49,410
throughput is actually now changing like

00:18:46,200 --> 00:18:52,710
DC TCP is actually getting more no

00:18:49,410 --> 00:18:55,350
throughput and the cubix no this is not

00:18:52,710 --> 00:18:58,830
throughput this is actually window I've

00:18:55,350 --> 00:19:02,820
changed throughput then you it's

00:18:58,830 --> 00:19:05,640
actually going to be because when the

00:19:02,820 --> 00:19:11,460
delay was low it was the vice versa and

00:19:05,640 --> 00:19:15,120
now you're seeing so so when the delay

00:19:11,460 --> 00:19:19,410
was low I I actually saw the throughput

00:19:15,120 --> 00:19:23,220
or the window size of DCT was actually

00:19:19,410 --> 00:19:25,350
smaller in comparison peered to the

00:19:23,220 --> 00:19:29,070
cubic but now it's not it's actually

00:19:25,350 --> 00:19:31,440
larger so so now it's I suspect to rate

00:19:29,070 --> 00:19:32,100
now you can see the rate of this DCP is

00:19:31,440 --> 00:19:36,020
actual

00:19:32,100 --> 00:19:39,410
a little bit I would one thing that on

00:19:36,020 --> 00:19:42,929
we need to I think it takes a little a

00:19:39,410 --> 00:19:45,809
while for the classic flows to get

00:19:42,929 --> 00:19:50,220
completely up to speed but right now are

00:19:45,809 --> 00:19:54,049
they so the window fairness is rather

00:19:50,220 --> 00:19:56,570
good for lower says it's a little bit

00:19:54,049 --> 00:20:01,370
different for higher artists but they

00:19:56,570 --> 00:20:04,020
it's still within the range of fair

00:20:01,370 --> 00:20:06,720
comparison for a window as you can see

00:20:04,020 --> 00:20:11,429
the line in the middle this is if we are

00:20:06,720 --> 00:20:14,880
window and the flows are close to it

00:20:11,429 --> 00:20:17,659
I can also start a few more flows to get

00:20:14,880 --> 00:20:17,659
a better comparison

00:20:17,680 --> 00:20:26,680
and the delay still low for the Alfre

00:20:22,480 --> 00:20:30,570
static so now that we have a bit more

00:20:26,680 --> 00:20:30,570
flows and see

00:20:30,850 --> 00:20:34,770
getting closer to the middle line

00:20:40,510 --> 00:20:45,460
so did that answer your question

00:20:43,570 --> 00:20:49,169
yeah I just wanted to actually see the

00:20:45,460 --> 00:20:49,169
behavior so thank you

00:20:55,940 --> 00:21:03,210
bringing more questions there are no

00:21:00,000 --> 00:21:05,120
more questions we'll go to the next

00:21:03,210 --> 00:21:08,060
thank you

00:21:05,120 --> 00:21:08,060

YouTube URL: https://www.youtube.com/watch?v=g9VfnNTDv-I


