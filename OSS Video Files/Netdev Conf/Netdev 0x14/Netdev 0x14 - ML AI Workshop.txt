Title: Netdev 0x14 - ML AI Workshop
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Chair: Tom Herbert

More info: https://netdevconf.info/0x14/session.html?workshop-ai-ml

Date: Friday, August 14, 2020

What is Machine Learning?
Machine learning (ML) is a subset of artificial intelligence(AI) 
that lets computer systems solve a specific task without using
explicit instructions, relying on patterns and inference instead
of human intervention.

But How Does ML Apply To Networking?
Machine learning can be used to observe patterns in network
traffic or configuration and use the resulting data for a
variety of things, some sample space:
- dynamic congestion control, see for example applicability of:
   https://netdevconf.info/0x12/session.html?restructuring-endpoint-congestion-control
- improve datapath performance 
- path optimization 
- anomaly detection from a baseline expectation and using the
  resulting data either for security or optimization end goals
- etc.

At 0x14 we have two moonshot talks that look at using ML for
networking on Linux. These talks will be part of the ML
workshop which is debutting in 0x14. We hope to able to
solicit discussions and feedback on the subject.

In the first moonshot talk Marta Plantykow, Piotr Raczynski,
Maciej Machnikowski and Pawel Szymanski will discuss an approach
to optimize networking performance alongside CPU utilization
with ML.
Marta et al propose an approach which will use ML to study RSS
patterns and the CPU spread and then react dynamically to
modify RSS hash parameters to improve CPU spread.

The authors will go over the challenges they overcame, show some
performance numbers and solicit feedback.

More info:
https://netdevconf.info/0x14/session.html?talk-performance-optimization-using-artificial-intelligence-methods

Our second talk is from Maciej Paczkowski, Aleksandra Jereczek, and
Patrycja Kochmanska. In this talk Maciej et al integrate into FRR
to understand how to best optimize the path selection in an environemnt
with multiple simultenous link faults and incestant link flapps.
Could routing decisions better helped with ML hooks in
the kernel/datapath? Could we make use of offloading some of
the algos to AI hardware?

More info:
https://netdevconf.info/0x14/session.html?talk-machine-learning-in-packet-routing-process-using-Quagga-Zebra-routing-sw-suite
Captions: 
	00:00:06,720 --> 00:00:10,639
okay

00:00:07,759 --> 00:00:11,920
so welcome to the ai machine learning

00:00:10,639 --> 00:00:14,240
workshop

00:00:11,920 --> 00:00:15,120
i believe this is the first time we've

00:00:14,240 --> 00:00:19,039
had

00:00:15,120 --> 00:00:21,600
such a workshop so it is

00:00:19,039 --> 00:00:22,160
somewhat of a new area and i wanted to

00:00:21,600 --> 00:00:24,480
give

00:00:22,160 --> 00:00:26,880
a little bit of preface before we get

00:00:24,480 --> 00:00:30,720
into the agenda so we will have

00:00:26,880 --> 00:00:33,840
two presentations today in the area

00:00:30,720 --> 00:00:37,200
but thinking about

00:00:33,840 --> 00:00:39,360
aia artificial intelligence machine

00:00:37,200 --> 00:00:42,320
learning and networking

00:00:39,360 --> 00:00:45,280
i tried to come up with three kind of

00:00:42,320 --> 00:00:48,800
use cases or applications for this

00:00:45,280 --> 00:00:50,160
um specific to to what this means to

00:00:48,800 --> 00:00:52,719
networking

00:00:50,160 --> 00:00:53,440
so the first one i i think even came up

00:00:52,719 --> 00:00:56,840
in

00:00:53,440 --> 00:01:01,440
the previous talk indirectly

00:00:56,840 --> 00:01:04,000
um in this room so how do we optimize

00:01:01,440 --> 00:01:05,920
ai machine learning applications that

00:01:04,000 --> 00:01:07,680
are connected to the network

00:01:05,920 --> 00:01:09,119
so in other words what can we do in the

00:01:07,680 --> 00:01:12,320
networking stack

00:01:09,119 --> 00:01:15,360
networking protocols networking layer

00:01:12,320 --> 00:01:16,479
specific to the needs of ai and machine

00:01:15,360 --> 00:01:19,680
learning

00:01:16,479 --> 00:01:20,880
and obviously we know that ai machine

00:01:19,680 --> 00:01:24,479
learning is all about

00:01:20,880 --> 00:01:26,080
getting large data sets and running a

00:01:24,479 --> 00:01:28,400
simple computation over those

00:01:26,080 --> 00:01:30,479
usually due to learning or inference but

00:01:28,400 --> 00:01:33,360
we also have to move this data

00:01:30,479 --> 00:01:34,960
and clearly that's where the networking

00:01:33,360 --> 00:01:38,159
part comes in

00:01:34,960 --> 00:01:38,159
so for instance we

00:01:38,400 --> 00:01:44,960
need to explore how to get data to gpus

00:01:42,399 --> 00:01:46,240
uh which are processing large data sets

00:01:44,960 --> 00:01:49,759
in a form that they

00:01:46,240 --> 00:01:51,520
they can understand um as opposed to

00:01:49,759 --> 00:01:53,439
just throwing everything to the cpu

00:01:51,520 --> 00:01:55,840
which they've done in the past so

00:01:53,439 --> 00:01:57,280
there's some some work ongoing in this i

00:01:55,840 --> 00:02:00,479
would expect it to

00:01:57,280 --> 00:02:03,600
continue uh to push forward

00:02:00,479 --> 00:02:07,040
the second one is how do we apply

00:02:03,600 --> 00:02:09,599
ai and machine learning to

00:02:07,040 --> 00:02:10,879
various aspects of of how we run

00:02:09,599 --> 00:02:14,480
networks

00:02:10,879 --> 00:02:18,080
how we parameterize protocol stacks

00:02:14,480 --> 00:02:20,000
how do we optimize optimize them using

00:02:18,080 --> 00:02:21,680
kind of learning algorithms and this is

00:02:20,000 --> 00:02:25,040
pretty straightforward

00:02:21,680 --> 00:02:26,879
conceptually clearly there's a lot of

00:02:25,040 --> 00:02:29,520
motivation to do this

00:02:26,879 --> 00:02:30,560
for instance if we can come up with a

00:02:29,520 --> 00:02:33,280
new algorithm

00:02:30,560 --> 00:02:35,680
that detects faults in a network way

00:02:33,280 --> 00:02:37,680
before humans do

00:02:35,680 --> 00:02:39,360
and can predict these thoughts based on

00:02:37,680 --> 00:02:41,760
the time of day or or

00:02:39,360 --> 00:02:43,680
circumstances in the world this is

00:02:41,760 --> 00:02:47,040
obviously a really good thing

00:02:43,680 --> 00:02:49,720
and the

00:02:47,040 --> 00:02:52,400
nominal goal is to have networks kind of

00:02:49,720 --> 00:02:54,720
self-adjust self-automate

00:02:52,400 --> 00:02:55,920
to conditions and to solve problems even

00:02:54,720 --> 00:02:58,800
before they

00:02:55,920 --> 00:02:59,680
actually occur so clearly we're going to

00:02:58,800 --> 00:03:03,280
see a lot of

00:02:59,680 --> 00:03:06,319
application here telemetry

00:03:03,280 --> 00:03:09,440
gathering is definitely one of the

00:03:06,319 --> 00:03:11,360
needs for this so once we have again the

00:03:09,440 --> 00:03:12,959
large data set for what's happening in

00:03:11,360 --> 00:03:15,200
the network

00:03:12,959 --> 00:03:16,720
then it's a matter of running the

00:03:15,200 --> 00:03:17,440
machine learning algorithms over that

00:03:16,720 --> 00:03:19,360
data set

00:03:17,440 --> 00:03:21,120
and and hopefully spit out some answers

00:03:19,360 --> 00:03:23,920
on on how to

00:03:21,120 --> 00:03:25,120
improve things so clearly that's uh an

00:03:23,920 --> 00:03:25,519
ongoing area of work and i believe

00:03:25,120 --> 00:03:27,920
that's

00:03:25,519 --> 00:03:30,480
mostly the subject of the two

00:03:27,920 --> 00:03:33,120
presentations we have today

00:03:30,480 --> 00:03:35,120
uh the third one's a little more i think

00:03:33,120 --> 00:03:38,400
research oriented at this point

00:03:35,120 --> 00:03:40,159
but um if you believe that

00:03:38,400 --> 00:03:42,000
uh things like self-driving cars will

00:03:40,159 --> 00:03:42,480
become the norm it's pretty clear that

00:03:42,000 --> 00:03:45,599
we have

00:03:42,480 --> 00:03:46,000
self-driving protocol implementation and

00:03:45,599 --> 00:03:49,599
network

00:03:46,000 --> 00:03:52,640
implementation meaning that the

00:03:49,599 --> 00:03:55,680
the algorithms themselves would be

00:03:52,640 --> 00:03:57,200
generated by machine learning for

00:03:55,680 --> 00:03:58,319
instance and

00:03:57,200 --> 00:04:00,560
like i said there's been some

00:03:58,319 --> 00:04:01,280
interesting research on this i believe

00:04:00,560 --> 00:04:05,040
the tx

00:04:01,280 --> 00:04:07,760
tcpx machina from

00:04:05,040 --> 00:04:10,159
mit was doing this where they were

00:04:07,760 --> 00:04:14,000
generating congestion control algorithms

00:04:10,159 --> 00:04:17,519
based on a large data set

00:04:14,000 --> 00:04:20,560
of tcp packets and

00:04:17,519 --> 00:04:22,560
run that through the data set or in the

00:04:20,560 --> 00:04:25,440
data set through machine learning

00:04:22,560 --> 00:04:27,120
and it created some sort of congestion

00:04:25,440 --> 00:04:28,720
control

00:04:27,120 --> 00:04:31,120
which is always interesting of course

00:04:28,720 --> 00:04:32,720
because uh the one interesting aspect of

00:04:31,120 --> 00:04:34,560
machine learning is

00:04:32,720 --> 00:04:37,199
the the output's only as good as the

00:04:34,560 --> 00:04:38,720
input so presumably if you can

00:04:37,199 --> 00:04:40,880
somehow capture all of the possible

00:04:38,720 --> 00:04:43,120
combinations of of tcp

00:04:40,880 --> 00:04:44,479
and congestion conditions and what what

00:04:43,120 --> 00:04:46,240
have you it

00:04:44,479 --> 00:04:48,320
could conceptually produce an algorithm

00:04:46,240 --> 00:04:52,479
that is is far better than what

00:04:48,320 --> 00:04:54,720
humans could produce uh clearly this is

00:04:52,479 --> 00:04:56,320
research and it'll be a while before

00:04:54,720 --> 00:04:58,400
they obsolete our job

00:04:56,320 --> 00:05:00,880
here um i sort of happened eventually

00:04:58,400 --> 00:05:03,360
but hopefully not for a few years

00:05:00,880 --> 00:05:04,000
so i just wanted to kind of frame that

00:05:03,360 --> 00:05:07,520
and

00:05:04,000 --> 00:05:09,199
my hope is that we actually see more of

00:05:07,520 --> 00:05:10,560
ai and machine learning and conferences

00:05:09,199 --> 00:05:12,639
like this

00:05:10,560 --> 00:05:14,320
i think the time has come and the real

00:05:12,639 --> 00:05:17,600
applications are

00:05:14,320 --> 00:05:17,600
are manifesting themselves

00:05:18,880 --> 00:05:22,639
hello my name is patricia kofmanska i

00:05:21,680 --> 00:05:25,199
work at intel

00:05:22,639 --> 00:05:27,520
and together with alexandria and matty

00:05:25,199 --> 00:05:29,199
pachkowski we prepare the talk regarding

00:05:27,520 --> 00:05:31,440
machine learning in pocket rooting

00:05:29,199 --> 00:05:33,840
process using quagga or zebra software

00:05:31,440 --> 00:05:33,840
suit

00:05:34,880 --> 00:05:38,800
we will start by introducing a bunch of

00:05:36,800 --> 00:05:39,759
information about linked state routing

00:05:38,800 --> 00:05:41,440
protocols

00:05:39,759 --> 00:05:43,039
then we will tell about possible

00:05:41,440 --> 00:05:44,880
improvement areas

00:05:43,039 --> 00:05:46,800
we will describe current link state

00:05:44,880 --> 00:05:48,720
routing protocols implementation

00:05:46,800 --> 00:05:51,280
and how it is possible to use machine

00:05:48,720 --> 00:05:52,479
learning in current ospf or isis

00:05:51,280 --> 00:05:54,000
networks

00:05:52,479 --> 00:05:56,240
we will show you the results of our

00:05:54,000 --> 00:05:58,240
experiments in simulated environments

00:05:56,240 --> 00:06:02,000
and describe potential practical

00:05:58,240 --> 00:06:02,000
applications and their limitations

00:06:03,120 --> 00:06:06,960
modern routing protocols are taking

00:06:05,120 --> 00:06:08,960
various approaches on how to select

00:06:06,960 --> 00:06:09,759
paths for packets in the most effective

00:06:08,960 --> 00:06:11,440
way

00:06:09,759 --> 00:06:13,039
there are two main classes of routing

00:06:11,440 --> 00:06:15,600
protocols the first one

00:06:13,039 --> 00:06:17,680
is distance vector routing protocols in

00:06:15,600 --> 00:06:19,360
which routers have no information about

00:06:17,680 --> 00:06:21,600
the whole network topology

00:06:19,360 --> 00:06:23,919
and the decisions about the best routes

00:06:21,600 --> 00:06:26,400
are based only on the data about costs

00:06:23,919 --> 00:06:27,360
gained from router's nearest neighbors

00:06:26,400 --> 00:06:29,440
and there are also

00:06:27,360 --> 00:06:31,759
link state routing protocols which

00:06:29,440 --> 00:06:33,440
assume that each network node creates

00:06:31,759 --> 00:06:35,120
and stores its own scheme of the whole

00:06:33,440 --> 00:06:37,360
network topology

00:06:35,120 --> 00:06:39,280
and then independently calculates the

00:06:37,360 --> 00:06:40,319
list goes back from itself to every

00:06:39,280 --> 00:06:42,479
other node

00:06:40,319 --> 00:06:43,680
the topology scheme may be considered as

00:06:42,479 --> 00:06:45,520
a graph

00:06:43,680 --> 00:06:47,120
the paths are calculated basing on

00:06:45,520 --> 00:06:49,120
dijkstra's algorithm

00:06:47,120 --> 00:06:50,960
it finds the shortest path between two

00:06:49,120 --> 00:06:52,479
graph nodes by adding up the costs of

00:06:50,960 --> 00:06:54,800
links by itself

00:06:52,479 --> 00:06:59,440
the most popular examples of link stage

00:06:54,800 --> 00:06:59,440
routing protocols are ospf and isis

00:07:00,479 --> 00:07:04,960
over recent years computer networks have

00:07:02,960 --> 00:07:07,120
experienced a huge and dynamic change

00:07:04,960 --> 00:07:09,280
modern networks are getting bigger more

00:07:07,120 --> 00:07:11,280
virtualized and more dynamic

00:07:09,280 --> 00:07:13,280
the virtualized environments are being

00:07:11,280 --> 00:07:14,319
widely used in data centers and lab

00:07:13,280 --> 00:07:17,120
providers

00:07:14,319 --> 00:07:18,880
the virtualization aspect allows us to

00:07:17,120 --> 00:07:20,000
allows networks to have more and more

00:07:18,880 --> 00:07:21,840
nodes and therefore

00:07:20,000 --> 00:07:23,680
have a huge amount of virtual machines

00:07:21,840 --> 00:07:25,520
that are being dynamically added or

00:07:23,680 --> 00:07:27,840
removed from the network

00:07:25,520 --> 00:07:29,440
initially network recovery time having

00:07:27,840 --> 00:07:31,680
reached few tens of seconds was

00:07:29,440 --> 00:07:34,319
considered sufficiently fast

00:07:31,680 --> 00:07:36,880
therefore ospf's original design was not

00:07:34,319 --> 00:07:39,360
comprehensively optimized in this field

00:07:36,880 --> 00:07:40,319
however over almost 20 years these

00:07:39,360 --> 00:07:43,039
requirements have

00:07:40,319 --> 00:07:44,960
changed nowadays networks are using more

00:07:43,039 --> 00:07:45,919
and more powerful and computationally

00:07:44,960 --> 00:07:48,160
efficient devices

00:07:45,919 --> 00:07:49,440
and currently such long inoperable

00:07:48,160 --> 00:07:51,520
network state

00:07:49,440 --> 00:07:52,560
would cause unacceptable traffic loss

00:07:51,520 --> 00:07:54,400
level

00:07:52,560 --> 00:07:55,759
having in mind the dynamic character of

00:07:54,400 --> 00:07:58,400
computer networks

00:07:55,759 --> 00:08:00,639
they are still expected to be stable and

00:07:58,400 --> 00:08:02,720
100 percent accurate

00:08:00,639 --> 00:08:04,639
the problem occurs when networks

00:08:02,720 --> 00:08:07,120
experience a huge

00:08:04,639 --> 00:08:08,479
amount of link failures in this case the

00:08:07,120 --> 00:08:11,199
adaptation time of such

00:08:08,479 --> 00:08:12,400
networks may take even a few seconds the

00:08:11,199 --> 00:08:14,960
core of our idea

00:08:12,400 --> 00:08:16,720
is to reduce recovery time and therefore

00:08:14,960 --> 00:08:19,360
faster adjust to the new network

00:08:16,720 --> 00:08:19,360
topology

00:08:20,879 --> 00:08:26,560
the main goal of ospf is to determine

00:08:23,840 --> 00:08:28,639
best paths between all network nodes

00:08:26,560 --> 00:08:30,720
link state advertisement packets are

00:08:28,639 --> 00:08:31,440
being sent each time a change occurs in

00:08:30,720 --> 00:08:33,519
the network

00:08:31,440 --> 00:08:35,760
and based on these packets link state

00:08:33,519 --> 00:08:38,000
database is being filled

00:08:35,760 --> 00:08:40,560
the extra algorithm that is used in

00:08:38,000 --> 00:08:42,959
current ostf implementation operates on

00:08:40,560 --> 00:08:43,680
data stored in lsdb in order to

00:08:42,959 --> 00:08:46,560
calculate

00:08:43,680 --> 00:08:48,560
shortest path tree the spt represents

00:08:46,560 --> 00:08:50,560
the shortest paths to each destination

00:08:48,560 --> 00:08:52,480
in giving routing area

00:08:50,560 --> 00:08:55,360
in the picture we can see how lsa

00:08:52,480 --> 00:08:57,200
packets are used to build up the sdt

00:08:55,360 --> 00:09:01,279
next packets are used to build the

00:08:57,200 --> 00:09:01,279
overall view of the network topology

00:09:02,640 --> 00:09:07,519
such lsdb is later used by ostf in spf

00:09:06,320 --> 00:09:09,760
algorithm

00:09:07,519 --> 00:09:10,640
in order to create routing information

00:09:09,760 --> 00:09:12,560
base

00:09:10,640 --> 00:09:14,000
changes in the topological database

00:09:12,560 --> 00:09:16,959
trigger partial or

00:09:14,000 --> 00:09:18,640
full routing table recalculations with

00:09:16,959 --> 00:09:20,560
spf algorithm

00:09:18,640 --> 00:09:22,800
full recalculation of course takes

00:09:20,560 --> 00:09:25,200
longer time and is more expensive

00:09:22,800 --> 00:09:26,399
since each transit link that fails is

00:09:25,200 --> 00:09:28,959
connected to at least

00:09:26,399 --> 00:09:30,080
two routers it results in at least two

00:09:28,959 --> 00:09:33,680
routers

00:09:30,080 --> 00:09:35,839
forced to run full spf recalculation

00:09:33,680 --> 00:09:37,519
this recalculation has a negative impact

00:09:35,839 --> 00:09:39,440
on the overall network of

00:09:37,519 --> 00:09:41,040
efficiency because it may result in

00:09:39,440 --> 00:09:43,200
packet losses

00:09:41,040 --> 00:09:45,200
the final base that is used to choose

00:09:43,200 --> 00:09:46,320
output interface to direct the packet to

00:09:45,200 --> 00:09:49,279
its destination

00:09:46,320 --> 00:09:50,640
is called forwarding information base in

00:09:49,279 --> 00:09:51,680
case of full routing table

00:09:50,640 --> 00:09:54,320
recalculations

00:09:51,680 --> 00:09:54,880
spf requires some time to generate new

00:09:54,320 --> 00:09:56,800
routing

00:09:54,880 --> 00:09:58,080
information base so the forwarding

00:09:56,800 --> 00:10:00,880
information base

00:09:58,080 --> 00:10:02,240
is not updated at the same time lsa come

00:10:00,880 --> 00:10:05,120
into the database

00:10:02,240 --> 00:10:07,760
and this create network outage this

00:10:05,120 --> 00:10:10,320
picture shows examples of routing table

00:10:07,760 --> 00:10:12,399
that is calculated by ospf

00:10:10,320 --> 00:10:14,480
and forwarding table that is finally

00:10:12,399 --> 00:10:17,279
used to forward packets

00:10:14,480 --> 00:10:18,720
it is worth mentioning that the rip is

00:10:17,279 --> 00:10:21,040
used in user space

00:10:18,720 --> 00:10:24,000
and tip in kernel space of linux

00:10:21,040 --> 00:10:24,000
operating system

00:10:26,320 --> 00:10:31,440
in order to create booting information

00:10:28,560 --> 00:10:32,640
base link state protocol undertakes few

00:10:31,440 --> 00:10:35,200
steps

00:10:32,640 --> 00:10:37,200
firstly it creates link state database

00:10:35,200 --> 00:10:39,839
based on lsa packets

00:10:37,200 --> 00:10:42,480
then it performs spf calculations in

00:10:39,839 --> 00:10:44,480
order to create shortest factory

00:10:42,480 --> 00:10:45,519
the tree is then used to determine the

00:10:44,480 --> 00:10:48,000
most efficient

00:10:45,519 --> 00:10:50,240
routes and create the routing table this

00:10:48,000 --> 00:10:52,240
routing table is then passed to the

00:10:50,240 --> 00:10:56,160
kernel space and effectively used to

00:10:52,240 --> 00:10:56,160
forward packets to specific ports

00:10:57,519 --> 00:11:03,279
the current ospf implementation is 100

00:11:00,720 --> 00:11:05,200
accurate but have an area of improvement

00:11:03,279 --> 00:11:07,279
when it comes to the recovery time after

00:11:05,200 --> 00:11:09,760
multiple link failures

00:11:07,279 --> 00:11:11,920
to optimize the problem of recovery time

00:11:09,760 --> 00:11:13,040
spf calculations are not only run in

00:11:11,920 --> 00:11:15,839
full mode but they're

00:11:13,040 --> 00:11:17,279
also partial or incremental modes of spf

00:11:15,839 --> 00:11:19,040
recalculations

00:11:17,279 --> 00:11:20,560
in which not all of the sdg is

00:11:19,040 --> 00:11:23,519
recalculated

00:11:20,560 --> 00:11:24,399
however full spf recalculations still

00:11:23,519 --> 00:11:26,560
occur

00:11:24,399 --> 00:11:27,680
and lead to an outdated forwarding

00:11:26,560 --> 00:11:30,480
information base

00:11:27,680 --> 00:11:32,800
and network outage here we can see on

00:11:30,480 --> 00:11:35,600
which stages of ospf operations

00:11:32,800 --> 00:11:35,920
the network may be in inoperable state

00:11:35,600 --> 00:11:38,000
and

00:11:35,920 --> 00:11:40,079
as we can see this is a relatively long

00:11:38,000 --> 00:11:43,120
period of time

00:11:40,079 --> 00:11:44,480
thank you patricia and now let's focus

00:11:43,120 --> 00:11:47,680
on how can we use

00:11:44,480 --> 00:11:52,480
machine learning in current links and

00:11:47,680 --> 00:11:54,320
database protocols like ocpf or isis

00:11:52,480 --> 00:11:55,600
we want to use machine learning to

00:11:54,320 --> 00:11:57,680
enhance the

00:11:55,600 --> 00:12:00,639
routing information based recalculation

00:11:57,680 --> 00:12:02,639
step in the picture on the right

00:12:00,639 --> 00:12:04,720
here you can notice that we added

00:12:02,639 --> 00:12:08,880
additional parallel

00:12:04,720 --> 00:12:10,800
ai algorithm so our solution is based on

00:12:08,880 --> 00:12:13,120
the running ai algorithm

00:12:10,800 --> 00:12:14,240
it can be neural network that would

00:12:13,120 --> 00:12:16,320
compute the routes

00:12:14,240 --> 00:12:17,839
in parallel with primary dextra

00:12:16,320 --> 00:12:20,959
algorithm

00:12:17,839 --> 00:12:23,680
and algorithm can be implemented in a

00:12:20,959 --> 00:12:24,000
as i said in form of a neural network it

00:12:23,680 --> 00:12:26,880
would

00:12:24,000 --> 00:12:28,000
be trained to output the next hub from

00:12:26,880 --> 00:12:32,720
the local router

00:12:28,000 --> 00:12:36,000
to reach other router in the network

00:12:32,720 --> 00:12:37,920
as patricia said before the overall goal

00:12:36,000 --> 00:12:40,320
is to reduce the time needed to

00:12:37,920 --> 00:12:43,120
construct the functional

00:12:40,320 --> 00:12:45,200
forwarding information base that is with

00:12:43,120 --> 00:12:48,480
high probability good enough to

00:12:45,200 --> 00:12:51,600
the erect bucket until the full spf

00:12:48,480 --> 00:12:54,880
calculation is done uh

00:12:51,600 --> 00:12:57,680
the additional parallel ai algorithm

00:12:54,880 --> 00:12:58,639
uh i mean this one uh creates a

00:12:57,680 --> 00:13:02,959
temporary

00:12:58,639 --> 00:13:04,720
uh routing information base

00:13:02,959 --> 00:13:06,720
and then we pass it to the system as a

00:13:04,720 --> 00:13:10,240
temporary firmware

00:13:06,720 --> 00:13:13,360
a forwarding information base

00:13:10,240 --> 00:13:15,440
a temporary fib with high probability

00:13:13,360 --> 00:13:16,880
allows reaching the destinations of the

00:13:15,440 --> 00:13:19,440
packets

00:13:16,880 --> 00:13:20,240
because of not fully predictable nature

00:13:19,440 --> 00:13:23,360
of ai

00:13:20,240 --> 00:13:26,000
algorithms the decision may not be 100

00:13:23,360 --> 00:13:28,959
accurate but well-trained neural network

00:13:26,000 --> 00:13:32,560
can get close to this value

00:13:28,959 --> 00:13:34,880
the ai algorithm will anticipate the spf

00:13:32,560 --> 00:13:36,079
recalculation providing probably the

00:13:34,880 --> 00:13:39,839
best next hop

00:13:36,079 --> 00:13:41,680
given that lsdb state

00:13:39,839 --> 00:13:42,880
the calculation of this next hop

00:13:41,680 --> 00:13:44,880
corresponds

00:13:42,880 --> 00:13:47,839
from the packet forwarding perspective

00:13:44,880 --> 00:13:48,720
to the minimum spanning tree calculation

00:13:47,839 --> 00:13:53,040
which is a

00:13:48,720 --> 00:13:53,040
completionary more intensive

00:13:53,440 --> 00:14:00,240
here here's another picture of our idea

00:13:58,000 --> 00:14:02,079
as you can see we want to add parallel

00:14:00,240 --> 00:14:04,880
roots recalculation

00:14:02,079 --> 00:14:06,079
only in case of full recalculation

00:14:04,880 --> 00:14:08,720
request

00:14:06,079 --> 00:14:09,440
since this is the only place where ai

00:14:08,720 --> 00:14:12,560
algorithm

00:14:09,440 --> 00:14:14,240
is faster than original approach full

00:14:12,560 --> 00:14:17,360
recalculation request

00:14:14,240 --> 00:14:20,720
is performed every time when transit

00:14:17,360 --> 00:14:21,360
link fails this results in two or more

00:14:20,720 --> 00:14:24,320
routers

00:14:21,360 --> 00:14:25,199
always forced to run expensive full spf

00:14:24,320 --> 00:14:27,839
recalculation

00:14:25,199 --> 00:14:28,959
with a very negative impact in the

00:14:27,839 --> 00:14:33,120
routing

00:14:28,959 --> 00:14:33,600
of overall network uh why do we claim

00:14:33,120 --> 00:14:35,839
that

00:14:33,600 --> 00:14:37,120
ai solution is faster than our original

00:14:35,839 --> 00:14:39,920
approach

00:14:37,120 --> 00:14:40,560
uh well it's simple mathematics uh let's

00:14:39,920 --> 00:14:42,240
say that

00:14:40,560 --> 00:14:45,279
n is the number of routers in the

00:14:42,240 --> 00:14:46,800
network so the extra algorithm is n

00:14:45,279 --> 00:14:48,720
square complexity

00:14:46,800 --> 00:14:49,920
while neural network is only and

00:14:48,720 --> 00:14:52,959
complexity

00:14:49,920 --> 00:14:55,519
so let's say uh for 50 nodes

00:14:52,959 --> 00:14:56,480
and neural network will be incomparably

00:14:55,519 --> 00:14:59,839
faster than

00:14:56,480 --> 00:14:59,839
old good extra

00:15:00,240 --> 00:15:05,680
of course as i mentioned before we

00:15:03,360 --> 00:15:07,839
don't want to replace dijkstra with

00:15:05,680 --> 00:15:08,240
neural network all we want to achieve is

00:15:07,839 --> 00:15:11,279
to

00:15:08,240 --> 00:15:12,000
generate temporary routing table and

00:15:11,279 --> 00:15:14,639
pass it as

00:15:12,000 --> 00:15:16,160
soon as possible to system forwarding

00:15:14,639 --> 00:15:18,720
table

00:15:16,160 --> 00:15:20,480
we realize that the forwarding table

00:15:18,720 --> 00:15:23,519
based on neural network

00:15:20,480 --> 00:15:26,160
is not 100 percent accurate but it's

00:15:23,519 --> 00:15:28,560
better than nothing at this point we

00:15:26,160 --> 00:15:31,759
have a choice to use totally outdated

00:15:28,560 --> 00:15:32,720
fib or writing table or use the new one

00:15:31,759 --> 00:15:36,240
generated by

00:15:32,720 --> 00:15:40,480
neural network and rely on it in case of

00:15:36,240 --> 00:15:44,000
the extra calculation are not done yet

00:15:40,480 --> 00:15:44,880
ai algorithm will be trained separately

00:15:44,000 --> 00:15:47,199
for each node

00:15:44,880 --> 00:15:48,399
to output the next hub from the local

00:15:47,199 --> 00:15:50,880
router to reach

00:15:48,399 --> 00:15:52,959
each other router in the network thanks

00:15:50,880 --> 00:15:54,000
to that we will have a forwarding table

00:15:52,959 --> 00:15:58,079
that is good enough

00:15:54,000 --> 00:15:59,600
until the full spf recalculation is done

00:15:58,079 --> 00:16:01,759
neural network is trained based on

00:15:59,600 --> 00:16:04,560
previously calculated spf routes

00:16:01,759 --> 00:16:07,040
and may be retrained adaptively while

00:16:04,560 --> 00:16:10,240
network is operating

00:16:07,040 --> 00:16:13,680
the data set of the ail

00:16:10,240 --> 00:16:15,680
algorithm is trained with the

00:16:13,680 --> 00:16:16,800
per router data set that can be easily

00:16:15,680 --> 00:16:20,959
obtained in uh

00:16:16,800 --> 00:16:23,040
simulations for every of network

00:16:20,959 --> 00:16:24,399
okay on this slide we have a firmware

00:16:23,040 --> 00:16:27,759
information

00:16:24,399 --> 00:16:29,279
uh base state chart with the spf

00:16:27,759 --> 00:16:33,360
dijkstra

00:16:29,279 --> 00:16:36,399
based process and ai based process

00:16:33,360 --> 00:16:37,360
please notice that the chart is not in a

00:16:36,399 --> 00:16:40,880
scale

00:16:37,360 --> 00:16:41,920
however the ai process part will be

00:16:40,880 --> 00:16:45,040
always less than

00:16:41,920 --> 00:16:46,720
spf algorithm part

00:16:45,040 --> 00:16:48,320
when you look at the fib state you can

00:16:46,720 --> 00:16:51,120
notice three colors

00:16:48,320 --> 00:16:53,279
three different colors the orange one is

00:16:51,120 --> 00:16:55,600
the network outage speed

00:16:53,279 --> 00:16:57,360
our goal is to reduce this orange part

00:16:55,600 --> 00:16:59,839
as much as possible

00:16:57,360 --> 00:17:00,720
this is stage when we need to rely on

00:16:59,839 --> 00:17:03,920
outdated

00:17:00,720 --> 00:17:05,360
old forwarding table and this state is a

00:17:03,920 --> 00:17:08,640
chaos

00:17:05,360 --> 00:17:09,039
the yellow part is the states we want to

00:17:08,640 --> 00:17:11,199
run

00:17:09,039 --> 00:17:12,160
our network using forwarding table

00:17:11,199 --> 00:17:15,839
produced

00:17:12,160 --> 00:17:17,039
by ai algorithm as i said this is not

00:17:15,839 --> 00:17:20,720
the final stand

00:17:17,039 --> 00:17:25,280
state and it's not 100 percent

00:17:20,720 --> 00:17:26,799
reliable in yellow part we want to

00:17:25,280 --> 00:17:29,440
control the chaos

00:17:26,799 --> 00:17:32,960
caused by topology changes and where the

00:17:29,440 --> 00:17:37,200
old forwarding table is totally outdated

00:17:32,960 --> 00:17:41,200
temporary fib is used until the full

00:17:37,200 --> 00:17:43,679
spf recalculation has come to its end

00:17:41,200 --> 00:17:44,559
as i mentioned before it's worth

00:17:43,679 --> 00:17:48,000
remembering

00:17:44,559 --> 00:17:52,559
that the complexity of the generation of

00:17:48,000 --> 00:17:55,679
table with our algorithms case with n

00:17:52,559 --> 00:17:57,600
when n is the number of routers as

00:17:55,679 --> 00:18:00,240
for each router we need to compute the

00:17:57,600 --> 00:18:02,400
next hop from the local router and this

00:18:00,240 --> 00:18:03,520
is done with the constant cost that is

00:18:02,400 --> 00:18:06,320
the cost of having

00:18:03,520 --> 00:18:06,799
ai algorithm to generate the output for

00:18:06,320 --> 00:18:10,400
each

00:18:06,799 --> 00:18:13,360
router to reach and due to the

00:18:10,400 --> 00:18:14,559
spf complexity this spark can much much

00:18:13,360 --> 00:18:17,600
longer

00:18:14,559 --> 00:18:20,960
than ai algorithm takes

00:18:17,600 --> 00:18:24,799
the green part is the state when all spf

00:18:20,960 --> 00:18:28,080
dijkstra calculations are already done

00:18:24,799 --> 00:18:30,720
and final fib is ready to use

00:18:28,080 --> 00:18:31,280
at this moment we stop using neural

00:18:30,720 --> 00:18:35,039
network

00:18:31,280 --> 00:18:37,919
based fib and we start using the

00:18:35,039 --> 00:18:43,520
final deterministically generated

00:18:37,919 --> 00:18:46,880
routing and forwarding tables

00:18:43,520 --> 00:18:49,360
okay on this slide we have a time chart

00:18:46,880 --> 00:18:50,000
based on our initial calculation and

00:18:49,360 --> 00:18:53,360
test in

00:18:50,000 --> 00:18:54,000
fully simulated environments as you can

00:18:53,360 --> 00:18:57,120
notice

00:18:54,000 --> 00:19:00,960
we are able to reduce network

00:18:57,120 --> 00:19:04,799
time outage even three to four times

00:19:00,960 --> 00:19:08,000
in case of 50 nodes networks

00:19:04,799 --> 00:19:08,799
so apparently temporary fib calculated

00:19:08,000 --> 00:19:10,880
by

00:19:08,799 --> 00:19:12,720
artificial intelligence significantly

00:19:10,880 --> 00:19:15,120
decrease the outage time

00:19:12,720 --> 00:19:18,160
and even if rooting decisions based on

00:19:15,120 --> 00:19:20,240
it are not hundred percent accurate

00:19:18,160 --> 00:19:22,559
they can decrease packet loss level when

00:19:20,240 --> 00:19:26,080
network state is not stable

00:19:22,559 --> 00:19:27,679
we didn't put exact time on a scale

00:19:26,080 --> 00:19:28,799
since for different simulation

00:19:27,679 --> 00:19:31,840
environments

00:19:28,799 --> 00:19:35,679
this time a are these times are

00:19:31,840 --> 00:19:38,799
different however the shape of this

00:19:35,679 --> 00:19:42,240
chart and the notes times correlation of

00:19:38,799 --> 00:19:42,240
the charts are constant

00:19:43,919 --> 00:19:50,640
okay now let's talk a little bit

00:19:47,200 --> 00:19:53,120
about potential practical applications

00:19:50,640 --> 00:19:56,320
and the limitations

00:19:53,120 --> 00:19:59,520
the main targets of our solution are

00:19:56,320 --> 00:20:02,640
big industrial size data centers

00:19:59,520 --> 00:20:05,120
with many physical and virtual devices

00:20:02,640 --> 00:20:08,480
connected in the network

00:20:05,120 --> 00:20:11,440
the proposed ospf or isis

00:20:08,480 --> 00:20:11,919
recovery time optimization applies to

00:20:11,440 --> 00:20:14,960
big

00:20:11,919 --> 00:20:18,159
and dynamic networks where a full

00:20:14,960 --> 00:20:22,000
spf recalculation of an occur

00:20:18,159 --> 00:20:25,600
and packet loss tolerance is quite low

00:20:22,000 --> 00:20:27,039
moreover the ai part of offred solution

00:20:25,600 --> 00:20:29,600
may be supported by

00:20:27,039 --> 00:20:30,799
using hardware ai accelerator

00:20:29,600 --> 00:20:33,679
accelerators

00:20:30,799 --> 00:20:35,760
that additionally would decrease the cpu

00:20:33,679 --> 00:20:37,760
usage of data center

00:20:35,760 --> 00:20:40,159
send servers which may have different

00:20:37,760 --> 00:20:43,039
efficiency

00:20:40,159 --> 00:20:45,600
we may imagine that solution can be

00:20:43,039 --> 00:20:48,720
implemented on a hardware working in a

00:20:45,600 --> 00:20:51,200
plug-and-play manner and this will be

00:20:48,720 --> 00:20:52,400
accelerate data centers network

00:20:51,200 --> 00:20:54,640
performance

00:20:52,400 --> 00:20:56,799
without significant inference in the

00:20:54,640 --> 00:20:59,919
network devices

00:20:56,799 --> 00:21:01,120
and this project can be also adapted to

00:20:59,919 --> 00:21:04,640
be used in a smart

00:21:01,120 --> 00:21:08,640
network interface cards

00:21:04,640 --> 00:21:09,760
called called smartnics they can act as

00:21:08,640 --> 00:21:12,159
a linux based

00:21:09,760 --> 00:21:14,240
routers while being a traditional

00:21:12,159 --> 00:21:17,360
hardware nic

00:21:14,240 --> 00:21:21,120
according to our observations

00:21:17,360 --> 00:21:23,600
uh even one ai based ruler note

00:21:21,120 --> 00:21:24,720
in the network can decrease the average

00:21:23,600 --> 00:21:27,919
overall network

00:21:24,720 --> 00:21:28,480
selfie time of course this is not the

00:21:27,919 --> 00:21:31,600
rule

00:21:28,480 --> 00:21:32,960
it depends where the router is placed in

00:21:31,600 --> 00:21:35,360
the topology

00:21:32,960 --> 00:21:36,400
so if the router is in some critical

00:21:35,360 --> 00:21:38,320
place

00:21:36,400 --> 00:21:39,679
with the plenty connection to other

00:21:38,320 --> 00:21:42,880
readers it will

00:21:39,679 --> 00:21:45,840
help decrease decrease average

00:21:42,880 --> 00:21:45,840
outage time

00:21:46,000 --> 00:21:52,159
now few words about limitations

00:21:49,679 --> 00:21:54,799
unfortunately our solution will not

00:21:52,159 --> 00:21:57,120
resolve congestion problems

00:21:54,799 --> 00:21:58,320
this one this is one of the biggest

00:21:57,120 --> 00:22:03,360
problem including

00:21:58,320 --> 00:22:06,400
link state protocols unfortunately

00:22:03,360 --> 00:22:08,240
our idea will not help here we believe

00:22:06,400 --> 00:22:11,120
this would require totally different

00:22:08,240 --> 00:22:14,960
approach and we just wanted to focus

00:22:11,120 --> 00:22:14,960
right now on a self-healing time

00:22:15,840 --> 00:22:20,720
okay for the end i want to say few words

00:22:18,880 --> 00:22:23,440
about next steps

00:22:20,720 --> 00:22:25,760
we still have a lot of work to do here

00:22:23,440 --> 00:22:28,400
for now we have an initial

00:22:25,760 --> 00:22:29,600
implementation quagga software routing

00:22:28,400 --> 00:22:31,520
suit

00:22:29,600 --> 00:22:35,039
however the solution can be implemented

00:22:31,520 --> 00:22:38,240
in fr or any other software routing suit

00:22:35,039 --> 00:22:39,120
and next steps are tests in the real

00:22:38,240 --> 00:22:41,120
unstable

00:22:39,120 --> 00:22:42,799
and predictable environment with the

00:22:41,120 --> 00:22:46,080
frequent topology changes

00:22:42,799 --> 00:22:48,559
so there is a lot of tests for us

00:22:46,080 --> 00:22:50,159
we need to compare results from real

00:22:48,559 --> 00:22:52,240
environment

00:22:50,159 --> 00:22:55,840
then we will assess the real advantage

00:22:52,240 --> 00:22:55,840
of our solution

00:22:57,679 --> 00:23:01,600
okay we put some more interesting

00:22:59,840 --> 00:23:04,640
information in our paper

00:23:01,600 --> 00:23:08,159
so i very encourage you to read it

00:23:04,640 --> 00:23:09,600
and i think that all we had that was our

00:23:08,159 --> 00:23:11,440
brief idea

00:23:09,600 --> 00:23:13,840
thank you very much for spending time

00:23:11,440 --> 00:23:16,240
with us and

00:23:13,840 --> 00:23:16,240
take care

00:23:18,320 --> 00:23:25,039
okay thank you so we have

00:23:22,000 --> 00:23:27,600
a few questions on the chat

00:23:25,039 --> 00:23:28,240
um i'll go ahead and read them then if

00:23:27,600 --> 00:23:31,280
the

00:23:28,240 --> 00:23:33,600
uh person asking the question wants to

00:23:31,280 --> 00:23:35,280
wants to have follow-up they are welcome

00:23:33,600 --> 00:23:38,080
to unmute their microphone

00:23:35,280 --> 00:23:39,440
and of course the presenters can answer

00:23:38,080 --> 00:23:41,520
as needed

00:23:39,440 --> 00:23:43,360
so the first one perhaps we could use

00:23:41,520 --> 00:23:47,120
reinforcement learning

00:23:43,360 --> 00:23:49,120
and use spf dijkstra's algorithm

00:23:47,120 --> 00:23:51,840
for reward function to make this more

00:23:49,120 --> 00:23:51,840
adaptive

00:23:52,480 --> 00:23:56,400
uh do you hear me sorry

00:23:57,200 --> 00:24:05,200
can you okay

00:24:00,400 --> 00:24:09,039
okay i think i'm a nude

00:24:05,200 --> 00:24:10,400
we can hear you okay great

00:24:09,039 --> 00:24:13,440
so yeah i had some problems when i

00:24:10,400 --> 00:24:16,559
headphones uh

00:24:13,440 --> 00:24:18,640
we could use a real informants well uh

00:24:16,559 --> 00:24:20,960
i would say that this is a

00:24:18,640 --> 00:24:25,679
implementation detail for this moment

00:24:20,960 --> 00:24:28,720
uh this is a brief idea uh

00:24:25,679 --> 00:24:28,720
yes we use uh

00:24:28,799 --> 00:24:36,000
we thought about this also there is a

00:24:32,640 --> 00:24:38,720
patricia who was more involved in uh in

00:24:36,000 --> 00:24:39,200
implementation part she's she couldn't

00:24:38,720 --> 00:24:42,159
join

00:24:39,200 --> 00:24:42,799
unfortunately because of the internet uh

00:24:42,159 --> 00:24:46,159
access on

00:24:42,799 --> 00:24:46,960
in her area right now uh we'll think

00:24:46,159 --> 00:24:50,799
about it

00:24:46,960 --> 00:24:52,960
of course uh about reinforcements but uh

00:24:50,799 --> 00:24:54,720
for now we just want to focus on the

00:24:52,960 --> 00:24:58,640
next step which is the

00:24:54,720 --> 00:25:01,440
uh test in the real environment

00:24:58,640 --> 00:25:03,600
so uh this is what we want to do right

00:25:01,440 --> 00:25:03,600
now

00:25:03,840 --> 00:25:07,360
yeah there was a kind of a part two that

00:25:06,080 --> 00:25:10,000
looks like we're

00:25:07,360 --> 00:25:11,200
using supervised learning here what is

00:25:10,000 --> 00:25:13,919
the neural network

00:25:11,200 --> 00:25:15,440
mod nor not model how many uh layers in

00:25:13,919 --> 00:25:19,200
it

00:25:15,440 --> 00:25:21,760
well as far as i remember is uh five

00:25:19,200 --> 00:25:22,880
layers but you know this is a kind of

00:25:21,760 --> 00:25:26,559
the two

00:25:22,880 --> 00:25:28,640
we have to experiment so this is not the

00:25:26,559 --> 00:25:30,480
this is not the final solution this is

00:25:28,640 --> 00:25:31,120
not the final implementation we still

00:25:30,480 --> 00:25:34,159
have to

00:25:31,120 --> 00:25:37,679
uh uh find they know

00:25:34,159 --> 00:25:38,320
that uh which which what what what is

00:25:37,679 --> 00:25:42,640
the best

00:25:38,320 --> 00:25:45,120
uh way to uh we

00:25:42,640 --> 00:25:45,840
we don't have that uh final solution so

00:25:45,120 --> 00:25:48,000
the

00:25:45,840 --> 00:25:49,360
as i said now we have a five layers but

00:25:48,000 --> 00:25:51,200
it can be five

00:25:49,360 --> 00:25:52,640
it could be six it wouldn't be seven so

00:25:51,200 --> 00:25:55,039
it's uh

00:25:52,640 --> 00:25:57,840
now now we stick to five as far as i

00:25:55,039 --> 00:25:57,840
remember

00:25:58,320 --> 00:26:01,440
okay uh and one more so what are the

00:26:00,880 --> 00:26:03,760
events

00:26:01,440 --> 00:26:06,400
and or the events and weights on the

00:26:03,760 --> 00:26:09,600
events in the model

00:26:06,400 --> 00:26:11,279
uh events in the model it's the same

00:26:09,600 --> 00:26:12,159
question dom it's basically what is the

00:26:11,279 --> 00:26:13,919
model right like

00:26:12,159 --> 00:26:16,000
what is the model that you're using and

00:26:13,919 --> 00:26:17,760
are you biasing them in any way right so

00:26:16,000 --> 00:26:19,600
yeah it's a full connected uh full

00:26:17,760 --> 00:26:19,919
connected network with the five layers

00:26:19,600 --> 00:26:22,240
but

00:26:19,919 --> 00:26:24,080
uh as i said this can be changed during

00:26:22,240 --> 00:26:26,400
the the next steps so it's

00:26:24,080 --> 00:26:28,320
i would not stick to this one no so i

00:26:26,400 --> 00:26:30,240
guess my question was what did you use

00:26:28,320 --> 00:26:34,640
today right like how did you

00:26:30,240 --> 00:26:38,720
train the model what was what were the

00:26:34,640 --> 00:26:41,840
design criteria uh

00:26:38,720 --> 00:26:43,039
to be honest uh i mean that pastorsia is

00:26:41,840 --> 00:26:44,640
not here

00:26:43,039 --> 00:26:46,240
because she was mostly involved in the

00:26:44,640 --> 00:26:48,720
implementation right now

00:26:46,240 --> 00:26:49,520
so uh i think we can take it offline and

00:26:48,720 --> 00:26:52,559
uh

00:26:49,520 --> 00:26:53,919
if you want we can just uh we can answer

00:26:52,559 --> 00:26:57,760
it by mail

00:26:53,919 --> 00:27:02,080
sounds good okay so it looks like uh

00:26:57,760 --> 00:27:07,360
christian has multiple questions so um

00:27:02,080 --> 00:27:10,159
go ahead christian hi

00:27:07,360 --> 00:27:11,679
okay i the little mic thing is moving so

00:27:10,159 --> 00:27:15,039
i hope you can hear me

00:27:11,679 --> 00:27:16,799
um yeah we can hear you yeah so this is

00:27:15,039 --> 00:27:18,960
pretty interesting to me um

00:27:16,799 --> 00:27:20,000
i'll just let you know i'm the co-chair

00:27:18,960 --> 00:27:23,039
of the uh

00:27:20,000 --> 00:27:24,320
lsr working group in itf so of isis and

00:27:23,039 --> 00:27:27,039
osbf

00:27:24,320 --> 00:27:27,919
um i also probably been working on this

00:27:27,039 --> 00:27:31,200
stuff for about

00:27:27,919 --> 00:27:31,600
20 years and one of the things that we

00:27:31,200 --> 00:27:33,919
did

00:27:31,600 --> 00:27:34,880
you know look at what for my decade at

00:27:33,919 --> 00:27:36,640
cisco we've

00:27:34,880 --> 00:27:38,320
worked particularly on a project for

00:27:36,640 --> 00:27:41,600
fast convergence

00:27:38,320 --> 00:27:42,480
so we were looking at you know um how to

00:27:41,600 --> 00:27:44,880
converge

00:27:42,480 --> 00:27:46,640
the network and end to end so in other

00:27:44,880 --> 00:27:50,159
words all the routers along the path

00:27:46,640 --> 00:27:52,640
right for any failure and we were using

00:27:50,159 --> 00:27:53,679
for to give you real world numbers we

00:27:52,640 --> 00:27:56,480
were using a

00:27:53,679 --> 00:27:58,320
a real world network of customer of ours

00:27:56,480 --> 00:28:01,679
and it was a thousand node network

00:27:58,320 --> 00:28:04,000
a thousand node global network and

00:28:01,679 --> 00:28:06,000
so we when we did all of our

00:28:04,000 --> 00:28:07,520
optimizations that we could we focused

00:28:06,000 --> 00:28:10,080
on this project for many months and

00:28:07,520 --> 00:28:11,919
optimized up and down the stack uh the

00:28:10,080 --> 00:28:14,799
fastest we could get the convergence

00:28:11,919 --> 00:28:16,640
down to was about 120 milliseconds

00:28:14,799 --> 00:28:18,960
uh we didn't advertise that but that's

00:28:16,640 --> 00:28:22,399
what we got um

00:28:18,960 --> 00:28:25,440
and uh of that 120 milliseconds

00:28:22,399 --> 00:28:26,960
uh the full spf calculation was seven

00:28:25,440 --> 00:28:30,080
milliseconds

00:28:26,960 --> 00:28:32,559
right so i you know

00:28:30,080 --> 00:28:33,279
it's there are so many bigger problems i

00:28:32,559 --> 00:28:34,799
i'm not

00:28:33,279 --> 00:28:36,640
not saying that your work isn't isn't

00:28:34,799 --> 00:28:38,159
useful right because knowing how to do

00:28:36,640 --> 00:28:39,919
things faster is

00:28:38,159 --> 00:28:41,600
but you know we you're sort of

00:28:39,919 --> 00:28:43,760
optimizing

00:28:41,600 --> 00:28:44,880
we have bigger much bigger issues the

00:28:43,760 --> 00:28:47,840
biggest one

00:28:44,880 --> 00:28:50,000
um is propagation delay right the amount

00:28:47,840 --> 00:28:51,039
of time that it takes to propagate the

00:28:50,000 --> 00:28:54,320
failure the

00:28:51,039 --> 00:28:55,919
the the new link state uh across the

00:28:54,320 --> 00:28:57,919
thousand node network now you talked

00:28:55,919 --> 00:28:59,279
about that i think you were sort of

00:28:57,919 --> 00:29:01,120
you were getting at that when you said

00:28:59,279 --> 00:29:02,799
congestion problems um

00:29:01,120 --> 00:29:04,399
it's not even i mean it's not even that

00:29:02,799 --> 00:29:04,960
it's just you know our protocols have

00:29:04,399 --> 00:29:07,200
built-in

00:29:04,960 --> 00:29:08,720
uh dampening and whatever we're we're

00:29:07,200 --> 00:29:10,159
working on the actively to actually

00:29:08,720 --> 00:29:13,279
really speed that up

00:29:10,159 --> 00:29:16,399
um but yeah

00:29:13,279 --> 00:29:18,960
and you know so so right so this is

00:29:16,399 --> 00:29:20,080
interesting but maybe not too useful

00:29:18,960 --> 00:29:22,720
right away

00:29:20,080 --> 00:29:24,240
what i um so that that wasn't really a

00:29:22,720 --> 00:29:25,200
question it was just kind of giving you

00:29:24,240 --> 00:29:29,440
some feedback

00:29:25,200 --> 00:29:31,360
um uh the question that i had

00:29:29,440 --> 00:29:33,360
was and where i would think that this

00:29:31,360 --> 00:29:36,880
might be interesting

00:29:33,360 --> 00:29:41,039
uh for current day use would be

00:29:36,880 --> 00:29:44,480
if you were doing something to

00:29:41,039 --> 00:29:46,480
guess guess at failures

00:29:44,480 --> 00:29:48,640
so instead of just trying to optimize

00:29:46,480 --> 00:29:49,840
the spf algorithm right which is pretty

00:29:48,640 --> 00:29:51,679
optimal even though it's

00:29:49,840 --> 00:29:53,279
i mean i don't know what how expensive

00:29:51,679 --> 00:29:54,000
each operation is in the machine

00:29:53,279 --> 00:29:56,399
learning right but

00:29:54,000 --> 00:29:57,679
it's super cheap in dykstra so even

00:29:56,399 --> 00:30:00,000
though it's n squared

00:29:57,679 --> 00:30:00,799
it's still really fast right until n

00:30:00,000 --> 00:30:03,440
gets

00:30:00,799 --> 00:30:05,600
you know unreasonably large right like

00:30:03,440 --> 00:30:08,480
maybe millions or something i don't know

00:30:05,600 --> 00:30:08,799
but um but what i'm what i'm wondering

00:30:08,480 --> 00:30:10,480
is

00:30:08,799 --> 00:30:12,000
can can we do something with machine

00:30:10,480 --> 00:30:15,279
learning where if i see

00:30:12,000 --> 00:30:16,399
a certain failure pattern right like i

00:30:15,279 --> 00:30:19,039
see a link fail

00:30:16,399 --> 00:30:20,880
in one place and you know and because of

00:30:19,039 --> 00:30:22,960
that it's going to shift traffic

00:30:20,880 --> 00:30:24,559
naturally to another link right which

00:30:22,960 --> 00:30:25,520
might because of the way the network is

00:30:24,559 --> 00:30:27,520
currently running

00:30:25,520 --> 00:30:29,679
might cause another failure downstream

00:30:27,520 --> 00:30:30,720
right which then has a cascading effect

00:30:29,679 --> 00:30:32,320
through the network

00:30:30,720 --> 00:30:34,480
it seems to me that that's the type of

00:30:32,320 --> 00:30:35,279
pattern stuff that maybe machine

00:30:34,480 --> 00:30:37,200
learning could

00:30:35,279 --> 00:30:38,960
could uh get at right so you could

00:30:37,200 --> 00:30:40,960
almost be

00:30:38,960 --> 00:30:42,640
predicting a better route because you

00:30:40,960 --> 00:30:44,080
know that if you took the first choice

00:30:42,640 --> 00:30:45,440
spf came up with

00:30:44,080 --> 00:30:47,440
you were going to cause a cascade

00:30:45,440 --> 00:30:49,039
failure and so it just avoided it all

00:30:47,440 --> 00:30:50,399
together and picked a different one like

00:30:49,039 --> 00:30:52,240
maybe a sub-optimal route

00:30:50,399 --> 00:30:54,240
you know i mean that's crazy talk from

00:30:52,240 --> 00:30:55,919
from the chair right don't you it's that

00:30:54,240 --> 00:30:58,080
sub-optimal route sounds like routing

00:30:55,919 --> 00:30:59,039
loops but uh but yeah i mean you know

00:30:58,080 --> 00:31:00,640
that that seems like

00:30:59,039 --> 00:31:02,840
like where the real power of machine

00:31:00,640 --> 00:31:06,480
learning maybe could show up is a better

00:31:02,840 --> 00:31:06,480
path prediction

00:31:06,559 --> 00:31:09,760
yeah have you thought about that yeah

00:31:09,200 --> 00:31:12,320
but

00:31:09,760 --> 00:31:13,120
this is a very good feedback to be

00:31:12,320 --> 00:31:16,320
honest

00:31:13,120 --> 00:31:19,840
uh so you know we tried to

00:31:16,320 --> 00:31:21,840
uh find a way how can we implement

00:31:19,840 --> 00:31:24,799
artificial intelligence machine learning

00:31:21,840 --> 00:31:27,919
in the in in

00:31:24,799 --> 00:31:30,480
reading protocols at all so uh that was

00:31:27,919 --> 00:31:33,919
our first guess that maybe we'll be able

00:31:30,480 --> 00:31:36,960
to you know decrease this uh

00:31:33,919 --> 00:31:40,000
time with uh dixtra so uh

00:31:36,960 --> 00:31:43,120
as you said that uh

00:31:40,000 --> 00:31:47,440
your idea or uh this this could be

00:31:43,120 --> 00:31:50,799
this would require changing our uh

00:31:47,440 --> 00:31:53,279
uh model i think that uh

00:31:50,799 --> 00:31:54,799
we we were focused only on the you know

00:31:53,279 --> 00:31:58,559
decrease the the

00:31:54,799 --> 00:32:00,320
this part with the with the dexter

00:31:58,559 --> 00:32:02,720
algorithm so well

00:32:00,320 --> 00:32:03,360
well and you know i mean also the

00:32:02,720 --> 00:32:05,440
dijkstra

00:32:03,360 --> 00:32:07,039
is used can use a lot of places in graph

00:32:05,440 --> 00:32:09,200
theory right so there might actually be

00:32:07,039 --> 00:32:10,559
applications for this you know maybe not

00:32:09,200 --> 00:32:12,399
in real world

00:32:10,559 --> 00:32:13,679
networks because they're not that large

00:32:12,399 --> 00:32:14,640
even at a thousand nodes they're not

00:32:13,679 --> 00:32:16,320
that large but

00:32:14,640 --> 00:32:18,399
yeah you know there might be other graph

00:32:16,320 --> 00:32:18,720
applications where the numbers get into

00:32:18,399 --> 00:32:20,799
the

00:32:18,720 --> 00:32:22,000
hundreds of thousands where it really

00:32:20,799 --> 00:32:26,799
could make a difference i

00:32:22,000 --> 00:32:28,960
i don't know yeah so as i said we

00:32:26,799 --> 00:32:30,080
we need to test first this on and in a

00:32:28,960 --> 00:32:33,120
real environment

00:32:30,080 --> 00:32:36,320
then we can uh say that we are

00:32:33,120 --> 00:32:38,320
achieved something here and

00:32:36,320 --> 00:32:40,720
but that was a very good feedback i

00:32:38,320 --> 00:32:40,720
would say

00:32:41,440 --> 00:32:48,240
okay so next question uh please share

00:32:44,640 --> 00:32:50,559
the link to your paper uh we will have

00:32:48,240 --> 00:32:54,159
those links on the net dev site

00:32:50,559 --> 00:32:56,720
i guess if you want um an advanced copy

00:32:54,159 --> 00:32:59,440
mate can can post it on the chat if at

00:32:56,720 --> 00:32:59,440
his discretion

00:33:00,080 --> 00:33:03,760
uh sorry uh

00:33:03,919 --> 00:33:09,440
i i i see the chat

00:33:07,120 --> 00:33:11,919
so that that yeah if you will if you

00:33:09,440 --> 00:33:18,000
want um you don't have to

00:33:11,919 --> 00:33:19,919
but uh there's one other question

00:33:18,000 --> 00:33:21,760
to give you a hint this problem looks

00:33:19,919 --> 00:33:24,320
close to

00:33:21,760 --> 00:33:26,000
autonomous autonomous self-driving car

00:33:24,320 --> 00:33:28,960
so if the car driver

00:33:26,000 --> 00:33:31,279
could drive from source to destination

00:33:28,960 --> 00:33:33,360
then a packet could be used to drive

00:33:31,279 --> 00:33:35,440
from source to destination

00:33:33,360 --> 00:33:38,720
so are we doing the same thing that

00:33:35,440 --> 00:33:38,720
self-driving cars are doing

00:33:40,960 --> 00:33:45,039
yeah feedback i don't know this is the

00:33:44,640 --> 00:33:47,840
real

00:33:45,039 --> 00:33:47,840
question uh

00:33:48,799 --> 00:33:52,159
so i wonder maybe it's more if we're

00:33:51,600 --> 00:33:55,440
asking

00:33:52,159 --> 00:33:59,519
to find commonalities and analogies

00:33:55,440 --> 00:34:02,559
in between different processes that

00:33:59,519 --> 00:34:04,480
don't appear to be uh the same thing off

00:34:02,559 --> 00:34:06,399
hand right so

00:34:04,480 --> 00:34:08,800
you know thinking about maybe not so

00:34:06,399 --> 00:34:11,599
much self-driving cars but

00:34:08,800 --> 00:34:12,639
when we look at uh google maps for

00:34:11,599 --> 00:34:16,560
instance

00:34:12,639 --> 00:34:19,359
okay so probably use a similar algorithm

00:34:16,560 --> 00:34:20,000
to to go from point a to point b um in

00:34:19,359 --> 00:34:21,760
an optimal

00:34:20,000 --> 00:34:23,919
fashion so that's that's an easy

00:34:21,760 --> 00:34:27,440
algorithm to implement

00:34:23,919 --> 00:34:29,520
but i think once you start looking at

00:34:27,440 --> 00:34:31,359
uh parameterizations of the algorithms

00:34:29,520 --> 00:34:33,440
and complexities

00:34:31,359 --> 00:34:35,679
uh that's where the ai and machine

00:34:33,440 --> 00:34:38,879
learning becomes interesting right so

00:34:35,679 --> 00:34:41,280
um to get from point a to point b in the

00:34:38,879 --> 00:34:43,679
shortest distance is fixed

00:34:41,280 --> 00:34:45,599
to get from point a to point d b in the

00:34:43,679 --> 00:34:47,760
shortest time

00:34:45,599 --> 00:34:49,040
uh is very variable and that could

00:34:47,760 --> 00:34:50,800
depend heavily on

00:34:49,040 --> 00:34:53,679
a lot of characteristics time of day and

00:34:50,800 --> 00:34:56,000
what have you so it seems like the

00:34:53,679 --> 00:34:58,960
ai and machine learning kicks in once

00:34:56,000 --> 00:35:01,359
the algorithms become

00:34:58,960 --> 00:35:02,079
less deterministic and you have more

00:35:01,359 --> 00:35:06,160
inputs

00:35:02,079 --> 00:35:07,920
and there's some sort of um randomness

00:35:06,160 --> 00:35:09,680
to those inputs that that maybe make it

00:35:07,920 --> 00:35:11,520
a chaotic system

00:35:09,680 --> 00:35:13,599
so do you think uh that's that's where

00:35:11,520 --> 00:35:14,960
all this is going is is going beyond

00:35:13,599 --> 00:35:16,960
just

00:35:14,960 --> 00:35:18,960
um re-implementing the the static

00:35:16,960 --> 00:35:21,440
algorithms but actually coming out with

00:35:18,960 --> 00:35:22,640
with improved algorithms that add in

00:35:21,440 --> 00:35:25,680
some of these

00:35:22,640 --> 00:35:28,839
uh less tangible inputs

00:35:25,680 --> 00:35:31,200
well i i think so so that that this is a

00:35:28,839 --> 00:35:33,119
direction

00:35:31,200 --> 00:35:34,640
however as i said we all not solve the

00:35:33,119 --> 00:35:36,560
congestion problems with the

00:35:34,640 --> 00:35:38,880
with the solution we we just described

00:35:36,560 --> 00:35:42,720
so uh

00:35:38,880 --> 00:35:45,520
we just want to make faster network

00:35:42,720 --> 00:35:46,640
but yes this is also the the the

00:35:45,520 --> 00:35:49,280
direction

00:35:46,640 --> 00:35:51,839
you want to go yeah so everybody wants

00:35:49,280 --> 00:35:54,960
faster networks

00:35:51,839 --> 00:35:57,040
uh let's see so it's a little uh

00:35:54,960 --> 00:35:58,560
sure for large scale networks this could

00:35:57,040 --> 00:36:01,680
be key

00:35:58,560 --> 00:36:03,920
so uh interesting statement so

00:36:01,680 --> 00:36:06,560
i guess the question there is in scaling

00:36:03,920 --> 00:36:09,200
do we do we believe that

00:36:06,560 --> 00:36:10,640
as we scale to larger and larger

00:36:09,200 --> 00:36:12,400
networks

00:36:10,640 --> 00:36:15,359
at some point i think the prediction was

00:36:12,400 --> 00:36:18,640
we'd have over a trillion iot devices

00:36:15,359 --> 00:36:21,520
uh clearly it's only going to get bigger

00:36:18,640 --> 00:36:24,240
do we start to need uh to go beyond the

00:36:21,520 --> 00:36:28,160
fixed algorithms into this sort of

00:36:24,240 --> 00:36:31,920
um ai machine learning algorithms

00:36:28,160 --> 00:36:33,599
yeah i think this is uh for uh

00:36:31,920 --> 00:36:36,160
our solution is designed for a

00:36:33,599 --> 00:36:39,359
large-scale networks i mean uh

00:36:36,160 --> 00:36:44,640
we are aiming into the big

00:36:39,359 --> 00:36:44,640
big data centers so uh yeah

00:36:47,280 --> 00:36:54,320
okay uh thank you

00:36:50,400 --> 00:36:57,200
hi my name is marta

00:36:54,320 --> 00:36:57,760
okay my name is matic and my name is

00:36:57,200 --> 00:37:01,200
piotr

00:36:57,760 --> 00:37:03,920
and we would like to present our

00:37:01,200 --> 00:37:06,880
idea and the results of the research how

00:37:03,920 --> 00:37:09,599
you like to optimize

00:37:06,880 --> 00:37:10,480
network performance using artificial

00:37:09,599 --> 00:37:12,960
intelligence

00:37:10,480 --> 00:37:12,960
methods

00:37:17,119 --> 00:37:22,720
so first of all our

00:37:20,160 --> 00:37:24,800
problem statement and the goal so the

00:37:22,720 --> 00:37:28,079
problem statement is that we

00:37:24,800 --> 00:37:28,960
were identified that under certain

00:37:28,079 --> 00:37:32,160
conditions

00:37:28,960 --> 00:37:35,280
a traffic coming to a system

00:37:32,160 --> 00:37:36,640
can be unevenly balanced even with all

00:37:35,280 --> 00:37:40,320
the features

00:37:36,640 --> 00:37:42,880
in working to spread the traffic

00:37:40,320 --> 00:37:44,480
between the cpus so you would like to

00:37:42,880 --> 00:37:47,920
find an

00:37:44,480 --> 00:37:51,599
efficient way to to spread the

00:37:47,920 --> 00:37:54,720
given ipv4 traffic evenly

00:37:51,599 --> 00:37:58,480
amongst the cpus in the system

00:37:54,720 --> 00:38:01,040
using our receive site scaling

00:37:58,480 --> 00:38:02,000
so first of all we'll go over briefly

00:38:01,040 --> 00:38:05,680
what's

00:38:02,000 --> 00:38:08,320
what's rss what are the problem how it

00:38:05,680 --> 00:38:10,400
works what are the problems with rss and

00:38:08,320 --> 00:38:12,079
i would like to show you how you would

00:38:10,400 --> 00:38:15,200
like to optimize

00:38:12,079 --> 00:38:19,839
a key key generation

00:38:15,200 --> 00:38:19,839
for rss using ai

00:38:20,640 --> 00:38:28,240
first of all uh rss stands for

00:38:24,240 --> 00:38:31,599
receive site scaling and it's a

00:38:28,240 --> 00:38:37,040
technology used in modern network cards

00:38:31,599 --> 00:38:40,400
that allows the received packets to be

00:38:37,040 --> 00:38:42,800
redirected to and balanced to

00:38:40,400 --> 00:38:45,200
uh between the queues and between the

00:38:42,800 --> 00:38:47,680
cpus in the system

00:38:45,200 --> 00:38:49,440
it enables efficient distribution of

00:38:47,680 --> 00:38:53,119
network packets

00:38:49,440 --> 00:38:56,160
uh between the cores and

00:38:53,119 --> 00:38:59,359
it also produces a delay

00:38:56,160 --> 00:39:04,000
uh with in the processing because the

00:38:59,359 --> 00:39:06,960
traffic is spread amongst the cpus

00:39:04,000 --> 00:39:08,720
it can also optimize the software

00:39:06,960 --> 00:39:11,760
processing the

00:39:08,720 --> 00:39:11,760
packets since

00:39:11,839 --> 00:39:15,760
all the packets of a particular collect

00:39:14,320 --> 00:39:19,119
connection or a flow

00:39:15,760 --> 00:39:19,680
are redirected to the same cpu that will

00:39:19,119 --> 00:39:22,480
be

00:39:19,680 --> 00:39:22,880
processing the packets and usually the

00:39:22,480 --> 00:39:26,160
same

00:39:22,880 --> 00:39:28,400
cpu and will process the packets in the

00:39:26,160 --> 00:39:32,160
user space as well so we can

00:39:28,400 --> 00:39:35,599
have the locality of the packet

00:39:32,160 --> 00:39:38,800
from a given flow also

00:39:35,599 --> 00:39:41,280
what's worth to mention is that rss

00:39:38,800 --> 00:39:42,640
while spreading and balancing the

00:39:41,280 --> 00:39:46,240
packets between the

00:39:42,640 --> 00:39:48,960
cpus it it also

00:39:46,240 --> 00:39:50,880
does not break by itself in order

00:39:48,960 --> 00:39:54,560
processing

00:39:50,880 --> 00:39:58,640
so the way the packets are processed

00:39:54,560 --> 00:40:02,320
in the network card which is done

00:39:58,640 --> 00:40:02,720
in order uh while the rss is spreading

00:40:02,320 --> 00:40:06,319
those

00:40:02,720 --> 00:40:09,680
packets among cpus it keeps the in order

00:40:06,319 --> 00:40:12,640
processing so uh

00:40:09,680 --> 00:40:13,760
we don't have reordering uh introduced

00:40:12,640 --> 00:40:17,680
by

00:40:13,760 --> 00:40:22,240
uh rss and also rss is um

00:40:17,680 --> 00:40:26,319
not meant to uh

00:40:22,240 --> 00:40:28,800
um to direct the packet to a specific

00:40:26,319 --> 00:40:29,359
cpu that the user would like to by

00:40:28,800 --> 00:40:32,160
default

00:40:29,359 --> 00:40:33,200
it just it's limited to just spread the

00:40:32,160 --> 00:40:36,640
packets

00:40:33,200 --> 00:40:41,040
and um balance the load

00:40:36,640 --> 00:40:41,040
over the cpus um in the system

00:40:41,119 --> 00:40:49,200
so the quick look how it works

00:40:45,200 --> 00:40:52,000
uh so network card extracts some data

00:40:49,200 --> 00:40:53,599
from the from the packet some correct

00:40:52,000 --> 00:40:56,160
characteristic data

00:40:53,599 --> 00:40:58,000
from the packet that will that can help

00:40:56,160 --> 00:41:01,680
to identify a flow

00:40:58,000 --> 00:41:04,880
uh those are usually ap addresses

00:41:01,680 --> 00:41:06,560
for ipv4 and some additional information

00:41:04,880 --> 00:41:09,920
depending on the protocol like

00:41:06,560 --> 00:41:09,920
source and destination port

00:41:10,240 --> 00:41:14,000
and those are given to the hashing

00:41:12,800 --> 00:41:17,599
function

00:41:14,000 --> 00:41:20,640
which also takes a initial

00:41:17,599 --> 00:41:21,920
value as a hash key and then as a result

00:41:20,640 --> 00:41:25,520
we have a

00:41:21,920 --> 00:41:29,040
uh a hash value which

00:41:25,520 --> 00:41:32,640
then the number of lower

00:41:29,040 --> 00:41:36,000
significant bits of the of the hash

00:41:32,640 --> 00:41:38,000
is taken and

00:41:36,000 --> 00:41:39,599
lookup is being made in the indirection

00:41:38,000 --> 00:41:43,119
table to

00:41:39,599 --> 00:41:44,000
specify the queue and eventually the cpu

00:41:43,119 --> 00:41:46,800
that will

00:41:44,000 --> 00:41:46,800
get the packet

00:41:47,760 --> 00:41:56,880
so for uh tcp over

00:41:51,680 --> 00:41:59,920
ipv4 the input set itself

00:41:56,880 --> 00:42:04,000
is contains both

00:41:59,920 --> 00:42:07,200
destination and source ip addresses and

00:42:04,000 --> 00:42:10,960
destination and source ports

00:42:07,200 --> 00:42:14,640
which totals with 12 bytes

00:42:10,960 --> 00:42:17,839
of the input set for ipv4

00:42:14,640 --> 00:42:22,800
compared to ipv6 which is uh

00:42:17,839 --> 00:42:27,280
for the same type of protocol it's um

00:42:22,800 --> 00:42:28,839
36 bytes extracted from the incoming

00:42:27,280 --> 00:42:33,200
packet

00:42:28,839 --> 00:42:33,200
um for rss

00:42:33,599 --> 00:42:37,280
so uh machi will go over and detail how

00:42:36,560 --> 00:42:41,200
the hashing

00:42:37,280 --> 00:42:41,200
function uh works

00:42:41,520 --> 00:42:45,599
okay so now that we've extracted the

00:42:44,800 --> 00:42:48,480
input set

00:42:45,599 --> 00:42:49,680
we need to run it through some hashing

00:42:48,480 --> 00:42:52,960
function

00:42:49,680 --> 00:42:56,560
the most popular one is stop leads hash

00:42:52,960 --> 00:42:57,520
and as a first step we wanted to

00:42:56,560 --> 00:43:00,720
understand

00:42:57,520 --> 00:43:03,280
better how this hashing method works

00:43:00,720 --> 00:43:04,960
and because that's crucial for key

00:43:03,280 --> 00:43:07,839
optimization

00:43:04,960 --> 00:43:08,240
the principle of top leads hash is that

00:43:07,839 --> 00:43:11,440
for

00:43:08,240 --> 00:43:12,720
every one bit in the input set the hash

00:43:11,440 --> 00:43:16,240
value is

00:43:12,720 --> 00:43:19,760
xor with the masked part of the key

00:43:16,240 --> 00:43:22,480
that corresponds to that bit

00:43:19,760 --> 00:43:23,359
so for the sake of presentation let's

00:43:22,480 --> 00:43:27,359
assume we are

00:43:23,359 --> 00:43:30,880
lazy and we've only extracted four bits

00:43:27,359 --> 00:43:34,319
for the input set and we used the

00:43:30,880 --> 00:43:38,079
so-called standard key for the

00:43:34,319 --> 00:43:41,280
hash function as a first step

00:43:38,079 --> 00:43:45,599
we need to mask

00:43:41,280 --> 00:43:48,480
30 to most significant bits of the key

00:43:45,599 --> 00:43:49,359
and check the bit in the input set in

00:43:48,480 --> 00:43:52,480
this case it's

00:43:49,359 --> 00:43:54,079
one so we need to xor the previous

00:43:52,480 --> 00:43:57,119
hashing result with the

00:43:54,079 --> 00:43:59,760
masked part of the key and save that

00:43:57,119 --> 00:44:03,040
value as a hashing result

00:43:59,760 --> 00:44:03,839
and as a second step we need to shift

00:44:03,040 --> 00:44:06,560
the key

00:44:03,839 --> 00:44:07,119
left or the mask right and check the

00:44:06,560 --> 00:44:10,240
next

00:44:07,119 --> 00:44:10,960
input bit set in this case it's 0 so we

00:44:10,240 --> 00:44:15,599
don't do

00:44:10,960 --> 00:44:18,640
any action and we just move the mask

00:44:15,599 --> 00:44:19,280
to the next part of the key and in this

00:44:18,640 --> 00:44:22,480
case

00:44:19,280 --> 00:44:24,880
the input set we have value of 1

00:44:22,480 --> 00:44:26,560
which means we need to xor the previous

00:44:24,880 --> 00:44:30,319
hashing result with the

00:44:26,560 --> 00:44:33,680
masked key and in the end

00:44:30,319 --> 00:44:36,079
we get the hashing result and as a last

00:44:33,680 --> 00:44:39,200
step we need to

00:44:36,079 --> 00:44:42,560
we have moved the mask forward

00:44:39,200 --> 00:44:46,000
and but the input set value is

00:44:42,560 --> 00:44:49,920
zero the bit is zero which means we

00:44:46,000 --> 00:44:51,119
skip the xor operation and as a result

00:44:49,920 --> 00:44:55,040
we get the hashing

00:44:51,119 --> 00:44:59,280
result which then we need to

00:44:55,040 --> 00:45:03,040
run over the indirection table

00:44:59,280 --> 00:45:03,680
the indirection table is in this case is

00:45:03,040 --> 00:45:07,760
a simple

00:45:03,680 --> 00:45:10,319
four bit indirection table

00:45:07,760 --> 00:45:12,480
so we use the four least significant

00:45:10,319 --> 00:45:13,200
bits of the hash value to select the

00:45:12,480 --> 00:45:16,400
index

00:45:13,200 --> 00:45:19,680
in the in direction table since we

00:45:16,400 --> 00:45:22,640
in this example only assumed to use and

00:45:19,680 --> 00:45:25,440
mapped the queues alternately which is

00:45:22,640 --> 00:45:28,000
the default way of programming the table

00:45:25,440 --> 00:45:31,040
we can see that our incoming pocket will

00:45:28,000 --> 00:45:33,920
go to the queue number one

00:45:31,040 --> 00:45:36,000
we can also see that many different

00:45:33,920 --> 00:45:38,240
hashes will actually go to the same

00:45:36,000 --> 00:45:38,240
queue

00:45:38,640 --> 00:45:42,960
and yeah this is actually the end of the

00:45:41,359 --> 00:45:45,359
rss operation

00:45:42,960 --> 00:45:46,880
and now the nik will start processing

00:45:45,359 --> 00:45:49,599
the next packet

00:45:46,880 --> 00:45:51,119
so the key take os from how the top

00:45:49,599 --> 00:45:54,720
leads hash works

00:45:51,119 --> 00:45:56,720
are first one is that we can

00:45:54,720 --> 00:45:58,480
immediately see that there is a

00:45:56,720 --> 00:46:01,680
connection between the parts of the

00:45:58,480 --> 00:46:04,400
input set and the parts of the key

00:46:01,680 --> 00:46:06,319
this means that we can clearly distinct

00:46:04,400 --> 00:46:07,760
which part of the key will change the

00:46:06,319 --> 00:46:12,160
hash value for

00:46:07,760 --> 00:46:14,560
source ip address those are bytes 0 to 8

00:46:12,160 --> 00:46:16,960
for destination ip address those are

00:46:14,560 --> 00:46:19,920
bytes 4 to 12

00:46:16,960 --> 00:46:21,520
and so on for source port and the

00:46:19,920 --> 00:46:23,680
destination port

00:46:21,520 --> 00:46:25,760
and also we can clearly see that some

00:46:23,680 --> 00:46:28,079
parts of the key

00:46:25,760 --> 00:46:30,319
are used for more than one part of the

00:46:28,079 --> 00:46:31,599
input set for example bytes four to

00:46:30,319 --> 00:46:34,560
eight are used for

00:46:31,599 --> 00:46:36,960
both source ip and the destination ip

00:46:34,560 --> 00:46:39,359
addresses

00:46:36,960 --> 00:46:40,319
and the other problem of the top list

00:46:39,359 --> 00:46:43,920
hash

00:46:40,319 --> 00:46:44,800
is that in fact we are not using the

00:46:43,920 --> 00:46:47,520
whole key

00:46:44,800 --> 00:46:49,280
we only are using a part of the key for

00:46:47,520 --> 00:46:52,800
our hash calculation

00:46:49,280 --> 00:46:54,880
and the size of the key depends on the

00:46:52,800 --> 00:46:58,800
length of the input set

00:46:54,880 --> 00:47:02,640
for example for ip over

00:46:58,800 --> 00:47:06,319
tcp over ipv4 we use 16 bytes

00:47:02,640 --> 00:47:06,640
of the key and for tcp over ipv6 we only

00:47:06,319 --> 00:47:10,000
use

00:47:06,640 --> 00:47:15,119
40 bytes of the input key which is

00:47:10,000 --> 00:47:18,319
usually around 54 or 56 bytes long

00:47:15,119 --> 00:47:21,359
and since top leads hash is

00:47:18,319 --> 00:47:23,280
based on the xor operation and the fact

00:47:21,359 --> 00:47:24,079
that more than one hash value is

00:47:23,280 --> 00:47:27,119
associated

00:47:24,079 --> 00:47:30,240
with the same queue it's not easy to

00:47:27,119 --> 00:47:34,000
find and predict where will our packet

00:47:30,240 --> 00:47:38,000
land after the key change and that makes

00:47:34,000 --> 00:47:42,000
predicting this very hard

00:47:38,000 --> 00:47:45,040
now go back to fiat uh yeah so um

00:47:42,000 --> 00:47:47,760
so rss has has some problems

00:47:45,040 --> 00:47:47,760
so as

00:47:48,400 --> 00:47:52,880
if the incoming traffic has a little

00:47:51,680 --> 00:47:56,480
entropy

00:47:52,880 --> 00:47:59,680
in the input set itself um we can

00:47:56,480 --> 00:48:03,839
we can have poor balancing between

00:47:59,680 --> 00:48:07,119
between the cpus for example with an app

00:48:03,839 --> 00:48:08,880
where we can have a node

00:48:07,119 --> 00:48:11,760
in the network that is receiving the

00:48:08,880 --> 00:48:14,559
packets and it's behind an app then

00:48:11,760 --> 00:48:15,200
most of the most of the pack or all the

00:48:14,559 --> 00:48:18,640
packets

00:48:15,200 --> 00:48:22,240
directed to the to this system are

00:48:18,640 --> 00:48:25,920
have the same destination ip

00:48:22,240 --> 00:48:27,119
so the active input set that takes part

00:48:25,920 --> 00:48:29,359
in the

00:48:27,119 --> 00:48:30,480
in the hash calculation that will

00:48:29,359 --> 00:48:33,839
differentiate the

00:48:30,480 --> 00:48:37,440
hash value from different flows is

00:48:33,839 --> 00:48:39,680
even less than the input set itself

00:48:37,440 --> 00:48:41,359
and then the other example can be a web

00:48:39,680 --> 00:48:44,400
server which also receives

00:48:41,359 --> 00:48:49,680
traffic with the same destination ip and

00:48:44,400 --> 00:48:52,400
destination port so

00:48:49,680 --> 00:48:53,520
again we can have depending on the

00:48:52,400 --> 00:48:57,359
traffic pattern

00:48:53,520 --> 00:49:02,160
we can have poor uh uh traffic balancing

00:48:57,359 --> 00:49:05,359
between the queues and then and the cpus

00:49:02,160 --> 00:49:09,119
um and

00:49:05,359 --> 00:49:12,559
also if we have um

00:49:09,119 --> 00:49:14,480
a lot of zeros and much you mentioned

00:49:12,559 --> 00:49:16,960
that briefly if we have a lot of zeros

00:49:14,480 --> 00:49:19,280
in the significant part of the key

00:49:16,960 --> 00:49:20,640
by the nature of the of the excel

00:49:19,280 --> 00:49:24,240
operation

00:49:20,640 --> 00:49:25,119
uh zero doesn't effectively do anything

00:49:24,240 --> 00:49:28,800
with x or

00:49:25,119 --> 00:49:29,520
operation so the more zeros we have the

00:49:28,800 --> 00:49:33,119
less

00:49:29,520 --> 00:49:36,720
alternation we have in the in the

00:49:33,119 --> 00:49:38,559
final hash value and then uh

00:49:36,720 --> 00:49:41,040
due to the nature of the indirection

00:49:38,559 --> 00:49:44,240
table and

00:49:41,040 --> 00:49:47,359
since more than one

00:49:44,240 --> 00:49:51,440
indirection index can uh

00:49:47,359 --> 00:49:55,440
can end up with the same q id

00:49:51,440 --> 00:49:58,720
for a for different flows

00:49:55,440 --> 00:50:01,520
the traffic can be bunched together

00:49:58,720 --> 00:50:02,400
and many flows can be directed to the

00:50:01,520 --> 00:50:04,720
same

00:50:02,400 --> 00:50:04,720
queue

00:50:05,839 --> 00:50:09,200
we can we can actually try to fix this

00:50:08,000 --> 00:50:11,040
by

00:50:09,200 --> 00:50:12,240
modifying the indirection table but

00:50:11,040 --> 00:50:15,520
anyway

00:50:12,240 --> 00:50:18,800
it needs a intervention

00:50:15,520 --> 00:50:22,000
from the user to to fix that

00:50:18,800 --> 00:50:25,040
and then what's even more

00:50:22,000 --> 00:50:28,000
more complicated is that

00:50:25,040 --> 00:50:30,079
some traffic uh can end up with uh with

00:50:28,000 --> 00:50:33,200
different hash value but

00:50:30,079 --> 00:50:34,640
since we only take a portion of the of

00:50:33,200 --> 00:50:38,240
the

00:50:34,640 --> 00:50:40,880
hash to point to the

00:50:38,240 --> 00:50:44,079
entry in the indirection table then it

00:50:40,880 --> 00:50:47,119
means that we can end up with

00:50:44,079 --> 00:50:49,760
more flows or more connections

00:50:47,119 --> 00:50:50,160
that will have the same more significant

00:50:49,760 --> 00:50:52,640
bits

00:50:50,160 --> 00:50:54,480
of the of the hash so those will end up

00:50:52,640 --> 00:50:57,760
on the same queue

00:50:54,480 --> 00:50:57,760
and we won't be able to

00:50:57,839 --> 00:51:02,480
simply modify the indirection table to

00:50:59,839 --> 00:51:05,599
split those flows from the

00:51:02,480 --> 00:51:06,319
from this particular cube because we

00:51:05,599 --> 00:51:09,599
will move

00:51:06,319 --> 00:51:12,400
all of them at once and probably

00:51:09,599 --> 00:51:14,640
for such a situation we would like to

00:51:12,400 --> 00:51:17,280
somehow split them to

00:51:14,640 --> 00:51:18,400
uh to balance the traffic between

00:51:17,280 --> 00:51:22,160
between the

00:51:18,400 --> 00:51:24,079
different queues also our service itself

00:51:22,160 --> 00:51:26,559
can have some issues

00:51:24,079 --> 00:51:27,920
with uh tunneled and encapsulated

00:51:26,559 --> 00:51:32,000
encapsulated traffic

00:51:27,920 --> 00:51:35,280
depending on the hardware uh

00:51:32,000 --> 00:51:37,760
capabilities and uh the the

00:51:35,280 --> 00:51:38,880
not all hardware can look at the

00:51:37,760 --> 00:51:41,730
innermost

00:51:38,880 --> 00:51:43,280
header to identify the

00:51:41,730 --> 00:51:47,839
[Music]

00:51:43,280 --> 00:51:47,839
input set in the innermost header

00:51:48,800 --> 00:51:55,760
by itself uh so

00:51:52,480 --> 00:51:58,880
we can try to improve uh

00:51:55,760 --> 00:52:00,800
rss and we have couple

00:51:58,880 --> 00:52:02,640
we have a few options we can we can

00:52:00,800 --> 00:52:04,960
modify like i mentioned we can modify

00:52:02,640 --> 00:52:07,680
the indirection table itself

00:52:04,960 --> 00:52:08,640
so we can split and balance the traffic

00:52:07,680 --> 00:52:10,480
a little bit

00:52:08,640 --> 00:52:12,400
this will help if the traffic has the

00:52:10,480 --> 00:52:15,040
same uh

00:52:12,400 --> 00:52:16,160
lower section bits of the of the hash

00:52:15,040 --> 00:52:18,880
because

00:52:16,160 --> 00:52:19,839
like i said before uh we won't be able

00:52:18,880 --> 00:52:23,440
to split those

00:52:19,839 --> 00:52:25,599
uh flows to um

00:52:23,440 --> 00:52:26,720
on a peripheral basis because we will

00:52:25,599 --> 00:52:28,480
move all of them

00:52:26,720 --> 00:52:30,319
and then we will break another

00:52:28,480 --> 00:52:33,920
processing for modified interaction

00:52:30,319 --> 00:52:36,319
uh for modified indirection table values

00:52:33,920 --> 00:52:37,760
and the other option is to modify the

00:52:36,319 --> 00:52:39,920
key itself

00:52:37,760 --> 00:52:41,599
uh this will help to like i said this

00:52:39,920 --> 00:52:44,640
will help to split

00:52:41,599 --> 00:52:45,280
the traffic which has the same hash

00:52:44,640 --> 00:52:48,640
value

00:52:45,280 --> 00:52:52,480
itself the problem is that this will

00:52:48,640 --> 00:52:53,280
also result in a overflow reassociation

00:52:52,480 --> 00:52:56,720
to different

00:52:53,280 --> 00:52:59,040
uh cpus probably

00:52:56,720 --> 00:53:01,599
because we will change the hash values

00:52:59,040 --> 00:53:04,960
for probably all of the

00:53:01,599 --> 00:53:06,160
flows and also we can we can we will

00:53:04,960 --> 00:53:10,319
break

00:53:06,160 --> 00:53:10,319
in order processing of incoming packets

00:53:11,040 --> 00:53:14,319
and we can also modify the input set

00:53:13,040 --> 00:53:17,440
itself but it's

00:53:14,319 --> 00:53:19,520
usually uh well it depends on the

00:53:17,440 --> 00:53:23,359
the hardware is capable of doing that

00:53:19,520 --> 00:53:23,359
and it's not easy to identify

00:53:23,760 --> 00:53:32,240
the input set correctly to to do the

00:53:28,880 --> 00:53:32,640
fur balancing so now marta will go over

00:53:32,240 --> 00:53:35,119
the

00:53:32,640 --> 00:53:35,940
ways that we would like to use to

00:53:35,119 --> 00:53:37,680
optimize the

00:53:35,940 --> 00:53:43,440
[Music]

00:53:37,680 --> 00:53:45,920
key modification for rsf

00:53:43,440 --> 00:53:46,800
um so as theatre and mighty mentioned

00:53:45,920 --> 00:53:49,040
before

00:53:46,800 --> 00:53:51,680
packet flies may not be spread equally

00:53:49,040 --> 00:53:53,359
between cpu cores by the rss under

00:53:51,680 --> 00:53:55,760
certain conditions

00:53:53,359 --> 00:53:57,200
but at the same time a different key

00:53:55,760 --> 00:53:59,599
used by templates

00:53:57,200 --> 00:54:01,920
hash function can potentially fix this

00:53:59,599 --> 00:54:04,160
problem and it can be easily modified

00:54:01,920 --> 00:54:05,839
using one of the standard driver

00:54:04,160 --> 00:54:08,319
functionalities

00:54:05,839 --> 00:54:09,680
it's necessary to keep in mind that each

00:54:08,319 --> 00:54:12,640
key change

00:54:09,680 --> 00:54:13,760
will mainly change the flow of the core

00:54:12,640 --> 00:54:15,680
affinity and

00:54:13,760 --> 00:54:16,800
as a result might degrade the

00:54:15,680 --> 00:54:18,720
performance

00:54:16,800 --> 00:54:21,599
because the application must be

00:54:18,720 --> 00:54:24,640
rescheduled to a different cpu core

00:54:21,599 --> 00:54:27,040
ethernet in general deals with this but

00:54:24,640 --> 00:54:28,400
with the performance penalty it's

00:54:27,040 --> 00:54:31,680
obvious that we should

00:54:28,400 --> 00:54:35,200
avoid it doing this too often

00:54:31,680 --> 00:54:38,640
so we start our journey um

00:54:35,200 --> 00:54:39,680
to find the best key with changing dq to

00:54:38,640 --> 00:54:42,559
a different

00:54:39,680 --> 00:54:44,640
random case the same method is currently

00:54:42,559 --> 00:54:46,480
used by the linux driver

00:54:44,640 --> 00:54:48,000
and the random key value is injected

00:54:46,480 --> 00:54:50,079
with every boot

00:54:48,000 --> 00:54:52,079
it shouldn't be surprising that results

00:54:50,079 --> 00:54:55,440
were also mostly

00:54:52,079 --> 00:54:57,680
random so in the next step we've

00:54:55,440 --> 00:55:00,319
analyzed the results returned by the

00:54:57,680 --> 00:55:01,280
genetic algorithm which is commonly used

00:55:00,319 --> 00:55:03,280
to generate

00:55:01,280 --> 00:55:04,799
high quality solutions to optimization

00:55:03,280 --> 00:55:07,119
and search problems

00:55:04,799 --> 00:55:09,119
by relying on biologically inspired

00:55:07,119 --> 00:55:11,680
operators such as notation

00:55:09,119 --> 00:55:14,079
crossover and selection and we've tried

00:55:11,680 --> 00:55:16,240
many combinations and options

00:55:14,079 --> 00:55:18,480
with surprisingly good results for

00:55:16,240 --> 00:55:20,319
pickup files containing limited number

00:55:18,480 --> 00:55:23,040
of handcrafted flies

00:55:20,319 --> 00:55:23,680
which emulated annuity traffic and only

00:55:23,040 --> 00:55:26,960
differed

00:55:23,680 --> 00:55:28,079
in uh source ap addresses unfortunately

00:55:26,960 --> 00:55:31,599
this solution

00:55:28,079 --> 00:55:34,000
proved not to be scalable and when we

00:55:31,599 --> 00:55:37,040
tried to use it in bigger like

00:55:34,000 --> 00:55:40,079
real life pick up dumps we got stuck

00:55:37,040 --> 00:55:40,960
and we were not able to find the key in

00:55:40,079 --> 00:55:44,799
a very long

00:55:40,960 --> 00:55:48,480
like week long friends and

00:55:44,799 --> 00:55:48,960
at this point we decided to try to focus

00:55:48,480 --> 00:55:51,599
on

00:55:48,960 --> 00:55:53,760
key bits which are more significant than

00:55:51,599 --> 00:55:55,760
the others to narrow down the scope of

00:55:53,760 --> 00:55:57,680
calculations

00:55:55,760 --> 00:56:00,079
that's why we started working on markov

00:55:57,680 --> 00:56:02,480
decision process implementation

00:56:00,079 --> 00:56:04,319
and i really don't want to dive deep

00:56:02,480 --> 00:56:08,559
into technical details

00:56:04,319 --> 00:56:10,480
of this um implementation um

00:56:08,559 --> 00:56:12,559
but at some point we just got an

00:56:10,480 --> 00:56:13,599
impression that we are going to deep and

00:56:12,559 --> 00:56:17,200
maybe it's time

00:56:13,599 --> 00:56:20,799
to try something less complicated like

00:56:17,200 --> 00:56:21,520
neural networks and that may sound a bit

00:56:20,799 --> 00:56:23,520
scary

00:56:21,520 --> 00:56:25,680
but the final solution which we believe

00:56:23,520 --> 00:56:27,839
may help is described as

00:56:25,680 --> 00:56:30,880
bayesian optimization algorithm with the

00:56:27,839 --> 00:56:33,920
usage of on policy prediction

00:56:30,880 --> 00:56:35,920
and with approximation so i

00:56:33,920 --> 00:56:37,839
really don't think we have enough time

00:56:35,920 --> 00:56:39,280
to go into artificial intelligence

00:56:37,839 --> 00:56:41,680
details

00:56:39,280 --> 00:56:43,280
and we would love to discuss it so if

00:56:41,680 --> 00:56:45,839
anyone has any comments

00:56:43,280 --> 00:56:46,480
or questions you can simply reach us

00:56:45,839 --> 00:56:49,359
using

00:56:46,480 --> 00:56:50,000
for example my email address but just

00:56:49,359 --> 00:56:52,160
for now

00:56:50,000 --> 00:56:53,599
i will try to describe the idea behind

00:56:52,160 --> 00:57:00,000
this fancy name

00:56:53,599 --> 00:57:00,000
in a friendly way so um

00:57:00,240 --> 00:57:04,640
proposed solution at this stage uses

00:57:03,599 --> 00:57:07,520
pre-collected

00:57:04,640 --> 00:57:08,559
pickup file and a resource software

00:57:07,520 --> 00:57:11,119
emulator

00:57:08,559 --> 00:57:13,280
to evaluate collected data and then this

00:57:11,119 --> 00:57:15,200
data will be used to train a neural

00:57:13,280 --> 00:57:16,720
network based model of an objective

00:57:15,200 --> 00:57:20,240
function

00:57:16,720 --> 00:57:22,079
and using this um we'll be able to

00:57:20,240 --> 00:57:25,280
determine the best possible

00:57:22,079 --> 00:57:26,640
rss hush key at this stage neural

00:57:25,280 --> 00:57:30,000
network will be trained

00:57:26,640 --> 00:57:31,520
manually by engineers and

00:57:30,000 --> 00:57:33,680
it's important that one of the

00:57:31,520 --> 00:57:35,760
assumption is that the solution works

00:57:33,680 --> 00:57:37,760
offline in user space

00:57:35,760 --> 00:57:38,960
and it's not interrupting platform

00:57:37,760 --> 00:57:42,160
standard rating

00:57:38,960 --> 00:57:45,359
mode until a satisfying key was found or

00:57:42,160 --> 00:57:46,480
established time passed or process was

00:57:45,359 --> 00:57:49,280
interrupted

00:57:46,480 --> 00:57:51,200
and then later user may choose to inject

00:57:49,280 --> 00:57:54,480
new key to his system

00:57:51,200 --> 00:57:56,799
improving traffic bones or or just

00:57:54,480 --> 00:58:00,000
weight with that

00:57:56,799 --> 00:58:02,319
but in a destiny solution

00:58:00,000 --> 00:58:04,160
everything will be automated and will

00:58:02,319 --> 00:58:07,200
happen in cycles

00:58:04,160 --> 00:58:10,799
like in each cycle

00:58:07,200 --> 00:58:12,720
we will like to evaluate each net

00:58:10,799 --> 00:58:16,720
previously evaluated

00:58:12,720 --> 00:58:20,079
uh hash key using rss software

00:58:16,720 --> 00:58:22,960
emulator then we would like to check if

00:58:20,079 --> 00:58:23,680
any of those keys is good enough if yes

00:58:22,960 --> 00:58:27,760
then we are

00:58:23,680 --> 00:58:31,440
done if no i would like to use

00:58:27,760 --> 00:58:33,680
all the data to train neural network

00:58:31,440 --> 00:58:35,839
based model of an objective function

00:58:33,680 --> 00:58:36,799
using the idea of automated neural

00:58:35,839 --> 00:58:38,400
networks

00:58:36,799 --> 00:58:40,240
and it's used to search for the best

00:58:38,400 --> 00:58:43,200
architecture for given

00:58:40,240 --> 00:58:44,319
purpose and the data then we can use

00:58:43,200 --> 00:58:47,680
this model

00:58:44,319 --> 00:58:51,040
to find next

00:58:47,680 --> 00:58:54,400
keys which should be checked in a

00:58:51,040 --> 00:58:57,920
cycle it's also important to mention

00:58:54,400 --> 00:59:02,559
that to reduce the computation time

00:58:57,920 --> 00:59:05,599
model which will be used for initial

00:59:02,559 --> 00:59:06,079
for each initial program will be pretend

00:59:05,599 --> 00:59:08,640
during

00:59:06,079 --> 00:59:11,599
uh research process and this is a very

00:59:08,640 --> 00:59:11,599
popular approach

00:59:14,000 --> 00:59:19,920
saying um

00:59:17,040 --> 00:59:22,000
so i'd like to answer for a question why

00:59:19,920 --> 00:59:24,480
artificial intelligence

00:59:22,000 --> 00:59:26,400
and to answer for this question we have

00:59:24,480 --> 00:59:28,079
to realize that this is a standard

00:59:26,400 --> 00:59:29,920
optimization problem

00:59:28,079 --> 00:59:31,760
of balancing the loads among the

00:59:29,920 --> 00:59:34,480
server's cpu cars

00:59:31,760 --> 00:59:35,440
and it's described but as follows on a

00:59:34,480 --> 00:59:38,559
slide

00:59:35,440 --> 00:59:41,680
where n stands for number of cars and

00:59:38,559 --> 00:59:45,760
l i stands for current load and

00:59:41,680 --> 00:59:49,040
la represents the average

00:59:45,760 --> 00:59:51,520
quite a lot and now when we know

00:59:49,040 --> 00:59:53,359
that this is standard optimization

00:59:51,520 --> 00:59:54,160
problem we can just connect the dots

00:59:53,359 --> 00:59:56,079
case

00:59:54,160 --> 00:59:58,640
complex optimization problems that

00:59:56,079 --> 01:00:01,359
cannot be tracked

00:59:58,640 --> 01:00:03,119
that cannot be solved yet traditional

01:00:01,359 --> 01:00:06,000
mathematical programming

01:00:03,119 --> 01:00:08,880
are commonly solved with artificial

01:00:06,000 --> 01:00:11,359
intelligence based solution approaches

01:00:08,880 --> 01:00:12,319
and this approaches provide optimal

01:00:11,359 --> 01:00:15,680
solutions

01:00:12,319 --> 01:00:18,720
avoiding consuming many computed

01:00:15,680 --> 01:00:19,920
computational resources but on the other

01:00:18,720 --> 01:00:22,400
hand

01:00:19,920 --> 01:00:24,400
they often find local minimums or

01:00:22,400 --> 01:00:29,760
maximums but in many cases

01:00:24,400 --> 01:00:34,079
it's still significant improvement

01:00:29,760 --> 01:00:37,040
so currently we are working on a

01:00:34,079 --> 01:00:40,160
model on this artificial intelligence

01:00:37,040 --> 01:00:44,799
site so we don't have any hard data

01:00:40,160 --> 01:00:48,319
to show yet but i hope that soon we'll

01:00:44,799 --> 01:00:51,520
get back with some results maybe in a

01:00:48,319 --> 01:00:54,079
form of a paper or another presentation

01:00:51,520 --> 01:00:55,760
um but when we're done with the model in

01:00:54,079 --> 01:00:59,119
the next steps

01:00:55,760 --> 01:01:02,160
we would like to think about more

01:00:59,119 --> 01:01:03,760
key generated generators like maybe it

01:01:02,160 --> 01:01:06,880
will be worth to add

01:01:03,760 --> 01:01:10,079
some randomness and maybe to reuse

01:01:06,880 --> 01:01:12,799
already prepared genetic algorithm

01:01:10,079 --> 01:01:13,760
then we'll work on automation

01:01:12,799 --> 01:01:16,720
measurements

01:01:13,760 --> 01:01:17,280
and support for hd6 because currently we

01:01:16,720 --> 01:01:20,799
are

01:01:17,280 --> 01:01:24,559
dealing only with ipv4

01:01:20,799 --> 01:01:27,839
and this is everything from my side

01:01:24,559 --> 01:01:29,599
thank you very much for your attention

01:01:27,839 --> 01:01:32,480
yeah thank you very much for your time

01:01:29,599 --> 01:01:35,520
we we realized that this is a

01:01:32,480 --> 01:01:38,319
work in progress but this is our

01:01:35,520 --> 01:01:39,920
lead in and basically would like to

01:01:38,319 --> 01:01:42,319
stimulate a discussion so

01:01:39,920 --> 01:01:43,359
if you have any feedback comments please

01:01:42,319 --> 01:01:48,319
reach out and

01:01:43,359 --> 01:01:48,319
all feedback will be appreciated thank

01:01:50,839 --> 01:01:57,039
you

01:01:52,240 --> 01:01:59,599
okay turn on my video

01:01:57,039 --> 01:02:01,200
so let's see uh what the questions look

01:01:59,599 --> 01:02:04,319
like

01:02:01,200 --> 01:02:07,440
uh not a whole lot um so i made a couple

01:02:04,319 --> 01:02:09,839
of points and it's interesting that

01:02:07,440 --> 01:02:12,079
packet reordering seems to be coming up

01:02:09,839 --> 01:02:14,799
uh time and time again

01:02:12,079 --> 01:02:16,480
i think uh there must be an obvious

01:02:14,799 --> 01:02:18,400
reason for this is trending on people's

01:02:16,480 --> 01:02:22,640
minds but clearly

01:02:18,400 --> 01:02:25,200
uh if we change the rss key

01:02:22,640 --> 01:02:26,640
or assist mappings continuously yes that

01:02:25,200 --> 01:02:28,720
would generate a lot of

01:02:26,640 --> 01:02:31,520
packet reordering but i'm assuming in

01:02:28,720 --> 01:02:34,720
this case we

01:02:31,520 --> 01:02:36,559
would only do it intermittently and

01:02:34,720 --> 01:02:38,400
hopefully there's some some hold down

01:02:36,559 --> 01:02:39,359
periods but i don't think it's

01:02:38,400 --> 01:02:41,359
reasonable to

01:02:39,359 --> 01:02:42,400
to say we can never change the key

01:02:41,359 --> 01:02:44,400
because

01:02:42,400 --> 01:02:45,760
we want to avoid out of order packets

01:02:44,400 --> 01:02:48,400
completely

01:02:45,760 --> 01:02:51,119
and i know that in some circumstances

01:02:48,400 --> 01:02:55,039
what we've seen in the past was

01:02:51,119 --> 01:02:56,400
a customer would basically run a test

01:02:55,039 --> 01:02:58,240
and if they saw even one

01:02:56,400 --> 01:03:01,119
out of order packet it would be flagged

01:02:58,240 --> 01:03:03,920
in the test as as a problem

01:03:01,119 --> 01:03:06,240
even if it wasn't even if it if it was

01:03:03,920 --> 01:03:08,480
an improvement in overall latency

01:03:06,240 --> 01:03:09,920
uh they test for this so there's some

01:03:08,480 --> 01:03:11,440
assumptions

01:03:09,920 --> 01:03:13,200
i think incorrect assumptions in the

01:03:11,440 --> 01:03:15,119
industry that ip is supposed to be

01:03:13,200 --> 01:03:17,839
somehow in order

01:03:15,119 --> 01:03:19,440
uh and and there's reasons there's valid

01:03:17,839 --> 01:03:22,960
reasons why it isn't not just

01:03:19,440 --> 01:03:26,480
because of the network so

01:03:22,960 --> 01:03:26,480
uh there was a question

01:03:28,240 --> 01:03:33,119
have you compared this approach with rss

01:03:31,920 --> 01:03:38,000
plus plus paper

01:03:33,119 --> 01:03:38,000
presented in 2019

01:03:38,720 --> 01:03:42,240
yes we have actually analyzed rss plus

01:03:41,760 --> 01:03:45,680
plus

01:03:42,240 --> 01:03:47,200
as well but this approach is not solving

01:03:45,680 --> 01:03:50,880
all the problems because in

01:03:47,200 --> 01:03:54,480
some of the examples that we mentioned

01:03:50,880 --> 01:03:58,640
in the talk like the push pop gateways

01:03:54,480 --> 01:04:01,599
for not networks and stuff like this

01:03:58,640 --> 01:04:03,599
are not really the change of the buckets

01:04:01,599 --> 01:04:06,400
that rss plus plus

01:04:03,599 --> 01:04:07,039
suggests is not enough there because if

01:04:06,400 --> 01:04:09,599
you

01:04:07,039 --> 01:04:11,359
have the wrong key you basically don't

01:04:09,599 --> 01:04:15,359
have don't use the entropy

01:04:11,359 --> 01:04:17,680
correctly and as a result you

01:04:15,359 --> 01:04:18,960
no matter how you change the buckets you

01:04:17,680 --> 01:04:22,480
will not get

01:04:18,960 --> 01:04:24,240
better balance of the traffic

01:04:22,480 --> 01:04:26,319
so uh could could you or someone else

01:04:24,240 --> 01:04:28,000
give a short description of what rss

01:04:26,319 --> 01:04:31,599
plus plus is

01:04:28,000 --> 01:04:33,599
yeah rss plus plus generally tries to

01:04:31,599 --> 01:04:36,960
modify the redirection table

01:04:33,599 --> 01:04:39,119
and try to

01:04:36,960 --> 01:04:40,160
check which flows in the redirection

01:04:39,119 --> 01:04:42,400
tables gets

01:04:40,160 --> 01:04:45,119
most of the hits and if there are some

01:04:42,400 --> 01:04:48,240
flows that can be rebalanced

01:04:45,119 --> 01:04:51,599
the rss plus plus changes the affinity

01:04:48,240 --> 01:04:55,760
of the bucket to the core to the queue

01:04:51,599 --> 01:04:59,119
or core basically it's also

01:04:55,760 --> 01:05:02,559
this way i see uh

01:04:59,119 --> 01:05:06,319
yet another spin on um package turn

01:05:02,559 --> 01:05:08,559
yep uh so there was a question

01:05:06,319 --> 01:05:09,839
uh is a patent involved yet with this

01:05:08,559 --> 01:05:13,280
approach

01:05:09,839 --> 01:05:16,640
um not sure that's a technical question

01:05:13,280 --> 01:05:18,160
maybe if um maybe you can

01:05:16,640 --> 01:05:20,559
describe a little bit about what what

01:05:18,160 --> 01:05:22,000
the plan is or or how you intend to move

01:05:20,559 --> 01:05:24,640
forward and what the reality

01:05:22,000 --> 01:05:24,640
is of this

01:05:27,599 --> 01:05:35,359
move forward with this uh idea yeah

01:05:31,599 --> 01:05:38,400
yeah we generally plan to train the

01:05:35,359 --> 01:05:41,520
network and we are looking for

01:05:38,400 --> 01:05:42,480
some real life traffic that we can use

01:05:41,520 --> 01:05:45,039
for that

01:05:42,480 --> 01:05:45,760
so if anyone can share some pickup files

01:05:45,039 --> 01:05:49,760
it would be

01:05:45,760 --> 01:05:51,039
awesome because we were mostly training

01:05:49,760 --> 01:05:54,240
on some

01:05:51,039 --> 01:05:59,680
trying to to run our algorithms

01:05:54,240 --> 01:05:59,680
on some completely artificial

01:05:59,760 --> 01:06:03,280
pickup files that we generated to

01:06:01,839 --> 01:06:05,520
actually simulate the

01:06:03,280 --> 01:06:08,160
issues that we have seen in the real

01:06:05,520 --> 01:06:11,119
like that users reported in the real

01:06:08,160 --> 01:06:11,119
life scenarios

01:06:11,839 --> 01:06:16,000
so when this is running the inference i

01:06:15,119 --> 01:06:19,440
assume it's

01:06:16,000 --> 01:06:22,880
it's adaptive but it's still based on

01:06:19,440 --> 01:06:25,760
the original learning or does it do

01:06:22,880 --> 01:06:25,760
continuous learning

01:06:26,400 --> 01:06:33,839
we plan to do the continuous learning

01:06:30,559 --> 01:06:36,480
it's not the ones off we rather

01:06:33,839 --> 01:06:38,799
that the plan is to actually when you

01:06:36,480 --> 01:06:42,559
see the imbalance you can run the

01:06:38,799 --> 01:06:42,880
script and it will recapture the packets

01:06:42,559 --> 01:06:46,240
and

01:06:42,880 --> 01:06:48,960
re-learn and read and change the key

01:06:46,240 --> 01:06:48,960
accordingly

01:06:50,640 --> 01:06:54,319
okay so so my impression and like i

01:06:52,799 --> 01:06:57,359
mentioned at the beginning

01:06:54,319 --> 01:07:01,039
this is a great case of

01:06:57,359 --> 01:07:04,319
a parameterization that

01:07:01,039 --> 01:07:06,000
we know it's really hard in practice to

01:07:04,319 --> 01:07:08,000
to find the right answer and there may

01:07:06,000 --> 01:07:10,319
not be any one right answer

01:07:08,000 --> 01:07:11,920
so we've seen um even simpler case

01:07:10,319 --> 01:07:16,559
simple cases like how many

01:07:11,920 --> 01:07:18,559
how many cues should we use on a system

01:07:16,559 --> 01:07:20,640
depending on on the number of queues if

01:07:18,559 --> 01:07:22,480
used too many

01:07:20,640 --> 01:07:23,839
that creates problems if you use too few

01:07:22,480 --> 01:07:26,000
that create some problems

01:07:23,839 --> 01:07:27,599
but we always have to take into account

01:07:26,000 --> 01:07:29,200
what the system is used for what the

01:07:27,599 --> 01:07:32,559
load is used for

01:07:29,200 --> 01:07:34,640
and i imagine this sort of of

01:07:32,559 --> 01:07:36,799
concept where we have to parameterize

01:07:34,640 --> 01:07:40,079
based on on real loading

01:07:36,799 --> 01:07:43,200
and real world

01:07:40,079 --> 01:07:44,720
heuristics probably scales to two

01:07:43,200 --> 01:07:47,839
different areas

01:07:44,720 --> 01:07:51,599
so i suspect that

01:07:47,839 --> 01:07:55,039
um as this as this sort of

01:07:51,599 --> 01:07:58,240
uh mentality goes forward uh we would

01:07:55,039 --> 01:08:01,200
continuously expand the

01:07:58,240 --> 01:08:02,640
um hopefully the data set but it seems

01:08:01,200 --> 01:08:04,720
like at some point you need to take into

01:08:02,640 --> 01:08:06,640
account more than just the pcap file we

01:08:04,720 --> 01:08:10,720
also have to consider

01:08:06,640 --> 01:08:10,960
somehow to measure user user experience

01:08:10,720 --> 01:08:14,559
and

01:08:10,960 --> 01:08:18,400
and um latency and

01:08:14,559 --> 01:08:20,880
usability so that's just a comment um

01:08:18,400 --> 01:08:21,440
i think this is uh i think we're on the

01:08:20,880 --> 01:08:24,480
on the

01:08:21,440 --> 01:08:27,679
precipice of a large

01:08:24,480 --> 01:08:27,679
work in this area hopefully

01:08:28,640 --> 01:08:32,159
okay do we have any other things

01:08:32,960 --> 01:08:38,000
go ahead hey how's it going so regarding

01:08:36,000 --> 01:08:39,199
the approaches actually i think it's

01:08:38,000 --> 01:08:42,319
pretty good and

01:08:39,199 --> 01:08:43,759
uh we haven't done a similar like like

01:08:42,319 --> 01:08:45,520
we haven't used machine learning

01:08:43,759 --> 01:08:48,000
for this approach but we have been doing

01:08:45,520 --> 01:08:48,400
like some testing with the hash keys as

01:08:48,000 --> 01:08:49,839
well

01:08:48,400 --> 01:08:51,600
because some of the things that some of

01:08:49,839 --> 01:08:52,640
the challenges that i'm facing from time

01:08:51,600 --> 01:08:54,480
to time

01:08:52,640 --> 01:08:55,839
are around the network security

01:08:54,480 --> 01:08:59,359
monitoring tools like

01:08:55,839 --> 01:09:01,920
ids ips and things like this

01:08:59,359 --> 01:09:03,440
and it's very important to kind of keep

01:09:01,920 --> 01:09:06,319
the flows stitched properly

01:09:03,440 --> 01:09:08,000
and together and parallel that flow as

01:09:06,319 --> 01:09:10,080
much as possible so you can actually

01:09:08,000 --> 01:09:11,600
do as much work as you can on a single

01:09:10,080 --> 01:09:14,080
sensor node um

01:09:11,600 --> 01:09:15,199
rather than scaling uh two multiple

01:09:14,080 --> 01:09:18,239
server architectures

01:09:15,199 --> 01:09:20,080
and of course multiple servers and so

01:09:18,239 --> 01:09:21,359
one thing i was curious about is like

01:09:20,080 --> 01:09:22,880
um you did mention that you're

01:09:21,359 --> 01:09:24,960
handcrafting your packets or you're

01:09:22,880 --> 01:09:27,759
using uh generated packets

01:09:24,960 --> 01:09:28,239
um i have like um i personally have been

01:09:27,759 --> 01:09:29,920
using

01:09:28,239 --> 01:09:31,679
um i'll just call it like you know

01:09:29,920 --> 01:09:34,159
breaking point as a solution

01:09:31,679 --> 01:09:34,719
um to generate um you know data center

01:09:34,159 --> 01:09:36,640
traffic

01:09:34,719 --> 01:09:37,920
and try to figure out like you know

01:09:36,640 --> 01:09:41,040
whether or not

01:09:37,920 --> 01:09:41,920
um the balancing on rss is working

01:09:41,040 --> 01:09:44,880
properly

01:09:41,920 --> 01:09:46,400
and you know if so like you know um i

01:09:44,880 --> 01:09:47,520
actually check the rss queues to see if

01:09:46,400 --> 01:09:49,600
they're properly balanced

01:09:47,520 --> 01:09:51,120
or not and sometimes it's not the case

01:09:49,600 --> 01:09:52,159
because it's all based off of like the

01:09:51,120 --> 01:09:53,679
cash flow and so

01:09:52,159 --> 01:09:55,840
if you're getting more data from like

01:09:53,679 --> 01:09:58,880
you know one uh particular

01:09:55,840 --> 01:09:59,840
um client versus another uh client they

01:09:58,880 --> 01:10:02,719
actually may

01:09:59,840 --> 01:10:03,920
be scaled to like different uh cues how

01:10:02,719 --> 01:10:05,679
are you actually like

01:10:03,920 --> 01:10:07,360
determining whether or not the balancing

01:10:05,679 --> 01:10:09,280
is working appropriately and things like

01:10:07,360 --> 01:10:12,239
that

01:10:09,280 --> 01:10:14,000
all right sir i hi i'm martin i think i

01:10:12,239 --> 01:10:17,440
can answer for this question

01:10:14,000 --> 01:10:20,640
so basically we are just um for now the

01:10:17,440 --> 01:10:24,239
only metric is we are just checking for

01:10:20,640 --> 01:10:26,960
uh like the approximate

01:10:24,239 --> 01:10:29,230
number of packets hitting one of the

01:10:26,960 --> 01:10:32,790
queues and counting

01:10:29,230 --> 01:10:32,790
[Music]

01:10:36,239 --> 01:10:40,080
yeah we basically wrote an rss emulator

01:10:39,679 --> 01:10:43,360
that

01:10:40,080 --> 01:10:46,560
run over the pickup file and then we

01:10:43,360 --> 01:10:48,560
tried to find the average number of

01:10:46,560 --> 01:10:50,000
packets like average then the number

01:10:48,560 --> 01:10:53,440
over the queues that

01:10:50,000 --> 01:10:56,719
we assigned which means we just

01:10:53,440 --> 01:10:57,199
checked the you know how well what was

01:10:56,719 --> 01:11:00,560
the

01:10:57,199 --> 01:11:04,719
average square root mean square

01:11:00,560 --> 01:11:07,360
error between the e q's

01:11:04,719 --> 01:11:09,040
oh god it got everything to minimize

01:11:07,360 --> 01:11:12,239
well that makes sense

01:11:09,040 --> 01:11:14,159
um because the number of flows generated

01:11:12,239 --> 01:11:16,000
by the client server pairs is going to

01:11:14,159 --> 01:11:18,000
greatly like you know impact

01:11:16,000 --> 01:11:20,640
what that interaction table kind of

01:11:18,000 --> 01:11:20,640
looks like right

01:11:21,280 --> 01:11:25,120
what do you mean by how in direction

01:11:23,760 --> 01:11:27,040
table looks like

01:11:25,120 --> 01:11:28,320
because you're going to be hashing off

01:11:27,040 --> 01:11:29,120
i'm assuming you're hashing off of the

01:11:28,320 --> 01:11:32,400
four tuples

01:11:29,120 --> 01:11:35,280
ip um source destination uh port

01:11:32,400 --> 01:11:36,560
all those things and so um if you have

01:11:35,280 --> 01:11:38,159
uh more

01:11:36,560 --> 01:11:40,080
oh i see so if you're having like more

01:11:38,159 --> 01:11:45,280
packets come from a specific

01:11:40,080 --> 01:11:47,120
um uh from a particular client

01:11:45,280 --> 01:11:48,640
i'm wondering if it'll stay balanced if

01:11:47,120 --> 01:11:50,400
it'll actually shift over to like more

01:11:48,640 --> 01:11:52,480
towards like one queue than another

01:11:50,400 --> 01:11:54,159
um how are you like kind of breaking

01:11:52,480 --> 01:11:56,480
that down like you mentioned web server

01:11:54,159 --> 01:11:59,120
traffic as an example

01:11:56,480 --> 01:11:59,920
well if you have one big connection and

01:11:59,120 --> 01:12:02,880
one

01:11:59,920 --> 01:12:05,040
client running a lot of traffic then you

01:12:02,880 --> 01:12:08,159
don't really have any entropy in this

01:12:05,040 --> 01:12:11,440
flow so you can't really rebalance that

01:12:08,159 --> 01:12:14,400
it's more for when you run the web

01:12:11,440 --> 01:12:16,000
server and you for example your system

01:12:14,400 --> 01:12:18,960
generated on the boot

01:12:16,000 --> 01:12:20,000
the key that is not doing a good use of

01:12:18,960 --> 01:12:23,120
the

01:12:20,000 --> 01:12:25,600
entropy in the source ip addresses for

01:12:23,120 --> 01:12:28,000
example

01:12:25,600 --> 01:12:30,719
and then you can rerun this yeah cool

01:12:28,000 --> 01:12:36,800
thank you we appreciate it great work

01:12:30,719 --> 01:12:36,800

YouTube URL: https://www.youtube.com/watch?v=KqYdH7IFo2I


