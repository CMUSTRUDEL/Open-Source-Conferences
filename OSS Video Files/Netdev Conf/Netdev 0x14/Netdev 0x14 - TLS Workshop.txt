Title: Netdev 0x14 - TLS Workshop
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Chair: Tom Herbert

More info: https://netdevconf.info/0x14/session.html?workshop-TLS-performance-and-implementation

Date: Friday, August 14, 2020

Tom Herbert chairs the TLS workshop.

TLS aint cheap on the CPU. The trend on Internet traffic
is indicating that the majority of internet traffic is being
encrypted with TLS.  In other words the most common packets
are using TLS! For this reason, we need to pay more attention
to TLS performance. At 0x14 we have a small TLS festival. And
its all about improving performance.

In the first talk, Pawel Szymanski and Manasi Deval that
assert the claim that you can achive good performance by
letting the CPU do its thing. Use modern CPU instructions
like X86 AESNI.
They run experiments that compare user-mode TLS, Kernel TLS
write and kernel TLS Sendfile to contrast various bottlenecks
in each one with regards to encryption and authentication,
cost of system calls and the memory bandwidth.
They will present their results during the talk.
The talk will also provide some insight on which of the
three approaches is best suited for different type of application
scenarios.

More info:
https://netdevconf.info/0x14/session.html?talk-TLS-performance-characterization-on-modern-x86-CPUs

And in the second TLS talk, Tariq Toukan, Bar Tuaf and
Tal Gilboa discuss offloading of TLS to the NIC.
They start by reviewing the life cycle of a HW
offloaded kTLS connection and the driver-HW interaction
in order to support it.

They will then present their experiments where NginX TLS
activity is offloaded to the Mellanox ConnectX-6Dx NIC
(using mlx5e driver). And finally, they present their experimental
results which show significant performance speed-up gained by
offloading kTLS operations to the HW.

More info:
https://netdevconf.info/0x14/session.html?talk-kTLS-HW-offload-implementation-and-performance-gains

In the third talk, Alexander Krizhanovsky and Ivan Koveshnikov continue
their quest(see netdev 0x12 talk) to investigate and improve TLS
handshake by moving it into the kernel (currently user space handled).

In continuation from 0x12, Alexander and Ivan have been experimenting
with new kernel approaches to reduce some perf culprits involved in
completing a TLS handshake, namely dynamic memory allocation and big
integer initialization. They will present their results which quantify
the new approach. The talk will cover many other perf topics related
to TLS handshake.

More info:
https://netdevconf.info/0x14/session.html?talk-performance-study-of-kernel-TLS-handshakes
Captions: 
	00:00:01,040 --> 00:00:06,480
today we are going to talk about

00:00:03,280 --> 00:00:10,160
uh the kernel uh cheerless handshakes

00:00:06,480 --> 00:00:13,040
uh this is actually the third part of

00:00:10,160 --> 00:00:13,519
discussion uh previously there were uh

00:00:13,040 --> 00:00:16,640
to

00:00:13,519 --> 00:00:19,920
our talks on previous df conferences

00:00:16,640 --> 00:00:23,359
and today we will have a look on why

00:00:19,920 --> 00:00:26,400
uh cannot tell us handshakes matter uh

00:00:23,359 --> 00:00:30,080
we will look on to some benchmarks

00:00:26,400 --> 00:00:32,640
and we'll discuss design proposal for

00:00:30,080 --> 00:00:36,480
the linux mainstream

00:00:32,640 --> 00:00:39,600
uh in our case we develop

00:00:36,480 --> 00:00:41,360
a temperature w which is a application

00:00:39,600 --> 00:00:43,440
delivery controller

00:00:41,360 --> 00:00:44,480
application delivery controllers are

00:00:43,440 --> 00:00:47,520
typically

00:00:44,480 --> 00:00:50,559
http and proxies which also provide you

00:00:47,520 --> 00:00:51,440
a lot of security features like ddos

00:00:50,559 --> 00:00:54,640
protection

00:00:51,440 --> 00:00:57,760
jls or ssl uploading

00:00:54,640 --> 00:00:58,960
web security and so on and typical

00:00:57,760 --> 00:01:02,800
players in the market

00:00:58,960 --> 00:01:06,080
are f5 big ip or fortunate adc

00:01:02,800 --> 00:01:09,360
and the tempest fw is considered to be

00:01:06,080 --> 00:01:12,720
an open source alternative to such

00:01:09,360 --> 00:01:13,840
proprietary appliances we do care about

00:01:12,720 --> 00:01:17,200
high performance

00:01:13,840 --> 00:01:20,240
uh chariot handshakes and

00:01:17,200 --> 00:01:21,600
telescope checks is a typical very

00:01:20,240 --> 00:01:24,720
important

00:01:21,600 --> 00:01:28,080
measurement in check specifications

00:01:24,720 --> 00:01:30,159
of the appliances for example how many

00:01:28,080 --> 00:01:32,159
connections per seconds you can

00:01:30,159 --> 00:01:35,600
establish uh i mean tls

00:01:32,159 --> 00:01:38,640
connections per second or how many https

00:01:35,600 --> 00:01:42,000
transactions per second you can make

00:01:38,640 --> 00:01:45,520
this also very good uh video

00:01:42,000 --> 00:01:48,640
uh from f5 uh guy who

00:01:45,520 --> 00:01:51,680
compared performance of uh big-ip and

00:01:48,640 --> 00:01:55,280
agent x on top of dbtk using

00:01:51,680 --> 00:01:57,920
f stack uh cpm ipa stack

00:01:55,280 --> 00:01:59,360
on top of dpdk inside the virtual

00:01:57,920 --> 00:02:02,560
machine using only

00:01:59,360 --> 00:02:05,920
a cpu and in that

00:02:02,560 --> 00:02:09,360
measurements big-ip wins about 30

00:02:05,920 --> 00:02:12,480
to 50 percent transactions per second

00:02:09,360 --> 00:02:15,760
and just because big-ip uses

00:02:12,480 --> 00:02:18,879
their own uh italian implementation

00:02:15,760 --> 00:02:22,879
so js is very very crucial for us

00:02:18,879 --> 00:02:26,400
however if we talk about generic

00:02:22,879 --> 00:02:29,200
linux users there are cases uh

00:02:26,400 --> 00:02:30,560
where you can benefit from uh fast

00:02:29,200 --> 00:02:33,040
satellites haven't checked

00:02:30,560 --> 00:02:34,480
or kernel jellies can checks in

00:02:33,040 --> 00:02:37,840
particular

00:02:34,480 --> 00:02:40,000
for example there are uh jidos uh

00:02:37,840 --> 00:02:40,959
attacks on chelsea's handshakes and

00:02:40,000 --> 00:02:44,480
everybody

00:02:40,959 --> 00:02:46,080
uh probably can we win from the faster

00:02:44,480 --> 00:02:47,519
handshakes to mitigate some kind of

00:02:46,080 --> 00:02:50,800
attacks and

00:02:47,519 --> 00:02:55,200
uh this particular concern about

00:02:50,800 --> 00:02:58,640
uh security security uh so

00:02:55,200 --> 00:03:01,760
uh actually it does make sense to

00:02:58,640 --> 00:03:04,239
uh separate your uh private

00:03:01,760 --> 00:03:06,080
uh private key and all security

00:03:04,239 --> 00:03:09,519
sensitive data

00:03:06,080 --> 00:03:12,800
outside of your main working threads

00:03:09,519 --> 00:03:17,280
so in varnish case uh

00:03:12,800 --> 00:03:19,760
guys you separate a huge jls blockchain

00:03:17,280 --> 00:03:21,120
outside of the main varnish uh working

00:03:19,760 --> 00:03:24,159
process

00:03:21,120 --> 00:03:27,760
so if you separate a key

00:03:24,159 --> 00:03:31,280
management and all security sensitive uh

00:03:27,760 --> 00:03:34,319
data like uh jls

00:03:31,280 --> 00:03:37,519
session keys and so on outside of the

00:03:34,319 --> 00:03:38,000
uh main working logic and uh actually

00:03:37,519 --> 00:03:40,720
this

00:03:38,000 --> 00:03:41,680
logic is considered to be evolved very

00:03:40,720 --> 00:03:44,400
very quickly

00:03:41,680 --> 00:03:45,120
and you could uh put a lot of bugs into

00:03:44,400 --> 00:03:48,400
production

00:03:45,120 --> 00:03:52,080
uh thanks to the uh quick development

00:03:48,400 --> 00:03:55,360
uh in particular in cloudbleed uh

00:03:52,080 --> 00:03:58,640
case for cloudflare the guy starts the

00:03:55,360 --> 00:04:02,080
blog post from the words that no one

00:03:58,640 --> 00:04:05,200
um client private key were

00:04:02,080 --> 00:04:10,080
compromised thanks to separation

00:04:05,200 --> 00:04:13,120
of the main or working logic from the

00:04:10,080 --> 00:04:16,560
tls termination logic so

00:04:13,120 --> 00:04:20,160
uh security can be generally uh improved

00:04:16,560 --> 00:04:23,199
uh by separation of quality

00:04:20,160 --> 00:04:24,000
management inside of kernel space and

00:04:23,199 --> 00:04:26,880
keeping

00:04:24,000 --> 00:04:28,080
uh the main working logic in user space

00:04:26,880 --> 00:04:31,759
uh

00:04:28,080 --> 00:04:35,280
in worker process besides this

00:04:31,759 --> 00:04:38,880
token there are also good good to have

00:04:35,280 --> 00:04:42,320
faster handshakes you see that even

00:04:38,880 --> 00:04:45,440
fast session resumption can be even more

00:04:42,320 --> 00:04:48,240
faster with kernel class

00:04:45,440 --> 00:04:50,000
uh speaking about uh performance let's

00:04:48,240 --> 00:04:53,440
have a look on

00:04:50,000 --> 00:04:57,759
profile this profile for openness cell

00:04:53,440 --> 00:05:00,800
and nginx with uh nist

00:04:57,759 --> 00:05:03,759
elliptic curve uh 256

00:05:00,800 --> 00:05:04,400
and in the profile we just establish uh

00:05:03,759 --> 00:05:07,840
in this case

00:05:04,400 --> 00:05:08,560
a lot of cherius connections and in the

00:05:07,840 --> 00:05:12,639
profile we

00:05:08,560 --> 00:05:15,199
see that uh most of the codes are about

00:05:12,639 --> 00:05:16,320
uh memory management uh copies they're

00:05:15,199 --> 00:05:19,600
going and so on

00:05:16,320 --> 00:05:22,840
uh genius speaking uh routines uh

00:05:19,600 --> 00:05:24,560
not uh not about cryptography

00:05:22,840 --> 00:05:27,280
mathematics

00:05:24,560 --> 00:05:28,639
also interesting routines are

00:05:27,280 --> 00:05:32,160
interesting in our

00:05:28,639 --> 00:05:35,600
presentation are in red in the side

00:05:32,160 --> 00:05:39,360
and also there are uh blue two routines

00:05:35,600 --> 00:05:41,680
in blue uh second and third it's about

00:05:39,360 --> 00:05:42,560
montgomery multiplication and montgomery

00:05:41,680 --> 00:05:46,320
squaring

00:05:42,560 --> 00:05:46,800
we'll talk more about the routines later

00:05:46,320 --> 00:05:50,400
in the

00:05:46,800 --> 00:05:53,120
presentation but this uh um

00:05:50,400 --> 00:05:54,320
in this site we uh can consider that we

00:05:53,120 --> 00:05:57,440
can dramatically improve

00:05:54,320 --> 00:05:59,120
performance of uh chileans can shakes uh

00:05:57,440 --> 00:06:02,880
just by eliminating

00:05:59,120 --> 00:06:06,639
uh the overhead of memory management

00:06:02,880 --> 00:06:09,759
and the copies and zeroing uh by the way

00:06:06,639 --> 00:06:13,199
uh since um the nist

00:06:09,759 --> 00:06:16,479
uh cove is still uh

00:06:13,199 --> 00:06:17,759
it's a quite old cool curve it's still

00:06:16,479 --> 00:06:22,639
important because

00:06:17,759 --> 00:06:25,680
uh you need uh to use the code for sdsa

00:06:22,639 --> 00:06:29,280
uh certificate so why is the core

00:06:25,680 --> 00:06:32,319
uh still important and it's old uh we

00:06:29,280 --> 00:06:35,360
still uh have have to have the code

00:06:32,319 --> 00:06:38,639
to manage our uh certificates

00:06:35,360 --> 00:06:39,440
and uh the more the mathematical

00:06:38,639 --> 00:06:42,960
evidence are

00:06:39,440 --> 00:06:45,840
involved in the cuff implementations uh

00:06:42,960 --> 00:06:47,120
some of them are just outdated we'll

00:06:45,840 --> 00:06:50,560
talk more about this

00:06:47,120 --> 00:06:53,599
uh later during the presentation we will

00:06:50,560 --> 00:06:54,319
use our two benchmark tools the first

00:06:53,599 --> 00:06:58,560
one

00:06:54,319 --> 00:07:02,000
italia's pf is developed by our team

00:06:58,560 --> 00:07:04,160
basically it just establishes as many

00:07:02,000 --> 00:07:06,160
cherries connections as possible and

00:07:04,160 --> 00:07:09,440
drop the connection

00:07:06,160 --> 00:07:14,840
velcro is very widely used

00:07:09,440 --> 00:07:17,599
on hp benchmarking tool and it also can

00:07:14,840 --> 00:07:20,720
use openness cell

00:07:17,599 --> 00:07:24,160
or another ssl uh libraries for

00:07:20,720 --> 00:07:28,000
challenges and we use a benchmark

00:07:24,160 --> 00:07:31,280
to measure hps transaction performance

00:07:28,000 --> 00:07:33,840
in our performance

00:07:31,280 --> 00:07:35,680
measurements we use a virtual machine

00:07:33,840 --> 00:07:39,520
unfortunately we had no

00:07:35,680 --> 00:07:42,160
uh machine with a virtual api team

00:07:39,520 --> 00:07:44,560
and the physical pair of physical

00:07:42,160 --> 00:07:47,599
machine with very fast uh

00:07:44,560 --> 00:07:51,120
internet connection uh

00:07:47,599 --> 00:07:54,319
so uh to start uh

00:07:51,120 --> 00:07:58,000
to start from from the source code

00:07:54,319 --> 00:07:58,479
uh actually we started champions jls

00:07:58,000 --> 00:08:01,520
from

00:07:58,479 --> 00:08:05,599
mbit uh jellies

00:08:01,520 --> 00:08:09,039
and we moved to mbtls

00:08:05,599 --> 00:08:12,400
uh to the kernel and uh

00:08:09,039 --> 00:08:14,000
we used vtls because of two four main

00:08:12,400 --> 00:08:18,000
factors the first one is it's

00:08:14,000 --> 00:08:21,199
uh very portable we needed only one

00:08:18,000 --> 00:08:23,840
uh human month to move into piano

00:08:21,199 --> 00:08:25,360
the second thing is that mbtos provides

00:08:23,840 --> 00:08:28,960
very serious security

00:08:25,360 --> 00:08:31,840
uh however vtos is too slow

00:08:28,960 --> 00:08:32,719
it's uh that doesn't care about

00:08:31,840 --> 00:08:35,760
performance

00:08:32,719 --> 00:08:37,839
at all and uh there are a lot of

00:08:35,760 --> 00:08:38,880
things which can be improved in which

00:08:37,839 --> 00:08:42,880
areas

00:08:38,880 --> 00:08:45,600
uh to improve our performance on btls we

00:08:42,880 --> 00:08:46,080
develop some of mathematical algorithms

00:08:45,600 --> 00:08:49,760
on

00:08:46,080 --> 00:08:51,120
our own and also we use a world-face

00:08:49,760 --> 00:08:54,160
cell for

00:08:51,120 --> 00:08:57,360
some routines which we can't

00:08:54,160 --> 00:09:01,200
make very better we didn't use

00:08:57,360 --> 00:09:04,880
volt for the sale uh because

00:09:01,200 --> 00:09:07,760
it's very very large and it's

00:09:04,880 --> 00:09:08,720
not so easy to port it into the linux

00:09:07,760 --> 00:09:10,640
kernel

00:09:08,720 --> 00:09:13,040
uh it's very fast and we'll see

00:09:10,640 --> 00:09:16,240
benchmarks for wi-fi today

00:09:13,040 --> 00:09:16,800
uh but there might be some uh security

00:09:16,240 --> 00:09:20,480
issues

00:09:16,800 --> 00:09:23,839
uh in the library and we i also

00:09:20,480 --> 00:09:27,200
cover um issues with datum

00:09:23,839 --> 00:09:30,320
uh so um speaking about mbtls

00:09:27,200 --> 00:09:32,240
uh we let's start from the benchmark of

00:09:30,320 --> 00:09:34,560
the original and bt areas

00:09:32,240 --> 00:09:35,519
are being ported into the linux kernel

00:09:34,560 --> 00:09:38,320
and

00:09:35,519 --> 00:09:38,959
our current implementation uh which is

00:09:38,320 --> 00:09:41,360
that

00:09:38,959 --> 00:09:42,000
the original implementation is about 30

00:09:41,360 --> 00:09:44,560
times

00:09:42,000 --> 00:09:46,080
slower than current uh temperature yes

00:09:44,560 --> 00:09:49,279
code

00:09:46,080 --> 00:09:52,800
this is just a proof how vtos

00:09:49,279 --> 00:09:56,560
so uh the next thing is

00:09:52,800 --> 00:09:59,920
let's see how how we uh compare with

00:09:56,560 --> 00:10:00,320
uh current open se and nginx performance

00:09:59,920 --> 00:10:03,360
this

00:10:00,320 --> 00:10:05,360
benchmark on a virtual machine

00:10:03,360 --> 00:10:07,760
which is that uh current impressive

00:10:05,360 --> 00:10:10,160
challenges is about 40

00:10:07,760 --> 00:10:11,040
percent uh better performance in

00:10:10,160 --> 00:10:13,839
connections

00:10:11,040 --> 00:10:14,800
js connections per second and uh

00:10:13,839 --> 00:10:18,079
provides about

00:10:14,800 --> 00:10:20,880
40 times lower latency also

00:10:18,079 --> 00:10:22,560
in leverage also if we see the best

00:10:20,880 --> 00:10:25,440
cases for performance peak

00:10:22,560 --> 00:10:27,040
and the latency we also provide that

00:10:25,440 --> 00:10:32,079
better numbers

00:10:27,040 --> 00:10:34,560
uh the next thing is about um

00:10:32,079 --> 00:10:35,600
the previous slide was about uh full

00:10:34,560 --> 00:10:39,680
challenges

00:10:35,600 --> 00:10:43,040
um handshake but this one is about

00:10:39,680 --> 00:10:44,640
uh cheerless session resumption in

00:10:43,040 --> 00:10:46,399
this case we provide about eighty

00:10:44,640 --> 00:10:49,040
percent battery performance

00:10:46,399 --> 00:10:50,959
and the same legacy as nginx and

00:10:49,040 --> 00:10:56,160
openness ssa

00:10:50,959 --> 00:10:56,160
however in our tests we observed

00:10:56,800 --> 00:11:04,640
spikes or huge spikes of latency

00:11:00,480 --> 00:11:09,519
and this github issue which we need to

00:11:04,640 --> 00:11:09,519
work on more in our implementation

00:11:09,920 --> 00:11:14,959
we also compared different linux kernel

00:11:14,160 --> 00:11:19,680
versions

00:11:14,959 --> 00:11:22,800
uh 4.14 and 5.7

00:11:19,680 --> 00:11:23,839
in uh chelya session resumption and we

00:11:22,800 --> 00:11:27,360
see that

00:11:23,839 --> 00:11:30,800
those kernels provide a bit

00:11:27,360 --> 00:11:31,360
higher performance this mostly because

00:11:30,800 --> 00:11:34,560
of

00:11:31,360 --> 00:11:37,760
uh recent uh

00:11:34,560 --> 00:11:42,399
attacks mitigation in

00:11:37,760 --> 00:11:44,880
intel cpus uh also

00:11:42,399 --> 00:11:46,240
speaking about um the recent cpu

00:11:44,880 --> 00:11:49,360
vulnerabilities we

00:11:46,240 --> 00:11:52,639
assumed that kpti will uh

00:11:49,360 --> 00:11:54,800
impart performance a lot however it

00:11:52,639 --> 00:11:56,399
actually isn't the case and we didn't

00:11:54,800 --> 00:11:59,279
observe more than four

00:11:56,399 --> 00:12:00,000
percent of performance degradation uh

00:11:59,279 --> 00:12:02,800
with uh

00:12:00,000 --> 00:12:04,240
kpti enabled and this uh pretty

00:12:02,800 --> 00:12:07,279
different from

00:12:04,240 --> 00:12:10,480
what mariadb observed but hdbr

00:12:07,279 --> 00:12:13,360
uh server observed up to 40 percent

00:12:10,480 --> 00:12:17,040
performance degradation with kpti

00:12:13,360 --> 00:12:20,959
uh actually the reason for this

00:12:17,040 --> 00:12:24,880
is that uh tls handshakes don't involve

00:12:20,959 --> 00:12:27,760
so many system calls

00:12:24,880 --> 00:12:29,760
uh either for network io or memory

00:12:27,760 --> 00:12:33,040
allocation or

00:12:29,760 --> 00:12:36,639
random uh generation

00:12:33,040 --> 00:12:40,480
uh this slide is uh maybe a

00:12:36,639 --> 00:12:43,760
kind of obvious but it's always good to

00:12:40,480 --> 00:12:47,920
see some numbers this uh how

00:12:43,760 --> 00:12:53,839
chelya's handshake impacts the

00:12:47,920 --> 00:12:53,839
hp as transaction performance

00:12:53,920 --> 00:13:02,079
this uh in this slide uh previously we

00:12:58,959 --> 00:13:05,120
discussed uh how the network

00:13:02,079 --> 00:13:05,920
performance differs in open cell case

00:13:05,120 --> 00:13:09,440
and generics

00:13:05,920 --> 00:13:12,720
or for infestations but in this uh

00:13:09,440 --> 00:13:14,079
to see uh how the cryptography

00:13:12,720 --> 00:13:18,160
mathematic uh

00:13:14,079 --> 00:13:20,800
is fast uh this completion of benchmark

00:13:18,160 --> 00:13:22,480
results for openness sale of welfare

00:13:20,800 --> 00:13:25,200
sale and invested areas

00:13:22,480 --> 00:13:26,639
we see that what if we say uh seems the

00:13:25,200 --> 00:13:30,320
fastest one

00:13:26,639 --> 00:13:33,360
uh however this uh this

00:13:30,320 --> 00:13:36,560
question why the cdh are

00:13:33,360 --> 00:13:41,279
so fast than the cdsa

00:13:36,560 --> 00:13:43,680
typically cghe uses

00:13:41,279 --> 00:13:45,279
unknown point multiplication i will

00:13:43,680 --> 00:13:48,480
describe about this

00:13:45,279 --> 00:13:49,600
math a bit later uh it means that each

00:13:48,480 --> 00:13:52,720
cdsa

00:13:49,600 --> 00:13:55,760
uh can be optimized uh for

00:13:52,720 --> 00:13:59,040
uh fixed point multiplication why cdhe

00:13:55,760 --> 00:14:01,519
uh cannot be optimized in this way

00:13:59,040 --> 00:14:03,199
and we actually see for open cell case

00:14:01,519 --> 00:14:06,480
that cgh e

00:14:03,199 --> 00:14:09,519
is much slower than csa however it's

00:14:06,480 --> 00:14:11,920
this not the case for world for sale

00:14:09,519 --> 00:14:12,800
if we look at the temperature numbers

00:14:11,920 --> 00:14:17,120
which is that

00:14:12,800 --> 00:14:20,240
the cgh also slows slower than cdsa

00:14:17,120 --> 00:14:22,240
uh however if we compare uh team pasta

00:14:20,240 --> 00:14:23,680
benchmark results with wordpress cell

00:14:22,240 --> 00:14:28,480
and open cell

00:14:23,680 --> 00:14:31,519
uh actually the results aren't um

00:14:28,480 --> 00:14:33,600
completely fair the reason is that word

00:14:31,519 --> 00:14:36,959
for cell and open cell

00:14:33,600 --> 00:14:40,000
uh measure only uh

00:14:36,959 --> 00:14:43,199
only ecgs uh say sign

00:14:40,000 --> 00:14:46,240
ncdhi uh secret key

00:14:43,199 --> 00:14:49,360
generation but in case of uh

00:14:46,240 --> 00:14:49,760
team pasta benchmark we uh beach markets

00:14:49,360 --> 00:14:52,240
uh

00:14:49,760 --> 00:14:52,959
home mathematic operations involve

00:14:52,240 --> 00:14:56,000
including

00:14:52,959 --> 00:14:59,360
ephemeral case generation uh it's

00:14:56,000 --> 00:15:02,639
maybe uh not so

00:14:59,360 --> 00:15:05,839
dramatic for a cdsa but this

00:15:02,639 --> 00:15:10,639
is absolutely dramatic for a cage in

00:15:05,839 --> 00:15:14,320
ecghj we in our case we

00:15:10,639 --> 00:15:17,279
execute uh as much as uh two more

00:15:14,320 --> 00:15:19,199
uh logic uh as uh in comparison with

00:15:17,279 --> 00:15:22,480
world for cell or openc

00:15:19,199 --> 00:15:26,160
this because uh in our case we have to

00:15:22,480 --> 00:15:30,000
perform uh two point multiplication

00:15:26,160 --> 00:15:35,440
instead of only one uh

00:15:30,000 --> 00:15:35,440
in um mean time while we

00:15:35,519 --> 00:15:41,120
probably were not so bad in uh low uh

00:15:38,880 --> 00:15:42,160
performance uh mathematical performance

00:15:41,120 --> 00:15:44,480
comparison

00:15:42,160 --> 00:15:46,959
against open source and voip said we

00:15:44,480 --> 00:15:49,440
know that our mathematic

00:15:46,959 --> 00:15:50,320
still isn't uh perfect and we need to

00:15:49,440 --> 00:15:54,399
work more

00:15:50,320 --> 00:15:57,199
uh however even if he is not so uh

00:15:54,399 --> 00:15:57,519
super optimized mathematically which is

00:15:57,199 --> 00:15:59,920
that

00:15:57,519 --> 00:16:01,199
investor jls can deliver much more

00:15:59,920 --> 00:16:05,600
performance than

00:16:01,199 --> 00:16:08,800
openness sale uh this uh exactly because

00:16:05,600 --> 00:16:11,199
of reducing memory copies

00:16:08,800 --> 00:16:12,320
contract switches no system calls for

00:16:11,199 --> 00:16:15,120
network io

00:16:12,320 --> 00:16:15,600
less message queues on circuit io and so

00:16:15,120 --> 00:16:17,920
on

00:16:15,600 --> 00:16:19,040
all the things which we discussed on the

00:16:17,920 --> 00:16:23,360
one of the first

00:16:19,040 --> 00:16:26,399
slides with nginx and open cell profiles

00:16:23,360 --> 00:16:30,079
speaking about elliptic of

00:16:26,399 --> 00:16:35,199
mathematics i want to

00:16:30,079 --> 00:16:38,240
reference more is nist elliptic off

00:16:35,199 --> 00:16:41,600
five r256 and this uh very nice

00:16:38,240 --> 00:16:45,440
um uh paper from uh guyron

00:16:41,600 --> 00:16:49,279
and kastnav by 2014.

00:16:45,440 --> 00:16:51,199
it's pretty old however this

00:16:49,279 --> 00:16:52,639
the paper describes the real

00:16:51,199 --> 00:16:56,000
implementation of

00:16:52,639 --> 00:16:59,279
current open cell implementation uh

00:16:56,000 --> 00:17:02,160
in the most simple case uh the most

00:16:59,279 --> 00:17:05,919
expensive operation in elliptic of

00:17:02,160 --> 00:17:09,280
course is uh to multiply uh point p

00:17:05,919 --> 00:17:12,799
on uh scalar m scalar m is always

00:17:09,280 --> 00:17:16,319
a security sensitive for some secret uh

00:17:12,799 --> 00:17:19,360
p uh is secret for ecghe

00:17:16,319 --> 00:17:22,880
and the fixed point for cgsa

00:17:19,360 --> 00:17:26,720
uh in uh the most uh straightforward

00:17:22,880 --> 00:17:30,840
uh algorithm we just iterate each bit of

00:17:26,720 --> 00:17:34,720
the scalar which is uh 256

00:17:30,840 --> 00:17:37,919
bits uh and for each bit we

00:17:34,720 --> 00:17:41,360
perform one point doubling and uh

00:17:37,919 --> 00:17:46,640
one half of the cases we uh perform

00:17:41,360 --> 00:17:49,840
point addition the uh second layer of

00:17:46,640 --> 00:17:50,799
mathematics uh actually there are

00:17:49,840 --> 00:17:54,720
several

00:17:50,799 --> 00:17:57,440
uh layers in mathematic construction the

00:17:54,720 --> 00:17:58,160
first layer is our point multiplication

00:17:57,440 --> 00:18:02,080
the second

00:17:58,160 --> 00:18:05,919
one is point doubling and additional

00:18:02,080 --> 00:18:08,240
uh after that there uh

00:18:05,919 --> 00:18:09,600
we you can make a choice in which

00:18:08,240 --> 00:18:12,960
coordinate system you

00:18:09,600 --> 00:18:15,039
uh you prefer to work uh

00:18:12,960 --> 00:18:16,960
perform a point doubling and radiation

00:18:15,039 --> 00:18:20,160
could be jacobian coordinates

00:18:16,960 --> 00:18:21,200
fin cabinets that schedule coordinates

00:18:20,160 --> 00:18:24,320
and so on

00:18:21,200 --> 00:18:29,360
um and after that uh

00:18:24,320 --> 00:18:33,039
you have uh operations uh

00:18:29,360 --> 00:18:36,160
operations on big integers and they also

00:18:33,039 --> 00:18:39,919
have modular reduction on the top layer

00:18:36,160 --> 00:18:42,320
of algorithms we just saw

00:18:39,919 --> 00:18:43,520
the most straightforward implementation

00:18:42,320 --> 00:18:46,480
however there are

00:18:43,520 --> 00:18:47,200
recent uh research uh exactly in the

00:18:46,480 --> 00:18:50,160
point

00:18:47,200 --> 00:18:51,280
of multiplication however open and cell

00:18:50,160 --> 00:18:54,480
and welfare say

00:18:51,280 --> 00:18:57,200
don't use their approach uh

00:18:54,480 --> 00:18:58,000
point dublin conditions uh seems the

00:18:57,200 --> 00:19:01,039
same for

00:18:58,000 --> 00:19:01,520
all the crypto libraries uh also it

00:19:01,039 --> 00:19:03,600
seems

00:19:01,520 --> 00:19:04,720
all the libraries use jacobian

00:19:03,600 --> 00:19:07,919
coordinates

00:19:04,720 --> 00:19:08,559
and if you use uh jacobian coordinates

00:19:07,919 --> 00:19:11,520
you need

00:19:08,559 --> 00:19:13,360
a modular inversion module inversion is

00:19:11,520 --> 00:19:17,919
the second most expensive

00:19:13,360 --> 00:19:19,360
mathematical operation after a point

00:19:17,919 --> 00:19:22,559
multiplication actually point

00:19:19,360 --> 00:19:26,000
multiplication includes model inversion

00:19:22,559 --> 00:19:28,400
uh and there's also very recent research

00:19:26,000 --> 00:19:31,440
from bench stain about fast model

00:19:28,400 --> 00:19:32,559
uh inversion we use uh this uh

00:19:31,440 --> 00:19:35,919
algorithms

00:19:32,559 --> 00:19:39,280
with some variations after that we

00:19:35,919 --> 00:19:40,799
may decide which uh model reduction we

00:19:39,280 --> 00:19:43,440
can use there

00:19:40,799 --> 00:19:43,840
montgomery reduction used by warfare

00:19:43,440 --> 00:19:46,960
cell

00:19:43,840 --> 00:19:50,559
and open cell and fips

00:19:46,960 --> 00:19:53,679
reduction used originally by mbtlis

00:19:50,559 --> 00:19:56,880
at the moment uh we use fips

00:19:53,679 --> 00:20:00,080
uh model reduction we applied uh

00:19:56,880 --> 00:20:00,640
all the research which we found to speed

00:20:00,080 --> 00:20:04,320
up the

00:20:00,640 --> 00:20:07,440
model reduction uh in particular the

00:20:04,320 --> 00:20:07,919
work from both but it seems this uh dead

00:20:07,440 --> 00:20:10,960
end

00:20:07,919 --> 00:20:12,159
and probably will move to montgomery

00:20:10,960 --> 00:20:15,520
reduction

00:20:12,159 --> 00:20:19,600
at some point um oh

00:20:15,520 --> 00:20:20,720
no mathematic layers and for example if

00:20:19,600 --> 00:20:24,159
you use

00:20:20,720 --> 00:20:28,159
uh first module inversion then uh

00:20:24,159 --> 00:20:31,520
you run less

00:20:28,159 --> 00:20:34,480
scalar multiplication or scalar uh

00:20:31,520 --> 00:20:34,960
squaring in this this algorithms this

00:20:34,480 --> 00:20:38,000
means

00:20:34,960 --> 00:20:41,039
uh in terms that you can

00:20:38,000 --> 00:20:43,520
use different model reduction maybe a

00:20:41,039 --> 00:20:44,320
model reduction with smaller heads than

00:20:43,520 --> 00:20:48,960
montgomery

00:20:44,320 --> 00:20:52,000
but uh costly at the end like fips

00:20:48,960 --> 00:20:52,480
so if you change one of the layer or of

00:20:52,000 --> 00:20:55,440
the

00:20:52,480 --> 00:20:56,880
elliptical algorithms you typically need

00:20:55,440 --> 00:20:59,520
to adjust all the

00:20:56,880 --> 00:21:00,720
layers above and below or this

00:20:59,520 --> 00:21:04,159
particular layer to

00:21:00,720 --> 00:21:08,400
make very balanced and well optimized

00:21:04,159 --> 00:21:11,600
implementation uh for example

00:21:08,400 --> 00:21:14,480
uh as an example of such

00:21:11,600 --> 00:21:15,200
optimization we can consider protection

00:21:14,480 --> 00:21:17,440
against

00:21:15,200 --> 00:21:18,720
side channel attacks there are a number

00:21:17,440 --> 00:21:20,799
of different

00:21:18,720 --> 00:21:22,799
side channel attacks like timing attacks

00:21:20,799 --> 00:21:26,080
power novelizers and so on

00:21:22,799 --> 00:21:28,799
and usually uh cryptography uh

00:21:26,080 --> 00:21:30,159
libraries use different approaches to

00:21:28,799 --> 00:21:33,280
protect against

00:21:30,159 --> 00:21:36,880
such an attack the first one is to use

00:21:33,280 --> 00:21:40,240
and construct constant time algorithms

00:21:36,880 --> 00:21:43,280
by design the second one is to

00:21:40,240 --> 00:21:47,520
have a non-constant time algorithm

00:21:43,280 --> 00:21:50,000
uh into add uh additional damien

00:21:47,520 --> 00:21:50,559
operations which can make the algorithm

00:21:50,000 --> 00:21:53,840
uh

00:21:50,559 --> 00:21:56,240
constant time essentially

00:21:53,840 --> 00:21:57,679
and the last approach is to use uh point

00:21:56,240 --> 00:22:00,880
randomization so

00:21:57,679 --> 00:22:04,000
if we uh randomize our calculations

00:22:00,880 --> 00:22:06,640
then the attacker can cannot uh make

00:22:04,000 --> 00:22:08,559
uh assumption what was the secret

00:22:06,640 --> 00:22:12,080
because we randomized it

00:22:08,559 --> 00:22:15,520
uh and in this point uh for example

00:22:12,080 --> 00:22:19,520
if we uh use the technique for

00:22:15,520 --> 00:22:22,799
model inversion of the benchtime

00:22:19,520 --> 00:22:26,159
benzene algorithm we can

00:22:22,799 --> 00:22:28,559
run up to three times less number of

00:22:26,159 --> 00:22:33,440
iterations in comparison with

00:22:28,559 --> 00:22:33,440
original constant time algorithm

00:22:34,320 --> 00:22:40,960
actually modern cpus

00:22:37,600 --> 00:22:44,480
provide regions insertion

00:22:40,960 --> 00:22:47,600
which allows you uh to get random

00:22:44,480 --> 00:22:51,039
values very very quickly so if we

00:22:47,600 --> 00:22:54,640
uh move from constant time algorithms to

00:22:51,039 --> 00:22:56,000
point randomization point randomization

00:22:54,640 --> 00:23:00,000
and non-constant time

00:22:56,000 --> 00:23:01,280
algorithms uh using the led land

00:23:00,000 --> 00:23:04,960
instructions means that

00:23:01,280 --> 00:23:08,320
tpu can go much much faster

00:23:04,960 --> 00:23:11,440
however unfortunately there are recent

00:23:08,320 --> 00:23:13,440
attacks against their instruction and

00:23:11,440 --> 00:23:16,640
mitigation against their attacks

00:23:13,440 --> 00:23:19,919
cost us about uh 97

00:23:16,640 --> 00:23:24,880
persons of performance

00:23:19,919 --> 00:23:28,400
uh the next topic about

00:23:24,880 --> 00:23:30,960
sca is memory usage actually

00:23:28,400 --> 00:23:31,840
different libraries uh use different

00:23:30,960 --> 00:23:37,280
approaches

00:23:31,840 --> 00:23:40,880
uh to compute for example cdsa

00:23:37,280 --> 00:23:43,360
cgsa actually as i

00:23:40,880 --> 00:23:44,640
mentioned before csa allows you to

00:23:43,360 --> 00:23:47,360
pre-compute

00:23:44,640 --> 00:23:48,559
uh some data for fixed point

00:23:47,360 --> 00:23:52,000
multiplication

00:23:48,559 --> 00:23:55,279
and with uh uses very small uh table

00:23:52,000 --> 00:23:58,320
relatively small of eight uh kilobytes

00:23:55,279 --> 00:23:59,520
and dynamically computed openness cell

00:23:58,320 --> 00:24:02,840
and volt facil

00:23:59,520 --> 00:24:06,159
use very similar tables of 150

00:24:02,840 --> 00:24:08,960
kilobytes and the um

00:24:06,159 --> 00:24:09,919
open cell and the vtls uh uses full

00:24:08,960 --> 00:24:11,440
table scan

00:24:09,919 --> 00:24:13,360
on each iteration of point

00:24:11,440 --> 00:24:14,559
multiplication uh the point

00:24:13,360 --> 00:24:18,080
multiplication algorithm

00:24:14,559 --> 00:24:21,520
uh uses about 36 uh iteration

00:24:18,080 --> 00:24:26,640
it means that all open cell and bts

00:24:21,520 --> 00:24:30,159
uh scans the whole tables about 36

00:24:26,640 --> 00:24:32,720
times uh also vtls uh uses

00:24:30,159 --> 00:24:34,000
uh point randomization uh for more

00:24:32,720 --> 00:24:37,520
security

00:24:34,000 --> 00:24:38,799
but warfare sale uh just uses direct

00:24:37,520 --> 00:24:41,760
access

00:24:38,799 --> 00:24:42,720
uh to the pre-computed values the worst

00:24:41,760 --> 00:24:46,960
thing is that

00:24:42,720 --> 00:24:50,640
uh the table is successful

00:24:46,960 --> 00:24:53,760
depending on the security values of the

00:24:50,640 --> 00:24:57,600
secret it means that uh measuring

00:24:53,760 --> 00:25:00,960
the time of access times

00:24:57,600 --> 00:25:01,679
to the table you can uh you you can

00:25:00,960 --> 00:25:05,760
reveal

00:25:01,679 --> 00:25:09,360
some secret bits from them

00:25:05,760 --> 00:25:12,720
secret scholar and having that

00:25:09,360 --> 00:25:15,600
we use a very large table which is

00:25:12,720 --> 00:25:16,320
much larger than first level of data

00:25:15,600 --> 00:25:20,080
cache

00:25:16,320 --> 00:25:23,600
it's probably not so hard to measure

00:25:20,080 --> 00:25:27,200
different access times uh we

00:25:23,600 --> 00:25:29,039
created a security issue security report

00:25:27,200 --> 00:25:32,240
for world for sale for this

00:25:29,039 --> 00:25:36,000
non-constant time success

00:25:32,240 --> 00:25:38,960
uh one of the uh most uh

00:25:36,000 --> 00:25:40,240
performance uh crucial part of nvidia

00:25:38,960 --> 00:25:43,520
less why it's so

00:25:40,240 --> 00:25:45,400
slow is uh managing uh

00:25:43,520 --> 00:25:46,720
big integer also known as

00:25:45,400 --> 00:25:50,400
multi-precision

00:25:46,720 --> 00:25:53,520
integers uh also we have apis in the

00:25:50,400 --> 00:25:54,960
linux kernel and most of the crypto

00:25:53,520 --> 00:25:58,960
libraries actually use

00:25:54,960 --> 00:26:02,559
uh mpis however opencl wordpress cell

00:25:58,960 --> 00:26:07,279
and current wireguard don't use

00:26:02,559 --> 00:26:10,559
apis and hotpath but embed cos uses

00:26:07,279 --> 00:26:13,600
guys everywhere apis for example

00:26:10,559 --> 00:26:17,039
in our case of ni tl curve

00:26:13,600 --> 00:26:20,480
is a large integer

00:26:17,039 --> 00:26:23,679
of 32 bits which is

00:26:20,480 --> 00:26:26,880
four longs and

00:26:23,679 --> 00:26:30,799
working with mpis you need to manage uh

00:26:26,880 --> 00:26:34,000
the data structure or like allocated

00:26:30,799 --> 00:26:36,640
uh size of buyers how

00:26:34,000 --> 00:26:37,200
or how many actual buyers are used to

00:26:36,640 --> 00:26:40,320
the sign

00:26:37,200 --> 00:26:43,520
and so on uh so

00:26:40,320 --> 00:26:46,159
uh in mbtl yes uh

00:26:43,520 --> 00:26:48,000
well uh elliptic off uh computation

00:26:46,159 --> 00:26:50,559
involves uh hundreds of

00:26:48,000 --> 00:26:52,159
mpis in each round it means that you

00:26:50,559 --> 00:26:54,640
need to

00:26:52,159 --> 00:26:55,360
allocate and then survive hundreds of

00:26:54,640 --> 00:26:58,799
this small

00:26:55,360 --> 00:26:59,600
data data structures so we significantly

00:26:58,799 --> 00:27:02,320
optimized

00:26:59,600 --> 00:27:04,080
a bit less approach by introducing uh

00:27:02,320 --> 00:27:07,840
memory pools

00:27:04,080 --> 00:27:10,640
which are actually just uh like uh

00:27:07,840 --> 00:27:12,640
static uh snapshots of all the

00:27:10,640 --> 00:27:15,840
elliptical computation means that

00:27:12,640 --> 00:27:18,559
at uh startup we allocate uh

00:27:15,840 --> 00:27:20,320
contiguous uh memory pages with all

00:27:18,559 --> 00:27:23,840
allocated and terrorized

00:27:20,320 --> 00:27:24,720
uh mpis and we when we go to handshake

00:27:23,840 --> 00:27:28,480
we just

00:27:24,720 --> 00:27:32,000
uh copy their whole uh pages

00:27:28,480 --> 00:27:35,120
uh in a stream uh fashion instead of

00:27:32,000 --> 00:27:36,480
installizing and copying apis are

00:27:35,120 --> 00:27:39,679
separately

00:27:36,480 --> 00:27:41,120
uh we still use uh memory pools but not

00:27:39,679 --> 00:27:44,240
so heavily because we

00:27:41,120 --> 00:27:47,520
mostly move from mpis to

00:27:44,240 --> 00:27:49,279
low uh integer computations just like

00:27:47,520 --> 00:27:52,080
another

00:27:49,279 --> 00:27:52,640
libraries so we are approaching end of

00:27:52,080 --> 00:27:56,559
this

00:27:52,640 --> 00:27:59,120
presentation with a proposal uh for the

00:27:56,559 --> 00:28:00,320
scanner inclusion of the canon tls

00:27:59,120 --> 00:28:04,000
implementation

00:28:00,320 --> 00:28:07,120
uh this is uh example of proposed api

00:28:04,000 --> 00:28:10,880
for circuit api more details

00:28:07,120 --> 00:28:13,840
will be described in our paper and this

00:28:10,880 --> 00:28:15,520
link to our github issue when we

00:28:13,840 --> 00:28:19,440
appreciate you to

00:28:15,520 --> 00:28:22,480
comment the api design

00:28:19,440 --> 00:28:26,000
and propose some

00:28:22,480 --> 00:28:29,279
additions or requests for the api

00:28:26,000 --> 00:28:33,440
uh typically we propose to

00:28:29,279 --> 00:28:35,039
load public key with a certificate and

00:28:33,440 --> 00:28:38,159
private key using existing

00:28:35,039 --> 00:28:41,679
ad key api so we create

00:28:38,159 --> 00:28:42,640
a separate key link for each pair of

00:28:41,679 --> 00:28:46,000
certificate

00:28:42,640 --> 00:28:49,120
and private key next we

00:28:46,000 --> 00:28:52,080
create a normal uh circuit

00:28:49,120 --> 00:28:52,960
and make a set scope just like contact

00:28:52,080 --> 00:28:56,399
elias

00:28:52,960 --> 00:28:59,520
and in the sets so we point out the

00:28:56,399 --> 00:29:03,200
key required carrying cypher suit

00:28:59,520 --> 00:29:06,480
and the cheerleaders version next accept

00:29:03,200 --> 00:29:08,559
system call will return you not only

00:29:06,480 --> 00:29:10,720
tcp connected circuit but also the

00:29:08,559 --> 00:29:13,919
socket with

00:29:10,720 --> 00:29:17,520
established uh cls connection this

00:29:13,919 --> 00:29:21,279
uh question how to fall back

00:29:17,520 --> 00:29:24,960
uh from uh if we not able to

00:29:21,279 --> 00:29:27,039
uh establish uh handshake to user space

00:29:24,960 --> 00:29:28,880
about this night and so on the other

00:29:27,039 --> 00:29:32,640
that will be described in

00:29:28,880 --> 00:29:36,159
paper and in github issue

00:29:32,640 --> 00:29:39,360
uh we propose the server side only

00:29:36,159 --> 00:29:42,480
implementation because servers are

00:29:39,360 --> 00:29:46,240
it seems uh service will benefit uh

00:29:42,480 --> 00:29:49,360
mostly from the uh in kernel inclusion

00:29:46,240 --> 00:29:51,919
uh next we propose to

00:29:49,360 --> 00:29:53,760
uh perform future less handshakes in

00:29:51,919 --> 00:29:57,760
software queue just like

00:29:53,760 --> 00:30:00,320
tcp handshakes this will improve

00:29:57,760 --> 00:30:01,360
overall throughput and reduce the

00:30:00,320 --> 00:30:06,159
latency

00:30:01,360 --> 00:30:06,159
and also while uh soft eq

00:30:06,399 --> 00:30:11,440
bypasses a bunch of network packets we

00:30:08,960 --> 00:30:14,960
can make only one fpu context

00:30:11,440 --> 00:30:18,080
uh uh storing and uh restoring

00:30:14,960 --> 00:30:19,440
and concepts of uh for the whole a batch

00:30:18,080 --> 00:30:22,480
of

00:30:19,440 --> 00:30:25,679
jls handshakes we made the

00:30:22,480 --> 00:30:28,880
micro benchmark uh showing how important

00:30:25,679 --> 00:30:31,919
uh to make exactly one uh fpu

00:30:28,880 --> 00:30:36,640
uh save and restore for batch of

00:30:31,919 --> 00:30:40,000
uh network packets uh in software queue

00:30:36,640 --> 00:30:43,039
session so it's just

00:30:40,000 --> 00:30:46,159
have no point to

00:30:43,039 --> 00:30:49,840
to save and restore fpu context

00:30:46,159 --> 00:30:53,039
for each packets or each uh

00:30:49,840 --> 00:30:57,200
jls handshake message

00:30:53,039 --> 00:31:00,640
um actually most of the code uh

00:30:57,200 --> 00:31:04,080
for charis can shakes can be found in uh

00:31:00,640 --> 00:31:06,399
current kernel linux kernel

00:31:04,080 --> 00:31:07,519
uh for example this symmetric keys

00:31:06,399 --> 00:31:11,120
management this

00:31:07,519 --> 00:31:14,399
ad key api the curve uh

00:31:11,120 --> 00:31:18,000
25 5 19 sa

00:31:14,399 --> 00:31:20,640
and maybe almost all

00:31:18,000 --> 00:31:21,279
the symmetric uh crypto algorithms are

00:31:20,640 --> 00:31:24,399
already in

00:31:21,279 --> 00:31:27,760
the linux girl so it uh allows us

00:31:24,399 --> 00:31:30,000
to introduce only about thirteen

00:31:27,760 --> 00:31:33,519
thousand lines of code official

00:31:30,000 --> 00:31:38,240
uh cherry state machine cherries tickets

00:31:33,519 --> 00:31:42,000
the nist elliptic roof and the

00:31:38,240 --> 00:31:42,000
logic for cyphex fields

00:31:42,640 --> 00:31:49,760
so before going to upstream we

00:31:46,080 --> 00:31:53,679
are planning to finish this uh tasks

00:31:49,760 --> 00:31:56,480
the first one is to finish our

00:31:53,679 --> 00:31:57,519
work with performance optimization of an

00:31:56,480 --> 00:32:00,640
ict

00:31:57,519 --> 00:32:04,559
carve the second one is we

00:32:00,640 --> 00:32:07,919
want to go to upstream with tls 1.3

00:32:04,559 --> 00:32:10,320
and also this task to

00:32:07,919 --> 00:32:11,440
match our current hls implementation

00:32:10,320 --> 00:32:13,519
with the kernel

00:32:11,440 --> 00:32:14,880
asymmetric keys api because at the

00:32:13,519 --> 00:32:18,640
moment this

00:32:14,880 --> 00:32:21,760
wasn't done yet uh that's all we love

00:32:18,640 --> 00:32:22,559
to hear from you or if you can benefit

00:32:21,760 --> 00:32:26,159
from

00:32:22,559 --> 00:32:29,039
uh the kernel jellies handshakes

00:32:26,159 --> 00:32:29,919
in particular uh if you can benefit even

00:32:29,039 --> 00:32:32,640
on

00:32:29,919 --> 00:32:33,360
one point uh two uh challenges can

00:32:32,640 --> 00:32:36,960
checks and

00:32:33,360 --> 00:32:40,080
don't need 1.3 uh also

00:32:36,960 --> 00:32:41,519
we'd love to see your feedback about our

00:32:40,080 --> 00:32:44,320
api

00:32:41,519 --> 00:32:46,000
some implementation requests and so on

00:32:44,320 --> 00:32:49,519
and also we will

00:32:46,000 --> 00:32:53,200
happy to receive your questions on

00:32:49,519 --> 00:32:57,519
our email or github so that's all

00:32:53,200 --> 00:33:00,640
thank you

00:32:57,519 --> 00:33:04,080
okay uh thank you uh

00:33:00,640 --> 00:33:06,640
so looks like i don't see any questions

00:33:04,080 --> 00:33:07,279
um if anyone has any i'll give a few

00:33:06,640 --> 00:33:11,039
seconds

00:33:07,279 --> 00:33:14,159
but um we'll go ahead and proceed to the

00:33:11,039 --> 00:33:14,159
next talk otherwise

00:33:16,559 --> 00:33:24,880
uh totally have a comment from

00:33:20,720 --> 00:33:25,279
um from our site that uh unfortunately i

00:33:24,880 --> 00:33:28,880
had

00:33:25,279 --> 00:33:31,919
not enough time to uh say

00:33:28,880 --> 00:33:32,799
speak more about uh the circuit api

00:33:31,919 --> 00:33:36,480
proposal for

00:33:32,799 --> 00:33:37,760
cannot s uh handshakes so i appreciate

00:33:36,480 --> 00:33:41,519
if you can visit

00:33:37,760 --> 00:33:44,960
our github issue one for free free

00:33:41,519 --> 00:33:47,440
uh to comment and read more about the

00:33:44,960 --> 00:33:48,240
particular technical proposal for the

00:33:47,440 --> 00:33:50,320
api

00:33:48,240 --> 00:33:51,360
the commands are very important for us

00:33:50,320 --> 00:33:55,600
to make the

00:33:51,360 --> 00:34:00,080
right uh design of the api and

00:33:55,600 --> 00:34:01,919
make something useful for other people

00:34:00,080 --> 00:34:04,240
so uh what forum do you think the

00:34:01,919 --> 00:34:04,559
discussion would take place on the apis

00:34:04,240 --> 00:34:08,000
is

00:34:04,559 --> 00:34:10,079
this one net dev or somewhere else uh we

00:34:08,000 --> 00:34:11,280
we had uh internally we had a lot of

00:34:10,079 --> 00:34:14,240
discussions about

00:34:11,280 --> 00:34:15,119
uh which features of uh challenge can

00:34:14,240 --> 00:34:18,000
shakes we

00:34:15,119 --> 00:34:18,480
have to support for example uh how to

00:34:18,000 --> 00:34:22,000
manage

00:34:18,480 --> 00:34:25,119
snipes how how snice

00:34:22,000 --> 00:34:29,040
should be matched we propose cbpf for

00:34:25,119 --> 00:34:32,159
custom matching for snice there are many

00:34:29,040 --> 00:34:35,359
issues with uh js api

00:34:32,159 --> 00:34:40,079
like uh sometimes for

00:34:35,359 --> 00:34:42,960
the same as 9 you need to load different

00:34:40,079 --> 00:34:43,599
certificates and private key in the case

00:34:42,960 --> 00:34:47,040
if the

00:34:43,599 --> 00:34:50,639
shi's shy

00:34:47,040 --> 00:34:52,879
certificate was revoked so uh

00:34:50,639 --> 00:34:54,000
if we speak on engineering level uh

00:34:52,879 --> 00:34:57,359
probably it's not so

00:34:54,000 --> 00:35:00,160
complex but overall api and

00:34:57,359 --> 00:35:01,200
the features to support uh certification

00:35:00,160 --> 00:35:04,320
management they are

00:35:01,200 --> 00:35:07,520
quite complex okay

00:35:04,320 --> 00:35:09,599
so uh so we'll look for that

00:35:07,520 --> 00:35:11,280
but let's go ahead and move on to the

00:35:09,599 --> 00:35:14,000
next talk thank you

00:35:11,280 --> 00:35:14,000
hey thank you

00:35:14,480 --> 00:35:20,079
my name is pawel schmanski and

00:35:17,680 --> 00:35:21,680
today i would like to present the

00:35:20,079 --> 00:35:24,400
results of my

00:35:21,680 --> 00:35:25,440
performance characterization experiments

00:35:24,400 --> 00:35:29,760
that

00:35:25,440 --> 00:35:33,440
i did with my colleague manasi deval

00:35:29,760 --> 00:35:35,119
so let's look on the agenda of the

00:35:33,440 --> 00:35:37,839
presentation

00:35:35,119 --> 00:35:38,960
uh first we'll start with some

00:35:37,839 --> 00:35:42,000
background about

00:35:38,960 --> 00:35:46,079
tls next i will talk about

00:35:42,000 --> 00:35:49,079
test setup and

00:35:46,079 --> 00:35:50,800
also give you results of the performance

00:35:49,079 --> 00:35:53,440
characterization

00:35:50,800 --> 00:35:54,800
and we'll have some short summary at the

00:35:53,440 --> 00:35:58,400
end

00:35:54,800 --> 00:36:01,760
so starting with some

00:35:58,400 --> 00:36:07,680
basic knowledge about tls so

00:36:01,760 --> 00:36:10,240
this is a most commonly used

00:36:07,680 --> 00:36:12,320
network protocol that provides privacy

00:36:10,240 --> 00:36:15,680
and data integrity

00:36:12,320 --> 00:36:19,119
it runs on on top of

00:36:15,680 --> 00:36:22,480
layer 3 protocols

00:36:19,119 --> 00:36:26,880
like tcp or udp

00:36:22,480 --> 00:36:29,200
and consists of two sub protocols

00:36:26,880 --> 00:36:31,119
the first one is uh the handshake

00:36:29,200 --> 00:36:34,320
protocol with

00:36:31,119 --> 00:36:37,920
which deals with negotiating

00:36:34,320 --> 00:36:41,119
the security parameters of the

00:36:37,920 --> 00:36:44,400
tls connection like

00:36:41,119 --> 00:36:46,160
crypt algorithms encryption keys and

00:36:44,400 --> 00:36:49,680
things like that

00:36:46,160 --> 00:36:53,359
the other sub protocol is

00:36:49,680 --> 00:36:55,040
record protocol and it is responsible

00:36:53,359 --> 00:36:58,720
for fragmenting

00:36:55,040 --> 00:37:02,320
the application data into records

00:36:58,720 --> 00:37:06,400
protecting them with crypto

00:37:02,320 --> 00:37:10,000
algorithm and then transmitting

00:37:06,400 --> 00:37:11,520
encrypted data over uh transport

00:37:10,000 --> 00:37:14,720
protocol

00:37:11,520 --> 00:37:17,839
and uh uh tls supports uh

00:37:14,720 --> 00:37:21,440
multiple cryptographic algorithm

00:37:17,839 --> 00:37:22,480
but in this presentation and in general

00:37:21,440 --> 00:37:26,160
in my

00:37:22,480 --> 00:37:26,160
uh in my experiments

00:37:26,880 --> 00:37:34,880
i focused on aes gcm

00:37:30,800 --> 00:37:38,320
encryption uh because

00:37:34,880 --> 00:37:41,760
this is uh the algorithm that

00:37:38,320 --> 00:37:46,079
could be uh improved by using

00:37:41,760 --> 00:37:49,680
special aesni instructions available on

00:37:46,079 --> 00:37:53,359
uh intel x86

00:37:49,680 --> 00:37:56,720
cpus uh

00:37:53,359 --> 00:38:00,079
so here you can see

00:37:56,720 --> 00:38:03,839
uh two uh options or

00:38:00,079 --> 00:38:06,880
two possibilities to to implement

00:38:03,839 --> 00:38:11,359
tls the first one i call

00:38:06,880 --> 00:38:11,359
user space tls and

00:38:12,000 --> 00:38:17,520
in this option all the tls functionality

00:38:16,160 --> 00:38:21,040
is implemented in a

00:38:17,520 --> 00:38:24,560
dls library in user space

00:38:21,040 --> 00:38:27,599
hence the name user space tls

00:38:24,560 --> 00:38:31,280
and the second

00:38:27,599 --> 00:38:34,640
option is ktls or kernel tls

00:38:31,280 --> 00:38:38,400
where the handshake protocol

00:38:34,640 --> 00:38:40,960
is still implemented in tls library

00:38:38,400 --> 00:38:42,880
but the record protocol is implemented

00:38:40,960 --> 00:38:46,079
inside the

00:38:42,880 --> 00:38:49,839
kernel and also the

00:38:46,079 --> 00:38:54,800
tls record protocol implementation

00:38:49,839 --> 00:38:58,000
uh uses a crypto algorithm

00:38:54,800 --> 00:39:02,800
module to to perform actual data

00:38:58,000 --> 00:39:02,800
encryption and decryption

00:39:04,000 --> 00:39:12,480
uh when when we uh

00:39:07,920 --> 00:39:17,839
look into a typical http

00:39:12,480 --> 00:39:22,079
uh server implementation

00:39:17,839 --> 00:39:25,680
the data flow when handling

00:39:22,079 --> 00:39:29,680
http get

00:39:25,680 --> 00:39:35,359
request looks like in this

00:39:29,680 --> 00:39:35,359
slide so basically after receiving the

00:39:35,599 --> 00:39:37,839
the

00:39:38,880 --> 00:39:45,040
http get request the

00:39:42,000 --> 00:39:49,200
server sends a

00:39:45,040 --> 00:39:49,839
file to read cisco to request kernel to

00:39:49,200 --> 00:39:52,880
read the

00:39:49,839 --> 00:39:56,480
the file from

00:39:52,880 --> 00:39:59,200
storage device so the

00:39:56,480 --> 00:40:00,079
file content is first copied to the

00:39:59,200 --> 00:40:02,880
kernel to

00:40:00,079 --> 00:40:04,480
the page cache inside kernel and next it

00:40:02,880 --> 00:40:08,079
it is copied

00:40:04,480 --> 00:40:11,680
into buffers in user space

00:40:08,079 --> 00:40:13,680
then the tls library performs the

00:40:11,680 --> 00:40:15,839
cryptographic operation and the

00:40:13,680 --> 00:40:20,319
encrypted

00:40:15,839 --> 00:40:24,640
data is sent from user space buffer

00:40:20,319 --> 00:40:29,440
to kernel using socket right

00:40:24,640 --> 00:40:29,440
cisco and finally it gets

00:40:29,520 --> 00:40:36,560
sent to network

00:40:33,040 --> 00:40:40,560
interface so this is this is what

00:40:36,560 --> 00:40:43,680
happens in case of user space tls

00:40:40,560 --> 00:40:47,760
and incur

00:40:43,680 --> 00:40:47,760
in case of kernel tls

00:40:48,480 --> 00:40:56,640
the only difference is that

00:40:52,880 --> 00:40:57,760
the data is not encrypted in user space

00:40:56,640 --> 00:41:02,400
it is still

00:40:57,760 --> 00:41:03,119
transferred to to buffers in user space

00:41:02,400 --> 00:41:07,440
but it's

00:41:03,119 --> 00:41:11,119
not encrypted in user space but it is

00:41:07,440 --> 00:41:14,560
uh sent to

00:41:11,119 --> 00:41:19,040
kernel still using socket write

00:41:14,560 --> 00:41:19,040
cisco and then it is

00:41:19,440 --> 00:41:26,480
again encrypted

00:41:22,560 --> 00:41:30,640
and sent to the network interface

00:41:26,480 --> 00:41:34,000
however with kernel tls there is a

00:41:30,640 --> 00:41:38,880
another option possible which

00:41:34,000 --> 00:41:42,400
i call a kernel tls send file

00:41:38,880 --> 00:41:45,440
flow so in this case

00:41:42,400 --> 00:41:48,480
the http server uses

00:41:45,440 --> 00:41:51,599
send file system call

00:41:48,480 --> 00:41:55,280
and that this system call a request a

00:41:51,599 --> 00:41:56,319
kernel to read data from the storage

00:41:55,280 --> 00:42:01,040
device

00:41:56,319 --> 00:42:02,220
and sends a content of a file

00:42:01,040 --> 00:42:04,319
um in

00:42:02,220 --> 00:42:07,890
[Music]

00:42:04,319 --> 00:42:09,359
tls or in general you know

00:42:07,890 --> 00:42:13,920
[Music]

00:42:09,359 --> 00:42:13,920
tcp connection

00:42:14,319 --> 00:42:18,800
and as you can see the data is not

00:42:18,880 --> 00:42:26,640
transferred to user space so it is

00:42:22,480 --> 00:42:26,640
read from storage device to

00:42:26,800 --> 00:42:34,000
page cache inside kernel

00:42:30,400 --> 00:42:39,359
next the tls module inside kernel

00:42:34,000 --> 00:42:42,400
encrypts the data and then the encrypted

00:42:39,359 --> 00:42:46,319
data is sent over

00:42:42,400 --> 00:42:49,839
network interface so in fact we have uh

00:42:46,319 --> 00:42:54,079
three options uh possible

00:42:49,839 --> 00:42:58,160
to to implement uh tls the first one

00:42:54,079 --> 00:43:01,359
is user space the second one is

00:42:58,160 --> 00:43:05,359
uh ktls with uh writes

00:43:01,359 --> 00:43:08,720
call and the third one is uh ktls with

00:43:05,359 --> 00:43:12,319
send file cisco and

00:43:08,720 --> 00:43:12,319
uh in our

00:43:12,440 --> 00:43:15,440
characterization performance

00:43:14,400 --> 00:43:18,720
characterization

00:43:15,440 --> 00:43:22,240
experiments we were focused on

00:43:18,720 --> 00:43:25,599
comparing these three

00:43:22,240 --> 00:43:26,160
scenarios these three implementation

00:43:25,599 --> 00:43:30,240
options

00:43:26,160 --> 00:43:30,240
with two

00:43:31,040 --> 00:43:35,280
scenarios the first scenario was

00:43:34,000 --> 00:43:38,720
something we

00:43:35,280 --> 00:43:44,160
named a simple web server

00:43:38,720 --> 00:43:44,160
in this case the http server

00:43:44,640 --> 00:43:47,920
sends files

00:43:48,640 --> 00:43:53,599
of size between one kilobyte and 10

00:43:51,680 --> 00:43:57,119
megabytes

00:43:53,599 --> 00:43:58,480
and the number of tls connection is

00:43:57,119 --> 00:44:01,760
quite moderate it's

00:43:58,480 --> 00:44:04,880
uh one we picked 100

00:44:01,760 --> 00:44:08,000
connections uh each connection

00:44:04,880 --> 00:44:11,119
sends http get requests

00:44:08,000 --> 00:44:16,000
back to back the second

00:44:11,119 --> 00:44:19,119
scenario that we measured was a

00:44:16,000 --> 00:44:22,400
simulation of media streaming

00:44:19,119 --> 00:44:25,520
similar to mpeg dash protocol

00:44:22,400 --> 00:44:28,720
although we didn't use actual

00:44:25,520 --> 00:44:29,920
empac dash protocol so the difference

00:44:28,720 --> 00:44:34,839
here is that

00:44:29,920 --> 00:44:38,800
the file size was fixed to one megabyte

00:44:34,839 --> 00:44:42,319
uh there was significantly

00:44:38,800 --> 00:44:47,040
more tls connections

00:44:42,319 --> 00:44:47,040
we used 10 000 connections and

00:44:47,359 --> 00:44:53,430
for each connection the http request

00:44:51,200 --> 00:44:54,720
requests were sent with some

00:44:53,430 --> 00:45:00,319
[Music]

00:44:54,720 --> 00:45:05,040
time gap or time window yeah there is

00:45:00,319 --> 00:45:08,800
one to five second space

00:45:05,040 --> 00:45:18,640
between each http get

00:45:08,800 --> 00:45:22,319
request sent over a single connection

00:45:18,640 --> 00:45:25,440
in terms of hardware setup

00:45:22,319 --> 00:45:29,119
we use two machines one with

00:45:25,440 --> 00:45:32,640
the http server

00:45:29,119 --> 00:45:36,640
or one playing the role of http server

00:45:32,640 --> 00:45:40,000
the other playing the role of a http

00:45:36,640 --> 00:45:43,040
client and both of them were equipped

00:45:40,000 --> 00:45:47,119
equipped with intel xeon

00:45:43,040 --> 00:45:51,280
gold cpu skylake

00:45:47,119 --> 00:45:55,040
generation with 32 uh cores

00:45:51,280 --> 00:45:58,000
and with 384 gigabyte of

00:45:55,040 --> 00:46:00,400
ddr memory and the machines were

00:45:58,000 --> 00:46:04,079
connected with 100 gigabit

00:46:00,400 --> 00:46:08,720
ethernet connection uh

00:46:04,079 --> 00:46:11,760
using intel 100

00:46:08,720 --> 00:46:15,359
800 series

00:46:11,760 --> 00:46:17,839
network controllers the bios

00:46:15,359 --> 00:46:21,680
configuration was

00:46:17,839 --> 00:46:26,480
modified to basically disabled

00:46:21,680 --> 00:46:26,480
to disable all the features that

00:46:26,839 --> 00:46:32,560
may

00:46:28,560 --> 00:46:35,680
or cause problem with the

00:46:32,560 --> 00:46:37,040
repeatability of results so things like

00:46:35,680 --> 00:46:40,839
hyper threading c

00:46:37,040 --> 00:46:43,839
states p states and turbo are

00:46:40,839 --> 00:46:43,839
disabled

00:46:44,640 --> 00:46:49,599
and in terms of software configuration

00:46:48,720 --> 00:46:54,160
we were using

00:46:49,599 --> 00:46:57,599
um ubuntu with the

00:46:54,160 --> 00:46:57,599
linux kernel 1.

00:46:58,040 --> 00:47:03,760
5.1.0

00:47:00,720 --> 00:47:07,680
and with obviously ktls

00:47:03,760 --> 00:47:11,839
enabled and also a esni crypto driver so

00:47:07,680 --> 00:47:11,839
this is the driver that is

00:47:13,119 --> 00:47:21,040
that supports aes gcm

00:47:17,920 --> 00:47:24,319
algorithm using

00:47:21,040 --> 00:47:28,160
a esi

00:47:24,319 --> 00:47:32,000
instructions uh similarly the openssl

00:47:28,160 --> 00:47:35,200
library so that the user space library

00:47:32,000 --> 00:47:38,640
uh was also compiled with the

00:47:35,200 --> 00:47:42,640
aesni support enabled

00:47:38,640 --> 00:47:46,319
on the server side we had nginx

00:47:42,640 --> 00:47:49,920
as an http server application

00:47:46,319 --> 00:47:53,280
with a special ktls

00:47:49,920 --> 00:47:54,480
send file patch so it was a patch that

00:47:53,280 --> 00:47:57,680
allows us to

00:47:54,480 --> 00:48:01,119
to use

00:47:57,680 --> 00:48:03,599
ktls with send file

00:48:01,119 --> 00:48:06,000
at least at that time the official

00:48:03,599 --> 00:48:09,200
version was not

00:48:06,000 --> 00:48:12,839
supporting such a combination

00:48:09,200 --> 00:48:15,839
and on the client side we we were using

00:48:12,839 --> 00:48:15,839
wrk

00:48:16,000 --> 00:48:21,839
traffic generator or http traffic

00:48:19,520 --> 00:48:21,839
generator

00:48:22,559 --> 00:48:30,240
we set the tls configuration

00:48:26,319 --> 00:48:34,960
to tls 1.2 with max record size

00:48:30,240 --> 00:48:40,800
16 kilobytes and the

00:48:34,960 --> 00:48:44,319
the crypto algorithm was aes 128

00:48:40,800 --> 00:48:47,520
with gcm

00:48:44,319 --> 00:48:48,960
and we also enabled the persistent

00:48:47,520 --> 00:48:53,680
connections

00:48:48,960 --> 00:48:56,640
in http to avoid

00:48:53,680 --> 00:48:56,640
the overhead

00:48:57,200 --> 00:49:05,680
required to perform connection

00:49:00,559 --> 00:49:09,920
establishment and both both tcp and tls

00:49:05,680 --> 00:49:09,920
connection establishment and handshake

00:49:10,000 --> 00:49:14,319
so basically we were measuring only the

00:49:13,119 --> 00:49:17,440
performance of the

00:49:14,319 --> 00:49:20,000
record protocol not the handshake

00:49:17,440 --> 00:49:20,000
protocol

00:49:20,240 --> 00:49:24,480
and in this

00:49:25,520 --> 00:49:35,839
chart you can see a comparison of

00:49:31,440 --> 00:49:40,800
throughput for the simple web server

00:49:35,839 --> 00:49:44,960
scenario so the dark blue

00:49:40,800 --> 00:49:48,559
bar is always 100 percent

00:49:44,960 --> 00:49:50,720
and it's the performance of user space

00:49:48,559 --> 00:49:50,720
and

00:49:50,960 --> 00:49:57,920
the all the other or

00:49:54,000 --> 00:50:03,280
the ktls write and ktls send file

00:49:57,920 --> 00:50:03,280
so you can see that

00:50:04,240 --> 00:50:12,880
for smaller file sizes like

00:50:07,680 --> 00:50:16,400
one kilobyte or four kilobytes uh

00:50:12,880 --> 00:50:19,280
the kdls and especially ktls send file

00:50:16,400 --> 00:50:20,559
performance it is much lower yes it's up

00:50:19,280 --> 00:50:23,920
to 40

00:50:20,559 --> 00:50:27,040
percent lower comparing to uh

00:50:23,920 --> 00:50:30,160
user space tls

00:50:27,040 --> 00:50:34,559
but with 64 kilobytes

00:50:30,160 --> 00:50:38,839
and above for

00:50:34,559 --> 00:50:41,440
the ktls file

00:50:38,839 --> 00:50:44,559
performance is higher than

00:50:41,440 --> 00:50:46,880
user space so this higher performance

00:50:44,559 --> 00:50:51,040
was something that

00:50:46,880 --> 00:50:51,770
we expected but the lower performance

00:50:51,040 --> 00:50:53,280
for

00:50:51,770 --> 00:50:57,200
[Music]

00:50:53,280 --> 00:51:00,400
smaller smaller file sizes and

00:50:57,200 --> 00:51:05,680
especially lower by 40 percent

00:51:00,400 --> 00:51:10,079
was a surprise definitely a surprise

00:51:05,680 --> 00:51:14,000
for us so we did some

00:51:10,079 --> 00:51:17,680
investigation and

00:51:14,000 --> 00:51:21,040
came to two conclusions or two

00:51:17,680 --> 00:51:24,319
two reasons why the

00:51:21,040 --> 00:51:28,079
ktls has a

00:51:24,319 --> 00:51:28,079
lower performance than

00:51:28,240 --> 00:51:34,640
user space for smaller files

00:51:31,359 --> 00:51:40,160
so the first reason is how the

00:51:34,640 --> 00:51:44,160
http response is sent by the http server

00:51:40,160 --> 00:51:47,359
so when a http response

00:51:44,160 --> 00:51:48,240
must be sent it consists of two parts

00:51:47,359 --> 00:51:52,480
the

00:51:48,240 --> 00:51:55,599
response header and the response payload

00:51:52,480 --> 00:51:58,319
and for in case of a

00:51:55,599 --> 00:51:58,319
send file

00:52:00,640 --> 00:52:07,200
flow the http server must

00:52:03,760 --> 00:52:11,440
first prepare the response

00:52:07,200 --> 00:52:16,319
header in user space buffer and use

00:52:11,440 --> 00:52:19,520
write cisco to transfer

00:52:16,319 --> 00:52:22,880
this buffer content to

00:52:19,520 --> 00:52:26,960
the network stack and next

00:52:22,880 --> 00:52:30,240
uh it is using uh send file cisco

00:52:26,960 --> 00:52:33,760
to send http response payload

00:52:30,240 --> 00:52:38,559
from file uh located in uh

00:52:33,760 --> 00:52:42,079
file system uh so in um

00:52:38,559 --> 00:52:45,599
as you can see you know it's uh two

00:52:42,079 --> 00:52:49,599
uh first of all there are

00:52:45,599 --> 00:52:53,839
two syscalls not one and

00:52:49,599 --> 00:52:53,839
the other reason is that okay this uh

00:52:54,240 --> 00:53:00,800
there is still some overhead of

00:52:57,599 --> 00:53:05,440
transferring data from user space

00:53:00,800 --> 00:53:05,440
if the files are i are

00:53:06,400 --> 00:53:12,480
small then the relative amount of

00:53:09,440 --> 00:53:13,040
data which is sent from user space

00:53:12,480 --> 00:53:15,839
buffer

00:53:13,040 --> 00:53:15,839
is a

00:53:16,079 --> 00:53:23,920
higher and

00:53:20,079 --> 00:53:28,000
the second reason for this

00:53:23,920 --> 00:53:32,100
lower efficiency kt less and file lower

00:53:28,000 --> 00:53:33,359
efficiency for smaller files is a

00:53:32,100 --> 00:53:37,839
[Music]

00:53:33,359 --> 00:53:41,200
way how asni driver and

00:53:37,839 --> 00:53:45,119
tls modules

00:53:41,200 --> 00:53:48,960
interact with each other so basically

00:53:45,119 --> 00:53:52,240
when yes and so the esni driver

00:53:48,960 --> 00:53:55,839
implements uh an algorithm

00:53:52,240 --> 00:53:58,960
uh called karatsuba algorithm and that

00:53:55,839 --> 00:53:58,960
pre-computes some

00:54:00,480 --> 00:54:06,160
values some you know heavy computation

00:54:03,680 --> 00:54:06,160
values

00:54:07,119 --> 00:54:14,880
which are which then can be reused for

00:54:10,400 --> 00:54:18,880
the entire tls

00:54:14,880 --> 00:54:18,880
connection lifetime

00:54:19,200 --> 00:54:25,839
however when tls

00:54:23,040 --> 00:54:25,839
module

00:54:26,079 --> 00:54:34,160
calls crypto driver uh

00:54:29,440 --> 00:54:34,160
it's there is no mechanism to

00:54:35,200 --> 00:54:42,400
let's say save this and provide

00:54:39,599 --> 00:54:42,400
again this uh

00:54:43,040 --> 00:54:52,400
precomputed values uh to the crypto

00:54:47,200 --> 00:54:55,280
driver so basically with each

00:54:52,400 --> 00:54:56,960
encryption request sent from tls to the

00:54:55,280 --> 00:55:00,799
crypto driver

00:54:56,960 --> 00:55:03,839
the driver pre-computes these

00:55:00,799 --> 00:55:05,359
values again and again so they are only

00:55:03,839 --> 00:55:09,440
reused for

00:55:05,359 --> 00:55:13,680
a single single tls

00:55:09,440 --> 00:55:16,559
record while in

00:55:13,680 --> 00:55:18,319
user space implementation these

00:55:16,559 --> 00:55:22,079
pre-computed values are

00:55:18,319 --> 00:55:26,240
reused for entire tls

00:55:22,079 --> 00:55:26,240
connection lifetime so that's

00:55:27,119 --> 00:55:31,839
that's the the problem and again

00:55:32,799 --> 00:55:40,640
it's uh more visible if the

00:55:37,359 --> 00:55:40,640
tls records are

00:55:41,359 --> 00:55:48,720
lower size which happens for

00:55:45,520 --> 00:55:53,040
small file sizes sent over

00:55:48,720 --> 00:55:53,040
http connection

00:55:54,880 --> 00:56:00,960
the second scenario that

00:55:58,000 --> 00:56:04,400
we looked into is media streaming

00:56:00,960 --> 00:56:07,520
scenario and in this case

00:56:04,400 --> 00:56:09,760
instead of measuring uh

00:56:07,520 --> 00:56:12,000
throughput with the maximum cpu

00:56:09,760 --> 00:56:16,559
utilization we did something

00:56:12,000 --> 00:56:20,640
different basically we uh

00:56:16,559 --> 00:56:23,680
kept that throughput on uh

00:56:20,640 --> 00:56:28,079
on a uh

00:56:23,680 --> 00:56:31,920
on the same uh level uh with uh

00:56:28,079 --> 00:56:32,400
70 gigabit per second or 30 gigabit per

00:56:31,920 --> 00:56:35,200
second

00:56:32,400 --> 00:56:36,720
so so we we had two options the first

00:56:35,200 --> 00:56:40,920
one was

00:56:36,720 --> 00:56:44,799
uh with files sent from

00:56:40,920 --> 00:56:48,559
tmpfs from memory and the other option

00:56:44,799 --> 00:56:52,160
was what the files

00:56:48,559 --> 00:56:55,760
transferred from nvme

00:56:52,160 --> 00:56:55,760
device and

00:56:56,400 --> 00:57:00,240
we measured in this case we measured the

00:56:58,799 --> 00:57:03,680
cpu utilization

00:57:00,240 --> 00:57:06,559
and memory bandwidth

00:57:03,680 --> 00:57:06,559
utilization

00:57:07,200 --> 00:57:14,160
and here you can see that the results of

00:57:11,040 --> 00:57:16,799
cpu utilization so there is no big

00:57:14,160 --> 00:57:21,920
surprise

00:57:16,799 --> 00:57:25,680
the user space efficiency in terms of

00:57:21,920 --> 00:57:28,960
cpu utilization was lower

00:57:25,680 --> 00:57:32,400
between 16 and 20

00:57:28,960 --> 00:57:36,559
percent lower

00:57:32,400 --> 00:57:37,200
and this was something we expected based

00:57:36,559 --> 00:57:40,319
on the

00:57:37,200 --> 00:57:44,000
results with uh

00:57:40,319 --> 00:57:44,799
simple web server uh scenario okay

00:57:44,000 --> 00:57:48,160
because

00:57:44,799 --> 00:57:51,280
for uh bigger file sizes

00:57:48,160 --> 00:57:52,000
and in this case that the file sizes

00:57:51,280 --> 00:57:56,400
were

00:57:52,000 --> 00:58:00,400
around one megabyte uh

00:57:56,400 --> 00:58:02,319
basically the user space tls performance

00:58:00,400 --> 00:58:06,559
was

00:58:02,319 --> 00:58:10,720
lower comparing to ktls

00:58:06,559 --> 00:58:15,359
both right and send file

00:58:10,720 --> 00:58:18,160
however the memory bandwidth

00:58:15,359 --> 00:58:18,160
measurements

00:58:18,480 --> 00:58:24,160
which you can see now were um

00:58:24,240 --> 00:58:31,520
a surprise again for us uh

00:58:28,319 --> 00:58:32,720
the basically we expected since the you

00:58:31,520 --> 00:58:36,000
know the the

00:58:32,720 --> 00:58:39,040
put the ktls send file

00:58:36,000 --> 00:58:42,160
eliminates some memory copy operations

00:58:39,040 --> 00:58:42,880
it eliminates transfer of data into user

00:58:42,160 --> 00:58:45,119
space we

00:58:42,880 --> 00:58:46,559
we expect that memory bandwidth

00:58:45,119 --> 00:58:51,680
utilization to be

00:58:46,559 --> 00:58:55,359
lower for kt less than file

00:58:51,680 --> 00:58:58,880
but it was

00:58:55,359 --> 00:59:02,240
the other way around is the kdls

00:58:58,880 --> 00:59:07,599
send file and also kdls write

00:59:02,240 --> 00:59:07,599
generated higher memory bandwidth

00:59:07,680 --> 00:59:16,319
we suspect that you know

00:59:10,880 --> 00:59:20,930
this is mainly related to the fact that

00:59:16,319 --> 00:59:22,880
in case of ktls the implementation is

00:59:20,930 --> 00:59:26,640
[Music]

00:59:22,880 --> 00:59:29,839
let's see scattered among

00:59:26,640 --> 00:59:31,280
three different pieces of code as there

00:59:29,839 --> 00:59:35,680
is a

00:59:31,280 --> 00:59:39,760
tls library in user space with

00:59:35,680 --> 00:59:43,599
which stills

00:59:39,760 --> 00:59:46,720
still transfers the data and

00:59:43,599 --> 00:59:52,640
from application then

00:59:46,720 --> 00:59:55,760
there is a ktls module and

00:59:52,640 --> 01:00:00,400
crypto module in in kernel

00:59:55,760 --> 01:00:03,680
and since this implementation is uh

01:00:00,400 --> 01:00:07,760
put in three different places

01:00:03,680 --> 01:00:15,839
uh the the cash efficiency

01:00:07,760 --> 01:00:15,839
is lower and so it basically

01:00:16,000 --> 01:00:24,240
generates more

01:00:20,079 --> 01:00:29,839
cash pollution and generates more

01:00:24,240 --> 01:00:29,839
traffic to memory

01:00:30,080 --> 01:00:37,520
[Music]

01:00:33,280 --> 01:00:41,680
uh so to summarize summarize my

01:00:37,520 --> 01:00:45,359
presentation in

01:00:41,680 --> 01:00:49,760
our experiments we focus on

01:00:45,359 --> 01:00:54,559
three implementation options for dls

01:00:49,760 --> 01:00:58,160
the user space tls ktls

01:00:54,559 --> 01:01:00,799
write and ktls send file

01:00:58,160 --> 01:01:02,880
and in the simple web server scenario

01:01:00,799 --> 01:01:06,240
the ktls send file

01:01:02,880 --> 01:01:09,280
provides its highest performance for

01:01:06,240 --> 01:01:13,760
files of

01:01:09,280 --> 01:01:18,160
64 kilobyte size and above and for lower

01:01:13,760 --> 01:01:21,839
uh sizes the user space tls

01:01:18,160 --> 01:01:25,760
provides better performance uh

01:01:21,839 --> 01:01:29,599
and in the multimedia streaming scenario

01:01:25,760 --> 01:01:31,920
the kdls send file and ktls

01:01:29,599 --> 01:01:31,920
write

01:01:32,720 --> 01:01:42,640
provide lower cpu utilization but

01:01:37,119 --> 01:01:42,640
higher memory bandwidth utilization

01:01:43,440 --> 01:01:49,280
that's all what i wanted to present

01:01:46,880 --> 01:01:49,280
thank you

01:01:50,880 --> 01:01:56,000
okay uh thank you that was uh

01:01:54,240 --> 01:01:57,920
somewhat enlightening performance

01:01:56,000 --> 01:02:01,680
results uh

01:01:57,920 --> 01:02:03,359
i don't see any questions at this point

01:02:01,680 --> 01:02:05,280
um do you have any further comments on

01:02:03,359 --> 01:02:08,720
that

01:02:05,280 --> 01:02:12,079
okay so there's a uh comment on the chat

01:02:08,720 --> 01:02:16,240
uh gcm aes encrypt goes through k

01:02:12,079 --> 01:02:18,960
malloc and scatter walk copy chunk calls

01:02:16,240 --> 01:02:20,640
uh so yes the the part about the

01:02:18,960 --> 01:02:21,440
complexity of the chrono implementation

01:02:20,640 --> 01:02:25,280
certainly was

01:02:21,440 --> 01:02:27,599
uh interesting and um

01:02:25,280 --> 01:02:29,039
i guess there's an opportunity there to

01:02:27,599 --> 01:02:32,240
clean that up and

01:02:29,039 --> 01:02:35,200
improve that okay so

01:02:32,240 --> 01:02:37,440
with that let's proceed to our uh third

01:02:35,200 --> 01:02:37,440
talk

01:02:39,440 --> 01:02:42,720
so we are going to talk about ktls

01:02:42,079 --> 01:02:44,880
offload

01:02:42,720 --> 01:02:47,680
and the benefit in offloading crypto

01:02:44,880 --> 01:02:49,359
test to the need

01:02:47,680 --> 01:02:52,319
this presentation is based on a

01:02:49,359 --> 01:02:55,680
mechanism presented at netapp 1.2

01:02:52,319 --> 01:02:56,640
with tx offload and net dev 2.2 with rx

01:02:55,680 --> 01:03:00,480
offload by

01:02:56,640 --> 01:03:01,520
by voice this man the slides i'm going

01:03:00,480 --> 01:03:05,440
to show today

01:03:01,520 --> 01:03:09,760
it was written and myself

01:03:05,440 --> 01:03:11,280
we are working with nvidia in the driver

01:03:09,760 --> 01:03:14,839
and will show the advantage in

01:03:11,280 --> 01:03:16,640
offloading crypto assignment to nvidia

01:03:14,839 --> 01:03:20,039
melanox network

01:03:16,640 --> 01:03:21,359
interconnect we will overview the

01:03:20,039 --> 01:03:25,119
responsibilities of

01:03:21,359 --> 01:03:27,520
each component pack driver and hardware

01:03:25,119 --> 01:03:30,240
and maintenance data flows and see some

01:03:27,520 --> 01:03:32,160
performance numbers for both small and

01:03:30,240 --> 01:03:34,160
large scale

01:03:32,160 --> 01:03:35,760
so let's start with the motivation to

01:03:34,160 --> 01:03:39,920
use ktls offload

01:03:35,760 --> 01:03:39,920
and introduction to common tls

01:03:40,400 --> 01:03:44,559
the motivation for this work mainly came

01:03:43,920 --> 01:03:48,160
from

01:03:44,559 --> 01:03:50,880
two perceptions the first one is that

01:03:48,160 --> 01:03:52,799
all internet services are becoming more

01:03:50,880 --> 01:03:55,920
secure

01:03:52,799 --> 01:03:56,880
most of the websites today uses https

01:03:55,920 --> 01:04:01,520
connections

01:03:56,880 --> 01:04:01,520
that mainly rely on tls and

01:04:03,119 --> 01:04:08,880
second perception is that offloading

01:04:06,720 --> 01:04:10,559
from the host and especially from the

01:04:08,880 --> 01:04:13,280
cpu is important

01:04:10,559 --> 01:04:17,039
in order to keep up the increasing phase

01:04:13,280 --> 01:04:20,160
in the in internet consumption

01:04:17,039 --> 01:04:22,880
100 ge link link speed

01:04:20,160 --> 01:04:23,359
need to use these offloads in order to

01:04:22,880 --> 01:04:26,480
release

01:04:23,359 --> 01:04:26,480
smart cpu power

01:04:27,760 --> 01:04:32,880
the solution is based on a pretty much

01:04:30,400 --> 01:04:35,920
unchanged linux networking stack

01:04:32,880 --> 01:04:38,880
with the addition of tls non-crypto flow

01:04:35,920 --> 01:04:39,599
so we get all the goods of a robust and

01:04:38,880 --> 01:04:42,160
resilient

01:04:39,599 --> 01:04:42,160
networking

01:04:44,079 --> 01:04:50,720
tls stands for transport level secure

01:04:48,400 --> 01:04:53,280
this protocol have several version and

01:04:50,720 --> 01:04:56,559
in this presentation we will use

01:04:53,280 --> 01:04:59,599
a tls version 1.2

01:04:56,559 --> 01:05:00,400
tls is a layer 4 protocol based on top

01:04:59,599 --> 01:05:03,359
of tcp

01:05:00,400 --> 01:05:05,440
protocol it was first implemented for

01:05:03,359 --> 01:05:08,079
user space application

01:05:05,440 --> 01:05:08,799
and was offloaded to kermel by ktls

01:05:08,079 --> 01:05:11,200
module

01:05:08,799 --> 01:05:13,920
and now to the nic to encrypt decrypt

01:05:11,200 --> 01:05:16,559
packets on the fly

01:05:13,920 --> 01:05:17,520
we will walk through ktls device data

01:05:16,559 --> 01:05:20,400
flows for

01:05:17,520 --> 01:05:22,160
both transmit and receive side and

01:05:20,400 --> 01:05:24,480
present performance result

01:05:22,160 --> 01:05:25,440
in a patch synthetic environment and

01:05:24,480 --> 01:05:29,920
engineering

01:05:25,440 --> 01:05:33,680
engine x real life server application

01:05:29,920 --> 01:05:34,720
ktls overflow time first implementation

01:05:33,680 --> 01:05:36,960
which offloads

01:05:34,720 --> 01:05:37,839
crypto processing from application layer

01:05:36,960 --> 01:05:41,440
to the kermit

01:05:37,839 --> 01:05:44,559
was introduced in kernel version 413

01:05:41,440 --> 01:05:46,240
as a software offload nvidia melanox

01:05:44,559 --> 01:05:49,280
connected 6dx

01:05:46,240 --> 01:05:53,359
ktls peaks offload the port was

01:05:49,280 --> 01:05:58,000
introduced by mlx5e driver in kernel

01:05:53,359 --> 01:06:01,680
version 5.6 and the rx ability was added

01:05:58,000 --> 01:06:05,520
in kernel version 5.9

01:06:01,680 --> 01:06:08,720
in between we had an fpga programmable

01:06:05,520 --> 01:06:13,760
implemented also by driver

01:06:08,720 --> 01:06:13,760
with supported offloading camera for it

01:06:13,839 --> 01:06:17,119
as this presentation is a part of pls

01:06:16,400 --> 01:06:19,039
workshop

01:06:17,119 --> 01:06:20,880
i believe that most of you are well

01:06:19,039 --> 01:06:24,160
known with the tls protocol

01:06:20,880 --> 01:06:27,119
pros and cons ideally

01:06:24,160 --> 01:06:29,200
we would process packet independent it

01:06:27,119 --> 01:06:30,079
works like that in most common security

01:06:29,200 --> 01:06:34,400
protocols

01:06:30,079 --> 01:06:34,400
for example like stack and dtls

01:06:34,559 --> 01:06:38,160
unlike those protocols cls process

01:06:37,280 --> 01:06:40,400
records

01:06:38,160 --> 01:06:43,280
and those records can be spread on a

01:06:40,400 --> 01:06:45,760
multiple tcp package

01:06:43,280 --> 01:06:47,920
at the right chart we can see an example

01:06:45,760 --> 01:06:50,559
for three tls records

01:06:47,920 --> 01:06:52,160
leads to four tcp max segment size

01:06:50,559 --> 01:06:54,079
packet

01:06:52,160 --> 01:06:56,000
we can see that all records share their

01:06:54,079 --> 01:06:58,799
data with another record

01:06:56,000 --> 01:06:58,799
the same tcp

01:06:59,520 --> 01:07:05,839
for example a packet number two

01:07:03,440 --> 01:07:07,920
tcp packet number two is shared between

01:07:05,839 --> 01:07:11,520
tls record number one and tsp

01:07:07,920 --> 01:07:14,480
number two we used

01:07:11,520 --> 01:07:15,920
aes counter mode which generates

01:07:14,480 --> 01:07:18,319
ciphertext by

01:07:15,920 --> 01:07:20,400
operation between the key stream and the

01:07:18,319 --> 01:07:23,880
data for encryption

01:07:20,400 --> 01:07:27,359
keystream can be generated using an

01:07:23,880 --> 01:07:29,440
initialization vector and key

01:07:27,359 --> 01:07:30,640
which can be found in the tls crypto

01:07:29,440 --> 01:07:33,359
content

01:07:30,640 --> 01:07:34,799
driver is responsible for initializing

01:07:33,359 --> 01:07:38,240
hardware with the

01:07:34,799 --> 01:07:40,319
opel context provided by ktls stack on

01:07:38,240 --> 01:07:43,039
creation

01:07:40,319 --> 01:07:43,839
in addition hardware must track the tls

01:07:43,039 --> 01:07:45,680
contacts

01:07:43,839 --> 01:07:48,240
in order to successfully encrypt the

01:07:45,680 --> 01:07:50,480
crypt packet

01:07:48,240 --> 01:07:51,280
some flows can cause hardware to lose

01:07:50,480 --> 01:07:53,520
its state

01:07:51,280 --> 01:07:56,160
in this case hardware will indicate it

01:07:53,520 --> 01:07:59,039
to the driver

01:07:56,160 --> 01:08:01,359
that will coordinate with the tls module

01:07:59,039 --> 01:08:04,240
to restore it

01:08:01,359 --> 01:08:05,520
this scheme shows the rules of each

01:08:04,240 --> 01:08:08,720
layer

01:08:05,520 --> 01:08:11,359
user space data is kept in memory

01:08:08,720 --> 01:08:12,400
pls protocol is responsible for that of

01:08:11,359 --> 01:08:15,760
fragmentation

01:08:12,400 --> 01:08:17,520
in case the crypto center as well as

01:08:15,760 --> 01:08:21,199
adding headers and trailers

01:08:17,520 --> 01:08:25,839
and to divide into echo

01:08:21,199 --> 01:08:25,839
then it will hand it to players

01:08:29,359 --> 01:08:33,359
we will start with the transmit flow i

01:08:32,239 --> 01:08:38,960
will go to

01:08:33,359 --> 01:08:41,120
the condition checked in case of a good

01:08:38,960 --> 01:08:43,040
first the driver will check each packet

01:08:41,120 --> 01:08:44,239
whether it belongs to an offloaded

01:08:43,040 --> 01:08:46,799
socket

01:08:44,239 --> 01:08:49,199
let's say it's true now it will check

01:08:46,799 --> 01:08:53,120
the packet tcp sequence number

01:08:49,199 --> 01:08:55,679
and compare it to the expected number

01:08:53,120 --> 01:08:56,560
if it's also true it will prepare a

01:08:55,679 --> 01:08:59,199
property

01:08:56,560 --> 01:09:00,239
descriptor and follow the packet for

01:08:59,199 --> 01:09:03,120
authentication

01:09:00,239 --> 01:09:07,839
and encryption on the fly by the neck

01:09:03,120 --> 01:09:07,839
and to the wire

01:09:09,920 --> 01:09:13,040
let's say once again that the packet is

01:09:12,799 --> 01:09:16,319
an

01:09:13,040 --> 01:09:19,040
offload packet but i will

01:09:16,319 --> 01:09:21,920
discover the pcp sequence number is not

01:09:19,040 --> 01:09:25,040
the expected piece

01:09:21,920 --> 01:09:27,759
now driver will trigger this ink flow

01:09:25,040 --> 01:09:31,520
first the driver will understand to

01:09:27,759 --> 01:09:34,080
which record this package belongs to

01:09:31,520 --> 01:09:35,759
followed by updating hardware with the

01:09:34,080 --> 01:09:38,319
suitable cryptocurrent

01:09:35,759 --> 01:09:40,640
using fastpass communication with

01:09:38,319 --> 01:09:44,159
fencing to guarantee that we supply

01:09:40,640 --> 01:09:44,159
hardware with the properties

01:09:46,960 --> 01:09:53,359
notice that ktls module is not involved

01:09:50,319 --> 01:09:55,040
involved in this procedure

01:09:53,359 --> 01:09:58,640
now the packet can be passed for

01:09:55,040 --> 01:09:58,640
encryption once again by then

01:09:59,280 --> 01:10:04,000
we will move to the receive side path

01:10:02,159 --> 01:10:06,239
when hardware is in offload state

01:10:04,000 --> 01:10:08,239
all packets belong to a connection

01:10:06,239 --> 01:10:11,360
passing through the crypto engines

01:10:08,239 --> 01:10:13,360
are decrypted and authenticated

01:10:11,360 --> 01:10:15,679
hardware will indicate it to the driver

01:10:13,360 --> 01:10:18,000
using alex descriptors

01:10:15,679 --> 01:10:19,520
and the driver will mark it to inform

01:10:18,000 --> 01:10:22,880
software tls using

01:10:19,520 --> 01:10:25,440
skb decrypted field

01:10:22,880 --> 01:10:27,360
now for the receive site traffic cls

01:10:25,440 --> 01:10:29,199
module will check each record

01:10:27,360 --> 01:10:31,280
independently

01:10:29,199 --> 01:10:33,679
it will check if all packets inside the

01:10:31,280 --> 01:10:36,719
tls record will be kept

01:10:33,679 --> 01:10:40,320
in this case it's true so we can copy

01:10:36,719 --> 01:10:40,320
the decrypted data to memo

01:10:40,560 --> 01:10:44,719
in case of out of orders or drops we

01:10:43,040 --> 01:10:46,800
will defend the data path to two

01:10:44,719 --> 01:10:49,199
scenario

01:10:46,800 --> 01:10:51,360
first one is partially decrypted tls

01:10:49,199 --> 01:10:54,000
spec

01:10:51,360 --> 01:10:54,880
not all packet inside the record is

01:10:54,000 --> 01:10:57,360
decrypted

01:10:54,880 --> 01:10:58,239
but some of them actually does in the

01:10:57,360 --> 01:11:00,560
stream below

01:10:58,239 --> 01:11:01,760
we can see that packet number plane is

01:11:00,560 --> 01:11:05,120
cipher text

01:11:01,760 --> 01:11:07,760
while packet two and four are planned

01:11:05,120 --> 01:11:09,120
means that tls record number two is

01:11:07,760 --> 01:11:12,719
mixed with what's

01:11:09,120 --> 01:11:15,760
with both typhu and plain

01:11:12,719 --> 01:11:18,480
packet 3 contains payload and tcp

01:11:15,760 --> 01:11:21,120
headers without any tls record header

01:11:18,480 --> 01:11:21,760
so in this case software will remove the

01:11:21,120 --> 01:11:24,400
job

01:11:21,760 --> 01:11:26,960
you can get packet number three after

01:11:24,400 --> 01:11:29,360
that we can copy all that

01:11:26,960 --> 01:11:32,320
we should know that it will not impact

01:11:29,360 --> 01:11:32,320
the following package

01:11:32,880 --> 01:11:39,520
packets four five and seven

01:11:35,920 --> 01:11:40,800
four five six and seven the other case

01:11:39,520 --> 01:11:43,920
will require hardware

01:11:40,800 --> 01:11:45,360
synchronization hardware must track

01:11:43,920 --> 01:11:49,199
crypto context

01:11:45,360 --> 01:11:50,640
for each connection and out of order or

01:11:49,199 --> 01:11:53,600
drops can cause outward

01:11:50,640 --> 01:11:55,840
weight off out of sync and when it does

01:11:53,600 --> 01:11:58,000
arduino will stop decrypting packets for

01:11:55,840 --> 01:11:58,000
this

01:11:58,480 --> 01:12:01,920
in this case it will send the listing

01:12:01,199 --> 01:12:05,360
request

01:12:01,920 --> 01:12:08,960
using alex descriptors to the driver

01:12:05,360 --> 01:12:11,920
that notifies tls stack using the new as

01:12:08,960 --> 01:12:11,920
increasing api

01:12:12,320 --> 01:12:15,360
following it the drive will query the

01:12:14,480 --> 01:12:17,920
device

01:12:15,360 --> 01:12:20,000
for a guest tcp sequence number and

01:12:17,920 --> 01:12:23,440
provide it to the stack

01:12:20,000 --> 01:12:26,560
ktlstack will check if it matches

01:12:23,440 --> 01:12:27,679
any of his direct incoming packet and

01:12:26,560 --> 01:12:31,440
call the driver

01:12:27,679 --> 01:12:34,840
which we think how do meanwhile software

01:12:31,440 --> 01:12:36,960
will finish the job and decrypt

01:12:34,840 --> 01:12:39,120
all

01:12:36,960 --> 01:12:40,800
after a successful listing the device

01:12:39,120 --> 01:12:43,920
will be turned to a float state and

01:12:40,800 --> 01:12:43,920
declared the receiver

01:12:44,640 --> 01:12:48,960
the child the broken line symbolized

01:12:47,199 --> 01:12:49,600
that missing flow is approved by

01:12:48,960 --> 01:12:52,960
software

01:12:49,600 --> 01:12:52,960
and our world can decrypt it

01:12:53,360 --> 01:12:57,600
so to show the performance impact of

01:12:55,679 --> 01:13:01,360
ktls audio offload with

01:12:57,600 --> 01:13:04,800
nginx server we use 2 amd epic system

01:13:01,360 --> 01:13:05,760
connected to switch with connecting 6dx

01:13:04,800 --> 01:13:09,199
nic

01:13:05,760 --> 01:13:09,199
to the wrk client

01:13:09,679 --> 01:13:17,679
wlk opens different amount of connection

01:13:13,920 --> 01:13:21,679
connections range between 1024

01:13:17,679 --> 01:13:24,320
and 32k connection with 64 threads

01:13:21,679 --> 01:13:27,360
and continuously requesting one megabyte

01:13:24,320 --> 01:13:27,360
file from the cell

01:13:27,440 --> 01:13:32,080
nginx responds with either plain text

01:13:31,280 --> 01:13:35,520
http

01:13:32,080 --> 01:13:38,719
response http

01:13:35,520 --> 01:13:43,120
with software ktls which uses openssl

01:13:38,719 --> 01:13:46,800
tls version 1.2 implementation

01:13:43,120 --> 01:13:49,199
and https with ktls hardware ltx

01:13:46,800 --> 01:13:49,199
upload

01:13:50,159 --> 01:13:54,000
in this case we modify the engine into

01:13:52,640 --> 01:13:56,320
send file chain

01:13:54,000 --> 01:13:59,120
also for tls traffic then the default is

01:13:56,320 --> 01:13:59,120
only for hd

01:13:59,679 --> 01:14:05,840
all implementations reach libraries from

01:14:02,719 --> 01:14:09,679
100 1024 and up to

01:14:05,840 --> 01:14:10,400
30k connections and the graph shows

01:14:09,679 --> 01:14:14,000
around

01:14:10,400 --> 01:14:17,440
85 gigabits per second at the w

01:14:14,000 --> 01:14:18,880
is the wrk reporting it in application

01:14:17,440 --> 01:14:22,320
layer

01:14:18,880 --> 01:14:25,679
which is not taking into consideration

01:14:22,320 --> 01:14:25,679
layer 3 and forehead

01:14:25,760 --> 01:14:31,520
basically it's a pure http object data

01:14:29,120 --> 01:14:33,120
means the bandwidth in which we can

01:14:31,520 --> 01:14:37,040
transfer http

01:14:33,120 --> 01:14:39,360
object this slide shows

01:14:37,040 --> 01:14:43,760
the cpu improvement followed by

01:14:39,360 --> 01:14:47,280
offloading cryptocurrency price

01:14:43,760 --> 01:14:49,840
we calculated active cost by summing the

01:14:47,280 --> 01:14:54,800
amount of cpu used by the server

01:14:49,840 --> 01:14:54,800
in order to transmit 100 gigabit line

01:14:54,880 --> 01:14:58,719
we calculated improvement with the delta

01:14:58,000 --> 01:15:02,480
between

01:14:58,719 --> 01:15:06,320
https using software and outdoor offload

01:15:02,480 --> 01:15:06,320
divided by the amount used by

01:15:06,960 --> 01:15:13,520
in for example in this

01:15:10,000 --> 01:15:15,920
server we used 60 focus

01:15:13,520 --> 01:15:16,800
and let's assume that all costs are

01:15:15,920 --> 01:15:21,760
working

01:15:16,800 --> 01:15:27,199
in 50 utilization 64 course multiple in

01:15:21,760 --> 01:15:27,199
0.5 equals to 32 active costs

01:15:28,400 --> 01:15:31,840
we saw a pretty much significant

01:15:30,960 --> 01:15:36,080
improvement

01:15:31,840 --> 01:15:38,719
of up to 50 percent reduction in cpus

01:15:36,080 --> 01:15:39,840
if we look at the table at 32k

01:15:38,719 --> 01:15:42,960
connection

01:15:39,840 --> 01:15:43,920
http response activate a bit more than

01:15:42,960 --> 01:15:47,280
12 cores

01:15:43,920 --> 01:15:48,560
in order to maintain language software

01:15:47,280 --> 01:15:51,520
ktls uses

01:15:48,560 --> 01:15:53,679
ninth active calls which is around seven

01:15:51,520 --> 01:15:57,679
more than http

01:15:53,679 --> 01:15:58,320
when ktlsr dual tx upload uses 14 active

01:15:57,679 --> 01:16:02,159
codes

01:15:58,320 --> 01:16:02,159
only two more than http

01:16:04,080 --> 01:16:11,120
and we should notice that for

01:16:08,199 --> 01:16:14,159
1024 to 8k cases

01:16:11,120 --> 01:16:15,440
so you we can see that http using ktlstx

01:16:14,159 --> 01:16:18,320
offload is using

01:16:15,440 --> 01:16:20,159
less active cores than http this is

01:16:18,320 --> 01:16:22,719
caused by a higher packet rate

01:16:20,159 --> 01:16:26,560
when transmit transmitting plaintext

01:16:22,719 --> 01:16:29,600
that leads to a higher cpu utilization

01:16:26,560 --> 01:16:31,840
i will clarify on the wire

01:16:29,600 --> 01:16:33,679
we have less headers for the same mtu

01:16:31,840 --> 01:16:37,120
and http connection

01:16:33,679 --> 01:16:37,120
can post small data

01:16:41,040 --> 01:16:47,360
this slide shows throughput speed up

01:16:43,520 --> 01:16:50,400
when using full unit directional off

01:16:47,360 --> 01:16:52,640
uh we used two intel servers connected

01:16:50,400 --> 01:16:56,080
back to back with connected 6x

01:16:52,640 --> 01:16:56,719
next we patched iperf to support tls

01:16:56,080 --> 01:16:59,840
launcher

01:16:56,719 --> 01:17:02,880
using openst libraries and compare

01:16:59,840 --> 01:17:06,719
between software ktls

01:17:02,880 --> 01:17:07,520
and receive methods ktls device offload

01:17:06,719 --> 01:17:12,400
and tcp

01:17:07,520 --> 01:17:12,400
traffic using stack send and receive net

01:17:12,719 --> 01:17:16,000
full loading directional offloads mean

01:17:14,960 --> 01:17:19,360
we use dx

01:17:16,000 --> 01:17:21,199
of loading server side and rx offload on

01:17:19,360 --> 01:17:26,719
the client

01:17:21,199 --> 01:17:26,719
we measure up to 2.5 x speed up

01:17:27,440 --> 01:17:32,239
like we can see in the single stream

01:17:31,040 --> 01:17:35,520
buffs

01:17:32,239 --> 01:17:37,920
and we can see 18 gigabit good put

01:17:35,520 --> 01:17:40,400
comparing to the 7 gigabit achieved by

01:17:37,920 --> 01:17:40,400
software

01:17:40,640 --> 01:17:45,679
ktls overflow breached line rate with

01:17:43,760 --> 01:17:48,000
less connection

01:17:45,679 --> 01:17:51,280
with less connections as we can see in

01:17:48,000 --> 01:17:51,280
the eight string case

01:17:51,360 --> 01:17:55,840
and once again the graph reports

01:17:53,520 --> 01:17:57,360
application layer throughput of output

01:17:55,840 --> 01:18:02,880
reported by ipel

01:17:57,360 --> 01:18:02,880
something like 94.5 gb

01:18:03,120 --> 01:18:06,960
in the following two slides we want to

01:18:06,480 --> 01:18:10,159
show

01:18:06,960 --> 01:18:11,280
how ktls device offload can save a lot

01:18:10,159 --> 01:18:14,800
of cpu power

01:18:11,280 --> 01:18:17,679
for both sender and receiver

01:18:14,800 --> 01:18:18,159
we took cases that achieve 100g line

01:18:17,679 --> 01:18:20,560
rate

01:18:18,159 --> 01:18:21,920
and compare the amount of active calls

01:18:20,560 --> 01:18:25,280
for each case

01:18:21,920 --> 01:18:29,760
again active cost is the amount of cpu

01:18:25,280 --> 01:18:29,760
used in order to get 100g

01:18:29,840 --> 01:18:32,960
so if we look at the center graph we can

01:18:32,400 --> 01:18:36,239
see that

01:18:32,960 --> 01:18:40,159
in order to act to achieve line rate

01:18:36,239 --> 01:18:42,800
with 64 128 and 512

01:18:40,159 --> 01:18:44,800
connections we use the same amount of

01:18:42,800 --> 01:18:48,239
active calls for ktls device

01:18:44,800 --> 01:18:51,360
and for plaintext tcp traffic means

01:18:48,239 --> 01:18:55,840
we recover all of the cpu overhead cost

01:18:51,360 --> 01:18:55,840
when using tls protocol

01:18:56,800 --> 01:19:03,280
a when i say we recover

01:19:00,080 --> 01:19:05,520
all i should use quotation marks and the

01:19:03,280 --> 01:19:08,560
small difference in cpu utilization

01:19:05,520 --> 01:19:11,360
when using tcp traffic caused by

01:19:08,560 --> 01:19:11,760
packet rate differences tcp packet rate

01:19:11,360 --> 01:19:14,480
was

01:19:11,760 --> 01:19:16,719
higher in comparison to the 2k dls

01:19:14,480 --> 01:19:23,840
implementation presented

01:19:16,719 --> 01:19:23,840
like in the case we saw earlier

01:19:24,480 --> 01:19:28,960
now for the received side so if we look

01:19:27,679 --> 01:19:31,520
at the left column

01:19:28,960 --> 01:19:32,159
we can see that in order to get 100g

01:19:31,520 --> 01:19:35,600
line rate

01:19:32,159 --> 01:19:38,320
over tcp traffic with 64 connections

01:19:35,600 --> 01:19:39,679
on the received side we used eight

01:19:38,320 --> 01:19:45,280
active calls

01:19:39,679 --> 01:19:49,199
from the 24 available in the system

01:19:45,280 --> 01:19:51,440
a bit more than 10 from ktls device

01:19:49,199 --> 01:19:57,120
offload and almost 18

01:19:51,440 --> 01:19:57,120
with software ktls implementation

01:19:57,440 --> 01:20:01,199
if we look at the right column in the

01:20:00,400 --> 01:20:04,600
graph

01:20:01,199 --> 01:20:08,320
we can see that using software ktls over

01:20:04,600 --> 01:20:10,960
512 connections uses 19 calls

01:20:08,320 --> 01:20:13,760
which is nine more active cost

01:20:10,960 --> 01:20:13,760
dentistically

01:20:13,840 --> 01:20:20,080
tls offload uses 13 calls

01:20:17,280 --> 01:20:21,280
limbo three more than tcp for this

01:20:20,080 --> 01:20:24,560
calculation

01:20:21,280 --> 01:20:27,760
and let's use

01:20:24,560 --> 01:20:28,719
tcp as a lower bound we can look at the

01:20:27,760 --> 01:20:32,080
delta between

01:20:28,719 --> 01:20:35,520
software and tcp as the tls overhead

01:20:32,080 --> 01:20:38,880
so we recover 66 66

01:20:35,520 --> 01:20:39,520
of the cpu overlay mainly crypto

01:20:38,880 --> 01:20:43,440
overhead

01:20:39,520 --> 01:20:43,440
caused by software implementation

01:20:44,880 --> 01:20:50,880
in the receive side we do not see

01:20:48,000 --> 01:20:52,800
a better cpu utilization for kpls device

01:20:50,880 --> 01:20:55,120
like we saw in the tx style

01:20:52,800 --> 01:20:56,400
in this case the overhead is too high to

01:20:55,120 --> 01:20:59,040
recover

01:20:56,400 --> 01:21:01,280
but we saw a bit higher packet late also

01:20:59,040 --> 01:21:01,280
will

01:21:01,760 --> 01:21:08,400
so we will walk through the benefits

01:21:04,880 --> 01:21:11,440
of using a tls device api

01:21:08,400 --> 01:21:13,360
so adding crypto state to the nic and

01:21:11,440 --> 01:21:15,679
the ability to encrypt the

01:21:13,360 --> 01:21:18,000
packets on the fly leaves us with a

01:21:15,679 --> 01:21:22,239
single pci router

01:21:18,000 --> 01:21:26,840
which helped to save pci e a bandwidth

01:21:22,239 --> 01:21:29,840
and added no additional latency in

01:21:26,840 --> 01:21:29,840
comparison

01:21:30,639 --> 01:21:36,239
data designated for encryption is not

01:21:33,440 --> 01:21:36,239
written to one

01:21:36,800 --> 01:21:44,719
the tls tags add non-crypto

01:21:40,560 --> 01:21:48,560
flow to deal with ktls device offload

01:21:44,719 --> 01:21:51,520
other than that it remains unchanged

01:21:48,560 --> 01:21:52,080
and so we profit the resilience tcp

01:21:51,520 --> 01:21:54,320
stack

01:21:52,080 --> 01:21:55,280
with memory management congestion

01:21:54,320 --> 01:21:57,280
control

01:21:55,280 --> 01:21:59,840
and other important components that

01:21:57,280 --> 01:22:02,239
comes with it

01:21:59,840 --> 01:22:03,040
and the last point is that the driver

01:22:02,239 --> 01:22:06,000
called

01:22:03,040 --> 01:22:08,560
the tls tax whenever it will need to be

01:22:06,000 --> 01:22:08,560
resynced

01:22:09,199 --> 01:22:14,239
so uh to summarize it we saw a

01:22:12,320 --> 01:22:16,560
significant performance throughput

01:22:14,239 --> 01:22:17,840
speed-up gain by offloading crypto

01:22:16,560 --> 01:22:20,880
processing

01:22:17,840 --> 01:22:23,840
into the network device

01:22:20,880 --> 01:22:25,520
ktls device offload recovers a massive

01:22:23,840 --> 01:22:29,040
amount of cpu overhead

01:22:25,520 --> 01:22:29,040
caused by tls protocol

01:22:29,600 --> 01:22:35,440
it can help prevent hackers from getting

01:22:33,280 --> 01:22:38,800
the data before it

01:22:35,440 --> 01:22:42,239
it is encrypted since no crypto data

01:22:38,800 --> 01:22:42,239
is written to run

01:22:43,760 --> 01:22:47,920
and when ends up with an easy to

01:22:47,120 --> 01:22:51,520
maintain

01:22:47,920 --> 01:22:56,239
how high performing tls implementation

01:22:51,520 --> 01:22:56,239
relying on the great linux network stack

01:22:59,520 --> 01:23:03,280
that's it thanks for listening

01:23:04,560 --> 01:23:11,600
okay uh thank you so we do have

01:23:08,159 --> 01:23:13,840
um a few questions on the chat

01:23:11,600 --> 01:23:15,040
that i'd like to get to however we're

01:23:13,840 --> 01:23:18,080
also at the

01:23:15,040 --> 01:23:19,280
official end of the session so unless

01:23:18,080 --> 01:23:22,480
there is um

01:23:19,280 --> 01:23:24,320
objective objections i propose that we

01:23:22,480 --> 01:23:28,080
extend this a few minutes so we can

01:23:24,320 --> 01:23:28,080
cover uh some of these questions

01:23:28,320 --> 01:23:32,840
so the first question and then i think

01:23:30,320 --> 01:23:36,480
there were several in this vein

01:23:32,840 --> 01:23:39,600
um reorder tcp packets

01:23:36,480 --> 01:23:42,480
so what exactly happens

01:23:39,600 --> 01:23:43,840
when they get reordered what is the

01:23:42,480 --> 01:23:47,040
impact

01:23:43,840 --> 01:23:48,080
what happens if we have a flow that's

01:23:47,040 --> 01:23:51,040
consistently getting

01:23:48,080 --> 01:23:51,040
reordered packets

01:23:53,840 --> 01:23:57,360
i'm not sure i i understand the whole

01:23:56,880 --> 01:24:01,280
question

01:23:57,360 --> 01:24:04,560
and i also we have boris here to help me

01:24:01,280 --> 01:24:07,470
as the architect of this feature so

01:24:04,560 --> 01:24:08,800
in case of reordering

01:24:07,470 --> 01:24:11,840
[Music]

01:24:08,800 --> 01:24:17,360
we can trigger software offload to take

01:24:11,840 --> 01:24:19,920
control and

01:24:17,360 --> 01:24:21,360
like finish the job for the offload for

01:24:19,920 --> 01:24:24,719
the auto offload

01:24:21,360 --> 01:24:28,239
and but in a pretty much

01:24:24,719 --> 01:24:29,120
reordering consistent environment ktls

01:24:28,239 --> 01:24:32,000
will

01:24:29,120 --> 01:24:33,440
will trigger this sync again and again

01:24:32,000 --> 01:24:35,920
so it will be

01:24:33,440 --> 01:24:35,920
harmful

01:24:38,080 --> 01:24:47,040
okay which real world website has http

01:24:42,840 --> 01:24:50,080
transfers with one megabyte sizes

01:24:47,040 --> 01:24:53,360
so this is a good question we

01:24:50,080 --> 01:24:57,679
took this file sizes

01:24:53,360 --> 01:25:01,440
as we tried several file sizes so

01:24:57,679 --> 01:25:04,960
we started with the 64k files

01:25:01,440 --> 01:25:04,960
uh up to checking

01:25:05,920 --> 01:25:13,280
try to check like streaming scenarios

01:25:09,280 --> 01:25:18,159
in which um companies that

01:25:13,280 --> 01:25:21,520
can support videos and streaming

01:25:18,159 --> 01:25:26,159
services to to the world so

01:25:21,520 --> 01:25:29,600
they use big files but um

01:25:26,159 --> 01:25:32,320
i don't know for a real website that

01:25:29,600 --> 01:25:35,520
using one megabyte files but

01:25:32,320 --> 01:25:39,120
pretty much large files i know

01:25:35,520 --> 01:25:41,679
about several customers of our company

01:25:39,120 --> 01:25:44,320
so it can be large files but

01:25:41,679 --> 01:25:47,679
specifically one megabyte

01:25:44,320 --> 01:25:49,040
i can find an example for it okay so i

01:25:47,679 --> 01:25:51,760
think the point there is

01:25:49,040 --> 01:25:52,719
for something like video where we're

01:25:51,760 --> 01:25:55,760
serving

01:25:52,719 --> 01:25:57,760
a massive amounts of data it's

01:25:55,760 --> 01:25:59,920
that's a clear use case for something

01:25:57,760 --> 01:26:03,600
like ktls where we're optimizing

01:25:59,920 --> 01:26:07,120
for the amount of data as opposed to

01:26:03,600 --> 01:26:10,159
small packets okay uh next question

01:26:07,120 --> 01:26:12,880
ktls works in process context

01:26:10,159 --> 01:26:13,600
so tls record and tcp segments are

01:26:12,880 --> 01:26:16,000
formed

01:26:13,600 --> 01:26:18,800
in different workflows wouldn't it

01:26:16,000 --> 01:26:19,600
simplify to manage tls records on tx

01:26:18,800 --> 01:26:23,199
path

01:26:19,600 --> 01:26:26,639
if ktls formed a tls record on sk

01:26:23,199 --> 01:26:26,639
wright transmit

01:26:27,040 --> 01:26:34,400
so i think the the question there is

01:26:30,480 --> 01:26:38,320
if we maybe deferred the

01:26:34,400 --> 01:26:40,639
the tls record into the kernel

01:26:38,320 --> 01:26:42,400
uh then we can construct tls records of

01:26:40,639 --> 01:26:43,040
optimal sizes that seems to be the

01:26:42,400 --> 01:26:44,400
question

01:26:43,040 --> 01:26:51,840
and then would that improve the

01:26:44,400 --> 01:26:51,840
performance for tls over tcp

01:26:54,719 --> 01:26:57,920
i'm not sure i i understand the question

01:26:57,360 --> 01:27:01,600
but

01:26:57,920 --> 01:27:03,360
if if i do some sounds like a pretty

01:27:01,600 --> 01:27:06,159
much

01:27:03,360 --> 01:27:06,159
something to check

01:27:08,320 --> 01:27:12,320
me okay can i ask my voice

01:27:13,120 --> 01:27:21,760
hello yes yes you can ask my voice

01:27:18,239 --> 01:27:25,040
the question is that um

01:27:21,760 --> 01:27:28,239
in sk writex mid uh

01:27:25,040 --> 01:27:32,159
the function calling in gpip stack

01:27:28,239 --> 01:27:35,600
we know exactly uh the state of tcp uh

01:27:32,159 --> 01:27:38,320
exactly how much data gp

01:27:35,600 --> 01:27:38,719
can send to the pair in this place we

01:27:38,320 --> 01:27:41,600
can

01:27:38,719 --> 01:27:42,719
form a challenge record of optimal size

01:27:41,600 --> 01:27:46,080
so we can

01:27:42,719 --> 01:27:49,199
split the jls record of are very

01:27:46,080 --> 01:27:50,960
accurately according to tcp segments

01:27:49,199 --> 01:27:52,560
that's the problem which you

01:27:50,960 --> 01:27:55,040
initially mentioned at the beginning of

01:27:52,560 --> 01:27:58,639
presentation when jls record doesn't

01:27:55,040 --> 01:28:01,679
match uh tcp segments exactly

01:27:58,639 --> 01:28:04,719
so i'm wondering uh

01:28:01,679 --> 01:28:07,679
whether a different design when

01:28:04,719 --> 01:28:08,719
rktls works inside the software queue

01:28:07,679 --> 01:28:12,080
contacts

01:28:08,719 --> 01:28:16,159
uh wouldn't such design simplify uh

01:28:12,080 --> 01:28:18,719
offloading on the network interface

01:28:16,159 --> 01:28:18,719
by the nic

01:28:22,080 --> 01:28:30,400
that makes sense yeah

01:28:26,560 --> 01:28:31,760
also to me so it might be interesting to

01:28:30,400 --> 01:28:34,560
see the uh

01:28:31,760 --> 01:28:35,360
prototype code of that uh we have a

01:28:34,560 --> 01:28:38,719
prototype

01:28:35,360 --> 01:28:41,360
our code works exactly this way it's not

01:28:38,719 --> 01:28:42,480
an upstream but it impressed works

01:28:41,360 --> 01:28:45,679
exactly this

01:28:42,480 --> 01:28:47,360
way so i would suggest to post it

01:28:45,679 --> 01:28:50,560
upstream i think there's

01:28:47,360 --> 01:28:51,040
um there needs to be discussion one of

01:28:50,560 --> 01:28:54,320
the

01:28:51,040 --> 01:28:57,760
one of the concerns i would have is that

01:28:54,320 --> 01:29:00,960
we're inserting uh data into the tcp

01:28:57,760 --> 01:29:02,320
stream as opposed to to modifying that

01:29:00,960 --> 01:29:05,280
i'm sure that would have some

01:29:02,320 --> 01:29:06,239
some ramifications obviously with tls

01:29:05,280 --> 01:29:08,880
we're already being

01:29:06,239 --> 01:29:10,719
or ktls we're to be in invasives on the

01:29:08,880 --> 01:29:14,159
transmit site

01:29:10,719 --> 01:29:15,280
so um without raised questions of how do

01:29:14,159 --> 01:29:17,920
we synchronize

01:29:15,280 --> 01:29:20,159
if for instance we wanted to move the

01:29:17,920 --> 01:29:21,199
tls back into the host and things like

01:29:20,159 --> 01:29:25,280
that

01:29:21,199 --> 01:29:28,560
so let's go to a next question

01:29:25,280 --> 01:29:29,600
if the channel is lossy then ktls

01:29:28,560 --> 01:29:32,480
offload might

01:29:29,600 --> 01:29:33,920
go for resync often i think that's a

01:29:32,480 --> 01:29:38,560
similar question to

01:29:33,920 --> 01:29:42,719
the reordering yeah

01:29:38,560 --> 01:29:45,760
and okay uh i think uh

01:29:42,719 --> 01:29:47,360
so that's about it uh so uh

01:29:45,760 --> 01:29:48,480
i think i'll thank you for everyone i

01:29:47,360 --> 01:29:49,440
think there's our three really good

01:29:48,480 --> 01:29:52,080
talks

01:29:49,440 --> 01:29:54,239
uh super excited to see tls performance

01:29:52,080 --> 01:29:57,520
getting such visibility

01:29:54,239 --> 01:29:59,440
um tls in general okay with that

01:29:57,520 --> 01:30:02,080
left adjourned and i'll see you in the

01:29:59,440 --> 01:30:07,840
next set of workshops

01:30:02,080 --> 01:30:07,840

YouTube URL: https://www.youtube.com/watch?v=rCiLuCYi4jI


