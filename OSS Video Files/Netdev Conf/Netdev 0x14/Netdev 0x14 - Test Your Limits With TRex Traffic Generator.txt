Title: Netdev 0x14 - Test Your Limits With TRex Traffic Generator
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speaker: Hanoch Haim

More info: https://netdevconf.info/0x14/session.html?talk-test-your-limits-with-trex-traffic-generator

Date: Tuesday, August 18, 2020

So you have 100Gbps port and you want to do high performance
network testing for your shiny new Linux kernel feature....
Folks, your options are pretty much limited to expensive,
commercial equipment(we are talking hundreds of thousands of $$).
Or you could use Trex.
Watch Hanoh talk about Trex.
Trex leverages COTS x86/ARM servers and most modern physical NICs.

What can it do for your perfomance testing?
- generate upto 200gbps/100mpps advance traffic pattern
- millions of real world tcp/udp flows both stateful and stateless
  for testing packet forwarding and end servers.
- Millions of Connection/s (CPS) for flows
- etc

Hanoh will share the experiences they had to deal with when writting
Trex that apply to scaling Linux networking; he hopes to inspire some
of the techniques to be adopted in Linux.
Captions: 
	00:00:02,000 --> 00:00:07,440
a t-rex traffic generator that started

00:00:04,960 --> 00:00:08,960
incubated in cisco system and we are

00:00:07,440 --> 00:00:11,599
using it internally and

00:00:08,960 --> 00:00:12,000
maybe it will be useful for you so let's

00:00:11,599 --> 00:00:13,759
start

00:00:12,000 --> 00:00:15,280
i will start with t-rex overview and

00:00:13,759 --> 00:00:18,160
then i will get into the

00:00:15,280 --> 00:00:19,039
mods more deeply so t-rex overview the

00:00:18,160 --> 00:00:22,160
problem that we had

00:00:19,039 --> 00:00:24,320
at start why we started it is a carrier

00:00:22,160 --> 00:00:26,080
grade or commercial traffic generator

00:00:24,320 --> 00:00:29,119
are super expensive

00:00:26,080 --> 00:00:31,039
not scalable and the main objective from

00:00:29,119 --> 00:00:31,840
our side it wasn't flexible we couldn't

00:00:31,039 --> 00:00:33,680
change it

00:00:31,840 --> 00:00:35,360
and the implication is quality the

00:00:33,680 --> 00:00:38,000
quality of our product

00:00:35,360 --> 00:00:39,360
and it means that we are limited late

00:00:38,000 --> 00:00:41,600
and late testing

00:00:39,360 --> 00:00:43,680
different benchmark and bottlenecks we

00:00:41,600 --> 00:00:45,039
cannot find the real bottlenecks of our

00:00:43,680 --> 00:00:49,039
product

00:00:45,039 --> 00:00:52,000
so how did you solve it tbx

00:00:49,039 --> 00:00:53,760
is a linux user space application it

00:00:52,000 --> 00:00:56,320
uses a dpdk library

00:00:53,760 --> 00:00:56,800
and by that we can reach very high rate

00:00:56,320 --> 00:00:59,440
because

00:00:56,800 --> 00:01:00,239
it has burst of packets in receiving

00:00:59,440 --> 00:01:03,120
transmit

00:01:00,239 --> 00:01:04,000
it uses standard hardware official

00:01:03,120 --> 00:01:06,640
hardware and

00:01:04,000 --> 00:01:08,560
nics and we can get very high

00:01:06,640 --> 00:01:11,439
performance like 200 gig for

00:01:08,560 --> 00:01:12,880
realistic traffic or 100 or 200 mega

00:01:11,439 --> 00:01:15,119
packets per second

00:01:12,880 --> 00:01:16,799
for a stateless i will talk about that

00:01:15,119 --> 00:01:17,920
it's flexible and it's open source you

00:01:16,799 --> 00:01:21,439
can change it to your

00:01:17,920 --> 00:01:23,600
own need if if it's it's required and

00:01:21,439 --> 00:01:24,560
supports fertilization it can work on

00:01:23,600 --> 00:01:27,840
azure

00:01:24,560 --> 00:01:30,880
on the new model and aws and

00:01:27,840 --> 00:01:34,159
there is very easy installation

00:01:30,880 --> 00:01:34,799
so what is the main operational mode of

00:01:34,159 --> 00:01:37,360
trx

00:01:34,799 --> 00:01:38,560
there is the stateless it's more for

00:01:37,360 --> 00:01:41,600
layer 2 and layer 3

00:01:38,560 --> 00:01:44,399
for switches and router and stateful

00:01:41,600 --> 00:01:45,439
which is more for stateful and complex

00:01:44,399 --> 00:01:48,960
feature like

00:01:45,439 --> 00:01:52,840
dpi knight and firewall that has a state

00:01:48,960 --> 00:01:56,479
for the packet and we have another two

00:01:52,840 --> 00:01:59,439
entity which is a services that works

00:01:56,479 --> 00:02:00,719
along the side to the traffic which is

00:01:59,439 --> 00:02:04,320
bird that we

00:02:00,719 --> 00:02:06,799
integrated bird is a open source daemon

00:02:04,320 --> 00:02:08,640
for routing protocol and can generate

00:02:06,799 --> 00:02:10,800
very high scale in the event driven of

00:02:08,640 --> 00:02:13,599
bgp ospf and isis

00:02:10,800 --> 00:02:14,080
and emu is something that we integrated

00:02:13,599 --> 00:02:16,080
for

00:02:14,080 --> 00:02:18,000
simulating clients in high school for

00:02:16,080 --> 00:02:18,959
example if you want one million client

00:02:18,000 --> 00:02:20,720
that do

00:02:18,959 --> 00:02:24,239
dot one x and authenticate to the

00:02:20,720 --> 00:02:26,319
network and do a dxcp and ipv6 and ipv4

00:02:24,239 --> 00:02:27,760
this is the way to go other solution

00:02:26,319 --> 00:02:30,400
didn't work for us

00:02:27,760 --> 00:02:31,360
okay and it works alongside the traffic

00:02:30,400 --> 00:02:33,440
so you can

00:02:31,360 --> 00:02:34,640
start with millions route of vgp and

00:02:33,440 --> 00:02:39,200
then start

00:02:34,640 --> 00:02:43,760
the traffic it's a stateless or stateful

00:02:39,200 --> 00:02:46,800
this slide shows the the main

00:02:43,760 --> 00:02:50,160
modes in in different way is there is

00:02:46,800 --> 00:02:53,440
stateful astf and stateless

00:02:50,160 --> 00:02:56,720
status again is for smart small

00:02:53,440 --> 00:02:59,680
switches or routers layer 2 and layer 3

00:02:56,720 --> 00:03:01,680
and advanced stateful is for smart

00:02:59,680 --> 00:03:03,519
features to test

00:03:01,680 --> 00:03:05,360
the high level architecture is splitted

00:03:03,519 --> 00:03:09,040
to server and client

00:03:05,360 --> 00:03:10,319
the server side is getting a json rpc

00:03:09,040 --> 00:03:13,519
over zeromq

00:03:10,319 --> 00:03:17,200
as a response a request response

00:03:13,519 --> 00:03:19,920
and it sends there is an event bus

00:03:17,200 --> 00:03:20,879
for messages to the course and core talk

00:03:19,920 --> 00:03:22,800
to the dpdk

00:03:20,879 --> 00:03:24,720
and send the traffic and there is a

00:03:22,800 --> 00:03:27,200
compiler there and

00:03:24,720 --> 00:03:29,599
i will show more about that the the left

00:03:27,200 --> 00:03:32,239
side is the control plane it's written

00:03:29,599 --> 00:03:34,400
it's a python wrapper on top of the json

00:03:32,239 --> 00:03:37,760
rpc and you can control

00:03:34,400 --> 00:03:39,599
automate a scenario of traffic

00:03:37,760 --> 00:03:42,640
for example load the profile start a

00:03:39,599 --> 00:03:43,840
traffic get statistics etc using an api

00:03:42,640 --> 00:03:46,319
everything is built on

00:03:43,840 --> 00:03:47,440
on top of this api and we have another

00:03:46,319 --> 00:03:50,000
two

00:03:47,440 --> 00:03:51,680
utility it's called a console that we

00:03:50,000 --> 00:03:54,799
are as a developer using

00:03:51,680 --> 00:03:57,280
to run the the

00:03:54,799 --> 00:03:58,159
scenarios and there is another gui that

00:03:57,280 --> 00:04:00,480
is built

00:03:58,159 --> 00:04:02,720
with a different group and is not

00:04:00,480 --> 00:04:06,080
maintained by us

00:04:02,720 --> 00:04:09,360
another way to look at it is this slide

00:04:06,080 --> 00:04:12,000
in this slide you can see that the main

00:04:09,360 --> 00:04:12,959
t-rex we call this project in git tvx

00:04:12,000 --> 00:04:16,000
core

00:04:12,959 --> 00:04:18,479
these represent the the

00:04:16,000 --> 00:04:21,440
the micro thread there is a control

00:04:18,479 --> 00:04:24,560
plane there is a data plane for each uh

00:04:21,440 --> 00:04:27,280
queue in the dpdk and there is a rx

00:04:24,560 --> 00:04:29,280
thread for measuring latency in

00:04:27,280 --> 00:04:31,680
resolution of all microseconds

00:04:29,280 --> 00:04:32,400
it's free this core is free and there is

00:04:31,680 --> 00:04:35,120
a message bus

00:04:32,400 --> 00:04:37,280
everything is event driven and zero

00:04:35,120 --> 00:04:39,520
sharing there is no locks at all

00:04:37,280 --> 00:04:40,560
this is the python that i talked in the

00:04:39,520 --> 00:04:43,440
last slide

00:04:40,560 --> 00:04:44,560
we can automate operation through the

00:04:43,440 --> 00:04:47,520
api here

00:04:44,560 --> 00:04:50,160
to the server and it can remove the user

00:04:47,520 --> 00:04:52,479
and emu and bird is located here

00:04:50,160 --> 00:04:53,600
bird is working on top of a linux

00:04:52,479 --> 00:04:57,040
namespace

00:04:53,600 --> 00:04:59,680
and can generate a high rate of bgp ospf

00:04:57,040 --> 00:05:01,840
and rip through vet so we have a switch

00:04:59,680 --> 00:05:05,120
of vet we can have

00:05:01,840 --> 00:05:06,800
many number of linux name space and emu

00:05:05,120 --> 00:05:07,919
works in a different way there is a

00:05:06,800 --> 00:05:10,240
tunnel

00:05:07,919 --> 00:05:11,759
with zero mq of the packet and it

00:05:10,240 --> 00:05:14,720
simulates its

00:05:11,759 --> 00:05:16,639
internal stack that simulates all the

00:05:14,720 --> 00:05:19,840
client control plane

00:05:16,639 --> 00:05:24,160
protocol at high rate for example.1x

00:05:19,840 --> 00:05:25,039
and authenticate the client itself okay

00:05:24,160 --> 00:05:28,320
let's start with

00:05:25,039 --> 00:05:30,080
stateless mode stateless mod is based on

00:05:28,320 --> 00:05:32,479
a profile

00:05:30,080 --> 00:05:33,919
based on stream stream based on packet

00:05:32,479 --> 00:05:36,320
it's a packet base

00:05:33,919 --> 00:05:38,720
and you can create many streams i will

00:05:36,320 --> 00:05:42,160
show what is what does it mean stream

00:05:38,720 --> 00:05:43,039
the scale is between 10 to 30 megapacket

00:05:42,160 --> 00:05:45,520
per second per

00:05:43,039 --> 00:05:48,080
for one core so you can easily get to

00:05:45,520 --> 00:05:48,880
100 or 200 it's depend on your pci

00:05:48,080 --> 00:05:52,479
bandwidth

00:05:48,880 --> 00:05:56,720
to a very high rate something that can

00:05:52,479 --> 00:05:59,280
can test a high high end router

00:05:56,720 --> 00:06:00,720
it has an interactive gui and tui and it

00:05:59,280 --> 00:06:03,840
can gather statistic

00:06:00,720 --> 00:06:04,560
per port or per stream latency jitter

00:06:03,840 --> 00:06:06,319
automation

00:06:04,560 --> 00:06:07,680
already talked about it can be

00:06:06,319 --> 00:06:09,600
multi-user

00:06:07,680 --> 00:06:10,960
number of user can be connected to the

00:06:09,600 --> 00:06:13,680
same server

00:06:10,960 --> 00:06:15,360
and acquire specific port and of course

00:06:13,680 --> 00:06:16,080
debug capability like capture the

00:06:15,360 --> 00:06:20,319
traffic

00:06:16,080 --> 00:06:22,880
analyze the traffic uh run some

00:06:20,319 --> 00:06:24,080
capability on on top of python like for

00:06:22,880 --> 00:06:26,080
example dhcp

00:06:24,080 --> 00:06:28,240
to debug a dhcp so you can get do

00:06:26,080 --> 00:06:31,280
everything in python

00:06:28,240 --> 00:06:32,160
let's talk about how the traffic is

00:06:31,280 --> 00:06:34,800
composed

00:06:32,160 --> 00:06:35,680
we call it the profile the profile is

00:06:34,800 --> 00:06:38,080
defining

00:06:35,680 --> 00:06:40,400
the traffic that you will send and in

00:06:38,080 --> 00:06:43,039
this example this simple example like

00:06:40,400 --> 00:06:44,319
hello world is we are defining three

00:06:43,039 --> 00:06:47,199
streams stream one

00:06:44,319 --> 00:06:48,800
stream two and three three stream one is

00:06:47,199 --> 00:06:50,319
continuous packet it's just the same

00:06:48,800 --> 00:06:50,880
packet again and again like in this

00:06:50,319 --> 00:06:54,240
example

00:06:50,880 --> 00:06:55,840
it's a ip a tcp packet the rate is one

00:06:54,240 --> 00:06:59,919
kilo packet per second you can

00:06:55,840 --> 00:07:02,160
set the rate and stream number two is

00:06:59,919 --> 00:07:04,880
the burst it's three packet

00:07:02,160 --> 00:07:05,599
and there is the inter packet guy inter

00:07:04,880 --> 00:07:09,039
stream gap

00:07:05,599 --> 00:07:10,000
it starts only after ifg the nice thing

00:07:09,039 --> 00:07:12,960
about that that

00:07:10,000 --> 00:07:14,080
stream number two can trigger that

00:07:12,960 --> 00:07:16,560
stream number three

00:07:14,080 --> 00:07:17,759
string number three we will not start at

00:07:16,560 --> 00:07:20,080
um

00:07:17,759 --> 00:07:20,880
at startup of the profile and stream

00:07:20,080 --> 00:07:23,360
number two

00:07:20,880 --> 00:07:24,400
will trigger it so you can we can see

00:07:23,360 --> 00:07:26,400
here

00:07:24,400 --> 00:07:28,080
first we have a template packet that we

00:07:26,400 --> 00:07:29,919
can send then the mode

00:07:28,080 --> 00:07:31,120
it's either continuous burst on

00:07:29,919 --> 00:07:34,319
multibelt

00:07:31,120 --> 00:07:37,520
and there is the capability to associate

00:07:34,319 --> 00:07:38,319
pro a stream to it to other stream and

00:07:37,520 --> 00:07:41,120
profile it's

00:07:38,319 --> 00:07:42,479
it's like a program of the traffic what

00:07:41,120 --> 00:07:44,319
do we want to send

00:07:42,479 --> 00:07:45,840
let's see how you define it in python

00:07:44,319 --> 00:07:48,160
because you need to define it

00:07:45,840 --> 00:07:49,840
in this example this is really simple

00:07:48,160 --> 00:07:53,599
example of a profile

00:07:49,840 --> 00:07:54,639
it has one stream which is ethernet or

00:07:53,599 --> 00:07:57,840
the ip

00:07:54,639 --> 00:07:58,720
or the udp over 10 xs which is the

00:07:57,840 --> 00:08:01,280
payload

00:07:58,720 --> 00:08:02,720
and then i define a stream with this

00:08:01,280 --> 00:08:05,680
packet

00:08:02,720 --> 00:08:06,639
and the mode of operation is continuous

00:08:05,680 --> 00:08:10,840
okay and the

00:08:06,639 --> 00:08:13,039
source and destination could be 16001 to

00:08:10,840 --> 00:08:14,800
00:08:13,039 --> 00:08:16,560
let's see another example more complex

00:08:14,800 --> 00:08:17,599
example in this case there are two

00:08:16,560 --> 00:08:21,440
stream

00:08:17,599 --> 00:08:23,440
one burst that starts after isg time

00:08:21,440 --> 00:08:24,639
in dot five second and then trigger

00:08:23,440 --> 00:08:27,360
multi burst

00:08:24,639 --> 00:08:29,199
stream which is the stream one let's see

00:08:27,360 --> 00:08:31,599
how it's defined that

00:08:29,199 --> 00:08:32,240
here we are defining an object of

00:08:31,599 --> 00:08:35,440
profile

00:08:32,240 --> 00:08:37,279
list of stream to stream and then here

00:08:35,440 --> 00:08:38,800
we need to define name for stream

00:08:37,279 --> 00:08:39,360
because there is association between

00:08:38,800 --> 00:08:43,200
stream

00:08:39,360 --> 00:08:46,399
so this is stream zero and this is

00:08:43,200 --> 00:08:48,800
stream one okay and next

00:08:46,399 --> 00:08:49,600
to stream zero is stream one so we will

00:08:48,800 --> 00:08:52,640
trigger that

00:08:49,600 --> 00:08:56,000
and out of seven is false so this will

00:08:52,640 --> 00:08:58,480
trigger it a burst of 10 packets

00:08:56,000 --> 00:08:59,440
and then it will trigger multi-burst of

00:08:58,480 --> 00:09:02,240
four packets

00:08:59,440 --> 00:09:03,839
multiple by five okay this is another

00:09:02,240 --> 00:09:06,240
example

00:09:03,839 --> 00:09:08,160
let's talk about field engine financing

00:09:06,240 --> 00:09:10,160
is the capability to change

00:09:08,160 --> 00:09:12,160
now sending the same packet again again

00:09:10,160 --> 00:09:14,480
it's not that

00:09:12,160 --> 00:09:15,519
nice we need something more so field

00:09:14,480 --> 00:09:17,920
engine is

00:09:15,519 --> 00:09:19,680
the way to change field inside the

00:09:17,920 --> 00:09:20,480
packet for example if i want to change

00:09:19,680 --> 00:09:22,720
the tools

00:09:20,480 --> 00:09:24,640
in the same stream if i want to send to

00:09:22,720 --> 00:09:25,680
change the range of the client or range

00:09:24,640 --> 00:09:28,160
of the server

00:09:25,680 --> 00:09:29,120
if i want to change the the packet size

00:09:28,160 --> 00:09:31,600
randomly

00:09:29,120 --> 00:09:32,560
this is the example so we built a

00:09:31,600 --> 00:09:34,720
program

00:09:32,560 --> 00:09:35,680
we built an interpreter that you can

00:09:34,720 --> 00:09:38,320
build program

00:09:35,680 --> 00:09:39,519
that associate with the stream that will

00:09:38,320 --> 00:09:42,320
change

00:09:39,519 --> 00:09:44,160
fields inside the packet in this example

00:09:42,320 --> 00:09:47,519
we create a syn attack that

00:09:44,160 --> 00:09:50,800
attack this filter 48001

00:09:47,519 --> 00:09:52,160
so we build the base packet of tcp scene

00:09:50,800 --> 00:09:54,080
with a flag scene

00:09:52,160 --> 00:09:56,399
and then we build this program let's see

00:09:54,080 --> 00:09:57,600
this program first we define variable

00:09:56,399 --> 00:10:03,440
ipsof

00:09:57,600 --> 00:10:06,880
we have a range of ids for 16.001 to

00:10:03,440 --> 00:10:07,920
255 and then we define another variable

00:10:06,880 --> 00:10:11,519
for source port

00:10:07,920 --> 00:10:13,760
we use random source port then we write

00:10:11,519 --> 00:10:14,720
so every packet that we generate we will

00:10:13,760 --> 00:10:16,720
choose

00:10:14,720 --> 00:10:18,320
source ip and source port we will write

00:10:16,720 --> 00:10:20,720
it to ipsource

00:10:18,320 --> 00:10:22,399
offset we will fix the checksum we

00:10:20,720 --> 00:10:25,920
resist the sap checksum

00:10:22,399 --> 00:10:28,320
and we write the tcp the source port so

00:10:25,920 --> 00:10:29,440
this will generate traffic from one

00:10:28,320 --> 00:10:32,800
stream

00:10:29,440 --> 00:10:35,200
to attack many number of clients

00:10:32,800 --> 00:10:37,680
attacking one server

00:10:35,200 --> 00:10:39,440
in very high rate let me show the rate

00:10:37,680 --> 00:10:41,360
let's talk about automation the python

00:10:39,440 --> 00:10:42,240
automation let's see an example how do

00:10:41,360 --> 00:10:45,440
we do that

00:10:42,240 --> 00:10:47,920
in this example we are

00:10:45,440 --> 00:10:48,959
taking an object of the client specify

00:10:47,920 --> 00:10:52,160
the the

00:10:48,959 --> 00:10:55,680
ip and the verbose log

00:10:52,160 --> 00:10:58,240
we are connecting to the server we are

00:10:55,680 --> 00:10:58,880
acquiring the port in this example 0 and

00:10:58,240 --> 00:11:01,680
00:10:58,880 --> 00:11:03,839
then we can add stream or add profile we

00:11:01,680 --> 00:11:05,360
can clear the statistic of the port

00:11:03,839 --> 00:11:07,519
and the stream by the way we can

00:11:05,360 --> 00:11:09,279
associate statistics per stream and

00:11:07,519 --> 00:11:11,360
latency in gta first stream

00:11:09,279 --> 00:11:13,920
we're adding some metadata into the

00:11:11,360 --> 00:11:16,160
stream and then we can start the traffic

00:11:13,920 --> 00:11:19,360
once we load the profile we can start

00:11:16,160 --> 00:11:22,959
the profile we can add multiple profile

00:11:19,360 --> 00:11:26,000
and we can multiply the rate

00:11:22,959 --> 00:11:27,440
the rate in the profile is a is not it's

00:11:26,000 --> 00:11:30,959
not

00:11:27,440 --> 00:11:31,680
absolute so we can take the rate higher

00:11:30,959 --> 00:11:33,360
and lower

00:11:31,680 --> 00:11:34,800
and then wait for the traffic get

00:11:33,360 --> 00:11:36,160
warning if there is in the server

00:11:34,800 --> 00:11:39,519
warning and disconnect

00:11:36,160 --> 00:11:42,000
this is really simple example let's see

00:11:39,519 --> 00:11:43,519
the performance this is mega packet per

00:11:42,000 --> 00:11:45,760
second per one core

00:11:43,519 --> 00:11:47,040
you can see that it depends on the mode

00:11:45,760 --> 00:11:51,120
it's between 10

00:11:47,040 --> 00:11:54,959
to 25 and the size of the packet is less

00:11:51,120 --> 00:11:57,279
of a factor so this is

00:11:54,959 --> 00:11:58,399
if you want performance and your feature

00:11:57,279 --> 00:12:01,440
is not that

00:11:58,399 --> 00:12:03,360
complicated this is the way to go let's

00:12:01,440 --> 00:12:06,079
talk about stateful

00:12:03,360 --> 00:12:07,920
stateful is more for features that are

00:12:06,079 --> 00:12:11,360
complex like load balancer

00:12:07,920 --> 00:12:14,000
dpi a firewall and not in those

00:12:11,360 --> 00:12:15,360
case the features are super complex and

00:12:14,000 --> 00:12:17,440
they have a context

00:12:15,360 --> 00:12:19,760
they have the context for a flow context

00:12:17,440 --> 00:12:23,040
for the client contact for the server

00:12:19,760 --> 00:12:24,720
caching of many things some packet need

00:12:23,040 --> 00:12:25,680
be packet inspection for example the

00:12:24,720 --> 00:12:27,360
certificate

00:12:25,680 --> 00:12:29,839
we need to analyze the certificate and

00:12:27,360 --> 00:12:32,320
look if the certificate is valid or not

00:12:29,839 --> 00:12:34,160
so some packets are huge amount of

00:12:32,320 --> 00:12:34,959
processing in some packets we know

00:12:34,160 --> 00:12:38,079
everything on

00:12:34,959 --> 00:12:38,720
on this flow so for understanding the

00:12:38,079 --> 00:12:40,880
bottleneck

00:12:38,720 --> 00:12:42,560
of those features there is a need for

00:12:40,880 --> 00:12:45,760
realistic traffic generation

00:12:42,560 --> 00:12:48,320
and this is the solution that we found

00:12:45,760 --> 00:12:49,040
the profile is defined differently it's

00:12:48,320 --> 00:12:52,240
defined

00:12:49,040 --> 00:12:54,000
with context of pool of clients pool of

00:12:52,240 --> 00:12:55,519
application like http

00:12:54,000 --> 00:12:58,160
and pools of servers so you can

00:12:55,519 --> 00:13:00,000
associate pools of applications for

00:12:58,160 --> 00:13:01,760
specific pools of clients for example

00:13:00,000 --> 00:13:04,880
engineering that uses

00:13:01,760 --> 00:13:08,240
specific application like vinci or

00:13:04,880 --> 00:13:10,800
or browsing specific browsing or citrix

00:13:08,240 --> 00:13:11,680
and and by that we can create more

00:13:10,800 --> 00:13:15,519
realistic

00:13:11,680 --> 00:13:18,000
uh traffic to exercise the device and

00:13:15,519 --> 00:13:20,880
understand where is the bottleneck

00:13:18,000 --> 00:13:21,440
in this mode by the way we can run into

00:13:20,880 --> 00:13:23,519
the in

00:13:21,440 --> 00:13:25,600
separate instances for example in the

00:13:23,519 --> 00:13:27,040
cloud if there is a compression between

00:13:25,600 --> 00:13:29,839
the client and the server

00:13:27,040 --> 00:13:30,639
as long as the layer 7 is the same trx

00:13:29,839 --> 00:13:32,399
will work

00:13:30,639 --> 00:13:34,000
one will be the client one will be the

00:13:32,399 --> 00:13:37,040
server and the state machine

00:13:34,000 --> 00:13:40,399
on top of this tcp will work

00:13:37,040 --> 00:13:40,880
how how this mode is is built so first

00:13:40,399 --> 00:13:44,160
we took

00:13:40,880 --> 00:13:48,480
a bsd tcp stack which is a kernel base

00:13:44,160 --> 00:13:50,800
and took it to user space and separate

00:13:48,480 --> 00:13:51,519
everything is again separated it's event

00:13:50,800 --> 00:13:54,079
driven

00:13:51,519 --> 00:13:55,440
and everything is very cool so the stack

00:13:54,079 --> 00:13:57,519
is per core

00:13:55,440 --> 00:13:59,680
per context there is no sharing of

00:13:57,519 --> 00:14:03,120
information even the counters are

00:13:59,680 --> 00:14:05,360
called and all the context is per

00:14:03,120 --> 00:14:06,480
core and we split the work between the

00:14:05,360 --> 00:14:09,839
calls

00:14:06,480 --> 00:14:11,920
and and by batching the packet in dpdk

00:14:09,839 --> 00:14:15,680
and batching the packet in the tcp stack

00:14:11,920 --> 00:14:17,360
we can achieve very high scale in memory

00:14:15,680 --> 00:14:19,279
and in performance and this is the

00:14:17,360 --> 00:14:20,639
reason we can can scan with the number

00:14:19,279 --> 00:14:22,959
of applications

00:14:20,639 --> 00:14:23,680
and we can of course measure latency and

00:14:22,959 --> 00:14:26,480
jitter

00:14:23,680 --> 00:14:28,560
and let's talk about the emulation layer

00:14:26,480 --> 00:14:29,440
how we emulate application it's not real

00:14:28,560 --> 00:14:31,920
application

00:14:29,440 --> 00:14:33,120
it's emulation of application this is an

00:14:31,920 --> 00:14:37,040
example of

00:14:33,120 --> 00:14:38,800
of a profile with let's say number of

00:14:37,040 --> 00:14:40,320
clients number of servers and this is

00:14:38,800 --> 00:14:42,320
the application we can take

00:14:40,320 --> 00:14:44,320
pickup files that represent an

00:14:42,320 --> 00:14:48,560
application like browsing

00:14:44,320 --> 00:14:51,839
video rtp or http or mail

00:14:48,560 --> 00:14:54,959
and amplify it for 100 gig

00:14:51,839 --> 00:14:58,079
and exercise the device under test

00:14:54,959 --> 00:14:59,600
this is an example of a profile

00:14:58,079 --> 00:15:01,199
it's written in python but it's

00:14:59,600 --> 00:15:04,079
different from stl

00:15:01,199 --> 00:15:04,959
here we are defining an stf profile and

00:15:04,079 --> 00:15:07,680
we can give

00:15:04,959 --> 00:15:08,880
list of pickup files that represent

00:15:07,680 --> 00:15:11,519
application

00:15:08,880 --> 00:15:12,160
each application we learn we analyze the

00:15:11,519 --> 00:15:14,959
application

00:15:12,160 --> 00:15:16,000
we analyze the request response and we

00:15:14,959 --> 00:15:19,760
build

00:15:16,000 --> 00:15:22,720
some instruction that say how to run

00:15:19,760 --> 00:15:24,959
this application on top of a tcp stack

00:15:22,720 --> 00:15:28,720
and of course we can associate

00:15:24,959 --> 00:15:32,160
a template to pools of client in this

00:15:28,720 --> 00:15:34,480
in this example there is default pool

00:15:32,160 --> 00:15:36,000
so all the template will be associated

00:15:34,480 --> 00:15:38,079
with a different client pool and so

00:15:36,000 --> 00:15:40,480
forth

00:15:38,079 --> 00:15:42,480
let's see how the emulation layer works

00:15:40,480 --> 00:15:45,519
in the emulation layer works there is a

00:15:42,480 --> 00:15:48,240
mini instruction and we in this case

00:15:45,519 --> 00:15:48,639
this is simple http request and response

00:15:48,240 --> 00:15:51,680
so

00:15:48,639 --> 00:15:54,320
we are building a request

00:15:51,680 --> 00:15:56,160
so we are sending this to the socket the

00:15:54,320 --> 00:15:56,880
request and wait for the response of the

00:15:56,160 --> 00:16:00,079
server

00:15:56,880 --> 00:16:03,360
the server has the op-sight

00:16:00,079 --> 00:16:06,639
instruction it is wait for the

00:16:03,360 --> 00:16:10,079
request and then answer with response

00:16:06,639 --> 00:16:13,279
the paradigm code beneath

00:16:10,079 --> 00:16:14,800
the the emulation layer

00:16:13,279 --> 00:16:16,399
it's like a socket like you are

00:16:14,800 --> 00:16:17,519
allocating a socket connect to the

00:16:16,399 --> 00:16:20,720
server

00:16:17,519 --> 00:16:22,480
write the request and read the response

00:16:20,720 --> 00:16:24,800
the nice thing about t-rex is that it's

00:16:22,480 --> 00:16:26,880
event driven we are doing it for you

00:16:24,800 --> 00:16:28,720
we just need to provide a pickup file

00:16:26,880 --> 00:16:30,320
and we analyze it and we will run the

00:16:28,720 --> 00:16:32,079
state machine and build it

00:16:30,320 --> 00:16:34,240
but you can still change the state

00:16:32,079 --> 00:16:36,480
machine now let's talk about

00:16:34,240 --> 00:16:37,360
why it's scalable why i cannot take

00:16:36,480 --> 00:16:40,399
linux

00:16:37,360 --> 00:16:42,480
and run it let's take an example let's

00:16:40,399 --> 00:16:44,800
say i want 10 million flows

00:16:42,480 --> 00:16:46,480
that attack the device under test with

00:16:44,800 --> 00:16:50,079
specific http

00:16:46,480 --> 00:16:52,639
in case of a normal tcp stack you have

00:16:50,079 --> 00:16:54,240
it a queue of packet sliding window of

00:16:52,639 --> 00:16:56,720
tx bucket let's say

00:16:54,240 --> 00:16:58,240
it's 32k which is very minimal today

00:16:56,720 --> 00:17:01,120
because of the rtt

00:16:58,240 --> 00:17:04,319
with 10 million flow you you need for

00:17:01,120 --> 00:17:06,720
the wolf's case about 400 gigabytes of

00:17:04,319 --> 00:17:08,480
traffic only for the transmit queue

00:17:06,720 --> 00:17:10,799
in our case we change the implementation

00:17:08,480 --> 00:17:13,760
because we know ahead of time what is

00:17:10,799 --> 00:17:15,360
the template data we just create a

00:17:13,760 --> 00:17:18,480
virtual cue

00:17:15,360 --> 00:17:20,240
that say in which point we are

00:17:18,480 --> 00:17:21,600
located and when we need to send the

00:17:20,240 --> 00:17:24,640
packet we go

00:17:21,600 --> 00:17:26,240
back to the constant memory and build

00:17:24,640 --> 00:17:28,160
everything it's the laser location

00:17:26,240 --> 00:17:31,280
instead of pushing the data

00:17:28,160 --> 00:17:33,679
we are asking the data from the

00:17:31,280 --> 00:17:34,400
layer or simulation and this way we are

00:17:33,679 --> 00:17:37,679
saving

00:17:34,400 --> 00:17:41,520
a lot of memory and can get to the scale

00:17:37,679 --> 00:17:44,720
of of uh performance of throughput

00:17:41,520 --> 00:17:48,160
and on scale and memory of

00:17:44,720 --> 00:17:49,919
memory scale okay let's talk about

00:17:48,160 --> 00:17:52,160
bird integration i talked about bird

00:17:49,919 --> 00:17:54,799
integration and emu integration and why

00:17:52,160 --> 00:17:57,200
do we need that bird integration is for

00:17:54,799 --> 00:17:59,520
routing protocol

00:17:57,200 --> 00:18:01,280
some use case of routing protocol is

00:17:59,520 --> 00:18:02,000
let's say i want to push one million

00:18:01,280 --> 00:18:04,400
routes

00:18:02,000 --> 00:18:06,480
to the switch and then send specific

00:18:04,400 --> 00:18:09,760
traffic that match those

00:18:06,480 --> 00:18:13,360
10 millions ram to push

00:18:09,760 --> 00:18:15,919
10 million route to a switch or router i

00:18:13,360 --> 00:18:16,640
i cannot do it through cli it's very

00:18:15,919 --> 00:18:20,000
very slow

00:18:16,640 --> 00:18:24,480
so the way to do it is using some

00:18:20,000 --> 00:18:26,880
daemon that implement the bgp and ospf

00:18:24,480 --> 00:18:28,000
and rip and we found that the daemon

00:18:26,880 --> 00:18:30,320
bird demon is fast

00:18:28,000 --> 00:18:31,440
enough so we run it on the linux

00:18:30,320 --> 00:18:34,880
namespace

00:18:31,440 --> 00:18:38,400
and open vet and create a way that we

00:18:34,880 --> 00:18:41,760
can talk to a processor

00:18:38,400 --> 00:18:44,240
that attached to a linux namespace and

00:18:41,760 --> 00:18:45,760
create an api the same python api that

00:18:44,240 --> 00:18:48,799
we have through tvx

00:18:45,760 --> 00:18:52,160
and simplify things that you can push

00:18:48,799 --> 00:18:54,799
one million drought and in a few seconds

00:18:52,160 --> 00:18:56,799
you can push all the route and then run

00:18:54,799 --> 00:18:58,320
the stateless or stateful traffic

00:18:56,799 --> 00:19:00,799
scenario that match

00:18:58,320 --> 00:19:02,559
those route so this is the bad

00:19:00,799 --> 00:19:05,120
integration bird is an open source

00:19:02,559 --> 00:19:07,600
we didn't invented it we just integrated

00:19:05,120 --> 00:19:09,679
it into t-rex

00:19:07,600 --> 00:19:11,600
okay let's talk about emu service that

00:19:09,679 --> 00:19:14,960
you can run in parallel like

00:19:11,600 --> 00:19:17,919
the bird emu services

00:19:14,960 --> 00:19:18,799
why did the question is why why why what

00:19:17,919 --> 00:19:22,080
is the requirement

00:19:18,799 --> 00:19:25,200
what what we tried to solve so first

00:19:22,080 --> 00:19:27,840
the requirement was that we need to

00:19:25,200 --> 00:19:29,120
emulate clients on the same network

00:19:27,840 --> 00:19:31,919
let's say

00:19:29,120 --> 00:19:32,400
64k clients so we need to implement up

00:19:31,919 --> 00:19:36,840
in

00:19:32,400 --> 00:19:39,440
ipv6 nd rfc to create a neighbor

00:19:36,840 --> 00:19:42,080
solicitation and all this rfc and

00:19:39,440 --> 00:19:45,440
protocol and then they came with igmp

00:19:42,080 --> 00:19:47,120
and mld i want multicast to membership

00:19:45,440 --> 00:19:49,600
of multicast addresses

00:19:47,120 --> 00:19:51,679
in case of multicast addresses it

00:19:49,600 --> 00:19:52,080
becomes more complex because if you want

00:19:51,679 --> 00:19:54,880
to do

00:19:52,080 --> 00:19:56,480
it with linux namespace you need to open

00:19:54,880 --> 00:19:58,480
a socket in the process

00:19:56,480 --> 00:20:01,679
and become complex and then they come

00:19:58,480 --> 00:20:04,400
with dhcp requirement the xp v6 dcp4

00:20:01,679 --> 00:20:06,240
and then we need to authenticate this

00:20:04,400 --> 00:20:08,720
client through the network because

00:20:06,240 --> 00:20:10,480
without authenticating it we cannot send

00:20:08,720 --> 00:20:13,120
traffic

00:20:10,480 --> 00:20:14,000
so this is the we needed to solve this

00:20:13,120 --> 00:20:17,520
requirement

00:20:14,000 --> 00:20:21,120
in holistic way that could be scalable

00:20:17,520 --> 00:20:23,360
and and would be simple to implement

00:20:21,120 --> 00:20:24,240
this is the requirement from rfc

00:20:23,360 --> 00:20:26,799
perspective

00:20:24,240 --> 00:20:29,039
this is the start of the requirement so

00:20:26,799 --> 00:20:32,960
we needed rfc icmp igmp

00:20:29,039 --> 00:20:35,919
mld ipv6 dhcp.1x

00:20:32,960 --> 00:20:36,640
tls on top of tcp and udp it's a full

00:20:35,919 --> 00:20:39,200
stack

00:20:36,640 --> 00:20:40,880
it's a full stack in user space and it

00:20:39,200 --> 00:20:41,840
should be lightweight and support all

00:20:40,880 --> 00:20:44,080
the protocol

00:20:41,840 --> 00:20:45,520
the control plane of the client the

00:20:44,080 --> 00:20:49,120
implementation

00:20:45,520 --> 00:20:52,400
what became the solution that we came to

00:20:49,120 --> 00:20:53,360
is emu it's a processing go even event

00:20:52,400 --> 00:20:56,559
driven

00:20:53,360 --> 00:20:59,280
that it's written it's like a plugin

00:20:56,559 --> 00:21:00,640
and there is a tunnel of packets so it

00:20:59,280 --> 00:21:02,559
can generate packet

00:21:00,640 --> 00:21:04,320
and set it through this tunnel and the

00:21:02,559 --> 00:21:07,600
rx core will send it to

00:21:04,320 --> 00:21:08,320
the port and back there is filter to the

00:21:07,600 --> 00:21:11,039
rx

00:21:08,320 --> 00:21:11,919
and back to the emu and it's really easy

00:21:11,039 --> 00:21:13,840
to implement

00:21:11,919 --> 00:21:16,080
those protocols plug-in because it's not

00:21:13,840 --> 00:21:19,679
dependable it's very fast to develop

00:21:16,080 --> 00:21:22,640
and so forth the design pattern

00:21:19,679 --> 00:21:24,240
it's pluggable with eventbus every

00:21:22,640 --> 00:21:28,240
plugin can attach

00:21:24,240 --> 00:21:30,159
to specific context client context

00:21:28,240 --> 00:21:31,760
or namespace context i will talk about

00:21:30,159 --> 00:21:35,200
what is namespace context

00:21:31,760 --> 00:21:37,120
and what is red context and

00:21:35,200 --> 00:21:39,039
and by that it can implement the state

00:21:37,120 --> 00:21:41,360
machine of the rfc

00:21:39,039 --> 00:21:42,960
and if there is a need to send an event

00:21:41,360 --> 00:21:45,520
to another plugin that

00:21:42,960 --> 00:21:47,679
there is no need to know about you can

00:21:45,520 --> 00:21:50,960
send an event you can file an event

00:21:47,679 --> 00:21:52,720
and the other other plugin can register

00:21:50,960 --> 00:21:53,679
to this event for example if there is a

00:21:52,720 --> 00:21:57,679
change

00:21:53,679 --> 00:22:00,480
in ip of the client the app can

00:21:57,679 --> 00:22:01,840
trigger again and the sap can trigger

00:22:00,480 --> 00:22:04,640
this event

00:22:01,840 --> 00:22:05,280
the same for that one x and and so forth

00:22:04,640 --> 00:22:08,720
so the

00:22:05,280 --> 00:22:11,280
h plugin can register callbacks of rfc

00:22:08,720 --> 00:22:13,039
counters time you can use timer you can

00:22:11,280 --> 00:22:17,200
use a simulator framework

00:22:13,039 --> 00:22:17,200
and login capability and so forth

00:22:18,080 --> 00:22:22,400
just to talk about namespace why what is

00:22:21,360 --> 00:22:25,679
namespace

00:22:22,400 --> 00:22:28,320
in case of namespace is our way because

00:22:25,679 --> 00:22:29,840
everything is implemented in one

00:22:28,320 --> 00:22:31,360
all the clients are implemented on the

00:22:29,840 --> 00:22:34,320
same

00:22:31,360 --> 00:22:36,320
context of thread we in case of

00:22:34,320 --> 00:22:38,080
broadcast and multicast we didn't want

00:22:36,320 --> 00:22:40,320
that the packet will be

00:22:38,080 --> 00:22:41,440
duplicate to all the client we wanted

00:22:40,320 --> 00:22:43,919
shared memory

00:22:41,440 --> 00:22:45,760
so for example in app we have one table

00:22:43,919 --> 00:22:47,919
that shares for all the names

00:22:45,760 --> 00:22:49,919
for all the client on the same namespace

00:22:47,919 --> 00:22:51,200
in case there is a broadcast packet it

00:22:49,919 --> 00:22:53,840
update the table

00:22:51,200 --> 00:22:55,360
in one namespace and then all the

00:22:53,840 --> 00:22:57,120
clients get

00:22:55,360 --> 00:22:58,640
their response there is no need for

00:22:57,120 --> 00:23:02,159
duplication of packets

00:22:58,640 --> 00:23:05,679
and by that we can get scale

00:23:02,159 --> 00:23:08,559
example of a profile okay

00:23:05,679 --> 00:23:09,280
this is a profile of emu it's different

00:23:08,559 --> 00:23:11,840
than

00:23:09,280 --> 00:23:13,600
state full and stateless and you can see

00:23:11,840 --> 00:23:17,520
that we can define

00:23:13,600 --> 00:23:20,159
a client object in this case the mac

00:23:17,520 --> 00:23:21,440
is the key we cannot have two entity

00:23:20,159 --> 00:23:25,520
with the same mac

00:23:21,440 --> 00:23:27,919
we can associate it with namespace.onex

00:23:25,520 --> 00:23:29,039
or virtual interface or physical

00:23:27,919 --> 00:23:32,559
interface

00:23:29,039 --> 00:23:33,440
and then we can add plugin every plugin

00:23:32,559 --> 00:23:34,799
is generic

00:23:33,440 --> 00:23:36,919
and we can add all the information on

00:23:34,799 --> 00:23:39,840
this plugin like arp

00:23:36,919 --> 00:23:43,039
icmp.1x dhcp

00:23:39,840 --> 00:23:47,440
ipv6 and so forth

00:23:43,039 --> 00:23:50,640
okay i think i gave you just a hint

00:23:47,440 --> 00:23:53,919
of the capability so let me

00:23:50,640 --> 00:23:54,720
just summarize tx is stateful and

00:23:53,919 --> 00:23:57,840
stateless

00:23:54,720 --> 00:24:00,000
everything works on on top of an api and

00:23:57,840 --> 00:24:03,039
it have two services emu

00:24:00,000 --> 00:24:06,720
for client emulation and bird

00:24:03,039 --> 00:24:11,360
an open source bird for routing protocol

00:24:06,720 --> 00:24:11,360
emulation that's it

00:24:14,400 --> 00:24:20,480
okay um thank you

00:24:17,679 --> 00:24:22,000
so uh we have a lot of questions um but

00:24:20,480 --> 00:24:24,559
before we dive into them

00:24:22,000 --> 00:24:25,679
i'd like to ask kind of uh a higher

00:24:24,559 --> 00:24:28,640
level level question

00:24:25,679 --> 00:24:30,559
what is the what is the nature of this

00:24:28,640 --> 00:24:31,279
project is this intended to be an open

00:24:30,559 --> 00:24:33,840
source

00:24:31,279 --> 00:24:35,279
uh project have you open source the code

00:24:33,840 --> 00:24:38,159
or is there an intent to build a

00:24:35,279 --> 00:24:38,159
community around it

00:24:39,360 --> 00:24:42,880
i don't know how to answer it but it's

00:24:41,360 --> 00:24:47,120
open source

00:24:42,880 --> 00:24:49,679
and everyone is welcome to use it and

00:24:47,120 --> 00:24:50,960
share the knowledge of the linux

00:24:49,679 --> 00:24:54,159
community

00:24:50,960 --> 00:24:57,600
but our need was to use

00:24:54,159 --> 00:25:00,159
to use it for internal testing of

00:24:57,600 --> 00:25:01,200
cisco gear and we open sourcing because

00:25:00,159 --> 00:25:04,000
you cannot

00:25:01,200 --> 00:25:04,640
do internal testing without sharing how

00:25:04,000 --> 00:25:08,320
you test

00:25:04,640 --> 00:25:08,559
right you need everyone to test it in

00:25:08,320 --> 00:25:12,720
the

00:25:08,559 --> 00:25:15,440
same way so user could

00:25:12,720 --> 00:25:17,360
have you seen much traction outside of

00:25:15,440 --> 00:25:20,480
cisco to use this

00:25:17,360 --> 00:25:20,880
yeah i'm using it internally and i know

00:25:20,480 --> 00:25:23,279
that

00:25:20,880 --> 00:25:24,720
small companies are using it internally

00:25:23,279 --> 00:25:26,840
and it's user space so

00:25:24,720 --> 00:25:29,760
it's not sorry it's not the linux

00:25:26,840 --> 00:25:33,279
scalable

00:25:29,760 --> 00:25:35,760
but you can use it to test linux kernel

00:25:33,279 --> 00:25:35,760
models

00:25:36,480 --> 00:25:39,679
okay uh so there were some questions

00:25:39,039 --> 00:25:43,360
about

00:25:39,679 --> 00:25:44,880
features one i'd like to cover so

00:25:43,360 --> 00:25:48,320
is there anything that prohibits this

00:25:44,880 --> 00:25:52,159
from being ported to af xdp

00:25:48,320 --> 00:25:55,919
no as i said in the comments

00:25:52,159 --> 00:25:57,120
every pmd could be converted and use the

00:25:55,919 --> 00:25:58,799
basic features

00:25:57,120 --> 00:26:00,799
but there are advanced features in

00:25:58,799 --> 00:26:01,440
traffic generator for example hardware

00:26:00,799 --> 00:26:05,520
counters

00:26:01,440 --> 00:26:08,880
in an outdoor or

00:26:05,520 --> 00:26:12,000
scale of advanced stateful how to split

00:26:08,880 --> 00:26:16,159
the traffic to cost we are using rss

00:26:12,000 --> 00:26:18,880
with specific so if the driver does not

00:26:16,159 --> 00:26:21,200
have this capability it won't be able to

00:26:18,880 --> 00:26:24,880
scale to a number of core for example

00:26:21,200 --> 00:26:26,000
so there are there is an internal driver

00:26:24,880 --> 00:26:29,360
that uses

00:26:26,000 --> 00:26:31,600
standard pmd driver and in this driver

00:26:29,360 --> 00:26:34,480
you are specifying what the driver

00:26:31,600 --> 00:26:36,799
is capable of you are asking it and do

00:26:34,480 --> 00:26:36,799
some

00:26:36,840 --> 00:26:42,480
some shim api

00:26:39,679 --> 00:26:43,039
to accommodate that and expose more

00:26:42,480 --> 00:26:47,760
feature

00:26:43,039 --> 00:26:50,559
of the driver if there is if you can

00:26:47,760 --> 00:26:51,279
so the short answer every pmd can be

00:26:50,559 --> 00:26:54,320
used

00:26:51,279 --> 00:26:57,200
but maybe not with the whole feature

00:26:54,320 --> 00:26:57,200
with all the features

00:26:58,400 --> 00:27:06,000
okay so does t-rex support http 2

00:27:01,919 --> 00:27:06,000
quick for layer 7 testing

00:27:11,520 --> 00:27:18,159
again can you repeat the question

00:27:14,799 --> 00:27:21,840
does t-rex support http 2

00:27:18,159 --> 00:27:25,600
and quick for uh testing

00:27:21,840 --> 00:27:29,760
well from advanced let me maybe um

00:27:25,600 --> 00:27:32,880
let me say a bit about that for astf

00:27:29,760 --> 00:27:33,360
we have a tcp and udp stack and on top

00:27:32,880 --> 00:27:36,320
of that

00:27:33,360 --> 00:27:38,880
we have we are replay we play the

00:27:36,320 --> 00:27:41,600
protocol on top of tcp

00:27:38,880 --> 00:27:42,399
so for example you can take a tls pick

00:27:41,600 --> 00:27:45,919
up file

00:27:42,399 --> 00:27:50,240
and replay the layer 7. so if the

00:27:45,919 --> 00:27:53,039
device under test doesn't do a tls proxy

00:27:50,240 --> 00:27:54,399
it will work if you do a dpi or if you

00:27:53,039 --> 00:27:57,200
do not

00:27:54,399 --> 00:27:58,399
if you do because you have the two sides

00:27:57,200 --> 00:28:02,080
it will work

00:27:58,399 --> 00:28:04,080
but if you want to do a test for these

00:28:02,080 --> 00:28:07,360
tls boxes uh

00:28:04,080 --> 00:28:11,120
for my understanding it's it's become

00:28:07,360 --> 00:28:12,559
impossible you know in tls 103 and so

00:28:11,120 --> 00:28:14,320
forth

00:28:12,559 --> 00:28:16,640
and because of that we didn't invest in

00:28:14,320 --> 00:28:16,640
that

00:28:16,960 --> 00:28:23,520
you you won't be able to do it in astf

00:28:20,320 --> 00:28:26,799
but with emu we have

00:28:23,520 --> 00:28:29,600
a user space stack that can do for

00:28:26,799 --> 00:28:30,799
scale with clients it's similar to the

00:28:29,600 --> 00:28:34,480
last

00:28:30,799 --> 00:28:38,640
meeting you know the kernel

00:28:34,480 --> 00:28:40,720
timestamp times the future timestamp

00:28:38,640 --> 00:28:42,240
but it's much more scalable because it's

00:28:40,720 --> 00:28:44,480
written from scratch and

00:28:42,240 --> 00:28:46,559
it's lightweight and you can scale

00:28:44,480 --> 00:28:51,120
millions of clients with one thread

00:28:46,559 --> 00:28:56,159
millions of vm we're very lightweight

00:28:51,120 --> 00:28:56,159
memory footprint of memory and cpu

00:28:56,320 --> 00:28:59,440
okay there were several questions about

00:28:58,720 --> 00:29:01,919
the cache

00:28:59,440 --> 00:29:04,480
um i don't quite understand uh the

00:29:01,919 --> 00:29:07,360
question maybe or somewhere

00:29:04,480 --> 00:29:08,880
so for field engine field engine is

00:29:07,360 --> 00:29:11,679
working per packet

00:29:08,880 --> 00:29:12,720
so you can run the field engine every

00:29:11,679 --> 00:29:16,000
packet i mean

00:29:12,720 --> 00:29:16,720
if you have infinite permutation it will

00:29:16,000 --> 00:29:20,240
do that

00:29:16,720 --> 00:29:23,679
but there is a cache mode cache won't

00:29:20,240 --> 00:29:26,720
mean that you run the field engine

00:29:23,679 --> 00:29:27,600
offline save the packet save all the

00:29:26,720 --> 00:29:30,320
permutation

00:29:27,600 --> 00:29:31,919
and then repeat them replay the packet

00:29:30,320 --> 00:29:34,559
so you have only limited

00:29:31,919 --> 00:29:35,919
permutation as your memory for example

00:29:34,559 --> 00:29:39,279
you can replay

00:29:35,919 --> 00:29:40,240
a field engine of 20k packets and then

00:29:39,279 --> 00:29:42,480
replay

00:29:40,240 --> 00:29:43,679
now replay the packet is very cheap

00:29:42,480 --> 00:29:46,799
because you have

00:29:43,679 --> 00:29:49,760
all the m-buff at hand right so it's

00:29:46,799 --> 00:29:51,679
faster much faster than the field engine

00:29:49,760 --> 00:29:54,159
that you need to allocate memory

00:29:51,679 --> 00:29:56,320
copy the packets change the field

00:29:54,159 --> 00:29:59,039
etcetera

00:29:56,320 --> 00:30:00,480
this is this is the difference between

00:29:59,039 --> 00:30:03,200
cash and non-cash but

00:30:00,480 --> 00:30:04,080
in cash you don't have all the limit all

00:30:03,200 --> 00:30:07,200
the

00:30:04,080 --> 00:30:09,360
capability of all the possibilities

00:30:07,200 --> 00:30:11,679
of the packets for example if you have

00:30:09,360 --> 00:30:15,200
unlimited number of possibilities

00:30:11,679 --> 00:30:15,200
it won't work with cash

00:30:18,000 --> 00:30:24,559
okay and then there was questions

00:30:21,440 --> 00:30:26,480
about simulating jitter

00:30:24,559 --> 00:30:29,120
and i guess we can kind of generalize

00:30:26,480 --> 00:30:33,679
that simulate various traffic conditions

00:30:29,120 --> 00:30:36,720
latency uh what have you yes um

00:30:33,679 --> 00:30:37,520
we can do that we can measure latency in

00:30:36,720 --> 00:30:40,240
parallel

00:30:37,520 --> 00:30:41,360
with the alex core that is free and

00:30:40,240 --> 00:30:44,559
because of that

00:30:41,360 --> 00:30:47,679
and we are using very high speed

00:30:44,559 --> 00:30:50,080
qos in the egress so

00:30:47,679 --> 00:30:51,120
the resolution of the latency packet is

00:30:50,080 --> 00:30:55,360
really good

00:30:51,120 --> 00:30:55,360
and in parallel you can tweak

00:30:55,520 --> 00:31:01,440
a language to specify how the

00:30:58,799 --> 00:31:02,159
traffic is is working so you can

00:31:01,440 --> 00:31:05,039
generally

00:31:02,159 --> 00:31:05,039
you can simulate

00:31:12,080 --> 00:31:17,919
uh somebody's not muted

00:31:15,279 --> 00:31:19,360
uh what is the actual packets per second

00:31:17,919 --> 00:31:22,320
when using the field engine

00:31:19,360 --> 00:31:23,919
i have not seen 300 megabits per sec per

00:31:22,320 --> 00:31:25,600
300 million packets per second i was

00:31:23,919 --> 00:31:28,000
wondering

00:31:25,600 --> 00:31:28,880
about the slide had the cash balance i

00:31:28,000 --> 00:31:31,679
showed for

00:31:28,880 --> 00:31:32,720
well in the example it's depend on the

00:31:31,679 --> 00:31:37,360
other way right

00:31:32,720 --> 00:31:40,480
so for cisco ucs with intel xl710

00:31:37,360 --> 00:31:43,600
i think we reached about 10

00:31:40,480 --> 00:31:44,559
a mega packet per second for one call so

00:31:43,600 --> 00:31:48,000
you can easily

00:31:44,559 --> 00:31:52,480
fill up the the bandwidth

00:31:48,000 --> 00:31:52,480
of techniques i think

00:31:53,600 --> 00:31:56,559
in one use yes

00:31:58,840 --> 00:32:05,039
okay so uh

00:32:01,440 --> 00:32:05,039
let's go ahead and move on um

00:32:05,279 --> 00:32:09,279
thank you that's uh very interesting uh

00:32:08,000 --> 00:32:11,919
i do think it would be

00:32:09,279 --> 00:32:14,559
nice to make sure that we hit the links

00:32:11,919 --> 00:32:15,760
to that open source um

00:32:14,559 --> 00:32:17,840
i think there's going to be a lot of

00:32:15,760 --> 00:32:18,799
interest in this and might even be some

00:32:17,840 --> 00:32:21,519
interest in

00:32:18,799 --> 00:32:22,559
perhaps developing against it which

00:32:21,519 --> 00:32:26,880
would be uh

00:32:22,559 --> 00:32:29,840
obviously really nice okay thank you

00:32:26,880 --> 00:32:29,840

YouTube URL: https://www.youtube.com/watch?v=G2v0bk6Wd8w


