Title: Netdev 0x14 - Workshop on XDP
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Chairs: Saeed Mahameed, David Ahern, Sameeh Jubran, Ilias Apalodimas, Sridhar Samudrala, Jesper Dangaard Brouer, Jonathan Lemon

More info: https://netdevconf.info/0x14/session.html?workshop-XDP

Date: Thursday, August 13, 2020

Saeed Mahameed chairs the netdev 0x14 XDP workshop.

Many discussions on a variety of XDP issues will take 
place - including follow ups on outstanding challenges, updates
on existing features and new ideas on a variety of topics ranging
from performance, acceleration and many more.

Partipants include:
Saeed Mahameed, David Ahern, Sameeh Jubran, Ilias Apalodimas, Sridhar Samudrala, Jesper Dangaard Brouer, Jonathan Lemon

Workshop agenda includes:

- XDP HW metadata hints
- XDP on hosts pain points summary
- XDP egress, development status and use case
- Page pool skb recycling
- Performance gains seen with iommu
- Death-by-a-thousand-paper-cuts parable (related to XDP!)
- etc
Captions: 
	00:00:03,199 --> 00:00:08,160
hi everyone i would like to

00:00:04,560 --> 00:00:10,720
welcome everyone to the xdp workshop

00:00:08,160 --> 00:00:10,720
um

00:00:13,040 --> 00:00:18,720
so before before we start i would like

00:00:16,320 --> 00:00:22,320
to discuss

00:00:18,720 --> 00:00:24,800
the agenda today and uh before we dive

00:00:22,320 --> 00:00:25,760
deep into technical details i would like

00:00:24,800 --> 00:00:28,320
to take the chance

00:00:25,760 --> 00:00:30,640
and introduce today's topics and our

00:00:28,320 --> 00:00:33,920
panel members

00:00:30,640 --> 00:00:36,559
and uh if we had the chance i would also

00:00:33,920 --> 00:00:39,360
try to quickly review xdp development

00:00:36,559 --> 00:00:42,640
updates in the past year and i can't

00:00:39,360 --> 00:00:46,000
ongoing related work to the talks and

00:00:42,640 --> 00:00:49,680
discussions we will have here today

00:00:46,000 --> 00:00:51,520
after that we will move to uh 10 to 15

00:00:49,680 --> 00:00:54,320
minutes technical discussion

00:00:51,520 --> 00:00:55,920
for the following topics uh we will

00:00:54,320 --> 00:00:59,120
start with the xvp

00:00:55,920 --> 00:01:01,600
and which is basically

00:00:59,120 --> 00:01:02,719
passing metadata from driver to xvp

00:01:01,600 --> 00:01:06,080
programs

00:01:02,719 --> 00:01:09,520
and um then

00:01:06,080 --> 00:01:12,320
uh david uh iron here he is

00:01:09,520 --> 00:01:13,920
part of the panel and he will summarize

00:01:12,320 --> 00:01:16,960
his experience with xdp

00:01:13,920 --> 00:01:21,920
and i will show some pain points he

00:01:16,960 --> 00:01:21,920
been experiencing while using zdp

00:01:23,680 --> 00:01:28,479
also please note that david will have a

00:01:25,439 --> 00:01:30,320
detailed tutorial on this topic and uh

00:01:28,479 --> 00:01:33,280
in this conference and so please don't

00:01:30,320 --> 00:01:36,560
hesitate to join this session

00:01:33,280 --> 00:01:39,280
uh that uh david will discuss xdp

00:01:36,560 --> 00:01:40,560
eagerness and development status and the

00:01:39,280 --> 00:01:43,840
use case

00:01:40,560 --> 00:01:47,119
then we will move to multibuffer for

00:01:43,840 --> 00:01:50,560
zp uh by samir jobran

00:01:47,119 --> 00:01:53,840
and then elias will talk about

00:01:50,560 --> 00:01:56,799
sdp time sensitive networking and uh

00:01:53,840 --> 00:02:00,479
timestamping for xvp

00:01:56,799 --> 00:02:04,079
busy pole support for 860p sockets

00:02:00,479 --> 00:02:06,560
by sridhar from intel

00:02:04,079 --> 00:02:07,200
and uh last thing we're going to discuss

00:02:06,560 --> 00:02:09,200
today

00:02:07,200 --> 00:02:10,879
it's going to be an open mic discussion

00:02:09,200 --> 00:02:14,160
but mainly

00:02:10,879 --> 00:02:16,720
we're going to discuss some performance

00:02:14,160 --> 00:02:19,920
aspects

00:02:16,720 --> 00:02:23,280
xvp performance page pool and

00:02:19,920 --> 00:02:24,000
zero copy with header data split mostly

00:02:23,280 --> 00:02:27,280
work by

00:02:24,000 --> 00:02:27,280
jasper jonathan

00:02:27,760 --> 00:02:31,599
they will be discussing their work on

00:02:30,640 --> 00:02:35,599
this area

00:02:31,599 --> 00:02:36,080
and for each topic here we will have

00:02:35,599 --> 00:02:38,160
we'll

00:02:36,080 --> 00:02:40,000
try to have five minutes average

00:02:38,160 --> 00:02:45,120
questions

00:02:40,000 --> 00:02:45,120
and free discussion um

00:02:45,360 --> 00:02:49,360
just before we dive in uh i would like

00:02:48,160 --> 00:02:52,319
to

00:02:49,360 --> 00:02:54,319
discuss or just review the development

00:02:52,319 --> 00:02:55,040
updates in the past year i i'm sure

00:02:54,319 --> 00:02:57,599
there were

00:02:55,040 --> 00:02:59,200
more than this but these are the topics

00:02:57,599 --> 00:03:01,840
or features

00:02:59,200 --> 00:03:03,120
that slightly relate to our discussions

00:03:01,840 --> 00:03:06,879
today

00:03:03,120 --> 00:03:08,000
so xdp programs in maps cpu and div maps

00:03:06,879 --> 00:03:10,159
which is basically

00:03:08,000 --> 00:03:11,680
the implementation of epa grace was

00:03:10,159 --> 00:03:14,720
accepted

00:03:11,680 --> 00:03:18,720
uh ppf xvp are just still

00:03:14,720 --> 00:03:22,480
to grow the packet size bounded a lot of

00:03:18,720 --> 00:03:24,560
bulking improvements and uh paid pool

00:03:22,480 --> 00:03:28,480
adaption for many drivers

00:03:24,560 --> 00:03:30,720
and many areas of the stack

00:03:28,480 --> 00:03:31,599
and uh this is a lot before done by

00:03:30,720 --> 00:03:35,360
jasper and

00:03:31,599 --> 00:03:36,799
turkey to improve using blocking game

00:03:35,360 --> 00:03:40,799
mechanisms that we were

00:03:36,799 --> 00:03:40,799
talking about in the past few years

00:03:40,879 --> 00:03:45,519
um one interesting topic is the

00:03:43,599 --> 00:03:47,680
introduction introduction of dynamic

00:03:45,519 --> 00:03:48,799
program extension by elixir where you

00:03:47,680 --> 00:03:51,440
can

00:03:48,799 --> 00:03:53,200
change a behavior of your xvp program

00:03:51,440 --> 00:03:55,840
without even

00:03:53,200 --> 00:03:58,319
unloading and reloading the program

00:03:55,840 --> 00:03:58,319
itself

00:03:58,560 --> 00:04:04,080
some improvements to the page pool where

00:04:00,799 --> 00:04:06,799
we made it move aware by jonathan and i

00:04:04,080 --> 00:04:08,560
and we will discuss this briefly in that

00:04:06,799 --> 00:04:11,920
performance

00:04:08,560 --> 00:04:15,439
um also a lot of work on

00:04:11,920 --> 00:04:16,320
fxdp uh i attached some links i'm not

00:04:15,439 --> 00:04:18,560
going to discuss

00:04:16,320 --> 00:04:22,079
all of these in detail so please you

00:04:18,560 --> 00:04:25,120
know go see the related work

00:04:22,079 --> 00:04:28,400
um also lots of driver made it

00:04:25,120 --> 00:04:31,759
and implemented xvp so we we're staying

00:04:28,400 --> 00:04:33,440
many drivers are trying to uh implement

00:04:31,759 --> 00:04:35,840
xvp

00:04:33,440 --> 00:04:37,600
uh a lot lots of drivers also are

00:04:35,840 --> 00:04:38,880
closing the gaps implementing more and

00:04:37,600 --> 00:04:41,680
more future like

00:04:38,880 --> 00:04:42,880
features like security direct xsk

00:04:41,680 --> 00:04:46,160
support

00:04:42,880 --> 00:04:49,280
and trying to

00:04:46,160 --> 00:04:50,720
adopt the page pull mechanisms and make

00:04:49,280 --> 00:04:56,240
it more standardized

00:04:50,720 --> 00:04:58,080
and used quietly amongst drivers stuff

00:04:56,240 --> 00:05:00,800
that are in progress and they will

00:04:58,080 --> 00:05:01,840
discuss today xdp i'm sure there are

00:05:00,800 --> 00:05:04,160
dozen more

00:05:01,840 --> 00:05:04,880
uh please speak up today if you are

00:05:04,160 --> 00:05:08,800
working

00:05:04,880 --> 00:05:11,600
something interesting we will discuss it

00:05:08,800 --> 00:05:15,120
but mainly we are working in xvp

00:05:11,600 --> 00:05:18,560
hardware hands a multi-buffer

00:05:15,120 --> 00:05:21,120
access for fdb of db

00:05:18,560 --> 00:05:21,759
for bridges from clinic bridges and xvp

00:05:21,120 --> 00:05:26,560
program

00:05:21,759 --> 00:05:29,919
and zero copy and performance of the

00:05:26,560 --> 00:05:30,479
improvements for gdp to allow use cases

00:05:29,919 --> 00:05:35,520
like

00:05:30,479 --> 00:05:35,520
gpu zero copy and many other

00:05:35,840 --> 00:05:41,840
stuff uh so

00:05:38,880 --> 00:05:43,759
for our first topic today uh we're gonna

00:05:41,840 --> 00:05:46,560
discuss xdp hardware hands

00:05:43,759 --> 00:05:47,680
so motivation for xdp metadata is that

00:05:46,560 --> 00:05:50,160
today xvp

00:05:47,680 --> 00:05:52,400
programs do not see anything other than

00:05:50,160 --> 00:05:54,160
the packet data itself we're missing

00:05:52,400 --> 00:05:57,120
lots of information

00:05:54,160 --> 00:05:59,280
informations and offloads and hints can

00:05:57,120 --> 00:06:01,440
be provided by hardware

00:05:59,280 --> 00:06:03,120
and today they are not provided to xdp

00:06:01,440 --> 00:06:07,039
programs and which makes

00:06:03,120 --> 00:06:08,800
xdp very hard to work with when

00:06:07,039 --> 00:06:10,240
some offloads are enabled like vlan

00:06:08,800 --> 00:06:13,120
stripping and imagine

00:06:10,240 --> 00:06:14,880
that you're writing http program where

00:06:13,120 --> 00:06:16,560
you don't know the checksum status and

00:06:14,880 --> 00:06:20,000
you need to calculate it

00:06:16,560 --> 00:06:22,800
by hand in the xdp program

00:06:20,000 --> 00:06:23,919
so uh offloads hardware floors are great

00:06:22,800 --> 00:06:26,160
for acceleration and

00:06:23,919 --> 00:06:28,160
uh it's really it is really a low

00:06:26,160 --> 00:06:31,360
hanging fruit that we can

00:06:28,160 --> 00:06:34,479
just enable easily today uh we just need

00:06:31,360 --> 00:06:38,160
to discuss some design details

00:06:34,479 --> 00:06:41,199
um also i think for a full xdp

00:06:38,160 --> 00:06:44,240
uh offload support

00:06:41,199 --> 00:06:44,880
for and every vendor will take years and

00:06:44,240 --> 00:06:47,600
i think

00:06:44,880 --> 00:06:49,759
uh defenders are trying to move away

00:06:47,600 --> 00:06:51,919
from that

00:06:49,759 --> 00:06:53,280
so agenda for today we'll discuss

00:06:51,919 --> 00:06:55,520
current api

00:06:53,280 --> 00:06:56,720
requirements and the tools we have today

00:06:55,520 --> 00:07:00,400
to achieve this

00:06:56,720 --> 00:07:02,720
btf uh type format for example

00:07:00,400 --> 00:07:04,479
uh then we'll discuss high level design

00:07:02,720 --> 00:07:08,000
and how drivers are going to

00:07:04,479 --> 00:07:11,039
uh register their btf

00:07:08,000 --> 00:07:12,479
format for their metadata and how the

00:07:11,039 --> 00:07:14,880
user gonna read it

00:07:12,479 --> 00:07:16,479
interpret it and try to write programs

00:07:14,880 --> 00:07:18,479
according to that

00:07:16,479 --> 00:07:20,560
we will try to see some examples if we

00:07:18,479 --> 00:07:24,560
have the time let me just make a time

00:07:20,560 --> 00:07:28,880
check so we have around 10 minutes

00:07:24,560 --> 00:07:32,479
to finish this um

00:07:28,880 --> 00:07:35,520
so easy metadata we uh

00:07:32,479 --> 00:07:37,680
we already have the buffer uh really for

00:07:35,520 --> 00:07:38,880
us many many hardware vendors and

00:07:37,680 --> 00:07:40,720
drivers they

00:07:38,880 --> 00:07:42,400
already have headroom before each

00:07:40,720 --> 00:07:45,599
backend date and preserved for

00:07:42,400 --> 00:07:49,599
this use case specifically and

00:07:45,599 --> 00:07:52,080
we just have we have the tools to

00:07:49,599 --> 00:07:52,879
populate it and as you see in this

00:07:52,080 --> 00:07:56,400
diagram

00:07:52,879 --> 00:07:58,800
how the what's the format of the

00:07:56,400 --> 00:08:00,319
xdp buffer today and how it looks like

00:07:58,800 --> 00:08:02,479
in most drivers

00:08:00,319 --> 00:08:04,000
so we're just going to utilize that to

00:08:02,479 --> 00:08:06,639
make it happen

00:08:04,000 --> 00:08:08,560
and write a the metadata from the

00:08:06,639 --> 00:08:12,479
hardware hands into this

00:08:08,560 --> 00:08:15,599
buffer area before the packing data

00:08:12,479 --> 00:08:16,240
um so the tools we have is xbp i just

00:08:15,599 --> 00:08:19,919
made a

00:08:16,240 --> 00:08:22,319
it's it's a helper function

00:08:19,919 --> 00:08:23,440
exit helper function that is available

00:08:22,319 --> 00:08:27,280
for drivers

00:08:23,440 --> 00:08:30,960
and for uh

00:08:27,280 --> 00:08:31,599
for xdp program to adjust metadata and

00:08:30,960 --> 00:08:35,039
uh

00:08:31,599 --> 00:08:38,240
write to it populate it with

00:08:35,039 --> 00:08:40,399
whatever information we want

00:08:38,240 --> 00:08:41,599
just a note here we have also adjust

00:08:40,399 --> 00:08:45,360
head which

00:08:41,599 --> 00:08:48,160
might conflict with the metadata buffer

00:08:45,360 --> 00:08:49,680
uh but we're gonna talk i will uh talk

00:08:48,160 --> 00:08:53,200
about the use case that

00:08:49,680 --> 00:08:55,839
this could be hurtful

00:08:53,200 --> 00:08:57,680
uh again the easy part we we have the

00:08:55,839 --> 00:08:58,720
tools we just need to define the

00:08:57,680 --> 00:09:03,120
standard

00:08:58,720 --> 00:09:04,000
and um basically the requirement here is

00:09:03,120 --> 00:09:07,200
that

00:09:04,000 --> 00:09:10,080
we need the standard that we can

00:09:07,200 --> 00:09:11,040
advertise the format of the buffer that

00:09:10,080 --> 00:09:14,240
the hardware rule

00:09:11,040 --> 00:09:17,680
to the metadata

00:09:14,240 --> 00:09:20,959
section and using the standard

00:09:17,680 --> 00:09:24,480
xdp programs can read it interpret it

00:09:20,959 --> 00:09:26,720
and work according to that

00:09:24,480 --> 00:09:28,000
we need also dynamic metadata settings

00:09:26,720 --> 00:09:31,120
we don't want

00:09:28,000 --> 00:09:33,360
to re-implement the skb uh we're

00:09:31,120 --> 00:09:35,120
actually trying to improve that so

00:09:33,360 --> 00:09:37,600
the way to improve it is to allow a

00:09:35,120 --> 00:09:41,040
dynamic metadata population if you

00:09:37,600 --> 00:09:41,760
need only vlan you get only vnn other

00:09:41,040 --> 00:09:44,240
than that

00:09:41,760 --> 00:09:44,240
we don't

00:09:46,480 --> 00:09:51,040
put into waste buffer space so you get

00:09:49,360 --> 00:09:53,279
the vlan in the metadata

00:09:51,040 --> 00:09:54,399
if you need vlan and checksum you can

00:09:53,279 --> 00:09:57,920
check some and

00:09:54,399 --> 00:10:01,600
uh it's a traffic uh we also want

00:09:57,920 --> 00:10:01,600
execute metadata direct access

00:10:02,240 --> 00:10:06,000
runtime or compile time you don't want

00:10:04,399 --> 00:10:09,040
lookups or

00:10:06,000 --> 00:10:09,839
hashes to look for the needed metadata

00:10:09,040 --> 00:10:13,360
so we want

00:10:09,839 --> 00:10:16,000
uh full wire speed without any delays or

00:10:13,360 --> 00:10:16,000
overhead

00:10:16,240 --> 00:10:21,200
uh so we are going to use bpf uh

00:10:18,770 --> 00:10:24,000
[Music]

00:10:21,200 --> 00:10:24,640
type format which is a mechanism that

00:10:24,000 --> 00:10:27,680
was

00:10:24,640 --> 00:10:30,240
pushed in the past year to the kernel

00:10:27,680 --> 00:10:32,320
uh not gonna talk about this uh

00:10:30,240 --> 00:10:33,120
basically this is our standard way to

00:10:32,320 --> 00:10:36,880
describe

00:10:33,120 --> 00:10:39,200
uh types c types or in binary and

00:10:36,880 --> 00:10:40,399
they can be shared between kernel space

00:10:39,200 --> 00:10:42,800
and user space

00:10:40,399 --> 00:10:45,120
to interpret the format of a specific

00:10:42,800 --> 00:10:48,240
buffer

00:10:45,120 --> 00:10:49,839
high level design um basically we want

00:10:48,240 --> 00:10:52,320
the drive and firmware to keep

00:10:49,839 --> 00:10:53,040
a specific layout of metadata and gta

00:10:52,320 --> 00:10:56,320
format

00:10:53,040 --> 00:11:01,360
uh any driver can register

00:10:56,320 --> 00:11:04,399
and advertise its own bta format

00:11:01,360 --> 00:11:07,519
user driver will query the

00:11:04,399 --> 00:11:08,560
uh gener this generated btf format the

00:11:07,519 --> 00:11:10,480
specific window

00:11:08,560 --> 00:11:12,560
and will generate the c header file and

00:11:10,480 --> 00:11:13,600
will just compile their own program with

00:11:12,560 --> 00:11:17,120
this

00:11:13,600 --> 00:11:22,000
and they will get access to the

00:11:17,120 --> 00:11:22,000
metadata buffer um

00:11:22,079 --> 00:11:25,279
we will also discuss them so this allows

00:11:24,800 --> 00:11:28,399
also

00:11:25,279 --> 00:11:32,079
proprietary and standard

00:11:28,399 --> 00:11:35,279
offloads like maybe checksum

00:11:32,079 --> 00:11:37,600
vlans and

00:11:35,279 --> 00:11:40,000
whatever you can think if you're using

00:11:37,600 --> 00:11:43,600
special vendor with special

00:11:40,000 --> 00:11:47,120
hints you can get that with the

00:11:43,600 --> 00:11:48,800
proprietary btf format but we at least

00:11:47,120 --> 00:11:52,000
need to keep a subset of

00:11:48,800 --> 00:11:52,959
standardized fields that should at least

00:11:52,000 --> 00:11:56,079
share the same

00:11:52,959 --> 00:11:59,279
attributes like name and length and type

00:11:56,079 --> 00:12:03,360
for each and every window

00:11:59,279 --> 00:12:06,480
uh data flow how how this will work so

00:12:03,360 --> 00:12:08,800
on driver load driver or register btf

00:12:06,480 --> 00:12:11,839
type for networks dpi metadata

00:12:08,800 --> 00:12:13,760
and uh basically this is the

00:12:11,839 --> 00:12:16,320
option where you when you enable

00:12:13,760 --> 00:12:16,959
metadata this is the format you get this

00:12:16,320 --> 00:12:19,279
is the

00:12:16,959 --> 00:12:21,279
fields you will get in the metadata

00:12:19,279 --> 00:12:24,560
buffer

00:12:21,279 --> 00:12:26,160
uh the kernel will verify validity of

00:12:24,560 --> 00:12:29,360
this btf and will store

00:12:26,160 --> 00:12:31,920
this pkf for each driver um

00:12:29,360 --> 00:12:33,600
then you can come from user space and

00:12:31,920 --> 00:12:37,120
using bpf tool

00:12:33,600 --> 00:12:40,079
you can query xdp this btf

00:12:37,120 --> 00:12:41,839
format from the driver parse it and dump

00:12:40,079 --> 00:12:43,680
it into c header file and then you can

00:12:41,839 --> 00:12:46,639
write your own program which is

00:12:43,680 --> 00:12:49,200
gonna enjoy the uploads of this uh

00:12:46,639 --> 00:12:49,200
driver

00:12:49,440 --> 00:12:55,839
um after that you can request to turn on

00:12:53,120 --> 00:12:56,639
xdp metadata and request the btf that

00:12:55,839 --> 00:13:00,000
you used

00:12:56,639 --> 00:13:02,800
for that muted metadata and um

00:13:00,000 --> 00:13:05,200
load your program and then the driver

00:13:02,800 --> 00:13:08,240
knows which ptf you are expecting

00:13:05,200 --> 00:13:10,639
and your program already knows what ptf

00:13:08,240 --> 00:13:12,480
to use and how to interpret the metadata

00:13:10,639 --> 00:13:15,360
buffer

00:13:12,480 --> 00:13:16,480
and uh the camera will do the cross

00:13:15,360 --> 00:13:19,040
checks needed

00:13:16,480 --> 00:13:20,320
and after that the driver is enabled

00:13:19,040 --> 00:13:23,279
everything's enabled

00:13:20,320 --> 00:13:27,360
that is flowing and uh pdf and the

00:13:23,279 --> 00:13:29,600
metadata is populated correctly

00:13:27,360 --> 00:13:31,120
this is an example code of how driver

00:13:29,600 --> 00:13:34,240
register a

00:13:31,120 --> 00:13:36,959
btf format it basically

00:13:34,240 --> 00:13:38,399
from the design you can register as many

00:13:36,959 --> 00:13:41,440
formats as you want

00:13:38,399 --> 00:13:43,040
uh so this is to allow the generic and

00:13:41,440 --> 00:13:46,320
dynamic uh

00:13:43,040 --> 00:13:47,360
enablement of various fields in this

00:13:46,320 --> 00:13:51,040
example we see

00:13:47,360 --> 00:13:55,199
that we're providing hash and flow mark

00:13:51,040 --> 00:13:58,399
uh in mx5 driver uh

00:13:55,199 --> 00:14:02,240
let's discuss the net link user api so

00:13:58,399 --> 00:14:05,360
uh we're gonna add some sub command uh

00:14:02,240 --> 00:14:08,160
for xvp and it will show that on a

00:14:05,360 --> 00:14:11,680
specific net device metadata btf is in

00:14:08,160 --> 00:14:15,120
is available we have id1

00:14:11,680 --> 00:14:15,519
and btf is not enabled yet we can dump

00:14:15,120 --> 00:14:18,639
it

00:14:15,519 --> 00:14:22,079
into c header theater file so

00:14:18,639 --> 00:14:25,760
this is what can be enabled this uh

00:14:22,079 --> 00:14:28,079
struct which can be used to populate

00:14:25,760 --> 00:14:29,199
the metadata from on this specific

00:14:28,079 --> 00:14:32,720
interface

00:14:29,199 --> 00:14:35,680
so it has checksum uh hash mark

00:14:32,720 --> 00:14:38,000
low mark and vlan and some specific

00:14:35,680 --> 00:14:39,279
offloads that are only specific to this

00:14:38,000 --> 00:14:44,160
driver where you can

00:14:39,279 --> 00:14:46,959
just use an access from your hdp program

00:14:44,160 --> 00:14:47,600
um but then you can just turn it on and

00:14:46,959 --> 00:14:49,600
uh

00:14:47,600 --> 00:14:50,800
compile your program and start working

00:14:49,600 --> 00:14:54,399
with it

00:14:50,800 --> 00:14:57,680
um this is an example uh user program

00:14:54,399 --> 00:14:59,680
so uh if you see at

00:14:57,680 --> 00:15:02,079
the include file here is gene auto

00:14:59,680 --> 00:15:06,160
generated and we just included it

00:15:02,079 --> 00:15:09,040
and you just point to metadata

00:15:06,160 --> 00:15:10,079
uh section from the context which is the

00:15:09,040 --> 00:15:13,600
xvp

00:15:10,079 --> 00:15:13,920
buffer and uh just have you need to have

00:15:13,600 --> 00:15:17,199
a

00:15:13,920 --> 00:15:19,199
um an if statement which validates the

00:15:17,199 --> 00:15:21,120
valid which validates the

00:15:19,199 --> 00:15:23,360
existence of the metadata if the

00:15:21,120 --> 00:15:25,120
metadata really exists

00:15:23,360 --> 00:15:26,800
then from your program you should know

00:15:25,120 --> 00:15:29,440
that it has this

00:15:26,800 --> 00:15:30,240
exact destruct type and you just access

00:15:29,440 --> 00:15:33,920
it using

00:15:30,240 --> 00:15:36,720
c style syntax

00:15:33,920 --> 00:15:40,079
md and the hash and you get the hash

00:15:36,720 --> 00:15:43,600
directly without any hassle

00:15:40,079 --> 00:15:46,639
um in my uh code

00:15:43,600 --> 00:15:47,920
uh which is not upstream but it's ready

00:15:46,639 --> 00:15:50,800
to be upstream and just

00:15:47,920 --> 00:15:51,920
i just got busy lately and couldn't

00:15:50,800 --> 00:15:54,720
upstream it but

00:15:51,920 --> 00:15:56,399
i'm hoping to to make it upstream very

00:15:54,720 --> 00:16:00,000
soon

00:15:56,399 --> 00:16:02,000
uh some examples that i also changed

00:16:00,000 --> 00:16:03,040
uh for example to just to see that this

00:16:02,000 --> 00:16:05,600
is working

00:16:03,040 --> 00:16:08,480
uh hdp sample packets will now show the

00:16:05,600 --> 00:16:09,440
metadata if it was enabled uh redirect

00:16:08,480 --> 00:16:12,880
cpu can

00:16:09,440 --> 00:16:15,600
enjoy a lot from this today redirect cpu

00:16:12,880 --> 00:16:17,279
is calculating the hash by itself uh now

00:16:15,600 --> 00:16:20,560
it can use the hash from the

00:16:17,279 --> 00:16:21,360
metadata because the ptx ip tunnel today

00:16:20,560 --> 00:16:24,560
it has

00:16:21,360 --> 00:16:25,680
some lookup table tables and it's doing

00:16:24,560 --> 00:16:27,519
a lot of

00:16:25,680 --> 00:16:28,800
processing processing in the package to

00:16:27,519 --> 00:16:31,279
find out

00:16:28,800 --> 00:16:31,839
uh like the five tuple to which tunnel

00:16:31,279 --> 00:16:34,880
they

00:16:31,839 --> 00:16:37,120
belong this can be accelerated using tc

00:16:34,880 --> 00:16:40,079
flow mark and the xdp meter

00:16:37,120 --> 00:16:40,399
data which the driver can provide them

00:16:40,079 --> 00:16:42,240
with

00:16:40,399 --> 00:16:44,480
the hardware can provide the flow mark

00:16:42,240 --> 00:16:46,240
saying okay this this five tuple and

00:16:44,480 --> 00:16:50,000
this should go to this

00:16:46,240 --> 00:16:53,279
tunnel this got accelerated i believe

00:16:50,000 --> 00:16:54,480
by 60 or 70 percent packet rate i don't

00:16:53,279 --> 00:16:56,880
have the new numbers

00:16:54,480 --> 00:16:58,880
uh this was a year ago but that's the

00:16:56,880 --> 00:17:02,399
main idea

00:16:58,880 --> 00:17:05,439
and um just a note

00:17:02,399 --> 00:17:08,640
here uh that xzp

00:17:05,439 --> 00:17:09,600
programs are going to do to read the

00:17:08,640 --> 00:17:12,559
metadata

00:17:09,600 --> 00:17:14,000
and consume it but if they leave it

00:17:12,559 --> 00:17:16,400
there and the next program

00:17:14,000 --> 00:17:17,120
the same program they are going to do

00:17:16,400 --> 00:17:20,240
just ahead

00:17:17,120 --> 00:17:22,480
just to expand the header then we get a

00:17:20,240 --> 00:17:25,199
performance hit

00:17:22,480 --> 00:17:27,520
which is a memo because we're trying to

00:17:25,199 --> 00:17:28,480
expand your header into the metadata

00:17:27,520 --> 00:17:31,440
buffer

00:17:28,480 --> 00:17:33,600
and we don't want to override it so

00:17:31,440 --> 00:17:33,919
possible solution here is once you get

00:17:33,600 --> 00:17:36,080
the

00:17:33,919 --> 00:17:37,360
metadata consumed you can adjust method

00:17:36,080 --> 00:17:41,280
and uh just

00:17:37,360 --> 00:17:44,840
invalidate the metadata and

00:17:41,280 --> 00:17:48,480
and the memo will be avoided

00:17:44,840 --> 00:17:52,000
um this is a guide on how we

00:17:48,480 --> 00:17:55,760
this works and

00:17:52,000 --> 00:17:58,080
uh this is basically it so

00:17:55,760 --> 00:17:59,520
uh next we need to define the tx hint

00:17:58,080 --> 00:18:02,000
usage how we're gonna

00:17:59,520 --> 00:18:02,880
do it for x-men it's basically should be

00:18:02,000 --> 00:18:06,160
the same but

00:18:02,880 --> 00:18:08,160
it's not defined in this design

00:18:06,160 --> 00:18:10,480
we need to talk about what the standard

00:18:08,160 --> 00:18:13,280
offloads need to be aligned in

00:18:10,480 --> 00:18:14,720
each and every vendor because checksums

00:18:13,280 --> 00:18:17,600
hash

00:18:14,720 --> 00:18:19,679
flow mark there are all standard fields

00:18:17,600 --> 00:18:21,600
which could be standardized and they

00:18:19,679 --> 00:18:23,840
should look the same so you could use

00:18:21,600 --> 00:18:26,480
the same metadata bta format for

00:18:23,840 --> 00:18:26,480
multiple

00:18:26,840 --> 00:18:30,799
windows

00:18:28,480 --> 00:18:30,799
um

00:18:31,600 --> 00:18:36,160
one downside of this is that today you

00:18:33,919 --> 00:18:38,799
need to recompile your xtp program

00:18:36,160 --> 00:18:42,160
make windows uh there are some solution

00:18:38,799 --> 00:18:44,720
for this uh to make the verifier

00:18:42,160 --> 00:18:45,520
adapt or instrument the code in a way

00:18:44,720 --> 00:18:48,799
that

00:18:45,520 --> 00:18:52,080
it adapts your program to the

00:18:48,799 --> 00:18:53,440
window you're running on so just binary

00:18:52,080 --> 00:18:56,880
code restructuring

00:18:53,440 --> 00:19:00,000
and the verifier can do this

00:18:56,880 --> 00:19:02,160
given that we resolve all the standard

00:19:00,000 --> 00:19:04,160
fields and standard offloads and

00:19:02,160 --> 00:19:07,120
we can reorder them in the program so

00:19:04,160 --> 00:19:10,799
they can work on the

00:19:07,120 --> 00:19:11,760
specific window also need to figure out

00:19:10,799 --> 00:19:14,000
a fxdp

00:19:11,760 --> 00:19:14,880
how this works with afx zp basically it

00:19:14,000 --> 00:19:18,799
should be the same

00:19:14,880 --> 00:19:22,960
but i did not think about it so

00:19:18,799 --> 00:19:22,960
um that's basically it and

00:19:23,280 --> 00:19:30,480
any questions uh comments

00:19:27,120 --> 00:19:33,440
i i do have a question so right now if

00:19:30,480 --> 00:19:35,280
xdp changes the packet and it gets

00:19:33,440 --> 00:19:37,039
delivered to the core stack does that

00:19:35,280 --> 00:19:38,559
mean that all the checksum stuff is

00:19:37,039 --> 00:19:42,000
broken

00:19:38,559 --> 00:19:44,559
exactly so this is why uh we disable

00:19:42,000 --> 00:19:46,559
checksum so in mx5 we have checksum

00:19:44,559 --> 00:19:49,440
complete

00:19:46,559 --> 00:19:50,880
and uh imagine check some other way to

00:19:49,440 --> 00:19:52,960
check some complete is that

00:19:50,880 --> 00:19:55,200
checks that the stack believes that the

00:19:52,960 --> 00:19:56,720
checksum that provided in the skb is the

00:19:55,200 --> 00:19:59,039
correct checksum for the

00:19:56,720 --> 00:20:01,440
all of the bytes in the skb right so if

00:19:59,039 --> 00:20:03,600
you touch it in the xvp program

00:20:01,440 --> 00:20:06,240
that's there that mean this this is

00:20:03,600 --> 00:20:08,159
immediately this immediately uh

00:20:06,240 --> 00:20:10,000
invalidates the checksum and make it

00:20:08,159 --> 00:20:12,320
strong

00:20:10,000 --> 00:20:13,280
so phenomenal x5 for example when you

00:20:12,320 --> 00:20:16,000
load an xcp

00:20:13,280 --> 00:20:17,840
program we immediately go and disable

00:20:16,000 --> 00:20:21,120
checksum complete and go back to the

00:20:17,840 --> 00:20:23,039
basics of checksum valid bits

00:20:21,120 --> 00:20:25,440
and also for that i don't think it's

00:20:23,039 --> 00:20:27,679
correct because

00:20:25,440 --> 00:20:29,600
so the original packet had a valid

00:20:27,679 --> 00:20:31,840
checksum but now once after you modified

00:20:29,600 --> 00:20:34,880
it in the xdp program

00:20:31,840 --> 00:20:35,919
these checks and validation bits do not

00:20:34,880 --> 00:20:38,640
really

00:20:35,919 --> 00:20:39,039
correspond to the new packet so yeah we

00:20:38,640 --> 00:20:42,159
have an

00:20:39,039 --> 00:20:44,880
issue today and uh edp hints should

00:20:42,159 --> 00:20:48,080
resolve that

00:20:44,880 --> 00:20:52,320
thank you and uh

00:20:48,080 --> 00:20:54,640
yeah so if you want for example

00:20:52,320 --> 00:20:57,280
so today are the hands are used in the

00:20:54,640 --> 00:21:00,159
other way right so

00:20:57,280 --> 00:21:02,880
we know how to pass uh hands from the

00:21:00,159 --> 00:21:06,480
xcp program to the stack to the skb

00:21:02,880 --> 00:21:08,080
metadata but we don't know

00:21:06,480 --> 00:21:11,200
how to do the other way around from

00:21:08,080 --> 00:21:12,960
driver to the xdp program

00:21:11,200 --> 00:21:14,799
and we need to do a two-way

00:21:12,960 --> 00:21:17,039
communication between xdp

00:21:14,799 --> 00:21:17,919
between driver xp drone program and back

00:21:17,039 --> 00:21:19,760
to the driver

00:21:17,919 --> 00:21:20,960
for example if you change the packet and

00:21:19,760 --> 00:21:24,159
you would like to keep

00:21:20,960 --> 00:21:24,799
some complete feature on we need a way

00:21:24,159 --> 00:21:26,960
to

00:21:24,799 --> 00:21:28,799
from the xdp program to tell the driver

00:21:26,960 --> 00:21:30,320
hey this is the new checksum complete so

00:21:28,799 --> 00:21:31,760
we give it the checksum complete the

00:21:30,320 --> 00:21:34,799
original one

00:21:31,760 --> 00:21:37,520
xdp program changes the packet

00:21:34,799 --> 00:21:38,799
then modifies the new checksum complete

00:21:37,520 --> 00:21:40,400
accordingly and

00:21:38,799 --> 00:21:42,480
should provide the checksum complete

00:21:40,400 --> 00:21:46,400
back to the driver so it's gonna

00:21:42,480 --> 00:21:50,720
populate the correct field in the skb

00:21:46,400 --> 00:21:53,679
um so it's a two-way proto

00:21:50,720 --> 00:21:55,200
this uh this metadata should be a

00:21:53,679 --> 00:21:58,159
two-way protocol between

00:21:55,200 --> 00:22:00,320
driver and then program them back to the

00:21:58,159 --> 00:22:02,640
driver

00:22:00,320 --> 00:22:04,880
are are you envisioning that you simply

00:22:02,640 --> 00:22:08,559
copy the transmit descriptor

00:22:04,880 --> 00:22:10,799
into the beginning of the packet or

00:22:08,559 --> 00:22:12,159
yeah that could be a possible solution

00:22:10,799 --> 00:22:13,919
because btf can be

00:22:12,159 --> 00:22:16,159
can describe anything if you want to put

00:22:13,919 --> 00:22:19,760
the whole descriptor then fine

00:22:16,159 --> 00:22:22,000
just describe it in btf and

00:22:19,760 --> 00:22:24,159
user program will read it and understand

00:22:22,000 --> 00:22:26,720
it

00:22:24,159 --> 00:22:27,760
that's a vision and therefore future

00:22:26,720 --> 00:22:29,120
hardware you just

00:22:27,760 --> 00:22:30,960
maybe you don't need to copy the

00:22:29,120 --> 00:22:32,240
descriptor you can configure the

00:22:30,960 --> 00:22:35,039
hardware in a way that

00:22:32,240 --> 00:22:37,039
it writes the exact bta format that you

00:22:35,039 --> 00:22:39,760
requested

00:22:37,039 --> 00:22:42,240
and you would save a copy on driver

00:22:39,760 --> 00:22:45,360
saeed i have one question

00:22:42,240 --> 00:22:46,080
so does this framework allow an xdp

00:22:45,360 --> 00:22:49,120
program

00:22:46,080 --> 00:22:52,840
to specify metadata

00:22:49,120 --> 00:22:55,840
per queue different

00:22:52,840 --> 00:22:55,840
metadata

00:22:55,919 --> 00:23:02,720
no actually uh yeah this is

00:22:59,440 --> 00:23:06,960
so today you basically don't have a prq

00:23:02,720 --> 00:23:10,159
control when it's dpqs it

00:23:06,960 --> 00:23:13,520
doesn't work that way it can happen once

00:23:10,159 --> 00:23:15,520
we have a new device queue api

00:23:13,520 --> 00:23:18,000
uh which we were talking about for the

00:23:15,520 --> 00:23:20,799
past year i think magnus and jasper

00:23:18,000 --> 00:23:22,640
tried to figure out a solution for this

00:23:20,799 --> 00:23:25,520
but we don't have the

00:23:22,640 --> 00:23:26,400
means to uh then the knobs to control it

00:23:25,520 --> 00:23:28,960
but it can

00:23:26,400 --> 00:23:30,880
can be done if your hardware supports it

00:23:28,960 --> 00:23:32,480
and if your drivers support it

00:23:30,880 --> 00:23:34,159
and i believe today you can run

00:23:32,480 --> 00:23:36,400
different programs

00:23:34,159 --> 00:23:39,609
using cpu maps

00:23:36,400 --> 00:23:39,609
[Music]

00:23:39,760 --> 00:23:45,600
but yeah so

00:23:42,960 --> 00:23:48,880
theoretically you can have different

00:23:45,600 --> 00:23:52,240
programs running on different

00:23:48,880 --> 00:23:55,440
cpus uh but

00:23:52,240 --> 00:23:56,720
but uh the the connection between cpus

00:23:55,440 --> 00:23:59,279
and trains it's uh

00:23:56,720 --> 00:23:59,919
really not hard it's it's hardcore today

00:23:59,279 --> 00:24:02,240
and

00:23:59,919 --> 00:24:02,960
just uh you don't have the control which

00:24:02,240 --> 00:24:05,360
queue

00:24:02,960 --> 00:24:06,400
belongs to which cpu and what what this

00:24:05,360 --> 00:24:08,400
skill should

00:24:06,400 --> 00:24:09,440
how's the security should behave so we

00:24:08,400 --> 00:24:13,200
need to figure out the

00:24:09,440 --> 00:24:14,880
apis first say it yeah

00:24:13,200 --> 00:24:16,640
there's a there's a question in the chat

00:24:14,880 --> 00:24:19,440
which is interesting

00:24:16,640 --> 00:24:20,240
which is which is uh if you use ejs tool

00:24:19,440 --> 00:24:24,400
to configure

00:24:20,240 --> 00:24:24,400
you can disable enable different uh

00:24:24,880 --> 00:24:29,039
the the different uh off-road features

00:24:28,400 --> 00:24:34,080
right

00:24:29,039 --> 00:24:34,080
and how how do we how do we avoid that

00:24:35,760 --> 00:24:38,880
uh let me just try to figure out what

00:24:37,760 --> 00:24:42,480
this means

00:24:38,880 --> 00:24:44,720
uh yeah so we have it each tool today to

00:24:42,480 --> 00:24:46,960
disable enable

00:24:44,720 --> 00:24:49,360
features but it's a it's a global

00:24:46,960 --> 00:24:50,720
features per hardware you don't have it

00:24:49,360 --> 00:24:54,320
per queue

00:24:50,720 --> 00:24:54,320
right so

00:24:54,720 --> 00:24:59,600
yeah so you could so one thing is that

00:24:58,159 --> 00:25:02,720
we could just lock it so say

00:24:59,600 --> 00:25:04,559
you once you enable it with btf tool

00:25:02,720 --> 00:25:06,000
then then you're not allowed to to

00:25:04,559 --> 00:25:08,320
change the the

00:25:06,000 --> 00:25:10,080
same settings that affects the hints you

00:25:08,320 --> 00:25:15,840
wanted

00:25:10,080 --> 00:25:15,840
but i'm not sure how good that is

00:25:16,159 --> 00:25:19,679
i don't believe there is a conflict yeah

00:25:18,400 --> 00:25:21,840
uh but why would

00:25:19,679 --> 00:25:23,679
you think so if i disable checks um and

00:25:21,840 --> 00:25:26,880
you start getting checks i'm zero this

00:25:23,679 --> 00:25:26,880
means it's disabled so

00:25:27,200 --> 00:25:31,600
exactly like how it works with skb right

00:25:29,679 --> 00:25:32,159
if you disable check some offload you

00:25:31,600 --> 00:25:34,240
will say

00:25:32,159 --> 00:25:35,360
okay this checks up this packet has no

00:25:34,240 --> 00:25:38,720
checksum offload

00:25:35,360 --> 00:25:43,840
we need to also have the bit in the

00:25:38,720 --> 00:25:45,760
descriptor explaining that

00:25:43,840 --> 00:25:46,880
i think we should keep them mutually

00:25:45,760 --> 00:25:48,960
exclusive

00:25:46,880 --> 00:25:51,440
otherwise it's a lot to otherwise it's a

00:25:48,960 --> 00:25:54,640
lot of overhead for the drivers

00:25:51,440 --> 00:25:58,080
try to cross check btf format with the

00:25:54,640 --> 00:25:58,080
current enabled features

00:25:58,960 --> 00:26:03,600
yeah i agree yeah

00:26:05,440 --> 00:26:11,760
okay uh after the next topic

00:26:08,840 --> 00:26:14,640
um

00:26:11,760 --> 00:26:15,200
so david wanted to talk about the hdp 10

00:26:14,640 --> 00:26:18,320
points and

00:26:15,200 --> 00:26:20,000
xvp egress um so

00:26:18,320 --> 00:26:21,440
i know the itinerary listed this as five

00:26:20,000 --> 00:26:24,880
minutes on pain points

00:26:21,440 --> 00:26:26,320
and mostly talking about uh

00:26:24,880 --> 00:26:27,760
egress but i'm actually going to flip

00:26:26,320 --> 00:26:29,360
that i'm going to have just a simple

00:26:27,760 --> 00:26:31,120
summary about the egr status and focus

00:26:29,360 --> 00:26:34,000
mainly on the pain points

00:26:31,120 --> 00:26:34,400
so to set the context this is an example

00:26:34,000 --> 00:26:36,159
of a

00:26:34,400 --> 00:26:37,679
typical networking setup for a

00:26:36,159 --> 00:26:41,600
hypervisor essentially

00:26:37,679 --> 00:26:43,520
multiple ingress necks into an lacp bond

00:26:41,600 --> 00:26:45,840
uh the tab devices for the virtual

00:26:43,520 --> 00:26:47,520
machines are connected to a bridge and

00:26:45,840 --> 00:26:49,840
that's its way of getting access to the

00:26:47,520 --> 00:26:51,440
bond

00:26:49,840 --> 00:26:54,000
so the goal of what i was going after

00:26:51,440 --> 00:26:57,360
with xdp is to have no skbs

00:26:54,000 --> 00:26:58,480
in the host for 90 plus percent of vm

00:26:57,360 --> 00:27:01,600
traffic

00:26:58,480 --> 00:27:03,760
so basically xdp provides this fast path

00:27:01,600 --> 00:27:05,440
and the bridge is the slow path for any

00:27:03,760 --> 00:27:08,159
kind of unknown traffic

00:27:05,440 --> 00:27:09,440
however you want to define unknown and

00:27:08,159 --> 00:27:11,520
while xdp

00:27:09,440 --> 00:27:12,720
has a lot of really awesome

00:27:11,520 --> 00:27:14,240
possibilities

00:27:12,720 --> 00:27:16,480
there's a lot of pain points which are

00:27:14,240 --> 00:27:19,520
kind of uh i would say

00:27:16,480 --> 00:27:20,640
restricting the argument for making xdp

00:27:19,520 --> 00:27:23,200
they're not working today

00:27:20,640 --> 00:27:23,840
we need some some changes done and we'll

00:27:23,200 --> 00:27:26,159
start with

00:27:23,840 --> 00:27:27,760
the hardware acceleration piece of that

00:27:26,159 --> 00:27:29,840
um

00:27:27,760 --> 00:27:31,520
right now if you want to make if you

00:27:29,840 --> 00:27:34,000
want to write an xtp program

00:27:31,520 --> 00:27:35,919
that makes a vlan based decision you

00:27:34,000 --> 00:27:37,440
have to disable vlan offload

00:27:35,919 --> 00:27:39,360
right and so that gets to be an

00:27:37,440 --> 00:27:41,360
extremely hard sell to say

00:27:39,360 --> 00:27:43,440
hey i've got this really cool software

00:27:41,360 --> 00:27:45,279
networking feature i want to use

00:27:43,440 --> 00:27:46,399
software acceleration feature but you

00:27:45,279 --> 00:27:48,159
got to disable your hardware

00:27:46,399 --> 00:27:50,559
acceleration features first

00:27:48,159 --> 00:27:52,080
right that's that's gonna that's a

00:27:50,559 --> 00:27:54,000
non-winning proposal

00:27:52,080 --> 00:27:55,360
to people who don't understand some some

00:27:54,000 --> 00:27:58,080
of the details

00:27:55,360 --> 00:27:59,120
and hardware hints is one way to address

00:27:58,080 --> 00:28:01,039
this problem

00:27:59,120 --> 00:28:03,200
and i guess i'm kind of making an appeal

00:28:01,039 --> 00:28:05,760
that we need to get this committed

00:28:03,200 --> 00:28:07,360
as opposed to you know dragging out

00:28:05,760 --> 00:28:08,640
discussions and thinking about what's

00:28:07,360 --> 00:28:10,880
the right way to do it

00:28:08,640 --> 00:28:12,720
we need something working today because

00:28:10,880 --> 00:28:13,840
you know timely solutions these problems

00:28:12,720 --> 00:28:16,640
are essential

00:28:13,840 --> 00:28:17,120
and you know shared his his patches with

00:28:16,640 --> 00:28:19,440
me

00:28:17,120 --> 00:28:21,600
and even gave me the the idea of marking

00:28:19,440 --> 00:28:22,880
the x-line packets for example using tc

00:28:21,600 --> 00:28:25,840
offload

00:28:22,880 --> 00:28:27,840
and the performance gain is just crazy

00:28:25,840 --> 00:28:30,159
to be able to have the hardware

00:28:27,840 --> 00:28:31,360
tag the excellent packets so the xtv

00:28:30,159 --> 00:28:32,880
program can say

00:28:31,360 --> 00:28:34,640
i don't need to parse it or i do need to

00:28:32,880 --> 00:28:36,159
parse it and then pop that header

00:28:34,640 --> 00:28:38,159
and do the forwarding based on the inner

00:28:36,159 --> 00:28:39,200
packet so it makes a huge performance

00:28:38,159 --> 00:28:40,880
difference

00:28:39,200 --> 00:28:42,840
and really this is just kind of appeal

00:28:40,880 --> 00:28:46,240
like we got to get moving

00:28:42,840 --> 00:28:48,960
on on the solutions

00:28:46,240 --> 00:28:49,600
but that said i also want to make the

00:28:48,960 --> 00:28:52,640
argument

00:28:49,600 --> 00:28:55,919
that some of these some of this data

00:28:52,640 --> 00:28:57,679
in 2020 is really kind of fundamental

00:28:55,919 --> 00:28:59,120
properties of networking

00:28:57,679 --> 00:29:01,039
you know high speed networking in a data

00:28:59,120 --> 00:29:02,000
center at this point right so for

00:29:01,039 --> 00:29:05,200
example

00:29:02,000 --> 00:29:08,480
hash and vlan are two that come to mind

00:29:05,200 --> 00:29:09,600
um vlan is actually a property of the

00:29:08,480 --> 00:29:12,960
packet itself

00:29:09,600 --> 00:29:14,240
and as opposed to some kind of metadata

00:29:12,960 --> 00:29:17,360
about the packet

00:29:14,240 --> 00:29:19,360
and even the hash you know um

00:29:17,360 --> 00:29:21,039
you can argue the hash is more metadata

00:29:19,360 --> 00:29:23,039
but in a sense

00:29:21,039 --> 00:29:24,559
the kernel stack makes so many decisions

00:29:23,039 --> 00:29:27,279
based on it that

00:29:24,559 --> 00:29:27,840
if it exists from the hardware we really

00:29:27,279 --> 00:29:30,159
need to take

00:29:27,840 --> 00:29:30,880
take advantage of that and have that

00:29:30,159 --> 00:29:33,120
that data

00:29:30,880 --> 00:29:35,120
follow the xtp frame for example and

00:29:33,120 --> 00:29:36,720
then make it make it available

00:29:35,120 --> 00:29:38,320
to actually be programmed through the

00:29:36,720 --> 00:29:41,120
xtp context

00:29:38,320 --> 00:29:42,399
an example of that is you get a packet

00:29:41,120 --> 00:29:44,720
coming in to the host

00:29:42,399 --> 00:29:47,520
you redirect it to a tap device and if

00:29:44,720 --> 00:29:49,600
it's a multi-queue tap device

00:29:47,520 --> 00:29:51,919
the tap device want the ton driver wants

00:29:49,600 --> 00:29:54,480
to compute the hash on the packet

00:29:51,919 --> 00:29:56,080
to figure out which one to put it in or

00:29:54,480 --> 00:29:58,799
for xdp it's using

00:29:56,080 --> 00:30:01,279
a cpu based algorithm i would rather

00:29:58,799 --> 00:30:03,520
have a hash based algorithm which means

00:30:01,279 --> 00:30:05,120
if the hash can follow the frame then

00:30:03,520 --> 00:30:06,559
you can use that queue selection

00:30:05,120 --> 00:30:09,840
using the data that you would just like

00:30:06,559 --> 00:30:12,399
you would for nxt for an xkb

00:30:09,840 --> 00:30:13,919
so anyway we can talk about that at some

00:30:12,399 --> 00:30:14,880
point but yeah i would like to see some

00:30:13,919 --> 00:30:18,240
of this data

00:30:14,880 --> 00:30:21,279
that is more standard become um

00:30:18,240 --> 00:30:22,720
kind of built in aspects of the context

00:30:21,279 --> 00:30:26,000
and frame as opposed to

00:30:22,720 --> 00:30:29,039
considering it as hardware or hints

00:30:26,000 --> 00:30:32,080
uh let's see switching gears from

00:30:29,039 --> 00:30:34,320
host ingress to vm egress so

00:30:32,080 --> 00:30:36,240
in this case we're talking about packets

00:30:34,320 --> 00:30:39,679
coming out of a virtual machine

00:30:36,240 --> 00:30:42,159
and going out of the the host itself

00:30:39,679 --> 00:30:43,120
so the focus for xp xdp development to

00:30:42,159 --> 00:30:46,159
date has been

00:30:43,120 --> 00:30:48,640
rx into a server and

00:30:46,159 --> 00:30:49,440
the tx path has its own set of

00:30:48,640 --> 00:30:52,000
challenges

00:30:49,440 --> 00:30:53,600
right and so some of those again you

00:30:52,000 --> 00:30:56,399
have to disable

00:30:53,600 --> 00:30:57,360
hardware acceleration features to take

00:30:56,399 --> 00:31:01,760
advantage of

00:30:57,360 --> 00:31:05,120
xdp which gets to be a borderline

00:31:01,760 --> 00:31:08,080
performance um

00:31:05,120 --> 00:31:09,760
so for example with checksum and tso

00:31:08,080 --> 00:31:11,440
it's not a matter of disabling it

00:31:09,760 --> 00:31:13,200
in the kernel it's a matter of disabling

00:31:11,440 --> 00:31:14,799
it in the guest right and so now you

00:31:13,200 --> 00:31:16,720
have to modify the vm config

00:31:14,799 --> 00:31:19,039
for example with with the burt to say

00:31:16,720 --> 00:31:21,760
disable tso disable csum

00:31:19,039 --> 00:31:22,799
and again you start getting into this

00:31:21,760 --> 00:31:24,320
battle of

00:31:22,799 --> 00:31:26,080
i have a cool software acceleration

00:31:24,320 --> 00:31:27,600
feature but you gotta disable hardware

00:31:26,080 --> 00:31:30,159
acceleration which is a

00:31:27,600 --> 00:31:32,080
non-winning argument another part of

00:31:30,159 --> 00:31:35,279
this is the vlan acceleration

00:31:32,080 --> 00:31:36,960
right so with cloud hosts vlans are used

00:31:35,279 --> 00:31:39,440
to separate traffic

00:31:36,960 --> 00:31:40,720
and the vlans are transparent to the

00:31:39,440 --> 00:31:44,000
virtual machines

00:31:40,720 --> 00:31:47,440
and so you right now to use xdp

00:31:44,000 --> 00:31:48,720
on uh vmware dress packets you have to

00:31:47,440 --> 00:31:50,559
insert the

00:31:48,720 --> 00:31:52,000
the vlan header yourself inside the

00:31:50,559 --> 00:31:53,600
program right

00:31:52,000 --> 00:31:56,159
now if the vlan tag was a part of the

00:31:53,600 --> 00:31:57,919
xtp frame and the driver knew to look

00:31:56,159 --> 00:31:58,960
and the ftp frame to say oh here's the

00:31:57,919 --> 00:32:00,720
vlan tag

00:31:58,960 --> 00:32:02,399
does it better integrate with the

00:32:00,720 --> 00:32:03,200
hardware acceleration aspects of it

00:32:02,399 --> 00:32:04,799
right

00:32:03,200 --> 00:32:07,120
and you know i've gone back and forth

00:32:04,799 --> 00:32:09,120
with some emails with saeed about this

00:32:07,120 --> 00:32:10,320
when i look at the hardware hints it's

00:32:09,120 --> 00:32:13,200
not obvious to me

00:32:10,320 --> 00:32:15,840
how to extend that proposal to the

00:32:13,200 --> 00:32:15,840
transmission path

00:32:16,720 --> 00:32:23,840
another major problem with

00:32:19,760 --> 00:32:27,840
using xtp on vm egress

00:32:23,840 --> 00:32:28,720
v hosts are threads and those threads

00:32:27,840 --> 00:32:30,799
are just any

00:32:28,720 --> 00:32:31,840
just any other scheduled task and that

00:32:30,799 --> 00:32:35,840
it can migrate

00:32:31,840 --> 00:32:37,679
from cpus across cpus in the host

00:32:35,840 --> 00:32:40,320
and if that cpu that the v-host thread

00:32:37,679 --> 00:32:42,640
is running on does not map to a net cue

00:32:40,320 --> 00:32:44,240
and you try to do an xcp redirect

00:32:42,640 --> 00:32:46,320
package is dropped

00:32:44,240 --> 00:32:48,000
because driver can't do anything with it

00:32:46,320 --> 00:32:49,200
and it is extremely puzzling to go

00:32:48,000 --> 00:32:51,120
figure this out

00:32:49,200 --> 00:32:52,320
because in some cases packets are moving

00:32:51,120 --> 00:32:55,519
just fine

00:32:52,320 --> 00:32:56,640
in other cases intermittent drops or

00:32:55,519 --> 00:32:58,159
just if the

00:32:56,640 --> 00:32:59,840
thread gets stuck on that cpu for a

00:32:58,159 --> 00:33:00,399
while you just get the loss of

00:32:59,840 --> 00:33:01,840
networking

00:33:00,399 --> 00:33:04,080
completely and you're like wait a minute

00:33:01,840 --> 00:33:05,519
it just worked what's going on

00:33:04,080 --> 00:33:07,200
and this is really becoming a problem

00:33:05,519 --> 00:33:09,039
with these larger systems right so for

00:33:07,200 --> 00:33:12,799
example we have a lot of servers

00:33:09,039 --> 00:33:14,960
with greater than 64 logical cpus

00:33:12,799 --> 00:33:16,640
and you're running out of per cpu nit

00:33:14,960 --> 00:33:19,519
cues at that point

00:33:16,640 --> 00:33:21,440
so we need to be thinking about how to

00:33:19,519 --> 00:33:23,200
address this problem and i don't

00:33:21,440 --> 00:33:24,720
maybe it's just a matter of always

00:33:23,200 --> 00:33:27,679
setting cpu affinity

00:33:24,720 --> 00:33:28,080
on the v host threads but even that is

00:33:27,679 --> 00:33:31,760
not

00:33:28,080 --> 00:33:35,039
a simple solution because the mapping of

00:33:31,760 --> 00:33:38,080
nit cues to cpus is a bit of a nightmare

00:33:35,039 --> 00:33:40,159
and this is a couple of examples of one

00:33:38,080 --> 00:33:42,240
server with 96 cpus

00:33:40,159 --> 00:33:43,360
and that's the mappings you know it

00:33:42,240 --> 00:33:46,000
starts off kind of

00:33:43,360 --> 00:33:46,960
0-19 easy to figure out and then it's

00:33:46,000 --> 00:33:50,000
all the odd

00:33:46,960 --> 00:33:52,159
logical cpus which is ignoring a numa

00:33:50,000 --> 00:33:53,519
domain but that's a different problem

00:33:52,159 --> 00:33:55,600
and then you go to a different server

00:33:53,519 --> 00:33:56,640
which has 112 cpus

00:33:55,600 --> 00:33:58,640
and you need a completely different

00:33:56,640 --> 00:34:00,880
mapping and again

00:33:58,640 --> 00:34:02,159
you have to have the software that has

00:34:00,880 --> 00:34:03,840
to go figure out

00:34:02,159 --> 00:34:05,200
what are the cpus that i could run the

00:34:03,840 --> 00:34:07,279
v-host thread on

00:34:05,200 --> 00:34:10,000
which also gets into other management

00:34:07,279 --> 00:34:13,919
aspects when you start talking about

00:34:10,000 --> 00:34:13,919
how you're setting up your cloud host

00:34:15,200 --> 00:34:19,520
the last set of problems gets into xdp

00:34:18,159 --> 00:34:21,520
with bonds

00:34:19,520 --> 00:34:22,639
so when you're redirecting a packet you

00:34:21,520 --> 00:34:25,359
have to tell it

00:34:22,639 --> 00:34:26,480
which nick port not the bond because the

00:34:25,359 --> 00:34:28,960
bond doesn't support

00:34:26,480 --> 00:34:31,599
xtp and so you're having to redirect

00:34:28,960 --> 00:34:34,000
this packet to either e0 or e1

00:34:31,599 --> 00:34:35,119
how do you pick that leg you know i've

00:34:34,000 --> 00:34:38,240
looked at

00:34:35,119 --> 00:34:41,359
trying to expose a helper to

00:34:38,240 --> 00:34:41,760
tap into the bond driver to say hey can

00:34:41,359 --> 00:34:44,159
you

00:34:41,760 --> 00:34:45,839
figure out based on your current logic

00:34:44,159 --> 00:34:48,879
and when it comes to

00:34:45,839 --> 00:34:51,760
l3 l4 on the packet to figure

00:34:48,879 --> 00:34:53,919
out which leg to take that's a

00:34:51,760 --> 00:34:55,760
non-winning direction because

00:34:53,919 --> 00:34:57,680
you have to have an skb and replicating

00:34:55,760 --> 00:35:00,079
that code for xdp

00:34:57,680 --> 00:35:02,560
is just it's way too much overhead for

00:35:00,079 --> 00:35:05,359
the return on the investment and

00:35:02,560 --> 00:35:07,040
what i've gone to for the past seven

00:35:05,359 --> 00:35:07,760
eight months now is i've just replicated

00:35:07,040 --> 00:35:10,880
the bond

00:35:07,760 --> 00:35:12,640
algorithm in evpf and again

00:35:10,880 --> 00:35:14,000
kind of like the vo thread maybe this is

00:35:12,640 --> 00:35:17,200
one we just have to eat love

00:35:14,000 --> 00:35:19,520
with that difference of xdp versus full

00:35:17,200 --> 00:35:19,520
stack

00:35:21,040 --> 00:35:24,240
the last topic is

00:35:25,200 --> 00:35:31,200
what it means to run xtv programs inside

00:35:28,320 --> 00:35:33,440
virtual machines so in this case

00:35:31,200 --> 00:35:35,280
you want to allow the guest os to attach

00:35:33,440 --> 00:35:36,320
an xtp program to one of the guestnet

00:35:35,280 --> 00:35:38,640
devices

00:35:36,320 --> 00:35:40,079
for example one use case is like the vm

00:35:38,640 --> 00:35:41,359
is running kubernetes

00:35:40,079 --> 00:35:43,599
and it wants to leverage the new

00:35:41,359 --> 00:35:44,800
psyllium options with xdp

00:35:43,599 --> 00:35:47,200
what are some things that have to be

00:35:44,800 --> 00:35:48,880
done for the guest

00:35:47,200 --> 00:35:50,880
well the first requirement you know v

00:35:48,880 --> 00:35:51,920
host and tap is the best networking

00:35:50,880 --> 00:35:54,480
performance

00:35:51,920 --> 00:35:56,839
outside of going with srov but let's

00:35:54,480 --> 00:35:59,839
stick with the v host tap device

00:35:56,839 --> 00:35:59,839
and

00:36:08,880 --> 00:36:12,000
and that's a pernic requirement and

00:36:11,040 --> 00:36:14,320
every one of those

00:36:12,000 --> 00:36:15,440
queues represents a v host thread so as

00:36:14,320 --> 00:36:18,320
an example

00:36:15,440 --> 00:36:20,560
4b cpu vm with two necks one on a public

00:36:18,320 --> 00:36:22,320
interface and one on a private network

00:36:20,560 --> 00:36:25,119
you have to have eight queues per nic

00:36:22,320 --> 00:36:28,560
and so that now that 4b cpu vm

00:36:25,119 --> 00:36:31,040
turns into a 20 some odd thread

00:36:28,560 --> 00:36:32,880
scheduling problem inside the host right

00:36:31,040 --> 00:36:35,520
and that's really a non-starter

00:36:32,880 --> 00:36:36,320
to have that many schedulable entities

00:36:35,520 --> 00:36:38,800
for a vm

00:36:36,320 --> 00:36:40,000
and the impact it has on the host side

00:36:38,800 --> 00:36:42,800
they really need a way to

00:36:40,000 --> 00:36:44,560
drop this requirement and one proposal i

00:36:42,800 --> 00:36:46,720
made was

00:36:44,560 --> 00:36:48,800
since this since the extra cues are only

00:36:46,720 --> 00:36:50,880
needed for the transmit path

00:36:48,800 --> 00:36:52,480
if the cues don't exist maybe just don't

00:36:50,880 --> 00:36:55,599
allow x to be transmitted

00:36:52,480 --> 00:36:57,040
right so which case vms can use xtp

00:36:55,599 --> 00:37:00,240
programs on the neck

00:36:57,040 --> 00:37:02,079
to do redirects inside but

00:37:00,240 --> 00:37:04,000
can't do redirect from one interface to

00:37:02,079 --> 00:37:07,520
another or from a container

00:37:04,000 --> 00:37:10,000
to a egress doing egress

00:37:07,520 --> 00:37:11,680
out of the vm the other option that was

00:37:10,000 --> 00:37:12,800
thrown out was to use locking for the

00:37:11,680 --> 00:37:15,760
queues

00:37:12,800 --> 00:37:16,560
um personally i don't have a preference

00:37:15,760 --> 00:37:19,359
it's more of

00:37:16,560 --> 00:37:21,359
finding a solution to the fact that you

00:37:19,359 --> 00:37:22,560
can't have this this hard requirement of

00:37:21,359 --> 00:37:26,240
so many queues

00:37:22,560 --> 00:37:26,240
and what that means from a cloud host

00:37:27,200 --> 00:37:33,359
let's see uh another program

00:37:30,400 --> 00:37:35,200
another problem with with running xtv

00:37:33,359 --> 00:37:37,680
programs and virtual machines

00:37:35,200 --> 00:37:39,359
is again hardware aspects of the

00:37:37,680 --> 00:37:42,480
hardware offload aspects of it

00:37:39,359 --> 00:37:44,079
so if you load an xtp program inside of

00:37:42,480 --> 00:37:46,320
a virtual machine

00:37:44,079 --> 00:37:48,720
the verdio netcode is calling down into

00:37:46,320 --> 00:37:50,400
the host in the qmu to say hey you need

00:37:48,720 --> 00:37:53,440
to disable tx checksum

00:37:50,400 --> 00:37:55,839
and tso on the tap device in the host

00:37:53,440 --> 00:37:57,520
and that gets really confusing you're

00:37:55,839 --> 00:37:59,440
running on running that perk stuff

00:37:57,520 --> 00:38:00,800
and all of a sudden you load a program

00:37:59,440 --> 00:38:03,200
because you want to drop packets you

00:38:00,800 --> 00:38:04,480
don't want the gas to spend any cycles

00:38:03,200 --> 00:38:06,640
processing stuff you just want to see

00:38:04,480 --> 00:38:08,000
how many you can push but as soon as you

00:38:06,640 --> 00:38:09,520
load an xtp program

00:38:08,000 --> 00:38:12,160
you just change the entire performance

00:38:09,520 --> 00:38:14,160
characteristic because of what the guest

00:38:12,160 --> 00:38:16,640
is doing inside the host or pushing down

00:38:14,160 --> 00:38:16,640
to the host

00:38:17,040 --> 00:38:20,800
and then it also has a negative impact

00:38:18,960 --> 00:38:22,880
as well in terms of

00:38:20,800 --> 00:38:24,560
the host having to whether it's segment

00:38:22,880 --> 00:38:28,240
packets or

00:38:24,560 --> 00:38:30,839
um you see a higher software cube

00:38:28,240 --> 00:38:32,720
load on the host when you load that

00:38:30,839 --> 00:38:35,920
program

00:38:32,720 --> 00:38:36,720
all right so that's all of the points i

00:38:35,920 --> 00:38:40,320
wanted to bring up

00:38:36,720 --> 00:38:42,960
about what it means to run xdp

00:38:40,320 --> 00:38:44,720
in a host or inside of a virtual machine

00:38:42,960 --> 00:38:47,280
and then shifting to the other topic

00:38:44,720 --> 00:38:50,000
which was you know i had spent a long

00:38:47,280 --> 00:38:53,440
time looking at ftp in the egress path

00:38:50,000 --> 00:38:56,800
and the short answer is it died

00:38:53,440 --> 00:38:57,839
i really wanted a solution that had both

00:38:56,800 --> 00:38:59,839
the skb path

00:38:57,839 --> 00:39:01,280
and the xtv path all hit the same

00:38:59,839 --> 00:39:04,720
program

00:39:01,280 --> 00:39:07,599
and one one location for

00:39:04,720 --> 00:39:09,520
the program and data that goes with it

00:39:07,599 --> 00:39:10,400
but the skb path was just too much to

00:39:09,520 --> 00:39:13,760
overcome

00:39:10,400 --> 00:39:16,800
and so what ended up getting put in is

00:39:13,760 --> 00:39:18,079
you can run programs on ftp redirect so

00:39:16,800 --> 00:39:21,599
essentially you

00:39:18,079 --> 00:39:24,240
have a dev map for your reader x

00:39:21,599 --> 00:39:25,839
and you use the new struct dev map valve

00:39:24,240 --> 00:39:28,960
as the value for those entries

00:39:25,839 --> 00:39:32,720
and one of the entries in the struct

00:39:28,960 --> 00:39:35,680
is a program fd that lets you attach the

00:39:32,720 --> 00:39:38,960
program to the map entry itself

00:39:35,680 --> 00:39:43,200
and that is all i had in terms of

00:39:38,960 --> 00:39:46,480
talking points

00:39:43,200 --> 00:39:49,760
uh thanks david that was uh very

00:39:46,480 --> 00:39:53,040
important uh details to share

00:39:49,760 --> 00:39:54,560
and uh so thanks for the motivation for

00:39:53,040 --> 00:39:56,880
the offloads i will

00:39:54,560 --> 00:39:58,240
uh the hover hands i'll make sure we

00:39:56,880 --> 00:40:00,960
will move forward

00:39:58,240 --> 00:40:02,320
before your use cases i really didn't

00:40:00,960 --> 00:40:03,920
have any user

00:40:02,320 --> 00:40:06,079
so i didn't have the motivation to push

00:40:03,920 --> 00:40:08,880
forward uh now we do

00:40:06,079 --> 00:40:11,040
and thanks for that um yeah it's just

00:40:08,880 --> 00:40:14,079
this generic thing of uh

00:40:11,040 --> 00:40:16,160
you know you xdp has mind share

00:40:14,079 --> 00:40:17,280
and i think dave miller a year ago made

00:40:16,160 --> 00:40:21,119
a comment about how

00:40:17,280 --> 00:40:23,680
it's xdp and ebpf are needed to keep

00:40:21,119 --> 00:40:24,880
linux networking stack relevant but also

00:40:23,680 --> 00:40:28,160
getting these features in

00:40:24,880 --> 00:40:28,960
timely because you know companies who

00:40:28,160 --> 00:40:32,160
don't work

00:40:28,960 --> 00:40:34,000
upstream are making changes

00:40:32,160 --> 00:40:34,800
to their architecture maybe they're

00:40:34,000 --> 00:40:36,000
making changes they're not working

00:40:34,800 --> 00:40:38,480
architecture

00:40:36,000 --> 00:40:39,440
it doesn't they don't do that very often

00:40:38,480 --> 00:40:41,440
and they're going to make these

00:40:39,440 --> 00:40:41,920
decisions based on what exists in a

00:40:41,440 --> 00:40:44,160
kernel

00:40:41,920 --> 00:40:45,040
at a particular time point in time and

00:40:44,160 --> 00:40:47,760
so to have these

00:40:45,040 --> 00:40:50,160
capabilities exist in a timely manner is

00:40:47,760 --> 00:40:52,720
important

00:40:50,160 --> 00:40:54,079
right well it's not about timing just

00:40:52,720 --> 00:40:57,920
about uh you know

00:40:54,079 --> 00:41:00,400
we want xdp to perform uh

00:40:57,920 --> 00:41:01,359
the best always and each and every

00:41:00,400 --> 00:41:03,680
feature we push

00:41:01,359 --> 00:41:05,040
in which is what jesper is trying to

00:41:03,680 --> 00:41:08,319
avoid that uh

00:41:05,040 --> 00:41:11,520
gonna break the performance

00:41:08,319 --> 00:41:14,560
bit by bit and uh as jasper

00:41:11,520 --> 00:41:16,319
puts it uh we're gonna have like a death

00:41:14,560 --> 00:41:20,240
situation by a thousand

00:41:16,319 --> 00:41:23,119
uh papercuts yeah yeah

00:41:20,240 --> 00:41:23,599
so we're trying to be careful here but

00:41:23,119 --> 00:41:26,640
if you're

00:41:23,599 --> 00:41:27,520
too careful yeah it's just hard to move

00:41:26,640 --> 00:41:29,920
i agree with you

00:41:27,520 --> 00:41:31,920
it is it is a it is a tough line right i

00:41:29,920 --> 00:41:33,040
mean xdp is about advanced networking

00:41:31,920 --> 00:41:33,680
it's for you're supposed to know what

00:41:33,040 --> 00:41:35,599
you're doing

00:41:33,680 --> 00:41:37,119
you're taking shortcuts around the full

00:41:35,599 --> 00:41:38,560
stack because you have a

00:41:37,119 --> 00:41:41,839
particular environment that you're

00:41:38,560 --> 00:41:44,079
deploying in and

00:41:41,839 --> 00:41:45,760
yes you you need the capabilities but

00:41:44,079 --> 00:41:47,760
then you also

00:41:45,760 --> 00:41:50,480
don't need the continual performance

00:41:47,760 --> 00:41:50,480
degradations

00:41:50,720 --> 00:41:55,599
right yeah i i also

00:41:53,760 --> 00:41:57,440
i also want to bring up the point that

00:41:55,599 --> 00:41:58,960
where you said you we should put some of

00:41:57,440 --> 00:42:02,079
the stuff in the

00:41:58,960 --> 00:42:03,920
xdp underscore frame and i

00:42:02,079 --> 00:42:05,680
actually agree that we actually need to

00:42:03,920 --> 00:42:07,680
figure out some of the stuff

00:42:05,680 --> 00:42:09,920
i think should blow me in the http frame

00:42:07,680 --> 00:42:12,079
and some of the stuff needs to belong in

00:42:09,920 --> 00:42:14,800
the the metadata area

00:42:12,079 --> 00:42:14,800
so yeah

00:42:15,200 --> 00:42:18,480
the hash interesting action vlan are

00:42:16,960 --> 00:42:21,839
like these are essential properties for

00:42:18,480 --> 00:42:23,760
data center not working these days

00:42:21,839 --> 00:42:25,839
and uh yeah i had checked them on there

00:42:23,760 --> 00:42:27,359
for a while and i'm like

00:42:25,839 --> 00:42:28,880
i could go either way on that one but

00:42:27,359 --> 00:42:30,960
yes having that there

00:42:28,880 --> 00:42:32,000
so that yeah programs can can do

00:42:30,960 --> 00:42:36,000
adjustments

00:42:32,000 --> 00:42:37,839
it's yeah it is this fine line between

00:42:36,000 --> 00:42:41,200
what is considered a standard

00:42:37,839 --> 00:42:43,280
attribute or feature and what is

00:42:41,200 --> 00:42:44,240
non-common and i don't want it to come

00:42:43,280 --> 00:42:46,000
down to a

00:42:44,240 --> 00:42:47,359
the common denominator of all hardware

00:42:46,000 --> 00:42:49,680
it needs to be

00:42:47,359 --> 00:42:51,200
what is a reasonable feature for common

00:42:49,680 --> 00:42:53,040
networking today

00:42:51,200 --> 00:42:54,240
yeah but i think it should sort of be

00:42:53,040 --> 00:42:56,160
optional so it's

00:42:54,240 --> 00:42:58,160
i was i would like to also define this

00:42:56,160 --> 00:43:00,880
as maybe with btf

00:42:58,160 --> 00:43:02,960
so you you enable you want you want the

00:43:00,880 --> 00:43:04,240
vlan to be put in there you want the

00:43:02,960 --> 00:43:05,839
hash to be put in there

00:43:04,240 --> 00:43:07,839
and then you are asking to get this

00:43:05,839 --> 00:43:09,119
extra overhead and then

00:43:07,839 --> 00:43:11,440
then people that don't want this

00:43:09,119 --> 00:43:14,079
overhead can say i didn't i don't enable

00:43:11,440 --> 00:43:15,680
this to be put in the http frame

00:43:14,079 --> 00:43:17,280
and i think we can do it exactly the

00:43:15,680 --> 00:43:20,480
same system as the

00:43:17,280 --> 00:43:23,920
as described by by i said

00:43:20,480 --> 00:43:27,839
it's a ptf format you enable in

00:43:23,920 --> 00:43:27,839
in the area in the hdp frame

00:43:28,480 --> 00:43:31,599
yeah so going back to that i i believe

00:43:31,040 --> 00:43:33,839
uh

00:43:31,599 --> 00:43:35,040
david you wanted to to make these

00:43:33,839 --> 00:43:39,119
standard

00:43:35,040 --> 00:43:43,520
offloads or fields to put them as

00:43:39,119 --> 00:43:46,560
built-in fields inside the xdp frame

00:43:43,520 --> 00:43:50,000
uh so yeah i like the idea

00:43:46,560 --> 00:43:50,880
just this makes the whole xdp metadata

00:43:50,000 --> 00:43:53,359
btf

00:43:50,880 --> 00:43:54,160
approach like a second class feature

00:43:53,359 --> 00:43:57,680
that

00:43:54,160 --> 00:43:59,520
gonna hardly use and well i guess i'm

00:43:57,680 --> 00:44:02,720
more separating it into

00:43:59,520 --> 00:44:05,119
custom and standard standards

00:44:02,720 --> 00:44:06,800
belongs in you know and built into the

00:44:05,119 --> 00:44:08,560
data structures where custom

00:44:06,800 --> 00:44:10,400
is more the you know like the

00:44:08,560 --> 00:44:11,280
flexibility of hardware hints is is

00:44:10,400 --> 00:44:13,119
awesome

00:44:11,280 --> 00:44:14,400
and it does allow the custom features

00:44:13,119 --> 00:44:18,079
for you know

00:44:14,400 --> 00:44:21,359
hardware nicks but yet

00:44:18,079 --> 00:44:24,000
yeah it's a fine line

00:44:21,359 --> 00:44:25,280
yeah it is but uh if we can join both in

00:44:24,000 --> 00:44:28,480
the same approach and

00:44:25,280 --> 00:44:31,599
uh just uh make the uh

00:44:28,480 --> 00:44:36,000
standard features in in bold font and

00:44:31,599 --> 00:44:37,520
make it uh like uh

00:44:36,000 --> 00:44:39,599
it obvious to everyone that these are

00:44:37,520 --> 00:44:42,480
standard field so

00:44:39,599 --> 00:44:44,800
it's better to have one approach but

00:44:42,480 --> 00:44:45,760
yeah let's discuss this in email and in

00:44:44,800 --> 00:44:48,880
my submission

00:44:45,760 --> 00:44:50,560
and uh we'll see

00:44:48,880 --> 00:44:52,160
another one that wasn't mentioned was

00:44:50,560 --> 00:44:57,760
the 4k barrier and

00:44:52,160 --> 00:45:01,680
gso which is also very very common

00:44:57,760 --> 00:45:03,520
right so i believe uh sami will address

00:45:01,680 --> 00:45:07,280
this in his multi-buffer

00:45:03,520 --> 00:45:08,560
session today so let's delegate this

00:45:07,280 --> 00:45:12,880
discussion to

00:45:08,560 --> 00:45:12,880
that session it's the next topic

00:45:15,040 --> 00:45:19,119
but yeah i agree this will solve a lot

00:45:16,800 --> 00:45:22,319
of the egress path issues that david

00:45:19,119 --> 00:45:23,040
was experiencing with tso and check some

00:45:22,319 --> 00:45:26,720
antiques

00:45:23,040 --> 00:45:30,960
uh i'm sorry uh t zone cheeks for vms

00:45:26,720 --> 00:45:33,920
and uh tap devices so i i i think with

00:45:30,960 --> 00:45:34,880
multi-buffer support we can enable back

00:45:33,920 --> 00:45:37,119
these so

00:45:34,880 --> 00:45:37,119
um

00:45:39,200 --> 00:45:45,119
what else so i have a lot of notes

00:45:42,240 --> 00:45:46,000
david and uh i think we're running out

00:45:45,119 --> 00:45:49,520
of time

00:45:46,000 --> 00:45:51,359
uh so let's check at the end of the

00:45:49,520 --> 00:45:52,880
session today if we have time to discuss

00:45:51,359 --> 00:45:55,680
this

00:45:52,880 --> 00:45:55,920
uh you raised a lot of valid points here

00:45:55,680 --> 00:45:58,880
and

00:45:55,920 --> 00:46:01,280
i would like to discuss them in more

00:45:58,880 --> 00:46:03,680
details

00:46:01,280 --> 00:46:04,319
uh but mainly most of your problems with

00:46:03,680 --> 00:46:08,720
the

00:46:04,319 --> 00:46:12,000
xdp uh with the xdptxq selection

00:46:08,720 --> 00:46:12,960
is should be solved by the there is a

00:46:12,000 --> 00:46:15,440
topic that we're

00:46:12,960 --> 00:46:16,880
in discussion like i think that the

00:46:15,440 --> 00:46:20,560
title was making

00:46:16,880 --> 00:46:24,000
a net device queues first uh citizen

00:46:20,560 --> 00:46:26,880
in in the kernel first class then

00:46:24,000 --> 00:46:28,079
and uh yeah we need to provide more

00:46:26,880 --> 00:46:30,480
control to the user

00:46:28,079 --> 00:46:32,079
in order to create xdpt executes

00:46:30,480 --> 00:46:33,920
redirect cues and

00:46:32,079 --> 00:46:36,240
to give the user more controls on the

00:46:33,920 --> 00:46:38,560
xdp cues today they are

00:46:36,240 --> 00:46:39,280
just magically created inside the kernel

00:46:38,560 --> 00:46:41,440
driver

00:46:39,280 --> 00:46:43,280
and they are getting assigned to cpus

00:46:41,440 --> 00:46:45,920
randomly

00:46:43,280 --> 00:46:47,760
and you get you get uh these weird

00:46:45,920 --> 00:46:49,440
performance problems or drops you're

00:46:47,760 --> 00:46:53,440
like what's going on

00:46:49,440 --> 00:46:56,720
well all known issues yeah but uh as

00:46:53,440 --> 00:46:57,920
as an advanced user as you are i believe

00:46:56,720 --> 00:47:02,240
you can run into these

00:46:57,920 --> 00:47:02,240
issues very easily i agree with you

00:47:03,839 --> 00:47:11,839
okay any other questions comments

00:47:08,079 --> 00:47:12,720
before we move on so one one one comment

00:47:11,839 --> 00:47:15,359
about the queues

00:47:12,720 --> 00:47:16,400
so what people people speculated that

00:47:15,359 --> 00:47:19,599
that we could use the

00:47:16,400 --> 00:47:21,520
device map extend the device map value

00:47:19,599 --> 00:47:23,040
with some information about what kind of

00:47:21,520 --> 00:47:24,480
queueing mechanisms you want if you

00:47:23,040 --> 00:47:27,680
wanted to have

00:47:24,480 --> 00:47:28,960
have a lock simple one want you one

00:47:27,680 --> 00:47:31,359
takes cue with a lock

00:47:28,960 --> 00:47:33,119
or you want something else but but that

00:47:31,359 --> 00:47:35,040
work has sort of stalled

00:47:33,119 --> 00:47:36,720
and i can see mamas is not on the

00:47:35,040 --> 00:47:39,839
participant list

00:47:36,720 --> 00:47:39,839
yeah he's on vacation

00:47:41,839 --> 00:47:45,119
yeah that's going to be a problem lots

00:47:44,000 --> 00:47:48,160
of great ideas

00:47:45,119 --> 00:47:51,359
and it's just taking a long time

00:47:48,160 --> 00:47:51,359
to to get things in

00:47:51,599 --> 00:47:56,319
and just remember device maps only works

00:47:54,079 --> 00:47:59,599
for xvp direct and we have the same

00:47:56,319 --> 00:48:02,880
problems with xdptx

00:47:59,599 --> 00:48:05,040
we need to generalize the solutions

00:48:02,880 --> 00:48:06,880
just remember we do have drivers on the

00:48:05,040 --> 00:48:09,760
cabinet right now that

00:48:06,880 --> 00:48:11,599
have the execute locking there's two of

00:48:09,760 --> 00:48:13,040
them one of them just has a single takes

00:48:11,599 --> 00:48:15,680
queue so there's nothing we can do and

00:48:13,040 --> 00:48:20,319
the second one was

00:48:15,680 --> 00:48:22,960
the mvneta driver from marvel which

00:48:20,319 --> 00:48:24,400
uh it was easier when we and faster when

00:48:22,960 --> 00:48:25,920
we upstream the whole thing to

00:48:24,400 --> 00:48:27,599
have locking instead of splitting the

00:48:25,920 --> 00:48:30,160
cues yeah

00:48:27,599 --> 00:48:32,079
i believe we should avoid locking as

00:48:30,160 --> 00:48:34,160
much as possible yeah yeah

00:48:32,079 --> 00:48:36,000
the the only the only case we should

00:48:34,160 --> 00:48:37,839
allow it is a single q device which

00:48:36,000 --> 00:48:39,760
xdp doesn't even make too much sense on

00:48:37,839 --> 00:48:42,160
it right

00:48:39,760 --> 00:48:43,040
okay uh let's move on uh we'll try to

00:48:42,160 --> 00:48:46,240
come back to

00:48:43,040 --> 00:48:47,760
uh some of these uh issues for

00:48:46,240 --> 00:48:49,970
more further discussion at the end of

00:48:47,760 --> 00:48:52,079
the meeting we get the time

00:48:49,970 --> 00:48:55,359
[Music]

00:48:52,079 --> 00:48:58,400
so next is the xdp multibuffer from

00:48:55,359 --> 00:49:00,720
sami so hi

00:48:58,400 --> 00:49:01,760
i'm samir from the ena linux team and

00:49:00,720 --> 00:49:05,440
i'm presenting to you

00:49:01,760 --> 00:49:05,440
xdp multibuffer support

00:49:05,920 --> 00:49:09,760
we'll be going over the motivation and

00:49:07,760 --> 00:49:10,240
the solution proposed then we'll walk

00:49:09,760 --> 00:49:12,480
through

00:49:10,240 --> 00:49:14,559
the code changes and upcoming milestones

00:49:12,480 --> 00:49:16,720
it's important to note that this work is

00:49:14,559 --> 00:49:20,559
presented here is based upon the

00:49:16,720 --> 00:49:20,559
original design draft by jasper

00:49:21,040 --> 00:49:25,920
so these are the typical use cases that

00:49:23,920 --> 00:49:26,800
the multibuffer support should enable us

00:49:25,920 --> 00:49:29,200
to do

00:49:26,800 --> 00:49:32,079
supporting such use cases will result in

00:49:29,200 --> 00:49:34,000
overall performance gain for the users

00:49:32,079 --> 00:49:37,040
first supporting jumbo frames will

00:49:34,000 --> 00:49:39,119
enable xdp to have a higher throughput

00:49:37,040 --> 00:49:40,559
this is the default mode for aws

00:49:39,119 --> 00:49:42,880
customers

00:49:40,559 --> 00:49:44,000
second supporting header data split

00:49:42,880 --> 00:49:46,319
should result in fewer

00:49:44,000 --> 00:49:48,000
accesses to the memory and in overall

00:49:46,319 --> 00:49:50,720
performance gain

00:49:48,000 --> 00:49:52,640
last xdp multi-buffer support will

00:49:50,720 --> 00:49:55,040
enable drivers to support tcp

00:49:52,640 --> 00:49:57,440
segmentation offload and large

00:49:55,040 --> 00:49:58,240
receive offload which should reflect an

00:49:57,440 --> 00:50:01,040
overall

00:49:58,240 --> 00:50:03,200
cpu over it so basically multiple

00:50:01,040 --> 00:50:03,839
packets can be segmented into one packet

00:50:03,200 --> 00:50:07,200
in both

00:50:03,839 --> 00:50:07,200
rx and tx path

00:50:08,960 --> 00:50:12,000
it's important to keep in mind the

00:50:10,800 --> 00:50:13,599
following points when

00:50:12,000 --> 00:50:16,079
trying to walk through the design and

00:50:13,599 --> 00:50:18,720
implementation of xtp multibuffer

00:50:16,079 --> 00:50:20,720
preserving the single buffer performance

00:50:18,720 --> 00:50:22,079
is important as we don't want to degrade

00:50:20,720 --> 00:50:25,520
it

00:50:22,079 --> 00:50:27,680
second the eba bf direct access feature

00:50:25,520 --> 00:50:29,520
is essential for xdp performance

00:50:27,680 --> 00:50:33,040
note that this feature requires a

00:50:29,520 --> 00:50:33,040
continuous memory area

00:50:34,079 --> 00:50:40,000
so let's walk over the solution first

00:50:37,920 --> 00:50:42,079
use the tail room of the first buffer to

00:50:40,000 --> 00:50:44,000
store the rest of the buffers in a given

00:50:42,079 --> 00:50:46,240
xdp multi buffer packet

00:50:44,000 --> 00:50:48,800
we use the fragments array in the skb

00:50:46,240 --> 00:50:51,119
shared infrastructure to store them

00:50:48,800 --> 00:50:53,119
all current drivers in the kernel that

00:50:51,119 --> 00:50:55,599
have xdp support have a tail room for

00:50:53,119 --> 00:50:57,440
the skb shared info construct

00:50:55,599 --> 00:50:58,720
this change was introduced by jasper a

00:50:57,440 --> 00:51:00,800
few months ago

00:50:58,720 --> 00:51:02,319
in the series that introduced frame size

00:51:00,800 --> 00:51:05,680
to the xdp buffer and

00:51:02,319 --> 00:51:07,440
xtp frame structs second

00:51:05,680 --> 00:51:09,280
avoid adding an instruction layer for

00:51:07,440 --> 00:51:11,760
supporting the diretaxis feature

00:51:09,280 --> 00:51:12,960
and limit xtp program to access the

00:51:11,760 --> 00:51:14,800
first buffer only

00:51:12,960 --> 00:51:16,000
this allows us to preserve the direct

00:51:14,800 --> 00:51:19,200
access feature

00:51:16,000 --> 00:51:21,520
for the first buffer third

00:51:19,200 --> 00:51:23,680
provide the xtp program with helpers

00:51:21,520 --> 00:51:25,920
that can access the page address

00:51:23,680 --> 00:51:28,160
size and offset of each fragment this

00:51:25,920 --> 00:51:29,040
provides the user with limited data on

00:51:28,160 --> 00:51:30,960
the fragments

00:51:29,040 --> 00:51:34,000
yet it should be more than enough for

00:51:30,960 --> 00:51:34,000
the typical use case

00:51:34,319 --> 00:51:37,760
so this is the graphical view of how the

00:51:36,960 --> 00:51:40,960
structure

00:51:37,760 --> 00:51:43,280
xdp buff will look like when the multi

00:51:40,960 --> 00:51:45,119
buffer support is introduced

00:51:43,280 --> 00:51:47,119
we can see the tail room contains the

00:51:45,119 --> 00:51:50,160
skb shared infrastructure

00:51:47,119 --> 00:51:51,839
where it uses the frax field only of

00:51:50,160 --> 00:51:55,119
this structure to store

00:51:51,839 --> 00:51:56,400
the information with the rest of the

00:51:55,119 --> 00:51:59,839
buffers

00:51:56,400 --> 00:52:00,400
so basically we end up having we end up

00:51:59,839 --> 00:52:04,160
using

00:52:00,400 --> 00:52:06,480
the frags field

00:52:04,160 --> 00:52:07,200
which is pointing out to the rest of the

00:52:06,480 --> 00:52:10,960
buffers

00:52:07,200 --> 00:52:14,400
in a multi-buffer packet let's walk over

00:52:10,960 --> 00:52:14,400
the necessary code changes

00:52:15,040 --> 00:52:18,880
a new bit was added to the xdp buff and

00:52:17,520 --> 00:52:21,680
xtp frame structs

00:52:18,880 --> 00:52:22,640
for indicating that a given structure is

00:52:21,680 --> 00:52:27,359
using

00:52:22,640 --> 00:52:27,359
xdp buffer a xdp multibuffer

00:52:28,160 --> 00:52:33,119
so for the first glance we can just

00:52:31,599 --> 00:52:36,400
check

00:52:33,119 --> 00:52:38,240
if nr frags doesn't equal zero

00:52:36,400 --> 00:52:40,640
this kind of approach assume that the

00:52:38,240 --> 00:52:42,400
buffer are zeroed out

00:52:40,640 --> 00:52:44,480
when they are sent to the device and in

00:52:42,400 --> 00:52:46,000
particular the tail room which holds the

00:52:44,480 --> 00:52:48,880
skb shared info

00:52:46,000 --> 00:52:50,240
and contains the nr frags field the main

00:52:48,880 --> 00:52:52,640
issue with this assumption

00:52:50,240 --> 00:52:55,119
is that the buffers that are sent to the

00:52:52,640 --> 00:52:57,599
device are not always zeroed out

00:52:55,119 --> 00:53:00,640
the page could be recycled for example

00:52:57,599 --> 00:53:03,920
using the page for recycle in cage

00:53:00,640 --> 00:53:07,119
function the page memory area is not

00:53:03,920 --> 00:53:10,960
re-initialized to zero this

00:53:07,119 --> 00:53:10,960
assumption then doesn't hold up

00:53:11,359 --> 00:53:15,839
so that we the mb bit is needed

00:53:16,000 --> 00:53:20,400
this additional bit requires us to

00:53:18,079 --> 00:53:22,720
update existing functions to use it

00:53:20,400 --> 00:53:25,040
we need to assign the multi buffer bit

00:53:22,720 --> 00:53:26,559
when converting from xdp buff to xtp

00:53:25,040 --> 00:53:28,640
frame and vice versa

00:53:26,559 --> 00:53:30,400
in addition we also need to handle

00:53:28,640 --> 00:53:34,800
fragments when returning

00:53:30,400 --> 00:53:34,800
the xdp buff or xdp frame structs

00:53:35,440 --> 00:53:40,160
in order to support xtp multibuffer some

00:53:38,160 --> 00:53:41,040
changes are required by the drivers as

00:53:40,160 --> 00:53:42,800
well

00:53:41,040 --> 00:53:45,280
we need to make sure that all the

00:53:42,800 --> 00:53:48,800
drivers that do not support multibuffer

00:53:45,280 --> 00:53:49,119
set the mb bit to zero moreover drivers

00:53:48,800 --> 00:53:52,319
that

00:53:49,119 --> 00:53:53,520
are not using build skb are required to

00:53:52,319 --> 00:53:56,800
start using it

00:53:53,520 --> 00:53:57,520
or at least when xdp is on using build

00:53:56,800 --> 00:53:59,920
this kb

00:53:57,520 --> 00:54:00,800
makes sense as it uses the pre-allocated

00:53:59,920 --> 00:54:02,960
buffer

00:54:00,800 --> 00:54:04,000
instead of allocating a new buffer as

00:54:02,960 --> 00:54:07,280
done in nappy

00:54:04,000 --> 00:54:10,800
get frags or netdev alloc skb align

00:54:07,280 --> 00:54:13,280
ipl line for example

00:54:10,800 --> 00:54:13,920
so when a multi-buffer packet is

00:54:13,280 --> 00:54:16,240
received

00:54:13,920 --> 00:54:17,599
we need to fill the xtp buff structure

00:54:16,240 --> 00:54:21,520
and pass it on

00:54:17,599 --> 00:54:24,800
to the xtp program you can see here the

00:54:21,520 --> 00:54:27,359
the flow that the driver will

00:54:24,800 --> 00:54:29,359
go into so we collect the buffers from

00:54:27,359 --> 00:54:31,040
the device we fill the xdp buff

00:54:29,359 --> 00:54:33,839
structure

00:54:31,040 --> 00:54:34,880
we fill the frags field only the flex

00:54:33,839 --> 00:54:36,839
field from the skb

00:54:34,880 --> 00:54:39,119
shared info which is stored in the tail

00:54:36,839 --> 00:54:42,319
room

00:54:39,119 --> 00:54:45,359
we set the multibuffer bit then we

00:54:42,319 --> 00:54:48,480
pass the xdp buffer xtp buff

00:54:45,359 --> 00:54:51,359
structure to the xtp program and

00:54:48,480 --> 00:54:53,520
when the verdict is returned we need to

00:54:51,359 --> 00:54:54,640
handle the cases accordingly so as i

00:54:53,520 --> 00:54:58,640
mentioned earlier

00:54:54,640 --> 00:54:59,280
earlier if the xtp we get an xtp pass we

00:54:58,640 --> 00:55:02,319
need use

00:54:59,280 --> 00:55:05,599
build skp to allocate the sk

00:55:02,319 --> 00:55:09,200
skb and pass it on to the kernel

00:55:05,599 --> 00:55:10,960
in cases of xtptx or xtp redirect we

00:55:09,200 --> 00:55:12,400
need to send the frags as well when

00:55:10,960 --> 00:55:15,359
transmitting

00:55:12,400 --> 00:55:15,839
and in case of xdp drop and xtp port we

00:55:15,359 --> 00:55:18,799
need to

00:55:15,839 --> 00:55:18,799
drop the fragments

00:55:19,520 --> 00:55:22,960
so in order to access the frags

00:55:22,079 --> 00:55:26,319
information

00:55:22,960 --> 00:55:27,440
size and offset new bpf xdp helpers have

00:55:26,319 --> 00:55:29,599
been introduced

00:55:27,440 --> 00:55:31,359
these helpers can help xdp users

00:55:29,599 --> 00:55:33,839
calculate the full packet size

00:55:31,359 --> 00:55:35,839
and more a sample program which uses

00:55:33,839 --> 00:55:37,680
these helpers is also available

00:55:35,839 --> 00:55:39,839
the program simply calculates the total

00:55:37,680 --> 00:55:41,440
number of bytes in the linear and page

00:55:39,839 --> 00:55:43,359
part of a given packet

00:55:41,440 --> 00:55:45,680
as well as displays the packets per

00:55:43,359 --> 00:55:45,680
second

00:55:46,400 --> 00:55:52,640
so i would like to thank you uh

00:55:50,240 --> 00:55:53,680
wait before i would like to thank you i

00:55:52,640 --> 00:55:55,440
would like to

00:55:53,680 --> 00:55:58,079
go over the current status and next

00:55:55,440 --> 00:56:00,720
steps uh me and lorenzo have

00:55:58,079 --> 00:56:03,040
sent multiple rfcs to the kernel that

00:56:00,720 --> 00:56:06,000
introduced the changes discussed here

00:56:03,040 --> 00:56:07,040
a nrc version should be submitted soon

00:56:06,000 --> 00:56:10,160
when net next

00:56:07,040 --> 00:56:10,880
opens in a few days the marvel driver

00:56:10,160 --> 00:56:13,280
has been

00:56:10,880 --> 00:56:14,160
prepared to support xtp multibuffer by

00:56:13,280 --> 00:56:16,640
lorenzo

00:56:14,160 --> 00:56:18,079
ena patches to support xtp multibuffer

00:56:16,640 --> 00:56:20,079
are still under work

00:56:18,079 --> 00:56:21,440
and are going to be submitted when they

00:56:20,079 --> 00:56:23,280
are ready

00:56:21,440 --> 00:56:25,359
we are also planning to do performance

00:56:23,280 --> 00:56:30,079
testing to compare

00:56:25,359 --> 00:56:33,839
multibuffer versus non-multibuffer

00:56:30,079 --> 00:56:37,599
so thank you um thanks samir

00:56:33,839 --> 00:56:39,280
i would like to uh raise some concerns

00:56:37,599 --> 00:56:41,839
here in the chat

00:56:39,280 --> 00:56:42,880
and i believe i also saw this myself

00:56:41,839 --> 00:56:46,160
when i saw the

00:56:42,880 --> 00:56:48,960
new struct uh of the xdp

00:56:46,160 --> 00:56:52,000
buffer and xzp frame i gathered the

00:56:48,960 --> 00:56:52,799
javon how the skb looked like i think 20

00:56:52,000 --> 00:56:56,000
years ago

00:56:52,799 --> 00:56:57,359
i'm exaggerating maybe but what are we

00:56:56,000 --> 00:57:00,319
doing to avoid

00:56:57,359 --> 00:57:01,599
going back to and trying to implement

00:57:00,319 --> 00:57:04,319
the skb and

00:57:01,599 --> 00:57:05,760
trying to resolve all the skb problems

00:57:04,319 --> 00:57:09,200
we had in the past

00:57:05,760 --> 00:57:10,079
all over all over again can you pinpoint

00:57:09,200 --> 00:57:13,280
an exact

00:57:10,079 --> 00:57:14,160
problem that you're referring to i i can

00:57:13,280 --> 00:57:16,880
see your point

00:57:14,160 --> 00:57:17,839
in general but we are actually working

00:57:16,880 --> 00:57:20,559
with the skb

00:57:17,839 --> 00:57:22,960
and not seeing that we're working in a

00:57:20,559 --> 00:57:25,520
similar way to the skb

00:57:22,960 --> 00:57:26,079
so so the whole idea of having xvp is

00:57:25,520 --> 00:57:28,319
the

00:57:26,079 --> 00:57:30,240
the whole thing is uh lightweight and

00:57:28,319 --> 00:57:32,400
bare bones like uh

00:57:30,240 --> 00:57:33,680
you get only the frame and some extra

00:57:32,400 --> 00:57:36,559
bits that you can

00:57:33,680 --> 00:57:38,400
interpret some way uh to extract some

00:57:36,559 --> 00:57:41,839
more information

00:57:38,400 --> 00:57:44,799
in an indirect we don't want the x

00:57:41,839 --> 00:57:45,839
the p frame to grow uh to be over cache

00:57:44,799 --> 00:57:47,920
line size or

00:57:45,839 --> 00:57:49,119
whatever that will hit performance or

00:57:47,920 --> 00:57:52,839
even

00:57:49,119 --> 00:57:55,599
uh be the uh like

00:57:52,839 --> 00:57:58,640
central

00:57:55,599 --> 00:58:01,119
yeah maybe i can answer that so

00:57:58,640 --> 00:58:04,160
like like jesse says on the chat which

00:58:01,119 --> 00:58:06,000
this is an hkb light

00:58:04,160 --> 00:58:07,760
and that was that was actually part of

00:58:06,000 --> 00:58:08,240
my original plan for like three years

00:58:07,760 --> 00:58:11,440
ago

00:58:08,240 --> 00:58:14,960
to have an skp light and and

00:58:11,440 --> 00:58:17,839
the xp frame is an skb light but

00:58:14,960 --> 00:58:19,599
but to keep performance people have to

00:58:17,839 --> 00:58:22,799
enable these different features

00:58:19,599 --> 00:58:24,160
and buy in into them and we have to be

00:58:22,799 --> 00:58:27,119
very careful we cannot

00:58:24,160 --> 00:58:29,040
have this xcp frame get get larger than

00:58:27,119 --> 00:58:30,640
one cast line because then performance

00:58:29,040 --> 00:58:32,559
is dead

00:58:30,640 --> 00:58:34,640
so that is that is why i also before i

00:58:32,559 --> 00:58:37,040
said i want actually this area we are

00:58:34,640 --> 00:58:41,040
extending the http frame to

00:58:37,040 --> 00:58:43,680
actually defined as as spgf so

00:58:41,040 --> 00:58:44,400
you we have this flexible area where we

00:58:43,680 --> 00:58:48,079
can define

00:58:44,400 --> 00:58:50,640
this the stuff that the drivers need

00:58:48,079 --> 00:58:51,440
and this can you have to fit in

00:58:50,640 --> 00:58:54,640
basically

00:58:51,440 --> 00:58:57,200
into the last 32 bytes and you cannot go

00:58:54,640 --> 00:58:59,839
beyond that

00:58:57,200 --> 00:59:00,960
but it isn't skb light because the next

00:58:59,839 --> 00:59:04,240
step after this

00:59:00,960 --> 00:59:06,480
is creating a full hkb so

00:59:04,240 --> 00:59:07,520
so that is why it's faster what happened

00:59:06,480 --> 00:59:10,559
before is that

00:59:07,520 --> 00:59:13,920
we kept extending this skb to handle

00:59:10,559 --> 00:59:16,079
handle uh higher layers why why i see

00:59:13,920 --> 00:59:18,000
xdp is layered layer 2 and layer 3 and

00:59:16,079 --> 00:59:20,640
that's the information we we

00:59:18,000 --> 00:59:20,640
have to provide

00:59:21,680 --> 00:59:25,920
so i hope like that that we can keep

00:59:25,200 --> 00:59:28,079
performance

00:59:25,920 --> 00:59:29,440
by having these you have to buy into

00:59:28,079 --> 00:59:31,280
enable these features and you can

00:59:29,440 --> 00:59:32,400
disable them again and not use use the

00:59:31,280 --> 00:59:36,000
fields

00:59:32,400 --> 00:59:36,000
okay so understand that

00:59:36,319 --> 00:59:39,359
currently in the design i don't see how

00:59:38,160 --> 00:59:42,720
we are making the

00:59:39,359 --> 00:59:46,559
xdp frame uh dynamic and

00:59:42,720 --> 00:59:49,920
extendable when you go to higher

00:59:46,559 --> 00:59:52,960
layer protocols and have layer uh levels

00:59:49,920 --> 00:59:56,000
but yeah this could be a valid approach

00:59:52,960 --> 00:59:57,359
and uh my question here is that i think

00:59:56,000 --> 01:00:01,040
we need the gatekeeper

00:59:57,359 --> 01:00:05,280
programming programmatic way to assure

01:00:01,040 --> 01:00:07,040
that uh we're not uh expanding the xdp

01:00:05,280 --> 01:00:10,240
buffer beyond it needs

01:00:07,040 --> 01:00:13,200
so i think i had a point in the

01:00:10,240 --> 01:00:16,480
uh pre-discussion at the end of this

01:00:13,200 --> 01:00:19,760
session so uh let's just

01:00:16,480 --> 01:00:23,280
put a pin on this one and we'll try to

01:00:19,760 --> 01:00:23,280
address it at the end of the meeting

01:00:24,079 --> 01:00:27,680
but basically jasper i think we need to

01:00:26,720 --> 01:00:30,960
have

01:00:27,680 --> 01:00:34,079
some way inside the kernel source tree

01:00:30,960 --> 01:00:36,000
that would have some sort of a benchmark

01:00:34,079 --> 01:00:36,400
or macro benchmark that would tell you

01:00:36,000 --> 01:00:38,319
hey

01:00:36,400 --> 01:00:41,200
you're you're doing something wrong

01:00:38,319 --> 01:00:43,520
you're breaking performance or

01:00:41,200 --> 01:00:44,960
uh increasing the size of the xvp buffer

01:00:43,520 --> 01:00:47,920
beyond the cash line

01:00:44,960 --> 01:00:47,920
whatever we define

01:00:48,480 --> 01:00:52,480
yeah i agree that's that's one of my

01:00:50,559 --> 01:00:55,280
topics that we are killing performance

01:00:52,480 --> 01:00:58,799
slowly by a thousand papercuts right

01:00:55,280 --> 01:01:00,880
right okay um

01:00:58,799 --> 01:01:03,520
any performance measurements made with

01:01:00,880 --> 01:01:03,520
this approach

01:01:03,920 --> 01:01:09,040
the multibuffer approach no i haven't

01:01:07,040 --> 01:01:10,240
performed performance testing yet but

01:01:09,040 --> 01:01:12,079
it's on

01:01:10,240 --> 01:01:15,280
the next milestones as i've already

01:01:12,079 --> 01:01:19,599
stated in my last slide

01:01:15,280 --> 01:01:22,160
so i i have nothing to share yet sorry

01:01:19,599 --> 01:01:23,119
and uh this mode is flexible right i can

01:01:22,160 --> 01:01:25,359
do it per

01:01:23,119 --> 01:01:26,960
buffer or when when you move to this

01:01:25,359 --> 01:01:30,079
mode the whole driver needs to be

01:01:26,960 --> 01:01:30,079
reconfigured that way

01:01:31,040 --> 01:01:35,760
yes the whole driver needs to support

01:01:34,240 --> 01:01:38,880
this mode obviously it's like

01:01:35,760 --> 01:01:40,640
if you have a few multiple buffers

01:01:38,880 --> 01:01:43,520
received then

01:01:40,640 --> 01:01:44,720
you're in a multi-buffer mode if you're

01:01:43,520 --> 01:01:46,799
not you know

01:01:44,720 --> 01:01:48,160
if you have single buffer you can work

01:01:46,799 --> 01:01:51,200
with single buffer and

01:01:48,160 --> 01:01:52,960
set the bit to zero

01:01:51,200 --> 01:01:55,119
one one thing you have to consider is

01:01:52,960 --> 01:01:56,880
when you do hdp redirect

01:01:55,119 --> 01:01:59,520
to another device that doesn't support

01:01:56,880 --> 01:02:02,480
this multi-buffer mode

01:01:59,520 --> 01:02:02,480
you have to actually

01:02:02,799 --> 01:02:07,359
decide what to do right now you will

01:02:05,599 --> 01:02:09,599
leak memory

01:02:07,359 --> 01:02:09,599
right

01:02:10,559 --> 01:02:17,520
and i can't imagine how this gonna

01:02:14,400 --> 01:02:21,440
affect the pitch pool today so

01:02:17,520 --> 01:02:22,799
i i believe it's gonna be yeah

01:02:21,440 --> 01:02:24,720
i think the number of drivers that

01:02:22,799 --> 01:02:26,079
support xdp is low enough that you can

01:02:24,720 --> 01:02:28,559
probably just add support for

01:02:26,079 --> 01:02:31,039
multibuffer to all of them

01:02:28,559 --> 01:02:31,839
yeah i'm also hoping that the ones that

01:02:31,039 --> 01:02:36,640
actually support

01:02:31,839 --> 01:02:38,559
xp redirects or the indio xtpxmid

01:02:36,640 --> 01:02:42,160
function i think that's slow enough that

01:02:38,559 --> 01:02:42,160
we can update all of them at once

01:02:44,160 --> 01:02:49,839
like i updated all the drivers recently

01:02:46,839 --> 01:02:49,839
too

01:02:51,839 --> 01:02:55,680
the main issue is with the mtu mismatch

01:02:54,160 --> 01:02:57,760
between

01:02:55,680 --> 01:02:59,760
two different drivers running

01:02:57,760 --> 01:03:01,599
concurrently and you're using xdp

01:02:59,760 --> 01:03:04,799
redirect so

01:03:01,599 --> 01:03:08,720
this still would not

01:03:04,799 --> 01:03:08,720
this would not work the work around

01:03:09,280 --> 01:03:14,319
i didn't say it was easy the fundamental

01:03:12,880 --> 01:03:15,359
problem with xdp right now

01:03:14,319 --> 01:03:17,440
is that it doesn't seem to be

01:03:15,359 --> 01:03:19,119
particularly usable with real world

01:03:17,440 --> 01:03:22,559
scenarios

01:03:19,119 --> 01:03:25,200
um mtu's above 4k

01:03:22,559 --> 01:03:26,240
are common having to turn off lro and

01:03:25,200 --> 01:03:30,000
stuff like that

01:03:26,240 --> 01:03:31,520
it's really frustrating

01:03:30,000 --> 01:03:33,359
yeah this this works should hopefully

01:03:31,520 --> 01:03:35,039
address uh

01:03:33,359 --> 01:03:37,119
a lot of the use cases where you have

01:03:35,039 --> 01:03:39,359
lunch i have to support

01:03:37,119 --> 01:03:40,240
nature package right yeah and i see i

01:03:39,359 --> 01:03:43,280
see a real

01:03:40,240 --> 01:03:45,760
this is jesse i see a real need for the

01:03:43,280 --> 01:03:46,559
checksum offload to be integrated with

01:03:45,760 --> 01:03:48,400
this work

01:03:46,559 --> 01:03:49,680
because especially as you're handing

01:03:48,400 --> 01:03:51,200
around these larger

01:03:49,680 --> 01:03:53,440
huge you know let's say they're nine

01:03:51,200 --> 01:03:55,359
nine kilobytes of data right

01:03:53,440 --> 01:03:57,599
um if you're having to check some of

01:03:55,359 --> 01:04:00,319
that in cpu

01:03:57,599 --> 01:04:01,599
it's going to become a real consumer of

01:04:00,319 --> 01:04:04,799
cycles

01:04:01,599 --> 01:04:06,720
okay so moving on uh thanks amy thanks

01:04:04,799 --> 01:04:09,599
everyone

01:04:06,720 --> 01:04:10,160
okay uh to the next topic i think i

01:04:09,599 --> 01:04:13,839
believe it's

01:04:10,160 --> 01:04:16,960
xdp for time sensitive network

01:04:13,839 --> 01:04:18,880
uh dam stamping for xdp so

01:04:16,960 --> 01:04:20,559
uh tsn stands for time-sensitive

01:04:18,880 --> 01:04:22,240
networking the

01:04:20,559 --> 01:04:24,480
uh the whole thing started trying to

01:04:22,240 --> 01:04:26,079
provide bounding latencies for ethernet

01:04:24,480 --> 01:04:29,599
and by bounded we mean

01:04:26,079 --> 01:04:32,000
exact time delivery is not not the

01:04:29,599 --> 01:04:33,520
little vegeta you had on the on the

01:04:32,000 --> 01:04:36,559
packet deliveries

01:04:33,520 --> 01:04:38,559
uh so there has been schedulers that

01:04:36,559 --> 01:04:40,799
have been sent and upstream on the

01:04:38,559 --> 01:04:42,480
kernel and you have hardware offload for

01:04:40,799 --> 01:04:44,160
a bunch of schedulers that work up to

01:04:42,480 --> 01:04:46,240
that

01:04:44,160 --> 01:04:49,039
so we did some tests trying to figure

01:04:46,240 --> 01:04:49,680
out if af xdp can be applied on the rx

01:04:49,039 --> 01:04:53,119
path not the

01:04:49,680 --> 01:04:55,920
tx but when your schedulers usually work

01:04:53,119 --> 01:04:57,200
but since the the the low la the bounded

01:04:55,920 --> 01:05:00,079
latency that tsn

01:04:57,200 --> 01:05:00,640
uh requires is not the only constraint

01:05:00,079 --> 01:05:02,240
you have

01:05:00,640 --> 01:05:03,920
there's applications that you also need

01:05:02,240 --> 01:05:07,359
really low latency

01:05:03,920 --> 01:05:09,440
so since the uh

01:05:07,359 --> 01:05:10,559
so since the kernel we we did some

01:05:09,440 --> 01:05:11,119
measurements on the camera on the

01:05:10,559 --> 01:05:12,720
initial

01:05:11,119 --> 01:05:14,480
on some arm devices on the kernel

01:05:12,720 --> 01:05:16,400
networking stack and we had average

01:05:14,480 --> 01:05:18,640
packet deliveries for udp packets

01:05:16,400 --> 01:05:20,319
small packets between 17 and 100

01:05:18,640 --> 01:05:22,720
microseconds

01:05:20,319 --> 01:05:24,640
uh this is way above than some of the

01:05:22,720 --> 01:05:27,760
applications expect so we

01:05:24,640 --> 01:05:29,599
we went around and tried xdp and the

01:05:27,760 --> 01:05:31,839
initial measurements we got were

01:05:29,599 --> 01:05:34,640
quite encouraging we managed to get

01:05:31,839 --> 01:05:37,599
under 20 microseconds af xdp actually

01:05:34,640 --> 01:05:38,640
uh we managed to get below 20

01:05:37,599 --> 01:05:40,799
microseconds for

01:05:38,640 --> 01:05:42,319
for center for certain packet loads and

01:05:40,799 --> 01:05:46,079
applications

01:05:42,319 --> 01:05:48,319
um now this this whole thing uh

01:05:46,079 --> 01:05:50,000
relies on some hacking we need to get

01:05:48,319 --> 01:05:53,039
the timestamp because i've we've

01:05:50,000 --> 01:05:55,680
also had the talk with dave uh

01:05:53,039 --> 01:05:57,119
that xdp doesn't support timestamping so

01:05:55,680 --> 01:05:59,359
what we did to measure the

01:05:57,119 --> 01:06:01,200
the latency was pick the timestamp from

01:05:59,359 --> 01:06:02,880
the network interface or from the kernel

01:06:01,200 --> 01:06:04,160
if the network interface didn't support

01:06:02,880 --> 01:06:07,119
that

01:06:04,160 --> 01:06:08,319
put it on the xtp metadata which was

01:06:07,119 --> 01:06:10,000
unused

01:06:08,319 --> 01:06:11,599
and then we could read that timestamp

01:06:10,000 --> 01:06:13,359
from the user space

01:06:11,599 --> 01:06:14,880
pick up the the equivalent timestamp

01:06:13,359 --> 01:06:16,000
from the user space and then compare the

01:06:14,880 --> 01:06:18,480
values

01:06:16,000 --> 01:06:20,559
although this is not 100 safe and this

01:06:18,480 --> 01:06:23,280
is not exactly what you want to do

01:06:20,559 --> 01:06:24,559
uh it it gives us a rough idea that the

01:06:23,280 --> 01:06:26,319
af xdp

01:06:24,559 --> 01:06:28,400
might be usable on the low level on the

01:06:26,319 --> 01:06:29,920
on the low latency use cases for tsm

01:06:28,400 --> 01:06:31,599
the only the only thing we haven't been

01:06:29,920 --> 01:06:33,920
able to test because this is testing the

01:06:31,599 --> 01:06:36,559
rx path the only thing we left out

01:06:33,920 --> 01:06:38,319
is the transmit path and the the problem

01:06:36,559 --> 01:06:42,000
with the transmit path is

01:06:38,319 --> 01:06:44,160
in order to get uh a clear idea of

01:06:42,000 --> 01:06:47,039
what's happening you need to implement

01:06:44,160 --> 01:06:48,559
the zero copy af xdp if you don't you're

01:06:47,039 --> 01:06:51,039
just going through the kernel

01:06:48,559 --> 01:06:52,640
network stack and there's nothing and

01:06:51,039 --> 01:06:54,799
there's nothing that changes that

01:06:52,640 --> 01:06:56,079
uh so you could get really similar

01:06:54,799 --> 01:06:59,039
results

01:06:56,079 --> 01:06:59,520
uh and one or another thing that makes

01:06:59,039 --> 01:07:01,200
this

01:06:59,520 --> 01:07:02,720
even better and the measurements we got

01:07:01,200 --> 01:07:03,039
are probably going to be even better is

01:07:02,720 --> 01:07:05,680
that

01:07:03,039 --> 01:07:07,599
we didn't measure all these on on a real

01:07:05,680 --> 01:07:10,160
time on a real-time kernel

01:07:07,599 --> 01:07:11,039
uh the problem is that a few months ago

01:07:10,160 --> 01:07:14,960
bpf

01:07:11,039 --> 01:07:16,640
and the preempter t couldn't coexist

01:07:14,960 --> 01:07:19,200
there's been some patches which i think

01:07:16,640 --> 01:07:21,599
they got merged a few months ago

01:07:19,200 --> 01:07:22,880
that allow this coexistence so if we do

01:07:21,599 --> 01:07:24,240
the measurements again i'm pretty sure

01:07:22,880 --> 01:07:25,520
we're going to get even better results

01:07:24,240 --> 01:07:27,839
on this

01:07:25,520 --> 01:07:29,280
at least on the data side can you try to

01:07:27,839 --> 01:07:31,119
explain again where were you

01:07:29,280 --> 01:07:33,359
making where you were making the

01:07:31,119 --> 01:07:36,160
timestamp for the xzp programs

01:07:33,359 --> 01:07:36,960
yeah if it's not supported by hardware

01:07:36,160 --> 01:07:38,480
uh

01:07:36,960 --> 01:07:40,480
you can you can pick up the timestamp

01:07:38,480 --> 01:07:42,480
from the for example on intel hardware

01:07:40,480 --> 01:07:43,839
you can get the rtdsc

01:07:42,480 --> 01:07:45,680
instruction and you can get the number

01:07:43,839 --> 01:07:47,359
of cycles and you can

01:07:45,680 --> 01:07:48,720
you can get the same thing on user space

01:07:47,359 --> 01:07:50,319
and then you can just compare how many

01:07:48,720 --> 01:07:53,599
cycles you've spent

01:07:50,319 --> 01:07:58,000
my question in their program

01:07:53,599 --> 01:07:58,000
like lifetime or

01:07:58,640 --> 01:08:02,160
you mean after we process the packet on

01:08:00,559 --> 01:08:04,480
user space or before we process the

01:08:02,160 --> 01:08:07,839
packet on user space

01:08:04,480 --> 01:08:09,599
no the processing on xdp how

01:08:07,839 --> 01:08:10,880
which time stamp do you attach to the

01:08:09,599 --> 01:08:12,880
xdp frame

01:08:10,880 --> 01:08:14,319
okay if the if the hardware if the

01:08:12,880 --> 01:08:16,159
network interface

01:08:14,319 --> 01:08:18,159
can get you a timestamp they can use you

01:08:16,159 --> 01:08:19,040
can use phc and sync your clocks with

01:08:18,159 --> 01:08:22,319
user space

01:08:19,040 --> 01:08:24,000
so you have a current clock so if if you

01:08:22,319 --> 01:08:26,000
can get the hardware timestamp and the

01:08:24,000 --> 01:08:26,880
hardware supports it you can compare the

01:08:26,000 --> 01:08:30,080
hardware's time

01:08:26,880 --> 01:08:31,120
which is pretty accurate if you don't

01:08:30,080 --> 01:08:33,040
you can pick up the

01:08:31,120 --> 01:08:35,040
software timestamp in the kernel and

01:08:33,040 --> 01:08:37,040
then try to approximate how much time

01:08:35,040 --> 01:08:38,319
your irq will take

01:08:37,040 --> 01:08:40,960
your software queue will take in the

01:08:38,319 --> 01:08:42,960
kernel and try to add these values

01:08:40,960 --> 01:08:44,319
again i understand that this is not this

01:08:42,960 --> 01:08:45,839
is not completely accurate and this is

01:08:44,319 --> 01:08:48,000
not the best thing you can do it's just

01:08:45,839 --> 01:08:50,880
a rough estimation

01:08:48,000 --> 01:08:52,000
and the proof of concept that afxdp can

01:08:50,880 --> 01:08:54,080
offer significantly low

01:08:52,000 --> 01:08:56,960
later lower latencies on at least

01:08:54,080 --> 01:08:56,960
dequeuing the packet

01:08:57,279 --> 01:09:02,400
and another thing that makes xdp a good

01:08:59,279 --> 01:09:04,159
model for that is that you don't need to

01:09:02,400 --> 01:09:05,920
do this for every packet so for example

01:09:04,159 --> 01:09:07,440
if you have time thinking packets going

01:09:05,920 --> 01:09:08,640
around the network that are trying to

01:09:07,440 --> 01:09:10,400
sync your whole network

01:09:08,640 --> 01:09:12,239
you don't need to offload those into

01:09:10,400 --> 01:09:13,199
user space you can just have a bpf

01:09:12,239 --> 01:09:14,880
program

01:09:13,199 --> 01:09:16,560
that sends the packets that are

01:09:14,880 --> 01:09:18,319
non-sensitive to the kernel and keep

01:09:16,560 --> 01:09:19,120
your normal network stack while you

01:09:18,319 --> 01:09:21,839
offload

01:09:19,120 --> 01:09:23,600
all of your uh significant traffic or

01:09:21,839 --> 01:09:25,920
time sensitive traffic down to user

01:09:23,600 --> 01:09:28,319
space

01:09:25,920 --> 01:09:30,000
uh you mean implementing the whole ptp

01:09:28,319 --> 01:09:32,319
protocol inside xzp

01:09:30,000 --> 01:09:34,080
yeah you don't need to do that you can

01:09:32,319 --> 01:09:35,520
just do the ptp on the kernel

01:09:34,080 --> 01:09:37,120
because you can just allow the ptp

01:09:35,520 --> 01:09:38,799
packets to go through the kernel

01:09:37,120 --> 01:09:41,440
get a response from your hardware sync

01:09:38,799 --> 01:09:43,440
properly and then you can just offload

01:09:41,440 --> 01:09:44,960
all the critical traffic to a user space

01:09:43,440 --> 01:09:47,440
application

01:09:44,960 --> 01:09:49,120
so you would avoid any jitter between

01:09:47,440 --> 01:09:50,480
kernels yeah it's not avoiding you can

01:09:49,120 --> 01:09:52,799
you can do a lot of things for example

01:09:50,480 --> 01:09:56,000
you can you know you can pin cpus

01:09:52,799 --> 01:09:57,920
uh with you can pin cpus uh for your

01:09:56,000 --> 01:09:59,840
user space application so you get even

01:09:57,920 --> 01:10:02,159
faster processing times the whole idea

01:09:59,840 --> 01:10:04,320
was to prove that

01:10:02,159 --> 01:10:05,840
tsn tries to guarantee you the jitter

01:10:04,320 --> 01:10:07,040
because it's a hardware scheduler that's

01:10:05,840 --> 01:10:08,560
running over there

01:10:07,040 --> 01:10:10,960
at least from the other side so you get

01:10:08,560 --> 01:10:12,640
the packet on a timely manner

01:10:10,960 --> 01:10:14,480
and then you have to get it to your

01:10:12,640 --> 01:10:16,400
application as fast as you can

01:10:14,480 --> 01:10:17,679
and with as little jitter as you can so

01:10:16,400 --> 01:10:20,000
what what elias did

01:10:17,679 --> 01:10:20,800
was he basically implemented the

01:10:20,000 --> 01:10:22,320
hardware

01:10:20,800 --> 01:10:24,960
the hardware hints so he put it in the

01:10:22,320 --> 01:10:27,040
metadata area

01:10:24,960 --> 01:10:28,159
so he just hacked up picked up did a

01:10:27,040 --> 01:10:30,800
modified kernel and

01:10:28,159 --> 01:10:32,800
and put this into the driver so it's

01:10:30,800 --> 01:10:36,560
it's just because we don't have

01:10:32,800 --> 01:10:38,880
what's what you're proposing yet

01:10:36,560 --> 01:10:40,640
the metadata is pretty important for the

01:10:38,880 --> 01:10:42,159
uh the time stamp is not really

01:10:40,640 --> 01:10:44,400
important for

01:10:42,159 --> 01:10:46,480
uh for anything that i have in mind of

01:10:44,400 --> 01:10:48,239
any of my use cases but for you know

01:10:46,480 --> 01:10:49,679
measuring this and trying to provide

01:10:48,239 --> 01:10:52,320
accurate results

01:10:49,679 --> 01:10:52,880
uh comparing apple to apples is pretty

01:10:52,320 --> 01:10:55,600
it's

01:10:52,880 --> 01:10:55,600
pretty important

01:10:56,960 --> 01:11:00,640
yeah well it's important if you want

01:10:58,560 --> 01:11:01,280
your logic to run in the inside xdp

01:11:00,640 --> 01:11:03,280
program

01:11:01,280 --> 01:11:05,120
so you need it there might want to put

01:11:03,280 --> 01:11:06,000
the uh the two presentations that i've

01:11:05,120 --> 01:11:07,440
linked on the email

01:11:06,000 --> 01:11:09,199
at least on the meeting note so that

01:11:07,440 --> 01:11:09,679
people can have an idea on you know when

01:11:09,199 --> 01:11:12,960
to look

01:11:09,679 --> 01:11:15,600
and where to look if they have any any

01:11:12,960 --> 01:11:16,239
oh yeah these this presentation is

01:11:15,600 --> 01:11:18,239
already

01:11:16,239 --> 01:11:19,280
uh one of them is already there yeah

01:11:18,239 --> 01:11:22,960
that's fine

01:11:19,280 --> 01:11:25,520
yeah so

01:11:22,960 --> 01:11:26,159
okay let's move on to the next topic

01:11:25,520 --> 01:11:30,560
which is

01:11:26,159 --> 01:11:33,760
uh similar uh latency related

01:11:30,560 --> 01:11:35,840
and i think it will also be

01:11:33,760 --> 01:11:37,280
time stamping and time sensitive

01:11:35,840 --> 01:11:41,360
applications

01:11:37,280 --> 01:11:43,440
so uh swedar will uh uh

01:11:41,360 --> 01:11:45,760
we'll talk about basic pulling support

01:11:43,440 --> 01:11:48,960
for afx bp

01:11:45,760 --> 01:11:49,679
okay so let me start with the use case

01:11:48,960 --> 01:11:52,320
where

01:11:49,679 --> 01:11:53,600
busy polling would be useful for af xtp

01:11:52,320 --> 01:11:55,679
sockets

01:11:53,600 --> 01:11:56,960
okay so this is mainly targeted for

01:11:55,679 --> 01:11:59,520
applications that

01:11:56,960 --> 01:12:02,000
create afx tp sockets that are bound to

01:11:59,520 --> 01:12:04,640
a dedicated and isolated queues

01:12:02,000 --> 01:12:06,080
so by dedicated what i mean is that a

01:12:04,640 --> 01:12:08,640
hardware filter directs

01:12:06,080 --> 01:12:10,000
packets to these queues based on some

01:12:08,640 --> 01:12:12,800
match criteria

01:12:10,000 --> 01:12:15,199
that identifies packets that for a

01:12:12,800 --> 01:12:17,679
specific af xtp socket

01:12:15,199 --> 01:12:19,440
and by isolated what i mean is that

01:12:17,679 --> 01:12:20,719
these queues don't receive any other

01:12:19,440 --> 01:12:23,679
packets

01:12:20,719 --> 01:12:24,320
so today this can be done using features

01:12:23,679 --> 01:12:27,679
like uh

01:12:24,320 --> 01:12:29,840
ease tool rss context uh where a group

01:12:27,679 --> 01:12:31,520
of queues can be associated with its own

01:12:29,840 --> 01:12:33,520
rss context

01:12:31,520 --> 01:12:35,520
and a hardware filter can direct package

01:12:33,520 --> 01:12:37,760
to this group of queues

01:12:35,520 --> 01:12:38,800
and similarly application device queues

01:12:37,760 --> 01:12:41,360
is another feature

01:12:38,800 --> 01:12:42,400
that allows creating dedicated and

01:12:41,360 --> 01:12:46,560
isolated

01:12:42,400 --> 01:12:49,679
rx and pxq groups so actually we have a

01:12:46,560 --> 01:12:50,239
talk that is going to we have a session

01:12:49,679 --> 01:12:52,719
that

01:12:50,239 --> 01:12:53,520
talks about this feature some in next

01:12:52,719 --> 01:12:56,239
week

01:12:53,520 --> 01:12:58,560
and we'll go over that in that session

01:12:56,239 --> 01:13:01,120
in more details

01:12:58,560 --> 01:13:02,719
okay so now once we have these dedicated

01:13:01,120 --> 01:13:05,679
and isolated cues

01:13:02,719 --> 01:13:07,520
associated with these sockets right we

01:13:05,679 --> 01:13:10,719
can enable busy polling

01:13:07,520 --> 01:13:13,440
and what it does is it allows an app

01:13:10,719 --> 01:13:14,960
to pull the queue only when it is ready

01:13:13,440 --> 01:13:18,640
to receive or send

01:13:14,960 --> 01:13:21,760
packets on that queue so this basically

01:13:18,640 --> 01:13:24,800
it avoids the context switches and

01:13:21,760 --> 01:13:28,159
interrupts and the nappy pole con can

01:13:24,800 --> 01:13:28,159
happen in the app context

01:13:31,040 --> 01:13:37,040
let me go to the itself slide so

01:13:34,719 --> 01:13:39,840
so let us go into the details on on the

01:13:37,040 --> 01:13:43,520
proposal and how this can be implemented

01:13:39,840 --> 01:13:47,040
so today when an afx dp app tries to

01:13:43,520 --> 01:13:49,520
send or receive packets uh it does a

01:13:47,040 --> 01:13:51,760
poll or a send system call

01:13:49,520 --> 01:13:53,120
is invoked and if the driver has set

01:13:51,760 --> 01:13:56,960
need wake up

01:13:53,120 --> 01:14:00,159
on that queue so the driver specific ndo

01:13:56,960 --> 01:14:03,600
xsk wake up is called so this

01:14:00,159 --> 01:14:05,360
call uh basically triggers an interrupt

01:14:03,600 --> 01:14:06,880
to schedule nappy from the interrupt

01:14:05,360 --> 01:14:10,000
context so

01:14:06,880 --> 01:14:11,040
instead the idea here is to basically do

01:14:10,000 --> 01:14:14,159
a non-blocking

01:14:11,040 --> 01:14:17,679
nappy busy loop that will trigger nappy

01:14:14,159 --> 01:14:21,600
pole from the app context itself

01:14:17,679 --> 01:14:25,679
okay so currently when nappy pole

01:14:21,600 --> 01:14:28,719
when is called from the nappy pc loop

01:14:25,679 --> 01:14:30,880
it uses a budget of eight

01:14:28,719 --> 01:14:33,280
so this is a one issue that needs to be

01:14:30,880 --> 01:14:36,159
resolved so basically this

01:14:33,280 --> 01:14:38,239
budget of eight is too low for af xdp as

01:14:36,159 --> 01:14:40,560
the packet processing time

01:14:38,239 --> 01:14:41,440
is much low compared to a normal skb

01:14:40,560 --> 01:14:45,600
data path

01:14:41,440 --> 01:14:47,600
so we need a way to increase this budget

01:14:45,600 --> 01:14:49,600
so one way to do this is basically we

01:14:47,600 --> 01:14:52,880
can have a configurable value

01:14:49,600 --> 01:14:56,800
via cctl and that can be used

01:14:52,880 --> 01:14:59,920
as an argument to the navi pc loop

01:14:56,800 --> 01:15:03,520
that is one change we are proposing

01:14:59,920 --> 01:15:03,520
okay and then

01:15:06,080 --> 01:15:11,679
so the other change we need is to

01:15:08,800 --> 01:15:13,600
introduce a new nappy state

01:15:11,679 --> 01:15:15,199
called basically i'm calling it as no

01:15:13,600 --> 01:15:18,320
busy pole stop

01:15:15,199 --> 01:15:20,239
so this bit can be set when an a

01:15:18,320 --> 01:15:21,679
on the nappy associated with the cube

01:15:20,239 --> 01:15:24,400
when an af xtp

01:15:21,679 --> 01:15:26,480
socket is bound to the device view and

01:15:24,400 --> 01:15:29,920
it can be cleared when it is detached

01:15:26,480 --> 01:15:30,800
from the queue and so the way we use

01:15:29,920 --> 01:15:34,239
this bit is

01:15:30,800 --> 01:15:36,320
basically after the at the end of a

01:15:34,239 --> 01:15:37,360
nappy busy loop right a busy pole stop

01:15:36,320 --> 01:15:40,159
is called

01:15:37,360 --> 01:15:40,880
today that invokes the nappy pool one

01:15:40,159 --> 01:15:44,719
more time

01:15:40,880 --> 01:15:48,320
to enable the interrupts if required

01:15:44,719 --> 01:15:50,480
so but in case of uh af xtp

01:15:48,320 --> 01:15:51,440
zero copy socket that is doing busy

01:15:50,480 --> 01:15:53,600
polling

01:15:51,440 --> 01:15:54,719
we what we want is we don't want to

01:15:53,600 --> 01:15:56,800
another

01:15:54,719 --> 01:15:57,840
nappy pole to be called so basically we

01:15:56,800 --> 01:16:00,080
can

01:15:57,840 --> 01:16:01,600
have this check this bit in the busy

01:16:00,080 --> 01:16:04,320
pool stop to avoid

01:16:01,600 --> 01:16:04,640
making the call to nap people yeah so

01:16:04,320 --> 01:16:06,800
the

01:16:04,640 --> 01:16:09,280
other thing actually so here is some

01:16:06,800 --> 01:16:12,239
some feedback i would like to get

01:16:09,280 --> 01:16:14,800
so basically when when should we do this

01:16:12,239 --> 01:16:17,520
busy pole triggering

01:16:14,800 --> 01:16:19,600
so the one the most simplest and the

01:16:17,520 --> 01:16:20,880
automatic way is basically

01:16:19,600 --> 01:16:23,199
what we what i am calling is

01:16:20,880 --> 01:16:26,239
opportunistic busy polling

01:16:23,199 --> 01:16:30,320
where this is done only when the

01:16:26,239 --> 01:16:32,480
app thread is bound to the same core as

01:16:30,320 --> 01:16:34,000
the q vector snappy

01:16:32,480 --> 01:16:35,679
so this is what actually magnus

01:16:34,000 --> 01:16:37,360
suggested this option

01:16:35,679 --> 01:16:39,040
so with this option basically we don't

01:16:37,360 --> 01:16:42,320
need any changes to the

01:16:39,040 --> 01:16:43,600
app and any configuration so it happens

01:16:42,320 --> 01:16:45,600
automatically

01:16:43,600 --> 01:16:47,520
but the only requirement is that the app

01:16:45,600 --> 01:16:50,080
thread has to be pinned to the same core

01:16:47,520 --> 01:16:50,080
as the queue

01:16:50,480 --> 01:16:53,920
okay so the other options are basically

01:16:52,719 --> 01:16:56,719
we can use a

01:16:53,920 --> 01:16:57,199
triggering the global the busy pole

01:16:56,719 --> 01:17:01,120
setting

01:16:57,199 --> 01:17:04,719
as one mechanism or uses socket

01:17:01,120 --> 01:17:04,719
per socket visible option

01:17:07,040 --> 01:17:12,000
yeah so actually yeah this these are the

01:17:10,000 --> 01:17:14,719
three options that i am thinking but

01:17:12,000 --> 01:17:15,760
if if it is okay we can just go with the

01:17:14,719 --> 01:17:18,560
first one

01:17:15,760 --> 01:17:20,080
we thought we basically we do it only if

01:17:18,560 --> 01:17:23,280
the app thread

01:17:20,080 --> 01:17:25,760
is bound to the same core as the queue

01:17:23,280 --> 01:17:27,840
yeah i think that's all i had so

01:17:25,760 --> 01:17:32,000
basically i would like to get some

01:17:27,840 --> 01:17:36,400
feedback on this specific

01:17:32,000 --> 01:17:38,400
issue on when to trigger the busy pole

01:17:36,400 --> 01:17:40,719
and with regards to the performance

01:17:38,400 --> 01:17:44,480
right so actually i collected some

01:17:40,719 --> 01:17:46,480
data on an intel 40 gig nic so

01:17:44,480 --> 01:17:48,400
what i noticed is that with basically

01:17:46,480 --> 01:17:51,600
the px only test

01:17:48,400 --> 01:17:55,040
i see significant improvement uh almost

01:17:51,600 --> 01:17:58,800
20 to 20 percent improvement

01:17:55,040 --> 01:18:02,159
compared to a without busy polling

01:17:58,800 --> 01:18:03,760
and it is basically same as a 2 core

01:18:02,159 --> 01:18:05,520
performance we get

01:18:03,760 --> 01:18:07,120
with a single core we can get the same

01:18:05,520 --> 01:18:09,360
thing and the

01:18:07,120 --> 01:18:10,880
l2 forward is another test case that i

01:18:09,360 --> 01:18:13,760
saw a

01:18:10,880 --> 01:18:15,520
significant improvement the received

01:18:13,760 --> 01:18:18,719
drop is almost the same i didn't see

01:18:15,520 --> 01:18:18,719
anything with the received drop

01:18:19,120 --> 01:18:27,840
so 20 in latency or packet rate

01:18:23,600 --> 01:18:27,840
packet rate

01:18:27,920 --> 01:18:34,000
okay and actually i have some concern

01:18:31,520 --> 01:18:35,840
regarding the shared nappy which we have

01:18:34,000 --> 01:18:37,600
in most driver today so most driver

01:18:35,840 --> 01:18:39,600
nappy function today is

01:18:37,600 --> 01:18:41,360
working on all of the cues that are

01:18:39,600 --> 01:18:44,520
sharing the same rq

01:18:41,360 --> 01:18:47,760
like the ptxq the kernel tq

01:18:44,520 --> 01:18:51,199
rxqs and for mx5

01:18:47,760 --> 01:18:55,440
i'm talking about mx5 uh fxp

01:18:51,199 --> 01:18:58,480
also queues also share the same nappy

01:18:55,440 --> 01:19:00,320
loop so it

01:18:58,480 --> 01:19:01,679
appears to me that we need some locking

01:19:00,320 --> 01:19:04,960
mechanism because

01:19:01,679 --> 01:19:09,679
calling nappy from user context is

01:19:04,960 --> 01:19:14,640
requires some locking

01:19:09,679 --> 01:19:14,640
yeah so this will require basically

01:19:15,280 --> 01:19:22,880
a vector associated with a q q pair

01:19:19,120 --> 01:19:26,560
so we cannot have a multiple

01:19:22,880 --> 01:19:26,560
queues sharing the same vector

01:19:27,199 --> 01:19:31,440
okay got it so yeah

01:19:31,520 --> 01:19:38,719
so driver changes are required

01:19:36,480 --> 01:19:40,480
yeah it depends on only if a driver

01:19:38,719 --> 01:19:43,760
supports that model i think

01:19:40,480 --> 01:19:46,080
yeah this will be useful

01:19:43,760 --> 01:19:46,080
okay

01:19:48,960 --> 01:19:54,719
so basically we need a mechanism to

01:19:50,719 --> 01:19:58,640
isolate traffic to those specific queues

01:19:54,719 --> 01:20:00,000
and also the vector associated with that

01:19:58,640 --> 01:20:03,120
queue

01:20:00,000 --> 01:20:07,920
and the application thread

01:20:03,120 --> 01:20:11,360
are on the same core then yeah we can

01:20:07,920 --> 01:20:11,360
do this type of busy calling

01:20:11,600 --> 01:20:15,120
right i'm thinking right now uh for the

01:20:14,159 --> 01:20:18,239
slow path right

01:20:15,120 --> 01:20:21,820
fx dp is electron traffic directly to

01:20:18,239 --> 01:20:23,120
the user space between the

01:20:21,820 --> 01:20:26,800
[Music]

01:20:23,120 --> 01:20:27,679
execute program that is parsing the

01:20:26,800 --> 01:20:29,440
packets and

01:20:27,679 --> 01:20:31,840
deciding if the package should go to the

01:20:29,440 --> 01:20:35,360
fxdp

01:20:31,840 --> 01:20:37,600
q or to the kernel stack then it copies

01:20:35,360 --> 01:20:39,600
the packet to the kernel stack

01:20:37,600 --> 01:20:40,719
trying to think if there are any issues

01:20:39,600 --> 01:20:43,920
with that um

01:20:40,719 --> 01:20:43,920
i don't believe so but

01:20:44,719 --> 01:20:48,639
i think you should look into that as

01:20:46,480 --> 01:20:48,639
well

01:20:49,360 --> 01:20:55,120
yeah so so that's why actually

01:20:52,800 --> 01:20:58,639
originally when i sat right it

01:20:55,120 --> 01:21:00,880
would be good if we can

01:20:58,639 --> 01:21:02,800
isolate the cues that the all the

01:21:00,880 --> 01:21:07,360
traffic that is coming on a queue

01:21:02,800 --> 01:21:09,360
is directed for this af xtp socket

01:21:07,360 --> 01:21:10,480
right so so this looks something like a

01:21:09,360 --> 01:21:14,080
new requirement

01:21:10,480 --> 01:21:16,560
yeah yeah yeah so this to get the

01:21:14,080 --> 01:21:19,040
real performance actually we need to

01:21:16,560 --> 01:21:22,400
isolate the queues

01:21:19,040 --> 01:21:24,960
traffic via some hardware filter

01:21:22,400 --> 01:21:25,679
okay i get no comments so uh thanks

01:21:24,960 --> 01:21:29,840
riddar

01:21:25,679 --> 01:21:29,840
uh move on to the next topic

01:21:32,000 --> 01:21:38,400
so before we uh go into the open

01:21:35,280 --> 01:21:38,400
discussion session

01:21:38,960 --> 01:21:43,920
i would like to try to discuss a

01:21:41,440 --> 01:21:47,520
performance and page pool

01:21:43,920 --> 01:21:47,520
some page will open issues

01:21:48,960 --> 01:21:52,639
i think jesper and jonathan wanted to

01:21:51,520 --> 01:21:56,320
talk about uh

01:21:52,639 --> 01:21:59,679
some uh prime work and

01:21:56,320 --> 01:22:02,719
uh for skb

01:21:59,679 --> 01:22:05,440
recycling in the page

01:22:02,719 --> 01:22:06,080
so the the idea came from the fact that

01:22:05,440 --> 01:22:08,960
uh

01:22:06,080 --> 01:22:10,159
we use an api in an internal api that

01:22:08,960 --> 01:22:13,280
allocates

01:22:10,159 --> 01:22:15,600
maps syncs your dma memory for the

01:22:13,280 --> 01:22:16,159
network interface in the xdp use case

01:22:15,600 --> 01:22:19,280
and that's

01:22:16,159 --> 01:22:21,520
one of the reasons xdp has

01:22:19,280 --> 01:22:23,199
such a big speed improvements compared

01:22:21,520 --> 01:22:26,400
to

01:22:23,199 --> 01:22:26,400
what happens on the skb

01:22:26,719 --> 01:22:30,800
so what we figured out is that we could

01:22:29,040 --> 01:22:34,480
apply the same principle on the

01:22:30,800 --> 01:22:38,639
on the skb uh device path uh we didn't

01:22:34,480 --> 01:22:40,480
we did send an rfc and this is the

01:22:38,639 --> 01:22:42,400
the the performance improvement you're

01:22:40,480 --> 01:22:44,320
seeing on the driver

01:22:42,400 --> 01:22:45,760
uh obviously as the packets get bigger

01:22:44,320 --> 01:22:48,800
the improvements get lost but

01:22:45,760 --> 01:22:50,080
up to 512 megabytes uh kilobytes

01:22:48,800 --> 01:22:52,000
uh you pretty much get double the

01:22:50,080 --> 01:22:54,560
performance the reason

01:22:52,000 --> 01:22:55,120
the 257 exists on this on the chart is

01:22:54,560 --> 01:22:58,159
that the

01:22:55,120 --> 01:23:01,199
the driver uh used to do zero copy

01:22:58,159 --> 01:23:03,280
up to six to 256 kilobytes

01:23:01,199 --> 01:23:04,560
uh so that's that's why you see the big

01:23:03,280 --> 01:23:07,120
performance boost

01:23:04,560 --> 01:23:08,320
uh the moment the the zero cop mechanism

01:23:07,120 --> 01:23:10,560
the

01:23:08,320 --> 01:23:11,440
the mechanism goes away so what we did

01:23:10,560 --> 01:23:14,480
is that we

01:23:11,440 --> 01:23:18,000
we added some fields on the on the skb

01:23:14,480 --> 01:23:19,600
uh and when the network driver was using

01:23:18,000 --> 01:23:24,800
page pull to allocate

01:23:19,600 --> 01:23:27,440
the buffers for the for the skb data

01:23:24,800 --> 01:23:30,960
we could intercept the packet during the

01:23:27,440 --> 01:23:33,199
skb3 and just recycle into page pool

01:23:30,960 --> 01:23:34,320
since page pool has two pools one fast

01:23:33,199 --> 01:23:36,639
path one really far

01:23:34,320 --> 01:23:38,400
one really fast and one slower path for

01:23:36,639 --> 01:23:40,400
for recycling the pages

01:23:38,400 --> 01:23:42,400
uh we could recycle the page and we

01:23:40,400 --> 01:23:44,560
didn't need to unmap the buffer

01:23:42,400 --> 01:23:46,560
and map it again in order to refill the

01:23:44,560 --> 01:23:48,239
descriptors of the device we just

01:23:46,560 --> 01:23:50,239
had to sync and invalidate the buffer

01:23:48,239 --> 01:23:52,080
towards the correct direction

01:23:50,239 --> 01:23:54,320
and fill in the hardware with the

01:23:52,080 --> 01:23:55,120
already allocated buffer so you get away

01:23:54,320 --> 01:23:58,239
with

01:23:55,120 --> 01:23:59,440
not mapping not not unmapping and not

01:23:58,239 --> 01:24:03,520
allocating a new buffer

01:23:59,440 --> 01:24:06,639
you just you just have it pre-allocated

01:24:03,520 --> 01:24:08,400
uh so the there has been some

01:24:06,639 --> 01:24:10,560
involvement since the first rfc

01:24:08,400 --> 01:24:12,000
uh one of the big issues we had on the

01:24:10,560 --> 01:24:13,280
first rfc and the way the code was

01:24:12,000 --> 01:24:16,719
structured is that

01:24:13,280 --> 01:24:19,360
we could only uh recycle escapees

01:24:16,719 --> 01:24:22,000
not escape b fragments there was there

01:24:19,360 --> 01:24:24,320
was quite a bit of refactoring and we

01:24:22,000 --> 01:24:25,760
we now have a path that recycles

01:24:24,320 --> 01:24:28,320
everything

01:24:25,760 --> 01:24:29,520
so you can recycle skb and skb fragments

01:24:28,320 --> 01:24:31,360
as well

01:24:29,520 --> 01:24:33,520
jonathan has added a bunch of

01:24:31,360 --> 01:24:34,639
protections against the initial code so

01:24:33,520 --> 01:24:36,719
you make sure you

01:24:34,639 --> 01:24:37,679
you always recycle a page pull buffer

01:24:36,719 --> 01:24:38,880
because if the

01:24:37,679 --> 01:24:40,719
you know if the buffer you end up

01:24:38,880 --> 01:24:41,199
recycling is not allocated from the page

01:24:40,719 --> 01:24:44,239
will you

01:24:41,199 --> 01:24:46,639
blow up everything uh

01:24:44,239 --> 01:24:47,520
and that's pretty much the status of we

01:24:46,639 --> 01:24:50,159
are at the moment

01:24:47,520 --> 01:24:50,800
the the unfortunately none of us had

01:24:50,159 --> 01:24:52,960
time to

01:24:50,800 --> 01:24:55,040
continue on this one and pick it up and

01:24:52,960 --> 01:24:57,280
you know try to merge it properly

01:24:55,040 --> 01:24:58,719
but where we stand at the moment is we

01:24:57,280 --> 01:25:00,960
have a code that's

01:24:58,719 --> 01:25:02,320
recycling everything it seems pretty

01:25:00,960 --> 01:25:04,800
fast to safe to use

01:25:02,320 --> 01:25:06,400
um jonathan correct me if i'm wrong you

01:25:04,800 --> 01:25:07,600
said it you were running it on some test

01:25:06,400 --> 01:25:10,840
servers

01:25:07,600 --> 01:25:12,719
and the results were encouraging and

01:25:10,840 --> 01:25:16,400
yeah

01:25:12,719 --> 01:25:17,360
we i modified the mlx forward driver

01:25:16,400 --> 01:25:21,360
code which

01:25:17,360 --> 01:25:23,920
does page full of recycling

01:25:21,360 --> 01:25:26,239
and also sends that page fragments

01:25:23,920 --> 01:25:28,159
because every buffer is 2k

01:25:26,239 --> 01:25:30,960
so when you send the buffers up through

01:25:28,159 --> 01:25:33,520
the stack it has an elevated rough count

01:25:30,960 --> 01:25:35,840
and then after it's deposited and it

01:25:33,520 --> 01:25:38,480
goes to page 3 and notices

01:25:35,840 --> 01:25:40,400
the elevated draft count and notices it

01:25:38,480 --> 01:25:42,880
belongs to the page pool

01:25:40,400 --> 01:25:43,600
and is then either returned to the

01:25:42,880 --> 01:25:46,480
driver

01:25:43,600 --> 01:25:48,400
or returned to the page pool so we

01:25:46,480 --> 01:25:51,920
preserve the iomu

01:25:48,400 --> 01:25:54,960
unmappings throughout the our expert

01:25:51,920 --> 01:25:55,679
that's working on our facebook external

01:25:54,960 --> 01:26:00,000
server

01:25:55,679 --> 01:26:04,000
no problem with that um what we did find

01:26:00,000 --> 01:26:06,639
is this only works for the rx path

01:26:04,000 --> 01:26:07,280
so since we're using the networking

01:26:06,639 --> 01:26:09,520
stack

01:26:07,280 --> 01:26:10,960
half of the pages are coming from the

01:26:09,520 --> 01:26:13,520
transmit side

01:26:10,960 --> 01:26:15,440
which are always hitting the iommu

01:26:13,520 --> 01:26:16,719
obviously because they can't go to the

01:26:15,440 --> 01:26:18,960
peach pool

01:26:16,719 --> 01:26:20,840
so that kind of kills the performance

01:26:18,960 --> 01:26:23,600
things that we're seeing

01:26:20,840 --> 01:26:25,280
there yeah but a similar technique could

01:26:23,600 --> 01:26:27,440
be applied on the tx path right i mean

01:26:25,280 --> 01:26:28,960
the the only the only

01:26:27,440 --> 01:26:30,239
difference we have on the i'm trying to

01:26:28,960 --> 01:26:30,560
think if you can do the same thing on

01:26:30,239 --> 01:26:33,520
the

01:26:30,560 --> 01:26:34,880
tx path because the only fundamental

01:26:33,520 --> 01:26:36,639
difference between the rx path and the

01:26:34,880 --> 01:26:38,560
tx path is that on the text path you

01:26:36,639 --> 01:26:42,880
allocate the skb on the fly from the

01:26:38,560 --> 01:26:44,800
from the uh from the slab and on the rx

01:26:42,880 --> 01:26:45,679
path you just have pre-allocated pages

01:26:44,800 --> 01:26:48,719
from page pool

01:26:45,679 --> 01:26:49,760
so if you could allocate we would have

01:26:48,719 --> 01:26:52,800
to change

01:26:49,760 --> 01:26:54,159
things so that the transmit path would

01:26:52,800 --> 01:26:56,960
get pages from

01:26:54,159 --> 01:27:00,239
a page full somehow and they don't think

01:26:56,960 --> 01:27:03,760
anyone's done that i certainly haven't

01:27:00,239 --> 01:27:04,400
yeah i can i think on txpath you don't

01:27:03,760 --> 01:27:06,080
know

01:27:04,400 --> 01:27:08,480
which net device you're bounded to

01:27:06,080 --> 01:27:10,800
allocate from

01:27:08,480 --> 01:27:12,560
you should do it as early as possible to

01:27:10,800 --> 01:27:13,760
allocate the buffer and copy them from

01:27:12,560 --> 01:27:19,840
the skb

01:27:13,760 --> 01:27:19,840
and from the socket buffer

01:27:21,120 --> 01:27:24,480
some of the game you can get from page 4

01:27:23,120 --> 01:27:26,880
is because it knows

01:27:24,480 --> 01:27:27,920
it doesn't have to lock anything because

01:27:26,880 --> 01:27:30,000
it knows that

01:27:27,920 --> 01:27:32,320
this this comes in from a hardware

01:27:30,000 --> 01:27:35,120
request and and it's it's

01:27:32,320 --> 01:27:36,159
uh protected by the soft iq and the tx

01:27:35,120 --> 01:27:38,639
case we don't have

01:27:36,159 --> 01:27:39,600
have this ability so you need to take

01:27:38,639 --> 01:27:41,920
some locks and

01:27:39,600 --> 01:27:43,360
all of a sudden it gets more expensive

01:27:41,920 --> 01:27:44,800
so yeah

01:27:43,360 --> 01:27:46,560
no i'm not i'm not implying this will

01:27:44,800 --> 01:27:49,280
have the same problem

01:27:46,560 --> 01:27:49,760
because when you come back in through

01:27:49,280 --> 01:27:53,520
the

01:27:49,760 --> 01:27:55,679
page three that doesn't go through the

01:27:53,520 --> 01:27:57,360
hot path to the page pool it doesn't go

01:27:55,679 --> 01:27:59,120
through the nappy protection so you

01:27:57,360 --> 01:28:02,560
always take the lock

01:27:59,120 --> 01:28:05,440
where you put it in the general ring

01:28:02,560 --> 01:28:06,560
so that was another um performance

01:28:05,440 --> 01:28:09,679
impact there

01:28:06,560 --> 01:28:11,280
that means your problems should be fixed

01:28:09,679 --> 01:28:12,880
yeah that's also that that's definitely

01:28:11,280 --> 01:28:13,600
something that we need to fix somehow

01:28:12,880 --> 01:28:15,840
but

01:28:13,600 --> 01:28:19,040
i have a lot of ideas but not enough

01:28:15,840 --> 01:28:19,040
time to implement them

01:28:19,280 --> 01:28:25,040
in any case just the i find a bsd

01:28:22,480 --> 01:28:26,239
page model where you have a per cpu

01:28:25,040 --> 01:28:28,000
allocator

01:28:26,239 --> 01:28:29,679
but that really doesn't get anywhere

01:28:28,000 --> 01:28:32,880
probably wasn't using the large enough

01:28:29,679 --> 01:28:35,920
magazine size to handle the traffic

01:28:32,880 --> 01:28:42,960
but you need a pair cpu and pair driver

01:28:35,920 --> 01:28:46,080
for the dma mappings

01:28:42,960 --> 01:28:46,800
you can have a purple driver i'm not so

01:28:46,080 --> 01:28:50,320
sure why

01:28:46,800 --> 01:28:50,320
it has to be pearl cpu though

01:28:51,040 --> 01:29:00,480
well to avoid locking

01:28:56,880 --> 01:29:00,480
yeah that okay

01:29:00,880 --> 01:29:05,760
there are some spatulas you can use

01:29:02,320 --> 01:29:05,760
without locking but yeah

01:29:07,280 --> 01:29:12,639
not when multiple cpus are touching

01:29:09,760 --> 01:29:12,639
these structures

01:29:14,400 --> 01:29:18,239
i mean a consumer producer i don't think

01:29:16,880 --> 01:29:21,760
it

01:29:18,239 --> 01:29:24,000
can work that way

01:29:21,760 --> 01:29:24,960
the main idea here is that this is

01:29:24,000 --> 01:29:27,040
easily black

01:29:24,960 --> 01:29:28,719
plugable on devices that support xdp

01:29:27,040 --> 01:29:31,040
because you're supposed to use page pull

01:29:28,719 --> 01:29:33,920
already when you're using xdp

01:29:31,040 --> 01:29:35,199
uh for your for your buffer allocation

01:29:33,920 --> 01:29:37,520
so since you're using

01:29:35,199 --> 01:29:38,639
page pool already for xdp why don't just

01:29:37,520 --> 01:29:41,760
allocate the skb

01:29:38,639 --> 01:29:43,360
data as well with that and if we manage

01:29:41,760 --> 01:29:44,800
to apply that and figure out that it's

01:29:43,360 --> 01:29:46,320
safe

01:29:44,800 --> 01:29:49,120
you can just get the performance for

01:29:46,320 --> 01:29:50,800
free because

01:29:49,120 --> 01:29:53,199
keep in mind these results are without

01:29:50,800 --> 01:29:56,080
an imu if an immune kicks in this is

01:29:53,199 --> 01:30:01,199
going to get even worse

01:29:56,080 --> 01:30:03,840
yeah i understand the need just

01:30:01,199 --> 01:30:05,280
for tx it's harder to achieve than rx

01:30:03,840 --> 01:30:08,080
and for rx i believe

01:30:05,280 --> 01:30:10,719
we have issues to resolve like in mx4

01:30:08,080 --> 01:30:12,639
and mx5 i think we have the flip page so

01:30:10,719 --> 01:30:16,639
the same page could be used for two

01:30:12,639 --> 01:30:19,679
skbs and uh with that enabled

01:30:16,639 --> 01:30:21,360
to you're just uh

01:30:19,679 --> 01:30:22,880
yeah yeah if you go back to the the

01:30:21,360 --> 01:30:23,600
notes i've sent you this is this is

01:30:22,880 --> 01:30:25,679
mentioned

01:30:23,600 --> 01:30:27,440
can you uh go back to the note so i

01:30:25,679 --> 01:30:29,199
won't we won't miss anything at least

01:30:27,440 --> 01:30:32,560
from what i had in mind

01:30:29,199 --> 01:30:34,560
right so the problem is the split page

01:30:32,560 --> 01:30:37,760
memory model

01:30:34,560 --> 01:30:39,600
yes desperate how the i i don't know if

01:30:37,760 --> 01:30:40,639
he wants to elaborate on the proposal he

01:30:39,600 --> 01:30:42,400
quickly said

01:30:40,639 --> 01:30:45,280
he quickly once we discussed this he had

01:30:42,400 --> 01:30:45,280
an interesting idea

01:30:45,440 --> 01:30:49,840
i did i just said i'm not sure it's a

01:30:47,280 --> 01:30:53,040
good idea to have a smooth edge

01:30:49,840 --> 01:30:56,080
memory model and if and and and

01:30:53,040 --> 01:30:56,320
i don't like that if you try to shoehorn

01:30:56,080 --> 01:30:58,719
in

01:30:56,320 --> 01:30:59,760
a split page memory model for for the

01:30:58,719 --> 01:31:03,120
page pool

01:30:59,760 --> 01:31:06,159
i would rather have we we add a new

01:31:03,120 --> 01:31:06,719
memory type that that we can identify

01:31:06,159 --> 01:31:10,400
because

01:31:06,719 --> 01:31:12,080
we added in the the

01:31:10,400 --> 01:31:13,679
the memory type so we can have another

01:31:12,080 --> 01:31:16,800
memory type that

01:31:13,679 --> 01:31:20,000
that supports this more explicitly

01:31:16,800 --> 01:31:22,639
but and i think we can still share some

01:31:20,000 --> 01:31:24,719
code with the page pool actually but

01:31:22,639 --> 01:31:25,920
maybe it's worth having another memory

01:31:24,719 --> 01:31:28,880
type if we have this

01:31:25,920 --> 01:31:29,440
the split page stuff instead of trying

01:31:28,880 --> 01:31:32,480
to shoot

01:31:29,440 --> 01:31:36,560
and hold it into a page pool but that's

01:31:32,480 --> 01:31:36,560
i think it's down to the coding details

01:31:38,719 --> 01:31:43,679
you can use the page pool with a split

01:31:41,679 --> 01:31:46,800
page memory model

01:31:43,679 --> 01:31:49,120
the issue is that the driver has to be

01:31:46,800 --> 01:31:50,480
responsible to maintain the elevated

01:31:49,120 --> 01:31:53,520
reference accounts

01:31:50,480 --> 01:31:53,520
not the page pool

01:31:54,239 --> 01:32:00,639
yes yeah that could work easily for xdp

01:31:58,320 --> 01:32:04,000
workloads like xtp drop and tx

01:32:00,639 --> 01:32:10,080
but for stack pass exp pass

01:32:04,000 --> 01:32:10,080
uh the last one to to free this is kb

01:32:10,840 --> 01:32:15,520
pipe

01:32:13,360 --> 01:32:17,199
but what is some something else elevates

01:32:15,520 --> 01:32:20,080
the rift count and

01:32:17,199 --> 01:32:22,239
it's not the last the page bull call is

01:32:20,080 --> 01:32:27,840
not the last one to decrement it

01:32:22,239 --> 01:32:27,840
then you miss a dma mapping

01:32:33,450 --> 01:32:36,609
[Music]

01:32:39,600 --> 01:32:43,920
so that's essentially the driver holds a

01:32:42,000 --> 01:32:45,040
certain number of rough counts as visual

01:32:43,920 --> 01:32:47,600
references

01:32:45,040 --> 01:32:48,800
you send the fragment up the stack and

01:32:47,600 --> 01:32:52,159
the fragment

01:32:48,800 --> 01:32:54,400
uh normally just release page

01:32:52,159 --> 01:32:55,520
and because the is holding elevated

01:32:54,400 --> 01:32:57,199
rough cuts

01:32:55,520 --> 01:32:59,520
it doesn't actually go back to the

01:32:57,199 --> 01:33:03,520
system when the driver goes

01:32:59,520 --> 01:33:06,880
to reuse the fragment again

01:33:03,520 --> 01:33:09,040
and notices that the reference counts on

01:33:06,880 --> 01:33:10,639
the page are higher than the rough camps

01:33:09,040 --> 01:33:13,040
that it still holds in

01:33:10,639 --> 01:33:14,239
at that point it was just the recycling

01:33:13,040 --> 01:33:17,440
from the driver

01:33:14,239 --> 01:33:19,120
which is from the driver so they can our

01:33:17,440 --> 01:33:21,040
page can be released from the driver

01:33:19,120 --> 01:33:22,880
back to the pool or from the system back

01:33:21,040 --> 01:33:26,080
to the pool and that works

01:33:22,880 --> 01:33:27,679
uh but my point is is that the driver

01:33:26,080 --> 01:33:29,840
still has to be responsible for

01:33:27,679 --> 01:33:33,280
maintaining this elevated rough camp we

01:33:29,840 --> 01:33:33,280
can't push it off to the page

01:33:34,320 --> 01:33:39,760
i don't know that's that's that's true

01:33:37,679 --> 01:33:41,360
but that's also why i think maybe we

01:33:39,760 --> 01:33:44,800
should be explicit about it and

01:33:41,360 --> 01:33:48,560
and put another another type on it so

01:33:44,800 --> 01:33:50,239
so the code can can tell the difference

01:33:48,560 --> 01:33:52,480
when when you do this extra elevated

01:33:50,239 --> 01:33:54,400
riff count what happens is that you have

01:33:52,480 --> 01:33:56,320
to touch the ref counts in the in the

01:33:54,400 --> 01:33:58,719
driver right

01:33:56,320 --> 01:33:59,360
and one of the key elements of the page

01:33:58,719 --> 01:34:01,040
pool is that

01:33:59,360 --> 01:34:02,400
you don't have you only have to read it

01:34:01,040 --> 01:34:04,080
on the remote cpu if it gets

01:34:02,400 --> 01:34:05,920
freaking remote you don't have to update

01:34:04,080 --> 01:34:09,600
it actually if you notice

01:34:05,920 --> 01:34:10,960
it it has it can only be one

01:34:09,600 --> 01:34:12,159
we've never thought about this before

01:34:10,960 --> 01:34:13,840
i've never thought about it before what

01:34:12,159 --> 01:34:17,120
if we added a new flag to the

01:34:13,840 --> 01:34:18,080
to the page um memory allocation

01:34:17,120 --> 01:34:19,520
structures that

01:34:18,080 --> 01:34:21,120
said you know the page was allocated

01:34:19,520 --> 01:34:23,520
with the dma mapping

01:34:21,120 --> 01:34:25,360
and then let the page allocator free the

01:34:23,520 --> 01:34:26,800
dma mapping as part of the free pass if

01:34:25,360 --> 01:34:29,280
it ever sees the ref count and go

01:34:26,800 --> 01:34:33,280
to zero well we have the dma mapping

01:34:29,280 --> 01:34:35,360
construct page

01:34:33,280 --> 01:34:36,800
so i know but does the page fool know

01:34:35,360 --> 01:34:38,800
about that

01:34:36,800 --> 01:34:40,239
the page pull out that but it doesn't

01:34:38,800 --> 01:34:41,679
piggyback on it to free that's an

01:34:40,239 --> 01:34:42,960
interesting idea that's that's all i'm

01:34:41,679 --> 01:34:45,760
saying

01:34:42,960 --> 01:34:47,920
yeah but i think for to call to do the

01:34:45,760 --> 01:34:51,360
ldma on my call you need the

01:34:47,920 --> 01:34:53,600
the device pointer also

01:34:51,360 --> 01:34:55,040
and that's that is stored in the page

01:34:53,600 --> 01:35:00,639
pool objects

01:34:55,040 --> 01:35:06,000
and that's how we get it back

01:35:00,639 --> 01:35:06,000
but you could it is sort of possible

01:35:06,320 --> 01:35:10,639
yeah as long as you can figure out the

01:35:07,679 --> 01:35:12,080
device and the we we have we don't

01:35:10,639 --> 01:35:13,280
we don't have the direction as well

01:35:12,080 --> 01:35:15,119
because you know you don't need the

01:35:13,280 --> 01:35:17,679
direction don't map

01:35:15,119 --> 01:35:18,400
you just need the device yeah that that

01:35:17,679 --> 01:35:20,639
might

01:35:18,400 --> 01:35:20,639
work

01:35:22,159 --> 01:35:26,080
it might work because because we made

01:35:24,560 --> 01:35:26,480
made to change that you make sure that

01:35:26,080 --> 01:35:29,280
the

01:35:26,480 --> 01:35:31,840
page pull id is still up and running to

01:35:29,280 --> 01:35:35,040
the pageable pointers to advance this

01:35:31,840 --> 01:35:36,719
yeah like jonathan made me change

01:35:35,040 --> 01:35:38,320
some some things to keep track of

01:35:36,719 --> 01:35:39,679
in-flight packets and that actually

01:35:38,320 --> 01:35:41,679
means that we can actually speed up

01:35:39,679 --> 01:35:42,320
freeing of the pages quite significantly

01:35:41,679 --> 01:35:45,920
by

01:35:42,320 --> 01:35:46,239
by using the directly accessing the page

01:35:45,920 --> 01:35:48,719
pro

01:35:46,239 --> 01:35:48,719
objects

01:35:49,520 --> 01:35:52,719
yeah pointer instead of having to do a

01:35:51,119 --> 01:35:57,280
lookup table but that's

01:35:52,719 --> 01:35:59,440
also coding details

01:35:57,280 --> 01:36:02,880
okay uh guys just a time check we only

01:35:59,440 --> 01:36:05,920
have 12 minutes left and i would like to

01:36:02,880 --> 01:36:07,440
have some free time for uh open

01:36:05,920 --> 01:36:10,080
discussion

01:36:07,440 --> 01:36:12,239
um jonathan just let me know if you want

01:36:10,080 --> 01:36:15,600
to talk about your xero copy

01:36:12,239 --> 01:36:17,760
there's split uh patches and new gpu

01:36:15,600 --> 01:36:19,920
stuff

01:36:17,760 --> 01:36:22,080
um open up to questions i don't have

01:36:19,920 --> 01:36:22,800
slides for an extra view so i could

01:36:22,080 --> 01:36:28,320
always just

01:36:22,800 --> 01:36:28,320
push from that if you want essentially

01:36:28,400 --> 01:36:31,679
open up questions

01:36:33,360 --> 01:36:42,480
okay so

01:36:39,440 --> 01:36:47,040
so we're here in the opening discussion

01:36:42,480 --> 01:36:49,360
and uh we got uh 12 minutes left so

01:36:47,040 --> 01:36:51,360
there are some topics to discuss uh

01:36:49,360 --> 01:36:53,840
basically we covered some of these

01:36:51,360 --> 01:36:53,840
and

01:36:55,119 --> 01:36:59,040
i would like to actually hear some

01:36:56,800 --> 01:36:59,920
comments or open issues that we didn't

01:36:59,040 --> 01:37:07,520
cover today

01:36:59,920 --> 01:37:09,040
so it's a time to raise them up

01:37:07,520 --> 01:37:11,280
i see a nice overlap for the

01:37:09,040 --> 01:37:12,880
multi-buffer stuff with the header data

01:37:11,280 --> 01:37:14,800
split

01:37:12,880 --> 01:37:16,000
um functionality those two can be

01:37:14,800 --> 01:37:18,000
combined

01:37:16,000 --> 01:37:19,679
um like if the multibuffer goes in in

01:37:18,000 --> 01:37:21,280
some way shape or form it would give us

01:37:19,679 --> 01:37:22,480
the ability to use header data split

01:37:21,280 --> 01:37:24,880
with xtp

01:37:22,480 --> 01:37:24,880
traffic

01:37:26,000 --> 01:37:30,639
oh right but i basically put the header

01:37:29,040 --> 01:37:31,360
that i split here because i wanted to

01:37:30,639 --> 01:37:34,239
talk

01:37:31,360 --> 01:37:35,199
api like how do you allow do you do it

01:37:34,239 --> 01:37:37,280
per queue

01:37:35,199 --> 01:37:39,280
how the user should ask to enable

01:37:37,280 --> 01:37:39,920
generator split because legacy hardware

01:37:39,280 --> 01:37:42,639
today

01:37:39,920 --> 01:37:43,920
most of them they don't know how to do

01:37:42,639 --> 01:37:47,679
data split they

01:37:43,920 --> 01:37:49,920
only know how to uh data split not

01:37:47,679 --> 01:37:54,159
according to specific header but

01:37:49,920 --> 01:37:56,719
just by fixed size uh header

01:37:54,159 --> 01:37:59,119
so we're missing some apis for this it's

01:37:56,719 --> 01:38:00,000
a low hanging fruit for many use cases

01:37:59,119 --> 01:38:03,040
like

01:38:00,000 --> 01:38:06,880
what jonathan is working on

01:38:03,040 --> 01:38:09,199
gpu zero copy yeah the intel hardware

01:38:06,880 --> 01:38:12,800
has a whole bunch of controls over this

01:38:09,199 --> 01:38:14,719
um we can support header data split on

01:38:12,800 --> 01:38:16,719
like four different modes there's the

01:38:14,719 --> 01:38:20,080
you know fixed split the

01:38:16,719 --> 01:38:22,320
header only split header replication

01:38:20,080 --> 01:38:24,080
right et cetera so you're right there is

01:38:22,320 --> 01:38:24,880
no api for configuring this stuff for

01:38:24,080 --> 01:38:27,760
our hardware

01:38:24,880 --> 01:38:30,000
and we can do it per queue although we

01:38:27,760 --> 01:38:33,600
would have to re-set the

01:38:30,000 --> 01:38:33,600
queue when you turned it on or off

01:38:34,080 --> 01:38:38,480
yeah i think it's a static configuration

01:38:36,000 --> 01:38:40,080
so i don't think anyone would mind if

01:38:38,480 --> 01:38:44,400
you reset the queue

01:38:40,080 --> 01:38:46,000
so what do you think well

01:38:44,400 --> 01:38:48,639
what do you think for interface ethool

01:38:46,000 --> 01:38:48,639
or devlink

01:38:48,960 --> 01:38:52,719
well neither well i mean we've got your

01:38:51,840 --> 01:38:54,560
senses

01:38:52,719 --> 01:38:56,400
um this is part of the work that i'm

01:38:54,560 --> 01:38:58,480
doing for a net gpu

01:38:56,400 --> 01:39:00,000
um in background this explains what i

01:38:58,480 --> 01:39:02,960
was trying to accomplish

01:39:00,000 --> 01:39:04,080
for a net gpu the initial goal which we

01:39:02,960 --> 01:39:06,880
have this huge

01:39:04,080 --> 01:39:08,320
machine learning cluster where we have

01:39:06,880 --> 01:39:10,480
incoming traffic

01:39:08,320 --> 01:39:12,239
and we want most of the data is going to

01:39:10,480 --> 01:39:14,480
go through the gpu

01:39:12,239 --> 01:39:15,440
now we don't fulfill this reasons we

01:39:14,480 --> 01:39:18,639
don't want to use

01:39:15,440 --> 01:39:21,280
rdma so what we'd like to do is keep all

01:39:18,639 --> 01:39:23,119
the protocol processing in the host but

01:39:21,280 --> 01:39:24,320
still there's no reason to send all the

01:39:23,119 --> 01:39:25,920
data to the host

01:39:24,320 --> 01:39:28,480
since it doesn't use that it's going to

01:39:25,920 --> 01:39:29,520
the gpu so once you split the headers to

01:39:28,480 --> 01:39:31,920
go to the host

01:39:29,520 --> 01:39:34,960
you can do the protocol processing and

01:39:31,920 --> 01:39:38,080
send all the data to the gpu directly

01:39:34,960 --> 01:39:40,320
right that was the initial motivation so

01:39:38,080 --> 01:39:42,000
we want to change the driver to do some

01:39:40,320 --> 01:39:45,360
kind of header splitting

01:39:42,000 --> 01:39:48,159
so we do split the buffer for

01:39:45,360 --> 01:39:50,080
the initial receive buffer will go into

01:39:48,159 --> 01:39:54,000
a buffer map to the host

01:39:50,080 --> 01:39:55,679
and all the other receipts for the data

01:39:54,000 --> 01:39:57,920
section of the packet we're going to

01:39:55,679 --> 01:40:01,679
buffer several map to the gpu

01:39:57,920 --> 01:40:04,400
or even an mdme device in the future

01:40:01,679 --> 01:40:05,679
so that's what we're trying to do here

01:40:04,400 --> 01:40:09,520
we found that

01:40:05,679 --> 01:40:13,040
the xcpas60p this is very similar to

01:40:09,520 --> 01:40:15,600
how afxdp works except the af xdp

01:40:13,040 --> 01:40:18,000
bypasses the entire protocol step it

01:40:15,600 --> 01:40:20,880
throws the baby out with the bathwater

01:40:18,000 --> 01:40:23,040
so to speak so what we want to do is do

01:40:20,880 --> 01:40:25,679
the same thing once you've seen

01:40:23,040 --> 01:40:27,360
the everything through the tcp staff or

01:40:25,679 --> 01:40:30,000
the udp staff

01:40:27,360 --> 01:40:32,800
and then data arrives in the form of the

01:40:30,000 --> 01:40:35,360
same afx cp form it'll just be

01:40:32,800 --> 01:40:36,000
written through user space ring they'll

01:40:35,360 --> 01:40:37,920
notify

01:40:36,000 --> 01:40:39,280
oh the data is here but now they may not

01:40:37,920 --> 01:40:42,159
be your girlfriend they treat

01:40:39,280 --> 01:40:44,159
you but you can still control it well

01:40:42,159 --> 01:40:47,440
that's essentially the genesis

01:40:44,159 --> 01:40:50,239
of what nutrients you're supposed to do

01:40:47,440 --> 01:40:51,920
to implement this the kind of extended

01:40:50,239 --> 01:40:54,880
some of the afx dp

01:40:51,920 --> 01:40:55,920
concepts so we have a circuit urine

01:40:54,880 --> 01:40:59,600
interface q

01:40:55,920 --> 01:41:02,719
and memory model current rss contact

01:40:59,600 --> 01:41:05,760
and so we change the mlx5 driver

01:41:02,719 --> 01:41:08,400
so when i bind to an interface

01:41:05,760 --> 01:41:10,560
i bind to a device that actually creates

01:41:08,400 --> 01:41:14,000
a new received queue

01:41:10,560 --> 01:41:16,159
uh kind of like the afx cpcq

01:41:14,000 --> 01:41:17,440
and then sets header split properties

01:41:16,159 --> 01:41:19,520
just on that shoe

01:41:17,440 --> 01:41:20,800
without changing anything else on the

01:41:19,520 --> 01:41:24,320
bus the

01:41:20,800 --> 01:41:27,520
current received juice so that's

01:41:24,320 --> 01:41:27,520
possible to do now

01:41:31,119 --> 01:41:35,199
and uh can you explain how you did the

01:41:33,440 --> 01:41:37,679
configuration for the data split

01:41:35,199 --> 01:41:38,560
what api did you use to configure the

01:41:37,679 --> 01:41:40,960
queue

01:41:38,560 --> 01:41:43,920
according to the data headers that you

01:41:40,960 --> 01:41:43,920
were looking for

01:41:44,480 --> 01:41:48,719
um for right now i'm trying to keep

01:41:46,480 --> 01:41:51,760
things as automatic as possible

01:41:48,719 --> 01:41:55,040
so right now the api is that

01:41:51,760 --> 01:41:58,080
the user space request in

01:41:55,040 --> 01:42:01,280
interface queue essentially we have a

01:41:58,080 --> 01:42:03,840
kind of like a rss context

01:42:01,280 --> 01:42:05,679
so we request an interface queue from

01:42:03,840 --> 01:42:06,400
the device that will be attached to the

01:42:05,679 --> 01:42:10,080
services

01:42:06,400 --> 01:42:13,840
contact right now the only api

01:42:10,080 --> 01:42:15,920
does someone has that which requires a

01:42:13,840 --> 01:42:19,280
header splitting on that

01:42:15,920 --> 01:42:20,560
uh received to in the future i think

01:42:19,280 --> 01:42:22,719
ambitions

01:42:20,560 --> 01:42:24,320
um i'm only trying to do what's needed

01:42:22,719 --> 01:42:24,960
right now instead of trying to come up

01:42:24,320 --> 01:42:28,320
with

01:42:24,960 --> 01:42:30,960
everything as a api

01:42:28,320 --> 01:42:31,920
but in the future i can see the request

01:42:30,960 --> 01:42:34,800
to the device

01:42:31,920 --> 01:42:36,400
for which you could be buying to a

01:42:34,800 --> 01:42:39,040
specific cpu

01:42:36,400 --> 01:42:41,360
bind to a nappy do other things but

01:42:39,040 --> 01:42:44,000
that's currently not implemented

01:42:41,360 --> 01:42:46,400
we do right now is we simply requesting

01:42:44,000 --> 01:42:48,480
the receive queue

01:42:46,400 --> 01:42:49,440
from the hardware that's doing header

01:42:48,480 --> 01:42:52,239
splits

01:42:49,440 --> 01:42:54,840
and then repopulate it with some memory

01:42:52,239 --> 01:42:57,840
specific memory that we want to receive

01:42:54,840 --> 01:42:57,840
into

01:43:00,480 --> 01:43:06,960
okay so uh i believe for afxdp

01:43:04,000 --> 01:43:07,679
uh api could be as easy as its socket

01:43:06,960 --> 01:43:11,360
options

01:43:07,679 --> 01:43:15,280
because hi fxdp socket has

01:43:11,360 --> 01:43:18,320
its own q inside the device

01:43:15,280 --> 01:43:20,000
and uh using set socket options you can

01:43:18,320 --> 01:43:21,360
configure whatever attribute you're

01:43:20,000 --> 01:43:24,719
looking for and uh

01:43:21,360 --> 01:43:29,840
we can make it standardize because the

01:43:24,719 --> 01:43:29,840
fxdp is a well-known socket and

01:43:30,239 --> 01:43:34,639
we can add to that api

01:43:34,719 --> 01:43:41,679
right app here

01:43:38,800 --> 01:43:42,719
uh it's kind of a mess but initial

01:43:41,679 --> 01:43:46,480
design was you

01:43:42,719 --> 01:43:50,239
open an afx cp socket and that

01:43:46,480 --> 01:43:52,880
kind of equals to an interface q

01:43:50,239 --> 01:43:55,520
we have uh extinct trends where you can

01:43:52,880 --> 01:43:57,199
open in multiple afx cp sockets and they

01:43:55,520 --> 01:44:00,560
share the same interface

01:43:57,199 --> 01:44:03,920
too that's kind of messy so in

01:44:00,560 --> 01:44:06,239
my gpu redress we have multiple

01:44:03,920 --> 01:44:07,280
protocol sockets that can share an

01:44:06,239 --> 01:44:10,400
interface to

01:44:07,280 --> 01:44:14,159
what we question and fix q specifically

01:44:10,400 --> 01:44:16,639
i was initially hoping to just use the

01:44:14,159 --> 01:44:19,840
asxcp api

01:44:16,639 --> 01:44:21,679
so it's currently not done i'm hoping in

01:44:19,840 --> 01:44:22,320
the future we can merge these two

01:44:21,679 --> 01:44:26,800
together

01:44:22,320 --> 01:44:26,800
so we don't have to separate apis

01:44:27,600 --> 01:44:30,800
right we can take the common parts and

01:44:29,920 --> 01:44:34,000
make it

01:44:30,800 --> 01:44:38,480
an abstract layer for both the fxdp and

01:44:34,000 --> 01:44:41,600
gpu but uh well i want to go back to

01:44:38,480 --> 01:44:45,119
just his comment here regarding the api

01:44:41,600 --> 01:44:48,800
uh because so uh as said

01:44:45,119 --> 01:44:52,800
uh header data split

01:44:48,800 --> 01:44:52,800
and the multi-buffer

01:44:53,920 --> 01:45:01,440
they go together and uh i think

01:44:58,320 --> 01:45:04,159
api is required to figure the

01:45:01,440 --> 01:45:05,760
standard net device queues to do hello

01:45:04,159 --> 01:45:10,000
data split and to do

01:45:05,760 --> 01:45:12,159
multibuffer in order to achieve dso gro

01:45:10,000 --> 01:45:14,480
whatever we want with the header that is

01:45:12,159 --> 01:45:14,480
played

01:45:14,719 --> 01:45:18,639
so again this all boils down to the uh

01:45:17,679 --> 01:45:21,199
device cues

01:45:18,639 --> 01:45:22,880
discussion where we want to make a new

01:45:21,199 --> 01:45:26,000
device queue is more controllable

01:45:22,880 --> 01:45:27,600
and in terms of api i think the proposed

01:45:26,000 --> 01:45:30,639
api was netlink

01:45:27,600 --> 01:45:33,440
and uh i support that decision and

01:45:30,639 --> 01:45:34,080
uh we need to try to push that forward

01:45:33,440 --> 01:45:36,560
as well

01:45:34,080 --> 01:45:39,920
so the answers should be init link and

01:45:36,560 --> 01:45:39,920
using new device keywords

01:45:40,560 --> 01:45:48,560
as a main object to manipulate and

01:45:45,280 --> 01:45:49,119
configure for multi-buffer either data

01:45:48,560 --> 01:45:52,400
split

01:45:49,119 --> 01:45:54,719
and attach to afx dp sockets uh tx

01:45:52,400 --> 01:45:57,360
keywords xdp redirect

01:45:54,719 --> 01:45:59,199
and give the user the full control that

01:45:57,360 --> 01:46:00,719
would solve many issues that were raised

01:45:59,199 --> 01:46:03,119
today in this uh

01:46:00,719 --> 01:46:03,119
session

01:46:05,119 --> 01:46:08,400
um i think when it resurrected that

01:46:07,440 --> 01:46:11,760
discussion

01:46:08,400 --> 01:46:13,600
i think it was a year ago a half a year

01:46:11,760 --> 01:46:17,840
ago

01:46:13,600 --> 01:46:17,840
and it's time to make it happen

01:46:18,400 --> 01:46:25,119
um but

01:46:21,440 --> 01:46:28,400
i believe right now the q trajectory

01:46:25,119 --> 01:46:32,400
is done via

01:46:28,400 --> 01:46:32,400
an ndo bps car

01:46:33,119 --> 01:46:38,960
uh be nice to have a better way of

01:46:35,760 --> 01:46:41,679
creating and manipulating fuse than that

01:46:38,960 --> 01:46:42,239
right so it's tailored to the use case

01:46:41,679 --> 01:46:45,840
right

01:46:42,239 --> 01:46:46,800
and in your code you just replicated the

01:46:45,840 --> 01:46:49,119
afx dp

01:46:46,800 --> 01:46:51,280
code to your own use case and uh there

01:46:49,119 --> 01:46:52,560
are a lot of common stuff

01:46:51,280 --> 01:46:54,800
and i think we need to make the

01:46:52,560 --> 01:46:57,119
networking queue as as it

01:46:54,800 --> 01:46:57,920
will say the first class that is inside

01:46:57,119 --> 01:47:00,880
then it

01:46:57,920 --> 01:47:01,280
inside the kernel and you can create one

01:47:00,880 --> 01:47:03,440
give it

01:47:01,280 --> 01:47:04,320
attribute and attach it to whatever

01:47:03,440 --> 01:47:07,520
application

01:47:04,320 --> 01:47:11,119
uh socket uh net device cues or

01:47:07,520 --> 01:47:13,280
hardware irq queues

01:47:11,119 --> 01:47:14,320
just give the user the full ability and

01:47:13,280 --> 01:47:17,360
control

01:47:14,320 --> 01:47:20,159
to put it and attach it to whatever the

01:47:17,360 --> 01:47:20,159
user wants

01:47:20,960 --> 01:47:29,119
um okay we have one minute left to go

01:47:25,920 --> 01:47:32,960
uh any questions

01:47:29,119 --> 01:47:34,960
uh comments so i have a

01:47:32,960 --> 01:47:36,639
question for jonathan regarding the net

01:47:34,960 --> 01:47:40,080
gpu patches

01:47:36,639 --> 01:47:40,800
so is that targeted mainly for gpu only

01:47:40,080 --> 01:47:42,880
or

01:47:40,800 --> 01:47:43,920
i thought i think your patches also

01:47:42,880 --> 01:47:47,199
supported

01:47:43,920 --> 01:47:49,840
tx rx zero copy to the host applications

01:47:47,199 --> 01:47:49,840
also right

01:47:51,760 --> 01:47:57,679
um zero copy to the host memory as well

01:47:58,239 --> 01:48:02,320
okay so it's the plan to support that

01:48:00,719 --> 01:48:04,719
mode also

01:48:02,320 --> 01:48:05,360
right no it does support this model

01:48:04,719 --> 01:48:09,040
already

01:48:05,360 --> 01:48:10,880
the support directly for um the nvidia

01:48:09,040 --> 01:48:13,520
show people not going to mix in but also

01:48:10,880 --> 01:48:16,800
support zero copy to host memory

01:48:13,520 --> 01:48:18,800
and that's working today

01:48:16,800 --> 01:48:19,920
someone currently working on extending

01:48:18,800 --> 01:48:23,520
udp

01:48:19,920 --> 01:48:27,840
to support zero copy udpvc

01:48:23,520 --> 01:48:31,119
if anyone feels okay thank you johnston

01:48:27,840 --> 01:48:33,280
and um case there are no more questions

01:48:31,119 --> 01:48:36,400
or comments um

01:48:33,280 --> 01:48:38,560
i would like to conclude here okay so

01:48:36,400 --> 01:48:40,800
thanks everyone for joining it was a

01:48:38,560 --> 01:48:43,840
really helpful and fruitful

01:48:40,800 --> 01:48:46,480
discussions today uh we got a lot to

01:48:43,840 --> 01:48:47,440
cover and we didn't cover also a lot of

01:48:46,480 --> 01:48:50,480
uh

01:48:47,440 --> 01:48:54,080
race issues um so

01:48:50,480 --> 01:48:57,119
thanks for joining and thanks for the

01:48:54,080 --> 01:48:58,639
fan members for sharing their work and

01:48:57,119 --> 01:49:02,080
thanks for the organizers

01:48:58,639 --> 01:49:09,360
for this great conference

01:49:02,080 --> 01:49:09,360

YouTube URL: https://www.youtube.com/watch?v=vMk0UWFBBlk


