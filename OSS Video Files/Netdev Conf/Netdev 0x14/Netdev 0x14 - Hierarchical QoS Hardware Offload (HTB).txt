Title: Netdev 0x14 - Hierarchical QoS Hardware Offload (HTB)
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Yosef Kuperman, Rony Efraim, Maxim Mikityanskiy

More info: https://netdevconf.info/0x14/session.html?talk-hierarchical-QoS-hardware-offload

Date: Thursday, August 20, 2020

Hierarchial bandwith management is a very important packet
service in a lot of use cases (ranging from large data centres
to service provider use cases, etc).
Over the last decade, the TC Hierarchical Token Bucket(HTB) qdisc
has emerged as the most popular non-work conserving 
queueing disciple for enabling this service in Linux. 
HTB is quite flexible and versatile, but at large scale
(think hundred thousand to millions of flows) it comes at
a cost: 1) cpu cycles predominantly due to stalls caused by shared
queus lock contentions 2)extensive memory costs when adding many flows.

In this talk Yosef Kuperman focuses on offloading HTB to the NIC
hardware(Mellanox cnx5).
Flow classification takes place in the egress clsact to avoid any sorts
of (queue) locking. Packets are tagged and the offloaded HTB uses these
tags as flow/classids to select the correct queue in the hierarchy.
The authors will go over the challenges they overcame, show some
performance numbers and solicit feedback.
Captions: 
	00:00:01,920 --> 00:00:04,880
hi everyone we are yossi and maxim from

00:00:04,160 --> 00:00:07,520
melanox

00:00:04,880 --> 00:00:08,320
now nvidia and today we would like to

00:00:07,520 --> 00:00:11,519
present

00:00:08,320 --> 00:00:13,840
our approach to http offload

00:00:11,519 --> 00:00:16,400
this is the outline for today's talk we

00:00:13,840 --> 00:00:19,840
will first start with a short overview

00:00:16,400 --> 00:00:22,720
on hdb and its shortcomings then we will

00:00:19,840 --> 00:00:24,960
discuss our proposal to http offload

00:00:22,720 --> 00:00:26,160
and last we will present the current

00:00:24,960 --> 00:00:29,199
status

00:00:26,160 --> 00:00:29,199
and known challenges

00:00:29,439 --> 00:00:34,079
let's first start with an up with a

00:00:31,439 --> 00:00:36,960
brief overview of hdb

00:00:34,079 --> 00:00:38,160
http hierarchical token bucket is a

00:00:36,960 --> 00:00:40,879
queuing discipline

00:00:38,160 --> 00:00:42,399
a q disk specifically it is a shaper for

00:00:40,879 --> 00:00:44,399
outgoing traffic

00:00:42,399 --> 00:00:46,160
which provides a user with the ability

00:00:44,399 --> 00:00:48,079
to manage the available bandwidth

00:00:46,160 --> 00:00:50,480
and to divide it between applications in

00:00:48,079 --> 00:00:53,520
a controlled manner

00:00:50,480 --> 00:00:54,559
each of these green nodes corresponds to

00:00:53,520 --> 00:00:56,239
a traffic class

00:00:54,559 --> 00:00:57,760
and each class has rate and seal

00:00:56,239 --> 00:01:00,800
configurations options among

00:00:57,760 --> 00:01:01,840
others but these are the most important

00:01:00,800 --> 00:01:04,320
ones

00:01:01,840 --> 00:01:06,640
rate is the minimum bandwidth share or

00:01:04,320 --> 00:01:08,560
the guaranteer that hdb provides for

00:01:06,640 --> 00:01:11,760
that particular class

00:01:08,560 --> 00:01:12,400
seal on the other hand is the maximum

00:01:11,760 --> 00:01:14,720
traffic

00:01:12,400 --> 00:01:15,840
that is allowed for this class and

00:01:14,720 --> 00:01:19,200
beyond this value

00:01:15,840 --> 00:01:20,479
the traffic is throttled or shaped

00:01:19,200 --> 00:01:22,320
sending a back pressure to the

00:01:20,479 --> 00:01:24,479
application stopping it from sending

00:01:22,320 --> 00:01:26,640
more traffic

00:01:24,479 --> 00:01:28,560
what is interesting about http is its

00:01:26,640 --> 00:01:30,560
hierarchical structure

00:01:28,560 --> 00:01:32,720
we can arrange the classes in a form of

00:01:30,560 --> 00:01:35,119
a tree where the lymph nodes

00:01:32,720 --> 00:01:37,200
perform the actual shaping and the inner

00:01:35,119 --> 00:01:40,320
nodes define the borrowing relationship

00:01:37,200 --> 00:01:42,960
between class siblings for example

00:01:40,320 --> 00:01:44,079
when lyft node can borrow tokens from a

00:01:42,960 --> 00:01:46,320
sibling

00:01:44,079 --> 00:01:47,439
up to its configure sealed given of

00:01:46,320 --> 00:01:50,880
course that those

00:01:47,439 --> 00:01:50,880
tokens are not being used

00:01:51,119 --> 00:01:56,640
however hdb has some drawbacks

00:01:54,720 --> 00:01:59,119
scarlet implementation does not scale

00:01:56,640 --> 00:02:01,280
well with the number of txqs

00:01:59,119 --> 00:02:03,600
in fact http was written well before

00:02:01,280 --> 00:02:06,640
multi-queue was introduced

00:02:03,600 --> 00:02:07,040
more specifically altxqs points to the

00:02:06,640 --> 00:02:09,759
same

00:02:07,040 --> 00:02:11,440
instance of httpq disk which creates a

00:02:09,759 --> 00:02:14,560
synchronization point

00:02:11,440 --> 00:02:16,480
missing the entire idea of multi-q

00:02:14,560 --> 00:02:18,560
also note that both traffic

00:02:16,480 --> 00:02:19,200
classification and the shaping algorithm

00:02:18,560 --> 00:02:21,840
take place

00:02:19,200 --> 00:02:23,040
under this lock we would like to avoid

00:02:21,840 --> 00:02:25,440
this log completely

00:02:23,040 --> 00:02:29,200
by handling the classification and

00:02:25,440 --> 00:02:31,680
offloading the shaping to the hardware

00:02:29,200 --> 00:02:33,680
so to tackle the first one we would like

00:02:31,680 --> 00:02:36,480
to move the classification out of the

00:02:33,680 --> 00:02:40,239
httpq disk itself to the class ack

00:02:36,480 --> 00:02:42,319
act egress this means that the user now

00:02:40,239 --> 00:02:44,319
needs to configure the filters in a

00:02:42,319 --> 00:02:47,040
slightly different way than the usual

00:02:44,319 --> 00:02:48,480
for example we would replace this filter

00:02:47,040 --> 00:02:50,239
with an equivalent one

00:02:48,480 --> 00:02:52,000
that writes the class directly to the

00:02:50,239 --> 00:02:55,040
skb priority field

00:02:52,000 --> 00:02:57,519
as in this example this should work

00:02:55,040 --> 00:02:59,599
today without any code modification

00:02:57,519 --> 00:03:01,519
as http examines the priority before

00:02:59,599 --> 00:03:03,360
performing the classification

00:03:01,519 --> 00:03:05,519
and since the priority already defines

00:03:03,360 --> 00:03:08,640
the class http skips

00:03:05,519 --> 00:03:10,720
the classification altogether

00:03:08,640 --> 00:03:11,680
now the classification can be performed

00:03:10,720 --> 00:03:15,360
in parallel

00:03:11,680 --> 00:03:17,360
as class act hook is lock free

00:03:15,360 --> 00:03:19,120
note that in our approach to http

00:03:17,360 --> 00:03:19,840
offload traffic classification is

00:03:19,120 --> 00:03:22,080
performed

00:03:19,840 --> 00:03:24,640
in software and is not meant to be

00:03:22,080 --> 00:03:24,640
offloaded

00:03:25,040 --> 00:03:28,879
now even though the classification is

00:03:27,680 --> 00:03:30,640
out of the way

00:03:28,879 --> 00:03:32,720
we still have the http cubist clock

00:03:30,640 --> 00:03:34,959
along the data path

00:03:32,720 --> 00:03:37,440
in order to remove it we suggest to

00:03:34,959 --> 00:03:39,360
modify htv to present itself

00:03:37,440 --> 00:03:43,040
is a multi-queue queue disk in a similar

00:03:39,360 --> 00:03:45,599
way that mq and mq prior does

00:03:43,040 --> 00:03:47,680
so we will have hdb as a root key risk

00:03:45,599 --> 00:03:49,680
which will serve as an interface for the

00:03:47,680 --> 00:03:51,120
user to configure classes and query

00:03:49,680 --> 00:03:53,519
statistics

00:03:51,120 --> 00:03:55,360
in addition we allocate simple fifo

00:03:53,519 --> 00:03:58,480
queue discretix queue

00:03:55,360 --> 00:04:00,560
as depicted in the four line diagram

00:03:58,480 --> 00:04:02,799
this way we remove http code entirely

00:04:00,560 --> 00:04:04,640
from the data path and essentially

00:04:02,799 --> 00:04:07,120
pushing down all the heavy lifting of

00:04:04,640 --> 00:04:08,640
traffic shaping and tokens handling to

00:04:07,120 --> 00:04:10,879
the hardware

00:04:08,640 --> 00:04:12,840
going forward with this approach allows

00:04:10,879 --> 00:04:14,000
us to provide the user with hdb

00:04:12,840 --> 00:04:15,920
semantics

00:04:14,000 --> 00:04:18,799
while maintaining wire speed performance

00:04:15,920 --> 00:04:20,560
for nowadays high speed sneaks

00:04:18,799 --> 00:04:22,400
thank you and maximum will now continue

00:04:20,560 --> 00:04:25,600
with more detail maxim

00:04:22,400 --> 00:04:28,320
it's all yours i'm going to continue

00:04:25,600 --> 00:04:29,600
this presentation i'm gonna give you a

00:04:28,320 --> 00:04:34,400
more detailed look

00:04:29,600 --> 00:04:37,759
at the offload part so basically

00:04:34,400 --> 00:04:40,720
we have two parts here

00:04:37,759 --> 00:04:42,240
first one relates to the control flow

00:04:40,720 --> 00:04:45,360
and the second one

00:04:42,240 --> 00:04:48,880
is for data paths for

00:04:45,360 --> 00:04:52,240
for the control flow we use

00:04:48,880 --> 00:04:55,520
ngo setup tc as an interface between

00:04:52,240 --> 00:04:59,520
hdbq disk and the driver

00:04:55,520 --> 00:05:00,160
with this interface http passes the qs3

00:04:59,520 --> 00:05:03,919
structure

00:05:00,160 --> 00:05:06,720
to the driver and the driver mirrors in

00:05:03,919 --> 00:05:08,560
its hindenique as will be shown in the

00:05:06,720 --> 00:05:11,919
next slide

00:05:08,560 --> 00:05:12,479
for for the data path as usual already

00:05:11,919 --> 00:05:16,080
said

00:05:12,479 --> 00:05:19,840
we eliminate locking by registering

00:05:16,080 --> 00:05:23,600
uh as a multi-queue disk and we create

00:05:19,840 --> 00:05:28,000
hardware send cues backing leaf nodes

00:05:23,600 --> 00:05:28,320
in the qos tree moving a classification

00:05:28,000 --> 00:05:32,720
to

00:05:28,320 --> 00:05:35,199
a stage uh before and your select queue

00:05:32,720 --> 00:05:36,240
callback allows to keep to allows the

00:05:35,199 --> 00:05:39,360
driver to

00:05:36,240 --> 00:05:40,400
select the queue by class by traffic

00:05:39,360 --> 00:05:43,759
class

00:05:40,400 --> 00:05:46,800
and afterwards the nic will perform

00:05:43,759 --> 00:05:50,639
all rate limiting and shaping algorithm

00:05:46,800 --> 00:05:53,840
itself so

00:05:50,639 --> 00:05:54,639
this is the picture how uh how we

00:05:53,840 --> 00:05:58,479
present

00:05:54,639 --> 00:06:01,360
uh this qs3 and unique

00:05:58,479 --> 00:06:02,319
basically as we can see all the nodes

00:06:01,360 --> 00:06:07,680
are mirrored

00:06:02,319 --> 00:06:10,880
uh using the hardware objects of the nic

00:06:07,680 --> 00:06:16,160
and each leaf class has a corresponding

00:06:10,880 --> 00:06:16,160
send queue called sq here

00:06:16,479 --> 00:06:20,479
so that the different traffic classes go

00:06:19,680 --> 00:06:24,400
to

00:06:20,479 --> 00:06:24,400
separate hardware cues

00:06:24,479 --> 00:06:31,680
and basically this is the sequence

00:06:28,319 --> 00:06:34,160
which steps the packet goes through

00:06:31,680 --> 00:06:35,360
when it's been transmitted with http

00:06:34,160 --> 00:06:38,800
offload

00:06:35,360 --> 00:06:42,479
so the first step is that

00:06:38,800 --> 00:06:46,080
we use the class act egress hook

00:06:42,479 --> 00:06:49,840
to set the skb priority field

00:06:46,080 --> 00:06:49,840
to a leaf class id

00:06:50,560 --> 00:06:57,520
based on the filters that the user can

00:06:53,840 --> 00:07:02,080
configure with tc next

00:06:57,520 --> 00:07:05,520
next step is that packet goes to

00:07:02,080 --> 00:07:08,639
ngo select queue and the driver

00:07:05,520 --> 00:07:12,080
looks up that skb priority field

00:07:08,639 --> 00:07:15,440
and picks the corresponding dxq

00:07:12,080 --> 00:07:18,479
as we remember each lift class has

00:07:15,440 --> 00:07:21,680
the back and sq and hardware and

00:07:18,479 --> 00:07:25,919
of course hsq corresponds to a tx q

00:07:21,680 --> 00:07:29,280
over over net dev so we pick the txq

00:07:25,919 --> 00:07:32,639
the next step is the

00:07:29,280 --> 00:07:36,319
q disk step um

00:07:32,639 --> 00:07:39,680
as we have uh per q q disks

00:07:36,319 --> 00:07:43,039
uh in in our offload

00:07:39,680 --> 00:07:45,680
implementation we will take the q disk

00:07:43,039 --> 00:07:48,800
of the corresponding txq

00:07:45,680 --> 00:07:51,440
these are the fifo q disks

00:07:48,800 --> 00:07:52,800
that were mentioned by yoshi before and

00:07:51,440 --> 00:07:56,479
we will enqueue

00:07:52,800 --> 00:07:59,520
our skb into that corresponding

00:07:56,479 --> 00:08:02,160
q disk so

00:07:59,520 --> 00:08:03,280
the next step will obviously be the

00:08:02,160 --> 00:08:07,120
queuing from

00:08:03,280 --> 00:08:08,400
that qrisk and the driver will get the

00:08:07,120 --> 00:08:12,160
skb

00:08:08,400 --> 00:08:15,440
to be put into the hardware send queue

00:08:12,160 --> 00:08:19,599
then the rest is left to the nick

00:08:15,440 --> 00:08:22,639
it will take the packet data

00:08:19,599 --> 00:08:25,840
do the shaping according to

00:08:22,639 --> 00:08:28,879
the algorithm configuration

00:08:25,840 --> 00:08:29,759
which which was performed by ngo setup

00:08:28,879 --> 00:08:33,760
tc

00:08:29,759 --> 00:08:37,360
and finally it will transmit the packet

00:08:33,760 --> 00:08:40,719
so we basically have obvious

00:08:37,360 --> 00:08:44,320
advantages with this approach

00:08:40,719 --> 00:08:46,320
first of all we eliminate the contention

00:08:44,320 --> 00:08:47,440
that single lock that was mentioned

00:08:46,320 --> 00:08:51,600
before

00:08:47,440 --> 00:08:54,959
we don't need to take it anymore

00:08:51,600 --> 00:08:58,160
the traffic classes don't interfere with

00:08:54,959 --> 00:08:58,880
each other and it allows for better

00:08:58,160 --> 00:09:02,800
throughput

00:08:58,880 --> 00:09:06,000
and actually it allows to scale well

00:09:02,800 --> 00:09:09,519
increasing the number of txqs and

00:09:06,000 --> 00:09:12,480
traffic classes um another advantage

00:09:09,519 --> 00:09:14,800
is that actually we don't do the

00:09:12,480 --> 00:09:17,920
algorithm on the cpu

00:09:14,800 --> 00:09:19,200
reducing cpu load and offloading that

00:09:17,920 --> 00:09:22,480
mechanism to

00:09:19,200 --> 00:09:25,120
to the hardware to the unique

00:09:22,480 --> 00:09:26,080
of course we have some challenges with

00:09:25,120 --> 00:09:29,839
this approach

00:09:26,080 --> 00:09:33,360
as it's a relatively

00:09:29,839 --> 00:09:36,080
new stage so

00:09:33,360 --> 00:09:38,320
first limitation that i can mention is

00:09:36,080 --> 00:09:38,320
that

00:09:39,120 --> 00:09:45,839
it's a slightly different behavior

00:09:42,560 --> 00:09:48,959
with the queue disks of leaf classes

00:09:45,839 --> 00:09:52,720
so as we mentioned before we create

00:09:48,959 --> 00:09:56,720
a simple fifo q disks

00:09:52,720 --> 00:09:56,720
per each txq

00:09:57,200 --> 00:10:04,320
and you might wonder what if i

00:10:01,920 --> 00:10:05,279
assign some different some more complex

00:10:04,320 --> 00:10:09,519
q disk

00:10:05,279 --> 00:10:11,839
other than uh just pv4 or similar

00:10:09,519 --> 00:10:14,800
so here we actually have a difference in

00:10:11,839 --> 00:10:18,320
behavior because with software hdb

00:10:14,800 --> 00:10:22,000
if we assign a q disk to

00:10:18,320 --> 00:10:25,120
some class um then it will be

00:10:22,000 --> 00:10:28,240
processed after the http logic

00:10:25,120 --> 00:10:30,560
and with hardware offload um

00:10:28,240 --> 00:10:32,240
the http logic will be the last in the

00:10:30,560 --> 00:10:35,680
chain because it's in the

00:10:32,240 --> 00:10:40,800
hardware so we'll have to run the logic

00:10:35,680 --> 00:10:43,680
of uh of the

00:10:40,800 --> 00:10:44,560
of some non-trival q disks uh of the

00:10:43,680 --> 00:10:48,000
classes

00:10:44,560 --> 00:10:50,800
we'll have to run that logic before so

00:10:48,000 --> 00:10:51,920
this is yet to be investigated but it

00:10:50,800 --> 00:10:55,600
can

00:10:51,920 --> 00:10:58,399
lead to some different behavior

00:10:55,600 --> 00:10:59,760
another tricky point is that we have to

00:10:58,399 --> 00:11:03,040
pre-allocate

00:10:59,760 --> 00:11:05,440
the excuse on

00:11:03,040 --> 00:11:06,160
actually creating the net dev the driver

00:11:05,440 --> 00:11:09,839
has

00:11:06,160 --> 00:11:13,279
has to do it and those cues

00:11:09,839 --> 00:11:14,320
will be used to back leave classes of

00:11:13,279 --> 00:11:17,120
htb

00:11:14,320 --> 00:11:18,640
when such classes are created and

00:11:17,120 --> 00:11:22,079
offloaded

00:11:18,640 --> 00:11:24,720
and the thing is that

00:11:22,079 --> 00:11:25,680
we need to set the max amount of such

00:11:24,720 --> 00:11:27,760
queues

00:11:25,680 --> 00:11:29,680
on the allocation stage and the kernel

00:11:27,760 --> 00:11:35,680
will create an array

00:11:29,680 --> 00:11:35,680
of net dev q structs

00:11:35,760 --> 00:11:40,320
according to the amount that we passed

00:11:40,399 --> 00:11:45,279
but later on uh we'll just

00:11:46,240 --> 00:11:50,639
adjust the value of real num tx qs

00:11:49,200 --> 00:11:53,519
according to

00:11:50,639 --> 00:11:55,360
how many lift classes we have when some

00:11:53,519 --> 00:11:57,760
of them are created or deleted

00:11:55,360 --> 00:12:00,079
the driver will call this the

00:11:57,760 --> 00:12:03,440
corresponding function in the kernel

00:12:00,079 --> 00:12:06,480
and it will also create the hardware

00:12:03,440 --> 00:12:09,519
resources on demand

00:12:06,480 --> 00:12:13,200
another point is that when

00:12:09,519 --> 00:12:13,839
some leaf class is deleted it can lead

00:12:13,200 --> 00:12:18,560
to some

00:12:13,839 --> 00:12:22,160
gaps in numeration so either we need to

00:12:18,560 --> 00:12:25,279
adjust the numeration or to allocate

00:12:22,160 --> 00:12:29,360
new class numbers like

00:12:25,279 --> 00:12:32,639
dxq numbers when a new class is created

00:12:29,360 --> 00:12:32,639
to fill those gaps

00:12:33,600 --> 00:12:41,120
currently we have

00:12:36,800 --> 00:12:45,440
it partially implemented uh in two parts

00:12:41,120 --> 00:12:49,680
uh first of all we have some pofc

00:12:45,440 --> 00:12:50,880
poc patches for the melanox summit x5

00:12:49,680 --> 00:12:55,040
driver

00:12:50,880 --> 00:12:58,079
it uses some non-standard csfs

00:12:55,040 --> 00:13:01,200
interface for configuration but

00:12:58,079 --> 00:13:04,320
it allows to basically test

00:13:01,200 --> 00:13:07,920
the hardware offload functionality

00:13:04,320 --> 00:13:12,480
the second part uh it's the rfc patch

00:13:07,920 --> 00:13:12,480
it was posted to netdevmylen list

00:13:12,720 --> 00:13:20,880
recently um it's showing

00:13:16,399 --> 00:13:24,079
the interface that we are going to use

00:13:20,880 --> 00:13:27,360
for the upstream implementation uh

00:13:24,079 --> 00:13:28,079
that is based on hdb and basically on

00:13:27,360 --> 00:13:31,680
the stuff

00:13:28,079 --> 00:13:35,040
that we just described

00:13:31,680 --> 00:13:37,839
so thank you for

00:13:35,040 --> 00:13:37,839
your attention

00:13:41,279 --> 00:13:47,600
okay uh thanks a lot so we do have

00:13:44,480 --> 00:13:48,079
uh some questions um i'd like to start

00:13:47,600 --> 00:13:51,600
it

00:13:48,079 --> 00:13:54,079
with uh the goal of eliminating the spin

00:13:51,600 --> 00:13:55,680
lock or spin lock contention

00:13:54,079 --> 00:13:57,440
so um i think i think i had some

00:13:55,680 --> 00:13:59,600
questions on that but let me let me

00:13:57,440 --> 00:14:03,120
start with eric's questions

00:13:59,600 --> 00:14:05,360
um if all skbs have the same priority

00:14:03,120 --> 00:14:07,920
all packets will still use the a single

00:14:05,360 --> 00:14:08,560
txq so we're back to the problem of spin

00:14:07,920 --> 00:14:12,480
lock

00:14:08,560 --> 00:14:14,240
contention the multi-q doesn't help

00:14:12,480 --> 00:14:16,079
unless we packets can be spread over

00:14:14,240 --> 00:14:18,800
many txqs

00:14:16,079 --> 00:14:19,600
so i guess uh that's a question somehow

00:14:18,800 --> 00:14:23,360
so

00:14:19,600 --> 00:14:23,360
how does how do we get around this

00:14:24,399 --> 00:14:32,000
um can you hear me yes

00:14:28,079 --> 00:14:36,720
excellent so that's right um

00:14:32,000 --> 00:14:38,560
currently there is a 100 cube per class

00:14:36,720 --> 00:14:41,199
uh this is the first phase and we plan

00:14:38,560 --> 00:14:44,959
to introduce an api

00:14:41,199 --> 00:14:46,800
or something like that to to a

00:14:44,959 --> 00:14:49,120
to allow the user to specify how many

00:14:46,800 --> 00:14:51,680
hardware queues or txqs

00:14:49,120 --> 00:14:54,560
he wants per per class and in the driver

00:14:51,680 --> 00:14:57,519
we can use the hash function to

00:14:54,560 --> 00:14:58,480
spread the packets uh over those uh over

00:14:57,519 --> 00:15:01,120
those d excuse

00:14:58,480 --> 00:15:01,120
other cues

00:15:04,560 --> 00:15:12,079
okay so let me follow up with

00:15:08,880 --> 00:15:15,360
my question so do we

00:15:12,079 --> 00:15:18,320
have to have hdb offload

00:15:15,360 --> 00:15:21,199
full hdb offload here to relieve this

00:15:18,320 --> 00:15:24,240
ben lock contention or could we do this

00:15:21,199 --> 00:15:26,480
with the technique you just described

00:15:24,240 --> 00:15:28,320
have multi-q but have the software

00:15:26,480 --> 00:15:31,360
actually handle

00:15:28,320 --> 00:15:31,360
uh the different kids

00:15:32,639 --> 00:15:36,959
uh that's a good question um i don't

00:15:35,839 --> 00:15:38,880
know exactly but

00:15:36,959 --> 00:15:40,560
it seems complicated taking into

00:15:38,880 --> 00:15:43,680
consideration the borrowing

00:15:40,560 --> 00:15:44,160
between different classes so avoiding a

00:15:43,680 --> 00:15:47,759
look

00:15:44,160 --> 00:15:50,959
completely maybe something like a thread

00:15:47,759 --> 00:15:52,959
running in the background and um

00:15:50,959 --> 00:15:55,199
dispatching the tokens or something like

00:15:52,959 --> 00:15:55,199
that

00:15:55,759 --> 00:16:01,440
yeah i think i can add something uh to

00:15:59,120 --> 00:16:05,440
answer this question

00:16:01,440 --> 00:16:07,920
so basically the classical hdb

00:16:05,440 --> 00:16:08,720
algorithm that is implemented uh

00:16:07,920 --> 00:16:11,759
currently

00:16:08,720 --> 00:16:15,279
it relies on that single lock

00:16:11,759 --> 00:16:18,639
i saw recently there was some

00:16:15,279 --> 00:16:22,639
uh submission of

00:16:18,639 --> 00:16:26,480
the scheduler called ltb

00:16:22,639 --> 00:16:29,920
to to the mailing list it attempts to

00:16:26,480 --> 00:16:32,959
solve this this issue

00:16:29,920 --> 00:16:36,240
by using a different algorithm with

00:16:32,959 --> 00:16:37,120
similar semantics to hdb and that

00:16:36,240 --> 00:16:41,199
algorithm

00:16:37,120 --> 00:16:45,440
runs in software and

00:16:41,199 --> 00:16:45,440
like it can

00:16:46,320 --> 00:16:53,519
it can do the similar stuff but

00:16:49,360 --> 00:16:57,759
as the implementation is different

00:16:53,519 --> 00:16:57,759
they avoid that single lock but

00:16:58,160 --> 00:17:05,760
i discovered issues with that scheduler

00:17:02,000 --> 00:17:09,280
as well and

00:17:05,760 --> 00:17:13,280
it actually has a single uh

00:17:09,280 --> 00:17:18,240
thread that handles um

00:17:13,280 --> 00:17:21,039
like all rate limits which also might be

00:17:18,240 --> 00:17:21,039
a bottleneck

00:17:21,760 --> 00:17:25,199
besides that there are some issues with

00:17:23,679 --> 00:17:28,880
the current implementation

00:17:25,199 --> 00:17:32,400
that have to be fixed but

00:17:28,880 --> 00:17:34,720
yeah in general if you invent like some

00:17:32,400 --> 00:17:38,240
separate algorithm

00:17:34,720 --> 00:17:39,360
like ltb tries to do it could be

00:17:38,240 --> 00:17:42,559
possible to avoid

00:17:39,360 --> 00:17:42,559
this locking in software

00:17:43,120 --> 00:17:48,240
okay uh so next question uh this

00:17:46,160 --> 00:17:50,160
one's from jamel what about statistics

00:17:48,240 --> 00:17:52,799
are they reflected in software

00:17:50,160 --> 00:17:55,039
and i guess we can we can kind of

00:17:52,799 --> 00:17:55,679
generalize this so how transparent is

00:17:55,039 --> 00:18:01,200
this

00:17:55,679 --> 00:18:04,640
to the current users of htv

00:18:01,200 --> 00:18:07,840
yes so so we we expose the other expose

00:18:04,640 --> 00:18:10,720
statistics and

00:18:07,840 --> 00:18:12,240
if the user query any any statistics we

00:18:10,720 --> 00:18:13,520
will delegate to the driver the driver

00:18:12,240 --> 00:18:15,600
will delegate to the hardware and we

00:18:13,520 --> 00:18:17,600
provide

00:18:15,600 --> 00:18:19,039
some of the statistics i'm not sure yet

00:18:17,600 --> 00:18:22,080
which we provide but

00:18:19,039 --> 00:18:24,640
i suppose most of them are reflected to

00:18:22,080 --> 00:18:24,640
to the user

00:18:25,919 --> 00:18:31,039
and well jamel's follow-up i think is

00:18:28,000 --> 00:18:33,360
probably a typical question so when we

00:18:31,039 --> 00:18:35,120
offload to the device it's gathering

00:18:33,360 --> 00:18:37,120
statistics and then the question always

00:18:35,120 --> 00:18:39,520
becomes how do we

00:18:37,120 --> 00:18:43,200
retrieve those statistics as a periodic

00:18:39,520 --> 00:18:46,480
or on demand

00:18:43,200 --> 00:18:48,240
uh it's possible to be both i mean on

00:18:46,480 --> 00:18:51,520
demand or periodic uh

00:18:48,240 --> 00:18:57,840
depends on the needs

00:18:51,520 --> 00:18:57,840
i mean

00:18:59,039 --> 00:19:03,200
okay uh next question does the hardware

00:19:01,360 --> 00:19:05,919
support a global rate limit

00:19:03,200 --> 00:19:07,039
rate limit current for interface for the

00:19:05,919 --> 00:19:10,559
interface

00:19:07,039 --> 00:19:14,080
and what about borrowing between classes

00:19:10,559 --> 00:19:18,000
so borrowing is supported in the same

00:19:14,080 --> 00:19:20,640
similar manner as hdb in the same way

00:19:18,000 --> 00:19:22,559
and as far as i remember i think we

00:19:20,640 --> 00:19:23,520
support we do support a global rate

00:19:22,559 --> 00:19:26,559
limiting

00:19:23,520 --> 00:19:29,760
and this is configured using ip i think

00:19:26,559 --> 00:19:32,720
max rate or something yeah actually

00:19:29,760 --> 00:19:33,200
you can just set the max rate limit to

00:19:32,720 --> 00:19:37,120
the

00:19:33,200 --> 00:19:41,679
root node of of your qs3 so

00:19:37,120 --> 00:19:41,679
this will give you the necessary effect

00:19:43,200 --> 00:19:47,520
okay uh does it make sense to put the

00:19:45,120 --> 00:19:50,559
logic outside the driver

00:19:47,520 --> 00:19:51,200
so that all drivers could benefit from

00:19:50,559 --> 00:19:53,840
it

00:19:51,200 --> 00:19:54,720
so yeah we make the uh common apis for

00:19:53,840 --> 00:19:57,840
this

00:19:54,720 --> 00:19:58,400
uh we are planning to put the common api

00:19:57,840 --> 00:20:01,520
part

00:19:58,400 --> 00:20:04,880
uh into the httpq disk

00:20:01,520 --> 00:20:07,600
and it will have an interface

00:20:04,880 --> 00:20:08,640
with the driver and of course the driver

00:20:07,600 --> 00:20:11,520
will have

00:20:08,640 --> 00:20:12,640
to contain uh parts which are hardware

00:20:11,520 --> 00:20:15,919
specific

00:20:12,640 --> 00:20:16,640
um like talking to the hardware to

00:20:15,919 --> 00:20:20,080
offload

00:20:16,640 --> 00:20:21,679
the three nodes uh set all the limits

00:20:20,080 --> 00:20:23,120
this is all hardware specific and

00:20:21,679 --> 00:20:26,159
belongs to the driver

00:20:23,120 --> 00:20:29,440
all the rest is common to

00:20:26,159 --> 00:20:32,559
all drivers and it's placed into a

00:20:29,440 --> 00:20:32,559
scheduler hdb

00:20:33,600 --> 00:20:37,600
okay and then we have a question about

00:20:35,440 --> 00:20:38,799
um where the performance gains are

00:20:37,600 --> 00:20:41,520
coming from

00:20:38,799 --> 00:20:42,720
so other than the interface global lock

00:20:41,520 --> 00:20:45,360
how much cpu

00:20:42,720 --> 00:20:46,640
time does this save so it's a good

00:20:45,360 --> 00:20:49,280
question so

00:20:46,640 --> 00:20:50,960
let me step it up a little bit so the

00:20:49,280 --> 00:20:53,919
obvious question here is

00:20:50,960 --> 00:20:54,799
do we get all the savings from the

00:20:53,919 --> 00:20:57,840
locking

00:20:54,799 --> 00:20:58,559
saving or is it the actual processing of

00:20:57,840 --> 00:21:01,600
hdb

00:20:58,559 --> 00:21:04,559
or what or what combination is it

00:21:01,600 --> 00:21:04,559
for performance gains

00:21:08,159 --> 00:21:13,919
well not sure that i got the question

00:21:11,679 --> 00:21:13,919
so

00:21:15,200 --> 00:21:21,840
is it about like how much cpu

00:21:18,799 --> 00:21:25,280
is saved uh besides

00:21:21,840 --> 00:21:28,000
like eliminating the global lock um

00:21:25,280 --> 00:21:28,720
i'm not sure we are ready to answer this

00:21:28,000 --> 00:21:31,200
because

00:21:28,720 --> 00:21:32,480
uh in order to do this comparison we

00:21:31,200 --> 00:21:36,000
need to compare

00:21:32,480 --> 00:21:36,880
the offloaded hdb to something in

00:21:36,000 --> 00:21:40,880
software

00:21:36,880 --> 00:21:44,640
that somehow eliminates global lock but

00:21:40,880 --> 00:21:47,840
doesn't offload the algorithm itself

00:21:44,640 --> 00:21:51,120
so the closest alternative to this

00:21:47,840 --> 00:21:53,600
is ltb but like

00:21:51,120 --> 00:21:54,880
it's a different algorithm compared to

00:21:53,600 --> 00:21:58,400
hdb

00:21:54,880 --> 00:22:01,440
and like as i said

00:21:58,400 --> 00:22:04,799
we experienced some problems in uh

00:22:01,440 --> 00:22:08,000
in running it properly so uh

00:22:04,799 --> 00:22:08,960
yeah i'm not ready to uh to give such

00:22:08,000 --> 00:22:12,080
numbers

00:22:08,960 --> 00:22:15,840
it's complicated to fetch them

00:22:12,080 --> 00:22:18,960
to measure okay so i think this

00:22:15,840 --> 00:22:20,080
is um personally i think it's a critical

00:22:18,960 --> 00:22:23,520
question

00:22:20,080 --> 00:22:26,960
this might be a good topic for um

00:22:23,520 --> 00:22:29,440
either a site meeting or

00:22:26,960 --> 00:22:31,039
at a happy hour i'm sure that some there

00:22:29,440 --> 00:22:32,720
should be some who have thoughts on on

00:22:31,039 --> 00:22:35,440
how to evaluate this

00:22:32,720 --> 00:22:37,520
uh obviously if somehow there's a way to

00:22:35,440 --> 00:22:37,840
get all the benefits without putting the

00:22:37,520 --> 00:22:40,880
full

00:22:37,840 --> 00:22:43,600
hdb implementation inside the hardware

00:22:40,880 --> 00:22:44,799
um that you know almost by definition

00:22:43,600 --> 00:22:47,440
would be preferable

00:22:44,799 --> 00:22:48,720
uh but but if if we can show that

00:22:47,440 --> 00:22:50,640
there's some intangible

00:22:48,720 --> 00:22:55,280
or inherent benefits of doing that then

00:22:50,640 --> 00:22:57,120
you know by all means

00:22:55,280 --> 00:23:00,240
okay so i'm looking through see if

00:22:57,120 --> 00:23:00,240
there's any more questions

00:23:00,880 --> 00:23:09,840
why not map class flow id semantics to

00:23:04,240 --> 00:23:09,840
hardware queue

00:23:12,320 --> 00:23:16,480
so jamel do you want to give a little

00:23:14,720 --> 00:23:20,320
more uh background on this question

00:23:16,480 --> 00:23:23,360
it's yours no i mean that

00:23:20,320 --> 00:23:24,960
today we actually have identifiers for

00:23:23,360 --> 00:23:27,440
each queue

00:23:24,960 --> 00:23:28,559
when i add a rule let's say class id 1

00:23:27,440 --> 00:23:30,320
column 2

00:23:28,559 --> 00:23:32,799
and that plays well in the hierarchy of

00:23:30,320 --> 00:23:32,799
hdb

00:23:32,880 --> 00:23:37,840
that makes sense

00:23:36,000 --> 00:23:39,600
if not we can take it off to happy hour

00:23:37,840 --> 00:23:43,039
i can i can i can say more there

00:23:39,600 --> 00:23:43,919
yes so i'm not sure i got the question

00:23:43,039 --> 00:23:46,799
correctly but

00:23:43,919 --> 00:23:47,600
yeah the classification happens you you

00:23:46,799 --> 00:23:50,480
put

00:23:47,600 --> 00:23:50,880
the class id to which you can you should

00:23:50,480 --> 00:23:54,080
like

00:23:50,880 --> 00:23:57,200
you want to classify the packet

00:23:54,080 --> 00:23:59,279
and there is a mapping between uh

00:23:57,200 --> 00:24:01,279
traffic classes and the corresponding

00:23:59,279 --> 00:24:02,799
hardware queues

00:24:01,279 --> 00:24:04,880
right andy but you mentioned you're

00:24:02,799 --> 00:24:06,080
using skb priority that's why i was kind

00:24:04,880 --> 00:24:09,679
of uh

00:24:06,080 --> 00:24:11,600
maybe curious as to yes whether that was

00:24:09,679 --> 00:24:12,880
the right semantic or

00:24:11,600 --> 00:24:15,039
because priority could also mean

00:24:12,880 --> 00:24:18,080
something else right you need a priority

00:24:15,039 --> 00:24:20,320
for the hq which is reflected in skb

00:24:18,080 --> 00:24:23,600
priority

00:24:20,320 --> 00:24:25,360
whereas the queue selection is reflected

00:24:23,600 --> 00:24:26,400
by the class id

00:24:25,360 --> 00:24:29,200
i'm not sure how we're doing the time

00:24:26,400 --> 00:24:32,240
but we could talk about this after

00:24:29,200 --> 00:24:35,520
okay great

00:24:32,240 --> 00:24:38,080
okay uh is is it supported on vf

00:24:35,520 --> 00:24:38,080
as well

00:24:39,440 --> 00:24:44,000
uh i don't know actually i don't know if

00:24:41,440 --> 00:24:47,840
it's supported or directly on on the vf

00:24:44,000 --> 00:24:47,840
i'm not sure

00:24:51,279 --> 00:24:56,240
okay just to mention the the use case or

00:24:54,320 --> 00:24:59,600
the use case here is to uh

00:24:56,240 --> 00:25:02,000
run on on top of the pf without the srv

00:24:59,600 --> 00:25:03,200
just for local locally generated traffic

00:25:02,000 --> 00:25:05,600
that's the use case but

00:25:03,200 --> 00:25:06,400
maybe it's also possible to enhance it

00:25:05,600 --> 00:25:10,000
to

00:25:06,400 --> 00:25:10,000
work on top of vf

00:25:12,000 --> 00:25:18,559
okay if there is no more questions let's

00:25:14,480 --> 00:25:18,559

YouTube URL: https://www.youtube.com/watch?v=HJVPtAuhrNs


