Title: Netdev 0x14 - The Path To TCP 4K MTU and RX ZeroCopy
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Eric Dumazet

More info: https://netdevconf.info/0x14/session.html?talk-the-path-to-tcp-4k-mtu-and-rx-zerocopy

Date: Tuesday, August 18, 2020

If you are a big shop like Google you are always looking to squeeze
all the juice performance you get out of your CPUs. Given the large
number of machines deployed, a 1% performance improvement could mean
millions of $$ in savings in both capital and operational costs.

In this talk, monsieur Eric Dumazet - the man sometimes refered to in
passing as some form of ninja - will describe changes introduced by
Google that help double TCP receiver efficiency in their datacentres.

Eric will discuss various issues that had to be overcome and a
variety of tuning that was needed to achieve these performance
improvements.
Captions: 
	00:00:01,280 --> 00:00:06,720
hello everybody so

00:00:03,520 --> 00:00:09,840
i will present today on work

00:00:06,720 --> 00:00:13,360
we have done uh in the past

00:00:09,840 --> 00:00:16,560
years adding 4k um

00:00:13,360 --> 00:00:20,240
tcp and

00:00:16,560 --> 00:00:20,240
tcp received zero copy

00:00:22,800 --> 00:00:28,960
so a bit of history by about

00:00:25,920 --> 00:00:30,560
zero copying tcp we had zero copy on the

00:00:28,960 --> 00:00:33,920
transmit site for

00:00:30,560 --> 00:00:36,960
quite a long time um

00:00:33,920 --> 00:00:39,360
the traditional way of doing zero copy

00:00:36,960 --> 00:00:40,399
is obviously send file because this is

00:00:39,360 --> 00:00:42,960
what was

00:00:40,399 --> 00:00:44,000
used by web server like in the days

00:00:42,960 --> 00:00:48,480
where the conference

00:00:44,000 --> 00:00:52,800
was static a lot of content was static

00:00:48,480 --> 00:00:54,559
so some file historically on linux has

00:00:52,800 --> 00:00:58,000
been implemented on top of

00:00:54,559 --> 00:01:01,440
splice which is a set of infrastructure

00:00:58,000 --> 00:01:05,519
to manipulate page around pipes and

00:01:01,440 --> 00:01:08,560
many consumers providers

00:01:05,519 --> 00:01:08,880
and so for example on top of splice you

00:01:08,560 --> 00:01:11,280
can

00:01:08,880 --> 00:01:11,930
implement vm splice which is basically a

00:01:11,280 --> 00:01:13,280
way to

00:01:11,930 --> 00:01:17,040
[Music]

00:01:13,280 --> 00:01:20,400
implement a zero copy not

00:01:17,040 --> 00:01:24,080
using a source stored on disk but

00:01:20,400 --> 00:01:24,080
source store on memory

00:01:24,240 --> 00:01:31,520
back in linux 4.14 william

00:01:27,439 --> 00:01:34,560
added a message zero copy flag to the

00:01:31,520 --> 00:01:35,520
generic send message system core so

00:01:34,560 --> 00:01:38,640
instead of using

00:01:35,520 --> 00:01:41,759
splice games which are

00:01:38,640 --> 00:01:45,840
which might be a bit complex

00:01:41,759 --> 00:01:48,960
application can just use some message

00:01:45,840 --> 00:01:51,680
um one of the problem of

00:01:48,960 --> 00:01:54,240
some message in the transmit is a way

00:01:51,680 --> 00:01:56,719
for application to know when a

00:01:54,240 --> 00:01:57,119
particular piece of memory can be reused

00:01:56,719 --> 00:02:00,320
like

00:01:57,119 --> 00:02:03,360
freed or recycled for another use

00:02:00,320 --> 00:02:03,360
and so willem added

00:02:04,960 --> 00:02:11,039
completion notification on the socket

00:02:08,080 --> 00:02:12,720
error queue so that an application could

00:02:11,039 --> 00:02:15,760
use a message and then

00:02:12,720 --> 00:02:18,239
later use a receive message on the

00:02:15,760 --> 00:02:22,000
to reader or queue to get the

00:02:18,239 --> 00:02:24,000
completions notification

00:02:22,000 --> 00:02:26,400
the send message system call is slightly

00:02:24,000 --> 00:02:29,440
more efficient than the splice

00:02:26,400 --> 00:02:33,280
in term of having to lock

00:02:29,440 --> 00:02:37,280
the socket for every page added because

00:02:33,280 --> 00:02:40,480
splice set of uh

00:02:37,280 --> 00:02:43,040
infrastructure manipulate a page one by

00:02:40,480 --> 00:02:43,680
one so basically tcp sandpage is

00:02:43,040 --> 00:02:46,319
basically

00:02:43,680 --> 00:02:48,160
called for every single page at a time

00:02:46,319 --> 00:02:50,959
so that's

00:02:48,160 --> 00:02:52,239
that's a bit complex because the circle

00:02:50,959 --> 00:02:55,599
uh

00:02:52,239 --> 00:02:58,239
the socket lock is actually

00:02:55,599 --> 00:03:00,080
like a semi-floor so it really release

00:02:58,239 --> 00:03:05,760
has to relock again the spindle

00:03:00,080 --> 00:03:09,519
can release the spin lock

00:03:05,760 --> 00:03:10,400
for hardware features needed for the

00:03:09,519 --> 00:03:13,120
transmit side

00:03:10,400 --> 00:03:13,120
it's basically

00:03:14,239 --> 00:03:18,879
no features at all the only thing you

00:03:17,200 --> 00:03:23,440
need is scatter gapper

00:03:18,879 --> 00:03:27,680
and check some of loading

00:03:23,440 --> 00:03:30,879
so it has been there for quite a while

00:03:27,680 --> 00:03:31,519
a lot of nic do support zero copy on the

00:03:30,879 --> 00:03:35,040
transmit

00:03:31,519 --> 00:03:38,480
quite easily so

00:03:35,040 --> 00:03:41,920
now to the topic we are hunting today

00:03:38,480 --> 00:03:45,120
uh zero copy on the receive side

00:03:41,920 --> 00:03:47,840
uh if you are really interested

00:03:45,120 --> 00:03:48,950
into the optimal performance of a single

00:03:47,840 --> 00:03:50,720
tcp flow

00:03:48,950 --> 00:03:54,080
[Music]

00:03:50,720 --> 00:03:59,920
with 100 gig and more

00:03:54,080 --> 00:04:01,680
speed you now have on the link rates um

00:03:59,920 --> 00:04:03,680
the bottleneck is really the copy we

00:04:01,680 --> 00:04:07,680
have in the receive message

00:04:03,680 --> 00:04:09,760
so so in this particular example

00:04:07,680 --> 00:04:11,120
if you try to reach a maximum throughput

00:04:09,760 --> 00:04:14,720
you can get

00:04:11,120 --> 00:04:17,840
for 100 giganic

00:04:14,720 --> 00:04:20,880
on the current generation of internal

00:04:17,840 --> 00:04:24,000
cpu you basically are limited to 80

00:04:20,880 --> 00:04:24,000
gigabits per second

00:04:25,520 --> 00:04:31,280
and the bottleneck is the receive side

00:04:29,040 --> 00:04:33,120
in this case and the receiver is

00:04:31,280 --> 00:04:36,160
spending about 17

00:04:33,120 --> 00:04:39,600
of the cycles doing just a copy from

00:04:36,160 --> 00:04:39,600
camera to the user

00:04:40,840 --> 00:04:47,520
so how to solve that

00:04:43,919 --> 00:04:50,400
if if we want sometime

00:04:47,520 --> 00:04:53,520
you could say oh it would be nice to

00:04:50,400 --> 00:04:55,910
eliminate all the copy

00:04:53,520 --> 00:04:58,800
so yes what about map right

00:04:55,910 --> 00:05:02,560
[Music]

00:04:58,800 --> 00:05:06,080
the problem of map is that once you get

00:05:02,560 --> 00:05:08,160
memory mapping from the kernel to user

00:05:06,080 --> 00:05:10,479
you basically land to the user the

00:05:08,160 --> 00:05:13,280
ability to

00:05:10,479 --> 00:05:15,039
read an arbitrary byte into a single

00:05:13,280 --> 00:05:18,240
page

00:05:15,039 --> 00:05:19,199
and the page size is architecture

00:05:18,240 --> 00:05:22,240
dependent

00:05:19,199 --> 00:05:24,880
so and so for example

00:05:22,240 --> 00:05:25,520
let's take power pc with 64 kilobyte

00:05:24,880 --> 00:05:27,199
page

00:05:25,520 --> 00:05:28,560
it's going to be very hard to implement

00:05:27,199 --> 00:05:32,320
zero copy for

00:05:28,560 --> 00:05:35,199
power pc because it turns out that

00:05:32,320 --> 00:05:36,320
there is no single uh datagram that

00:05:35,199 --> 00:05:39,600
could

00:05:36,320 --> 00:05:42,720
feel completely a 64

00:05:39,600 --> 00:05:44,800
kilobyte page so it's not going to work

00:05:42,720 --> 00:05:47,120
on borah pc

00:05:44,800 --> 00:05:48,400
but you know maybe it's not a big deal

00:05:47,120 --> 00:05:51,600
because uh

00:05:48,400 --> 00:05:54,639
at least for google many servers

00:05:51,600 --> 00:05:57,759
are actually using 4k as a page size

00:05:54,639 --> 00:06:01,680
so all this work is really tied

00:05:57,759 --> 00:06:01,680
to the 4k page choice

00:06:03,280 --> 00:06:10,639
so sometimes the size of the interviewer

00:06:07,120 --> 00:06:13,680
packet is not a big deal because

00:06:10,639 --> 00:06:14,880
error is supposed to correlate the

00:06:13,680 --> 00:06:20,560
payload

00:06:14,880 --> 00:06:23,919
of multiple incoming packets into

00:06:20,560 --> 00:06:27,680
a set of page uh and so you could say

00:06:23,919 --> 00:06:28,160
um whatever segment size the sender is

00:06:27,680 --> 00:06:30,720
using

00:06:28,160 --> 00:06:31,919
the receiver is able to coalesce

00:06:30,720 --> 00:06:34,240
multiple payload

00:06:31,919 --> 00:06:35,919
in order to load segments into a

00:06:34,240 --> 00:06:40,000
contiguous piece of memory

00:06:35,919 --> 00:06:42,160
and so maybe you could also get um

00:06:40,000 --> 00:06:43,600
zero copy if you happen to fill the

00:06:42,160 --> 00:06:46,160
whole page

00:06:43,600 --> 00:06:47,120
with the appropriate choice of number of

00:06:46,160 --> 00:06:51,680
segments

00:06:47,120 --> 00:06:51,680
okay but um

00:06:52,000 --> 00:06:56,720
we don't use arrow right many modern

00:06:55,120 --> 00:07:00,000
links don't really

00:06:56,720 --> 00:07:03,680
under arrow at line related speed

00:07:00,000 --> 00:07:03,680
if they implement or whatever

00:07:04,080 --> 00:07:11,680
so what we chose instead was to really

00:07:07,680 --> 00:07:15,039
up the mtu so that every single mss

00:07:11,680 --> 00:07:17,919
fully filled

00:07:15,039 --> 00:07:18,800
mss would be exactly four kilobytes of

00:07:17,919 --> 00:07:21,599
memory

00:07:18,800 --> 00:07:21,599
for the payload

00:07:23,440 --> 00:07:28,880
okay so now we decided to use four

00:07:26,639 --> 00:07:30,639
kilobytes for the payload what about

00:07:28,880 --> 00:07:33,440
headers

00:07:30,639 --> 00:07:34,960
um yes you could say because you know

00:07:33,440 --> 00:07:37,039
tcp

00:07:34,960 --> 00:07:39,520
or the network header can have variable

00:07:37,039 --> 00:07:41,680
size for example if you are using a pc4

00:07:39,520 --> 00:07:42,639
ipv6 obviously the size of the network

00:07:41,680 --> 00:07:46,240
editor is different

00:07:42,639 --> 00:07:48,479
and even with tcp the size of the tcp

00:07:46,240 --> 00:07:51,520
option can vary

00:07:48,479 --> 00:07:53,759
based on the dynamic of the

00:07:51,520 --> 00:07:55,280
the flow like the sat blocks if you have

00:07:53,759 --> 00:07:59,840
losses on the network something like

00:07:55,280 --> 00:07:59,840
that so

00:08:00,000 --> 00:08:05,759
yeah so it would be nice if

00:08:03,360 --> 00:08:06,720
uh the size of the headers wouldn't

00:08:05,759 --> 00:08:09,919
matter right

00:08:06,720 --> 00:08:12,800
but unfortunately it does because

00:08:09,919 --> 00:08:14,240
some nick don't really have the ability

00:08:12,800 --> 00:08:17,599
to

00:08:14,240 --> 00:08:18,720
split the header and the data into

00:08:17,599 --> 00:08:20,879
different zone

00:08:18,720 --> 00:08:21,759
and different piece of memory what why

00:08:20,879 --> 00:08:24,720
do we want

00:08:21,759 --> 00:08:25,280
to split the memory because it would be

00:08:24,720 --> 00:08:28,319
nice

00:08:25,280 --> 00:08:29,520
if uh the application don't have to

00:08:28,319 --> 00:08:31,759
worry about the

00:08:29,520 --> 00:08:33,120
headers at all right the application

00:08:31,759 --> 00:08:35,279
doing a receive message

00:08:33,120 --> 00:08:36,719
normally on the tcp circuit just receive

00:08:35,279 --> 00:08:40,159
a payload and

00:08:36,719 --> 00:08:43,120
the the headers are completely hidden

00:08:40,159 --> 00:08:44,080
unless the application uses a socket

00:08:43,120 --> 00:08:48,240
option to

00:08:44,080 --> 00:08:48,240
reach address something like that

00:08:50,480 --> 00:08:54,880
so the problem we had to face was that

00:08:53,360 --> 00:08:58,160
google had

00:08:54,880 --> 00:09:01,040
many many different servers generation

00:08:58,160 --> 00:09:01,920
with different technique providers and

00:09:01,040 --> 00:09:05,440
so

00:09:01,920 --> 00:09:08,640
sony could do some header data split and

00:09:05,440 --> 00:09:11,760
others couldn't for example

00:09:08,640 --> 00:09:13,279
we use a fair number of servers using

00:09:11,760 --> 00:09:16,399
the melanox

00:09:13,279 --> 00:09:20,000
4 driver and melanox

00:09:16,399 --> 00:09:23,279
4 doesn't implement a

00:09:20,000 --> 00:09:26,320
native header data splits

00:09:23,279 --> 00:09:30,160
so instead we have to

00:09:26,320 --> 00:09:33,680
implement pseudo

00:09:30,160 --> 00:09:35,920
hello data speed and

00:09:33,680 --> 00:09:36,720
this was possible because the melanox

00:09:35,920 --> 00:09:39,279
hardware

00:09:36,720 --> 00:09:40,160
has the ability to store incoming packet

00:09:39,279 --> 00:09:43,680
into up to

00:09:40,160 --> 00:09:48,240
four different segments and you can

00:09:43,680 --> 00:09:51,040
precisely size each of them

00:09:48,240 --> 00:09:51,680
and so noting that a lot of our packets

00:09:51,040 --> 00:09:55,040
in the

00:09:51,680 --> 00:09:59,040
google fleet are actually ipv6

00:09:55,040 --> 00:10:02,560
and tcp packets and that we are using

00:09:59,040 --> 00:10:05,279
the tcp timestamp option um

00:10:02,560 --> 00:10:06,560
it turns out that for 99 percent of the

00:10:05,279 --> 00:10:10,240
packets

00:10:06,560 --> 00:10:13,519
the size of the headers is a well-known

00:10:10,240 --> 00:10:16,560
value of 86 bytes which are

00:10:13,519 --> 00:10:20,399
the 14 bytes of 800 plus 40 bytes of

00:10:16,560 --> 00:10:23,360
ipd6 plus 32 bytes for the tcp with 12

00:10:20,399 --> 00:10:23,360
bytes of options

00:10:25,279 --> 00:10:30,880
so the change in melanox for driver

00:10:28,480 --> 00:10:34,079
which is not upstream yet

00:10:30,880 --> 00:10:35,519
was just size the first segment to 86

00:10:34,079 --> 00:10:39,760
bytes

00:10:35,519 --> 00:10:39,760
and the second segment to be 4k

00:10:41,120 --> 00:10:44,640
and so the driver just instead of

00:10:43,920 --> 00:10:47,440
attaching

00:10:44,640 --> 00:10:47,839
to each rescaby two fragments which

00:10:47,440 --> 00:10:51,519
would be

00:10:47,839 --> 00:10:52,959
a bit stupid right we just copy the 86

00:10:51,519 --> 00:10:55,200
byte of the headers

00:10:52,959 --> 00:10:56,079
into the escape head and then we attach

00:10:55,200 --> 00:10:59,440
a single

00:10:56,079 --> 00:11:02,959
page into the skb before feeding this

00:10:59,440 --> 00:11:02,959
skb to the gyro stack

00:11:03,680 --> 00:11:08,560
so what happens with other brackets so

00:11:06,079 --> 00:11:09,120
packets you're not using ipv6 plus tcp

00:11:08,560 --> 00:11:11,120
whatever

00:11:09,120 --> 00:11:13,839
so for example if you receive an ipv4

00:11:11,120 --> 00:11:18,079
packet with udp or tcp

00:11:13,839 --> 00:11:18,079
the headers will be smaller right so

00:11:19,279 --> 00:11:23,680
no big deal it means that the skb head

00:11:21,360 --> 00:11:26,480
will contain a bit of payload

00:11:23,680 --> 00:11:28,720
and the remaining of the stack is just

00:11:26,480 --> 00:11:31,519
happy with that

00:11:28,720 --> 00:11:33,200
that is the udp or tcp or whatever

00:11:31,519 --> 00:11:36,560
protocol receive message will

00:11:33,200 --> 00:11:37,440
copy whatever bytes are in the skb head

00:11:36,560 --> 00:11:40,640
and or

00:11:37,440 --> 00:11:40,640
in the page fragment

00:11:40,880 --> 00:11:45,360
in case the headers are bigger than 86

00:11:43,440 --> 00:11:48,399
bytes

00:11:45,360 --> 00:11:50,880
no bigger at all the 0 or the ip layer

00:11:48,399 --> 00:11:53,519
or tcp layer or whatever layer will

00:11:50,880 --> 00:11:56,399
actually pull the missing bytes from the

00:11:53,519 --> 00:11:58,959
page frag to the skb head

00:11:56,399 --> 00:11:58,959
on demand

00:11:59,519 --> 00:12:03,120
but the point here is that 99 percent of

00:12:02,160 --> 00:12:08,839
the packets

00:12:03,120 --> 00:12:10,000
are exactly meeting this 86 bytes magic

00:12:08,839 --> 00:12:13,760
value

00:12:10,000 --> 00:12:14,639
and so another issue we had at google

00:12:13,760 --> 00:12:18,079
was that

00:12:14,639 --> 00:12:19,440
autonics were using implementing header

00:12:18,079 --> 00:12:22,639
data split

00:12:19,440 --> 00:12:26,320
but they couldn't really

00:12:22,639 --> 00:12:28,560
implement a schedule getter and

00:12:26,320 --> 00:12:28,560
so

00:12:29,519 --> 00:12:36,320
meaning that because we had to size

00:12:32,560 --> 00:12:40,399
the the frame content to

00:12:36,320 --> 00:12:43,360
an exact page size

00:12:40,399 --> 00:12:44,560
to limit ourselves to normal order zero

00:12:43,360 --> 00:12:48,720
allocation

00:12:44,560 --> 00:12:48,720
we had to limit the payload to 4k

00:12:49,200 --> 00:12:54,000
and so combined with the melanox 4

00:12:52,079 --> 00:12:57,440
constraints

00:12:54,000 --> 00:13:00,959
we have this combination of

00:12:57,440 --> 00:13:03,760
header size payload size and the size of

00:13:00,959 --> 00:13:03,760
the combination

00:13:04,839 --> 00:13:07,839
right

00:13:08,959 --> 00:13:15,440
so in the case tcp

00:13:12,079 --> 00:13:18,480
has to increase the tcp header

00:13:15,440 --> 00:13:20,880
size because it adds another option

00:13:18,480 --> 00:13:21,600
to the tcp time stamp option like a side

00:13:20,880 --> 00:13:24,639
block

00:13:21,600 --> 00:13:27,839
something like that then

00:13:24,639 --> 00:13:30,240
we have also to reduce the payload

00:13:27,839 --> 00:13:30,880
of this segment and that's fine because

00:13:30,240 --> 00:13:34,160
tcp

00:13:30,880 --> 00:13:38,160
just handles us very nicely with the

00:13:34,160 --> 00:13:38,160
mss option idbnss option

00:13:38,959 --> 00:13:46,399
so if data packets carries

00:13:42,160 --> 00:13:49,360
not usual options then

00:13:46,399 --> 00:13:51,040
the payload won't be exactly 4k and that

00:13:49,360 --> 00:13:52,560
means that the receiver might not be

00:13:51,040 --> 00:13:55,680
able to use zero copy

00:13:52,560 --> 00:13:55,680
for this kind of packets

00:13:57,920 --> 00:14:02,320
so what are the actual change we did in

00:14:00,240 --> 00:14:04,560
the routing configuration to make that

00:14:02,320 --> 00:14:08,160
happen

00:14:04,560 --> 00:14:08,480
first we decided to even if we increased

00:14:08,160 --> 00:14:11,519
the

00:14:08,480 --> 00:14:14,399
mq on the

00:14:11,519 --> 00:14:15,040
various hosted host and switch and

00:14:14,399 --> 00:14:17,199
whatever

00:14:15,040 --> 00:14:18,880
we couldn't make sure that all the

00:14:17,199 --> 00:14:21,839
switch all the fabrics were

00:14:18,880 --> 00:14:23,279
updated at once so we had to do

00:14:21,839 --> 00:14:26,800
progressive deployments

00:14:23,279 --> 00:14:30,240
and so we chose to

00:14:26,800 --> 00:14:34,079
let the default mq default route and you

00:14:30,240 --> 00:14:37,120
to be the 1500 byte mq

00:14:34,079 --> 00:14:40,800
and we selectively enabled the 4k

00:14:37,120 --> 00:14:43,600
tcp traffic for specific destinations

00:14:40,800 --> 00:14:45,040
and so we upgraded the cluster one by

00:14:43,600 --> 00:14:48,639
one

00:14:45,040 --> 00:14:50,000
and so this slide presents roughly the

00:14:48,639 --> 00:14:51,839
the configuration we set

00:14:50,000 --> 00:14:53,440
on the device with a big intuit

00:14:51,839 --> 00:14:55,440
arbitrary begin to you because the

00:14:53,440 --> 00:14:58,079
driver might limit the vector

00:14:55,440 --> 00:14:59,040
configuration because of order zero

00:14:58,079 --> 00:15:02,480
constraints

00:14:59,040 --> 00:15:06,160
allocation and then the default uh

00:15:02,480 --> 00:15:08,320
route here is 1500 by mq and then we

00:15:06,160 --> 00:15:12,000
have some specific rule

00:15:08,320 --> 00:15:15,600
to enable tcp traffic only to use

00:15:12,000 --> 00:15:16,560
another set of routes enabling the

00:15:15,600 --> 00:15:21,440
bigger and here

00:15:16,560 --> 00:15:25,839
and an id vmss set to

00:15:21,440 --> 00:15:28,639
4 108 bytes

00:15:25,839 --> 00:15:30,240
why we did that because we wanted both

00:15:28,639 --> 00:15:33,600
sides to be on the board right

00:15:30,240 --> 00:15:34,320
you can't uh enable 4km2 on one side

00:15:33,600 --> 00:15:37,759
only so

00:15:34,320 --> 00:15:40,000
we rely on the fact that tcp exchange

00:15:37,759 --> 00:15:42,800
the idvnss option on the scene and

00:15:40,000 --> 00:15:42,800
synapt packets

00:15:44,000 --> 00:15:49,680
and one thing we did also was to be able

00:15:47,440 --> 00:15:51,600
to fall back immediately to the standard

00:15:49,680 --> 00:15:54,000
mq

00:15:51,600 --> 00:15:57,360
whenever we detected some issues with

00:15:54,000 --> 00:15:57,360
one of the packets we sent

00:16:02,639 --> 00:16:08,720
and so by doing so

00:16:06,320 --> 00:16:10,079
we had tremendous results by switching

00:16:08,720 --> 00:16:13,040
the mq for

00:16:10,079 --> 00:16:14,000
from 1500 bytes to 4km2 obviously

00:16:13,040 --> 00:16:15,199
because you receive the number of

00:16:14,000 --> 00:16:18,399
packets you have to under

00:16:15,199 --> 00:16:22,079
both on the sun and the receive side but

00:16:18,399 --> 00:16:25,120
it also exhibits some benchmark

00:16:22,079 --> 00:16:29,040
issues because of some

00:16:25,120 --> 00:16:29,759
heuristics done in dcp stack for example

00:16:29,040 --> 00:16:32,399
one of them

00:16:29,759 --> 00:16:33,839
is the delay dac so whenever tcp

00:16:32,399 --> 00:16:36,639
receives a single segment

00:16:33,839 --> 00:16:38,320
usually it will not send any media

00:16:36,639 --> 00:16:38,880
attack because there is this rule about

00:16:38,320 --> 00:16:42,000
tcp

00:16:38,880 --> 00:16:44,160
sending one hack for every two packets

00:16:42,000 --> 00:16:45,759
so if you receive a single package just

00:16:44,160 --> 00:16:48,800
wait a bit to send the ack

00:16:45,759 --> 00:16:51,120
and hope to receive the second packet

00:16:48,800 --> 00:16:53,360
but so if you have a workload sending

00:16:51,120 --> 00:16:56,880
the rpc of 2 000 bytes

00:16:53,360 --> 00:16:59,040
for example with the standard mtu

00:16:56,880 --> 00:17:00,639
you are actually sending two packets all

00:16:59,040 --> 00:17:02,399
right and so

00:17:00,639 --> 00:17:03,680
by receiving these two packets you send

00:17:02,399 --> 00:17:06,959
any media attack

00:17:03,680 --> 00:17:10,079
but now if you have 4km2 then this two

00:17:06,959 --> 00:17:13,199
kilobyte packets is alone and so

00:17:10,079 --> 00:17:15,199
tcp enters this heuristic of not sending

00:17:13,199 --> 00:17:19,120
an immediate attack

00:17:15,199 --> 00:17:22,000
and some benchmark can

00:17:19,120 --> 00:17:23,760
not deal properly with that because you

00:17:22,000 --> 00:17:25,360
know there are benchmarks and so they

00:17:23,760 --> 00:17:27,039
just measure some numbers and

00:17:25,360 --> 00:17:30,160
if you change these numbers they look

00:17:27,039 --> 00:17:30,160
bad for some reason

00:17:31,679 --> 00:17:35,120
another issue on tcp standard side is

00:17:34,640 --> 00:17:38,640
that

00:17:35,120 --> 00:17:41,520
tcp has this um way of

00:17:38,640 --> 00:17:42,400
cooking skb which are put in the

00:17:41,520 --> 00:17:44,720
transmit queue

00:17:42,400 --> 00:17:46,000
before they are actually sent and the

00:17:44,720 --> 00:17:49,679
way it works is

00:17:46,000 --> 00:17:53,919
that we divide a maximum number of

00:17:49,679 --> 00:17:57,360
bytes that can be set in the ip datagram

00:17:53,919 --> 00:17:59,280
which is 64k and you you we divide that

00:17:57,360 --> 00:18:00,000
by the mss value to get a number of

00:17:59,280 --> 00:18:03,440
segments per

00:18:00,000 --> 00:18:06,559
escb so if you use a 1500

00:18:03,440 --> 00:18:09,120
1500 byte mq it leads to about 45

00:18:06,559 --> 00:18:12,720
segments per tsu packet

00:18:09,120 --> 00:18:15,840
each segment can carry 14 28 bytes

00:18:12,720 --> 00:18:19,120
on ipv6 and for

00:18:15,840 --> 00:18:20,480
kmq it's only full 250 right and so you

00:18:19,120 --> 00:18:23,120
have a slight

00:18:20,480 --> 00:18:25,200
difference in the overall occupancy of

00:18:23,120 --> 00:18:26,880
one ts2 packet

00:18:25,200 --> 00:18:28,799
and that could matter if you have a

00:18:26,880 --> 00:18:32,240
workload sending something like

00:18:28,799 --> 00:18:34,000
you know the packet with 62 kilobytes or

00:18:32,240 --> 00:18:36,080
62 000 bytes

00:18:34,000 --> 00:18:38,720
because before it was sent as a single

00:18:36,080 --> 00:18:42,400
tierso packet and after

00:18:38,720 --> 00:18:46,720
going to 4k then now it's two packets

00:18:42,400 --> 00:18:46,720
with the second one we very very small

00:18:47,840 --> 00:18:55,120
uh another issue we had was um

00:18:51,760 --> 00:18:57,919
by switching the the mtu to 4k

00:18:55,120 --> 00:19:00,320
it means that the nic on the received

00:18:57,919 --> 00:19:03,840
side has to allocate 4k

00:19:00,320 --> 00:19:07,120
fragments instead of 2k

00:19:03,840 --> 00:19:09,520
and it's fine if you receive traffic

00:19:07,120 --> 00:19:10,320
using this full 4k but if you are

00:19:09,520 --> 00:19:13,520
receiving

00:19:10,320 --> 00:19:16,559
normal traffic 1500 by tmq traffic

00:19:13,520 --> 00:19:20,480
because the sender was not updated yet

00:19:16,559 --> 00:19:20,480
or the network was not updated yet

00:19:20,799 --> 00:19:26,720
it reduced the the overall occupancy

00:19:24,000 --> 00:19:27,600
of one fragment and we have this notion

00:19:26,720 --> 00:19:29,760
of

00:19:27,600 --> 00:19:31,600
ratio between the true size of the skv

00:19:29,760 --> 00:19:34,080
which is the memory size allocated

00:19:31,600 --> 00:19:36,240
divided by the skb length which is the

00:19:34,080 --> 00:19:39,440
occupancy of the frame

00:19:36,240 --> 00:19:41,039
and if its ratio is lowered by 50

00:19:39,440 --> 00:19:44,320
percent in this case

00:19:41,039 --> 00:19:47,520
tcp might end up sending uh

00:19:44,320 --> 00:19:50,240
smaller receive window uh columns

00:19:47,520 --> 00:19:50,960
and it has an impact on the maximum

00:19:50,240 --> 00:19:55,039
throughput you can

00:19:50,960 --> 00:19:55,039
get on long distance flow

00:19:55,200 --> 00:19:59,039
so there was a period of time where when

00:19:58,080 --> 00:20:02,559
we adopted

00:19:59,039 --> 00:20:05,600
4kg where we knew that

00:20:02,559 --> 00:20:08,320
some workload would be hurt

00:20:05,600 --> 00:20:08,320
for some time

00:20:09,760 --> 00:20:16,000
in terms of api um

00:20:13,039 --> 00:20:17,440
first when i implemented the tcpm core i

00:20:16,000 --> 00:20:21,600
just did a patch with an

00:20:17,440 --> 00:20:23,919
app call and then lkml um

00:20:21,600 --> 00:20:27,039
reviewers were not very happy about that

00:20:23,919 --> 00:20:30,159
and then we changed that

00:20:27,039 --> 00:20:32,240
later to give two operations

00:20:30,159 --> 00:20:33,200
the first one being the map system call

00:20:32,240 --> 00:20:36,320
which is basically

00:20:33,200 --> 00:20:38,000
reserving a space in the vma on the user

00:20:36,320 --> 00:20:40,880
process

00:20:38,000 --> 00:20:43,200
which will make sure that future mapping

00:20:40,880 --> 00:20:46,559
will be only on the read-only

00:20:43,200 --> 00:20:47,039
operation again because when you receive

00:20:46,559 --> 00:20:50,159
traffic

00:20:47,039 --> 00:20:52,799
over on nick the skb flags are

00:20:50,159 --> 00:20:55,039
read only because they could be shared

00:20:52,799 --> 00:20:57,679
by another user like tcp

00:20:55,039 --> 00:20:58,159
tcp dump or packet capture who could you

00:20:57,679 --> 00:21:00,159
know

00:20:58,159 --> 00:21:03,039
get a handle on this page and you don't

00:21:00,159 --> 00:21:06,720
want to change them because that could

00:21:03,039 --> 00:21:08,799
change the packet capture

00:21:06,720 --> 00:21:10,960
and so after that the second operation

00:21:08,799 --> 00:21:15,039
is the get started option

00:21:10,960 --> 00:21:15,039
which is doing the actual mapping

00:21:15,200 --> 00:21:20,880
based on the content of the queue and uh

00:21:18,240 --> 00:21:22,720
based on the fact that all the page all

00:21:20,880 --> 00:21:23,039
the the skb contained in the receive

00:21:22,720 --> 00:21:26,480
queue

00:21:23,039 --> 00:21:26,480
are candidates to mapping

00:21:27,760 --> 00:21:31,970
so the raw results are quite good

00:21:30,000 --> 00:21:34,559
because instead of 80 gig

00:21:31,970 --> 00:21:36,720
[Music]

00:21:34,559 --> 00:21:38,720
being limited by the zero the the copy

00:21:36,720 --> 00:21:41,919
in the camera we can now

00:21:38,720 --> 00:21:42,799
reach a line rate easily and it seems

00:21:41,919 --> 00:21:46,320
that we could

00:21:42,799 --> 00:21:48,320
probably reach 200 gig line rate

00:21:46,320 --> 00:21:49,600
the problem is i don't have such niche

00:21:48,320 --> 00:21:52,000
right now so i can't

00:21:49,600 --> 00:21:52,000
tell that

00:21:57,360 --> 00:22:01,600
so one of the me or one we have many

00:22:00,640 --> 00:22:04,799
issues with the

00:22:01,600 --> 00:22:09,039
the zero copy

00:22:04,799 --> 00:22:09,039
zero copy uh

00:22:09,600 --> 00:22:14,159
is all about memory management issues

00:22:14,400 --> 00:22:19,600
for example you have this

00:22:17,600 --> 00:22:22,000
map semaphore which was converted to

00:22:19,600 --> 00:22:24,640
maplock recently

00:22:22,000 --> 00:22:26,320
which has to be grabbed every time

00:22:24,640 --> 00:22:29,360
processed does a map system

00:22:26,320 --> 00:22:31,679
or a manmap system call and so if you

00:22:29,360 --> 00:22:34,960
have a multi-threaded application

00:22:31,679 --> 00:22:38,480
all the threads might compete onto this

00:22:34,960 --> 00:22:42,080
map block

00:22:38,480 --> 00:22:42,080
read write work

00:22:43,440 --> 00:22:47,520
another issue is the cost of the tlb

00:22:45,520 --> 00:22:49,360
invalidation so at the time you want to

00:22:47,520 --> 00:22:50,559
release the the mapping you did in the

00:22:49,360 --> 00:22:52,960
past

00:22:50,559 --> 00:22:55,200
you might have to see more other cpu

00:22:52,960 --> 00:22:58,320
that the mapping is gone

00:22:55,200 --> 00:23:01,360
because you want the user

00:22:58,320 --> 00:23:04,000
other user trying to still reference the

00:23:01,360 --> 00:23:07,440
whole mapping to get the same annotation

00:23:04,000 --> 00:23:08,400
for it and this is still being

00:23:07,440 --> 00:23:10,880
validation is

00:23:08,400 --> 00:23:12,400
hugely expensive because it sends an ipi

00:23:10,880 --> 00:23:15,679
to all cpu

00:23:12,400 --> 00:23:18,640
involved in this workload

00:23:15,679 --> 00:23:20,559
another issue we have is the lack of

00:23:18,640 --> 00:23:23,679
transparent huge page

00:23:20,559 --> 00:23:24,440
so for example why it matters if you are

00:23:23,679 --> 00:23:25,760
receiving

00:23:24,440 --> 00:23:29,440
[Music]

00:23:25,760 --> 00:23:32,400
60 kilobyte of data using receive

00:23:29,440 --> 00:23:32,400
message system core

00:23:33,760 --> 00:23:40,480
if the application is using a huge page

00:23:37,360 --> 00:23:44,000
for the map for the data

00:23:40,480 --> 00:23:44,720
portion in user space then the copy will

00:23:44,000 --> 00:23:49,200
not trigger

00:23:44,720 --> 00:23:51,279
any tlb fault

00:23:49,200 --> 00:23:54,000
because it can be all mapped into a

00:23:51,279 --> 00:23:54,000
single page

00:23:54,320 --> 00:24:01,360
if you are using xerocopy because we

00:23:57,520 --> 00:24:04,480
mapped single 4k page then this 60

00:24:01,360 --> 00:24:07,120
kilobyte data will map to

00:24:04,480 --> 00:24:08,960
15 page and so that's an additional

00:24:07,120 --> 00:24:13,279
course of the tier because

00:24:08,960 --> 00:24:13,279
right after the mapping has been done

00:24:14,000 --> 00:24:17,360
another issue we had was about xdp

00:24:16,400 --> 00:24:20,720
because xdp

00:24:17,360 --> 00:24:23,840
was for a while limited to 4k uh

00:24:20,720 --> 00:24:27,440
frame and so this frame had to be

00:24:23,840 --> 00:24:29,760
the size of the headers plus the payload

00:24:27,440 --> 00:24:31,120
there was no concept of header data

00:24:29,760 --> 00:24:33,840
splits and xzp

00:24:31,120 --> 00:24:33,840
for quite a while

00:24:34,400 --> 00:24:39,279
and of course another issue with the

00:24:36,559 --> 00:24:42,000
zero copy stuff is that

00:24:39,279 --> 00:24:44,480
holding page in user space after the map

00:24:42,000 --> 00:24:44,480
operation

00:24:44,799 --> 00:24:49,600
is defeating the page pool uh page

00:24:48,080 --> 00:24:52,799
recycling heuristic

00:24:49,600 --> 00:24:55,679
done by the various snake right

00:24:52,799 --> 00:24:57,279
so the page pool recycling is really a

00:24:55,679 --> 00:25:01,360
way for the new driver to

00:24:57,279 --> 00:25:04,320
constantly reuse the page knowing that

00:25:01,360 --> 00:25:05,440
the consumer of the escape already

00:25:04,320 --> 00:25:08,320
released

00:25:05,440 --> 00:25:11,679
its own page ref so by the time you are

00:25:08,320 --> 00:25:11,679
trying to reuse this page

00:25:11,919 --> 00:25:15,840
you look at the page count and if the

00:25:13,440 --> 00:25:16,880
page count was returned to one it means

00:25:15,840 --> 00:25:18,880
that you are the only

00:25:16,880 --> 00:25:21,200
user of this page and you don't have to

00:25:18,880 --> 00:25:24,159
do the page free and page analog

00:25:21,200 --> 00:25:26,840
which is very expensive why it's

00:25:24,159 --> 00:25:30,400
expensive it's expensive because

00:25:26,840 --> 00:25:33,440
um all the memory management zone are

00:25:30,400 --> 00:25:35,600
protected by their own spin lock so

00:25:33,440 --> 00:25:38,240
if you have a lot of cpu sharing the

00:25:35,600 --> 00:25:41,600
same m zone

00:25:38,240 --> 00:25:43,840
uh you can hit a spindle contention on

00:25:41,600 --> 00:25:43,840
this

00:25:45,360 --> 00:25:49,600
and last but not least of course if the

00:25:47,679 --> 00:25:52,880
application needs to access the data

00:25:49,600 --> 00:25:55,360
right after doing the receive from the

00:25:52,880 --> 00:25:58,480
receiver queue of the socket

00:25:55,360 --> 00:26:00,640
avoiding the copy might not be very

00:25:58,480 --> 00:26:02,640
useful because

00:26:00,640 --> 00:26:03,679
only the first copy is expensive the

00:26:02,640 --> 00:26:06,080
second one

00:26:03,679 --> 00:26:07,600
is almost free because the data is

00:26:06,080 --> 00:26:11,360
already in the cpu cache

00:26:07,600 --> 00:26:14,159
so zero cup is nice if you can really

00:26:11,360 --> 00:26:14,559
remove all the copies but as long as you

00:26:14,159 --> 00:26:18,559
have

00:26:14,559 --> 00:26:25,840
one copy somewhere the gain of zero b

00:26:18,559 --> 00:26:29,360
won't show up yet

00:26:25,840 --> 00:26:30,000
so we had to do some optimization in the

00:26:29,360 --> 00:26:34,240
stack

00:26:30,000 --> 00:26:37,360
to make full use of the zero copy

00:26:34,240 --> 00:26:39,440
first thing was trying to

00:26:37,360 --> 00:26:40,640
reduce the number of acquisition of the

00:26:39,440 --> 00:26:43,039
map lock right

00:26:40,640 --> 00:26:44,159
so because when you do receive message

00:26:43,039 --> 00:26:46,720
call if you

00:26:44,159 --> 00:26:49,279
do receive for every incoming escape

00:26:46,720 --> 00:26:53,279
it's not a big deal because you only

00:26:49,279 --> 00:26:55,200
hit a local circuits pinlock

00:26:53,279 --> 00:26:56,400
and so there is no contention there

00:26:55,200 --> 00:27:00,000
because each user

00:26:56,400 --> 00:27:03,520
has its own socket if you do

00:27:00,000 --> 00:27:05,520
however the mapping operation

00:27:03,520 --> 00:27:07,360
on the multi-thread program every time

00:27:05,520 --> 00:27:08,799
you try to grab some

00:27:07,360 --> 00:27:10,640
something from the receive queue of the

00:27:08,799 --> 00:27:13,760
circuit you will have to map

00:27:10,640 --> 00:27:16,960
to grab the the map block semaphore

00:27:13,760 --> 00:27:19,600
in read read access so that might be

00:27:16,960 --> 00:27:22,480
a construction point so really here you

00:27:19,600 --> 00:27:22,480
want to avoid

00:27:22,799 --> 00:27:26,240
the number of system called to to do

00:27:25,520 --> 00:27:28,640
this uh

00:27:26,240 --> 00:27:30,399
and map blocks uh acquisition and

00:27:28,640 --> 00:27:32,720
transfer of the page from the receive q

00:27:30,399 --> 00:27:32,720
to the

00:27:32,840 --> 00:27:39,360
user

00:27:35,679 --> 00:27:42,640
another issue we had was to because

00:27:39,360 --> 00:27:43,120
many rpc are not exactly a multiple of

00:27:42,640 --> 00:27:46,320
4k

00:27:43,120 --> 00:27:48,559
right the very

00:27:46,320 --> 00:27:49,440
often the last segment is incomplete

00:27:48,559 --> 00:27:52,640
it's not fully

00:27:49,440 --> 00:27:56,159
forked but arbitrary number of lights

00:27:52,640 --> 00:27:59,840
and so we had to implement uh

00:27:56,159 --> 00:28:00,799
in the zero copy system core a way to

00:27:59,840 --> 00:28:04,080
transfer

00:28:00,799 --> 00:28:08,080
this reminder this 4k

00:28:04,080 --> 00:28:10,320
up to 4k minus 1 bytes

00:28:08,080 --> 00:28:11,520
and not having to do a second system

00:28:10,320 --> 00:28:15,120
called receive message

00:28:11,520 --> 00:28:15,120
to only get this content

00:28:15,200 --> 00:28:21,039
um we also implemented the

00:28:18,480 --> 00:28:21,600
vm insert pages meaning that we could

00:28:21,039 --> 00:28:24,399
insert

00:28:21,600 --> 00:28:28,799
a multiple by page at once and to reduce

00:28:24,399 --> 00:28:28,799
the pte spin acquisition of cost

00:28:28,960 --> 00:28:32,000
another uh performance issue was that

00:28:31,679 --> 00:28:35,039
the

00:28:32,000 --> 00:28:38,480
original map operation and tcp i did

00:28:35,039 --> 00:28:39,679
two years ago was um a combined system

00:28:38,480 --> 00:28:42,159
called trying to

00:28:39,679 --> 00:28:43,120
unmap the prior content and then map the

00:28:42,159 --> 00:28:45,440
new one

00:28:43,120 --> 00:28:46,320
and it turns out that unmapping the

00:28:45,440 --> 00:28:49,200
prior content

00:28:46,320 --> 00:28:50,399
while holding the socket a lot is not

00:28:49,200 --> 00:28:53,440
very good because

00:28:50,399 --> 00:28:56,240
it prevents bottom half handler to

00:28:53,440 --> 00:28:57,440
feed more packets into your socket while

00:28:56,240 --> 00:29:00,240
you are doing this

00:28:57,440 --> 00:29:03,840
potentially expensive mm operation

00:29:00,240 --> 00:29:03,840
involving tlb invalidation

00:29:06,159 --> 00:29:13,600
so a bit of credits here um

00:29:10,480 --> 00:29:16,640
eric andy bukomovsky did the

00:29:13,600 --> 00:29:17,360
first implementation and arjun and

00:29:16,640 --> 00:29:21,120
sohail

00:29:17,360 --> 00:29:26,159
did many improvements in this

00:29:21,120 --> 00:29:28,480
memory this tcp zero copy and stuff uh

00:29:26,159 --> 00:29:31,279
some of them are not yet upstream but

00:29:28,480 --> 00:29:34,240
hopefully they will in the

00:29:31,279 --> 00:29:34,240
in the near future

00:29:36,880 --> 00:29:41,840
that's it time for questions now

00:29:43,760 --> 00:29:49,120
okay um thanks eric

00:29:46,880 --> 00:29:50,240
so um yes if you want to uh read

00:29:49,120 --> 00:29:52,720
something amusing

00:29:50,240 --> 00:29:54,080
go back in time and look at some of

00:29:52,720 --> 00:29:57,919
linus comments

00:29:54,080 --> 00:30:01,200
when people were proposing page flipping

00:29:57,919 --> 00:30:04,559
as a way to do zero copy so the idea was

00:30:01,200 --> 00:30:07,679
uh reading to uh some virtual address

00:30:04,559 --> 00:30:09,200
uh size of a page we would flip it flip

00:30:07,679 --> 00:30:12,000
the physical address

00:30:09,200 --> 00:30:12,799
and lightness as arguments were kind of

00:30:12,000 --> 00:30:16,000
interesting

00:30:12,799 --> 00:30:16,640
um in a very animated way uh the obvious

00:30:16,000 --> 00:30:18,240
tlb

00:30:16,640 --> 00:30:19,919
issues and things like that but one of

00:30:18,240 --> 00:30:23,840
the things that

00:30:19,919 --> 00:30:26,559
i think you kind of mentioned when

00:30:23,840 --> 00:30:28,559
we do a copy uh say from the kernel to

00:30:26,559 --> 00:30:29,520
user space we're actually populating the

00:30:28,559 --> 00:30:31,840
cache

00:30:29,520 --> 00:30:34,000
with the data so if the application

00:30:31,840 --> 00:30:37,120
turns around and accesses the data

00:30:34,000 --> 00:30:38,799
then um there was no there was no loss

00:30:37,120 --> 00:30:40,080
in doing the copy because you really get

00:30:38,799 --> 00:30:43,520
in the

00:30:40,080 --> 00:30:45,600
cash population so i'm wondering

00:30:43,520 --> 00:30:47,679
in this case it's very clear if there's

00:30:45,600 --> 00:30:50,720
no access in user space

00:30:47,679 --> 00:30:53,200
it's a win but then the question is what

00:30:50,720 --> 00:30:54,000
what sort of applications benefit from

00:30:53,200 --> 00:30:56,960
this

00:30:54,000 --> 00:31:05,120
that don't access the data after they

00:30:56,960 --> 00:31:08,320
get it from the kernel

00:31:05,120 --> 00:31:11,360
i'm sorry i had some kind of uh audio

00:31:08,320 --> 00:31:13,279
breakage so okay so uh

00:31:11,360 --> 00:31:14,640
what's what's the application for zero

00:31:13,279 --> 00:31:19,440
copy that uh

00:31:14,640 --> 00:31:21,760
is getting benefit um

00:31:19,440 --> 00:31:23,440
so i'm not sure i can comment on this uh

00:31:21,760 --> 00:31:25,679
really i i try to

00:31:23,440 --> 00:31:26,559
make uh some very generic uh

00:31:25,679 --> 00:31:30,159
presentation

00:31:26,559 --> 00:31:32,480
i can't really say anything about

00:31:30,159 --> 00:31:33,760
what are the actual use at google for

00:31:32,480 --> 00:31:36,960
this technology

00:31:33,760 --> 00:31:38,799
yes i'm sorry but but there's the

00:31:36,960 --> 00:31:41,919
general use cases right which is

00:31:38,799 --> 00:31:44,559
storage is one as uh yeah

00:31:41,919 --> 00:31:44,559
i really think

00:31:45,120 --> 00:31:48,640
we we want the technology to enable a

00:31:47,440 --> 00:31:52,240
fully zero copy

00:31:48,640 --> 00:31:53,600
uh behavior like between a nick and

00:31:52,240 --> 00:31:55,120
between a storage unit

00:31:53,600 --> 00:31:57,519
something like that if you can really

00:31:55,120 --> 00:31:59,919
avoid

00:31:57,519 --> 00:32:01,279
the copy from kernel to user and then

00:31:59,919 --> 00:32:05,519
kernel to you to

00:32:01,279 --> 00:32:07,200
user to camera that's a huge win uh

00:32:05,519 --> 00:32:08,880
and i wanted to add a comment that i

00:32:07,200 --> 00:32:11,200
reckon

00:32:08,880 --> 00:32:12,399
tom that the the caching the cache

00:32:11,200 --> 00:32:14,000
effect is actually

00:32:12,399 --> 00:32:16,159
obviously very real but it's also

00:32:14,000 --> 00:32:18,559
temporal right if you are

00:32:16,159 --> 00:32:20,080
pulling in like a megabyte of data yes

00:32:18,559 --> 00:32:21,840
if you touch it right away that's good

00:32:20,080 --> 00:32:24,080
but if in time

00:32:21,840 --> 00:32:25,600
you've done it later your cache locality

00:32:24,080 --> 00:32:28,640
has already been moved past

00:32:25,600 --> 00:32:29,600
so just because i'm using the data right

00:32:28,640 --> 00:32:32,080
after

00:32:29,600 --> 00:32:34,080
is not sufficient it also needs to be

00:32:32,080 --> 00:32:35,760
while the cache is still hot

00:32:34,080 --> 00:32:38,159
which may or may not be true because

00:32:35,760 --> 00:32:40,840
your your transfer patterns and your

00:32:38,159 --> 00:32:42,399
application patterns may or may not

00:32:40,840 --> 00:32:46,720
overlap

00:32:42,399 --> 00:32:49,039
uh yes it's obviously a complex issue um

00:32:46,720 --> 00:32:50,960
i think the the argument that uh reading

00:32:49,039 --> 00:32:52,640
into user space and then turning around

00:32:50,960 --> 00:32:55,919
and sending it back into the kernel with

00:32:52,640 --> 00:32:57,600
what splice is pretty compelling um so

00:32:55,919 --> 00:32:59,200
let's see we have a couple questions

00:32:57,600 --> 00:33:01,760
about uh

00:32:59,200 --> 00:33:02,720
page allocation so why limit orders your

00:33:01,760 --> 00:33:04,399
pages

00:33:02,720 --> 00:33:08,240
uh there was quite a bit of discussion

00:33:04,399 --> 00:33:11,760
on this but maybe we can summarize

00:33:08,240 --> 00:33:13,840
yeah so really the having having worked

00:33:11,760 --> 00:33:15,440
for more than eight years at google i

00:33:13,840 --> 00:33:18,799
can tell that

00:33:15,440 --> 00:33:21,519
um high order allocation don't do that

00:33:18,799 --> 00:33:22,159
that's that's guaranteed to fail at some

00:33:21,519 --> 00:33:25,039
point

00:33:22,159 --> 00:33:26,480
so you don't do that or you don't let

00:33:25,039 --> 00:33:30,799
your friends do that

00:33:26,480 --> 00:33:34,240
really okay so that's pretty

00:33:30,799 --> 00:33:35,039
uh clear advice so uh i'll assume we'll

00:33:34,240 --> 00:33:39,120
follow it

00:33:35,039 --> 00:33:41,279
uh xdp so um

00:33:39,120 --> 00:33:43,200
i guess there's two questions here so so

00:33:41,279 --> 00:33:46,399
first of all obviously you want this to

00:33:43,200 --> 00:33:49,200
be compatible with xdp but then

00:33:46,399 --> 00:33:50,559
follow-up would be how can xdp help or

00:33:49,200 --> 00:33:53,279
or

00:33:50,559 --> 00:33:55,679
would it have any uh place in in helping

00:33:53,279 --> 00:33:55,679
with this

00:33:56,480 --> 00:34:00,640
yeah so definitely the the the hdp

00:33:59,039 --> 00:34:02,399
program has been a problem for me

00:34:00,640 --> 00:34:03,519
because uh when i was working for

00:34:02,399 --> 00:34:06,799
example on the menu

00:34:03,519 --> 00:34:09,359
driver i was clearly

00:34:06,799 --> 00:34:11,839
in the middle of the xzp way of handling

00:34:09,359 --> 00:34:13,760
the receiver packet so

00:34:11,839 --> 00:34:16,480
i could not really upstream my work

00:34:13,760 --> 00:34:18,320
because uh there was quite a lot of work

00:34:16,480 --> 00:34:21,440
in xdp land at that time

00:34:18,320 --> 00:34:21,440
and so it means that

00:34:21,679 --> 00:34:28,879
the current way of xdp being

00:34:25,839 --> 00:34:32,320
you know deep inside the driver

00:34:28,879 --> 00:34:36,159
makes anything

00:34:32,320 --> 00:34:39,280
any new development quite harder now so

00:34:36,159 --> 00:34:42,879
i was hoping the xdp situation was

00:34:39,280 --> 00:34:44,320
uh about to be stabilized

00:34:42,879 --> 00:34:47,280
because there are definitely efforts

00:34:44,320 --> 00:34:50,720
trying to cope with multi-segment

00:34:47,280 --> 00:34:53,040
packets so hopefully we get that and we

00:34:50,720 --> 00:34:55,359
can upstream this

00:34:53,040 --> 00:34:55,359
patch

00:34:56,879 --> 00:35:05,359
okay let's see um

00:35:03,119 --> 00:35:06,400
so the the interesting thing about the

00:35:05,359 --> 00:35:09,599
86 byte

00:35:06,400 --> 00:35:12,000
um clearly that was a

00:35:09,599 --> 00:35:13,520
kind of a heuristic right and in the

00:35:12,000 --> 00:35:15,839
general case

00:35:13,520 --> 00:35:18,240
obviously not all sites can assume that

00:35:15,839 --> 00:35:20,640
there's a single packet size

00:35:18,240 --> 00:35:21,680
uh do you have any insights on on how to

00:35:20,640 --> 00:35:22,960
make that generic

00:35:21,680 --> 00:35:25,359
it seems like we have to change the

00:35:22,960 --> 00:35:28,640
hardware to to be able to parse packets

00:35:25,359 --> 00:35:28,640
i'm not sure there's a way around that

00:35:29,359 --> 00:35:34,320
uh i think uh if you want something in

00:35:31,760 --> 00:35:36,480
generic then you need some kind of uh

00:35:34,320 --> 00:35:37,680
header split header data split

00:35:36,480 --> 00:35:40,640
underneath

00:35:37,680 --> 00:35:41,760
otherwise you are going to use some

00:35:40,640 --> 00:35:45,760
tricks we did

00:35:41,760 --> 00:35:47,920
uh with this melanox nick

00:35:45,760 --> 00:35:50,000
obviously for us it was not a big deal

00:35:47,920 --> 00:35:52,720
because everything has happened v6 since

00:35:50,000 --> 00:35:54,880
uh i don't know two years or more so

00:35:52,720 --> 00:35:54,880
it's

00:35:56,960 --> 00:36:00,400
there was no real heuristic to take for

00:35:59,599 --> 00:36:03,520
us so

00:36:00,400 --> 00:36:03,520
that was quite easy

00:36:04,640 --> 00:36:09,200
okay uh so i haven't seen any hands

00:36:08,160 --> 00:36:12,320
being raised

00:36:09,200 --> 00:36:17,040
um so thank you let's go on

00:36:12,320 --> 00:36:17,040

YouTube URL: https://www.youtube.com/watch?v=_ZfiQGWFvg0


