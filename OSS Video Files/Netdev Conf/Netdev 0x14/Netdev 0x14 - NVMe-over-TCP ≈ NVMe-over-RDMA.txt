Title: Netdev 0x14 - NVMe-over-TCP â‰ˆ NVMe-over-RDMA
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Rachit Agarwal, Qizhe Cai, Jaehyun Hwang

More info: https://netdevconf.info/0x14/session.html?talk-NVMe-over-TCP-is-not-NVMe-over-RDMA

Date: Wednesday, August 19, 2020

Networks are getting faster but so is storage. It is not uncommon anymore
for a high performance storage device to deliver as much as a million
read/write operations per second. For traditional remote storage accesses
CPU requirements have become unsustainable. The recently merged NVMe-over-TCP
has improved performance lowering CPU utilization. However, despite of its
advantages of being able to use commodity NICs, NVMe-over-TCP's
performance is still significantly worse than NVMe-over-RDMA.

Rachit Agarwal describes some new ideas on how to significantly
improve the status quo. He will discuss a design, implementation
and evaluation of several new ideas within
NVMe-over-TCP. Their design maintains all the desirable
properties of NVMe-over-TCP (unmodified applications,
unmodified TCP/IP stack, etc.), and yet, saturates a
100Gbps link for remote accesses using CPU utilization
similar to state-of-the-art user-space and RDMA-based
solutions.
Captions: 
	00:00:02,879 --> 00:00:08,080
thank you everybody for coming over

00:00:04,640 --> 00:00:10,960
um i am going to present this talk

00:00:08,080 --> 00:00:12,240
which is uh a work that we started

00:00:10,960 --> 00:00:14,480
around two years ago

00:00:12,240 --> 00:00:16,800
uh and it was at one of the academic

00:00:14,480 --> 00:00:20,480
conferences this year which is called

00:00:16,800 --> 00:00:20,800
nsti uh design implementation our system

00:00:20,480 --> 00:00:24,480
for

00:00:20,800 --> 00:00:27,279
for many reasons is is called i10

00:00:24,480 --> 00:00:28,960
but if you want to remember something uh

00:00:27,279 --> 00:00:32,239
to take away i think what i'm going to

00:00:28,960 --> 00:00:35,840
talk about is really our attempt

00:00:32,239 --> 00:00:39,280
to meet the throughput parkour

00:00:35,840 --> 00:00:42,320
of nvme or rdma using um

00:00:39,280 --> 00:00:43,520
using internal techniques without

00:00:42,320 --> 00:00:45,520
changing any applications

00:00:43,520 --> 00:00:46,640
or without changing any hardware

00:00:45,520 --> 00:00:49,440
infrastructure

00:00:46,640 --> 00:00:50,879
um most of the work was done by my

00:00:49,440 --> 00:00:53,920
postdoc xiaomi

00:00:50,879 --> 00:00:56,480
and gigi kai who is a phd student

00:00:53,920 --> 00:00:58,239
here at cornell university um i'm going

00:00:56,480 --> 00:00:59,359
to start with a little bit of motivation

00:00:58,239 --> 00:01:01,440
i think this is

00:00:59,359 --> 00:01:02,399
almost well known to everybody but let

00:01:01,440 --> 00:01:04,400
me give

00:01:02,399 --> 00:01:06,479
a little bit of motivation on why we

00:01:04,400 --> 00:01:08,640
start thinking about this work so

00:01:06,479 --> 00:01:09,760
i think we are motivated by two main

00:01:08,640 --> 00:01:13,280
hardware trends

00:01:09,760 --> 00:01:16,080
uh the first one is if you look at

00:01:13,280 --> 00:01:16,640
a typical data center deployment today

00:01:16,080 --> 00:01:19,520
um

00:01:16,640 --> 00:01:21,119
we have multiple servers uh each of

00:01:19,520 --> 00:01:23,119
which might have certain amount of cpu

00:01:21,119 --> 00:01:23,759
certain amount of ssd certain amount of

00:01:23,119 --> 00:01:27,360
ram

00:01:23,759 --> 00:01:30,000
um all connected within a rack uh

00:01:27,360 --> 00:01:30,799
using some intra-rack network fabric and

00:01:30,000 --> 00:01:32,960
this could

00:01:30,799 --> 00:01:34,799
probably be a single tar switch or it

00:01:32,960 --> 00:01:35,840
could be more complicated in many

00:01:34,799 --> 00:01:38,320
deployments

00:01:35,840 --> 00:01:38,960
so the two emerging trends that we

00:01:38,320 --> 00:01:42,240
noticed

00:01:38,960 --> 00:01:44,399
as jamal was pointing out earlier is

00:01:42,240 --> 00:01:45,759
that we have these really fast and

00:01:44,399 --> 00:01:48,320
nonvertile memory

00:01:45,759 --> 00:01:50,399
solid state drives that can now support

00:01:48,320 --> 00:01:53,600
as many as 1 million iops

00:01:50,399 --> 00:01:54,479
for reits and almost 400k i ops for

00:01:53,600 --> 00:01:57,920
rights

00:01:54,479 --> 00:01:59,439
um and on the other hand

00:01:57,920 --> 00:02:01,600
we are also seeing if you look at the

00:01:59,439 --> 00:02:02,479
links connecting the servers to the

00:02:01,600 --> 00:02:04,159
network

00:02:02,479 --> 00:02:06,399
uh we see that there has been a

00:02:04,159 --> 00:02:09,599
continuous increase of network bandwidth

00:02:06,399 --> 00:02:12,319
in fact a lot of data center providers

00:02:09,599 --> 00:02:15,120
are already deploying hundred gps links

00:02:12,319 --> 00:02:16,319
what that really means is we are talking

00:02:15,120 --> 00:02:18,959
about

00:02:16,319 --> 00:02:19,680
um quite a few iops going through the

00:02:18,959 --> 00:02:22,879
network

00:02:19,680 --> 00:02:25,920
um or or the bandwidth uh uh

00:02:22,879 --> 00:02:27,200
being very high means uh a lot of the

00:02:25,920 --> 00:02:30,720
hardware can support

00:02:27,200 --> 00:02:31,120
uh very high throughput um and since

00:02:30,720 --> 00:02:33,920
these

00:02:31,120 --> 00:02:35,680
uh solid state drives and and links have

00:02:33,920 --> 00:02:37,920
gotten much much

00:02:35,680 --> 00:02:38,800
faster or higher performance the

00:02:37,920 --> 00:02:40,800
bottlenecks have

00:02:38,800 --> 00:02:42,560
have been pushed back to the software

00:02:40,800 --> 00:02:43,920
stack i think this is this comes as no

00:02:42,560 --> 00:02:46,800
surprise to anybody

00:02:43,920 --> 00:02:48,800
the second trend which is perhaps one of

00:02:46,800 --> 00:02:51,280
the more motivating factors for the work

00:02:48,800 --> 00:02:53,280
is the strength about resource

00:02:51,280 --> 00:02:55,519
disaggregation which is basically

00:02:53,280 --> 00:02:57,680
a lot of applications are now accessing

00:02:55,519 --> 00:02:59,840
data over storage devices that are

00:02:57,680 --> 00:03:01,760
typically disaggregated from compute

00:02:59,840 --> 00:03:02,959
and this is happening in many many forms

00:03:01,760 --> 00:03:04,640
and hence

00:03:02,959 --> 00:03:06,239
a lot of requests are actually going

00:03:04,640 --> 00:03:08,159
over the network uh

00:03:06,239 --> 00:03:09,680
from the compute side to the storage

00:03:08,159 --> 00:03:11,599
side um

00:03:09,680 --> 00:03:13,280
which means that there's an increasing

00:03:11,599 --> 00:03:14,560
overlap between the storage and network

00:03:13,280 --> 00:03:16,879
data paths

00:03:14,560 --> 00:03:18,239
so once we put these two together we can

00:03:16,879 --> 00:03:21,360
start seeing uh

00:03:18,239 --> 00:03:22,159
some interesting implications of these

00:03:21,360 --> 00:03:25,599
trends

00:03:22,159 --> 00:03:27,040
um and i think

00:03:25,599 --> 00:03:28,640
these trends are not something that we

00:03:27,040 --> 00:03:30,400
identified a lot of people have already

00:03:28,640 --> 00:03:32,080
been thinking about these problems

00:03:30,400 --> 00:03:33,920
for example there's been a lot of work

00:03:32,080 --> 00:03:34,720
in in the academic world and i think in

00:03:33,920 --> 00:03:39,360
in the

00:03:34,720 --> 00:03:43,120
um in industry as well on uh

00:03:39,360 --> 00:03:46,000
pushing the entire stack

00:03:43,120 --> 00:03:47,360
both the user and sorry both storage and

00:03:46,000 --> 00:03:49,599
the network stack to that

00:03:47,360 --> 00:03:51,680
user space and saying that hey kernel

00:03:49,599 --> 00:03:52,640
was just not designed for this kind of

00:03:51,680 --> 00:03:55,439
performance

00:03:52,640 --> 00:03:56,159
um and they use a variety of techniques

00:03:55,439 --> 00:03:58,319
to get

00:03:56,159 --> 00:04:00,560
really really high performance so these

00:03:58,319 --> 00:04:03,680
user space storage and network stocks

00:04:00,560 --> 00:04:05,040
can get uh typically uh sustained very

00:04:03,680 --> 00:04:07,280
high throughput

00:04:05,040 --> 00:04:08,080
um and like jamal was pointing out

00:04:07,280 --> 00:04:09,599
earlier

00:04:08,080 --> 00:04:11,840
uh usually you have to rewrite your

00:04:09,599 --> 00:04:12,959
applications or uh you know there's

00:04:11,840 --> 00:04:15,760
there's some

00:04:12,959 --> 00:04:16,639
uh modulo modulo that these stacks are

00:04:15,760 --> 00:04:19,120
being developed

00:04:16,639 --> 00:04:19,840
still and there are still some uh tricky

00:04:19,120 --> 00:04:22,240
parts in

00:04:19,840 --> 00:04:23,199
in really large scale deployments the

00:04:22,240 --> 00:04:26,240
second side

00:04:23,199 --> 00:04:28,160
is you can take uh your stack which is

00:04:26,240 --> 00:04:28,560
the network side of the stack and push

00:04:28,160 --> 00:04:31,120
it

00:04:28,560 --> 00:04:32,800
to hardware so there's a lot of work on

00:04:31,120 --> 00:04:35,600
nvme over rdma

00:04:32,800 --> 00:04:37,440
where we keep the storage stack in the

00:04:35,600 --> 00:04:39,759
network in the kernel and the network

00:04:37,440 --> 00:04:42,160
stack is pushed out to the hardware

00:04:39,759 --> 00:04:46,000
and the idea is that a lot of network

00:04:42,160 --> 00:04:48,560
stack related overheads can be avoided

00:04:46,000 --> 00:04:49,199
again you can get very high performance

00:04:48,560 --> 00:04:50,560
and

00:04:49,199 --> 00:04:52,320
almost meet the performance of user

00:04:50,560 --> 00:04:54,720
space tax

00:04:52,320 --> 00:04:55,600
using this hardware but again you

00:04:54,720 --> 00:04:58,080
require some

00:04:55,600 --> 00:04:59,759
specialized hardware and somewhere in

00:04:58,080 --> 00:05:03,120
the middle is uh

00:04:59,759 --> 00:05:05,199
what uh a lot of work uh recent work has

00:05:03,120 --> 00:05:07,520
been happening over and gave me over tcp

00:05:05,199 --> 00:05:10,320
where we keep everything in our favorite

00:05:07,520 --> 00:05:11,919
kernel and we say that uh

00:05:10,320 --> 00:05:13,759
today at least we have to give up on

00:05:11,919 --> 00:05:15,360
performance and if you

00:05:13,759 --> 00:05:16,960
i'll show you some numbers later on

00:05:15,360 --> 00:05:19,360
during the talk um

00:05:16,960 --> 00:05:20,080
today's nvme over tcp performance can be

00:05:19,360 --> 00:05:22,400
roughly

00:05:20,080 --> 00:05:24,800
two to three x worse compared to user

00:05:22,400 --> 00:05:26,320
space tax or nvme over rdma

00:05:24,800 --> 00:05:28,800
and this is in terms of throughput per

00:05:26,320 --> 00:05:30,720
core so which means throughput per core

00:05:28,800 --> 00:05:32,880
could be two to three times worse so

00:05:30,720 --> 00:05:34,560
you have two ways to look at it that

00:05:32,880 --> 00:05:35,440
either i have to give up two to three x

00:05:34,560 --> 00:05:38,720
more cpu

00:05:35,440 --> 00:05:41,919
cycles or i i'm happy with the

00:05:38,720 --> 00:05:42,560
lower throughput and the question really

00:05:41,919 --> 00:05:45,840
was

00:05:42,560 --> 00:05:47,280
is this really fundamental uh do

00:05:45,840 --> 00:05:49,440
do we have to really give up this

00:05:47,280 --> 00:05:52,080
performance for for staying

00:05:49,440 --> 00:05:53,120
with the traditional uh let's kind of

00:05:52,080 --> 00:05:54,880
stack

00:05:53,120 --> 00:05:56,720
so that's the question that drove us

00:05:54,880 --> 00:05:59,280
initially um

00:05:56,720 --> 00:06:00,800
and then we started digging deeper that

00:05:59,280 --> 00:06:02,319
where are these performance gaps coming

00:06:00,800 --> 00:06:05,840
from so let me show you some

00:06:02,319 --> 00:06:07,440
uh basic uh cpu profiling results so we

00:06:05,840 --> 00:06:08,000
actually profiled and removed tcp and

00:06:07,440 --> 00:06:09,759
then we um

00:06:08,000 --> 00:06:11,199
rdma i'll show you some of the setup

00:06:09,759 --> 00:06:12,240
later on but let me tell you what the

00:06:11,199 --> 00:06:14,880
x-axis is

00:06:12,240 --> 00:06:16,160
so there's some application running we

00:06:14,880 --> 00:06:18,880
have the block layer

00:06:16,160 --> 00:06:20,960
which i'm dividing into two uh different

00:06:18,880 --> 00:06:22,639
aspects block tx and block rx which is

00:06:20,960 --> 00:06:24,319
the processing the request and the

00:06:22,639 --> 00:06:26,639
responses

00:06:24,319 --> 00:06:27,680
and network tx and network rx similarly

00:06:26,639 --> 00:06:29,280
and then there is some

00:06:27,680 --> 00:06:31,199
others which is basically scheduling

00:06:29,280 --> 00:06:33,280
overheads and everything coming

00:06:31,199 --> 00:06:34,960
so if you look at nvme over tcp so this

00:06:33,280 --> 00:06:35,199
is i'm going to call it storage stack

00:06:34,960 --> 00:06:37,039
and

00:06:35,199 --> 00:06:38,800
this is the network stack if you look at

00:06:37,039 --> 00:06:40,319
nvme over tcp most of the cycles it

00:06:38,800 --> 00:06:43,120
should not be surprising are going into

00:06:40,319 --> 00:06:46,560
network processing

00:06:43,120 --> 00:06:48,000
and if i superimpose net nvme over rdma

00:06:46,560 --> 00:06:51,120
here then you start seeing interesting

00:06:48,000 --> 00:06:52,800
trends nvme over rdma essentially does

00:06:51,120 --> 00:06:54,720
exactly what it is supposed to do it

00:06:52,800 --> 00:06:57,599
avoids the cycles that you spend

00:06:54,720 --> 00:06:58,240
on the network processing right and it

00:06:57,599 --> 00:07:01,520
allows

00:06:58,240 --> 00:07:02,800
applications and the block layer to

00:07:01,520 --> 00:07:05,440
use those cycles to get higher

00:07:02,800 --> 00:07:06,160
throughput this is essentially what nvme

00:07:05,440 --> 00:07:07,919
or rdm is

00:07:06,160 --> 00:07:09,280
supposed to be doing and we can see very

00:07:07,919 --> 00:07:11,680
clear numbers that

00:07:09,280 --> 00:07:14,000
this roughly matches the throughput that

00:07:11,680 --> 00:07:17,120
improvements that you get

00:07:14,000 --> 00:07:18,319
good so this is nice so somehow we have

00:07:17,120 --> 00:07:20,960
to bridge this gap

00:07:18,319 --> 00:07:21,440
uh for uh existing stacks so there's

00:07:20,960 --> 00:07:23,440
some

00:07:21,440 --> 00:07:25,280
crossing over it and then there is a

00:07:23,440 --> 00:07:28,000
more implementation level detail that in

00:07:25,280 --> 00:07:30,319
nvme or tcp you are using two threads

00:07:28,000 --> 00:07:31,759
um one at the block layer one of the tcp

00:07:30,319 --> 00:07:34,080
layer or the nvme

00:07:31,759 --> 00:07:35,440
layer uh and in the medium or rdma you

00:07:34,080 --> 00:07:36,639
don't have any context switches so

00:07:35,440 --> 00:07:37,360
there's some context switching overhead

00:07:36,639 --> 00:07:40,240
coming

00:07:37,360 --> 00:07:40,800
uh in nvme or tcp right so these are the

00:07:40,240 --> 00:07:42,720
two

00:07:40,800 --> 00:07:45,120
specific overheads that lead to this

00:07:42,720 --> 00:07:48,240
performance gap

00:07:45,120 --> 00:07:50,879
good so uh what is it that

00:07:48,240 --> 00:07:53,120
if you want to take away one thing from

00:07:50,879 --> 00:07:56,160
the stock here is uh here's what

00:07:53,120 --> 00:07:56,879
would be fantastic to take away uh one

00:07:56,160 --> 00:07:59,919
is

00:07:56,879 --> 00:08:01,680
that with very very uh

00:07:59,919 --> 00:08:03,120
simple modifications it's actually

00:08:01,680 --> 00:08:05,440
possible to

00:08:03,120 --> 00:08:06,319
get throughput parkour similar to nvme

00:08:05,440 --> 00:08:09,440
over rdma

00:08:06,319 --> 00:08:10,960
um using what i'm calling extremely

00:08:09,440 --> 00:08:14,080
simple mechanisms

00:08:10,960 --> 00:08:15,840
and i hope all of you would agree that

00:08:14,080 --> 00:08:17,599
what we are doing is really nothing

00:08:15,840 --> 00:08:19,680
other than batching uh we are just

00:08:17,599 --> 00:08:23,759
finding the right way to batch

00:08:19,680 --> 00:08:26,080
things and that leads to um

00:08:23,759 --> 00:08:27,199
nvme over tcp and extensions what we are

00:08:26,080 --> 00:08:28,400
calling i10 as the

00:08:27,199 --> 00:08:30,879
chief throughput per core center and

00:08:28,400 --> 00:08:32,479
gaming or rdma we don't we are not using

00:08:30,879 --> 00:08:33,279
any specialized functionality from

00:08:32,479 --> 00:08:35,440
hardware

00:08:33,279 --> 00:08:36,640
uh and what i'm going to tell you is

00:08:35,440 --> 00:08:38,479
three very sim

00:08:36,640 --> 00:08:40,240
very simple techniques that i'm going to

00:08:38,479 --> 00:08:42,399
break into i-10 lane

00:08:40,240 --> 00:08:43,599
caravans and the first one is simply

00:08:42,399 --> 00:08:45,200
saying that we should have dedicated

00:08:43,599 --> 00:08:45,920
resources for individual cores and

00:08:45,200 --> 00:08:48,240
requests

00:08:45,920 --> 00:08:50,160
generated at each core caravans is one

00:08:48,240 --> 00:08:52,640
way of batching requests and

00:08:50,160 --> 00:08:53,839
data packets response packets uh and

00:08:52,640 --> 00:08:55,200
then there is the idea of delayed

00:08:53,839 --> 00:08:57,839
doorbells which is

00:08:55,200 --> 00:08:59,760
uh just interrupt coalescing right and

00:08:57,839 --> 00:09:02,160
i'll i'll try to give you some insights

00:08:59,760 --> 00:09:04,240
about how the first two are reducing the

00:09:02,160 --> 00:09:06,080
network processing overheads and the

00:09:04,240 --> 00:09:07,600
last one is trying to reduce the context

00:09:06,080 --> 00:09:08,080
switching overs the two overheads that i

00:09:07,600 --> 00:09:10,240
talked about

00:09:08,080 --> 00:09:11,200
in the last slide so if i showed you

00:09:10,240 --> 00:09:13,519
just this

00:09:11,200 --> 00:09:15,360
it would not be a complete picture uh

00:09:13,519 --> 00:09:17,519
here i'm only talking about throughput

00:09:15,360 --> 00:09:18,240
parkour of course nvme or rdma is much

00:09:17,519 --> 00:09:20,720
more than

00:09:18,240 --> 00:09:21,440
uh or rdma in general is more than just

00:09:20,720 --> 00:09:24,959
throughput

00:09:21,440 --> 00:09:28,080
um what what is it that is still missing

00:09:24,959 --> 00:09:30,240
um our tail latency which means that

00:09:28,080 --> 00:09:32,240
once you go through the network or the

00:09:30,240 --> 00:09:34,399
linux kernel stack including

00:09:32,240 --> 00:09:36,959
nvme over tcp or item uh your tail

00:09:34,399 --> 00:09:38,720
latency is roughly 117 microseconds

00:09:36,959 --> 00:09:41,040
higher than nvme over rdma

00:09:38,720 --> 00:09:41,839
so this is what i am calling the real

00:09:41,040 --> 00:09:43,519
overhead if

00:09:41,839 --> 00:09:45,360
if you really want to think about what

00:09:43,519 --> 00:09:46,640
is the gap between and gaming over tcp

00:09:45,360 --> 00:09:48,240
and nvme or rdma

00:09:46,640 --> 00:09:50,160
we should be really thinking about

00:09:48,240 --> 00:09:51,519
latency as the core overhead not the

00:09:50,160 --> 00:09:55,519
throughput per core

00:09:51,519 --> 00:09:55,839
and um one question that one could ask

00:09:55,519 --> 00:09:58,000
is

00:09:55,839 --> 00:09:59,600
how how interesting this tail latency

00:09:58,000 --> 00:10:01,519
overhead is given that

00:09:59,600 --> 00:10:04,320
data center deployments typically have

00:10:01,519 --> 00:10:06,480
hundreds of microseconds of rtts

00:10:04,320 --> 00:10:08,079
on an average anyways but this is

00:10:06,480 --> 00:10:09,519
nevertheless an overhead

00:10:08,079 --> 00:10:10,959
so this is the slide if you want to take

00:10:09,519 --> 00:10:12,560
away anything this is the slide that you

00:10:10,959 --> 00:10:14,240
want to take away that we can achieve

00:10:12,560 --> 00:10:16,480
throughput parkour using extremely

00:10:14,240 --> 00:10:19,839
simple techniques and there's some tail

00:10:16,480 --> 00:10:19,839
latency overhead

00:10:20,640 --> 00:10:24,800
good so let me show you one brief result

00:10:23,600 --> 00:10:26,320
and then i'll tell you a little bit

00:10:24,800 --> 00:10:27,200
about these very simple techniques so

00:10:26,320 --> 00:10:28,560
that we are doing

00:10:27,200 --> 00:10:30,560
uh and then we will dive deeper into

00:10:28,560 --> 00:10:32,079
evaluation as well so

00:10:30,560 --> 00:10:33,440
because the the previous flight might

00:10:32,079 --> 00:10:34,959
come as a surprise so let me show you

00:10:33,440 --> 00:10:36,399
some numbers so on the x-axis i'm

00:10:34,959 --> 00:10:39,040
increasing the load on the system

00:10:36,399 --> 00:10:41,360
uh a single core uh we are basically

00:10:39,040 --> 00:10:44,000
have two servers attached using 100 gps

00:10:41,360 --> 00:10:47,120
links and there is a target side where

00:10:44,000 --> 00:10:48,800
uh nvme ssd sitting uh and

00:10:47,120 --> 00:10:50,320
we are trying to measure trail latency

00:10:48,800 --> 00:10:52,959
and throughput parkour

00:10:50,320 --> 00:10:55,040
perfect um and i'm going to show you

00:10:52,959 --> 00:10:56,480
four systems one is nvme or tcp

00:10:55,040 --> 00:10:58,560
deployment when we were actually

00:10:56,480 --> 00:11:00,800
originally evaluating these results

00:10:58,560 --> 00:11:01,920
for our academic work around a year ago

00:11:00,800 --> 00:11:04,000
a year and a half ago

00:11:01,920 --> 00:11:05,600
nvme or tcp has improved that sagi will

00:11:04,000 --> 00:11:06,800
talk about later in the session

00:11:05,600 --> 00:11:08,640
and then i'm going to show you results

00:11:06,800 --> 00:11:12,800
while i'm giving you over rdma and night

00:11:08,640 --> 00:11:16,000
so uh nvme over tcp in 2019

00:11:12,800 --> 00:11:17,760
peaked out around at 100 um thousand

00:11:16,000 --> 00:11:21,279
iops

00:11:17,760 --> 00:11:24,000
iops um for for our setting

00:11:21,279 --> 00:11:25,920
um and remember this ssd can actually do

00:11:24,000 --> 00:11:28,480
roughly 700 iops

00:11:25,920 --> 00:11:30,640
read throughput the more recent version

00:11:28,480 --> 00:11:32,880
of nvme opcp improves it

00:11:30,640 --> 00:11:34,079
almost by 50 which is significant

00:11:32,880 --> 00:11:35,519
actually uh so

00:11:34,079 --> 00:11:38,720
they have been doing a lot of exciting

00:11:35,519 --> 00:11:41,279
work um and and your rdma

00:11:38,720 --> 00:11:42,480
um actually gets roughly 2x higher

00:11:41,279 --> 00:11:43,360
overhead that i was talking about

00:11:42,480 --> 00:11:45,839
earlier

00:11:43,360 --> 00:11:47,760
uh and whereas uh item sitting in this

00:11:45,839 --> 00:11:49,920
picture is roughly here

00:11:47,760 --> 00:11:51,600
which means like i said there's slightly

00:11:49,920 --> 00:11:53,120
higher latency uh but

00:11:51,600 --> 00:11:56,240
you can actually sustain roughly the

00:11:53,120 --> 00:11:57,440
same throughput or peak throughput uh as

00:11:56,240 --> 00:11:59,200
and give me your rdma

00:11:57,440 --> 00:12:00,560
there's some gap but i wouldn't think

00:11:59,200 --> 00:12:01,680
too much into that cap i don't think

00:12:00,560 --> 00:12:02,800
that's fundamental that's just

00:12:01,680 --> 00:12:04,480
evaluation settings

00:12:02,800 --> 00:12:06,480
uh but really the important thing is

00:12:04,480 --> 00:12:08,079
that we can meet and give your rdma kind

00:12:06,480 --> 00:12:11,200
of throughput at least for single core

00:12:08,079 --> 00:12:13,920
um and uh and

00:12:11,200 --> 00:12:14,880
uh there's some latency overhead so one

00:12:13,920 --> 00:12:17,680
might ask

00:12:14,880 --> 00:12:19,279
where is this lower part why is nvme or

00:12:17,680 --> 00:12:20,959
rdm is still taking 100 microsecond

00:12:19,279 --> 00:12:22,480
latency this is really ssd access

00:12:20,959 --> 00:12:25,279
latency

00:12:22,480 --> 00:12:27,040
right so what if i want to hide the ssd

00:12:25,279 --> 00:12:28,560
access latency what i'm going to show

00:12:27,040 --> 00:12:30,240
you another result which is

00:12:28,560 --> 00:12:32,160
i'm going to create a ram block device

00:12:30,240 --> 00:12:34,000
so that you know one could ask that hey

00:12:32,160 --> 00:12:35,040
what happens if ssd access latencies

00:12:34,000 --> 00:12:37,279
improve in future

00:12:35,040 --> 00:12:39,200
uh so let me show you another result uh

00:12:37,279 --> 00:12:40,959
exactly everything the same we just have

00:12:39,200 --> 00:12:41,920
now the same experiment running on ram

00:12:40,959 --> 00:12:45,360
block device

00:12:41,920 --> 00:12:47,440
um and uh here's what nvme or tcp is

00:12:45,360 --> 00:12:49,519
doing so slightly throughput improves

00:12:47,440 --> 00:12:50,560
the new and vimeo tcp has slightly

00:12:49,519 --> 00:12:52,720
improved throughput

00:12:50,560 --> 00:12:54,480
uh and then viewing our rdma here you

00:12:52,720 --> 00:12:56,240
see the real latency benefits of

00:12:54,480 --> 00:12:56,800
invading your rdma which is now we are

00:12:56,240 --> 00:12:58,639
peaking

00:12:56,800 --> 00:13:00,240
around 20 microsecond latency at the

00:12:58,639 --> 00:13:02,959
knee point um

00:13:00,240 --> 00:13:04,399
and this is where nvm where i10 is

00:13:02,959 --> 00:13:06,399
basically you still give up that 100

00:13:04,399 --> 00:13:07,760
microsecond latencies

00:13:06,399 --> 00:13:09,040
and you need roughly the throughput of

00:13:07,760 --> 00:13:10,959
family on your rdma this is all

00:13:09,040 --> 00:13:13,120
throughput per core

00:13:10,959 --> 00:13:14,880
good so the takeaway here is like i said

00:13:13,120 --> 00:13:16,399
throughput particular comparable to nvme

00:13:14,880 --> 00:13:19,519
or rdma can be achieved

00:13:16,399 --> 00:13:20,320
with the roughly uh 117 microsecond high

00:13:19,519 --> 00:13:22,399
temp latency

00:13:20,320 --> 00:13:23,360
so now let me spend 10 minutes on

00:13:22,399 --> 00:13:25,760
telling you

00:13:23,360 --> 00:13:27,360
uh quickly how we achieve these results

00:13:25,760 --> 00:13:28,000
uh and then i'll dive deeper into these

00:13:27,360 --> 00:13:29,519
those

00:13:28,000 --> 00:13:31,120
perfect so the first thing is very

00:13:29,519 --> 00:13:34,079
simple uh that

00:13:31,120 --> 00:13:36,079
hey somehow we want to avoid sharing

00:13:34,079 --> 00:13:37,120
resources across different core sending

00:13:36,079 --> 00:13:39,519
out different requests

00:13:37,120 --> 00:13:41,040
so what we are going to do is we are

00:13:39,519 --> 00:13:42,880
going to have

00:13:41,040 --> 00:13:44,079
let's say there is a single core

00:13:42,880 --> 00:13:46,880
applications are submitting their

00:13:44,079 --> 00:13:48,399
request using standard ios as calls

00:13:46,880 --> 00:13:50,079
and what we are going to do is we are

00:13:48,399 --> 00:13:52,079
going to have block multi queue is

00:13:50,079 --> 00:13:54,880
already parkour so that's fine

00:13:52,079 --> 00:13:56,160
uh i don't have to do anything uh shared

00:13:54,880 --> 00:13:58,000
across different course

00:13:56,160 --> 00:14:00,240
uh i'm going to have a dedicated item

00:13:58,000 --> 00:14:01,839
queue which is uh in the most recent and

00:14:00,240 --> 00:14:02,959
mma or tcp version this basically

00:14:01,839 --> 00:14:05,519
dedicated

00:14:02,959 --> 00:14:07,519
um nvme or tcp queues and then i will

00:14:05,519 --> 00:14:10,000
have dedicated tcp sockets

00:14:07,519 --> 00:14:11,519
uh on both the host side and on the left

00:14:10,000 --> 00:14:13,839
you see the target side

00:14:11,519 --> 00:14:15,360
um and then this becomes an item lane

00:14:13,839 --> 00:14:16,399
basically you have created a dedicated

00:14:15,360 --> 00:14:19,120
channel between

00:14:16,399 --> 00:14:20,480
between the host and the target um and

00:14:19,120 --> 00:14:21,040
you can send the request over these

00:14:20,480 --> 00:14:23,040
channels

00:14:21,040 --> 00:14:24,639
uh and the target will access and remove

00:14:23,040 --> 00:14:26,880
ssd and respond everything

00:14:24,639 --> 00:14:27,839
so every core basically at least the

00:14:26,880 --> 00:14:31,120
host side

00:14:27,839 --> 00:14:32,880
is completely separated out resources

00:14:31,120 --> 00:14:34,160
and nothing is shared so that's what we

00:14:32,880 --> 00:14:38,000
are calling a dedicated

00:14:34,160 --> 00:14:41,199
um pipe and if i had

00:14:38,000 --> 00:14:41,760
a different target with a red mvm ssd on

00:14:41,199 --> 00:14:43,760
the right

00:14:41,760 --> 00:14:45,040
what will happen here is that we don't

00:14:43,760 --> 00:14:47,839
want to share resources

00:14:45,040 --> 00:14:49,199
uh even across these two ssds so even if

00:14:47,839 --> 00:14:49,600
the requests are coming from the same

00:14:49,199 --> 00:14:51,680
core

00:14:49,600 --> 00:14:53,279
so we are going to create a separate

00:14:51,680 --> 00:14:54,560
pipe and the request

00:14:53,279 --> 00:14:58,000
the only thing that will be shared is

00:14:54,560 --> 00:14:59,600
block multi queue

00:14:58,000 --> 00:15:01,120
right so every resource every other

00:14:59,600 --> 00:15:02,959
resource is shared and

00:15:01,120 --> 00:15:04,639
there's some overhead in terms of memory

00:15:02,959 --> 00:15:06,160
or storage or memory overhead

00:15:04,639 --> 00:15:08,560
in terms of keeping some additional data

00:15:06,160 --> 00:15:10,959
structures but that's fine so with that

00:15:08,560 --> 00:15:12,480
dedicated resources are settled uh the

00:15:10,959 --> 00:15:14,000
only thing remaining is what how do we

00:15:12,480 --> 00:15:17,839
use these dedicated sources

00:15:14,000 --> 00:15:19,920
so i want you to focus on uh the

00:15:17,839 --> 00:15:22,880
middle part here so everything after the

00:15:19,920 --> 00:15:24,480
block layer so block multi-cue is has

00:15:22,880 --> 00:15:26,320
all the requests sitting that is coming

00:15:24,480 --> 00:15:29,360
from those that core uh

00:15:26,320 --> 00:15:32,800
the first thing that is happening is

00:15:29,360 --> 00:15:34,320
if you look at these i10 cues

00:15:32,800 --> 00:15:35,920
you would notice that all the requests

00:15:34,320 --> 00:15:37,759
sitting in that queue are going to the

00:15:35,920 --> 00:15:39,680
same destination over the same tcp

00:15:37,759 --> 00:15:42,240
socket over the same connection

00:15:39,680 --> 00:15:42,880
right so what we can actually do is the

00:15:42,240 --> 00:15:44,800
first thing

00:15:42,880 --> 00:15:46,240
very simple thing to do is batch these

00:15:44,800 --> 00:15:49,360
requests uh

00:15:46,240 --> 00:15:51,600
and uh at least for now we are batching

00:15:49,360 --> 00:15:55,199
them at 64 kilobyte granularity just to

00:15:51,600 --> 00:15:57,120
meet um just to meet some of the other

00:15:55,199 --> 00:15:59,040
functionalities i'll tell you later but

00:15:57,120 --> 00:16:01,199
you can create any size of batches

00:15:59,040 --> 00:16:03,199
and then you do one socket called per

00:16:01,199 --> 00:16:04,800
per batch right very simple technique

00:16:03,199 --> 00:16:06,880
nothing fancy happening here

00:16:04,800 --> 00:16:08,639
uh and the 64 kilobyte was chosen

00:16:06,880 --> 00:16:09,839
because it allows larger payloads up to

00:16:08,639 --> 00:16:12,880
64 kilobytes

00:16:09,839 --> 00:16:16,160
to use tso and then uh the nic process

00:16:12,880 --> 00:16:17,920
all its uh offloads and there are no cp

00:16:16,160 --> 00:16:21,120
cycles needed for packet segmentation

00:16:17,920 --> 00:16:23,839
right so we are just using tsogro good

00:16:21,120 --> 00:16:24,320
so that's one simple batching mechanism

00:16:23,839 --> 00:16:26,560
uh

00:16:24,320 --> 00:16:28,800
and i'll show you that it reduces the

00:16:26,560 --> 00:16:32,079
permit network crossing overheads uh

00:16:28,800 --> 00:16:33,839
uh at the host site this uh the second

00:16:32,079 --> 00:16:35,279
one is uh i talked about a little bit

00:16:33,839 --> 00:16:36,800
about the context switching overhead so

00:16:35,279 --> 00:16:37,920
let's see a little bit on the host side

00:16:36,800 --> 00:16:40,639
what is happening so

00:16:37,920 --> 00:16:41,360
in in traditional nvme or tcp what would

00:16:40,639 --> 00:16:44,320
happen is

00:16:41,360 --> 00:16:46,000
you receive a request and as soon as you

00:16:44,320 --> 00:16:48,399
receive a request here you

00:16:46,000 --> 00:16:49,120
do what is called you ring the doorbell

00:16:48,399 --> 00:16:50,880
um

00:16:49,120 --> 00:16:52,399
for the nvme associate device to know

00:16:50,880 --> 00:16:54,000
that there's a request now

00:16:52,399 --> 00:16:55,519
um and there's a context switch

00:16:54,000 --> 00:16:57,279
happening because there's a single

00:16:55,519 --> 00:17:00,160
thread processing these requests

00:16:57,279 --> 00:17:01,600
um and this request is individually

00:17:00,160 --> 00:17:03,040
processed by everybody and then you get

00:17:01,600 --> 00:17:05,199
the response you're done

00:17:03,040 --> 00:17:07,199
the the problem here is that you have

00:17:05,199 --> 00:17:09,039
high thread switching overheads uh

00:17:07,199 --> 00:17:10,319
and today i think we measured something

00:17:09,039 --> 00:17:11,520
like one two three microsecond per

00:17:10,319 --> 00:17:14,559
request

00:17:11,520 --> 00:17:16,319
um which uh if you do the math for

00:17:14,559 --> 00:17:18,000
each core you would just not be able to

00:17:16,319 --> 00:17:19,120
meet the kind of uh throughput that we

00:17:18,000 --> 00:17:22,000
are trying to achieve

00:17:19,120 --> 00:17:23,280
so another very simple thing is which

00:17:22,000 --> 00:17:24,640
can be combined with the

00:17:23,280 --> 00:17:27,199
with the batching idea i talked about

00:17:24,640 --> 00:17:29,360
earlier is that you actually coalesce

00:17:27,199 --> 00:17:30,799
a lot of requests and you ring the

00:17:29,360 --> 00:17:32,960
doorbell only after

00:17:30,799 --> 00:17:34,240
certain number of batching uh some

00:17:32,960 --> 00:17:35,360
certain number of requests have been

00:17:34,240 --> 00:17:36,720
bashed

00:17:35,360 --> 00:17:39,120
and once you have done that then you

00:17:36,720 --> 00:17:40,080
ring the doorbell at this point you are

00:17:39,120 --> 00:17:42,080
basically doing

00:17:40,080 --> 00:17:44,840
one context switch for number of

00:17:42,080 --> 00:17:46,080
requests rather than for each individual

00:17:44,840 --> 00:17:49,360
request

00:17:46,080 --> 00:17:52,720
good and then this is a process as

00:17:49,360 --> 00:17:52,720
as if it was just a batch

00:17:54,400 --> 00:17:57,840
and in case that the system is slowly

00:17:57,039 --> 00:18:00,640
loaded

00:17:57,840 --> 00:18:01,120
then you basically use a timeout period

00:18:00,640 --> 00:18:02,880
uh

00:18:01,120 --> 00:18:04,240
to ensure that the requests are not

00:18:02,880 --> 00:18:06,960
starved on

00:18:04,240 --> 00:18:08,799
uh on the host side so these are very

00:18:06,960 --> 00:18:11,120
two simple optimizations

00:18:08,799 --> 00:18:12,480
uh that's that's all i wanted to tell

00:18:11,120 --> 00:18:15,200
you and

00:18:12,480 --> 00:18:16,640
uh to show you some of the results uh we

00:18:15,200 --> 00:18:18,640
have the 224

00:18:16,640 --> 00:18:20,799
score servers connected directly using a

00:18:18,640 --> 00:18:22,240
100 gps manox cx-5 neck

00:18:20,799 --> 00:18:23,679
uh there are no switches in the middle

00:18:22,240 --> 00:18:24,320
to ensure that the bottleneck stays in

00:18:23,679 --> 00:18:26,240
the kernel

00:18:24,320 --> 00:18:27,440
uh results will only improve if there

00:18:26,240 --> 00:18:30,559
were switches in the middle

00:18:27,440 --> 00:18:31,840
um we have some numbers from the ssds

00:18:30,559 --> 00:18:33,679
and then we are not using any

00:18:31,840 --> 00:18:34,720
specialized functionalities like nvme or

00:18:33,679 --> 00:18:36,880
rdm or anything

00:18:34,720 --> 00:18:38,720
uh good so the questions that i wanted

00:18:36,880 --> 00:18:41,440
to understand using this evaluation

00:18:38,720 --> 00:18:42,720
is how does item performance compared to

00:18:41,440 --> 00:18:44,240
when giving your rdma

00:18:42,720 --> 00:18:45,840
and here we want to measure throughput

00:18:44,240 --> 00:18:48,559
per core average latency

00:18:45,840 --> 00:18:49,919
latency etc uh we wanted to compare to

00:18:48,559 --> 00:18:52,559
user space stacks

00:18:49,919 --> 00:18:54,240
and then we wanted to understand a whole

00:18:52,559 --> 00:18:55,840
variety of different workloads

00:18:54,240 --> 00:18:57,520
and applications what happens if we have

00:18:55,840 --> 00:18:58,720
different read write ratios what happens

00:18:57,520 --> 00:19:01,440
if we have different

00:18:58,720 --> 00:19:03,200
uh doorbell timers uh what happens if we

00:19:01,440 --> 00:19:05,200
do batching at different granularities

00:19:03,200 --> 00:19:07,760
what if request sizes are of different

00:19:05,200 --> 00:19:09,600
uh granularities um storage devices

00:19:07,760 --> 00:19:12,240
having different latencies etc

00:19:09,600 --> 00:19:14,400
and then scalability that how does the

00:19:12,240 --> 00:19:17,360
uh the system scale with number of cores

00:19:14,400 --> 00:19:17,840
and finally you know how does each of

00:19:17,360 --> 00:19:21,679
this

00:19:17,840 --> 00:19:24,559
batching mechanism uh help with the

00:19:21,679 --> 00:19:25,440
uh with the performance numbers so uh

00:19:24,559 --> 00:19:27,039
all these are

00:19:25,440 --> 00:19:28,960
in the paper i'll talk mostly about

00:19:27,039 --> 00:19:31,919
these three that are in bold now

00:19:28,960 --> 00:19:32,720
uh for everything else uh um basically

00:19:31,919 --> 00:19:34,240
we have uh

00:19:32,720 --> 00:19:35,919
we have quite a few of results in the

00:19:34,240 --> 00:19:36,720
paper and i encourage you to see the

00:19:35,919 --> 00:19:38,559
paper

00:19:36,720 --> 00:19:40,000
good so let's focus on these two so

00:19:38,559 --> 00:19:41,200
single code throughput i already showed

00:19:40,000 --> 00:19:43,440
you some numbers

00:19:41,200 --> 00:19:44,240
uh and now we can see that where it

00:19:43,440 --> 00:19:46,640
makes sense

00:19:44,240 --> 00:19:48,720
the gap the latency gap that you see

00:19:46,640 --> 00:19:51,360
even between nvme over tcp

00:19:48,720 --> 00:19:52,880
uh and i10 is really coming out of that

00:19:51,360 --> 00:19:53,520
batching number so we are spending some

00:19:52,880 --> 00:19:56,160
extra

00:19:53,520 --> 00:19:57,840
uh some extra time because of uh waiting

00:19:56,160 --> 00:19:59,520
for these patches to complete and hence

00:19:57,840 --> 00:20:02,320
there's some extra latency

00:19:59,520 --> 00:20:04,159
right uh but uh like i said this is

00:20:02,320 --> 00:20:06,000
roughly 117 microsecond of

00:20:04,159 --> 00:20:08,000
uh tail latency coming because of the

00:20:06,000 --> 00:20:10,720
patching side

00:20:08,000 --> 00:20:13,280
good uh the second thing which is

00:20:10,720 --> 00:20:15,120
perhaps

00:20:13,280 --> 00:20:16,240
also an interesting result is here i am

00:20:15,120 --> 00:20:17,120
going to show you how does the

00:20:16,240 --> 00:20:19,440
performance

00:20:17,120 --> 00:20:21,600
scale with number of cores in the system

00:20:19,440 --> 00:20:24,000
uh so on the x axis i'm going to scale

00:20:21,600 --> 00:20:25,520
the number of cores from 1 to 24

00:20:24,000 --> 00:20:27,360
and i want to understand what is the

00:20:25,520 --> 00:20:30,159
throughput overall system throughput

00:20:27,360 --> 00:20:31,360
uh that one can achieve so for nvme over

00:20:30,159 --> 00:20:33,440
tcp

00:20:31,360 --> 00:20:34,799
in 2019 when we did the original

00:20:33,440 --> 00:20:35,520
evaluation this was kind of the

00:20:34,799 --> 00:20:39,520
scalability

00:20:35,520 --> 00:20:43,120
peaked out roughly at 1300 um kilowatts

00:20:39,520 --> 00:20:46,159
uh with 24 cores

00:20:43,120 --> 00:20:48,320
for nvme or tcp with the recent one

00:20:46,159 --> 00:20:50,080
you get some benefits uh and it still

00:20:48,320 --> 00:20:52,400
peaks out around 1500

00:20:50,080 --> 00:20:53,280
and nvme your rdma i want to point you

00:20:52,400 --> 00:20:57,200
until core

00:20:53,280 --> 00:20:58,400
14 um and until core 14 you see really

00:20:57,200 --> 00:21:00,799
good scalability

00:20:58,400 --> 00:21:02,080
and beyond core 14 there is some issues

00:21:00,799 --> 00:21:03,440
that we have been talking to a lot of

00:21:02,080 --> 00:21:04,799
people and trying to understand what's

00:21:03,440 --> 00:21:06,320
happening with the hardware it seems to

00:21:04,799 --> 00:21:07,919
be something related to the hardware

00:21:06,320 --> 00:21:09,600
but i wouldn't again i don't think

00:21:07,919 --> 00:21:10,559
that's fundamental it scales actually

00:21:09,600 --> 00:21:14,799
pretty well

00:21:10,559 --> 00:21:18,799
right and again i10 is able to scale

00:21:14,799 --> 00:21:21,280
um almost uh well

00:21:18,799 --> 00:21:22,159
it's able to scale well just like nvme

00:21:21,280 --> 00:21:24,320
or rdma does

00:21:22,159 --> 00:21:26,159
until uh core 14 but since we are not

00:21:24,320 --> 00:21:28,480
relying on that hardware our scalability

00:21:26,159 --> 00:21:31,520
continues beyond core 14 as well

00:21:28,480 --> 00:21:31,919
right again there's some gap but uh you

00:21:31,520 --> 00:21:33,679
know

00:21:31,919 --> 00:21:35,520
uh i would like to think that that's not

00:21:33,679 --> 00:21:37,280
fundamental between item and then

00:21:35,520 --> 00:21:39,919
your rdna and the important thing is

00:21:37,280 --> 00:21:42,320
that the curve kind of matches there

00:21:39,919 --> 00:21:43,440
good so i don't scale similar to current

00:21:42,320 --> 00:21:46,480
and beam over rdma

00:21:43,440 --> 00:21:49,600
and that what you're seeing in nvme

00:21:46,480 --> 00:21:51,760
rdm beyond 14 cores is uh really uh

00:21:49,600 --> 00:21:53,039
some issue related to hardware but not

00:21:51,760 --> 00:21:55,919
fundamental

00:21:53,039 --> 00:21:57,200
perfect uh the last thing is i want to

00:21:55,919 --> 00:21:59,919
talk a little bit about

00:21:57,200 --> 00:22:00,880
you may say what is happening why is it

00:21:59,919 --> 00:22:02,720
that

00:22:00,880 --> 00:22:04,159
simple batching mechanisms are helping

00:22:02,720 --> 00:22:06,320
so much um

00:22:04,159 --> 00:22:07,679
intuition is very clear that one but one

00:22:06,320 --> 00:22:09,440
kind of batching that i talked about

00:22:07,679 --> 00:22:10,799
reduces the network processing overheads

00:22:09,440 --> 00:22:12,320
and the other kind of patching reduces

00:22:10,799 --> 00:22:16,159
the context switching overheads

00:22:12,320 --> 00:22:19,280
um but how much is it really helping so

00:22:16,159 --> 00:22:20,720
the on this plot what i'm showing you

00:22:19,280 --> 00:22:22,720
is on the x-axis we have different

00:22:20,720 --> 00:22:24,720
number of cores uh so each core is

00:22:22,720 --> 00:22:28,080
generating request to the target server

00:22:24,720 --> 00:22:30,000
and then the

00:22:28,080 --> 00:22:32,000
blue one at the bottom is basically

00:22:30,000 --> 00:22:33,679
think about it as nvme or tcp

00:22:32,000 --> 00:22:36,080
and the red one is showing what is the

00:22:33,679 --> 00:22:37,200
contribution because of using tso gro

00:22:36,080 --> 00:22:40,720
and jumbo frames

00:22:37,200 --> 00:22:41,840
or larger packet sizes oh maybe i didn't

00:22:40,720 --> 00:22:43,280
mention this earlier i should have

00:22:41,840 --> 00:22:44,159
mentioned that we're also using jumbo

00:22:43,280 --> 00:22:47,760
frames

00:22:44,159 --> 00:22:50,640
um and uh and that's one thing

00:22:47,760 --> 00:22:51,360
uh the next green one is the benefits

00:22:50,640 --> 00:22:54,400
that we get

00:22:51,360 --> 00:22:56,799
out of doing batching of request

00:22:54,400 --> 00:22:58,400
and the responses and the last one is

00:22:56,799 --> 00:23:00,400
batching off

00:22:58,400 --> 00:23:02,159
requests before we ring the doorbell and

00:23:00,400 --> 00:23:04,480
hence the interrupt cola sync

00:23:02,159 --> 00:23:05,440
right uh the important and interesting

00:23:04,480 --> 00:23:07,360
thing to think about

00:23:05,440 --> 00:23:09,120
here is that really people talk about

00:23:07,360 --> 00:23:12,080
jumbo frames quite a bit that

00:23:09,120 --> 00:23:13,120
but it's really just helping 14 in terms

00:23:12,080 --> 00:23:15,440
of throughput

00:23:13,120 --> 00:23:16,480
right so it's not significant benefits

00:23:15,440 --> 00:23:18,000
but if we look at

00:23:16,480 --> 00:23:20,480
these two very simple batching

00:23:18,000 --> 00:23:21,360
mechanisms they almost lead to 62

00:23:20,480 --> 00:23:24,240
percent

00:23:21,360 --> 00:23:25,360
uh improvements and throughput right on

00:23:24,240 --> 00:23:28,640
the baseline

00:23:25,360 --> 00:23:29,520
62 and that's where most of our benefits

00:23:28,640 --> 00:23:31,520
are coming from

00:23:29,520 --> 00:23:33,600
so uh the important thing i want to

00:23:31,520 --> 00:23:36,880
point out is that we have not yet found

00:23:33,600 --> 00:23:38,640
we have done extensive experiments but

00:23:36,880 --> 00:23:40,640
if we take away any of these components

00:23:38,640 --> 00:23:41,120
we are not able to saturate hundred gps

00:23:40,640 --> 00:23:42,799
links

00:23:41,120 --> 00:23:44,400
but when we put all of these components

00:23:42,799 --> 00:23:46,159
together we are able to saturate hundred

00:23:44,400 --> 00:23:47,760
gps links so it seems like

00:23:46,159 --> 00:23:49,440
at least in the current implementation

00:23:47,760 --> 00:23:50,640
each design component is important for

00:23:49,440 --> 00:23:52,880
the final performance

00:23:50,640 --> 00:23:53,919
and very very simple batch mechanisms

00:23:52,880 --> 00:23:55,520
lead to 62

00:23:53,919 --> 00:23:57,520
of the improvement with some latency

00:23:55,520 --> 00:24:00,640
overheads

00:23:57,520 --> 00:24:02,000
good so uh everything that i talked

00:24:00,640 --> 00:24:04,640
about as i also

00:24:02,000 --> 00:24:05,279
copy pasted in the chat window every uh

00:24:04,640 --> 00:24:08,720
thing

00:24:05,279 --> 00:24:12,880
is on our github repo

00:24:08,720 --> 00:24:14,720
we have had at least four or five

00:24:12,880 --> 00:24:16,400
institutions that have reproduced our

00:24:14,720 --> 00:24:17,760
results but

00:24:16,400 --> 00:24:19,679
everything that you need to reproduce

00:24:17,760 --> 00:24:21,520
our resources is in the rebel

00:24:19,679 --> 00:24:22,960
we are working with the linux community

00:24:21,520 --> 00:24:26,320
to push it upstream

00:24:22,960 --> 00:24:33,840
uh and i think i would be happy to take

00:24:26,320 --> 00:24:33,840
questions at this point

00:24:40,159 --> 00:24:43,360
jamalia muted again

00:24:44,480 --> 00:24:48,799
yeah sorry about that um do you want me

00:24:47,520 --> 00:24:49,440
to read the questions rasheed or you

00:24:48,799 --> 00:24:52,080
want to

00:24:49,440 --> 00:24:53,600
respond yourself uh jamal can i stop

00:24:52,080 --> 00:24:54,000
sharing because i can't see something

00:24:53,600 --> 00:24:56,400
yes

00:24:54,000 --> 00:24:57,279
allow me to uh yeah yes you can stop

00:24:56,400 --> 00:25:01,279
sharing thank you

00:24:57,279 --> 00:25:02,480
okay thanks so i'm gonna i'm gonna

00:25:01,279 --> 00:25:04,400
i think i'll just repeat the question

00:25:02,480 --> 00:25:06,720
for the sake of time i

00:25:04,400 --> 00:25:08,799
will there's not as many questions so no

00:25:06,720 --> 00:25:11,679
more questions please

00:25:08,799 --> 00:25:14,799
uh come come to the happy eye you'll be

00:25:11,679 --> 00:25:14,799
there at the happy hour

00:25:19,039 --> 00:25:29,600
okay good all right so uh first question

00:25:22,400 --> 00:25:31,520
is from

00:25:29,600 --> 00:25:33,120
he says i i think i hope i didn't miss

00:25:31,520 --> 00:25:34,799
anybody but it's interesting that the

00:25:33,120 --> 00:25:36,080
network transmit overhead is higher than

00:25:34,799 --> 00:25:40,320
receive any insights

00:25:36,080 --> 00:25:43,360
uh why is is it the resegmentation

00:25:40,320 --> 00:25:45,039
so if you did is uh awesome enough to

00:25:43,360 --> 00:25:45,440
answer his own question i actually it's

00:25:45,039 --> 00:25:48,240
it's

00:25:45,440 --> 00:25:49,440
actually actually it was boris yeah yeah

00:25:48,240 --> 00:25:51,120
so all the questions

00:25:49,440 --> 00:25:53,520
that are current here i think have been

00:25:51,120 --> 00:25:54,799
answered the one or has a question that

00:25:53,520 --> 00:25:56,880
i think is

00:25:54,799 --> 00:25:59,840
maybe worth it or do you want to go you

00:25:56,880 --> 00:25:59,840
want me to repeat what you say

00:26:03,279 --> 00:26:07,919
yes i just commented that for those

00:26:05,679 --> 00:26:12,799
storied workloads uh

00:26:07,919 --> 00:26:12,799
in modern stack of tso lro um

00:26:12,880 --> 00:26:15,919
i think you can get the same results

00:26:14,240 --> 00:26:19,360
also with job jumper frames and

00:26:15,919 --> 00:26:21,200
uh i gave you this comment a couple of

00:26:19,360 --> 00:26:24,799
months ago and um

00:26:21,200 --> 00:26:27,360
for instance in the spdk framework so uh

00:26:24,799 --> 00:26:30,000
um their performance team they stopped

00:26:27,360 --> 00:26:33,440
using jumbo and they

00:26:30,000 --> 00:26:36,000
and they got same results even better so

00:26:33,440 --> 00:26:37,760
perfect yes or like we talked are in the

00:26:36,000 --> 00:26:39,600
past

00:26:37,760 --> 00:26:41,440
jumbo frames are really not helping all

00:26:39,600 --> 00:26:43,200
that much so

00:26:41,440 --> 00:26:45,679
uh we should we should rerun more

00:26:43,200 --> 00:26:45,679
results

00:26:46,320 --> 00:26:49,520
sorry for nudging on this on this piece

00:26:48,400 --> 00:26:52,880
but you know

00:26:49,520 --> 00:26:54,880
uh i i believe that most

00:26:52,880 --> 00:26:57,279
modern data centers do not use jumbo

00:26:54,880 --> 00:26:59,679
frames in google they do use it but it's

00:26:57,279 --> 00:27:01,520
a special case and they did lots of work

00:26:59,679 --> 00:27:02,720
in their infrastructure to enable jumbo

00:27:01,520 --> 00:27:04,080
frames so

00:27:02,720 --> 00:27:06,000
if you want to provide something which

00:27:04,080 --> 00:27:06,799
is more general you you don't want to

00:27:06,000 --> 00:27:10,400
assume that

00:27:06,799 --> 00:27:10,400
the deployment has jumbo

00:27:12,080 --> 00:27:15,120
perfect yes like i showed in the results

00:27:14,640 --> 00:27:17,039
that

00:27:15,120 --> 00:27:19,440
you know jumbo frames actually just give

00:27:17,039 --> 00:27:23,360
us a very small improvements and

00:27:19,440 --> 00:27:24,880
um and you know if people are not using

00:27:23,360 --> 00:27:25,520
jumbo frames i think that would be just

00:27:24,880 --> 00:27:28,399
fine

00:27:25,520 --> 00:27:29,760
uh but it is interesting that we should

00:27:28,399 --> 00:27:32,240
we should probably

00:27:29,760 --> 00:27:34,399
uh run some more results with without

00:27:32,240 --> 00:27:36,480
jumbo frames as well okay

00:27:34,399 --> 00:27:38,320
let's move that to the happy hour uh

00:27:36,480 --> 00:27:40,000
question from masiak

00:27:38,320 --> 00:27:42,000
how many calls do you need to saturate

00:27:40,000 --> 00:27:43,279
the hundred gigabits with all your

00:27:42,000 --> 00:27:46,320
improvements

00:27:43,279 --> 00:27:50,320
uh perfect so uh this is uh

00:27:46,320 --> 00:27:52,799
magic let me pull up the exact number

00:27:50,320 --> 00:27:54,159
uh if you just give me one second i

00:27:52,799 --> 00:27:59,120
think we needed

00:27:54,159 --> 00:28:02,399
22 cores um to saturate the 100 gps link

00:27:59,120 --> 00:28:07,039
which was i think around 2.1

00:28:02,399 --> 00:28:10,159
sorry 3.1 million ions

00:28:07,039 --> 00:28:13,279
and last question is from david niemi uh

00:28:10,159 --> 00:28:16,080
have you looked at which features

00:28:13,279 --> 00:28:16,559
affect latency advancely and by how much

00:28:16,080 --> 00:28:19,039
yes

00:28:16,559 --> 00:28:20,240
uh perfect david uh i think the latency

00:28:19,039 --> 00:28:22,640
is just coming out of

00:28:20,240 --> 00:28:24,000
the matching part uh it's uh it's just

00:28:22,640 --> 00:28:26,320
uh

00:28:24,000 --> 00:28:28,080
in in the tail case what happens is

00:28:26,320 --> 00:28:30,320
because of the bursty nature if

00:28:28,080 --> 00:28:33,360
applications submit requests

00:28:30,320 --> 00:28:34,320
um you know in a bursty form sometimes

00:28:33,360 --> 00:28:36,320
it happens that we are

00:28:34,320 --> 00:28:38,399
waiting for 50 microseconds of timeout

00:28:36,320 --> 00:28:41,360
period that i talked about in batching

00:28:38,399 --> 00:28:42,880
uh to send out the pool and when the

00:28:41,360 --> 00:28:44,720
timeout happens you give up 100

00:28:42,880 --> 00:28:47,200
microseconds and i think with some

00:28:44,720 --> 00:28:49,679
slight uh orange or here or there it

00:28:47,200 --> 00:28:51,200
becomes 117 microseconds so that's where

00:28:49,679 --> 00:28:53,919
the latency overheads are coming from

00:28:51,200 --> 00:28:56,000
does that make sense

00:28:53,919 --> 00:28:58,240
jamal i would add one more question to

00:28:56,000 --> 00:29:00,720
skip which is lawrence's question

00:28:58,240 --> 00:29:01,919
oh yes i'm sorry lawrence brockmore uh

00:29:00,720 --> 00:29:04,080
please prefix with q

00:29:01,919 --> 00:29:04,960
it's my eyes come for the q for all the

00:29:04,080 --> 00:29:08,240
questions

00:29:04,960 --> 00:29:09,039
um do you know what flavor of tcp that

00:29:08,240 --> 00:29:11,039
you use

00:29:09,039 --> 00:29:12,559
and uh could you could i change this

00:29:11,039 --> 00:29:15,679
flavor of tcp affect the

00:29:12,559 --> 00:29:19,919
tail latency ah that's a good question

00:29:15,679 --> 00:29:20,640
um i don't know at the top of my head

00:29:19,919 --> 00:29:22,799
shy who

00:29:20,640 --> 00:29:24,640
my postdoc is here do you know who what

00:29:22,799 --> 00:29:27,120
flavor of tcp what are we using

00:29:24,640 --> 00:29:27,760
um i i can answer the second part of the

00:29:27,120 --> 00:29:30,799
question but

00:29:27,760 --> 00:29:33,039
uh um i don't remember exactly at the

00:29:30,799 --> 00:29:36,480
top of my head which tcp would be using

00:29:33,039 --> 00:29:38,960
okay we were using cubic so uh

00:29:36,480 --> 00:29:40,399
lawrence in in a general deployment

00:29:38,960 --> 00:29:44,159
you're absolutely

00:29:40,399 --> 00:29:44,960
right that the different tcp flavors

00:29:44,159 --> 00:29:47,120
could have different

00:29:44,960 --> 00:29:48,960
latency our setup was so easy because we

00:29:47,120 --> 00:29:50,480
just connected to servers directly

00:29:48,960 --> 00:29:53,200
because we were trying to avoid the

00:29:50,480 --> 00:29:54,559
liquid latencies to make our results

00:29:53,200 --> 00:29:56,080
look good because if then

00:29:54,559 --> 00:29:58,320
if there is higher network latency then

00:29:56,080 --> 00:30:01,440
our results will actually look nicer

00:29:58,320 --> 00:30:02,880
um so uh there was no congestion in the

00:30:01,440 --> 00:30:05,039
network and hence

00:30:02,880 --> 00:30:06,840
these results would not be affected by

00:30:05,039 --> 00:30:10,240
tail latency that much

00:30:06,840 --> 00:30:12,559
um we

00:30:10,240 --> 00:30:14,080
are hoping to run larger experiments we

00:30:12,559 --> 00:30:16,159
are kind of limited in terms of

00:30:14,080 --> 00:30:17,919
you know the academics and and we don't

00:30:16,159 --> 00:30:18,960
have a large base testbed to run these

00:30:17,919 --> 00:30:21,679
experiments but

00:30:18,960 --> 00:30:23,600
we are hoping that once we once the

00:30:21,679 --> 00:30:26,880
upstream code is out and

00:30:23,600 --> 00:30:28,000
we have some uh some

00:30:26,880 --> 00:30:29,919
and we have been working with the

00:30:28,000 --> 00:30:32,559
lightweights people and and

00:30:29,919 --> 00:30:33,760
uh sanji to see if we can actually do

00:30:32,559 --> 00:30:36,159
more and more evaluation i think that

00:30:33,760 --> 00:30:39,200
would be fantastic

00:30:36,159 --> 00:30:40,960
thank you rashid so if people are

00:30:39,200 --> 00:30:42,240
interested we can also create a breakout

00:30:40,960 --> 00:30:45,520
room for storage

00:30:42,240 --> 00:30:46,960
of a networking uh just comment on the

00:30:45,520 --> 00:30:50,240
chat

00:30:46,960 --> 00:30:52,320
thank you richie

00:30:50,240 --> 00:30:52,320

YouTube URL: https://www.youtube.com/watch?v=I86ZNKGB1GA


