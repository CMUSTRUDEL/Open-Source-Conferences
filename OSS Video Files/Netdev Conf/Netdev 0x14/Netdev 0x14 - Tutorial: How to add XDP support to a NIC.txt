Title: Netdev 0x14 - Tutorial: How to add XDP support to a NIC
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Instructors: Lorenzo Bianconi, Jesper Brouer, and Ilias Apalodimas

More info: https://netdevconf.info/0x14/session.html?tutorial-add-XDP-support-to-a-NIC-driver

Date: Friday, August 14, 2020

Lorenzo Bianconi et al will provide a tutorial on how to add XDP support
to a NIC driver. They will start with the basics and lead to the challenging
issues a driver author would have to deal with.
The mvneta driver (Marvell Armada 3700LP chip on Marvell ESPRSSObin board) will
be used as an example.
The tutorial will be interactive and dissect code samples while describing
architectural challenges.
Captions: 
	00:00:03,040 --> 00:00:06,640
that's great

00:00:04,319 --> 00:00:07,440
so yeah this is a joint talk i think

00:00:06,640 --> 00:00:10,240
it's we have

00:00:07,440 --> 00:00:10,880
we are actually a tutorial and we will

00:00:10,240 --> 00:00:14,240
show like

00:00:10,880 --> 00:00:18,720
demo code and everything uh

00:00:14,240 --> 00:00:18,720
so let's let's take the next next slide

00:00:23,439 --> 00:00:26,880
strange a thing in the middle of the

00:00:25,680 --> 00:00:30,320
screen okay

00:00:26,880 --> 00:00:33,280
so yeah the target audience for this

00:00:30,320 --> 00:00:34,559
tutorial is other kernel developers who

00:00:33,280 --> 00:00:37,920
wants to add

00:00:34,559 --> 00:00:39,680
http support to an existing driver and

00:00:37,920 --> 00:00:42,000
as you can see from this picture you

00:00:39,680 --> 00:00:44,079
will get really happy if you add a http

00:00:42,000 --> 00:00:46,719
supporter driver

00:00:44,079 --> 00:00:47,200
so that's very positive right so let's

00:00:46,719 --> 00:00:50,160
let's

00:00:47,200 --> 00:00:50,160
look at the next line

00:00:51,199 --> 00:00:58,239
so some of the technical requirements

00:00:54,719 --> 00:01:01,440
let's just let's dig in into those

00:00:58,239 --> 00:01:01,440
can i get the next slide

00:01:03,600 --> 00:01:09,360
so the xtp

00:01:06,640 --> 00:01:11,360
received processing and that happens in

00:01:09,360 --> 00:01:13,920
the nappy loop is actually quite

00:01:11,360 --> 00:01:16,799
simple and i've managed to fit it all in

00:01:13,920 --> 00:01:19,439
in one slide here

00:01:16,799 --> 00:01:20,720
and i'm not going to go too much into

00:01:19,439 --> 00:01:22,640
the coating

00:01:20,720 --> 00:01:25,200
details in this slide because the rinser

00:01:22,640 --> 00:01:28,000
is going to take you through

00:01:25,200 --> 00:01:30,400
uh some some of the codes for a specific

00:01:28,000 --> 00:01:32,799
driver

00:01:30,400 --> 00:01:34,880
but the point the point i want to make

00:01:32,799 --> 00:01:37,520
is the tricky part is actually changing

00:01:34,880 --> 00:01:41,280
the memory model to be compatible

00:01:37,520 --> 00:01:44,159
with with xtp and if you can guess

00:01:41,280 --> 00:01:44,159
get the next slide

00:01:46,640 --> 00:01:52,960
so for xdp we have the frame

00:01:50,560 --> 00:01:55,280
that has to be in physical particular

00:01:52,960 --> 00:01:58,479
memory

00:01:55,280 --> 00:01:59,600
and we need that because we are taking

00:01:58,479 --> 00:02:02,079
advantage of

00:01:59,600 --> 00:02:04,159
bpf feature that's called direct access

00:02:02,079 --> 00:02:05,119
which allows the ppf program to get

00:02:04,159 --> 00:02:08,239
direct access

00:02:05,119 --> 00:02:11,599
to the to the memory

00:02:08,239 --> 00:02:13,520
uh and this is uh

00:02:11,599 --> 00:02:15,599
one of the things that that can speed it

00:02:13,520 --> 00:02:17,360
up right and this direct access

00:02:15,599 --> 00:02:19,280
requires the physical contiguous memory

00:02:17,360 --> 00:02:21,280
because the verifier can verify that we

00:02:19,280 --> 00:02:23,200
don't go out of bound

00:02:21,280 --> 00:02:24,560
but that also means that we cannot have

00:02:23,200 --> 00:02:27,360
data split across

00:02:24,560 --> 00:02:27,360
different pages

00:02:27,840 --> 00:02:30,959
one of the other things is that we

00:02:29,440 --> 00:02:32,640
actually have direct

00:02:30,959 --> 00:02:34,000
read and write access to the dma

00:02:32,640 --> 00:02:35,840
preference so you will see

00:02:34,000 --> 00:02:37,519
in later slides that you need to take

00:02:35,840 --> 00:02:41,519
care of the dma syncing

00:02:37,519 --> 00:02:46,239
in your in your driver uh

00:02:41,519 --> 00:02:47,680
so that is part part of

00:02:46,239 --> 00:02:50,239
some of the tricks you have to do in

00:02:47,680 --> 00:02:52,239
your driver code then another thing is

00:02:50,239 --> 00:02:52,800
that you have to disable jumper frames

00:02:52,239 --> 00:02:55,200
or

00:02:52,800 --> 00:02:56,720
larger mtu frames that goes beyond one

00:02:55,200 --> 00:03:00,480
page size

00:02:56,720 --> 00:03:00,480
when loading this xtp program

00:03:00,640 --> 00:03:05,120
there is also a limitation we are

00:03:03,120 --> 00:03:07,760
working on we'll discuss that later

00:03:05,120 --> 00:03:09,760
on how we can get around this but that

00:03:07,760 --> 00:03:11,840
is not true in the upstream

00:03:09,760 --> 00:03:13,280
then we also have the http headroom

00:03:11,840 --> 00:03:15,840
where we actually store the

00:03:13,280 --> 00:03:18,480
what's called the xtp frame area when we

00:03:15,840 --> 00:03:21,440
are retargeting frames around

00:03:18,480 --> 00:03:22,560
we also use the headroom for pushing and

00:03:21,440 --> 00:03:24,400
popping headers

00:03:22,560 --> 00:03:26,000
through a helper function and then we

00:03:24,400 --> 00:03:30,159
have reserved some tail room

00:03:26,000 --> 00:03:33,760
in this this page which we use for

00:03:30,159 --> 00:03:34,480
when we create an skb we have to have

00:03:33,760 --> 00:03:37,519
this area

00:03:34,480 --> 00:03:38,400
for the sqb shared shared info and all

00:03:37,519 --> 00:03:41,280
the

00:03:38,400 --> 00:03:42,720
drivers they have to use rely on build

00:03:41,280 --> 00:03:46,239
skb

00:03:42,720 --> 00:03:47,920
for example when you do the http pass

00:03:46,239 --> 00:03:50,319
so in your driver you're not allowed to

00:03:47,920 --> 00:03:52,000
have fragments of pages

00:03:50,319 --> 00:03:53,680
that is if your driver used the call

00:03:52,000 --> 00:03:54,640
that is mentioned here the nappy airlock

00:03:53,680 --> 00:03:57,360
escaping

00:03:54,640 --> 00:03:59,680
that is not something you allow for http

00:03:57,360 --> 00:04:02,000
drivers to do

00:03:59,680 --> 00:04:03,200
your driver you have to have a recycle

00:04:02,000 --> 00:04:04,799
mechanism

00:04:03,200 --> 00:04:07,360
that can recycle this frame at a very

00:04:04,799 --> 00:04:11,200
early points and

00:04:07,360 --> 00:04:15,840
to get some of the high speed

00:04:11,200 --> 00:04:15,840
can i get the next slide

00:04:16,479 --> 00:04:20,160
so i said the problem is the memory

00:04:18,400 --> 00:04:22,639
model right so what we

00:04:20,160 --> 00:04:23,840
changed with xtp is that we force the

00:04:22,639 --> 00:04:25,840
drivers

00:04:23,840 --> 00:04:29,280
to explicitly say what kind of memory

00:04:25,840 --> 00:04:32,160
model they use

00:04:29,280 --> 00:04:32,639
and i've listed the structures here as

00:04:32,160 --> 00:04:36,320
this is

00:04:32,639 --> 00:04:39,520
a coding tutorial where you need to look

00:04:36,320 --> 00:04:41,440
i also list the api as you can see you

00:04:39,520 --> 00:04:43,520
will see your slides later that to show

00:04:41,440 --> 00:04:46,800
you how to use this api to register

00:04:43,520 --> 00:04:49,840
a memory model so the real

00:04:46,800 --> 00:04:51,600
advantage of of doing this having the

00:04:49,840 --> 00:04:52,960
drivers explicitly say which memory

00:04:51,600 --> 00:04:54,960
model they want to use

00:04:52,960 --> 00:04:57,040
is that we allow to invent new memory

00:04:54,960 --> 00:04:58,400
models and that has turned out pretty

00:04:57,040 --> 00:05:01,039
good so far

00:04:58,400 --> 00:05:02,479
so we have the memory type page share

00:05:01,039 --> 00:05:04,960
which is the normal riffcount

00:05:02,479 --> 00:05:07,039
page refcon based model then we have the

00:05:04,960 --> 00:05:09,600
pagepool which is optimized for http

00:05:07,039 --> 00:05:10,479
we'll talk more about that later so

00:05:09,600 --> 00:05:13,600
thank you

00:05:10,479 --> 00:05:16,320
elias will talk about that then we have

00:05:13,600 --> 00:05:18,240
the sk buff pool which is for serial

00:05:16,320 --> 00:05:22,240
copying into user space with the af

00:05:18,240 --> 00:05:22,880
xtp socket what i really hope to see is

00:05:22,240 --> 00:05:25,440
that

00:05:22,880 --> 00:05:27,840
we see new models being invented i think

00:05:25,440 --> 00:05:30,880
at this conference

00:05:27,840 --> 00:05:32,639
jonathan will talk about how he's

00:05:30,880 --> 00:05:33,280
building a new memory model that allows

00:05:32,639 --> 00:05:37,039
nics

00:05:33,280 --> 00:05:40,560
to use memory that belongs to the dpu

00:05:37,039 --> 00:05:43,840
and and this way have direct dma into

00:05:40,560 --> 00:05:45,120
gpu memory and do funniest things i also

00:05:43,840 --> 00:05:47,440
imagine that people

00:05:45,120 --> 00:05:49,199
from the storage area will take

00:05:47,440 --> 00:05:52,639
advantage of this

00:05:49,199 --> 00:05:54,800
so you can dma directly into

00:05:52,639 --> 00:05:55,919
a memory that belongs to a storage

00:05:54,800 --> 00:05:59,039
device

00:05:55,919 --> 00:05:59,039
so you can get the next slide

00:06:00,960 --> 00:06:06,240
so this is the xtp architecture and i

00:06:02,639 --> 00:06:09,039
think we have so many slides

00:06:06,240 --> 00:06:11,440
took to cover that we can we can skip

00:06:09,039 --> 00:06:11,440
this one

00:06:12,400 --> 00:06:16,000
to enter can you take the next slide

00:06:16,400 --> 00:06:24,560
and then i'll hand it over to lease yeah

00:06:21,199 --> 00:06:25,759
hello everyone so based on what jasper

00:06:24,560 --> 00:06:27,440
tried to explain

00:06:25,759 --> 00:06:30,880
on the different memory models and the

00:06:27,440 --> 00:06:32,639
design principles we need and the xdp

00:06:30,880 --> 00:06:35,039
restrictions we have and the specific

00:06:32,639 --> 00:06:37,120
requirements for the memory accesses

00:06:35,039 --> 00:06:38,800
uh we came up there's a there's a

00:06:37,120 --> 00:06:42,080
internal api

00:06:38,800 --> 00:06:44,560
uh for allocating your pages uh

00:06:42,080 --> 00:06:45,280
managing the dma that jesper mentioned

00:06:44,560 --> 00:06:48,319
then

00:06:45,280 --> 00:06:50,319
most the most important thing is that

00:06:48,319 --> 00:06:51,759
it automatically takes care of the of

00:06:50,319 --> 00:06:54,080
the recycling

00:06:51,759 --> 00:06:55,440
capabilities and infrastructure you need

00:06:54,080 --> 00:06:59,039
to add on your driver

00:06:55,440 --> 00:06:59,039
to make xdp fast

00:06:59,120 --> 00:07:04,880
so uh xdp is fast for

00:07:03,039 --> 00:07:06,720
for a number of reasons but one of the

00:07:04,880 --> 00:07:07,280
the most significant reasons is the fact

00:07:06,720 --> 00:07:09,520
that we

00:07:07,280 --> 00:07:11,280
we managed to recycle the memory we were

00:07:09,520 --> 00:07:14,400
using on the network card

00:07:11,280 --> 00:07:14,400
in most of the cases

00:07:15,039 --> 00:07:18,400
so can you move to the next slide

00:07:21,599 --> 00:07:26,800
so uh the design principles that were

00:07:24,880 --> 00:07:28,000
considered before coming up and

00:07:26,800 --> 00:07:31,759
committing this api

00:07:28,000 --> 00:07:32,960
was that this is optimized this is

00:07:31,759 --> 00:07:34,639
mostly optimized

00:07:32,960 --> 00:07:37,039
the page pool is mostly optimized around

00:07:34,639 --> 00:07:40,160
the xdp memory model

00:07:37,039 --> 00:07:42,479
ideally and when everything started

00:07:40,160 --> 00:07:44,639
this used to be a requirement we had one

00:07:42,479 --> 00:07:47,759
packet per page one one

00:07:44,639 --> 00:07:48,879
payload for per page uh this is not true

00:07:47,759 --> 00:07:51,039
anymore since there

00:07:48,879 --> 00:07:53,039
there's driver examples in the kernel at

00:07:51,039 --> 00:07:55,360
the moment that are splitting the page

00:07:53,039 --> 00:07:56,960
and that are using one page to fit in

00:07:55,360 --> 00:07:59,199
multiple packets

00:07:56,960 --> 00:08:00,720
actually they fit in two packets at the

00:07:59,199 --> 00:08:03,199
moment but there's nothing preventing

00:08:00,720 --> 00:08:05,840
you to fit multiple packets in it

00:08:03,199 --> 00:08:07,360
uh the only thing that you have to do is

00:08:05,840 --> 00:08:09,039
that you have to play some tricks around

00:08:07,360 --> 00:08:11,280
the reference count of the page and make

00:08:09,039 --> 00:08:14,240
sure that your page is not recycled

00:08:11,280 --> 00:08:15,759
when it still holds valid buffers that

00:08:14,240 --> 00:08:18,160
you need to consume from the networking

00:08:15,759 --> 00:08:18,160
stack

00:08:19,039 --> 00:08:24,160
now the another reason that the the

00:08:21,919 --> 00:08:25,360
whole idea is fast is that we're trying

00:08:24,160 --> 00:08:28,000
to avoid locking

00:08:25,360 --> 00:08:29,360
uh as much as possible so considering

00:08:28,000 --> 00:08:30,879
the fact that we're usually

00:08:29,360 --> 00:08:33,120
running under an upper loop and there's

00:08:30,879 --> 00:08:35,919
a software queue already in there

00:08:33,120 --> 00:08:37,360
we're trying to to prevent extra locking

00:08:35,919 --> 00:08:40,159
so

00:08:37,360 --> 00:08:40,719
you should when you're writing a driver

00:08:40,159 --> 00:08:43,440
model and

00:08:40,719 --> 00:08:45,200
you're using the page pull api one pull

00:08:43,440 --> 00:08:47,440
should be allocated for every hardware

00:08:45,200 --> 00:08:47,440
queue

00:08:52,320 --> 00:08:56,480
so you can once once the recycling hits

00:08:55,040 --> 00:08:58,320
in you can avoid locking and you can

00:08:56,480 --> 00:09:00,320
recycle the packets lockless

00:08:58,320 --> 00:09:02,880
uh there's still extra exceptions to

00:09:00,320 --> 00:09:04,000
this uh because there's uh there's some

00:09:02,880 --> 00:09:06,399
hardware

00:09:04,000 --> 00:09:08,399
that due to design idea and how the

00:09:06,399 --> 00:09:09,920
memory model operates on it can't do

00:09:08,399 --> 00:09:11,519
that so you end up with

00:09:09,920 --> 00:09:13,760
a single pull for the whole for the

00:09:11,519 --> 00:09:18,800
whole hardware and

00:09:13,760 --> 00:09:18,800
you end up having to lock the the pull

00:09:19,279 --> 00:09:23,279
the and this also as i told you this

00:09:22,240 --> 00:09:25,519
offers

00:09:23,279 --> 00:09:26,560
native recycling capabilities so we keep

00:09:25,519 --> 00:09:28,560
two cases there's

00:09:26,560 --> 00:09:30,080
nina and in software cubecast which is

00:09:28,560 --> 00:09:31,680
the fastest one you can use and there's

00:09:30,080 --> 00:09:34,399
a pointer ring cast

00:09:31,680 --> 00:09:35,519
uh which we use to refill the insoft iq

00:09:34,399 --> 00:09:38,720
cast

00:09:35,519 --> 00:09:41,360
once that's that's drained and

00:09:38,720 --> 00:09:42,959
finally the rece we recently introduced

00:09:41,360 --> 00:09:44,560
like i don't know if it was five six

00:09:42,959 --> 00:09:48,839
months ago or something like that

00:09:44,560 --> 00:09:51,279
uh we can support your dma syncing

00:09:48,839 --> 00:09:52,959
mapping depending on the flags you're

00:09:51,279 --> 00:09:54,160
using when registering the pool for the

00:09:52,959 --> 00:09:56,160
driver

00:09:54,160 --> 00:09:57,680
now this this all sounds really nice but

00:09:56,160 --> 00:09:59,600
that's that's not exactly

00:09:57,680 --> 00:10:01,279
uh that nice because the problem there's

00:09:59,600 --> 00:10:02,720
a there's an action problem

00:10:01,279 --> 00:10:04,880
due to the fact that you allocate a

00:10:02,720 --> 00:10:06,959
single page for a single packet

00:10:04,880 --> 00:10:09,120
if you use the page pull allocator to

00:10:06,959 --> 00:10:11,120
write a simple network driver without

00:10:09,120 --> 00:10:12,160
xdp you actually get a performance

00:10:11,120 --> 00:10:14,399
penalty

00:10:12,160 --> 00:10:16,560
compared to what you would get if you

00:10:14,399 --> 00:10:18,640
were allocating page fragments via not

00:10:16,560 --> 00:10:20,880
be a lock frog or whatever we have in

00:10:18,640 --> 00:10:23,680
the kernel

00:10:20,880 --> 00:10:26,399
to to allocate pages to allocate memory

00:10:23,680 --> 00:10:26,399
for your buffers

00:10:27,760 --> 00:10:34,880
can you move to the next slide

00:10:32,880 --> 00:10:36,480
so this is an overview of what happens

00:10:34,880 --> 00:10:39,600
on your network interface

00:10:36,480 --> 00:10:41,360
uh once you once you start using page

00:10:39,600 --> 00:10:42,880
pool so what happens is that when the

00:10:41,360 --> 00:10:44,959
device comes up

00:10:42,880 --> 00:10:46,880
you create the page pool uh you have

00:10:44,959 --> 00:10:48,560
specific parameters that control the the

00:10:46,880 --> 00:10:50,240
pulse behavior like the

00:10:48,560 --> 00:10:52,000
if you're going to map those buffers if

00:10:50,240 --> 00:10:54,720
you're gonna sync them on on

00:10:52,000 --> 00:10:56,480
on every operation uh at which offset

00:10:54,720 --> 00:10:58,000
you should sync them what's the maximum

00:10:56,480 --> 00:11:00,800
length you can sing

00:10:58,000 --> 00:11:02,480
and what's the pointer cast size so

00:11:00,800 --> 00:11:05,040
during the nappy poll you get

00:11:02,480 --> 00:11:06,800
an xdp decision so depending on what

00:11:05,040 --> 00:11:07,760
happens on the xdp verdict you see on

00:11:06,800 --> 00:11:10,079
the slide

00:11:07,760 --> 00:11:11,120
the page either gets recycled directly

00:11:10,079 --> 00:11:13,760
into the ring

00:11:11,120 --> 00:11:15,360
or gets recycled in cast or there's okay

00:11:13,760 --> 00:11:15,760
the the slide would get too complex

00:11:15,360 --> 00:11:18,399
there's

00:11:15,760 --> 00:11:20,079
even rare use cases where we don't

00:11:18,399 --> 00:11:23,279
recycle the page at all

00:11:20,079 --> 00:11:24,720
uh but that's that's a really corner

00:11:23,279 --> 00:11:26,720
corner case that shouldn't happen most

00:11:24,720 --> 00:11:29,120
of the time

00:11:26,720 --> 00:11:30,720
so after that once the the pay if if the

00:11:29,120 --> 00:11:33,600
pace goes back in the cars

00:11:30,720 --> 00:11:35,120
and you're about to refill your uh your

00:11:33,600 --> 00:11:38,240
tma buffers then you

00:11:35,120 --> 00:11:41,600
do get cast a paid will get cast

00:11:38,240 --> 00:11:43,519
so if the locus is if the locus is full

00:11:41,600 --> 00:11:43,920
and you you have pages to serve that you

00:11:43,519 --> 00:11:46,240
get

00:11:43,920 --> 00:11:48,480
a page out of the fast path and you

00:11:46,240 --> 00:11:51,440
refill your buffer if it's not

00:11:48,480 --> 00:11:54,839
uh then you use the pointer uh the

00:11:51,440 --> 00:11:57,680
pointer ring to refill the cases

00:11:54,839 --> 00:11:59,600
and that's that's a slightly

00:11:57,680 --> 00:12:01,360
slower path but then eventually your dma

00:11:59,600 --> 00:12:04,959
refill happens

00:12:01,360 --> 00:12:04,959
uh you can move to the next slide

00:12:07,760 --> 00:12:12,079
so this is there's a detailed internal

00:12:10,079 --> 00:12:13,600
documentation with uh

00:12:12,079 --> 00:12:15,200
with a schema that you can probably

00:12:13,600 --> 00:12:16,959
figure out exactly how page pull

00:12:15,200 --> 00:12:18,079
operates and how how the classes are

00:12:16,959 --> 00:12:20,079
refilled

00:12:18,079 --> 00:12:21,839
but the main things you need for the api

00:12:20,079 --> 00:12:24,959
is paid pool create which creates the

00:12:21,839 --> 00:12:27,200
page pull object

00:12:24,959 --> 00:12:28,880
so you actually need to run this for for

00:12:27,200 --> 00:12:29,200
every queue then you have the flags

00:12:28,880 --> 00:12:30,880
which

00:12:29,200 --> 00:12:33,680
the flags we currently support is the

00:12:30,880 --> 00:12:35,440
dma map flag which

00:12:33,680 --> 00:12:38,000
maps your buffer and there's the dma

00:12:35,440 --> 00:12:40,800
sync flag which syncs your buffer

00:12:38,000 --> 00:12:41,600
then the order is the size of the the

00:12:40,800 --> 00:12:43,920
page your

00:12:41,600 --> 00:12:46,000
the memory you're going to allocate then

00:12:43,920 --> 00:12:48,079
you have the pool size which controls

00:12:46,000 --> 00:12:50,959
the size of the pointer ring for your

00:12:48,079 --> 00:12:51,760
for your cast buffers then you have the

00:12:50,959 --> 00:12:55,200
pneuma node

00:12:51,760 --> 00:12:55,200
and this is important because the

00:12:55,680 --> 00:12:57,920
uh

00:12:58,959 --> 00:13:04,000
when you first allocate a pool uh you're

00:13:01,600 --> 00:13:05,360
gonna get the pneuma node id on the cpu

00:13:04,000 --> 00:13:06,639
you're running now eventually this is

00:13:05,360 --> 00:13:09,120
going to change and

00:13:06,639 --> 00:13:10,880
so if if this changes and that's one of

00:13:09,120 --> 00:13:11,920
the corner cases we don't recycle the

00:13:10,880 --> 00:13:14,639
buffer

00:13:11,920 --> 00:13:16,240
uh we're also changing the pneuma node

00:13:14,639 --> 00:13:18,079
we're allocating and we're

00:13:16,240 --> 00:13:21,040
allocating from the closest numa node on

00:13:18,079 --> 00:13:22,560
the next on the next request

00:13:21,040 --> 00:13:25,120
then on the page will create you have

00:13:22,560 --> 00:13:27,440
the dma direction dma length and the max

00:13:25,120 --> 00:13:30,480
length you can you can sync

00:13:27,440 --> 00:13:32,079
then you have page pull put page and

00:13:30,480 --> 00:13:34,240
this is a bit

00:13:32,079 --> 00:13:35,519
this has two functionalities and this is

00:13:34,240 --> 00:13:37,440
because

00:13:35,519 --> 00:13:39,440
as i told you in the beginning you can

00:13:37,440 --> 00:13:40,160
you can use a page and split it and fit

00:13:39,440 --> 00:13:42,240
in

00:13:40,160 --> 00:13:44,320
two buffers or three packets or however

00:13:42,240 --> 00:13:45,760
you wanna how many packets you wanna fit

00:13:44,320 --> 00:13:47,199
in a page

00:13:45,760 --> 00:13:49,199
uh so the outcome of this actually

00:13:47,199 --> 00:13:51,600
depends on the paid ref count if the

00:13:49,199 --> 00:13:53,199
driver is using page recycling tricks

00:13:51,600 --> 00:13:53,920
and is splitting the page then the ref

00:13:53,199 --> 00:13:56,959
count is

00:13:53,920 --> 00:13:58,560
uh greater than one and what this will

00:13:56,959 --> 00:13:59,199
do is that will eventually unmap the

00:13:58,560 --> 00:14:01,120
page

00:13:59,199 --> 00:14:02,800
and the driver is in charge of it and

00:14:01,120 --> 00:14:04,800
he'll have to recycle it internally and

00:14:02,800 --> 00:14:06,880
figure out the ref counts internally

00:14:04,800 --> 00:14:08,240
now if the rest count is one it means

00:14:06,880 --> 00:14:09,440
that the page pool allocator is in

00:14:08,240 --> 00:14:11,680
charge of the page

00:14:09,440 --> 00:14:13,360
so what it does is that it recycles for

00:14:11,680 --> 00:14:14,639
the network interface and it syncs it to

00:14:13,360 --> 00:14:16,560
the correct direction

00:14:14,639 --> 00:14:17,839
uh that has to be seen to be used by the

00:14:16,560 --> 00:14:19,760
device again

00:14:17,839 --> 00:14:21,120
uh then you have the another helper

00:14:19,760 --> 00:14:23,839
which is paid for

00:14:21,120 --> 00:14:25,760
put full page uh this is exactly what

00:14:23,839 --> 00:14:30,079
the

00:14:25,760 --> 00:14:33,120
put page does but it's

00:14:30,079 --> 00:14:33,120
syncing the whole page

00:14:33,680 --> 00:14:37,680
then you have uh release page which

00:14:36,560 --> 00:14:40,000
unmapped the page

00:14:37,680 --> 00:14:40,959
and it accounts for the in-flight

00:14:40,000 --> 00:14:42,720
buffers

00:14:40,959 --> 00:14:44,560
one of the details i forgot is that we

00:14:42,720 --> 00:14:46,480
keep some statistics for the pages we

00:14:44,560 --> 00:14:47,519
use and map because if the user does a

00:14:46,480 --> 00:14:49,680
mistake

00:14:47,519 --> 00:14:51,760
and doesn't free the the page correctly

00:14:49,680 --> 00:14:53,600
the moment you destroy the pool

00:14:51,760 --> 00:14:56,800
you leave stale buffers on your dma

00:14:53,600 --> 00:14:58,480
memory mapped as dma so

00:14:56,800 --> 00:14:59,839
the release page is releasing the page

00:14:58,480 --> 00:15:00,800
and it's accounting for the in-flight

00:14:59,839 --> 00:15:04,079
counter so you

00:15:00,800 --> 00:15:07,839
you won't get warnings uh while

00:15:04,079 --> 00:15:10,560
uh freeing the pages then you have

00:15:07,839 --> 00:15:12,240
the get the dma direction and the dma

00:15:10,560 --> 00:15:13,760
address which are internal

00:15:12,240 --> 00:15:16,000
things that are stored in the page pool

00:15:13,760 --> 00:15:17,600
and you you usually use those when you

00:15:16,000 --> 00:15:18,399
refill your dma buffers for the

00:15:17,600 --> 00:15:20,320
interface

00:15:18,399 --> 00:15:21,600
and you have page pool recycled direct

00:15:20,320 --> 00:15:25,040
which is used on

00:15:21,600 --> 00:15:27,199
unlimited cases where uh

00:15:25,040 --> 00:15:28,639
you break out early you have an error

00:15:27,199 --> 00:15:30,959
for example and you can recycle the

00:15:28,639 --> 00:15:34,720
packets really really fast

00:15:30,959 --> 00:15:34,720
uh you can go to the next slide now

00:15:36,800 --> 00:15:40,639
so this is this is a dummy example of

00:15:39,199 --> 00:15:43,040
how you add it

00:15:40,639 --> 00:15:44,800
on an actual driver as you see the order

00:15:43,040 --> 00:15:45,680
is zero so you're allocating a single

00:15:44,800 --> 00:15:48,639
page you

00:15:45,680 --> 00:15:50,079
you only request it to map your pages

00:15:48,639 --> 00:15:51,440
the pool size is the number of your

00:15:50,079 --> 00:15:53,440
descriptors you

00:15:51,440 --> 00:15:55,040
choose no numa node the device is just

00:15:53,440 --> 00:15:56,880
the

00:15:55,040 --> 00:15:58,320
the struct device from the kernel and

00:15:56,880 --> 00:16:01,519
the dma direction

00:15:58,320 --> 00:16:03,920
that depends uh on if you're using xdp

00:16:01,519 --> 00:16:06,000
or not so if it's

00:16:03,920 --> 00:16:07,600
if you're using xdp it has to be by

00:16:06,000 --> 00:16:09,839
direction if you're not you can just

00:16:07,600 --> 00:16:11,199
do dma from device and then you just

00:16:09,839 --> 00:16:13,759
create the page pool

00:16:11,199 --> 00:16:14,959
you register the you register the the

00:16:13,759 --> 00:16:16,480
rxq info

00:16:14,959 --> 00:16:18,320
information and then you register the

00:16:16,480 --> 00:16:20,560
memory model which is the

00:16:18,320 --> 00:16:23,600
the thing that jasper mentioned on the

00:16:20,560 --> 00:16:26,480
beginning so the member the memory

00:16:23,600 --> 00:16:28,720
forex dpn for the use case is type page

00:16:26,480 --> 00:16:28,720
pool

00:16:29,279 --> 00:16:31,759
next slide

00:16:32,560 --> 00:16:36,240
so this is what happens on the nappy

00:16:34,000 --> 00:16:37,920
polar and this is what this is the

00:16:36,240 --> 00:16:39,600
opposite side of what jasper explained

00:16:37,920 --> 00:16:42,399
so after your xdp decision

00:16:39,600 --> 00:16:43,040
of your your networking stack decision

00:16:42,399 --> 00:16:45,040
uh

00:16:43,040 --> 00:16:46,480
if there's some error any error that

00:16:45,040 --> 00:16:48,240
makes sense you can

00:16:46,480 --> 00:16:50,880
just go and recycle the page directly

00:16:48,240 --> 00:16:53,519
into the fastcas if it's a next

00:16:50,880 --> 00:16:55,600
for example if it's an xdp drop you can

00:16:53,519 --> 00:16:57,600
still recycle the page really fast

00:16:55,600 --> 00:17:00,160
if it's if it's an skb you release the

00:16:57,600 --> 00:17:02,240
page which

00:17:00,160 --> 00:17:04,079
what does is that it destroys the memory

00:17:02,240 --> 00:17:06,079
mappings that you have since the page is

00:17:04,079 --> 00:17:08,000
still actively used by the by the

00:17:06,079 --> 00:17:11,199
network interface

00:17:08,000 --> 00:17:13,199
and you then allocate new pages either

00:17:11,199 --> 00:17:14,000
from the cast or directly from your arm

00:17:13,199 --> 00:17:17,039
depending on

00:17:14,000 --> 00:17:19,039
how many packets you manage to

00:17:17,039 --> 00:17:20,799
to restore on your cast and then you

00:17:19,039 --> 00:17:22,959
fill in your your dma

00:17:20,799 --> 00:17:24,480
descriptors and and push them down the

00:17:22,959 --> 00:17:26,959
hardware

00:17:24,480 --> 00:17:28,559
uh and on the and once you're done with

00:17:26,959 --> 00:17:30,000
the driver and you want to unload it

00:17:28,559 --> 00:17:30,640
this is the two calls that you have to

00:17:30,000 --> 00:17:32,880
put to

00:17:30,640 --> 00:17:33,760
to call in order to destroy everything

00:17:32,880 --> 00:17:35,760
destroy all your

00:17:33,760 --> 00:17:38,880
dma mappings free the memory and then

00:17:35,760 --> 00:17:38,880
register the memory model

00:17:44,160 --> 00:17:49,039
that's it on the on the api uh lorenzo

00:17:53,440 --> 00:18:01,200
yes i can take the mic now

00:17:56,799 --> 00:18:04,480
and while elias and jasper

00:18:01,200 --> 00:18:08,080
introduce some general information about

00:18:04,480 --> 00:18:12,240
xdp and page pull apis i will

00:18:08,080 --> 00:18:12,880
give some k concept about adding in xdp

00:18:12,240 --> 00:18:15,760
support

00:18:12,880 --> 00:18:16,320
to a real internet driver and i will

00:18:15,760 --> 00:18:19,440
present

00:18:16,320 --> 00:18:22,960
some code snippet from the mfnet

00:18:19,440 --> 00:18:25,919
driver in particular

00:18:22,960 --> 00:18:27,120
in this section i will present the page

00:18:25,919 --> 00:18:29,360
pull lipocycle

00:18:27,120 --> 00:18:30,720
into the driver for example like

00:18:29,360 --> 00:18:34,080
creating and destroying

00:18:30,720 --> 00:18:37,679
the pool or refilling the dma engine

00:18:34,080 --> 00:18:41,120
and i will describe the xdp architecture

00:18:37,679 --> 00:18:44,400
used in the event in the mfuneta driver

00:18:41,120 --> 00:18:47,360
and i will present each supported xtp

00:18:44,400 --> 00:18:49,919
verdict with some demos

00:18:47,360 --> 00:18:50,720
and then at the end i will present some

00:18:49,919 --> 00:18:52,960
new feature

00:18:50,720 --> 00:18:57,360
some new xtp features that has been

00:18:52,960 --> 00:19:00,640
added to the driver

00:18:57,360 --> 00:19:01,679
so in this slide i reported the other

00:19:00,640 --> 00:19:03,840
specification

00:19:01,679 --> 00:19:05,600
of the marvel espresso bin that is the

00:19:03,840 --> 00:19:09,600
development board i used

00:19:05,600 --> 00:19:12,400
to add xdp support to the mfnet driver

00:19:09,600 --> 00:19:13,520
and we can see that the marvel espresso

00:19:12,400 --> 00:19:16,640
bean run

00:19:13,520 --> 00:19:19,280
a cortex a53 and

00:19:16,640 --> 00:19:21,200
for networking we have two gigabit

00:19:19,280 --> 00:19:24,799
ethernet lan ports

00:19:21,200 --> 00:19:27,039
one a fast ethernet one port and

00:19:24,799 --> 00:19:27,840
all of them are connected together

00:19:27,039 --> 00:19:30,799
through a

00:19:27,840 --> 00:19:30,799
dsa switch

00:19:33,600 --> 00:19:36,799
so let's start with page pull lifecycle

00:19:36,160 --> 00:19:39,120
and as

00:19:36,799 --> 00:19:40,640
elia said the page pull is usually

00:19:39,120 --> 00:19:42,799
associated to the

00:19:40,640 --> 00:19:44,960
to another eric's queue in order to

00:19:42,799 --> 00:19:47,520
avoid locking penalties

00:19:44,960 --> 00:19:48,080
so the page pool is usually created

00:19:47,520 --> 00:19:51,039
opening the

00:19:48,080 --> 00:19:51,360
device and is usually destroyed closing

00:19:51,039 --> 00:19:54,960
the

00:19:51,360 --> 00:19:56,960
device and the neta create page pull

00:19:54,960 --> 00:19:58,320
is responsible to actually create the

00:19:56,960 --> 00:20:01,120
pool and

00:19:58,320 --> 00:20:02,400
in page pull parameters we have the pool

00:20:01,120 --> 00:20:05,440
configuration used

00:20:02,400 --> 00:20:09,039
in the mf monitor driver as

00:20:05,440 --> 00:20:11,440
said means that we are you

00:20:09,039 --> 00:20:12,400
we are using a single page for each

00:20:11,440 --> 00:20:16,400
buffer

00:20:12,400 --> 00:20:17,440
while we flag we are relying on the page

00:20:16,400 --> 00:20:21,039
pull apis

00:20:17,440 --> 00:20:24,640
to dma mapping and mapping the page

00:20:21,039 --> 00:20:28,000
and even for dma thinking that the

00:20:24,640 --> 00:20:31,919
the buffer since the mf net the marble

00:20:28,000 --> 00:20:31,919
espresso bean is not cache current

00:20:32,000 --> 00:20:35,840
with the maid here we set the dma

00:20:34,400 --> 00:20:38,400
direction

00:20:35,840 --> 00:20:39,600
and we can see that if you are if you

00:20:38,400 --> 00:20:43,120
are loading an xdp

00:20:39,600 --> 00:20:43,919
program on the interface we it is set to

00:20:43,120 --> 00:20:46,240
the

00:20:43,919 --> 00:20:47,120
dma bi-directional in since we are

00:20:46,240 --> 00:20:51,360
supporting

00:20:47,120 --> 00:20:54,720
xdptx and the ngo xdp

00:20:51,360 --> 00:20:55,200
xmit function pointer otherwise it's set

00:20:54,720 --> 00:20:59,039
to

00:20:55,200 --> 00:21:02,000
dma from device offset

00:20:59,039 --> 00:21:03,280
is the is where the dma engine start

00:21:02,000 --> 00:21:06,640
copying the data

00:21:03,280 --> 00:21:09,760
while maxlen is the maximum sync sites

00:21:06,640 --> 00:21:11,919
supported by the driver

00:21:09,760 --> 00:21:13,120
the pool is actually created running

00:21:11,919 --> 00:21:16,480
page pool create

00:21:13,120 --> 00:21:19,760
while xdp rxq infrared

00:21:16,480 --> 00:21:20,559
and xdp rx you in for eggman model are

00:21:19,760 --> 00:21:23,440
used to

00:21:20,559 --> 00:21:24,159
as jasper said to register the memory

00:21:23,440 --> 00:21:26,799
model

00:21:24,159 --> 00:21:31,840
and to track in-flight pages in order to

00:21:26,799 --> 00:21:31,840
avoid possible memory leaks

00:21:32,640 --> 00:21:38,880
now let's consider a mvneta rx refill

00:21:35,679 --> 00:21:39,200
that is used in the napi poll in order

00:21:38,880 --> 00:21:42,799
to

00:21:39,200 --> 00:21:46,159
refill new pages to the dma engines

00:21:42,799 --> 00:21:49,520
and we can see that running page pool

00:21:46,159 --> 00:21:51,600
unlock pages we get page from the page

00:21:49,520 --> 00:21:55,039
pull caches if are available

00:21:51,600 --> 00:21:58,400
in and we avoid to going through the

00:21:55,039 --> 00:22:01,280
standard page allocator

00:21:58,400 --> 00:22:02,000
moreover setting the dma address we

00:22:01,280 --> 00:22:05,039
reserve

00:22:02,000 --> 00:22:08,080
enough space for the

00:22:05,039 --> 00:22:11,520
enough space in the head room to push

00:22:08,080 --> 00:22:12,799
new headers to the frame and we can

00:22:11,520 --> 00:22:16,080
notice here that

00:22:12,799 --> 00:22:16,880
we are not going to uh sync the thing

00:22:16,080 --> 00:22:20,000
for device

00:22:16,880 --> 00:22:22,720
the page here since the page pool

00:22:20,000 --> 00:22:23,440
will take care of these reinserting the

00:22:22,720 --> 00:22:26,720
page

00:22:23,440 --> 00:22:29,280
into the into the pool running page pull

00:22:26,720 --> 00:22:29,280
put page

00:22:32,960 --> 00:22:39,280
so the buffers the pages associated to

00:22:37,760 --> 00:22:42,080
the dma engines

00:22:39,280 --> 00:22:44,159
as i said are destroyed closing the net

00:22:42,080 --> 00:22:47,760
device

00:22:44,159 --> 00:22:49,520
and in particular uh m evonet rxdrop

00:22:47,760 --> 00:22:52,480
packet runs

00:22:49,520 --> 00:22:55,200
a page pool put full page that is used

00:22:52,480 --> 00:22:57,200
to move the pages from the gaming giants

00:22:55,200 --> 00:23:00,159
back to the page pool

00:22:57,200 --> 00:23:01,039
and then the pool is destroyed running

00:23:00,159 --> 00:23:04,400
page pull

00:23:01,039 --> 00:23:07,760
destroy and xdp

00:23:04,400 --> 00:23:08,960
rx easter eggs queueing for reg is used

00:23:07,760 --> 00:23:11,919
to

00:23:08,960 --> 00:23:12,480
register the memory model and to track

00:23:11,919 --> 00:23:15,440
the

00:23:12,480 --> 00:23:17,760
in-flight pages and to raise a warning

00:23:15,440 --> 00:23:23,840
if for example there is

00:23:17,760 --> 00:23:23,840
untracked pages not freed

00:23:26,080 --> 00:23:32,320
in this slide i reported the code run by

00:23:29,360 --> 00:23:35,360
the networking code and in particular by

00:23:32,320 --> 00:23:38,960
the ndo bpf pointer

00:23:35,360 --> 00:23:39,520
to load or unload a program in a bpf

00:23:38,960 --> 00:23:42,720
program

00:23:39,520 --> 00:23:45,039
to or from the interface as

00:23:42,720 --> 00:23:46,720
aliens easy elias and jasper said the

00:23:45,039 --> 00:23:51,520
memonet xtp setup

00:23:46,720 --> 00:23:55,760
must respect the http memory model

00:23:51,520 --> 00:23:59,200
so uh we the emi monetix dt setup

00:23:55,760 --> 00:24:01,340
fails if we are trying to load the abp

00:23:59,200 --> 00:24:02,720
any bpf program

00:24:01,340 --> 00:24:05,360
[Music]

00:24:02,720 --> 00:24:05,840
and the mq is bigger than a single page

00:24:05,360 --> 00:24:09,200
since

00:24:05,840 --> 00:24:10,720
for the moment we do not support jumbo

00:24:09,200 --> 00:24:12,640
frames for xdp

00:24:10,720 --> 00:24:16,400
but we will see later that we are trying

00:24:12,640 --> 00:24:16,400
to overcome this limitation

00:24:16,720 --> 00:24:24,720
since uh abtf require the direct access

00:24:20,880 --> 00:24:27,200
and uh and

00:24:24,720 --> 00:24:29,039
the physical memory associated the

00:24:27,200 --> 00:24:32,320
buffer must be contiguous

00:24:29,039 --> 00:24:35,440
we need to reconfigure the dma buffer

00:24:32,320 --> 00:24:38,960
so what we are doing is just

00:24:35,440 --> 00:24:39,600
stopping the net device load the xtp

00:24:38,960 --> 00:24:43,679
program

00:24:39,600 --> 00:24:43,679
on the interface and then

00:24:43,760 --> 00:24:47,840
restart the starting device

00:24:52,480 --> 00:25:01,120
here we have the description of the

00:24:56,799 --> 00:25:03,679
xtp architecture of the mvneta driver

00:25:01,120 --> 00:25:05,120
and we can see that the processing is

00:25:03,679 --> 00:25:09,039
starting in

00:25:05,120 --> 00:25:11,360
the napipol running m netapol

00:25:09,039 --> 00:25:12,559
and mevo netapol is responsible to

00:25:11,360 --> 00:25:16,080
allocate on the stack

00:25:12,559 --> 00:25:19,120
the xtp buffer and to execute

00:25:16,080 --> 00:25:20,000
the mv network xdp that is the function

00:25:19,120 --> 00:25:23,360
that is

00:25:20,000 --> 00:25:26,720
actually running the ebpf program

00:25:23,360 --> 00:25:29,200
in the ebtf sandbox

00:25:26,720 --> 00:25:30,159
and then according to the xtp verdict

00:25:29,200 --> 00:25:33,679
returned by the

00:25:30,159 --> 00:25:34,640
the by the bpf program we'll execute the

00:25:33,679 --> 00:25:37,919
proper code

00:25:34,640 --> 00:25:40,240
for example for xdp pass we build the

00:25:37,919 --> 00:25:41,520
skb and we forward it to the networking

00:25:40,240 --> 00:25:44,960
stack

00:25:41,520 --> 00:25:47,679
while for xtp drop we recycle the ps

00:25:44,960 --> 00:25:50,000
as said we recycle the page running page

00:25:47,679 --> 00:25:54,000
pull put page

00:25:50,000 --> 00:25:57,279
forex tptx we running mfnetta xtp

00:25:54,000 --> 00:25:58,240
xmit back we forward the packet back to

00:25:57,279 --> 00:26:01,520
the interface where

00:25:58,240 --> 00:26:05,600
it has been received and for uh

00:26:01,520 --> 00:26:06,720
xtp redirect we redirect the packet with

00:26:05,600 --> 00:26:09,440
frame

00:26:06,720 --> 00:26:10,559
to a remote interface for example using

00:26:09,440 --> 00:26:14,400
a devmap

00:26:10,559 --> 00:26:16,480
or for example using a cpu map

00:26:14,400 --> 00:26:18,000
using a cpu map we can redirect it to a

00:26:16,480 --> 00:26:21,600
remote cpu as we

00:26:18,000 --> 00:26:24,880
will see later in the presentation

00:26:21,600 --> 00:26:25,120
if the mvneta device is the destination

00:26:24,880 --> 00:26:28,799
of

00:26:25,120 --> 00:26:32,159
s of nxt period direct we will execute

00:26:28,799 --> 00:26:32,880
the ndo xdpx mid pointer and then we

00:26:32,159 --> 00:26:37,039
will run

00:26:32,880 --> 00:26:41,120
the mvneta xtpxmid

00:26:37,039 --> 00:26:41,120
and we will send the packet out

00:26:44,240 --> 00:26:51,120
in this slide i reported the main code

00:26:47,679 --> 00:26:52,080
the main xtp loop and in particular the

00:26:51,120 --> 00:26:55,200
mv neta

00:26:52,080 --> 00:26:57,919
rx software buffer manager

00:26:55,200 --> 00:27:00,960
and we can see that the mf netapp all

00:26:57,919 --> 00:27:03,919
receiving ddma descriptor

00:27:00,960 --> 00:27:04,799
invalid cpu caches running the amazing

00:27:03,919 --> 00:27:08,000
sinkhole

00:27:04,799 --> 00:27:10,000
for cpus since the espresso bean is not

00:27:08,000 --> 00:27:13,679
cache coherent

00:27:10,000 --> 00:27:17,200
and then we initialize the xdp buffer

00:27:13,679 --> 00:27:19,200
setting the data start pointer to the

00:27:17,200 --> 00:27:21,679
beginning of the page

00:27:19,200 --> 00:27:22,240
data pointer to the beginning of the

00:27:21,679 --> 00:27:25,279
data

00:27:22,240 --> 00:27:29,520
of the data copied by the dma engines

00:27:25,279 --> 00:27:31,679
and that at the end to the

00:27:29,520 --> 00:27:33,279
end of the data of the data copied by

00:27:31,679 --> 00:27:36,640
the dma engine

00:27:33,279 --> 00:27:39,440
then we'll execute mv-net run xtp

00:27:36,640 --> 00:27:40,159
to run the ebtf program and according to

00:27:39,440 --> 00:27:42,640
the result

00:27:40,159 --> 00:27:43,520
if the result is different from fm neta

00:27:42,640 --> 00:27:46,960
pass

00:27:43,520 --> 00:27:49,760
and magnetic tp-pass we will

00:27:46,960 --> 00:27:50,640
uh refill the dma giants with a new

00:27:49,760 --> 00:27:54,159
buffer

00:27:50,640 --> 00:27:55,760
get from uh get from the page pool

00:27:54,159 --> 00:27:59,039
otherwise on the contrary

00:27:55,760 --> 00:28:01,360
we will need to build this k in skb

00:27:59,039 --> 00:28:02,080
forward skb to the network in stack and

00:28:01,360 --> 00:28:05,039
then

00:28:02,080 --> 00:28:07,520
refill the dma engine with the new

00:28:05,039 --> 00:28:07,520
buffer

00:28:10,080 --> 00:28:14,640
in this slide i reported the code

00:28:13,120 --> 00:28:18,159
related to the mf

00:28:14,640 --> 00:28:21,360
network xtp where as i said it we run

00:28:18,159 --> 00:28:24,399
we actually run the edtf program loaded

00:28:21,360 --> 00:28:27,279
on the interface and then according to

00:28:24,399 --> 00:28:28,159
the xtp verdict returned by the ebpf

00:28:27,279 --> 00:28:31,520
program

00:28:28,159 --> 00:28:32,880
we execute the proper code we will see

00:28:31,520 --> 00:28:36,720
in the next slide

00:28:32,880 --> 00:28:39,440
that uh we will go through each possible

00:28:36,720 --> 00:28:40,559
each supporting xdp verdict in the next

00:28:39,440 --> 00:28:45,679
slides

00:28:40,559 --> 00:28:49,120
with some demos so

00:28:45,679 --> 00:28:52,320
let's start with xtp drop

00:28:49,120 --> 00:28:54,960
as we know xdp drop is returned by dbtf

00:28:52,320 --> 00:28:56,000
program in order to drop the received

00:28:54,960 --> 00:28:58,559
frame

00:28:56,000 --> 00:29:00,559
and since that now the driver is running

00:28:58,559 --> 00:29:02,320
in the nappy context and the page ref

00:29:00,559 --> 00:29:05,840
count x1

00:29:02,320 --> 00:29:10,240
page pull put page can recycle the page

00:29:05,840 --> 00:29:13,919
in the in software irq page pull cache

00:29:10,240 --> 00:29:15,039
and inside if requested inside the page

00:29:13,919 --> 00:29:18,159
pull put page

00:29:15,039 --> 00:29:19,440
the the the buffer will be syncing for

00:29:18,159 --> 00:29:22,880
device

00:29:19,440 --> 00:29:26,159
with the proper size that is the maximum

00:29:22,880 --> 00:29:28,000
between the uh between the sites covered

00:29:26,159 --> 00:29:31,200
by the dma engine

00:29:28,000 --> 00:29:33,520
and the sites accessed by the ebpf

00:29:31,200 --> 00:29:33,520
program

00:29:34,799 --> 00:29:44,080
here we have a demo to show how

00:29:38,799 --> 00:29:44,080
xtp drop works let's see

00:29:44,240 --> 00:29:51,360
so on the left there is a

00:29:47,279 --> 00:29:54,640
cell connection on my espresso bin

00:29:51,360 --> 00:29:58,320
my espresso bin is connected through

00:29:54,640 --> 00:29:58,320
the ethernet interface with

00:29:58,520 --> 00:30:04,880
ip192.168.1.2

00:30:00,559 --> 00:30:07,679
as we can see and

00:30:04,880 --> 00:30:09,039
as we can see there are no evpf program

00:30:07,679 --> 00:30:12,320
attached to the network

00:30:09,039 --> 00:30:12,320
to the network interface

00:30:13,600 --> 00:30:18,640
while on the right we have my desktop

00:30:20,240 --> 00:30:27,960
and my desktop is connected through the

00:30:23,440 --> 00:30:30,960
ethernet card with ip192 168

00:30:27,960 --> 00:30:30,960
1.30

00:30:31,440 --> 00:30:39,039
so now let's ping

00:30:35,760 --> 00:30:41,919
let's try to ping the espresso bin

00:30:39,039 --> 00:30:41,919
from the desktop

00:30:42,240 --> 00:30:48,640
and uh let's now

00:30:45,520 --> 00:30:50,720
on the on the espresso bin let's try to

00:30:48,640 --> 00:30:54,080
load the simple xtp program that just

00:30:50,720 --> 00:30:54,080
return it xtp drop

00:30:55,679 --> 00:30:58,720
okay it's loaded as you and as you can

00:30:57,919 --> 00:31:02,799
see

00:30:58,720 --> 00:31:02,799
the traffic is stopped

00:31:02,960 --> 00:31:09,600
and we have loaded on the ethernet card

00:31:05,519 --> 00:31:09,600
in xtp program with the id 1.

00:31:12,960 --> 00:31:16,000
now let's dump some xtp ath tool

00:31:15,279 --> 00:31:19,039
statistic

00:31:16,000 --> 00:31:23,360
from ath0 and in particular let's

00:31:19,039 --> 00:31:23,360
dump xdp drop counter

00:31:26,399 --> 00:31:29,679
and as you can see the xtp drop counter

00:31:29,360 --> 00:31:33,840
is

00:31:29,679 --> 00:31:33,840
is increasing

00:31:37,360 --> 00:31:46,080
okay so the driver

00:31:41,039 --> 00:31:48,399
is dropping dropping the packets

00:31:46,080 --> 00:31:48,399
okay

00:31:49,360 --> 00:31:54,799
now let's try to remove the ebpf program

00:31:51,760 --> 00:31:54,799
from h0

00:31:56,000 --> 00:32:01,679
okay and you as you can see the traffic

00:31:58,840 --> 00:32:06,480
restarts

00:32:01,679 --> 00:32:06,480
and there are no no more evpf program on

00:32:12,840 --> 00:32:17,279
eth0

00:32:14,640 --> 00:32:19,760
let's try to redump the same xtp drop

00:32:17,279 --> 00:32:19,760
counter

00:32:21,279 --> 00:32:26,159
and then as you can see the counter now

00:32:24,000 --> 00:32:26,159
is

00:32:26,840 --> 00:32:29,840
start

00:32:30,080 --> 00:32:32,799
so it works

00:32:38,080 --> 00:32:42,559
in this slide i reported the packet

00:32:41,039 --> 00:32:46,799
dropping comparison

00:32:42,559 --> 00:32:49,919
between xtp drop and dc drop

00:32:46,799 --> 00:32:54,320
and during the test i disabled dsa

00:32:49,919 --> 00:32:56,880
and i was sending a packet of 64 bytes

00:32:54,320 --> 00:32:57,840
so loading a simple xdp program that

00:32:56,880 --> 00:33:00,640
just return

00:32:57,840 --> 00:33:02,320
xp drop on the marvel espresso bin we

00:33:00,640 --> 00:33:05,519
are able to drop roughly

00:33:02,320 --> 00:33:09,760
600 kilo packets per second while

00:33:05,519 --> 00:33:12,799
we tc creating a cls act

00:33:09,760 --> 00:33:15,440
on the ingress q disc and creating

00:33:12,799 --> 00:33:18,080
a natural field to drop all packets we

00:33:15,440 --> 00:33:25,840
are able to drop roughly less than

00:33:18,080 --> 00:33:25,840
200 kilo packets per second

00:33:26,320 --> 00:33:32,559
now let's move to xtp pass and

00:33:29,519 --> 00:33:35,039
xtp pass is used to forward the frame

00:33:32,559 --> 00:33:37,440
to net to the networking stack in

00:33:35,039 --> 00:33:38,000
particular emunetta software buffer

00:33:37,440 --> 00:33:41,279
manager

00:33:38,000 --> 00:33:43,200
eric's frame relies on build skb

00:33:41,279 --> 00:33:44,399
to create the skv and to avoid

00:33:43,200 --> 00:33:47,760
allocating memory

00:33:44,399 --> 00:33:49,840
since we take into account the skb

00:33:47,760 --> 00:33:51,120
shared info at the end of the buffer

00:33:49,840 --> 00:33:54,000
tail room

00:33:51,120 --> 00:33:56,559
so we don't need to allocate memory and

00:33:54,000 --> 00:34:00,240
we can notice here that we need to run

00:33:56,559 --> 00:34:02,320
page pool release page since

00:34:00,240 --> 00:34:03,519
to inform the page pull apis that the

00:34:02,320 --> 00:34:06,159
page is actually

00:34:03,519 --> 00:34:08,480
leaving the pool since for the moment

00:34:06,159 --> 00:34:12,000
the page pull apis does not support

00:34:08,480 --> 00:34:13,599
skb recycling and so the page associated

00:34:12,000 --> 00:34:18,159
to the

00:34:13,599 --> 00:34:18,159
to the skb will go back to the page

00:34:18,839 --> 00:34:23,679
allocator

00:34:20,639 --> 00:34:26,879
here we have a demos related to show how

00:34:23,679 --> 00:34:26,879
hdp pass works

00:34:29,040 --> 00:34:35,359
so as before on the left we have my

00:34:32,560 --> 00:34:35,359
espresso bean

00:34:35,919 --> 00:34:40,240
my specimen is connected with the

00:34:38,800 --> 00:34:49,839
internet card

00:34:40,240 --> 00:34:49,839
and with the same ap 192.168.1.2

00:34:51,280 --> 00:34:54,480
and there are no ebpf program attached

00:34:53,760 --> 00:34:57,920
on to the

00:34:54,480 --> 00:35:03,839
network interface for the moment

00:34:57,920 --> 00:35:03,839
as you can see

00:35:04,560 --> 00:35:10,400
while on the right we have my desktop

00:35:08,640 --> 00:35:17,839
it is always connected with the same

00:35:10,400 --> 00:35:17,839
ethernet card with the same ip 1.30

00:35:20,960 --> 00:35:26,720
so let's bring the espresso bin from my

00:35:24,240 --> 00:35:28,720
desktop

00:35:26,720 --> 00:35:31,520
and on the espresso bin let's load a

00:35:28,720 --> 00:35:33,839
simple xtp program that just return xtp

00:35:31,520 --> 00:35:33,839
pass

00:35:38,079 --> 00:35:44,880
okay as you can see the traffic traffic

00:35:40,960 --> 00:35:48,240
is still flowing

00:35:44,880 --> 00:35:49,200
and we have loaded an xtp program with

00:35:48,240 --> 00:35:59,040
id1

00:35:49,200 --> 00:36:02,400
on eth0

00:35:59,040 --> 00:36:03,119
now let's dump the some xtp eth2

00:36:02,400 --> 00:36:09,280
statistic

00:36:03,119 --> 00:36:09,280
from ath0 and in particular the xtp pass

00:36:10,839 --> 00:36:17,200
counter

00:36:12,800 --> 00:36:17,200
okay as you can see the counter is

00:36:24,839 --> 00:36:29,200
increasing

00:36:26,640 --> 00:36:32,000
now let's remove the ebpf program from

00:36:29,200 --> 00:36:32,000
ath0

00:36:34,640 --> 00:36:41,839
okay the traffic is still flowing

00:36:41,920 --> 00:36:45,839
and as you can see there are no more

00:36:44,480 --> 00:36:48,800
ebpf programs

00:36:45,839 --> 00:36:48,800
on ath0

00:36:53,520 --> 00:36:58,880
if we try to dump again the same http

00:36:56,160 --> 00:36:58,880
pass counter

00:37:02,960 --> 00:37:06,480
now the counter is stuck since there are

00:37:05,440 --> 00:37:12,240
no program on

00:37:06,480 --> 00:37:12,240
on h0 so it worked as expected

00:37:20,079 --> 00:37:26,079
let's consider now xtptx xtptx

00:37:23,599 --> 00:37:27,200
as we know is is used to transmit back

00:37:26,079 --> 00:37:30,720
the frame

00:37:27,200 --> 00:37:32,640
to the network interface where it has

00:37:30,720 --> 00:37:34,800
been received

00:37:32,640 --> 00:37:37,040
and in particular we can notice here

00:37:34,800 --> 00:37:40,880
that in mfnet xtp

00:37:37,040 --> 00:37:43,280
smithbeck that is run by meganetronicsdp

00:37:40,880 --> 00:37:46,480
if the program returns extptx

00:37:43,280 --> 00:37:49,599
there are no needs to give me

00:37:46,480 --> 00:37:50,000
map the page on the tx ring since it is

00:37:49,599 --> 00:37:53,200
already

00:37:50,000 --> 00:37:58,320
mapped but we just need to

00:37:53,200 --> 00:37:58,320
dma sync for device the page

00:38:03,599 --> 00:38:06,880
in this slide i reported the ebpf pro

00:38:06,480 --> 00:38:10,480
the

00:38:06,880 --> 00:38:12,000
any bpf program called ssh mirror done

00:38:10,480 --> 00:38:13,839
by mateo croce

00:38:12,000 --> 00:38:16,800
you know that is will be used in the

00:38:13,839 --> 00:38:20,160
next slide in the next demo

00:38:16,800 --> 00:38:23,040
ssh mirror is used to swap

00:38:20,160 --> 00:38:23,440
for ssh connection is used to swap layer

00:38:23,040 --> 00:38:26,880
00:38:23,440 --> 00:38:30,240
layer 2 addresses and tcp ports

00:38:26,880 --> 00:38:34,240
and return xt ptx so if you try to

00:38:30,240 --> 00:38:35,079
to log using ssh remotely and you have a

00:38:34,240 --> 00:38:38,000
running

00:38:35,079 --> 00:38:38,960
ssh server locally what you're going to

00:38:38,000 --> 00:38:42,160
do is actually

00:38:38,960 --> 00:38:42,160
connect it locally

00:38:42,240 --> 00:38:48,240
so let's see how ssh mirror works

00:38:45,280 --> 00:38:48,240
on the espresso bin

00:38:50,240 --> 00:38:56,000
and on the left i will we

00:38:53,839 --> 00:38:59,200
always have my espresso bin my cellar

00:38:56,000 --> 00:39:01,520
connection on the espresso bean

00:38:59,200 --> 00:39:05,040
it is the specimen is connected using

00:39:01,520 --> 00:39:05,040
eth0 with the same ip

00:39:08,880 --> 00:39:14,160
and there are no ebpf program attached

00:39:11,280 --> 00:39:17,839
to ath0

00:39:14,160 --> 00:39:17,839
for the moment okay

00:39:21,760 --> 00:39:25,200
on the right is my desktop

00:39:26,480 --> 00:39:32,079
the hostname is loridesk and

00:39:29,599 --> 00:39:34,880
is connected using the ethernet card

00:39:32,079 --> 00:39:34,880
with the same ap

00:39:40,839 --> 00:39:46,400
192.168.130

00:39:42,480 --> 00:39:46,960
as you can see the connection to the

00:39:46,400 --> 00:39:50,240
espresso

00:39:46,960 --> 00:39:53,280
is working so now

00:39:50,240 --> 00:39:57,839
let's load the ssh mirror on the

00:39:53,280 --> 00:39:57,839
ethernet card of the espresso bin

00:39:59,280 --> 00:40:06,480
as i said before for ssh connections

00:40:02,480 --> 00:40:08,000
ssh mirror swap layer 2 already

00:40:06,480 --> 00:40:13,200
addressing

00:40:08,000 --> 00:40:13,200
and tcp ports and returning xtbtx

00:40:21,520 --> 00:40:25,839
okay the program is loaded

00:40:27,839 --> 00:40:39,440
and you can see there is an xtp program

00:40:30,640 --> 00:40:41,839
with id1 on h0

00:40:39,440 --> 00:40:44,400
now let's dump some eth tool statistics

00:40:41,839 --> 00:40:52,160
from 880 and in particular

00:40:44,400 --> 00:40:56,160
the xtptx counter

00:40:52,160 --> 00:40:59,280
okay and then

00:40:56,160 --> 00:41:07,839
from the desktop let's try to ssh

00:40:59,280 --> 00:41:07,839
into the espresso bin

00:41:10,400 --> 00:41:15,040
okay as you can see the counter the xt

00:41:12,800 --> 00:41:18,319
ptx counter is increasing

00:41:15,040 --> 00:41:21,599
and the hostname is still lower desk so

00:41:18,319 --> 00:41:24,800
we are actually connected into

00:41:21,599 --> 00:41:27,520
uh into dex the desktop instead of the

00:41:24,800 --> 00:41:27,520
espresso bin

00:41:33,119 --> 00:41:35,839
okay

00:41:38,240 --> 00:41:46,839
now let's

00:41:42,560 --> 00:41:49,839
let's remove the ssh mirror from

00:41:46,839 --> 00:41:49,839
h0

00:41:50,880 --> 00:41:53,200
okay

00:41:57,119 --> 00:42:00,480
and as you can see there are no more

00:41:58,800 --> 00:42:03,839
ebpf program attached

00:42:00,480 --> 00:42:03,839
to h0

00:42:12,000 --> 00:42:20,000
let's dump some xtph tool statistics

00:42:17,520 --> 00:42:23,599
and in particular the same let's dump

00:42:20,000 --> 00:42:23,599
the same counter xt ptx

00:42:24,800 --> 00:42:31,839
and then let's try to ssh

00:42:28,079 --> 00:42:31,839
again into the espresso bin from the

00:42:42,839 --> 00:42:48,880
desktop

00:42:45,440 --> 00:42:51,040
okay as you can see now the xtptx

00:42:48,880 --> 00:42:54,480
counter is stacked

00:42:51,040 --> 00:42:54,800
and then and this name is espresso bean

00:42:54,480 --> 00:42:59,200
so

00:42:54,800 --> 00:43:01,200
we are now we are actually uh

00:42:59,200 --> 00:43:02,319
connected into the espresso instead of

00:43:01,200 --> 00:43:13,839
the

00:43:02,319 --> 00:43:13,839
into the desktop

00:43:22,319 --> 00:43:27,839
so it works

00:43:32,800 --> 00:43:41,040
now let's consider xtp redirect

00:43:37,119 --> 00:43:44,400
and xtp2do redirect is run by

00:43:41,040 --> 00:43:47,599
mfnetta runixtp if the ebpf program

00:43:44,400 --> 00:43:50,079
retargets the playdirect

00:43:47,599 --> 00:43:51,440
doing the redirect we forward the frame

00:43:50,079 --> 00:43:55,359
to a remote interface

00:43:51,440 --> 00:43:58,000
where we run the ndo xdpx mid

00:43:55,359 --> 00:43:58,560
function pointer or if for example we

00:43:58,000 --> 00:44:00,560
can

00:43:58,560 --> 00:44:02,400
forward the frame to a remote cpu as we

00:44:00,560 --> 00:44:09,760
can see using the cpu map

00:44:02,400 --> 00:44:12,000
or for example to any af xtp socket

00:44:09,760 --> 00:44:13,359
if the memoneta device is the

00:44:12,000 --> 00:44:16,720
destination of

00:44:13,359 --> 00:44:20,240
the xlipido direct we will run the

00:44:16,720 --> 00:44:23,599
ndo xtpxmid pointer and then

00:44:20,240 --> 00:44:25,760
mvneta meets routine

00:44:23,599 --> 00:44:27,359
and we can see here that the main

00:44:25,760 --> 00:44:31,119
difference respect to

00:44:27,359 --> 00:44:33,760
the xtptx use case is that now we need

00:44:31,119 --> 00:44:35,119
to since we are now receiving the frame

00:44:33,760 --> 00:44:38,240
from a remote interface

00:44:35,119 --> 00:44:41,359
we need to map the the frame

00:44:38,240 --> 00:44:44,240
into the dma engine running dma

00:44:41,359 --> 00:44:44,240
map symbol

00:44:48,079 --> 00:44:54,000
here we have a demo related to exterior

00:44:51,440 --> 00:44:54,000
director

00:44:54,800 --> 00:45:02,480
so on the top left is the there is a

00:44:58,880 --> 00:45:04,640
in a serial connection my espresso bean

00:45:02,480 --> 00:45:06,160
the espresso bean is connected using

00:45:04,640 --> 00:45:19,839
eth0

00:45:06,160 --> 00:45:19,839
with same ip 192 168 122

00:45:20,160 --> 00:45:26,160
and there are no ebpf program attached

00:45:22,800 --> 00:45:30,400
to 880 for the moment

00:45:26,160 --> 00:45:33,839
as you can see on the top right there is

00:45:30,400 --> 00:45:33,839
my desktop

00:45:39,440 --> 00:45:43,359
the host name is loridesk and is

00:45:41,280 --> 00:45:44,319
connected using the same ethernet

00:45:43,359 --> 00:45:49,520
interface

00:45:44,319 --> 00:45:49,520
with the same ip 192 168 130

00:45:52,560 --> 00:45:54,800
okay

00:45:57,359 --> 00:46:04,160
the connection to the espresso bin works

00:46:00,720 --> 00:46:15,599
and on the bottom right there is another

00:46:04,160 --> 00:46:18,160
serial connection to the my espresso bin

00:46:15,599 --> 00:46:28,720
and on the bottom left there is another

00:46:18,160 --> 00:46:32,720
cellular connection on my espresso bin

00:46:28,720 --> 00:46:43,839
okay so on the espresso bean

00:46:32,720 --> 00:46:43,839
let's try to run qimo

00:46:54,839 --> 00:46:57,839
okay

00:47:06,880 --> 00:47:15,119
inside this vm there is a bash scripts

00:47:11,839 --> 00:47:17,920
script that is

00:47:15,119 --> 00:47:19,440
will be used to inject traffic udp

00:47:17,920 --> 00:47:25,839
traffic

00:47:19,440 --> 00:47:25,839
from the vm to the top interface

00:47:37,839 --> 00:47:40,079
okay

00:47:44,800 --> 00:47:48,559
on the espresso bean hypervisor let's

00:47:47,920 --> 00:47:51,680
start

00:47:48,559 --> 00:47:54,720
the xtp redirect

00:47:51,680 --> 00:47:55,200
ppf sample avail available on the in the

00:47:54,720 --> 00:47:58,880
bp

00:47:55,200 --> 00:48:02,079
in the kernel source 3 and xdpr direct

00:47:58,880 --> 00:48:05,920
will be used to redirect the traffic

00:48:02,079 --> 00:48:08,400
from the tap interface to the

00:48:05,920 --> 00:48:08,400
device

00:48:19,599 --> 00:48:21,839
okay

00:48:35,599 --> 00:48:41,680
in this serial connection uh

00:48:38,640 --> 00:48:45,520
let's dump some xtp

00:48:41,680 --> 00:48:49,599
tool statistics from h0

00:48:45,520 --> 00:48:52,880
and in particular let's dump xtpxmid

00:48:49,599 --> 00:48:53,920
counter that is the counter incremented

00:48:52,880 --> 00:48:57,520
whenever we

00:48:53,920 --> 00:49:01,680
run ndo xdpx mid function pointer

00:48:57,520 --> 00:49:04,480
and on my desktop let's start

00:49:01,680 --> 00:49:04,480
tcp dump

00:49:08,960 --> 00:49:17,839
okay there is some traffic

00:49:23,760 --> 00:49:31,839
from inside the vm let's start sending

00:49:27,040 --> 00:49:31,839
udp traffic to the tab interface

00:49:37,200 --> 00:49:43,119
okay and as you can see the xtp xmid

00:49:40,800 --> 00:49:45,839
counter is increasing and the traffic is

00:49:43,119 --> 00:49:48,400
redirected the udp traffic from the vm

00:49:45,839 --> 00:49:50,319
is redirected from the tap interface to

00:49:48,400 --> 00:49:51,040
the memo net device and the immigrator

00:49:50,319 --> 00:49:54,319
device

00:49:51,040 --> 00:49:59,839
is sending it out to the to my desktop

00:49:54,319 --> 00:49:59,839
as you can see

00:50:25,200 --> 00:50:27,839
okay

00:50:38,559 --> 00:50:55,839
so the the demo is working

00:51:10,800 --> 00:51:18,800
now let's consider uh

00:51:15,520 --> 00:51:19,280
now this we noticed that the espresso

00:51:18,800 --> 00:51:22,720
bean

00:51:19,280 --> 00:51:23,599
does not support other received packet

00:51:22,720 --> 00:51:27,119
steering

00:51:23,599 --> 00:51:30,880
so all the interrupts are managed by

00:51:27,119 --> 00:51:33,440
the cpu zero since the espresso bin is

00:51:30,880 --> 00:51:33,440
dark core

00:51:33,760 --> 00:51:38,160
so we got the idea to approximate hard

00:51:36,800 --> 00:51:43,359
received packet steering

00:51:38,160 --> 00:51:46,400
with cpu maps and xtp redirect

00:51:43,359 --> 00:51:48,160
cpu maps are used to move the processing

00:51:46,400 --> 00:51:50,800
to a remote cpu

00:51:48,160 --> 00:51:51,839
but so far they support just building

00:51:50,800 --> 00:51:55,119
the skb

00:51:51,839 --> 00:51:59,200
and forward the networking stack

00:51:55,119 --> 00:52:00,880
so what we did is extended

00:51:59,200 --> 00:52:02,480
the cpu map in order to add the

00:52:00,880 --> 00:52:06,000
capability to

00:52:02,480 --> 00:52:07,520
attach any bpf program to the cpu map

00:52:06,000 --> 00:52:10,640
entries

00:52:07,520 --> 00:52:12,720
and to execute this ebpf program on the

00:52:10,640 --> 00:52:16,000
remote cpu

00:52:12,720 --> 00:52:17,760
so with xdp redirect and cpu map

00:52:16,000 --> 00:52:19,920
we can have something similar to a

00:52:17,760 --> 00:52:22,960
software received packet steering

00:52:19,920 --> 00:52:24,720
since we receive traffic on the cpu 0 on

00:52:22,960 --> 00:52:28,319
the meduneta

00:52:24,720 --> 00:52:31,680
and then we perform xtp redirect

00:52:28,319 --> 00:52:34,079
on the remote cpu cpu 1

00:52:31,680 --> 00:52:36,240
and on this remote cpu we execute

00:52:34,079 --> 00:52:39,200
another abpf program

00:52:36,240 --> 00:52:47,280
that for example can perform nxtp

00:52:39,200 --> 00:52:50,559
redirect to another device

00:52:47,280 --> 00:52:53,760
here in this slide i reported the code

00:52:50,559 --> 00:52:54,880
running by the k thread binded to the

00:52:53,760 --> 00:52:59,520
remote cpu

00:52:54,880 --> 00:53:01,839
of the cpu map and we can see that the

00:52:59,520 --> 00:53:02,640
k thread run okay thread the queue

00:53:01,839 --> 00:53:06,400
packet from

00:53:02,640 --> 00:53:11,839
a pointer link queue and execute

00:53:06,400 --> 00:53:11,839
the ebpf program attached to the cpu map

00:53:12,839 --> 00:53:15,839
entry

00:53:16,800 --> 00:53:20,160
here i reported the scenario that will

00:53:19,119 --> 00:53:23,920
be uh

00:53:20,160 --> 00:53:26,079
tested in the next demo and

00:53:23,920 --> 00:53:28,079
we can see that we are we are receiving

00:53:26,079 --> 00:53:31,680
traffic

00:53:28,079 --> 00:53:33,839
on the mv net on cpu zero on a mega net

00:53:31,680 --> 00:53:36,880
we will perform an xdp redirect

00:53:33,839 --> 00:53:38,240
on the cpu map entry associated to see

00:53:36,880 --> 00:53:41,280
the cp-1

00:53:38,240 --> 00:53:43,520
and then the k-thread binded to cpu one

00:53:41,280 --> 00:53:45,520
will execute the any bpf program

00:53:43,520 --> 00:53:48,160
attached to the cpu map entry

00:53:45,520 --> 00:53:48,720
to perform another xtp redirect on a

00:53:48,160 --> 00:53:51,760
virton

00:53:48,720 --> 00:53:55,359
virtual ethernet pair

00:53:51,760 --> 00:53:57,839
for example vh0 running on cpu 1

00:53:55,359 --> 00:53:59,040
will perform will forward the traffic to

00:53:57,839 --> 00:54:02,400
the pr that is

00:53:59,040 --> 00:54:05,359
running in a remote namespace so

00:54:02,400 --> 00:54:07,200
as you can see most of the code exe is

00:54:05,359 --> 00:54:10,960
executed on cpu one

00:54:07,200 --> 00:54:13,520
so we are kind approximating of

00:54:10,960 --> 00:54:16,319
are the received packets dealing with

00:54:13,520 --> 00:54:16,319
with this approach

00:54:17,040 --> 00:54:20,559
so let's let's see the demo

00:54:22,880 --> 00:54:30,240
on the top left there there is the

00:54:26,079 --> 00:54:33,280
nssh connection to my espresso bin

00:54:30,240 --> 00:54:38,440
the espresso bin is connected using

00:54:33,280 --> 00:54:41,440
the same interface and the same ip 192.1

00:54:38,440 --> 00:54:41,440
192.168.1.2

00:54:48,240 --> 00:54:53,520
okay and as you can see for the moment

00:54:51,280 --> 00:54:56,559
there are no ebpf program attached

00:54:53,520 --> 00:54:56,559
to eth0

00:55:01,040 --> 00:55:10,240
on the top right there is my desktop

00:55:07,359 --> 00:55:12,400
his name is loridesk and is connected

00:55:10,240 --> 00:55:21,839
using the same internet interface

00:55:12,400 --> 00:55:21,839
with the same id

00:55:23,440 --> 00:55:27,440
the connection to the espresso meal is

00:55:24,960 --> 00:55:27,440
working

00:55:29,599 --> 00:55:36,000
on the bottom right

00:55:33,359 --> 00:55:38,640
there is an ssh connection to the

00:55:36,000 --> 00:55:38,640
espresso bin

00:55:41,599 --> 00:55:49,839
and even on the bottom left there is an

00:55:43,920 --> 00:55:49,839
ssh connection to the espresso bin

00:55:53,760 --> 00:56:00,400
so here on the espresso bin

00:55:57,040 --> 00:56:06,480
uh we will have

00:56:00,400 --> 00:56:06,480
a virtual inter ethernet pair ph0 vh1

00:56:09,200 --> 00:56:17,839
vh0 is it in the initial space with ip

00:56:14,839 --> 00:56:17,839
192.168.2.2

00:56:19,760 --> 00:56:26,640
while vh1

00:56:23,359 --> 00:56:31,839
is in the remote namespace with

00:56:26,640 --> 00:56:31,839
the ap 192.168.2.3

00:56:42,839 --> 00:56:50,079
okay so

00:56:46,960 --> 00:56:59,839
let's start in iperf server

00:56:50,079 --> 00:56:59,839
on the remote namespace

00:57:07,119 --> 00:57:13,839
and then

00:57:14,720 --> 00:57:22,240
let's start the xtp redirect cpu sample

00:57:18,880 --> 00:57:26,000
available in the kernel 3.

00:57:22,240 --> 00:57:27,520
xtp redirect sample will be used to

00:57:26,000 --> 00:57:29,839
create the cpu map

00:57:27,520 --> 00:57:30,559
to create a cpu map with two entries

00:57:29,839 --> 00:57:32,640
related

00:57:30,559 --> 00:57:35,359
to the two cpus available on the

00:57:32,640 --> 00:57:35,359
espresso bin

00:57:36,079 --> 00:57:42,960
and then on

00:57:39,119 --> 00:57:44,000
we will attach an xtp program xdp cpu

00:57:42,960 --> 00:57:47,599
map 0 on

00:57:44,000 --> 00:57:51,280
880 to redirect traffic to the cpu map

00:57:47,599 --> 00:57:51,280
associated to cpu 1

00:57:51,839 --> 00:58:00,000
and then on cpu one will we will attach

00:57:56,000 --> 00:58:01,200
uh on the cpmap entries entry associated

00:58:00,000 --> 00:58:04,640
to the cp1

00:58:01,200 --> 00:58:07,920
we'll load another dtf program

00:58:04,640 --> 00:58:10,240
xdp directly for to redirect ipf traffic

00:58:07,920 --> 00:58:12,559
from the cpu map to the virtual ethernet

00:58:10,240 --> 00:58:12,559
pair

00:58:18,799 --> 00:58:22,480
so let's start the program as you can

00:58:20,319 --> 00:58:26,960
see on a th0 we

00:58:22,480 --> 00:58:26,960
we load the xdpcp map program

00:58:27,599 --> 00:58:32,079
on cpu on cpu one on the entry

00:58:30,079 --> 00:58:37,359
associated to cp1 we

00:58:32,079 --> 00:58:41,119
load xtp redirect ipf

00:58:37,359 --> 00:58:43,599
this is available in xtp redirect kernel

00:58:41,119 --> 00:58:43,599
object

00:58:46,079 --> 00:58:51,280
and then we redirect from cpu receipt

00:58:49,040 --> 00:58:55,680
map entries associated to cp1

00:58:51,280 --> 00:58:55,680
pro to the filter ethernet pair

00:59:01,760 --> 00:59:06,160
now let's dump some et xtp eth2

00:59:05,119 --> 00:59:09,280
statistics

00:59:06,160 --> 00:59:09,280
from eth0

00:59:10,160 --> 00:59:19,839
and in particular let's dump xdp

00:59:13,440 --> 00:59:19,839
redirect counter

00:59:30,839 --> 00:59:34,640
okay

00:59:32,240 --> 00:59:35,520
here let's dump let's monitor the cpu

00:59:34,640 --> 00:59:38,400
utilization

00:59:35,520 --> 00:59:38,400
on the espresso b

00:59:41,680 --> 00:59:44,720
and from my desktop let's start let's

00:59:43,680 --> 01:00:00,480
start sending

00:59:44,720 --> 01:00:04,240
traffic to the virtual internet pair

01:00:00,480 --> 01:00:05,599
okay as you can see the load is balanced

01:00:04,240 --> 01:00:08,640
between the two cpu

01:00:05,599 --> 01:00:09,920
because on cpu zero the mvneta is

01:00:08,640 --> 01:00:13,200
receiving the traffic while

01:00:09,920 --> 01:00:15,119
on cpu one the cpu map is forwarding the

01:00:13,200 --> 01:00:18,559
traffic to the

01:00:15,119 --> 01:00:20,079
virtual internet pair and the xtp

01:00:18,559 --> 01:00:29,839
redirect counter on

01:00:20,079 --> 01:00:29,839
eth0 is increasing

01:00:34,640 --> 01:00:36,880
okay

01:00:37,920 --> 01:00:44,799
now let's dump some xtph tool statistics

01:00:41,440 --> 01:00:44,799
of vh1

01:00:54,000 --> 01:01:00,799
so as you can see the device is

01:00:57,200 --> 01:01:04,880
receiving traffic redirected

01:01:00,799 --> 01:01:07,040
from from the netapp to

01:01:04,880 --> 01:01:11,839
the cpu map and then to the virtual

01:01:07,040 --> 01:01:11,839
internet pair

01:01:17,839 --> 01:01:25,839
so it works

01:01:27,599 --> 01:01:32,000
now an important point to notice that in

01:01:30,720 --> 01:01:35,200
order

01:01:32,000 --> 01:01:39,440
to er for xtp success

01:01:35,200 --> 01:01:42,240
is essential that xdp is easy to debug

01:01:39,440 --> 01:01:43,760
so a proper stats accounting is

01:01:42,240 --> 01:01:47,920
fundamental

01:01:43,760 --> 01:01:51,359
for xdp success upstream

01:01:47,920 --> 01:01:51,359
we agreed to

01:01:51,599 --> 01:01:56,000
increment to always increment net device

01:01:54,400 --> 01:02:00,720
rx counters

01:01:56,000 --> 01:02:04,079
even if the program return xtp drop

01:02:00,720 --> 01:02:05,200
and we will have some more fine-grained

01:02:04,079 --> 01:02:08,240
statistics

01:02:05,200 --> 01:02:11,680
through ath tool as we see as we saw

01:02:08,240 --> 01:02:13,039
for example in the hth tool we will have

01:02:11,680 --> 01:02:16,480
a counter for

01:02:13,039 --> 01:02:19,599
redirect for xtp pass for xtp drop

01:02:16,480 --> 01:02:22,720
for xdpx and for xmit

01:02:19,599 --> 01:02:26,079
that is incremented as we saw

01:02:22,720 --> 01:02:29,200
when ndo xdp smith countries is

01:02:26,079 --> 01:02:32,240
run one point to notice

01:02:29,200 --> 01:02:36,559
that is essential that all the drivers

01:02:32,240 --> 01:02:39,280
are relying on the same name convention

01:02:36,559 --> 01:02:39,280
showed here

01:02:42,480 --> 01:02:47,119
so now i will give the mic back to

01:02:45,039 --> 01:02:51,039
jasper to talk about

01:02:47,119 --> 01:02:53,599
ecp multibuffer yes

01:02:51,039 --> 01:02:55,520
i think people can name it yes and this

01:02:53,599 --> 01:02:57,599
is i want to stress that this is work in

01:02:55,520 --> 01:02:59,680
progress this is not upstream yet but we

01:02:57,599 --> 01:03:01,599
mentioned that we want to

01:02:59,680 --> 01:03:03,280
address the problem of how we can

01:03:01,599 --> 01:03:06,559
support

01:03:03,280 --> 01:03:07,599
the jumbo frame stuff and there's also a

01:03:06,559 --> 01:03:09,839
design document

01:03:07,599 --> 01:03:11,680
that we are following i also want to

01:03:09,839 --> 01:03:14,400
mention that this is joint break between

01:03:11,680 --> 01:03:15,119
amazon and red hat so the future credit

01:03:14,400 --> 01:03:18,319
goes to

01:03:15,119 --> 01:03:19,200
the sami and lorenzo and elko who has

01:03:18,319 --> 01:03:22,079
volunteered to

01:03:19,200 --> 01:03:24,559
to work on this project can i get the

01:03:22,079 --> 01:03:24,559
next slide

01:03:28,160 --> 01:03:31,839
so the xp multibuffer has several use

01:03:31,039 --> 01:03:34,559
cases

01:03:31,839 --> 01:03:35,359
one of them is jumbo frames another one

01:03:34,559 --> 01:03:39,440
is the

01:03:35,359 --> 01:03:44,000
tso the tcp sigmund offloading and the

01:03:39,440 --> 01:03:46,319
large receive offload lro

01:03:44,000 --> 01:03:48,400
and there's also an interesting use case

01:03:46,319 --> 01:03:52,160
which is packet header split

01:03:48,400 --> 01:03:55,520
uh which is why we split

01:03:52,160 --> 01:03:57,920
the the headers into one

01:03:55,520 --> 01:03:59,599
on one segment and and the data into an

01:03:57,920 --> 01:04:02,400
another segment which

01:03:59,599 --> 01:04:03,599
i think google want to use this to do

01:04:02,400 --> 01:04:06,400
some zero copy stuff

01:04:03,599 --> 01:04:06,400
into user space

01:04:07,599 --> 01:04:10,640
and something that most people don't

01:04:10,160 --> 01:04:12,880
realize

01:04:10,640 --> 01:04:14,880
is that once we have this we can also

01:04:12,880 --> 01:04:18,720
get allow

01:04:14,880 --> 01:04:21,920
gio packets to go into a vh and

01:04:18,720 --> 01:04:25,119
and when redirecting it the opacity into

01:04:21,920 --> 01:04:26,720
generic http uh you can actually

01:04:25,119 --> 01:04:30,319
handle this better and we can also do

01:04:26,720 --> 01:04:33,280
some optimizations for cpu map

01:04:30,319 --> 01:04:34,000
but as we talked about earlier we have

01:04:33,280 --> 01:04:36,799
the

01:04:34,000 --> 01:04:37,440
epf direct access that this conjugation

01:04:36,799 --> 01:04:39,520
memory

01:04:37,440 --> 01:04:41,680
how do we satisfy that design when we

01:04:39,520 --> 01:04:43,839
all of a sudden have

01:04:41,680 --> 01:04:44,720
paid several pages that contain packet

01:04:43,839 --> 01:04:48,000
data

01:04:44,720 --> 01:04:51,359
so the proposal is that

01:04:48,000 --> 01:04:53,440
vpf we simply only allow bpf to only

01:04:51,359 --> 01:04:56,799
access the first

01:04:53,440 --> 01:05:00,319
packet buffer because that's

01:04:56,799 --> 01:05:04,079
the argument is that bbf or http

01:05:00,319 --> 01:05:06,400
is meant for layer 2 and layer 3 and

01:05:04,079 --> 01:05:08,400
it is likely that information you want

01:05:06,400 --> 01:05:09,200
and the rest of the pages that contain

01:05:08,400 --> 01:05:11,359
more have

01:05:09,200 --> 01:05:13,760
have to be handled by likely have to be

01:05:11,359 --> 01:05:16,960
handled by another layer

01:05:13,760 --> 01:05:19,359
so this to solve other things we had

01:05:16,960 --> 01:05:20,640
we chose to have the storage space for

01:05:19,359 --> 01:05:22,559
these

01:05:20,640 --> 01:05:24,240
multibuffer segments and the reference

01:05:22,559 --> 01:05:27,119
to those

01:05:24,240 --> 01:05:27,920
at the end of the first segment in the

01:05:27,119 --> 01:05:30,160
tale room

01:05:27,920 --> 01:05:31,039
and that is the same thing we do with

01:05:30,160 --> 01:05:34,079
the

01:05:31,039 --> 01:05:36,480
sktp shared info to

01:05:34,079 --> 01:05:37,200
mimic that behavior which also makes it

01:05:36,480 --> 01:05:40,079
easier

01:05:37,200 --> 01:05:42,000
when we do ftp paths because then we can

01:05:40,079 --> 01:05:45,280
create

01:05:42,000 --> 01:05:48,480
an skb based on the http frame

01:05:45,280 --> 01:05:50,400
in an easier way

01:05:48,480 --> 01:05:52,559
so i chose to call the xcp sharing for

01:05:50,400 --> 01:05:54,559
here so this area provides

01:05:52,559 --> 01:05:56,079
some user data so for each buffer we

01:05:54,559 --> 01:05:58,559
have the the page pointer

01:05:56,079 --> 01:05:59,920
the offset and the length it does the

01:05:58,559 --> 01:06:02,319
same as when

01:05:59,920 --> 01:06:03,280
x is the same way as we have the skb

01:06:02,319 --> 01:06:07,039
frac

01:06:03,280 --> 01:06:09,039
of frax in the skb

01:06:07,039 --> 01:06:10,400
it also contains mutualization of number

01:06:09,039 --> 01:06:13,359
of segments and

01:06:10,400 --> 01:06:13,359
the full packet length

01:06:13,440 --> 01:06:16,559
the trick here is that we only need a

01:06:15,359 --> 01:06:19,599
single

01:06:16,559 --> 01:06:23,200
multiple a bit indication in the http

01:06:19,599 --> 01:06:27,039
buff and the http md

01:06:23,200 --> 01:06:29,119
area so that that saves

01:06:27,039 --> 01:06:30,400
the fast path for when we don't need

01:06:29,119 --> 01:06:32,720
this multi buffer

01:06:30,400 --> 01:06:33,520
so we need to do the minimum amount of

01:06:32,720 --> 01:06:36,480
work

01:06:33,520 --> 01:06:36,480
so the next slide

01:06:39,119 --> 01:06:44,079
so in your driver you need to modify the

01:06:41,920 --> 01:06:47,200
the nappy receive loop

01:06:44,079 --> 01:06:50,799
so you take all the segments

01:06:47,200 --> 01:06:53,359
and and will build this up uh

01:06:50,799 --> 01:06:54,319
before calling xtp so you have the hdp

01:06:53,359 --> 01:06:57,520
path and you

01:06:54,319 --> 01:06:59,280
attach the segments to the shared info

01:06:57,520 --> 01:07:02,079
area

01:06:59,280 --> 01:07:02,559
and then you run the http program when

01:07:02,079 --> 01:07:07,039
all the

01:07:02,559 --> 01:07:07,039
the descriptors are attached to it

01:07:07,760 --> 01:07:11,839
and then we have some helper functions

01:07:09,359 --> 01:07:16,240
that can access this information about

01:07:11,839 --> 01:07:19,200
the the segments themselves

01:07:16,240 --> 01:07:21,280
and then you need to change the xp tx

01:07:19,200 --> 01:07:23,680
and the indio httpx mid

01:07:21,280 --> 01:07:24,480
to handle when you get these non-linear

01:07:23,680 --> 01:07:27,760
buffers in

01:07:24,480 --> 01:07:30,319
and how to to transmit this out

01:07:27,760 --> 01:07:30,960
we have mentioned the different uh viral

01:07:30,319 --> 01:07:33,520
functions

01:07:30,960 --> 01:07:35,119
that were that lorenzo demonstrated how

01:07:33,520 --> 01:07:36,960
to modify

01:07:35,119 --> 01:07:38,880
this also means we can remove the m2

01:07:36,960 --> 01:07:40,240
jack when loading the xp program because

01:07:38,880 --> 01:07:44,640
now we can support

01:07:40,240 --> 01:07:48,640
the jumbo frames and other use cases

01:07:44,640 --> 01:07:51,920
so the next slide i have

01:07:48,640 --> 01:07:55,359
a drawing how how this sort of works

01:07:51,920 --> 01:07:55,359
so we have the

01:07:55,520 --> 01:07:59,359
the first frame layout and that's has

01:07:58,079 --> 01:08:03,280
the buffer zero

01:07:59,359 --> 01:08:06,480
and we have the headroom and in the end

01:08:03,280 --> 01:08:07,520
uh we we have the shared info and this

01:08:06,480 --> 01:08:10,319
yetiva have

01:08:07,520 --> 01:08:11,359
have an array which which would point us

01:08:10,319 --> 01:08:13,760
to

01:08:11,359 --> 01:08:16,640
what it described before so that's

01:08:13,760 --> 01:08:18,480
that's how the memory layout is

01:08:16,640 --> 01:08:20,960
so the next slide i'm going to give to

01:08:18,480 --> 01:08:25,839
lorenzo because it contains

01:08:20,960 --> 01:08:29,679
some code he wrote

01:08:25,839 --> 01:08:33,040
yes actually in this slide

01:08:29,679 --> 01:08:36,159
uh this work as jasper said is

01:08:33,040 --> 01:08:38,239
work in progress and in this slide i

01:08:36,159 --> 01:08:42,719
reported the

01:08:38,239 --> 01:08:45,440
two functions that are already upstream

01:08:42,719 --> 01:08:47,199
and this is the first series to

01:08:45,440 --> 01:08:48,560
implement

01:08:47,199 --> 01:08:50,480
this is a preliminary series to

01:08:48,560 --> 01:08:52,640
implement

01:08:50,480 --> 01:08:55,040
xtp multi-buffer support in the mf

01:08:52,640 --> 01:08:55,040
network

01:08:55,120 --> 01:08:58,719
in particular mvneta software buffer

01:08:58,000 --> 01:09:02,159
manager

01:08:58,719 --> 01:09:05,839
at the rigs fragment is used

01:09:02,159 --> 01:09:10,560
to create the xtp nonlinear buffer

01:09:05,839 --> 01:09:13,600
receiving the dma descriptor

01:09:10,560 --> 01:09:14,400
and as you can see we access the skb

01:09:13,600 --> 01:09:17,279
share this

01:09:14,400 --> 01:09:18,640
let's call charity info at the end of

01:09:17,279 --> 01:09:21,759
the first buffer

01:09:18,640 --> 01:09:25,359
and then we link together uh

01:09:21,759 --> 01:09:28,560
using the the skb shared info

01:09:25,359 --> 01:09:32,080
all the subsequent buffers

01:09:28,560 --> 01:09:34,400
and uh in the while

01:09:32,080 --> 01:09:35,359
mvneta software buffer manager build

01:09:34,400 --> 01:09:38,400
scabie

01:09:35,359 --> 01:09:40,319
will be used whenever the program the

01:09:38,400 --> 01:09:44,159
epf program return xt

01:09:40,319 --> 01:09:48,239
pass and will be used to construct

01:09:44,159 --> 01:09:49,199
a non-linear skb from a non-linear xtp

01:09:48,239 --> 01:09:52,319
buffer

01:09:49,199 --> 01:09:53,440
as we can see we build the skb from the

01:09:52,319 --> 01:09:57,120
first buffer

01:09:53,440 --> 01:09:58,480
and then we access the sqb shared info

01:09:57,120 --> 01:10:02,000
at the end of the first

01:09:58,480 --> 01:10:05,040
xtp buffer to construct the non-linear

01:10:02,000 --> 01:10:07,520
part of the skb

01:10:05,040 --> 01:10:08,400
and this as i said this code is already

01:10:07,520 --> 01:10:11,520
upstream

01:10:08,400 --> 01:10:16,960
while i'm still i'm actually

01:10:11,520 --> 01:10:20,080
working on the second part to support

01:10:16,960 --> 01:10:23,120
the xtp to support the

01:10:20,080 --> 01:10:26,320
tx part we need to merge

01:10:23,120 --> 01:10:29,440
uh some as jesper said

01:10:26,320 --> 01:10:30,159
some metadata information to properly

01:10:29,440 --> 01:10:34,000
support

01:10:30,159 --> 01:10:34,000
the tx the tx part

01:10:35,280 --> 01:10:39,520
i will now give the mic back to ilias to

01:10:38,400 --> 01:10:45,679
talk about

01:10:39,520 --> 01:10:48,400
how we can test an xtp driver

01:10:45,679 --> 01:10:49,040
yeah hello this is mostly a sum up of

01:10:48,400 --> 01:10:50,800
what

01:10:49,040 --> 01:10:53,280
you saw in lorenzo slides once you're

01:10:50,800 --> 01:10:55,360
done with the driver you need to check

01:10:53,280 --> 01:10:56,800
uh all of the functionality it offers so

01:10:55,360 --> 01:10:58,640
these are the actions we

01:10:56,800 --> 01:11:01,280
we actually require for a driver to be

01:10:58,640 --> 01:11:04,320
merged into mainline you need to support

01:11:01,280 --> 01:11:05,840
uh all of the xdp actions we we have at

01:11:04,320 --> 01:11:09,760
the moment so

01:11:05,840 --> 01:11:09,760
uh first of all the xdb pass

01:11:10,000 --> 01:11:13,840
can you move on the next slide lorenzo

01:11:16,480 --> 01:11:20,239
so uh the first thing is that you need

01:11:18,560 --> 01:11:22,239
the the kernel has a bunch of samples

01:11:20,239 --> 01:11:23,600
you can compile and and do the testing

01:11:22,239 --> 01:11:26,560
so you can see

01:11:23,600 --> 01:11:27,520
on the middle of the page there's make m

01:11:26,560 --> 01:11:29,600
samples

01:11:27,520 --> 01:11:31,600
uh so you this way you can build all the

01:11:29,600 --> 01:11:34,239
kernel samples you want

01:11:31,600 --> 01:11:34,880
uh so the xdp pass the xdp pass is an

01:11:34,239 --> 01:11:37,199
easy one

01:11:34,880 --> 01:11:39,120
uh you just need to verify that whenever

01:11:37,199 --> 01:11:41,120
you're sending traffic towards the host

01:11:39,120 --> 01:11:43,280
if you load the bpf program

01:11:41,120 --> 01:11:46,239
that only allows xdp pass you the

01:11:43,280 --> 01:11:48,400
network traffic just continues to exist

01:11:46,239 --> 01:11:50,159
uh so what you do is load the bps

01:11:48,400 --> 01:11:52,560
program and

01:11:50,159 --> 01:11:54,080
verify that the kernel the packets are

01:11:52,560 --> 01:11:57,120
still that you're on your

01:11:54,080 --> 01:11:59,600
receiving end kernel network stack

01:11:57,120 --> 01:12:00,239
the xdp drop is pretty much the same

01:11:59,600 --> 01:12:02,960
story

01:12:00,239 --> 01:12:05,199
there's a bpf program in the kernel

01:12:02,960 --> 01:12:08,880
samples you can use and what you can do

01:12:05,199 --> 01:12:11,600
is load and make sure that every packet

01:12:08,880 --> 01:12:14,960
you send is dropped

01:12:11,600 --> 01:12:18,159
next slide so on the

01:12:14,960 --> 01:12:20,320
the xdptx now the xdptx as lorenzo said

01:12:18,159 --> 01:12:22,080
this means that you're actually sending

01:12:20,320 --> 01:12:25,360
the packet back out of the interface

01:12:22,080 --> 01:12:27,440
it came from so an easy way to verify

01:12:25,360 --> 01:12:29,120
that and since there's

01:12:27,440 --> 01:12:30,960
quite a few things that could go wrong

01:12:29,120 --> 01:12:32,560
during the driver development

01:12:30,960 --> 01:12:34,800
you need to be accurate on your testing

01:12:32,560 --> 01:12:37,280
so one of the things that we did during

01:12:34,800 --> 01:12:39,760
developing drivers was that

01:12:37,280 --> 01:12:41,840
uh we started xdptx on the receiving

01:12:39,760 --> 01:12:43,840
cost and then on the

01:12:41,840 --> 01:12:45,440
on the on the host that the xdp driver

01:12:43,840 --> 01:12:47,360
was running and then on your testing

01:12:45,440 --> 01:12:47,760
machine you can start sending packets

01:12:47,360 --> 01:12:51,199
while

01:12:47,760 --> 01:12:51,600
starting tcp dump as well as you know

01:12:51,199 --> 01:12:53,520
the

01:12:51,600 --> 01:12:55,040
probably as you probably know the kernel

01:12:53,520 --> 01:12:58,560
has an api

01:12:55,040 --> 01:13:01,120
uh that you can create a

01:12:58,560 --> 01:13:03,120
number of packets and send it over so by

01:13:01,120 --> 01:13:05,280
using this and sending dummy packets

01:13:03,120 --> 01:13:07,120
uh dummy udp packets that's the

01:13:05,280 --> 01:13:08,320
important detail you have to notice here

01:13:07,120 --> 01:13:10,800
that the

01:13:08,320 --> 01:13:12,080
sample program in the kernel only

01:13:10,800 --> 01:13:14,480
directs the packet if it

01:13:12,080 --> 01:13:16,000
only swaps the mac addresses if it's a

01:13:14,480 --> 01:13:19,120
udp packet

01:13:16,000 --> 01:13:21,040
so by sending a discrete amount uh

01:13:19,120 --> 01:13:22,239
explicit amount of packets what you do

01:13:21,040 --> 01:13:24,560
is that you

01:13:22,239 --> 01:13:26,400
open your tcp dump again once you're

01:13:24,560 --> 01:13:28,000
done and you expect to find the exact

01:13:26,400 --> 01:13:30,159
same number of packets

01:13:28,000 --> 01:13:31,520
which means that all of the packets you

01:13:30,159 --> 01:13:34,000
send to the host

01:13:31,520 --> 01:13:36,320
got redirected correctly and came back

01:13:34,000 --> 01:13:36,320
to you

01:13:37,360 --> 01:13:41,280
and you can do this for for a different

01:13:39,520 --> 01:13:43,760
amount of packets to make sure that the

01:13:41,280 --> 01:13:48,400
xdptx functionality on the driver

01:13:43,760 --> 01:13:48,400
is correct next slide

01:13:49,520 --> 01:13:54,960
so the last one is not the last one

01:13:51,679 --> 01:13:54,960
there's xdp redirect

01:13:55,040 --> 01:14:00,960
and this is testing af xdp this is the

01:13:58,800 --> 01:14:02,320
xdp part that can offload packets to

01:14:00,960 --> 01:14:04,000
user space

01:14:02,320 --> 01:14:06,640
again there's a sample in the kernel

01:14:04,000 --> 01:14:09,199
that you can use and what you can do is

01:14:06,640 --> 01:14:10,560
run on on your xtp host you can run the

01:14:09,199 --> 01:14:12,800
driver

01:14:10,560 --> 01:14:14,080
uh which what it does is that it dumps

01:14:12,800 --> 01:14:16,560
all the packets on the

01:14:14,080 --> 01:14:19,280
on the user space and you and drops them

01:14:16,560 --> 01:14:21,120
at the same time so you get some reports

01:14:19,280 --> 01:14:23,040
uh on how many packets you received and

01:14:21,120 --> 01:14:25,040
how many packets you dropped and

01:14:23,040 --> 01:14:27,199
then from another host start sending

01:14:25,040 --> 01:14:29,040
traffic to that interface

01:14:27,199 --> 01:14:31,920
and make sure that the packets arrive

01:14:29,040 --> 01:14:34,800
and are getting dropped correctly

01:14:31,920 --> 01:14:34,800
uh the next slide

01:14:35,360 --> 01:14:38,640
the next one is what you saw there's two

01:14:37,840 --> 01:14:40,640
things you can

01:14:38,640 --> 01:14:43,360
you can do to test over here one of them

01:14:40,640 --> 01:14:46,159
is create a namespace

01:14:43,360 --> 01:14:47,679
and then start injecting packets into

01:14:46,159 --> 01:14:50,960
that namespace

01:14:47,679 --> 01:14:52,719
while on your on your bpf program and

01:14:50,960 --> 01:14:55,679
the host that's running the bpf you can

01:14:52,719 --> 01:14:58,159
run the xdp redirect from samples

01:14:55,679 --> 01:14:59,679
now what should go in there what should

01:14:58,159 --> 01:15:00,719
happen is that the moment you start

01:14:59,679 --> 01:15:03,440
injecting

01:15:00,719 --> 01:15:05,280
packets into the the interface of the

01:15:03,440 --> 01:15:08,159
first namespace that should be

01:15:05,280 --> 01:15:09,040
redirected into v0 and then you should

01:15:08,159 --> 01:15:10,800
eventually

01:15:09,040 --> 01:15:13,120
check the outgoing traffic or your of

01:15:10,800 --> 01:15:14,880
your eth interface and you should see

01:15:13,120 --> 01:15:17,760
the packets that you initially injected

01:15:14,880 --> 01:15:20,960
in v1 coming out of 88 0.

01:15:17,760 --> 01:15:21,360
the second thing you can do to test is

01:15:20,960 --> 01:15:23,360
the

01:15:21,360 --> 01:15:25,040
the slide that you saw from lorenzo

01:15:23,360 --> 01:15:26,480
launching a virtual machine

01:15:25,040 --> 01:15:29,199
and doing pretty much the same thing

01:15:26,480 --> 01:15:32,719
there uh the reason that this

01:15:29,199 --> 01:15:35,679
works is that you could very well have a

01:15:32,719 --> 01:15:37,360
single a single host with two interfaces

01:15:35,679 --> 01:15:39,280
on it and do the test on that host on

01:15:37,360 --> 01:15:40,320
real hardware the problem with this

01:15:39,280 --> 01:15:42,880
approach

01:15:40,320 --> 01:15:44,239
is that both of the drivers you'll have

01:15:42,880 --> 01:15:47,920
on that host need to support

01:15:44,239 --> 01:15:49,760
ndo xdpx mit and an xdp now keeping in

01:15:47,920 --> 01:15:53,360
mind that the amount of drivers that

01:15:49,760 --> 01:15:54,960
that support xdp is not it's it's rather

01:15:53,360 --> 01:15:57,920
limited at the moment and it's mostly

01:15:54,960 --> 01:15:59,920
limited to 40 and 100 gigabit interfaces

01:15:57,920 --> 01:16:01,520
getting access to two of them is

01:15:59,920 --> 01:16:03,199
substantially harder so

01:16:01,520 --> 01:16:05,040
what you can piggyback on and what you

01:16:03,199 --> 01:16:06,880
can work on is that the

01:16:05,040 --> 01:16:09,760
verteio interfaces on the kernel the

01:16:06,880 --> 01:16:13,040
verteionet and the v8 interface

01:16:09,760 --> 01:16:14,000
do support the xdp redirect so you can

01:16:13,040 --> 01:16:15,600
use those

01:16:14,000 --> 01:16:17,679
with a few tricks and make sure that

01:16:15,600 --> 01:16:19,600
your driver is working

01:16:17,679 --> 01:16:21,840
on all of the use cases xtp is trying to

01:16:19,600 --> 01:16:21,840
cover

01:16:22,320 --> 01:16:25,600
yeah i think it's worth mentioning here

01:16:24,000 --> 01:16:26,000
that there's also an irc channel that

01:16:25,600 --> 01:16:27,440
we're

01:16:26,000 --> 01:16:31,280
trying to be active and help people

01:16:27,440 --> 01:16:31,280
around in case they need anything

01:16:31,840 --> 01:16:37,280
los angeles name is not too original

01:16:33,760 --> 01:16:40,560
it's xdp on freenode

01:16:37,280 --> 01:16:42,000
so do anyone have any questions

01:16:40,560 --> 01:16:44,880
there is a question on the there's a

01:16:42,000 --> 01:16:44,880
question on the chat

01:16:44,960 --> 01:16:52,239
i will stop my share the screen now

01:16:48,159 --> 01:16:52,960
yeah for multi buffers does the

01:16:52,239 --> 01:16:54,960
requirement

01:16:52,960 --> 01:16:57,679
hold that its buffer is still contained

01:16:54,960 --> 01:16:57,679
within a page

01:16:58,560 --> 01:17:02,480
you mean from the fragment or the the

01:17:00,960 --> 01:17:04,080
first one has to be contained on a

01:17:02,480 --> 01:17:06,000
single page because that's that's what

01:17:04,080 --> 01:17:08,800
serves the direct access the bpf direct

01:17:06,000 --> 01:17:08,800
access requirement

01:17:09,440 --> 01:17:13,199
so yeah the the memory model still still

01:17:11,679 --> 01:17:13,840
remains there and that's that's the

01:17:13,199 --> 01:17:15,600
whole

01:17:13,840 --> 01:17:17,360
reason for creating the page build api

01:17:15,600 --> 01:17:18,960
in the first place we do realize that

01:17:17,360 --> 01:17:21,679
you know allocating a single

01:17:18,960 --> 01:17:22,400
a single page for the network interfaces

01:17:21,679 --> 01:17:25,280
and and

01:17:22,400 --> 01:17:27,360
doing the manual recycling uh in every

01:17:25,280 --> 01:17:27,679
driver would mess things up and people

01:17:27,360 --> 01:17:30,159
would

01:17:27,679 --> 01:17:31,679
get out of line and probably have to fix

01:17:30,159 --> 01:17:34,080
a bunch of bugs so that that was the

01:17:31,679 --> 01:17:36,159
reason the baseball api was introduced

01:17:34,080 --> 01:17:39,520
in the first place because you pretty

01:17:36,159 --> 01:17:42,840
much get the recycling for free it's

01:17:39,520 --> 01:17:45,040
mostly debugged up to now yeah we do

01:17:42,840 --> 01:17:48,159
have

01:17:45,040 --> 01:17:50,080
okay on arm64 servers we don't have too

01:17:48,159 --> 01:17:51,520
many statistics because there is one

01:17:50,080 --> 01:17:54,159
driver that has been added

01:17:51,520 --> 01:17:56,400
uh on on an nxp platform i don't have

01:17:54,159 --> 01:17:59,199
numbers to it at the moment but

01:17:56,400 --> 01:18:00,719
uh the only tests we managed to do on an

01:17:59,199 --> 01:18:02,800
arm server

01:18:00,719 --> 01:18:05,120
was on a one gigabit interface and the

01:18:02,800 --> 01:18:06,960
performance was very similar but

01:18:05,120 --> 01:18:09,440
frankly with the amount of you know the

01:18:06,960 --> 01:18:11,040
cpu power you get nowadays

01:18:09,440 --> 01:18:14,320
anyone could get the performance up on

01:18:11,040 --> 01:18:14,320
one gigabit interfaces

01:18:17,840 --> 01:18:21,760
moreover if i can add something i guess

01:18:20,800 --> 01:18:25,040
from my point of view

01:18:21,760 --> 01:18:27,520
is even very interesting to

01:18:25,040 --> 01:18:29,600
add the xtp support even like on

01:18:27,520 --> 01:18:33,040
embedded device for example

01:18:29,600 --> 01:18:36,480
i think we should try to find a way to

01:18:33,040 --> 01:18:39,520
link together the ethernet part with

01:18:36,480 --> 01:18:42,320
the wireless one for example as

01:18:39,520 --> 01:18:42,719
far as i know at the moment there are

01:18:42,320 --> 01:18:46,719
some

01:18:42,719 --> 01:18:49,440
device that wireless device that support

01:18:46,719 --> 01:18:50,480
uh to offload the ethernet frame instead

01:18:49,440 --> 01:18:53,280
of the wireless

01:18:50,480 --> 01:18:54,400
instead of using the wireless header so

01:18:53,280 --> 01:18:56,880
probably we

01:18:54,400 --> 01:18:57,520
i guess from a point of view is doable

01:18:56,880 --> 01:19:01,040
to

01:18:57,520 --> 01:19:04,640
support at least the xtptx part

01:19:01,040 --> 01:19:06,960
on the on the

01:19:04,640 --> 01:19:08,800
on the wireless stuff so we can just for

01:19:06,960 --> 01:19:09,280
example redirect from the internet into

01:19:08,800 --> 01:19:11,120
from

01:19:09,280 --> 01:19:12,719
in the in a router from the internet

01:19:11,120 --> 01:19:15,679
interface to the

01:19:12,719 --> 01:19:15,679
to the wireless one

01:19:16,320 --> 01:19:19,920
uh there's another question jasper

01:19:18,960 --> 01:19:22,480
jasper on the

01:19:19,920 --> 01:19:24,400
af xdp deployment you you did have an

01:19:22,480 --> 01:19:26,640
interesting uk use case some months ago

01:19:24,400 --> 01:19:26,640
right

01:19:27,600 --> 01:19:32,400
i can't remember that one you have to

01:19:29,360 --> 01:19:33,120
did the time sensitive networking as a

01:19:32,400 --> 01:19:35,920
use case

01:19:33,120 --> 01:19:36,400
yeah the the af xdp one i we try to run

01:19:35,920 --> 01:19:38,239
some

01:19:36,400 --> 01:19:39,520
some examples even on arm servers that

01:19:38,239 --> 01:19:41,760
you mentioned uh

01:19:39,520 --> 01:19:42,800
so there's there's time sensitive

01:19:41,760 --> 01:19:45,760
networking

01:19:42,800 --> 01:19:47,440
going on uh in linux at the moment that

01:19:45,760 --> 01:19:48,800
there has been some drivers that have

01:19:47,440 --> 01:19:50,960
been committed

01:19:48,800 --> 01:19:52,080
and although you know when you're

01:19:50,960 --> 01:19:53,760
talking about

01:19:52,080 --> 01:19:55,600
traffic shaping and stuff like that you

01:19:53,760 --> 01:19:59,040
usually mention the upstream

01:19:55,600 --> 01:20:00,480
uh traffic time sensitive networking has

01:19:59,040 --> 01:20:04,080
very

01:20:00,480 --> 01:20:07,679
very bound latencies uh

01:20:04,080 --> 01:20:08,639
so since you are really interested in

01:20:07,679 --> 01:20:11,600
speeding up

01:20:08,639 --> 01:20:13,760
uh the use case the packet delivery from

01:20:11,600 --> 01:20:15,600
the driver down to user spaces you have

01:20:13,760 --> 01:20:17,760
mostly user space applications dealing

01:20:15,600 --> 01:20:19,679
with the time sensitive stuff

01:20:17,760 --> 01:20:21,840
the afx dp use case proved to be

01:20:19,679 --> 01:20:26,080
interesting enough i mean we got

01:20:21,840 --> 01:20:29,280
i think we got four times less

01:20:26,080 --> 01:20:30,639
less latency than we got on the initial

01:20:29,280 --> 01:20:32,320
measurements we did on the kernel

01:20:30,639 --> 01:20:33,840
network stack but that that kind of

01:20:32,320 --> 01:20:36,639
makes sense because you

01:20:33,840 --> 01:20:38,080
you know you you skip a bunch of stuff

01:20:36,639 --> 01:20:41,920
you don't need on this one

01:20:38,080 --> 01:20:46,880
uh like firewalling or the tc hooks and

01:20:41,920 --> 01:20:49,199
the skb allocation and a bunch of stuff

01:20:46,880 --> 01:20:50,159
yeah i can also see in the in in the

01:20:49,199 --> 01:20:52,800
chat that

01:20:50,159 --> 01:20:53,840
people mentioned that actually you can

01:20:52,800 --> 01:20:57,199
run dpdk

01:20:53,840 --> 01:21:00,639
on top of af xdp and

01:20:57,199 --> 01:21:03,760
yeah you can

01:21:00,639 --> 01:21:08,320
and there's also obs open v switch

01:21:03,760 --> 01:21:08,320
that also have support for efx dp

01:21:11,120 --> 01:21:15,520
wasn't there an interesting application

01:21:12,719 --> 01:21:18,239
of af afxdp from

01:21:15,520 --> 01:21:20,560
cloudfare where they were dumping the

01:21:18,239 --> 01:21:21,360
ddos packets they got on a honeypot and

01:21:20,560 --> 01:21:23,360
then they

01:21:21,360 --> 01:21:25,120
i think they did it with af xdp so they

01:21:23,360 --> 01:21:26,800
had to drop a bunch of packets in user

01:21:25,120 --> 01:21:27,760
space and they had a special application

01:21:26,800 --> 01:21:30,880
to

01:21:27,760 --> 01:21:33,600
to pass the packets and figure out uh

01:21:30,880 --> 01:21:34,719
some of the things they wanted on the

01:21:33,600 --> 01:21:37,040
attack

01:21:34,719 --> 01:21:39,360
i can find a block let me try and dig up

01:21:37,040 --> 01:21:39,360
the block

01:21:41,120 --> 01:21:44,719
so so that the i want to mention the

01:21:43,440 --> 01:21:47,760
espresso bin that we

01:21:44,719 --> 01:21:49,040
we chose to play with and the the driver

01:21:47,760 --> 01:21:52,800
we have mentioned

01:21:49,040 --> 01:21:54,320
in this this talk we sort of chose it

01:21:52,800 --> 01:21:56,000
for several reasons that was so there

01:21:54,320 --> 01:22:00,080
was actually a real use case

01:21:56,000 --> 01:22:03,440
that that was i think it was a canadian

01:22:00,080 --> 01:22:06,560
telco that wanted that had this chip in

01:22:03,440 --> 01:22:07,520
in some of the the customer premises

01:22:06,560 --> 01:22:10,880
equipment

01:22:07,520 --> 01:22:13,920
uh where or was it g-pawn or something

01:22:10,880 --> 01:22:15,760
where they wanted to redirect or

01:22:13,920 --> 01:22:18,159
have something faster they could could

01:22:15,760 --> 01:22:21,199
do routing for them

01:22:18,159 --> 01:22:22,320
so i hope they are using that because i

01:22:21,199 --> 01:22:24,320
sort of lost contact

01:22:22,320 --> 01:22:26,719
with those guys but they had a real use

01:22:24,320 --> 01:22:29,840
case for for speeding up this specific

01:22:26,719 --> 01:22:33,520
chip and we definitely

01:22:29,840 --> 01:22:36,560
done that moreover now

01:22:33,520 --> 01:22:36,560
there is even the

01:22:36,960 --> 01:22:40,159
let's say i hand counterpart that is the

01:22:39,679 --> 01:22:43,120
uh

01:22:40,159 --> 01:22:45,199
machiato bean that is now supporting zdp

01:22:43,120 --> 01:22:48,080
as well

01:22:45,199 --> 01:22:50,080
oh yeah that's right machiato and i

01:22:48,080 --> 01:22:52,239
actually have that hardware

01:22:50,080 --> 01:22:54,080
right behind me somewhere but i haven't

01:22:52,239 --> 01:22:57,520
hadn't hadn't tested a kernel

01:22:54,080 --> 01:23:00,400
with that support yet that will actually

01:22:57,520 --> 01:23:00,400
be interesting to see

01:23:00,800 --> 01:23:04,239
so there's one question if if we have to

01:23:02,480 --> 01:23:08,080
disable hardware check some

01:23:04,239 --> 01:23:11,199
offloading with http uh not

01:23:08,080 --> 01:23:13,120
we don't need to not

01:23:11,199 --> 01:23:15,360
it's not fully supported i don't know

01:23:13,120 --> 01:23:17,280
how to explain it directly

01:23:15,360 --> 01:23:18,800
so i wouldn't say we need to disable

01:23:17,280 --> 01:23:22,239
hardware take some

01:23:18,800 --> 01:23:23,679
uh but right now at least when we send

01:23:22,239 --> 01:23:24,560
the packet we have to run the receive we

01:23:23,679 --> 01:23:26,960
can

01:23:24,560 --> 01:23:28,639
we can still have it but not we cannot

01:23:26,960 --> 01:23:31,120
access it yet

01:23:28,639 --> 01:23:33,040
but there's work in this area to improve

01:23:31,120 --> 01:23:36,239
this

01:23:33,040 --> 01:23:36,719
maze have a funny question is there

01:23:36,239 --> 01:23:39,679
something

01:23:36,719 --> 01:23:41,600
lighter than a hdp for fast packet

01:23:39,679 --> 01:23:44,960
forwarding

01:23:41,600 --> 01:23:44,960
i don't quite get the question

01:23:45,600 --> 01:23:49,360
yeah maybe i should just unmute um

01:23:47,900 --> 01:23:51,600
[Music]

01:23:49,360 --> 01:23:53,360
so i've been trying to you know get

01:23:51,600 --> 01:23:56,000
faster tethering for android

01:23:53,360 --> 01:23:57,520
and the problem is we have a bazillion

01:23:56,000 --> 01:23:59,520
different drivers you've got cellular

01:23:57,520 --> 01:24:00,639
interfaces wi-fi interfaces usb

01:23:59,520 --> 01:24:02,719
interfaces

01:24:00,639 --> 01:24:05,520
the number of chipsets is infinite the

01:24:02,719 --> 01:24:08,960
number of drivers is infinite etc etc

01:24:05,520 --> 01:24:10,400
um trying to add xtp support to all of

01:24:08,960 --> 01:24:13,840
these drivers

01:24:10,400 --> 01:24:15,679
doesn't seem feasible um correct me if

01:24:13,840 --> 01:24:16,639
i'm wrong but that's that's kind of my

01:24:15,679 --> 01:24:20,000
takeaway from

01:24:16,639 --> 01:24:20,639
from from this talk um and at the same

01:24:20,000 --> 01:24:23,520
time

01:24:20,639 --> 01:24:26,159
um obviously we'd like to get better

01:24:23,520 --> 01:24:29,920
performance especially with 5g

01:24:26,159 --> 01:24:30,960
um i've been looking at some of these

01:24:29,920 --> 01:24:35,280
drivers

01:24:30,960 --> 01:24:37,920
and they seem pretty bad um

01:24:35,280 --> 01:24:38,320
one of the common problems is there's

01:24:37,920 --> 01:24:41,040
like

01:24:38,320 --> 01:24:43,199
usually some sort of crazy non-gso

01:24:41,040 --> 01:24:45,840
aggregation happening

01:24:43,199 --> 01:24:46,960
where you get like multiple packets and

01:24:45,840 --> 01:24:49,600
a buffer

01:24:46,960 --> 01:24:51,760
and then you basically allocate skbs and

01:24:49,600 --> 01:24:53,600
copy the packets into the new skb

01:24:51,760 --> 01:24:55,440
and or you do the reverse on the

01:24:53,600 --> 01:24:56,880
transmit side where you aggregate the

01:24:55,440 --> 01:24:58,639
packets like for example

01:24:56,880 --> 01:25:00,400
in order to get good throughput on usb

01:24:58,639 --> 01:25:01,840
you basically have to send more than an

01:25:00,400 --> 01:25:03,520
mtu of data

01:25:01,840 --> 01:25:05,679
yeah that's that's that's typically you

01:25:03,520 --> 01:25:09,840
know from all the older 4g

01:25:05,679 --> 01:25:11,440
modems right so

01:25:09,840 --> 01:25:13,840
kind of like what what do you think is

01:25:11,440 --> 01:25:13,840
the solution

01:25:13,920 --> 01:25:19,440
please do my job for me one

01:25:17,440 --> 01:25:21,120
one one comment first then i'll just

01:25:19,440 --> 01:25:23,520
leave it to lorenzo i think

01:25:21,120 --> 01:25:24,639
and one comment is that i talk to the

01:25:23,520 --> 01:25:26,800
wireless guys

01:25:24,639 --> 01:25:28,800
and i think we agreed that we should

01:25:26,800 --> 01:25:32,080
call something else than http

01:25:28,800 --> 01:25:33,440
but we might want to support like

01:25:32,080 --> 01:25:35,600
and lorenzo talked about that we can

01:25:33,440 --> 01:25:38,719
redirect into a

01:25:35,600 --> 01:25:40,159
wi-fi device but and then you mentioned

01:25:38,719 --> 01:25:41,920
you have to have aggregates to actually

01:25:40,159 --> 01:25:44,159
make it fast

01:25:41,920 --> 01:25:45,600
which is depends on the multi buffer

01:25:44,159 --> 01:25:48,000
work

01:25:45,600 --> 01:25:49,280
but we i talked to the wireless guys and

01:25:48,000 --> 01:25:51,920
they they wanted

01:25:49,280 --> 01:25:53,040
they wanted a bpf hook but

01:25:51,920 --> 01:25:54,719
[Music]

01:25:53,040 --> 01:25:56,719
we we should call it something else it

01:25:54,719 --> 01:25:57,920
needs to be a new bpf hook that's not

01:25:56,719 --> 01:25:59,840
called http and then

01:25:57,920 --> 01:26:01,360
there's some other limitations with that

01:25:59,840 --> 01:26:02,320
and there's there are definitely some

01:26:01,360 --> 01:26:04,560
things we can

01:26:02,320 --> 01:26:06,320
can do and maybe we can do it in the mac

01:26:04,560 --> 01:26:07,199
layer instead of doing for every driver

01:26:06,320 --> 01:26:10,639
so

01:26:07,199 --> 01:26:12,400
as there's people interested in this

01:26:10,639 --> 01:26:13,760
and i think lorenzo know more about this

01:26:12,400 --> 01:26:16,880
to me i

01:26:13,760 --> 01:26:18,560
think i mean sometimes ago like

01:26:16,880 --> 01:26:20,639
a couple of years ago or something like

01:26:18,560 --> 01:26:23,040
this i tried to

01:26:20,639 --> 01:26:24,000
propose something related to xdp or over

01:26:23,040 --> 01:26:26,480
wi-fi

01:26:24,000 --> 01:26:28,800
and the main they mean the difficulties

01:26:26,480 --> 01:26:30,560
are let's say as i said before on the

01:26:28,800 --> 01:26:31,840
rake side since there are a lot of

01:26:30,560 --> 01:26:34,639
complexity and

01:26:31,840 --> 01:26:35,600
at mac layer for the protocol but at

01:26:34,639 --> 01:26:38,800
least on the

01:26:35,600 --> 01:26:42,239
just considering the thick side i guess

01:26:38,800 --> 01:26:45,440
this is doable because so far

01:26:42,239 --> 01:26:48,639
there are uh some devices at least

01:26:45,440 --> 01:26:51,760
i can think about a couple of them

01:26:48,639 --> 01:26:54,880
that just i'm just uh

01:26:51,760 --> 01:26:56,400
speaking about excite that support the

01:26:54,880 --> 01:26:59,360
capability to offload

01:26:56,400 --> 01:27:00,080
an ethernet frame instead of a wireless

01:26:59,360 --> 01:27:02,400
one

01:27:00,080 --> 01:27:04,239
so all the let's say the complexity now

01:27:02,400 --> 01:27:08,000
is offloaded

01:27:04,239 --> 01:27:09,600
into the card so from let's say that not

01:27:08,000 --> 01:27:11,760
working point of view

01:27:09,600 --> 01:27:13,920
on the tick side there is no much

01:27:11,760 --> 01:27:16,639
difference between an ethernet card

01:27:13,920 --> 01:27:17,120
and the wire and this wireless card so i

01:27:16,639 --> 01:27:20,159
guess

01:27:17,120 --> 01:27:23,280
it is doable to

01:27:20,159 --> 01:27:25,760
support i'll interrupt you here because

01:27:23,280 --> 01:27:29,280
again getting wi-fi chipset vendors to

01:27:25,760 --> 01:27:29,280
implement checksumming support is

01:27:30,840 --> 01:27:33,840
impossible

01:27:38,480 --> 01:27:45,360
honestly this is in my to-do list

01:27:41,920 --> 01:27:48,159
do i say to understand how let's say

01:27:45,360 --> 01:27:48,639
protect you by something on this so this

01:27:48,159 --> 01:27:51,040
is

01:27:48,639 --> 01:27:53,040
i can just explain my current idea so

01:27:51,040 --> 01:27:56,560
far i have no

01:27:53,040 --> 01:27:59,600
the time to work on this

01:27:56,560 --> 01:28:02,800
yet but i am

01:27:59,600 --> 01:28:05,840
sure that some some

01:28:02,800 --> 01:28:06,480
some device can support it i'm i'm i'm

01:28:05,840 --> 01:28:08,560
confident

01:28:06,480 --> 01:28:09,840
no no i i'm not i'm not disputing that

01:28:08,560 --> 01:28:11,520
there's hardware out there that can

01:28:09,840 --> 01:28:12,800
support it i'm just pointing out that

01:28:11,520 --> 01:28:15,840
there's plenty

01:28:12,800 --> 01:28:18,239
plenty of hearts reported

01:28:15,840 --> 01:28:19,199
um and i am saying i'm just saying that

01:28:18,239 --> 01:28:22,719
it's doable

01:28:19,199 --> 01:28:24,400
for these new chips not for all of them

01:28:22,719 --> 01:28:26,239
right i mean there's there's chipsets

01:28:24,400 --> 01:28:27,199
coming out this year that don't support

01:28:26,239 --> 01:28:31,840
checksum

01:28:27,199 --> 01:28:33,920
um so yeah that's kind of like i

01:28:31,840 --> 01:28:35,679
i don't really have like you can't

01:28:33,920 --> 01:28:37,840
really go lower barrel

01:28:35,679 --> 01:28:39,280
than you know no offloads that's kind of

01:28:37,840 --> 01:28:40,239
what the hardware we have to deal with

01:28:39,280 --> 01:28:43,679
is

01:28:40,239 --> 01:28:44,000
um so so so the reason i'm asking this

01:28:43,679 --> 01:28:46,320
is

01:28:44,000 --> 01:28:48,000
is because one of the things i see is

01:28:46,320 --> 01:28:49,679
for example when we receive from a

01:28:48,000 --> 01:28:52,159
seller interface

01:28:49,679 --> 01:28:53,840
um we receive an aggregated packet but

01:28:52,159 --> 01:28:55,600
of course it's not jso aggregation it's

01:28:53,840 --> 01:28:56,400
just a bunch of packets that are back to

01:28:55,600 --> 01:28:59,679
back

01:28:56,400 --> 01:29:01,920
we have to split this into multiple skbs

01:28:59,679 --> 01:29:03,040
so we basically allocate all the skbs we

01:29:01,920 --> 01:29:04,400
copy the data

01:29:03,040 --> 01:29:06,639
and then we decide to forward the

01:29:04,400 --> 01:29:08,239
packets to a usb interface where the usb

01:29:06,639 --> 01:29:08,960
interface wants to aggregate all the

01:29:08,239 --> 01:29:10,960
packets

01:29:08,960 --> 01:29:12,639
so now it basically allocates a bigger

01:29:10,960 --> 01:29:13,760
packet and copies all the data so we're

01:29:12,639 --> 01:29:17,120
basically doing two

01:29:13,760 --> 01:29:20,400
to two copies right yes um that's to get

01:29:17,120 --> 01:29:23,440
packets from cellular to to to usb and

01:29:20,400 --> 01:29:27,360
if we could do something you know

01:29:23,440 --> 01:29:27,360
to avoid one of those copies at least

01:29:29,280 --> 01:29:32,560
first let's say if i unfold correctly

01:29:31,840 --> 01:29:34,880
what you mean

01:29:32,560 --> 01:29:37,440
is that this is on the rig side not on

01:29:34,880 --> 01:29:39,280
the tx one right

01:29:37,440 --> 01:29:41,280
well there's there's de-aggregation on

01:29:39,280 --> 01:29:42,639
the rx side and there's re-aggregation

01:29:41,280 --> 01:29:46,320
on the transmit side

01:29:42,639 --> 01:29:49,360
it's just that you know yeah sorry

01:29:46,320 --> 01:29:51,120
go ahead well the the driver it's coming

01:29:49,360 --> 01:29:52,880
in on and the driver is leaving on are

01:29:51,120 --> 01:29:53,840
basically guaranteed to be different

01:29:52,880 --> 01:29:55,679
right

01:29:53,840 --> 01:29:58,080
so the aggregation requirements are

01:29:55,679 --> 01:30:00,159
basically guaranteed to be different

01:29:58,080 --> 01:30:01,920
plus along the way some packets might

01:30:00,159 --> 01:30:04,840
have been not forwarded so

01:30:01,920 --> 01:30:07,840
might have gone you know locally to the

01:30:04,840 --> 01:30:07,840
device

01:30:15,920 --> 01:30:19,280
because essentially you need a fast path

01:30:17,840 --> 01:30:21,360
between

01:30:19,280 --> 01:30:23,040
the receiving end and the device you're

01:30:21,360 --> 01:30:23,760
going to send the packets out from now

01:30:23,040 --> 01:30:26,320
the

01:30:23,760 --> 01:30:27,600
the fact that you know you have to sorry

01:30:26,320 --> 01:30:31,440
aggregate

01:30:27,600 --> 01:30:34,480
off list because this is we are now in

01:30:31,440 --> 01:30:37,840
this is another sessions time

01:30:34,480 --> 01:30:40,000
yeah sorry i i think we can continue to

01:30:37,840 --> 01:30:46,159
discuss offline about this

01:30:40,000 --> 01:30:46,159
yeah okay so thank you everybody for

01:30:46,840 --> 01:30:50,960
participating

01:30:48,400 --> 01:30:52,159
thank you very much thank you very much

01:30:50,960 --> 01:30:55,920
guys

01:30:52,159 --> 01:30:55,920

YouTube URL: https://www.youtube.com/watch?v=ayFWnFj5fY8


