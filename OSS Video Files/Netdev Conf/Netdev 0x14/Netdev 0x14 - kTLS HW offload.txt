Title: Netdev 0x14 - kTLS HW offload
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Tariq Toukan, Bar Tuaf, Tal Gilboa

More info: https://netdevconf.info/0x14/session.html?talk-kTLS-HW-offload-implementation-and-performance-gains

Date: Friday, August 14, 2020

In this TLS talk, Bar Tuaf discusses offloading of TLS to the NIC.
He starts by reviewing the life cycle of a HW
offloaded kTLS connection and the driver-HW interaction
in order to support it.

Bar then will present their experiments where NginX TLS
activity is offloaded to the Mellanox ConnectX-6Dx NIC
(using mlx5e driver). And finally their experimental results
which show significant performance speed-up gained by offloading
kTLS operations to the HW.
Captions: 
	00:00:03,360 --> 00:00:07,759
so we are going to talk about

00:00:05,279 --> 00:00:09,440
ktls offload and the benefit in

00:00:07,759 --> 00:00:12,960
offloading crypto test

00:00:09,440 --> 00:00:16,080
to the knee this presentation is based

00:00:12,960 --> 00:00:19,439
on a mechanism presented at netapp 1.2

00:00:16,080 --> 00:00:20,480
with tx offload another 2.2 with rx

00:00:19,439 --> 00:00:24,480
offload by

00:00:20,480 --> 00:00:25,359
by both this man the slides i'm going to

00:00:24,480 --> 00:00:29,279
show today

00:00:25,359 --> 00:00:33,599
it was written and myself

00:00:29,279 --> 00:00:33,599
we are working with nvidia in the driver

00:00:33,600 --> 00:00:38,280
and will show the advantage in

00:00:35,040 --> 00:00:39,760
offloading crypto assignment to nvidia

00:00:38,280 --> 00:00:43,600
melanox

00:00:39,760 --> 00:00:45,200
network interconnect we will overview

00:00:43,600 --> 00:00:48,960
the responsibilities of

00:00:45,200 --> 00:00:51,360
each component pack driver and hardware

00:00:48,960 --> 00:00:53,520
and maintenance data flows and see some

00:00:51,360 --> 00:00:56,879
performance numbers for both

00:00:53,520 --> 00:00:59,600
small and large scale so let's start

00:00:56,879 --> 00:01:03,760
with the motivation to use ktls offload

00:00:59,600 --> 00:01:03,760
and introduction to common tls

00:01:03,840 --> 00:01:10,080
uh the motivation for this work mainly

00:01:06,880 --> 00:01:13,040
came from two perceptions

00:01:10,080 --> 00:01:13,840
the first one is that all internet

00:01:13,040 --> 00:01:17,280
services

00:01:13,840 --> 00:01:18,799
are becoming more secure most of the

00:01:17,280 --> 00:01:22,040
websites today uses

00:01:18,799 --> 00:01:24,840
https connections that mainly rely on

00:01:22,040 --> 00:01:26,880
tls

00:01:24,840 --> 00:01:30,000
and

00:01:26,880 --> 00:01:32,560
second perception is that offloading

00:01:30,000 --> 00:01:33,439
tasks from the host and especially from

00:01:32,560 --> 00:01:35,680
the cpu

00:01:33,439 --> 00:01:37,119
is important in order to keep up the

00:01:35,680 --> 00:01:40,880
increasing phase

00:01:37,119 --> 00:01:44,000
in the in internet consumption

00:01:40,880 --> 00:01:46,720
100g link link speed

00:01:44,000 --> 00:01:48,799
need to use these offloads in order to

00:01:46,720 --> 00:01:53,520
release smart cpu

00:01:48,799 --> 00:01:54,159
data the solution is based on a pretty

00:01:53,520 --> 00:01:56,880
much

00:01:54,159 --> 00:01:58,079
unchanged linux networking stack with

00:01:56,880 --> 00:02:01,759
the addition of tls

00:01:58,079 --> 00:02:03,360
non-crypto flow so we get all the goods

00:02:01,759 --> 00:02:06,000
of a robust and resilient

00:02:03,360 --> 00:02:06,000
networking

00:02:07,520 --> 00:02:12,239
uh tls stands for transport level

00:02:10,000 --> 00:02:14,560
security

00:02:12,239 --> 00:02:17,280
this protocol have several version and

00:02:14,560 --> 00:02:20,319
in this presentation we will use

00:02:17,280 --> 00:02:23,440
tls version 1.2

00:02:20,319 --> 00:02:25,120
tls is a layer 4 protocol based on top

00:02:23,440 --> 00:02:27,920
of tcp protocol

00:02:25,120 --> 00:02:29,200
it was first implemented for user space

00:02:27,920 --> 00:02:31,920
application

00:02:29,200 --> 00:02:32,640
and was offloaded to kernel by ktrs

00:02:31,920 --> 00:02:34,959
module

00:02:32,640 --> 00:02:37,680
and now to the nic to encrypt the clip

00:02:34,959 --> 00:02:40,319
packets on the fly

00:02:37,680 --> 00:02:41,360
we will work through ktls device data

00:02:40,319 --> 00:02:44,160
flows for

00:02:41,360 --> 00:02:46,000
both transmit and receive side and

00:02:44,160 --> 00:02:48,239
present performance results

00:02:46,000 --> 00:02:49,280
in a patch synthetic environment and

00:02:48,239 --> 00:02:53,760
engineering

00:02:49,280 --> 00:02:57,440
nginx real life server application

00:02:53,760 --> 00:02:57,840
ktls offload time first implementation

00:02:57,440 --> 00:02:59,840
which

00:02:57,840 --> 00:03:01,599
offloads crypto processing from

00:02:59,840 --> 00:03:05,280
application layer to the kernel

00:03:01,599 --> 00:03:08,400
was introduced in kernel version 413

00:03:05,280 --> 00:03:10,080
as a software offload nvidia melanox

00:03:08,400 --> 00:03:13,599
connected 6dx

00:03:10,080 --> 00:03:16,080
ktls px offload the port was introduced

00:03:13,599 --> 00:03:19,840
by mlx5e driver

00:03:16,080 --> 00:03:22,319
in kernel version 5.6

00:03:19,840 --> 00:03:25,519
and the rx stability was added in kernel

00:03:22,319 --> 00:03:29,360
version 5.9

00:03:25,519 --> 00:03:32,560
in between we had an fpga programmable

00:03:29,360 --> 00:03:37,599
implemented also by emily driver

00:03:32,560 --> 00:03:40,239
with supported offloading camera for it

00:03:37,599 --> 00:03:40,959
as this presentation is a part of pls

00:03:40,239 --> 00:03:42,799
workshop

00:03:40,959 --> 00:03:45,200
i believe that most of you are well

00:03:42,799 --> 00:03:46,959
known with the tls protocol pros and

00:03:45,200 --> 00:03:49,120
cons

00:03:46,959 --> 00:03:50,720
ideally we would process packet

00:03:49,120 --> 00:03:52,480
independence

00:03:50,720 --> 00:03:53,920
it works like that in most common

00:03:52,480 --> 00:03:58,239
security protocols

00:03:53,920 --> 00:03:58,239
for example like stack and dtls

00:03:58,400 --> 00:04:02,000
unlike those protocols tls process

00:04:01,040 --> 00:04:04,239
records

00:04:02,000 --> 00:04:07,040
and those records can be spread on a

00:04:04,239 --> 00:04:09,599
multiple tcp package

00:04:07,040 --> 00:04:11,680
at the right chart we can see an example

00:04:09,599 --> 00:04:14,400
for three tls records

00:04:11,680 --> 00:04:15,920
leads to four tcp max segment size

00:04:14,400 --> 00:04:17,840
packet

00:04:15,920 --> 00:04:19,840
we can see that all records share the

00:04:17,840 --> 00:04:22,639
data with another record

00:04:19,840 --> 00:04:22,639
the same tcp

00:04:23,360 --> 00:04:30,000
for example a packet number 2

00:04:27,759 --> 00:04:31,759
packet number two is shared between tls

00:04:30,000 --> 00:04:35,360
record number one and tsp

00:04:31,759 --> 00:04:38,720
number two we used

00:04:35,360 --> 00:04:39,120
aes counter mode which generates cipher

00:04:38,720 --> 00:04:41,120
text

00:04:39,120 --> 00:04:44,160
by extort operation between the key

00:04:41,120 --> 00:04:47,639
stream and the data for encryption

00:04:44,160 --> 00:04:51,199
key stream can be generated using an

00:04:47,639 --> 00:04:53,199
initialization vector and key and key

00:04:51,199 --> 00:04:54,400
which can be found in the tls crypto

00:04:53,199 --> 00:04:57,199
content

00:04:54,400 --> 00:04:58,639
driver is responsible for initializing

00:04:57,199 --> 00:05:02,000
hardware with the

00:04:58,639 --> 00:05:04,080
propel context provided by ktls stack on

00:05:02,000 --> 00:05:06,800
creation

00:05:04,080 --> 00:05:07,600
in addition hardware must track the tls

00:05:06,800 --> 00:05:09,520
contacts

00:05:07,600 --> 00:05:12,080
in order to successfully encrypt the

00:05:09,520 --> 00:05:14,240
crypt packet

00:05:12,080 --> 00:05:15,120
some flows can cause hardware to lose

00:05:14,240 --> 00:05:17,280
its state

00:05:15,120 --> 00:05:19,919
in this case hardware will indicate it

00:05:17,280 --> 00:05:22,880
to the driver

00:05:19,919 --> 00:05:25,199
that will coordinate with the tls module

00:05:22,880 --> 00:05:28,080
to restore it

00:05:25,199 --> 00:05:29,360
this scheme shows the rules of each

00:05:28,080 --> 00:05:32,560
layer

00:05:29,360 --> 00:05:35,120
user space data is kept in memory

00:05:32,560 --> 00:05:36,240
pls protocol is responsible for that of

00:05:35,120 --> 00:05:40,000
fragmentation

00:05:36,240 --> 00:05:41,280
in case decrypted as well as adding

00:05:40,000 --> 00:05:44,960
adders and trailers

00:05:41,280 --> 00:05:48,560
and to divide into echo

00:05:44,960 --> 00:05:50,840
then it will hand it to the pc

00:05:48,560 --> 00:05:53,120
that splits the record into mac segment

00:05:50,840 --> 00:05:56,080
size

00:05:53,120 --> 00:05:57,120
we will start with the transmit flow i

00:05:56,080 --> 00:05:58,800
will go through

00:05:57,120 --> 00:06:00,160
the condition checked in case of the

00:05:58,800 --> 00:06:04,080
good

00:06:00,160 --> 00:06:04,960
transparent first the driver will check

00:06:04,080 --> 00:06:06,800
each packet

00:06:04,960 --> 00:06:08,000
whether it belongs to an offloaded

00:06:06,800 --> 00:06:10,639
socket

00:06:08,000 --> 00:06:13,039
let's say it's true now it will check

00:06:10,639 --> 00:06:16,960
the packet tcp sequence number

00:06:13,039 --> 00:06:19,520
and compare it to the expected number

00:06:16,960 --> 00:06:21,759
if it's also true it will prepare a

00:06:19,520 --> 00:06:24,000
property

00:06:21,759 --> 00:06:25,039
and follow the packet for authentication

00:06:24,000 --> 00:06:29,759
and encryption

00:06:25,039 --> 00:06:29,759
on the fly by the neck and to the wire

00:06:33,759 --> 00:06:36,880
let's say once again that the packet is

00:06:36,639 --> 00:06:40,080
an

00:06:36,880 --> 00:06:42,639
offload packet but the driver

00:06:40,080 --> 00:06:45,759
discovered that pcp sequence number is

00:06:42,639 --> 00:06:47,680
not the expected piece

00:06:45,759 --> 00:06:48,880
now driver will trigger legally sink

00:06:47,680 --> 00:06:51,599
flow

00:06:48,880 --> 00:06:55,360
first the driver will understand to

00:06:51,599 --> 00:06:57,840
which record this packet belongs to

00:06:55,360 --> 00:06:59,599
followed by updating hardware with a

00:06:57,840 --> 00:07:02,160
suitable cryptocurrent

00:06:59,599 --> 00:07:03,520
using fast passed communication with

00:07:02,160 --> 00:07:05,280
fencing to guarantee

00:07:03,520 --> 00:07:08,000
that we supply hardware with the

00:07:05,280 --> 00:07:08,000
properties

00:07:10,800 --> 00:07:15,039
notice that ktls module is not involved

00:07:14,080 --> 00:07:18,240
involved

00:07:15,039 --> 00:07:22,400
in this procedure now the packet can be

00:07:18,240 --> 00:07:22,400
passed for encryption once again by then

00:07:23,039 --> 00:07:27,759
we will move to the receive side path

00:07:26,000 --> 00:07:30,080
when hardware is in offload state

00:07:27,759 --> 00:07:32,000
all packets belong to a connection

00:07:30,080 --> 00:07:35,120
passing through the crypto engines

00:07:32,000 --> 00:07:37,199
are decrypted and authenticated

00:07:35,120 --> 00:07:39,440
hardware will indicate it to the driver

00:07:37,199 --> 00:07:41,840
using alex descriptors

00:07:39,440 --> 00:07:42,880
and the driver will mark it to inform

00:07:41,840 --> 00:07:46,720
software tls

00:07:42,880 --> 00:07:49,280
using skb decrypted field

00:07:46,720 --> 00:07:51,120
now for the receive site traffic tls

00:07:49,280 --> 00:07:52,960
module will check each record

00:07:51,120 --> 00:07:55,039
independently

00:07:52,960 --> 00:07:57,520
it will check if all packets inside the

00:07:55,039 --> 00:08:00,560
tls record will be created

00:07:57,520 --> 00:08:04,080
in this case it's true so we can copy

00:08:00,560 --> 00:08:04,080
the decrypt the data to memo

00:08:04,319 --> 00:08:10,639
in case of out of orders or drops we

00:08:06,879 --> 00:08:13,039
will defer the data path to 29

00:08:10,639 --> 00:08:15,120
first one is partially decrypted tls

00:08:13,039 --> 00:08:17,759
spec

00:08:15,120 --> 00:08:18,720
not all packet inside the record is

00:08:17,759 --> 00:08:21,120
decrypted

00:08:18,720 --> 00:08:22,000
but some of them actually does in the

00:08:21,120 --> 00:08:24,240
stream below

00:08:22,000 --> 00:08:25,599
we can see that packet number play

00:08:24,240 --> 00:08:28,960
cipher text

00:08:25,599 --> 00:08:31,919
while packet 2 and 4 are planned

00:08:28,960 --> 00:08:32,880
means the tls record number 2 is mixed

00:08:31,919 --> 00:08:36,479
with what's

00:08:32,880 --> 00:08:39,519
with both cipher and plain

00:08:36,479 --> 00:08:42,320
packet 3 contains payload and tcp

00:08:39,519 --> 00:08:44,880
address without any tls record header

00:08:42,320 --> 00:08:45,600
so in this case software will remove the

00:08:44,880 --> 00:08:48,160
job

00:08:45,600 --> 00:08:50,720
you can get packet number three after

00:08:48,160 --> 00:08:53,200
that we can copy all that

00:08:50,720 --> 00:08:56,160
we should know that it will not impact

00:08:53,200 --> 00:08:56,160
the following package

00:08:56,720 --> 00:09:03,600
packets four five and seven four five

00:09:00,399 --> 00:09:04,640
six and seven the other case will

00:09:03,600 --> 00:09:07,760
require hardware

00:09:04,640 --> 00:09:09,200
synchronization hardware must track

00:09:07,760 --> 00:09:12,959
crypto contacts

00:09:09,200 --> 00:09:15,519
for each connection and out of order or

00:09:12,959 --> 00:09:16,320
drops can cause outlet after out of

00:09:15,519 --> 00:09:18,399
thing

00:09:16,320 --> 00:09:22,240
and when it does alduin will stop

00:09:18,399 --> 00:09:25,040
decrypting packets for this

00:09:22,240 --> 00:09:25,680
in this case it will send the listing

00:09:25,040 --> 00:09:29,200
request

00:09:25,680 --> 00:09:32,720
using alex descriptors to the driver

00:09:29,200 --> 00:09:36,080
that notifies tls stack using the new as

00:09:32,720 --> 00:09:39,200
increasing api

00:09:36,080 --> 00:09:41,760
following it the driver query the device

00:09:39,200 --> 00:09:43,760
for a guest tcp sequence number and

00:09:41,760 --> 00:09:47,279
provide it to the stack

00:09:43,760 --> 00:09:50,320
ktlstack will check if it matches

00:09:47,279 --> 00:09:51,519
any of his direct incoming packet and

00:09:50,320 --> 00:09:55,200
call the driver

00:09:51,519 --> 00:09:56,320
which using hardware meanwhile software

00:09:55,200 --> 00:09:59,279
will finish the job

00:09:56,320 --> 00:09:59,279
and decrypt all

00:10:00,800 --> 00:10:04,560
after a successful listing the device

00:10:02,880 --> 00:10:07,680
will be turned to a float state and

00:10:04,560 --> 00:10:07,680
decrypt the receiver

00:10:08,399 --> 00:10:13,440
the child the broken line symbolize that

00:10:11,360 --> 00:10:16,800
missing flow is approved by software

00:10:13,440 --> 00:10:16,800
and outdoor can decrypt it

00:10:17,120 --> 00:10:21,440
so to show the performance impact of

00:10:19,440 --> 00:10:25,120
ktls auto offload with

00:10:21,440 --> 00:10:28,640
nginx server we use two amd epic system

00:10:25,120 --> 00:10:33,440
connected to switch with connected 6dx

00:10:28,640 --> 00:10:37,680
nic to the wrk client

00:10:33,440 --> 00:10:41,440
wok opens different amount of connection

00:10:37,680 --> 00:10:45,440
connections range between 1024

00:10:41,440 --> 00:10:48,079
and 32k connection with 64 threads

00:10:45,440 --> 00:10:51,120
and continuously requesting one megabyte

00:10:48,079 --> 00:10:51,120
file from the cell

00:10:51,200 --> 00:10:58,399
nginx responds with either

00:10:54,320 --> 00:11:01,600
plain text http response

00:10:58,399 --> 00:11:06,880
http with software ktls which uses

00:11:01,600 --> 00:11:12,959
openssl tls version 1.2 implementation

00:11:06,880 --> 00:11:12,959
and https with ktls hardware ltx upload

00:11:13,200 --> 00:11:16,480
in in this case we modify the engine

00:11:15,920 --> 00:11:19,279
into

00:11:16,480 --> 00:11:22,800
send file chain also for tls traffic

00:11:19,279 --> 00:11:22,800
then the default is only for which

00:11:23,440 --> 00:11:28,880
all implementations reach libraries from

00:11:26,839 --> 00:11:32,160
00:11:28,880 --> 00:11:35,600
and up to 30k connections

00:11:32,160 --> 00:11:36,480
and the graph shows around 85 gigabits

00:11:35,600 --> 00:11:40,320
per second

00:11:36,480 --> 00:11:43,920
at the w is the wrk reporting

00:11:40,320 --> 00:11:47,040
in application layer which is not

00:11:43,920 --> 00:11:49,440
taking into consideration layer 3 and

00:11:47,040 --> 00:11:49,440
forehead

00:11:49,519 --> 00:11:55,279
basically it's a pure http object data

00:11:52,880 --> 00:11:56,959
means the bandwidth in which we can

00:11:55,279 --> 00:12:00,800
transfer http

00:11:56,959 --> 00:12:03,120
object this slide shows

00:12:00,800 --> 00:12:05,600
the cpu improvement followed by

00:12:03,120 --> 00:12:08,800
offloading cryptocurrency

00:12:05,600 --> 00:12:12,320
price we calculated

00:12:08,800 --> 00:12:15,040
active cost by summing the amount of cpu

00:12:12,320 --> 00:12:16,399
used by the server in order to transmit

00:12:15,040 --> 00:12:19,920
100 gigabit

00:12:16,399 --> 00:12:23,760
line we calculated

00:12:19,920 --> 00:12:26,880
improvement with the delta between https

00:12:23,760 --> 00:12:27,760
using software and audio offload divided

00:12:26,880 --> 00:12:31,839
by the amount

00:12:27,760 --> 00:12:35,360
used by in

00:12:31,839 --> 00:12:38,639
for example in this server we used

00:12:35,360 --> 00:12:42,160
60 focus and let's assume

00:12:38,639 --> 00:12:45,519
that all costs are working in 50

00:12:42,160 --> 00:12:52,240
utilization 64 course multiple in

00:12:45,519 --> 00:12:54,720
0.5 equals to 32 active costs

00:12:52,240 --> 00:12:55,680
we saw a pretty much significant

00:12:54,720 --> 00:12:59,920
improvement

00:12:55,680 --> 00:13:02,560
of up to 50 reduction in cpus

00:12:59,920 --> 00:13:03,680
if we look at the table at 32k

00:13:02,560 --> 00:13:06,720
connection

00:13:03,680 --> 00:13:07,200
http response activate a bit more than

00:13:06,720 --> 00:13:10,560
00:13:07,200 --> 00:13:14,079
cores in order to maintain line

00:13:10,560 --> 00:13:17,440
software ktls uses nine active calls

00:13:14,079 --> 00:13:21,440
which is around seven more than http

00:13:17,440 --> 00:13:22,160
when ktlsr dual tx upload uses 14 active

00:13:21,440 --> 00:13:26,000
calls

00:13:22,160 --> 00:13:26,000
only two more than http

00:13:27,839 --> 00:13:34,880
and we should notice that

00:13:30,880 --> 00:13:38,000
for 1024 to 8k cases

00:13:34,880 --> 00:13:38,639
so you we can see that http using ktlstx

00:13:38,000 --> 00:13:41,680
offload

00:13:38,639 --> 00:13:44,000
is using less active cores than http

00:13:41,680 --> 00:13:44,880
this is caused by a higher packet late

00:13:44,000 --> 00:13:47,440
when transmit

00:13:44,880 --> 00:13:50,320
transmitting plain text that leads to a

00:13:47,440 --> 00:13:54,000
higher cpu utilization

00:13:50,320 --> 00:13:55,600
i will clarify on the wire we have less

00:13:54,000 --> 00:14:03,839
headers for the same mtu

00:13:55,600 --> 00:14:03,839
and http connection can post small data

00:14:04,880 --> 00:14:11,360
this slide showed throughput speedup

00:14:07,279 --> 00:14:11,360
when using full unit directional off

00:14:11,600 --> 00:14:17,600
we used two intel servers connected back

00:14:14,399 --> 00:14:20,560
to back with connectex 6x next

00:14:17,600 --> 00:14:22,959
we patched icons to support tls launcher

00:14:20,560 --> 00:14:25,360
using openst libraries

00:14:22,959 --> 00:14:26,720
and compare between software ktls

00:14:25,360 --> 00:14:30,000
[Music]

00:14:26,720 --> 00:14:33,120
and receive methods ktls device

00:14:30,000 --> 00:14:36,320
offload and tcp traffic using stack

00:14:33,120 --> 00:14:36,320
send and receive network

00:14:36,480 --> 00:14:41,760
full loading directional offloads mean

00:14:38,800 --> 00:14:45,040
we use dx offloading server side

00:14:41,760 --> 00:14:50,480
and rx offload on the client

00:14:45,040 --> 00:14:50,480
we measure up to 2.5 x speed up

00:14:51,279 --> 00:14:59,279
like we can see in the single stream bus

00:14:56,000 --> 00:15:01,519
and we can see 18 gigabit good put

00:14:59,279 --> 00:15:04,240
comparing to the seven gigabit achieved

00:15:01,519 --> 00:15:04,240
by software

00:15:04,480 --> 00:15:10,720
ktls offload reached line rate with

00:15:07,600 --> 00:15:15,040
less connection with less connections

00:15:10,720 --> 00:15:15,040
as we can see in the eight string case

00:15:15,199 --> 00:15:18,880
and once again the graph reports

00:15:17,360 --> 00:15:21,680
application layer throughput

00:15:18,880 --> 00:15:23,399
of output reported by ipel something

00:15:21,680 --> 00:15:26,880
like

00:15:23,399 --> 00:15:30,320
94.5 gb

00:15:26,880 --> 00:15:30,720
in the following two slides we want to

00:15:30,320 --> 00:15:34,000
show

00:15:30,720 --> 00:15:35,120
how ktls device offload can save a lot

00:15:34,000 --> 00:15:38,560
of cpu power

00:15:35,120 --> 00:15:41,440
for both sender and receiver

00:15:38,560 --> 00:15:42,000
we took cases that achieve 100g line

00:15:41,440 --> 00:15:44,399
rate

00:15:42,000 --> 00:15:45,759
and compare the amount of active calls

00:15:44,399 --> 00:15:49,120
for each case

00:15:45,759 --> 00:15:53,519
again active cost is the amount of cpu

00:15:49,120 --> 00:15:53,519
used in order to get 100g

00:15:53,680 --> 00:15:56,720
so if we look at the center graph we can

00:15:56,160 --> 00:16:00,079
see that

00:15:56,720 --> 00:16:04,000
in order to act to achieve line rate

00:16:00,079 --> 00:16:06,639
with 64 128 and 512

00:16:04,000 --> 00:16:08,959
connections we use the same amount of

00:16:06,639 --> 00:16:11,519
active calls for ktls device and for

00:16:08,959 --> 00:16:14,800
plaintext tcp traffic

00:16:11,519 --> 00:16:18,639
means we recover all of the cpu overhead

00:16:14,800 --> 00:16:18,639
caused when using tls protocol

00:16:20,639 --> 00:16:27,040
a when i say we recover

00:16:23,839 --> 00:16:29,519
all i should use quotation marks and the

00:16:27,040 --> 00:16:31,279
small difference in cpu utilization when

00:16:29,519 --> 00:16:34,480
using tcp traffic

00:16:31,279 --> 00:16:37,199
caused by packet rate differences tcp

00:16:34,480 --> 00:16:38,320
packet rate was higher in comparison to

00:16:37,199 --> 00:16:41,040
the 2k dls

00:16:38,320 --> 00:16:44,079
implementation presented like in the

00:16:41,040 --> 00:16:44,079
case we saw earlier

00:16:48,240 --> 00:16:52,720
now for the received side so if we look

00:16:51,440 --> 00:16:55,279
at the left column

00:16:52,720 --> 00:16:56,000
we can see that in order to get 100g

00:16:55,279 --> 00:16:59,440
line rate

00:16:56,000 --> 00:17:02,560
over tcp traffic with 64 connections

00:16:59,440 --> 00:17:03,440
on the received side we use eight active

00:17:02,560 --> 00:17:09,039
cores

00:17:03,440 --> 00:17:13,039
from the 24 available in the system

00:17:09,039 --> 00:17:13,760
a bit more than 10 from ktls device

00:17:13,039 --> 00:17:18,160
offload

00:17:13,760 --> 00:17:20,959
and almost 18 with software ktls

00:17:18,160 --> 00:17:20,959
implementation

00:17:21,199 --> 00:17:27,280
if we look at the right column in the

00:17:24,240 --> 00:17:28,440
graph we can see that using software

00:17:27,280 --> 00:17:32,160
ktls over

00:17:28,440 --> 00:17:37,440
512 connections uses 19 calls

00:17:32,160 --> 00:17:37,440
which is nine more active cost than tcp

00:17:37,679 --> 00:17:44,840
tls offload uses 13 calls

00:17:41,120 --> 00:17:47,280
limo three more than tcp for this

00:17:44,840 --> 00:17:47,280
calculation and

00:17:47,520 --> 00:17:52,480
let's use tcp as a lower bound

00:17:50,720 --> 00:17:53,840
we can look at the delta between

00:17:52,480 --> 00:17:56,880
software and tcp

00:17:53,840 --> 00:18:00,080
as the tls overhead so we recover

00:17:56,880 --> 00:18:04,000
66 66 of the cpu

00:18:00,080 --> 00:18:07,280
overlay mainly crypto overhead caused by

00:18:04,000 --> 00:18:07,280
software implementation

00:18:08,640 --> 00:18:14,720
uh in the receive site we do not see

00:18:11,760 --> 00:18:16,559
a better cpu utilization for ktls device

00:18:14,720 --> 00:18:18,880
like we saw in the tx style

00:18:16,559 --> 00:18:20,240
in this case the overhead is too high to

00:18:18,880 --> 00:18:22,880
recover

00:18:20,240 --> 00:18:25,120
but we saw a bit higher packet late also

00:18:22,880 --> 00:18:25,120
here

00:18:25,600 --> 00:18:32,240
so we will walk through the benefits

00:18:28,640 --> 00:18:35,200
of using a tls device api

00:18:32,240 --> 00:18:37,600
so adding crypto state to the nic and

00:18:35,200 --> 00:18:38,559
the ability to include declared packets

00:18:37,600 --> 00:18:41,760
on the fly

00:18:38,559 --> 00:18:46,080
leaves us with single pci router

00:18:41,760 --> 00:18:49,200
which helped to save pci e a bandwidth

00:18:46,080 --> 00:18:51,840
and added no additional latency in

00:18:49,200 --> 00:18:51,840
comparison

00:18:54,480 --> 00:19:00,080
data designated for encryption is not

00:18:57,200 --> 00:19:00,080
written to one

00:19:00,559 --> 00:19:08,480
the tls tags add non-crypto

00:19:04,400 --> 00:19:12,400
flow to deal with ktls device offload

00:19:08,480 --> 00:19:15,280
other than that it remains unchanged

00:19:12,400 --> 00:19:15,840
and so we profit the resilience tcp

00:19:15,280 --> 00:19:18,080
stack

00:19:15,840 --> 00:19:19,039
with memory management congestion

00:19:18,080 --> 00:19:21,120
control

00:19:19,039 --> 00:19:23,679
and other important components that

00:19:21,120 --> 00:19:26,080
comes with it

00:19:23,679 --> 00:19:26,880
and the last point is that the driver

00:19:26,080 --> 00:19:29,840
called

00:19:26,880 --> 00:19:32,320
the tls tax whenever it will need to be

00:19:29,840 --> 00:19:32,320
racing

00:19:33,039 --> 00:19:38,080
so to summarize it we saw a significant

00:19:36,799 --> 00:19:40,400
performance to put

00:19:38,080 --> 00:19:41,679
speed up gain by offloading crypto

00:19:40,400 --> 00:19:44,720
processing

00:19:41,679 --> 00:19:47,679
into the network device

00:19:44,720 --> 00:19:50,840
ktls device offload recovers a massive

00:19:47,679 --> 00:19:53,440
amount of cpu overhead caused by tls

00:19:50,840 --> 00:19:57,039
protocol

00:19:53,440 --> 00:19:58,000
it can help prevent hackers from getting

00:19:57,039 --> 00:20:01,280
the data before

00:19:58,000 --> 00:20:07,600
it it is encrypted since no

00:20:01,280 --> 00:20:11,760
crypto data is written to run

00:20:07,600 --> 00:20:15,360
and we ends up with an easy to maintain

00:20:11,760 --> 00:20:21,840
our high-performing tls implementation

00:20:15,360 --> 00:20:21,840
relying on the great linux network

00:20:23,280 --> 00:20:27,039
that's it thanks for listening

00:20:28,720 --> 00:20:35,760
okay thank you so we do have

00:20:32,320 --> 00:20:38,000
a few questions on the chat

00:20:35,760 --> 00:20:39,200
that i'd like to get to however we're

00:20:38,000 --> 00:20:42,240
also at the

00:20:39,200 --> 00:20:43,440
official end of the session so unless

00:20:42,240 --> 00:20:46,640
there's um

00:20:43,440 --> 00:20:48,480
objective objections i propose that we

00:20:46,640 --> 00:20:52,240
extend this a few minutes so we can

00:20:48,480 --> 00:20:52,240
cover uh some of these questions

00:20:52,480 --> 00:20:58,159
so the first question and then i think

00:20:54,480 --> 00:21:02,080
there were several in this vein

00:20:58,159 --> 00:21:04,799
reorder tcp packets so

00:21:02,080 --> 00:21:06,080
what exactly happens when they get

00:21:04,799 --> 00:21:09,120
reordered

00:21:06,080 --> 00:21:11,840
what is the impact what happens

00:21:09,120 --> 00:21:17,840
if we have a flow that's consistently

00:21:11,840 --> 00:21:17,840
getting reordered packets

00:21:18,000 --> 00:21:22,080
i'm not sure i i understand the whole

00:21:21,039 --> 00:21:25,440
question and

00:21:22,080 --> 00:21:28,720
i also we have boris here to help me

00:21:25,440 --> 00:21:31,630
as the architect of this feature so

00:21:28,720 --> 00:21:32,960
in case of reordering uh um

00:21:31,630 --> 00:21:36,000
[Music]

00:21:32,960 --> 00:21:38,720
we can trigger software offload to take

00:21:36,000 --> 00:21:38,720
control and

00:21:41,520 --> 00:21:46,320
like finish the job for the offload for

00:21:44,080 --> 00:21:49,679
the auto offload

00:21:46,320 --> 00:21:50,559
but in a pretty much reordering

00:21:49,679 --> 00:21:54,080
consistent

00:21:50,559 --> 00:21:56,559
environment ktls will will

00:21:54,080 --> 00:21:57,600
trigger this sync again and again so it

00:21:56,559 --> 00:22:00,080
will be

00:21:57,600 --> 00:22:00,080
harmful

00:22:02,240 --> 00:22:06,640
okay uh which real world website has

00:22:05,760 --> 00:22:11,200
http

00:22:06,640 --> 00:22:14,240
transfers with one megabyte sizes

00:22:11,200 --> 00:22:17,520
so this is a good question we we

00:22:14,240 --> 00:22:21,840
took this file sizes

00:22:17,520 --> 00:22:26,159
as we tried several file sizes so

00:22:21,840 --> 00:22:29,120
we started with the 64k files

00:22:26,159 --> 00:22:29,120
up to checking

00:22:30,080 --> 00:22:37,440
try to check like streaming scenarios

00:22:33,440 --> 00:22:42,320
in which companies that

00:22:37,440 --> 00:22:45,679
can support videos and streaming

00:22:42,320 --> 00:22:49,200
services to to the world so

00:22:45,679 --> 00:22:49,200
they use big files but

00:22:50,320 --> 00:22:56,480
i don't know for a real website that

00:22:53,760 --> 00:22:59,679
using one megabyte files but

00:22:56,480 --> 00:23:03,280
pretty much large files i know

00:22:59,679 --> 00:23:05,840
about several customers of our company

00:23:03,280 --> 00:23:08,480
so it can be large files but

00:23:05,840 --> 00:23:11,840
specifically one megabyte

00:23:08,480 --> 00:23:13,200
i can find an example for it okay so i

00:23:11,840 --> 00:23:15,919
think the point there is

00:23:13,200 --> 00:23:16,880
for something like video where we're

00:23:15,919 --> 00:23:19,919
serving

00:23:16,880 --> 00:23:21,919
uh massive amounts of data it's

00:23:19,919 --> 00:23:24,080
that's a clear use case for something

00:23:21,919 --> 00:23:27,760
like ktls where we're optimizing

00:23:24,080 --> 00:23:31,280
for the amount of data as opposed to

00:23:27,760 --> 00:23:34,320
small packets okay uh next question

00:23:31,280 --> 00:23:37,039
ktls works in process context

00:23:34,320 --> 00:23:37,760
so tls record and tcp segments are

00:23:37,039 --> 00:23:40,159
formed

00:23:37,760 --> 00:23:42,960
in different workflows wouldn't it

00:23:40,159 --> 00:23:43,760
simplify to manage tls records on tx

00:23:42,960 --> 00:23:48,840
path

00:23:43,760 --> 00:23:51,200
if ktls formed a tls record on sk wright

00:23:48,840 --> 00:23:54,640
transmit

00:23:51,200 --> 00:23:58,559
so i think the the question there is

00:23:54,640 --> 00:24:02,480
if we maybe deferred the

00:23:58,559 --> 00:24:04,799
the tls record into the kernel

00:24:02,480 --> 00:24:06,559
uh then we can construct tls records of

00:24:04,799 --> 00:24:07,200
optimal sizes that seems to be the

00:24:06,559 --> 00:24:08,559
question

00:24:07,200 --> 00:24:18,880
and then would that improve the

00:24:08,559 --> 00:24:21,520
performance for tls over tcp

00:24:18,880 --> 00:24:22,080
i'm not sure i i understand the question

00:24:21,520 --> 00:24:25,760
but

00:24:22,080 --> 00:24:27,520
if if i do some sounds like a pretty

00:24:25,760 --> 00:24:33,840
much

00:24:27,520 --> 00:24:33,840
something to check

00:24:37,279 --> 00:24:44,960
hello yes yes you can ask my voice

00:24:41,679 --> 00:24:48,799
um the question is that um we

00:24:44,960 --> 00:24:52,400
can in sk writex mid

00:24:48,799 --> 00:24:55,919
uh the function calling in gpip stack

00:24:52,400 --> 00:24:59,600
we know exactly uh the state of tcp

00:24:55,919 --> 00:25:02,320
uh exactly how much data gp

00:24:59,600 --> 00:25:02,880
are concerned to the pair in this place

00:25:02,320 --> 00:25:05,760
we can

00:25:02,880 --> 00:25:06,880
form a challenge record of optimal size

00:25:05,760 --> 00:25:10,240
so we can

00:25:06,880 --> 00:25:13,279
split the gls record of very

00:25:10,240 --> 00:25:15,120
accurately according to gp segments

00:25:13,279 --> 00:25:16,720
that's the problem which you

00:25:15,120 --> 00:25:19,200
initially mentioned at the beginning of

00:25:16,720 --> 00:25:22,799
presentation when jls record doesn't

00:25:19,200 --> 00:25:25,840
match uh tcp segments exactly

00:25:22,799 --> 00:25:28,880
so i'm wondering uh

00:25:25,840 --> 00:25:31,840
whether different design when

00:25:28,880 --> 00:25:33,279
rktls works inside the software queue

00:25:31,840 --> 00:25:36,960
contacts

00:25:33,279 --> 00:25:37,840
wouldn't such design simplify offloading

00:25:36,960 --> 00:25:43,840
on the

00:25:37,840 --> 00:25:43,840
network interface by the nick

00:25:46,240 --> 00:25:54,559
that makes sense yeah

00:25:50,720 --> 00:25:58,159
also to me so it might be interesting to

00:25:54,559 --> 00:26:00,559
see the uh prototype code of that

00:25:58,159 --> 00:26:01,600
we have a prototype our code works

00:26:00,559 --> 00:26:05,120
exactly this

00:26:01,600 --> 00:26:06,640
way it's not an upstream but it imps

00:26:05,120 --> 00:26:09,840
works exactly this

00:26:06,640 --> 00:26:11,520
way so i would suggest to post it

00:26:09,840 --> 00:26:14,720
upstream i think there's

00:26:11,520 --> 00:26:15,200
um there needs to be discussion one of

00:26:14,720 --> 00:26:18,480
the

00:26:15,200 --> 00:26:21,919
one of the concerns i would have is that

00:26:18,480 --> 00:26:25,120
we're inserting uh data into the tcp

00:26:21,919 --> 00:26:26,480
stream as opposed to to modifying that

00:26:25,120 --> 00:26:29,440
i'm sure that would have some

00:26:26,480 --> 00:26:30,400
some ramifications obviously with tls

00:26:29,440 --> 00:26:32,799
we're already being

00:26:30,400 --> 00:26:34,880
or ktls we're already being invasive on

00:26:32,799 --> 00:26:38,480
the transmit site

00:26:34,880 --> 00:26:39,440
so without raised questions of how do we

00:26:38,480 --> 00:26:42,080
synchronize

00:26:39,440 --> 00:26:44,320
if for instance we wanted to move the

00:26:42,080 --> 00:26:45,360
tls back into the host and things like

00:26:44,320 --> 00:26:49,440
that

00:26:45,360 --> 00:26:52,720
so let's go to a next question

00:26:49,440 --> 00:26:53,760
if the channel is lossy then ktls

00:26:52,720 --> 00:26:56,640
offload might

00:26:53,760 --> 00:26:58,080
go for resync often i think that's a

00:26:56,640 --> 00:27:01,679
similar question to

00:26:58,080 --> 00:27:01,679
the reordering yeah

00:27:02,720 --> 00:27:09,919
and okay uh i think uh

00:27:06,880 --> 00:27:11,679
so that's about it uh so uh

00:27:09,919 --> 00:27:13,600
i think thank you for everyone i think

00:27:11,679 --> 00:27:16,240
those are three really good talks

00:27:13,600 --> 00:27:18,399
uh super excited to see tls performance

00:27:16,240 --> 00:27:21,919
getting such visibility

00:27:18,399 --> 00:27:22,559
um tls in general okay with that uh left

00:27:21,919 --> 00:27:24,880
adjourned

00:27:22,559 --> 00:27:26,240
and i'll see you in the next set of

00:27:24,880 --> 00:27:32,240
workshops

00:27:26,240 --> 00:27:32,240

YouTube URL: https://www.youtube.com/watch?v=gkKrTXxfImc


