Title: Netdev 0x14 - Application Device Queues (ADQ) for System-Level Network I O Performance Improvments
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Amritha Nambiar, Kiran Patil, Sridhar Samudrala

More info: https://netdevconf.info/0x14/session.html?talk-ADQ-for-system-level-network-io-performance-improvements

Date: Wednesday, August 19, 2020

ADQ(Application Device Queues) support has been in the
kernel for a while now. ADQ enables network application
data to be isolated to specific (symmetric rx/tx) hardware
netdevice queue pair(s).
Application specific-data is ingressed towards these dedicated
queues and rate controlled in the egress direction - all using
policy definitions. ADQ uses standard Linux interfaces to achieve
its goals.

In this talk, Amritha Nambiar, Kiran Patil and Sridhar Samudrala
will:
- dig into the details of ADQ architecture and operations.
- illustrate collected data on how ADQ helps to improve
  predictability by reducing jitter, lowering latency and improving
  throughput.
- show, via an example application, how developers can take
  advantage of ADQ.

Amritha et al will also discuss future plans for ADQ such
as enabling busy polling on AF_XDP socket by associating a
NAPI_ID to an AF_XDP socket at bind time.
Captions: 
	00:00:03,360 --> 00:00:06,560
hi

00:00:03,919 --> 00:00:08,080
um my name is kiran pati and i have

00:00:06,560 --> 00:00:11,599
joined with my colleagues

00:00:08,080 --> 00:00:13,599
amrita namir and sridhar samudrala today

00:00:11,599 --> 00:00:14,719
we will be talking about application

00:00:13,599 --> 00:00:16,960
device skews

00:00:14,719 --> 00:00:19,359
for system level network io performance

00:00:16,960 --> 00:00:19,359
improvement

00:00:22,640 --> 00:00:28,080
okay so as we go along the

00:00:25,760 --> 00:00:30,640
agenda is going to be what is the

00:00:28,080 --> 00:00:32,320
motivation of talking about this

00:00:30,640 --> 00:00:35,040
uh what are all the system

00:00:32,320 --> 00:00:37,120
inefficiencies to solve the said problem

00:00:35,040 --> 00:00:38,640
in the brief introduction about what is

00:00:37,120 --> 00:00:42,079
adq

00:00:38,640 --> 00:00:43,200
uh the ingredients uh kind of putting it

00:00:42,079 --> 00:00:44,640
all together

00:00:43,200 --> 00:00:46,719
and we will move on to the performance

00:00:44,640 --> 00:00:48,239
section and

00:00:46,719 --> 00:00:50,879
look at the high level of the benefits

00:00:48,239 --> 00:00:51,760
of adq and translate it back to how the

00:00:50,879 --> 00:00:53,920
system

00:00:51,760 --> 00:00:55,120
inefficiencies are addressed and kind of

00:00:53,920 --> 00:00:57,199
what is the next step

00:00:55,120 --> 00:00:58,800
where do we go from here and then at the

00:00:57,199 --> 00:01:00,559
end there will be references

00:00:58,800 --> 00:01:02,879
and will open up for the question and

00:01:00,559 --> 00:01:02,879
answer

00:01:04,559 --> 00:01:10,320
okay so talking about the motivation and

00:01:07,439 --> 00:01:12,479
the speeding of the networking

00:01:10,320 --> 00:01:13,840
why does it matter basically in the

00:01:12,479 --> 00:01:16,960
linux conference

00:01:13,840 --> 00:01:18,960
2006 there was a linux there was this

00:01:16,960 --> 00:01:22,080
paper presented by john jacobson

00:01:18,960 --> 00:01:24,640
and the team and it talked about how the

00:01:22,080 --> 00:01:26,320
end of the wire isn't the end of the net

00:01:24,640 --> 00:01:28,560
so if you look at the picture on the

00:01:26,320 --> 00:01:30,479
left it talks about

00:01:28,560 --> 00:01:32,079
uh basically what they did before that

00:01:30,479 --> 00:01:33,759
one they did a small experiment on the

00:01:32,079 --> 00:01:35,040
uniprocessor system as well as on the

00:01:33,759 --> 00:01:36,720
multiprocessor system

00:01:35,040 --> 00:01:38,079
the experiment was about running the

00:01:36,720 --> 00:01:40,240
network test

00:01:38,079 --> 00:01:41,680
and seeing that do they get the same

00:01:40,240 --> 00:01:43,680
benefit or does the

00:01:41,680 --> 00:01:44,960
benefit or the performance improves when

00:01:43,680 --> 00:01:46,000
they go from uni processor to

00:01:44,960 --> 00:01:48,159
multi-processor

00:01:46,000 --> 00:01:50,560
actually the result were opposite on

00:01:48,159 --> 00:01:52,880
uniprocessor system the performance was

00:01:50,560 --> 00:01:54,479
say x and on the multiprocessor system

00:01:52,880 --> 00:01:57,280
it got degraded

00:01:54,479 --> 00:01:59,439
so then he talked then there was a

00:01:57,280 --> 00:02:01,360
discussion about the model where

00:01:59,439 --> 00:02:02,479
uh you know typically you have a network

00:02:01,360 --> 00:02:04,719
device it

00:02:02,479 --> 00:02:06,159
generates the interrupt uh then there is

00:02:04,719 --> 00:02:09,200
isr runs

00:02:06,159 --> 00:02:10,720
then it triggered the software interrupt

00:02:09,200 --> 00:02:13,280
and then in between there is a protocol

00:02:10,720 --> 00:02:15,280
processing so this is basically the

00:02:13,280 --> 00:02:17,200
interrupt level work and then there is a

00:02:15,280 --> 00:02:18,400
task level work where the application

00:02:17,200 --> 00:02:20,400
talks the socket

00:02:18,400 --> 00:02:21,520
this is the model we know of it became

00:02:20,400 --> 00:02:24,000
the standard

00:02:21,520 --> 00:02:25,760
but the idea of this one saying that hey

00:02:24,000 --> 00:02:27,440
if you want really the performance in

00:02:25,760 --> 00:02:28,560
the multiprocessor system we need to

00:02:27,440 --> 00:02:30,720
think differently

00:02:28,560 --> 00:02:33,360
what we have done one way is not

00:02:30,720 --> 00:02:35,440
necessarily always the same way to do it

00:02:33,360 --> 00:02:37,120
so what what the gist of that one is to

00:02:35,440 --> 00:02:40,160
say that

00:02:37,120 --> 00:02:42,640
you want to have a model where

00:02:40,160 --> 00:02:43,680
most of the work is done at the sockets

00:02:42,640 --> 00:02:45,599
at the application

00:02:43,680 --> 00:02:48,239
and driven from the application and do a

00:02:45,599 --> 00:02:49,599
minimal work inside the net

00:02:48,239 --> 00:02:51,440
so basically let the socket do

00:02:49,599 --> 00:02:54,800
everything and do the as less

00:02:51,440 --> 00:02:56,879
work possible uh inside the net and here

00:02:54,800 --> 00:02:58,959
when we talk about the net it comprises

00:02:56,879 --> 00:03:01,680
of the several element which is your

00:02:58,959 --> 00:03:02,640
the network device then the os software

00:03:01,680 --> 00:03:05,040
and whatnot

00:03:02,640 --> 00:03:08,400
until it hits the socket now how does

00:03:05,040 --> 00:03:09,680
this translates to internet stability

00:03:08,400 --> 00:03:11,360
so there is a picture on the right hand

00:03:09,680 --> 00:03:12,480
side it shows think like a network

00:03:11,360 --> 00:03:15,760
connection is like a

00:03:12,480 --> 00:03:17,599
loop sender receiver so in the back in

00:03:15,760 --> 00:03:18,800
the old days when there was no kernel

00:03:17,599 --> 00:03:19,760
and there was no protocol based

00:03:18,800 --> 00:03:21,440
implementation

00:03:19,760 --> 00:03:23,840
it was like a simple loop and there was

00:03:21,440 --> 00:03:25,519
a strong feedback mechanism both way

00:03:23,840 --> 00:03:27,519
so the receiving application and the

00:03:25,519 --> 00:03:29,360
sender application kind of

00:03:27,519 --> 00:03:31,040
knows each other's dynamics and they are

00:03:29,360 --> 00:03:33,760
reflected on the wire

00:03:31,040 --> 00:03:35,680
and this model had worked correctly but

00:03:33,760 --> 00:03:37,920
as the system evolved where we moved to

00:03:35,680 --> 00:03:40,159
the kernel base protocol implementation

00:03:37,920 --> 00:03:41,599
now suddenly it became from one loop to

00:03:40,159 --> 00:03:45,040
two loop

00:03:41,599 --> 00:03:48,159
now as we know the general theorem

00:03:45,040 --> 00:03:49,680
as per the basically uh raj

00:03:48,159 --> 00:03:51,200
ruth and herwitz saying that hey when

00:03:49,680 --> 00:03:53,519
you have two couple loops

00:03:51,200 --> 00:03:55,760
they will be always less stable than one

00:03:53,519 --> 00:03:56,879
great now how do we apply this to the

00:03:55,760 --> 00:03:58,720
networking

00:03:56,879 --> 00:04:00,080
so when you apply this to the networking

00:03:58,720 --> 00:04:02,080
what it means is when you have the

00:04:00,080 --> 00:04:04,400
kernel based protocol implementation

00:04:02,080 --> 00:04:06,080
it essentially hides the receiving app

00:04:04,400 --> 00:04:08,000
dynamics from the sender

00:04:06,080 --> 00:04:09,680
which essentially complicates the rtt

00:04:08,000 --> 00:04:10,799
estimate and causes the serious

00:04:09,680 --> 00:04:13,840
retransmission

00:04:10,799 --> 00:04:16,400
and all the fun begins so basically

00:04:13,840 --> 00:04:17,919
now to do the everything from the

00:04:16,400 --> 00:04:19,919
application we still need to come up

00:04:17,919 --> 00:04:21,120
with a model where it acts like one big

00:04:19,919 --> 00:04:22,880
giant loop

00:04:21,120 --> 00:04:25,759
basically no intermediate entity kind of

00:04:22,880 --> 00:04:28,320
hides the detail from each other

00:04:25,759 --> 00:04:29,759
so that's really the point of that

00:04:28,320 --> 00:04:32,240
saying the internet stability

00:04:29,759 --> 00:04:34,320
is important and how do you really solve

00:04:32,240 --> 00:04:36,720
that even in the modern system

00:04:34,320 --> 00:04:37,919
when you have systems with hundreds of

00:04:36,720 --> 00:04:39,919
cpus and

00:04:37,919 --> 00:04:42,240
faster and faster devices high speed

00:04:39,919 --> 00:04:46,400
devices with lots of queues and whatnot

00:04:42,240 --> 00:04:48,720
okay moving on to next one

00:04:46,400 --> 00:04:49,840
the the idea is let the application do

00:04:48,720 --> 00:04:51,680
everything

00:04:49,840 --> 00:04:52,960
basically let the socket do everything

00:04:51,680 --> 00:04:55,600
and do inside then

00:04:52,960 --> 00:04:57,280
do minimum work inside the net but to do

00:04:55,600 --> 00:05:00,000
that in a modern system

00:04:57,280 --> 00:05:00,560
there are some uh several inefficiencies

00:05:00,000 --> 00:05:02,240
exist

00:05:00,560 --> 00:05:04,320
and we need to look at each of those

00:05:02,240 --> 00:05:06,160
inefficiencies and essentially

00:05:04,320 --> 00:05:07,600
see how we can address those

00:05:06,160 --> 00:05:10,479
inefficiencies

00:05:07,600 --> 00:05:12,160
now the uh first and the four most

00:05:10,479 --> 00:05:13,280
important which affects the performance

00:05:12,160 --> 00:05:15,039
is the interrupt

00:05:13,280 --> 00:05:17,039
so when we talk about interrupt what it

00:05:15,039 --> 00:05:18,160
means when the application is doing some

00:05:17,039 --> 00:05:20,240
useful work

00:05:18,160 --> 00:05:21,840
if the device generates the interrupt

00:05:20,240 --> 00:05:22,800
and enter being the high priority it

00:05:21,840 --> 00:05:24,639
gives service

00:05:22,800 --> 00:05:25,919
what it means that application which is

00:05:24,639 --> 00:05:28,000
doing the useful work

00:05:25,919 --> 00:05:30,160
is getting context switch out and the

00:05:28,000 --> 00:05:32,000
interrupts takes a priority it executes

00:05:30,160 --> 00:05:33,600
and as a result of the application which

00:05:32,000 --> 00:05:36,000
is processing say some requests

00:05:33,600 --> 00:05:36,720
and trying to send some response the

00:05:36,000 --> 00:05:39,039
response

00:05:36,720 --> 00:05:40,720
doesn't get sent right away it it adds a

00:05:39,039 --> 00:05:43,039
jitter

00:05:40,720 --> 00:05:44,880
the second important point about the

00:05:43,039 --> 00:05:47,199
performance is the

00:05:44,880 --> 00:05:48,320
context switches and the synchronization

00:05:47,199 --> 00:05:50,160
so those two things

00:05:48,320 --> 00:05:51,840
typically you see they show up more and

00:05:50,160 --> 00:05:52,720
more in the multiprocessor system

00:05:51,840 --> 00:05:54,960
because

00:05:52,720 --> 00:05:57,280
they occur due to the sharing happens in

00:05:54,960 --> 00:05:59,120
the networking stack

00:05:57,280 --> 00:06:00,800
so idea that is how do you minimize the

00:05:59,120 --> 00:06:02,720
context switches how do you

00:06:00,800 --> 00:06:04,720
minimize or eliminate the

00:06:02,720 --> 00:06:06,639
synchronization needs such as locks and

00:06:04,720 --> 00:06:08,720
as we know locks basically

00:06:06,639 --> 00:06:10,000
uh affects the performance in a negative

00:06:08,720 --> 00:06:12,000
way

00:06:10,000 --> 00:06:14,960
so the second point is how do we address

00:06:12,000 --> 00:06:18,160
and minimize the synchronization needs

00:06:14,960 --> 00:06:19,280
the third the third system uh basically

00:06:18,160 --> 00:06:22,000
inefficiencies come

00:06:19,280 --> 00:06:22,560
about working set locality how do you

00:06:22,000 --> 00:06:24,639
contain

00:06:22,560 --> 00:06:25,680
or address the working site locality

00:06:24,639 --> 00:06:27,440
issue means

00:06:25,680 --> 00:06:29,600
how do you make sure your application

00:06:27,440 --> 00:06:32,560
and the protocol processing

00:06:29,600 --> 00:06:34,319
operate in the same context and that

00:06:32,560 --> 00:06:37,199
also will result into

00:06:34,319 --> 00:06:38,720
reducing the data moment uh so basically

00:06:37,199 --> 00:06:40,479
the data moment during the packet

00:06:38,720 --> 00:06:42,639
processing for control and data we want

00:06:40,479 --> 00:06:44,319
to minimize that if you want to contain

00:06:42,639 --> 00:06:47,120
the working set locality

00:06:44,319 --> 00:06:48,960
so the idea of this one is that last

00:06:47,120 --> 00:06:49,840
mile which is the end of the wire is not

00:06:48,960 --> 00:06:51,120
end of the net

00:06:49,840 --> 00:06:53,360
essentially the end of the wire is the

00:06:51,120 --> 00:06:55,039
end of the app to make that happen

00:06:53,360 --> 00:06:56,639
we really need to look at the system

00:06:55,039 --> 00:06:57,440
inefficiencies and how do we address

00:06:56,639 --> 00:07:00,000
those

00:06:57,440 --> 00:07:02,880
so last year intel announced a

00:07:00,000 --> 00:07:03,759
technology called adq application device

00:07:02,880 --> 00:07:05,840
queues

00:07:03,759 --> 00:07:07,440
it is primarily a workload optimization

00:07:05,840 --> 00:07:11,039
technology aimed at

00:07:07,440 --> 00:07:14,400
package steering and queueing and

00:07:11,039 --> 00:07:16,720
basically we will see that

00:07:14,400 --> 00:07:19,120
how this technology is trying to solve

00:07:16,720 --> 00:07:21,280
the system inefficiencies and what not

00:07:19,120 --> 00:07:23,280
so before we go and dive into the

00:07:21,280 --> 00:07:23,919
details how the technology i would like

00:07:23,280 --> 00:07:27,840
to give

00:07:23,919 --> 00:07:30,800
give the brief overview about adq

00:07:27,840 --> 00:07:31,599
now what is really the adq so at its

00:07:30,800 --> 00:07:34,240
heart

00:07:31,599 --> 00:07:35,680
it is queuing and steering technology by

00:07:34,240 --> 00:07:37,599
the way it is not a protocol

00:07:35,680 --> 00:07:39,440
it is a queuing and steering technology

00:07:37,599 --> 00:07:41,360
built on the top of the existing rich

00:07:39,440 --> 00:07:43,039
history of the queueing and technology

00:07:41,360 --> 00:07:44,639
exist in the linux stack in the linux

00:07:43,039 --> 00:07:46,400
kernel networking stack

00:07:44,639 --> 00:07:48,319
but with the slight twist and the slight

00:07:46,400 --> 00:07:49,039
twist is by bringing the application in

00:07:48,319 --> 00:07:51,280
the picture

00:07:49,039 --> 00:07:52,400
and putting it at the forefront uh

00:07:51,280 --> 00:07:54,560
forefront of the

00:07:52,400 --> 00:07:55,520
decision making decision making about

00:07:54,560 --> 00:07:57,199
the packaging

00:07:55,520 --> 00:07:59,120
package steering which is done on its

00:07:57,199 --> 00:08:00,479
behalf so you are essentially putting

00:07:59,120 --> 00:08:02,720
the application at the front and

00:08:00,479 --> 00:08:04,319
it is becoming responsible or becoming

00:08:02,720 --> 00:08:08,560
the driving factor for such

00:08:04,319 --> 00:08:11,199
decision making the what you can also do

00:08:08,560 --> 00:08:12,080
with the now you can see the picture on

00:08:11,199 --> 00:08:14,160
the left hand side

00:08:12,080 --> 00:08:15,120
it shows you how in the network device

00:08:14,160 --> 00:08:16,000
and on the right side you have

00:08:15,120 --> 00:08:18,240
application

00:08:16,000 --> 00:08:19,280
and each application is color coded

00:08:18,240 --> 00:08:21,759
differently

00:08:19,280 --> 00:08:24,400
uh so what it gives it gives a dedicated

00:08:21,759 --> 00:08:26,080
and isolated queues

00:08:24,400 --> 00:08:27,440
dedicated and isolated queues per

00:08:26,080 --> 00:08:30,400
application

00:08:27,440 --> 00:08:32,000
so with that one can create express

00:08:30,400 --> 00:08:33,519
dedicated express lane

00:08:32,000 --> 00:08:37,120
between the application thread of

00:08:33,519 --> 00:08:40,159
execution to the hardware queues

00:08:37,120 --> 00:08:42,959
and so in addition to that

00:08:40,159 --> 00:08:43,440
uh what one can do with the adq is about

00:08:42,959 --> 00:08:46,000
the

00:08:43,440 --> 00:08:48,480
egress egress bandwidth basically when

00:08:46,000 --> 00:08:50,560
you have a device say 100 gig device

00:08:48,480 --> 00:08:52,080
and you're running multiple application

00:08:50,560 --> 00:08:53,440
uh if you don't do anything

00:08:52,080 --> 00:08:55,839
it's like a first come first served

00:08:53,440 --> 00:08:57,680
basis you may have situation where some

00:08:55,839 --> 00:08:59,360
application may not get bandwidth or may

00:08:57,680 --> 00:09:00,080
get a less bandwidth whereas they need

00:08:59,360 --> 00:09:02,720
more

00:09:00,080 --> 00:09:03,519
so adq provides a standard mechanism

00:09:02,720 --> 00:09:06,080
where

00:09:03,519 --> 00:09:07,120
uh in application specific manner you

00:09:06,080 --> 00:09:09,120
can prioritize

00:09:07,120 --> 00:09:10,800
and divide that bandwidth among the

00:09:09,120 --> 00:09:13,440
multiple applications

00:09:10,800 --> 00:09:14,399
as the all all this one is done using

00:09:13,440 --> 00:09:17,680
the

00:09:14,399 --> 00:09:19,680
standard internal networking framework

00:09:17,680 --> 00:09:21,600
uh which has been made basically the

00:09:19,680 --> 00:09:23,360
there has been substantial changes made

00:09:21,600 --> 00:09:25,120
to that and the contribution is done to

00:09:23,360 --> 00:09:28,480
the back in the open source community

00:09:25,120 --> 00:09:30,959
and you will see that as we go along

00:09:28,480 --> 00:09:32,160
the from the slide perspective the third

00:09:30,959 --> 00:09:34,080
point is

00:09:32,160 --> 00:09:36,560
from application there is a small change

00:09:34,080 --> 00:09:38,560
is desired the small change is desired

00:09:36,560 --> 00:09:39,120
to create the single producer consumer

00:09:38,560 --> 00:09:40,640
model

00:09:39,120 --> 00:09:42,720
between the application thread to this

00:09:40,640 --> 00:09:44,240
hardware queues

00:09:42,720 --> 00:09:46,800
as you can see those green light from

00:09:44,240 --> 00:09:49,680
each application or from nvmu or tcp

00:09:46,800 --> 00:09:51,440
application going to

00:09:49,680 --> 00:09:53,760
basically having this dedicated lane

00:09:51,440 --> 00:09:56,240
with the hardware cues likewise

00:09:53,760 --> 00:09:57,680
the yellow and the blue and whatnot so

00:09:56,240 --> 00:09:59,360
essentially to create this single

00:09:57,680 --> 00:10:00,320
producer consumer model the small

00:09:59,360 --> 00:10:02,079
application

00:10:00,320 --> 00:10:03,760
exchange is desired so that the

00:10:02,079 --> 00:10:06,959
application thread of execution is

00:10:03,760 --> 00:10:09,519
aligned with those hardware queues

00:10:06,959 --> 00:10:11,440
and in general the core design principle

00:10:09,519 --> 00:10:13,120
of the application is

00:10:11,440 --> 00:10:15,440
because application is aware of its

00:10:13,120 --> 00:10:18,320
action application best know its need

00:10:15,440 --> 00:10:20,399
so let it let application be the

00:10:18,320 --> 00:10:22,240
deciding factor or at least have a say

00:10:20,399 --> 00:10:24,160
in the package queueing packet sharing

00:10:22,240 --> 00:10:27,519
and the packet processing which is done

00:10:24,160 --> 00:10:29,200
on on its behalf and as you will see

00:10:27,519 --> 00:10:30,560
that once we have the single producer

00:10:29,200 --> 00:10:33,120
consumer and

00:10:30,560 --> 00:10:34,079
dedicated and isolated queues how some

00:10:33,120 --> 00:10:37,279
of this

00:10:34,079 --> 00:10:40,800
system inefficiencies gets addressed uh

00:10:37,279 --> 00:10:42,240
one by one so uh now i will hand it over

00:10:40,800 --> 00:10:42,720
to the amrita which will walk you

00:10:42,240 --> 00:10:46,399
through

00:10:42,720 --> 00:10:49,839
the adq ingredients

00:10:46,399 --> 00:10:53,839
yeah so as kieran just explained

00:10:49,839 --> 00:10:56,160
what adq provides is applications can

00:10:53,839 --> 00:10:57,279
have their own customizable range of

00:10:56,160 --> 00:11:00,079
queues

00:10:57,279 --> 00:11:01,440
and these cues can be dedicated for the

00:11:00,079 --> 00:11:03,839
application

00:11:01,440 --> 00:11:06,959
so let's now look into the details of

00:11:03,839 --> 00:11:09,360
how we can make this practical

00:11:06,959 --> 00:11:10,079
the first step here is configuring the

00:11:09,360 --> 00:11:13,200
application

00:11:10,079 --> 00:11:15,360
priority so

00:11:13,200 --> 00:11:16,720
a network priority needs to be

00:11:15,360 --> 00:11:18,720
associated with

00:11:16,720 --> 00:11:20,320
all traffic originating from the

00:11:18,720 --> 00:11:22,880
application

00:11:20,320 --> 00:11:24,160
in linux we have various mechanisms to

00:11:22,880 --> 00:11:27,680
do this

00:11:24,160 --> 00:11:31,600
so firstly we have the c group and c

00:11:27,680 --> 00:11:33,760
group v1 has a controller called netprio

00:11:31,600 --> 00:11:35,200
we could use the netprior controller to

00:11:33,760 --> 00:11:37,600
set priority for

00:11:35,200 --> 00:11:38,959
applications belonging to a certain c

00:11:37,600 --> 00:11:41,600
group

00:11:38,959 --> 00:11:43,200
now since c group is moving on to a

00:11:41,600 --> 00:11:46,399
newer version called c

00:11:43,200 --> 00:11:50,320
group v2 which is going to be

00:11:46,399 --> 00:11:53,440
adopted by uh distros and containers

00:11:50,320 --> 00:11:54,800
c group v2 has deprecated the net prior

00:11:53,440 --> 00:11:58,399
controller

00:11:54,800 --> 00:12:01,600
so here we are using the bpf approach

00:11:58,399 --> 00:12:04,560
uh we use the c group sock up

00:12:01,600 --> 00:12:06,959
type of bpf program to set the socket

00:12:04,560 --> 00:12:09,760
priority for the application

00:12:06,959 --> 00:12:12,000
and lastly we also have the option of

00:12:09,760 --> 00:12:14,959
modifying the application itself

00:12:12,000 --> 00:12:17,680
uh using the so priority socket option

00:12:14,959 --> 00:12:20,560
to set the application priority

00:12:17,680 --> 00:12:21,440
now once the priority is set we have the

00:12:20,560 --> 00:12:23,760
second step

00:12:21,440 --> 00:12:26,000
which is configuring the queue set or

00:12:23,760 --> 00:12:29,600
the queue group itself

00:12:26,000 --> 00:12:33,360
here we use the standard linux

00:12:29,600 --> 00:12:33,839
tc subsystem the mq prior scheduler to

00:12:33,360 --> 00:12:37,120
be

00:12:33,839 --> 00:12:40,160
specific so linux has

00:12:37,120 --> 00:12:42,720
a multi-queue priority scheduler

00:12:40,160 --> 00:12:44,399
and the multi-cue priority scheduler

00:12:42,720 --> 00:12:47,680
already had support for

00:12:44,399 --> 00:12:51,040
creating traffic classes and mapping the

00:12:47,680 --> 00:12:53,040
application priority to a traffic class

00:12:51,040 --> 00:12:54,240
so we essentially leveraged this

00:12:53,040 --> 00:12:57,040
mechanism

00:12:54,240 --> 00:12:58,959
and we also extended the mq prior

00:12:57,040 --> 00:13:01,120
scheduler

00:12:58,959 --> 00:13:02,560
we added shaping features to the mq

00:13:01,120 --> 00:13:05,760
prior scheduler

00:13:02,560 --> 00:13:08,240
wherein we could configure minimum and

00:13:05,760 --> 00:13:10,000
maximum bandwidth rates for each of the

00:13:08,240 --> 00:13:12,560
traffic classes

00:13:10,000 --> 00:13:13,760
so this would ensure a guaranteed

00:13:12,560 --> 00:13:16,800
bandwidth rate

00:13:13,760 --> 00:13:20,160
uh for every application and avoid

00:13:16,800 --> 00:13:22,959
any resource starving and

00:13:20,160 --> 00:13:24,399
we also introduced a new offload mode in

00:13:22,959 --> 00:13:26,959
mq prior

00:13:24,399 --> 00:13:28,480
so mq prior has a default hardware

00:13:26,959 --> 00:13:32,000
offload mode it's called

00:13:28,480 --> 00:13:35,040
dcb wherein the number of pcs could be

00:13:32,000 --> 00:13:37,040
offloaded to the device so in the new

00:13:35,040 --> 00:13:38,720
offload mode that we introduced it's

00:13:37,040 --> 00:13:41,040
called the channel mode

00:13:38,720 --> 00:13:43,279
we could offload the number of pcs the

00:13:41,040 --> 00:13:45,040
priority to pc mapping

00:13:43,279 --> 00:13:46,720
uh the queue layout or the queue

00:13:45,040 --> 00:13:48,560
configuration itself

00:13:46,720 --> 00:13:50,880
and the bandwidth rates per traffic

00:13:48,560 --> 00:13:53,519
class to the device

00:13:50,880 --> 00:13:54,639
and we also extended the user space

00:13:53,519 --> 00:13:58,000
counterpart the

00:13:54,639 --> 00:14:00,800
ip route 2 for each of the additional

00:13:58,000 --> 00:14:03,519
functionalities we added for mkprio

00:14:00,800 --> 00:14:04,480
so here's an example of how we configure

00:14:03,519 --> 00:14:06,720
this

00:14:04,480 --> 00:14:07,920
uh so using the mq prior queueing

00:14:06,720 --> 00:14:10,800
discipline we create

00:14:07,920 --> 00:14:11,360
four pcs and we can also see the map

00:14:10,800 --> 00:14:13,519
where

00:14:11,360 --> 00:14:16,240
the priorities are mapped to the pc so

00:14:13,519 --> 00:14:19,680
we have priority zero mapped to tc0

00:14:16,240 --> 00:14:22,160
priority 1 to tc1 and likewise

00:14:19,680 --> 00:14:23,360
then in the next line we have the queue

00:14:22,160 --> 00:14:25,680
layout

00:14:23,360 --> 00:14:27,279
so this is the queue distribution per

00:14:25,680 --> 00:14:30,399
traffic class

00:14:27,279 --> 00:14:32,880
we specify the base q

00:14:30,399 --> 00:14:34,480
and the number of queues for each of the

00:14:32,880 --> 00:14:36,800
traffic class

00:14:34,480 --> 00:14:38,880
and in the next line we have the minimum

00:14:36,800 --> 00:14:39,920
rates configured for each of the traffic

00:14:38,880 --> 00:14:42,240
class

00:14:39,920 --> 00:14:44,160
and in the last line we have the max

00:14:42,240 --> 00:14:45,440
rates specified for each of the traffic

00:14:44,160 --> 00:14:48,320
class

00:14:45,440 --> 00:14:51,440
so this finishes up the queue group

00:14:48,320 --> 00:14:55,279
configuration for the applications

00:14:51,440 --> 00:14:55,279
now let's move on to the next slide

00:14:57,760 --> 00:15:03,680
so once the queue groups are configured

00:15:00,959 --> 00:15:06,240
we need to isolate the incoming traffic

00:15:03,680 --> 00:15:09,839
into each of these queue groups

00:15:06,240 --> 00:15:11,360
for this we use the tc flower classifier

00:15:09,839 --> 00:15:14,639
in linux

00:15:11,360 --> 00:15:17,440
we extended tc flower uh

00:15:14,639 --> 00:15:17,839
to direct incoming traffic to the queue

00:15:17,440 --> 00:15:20,240
group

00:15:17,839 --> 00:15:21,040
or the cue set that we created using mq

00:15:20,240 --> 00:15:23,199
prior

00:15:21,040 --> 00:15:24,720
and we could also offload this filter

00:15:23,199 --> 00:15:26,880
into the hardware

00:15:24,720 --> 00:15:27,920
so on the right hand side i have some

00:15:26,880 --> 00:15:30,880
examples of

00:15:27,920 --> 00:15:31,279
how to add this filter and offload it to

00:15:30,880 --> 00:15:34,320
the

00:15:31,279 --> 00:15:35,600
device so we use some application

00:15:34,320 --> 00:15:37,839
identifier

00:15:35,600 --> 00:15:39,519
uh here we are using the destination ip

00:15:37,839 --> 00:15:42,000
and the destination port

00:15:39,519 --> 00:15:43,839
and based on the application identifier

00:15:42,000 --> 00:15:44,639
we direct the incoming traffic to a

00:15:43,839 --> 00:15:46,720
certain

00:15:44,639 --> 00:15:48,320
traffic class or queue group created

00:15:46,720 --> 00:15:51,120
using mqprio

00:15:48,320 --> 00:15:54,800
and in tc flower we use the hardware tc

00:15:51,120 --> 00:15:57,519
option to specify the traffic class

00:15:54,800 --> 00:15:59,279
and finally we need to select the queue

00:15:57,519 --> 00:16:02,000
within the queue group

00:15:59,279 --> 00:16:04,320
for this we use some hardware mechanism

00:16:02,000 --> 00:16:05,360
like rss or flow director for the queue

00:16:04,320 --> 00:16:07,199
selection

00:16:05,360 --> 00:16:08,399
so now we have seen the queue group

00:16:07,199 --> 00:16:10,639
configuration

00:16:08,399 --> 00:16:13,199
the isolation into the queue group and

00:16:10,639 --> 00:16:17,040
the queue selection itself

00:16:13,199 --> 00:16:17,040
let's move on to the next slide

00:16:18,560 --> 00:16:22,959
so uh to set up rest of the adq

00:16:21,839 --> 00:16:26,480
ingredients

00:16:22,959 --> 00:16:29,199
we had to extend and move beyond the tc

00:16:26,480 --> 00:16:33,040
subsystem itself we worked in other

00:16:29,199 --> 00:16:36,800
networking subsystems in linux

00:16:33,040 --> 00:16:39,000
in adq the core idea is that

00:16:36,800 --> 00:16:40,800
we try to maintain a single

00:16:39,000 --> 00:16:43,920
synchronization free

00:16:40,800 --> 00:16:45,120
single producer consumer model to

00:16:43,920 --> 00:16:47,440
accomplish this

00:16:45,120 --> 00:16:49,120
we try to establish a unique pipe

00:16:47,440 --> 00:16:52,320
between the application thread

00:16:49,120 --> 00:16:53,839
and the device queue for this we follow

00:16:52,320 --> 00:16:55,920
two-fold approach

00:16:53,839 --> 00:16:56,959
so firstly we align the application

00:16:55,920 --> 00:16:59,839
thread to the receive

00:16:56,959 --> 00:17:01,360
side and then to the transmit side so

00:16:59,839 --> 00:17:04,720
let's see how we

00:17:01,360 --> 00:17:06,959
can do the receive side alignment so

00:17:04,720 --> 00:17:08,559
we worked in the busy polling subsystem

00:17:06,959 --> 00:17:10,959
so busy poll already had

00:17:08,559 --> 00:17:13,439
support for standard system calls like

00:17:10,959 --> 00:17:17,280
receive and poll

00:17:13,439 --> 00:17:20,640
we then extended the e full subsystem

00:17:17,280 --> 00:17:21,760
uh so we added visible support to e-pole

00:17:20,640 --> 00:17:23,760
sockets

00:17:21,760 --> 00:17:25,199
so with this e-ball enablement what the

00:17:23,760 --> 00:17:27,280
application gets is

00:17:25,199 --> 00:17:28,240
application threads can have multiple

00:17:27,280 --> 00:17:31,039
sockets

00:17:28,240 --> 00:17:32,400
which get traffic from the same received

00:17:31,039 --> 00:17:34,559
queue and when

00:17:32,400 --> 00:17:37,200
applications call e-poll wait and there

00:17:34,559 --> 00:17:39,760
are no events available to report

00:17:37,200 --> 00:17:41,440
then busy poll can pull packets from

00:17:39,760 --> 00:17:43,919
this receipt queue

00:17:41,440 --> 00:17:45,840
so here's an example of how we use this

00:17:43,919 --> 00:17:46,960
control to specify the busy pole

00:17:45,840 --> 00:17:50,000
configuration

00:17:46,960 --> 00:17:50,720
the value is the time in microseconds to

00:17:50,000 --> 00:17:52,799
wait

00:17:50,720 --> 00:17:54,880
until a package arrives on the received

00:17:52,799 --> 00:17:56,720
queue for polling

00:17:54,880 --> 00:17:59,600
we'll now look into the queue

00:17:56,720 --> 00:18:02,559
identification aspect of this

00:17:59,600 --> 00:18:04,960
for this we use the navi id as a unique

00:18:02,559 --> 00:18:07,840
metadata to identify the queue

00:18:04,960 --> 00:18:09,280
the nav pid is associated with a queue

00:18:07,840 --> 00:18:11,919
vector

00:18:09,280 --> 00:18:14,799
so we introduced a new socket option

00:18:11,919 --> 00:18:18,400
called the so incoming nappy id

00:18:14,799 --> 00:18:21,039
applications can query the nappy id

00:18:18,400 --> 00:18:22,480
and then split the incoming traffic

00:18:21,039 --> 00:18:24,640
across the threads

00:18:22,480 --> 00:18:26,480
depending on the queue the packet

00:18:24,640 --> 00:18:30,000
arrived on

00:18:26,480 --> 00:18:32,880
so with this we complete the application

00:18:30,000 --> 00:18:36,240
thread alignment to the received queue

00:18:32,880 --> 00:18:40,000
uh we'll now move on to the next slide

00:18:36,240 --> 00:18:40,799
where we can see how uh the application

00:18:40,000 --> 00:18:43,520
threads get

00:18:40,799 --> 00:18:44,720
aligned to the transmit side or the

00:18:43,520 --> 00:18:48,000
transmit queue

00:18:44,720 --> 00:18:51,919
so here we work with the xps mechanism

00:18:48,000 --> 00:18:55,919
so linux xps or transmit packet steering

00:18:51,919 --> 00:18:57,200
is based on sender cpu we extended this

00:18:55,919 --> 00:19:00,480
to support

00:18:57,200 --> 00:19:02,960
uh receive cube based transmit packet

00:19:00,480 --> 00:19:05,120
transmit queue selection so receive

00:19:02,960 --> 00:19:08,080
queue or receive qmap is provided as a

00:19:05,120 --> 00:19:09,919
hint to select the transmit queue

00:19:08,080 --> 00:19:12,160
and this can be done by writing the

00:19:09,919 --> 00:19:12,720
queue mask for each of the transmit

00:19:12,160 --> 00:19:15,919
queues

00:19:12,720 --> 00:19:19,039
to the csfs attribute which is uh called

00:19:15,919 --> 00:19:22,080
xpsr excuse so this is an optional

00:19:19,039 --> 00:19:25,039
configuration using the csfs interface

00:19:22,080 --> 00:19:26,480
and the fallback mechanism is the

00:19:25,039 --> 00:19:30,160
already supported

00:19:26,480 --> 00:19:31,919
xps using sender cpus or the jhash based

00:19:30,160 --> 00:19:34,960
algorithm

00:19:31,919 --> 00:19:38,559
now the idea behind doing this

00:19:34,960 --> 00:19:39,520
uh is to align the transmit and receive

00:19:38,559 --> 00:19:42,400
use

00:19:39,520 --> 00:19:43,919
so with this we can have egress and

00:19:42,400 --> 00:19:46,720
ingress traffic

00:19:43,919 --> 00:19:48,480
traversing on a symmetric uh transmit

00:19:46,720 --> 00:19:51,120
and receive q pair

00:19:48,480 --> 00:19:53,520
so what we get out of this is that

00:19:51,120 --> 00:19:55,280
transmit completions can be locked to

00:19:53,520 --> 00:19:57,520
the same queue association that

00:19:55,280 --> 00:20:00,160
application is pulling on

00:19:57,520 --> 00:20:01,760
so this avoids the overhead of having to

00:20:00,160 --> 00:20:05,280
trigger an interrupt on

00:20:01,760 --> 00:20:07,520
a different cpu and when the application

00:20:05,280 --> 00:20:09,200
is busy pulling the

00:20:07,520 --> 00:20:10,880
packets from the receive queue and

00:20:09,200 --> 00:20:13,919
processing it in the

00:20:10,880 --> 00:20:16,400
application thread context

00:20:13,919 --> 00:20:17,200
the transmit completions can also happen

00:20:16,400 --> 00:20:19,760
along with it

00:20:17,200 --> 00:20:22,320
in the same thread context so this

00:20:19,760 --> 00:20:24,880
avoids any latency as well

00:20:22,320 --> 00:20:28,320
so this completes the application thread

00:20:24,880 --> 00:20:30,720
alignment to the transfer queue

00:20:28,320 --> 00:20:32,159
so in the next slide we'll see various

00:20:30,720 --> 00:20:35,200
linux versions

00:20:32,159 --> 00:20:38,159
where each of the adq ingredients got

00:20:35,200 --> 00:20:39,120
upstreamed into the linux kernel so

00:20:38,159 --> 00:20:41,840
before

00:20:39,120 --> 00:20:43,840
19 we complete uh upstreaming the

00:20:41,840 --> 00:20:46,480
various adq ingredients

00:20:43,840 --> 00:20:47,039
and now i'll hand over back to kieran

00:20:46,480 --> 00:20:49,840
for

00:20:47,039 --> 00:20:49,840
rest of the session

00:20:53,280 --> 00:20:56,880
thank you amrita as we saw the overview

00:20:56,080 --> 00:20:59,760
of the various

00:20:56,880 --> 00:21:00,720
ingredients now the last last part of

00:20:59,760 --> 00:21:02,799
the puzzle

00:21:00,720 --> 00:21:04,000
is the align application thread to a

00:21:02,799 --> 00:21:05,679
queue so

00:21:04,000 --> 00:21:07,360
this is what we also refer as a change

00:21:05,679 --> 00:21:09,120
in the application so

00:21:07,360 --> 00:21:12,000
there are class of application where you

00:21:09,120 --> 00:21:14,240
do not need to change anything

00:21:12,000 --> 00:21:16,240
such as single threaded application or

00:21:14,240 --> 00:21:19,120
the application who do not wish to

00:21:16,240 --> 00:21:19,919
perform any busy polling or if there are

00:21:19,120 --> 00:21:22,799
application

00:21:19,919 --> 00:21:23,280
where they are using the standard linux

00:21:22,799 --> 00:21:26,000
way

00:21:23,280 --> 00:21:26,880
to do the distribution of connections to

00:21:26,000 --> 00:21:29,520
the worker queue

00:21:26,880 --> 00:21:31,360
uh to the worker thread uh where if they

00:21:29,520 --> 00:21:33,600
decide to use s or u sport then the

00:21:31,360 --> 00:21:35,760
kernel based load balancing system

00:21:33,600 --> 00:21:36,799
just works fine the small bpf program

00:21:35,760 --> 00:21:39,840
can be attached

00:21:36,799 --> 00:21:40,640
and it can do a job uh so there but

00:21:39,840 --> 00:21:42,640
there is a

00:21:40,640 --> 00:21:44,400
one class of application like memcache

00:21:42,640 --> 00:21:46,400
which it employs his own load

00:21:44,400 --> 00:21:48,559
distribution technique

00:21:46,400 --> 00:21:49,600
because it's a server application so it

00:21:48,559 --> 00:21:51,840
has uh

00:21:49,600 --> 00:21:53,280
in in their case they have one thread

00:21:51,840 --> 00:21:54,559
doing all the accepts connection and

00:21:53,280 --> 00:21:56,080
dispatching the connection to the

00:21:54,559 --> 00:21:58,720
different worker thread

00:21:56,080 --> 00:21:59,200
to essentially handle those connections

00:21:58,720 --> 00:22:02,799
so

00:21:59,200 --> 00:22:05,120
that logic needs a small modification uh

00:22:02,799 --> 00:22:06,799
where it will use the so incoming api id

00:22:05,120 --> 00:22:08,880
as a socket option and based on the

00:22:06,799 --> 00:22:10,960
return value it will decide and pick the

00:22:08,880 --> 00:22:12,880
worker thread

00:22:10,960 --> 00:22:15,440
so this change is only needed if you

00:22:12,880 --> 00:22:18,240
want optimal performance

00:22:15,440 --> 00:22:19,760
uh and this change will allow

00:22:18,240 --> 00:22:21,120
essentially to get to the single

00:22:19,760 --> 00:22:22,640
producer consumer

00:22:21,120 --> 00:22:25,200
all the way from the application thread

00:22:22,640 --> 00:22:27,280
of execution to the hardware queues

00:22:25,200 --> 00:22:29,200
but as i have explained there are other

00:22:27,280 --> 00:22:30,159
ways to do it for some other application

00:22:29,200 --> 00:22:32,000
one can use

00:22:30,159 --> 00:22:33,520
just the sideband filter if it is just

00:22:32,000 --> 00:22:36,159
single threaded application

00:22:33,520 --> 00:22:37,760
or a other means but there are certain

00:22:36,159 --> 00:22:39,360
class of application where you need to

00:22:37,760 --> 00:22:42,559
make small changes

00:22:39,360 --> 00:22:43,840
uh with respect to how the basically the

00:22:42,559 --> 00:22:46,480
connections are handled

00:22:43,840 --> 00:22:46,480
and dispatch

00:22:49,120 --> 00:22:54,880
now we saw the various ingredients and

00:22:53,360 --> 00:22:56,480
basically how do you align your

00:22:54,880 --> 00:22:57,039
application thread to the hardware

00:22:56,480 --> 00:22:58,960
queues

00:22:57,039 --> 00:23:00,080
to get to the single producer consumer

00:22:58,960 --> 00:23:03,039
now we will look at some of the

00:23:00,080 --> 00:23:03,039
performance aspect

00:23:03,200 --> 00:23:06,960
so using all this one we put this

00:23:05,840 --> 00:23:09,200
memcache

00:23:06,960 --> 00:23:10,240
application to the test so what you see

00:23:09,200 --> 00:23:12,799
on the right hand side

00:23:10,240 --> 00:23:14,640
is the device under test which is the

00:23:12,799 --> 00:23:15,840
next generation intel's next generation

00:23:14,640 --> 00:23:18,080
neck

00:23:15,840 --> 00:23:19,600
which is hosting the memcache server and

00:23:18,080 --> 00:23:21,840
on the left hand side the standard

00:23:19,600 --> 00:23:25,200
client it's a 10 clients

00:23:21,840 --> 00:23:27,440
environment and with memcache it is like

00:23:25,200 --> 00:23:30,240
a request response you send a key

00:23:27,440 --> 00:23:33,039
and saying that i want a basically

00:23:30,240 --> 00:23:35,679
response it's like a key value pair

00:23:33,039 --> 00:23:37,039
okay so that's really the test topology

00:23:35,679 --> 00:23:39,039
now let's look at the

00:23:37,039 --> 00:23:40,240
some of the performance results so when

00:23:39,039 --> 00:23:41,919
you talk about

00:23:40,240 --> 00:23:43,679
the application like memcache

00:23:41,919 --> 00:23:45,840
essentially they look the

00:23:43,679 --> 00:23:47,120
three or four factor one of them is

00:23:45,840 --> 00:23:50,000
obviously a throughput

00:23:47,120 --> 00:23:51,200
as you can see on the y axis is your

00:23:50,000 --> 00:23:53,520
throughput on the xs

00:23:51,200 --> 00:23:54,640
axis is your total number of connections

00:23:53,520 --> 00:23:56,960
the light blue bar

00:23:54,640 --> 00:23:58,000
is when you don't have a dq basically

00:23:56,960 --> 00:24:00,320
adq off

00:23:58,000 --> 00:24:02,000
and the dark blue vertical bar is it

00:24:00,320 --> 00:24:04,240
represents the adq on

00:24:02,000 --> 00:24:05,679
and as you can see irrespective of the

00:24:04,240 --> 00:24:07,840
number of connections

00:24:05,679 --> 00:24:09,440
right say around from 300 connections to

00:24:07,840 --> 00:24:11,200
4400 connections

00:24:09,440 --> 00:24:13,039
there is a healthy increase when you

00:24:11,200 --> 00:24:13,679
have the adq turn on with respect to the

00:24:13,039 --> 00:24:16,559
throughput

00:24:13,679 --> 00:24:17,760
so more or less 70 or greater than 70

00:24:16,559 --> 00:24:19,600
throughput improvement

00:24:17,760 --> 00:24:20,799
all the way to the end means it is

00:24:19,600 --> 00:24:22,240
scaling with respect to you and the

00:24:20,799 --> 00:24:24,559
number of connections

00:24:22,240 --> 00:24:27,279
so obviously when you talk throughput

00:24:24,559 --> 00:24:27,279
higher is better

00:24:27,520 --> 00:24:32,960
the second important factor basically

00:24:31,120 --> 00:24:35,600
for this application which they look for

00:24:32,960 --> 00:24:37,600
is the latency now latency has

00:24:35,600 --> 00:24:40,080
you know various factor uh or the

00:24:37,600 --> 00:24:41,919
various point one is the average latency

00:24:40,080 --> 00:24:44,080
so again they are here the lower is

00:24:41,919 --> 00:24:45,039
better so you can see a dark blue bar is

00:24:44,080 --> 00:24:47,039
lower

00:24:45,039 --> 00:24:48,799
across all the connections and there is

00:24:47,039 --> 00:24:49,840
reduction of the average latency

00:24:48,799 --> 00:24:53,520
approximately

00:24:49,840 --> 00:24:55,120
60 percent or so and it scales well even

00:24:53,520 --> 00:24:55,919
with them as the number of connections

00:24:55,120 --> 00:24:58,720
are increasing

00:24:55,919 --> 00:25:00,799
when you have the adq turned on so that

00:24:58,720 --> 00:25:03,760
that is a second important factor when

00:25:00,799 --> 00:25:07,120
we talk about the performance

00:25:03,760 --> 00:25:09,600
the third one is the predictability okay

00:25:07,120 --> 00:25:10,480
now uh the predictability is probably

00:25:09,600 --> 00:25:12,080
the the

00:25:10,480 --> 00:25:14,559
most important factor for this

00:25:12,080 --> 00:25:18,480
application they are looking for

00:25:14,559 --> 00:25:21,600
predictability is measured by 99.9

00:25:18,480 --> 00:25:22,080
latency so there also you can see across

00:25:21,600 --> 00:25:24,640
the

00:25:22,080 --> 00:25:25,760
all number of connections from 200 to

00:25:24,640 --> 00:25:29,279
00:25:25,760 --> 00:25:32,799
there is a reduction in the latency

00:25:29,279 --> 00:25:34,240
at 99 percentile and the reduction is

00:25:32,799 --> 00:25:36,640
almost like 60 percent

00:25:34,240 --> 00:25:39,039
and this reductions directly translates

00:25:36,640 --> 00:25:41,200
to the predictability so this is a 60

00:25:39,039 --> 00:25:43,679
predictability increase when you have

00:25:41,200 --> 00:25:45,279
adq turned on

00:25:43,679 --> 00:25:47,600
for a given application compared to the

00:25:45,279 --> 00:25:50,000
baseline

00:25:47,600 --> 00:25:50,960
now putting this all together i know

00:25:50,000 --> 00:25:53,919
this one

00:25:50,960 --> 00:25:56,080
one one graph is probably overcrowded

00:25:53,919 --> 00:25:58,720
but the idea of this one is it shows

00:25:56,080 --> 00:25:59,600
throughput as well as it shows the

00:25:58,720 --> 00:26:02,480
latency

00:25:59,600 --> 00:26:03,200
and the x-axis is your total number of

00:26:02,480 --> 00:26:06,320
connections

00:26:03,200 --> 00:26:08,320
so that red line it stocks it's about

00:26:06,320 --> 00:26:10,720
sla service level agreement

00:26:08,320 --> 00:26:11,760
so as you can see when you have the adq

00:26:10,720 --> 00:26:14,720
is turned off

00:26:11,760 --> 00:26:15,919
that red box which says adq off exceeds

00:26:14,720 --> 00:26:19,039
one millisecond

00:26:15,919 --> 00:26:22,400
so when you when the edit is off the sla

00:26:19,039 --> 00:26:24,000
is exceeded at 8 million transactions

00:26:22,400 --> 00:26:26,400
per second

00:26:24,000 --> 00:26:27,760
it means after that one your service

00:26:26,400 --> 00:26:29,279
level agreement is not meet

00:26:27,760 --> 00:26:31,279
and some requests are taking more than

00:26:29,279 --> 00:26:34,880
one millisecond

00:26:31,279 --> 00:26:36,880
whereas what you see is even with adq as

00:26:34,880 --> 00:26:39,120
the throughput is increasing going from

00:26:36,880 --> 00:26:39,600
eight million all the way to 19 million

00:26:39,120 --> 00:26:41,200
here

00:26:39,600 --> 00:26:43,039
the service level agreement is still

00:26:41,200 --> 00:26:44,960
made means the latency is still less

00:26:43,039 --> 00:26:48,640
than 1 millisecond

00:26:44,960 --> 00:26:51,039
as you can see that dark

00:26:48,640 --> 00:26:52,000
line which is just below the red line

00:26:51,039 --> 00:26:53,600
about sla

00:26:52,000 --> 00:26:56,240
that is basically the latency number

00:26:53,600 --> 00:26:59,279
with adq and you can see them they are

00:26:56,240 --> 00:26:59,919
well below the sla so here with the adq

00:26:59,279 --> 00:27:01,919
on

00:26:59,919 --> 00:27:04,000
it's more or less whatever the

00:27:01,919 --> 00:27:07,200
throughput improvement is 140

00:27:04,000 --> 00:27:09,960
compared to 8 million to 19 million

00:27:07,200 --> 00:27:11,520
while meeting the sla as measured by

00:27:09,960 --> 00:27:17,200
99.9

00:27:11,520 --> 00:27:17,200
uh percentile when the adq is turned on

00:27:18,000 --> 00:27:23,600
now at high level the the

00:27:21,120 --> 00:27:25,279
benefits of the adq is yes it improves

00:27:23,600 --> 00:27:26,480
the throughput that's great it reduces

00:27:25,279 --> 00:27:28,559
the latency that's great

00:27:26,480 --> 00:27:29,840
but more than that it increases the

00:27:28,559 --> 00:27:32,080
predictability

00:27:29,840 --> 00:27:33,520
by providing consistent reduction in the

00:27:32,080 --> 00:27:35,600
latency

00:27:33,520 --> 00:27:38,880
and it is scaling all the way to even

00:27:35,600 --> 00:27:38,880
the higher number of connections

00:27:40,240 --> 00:27:44,240
moving on to next slide so we saw those

00:27:42,640 --> 00:27:46,080
where a lot of ingredients

00:27:44,240 --> 00:27:48,480
and in the first few slides we talked

00:27:46,080 --> 00:27:50,480
about hey how do we let uh

00:27:48,480 --> 00:27:52,240
let application do everything basically

00:27:50,480 --> 00:27:54,480
let socket do everything

00:27:52,240 --> 00:27:56,000
and now we have these two two loops

00:27:54,480 --> 00:27:56,880
where you have the kernel based protocol

00:27:56,000 --> 00:27:58,640
implementation

00:27:56,880 --> 00:27:59,919
and we identified some of the system

00:27:58,640 --> 00:28:02,480
inefficiencies

00:27:59,919 --> 00:28:03,520
uh which which was hindering to get to

00:28:02,480 --> 00:28:05,039
that uh

00:28:03,520 --> 00:28:07,600
goal of let the application do

00:28:05,039 --> 00:28:09,919
everything so let's see how we address

00:28:07,600 --> 00:28:11,600
each of those inefficiencies so we

00:28:09,919 --> 00:28:12,080
talked about the interrupts so first

00:28:11,600 --> 00:28:13,440
three

00:28:12,080 --> 00:28:15,200
interrupts context switches and

00:28:13,440 --> 00:28:17,760
synchronization they

00:28:15,200 --> 00:28:18,320
are primarily addressed because we

00:28:17,760 --> 00:28:19,840
introduce

00:28:18,320 --> 00:28:21,679
something called event based busy

00:28:19,840 --> 00:28:23,919
polling and

00:28:21,679 --> 00:28:25,279
we use a socket priority and along with

00:28:23,919 --> 00:28:27,679
that the symmetric queue

00:28:25,279 --> 00:28:28,480
that essentially creates isolation

00:28:27,679 --> 00:28:31,039
between the

00:28:28,480 --> 00:28:32,320
application thread of execution or to

00:28:31,039 --> 00:28:34,559
the hardware queues

00:28:32,320 --> 00:28:35,520
and by providing those dedicated express

00:28:34,559 --> 00:28:37,520
lane

00:28:35,520 --> 00:28:39,120
we are able to avoid synchronization

00:28:37,520 --> 00:28:41,840
means we are able to get to

00:28:39,120 --> 00:28:43,039
the lockless free single producer

00:28:41,840 --> 00:28:45,440
consumer model

00:28:43,039 --> 00:28:46,320
and everything is driving through the

00:28:45,440 --> 00:28:48,960
event both

00:28:46,320 --> 00:28:50,480
event-based busy polling so that reduces

00:28:48,960 --> 00:28:52,559
the interrupt substantially

00:28:50,480 --> 00:28:53,760
and this one eventually helps to reduce

00:28:52,559 --> 00:28:55,520
the context switches

00:28:53,760 --> 00:28:57,279
so first three is primarily addressed

00:28:55,520 --> 00:28:58,960
because of two things event based busy

00:28:57,279 --> 00:29:01,440
polling

00:28:58,960 --> 00:29:02,399
and symmetric queueing which is

00:29:01,440 --> 00:29:04,799
providing that

00:29:02,399 --> 00:29:06,640
which is creating that isolation between

00:29:04,799 --> 00:29:07,039
the application thread of execution and

00:29:06,640 --> 00:29:10,000
the

00:29:07,039 --> 00:29:10,480
device queues the next two uh which is

00:29:10,000 --> 00:29:13,679
about

00:29:10,480 --> 00:29:16,720
how to improve the working set locality

00:29:13,679 --> 00:29:18,080
uh because the in inefficiencies we saw

00:29:16,720 --> 00:29:19,760
the problem about you know

00:29:18,080 --> 00:29:21,120
when you have the kernel based protocol

00:29:19,760 --> 00:29:23,360
implementation it is

00:29:21,120 --> 00:29:25,120
challenging where your application and

00:29:23,360 --> 00:29:25,760
the protocol processing was not really

00:29:25,120 --> 00:29:28,480
happening

00:29:25,760 --> 00:29:29,200
in the same context so here again with

00:29:28,480 --> 00:29:31,919
the

00:29:29,200 --> 00:29:33,279
event-based busy polling uh basically we

00:29:31,919 --> 00:29:35,679
let the application

00:29:33,279 --> 00:29:36,399
decide when when to trigger the busy

00:29:35,679 --> 00:29:38,799
polling

00:29:36,399 --> 00:29:40,159
and now since the busy pooling is

00:29:38,799 --> 00:29:42,000
happening in the context of the

00:29:40,159 --> 00:29:43,600
application the protocol processing is

00:29:42,000 --> 00:29:45,600
naturally happening in the context of

00:29:43,600 --> 00:29:47,760
the application

00:29:45,600 --> 00:29:50,240
and we could do this because now we have

00:29:47,760 --> 00:29:53,200
this dedicated queues

00:29:50,240 --> 00:29:55,200
per application so that we can put the

00:29:53,200 --> 00:29:56,640
owners back onto the application that

00:29:55,200 --> 00:29:58,559
how they want to drain the queue and

00:29:56,640 --> 00:30:00,080
when they want to drain the queue and as

00:29:58,559 --> 00:30:01,840
part of draining the queue the protocol

00:30:00,080 --> 00:30:03,679
processing will naturally happen in the

00:30:01,840 --> 00:30:06,000
context of the application

00:30:03,679 --> 00:30:08,720
and as a result of that your working set

00:30:06,000 --> 00:30:11,360
locality issue is also addressed

00:30:08,720 --> 00:30:11,840
uh with this one we couldn't do earlier

00:30:11,360 --> 00:30:13,279
in that

00:30:11,840 --> 00:30:15,200
when you typically have the shared queue

00:30:13,279 --> 00:30:16,159
model where essentially the kernel

00:30:15,200 --> 00:30:19,200
proxies

00:30:16,159 --> 00:30:20,080
the uh basically the processing of the

00:30:19,200 --> 00:30:22,080
packets

00:30:20,080 --> 00:30:23,600
uh basically the protocol processing

00:30:22,080 --> 00:30:25,039
which is handled by the kernel in case

00:30:23,600 --> 00:30:27,039
of the shared queue concept

00:30:25,039 --> 00:30:29,840
but this is the important property you

00:30:27,039 --> 00:30:31,279
get or one get with the adq where your

00:30:29,840 --> 00:30:34,159
isolated queues

00:30:31,279 --> 00:30:35,200
and uh where the application is

00:30:34,159 --> 00:30:37,600
essentially

00:30:35,200 --> 00:30:38,960
driving the protocol processing but more

00:30:37,600 --> 00:30:41,039
important to that one

00:30:38,960 --> 00:30:42,720
now since it is applica happening in

00:30:41,039 --> 00:30:44,159
application context the application

00:30:42,720 --> 00:30:47,200
context behavior

00:30:44,159 --> 00:30:49,279
is truly reflected in the wire behavior

00:30:47,200 --> 00:30:50,640
which otherwise doesn't happen if you

00:30:49,279 --> 00:30:52,720
are going through the kernel base

00:30:50,640 --> 00:30:53,679
uh basically where the kernel is

00:30:52,720 --> 00:30:55,679
processing the

00:30:53,679 --> 00:30:57,519
protocol processing for you on your

00:30:55,679 --> 00:30:59,679
behalf so this is a

00:30:57,519 --> 00:31:00,960
most important property and this this

00:30:59,679 --> 00:31:03,200
look goes back to

00:31:00,960 --> 00:31:05,279
that internet stability that one loop to

00:31:03,200 --> 00:31:07,679
two loop right where we want

00:31:05,279 --> 00:31:08,720
the application behavior to be reflected

00:31:07,679 --> 00:31:11,600
on the wire

00:31:08,720 --> 00:31:12,399
and the third as as a result of that we

00:31:11,600 --> 00:31:15,519
also

00:31:12,399 --> 00:31:18,000
reduce the data moment uh which because

00:31:15,519 --> 00:31:20,000
of the single producer consumer model

00:31:18,000 --> 00:31:22,399
it's all happening in the context of the

00:31:20,000 --> 00:31:24,320
application and there is no data moment

00:31:22,399 --> 00:31:26,159
happen during the packet processing so

00:31:24,320 --> 00:31:27,039
working such locality issue is also

00:31:26,159 --> 00:31:30,159
addressed

00:31:27,039 --> 00:31:30,159
as part of the adq

00:31:30,399 --> 00:31:35,039
now we saw those uh uh performance

00:31:33,279 --> 00:31:37,760
number and how we address the system

00:31:35,039 --> 00:31:40,559
inefficiencies but what is next really

00:31:37,760 --> 00:31:42,480
so obviously there is a lot has been

00:31:40,559 --> 00:31:45,919
done and there is a lot more to be done

00:31:42,480 --> 00:31:47,360
but at high level uh today we can uh we

00:31:45,919 --> 00:31:50,399
use this uh tc

00:31:47,360 --> 00:31:51,840
based approach to basically configure

00:31:50,399 --> 00:31:53,679
the application and accelerate

00:31:51,840 --> 00:31:55,279
that that has a limit it is a limit of

00:31:53,679 --> 00:31:56,720
16 as it stands now

00:31:55,279 --> 00:31:58,320
so with that approach we could

00:31:56,720 --> 00:32:01,360
essentially accelerate only

00:31:58,320 --> 00:32:02,960
15 adq applications so that is one of

00:32:01,360 --> 00:32:06,000
the limiting factor

00:32:02,960 --> 00:32:08,080
the second one is uh it

00:32:06,000 --> 00:32:09,840
it has a limited number of limited

00:32:08,080 --> 00:32:11,519
configurability such as we can do

00:32:09,840 --> 00:32:13,279
bandwidth limit configuration and what

00:32:11,519 --> 00:32:14,159
not for application here we refer

00:32:13,279 --> 00:32:17,519
application means

00:32:14,159 --> 00:32:19,039
per cue set means per tc but we

00:32:17,519 --> 00:32:21,840
we want to do more and more such

00:32:19,039 --> 00:32:23,840
configurability one just example is the

00:32:21,840 --> 00:32:25,519
uh the interrupt setting today we have

00:32:23,840 --> 00:32:27,440
mechanism from it tool where you can

00:32:25,519 --> 00:32:28,000
configure for a given network device but

00:32:27,440 --> 00:32:30,240
it is still

00:32:28,000 --> 00:32:31,039
not for tc so if you want to provide

00:32:30,240 --> 00:32:33,440
such an

00:32:31,039 --> 00:32:34,320
such more configurability option we have

00:32:33,440 --> 00:32:36,559
to look

00:32:34,320 --> 00:32:38,960
uh how it can be either extended or do

00:32:36,559 --> 00:32:42,080
we need a different tool or whatnot

00:32:38,960 --> 00:32:44,159
and the third one is basically today

00:32:42,080 --> 00:32:47,679
we have single interface where to

00:32:44,159 --> 00:32:50,640
configure both tx and rx queues

00:32:47,679 --> 00:32:52,799
uh dc natively the basically tc doesn't

00:32:50,640 --> 00:32:56,000
support rxq configuration

00:32:52,799 --> 00:32:57,919
so we need to look at it do we need to

00:32:56,000 --> 00:32:59,919
have uh

00:32:57,919 --> 00:33:01,600
basically yet another interface

00:32:59,919 --> 00:33:04,799
something like tc

00:33:01,600 --> 00:33:08,080
but uh not be limited by the 16 and

00:33:04,799 --> 00:33:10,320
provides more configurability uh

00:33:08,080 --> 00:33:12,799
that's really the next step and i'm sure

00:33:10,320 --> 00:33:14,799
uh as we go along we will have more and

00:33:12,799 --> 00:33:15,919
more use cases which we can try to

00:33:14,799 --> 00:33:20,960
leverage adq

00:33:15,919 --> 00:33:20,960
and optimize it as we go along

00:33:22,559 --> 00:33:28,559
most uh so to prepare this presentation

00:33:25,679 --> 00:33:30,559
we use a lot of the material and here

00:33:28,559 --> 00:33:33,039
are the references listed out

00:33:30,559 --> 00:33:35,200
uh there is a documentation in the about

00:33:33,039 --> 00:33:37,840
the xps using receive queues

00:33:35,200 --> 00:33:38,480
and what not uh likewise there is a

00:33:37,840 --> 00:33:40,240
similar

00:33:38,480 --> 00:33:43,039
or likewise presentation was done last

00:33:40,240 --> 00:33:45,519
year at the storage developer conference

00:33:43,039 --> 00:33:46,799
and there is a adq configuration guide

00:33:45,519 --> 00:33:52,320
and demo

00:33:46,799 --> 00:33:54,720
kit available at that link

00:33:52,320 --> 00:33:57,679
uh with this one i will open it up for

00:33:54,720 --> 00:33:57,679
the question and answer

00:33:59,120 --> 00:34:07,840
thank you

00:34:25,919 --> 00:34:30,399
okay i thank the speakers and i said

00:34:28,800 --> 00:34:32,639
we're already behind schedule

00:34:30,399 --> 00:34:34,240
it's uh we're already five minutes past

00:34:32,639 --> 00:34:37,679
q a

00:34:34,240 --> 00:34:38,399
uh so i'm i'm going to take advantage of

00:34:37,679 --> 00:34:41,520
being the host

00:34:38,399 --> 00:34:44,639
to ask the first question okay uh

00:34:41,520 --> 00:34:46,240
it seems to me and this hasn't changed

00:34:44,639 --> 00:34:50,079
since last i looked at um

00:34:46,240 --> 00:34:50,960
at adq that it requires application

00:34:50,079 --> 00:34:54,000
changes

00:34:50,960 --> 00:34:55,359
right it is that well uh

00:34:54,000 --> 00:34:57,680
why not take advantage of things like

00:34:55,359 --> 00:34:59,200
arfs and

00:34:57,680 --> 00:35:00,800
things of that data people don't like to

00:34:59,200 --> 00:35:02,720
go and change their applications

00:35:00,800 --> 00:35:03,920
i mean you'll you'll find some niche

00:35:02,720 --> 00:35:06,400
users maybe that

00:35:03,920 --> 00:35:06,960
would be more than happy to start using

00:35:06,400 --> 00:35:10,400
your

00:35:06,960 --> 00:35:13,680
uh get stock opts or set stock ups

00:35:10,400 --> 00:35:15,280
but in general uh if i could just run

00:35:13,680 --> 00:35:17,280
memcached without making any changes

00:35:15,280 --> 00:35:20,000
that would be great

00:35:17,280 --> 00:35:20,000
yes that's fine

00:35:21,359 --> 00:35:24,640
that's right what you said is true but

00:35:22,880 --> 00:35:28,000
that application change

00:35:24,640 --> 00:35:29,920
is only uh basically is only

00:35:28,000 --> 00:35:32,880
needed if you want the best and optimal

00:35:29,920 --> 00:35:36,240
performance you can still use the arfs

00:35:32,880 --> 00:35:38,240
with adq and run the unmodified memcache

00:35:36,240 --> 00:35:40,560
you will still get the benefits of

00:35:38,240 --> 00:35:41,520
you know what the adq filtering the busy

00:35:40,560 --> 00:35:43,599
polling

00:35:41,520 --> 00:35:44,960
and symmetric queues so the performance

00:35:43,599 --> 00:35:47,440
mileage will vary

00:35:44,960 --> 00:35:48,079
it you may not get the best performance

00:35:47,440 --> 00:35:50,320
uh and

00:35:48,079 --> 00:35:51,440
but you can still use the arfs if you

00:35:50,320 --> 00:35:54,720
wish to

00:35:51,440 --> 00:35:54,720
do did you have any numbers here

00:35:55,760 --> 00:36:00,480
not in this slide set but we do have

00:35:58,800 --> 00:36:02,480
internal comparison done say if you

00:36:00,480 --> 00:36:02,800
don't run the unmodified memcache as it

00:36:02,480 --> 00:36:04,960
is

00:36:02,800 --> 00:36:06,720
right and you use all the other

00:36:04,960 --> 00:36:07,440
ingredients of the adq which we talked

00:36:06,720 --> 00:36:09,440
about

00:36:07,440 --> 00:36:10,560
you still get the benefits you still get

00:36:09,440 --> 00:36:13,680
the benefits and

00:36:10,560 --> 00:36:14,640
they're like more affected at 10 to 15

00:36:13,680 --> 00:36:18,800
percent you know

00:36:14,640 --> 00:36:21,119
that's a delta we observe morally but

00:36:18,800 --> 00:36:23,119
to get the best performance where you

00:36:21,119 --> 00:36:25,520
have the single producer and consumer

00:36:23,119 --> 00:36:26,480
and then use the busy polling that's

00:36:25,520 --> 00:36:28,560
where you

00:36:26,480 --> 00:36:31,200
try to see the tail latencies are not of

00:36:28,560 --> 00:36:33,200
the chart your p99 p999

00:36:31,200 --> 00:36:34,880
and that's where it it really shows up

00:36:33,200 --> 00:36:37,599
so the predictability

00:36:34,880 --> 00:36:38,240
is much better if you make that small

00:36:37,599 --> 00:36:40,800
change

00:36:38,240 --> 00:36:42,400
but and again it's for memcache but

00:36:40,800 --> 00:36:44,640
there are certain class of application

00:36:42,400 --> 00:36:47,520
where you don't need to make any change

00:36:44,640 --> 00:36:49,280
like nginx if you use the standard in

00:36:47,520 --> 00:36:51,200
kernel load balancer technique of

00:36:49,280 --> 00:36:54,800
distributing connections to thread

00:36:51,200 --> 00:36:56,640
uh you don't modify an application

00:36:54,800 --> 00:36:58,960
okay maybe i'll define this too later on

00:36:56,640 --> 00:37:01,359
if you can show up but happy hour maybe

00:36:58,960 --> 00:37:03,200
show some numbers or it could be uh it

00:37:01,359 --> 00:37:07,040
could be an interesting topic to discuss

00:37:03,200 --> 00:37:08,000
um okay i'll squeeze in one more

00:37:07,040 --> 00:37:11,119
question for me

00:37:08,000 --> 00:37:12,880
uh or or a comment on your last the next

00:37:11,119 --> 00:37:13,920
steps there for the tcq so it sounds

00:37:12,880 --> 00:37:16,480
like devlink

00:37:13,920 --> 00:37:19,359
that was given out yesterday is uh maybe

00:37:16,480 --> 00:37:21,359
a good solution for that

00:37:19,359 --> 00:37:23,599
uh maybe we had to look at it more

00:37:21,359 --> 00:37:25,520
carefully uh how we can

00:37:23,599 --> 00:37:27,520
do the configuration with that and now

00:37:25,520 --> 00:37:31,200
i'm going to start to read questions

00:37:27,520 --> 00:37:31,680
from uh the chat here so the first one

00:37:31,200 --> 00:37:35,440
is for

00:37:31,680 --> 00:37:37,680
marcelo uh is it considering the tc

00:37:35,440 --> 00:37:40,160
example you gave

00:37:37,680 --> 00:37:40,960
can you easily add and remove queues on

00:37:40,160 --> 00:37:42,400
demand

00:37:40,960 --> 00:37:44,480
for example when a container starts or

00:37:42,400 --> 00:37:47,200
dies

00:37:44,480 --> 00:37:47,760
no so tc in general is like kind of the

00:37:47,200 --> 00:37:51,280
one time

00:37:47,760 --> 00:37:52,960
shot it doesn't allow dynamically adding

00:37:51,280 --> 00:37:54,240
or changing the queue layout if you are

00:37:52,960 --> 00:37:56,560
changing it you are essentially

00:37:54,240 --> 00:37:58,320
reconfiguring the whole network device

00:37:56,560 --> 00:37:59,839
okay so this is basically that uh thing

00:37:58,320 --> 00:38:02,800
that you said next steps

00:37:59,839 --> 00:38:03,680
right yeah yeah okay so so i i figured

00:38:02,800 --> 00:38:05,760
that maybe devlin

00:38:03,680 --> 00:38:07,680
could be a candidate there next question

00:38:05,760 --> 00:38:09,119
is from boris

00:38:07,680 --> 00:38:11,440
what happens to traffic that's already

00:38:09,119 --> 00:38:12,880
directed to these queues does it require

00:38:11,440 --> 00:38:16,160
closing and opening of all the channels

00:38:12,880 --> 00:38:16,160
and queues to remap traffic

00:38:17,359 --> 00:38:21,200
so i think it's probably the along the

00:38:19,520 --> 00:38:22,960
similar line the question once you have

00:38:21,200 --> 00:38:24,079
the adq configure and if there is active

00:38:22,960 --> 00:38:26,160
traffic going on

00:38:24,079 --> 00:38:27,680
and you want to change something then

00:38:26,160 --> 00:38:30,400
yes in that case

00:38:27,680 --> 00:38:31,280
the expectation will be kind of close

00:38:30,400 --> 00:38:34,079
the application

00:38:31,280 --> 00:38:35,440
reconfigure your device and kind of uh

00:38:34,079 --> 00:38:37,119
re-initiate

00:38:35,440 --> 00:38:39,280
okay so so there's a disconnect between

00:38:37,119 --> 00:38:40,480
the app you can close the app but the

00:38:39,280 --> 00:38:41,680
packets will still be queued on these

00:38:40,480 --> 00:38:44,400
queues right

00:38:41,680 --> 00:38:45,839
right yes you have to reconfigure you

00:38:44,400 --> 00:38:48,480
have to reconfigure it's

00:38:45,839 --> 00:38:49,920
okay i don't know if that answered uh

00:38:48,480 --> 00:38:51,599
boris's question but um

00:38:49,920 --> 00:38:53,359
we don't have much time we can discuss

00:38:51,599 --> 00:38:57,599
this during happy hour

00:38:53,359 --> 00:38:58,079
um am i skipping i'm just gonna look for

00:38:57,599 --> 00:38:59,599
some

00:38:58,079 --> 00:39:01,119
someone who followed the protocol we sat

00:38:59,599 --> 00:39:04,400
with the q q

00:39:01,119 --> 00:39:05,040
q okay here maxim uh as far as i

00:39:04,400 --> 00:39:07,920
understand

00:39:05,040 --> 00:39:08,640
the adq solution you in the adq solution

00:39:07,920 --> 00:39:10,640
you tie the

00:39:08,640 --> 00:39:12,079
rxq processing to the application call

00:39:10,640 --> 00:39:14,079
by doing busy power

00:39:12,079 --> 00:39:16,000
and manual configuration of cube groups

00:39:14,079 --> 00:39:18,560
with tcmq prior and steering

00:39:16,000 --> 00:39:20,320
with tc flower as needed what are the

00:39:18,560 --> 00:39:21,839
advantages of such a product compared to

00:39:20,320 --> 00:39:23,760
i guess this is almost my question

00:39:21,839 --> 00:39:26,720
compared to arfs

00:39:23,760 --> 00:39:27,040
which also ensures that rxq processing

00:39:26,720 --> 00:39:28,720
and

00:39:27,040 --> 00:39:30,320
application processing run on the same

00:39:28,720 --> 00:39:31,119
cpu core and doesn't require manual

00:39:30,320 --> 00:39:34,480
configuration

00:39:31,119 --> 00:39:38,320
is is adq fully

00:39:34,480 --> 00:39:39,119
upstream adq is uh all the kernel bits

00:39:38,320 --> 00:39:41,599
are upstream

00:39:39,119 --> 00:39:43,200
the we don't plan to any change anything

00:39:41,599 --> 00:39:44,160
else the driver bits are coming there in

00:39:43,200 --> 00:39:46,880
the pipeline

00:39:44,160 --> 00:39:47,359
but to answer to that question that arfs

00:39:46,880 --> 00:39:50,480
is

00:39:47,359 --> 00:39:52,240
it's solving the problem it doesn't

00:39:50,480 --> 00:39:53,920
solve the problem about the first level

00:39:52,240 --> 00:39:56,000
of filter for the given application

00:39:53,920 --> 00:39:57,040
adq is trying to solve that it provides

00:39:56,000 --> 00:39:59,280
uh

00:39:57,040 --> 00:40:02,880
the first level of isolation in the

00:39:59,280 --> 00:40:05,760
hardware for a given application

00:40:02,880 --> 00:40:07,200
okay uh maybe i'll i'm sorry for

00:40:05,760 --> 00:40:08,640
everybody who's been asking a lot of

00:40:07,200 --> 00:40:11,200
questions here okay

00:40:08,640 --> 00:40:12,720
you get an opportunity did i miss how

00:40:11,200 --> 00:40:14,240
rxq

00:40:12,720 --> 00:40:16,319
okay you want to you want to talk yeah

00:40:14,240 --> 00:40:17,839
so i can summarize and i think we are

00:40:16,319 --> 00:40:20,000
running out of time right so

00:40:17,839 --> 00:40:21,119
uh firstly i'm i'm a little confused

00:40:20,000 --> 00:40:22,400
because flow

00:40:21,119 --> 00:40:24,079
if you're trying to solve the net

00:40:22,400 --> 00:40:24,800
channel design you have to have per

00:40:24,079 --> 00:40:27,200
application

00:40:24,800 --> 00:40:28,560
control which basically means either

00:40:27,200 --> 00:40:31,040
you're doing flow director or something

00:40:28,560 --> 00:40:33,359
else rss is not a sufficient answer

00:40:31,040 --> 00:40:35,680
at that point you are fundamentally in

00:40:33,359 --> 00:40:38,480
the same spot as airfs and

00:40:35,680 --> 00:40:40,560
flow director and rfs last i checked ice

00:40:38,480 --> 00:40:43,040
and i40e does not have

00:40:40,560 --> 00:40:44,160
an upstream a rfs support so i unless

00:40:43,040 --> 00:40:46,720
you guys are doing

00:40:44,160 --> 00:40:47,920
yeah so uh it's implemented it's in the

00:40:46,720 --> 00:40:50,160
pipeline so it's

00:40:47,920 --> 00:40:50,160
okay

00:40:51,040 --> 00:40:54,720
and it would be um interesting to see

00:40:52,880 --> 00:40:56,000
why this is in any way different because

00:40:54,720 --> 00:40:56,640
if you're in the kernel and you're not

00:40:56,000 --> 00:40:58,160
running

00:40:56,640 --> 00:40:59,760
a user space stack like what net

00:40:58,160 --> 00:41:01,680
channels was doing

00:40:59,760 --> 00:41:03,200
i i'm actually having difficulty

00:41:01,680 --> 00:41:04,560
understanding mechanically

00:41:03,200 --> 00:41:06,240
what did you skip because you're going

00:41:04,560 --> 00:41:09,280
to go through the same

00:41:06,240 --> 00:41:10,319
process interrupt send signal give tcp

00:41:09,280 --> 00:41:12,800
completion

00:41:10,319 --> 00:41:14,240
give socket wake up loop all you're

00:41:12,800 --> 00:41:15,440
going to do is separate into queues

00:41:14,240 --> 00:41:17,599
which is what rfs is

00:41:15,440 --> 00:41:19,119
already capable of doing so i'm missing

00:41:17,599 --> 00:41:21,359
something very fundamental

00:41:19,119 --> 00:41:23,359
yeah so there are additional driver

00:41:21,359 --> 00:41:23,920
level optimization srijit which will

00:41:23,359 --> 00:41:25,760
come

00:41:23,920 --> 00:41:27,280
as the driver gets more and more

00:41:25,760 --> 00:41:29,119
upstream and

00:41:27,280 --> 00:41:31,200
they are one of the catalysts for

00:41:29,119 --> 00:41:32,960
improving the performance

00:41:31,200 --> 00:41:39,359
and we are always on the lookout of

00:41:32,960 --> 00:41:39,359

YouTube URL: https://www.youtube.com/watch?v=NWgXJwyKrqs


