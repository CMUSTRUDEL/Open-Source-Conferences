Title: Netdev 0x14 - Performance improvements for NVMe TCP
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Sagi Grimberg

More info: https://netdevconf.info/0x14/session.html?talk-performance-improvements-for-NVMe-TCP

Date: Wednesday, August 19, 2020

In this talk Sagi Grimberg will look back at the journey
and experience gained so far and set the context for the talk.
He will then build on that context to describe various experiments
conducted to measure performance  - more importantly the latency
achieved with current kernel NVMe/TCP. Part of the experiments
will explore different NIC vendors' features (Intel ADQ,
Mellanox aRFS, etc) that can be put to bear on behalf of
improving overall performance and latency.

And last, Sagi hopes to create a discussion based on some
suggestions on what he believes are useful enhancements that
can be made to the driver.
Captions: 
	00:00:03,199 --> 00:00:07,520
so the topic

00:00:04,080 --> 00:00:10,320
today is basically um i'm going to cover

00:00:07,520 --> 00:00:11,360
some of the performance uh improvements

00:00:10,320 --> 00:00:14,880
that were done

00:00:11,360 --> 00:00:18,480
um uh in the past

00:00:14,880 --> 00:00:19,600
in the past uh six months to a year i

00:00:18,480 --> 00:00:22,560
think

00:00:19,600 --> 00:00:23,760
um that uh people are starting to sort

00:00:22,560 --> 00:00:26,800
of look deeper into

00:00:23,760 --> 00:00:30,320
nvme or tcp and understand how they can

00:00:26,800 --> 00:00:33,520
optimize it further we're going to cover

00:00:30,320 --> 00:00:36,880
um a bit of that or um

00:00:33,520 --> 00:00:38,879
at least in high level um

00:00:36,880 --> 00:00:40,160
um in case i didn't introduce myself

00:00:38,879 --> 00:00:42,160
sage grimberg

00:00:40,160 --> 00:00:44,000
i work for light bits where i act as a

00:00:42,160 --> 00:00:45,920
cto i'm also a co

00:00:44,000 --> 00:00:47,600
one of the maintainers of the nvme

00:00:45,920 --> 00:00:51,039
subsystem the linux kernel

00:00:47,600 --> 00:00:54,079
there are three of us um

00:00:51,039 --> 00:00:56,960
and um that's it basically

00:00:54,079 --> 00:00:58,079
happy to be here so i'm gonna start with

00:00:56,960 --> 00:01:00,480
a short intro

00:00:58,079 --> 00:01:03,680
if you didn't happen to hear nvme over

00:01:00,480 --> 00:01:05,920
tcp and former

00:01:03,680 --> 00:01:07,119
talks that were here today or before

00:01:05,920 --> 00:01:10,159
that

00:01:07,119 --> 00:01:11,760
nvme over tcp is basically just the

00:01:10,159 --> 00:01:15,439
standard transport binding

00:01:11,760 --> 00:01:18,159
to basically network nvme protocol

00:01:15,439 --> 00:01:21,840
and and the queueing interface on top of

00:01:18,159 --> 00:01:21,840
a standard tcp network

00:01:22,080 --> 00:01:25,520
basically it's the same nvme multi-q

00:01:24,640 --> 00:01:28,640
interface

00:01:25,520 --> 00:01:30,000
in the linux case it runs just on top of

00:01:28,640 --> 00:01:35,200
tcp

00:01:30,000 --> 00:01:35,200
sockets nothing very fancy around that

00:01:35,680 --> 00:01:40,159
in terms of the interface you know from

00:01:38,320 --> 00:01:42,399
the spec perspective um

00:01:40,159 --> 00:01:43,200
it's really transparent into the you

00:01:42,399 --> 00:01:45,280
know the core

00:01:43,200 --> 00:01:46,240
command set so it's basically exactly

00:01:45,280 --> 00:01:49,680
the same

00:01:46,240 --> 00:01:54,720
what nvme over tcp does is basically um

00:01:49,680 --> 00:01:57,520
encapsulates um nvme commands or data

00:01:54,720 --> 00:01:58,640
into what we call nvme over tcp pdus

00:01:57,520 --> 00:02:01,200
which

00:01:58,640 --> 00:02:03,200
stands for protocol data units if you

00:02:01,200 --> 00:02:04,000
look at the layering basically nvme is

00:02:03,200 --> 00:02:07,759
the high level

00:02:04,000 --> 00:02:10,879
architecture queuing interface

00:02:07,759 --> 00:02:14,160
and command sets nvme over fabrics

00:02:10,879 --> 00:02:14,800
sort of took nvme and um expanded it to

00:02:14,160 --> 00:02:17,520
something that

00:02:14,800 --> 00:02:19,280
doesn't have you know pcie hardly bound

00:02:17,520 --> 00:02:23,760
into the into the concept of

00:02:19,280 --> 00:02:25,760
of the of the overall nvme architecture

00:02:23,760 --> 00:02:26,800
and the transport binding part that's

00:02:25,760 --> 00:02:30,480
where nvme

00:02:26,800 --> 00:02:33,920
uh tcp sort of sits basically translates

00:02:30,480 --> 00:02:37,519
into uh the encapsulation and

00:02:33,920 --> 00:02:40,239
and uh and mapping um between the

00:02:37,519 --> 00:02:42,800
transport or network channel to the

00:02:40,239 --> 00:02:44,959
concept of an nvme queue

00:02:42,800 --> 00:02:46,080
below that there's the nvme transport

00:02:44,959 --> 00:02:48,640
itself and

00:02:46,080 --> 00:02:49,360
nvme tcp case that will be tcpap of

00:02:48,640 --> 00:02:52,560
course

00:02:49,360 --> 00:02:56,000
um you know

00:02:52,560 --> 00:02:59,120
just uh to make it one what

00:02:56,000 --> 00:03:01,519
equivalent to on on other transports

00:02:59,120 --> 00:03:05,840
that could be an rdma based transport or

00:03:01,519 --> 00:03:05,840
fiber channel that also exists

00:03:06,400 --> 00:03:13,120
um in terms of the implementation

00:03:10,000 --> 00:03:15,680
in linux basically

00:03:13,120 --> 00:03:17,920
every nvme q pair is mapped into a

00:03:15,680 --> 00:03:19,599
bi-directional tcp socket we don't sort

00:03:17,920 --> 00:03:22,640
of use two sockets for

00:03:19,599 --> 00:03:25,440
you know submission completion uh we use

00:03:22,640 --> 00:03:28,879
a single tcp socket for that

00:03:25,440 --> 00:03:30,799
um command

00:03:28,879 --> 00:03:33,040
commands and data transfers and if we

00:03:30,799 --> 00:03:34,959
look at

00:03:33,040 --> 00:03:36,959
at the drawing here we have the host on

00:03:34,959 --> 00:03:39,440
the left side and the controller

00:03:36,959 --> 00:03:40,720
um you know from the host perspective

00:03:39,440 --> 00:03:41,760
there's a submission queue and a

00:03:40,720 --> 00:03:45,599
completion queue

00:03:41,760 --> 00:03:46,000
this sort of uh our queueing interface

00:03:45,599 --> 00:03:48,319
from the

00:03:46,000 --> 00:03:50,400
core layer or from you know higher level

00:03:48,319 --> 00:03:51,680
block layer of other urps into the

00:03:50,400 --> 00:03:53,840
driver itself

00:03:51,680 --> 00:03:56,400
basically submit it through through a

00:03:53,840 --> 00:03:58,480
queue or a virtual queue i would say

00:03:56,400 --> 00:03:59,599
and then we have basically an i o thread

00:03:58,480 --> 00:04:02,159
that sort of uh

00:03:59,599 --> 00:04:02,959
is either triggered from this whole

00:04:02,159 --> 00:04:06,239
software

00:04:02,959 --> 00:04:07,760
you know the request cued by um

00:04:06,239 --> 00:04:09,439
you know either the user or the file

00:04:07,760 --> 00:04:12,080
system um

00:04:09,439 --> 00:04:14,080
or it gets triggered from from network

00:04:12,080 --> 00:04:16,079
if it has you know sk buffs to

00:04:14,080 --> 00:04:17,440
sort of crunch from the network or it

00:04:16,079 --> 00:04:21,199
has some space

00:04:17,440 --> 00:04:24,400
it has a um space that's

00:04:21,199 --> 00:04:26,320
uh sort of um available in the socket or

00:04:24,400 --> 00:04:28,960
the socket has a state change that you

00:04:26,320 --> 00:04:32,080
know it needs to go into recover stuff

00:04:28,960 --> 00:04:34,560
for whatever reason or clean up

00:04:32,080 --> 00:04:35,199
so this sort of i o thread it's a single

00:04:34,560 --> 00:04:37,919
thread that

00:04:35,199 --> 00:04:39,759
you know in in the general case it will

00:04:37,919 --> 00:04:41,759
do a send receive we don't use different

00:04:39,759 --> 00:04:42,639
threads to do send and receives it's

00:04:41,759 --> 00:04:45,759
going to do a

00:04:42,639 --> 00:04:47,840
in a loop the implementation

00:04:45,759 --> 00:04:50,960
specifically is a bound work you

00:04:47,840 --> 00:04:52,880
um i use the bound work you initially

00:04:50,960 --> 00:04:55,040
just because it's a it's a very

00:04:52,880 --> 00:04:57,360
convenient interface you know you can

00:04:55,040 --> 00:04:59,120
basically it has clear semantics uh

00:04:57,360 --> 00:05:02,160
on how the cue work you can sort of

00:04:59,120 --> 00:05:04,880
affinitize it to a cpu core if you want

00:05:02,160 --> 00:05:07,280
you can easily cancel work you can you

00:05:04,880 --> 00:05:09,600
can fl you can drain a work you

00:05:07,280 --> 00:05:10,639
um so it's very convenient if you want

00:05:09,600 --> 00:05:14,560
to sort of

00:05:10,639 --> 00:05:17,120
uh handle uh errors and whatnot

00:05:14,560 --> 00:05:18,320
correctly quiesce io it has very clear

00:05:17,120 --> 00:05:21,360
semantics

00:05:18,320 --> 00:05:24,880
um i think that you know if we're uh

00:05:21,360 --> 00:05:28,320
digging deeper and looking into

00:05:24,880 --> 00:05:29,600
pro pro improving the performance of the

00:05:28,320 --> 00:05:32,720
driver

00:05:29,600 --> 00:05:35,840
maybe you know lightweight k threads um

00:05:32,720 --> 00:05:37,919
or a different interface that is

00:05:35,840 --> 00:05:39,840
less heavy weight still but still

00:05:37,919 --> 00:05:44,080
guarantees some of the

00:05:39,840 --> 00:05:45,919
work you semantics that we rely on for

00:05:44,080 --> 00:05:48,160
example for a work item not to be

00:05:45,919 --> 00:05:50,720
reentrant

00:05:48,160 --> 00:05:53,280
that could boost also performance and

00:05:50,720 --> 00:05:56,880
improve latency quite a bit

00:05:53,280 --> 00:05:57,759
um basically this whole picture of uh

00:05:56,880 --> 00:05:59,520
you know having

00:05:57,759 --> 00:06:01,120
you know a queuing interface and a

00:05:59,520 --> 00:06:04,000
completion context uh

00:06:01,120 --> 00:06:06,000
from usually from driven from the user

00:06:04,000 --> 00:06:09,520
context itself

00:06:06,000 --> 00:06:13,680
or file system or whoever

00:06:09,520 --> 00:06:14,080
issued the i o we have that context we

00:06:13,680 --> 00:06:17,039
have

00:06:14,080 --> 00:06:18,160
a general context that is uh the the i o

00:06:17,039 --> 00:06:21,840
thread itself that

00:06:18,160 --> 00:06:21,840
handles the network mostly

00:06:22,240 --> 00:06:25,919
this is sort of on a per q basis and by

00:06:25,280 --> 00:06:29,520
default

00:06:25,919 --> 00:06:32,479
usually um that will be sort of

00:06:29,520 --> 00:06:33,199
replicated on a per core basis or you

00:06:32,479 --> 00:06:36,720
have a

00:06:33,199 --> 00:06:38,160
multi-core system each cpu core will

00:06:36,720 --> 00:06:42,000
have its own dedicated

00:06:38,160 --> 00:06:46,400
queue and dedicated tcp socket

00:06:42,000 --> 00:06:46,400
so in terms of all the

00:06:47,280 --> 00:06:51,039
scaling it should scale fairly well

00:06:50,240 --> 00:06:53,360
depending uh

00:06:51,039 --> 00:06:55,039
of course on on stuff that we'll get

00:06:53,360 --> 00:06:56,880
into

00:06:55,039 --> 00:06:58,720
so if we look at you know what are the

00:06:56,880 --> 00:07:00,800
latency contributions that were

00:06:58,720 --> 00:07:04,560
identified and sort of

00:07:00,800 --> 00:07:06,479
raised in in recent times we can see

00:07:04,560 --> 00:07:07,919
uh different aspects first of all is

00:07:06,479 --> 00:07:11,919
serialization

00:07:07,919 --> 00:07:12,639
so i mean nvme um over tcp was such a

00:07:11,919 --> 00:07:15,360
heavy

00:07:12,639 --> 00:07:16,720
uh a big lift from you know scuzzy based

00:07:15,360 --> 00:07:19,120
protocols that you

00:07:16,720 --> 00:07:20,240
have a really native sort of multi-q

00:07:19,120 --> 00:07:22,080
interface so civil

00:07:20,240 --> 00:07:23,360
serialization is pretty lightweight it's

00:07:22,080 --> 00:07:26,880
only on a

00:07:23,360 --> 00:07:28,479
um on a per-cube basis and you know all

00:07:26,880 --> 00:07:30,639
throughout the stack

00:07:28,479 --> 00:07:32,479
from the block layer and down downwards

00:07:30,639 --> 00:07:35,599
we put a lot of emphasis to not

00:07:32,479 --> 00:07:39,520
have sort of uh global um

00:07:35,599 --> 00:07:41,680
global wide serialization points um

00:07:39,520 --> 00:07:43,599
but there are some serializations that

00:07:41,680 --> 00:07:45,360
are on a per-cube basis

00:07:43,599 --> 00:07:46,639
we're going to talk about that you need

00:07:45,360 --> 00:07:49,440
to serialize the

00:07:46,639 --> 00:07:51,120
the hardware context that you're

00:07:49,440 --> 00:07:53,199
processing

00:07:51,120 --> 00:07:55,039
each hardware context or hardware queue

00:07:53,199 --> 00:07:58,319
has a dedicated socket which is

00:07:55,039 --> 00:08:01,520
also sort of needs to be serialized

00:07:58,319 --> 00:08:02,639
um context switching um i think that the

00:08:01,520 --> 00:08:05,120
cornell

00:08:02,639 --> 00:08:06,080
team has talked about that before we

00:08:05,120 --> 00:08:08,400
have um

00:08:06,080 --> 00:08:09,680
two at a minimum if we're talking about

00:08:08,400 --> 00:08:11,759
low q depth

00:08:09,680 --> 00:08:14,160
with high q depths could be slightly

00:08:11,759 --> 00:08:18,639
amortized but still

00:08:14,160 --> 00:08:21,759
we have two at a minimum per i o

00:08:18,639 --> 00:08:21,759
memory copy so

00:08:21,840 --> 00:08:26,560
it's only on rx all the send operation

00:08:24,639 --> 00:08:30,319
antiques operations are

00:08:26,560 --> 00:08:32,159
zero copy on rx not a huge contributor

00:08:30,319 --> 00:08:32,560
to latency from what we see at least

00:08:32,159 --> 00:08:35,919
from

00:08:32,560 --> 00:08:39,279
you know a generic x86

00:08:35,919 --> 00:08:42,240
system that are usually usually

00:08:39,279 --> 00:08:43,599
tend to get bigger if they hosting you

00:08:42,240 --> 00:08:44,640
know something like a database or

00:08:43,599 --> 00:08:47,600
whatnot

00:08:44,640 --> 00:08:48,800
um but in high load it it's it's uh it

00:08:47,600 --> 00:08:52,160
can get

00:08:48,800 --> 00:08:54,959
visible for sure interrupts

00:08:52,160 --> 00:08:56,320
uh definitely impactful um the

00:08:54,959 --> 00:08:58,000
proliferation or

00:08:56,320 --> 00:09:00,160
you know once the load starts to build

00:08:58,000 --> 00:09:03,600
up um interrupts and

00:09:00,160 --> 00:09:06,000
uh and context switches together um

00:09:03,600 --> 00:09:08,959
make uh definitely an impact you know

00:09:06,000 --> 00:09:10,640
you have receive offloads or adoptive

00:09:08,959 --> 00:09:13,440
adaptive interrupt moderation can

00:09:10,640 --> 00:09:13,440
mitigate a bit

00:09:13,680 --> 00:09:17,920
and they do often but the problem is

00:09:16,800 --> 00:09:19,519
that it doesn't come with

00:09:17,920 --> 00:09:21,839
it comes with the cost latency is

00:09:19,519 --> 00:09:24,480
usually less consistent

00:09:21,839 --> 00:09:25,600
and depending on whatever heuristics

00:09:24,480 --> 00:09:28,000
that

00:09:25,600 --> 00:09:29,519
that comes from the nik implementation

00:09:28,000 --> 00:09:33,120
itself

00:09:29,519 --> 00:09:34,640
the circuit overhead it exists not not

00:09:33,120 --> 00:09:36,959
huge

00:09:34,640 --> 00:09:37,839
what we see is that if we have a lot of

00:09:36,959 --> 00:09:41,760
small

00:09:37,839 --> 00:09:45,120
um um either rx

00:09:41,760 --> 00:09:47,680
activity or small payload tx activity

00:09:45,120 --> 00:09:51,200
uh we start to see this overhead sort of

00:09:47,680 --> 00:09:53,440
bottle up uh we all

00:09:51,200 --> 00:09:55,519
also have affinitization where

00:09:53,440 --> 00:09:58,320
definitely it can be a contributor to

00:09:55,519 --> 00:10:01,680
latency if not affinitized correctly

00:09:58,320 --> 00:10:04,560
um you know if you have a usually a a

00:10:01,680 --> 00:10:06,640
queue on a per cpu basis um

00:10:04,560 --> 00:10:08,800
if you know the five tuple doesn't

00:10:06,640 --> 00:10:12,079
happen to map to that uh

00:10:08,800 --> 00:10:14,079
cpu core it may happen to sort of uh

00:10:12,079 --> 00:10:16,079
affinitize to a different pneuma socket

00:10:14,079 --> 00:10:18,079
can definitely have some impact

00:10:16,079 --> 00:10:19,360
there are some techniques around it you

00:10:18,079 --> 00:10:22,160
know have been mentioned

00:10:19,360 --> 00:10:22,560
here and in other places uh techniques

00:10:22,160 --> 00:10:26,160
like

00:10:22,560 --> 00:10:29,279
a arfs some flow steering magic

00:10:26,160 --> 00:10:33,680
or um or also

00:10:29,279 --> 00:10:36,800
adq as well cash pollution

00:10:33,680 --> 00:10:40,240
again not as excessive given the

00:10:36,800 --> 00:10:40,880
assuming you have a sufficiently large

00:10:40,240 --> 00:10:43,040
cash

00:10:40,880 --> 00:10:45,200
you know that you can find on you know

00:10:43,040 --> 00:10:47,360
mid-range cpu

00:10:45,200 --> 00:10:48,959
as i said as the load starts to build up

00:10:47,360 --> 00:10:49,519
and you're talking about millions of

00:10:48,959 --> 00:10:51,519
iops

00:10:49,519 --> 00:10:52,800
in a single system from the host

00:10:51,519 --> 00:10:56,000
perspective it can

00:10:52,800 --> 00:10:58,160
bottle up and one more item is the

00:10:56,000 --> 00:11:00,079
head-of-line blocking part

00:10:58,160 --> 00:11:02,000
that is definitely apparent in mixed

00:11:00,079 --> 00:11:05,040
workloads um

00:11:02,000 --> 00:11:06,800
so basically nvme over tcp is basically

00:11:05,040 --> 00:11:09,120
you know if we look at from the bottom

00:11:06,800 --> 00:11:11,839
side it's really just messaging protocol

00:11:09,120 --> 00:11:14,560
on top of tcp streams right

00:11:11,839 --> 00:11:14,880
so if you know my queue happens to now

00:11:14,560 --> 00:11:19,680
be

00:11:14,880 --> 00:11:22,720
transferring um um one meg of data

00:11:19,680 --> 00:11:25,440
it really cannot fit sort of a a small

00:11:22,720 --> 00:11:26,150
read that's you know only a 64 byte

00:11:25,440 --> 00:11:27,839
command

00:11:26,150 --> 00:11:30,000
[Music]

00:11:27,839 --> 00:11:30,880
down all the way down to the controller

00:11:30,000 --> 00:11:33,279
um because

00:11:30,880 --> 00:11:34,880
you know once it started the message it

00:11:33,279 --> 00:11:37,920
sort of committed

00:11:34,880 --> 00:11:39,120
for you know a large payload so this is

00:11:37,920 --> 00:11:40,800
something that uh

00:11:39,120 --> 00:11:43,120
can definitely be apparent we're gonna

00:11:40,800 --> 00:11:48,079
talk about some of the stuff

00:11:43,120 --> 00:11:48,079
that were added to address that

00:11:48,160 --> 00:11:52,800
so here's just an illustration very very

00:11:51,120 --> 00:11:55,600
high level of

00:11:52,800 --> 00:11:56,000
you know asynchronous direct io usually

00:11:55,600 --> 00:11:57,920
where

00:11:56,000 --> 00:12:00,560
applications sort of care more about

00:11:57,920 --> 00:12:04,480
latency or throughput

00:12:00,560 --> 00:12:07,600
they'll use that interface

00:12:04,480 --> 00:12:08,399
all the other interfaces beneath the

00:12:07,600 --> 00:12:11,680
covers will

00:12:08,399 --> 00:12:14,480
that work that way just uh um you know

00:12:11,680 --> 00:12:16,560
um it could be the file system itself

00:12:14,480 --> 00:12:17,440
that uh that is issuing the ai or not

00:12:16,560 --> 00:12:19,040
the user

00:12:17,440 --> 00:12:20,800
but definitely but you know if we look

00:12:19,040 --> 00:12:24,079
at step one user user

00:12:20,800 --> 00:12:27,519
issues a file or a block i o

00:12:24,079 --> 00:12:29,680
using you know aio or iu ring that

00:12:27,519 --> 00:12:31,040
you know goes down to to the kernel it's

00:12:29,680 --> 00:12:32,720
a syscall obviously

00:12:31,040 --> 00:12:35,519
that may be issued directly to the

00:12:32,720 --> 00:12:38,480
driver itself or it

00:12:35,519 --> 00:12:41,200
goes into an i o scheduler mesa may

00:12:38,480 --> 00:12:41,200
context switch

00:12:42,560 --> 00:12:46,480
or not eventually you know all the block

00:12:45,040 --> 00:12:49,120
i o is being

00:12:46,480 --> 00:12:52,079
prepared and then um it's calling

00:12:49,120 --> 00:12:55,200
directly it's going to in step 2 to the

00:12:52,079 --> 00:12:56,079
device driver or the block driver and

00:12:55,200 --> 00:12:59,839
calls it q

00:12:56,079 --> 00:13:00,320
q that request now um what nvme over tcp

00:12:59,839 --> 00:13:03,360
does

00:13:00,320 --> 00:13:05,760
is that it prepares a pdu um and it

00:13:03,360 --> 00:13:09,279
places it on its own queue

00:13:05,760 --> 00:13:12,480
the historic reason for that was that

00:13:09,279 --> 00:13:15,440
the cure request in

00:13:12,480 --> 00:13:17,170
context was not allowed to sleep and we

00:13:15,440 --> 00:13:19,360
cannot send anything that

00:13:17,170 --> 00:13:21,519
[Music]

00:13:19,360 --> 00:13:22,720
and guarantee that we won't ever sleep

00:13:21,519 --> 00:13:25,920
in there because it has

00:13:22,720 --> 00:13:27,680
at least the mutex involved so what we

00:13:25,920 --> 00:13:28,480
did we always deferred into a different

00:13:27,680 --> 00:13:32,079
context to

00:13:28,480 --> 00:13:34,480
to uh uh to

00:13:32,079 --> 00:13:35,680
send it and process it from the network

00:13:34,480 --> 00:13:38,160
and also you know

00:13:35,680 --> 00:13:40,000
everything is uh is non-blocking so

00:13:38,160 --> 00:13:40,880
basically we have this i o context that

00:13:40,000 --> 00:13:43,120
will just

00:13:40,880 --> 00:13:44,240
pick pick requests and process pick

00:13:43,120 --> 00:13:46,399
requests from the this

00:13:44,240 --> 00:13:47,680
queue process them send them to the

00:13:46,399 --> 00:13:50,399
network

00:13:47,680 --> 00:13:51,279
get network from get payload from the

00:13:50,399 --> 00:13:53,040
network

00:13:51,279 --> 00:13:56,079
complete ios will locate the original

00:13:53,040 --> 00:13:56,079
request complete them

00:13:56,480 --> 00:14:01,600
when an i o basically uh

00:13:59,760 --> 00:14:03,519
completes from the controller and after

00:14:01,600 --> 00:14:07,519
being sent

00:14:03,519 --> 00:14:09,600
in step 3 from the i o context

00:14:07,519 --> 00:14:11,680
it will send back you know the data and

00:14:09,600 --> 00:14:13,360
or the completion

00:14:11,680 --> 00:14:15,760
and the nic on the host side will

00:14:13,360 --> 00:14:17,920
basically generate an interrupt

00:14:15,760 --> 00:14:19,600
then nappy is triggered from software eq

00:14:17,920 --> 00:14:22,000
obviously

00:14:19,600 --> 00:14:24,160
and then you know calls uh the driver

00:14:22,000 --> 00:14:27,920
that says that you know we have some skb

00:14:24,160 --> 00:14:31,120
esk buffs to uh to basically process

00:14:27,920 --> 00:14:35,040
from there we're not going to

00:14:31,120 --> 00:14:36,800
process them directly and sort of

00:14:35,040 --> 00:14:38,880
read from the socket we're going to

00:14:36,800 --> 00:14:42,560
trigger the same i o context that does

00:14:38,880 --> 00:14:44,639
send receive send receives and receive

00:14:42,560 --> 00:14:46,240
so that is basically one more inherent

00:14:44,639 --> 00:14:48,000
context switches

00:14:46,240 --> 00:14:49,839
uh one one more context switch and then

00:14:48,000 --> 00:14:53,040
the user context completes

00:14:49,839 --> 00:14:55,360
you called io get events um

00:14:53,040 --> 00:14:56,399
and found you know an event that it can

00:14:55,360 --> 00:15:00,160
basically its

00:14:56,399 --> 00:15:00,160
original i o has completed

00:15:00,959 --> 00:15:04,639
so the point here is basically uh to

00:15:03,920 --> 00:15:07,839
focus that

00:15:04,639 --> 00:15:07,839
uh you know all the

00:15:08,240 --> 00:15:11,920
syscalls contact with software queue is

00:15:10,160 --> 00:15:14,959
not within the control but we do have

00:15:11,920 --> 00:15:18,160
these two contact switches that are

00:15:14,959 --> 00:15:20,480
are sort of native to the to the driver

00:15:18,160 --> 00:15:20,480
itself

00:15:21,360 --> 00:15:26,639
so let's talk a bit about i'm just going

00:15:24,800 --> 00:15:28,959
to enumerate some of the uh

00:15:26,639 --> 00:15:30,800
optimizations that were that were made

00:15:28,959 --> 00:15:32,480
uh first of all the linux

00:15:30,800 --> 00:15:34,160
block layer we i talked about head of

00:15:32,480 --> 00:15:35,440
cue blocking and how it affects mixed

00:15:34,160 --> 00:15:38,480
workloads

00:15:35,440 --> 00:15:41,040
um initially i've attempted to

00:15:38,480 --> 00:15:42,480
sort of whenever i sort of uh from the i

00:15:41,040 --> 00:15:45,199
o context whenever we

00:15:42,480 --> 00:15:46,480
sort of process an i o or select a an i

00:15:45,199 --> 00:15:49,199
o to be processed

00:15:46,480 --> 00:15:50,880
we try to look at the queue and try to

00:15:49,199 --> 00:15:52,720
sort of reorder ios

00:15:50,880 --> 00:15:54,720
that you know small payloads will come

00:15:52,720 --> 00:15:56,399
first being so we don't get head offline

00:15:54,720 --> 00:16:00,240
docking or whatnot

00:15:56,399 --> 00:16:01,279
um really um took a step back and said

00:16:00,240 --> 00:16:03,680
this is something that

00:16:01,279 --> 00:16:04,800
fits an i o scheduler and block driver

00:16:03,680 --> 00:16:06,959
has no business

00:16:04,800 --> 00:16:08,560
uh deciding and doing anything any of

00:16:06,959 --> 00:16:10,480
this stuff

00:16:08,560 --> 00:16:11,759
so i sort of dropped it and said all

00:16:10,480 --> 00:16:13,440
right

00:16:11,759 --> 00:16:14,340
you know someone that cares about that

00:16:13,440 --> 00:16:17,529
will implement

00:16:14,340 --> 00:16:17,529
[Music]

00:16:17,600 --> 00:16:21,199
an i o scheduler and i don't need to

00:16:19,519 --> 00:16:24,399
worry about that

00:16:21,199 --> 00:16:26,240
but the best solution that uh that

00:16:24,399 --> 00:16:27,920
to a problem is not to have the problem

00:16:26,240 --> 00:16:30,160
at all so linux

00:16:27,920 --> 00:16:32,800
the block layer basically grew the

00:16:30,160 --> 00:16:35,600
capabilities to have multiple queue maps

00:16:32,800 --> 00:16:36,959
which means that now you know it comes

00:16:35,600 --> 00:16:39,680
in the expense that

00:16:36,959 --> 00:16:40,240
now basically the host will open more

00:16:39,680 --> 00:16:42,399
cues

00:16:40,240 --> 00:16:43,519
basically it will have a set of queues

00:16:42,399 --> 00:16:46,639
that will uh

00:16:43,519 --> 00:16:50,320
just uh transfer normal i o or

00:16:46,639 --> 00:16:52,880
stuff that uh we call right that has the

00:16:50,320 --> 00:16:55,440
data going from the host to the target

00:16:52,880 --> 00:16:58,480
it has dedicated set of

00:16:55,440 --> 00:17:01,040
cues that are designed that read i o

00:16:58,480 --> 00:17:02,720
or data that coming from the controller

00:17:01,040 --> 00:17:04,720
back to the host

00:17:02,720 --> 00:17:05,760
are going to be placed on and then

00:17:04,720 --> 00:17:08,319
there's a set of

00:17:05,760 --> 00:17:09,039
dedicated cues that are designed for

00:17:08,319 --> 00:17:14,000
polling

00:17:09,039 --> 00:17:17,280
or for high priority function

00:17:14,000 --> 00:17:20,720
flags on specific ios and that's

00:17:17,280 --> 00:17:21,919
optimizing uh for you know latency

00:17:20,720 --> 00:17:23,919
sensitive io

00:17:21,919 --> 00:17:25,600
the user interface you can do that on a

00:17:23,919 --> 00:17:28,400
per i o basis you can turn

00:17:25,600 --> 00:17:29,919
on or off this flag using either i o u

00:17:28,400 --> 00:17:35,280
ring

00:17:29,919 --> 00:17:38,720
or period period v2 or p write v2

00:17:35,280 --> 00:17:42,240
um so basically what we have here

00:17:38,720 --> 00:17:46,160
is that you know if the if an io that uh

00:17:42,240 --> 00:17:47,120
that we're sending we're sending it to a

00:17:46,160 --> 00:17:49,600
controller

00:17:47,120 --> 00:17:52,400
it goes to a different queue then you

00:17:49,600 --> 00:17:56,240
know a small payload that we send uh

00:17:52,400 --> 00:17:58,559
um uh to the controller it's not really

00:17:56,240 --> 00:18:00,160
uh stuck behind this large transfer

00:17:58,559 --> 00:18:03,360
because these are now different

00:18:00,160 --> 00:18:04,240
nvme cues which maps to different tcp

00:18:03,360 --> 00:18:05,919
sockets so

00:18:04,240 --> 00:18:08,240
basically the whole messaging is

00:18:05,919 --> 00:18:11,600
completely um

00:18:08,240 --> 00:18:13,520
orthogonal now so what happened is that

00:18:11,600 --> 00:18:16,080
once you turn this on it's not

00:18:13,520 --> 00:18:17,679
on by default because it as i said it

00:18:16,080 --> 00:18:21,360
will translate into more

00:18:17,679 --> 00:18:24,000
nvme cues and which maps to more

00:18:21,360 --> 00:18:24,640
tcp sockets which is pretty cheap today

00:18:24,000 --> 00:18:27,280
so

00:18:24,640 --> 00:18:28,880
people do tend to use it and i've seen

00:18:27,280 --> 00:18:31,679
people sort of turning this

00:18:28,880 --> 00:18:32,320
on before but the test i did to sort of

00:18:31,679 --> 00:18:35,360
uh

00:18:32,320 --> 00:18:38,720
um or the test that was used to sort of

00:18:35,360 --> 00:18:41,840
um explain and and show the benefit

00:18:38,720 --> 00:18:42,720
is that i had like 16 sort of readers

00:18:41,840 --> 00:18:44,720
quote-unquote

00:18:42,720 --> 00:18:47,760
that issued you know qdp1 just

00:18:44,720 --> 00:18:50,320
synchronous 4k reads

00:18:47,760 --> 00:18:50,960
just one by one and in the background i

00:18:50,320 --> 00:18:53,600
have this

00:18:50,960 --> 00:18:56,799
unbound writer thread that issues a

00:18:53,600 --> 00:18:59,520
large qdap 32 of one megabyte ios

00:18:56,799 --> 00:19:01,039
and sort of bounced around different

00:18:59,520 --> 00:19:02,960
cpus

00:19:01,039 --> 00:19:04,480
and then we looked at you know the read

00:19:02,960 --> 00:19:06,320
qos that

00:19:04,480 --> 00:19:08,720
was derived from that there's the read

00:19:06,320 --> 00:19:11,919
iops um

00:19:08,720 --> 00:19:14,000
almost uh more than 2x improvement

00:19:11,919 --> 00:19:15,760
same for the average latency obviously

00:19:14,000 --> 00:19:16,320
we're talking about qt1 so it can

00:19:15,760 --> 00:19:18,559
definitely

00:19:16,320 --> 00:19:19,760
uh have a one-to-one impact but the

00:19:18,559 --> 00:19:21,600
biggest uh

00:19:19,760 --> 00:19:23,360
improvement was the four nines or the

00:19:21,600 --> 00:19:27,520
four nine tail latency

00:19:23,360 --> 00:19:29,520
which was an order of magnitude better

00:19:27,520 --> 00:19:32,720
so this is something that uh sort of the

00:19:29,520 --> 00:19:36,240
block layer has grown the capability

00:19:32,720 --> 00:19:38,880
to address for different use cases for

00:19:36,240 --> 00:19:40,880
you know nvme pci devices but something

00:19:38,880 --> 00:19:41,679
that definitely helps nvme over tcp the

00:19:40,880 --> 00:19:44,480
support

00:19:41,679 --> 00:19:45,760
um adding in the driver was easy just at

00:19:44,480 --> 00:19:48,320
the ability to

00:19:45,760 --> 00:19:49,120
allocate more tcp connections or add

00:19:48,320 --> 00:19:51,520
more cues

00:19:49,120 --> 00:19:53,200
if the user requests it just plug it

00:19:51,520 --> 00:19:54,400
into the infrastructure telling the

00:19:53,200 --> 00:19:57,360
block layer that would just have

00:19:54,400 --> 00:19:58,000
we support more mapping more qmaps and

00:19:57,360 --> 00:20:00,559
just

00:19:58,000 --> 00:20:06,240
map the software context into the

00:20:00,559 --> 00:20:09,760
hardware queue correctly

00:20:06,240 --> 00:20:10,799
some some affinity affinitization

00:20:09,760 --> 00:20:13,919
improvements

00:20:10,799 --> 00:20:17,039
um have affinity improvements so as i

00:20:13,919 --> 00:20:20,960
said um the performance

00:20:17,039 --> 00:20:24,559
um and the latency specifically

00:20:20,960 --> 00:20:27,120
um has is impacted by affinity

00:20:24,559 --> 00:20:29,120
so again linux has grown some

00:20:27,120 --> 00:20:31,600
capabilities for us to split different

00:20:29,120 --> 00:20:34,880
io types into different queue maps

00:20:31,600 --> 00:20:38,320
um and as i also mentioned

00:20:34,880 --> 00:20:39,120
each io each ioq will have its own cpu

00:20:38,320 --> 00:20:41,600
that it will

00:20:39,120 --> 00:20:42,400
always work on right the the i o thread

00:20:41,600 --> 00:20:45,919
will always

00:20:42,400 --> 00:20:48,799
run on that uh cpu core and in order to

00:20:45,919 --> 00:20:51,200
have uh um

00:20:48,799 --> 00:20:53,600
better support for you know read the

00:20:51,200 --> 00:20:56,880
specific

00:20:53,600 --> 00:20:59,360
workload and polling driven workload

00:20:56,880 --> 00:21:00,240
you want to have the affinity sort of

00:20:59,360 --> 00:21:02,320
aligned well

00:21:00,240 --> 00:21:04,480
in case you have a set of queues that

00:21:02,320 --> 00:21:04,480
are

00:21:05,440 --> 00:21:11,280
that are dedicated to host each

00:21:08,720 --> 00:21:12,240
type of i o so what was done here is

00:21:11,280 --> 00:21:15,440
that basically

00:21:12,240 --> 00:21:18,240
you have multiple queue maps

00:21:15,440 --> 00:21:18,640
the affinity or the i o cpu that we use

00:21:18,240 --> 00:21:21,919
to

00:21:18,640 --> 00:21:25,440
uh to give any type of queue

00:21:21,919 --> 00:21:28,960
sort of calculated all the q maps um

00:21:25,440 --> 00:21:31,360
combined which got us into some

00:21:28,960 --> 00:21:32,720
given that we have different mapping of

00:21:31,360 --> 00:21:35,280
queues that uh

00:21:32,720 --> 00:21:36,640
uh could be of different sizes could got

00:21:35,280 --> 00:21:39,840
us to bad accounting

00:21:36,640 --> 00:21:41,520
in terms of uh uh you know you have an

00:21:39,840 --> 00:21:44,559
application thread that runs on

00:21:41,520 --> 00:21:46,159
cpu number three for example and then

00:21:44,559 --> 00:21:47,039
from all the accounting given that you

00:21:46,159 --> 00:21:50,000
have multiple q

00:21:47,039 --> 00:21:52,000
maps the i o context will be would run

00:21:50,000 --> 00:21:53,679
either on a different core on sibling

00:21:52,000 --> 00:21:56,159
core on the same pneuma node

00:21:53,679 --> 00:21:58,400
um or you know even a core on a

00:21:56,159 --> 00:22:02,720
different pneuma node

00:21:58,400 --> 00:22:06,480
that definitely hurt the performance

00:22:02,720 --> 00:22:10,840
and also you know turning on

00:22:06,480 --> 00:22:13,919
arfs or you know specific adq techniques

00:22:10,840 --> 00:22:15,919
um that were

00:22:13,919 --> 00:22:17,039
that were discussed and referenced

00:22:15,919 --> 00:22:19,120
before

00:22:17,039 --> 00:22:21,600
so the improvement was you know on the

00:22:19,120 --> 00:22:23,520
qtap one latency we can see a 10

00:22:21,600 --> 00:22:26,080
improvement you know i'll buy that any

00:22:23,520 --> 00:22:28,480
day you know from 30 microseconds

00:22:26,080 --> 00:22:31,280
added latency compared to a disk device

00:22:28,480 --> 00:22:34,880
this specific test that was done by mark

00:22:31,280 --> 00:22:37,760
uh from from intel um

00:22:34,880 --> 00:22:38,080
30 micro seconds cut is it down to 27

00:22:37,760 --> 00:22:41,440
the

00:22:38,080 --> 00:22:42,400
average latency into in microseconds and

00:22:41,440 --> 00:22:45,679
the iops

00:22:42,400 --> 00:22:46,559
also um in a high q depth on a single

00:22:45,679 --> 00:22:51,600
core

00:22:46,559 --> 00:22:54,240
uh rose from 180 k iops into 230k iops

00:22:51,600 --> 00:22:55,520
so maybe uh i'm now noticing that i

00:22:54,240 --> 00:22:56,880
don't have a lot of information on the

00:22:55,520 --> 00:22:59,600
graph but basically it's just

00:22:56,880 --> 00:23:00,880
4k reads on a single cpu core on the

00:22:59,600 --> 00:23:03,760
whole side

00:23:00,880 --> 00:23:04,400
um and you know we have qdp1 latency and

00:23:03,760 --> 00:23:08,320
then

00:23:04,400 --> 00:23:11,679
qf32 iops number so the iop sort of

00:23:08,320 --> 00:23:14,480
improved just by affinitizing correctly

00:23:11,679 --> 00:23:15,360
and the latency improved based on the

00:23:14,480 --> 00:23:17,840
fact that

00:23:15,360 --> 00:23:23,440
now you know the application and the io

00:23:17,840 --> 00:23:25,520
thread are running on the same cpu

00:23:23,440 --> 00:23:27,039
another or another set of the

00:23:25,520 --> 00:23:29,120
optimizations were

00:23:27,039 --> 00:23:31,600
specifically around low queued up

00:23:29,120 --> 00:23:35,600
latency optimization so we talked about

00:23:31,600 --> 00:23:39,120
this nvme tcp io context that will

00:23:35,600 --> 00:23:40,159
own the i o i'm sorry that will own the

00:23:39,120 --> 00:23:43,760
i o

00:23:40,159 --> 00:23:46,000
processing um

00:23:43,760 --> 00:23:47,039
the point is that you know when the

00:23:46,000 --> 00:23:50,320
request is

00:23:47,039 --> 00:23:50,720
queued we want to if the queue is sort

00:23:50,320 --> 00:23:53,120
of

00:23:50,720 --> 00:23:54,640
uh if we if it's possible we wanna send

00:23:53,120 --> 00:23:57,200
it down the network and avoid the

00:23:54,640 --> 00:24:02,320
context switching into the nvme tcp io

00:23:57,200 --> 00:24:02,320
uh working um

00:24:02,400 --> 00:24:08,880
the uh the network of as i said

00:24:05,679 --> 00:24:10,799
sending something from the network uh

00:24:08,880 --> 00:24:13,360
sending something to the network might

00:24:10,799 --> 00:24:15,919
sleep so we need to convert the hardware

00:24:13,360 --> 00:24:18,400
context or the context that hue request

00:24:15,919 --> 00:24:20,159
is running in into a sleepable context

00:24:18,400 --> 00:24:23,120
so we need to change the locking scheme

00:24:20,159 --> 00:24:24,640
into from rcu based into srcu alarm

00:24:23,120 --> 00:24:26,559
allowing us to sleep

00:24:24,640 --> 00:24:28,400
the block layer you know grew that

00:24:26,559 --> 00:24:29,120
capability so we can leverage it now

00:24:28,400 --> 00:24:32,320
although it's

00:24:29,120 --> 00:24:34,480
it's more heavy weight but you know uh

00:24:32,320 --> 00:24:36,080
empirical trial experiments show that

00:24:34,480 --> 00:24:38,000
it's worthwhile

00:24:36,080 --> 00:24:40,000
to avoid the context which is which is

00:24:38,000 --> 00:24:43,600
really you know the enemy

00:24:40,000 --> 00:24:44,799
um you need to now basically serialize

00:24:43,600 --> 00:24:48,559
two contacts that

00:24:44,799 --> 00:24:50,640
will now access the sockets

00:24:48,559 --> 00:24:51,840
the same socket basically so we haven't

00:24:50,640 --> 00:24:54,159
introduced the mutex

00:24:51,840 --> 00:24:56,159
around that but the point is that we

00:24:54,159 --> 00:24:58,559
don't want to take it

00:24:56,159 --> 00:24:59,360
unless we know that the overhead is

00:24:58,559 --> 00:25:02,320
small

00:24:59,360 --> 00:25:02,640
so we only try this optimization if the

00:25:02,320 --> 00:25:04,559
queue

00:25:02,640 --> 00:25:06,000
is empty meaning that we don't have a

00:25:04,559 --> 00:25:10,000
lot of requests uh

00:25:06,000 --> 00:25:13,279
that are bottled before us and only

00:25:10,000 --> 00:25:14,720
if the i o thread mapped cpu matches the

00:25:13,279 --> 00:25:17,039
context that we're running on

00:25:14,720 --> 00:25:18,960
meaning that if we do take the lock it

00:25:17,039 --> 00:25:22,480
will be only contending with the same

00:25:18,960 --> 00:25:25,760
with another context on the same cpu

00:25:22,480 --> 00:25:29,200
um so that will

00:25:25,760 --> 00:25:31,919
give the results later after we talk

00:25:29,200 --> 00:25:32,960
also about the rx path optimization so

00:25:31,919 --> 00:25:35,440
basically here

00:25:32,960 --> 00:25:37,600
instead of queuing the the i o to a

00:25:35,440 --> 00:25:39,919
different context to be sent from there

00:25:37,600 --> 00:25:42,240
we just send it directly from the user i

00:25:39,919 --> 00:25:43,919
o context remember the user issued the i

00:25:42,240 --> 00:25:46,080
o it goes down to the driver it actually

00:25:43,919 --> 00:25:47,919
in the same context will go down

00:25:46,080 --> 00:25:50,159
to the network down to the controller as

00:25:47,919 --> 00:25:50,159
well

00:25:50,400 --> 00:25:57,840
so on the rx path linux screw

00:25:54,080 --> 00:25:58,880
polling interface uh for i o for latency

00:25:57,840 --> 00:26:01,760
sensitive i o

00:25:58,880 --> 00:26:02,240
and polling is becoming more of a thing

00:26:01,760 --> 00:26:05,120
uh

00:26:02,240 --> 00:26:06,960
with the emergence of fast devices both

00:26:05,120 --> 00:26:09,520
on networking and storage

00:26:06,960 --> 00:26:10,799
the interface is basically submission

00:26:09,520 --> 00:26:13,919
with a

00:26:10,799 --> 00:26:14,880
high priority flag and you can basically

00:26:13,919 --> 00:26:18,240
pull for complete

00:26:14,880 --> 00:26:18,559
completion of either uh via iou ring

00:26:18,240 --> 00:26:21,840
that

00:26:18,559 --> 00:26:24,720
basically io get events will uh go down

00:26:21,840 --> 00:26:25,279
actually all the way down to the device

00:26:24,720 --> 00:26:28,720
level

00:26:25,279 --> 00:26:31,200
and pole for a completion uh

00:26:28,720 --> 00:26:32,000
what we did in nvme over tcp we added

00:26:31,200 --> 00:26:35,039
basically

00:26:32,000 --> 00:26:36,480
uh nvme tcp poll and plug it into you

00:26:35,039 --> 00:26:39,679
know the polling infrastructure and the

00:26:36,480 --> 00:26:42,720
block layer

00:26:39,679 --> 00:26:43,600
and in upper level stacks hook it you

00:26:42,720 --> 00:26:47,279
know all the way up to

00:26:43,600 --> 00:26:51,919
iou ring in terms of flag flag

00:26:47,279 --> 00:26:53,919
exposure we added basically now a set of

00:26:51,919 --> 00:26:56,320
queues that are designed for polling

00:26:53,919 --> 00:26:57,200
um but you know they're still interrupt

00:26:56,320 --> 00:26:59,039
driven

00:26:57,200 --> 00:27:00,640
but opportunistically you know if the

00:26:59,039 --> 00:27:03,440
application will fi will

00:27:00,640 --> 00:27:04,480
enter uh polling will basically call

00:27:03,440 --> 00:27:08,559
polling

00:27:04,480 --> 00:27:12,240
we do sk bezel loop assuming that we can

00:27:08,559 --> 00:27:16,000
and it has a chance to actually find

00:27:12,240 --> 00:27:19,679
uh completions or payload or data

00:27:16,000 --> 00:27:20,399
or rx payload um before you know an

00:27:19,679 --> 00:27:23,440
interrupt is

00:27:20,399 --> 00:27:25,520
is triggered and possibly will have

00:27:23,440 --> 00:27:28,960
latency improvements

00:27:25,520 --> 00:27:29,919
so that's one optimization the other

00:27:28,960 --> 00:27:32,640
optimization

00:27:29,919 --> 00:27:34,640
is basically when we do see a data ready

00:27:32,640 --> 00:27:36,720
and whether we called polling or not we

00:27:34,640 --> 00:27:37,679
will see data ready once uh you know

00:27:36,720 --> 00:27:40,640
nappy prep

00:27:37,679 --> 00:27:42,080
you know we have data prepared for us in

00:27:40,640 --> 00:27:45,039
the socket layer

00:27:42,080 --> 00:27:46,880
um we're going to context switch

00:27:45,039 --> 00:27:49,039
automatically

00:27:46,880 --> 00:27:50,000
what the optimization here is that if

00:27:49,039 --> 00:27:52,720
we're uh

00:27:50,000 --> 00:27:54,880
detecting that the user is polling we

00:27:52,720 --> 00:27:58,960
sort of skip

00:27:54,880 --> 00:28:00,880
firing that rx context avoid the context

00:27:58,960 --> 00:28:04,880
switch

00:28:00,880 --> 00:28:08,320
and let the polling uh sort of uh uh

00:28:04,880 --> 00:28:10,720
um pull the data pull the rx data itself

00:28:08,320 --> 00:28:11,840
mostly you know experiments show that

00:28:10,720 --> 00:28:14,559
mostly true

00:28:11,840 --> 00:28:15,279
um if nic moderation adaptive moderation

00:28:14,559 --> 00:28:16,559
works well

00:28:15,279 --> 00:28:19,600
you know we don't see a lot of

00:28:16,559 --> 00:28:21,440
interrupts and if the device holds off

00:28:19,600 --> 00:28:22,640
interrupts more aggressively like in the

00:28:21,440 --> 00:28:26,159
adq case it works

00:28:22,640 --> 00:28:28,399
even better so

00:28:26,159 --> 00:28:29,679
moving on forward after that we have

00:28:28,399 --> 00:28:32,559
some more improvements

00:28:29,679 --> 00:28:33,760
just uh on the single qtap we see that

00:28:32,559 --> 00:28:36,799
uh

00:28:33,760 --> 00:28:38,480
we have uh um another sort of ten

00:28:36,799 --> 00:28:41,520
percent uh

00:28:38,480 --> 00:28:44,480
improvement in the heat up one latency

00:28:41,520 --> 00:28:45,760
and then uh and high qdap which

00:28:44,480 --> 00:28:49,440
surprisingly

00:28:45,760 --> 00:28:52,480
um um also got an improvement

00:28:49,440 --> 00:28:55,760
what we're thinking is that basically um

00:28:52,480 --> 00:28:58,320
the specific experiment that uh

00:28:55,760 --> 00:28:59,919
issued ios one by one so basically it

00:28:58,320 --> 00:29:02,159
was sent to the wire

00:28:59,919 --> 00:29:03,760
and the wire was not congested so it was

00:29:02,159 --> 00:29:07,440
able to pass

00:29:03,760 --> 00:29:08,320
so another improvement for low qdp but

00:29:07,440 --> 00:29:11,520
also

00:29:08,320 --> 00:29:13,840
high q depth optimization basically

00:29:11,520 --> 00:29:16,320
avoiding context switches um if it's

00:29:13,840 --> 00:29:18,399
possible

00:29:16,320 --> 00:29:20,159
in terms of high queued up latency

00:29:18,399 --> 00:29:22,399
optimization this uh

00:29:20,159 --> 00:29:23,440
sort of uh the concept came from the

00:29:22,399 --> 00:29:27,200
cornell gang

00:29:23,440 --> 00:29:27,760
retreat and the in the team if a high q

00:29:27,200 --> 00:29:30,240
depth

00:29:27,760 --> 00:29:31,679
uh is building up in the queue we want

00:29:30,240 --> 00:29:32,480
to leverage that information and

00:29:31,679 --> 00:29:34,720
optimize

00:29:32,480 --> 00:29:36,640
you know mitigate some context switches

00:29:34,720 --> 00:29:37,679
and optimize how we handle the network

00:29:36,640 --> 00:29:41,279
traffic

00:29:37,679 --> 00:29:43,520
and the block layer is able to do that

00:29:41,279 --> 00:29:45,279
to indicate down to the driver if this

00:29:43,520 --> 00:29:48,960
request that it's being

00:29:45,279 --> 00:29:51,870
queued is the last one in in the batch

00:29:48,960 --> 00:29:53,279
that was sort of introduced into a

00:29:51,870 --> 00:29:56,240
[Music]

00:29:53,279 --> 00:29:58,399
batch doorbells and nvme pci mostly for

00:29:56,240 --> 00:30:00,159
shadow doorbells for virtualization

00:29:58,399 --> 00:30:02,320
uh use cases where doorbells are more

00:30:00,159 --> 00:30:05,760
expensive but we can

00:30:02,320 --> 00:30:08,000
now utilize it and and and nvme over tcp

00:30:05,760 --> 00:30:10,480
basically the block data that arrives uh

00:30:08,000 --> 00:30:11,840
basically has the last um boolean

00:30:10,480 --> 00:30:14,799
indicator

00:30:11,840 --> 00:30:15,679
um what we did is to basically modify

00:30:14,799 --> 00:30:18,480
the driver

00:30:15,679 --> 00:30:19,840
that you know it's a software or virtual

00:30:18,480 --> 00:30:23,440
cue from

00:30:19,840 --> 00:30:25,760
the request submission into the io

00:30:23,440 --> 00:30:28,000
context

00:30:25,760 --> 00:30:30,240
move from a list into a lockless list

00:30:28,000 --> 00:30:32,880
that is being processed in a batch

00:30:30,240 --> 00:30:35,200
gives the driver some better view of the

00:30:32,880 --> 00:30:37,840
queue that is building within the driver

00:30:35,200 --> 00:30:39,120
and also we sort of we looked at uh this

00:30:37,840 --> 00:30:42,320
last indicator that

00:30:39,120 --> 00:30:43,840
had a have a more clear a better view

00:30:42,320 --> 00:30:45,600
into the queue that is building up in

00:30:43,840 --> 00:30:48,320
the block layer itself

00:30:45,600 --> 00:30:49,260
uh whether it's an i o scheduler or just

00:30:48,320 --> 00:30:50,399
a

00:30:49,260 --> 00:30:54,960
[Music]

00:30:50,399 --> 00:30:58,000
a queue that is building up due to to

00:30:54,960 --> 00:30:58,799
the way that the i o pattern is and then

00:30:58,000 --> 00:31:01,200
we basically

00:30:58,799 --> 00:31:01,840
schedule the i o thread only when the

00:31:01,200 --> 00:31:04,240
last is

00:31:01,840 --> 00:31:05,919
last and bad sort of arrives and the

00:31:04,240 --> 00:31:09,039
second optimization is that we

00:31:05,919 --> 00:31:12,080
sort of uh use message flags for message

00:31:09,039 --> 00:31:15,519
more and send page no not last

00:31:12,080 --> 00:31:17,360
um if it's not you know the last i o

00:31:15,519 --> 00:31:19,039
the last payload that we have to send it

00:31:17,360 --> 00:31:20,000
into the network we have we know that we

00:31:19,039 --> 00:31:23,120
have more

00:31:20,000 --> 00:31:26,240
um or just turn on message eor if it's

00:31:23,120 --> 00:31:30,000
it is the last in batch

00:31:26,240 --> 00:31:33,279
um and on top of that we have the

00:31:30,000 --> 00:31:37,279
uh batching support uh or improvement

00:31:33,279 --> 00:31:40,559
that came uh from minglay from red hat

00:31:37,279 --> 00:31:41,840
uh for uh specifically for in the case

00:31:40,559 --> 00:31:45,039
of i o schedulers

00:31:41,840 --> 00:31:46,000
and the specific graph uh the specific

00:31:45,039 --> 00:31:47,600
improvement

00:31:46,000 --> 00:31:49,600
is also measurable without eye

00:31:47,600 --> 00:31:53,679
schedulers but with our schedulers

00:31:49,600 --> 00:31:56,720
um this fix is needed

00:31:53,679 --> 00:31:59,760
and then um the third in

00:31:56,720 --> 00:32:00,399
the third part is the work that retreat

00:31:59,760 --> 00:32:02,159
and

00:32:00,399 --> 00:32:04,159
the cornell team is doing right now is

00:32:02,159 --> 00:32:07,200
basically implement the concepts

00:32:04,159 --> 00:32:11,279
of the i10 paper which really

00:32:07,200 --> 00:32:14,480
introduces io scheduling optimization

00:32:11,279 --> 00:32:17,200
as a first class citizen i o scheduler

00:32:14,480 --> 00:32:18,320
that is based into that is sort of

00:32:17,200 --> 00:32:20,880
optimized

00:32:18,320 --> 00:32:23,039
um optimizations that it introduced are

00:32:20,880 --> 00:32:27,360
centered around tcp streams

00:32:23,039 --> 00:32:31,039
for you know these sort of blog devices

00:32:27,360 --> 00:32:33,039
i'll give some uh reference um

00:32:31,039 --> 00:32:35,039
from the measurements that uh the team

00:32:33,039 --> 00:32:37,760
has done

00:32:35,039 --> 00:32:38,640
um so for you know on the left hand side

00:32:37,760 --> 00:32:42,320
if we looked at

00:32:38,640 --> 00:32:45,840
latency and iops the similar

00:32:42,320 --> 00:32:49,919
uh graphs uh that uh ratchet has

00:32:45,840 --> 00:32:51,919
presented with if you sacrifice some of

00:32:49,919 --> 00:32:55,840
the uh

00:32:51,919 --> 00:32:56,720
latency attributes you can actually get

00:32:55,840 --> 00:32:59,519
higher

00:32:56,720 --> 00:33:00,159
uh or you know a noticeable improvement

00:32:59,519 --> 00:33:03,120
um

00:33:00,159 --> 00:33:04,480
in iops per core and that is specific to

00:33:03,120 --> 00:33:07,039
the test set up in

00:33:04,480 --> 00:33:08,640
the the test configuration that was done

00:33:07,039 --> 00:33:10,880
by the cornell team

00:33:08,640 --> 00:33:12,799
if we look at the right hand side

00:33:10,880 --> 00:33:15,279
basically you know for different request

00:33:12,799 --> 00:33:15,279
sizes

00:33:15,360 --> 00:33:23,279
there is obviously an advantage here

00:33:18,960 --> 00:33:26,799
on throughput with the i10 io scheduler

00:33:23,279 --> 00:33:30,159
uh or lying running on top of nvme tcp

00:33:26,799 --> 00:33:33,360
from nvme tcp alone um

00:33:30,159 --> 00:33:36,840
that is sort of um both of them uh

00:33:33,360 --> 00:33:39,840
sort of emphasize the advantages of of

00:33:36,840 --> 00:33:39,840
batching

00:33:41,039 --> 00:33:46,799
and i think that i i am now

00:33:44,320 --> 00:33:48,399
ready to take questions i wasn't sure

00:33:46,799 --> 00:33:52,000
how i'm doing on time so i

00:33:48,399 --> 00:33:52,720
might have rushed a bit so um sharing

00:33:52,000 --> 00:33:55,600
and i'll take

00:33:52,720 --> 00:33:57,200
questions okay so i'll i'll pass on the

00:33:55,600 --> 00:33:59,440
questions to you second

00:33:57,200 --> 00:34:01,200
oh okay uh well if you can put up your

00:33:59,440 --> 00:34:02,880
slides actually it seems like your slide

00:34:01,200 --> 00:34:05,200
number four

00:34:02,880 --> 00:34:07,120
has generated some questions can you put

00:34:05,200 --> 00:34:09,440
back your slide number four

00:34:07,120 --> 00:34:09,440
yes

00:34:13,200 --> 00:34:16,320
this is like all right so slide number

00:34:15,760 --> 00:34:19,599
four

00:34:16,320 --> 00:34:20,720
uh and now my q a disappeared from this

00:34:19,599 --> 00:34:23,040
screen

00:34:20,720 --> 00:34:24,399
you want to ask the question it's a very

00:34:23,040 --> 00:34:26,480
simple question so this

00:34:24,399 --> 00:34:28,000
this latency contribution summary is

00:34:26,480 --> 00:34:29,839
that host

00:34:28,000 --> 00:34:31,679
controller or both right because i think

00:34:29,839 --> 00:34:32,800
the pattern and the profile changes

00:34:31,679 --> 00:34:35,280
whether you're

00:34:32,800 --> 00:34:37,440
looking at it as an initiator yeah yeah

00:34:35,280 --> 00:34:40,480
i maybe i did not mention that

00:34:37,440 --> 00:34:43,919
um the focus here is

00:34:40,480 --> 00:34:44,240
is really on the whole side um there has

00:34:43,919 --> 00:34:46,879
been

00:34:44,240 --> 00:34:48,399
work to also optimize the target side

00:34:46,879 --> 00:34:51,440
but

00:34:48,399 --> 00:34:53,119
the target side is usually people view

00:34:51,440 --> 00:34:57,280
it as a reference

00:34:53,119 --> 00:34:57,280
implementation or a testing vehicle

00:34:57,599 --> 00:35:03,280
usually people use commercial targets so

00:35:00,800 --> 00:35:05,280
the focus of the conversation is really

00:35:03,280 --> 00:35:09,280
on the host side itself

00:35:05,280 --> 00:35:12,720
um the target uh that is being used for

00:35:09,280 --> 00:35:15,119
um most of of uh of uh

00:35:12,720 --> 00:35:16,160
of the measurements is indeed the linux

00:35:15,119 --> 00:35:19,680
target

00:35:16,160 --> 00:35:22,800
um but there is but there's uh

00:35:19,680 --> 00:35:24,960
less work um that has been done

00:35:22,800 --> 00:35:25,839
to optimize the target side although i

00:35:24,960 --> 00:35:29,280
do know that

00:35:25,839 --> 00:35:30,400
the intel team has uh some uh patches

00:35:29,280 --> 00:35:33,200
that uh

00:35:30,400 --> 00:35:33,920
um are in the pipe to also optimize and

00:35:33,200 --> 00:35:38,640
improve the

00:35:33,920 --> 00:35:41,440
performance of the linux nvme tcp target

00:35:38,640 --> 00:35:41,839
excellent all right next question is

00:35:41,440 --> 00:35:45,680
from

00:35:41,839 --> 00:35:48,480
ratchet um okay so this

00:35:45,680 --> 00:35:50,160
he thought uh you had an interesting

00:35:48,480 --> 00:35:53,599
work on the mixed workloads

00:35:50,160 --> 00:35:54,960
uh question is what is the impact on

00:35:53,599 --> 00:35:57,599
throughput

00:35:54,960 --> 00:35:58,640
parkour right also what is the overhead

00:35:57,599 --> 00:36:02,160
of having more queues

00:35:58,640 --> 00:36:05,280
more queue maps yeah so the overhead

00:36:02,160 --> 00:36:07,200
is uh is is

00:36:05,280 --> 00:36:08,480
not uh substantial at all it really

00:36:07,200 --> 00:36:10,079
depends on what are the hardware

00:36:08,480 --> 00:36:13,200
resources uh

00:36:10,079 --> 00:36:16,000
uh that are needed uh for every

00:36:13,200 --> 00:36:17,920
specific uh hardware queue in the case

00:36:16,000 --> 00:36:20,320
of npm tcps

00:36:17,920 --> 00:36:21,440
is really um only the infrastructure

00:36:20,320 --> 00:36:24,720
that's needed to add

00:36:21,440 --> 00:36:27,359
another tcp socket context

00:36:24,720 --> 00:36:28,160
so it's very very fairly cheap compared

00:36:27,359 --> 00:36:30,560
to uh

00:36:28,160 --> 00:36:32,640
for example uh if you take hardware

00:36:30,560 --> 00:36:35,200
resources for an rdma adapter

00:36:32,640 --> 00:36:36,800
which is also not not as not very

00:36:35,200 --> 00:36:40,160
expensive

00:36:36,800 --> 00:36:42,079
so the cost is not is not huge uh

00:36:40,160 --> 00:36:43,440
so it's definitely worth to have

00:36:42,079 --> 00:36:45,599
multiple

00:36:43,440 --> 00:36:47,040
queue maps and also if you think about

00:36:45,599 --> 00:36:50,160
it usually a host will not

00:36:47,040 --> 00:36:50,960
be connected to thousands of controllers

00:36:50,160 --> 00:36:54,000
it will have

00:36:50,960 --> 00:36:56,960
you know up to tens of controllers

00:36:54,000 --> 00:36:59,040
and even the most uh effort if the host

00:36:56,960 --> 00:37:00,960
is connected into a large cluster it

00:36:59,040 --> 00:37:04,240
will have tens of controllers

00:37:00,960 --> 00:37:07,359
and each of them will have uh um

00:37:04,240 --> 00:37:09,040
um you know it will open on a per per

00:37:07,359 --> 00:37:11,839
core basis a set of q

00:37:09,040 --> 00:37:13,760
you can multiply that by two not we're

00:37:11,839 --> 00:37:14,320
not talking about huge numbers that can

00:37:13,760 --> 00:37:16,960
actually

00:37:14,320 --> 00:37:17,440
impact you know how the stack is running

00:37:16,960 --> 00:37:20,720
even

00:37:17,440 --> 00:37:24,079
for the most most extreme cases so

00:37:20,720 --> 00:37:27,119
not a lot of overhead in terms of the

00:37:24,079 --> 00:37:27,520
throughput improvement um if you know

00:37:27,119 --> 00:37:30,000
the

00:37:27,520 --> 00:37:31,040
mix if the workload is mixed then

00:37:30,000 --> 00:37:34,079
definitely

00:37:31,040 --> 00:37:36,720
you have zero blocking between you know

00:37:34,079 --> 00:37:37,359
rx and tx or redriven operations and

00:37:36,720 --> 00:37:39,839
write

00:37:37,359 --> 00:37:42,960
driven operations so the the throughput

00:37:39,839 --> 00:37:42,960
improves dramatically

00:37:43,760 --> 00:37:50,160
i hope that answers the question

00:37:46,960 --> 00:37:57,440
does it answer the question rashid as

00:37:50,160 --> 00:37:57,440

YouTube URL: https://www.youtube.com/watch?v=KdZLu42oNvA


