Title: Netdev 0x14 - Hardware offload for K8s container networking
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Rony Efraim, Liel Shoshan

More info: https://netdevconf.info/0x14/session.html?talk-hardware-offload-for-k8s-container-networking

Date: Monday, August 17, 2020

Kubernetes(k8s) Container Network Interface (CNI) is a specification for managing
network resources on a kubernetes cluster. CNI enables plugin-based networking
solution for containers ranging from IP address management to access control
policy management etc. Operators can pick and choose from the many packaged
and open implementations that exist or could create custom CNIs to serve their
needs.

In this talk Liel Shoshan describes how they approach hardware
network offloading in k8s and challenges faced.
They will illustrate offloading with a bunch of CNIs that can be used in
conjunction with OVS. In addition, they will also describe how to offload for
other K8s use cases like pod-to-pod intra networking and enhancing ingress
service load balancing via dp_hash.
Captions: 
	00:00:02,560 --> 00:00:06,799
hi my name is lial

00:00:04,240 --> 00:00:08,240
i'm a software architect at melanox

00:00:06,799 --> 00:00:11,679
nvidia

00:00:08,240 --> 00:00:17,920
and today i'll discuss hardware offload

00:00:11,679 --> 00:00:17,920
for kubernetes container networking

00:00:19,760 --> 00:00:27,439
i'll talk shortly about the need for

00:00:23,359 --> 00:00:29,279
container hardware offload then

00:00:27,439 --> 00:00:32,160
do a short overview of kubernetes

00:00:29,279 --> 00:00:34,399
networking model

00:00:32,160 --> 00:00:35,600
i'll talk about the use case we chose

00:00:34,399 --> 00:00:39,040
which is ovn

00:00:35,600 --> 00:00:41,360
cni and

00:00:39,040 --> 00:00:42,640
the needed work that needed to be done

00:00:41,360 --> 00:00:46,320
on ovs

00:00:42,640 --> 00:00:46,320
and ovn hardware offload

00:00:50,480 --> 00:00:55,600
so linux containers as you all know have

00:00:53,360 --> 00:00:56,160
changed the world of cloud computing and

00:00:55,600 --> 00:01:00,000
the way

00:00:56,160 --> 00:01:02,079
enterprises do software as a lightweight

00:01:00,000 --> 00:01:03,039
standalone executable package of

00:01:02,079 --> 00:01:04,799
software

00:01:03,039 --> 00:01:06,159
that includes everything needed to run

00:01:04,799 --> 00:01:10,799
in applications

00:01:06,159 --> 00:01:13,840
container can run as vms never could

00:01:10,799 --> 00:01:16,000
because it's much lighter this enhances

00:01:13,840 --> 00:01:18,080
the already existing need for network

00:01:16,000 --> 00:01:20,560
hardware offloading the cloud

00:01:18,080 --> 00:01:23,200
in order for the network not to be the

00:01:20,560 --> 00:01:23,200
bottleneck

00:01:23,520 --> 00:01:30,799
kubernetes is the de facto

00:01:26,640 --> 00:01:32,560
orchestrator on big cloud in production

00:01:30,799 --> 00:01:34,320
it's an open source container

00:01:32,560 --> 00:01:37,360
orchestration system

00:01:34,320 --> 00:01:38,640
for deployment automation scaling and

00:01:37,360 --> 00:01:42,479
management

00:01:38,640 --> 00:01:42,479
of containerized applications

00:01:42,880 --> 00:01:46,640
and in this session we will discuss the

00:01:45,759 --> 00:01:48,880
enablement

00:01:46,640 --> 00:01:55,520
of networking how to offload in a

00:01:48,880 --> 00:01:58,799
kubernetes environment

00:01:55,520 --> 00:01:59,920
so a few words about kubernetes bernadis

00:01:58,799 --> 00:02:01,680
was designed

00:01:59,920 --> 00:02:03,119
as a loosely coupled collection of

00:02:01,680 --> 00:02:05,280
components which are

00:02:03,119 --> 00:02:08,160
centered around deploying maintaining

00:02:05,280 --> 00:02:11,039
and scaling applications

00:02:08,160 --> 00:02:11,760
it abstracts the underlying hardware of

00:02:11,039 --> 00:02:14,000
the nodes

00:02:11,760 --> 00:02:17,360
and provides a uniform interface for

00:02:14,000 --> 00:02:20,800
application to be deployed

00:02:17,360 --> 00:02:23,599
so when using kubernetes to deploy

00:02:20,800 --> 00:02:24,640
your application you can deploy them

00:02:23,599 --> 00:02:27,840
quickly

00:02:24,640 --> 00:02:31,200
you can scale them up and down

00:02:27,840 --> 00:02:32,879
roll out new feature monitor and health

00:02:31,200 --> 00:02:35,760
check the containers

00:02:32,879 --> 00:02:39,680
and limit the hardware usages the harder

00:02:35,760 --> 00:02:39,680
usage to the required resources

00:02:40,310 --> 00:02:43,490
[Music]

00:02:44,560 --> 00:02:52,400
kubernetes networking model is based on

00:02:48,720 --> 00:02:56,239
cni connect container network interface

00:02:52,400 --> 00:03:00,879
an interface uh which defines uh

00:02:56,239 --> 00:03:00,879
the network this interface

00:03:03,360 --> 00:03:07,040
kubernetes networking model is built on

00:03:06,480 --> 00:03:10,319
top of

00:03:07,040 --> 00:03:12,319
container network interface cni

00:03:10,319 --> 00:03:14,480
an interface between the container

00:03:12,319 --> 00:03:16,879
runtime and the network implementation

00:03:14,480 --> 00:03:19,840
plugin

00:03:16,879 --> 00:03:22,560
different cni plugins utilizes different

00:03:19,840 --> 00:03:26,959
linux networking mechanism

00:03:22,560 --> 00:03:30,239
so for example i listed a few here

00:03:26,959 --> 00:03:33,519
flannel which is a linux bridge base

00:03:30,239 --> 00:03:36,400
with vxlan calico

00:03:33,519 --> 00:03:37,840
uh which is a routing to the host-based

00:03:36,400 --> 00:03:40,319
sdn

00:03:37,840 --> 00:03:42,159
there are several ovs open

00:03:40,319 --> 00:03:45,280
v-switch-based ones

00:03:42,159 --> 00:03:48,319
uh among them are ovn

00:03:45,280 --> 00:03:52,159
and tria and openv switch

00:03:48,319 --> 00:03:53,840
and celium as an example for bpf based

00:03:52,159 --> 00:03:56,879
one

00:03:53,840 --> 00:03:59,519
so talking about hardware offload there

00:03:56,879 --> 00:04:02,879
are many opt and acceleration in general

00:03:59,519 --> 00:04:02,879
there are many options here

00:04:04,080 --> 00:04:08,080
for example with flannel and linux

00:04:07,120 --> 00:04:10,879
bridge

00:04:08,080 --> 00:04:12,959
there is uh there is thinking of how to

00:04:10,879 --> 00:04:16,079
offload that

00:04:12,959 --> 00:04:18,400
we can use driver to listen to both net

00:04:16,079 --> 00:04:21,359
linking events coming from the kernel

00:04:18,400 --> 00:04:22,000
for configuration purposes such as

00:04:21,359 --> 00:04:25,520
static

00:04:22,000 --> 00:04:28,800
mac vlan configuration vgt

00:04:25,520 --> 00:04:31,600
vst etc and listening

00:04:28,800 --> 00:04:33,120
to hardware internal events of the

00:04:31,600 --> 00:04:36,240
device

00:04:33,120 --> 00:04:39,840
such as learning port events and

00:04:36,240 --> 00:04:39,840
port changes etc

00:04:40,880 --> 00:04:44,479
calico on the other hand which is based

00:04:43,120 --> 00:04:49,600
on linux routing

00:04:44,479 --> 00:04:53,040
fib uh is also a part depended on

00:04:49,600 --> 00:04:56,240
ip tables and uh

00:04:53,040 --> 00:04:58,160
this is already this already has several

00:04:56,240 --> 00:05:01,280
implementation

00:04:58,160 --> 00:05:05,039
also with ipvs and with

00:05:01,280 --> 00:05:06,960
ebpf both are

00:05:05,039 --> 00:05:09,199
accelerating and boosting the

00:05:06,960 --> 00:05:11,680
performance

00:05:09,199 --> 00:05:12,400
and there can be several other direction

00:05:11,680 --> 00:05:16,000
as net

00:05:12,400 --> 00:05:18,800
filter flow table infrastructure um

00:05:16,000 --> 00:05:20,320
which provides a fast data path for the

00:05:18,800 --> 00:05:24,240
classic linux folder

00:05:20,320 --> 00:05:26,320
or forwarding path um

00:05:24,240 --> 00:05:27,520
it allows you to accelerate packet

00:05:26,320 --> 00:05:30,880
forwarding

00:05:27,520 --> 00:05:34,400
in software and in hardware if your nic

00:05:30,880 --> 00:05:36,840
supports it by using contract-based

00:05:34,400 --> 00:05:40,800
network stack

00:05:36,840 --> 00:05:43,680
bypass in these

00:05:40,800 --> 00:05:44,960
flow tables entries are represented

00:05:43,680 --> 00:05:48,400
through a topper

00:05:44,960 --> 00:05:50,639
that is composed of the input interface

00:05:48,400 --> 00:05:52,320
source and a destination address source

00:05:50,639 --> 00:05:54,800
and destination port

00:05:52,320 --> 00:05:56,240
and there are three or four protocol and

00:05:54,800 --> 00:05:59,280
each entry

00:05:56,240 --> 00:06:01,280
also caches the destination interface

00:05:59,280 --> 00:06:03,680
and the gateway address to forward the

00:06:01,280 --> 00:06:03,680
packet

00:06:04,240 --> 00:06:13,280
we have chosen to go with ovn one

00:06:08,639 --> 00:06:16,730
ovs one and i'll talk later about it

00:06:13,280 --> 00:06:16,730
[Music]

00:06:17,280 --> 00:06:24,160
one key component in kubernetes is

00:06:20,639 --> 00:06:26,319
a kubernetes service the service

00:06:24,160 --> 00:06:27,919
is an abstraction which defines a

00:06:26,319 --> 00:06:31,360
logical set of pods

00:06:27,919 --> 00:06:34,960
and the policy by which to access them

00:06:31,360 --> 00:06:38,160
so as you can see here there are several

00:06:34,960 --> 00:06:39,840
pods uh deployed as a service

00:06:38,160 --> 00:06:41,680
therefore they're assigned of one

00:06:39,840 --> 00:06:44,960
virtual ap

00:06:41,680 --> 00:06:45,360
and when client want to access this this

00:06:44,960 --> 00:06:48,240
one

00:06:45,360 --> 00:06:49,280
one of the backend ports it access the

00:06:48,240 --> 00:06:53,520
virtual ap

00:06:49,280 --> 00:06:54,560
the service kubernetes provides the

00:06:53,520 --> 00:06:57,680
routing

00:06:54,560 --> 00:06:58,560
among the dependent pods and load

00:06:57,680 --> 00:07:01,440
balancing

00:06:58,560 --> 00:07:02,880
this is all being handled by kubernetes

00:07:01,440 --> 00:07:06,160
services

00:07:02,880 --> 00:07:09,120
and therefore pods can die

00:07:06,160 --> 00:07:11,680
replicate move to other nodes without

00:07:09,120 --> 00:07:15,280
the front end

00:07:11,680 --> 00:07:18,400
needed to know about it the

00:07:15,280 --> 00:07:21,840
service mechanism is implemented using

00:07:18,400 --> 00:07:25,599
not can be implemented in ip tables

00:07:21,840 --> 00:07:28,880
tc ovs or ipvs

00:07:25,599 --> 00:07:31,919
and which makes not performance

00:07:28,880 --> 00:07:36,319
uh a key element in container deployment

00:07:31,919 --> 00:07:39,599
performance so in additional to the

00:07:36,319 --> 00:07:40,479
cni's i mentioned before there are also

00:07:39,599 --> 00:07:43,919
simple

00:07:40,479 --> 00:07:45,039
simpler cni which doesn't implement a

00:07:43,919 --> 00:07:48,160
full

00:07:45,039 --> 00:07:50,720
sdn uh

00:07:48,160 --> 00:07:53,759
uh two examples here are mac vlan

00:07:50,720 --> 00:07:53,759
ganesha uv

00:07:54,080 --> 00:07:59,919
and the problem with these cni's is that

00:07:58,160 --> 00:08:02,960
they cannot serve

00:07:59,919 --> 00:08:04,560
for uh for the purpose of being a

00:08:02,960 --> 00:08:08,160
primary

00:08:04,560 --> 00:08:10,879
kubernetes network because it cannot

00:08:08,160 --> 00:08:14,000
they cannot implement that they cannot

00:08:10,879 --> 00:08:17,520
you cannot deploy services on them

00:08:14,000 --> 00:08:19,120
and the way it is being solved is by

00:08:17,520 --> 00:08:22,400
deploying them

00:08:19,120 --> 00:08:24,000
as a secondary network meaning that in

00:08:22,400 --> 00:08:26,720
this case

00:08:24,000 --> 00:08:28,560
each container will have two network

00:08:26,720 --> 00:08:32,080
interfaces

00:08:28,560 --> 00:08:35,279
one which is managed by kubernetes

00:08:32,080 --> 00:08:36,479
the primary network and another one only

00:08:35,279 --> 00:08:42,080
for

00:08:36,479 --> 00:08:42,080
contain the container workload data path

00:08:42,320 --> 00:08:45,440
when thinking about offloading and

00:08:44,399 --> 00:08:49,120
acceleration

00:08:45,440 --> 00:08:52,480
we wanted to choose a cni

00:08:49,120 --> 00:08:53,680
which implements a full sdn that we can

00:08:52,480 --> 00:08:57,279
offload

00:08:53,680 --> 00:09:01,440
without the need to edit as a

00:08:57,279 --> 00:09:06,160
secondary network and

00:09:01,440 --> 00:09:08,640
just use one network interface

00:09:06,160 --> 00:09:09,610
inside the container which is managed by

00:09:08,640 --> 00:09:12,649
kubernetes

00:09:09,610 --> 00:09:12,649
[Music]

00:09:15,680 --> 00:09:24,480
so we choose we chose ovncni

00:09:19,120 --> 00:09:24,480
which is an obvious based cni

00:09:27,040 --> 00:09:31,839
ovn is an open source network

00:09:30,120 --> 00:09:35,200
virtualization solution

00:09:31,839 --> 00:09:38,320
developed by the open vswitch community

00:09:35,200 --> 00:09:39,040
it has a specific uh kubernetes cni

00:09:38,320 --> 00:09:42,560
plugin

00:09:39,040 --> 00:09:46,240
called ovn kubernetes

00:09:42,560 --> 00:09:50,480
so it's an ovs based solution it has

00:09:46,240 --> 00:09:54,000
l2 l3 virtual networking

00:09:50,480 --> 00:09:57,680
composed of logical switches and routers

00:09:54,000 --> 00:10:02,959
a user can config network policies

00:09:57,680 --> 00:10:06,000
and between the worker nodes

00:10:02,959 --> 00:10:19,680
there is a geneve

00:10:06,000 --> 00:10:22,800
overlay tunneling

00:10:19,680 --> 00:10:24,800
so the goal was to achieve full ovn data

00:10:22,800 --> 00:10:27,839
pass hardware offload

00:10:24,800 --> 00:10:29,760
and for that we needed to have full

00:10:27,839 --> 00:10:31,440
obvious hardware offload with the

00:10:29,760 --> 00:10:35,360
relevant capabilities

00:10:31,440 --> 00:10:38,560
that ovn utilizes and obvious

00:10:35,360 --> 00:10:40,000
ovs is offloaded using linux dc

00:10:38,560 --> 00:10:43,279
mechanism

00:10:40,000 --> 00:10:45,040
so when by achieving full ovn data path

00:10:43,279 --> 00:10:48,480
hardware offload

00:10:45,040 --> 00:10:51,519
we reduced cpu utilization

00:10:48,480 --> 00:10:54,959
uh dramatically

00:10:51,519 --> 00:10:56,480
for that we leveraged linux tc flower

00:10:54,959 --> 00:11:01,760
support for

00:10:56,480 --> 00:11:05,040
geneva encapsulation and l2 and l3 acls

00:11:01,760 --> 00:11:07,680
and we needed to have also contract

00:11:05,040 --> 00:11:09,360
connection tracking not hardware offload

00:11:07,680 --> 00:11:11,600
through linux dc

00:11:09,360 --> 00:11:12,480
so there was a work being done about

00:11:11,600 --> 00:11:16,399
that and it's

00:11:12,480 --> 00:11:18,720
um upstream now to enable this

00:11:16,399 --> 00:11:19,839
uh connection tracking that hardware

00:11:18,720 --> 00:11:23,200
offload

00:11:19,839 --> 00:11:24,720
tomorrow there will be a talk uh by a

00:11:23,200 --> 00:11:26,399
few of my colleagues

00:11:24,720 --> 00:11:28,220
about the work that was done there in

00:11:26,399 --> 00:11:31,409
order to enable that

00:11:28,220 --> 00:11:31,409
[Music]

00:11:34,000 --> 00:11:42,010
so we came to obvious ovs is an

00:11:37,360 --> 00:11:45,059
open this is the most

00:11:42,010 --> 00:11:45,059
[Music]

00:11:45,279 --> 00:11:51,839
so we came to obvious ovs

00:11:48,640 --> 00:11:54,639
is the most popular virtual switch

00:11:51,839 --> 00:11:55,680
it's a flow based one with many

00:11:54,639 --> 00:11:58,880
capabilities

00:11:55,680 --> 00:12:02,160
among which are l2 l3

00:11:58,880 --> 00:12:05,680
nat vlan vxlan

00:12:02,160 --> 00:12:06,959
mirroring connection tracking geneve and

00:12:05,680 --> 00:12:09,120
more

00:12:06,959 --> 00:12:10,000
and there are multiple control plans

00:12:09,120 --> 00:12:13,200
built on top of

00:12:10,000 --> 00:12:17,120
it there is a user space

00:12:13,200 --> 00:12:20,000
model there is it has a kernel

00:12:17,120 --> 00:12:20,000
space model

00:12:22,959 --> 00:12:28,240
in the traditional way the packet will

00:12:26,079 --> 00:12:31,120
would arrive the first packet of a flow

00:12:28,240 --> 00:12:34,160
would arrive to the user space

00:12:31,120 --> 00:12:38,079
and then it will insert

00:12:34,160 --> 00:12:40,000
the ovs will insert a rule to the kernel

00:12:38,079 --> 00:12:43,200
and the next packets will just go

00:12:40,000 --> 00:12:46,720
directly through the kernel

00:12:43,200 --> 00:12:49,120
we wanted to add another hardware layer

00:12:46,720 --> 00:12:49,920
in which the second packet of a flow

00:12:49,120 --> 00:12:51,760
could go

00:12:49,920 --> 00:12:55,680
directly through the hardware without

00:12:51,760 --> 00:12:55,680
the need to go through the kernel

00:12:56,800 --> 00:13:01,040
so in this arc we keep the first packet

00:12:59,760 --> 00:13:04,800
miss

00:13:01,040 --> 00:13:07,510
behavior each pod should be assigned

00:13:04,800 --> 00:13:10,000
an srlv virtual function

00:13:07,510 --> 00:13:13,440
[Music]

00:13:10,000 --> 00:13:16,560
and while obvious set the policies

00:13:13,440 --> 00:13:20,320
the hardware executes them

00:13:16,560 --> 00:13:23,920
so there are open flow

00:13:20,320 --> 00:13:26,639
rules which are the ovs policies that

00:13:23,920 --> 00:13:29,120
being inserted to the hardware through

00:13:26,639 --> 00:13:32,560
to the kernel through linux dc

00:13:29,120 --> 00:13:35,839
and the driver is also inserting them

00:13:32,560 --> 00:13:35,839
to the hardware

00:13:38,399 --> 00:13:43,580
as you can see it in here

00:13:41,760 --> 00:13:44,959
so

00:13:43,580 --> 00:13:48,079
[Music]

00:13:44,959 --> 00:13:51,040
we can see the tc rules are

00:13:48,079 --> 00:13:51,040
being um

00:13:51,199 --> 00:13:55,199
inserted on top of the configured on top

00:13:54,160 --> 00:13:58,480
of the

00:13:55,199 --> 00:14:01,279
relevant net device and then

00:13:58,480 --> 00:14:03,839
these rules are also being configured in

00:14:01,279 --> 00:14:03,839
the hardware

00:14:04,160 --> 00:14:12,320
the problem with this design is that

00:14:07,839 --> 00:14:12,320
it's limited by scale

00:14:12,480 --> 00:14:16,240
since each pod is assigned a virtual

00:14:15,600 --> 00:14:19,760
function

00:14:16,240 --> 00:14:24,480
and sorry v1 we are limited

00:14:19,760 --> 00:14:24,480
by the number of virtual functions

00:14:26,639 --> 00:14:33,680
also it has this the static nature

00:14:30,160 --> 00:14:36,800
of pci device

00:14:33,680 --> 00:14:45,839
therefore the vfs cannot be

00:14:36,800 --> 00:14:45,839
created or removed on the fly

00:14:45,920 --> 00:14:49,600
so for this purpose uh scalability

00:14:48,720 --> 00:14:52,480
functions

00:14:49,600 --> 00:14:53,360
are now being developed they are

00:14:52,480 --> 00:14:56,480
designed to

00:14:53,360 --> 00:14:59,680
address the scalability constraints of

00:14:56,480 --> 00:15:00,720
estro iov while keeping the high

00:14:59,680 --> 00:15:04,160
performance

00:15:00,720 --> 00:15:08,000
and still provide

00:15:04,160 --> 00:15:12,160
the full obvious offloads as we

00:15:08,000 --> 00:15:16,639
described before they can be

00:15:12,160 --> 00:15:20,480
dynamically dynamically allocated

00:15:16,639 --> 00:15:24,160
added and removed on the fly

00:15:20,480 --> 00:15:27,199
hsf has its own nedev

00:15:24,160 --> 00:15:30,160
and they are located on the vert bus

00:15:27,199 --> 00:15:32,079
where other devices can be located as

00:15:30,160 --> 00:15:35,519
well

00:15:32,079 --> 00:15:38,720
in terms of management the creation

00:15:35,519 --> 00:15:41,839
and query of such devices will be done

00:15:38,720 --> 00:15:41,839
using devlink

00:15:42,079 --> 00:15:48,560
um besides being more

00:15:45,440 --> 00:15:51,759
cloud native and

00:15:48,560 --> 00:15:56,160
flexible stuffs

00:15:51,759 --> 00:16:01,519
are a will scale much higher in numbers

00:15:56,160 --> 00:16:04,800
than sra vvfs

00:16:01,519 --> 00:16:08,000
so this was the work

00:16:04,800 --> 00:16:11,040
we've done in order to

00:16:08,000 --> 00:16:14,160
provide better performance in

00:16:11,040 --> 00:16:17,519
kubernetes environment

00:16:14,160 --> 00:16:20,720
uh using ovn cni

00:16:17,519 --> 00:16:23,839
and nicoder offload

00:16:20,720 --> 00:16:23,839
thank you

00:16:33,920 --> 00:16:39,759
thank you love uh okay so there's a few

00:16:37,199 --> 00:16:41,360
questions uh there's one from jamal so

00:16:39,759 --> 00:16:44,000
we'll go with that

00:16:41,360 --> 00:16:45,519
um what is the latency cost of creating

00:16:44,000 --> 00:16:46,480
these sub functions and doing this

00:16:45,519 --> 00:16:50,490
dynamic

00:16:46,480 --> 00:16:51,920
device ads and deletes

00:16:50,490 --> 00:16:54,480
[Music]

00:16:51,920 --> 00:16:56,160
uh actually we don't have uh we don't

00:16:54,480 --> 00:17:00,320
have numbers yet

00:16:56,160 --> 00:17:04,160
but uh we do consider that as a

00:17:00,320 --> 00:17:05,039
as a as a control plan and therefore

00:17:04,160 --> 00:17:08,400
it's

00:17:05,039 --> 00:17:12,640
um it's more it's less sensitive but

00:17:08,400 --> 00:17:15,439
uh i don't think we have the numbers yet

00:17:12,640 --> 00:17:16,079
so tomorrow tomorrow we have a sub

00:17:15,439 --> 00:17:19,839
function

00:17:16,079 --> 00:17:19,839
meeting yes

00:17:21,120 --> 00:17:28,079
yeah i'm here can you hear me yeah

00:17:25,199 --> 00:17:30,080
okay okay so i think that the latency of

00:17:28,079 --> 00:17:31,200
the sub function the major cut down is

00:17:30,080 --> 00:17:34,240
on the pci

00:17:31,200 --> 00:17:37,200
flr that happens on the each vf

00:17:34,240 --> 00:17:39,200
that will be less because now what we

00:17:37,200 --> 00:17:42,559
have to do is create doubling port

00:17:39,200 --> 00:17:45,440
representer and create the net dev

00:17:42,559 --> 00:17:47,120
bring it up and in some cases an rdma

00:17:45,440 --> 00:17:48,240
device also is there for the sub

00:17:47,120 --> 00:17:50,640
function

00:17:48,240 --> 00:17:52,400
so the essential latency is about

00:17:50,640 --> 00:17:54,080
creating these four devices and

00:17:52,400 --> 00:17:56,799
configuring it

00:17:54,080 --> 00:17:58,880
and in in in cases where if the user

00:17:56,799 --> 00:18:00,880
still wants to use it as like a bulk

00:17:58,880 --> 00:18:01,760
creation and destruction then it can

00:18:00,880 --> 00:18:04,640
still do it

00:18:01,760 --> 00:18:08,320
like an sri or v mode where user can

00:18:04,640 --> 00:18:10,160
create 128 or 256 of functions up front

00:18:08,320 --> 00:18:15,840
and then provision one by one to the

00:18:10,160 --> 00:18:15,840
container at runtime

00:18:18,000 --> 00:18:23,760
yeah that may make more sense because uh

00:18:21,919 --> 00:18:26,799
if you're bringing this every time your

00:18:23,760 --> 00:18:29,200
cni gets involved

00:18:26,799 --> 00:18:32,240
and it costs you even milliseconds it's

00:18:29,200 --> 00:18:32,240
it's not cheap right

00:18:32,320 --> 00:18:37,840
because it limits how many how many how

00:18:34,720 --> 00:18:41,200
many ports you can bring up per second

00:18:37,840 --> 00:18:43,039
that's right yeah well

00:18:41,200 --> 00:18:44,880
i mean there is but usually you know it

00:18:43,039 --> 00:18:46,720
take time to bring a

00:18:44,880 --> 00:18:48,080
a container it's not like it's tech you

00:18:46,720 --> 00:18:51,679
you're bringing

00:18:48,080 --> 00:18:53,280
10 10 containers in a second

00:18:51,679 --> 00:18:54,880
uh you should be able to do faster than

00:18:53,280 --> 00:18:58,480
that it defines your scale

00:18:54,880 --> 00:19:00,480
no so i believe that there are

00:18:58,480 --> 00:19:02,000
other things that limit not just the

00:19:00,480 --> 00:19:03,679
networking creation

00:19:02,000 --> 00:19:05,840
i i don't disagree with you i think

00:19:03,679 --> 00:19:07,600
there's all this crap like etcd that

00:19:05,840 --> 00:19:09,760
will slow you down more than this but

00:19:07,600 --> 00:19:11,280
you know you always try to be faster

00:19:09,760 --> 00:19:14,240
because if you have a sub

00:19:11,280 --> 00:19:15,280
sub function talk let's convert these

00:19:14,240 --> 00:19:18,320
questions to them

00:19:15,280 --> 00:19:19,760
because let's talk a little bit about

00:19:18,320 --> 00:19:22,160
or if you have questions about this

00:19:19,760 --> 00:19:23,919
particular topic which is

00:19:22,160 --> 00:19:26,320
how this got integrated into the

00:19:23,919 --> 00:19:28,160
kubernetes infrastructure let's

00:19:26,320 --> 00:19:31,919
limit those questions now since we are

00:19:28,160 --> 00:19:31,919

YouTube URL: https://www.youtube.com/watch?v=g7D4Xj0klqE


