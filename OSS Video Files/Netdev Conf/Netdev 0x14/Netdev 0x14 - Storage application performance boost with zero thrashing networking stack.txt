Title: Netdev 0x14 - Storage application performance boost with zero thrashing networking stack
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Or Gerlitz

More info: https://netdevconf.info/0x14/session.html?talk-storage-application-performance-boost-with-zero-thrashing-networking-stack

Date: Wednesday, August 19, 2020

There has been an ongoing effort in the community to get Zero-copy working over
the last few years - with much success. While both MSG_ZEROCOPY (zero copy transmit) and
socket mmap (zero copy receive) have been out in the wild for sometime now, there are
no known high scale application consumers of these interfaces that are open source.

In this talk, Or Gerlitz will describe how he makes use of these interfaces to integrate
into spdk(https://spdk.io/) - an open source storage framework which uses sockets for nvme-over-tcp
in a smart NIC environment. The goal is to use kernel uAPIs while achieving high scale performance.

Or will delve into MSG_ZEROCPOY and challenges involved that he had to deal with. He will describe
the need for an app author to understand and how to tie-in their app state machine to the interfaces:
to address both transactions responses from the peer app and zero-copy notifications from the local socket
provider. Should the app use ZC with all or nothing approach or sometimes
yes and other-times no? Come to the talk to get the answer.

In addition Or will spend time going into the performance analysis details correlating
I/O performance visavis CPU cost (which is often tricky to get right with traditional tools like
profiler/flame-graphs).
Captions: 
	00:00:01,839 --> 00:00:06,000
so my name is orga elits from metal dogs

00:00:04,319 --> 00:00:07,759
and i'll talk today about storage

00:00:06,000 --> 00:00:10,559
application performance boost with zero

00:00:07,759 --> 00:00:13,519
thrashing network stack

00:00:10,559 --> 00:00:15,679
uh what would the motivation for this uh

00:00:13,519 --> 00:00:18,400
performance study that we did

00:00:15,679 --> 00:00:19,600
is um something that has to do with nvme

00:00:18,400 --> 00:00:22,640
over tcp

00:00:19,600 --> 00:00:24,160
uh nvme over tcp is an nvme of a fabrics

00:00:22,640 --> 00:00:25,599
protocol that

00:00:24,160 --> 00:00:27,920
was discussed in this conference last

00:00:25,599 --> 00:00:30,560
year and also this year will be a few

00:00:27,920 --> 00:00:33,600
sessions of that and uh we noted that

00:00:30,560 --> 00:00:36,079
the overhead related to copy

00:00:33,600 --> 00:00:36,800
is something you can easily see which

00:00:36,079 --> 00:00:40,559
covers

00:00:36,800 --> 00:00:43,200
um takes lots of of the cpu

00:00:40,559 --> 00:00:44,000
cycles of the protocol and this actually

00:00:43,200 --> 00:00:47,680
has larger

00:00:44,000 --> 00:00:49,680
impact um that was the

00:00:47,680 --> 00:00:51,600
uh something we wanted to check if

00:00:49,680 --> 00:00:54,640
indeed this has larger impact on

00:00:51,600 --> 00:00:57,600
smartnic system ownership systems

00:00:54,640 --> 00:00:58,079
which tend to have more limited caching

00:00:57,600 --> 00:01:02,160
and mirror

00:00:58,079 --> 00:01:04,799
systems uh caching and mirror systems

00:01:02,160 --> 00:01:05,840
uh and the idea in this work was to

00:01:04,799 --> 00:01:07,360
apply

00:01:05,840 --> 00:01:08,960
some of the new apis that were

00:01:07,360 --> 00:01:10,840
introduced in linux in the last year for

00:01:08,960 --> 00:01:13,520
zero copying

00:01:10,840 --> 00:01:16,000
uh to optimize the the

00:01:13,520 --> 00:01:17,200
uh this workload in an environment of

00:01:16,000 --> 00:01:20,000
smartnic which does

00:01:17,200 --> 00:01:20,560
nvme emulation and i will elaborate on

00:01:20,000 --> 00:01:24,640
that

00:01:20,560 --> 00:01:28,159
later um first i'd like to go

00:01:24,640 --> 00:01:30,560
uh over over the models that

00:01:28,159 --> 00:01:31,280
the general motors that could be used

00:01:30,560 --> 00:01:34,000
for

00:01:31,280 --> 00:01:36,079
tcp base zero copy it can be applied

00:01:34,000 --> 00:01:38,320
actually to

00:01:36,079 --> 00:01:40,159
any socket application also for instance

00:01:38,320 --> 00:01:43,439
udp but the focus here was

00:01:40,159 --> 00:01:46,000
a tcp so on the transmission side

00:01:43,439 --> 00:01:47,360
how would a typical zero copy workload

00:01:46,000 --> 00:01:49,680
would look like

00:01:47,360 --> 00:01:51,280
um the application buffer has to be

00:01:49,680 --> 00:01:54,479
pinned for the time of the

00:01:51,280 --> 00:01:57,920
transmit plus a plus acknowledgement

00:01:54,479 --> 00:02:01,360
and after this stuff is done

00:01:57,920 --> 00:02:03,200
the the system the stack has to provide

00:02:01,360 --> 00:02:04,880
some sort of buffer reclaim notification

00:02:03,200 --> 00:02:07,280
to the application so they know

00:02:04,880 --> 00:02:09,119
they can reuse their buffers this

00:02:07,280 --> 00:02:12,560
typically does not require

00:02:09,119 --> 00:02:16,080
special nick however assistance

00:02:12,560 --> 00:02:17,760
on the receive side generally we can say

00:02:16,080 --> 00:02:21,120
there are two models

00:02:17,760 --> 00:02:24,000
on the first model the driver packet

00:02:21,120 --> 00:02:25,599
is provided to the application plus a

00:02:24,000 --> 00:02:27,920
way for the application to recycle

00:02:25,599 --> 00:02:29,680
buffers after they use them

00:02:27,920 --> 00:02:32,319
in the second model the application

00:02:29,680 --> 00:02:34,319
buffer is given to the driver for packet

00:02:32,319 --> 00:02:37,200
receiving

00:02:34,319 --> 00:02:39,680
this typically does required some

00:02:37,200 --> 00:02:42,879
hardware assistance from the nic

00:02:39,680 --> 00:02:45,120
for model number one um

00:02:42,879 --> 00:02:47,840
what one needs typically is a header

00:02:45,120 --> 00:02:50,319
data splits also called hds

00:02:47,840 --> 00:02:52,000
and in the second model some sort of

00:02:50,319 --> 00:02:56,160
direct data placements or dap

00:02:52,000 --> 00:02:59,200
is needed our present my presentation

00:02:56,160 --> 00:03:01,920
today will focus on transmit

00:02:59,200 --> 00:03:01,920
zero copy

00:03:02,560 --> 00:03:07,599
but i will now briefly also sketch for

00:03:05,120 --> 00:03:10,800
both sides both receive and transmit

00:03:07,599 --> 00:03:13,280
what do we have in linux so for transmit

00:03:10,800 --> 00:03:15,360
the send basic system call was extended

00:03:13,280 --> 00:03:16,400
with a flag called message zero copy it

00:03:15,360 --> 00:03:18,159
was done

00:03:16,400 --> 00:03:20,400
it was a work by google and william de

00:03:18,159 --> 00:03:23,519
bruyne in kernel 414

00:03:20,400 --> 00:03:24,959
and was presented in netdev 2.1 on the

00:03:23,519 --> 00:03:26,959
received side

00:03:24,959 --> 00:03:28,640
also from google and eric domizett on

00:03:26,959 --> 00:03:30,879
kernel 418

00:03:28,640 --> 00:03:32,400
and up support for socket which is model

00:03:30,879 --> 00:03:34,560
number one

00:03:32,400 --> 00:03:37,040
uh and this will be presented in this

00:03:34,560 --> 00:03:37,040
conference

00:03:37,120 --> 00:03:42,319
continuing uh in our um

00:03:40,400 --> 00:03:44,640
in the past to what we were focused in

00:03:42,319 --> 00:03:46,000
this work i would like to

00:03:44,640 --> 00:03:49,599
talk about what are the different

00:03:46,000 --> 00:03:52,720
layering options for nvme over tcp

00:03:49,599 --> 00:03:55,120
so nvme of a tcp

00:03:52,720 --> 00:03:57,120
is a driver which we can think of also

00:03:55,120 --> 00:03:57,599
in our application for zero copy in this

00:03:57,120 --> 00:04:00,000
uh

00:03:57,599 --> 00:04:01,680
performance study so we have the nvme

00:04:00,000 --> 00:04:04,400
tcp driver

00:04:01,680 --> 00:04:05,920
which is layered on top of the tcp stack

00:04:04,400 --> 00:04:09,680
and then the nic driver

00:04:05,920 --> 00:04:11,360
we have generally speaking three options

00:04:09,680 --> 00:04:15,120
in the first one everyone is in the

00:04:11,360 --> 00:04:17,040
kernel both the driver and the tcp stack

00:04:15,120 --> 00:04:20,400
and the nic driver

00:04:17,040 --> 00:04:22,800
this is more or less the more historical

00:04:20,400 --> 00:04:24,400
or conventional

00:04:22,800 --> 00:04:28,000
setup for storage stack all in the

00:04:24,400 --> 00:04:31,360
kernel and this uses in-kernel sockets

00:04:28,000 --> 00:04:32,240
the sockets that are widely used on user

00:04:31,360 --> 00:04:34,960
space application

00:04:32,240 --> 00:04:36,800
also have a kernel counterparts which

00:04:34,960 --> 00:04:38,240
are less known that are work for many

00:04:36,800 --> 00:04:40,000
many years typically in the storage

00:04:38,240 --> 00:04:42,000
space

00:04:40,000 --> 00:04:43,440
the second option is a combined one we

00:04:42,000 --> 00:04:48,720
have a user space

00:04:43,440 --> 00:04:50,479
nvme tcp driver that uses sockets

00:04:48,720 --> 00:04:51,759
and when getting into the kernel goes to

00:04:50,479 --> 00:04:54,320
the tcp stack

00:04:51,759 --> 00:04:55,759
and nick driver the third option which

00:04:54,320 --> 00:04:59,680
might be

00:04:55,759 --> 00:05:02,560
less common but or more

00:04:59,680 --> 00:05:03,280
less common is the one we are using in

00:05:02,560 --> 00:05:05,680
our work

00:05:03,280 --> 00:05:06,320
where everybody is in user space the

00:05:05,680 --> 00:05:09,759
nvme

00:05:06,320 --> 00:05:11,039
tcp driver the tcp stack and the nic

00:05:09,759 --> 00:05:15,520
driver

00:05:11,039 --> 00:05:18,000
so this tcp stack has to be um

00:05:15,520 --> 00:05:19,680
api or interchanged by a socket offshore

00:05:18,000 --> 00:05:22,080
technique i'll elaborate

00:05:19,680 --> 00:05:23,919
in a minute on that and the nic driver

00:05:22,080 --> 00:05:25,759
can be either the use of space next

00:05:23,919 --> 00:05:27,199
driver can be either part of this tcp

00:05:25,759 --> 00:05:31,520
stack

00:05:27,199 --> 00:05:31,520
or based on dpdk

00:05:33,199 --> 00:05:38,479
for this work we used storage stack

00:05:36,639 --> 00:05:40,000
which is called spdk the storage

00:05:38,479 --> 00:05:43,120
performance development kit

00:05:40,000 --> 00:05:44,880
it's a storage stack all user space that

00:05:43,120 --> 00:05:47,280
was reduced in the last uh

00:05:44,880 --> 00:05:49,680
four years by intel and it's an open

00:05:47,280 --> 00:05:53,360
source high performance scalable

00:05:49,680 --> 00:05:56,160
fully user mode stack and for the socket

00:05:53,360 --> 00:05:58,880
for the tcp stack in user space we used

00:05:56,160 --> 00:06:01,600
a package from melanox called vma

00:05:58,880 --> 00:06:03,440
which is melox messaging accelerator

00:06:01,600 --> 00:06:04,880
it's an open source

00:06:03,440 --> 00:06:07,199
circuit of library that allowed

00:06:04,880 --> 00:06:09,440
non-modified applications

00:06:07,199 --> 00:06:11,120
written over socket api to run from user

00:06:09,440 --> 00:06:11,600
space with a full kernel and network

00:06:11,120 --> 00:06:14,800
stack

00:06:11,600 --> 00:06:18,560
bypass and this uses something called

00:06:14,800 --> 00:06:21,440
ld preload it's a way to

00:06:18,560 --> 00:06:22,160
load the symbols of this library into

00:06:21,440 --> 00:06:24,560
the

00:06:22,160 --> 00:06:26,000
space of the of the application process

00:06:24,560 --> 00:06:27,039
and then all the calls of these

00:06:26,000 --> 00:06:28,560
applications to

00:06:27,039 --> 00:06:30,319
conventional socket system calls are

00:06:28,560 --> 00:06:32,800
hooked into this library

00:06:30,319 --> 00:06:35,919
it's a standard way to do this stuff and

00:06:32,800 --> 00:06:39,039
this way the application is not modified

00:06:35,919 --> 00:06:40,960
so uh looking in what we are

00:06:39,039 --> 00:06:42,319
the workload we actually came to

00:06:40,960 --> 00:06:45,840
optimize

00:06:42,319 --> 00:06:48,639
um is something called nvme emulation

00:06:45,840 --> 00:06:50,880
it's a technique or a technology or a

00:06:48,639 --> 00:06:52,400
product by melanox called snap but nbn

00:06:50,880 --> 00:06:55,039
emulation is something that

00:06:52,400 --> 00:06:56,880
we start to see from multiple vendors on

00:06:55,039 --> 00:06:59,039
the left hand side we see

00:06:56,880 --> 00:07:00,319
a server a linux server can be bare

00:06:59,039 --> 00:07:02,720
metal server that

00:07:00,319 --> 00:07:03,680
none of our software runs on it and this

00:07:02,720 --> 00:07:06,479
server

00:07:03,680 --> 00:07:07,360
tends to be x86 but it can be any server

00:07:06,479 --> 00:07:11,039
it has an

00:07:07,360 --> 00:07:14,000
emulated nvme nvme pci disk

00:07:11,039 --> 00:07:14,560
and then what i call here snap hardware

00:07:14,000 --> 00:07:18,800
uh

00:07:14,560 --> 00:07:20,800
is the holder that we are using to um

00:07:18,800 --> 00:07:22,800
create this emulation for the disk i

00:07:20,800 --> 00:07:23,599
called here snap holder and it's beyond

00:07:22,800 --> 00:07:26,800
the scope of this

00:07:23,599 --> 00:07:28,720
uh presentation and after the

00:07:26,800 --> 00:07:30,800
emulation is done the question comes

00:07:28,720 --> 00:07:33,599
what would be the back-end protocol

00:07:30,800 --> 00:07:34,720
how this how this emulated nvme disk

00:07:33,599 --> 00:07:37,759
traffic will go

00:07:34,720 --> 00:07:39,599
to the fabric assuming the actual disk

00:07:37,759 --> 00:07:41,599
is not on the soc

00:07:39,599 --> 00:07:43,840
in this case we have environment where

00:07:41,599 --> 00:07:44,960
the fabric traffic has to be nvme over

00:07:43,840 --> 00:07:47,599
tcp

00:07:44,960 --> 00:07:49,039
so how does this work after the

00:07:47,599 --> 00:07:52,800
emulation

00:07:49,039 --> 00:07:53,520
this nvme spooky or nvme traffic somehow

00:07:52,800 --> 00:07:56,000
gets

00:07:53,520 --> 00:07:58,240
into the system on a chip in the melomox

00:07:56,000 --> 00:08:00,319
case our arrow smart thing is called

00:07:58,240 --> 00:08:02,160
bluefield

00:08:00,319 --> 00:08:03,680
and it can be if bluefight one bluefield

00:08:02,160 --> 00:08:05,840
two and so on

00:08:03,680 --> 00:08:07,680
uh we have received this traffic in

00:08:05,840 --> 00:08:10,240
something i would call here a snapdemon

00:08:07,680 --> 00:08:13,680
and then our business for today starts

00:08:10,240 --> 00:08:14,479
we have spd initiator interacting with

00:08:13,680 --> 00:08:17,759
the vma

00:08:14,479 --> 00:08:18,319
socket offload library and then it goes

00:08:17,759 --> 00:08:21,440
over the

00:08:18,319 --> 00:08:23,280
the network with nvme over tcp and then

00:08:21,440 --> 00:08:25,680
we are interchanging with a remote

00:08:23,280 --> 00:08:26,720
target which can be on x86 server or

00:08:25,680 --> 00:08:29,840
whatever

00:08:26,720 --> 00:08:31,520
uh nvme of a tcp target

00:08:29,840 --> 00:08:33,279
and the point where implemented zero

00:08:31,520 --> 00:08:36,959
copy is exactly

00:08:33,279 --> 00:08:39,519
in this um linking between spdk

00:08:36,959 --> 00:08:40,800
tcp initiator to vma this is a place

00:08:39,519 --> 00:08:44,800
where we

00:08:40,800 --> 00:08:48,800
deployed the tcp zero copy

00:08:44,800 --> 00:08:50,640
the tcp zero copy what we will see today

00:08:48,800 --> 00:08:52,320
we'll see

00:08:50,640 --> 00:08:54,000
when we will see results some of the

00:08:52,320 --> 00:08:56,320
results were obtained by only

00:08:54,000 --> 00:08:58,959
running this initiator by some synthetic

00:08:56,320 --> 00:09:00,880
benchmark that runs on the soc

00:08:58,959 --> 00:09:02,240
and then we have also a result from

00:09:00,880 --> 00:09:04,000
running the full

00:09:02,240 --> 00:09:05,600
the full pass when we have some

00:09:04,000 --> 00:09:09,680
application on the server

00:09:05,600 --> 00:09:12,000
over the emulated pci nvme pci disk it

00:09:09,680 --> 00:09:14,959
goes to the snap demon to the spdk

00:09:12,000 --> 00:09:14,959
and to the target

00:09:15,120 --> 00:09:19,360
so what are the layers that are related

00:09:17,279 --> 00:09:21,200
to this work

00:09:19,360 --> 00:09:22,800
the letters that relate to this work are

00:09:21,200 --> 00:09:24,000
the one in the middle in the yellow

00:09:22,800 --> 00:09:26,320
brackets

00:09:24,000 --> 00:09:27,040
but just for a context on from above we

00:09:26,320 --> 00:09:29,360
have either

00:09:27,040 --> 00:09:31,120
the spdk perf which is the benchmark or

00:09:29,360 --> 00:09:34,000
the nvme emulation

00:09:31,120 --> 00:09:38,640
we call it proxy because we kind of

00:09:34,000 --> 00:09:41,519
proxy nvme emulated mdma pci to nvme tcp

00:09:38,640 --> 00:09:43,360
and in the middle we have spdk that spck

00:09:41,519 --> 00:09:45,600
is a storage stack so it has a blocked

00:09:43,360 --> 00:09:47,680
is a block layer and then nvme later and

00:09:45,600 --> 00:09:50,080
then a specific nvme tcp

00:09:47,680 --> 00:09:51,040
layer and then below that we have the

00:09:50,080 --> 00:09:53,440
vma which

00:09:51,040 --> 00:09:56,080
uh the vma library has both the tcpap

00:09:53,440 --> 00:09:58,480
stack and the nic driver for mlx5

00:09:56,080 --> 00:09:59,120
and below that we have the hardware so

00:09:58,480 --> 00:10:02,560
all this

00:09:59,120 --> 00:10:06,640
is um is a single process but i kind of

00:10:02,560 --> 00:10:06,640
sketch the different layers of software

00:10:06,800 --> 00:10:12,880
before going into the actual

00:10:10,320 --> 00:10:14,000
details of what we did here what were

00:10:12,880 --> 00:10:16,399
results

00:10:14,000 --> 00:10:17,440
and some sort of performance discussion

00:10:16,399 --> 00:10:20,079
i would like to go

00:10:17,440 --> 00:10:21,680
into the uh a bit into the details of

00:10:20,079 --> 00:10:24,800
the linux tx zero copy

00:10:21,680 --> 00:10:26,240
api uh this will serve me to later

00:10:24,800 --> 00:10:29,200
present the changes or the

00:10:26,240 --> 00:10:29,519
assumptions that we had so the workflow

00:10:29,200 --> 00:10:31,360
for

00:10:29,519 --> 00:10:33,279
applications to use linux stick zero

00:10:31,360 --> 00:10:36,320
copy

00:10:33,279 --> 00:10:38,399
is made of four four steps

00:10:36,320 --> 00:10:40,399
the first is is a discovery the

00:10:38,399 --> 00:10:43,279
application can realize but it gets

00:10:40,399 --> 00:10:44,880
up system call if zero copy is actually

00:10:43,279 --> 00:10:46,880
supported

00:10:44,880 --> 00:10:48,240
the second is a set sock opt which is

00:10:46,880 --> 00:10:50,320
just a general

00:10:48,240 --> 00:10:52,079
usage declaration applications that

00:10:50,320 --> 00:10:55,279
wants the socket provider

00:10:52,079 --> 00:10:57,200
to use it later they have to declare it

00:10:55,279 --> 00:10:59,519
but the actual usage is first system

00:10:57,200 --> 00:11:01,600
call per send message system call

00:10:59,519 --> 00:11:02,720
optionally it has it doesn't have to be

00:11:01,600 --> 00:11:04,640
all or nothing

00:11:02,720 --> 00:11:06,800
after you did the declaration optionally

00:11:04,640 --> 00:11:09,839
you can say on a given system call

00:11:06,800 --> 00:11:11,519
uh provide the zero copy flag and

00:11:09,839 --> 00:11:13,920
please note it's important that this

00:11:11,519 --> 00:11:15,600
sent this system call gets an i o vector

00:11:13,920 --> 00:11:16,800
so message header is actually an io

00:11:15,600 --> 00:11:18,959
vector

00:11:16,800 --> 00:11:21,120
which spans multiple discharge buffers

00:11:18,959 --> 00:11:22,800
in the example of nvme tcp initiator

00:11:21,120 --> 00:11:24,640
some of these buffer mates will be

00:11:22,800 --> 00:11:26,320
protocol headers what's called the nvme

00:11:24,640 --> 00:11:30,240
tcp pdu headers

00:11:26,320 --> 00:11:32,240
other ones can be data buffers um

00:11:30,240 --> 00:11:34,000
now comes the last part which is the the

00:11:32,240 --> 00:11:36,079
most involved one

00:11:34,000 --> 00:11:37,200
is the buffer reclaim notification we

00:11:36,079 --> 00:11:40,880
said that uh

00:11:37,200 --> 00:11:40,880
the socket provider has to

00:11:41,360 --> 00:11:46,079
has to um they will have to hand this

00:11:44,640 --> 00:11:48,000
buffer to the hardware

00:11:46,079 --> 00:11:49,120
so they would need to somehow register

00:11:48,000 --> 00:11:50,959
and pin it

00:11:49,120 --> 00:11:52,639
um and then when they are done with the

00:11:50,959 --> 00:11:54,480
buffer they should provide some

00:11:52,639 --> 00:11:56,079
notification to the application

00:11:54,480 --> 00:11:58,880
that they know that they can reuse

00:11:56,079 --> 00:12:01,519
buffer so the way it works is via the

00:11:58,880 --> 00:12:03,360
socket the socket error cue

00:12:01,519 --> 00:12:05,440
and then the deal is that both the

00:12:03,360 --> 00:12:07,360
application and the stack they maintain

00:12:05,440 --> 00:12:09,600
an invocation id counter which is

00:12:07,360 --> 00:12:11,360
incremented per system called invocation

00:12:09,600 --> 00:12:14,079
start from zero

00:12:11,360 --> 00:12:16,160
and then the notifications are provided

00:12:14,079 --> 00:12:17,839
in the form of a range a closed range

00:12:16,160 --> 00:12:20,560
between low to high let's say

00:12:17,839 --> 00:12:21,680
six to eleven which means six seven

00:12:20,560 --> 00:12:24,320
eight nine ten eleven

00:12:21,680 --> 00:12:25,519
and so on range of acknowledged iov

00:12:24,320 --> 00:12:29,279
buffer ids

00:12:25,519 --> 00:12:30,959
so it's an id that represents a a set of

00:12:29,279 --> 00:12:32,720
buffers

00:12:30,959 --> 00:12:34,639
and this is the agreement between the

00:12:32,720 --> 00:12:36,639
the application to the stack

00:12:34,639 --> 00:12:39,839
uh both income and the counter and the

00:12:36,639 --> 00:12:43,040
stack provided notifications

00:12:39,839 --> 00:12:44,160
um now starting to dive into our actual

00:12:43,040 --> 00:12:46,000
environment

00:12:44,160 --> 00:12:48,000
when we come to this this zero copy

00:12:46,000 --> 00:12:50,560
performance data

00:12:48,000 --> 00:12:51,680
we took two points of the protocol and

00:12:50,560 --> 00:12:53,760
we actually

00:12:51,680 --> 00:12:54,880
adjusted them to optimize the

00:12:53,760 --> 00:12:57,040
performance

00:12:54,880 --> 00:12:58,480
the first has to do with the zero copy

00:12:57,040 --> 00:13:01,200
buffer pinning

00:12:58,480 --> 00:13:01,839
so we wanted to amortize the cost of

00:13:01,200 --> 00:13:03,680
doing

00:13:01,839 --> 00:13:06,399
pinning and unpinning for every system

00:13:03,680 --> 00:13:08,560
call and the idea was that

00:13:06,399 --> 00:13:10,160
uh was made from two building blocks the

00:13:08,560 --> 00:13:12,800
first is that

00:13:10,160 --> 00:13:14,959
in spdk dpdk social packages there is

00:13:12,800 --> 00:13:16,959
the concept of dma buffers dna buffers

00:13:14,959 --> 00:13:19,680
are typically huge page based

00:13:16,959 --> 00:13:21,440
so we wanted to make sure first that all

00:13:19,680 --> 00:13:21,920
the traffic used by this application

00:13:21,440 --> 00:13:24,959
that this

00:13:21,920 --> 00:13:27,600
nvme tcp initiator is

00:13:24,959 --> 00:13:29,279
originated from dma buffers so we had to

00:13:27,600 --> 00:13:31,920
do some patch for that

00:13:29,279 --> 00:13:33,680
and the second one everything is made of

00:13:31,920 --> 00:13:36,240
huge pages

00:13:33,680 --> 00:13:36,959
buffers that reside within use pages we

00:13:36,240 --> 00:13:38,880
did a

00:13:36,959 --> 00:13:40,240
holder registration cache in the socket

00:13:38,880 --> 00:13:41,920
provider layer

00:13:40,240 --> 00:13:44,800
so when the socket provider layer they

00:13:41,920 --> 00:13:47,600
see a buffer they quickly mask it

00:13:44,800 --> 00:13:49,360
to the base of this huge face size and

00:13:47,600 --> 00:13:51,040
they and they look in their cash if they

00:13:49,360 --> 00:13:51,760
have a heat for registration they use

00:13:51,040 --> 00:13:53,519
the

00:13:51,760 --> 00:13:56,399
registration if not they register and

00:13:53,519 --> 00:13:59,440
put it in the cash this way the cost of

00:13:56,399 --> 00:13:59,920
pinning unpinning is amortized the

00:13:59,440 --> 00:14:01,600
second

00:13:59,920 --> 00:14:03,199
has to do is the buffer reclaim

00:14:01,600 --> 00:14:05,920
notifications

00:14:03,199 --> 00:14:06,720
so uh this buffer require notification

00:14:05,920 --> 00:14:08,720
they

00:14:06,720 --> 00:14:10,399
they actually add some overhead to the

00:14:08,720 --> 00:14:11,600
application and they can even cause some

00:14:10,399 --> 00:14:14,160
slowdown

00:14:11,600 --> 00:14:15,440
uh due to overcomplexity in the state

00:14:14,160 --> 00:14:16,959
machine and waiting for those

00:14:15,440 --> 00:14:20,560
notification

00:14:16,959 --> 00:14:21,519
so the idea here was that nvme tcp

00:14:20,560 --> 00:14:24,000
protocol like

00:14:21,519 --> 00:14:25,120
virtual maybe every or almost all

00:14:24,000 --> 00:14:27,920
storage protocols

00:14:25,120 --> 00:14:29,760
is transactional so on the initial side

00:14:27,920 --> 00:14:31,680
we initiate a transaction

00:14:29,760 --> 00:14:33,120
we met we passed the data and then we

00:14:31,680 --> 00:14:35,199
get a response

00:14:33,120 --> 00:14:37,279
by the time we get the response we can

00:14:35,199 --> 00:14:38,399
be sure that the traffic arrived to the

00:14:37,279 --> 00:14:40,639
target

00:14:38,399 --> 00:14:42,000
so we can actually simply skip those

00:14:40,639 --> 00:14:45,120
notifications

00:14:42,000 --> 00:14:47,839
uh and correct it will will remain

00:14:45,120 --> 00:14:48,240
one one bit to handle here is to think

00:14:47,839 --> 00:14:50,880
about

00:14:48,240 --> 00:14:51,680
the case of of lost acknowledgement so

00:14:50,880 --> 00:14:53,839
we send it

00:14:51,680 --> 00:14:55,600
we send the data to the target the

00:14:53,839 --> 00:14:57,600
target send us an acknowledgement but

00:14:55,600 --> 00:15:00,480
this acknowledgement was lost

00:14:57,600 --> 00:15:01,839
so it's theoretically possible that the

00:15:00,480 --> 00:15:04,720
local stack

00:15:01,839 --> 00:15:05,120
will try to retransmit this buffer again

00:15:04,720 --> 00:15:07,040
but

00:15:05,120 --> 00:15:08,399
even if this happens there are two

00:15:07,040 --> 00:15:10,480
points to note here

00:15:08,399 --> 00:15:13,440
first the stack has read only access to

00:15:10,480 --> 00:15:15,760
this buffer because it's transmit

00:15:13,440 --> 00:15:16,800
and the second one is that this buffer

00:15:15,760 --> 00:15:18,959
is safely reside

00:15:16,800 --> 00:15:20,320
this this buffer safely resides in

00:15:18,959 --> 00:15:21,920
addressable memory

00:15:20,320 --> 00:15:24,560
because use pages in linux are

00:15:21,920 --> 00:15:28,240
practically pinned so overall

00:15:24,560 --> 00:15:30,639
we are safe and we um and we could uh

00:15:28,240 --> 00:15:31,440
do this uh trick so to sum up and it's

00:15:30,639 --> 00:15:34,079
important

00:15:31,440 --> 00:15:35,680
for this performance study we used a pin

00:15:34,079 --> 00:15:36,720
down cache at the socket provider

00:15:35,680 --> 00:15:39,519
provider layer

00:15:36,720 --> 00:15:43,680
for the buffer pinning and we skipped

00:15:39,519 --> 00:15:45,600
zero copy buffer reclaim notifications

00:15:43,680 --> 00:15:47,839
the implementation itself was not

00:15:45,600 --> 00:15:50,560
something to do too much elaborate on in

00:15:47,839 --> 00:15:52,720
the in spdk it was trivial

00:15:50,560 --> 00:15:54,399
because we skipped the notifications so

00:15:52,720 --> 00:15:57,040
we only had to do

00:15:54,399 --> 00:15:58,240
uh a patch that make sure that everyone

00:15:57,040 --> 00:16:00,079
uses dma

00:15:58,240 --> 00:16:02,959
memory both the protocols header and the

00:16:00,079 --> 00:16:04,480
data buffers and of course use the flag

00:16:02,959 --> 00:16:06,880
for the same message call and also the

00:16:04,480 --> 00:16:09,759
get sock opt and set soft and so on

00:16:06,880 --> 00:16:10,320
and in the vma library it was miss

00:16:09,759 --> 00:16:12,399
changes

00:16:10,320 --> 00:16:13,360
around the place because you take a tcp

00:16:12,399 --> 00:16:15,680
stack

00:16:13,360 --> 00:16:16,399
uh it's a compact one but still and you

00:16:15,680 --> 00:16:18,639
want to

00:16:16,399 --> 00:16:20,480
uh have this tax know how to work tcp

00:16:18,639 --> 00:16:23,600
stacks tend to do lots of copies

00:16:20,480 --> 00:16:25,040
so you have to modify them at least when

00:16:23,600 --> 00:16:27,519
this flag is provided

00:16:25,040 --> 00:16:29,839
not to do any copies in the older flows

00:16:27,519 --> 00:16:32,079
and we added also the huge page

00:16:29,839 --> 00:16:33,600
uh base registration cache for zero copy

00:16:32,079 --> 00:16:35,759
buffers

00:16:33,600 --> 00:16:37,759
regarding the testing setup before i go

00:16:35,759 --> 00:16:41,600
into the results and say a few words

00:16:37,759 --> 00:16:42,800
so um the the software on the initiator

00:16:41,600 --> 00:16:45,519
on the smart nic

00:16:42,800 --> 00:16:46,240
was spdk 1907. this work has been

00:16:45,519 --> 00:16:50,160
actually done

00:16:46,240 --> 00:16:53,279
last last autumn even before corona time

00:16:50,160 --> 00:16:56,560
um and um so it's

00:16:53,279 --> 00:16:58,160
spdk 1907 dma whatever plus those

00:16:56,560 --> 00:17:01,040
patches and the smart decrypt

00:16:58,160 --> 00:17:02,240
runs seven center seven six the small

00:17:01,040 --> 00:17:05,439
tech itself

00:17:02,240 --> 00:17:07,039
was bluefield one 100 um 100 gig

00:17:05,439 --> 00:17:09,520
megabytes in this specific

00:17:07,039 --> 00:17:10,079
instance we have few few models this

00:17:09,520 --> 00:17:12,799
specific

00:17:10,079 --> 00:17:14,559
instance has 16 a7 arm aces and 2 cores

00:17:12,799 --> 00:17:16,640
at 800 megahertz

00:17:14,559 --> 00:17:18,799
and the target details the target was a

00:17:16,640 --> 00:17:21,120
remote x86 node

00:17:18,799 --> 00:17:21,839
with those details it's not a bottleneck

00:17:21,120 --> 00:17:25,280
and it's not

00:17:21,839 --> 00:17:27,360
um it's not something to to focus on it

00:17:25,280 --> 00:17:28,880
was a it was a null backing store

00:17:27,360 --> 00:17:32,960
because we wanted to just see the

00:17:28,880 --> 00:17:35,280
improvement in the network protocol and

00:17:32,960 --> 00:17:37,039
what were the results and the row the

00:17:35,280 --> 00:17:38,480
workloads and the raw results the first

00:17:37,039 --> 00:17:40,720
one is a benchmark

00:17:38,480 --> 00:17:41,919
as i said spdk proof that we run it from

00:17:40,720 --> 00:17:44,480
the soc

00:17:41,919 --> 00:17:47,440
we of course do a route right because

00:17:44,480 --> 00:17:49,919
it's a tx0 copy with the qrdx of 64.

00:17:47,440 --> 00:17:52,320
and then we had two workloads in the

00:17:49,919 --> 00:17:55,120
first one we varied the number of cores

00:17:52,320 --> 00:17:56,720
between 1 2 and 15. by the way we

00:17:55,120 --> 00:17:58,000
stopped at 15 because in this

00:17:56,720 --> 00:18:01,039
implementation

00:17:58,000 --> 00:18:03,600
vma stack needs one core for their tcp

00:18:01,039 --> 00:18:04,880
progress thread okay that's why we we

00:18:03,600 --> 00:18:08,240
could use for the application

00:18:04,880 --> 00:18:09,600
only 15 cores in the other case we uh

00:18:08,240 --> 00:18:11,440
we were running on the phone number of

00:18:09,600 --> 00:18:13,520
course but we were varying the

00:18:11,440 --> 00:18:15,200
io size between five five twelve bytes

00:18:13,520 --> 00:18:18,080
to 4k

00:18:15,200 --> 00:18:19,600
and um what were the results for one

00:18:18,080 --> 00:18:20,559
called the non-zero copy gave us a

00:18:19,600 --> 00:18:24,240
baseline number of

00:18:20,559 --> 00:18:26,320
85 85k co iops and zero copy has

00:18:24,240 --> 00:18:28,480
a few times percentages of improvement

00:18:26,320 --> 00:18:31,520
for 120k

00:18:28,480 --> 00:18:33,679
in the 15 call we got a very surprising

00:18:31,520 --> 00:18:38,080
result so we got about 100

00:18:33,679 --> 00:18:40,559
improvement from 65k to 1 to 1.2 million

00:18:38,080 --> 00:18:41,760
just to note the golden number in this

00:18:40,559 --> 00:18:43,840
configuration for

00:18:41,760 --> 00:18:46,000
if you do the mass for 4 kilobytes over

00:18:43,840 --> 00:18:48,880
a 100 gig link you need to get to

00:18:46,000 --> 00:18:52,000
2.7 million iops so even with this

00:18:48,880 --> 00:18:56,000
experiment we're only

00:18:52,000 --> 00:18:58,799
almost halfway a much more interesting

00:18:56,000 --> 00:18:59,120
result was for the full emulation when

00:18:58,799 --> 00:19:00,960
we

00:18:59,120 --> 00:19:02,160
run the traffic from the hosts through

00:19:00,960 --> 00:19:05,840
the emulated

00:19:02,160 --> 00:19:08,400
nvme pci through the snap and everything

00:19:05,840 --> 00:19:12,799
so here again we got a 100 improvement

00:19:08,400 --> 00:19:16,080
for 500 kiosks to 1 million alps

00:19:12,799 --> 00:19:19,360
which is a bit of impressive and now we

00:19:16,080 --> 00:19:22,960
have to try and understand so how come

00:19:19,360 --> 00:19:24,000
we got there let's see some graphs

00:19:22,960 --> 00:19:27,679
before diving into

00:19:24,000 --> 00:19:30,640
the results so here in this graph

00:19:27,679 --> 00:19:32,640
we see how scalability goes so the

00:19:30,640 --> 00:19:34,240
baseline here the orange one is referred

00:19:32,640 --> 00:19:37,039
to as tso

00:19:34,240 --> 00:19:38,880
it's uh uh it's the baseline of the code

00:19:37,039 --> 00:19:42,000
that does also tso and then

00:19:38,880 --> 00:19:44,960
the black one is tso plus zero copy we

00:19:42,000 --> 00:19:46,880
we can see that for zero copy uh the

00:19:44,960 --> 00:19:49,600
increase in iops is pretty much

00:19:46,880 --> 00:19:52,480
linear word for the non-zero copy after

00:19:49,600 --> 00:19:55,840
eight calls scalability stops

00:19:52,480 --> 00:19:57,520
for the latency we see that the orange

00:19:55,840 --> 00:19:58,880
one has also a linear increase in

00:19:57,520 --> 00:20:01,919
latency more or less

00:19:58,880 --> 00:20:03,280
after four calls and the the black the

00:20:01,919 --> 00:20:05,039
zero copy

00:20:03,280 --> 00:20:06,559
has some increase in latency but it's

00:20:05,039 --> 00:20:09,039
much more moderate

00:20:06,559 --> 00:20:10,480
the absolute number for the latency are

00:20:09,039 --> 00:20:12,320
a bit of high here

00:20:10,480 --> 00:20:14,000
and this is due to curing effects

00:20:12,320 --> 00:20:16,400
because the q index we are testing with

00:20:14,000 --> 00:20:17,840
was 64. so we see here numbers for qlips

00:20:16,400 --> 00:20:21,360
of 64.

00:20:17,840 --> 00:20:22,000
the second graph is on 15 course and now

00:20:21,360 --> 00:20:25,280
we take

00:20:22,000 --> 00:20:26,640
a few i o sizes starting from 512 bytes

00:20:25,280 --> 00:20:29,039
to 4k

00:20:26,640 --> 00:20:29,760
we can see that for the non-zero copy

00:20:29,039 --> 00:20:32,320
the drop

00:20:29,760 --> 00:20:33,280
in performance is linear for the zero

00:20:32,320 --> 00:20:36,000
copy the drop

00:20:33,280 --> 00:20:39,360
is more moderate and same for the

00:20:36,000 --> 00:20:43,280
increase in latency for both options

00:20:39,360 --> 00:20:45,760
now um when coming to try and realize

00:20:43,280 --> 00:20:47,840
uh or analyzing the the results we get

00:20:45,760 --> 00:20:51,120
and even during the work

00:20:47,840 --> 00:20:53,520
people typically do and so so did us

00:20:51,120 --> 00:20:55,360
running perf and getting flame graphs

00:20:53,520 --> 00:20:58,000
for the cpu cycles

00:20:55,360 --> 00:20:58,960
so what we see here we see the non-zero

00:20:58,000 --> 00:21:01,120
copy

00:20:58,960 --> 00:21:02,880
the first is the distribution for one

00:21:01,120 --> 00:21:06,880
call and then the distribution

00:21:02,880 --> 00:21:10,400
for uh 15 crores we can see that there

00:21:06,880 --> 00:21:13,440
there's a notable copy cost

00:21:10,400 --> 00:21:16,080
which actually uh increases not

00:21:13,440 --> 00:21:18,240
dramatically but it increases from

00:21:16,080 --> 00:21:20,159
one core to 15 core the cost of copy

00:21:18,240 --> 00:21:23,200
increase

00:21:20,159 --> 00:21:23,440
in the zero copy case uh we eliminated

00:21:23,200 --> 00:21:25,440
the

00:21:23,440 --> 00:21:26,640
we eliminated the copy and the cost is

00:21:25,440 --> 00:21:28,880
also eliminated

00:21:26,640 --> 00:21:30,240
and we have new battle net or new

00:21:28,880 --> 00:21:33,360
battles to to fight

00:21:30,240 --> 00:21:36,400
beyond the scope of this uh work

00:21:33,360 --> 00:21:38,080
now and here comes the kind of the heart

00:21:36,400 --> 00:21:39,919
of my presentation

00:21:38,080 --> 00:21:42,799
so what makes your copy so much

00:21:39,919 --> 00:21:44,799
efficient we try to look into that

00:21:42,799 --> 00:21:46,559
what we did we we looked on various

00:21:44,799 --> 00:21:48,240
low-level statistics

00:21:46,559 --> 00:21:50,559
some you can get prepared for some we

00:21:48,240 --> 00:21:53,679
had to take from a hydro monitors

00:21:50,559 --> 00:21:56,400
hardware a monster driver so

00:21:53,679 --> 00:21:58,480
um if we look on caching which is the

00:21:56,400 --> 00:22:01,919
focus of this uh work

00:21:58,480 --> 00:22:04,640
so uh typically has system has um

00:22:01,919 --> 00:22:06,240
l1 l2 and last level cache at least

00:22:04,640 --> 00:22:09,120
three levels of caches

00:22:06,240 --> 00:22:10,960
the first cache in our soc but also many

00:22:09,120 --> 00:22:14,080
other system is very small

00:22:10,960 --> 00:22:15,200
and it tends to be occupied by local

00:22:14,080 --> 00:22:18,559
variables and

00:22:15,200 --> 00:22:19,919
not much but beyond that the second

00:22:18,559 --> 00:22:21,840
cache

00:22:19,919 --> 00:22:23,360
has to has typically the software

00:22:21,840 --> 00:22:26,880
context or the metadata

00:22:23,360 --> 00:22:30,320
for the for the for the transactions

00:22:26,880 --> 00:22:32,400
uh this has some relevancy but uh in

00:22:30,320 --> 00:22:35,039
storage workload

00:22:32,400 --> 00:22:36,559
less relevancy that it would have uh on

00:22:35,039 --> 00:22:38,320
packet processing workload

00:22:36,559 --> 00:22:40,799
because in packet processing workload

00:22:38,320 --> 00:22:42,559
you can have 100 or 200 million

00:22:40,799 --> 00:22:44,159
packets per second so each packet

00:22:42,559 --> 00:22:45,600
carries their metadata

00:22:44,159 --> 00:22:47,440
and it's really important to have this

00:22:45,600 --> 00:22:48,960
in l2 cache at least

00:22:47,440 --> 00:22:50,640
where in storage you the order of

00:22:48,960 --> 00:22:52,159
transactions is let's say 1 million 2

00:22:50,640 --> 00:22:55,600
million 5 million

00:22:52,159 --> 00:22:56,640
so um it's less of um of critical aspect

00:22:55,600 --> 00:22:59,679
to this um

00:22:56,640 --> 00:23:02,720
to storage workloads in my opinion

00:22:59,679 --> 00:23:04,880
the last cache is uh is critical in our

00:23:02,720 --> 00:23:08,000
soc and maybe also in others so

00:23:04,880 --> 00:23:10,000
the data the the the access to

00:23:08,000 --> 00:23:12,720
that to data which is in the last level

00:23:10,000 --> 00:23:13,679
cache versus access to data which is the

00:23:12,720 --> 00:23:16,640
theorem

00:23:13,679 --> 00:23:19,200
is has a basic difference and it uh it

00:23:16,640 --> 00:23:21,280
matters a lot for performance

00:23:19,200 --> 00:23:22,799
another measurement of statistics which

00:23:21,280 --> 00:23:24,880
is commonly used in this field of

00:23:22,799 --> 00:23:29,200
performance analysis is ipc

00:23:24,880 --> 00:23:32,720
instruction per cycle so um

00:23:29,200 --> 00:23:34,559
processors attempt to run instructions

00:23:32,720 --> 00:23:36,320
and if this instruction has to load

00:23:34,559 --> 00:23:36,799
something into memory from memory excuse

00:23:36,320 --> 00:23:38,960
me

00:23:36,799 --> 00:23:40,320
and the memory and then you have to go

00:23:38,960 --> 00:23:43,440
from to l1

00:23:40,320 --> 00:23:43,840
to l2 l3 cache into the drum so this

00:23:43,440 --> 00:23:45,520
would

00:23:43,840 --> 00:23:47,919
at some point the processor will be

00:23:45,520 --> 00:23:50,960
stalled for half a part of the time

00:23:47,919 --> 00:23:53,039
and wait for the data to arrive so

00:23:50,960 --> 00:23:54,000
instruction per cycle is a number which

00:23:53,039 --> 00:23:55,279
is uh

00:23:54,000 --> 00:23:57,279
it's convenient to think about it

00:23:55,279 --> 00:23:59,440
between zero to one like

00:23:57,279 --> 00:24:00,799
zero we are totally uneffective and one

00:23:59,440 --> 00:24:03,279
we are fully effective

00:24:00,799 --> 00:24:04,320
but in modern system it can go a bit

00:24:03,279 --> 00:24:06,559
over one

00:24:04,320 --> 00:24:08,240
but let us not let it to confuse us

00:24:06,559 --> 00:24:10,720
let's think about it between a number

00:24:08,240 --> 00:24:11,840
between zero to one one to ten one to

00:24:10,720 --> 00:24:13,520
two this is a

00:24:11,840 --> 00:24:15,360
in io application this is the best

00:24:13,520 --> 00:24:17,120
number i think we can get

00:24:15,360 --> 00:24:19,279
and this single number is typically a

00:24:17,120 --> 00:24:21,120
very good indication for efficiency

00:24:19,279 --> 00:24:22,559
so many times when you'll solve

00:24:21,120 --> 00:24:24,000
something performance they will ask you

00:24:22,559 --> 00:24:26,720
what was your rpc

00:24:24,000 --> 00:24:28,240
or you can see ipc comparisons we also

00:24:26,720 --> 00:24:30,320
looked on the instruction per i o and

00:24:28,240 --> 00:24:33,520
cycles for io

00:24:30,320 --> 00:24:34,559
so when we put those numbers into a

00:24:33,520 --> 00:24:37,679
table

00:24:34,559 --> 00:24:39,760
uh and we had it in front of us we can

00:24:37,679 --> 00:24:41,760
see that when we own one call

00:24:39,760 --> 00:24:44,080
both workloads the non-zero copy and the

00:24:41,760 --> 00:24:47,360
zero copy they started ipc of

00:24:44,080 --> 00:24:48,880
one a bit of more and we can see the

00:24:47,360 --> 00:24:49,279
difference in destruction to the higher

00:24:48,880 --> 00:24:51,039
cost

00:24:49,279 --> 00:24:53,120
because this is the cost of the copy you

00:24:51,039 --> 00:24:55,600
know the whatever 20 30

00:24:53,120 --> 00:24:58,080
that we that we have there but what

00:24:55,600 --> 00:25:01,360
happens that we go to 15 calls

00:24:58,080 --> 00:25:02,320
we see that the ipc for the zero for the

00:25:01,360 --> 00:25:05,840
non-zero

00:25:02,320 --> 00:25:07,919
copy kind of collapse and and drops to

00:25:05,840 --> 00:25:08,960
one half so it's a hundred percent

00:25:07,919 --> 00:25:12,240
decrease

00:25:08,960 --> 00:25:13,120
from one to or or whatever uh percentage

00:25:12,240 --> 00:25:15,760
from one to

00:25:13,120 --> 00:25:17,279
to half of that and if we look on the

00:25:15,760 --> 00:25:21,679
last level cash hits

00:25:17,279 --> 00:25:24,480
we can see that uh we are only at 0.60

00:25:21,679 --> 00:25:25,360
uh hits on the last level cash and then

00:25:24,480 --> 00:25:28,159
what happens

00:25:25,360 --> 00:25:29,360
is that we have many last level cash

00:25:28,159 --> 00:25:32,640
cash misses

00:25:29,360 --> 00:25:33,520
ipc decreases we can see that we have

00:25:32,640 --> 00:25:37,440
now 70k

00:25:33,520 --> 00:25:41,440
cycles per io and it just doesn't scale

00:25:37,440 --> 00:25:44,400
so our our um our main conclusion

00:25:41,440 --> 00:25:46,000
from this whole performance study is

00:25:44,400 --> 00:25:48,640
that copy has a notable

00:25:46,000 --> 00:25:50,400
system-wise cost and this system-wise

00:25:48,640 --> 00:25:54,159
cost is something you can just

00:25:50,400 --> 00:25:55,600
not read from cycles flame graph it's

00:25:54,159 --> 00:25:58,640
not going to tell you that

00:25:55,600 --> 00:26:01,200
only when you uh start to look on the

00:25:58,640 --> 00:26:02,640
on the combination of last level cash

00:26:01,200 --> 00:26:07,120
hits or whatever

00:26:02,640 --> 00:26:09,039
um payments you measure somehow the

00:26:07,120 --> 00:26:10,960
how frequency you go to last level cash

00:26:09,039 --> 00:26:12,080
or how frequently you go beyond last

00:26:10,960 --> 00:26:14,400
letter of cash actually

00:26:12,080 --> 00:26:15,760
to the drum and what does this do to

00:26:14,400 --> 00:26:19,360
your ipc

00:26:15,760 --> 00:26:22,480
you you you you realize that the

00:26:19,360 --> 00:26:26,159
um the the the cycles for io become

00:26:22,480 --> 00:26:26,159
so much high and then

00:26:26,240 --> 00:26:33,520
it didn't scale anymore um

00:26:30,480 --> 00:26:36,559
before concluding i would like to do her

00:26:33,520 --> 00:26:38,240
a short sketch of the of uh

00:26:36,559 --> 00:26:40,080
what's going on in open source for

00:26:38,240 --> 00:26:43,039
twisting zero copy and

00:26:40,080 --> 00:26:45,039
suggestions for some further work so

00:26:43,039 --> 00:26:47,120
what is the state of the union at least

00:26:45,039 --> 00:26:47,760
what i was able to gather for tx open

00:26:47,120 --> 00:26:50,880
source

00:26:47,760 --> 00:26:53,200
in linux so it merged to google rpc

00:26:50,880 --> 00:26:54,159
and actually also to spdk since this

00:26:53,200 --> 00:26:57,840
work has been done

00:26:54,159 --> 00:27:01,679
in january this year um um

00:26:57,840 --> 00:27:04,080
a cool work by the intel guys um we also

00:27:01,679 --> 00:27:05,440
had a chance to participate a bit so in

00:27:04,080 --> 00:27:07,520
the target side

00:27:05,440 --> 00:27:09,120
zero copy is there and it provided a

00:27:07,520 --> 00:27:13,360
nice performance boost

00:27:09,120 --> 00:27:16,400
for both x86 systems and all systems

00:27:13,360 --> 00:27:16,880
now a few uh suggesting or thoughts

00:27:16,400 --> 00:27:19,760
about

00:27:16,880 --> 00:27:20,000
further work the first is i think we

00:27:19,760 --> 00:27:23,360
should

00:27:20,000 --> 00:27:24,080
investigate ways to avoid the uh the the

00:27:23,360 --> 00:27:26,720
cost of

00:27:24,080 --> 00:27:28,320
per i o per i o per buffer doing a pin

00:27:26,720 --> 00:27:30,640
and pin

00:27:28,320 --> 00:27:32,880
uh something in the kernel that which is

00:27:30,640 --> 00:27:36,559
called get put user pages

00:27:32,880 --> 00:27:38,000
so um hopefully we can use the recently

00:27:36,559 --> 00:27:41,039
introduced i o urine

00:27:38,000 --> 00:27:43,039
api that was used maybe a year or in the

00:27:41,039 --> 00:27:45,840
last year or last two years in linux

00:27:43,039 --> 00:27:47,760
to actually make an initial declaration

00:27:45,840 --> 00:27:49,919
towards the socket provider

00:27:47,760 --> 00:27:51,840
on buffers that we want to pin and later

00:27:49,919 --> 00:27:53,279
there will no there will not be any more

00:27:51,840 --> 00:27:56,880
pinning in unpinning

00:27:53,279 --> 00:27:59,919
this is can be a critical effect

00:27:56,880 --> 00:28:01,840
the second nice and

00:27:59,919 --> 00:28:03,360
low hanging fruit that could help

00:28:01,840 --> 00:28:05,919
applications

00:28:03,360 --> 00:28:07,440
is to actually extend send a message

00:28:05,919 --> 00:28:09,760
send a message

00:28:07,440 --> 00:28:10,480
is like send em like send message but

00:28:09,760 --> 00:28:13,679
sending m

00:28:10,480 --> 00:28:16,240
iovs or m buffers so this way

00:28:13,679 --> 00:28:18,080
application would be still be able to do

00:28:16,240 --> 00:28:19,120
one system call that provide the socket

00:28:18,080 --> 00:28:21,279
provider

00:28:19,120 --> 00:28:22,559
uh give the socket provider bunch of

00:28:21,279 --> 00:28:24,000
buffers so they don't have to do a

00:28:22,559 --> 00:28:25,520
system copper buffer

00:28:24,000 --> 00:28:27,520
but they would have they would be able

00:28:25,520 --> 00:28:27,840
to provide a hint per buffer if they

00:28:27,520 --> 00:28:31,840
want

00:28:27,840 --> 00:28:31,840
zero copy or don't want zero copy

00:28:32,080 --> 00:28:35,919
the last one is see if and where we can

00:28:34,320 --> 00:28:38,320
relax and refine the buffer reclaim

00:28:35,919 --> 00:28:40,640
notification overhead

00:28:38,320 --> 00:28:42,840
in our case we had a transactional

00:28:40,640 --> 00:28:45,440
application and we just skipped it all

00:28:42,840 --> 00:28:47,760
together

00:28:45,440 --> 00:28:48,799
we can we should see if this can be

00:28:47,760 --> 00:28:51,440
across the place

00:28:48,799 --> 00:28:52,640
is it correct that every transactional

00:28:51,440 --> 00:28:56,840
application can do that

00:28:52,640 --> 00:28:58,880
are there more cases so it point to

00:28:56,840 --> 00:29:01,760
those to conclude

00:28:58,880 --> 00:29:03,200
uh what are the takeaways uh from this

00:29:01,760 --> 00:29:07,279
performance study

00:29:03,200 --> 00:29:09,360
um our socs and maybe more resources

00:29:07,279 --> 00:29:10,559
uh can be sensitive to last level cash

00:29:09,360 --> 00:29:12,320
misses

00:29:10,559 --> 00:29:14,960
and if there are many cache last level

00:29:12,320 --> 00:29:18,080
cache misses it has a system effect

00:29:14,960 --> 00:29:20,000
um in this case the when scaling the

00:29:18,080 --> 00:29:21,200
improvement in iop so from zero copy

00:29:20,000 --> 00:29:23,840
goes beyond

00:29:21,200 --> 00:29:26,320
the cpu cycles you would see uh for

00:29:23,840 --> 00:29:29,440
flame graphs

00:29:26,320 --> 00:29:31,039
if we if we and we do assume some some

00:29:29,440 --> 00:29:33,760
symmetry between receive and

00:29:31,039 --> 00:29:36,799
transmitting networking applications

00:29:33,760 --> 00:29:39,039
uh this study showed that we got a very

00:29:36,799 --> 00:29:39,679
nice boost from transmit zero copy and

00:29:39,039 --> 00:29:42,480
same

00:29:39,679 --> 00:29:44,480
or similar or we expect for to happen

00:29:42,480 --> 00:29:47,679
for receive zero copy

00:29:44,480 --> 00:29:50,000
for similar storage applications

00:29:47,679 --> 00:29:50,880
and the zero charging network stack has

00:29:50,000 --> 00:29:54,480
critical role

00:29:50,880 --> 00:29:56,880
for smartnic socs

00:29:54,480 --> 00:29:56,880
thank you

00:30:02,159 --> 00:30:05,360
thanks uh off

00:30:07,600 --> 00:30:11,440
okay we're still behind but there's not

00:30:10,480 --> 00:30:13,679
that many questions

00:30:11,440 --> 00:30:14,799
uh so you're saying now we have open

00:30:13,679 --> 00:30:17,039
source projects that

00:30:14,799 --> 00:30:19,120
use uh zero copy i just want to correct

00:30:17,039 --> 00:30:20,559
what i said yeah i updated the slides so

00:30:19,120 --> 00:30:23,679
spdk

00:30:20,559 --> 00:30:25,840
january 220 they used it in uh and

00:30:23,679 --> 00:30:27,919
get was a great success not over melanox

00:30:25,840 --> 00:30:29,120
vma they tested it it's the intel team

00:30:27,919 --> 00:30:30,799
they test it

00:30:29,120 --> 00:30:32,559
uh it's nice they test it with melanox

00:30:30,799 --> 00:30:36,000
nick but they test it with uh

00:30:32,559 --> 00:30:38,880
kernel sockets and they got uh um

00:30:36,000 --> 00:30:40,240
nice improvement and of course google

00:30:38,880 --> 00:30:42,000
rpc is also there

00:30:40,240 --> 00:30:44,240
uh this is what these are the two

00:30:42,000 --> 00:30:44,640
packages that i was able to find in my

00:30:44,240 --> 00:30:48,080
uh

00:30:44,640 --> 00:30:49,760
search okay

00:30:48,080 --> 00:30:51,520
all right so to the questions first one

00:30:49,760 --> 00:30:52,480
is from shujit uh should

00:30:51,520 --> 00:30:55,039
you want to talk or you want me to

00:30:52,480 --> 00:30:55,039
repeat this

00:30:55,679 --> 00:30:59,840
actually it's already covered you can

00:30:57,600 --> 00:31:04,000
skip all of my questions

00:30:59,840 --> 00:31:05,919
okay uh i'll jump into

00:31:04,000 --> 00:31:08,480
the way to ian forbes did i get that

00:31:05,919 --> 00:31:11,840
right that's the last person after you

00:31:08,480 --> 00:31:13,840
uh um has anyone looked at using

00:31:11,840 --> 00:31:15,440
receive mm message to process

00:31:13,840 --> 00:31:20,480
application and message

00:31:15,440 --> 00:31:23,519
rq data in the same cisco

00:31:20,480 --> 00:31:24,640
i'm not familiar with that i have a

00:31:23,519 --> 00:31:27,120
message where

00:31:24,640 --> 00:31:29,840
you can patch i think in one socket

00:31:27,120 --> 00:31:29,840
called multiple

00:31:30,840 --> 00:31:36,080
messages

00:31:32,000 --> 00:31:40,240
not familiar with it okay uh

00:31:36,080 --> 00:31:43,600
uh did i miss some question

00:31:40,240 --> 00:31:45,360
okay the rest of us should use questions

00:31:43,600 --> 00:31:46,960
all right maybe i'll ask one more i know

00:31:45,360 --> 00:31:48,720
we are behind a little bit

00:31:46,960 --> 00:31:50,240
okay somebody just popped up you want to

00:31:48,720 --> 00:31:53,679
ask uh

00:31:50,240 --> 00:31:54,320
no that's saggy okay um does it make

00:31:53,679 --> 00:31:56,080
sense

00:31:54,320 --> 00:31:58,080
so so i can see why zero copy makes a

00:31:56,080 --> 00:31:58,799
lot of sense for bulk type transfers

00:31:58,080 --> 00:32:01,840
yeah

00:31:58,799 --> 00:32:04,320
but when you say grpc has support for it

00:32:01,840 --> 00:32:04,960
uh does it make sense in that case

00:32:04,320 --> 00:32:06,559
because it's a

00:32:04,960 --> 00:32:09,600
if you're having some small

00:32:06,559 --> 00:32:12,799
transactional type messaging

00:32:09,600 --> 00:32:14,720
so that was also when they um i don't

00:32:12,799 --> 00:32:16,799
know exactly how it works in grpc if

00:32:14,720 --> 00:32:20,000
there's a threshold or no threshold but

00:32:16,799 --> 00:32:20,720
um at least they actually communicate

00:32:20,000 --> 00:32:24,559
when

00:32:20,720 --> 00:32:26,559
they um when they adapted zero copies so

00:32:24,559 --> 00:32:28,320
it's not it's without any threshold so

00:32:26,559 --> 00:32:30,720
uh currently

00:32:28,320 --> 00:32:32,399
uh it's across the place that was the

00:32:30,720 --> 00:32:35,760
inclined design decision

00:32:32,399 --> 00:32:37,360
but i can see cases where uh

00:32:35,760 --> 00:32:39,120
if you really know what we are doing and

00:32:37,360 --> 00:32:42,799
what is going on beyond your

00:32:39,120 --> 00:32:44,559
legs so you can decide you can decide a

00:32:42,799 --> 00:32:47,840
threshold or

00:32:44,559 --> 00:32:50,000
um as i said the um

00:32:47,840 --> 00:32:51,840
protocol headers like 40 bytes why you

00:32:50,000 --> 00:32:52,880
want to do zero copy for 40 bytes so

00:32:51,840 --> 00:32:56,640
today it's

00:32:52,880 --> 00:32:57,279
it's today you don't want to do a system

00:32:56,640 --> 00:32:59,519
called

00:32:57,279 --> 00:33:00,720
per buffer so you you on the one hand

00:32:59,519 --> 00:33:03,919
you want to use the iov

00:33:00,720 --> 00:33:05,519
on the other hand so this extension that

00:33:03,919 --> 00:33:05,840
actually william de bruyne suggested to

00:33:05,519 --> 00:33:08,840
me

00:33:05,840 --> 00:33:10,000
uh last year uh of send a message could

00:33:08,840 --> 00:33:12,320
serve

00:33:10,000 --> 00:33:15,120
protocols that want to do do it in a

00:33:12,320 --> 00:33:15,120
more granular

00:33:17,159 --> 00:33:20,159

YouTube URL: https://www.youtube.com/watch?v=GPc1uNLcwZ4


