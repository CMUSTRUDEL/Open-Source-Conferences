Title: Netdev 0x14 - SONiC and Linux
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Guohan Lu, Matty Kadosh

More info: https://netdevconf.info/0x14/session.html?talk-sonic-and-linux

Date: Monday, August 17, 2020

In this Guohan Lu, Kalimuthu Velappan, Kiran Kella and
Marian Pritzak  introduce SONiC, a Linux based open source NOS
(https://azure.github.io/SONiC/).
They discuss the SONiC architecture, its integration with FRR
and kernel routing stack and the current overall status of SONiC.
They then go over some of the challenges they faced and how they
resolved them and further discuss outstanding issues in Linux Networking
that they are still facing.
More importantly, they take this opportunity to provide
feedback to the Linux Networking community as well as solicit feedback.
Captions: 
	00:00:01,680 --> 00:00:06,720
hi um

00:00:03,120 --> 00:00:09,920
this is gohan from microsoft and today

00:00:06,720 --> 00:00:12,639
i'm jointly with broadcom the nvidia

00:00:09,920 --> 00:00:14,000
and we're going to talk about the sonic

00:00:12,639 --> 00:00:18,160
and

00:00:14,000 --> 00:00:18,160
how it's working on top of the linux

00:00:21,039 --> 00:00:25,359
so here is today's agenda so we're going

00:00:24,160 --> 00:00:28,000
to talk about the

00:00:25,359 --> 00:00:30,720
sonic and motivation and then we'll give

00:00:28,000 --> 00:00:34,320
a little bit introduction of the sonic

00:00:30,720 --> 00:00:35,280
architecture and and then we're going to

00:00:34,320 --> 00:00:39,040
introduce

00:00:35,280 --> 00:00:39,840
how we um expand the sonic beyond the

00:00:39,040 --> 00:00:43,520
singles

00:00:39,840 --> 00:00:44,719
asic switch system and then the maria is

00:00:43,520 --> 00:00:46,800
going to talk up

00:00:44,719 --> 00:00:48,960
to the little bit of deep dive under

00:00:46,800 --> 00:00:52,079
sonic features and linux

00:00:48,960 --> 00:00:54,800
and then our broadcom colleagues

00:00:52,079 --> 00:00:55,840
will talk about some specific issues uh

00:00:54,800 --> 00:00:58,000
and challenges

00:00:55,840 --> 00:00:58,960
we're saying to run sonic on top of

00:00:58,000 --> 00:01:02,879
linux and

00:00:58,960 --> 00:01:02,879
then how they address those challenges

00:01:05,680 --> 00:01:11,920
so what is sonic so sonic stands for

00:01:09,520 --> 00:01:13,040
software for open networking in the

00:01:11,920 --> 00:01:16,240
cloud

00:01:13,040 --> 00:01:19,840
it's a collection of software components

00:01:16,240 --> 00:01:23,200
that running on top of linux and then

00:01:19,840 --> 00:01:25,520
operating the switches s6

00:01:23,200 --> 00:01:27,119
so first the sonic built on the

00:01:25,520 --> 00:01:30,159
foundation of site

00:01:27,119 --> 00:01:32,880
which is the

00:01:30,159 --> 00:01:35,200
switch abstraction interface the site

00:01:32,880 --> 00:01:38,640
provides the apis

00:01:35,200 --> 00:01:42,159
to for the operating system to interact

00:01:38,640 --> 00:01:44,960
with the switching essex the sonic

00:01:42,159 --> 00:01:46,079
provides layer 2 and the layer 3

00:01:44,960 --> 00:01:49,840
functions

00:01:46,079 --> 00:01:52,720
targeted for cloud it's a linux based

00:01:49,840 --> 00:01:53,520
switching opera system operating system

00:01:52,720 --> 00:01:56,640
it looks

00:01:53,520 --> 00:01:59,759
and feels like linux

00:01:56,640 --> 00:02:03,200
and sonic is a community driven project

00:01:59,759 --> 00:02:05,119
it's a full open source project

00:02:03,200 --> 00:02:08,640
and all the source code are shared on

00:02:05,119 --> 00:02:10,720
the github with apache license

00:02:08,640 --> 00:02:14,319
the community believes in the working

00:02:10,720 --> 00:02:14,319
code and the quick iterations

00:02:17,840 --> 00:02:24,640
so the main sonic architecture is a

00:02:21,120 --> 00:02:27,360
container based architecture

00:02:24,640 --> 00:02:28,239
this graph shows how the how the

00:02:27,360 --> 00:02:32,480
components

00:02:28,239 --> 00:02:35,680
work on top of linux on a single asic

00:02:32,480 --> 00:02:37,120
so sonic is primarily depending on a

00:02:35,680 --> 00:02:41,440
power api

00:02:37,120 --> 00:02:44,560
a site api and the linux network

00:02:41,440 --> 00:02:45,599
and other components are swapped by a

00:02:44,560 --> 00:02:48,879
swampable

00:02:45,599 --> 00:02:52,080
and changed by the suppliers over time

00:02:48,879 --> 00:02:55,040
so on top of the sonic stack and there

00:02:52,080 --> 00:02:58,239
is a networking application stack

00:02:55,040 --> 00:03:01,280
here we have we have bgp

00:02:58,239 --> 00:03:04,480
lldp smp

00:03:01,280 --> 00:03:06,239
and azure networking modules where they

00:03:04,480 --> 00:03:08,239
speak with

00:03:06,239 --> 00:03:11,680
with their narrative neighbors to

00:03:08,239 --> 00:03:16,239
exchange network particles

00:03:11,680 --> 00:03:19,120
at the middle of the layer there were

00:03:16,239 --> 00:03:21,680
we called switch state service and

00:03:19,120 --> 00:03:23,840
basically provides the interface allows

00:03:21,680 --> 00:03:27,280
the network applications

00:03:23,840 --> 00:03:30,959
um to send their commands

00:03:27,280 --> 00:03:34,080
and to control the a6 the switch

00:03:30,959 --> 00:03:35,120
service has a back-end the database all

00:03:34,080 --> 00:03:37,280
the commands

00:03:35,120 --> 00:03:39,680
sent by the application will be stored

00:03:37,280 --> 00:03:41,680
in the database

00:03:39,680 --> 00:03:42,959
and we have under the street data

00:03:41,680 --> 00:03:46,799
service also

00:03:42,959 --> 00:03:49,920
translate those application commands

00:03:46,799 --> 00:03:53,519
into site commands um

00:03:49,920 --> 00:03:56,239
we'll see the later slides the sync d

00:03:53,519 --> 00:03:57,680
is another components as a on the bottom

00:03:56,239 --> 00:03:59,920
of the database

00:03:57,680 --> 00:04:01,760
it tries to read all the side commands

00:03:59,920 --> 00:04:05,040
stored in the database

00:04:01,760 --> 00:04:08,400
and then using the psi driver

00:04:05,040 --> 00:04:08,400
to configure the s6

00:04:08,959 --> 00:04:17,120
and the site composed

00:04:13,040 --> 00:04:20,720
of vendor

00:04:17,120 --> 00:04:24,720
sdks and also the asic drivers

00:04:20,720 --> 00:04:24,720
that lives in the linux kernel

00:04:25,280 --> 00:04:30,240
so the the core components here we have

00:04:28,720 --> 00:04:32,400
a set of

00:04:30,240 --> 00:04:34,000
um call services which are the

00:04:32,400 --> 00:04:36,560
application service

00:04:34,000 --> 00:04:37,280
plus database plus a switch their

00:04:36,560 --> 00:04:39,600
service

00:04:37,280 --> 00:04:43,840
that's the main component of a sonic

00:04:39,600 --> 00:04:43,840
that work on a single asic

00:04:48,479 --> 00:04:52,240
here is how the stage switch date

00:04:50,800 --> 00:04:54,960
service works

00:04:52,240 --> 00:04:56,560
so inside the database there are two

00:04:54,960 --> 00:04:59,919
main databases

00:04:56,560 --> 00:05:02,240
one is i called applicationdb which

00:04:59,919 --> 00:05:04,479
is processed all the application

00:05:02,240 --> 00:05:07,919
commands and objects

00:05:04,479 --> 00:05:11,520
another one is side db which persists

00:05:07,919 --> 00:05:14,160
all the side components

00:05:11,520 --> 00:05:16,400
so applications send their commands to

00:05:14,160 --> 00:05:19,759
the application db

00:05:16,400 --> 00:05:23,199
and then in the middle there is a

00:05:19,759 --> 00:05:26,320
orchestration agent which translates

00:05:23,199 --> 00:05:29,759
all the application object into the site

00:05:26,320 --> 00:05:33,120
objects and then it puts all the site

00:05:29,759 --> 00:05:35,120
object into the site db

00:05:33,120 --> 00:05:36,240
once the object has been put into the

00:05:35,120 --> 00:05:39,280
site db

00:05:36,240 --> 00:05:40,720
there is a syncd service which reads

00:05:39,280 --> 00:05:43,919
those site objects

00:05:40,720 --> 00:05:47,039
and translate into the site api and send

00:05:43,919 --> 00:05:50,400
those commands to the s6

00:05:47,039 --> 00:05:53,680
so here the goal is that um

00:05:50,400 --> 00:05:56,000
we have those two different applications

00:05:53,680 --> 00:05:56,800
different application database and site

00:05:56,000 --> 00:06:00,880
database

00:05:56,800 --> 00:06:07,840
so that we can separate those components

00:06:00,880 --> 00:06:07,840
and they they can evolve independently

00:06:08,639 --> 00:06:13,280
so take a cl a more close look at the

00:06:11,199 --> 00:06:15,840
software modules

00:06:13,280 --> 00:06:18,479
we have all the different modules from

00:06:15,840 --> 00:06:22,400
the linux communities

00:06:18,479 --> 00:06:25,680
for example we have the team d ladp

00:06:22,400 --> 00:06:26,479
and bgp snp they are all from open

00:06:25,680 --> 00:06:29,280
source

00:06:26,479 --> 00:06:31,440
communities so the main component the

00:06:29,280 --> 00:06:37,039
sony community is developing

00:06:31,440 --> 00:06:37,039
is the swss and the zinc d components

00:06:40,960 --> 00:06:44,880
so here is how the routing works in

00:06:43,759 --> 00:06:49,280
sonic

00:06:44,880 --> 00:06:51,840
on the right side it's a sonic instance

00:06:49,280 --> 00:06:52,479
on the left side you can think of is a

00:06:51,840 --> 00:06:56,560
bgp

00:06:52,479 --> 00:07:00,720
neighbor so first the bgp neighbor

00:06:56,560 --> 00:07:03,039
will send packets to the asic

00:07:00,720 --> 00:07:05,039
and the package will be received by

00:07:03,039 --> 00:07:07,520
linux netdev

00:07:05,039 --> 00:07:08,160
and then going through the socket and

00:07:07,520 --> 00:07:10,960
those

00:07:08,160 --> 00:07:12,639
t packets will received by the bgpd

00:07:10,960 --> 00:07:16,639
process

00:07:12,639 --> 00:07:20,560
and after exchanging those bgp protocol

00:07:16,639 --> 00:07:24,080
and the bgpd will send those routing

00:07:20,560 --> 00:07:27,440
interests into the zebra and we have

00:07:24,080 --> 00:07:30,000
developed a component called fpmcd

00:07:27,440 --> 00:07:31,840
which reads the routing interface

00:07:30,000 --> 00:07:34,800
information from zebra

00:07:31,840 --> 00:07:37,120
and send to the application db once the

00:07:34,800 --> 00:07:40,160
routes reach the application db

00:07:37,120 --> 00:07:44,400
the occasions will reduce routes

00:07:40,160 --> 00:07:48,240
and translate them into the site object

00:07:44,400 --> 00:07:51,280
and then after those routing interests

00:07:48,240 --> 00:07:53,680
um reach the side db then the sync d

00:07:51,280 --> 00:07:56,080
will read throughout those routes and

00:07:53,680 --> 00:08:00,319
then put into

00:07:56,080 --> 00:08:00,319
the asic choose the site

00:08:02,560 --> 00:08:09,440
here is another example of how the lag

00:08:05,759 --> 00:08:12,639
works it works similarly with

00:08:09,440 --> 00:08:16,479
similarly as a bgp in this case

00:08:12,639 --> 00:08:20,080
on the right side it's a lag processing

00:08:16,479 --> 00:08:23,440
in sonic and on the left side

00:08:20,080 --> 00:08:27,360
it's a lag labor so the

00:08:23,440 --> 00:08:29,919
lacp neighbor and send lacp package to

00:08:27,360 --> 00:08:29,919
the asic

00:08:30,080 --> 00:08:37,279
um those package will be trapped to

00:08:33,279 --> 00:08:40,159
the host netdev interfaces

00:08:37,279 --> 00:08:41,120
and then this if this is a control plan

00:08:40,159 --> 00:08:44,320
package

00:08:41,120 --> 00:08:46,880
will be sent to the um

00:08:44,320 --> 00:08:47,680
socket the team d and team d will

00:08:46,880 --> 00:08:50,800
receive and

00:08:47,680 --> 00:08:54,000
process them however if it's a data plan

00:08:50,800 --> 00:08:57,360
packet it will be

00:08:54,000 --> 00:09:01,040
sent to the tmd driver and then

00:08:57,360 --> 00:09:03,040
those data pack packets will be received

00:09:01,040 --> 00:09:04,480
through the application from the tmd

00:09:03,040 --> 00:09:07,200
driver

00:09:04,480 --> 00:09:08,959
so going back to the control flow once

00:09:07,200 --> 00:09:11,839
the tmd receive those

00:09:08,959 --> 00:09:13,760
lacp packet and process them it will

00:09:11,839 --> 00:09:16,240
send those

00:09:13,760 --> 00:09:17,600
information state changes through the

00:09:16,240 --> 00:09:19,519
team syncd

00:09:17,600 --> 00:09:22,399
and the team syncd will get those state

00:09:19,519 --> 00:09:25,680
changes and then put those states

00:09:22,399 --> 00:09:26,560
uh into application db the occasion will

00:09:25,680 --> 00:09:29,360
understand

00:09:26,560 --> 00:09:30,880
uh whether when a lag has been created

00:09:29,360 --> 00:09:32,959
or lack member

00:09:30,880 --> 00:09:34,240
has been added to a lag through the

00:09:32,959 --> 00:09:36,640
application db

00:09:34,240 --> 00:09:38,080
and then translate them into the site

00:09:36,640 --> 00:09:39,920
object

00:09:38,080 --> 00:09:42,000
then the thing d will get those side

00:09:39,920 --> 00:09:44,959
objects and send

00:09:42,000 --> 00:09:47,440
to the essay using the side and lag

00:09:44,959 --> 00:09:47,440
commands

00:09:48,080 --> 00:09:55,360
so the next um that that is

00:09:51,680 --> 00:09:58,720
how the uh single essex works

00:09:55,360 --> 00:10:00,080
on top of sonic then we're going to talk

00:09:58,720 --> 00:10:03,360
a little bit more about

00:10:00,080 --> 00:10:06,880
how we extend this architecture

00:10:03,360 --> 00:10:06,880
beyond the single essay

00:10:08,000 --> 00:10:16,560
so here is a typical multi-asset device

00:10:12,480 --> 00:10:19,600
switch device so in this in this device

00:10:16,560 --> 00:10:22,720
there is still a single cpu

00:10:19,600 --> 00:10:27,040
but within that device there are six

00:10:22,720 --> 00:10:30,560
six um on that device

00:10:27,040 --> 00:10:30,880
you can see s60 from s63 there are front

00:10:30,560 --> 00:10:34,480
and

00:10:30,880 --> 00:10:37,839
six where they have ports

00:10:34,480 --> 00:10:40,959
that connected to the front panel parts

00:10:37,839 --> 00:10:44,399
however those frontend essex

00:10:40,959 --> 00:10:46,160
are connected by two back in the assets

00:10:44,399 --> 00:10:47,600
and the backing actually provide

00:10:46,160 --> 00:10:51,440
connectivity among

00:10:47,600 --> 00:10:54,160
all those front-end assets when a packet

00:10:51,440 --> 00:10:55,680
and reaches the front end asset 0 and

00:10:54,160 --> 00:10:58,959
wants to go to

00:10:55,680 --> 00:11:01,680
front asic 1 then the packet will be

00:10:58,959 --> 00:11:03,040
switched to first to one over the

00:11:01,680 --> 00:11:06,560
backhand asic

00:11:03,040 --> 00:11:10,320
and then to front and asic one

00:11:06,560 --> 00:11:13,760
so the challenge here is that uh

00:11:10,320 --> 00:11:14,640
in linux how do we manage those six

00:11:13,760 --> 00:11:18,079
asics

00:11:14,640 --> 00:11:21,120
in a single box and how do we exchange

00:11:18,079 --> 00:11:24,399
routing formations among those

00:11:21,120 --> 00:11:25,200
asics and makes them appear like one

00:11:24,399 --> 00:11:29,839
asic

00:11:25,200 --> 00:11:29,839
to the outside world

00:11:32,000 --> 00:11:39,519
so to solve this issue we're leveraging

00:11:36,480 --> 00:11:42,800
the linux namespace and

00:11:39,519 --> 00:11:46,320
to abstract the single asic

00:11:42,800 --> 00:11:47,600
so linux network namespace provides a

00:11:46,320 --> 00:11:51,279
way

00:11:47,600 --> 00:11:52,320
clean way and for isolated networking

00:11:51,279 --> 00:11:55,519
environment

00:11:52,320 --> 00:11:58,000
to process and services

00:11:55,519 --> 00:11:59,920
so in this case we create one namespace

00:11:58,000 --> 00:12:03,040
per asic

00:11:59,920 --> 00:12:04,800
then we run the docker stacks and the

00:12:03,040 --> 00:12:09,279
management services for each

00:12:04,800 --> 00:12:13,279
essay inside the associate namespace

00:12:09,279 --> 00:12:16,800
for example in namespace 1

00:12:13,279 --> 00:12:20,480
we run the full step full sonic stack

00:12:16,800 --> 00:12:24,399
from the bgp lldp at tmd

00:12:20,480 --> 00:12:28,000
swss and syncd

00:12:24,399 --> 00:12:31,519
the full stack manage this single

00:12:28,000 --> 00:12:35,519
asic independently with other

00:12:31,519 --> 00:12:35,519
as against the other namespaces

00:12:35,680 --> 00:12:43,200
then all these six stacks

00:12:39,680 --> 00:12:46,800
are talking to each other using

00:12:43,200 --> 00:12:48,480
the um peer links that connecting those

00:12:46,800 --> 00:12:52,399
name spaces

00:12:48,480 --> 00:12:55,440
each asic will operate independently

00:12:52,399 --> 00:12:58,720
as if they are single asic device

00:12:55,440 --> 00:13:01,920
and then they have full sonic stack to

00:12:58,720 --> 00:13:05,120
run routing protocols between

00:13:01,920 --> 00:13:08,240
those name spaces

00:13:05,120 --> 00:13:11,600
and to the outside world

00:13:08,240 --> 00:13:15,040
all this routing stack will appear

00:13:11,600 --> 00:13:18,480
as a as a single bgp routing domain

00:13:15,040 --> 00:13:21,680
so that appears to

00:13:18,480 --> 00:13:24,560
once appears to its neighbors as one

00:13:21,680 --> 00:13:27,680
single device

00:13:24,560 --> 00:13:29,360
however to note here different from the

00:13:27,680 --> 00:13:32,079
single asic

00:13:29,360 --> 00:13:32,720
we are not running all sonic components

00:13:32,079 --> 00:13:36,959
within

00:13:32,720 --> 00:13:40,160
a single with a single namespace

00:13:36,959 --> 00:13:43,360
some components are shared by

00:13:40,160 --> 00:13:47,120
all the different namespaces for example

00:13:43,360 --> 00:13:50,160
we have smp and pimol

00:13:47,120 --> 00:13:53,279
and the telemetry dockers

00:13:50,160 --> 00:13:53,279
that is running

00:13:53,519 --> 00:13:59,519
single instance within the switch this

00:13:56,800 --> 00:14:02,959
is because

00:13:59,519 --> 00:14:05,279
those components are many interact

00:14:02,959 --> 00:14:06,240
with outside world to provide the

00:14:05,279 --> 00:14:10,480
operational

00:14:06,240 --> 00:14:13,519
status for the box and the operators

00:14:10,480 --> 00:14:16,720
want to still operate those this

00:14:13,519 --> 00:14:20,720
this box as a single box there

00:14:16,720 --> 00:14:23,920
therefore we provide a unified view

00:14:20,720 --> 00:14:25,279
for the operators to check the status of

00:14:23,920 --> 00:14:31,839
the system

00:14:25,279 --> 00:14:31,839
so one single docker

00:14:34,000 --> 00:14:41,279
so to reiterate here is how

00:14:37,680 --> 00:14:44,399
um the how the routing works

00:14:41,279 --> 00:14:48,000
so in this example we have two front

00:14:44,399 --> 00:14:51,760
and six s60 and s61

00:14:48,000 --> 00:14:54,880
they are running bgp instance

00:14:51,760 --> 00:14:57,279
and there is a backend asic running

00:14:54,880 --> 00:15:00,160
another bgp instance

00:14:57,279 --> 00:15:00,959
so between the front end and batting we

00:15:00,160 --> 00:15:04,800
have set up

00:15:00,959 --> 00:15:09,120
ibtp sessions between them

00:15:04,800 --> 00:15:12,160
so that it can change routing formations

00:15:09,120 --> 00:15:14,720
and on the outside the neighbor is

00:15:12,160 --> 00:15:18,480
connecting to the front end

00:15:14,720 --> 00:15:21,839
um and set up the b2b

00:15:18,480 --> 00:15:24,560
sessions so the

00:15:21,839 --> 00:15:25,120
routing information from route a will

00:15:24,560 --> 00:15:28,480
first

00:15:25,120 --> 00:15:31,920
receive the divided bgp zero

00:15:28,480 --> 00:15:35,279
and then propagate to the bgp

00:15:31,920 --> 00:15:38,320
to the asic one while the backend asic

00:15:35,279 --> 00:15:41,759
through the bgp instance two

00:15:38,320 --> 00:15:43,920
then once bgp y instance receive the

00:15:41,759 --> 00:15:47,040
routing information

00:15:43,920 --> 00:15:49,360
it will advertise the routes to the

00:15:47,040 --> 00:15:52,560
router b

00:15:49,360 --> 00:15:57,120
so this this is how

00:15:52,560 --> 00:15:57,120
we are extending the

00:15:57,440 --> 00:16:04,720
the sonics to match s6 switches

00:16:02,160 --> 00:16:07,839
next i will hand over on the

00:16:04,720 --> 00:16:11,040
presentation to my colleague

00:16:07,839 --> 00:16:12,079
hi everyone my name is marian i am a

00:16:11,040 --> 00:16:16,480
software architect

00:16:12,079 --> 00:16:19,600
at nvidia mellanox business unit

00:16:16,480 --> 00:16:23,279
and to for today

00:16:19,600 --> 00:16:26,320
next slide please so for today

00:16:23,279 --> 00:16:29,680
uh what i have is i compiled a list of

00:16:26,320 --> 00:16:33,360
features in sonic that in my opinion are

00:16:29,680 --> 00:16:35,920
interesting uh from

00:16:33,360 --> 00:16:36,399
the point of view of how they interact

00:16:35,920 --> 00:16:39,839
with

00:16:36,399 --> 00:16:40,639
uh linux so i'm not going to talk about

00:16:39,839 --> 00:16:44,079
the core

00:16:40,639 --> 00:16:47,759
routing and bridging here but

00:16:44,079 --> 00:16:51,600
rather i will stop on uh other

00:16:47,759 --> 00:16:55,920
components and features that

00:16:51,600 --> 00:16:58,959
that fulfill the dc requirements

00:16:55,920 --> 00:17:01,839
so they include v-net

00:16:58,959 --> 00:17:02,880
which is the vxlan routing s-flow for

00:17:01,839 --> 00:17:06,480
sampling

00:17:02,880 --> 00:17:07,520
access control lists not switch memory

00:17:06,480 --> 00:17:11,520
management

00:17:07,520 --> 00:17:14,720
and sonic virtual switch

00:17:11,520 --> 00:17:16,160
that is used for generic feature testing

00:17:14,720 --> 00:17:21,199
that i will also

00:17:16,160 --> 00:17:21,199
talk about at the end uh next slide

00:17:21,360 --> 00:17:28,000
okay so let's start with uh vxlan

00:17:24,959 --> 00:17:32,480
uh routing and sonic feature called

00:17:28,000 --> 00:17:35,760
v-net um so

00:17:32,480 --> 00:17:36,799
it is used to connect bar metal machines

00:17:35,760 --> 00:17:40,960
to

00:17:36,799 --> 00:17:46,080
uh virtual virtual machines in the cloud

00:17:40,960 --> 00:17:48,400
uh the interesting um uh

00:17:46,080 --> 00:17:50,240
point about this feature is that the

00:17:48,400 --> 00:17:53,679
provisioning of the overlay

00:17:50,240 --> 00:17:56,960
is done by the controller so basically

00:17:53,679 --> 00:17:59,679
uh all the

00:17:56,960 --> 00:18:00,400
uh tunnel routes or the all the local

00:17:59,679 --> 00:18:02,559
routes

00:18:00,400 --> 00:18:04,720
as well as the neighbors are provisioned

00:18:02,559 --> 00:18:06,960
by sdn controller

00:18:04,720 --> 00:18:08,400
hence they are not programmed in linux

00:18:06,960 --> 00:18:12,240
and uh

00:18:08,400 --> 00:18:15,440
neither is the vxlan net dev

00:18:12,240 --> 00:18:18,559
also provisioned in linux however uh

00:18:15,440 --> 00:18:21,840
under underlay routing is uh

00:18:18,559 --> 00:18:26,000
still uh programmed by linux and bgp

00:18:21,840 --> 00:18:29,440
so we have a kind of a hybrid model

00:18:26,000 --> 00:18:32,400
where underlay is run by bgp

00:18:29,440 --> 00:18:33,280
but the overlay is provisioned by the

00:18:32,400 --> 00:18:35,600
controller

00:18:33,280 --> 00:18:37,840
as opposed to evpn design and sonic

00:18:35,600 --> 00:18:40,880
which is still work in progress but

00:18:37,840 --> 00:18:44,320
evpn will be fully uh reflected and

00:18:40,880 --> 00:18:46,880
linux pulls overly and underlay

00:18:44,320 --> 00:18:46,880
next slide

00:18:49,440 --> 00:18:56,400
uh s flow so this is

00:18:52,480 --> 00:18:59,600
um s flow feature in sonic

00:18:56,400 --> 00:19:02,880
um it is used to sample packets uh

00:18:59,600 --> 00:19:06,160
by the ac hardware

00:19:02,880 --> 00:19:09,600
uh and then lift them uh to

00:19:06,160 --> 00:19:13,120
linux and application running on top for

00:19:09,600 --> 00:19:13,679
the future processing uh to make it work

00:19:13,120 --> 00:19:18,000
uh

00:19:13,679 --> 00:19:20,400
in sonic some boarding work was done

00:19:18,000 --> 00:19:23,039
in particular the p-sample driver was

00:19:20,400 --> 00:19:26,480
ported to linux 4.9

00:19:23,039 --> 00:19:29,520
with the asic drivers uh with the

00:19:26,480 --> 00:19:32,559
required support support of course

00:19:29,520 --> 00:19:34,480
uh netlink was used for the first time

00:19:32,559 --> 00:19:36,000
uh as a host interface channel for

00:19:34,480 --> 00:19:38,559
tourist packets

00:19:36,000 --> 00:19:39,120
uh as opposed to a net device that were

00:19:38,559 --> 00:19:42,559
that is

00:19:39,120 --> 00:19:45,840
traditionally used uh and uh our c

00:19:42,559 --> 00:19:48,160
answer uh tc sample is used to

00:19:45,840 --> 00:19:49,360
uh in uh virtual sonic switch that i

00:19:48,160 --> 00:19:53,600
mentioned uh

00:19:49,360 --> 00:19:56,240
as a software emulation for the

00:19:53,600 --> 00:19:57,440
testing environment to do to do the

00:19:56,240 --> 00:20:02,720
validation of

00:19:57,440 --> 00:20:02,720
this feature okay next one

00:20:04,720 --> 00:20:12,720
access control list uh so acl and sonic

00:20:08,720 --> 00:20:16,320
nsai has a very

00:20:12,720 --> 00:20:20,320
rich number of keys and actions and

00:20:16,320 --> 00:20:24,559
it is an always growing

00:20:20,320 --> 00:20:26,960
uh set of different use cases

00:20:24,559 --> 00:20:27,760
and and sonic starting from some basic

00:20:26,960 --> 00:20:30,400
one uh

00:20:27,760 --> 00:20:32,000
ones like firewalling uh to more

00:20:30,400 --> 00:20:35,200
advanced like data plane

00:20:32,000 --> 00:20:38,400
telemetry packet mirroring

00:20:35,200 --> 00:20:40,400
interactions with the net policy-based

00:20:38,400 --> 00:20:45,760
routing

00:20:40,400 --> 00:20:45,760
also some features use them to

00:20:45,840 --> 00:20:54,000
basically uh close

00:20:48,880 --> 00:20:54,000
uh the ports like uh pfc watchdog

00:20:54,080 --> 00:21:00,559
so we are multi

00:20:57,280 --> 00:21:03,679
we have multiple use cases for htl

00:21:00,559 --> 00:21:05,760
but the uh

00:21:03,679 --> 00:21:07,280
but this feature is mostly not uh

00:21:05,760 --> 00:21:10,000
reflected in linux so

00:21:07,280 --> 00:21:11,039
uh most acl tables are not programmed in

00:21:10,000 --> 00:21:15,360
linux

00:21:11,039 --> 00:21:18,480
uh except for uh uh for not

00:21:15,360 --> 00:21:22,480
and also to support that control plane

00:21:18,480 --> 00:21:24,799
acl we have a separate

00:21:22,480 --> 00:21:25,520
configuration table for that which is

00:21:24,799 --> 00:21:28,720
implemented

00:21:25,520 --> 00:21:31,840
uh using ip tables

00:21:28,720 --> 00:21:31,840
okay next one

00:21:32,960 --> 00:21:39,760
uh shared buffer oh sorry not

00:21:36,080 --> 00:21:45,120
uh skipped one uh uh network address

00:21:39,760 --> 00:21:49,840
translation uh also has

00:21:45,120 --> 00:21:53,120
rich number of use cases in linux

00:21:49,840 --> 00:21:55,200
so it covers uh

00:21:53,120 --> 00:21:56,799
the implementation in sonic covers site

00:21:55,200 --> 00:22:00,799
use cases

00:21:56,799 --> 00:22:04,640
basic dna test not also not hairpinning

00:22:00,799 --> 00:22:06,320
port translation uh and other

00:22:04,640 --> 00:22:07,919
more interesting use cases like full

00:22:06,320 --> 00:22:11,520
cone nut which

00:22:07,919 --> 00:22:15,120
will be discussed in more detail by

00:22:11,520 --> 00:22:18,400
my colleagues from broadcom later

00:22:15,120 --> 00:22:20,720
so this feature uh

00:22:18,400 --> 00:22:20,720
was

00:22:22,000 --> 00:22:27,360
its implementation and linux was fully

00:22:24,799 --> 00:22:27,679
utilized and also with some enhancements

00:22:27,360 --> 00:22:30,799
that

00:22:27,679 --> 00:22:35,679
again uh broadcom will follow up

00:22:30,799 --> 00:22:35,679
in a few minutes and next one

00:22:36,400 --> 00:22:40,880
shared buffer so switch memory

00:22:39,919 --> 00:22:45,440
management is

00:22:40,880 --> 00:22:48,960
also very important uh in

00:22:45,440 --> 00:22:49,919
dc infrastructure because uh we also

00:22:48,960 --> 00:22:53,679
need to do

00:22:49,919 --> 00:22:57,200
a fine tuning for uh specific use cases

00:22:53,679 --> 00:22:58,480
would it be by for tcp networks or for

00:22:57,200 --> 00:23:01,520
rdma networks

00:22:58,480 --> 00:23:04,960
or maybe

00:23:01,520 --> 00:23:08,799
to connect storage any

00:23:04,960 --> 00:23:13,039
use case might require a

00:23:08,799 --> 00:23:16,559
very fine configuration of mmu and

00:23:13,039 --> 00:23:19,760
in switching ace so for that uh

00:23:16,559 --> 00:23:23,120
masai has introduced

00:23:19,760 --> 00:23:25,600
a comprehensive mmu object model that

00:23:23,120 --> 00:23:25,600
covers

00:23:26,240 --> 00:23:30,320
pretty much all the qos use cases

00:23:29,039 --> 00:23:33,600
including

00:23:30,320 --> 00:23:37,039
u.s mapping shared buffers

00:23:33,600 --> 00:23:38,640
policers and schedulers also profiles

00:23:37,039 --> 00:23:41,160
for them that you can attach

00:23:38,640 --> 00:23:43,039
per port per queue

00:23:41,160 --> 00:23:46,400
[Music]

00:23:43,039 --> 00:23:49,360
it also includes uh support for

00:23:46,400 --> 00:23:51,039
multiple queuing algorithms including

00:23:49,360 --> 00:23:54,080
ecm and so on

00:23:51,039 --> 00:23:57,600
and right now uh all

00:23:54,080 --> 00:24:00,640
all of the mmu is controlled directly

00:23:57,600 --> 00:24:03,039
through psy without interacting with

00:24:00,640 --> 00:24:03,039
linux

00:24:03,279 --> 00:24:07,039
um next one

00:24:09,440 --> 00:24:16,720
a sonic virtual switch so this is

00:24:13,279 --> 00:24:20,320
um as opposed to what i

00:24:16,720 --> 00:24:22,640
have talked about before it is not a

00:24:20,320 --> 00:24:25,440
function it is not a network function

00:24:22,640 --> 00:24:28,960
that is implemented in linux but rather

00:24:25,440 --> 00:24:30,559
this is a virtual sonic platform for

00:24:28,960 --> 00:24:34,640
generic feature validation

00:24:30,559 --> 00:24:37,440
powered by linux so it comes into flavor

00:24:34,640 --> 00:24:37,760
two flavors uh first one is docker image

00:24:37,440 --> 00:24:40,960
uh

00:24:37,760 --> 00:24:42,400
second one is uh and virtual as a

00:24:40,960 --> 00:24:45,919
virtual machine uh

00:24:42,400 --> 00:24:47,440
image uh it has a core forwarding

00:24:45,919 --> 00:24:50,240
behavior modeled by

00:24:47,440 --> 00:24:51,840
uh linux kernel all the virtual ports

00:24:50,240 --> 00:24:54,390
for the data plane

00:24:51,840 --> 00:24:55,440
to simulate uh

00:24:54,390 --> 00:24:58,559
[Music]

00:24:55,440 --> 00:25:01,760
multiple port device layer to forwarding

00:24:58,559 --> 00:25:03,600
layer three routing

00:25:01,760 --> 00:25:06,159
including some additional features that

00:25:03,600 --> 00:25:09,440
i also mentioned like as flowers tc

00:25:06,159 --> 00:25:12,080
sample and control plane acls that

00:25:09,440 --> 00:25:13,200
are also working on this virtual switch

00:25:12,080 --> 00:25:17,440
which basically

00:25:13,200 --> 00:25:20,640
provides you with a

00:25:17,440 --> 00:25:24,000
linux switch with a sonic

00:25:20,640 --> 00:25:27,120
application stack so right now it is

00:25:24,000 --> 00:25:30,240
used for a generic feature validation

00:25:27,120 --> 00:25:33,600
with some capabilities of uh

00:25:30,240 --> 00:25:35,919
data plane forwarding uh

00:25:33,600 --> 00:25:36,640
and with that i want to thank everyone

00:25:35,919 --> 00:25:39,120
and

00:25:36,640 --> 00:25:40,240
give a stage to the next presenter hi

00:25:39,120 --> 00:25:43,440
everyone

00:25:40,240 --> 00:25:46,480
my name is kolimoto i work for sonic in

00:25:43,440 --> 00:25:49,120
broadcom communication

00:25:46,480 --> 00:25:49,120
and next slide

00:25:50,559 --> 00:25:54,480
here is the quick agenda for for this

00:25:53,279 --> 00:25:58,320
section

00:25:54,480 --> 00:26:00,799
um we will talk about the uh

00:25:58,320 --> 00:26:02,559
the net link messaging architecture in

00:26:00,799 --> 00:26:05,360
current linux

00:26:02,559 --> 00:26:08,000
and we'll see the system scaling issue

00:26:05,360 --> 00:26:10,000
with the current networking architecture

00:26:08,000 --> 00:26:13,120
and the proposed solution using the

00:26:10,000 --> 00:26:13,120
weekly socket filter

00:26:13,919 --> 00:26:20,480
next slide so

00:26:17,520 --> 00:26:23,200
in a typical networking world so the

00:26:20,480 --> 00:26:24,960
networking application mainly uses the

00:26:23,200 --> 00:26:27,279
net link route family circuit for

00:26:24,960 --> 00:26:29,919
receiving the notification from

00:26:27,279 --> 00:26:30,960
from the kernel net devices so

00:26:29,919 --> 00:26:33,200
specifically

00:26:30,960 --> 00:26:34,640
it will these applications are

00:26:33,200 --> 00:26:37,440
interested in

00:26:34,640 --> 00:26:39,039
port upper status mtu speed change and

00:26:37,440 --> 00:26:41,600
etc

00:26:39,039 --> 00:26:42,720
so any changes in these properties of

00:26:41,600 --> 00:26:44,320
net device

00:26:42,720 --> 00:26:46,080
the application should get a

00:26:44,320 --> 00:26:49,120
notification from the kernel

00:26:46,080 --> 00:26:52,400
so it mainly uses the netlink route

00:26:49,120 --> 00:26:55,120
for receiving those notifications

00:26:52,400 --> 00:26:56,960
so it's a broadcast domain so any

00:26:55,120 --> 00:26:59,200
attribute change in the

00:26:56,960 --> 00:27:01,039
netlink device it will generate a net

00:26:59,200 --> 00:27:03,520
link message

00:27:01,039 --> 00:27:05,520
so the netlink subsystem will will post

00:27:03,520 --> 00:27:06,640
them the nettling message to all the

00:27:05,520 --> 00:27:09,360
application

00:27:06,640 --> 00:27:10,960
who were registered for the framework so

00:27:09,360 --> 00:27:12,000
that then the application will receive

00:27:10,960 --> 00:27:15,520
the message from the

00:27:12,000 --> 00:27:15,520
queue and then it will process it

00:27:16,400 --> 00:27:22,080
this is the overview of the net link

00:27:19,760 --> 00:27:25,120
subsystem in the linux kernel

00:27:22,080 --> 00:27:28,480
and and we'll see uh the but scaling

00:27:25,120 --> 00:27:32,320
issue with this framework in next slide

00:27:28,480 --> 00:27:32,320
next slide yeah

00:27:32,840 --> 00:27:40,320
uh so as i said um so each net device

00:27:37,120 --> 00:27:43,279
uh will have a

00:27:40,320 --> 00:27:43,760
multiple attributes so each attribute

00:27:43,279 --> 00:27:46,640
change

00:27:43,760 --> 00:27:48,399
will generate a net link message so

00:27:46,640 --> 00:27:51,679
here's a typical example

00:27:48,399 --> 00:27:53,200
we have four applications and uh

00:27:51,679 --> 00:27:54,720
let's take one of the applications

00:27:53,200 --> 00:27:57,679
generating the net link event

00:27:54,720 --> 00:27:59,120
which is basically adding 4k vlan to the

00:27:57,679 --> 00:28:02,480
ethernet interface

00:27:59,120 --> 00:28:04,799
which in turn generates um 8k

00:28:02,480 --> 00:28:06,480
net link messages so let's say three

00:28:04,799 --> 00:28:07,760
applications are registered for these

00:28:06,480 --> 00:28:10,080
messages

00:28:07,760 --> 00:28:10,799
and each of them will get 8k messages

00:28:10,080 --> 00:28:13,679
messages and

00:28:10,799 --> 00:28:16,240
and and then it will um all the

00:28:13,679 --> 00:28:18,880
application will process them

00:28:16,240 --> 00:28:19,600
um so let's say like application two and

00:28:18,880 --> 00:28:22,480
three

00:28:19,600 --> 00:28:23,440
are not interested for the certain

00:28:22,480 --> 00:28:26,000
attribute change

00:28:23,440 --> 00:28:27,600
but still these applications should

00:28:26,000 --> 00:28:29,200
receive all the messages and then they

00:28:27,600 --> 00:28:32,320
have to process it

00:28:29,200 --> 00:28:35,279
um this is the problem uh where

00:28:32,320 --> 00:28:36,080
when in a scaled setup let's say in a

00:28:35,279 --> 00:28:39,200
fully scaled

00:28:36,080 --> 00:28:39,600
uh networking environment uh there will

00:28:39,200 --> 00:28:42,559
be

00:28:39,600 --> 00:28:44,720
uh more than one million uh net link

00:28:42,559 --> 00:28:46,880
message is getting flooded in the kernel

00:28:44,720 --> 00:28:49,600
where all the application has to process

00:28:46,880 --> 00:28:49,600
these messages

00:28:49,919 --> 00:28:54,399
in a loaded scenario uh there could be

00:28:53,279 --> 00:28:58,080
chance that these

00:28:54,399 --> 00:28:59,679
messages could be dropped or

00:28:58,080 --> 00:29:02,320
it cannot be processed because of the

00:28:59,679 --> 00:29:02,320
system load

00:29:02,559 --> 00:29:10,080
so this is the current uh issue with the

00:29:06,240 --> 00:29:13,679
net link subsystem in linux and

00:29:10,080 --> 00:29:17,520
we will see how we will address this

00:29:13,679 --> 00:29:17,520
this issue next slide

00:29:19,120 --> 00:29:26,240
so we solve this problem using a bpf

00:29:22,320 --> 00:29:29,279
filter berkeley circuit filter

00:29:26,240 --> 00:29:29,919
so what it does this filter is getting

00:29:29,279 --> 00:29:32,080
attached

00:29:29,919 --> 00:29:33,039
to uh every application socket in the

00:29:32,080 --> 00:29:36,080
kernel

00:29:33,039 --> 00:29:38,559
so whenever it receives a packet

00:29:36,080 --> 00:29:41,520
in the netflix sub system this filter is

00:29:38,559 --> 00:29:44,080
being executed as part of the

00:29:41,520 --> 00:29:46,559
the application circuit so the builder

00:29:44,080 --> 00:29:48,559
the filters basically

00:29:46,559 --> 00:29:52,559
drop all the message which the

00:29:48,559 --> 00:29:52,559
application is not interested for that

00:29:52,960 --> 00:29:56,000
let's say here um like vlan manager is

00:29:55,279 --> 00:30:00,640
generating

00:29:56,000 --> 00:30:02,320
uh 4k um

00:30:00,640 --> 00:30:04,080
basically it's adding a 4k vlan to the

00:30:02,320 --> 00:30:05,360
ethernet interface which in turn

00:30:04,080 --> 00:30:08,000
generates 8k

00:30:05,360 --> 00:30:08,640
message and the only interested

00:30:08,000 --> 00:30:11,840
application

00:30:08,640 --> 00:30:14,240
is uh the udl so the

00:30:11,840 --> 00:30:15,520
stpd and tmd are not interested for

00:30:14,240 --> 00:30:18,080
those messages

00:30:15,520 --> 00:30:18,799
so the field the net the pdf filter will

00:30:18,080 --> 00:30:20,399
automatically

00:30:18,799 --> 00:30:21,840
uh attach to the socket and then it

00:30:20,399 --> 00:30:23,360
filter all those messages so the

00:30:21,840 --> 00:30:25,120
application 2 and 3 will not receive

00:30:23,360 --> 00:30:27,919
these messages only application 4 will

00:30:25,120 --> 00:30:27,919
receive this message

00:30:29,200 --> 00:30:31,679
next slide

00:30:35,120 --> 00:30:40,480
uh here's a quick overview of how this

00:30:37,679 --> 00:30:40,480
filter will work

00:30:41,760 --> 00:30:48,320
so um so as part of the

00:30:45,039 --> 00:30:49,760
berkeley filter it creates the hashmap

00:30:48,320 --> 00:30:53,679
in the kernel

00:30:49,760 --> 00:30:56,799
so every application socket it creates

00:30:53,679 --> 00:30:57,840
a hashtag for a hash entry for each of

00:30:56,799 --> 00:30:59,440
the interface

00:30:57,840 --> 00:31:01,120
so the application will register with

00:30:59,440 --> 00:31:03,519
the netlink subsystem

00:31:01,120 --> 00:31:05,519
and and request for interested attribute

00:31:03,519 --> 00:31:08,960
let's say application one interested in

00:31:05,519 --> 00:31:12,000
only the port open um upper status

00:31:08,960 --> 00:31:15,039
then this the socket filter is created

00:31:12,000 --> 00:31:15,519
with attribute operation uh i mean the

00:31:15,039 --> 00:31:17,519
the

00:31:15,519 --> 00:31:18,720
filter is created with opera status as a

00:31:17,519 --> 00:31:21,120
key value

00:31:18,720 --> 00:31:22,559
so the the key will be the index and the

00:31:21,120 --> 00:31:25,679
value will be the attribute

00:31:22,559 --> 00:31:28,000
in this case uh upper status will be the

00:31:25,679 --> 00:31:31,279
attribute

00:31:28,000 --> 00:31:34,480
so whenever the net link subsystem

00:31:31,279 --> 00:31:37,120
sends the message to this socket it

00:31:34,480 --> 00:31:38,960
it basically checks the state of the

00:31:37,120 --> 00:31:40,880
previous state of this attribute

00:31:38,960 --> 00:31:42,640
if these two states are same then it

00:31:40,880 --> 00:31:44,799
simply drop that message

00:31:42,640 --> 00:31:46,960
there is a state change it's it sends

00:31:44,799 --> 00:31:49,600
the message to the application

00:31:46,960 --> 00:31:50,080
if any other other attribute comes i

00:31:49,600 --> 00:31:52,399
mean

00:31:50,080 --> 00:31:54,000
if network message comes with any other

00:31:52,399 --> 00:31:57,039
attribute change other than the

00:31:54,000 --> 00:31:58,960
offer status it simply drop that message

00:31:57,039 --> 00:32:01,039
so with that the application will

00:31:58,960 --> 00:32:02,640
receive only the interested attribute

00:32:01,039 --> 00:32:05,840
and it will drop all other attribute

00:32:02,640 --> 00:32:09,200
changes in the kernel itself

00:32:05,840 --> 00:32:12,799
so the so this is a quick

00:32:09,200 --> 00:32:14,559
quick overview of pbf pvf is a

00:32:12,799 --> 00:32:16,720
it's a socket filled socket filter

00:32:14,559 --> 00:32:18,399
written in a micro sm

00:32:16,720 --> 00:32:21,039
it will be executed in the kernel as a

00:32:18,399 --> 00:32:24,399
minimal vm

00:32:21,039 --> 00:32:27,279
so this microcode is gets executed uh

00:32:24,399 --> 00:32:28,880
as part of the socket for every circuit

00:32:27,279 --> 00:32:31,600
uh every needling

00:32:28,880 --> 00:32:33,039
message that that is received for that

00:32:31,600 --> 00:32:34,880
circuit

00:32:33,039 --> 00:32:36,480
so the return value decides whether the

00:32:34,880 --> 00:32:40,080
packet to be dropped or

00:32:36,480 --> 00:32:40,080
accept and send it to the application

00:32:40,320 --> 00:32:48,720
so this uh the uh asm code gets executed

00:32:45,360 --> 00:32:51,840
with respect to the center contacts so

00:32:48,720 --> 00:32:53,200
here if a network device is sending this

00:32:51,840 --> 00:32:56,320
event so it will be

00:32:53,200 --> 00:32:58,799
executed as part of the kernel context

00:32:56,320 --> 00:32:59,519
if application is sending this network

00:32:58,799 --> 00:33:01,120
message

00:32:59,519 --> 00:33:02,559
then it will be executed as part of the

00:33:01,120 --> 00:33:06,960
application

00:33:02,559 --> 00:33:10,000
contacts in kernel so to achieve this

00:33:06,960 --> 00:33:11,600
we need to do two things one so we have

00:33:10,000 --> 00:33:14,640
submitted a minimal

00:33:11,600 --> 00:33:16,559
kernel patch to support this

00:33:14,640 --> 00:33:18,399
attribute processing helper function in

00:33:16,559 --> 00:33:21,519
kernel and this

00:33:18,399 --> 00:33:23,840
patch is under review

00:33:21,519 --> 00:33:25,200
second the each application will need to

00:33:23,840 --> 00:33:27,840
write a

00:33:25,200 --> 00:33:28,320
filter code and they have to register

00:33:27,840 --> 00:33:31,200
with the

00:33:28,320 --> 00:33:32,720
um the net link framework in the corner

00:33:31,200 --> 00:33:33,440
stating that what are all the interested

00:33:32,720 --> 00:33:35,840
attribute

00:33:33,440 --> 00:33:38,080
that that application needs to receive

00:33:35,840 --> 00:33:41,840
the net language is from the kernel

00:33:38,080 --> 00:33:42,480
hi this is kiran keller i work on sony

00:33:41,840 --> 00:33:46,000
software

00:33:42,480 --> 00:33:48,320
with broadcom communications so

00:33:46,000 --> 00:33:52,000
i'm going to cover how fulcrum nat

00:33:48,320 --> 00:33:55,200
requirement is handled in linux

00:33:52,000 --> 00:33:58,399
so today uh the

00:33:55,200 --> 00:34:00,960
traditional nat that linux does uh is

00:33:58,399 --> 00:34:03,279
based on the fight triple uniqueness of

00:34:00,960 --> 00:34:06,320
the translated contract entries

00:34:03,279 --> 00:34:07,519
so for each translation of any traffic

00:34:06,320 --> 00:34:11,200
going through the linux

00:34:07,519 --> 00:34:14,320
contract entry is created so

00:34:11,200 --> 00:34:16,960
in this contract entries we can see the

00:34:14,320 --> 00:34:17,599
two tuples one for the original

00:34:16,960 --> 00:34:19,599
direction

00:34:17,599 --> 00:34:21,119
and the other for the written direction

00:34:19,599 --> 00:34:23,119
reply direction

00:34:21,119 --> 00:34:26,320
uh in the reply direction the contract

00:34:23,119 --> 00:34:29,440
entry will have the translated values

00:34:26,320 --> 00:34:31,839
uh this is how an ip tables look like

00:34:29,440 --> 00:34:33,359
looks like a rule looks like where we

00:34:31,839 --> 00:34:36,399
are adding a rule to do a

00:34:33,359 --> 00:34:38,159
snap which does the source address and

00:34:36,399 --> 00:34:41,599
port translation

00:34:38,159 --> 00:34:45,200
from a source 192 168 10.0 network

00:34:41,599 --> 00:34:46,079
to a network 125 56 1950 uh within a

00:34:45,200 --> 00:34:48,560
port range

00:34:46,079 --> 00:34:50,000
so that is an external ip with a port

00:34:48,560 --> 00:34:52,240
range

00:34:50,000 --> 00:34:53,839
so here if we take two sample flows uh

00:34:52,240 --> 00:34:56,000
where uh

00:34:53,839 --> 00:34:57,040
traffic from two different internal

00:34:56,000 --> 00:34:59,440
hosts

00:34:57,040 --> 00:35:00,240
uh 10.1 and tender two are sending

00:34:59,440 --> 00:35:03,599
traffic to

00:35:00,240 --> 00:35:05,200
external uh world we see that the linux

00:35:03,599 --> 00:35:07,839
chooses

00:35:05,200 --> 00:35:09,119
the same external ip and the port for

00:35:07,839 --> 00:35:11,119
both the flows

00:35:09,119 --> 00:35:12,640
as long as the phi triple uniqueness is

00:35:11,119 --> 00:35:15,760
maintained

00:35:12,640 --> 00:35:17,119
so here the translated ipn port or

00:35:15,760 --> 00:35:18,640
both of them though that though they

00:35:17,119 --> 00:35:19,280
translated to the same external ip and

00:35:18,640 --> 00:35:21,440
port

00:35:19,280 --> 00:35:22,880
still they are fightable unique so in

00:35:21,440 --> 00:35:28,160
linux this is the

00:35:22,880 --> 00:35:28,160
regular net flow next slide please

00:35:29,359 --> 00:35:36,960
uh pulco nat is where uh

00:35:34,480 --> 00:35:37,760
any request from a particular internal

00:35:36,960 --> 00:35:40,560
ip and

00:35:37,760 --> 00:35:42,640
a port is always translated to the same

00:35:40,560 --> 00:35:44,160
external ipn port irrespective of the

00:35:42,640 --> 00:35:46,240
external destination that it is

00:35:44,160 --> 00:35:48,000
communicating with

00:35:46,240 --> 00:35:49,680
similarly in the reverse direction any

00:35:48,000 --> 00:35:53,200
external host which sends

00:35:49,680 --> 00:35:54,720
traffic to the same external ipn port

00:35:53,200 --> 00:35:57,119
will always get transferred to the same

00:35:54,720 --> 00:35:59,359
internal ipm port

00:35:57,119 --> 00:36:00,720
in some of the switching a6 hardware

00:35:59,359 --> 00:36:03,680
architectures uh

00:36:00,720 --> 00:36:06,480
the way the nat functions is uh using

00:36:03,680 --> 00:36:09,680
the three tuple uniqueness

00:36:06,480 --> 00:36:12,640
and such asics which wants to leverage

00:36:09,680 --> 00:36:13,680
on the nat functionality in linux i need

00:36:12,640 --> 00:36:15,920
this

00:36:13,680 --> 00:36:17,119
three tuple unique fulcrum add behavior

00:36:15,920 --> 00:36:20,240
in linux

00:36:17,119 --> 00:36:23,200
so we need so that both the linux

00:36:20,240 --> 00:36:24,079
nat uh ion contract entries as well as

00:36:23,200 --> 00:36:27,599
the a6

00:36:24,079 --> 00:36:31,119
nat entries are in sync so

00:36:27,599 --> 00:36:35,119
if you see example flows here uh we

00:36:31,119 --> 00:36:36,560
uh marked in red the source ipn port

00:36:35,119 --> 00:36:38,720
which are coming from the same internal

00:36:36,560 --> 00:36:40,480
host that are translated to the same

00:36:38,720 --> 00:36:45,040
external ip and

00:36:40,480 --> 00:36:48,320
port similarly any a different external

00:36:45,040 --> 00:36:50,960
host is sending traffic to the same

00:36:48,320 --> 00:36:52,720
external ipn port gets transferred to

00:36:50,960 --> 00:36:54,720
the

00:36:52,720 --> 00:36:56,240
pane three unique uh three tuple unique

00:36:54,720 --> 00:36:59,920
internal host and ip

00:36:56,240 --> 00:37:03,359
ipn port so to support this fulcrum at

00:36:59,920 --> 00:37:05,760
behavior needs the change in linux

00:37:03,359 --> 00:37:07,359
support additional enhancement in linux

00:37:05,760 --> 00:37:08,640
so that the three tuple uniqueness is

00:37:07,359 --> 00:37:10,480
maintained

00:37:08,640 --> 00:37:13,280
whenever user wants to create such

00:37:10,480 --> 00:37:13,280
iptable rules

00:37:13,440 --> 00:37:16,240
next slide please

00:37:16,960 --> 00:37:20,640
so to achieve that we need to introduce

00:37:19,760 --> 00:37:22,720
a

00:37:20,640 --> 00:37:23,839
new hash table similar to the existing

00:37:22,720 --> 00:37:27,839
hash table

00:37:23,839 --> 00:37:31,440
which is hashed on the

00:37:27,839 --> 00:37:34,480
source ipn port similar to that

00:37:31,440 --> 00:37:36,320
we had to add a new hash table that is

00:37:34,480 --> 00:37:38,720
hashing based on the

00:37:36,320 --> 00:37:40,720
translated ipn port so that this is

00:37:38,720 --> 00:37:43,119
useful in checking the uniqueness

00:37:40,720 --> 00:37:44,800
of the translated ipn port so that is

00:37:43,119 --> 00:37:47,280
needed for the full contact

00:37:44,800 --> 00:37:48,560
so in the snap direction uh whenever

00:37:47,280 --> 00:37:50,880
source is translated

00:37:48,560 --> 00:37:51,760
we check on this hash table and ensure

00:37:50,880 --> 00:37:54,079
that

00:37:51,760 --> 00:37:55,200
the translation is unique in the

00:37:54,079 --> 00:37:57,920
original direction

00:37:55,200 --> 00:37:58,720
snap direction similar to that whenever

00:37:57,920 --> 00:38:02,320
any traffic

00:37:58,720 --> 00:38:04,640
is coming in the reverse direction

00:38:02,320 --> 00:38:05,440
from any different exceptional source

00:38:04,640 --> 00:38:08,480
external

00:38:05,440 --> 00:38:08,960
host source uh we check on the same hash

00:38:08,480 --> 00:38:12,800
table

00:38:08,960 --> 00:38:14,079
to check the contract into corresponding

00:38:12,800 --> 00:38:16,480
contract entry uh

00:38:14,079 --> 00:38:17,680
based on the hashing of the translated

00:38:16,480 --> 00:38:20,720
ipn port

00:38:17,680 --> 00:38:23,680
so that ensures the fulcrum add behavior

00:38:20,720 --> 00:38:24,800
is achieved in both the directions the

00:38:23,680 --> 00:38:27,760
code changes

00:38:24,800 --> 00:38:28,480
are done in the nf nat code dot c and

00:38:27,760 --> 00:38:30,160
the

00:38:28,480 --> 00:38:32,560
routines that are changed are get unique

00:38:30,160 --> 00:38:33,359
tuple and nf net alpha proto-unique

00:38:32,560 --> 00:38:35,359
tuple

00:38:33,359 --> 00:38:37,680
like i said in the earlier slide so this

00:38:35,359 --> 00:38:41,680
uh these changes ensure that

00:38:37,680 --> 00:38:41,680
the fulcrum nad behavior is achieved

00:38:41,839 --> 00:38:46,720
so that the a6 which are supporting the

00:38:44,800 --> 00:38:50,800
three tuple unique translations

00:38:46,720 --> 00:38:53,839
can leverage on this change in the linux

00:38:50,800 --> 00:38:54,640
and to be able to program this fulcrum

00:38:53,839 --> 00:38:58,320
nat behavior

00:38:54,640 --> 00:39:01,440
in linux we need corresponding

00:38:58,320 --> 00:39:05,280
change in the iptables tool so that

00:39:01,440 --> 00:39:06,240
uh the new fulcrum attribute is passed

00:39:05,280 --> 00:39:08,320
along with the

00:39:06,240 --> 00:39:10,320
snat rule or the dna tool that we

00:39:08,320 --> 00:39:12,560
program in the linux

00:39:10,320 --> 00:39:13,520
so for that uh the iptables rule tool

00:39:12,560 --> 00:39:16,640
needs to be enhanced

00:39:13,520 --> 00:39:19,359
to pass on this fulcrum attribute

00:39:16,640 --> 00:39:23,040
and which will show up in the same ip

00:39:19,359 --> 00:39:24,640
display as well as the fulcrum rule

00:39:23,040 --> 00:39:27,200
so this is how overall we can achieve

00:39:24,640 --> 00:39:29,839
the fulcrum at in linux

00:39:27,200 --> 00:39:29,839
thank you

00:39:32,000 --> 00:39:36,640
okay um just as a quick time check we

00:39:35,520 --> 00:39:39,200
are currently one

00:39:36,640 --> 00:39:40,640
entire presentation behind so we're

00:39:39,200 --> 00:39:41,680
going to have to try and catch up a

00:39:40,640 --> 00:39:44,640
little bit there's a

00:39:41,680 --> 00:39:46,640
ton of questions in this topic and i

00:39:44,640 --> 00:39:50,400
think i'm going to have to try and

00:39:46,640 --> 00:39:51,680
summarize as best as i can apologies for

00:39:50,400 --> 00:39:52,640
people whose questions don't get

00:39:51,680 --> 00:39:54,160
addressed but

00:39:52,640 --> 00:39:56,240
maybe we can take it up in the happy

00:39:54,160 --> 00:39:58,240
hour or something else

00:39:56,240 --> 00:39:59,920
the high order questions that i'll

00:39:58,240 --> 00:40:02,800
summarize them all and maybe

00:39:59,920 --> 00:40:03,200
marianne or whoever's answering them one

00:40:02,800 --> 00:40:06,560
can

00:40:03,200 --> 00:40:07,200
answer them in in a batch number one is

00:40:06,560 --> 00:40:08,880
sonic

00:40:07,200 --> 00:40:10,640
best described as a routing payment

00:40:08,880 --> 00:40:12,400
infrastructure or a whole router

00:40:10,640 --> 00:40:15,520
including the data path

00:40:12,400 --> 00:40:17,359
number two seems to be there's a lot of

00:40:15,520 --> 00:40:19,200
overhead it's not clear what all the

00:40:17,359 --> 00:40:23,680
additional sonic functions are doing

00:40:19,200 --> 00:40:25,520
how does it affect latency number three

00:40:23,680 --> 00:40:26,800
seems to be about how are external

00:40:25,520 --> 00:40:29,599
signals provided

00:40:26,800 --> 00:40:31,200
is it isis are you using only bgp and so

00:40:29,599 --> 00:40:34,000
on and so forth

00:40:31,200 --> 00:40:35,920
the last one we'll glom in there is

00:40:34,000 --> 00:40:36,880
sasai exposes a lot of detail of the

00:40:35,920 --> 00:40:39,280
switch asic

00:40:36,880 --> 00:40:40,880
hardware a lot of it is hidden by the

00:40:39,280 --> 00:40:42,560
sonic user space demons

00:40:40,880 --> 00:40:43,920
how does this relate to switch div and

00:40:42,560 --> 00:40:47,200
unmodified user space

00:40:43,920 --> 00:40:50,079
where are the benefits um maybe

00:40:47,200 --> 00:40:51,839
as efficiently as you can answer these

00:40:50,079 --> 00:40:53,119
questions and we'll move on to the next

00:40:51,839 --> 00:40:56,160
topic

00:40:53,119 --> 00:40:58,000
uh yeah sure uh hi everyone uh so

00:40:56,160 --> 00:41:00,000
starting with the first question

00:40:58,000 --> 00:41:02,240
uh what is the best way to describe

00:41:00,000 --> 00:41:03,119
sonic i think the best way to describe

00:41:02,240 --> 00:41:05,520
it is to

00:41:03,119 --> 00:41:06,480
say that this is a router with the data

00:41:05,520 --> 00:41:08,560
plane

00:41:06,480 --> 00:41:09,599
uh so regarding with all the

00:41:08,560 --> 00:41:12,640
indirections

00:41:09,599 --> 00:41:15,680
and regarding uh to the net dev

00:41:12,640 --> 00:41:16,560
uh so why it is not used in sonic so

00:41:15,680 --> 00:41:20,160
today we have

00:41:16,560 --> 00:41:23,280
multiple vendors support uh who do not

00:41:20,160 --> 00:41:27,119
uh not all of them uh have

00:41:23,280 --> 00:41:30,400
uh support for uh for swiss though

00:41:27,119 --> 00:41:34,160
so uh for today the

00:41:30,400 --> 00:41:35,920
decision is to be able to run on the

00:41:34,160 --> 00:41:38,319
in the multi-vendor environment the same

00:41:35,920 --> 00:41:42,319
operating system is to use

00:41:38,319 --> 00:41:44,000
psy that all of the vendors provide the

00:41:42,319 --> 00:41:46,240
support for today

00:41:44,000 --> 00:41:48,319
uh we are looking to switch their

00:41:46,240 --> 00:41:52,640
directions but

00:41:48,319 --> 00:41:56,240
not as a plan to move it for now

00:41:52,640 --> 00:41:59,599
uh just in the research manner um

00:41:56,240 --> 00:42:00,839
so regarding uh what what is used in

00:41:59,599 --> 00:42:04,640
sonic for

00:42:00,839 --> 00:42:06,079
uh except bgp so isis is not used only

00:42:04,640 --> 00:42:09,200
bgp

00:42:06,079 --> 00:42:12,400
and uh

00:42:09,200 --> 00:42:12,400
did i miss any question

00:42:12,800 --> 00:42:18,880
no yes but we i mean there's a whole

00:42:16,400 --> 00:42:20,560
bunch of restartability and overhead and

00:42:18,880 --> 00:42:22,720
wires there containers for everything

00:42:20,560 --> 00:42:24,319
type of questions including one from me

00:42:22,720 --> 00:42:26,240
we'll just skip over those we'll take

00:42:24,319 --> 00:42:27,680
them offline there's a question from

00:42:26,240 --> 00:42:29,440
rupa about

00:42:27,680 --> 00:42:31,520
full cone map being upstream but

00:42:29,440 --> 00:42:33,440
actually i would expand that question to

00:42:31,520 --> 00:42:34,720
all of the comments changes that you

00:42:33,440 --> 00:42:35,839
guys have talked about including the

00:42:34,720 --> 00:42:38,079
filtering

00:42:35,839 --> 00:42:41,200
is relevant is are they all in an

00:42:38,079 --> 00:42:41,200
upstream path right now

00:42:42,480 --> 00:42:46,319
uh so for that uh i would need to

00:42:45,440 --> 00:42:49,599
actually

00:42:46,319 --> 00:42:52,079
do it uh patch by patch

00:42:49,599 --> 00:42:53,680
so uh because they are all upstream uh

00:42:52,079 --> 00:42:54,640
they are all provided to sonic by a

00:42:53,680 --> 00:42:58,000
different vendors

00:42:54,640 --> 00:43:00,079
so uh i don't have a single answer for

00:42:58,000 --> 00:43:01,599
all the patches and i think we don't

00:43:00,079 --> 00:43:02,480
have a single strategy for all the

00:43:01,599 --> 00:43:05,760
patches so

00:43:02,480 --> 00:43:08,000
it should be answered uh for us

00:43:05,760 --> 00:43:09,920
that that does i just make the comment

00:43:08,000 --> 00:43:10,800
that makes it difficult to say this is a

00:43:09,920 --> 00:43:13,760
linux based

00:43:10,800 --> 00:43:15,280
solution right because that answer needs

00:43:13,760 --> 00:43:17,839
to become everything is

00:43:15,280 --> 00:43:19,839
everything that touches linux is going

00:43:17,839 --> 00:43:20,880
upstream in a coherent consistently

00:43:19,839 --> 00:43:23,839
otherwise

00:43:20,880 --> 00:43:26,000
it's a patchwork solution right anyway

00:43:23,839 --> 00:43:29,599
yes so we are doing our best and

00:43:26,000 --> 00:43:29,599

YouTube URL: https://www.youtube.com/watch?v=BJGk-Rcdf48


