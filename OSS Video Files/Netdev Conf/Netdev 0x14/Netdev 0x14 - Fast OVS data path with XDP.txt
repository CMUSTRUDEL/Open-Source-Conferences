Title: Netdev 0x14 - Fast OVS data path with XDP
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Toshiaki Makita, William Tu

More info: https://netdevconf.info/0x14/session.html?talk-fast-OVS-data-path-with-XDP

Date: Thursday, August 20, 2020

Being able to marry new Linux technology with existing "legacy"
infra is valuable to allow for continuity. XDP is a relatively
new technology introduced in Linux networking which enables
more efficient packet processing. A common desire is to allow
"legacy" packet processing functions to take advantage of the
speed advantages offered by XDP.
In addition to the efficiency factor, XDP also provides a
time-to-market advantage: new network packet processing features
can be added without need to go through the complexity of the
upstreaming process.

Toshiaki Makita and William Tu are still in pursuit of getting
openvswitch to use XDP. In this talk Toshiaki and William attempt
to marry OVS to XDP via:
  1. Attaching XDP from kernel through UMH (user mode helper),
  2. Reusing TC HW acceleration interface
  3. Attaching XDP from OVS userspace daemon

The authors will describe their approaches and challenges faced.
Captions: 
	00:00:02,240 --> 00:00:06,000
the session title is fast always state

00:00:04,640 --> 00:00:09,760
path with

00:00:06,000 --> 00:00:14,320
xdp i'm toshiyaki makshita from ntt

00:00:09,760 --> 00:00:14,320
and the co-author is william 2 from

00:00:14,920 --> 00:00:20,320
vmware

00:00:17,279 --> 00:00:24,880
this is outline we first

00:00:20,320 --> 00:00:30,320
explain what is xdp and fxdp

00:00:24,880 --> 00:00:33,280
next we introduce osfxtp native

00:00:30,320 --> 00:00:34,320
the main contents are first of a state

00:00:33,280 --> 00:00:37,520
path

00:00:34,320 --> 00:00:40,559
we have two approaches

00:00:37,520 --> 00:00:41,120
one is approach a except flow and the

00:00:40,559 --> 00:00:46,399
other is

00:00:41,120 --> 00:00:49,440
approach b obvious xdp flow api provider

00:00:46,399 --> 00:00:51,840
then performance challenges and summary

00:00:49,440 --> 00:00:51,840
followed

00:00:54,079 --> 00:00:59,600
okay so first i explain what is xdp

00:01:00,879 --> 00:01:08,240
xdp is short for express database

00:01:04,559 --> 00:01:12,159
it's an increment fast database

00:01:08,240 --> 00:01:15,200
it allows users to attach ebps program

00:01:12,159 --> 00:01:15,200
to unique driver

00:01:16,640 --> 00:01:23,439
the ebp program runs immediately after

00:01:19,840 --> 00:01:23,439
the driver receives packets

00:01:23,520 --> 00:01:28,240
and it can modify packets or determine

00:01:26,000 --> 00:01:31,439
how to handle the packets

00:01:28,240 --> 00:01:33,520
for example it can drop packets by xtp

00:01:31,439 --> 00:01:38,560
drop auction

00:01:33,520 --> 00:01:42,560
or send packets back by xcptx

00:01:38,560 --> 00:01:44,880
or pass packets to the upper layer

00:01:42,560 --> 00:01:51,759
traditional network stock

00:01:44,880 --> 00:01:51,759
by xdp pass or forward packets by xtp

00:01:52,840 --> 00:01:55,840
direct

00:01:56,479 --> 00:02:04,880
the characteristics of x2p are

00:02:00,399 --> 00:02:08,640
fast it's fast with minimum overhead

00:02:04,880 --> 00:02:11,840
and flexible it has high programmability

00:02:08,640 --> 00:02:14,239
thanks to ebpf and

00:02:11,840 --> 00:02:15,360
integration with existing kernel network

00:02:14,239 --> 00:02:19,520
stock

00:02:15,360 --> 00:02:19,520
because it has xdp pass action

00:02:20,560 --> 00:02:25,120
but one of the problems is it's hard to

00:02:23,840 --> 00:02:29,360
use

00:02:25,120 --> 00:02:32,800
mainly because of ebp verifier

00:02:29,360 --> 00:02:37,840
ebp has verified it checks if

00:02:32,800 --> 00:02:37,840
the ebpf program is safe or not

00:02:38,720 --> 00:02:42,400
and if it determines the program it's

00:02:40,879 --> 00:02:45,040
not safe

00:02:42,400 --> 00:02:46,720
loading of epp program will fail with

00:02:45,040 --> 00:02:49,920
error

00:02:46,720 --> 00:02:50,400
the verifier is very strict so in many

00:02:49,920 --> 00:02:53,120
cases

00:02:50,400 --> 00:02:55,040
ebp programmer is to be very familiar

00:02:53,120 --> 00:03:00,080
with ebpf

00:02:55,040 --> 00:03:00,080
this can discourage people to use xgb

00:03:03,680 --> 00:03:10,640
so what we are thinking is

00:03:07,200 --> 00:03:11,920
if someone prepared typical xdp network

00:03:10,640 --> 00:03:16,560
functions

00:03:11,920 --> 00:03:19,200
it would help users in other words

00:03:16,560 --> 00:03:21,040
let's accelerate existing features by

00:03:19,200 --> 00:03:24,159
xjp

00:03:21,040 --> 00:03:27,920
like virtual switch routing

00:03:24,159 --> 00:03:27,920
firewall traffic control

00:03:28,400 --> 00:03:32,799
and now we are focusing on virtual

00:03:31,360 --> 00:03:35,200
cities

00:03:32,799 --> 00:03:35,840
especially the default standard budget

00:03:35,200 --> 00:03:38,560
switch

00:03:35,840 --> 00:03:38,560
open visit

00:03:42,959 --> 00:03:51,280
okay so linguinix afx dp

00:03:47,200 --> 00:03:54,799
actually is another way of using xdp

00:03:51,280 --> 00:03:57,840
so it's a new socket type that enable

00:03:54,799 --> 00:03:58,879
receiving and sending raw friends with a

00:03:57,840 --> 00:04:01,920
very high speed

00:03:58,879 --> 00:04:06,080
close close to the line rate of the

00:04:01,920 --> 00:04:09,760
new car so the idea is that users can

00:04:06,080 --> 00:04:12,000
inject or inject xdp program

00:04:09,760 --> 00:04:13,200
at the driver layer like showing on the

00:04:12,000 --> 00:04:15,920
right hand side

00:04:13,200 --> 00:04:17,199
so the xtp layer can decide whether this

00:04:15,920 --> 00:04:19,519
package should

00:04:17,199 --> 00:04:21,919
continue going into the linux kernel

00:04:19,519 --> 00:04:26,160
stack or bypass the kernel

00:04:21,919 --> 00:04:29,360
and send the packet to the af xdp socket

00:04:26,160 --> 00:04:30,000
so with the driver supporting zero copy

00:04:29,360 --> 00:04:32,720
and

00:04:30,000 --> 00:04:34,639
enabling dma buffer directly from the

00:04:32,720 --> 00:04:37,840
driver to the user space

00:04:34,639 --> 00:04:41,199
afx dp socket buffer uh

00:04:37,840 --> 00:04:43,840
this shows a very good performance ah

00:04:41,199 --> 00:04:43,840
next slide

00:04:45,040 --> 00:04:52,639
so since ovs 212

00:04:49,040 --> 00:04:57,040
we integrate the idea of afxtp

00:04:52,639 --> 00:05:00,639
into the ovs so the idea is that

00:04:57,040 --> 00:05:02,479
for obs we can use afx dp circuit as a

00:05:00,639 --> 00:05:05,280
fast channel

00:05:02,479 --> 00:05:06,800
to bypass the kernel so very similar to

00:05:05,280 --> 00:05:09,759
the idea of dpdk

00:05:06,800 --> 00:05:11,360
but instead of using dpdk library to

00:05:09,759 --> 00:05:15,280
send and receive packet

00:05:11,360 --> 00:05:18,320
we are using af xdp here

00:05:15,280 --> 00:05:19,120
and once the ovs user space receives the

00:05:18,320 --> 00:05:21,280
packet

00:05:19,120 --> 00:05:22,720
the rest of the package processing like

00:05:21,280 --> 00:05:24,560
passing the packet

00:05:22,720 --> 00:05:25,840
look up the different tables and

00:05:24,560 --> 00:05:29,680
applying action

00:05:25,840 --> 00:05:32,400
so the code is you is the same as

00:05:29,680 --> 00:05:33,120
used by the obvious dpdk so it's sharing

00:05:32,400 --> 00:05:36,000
the same

00:05:33,120 --> 00:05:36,800
user space data path implementation it's

00:05:36,000 --> 00:05:39,520
just the

00:05:36,800 --> 00:05:42,080
packet io library is different than the

00:05:39,520 --> 00:05:44,080
dpdk one

00:05:42,080 --> 00:05:45,440
however the current implementation of

00:05:44,080 --> 00:05:48,880
ovs

00:05:45,440 --> 00:05:52,080
we inject only a minimum

00:05:48,880 --> 00:05:55,039
xtp program which forwards all the

00:05:52,080 --> 00:05:58,400
package from the xdp to the user space

00:05:55,039 --> 00:06:00,000
so we didn't do anything in the xdp

00:05:58,400 --> 00:06:02,479
context

00:06:00,000 --> 00:06:02,479
next slide

00:06:05,199 --> 00:06:08,720
so uh when we are measuring the

00:06:08,000 --> 00:06:11,120
performance

00:06:08,720 --> 00:06:13,280
using uh virtual machine and container

00:06:11,120 --> 00:06:15,280
we actually found that for the

00:06:13,280 --> 00:06:18,319
virtual machine use case because it's

00:06:15,280 --> 00:06:18,960
running in user space like qemu process

00:06:18,319 --> 00:06:21,840
right

00:06:18,960 --> 00:06:22,560
so we can get pretty good performance

00:06:21,840 --> 00:06:25,440
however

00:06:22,560 --> 00:06:27,039
for the container use case because

00:06:25,440 --> 00:06:30,080
typical containers

00:06:27,039 --> 00:06:33,360
require using the kernel

00:06:30,080 --> 00:06:36,639
device for example vsphere device

00:06:33,360 --> 00:06:39,759
so when using ovs with afx dp

00:06:36,639 --> 00:06:42,720
the packet actually been forwarded by

00:06:39,759 --> 00:06:44,240
xdb to the user space and then inject

00:06:42,720 --> 00:06:47,840
into the kernel again

00:06:44,240 --> 00:06:50,319
and to the ve drive and then send it to

00:06:47,840 --> 00:06:52,160
the container traffic

00:06:50,319 --> 00:06:54,639
so this shows a little bit lower

00:06:52,160 --> 00:06:57,759
performance than we expect

00:06:54,639 --> 00:07:01,039
so in this work uh we are going to

00:06:57,759 --> 00:07:02,639
introduce like xdp we are going to

00:07:01,039 --> 00:07:05,280
enable xdp to do

00:07:02,639 --> 00:07:07,360
more processing in the kernel so

00:07:05,280 --> 00:07:08,880
basically steering the package in the

00:07:07,360 --> 00:07:12,080
xdp context

00:07:08,880 --> 00:07:14,240
and we implement the

00:07:12,080 --> 00:07:15,759
flow processing like pass the package

00:07:14,240 --> 00:07:19,199
lookup and action

00:07:15,759 --> 00:07:20,960
in the inside the xtp so that we can

00:07:19,199 --> 00:07:23,520
avoid this

00:07:20,960 --> 00:07:25,520
copy from kernel to user space and user

00:07:23,520 --> 00:07:31,840
space back to the kernel

00:07:25,520 --> 00:07:31,840
the problem next

00:07:32,639 --> 00:07:39,680
so this is our main contents

00:07:36,240 --> 00:07:43,360
first always datapath approaches

00:07:39,680 --> 00:07:46,879
we propose two approaches approach a

00:07:43,360 --> 00:07:48,080
is xcp flow this is an incarnate

00:07:46,879 --> 00:07:51,199
solution

00:07:48,080 --> 00:07:52,800
and approach b is obvious xdp flow api

00:07:51,199 --> 00:07:58,479
provider

00:07:52,800 --> 00:08:01,759
this is an in user specification

00:07:58,479 --> 00:08:04,560
so first let me explain approach a

00:08:01,759 --> 00:08:04,560
xdp flow

00:08:05,039 --> 00:08:11,440
xp flow is a

00:08:08,240 --> 00:08:14,720
generic flow of load enzyme to xzp

00:08:11,440 --> 00:08:18,000
in kernel so

00:08:14,720 --> 00:08:20,240
why doing that in kernel

00:08:18,000 --> 00:08:21,360
actually there are multiple network

00:08:20,240 --> 00:08:23,680
functions

00:08:21,360 --> 00:08:24,800
which are doing similar flow handling

00:08:23,680 --> 00:08:28,080
obs tc

00:08:24,800 --> 00:08:31,840
flow and nf tables so

00:08:28,080 --> 00:08:32,880
the idea is to create a generic xtp flow

00:08:31,840 --> 00:08:35,919
of load engine

00:08:32,880 --> 00:08:38,839
in kernel so that we can float all of

00:08:35,919 --> 00:08:41,360
similar flow features with the same

00:08:38,839 --> 00:08:43,760
mechanism

00:08:41,360 --> 00:08:44,640
those features are using existing

00:08:43,760 --> 00:08:47,760
generic flow

00:08:44,640 --> 00:08:51,200
hardware flows framework in kernel so we

00:08:47,760 --> 00:08:51,200
make use of this framework

00:08:51,440 --> 00:08:55,600
always is not directly uploaded by this

00:08:54,399 --> 00:08:57,760
framework

00:08:55,600 --> 00:09:00,640
but can be uploaded through tc flow or

00:08:57,760 --> 00:09:00,640
highly offload

00:09:03,519 --> 00:09:07,519
so before going into the details of xdp

00:09:07,040 --> 00:09:10,240
flow

00:09:07,519 --> 00:09:13,040
i'll explain how existing obs are we

00:09:10,240 --> 00:09:13,040
upload it's done

00:09:13,120 --> 00:09:17,680
the diagram shows how ovs handles

00:09:16,480 --> 00:09:21,360
packets without

00:09:17,680 --> 00:09:23,839
how they offload when the nic receives

00:09:21,360 --> 00:09:23,839
packets

00:09:28,240 --> 00:09:32,959
when the nick received packets they are

00:09:29,920 --> 00:09:35,600
100 in the first pass

00:09:32,959 --> 00:09:36,880
called date path in kernel or in user

00:09:35,600 --> 00:09:40,080
space

00:09:36,880 --> 00:09:43,760
the database handles packets based on

00:09:40,080 --> 00:09:47,600
flow tables in it but if database misses

00:09:43,760 --> 00:09:49,519
an appropriate entry in flow tables

00:09:47,600 --> 00:09:52,160
it passes buckets to the upper layer

00:09:49,519 --> 00:09:52,160
slow bus

00:09:52,399 --> 00:09:57,279
this is called opcode then the slowpass

00:09:56,240 --> 00:10:05,600
installs flows

00:09:57,279 --> 00:10:09,360
into database flow tables

00:10:05,600 --> 00:10:12,560
and with dc harder flow of obs

00:10:09,360 --> 00:10:15,440
the slow pass installs flows into

00:10:12,560 --> 00:10:16,160
tc floor at the same time when it

00:10:15,440 --> 00:10:19,519
installs

00:10:16,160 --> 00:10:19,519
flows into data database

00:10:20,240 --> 00:10:29,040
and then tc flow installs flows

00:10:24,399 --> 00:10:29,040
into hardware and x

00:10:29,920 --> 00:10:37,440
this way always flows can be uploaded to

00:10:33,519 --> 00:10:37,440
hardware index through tc floor

00:10:41,680 --> 00:10:45,600
and this is more detailed diagram of tc

00:10:44,800 --> 00:10:48,560
floor

00:10:45,600 --> 00:10:48,560
audio float

00:10:50,000 --> 00:10:57,680
tc flow actually uses the generic flow

00:10:53,920 --> 00:11:01,839
offload infrastructure in kernel

00:10:57,680 --> 00:11:01,839
this is shared with nf tables

00:11:02,880 --> 00:11:10,079
and the flow of the infrastructure uses

00:11:05,920 --> 00:11:15,839
callbacks in flow flow driver for ethnic

00:11:10,079 --> 00:11:15,839
in order to do the real of loading tasks

00:11:17,120 --> 00:11:22,240
okay so let's return to xzp flow

00:11:26,640 --> 00:11:31,680
xcp flow is a flow flow driver in kernel

00:11:32,640 --> 00:11:39,680
you can see that this nick

00:11:36,160 --> 00:11:44,240
nick flow flow driver is replaced with

00:11:39,680 --> 00:11:48,959
xdp flow it installs flows

00:11:44,240 --> 00:11:48,959
into xdp instead of hardwnx

00:11:49,440 --> 00:11:56,079
obviously is offloaded by xjp flow

00:11:52,560 --> 00:11:58,560
through tc flow offload also

00:11:56,079 --> 00:12:00,079
with this mechanism nf tables can be

00:11:58,560 --> 00:12:04,240
uploaded to xzp

00:12:00,079 --> 00:12:06,560
as well because xtp flow uses

00:12:04,240 --> 00:12:11,839
the generic flow of the infrastructure

00:12:06,560 --> 00:12:11,839
shared with nf tables

00:12:13,519 --> 00:12:18,880
and this is more detail the xdp flow

00:12:16,160 --> 00:12:18,880
control plane

00:12:19,519 --> 00:12:24,240
xcp flow actually needs a user space

00:12:22,839 --> 00:12:27,519
program

00:12:24,240 --> 00:12:29,839
to attach an xtp program because

00:12:27,519 --> 00:12:32,480
xcp programs cannot be attached from

00:12:29,839 --> 00:12:36,800
kernel context

00:12:32,480 --> 00:12:40,320
so xdpflow uses user mode helper

00:12:36,800 --> 00:12:44,079
embedded in xdp flowcarnal module

00:12:40,320 --> 00:12:45,200
xzp flow launches the embedded user mode

00:12:44,079 --> 00:12:49,200
helper

00:12:45,200 --> 00:12:51,680
a demon program called xjpflow umh

00:12:49,200 --> 00:12:54,320
and the umh does any bp in bpf

00:12:51,680 --> 00:12:57,440
operations

00:12:54,320 --> 00:13:01,040
flow tables for the xdp program

00:12:57,440 --> 00:13:04,480
are ebp maps and map manipulation is

00:13:01,040 --> 00:13:04,480
also done by your image

00:13:04,880 --> 00:13:21,120
in this model the xjp program

00:13:08,959 --> 00:13:25,600
used by xcp flow is also embedded in umh

00:13:21,120 --> 00:13:25,600
this shows data plan of xp flow

00:13:26,880 --> 00:13:33,680
with xcp flows xjp program attached

00:13:31,040 --> 00:13:35,839
most received packets are handled in xcp

00:13:33,680 --> 00:13:38,160
context

00:13:35,839 --> 00:13:39,920
if the xcp program misses an appropriate

00:13:38,160 --> 00:13:43,279
flow table entry

00:13:39,920 --> 00:13:44,160
in this flow table the received packet

00:13:43,279 --> 00:13:48,320
is passed to

00:13:44,160 --> 00:13:52,480
os database in kernel

00:13:48,320 --> 00:13:52,480
and data path handles packets as normal

00:13:57,519 --> 00:14:00,639
so pros and cons

00:14:01,040 --> 00:14:08,000
the advantages of xzp flow is that

00:14:05,440 --> 00:14:09,120
it can share codes with multiple

00:14:08,000 --> 00:14:12,560
functions

00:14:09,120 --> 00:14:15,680
obs tc and nf tables

00:14:12,560 --> 00:14:17,680
it also has transparent ui

00:14:15,680 --> 00:14:20,959
users can use existing commands like

00:14:17,680 --> 00:14:20,959
always or ctl

00:14:21,120 --> 00:14:26,399
and the ui to enable the feature is also

00:14:24,480 --> 00:14:30,240
simple

00:14:26,399 --> 00:14:32,480
we can just use east to k to enable xdp

00:14:30,240 --> 00:14:32,480
flow

00:14:35,199 --> 00:14:45,760
but the drawback is the complexity due

00:14:37,839 --> 00:14:48,880
to indirection layers like umh

00:14:45,760 --> 00:14:52,800
you can see there are many

00:14:48,880 --> 00:14:52,800
direction layers in the diagram

00:14:55,680 --> 00:15:01,519
and another problem is that

00:14:58,800 --> 00:15:03,760
the xcp program is embedded and cannot

00:15:01,519 --> 00:15:08,480
be modified

00:15:03,760 --> 00:15:11,440
so users cannot customize xjp programs

00:15:08,480 --> 00:15:12,160
for their use cases and there's no

00:15:11,440 --> 00:15:17,120
integration

00:15:12,160 --> 00:15:17,120
with fxjp native of obvious

00:15:22,240 --> 00:15:28,720
so xzp flow is not necessarily

00:15:25,760 --> 00:15:28,720
the best solution

00:15:29,040 --> 00:15:33,680
i posted xdp floor rsc in native

00:15:32,320 --> 00:15:38,560
committee

00:15:33,680 --> 00:15:38,560
mailing list and got some alternative

00:15:38,839 --> 00:15:43,680
approaches

00:15:40,399 --> 00:15:46,320
the first one is to create a new user

00:15:43,680 --> 00:15:49,199
space demo for tc flow

00:15:46,320 --> 00:15:49,199
xdp of load

00:15:50,399 --> 00:15:57,360
it snoops dc net link events

00:15:53,759 --> 00:16:00,000
ebv expressions based on the events

00:15:57,360 --> 00:16:00,800
the problem of this solution is that tc

00:16:00,000 --> 00:16:03,440
emulation

00:16:00,800 --> 00:16:07,040
from user space is difficult because pc

00:16:03,440 --> 00:16:07,040
flow is changing frequently

00:16:07,759 --> 00:16:14,399
and the second one is to create an ebpf

00:16:11,120 --> 00:16:15,519
helper function in kernel to access flow

00:16:14,399 --> 00:16:20,240
tables of

00:16:15,519 --> 00:16:22,720
obvious current module from xcp programs

00:16:20,240 --> 00:16:25,279
but it's hard to refactor obvious module

00:16:22,720 --> 00:16:28,399
to expose the api

00:16:25,279 --> 00:16:29,360
and also this does not work with obvious

00:16:28,399 --> 00:16:32,240
af except

00:16:29,360 --> 00:16:32,240
space database

00:16:33,839 --> 00:16:39,360
the last one is to handle ebv tasks in

00:16:36,959 --> 00:16:42,720
obvious user space demo

00:16:39,360 --> 00:16:42,720
this is our approach b

00:16:47,519 --> 00:16:53,360
approach b is obvious xdp flow api

00:16:50,880 --> 00:16:55,360
provider

00:16:53,360 --> 00:16:56,560
it's essentially user space

00:16:55,360 --> 00:16:59,920
implementation of

00:16:56,560 --> 00:17:04,079
xcp flow approach a

00:16:59,920 --> 00:17:07,280
it uses offload mechanism in obs pchd

00:17:04,079 --> 00:17:12,400
so it's solution specific to obs

00:17:07,280 --> 00:17:12,400
but it can work with fxdp native

00:17:13,600 --> 00:17:17,439
actually with this approach

00:17:16,079 --> 00:17:20,480
administrator

00:17:17,439 --> 00:17:24,480
touch any xcp programs as

00:17:20,480 --> 00:17:27,120
long as the program uses fxdp

00:17:24,480 --> 00:17:28,640
this means the program can be minimal

00:17:27,120 --> 00:17:32,400
for each use case

00:17:28,640 --> 00:17:34,960
which has minimal overhead

00:17:32,400 --> 00:17:35,520
at the same time we provide a refinance

00:17:34,960 --> 00:17:39,200
program

00:17:35,520 --> 00:17:39,200
for xcp flow of load

00:17:39,280 --> 00:17:47,840
in the overstock s3 and with that

00:17:43,280 --> 00:17:47,840
users can easily use the feature

00:17:52,960 --> 00:18:00,400
as i have talked obs has

00:17:56,160 --> 00:18:04,000
offline mechanism through tc flora

00:18:00,400 --> 00:18:07,679
the offload mechanism in obs is actually

00:18:04,000 --> 00:18:08,880
called flow api and there is another

00:18:07,679 --> 00:18:13,039
flow api

00:18:08,880 --> 00:18:15,919
called rd flow which is used by dpdk

00:18:13,039 --> 00:18:17,919
so currently there are two flow apis in

00:18:15,919 --> 00:18:22,720
obs

00:18:17,919 --> 00:18:22,720
and our proto-p has one more fluid api

00:18:26,840 --> 00:18:29,840
xzb

00:18:31,039 --> 00:18:34,400
currently approach b can be used only

00:18:34,000 --> 00:18:37,840
with

00:18:34,400 --> 00:18:40,080
f x d p native so

00:18:37,840 --> 00:18:41,520
this diagram shows obvious with user

00:18:40,080 --> 00:18:46,640
space database using

00:18:41,520 --> 00:18:46,640
fxdp native without our press key

00:18:46,799 --> 00:18:53,200
the xcp program is attached by

00:18:50,240 --> 00:18:54,480
fxdp native and it just passes all

00:18:53,200 --> 00:18:58,720
packets to

00:18:54,480 --> 00:18:58,720
database through fxjp socket

00:19:01,919 --> 00:19:09,039
and without approach b when the slow bus

00:19:05,600 --> 00:19:13,200
installs the flows into database

00:19:09,039 --> 00:19:19,120
xcp flow api provider installs the flows

00:19:13,200 --> 00:19:19,120
into ebp maps for the xdp program

00:19:20,559 --> 00:19:27,919
so xdp flow api provider plays

00:19:23,679 --> 00:19:27,919
the role of umh in approach a

00:19:31,840 --> 00:19:35,679
in approach a there were many

00:19:33,840 --> 00:19:38,400
indirection layers

00:19:35,679 --> 00:19:40,240
but in approach b we always know bus

00:19:38,400 --> 00:19:45,200
directory does

00:19:40,240 --> 00:19:45,200
evp tasks so it's very simple

00:19:47,280 --> 00:19:49,760
like this

00:19:50,960 --> 00:19:58,799
and note that users can use

00:19:54,480 --> 00:19:58,799
arbitrary xdp program here

00:20:01,840 --> 00:20:05,280
and as i said we provide a reference

00:20:03,679 --> 00:20:08,320
txtp program

00:20:05,280 --> 00:20:10,480
that works as a caching layer user space

00:20:08,320 --> 00:20:14,400
data path

00:20:10,480 --> 00:20:17,200
but users can customize this program

00:20:14,400 --> 00:20:18,320
for example they can remove unneeded

00:20:17,200 --> 00:20:22,400
flow keys

00:20:18,320 --> 00:20:22,400
or action handling in the program

00:20:28,240 --> 00:20:34,640
with approach b if the xcp program is

00:20:31,280 --> 00:20:38,640
our reference program or similar one

00:20:34,640 --> 00:20:40,960
most packets are handled in xzp context

00:20:38,640 --> 00:20:42,240
if the xjp program misses a flow table

00:20:40,960 --> 00:20:46,240
entry in

00:20:42,240 --> 00:20:49,280
xdb evp maps the packet is passed

00:20:46,240 --> 00:20:52,240
to user space date path through the af

00:20:49,280 --> 00:20:52,240
xdp socket

00:20:55,520 --> 00:21:02,480
and here's comparison between easy to

00:20:57,600 --> 00:21:05,840
flow the flow api

00:21:02,480 --> 00:21:07,600
xdp flow api provider has driver level

00:21:05,840 --> 00:21:10,960
performance

00:21:07,600 --> 00:21:14,159
its flexibility is high as users can

00:21:10,960 --> 00:21:17,200
attach any xcp program

00:21:14,159 --> 00:21:21,120
system requirement is linux kernel

00:21:17,200 --> 00:21:21,120
ebp fxdp feature support

00:21:23,440 --> 00:21:27,280
tunnel on the contract are not yet

00:21:28,840 --> 00:21:34,880
supported

00:21:31,440 --> 00:21:38,320
okay we

00:21:34,880 --> 00:21:43,280
measured the performance of approach b

00:21:38,320 --> 00:21:47,039
folding a little bit we used packet gen

00:21:43,280 --> 00:21:51,039
and traffic is one flow udp stream

00:21:47,039 --> 00:21:59,840
the sending rate is about 37 mbps

00:21:51,039 --> 00:21:59,840
which is 25 gigabit ethernet wire rate

00:22:00,640 --> 00:22:04,240
we have done two kind of tests

00:22:04,880 --> 00:22:09,919
one is i-40 to this

00:22:08,000 --> 00:22:11,039
we can't package successfully for it

00:22:09,919 --> 00:22:14,400
from i-40

00:22:11,039 --> 00:22:16,960
to this device and the other is

00:22:14,400 --> 00:22:17,919
i-40 to i-40 current package

00:22:16,960 --> 00:22:26,159
successfully

00:22:17,919 --> 00:22:29,440
forwarded from i-40 to i-40

00:22:26,159 --> 00:22:29,440
and this is the results

00:22:31,919 --> 00:22:38,960
the green one is open visit color module

00:22:35,679 --> 00:22:42,080
the baseline the blue one

00:22:38,960 --> 00:22:45,200
is fxdp native

00:22:42,080 --> 00:22:48,400
and the orange one is our approach b xdp

00:22:45,200 --> 00:22:48,400
flow api provider

00:22:49,840 --> 00:22:56,799
you can see that in both tests approach

00:22:53,280 --> 00:23:00,559
improves falling throughput

00:22:56,799 --> 00:23:04,240
the performance number is about 3.5 to

00:23:00,559 --> 00:23:06,640
3.7 millipps parkour

00:23:04,240 --> 00:23:08,400
here we use power core for the unit

00:23:06,640 --> 00:23:12,159
because afx zp uses

00:23:08,400 --> 00:23:15,200
two cores for one floor so fxdp result

00:23:12,159 --> 00:23:15,200
is divided by two

00:23:15,840 --> 00:23:22,159
and note that in i-40 to v-test

00:23:19,760 --> 00:23:23,679
the af xtp performance is low water

00:23:22,159 --> 00:23:26,640
kernel

00:23:23,679 --> 00:23:29,200
because this does not support zero copy

00:23:26,640 --> 00:23:32,480
feature with fxdb

00:23:29,200 --> 00:23:35,840
in that case afxjp needs to copy from

00:23:32,480 --> 00:23:37,919
copy packets from kernel to user space

00:23:35,840 --> 00:23:40,880
and vice versa

00:23:37,919 --> 00:23:42,159
so xdp flow api provider has more

00:23:40,880 --> 00:23:44,720
advantages

00:23:42,159 --> 00:23:51,840
with container traffic which often uses

00:23:44,720 --> 00:23:51,840
these devices

00:23:54,400 --> 00:23:58,320
and we have a couple of challenges

00:24:00,000 --> 00:24:07,600
first more keys and action support

00:24:04,320 --> 00:24:09,840
currently we support only very basic

00:24:07,600 --> 00:24:12,400
keys and options

00:24:09,840 --> 00:24:15,919
in the future more keys and actions like

00:24:12,400 --> 00:24:15,919
tunneling should be supported

00:24:16,080 --> 00:24:21,039
and second for the performance

00:24:18,799 --> 00:24:23,679
improvement

00:24:21,039 --> 00:24:26,400
actually i have tested an experimented

00:24:23,679 --> 00:24:30,640
patch in xdp flow rfc

00:24:26,400 --> 00:24:32,559
and it showed 5.2 mbps score

00:24:30,640 --> 00:24:34,799
so there is room to improve the

00:24:32,559 --> 00:24:38,400
performance

00:24:34,799 --> 00:24:41,440
the last one is how to offload

00:24:38,400 --> 00:24:42,320
xjp itself has a feature for hard

00:24:41,440 --> 00:24:44,240
offload

00:24:42,320 --> 00:24:47,039
but our reference program cannot be

00:24:44,240 --> 00:24:49,600
offloaded at this point

00:24:47,039 --> 00:24:51,360
one of the reasons is mapping map ebp

00:24:49,600 --> 00:24:53,919
feature

00:24:51,360 --> 00:25:01,840
we use mapping map but xcp hardware

00:24:53,919 --> 00:25:01,840
float doesn't support mapping map

00:25:02,080 --> 00:25:06,720
summary we proposed obvious performance

00:25:05,679 --> 00:25:10,960
acceleration by

00:25:06,720 --> 00:25:13,600
xdp we have two approaches

00:25:10,960 --> 00:25:14,880
approaching xcp flow is incarnal

00:25:13,600 --> 00:25:17,679
solution

00:25:14,880 --> 00:25:18,320
and approach b obvious xcp flow api

00:25:17,679 --> 00:25:21,760
provider

00:25:18,320 --> 00:25:25,039
is in user space solution approach b

00:25:21,760 --> 00:25:28,720
works with obvious fxjp

00:25:25,039 --> 00:25:31,279
native and we are now working on this

00:25:28,720 --> 00:25:31,279
approach b

00:25:32,000 --> 00:25:35,279
the forwarding throughput of approach b

00:25:34,240 --> 00:25:40,480
achieved about

00:25:35,279 --> 00:25:40,480
3.5 to 3.7 megapps parkour

00:25:41,760 --> 00:25:46,000
with our problem with our approach users

00:25:44,799 --> 00:25:49,120
can easily use

00:25:46,000 --> 00:25:53,440
xgb accelerated virtual switch

00:25:49,120 --> 00:25:53,440
fast and highly flexible obvious

00:25:55,440 --> 00:25:58,240
thank you very much

00:26:02,080 --> 00:26:06,880
okay uh thank you so we do have a few

00:26:05,679 --> 00:26:11,600
questions

00:26:06,880 --> 00:26:15,039
um but uh i wanted to start off with

00:26:11,600 --> 00:26:18,480
um performance

00:26:15,039 --> 00:26:18,799
so it seemed like the the reason to do

00:26:18,480 --> 00:26:21,919
this

00:26:18,799 --> 00:26:25,440
is mostly for performance um

00:26:21,919 --> 00:26:26,640
specifically the normal kernel ovs

00:26:25,440 --> 00:26:28,480
module

00:26:26,640 --> 00:26:39,039
we think we can do better is that the

00:26:28,480 --> 00:26:42,159
primary motivation here

00:26:39,039 --> 00:26:44,320
so sorry so what's the question uh is

00:26:42,159 --> 00:26:45,360
is performance the primary motivation

00:26:44,320 --> 00:26:48,080
for this work yeah

00:26:45,360 --> 00:26:49,120
yeah that's right okay so let's assume

00:26:48,080 --> 00:26:50,880
that that

00:26:49,120 --> 00:26:52,240
primary motivation is performance and i

00:26:50,880 --> 00:26:56,080
wanted to tie this into

00:26:52,240 --> 00:26:59,760
a discussion that we had around smartnix

00:26:56,080 --> 00:27:01,360
and i think um it was either andy orr

00:26:59,760 --> 00:27:04,240
mentioned something interesting so one

00:27:01,360 --> 00:27:07,600
of the value propositions of smartnix

00:27:04,240 --> 00:27:10,880
is that we can offload host cpu cycles

00:27:07,600 --> 00:27:12,559
into the smartnic and one of the side

00:27:10,880 --> 00:27:13,120
comments was something to the effect

00:27:12,559 --> 00:27:16,320
that

00:27:13,120 --> 00:27:20,080
ovs right now can take up to six

00:27:16,320 --> 00:27:21,039
xeon cores in the whole cpu to do its

00:27:20,080 --> 00:27:23,600
work

00:27:21,039 --> 00:27:25,200
and that's like 25 million packets per

00:27:23,600 --> 00:27:27,440
second or something like that

00:27:25,200 --> 00:27:29,600
so the obvious question i have here is

00:27:27,440 --> 00:27:32,559
would with this implementation

00:27:29,600 --> 00:27:34,240
improve that so we can reduce the the

00:27:32,559 --> 00:27:36,880
number of host scores that we're burning

00:27:34,240 --> 00:27:36,880
for vs

00:27:41,200 --> 00:27:44,640
uh yes so let me try to answer this

00:27:43,600 --> 00:27:46,240
question so

00:27:44,640 --> 00:27:48,080
yes we understand that when we are

00:27:46,240 --> 00:27:50,080
running ovs either

00:27:48,080 --> 00:27:51,279
when we are using obvious dpdk or

00:27:50,080 --> 00:27:54,640
obvious kernel

00:27:51,279 --> 00:27:58,720
or using this uh obvious xdp so

00:27:54,640 --> 00:28:02,000
obvious indeed to to hit light ray or

00:27:58,720 --> 00:28:04,480
we use a lot of cpu cycle and

00:28:02,000 --> 00:28:05,600
one solution is of course to use uh

00:28:04,480 --> 00:28:08,240
smartnig or to

00:28:05,600 --> 00:28:09,080
offload the traffic into the hardware

00:28:08,240 --> 00:28:12,480
asic or

00:28:09,080 --> 00:28:13,760
accelerators so we'll get the best

00:28:12,480 --> 00:28:17,039
performance when we can

00:28:13,760 --> 00:28:19,440
offload the hardware however some

00:28:17,039 --> 00:28:22,000
cases for some cases like we are doing

00:28:19,440 --> 00:28:22,880
pre obvious is doing pretty complicated

00:28:22,000 --> 00:28:25,440
pipeline like

00:28:22,880 --> 00:28:27,679
connection tracking or different tunnel

00:28:25,440 --> 00:28:29,360
setups

00:28:27,679 --> 00:28:31,120
it might not be able to overflow into

00:28:29,360 --> 00:28:33,360
the hardware or smart nic

00:28:31,120 --> 00:28:34,480
so that's why we are thinking about a

00:28:33,360 --> 00:28:37,679
better solution

00:28:34,480 --> 00:28:41,440
so one solution is of of course to use

00:28:37,679 --> 00:28:45,679
uh dpdk and another solution here

00:28:41,440 --> 00:28:48,799
is to use the xdp work here so

00:28:45,679 --> 00:28:51,120
which will provide better performance

00:28:48,799 --> 00:28:51,840
but of course slower than the highway of

00:28:51,120 --> 00:28:54,720
low

00:28:51,840 --> 00:28:56,960
but should be much better than the

00:28:54,720 --> 00:28:59,679
kernel they have passed

00:28:56,960 --> 00:29:00,399
so i so i say that the goal is of course

00:28:59,679 --> 00:29:04,000
performance

00:29:00,399 --> 00:29:04,720
or performance efficiency by using less

00:29:04,000 --> 00:29:08,080
corn to

00:29:04,720 --> 00:29:10,159
get better performance uh using the idea

00:29:08,080 --> 00:29:10,159
of

00:29:10,840 --> 00:29:13,840
xdp

00:29:15,679 --> 00:29:19,919
i also assume there's an ease of use

00:29:17,200 --> 00:29:22,480
issue that's probably

00:29:19,919 --> 00:29:24,159
applicable so doing something in the

00:29:22,480 --> 00:29:26,640
host on in user space

00:29:24,159 --> 00:29:28,399
as opposed to getting that in smart it's

00:29:26,640 --> 00:29:31,840
probably easier to use

00:29:28,399 --> 00:29:32,640
uh so the next question uh from strategy

00:29:31,840 --> 00:29:35,279
the two schemes

00:29:32,640 --> 00:29:36,320
ignore hardware offload hardware that

00:29:35,279 --> 00:29:40,720
can do flow

00:29:36,320 --> 00:29:44,559
cues steering tc hardware and the qx

00:29:40,720 --> 00:29:47,919
text ep af xdp could be an option nope

00:29:44,559 --> 00:29:48,880
uh yes so so i think it's a it's a good

00:29:47,919 --> 00:29:52,240
idea so

00:29:48,880 --> 00:29:55,679
but but right now for the uh approach be

00:29:52,240 --> 00:29:57,440
that uh we are proposing probably we are

00:29:55,679 --> 00:30:00,880
not using

00:29:57,440 --> 00:30:01,840
the this idea so right now in the case

00:30:00,880 --> 00:30:04,799
of obs

00:30:01,840 --> 00:30:06,559
it's either you choose the way to do tc

00:30:04,799 --> 00:30:09,679
flower or flow

00:30:06,559 --> 00:30:10,799
or you choose the another one you choose

00:30:09,679 --> 00:30:14,480
this one to use

00:30:10,799 --> 00:30:15,520
xtpl flow so the idea should you propose

00:30:14,480 --> 00:30:19,279
is to

00:30:15,520 --> 00:30:21,760
um put the tc highway of low at button

00:30:19,279 --> 00:30:22,799
and then if it doesn't fall back to the

00:30:21,760 --> 00:30:24,880
xdp one

00:30:22,799 --> 00:30:28,960
so we haven't implemented it this way

00:30:24,880 --> 00:30:28,960
but i think it's also a good idea

00:30:29,919 --> 00:30:34,640
what operations can be offloaded to the

00:30:33,039 --> 00:30:38,320
xdp

00:30:34,640 --> 00:30:41,360
uh yes so i think in the last slide

00:30:38,320 --> 00:30:41,840
we mentioned that they are still so the

00:30:41,360 --> 00:30:44,880
xc

00:30:41,840 --> 00:30:47,120
program right now is still pretty uh

00:30:44,880 --> 00:30:49,120
basic actually we have a lot of stuff

00:30:47,120 --> 00:30:50,240
but but for example like cognition

00:30:49,120 --> 00:30:54,320
tracking or

00:30:50,240 --> 00:30:57,279
the tunnel we haven't implemented in xdp

00:30:54,320 --> 00:30:57,919
but basic packet parsing and then table

00:30:57,279 --> 00:31:01,200
lookup

00:30:57,919 --> 00:31:04,320
and then actions are not there yet

00:31:01,200 --> 00:31:06,559
are there in our current patches patch

00:31:04,320 --> 00:31:09,200
set you know

00:31:06,559 --> 00:31:10,960
but the idea is we are hoping that with

00:31:09,200 --> 00:31:13,840
this framework there

00:31:10,960 --> 00:31:15,480
we can we hope people can contribute

00:31:13,840 --> 00:31:18,799
their x typical we can

00:31:15,480 --> 00:31:21,440
incrementally add more features

00:31:18,799 --> 00:31:23,679
in the xdp code to support like

00:31:21,440 --> 00:31:26,799
different actions

00:31:23,679 --> 00:31:29,840
so i think that also folds into the

00:31:26,799 --> 00:31:33,600
next class of questions so

00:31:29,840 --> 00:31:35,760
when we add bpf helpers

00:31:33,600 --> 00:31:38,559
the preference would be that they're not

00:31:35,760 --> 00:31:40,320
application specific not ovs specific

00:31:38,559 --> 00:31:42,159
so for instance we could have a generic

00:31:40,320 --> 00:31:44,799
tc flower

00:31:42,159 --> 00:31:45,679
bpf helper and i think that model could

00:31:44,799 --> 00:31:48,640
be extended

00:31:45,679 --> 00:31:50,480
if we can break down like all of the

00:31:48,640 --> 00:31:51,519
features that something like vswitch

00:31:50,480 --> 00:31:53,919
needs

00:31:51,519 --> 00:31:55,840
then those might define the sort of

00:31:53,919 --> 00:31:58,799
helpers or accelerations

00:31:55,840 --> 00:31:59,600
within that so i think that was answered

00:31:58,799 --> 00:32:04,480
reasonably

00:31:59,600 --> 00:32:07,679
on the chat yes

00:32:04,480 --> 00:32:11,039
uh so what is the performance with

00:32:07,679 --> 00:32:15,120
ovs dpdk and rte flow i guess

00:32:11,039 --> 00:32:18,480
what's a comparison to those

00:32:15,120 --> 00:32:21,600
so sorry i didn't compare it to

00:32:18,480 --> 00:32:24,799
comparison of this solution to dpdk

00:32:21,600 --> 00:32:26,880
and rte flow yeah well dpdk is faster

00:32:24,799 --> 00:32:29,919
but

00:32:26,880 --> 00:32:34,320
the xtp is more flexible and

00:32:29,919 --> 00:32:34,320
so for example dpdk

00:32:35,120 --> 00:32:42,880
need to assign memories and

00:32:38,480 --> 00:32:46,880
cpus so the characters can cursive

00:32:42,880 --> 00:32:49,919
different from xdp so we

00:32:46,880 --> 00:32:55,120
we don't need uh we shouldn't just

00:32:49,919 --> 00:32:57,200
compare the performance of them i think

00:32:55,120 --> 00:32:58,640
so uh yeah so let me add some comments

00:32:57,200 --> 00:33:01,200
so we didn't do

00:32:58,640 --> 00:33:02,960
the comparison but our performance

00:33:01,200 --> 00:33:06,799
should be better than

00:33:02,960 --> 00:33:09,760
uh should be better than if we consider

00:33:06,799 --> 00:33:11,440
container like in kernel processing then

00:33:09,760 --> 00:33:14,240
our performance should be better than

00:33:11,440 --> 00:33:15,519
obvious dpdk so imagine you have a

00:33:14,240 --> 00:33:18,559
container using

00:33:15,519 --> 00:33:21,039
kernel vs uh driver right so if you are

00:33:18,559 --> 00:33:22,880
using dpdk then then the problem is you

00:33:21,039 --> 00:33:25,120
send a packet to user space

00:33:22,880 --> 00:33:26,080
we obviously bdk then you pump the

00:33:25,120 --> 00:33:28,960
packet back

00:33:26,080 --> 00:33:29,519
into the kernel inject into the ves

00:33:28,960 --> 00:33:32,559
driver

00:33:29,519 --> 00:33:34,720
and then container get the packet

00:33:32,559 --> 00:33:37,679
while in well this work we don't need to

00:33:34,720 --> 00:33:39,840
traverse to the user space right so

00:33:37,679 --> 00:33:42,080
the xdb program was directly forward

00:33:39,840 --> 00:33:44,320
from the

00:33:42,080 --> 00:33:45,600
xdb friend and to the inject into the

00:33:44,320 --> 00:33:49,279
vis

00:33:45,600 --> 00:33:51,679
so yeah so we didn't do a

00:33:49,279 --> 00:33:54,399
performance comparison but we expect

00:33:51,679 --> 00:33:54,399
should be better

00:33:55,360 --> 00:33:59,919
okay so i guess what we're saying here

00:33:58,640 --> 00:34:01,760
is

00:33:59,919 --> 00:34:04,480
the performance different difference

00:34:01,760 --> 00:34:08,079
isn't necessarily a show stopper

00:34:04,480 --> 00:34:12,079
and there are some other advantages

00:34:08,079 --> 00:34:15,119
of of using xdp in the coronal data path

00:34:12,079 --> 00:34:16,639
uh in terms of flexibility uh

00:34:15,119 --> 00:34:20,320
so that's always good input to the

00:34:16,639 --> 00:34:23,359
community right so clearly dpdk

00:34:20,320 --> 00:34:25,359
uh was in the sense of motivation for us

00:34:23,359 --> 00:34:27,440
to spin up xdp

00:34:25,359 --> 00:34:28,879
so we're always looking for you know to

00:34:27,440 --> 00:34:31,119
validate that

00:34:28,879 --> 00:34:32,159
we're on the right path and that you

00:34:31,119 --> 00:34:33,440
know obviously we want to get every

00:34:32,159 --> 00:34:36,560
ounce of performance

00:34:33,440 --> 00:34:38,399
but at some level dptk would would

00:34:36,560 --> 00:34:41,520
inherently be faster

00:34:38,399 --> 00:34:42,639
but for real applications for real use

00:34:41,520 --> 00:34:44,639
cases

00:34:42,639 --> 00:34:45,839
um which were targeted at the end of the

00:34:44,639 --> 00:34:47,919
day as opposed to some

00:34:45,839 --> 00:34:50,639
synthetic benchmarks that's where you

00:34:47,919 --> 00:34:53,599
know we definitely need that input from

00:34:50,639 --> 00:34:55,679
um operators and mm versus thanks for

00:34:53,599 --> 00:34:58,880
that

00:34:55,679 --> 00:34:59,359
uh yes no problem so so uh one thing

00:34:58,880 --> 00:35:03,920
that

00:34:59,359 --> 00:35:06,240
so so we also did measurement using

00:35:03,920 --> 00:35:08,560
the virtual machine right so in the case

00:35:06,240 --> 00:35:10,480
of virtual machine then obvious dpdk

00:35:08,560 --> 00:35:13,920
will be faster because

00:35:10,480 --> 00:35:16,720
qemu runs in user space and if you apply

00:35:13,920 --> 00:35:17,599
like vhost user protocol for the virtual

00:35:16,720 --> 00:35:22,000
port

00:35:17,599 --> 00:35:24,000
then you get the performance right now

00:35:22,000 --> 00:35:25,920
okay that's good to know so there was

00:35:24,000 --> 00:35:26,880
another i guess a little bit of a side

00:35:25,920 --> 00:35:30,560
discussion

00:35:26,880 --> 00:35:35,440
on tc flower and that might be

00:35:30,560 --> 00:35:38,400
one to uh maybe defer to happy hour

00:35:35,440 --> 00:35:39,599
or a side talk but i think it's really

00:35:38,400 --> 00:35:43,839
an important topic

00:35:39,599 --> 00:35:44,320
as gml points out we're putting a lot of

00:35:43,839 --> 00:35:47,680
stuff

00:35:44,320 --> 00:35:48,000
on it and i think it's another case

00:35:47,680 --> 00:35:51,119
where

00:35:48,000 --> 00:35:53,359
we we invented something that's useful

00:35:51,119 --> 00:35:55,440
and very quickly becomes a victim of its

00:35:53,359 --> 00:35:58,800
own success because we just want to keep

00:35:55,440 --> 00:36:01,119
dumping on it so it might be interesting

00:35:58,800 --> 00:36:02,160
to figure out how to do that and then uh

00:36:01,119 --> 00:36:06,000
tc flower

00:36:02,160 --> 00:36:06,320
offload um how do we keep that in sync

00:36:06,000 --> 00:36:09,200
with

00:36:06,320 --> 00:36:09,760
with the host software so let's differ

00:36:09,200 --> 00:36:12,079
on that

00:36:09,760 --> 00:36:12,880
i think that maybe we can bring that up

00:36:12,079 --> 00:36:18,000
in

00:36:12,880 --> 00:36:18,000
happy hour uh okay

00:36:19,359 --> 00:36:23,440
okay so taurus i hope we get the name

00:36:22,079 --> 00:36:27,040
right i have a question

00:36:23,440 --> 00:36:29,599
raise your hand yes uh thanks uh

00:36:27,040 --> 00:36:31,839
just uh when you're talking about

00:36:29,599 --> 00:36:34,720
containers and this xdp approach

00:36:31,839 --> 00:36:35,280
uh can we say that with this approach we

00:36:34,720 --> 00:36:37,119
can get

00:36:35,280 --> 00:36:39,119
better price for performance because we

00:36:37,119 --> 00:36:42,960
do know that using dpdk is quite

00:36:39,119 --> 00:36:45,359
cpu intensive uh because of its nature

00:36:42,960 --> 00:36:46,079
can we say that for ex explicitly

00:36:45,359 --> 00:36:48,800
container

00:36:46,079 --> 00:36:50,480
use case we will get the better price

00:36:48,800 --> 00:36:53,599
for cpu utilization in

00:36:50,480 --> 00:36:56,720
case of performance or not

00:36:53,599 --> 00:36:59,359
um i i think yes we yes yes

00:36:56,720 --> 00:37:00,880
so so think about the code path right so

00:36:59,359 --> 00:37:02,640
so

00:37:00,880 --> 00:37:04,960
think about the code pass when you are

00:37:02,640 --> 00:37:08,160
using dp obviously btk right so

00:37:04,960 --> 00:37:09,119
you basically run much longer much

00:37:08,160 --> 00:37:11,599
larger code

00:37:09,119 --> 00:37:12,160
to go into the user space and come back

00:37:11,599 --> 00:37:15,119
then

00:37:12,160 --> 00:37:15,920
doing very very minimum processing like

00:37:15,119 --> 00:37:17,599
xdp

00:37:15,920 --> 00:37:19,760
just get the package from the driver

00:37:17,599 --> 00:37:23,359
immediate immediately and

00:37:19,760 --> 00:37:26,320
do something then just just send send it

00:37:23,359 --> 00:37:29,680
to the v's container

00:37:26,320 --> 00:37:29,680
so i guess yeah yeah

00:37:30,000 --> 00:37:33,760
so i'm assuming by price you mean

00:37:32,640 --> 00:37:36,960
technical debt

00:37:33,760 --> 00:37:38,320
uh cost not actual price so we kind of

00:37:36,960 --> 00:37:41,440
don't want to delve into marketing

00:37:38,320 --> 00:37:43,119
concerns here but uh good answer but by

00:37:41,440 --> 00:37:44,560
price i meant uh the

00:37:43,119 --> 00:37:46,560
when you when you have some performance

00:37:44,560 --> 00:37:48,079
it always comes as the cost cpu cycles

00:37:46,560 --> 00:37:50,720
number of course used

00:37:48,079 --> 00:37:51,680
or or the total processing power that

00:37:50,720 --> 00:37:53,680
was what that was

00:37:51,680 --> 00:37:55,359
basically used to to get this

00:37:53,680 --> 00:37:57,200
performance and my understanding that

00:37:55,359 --> 00:37:59,680
with this approach for container

00:37:57,200 --> 00:38:01,440
uh use case we can get it better so we

00:37:59,680 --> 00:38:05,040
can utilize cpu

00:38:01,440 --> 00:38:06,240
uh better to get uh more packets million

00:38:05,040 --> 00:38:07,520
packets per second i don't know maybe

00:38:06,240 --> 00:38:11,119
sometimes you'll get

00:38:07,520 --> 00:38:13,680
gigabits 100 200 we'll see

00:38:11,119 --> 00:38:15,520
oh yeah yeah so so i think one one

00:38:13,680 --> 00:38:18,240
matrix is to see the

00:38:15,520 --> 00:38:19,200
performance per coins right how many you

00:38:18,240 --> 00:38:21,520
achieve this

00:38:19,200 --> 00:38:22,560
performance but how many numbers of

00:38:21,520 --> 00:38:24,720
coins do you

00:38:22,560 --> 00:38:26,640
consume and i guess that's what you're

00:38:24,720 --> 00:38:28,480
talking about the price right so

00:38:26,640 --> 00:38:30,240
cloud customers today they care about

00:38:28,480 --> 00:38:33,440
like how many coins do i

00:38:30,240 --> 00:38:36,480
do i spin to achieve this performance

00:38:33,440 --> 00:38:40,079
or package rate so i guess xtp is

00:38:36,480 --> 00:38:46,079
way more efficiency in this case

00:38:40,079 --> 00:38:49,599
in this particular case

00:38:46,079 --> 00:38:51,040
okay so looks like i think we went over

00:38:49,599 --> 00:38:54,480
all the questions

00:38:51,040 --> 00:38:54,480

YouTube URL: https://www.youtube.com/watch?v=DU4POxNfYjM


