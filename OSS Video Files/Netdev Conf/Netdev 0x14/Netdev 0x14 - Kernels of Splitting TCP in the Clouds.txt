Title: Netdev 0x14 - Kernels of Splitting TCP in the Clouds
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Speakers: Markuze Alex, Bergman Aran, Dar Chen, Isaac Keslassy, Israel Cidon

More info: https://netdevconf.info/0x14/session.html?talk-kernels-of-splitting-TCP-in-the-clouds

Date: Tuesday, August 18, 2020

Markuze Alex will talk about how they improved
by orders of magnitude client download times of a
global overlay network across public clouds.
The overlay network known as the Pathway project
(operated by VMware Research) interconnects
geographical spread of public clouds and their vast
compute and networking infrastructure

The secret sauce? KTCP, a Kernel module running on a
modified Linux Kernel which implements novel TCP
splicing.

Markuze and co. will discuss why their approach is
different relative to the many approaches already
out in the wild that implement TCP proxying. 
They will present numbers against classical approaches
which will demonstrate that KTCP is able to considerably
increase the link utilization by TCP connections and
reduce the connection latency close to its theoretical
minimum.
Captions: 
	00:00:01,520 --> 00:00:04,240
hi my name is alex

00:00:03,040 --> 00:00:09,920
hey and i would like to talk to you

00:00:04,240 --> 00:00:12,799
about ktcp and in kernel split tcp proxy

00:00:09,920 --> 00:00:13,360
um the stock is partitioned into three

00:00:12,799 --> 00:00:14,639
sections

00:00:13,360 --> 00:00:17,359
first it will provide additional

00:00:14,639 --> 00:00:18,800
background in context the ktxp

00:00:17,359 --> 00:00:20,960
next i will cover the feature we've

00:00:18,800 --> 00:00:22,320
added to ktsp in order to improve tcp

00:00:20,960 --> 00:00:24,400
performance

00:00:22,320 --> 00:00:26,480
and finally i will talk about the

00:00:24,400 --> 00:00:28,560
specifics of get speed implementation

00:00:26,480 --> 00:00:32,160
and how we attempt to squeeze

00:00:28,560 --> 00:00:32,160
the most of each skill cycle

00:00:34,399 --> 00:00:37,760
kdcp was built as part of the pathway

00:00:37,120 --> 00:00:40,000
project

00:00:37,760 --> 00:00:41,200
an innovation program under the vemma

00:00:40,000 --> 00:00:44,239
office of the ceo

00:00:41,200 --> 00:00:46,000
specifically under vmware research

00:00:44,239 --> 00:00:48,079
in project pathway we were exploring

00:00:46,000 --> 00:00:51,200
ways to improve the mid-mile performance

00:00:48,079 --> 00:00:51,200
of sd1 servers

00:00:51,520 --> 00:00:58,160
for those of you who may not know sd1

00:00:54,960 --> 00:01:01,039
or software defined by their networks

00:00:58,160 --> 00:01:02,000
is a way to connect multi-site clients

00:01:01,039 --> 00:01:04,080
reliably

00:01:02,000 --> 00:01:06,000
this comes to this comes as a

00:01:04,080 --> 00:01:09,439
replacement with costly

00:01:06,000 --> 00:01:11,760
and inflexible in pls

00:01:09,439 --> 00:01:12,720
now the basic insight of project pathway

00:01:11,760 --> 00:01:14,799
is there are

00:01:12,720 --> 00:01:16,080
better infrastructure alternatives to

00:01:14,799 --> 00:01:19,520
the public internet

00:01:16,080 --> 00:01:19,520
which often serves as a midpoint

00:01:20,400 --> 00:01:24,159
each of the big three clouds have their

00:01:23,680 --> 00:01:26,560
own

00:01:24,159 --> 00:01:29,200
globe spanning network infrastructure

00:01:26,560 --> 00:01:31,920
that's aws gcp entitled

00:01:29,200 --> 00:01:33,600
it's possible to connect remote regions

00:01:31,920 --> 00:01:36,640
almost exclusively

00:01:33,600 --> 00:01:37,759
on existing cloud information and that's

00:01:36,640 --> 00:01:40,799
exactly

00:01:37,759 --> 00:01:42,640
what we did

00:01:40,799 --> 00:01:43,920
we build a separate link health

00:01:42,640 --> 00:01:47,439
monitoring

00:01:43,920 --> 00:01:49,520
system and we build a management system

00:01:47,439 --> 00:01:53,119
for deploying and management

00:01:49,520 --> 00:01:56,240
overland networks and we build ktcp

00:01:53,119 --> 00:01:58,000
kernel at this specific proxy

00:01:56,240 --> 00:02:00,240
today there are seven different regions

00:01:58,000 --> 00:02:00,960
around the globe a not counting our

00:02:00,240 --> 00:02:02,399
cloud

00:02:00,960 --> 00:02:04,240
where we can bring up our software

00:02:02,399 --> 00:02:06,560
routers where we monitor the connection

00:02:04,240 --> 00:02:08,560
quality

00:02:06,560 --> 00:02:10,640
in our experiments we have seen that the

00:02:08,560 --> 00:02:11,360
biggest impact on display performance is

00:02:10,640 --> 00:02:14,800
gained

00:02:11,360 --> 00:02:16,640
by splitting tcp on every proxy

00:02:14,800 --> 00:02:18,720
and in this talk i will obviously focus

00:02:16,640 --> 00:02:21,360
on the specific proxy

00:02:18,720 --> 00:02:22,720
and a detailed discussion regarding the

00:02:21,360 --> 00:02:25,920
whole system you can be found

00:02:22,720 --> 00:02:25,920
in this paper

00:02:26,560 --> 00:02:31,120
so what is split sipping split tcp is a

00:02:29,599 --> 00:02:32,959
well-known technique for enhancing

00:02:31,120 --> 00:02:35,519
display connections

00:02:32,959 --> 00:02:36,400
the purchasers is simple a single tcp

00:02:35,519 --> 00:02:39,920
connection

00:02:36,400 --> 00:02:43,360
is terminated on a proxy and two new

00:02:39,920 --> 00:02:45,760
tv connections are created each tcp

00:02:43,360 --> 00:02:48,080
connection is independent

00:02:45,760 --> 00:02:48,959
and may have a different configurations

00:02:48,080 --> 00:02:51,680
and the

00:02:48,959 --> 00:02:54,000
different projection control different

00:02:51,680 --> 00:02:55,920
setups at the

00:02:54,000 --> 00:02:57,599
so each has its own control loop and

00:02:55,920 --> 00:03:00,000
congestion crawl and that's completely

00:02:57,599 --> 00:03:03,680
separate

00:03:00,000 --> 00:03:06,720
so why is it a tcp

00:03:03,680 --> 00:03:07,920
modulates its behavior based on the acts

00:03:06,720 --> 00:03:10,319
it receives

00:03:07,920 --> 00:03:11,120
a tighter control loops provide a better

00:03:10,319 --> 00:03:14,640
tcp

00:03:11,120 --> 00:03:16,560
experience a shorter round trip time

00:03:14,640 --> 00:03:20,080
provides several benefits

00:03:16,560 --> 00:03:23,680
faster recovery from drops a faster

00:03:20,080 --> 00:03:27,200
ramp up of the speed the throughput

00:03:23,680 --> 00:03:30,799
a better throughput with smaller

00:03:27,200 --> 00:03:33,280
uh buffers and better competition with

00:03:30,799 --> 00:03:34,480
other small tt flows let me share the

00:03:33,280 --> 00:03:37,040
link

00:03:34,480 --> 00:03:39,680
we'll talk in greater detail how split

00:03:37,040 --> 00:03:39,680
xp works

00:03:40,319 --> 00:03:48,959
and this was the backdrop for ktcp

00:03:45,440 --> 00:03:52,000
and now describe the high level uh

00:03:48,959 --> 00:03:53,920
atc currentization that we've

00:03:52,000 --> 00:03:56,319
added and then we'll go and look under

00:03:53,920 --> 00:03:56,319
the hood

00:03:57,360 --> 00:04:00,640
for the discussion we will need to dive

00:03:59,200 --> 00:04:02,400
deeper into recipe

00:04:00,640 --> 00:04:04,799
and we will use this graph to show the

00:04:02,400 --> 00:04:07,519
flow of time and packets

00:04:04,799 --> 00:04:09,040
the time flowing from top to bottom and

00:04:07,519 --> 00:04:12,239
packets going from left

00:04:09,040 --> 00:04:14,159
to right and back

00:04:12,239 --> 00:04:15,760
so in order to establish a tcp

00:04:14,159 --> 00:04:18,479
connection two packets

00:04:15,760 --> 00:04:20,000
uh must be exchanged namely same packet

00:04:18,479 --> 00:04:22,880
from the client

00:04:20,000 --> 00:04:23,919
a synagogue from the server and finally

00:04:22,880 --> 00:04:27,120
a client arc

00:04:23,919 --> 00:04:29,600
in any possible payload

00:04:27,120 --> 00:04:32,160
going back to the server this is known

00:04:29,600 --> 00:04:33,919
as the tcp freeware engine

00:04:32,160 --> 00:04:35,360
and for example establishing tcp

00:04:33,919 --> 00:04:36,639
connection between san francisco and

00:04:35,360 --> 00:04:42,000
bungalow

00:04:36,639 --> 00:04:45,040
takes about 411 milliseconds

00:04:42,000 --> 00:04:47,680
and it takes more than 500 milliseconds

00:04:45,040 --> 00:04:49,280
uh for the first bytes of the response

00:04:47,680 --> 00:04:50,880
to actually arrive

00:04:49,280 --> 00:04:53,440
this time between when the connection

00:04:50,880 --> 00:04:54,240
was initiated and until the first bytes

00:04:53,440 --> 00:04:57,360
of the reply

00:04:54,240 --> 00:05:00,320
arrived is called time to first bite and

00:04:57,360 --> 00:05:00,560
actually corresponds to the response of

00:05:00,320 --> 00:05:02,880
the

00:05:00,560 --> 00:05:04,000
of the site that you're visiting half a

00:05:02,880 --> 00:05:06,320
second is

00:05:04,000 --> 00:05:06,320
the law

00:05:07,120 --> 00:05:14,000
now tcp is by design

00:05:10,720 --> 00:05:16,800
uh trying to be nice to other flows

00:05:14,000 --> 00:05:18,160
and builds up its speed slowly and

00:05:16,800 --> 00:05:21,360
that's called tcp slow

00:05:18,160 --> 00:05:22,560
cluster this stimulant behavior really

00:05:21,360 --> 00:05:25,280
impacts download time

00:05:22,560 --> 00:05:27,120
and breaking the distances it effect

00:05:25,280 --> 00:05:29,120
compounds

00:05:27,120 --> 00:05:30,880
in this example the payload is three

00:05:29,120 --> 00:05:31,759
times the size of the initial sand

00:05:30,880 --> 00:05:33,520
window

00:05:31,759 --> 00:05:35,759
and so the arc from the client must

00:05:33,520 --> 00:05:38,639
travel all the way back to bangalore

00:05:35,759 --> 00:05:40,639
before the rest of the payload is sent

00:05:38,639 --> 00:05:41,680
in our example it will take almost a

00:05:40,639 --> 00:05:43,680
second

00:05:41,680 --> 00:05:45,440
to get all three packets of the response

00:05:43,680 --> 00:05:47,120
to the right

00:05:45,440 --> 00:05:49,600
so you see what kind of issues we're

00:05:47,120 --> 00:05:49,600
trying to do

00:05:51,120 --> 00:05:54,880
and when trying to resolve uh

00:05:52,880 --> 00:05:55,520
bottlenecks and introduce improvements

00:05:54,880 --> 00:05:58,960
it would

00:05:55,520 --> 00:06:00,000
it's it's good just to understand the

00:05:58,960 --> 00:06:03,120
theoretical limit

00:06:00,000 --> 00:06:05,120
of what you can achieve

00:06:03,120 --> 00:06:07,039
suppose we're limited only by the speed

00:06:05,120 --> 00:06:09,919
of light in copper

00:06:07,039 --> 00:06:10,560
in this case ideally we'd like to see

00:06:09,919 --> 00:06:14,240
the request

00:06:10,560 --> 00:06:16,639
going out and the packets of this phone

00:06:14,240 --> 00:06:18,840
arriving all together all just under 300

00:06:16,639 --> 00:06:22,080
milliseconds

00:06:18,840 --> 00:06:23,919
274 and this would correspond to a 2x

00:06:22,080 --> 00:06:25,759
improvement in the time to first byte

00:06:23,919 --> 00:06:27,280
and frequency improvement in download

00:06:25,759 --> 00:06:30,080
time

00:06:27,280 --> 00:06:32,240
next i will show how we attempt to reach

00:06:30,080 --> 00:06:32,240
that

00:06:32,639 --> 00:06:39,039
so in this experiment we added

00:06:35,919 --> 00:06:40,160
a two cloud gateways one in oregon close

00:06:39,039 --> 00:06:43,199
to the client

00:06:40,160 --> 00:06:45,680
in our mumbai close to the server

00:06:43,199 --> 00:06:47,120
we determine the cp connection a on each

00:06:45,680 --> 00:06:51,039
hope

00:06:47,120 --> 00:06:54,000
so now we have a free tcp handshakes

00:06:51,039 --> 00:06:57,360
each with smaller rtt but the sum of the

00:06:54,000 --> 00:07:02,240
rtt is the same as before so the

00:06:57,360 --> 00:07:03,919
time to establish them is unchanged uh

00:07:02,240 --> 00:07:05,120
but now when the server sends its

00:07:03,919 --> 00:07:08,479
payload

00:07:05,120 --> 00:07:11,840
the act from mumbai is much faster to

00:07:08,479 --> 00:07:13,520
to go back to bangalore and its

00:07:11,840 --> 00:07:16,160
server is able to ramp up with speeds

00:07:13,520 --> 00:07:16,960
faster and already send the packets it

00:07:16,160 --> 00:07:20,000
needs to send

00:07:16,960 --> 00:07:23,039
even before the initial bytes arrived

00:07:20,000 --> 00:07:23,039
back at san francisco

00:07:23,599 --> 00:07:27,280
the symbol tcp split improves

00:07:27,840 --> 00:07:33,520
the download time by 60 milliseconds

00:07:31,440 --> 00:07:34,560
and that's even before we do anything

00:07:33,520 --> 00:07:40,160
fancy with our

00:07:34,560 --> 00:07:43,680
god just by tcp splitting on two hops

00:07:40,160 --> 00:07:47,039
what we can do

00:07:43,680 --> 00:07:48,960
to improve the tcp connection is

00:07:47,039 --> 00:07:50,400
start with the tsp through the

00:07:48,960 --> 00:07:52,160
handshakes

00:07:50,400 --> 00:07:53,840
so there is no reason to create the

00:07:52,160 --> 00:07:56,879
handshake sequentially

00:07:53,840 --> 00:07:58,960
the limitations only in the socket api

00:07:56,879 --> 00:08:00,479
that the process that invoked except

00:07:58,960 --> 00:08:01,599
we're done only when the handshake is

00:08:00,479 --> 00:08:04,000
complete

00:08:01,599 --> 00:08:05,120
there's no reason for for our code to

00:08:04,000 --> 00:08:08,560
you know suffer from

00:08:05,120 --> 00:08:10,720
from the consequences so we've added

00:08:08,560 --> 00:08:12,639
netflix filter hooks to capturing common

00:08:10,720 --> 00:08:16,319
syn packets

00:08:12,639 --> 00:08:18,160
and now when a new scene packet

00:08:16,319 --> 00:08:19,599
arrives we immediately begin the

00:08:18,160 --> 00:08:22,960
connection with the

00:08:19,599 --> 00:08:24,240
next cavity and the handshakes happen

00:08:22,960 --> 00:08:27,440
concurrently this happens

00:08:24,240 --> 00:08:31,680
on both gateways

00:08:27,440 --> 00:08:34,880
in this case time to first byte uh

00:08:31,680 --> 00:08:36,320
will be shorter and

00:08:34,880 --> 00:08:37,919
by doing current current handshakes we

00:08:36,320 --> 00:08:40,159
were able to save about six

00:08:37,919 --> 00:08:40,959
another 60 milliseconds from download

00:08:40,159 --> 00:08:44,719
diode

00:08:40,959 --> 00:08:44,719
and the time to first byte

00:08:45,839 --> 00:08:51,920
now the important thing to notice

00:08:48,880 --> 00:08:54,240
is that while the number of a

00:08:51,920 --> 00:08:55,200
possible endpoints like san francisco

00:08:54,240 --> 00:08:58,320
and bangor

00:08:55,200 --> 00:08:59,519
is quite large the number of locations

00:08:58,320 --> 00:09:02,480
where we can

00:08:59,519 --> 00:09:04,480
bring up a gateway is is fairly limited

00:09:02,480 --> 00:09:06,640
73 in our case

00:09:04,480 --> 00:09:07,760
what that means is that between each

00:09:06,640 --> 00:09:09,760
gateway

00:09:07,760 --> 00:09:12,720
that we control we can establish

00:09:09,760 --> 00:09:12,720
connection preemptively

00:09:13,839 --> 00:09:17,440
those this connection is in independent

00:09:16,800 --> 00:09:19,040
of the

00:09:17,440 --> 00:09:20,720
other two sides that connect to the

00:09:19,040 --> 00:09:23,920
endpoints so

00:09:20,720 --> 00:09:26,720
they can be between

00:09:23,920 --> 00:09:27,360
san jose in oregon by bangor or any

00:09:26,720 --> 00:09:30,399
other

00:09:27,360 --> 00:09:31,839
configuration so in this scenario

00:09:30,399 --> 00:09:34,320
there's all the connection

00:09:31,839 --> 00:09:36,240
between organ and pi it's actually pool

00:09:34,320 --> 00:09:39,920
of connections

00:09:36,240 --> 00:09:39,920
but for this example we need a new one

00:09:40,000 --> 00:09:46,160
so when the syn packets arrives

00:09:43,120 --> 00:09:48,399
in this case

00:09:46,160 --> 00:09:50,399
the existing connection is allocated

00:09:48,399 --> 00:09:54,160
from both connections

00:09:50,399 --> 00:09:55,120
and the scene itself is forwarded to

00:09:54,160 --> 00:09:58,560
mumbai

00:09:55,120 --> 00:10:00,880
as payload and initiates the

00:09:58,560 --> 00:10:03,200
next the freeway handshake from mumbai

00:10:00,880 --> 00:10:06,399
to bangalore

00:10:03,200 --> 00:10:08,240
now when the http request uh arrives

00:10:06,399 --> 00:10:09,600
at the organ guard it is immediately

00:10:08,240 --> 00:10:13,279
sent to mumbai

00:10:09,600 --> 00:10:16,560
as the connection is already established

00:10:13,279 --> 00:10:20,079
and the full payload still needs to

00:10:16,560 --> 00:10:23,519
wait for the arc between oregon and

00:10:20,079 --> 00:10:26,959
by to arrive but the time the first by

00:10:23,519 --> 00:10:27,920
now is 302 milliseconds and download

00:10:26,959 --> 00:10:31,040
time is at

00:10:27,920 --> 00:10:32,240
500 milliseconds so by avoiding the long

00:10:31,040 --> 00:10:34,560
handshake

00:10:32,240 --> 00:10:36,000
we were able to shave off about 200

00:10:34,560 --> 00:10:39,040
additional milliseconds

00:10:36,000 --> 00:10:41,680
from both the time to first byte and

00:10:39,040 --> 00:10:41,680
download time

00:10:43,200 --> 00:10:49,360
now let us consider the fact that the

00:10:46,399 --> 00:10:50,880
pre-connection that we've established is

00:10:49,360 --> 00:10:54,399
over the cloud infrastructure

00:10:50,880 --> 00:10:54,800
and not the public internet we know that

00:10:54,399 --> 00:10:58,240
the

00:10:54,800 --> 00:10:59,360
channel is wide the the cloud providers

00:10:58,240 --> 00:11:01,360
invest

00:10:59,360 --> 00:11:05,120
millions of billions of dollars each

00:11:01,360 --> 00:11:05,120
year to improve this infrastructure

00:11:06,399 --> 00:11:09,920
and in addition this cloud provider

00:11:08,800 --> 00:11:12,240
needs to provide

00:11:09,920 --> 00:11:14,000
some benefit guarantees to its clients

00:11:12,240 --> 00:11:17,519
and all that has a mechanism

00:11:14,000 --> 00:11:19,440
to make sure that the resources are

00:11:17,519 --> 00:11:22,079
shared fairly

00:11:19,440 --> 00:11:25,279
that means that we can set the initial

00:11:22,079 --> 00:11:27,680
the cp send window to a much higher part

00:11:25,279 --> 00:11:28,320
because we are not afraid to hinder

00:11:27,680 --> 00:11:31,360
other

00:11:28,320 --> 00:11:33,279
tcp connections so by effectively

00:11:31,360 --> 00:11:35,200
eliminating slow start over the long

00:11:33,279 --> 00:11:39,200
middle connection

00:11:35,200 --> 00:11:42,560
and more payload can go out immediately

00:11:39,200 --> 00:11:43,600
so in this case it happens the same as

00:11:42,560 --> 00:11:46,880
before

00:11:43,600 --> 00:11:49,839
a pre-connection is allocated the

00:11:46,880 --> 00:11:51,440
arc from oregon goes back to mumbai but

00:11:49,839 --> 00:11:54,000
the thing is that

00:11:51,440 --> 00:11:54,560
the mumbai server doesn't actually need

00:11:54,000 --> 00:11:57,200
to wait

00:11:54,560 --> 00:11:58,399
to wait for the app to arrive because

00:11:57,200 --> 00:12:01,440
the

00:11:58,399 --> 00:12:02,079
send window is large enough now we shave

00:12:01,440 --> 00:12:06,320
another

00:12:02,079 --> 00:12:09,120
150 milliseconds from the download time

00:12:06,320 --> 00:12:10,560
and this is probably as close as we can

00:12:09,120 --> 00:12:14,720
get

00:12:10,560 --> 00:12:18,160
to the limit set by the speed of light

00:12:14,720 --> 00:12:20,639
uh of course by moving the

00:12:18,160 --> 00:12:21,839
gateways orange and by even closer to

00:12:20,639 --> 00:12:25,680
the endpoints we can

00:12:21,839 --> 00:12:29,040
improve better prove even more

00:12:25,680 --> 00:12:30,000
and and gain as much performance as we

00:12:29,040 --> 00:12:32,880
need so

00:12:30,000 --> 00:12:33,519
we got the 2x performance the time to

00:12:32,880 --> 00:12:36,959
first byte

00:12:33,519 --> 00:12:40,000
and almost the x3 performance gain in

00:12:36,959 --> 00:12:40,000
the download time

00:12:40,480 --> 00:12:43,600
and this is the results of an experiment

00:12:43,200 --> 00:12:46,959
of

00:12:43,600 --> 00:12:50,720
a 50 megabyte download

00:12:46,959 --> 00:12:54,160
uh here end to end means the

00:12:50,720 --> 00:12:56,800
download was done over the internet

00:12:54,160 --> 00:12:57,360
baseline means that the download was

00:12:56,800 --> 00:13:00,880
done with

00:12:57,360 --> 00:13:02,480
just standard tcp splitting and ktcp is

00:13:00,880 --> 00:13:05,600
just the whole

00:13:02,480 --> 00:13:08,959
package at once and what we see it

00:13:05,600 --> 00:13:10,959
on the left is that the tantros bite was

00:13:08,959 --> 00:13:14,320
the best and it's actually

00:13:10,959 --> 00:13:19,040
a bit larger because uh the path

00:13:14,320 --> 00:13:22,240
uh the detours to organ and by actually

00:13:19,040 --> 00:13:23,839
the initial is several milliseconds but

00:13:22,240 --> 00:13:26,399
overall the download time is

00:13:23,839 --> 00:13:29,200
much faster both in in the baseline in

00:13:26,399 --> 00:13:29,200
mktcp

00:13:30,320 --> 00:13:34,480
so now that i've showed you why we do

00:13:33,040 --> 00:13:37,760
ktcp and what kind of

00:13:34,480 --> 00:13:40,480
performance you can get over the uh

00:13:37,760 --> 00:13:40,800
the long connections let's talk about

00:13:40,480 --> 00:13:44,639
the

00:13:40,800 --> 00:13:48,560
ktcp implementation let's drill down

00:13:44,639 --> 00:13:51,519
so why the kernel we built tcp split as

00:13:48,560 --> 00:13:53,120
a linux camera model and we do and we

00:13:51,519 --> 00:13:55,040
chose to do it this way because the

00:13:53,120 --> 00:13:58,399
interaction with the network happens

00:13:55,040 --> 00:14:01,199
in the kernel and all the

00:13:58,399 --> 00:14:03,120
tools that we need to build a proxy or

00:14:01,199 --> 00:14:06,560
in the product in the kernel

00:14:03,120 --> 00:14:07,839
as well i've already mentioned that we

00:14:06,560 --> 00:14:10,639
use net filter

00:14:07,839 --> 00:14:11,199
to capture syn packets and the user

00:14:10,639 --> 00:14:14,839
space

00:14:11,199 --> 00:14:17,600
has uh expensive code switches

00:14:14,839 --> 00:14:20,959
expensive system calls and most of all

00:14:17,600 --> 00:14:24,160
expensive copy so for our use case

00:14:20,959 --> 00:14:26,720
it's slow and limited

00:14:24,160 --> 00:14:27,279
other user space solutions like ldk and

00:14:26,720 --> 00:14:29,839
map

00:14:27,279 --> 00:14:30,320
or netmap are no better as we would have

00:14:29,839 --> 00:14:34,240
to

00:14:30,320 --> 00:14:37,120
build our own network infrastructure

00:14:34,240 --> 00:14:38,560
all on our own i will address

00:14:37,120 --> 00:14:44,720
alternative solutions

00:14:38,560 --> 00:14:46,480
uh at the end of the of the talk

00:14:44,720 --> 00:14:49,519
the basic data structure that is used to

00:14:46,480 --> 00:14:51,839
manage a split connection

00:14:49,519 --> 00:14:54,399
is called a connection qp it's a

00:14:51,839 --> 00:14:57,680
misleading misnomer i admit

00:14:54,399 --> 00:15:00,800
uh but each qp is identified by its

00:14:57,680 --> 00:15:03,279
a tcp for tuple so we have

00:15:00,800 --> 00:15:04,639
five tuple tcp is one of them although

00:15:03,279 --> 00:15:06,160
it's a protocol so we have the fourth

00:15:04,639 --> 00:15:09,199
level namely the

00:15:06,160 --> 00:15:13,839
source destination prototyping

00:15:09,199 --> 00:15:13,839
which provides us with a 12 byte key

00:15:14,000 --> 00:15:19,920
we have two tcp sockets one rx and tx

00:15:18,000 --> 00:15:22,320
where both obviously can send and

00:15:19,920 --> 00:15:23,600
receive but the idea is that the client

00:15:22,320 --> 00:15:26,560
facing socket

00:15:23,600 --> 00:15:29,440
is called the rx and the server facing

00:15:26,560 --> 00:15:29,440
is called the dx

00:15:31,279 --> 00:15:35,040
additionally we have a ref count to

00:15:34,160 --> 00:15:37,920
facilitate

00:15:35,040 --> 00:15:40,160
an early teardown of the qp an rb node

00:15:37,920 --> 00:15:43,040
and a weight key

00:15:40,160 --> 00:15:44,639
they're used for initialization when the

00:15:43,040 --> 00:15:46,880
connection is established

00:15:44,639 --> 00:15:48,000
two kernel threads are forwarding the

00:15:46,880 --> 00:15:50,880
information between

00:15:48,000 --> 00:15:51,440
the actual bytes within the rx and dx

00:15:50,880 --> 00:15:54,399
each

00:15:51,440 --> 00:15:57,279
thread is receiving on one socket and

00:15:54,399 --> 00:15:57,279
sending to the arrow

00:15:58,399 --> 00:16:01,519
both send and receive in a blocking

00:16:00,959 --> 00:16:04,399
fashion

00:16:01,519 --> 00:16:05,759
which implicitly creates a back pressure

00:16:04,399 --> 00:16:08,639
when needed

00:16:05,759 --> 00:16:10,160
and when you have one side a cloud

00:16:08,639 --> 00:16:11,040
gateway and the other is the public

00:16:10,160 --> 00:16:14,320
internet

00:16:11,040 --> 00:16:14,320
you often need it

00:16:15,600 --> 00:16:19,680
now i'll the keeper creation has a four

00:16:18,800 --> 00:16:22,240
step

00:16:19,680 --> 00:16:23,759
on scene afraid is scheduled to run and

00:16:22,240 --> 00:16:27,759
its job is to set up

00:16:23,759 --> 00:16:31,040
the server side tcp connection

00:16:27,759 --> 00:16:32,399
pre-existing or new so either way

00:16:31,040 --> 00:16:34,800
when the client-side connection is

00:16:32,399 --> 00:16:37,120
complete a second thread is scheduled

00:16:34,800 --> 00:16:38,399
and a job to manage the client-side tsp

00:16:37,120 --> 00:16:41,279
connections

00:16:38,399 --> 00:16:42,160
and both have a cpu mask with one bit

00:16:41,279 --> 00:16:45,440
set

00:16:42,160 --> 00:16:50,560
to a core that is set by simple hash

00:16:45,440 --> 00:16:51,279
of the fourth table both try to modify

00:16:50,560 --> 00:16:54,320
and create

00:16:51,279 --> 00:16:56,639
a the qp into a qp

00:16:54,320 --> 00:16:58,880
in a local cupid tree on that specific

00:16:56,639 --> 00:16:58,880
car

00:16:58,959 --> 00:17:04,559
and when the complete qp exists both red

00:17:02,560 --> 00:17:06,000
get a full cpu mask and start shuffling

00:17:04,559 --> 00:17:09,520
bytes

00:17:06,000 --> 00:17:09,520
as the scheduler deems

00:17:09,839 --> 00:17:13,199
now let's show a demonstration how to

00:17:12,160 --> 00:17:16,240
work

00:17:13,199 --> 00:17:19,760
a natural callback is invoked on

00:17:16,240 --> 00:17:20,160
core zero it's a relevant scene packets

00:17:19,760 --> 00:17:23,600
so

00:17:20,160 --> 00:17:26,319
an a thread is allocated to handle

00:17:23,600 --> 00:17:27,360
this in packet what it does it creates a

00:17:26,319 --> 00:17:30,960
connection with the

00:17:27,360 --> 00:17:34,320
server and it's scheduled to run

00:17:30,960 --> 00:17:38,320
on cpu 2 on core 2

00:17:34,320 --> 00:17:38,320
due to the 5 double hash

00:17:38,559 --> 00:17:45,360
now the scene handler adds its qp to the

00:17:41,760 --> 00:17:49,120
local arbitrary and waits for the

00:17:45,360 --> 00:17:52,799
receive socket namely the client-side

00:17:49,120 --> 00:17:55,280
circuit to edit this information

00:17:52,799 --> 00:17:56,720
at the same time while the way the proxy

00:17:55,280 --> 00:17:58,080
server

00:17:56,720 --> 00:17:59,440
accepts the connection from the the

00:17:58,080 --> 00:18:00,720
source now the freeware handshake is

00:17:59,440 --> 00:18:03,760
complete

00:18:00,720 --> 00:18:05,280
a new copy is allocated a new thread is

00:18:03,760 --> 00:18:10,000
allocated

00:18:05,280 --> 00:18:13,760
and scheduled to run on core 2.

00:18:10,000 --> 00:18:16,080
so the connection starts on core 2

00:18:13,760 --> 00:18:16,799
finds the qp in the cupid tree sees that

00:18:16,080 --> 00:18:20,080
the

00:18:16,799 --> 00:18:20,880
there is a qp with the four type of 12

00:18:20,080 --> 00:18:23,039
byte

00:18:20,880 --> 00:18:24,080
key all the end there it removes this

00:18:23,039 --> 00:18:27,760
from the key from the

00:18:24,080 --> 00:18:31,200
tree wakes up the waiting pier

00:18:27,760 --> 00:18:34,400
well obviously after updating the

00:18:31,200 --> 00:18:37,760
second socket and starts forwarding a

00:18:34,400 --> 00:18:40,240
bytes from rxtx while the other does the

00:18:37,760 --> 00:18:40,240
other way

00:18:40,559 --> 00:18:46,080
and after the

00:18:43,679 --> 00:18:46,720
this handshake between the two threads

00:18:46,080 --> 00:18:49,120
they are

00:18:46,720 --> 00:18:51,440
free to move around as they reach on the

00:18:49,120 --> 00:18:51,440
machine

00:18:52,000 --> 00:18:55,440
kernel threads are great they provide a

00:18:54,400 --> 00:18:58,480
process context

00:18:55,440 --> 00:18:58,960
that allows you to sleep and allows you

00:18:58,480 --> 00:19:01,679
to get

00:18:58,960 --> 00:19:02,240
a fair time share as it's something that

00:19:01,679 --> 00:19:05,440
the

00:19:02,240 --> 00:19:08,320
scheduler does the caller

00:19:05,440 --> 00:19:11,200
provides a function context a function

00:19:08,320 --> 00:19:14,320
and a context that gets

00:19:11,200 --> 00:19:16,400
used by the function and the caller gets

00:19:14,320 --> 00:19:19,200
the

00:19:16,400 --> 00:19:20,799
thread contacts in struct form the

00:19:19,200 --> 00:19:23,919
problem of that api

00:19:20,799 --> 00:19:25,679
is that the calls are slow it made the

00:19:23,919 --> 00:19:29,840
the outliers may take several

00:19:25,679 --> 00:19:30,320
milliseconds it was really a tangible in

00:19:29,840 --> 00:19:34,000
our

00:19:30,320 --> 00:19:37,280
experience where the variance in the the

00:19:34,000 --> 00:19:39,760
results were quite large

00:19:37,280 --> 00:19:40,400
and the greatest issue is that the fact

00:19:39,760 --> 00:19:43,120
that we

00:19:40,400 --> 00:19:44,400
can't really call this api in the nappy

00:19:43,120 --> 00:19:47,280
context

00:19:44,400 --> 00:19:50,720
where we need it as i've just

00:19:47,280 --> 00:19:53,280
demonstrated in the previous slides

00:19:50,720 --> 00:19:55,520
so in order to solve this problem we

00:19:53,280 --> 00:19:58,320
created a pool of threads

00:19:55,520 --> 00:19:59,320
let's talk about that so we create a

00:19:58,320 --> 00:20:01,600
pool of

00:19:59,320 --> 00:20:02,880
pre-pre-allocated and reusable kernel

00:20:01,600 --> 00:20:05,360
threads that can

00:20:02,880 --> 00:20:06,159
execute arbitrary functions set it

00:20:05,360 --> 00:20:09,600
runtime

00:20:06,159 --> 00:20:13,840
rather than on creation time

00:20:09,600 --> 00:20:17,200
each pull element has a task struct

00:20:13,840 --> 00:20:20,320
a callback pointer

00:20:17,200 --> 00:20:23,679
and the data now the third function

00:20:20,320 --> 00:20:26,799
itself that we called initially

00:20:23,679 --> 00:20:27,679
had to create the thread the original

00:20:26,799 --> 00:20:30,480
kelvin

00:20:27,679 --> 00:20:33,360
is is quite simple what it does it runs

00:20:30,480 --> 00:20:33,360
in an endless loop

00:20:33,520 --> 00:20:38,640
a running this callback

00:20:36,559 --> 00:20:39,919
and when it's done it just goes back to

00:20:38,640 --> 00:20:41,840
sleep

00:20:39,919 --> 00:20:45,440
it's obviously use its own data that can

00:20:41,840 --> 00:20:47,919
be modified in random as well

00:20:45,440 --> 00:20:49,600
now this system is fast you just

00:20:47,919 --> 00:20:50,960
schedule you don't need to allocate

00:20:49,600 --> 00:20:54,000
anything

00:20:50,960 --> 00:20:56,880
and you can run it from anywhere so both

00:20:54,000 --> 00:20:56,880
problems are solved

00:20:57,440 --> 00:21:00,559
one last thing i wanted to mention is

00:21:00,000 --> 00:21:02,480
the

00:21:00,559 --> 00:21:04,559
we have two poles here from connection

00:21:02,480 --> 00:21:08,640
pools and threat pools

00:21:04,559 --> 00:21:12,400
and the thing is that parkour

00:21:08,640 --> 00:21:14,799
caches are well stuck

00:21:12,400 --> 00:21:15,679
and there was a solution proposed 20

00:21:14,799 --> 00:21:19,600
years ago

00:21:15,679 --> 00:21:21,280
in an atc paper 2001 the basic idea is

00:21:19,600 --> 00:21:24,640
called magazines

00:21:21,280 --> 00:21:27,200
which provide actually two per core

00:21:24,640 --> 00:21:29,679
caches each of limited size

00:21:27,200 --> 00:21:30,559
and you have two to avoid thrashing so

00:21:29,679 --> 00:21:33,760
so let's

00:21:30,559 --> 00:21:35,520
see how it works we we look at see it at

00:21:33,760 --> 00:21:37,520
core 2 for this example and

00:21:35,520 --> 00:21:39,039
we'll try to remove two elements from

00:21:37,520 --> 00:21:41,440
the from

00:21:39,039 --> 00:21:42,400
from the cache so we element one we get

00:21:41,440 --> 00:21:46,480
just one element

00:21:42,400 --> 00:21:49,760
and then it's empty now when the

00:21:46,480 --> 00:21:53,280
cord has two empty magazines

00:21:49,760 --> 00:21:56,960
what it does it now accesses the global

00:21:53,280 --> 00:22:00,000
pool and replenishes by receiving

00:21:56,960 --> 00:22:02,559
a full magazine instead of an empty one

00:22:00,000 --> 00:22:03,360
so in this animation the full the mtr

00:22:02,559 --> 00:22:07,039
magazine

00:22:03,360 --> 00:22:11,200
also joined the empty magazine list

00:22:07,039 --> 00:22:14,080
this provides with very good performance

00:22:11,200 --> 00:22:15,120
with high rates so it's an unfortunate

00:22:14,080 --> 00:22:18,000
it's not part of the

00:22:15,120 --> 00:22:18,000
linux kernel today

00:22:18,559 --> 00:22:25,120
and now let's go talk about zero copy

00:22:22,400 --> 00:22:26,880
my main issue with user space was that

00:22:25,120 --> 00:22:28,960
the copy costs

00:22:26,880 --> 00:22:30,559
but the thing that i did not mention is

00:22:28,960 --> 00:22:33,919
that the kernel socket api

00:22:30,559 --> 00:22:37,280
also performs a copy

00:22:33,919 --> 00:22:39,200
some modifications were needed to

00:22:37,280 --> 00:22:41,760
well allow zero copy as it's much more

00:22:39,200 --> 00:22:44,640
simple to do in in kernel space

00:22:41,760 --> 00:22:45,360
so there is also already infrastructure

00:22:44,640 --> 00:22:48,880
in place

00:22:45,360 --> 00:22:51,280
for a for reading

00:22:48,880 --> 00:22:51,919
with zero copy that's called the spirit

00:22:51,280 --> 00:22:55,679
sock

00:22:51,919 --> 00:22:58,640
and it's used by splice and the 4k user

00:22:55,679 --> 00:23:02,080
space geocopy efforts

00:22:58,640 --> 00:23:04,400
what it needs is needs a

00:23:02,080 --> 00:23:07,039
your own uh well it's called an actor on

00:23:04,400 --> 00:23:10,159
this api your own callback that just

00:23:07,039 --> 00:23:14,240
collates the packets from the from the

00:23:10,159 --> 00:23:17,440
fragments from the skbs

00:23:14,240 --> 00:23:20,000
ontarix is actually simpler

00:23:17,440 --> 00:23:21,280
or looks like it's simpler as there are

00:23:20,000 --> 00:23:24,080
two options

00:23:21,280 --> 00:23:26,159
one and just do tcp send pages and which

00:23:24,080 --> 00:23:28,559
is used also in splice

00:23:26,159 --> 00:23:29,360
and it's also a misnomer because

00:23:28,559 --> 00:23:32,640
ironically

00:23:29,360 --> 00:23:36,480
tcp send pages only accepts one page

00:23:32,640 --> 00:23:39,360
to send unlike a the

00:23:36,480 --> 00:23:40,159
second option tcp send message is locked

00:23:39,360 --> 00:23:43,360
where

00:23:40,159 --> 00:23:44,960
there is already code for the message

00:23:43,360 --> 00:23:48,159
zero copy

00:23:44,960 --> 00:23:51,679
efforts uh that allow zero copying for

00:23:48,159 --> 00:23:55,120
user space so by well breaking

00:23:51,679 --> 00:23:59,120
the changes that were made in

00:23:55,120 --> 00:24:01,360
removing the user space support

00:23:59,120 --> 00:24:03,440
we were able to get this percent message

00:24:01,360 --> 00:24:04,240
to work with kernel buffers as well with

00:24:03,440 --> 00:24:07,440
zero copy

00:24:04,240 --> 00:24:09,039
uh settings let's see the evaluation

00:24:07,440 --> 00:24:12,559
hours

00:24:09,039 --> 00:24:14,159
so we run our regulation of

00:24:12,559 --> 00:24:16,640
a virtual environment on the google

00:24:14,159 --> 00:24:18,880
cloud for our testing we create

00:24:16,640 --> 00:24:20,559
16 core intel cascade like high

00:24:18,880 --> 00:24:23,600
frequency

00:24:20,559 --> 00:24:26,080
each cable of 3 and 32 gigabits of

00:24:23,600 --> 00:24:30,000
igor's bandwidth and each with 64

00:24:26,080 --> 00:24:32,400
gigabytes of ram in the first

00:24:30,000 --> 00:24:34,960
experiment we wanted to evaluate the

00:24:32,400 --> 00:24:36,559
cost of data sent

00:24:34,960 --> 00:24:38,880
we created three processes that were

00:24:36,559 --> 00:24:40,559
attempting to put as much bytes as

00:24:38,880 --> 00:24:43,440
possible on a single core

00:24:40,559 --> 00:24:45,360
64 kilobytes at a time just to minimize

00:24:43,440 --> 00:24:46,960
the impact of the system calls on the

00:24:45,360 --> 00:24:49,360
results

00:24:46,960 --> 00:24:51,840
and we see that the number of cycles

00:24:49,360 --> 00:24:54,320
spent per byte on average

00:24:51,840 --> 00:24:55,600
that's the you that's the metrics i i

00:24:54,320 --> 00:24:59,520
like best to

00:24:55,600 --> 00:25:03,039
show the uh efficiency of

00:24:59,520 --> 00:25:05,520
each solution now

00:25:03,039 --> 00:25:07,039
what we see here is message zero copy

00:25:05,520 --> 00:25:10,640
compared to

00:25:07,039 --> 00:25:13,760
ktcp with the zero copy

00:25:10,640 --> 00:25:16,640
fixes ktcp without the zero copy fixes

00:25:13,760 --> 00:25:16,640
and send page

00:25:17,279 --> 00:25:22,799
and all the flows are

00:25:20,400 --> 00:25:23,440
reach about 27 gigabit gigabytes per

00:25:22,799 --> 00:25:25,679
second

00:25:23,440 --> 00:25:28,159
but the ktsp solution that is stuck at

00:25:25,679 --> 00:25:29,919
about 19 gigabits per second

00:25:28,159 --> 00:25:32,799
and spending about 50 percent of its

00:25:29,919 --> 00:25:35,039
cycle on copying

00:25:32,799 --> 00:25:36,480
message zero copy or spending about a

00:25:35,039 --> 00:25:39,440
third of its cycles

00:25:36,480 --> 00:25:42,000
on get users pages fast which doesn't

00:25:39,440 --> 00:25:44,960
seem that fast in this context

00:25:42,000 --> 00:25:46,480
and it confirms the original uh numbers

00:25:44,960 --> 00:25:49,760
reported in the in the

00:25:46,480 --> 00:25:52,159
original native conference someone

00:25:49,760 --> 00:25:53,679
surprisingly sandpage also had lots of

00:25:52,159 --> 00:25:56,960
redundant overheads about

00:25:53,679 --> 00:26:00,720
90 percent of its cycle were

00:25:56,960 --> 00:26:04,080
spent on generic file splice rate

00:26:00,720 --> 00:26:04,720
so just doing zero copy sends in the

00:26:04,080 --> 00:26:08,000
kernel

00:26:04,720 --> 00:26:08,000
is is much faster

00:26:08,400 --> 00:26:12,559
we also run a similar experiment for

00:26:10,880 --> 00:26:17,039
splicing performance

00:26:12,559 --> 00:26:18,880
uh with free gcp vms uh the main c

00:26:17,039 --> 00:26:21,200
thing we see here as there are a lot of

00:26:18,880 --> 00:26:24,480
uh evaluation here

00:26:21,200 --> 00:26:26,799
is that all the the copying

00:26:24,480 --> 00:26:29,279
solutions are much more expensive than

00:26:26,799 --> 00:26:32,720
the zero copying solutions

00:26:29,279 --> 00:26:35,840
and we see here that ktcp versus ktp cb0

00:26:32,720 --> 00:26:40,640
is well it's more than two times

00:26:35,840 --> 00:26:40,640
faster when you add zero copy

00:26:42,960 --> 00:26:47,679
the other two that intersect are sock

00:26:46,480 --> 00:26:52,240
mag and splice

00:26:47,679 --> 00:26:52,640
and well sock as i've mentioned doesn't

00:26:52,240 --> 00:26:54,880
have

00:26:52,640 --> 00:26:56,559
a back pressure which is important in

00:26:54,880 --> 00:26:59,520
our case

00:26:56,559 --> 00:27:01,360
and splice is actually user space a

00:26:59,520 --> 00:27:05,679
system call

00:27:01,360 --> 00:27:08,720
with sometimes a lot of overheads and

00:27:05,679 --> 00:27:11,760
would those that we would rather avoid

00:27:08,720 --> 00:27:13,360
so for more details about the various

00:27:11,760 --> 00:27:16,080
splicing techniques i can

00:27:13,360 --> 00:27:16,880
recommend this to blog post by market

00:27:16,080 --> 00:27:21,120
cloud5

00:27:16,880 --> 00:27:22,080
lovely uh if you need to reach out this

00:27:21,120 --> 00:27:25,679
is my email

00:27:22,080 --> 00:27:29,919
and the code is available at my github

00:27:25,679 --> 00:27:29,919
and get this video kit thank you

00:27:32,480 --> 00:27:39,120
okay uh thank you so we do have um

00:27:35,840 --> 00:27:39,919
a number of questions um i'd like to

00:27:39,120 --> 00:27:41,440
kick it off i

00:27:39,919 --> 00:27:43,200
i don't know if this was a question from

00:27:41,440 --> 00:27:46,640
eric

00:27:43,200 --> 00:27:48,799
um but uh he mentioned a couple of

00:27:46,640 --> 00:27:50,240
things which were more points uh

00:27:48,799 --> 00:27:52,159
let's hope middle boxes will be

00:27:50,240 --> 00:27:55,360
maintained up to date

00:27:52,159 --> 00:28:02,720
um is that in reference to

00:27:55,360 --> 00:28:07,200
anything special with regards to ktcp

00:28:02,720 --> 00:28:08,720
can you hear me yes yeah so

00:28:07,200 --> 00:28:10,480
i guess the question of is about

00:28:08,720 --> 00:28:11,360
splitting you know having splitting

00:28:10,480 --> 00:28:14,559
points uh

00:28:11,360 --> 00:28:16,399
along the path you install a software

00:28:14,559 --> 00:28:18,960
one day and then this software can last

00:28:16,399 --> 00:28:21,440
for years and maybe this software

00:28:18,960 --> 00:28:22,240
will not be updated if some bug needs to

00:28:21,440 --> 00:28:25,360
be fixed

00:28:22,240 --> 00:28:26,880
and so things like new tcp things like

00:28:25,360 --> 00:28:30,159
tcp fast open

00:28:26,880 --> 00:28:32,320
had to suffer some kind of deployment

00:28:30,159 --> 00:28:33,760
issue because some middle books were

00:28:32,320 --> 00:28:36,559
like set in stone

00:28:33,760 --> 00:28:37,360
like uh the software was never updated

00:28:36,559 --> 00:28:40,000
on soda

00:28:37,360 --> 00:28:44,559
and so these first open uh requests were

00:28:40,000 --> 00:28:47,279
completely denied

00:28:44,559 --> 00:28:47,919
so i might have missed this but is this

00:28:47,279 --> 00:28:51,600
um

00:28:47,919 --> 00:28:54,720
a form of proxy or are we doing some uh

00:28:51,600 --> 00:28:58,159
some sort of transparent in in network

00:28:54,720 --> 00:29:00,399
uh tcp translation it's

00:28:58,159 --> 00:29:02,240
transparent it transparent in the sense

00:29:00,399 --> 00:29:05,200
that we use

00:29:02,240 --> 00:29:05,520
it's well let me correct myself it's not

00:29:05,200 --> 00:29:07,600
a

00:29:05,520 --> 00:29:09,039
transparency you have tcp sockets and

00:29:07,600 --> 00:29:12,480
you have to speed termination

00:29:09,039 --> 00:29:13,039
on each on each node and the middle

00:29:12,480 --> 00:29:15,039
boxes

00:29:13,039 --> 00:29:16,159
are actually vms that can be replaced

00:29:15,039 --> 00:29:19,600
and

00:29:16,159 --> 00:29:20,720
you can well it's not mentioned in the

00:29:19,600 --> 00:29:23,520
talk because it's not

00:29:20,720 --> 00:29:25,600
part of the talk but you can do load

00:29:23,520 --> 00:29:29,520
balancing and redirect

00:29:25,600 --> 00:29:33,039
new connections to new boxes and update

00:29:29,520 --> 00:29:34,799
the machines with new code incrementally

00:29:33,039 --> 00:29:36,960
so it's not actually a problem i'm

00:29:34,799 --> 00:29:40,159
answering to the whatever scene was the

00:29:36,960 --> 00:29:42,399
question barry yeah

00:29:40,159 --> 00:29:43,600
okay so as far as a network is concerned

00:29:42,399 --> 00:29:47,279
these are just tcp

00:29:43,600 --> 00:29:48,720
packets going to uh a destination

00:29:47,279 --> 00:29:51,840
address that happens to be somewhere in

00:29:48,720 --> 00:29:54,000
the network yeah

00:29:51,840 --> 00:29:55,600
okay so along those lines i think a good

00:29:54,000 --> 00:30:00,159
follow-up is

00:29:55,600 --> 00:30:03,279
uh is this capable of converting tcp to

00:30:00,159 --> 00:30:05,200
multi-path tcp uh

00:30:03,279 --> 00:30:06,559
there is no limitation to what you can

00:30:05,200 --> 00:30:08,880
do so you have

00:30:06,559 --> 00:30:09,760
a on the one hand you terminate the

00:30:08,880 --> 00:30:13,440
connection from

00:30:09,760 --> 00:30:15,600
from the client and what you do to

00:30:13,440 --> 00:30:17,120
to to make the data available on the

00:30:15,600 --> 00:30:19,039
outside is is

00:30:17,120 --> 00:30:20,799
fully up to you you want to use udp

00:30:19,039 --> 00:30:23,440
transport go ahead

00:30:20,799 --> 00:30:25,279
so what what's important today is that i

00:30:23,440 --> 00:30:28,799
open the second tcp socket

00:30:25,279 --> 00:30:31,600
and send the a the data for it but

00:30:28,799 --> 00:30:31,919
there was talk about implement whatever

00:30:31,600 --> 00:30:34,960
is

00:30:31,919 --> 00:30:35,520
best for that specific destination it

00:30:34,960 --> 00:30:38,000
can be

00:30:35,520 --> 00:30:39,039
i don't know an infiniband

00:30:38,000 --> 00:30:41,520
infrastructure

00:30:39,039 --> 00:30:43,440
on the left hand or so you'd open real

00:30:41,520 --> 00:30:46,480
infiniband cupids and

00:30:43,440 --> 00:30:48,080
so you know whatever you need

00:30:46,480 --> 00:30:49,840
okay and i assume that's going to extend

00:30:48,080 --> 00:30:50,399
to the question about quick so quick

00:30:49,840 --> 00:30:52,000
could be

00:30:50,399 --> 00:30:53,840
on one side and that could be converted

00:30:52,000 --> 00:30:56,960
to tcp on the other

00:30:53,840 --> 00:30:59,519
quick is an issue because qui is

00:30:56,960 --> 00:31:00,080
fully its headers are fully encrypted so

00:30:59,519 --> 00:31:03,120
unlike

00:31:00,080 --> 00:31:05,679
tcp which i can open i can terminate the

00:31:03,120 --> 00:31:07,840
connection in the mailbox

00:31:05,679 --> 00:31:10,720
it's not something i can do for quick

00:31:07,840 --> 00:31:10,720
quick is end to end

00:31:10,880 --> 00:31:14,399
well but that also assumes in the case

00:31:13,039 --> 00:31:18,000
of tcp there's no

00:31:14,399 --> 00:31:21,360
tcp authentication header and tls

00:31:18,000 --> 00:31:24,799
goes above us so well tls is but but

00:31:21,360 --> 00:31:28,080
we could also um encrypt the tcp header

00:31:24,799 --> 00:31:29,600
so yes so i always wondered this

00:31:28,080 --> 00:31:32,240
how do we how do we implement quick

00:31:29,600 --> 00:31:35,279
proxies um because of this

00:31:32,240 --> 00:31:38,559
encrypted quick headers uh well is there

00:31:35,279 --> 00:31:40,559
any ideas on that uh you should ask

00:31:38,559 --> 00:31:42,240
the quick guys but as far as i

00:31:40,559 --> 00:31:45,519
understand

00:31:42,240 --> 00:31:46,799
not being able to split it

00:31:45,519 --> 00:31:49,600
you know do the same thing you did for

00:31:46,799 --> 00:31:51,760
tcp it was part of the design of quick

00:31:49,600 --> 00:31:53,440
as as far as i remember from the

00:31:51,760 --> 00:31:56,159
original quick paper

00:31:53,440 --> 00:31:58,240
several years ago they didn't want

00:31:56,159 --> 00:32:00,320
especially because of what eric asks

00:31:58,240 --> 00:32:01,440
they mentioned those middle boxes that

00:32:00,320 --> 00:32:03,760
you know can

00:32:01,440 --> 00:32:05,519
uh well i don't want to go into the

00:32:03,760 --> 00:32:06,480
quick paper itself but it was part of

00:32:05,519 --> 00:32:08,080
the design

00:32:06,480 --> 00:32:10,559
as much as i understand please correct

00:32:08,080 --> 00:32:12,559
me for more

00:32:10,559 --> 00:32:14,399
well i mean so there's an obvious issue

00:32:12,559 --> 00:32:17,120
and it's not just here but

00:32:14,399 --> 00:32:19,600
quick becomes more and more predominant

00:32:17,120 --> 00:32:21,919
traffic on the internet

00:32:19,600 --> 00:32:23,519
tcp only solutions become less and less

00:32:21,919 --> 00:32:26,720
valuable in some sense

00:32:23,519 --> 00:32:28,559
um next question so is there any

00:32:26,720 --> 00:32:30,080
plan to implement these features in sock

00:32:28,559 --> 00:32:32,640
map

00:32:30,080 --> 00:32:33,360
so stockman i've answered in the chat so

00:32:32,640 --> 00:32:36,320
segment is an

00:32:33,360 --> 00:32:37,440
epf implementation and i think i've

00:32:36,320 --> 00:32:40,080
mentioned in the talk

00:32:37,440 --> 00:32:41,200
uh the problem with softmap that you

00:32:40,080 --> 00:32:44,159
don't have

00:32:41,200 --> 00:32:45,600
explicit or implicit a back pressure

00:32:44,159 --> 00:32:47,919
which is important for

00:32:45,600 --> 00:32:50,320
where the two legs of the connection are

00:32:47,919 --> 00:32:53,600
not equal in in bandwidth

00:32:50,320 --> 00:32:56,080
uh so when the two

00:32:53,600 --> 00:32:56,960
two sides are equal everything is great

00:32:56,080 --> 00:32:59,679
but

00:32:56,960 --> 00:33:00,559
ebpf is a callback that happens in nappy

00:32:59,679 --> 00:33:03,200
so it

00:33:00,559 --> 00:33:04,640
you don't really know who pays the is

00:33:03,200 --> 00:33:06,720
the the price

00:33:04,640 --> 00:33:07,919
for this callback and you don't have

00:33:06,720 --> 00:33:11,679
that pressure namely

00:33:07,919 --> 00:33:13,519
you can't pause the sender on the left

00:33:11,679 --> 00:33:15,600
because the receiver on the right is not

00:33:13,519 --> 00:33:19,840
fast enough so it's just not

00:33:15,600 --> 00:33:22,480
incompatible so ktcp uses kernel threads

00:33:19,840 --> 00:33:24,000
exactly for this so you can a time share

00:33:22,480 --> 00:33:26,960
you can actually allow the kernel to

00:33:24,000 --> 00:33:28,000
to do time sharing you can do you can do

00:33:26,960 --> 00:33:31,120
traffic classes

00:33:28,000 --> 00:33:32,240
this way with a soft map it's just a

00:33:31,120 --> 00:33:33,919
different implementation

00:33:32,240 --> 00:33:36,080
you know different approaches need to be

00:33:33,919 --> 00:33:39,120
you know taken

00:33:36,080 --> 00:33:41,279
short the answer it's not comparable

00:33:39,120 --> 00:33:42,240
so i'm going to ask i think i think this

00:33:41,279 --> 00:33:45,200
is

00:33:42,240 --> 00:33:46,000
a question i'll try to interpret it um

00:33:45,200 --> 00:33:48,880
what is the

00:33:46,000 --> 00:33:50,159
the real use case for tc tcp splitting

00:33:48,880 --> 00:33:51,679
if both nodes

00:33:50,159 --> 00:33:53,360
are close to the server and close to the

00:33:51,679 --> 00:33:57,360
client

00:33:53,360 --> 00:34:00,159
um it makes does it make sense to use uh

00:33:57,360 --> 00:34:01,039
http reverse reverse proxy closer to the

00:34:00,159 --> 00:34:02,880
client

00:34:01,039 --> 00:34:04,840
so uh i guess generally what the

00:34:02,880 --> 00:34:06,320
question is what is the best use case of

00:34:04,840 --> 00:34:08,639
this

00:34:06,320 --> 00:34:11,520
when you have long long connections so

00:34:08,639 --> 00:34:13,119
if you it's not there's no

00:34:11,520 --> 00:34:14,800
it's meaningless to use it in the same

00:34:13,119 --> 00:34:18,079
data center

00:34:14,800 --> 00:34:20,879
you just waste power

00:34:18,079 --> 00:34:21,359
but if you have someone a client and a

00:34:20,879 --> 00:34:24,800
server

00:34:21,359 --> 00:34:25,599
a in on the east coast on on the west

00:34:24,800 --> 00:34:27,440
coast

00:34:25,599 --> 00:34:28,800
that's where you can get you know you

00:34:27,440 --> 00:34:31,599
can see a

00:34:28,800 --> 00:34:32,399
improved performance the example they've

00:34:31,599 --> 00:34:35,599
shown like

00:34:32,399 --> 00:34:38,399
uh west coast and the india that's

00:34:35,599 --> 00:34:40,240
an incredible use case uh so anyway

00:34:38,399 --> 00:34:44,560
where you have a

00:34:40,240 --> 00:34:48,399
global or twist is u.s spanning a

00:34:44,560 --> 00:34:52,480
entity that needs to connect its uh

00:34:48,399 --> 00:34:52,480
pops that would be a good use case

00:34:54,000 --> 00:34:57,760
okay so there were a few more options if

00:34:56,800 --> 00:34:59,920
you're in europe you can

00:34:57,760 --> 00:35:01,440
stream netflix probably you know if

00:34:59,920 --> 00:35:05,440
you're blocked

00:35:01,440 --> 00:35:09,359
via a usb just an option

00:35:05,440 --> 00:35:12,720
if you're willing to pay more of a joke

00:35:09,359 --> 00:35:14,960
okay so i think there were a few more

00:35:12,720 --> 00:35:17,839
questions on

00:35:14,960 --> 00:35:19,280
the zero copy and the splicing uh but i

00:35:17,839 --> 00:35:21,280
think that's actually a pretty good

00:35:19,280 --> 00:35:23,680
segue into our next talk

00:35:21,280 --> 00:35:24,640
so uh let's go ahead and move on thank

00:35:23,680 --> 00:35:28,320
you very much

00:35:24,640 --> 00:35:28,320

YouTube URL: https://www.youtube.com/watch?v=QkKFP-W-AJo


