Title: Netdev 0x14 - Transport Workshop
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Chairs: Neal Cardwell, Yuchung Cheng, Soheil Hassas Yeganeh

More info: https://netdevconf.info/0x14/session.html?workshop-transport

Date: Thursday, August 13, 2020

Neal Cardwell, Yuchung Cheng and Soheil Hassas Yeganeh chair
the Transport workshop.

The workshop entails discussions on current topics in transport-layer issues in the
kernel, including (but not limited to):
- different protocols in the Linux kernel: TCP, MPTCP, SCTP, ...
- congestion control, loss recovery, and protocol improvements
- new development, performance work, measurement, real-world production experience
- eBPF integration with the transport layer

The workshop consists of slide presentations and extended group discussions.
Captions: 
	00:00:03,199 --> 00:00:08,480
hi everyone

00:00:04,240 --> 00:00:13,360
um welcome to the transport workshop

00:00:08,480 --> 00:00:17,520
and today uh this is being hosted by

00:00:13,360 --> 00:00:20,560
neil um myself yuchon and sohail

00:00:17,520 --> 00:00:21,119
and let's get that started okay so this

00:00:20,560 --> 00:00:25,519
is our

00:00:21,119 --> 00:00:28,560
tentative agenda uh today um

00:00:25,519 --> 00:00:31,519
first we'll talk about um um the

00:00:28,560 --> 00:00:32,320
transport work by at the facebook uh you

00:00:31,519 --> 00:00:35,600
know

00:00:32,320 --> 00:00:39,040
the btf uh extensions and then

00:00:35,600 --> 00:00:42,239
uh we're going to move to

00:00:39,040 --> 00:00:44,719
uh some pos improvement

00:00:42,239 --> 00:00:47,360
and then we'll briefly talk about you

00:00:44,719 --> 00:00:49,039
know the bbr2 uh congestion control that

00:00:47,360 --> 00:00:52,800
we have been working on

00:00:49,039 --> 00:00:55,120
uh here uh google and i think um

00:00:52,800 --> 00:00:56,399
galaxy from dropbox is going to talk

00:00:55,120 --> 00:00:59,520
about how they

00:00:56,399 --> 00:01:02,960
evaluate the bbr2 um on their

00:00:59,520 --> 00:01:03,760
edge network and uh in the last 25

00:01:02,960 --> 00:01:05,840
minutes

00:01:03,760 --> 00:01:06,880
uh well talk about some kind of like

00:01:05,840 --> 00:01:10,960
open mic

00:01:06,880 --> 00:01:12,640
topics things that we have been sort of

00:01:10,960 --> 00:01:14,400
looking and might be interested to

00:01:12,640 --> 00:01:17,680
people um

00:01:14,400 --> 00:01:20,560
um here are the two sort of uh items

00:01:17,680 --> 00:01:22,320
that we propose and uh people are feel

00:01:20,560 --> 00:01:25,439
free to you know

00:01:22,320 --> 00:01:26,000
add to uh this uh you can send it uh

00:01:25,439 --> 00:01:28,960
over the

00:01:26,000 --> 00:01:30,560
chat or you can speak up now uh if you

00:01:28,960 --> 00:01:32,079
have some interesting things that you

00:01:30,560 --> 00:01:35,520
really want to gather

00:01:32,079 --> 00:01:38,560
opinions or share with um

00:01:35,520 --> 00:01:39,600
this transport area folks then uh we

00:01:38,560 --> 00:01:42,640
will move to

00:01:39,600 --> 00:01:46,000
the first uh talk by

00:01:42,640 --> 00:01:48,399
uh martin lu at facebook

00:01:46,000 --> 00:01:49,119
my name is martin um i work in the

00:01:48,399 --> 00:01:52,320
kernel team

00:01:49,119 --> 00:01:53,040
in facebook um so today i want to talk

00:01:52,320 --> 00:01:56,640
about the

00:01:53,040 --> 00:02:00,560
bpl4 we have done to make the

00:01:56,640 --> 00:02:00,560
linux networking stack extensible

00:02:04,960 --> 00:02:08,800
so those are the common we have some

00:02:07,040 --> 00:02:12,400
common requests to our team uh to

00:02:08,800 --> 00:02:16,800
to make uh kernel changes and

00:02:12,400 --> 00:02:19,440
they uh um for the transport uh

00:02:16,800 --> 00:02:22,480
specific uh they are the modifying the

00:02:19,440 --> 00:02:25,120
tcp contraction control or creating new

00:02:22,480 --> 00:02:26,879
tcp contraction control and another one

00:02:25,120 --> 00:02:29,920
is the uh

00:02:26,879 --> 00:02:31,040
writing new options to the tcp header so

00:02:29,920 --> 00:02:34,400
those are the two

00:02:31,040 --> 00:02:37,840
things that i will want to talk about in

00:02:34,400 --> 00:02:37,840
today's discussion

00:02:41,440 --> 00:02:45,840
so before we make it uh uh extensible by

00:02:44,800 --> 00:02:49,200
bpf how

00:02:45,840 --> 00:02:50,879
do we usually do that now is it's not

00:02:49,200 --> 00:02:53,599
easy for most of the

00:02:50,879 --> 00:02:54,800
uh developers and research researchers

00:02:53,599 --> 00:02:56,959
because

00:02:54,800 --> 00:02:58,959
they have to make kernel change

00:02:56,959 --> 00:03:01,360
carefully carefully in the sense that

00:02:58,959 --> 00:03:03,760
don't crash the kernel and things like

00:03:01,360 --> 00:03:04,080
that and also it will take quite a long

00:03:03,760 --> 00:03:07,200
time

00:03:04,080 --> 00:03:09,519
to turn around from deploy gather data

00:03:07,200 --> 00:03:10,720
and then reiterate and then try again

00:03:09,519 --> 00:03:12,560
because uh

00:03:10,720 --> 00:03:14,000
that means uh for making kernel changes

00:03:12,560 --> 00:03:15,120
that means you need to upgrade the

00:03:14,000 --> 00:03:18,080
kernel or at least

00:03:15,120 --> 00:03:19,920
upgrade the kernel module for the tcp

00:03:18,080 --> 00:03:22,080
contraction control case

00:03:19,920 --> 00:03:24,000
but for the head option case pretty much

00:03:22,080 --> 00:03:26,400
we have to upgrade the kernel

00:03:24,000 --> 00:03:28,640
and for some of our production

00:03:26,400 --> 00:03:29,920
environment it has a very long tail of

00:03:28,640 --> 00:03:33,120
kernel versions

00:03:29,920 --> 00:03:33,760
which make this whole process not super

00:03:33,120 --> 00:03:37,200
easy

00:03:33,760 --> 00:03:39,200
uh for for most of the developers

00:03:37,200 --> 00:03:40,480
so what we want to do is to make use of

00:03:39,200 --> 00:03:43,840
bpf to make

00:03:40,480 --> 00:03:45,519
to to make this whole uh our process

00:03:43,840 --> 00:03:49,519
much easier and much safer

00:03:45,519 --> 00:03:52,319
to the production by using bpfo

00:03:49,519 --> 00:03:53,760
the bpf verifier provides a lot of

00:03:52,319 --> 00:03:56,799
safety guarantee

00:03:53,760 --> 00:03:58,480
for example it will ensure that the bpa

00:03:56,799 --> 00:04:00,799
program itself won't

00:03:58,480 --> 00:04:02,799
crash the kernel or won't cause any

00:04:00,799 --> 00:04:03,760
bamboo corruption or deadline things

00:04:02,799 --> 00:04:06,640
like that

00:04:03,760 --> 00:04:07,920
uh so that will provide more confidence

00:04:06,640 --> 00:04:11,280
to test in the production

00:04:07,920 --> 00:04:12,400
and and make the uh developer live

00:04:11,280 --> 00:04:15,680
easier

00:04:12,400 --> 00:04:17,600
and the second thing is um the developer

00:04:15,680 --> 00:04:18,079
only need to compile the bpa program

00:04:17,600 --> 00:04:20,880
once

00:04:18,079 --> 00:04:22,400
and then it can be the same bpa program

00:04:20,880 --> 00:04:25,199
can be run in

00:04:22,400 --> 00:04:29,840
uh different kind of versions uh so that

00:04:25,199 --> 00:04:29,840
will also make the deployment easier

00:04:30,320 --> 00:04:36,960
so now let's dive in um to the

00:04:33,759 --> 00:04:40,800
tcp construction control

00:04:36,960 --> 00:04:42,160
so in this slide i have put two pieces

00:04:40,800 --> 00:04:46,400
of coke here

00:04:42,160 --> 00:04:49,520
one is uh one is uh the bpf cubic

00:04:46,400 --> 00:04:53,120
and one is the uh kernel cubic

00:04:49,520 --> 00:04:55,199
uh from these two pieces of coke

00:04:53,120 --> 00:04:57,759
can you guys quickly tell which one is

00:04:55,199 --> 00:05:00,639
the kernel cubic which one is the

00:04:57,759 --> 00:05:00,639
epf cubic

00:05:04,560 --> 00:05:09,840
i think this is easier to guess because

00:05:07,840 --> 00:05:12,960
the oil

00:05:09,840 --> 00:05:16,880
casting on the left is sort of uh

00:05:12,960 --> 00:05:16,880
giving it away um

00:05:18,160 --> 00:05:24,000
which is a some boilerplate code that

00:05:21,759 --> 00:05:26,720
required for the bpf construction

00:05:24,000 --> 00:05:27,280
control to cast the function pointer but

00:05:26,720 --> 00:05:29,919
uh

00:05:27,280 --> 00:05:31,360
but other than that is it it is written

00:05:29,919 --> 00:05:34,639
pretty much the same

00:05:31,360 --> 00:05:35,280
uh as you know uh to implement a

00:05:34,639 --> 00:05:39,199
conjunction

00:05:35,280 --> 00:05:40,720
control in linux um we need to fill

00:05:39,199 --> 00:05:42,720
implement a lot of function i feel

00:05:40,720 --> 00:05:46,240
function pointers in the stroke tcp

00:05:42,720 --> 00:05:48,479
construction ops so that uh

00:05:46,240 --> 00:05:50,479
the bpf cuber is doing exactly the same

00:05:48,479 --> 00:05:55,600
thing is filling out the exact

00:05:50,479 --> 00:05:55,600
same tcp congestion ops structure

00:05:55,759 --> 00:05:59,199
so if you don't believe me it's written

00:05:57,840 --> 00:06:02,880
pretty much the same way

00:05:59,199 --> 00:06:05,919
we can dive into one of the

00:06:02,880 --> 00:06:06,800
function point implementation i take the

00:06:05,919 --> 00:06:09,919
access

00:06:06,800 --> 00:06:10,319
trash here because it's small enough to

00:06:09,919 --> 00:06:13,600
be

00:06:10,319 --> 00:06:17,280
demonstrated in one slide

00:06:13,600 --> 00:06:20,880
so in this slide is the access trash

00:06:17,280 --> 00:06:22,639
implementation for cubic uh so one piece

00:06:20,880 --> 00:06:25,600
of code is uh

00:06:22,639 --> 00:06:26,319
written in bpf another piece of code is

00:06:25,600 --> 00:06:29,759
uh

00:06:26,319 --> 00:06:30,720
written in the kernel uh way kernel cc

00:06:29,759 --> 00:06:34,720
way

00:06:30,720 --> 00:06:36,720
uh from these two pieces of i think is

00:06:34,720 --> 00:06:37,759
you won't be able to tell which one is

00:06:36,720 --> 00:06:40,160
which because they

00:06:37,759 --> 00:06:41,280
should be exactly the same except the

00:06:40,160 --> 00:06:44,319
very first slide i

00:06:41,280 --> 00:06:44,319
i have a cover up

00:06:46,560 --> 00:06:50,080
so the left one will be the one

00:06:48,160 --> 00:06:51,840
implemented in bpf um

00:06:50,080 --> 00:06:54,080
the only difference is again the very

00:06:51,840 --> 00:06:56,560
first line um

00:06:54,080 --> 00:06:57,520
um that are put in the function

00:06:56,560 --> 00:07:01,280
signature

00:06:57,520 --> 00:07:04,960
in in the bpf it require a macro

00:07:01,280 --> 00:07:07,759
uh which is also some bpf boilerplate

00:07:04,960 --> 00:07:09,360
uh code that require uh the bpa program

00:07:07,759 --> 00:07:10,960
to do

00:07:09,360 --> 00:07:13,360
uh but other than that in the in the

00:07:10,960 --> 00:07:15,759
function uh body it should

00:07:13,360 --> 00:07:16,880
be live by like the same as you can see

00:07:15,759 --> 00:07:20,240
the b-pair program

00:07:16,880 --> 00:07:23,599
can access the tcp socket

00:07:20,240 --> 00:07:26,960
uh you can even access the uh ca

00:07:23,599 --> 00:07:28,840
private space and even write to it and

00:07:26,960 --> 00:07:30,000
do exactly the same things that the

00:07:28,840 --> 00:07:33,120
kernel

00:07:30,000 --> 00:07:33,120
cc module can do

00:07:33,840 --> 00:07:39,759
so after we have a director after the

00:07:36,880 --> 00:07:43,360
bpfc has been written how can we be

00:07:39,759 --> 00:07:46,160
used in the production um

00:07:43,360 --> 00:07:47,360
so first uh it needs to be loaded by a

00:07:46,160 --> 00:07:50,639
command

00:07:47,360 --> 00:07:55,199
called bpf2 after loading it

00:07:50,639 --> 00:07:59,199
the bpfc can be used exactly the same as

00:07:55,199 --> 00:08:01,759
other kernel cc module can be used

00:07:59,199 --> 00:08:02,720
for example it can be observed in the

00:08:01,759 --> 00:08:06,160
syscuddle

00:08:02,720 --> 00:08:08,319
um in the second command line

00:08:06,160 --> 00:08:10,720
we can see it as one of the available

00:08:08,319 --> 00:08:13,599
conjunction control

00:08:10,720 --> 00:08:14,319
and we can you even use the bpf cube as

00:08:13,599 --> 00:08:17,759
the default

00:08:14,319 --> 00:08:22,160
default conjunction control by

00:08:17,759 --> 00:08:22,160
by uh setting in the syscutter also

00:08:22,319 --> 00:08:29,039
and yes it can be used in sets or up to

00:08:25,360 --> 00:08:30,240
um so if we want to only use the bpf

00:08:29,039 --> 00:08:33,599
kubrick in

00:08:30,240 --> 00:08:37,120
some specific connection or it can be

00:08:33,599 --> 00:08:38,159
done too so again once once the bpf

00:08:37,120 --> 00:08:41,360
cooper is loaded

00:08:38,159 --> 00:08:44,640
it could be used exactly the same way as

00:08:41,360 --> 00:08:46,590
other kernel conjunction control model

00:08:44,640 --> 00:08:49,789
do

00:08:46,590 --> 00:08:49,789
[Music]

00:08:50,000 --> 00:08:56,800
so what was the status of this um

00:08:53,040 --> 00:08:59,519
it is available since kernel 5.6

00:08:56,800 --> 00:09:02,959
and i have also landed the cuba and dc

00:08:59,519 --> 00:09:04,959
tcp example in the self-test directory

00:09:02,959 --> 00:09:06,720
um so if you're interested please take a

00:09:04,959 --> 00:09:11,600
look and see how

00:09:06,720 --> 00:09:11,600
how it is written in the bpf

00:09:12,959 --> 00:09:18,959
so next i want to talk about is the tcp

00:09:16,560 --> 00:09:20,399
head options so the idea is to allow the

00:09:18,959 --> 00:09:23,120
bpa program

00:09:20,399 --> 00:09:24,880
to write tcp head option and pass the

00:09:23,120 --> 00:09:27,920
tcp head options

00:09:24,880 --> 00:09:30,080
so the use cases uh we have internally

00:09:27,920 --> 00:09:33,040
we have in mind internally is for

00:09:30,080 --> 00:09:36,240
example writing the maximum delay at

00:09:33,040 --> 00:09:39,360
in the header and the receiver set

00:09:36,240 --> 00:09:41,279
a lower uh with transmission timeout

00:09:39,360 --> 00:09:43,600
and other things that we want to try is

00:09:41,279 --> 00:09:44,640
uh putting the next speed in the header

00:09:43,600 --> 00:09:46,880
or

00:09:44,640 --> 00:09:48,000
or the conjunction control one side is

00:09:46,880 --> 00:09:52,000
preferred

00:09:48,000 --> 00:09:54,160
and etc um so the idea is um

00:09:52,000 --> 00:09:55,839
we will allow the bpa program uh to

00:09:54,160 --> 00:09:58,080
write any header option

00:09:55,839 --> 00:09:58,080
con

00:09:59,519 --> 00:10:02,880
the kernel doesn't put a lot of

00:10:00,800 --> 00:10:04,399
restriction on what header option kind

00:10:02,880 --> 00:10:06,000
you can write

00:10:04,399 --> 00:10:08,399
the only things the kernel pretty much

00:10:06,000 --> 00:10:11,200
will track is to ensure

00:10:08,399 --> 00:10:12,240
uh the option is not duplicated for

00:10:11,200 --> 00:10:15,360
example the

00:10:12,240 --> 00:10:16,959
bpa program kind of write the windows

00:10:15,360 --> 00:10:19,440
scale options because

00:10:16,959 --> 00:10:21,120
it has already been written by the

00:10:19,440 --> 00:10:24,560
kernel

00:10:21,120 --> 00:10:25,360
so this feature i expect will be mostly

00:10:24,560 --> 00:10:28,480
used

00:10:25,360 --> 00:10:30,640
during the freeway handshake but

00:10:28,480 --> 00:10:32,079
that can also be used in the data packet

00:10:30,640 --> 00:10:36,320
header pure ad

00:10:32,079 --> 00:10:39,600
header or even a thin packet header

00:10:36,320 --> 00:10:42,560
um so i've already posted

00:10:39,600 --> 00:10:44,800
this change to the mounting disk um it's

00:10:42,560 --> 00:10:47,680
ran for a few we vision already

00:10:44,800 --> 00:10:48,000
and i got some good feedback from eric

00:10:47,680 --> 00:10:51,440
and

00:10:48,000 --> 00:10:53,279
uh and a few other bpf maintainer

00:10:51,440 --> 00:10:56,240
hopefully it will be

00:10:53,279 --> 00:10:58,079
landed in the upcoming release cycle so

00:10:56,240 --> 00:11:00,560
i'm hoping

00:10:58,079 --> 00:11:01,519
to be some time later this month or

00:11:00,560 --> 00:11:04,720
early next month

00:11:01,519 --> 00:11:04,720
hopefully we'll get landed

00:11:06,720 --> 00:11:09,839
so the last i want to talk about is the

00:11:08,480 --> 00:11:14,320
um

00:11:09,839 --> 00:11:18,320
socket storage for bpf program

00:11:14,320 --> 00:11:21,839
so as we know we we try to make the

00:11:18,320 --> 00:11:24,560
more more networking uh

00:11:21,839 --> 00:11:26,240
feature extensible by ppf we'll start

00:11:24,560 --> 00:11:29,760
writing more bpa program to

00:11:26,240 --> 00:11:31,600
to program the leveling step it is uh

00:11:29,760 --> 00:11:33,440
getting more and more common that the

00:11:31,600 --> 00:11:36,880
ppl program want to store

00:11:33,440 --> 00:11:37,760
something uh or associate some data to a

00:11:36,880 --> 00:11:41,720
specific

00:11:37,760 --> 00:11:43,279
socket for example uh we may have a new

00:11:41,720 --> 00:11:46,640
tcpcc

00:11:43,279 --> 00:11:47,920
uh algorithm that may want to store a

00:11:46,640 --> 00:11:50,480
few more datas

00:11:47,920 --> 00:11:51,200
for a connection for example it could be

00:11:50,480 --> 00:11:54,839
a few more

00:11:51,200 --> 00:11:58,399
round trip time sample to a connection

00:11:54,839 --> 00:12:01,600
um so the only way to do it is to um

00:11:58,399 --> 00:12:03,360
create bpf map with the with the four

00:12:01,600 --> 00:12:06,240
tuples as the

00:12:03,360 --> 00:12:08,959
as the key um default tuple is the user

00:12:06,240 --> 00:12:11,920
source destination ip and port

00:12:08,959 --> 00:12:14,320
and then you store the data as the value

00:12:11,920 --> 00:12:14,320
of the map

00:12:14,399 --> 00:12:17,839
it can be do this way and it works fine

00:12:17,200 --> 00:12:21,040
but

00:12:17,839 --> 00:12:23,760
the downside is um it's quite expensive

00:12:21,040 --> 00:12:24,800
uh for example the lookup time is

00:12:23,760 --> 00:12:28,639
expensive and

00:12:24,800 --> 00:12:32,000
it also cause memory to store these four

00:12:28,639 --> 00:12:34,320
x four tuples as the key

00:12:32,000 --> 00:12:36,240
and more importantly is a nightmare to

00:12:34,320 --> 00:12:39,279
maintain for example um

00:12:36,240 --> 00:12:41,440
we need to remove this key

00:12:39,279 --> 00:12:42,959
or remove this entry from the map when

00:12:41,440 --> 00:12:45,839
the socket is closed

00:12:42,959 --> 00:12:47,360
or the connection is closed otherwise

00:12:45,839 --> 00:12:50,079
the map will get exposed

00:12:47,360 --> 00:12:53,200
exposed if we don't remove them from the

00:12:50,079 --> 00:12:56,399
map when the socket is closed

00:12:53,200 --> 00:12:57,279
so a new way to do it is to uh the bpa

00:12:56,399 --> 00:12:59,440
program allow

00:12:57,279 --> 00:13:01,120
right now we allow the bpa program to

00:12:59,440 --> 00:13:04,839
store something

00:13:01,120 --> 00:13:08,240
to store extra data relatively to the

00:13:04,839 --> 00:13:08,959
socket and this data will go away with

00:13:08,240 --> 00:13:13,120
the socket

00:13:08,959 --> 00:13:15,040
so if the socket get closed this

00:13:13,120 --> 00:13:16,720
data started by the bpa program will

00:13:15,040 --> 00:13:19,600
also go away

00:13:16,720 --> 00:13:21,519
automatically and it's pretty easy to

00:13:19,600 --> 00:13:24,160
use um

00:13:21,519 --> 00:13:27,040
the bpa program only you need to use a

00:13:24,160 --> 00:13:29,920
helper called bpfsk storage

00:13:27,040 --> 00:13:31,600
get and then pass in the sk ponder as

00:13:29,920 --> 00:13:33,519
one of the argument and

00:13:31,600 --> 00:13:36,560
then you can get access to the data it

00:13:33,519 --> 00:13:41,279
has started in the socket

00:13:36,560 --> 00:13:44,320
um that's pretty much what i have um

00:13:41,279 --> 00:13:46,399
so um what legs i think is

00:13:44,320 --> 00:13:48,320
i want to hear from you guys what do you

00:13:46,399 --> 00:13:52,240
want to be extensible

00:13:48,320 --> 00:13:54,639
extensible by bpf2 um

00:13:52,240 --> 00:13:55,680
could be other transport or the ip

00:13:54,639 --> 00:13:58,800
header or

00:13:55,680 --> 00:13:58,800
routing or qd's

00:14:06,240 --> 00:14:08,959
that's all i have

00:14:10,000 --> 00:14:17,839
so do we just want questions over the

00:14:12,160 --> 00:14:17,839
mic or chat

00:14:19,279 --> 00:14:23,199
anyway let me ask a question uh so with

00:14:22,240 --> 00:14:26,160
the

00:14:23,199 --> 00:14:27,120
tcp option breathe right have you

00:14:26,160 --> 00:14:28,959
considered

00:14:27,120 --> 00:14:32,720
um how to deal with that in the presence

00:14:28,959 --> 00:14:32,720
of tcp authentication option

00:14:35,440 --> 00:14:40,240
i'm not familiar with that option can

00:14:37,120 --> 00:14:40,240
you tell me more about that

00:14:41,680 --> 00:14:45,600
so it's uh it is it's what it sounds

00:14:44,000 --> 00:14:49,440
like it basically authenticates

00:14:45,600 --> 00:14:49,920
the tcp header so the obvious thing is

00:14:49,440 --> 00:14:53,199
if

00:14:49,920 --> 00:14:56,160
if that option is used then no one

00:14:53,199 --> 00:14:59,040
no one below the stack can change any of

00:14:56,160 --> 00:15:02,560
the tcp header and if they do

00:14:59,040 --> 00:15:05,120
then um verification fails

00:15:02,560 --> 00:15:06,880
at at the peer end point it's kind of a

00:15:05,120 --> 00:15:09,199
common problem we see

00:15:06,880 --> 00:15:10,839
um we'll see that in certain cases with

00:15:09,199 --> 00:15:13,760
ipsec

00:15:10,839 --> 00:15:17,600
and um other cases

00:15:13,760 --> 00:15:18,320
uh ah option um what else would i have

00:15:17,600 --> 00:15:20,720
that impact

00:15:18,320 --> 00:15:22,959
if we were rewriting an ip uh ip

00:15:20,720 --> 00:15:26,959
extension header option for instance

00:15:22,959 --> 00:15:30,160
so if it's kind of tricky because

00:15:26,959 --> 00:15:32,959
if the soho stack doesn't realize that

00:15:30,160 --> 00:15:34,639
somebody may change it underneath and

00:15:32,959 --> 00:15:36,000
the thing underneath doesn't realize

00:15:34,639 --> 00:15:38,959
that the host stack

00:15:36,000 --> 00:15:40,720
doesn't want things to change uh then we

00:15:38,959 --> 00:15:43,360
have a problem because

00:15:40,720 --> 00:15:44,079
um packets may be dropped because of

00:15:43,360 --> 00:15:46,639
these

00:15:44,079 --> 00:15:47,199
um changes in the middle of the network

00:15:46,639 --> 00:15:49,440
and

00:15:47,199 --> 00:15:50,800
this option would be particularly useful

00:15:49,440 --> 00:15:52,560
uh for rewriting

00:15:50,800 --> 00:15:55,279
i think you mentioned rewriting things

00:15:52,560 --> 00:15:57,839
like um

00:15:55,279 --> 00:15:58,639
sender window which a lot of satellite

00:15:57,839 --> 00:16:00,959
links do

00:15:58,639 --> 00:16:03,040
in order to optimize them but turn on

00:16:00,959 --> 00:16:09,839
any sort of authentication of the header

00:16:03,040 --> 00:16:09,839
and all of that just falls down

00:16:16,320 --> 00:16:22,480
because that is the same as any other

00:16:19,839 --> 00:16:23,199
new option getting written by the tcp

00:16:22,480 --> 00:16:25,120
stat ring

00:16:23,199 --> 00:16:26,560
if if the lower stack want to

00:16:25,120 --> 00:16:30,320
authenticate it

00:16:26,560 --> 00:16:34,560
has to deal with it as of today how

00:16:30,320 --> 00:16:36,240
would that be any different well

00:16:34,560 --> 00:16:38,720
if i understand it correctly if you're

00:16:36,240 --> 00:16:41,680
doing this and bpf

00:16:38,720 --> 00:16:42,399
if it's being done as part of the stack

00:16:41,680 --> 00:16:45,440
then

00:16:42,399 --> 00:16:47,680
that's not that's not much of an issue

00:16:45,440 --> 00:16:49,519
um because it's because the stack is is

00:16:47,680 --> 00:16:51,759
basically asking for it and it would

00:16:49,519 --> 00:16:53,680
set up the if it sets up the tcp

00:16:51,759 --> 00:16:56,720
authentication after

00:16:53,680 --> 00:16:58,160
the bpf is run then that's fine but the

00:16:56,720 --> 00:17:00,480
problem we have is when

00:16:58,160 --> 00:17:01,680
it's being done outside of the auspices

00:17:00,480 --> 00:17:03,759
of the stack

00:17:01,680 --> 00:17:05,360
like a like i said the networking ev

00:17:03,759 --> 00:17:06,959
networking device at the ingress to a

00:17:05,360 --> 00:17:10,000
satellite link

00:17:06,959 --> 00:17:13,439
may try to write its own option data

00:17:10,000 --> 00:17:16,559
or the receive window

00:17:13,439 --> 00:17:18,480
may try to rewrite those and if if

00:17:16,559 --> 00:17:19,839
authentication is present the packets

00:17:18,480 --> 00:17:22,319
dropped because the data

00:17:19,839 --> 00:17:22,880
you know that's in the authentication

00:17:22,319 --> 00:17:25,280
the

00:17:22,880 --> 00:17:26,000
computation for authentication fails so

00:17:25,280 --> 00:17:29,919
anyway so

00:17:26,000 --> 00:17:32,320
let's take it offline but um

00:17:29,919 --> 00:17:33,919
like whenever we do this sort of thing

00:17:32,320 --> 00:17:35,200
it adds a lot of power people are going

00:17:33,919 --> 00:17:38,240
to use it

00:17:35,200 --> 00:17:42,160
but there are potential downsides

00:17:38,240 --> 00:17:44,480
that particularly in security

00:17:42,160 --> 00:17:46,000
yeah right now right now this feature is

00:17:44,480 --> 00:17:49,039
implemented in the tcp

00:17:46,000 --> 00:17:50,720
stack so anything goes under and if

00:17:49,039 --> 00:17:51,280
anything want to do authentication under

00:17:50,720 --> 00:17:54,799
this

00:17:51,280 --> 00:17:56,559
under the tcp layers should

00:17:54,799 --> 00:17:58,559
take it should work or take care of it

00:17:56,559 --> 00:18:00,400
otherwise for example i would imagine

00:17:58,559 --> 00:18:03,679
it's like

00:18:00,400 --> 00:18:04,799
it's like uh let's say a new new option

00:18:03,679 --> 00:18:07,520
get added to the

00:18:04,799 --> 00:18:09,440
standard we a new kernel role i will

00:18:07,520 --> 00:18:11,120
also do the same thing add new

00:18:09,440 --> 00:18:12,720
write this new standard option at the

00:18:11,120 --> 00:18:15,440
tcp layer also

00:18:12,720 --> 00:18:15,840
but if i think you're talking about more

00:18:15,440 --> 00:18:20,559
a

00:18:15,840 --> 00:18:23,919
out of band header changes may

00:18:20,559 --> 00:18:28,080
make top hole the uh the authentication

00:18:23,919 --> 00:18:31,520
yeah but yeah that's the way the concern

00:18:28,080 --> 00:18:33,039
is it's inevitable um and

00:18:31,520 --> 00:18:35,280
and it's something we're gonna have to

00:18:33,039 --> 00:18:36,160
deal with uh we're looking at this

00:18:35,280 --> 00:18:38,559
problem in

00:18:36,160 --> 00:18:40,160
uh six man you may wanna follow that but

00:18:38,559 --> 00:18:42,720
that's for

00:18:40,160 --> 00:18:44,559
destination hop by hop options anyway so

00:18:42,720 --> 00:18:46,480
i i think it's uh it's an issue

00:18:44,559 --> 00:18:48,720
um probably not a show stopper but i

00:18:46,480 --> 00:18:51,840
just wanted to bring it up

00:18:48,720 --> 00:18:53,280
okay that's great uh i think uh judge

00:18:51,840 --> 00:18:56,080
honked on the chat

00:18:53,280 --> 00:18:57,440
um asked this question uh do you have

00:18:56,080 --> 00:19:00,080
any performance data

00:18:57,440 --> 00:19:02,320
comparing in kernel versus bpf

00:19:00,080 --> 00:19:06,080
congestion control

00:19:02,320 --> 00:19:08,480
yeah i have we did some tests um

00:19:06,080 --> 00:19:09,679
i we did some i hope i should have

00:19:08,480 --> 00:19:12,160
proposed

00:19:09,679 --> 00:19:12,880
this graph um yes we did sometimes we

00:19:12,160 --> 00:19:15,679
don't see

00:19:12,880 --> 00:19:17,440
we don't see meaningful difference in

00:19:15,679 --> 00:19:20,160
terms of performance

00:19:17,440 --> 00:19:24,160
football performance and cpu performance

00:19:20,160 --> 00:19:26,160
we don't see meaningful difference

00:19:24,160 --> 00:19:28,880
okay and there's another question on

00:19:26,160 --> 00:19:29,440
chat that are there any new verifier

00:19:28,880 --> 00:19:39,840
checks

00:19:29,440 --> 00:19:39,840
for bpf cc that we should be aware of

00:19:40,480 --> 00:19:46,880
what do you mean by verified trap

00:19:44,080 --> 00:19:46,880
you mean the

00:19:47,760 --> 00:19:51,280
there's that's some change to verify for

00:19:50,160 --> 00:19:54,799
sure but

00:19:51,280 --> 00:19:58,400
i'm not sure there's something that uh

00:19:54,799 --> 00:19:58,400
you need to worry about yeah

00:19:59,840 --> 00:20:03,520
yeah i'm also not very clear what

00:20:02,080 --> 00:20:07,919
verifier checks

00:20:03,520 --> 00:20:09,760
um yeah because uh for new feature ad

00:20:07,919 --> 00:20:11,280
for for adding new features like that

00:20:09,760 --> 00:20:13,919
there will be some

00:20:11,280 --> 00:20:15,039
change in the verifier because they need

00:20:13,919 --> 00:20:16,640
to chat

00:20:15,039 --> 00:20:18,159
a few more things try to ensure the

00:20:16,640 --> 00:20:21,440
program is

00:20:18,159 --> 00:20:24,159
is legit and not harming the kernel but

00:20:21,440 --> 00:20:25,600
i don't think that's something that um

00:20:24,159 --> 00:20:30,480
the bpf programmer

00:20:25,600 --> 00:20:33,679
need to be worried about you know

00:20:30,480 --> 00:20:35,840
yeah okay um

00:20:33,679 --> 00:20:37,200
i have a question um by the way thanks

00:20:35,840 --> 00:20:40,320
for this uh work

00:20:37,200 --> 00:20:43,600
i really appreciate that um this kernel

00:20:40,320 --> 00:20:45,440
tcp changes in vpn is really a big

00:20:43,600 --> 00:20:48,640
advancement at least for

00:20:45,440 --> 00:20:52,640
um people who work in you know

00:20:48,640 --> 00:20:56,320
large data center companies because

00:20:52,640 --> 00:20:59,679
this allows very fast deployment of

00:20:56,320 --> 00:21:03,600
tcp changes or fixes

00:20:59,679 --> 00:21:05,600
my question is that how big of

00:21:03,600 --> 00:21:07,360
the tcp congestion control is there a

00:21:05,600 --> 00:21:10,480
size limit for example

00:21:07,360 --> 00:21:13,840
bbr is obviously more complicated than

00:21:10,480 --> 00:21:16,159
cubic so is there a particular size

00:21:13,840 --> 00:21:18,720
limit that we should be aware of when

00:21:16,159 --> 00:21:22,720
people trying to port

00:21:18,720 --> 00:21:26,480
their condition control using your work

00:21:22,720 --> 00:21:28,840
no in terms of uh in terms of number of

00:21:26,480 --> 00:21:30,799
instruction bpf instruction i don't

00:21:28,840 --> 00:21:32,400
think

00:21:30,799 --> 00:21:34,080
i don't think it is something that we

00:21:32,400 --> 00:21:35,600
have to worry about because right now i

00:21:34,080 --> 00:21:38,480
think the limit is in

00:21:35,600 --> 00:21:39,120
more than meeting instruction so i don't

00:21:38,480 --> 00:21:42,880
think

00:21:39,120 --> 00:21:44,960
we will hit that any any any tcp cc will

00:21:42,880 --> 00:21:49,760
hit that limit

00:21:44,960 --> 00:21:49,760
okay sounds very promising okay

00:21:50,400 --> 00:22:01,679
any other questions

00:21:58,000 --> 00:22:02,240
um i think srinivas mentioned another

00:22:01,679 --> 00:22:05,039
question

00:22:02,240 --> 00:22:06,240
um the performance is similar to in

00:22:05,039 --> 00:22:09,200
kernel

00:22:06,240 --> 00:22:13,840
but should we expect lower latency and

00:22:09,200 --> 00:22:13,840
cpu with bpf

00:22:16,960 --> 00:22:20,720
lower latency in terms of the response

00:22:18,960 --> 00:22:22,960
time latency or

00:22:20,720 --> 00:22:22,960
right

00:22:24,000 --> 00:22:28,080
in terms of response time latency lower

00:22:26,240 --> 00:22:30,080
in bpf no we don't

00:22:28,080 --> 00:22:32,720
we don't we don't see that the latency

00:22:30,080 --> 00:22:35,360
time from the test i have

00:22:32,720 --> 00:22:37,120
i've done is also pretty much the same

00:22:35,360 --> 00:22:41,840
as the kernel

00:22:37,120 --> 00:22:41,840
the kernel uh implementation

00:22:45,360 --> 00:22:47,600
cool

00:22:48,400 --> 00:22:57,840
any other people have questions

00:22:54,640 --> 00:23:00,559
okay um then i have another so

00:22:57,840 --> 00:23:01,200
um when we are loading this uh bpf let's

00:23:00,559 --> 00:23:04,480
say right

00:23:01,200 --> 00:23:07,520
in um this is a live host many people

00:23:04,480 --> 00:23:11,360
many connections are using the bpfcc

00:23:07,520 --> 00:23:13,600
like the vpn cubic right and

00:23:11,360 --> 00:23:15,600
let's say and then we decided to upgrade

00:23:13,600 --> 00:23:18,880
this bpf cubic

00:23:15,600 --> 00:23:19,600
can the host literally like just plug in

00:23:18,880 --> 00:23:23,600
another

00:23:19,600 --> 00:23:24,320
new bpf cubic and the new flows will

00:23:23,600 --> 00:23:27,440
immediately

00:23:24,320 --> 00:23:30,640
hop onto this new uh code like

00:23:27,440 --> 00:23:32,159
what's the transition scenario so

00:23:30,640 --> 00:23:33,919
that's a great question the transition

00:23:32,159 --> 00:23:38,000
will be exactly the same as the

00:23:33,919 --> 00:23:41,039
native kernel module so um

00:23:38,000 --> 00:23:44,640
so can you load the can we load the bp a

00:23:41,039 --> 00:23:44,640
new bpf curve um

00:23:45,120 --> 00:23:49,600
yes if you give it a different name

00:23:47,360 --> 00:23:50,000
called bpf cubic version two yes then we

00:23:49,600 --> 00:23:54,640
can know

00:23:50,000 --> 00:23:57,520
it um will it automatically switch the

00:23:54,640 --> 00:23:58,320
existing connection to use the bpf cubic

00:23:57,520 --> 00:24:01,919
v2

00:23:58,320 --> 00:24:05,679
uh no because uh that is also the

00:24:01,919 --> 00:24:06,960
limitation of the of the kernel cc

00:24:05,679 --> 00:24:09,520
module right

00:24:06,960 --> 00:24:10,480
because loading loading a new kernel cc

00:24:09,520 --> 00:24:13,039
module doesn't

00:24:10,480 --> 00:24:15,360
automatically uh switch the existing

00:24:13,039 --> 00:24:19,120
connection to use that too

00:24:15,360 --> 00:24:20,799
but um but we can

00:24:19,120 --> 00:24:22,799
that's also a common problem that we

00:24:20,799 --> 00:24:25,919
have um

00:24:22,799 --> 00:24:26,480
so uh one solution we have start to come

00:24:25,919 --> 00:24:29,760
together

00:24:26,480 --> 00:24:34,320
is to is um is

00:24:29,760 --> 00:24:37,760
uh there is way in the bpf to iterate

00:24:34,320 --> 00:24:40,240
uh all the connection now so

00:24:37,760 --> 00:24:41,440
uh it's called the bpf iterator one of

00:24:40,240 --> 00:24:44,720
the iterator is to

00:24:41,440 --> 00:24:48,240
iterate all the tcp connection so what

00:24:44,720 --> 00:24:51,200
we can do now is um we can

00:24:48,240 --> 00:24:53,679
use this bpf iterator to iterate all

00:24:51,200 --> 00:24:55,919
this tcp connection

00:24:53,679 --> 00:24:58,320
and then check which one we need to

00:24:55,919 --> 00:25:00,159
switch to conjunction control algorithm

00:24:58,320 --> 00:25:01,440
and then we can switch that to bpf

00:25:00,159 --> 00:25:06,240
cooperate we too

00:25:01,440 --> 00:25:06,240
so that's one way to do it now

00:25:06,559 --> 00:25:10,240
but of course it's not as seamless as

00:25:08,559 --> 00:25:15,039
what you have just described it

00:25:10,240 --> 00:25:17,279
um but there are ways to do it

00:25:15,039 --> 00:25:18,720
so it sounds like the um the new

00:25:17,279 --> 00:25:22,640
connection will have to

00:25:18,720 --> 00:25:26,320
uh still hop on this new

00:25:22,640 --> 00:25:30,960
sort of bpf cubic tool or

00:25:26,320 --> 00:25:33,039
um first like we cannot

00:25:30,960 --> 00:25:35,600
trying to migrate let's say i want i

00:25:33,039 --> 00:25:38,480
still want to keep it the same

00:25:35,600 --> 00:25:39,279
um congestion control name like bpf

00:25:38,480 --> 00:25:42,640
cubic

00:25:39,279 --> 00:25:44,400
then there's not a way that we can just

00:25:42,640 --> 00:25:46,480
make the new connection move and

00:25:44,400 --> 00:25:50,159
eventually all the old connections

00:25:46,480 --> 00:25:53,200
sort of um close and

00:25:50,159 --> 00:25:55,520
smoothly transition to sort of a

00:25:53,200 --> 00:25:56,640
like from the surface it would like it

00:25:55,520 --> 00:26:00,320
would look like we just

00:25:56,640 --> 00:26:04,080
uh transparently upgraded um

00:26:00,320 --> 00:26:05,950
the module or the the congestion control

00:26:04,080 --> 00:26:07,679
and yeah

00:26:05,950 --> 00:26:11,200
[Music]

00:26:07,679 --> 00:26:13,039
yeah no not see as steamness as

00:26:11,200 --> 00:26:14,559
you have in mind and also i would like

00:26:13,039 --> 00:26:16,400
that in this way like

00:26:14,559 --> 00:26:18,400
just load the little bpf cube and then

00:26:16,400 --> 00:26:19,039
all the connections existing connection

00:26:18,400 --> 00:26:22,400
will

00:26:19,039 --> 00:26:26,000
will automatically switch to that um no

00:26:22,400 --> 00:26:29,200
it can be done right now okay

00:26:26,000 --> 00:26:33,120
uh thanks so i'll have the last question

00:26:29,200 --> 00:26:36,320
from um uh

00:26:33,120 --> 00:26:38,960
is it possible to register a bpfcc

00:26:36,320 --> 00:26:42,799
without root access

00:26:38,960 --> 00:26:42,799
no no no

00:26:43,440 --> 00:26:46,880
yeah i'll probably consider that's a bit

00:26:45,919 --> 00:26:50,400
risky

00:26:46,880 --> 00:26:53,360
too okay so

00:26:50,400 --> 00:26:54,320
thanks martin for your great talk so

00:26:53,360 --> 00:26:57,520
we'll move

00:26:54,320 --> 00:27:01,440
next to about the tls

00:26:57,520 --> 00:27:01,440
improvement by utaro

00:27:02,799 --> 00:27:06,320
yes let me share my screen

00:27:07,600 --> 00:27:11,200
anyways can you hear me

00:27:11,360 --> 00:27:18,320
yes okay thanks so

00:27:14,400 --> 00:27:22,799
and can you shoot can you see my screen

00:27:18,320 --> 00:27:22,799
yes okay so

00:27:22,880 --> 00:27:27,120
now let me start my presentation

00:27:27,520 --> 00:27:31,039
um hello uh i'm utah ohio cover uh

00:27:30,320 --> 00:27:32,720
thanks for

00:27:31,039 --> 00:27:34,720
giving me the opportunity to make this

00:27:32,720 --> 00:27:37,120
presentation in this workshop

00:27:34,720 --> 00:27:38,320
uh today i'm going to talk about the

00:27:37,120 --> 00:27:40,960
brief introduction

00:27:38,320 --> 00:27:42,960
of our research work it's it's pretty

00:27:40,960 --> 00:27:46,320
much different from the agenda but

00:27:42,960 --> 00:27:48,799
the present the proxies without the pain

00:27:46,320 --> 00:27:50,960
during uh during working on this work

00:27:48,799 --> 00:27:52,960
the we have came up with the good use

00:27:50,960 --> 00:27:56,480
cases of some linux kernel features

00:27:52,960 --> 00:27:59,679
like tcp repair or the ktls so

00:27:56,480 --> 00:28:03,440
let me let us introduce those as well so

00:27:59,679 --> 00:28:05,840
that would be interesting

00:28:03,440 --> 00:28:06,559
so first of all the main target of this

00:28:05,840 --> 00:28:09,279
research work

00:28:06,559 --> 00:28:11,520
is an object stress systems so the

00:28:09,279 --> 00:28:14,720
simplified architecture look like this

00:28:11,520 --> 00:28:16,960
uh it consists of the front end proxies

00:28:14,720 --> 00:28:18,799
and the background searchable the

00:28:16,960 --> 00:28:20,000
clients request their objects through

00:28:18,799 --> 00:28:23,039
the http

00:28:20,000 --> 00:28:25,279
always keep alive over persistent tcp or

00:28:23,039 --> 00:28:27,360
dls connections

00:28:25,279 --> 00:28:29,120
and front-end proxies terminates

00:28:27,360 --> 00:28:31,039
terminates the client connections

00:28:29,120 --> 00:28:32,559
and forwards the requests to the backend

00:28:31,039 --> 00:28:34,960
based on the content

00:28:32,559 --> 00:28:37,039
like object ids or any other application

00:28:34,960 --> 00:28:38,799
level information

00:28:37,039 --> 00:28:42,880
and they are also responsible for

00:28:38,799 --> 00:28:46,399
relaying the responses from the backends

00:28:42,880 --> 00:28:48,799
and encrypting it in case of the tls

00:28:46,399 --> 00:28:51,840
as a result the front-end proxies will

00:28:48,799 --> 00:28:54,000
experience the high cpu utilization

00:28:51,840 --> 00:28:55,840
the primary cause is the cost of just

00:28:54,000 --> 00:28:57,039
forwarding the large amount of the data

00:28:55,840 --> 00:28:59,600
and the encryption

00:28:57,039 --> 00:29:01,440
uh at the same time the entire cluster

00:28:59,600 --> 00:29:03,520
will end up to the load network

00:29:01,440 --> 00:29:04,080
utilization because all of the traffic

00:29:03,520 --> 00:29:05,760
you

00:29:04,080 --> 00:29:08,720
between the clients and the backends

00:29:05,760 --> 00:29:12,159
will go through the front-end process

00:29:08,720 --> 00:29:14,960
so that then that is the problem

00:29:12,159 --> 00:29:16,240
and so what we came up for eliminating

00:29:14,960 --> 00:29:19,039
this bottleneck

00:29:16,240 --> 00:29:20,240
is allow back-ends to directly respond

00:29:19,039 --> 00:29:23,760
to the clients

00:29:20,240 --> 00:29:25,919
by migrating the tcp or tls connections

00:29:23,760 --> 00:29:27,039
instead of rewriting everything on the

00:29:25,919 --> 00:29:30,240
front end

00:29:27,039 --> 00:29:32,640
so the basic mechanism

00:29:30,240 --> 00:29:33,919
is after the front end node receives the

00:29:32,640 --> 00:29:36,559
request

00:29:33,919 --> 00:29:37,279
that it extracts the tcp connection

00:29:36,559 --> 00:29:40,880
state

00:29:37,279 --> 00:29:41,760
using the tcp repair and send it to the

00:29:40,880 --> 00:29:44,559
back end

00:29:41,760 --> 00:29:46,399
together with the tls state and the http

00:29:44,559 --> 00:29:49,200
request

00:29:46,399 --> 00:29:51,360
then backend then restore the connection

00:29:49,200 --> 00:29:52,000
and configure the special programmable

00:29:51,360 --> 00:29:54,399
switch

00:29:52,000 --> 00:29:56,960
inside the network to change the loading

00:29:54,399 --> 00:29:59,039
or or the ib address

00:29:56,960 --> 00:30:00,799
then directory send back the response to

00:29:59,039 --> 00:30:02,159
the client through the migrated

00:30:00,799 --> 00:30:05,679
connection

00:30:02,159 --> 00:30:07,679
so in this mechanism the front end is no

00:30:05,679 --> 00:30:08,320
longer responsible for rewriting the

00:30:07,679 --> 00:30:10,640
response

00:30:08,320 --> 00:30:12,240
and the encryption were performed in the

00:30:10,640 --> 00:30:15,279
back end

00:30:12,240 --> 00:30:17,600
so we no longer have the bottleneck on

00:30:15,279 --> 00:30:19,440
the front end

00:30:17,600 --> 00:30:21,039
actually this is the technique known as

00:30:19,440 --> 00:30:23,120
the tcp handle

00:30:21,039 --> 00:30:25,360
and introduced in some academic paper

00:30:23,120 --> 00:30:28,559
about 20 years ago

00:30:25,360 --> 00:30:32,559
but not widely deployed yet

00:30:28,559 --> 00:30:34,960
but as of 2020 uh it is now becoming

00:30:32,559 --> 00:30:37,120
the realistic thanks to the support of

00:30:34,960 --> 00:30:39,440
tcp appear in linux

00:30:37,120 --> 00:30:41,520
and it's now becoming easy to develop

00:30:39,440 --> 00:30:42,320
the special switching logic like this in

00:30:41,520 --> 00:30:44,480
speed

00:30:42,320 --> 00:30:47,120
with the framework like the p4 or the

00:30:44,480 --> 00:30:47,120
ebpf

00:30:49,200 --> 00:30:54,159
but we have soon uh we have soon

00:30:51,919 --> 00:30:55,840
realized that the tcp handoff is too

00:30:54,159 --> 00:30:58,960
much difficult to implement

00:30:55,840 --> 00:31:01,600
correctly uh let me explain one simple

00:30:58,960 --> 00:31:03,279
example that the tcp connection corrupts

00:31:01,600 --> 00:31:06,640
because of the naive

00:31:03,279 --> 00:31:08,559
implementation so here we have a

00:31:06,640 --> 00:31:11,120
sequence diagram of the

00:31:08,559 --> 00:31:13,279
idea this is the b handle scenario the

00:31:11,120 --> 00:31:16,320
client send the get request

00:31:13,279 --> 00:31:19,120
front end uh checkpoint the tcp state

00:31:16,320 --> 00:31:20,559
send send it over the network and back

00:31:19,120 --> 00:31:23,200
and configure the switch

00:31:20,559 --> 00:31:24,559
and restore the sdp state and send back

00:31:23,200 --> 00:31:28,480
the response to the client

00:31:24,559 --> 00:31:30,640
this is the ideal scenario

00:31:28,480 --> 00:31:32,559
but for now think about the case that

00:31:30,640 --> 00:31:34,159
the client sends something to the front

00:31:32,559 --> 00:31:37,760
end during the handoff

00:31:34,159 --> 00:31:40,399
so the tcp socket on the front end is

00:31:37,760 --> 00:31:41,120
already in the repair mode but the back

00:31:40,399 --> 00:31:43,039
end

00:31:41,120 --> 00:31:44,960
doesn't restore the connection or the

00:31:43,039 --> 00:31:48,320
configure the switch yet

00:31:44,960 --> 00:31:50,080
so what will happen in here uh is the

00:31:48,320 --> 00:31:52,720
reset from the front end

00:31:50,080 --> 00:31:54,399
so this behavior is just a specification

00:31:52,720 --> 00:31:56,399
of the dcp repair but

00:31:54,399 --> 00:31:58,960
it makes sense because the other words

00:31:56,399 --> 00:32:00,240
the state of the connection will become

00:31:58,960 --> 00:32:01,840
inconsistent

00:32:00,240 --> 00:32:04,640
among the back-end and the front-end

00:32:01,840 --> 00:32:04,640
after migration

00:32:05,519 --> 00:32:10,080
so to overcome this problem that we

00:32:08,000 --> 00:32:13,519
extend the handoff protocol to

00:32:10,080 --> 00:32:14,240
lock the tcp connection by extra switch

00:32:13,519 --> 00:32:17,600
rules

00:32:14,240 --> 00:32:21,120
uh when the lock is active on the switch

00:32:17,600 --> 00:32:22,080
uh that it will block all of the packets

00:32:21,120 --> 00:32:24,399
from the client

00:32:22,080 --> 00:32:25,200
so then we can avoid the resetting

00:32:24,399 --> 00:32:28,480
problem

00:32:25,200 --> 00:32:31,120
so this is just one example

00:32:28,480 --> 00:32:32,320
uh there are many other problems like

00:32:31,120 --> 00:32:35,760
like this uh for

00:32:32,320 --> 00:32:37,840
implementing the tcp hand of quality so

00:32:35,760 --> 00:32:39,519
one of our contributions in this work is

00:32:37,840 --> 00:32:43,840
to make the solution against

00:32:39,519 --> 00:32:43,840
this kind of problems

00:32:44,960 --> 00:32:50,399
actually most of the functionalities of

00:32:47,840 --> 00:32:52,960
our system didn't require the kind of

00:32:50,399 --> 00:32:55,840
kernel modifications uh we have

00:32:52,960 --> 00:32:59,279
developed the features that they're not

00:32:55,840 --> 00:33:01,760
the complete uh circuit removal uh

00:32:59,279 --> 00:33:02,799
through event ft to deal with the time

00:33:01,760 --> 00:33:06,159
weight state

00:33:02,799 --> 00:33:09,200
but this can be implemented with

00:33:06,159 --> 00:33:10,880
kernel module but there is now one

00:33:09,200 --> 00:33:12,080
feature we couldn't achieve with the

00:33:10,880 --> 00:33:16,000
kernel module

00:33:12,080 --> 00:33:17,919
that was the ktls integration

00:33:16,000 --> 00:33:20,000
so it was not mandatory feature to

00:33:17,919 --> 00:33:22,799
support but we were interested in

00:33:20,000 --> 00:33:24,720
applying ktls to enhance the performance

00:33:22,799 --> 00:33:28,320
of our plc

00:33:24,720 --> 00:33:31,279
to apply ktls to the tcp handoff uh

00:33:28,320 --> 00:33:32,159
kdls also have to support checkpoint and

00:33:31,279 --> 00:33:34,720
restore

00:33:32,159 --> 00:33:36,960
the tls state such as sequence number or

00:33:34,720 --> 00:33:36,960
shift

00:33:37,279 --> 00:33:40,880
actually now most of our requirement was

00:33:39,679 --> 00:33:44,480
already satisfied

00:33:40,880 --> 00:33:46,320
uh we can get the current pc tls date uh

00:33:44,480 --> 00:33:49,120
from the ktls socket

00:33:46,320 --> 00:33:50,080
by get sub opt uh this works as the

00:33:49,120 --> 00:33:52,240
checkpointing

00:33:50,080 --> 00:33:53,440
and we can put the state again to the

00:33:52,240 --> 00:33:55,840
migrated socket

00:33:53,440 --> 00:33:58,240
with that sock opt uh this works as a

00:33:55,840 --> 00:34:00,640
restoring of the dls state

00:33:58,240 --> 00:34:02,640
but currently the get stock ops for the

00:34:00,640 --> 00:34:05,519
rx side is somehow missing

00:34:02,640 --> 00:34:07,679
so we have made a small patch that we

00:34:05,519 --> 00:34:10,639
would like to upstream it

00:34:07,679 --> 00:34:11,599
and now we are planning to issue the

00:34:10,639 --> 00:34:13,679
hours apart

00:34:11,599 --> 00:34:15,280
so we are very happy if someone supports

00:34:13,679 --> 00:34:17,280
this change uh

00:34:15,280 --> 00:34:18,480
and if you have uh if you have any

00:34:17,280 --> 00:34:20,960
particular concern

00:34:18,480 --> 00:34:23,280
about this feature please let us know in

00:34:20,960 --> 00:34:23,280
here

00:34:24,800 --> 00:34:30,159
so now i'll wrap up this talk that

00:34:28,000 --> 00:34:32,000
i introduced a brief overview of our

00:34:30,159 --> 00:34:34,320
research work uh present

00:34:32,000 --> 00:34:36,240
proxy with that brain without the pain

00:34:34,320 --> 00:34:39,679
so our paper we are appears

00:34:36,240 --> 00:34:40,560
obvious to the nsdi21 we have many other

00:34:39,679 --> 00:34:43,839
stuff we couldn't

00:34:40,560 --> 00:34:46,240
introduce in here uh

00:34:43,839 --> 00:34:47,040
in this paper so like how we can design

00:34:46,240 --> 00:34:49,760
the programming

00:34:47,040 --> 00:34:51,520
interface to integrate the tcp handoff

00:34:49,760 --> 00:34:53,520
to the applications

00:34:51,520 --> 00:34:54,560
optimization to minimize the hand of

00:34:53,520 --> 00:34:56,879
latency

00:34:54,560 --> 00:34:58,320
uh and performance number with veristic

00:34:56,879 --> 00:35:00,800
object storage workload

00:34:58,320 --> 00:35:02,240
and so on so if you are interested

00:35:00,800 --> 00:35:05,359
please check it out

00:35:02,240 --> 00:35:06,480
so that's all from me and i'll take a

00:35:05,359 --> 00:35:10,640
question

00:35:06,480 --> 00:35:14,240
thanks um i have a quick question

00:35:10,640 --> 00:35:16,240
that um so is the option of simply have

00:35:14,240 --> 00:35:16,960
all the back end handling the tos

00:35:16,240 --> 00:35:20,240
connections

00:35:16,960 --> 00:35:23,640
not a viable option because that sounds

00:35:20,240 --> 00:35:26,720
we can avoid all this uh complicated

00:35:23,640 --> 00:35:29,599
hijacking um just

00:35:26,720 --> 00:35:30,880
i can handle those that's nice that uh

00:35:29,599 --> 00:35:33,760
that could be the pro

00:35:30,880 --> 00:35:33,760
practical uh

00:35:33,920 --> 00:35:40,000
practical method we have in uh

00:35:36,960 --> 00:35:40,960
we have in the the real environment

00:35:40,000 --> 00:35:43,920
deployment but

00:35:40,960 --> 00:35:44,640
the benefit of this approach is we still

00:35:43,920 --> 00:35:48,240
can

00:35:44,640 --> 00:35:49,040
um still can handle the request based on

00:35:48,240 --> 00:35:53,839
the content

00:35:49,040 --> 00:35:53,839
uh for the application level information

00:35:54,000 --> 00:35:57,280
so we don't lose the benefit of the

00:35:56,240 --> 00:36:00,960
proxies but

00:35:57,280 --> 00:36:04,320
we can get the performance

00:36:00,960 --> 00:36:04,320
i see okay thank you

00:36:06,640 --> 00:36:12,240
any questions from the audience

00:36:09,920 --> 00:36:13,599
okay so the main thing about this is the

00:36:12,240 --> 00:36:15,119
presentation or the

00:36:13,599 --> 00:36:16,800
why we asked some of the presentation

00:36:15,119 --> 00:36:19,599
slots here it

00:36:16,800 --> 00:36:21,359
so the and we have some pretty good use

00:36:19,599 --> 00:36:24,000
case of tcp repair

00:36:21,359 --> 00:36:24,960
so but we have one missing thing in the

00:36:24,000 --> 00:36:26,800
car now as

00:36:24,960 --> 00:36:29,359
the uterus explained which is some get

00:36:26,800 --> 00:36:33,359
stock opt of the theater state

00:36:29,359 --> 00:36:35,280
so i really want to hear if there is any

00:36:33,359 --> 00:36:38,160
objection or problem with

00:36:35,280 --> 00:36:39,440
adding this from the get stock of the

00:36:38,160 --> 00:36:42,640
option

00:36:39,440 --> 00:36:44,720
into the main corner

00:36:42,640 --> 00:36:46,880
i think we have a question from the chat

00:36:44,720 --> 00:36:50,560
that is the architecture who is

00:36:46,880 --> 00:36:54,560
responsible for security filtration

00:36:50,560 --> 00:36:56,640
for example application layer ddos

00:36:54,560 --> 00:36:59,359
who is responsible for the security

00:36:56,640 --> 00:36:59,359
filtration

00:37:01,760 --> 00:37:08,400
so uh you mean the

00:37:05,520 --> 00:37:09,119
in the in the typical proximi focusing

00:37:08,400 --> 00:37:11,920
workload

00:37:09,119 --> 00:37:13,920
so the the front-end proxy is usually

00:37:11,920 --> 00:37:14,800
usually responsible for the security

00:37:13,920 --> 00:37:17,839
implication

00:37:14,800 --> 00:37:20,640
so that is the same same for this

00:37:17,839 --> 00:37:22,079
uh this architecture because the

00:37:20,640 --> 00:37:25,839
front-end still see

00:37:22,079 --> 00:37:30,320
the application level request uh

00:37:25,839 --> 00:37:33,839
so is that answer to your question

00:37:30,320 --> 00:37:37,599
excuse me do you hear me hello

00:37:33,839 --> 00:37:38,079
yes yes um yeah okay uh the question is

00:37:37,599 --> 00:37:41,119
that

00:37:38,079 --> 00:37:41,920
um in application there uh ddos attacks

00:37:41,119 --> 00:37:45,760
or

00:37:41,920 --> 00:37:49,200
application attacks like uh web attacks

00:37:45,760 --> 00:37:52,560
consider some injections like

00:37:49,200 --> 00:37:53,680
xss or sql injections you need to access

00:37:52,560 --> 00:37:56,960
to the

00:37:53,680 --> 00:38:00,400
body of http requests so

00:37:56,960 --> 00:38:04,000
um you need um you need to remove

00:38:00,400 --> 00:38:07,680
uh it is decrypted areas so

00:38:04,000 --> 00:38:08,880
usually if you face some application led

00:38:07,680 --> 00:38:12,960
those attacks you need to

00:38:08,880 --> 00:38:16,599
plain http and process the plain http

00:38:12,960 --> 00:38:19,200
request i'm wondering how in your

00:38:16,599 --> 00:38:19,760
architecture and who will be responsible

00:38:19,200 --> 00:38:22,880
for

00:38:19,760 --> 00:38:25,040
of the filtration it seems like web

00:38:22,880 --> 00:38:26,320
application firewalls or application

00:38:25,040 --> 00:38:28,560
layer ddos

00:38:26,320 --> 00:38:29,839
mitigation systems must move from

00:38:28,560 --> 00:38:32,560
front-end

00:38:29,839 --> 00:38:35,040
systems to back-end systems is it

00:38:32,560 --> 00:38:35,040
correct

00:38:35,200 --> 00:38:42,800
not really uh because the as we

00:38:38,480 --> 00:38:45,760
like explained in the the beginning the

00:38:42,800 --> 00:38:46,320
let's see this frequency diagram this is

00:38:45,760 --> 00:38:50,400
the

00:38:46,320 --> 00:38:53,280
id ideal state idea scenario for this

00:38:50,400 --> 00:38:55,680
architecture so the as you see the

00:38:53,280 --> 00:38:59,119
front-end still can see the get request

00:38:55,680 --> 00:39:02,960
from the client so that kind of the

00:38:59,119 --> 00:39:06,160
security mechanism can be worked in here

00:39:02,960 --> 00:39:08,640
is that answer to your question

00:39:06,160 --> 00:39:09,680
yeah yeah okay okay it's you know thank

00:39:08,640 --> 00:39:12,720
you very much

00:39:09,680 --> 00:39:16,240
thank you thank you

00:39:12,720 --> 00:39:18,720
um i think uh in order to um

00:39:16,240 --> 00:39:20,960
uh move on to make sure that we are on

00:39:18,720 --> 00:39:23,119
time i do have to uh cut the mic

00:39:20,960 --> 00:39:24,720
uh i know lars just posted a question

00:39:23,119 --> 00:39:28,160
but we're a little bit behind

00:39:24,720 --> 00:39:31,359
so please uh take it offline um

00:39:28,160 --> 00:39:33,520
over the chat thank you and i think

00:39:31,359 --> 00:39:36,000
that's a great talk uh we'll move on to

00:39:33,520 --> 00:39:39,200
our next talk which is the bbr2

00:39:36,000 --> 00:39:41,599
summary so um

00:39:39,200 --> 00:39:42,960
today uh we wanted to give a little uh

00:39:41,599 --> 00:39:45,680
overview and update

00:39:42,960 --> 00:39:46,320
on the uh work our team at google has

00:39:45,680 --> 00:39:49,839
been doing

00:39:46,320 --> 00:39:52,160
on bbr congestion control at google

00:39:49,839 --> 00:39:53,040
uh and of course this is joint work with

00:39:52,160 --> 00:39:55,359
my team

00:39:53,040 --> 00:39:57,040
uh at google including uh yuchang and

00:39:55,359 --> 00:40:00,160
sohail on this call

00:39:57,040 --> 00:40:02,960
and the team that you see here uh below

00:40:00,160 --> 00:40:02,960
next slide please

00:40:03,359 --> 00:40:07,280
so you may be asking yourself uh why are

00:40:05,520 --> 00:40:07,599
we talking about bbr did we talk about

00:40:07,280 --> 00:40:10,880
this

00:40:07,599 --> 00:40:13,680
at net dev a few years ago and indeed we

00:40:10,880 --> 00:40:15,359
we talked a bit about bbr um in the fall

00:40:13,680 --> 00:40:18,000
of 2016

00:40:15,359 --> 00:40:18,400
uh net dev uh just after we released the

00:40:18,000 --> 00:40:21,599
first

00:40:18,400 --> 00:40:23,680
version of the code upstream um but

00:40:21,599 --> 00:40:24,000
we've been working on it since then to

00:40:23,680 --> 00:40:27,040
improve

00:40:24,000 --> 00:40:28,560
things and in particular um there was an

00:40:27,040 --> 00:40:31,599
effort that we've been calling

00:40:28,560 --> 00:40:35,280
bbr v2 that started um

00:40:31,599 --> 00:40:36,000
in 2017 and the high level goals were

00:40:35,280 --> 00:40:39,359
basically

00:40:36,000 --> 00:40:41,680
in two parts um one is that

00:40:39,359 --> 00:40:43,599
in the internet we had a couple things

00:40:41,680 --> 00:40:44,880
we wanted to improve uh first we wanted

00:40:43,599 --> 00:40:48,000
to improve

00:40:44,880 --> 00:40:48,720
bbr's coexistence with aimd style

00:40:48,000 --> 00:40:52,000
protocols

00:40:48,720 --> 00:40:52,960
like reno and cubic also we wanted to

00:40:52,000 --> 00:40:55,200
improve the

00:40:52,960 --> 00:40:56,319
throughput of bbr for high-speed wi-fi

00:40:55,200 --> 00:40:59,359
links

00:40:56,319 --> 00:41:02,160
um and then we also wanted to improve

00:40:59,359 --> 00:41:02,480
bbr's performance inside data centers uh

00:41:02,160 --> 00:41:04,560
we

00:41:02,480 --> 00:41:06,160
already wanted to beat or at least match

00:41:04,560 --> 00:41:09,280
dc tcp

00:41:06,160 --> 00:41:12,160
by using initially ecn signals

00:41:09,280 --> 00:41:12,480
and then also potentially delay signals

00:41:12,160 --> 00:41:15,520
as

00:41:12,480 --> 00:41:17,119
congestion signals and you know it's

00:41:15,520 --> 00:41:18,960
kind of

00:41:17,119 --> 00:41:20,480
been an interesting journey with all of

00:41:18,960 --> 00:41:22,800
this because it's definitely

00:41:20,480 --> 00:41:23,920
pretty challenging to try to be a

00:41:22,800 --> 00:41:26,000
general purpose

00:41:23,920 --> 00:41:26,960
or all-weather congestion control that

00:41:26,000 --> 00:41:30,400
works both

00:41:26,960 --> 00:41:30,880
in the public internet and within a data

00:41:30,400 --> 00:41:33,119
center

00:41:30,880 --> 00:41:34,400
and between data centers over a high

00:41:33,119 --> 00:41:36,240
speed wind

00:41:34,400 --> 00:41:37,680
those are those environments all have

00:41:36,240 --> 00:41:40,960
their own challenges

00:41:37,680 --> 00:41:43,359
um so for bbr v1 the challenges that we

00:41:40,960 --> 00:41:46,240
were focusing on were

00:41:43,359 --> 00:41:47,760
number one avoiding buffer bloat that is

00:41:46,240 --> 00:41:50,880
keeping the queues short

00:41:47,760 --> 00:41:51,599
even in these last mile bottlenecks that

00:41:50,880 --> 00:41:55,119
might have

00:41:51,599 --> 00:41:56,720
uh seconds worth of buffer available um

00:41:55,119 --> 00:41:58,640
another thing we were focused on with

00:41:56,720 --> 00:42:01,760
version one was

00:41:58,640 --> 00:42:02,079
avoiding overreacting to small amounts

00:42:01,760 --> 00:42:05,200
of

00:42:02,079 --> 00:42:05,839
packet loss which is a kind of classic

00:42:05,200 --> 00:42:09,040
issue that

00:42:05,839 --> 00:42:10,319
that reno and cubic have another issue

00:42:09,040 --> 00:42:12,800
that we've had to work on

00:42:10,319 --> 00:42:14,079
a lot over the years is dealing with

00:42:12,800 --> 00:42:17,280
aggregation effects

00:42:14,079 --> 00:42:19,280
so a lot of link layers have

00:42:17,280 --> 00:42:20,960
various aggregation mechanisms where

00:42:19,280 --> 00:42:22,160
they build up big batches of data

00:42:20,960 --> 00:42:23,839
packets or acts

00:42:22,160 --> 00:42:26,240
and then release those all in a big

00:42:23,839 --> 00:42:29,760
burst so a lot of

00:42:26,240 --> 00:42:32,000
link layers have that kind of behavior

00:42:29,760 --> 00:42:34,480
it's particularly pronounced in wi-fi

00:42:32,000 --> 00:42:37,680
but you also see it in cellular links

00:42:34,480 --> 00:42:38,160
docsis links and high-speed um data

00:42:37,680 --> 00:42:41,280
center

00:42:38,160 --> 00:42:43,200
uh nics um and so it

00:42:41,280 --> 00:42:44,720
you know when you add all those together

00:42:43,200 --> 00:42:46,880
that's kind of most connections

00:42:44,720 --> 00:42:48,160
so it's a real issue to to uh to work

00:42:46,880 --> 00:42:50,000
with um

00:42:48,160 --> 00:42:52,400
another thing that we've been working on

00:42:50,000 --> 00:42:56,160
is is coexistence with flows

00:42:52,400 --> 00:42:57,599
on out there in the uh in networks where

00:42:56,160 --> 00:42:59,280
you know the thing about congestion

00:42:57,599 --> 00:43:00,640
control is it's a it's a difficult

00:42:59,280 --> 00:43:02,160
problem because it's sort of a

00:43:00,640 --> 00:43:04,079
distributed algorithm where you don't

00:43:02,160 --> 00:43:06,079
even know what algorithms other people

00:43:04,079 --> 00:43:07,760
are using when they're sharing the

00:43:06,079 --> 00:43:09,520
network with you you know and

00:43:07,760 --> 00:43:11,839
in case of congestion control it might

00:43:09,520 --> 00:43:13,280
be reno or cubic or bbr

00:43:11,839 --> 00:43:14,560
you just don't know so you have to have

00:43:13,280 --> 00:43:15,440
an algorithm that can handle any of

00:43:14,560 --> 00:43:18,640
those cases

00:43:15,440 --> 00:43:19,920
in some reasonable way um data centers

00:43:18,640 --> 00:43:21,359
are particularly challenging because you

00:43:19,920 --> 00:43:23,680
get these forms of

00:43:21,359 --> 00:43:24,400
of thousands or even millions of bursty

00:43:23,680 --> 00:43:28,319
connections

00:43:24,400 --> 00:43:30,079
inside tiny data center switch buffers

00:43:28,319 --> 00:43:32,160
it's always a challenge to balance cpu

00:43:30,079 --> 00:43:33,920
usage and network friendliness

00:43:32,160 --> 00:43:36,480
there's always this question do you try

00:43:33,920 --> 00:43:39,599
to build bursts in order to save cpu

00:43:36,480 --> 00:43:40,400
or do you try to build smaller clumps of

00:43:39,599 --> 00:43:42,240
packets

00:43:40,400 --> 00:43:43,599
so that you can be more friendly to the

00:43:42,240 --> 00:43:46,160
network buffers

00:43:43,599 --> 00:43:47,599
it's always a tricky trade-off of course

00:43:46,160 --> 00:43:50,319
in the public internet

00:43:47,599 --> 00:43:51,040
policing is is widely used in other

00:43:50,319 --> 00:43:53,599
weird

00:43:51,040 --> 00:43:55,359
throttling and aqm schemes and you have

00:43:53,599 --> 00:43:57,119
to have a story for those

00:43:55,359 --> 00:43:58,800
and then of course at high level there's

00:43:57,119 --> 00:44:01,440
always this question of

00:43:58,800 --> 00:44:03,119
how do you make the trade-off um when

00:44:01,440 --> 00:44:03,760
you're looking at the complexity in the

00:44:03,119 --> 00:44:05,040
code

00:44:03,760 --> 00:44:07,040
you know you would like to have the code

00:44:05,040 --> 00:44:09,359
be as simple and elegant as possible but

00:44:07,040 --> 00:44:11,200
you also need to achieve usable

00:44:09,359 --> 00:44:14,560
performance

00:44:11,200 --> 00:44:14,560
so next slide please

00:44:15,839 --> 00:44:22,319
so one of the improvements we've been

00:44:19,280 --> 00:44:25,280
making as we go from bbr v1 to b2

00:44:22,319 --> 00:44:26,560
is a very much improved coexistence with

00:44:25,280 --> 00:44:29,599
reno and cubic

00:44:26,560 --> 00:44:30,640
so the issue with bbrb1 was that if you

00:44:29,599 --> 00:44:33,200
had bbr

00:44:30,640 --> 00:44:34,640
and green or cubic flows that were

00:44:33,200 --> 00:44:36,960
sharing a bottleneck

00:44:34,640 --> 00:44:38,720
then the reno or cubic flows in some

00:44:36,960 --> 00:44:40,960
cases would get

00:44:38,720 --> 00:44:43,200
considerably lower throughput um with

00:44:40,960 --> 00:44:46,240
bbr v2 we take this approach

00:44:43,200 --> 00:44:47,119
where we explicitly adapt the time scale

00:44:46,240 --> 00:44:49,359
on which

00:44:47,119 --> 00:44:51,760
the flows are probing for bandwidth and

00:44:49,359 --> 00:44:55,359
putting more packets in the network

00:44:51,760 --> 00:44:57,760
so that the bbr flows can better coexist

00:44:55,359 --> 00:44:59,200
with the reno and cubic flows

00:44:57,760 --> 00:45:01,440
because those aren't cubic fields are

00:44:59,200 --> 00:45:03,440
very sensitive to packet loss

00:45:01,440 --> 00:45:05,200
and in particular the amount of time

00:45:03,440 --> 00:45:05,680
between the packet losses that those

00:45:05,200 --> 00:45:08,240
flows

00:45:05,680 --> 00:45:09,200
see so here you can see some examples

00:45:08,240 --> 00:45:12,160
this is with a

00:45:09,200 --> 00:45:12,960
a tool that we actually open sourced

00:45:12,160 --> 00:45:15,760
last summer

00:45:12,960 --> 00:45:16,160
called transperf where it's running a

00:45:15,760 --> 00:45:18,960
few

00:45:16,160 --> 00:45:19,440
cubic flows and a bbr flow sharing with

00:45:18,960 --> 00:45:21,440
them

00:45:19,440 --> 00:45:22,960
and you can see the bbr flow down there

00:45:21,440 --> 00:45:23,440
at the bottom in the sort of pinkish

00:45:22,960 --> 00:45:24,960
color

00:45:23,440 --> 00:45:27,440
is doing a pretty good job of sharing

00:45:24,960 --> 00:45:29,520
that bandwidth uh and also keeping

00:45:27,440 --> 00:45:30,720
the retransmit rate on the bottleneck

00:45:29,520 --> 00:45:34,079
pretty low

00:45:30,720 --> 00:45:35,040
uh next slide please another thing that

00:45:34,079 --> 00:45:37,280
we've focused on

00:45:35,040 --> 00:45:38,240
uh over the last couple years is making

00:45:37,280 --> 00:45:40,640
sure that bbr

00:45:38,240 --> 00:45:41,359
achieves uh good throughput for wi-fi

00:45:40,640 --> 00:45:43,839
links

00:45:41,359 --> 00:45:45,119
um as i mentioned before wi-fi is one of

00:45:43,839 --> 00:45:46,480
these linked technologies

00:45:45,119 --> 00:45:48,400
where you see these pretty big

00:45:46,480 --> 00:45:50,880
aggregation effects

00:45:48,400 --> 00:45:53,280
and with bbr v1 the initial release we

00:45:50,880 --> 00:45:55,440
saw that there were some cases where

00:45:53,280 --> 00:45:57,119
the minimum round trip time for a

00:45:55,440 --> 00:46:00,160
connection was very low

00:45:57,119 --> 00:46:02,720
but the bottleneck was wi-fi in those

00:46:00,160 --> 00:46:05,920
cases you could get low throughput

00:46:02,720 --> 00:46:07,920
and so we did some work on this and

00:46:05,920 --> 00:46:09,040
with bbr v2 we think we see pretty good

00:46:07,920 --> 00:46:11,839
results

00:46:09,040 --> 00:46:12,880
the basic approach is to have the flow

00:46:11,839 --> 00:46:14,400
explicitly

00:46:12,880 --> 00:46:16,400
estimate the recent degree of

00:46:14,400 --> 00:46:18,560
aggregation effects that it's seeing

00:46:16,400 --> 00:46:19,440
and then budget some extra congestion

00:46:18,560 --> 00:46:21,760
window to

00:46:19,440 --> 00:46:23,920
to allow reaching full throughput in

00:46:21,760 --> 00:46:25,760
those kind of paths with aggregation

00:46:23,920 --> 00:46:28,240
and so in our tests for example on

00:46:25,760 --> 00:46:31,280
youtube we're saying pbrv2 matches

00:46:28,240 --> 00:46:32,640
uh cubic for three throughput on users

00:46:31,280 --> 00:46:34,880
with wi-fi links

00:46:32,640 --> 00:46:36,640
um and in controlled tests we can see

00:46:34,880 --> 00:46:39,359
the same

00:46:36,640 --> 00:46:40,800
kind of results uh and that aggregation

00:46:39,359 --> 00:46:43,760
modeling code is now

00:46:40,800 --> 00:46:44,480
available in both the quick bbr v2 and

00:46:43,760 --> 00:46:47,760
in the linux

00:46:44,480 --> 00:46:52,319
bbr v2 code sorry in the linux

00:46:47,760 --> 00:46:56,319
bbr v1 code as of linux 5.1

00:46:52,319 --> 00:46:57,040
next slide please so another thing uh we

00:46:56,319 --> 00:47:00,240
worked on

00:46:57,040 --> 00:47:01,599
uh as i mentioned was was allowing bbr

00:47:00,240 --> 00:47:05,440
v2 to use

00:47:01,599 --> 00:47:09,040
ecn as a congestion signal um bbr v1

00:47:05,440 --> 00:47:13,200
does not use ecn but with v2 uh is

00:47:09,040 --> 00:47:15,920
using um dc tcp style ecn signals

00:47:13,200 --> 00:47:16,319
uh if those are available in order to

00:47:15,920 --> 00:47:18,800
have

00:47:16,319 --> 00:47:19,680
extra signaling information to help the

00:47:18,800 --> 00:47:23,040
flows keep

00:47:19,680 --> 00:47:25,920
cues nice and short so here are some

00:47:23,040 --> 00:47:26,800
just example test results showing that

00:47:25,920 --> 00:47:29,839
you know if you've got

00:47:26,800 --> 00:47:33,440
some bbr flows and you're getting

00:47:29,839 --> 00:47:36,559
um dc tcp style ecm signals

00:47:33,440 --> 00:47:38,960
at a particular queue

00:47:36,559 --> 00:47:40,160
threshold that the flows are able to

00:47:38,960 --> 00:47:42,720
keep the queues

00:47:40,160 --> 00:47:43,680
nice and short i should mention that

00:47:42,720 --> 00:47:46,160
also there's

00:47:43,680 --> 00:47:47,520
in the public internet there is some

00:47:46,160 --> 00:47:51,119
work underway

00:47:47,520 --> 00:47:53,599
on with an effort called l4s

00:47:51,119 --> 00:47:54,559
i think it's something like low latency

00:47:53,599 --> 00:47:57,119
low loss

00:47:54,559 --> 00:47:59,119
scalable throughput something like that

00:47:57,119 --> 00:48:00,400
it's basically an effort to take dc tcp

00:47:59,119 --> 00:48:02,240
style ecn

00:48:00,400 --> 00:48:04,400
and use that on the public internet and

00:48:02,240 --> 00:48:08,079
so we're also hoping that

00:48:04,400 --> 00:48:10,400
bbrb2 will be able to go ahead and use

00:48:08,079 --> 00:48:12,400
l4s style signals when those are

00:48:10,400 --> 00:48:15,599
standardized and available

00:48:12,400 --> 00:48:18,480
next slide please so

00:48:15,599 --> 00:48:20,079
here's just a quick summary um to sort

00:48:18,480 --> 00:48:22,160
of compare

00:48:20,079 --> 00:48:23,520
cubic which of course is the default

00:48:22,160 --> 00:48:25,599
congestion control

00:48:23,520 --> 00:48:27,119
for linux and also these days even for

00:48:25,599 --> 00:48:31,839
windows and apple

00:48:27,119 --> 00:48:34,400
uh oss uh to bbr v1 and bbrp2

00:48:31,839 --> 00:48:35,920
um so if we look at the the model

00:48:34,400 --> 00:48:36,640
parameters to the state machine of

00:48:35,920 --> 00:48:38,240
course

00:48:36,640 --> 00:48:40,400
cubic doesn't really think of itself as

00:48:38,240 --> 00:48:43,599
a model based congestion control

00:48:40,400 --> 00:48:47,599
but bbr v1 uh uses the bandwidth

00:48:43,599 --> 00:48:50,240
or throughput and the round trip time

00:48:47,599 --> 00:48:50,720
while bbr v2 has a richer model that

00:48:50,240 --> 00:48:53,280
also

00:48:50,720 --> 00:48:55,280
incorporates as i was mentioning the

00:48:53,280 --> 00:48:56,640
maximum recent amount of aggregation

00:48:55,280 --> 00:49:00,559
that the flow is seeing

00:48:56,640 --> 00:49:02,880
and then also the maximum safe in flight

00:49:00,559 --> 00:49:04,079
based on the congestion signals it's

00:49:02,880 --> 00:49:07,520
seeing like ecn

00:49:04,079 --> 00:49:09,520
or loss so when we look at the loss

00:49:07,520 --> 00:49:12,640
response of these algorithms

00:49:09,520 --> 00:49:14,000
so cubic has

00:49:12,640 --> 00:49:15,920
an approach where it will reduce the

00:49:14,000 --> 00:49:18,800
congestion window by 30

00:49:15,920 --> 00:49:19,839
on any round trip where there's packet

00:49:18,800 --> 00:49:22,319
loss

00:49:19,839 --> 00:49:22,960
whereas bbr v1 that sort of longer time

00:49:22,319 --> 00:49:25,200
scales

00:49:22,960 --> 00:49:26,000
tries to be lost agnostic to a high

00:49:25,200 --> 00:49:29,040
level

00:49:26,000 --> 00:49:32,000
um and by contrast bbr v2 does

00:49:29,040 --> 00:49:33,359
have um an explicit handling of loss

00:49:32,000 --> 00:49:35,200
where it tries to target

00:49:33,359 --> 00:49:36,720
making sure that the loss rate stays

00:49:35,200 --> 00:49:39,200
below uh

00:49:36,720 --> 00:49:39,760
two percent on the instantaneous basis

00:49:39,200 --> 00:49:42,880
and

00:49:39,760 --> 00:49:45,359
within ours it's much lower um

00:49:42,880 --> 00:49:45,920
so if we look at ecm response uh cubic

00:49:45,359 --> 00:49:50,000
um

00:49:45,920 --> 00:49:50,800
you can use the rc 3168 sort of classic

00:49:50,000 --> 00:49:53,359
ecm

00:49:50,800 --> 00:49:54,800
standardized about 20 years ago um bbr

00:49:53,359 --> 00:49:58,000
v1 doesn't use that

00:49:54,800 --> 00:50:01,760
bbr v2 as i mentioned can use these tcp

00:49:58,000 --> 00:50:02,960
style ecm or l4s style ecm once that's

00:50:01,760 --> 00:50:04,640
standardized

00:50:02,960 --> 00:50:06,319
um if we look at the sort of startup

00:50:04,640 --> 00:50:09,440
behavior as flows are starting up

00:50:06,319 --> 00:50:13,040
um qbik is a slow start until the rtt

00:50:09,440 --> 00:50:16,480
rises dbr v1 just looks for

00:50:13,040 --> 00:50:16,960
a throughput plateau bbr v2 uh will slow

00:50:16,480 --> 00:50:18,480
start

00:50:16,960 --> 00:50:22,160
until either the throughput flight

00:50:18,480 --> 00:50:24,800
plateaus or either econ or the loss rate

00:50:22,160 --> 00:50:27,359
goes above its its target rate uh next

00:50:24,800 --> 00:50:30,559
slide please

00:50:27,359 --> 00:50:31,839
so just to give a quick visual um

00:50:30,559 --> 00:50:34,800
overview of how these different

00:50:31,839 --> 00:50:37,839
algorithms behave so reno has this

00:50:34,800 --> 00:50:39,200
classic sawtooth behavior that you've

00:50:37,839 --> 00:50:41,359
probably seen before

00:50:39,200 --> 00:50:42,880
or there's a multiplicative decrease by

00:50:41,359 --> 00:50:44,559
50 every time

00:50:42,880 --> 00:50:46,640
there's a round trip time with loss and

00:50:44,559 --> 00:50:49,839
then there's this slow linear growth

00:50:46,640 --> 00:50:50,319
of one packet per round trip time that's

00:50:49,839 --> 00:50:52,640
added

00:50:50,319 --> 00:50:54,720
into the network um but that gives you

00:50:52,640 --> 00:50:57,200
this sort of very norm

00:50:54,720 --> 00:50:58,960
scalable growth where you need a

00:50:57,200 --> 00:51:00,559
thousand times more time

00:50:58,960 --> 00:51:02,559
to reach a thousand times higher

00:51:00,559 --> 00:51:04,640
bandwidth and that means

00:51:02,559 --> 00:51:06,079
in particular if you think about high

00:51:04,640 --> 00:51:09,520
speed lands

00:51:06,079 --> 00:51:11,440
with long rtts uh you need a long time

00:51:09,520 --> 00:51:12,960
between any losses in order to ramp back

00:51:11,440 --> 00:51:15,599
up to full utilization

00:51:12,960 --> 00:51:16,880
um in fact at 10 gigabits 100

00:51:15,599 --> 00:51:19,119
millisecond path

00:51:16,880 --> 00:51:20,960
you need an hour between packet losses

00:51:19,119 --> 00:51:23,359
to maintain full utilization

00:51:20,960 --> 00:51:24,000
which is super difficult that works out

00:51:23,359 --> 00:51:26,240
to like

00:51:24,000 --> 00:51:27,040
two times ten to the negative tenth as a

00:51:26,240 --> 00:51:30,160
long string

00:51:27,040 --> 00:51:30,960
which is just impossible to maintain on

00:51:30,160 --> 00:51:34,400
a real

00:51:30,960 --> 00:51:34,400
network next slide please

00:51:34,640 --> 00:51:40,640
so cubic tries to improve uh this uh

00:51:38,240 --> 00:51:42,480
scalability of this human growth and it

00:51:40,640 --> 00:51:45,599
does to some extent

00:51:42,480 --> 00:51:46,640
but you because it's a cubic effect you

00:51:45,599 --> 00:51:49,359
still need

00:51:46,640 --> 00:51:50,079
10 times more time to reach a thousand

00:51:49,359 --> 00:51:52,640
times higher

00:51:50,079 --> 00:51:53,680
bandwidth um so this curve that you can

00:51:52,640 --> 00:51:55,359
see here

00:51:53,680 --> 00:51:57,520
that sort of requires to quickly ramp up

00:51:55,359 --> 00:51:59,440
to the last point where you saw loss

00:51:57,520 --> 00:52:01,280
does buy you some improvements but it

00:51:59,440 --> 00:52:02,559
still has some scalability issues

00:52:01,280 --> 00:52:04,880
and in particular if we look at this

00:52:02,559 --> 00:52:07,040
benchmark of a 10 gigabit path with a

00:52:04,880 --> 00:52:10,720
100 millisecond round trip time

00:52:07,040 --> 00:52:12,000
you need 40 seconds between any losses

00:52:10,720 --> 00:52:13,920
that a flow sees

00:52:12,000 --> 00:52:16,640
which is a definite improvement over an

00:52:13,920 --> 00:52:18,480
hour but it's still a lot of

00:52:16,640 --> 00:52:20,160
time that you need to avoid losses so

00:52:18,480 --> 00:52:23,920
the last repeat has to be

00:52:20,160 --> 00:52:25,599
something like 3 to the

00:52:23,920 --> 00:52:28,160
times 10 to the negative 8 which is

00:52:25,599 --> 00:52:31,839
still infeasible for most

00:52:28,160 --> 00:52:34,160
networks next slide please

00:52:31,839 --> 00:52:34,880
so bbr has um a slightly different

00:52:34,160 --> 00:52:37,920
approach

00:52:34,880 --> 00:52:40,160
um the uh you know overall it aims to

00:52:37,920 --> 00:52:41,280
to reduce the time with the queue full

00:52:40,160 --> 00:52:45,200
uh versus

00:52:41,280 --> 00:52:47,440
um uh bbr v1 or cubic and it tries to

00:52:45,200 --> 00:52:48,480
have a sort of more scalable exponential

00:52:47,440 --> 00:52:51,119
growth

00:52:48,480 --> 00:52:52,800
so that it can use newly available

00:52:51,119 --> 00:52:56,160
bandwidth in logarithmic time

00:52:52,800 --> 00:52:58,960
instead of q group and

00:52:56,160 --> 00:53:00,559
to consider again our benchmark of a 10

00:52:58,960 --> 00:53:03,280
gigabit uh

00:53:00,559 --> 00:53:04,960
path with 100 milliseconds time um the

00:53:03,280 --> 00:53:07,119
goal here with bbrv2 is to

00:53:04,960 --> 00:53:08,160
be able to have uh up to the target

00:53:07,119 --> 00:53:10,400
amount of loss

00:53:08,160 --> 00:53:12,800
in every round trip and still be able to

00:53:10,400 --> 00:53:16,400
fully utilize that path

00:53:12,800 --> 00:53:17,359
next slide please so where are we with

00:53:16,400 --> 00:53:19,599
bbrv2

00:53:17,359 --> 00:53:20,800
um so we are in the middle of deploying

00:53:19,599 --> 00:53:24,079
it at google

00:53:20,800 --> 00:53:27,520
uh on google.com google.com and youtube

00:53:24,079 --> 00:53:28,480
uh we're running um global uh but small

00:53:27,520 --> 00:53:32,000
percentage

00:53:28,480 --> 00:53:33,520
av experiments comparing cubic bbr v1

00:53:32,000 --> 00:53:36,079
and bbr v2

00:53:33,520 --> 00:53:36,800
we're seeing reduced queuing delays

00:53:36,079 --> 00:53:38,960
versus

00:53:36,800 --> 00:53:40,319
both brb1 and cubic and we're seeing

00:53:38,960 --> 00:53:43,839
reduced packet loss

00:53:40,319 --> 00:53:46,480
versus vbrv1 um internally

00:53:43,839 --> 00:53:47,119
uh bbr v2 is currently being deployed as

00:53:46,480 --> 00:53:49,440
the default

00:53:47,119 --> 00:53:50,480
congestion control for traffic within

00:53:49,440 --> 00:53:51,680
google

00:53:50,480 --> 00:53:54,079
and of course we're continuing to

00:53:51,680 --> 00:53:57,200
iterate uh using both

00:53:54,079 --> 00:53:57,920
this production rollout um production

00:53:57,200 --> 00:54:02,160
experiments

00:53:57,920 --> 00:54:06,000
and lab tests um next slide please

00:54:02,160 --> 00:54:08,559
so uh in conclusion um the bbr v2 code

00:54:06,000 --> 00:54:10,720
is uh out there and ready for uh

00:54:08,559 --> 00:54:14,480
research experiments

00:54:10,720 --> 00:54:15,440
you can find the uh the linux tcp bbr of

00:54:14,480 --> 00:54:18,480
youtube code

00:54:15,440 --> 00:54:22,160
uh in our team's github repo

00:54:18,480 --> 00:54:23,599
at github.com google pbr

00:54:22,160 --> 00:54:25,760
and here's the particular link for the

00:54:23,599 --> 00:54:28,319
readme for the the v2 release

00:54:25,760 --> 00:54:29,440
uh it's vbrv2 is also available for

00:54:28,319 --> 00:54:30,960
quick um

00:54:29,440 --> 00:54:32,480
if you're someone who works in the quick

00:54:30,960 --> 00:54:35,760
transport protocol

00:54:32,480 --> 00:54:37,680
um and um overall we invite researchers

00:54:35,760 --> 00:54:40,640
to go ahead and

00:54:37,680 --> 00:54:41,440
check it out take it for a spin see what

00:54:40,640 --> 00:54:44,559
you see

00:54:41,440 --> 00:54:45,920
on you know and there's a mailing list

00:54:44,559 --> 00:54:49,599
for the bbr

00:54:45,920 --> 00:54:52,000
dev effort and feel free to share your

00:54:49,599 --> 00:54:53,440
ideas about test cases that you think

00:54:52,000 --> 00:54:54,319
are important or metrics that should be

00:54:53,440 --> 00:54:57,760
evaluated

00:54:54,319 --> 00:55:00,400
any test results you see any ideas or

00:54:57,760 --> 00:55:02,319
patches you have for algorithms and

00:55:00,400 --> 00:55:06,079
we're always happy to see patches or

00:55:02,319 --> 00:55:07,200
look at packet traces and i think that's

00:55:06,079 --> 00:55:10,319
it and i think we might have

00:55:07,200 --> 00:55:12,640
just one minute for q a before

00:55:10,319 --> 00:55:14,799
the next uh talk which will give uh some

00:55:12,640 --> 00:55:19,440
more experience with uh bbr

00:55:14,799 --> 00:55:22,640
v2 um so we got a question on chad

00:55:19,440 --> 00:55:25,520
on the network without ecn will bbr v2

00:55:22,640 --> 00:55:26,559
increase throughput until it hits the 2

00:55:25,520 --> 00:55:32,000
loss rate

00:55:26,559 --> 00:55:32,000
target and why is that 2 why not 0.5

00:55:32,079 --> 00:55:38,720
right um so bbr v2

00:55:35,280 --> 00:55:41,920
um when there's no ecn it will um

00:55:38,720 --> 00:55:44,559
it will either find its operating point

00:55:41,920 --> 00:55:45,119
based on its estimate of the bandwidth

00:55:44,559 --> 00:55:48,559
delay

00:55:45,119 --> 00:55:50,079
product um so if it's not seeing loss

00:55:48,559 --> 00:55:51,760
then it will basically look at the

00:55:50,079 --> 00:55:54,799
bandwidth it's achieving

00:55:51,760 --> 00:55:55,680
uh and and the uh the minimum round trip

00:55:54,799 --> 00:55:58,079
time of the path

00:55:55,680 --> 00:56:00,400
and estimate what's a reasonable amount

00:55:58,079 --> 00:56:02,400
of data to keep in the network that will

00:56:00,400 --> 00:56:04,000
allow it to achieve full throughput

00:56:02,400 --> 00:56:05,920
without building too much q

00:56:04,000 --> 00:56:07,599
so if it's not seeing loss it will use

00:56:05,920 --> 00:56:09,040
that approach which was sort of the core

00:56:07,599 --> 00:56:12,319
of bbr v1

00:56:09,040 --> 00:56:15,200
um if it does see loss then it will be

00:56:12,319 --> 00:56:16,559
um you know ensuring that as it's

00:56:15,200 --> 00:56:20,319
probing for bandwidth

00:56:16,559 --> 00:56:22,079
it makes sure that once it sees

00:56:20,319 --> 00:56:24,000
an instantaneous loss rate in a

00:56:22,079 --> 00:56:24,880
particular round trip that's at two

00:56:24,000 --> 00:56:28,000
percent or higher

00:56:24,880 --> 00:56:30,799
it goes ahead and it stops probing um

00:56:28,000 --> 00:56:32,880
and the because these flows are only

00:56:30,799 --> 00:56:34,559
probing some of the time the average

00:56:32,880 --> 00:56:36,640
loss rate tends to be much lower than

00:56:34,559 --> 00:56:37,440
that sort of instantaneous two percent

00:56:36,640 --> 00:56:40,799
number

00:56:37,440 --> 00:56:44,000
um and that target um was

00:56:40,799 --> 00:56:46,720
chosen um based on a sort of a series

00:56:44,000 --> 00:56:47,680
of trade-offs um and we are still

00:56:46,720 --> 00:56:50,160
evaluating

00:56:47,680 --> 00:56:50,720
um you know what exact value there makes

00:56:50,160 --> 00:56:52,880
sense

00:56:50,720 --> 00:56:54,720
and it's this sort of set of trade-offs

00:56:52,880 --> 00:56:57,040
about you know how well does the

00:56:54,720 --> 00:57:00,319
algorithm coexist with reno and cubic

00:56:57,040 --> 00:57:03,440
how good does it do

00:57:00,319 --> 00:57:06,559
at you fully utilizing paths

00:57:03,440 --> 00:57:09,839
that might have shallow buffers

00:57:06,559 --> 00:57:14,720
and really high speed

00:57:09,839 --> 00:57:17,839
long rtt paths with big bdps

00:57:14,720 --> 00:57:20,880
and and that sort of thing so we welcome

00:57:17,839 --> 00:57:20,880
experiments or

00:57:21,680 --> 00:57:25,599
results from people's production

00:57:23,200 --> 00:57:26,000
deployments to help inform the sort of

00:57:25,599 --> 00:57:28,559
research

00:57:26,000 --> 00:57:30,480
discussion about exactly how what uh

00:57:28,559 --> 00:57:33,680
threshold that

00:57:30,480 --> 00:57:33,680
we should be using for that

00:57:34,000 --> 00:57:37,359
okay and the next question is any idea

00:57:36,799 --> 00:57:41,280
when

00:57:37,359 --> 00:57:44,240
bbib2 will be upstreamed

00:57:41,280 --> 00:57:45,040
that's a good question it's a it's tough

00:57:44,240 --> 00:57:47,520
to guess

00:57:45,040 --> 00:57:48,400
exactly when the we the main thing is

00:57:47,520 --> 00:57:52,160
that we want to

00:57:48,400 --> 00:57:53,680
be sure that um its work bbr v2 is

00:57:52,160 --> 00:57:56,160
working well for

00:57:53,680 --> 00:57:58,000
all of the important workloads at google

00:57:56,160 --> 00:58:00,160
so that we have confidence that

00:57:58,000 --> 00:58:02,480
it's a good solid general purpose

00:58:00,160 --> 00:58:04,160
congestion control algorithm

00:58:02,480 --> 00:58:05,599
and in the meantime of course people can

00:58:04,160 --> 00:58:07,440
always

00:58:05,599 --> 00:58:09,760
download the code from github and go

00:58:07,440 --> 00:58:13,200
ahead and take it for a spin

00:58:09,760 --> 00:58:13,920
um that's i think that's uh that's what

00:58:13,200 --> 00:58:16,319
we can offer

00:58:13,920 --> 00:58:16,319
right now

00:58:17,040 --> 00:58:21,520
okay so next talk is for the alexis um

00:58:20,380 --> 00:58:24,640
[Music]

00:58:21,520 --> 00:58:27,040
evaluating bbr2 on another different

00:58:24,640 --> 00:58:27,040
network

00:58:27,920 --> 00:58:31,760
hi um my name is alexey i and today i'm

00:58:30,880 --> 00:58:35,359
gonna talk about

00:58:31,760 --> 00:58:38,160
uh evaluating bbr v2 on badge

00:58:35,359 --> 00:58:39,839
so i'm an infrastructure engineer i work

00:58:38,160 --> 00:58:41,440
at dropbox previously i worked at

00:58:39,839 --> 00:58:43,839
linkedin and yandex

00:58:41,440 --> 00:58:44,960
i was working on databases storage

00:58:43,839 --> 00:58:48,400
systems and

00:58:44,960 --> 00:58:51,599
right now on traffic slash networking

00:58:48,400 --> 00:58:52,000
so we talk a lot in our tech blog about

00:58:51,599 --> 00:58:55,520
uh

00:58:52,000 --> 00:58:58,799
all the uh optimizations that we do to

00:58:55,520 --> 00:59:01,200
our traffic stack from like lower levels

00:58:58,799 --> 00:59:02,400
on operating system driver side to a

00:59:01,200 --> 00:59:04,559
higher level of

00:59:02,400 --> 00:59:05,760
intelligent dns routing and library

00:59:04,559 --> 00:59:08,960
optimizations

00:59:05,760 --> 00:59:11,119
uh but uh our team is very small and uh

00:59:08,960 --> 00:59:12,799
we don't have a dedicated uh

00:59:11,119 --> 00:59:14,640
kernel engineers we don't even have

00:59:12,799 --> 00:59:17,040
kernel team currently

00:59:14,640 --> 00:59:18,319
uh but we really like to experiment with

00:59:17,040 --> 00:59:21,440
different types of things

00:59:18,319 --> 00:59:22,640
and b stock will be about one of these

00:59:21,440 --> 00:59:25,920
experiments

00:59:22,640 --> 00:59:28,880
so it's all started with bbr v1 uh

00:59:25,920 --> 00:59:29,680
when we tried out hot new congestion

00:59:28,880 --> 00:59:33,760
control thing

00:59:29,680 --> 00:59:36,640
back in um so

00:59:33,760 --> 00:59:38,480
our first experiment was bbr v1 showed

00:59:36,640 --> 00:59:41,280
great results we saw

00:59:38,480 --> 00:59:42,640
uh so the users that we thought were

00:59:41,280 --> 00:59:46,559
actually bottlenecked by

00:59:42,640 --> 00:59:49,119
their um internet connection

00:59:46,559 --> 00:59:50,240
actually happened to be uh bottlenecked

00:59:49,119 --> 00:59:52,480
on the

00:59:50,240 --> 00:59:53,520
our congestion control and when we

00:59:52,480 --> 00:59:57,359
rolled out uh

00:59:53,520 --> 01:00:00,079
bbrv1 which we seen great results from

00:59:57,359 --> 01:00:01,200
uh that was very interesting food for

01:00:00,079 --> 01:00:03,599
thought

01:00:01,200 --> 01:00:06,799
and eventually we rolled out bbr we want

01:00:03,599 --> 01:00:09,520
to our whole edge network

01:00:06,799 --> 01:00:11,680
over time though we saw some downsides

01:00:09,520 --> 01:00:14,640
of bbr we won

01:00:11,680 --> 01:00:14,960
mainly probably all of you know that

01:00:14,640 --> 01:00:17,119
it's

01:00:14,960 --> 01:00:18,799
not very fair to the rest of congestion

01:00:17,119 --> 01:00:21,760
control algorithms

01:00:18,799 --> 01:00:23,920
it has an insane packet loss of up to

01:00:21,760 --> 01:00:28,000
six percent we saw in our boxes

01:00:23,920 --> 01:00:28,559
um like all all the downsides started to

01:00:28,000 --> 01:00:31,359
be

01:00:28,559 --> 01:00:32,079
more and more apparent still a good

01:00:31,359 --> 01:00:35,119
benefit

01:00:32,079 --> 01:00:38,480
in performance so

01:00:35,119 --> 01:00:41,200
bbr developers also notice that so

01:00:38,480 --> 01:00:42,079
the that is a list of issues with bbr we

01:00:41,200 --> 01:00:45,160
want straight

01:00:42,079 --> 01:00:47,599
from the developers so they've

01:00:45,160 --> 01:00:49,599
identified

01:00:47,599 --> 01:00:50,880
unfairness of bbr we want they've

01:00:49,599 --> 01:00:54,400
identified

01:00:50,880 --> 01:00:58,480
its aggressiveness um also some issues

01:00:54,400 --> 01:01:00,880
in aggregated parts when there is

01:00:58,480 --> 01:01:02,160
aggregations and of course no support

01:01:00,880 --> 01:01:06,240
for ecm

01:01:02,160 --> 01:01:09,040
so they started to address that

01:01:06,240 --> 01:01:10,000
before we jump into the experimental

01:01:09,040 --> 01:01:13,359
results couple of

01:01:10,000 --> 01:01:15,760
caveats about uh all of you

01:01:13,359 --> 01:01:16,640
who own production systems just all of

01:01:15,760 --> 01:01:19,200
you probably know but

01:01:16,640 --> 01:01:21,680
just want to repeat it uh please upgrade

01:01:19,200 --> 01:01:24,720
your kernels if you

01:01:21,680 --> 01:01:25,680
uh support any uh operate any production

01:01:24,720 --> 01:01:28,000
uh networks

01:01:25,680 --> 01:01:29,680
like new kernel usually gives a lot of

01:01:28,000 --> 01:01:30,640
uh performance so the best thing for

01:01:29,680 --> 01:01:32,799
performance

01:01:30,640 --> 01:01:34,400
uh is usually upgrading your kernel of

01:01:32,799 --> 01:01:35,200
course there are occasional regressions

01:01:34,400 --> 01:01:37,040
there are

01:01:35,200 --> 01:01:38,799
new vulnerabilities that are getting

01:01:37,040 --> 01:01:40,960
mitigated with near kernels and

01:01:38,799 --> 01:01:42,000
occasional slowdowns from networking

01:01:40,960 --> 01:01:45,839
perspective

01:01:42,000 --> 01:01:47,520
um each new kernel is generally faster

01:01:45,839 --> 01:01:49,040
than the previous one here is an example

01:01:47,520 --> 01:01:52,640
how

01:01:49,040 --> 01:01:54,640
uh just for upgrading from our main

01:01:52,640 --> 01:01:57,599
production kernel 415

01:01:54,640 --> 01:01:58,960
uh to 5.3 test kernel that we used for

01:01:57,599 --> 01:02:02,319
bbr we do test

01:01:58,960 --> 01:02:02,960
we got around 15 performance across 15

01:02:02,319 --> 01:02:06,079
01:02:02,960 --> 01:02:08,160
performance degrees just for free uh

01:02:06,079 --> 01:02:09,520
but that most likely related to

01:02:08,160 --> 01:02:12,319
improvements to bdrv1 and

01:02:09,520 --> 01:02:14,160
uh like uh they've added uh

01:02:12,319 --> 01:02:14,720
optimizations for wi-fi and aggregation

01:02:14,160 --> 01:02:17,280
but

01:02:14,720 --> 01:02:18,160
uh regardless each new kernel initial

01:02:17,280 --> 01:02:21,280
art does

01:02:18,160 --> 01:02:23,680
usually performs better second

01:02:21,280 --> 01:02:24,400
upgrading your user space so if you

01:02:23,680 --> 01:02:27,039
operate

01:02:24,400 --> 01:02:27,760
again any product any large scale

01:02:27,039 --> 01:02:32,240
networks

01:02:27,760 --> 01:02:33,119
having new user space helps a lot with

01:02:32,240 --> 01:02:36,799
troubleshooting

01:02:33,119 --> 01:02:39,920
for example uh here is a ipr

01:02:36,799 --> 01:02:43,039
ip route that comes with ubuntu16

01:02:39,920 --> 01:02:45,760
and that is a relatively new ip route

01:02:43,039 --> 01:02:46,960
just notice that like all the new fields

01:02:45,760 --> 01:02:50,799
that we have

01:02:46,960 --> 01:02:52,640
um it's essential for troubleshooting

01:02:50,799 --> 01:02:54,559
uh especially any performance issues so

01:02:52,640 --> 01:02:56,400
in the uip route for example we see all

01:02:54,559 --> 01:02:59,039
the internal bbr data

01:02:56,400 --> 01:03:00,079
uh like basin gain and estimated arc

01:02:59,039 --> 01:03:02,960
mean rtt

01:03:00,079 --> 01:03:04,559
uh to worry very very useful things like

01:03:02,960 --> 01:03:06,240
how much time you spend being

01:03:04,559 --> 01:03:07,680
receiving the limited send window

01:03:06,240 --> 01:03:11,680
limited um

01:03:07,680 --> 01:03:14,000
like that is uh generally

01:03:11,680 --> 01:03:16,160
a very useful tools for you to be able

01:03:14,000 --> 01:03:19,599
to troubleshoot any issues with

01:03:16,160 --> 01:03:21,680
this performance and third one uh furqan

01:03:19,599 --> 01:03:22,319
scheduler please use first human

01:03:21,680 --> 01:03:25,440
scheduler

01:03:22,319 --> 01:03:27,039
if you can um it's not for fair queue in

01:03:25,440 --> 01:03:29,359
itself it's mostly for

01:03:27,039 --> 01:03:30,079
pacing pacing is very important in the

01:03:29,359 --> 01:03:33,599
modern network

01:03:30,079 --> 01:03:35,520
especially high speed um and

01:03:33,599 --> 01:03:37,599
where is there a lot of symmetry between

01:03:35,520 --> 01:03:42,400
your speeds and your client speeds

01:03:37,599 --> 01:03:45,839
uh we he and even within the backbone so

01:03:42,400 --> 01:03:48,160
there is great talk from uh van jacobson

01:03:45,839 --> 01:03:49,359
about beyond as fast as possible that

01:03:48,160 --> 01:03:51,839
sending data

01:03:49,359 --> 01:03:53,359
as fast as possible is not actually the

01:03:51,839 --> 01:03:55,359
uh in theory

01:03:53,359 --> 01:03:57,119
is not actually the best thing you can

01:03:55,359 --> 01:04:00,240
do but in practice it

01:03:57,119 --> 01:04:03,359
also like

01:04:00,240 --> 01:04:05,280
there there isn't any girls for example

01:04:03,359 --> 01:04:07,920
from our production network when we

01:04:05,280 --> 01:04:09,760
uh saw a lot like packet drops of a

01:04:07,920 --> 01:04:12,480
large problem in our networks and

01:04:09,760 --> 01:04:13,920
uh our network engineering actually

01:04:12,480 --> 01:04:15,839
wanted to replace some of the shallow

01:04:13,920 --> 01:04:18,960
buffered switches with the more

01:04:15,839 --> 01:04:20,160
um advanced ones uh with uh larger

01:04:18,960 --> 01:04:23,280
buffer spaces but

01:04:20,160 --> 01:04:23,760
just by rolling out fq we get rid of all

01:04:23,280 --> 01:04:26,240
these

01:04:23,760 --> 01:04:26,799
drops even on the backbone when there is

01:04:26,240 --> 01:04:30,000
not

01:04:26,799 --> 01:04:33,200
as much uh speedy symmetry

01:04:30,000 --> 01:04:35,680
anyway um use fq fq

01:04:33,200 --> 01:04:37,520
can actually improve your packet loss

01:04:35,680 --> 01:04:41,039
quite a bit

01:04:37,520 --> 01:04:42,400
uh and before we jump into experimental

01:04:41,039 --> 01:04:44,720
results of bbr v2

01:04:42,400 --> 01:04:45,680
a couple of disclaimers and the test

01:04:44,720 --> 01:04:48,640
setup

01:04:45,680 --> 01:04:50,799
so first of all it's not a low latency

01:04:48,640 --> 01:04:51,599
experiment all our flows that we were

01:04:50,799 --> 01:04:55,119
examining

01:04:51,599 --> 01:04:57,599
were around were filtered by

01:04:55,119 --> 01:04:59,119
one megabyte of transfer data so this is

01:04:57,599 --> 01:05:02,079
high throughput bulk

01:04:59,119 --> 01:05:04,240
bulk flows uh second data is heavily

01:05:02,079 --> 01:05:05,359
aggregated there are no like single tcp

01:05:04,240 --> 01:05:07,760
dump slash tcp

01:05:05,359 --> 01:05:09,440
trace drill downs uh we have millions

01:05:07,760 --> 01:05:12,799
and millions of connections

01:05:09,440 --> 01:05:14,880
um so uh data is heavily aggregated

01:05:12,799 --> 01:05:16,319
and of course that is the real

01:05:14,880 --> 01:05:20,160
production test so

01:05:16,319 --> 01:05:23,119
all the imperfections of traffic uh be

01:05:20,160 --> 01:05:25,039
like duplicated packets real heavy

01:05:23,119 --> 01:05:27,039
reordering passwords high packet loss

01:05:25,039 --> 01:05:29,039
everything will be present in that uh

01:05:27,039 --> 01:05:30,559
in that data set it's a real production

01:05:29,039 --> 01:05:33,839
traffic

01:05:30,559 --> 01:05:35,599
uh even more specifically we used one

01:05:33,839 --> 01:05:38,400
single pop in tokyo

01:05:35,599 --> 01:05:40,720
four boxes in it uh three one boxes with

01:05:38,400 --> 01:05:43,599
all the kernel all bbr we want

01:05:40,720 --> 01:05:46,079
and three boxes with the new kernel uh

01:05:43,599 --> 01:05:48,880
with cubic bbr v1 and bbr return

01:05:46,079 --> 01:05:51,760
again only bulk flows uh and we were

01:05:48,880 --> 01:05:55,119
looking at sampled ss data

01:05:51,760 --> 01:05:59,119
and sampled nginx logs

01:05:55,119 --> 01:06:02,079
um yep that's pretty much all of it

01:05:59,119 --> 01:06:03,359
uh we'll be covering only new kernels

01:06:02,079 --> 01:06:05,359
from now on

01:06:03,359 --> 01:06:06,559
comparison between all the new kernel

01:06:05,359 --> 01:06:08,799
bbr v1 code

01:06:06,559 --> 01:06:10,960
i've showed previously so now we will be

01:06:08,799 --> 01:06:14,000
only looking at 5.3

01:06:10,960 --> 01:06:15,039
um yeah so sorry the kernel is a bit too

01:06:14,000 --> 01:06:17,440
old at that point

01:06:15,039 --> 01:06:20,240
the presentation was for the first um

01:06:17,440 --> 01:06:20,240
first version of

01:06:20,559 --> 01:06:23,599
netcon conference

01:06:23,680 --> 01:06:28,720
okay so and again before we jump into

01:06:27,039 --> 01:06:31,440
the

01:06:28,720 --> 01:06:32,960
practical results a bit of theory so

01:06:31,440 --> 01:06:33,839
that slide is straight from neil's

01:06:32,960 --> 01:06:37,200
presentation

01:06:33,839 --> 01:06:40,640
from um ietf um

01:06:37,200 --> 01:06:44,640
it shows pbr design principles with

01:06:40,640 --> 01:06:46,400
all the new stuff from bbrv2 in bold

01:06:44,640 --> 01:06:48,240
you can see that uh there is a lot of

01:06:46,400 --> 01:06:52,160
stuff but the general idea

01:06:48,240 --> 01:06:55,280
is uh twofold first make it fair

01:06:52,160 --> 01:06:57,359
uh make it more fair fairer to other

01:06:55,280 --> 01:07:00,880
congestion control protocols

01:06:57,359 --> 01:07:04,079
uh and uh put some

01:07:00,880 --> 01:07:07,280
notion of packet loss into the model so

01:07:04,079 --> 01:07:10,079
uh do uh react quickly

01:07:07,280 --> 01:07:11,200
to changing conditions so the these are

01:07:10,079 --> 01:07:14,400
two things and

01:07:11,200 --> 01:07:15,520
we'll see um we'll see how it actually

01:07:14,400 --> 01:07:17,119
affects um

01:07:15,520 --> 01:07:19,039
experimental results here here is a

01:07:17,119 --> 01:07:22,160
comparison from um

01:07:19,039 --> 01:07:24,319
between bbr v1 and vbrv2

01:07:22,160 --> 01:07:26,160
in a more table format so we can see

01:07:24,319 --> 01:07:29,839
that there are no new additions

01:07:26,160 --> 01:07:29,839
to the uh

01:07:29,920 --> 01:07:36,079
network model itself there is also

01:07:33,200 --> 01:07:37,680
finally the explicit loss targets and

01:07:36,079 --> 01:07:39,680
early exit from startup

01:07:37,680 --> 01:07:42,559
if these loss targets are either

01:07:39,680 --> 01:07:45,280
explicit or implicit are broken

01:07:42,559 --> 01:07:46,559
okay and ecn we did not test this n in

01:07:45,280 --> 01:07:49,599
our test but

01:07:46,559 --> 01:07:50,480
it seems like bbr v2 can be a drop-in

01:07:49,599 --> 01:07:54,559
replacement

01:07:50,480 --> 01:07:58,400
for uh dc tcp but who knows

01:07:54,559 --> 01:07:59,599
okay now uh we jump into all the graphs

01:07:58,400 --> 01:08:02,160
so first we're gonna

01:07:59,599 --> 01:08:03,359
go over properties that we see on the

01:08:02,160 --> 01:08:06,000
link and then we

01:08:03,359 --> 01:08:07,200
will jump into what it how does it

01:08:06,000 --> 01:08:10,000
actually affect

01:08:07,200 --> 01:08:11,200
the throughput uh throughput that we see

01:08:10,000 --> 01:08:13,520
on the link

01:08:11,200 --> 01:08:15,599
so first thing that we do even without

01:08:13,520 --> 01:08:19,199
like uh

01:08:15,599 --> 01:08:22,400
first thing that we noticed even without

01:08:19,199 --> 01:08:24,960
actually looking at this per connection

01:08:22,400 --> 01:08:28,080
stars is that when we deploy bbr

01:08:24,960 --> 01:08:28,560
uh to code we see way lower packet loss

01:08:28,080 --> 01:08:32,000
so

01:08:28,560 --> 01:08:35,440
immediately packet loss drops a lot uh

01:08:32,000 --> 01:08:38,000
it's still higher than cubic uh but i'd

01:08:35,440 --> 01:08:42,000
assume that that is expected

01:08:38,000 --> 01:08:46,319
uh if we look deeper into our

01:08:42,000 --> 01:08:49,759
uh retransmission percentage

01:08:46,319 --> 01:08:50,960
we uh on per connection level so we see

01:08:49,759 --> 01:08:54,400
that

01:08:50,960 --> 01:08:55,120
it generally bbr v2 looks way better on

01:08:54,400 --> 01:08:59,440
pdf

01:08:55,120 --> 01:09:01,359
graphs the only caveat is there are some

01:08:59,440 --> 01:09:02,960
connections that are that have higher

01:09:01,359 --> 01:09:06,960
packet loss at 60

01:09:02,960 --> 01:09:09,279
sometimes 80 or even 90. so there is

01:09:06,960 --> 01:09:12,319
something is definitely wrong there

01:09:09,279 --> 01:09:14,960
beside that a small percentage of

01:09:12,319 --> 01:09:16,159
connections with really bad packet loss

01:09:14,960 --> 01:09:19,520
everything else

01:09:16,159 --> 01:09:21,600
looks uh looks really good i would

01:09:19,520 --> 01:09:22,640
suspect some kind of bug there but i'm

01:09:21,600 --> 01:09:26,080
not sure

01:09:22,640 --> 01:09:28,880
uh if we compare to cubic um

01:09:26,080 --> 01:09:32,159
bbr v2 has still higher connect uh

01:09:28,880 --> 01:09:32,159
packet loss on um

01:09:32,480 --> 01:09:38,159
on per connection basis uh still i would

01:09:35,920 --> 01:09:41,199
assume that is expected given

01:09:38,159 --> 01:09:43,759
its tolerance uh to some packet loss so

01:09:41,199 --> 01:09:44,719
since it has that packet loss uh target

01:09:43,759 --> 01:09:46,799
i would assume

01:09:44,719 --> 01:09:48,960
it is fine for it to have higher packet

01:09:46,799 --> 01:09:50,400
loss except for that 60 percent case and

01:09:48,960 --> 01:09:53,600
above

01:09:50,400 --> 01:09:54,640
uh if we look at uh heat map of that we

01:09:53,600 --> 01:09:58,719
can see that the

01:09:54,640 --> 01:09:59,920
vbr um we we won on the top v2 on the

01:09:58,719 --> 01:10:02,960
bottom it's more

01:09:59,920 --> 01:10:04,960
v2 is more squashed along all rtt so

01:10:02,960 --> 01:10:08,320
it's not

01:10:04,960 --> 01:10:10,000
it's very fairly distributed so there is

01:10:08,320 --> 01:10:13,120
no uh correlation between like

01:10:10,000 --> 01:10:16,159
packet loss and um

01:10:13,120 --> 01:10:18,640
and rtt then

01:10:16,159 --> 01:10:20,960
we can look into in flight packets and

01:10:18,640 --> 01:10:24,960
we see that bbr v1 has

01:10:20,960 --> 01:10:24,960
way less packets in flight

01:10:25,199 --> 01:10:29,280
that is actually one of the properties

01:10:28,880 --> 01:10:32,960
of

01:10:29,280 --> 01:10:36,080
bbr v2 model so they have max

01:10:32,960 --> 01:10:37,440
max in flight now and that in theory

01:10:36,080 --> 01:10:39,120
that should be

01:10:37,440 --> 01:10:41,120
like that and in private practice

01:10:39,120 --> 01:10:44,239
actually prove that

01:10:41,120 --> 01:10:44,640
what's even more interesting that bbr v2

01:10:44,239 --> 01:10:47,679
has

01:10:44,640 --> 01:10:49,120
less packets in flight than cubic which

01:10:47,679 --> 01:10:52,320
makes it better which is

01:10:49,120 --> 01:10:57,440
quite uh quite interesting

01:10:52,320 --> 01:11:01,280
so less buffer bloat from bbr v2

01:10:57,440 --> 01:11:04,320
uh if we plot uh rtt versus in flight

01:11:01,280 --> 01:11:04,960
in bbr v2 we can see that general upward

01:11:04,320 --> 01:11:08,080
trend with

01:11:04,960 --> 01:11:12,239
more like data on the wire depending on

01:11:08,080 --> 01:11:15,920
rtt uh and uh

01:11:12,239 --> 01:11:16,960
in we v2 we can actually see it more

01:11:15,920 --> 01:11:19,280
down to earth

01:11:16,960 --> 01:11:20,800
uh there is one line uh strange line

01:11:19,280 --> 01:11:24,159
that um

01:11:20,800 --> 01:11:26,400
dependency between uh rtt and uh

01:11:24,159 --> 01:11:27,520
in flight segments which which looks

01:11:26,400 --> 01:11:29,760
weird besides that

01:11:27,520 --> 01:11:30,800
it's all squished normally distributed

01:11:29,760 --> 01:11:33,840
and if we

01:11:30,800 --> 01:11:35,840
compare it to uh cubic even

01:11:33,840 --> 01:11:36,960
we can see that it's still better it's

01:11:35,840 --> 01:11:39,520
more uh

01:11:36,960 --> 01:11:40,719
more down to earth so uh even compared

01:11:39,520 --> 01:11:44,640
to cubic in flight

01:11:40,719 --> 01:11:46,239
looks way better on rtt wise we did not

01:11:44,640 --> 01:11:48,400
look in rtt too much because

01:11:46,239 --> 01:11:50,480
these are about closed but since we

01:11:48,400 --> 01:11:54,080
still collected that data

01:11:50,480 --> 01:11:56,000
we know that bbr v2 rtgs are better than

01:11:54,080 --> 01:11:59,199
bbr v1

01:11:56,000 --> 01:12:02,640
based on pdfs and but still worse than

01:11:59,199 --> 01:12:04,480
cubic for some reason that may be

01:12:02,640 --> 01:12:06,960
some bug in our code because we see

01:12:04,480 --> 01:12:08,400
lower in flight but for some reason rtts

01:12:06,960 --> 01:12:10,800
are

01:12:08,400 --> 01:12:12,880
uh are higher from our from our

01:12:10,800 --> 01:12:14,960
perspective

01:12:12,880 --> 01:12:17,120
oh one one more interesting graph is

01:12:14,960 --> 01:12:18,640
about receive window limiter that's in

01:12:17,120 --> 01:12:22,239
that new start from new

01:12:18,640 --> 01:12:25,360
uh ip route version ss version we

01:12:22,239 --> 01:12:29,679
see that dbr v2 is

01:12:25,360 --> 01:12:33,040
way way less less often

01:12:29,679 --> 01:12:35,440
window uh limited receiving delimited

01:12:33,040 --> 01:12:37,440
which is also great that means we burst

01:12:35,440 --> 01:12:40,480
less on the wire

01:12:37,440 --> 01:12:41,040
and leave some headroom which which is

01:12:40,480 --> 01:12:43,679
good

01:12:41,040 --> 01:12:44,400
a it even uh less received window

01:12:43,679 --> 01:12:47,840
limited

01:12:44,400 --> 01:12:51,760
than uh cubic which is again uh

01:12:47,840 --> 01:12:55,199
quite quite good with burst less uh

01:12:51,760 --> 01:12:57,520
we buffer below it less

01:12:55,199 --> 01:12:59,360
so all of these theoretical properties

01:12:57,520 --> 01:13:01,120
lead to some interesting practical

01:12:59,360 --> 01:13:03,840
results in terms of bandwidth so

01:13:01,120 --> 01:13:04,320
what we saw is all different facets of

01:13:03,840 --> 01:13:06,080
the

01:13:04,320 --> 01:13:07,840
of the traffic and now we can actually

01:13:06,080 --> 01:13:10,960
see what what the bandwidth

01:13:07,840 --> 01:13:14,800
looks like and bandwidth wise uh

01:13:10,960 --> 01:13:18,960
bbr v2 is slower uh and we can see that

01:13:14,800 --> 01:13:23,040
like the it is

01:13:18,960 --> 01:13:23,040
it is slower in the the

01:13:23,280 --> 01:13:30,080
we can see that it is slower in that um

01:13:27,040 --> 01:13:32,400
high uh like low low performance range

01:13:30,080 --> 01:13:36,560
with on low speeds it is uh

01:13:32,400 --> 01:13:38,880
slower than bbr v2 so and on higher

01:13:36,560 --> 01:13:41,199
connection speeds it is actually quite

01:13:38,880 --> 01:13:44,840
close to bvr v2

01:13:41,199 --> 01:13:47,679
uh we have hard cut off here at around

01:13:44,840 --> 01:13:51,440
one i think one

01:13:47,679 --> 01:13:52,000
135 or so or so but the the further down

01:13:51,440 --> 01:13:54,080
the line

01:13:52,000 --> 01:13:56,400
data gets more noisy but we're very in

01:13:54,080 --> 01:13:58,880
line so the on higher speeds

01:13:56,400 --> 01:13:59,520
bbr v2 actually very comparable to bbr

01:13:58,880 --> 01:14:02,320
everyone

01:13:59,520 --> 01:14:04,080
on lower speeds uh speeds where

01:14:02,320 --> 01:14:07,920
congestion is more

01:14:04,080 --> 01:14:09,920
uh likely uh we we see that bbr v2 is

01:14:07,920 --> 01:14:13,199
slower

01:14:09,920 --> 01:14:13,679
okay and compared to cubic compared to

01:14:13,199 --> 01:14:17,040
cubic

01:14:13,679 --> 01:14:19,520
bbr v2 is faster and you can see that

01:14:17,040 --> 01:14:22,000
on the further side of the graph the the

01:14:19,520 --> 01:14:25,120
faster connection of user becomes

01:14:22,000 --> 01:14:26,880
the more is the difference so the more

01:14:25,120 --> 01:14:28,880
the faster is users connection the more

01:14:26,880 --> 01:14:31,679
benefit you get from bbr v2

01:14:28,880 --> 01:14:32,480
without it like increasing congestion uh

01:14:31,679 --> 01:14:35,120
that one

01:14:32,480 --> 01:14:36,560
so as you can see it just gets further

01:14:35,120 --> 01:14:39,360
and further apart

01:14:36,560 --> 01:14:41,520
as connection speed grows and if you

01:14:39,360 --> 01:14:42,560
look at good put from my engine x point

01:14:41,520 --> 01:14:44,960
of view

01:14:42,560 --> 01:14:46,400
these are based on nginx logs we can see

01:14:44,960 --> 01:14:48,990
that on lower speeds

01:14:46,400 --> 01:14:50,400
bbr v2 is way closer to

01:14:48,990 --> 01:14:53,440
[Music]

01:14:50,400 --> 01:14:56,640
uh cubic but on higher

01:14:53,440 --> 01:14:59,199
connections it's actually closer to uh

01:14:56,640 --> 01:15:00,800
bbr v2 performance so uh you can see

01:14:59,199 --> 01:15:03,679
here that balance that

01:15:00,800 --> 01:15:05,760
it became more cubic friendly uh and at

01:15:03,679 --> 01:15:06,960
the same time it's way faster for users

01:15:05,760 --> 01:15:10,400
who can actually

01:15:06,960 --> 01:15:14,480
uh use that spin um

01:15:10,400 --> 01:15:16,080
as far for conclusions uh the that is

01:15:14,480 --> 01:15:20,080
initial slide with all the issues

01:15:16,080 --> 01:15:20,080
highlighted for bbm v1

01:15:23,199 --> 01:15:26,640
we can actually prove that most of them

01:15:25,520 --> 01:15:29,600
were

01:15:26,640 --> 01:15:30,239
fixed uh except for ecn that we did not

01:15:29,600 --> 01:15:33,360
test

01:15:30,239 --> 01:15:34,159
everything else uh from throughput

01:15:33,360 --> 01:15:38,000
perspective

01:15:34,159 --> 01:15:41,360
it does look way uh fairer um

01:15:38,000 --> 01:15:44,560
and packet loss is reduced and the

01:15:41,360 --> 01:15:47,600
throughput variation is reduced so

01:15:44,560 --> 01:15:48,239
um all of that our experimental results

01:15:47,600 --> 01:15:50,960
show that

01:15:48,239 --> 01:15:52,239
bandwidth is comparable to cubic on for

01:15:50,960 --> 01:15:55,280
users that have lower

01:15:52,239 --> 01:15:58,480
internet speeds and way better

01:15:55,280 --> 01:15:59,760
and comparable to bbr v1 for users with

01:15:58,480 --> 01:16:02,400
higher internet speeds

01:15:59,760 --> 01:16:04,080
lower packet loss than the bbr v1 still

01:16:02,400 --> 01:16:07,360
a bit higher than cubic

01:16:04,080 --> 01:16:11,120
uh that data in flight is comparable

01:16:07,360 --> 01:16:14,800
uh uh sorry so slightly lower than cubic

01:16:11,120 --> 01:16:18,400
and way lower than bbr v1

01:16:14,800 --> 01:16:21,520
uh yeah our better rtt fairness

01:16:18,400 --> 01:16:22,239
and uh slightly lower rtt that bbr1

01:16:21,520 --> 01:16:24,480
overall

01:16:22,239 --> 01:16:27,760
across all of that i can say that from

01:16:24,480 --> 01:16:30,800
our test results it seems like bbr v2

01:16:27,760 --> 01:16:31,520
is drop-in replacement for bbr v1 which

01:16:30,800 --> 01:16:34,800
is better

01:16:31,520 --> 01:16:38,640
in all all the cases

01:16:34,800 --> 01:16:40,719
the only word thing that we saw is that

01:16:38,640 --> 01:16:43,840
that small amount of connections with

01:16:40,719 --> 01:16:43,840
more than um

01:16:44,400 --> 01:16:49,199
60 packet loss except that it's

01:16:47,360 --> 01:16:51,199
literally dropping replacement that is

01:16:49,199 --> 01:16:55,360
better in all the respects

01:16:51,199 --> 01:16:58,640
uh it may actually be even considered as

01:16:55,360 --> 01:17:00,560
a relatively good drop in replacement

01:16:58,640 --> 01:17:02,719
for cubic especially if you have a high

01:17:00,560 --> 01:17:06,719
performance clients they will benefit

01:17:02,719 --> 01:17:10,159
from bbr v1 compared to qubit um

01:17:06,719 --> 01:17:12,239
and uh depending on how easy and test go

01:17:10,159 --> 01:17:13,679
goes in our data center if we ever get

01:17:12,239 --> 01:17:16,800
to do that

01:17:13,679 --> 01:17:19,520
maybe we can prove that bbr v1 can be

01:17:16,800 --> 01:17:20,000
a drop-in replacement for dctc but

01:17:19,520 --> 01:17:23,040
that's

01:17:20,000 --> 01:17:23,920
that's way further out so recently i was

01:17:23,040 --> 01:17:27,520
troubleshooting

01:17:23,920 --> 01:17:30,560
uh tcp performance in windows

01:17:27,520 --> 01:17:32,640
and very small note uh for

01:17:30,560 --> 01:17:33,920
all of you software engineers out here

01:17:32,640 --> 01:17:36,880
like windows not

01:17:33,920 --> 01:17:37,280
shell trace is way ahead what we have in

01:17:36,880 --> 01:17:40,480
uh

01:17:37,280 --> 01:17:43,040
in linux world so nutshell trace notch

01:17:40,480 --> 01:17:44,239
trace for those who don't know it's

01:17:43,040 --> 01:17:47,760
actually uh

01:17:44,239 --> 01:17:51,280
it collects data like tcp dump um

01:17:47,760 --> 01:17:53,280
but also puts in all data from all the

01:17:51,280 --> 01:17:55,440
various subsystems in the kernel so you

01:17:53,280 --> 01:17:57,520
not only have packet trace you also have

01:17:55,440 --> 01:17:58,640
all the events that happen inside the

01:17:57,520 --> 01:18:01,760
kernel you have

01:17:58,640 --> 01:18:04,800
um i i think async subsystem

01:18:01,760 --> 01:18:07,280
events like uh iocp events for example

01:18:04,800 --> 01:18:09,040
you have socket events like rights and

01:18:07,280 --> 01:18:12,000
reads you have uh

01:18:09,040 --> 01:18:12,320
data from what you usually get from tcp

01:18:12,000 --> 01:18:15,679
for

01:18:12,320 --> 01:18:18,320
net link about congestion windows um

01:18:15,679 --> 01:18:20,560
and properties of of the tcp connection

01:18:18,320 --> 01:18:23,760
you you get reordering you get

01:18:20,560 --> 01:18:26,880
uh some memory subsystem events like

01:18:23,760 --> 01:18:29,280
buffers etc etc like uh all the things

01:18:26,880 --> 01:18:29,679
in one place you run natural trace you

01:18:29,280 --> 01:18:31,840
get

01:18:29,679 --> 01:18:32,880
all the things that you need for

01:18:31,840 --> 01:18:36,000
troubleshooting

01:18:32,880 --> 01:18:38,320
while in linux world uh when you

01:18:36,000 --> 01:18:40,000
collect just plain tcp dump it is the

01:18:38,320 --> 01:18:43,199
same tcp dump that was uh

01:18:40,000 --> 01:18:46,560
20 years ago uh what you get is

01:18:43,199 --> 01:18:47,120
uh data on the wire where you need to

01:18:46,560 --> 01:18:48,880
infer

01:18:47,120 --> 01:18:50,239
what actually happened into in the

01:18:48,880 --> 01:18:54,320
kernel

01:18:50,239 --> 01:18:56,960
and uh like based on sex based on

01:18:54,320 --> 01:18:57,360
some other uh some other weird things

01:18:56,960 --> 01:18:59,760
like

01:18:57,360 --> 01:19:00,719
window advertisement changes like there

01:18:59,760 --> 01:19:04,320
is

01:19:00,719 --> 01:19:06,719
there is not enough uh debug data

01:19:04,320 --> 01:19:06,719
in there

01:19:07,360 --> 01:19:10,480
if we see in bbr we took out they added

01:19:09,760 --> 01:19:13,760
a lot of

01:19:10,480 --> 01:19:14,960
debugging code uh just for ease of

01:19:13,760 --> 01:19:18,239
troubleshooting

01:19:14,960 --> 01:19:19,280
um nowadays with ebpf we can actually

01:19:18,239 --> 01:19:22,159
get

01:19:19,280 --> 01:19:23,440
a utility we can create as a community

01:19:22,159 --> 01:19:26,320
utility that can

01:19:23,440 --> 01:19:28,239
pipe all the relevant data from all the

01:19:26,320 --> 01:19:31,840
subsystems in the kernel

01:19:28,239 --> 01:19:34,320
um inside the trace as for example

01:19:31,840 --> 01:19:36,239
comments anticipated

01:19:34,320 --> 01:19:37,440
that would be quite useful for any

01:19:36,239 --> 01:19:40,640
performance

01:19:37,440 --> 01:19:42,800
related uh either research like we did

01:19:40,640 --> 01:19:44,560
or uh any performance related

01:19:42,800 --> 01:19:45,360
troubleshooting so please for those of

01:19:44,560 --> 01:19:48,800
you who

01:19:45,360 --> 01:19:50,320
never tried like touching windows please

01:19:48,800 --> 01:19:54,400
look into nutshell trace

01:19:50,320 --> 01:19:56,400
and message analyzer they are great

01:19:54,400 --> 01:19:59,760
okay so uh that's pretty much all of it

01:19:56,400 --> 01:19:59,760
uh now qna

01:20:01,280 --> 01:20:04,560
okay thanks for the great talk okay so i

01:20:04,239 --> 01:20:07,040
guess

01:20:04,560 --> 01:20:10,000
we have someone ready to ask the

01:20:07,040 --> 01:20:10,000
question go ahead

01:20:13,920 --> 01:20:19,360
um then i'll read out this question on

01:20:18,000 --> 01:20:22,960
the chat

01:20:19,360 --> 01:20:25,120
um we

01:20:22,960 --> 01:20:26,400
on the higher rtt then cubic can you

01:20:25,120 --> 01:20:29,440
please elaborate

01:20:26,400 --> 01:20:32,480
what do you mean by bug in our code

01:20:29,440 --> 01:20:35,520
is that the stats gathering code in this

01:20:32,480 --> 01:20:39,600
test okay

01:20:35,520 --> 01:20:43,679
yeah it is likely the bug in our code

01:20:39,600 --> 01:20:43,679
i was recently reviewing it um

01:20:44,480 --> 01:20:48,639
ss basically provides two sets of

01:20:46,400 --> 01:20:53,679
information for bbr it has

01:20:48,639 --> 01:20:57,120
mrtt from bbr and mrtk from

01:20:53,679 --> 01:20:58,400
a generic mean rtt so i think uh we

01:20:57,120 --> 01:21:00,960
misplaced these two

01:20:58,400 --> 01:21:01,679
and for cubic because it doesn't provide

01:21:00,960 --> 01:21:04,719
uh

01:21:01,679 --> 01:21:08,159
mrtt from bbr cells because it's cubic

01:21:04,719 --> 01:21:09,600
um we actually compare two different

01:21:08,159 --> 01:21:12,800
sets of data that may

01:21:09,600 --> 01:21:14,639
uh that may uh actually result in that

01:21:12,800 --> 01:21:16,560
kind of inconsistency

01:21:14,639 --> 01:21:18,480
so it actually ascribes to that part

01:21:16,560 --> 01:21:21,920
from the presentation about comparing

01:21:18,480 --> 01:21:22,800
uh specifically dbr to bbr1 to bbrv2

01:21:21,920 --> 01:21:26,320
comparison

01:21:22,800 --> 01:21:28,639
is right bbr uh versus cubic may be

01:21:26,320 --> 01:21:30,480
a bug in our code just because we use

01:21:28,639 --> 01:21:32,639
two different sets of data

01:21:30,480 --> 01:21:33,920
and i haven't looked at how they are

01:21:32,639 --> 01:21:37,679
implemented in kernel

01:21:33,920 --> 01:21:38,480
how does bbr estimate uh mrtt in its

01:21:37,679 --> 01:21:41,679
code and how

01:21:38,480 --> 01:21:42,639
uh the last remaining question on your

01:21:41,679 --> 01:21:44,880
talk

01:21:42,639 --> 01:21:47,920
was that when you were running the

01:21:44,880 --> 01:21:50,080
experiment let's say for bbr v2

01:21:47,920 --> 01:21:51,199
uh was the other traffic still using

01:21:50,080 --> 01:21:53,199
cubic so

01:21:51,199 --> 01:21:56,960
maybe you are not you are seeing an

01:21:53,199 --> 01:22:00,080
effect of like the bbr v2 versus bvr

01:21:56,960 --> 01:22:01,520
uh and cubic competition i think that

01:22:00,080 --> 01:22:04,639
was the intention of the

01:22:01,520 --> 01:22:08,320
question okay so in our tasks

01:22:04,639 --> 01:22:09,040
uh bottleneck is uh way outside of our

01:22:08,320 --> 01:22:11,280
network

01:22:09,040 --> 01:22:12,080
bottleneck usually is a network

01:22:11,280 --> 01:22:14,320
equipment

01:22:12,080 --> 01:22:15,760
or the police or slash shaper on the

01:22:14,320 --> 01:22:18,960
client side

01:22:15,760 --> 01:22:21,440
uh or closer to client so

01:22:18,960 --> 01:22:22,400
the reason there will be some

01:22:21,440 --> 01:22:25,760
competition

01:22:22,400 --> 01:22:28,159
uh depending on users um usage between

01:22:25,760 --> 01:22:31,360
bbr v1 bbr v2

01:22:28,159 --> 01:22:36,000
and cubic and

01:22:31,360 --> 01:22:38,000
on these links that's true we just

01:22:36,000 --> 01:22:39,440
don't know what what is the proportion

01:22:38,000 --> 01:22:42,800
depending on what

01:22:39,440 --> 01:22:43,920
site's user uses um there will be some

01:22:42,800 --> 01:22:47,199
competition so of course

01:22:43,920 --> 01:22:49,360
us change and congestion control um

01:22:47,199 --> 01:22:51,360
changes the mix of that competition just

01:22:49,360 --> 01:22:53,280
like if google changes

01:22:51,360 --> 01:22:55,360
congestion control on youtube that

01:22:53,280 --> 01:22:57,199
changes the mix of that competition on

01:22:55,360 --> 01:22:59,440
the bottleneck

01:22:57,199 --> 01:23:01,280
which is way closer to the client on our

01:22:59,440 --> 01:23:03,520
networks we don't uh

01:23:01,280 --> 01:23:05,280
since we are not using dbrv2 internally

01:23:03,520 --> 01:23:08,320
inside our networks we

01:23:05,280 --> 01:23:10,880
we cannot say what is happening when

01:23:08,320 --> 01:23:13,199
uh these uh tcp congestion control

01:23:10,880 --> 01:23:16,400
actually compete directly on our network

01:23:13,199 --> 01:23:19,679
in known proportions at least

01:23:16,400 --> 01:23:23,120
okay great and then um i just remembered

01:23:19,679 --> 01:23:27,199
another question uh before uh

01:23:23,120 --> 01:23:30,639
the outage um the

01:23:27,199 --> 01:23:34,320
i think the the data that on the receive

01:23:30,639 --> 01:23:37,679
limited um that we are seeing bbr v2

01:23:34,320 --> 01:23:40,639
uh has lower receive limited uh cases

01:23:37,679 --> 01:23:43,280
probably because it has lower in flight

01:23:40,639 --> 01:23:43,280
in general

01:23:46,840 --> 01:23:52,400
mm-hmm

01:23:49,600 --> 01:23:54,400
that that is true that that that that

01:23:52,400 --> 01:23:56,820
theory correlates uh

01:23:54,400 --> 01:23:57,920
very well with um uh

01:23:56,820 --> 01:23:59,840
[Music]

01:23:57,920 --> 01:24:01,360
with the data though at the same time

01:23:59,840 --> 01:24:04,800
like just having

01:24:01,360 --> 01:24:07,120
lower received window limit does not

01:24:04,800 --> 01:24:08,639
uh mean much by itself like if you're

01:24:07,120 --> 01:24:10,239
zero percent receive

01:24:08,639 --> 01:24:11,760
window limited you're probably doing

01:24:10,239 --> 01:24:14,880
something wrong so

01:24:11,760 --> 01:24:14,880
um just by

01:24:15,040 --> 01:24:18,960
just by saying that it is lower um

01:24:17,920 --> 01:24:20,960
doesn't say much

01:24:18,960 --> 01:24:22,159
in the meantime if it's the full graph

01:24:20,960 --> 01:24:25,520
is kind of

01:24:22,159 --> 01:24:30,320
lower than it is probably a a good thing

01:24:25,520 --> 01:24:33,520
um yeah it means less buffer blood

01:24:30,320 --> 01:24:37,199
yep okay great

01:24:33,520 --> 01:24:40,080
um i think last question um clarify

01:24:37,199 --> 01:24:40,639
clarifying question when you say cubic

01:24:40,080 --> 01:24:45,920
is this

01:24:40,639 --> 01:24:48,480
cubic plus p50 fast or cubic plus fq

01:24:45,920 --> 01:24:49,040
oh yeah uh that that's a very good one

01:24:48,480 --> 01:24:53,040
yes

01:24:49,040 --> 01:24:56,400
everywhere we use uh cubic plus at q

01:24:53,040 --> 01:24:59,600
so fq is spacing uh so that is a very

01:24:56,400 --> 01:25:02,639
um i would say

01:24:59,600 --> 01:25:06,400
even specifically it's cubic

01:25:02,639 --> 01:25:09,920
with fq with spacing without the

01:25:06,400 --> 01:25:12,480
i think high start packet train uh

01:25:09,920 --> 01:25:15,440
heuristic i think that's that's a full

01:25:12,480 --> 01:25:15,440
technical definition

01:25:17,679 --> 01:25:24,560
okay sounds good it's glad to see that

01:25:20,880 --> 01:25:27,280
fq is used in any kind any congestion

01:25:24,560 --> 01:25:29,520
control i think

01:25:27,280 --> 01:25:31,280
yeah one small node the the the thing

01:25:29,520 --> 01:25:34,719
that i've mentioned about

01:25:31,280 --> 01:25:36,639
switches running out of um of buffer

01:25:34,719 --> 01:25:38,239
space and producing drops even on our

01:25:36,639 --> 01:25:40,159
like uh hd network

01:25:38,239 --> 01:25:41,360
like top of the rack switches for age

01:25:40,159 --> 01:25:43,280
rights uh

01:25:41,360 --> 01:25:46,000
the problem i described when we rolled

01:25:43,280 --> 01:25:49,199
out fq and these packet drop

01:25:46,000 --> 01:25:50,639
drops disappeared um basically the first

01:25:49,199 --> 01:25:52,639
hop of our network

01:25:50,639 --> 01:25:55,520
uh it happened even with cubic so it

01:25:52,639 --> 01:25:58,159
wasn't bbr v1 or bbrv 2 test 7

01:25:55,520 --> 01:25:59,920
it was preparation for initial bbr v1

01:25:58,159 --> 01:26:02,639
rollout when we first rolled out

01:25:59,920 --> 01:26:03,440
um basin everywhere and then we started

01:26:02,639 --> 01:26:06,639
rolling out

01:26:03,440 --> 01:26:10,560
the bbr v1 itself

01:26:06,639 --> 01:26:11,600
so even with uh cubic uh fq is very very

01:26:10,560 --> 01:26:14,719
useful

01:26:11,600 --> 01:26:17,840
um it does uh or facing

01:26:14,719 --> 01:26:17,840
up queue is very very useful

01:26:18,560 --> 01:26:22,880
okay i agree sounds good um alright so i

01:26:21,840 --> 01:26:24,719
think for that

01:26:22,880 --> 01:26:26,159
thank you for your talk and sorry about

01:26:24,719 --> 01:26:28,960
the outage

01:26:26,159 --> 01:26:30,639
we'll move to our open mic uh

01:26:28,960 --> 01:26:34,239
discussions

01:26:30,639 --> 01:26:34,239
um next slide please

01:26:35,679 --> 01:26:42,960
um so here we uh chairs propose like uh

01:26:39,679 --> 01:26:45,920
two topics uh that we can uh potentially

01:26:42,960 --> 01:26:46,639
discuss and happy for anyone to if you

01:26:45,920 --> 01:26:49,120
have a

01:26:46,639 --> 01:26:50,159
topic you would like to cover uh please

01:26:49,120 --> 01:26:53,040
uh

01:26:50,159 --> 01:26:53,040
tim in uh

01:26:53,280 --> 01:26:57,440
um chime in right now and

01:26:59,199 --> 01:27:02,320
and then uh you can post this on the

01:27:01,440 --> 01:27:05,679
group chat

01:27:02,320 --> 01:27:09,760
uh as well um

01:27:05,679 --> 01:27:12,800
so um one topic that we have been

01:27:09,760 --> 01:27:14,960
sort of uh looking into uh is that the

01:27:12,800 --> 01:27:18,239
cloud vm networking

01:27:14,960 --> 01:27:20,159
performance issues uh

01:27:18,239 --> 01:27:21,840
i think the first one is maybe also

01:27:20,159 --> 01:27:25,040
related to alex's

01:27:21,840 --> 01:27:28,080
comments on linux tcp uh

01:27:25,040 --> 01:27:31,120
debugging that it has very low

01:27:28,080 --> 01:27:34,719
visibilities uh for a cloud

01:27:31,120 --> 01:27:37,440
operator are hosting a lot of this vm

01:27:34,719 --> 01:27:37,920
services and when they have networking

01:27:37,440 --> 01:27:40,639
when the

01:27:37,920 --> 01:27:41,840
customers have networking issues right

01:27:40,639 --> 01:27:44,800
generally

01:27:41,840 --> 01:27:46,320
they don't have much visibility even if

01:27:44,800 --> 01:27:49,040
they want to debug

01:27:46,320 --> 01:27:49,679
on their own um just by running on the

01:27:49,040 --> 01:27:53,760
vms

01:27:49,679 --> 01:27:56,960
um and the tools that we have

01:27:53,760 --> 01:27:58,639
are not very mature uh there and then

01:27:56,960 --> 01:28:00,639
now when you add another layer of

01:27:58,639 --> 01:28:02,320
hypervisor you just make things even

01:28:00,639 --> 01:28:05,120
harder

01:28:02,320 --> 01:28:05,120
to diagnose

01:28:05,360 --> 01:28:11,920
another thing that we have looked at is

01:28:08,880 --> 01:28:14,960
called an rptx back pressure

01:28:11,920 --> 01:28:18,000
this has been fixed upstream um

01:28:14,960 --> 01:28:21,199
the problem is

01:28:18,000 --> 01:28:22,480
that in the older version of uh linux

01:28:21,199 --> 01:28:25,600
kernel which a lot of

01:28:22,480 --> 01:28:28,239
cloud users still use that

01:28:25,600 --> 01:28:30,239
when they send the packet into the uh

01:28:28,239 --> 01:28:32,639
virtual nick

01:28:30,239 --> 01:28:33,679
and the packet will be marked as you

01:28:32,639 --> 01:28:37,120
know

01:28:33,679 --> 01:28:40,239
been released by the nic right so

01:28:37,120 --> 01:28:40,639
from tcp point of view the packet has

01:28:40,239 --> 01:28:43,840
been

01:28:40,639 --> 01:28:44,239
delivered into the network and when you

01:28:43,840 --> 01:28:47,440
run

01:28:44,239 --> 01:28:50,000
a basically a loss-based

01:28:47,440 --> 01:28:52,159
purely loss-based congestion control

01:28:50,000 --> 01:28:57,440
what it can cause is that the

01:28:52,159 --> 01:28:59,760
tcp would get into this sort of wrong

01:28:57,440 --> 01:29:01,040
impression that oh the packet is being

01:28:59,760 --> 01:29:03,920
released by the nic

01:29:01,040 --> 01:29:05,040
so quickly that i should shuffle more

01:29:03,920 --> 01:29:06,960
and

01:29:05,040 --> 01:29:08,960
potentially it could build a big buffer

01:29:06,960 --> 01:29:11,760
bloat inside the hypervisor

01:29:08,960 --> 01:29:12,960
until the hypervisor says you know no

01:29:11,760 --> 01:29:15,440
more packet

01:29:12,960 --> 01:29:18,159
i can chew up and then there will be a

01:29:15,440 --> 01:29:21,120
massive packet loss

01:29:18,159 --> 01:29:22,239
and then that would then percolate into

01:29:21,120 --> 01:29:24,800
the customers

01:29:22,239 --> 01:29:25,600
and they will see this massive losses

01:29:24,800 --> 01:29:27,920
which

01:29:25,600 --> 01:29:29,120
again back to the issue one they will

01:29:27,920 --> 01:29:31,840
struggle to

01:29:29,120 --> 01:29:33,040
figure out what's going on so this has

01:29:31,840 --> 01:29:36,239
been sort of fixed

01:29:33,040 --> 01:29:36,639
by uh some recent changes so that the

01:29:36,239 --> 01:29:40,400
nick

01:29:36,639 --> 01:29:43,520
will not just orphan the skb right away

01:29:40,400 --> 01:29:46,320
and but the issue is that

01:29:43,520 --> 01:29:48,480
some of these upstream changes right um

01:29:46,320 --> 01:29:51,679
they are slowly being deployed

01:29:48,480 --> 01:29:53,040
by or adopted by the customers so they

01:29:51,679 --> 01:29:56,480
continue to see

01:29:53,040 --> 01:29:59,679
this case some of those

01:29:56,480 --> 01:30:01,679
are kind of like obvious which is uh for

01:29:59,679 --> 01:30:04,880
example

01:30:01,679 --> 01:30:06,080
our upstream linux stacks still use the

01:30:04,880 --> 01:30:09,679
200 millisecond

01:30:06,080 --> 01:30:10,719
min rto um or like they have a very

01:30:09,679 --> 01:30:14,159
conservative

01:30:10,719 --> 01:30:17,199
say receiver memory uh configuration

01:30:14,159 --> 01:30:19,679
so that in general uh

01:30:17,199 --> 01:30:21,280
if you two vms are attacking just inside

01:30:19,679 --> 01:30:25,120
one cloud zone

01:30:21,280 --> 01:30:28,800
this rto is just too conservative

01:30:25,120 --> 01:30:32,239
and i think um work like martin's um

01:30:28,800 --> 01:30:36,480
which allows us to sort of negotiate

01:30:32,239 --> 01:30:38,560
a lower rto if we trusted that

01:30:36,480 --> 01:30:39,679
you know or sort of we kind of foreknown

01:30:38,560 --> 01:30:42,719
the rtt

01:30:39,679 --> 01:30:45,360
is much shorter will greatly help

01:30:42,719 --> 01:30:46,960
in this case but again some of these

01:30:45,360 --> 01:30:50,159
default settings are just

01:30:46,960 --> 01:30:51,520
not quite adequate and when the cloud

01:30:50,159 --> 01:30:54,800
users when they use

01:30:51,520 --> 01:30:57,920
linux um they don't really

01:30:54,800 --> 01:31:00,480
know or what to

01:30:57,920 --> 01:31:02,320
sort of optimize this kind of settings

01:31:00,480 --> 01:31:03,280
they just run it as the you know the

01:31:02,320 --> 01:31:05,600
default

01:31:03,280 --> 01:31:08,000
right and then they struggle to debug

01:31:05,600 --> 01:31:11,199
like okay why can i reach

01:31:08,000 --> 01:31:13,360
more than 100 megabits per second on

01:31:11,199 --> 01:31:16,639
your fast backbone network

01:31:13,360 --> 01:31:17,600
well because your receive memory is says

01:31:16,639 --> 01:31:20,320
too low

01:31:17,600 --> 01:31:20,719
uh so that becomes your bottleneck and

01:31:20,320 --> 01:31:22,960
which

01:31:20,719 --> 01:31:24,960
if you use the ss2 or you'll see that

01:31:22,960 --> 01:31:27,199
you are always on win limited

01:31:24,960 --> 01:31:29,120
but again this kind of two words are

01:31:27,199 --> 01:31:34,320
generally not aware by

01:31:29,120 --> 01:31:38,000
people sometimes even the the cloud

01:31:34,320 --> 01:31:39,600
engineers so and then the lastly what

01:31:38,000 --> 01:31:41,840
have seen is that

01:31:39,600 --> 01:31:42,639
things like ecn which has proven to be

01:31:41,840 --> 01:31:45,920
very useful

01:31:42,639 --> 01:31:46,880
in the data center environment may not

01:31:45,920 --> 01:31:49,360
be available

01:31:46,880 --> 01:31:50,080
to these cloud users when they run the

01:31:49,360 --> 01:31:52,639
vms

01:31:50,080 --> 01:31:54,320
because it all goes into some kind of

01:31:52,639 --> 01:31:56,719
encapsulation

01:31:54,320 --> 01:31:58,000
and you kind of have to make sure that

01:31:56,719 --> 01:32:00,800
the in-cap headers

01:31:58,000 --> 01:32:01,760
ecl marks will get sort of copied or

01:32:00,800 --> 01:32:04,639
mirrored

01:32:01,760 --> 01:32:05,120
into the internal packets um this kind

01:32:04,639 --> 01:32:07,920
of thing

01:32:05,120 --> 01:32:09,440
also the flow label changes right like i

01:32:07,920 --> 01:32:12,719
think

01:32:09,440 --> 01:32:14,480
larry um and some facebook developer

01:32:12,719 --> 01:32:16,159
check in some really nice features so

01:32:14,480 --> 01:32:17,520
that when tcp time out it will change

01:32:16,159 --> 01:32:19,520
the flow labels

01:32:17,520 --> 01:32:21,520
but this is all happening at the

01:32:19,520 --> 01:32:22,080
internal header so you need to uh sort

01:32:21,520 --> 01:32:24,960
of

01:32:22,080 --> 01:32:26,320
put that into the external header um in

01:32:24,960 --> 01:32:28,800
order for this to take

01:32:26,320 --> 01:32:30,159
effect so these are just some of the

01:32:28,800 --> 01:32:31,760
general problems

01:32:30,159 --> 01:32:34,719
uh we have been seeing some of these

01:32:31,760 --> 01:32:37,040
just require very simple tunings

01:32:34,719 --> 01:32:37,920
uh some of them probably need a like a

01:32:37,040 --> 01:32:41,199
bigger change

01:32:37,920 --> 01:32:44,320
for example how can we make

01:32:41,199 --> 01:32:45,760
cloud vms visibility a lot better and

01:32:44,320 --> 01:32:50,320
then available for

01:32:45,760 --> 01:32:50,320
a cloud operator to monitor

01:32:50,880 --> 01:32:54,880
um so that's one topic we can happy to

01:32:54,080 --> 01:32:57,920
brainstorm

01:32:54,880 --> 01:32:59,440
um another one we have been seeing uh

01:32:57,920 --> 01:33:03,360
and by my account

01:32:59,440 --> 01:33:06,560
um every three months um

01:33:03,360 --> 01:33:08,800
there are um there is always

01:33:06,560 --> 01:33:09,760
every every about every quarter i'll be

01:33:08,800 --> 01:33:13,440
asked that

01:33:09,760 --> 01:33:15,360
hey can tcp deal with extreme reordering

01:33:13,440 --> 01:33:17,199
and usually my question is so what are

01:33:15,360 --> 01:33:18,800
you trying to do is i would like to

01:33:17,199 --> 01:33:23,600
spray the packet

01:33:18,800 --> 01:33:26,719
over this um um data center fabric or

01:33:23,600 --> 01:33:28,800
you know over even the internet uh we

01:33:26,719 --> 01:33:31,760
would like to reroute them as much

01:33:28,800 --> 01:33:34,239
as possible in order to for whatever

01:33:31,760 --> 01:33:36,239
routing reasons or to build a high speed

01:33:34,239 --> 01:33:39,440
network

01:33:36,239 --> 01:33:41,679
and their concern is always that i heard

01:33:39,440 --> 01:33:42,320
tcp doesn't deal with reordering very

01:33:41,679 --> 01:33:45,760
well

01:33:42,320 --> 01:33:50,000
um and they are right um the gro

01:33:45,760 --> 01:33:52,560
becomes uh the bottleneck um

01:33:50,000 --> 01:33:53,040
um because of the aggregation won't take

01:33:52,560 --> 01:33:56,159
a good

01:33:53,040 --> 01:33:57,760
effect uh when you are getting packets

01:33:56,159 --> 01:33:59,840
out of order

01:33:57,760 --> 01:34:01,120
and then uh there are also tricks like

01:33:59,840 --> 01:34:06,639
okay the street you back

01:34:01,120 --> 01:34:06,639
may not work well um so i'm seeing like

01:34:06,800 --> 01:34:14,159
a lot of uh nice um comments

01:34:10,400 --> 01:34:16,239
um eric mentioned that uh flow variable

01:34:14,159 --> 01:34:17,520
are not available for cloud users as you

01:34:16,239 --> 01:34:19,760
used before

01:34:17,520 --> 01:34:20,639
that's true i'm saying that even if you

01:34:19,760 --> 01:34:25,120
using

01:34:20,639 --> 01:34:29,840
v6 um it still takes some time to

01:34:25,120 --> 01:34:33,199
get all these in-cap changes on the

01:34:29,840 --> 01:34:37,520
um on the the hypervisor

01:34:33,199 --> 01:34:38,960
um uh people feel free to chime in i

01:34:37,520 --> 01:34:41,040
feel i'm the only one talking

01:34:38,960 --> 01:34:43,440
uh tom made another comment i don't

01:34:41,040 --> 01:34:46,560
understand the flow label problem

01:34:43,440 --> 01:34:48,719
wouldn't we just set the outer header

01:34:46,560 --> 01:34:52,560
label based on the inner header yes

01:34:48,719 --> 01:34:55,679
it's not a problem per se um

01:34:52,560 --> 01:34:57,679
meaning that these are all can be done

01:34:55,679 --> 01:35:00,800
by changing the switches

01:34:57,679 --> 01:35:02,080
by changing the hypervisors i'm just

01:35:00,800 --> 01:35:05,760
saying that

01:35:02,080 --> 01:35:08,800
when people run linux vms

01:35:05,760 --> 01:35:09,520
they would expect that okay i heard

01:35:08,800 --> 01:35:12,719
linux

01:35:09,520 --> 01:35:16,159
tcp has this and that capabilities

01:35:12,719 --> 01:35:17,199
they may not realize that it may not be

01:35:16,159 --> 01:35:21,440
used

01:35:17,199 --> 01:35:25,600
or enabled um in the cloud environment

01:35:21,440 --> 01:35:28,639
compared to just running it um in

01:35:25,600 --> 01:35:30,880
without the cloud layer so well we

01:35:28,639 --> 01:35:33,040
cloud trying to make it sort of

01:35:30,880 --> 01:35:34,239
transparent right you can run the vm

01:35:33,040 --> 01:35:37,440
just like you're running it

01:35:34,239 --> 01:35:39,679
as a native host there are tricky parts

01:35:37,440 --> 01:35:43,840
like this which are not visible to the

01:35:39,679 --> 01:35:43,840
uh the users easily

01:35:44,560 --> 01:35:48,480
um kind of make a couple comments on

01:35:46,560 --> 01:35:50,239
that so

01:35:48,480 --> 01:35:52,400
i think what you said is true we

01:35:50,239 --> 01:35:55,440
definitely want the

01:35:52,400 --> 01:35:56,880
um the under overlay network to be

01:35:55,440 --> 01:35:59,360
transparent

01:35:56,880 --> 01:36:00,719
but what i'm missing here is is why we

01:35:59,360 --> 01:36:04,639
don't already have this

01:36:00,719 --> 01:36:08,080
so a user sends a packet and a vm

01:36:04,639 --> 01:36:10,000
that goes into the hypervisor or device

01:36:08,080 --> 01:36:13,840
and if we're encapsulating that in an

01:36:10,000 --> 01:36:17,119
ipv6 or in a udp encapsulation

01:36:13,840 --> 01:36:20,080
i don't see much difficulty in setting

01:36:17,119 --> 01:36:21,440
either the flow label or the udp source

01:36:20,080 --> 01:36:23,840
port

01:36:21,440 --> 01:36:25,520
by hashing the packet that that the user

01:36:23,840 --> 01:36:26,480
is sending i mean that's my assumption

01:36:25,520 --> 01:36:27,840
is that how it

01:36:26,480 --> 01:36:30,560
would typically work we definitely do

01:36:27,840 --> 01:36:31,760
that in the stack i think in the vm case

01:36:30,560 --> 01:36:35,040
it's just a matter

01:36:31,760 --> 01:36:38,239
of the the device or the entity that's

01:36:35,040 --> 01:36:39,040
encapsulating would would take that same

01:36:38,239 --> 01:36:40,960
process so

01:36:39,040 --> 01:36:42,239
i mean it's just a matter of computing a

01:36:40,960 --> 01:36:45,360
hash really

01:36:42,239 --> 01:36:47,119
on whatever the the user's packet is so

01:36:45,360 --> 01:36:49,280
i'm still kind of missing what the issue

01:36:47,119 --> 01:36:49,280
is

01:36:49,360 --> 01:36:53,119
um i think the issue the issue i was

01:36:51,679 --> 01:36:57,600
highlighting was that this

01:36:53,119 --> 01:37:00,639
is generally invisible to the users

01:36:57,600 --> 01:37:03,040
uh when they um trying to use this

01:37:00,639 --> 01:37:05,679
feature or assume that

01:37:03,040 --> 01:37:06,880
things like flow label will be used or

01:37:05,679 --> 01:37:09,440
like ecn

01:37:06,880 --> 01:37:10,400
for example there will be like users who

01:37:09,440 --> 01:37:14,000
would use

01:37:10,400 --> 01:37:15,760
dc-tcp and um

01:37:14,000 --> 01:37:18,239
congestion control they would set the

01:37:15,760 --> 01:37:19,920
condition control to use dc-tcp

01:37:18,239 --> 01:37:22,320
and they they would expect okay i'm

01:37:19,920 --> 01:37:26,320
enjoying the dc tcp performance

01:37:22,320 --> 01:37:28,880
but if the incap does not support them

01:37:26,320 --> 01:37:30,000
obviously you are just running the plano

01:37:28,880 --> 01:37:32,159
reno

01:37:30,000 --> 01:37:34,320
um so it's really like the the

01:37:32,159 --> 01:37:37,760
visibility issue there is not like

01:37:34,320 --> 01:37:40,960
technically this is not a feasible issue

01:37:37,760 --> 01:37:42,960
right so i hope is that clear well no

01:37:40,960 --> 01:37:44,960
i'm still kind of missing it because

01:37:42,960 --> 01:37:46,560
even when we encapsulate say an ip

01:37:44,960 --> 01:37:48,880
packet

01:37:46,560 --> 01:37:50,000
isn't there a process to copy the ecm

01:37:48,880 --> 01:37:53,760
bits disk or

01:37:50,000 --> 01:37:57,440
bits um into the in the flow label into

01:37:53,760 --> 01:37:57,440
the outer header in some fashions

01:37:57,760 --> 01:38:01,440
so it sounds like so we don't want users

01:38:00,159 --> 01:38:03,840
to have visibility

01:38:01,440 --> 01:38:04,560
to the overlay i agree with that but if

01:38:03,840 --> 01:38:08,880
the user

01:38:04,560 --> 01:38:11,840
set uh set qos or ecn

01:38:08,880 --> 01:38:12,880
in in their in their ip header why

01:38:11,840 --> 01:38:14,800
aren't we just

01:38:12,880 --> 01:38:16,239
um moving that information to the outer

01:38:14,800 --> 01:38:17,360
right header i guess that's my basic

01:38:16,239 --> 01:38:20,719
question

01:38:17,360 --> 01:38:21,199
yeah so i i'll give a counter example

01:38:20,719 --> 01:38:24,639
and then

01:38:21,199 --> 01:38:28,080
a user will very happily set all their

01:38:24,639 --> 01:38:30,480
uh dscp2 af4

01:38:28,080 --> 01:38:31,679
to enjoy the highest class if they know

01:38:30,480 --> 01:38:34,400
that oh

01:38:31,679 --> 01:38:37,040
the um the cloud operator will just

01:38:34,400 --> 01:38:39,679
happily copy the dhcp bits

01:38:37,040 --> 01:38:41,280
um right uh same thing with ecn same

01:38:39,679 --> 01:38:44,000
thing with flow

01:38:41,280 --> 01:38:45,360
label that means hey i can express my

01:38:44,000 --> 01:38:47,760
intent to reroute

01:38:45,360 --> 01:38:49,119
i can express my intent to be treated as

01:38:47,760 --> 01:38:52,400
the premium class

01:38:49,119 --> 01:38:52,719
packets uh very easily and then there is

01:38:52,400 --> 01:38:56,159
this

01:38:52,719 --> 01:38:58,560
uh another policy gatekeeper to say are

01:38:56,159 --> 01:38:58,880
you allowed to do that but typically

01:38:58,560 --> 01:39:03,679
this

01:38:58,880 --> 01:39:03,679
is not transparent to the users

01:39:05,520 --> 01:39:09,440
okay so i think i think this is a

01:39:07,040 --> 01:39:12,239
problem a little more general problem

01:39:09,440 --> 01:39:14,719
when you start talking about trying to

01:39:12,239 --> 01:39:18,400
use disserve

01:39:14,719 --> 01:39:21,040
where we we may or may not trust the

01:39:18,400 --> 01:39:22,960
the entity and we may we may or may not

01:39:21,040 --> 01:39:24,840
transform that into

01:39:22,960 --> 01:39:26,239
encapsulation that's really a policy

01:39:24,840 --> 01:39:28,400
thing and

01:39:26,239 --> 01:39:29,679
i think there's some related work on

01:39:28,400 --> 01:39:32,400
this

01:39:29,679 --> 01:39:34,000
but generally it seems like whoever's

01:39:32,400 --> 01:39:36,159
doing this encapsulation

01:39:34,000 --> 01:39:37,679
they have they have the visibility

01:39:36,159 --> 01:39:39,360
necessary right so

01:39:37,679 --> 01:39:40,960
they know what the user's intent is from

01:39:39,360 --> 01:39:42,960
the packet they're sending

01:39:40,960 --> 01:39:44,239
they should be able to map that into

01:39:42,960 --> 01:39:47,760
what

01:39:44,239 --> 01:39:50,320
what the encapsulation actually does

01:39:47,760 --> 01:39:50,880
i'm a little surprised that you seem to

01:39:50,320 --> 01:39:54,719
think that

01:39:50,880 --> 01:39:56,400
there's not a lot of i guess vendors or

01:39:54,719 --> 01:39:58,880
whomever uh that's that's doing this

01:39:56,400 --> 01:39:58,880
properly

01:39:59,679 --> 01:40:04,080
yeah i agree sorry i was if i was like

01:40:02,159 --> 01:40:07,360
confusing that making this sounds like

01:40:04,080 --> 01:40:10,400
it's technically very difficult it's not

01:40:07,360 --> 01:40:14,239
yeah um so um

01:40:10,400 --> 01:40:17,520
another uh question i think alexey

01:40:14,239 --> 01:40:19,440
mentioned that lack of gro already a

01:40:17,520 --> 01:40:23,760
bottleneck

01:40:19,440 --> 01:40:25,760
with ecn

01:40:23,760 --> 01:40:28,080
yeah i'm actually not sure like with

01:40:25,760 --> 01:40:28,480
high degree of ecm marking whether we

01:40:28,080 --> 01:40:31,119
can

01:40:28,480 --> 01:40:32,960
again i'm not a kernel engineer i just

01:40:31,119 --> 01:40:34,880
have a conjecture that maybe

01:40:32,960 --> 01:40:36,239
uh with high degree of ecm market we

01:40:34,880 --> 01:40:39,920
cannot either

01:40:36,239 --> 01:40:43,280
jiro or gso slash tsl

01:40:39,920 --> 01:40:45,679
packets again i have uh relatively a low

01:40:43,280 --> 01:40:48,639
understanding of that kernel code

01:40:45,679 --> 01:40:50,080
or network yeah and so that's a good

01:40:48,639 --> 01:40:52,960
question

01:40:50,080 --> 01:40:54,480
so the the quick answer is it highly

01:40:52,960 --> 01:40:58,560
depends on how the

01:40:54,480 --> 01:41:01,840
packets are marked um so in the case of

01:40:58,560 --> 01:41:04,480
a dc-tcp typically

01:41:01,840 --> 01:41:06,239
the way the switch is configured to say

01:41:04,480 --> 01:41:08,880
uh once the queue is above

01:41:06,239 --> 01:41:10,400
a particular threshold then uh it will

01:41:08,880 --> 01:41:12,719
start marking

01:41:10,400 --> 01:41:14,480
so you can imagine that you know when

01:41:12,719 --> 01:41:16,080
tcp is bursting

01:41:14,480 --> 01:41:18,080
they traverse through that queue

01:41:16,080 --> 01:41:20,159
typically the same queue

01:41:18,080 --> 01:41:21,600
um then you will get a continuous

01:41:20,159 --> 01:41:24,639
marking as the cube

01:41:21,600 --> 01:41:28,400
keeps building up right and

01:41:24,639 --> 01:41:30,800
so uh the receiver will receive this uh

01:41:28,400 --> 01:41:33,440
sort of a burst of packets that with

01:41:30,800 --> 01:41:35,679
this continuous easier mark that will

01:41:33,440 --> 01:41:38,960
allow them to actually collapse

01:41:35,679 --> 01:41:42,159
um coalesce the the packets

01:41:38,960 --> 01:41:45,199
into a big jungle one so it's not

01:41:42,159 --> 01:41:48,400
a big problem in

01:41:45,199 --> 01:41:49,760
the data center environment but for

01:41:48,400 --> 01:41:52,159
example if your

01:41:49,760 --> 01:41:52,960
switch actually does some kind of like

01:41:52,159 --> 01:41:56,159
different

01:41:52,960 --> 01:41:58,719
marking for example i want to mark one

01:41:56,159 --> 01:42:02,239
out of every four packets

01:41:58,719 --> 01:42:05,840
under certain condition then it becomes

01:42:02,239 --> 01:42:08,239
an issue um so i think for

01:42:05,840 --> 01:42:09,199
date inside the death center uh that's

01:42:08,239 --> 01:42:15,119
not a big

01:42:09,199 --> 01:42:15,600
issue for the 3168 um since we have not

01:42:15,119 --> 01:42:18,800
really

01:42:15,600 --> 01:42:22,880
tested that then i am not

01:42:18,800 --> 01:42:23,600
too sure um there has been a recent

01:42:22,880 --> 01:42:26,960
development

01:42:23,600 --> 01:42:30,159
on ecn called the accurate ecn

01:42:26,960 --> 01:42:33,360
and ietf um

01:42:30,159 --> 01:42:36,000
there they want to do more

01:42:33,360 --> 01:42:37,600
precise easier marking instead of just

01:42:36,000 --> 01:42:39,920
one bit they want to

01:42:37,600 --> 01:42:41,119
signal that hey what's the sequence

01:42:39,920 --> 01:42:45,920
range that gets

01:42:41,119 --> 01:42:49,360
marked so in those cases

01:42:45,920 --> 01:42:52,159
because they may use tcp options

01:42:49,360 --> 01:42:53,280
to convey this information sequence

01:42:52,159 --> 01:42:56,639
being marked

01:42:53,280 --> 01:42:59,119
it may potentially break

01:42:56,639 --> 01:43:00,880
the segmentation of flow and that has

01:42:59,119 --> 01:43:04,080
always been my concern of that

01:43:00,880 --> 01:43:04,080
particular approach

01:43:05,600 --> 01:43:08,880
quick note about the cloud network in

01:43:07,920 --> 01:43:11,679
the first subject

01:43:08,880 --> 01:43:13,440
um the do you understand correctly if

01:43:11,679 --> 01:43:15,280
people are using ser iov

01:43:13,440 --> 01:43:17,199
some of these problems go away i'm not

01:43:15,280 --> 01:43:18,000
talking about the setup of the system

01:43:17,199 --> 01:43:20,639
but if they

01:43:18,000 --> 01:43:21,040
just give a pc like a virtual function

01:43:20,639 --> 01:43:23,679
uh

01:43:21,040 --> 01:43:25,040
from pc pci express device and give it

01:43:23,679 --> 01:43:28,480
to the vm

01:43:25,040 --> 01:43:31,360
like none of these um at least

01:43:28,480 --> 01:43:33,360
like some of these problems go away

01:43:31,360 --> 01:43:36,000
performance problems

01:43:33,360 --> 01:43:36,800
um can you maybe give a particular

01:43:36,000 --> 01:43:39,840
example

01:43:36,800 --> 01:43:43,040
of the performance issue that would be

01:43:39,840 --> 01:43:46,880
resolved by the sir alby like

01:43:43,040 --> 01:43:48,480
the nappy um back pressure

01:43:46,880 --> 01:43:51,199
or stuff like that because they they if

01:43:48,480 --> 01:43:54,320
they get a like physical device or as

01:43:51,199 --> 01:43:58,239
physical as it can get um

01:43:54,320 --> 01:43:59,760
almost they don't want me to upgrade

01:43:58,239 --> 01:44:00,800
their kernel to get all these back

01:43:59,760 --> 01:44:03,119
pressure patches

01:44:00,800 --> 01:44:04,560
uh they they just give a pci express

01:44:03,119 --> 01:44:06,800
device to vm and it

01:44:04,560 --> 01:44:07,760
kind of deals with that so it will see

01:44:06,800 --> 01:44:10,960
almost a

01:44:07,760 --> 01:44:10,960
real device on the

01:44:12,000 --> 01:44:19,440
okay yeah

01:44:16,159 --> 01:44:20,159
yeah um yeah i didn't know about that

01:44:19,440 --> 01:44:23,199
but that's

01:44:20,159 --> 01:44:25,440
uh interesting to know yeah uh the

01:44:23,199 --> 01:44:27,040
second note about transport design so

01:44:25,440 --> 01:44:30,639
recently uh the

01:44:27,040 --> 01:44:31,199
high reordering so uh what i've recently

01:44:30,639 --> 01:44:34,080
learned

01:44:31,199 --> 01:44:34,880
that uh again during that windows

01:44:34,080 --> 01:44:37,920
debugging

01:44:34,880 --> 01:44:40,639
um but linux actually has rack

01:44:37,920 --> 01:44:43,040
and heuristic for uh reordering not very

01:44:40,639 --> 01:44:45,119
high reordering but

01:44:43,040 --> 01:44:47,360
linux can tolerate some amount of

01:44:45,119 --> 01:44:49,920
reordering on the link

01:44:47,360 --> 01:44:50,639
windows actually cannot it treats most

01:44:49,920 --> 01:44:53,840
of the

01:44:50,639 --> 01:44:55,199
reordering um even it implements rack

01:44:53,840 --> 01:44:57,920
but it cannot deal with

01:44:55,199 --> 01:45:00,320
reordering and the one additional bit of

01:44:57,920 --> 01:45:02,560
information here is that the right

01:45:00,320 --> 01:45:03,840
nics that are too smart for example

01:45:02,560 --> 01:45:07,280
intel nix can do

01:45:03,840 --> 01:45:09,679
flow director with atr like um

01:45:07,280 --> 01:45:10,320
atr function when it tries to direct

01:45:09,679 --> 01:45:13,679
packets

01:45:10,320 --> 01:45:17,040
to the cpu that owns that flow

01:45:13,679 --> 01:45:18,800
in the user space right now to like

01:45:17,040 --> 01:45:20,159
to decrease the amount of uh cache

01:45:18,800 --> 01:45:23,520
misses etc

01:45:20,159 --> 01:45:25,440
so that thing that smart me can actually

01:45:23,520 --> 01:45:29,600
reorder packets by doing

01:45:25,440 --> 01:45:31,920
um at least from transport perspective

01:45:29,600 --> 01:45:33,679
transport will think that it receives uh

01:45:31,920 --> 01:45:35,840
packets out of order

01:45:33,679 --> 01:45:38,719
even though they are perfectly in order

01:45:35,840 --> 01:45:38,719
on the link itself

01:45:38,800 --> 01:45:45,199
excuse me uh so that combination of uh

01:45:42,000 --> 01:45:47,199
like there can be basically a reordering

01:45:45,199 --> 01:45:49,920
on the networks even without any

01:45:47,199 --> 01:45:52,000
very high uh complexity just because of

01:45:49,920 --> 01:45:55,280
very smart technique that tries to

01:45:52,000 --> 01:45:57,040
um direct packets uh not based on hash

01:45:55,280 --> 01:46:01,040
but based on the flow

01:45:57,040 --> 01:46:04,480
uh flow the like

01:46:01,040 --> 01:46:06,239
um sorry yes

01:46:04,480 --> 01:46:07,520
not on the flow but based on some

01:46:06,239 --> 01:46:10,960
heuristic on like

01:46:07,520 --> 01:46:13,520
what uh what process runs on which uh

01:46:10,960 --> 01:46:15,360
which die of the processor right now

01:46:13,520 --> 01:46:18,159
sorry

01:46:15,360 --> 01:46:20,880
um yes i agree i think the the kind of

01:46:18,159 --> 01:46:22,960
parallel processing in general um

01:46:20,880 --> 01:46:24,080
packets being processed by different

01:46:22,960 --> 01:46:26,719
processor or

01:46:24,080 --> 01:46:28,960
different route paths is the source of

01:46:26,719 --> 01:46:30,080
the more and more frequent uh reordering

01:46:28,960 --> 01:46:31,840
we're seeing

01:46:30,080 --> 01:46:33,520
and you know people are actually getting

01:46:31,840 --> 01:46:36,480
even more aggressive in

01:46:33,520 --> 01:46:37,520
i want to make it even more uh a higher

01:46:36,480 --> 01:46:40,880
a large scale

01:46:37,520 --> 01:46:43,199
more multiple paths and that's why

01:46:40,880 --> 01:46:44,480
we keep seeing this so uh i think it

01:46:43,199 --> 01:46:47,199
will maybe come

01:46:44,480 --> 01:46:48,400
soon that we will need to rethink how

01:46:47,199 --> 01:46:51,920
gro is being

01:46:48,400 --> 01:46:56,400
uh implemented and to deal with that

01:46:51,920 --> 01:46:59,600
uh well um and i think time to say

01:46:56,400 --> 01:47:03,199
pack and reorder is a fact of life um

01:46:59,600 --> 01:47:06,840
i agree yeah i'm not complaining uh

01:47:03,199 --> 01:47:10,159
and so you know we need to

01:47:06,840 --> 01:47:12,800
adapt or the transport needs to adapt uh

01:47:10,159 --> 01:47:13,920
to that um but i don't think we want to

01:47:12,800 --> 01:47:17,520
encourage

01:47:13,920 --> 01:47:20,000
sort of this careless reorderings right

01:47:17,520 --> 01:47:22,960
but working in harmony with the network

01:47:20,000 --> 01:47:26,239
and the transport i think is important

01:47:22,960 --> 01:47:26,719
um i think we are two minutes uh over

01:47:26,239 --> 01:47:30,639
time

01:47:26,719 --> 01:47:33,840
so um i just want to um

01:47:30,639 --> 01:47:35,040
conclude that this workshop and thanks

01:47:33,840 --> 01:47:37,840
for all these

01:47:35,040 --> 01:47:38,960
speakers and as well as all the audience

01:47:37,840 --> 01:47:43,119
asking

01:47:38,960 --> 01:47:45,679
so many great questions and um

01:47:43,119 --> 01:47:47,280
glad to really see all these advancement

01:47:45,679 --> 01:47:50,719
in in the transport

01:47:47,280 --> 01:47:53,679
layer in netdev and with that

01:47:50,719 --> 01:47:55,679
um thanks everyone and i will see you in

01:47:53,679 --> 01:48:01,440
the next netapp

01:47:55,679 --> 01:48:01,440

YouTube URL: https://www.youtube.com/watch?v=goyZF4JmRyA


