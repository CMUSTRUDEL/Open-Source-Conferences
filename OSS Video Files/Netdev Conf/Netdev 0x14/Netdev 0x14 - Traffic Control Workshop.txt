Title: Netdev 0x14 - Traffic Control Workshop
Publication date: 2020-10-09
Playlist: Netdev 0x14
Description: 
	Chair: Jamal Hadi Salim

Agenda includes:
-TC path performance status update - Vlad Buslov
-Extending TC with 5-tuple hash offload - Roni Bar Yanai
-Support for reassembled IP packets in relation to act_ct - Marcelo Ricardo Leitner
-TCD Update - Briana Oursler
Captions: 
	00:00:03,199 --> 00:00:07,200
so

00:00:03,760 --> 00:00:08,000
hi everyone i'm glad from melanox now

00:00:07,200 --> 00:00:09,519
nvidia

00:00:08,000 --> 00:00:11,519
and today i would like to give you an

00:00:09,519 --> 00:00:13,759
update on tc performance

00:00:11,519 --> 00:00:14,799
and performance of control path

00:00:13,759 --> 00:00:16,560
specifically

00:00:14,799 --> 00:00:19,199
i will not be talking about data paths

00:00:16,560 --> 00:00:20,960
in this talk

00:00:19,199 --> 00:00:22,720
so of course just to give you a quick

00:00:20,960 --> 00:00:23,920
recap so i've been working on this for

00:00:22,720 --> 00:00:27,359
several years now

00:00:23,920 --> 00:00:29,519
and i have already presented

00:00:27,359 --> 00:00:32,000
initially presented this two net devs

00:00:29,519 --> 00:00:33,680
ago and then give an update on upstream

00:00:32,000 --> 00:00:35,920
status of all of these patches on

00:00:33,680 --> 00:00:38,480
previous net development prague

00:00:35,920 --> 00:00:40,399
so this one is just a quick recap if you

00:00:38,480 --> 00:00:43,280
would like more details here

00:00:40,399 --> 00:00:45,200
i refer you to this previous talks from

00:00:43,280 --> 00:00:48,239
tc workshops

00:00:45,200 --> 00:00:51,440
from two previous network conferences so

00:00:48,239 --> 00:00:54,000
first of all cls api filter update pass

00:00:51,440 --> 00:00:54,960
art analog dependency was removed from

00:00:54,000 --> 00:00:57,920
there

00:00:54,960 --> 00:01:00,079
and one single classifier cls flower was

00:00:57,920 --> 00:01:01,840
converted to use internal fine grained

00:01:00,079 --> 00:01:04,000
login

00:01:01,840 --> 00:01:05,840
so it's the only classifier i think

00:01:04,000 --> 00:01:06,720
which is unlocked now and it's widely

00:01:05,840 --> 00:01:10,400
used

00:01:06,720 --> 00:01:12,000
by for hardware of loading

00:01:10,400 --> 00:01:14,560
besides that all of the actions were

00:01:12,000 --> 00:01:17,360
converted uh

00:01:14,560 --> 00:01:19,200
so it's all of them in case of actions

00:01:17,360 --> 00:01:22,000
no pitfalls or anything

00:01:19,200 --> 00:01:22,880
and drivers can now signal that they do

00:01:22,000 --> 00:01:25,840
not require

00:01:22,880 --> 00:01:28,080
cls api to hold their channel while

00:01:25,840 --> 00:01:31,360
calling their callback to a float

00:01:28,080 --> 00:01:34,400
or remove a floating of a filter

00:01:31,360 --> 00:01:36,320
so that's just a quick recap

00:01:34,400 --> 00:01:38,320
and today i would like to talk about

00:01:36,320 --> 00:01:40,320
some more challenges that i faced

00:01:38,320 --> 00:01:42,560
on further optimizations of performance

00:01:40,320 --> 00:01:44,560
of tc

00:01:42,560 --> 00:01:46,240
so first challenge is interaction

00:01:44,560 --> 00:01:49,280
between tc filter

00:01:46,240 --> 00:01:52,799
add and delete also and the dump

00:01:49,280 --> 00:01:54,960
so prior to 5.7

00:01:52,799 --> 00:01:56,079
flow action infrastructure code briefly

00:01:54,960 --> 00:01:58,240
obtained rtnl

00:01:56,079 --> 00:01:59,680
just in this small function tc setup

00:01:58,240 --> 00:02:02,320
flow action

00:01:59,680 --> 00:02:09,840
internally this function is very simple

00:02:02,320 --> 00:02:09,840
it just three

00:02:10,080 --> 00:02:17,360
i think we just lost blood again blood

00:02:14,000 --> 00:02:19,760
can you hear me lock removal from tc and

00:02:17,360 --> 00:02:22,239
during that time i think pablo from

00:02:19,760 --> 00:02:24,160
netfilter he wanted to have hardware

00:02:22,239 --> 00:02:26,000
fluid and for net filter and he wanted

00:02:24,160 --> 00:02:28,319
to reuse tc for this

00:02:26,000 --> 00:02:30,879
so he introduces this kind the

00:02:28,319 --> 00:02:33,760
intermediate representation

00:02:30,879 --> 00:02:36,560
to encode all the data from tc which can

00:02:33,760 --> 00:02:38,480
also be used from net filter and

00:02:36,560 --> 00:02:39,840
when i upstreamed my stuff i saw this

00:02:38,480 --> 00:02:41,680
function i

00:02:39,840 --> 00:02:43,599
saw that it's very simple so i just

00:02:41,680 --> 00:02:51,840
decided to put a channel lock

00:02:43,599 --> 00:02:51,840
internally around its code

00:02:58,319 --> 00:03:02,000
but however then we started testing on

00:03:01,280 --> 00:03:04,560
actual

00:03:02,000 --> 00:03:06,319
virtual switch implementation and the

00:03:04,560 --> 00:03:09,120
way virtual switches work is that

00:03:06,319 --> 00:03:11,280
usually they don't just have to remove

00:03:09,120 --> 00:03:12,400
loads they also need to get their flow

00:03:11,280 --> 00:03:14,319
stats

00:03:12,400 --> 00:03:15,599
and usually they do it concurrently

00:03:14,319 --> 00:03:16,640
because they're multi-threaded

00:03:15,599 --> 00:03:19,920
internally

00:03:16,640 --> 00:03:20,879
so what i observed is that when virtual

00:03:19,920 --> 00:03:22,560
switch

00:03:20,879 --> 00:03:25,120
so initially we didn't know what's wrong

00:03:22,560 --> 00:03:27,920
because the performance was good but

00:03:25,120 --> 00:03:29,280
in actual real-life use cases sometimes

00:03:27,920 --> 00:03:30,720
insertion rate dropped down

00:03:29,280 --> 00:03:33,040
significantly

00:03:30,720 --> 00:03:33,920
and when i started debugging it turned

00:03:33,040 --> 00:03:36,799
out that

00:03:33,920 --> 00:03:38,080
switch initiates a dump of all filters

00:03:36,799 --> 00:03:41,599
concurrently

00:03:38,080 --> 00:03:44,000
and the way dump is implemented in tc

00:03:41,599 --> 00:03:44,959
is that it fully depends on our channel

00:03:44,000 --> 00:03:47,040
lock

00:03:44,959 --> 00:04:00,319
so basically when dump is executed it

00:03:47,040 --> 00:04:03,519
obtains certain lock

00:04:00,319 --> 00:04:05,360
filter add and delete tasks they have to

00:04:03,519 --> 00:04:07,360
wait for research analog even though

00:04:05,360 --> 00:04:08,400
they only need it for very brief moment

00:04:07,360 --> 00:04:10,640
of time

00:04:08,400 --> 00:04:13,680
just to read several feed data fields

00:04:10,640 --> 00:04:15,920
from action implementation

00:04:13,680 --> 00:04:17,919
so this resulted significant law

00:04:15,920 --> 00:04:19,199
contention especially when there were

00:04:17,919 --> 00:04:22,720
several dump threats

00:04:19,199 --> 00:04:24,240
in virtual switches and

00:04:22,720 --> 00:04:26,720
i would like to show you a flame graph

00:04:24,240 --> 00:04:28,160
on this so i know that most of the

00:04:26,720 --> 00:04:31,520
people here are probably

00:04:28,160 --> 00:04:33,759
quite familiar with flame graph but just

00:04:31,520 --> 00:04:36,000
maybe for people who are not that

00:04:33,759 --> 00:04:40,320
familiar just want to remind that

00:04:36,000 --> 00:04:42,960
flame graph on a vertical axis

00:04:40,320 --> 00:04:44,880
it's basically a call stack and the

00:04:42,960 --> 00:04:47,600
caller is on the bottom

00:04:44,880 --> 00:04:48,560
and the kali is on the top so this guy

00:04:47,600 --> 00:04:50,560
tc calls

00:04:48,560 --> 00:04:52,160
ellipsis and message which calls entry

00:04:50,560 --> 00:04:55,600
syscall

00:04:52,160 --> 00:04:58,720
and horizontally it's

00:04:55,600 --> 00:05:00,960
each width of each body represents how

00:04:58,720 --> 00:05:03,520
many samples were obtained by profiler

00:05:00,960 --> 00:05:07,039
basically how much cpu time

00:05:03,520 --> 00:05:10,160
did each function took so

00:05:07,039 --> 00:05:12,720
here we can see that a

00:05:10,160 --> 00:05:14,479
flower change calls flower hardware

00:05:12,720 --> 00:05:17,199
float replace filter

00:05:14,479 --> 00:05:19,039
which calls the tc setup flow action and

00:05:17,199 --> 00:05:22,000
we can zoom in

00:05:19,039 --> 00:05:23,360
and all this tc setup flow action does

00:05:22,000 --> 00:05:27,360
it just spins

00:05:23,360 --> 00:05:30,720
on narcianal mutex because

00:05:27,360 --> 00:05:34,320
it contents for other add and delete

00:05:30,720 --> 00:05:36,320
threats and also with dumb threat which

00:05:34,320 --> 00:05:37,520
monopolizes certain analog for quite

00:05:36,320 --> 00:05:39,840
some amount of time

00:05:37,520 --> 00:05:43,520
up till the net link packet size is

00:05:39,840 --> 00:05:43,520
fully filled with filter data

00:05:43,600 --> 00:05:45,840
so

00:05:47,199 --> 00:05:52,160
so solution was quite simple to just

00:05:50,000 --> 00:05:54,080
finish the job and fully remove art

00:05:52,160 --> 00:05:56,400
analog dependency from this particular

00:05:54,080 --> 00:05:57,680
function and it was not that challenging

00:05:56,400 --> 00:05:59,440
to do because

00:05:57,680 --> 00:06:01,039
all actions as i said before were

00:05:59,440 --> 00:06:03,360
already converted to use

00:06:01,039 --> 00:06:06,639
their own internal fine grained locks

00:06:03,360 --> 00:06:09,199
tcfa lock which is per correction

00:06:06,639 --> 00:06:10,560
and i used it instead of requiring

00:06:09,199 --> 00:06:13,039
global rtl lock

00:06:10,560 --> 00:06:14,880
and of course i had to fix several small

00:06:13,039 --> 00:06:16,560
issues like

00:06:14,880 --> 00:06:18,319
some of the functions that were called

00:06:16,560 --> 00:06:20,639
by this function were sleeping and

00:06:18,319 --> 00:06:22,240
tcfa lock is spin lock and earth analog

00:06:20,639 --> 00:06:25,039
is mutex so we cannot sleep while

00:06:22,240 --> 00:06:28,319
holding tcf lock but it was minor

00:06:25,039 --> 00:06:31,280
and it resulted significant

00:06:28,319 --> 00:06:32,319
performance improvement when running tc

00:06:31,280 --> 00:06:36,080
filter update

00:06:32,319 --> 00:06:37,039
and the dump concurrently i no longer

00:06:36,080 --> 00:06:40,000
observe

00:06:37,039 --> 00:06:42,240
almost any kind of degradation of

00:06:40,000 --> 00:06:44,639
insertion rate while running concurrent

00:06:42,240 --> 00:06:48,479
filter dump

00:06:44,639 --> 00:06:50,400
particularly only if i dump and update

00:06:48,479 --> 00:06:52,639
same classifier instance at the same

00:06:50,400 --> 00:06:57,120
time it results like contention of

00:06:52,639 --> 00:06:59,599
internal flower locks but it's not

00:06:57,120 --> 00:07:01,440
like before drop down and insertion rate

00:06:59,599 --> 00:07:03,039
was in hundreds of percent and now the

00:07:01,440 --> 00:07:06,080
total price might be

00:07:03,039 --> 00:07:10,080
five percent in my tests

00:07:06,080 --> 00:07:10,080
so i'm quite satisfied with this result

00:07:10,400 --> 00:07:14,560
another challenge was actually per cpu

00:07:13,199 --> 00:07:16,880
allocator

00:07:14,560 --> 00:07:19,280
so all actions not like most of the

00:07:16,880 --> 00:07:20,880
actions internally use per cpu allocator

00:07:19,280 --> 00:07:24,319
for stats

00:07:20,880 --> 00:07:24,639
uh and the vapor cpu allocator works is

00:07:24,319 --> 00:07:26,639
that

00:07:24,639 --> 00:07:29,039
internally it's synchronized by single

00:07:26,639 --> 00:07:30,080
global mutex not rtnl but dedicated

00:07:29,039 --> 00:07:32,880
mutex but still

00:07:30,080 --> 00:07:34,880
it's global it's shared by all per cpu

00:07:32,880 --> 00:07:38,000
users

00:07:34,880 --> 00:07:39,599
and another problem is that precipito

00:07:38,000 --> 00:07:42,080
allocator memory usage

00:07:39,599 --> 00:07:44,160
grows linearly with number of cpus

00:07:42,080 --> 00:07:44,879
because it has to allocate some amount

00:07:44,160 --> 00:07:48,400
of memory

00:07:44,879 --> 00:07:49,039
per each cpu in system so while it's

00:07:48,400 --> 00:07:51,039
great for

00:07:49,039 --> 00:07:52,879
data pass performance it's not that

00:07:51,039 --> 00:07:55,759
great for control path performance and

00:07:52,879 --> 00:07:55,759
memory usage

00:07:56,240 --> 00:08:00,639
so here is a flame graph of filter

00:07:59,280 --> 00:08:03,280
update

00:08:00,639 --> 00:08:05,759
when a precipito locator is used is

00:08:03,280 --> 00:08:10,000
being used for stats

00:08:05,759 --> 00:08:13,280
and here we see flower change calls

00:08:10,000 --> 00:08:16,400
actions in it and here in this case

00:08:13,280 --> 00:08:18,400
i use two most used actions in my case

00:08:16,400 --> 00:08:19,919
one is tunnel key action and another is

00:08:18,400 --> 00:08:22,639
mirrored action

00:08:19,919 --> 00:08:23,919
and we can see that both of them consume

00:08:22,639 --> 00:08:27,039
significant amount

00:08:23,919 --> 00:08:30,560
of resources by per cpu

00:08:27,039 --> 00:08:33,919
allocator for example we can zoom out

00:08:30,560 --> 00:08:34,560
zoom in here and see precipitates a lot

00:08:33,919 --> 00:08:37,279
of

00:08:34,560 --> 00:08:37,919
time and also most of the time is again

00:08:37,279 --> 00:08:40,880
spent

00:08:37,919 --> 00:08:44,959
just spinning on mutex because once in

00:08:40,880 --> 00:08:44,959
global mutex is highly contended

00:08:49,200 --> 00:08:55,120
so implement i wanted to allow users to

00:08:53,600 --> 00:08:58,880
fix this to choose which

00:08:55,120 --> 00:09:02,160
stats implementation they want because

00:08:58,880 --> 00:09:02,880
even though per cpu locator uses a lot

00:09:02,160 --> 00:09:05,839
of memory

00:09:02,880 --> 00:09:08,560
and is slow on control paths it's it's

00:09:05,839 --> 00:09:10,320
quite fast for data paths so it's very

00:09:08,560 --> 00:09:11,360
beneficial for people who don't use

00:09:10,320 --> 00:09:13,279
hardware float

00:09:11,360 --> 00:09:15,120
and actually it was implemented as a

00:09:13,279 --> 00:09:15,760
performance optimization so i couldn't

00:09:15,120 --> 00:09:19,200
just

00:09:15,760 --> 00:09:22,399
remove it instead i just i decided to

00:09:19,200 --> 00:09:23,680
implement a flag that allows user to

00:09:22,399 --> 00:09:27,360
control it

00:09:23,680 --> 00:09:30,399
so the flag name is flex nope cpu starts

00:09:27,360 --> 00:09:30,880
the name was discussed a lot in mailing

00:09:30,399 --> 00:09:33,760
list

00:09:30,880 --> 00:09:36,160
because i wanted to make it generic

00:09:33,760 --> 00:09:40,160
since it's not only user of per cpu

00:09:36,160 --> 00:09:44,080
in actions but we will get to that in

00:09:40,160 --> 00:09:47,839
in problems that we still have as of now

00:09:44,080 --> 00:09:50,000
and okay so

00:09:47,839 --> 00:09:51,600
when user doesn't does not request to

00:09:50,000 --> 00:09:54,000
have this pcp allocator

00:09:51,600 --> 00:09:56,160
regular integers reused and are

00:09:54,000 --> 00:09:59,040
synchronized with the same correction

00:09:56,160 --> 00:09:59,680
fine grained lock and the good news for

00:09:59,040 --> 00:10:02,640
me

00:09:59,680 --> 00:10:03,360
were that such infrastructure already

00:10:02,640 --> 00:10:05,200
existed

00:10:03,360 --> 00:10:06,959
from before from before the cpu

00:10:05,200 --> 00:10:09,600
allocator was introduced

00:10:06,959 --> 00:10:12,720
and since some of the actions are still

00:10:09,600 --> 00:10:14,640
not converted to per cpu locator

00:10:12,720 --> 00:10:16,880
they're still using the infrastructure

00:10:14,640 --> 00:10:20,640
so i just changed all the actions

00:10:16,880 --> 00:10:24,880
to use either per cpu or this old

00:10:20,640 --> 00:10:24,880
regular integers for stats counting

00:10:25,760 --> 00:10:29,920
and it resulted significant impact on

00:10:28,720 --> 00:10:32,880
insertion rate

00:10:29,920 --> 00:10:34,399
increase from 30 to 120 percent in my

00:10:32,880 --> 00:10:36,800
tests and

00:10:34,399 --> 00:10:37,600
actually i'm only testing with one or

00:10:36,800 --> 00:10:41,920
two actions

00:10:37,600 --> 00:10:45,519
but a cls api supports attaching up to

00:10:41,920 --> 00:10:47,600
what's 32 actions per per filter

00:10:45,519 --> 00:10:49,680
so the more actions that use

00:10:47,600 --> 00:10:50,880
precipitator you have the more you will

00:10:49,680 --> 00:10:54,480
be impacted by

00:10:50,880 --> 00:10:55,920
uh by this so it's quite but even with

00:10:54,480 --> 00:10:58,959
single action it's still

00:10:55,920 --> 00:11:00,720
30 to 50 so it's quite significant and

00:10:58,959 --> 00:11:02,640
this allowed to improve first of all

00:11:00,720 --> 00:11:05,600
memory usage because the total

00:11:02,640 --> 00:11:07,600
of all per cpu stats is around 52 bytes

00:11:05,600 --> 00:11:10,240
plus some metadata

00:11:07,600 --> 00:11:11,200
and this calculation also matched what i

00:11:10,240 --> 00:11:14,560
observed

00:11:11,200 --> 00:11:15,279
on real system so you can see that with

00:11:14,560 --> 00:11:18,320
current

00:11:15,279 --> 00:11:20,240
multi-core servers uh like now you can

00:11:18,320 --> 00:11:22,240
have i don't know hundreds of

00:11:20,240 --> 00:11:24,880
cpus and systems so it will be quite

00:11:22,240 --> 00:11:24,880
significant

00:11:26,399 --> 00:11:31,279
and the i also would like to give you

00:11:29,600 --> 00:11:32,640
updates on current net next performance

00:11:31,279 --> 00:11:35,760
and current challenges

00:11:32,640 --> 00:11:37,120
that we are still facing even after this

00:11:35,760 --> 00:11:40,320
update

00:11:37,120 --> 00:11:44,000
so this is a flame graph obtained on

00:11:40,320 --> 00:11:46,399
current net next we did it this week

00:11:44,000 --> 00:11:47,760
so as you can see there are still some

00:11:46,399 --> 00:11:50,959
mutex contention

00:11:47,760 --> 00:11:53,519
and it's mainly in idr

00:11:50,959 --> 00:11:56,399
so idr now becomes a bottleneck we are

00:11:53,519 --> 00:11:59,440
using adr in both flower

00:11:56,399 --> 00:12:01,440
and in in actions

00:11:59,440 --> 00:12:03,200
so actually when i insert one filter

00:12:01,440 --> 00:12:07,200
with several action i need to update

00:12:03,200 --> 00:12:09,279
the flower idr plus

00:12:07,200 --> 00:12:11,519
idr projection so it's a lot and it

00:12:09,279 --> 00:12:14,560
becomes a bottleneck

00:12:11,519 --> 00:12:17,440
another bottleneck is the gst cache it's

00:12:14,560 --> 00:12:19,600
specific to tunnel key action but

00:12:17,440 --> 00:12:22,240
it still impacts us because tunnel key

00:12:19,600 --> 00:12:24,800
action is in my opinion like most

00:12:22,240 --> 00:12:26,079
maybe not most used action after a giact

00:12:24,800 --> 00:12:29,360
and mirad but it's

00:12:26,079 --> 00:12:31,360
heavily used and dst cache you can zoom

00:12:29,360 --> 00:12:32,800
in internally again it uses per cpu

00:12:31,360 --> 00:12:35,839
allocation

00:12:32,800 --> 00:12:38,079
which becomes a bottleneck and here i

00:12:35,839 --> 00:12:40,000
obtain this flame graphicstc only but

00:12:38,079 --> 00:12:42,000
for example when using it together with

00:12:40,000 --> 00:12:43,120
open v switch open the switch internally

00:12:42,000 --> 00:12:45,519
also uses

00:12:43,120 --> 00:12:47,200
in its kernel data paths precipi

00:12:45,519 --> 00:12:49,040
allocation which results in more

00:12:47,200 --> 00:12:52,079
contentions so

00:12:49,040 --> 00:12:53,680
per cpu is reciprocator is a problem all

00:12:52,079 --> 00:12:57,760
around when used

00:12:53,680 --> 00:12:57,760
in high performance control paths

00:12:59,600 --> 00:13:05,680
so these are two main bottlenecks

00:13:02,639 --> 00:13:09,839
adr and per cpu besides this

00:13:05,680 --> 00:13:13,920
we also see filter notification

00:13:09,839 --> 00:13:17,279
which consumes like what almost

00:13:13,920 --> 00:13:18,959
seven and a half percent but i'm not

00:13:17,279 --> 00:13:20,720
sure that we can really do something

00:13:18,959 --> 00:13:23,440
about it because

00:13:20,720 --> 00:13:24,320
i cannot just disable it i mean other

00:13:23,440 --> 00:13:27,040
users

00:13:24,320 --> 00:13:28,240
user and systems might be dependent on

00:13:27,040 --> 00:13:31,760
this notification

00:13:28,240 --> 00:13:32,720
for something so currently i don't see a

00:13:31,760 --> 00:13:35,120
way to

00:13:32,720 --> 00:13:38,480
somehow remove or lift this restriction

00:13:35,120 --> 00:13:38,480
with the filter notify

00:13:38,959 --> 00:13:42,880
yes here i just reiterate the main

00:13:41,040 --> 00:13:46,240
bottlenecks and the

00:13:42,880 --> 00:13:48,399
idr so i looked into idr api

00:13:46,240 --> 00:13:50,560
so that maybe there are some knobs there

00:13:48,399 --> 00:13:51,120
for me to play this to configure but it

00:13:50,560 --> 00:13:53,760
doesn't

00:13:51,120 --> 00:13:56,880
seem to be very configurable so if

00:13:53,760 --> 00:13:59,839
anyone has any experiences tuning hdr

00:13:56,880 --> 00:14:00,399
or knows how performance can be improved

00:13:59,839 --> 00:14:03,760
the

00:14:00,399 --> 00:14:05,920
suggestions are very welcome because

00:14:03,760 --> 00:14:08,160
if you look at flame graph again we see

00:14:05,920 --> 00:14:08,160
that

00:14:08,959 --> 00:14:14,320
idr is significant

00:14:12,079 --> 00:14:17,440
significantly impacts our insertion rate

00:14:14,320 --> 00:14:19,680
here and here for every action

00:14:17,440 --> 00:14:22,480
also for filter change i think we should

00:14:19,680 --> 00:14:26,000
also see it somewhere here

00:14:22,480 --> 00:14:30,160
maybe it's dwarfed by other calls

00:14:26,000 --> 00:14:33,839
so anyway that's the update

00:14:30,160 --> 00:14:33,839
i guess it's time for questions

00:14:34,560 --> 00:14:40,000
okay so for questions um if you can okay

00:14:38,320 --> 00:14:41,279
we're not gonna force anybody to turn on

00:14:40,000 --> 00:14:43,680
their video but if you can

00:14:41,279 --> 00:14:45,279
just uh raise your let's try the two

00:14:43,680 --> 00:14:47,920
models you can type the question

00:14:45,279 --> 00:14:48,720
on the chat on the chat you can raise

00:14:47,920 --> 00:14:52,560
your hand

00:14:48,720 --> 00:14:53,920
i can see your hand uh and

00:14:52,560 --> 00:14:55,760
then you turn on your video and you

00:14:53,920 --> 00:14:59,519
speak uh

00:14:55,760 --> 00:15:02,079
so let's let's take the questions now

00:14:59,519 --> 00:15:02,079
for vlad

00:15:06,639 --> 00:15:13,920
anybody on the chat maybe

00:15:12,320 --> 00:15:16,560
so waiting for that maybe i'll ask you a

00:15:13,920 --> 00:15:20,639
question vlad so

00:15:16,560 --> 00:15:22,320
it sounds to me like your solution to

00:15:20,639 --> 00:15:25,040
get rid of the past cpu allocation

00:15:22,320 --> 00:15:28,160
basically you bypass it

00:15:25,040 --> 00:15:30,720
what could you not have fixed the cpu

00:15:28,160 --> 00:15:33,920
allocated itself as opposed to

00:15:30,720 --> 00:15:35,920
uh you know avoiding it

00:15:33,920 --> 00:15:37,839
yes some of the things might be fixed

00:15:35,920 --> 00:15:41,040
like for example the global lock

00:15:37,839 --> 00:15:43,680
but the memory usage itself i think it's

00:15:41,040 --> 00:15:45,600
inherent to design because if you want

00:15:43,680 --> 00:15:47,440
to have per cpu memory you need to

00:15:45,600 --> 00:15:48,079
allocate memory which is linear the

00:15:47,440 --> 00:15:50,480
number in

00:15:48,079 --> 00:15:52,160
of cpu using system right there is no

00:15:50,480 --> 00:15:54,800
way around that

00:15:52,160 --> 00:15:56,320
so even if you could do something with

00:15:54,800 --> 00:15:59,360
performance

00:15:56,320 --> 00:16:00,720
memory usage in my opinion is not really

00:15:59,360 --> 00:16:03,680
fixable

00:16:00,720 --> 00:16:05,519
uh also i don't think it's heavily used

00:16:03,680 --> 00:16:07,839
for any performance stuff because for

00:16:05,519 --> 00:16:09,680
example like a year ago maybe more we

00:16:07,839 --> 00:16:11,759
encountered significant degradation

00:16:09,680 --> 00:16:12,480
insertion rate and culprit apparently

00:16:11,759 --> 00:16:15,360
was

00:16:12,480 --> 00:16:17,519
some very small change eric dumas that

00:16:15,360 --> 00:16:19,120
sets some kind of

00:16:17,519 --> 00:16:20,800
alignment changed alignment for a

00:16:19,120 --> 00:16:22,800
structure and

00:16:20,800 --> 00:16:25,120
it completely resulted some kind of

00:16:22,800 --> 00:16:27,360
corner case in per cpu

00:16:25,120 --> 00:16:28,480
and i had to report it to per cpu

00:16:27,360 --> 00:16:31,360
allocator people

00:16:28,480 --> 00:16:33,279
so i don't even believe that anyone else

00:16:31,360 --> 00:16:34,880
uses it for anything that require high

00:16:33,279 --> 00:16:38,079
performance on control paths

00:16:34,880 --> 00:16:40,320
so i think it would be quite painful

00:16:38,079 --> 00:16:41,920
for these people if i go into their code

00:16:40,320 --> 00:16:45,440
and you know try to make it

00:16:41,920 --> 00:16:48,880
fast while it's not never designed

00:16:45,440 --> 00:16:50,880
to be so eric is i think is here

00:16:48,880 --> 00:16:52,720
yeah but it wasn't eric's fault he just

00:16:50,880 --> 00:16:55,759
said some alignment of some field

00:16:52,720 --> 00:16:58,240
it was cpu like a recipe allocator fold

00:16:55,759 --> 00:17:00,639
basically

00:16:58,240 --> 00:17:01,279
yeah but what if i want to use stats and

00:17:00,639 --> 00:17:04,559
do control

00:17:01,279 --> 00:17:06,160
basically uh it won't work right if i

00:17:04,559 --> 00:17:08,240
wanted these stats

00:17:06,160 --> 00:17:09,760
you're doing this for for the benefit of

00:17:08,240 --> 00:17:11,120
hardware offload basically because if

00:17:09,760 --> 00:17:12,400
you're offloading to hardware the stats

00:17:11,120 --> 00:17:16,959
are in hardware so you

00:17:12,400 --> 00:17:19,919
really don't want to use the cpu data

00:17:16,959 --> 00:17:21,280
yeah but stats still work you just use

00:17:19,919 --> 00:17:23,839
regular integers so

00:17:21,280 --> 00:17:25,679
you can create theoretically if you want

00:17:23,839 --> 00:17:26,240
you can create a filter with action

00:17:25,679 --> 00:17:28,319
without

00:17:26,240 --> 00:17:29,760
cpu stats and stats will continue to

00:17:28,319 --> 00:17:32,240
work you will just absorb

00:17:29,760 --> 00:17:34,880
degradation of that data past

00:17:32,240 --> 00:17:34,880
performance

00:17:35,360 --> 00:17:40,480
okay so much has raised his hand

00:17:38,480 --> 00:17:42,080
uh you if you want to go on video

00:17:40,480 --> 00:17:44,799
material please go ahead if you want to

00:17:42,080 --> 00:17:47,919
type the question i can read it or

00:17:44,799 --> 00:17:52,720
live can read it as well all right

00:17:47,919 --> 00:17:56,080
it is fine hi vlad um

00:17:52,720 --> 00:17:59,120
on the new filter notification it dumps

00:17:56,080 --> 00:18:00,799
stats together with it i will consider

00:17:59,120 --> 00:18:04,160
not dumping just the stats because there

00:18:00,799 --> 00:18:07,200
are zero anyway right it's a new filter

00:18:04,160 --> 00:18:07,200
at least for that moment

00:18:07,679 --> 00:18:14,400
i did not maybe it's a good idea but

00:18:11,200 --> 00:18:17,679
then again how can we ensure that any

00:18:14,400 --> 00:18:19,679
software in system like it yes there is

00:18:17,679 --> 00:18:21,679
no point in reading these stats but

00:18:19,679 --> 00:18:24,880
maybe people reuse code or something

00:18:21,679 --> 00:18:27,200
wouldn't we just break user space

00:18:24,880 --> 00:18:28,799
some obscure corner case when someone

00:18:27,200 --> 00:18:31,280
for some reason

00:18:28,799 --> 00:18:34,400
checks that stats are present that's my

00:18:31,280 --> 00:18:37,200
main concern with that changes

00:18:34,400 --> 00:18:39,039
maybe it's more question to jamal can we

00:18:37,200 --> 00:18:40,480
do that

00:18:39,039 --> 00:18:42,000
i think you can you can break user

00:18:40,480 --> 00:18:42,720
specific it will break use especially

00:18:42,000 --> 00:18:46,480
right

00:18:42,720 --> 00:18:48,559
because people are expecting stats and

00:18:46,480 --> 00:18:52,799
now you remove but if it's an optional

00:18:48,559 --> 00:18:55,280
thing it means if i specify an option

00:18:52,799 --> 00:18:57,039
the old code will not use this option

00:18:55,280 --> 00:18:59,440
should it i think it may work actually

00:18:57,039 --> 00:19:00,720
i don't know what other people think

00:18:59,440 --> 00:19:02,799
don't specify the command

00:19:00,720 --> 00:19:05,600
specify a specific net link attribute

00:19:02,799 --> 00:19:07,919
which says i don't want the stats

00:19:05,600 --> 00:19:09,200
and then old user code will never ask

00:19:07,919 --> 00:19:10,400
for this

00:19:09,200 --> 00:19:12,720
and therefore it's still going to get

00:19:10,400 --> 00:19:15,840
the stats

00:19:12,720 --> 00:19:16,799
yeah but the notification is broadcast

00:19:15,840 --> 00:19:19,840
right

00:19:16,799 --> 00:19:22,240
oh you're talking about events as

00:19:19,840 --> 00:19:25,840
opposed to

00:19:22,240 --> 00:19:25,840
request response

00:19:30,559 --> 00:19:34,000
when a new filter is had it's

00:19:32,240 --> 00:19:36,559
broadcasted that

00:19:34,000 --> 00:19:38,000
this new filter is added so you can

00:19:36,559 --> 00:19:41,280
monitor it choosing tc

00:19:38,000 --> 00:19:43,440
monitor and meanwhile vs still works

00:19:41,280 --> 00:19:44,960
because it's a broadcast

00:19:43,440 --> 00:19:46,799
yeah we're talking about this guy the

00:19:44,960 --> 00:19:49,360
filter notify

00:19:46,799 --> 00:19:49,360
yeah but that

00:19:50,840 --> 00:19:58,480
okay right that won't work here right

00:19:54,960 --> 00:19:58,480
i thought you you're doing the dump

00:19:59,039 --> 00:20:03,520
yeah dump is already quite optimized to

00:20:01,600 --> 00:20:07,840
just dump

00:20:03,520 --> 00:20:07,840
the counters without anything else

00:20:14,240 --> 00:20:18,080
so anybody else with questions for glad

00:20:16,720 --> 00:20:19,440
we can always come back at the end we'll

00:20:18,080 --> 00:20:21,200
have a few more minutes

00:20:19,440 --> 00:20:22,720
hey jamal i have a good question can you

00:20:21,200 --> 00:20:26,320
guys hear me

00:20:22,720 --> 00:20:29,760
yeah so who is this uh this is kiran

00:20:26,320 --> 00:20:32,400
oh hey kevin uh hey lord so i

00:20:29,760 --> 00:20:34,000
i saw that uh the lock removal and that

00:20:32,400 --> 00:20:36,640
is improved the performance

00:20:34,000 --> 00:20:38,640
do a absolute number with respect to the

00:20:36,640 --> 00:20:40,799
filter add from each core

00:20:38,640 --> 00:20:43,360
how much it is improved so let's say if

00:20:40,799 --> 00:20:46,480
it was 50k i'm just picking a number now

00:20:43,360 --> 00:20:49,840
did it become like 100k 200k

00:20:46,480 --> 00:20:49,840
something like that and does it scale

00:20:54,960 --> 00:21:02,159
hello well yes sir

00:20:58,559 --> 00:21:02,159
this question being asked in chat

00:21:06,159 --> 00:21:09,440
okay i'll repeat the question is is

00:21:07,760 --> 00:21:10,640
asking for numbers if you have numbers

00:21:09,440 --> 00:21:13,120
uh

00:21:10,640 --> 00:21:14,320
what was the you should percentage but

00:21:13,120 --> 00:21:16,080
what numbers

00:21:14,320 --> 00:21:18,159
do you have did i say that correctly

00:21:16,080 --> 00:21:20,240
karen yes that's right

00:21:18,159 --> 00:21:22,080
oh do we have that yeah so in this

00:21:20,240 --> 00:21:24,000
presentation

00:21:22,080 --> 00:21:26,799
i specifically did not include numbers

00:21:24,000 --> 00:21:30,559
because now they're heavily dependent on

00:21:26,799 --> 00:21:32,720
which filter which traffic profile do

00:21:30,559 --> 00:21:34,400
you try to parse like basically how many

00:21:32,720 --> 00:21:38,640
actions do you have

00:21:34,400 --> 00:21:40,240
and it's linear to number of actions

00:21:38,640 --> 00:21:43,520
attached to your filter

00:21:40,240 --> 00:21:47,840
so on my system with 10

00:21:43,520 --> 00:21:51,919
tc tasks i get around

00:21:47,840 --> 00:21:55,760
200 000 plus rules per second

00:21:51,919 --> 00:21:57,280
add or delete this more simple rules and

00:21:55,760 --> 00:22:01,520
the more rules i get

00:21:57,280 --> 00:22:03,520
it goes down to 100 000 rules per second

00:22:01,520 --> 00:22:06,080
but it's heavily dependent on how many

00:22:03,520 --> 00:22:09,280
actions do you have

00:22:06,080 --> 00:22:11,600
and this is a collective from alt 10 tc

00:22:09,280 --> 00:22:11,600
tasks

00:22:11,679 --> 00:22:15,840
this is a collective number right 200k

00:22:17,200 --> 00:22:22,080
uh so sorry i guess you can't hear you

00:22:19,039 --> 00:22:22,080
have to be the bridge um

00:22:22,559 --> 00:22:27,919
so is this uh

00:22:25,600 --> 00:22:30,159
repeat that question again is this a

00:22:27,919 --> 00:22:31,679
collective number for across all 10 tc

00:22:30,159 --> 00:22:34,640
tasks

00:22:31,679 --> 00:22:34,960
right so this okay is this the aggregate

00:22:34,640 --> 00:22:36,880
uh

00:22:34,960 --> 00:22:38,159
value for all the 10 tc tasks or just

00:22:36,880 --> 00:22:40,480
for one

00:22:38,159 --> 00:22:40,480
yes

00:22:42,480 --> 00:22:46,400
okay so i don't know why he can't hear

00:22:44,400 --> 00:22:49,039
you uh we may have to talk to the

00:22:46,400 --> 00:22:49,600
oav guys but anyways i i if you guys

00:22:49,039 --> 00:22:52,559
don't mind

00:22:49,600 --> 00:22:52,559
uh julian are you there

00:22:53,120 --> 00:22:58,320
yes okay so uh

00:22:56,960 --> 00:23:00,000
uh if there's one more question we'll

00:22:58,320 --> 00:23:01,039
take it otherwise we can move to the

00:23:00,000 --> 00:23:02,640
next session

00:23:01,039 --> 00:23:05,039
because just to try and maintain the

00:23:02,640 --> 00:23:07,840
time thank you okay

00:23:05,039 --> 00:23:08,720
hi uh i have let i have one more

00:23:07,840 --> 00:23:12,640
question

00:23:08,720 --> 00:23:14,880
so have you this is freedom here

00:23:12,640 --> 00:23:16,480
have you collected any data while

00:23:14,880 --> 00:23:19,280
running ovs

00:23:16,480 --> 00:23:20,480
with dc flower offloads so basically

00:23:19,280 --> 00:23:22,480
there we have

00:23:20,480 --> 00:23:24,080
ads as well as dumps happening in

00:23:22,480 --> 00:23:28,720
parallel right

00:23:24,080 --> 00:23:28,720
so how does that behave in that scenario

00:23:29,120 --> 00:23:35,679
how does filter update impact

00:23:32,159 --> 00:23:37,840
current dumps yeah that

00:23:35,679 --> 00:23:39,440
when we that happens when we do obvious

00:23:37,840 --> 00:23:42,480
kernel offload

00:23:39,440 --> 00:23:44,400
right yeah so

00:23:42,480 --> 00:23:46,640
right now as i described at the

00:23:44,400 --> 00:23:49,039
beginning of presentation

00:23:46,640 --> 00:23:50,960
its concurrent dump does not

00:23:49,039 --> 00:23:52,480
significantly impact the rule update

00:23:50,960 --> 00:23:54,880
anymore

00:23:52,480 --> 00:23:56,000
okay so basically we should be able to

00:23:54,880 --> 00:23:59,279
get around 200k

00:23:56,000 --> 00:24:01,120
per second even when dumping

00:23:59,279 --> 00:24:03,120
so it depends on hardware which you are

00:24:01,120 --> 00:24:05,919
floating to because

00:24:03,120 --> 00:24:07,440
like on you're asking on melanox

00:24:05,919 --> 00:24:09,919
hardware

00:24:07,440 --> 00:24:13,760
because when you do hardware float the

00:24:09,919 --> 00:24:13,760
bottleneck becomes a driver mostly

00:24:16,840 --> 00:24:21,520
okay okay

00:24:18,400 --> 00:24:22,880
um thanks vlad i guess we i don't know

00:24:21,520 --> 00:24:25,919
if we can clap for you but

00:24:22,880 --> 00:24:30,080
uh upload your efforts

00:24:25,919 --> 00:24:30,080
thanks thank you all right

00:24:30,720 --> 00:24:37,679
okay so my name is uh ronnie by an i

00:24:34,320 --> 00:24:40,400
from nvidia i'm here to present

00:24:37,679 --> 00:24:40,720
uh an idea we have of extending the tc

00:24:40,400 --> 00:24:44,080
with

00:24:40,720 --> 00:24:45,120
a five tuple hash offload this is the

00:24:44,080 --> 00:24:47,840
work of me and

00:24:45,120 --> 00:24:48,400
also ronnie frye and also i want to

00:24:47,840 --> 00:24:51,039
thank

00:24:48,400 --> 00:24:53,120
a real leftover which is not appearing

00:24:51,039 --> 00:24:55,120
here in the title

00:24:53,120 --> 00:24:57,840
so first i'm going to talk about a

00:24:55,120 --> 00:25:00,880
programmable a data plane

00:24:57,840 --> 00:25:03,760
and if what it is

00:25:00,880 --> 00:25:05,520
and about the tc how tc can be used to

00:25:03,760 --> 00:25:06,640
implement this uh programmable data

00:25:05,520 --> 00:25:10,320
plane concept

00:25:06,640 --> 00:25:12,960
and when we have tc we also have a tc

00:25:10,320 --> 00:25:14,559
other offload and i'm going to do some

00:25:12,960 --> 00:25:18,480
recap about it

00:25:14,559 --> 00:25:21,520
um as it already exists in

00:25:18,480 --> 00:25:22,240
in the kernel and we just want to extend

00:25:21,520 --> 00:25:24,480
it

00:25:22,240 --> 00:25:26,159
then for the specific use case of

00:25:24,480 --> 00:25:27,520
hashing i'm going to explain a little

00:25:26,159 --> 00:25:30,400
bit about ecmp

00:25:27,520 --> 00:25:31,520
is the the main use case here but we

00:25:30,400 --> 00:25:34,640
have a lot of other

00:25:31,520 --> 00:25:38,720
more general general generalized

00:25:34,640 --> 00:25:42,080
use case for it and last uh we focused

00:25:38,720 --> 00:25:46,159
a little bit about the details

00:25:42,080 --> 00:25:48,720
offloading hash has some pitfalls

00:25:46,159 --> 00:25:50,400
on synchronizing between the the data

00:25:48,720 --> 00:25:53,600
plan and the control plan

00:25:50,400 --> 00:25:56,960
and we talk about it

00:25:53,600 --> 00:25:59,600
okay so about tc and dc offload

00:25:56,960 --> 00:26:00,320
and programmable data plan so the

00:25:59,600 --> 00:26:03,440
concept of

00:26:00,320 --> 00:26:04,480
uh programmable data plane is uh i think

00:26:03,440 --> 00:26:07,919
driven from that

00:26:04,480 --> 00:26:10,000
from the understanding that when we have

00:26:07,919 --> 00:26:11,679
a flow let's say you have a cloud and

00:26:10,000 --> 00:26:13,600
you have the web server

00:26:11,679 --> 00:26:16,400
and someone is trying to access the web

00:26:13,600 --> 00:26:18,080
server and download the page

00:26:16,400 --> 00:26:19,440
the first packet that will arrive it

00:26:18,080 --> 00:26:22,559
will be a packet with

00:26:19,440 --> 00:26:25,440
an underlay which

00:26:22,559 --> 00:26:27,440
carried the packets on the

00:26:25,440 --> 00:26:29,679
infrastructure of the cloud maybe it

00:26:27,440 --> 00:26:30,960
would be vixlan geneve or other

00:26:29,679 --> 00:26:33,039
tunneling

00:26:30,960 --> 00:26:34,960
and the first thing that will be done on

00:26:33,039 --> 00:26:36,240
the host after receiving the packet is

00:26:34,960 --> 00:26:39,760
understanding the

00:26:36,240 --> 00:26:42,720
understanding which vm you know

00:26:39,760 --> 00:26:44,080
or which port basically this packets

00:26:42,720 --> 00:26:46,720
belong to

00:26:44,080 --> 00:26:47,840
and it probably strip the packet from

00:26:46,720 --> 00:26:49,760
the vxlan

00:26:47,840 --> 00:26:51,360
and we fold it to the next phase the

00:26:49,760 --> 00:26:53,760
next phase can be

00:26:51,360 --> 00:26:56,159
for example connection tracking if you

00:26:53,760 --> 00:26:59,840
have a security groups you want to

00:26:56,159 --> 00:27:02,159
add some to have some window validation

00:26:59,840 --> 00:27:04,000
or just to uh to make sure that

00:27:02,159 --> 00:27:05,120
connection is coming in and not coming

00:27:04,000 --> 00:27:08,720
out

00:27:05,120 --> 00:27:12,000
and and and last you probably have not

00:27:08,720 --> 00:27:13,200
because usually there is the use of a

00:27:12,000 --> 00:27:17,120
different ips

00:27:13,200 --> 00:27:19,440
locally so the first packet

00:27:17,120 --> 00:27:21,679
we have a series of steps that we need

00:27:19,440 --> 00:27:23,840
to take but

00:27:21,679 --> 00:27:25,840
after we understand this stack this is

00:27:23,840 --> 00:27:28,399
this is the first packet in a stream of

00:27:25,840 --> 00:27:30,080
packet because when we download the page

00:27:28,399 --> 00:27:32,000
we probably are going to download

00:27:30,080 --> 00:27:35,120
content it will open a data

00:27:32,000 --> 00:27:36,159
a tcp flow and will download the the

00:27:35,120 --> 00:27:39,600
data itself

00:27:36,159 --> 00:27:40,000
it will be stream of packets but after

00:27:39,600 --> 00:27:42,080
the

00:27:40,000 --> 00:27:44,000
the first few packets we can say that

00:27:42,080 --> 00:27:46,880
the the the flow is now

00:27:44,000 --> 00:27:49,360
deterministic we have a some kind of

00:27:46,880 --> 00:27:50,720
execution pipeline that we that is it

00:27:49,360 --> 00:27:54,240
will not change during the

00:27:50,720 --> 00:27:57,360
the lifetime of this uh flow

00:27:54,240 --> 00:28:02,159
and this is where program data planning

00:27:57,360 --> 00:28:05,120
comes in we can express it the the the

00:28:02,159 --> 00:28:05,440
demonistic uh part we can express uh

00:28:05,120 --> 00:28:09,840
with

00:28:05,440 --> 00:28:12,080
um a concept of uh tables and rules

00:28:09,840 --> 00:28:13,360
and the programmable data pain have a

00:28:12,080 --> 00:28:15,600
tables each

00:28:13,360 --> 00:28:16,799
has an id and for in each table we have

00:28:15,600 --> 00:28:19,360
the rules

00:28:16,799 --> 00:28:20,559
each rule is consisted of matches and

00:28:19,360 --> 00:28:23,600
actions

00:28:20,559 --> 00:28:24,159
match can be usually it will be a part

00:28:23,600 --> 00:28:26,880
of the

00:28:24,159 --> 00:28:28,960
of the packet and fields like the ip

00:28:26,880 --> 00:28:29,520
adder the mac addresses if it's tunnel

00:28:28,960 --> 00:28:33,360
it can be

00:28:29,520 --> 00:28:35,840
vni and so on it can also be a metadata

00:28:33,360 --> 00:28:36,399
for example after we sweep the packet

00:28:35,840 --> 00:28:39,679
and

00:28:36,399 --> 00:28:42,080
the metadata itself is still kept in

00:28:39,679 --> 00:28:44,960
somewhere and we can compare it

00:28:42,080 --> 00:28:47,200
when the processing continue and other

00:28:44,960 --> 00:28:49,120
actions can be modified the header

00:28:47,200 --> 00:28:51,120
for example when when we have not we

00:28:49,120 --> 00:28:52,320
will modify the source ip or destination

00:28:51,120 --> 00:28:55,840
api port

00:28:52,320 --> 00:28:58,399
and so on and last is

00:28:55,840 --> 00:28:59,120
steering action steering actions means

00:28:58,399 --> 00:29:01,919
that

00:28:59,120 --> 00:29:03,760
we can tell what is the next table in

00:29:01,919 --> 00:29:06,240
the pipeline that we need

00:29:03,760 --> 00:29:07,039
to execute or it could be if this is the

00:29:06,240 --> 00:29:09,600
last table

00:29:07,039 --> 00:29:10,640
we can take a decision about forwarding

00:29:09,600 --> 00:29:13,360
the packet to

00:29:10,640 --> 00:29:14,880
the vm to the where it will be handled

00:29:13,360 --> 00:29:16,159
and given to the web service in this

00:29:14,880 --> 00:29:18,559
case in this example

00:29:16,159 --> 00:29:19,919
or we can take an other action of drop

00:29:18,559 --> 00:29:22,640
and so on

00:29:19,919 --> 00:29:24,000
so we have a control plane which is uh

00:29:22,640 --> 00:29:27,279
can be more complex

00:29:24,000 --> 00:29:29,919
um that learned the first flow and

00:29:27,279 --> 00:29:31,520
we configured we like with sdn

00:29:29,919 --> 00:29:34,880
controller maybe open flow

00:29:31,520 --> 00:29:38,080
and we have a very simple data plane and

00:29:34,880 --> 00:29:39,840
and the data plane is simple it means

00:29:38,080 --> 00:29:44,240
that we can

00:29:39,840 --> 00:29:44,240
implement it also in in hardware

00:29:45,760 --> 00:29:51,600
okay so a few years ago and the

00:29:48,799 --> 00:29:55,039
community started to

00:29:51,600 --> 00:29:56,799
to implement this kind of um

00:29:55,039 --> 00:29:58,320
let's start with the beginning this is

00:29:56,799 --> 00:30:01,760
existing for a long time but

00:29:58,320 --> 00:30:05,120
tc was chosen as the the right tool

00:30:01,760 --> 00:30:06,640
of implementing programmable data plan

00:30:05,120 --> 00:30:09,200
and with also hardware

00:30:06,640 --> 00:30:09,679
offload support the reason for that is

00:30:09,200 --> 00:30:13,120
that this

00:30:09,679 --> 00:30:16,480
is first in line except xdp

00:30:13,120 --> 00:30:19,120
and um and it already has a lot of

00:30:16,480 --> 00:30:20,720
a classifier from this classifier and tc

00:30:19,120 --> 00:30:23,039
flowers chosen because

00:30:20,720 --> 00:30:24,480
it has a very flexible key structure

00:30:23,039 --> 00:30:27,919
that you can extend

00:30:24,480 --> 00:30:31,600
and you can express a lot of matches

00:30:27,919 --> 00:30:34,159
different matches with this key and

00:30:31,600 --> 00:30:35,520
tc already has a lot of different

00:30:34,159 --> 00:30:38,240
actions

00:30:35,520 --> 00:30:41,120
they can be used for the steering for

00:30:38,240 --> 00:30:44,240
the steering or for modifying a packet

00:30:41,120 --> 00:30:45,200
so it has very expressive language you

00:30:44,240 --> 00:30:48,480
can say

00:30:45,200 --> 00:30:52,240
and it's also a linux tool

00:30:48,480 --> 00:30:54,399
and also vendor agnostic so um

00:30:52,240 --> 00:30:56,399
you can implement using the tc you can

00:30:54,399 --> 00:30:57,200
implement a programmable database a data

00:30:56,399 --> 00:30:59,840
plane

00:30:57,200 --> 00:31:01,120
using the tc software only and if you

00:30:59,840 --> 00:31:03,840
have a nick that support

00:31:01,120 --> 00:31:05,279
hardware offload this those rules that

00:31:03,840 --> 00:31:07,519
you configure can be

00:31:05,279 --> 00:31:09,600
offloaded and packet will be handled

00:31:07,519 --> 00:31:11,840
complete completely in hardware

00:31:09,600 --> 00:31:13,679
so here is an example this is a i think

00:31:11,840 --> 00:31:16,320
a very famous one

00:31:13,679 --> 00:31:16,720
how you add a flow for fixed and decap

00:31:16,320 --> 00:31:20,080
okay

00:31:16,720 --> 00:31:22,559
you see that we configure a tca uh

00:31:20,080 --> 00:31:24,559
flower a rule uh for matching the

00:31:22,559 --> 00:31:26,320
destination mark source mark

00:31:24,559 --> 00:31:28,320
uh source ip and destination appear of

00:31:26,320 --> 00:31:30,480
the tunnel itself and also there

00:31:28,320 --> 00:31:32,159
there is the destination port in this

00:31:30,480 --> 00:31:35,919
case four seven

00:31:32,159 --> 00:31:39,279
eight nine which is the the default

00:31:35,919 --> 00:31:42,240
vxlan there is the vni

00:31:39,279 --> 00:31:42,960
which is a 16 in this case and the

00:31:42,240 --> 00:31:46,480
action is

00:31:42,960 --> 00:31:49,679
a decap unset that the tunnel and

00:31:46,480 --> 00:31:52,799
the next action is sent to the vm so

00:31:49,679 --> 00:31:54,720
packets with this vni with v916 from

00:31:52,799 --> 00:32:00,080
this source

00:31:54,720 --> 00:32:02,080
ipn destination ip will go to this spot

00:32:00,080 --> 00:32:03,519
of course this is a very simple rule

00:32:02,080 --> 00:32:07,279
there is no way

00:32:03,519 --> 00:32:10,000
next table but a tc also is a

00:32:07,279 --> 00:32:10,960
go to chain which can be which can

00:32:10,000 --> 00:32:14,320
express a

00:32:10,960 --> 00:32:16,159
table so

00:32:14,320 --> 00:32:17,840
um there's a lot of benefit in

00:32:16,159 --> 00:32:19,919
offloading later the

00:32:17,840 --> 00:32:21,919
the data plan and the benefit is mainly

00:32:19,919 --> 00:32:25,600
performance here is a simple

00:32:21,919 --> 00:32:28,960
example we tested and

00:32:25,600 --> 00:32:30,720
this is a test that we did vxlan with

00:32:28,960 --> 00:32:34,480
the connection tracking we do

00:32:30,720 --> 00:32:37,039
a two hundred thousand flows

00:32:34,480 --> 00:32:37,919
hundred thousand flows we do have the

00:32:37,039 --> 00:32:40,799
different

00:32:37,919 --> 00:32:43,279
tcp and udp sessions concurrently when

00:32:40,799 --> 00:32:45,440
we run packet from each flow

00:32:43,279 --> 00:32:46,720
like this is the the worst case there is

00:32:45,440 --> 00:32:49,760
there's there's a

00:32:46,720 --> 00:32:50,559
cache missed for each a packet as you

00:32:49,760 --> 00:32:53,840
can see the

00:32:50,559 --> 00:32:57,679
the the the number of packets

00:32:53,840 --> 00:32:58,320
you can reach um using offer the 27

00:32:57,679 --> 00:33:01,360
million

00:32:58,320 --> 00:33:02,080
packets per second doing that with the

00:33:01,360 --> 00:33:05,600
software

00:33:02,080 --> 00:33:07,679
and let's say software

00:33:05,600 --> 00:33:09,519
with connection tracking a core can do

00:33:07,679 --> 00:33:12,559
maybe 500k

00:33:09,519 --> 00:33:13,760
even less this depends without this is a

00:33:12,559 --> 00:33:14,559
call without hyperthread with

00:33:13,760 --> 00:33:17,760
hyperthread

00:33:14,559 --> 00:33:21,760
it's half so and this is also a lower

00:33:17,760 --> 00:33:24,559
uh i'm giving a higher rate than

00:33:21,760 --> 00:33:25,519
where we saw but you can see that for

00:33:24,559 --> 00:33:28,640
yourself that for

00:33:25,519 --> 00:33:30,159
pushing 27 million packets per second

00:33:28,640 --> 00:33:34,480
you you will have to use at least

00:33:30,159 --> 00:33:36,720
54 calls and those are host codes that

00:33:34,480 --> 00:33:38,559
can be used for other purposes

00:33:36,720 --> 00:33:41,679
when you're doing offload you use zero

00:33:38,559 --> 00:33:43,360
cpu cycles for pushing this traffic

00:33:41,679 --> 00:33:45,440
of course you'll also have the the

00:33:43,360 --> 00:33:46,559
controller part that you need that to

00:33:45,440 --> 00:33:48,960
learn the first flow

00:33:46,559 --> 00:33:50,960
okay of the part that you learn and the

00:33:48,960 --> 00:33:52,080
part that you configure but i feel after

00:33:50,960 --> 00:33:54,159
you configure

00:33:52,080 --> 00:33:55,360
everything run in in hardware so you

00:33:54,159 --> 00:33:57,840
have to maybe invest

00:33:55,360 --> 00:33:59,760
two three cores for the control and all

00:33:57,840 --> 00:34:01,279
the rest is done in hardware so this is

00:33:59,760 --> 00:34:04,720
the benefit

00:34:01,279 --> 00:34:07,360
um okay now we go for

00:34:04,720 --> 00:34:08,240
for the ecmps i just want to take the

00:34:07,360 --> 00:34:11,919
time

00:34:08,240 --> 00:34:15,760
um so ecmp equal costs

00:34:11,919 --> 00:34:19,440
multipaths okay this is a very a

00:34:15,760 --> 00:34:21,760
known problem

00:34:19,440 --> 00:34:23,760
you have a network we have multiple

00:34:21,760 --> 00:34:25,520
routers and you have

00:34:23,760 --> 00:34:28,159
two nodes that are connecting between

00:34:25,520 --> 00:34:31,520
them there's the source and destination

00:34:28,159 --> 00:34:34,639
and the packet can go through router a

00:34:31,520 --> 00:34:37,440
router b around the c from l three

00:34:34,639 --> 00:34:40,480
perspective this is the same

00:34:37,440 --> 00:34:44,399
distance the same number of hops so

00:34:40,480 --> 00:34:47,599
all are all considered as the best paths

00:34:44,399 --> 00:34:50,240
um if we would choose

00:34:47,599 --> 00:34:50,720
just one of them uh for example always

00:34:50,240 --> 00:34:53,280
go

00:34:50,720 --> 00:34:54,240
go to router a and we have other nodes

00:34:53,280 --> 00:34:57,040
this is

00:34:54,240 --> 00:34:57,599
of course a very simplified um drawing

00:34:57,040 --> 00:34:59,359
they

00:34:57,599 --> 00:35:01,359
will have a much more will have a lot of

00:34:59,359 --> 00:35:04,720
nodes and if for example

00:35:01,359 --> 00:35:07,599
another source which is also route right

00:35:04,720 --> 00:35:08,880
and we will get rotary uh pushing all

00:35:07,599 --> 00:35:12,000
the traffic and router b

00:35:08,880 --> 00:35:15,440
and c will not see any packet

00:35:12,000 --> 00:35:18,160
so to achieve much better utilization uh

00:35:15,440 --> 00:35:20,240
with equal cost multipass in this case

00:35:18,160 --> 00:35:21,920
we split the traffic we send some of

00:35:20,240 --> 00:35:24,880
the traffic throughout our a sound to

00:35:21,920 --> 00:35:28,000
the b and some to see

00:35:24,880 --> 00:35:30,880
and when we do that we have to

00:35:28,000 --> 00:35:32,079
to make sure that packets arrive on the

00:35:30,880 --> 00:35:33,920
same order that

00:35:32,079 --> 00:35:36,560
they were sent at least for the same

00:35:33,920 --> 00:35:38,560
session okay if we have a tcp session we

00:35:36,560 --> 00:35:40,400
cannot send some of the packet to router

00:35:38,560 --> 00:35:44,079
any sound some to be

00:35:40,400 --> 00:35:47,280
because on a on a local time uh

00:35:44,079 --> 00:35:49,760
there might be a different load

00:35:47,280 --> 00:35:51,839
on the routers so router a might have

00:35:49,760 --> 00:35:53,119
different latency it will be more loaded

00:35:51,839 --> 00:35:56,720
than router b

00:35:53,119 --> 00:35:59,280
so packets might arrive out of order

00:35:56,720 --> 00:36:00,400
um if this is only one two packets or a

00:35:59,280 --> 00:36:03,359
very short time

00:36:00,400 --> 00:36:04,079
it might not affect tcp but uh if we

00:36:03,359 --> 00:36:07,440
have a lot of

00:36:04,079 --> 00:36:09,680
reorders tcp might consider this as

00:36:07,440 --> 00:36:11,119
bucket loss and when you have packet

00:36:09,680 --> 00:36:13,359
loss there there are

00:36:11,119 --> 00:36:14,560
re-transmissions of packets and the

00:36:13,359 --> 00:36:16,240
throughput is is

00:36:14,560 --> 00:36:18,800
dropping and of course we don't want

00:36:16,240 --> 00:36:21,839
that uh udp

00:36:18,800 --> 00:36:23,760
it doesn't have a data ordering but

00:36:21,839 --> 00:36:26,000
when we look at protocols that are using

00:36:23,760 --> 00:36:28,079
udp on top of it

00:36:26,000 --> 00:36:30,960
and it's also required for example if

00:36:28,079 --> 00:36:32,960
you have rtp and you have a video or

00:36:30,960 --> 00:36:35,839
a voice and you start to get packets out

00:36:32,960 --> 00:36:39,040
of order it will affect the quality

00:36:35,839 --> 00:36:41,760
and jitter and if you have

00:36:39,040 --> 00:36:42,160
uh also there are protocols like google

00:36:41,760 --> 00:36:45,119
quick

00:36:42,160 --> 00:36:45,440
that are completely and like tcp it uses

00:36:45,119 --> 00:36:48,800
the

00:36:45,440 --> 00:36:51,040
udp as the underlayer but the protocol

00:36:48,800 --> 00:36:52,560
itself has retransmission and sequence

00:36:51,040 --> 00:36:55,680
as congestion

00:36:52,560 --> 00:36:56,480
control and so on so we must keep the

00:36:55,680 --> 00:37:00,000
order

00:36:56,480 --> 00:37:00,000
on both udp and tcp

00:37:01,119 --> 00:37:05,760
so how we do that the most simple way of

00:37:03,760 --> 00:37:08,720
doing that or the first thing that come

00:37:05,760 --> 00:37:10,400
comes in mind is that you build a table

00:37:08,720 --> 00:37:11,599
and when you have a new flow you look at

00:37:10,400 --> 00:37:15,359
that table

00:37:11,599 --> 00:37:17,119
and you look for the this five tuple

00:37:15,359 --> 00:37:18,400
if you find this five tuple you know

00:37:17,119 --> 00:37:21,520
what is the node

00:37:18,400 --> 00:37:22,320
if not you allocate the next node maybe

00:37:21,520 --> 00:37:25,599
in round

00:37:22,320 --> 00:37:28,880
robin and and keep it and the

00:37:25,599 --> 00:37:30,480
following packets uh um

00:37:28,880 --> 00:37:32,960
you will look you will look up for the

00:37:30,480 --> 00:37:35,680
five tuple and you find the same node

00:37:32,960 --> 00:37:37,359
of course this is uh not a very

00:37:35,680 --> 00:37:39,760
performance

00:37:37,359 --> 00:37:40,800
or very effective method of doing that

00:37:39,760 --> 00:37:43,920
because you have to keep

00:37:40,800 --> 00:37:45,040
all this formation you have to now do

00:37:43,920 --> 00:37:48,160
aging

00:37:45,040 --> 00:37:50,079
and on the on

00:37:48,160 --> 00:37:51,280
on the floors because some of the flaws

00:37:50,079 --> 00:37:53,599
will not terminate

00:37:51,280 --> 00:37:55,200
normally also for udp don't have

00:37:53,599 --> 00:37:58,079
domination at all

00:37:55,200 --> 00:37:59,200
um if there is a change in the network

00:37:58,079 --> 00:38:02,400
you now have to

00:37:59,200 --> 00:38:05,839
scan all the um the entries in the table

00:38:02,400 --> 00:38:06,640
and find the table that are related to

00:38:05,839 --> 00:38:09,520
the node that

00:38:06,640 --> 00:38:10,400
was changed and update them this is not

00:38:09,520 --> 00:38:14,000
efficient

00:38:10,400 --> 00:38:16,880
a much better um method or much

00:38:14,000 --> 00:38:19,280
uh more resilient method of doing that

00:38:16,880 --> 00:38:21,920
is like um

00:38:19,280 --> 00:38:23,760
similar to hash table we do the hash

00:38:21,920 --> 00:38:24,720
calculation in separate we have the

00:38:23,760 --> 00:38:27,359
packet

00:38:24,720 --> 00:38:28,160
we calculate hash on five tuple we have

00:38:27,359 --> 00:38:30,880
a number

00:38:28,160 --> 00:38:32,000
we're doing a bit mask on that now and

00:38:30,880 --> 00:38:33,680
jump to one of

00:38:32,000 --> 00:38:36,560
the of the brackets we have a list of

00:38:33,680 --> 00:38:38,480
packets or array of brackets

00:38:36,560 --> 00:38:39,680
and in each bracket well what is the

00:38:38,480 --> 00:38:42,079
next action

00:38:39,680 --> 00:38:43,040
in ecmp it will go you will go to the

00:38:42,079 --> 00:38:45,119
next node by

00:38:43,040 --> 00:38:47,119
probably setting the mac address the

00:38:45,119 --> 00:38:49,130
right mac address and

00:38:47,119 --> 00:38:50,320
and sending to the wire um

00:38:49,130 --> 00:38:52,720
[Music]

00:38:50,320 --> 00:38:55,040
for other use cases you might choose to

00:38:52,720 --> 00:38:57,200
do something else

00:38:55,040 --> 00:38:59,599
so and with this method you have the

00:38:57,200 --> 00:39:02,880
cons the same consistency because each

00:38:59,599 --> 00:39:04,720
flow will always go to the same bucket

00:39:02,880 --> 00:39:05,920
and you don't need to save anything

00:39:04,720 --> 00:39:07,839
beside the buckets

00:39:05,920 --> 00:39:09,760
themselves and it would be a small

00:39:07,839 --> 00:39:13,200
number even if we would uh

00:39:09,760 --> 00:39:16,560
as we see we will we will use the

00:39:13,200 --> 00:39:18,160
in array of buckets it's probably 256

00:39:16,560 --> 00:39:20,800
maybe more

00:39:18,160 --> 00:39:22,640
but it's a very small number if you need

00:39:20,800 --> 00:39:24,000
an update so you update 10 buckets 20

00:39:22,640 --> 00:39:26,720
buckets it's not like

00:39:24,000 --> 00:39:27,520
updating thousands of flows there is no

00:39:26,720 --> 00:39:31,920
aging

00:39:27,520 --> 00:39:34,640
and so on so it's much more simple

00:39:31,920 --> 00:39:36,160
here is an example um of how we can

00:39:34,640 --> 00:39:39,280
implement

00:39:36,160 --> 00:39:40,160
the hashing with with this uh hash

00:39:39,280 --> 00:39:42,880
method

00:39:40,160 --> 00:39:45,599
okay we have a list of of packets

00:39:42,880 --> 00:39:49,520
consistent hash means that if you have

00:39:45,599 --> 00:39:53,520
a change change in the hash

00:39:49,520 --> 00:39:56,640
it affects only a k divided by n

00:39:53,520 --> 00:39:59,920
of the of the entries in the hash

00:39:56,640 --> 00:40:02,480
okay um it would try to be as

00:39:59,920 --> 00:40:03,119
consistable the more nodes you have and

00:40:02,480 --> 00:40:05,119
the less

00:40:03,119 --> 00:40:06,720
entries that will be affected by a

00:40:05,119 --> 00:40:09,920
change and

00:40:06,720 --> 00:40:11,599
you want that most of the of the entries

00:40:09,920 --> 00:40:13,920
will remain the same

00:40:11,599 --> 00:40:14,800
so in consistent hashing we have your

00:40:13,920 --> 00:40:17,119
node d

00:40:14,800 --> 00:40:18,480
and let's say there is a update in the

00:40:17,119 --> 00:40:21,839
in the network and node d

00:40:18,480 --> 00:40:24,319
is now a removal we need to do is

00:40:21,839 --> 00:40:25,200
update all the brackets that has node

00:40:24,319 --> 00:40:28,400
and changing

00:40:25,200 --> 00:40:30,400
to a to another node

00:40:28,400 --> 00:40:33,119
this means that all the green buckets

00:40:30,400 --> 00:40:35,440
node a b and c will remain the same

00:40:33,119 --> 00:40:36,400
we will not have packet reorder and we

00:40:35,440 --> 00:40:40,240
will not

00:40:36,400 --> 00:40:42,960
have any effect on on the traffic

00:40:40,240 --> 00:40:44,079
and then instead of not d we'll choose

00:40:42,960 --> 00:40:45,760
other nodes

00:40:44,079 --> 00:40:47,200
probably better to do it in a balanced

00:40:45,760 --> 00:40:49,119
way so here

00:40:47,200 --> 00:40:50,800
we see that the first one was replaced

00:40:49,119 --> 00:40:54,400
by node a second way not b

00:40:50,800 --> 00:40:57,040
and third by node c of course the high

00:40:54,400 --> 00:40:58,240
number of buckets uh will enable you

00:40:57,040 --> 00:41:00,720
also to have weights

00:40:58,240 --> 00:41:02,880
you can allocate seventy percent of the

00:41:00,720 --> 00:41:04,880
packets for node a and thirty percent of

00:41:02,880 --> 00:41:08,000
the brackets for the rest

00:41:04,880 --> 00:41:09,680
and so on when you add a new

00:41:08,000 --> 00:41:11,440
node let's say there's another update on

00:41:09,680 --> 00:41:14,240
the network and you want to

00:41:11,440 --> 00:41:15,599
to add a new node uh you have to choose

00:41:14,240 --> 00:41:18,720
which bracket you change

00:41:15,599 --> 00:41:22,640
here we took three buckets one from a

00:41:18,720 --> 00:41:25,920
one for b and one from c and now

00:41:22,640 --> 00:41:27,440
there we go to node e the new node and

00:41:25,920 --> 00:41:30,400
only this traffic

00:41:27,440 --> 00:41:32,400
will change so it will not prevent uh

00:41:30,400 --> 00:41:34,880
real though but the reorder happens

00:41:32,400 --> 00:41:35,520
on network update and only for very

00:41:34,880 --> 00:41:38,079
short

00:41:35,520 --> 00:41:39,200
time in the the transition time and the

00:41:38,079 --> 00:41:41,040
assumption that

00:41:39,200 --> 00:41:42,319
removing and adding node nodes in

00:41:41,040 --> 00:41:45,119
network is much

00:41:42,319 --> 00:41:46,720
slower than the the rate that you open

00:41:45,119 --> 00:41:49,520
session in closed session and

00:41:46,720 --> 00:41:49,520
slow duration

00:41:50,640 --> 00:41:55,760
so we can generalize uh this hashing and

00:41:54,480 --> 00:41:59,040
have a much more

00:41:55,760 --> 00:42:02,480
complex or other use cases that

00:41:59,040 --> 00:42:03,680
will use the the hash and here is an

00:42:02,480 --> 00:42:06,880
example we want to do

00:42:03,680 --> 00:42:08,480
uh redundancy so we have

00:42:06,880 --> 00:42:10,960
vm number one and we have number two

00:42:08,480 --> 00:42:13,440
that are sending traffic the host is

00:42:10,960 --> 00:42:14,839
has two routers connected to him active

00:42:13,440 --> 00:42:18,000
active

00:42:14,839 --> 00:42:20,480
um and in doing a hash

00:42:18,000 --> 00:42:21,920
and choosing route array on router v

00:42:20,480 --> 00:42:25,280
according to the hash

00:42:21,920 --> 00:42:28,079
if one of the router is going down it

00:42:25,280 --> 00:42:29,280
will update the buckets and that the the

00:42:28,079 --> 00:42:32,480
traffic would be

00:42:29,280 --> 00:42:35,359
diverted to the active one and

00:42:32,480 --> 00:42:36,560
the vm has no idea about that this is an

00:42:35,359 --> 00:42:39,599
infrastructure

00:42:36,560 --> 00:42:41,599
and also you don't need the special

00:42:39,599 --> 00:42:43,599
feature or something like that you just

00:42:41,599 --> 00:42:45,839
can program the data plane

00:42:43,599 --> 00:42:48,000
to do it easily and there are the use

00:42:45,839 --> 00:42:52,640
cases like service if you if we look on

00:42:48,000 --> 00:42:55,839
a higher level you can have uh

00:42:52,640 --> 00:42:56,480
servers and on congestion time or busy

00:42:55,839 --> 00:42:59,920
time

00:42:56,480 --> 00:43:02,240
you might want to um add more vms

00:42:59,920 --> 00:43:04,079
and and you need to split the traffic

00:43:02,240 --> 00:43:06,960
between them so you have a gateway

00:43:04,079 --> 00:43:07,920
and when you add the ends you had a more

00:43:06,960 --> 00:43:10,839
backer

00:43:07,920 --> 00:43:13,119
or more buckets or updated brackets to

00:43:10,839 --> 00:43:15,839
direct the traffic to more

00:43:13,119 --> 00:43:16,319
nodes to handle the traffic okay there

00:43:15,839 --> 00:43:18,480
are

00:43:16,319 --> 00:43:19,520
there are other use cases so the hashing

00:43:18,480 --> 00:43:23,119
it can be

00:43:19,520 --> 00:43:26,560
generally generalized and using other

00:43:23,119 --> 00:43:28,319
cases as well

00:43:26,560 --> 00:43:30,240
so we talked about what is a

00:43:28,319 --> 00:43:32,720
programmable data plan about the tc and

00:43:30,240 --> 00:43:34,960
ntc how to offload and its benefits we

00:43:32,720 --> 00:43:35,839
talked about ecmp and hashing in in

00:43:34,960 --> 00:43:39,280
general

00:43:35,839 --> 00:43:42,640
and now and we want uh i want to present

00:43:39,280 --> 00:43:45,440
how we can extend tc

00:43:42,640 --> 00:43:45,440
to support that

00:43:47,359 --> 00:43:54,480
so the initial initial suggestion is

00:43:50,960 --> 00:43:57,440
adding a dedicated hash action

00:43:54,480 --> 00:43:59,839
and this would be a new new action in dc

00:43:57,440 --> 00:44:03,359
and and the action

00:43:59,839 --> 00:44:04,000
will calculate on the first apple i will

00:44:03,359 --> 00:44:07,520
talk about

00:44:04,000 --> 00:44:09,119
which hash you can say the which

00:44:07,520 --> 00:44:10,880
are you going to calculate what is the

00:44:09,119 --> 00:44:11,340
hash function

00:44:10,880 --> 00:44:14,000
and

00:44:11,340 --> 00:44:16,880
[Music]

00:44:14,000 --> 00:44:18,079
and we put the the result in the skb

00:44:16,880 --> 00:44:20,560
hash

00:44:18,079 --> 00:44:22,480
and the other side of it is the bucket

00:44:20,560 --> 00:44:23,599
itself so the bracket can be implemented

00:44:22,480 --> 00:44:26,720
by tc flower

00:44:23,599 --> 00:44:29,760
or you go to chain number two okay

00:44:26,720 --> 00:44:31,599
the first tc will calculate the hash

00:44:29,760 --> 00:44:33,040
and we'll jump to the hash table which

00:44:31,599 --> 00:44:36,000
is chain two

00:44:33,040 --> 00:44:37,520
in chain two we will have a match on the

00:44:36,000 --> 00:44:39,680
hash itself so

00:44:37,520 --> 00:44:41,920
in this case the bitmask is for the last

00:44:39,680 --> 00:44:44,400
four beats we have 16 brackets

00:44:41,920 --> 00:44:45,680
in this case it will go to bucket number

00:44:44,400 --> 00:44:48,640
one

00:44:45,680 --> 00:44:51,040
and if you go to action to back at

00:44:48,640 --> 00:44:52,000
number one the in this case the action

00:44:51,040 --> 00:44:55,040
will be

00:44:52,000 --> 00:44:57,839
redirect the packet to this

00:44:55,040 --> 00:44:57,839
this interface

00:44:58,480 --> 00:45:01,839
however and we have a problem with the

00:45:00,800 --> 00:45:05,040
hash function

00:45:01,839 --> 00:45:07,440
itself and since we have a

00:45:05,040 --> 00:45:09,040
control plane and data plane or

00:45:07,440 --> 00:45:10,880
programmable data plane

00:45:09,040 --> 00:45:12,240
you know if we have auto offload it's

00:45:10,880 --> 00:45:14,960
another layer

00:45:12,240 --> 00:45:15,599
and the question is how what which what

00:45:14,960 --> 00:45:19,119
hash

00:45:15,599 --> 00:45:19,760
are we going to use um the control

00:45:19,119 --> 00:45:22,319
player might

00:45:19,760 --> 00:45:25,040
might choose one hash because it was

00:45:22,319 --> 00:45:28,480
written before this feature because

00:45:25,040 --> 00:45:30,079
um it's very effective hash

00:45:28,480 --> 00:45:32,160
for this use case i don't know what is

00:45:30,079 --> 00:45:34,800
the reason the hardware

00:45:32,160 --> 00:45:36,560
probably have its own hash because for

00:45:34,800 --> 00:45:38,079
hardware is usually different than

00:45:36,560 --> 00:45:41,119
software because

00:45:38,079 --> 00:45:43,119
um because of efficiency there are some

00:45:41,119 --> 00:45:45,599
hashes that are more efficient

00:45:43,119 --> 00:45:47,040
that you want to implement and of course

00:45:45,599 --> 00:45:50,400
the software might change

00:45:47,040 --> 00:45:51,520
it's uh it's hashing so uh for example

00:45:50,400 --> 00:45:54,960
in the linux

00:45:51,520 --> 00:45:56,880
usually use a jenkins hash uh

00:45:54,960 --> 00:45:58,319
for the last year i don't know 10 years

00:45:56,880 --> 00:46:00,400
15 years

00:45:58,319 --> 00:46:02,000
and it might be that in the future

00:46:00,400 --> 00:46:04,319
someone will create a

00:46:02,000 --> 00:46:06,000
better hash that they have that will be

00:46:04,319 --> 00:46:07,920
double the performance

00:46:06,000 --> 00:46:09,760
reduced by half the time we take to

00:46:07,920 --> 00:46:11,680
execute or the cycles they take to

00:46:09,760 --> 00:46:14,880
execute the hashing

00:46:11,680 --> 00:46:16,480
and everything will start to use this

00:46:14,880 --> 00:46:19,680
hash

00:46:16,480 --> 00:46:21,440
so the ash itself it's a problem because

00:46:19,680 --> 00:46:23,839
we have different different components

00:46:21,440 --> 00:46:26,800
that are going to use this hash

00:46:23,839 --> 00:46:28,400
and why it is a problem in this case

00:46:26,800 --> 00:46:30,960
here we have an example

00:46:28,400 --> 00:46:32,319
in in sdn there are two use cases the

00:46:30,960 --> 00:46:34,560
the simple use case that

00:46:32,319 --> 00:46:35,760
is you you know your programmable data

00:46:34,560 --> 00:46:38,319
plane in advance

00:46:35,760 --> 00:46:39,359
it's all static and you can configure

00:46:38,319 --> 00:46:41,680
everything

00:46:39,359 --> 00:46:43,760
before before you see the first packet

00:46:41,680 --> 00:46:48,079
in this case you don't have a problem

00:46:43,760 --> 00:46:51,119
because hash is always calculated in the

00:46:48,079 --> 00:46:54,800
in the hardware or on the data plane

00:46:51,119 --> 00:46:58,000
and and we know a

00:46:54,800 --> 00:46:58,720
sync problem but if you in use case

00:46:58,000 --> 00:47:01,599
where you have

00:46:58,720 --> 00:47:03,040
sdn usually in sdn controller you have a

00:47:01,599 --> 00:47:06,160
rules it's very dynamic

00:47:03,040 --> 00:47:07,599
because you're migrating vns vms are

00:47:06,160 --> 00:47:11,200
going down going up

00:47:07,599 --> 00:47:14,480
everything changes all the time

00:47:11,200 --> 00:47:15,040
so usually the data plan is empty on

00:47:14,480 --> 00:47:17,040
start

00:47:15,040 --> 00:47:18,880
and when you see traffic the first

00:47:17,040 --> 00:47:21,200
traffic you see and you don't have

00:47:18,880 --> 00:47:22,640
the data plan has no rule to what to do

00:47:21,200 --> 00:47:25,040
with this packet

00:47:22,640 --> 00:47:26,720
the packet will go to the control and

00:47:25,040 --> 00:47:29,200
the control will consult

00:47:26,720 --> 00:47:29,839
with the configuration that can be open

00:47:29,200 --> 00:47:34,319
flow

00:47:29,839 --> 00:47:37,040
or or other specific implementation

00:47:34,319 --> 00:47:38,800
and it will generate the pipeline the

00:47:37,040 --> 00:47:42,240
data plane pipeline

00:47:38,800 --> 00:47:42,240
so if we follow this

00:47:43,280 --> 00:47:47,920
and see what happens the first packet

00:47:46,880 --> 00:47:51,440
will not match

00:47:47,920 --> 00:47:54,880
in hardware because there is no buckets

00:47:51,440 --> 00:47:57,359
or yeah or in tc if you go to software

00:47:54,880 --> 00:47:57,920
the software will calculate some kind of

00:47:57,359 --> 00:48:00,319
hash

00:47:57,920 --> 00:48:02,960
and for the software the result was it

00:48:00,319 --> 00:48:05,200
is going to back it number zero

00:48:02,960 --> 00:48:06,079
now it generates the data plane

00:48:05,200 --> 00:48:09,440
configuration

00:48:06,079 --> 00:48:12,559
the data plane configuration uh

00:48:09,440 --> 00:48:13,599
is done and the next packet will hit the

00:48:12,559 --> 00:48:16,160
data plane

00:48:13,599 --> 00:48:17,040
now there is an there is a hash and and

00:48:16,160 --> 00:48:18,559
the result

00:48:17,040 --> 00:48:20,079
is different because there is a

00:48:18,559 --> 00:48:24,000
different hash

00:48:20,079 --> 00:48:27,599
in the data plane so here it it hit a

00:48:24,000 --> 00:48:30,319
bucket the number uh number two

00:48:27,599 --> 00:48:30,960
and there is no rule in in bucket number

00:48:30,319 --> 00:48:34,880
two

00:48:30,960 --> 00:48:36,960
so it will enter software

00:48:34,880 --> 00:48:39,680
or to control plane so in the control

00:48:36,960 --> 00:48:42,240
plane we we calculate again the hash

00:48:39,680 --> 00:48:43,440
and we hit bucket number zero and this

00:48:42,240 --> 00:48:45,760
will not converge

00:48:43,440 --> 00:48:46,480
the result of this is that your data

00:48:45,760 --> 00:48:48,800
plane

00:48:46,480 --> 00:48:49,520
is not the programmable data plan is not

00:48:48,800 --> 00:48:51,760
configured

00:48:49,520 --> 00:48:53,040
correctly and you see all the traffic in

00:48:51,760 --> 00:48:56,400
the control

00:48:53,040 --> 00:48:59,680
and the control is the slowest path

00:48:56,400 --> 00:49:02,640
in in the system okay it's more complex

00:48:59,680 --> 00:49:04,079
and this is why it's more slow and the

00:49:02,640 --> 00:49:07,440
data plays simple and

00:49:04,079 --> 00:49:10,640
fast and in this case we cannot use it

00:49:07,440 --> 00:49:13,280
so what is the solution that we are

00:49:10,640 --> 00:49:13,280
suggesting

00:49:14,000 --> 00:49:18,800
and there is very nice thing in the

00:49:15,839 --> 00:49:22,640
kernel that called the edpf or bps

00:49:18,800 --> 00:49:25,040
and you can provide a code that runs

00:49:22,640 --> 00:49:26,480
on on the on the packets and can do a

00:49:25,040 --> 00:49:29,119
lot of stuff

00:49:26,480 --> 00:49:30,640
one of them is calculating hash you can

00:49:29,119 --> 00:49:33,839
implement any hash

00:49:30,640 --> 00:49:37,359
or any common hash that we have crc

00:49:33,839 --> 00:49:40,079
or other jenkins or or i don't know

00:49:37,359 --> 00:49:41,760
the common that that are used you can

00:49:40,079 --> 00:49:45,599
implement it with

00:49:41,760 --> 00:49:49,680
with this code and you can provide it

00:49:45,599 --> 00:49:53,920
as an epf code or bpf vpf code

00:49:49,680 --> 00:49:56,839
together with the uh with the

00:49:53,920 --> 00:49:58,319
data plan and the control point

00:49:56,839 --> 00:50:01,280
configuration

00:49:58,319 --> 00:50:02,480
so you will have the same hash dash is

00:50:01,280 --> 00:50:05,680
defined

00:50:02,480 --> 00:50:09,040
and now a dash is defined

00:50:05,680 --> 00:50:12,640
it is a bpf code and it depends if

00:50:09,040 --> 00:50:15,680
you're doing for example how to offload

00:50:12,640 --> 00:50:17,760
and the hardware can or the driver of

00:50:15,680 --> 00:50:21,280
the hardware can provide you

00:50:17,760 --> 00:50:22,319
the right function that is used for

00:50:21,280 --> 00:50:24,240
hardware you will not

00:50:22,319 --> 00:50:26,079
be able to change how the hardware

00:50:24,240 --> 00:50:30,000
behave but the hardware

00:50:26,079 --> 00:50:30,960
can supply a vpf code that can calculate

00:50:30,000 --> 00:50:33,920
the same hashing

00:50:30,960 --> 00:50:37,040
exactly the same hashing as the hardware

00:50:33,920 --> 00:50:37,040
so once you do that

00:50:37,119 --> 00:50:40,559
and and if if you have a missing into

00:50:39,920 --> 00:50:44,640
software

00:50:40,559 --> 00:50:47,680
you run the the evp for the bpf code

00:50:44,640 --> 00:50:49,839
uh in software and

00:50:47,680 --> 00:50:50,960
the result will be that you will have

00:50:49,839 --> 00:50:54,160
the hash number

00:50:50,960 --> 00:50:55,200
or the result in the skb hash and it

00:50:54,160 --> 00:50:58,480
will go to bucket

00:50:55,200 --> 00:51:02,079
number two you configure the hardware

00:50:58,480 --> 00:51:05,119
and now okay and now

00:51:02,079 --> 00:51:07,359
when after you configured the first

00:51:05,119 --> 00:51:09,040
packet in that will be with that will be

00:51:07,359 --> 00:51:12,240
matched in hardware

00:51:09,040 --> 00:51:12,880
and we calculate the exact same hash

00:51:12,240 --> 00:51:15,040
function

00:51:12,880 --> 00:51:16,400
and as a result it will go to the exact

00:51:15,040 --> 00:51:20,079
same packet

00:51:16,400 --> 00:51:24,400
and we have a synchronization between

00:51:20,079 --> 00:51:24,400
and the hardware and and the software

00:51:25,119 --> 00:51:31,200
okay so uh we suggest to to extend

00:51:28,720 --> 00:51:32,400
the hash and beside saying that this is

00:51:31,200 --> 00:51:35,839
a hash

00:51:32,400 --> 00:51:39,440
uh we also provide um

00:51:35,839 --> 00:51:41,440
how to calculate it okay so in case of

00:51:39,440 --> 00:51:42,480
hardware offload you will take it from

00:51:41,440 --> 00:51:45,680
the from the

00:51:42,480 --> 00:51:49,680
from the driver and in other use cases

00:51:45,680 --> 00:51:52,640
um it it can be a way to sync you can

00:51:49,680 --> 00:51:54,960
think software and the tc to do the same

00:51:52,640 --> 00:51:56,880
maybe your software is old and is doing

00:51:54,960 --> 00:51:58,319
another kind of hashing so you can

00:51:56,880 --> 00:52:00,640
provide the same hashing

00:51:58,319 --> 00:52:01,680
to the tc also in software that you

00:52:00,640 --> 00:52:05,359
would calculate

00:52:01,680 --> 00:52:08,559
the exact same hash and this solves

00:52:05,359 --> 00:52:08,559
the the sync problem

00:52:10,559 --> 00:52:16,640
okay so um to summarize

00:52:14,160 --> 00:52:18,160
we talked about programmable data plane

00:52:16,640 --> 00:52:21,440
as a concept

00:52:18,160 --> 00:52:23,839
and is in it about tc how

00:52:21,440 --> 00:52:24,480
it is it can be the right tool of doing

00:52:23,839 --> 00:52:28,079
that and

00:52:24,480 --> 00:52:29,280
and even more this can be used as a

00:52:28,079 --> 00:52:33,440
hardware offload

00:52:29,280 --> 00:52:34,480
a tool in in the programmable data plan

00:52:33,440 --> 00:52:37,200
can be offloaded

00:52:34,480 --> 00:52:37,920
to the hardware and the benefit of that

00:52:37,200 --> 00:52:41,040
which is

00:52:37,920 --> 00:52:44,400
uh offloading the packet forwarding

00:52:41,040 --> 00:52:47,920
entirely to the hardware and and giving

00:52:44,400 --> 00:52:49,119
the cycles to the host that they can be

00:52:47,920 --> 00:52:52,079
used

00:52:49,119 --> 00:52:54,240
for virtual machine or for anything else

00:52:52,079 --> 00:52:57,040
besides the packet folding

00:52:54,240 --> 00:52:59,440
um we explained a little bit about hash

00:52:57,040 --> 00:53:03,359
use cases when the first

00:52:59,440 --> 00:53:06,400
with the first use case is ecmp

00:53:03,359 --> 00:53:06,720
but the other use case the suggested way

00:53:06,400 --> 00:53:09,200
of

00:53:06,720 --> 00:53:09,839
solving ecmp can be used in other use

00:53:09,200 --> 00:53:12,520
cases

00:53:09,839 --> 00:53:14,240
like redundancy and service load

00:53:12,520 --> 00:53:18,400
balances and

00:53:14,240 --> 00:53:21,520
probably other stuff um

00:53:18,400 --> 00:53:25,200
we presented how we're going to extend

00:53:21,520 --> 00:53:26,480
or suggest to extend tc and to support

00:53:25,200 --> 00:53:29,040
this kind of hashing

00:53:26,480 --> 00:53:30,720
hash and the problem the six the sync

00:53:29,040 --> 00:53:33,359
challenge when you do that

00:53:30,720 --> 00:53:34,640
because you have hardware or data plane

00:53:33,359 --> 00:53:37,760
um

00:53:34,640 --> 00:53:39,040
hash and it might be different than the

00:53:37,760 --> 00:53:41,920
software hash

00:53:39,040 --> 00:53:44,079
and the way for solve that we suggest

00:53:41,920 --> 00:53:48,640
that we use ebpf

00:53:44,079 --> 00:53:48,640
to sync this this hash

00:53:48,720 --> 00:53:52,559
and the net dev conference should have

00:53:50,640 --> 00:53:55,839
been a few months ago

00:53:52,559 --> 00:53:56,480
but we didn't wait so the patches are

00:53:55,839 --> 00:54:00,319
now

00:53:56,480 --> 00:54:03,119
discussed in the community and the main

00:54:00,319 --> 00:54:04,480
rejection that we hear is about that

00:54:03,119 --> 00:54:08,559
there is already

00:54:04,480 --> 00:54:11,280
ebpf action so we can use that

00:54:08,559 --> 00:54:13,520
this is of course true and it's also

00:54:11,280 --> 00:54:16,240
true for other stuff you can do

00:54:13,520 --> 00:54:17,440
a lot of stuff with the bps and and

00:54:16,240 --> 00:54:21,119
there are

00:54:17,440 --> 00:54:25,359
actions and specific actions for dust

00:54:21,119 --> 00:54:27,520
i guess that it's a trade-off

00:54:25,359 --> 00:54:29,280
of between giving the user the

00:54:27,520 --> 00:54:32,319
expressiveness because it's much

00:54:29,280 --> 00:54:35,680
much more easy to configure tc hash

00:54:32,319 --> 00:54:35,680
and know what you are doing

00:54:37,040 --> 00:54:44,000
comparing to uh providing evpf code

00:54:40,400 --> 00:54:45,040
but um and this is the the trade-off and

00:54:44,000 --> 00:54:48,079
this is currently

00:54:45,040 --> 00:54:53,040
discussed in the community

00:54:48,079 --> 00:54:55,119
and that's it thank you for your time

00:54:53,040 --> 00:54:56,079
hey jamal i have a question this is

00:54:55,119 --> 00:55:00,240
anjali oh

00:54:56,079 --> 00:55:02,400
hi uh so the question is uh

00:55:00,240 --> 00:55:04,400
maybe i missed but is there a way to

00:55:02,400 --> 00:55:07,200
specify the seed for the hash as well

00:55:04,400 --> 00:55:10,400
like the key

00:55:07,200 --> 00:55:13,760
you know not just the five tuple

00:55:10,400 --> 00:55:16,799
and then the other question um is uh

00:55:13,760 --> 00:55:19,760
i mean as i understood the the

00:55:16,799 --> 00:55:20,880
the bpf hook is to align the software

00:55:19,760 --> 00:55:24,880
hash algorithm with

00:55:20,880 --> 00:55:24,880
the hardware one um

00:55:25,040 --> 00:55:29,040
can i get the answers for those so i i

00:55:27,680 --> 00:55:32,079
don't know is ron in here

00:55:29,040 --> 00:55:33,440
anybody from melox worked on this no

00:55:32,079 --> 00:55:35,440
i can give you my point of view from

00:55:33,440 --> 00:55:36,400
what i understood would that help i'm

00:55:35,440 --> 00:55:40,160
not sure

00:55:36,400 --> 00:55:40,559
yeah okay well we'll we will make up

00:55:40,160 --> 00:55:43,280
some

00:55:40,559 --> 00:55:44,079
theories about what he's trying to say

00:55:43,280 --> 00:55:49,440
so

00:55:44,079 --> 00:55:52,559
uh my understanding is um

00:55:49,440 --> 00:55:54,400
the there is the software in the in the

00:55:52,559 --> 00:55:56,720
kernel

00:55:54,400 --> 00:55:57,760
has to be aware of what's being offered

00:55:56,720 --> 00:56:00,559
in the hardware

00:55:57,760 --> 00:56:00,960
correct right in order for the hash to

00:56:00,559 --> 00:56:05,280
work

00:56:00,960 --> 00:56:05,280
and effectively

00:56:05,760 --> 00:56:09,200
hash correctly so you can you can you

00:56:07,440 --> 00:56:11,839
can pick the the correct

00:56:09,200 --> 00:56:11,839
values right

00:56:12,960 --> 00:56:19,359
and so i i suspect that

00:56:16,079 --> 00:56:19,359
part of that uh

00:56:19,920 --> 00:56:23,200
policy definition would include this

00:56:22,160 --> 00:56:26,480
this uh

00:56:23,200 --> 00:56:26,480
seed is described

00:56:26,559 --> 00:56:32,799
that's just a suspicion i have no uh

00:56:30,000 --> 00:56:34,400
so the patches are posted and it seems

00:56:32,799 --> 00:56:36,240
that like there's a variety of hash

00:56:34,400 --> 00:56:37,520
algorithms and some of them were to be

00:56:36,240 --> 00:56:40,960
expressed using

00:56:37,520 --> 00:56:43,520
ebpf because uh they may not have made a

00:56:40,960 --> 00:56:58,559
lot of sense for

00:56:43,520 --> 00:57:01,680
software anybody else wants to chime in

00:56:58,559 --> 00:57:04,720
uh sorry marcelo raised his hand

00:57:01,680 --> 00:57:07,280
hey jamal um as i

00:57:04,720 --> 00:57:08,400
was asking on the chat uh one question

00:57:07,280 --> 00:57:11,680
would be

00:57:08,400 --> 00:57:13,040
if this ppf object needs to be in sync

00:57:11,680 --> 00:57:15,680
with the hardware

00:57:13,040 --> 00:57:17,440
who would be distributing it but it

00:57:15,680 --> 00:57:19,440
would be distributed together with the

00:57:17,440 --> 00:57:21,920
driver with linux streamer

00:57:19,440 --> 00:57:23,520
or oh because they need to be maintained

00:57:21,920 --> 00:57:25,520
right

00:57:23,520 --> 00:57:27,200
that's always a problem with bpf right i

00:57:25,520 --> 00:57:29,359
mean it's not really part of

00:57:27,200 --> 00:57:31,200
upstream kernel it may be there may be a

00:57:29,359 --> 00:57:34,400
sample code in

00:57:31,200 --> 00:57:38,079
in tools testing or

00:57:34,400 --> 00:57:38,079
samples ppf but

00:57:39,680 --> 00:57:43,440
that's why i like these simple things

00:57:41,920 --> 00:57:46,319
like this to be part of the camel

00:57:43,440 --> 00:57:46,319
upstream right so

00:57:46,480 --> 00:57:51,599
um i i don't know the answer to that to

00:57:49,280 --> 00:57:51,599
be honest

00:57:52,079 --> 00:57:55,680
but but i think it's a generic problem

00:57:53,680 --> 00:57:58,880
it's not just uh this scenario but

00:57:55,680 --> 00:58:02,400
uh uh

00:57:58,880 --> 00:58:05,440
but possibly uh yeah but

00:58:02,400 --> 00:58:08,720
a vpf the thing in this case that

00:58:05,440 --> 00:58:10,799
we need to have it in sync with the nick

00:58:08,720 --> 00:58:12,640
and more specifically with the nick

00:58:10,799 --> 00:58:14,400
firmer

00:58:12,640 --> 00:58:16,640
so we need to know what the nick is

00:58:14,400 --> 00:58:17,599
capable so that we can use that specific

00:58:16,640 --> 00:58:21,040
bpf file

00:58:17,599 --> 00:58:24,640
with it it's harder dependent

00:58:21,040 --> 00:58:26,880
yeah right sorry guys yeah

00:58:24,640 --> 00:58:28,240
i believe so uh the same thing jamal

00:58:26,880 --> 00:58:30,160
that it is you know you have to know

00:58:28,240 --> 00:58:32,799
what is nick using for you to

00:58:30,160 --> 00:58:33,760
match it up in the bpf program there was

00:58:32,799 --> 00:58:35,920
one more question

00:58:33,760 --> 00:58:38,000
uh you know related to this which was

00:58:35,920 --> 00:58:40,240
about hash balancing

00:58:38,000 --> 00:58:41,119
so what i understood as the hash

00:58:40,240 --> 00:58:43,280
balancing

00:58:41,119 --> 00:58:44,240
uh you know it's more resilient hash

00:58:43,280 --> 00:58:47,119
balance that

00:58:44,240 --> 00:58:48,160
is being done but i couldn't uh gather

00:58:47,119 --> 00:58:52,000
if there was a way

00:58:48,160 --> 00:58:55,359
for uh you know the

00:58:52,000 --> 00:58:58,079
you know for from software to actually

00:58:55,359 --> 00:58:59,280
do the hash balancing themselves you

00:58:58,079 --> 00:59:02,720
know

00:58:59,280 --> 00:59:03,280
um through whatever uh scripts or

00:59:02,720 --> 00:59:05,359
whatever

00:59:03,280 --> 00:59:08,319
they want to use like like in each tool

00:59:05,359 --> 00:59:11,359
we do have the hash that's exposed

00:59:08,319 --> 00:59:13,119
uh i'm not sure that is the case here so

00:59:11,359 --> 00:59:14,000
you're talking about rss hashing in this

00:59:13,119 --> 00:59:16,559
case or

00:59:14,000 --> 00:59:17,359
yeah yeah rss hashing so i mean any hash

00:59:16,559 --> 00:59:19,920
will have

00:59:17,359 --> 00:59:22,000
their lookup table that you can the

00:59:19,920 --> 00:59:25,040
buckets that you could expose

00:59:22,000 --> 00:59:26,000
to balance either uh through the user

00:59:25,040 --> 00:59:29,280
space or

00:59:26,000 --> 00:59:31,200
you know you assume a balancing

00:59:29,280 --> 00:59:32,880
either in the driver or in the hardware

00:59:31,200 --> 00:59:34,640
so i couldn't gather if it was

00:59:32,880 --> 00:59:36,640
completely configurable or it was

00:59:34,640 --> 00:59:39,119
something

00:59:36,640 --> 00:59:39,760
that was not it sounded to me like

00:59:39,119 --> 00:59:41,280
there's

00:59:39,760 --> 00:59:42,880
multiple ways you can do hashing in the

00:59:41,280 --> 00:59:45,680
hardware right

00:59:42,880 --> 00:59:46,240
and you so whoever setting policy and

00:59:45,680 --> 00:59:48,400
software so

00:59:46,240 --> 00:59:49,440
here's my understanding right i like uh

00:59:48,400 --> 00:59:51,920
and

00:59:49,440 --> 00:59:54,000
i'm hoping that's how it works is

00:59:51,920 --> 00:59:54,880
somebody in user space will configure

00:59:54,000 --> 00:59:58,480
the hardware

00:59:54,880 --> 00:59:59,839
to do a hash and set the skb hash so

00:59:58,480 --> 01:00:01,359
when it arrives in software in the

00:59:59,839 --> 01:00:04,160
kernel you already know what

01:00:01,359 --> 01:00:04,799
let's say one two three four means right

01:00:04,160 --> 01:00:07,040
right

01:00:04,799 --> 01:00:08,400
and then you can classify on one two

01:00:07,040 --> 01:00:10,480
three four and the approach was to

01:00:08,400 --> 01:00:13,520
classify using a flower

01:00:10,480 --> 01:00:14,960
so you can say tc flower match hash one

01:00:13,520 --> 01:00:18,000
two three four

01:00:14,960 --> 01:00:20,559
run these extra actions in in the camera

01:00:18,000 --> 01:00:22,079
but the hardware would have tagged that

01:00:20,559 --> 01:00:23,440
hash

01:00:22,079 --> 01:00:25,920
and you could control that and there's

01:00:23,440 --> 01:00:28,720
multiple ways that that hash would be

01:00:25,920 --> 01:00:30,319
created in the hardware and i think the

01:00:28,720 --> 01:00:32,880
attributes of how you download that

01:00:30,319 --> 01:00:34,400
is the example unfortunately provided

01:00:32,880 --> 01:00:35,280
was not very good it just said action

01:00:34,400 --> 01:00:37,520
hash he didn't say

01:00:35,280 --> 01:00:38,880
anything other parameters in addition to

01:00:37,520 --> 01:00:41,280
that

01:00:38,880 --> 01:00:43,839
yeah jamal my question was more related

01:00:41,280 --> 01:00:46,880
to you know once you have done that

01:00:43,839 --> 01:00:50,079
later on you want to do a rebalance

01:00:46,880 --> 01:00:53,280
and what do you do a rebalance yes

01:00:50,079 --> 01:00:55,920
and is the rebalance uh being done as

01:00:53,280 --> 01:00:57,599
a policy in the device or the driver or

01:00:55,920 --> 01:01:00,880
is it something exposed

01:00:57,599 --> 01:01:04,000
so uh so somebody could decide if if a

01:01:00,880 --> 01:01:07,599
new port is added or removed

01:01:04,000 --> 01:01:10,079
how do they want to balance that hash

01:01:07,599 --> 01:01:12,079
i don't know i wish one of is like four

01:01:10,079 --> 01:01:13,760
or five people involved in this work

01:01:12,079 --> 01:01:15,520
and none of them are here and they were

01:01:13,760 --> 01:01:17,839
all i mean i copy them on the agenda i

01:01:15,520 --> 01:01:21,440
think i may have missed somebody but

01:01:17,839 --> 01:01:22,799
um so i don't know the answer to that

01:01:21,440 --> 01:01:24,480
but it's a good question

01:01:22,799 --> 01:01:26,960
i think you would need to deal with that

01:01:24,480 --> 01:01:27,680
right yes you know that or you deal you

01:01:26,960 --> 01:01:31,920
know

01:01:27,680 --> 01:01:35,280
and you know uh or flows go away or

01:01:31,920 --> 01:01:36,839
you have infinite amount of buckets then

01:01:35,280 --> 01:01:38,960
then it's probably not a very big

01:01:36,839 --> 01:01:42,160
problem

01:01:38,960 --> 01:01:44,079
uh yeah okay thanks jamal i think yeah

01:01:42,160 --> 01:01:47,119
yeah

01:01:44,079 --> 01:01:47,839
yeah so so i i i have been i did a poor

01:01:47,119 --> 01:01:50,000
job of

01:01:47,839 --> 01:01:51,040
managing the queue there's a question

01:01:50,000 --> 01:01:54,319
from

01:01:51,040 --> 01:01:57,599
massage on on the chat

01:01:54,319 --> 01:01:58,160
is asking if but do we have do we know

01:01:57,599 --> 01:02:01,760
what

01:01:58,160 --> 01:02:04,160
performance hit evpf would have with the

01:02:01,760 --> 01:02:04,160
hashing

01:02:04,240 --> 01:02:08,480
i don't know the answer to that is any

01:02:07,039 --> 01:02:11,760
evpf guru here

01:02:08,480 --> 01:02:12,799
i mean it this this ebpf will just be a

01:02:11,760 --> 01:02:15,280
phony

01:02:12,799 --> 01:02:17,440
uh hash algorithm software but in

01:02:15,280 --> 01:02:19,839
reality it's trying to be

01:02:17,440 --> 01:02:22,000
mimicked what's running in hardware so i

01:02:19,839 --> 01:02:24,880
think when somebody

01:02:22,000 --> 01:02:26,400
adds the tc rule that rule will probably

01:02:24,880 --> 01:02:28,400
have like skipped software and it will

01:02:26,400 --> 01:02:29,760
go to hardware over

01:02:28,400 --> 01:02:31,280
at least that's my understanding of it

01:02:29,760 --> 01:02:35,839
is anybody else has a different

01:02:31,280 --> 01:02:35,839
understanding of this

01:02:40,160 --> 01:02:45,039
yeah i also think that it is just maybe

01:02:42,799 --> 01:02:45,440
only one packet of a flow will match

01:02:45,039 --> 01:02:48,400
that

01:02:45,440 --> 01:02:49,280
ebps cache program i think right the

01:02:48,400 --> 01:02:53,200
later packets

01:02:49,280 --> 01:02:55,039
will be offloaded to hardware

01:02:53,200 --> 01:02:56,559
okay so it goes the first packet goes to

01:02:55,039 --> 01:02:59,599
software and then that

01:02:56,559 --> 01:03:02,000
installs the policy in the hardware yeah

01:02:59,599 --> 01:03:04,880
check the hash in the skb and then the

01:03:02,000 --> 01:03:06,720
pc there is a filter rule that matches

01:03:04,880 --> 01:03:09,039
that hash and then

01:03:06,720 --> 01:03:12,079
offload it to hardware so that the later

01:03:09,039 --> 01:03:13,839
packets will hit the hardware i think

01:03:12,079 --> 01:03:15,200
but doesn't that make it uh then you

01:03:13,839 --> 01:03:16,799
have to do puff floor then after that

01:03:15,200 --> 01:03:17,839
because you have to say this flow

01:03:16,799 --> 01:03:21,119
matches

01:03:17,839 --> 01:03:23,280
this skb hash right

01:03:21,119 --> 01:03:25,599
yeah that's what i thought that they had

01:03:23,280 --> 01:03:27,839
two filters right the first filter was

01:03:25,599 --> 01:03:30,240
setting the hash and the second filter

01:03:27,839 --> 01:03:35,200
was matching on the hash

01:03:30,240 --> 01:03:39,200
and then redirecting okay

01:03:35,200 --> 01:03:42,640
uh taras raise your hand

01:03:39,200 --> 01:03:43,039
you you you can go ahead please yes so i

01:03:42,640 --> 01:03:46,000
was

01:03:43,039 --> 01:03:46,880
wondering uh for example two different

01:03:46,000 --> 01:03:49,280
devices

01:03:46,880 --> 01:03:50,480
uh from two different vendors might have

01:03:49,280 --> 01:03:52,480
completely

01:03:50,480 --> 01:03:54,079
different hash even from the size

01:03:52,480 --> 01:03:57,280
perspective so like

01:03:54,079 --> 01:04:01,039
16 16 or 32 bits

01:03:57,280 --> 01:04:03,920
and uh keeping this keeping this in sync

01:04:01,039 --> 01:04:05,440
that software okay for ingress we can we

01:04:03,920 --> 01:04:06,720
can definitely know okay this one comes

01:04:05,440 --> 01:04:07,599
from that hardware we should do that

01:04:06,720 --> 01:04:10,640
hash

01:04:07,599 --> 01:04:12,559
but how to apply this for the egress

01:04:10,640 --> 01:04:14,799
for me it's completely unknown i don't

01:04:12,559 --> 01:04:16,400
see how we can easily

01:04:14,799 --> 01:04:19,119
match two different hashes with two

01:04:16,400 --> 01:04:19,119
different sizes

01:04:20,079 --> 01:04:24,240
right anybody wants to chime into that

01:04:24,720 --> 01:04:28,480
i mean yes these are attributes you have

01:04:27,119 --> 01:04:30,000
to know you have to know what

01:04:28,480 --> 01:04:32,640
when you specify a hash what does it

01:04:30,000 --> 01:04:35,280
mean what does which tuples it takes

01:04:32,640 --> 01:04:37,119
to create rss for example is well known

01:04:35,280 --> 01:04:39,440
even with rss

01:04:37,119 --> 01:04:41,359
you could still inject different seeds

01:04:39,440 --> 01:04:43,760
and have different results for rss

01:04:41,359 --> 01:04:45,280
as i think that's what angeli was

01:04:43,760 --> 01:04:47,200
hinting at

01:04:45,280 --> 01:04:48,960
you know if you put it's still the same

01:04:47,200 --> 01:04:50,480
five tuples but different you get

01:04:48,960 --> 01:04:54,160
different values if you

01:04:50,480 --> 01:04:57,359
use different seeds right and then is it

01:04:54,160 --> 01:05:00,480
i mean tricks like that where symmetra's

01:04:57,359 --> 01:05:04,640
symmetric uh hashes are used to

01:05:00,480 --> 01:05:07,200
identify different directions so i i'm

01:05:04,640 --> 01:05:08,000
uh unfortunately there's nothing much we

01:05:07,200 --> 01:05:12,319
can do up here

01:05:08,000 --> 01:05:14,480
so uh it's time-wise

01:05:12,319 --> 01:05:16,160
we still have about 10-15 minutes we can

01:05:14,480 --> 01:05:19,359
continue talking about it but

01:05:16,160 --> 01:05:21,200
i think it's uh so my concern was

01:05:19,359 --> 01:05:22,960
when these patches are posted maybe we

01:05:21,200 --> 01:05:26,400
should

01:05:22,960 --> 01:05:26,400
people to that question is

01:05:26,880 --> 01:05:31,520
they used flour for classifying hash and

01:05:29,680 --> 01:05:34,559
i

01:05:31,520 --> 01:05:37,200
i raised the concern that

01:05:34,559 --> 01:05:38,799
flour tends to be so if i now i have to

01:05:37,200 --> 01:05:39,680
add every other classifier i have to

01:05:38,799 --> 01:05:43,039
extend it

01:05:39,680 --> 01:05:44,240
and make it do uh have skb hash as part

01:05:43,039 --> 01:05:47,599
of its classification so i have to

01:05:44,240 --> 01:05:50,000
i'm using u32 i now have to extend u32

01:05:47,599 --> 01:05:50,880
uh i was kind of concerned about that

01:05:50,000 --> 01:05:53,119
and so i

01:05:50,880 --> 01:05:54,880
posted some patches as well to introduce

01:05:53,119 --> 01:05:57,440
a brand new classifier

01:05:54,880 --> 01:05:59,920
called skb hash or for a lack of a

01:05:57,440 --> 01:05:59,920
better name

01:06:00,960 --> 01:06:04,559
and i so that was one of the reasons i

01:06:03,200 --> 01:06:06,400
didn't want to go and change

01:06:04,559 --> 01:06:09,440
other classifiers i'm using i'm not a

01:06:06,400 --> 01:06:10,880
big use of flower for example

01:06:09,440 --> 01:06:13,440
and the other thing is i don't think the

01:06:10,880 --> 01:06:13,440
performance

01:06:14,160 --> 01:06:17,599
would be the same because the passing in

01:06:16,640 --> 01:06:19,839
flower will be a little bit more

01:06:17,599 --> 01:06:19,839
expensive

01:06:25,440 --> 01:06:33,119
well nobody is uh should i start picking

01:06:28,799 --> 01:06:36,880
people to say something here

01:06:33,119 --> 01:06:36,880
paul do you say uh

01:06:37,440 --> 01:06:41,280
you probably need to separate separate

01:06:39,520 --> 01:06:43,119
secondary tables for the different size

01:06:41,280 --> 01:06:46,319
hashes

01:06:43,119 --> 01:06:48,480
in that in the example he was showing it

01:06:46,319 --> 01:06:51,039
he would have he would he is he used two

01:06:48,480 --> 01:06:54,000
tables the second table was the buckets

01:06:51,039 --> 01:06:55,280
so if you had used different size hashes

01:06:54,000 --> 01:06:57,520
you could just have

01:06:55,280 --> 01:06:59,680
set two different tables for that second

01:06:57,520 --> 01:07:02,160
step they'd all map to the same egress

01:06:59,680 --> 01:07:05,520
ports set of egress ports but

01:07:02,160 --> 01:07:08,400
that might be one approach um

01:07:05,520 --> 01:07:08,799
okay and you're going to have different

01:07:08,400 --> 01:07:10,880
the

01:07:08,799 --> 01:07:12,000
the that i'm assuming you're having

01:07:10,880 --> 01:07:13,760
different nicks

01:07:12,000 --> 01:07:15,920
different input ports so you're actually

01:07:13,760 --> 01:07:18,319
your table the definitions would kind of

01:07:15,920 --> 01:07:20,640
be per port i think anyway so

01:07:18,319 --> 01:07:22,319
the separate size hashes should

01:07:20,640 --> 01:07:24,480
shouldn't necessarily be a problem i

01:07:22,319 --> 01:07:24,480
think

01:07:26,000 --> 01:07:29,680
so the separate hashes would uh what

01:07:28,720 --> 01:07:31,039
would be the impact is that a

01:07:29,680 --> 01:07:33,839
performance impact

01:07:31,039 --> 01:07:35,359
well you might not get a uh your load

01:07:33,839 --> 01:07:37,520
distribution might

01:07:35,359 --> 01:07:38,720
not be as consistent because you're you

01:07:37,520 --> 01:07:42,240
have different

01:07:38,720 --> 01:07:44,000
mechanisms for for load balancing uh

01:07:42,240 --> 01:07:46,240
particular five tuples but they should

01:07:44,000 --> 01:07:47,119
still always map to the same but you

01:07:46,240 --> 01:07:49,440
know to a buck

01:07:47,119 --> 01:07:51,359
to a consistent bucket so no matter what

01:07:49,440 --> 01:07:53,200
you won't have an out of order

01:07:51,359 --> 01:07:56,319
out of order problem you'll just may

01:07:53,200 --> 01:07:58,240
have different load distribution

01:07:56,319 --> 01:07:59,839
taking place i don't think it's

01:07:58,240 --> 01:08:02,000
necessarily a big problem

01:07:59,839 --> 01:08:03,440
that they have different size hashes

01:08:02,000 --> 01:08:06,079
seems like it's manageable in that

01:08:03,440 --> 01:08:06,079
architecture

01:08:07,200 --> 01:08:10,559
so anybody from other nick vendors who

01:08:09,200 --> 01:08:14,000
has

01:08:10,559 --> 01:08:16,319
anjali or jcsu there

01:08:14,000 --> 01:08:17,440
yeah jamal sorry i missed what was the

01:08:16,319 --> 01:08:19,520
question

01:08:17,440 --> 01:08:20,480
no i mean are you guys uh what do you

01:08:19,520 --> 01:08:23,199
guys think of this

01:08:20,480 --> 01:08:24,080
uh implementation this uh talk but

01:08:23,199 --> 01:08:26,880
specifically

01:08:24,080 --> 01:08:27,839
to your hardware basically right so so i

01:08:26,880 --> 01:08:30,560
mean

01:08:27,839 --> 01:08:32,319
definitely a good effort but i think uh

01:08:30,560 --> 01:08:33,279
there is more configuration that needs

01:08:32,319 --> 01:08:36,159
to be exposed

01:08:33,279 --> 01:08:37,120
um particularly as i messaged on the

01:08:36,159 --> 01:08:39,839
chat side

01:08:37,120 --> 01:08:42,480
you know the seed rebalancing policy or

01:08:39,839 --> 01:08:44,960
exposure to hash buckets for balancing

01:08:42,480 --> 01:08:46,799
and then the uh you know i would also

01:08:44,960 --> 01:08:47,600
like the algorithm to be internal

01:08:46,799 --> 01:08:50,719
instead of

01:08:47,600 --> 01:08:53,359
you know through bpf uh and and uh

01:08:50,719 --> 01:08:54,799
yeah i mean performance um you know

01:08:53,359 --> 01:08:58,400
depends on

01:08:54,799 --> 01:09:00,799
um yeah so i mean the way it is being

01:08:58,400 --> 01:09:03,440
done is like only the first packet

01:09:00,799 --> 01:09:05,440
uh is used in software and then uh you

01:09:03,440 --> 01:09:09,440
know the rest of it is in hardware

01:09:05,440 --> 01:09:12,080
which is fine uh and and uh maybe in

01:09:09,440 --> 01:09:14,080
with that respect you know u32 versus

01:09:12,080 --> 01:09:17,520
flower i i don't really know

01:09:14,080 --> 01:09:17,520
if uh there is uh

01:09:18,640 --> 01:09:23,359
any benefit to have because of course

01:09:21,440 --> 01:09:26,000
once it's offloaded the performance is

01:09:23,359 --> 01:09:27,120
hardware performance so yeah but then i

01:09:26,000 --> 01:09:28,799
i would

01:09:27,120 --> 01:09:31,839
you know feel comfortable if we had a

01:09:28,799 --> 01:09:33,759
little bit more configuration available

01:09:31,839 --> 01:09:34,880
and whether i've just been informed that

01:09:33,759 --> 01:09:36,799
uh the reason

01:09:34,880 --> 01:09:38,560
this gentleman are probably not here is

01:09:36,799 --> 01:09:41,839
because it's a weekend

01:09:38,560 --> 01:09:43,440
in this trail right now so they can be

01:09:41,839 --> 01:09:46,319
excused

01:09:43,440 --> 01:09:47,359
uh so your hardware though intel

01:09:46,319 --> 01:09:51,520
hardware

01:09:47,359 --> 01:09:54,320
because would this be useful

01:09:51,520 --> 01:09:55,120
yes for sure which one because i have a

01:09:54,320 --> 01:09:58,880
lot of intel

01:09:55,120 --> 01:09:58,880
xdbe or i40e

01:09:59,040 --> 01:10:03,760
sorry i can't comment okay

01:10:02,159 --> 01:10:07,199
but you're probably gonna add this

01:10:03,760 --> 01:10:07,199
functionality here yes

01:10:08,000 --> 01:10:11,440
all right so i'm marcelo from red hat

01:10:10,640 --> 01:10:16,080
and

01:10:11,440 --> 01:10:19,840
this is a a cultural brainstorm

01:10:16,080 --> 01:10:23,360
when the action ct was added it

01:10:19,840 --> 01:10:23,360
also had the

01:10:23,600 --> 01:10:30,640
the new the new feature let's say to tc

01:10:26,960 --> 01:10:32,800
that it started to defragment ap packets

01:10:30,640 --> 01:10:35,520
because it is needed to do proper

01:10:32,800 --> 01:10:39,679
contract on udp packets

01:10:35,520 --> 01:10:42,400
even though the electricity was added

01:10:39,679 --> 01:10:43,199
because of hardware of load and ip

01:10:42,400 --> 01:10:46,640
fragments

01:10:43,199 --> 01:10:49,199
are hardly offloaded we still have the

01:10:46,640 --> 01:10:53,120
tc software data path that we

01:10:49,199 --> 01:10:56,840
we strive to maintain a

01:10:53,120 --> 01:10:58,880
feature set in there that is well tied

01:10:56,840 --> 01:11:02,719
together

01:10:58,880 --> 01:11:05,199
and then we now we have the problem once

01:11:02,719 --> 01:11:12,640
the packets are defragmented

01:11:05,199 --> 01:11:16,000
how we can output them

01:11:12,640 --> 01:11:18,880
so xct will

01:11:16,000 --> 01:11:20,080
when packets are missing uh triggering

01:11:18,880 --> 01:11:23,520
hardware miss

01:11:20,080 --> 01:11:24,400
it comes to software top it they hit

01:11:23,520 --> 01:11:26,560
activity

01:11:24,400 --> 01:11:27,920
it is ip fragment it's not reassembled

01:11:26,560 --> 01:11:31,040
yet it will

01:11:27,920 --> 01:11:34,480
use the kernel stack that

01:11:31,040 --> 01:11:37,600
we already have to do the fragmentation

01:11:34,480 --> 01:11:41,040
the packs will be stolen until they are

01:11:37,600 --> 01:11:42,640
fully reassembled but once that big

01:11:41,040 --> 01:11:45,679
packet is returned by

01:11:42,640 --> 01:11:46,640
by its dc or more specifically mirrored

01:11:45,679 --> 01:11:48,800
the action

01:11:46,640 --> 01:11:50,000
can't output it because it has no

01:11:48,800 --> 01:11:52,560
knowledge today of

01:11:50,000 --> 01:11:54,000
how to fragment an ip packet it just

01:11:52,560 --> 01:11:57,679
takes an skb it puts

01:11:54,000 --> 01:11:57,679
on a device queue and that's it

01:11:57,920 --> 01:12:03,679
and what will happen is that the net

01:12:01,679 --> 01:12:06,320
device will reject because it's not the

01:12:03,679 --> 01:12:06,320
net device

01:12:07,040 --> 01:12:11,280
responsibility to deal with this level

01:12:09,760 --> 01:12:13,520
of functionality

01:12:11,280 --> 01:12:15,199
and we have no other action today that

01:12:13,520 --> 01:12:19,120
cannot put such packets

01:12:15,199 --> 01:12:19,120
yes not considering act bpf

01:12:19,280 --> 01:12:22,880
the issue was noticed and patch was

01:12:21,440 --> 01:12:25,840
proposed by ranksu

01:12:22,880 --> 01:12:27,840
in upstream the reference is there and

01:12:25,840 --> 01:12:31,440
it was rejected by kong wang

01:12:27,840 --> 01:12:34,640
because in theory tc is l2

01:12:31,440 --> 01:12:35,360
and also shouldn't be messing with l3

01:12:34,640 --> 01:12:39,199
stuff

01:12:35,360 --> 01:12:41,679
more specifically fragmenting packets

01:12:39,199 --> 01:12:42,400
but at the same time this is doing l3

01:12:41,679 --> 01:12:45,280
stuff for

01:12:42,400 --> 01:12:48,239
quite a while it is doing statelessnet

01:12:45,280 --> 01:12:51,760
it is classifying based on routers

01:12:48,239 --> 01:12:56,800
and other stuff but kong didn't want

01:12:51,760 --> 01:12:56,800
uh that patches was proposed

01:12:57,440 --> 01:13:01,840
the current state wangsu proposed a new

01:13:01,199 --> 01:13:05,199
patch

01:13:01,840 --> 01:13:07,760
which got applied it is enough to get it

01:13:05,199 --> 01:13:10,640
working with ovs but only for it

01:13:07,760 --> 01:13:13,520
because it relies on a chemist so that

01:13:10,640 --> 01:13:16,320
the pact will be picked up by obs

01:13:13,520 --> 01:13:17,679
and then ovs will output it using its

01:13:16,320 --> 01:13:20,800
own kernel data path

01:13:17,679 --> 01:13:23,760
so ovs will fragment the packet and

01:13:20,800 --> 01:13:24,719
put it out but we still have this

01:13:23,760 --> 01:13:28,239
unbalance

01:13:24,719 --> 01:13:32,159
that we now can create

01:13:28,239 --> 01:13:33,440
such big packets on tc data path that

01:13:32,159 --> 01:13:36,719
can traverse

01:13:33,440 --> 01:13:38,400
dc actions and also the classifiers

01:13:36,719 --> 01:13:41,440
later on if we have a

01:13:38,400 --> 01:13:45,840
go-to chain but

01:13:41,440 --> 01:13:48,480
dc can't output such packets on its own

01:13:45,840 --> 01:13:50,719
and it would be very nice to have a fix

01:13:48,480 --> 01:13:53,440
for that

01:13:50,719 --> 01:13:55,520
this is a sample of the uses of the act

01:13:53,440 --> 01:13:59,040
ct

01:13:55,520 --> 01:14:03,840
i believe you can see my mouse pointer

01:13:59,040 --> 01:14:07,440
but if not this is the

01:14:03,840 --> 01:14:09,679
okay thanks this is the

01:14:07,440 --> 01:14:13,520
the first rule that would be matched on

01:14:09,679 --> 01:14:16,960
the packed triggers i'm using hardware

01:14:13,520 --> 01:14:21,760
it will do ct net here and just that

01:14:16,960 --> 01:14:24,880
it will do initial ct processing

01:14:21,760 --> 01:14:28,080
this will enough will be enough to do

01:14:24,880 --> 01:14:31,120
packet reassembling

01:14:28,080 --> 01:14:31,120
go to chain two

01:14:32,880 --> 01:14:40,159
on chain two if it is a new

01:14:36,800 --> 01:14:41,120
packet for a new connection it will

01:14:40,159 --> 01:14:43,840
configure

01:14:41,120 --> 01:14:43,840
the net

01:14:44,320 --> 01:14:52,880
save that on the contract entry

01:14:47,360 --> 01:14:52,880
and output it to a certain net device

01:14:53,120 --> 01:15:00,560
so if it is is reassemble packet

01:14:57,920 --> 01:15:04,080
this this is the place that it will fail

01:15:00,560 --> 01:15:08,480
it can't output packets here

01:15:04,080 --> 01:15:11,360
and for them subsequent packets for

01:15:08,480 --> 01:15:12,400
track and establish states it's just

01:15:11,360 --> 01:15:16,800
outputs because

01:15:12,400 --> 01:15:16,800
the net was already handled here

01:15:18,560 --> 01:15:23,520
um the current state realized that

01:15:23,679 --> 01:15:30,480
the chain two wouldn't exist or

01:15:27,199 --> 01:15:33,679
we would have a go to chain four

01:15:30,480 --> 01:15:35,679
or something that would trigger a miss

01:15:33,679 --> 01:15:36,960
and then ovs will pick up the packet and

01:15:35,679 --> 01:15:41,360
output but

01:15:36,960 --> 01:15:41,360
we dc itself cannot put it

01:15:42,800 --> 01:15:49,440
and on a brainstorm on how to solve it

01:15:46,880 --> 01:15:51,360
so far we we couldn't find a good way to

01:15:49,440 --> 01:15:53,920
to solve it there was some discussion on

01:15:51,360 --> 01:15:57,360
the initial thread in upstream

01:15:53,920 --> 01:16:00,480
but uh that led to nearly nowhere

01:15:57,360 --> 01:16:02,400
to this day that i'm aware of

01:16:00,480 --> 01:16:04,239
the main issue is that ac pipeline works

01:16:02,400 --> 01:16:08,480
with a single packet at a time

01:16:04,239 --> 01:16:12,880
so xct can just return a list of packets

01:16:08,480 --> 01:16:16,080
because one could say that it is

01:16:12,880 --> 01:16:19,679
actually responsibility to bring

01:16:16,080 --> 01:16:20,960
it back to dc semantics of one packet at

01:16:19,679 --> 01:16:24,159
a time but

01:16:20,960 --> 01:16:26,640
it's not easy to be doable we would have

01:16:24,159 --> 01:16:28,239
to handle multiple return codes on dcif

01:16:26,640 --> 01:16:31,280
action exact

01:16:28,239 --> 01:16:33,280
and each packet can have its own control

01:16:31,280 --> 01:16:36,640
action

01:16:33,280 --> 01:16:39,199
so it gets more recursive because we

01:16:36,640 --> 01:16:39,199
have to

01:16:39,280 --> 01:16:47,840
to store the the context for each packet

01:16:44,400 --> 01:16:48,640
that can flow anywhere else because

01:16:47,840 --> 01:16:50,960
we're not

01:16:48,640 --> 01:16:52,159
once we fragmented it we we can't be

01:16:50,960 --> 01:16:55,360
certain that they will

01:16:52,159 --> 01:16:55,679
hit the same filters the same actions

01:16:55,360 --> 01:16:58,960
and

01:16:55,679 --> 01:17:01,120
and so on and also

01:16:58,960 --> 01:17:02,239
because xct needs to reassemble packet

01:17:01,120 --> 01:17:05,520
in a second moment

01:17:02,239 --> 01:17:08,800
as we were seeing on the previous slide

01:17:05,520 --> 01:17:12,159
or we can if we do

01:17:08,800 --> 01:17:13,760
this on the chain 2 in the example

01:17:12,159 --> 01:17:16,320
it would have to reassemble the packet

01:17:13,760 --> 01:17:19,600
again and then disassembled

01:17:16,320 --> 01:17:23,360
so it can traverse the pipeline no not

01:17:19,600 --> 01:17:26,000
optimal and uh

01:17:23,360 --> 01:17:27,679
one proposal one idea would be similar

01:17:26,000 --> 01:17:29,920
to ranks of original approach

01:17:27,679 --> 01:17:31,760
fixing in act mirror itself but only

01:17:29,920 --> 01:17:34,320
enabled the feature

01:17:31,760 --> 01:17:34,880
if the user says so if we add a flag

01:17:34,320 --> 01:17:37,120
saying

01:17:34,880 --> 01:17:40,000
yeah you are allowed to fragment packets

01:17:37,120 --> 01:17:43,840
so please fragment it

01:17:40,000 --> 01:17:45,280
or extending that create a new more

01:17:43,840 --> 01:17:48,400
explicit action

01:17:45,280 --> 01:17:48,800
actile three buried like that yeah this

01:17:48,400 --> 01:17:52,719
is

01:17:48,800 --> 01:17:52,719
supposed to be doing l3 stuff

01:17:54,480 --> 01:18:00,800
one other idea is to user abuse

01:17:57,920 --> 01:18:01,120
interface backlog similar to reclassify

01:18:00,800 --> 01:18:03,120
but

01:18:01,120 --> 01:18:05,280
doesn't need to hold context anymore

01:18:03,120 --> 01:18:07,199
because it is a

01:18:05,280 --> 01:18:08,719
brand new packet that the interface is

01:18:07,199 --> 01:18:10,880
receiving it adds

01:18:08,719 --> 01:18:13,600
latency and reordering because now these

01:18:10,880 --> 01:18:16,960
packets are put back in the queue

01:18:13,600 --> 01:18:20,400
but it's a few frags anyway and we don't

01:18:16,960 --> 01:18:23,679
expect much performance out of it

01:18:20,400 --> 01:18:27,520
and one thing to keep in mind in here

01:18:23,679 --> 01:18:31,120
is that as we are dealing with the

01:18:27,520 --> 01:18:33,520
reassemble and fragmentation i think

01:18:31,120 --> 01:18:34,880
fragmentation needed should be

01:18:33,520 --> 01:18:37,280
considered

01:18:34,880 --> 01:18:38,400
it would be nice to have them happening

01:18:37,280 --> 01:18:42,239
although

01:18:38,400 --> 01:18:42,239
they see today has no idea about it

01:18:43,440 --> 01:18:47,360
yeah my question always any other ideas

01:18:45,920 --> 01:18:52,080
any

01:18:47,360 --> 01:18:52,080
paths that you think we could pursue

01:18:52,840 --> 01:18:58,640
here

01:18:55,440 --> 01:19:01,440
okay so same deal if you have a question

01:18:58,640 --> 01:19:02,480
you can type it much i will read it or

01:19:01,440 --> 01:19:09,840
you can raise your hand

01:19:02,480 --> 01:19:09,840
and will allow you to speak

01:19:12,239 --> 01:19:16,400
so i shall well like we're waiting for

01:19:14,719 --> 01:19:18,800
other people to ask i'll ask

01:19:16,400 --> 01:19:18,800
um

01:19:20,080 --> 01:19:23,280
i mean the problem is you want to

01:19:22,400 --> 01:19:25,600
fragment

01:19:23,280 --> 01:19:27,360
right it doesn't seem like it belongs to

01:19:25,600 --> 01:19:29,360
in an act

01:19:27,360 --> 01:19:31,280
of uh miranda to be honest with you

01:19:29,360 --> 01:19:32,960
because

01:19:31,280 --> 01:19:34,320
if the problem is fragmentation this

01:19:32,960 --> 01:19:36,000
fragmentation we need it in a lot of

01:19:34,320 --> 01:19:37,600
other places right

01:19:36,000 --> 01:19:40,080
it should be a generic feature as

01:19:37,600 --> 01:19:40,080
opposed to

01:19:43,840 --> 01:19:48,320
as opposed to something that

01:19:49,040 --> 01:19:52,880
is specific right to so act ct returns

01:19:52,320 --> 01:19:56,640
you this

01:19:52,880 --> 01:19:59,760
uh big list of packets right

01:19:56,640 --> 01:20:01,600
no it returns the reassemble packets

01:19:59,760 --> 01:20:03,280
so the reassemble packet and you want to

01:20:01,600 --> 01:20:04,000
you want to put it into smaller packets

01:20:03,280 --> 01:20:08,719
so you can send it

01:20:04,000 --> 01:20:11,920
out with the correct mtu yes yes

01:20:08,719 --> 01:20:14,159
and usually these fragments then will go

01:20:11,920 --> 01:20:15,120
to the same destination right so if we

01:20:14,159 --> 01:20:18,719
are doing

01:20:15,120 --> 01:20:19,840
any tunneling and we're pushing a vlan

01:20:18,719 --> 01:20:21,840
tag on it

01:20:19,840 --> 01:20:25,199
we could be doing on the assembled

01:20:21,840 --> 01:20:25,199
packets and fragmented later

01:20:26,320 --> 01:20:30,000
so if there's an action that is called

01:20:28,080 --> 01:20:31,280
fragment packets

01:20:30,000 --> 01:20:34,880
and you want to put that fragment

01:20:31,280 --> 01:20:34,880
packets inside a mirrored

01:20:36,320 --> 01:20:40,560
yes yes that action doesn't exist today

01:20:40,239 --> 01:20:42,800
but

01:20:40,560 --> 01:20:45,440
it's something that we it's an operation

01:20:42,800 --> 01:20:48,239
that we need to do

01:20:45,440 --> 01:20:49,840
so so it's to fragment packets at l3

01:20:48,239 --> 01:20:52,080
level basically into

01:20:49,840 --> 01:20:53,840
uh ethernet packets that will then be

01:20:52,080 --> 01:20:54,239
transmitted out would then be mirrored

01:20:53,840 --> 01:21:01,440
right

01:20:54,239 --> 01:21:04,639
or or redirected

01:21:01,440 --> 01:21:06,719
is that what act l3 mirrored is

01:21:04,639 --> 01:21:07,920
it goes in a loop takes a big packet

01:21:06,719 --> 01:21:10,239
fragments it puts

01:21:07,920 --> 01:21:12,320
the proper ethernet headers out and off

01:21:10,239 --> 01:21:13,840
it goes or

01:21:12,320 --> 01:21:16,719
the other way you can look at it as a

01:21:13,840 --> 01:21:20,400
graph with fragments and calls mirade

01:21:16,719 --> 01:21:23,600
right fragments slaps the

01:21:20,400 --> 01:21:25,360
ethernet yep and the three mirror could

01:21:23,600 --> 01:21:28,719
be a super set of mirrored

01:21:25,360 --> 01:21:29,040
could be even coded in the same c file

01:21:28,719 --> 01:21:32,159
so

01:21:29,040 --> 01:21:36,080
you can reuse most of its code

01:21:32,159 --> 01:21:36,080
but with that extra feature added

01:21:37,199 --> 01:21:43,840
all right anybody else with a comment on

01:21:40,840 --> 01:21:43,840
this

01:21:44,800 --> 01:21:48,080
just raise your hand or put your

01:21:46,320 --> 01:21:50,239
question on chat so

01:21:48,080 --> 01:21:52,639
the only concern i have is now we have

01:21:50,239 --> 01:21:54,239
two ways of doing this right which

01:21:52,639 --> 01:21:56,639
obvious has already invented their own

01:21:54,239 --> 01:21:58,000
way of handling it

01:21:56,639 --> 01:21:59,920
right and the obvious code is in the

01:21:58,000 --> 01:22:00,480
kernel and now we have tc which is like

01:21:59,920 --> 01:22:10,719
which

01:22:00,480 --> 01:22:13,120
needs this but is lacking right

01:22:10,719 --> 01:22:13,120
um

01:22:13,840 --> 01:22:18,159
yeah i think you should look at it doing

01:22:15,040 --> 01:22:18,159
it as an action set

01:22:20,960 --> 01:22:25,840
uh i do to maybe

01:22:28,719 --> 01:22:32,159
i don't know if this the result of this

01:22:30,400 --> 01:22:33,040
action the fragmented packets are only

01:22:32,159 --> 01:22:34,960
going to be needed for

01:22:33,040 --> 01:22:36,719
for mirroring and redirecting or for

01:22:34,960 --> 01:22:37,600
other maybe it could be fed into other

01:22:36,719 --> 01:22:39,840
actions as well

01:22:37,600 --> 01:22:39,840
right

01:22:43,199 --> 01:22:46,560
to my knowledge other actions they

01:22:45,440 --> 01:22:50,000
wouldn't have a problem

01:22:46,560 --> 01:22:52,960
dealing with such reassembled packets

01:22:50,000 --> 01:22:54,719
and we even have uh performance

01:22:52,960 --> 01:22:56,480
improvements by keeping it that way

01:22:54,719 --> 01:22:58,639
because we are processing in one packet

01:22:56,480 --> 01:23:01,679
and not several packets

01:22:58,639 --> 01:23:01,679
for the same actions

01:23:02,000 --> 01:23:07,520
all right well first off

01:23:05,280 --> 01:23:09,600
hi my name is brianna owersler i'm an

01:23:07,520 --> 01:23:12,320
outreachy intern this summer

01:23:09,600 --> 01:23:14,080
uh working with mentors david karate and

01:23:12,320 --> 01:23:17,040
stefano brevio

01:23:14,080 --> 01:23:18,719
our project titled improve and extend

01:23:17,040 --> 01:23:21,040
kernel networking

01:23:18,719 --> 01:23:22,080
self-test running in namespaces and my

01:23:21,040 --> 01:23:25,040
project this summer

01:23:22,080 --> 01:23:25,520
is largely concerned with pdc so i'm

01:23:25,040 --> 01:23:29,120
doing

01:23:25,520 --> 01:23:31,920
a quick pdc update about uh

01:23:29,120 --> 01:23:34,000
the work that we did to add cdc to k

01:23:31,920 --> 01:23:36,719
self tab

01:23:34,000 --> 01:23:37,440
tdc as you all probably know is a fleet

01:23:36,719 --> 01:23:39,600
of

01:23:37,440 --> 01:23:41,520
unit tests written in python for traffic

01:23:39,600 --> 01:23:43,280
control

01:23:41,520 --> 01:23:45,120
so here on this slide you can see that

01:23:43,280 --> 01:23:47,600
it was added

01:23:45,120 --> 01:23:49,360
by this patch as a target as under the

01:23:47,600 --> 01:23:52,080
name tc testing

01:23:49,360 --> 01:23:53,520
at the self test make file level this

01:23:52,080 --> 01:23:56,480
work was done to

01:23:53,520 --> 01:23:58,320
align with the requirements outlined by

01:23:56,480 --> 01:24:02,000
schwa khan in a discussion

01:23:58,320 --> 01:24:04,080
on the lkml i did provide a link below

01:24:02,000 --> 01:24:06,639
and i will be sending the powerpoint

01:24:04,080 --> 01:24:10,719
presentation for upload

01:24:06,639 --> 01:24:12,639
so the work was done to align with

01:24:10,719 --> 01:24:14,719
that conversation and to comply with

01:24:12,639 --> 01:24:17,199
case self-test documentation

01:24:14,719 --> 01:24:18,239
in that conversation uh schwa khan is

01:24:17,199 --> 01:24:20,800
talking about

01:24:18,239 --> 01:24:21,840
integrating hazel tests with kernel pi

01:24:20,800 --> 01:24:25,920
rings

01:24:21,840 --> 01:24:27,840
and tdc is

01:24:25,920 --> 01:24:29,520
on a table that he has that describes

01:24:27,840 --> 01:24:30,560
where everybody is with respect to that

01:24:29,520 --> 01:24:33,440
work

01:24:30,560 --> 01:24:35,920
tdc was not yet a target of the main

01:24:33,440 --> 01:24:44,000
self-make file

01:24:35,920 --> 01:24:47,199
so that it could not be evaluated

01:24:44,000 --> 01:24:51,760
so the goal of this work was to add tdc

01:24:47,199 --> 01:24:54,080
to uh you know

01:24:51,760 --> 01:24:55,920
make traffic control testing available

01:24:54,080 --> 01:24:57,760
at the self-test level for

01:24:55,920 --> 01:24:59,360
developers who test for regressions that

01:24:57,760 --> 01:25:01,280
might affect traffic control

01:24:59,360 --> 01:25:02,480
as well as to increase awareness and

01:25:01,280 --> 01:25:04,639
traffic to tdc

01:25:02,480 --> 01:25:05,760
encouraging contribution of new test

01:25:04,639 --> 01:25:09,360
cases

01:25:05,760 --> 01:25:12,480
and introduce that opportunity to be run

01:25:09,360 --> 01:25:15,920
by these larger ci projects

01:25:12,480 --> 01:25:16,320
so here i provided and i hope everybody

01:25:15,920 --> 01:25:19,600
can

01:25:16,320 --> 01:25:21,280
read it the make file that we introduced

01:25:19,600 --> 01:25:25,040
at the top level of

01:25:21,280 --> 01:25:28,239
the tc testing directory where pdc live

01:25:25,040 --> 01:25:31,600
so basically what i did here was

01:25:28,239 --> 01:25:35,040
merge the existing make files that

01:25:31,600 --> 01:25:40,080
make cps tests

01:25:35,040 --> 01:25:44,800
for pdc or ppf object code files for pvc

01:25:40,080 --> 01:25:48,800
i export that as test gen files and then

01:25:44,800 --> 01:25:51,840
we go ahead and export test progs

01:25:48,800 --> 01:25:52,800
because we created a wrapper script to

01:25:51,840 --> 01:25:57,040
run pdc

01:25:52,800 --> 01:25:57,040
and then export those test files

01:25:58,080 --> 01:26:02,159
why did we do a wrapper script we did

01:26:00,560 --> 01:26:05,120
that basically so that we could

01:26:02,159 --> 01:26:06,880
avoid calling the build ebps plugin that

01:26:05,120 --> 01:26:10,400
exports as the default action

01:26:06,880 --> 01:26:12,560
or actions tests that tdc do

01:26:10,400 --> 01:26:14,239
to avoid running make over and over

01:26:12,560 --> 01:26:16,159
every time somebody tries to run

01:26:14,239 --> 01:26:18,880
so that if they export to a custom

01:26:16,159 --> 01:26:21,280
object directory

01:26:18,880 --> 01:26:23,920
and of course self-test doesn't copy

01:26:21,280 --> 01:26:26,239
over the testing switch make file

01:26:23,920 --> 01:26:27,679
there's no breakage there we also did

01:26:26,239 --> 01:26:29,199
that so that we could control the

01:26:27,679 --> 01:26:31,840
targets

01:26:29,199 --> 01:26:33,360
only targeting actions and qdisk tests

01:26:31,840 --> 01:26:36,639
for now

01:26:33,360 --> 01:26:40,639
filters is being updated to give it more

01:26:36,639 --> 01:26:42,960
complete testing coverage

01:26:40,639 --> 01:26:45,040
so future goals are to better understand

01:26:42,960 --> 01:26:45,760
what those impacts are for bigger ci

01:26:45,040 --> 01:26:48,239
projects

01:26:45,760 --> 01:26:49,679
understand what other work needs to be

01:26:48,239 --> 01:26:54,239
done in tdc

01:26:49,679 --> 01:26:54,239
to basically

01:26:54,560 --> 01:27:01,679
render it helpful for that use case

01:26:59,120 --> 01:27:03,280
and just understand what other

01:27:01,679 --> 01:27:06,320
challenges or limitations are involved

01:27:03,280 --> 01:27:06,320
that might need to be addressed

01:27:06,800 --> 01:27:11,520
so i went ahead and

01:27:12,080 --> 01:27:15,520
used some of these creative commons

01:27:14,480 --> 01:27:18,880
license

01:27:15,520 --> 01:27:22,480
to make this powerpoint and uh of course

01:27:18,880 --> 01:27:26,159
we love your traffic and use cases in uh

01:27:22,480 --> 01:27:28,000
tdc thank you

01:27:26,159 --> 01:27:29,199
i noticed you've been finding bugs and

01:27:28,000 --> 01:27:32,320
fixing them

01:27:29,199 --> 01:27:35,600
with tdc kudos

01:27:32,320 --> 01:27:38,719
yeah thanks yes

01:27:35,600 --> 01:27:41,040
uh yeah so yeah that's that's

01:27:38,719 --> 01:27:42,960
exactly right i've been fighting bugs

01:27:41,040 --> 01:27:46,080
and then currently when i'm

01:27:42,960 --> 01:27:49,520
i'm writing some tests for

01:27:46,080 --> 01:27:50,800
the scale of cps control plane and that

01:27:49,520 --> 01:27:52,880
work that

01:27:50,800 --> 01:27:54,320
isn't done in time for me to really talk

01:27:52,880 --> 01:27:55,600
about it today but

01:27:54,320 --> 01:27:57,040
uh hopefully we'll be seeing some

01:27:55,600 --> 01:27:58,000
patches hitting the mailing list for

01:27:57,040 --> 01:28:00,080
that

01:27:58,000 --> 01:28:01,120
in the next couple weeks as are we still

01:28:00,080 --> 01:28:02,960
having those uh

01:28:01,120 --> 01:28:05,600
two or three week meetings michelle with

01:28:02,960 --> 01:28:05,600
on tdc

01:28:06,320 --> 01:28:09,840
i'm sorry oh okay i guess i've been i've

01:28:09,120 --> 01:28:14,239
been skipping

01:28:09,840 --> 01:28:17,280
okay um

01:28:14,239 --> 01:28:19,760
okay uh i'm go

01:28:17,280 --> 01:28:21,840
let's open the for any discussions but

01:28:19,760 --> 01:28:24,880
we're kind of chewing into break time

01:28:21,840 --> 01:28:26,560
right now but uh i feel less if anybody

01:28:24,880 --> 01:28:29,840
wants to talk let's feel free to have

01:28:26,560 --> 01:28:29,840
this discussion

01:28:30,159 --> 01:28:33,840
it's open mic time anybody anybody can

01:28:32,480 --> 01:28:37,199
talk about anything

01:28:33,840 --> 01:28:37,199
as long as related to tc

01:28:37,760 --> 01:28:43,600
i i i should point out there is a talk

01:28:40,800 --> 01:28:44,560
in this conference that's going to show

01:28:43,600 --> 01:28:47,520
how to do

01:28:44,560 --> 01:28:48,880
to invoke some of the socket layer code

01:28:47,520 --> 01:28:50,960
from xdp

01:28:48,880 --> 01:28:52,480
right so the layer violation is not

01:28:50,960 --> 01:28:55,199
really a good reason it's a purist point

01:28:52,480 --> 01:28:56,719
of view in my opinion

01:28:55,199 --> 01:28:58,719
sometimes you've got to be pragmatic and

01:28:56,719 --> 01:29:01,360
solve problems you know as opposed to

01:28:58,719 --> 01:29:02,880
i'm worried i'm doing this from tc and

01:29:01,360 --> 01:29:05,040
it's supposed to be l3

01:29:02,880 --> 01:29:08,480
or a socket layer code right i'm ready

01:29:05,040 --> 01:29:08,480
to say something on the chat

01:29:08,840 --> 01:29:12,000
right uh

01:29:12,719 --> 01:29:19,840
that she agrees there was already c s

01:29:16,000 --> 01:29:23,280
c group level bpf programs uh

01:29:19,840 --> 01:29:25,120
and the apache the purchase from wengsu

01:29:23,280 --> 01:29:26,400
for netflix ntc making it possible for

01:29:25,120 --> 01:29:30,239
them to use the tunnel flow

01:29:26,400 --> 01:29:32,880
block infrastructure to see

01:29:30,239 --> 01:29:33,679
i'm sorry double muted i don't know

01:29:32,880 --> 01:29:37,199
those patches

01:29:33,679 --> 01:29:37,199
sorry i need to check them

01:29:37,280 --> 01:29:44,080
okay and i you know what let's uh

01:29:40,560 --> 01:29:50,639
let's just wrap it up here

01:29:44,080 --> 01:29:50,639

YouTube URL: https://www.youtube.com/watch?v=lTwYxrsA3Ss


