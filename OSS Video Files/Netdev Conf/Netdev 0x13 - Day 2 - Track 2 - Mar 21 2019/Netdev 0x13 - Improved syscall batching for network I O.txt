Title: Netdev 0x13 - Improved syscall batching for network I O
Publication date: 2019-05-03
Playlist: Netdev 0x13 - Day 2 - Track 2 - Mar 21 2019
Description: 
	Rahul Jadhav, Zhen Cao and Anmol Sarma talk about improving Network applications performance by improving syscall batching.
Applicaction can benefit from reduced CPU cycles by amortizing the system call overhead of
network I/O operations. In this talk Rahul et al review two existing interfaces for network I/O batching namely recvmmsg()/sendmmsg() and SO_RCVLOWAT and then
propose extensions to these mechanisms.

They show an 8x syscall reduction with our traffic patterns (gaming scenarios) with the usage of
such extensions.

More info:
https://netdevconf.org/0x13/session.html?talk-syscall-batch
Captions: 
	00:00:00,120 --> 00:00:04,890
so I'll try to make this quick not take

00:00:03,330 --> 00:00:08,370
away more of your break time

00:00:04,890 --> 00:00:16,770
Rahul Jadhav so this is regarding system

00:00:08,370 --> 00:00:20,220
call improvement for Network IO okay so

00:00:16,770 --> 00:00:22,080
the primary motivation for us was the

00:00:20,220 --> 00:00:23,730
gaming scenario where we were

00:00:22,080 --> 00:00:27,689
experimenting with a particular game

00:00:23,730 --> 00:00:29,580
which made use of UDP packets so to

00:00:27,689 --> 00:00:34,530
improve the latency we made use of multi

00:00:29,580 --> 00:00:36,270
path UDP in a and eventually we realize

00:00:34,530 --> 00:00:39,570
that on the server we are not able to

00:00:36,270 --> 00:00:42,960
scale very well when we are using multi

00:00:39,570 --> 00:00:44,700
path UDP scheme so what happened was

00:00:42,960 --> 00:00:47,520
there were multiple so what we try to

00:00:44,700 --> 00:00:49,289
check was whether receive M message can

00:00:47,520 --> 00:00:51,690
be made use of in the but in this

00:00:49,289 --> 00:00:54,000
particular context and see whether it

00:00:51,690 --> 00:00:55,620
can give any improvement and to what we

00:00:54,000 --> 00:00:58,469
realized eventually is that receive a

00:00:55,620 --> 00:01:01,320
message couldn't get any any any

00:00:58,469 --> 00:01:03,210
improvement so we try to analyze why and

00:01:01,320 --> 00:01:06,439
we saw that there were multiple small

00:01:03,210 --> 00:01:08,729
sized packets across multiple sockets

00:01:06,439 --> 00:01:11,250
especially for the gaming scenario if we

00:01:08,729 --> 00:01:12,990
had both TCP and UDP sockets the packet

00:01:11,250 --> 00:01:15,930
rate characteristic the throughput per

00:01:12,990 --> 00:01:18,259
connection was very less but having said

00:01:15,930 --> 00:01:21,320
that there were thousands of connections

00:01:18,259 --> 00:01:24,540
gaming connections on the server side

00:01:21,320 --> 00:01:26,009
reverse proxies so the I'm basically

00:01:24,540 --> 00:01:29,659
talking about the motivation for this

00:01:26,009 --> 00:01:32,130
work we are extending three system calls

00:01:29,659 --> 00:01:34,829
one is the received message

00:01:32,130 --> 00:01:36,600
another is the send message and the

00:01:34,829 --> 00:01:40,500
third one is the received low at

00:01:36,600 --> 00:01:41,970
extension so this is the motivations

00:01:40,500 --> 00:01:43,560
another motivation was multiplied

00:01:41,970 --> 00:01:45,030
transport especially the multiple

00:01:43,560 --> 00:01:47,820
multipath transports which are

00:01:45,030 --> 00:01:50,070
implemented in the user space which

00:01:47,820 --> 00:01:51,930
requires send on multiple sockets which

00:01:50,070 --> 00:01:57,390
may require send on multiple sockets at

00:01:51,930 --> 00:02:00,240
the same time so just a quick just a

00:01:57,390 --> 00:02:01,469
quick recap of the received progression

00:02:00,240 --> 00:02:03,840
so I'm going to talk about receive a

00:02:01,469 --> 00:02:05,100
message mmm message so this is the this

00:02:03,840 --> 00:02:07,469
is the new thing that we are trying to

00:02:05,100 --> 00:02:10,129
do so what it does is we already have a

00:02:07,469 --> 00:02:12,599
receive a message which sort of tries to

00:02:10,129 --> 00:02:13,410
get multiple messages from the same

00:02:12,599 --> 00:02:15,360
sockets in

00:02:13,410 --> 00:02:18,960
same system call what we are trying to

00:02:15,360 --> 00:02:22,800
do here is try to batch multiple sockets

00:02:18,960 --> 00:02:24,990
and try to receive packets across these

00:02:22,800 --> 00:02:29,460
sockets in the same system call we found

00:02:24,990 --> 00:02:32,460
that there is a lot of CPU overhead when

00:02:29,460 --> 00:02:37,590
it comes to system called usage of this

00:02:32,460 --> 00:02:39,690
system calls so we are just extending

00:02:37,590 --> 00:02:43,110
the receive a message to receive mmm

00:02:39,690 --> 00:02:45,600
message the new M here stands for doing

00:02:43,110 --> 00:02:50,520
doing receive message and message across

00:02:45,600 --> 00:02:52,080
multiple sockets ok just to give a

00:02:50,520 --> 00:02:54,180
pictorial representation of what we are

00:02:52,080 --> 00:02:55,680
trying to do here we have receive a

00:02:54,180 --> 00:02:57,840
message which in a single system call

00:02:55,680 --> 00:02:59,970
can receive across multiple sockets we

00:02:57,840 --> 00:03:03,180
don't have any API which does that

00:02:59,970 --> 00:03:04,920
currently with the but this is

00:03:03,180 --> 00:03:06,900
essentially what we are trying to do I

00:03:04,920 --> 00:03:10,080
can't get into the details of the

00:03:06,900 --> 00:03:12,180
comparison here so the received message

00:03:10,080 --> 00:03:14,400
received M message and the receive mmm

00:03:12,180 --> 00:03:16,890
message so in case of resume message you

00:03:14,400 --> 00:03:19,980
can pull multiple packets on the same

00:03:16,890 --> 00:03:25,470
socket in the same system call here we

00:03:19,980 --> 00:03:28,620
can do it across the sockets so that

00:03:25,470 --> 00:03:32,370
user space changes what already if you

00:03:28,620 --> 00:03:35,130
see the kind of API switch we have the e

00:03:32,370 --> 00:03:37,920
pole a pole weight it already gives you

00:03:35,130 --> 00:03:40,020
an indication of the events that are

00:03:37,920 --> 00:03:41,490
received across sockets what we are

00:03:40,020 --> 00:03:43,580
trying to do here is we are trying to

00:03:41,490 --> 00:03:46,709
pull in all this events

00:03:43,580 --> 00:03:49,709
batch all these socket descriptors and

00:03:46,709 --> 00:03:52,280
then make us a single system receive a

00:03:49,709 --> 00:03:54,630
message called receive mmm message call

00:03:52,280 --> 00:03:56,459
inside the kernel the implementation is

00:03:54,630 --> 00:03:58,580
pretty straightforward especially for

00:03:56,459 --> 00:04:00,989
the receive a message because we are

00:03:58,580 --> 00:04:02,459
primarily wrapping up the existing

00:04:00,989 --> 00:04:05,660
receive a message calls which takes care

00:04:02,459 --> 00:04:11,520
of security and all the other handling

00:04:05,660 --> 00:04:14,070
so in the user in the user space what

00:04:11,520 --> 00:04:15,630
changes is that before making a receive

00:04:14,070 --> 00:04:18,150
call you you batch all they have this

00:04:15,630 --> 00:04:20,880
together and then make a single receive

00:04:18,150 --> 00:04:23,669
mmm message calls the primary complexity

00:04:20,880 --> 00:04:25,979
that we found was with regards to the

00:04:23,669 --> 00:04:26,770
error handling across sockets how do you

00:04:25,979 --> 00:04:29,289
report error

00:04:26,770 --> 00:04:32,379
across multiple sockets how do you how

00:04:29,289 --> 00:04:33,580
do you do sockets to message messages

00:04:32,379 --> 00:04:36,129
mapping so that was the primary

00:04:33,580 --> 00:04:37,650
complexity here but otherwise it was we

00:04:36,129 --> 00:04:41,020
otherwise it was pretty straightforward

00:04:37,650 --> 00:04:43,449
we primarily used the non-blocking mode

00:04:41,020 --> 00:04:44,979
in fact the MM message extensions that

00:04:43,449 --> 00:04:50,620
we are talking about here only works

00:04:44,979 --> 00:04:52,900
with the non blocking mode of usage the

00:04:50,620 --> 00:04:55,120
send a message again works in the

00:04:52,900 --> 00:04:58,150
similar fashion it is wrapped across

00:04:55,120 --> 00:04:59,500
send a message internally but the

00:04:58,150 --> 00:05:01,300
advantage here is that in case of

00:04:59,500 --> 00:05:03,340
multipath user space multiplied

00:05:01,300 --> 00:05:05,020
transport you can have the same message

00:05:03,340 --> 00:05:07,270
vector to be sent across multiple

00:05:05,020 --> 00:05:10,449
empties so you can essentially have

00:05:07,270 --> 00:05:12,190
overlapping message vectors that are

00:05:10,449 --> 00:05:14,319
that that can be sent across multiple

00:05:12,190 --> 00:05:16,360
sockets here you can also have partially

00:05:14,319 --> 00:05:17,860
overlapping messages theoretical it is

00:05:16,360 --> 00:05:20,889
possible but I don't know if any use

00:05:17,860 --> 00:05:25,840
case where it can be made use of right

00:05:20,889 --> 00:05:28,300
now again the gaming scenario with using

00:05:25,840 --> 00:05:29,650
MPI UDP we made use of the the redundant

00:05:28,300 --> 00:05:32,650
scheduler we're in the same packet was

00:05:29,650 --> 00:05:37,330
sent across multiple sockets was the use

00:05:32,650 --> 00:05:40,300
case here it word about this calls a

00:05:37,330 --> 00:05:41,979
quick the overhead is in terms of two

00:05:40,300 --> 00:05:43,360
different costs the direct cost and the

00:05:41,979 --> 00:05:45,400
indirect cost the direct cost which is

00:05:43,360 --> 00:05:47,380
actually the mode switching cost which

00:05:45,400 --> 00:05:48,789
involves switching from during zero

00:05:47,380 --> 00:05:51,370
Turing three and vice versa

00:05:48,789 --> 00:05:53,050
the indirect cost is the result of

00:05:51,370 --> 00:05:55,659
processor state pollution wherein there

00:05:53,050 --> 00:05:58,120
will be added there will be additional

00:05:55,659 --> 00:06:00,580
translation lookaside buffer lookups and

00:05:58,120 --> 00:06:03,460
the cache coherency issues that might

00:06:00,580 --> 00:06:05,380
arise out of the system call issued so

00:06:03,460 --> 00:06:08,199
we were trying to tackle these two

00:06:05,380 --> 00:06:11,370
system called overheads in this talk I'm

00:06:08,199 --> 00:06:11,370
primarily going to concentrate

00:06:11,400 --> 00:06:16,449
concentrate upon the direct cost the

00:06:14,830 --> 00:06:19,240
indirect cost having said that the

00:06:16,449 --> 00:06:21,690
indirect cost is the major cost with

00:06:19,240 --> 00:06:26,529
regards to system called neutralization

00:06:21,690 --> 00:06:28,479
you use of system calls the direct so

00:06:26,529 --> 00:06:33,400
these are some of the numbers we had a

00:06:28,479 --> 00:06:34,949
UDP server you hear the N is essentially

00:06:33,400 --> 00:06:39,459
what is written from the e pole weight

00:06:34,949 --> 00:06:45,039
the N tells you what kind of batching

00:06:39,459 --> 00:06:47,889
is done it can be achieved so if you see

00:06:45,039 --> 00:06:49,269
where when it goes so if we have a lot

00:06:47,889 --> 00:06:52,149
of user space processing in the

00:06:49,269 --> 00:06:54,849
application the ends increases

00:06:52,149 --> 00:06:57,759
substantially this is the return value

00:06:54,849 --> 00:07:00,459
of the e-poll weight in this case what

00:06:57,759 --> 00:07:02,919
you can do you can see what receive a

00:07:00,459 --> 00:07:05,199
message m message is not able to get

00:07:02,919 --> 00:07:06,939
enough for batching improvement because

00:07:05,199 --> 00:07:09,249
there were not many packets received on

00:07:06,939 --> 00:07:11,169
the same socket but there was lot of

00:07:09,249 --> 00:07:14,709
packets received across the sockets so

00:07:11,169 --> 00:07:16,959
you can reduce the system call overhead

00:07:14,709 --> 00:07:19,809
with receive mmm message as compared to

00:07:16,959 --> 00:07:21,669
receive a message so in this case we

00:07:19,809 --> 00:07:24,879
found that the CPU cycle utilization was

00:07:21,669 --> 00:07:27,759
like almost 14% less in in context to

00:07:24,879 --> 00:07:29,259
the receive I'm a message now again I

00:07:27,759 --> 00:07:31,029
have to stress here that this is the

00:07:29,259 --> 00:07:33,729
direct cost or the direct overhead that

00:07:31,029 --> 00:07:35,699
I am talking of only this can increase

00:07:33,729 --> 00:07:38,050
substantially with the indirect cost

00:07:35,699 --> 00:07:42,699
which means what happens if the

00:07:38,050 --> 00:07:44,229
processor state is getting polluted the

00:07:42,699 --> 00:07:46,959
per profile if you can see this is a

00:07:44,229 --> 00:07:48,699
simple receiver test wherein we are just

00:07:46,959 --> 00:07:50,229
receiving the packet and dropping it you

00:07:48,699 --> 00:07:51,909
can see that the most majority of the

00:07:50,229 --> 00:07:55,509
overhead is with regards to the system

00:07:51,909 --> 00:07:57,789
car processing what you can see is with

00:07:55,509 --> 00:08:01,809
the receive mm message the system call

00:07:57,789 --> 00:08:03,339
overhead processing is is much less as

00:08:01,809 --> 00:08:05,259
compared to the receiver message

00:08:03,339 --> 00:08:07,149
naturally we are getting a very high

00:08:05,259 --> 00:08:08,740
rate of batching this the the batching

00:08:07,149 --> 00:08:13,179
rate that we are getting in this case is

00:08:08,740 --> 00:08:14,889
approximately 30 are there in your

00:08:13,179 --> 00:08:16,959
existing alternatives well desc yes

00:08:14,889 --> 00:08:19,059
there are existing alternatives in the

00:08:16,959 --> 00:08:21,429
form of a synchronous i/o which is

00:08:19,059 --> 00:08:23,169
already there in the Linux kernel but

00:08:21,429 --> 00:08:25,599
having said that it's not very easy to

00:08:23,169 --> 00:08:27,579
use at the moment you have you have to

00:08:25,599 --> 00:08:30,429
it's much more difficult than what we

00:08:27,579 --> 00:08:32,380
are trying to propose this in sure but

00:08:30,429 --> 00:08:34,209
one interesting thing about the

00:08:32,380 --> 00:08:35,949
asynchronous i/o is that you can group

00:08:34,209 --> 00:08:38,289
read and write system calls together as

00:08:35,949 --> 00:08:41,969
well so that is that is an additional

00:08:38,289 --> 00:08:44,079
improvement that you get with a i/o

00:08:41,969 --> 00:08:45,910
another improvement that we want to talk

00:08:44,079 --> 00:08:47,589
I want to talk about here is the

00:08:45,910 --> 00:08:49,779
receiver Lobot improve improvement so

00:08:47,589 --> 00:08:51,130
what is receiver word there is a

00:08:49,779 --> 00:08:53,110
low-water mark that can be set in the

00:08:51,130 --> 00:08:55,390
kernel before the

00:08:53,110 --> 00:08:56,769
Elementor the appalling event is is set

00:08:55,390 --> 00:08:59,680
by the colonel for that particular

00:08:56,769 --> 00:09:02,440
socket so the the primary problem that

00:08:59,680 --> 00:09:08,380
we faced in this context was that there

00:09:02,440 --> 00:09:11,800
was no way of setting the time out so

00:09:08,380 --> 00:09:15,040
what happens if we want a low-water mark

00:09:11,800 --> 00:09:17,350
to be to be to be set and then we want a

00:09:15,040 --> 00:09:19,990
time out to happen if enough data is not

00:09:17,350 --> 00:09:22,240
received right now the time out can be

00:09:19,990 --> 00:09:26,350
AI can be used only with the received

00:09:22,240 --> 00:09:28,480
time with this particular this

00:09:26,350 --> 00:09:32,050
particular socket option but this socket

00:09:28,480 --> 00:09:33,519
option works only with if you are

00:09:32,050 --> 00:09:34,480
directly calling the receive from what

00:09:33,519 --> 00:09:36,250
the receive API

00:09:34,480 --> 00:09:40,529
this socket option doesn't work for

00:09:36,250 --> 00:09:43,779
sockets which are used alongside a pole

00:09:40,529 --> 00:09:45,880
so if you're waiting on a ve pole wait

00:09:43,779 --> 00:09:47,589
and if you have set a receive Lovat

00:09:45,880 --> 00:09:49,600
unless and until the low-water mark is

00:09:47,589 --> 00:09:52,450
hit you won't be able to receive any

00:09:49,600 --> 00:09:54,040
event what that means is if you have a

00:09:52,450 --> 00:09:56,740
buffer and for some reason the TCP

00:09:54,040 --> 00:09:58,870
connection or the TCP connection reduces

00:09:56,740 --> 00:10:01,930
the rate or doesn't fill up the packet

00:09:58,870 --> 00:10:03,670
you won't get an event even if even if

00:10:01,930 --> 00:10:06,399
there is even if the timeout has

00:10:03,670 --> 00:10:10,690
occurred so what we had done is

00:10:06,399 --> 00:10:14,709
essentially this we have introduced a

00:10:10,690 --> 00:10:17,019
way of adding a timeout along with the

00:10:14,709 --> 00:10:18,579
receive low adoption what it means is

00:10:17,019 --> 00:10:20,560
that along with the receiver where you

00:10:18,579 --> 00:10:23,829
can set a timeout and if there are no

00:10:20,560 --> 00:10:25,600
packets or if there is any packet if

00:10:23,829 --> 00:10:27,310
there is any data in that particular

00:10:25,600 --> 00:10:29,140
socket and if there is a timeout you

00:10:27,310 --> 00:10:32,260
will receive an e polling or the polling

00:10:29,140 --> 00:10:35,230
event so this what what it allows you to

00:10:32,260 --> 00:10:39,220
do is you can set more aggressive lower

00:10:35,230 --> 00:10:41,949
low low watermark thresholds on your

00:10:39,220 --> 00:10:48,070
socket thus reducing the system call

00:10:41,949 --> 00:10:50,010
eventually so the concluding remarks so

00:10:48,070 --> 00:10:52,540
what we are trying essentially to do is

00:10:50,010 --> 00:10:55,240
reduce the system call overhead by

00:10:52,540 --> 00:10:57,760
batching it more across them across

00:10:55,240 --> 00:10:59,740
multiple sockets we are trying to make

00:10:57,760 --> 00:11:01,690
sure that the interfaces are easy to use

00:10:59,740 --> 00:11:05,410
and our primary motivation was to make

00:11:01,690 --> 00:11:07,030
sure that we can extend existing stacks

00:11:05,410 --> 00:11:12,940
for example the

00:11:07,030 --> 00:11:14,500
next act for such such changes we want

00:11:12,940 --> 00:11:15,940
to make sure that the receive that the

00:11:14,500 --> 00:11:17,230
system overhead for the multi-part

00:11:15,940 --> 00:11:20,020
transport especially the multi-part

00:11:17,230 --> 00:11:22,840
transports in the user space is less and

00:11:20,020 --> 00:11:25,000
the receive lower timeout it allows us

00:11:22,840 --> 00:11:26,950
to set more aggressive watermarks or the

00:11:25,000 --> 00:11:29,050
more aggressive thresholds for the

00:11:26,950 --> 00:11:37,290
watermarks that's all that's all from my

00:11:29,050 --> 00:11:37,290
side any questions yeah

00:11:42,690 --> 00:11:50,519
so I'm I will ask about UDP sockets

00:11:47,790 --> 00:11:54,089
how many UDP sockets a particular thread

00:11:50,519 --> 00:11:58,589
is only at so in this case we have a

00:11:54,089 --> 00:12:02,759
using a 512 UDP sockets 512 if I went to

00:11:58,589 --> 00:12:07,500
2 UDP sockets why are using so many sir

00:12:02,759 --> 00:12:09,449
yeah that's in the memory of the camera

00:12:07,500 --> 00:12:11,250
sorry it's a huge footprint memory

00:12:09,449 --> 00:12:14,009
footprint correct just having all these

00:12:11,250 --> 00:12:16,740
objects in the camera so and having to

00:12:14,009 --> 00:12:18,930
store an S key be to a lot of different

00:12:16,740 --> 00:12:21,480
receive q is incurring a lot of cache

00:12:18,930 --> 00:12:24,540
line amis anyway right regardless of how

00:12:21,480 --> 00:12:28,620
fast you try to read them with a system

00:12:24,540 --> 00:12:31,980
core and you know that this system core

00:12:28,620 --> 00:12:34,170
becomes like crazy complex so I'm not

00:12:31,980 --> 00:12:36,870
sure that's the right approach to solve

00:12:34,170 --> 00:12:38,519
your program so so the final tangent

00:12:36,870 --> 00:12:40,230
well is the total number of UDP sockets

00:12:38,519 --> 00:12:42,449
that we are talking of but at any point

00:12:40,230 --> 00:12:47,910
the number of events that we are getting

00:12:42,449 --> 00:12:49,139
is approximately this the end size so

00:12:47,910 --> 00:12:50,939
these are the total number of UDP

00:12:49,139 --> 00:12:52,649
sockets having said that the use case is

00:12:50,939 --> 00:12:54,420
not only for the UDP socket II works

00:12:52,649 --> 00:13:00,779
equally well for the TCP sockets as well

00:12:54,420 --> 00:13:07,110
what we are trying to do is I think this

00:13:00,779 --> 00:13:09,209
is this so so this structure the

00:13:07,110 --> 00:13:12,480
structure is can be controlled by the

00:13:09,209 --> 00:13:14,819
user so you have the well you can put a

00:13:12,480 --> 00:13:17,339
limit on the count from the user space

00:13:14,819 --> 00:13:19,019
application if you don't want to have

00:13:17,339 --> 00:13:20,550
that kind of batching that higher number

00:13:19,019 --> 00:13:24,420
of batching then you can reduce this

00:13:20,550 --> 00:13:26,880
count yeah so I will repeat my question

00:13:24,420 --> 00:13:28,589
if your application is try trying to

00:13:26,880 --> 00:13:30,630
drain all the receive queue of 5

00:13:28,589 --> 00:13:35,449
hundredths ok - why do you have 500

00:13:30,630 --> 00:13:38,759
circuits and just one single one oh yeah

00:13:35,449 --> 00:13:40,199
yes what you're essentially saying is on

00:13:38,759 --> 00:13:44,339
the server side why do you really need

00:13:40,199 --> 00:13:47,430
512 so exactly so yeah in design you're

00:13:44,339 --> 00:13:50,309
all right yeah if you consider case of

00:13:47,430 --> 00:13:51,809
the reverse proxy where the UDP the UDP

00:13:50,309 --> 00:13:53,550
packets are received on a given socket

00:13:51,809 --> 00:13:55,090
and then the client connections are made

00:13:53,550 --> 00:13:57,040
to the server

00:13:55,090 --> 00:13:59,760
to the different server then you might

00:13:57,040 --> 00:14:03,610
need multiple sockets having said that

00:13:59,760 --> 00:14:06,130
what we in our case we made use of

00:14:03,610 --> 00:14:07,990
connected UDP sockets because we wanted

00:14:06,130 --> 00:14:10,330
to manage session directly on the

00:14:07,990 --> 00:14:11,860
connected UDP socket so you get an event

00:14:10,330 --> 00:14:13,570
you should be able to directly map to

00:14:11,860 --> 00:14:15,550
the session but what you're saying is

00:14:13,570 --> 00:14:19,210
yes it is possible that you have only a

00:14:15,550 --> 00:14:24,700
single UDP server socket but in case of

00:14:19,210 --> 00:14:30,790
TCP this is not possible right okay for

00:14:24,700 --> 00:14:32,470
UDP do one question what if some of

00:14:30,790 --> 00:14:37,300
these if these are blocking some are not

00:14:32,470 --> 00:14:39,520
right so is this so what happens is you

00:14:37,300 --> 00:14:42,450
are receiving an appalling event on that

00:14:39,520 --> 00:14:44,650
FD anyways that's when you batch it up

00:14:42,450 --> 00:14:46,420
but I've clearly mentioned here that

00:14:44,650 --> 00:14:48,400
this works only for the non blocking

00:14:46,420 --> 00:14:50,140
mode so there is no point in having

00:14:48,400 --> 00:14:51,850
certain F days which are blocking and

00:14:50,140 --> 00:14:53,290
then matching it up together with a non

00:14:51,850 --> 00:14:57,210
blocking a please so user space

00:14:53,290 --> 00:14:57,210
application has to make sure that all on

00:15:04,380 --> 00:15:12,120
so we are into our breaks or take a

00:15:07,570 --> 00:15:12,120

YouTube URL: https://www.youtube.com/watch?v=hJrXbqttJC4


