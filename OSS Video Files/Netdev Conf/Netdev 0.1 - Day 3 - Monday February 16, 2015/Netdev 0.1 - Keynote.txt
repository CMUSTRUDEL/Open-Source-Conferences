Title: Netdev 0.1 - Keynote
Publication date: 2015-03-21
Playlist: Netdev 0.1 - Day 3 - Monday February 16, 2015
Description: 
	Opening Keynote speech by David Miller. 
February 2015

State of the union on Linux kernel networking. 

https://www.netdev01.org

This video is licensed under Creative Commons Attribution-ShareAlike 4.0 International license. Feel free to download and distribute.
Captions: 
	00:00:00,540 --> 00:00:05,320
I'd like to thank that really cool

00:00:03,100 --> 00:00:07,030
vendor who's gonna add the first

00:00:05,320 --> 00:00:09,070
Hardware switching driver upstream you

00:00:07,030 --> 00:00:10,270
have no idea how cool you're gonna be

00:00:09,070 --> 00:00:12,280
when you do that you would be the

00:00:10,270 --> 00:00:15,750
Trailblazer in the Linux networking

00:00:12,280 --> 00:00:18,730
stack he loves hockey

00:00:15,750 --> 00:00:21,070
he loves Tim Hortons he loves Canada

00:00:18,730 --> 00:00:24,010
that's why it was easy to bring him over

00:00:21,070 --> 00:00:25,960
here this other things you don't know

00:00:24,010 --> 00:00:31,000
about him he is the IDE maintain I

00:00:25,960 --> 00:00:33,159
believe it or not I was I could read

00:00:31,000 --> 00:00:36,069
from his Wikipedia page anybody doesn't

00:00:33,159 --> 00:00:38,229
know Dave or I could I could go and read

00:00:36,069 --> 00:00:41,350
from his numerous courts I'll just go

00:00:38,229 --> 00:00:46,179
and cut and paste from net Dave or from

00:00:41,350 --> 00:00:56,199
this conference but I'm not gonna

00:00:46,179 --> 00:00:59,379
embarrass you Dave so so where is Dave

00:00:56,199 --> 00:01:01,929
has been a excellent custodian of the

00:00:59,379 --> 00:01:05,500
Linux networking stack it's a very tough

00:01:01,929 --> 00:01:07,150
job hard position to be in but despite

00:01:05,500 --> 00:01:08,650
the different calls the hard constraints

00:01:07,150 --> 00:01:12,870
he's done an excellent job it tries to

00:01:08,650 --> 00:01:20,920
be fair he's become very good at

00:01:12,870 --> 00:01:23,050
diplomacy over the years but is mine is

00:01:20,920 --> 00:01:25,630
the right place it's all about getting a

00:01:23,050 --> 00:01:27,730
good networking stack and the reason

00:01:25,630 --> 00:01:30,010
it's in the status in today is because

00:01:27,730 --> 00:01:32,820
of Dave so please give him a round

00:01:30,010 --> 00:01:32,820
applause unlike

00:01:35,350 --> 00:01:40,180
I like to thank Jamal for not

00:01:37,360 --> 00:01:41,439
embarrassing me the introduction he had

00:01:40,180 --> 00:01:43,090
the opportunity to do so I'm just gonna

00:01:41,439 --> 00:01:46,060
remind him of that someday you ever

00:01:43,090 --> 00:01:47,530
wanted to get me again so basically

00:01:46,060 --> 00:01:51,310
today I'm gonna talk about the state of

00:01:47,530 --> 00:01:53,380
the Linux networking stack so this is a

00:01:51,310 --> 00:01:54,940
selection of topics that I myself think

00:01:53,380 --> 00:01:56,649
are extremely important or extremely

00:01:54,940 --> 00:01:57,940
interesting to me it's not if I leave

00:01:56,649 --> 00:02:00,039
something out it doesn't mean it's not

00:01:57,940 --> 00:02:02,560
important or it's not interesting to me

00:02:00,039 --> 00:02:05,680
I had to choose something so we have a

00:02:02,560 --> 00:02:08,769
lot of components that have had serious

00:02:05,680 --> 00:02:12,430
improvements recently and we'll start

00:02:08,769 --> 00:02:14,680
with TCP so you think after 20 30 years

00:02:12,430 --> 00:02:15,670
of protocol development we kind of like

00:02:14,680 --> 00:02:17,769
figured everything out but that's

00:02:15,670 --> 00:02:19,930
definitely not the case is very far from

00:02:17,769 --> 00:02:21,790
the case and actually part of this is

00:02:19,930 --> 00:02:23,860
that the infrastructure over which are

00:02:21,790 --> 00:02:25,480
running TCP is kind of changing all the

00:02:23,860 --> 00:02:28,810
time as well so it is it's kind of like

00:02:25,480 --> 00:02:30,610
a moving target so one component of the

00:02:28,810 --> 00:02:33,400
improvements we've been making in TCP is

00:02:30,610 --> 00:02:37,720
per flow pacing sort by Erik Doom is a

00:02:33,400 --> 00:02:39,970
and others at Google and this kind of

00:02:37,720 --> 00:02:42,610
ties in a lot of these topics are

00:02:39,970 --> 00:02:44,769
related so first of all once we have per

00:02:42,610 --> 00:02:46,450
flow pacing and we kind of can know the

00:02:44,769 --> 00:02:48,040
pace the sending ready to map

00:02:46,450 --> 00:02:50,079
application we can make more intelligent

00:02:48,040 --> 00:02:52,480
decisions and elsewhere in a TCP stack

00:02:50,079 --> 00:02:56,590
so that is what ties into the third

00:02:52,480 --> 00:02:58,269
entry dynamic TSO sizing so if you're

00:02:56,590 --> 00:03:02,350
not familiar with tia so what that's all

00:02:58,269 --> 00:03:04,030
about is if we've got five 1500 byte

00:03:02,350 --> 00:03:06,340
segments that sent on the wire for TCP

00:03:04,030 --> 00:03:07,930
we can send them as one big super packet

00:03:06,340 --> 00:03:10,470
to the networking car and it'll split it

00:03:07,930 --> 00:03:13,690
up for us into the MTU size frames

00:03:10,470 --> 00:03:15,190
problem with this is when you batch up

00:03:13,690 --> 00:03:16,690
stuff you have to pick how much you want

00:03:15,190 --> 00:03:19,690
to send because there's a trade-off

00:03:16,690 --> 00:03:21,730
involved if you batch too much yokes and

00:03:19,690 --> 00:03:25,780
a gigantic burst on the wire and that

00:03:21,730 --> 00:03:28,060
trips up a lot of TCP algorithms and

00:03:25,780 --> 00:03:30,549
acts come back stretched quote/unquote

00:03:28,060 --> 00:03:31,690
stretched if you send too little then

00:03:30,549 --> 00:03:33,370
you're not taking advantage of the

00:03:31,690 --> 00:03:36,700
offload as much as you can so the the

00:03:33,370 --> 00:03:37,480
benefit is minimalized so dynamic TSO

00:03:36,700 --> 00:03:39,130
sizing

00:03:37,480 --> 00:03:42,220
makes the following compromise it says

00:03:39,130 --> 00:03:45,040
based upon the measured sending rate as

00:03:42,220 --> 00:03:47,949
computed by the pacing and other aspects

00:03:45,040 --> 00:03:49,060
of the networking stack don't make a TSO

00:03:47,949 --> 00:03:51,220
frame that would consume

00:03:49,060 --> 00:03:52,720
more than a millisecond on the wire so

00:03:51,220 --> 00:03:54,730
that's that's that's basically the

00:03:52,720 --> 00:03:58,510
trade-off that dynamic t is so sizing is

00:03:54,730 --> 00:04:04,180
making right now fast open is really

00:03:58,510 --> 00:04:05,920
interesting i if you're if you're doing

00:04:04,180 --> 00:04:07,239
transactions and that includes web

00:04:05,920 --> 00:04:08,739
requests and things like that what

00:04:07,239 --> 00:04:10,630
you're interested in is minimizing the

00:04:08,739 --> 00:04:12,959
amount of round trips that occur to get

00:04:10,630 --> 00:04:15,760
the information that the user asked for

00:04:12,959 --> 00:04:16,959
we lose to round trips by default just

00:04:15,760 --> 00:04:19,090
with the handshake to make the

00:04:16,959 --> 00:04:21,910
connections at a remote site period

00:04:19,090 --> 00:04:23,560
so what tcp fast opened is about is

00:04:21,910 --> 00:04:26,530
getting the request out in the initial

00:04:23,560 --> 00:04:29,200
handshake syn packet and being able to

00:04:26,530 --> 00:04:31,570
send data back on the syn ACK and that

00:04:29,200 --> 00:04:33,990
that saves you to round trips so that

00:04:31,570 --> 00:04:38,110
helps performance a lot in the

00:04:33,990 --> 00:04:39,790
request/response type situations another

00:04:38,110 --> 00:04:42,729
development is in the area of congestion

00:04:39,790 --> 00:04:45,070
control and this is specifically

00:04:42,729 --> 00:04:49,479
datacenter TCP this is done by Microsoft

00:04:45,070 --> 00:04:52,030
and some other people basically the only

00:04:49,479 --> 00:04:54,970
notification we have that congestion is

00:04:52,030 --> 00:04:57,460
coming it comes in a 1-bit ecn field

00:04:54,970 --> 00:04:59,410
that's a boolean right and data center

00:04:57,460 --> 00:05:02,410
TCP tries to extend that so that we can

00:04:59,410 --> 00:05:04,660
not only know we've got some congestion

00:05:02,410 --> 00:05:07,180
but rather the extent of the congestion

00:05:04,660 --> 00:05:08,740
so that's the long and short about data

00:05:07,180 --> 00:05:12,190
center TCP and now we have support for

00:05:08,740 --> 00:05:14,140
that TCP small skews is an interesting

00:05:12,190 --> 00:05:15,550
feature but I'm gonna talk extensively

00:05:14,140 --> 00:05:17,800
about that in the next slide because

00:05:15,550 --> 00:05:22,000
this is a one part of a much larger

00:05:17,800 --> 00:05:23,350
problem and we've also hopefully will

00:05:22,000 --> 00:05:26,200
make some progress in the area of

00:05:23,350 --> 00:05:30,220
advanced TCP statistics if you're at the

00:05:26,200 --> 00:05:31,750
web 10g presentation yesterday you'll

00:05:30,220 --> 00:05:33,280
understand what that's all about

00:05:31,750 --> 00:05:35,229
so basically there are a lot of things

00:05:33,280 --> 00:05:39,460
that people would like to know about how

00:05:35,229 --> 00:05:41,289
many times a certain a certain event

00:05:39,460 --> 00:05:42,789
happens within the TCP stack to help

00:05:41,289 --> 00:05:43,900
them diagnose their applications and

00:05:42,789 --> 00:05:47,830
we're trying to figure out a way to do

00:05:43,900 --> 00:05:49,330
that without minimizing overhead and

00:05:47,830 --> 00:05:50,650
making sure we put values and there that

00:05:49,330 --> 00:05:52,470
really makes sense and are useful for

00:05:50,650 --> 00:05:55,840
people so I'm looking forward to some

00:05:52,470 --> 00:05:58,260
advancement in that area so like I said

00:05:55,840 --> 00:06:00,159
I'll talk about TCP small cues here

00:05:58,260 --> 00:06:01,500
basically we have the buffer blow

00:06:00,159 --> 00:06:03,750
problem and

00:06:01,500 --> 00:06:06,240
the issue is that buffer bloat exists at

00:06:03,750 --> 00:06:07,260
many levels of the networking statin in

00:06:06,240 --> 00:06:09,690
the hierarchy of the networking

00:06:07,260 --> 00:06:11,810
protocols so you can't just fix it in

00:06:09,690 --> 00:06:14,850
one place it's not gonna work like that

00:06:11,810 --> 00:06:17,580
so starting at the application to TCP

00:06:14,850 --> 00:06:19,140
socket queues they they there's a buffer

00:06:17,580 --> 00:06:20,970
blow problem there because T s-- p oq'

00:06:19,140 --> 00:06:23,580
up as much as it can to keep the pipe

00:06:20,970 --> 00:06:25,320
full more than it needs to usually so

00:06:23,580 --> 00:06:26,940
TCP small cubes tries to minimize the

00:06:25,320 --> 00:06:28,530
amount of socket buffer space sent on

00:06:26,940 --> 00:06:31,710
the send sign will block the application

00:06:28,530 --> 00:06:35,010
until we're back onto the threshold and

00:06:31,710 --> 00:06:36,480
now in the cutest as well packets can

00:06:35,010 --> 00:06:37,890
sit there for a while until they're

00:06:36,480 --> 00:06:40,470
freed up and allowed to get to the

00:06:37,890 --> 00:06:43,470
device and then the device queues

00:06:40,470 --> 00:06:45,930
themselves can consume lots of space so

00:06:43,470 --> 00:06:48,840
a lot of these devices come up with 256

00:06:45,930 --> 00:06:51,720
or more or 512 or 1024 transmit queue

00:06:48,840 --> 00:06:53,400
entries and there's a lot of time that a

00:06:51,720 --> 00:06:54,930
packet will sit from the time it gets

00:06:53,400 --> 00:06:57,180
into the device driver to the point at

00:06:54,930 --> 00:06:58,440
which it hits the wire and the interrupt

00:06:57,180 --> 00:07:00,840
comes in to say that the packet is

00:06:58,440 --> 00:07:02,850
complete this really matters for TCP

00:07:00,840 --> 00:07:04,380
because that that interrupt completion

00:07:02,850 --> 00:07:05,940
inter up is what signals that we can

00:07:04,380 --> 00:07:07,710
make more socket space available again

00:07:05,940 --> 00:07:09,480
so this whole machine doesn't run until

00:07:07,710 --> 00:07:10,919
that whole queue gets running that guy

00:07:09,480 --> 00:07:14,070
at the back can make it to the front and

00:07:10,919 --> 00:07:16,680
get on the wire queue this is an

00:07:14,070 --> 00:07:19,580
interesting area if you consider the

00:07:16,680 --> 00:07:21,840
sukkah queues themselves and the cutest

00:07:19,580 --> 00:07:23,250
we're creating buffer bloat within the

00:07:21,840 --> 00:07:24,930
system itself at this point which is

00:07:23,250 --> 00:07:28,580
kind of interesting and the problem that

00:07:24,930 --> 00:07:31,830
this creates is that as on faster and

00:07:28,580 --> 00:07:33,090
shorter latency networks your TCP is

00:07:31,830 --> 00:07:35,910
time stamp stuff to be inaccurate

00:07:33,090 --> 00:07:37,260
because you're measuring the cutest time

00:07:35,910 --> 00:07:39,570
the time of the packet sitting in the

00:07:37,260 --> 00:07:42,060
cutest has nothing to do about the path

00:07:39,570 --> 00:07:45,690
between your link and destination

00:07:42,060 --> 00:07:47,070
machine so you so by minimizing the size

00:07:45,690 --> 00:07:48,870
of these queues at the top and the

00:07:47,070 --> 00:07:50,070
middle end of the stack our TCP

00:07:48,870 --> 00:07:52,260
timestamps are always sending more

00:07:50,070 --> 00:07:56,669
accurate again we'll have better our RTT

00:07:52,260 --> 00:07:58,290
estimates another area that we've been

00:07:56,669 --> 00:08:01,080
doing some really interesting work is

00:07:58,290 --> 00:08:04,380
checksumming Tom Herbert at Google has

00:08:01,080 --> 00:08:07,010
been doing a lot of work to basically

00:08:04,380 --> 00:08:12,120
facilitate encapsulations

00:08:07,010 --> 00:08:13,110
so VX lands any and it seems like almost

00:08:12,120 --> 00:08:14,370
everything that's going to be

00:08:13,110 --> 00:08:15,030
encapsulated in a data center will be

00:08:14,370 --> 00:08:18,060
something

00:08:15,030 --> 00:08:25,470
over UDP so this is an important case to

00:08:18,060 --> 00:08:27,000
look into basically one thing you need

00:08:25,470 --> 00:08:28,710
to realize that there's two sets of NICs

00:08:27,000 --> 00:08:30,870
out there there's one set of Nicks it

00:08:28,710 --> 00:08:32,850
says I can specifically check some IP

00:08:30,870 --> 00:08:34,320
packets that are TCP or UDP and I can

00:08:32,850 --> 00:08:37,530
specifically check some packets that are

00:08:34,320 --> 00:08:39,260
ipv6 needs to be or UDP and frankly

00:08:37,530 --> 00:08:41,610
that's not really interesting anymore

00:08:39,260 --> 00:08:43,760
what's interesting is a facility that

00:08:41,610 --> 00:08:46,020
could that could handle any checksumming

00:08:43,760 --> 00:08:47,490
type that we'd want to do and the only

00:08:46,020 --> 00:08:49,440
way to do that is a facility called

00:08:47,490 --> 00:08:50,940
checksum complete basically what

00:08:49,440 --> 00:08:54,600
checksum complete is is that the card

00:08:50,940 --> 00:08:56,160
will basically compute the 16 bits to

00:08:54,600 --> 00:08:58,350
two's-complement checksum across the

00:08:56,160 --> 00:09:00,210
whole level to payload and give you that

00:08:58,350 --> 00:09:02,880
in the receive descriptor and with that

00:09:00,210 --> 00:09:06,630
we can do lots of stuff for example when

00:09:02,880 --> 00:09:07,950
we receive a packet we can say ok here

00:09:06,630 --> 00:09:10,920
is the checksum for the whole packet

00:09:07,950 --> 00:09:12,960
packet if we D capsulate the outer UDP

00:09:10,920 --> 00:09:14,970
frame and subtract the checksum from

00:09:12,960 --> 00:09:16,140
there to the UDP payload we know what

00:09:14,970 --> 00:09:19,080
the checksum should have been for the

00:09:16,140 --> 00:09:21,360
inner for the encapsulated IP packet for

00:09:19,080 --> 00:09:23,160
example so that's really interesting and

00:09:21,360 --> 00:09:24,690
tom has worked on the internet draft for

00:09:23,160 --> 00:09:28,200
a facility called remote checksum

00:09:24,690 --> 00:09:30,600
offload and this provides a way to add

00:09:28,200 --> 00:09:32,280
metadata to the various encapsulation

00:09:30,600 --> 00:09:34,470
protocols that can tell the other end

00:09:32,280 --> 00:09:36,950
where the checksum in the inner part

00:09:34,470 --> 00:09:40,730
actually is located so that facilitates

00:09:36,950 --> 00:09:43,050
doing to checksums at once one implicit

00:09:40,730 --> 00:09:45,180
piece of this problem is that when the

00:09:43,050 --> 00:09:47,700
chips support checksum offload they

00:09:45,180 --> 00:09:49,650
typically only support one level awful

00:09:47,700 --> 00:09:51,810
offloading so you can you can tell it

00:09:49,650 --> 00:09:53,160
when transmitting here's the place to

00:09:51,810 --> 00:09:55,500
put the checksum and here's the area

00:09:53,160 --> 00:09:56,970
that it covers you can't say two

00:09:55,500 --> 00:10:00,030
locations and that's what you need for

00:09:56,970 --> 00:10:01,980
encapsulation situations but with the

00:10:00,030 --> 00:10:04,800
stuff that tom is working on we can we

00:10:01,980 --> 00:10:06,930
can still handle check sums of the inner

00:10:04,800 --> 00:10:09,990
packets as well that's a really powerful

00:10:06,930 --> 00:10:11,370
facility in my opinion because we

00:10:09,990 --> 00:10:13,050
haven't talked enough about Switch

00:10:11,370 --> 00:10:14,640
offloading during the last couple of

00:10:13,050 --> 00:10:17,640
days I'm going to make up for that gap

00:10:14,640 --> 00:10:19,050
in the next couple of slides so I guess

00:10:17,640 --> 00:10:21,510
this is a really important development

00:10:19,050 --> 00:10:23,960
it's something that you could argue

00:10:21,510 --> 00:10:27,070
we've been putting off for too long so

00:10:23,960 --> 00:10:28,990
the main areas of focus right now and

00:10:27,070 --> 00:10:30,820
the areas where I see people doing after

00:10:28,990 --> 00:10:33,790
active work is in bridge forwarding so

00:10:30,820 --> 00:10:36,399
the FT be putting FTB and on cydia this

00:10:33,790 --> 00:10:39,880
would switch off load doing route

00:10:36,399 --> 00:10:41,829
forwarding for ipv4 and ipv6 and the

00:10:39,880 --> 00:10:44,740
people are talking about doing NF table

00:10:41,829 --> 00:10:47,050
rules in offloaded into the hardware as

00:10:44,740 --> 00:10:49,269
well we have some things to attend to

00:10:47,050 --> 00:10:51,880
so let's let's talk about specific

00:10:49,269 --> 00:10:55,329
example let's say you go I peer out add

00:10:51,880 --> 00:10:57,310
foo right so if we're offloading ipv4

00:10:55,329 --> 00:10:59,889
forwarding into the hardware what should

00:10:57,310 --> 00:11:01,660
we do if the route can't fit in in the

00:10:59,889 --> 00:11:03,100
hardware's tables we've exceeded its

00:11:01,660 --> 00:11:06,120
capacity and this is a really important

00:11:03,100 --> 00:11:09,250
issue because it delves into all the the

00:11:06,120 --> 00:11:12,130
realms of the implementation that will

00:11:09,250 --> 00:11:15,160
just that that are important for doing

00:11:12,130 --> 00:11:16,750
this kind of stuff these aren't the only

00:11:15,160 --> 00:11:18,130
options these are just three options

00:11:16,750 --> 00:11:19,509
I've discussed with various developers

00:11:18,130 --> 00:11:22,420
over the last couple days and in the

00:11:19,509 --> 00:11:25,779
past don't install the route and a

00:11:22,420 --> 00:11:27,579
return error okay so if we can't fit the

00:11:25,779 --> 00:11:29,350
route into the hardware's kam table or

00:11:27,579 --> 00:11:30,699
whatever we just don't install the route

00:11:29,350 --> 00:11:33,250
at all we tell the user we couldn't do

00:11:30,699 --> 00:11:35,440
it here's the error code and the next

00:11:33,250 --> 00:11:37,300
option is if we exceed the capacity of

00:11:35,440 --> 00:11:38,709
the hardware take everything out of the

00:11:37,300 --> 00:11:41,260
hardware and do all the forwarding and

00:11:38,709 --> 00:11:44,199
saw in software it's another option and

00:11:41,260 --> 00:11:47,649
the third option is to try to find some

00:11:44,199 --> 00:11:49,360
midway point where we put as much as we

00:11:47,649 --> 00:11:51,839
possibly can in the hardware and fall

00:11:49,360 --> 00:11:53,529
back to software for everything else so

00:11:51,839 --> 00:11:54,850
option three is kind of interesting

00:11:53,529 --> 00:11:57,430
because it delves into all kinds of

00:11:54,850 --> 00:11:59,290
issues of you have to make sure you put

00:11:57,430 --> 00:12:02,860
more specific routes into the hardware

00:11:59,290 --> 00:12:04,569
offload because you have the issue of if

00:12:02,860 --> 00:12:06,279
you have two routes that overlap one is

00:12:04,569 --> 00:12:07,899
has a smaller prefix one has a larger

00:12:06,279 --> 00:12:10,360
prefix you can run into the situation

00:12:07,899 --> 00:12:11,800
where these two switch would forward the

00:12:10,360 --> 00:12:13,240
less specific route whereas you have a

00:12:11,800 --> 00:12:15,010
more specific one in the hard in

00:12:13,240 --> 00:12:16,180
software but it wouldn't get checked at

00:12:15,010 --> 00:12:20,980
all since the hardware forwarded the

00:12:16,180 --> 00:12:23,560
packet already and so these are kind of

00:12:20,980 --> 00:12:28,439
like the trade-offs that you have to

00:12:23,560 --> 00:12:30,310
consider so here's my basic guideline

00:12:28,439 --> 00:12:32,410
whatever we do it has to be a hundred

00:12:30,310 --> 00:12:33,959
percent transparent to the user using

00:12:32,410 --> 00:12:36,459
facilities that we have right now

00:12:33,959 --> 00:12:37,569
something the user would have got the

00:12:36,459 --> 00:12:39,069
route installed and it would have

00:12:37,569 --> 00:12:40,780
properly forwarded all the pack words it

00:12:39,069 --> 00:12:43,960
should still do so after

00:12:40,780 --> 00:12:46,870
we do hardware for loading so this means

00:12:43,960 --> 00:12:48,460
that the returning an error when we

00:12:46,870 --> 00:12:52,230
exceed Hardware capacity is kind of not

00:12:48,460 --> 00:12:56,310
not an option in that particular example

00:12:52,230 --> 00:12:58,330
but two and three would be okay

00:12:56,310 --> 00:13:00,430
however listen that does not preclude

00:12:58,330 --> 00:13:02,380
adding facilities for people who want to

00:13:00,430 --> 00:13:04,750
create specific configurations of

00:13:02,380 --> 00:13:06,940
sharing hardware and software or would

00:13:04,750 --> 00:13:09,820
like to be have the error code sent back

00:13:06,940 --> 00:13:11,860
on Hardware capacity being exceeded for

00:13:09,820 --> 00:13:14,130
example or some other facility to ever

00:13:11,860 --> 00:13:16,390
allow them to control the situation

00:13:14,130 --> 00:13:18,160
understand what the capabilities of the

00:13:16,390 --> 00:13:20,290
hardware are learning exactly what kind

00:13:18,160 --> 00:13:21,580
of capacity miss happened etc so I'm not

00:13:20,290 --> 00:13:23,680
against that I'm just saying that by

00:13:21,580 --> 00:13:25,030
default when you use Linux tools they

00:13:23,680 --> 00:13:28,110
should behave the way that they behave

00:13:25,030 --> 00:13:32,830
in it with the software implementation

00:13:28,110 --> 00:13:36,940
so in the realm of switch offloading we

00:13:32,830 --> 00:13:39,970
have a lot of tracks of implementations

00:13:36,940 --> 00:13:42,700
going right now so we have specific

00:13:39,970 --> 00:13:44,380
offload operations for the device

00:13:42,700 --> 00:13:48,130
drivers for doing bridge offloading and

00:13:44,380 --> 00:13:50,100
ipv4 device route forwarding and this is

00:13:48,130 --> 00:13:56,620
work by Scott Feldman engineering people

00:13:50,100 --> 00:13:59,080
basically this is the line of thought

00:13:56,620 --> 00:14:01,540
that if we just tackle individual

00:13:59,080 --> 00:14:03,010
offloading problems we can have a better

00:14:01,540 --> 00:14:06,580
idea about how these things interact

00:14:03,010 --> 00:14:07,990
with each other over time but eventually

00:14:06,580 --> 00:14:09,400
we're going to have to come to a point

00:14:07,990 --> 00:14:10,600
where we have to kind of encapsulate all

00:14:09,400 --> 00:14:11,770
these things because eventually we're

00:14:10,600 --> 00:14:13,450
gonna have to deal with the fact that

00:14:11,770 --> 00:14:15,970
these these things interact with each

00:14:13,450 --> 00:14:18,040
other the table that holds the bridge

00:14:15,970 --> 00:14:22,150
FDB entries in ipv4 routes might be

00:14:18,040 --> 00:14:23,980
shared there might be some way to store

00:14:22,150 --> 00:14:25,660
more entries if we pack things a certain

00:14:23,980 --> 00:14:27,790
way or if we inserted them in a certain

00:14:25,660 --> 00:14:30,100
way so we need some kind of like

00:14:27,790 --> 00:14:31,540
holistic model for this but we're we

00:14:30,100 --> 00:14:33,310
don't have enough experience with what

00:14:31,540 --> 00:14:34,720
these cards need or how these things are

00:14:33,310 --> 00:14:38,140
going to interact to do that yet but

00:14:34,720 --> 00:14:40,360
meanwhile in that vein the flow api by

00:14:38,140 --> 00:14:43,240
john faster ben is trying to think about

00:14:40,360 --> 00:14:45,400
that angle of the problem currently he's

00:14:43,240 --> 00:14:48,400
proposed a set of API calls that allow

00:14:45,400 --> 00:14:51,550
you to get the configuration of the

00:14:48,400 --> 00:14:52,930
hardw hardware switch basically it's the

00:14:51,550 --> 00:14:54,100
geography of the switch and what it's

00:14:52,930 --> 00:14:55,449
capable of what kind

00:14:54,100 --> 00:14:57,339
does it have what kind of pipe line

00:14:55,449 --> 00:14:59,259
flows does it have what can you do with

00:14:57,339 --> 00:15:00,940
with the piece of hardware so and

00:14:59,259 --> 00:15:03,579
eventually it'll have a set of

00:15:00,940 --> 00:15:05,589
operations to load flow entries into

00:15:03,579 --> 00:15:07,630
load rules into Institute hardware as

00:15:05,589 --> 00:15:09,940
well and we can build things on top of

00:15:07,630 --> 00:15:12,279
that so you could think in the future at

00:15:09,940 --> 00:15:13,720
some point we've figured out what we

00:15:12,279 --> 00:15:16,420
need to do with bridge forwarding an

00:15:13,720 --> 00:15:18,550
ipv4 route device operations and we

00:15:16,420 --> 00:15:21,670
implement them in terms of the flow API

00:15:18,550 --> 00:15:23,620
that's one possible result and if a

00:15:21,670 --> 00:15:25,120
third set of APs comes along that thinks

00:15:23,620 --> 00:15:26,649
they can solve the bottle the problem

00:15:25,120 --> 00:15:29,500
better by being somewhere between these

00:15:26,649 --> 00:15:31,329
two extremes that's okay too that's part

00:15:29,500 --> 00:15:32,949
of the learning process and right now

00:15:31,329 --> 00:15:34,180
it's kind of a good time to be doing

00:15:32,949 --> 00:15:36,250
this because we don't have a lot of

00:15:34,180 --> 00:15:38,259
drivers in fact we have just one the

00:15:36,250 --> 00:15:39,759
rocker switch driver and so we don't

00:15:38,259 --> 00:15:41,889
have a lot of changes to do if we want

00:15:39,759 --> 00:15:42,699
to adjust the api's at this point so

00:15:41,889 --> 00:15:46,839
it's a really good time for

00:15:42,699 --> 00:15:48,370
experimentation so like I said we should

00:15:46,839 --> 00:15:51,730
be converging at some point but that's

00:15:48,370 --> 00:15:54,759
that's that's something in the future so

00:15:51,730 --> 00:15:57,100
to me all these technical issues are

00:15:54,759 --> 00:15:58,630
third second place to what really

00:15:57,100 --> 00:16:01,839
matters which is that we should really

00:15:58,630 --> 00:16:03,519
have a very clear story about why we're

00:16:01,839 --> 00:16:05,680
making certain trade-offs and we should

00:16:03,519 --> 00:16:07,810
be able to clearly explain this to

00:16:05,680 --> 00:16:12,310
anyone who's unhappy with the design

00:16:07,810 --> 00:16:14,290
decisions we've made because unlike the

00:16:12,310 --> 00:16:16,750
purse the person who implements this at

00:16:14,290 --> 00:16:18,279
a person who submits the code to me I'm

00:16:16,750 --> 00:16:19,600
the one who's gonna have to answer to

00:16:18,279 --> 00:16:23,649
the person who's unhappy about how the

00:16:19,600 --> 00:16:26,319
API saw so when you really need to be

00:16:23,649 --> 00:16:27,639
very very ly at that we have a really

00:16:26,319 --> 00:16:31,990
good understanding about why we made

00:16:27,639 --> 00:16:33,279
this or that trade-off next thing I'd

00:16:31,990 --> 00:16:35,620
like to talk about is our hash tables

00:16:33,279 --> 00:16:38,410
this isn't a networking specific thing

00:16:35,620 --> 00:16:40,420
this is a generic data structure

00:16:38,410 --> 00:16:45,310
facility so the problem we have in the

00:16:40,420 --> 00:16:46,990
kernel is for high performance on SP we

00:16:45,310 --> 00:16:49,470
use our Cu locking which basically

00:16:46,990 --> 00:16:50,620
allows readers to unrestricted

00:16:49,470 --> 00:16:52,689
unrestrictedly

00:16:50,620 --> 00:16:54,069
traverse a table even when updates are

00:16:52,689 --> 00:16:57,279
happening new entries are being inserted

00:16:54,069 --> 00:16:59,230
etc etc no locking at all just a little

00:16:57,279 --> 00:17:02,290
bit of a couple of CPU memory barriers

00:16:59,230 --> 00:17:04,419
on either side of the operation one

00:17:02,290 --> 00:17:06,400
problem with this is traditionally we

00:17:04,419 --> 00:17:07,420
haven't had a facility to change the

00:17:06,400 --> 00:17:09,130
size of the hash table

00:17:07,420 --> 00:17:11,079
so we couldn't dynamically drove the

00:17:09,130 --> 00:17:13,360
hash-table over time as more and more

00:17:11,079 --> 00:17:15,370
entries are populated into the table our

00:17:13,360 --> 00:17:17,410
hash tables is a facility for doing this

00:17:15,370 --> 00:17:19,540
it has a really clever algorithm for

00:17:17,410 --> 00:17:21,670
zipping and unzipping the hash chains as

00:17:19,540 --> 00:17:24,010
you grow by powers of two the hash table

00:17:21,670 --> 00:17:29,110
from one size to another and therefore

00:17:24,010 --> 00:17:31,660
it allows dynamic sizing so one

00:17:29,110 --> 00:17:33,880
manifestation of the RCU limitation for

00:17:31,660 --> 00:17:35,740
hash tables is that at boot time we have

00:17:33,880 --> 00:17:37,570
a fixed size for the TCP hash table and

00:17:35,740 --> 00:17:39,040
it's it's gigantic we computed based

00:17:37,570 --> 00:17:40,990
upon a percentage of the amount of

00:17:39,040 --> 00:17:42,640
physical memory in the system so if you

00:17:40,990 --> 00:17:43,960
got this gigantic machine and you really

00:17:42,640 --> 00:17:46,330
don't do a lot networking on it we're

00:17:43,960 --> 00:17:47,950
still chewing up megabytes of megabytes

00:17:46,330 --> 00:17:50,560
and megabytes of memory for the TCP hash

00:17:47,950 --> 00:17:53,170
table that's completely mostly unused so

00:17:50,560 --> 00:17:55,330
with that if we convert TCP to use our

00:17:53,170 --> 00:17:57,100
hash tables then we can dynamically size

00:17:55,330 --> 00:17:59,200
the table based upon actual usage

00:17:57,100 --> 00:18:01,180
instead of just having to make the

00:17:59,200 --> 00:18:04,410
maximum reservation of memory at boot

00:18:01,180 --> 00:18:06,730
size so it's a really really valuable

00:18:04,410 --> 00:18:08,950
facility in my in my opinion so right

00:18:06,730 --> 00:18:11,890
now we already have two users the net

00:18:08,950 --> 00:18:14,320
link socket table and nf tables is hash

00:18:11,890 --> 00:18:17,500
table also use both use these facility

00:18:14,320 --> 00:18:19,840
it is facility so that should really

00:18:17,500 --> 00:18:26,050
improve set lookup performance for a net

00:18:19,840 --> 00:18:28,240
of tables so last August in Chicago we

00:18:26,050 --> 00:18:30,820
had a networking subgroup in the kernel

00:18:28,240 --> 00:18:32,800
summit and we discussed several things

00:18:30,820 --> 00:18:34,870
and I didn't we didn't plan to talk

00:18:32,800 --> 00:18:37,150
about this but rusty Russell pulled me

00:18:34,870 --> 00:18:39,070
aside and he said hey do you have any

00:18:37,150 --> 00:18:40,450
concept of how expensive it is to go

00:18:39,070 --> 00:18:42,430
into the hypervisor when I transmit

00:18:40,450 --> 00:18:44,560
packets over a virtualized networking

00:18:42,430 --> 00:18:46,240
device and I said no but I can imagine

00:18:44,560 --> 00:18:47,410
how expensive it is he's like have you

00:18:46,240 --> 00:18:49,270
ever thought about batching and

00:18:47,410 --> 00:18:50,980
mitigating this cost and I said oh we

00:18:49,270 --> 00:18:52,390
should look into that so we had a

00:18:50,980 --> 00:18:55,660
discussion and everyone seemed very

00:18:52,390 --> 00:18:57,280
interested in this and we went back and

00:18:55,660 --> 00:18:59,800
forth about how we would represent this

00:18:57,280 --> 00:19:03,460
and the drivers I I didn't want to

00:18:59,800 --> 00:19:05,620
change 500 transmit operations

00:19:03,460 --> 00:19:07,120
signatures and in in the tree I didn't

00:19:05,620 --> 00:19:08,800
want to edit 500 drivers at this

00:19:07,120 --> 00:19:11,230
facility so I try to come up with some

00:19:08,800 --> 00:19:13,270
signaling mechanism that's outside

00:19:11,230 --> 00:19:15,310
having to change the argument set or the

00:19:13,270 --> 00:19:17,380
for the semantics of the transmit

00:19:15,310 --> 00:19:19,780
operation in the networking drivers and

00:19:17,380 --> 00:19:21,220
where we came up with is a flag it's in

00:19:19,780 --> 00:19:24,280
a socket buffer called

00:19:21,220 --> 00:19:27,940
Smit more basically what XML x-min more

00:19:24,280 --> 00:19:30,340
says there's definitely another packet

00:19:27,940 --> 00:19:32,409
coming right after your return so if you

00:19:30,340 --> 00:19:34,030
can defer any expensive operation to

00:19:32,409 --> 00:19:35,679
batch the processing of transmitting

00:19:34,030 --> 00:19:37,870
packets at this point you should do so

00:19:35,679 --> 00:19:39,490
so what a driver basically does is it

00:19:37,870 --> 00:19:42,130
will cue the packet up in the transmit

00:19:39,490 --> 00:19:43,900
queue and unless it's filled the queue

00:19:42,130 --> 00:19:46,179
and it has to get the card going anyways

00:19:43,900 --> 00:19:48,400
it doesn't write the doorbell register

00:19:46,179 --> 00:19:49,900
that triggers the the chip to start

00:19:48,400 --> 00:19:51,850
reading the new transmit queue entries

00:19:49,900 --> 00:19:53,289
and in the case of the virtualization

00:19:51,850 --> 00:19:54,850
driver it doesn't call into the

00:19:53,289 --> 00:19:58,320
hypervisor to tell it at arc to start

00:19:54,850 --> 00:19:58,320
pulling packets off the transmit queue

00:19:59,010 --> 00:20:03,130
so once you've done this you have to

00:20:01,240 --> 00:20:04,600
have a facility inside the kernel to

00:20:03,130 --> 00:20:06,700
start setting this bit and that facility

00:20:04,600 --> 00:20:09,520
is booked to queue support and acutest

00:20:06,700 --> 00:20:11,350
letters so the cutest layer is now the

00:20:09,520 --> 00:20:13,150
cutest will have one or more packets

00:20:11,350 --> 00:20:14,770
sitting in its queue ready to go to the

00:20:13,150 --> 00:20:16,600
device and since it has all this

00:20:14,770 --> 00:20:21,159
knowledge that's where we set this to

00:20:16,600 --> 00:20:22,990
transmit more bit so there are some

00:20:21,159 --> 00:20:24,929
issues with this okay so if you're in

00:20:22,990 --> 00:20:27,549
steady-state on a local system

00:20:24,929 --> 00:20:29,799
transmitting over a saw a TCP socket and

00:20:27,549 --> 00:20:31,750
you've got the the card going at LAN ray

00:20:29,799 --> 00:20:33,250
yes you will back up into the queue just

00:20:31,750 --> 00:20:35,679
and will always have a bulk set of

00:20:33,250 --> 00:20:36,870
packets to to send at once however when

00:20:35,679 --> 00:20:39,490
you're forwarding

00:20:36,870 --> 00:20:41,710
receive is a little bit more expensive

00:20:39,490 --> 00:20:43,929
than transmitted so you may not be

00:20:41,710 --> 00:20:47,620
writing the transmits a deadline rate

00:20:43,929 --> 00:20:50,140
and therefore you're not queuing up

00:20:47,620 --> 00:20:52,570
things to gather into the queue disk on

00:20:50,140 --> 00:20:54,460
a transmit side so you there has to be

00:20:52,570 --> 00:20:56,140
some facility by which we can we can

00:20:54,460 --> 00:20:57,460
make blobs at the front and then give

00:20:56,140 --> 00:20:59,860
them out in the back what usually

00:20:57,460 --> 00:21:02,169
happens is we would do generic receive

00:20:59,860 --> 00:21:05,890
offload this works for TCP flows that

00:21:02,169 --> 00:21:09,900
are in packet trains so as I explained

00:21:05,890 --> 00:21:13,419
earlier the TCP send off flow

00:21:09,900 --> 00:21:15,580
segmentation offload is we reverse the

00:21:13,419 --> 00:21:17,020
process on receive we basically look at

00:21:15,580 --> 00:21:19,390
every packet that comes to quench late

00:21:17,020 --> 00:21:21,039
and if they have the same flow ID then

00:21:19,390 --> 00:21:22,510
we'll and the same sequential sequence

00:21:21,039 --> 00:21:23,830
numbers will gather them into a super

00:21:22,510 --> 00:21:27,159
packet and pass that through the stack

00:21:23,830 --> 00:21:28,690
this has a lot of advantages first every

00:21:27,159 --> 00:21:30,309
single decision that has to get made in

00:21:28,690 --> 00:21:32,380
the stack to figure out where it goes

00:21:30,309 --> 00:21:33,840
routing policy whatever only it gets to

00:21:32,380 --> 00:21:35,990
be done one

00:21:33,840 --> 00:21:39,809
I'm 4n packets so that's really powerful

00:21:35,990 --> 00:21:42,419
and but this doesn't happen generally

00:21:39,809 --> 00:21:44,520
speaking on a back-end router a very

00:21:42,419 --> 00:21:46,679
heavily used router we have very lots of

00:21:44,520 --> 00:21:49,110
unrelated flows not a lot of packet

00:21:46,679 --> 00:21:51,720
gathering for TCP so basically we don't

00:21:49,110 --> 00:21:53,039
get the batching of any type on the

00:21:51,720 --> 00:21:54,029
transmit side so that's something we

00:21:53,039 --> 00:21:56,130
need to look into

00:21:54,029 --> 00:21:57,600
Honus who is looking into some solutions

00:21:56,130 --> 00:21:59,130
and that we could do with that we came

00:21:57,600 --> 00:22:01,200
up with some ideas the other day during

00:21:59,130 --> 00:22:04,500
that cough we'll see where that goes but

00:22:01,200 --> 00:22:07,230
for the cases where this does trigger it

00:22:04,500 --> 00:22:11,220
helps a lot it can be a difference with

00:22:07,230 --> 00:22:13,039
our packet gen transmit testing facility

00:22:11,220 --> 00:22:16,799
of hitting line rate or not on

00:22:13,039 --> 00:22:18,919
high-speed networks another facility

00:22:16,799 --> 00:22:21,630
that I think is kind of interesting is

00:22:18,919 --> 00:22:24,740
and I kind of have a background on this

00:22:21,630 --> 00:22:30,330
one thing that drives me really crazy is

00:22:24,740 --> 00:22:32,880
our DMA because it takes all all the

00:22:30,330 --> 00:22:35,010
nice things all the nice facilities all

00:22:32,880 --> 00:22:36,840
the nice ways of doing things that we've

00:22:35,010 --> 00:22:39,029
created for people for flexibility out

00:22:36,840 --> 00:22:41,460
of the equation you're really stuck with

00:22:39,029 --> 00:22:43,080
exactly what that card can do and that's

00:22:41,460 --> 00:22:44,970
it you don't get netfilter

00:22:43,080 --> 00:22:46,320
you don't get traffic classification you

00:22:44,970 --> 00:22:48,809
don't get the packet scheduler you don't

00:22:46,320 --> 00:22:50,789
get our pacing algorithms you don't get

00:22:48,809 --> 00:22:53,640
fair queuing you don't get any of these

00:22:50,789 --> 00:22:55,529
facilities and one of the main consumers

00:22:53,640 --> 00:22:57,029
of this technology seems to be financial

00:22:55,529 --> 00:22:59,299
institutions because they want the

00:22:57,029 --> 00:23:03,510
lowest latency possible to process small

00:22:59,299 --> 00:23:05,309
RPC packets so one one thing I really

00:23:03,510 --> 00:23:07,260
like to see is alternative approaches to

00:23:05,309 --> 00:23:09,450
trying to solve this problem I'm not

00:23:07,260 --> 00:23:11,580
saying that any one of these individual

00:23:09,450 --> 00:23:14,700
solutions leads us all the way to what

00:23:11,580 --> 00:23:16,020
our DMA can achieve but I think it's a

00:23:14,700 --> 00:23:17,429
better solution because it doesn't

00:23:16,020 --> 00:23:19,590
bypass everything else that the Linux

00:23:17,429 --> 00:23:21,990
networking stack can do to me busy

00:23:19,590 --> 00:23:27,000
polling is one instance of this this

00:23:21,990 --> 00:23:28,559
kind of problem solving so basically if

00:23:27,000 --> 00:23:30,120
you do a receive message and there's no

00:23:28,559 --> 00:23:31,440
packets in your queue the process just

00:23:30,120 --> 00:23:32,850
goes to sleep and then you're done and

00:23:31,440 --> 00:23:34,260
then you have the whole overhead of

00:23:32,850 --> 00:23:35,580
going to sleep and waking up again this

00:23:34,260 --> 00:23:37,860
is expensive especially if it's

00:23:35,580 --> 00:23:40,020
happening over and over again what often

00:23:37,860 --> 00:23:41,970
happens is the packet the packets there

00:23:40,020 --> 00:23:43,409
we just have to process the inter up go

00:23:41,970 --> 00:23:44,940
through the software interrupt all the

00:23:43,409 --> 00:23:47,640
packets out of the queue during nappy

00:23:44,940 --> 00:23:50,250
poling and then wake up the process and

00:23:47,640 --> 00:23:51,930
he can he can do to receive why don't we

00:23:50,250 --> 00:23:53,400
just go directly into the device driver

00:23:51,930 --> 00:23:55,230
when this happens and do a receive

00:23:53,400 --> 00:23:58,140
message to pull the data out and that's

00:23:55,230 --> 00:24:00,960
what busy polling does is as a facility

00:23:58,140 --> 00:24:05,490
for doing poll ii ii ii ii poll calls

00:24:00,960 --> 00:24:08,370
for event notifications to and this has

00:24:05,490 --> 00:24:10,470
one limitation it doesn't handle well

00:24:08,370 --> 00:24:11,960
lots of connections using this facility

00:24:10,470 --> 00:24:14,670
it's really good for one application

00:24:11,960 --> 00:24:17,640
that's doing our pcs at very low latency

00:24:14,670 --> 00:24:21,060
x' and wants to get the packet as fast

00:24:17,640 --> 00:24:23,220
as possible so i want people to look

00:24:21,060 --> 00:24:24,720
into more solutions like this that get

00:24:23,220 --> 00:24:29,730
us right to the packets that we need as

00:24:24,720 --> 00:24:31,500
fast as possible in a device memory

00:24:29,730 --> 00:24:34,760
allocation batching so this is kind of

00:24:31,500 --> 00:24:38,940
like the culmination of the effort of

00:24:34,760 --> 00:24:40,770
yes / brewer he's been looking to so

00:24:38,940 --> 00:24:44,790
what we need to do to do riot wire rate

00:24:40,770 --> 00:24:48,090
in various perspectives under linux so

00:24:44,790 --> 00:24:49,860
and there have been other presentations

00:24:48,090 --> 00:24:52,260
alexander the other day gave a

00:24:49,860 --> 00:24:54,240
presentation on what our packet budgets

00:24:52,260 --> 00:24:56,220
are and yes we're Conor was playing with

00:24:54,240 --> 00:24:58,610
these numbers too you have something

00:24:56,220 --> 00:25:01,140
like less than 100 nanoseconds to

00:24:58,610 --> 00:25:04,740
process a packet at 10 gigabit when

00:25:01,140 --> 00:25:08,760
forwarding so just one l2 cache access

00:25:04,740 --> 00:25:10,350
could be 12 nanoseconds and so as you

00:25:08,760 --> 00:25:13,100
access more and more cache lines you've

00:25:10,350 --> 00:25:15,470
already busted your budget so memory

00:25:13,100 --> 00:25:19,650
latency is a huge issue

00:25:15,470 --> 00:25:21,390
another thing yes / notice was that we

00:25:19,650 --> 00:25:23,010
really stress out the memory out the

00:25:21,390 --> 00:25:25,320
generic memory allocator in the kernel

00:25:23,010 --> 00:25:27,750
which is extremely general-purpose

00:25:25,320 --> 00:25:29,670
it's not meant it's not tweaked for

00:25:27,750 --> 00:25:32,580
specific workloads it's meant to be a

00:25:29,670 --> 00:25:34,410
well behaving allocator for everybody in

00:25:32,580 --> 00:25:36,960
the kernel and sometimes what that

00:25:34,410 --> 00:25:40,020
uncovers is things like what we what we

00:25:36,960 --> 00:25:43,350
saw here so imagine that you have a full

00:25:40,020 --> 00:25:46,140
transmit cue and you've got tons of

00:25:43,350 --> 00:25:50,190
receive packets coming in typically what

00:25:46,140 --> 00:25:53,550
happens is the transmit cue is perhaps

00:25:50,190 --> 00:25:56,700
some order of magnitude smaller than

00:25:53,550 --> 00:25:59,040
your receive cue or the interval at

00:25:56,700 --> 00:26:00,909
which you've set the transfer cue

00:25:59,040 --> 00:26:02,739
mitigation parameters is

00:26:00,909 --> 00:26:04,210
unbalanced between transmit and receive

00:26:02,739 --> 00:26:06,820
because you're trying to balance the a

00:26:04,210 --> 00:26:08,979
the higher cost of doing receive packets

00:26:06,820 --> 00:26:10,899
and the smaller cost of recycling

00:26:08,979 --> 00:26:13,419
transmit packets so what can end up

00:26:10,899 --> 00:26:16,749
happening is you free up a small number

00:26:13,419 --> 00:26:20,409
of transmit frames when you get your

00:26:16,749 --> 00:26:22,359
done and then you have your refilling

00:26:20,409 --> 00:26:24,729
half of your receive ring to replenish

00:26:22,359 --> 00:26:26,320
the empty entries and that's a larger

00:26:24,729 --> 00:26:28,690
number perhaps twice as many packets

00:26:26,320 --> 00:26:31,239
this uneven allocation and freeing

00:26:28,690 --> 00:26:33,999
pattern is really bad first for the slab

00:26:31,239 --> 00:26:35,679
in slab alligators in a kernel another

00:26:33,999 --> 00:26:37,659
thing is we typically like I just

00:26:35,679 --> 00:26:39,970
mentioned allocate a ton of these at

00:26:37,659 --> 00:26:42,609
once we never allocate just one in the

00:26:39,970 --> 00:26:44,289
really expensive paths so yes were

00:26:42,609 --> 00:26:46,029
allocated a test framework it was a

00:26:44,289 --> 00:26:48,279
basically a networking specific a

00:26:46,029 --> 00:26:50,710
locator called queue mempool as an

00:26:48,279 --> 00:26:52,779
experiment just to see what a different

00:26:50,710 --> 00:26:56,139
allocator facility would would do and he

00:26:52,779 --> 00:26:57,669
got extremely higher performance simply

00:26:56,139 --> 00:26:59,049
by adding batching facilities at

00:26:57,669 --> 00:27:02,109
basically an interface where you could

00:26:59,049 --> 00:27:03,429
say give me 64 buffers of this size from

00:27:02,109 --> 00:27:05,409
the alligator instead of just one at a

00:27:03,429 --> 00:27:06,879
time over and over again

00:27:05,409 --> 00:27:08,679
another thing about queue mempool that

00:27:06,879 --> 00:27:11,109
was really cool is he used lock free

00:27:08,679 --> 00:27:13,899
queue data structure to pass these

00:27:11,109 --> 00:27:16,179
objects back and forth so there was it

00:27:13,899 --> 00:27:19,869
was it was less expensive from an SP

00:27:16,179 --> 00:27:22,059
perspective so yes were presented these

00:27:19,869 --> 00:27:25,899
results and he got the attention of the

00:27:22,059 --> 00:27:27,580
slab in slow maintainer and he proposed

00:27:25,899 --> 00:27:30,429
recently a set of slab and slab

00:27:27,580 --> 00:27:31,929
extensions to allow batch to allocation

00:27:30,429 --> 00:27:34,359
how the slab allocator and I hope that

00:27:31,929 --> 00:27:35,679
that moves forward at some point in time

00:27:34,359 --> 00:27:37,450
in the future I mean that was really

00:27:35,679 --> 00:27:39,759
interesting work to me

00:27:37,450 --> 00:27:44,590
so there's just one component of solving

00:27:39,759 --> 00:27:46,269
the the forwarding overhead problem X

00:27:44,590 --> 00:27:48,340
MIT Moore was kind of part of that too

00:27:46,269 --> 00:27:50,889
that helps so that that helps as well so

00:27:48,340 --> 00:27:53,859
there's there's lots of areas Alexander

00:27:50,889 --> 00:27:57,429
is working on optimizing our fib tree

00:27:53,859 --> 00:27:59,889
data structure look up the story with

00:27:57,429 --> 00:28:02,349
that is that we use something called LC

00:27:59,889 --> 00:28:05,580
tree which is a longer longest matching

00:28:02,349 --> 00:28:07,960
prefix looking up lookup algorithm

00:28:05,580 --> 00:28:09,489
problem is it was directly translated

00:28:07,960 --> 00:28:11,499
from the researchers code which was in

00:28:09,489 --> 00:28:13,479
Java and although he did a reasonable

00:28:11,499 --> 00:28:13,960
job there's a lot of optimizations that

00:28:13,479 --> 00:28:16,600
were still

00:28:13,960 --> 00:28:18,309
possible there are cases where Alexander

00:28:16,600 --> 00:28:20,020
found that we had say ten lines of code

00:28:18,309 --> 00:28:22,390
twiddling bits and trying to looking up

00:28:20,020 --> 00:28:25,600
prefixes and you could deduce this to

00:28:22,390 --> 00:28:28,299
one logical bit operation on a single

00:28:25,600 --> 00:28:30,549
line of code so you'd be surprised how

00:28:28,299 --> 00:28:34,450
many of those still exist in the in the

00:28:30,549 --> 00:28:37,120
networking tree I'm kind of done with

00:28:34,450 --> 00:28:38,740
the material I've I've come up with I

00:28:37,120 --> 00:28:40,390
could talk about some other things or if

00:28:38,740 --> 00:28:43,440
anyone has any questions I really would

00:28:40,390 --> 00:28:43,440
like to feel those right now

00:28:56,990 --> 00:29:02,730
so I don't have the whole history here

00:29:00,330 --> 00:29:05,940
but we've been dealing in sort of the

00:29:02,730 --> 00:29:09,030
home router space with samba and one of

00:29:05,940 --> 00:29:12,450
the areas that you find in samba is a

00:29:09,030 --> 00:29:13,560
transmit works pretty well because

00:29:12,450 --> 00:29:15,390
you've got a whole page and you're

00:29:13,560 --> 00:29:17,040
actually you know can transmit out you

00:29:15,390 --> 00:29:19,320
can make all the optimizations to never

00:29:17,040 --> 00:29:23,820
do any copying receive of course is

00:29:19,320 --> 00:29:26,490
scattered all over the place what what

00:29:23,820 --> 00:29:28,500
that results in very often is a very

00:29:26,490 --> 00:29:31,200
disparate difference between read and

00:29:28,500 --> 00:29:33,690
write and Samba applications what has

00:29:31,200 --> 00:29:35,370
been looked at where you know there any

00:29:33,690 --> 00:29:37,980
thoughts or anyone working on trying to

00:29:35,370 --> 00:29:40,380
look at that problem in more detail okay

00:29:37,980 --> 00:29:43,980
so one thing I'll mention about the

00:29:40,380 --> 00:29:45,870
received path is especially over the

00:29:43,980 --> 00:29:47,490
last four or five years I'd say that

00:29:45,870 --> 00:29:49,380
we've been getting increasingly

00:29:47,490 --> 00:29:51,990
sophisticated receive buffering

00:29:49,380 --> 00:29:54,270
facilities and nickk nickk cards so what

00:29:51,990 --> 00:29:56,040
this means is instead of just having

00:29:54,270 --> 00:29:57,960
flat linear buffers we're starting to

00:29:56,040 --> 00:30:00,630
use blocks of various sizes and one of

00:29:57,960 --> 00:30:02,430
those block sizes page size and if you

00:30:00,630 --> 00:30:03,930
could land that the buffers directly in

00:30:02,430 --> 00:30:06,180
the page the way that they would want to

00:30:03,930 --> 00:30:08,400
be in the file system page cache or

00:30:06,180 --> 00:30:10,920
samba x' application buffers or what

00:30:08,400 --> 00:30:12,420
have you you could start just getting

00:30:10,920 --> 00:30:15,540
things directly to users base and

00:30:12,420 --> 00:30:16,860
avoiding copies and optimal scenarios so

00:30:15,540 --> 00:30:20,670
that's one angle to kind of think about

00:30:16,860 --> 00:30:21,990
this but you're definitely right the the

00:30:20,670 --> 00:30:26,430
received path could definitely be always

00:30:21,990 --> 00:30:30,150
be more more expensive but I kind of

00:30:26,430 --> 00:30:31,710
want to encourage people to wrap their

00:30:30,150 --> 00:30:34,230
head around the receive problem from

00:30:31,710 --> 00:30:37,430
this following perspective everyone

00:30:34,230 --> 00:30:39,720
wants to do eliminate the copies and not

00:30:37,430 --> 00:30:42,390
somehow avoid all the overhead of the

00:30:39,720 --> 00:30:44,370
receive side but you have to understand

00:30:42,390 --> 00:30:47,100
that data isn't just as opaque thing we

00:30:44,370 --> 00:30:48,810
push here and there people operate on

00:30:47,100 --> 00:30:50,100
this data and then once they operate on

00:30:48,810 --> 00:30:51,390
that data they write to the data

00:30:50,100 --> 00:30:54,300
therefore they need to have a private

00:30:51,390 --> 00:30:56,280
copy of this data so a lot of these

00:30:54,300 --> 00:30:57,990
benchmarks that just try to say how fast

00:30:56,280 --> 00:30:59,670
can you push things into user space they

00:30:57,990 --> 00:31:00,840
missed it the fact that you have to

00:30:59,670 --> 00:31:01,710
touch the data because that's what the

00:31:00,840 --> 00:31:03,600
application

00:31:01,710 --> 00:31:05,220
the applications gonna execute some kind

00:31:03,600 --> 00:31:06,990
of computation on that data afterwards

00:31:05,220 --> 00:31:09,270
so that's another angle of this problem

00:31:06,990 --> 00:31:12,330
so my answer is I'm hoping that nick

00:31:09,270 --> 00:31:13,740
buffering facilities facilitate

00:31:12,330 --> 00:31:15,600
improving performance to a certain

00:31:13,740 --> 00:31:18,090
extent but realize that there is a kind

00:31:15,600 --> 00:31:20,280
of like a wall up there because the CPUs

00:31:18,090 --> 00:31:22,170
speed kind of determines how fast we can

00:31:20,280 --> 00:31:23,400
process receive information to a certain

00:31:22,170 --> 00:31:28,310
extent so I hope that answers your

00:31:23,400 --> 00:31:28,310
question anyone else

00:31:31,850 --> 00:31:37,670
okay I'd like to thank some people I

00:31:35,810 --> 00:31:40,160
like to thank Lynn is for giving us a

00:31:37,670 --> 00:31:42,290
playground to be in for the last couple

00:31:40,160 --> 00:31:44,180
decades without him we wouldn't have

00:31:42,290 --> 00:31:45,890
this cool toy to play with every day and

00:31:44,180 --> 00:31:47,450
I'd like to thank Jamal course for

00:31:45,890 --> 00:31:48,740
running the conference this none of this

00:31:47,450 --> 00:31:51,910
would happen over the past couple of

00:31:48,740 --> 00:31:54,440
days without him and in advance

00:31:51,910 --> 00:31:56,630
I'd like to thank that really cool

00:31:54,440 --> 00:31:58,400
vendor who's going to add the first

00:31:56,630 --> 00:32:00,440
Hardware switching driver upstream you

00:31:58,400 --> 00:32:01,640
have no idea how cool you're going to be

00:32:00,440 --> 00:32:03,680
when you do that you would be the

00:32:01,640 --> 00:32:05,920
Trailblazer in the Linux networking

00:32:03,680 --> 00:32:05,920

YouTube URL: https://www.youtube.com/watch?v=QDxM83YaI0E


