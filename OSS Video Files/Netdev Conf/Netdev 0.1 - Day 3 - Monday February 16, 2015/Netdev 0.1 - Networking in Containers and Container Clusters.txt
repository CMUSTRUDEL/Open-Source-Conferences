Title: Netdev 0.1 - Networking in Containers and Container Clusters
Publication date: 2015-03-21
Playlist: Netdev 0.1 - Day 3 - Monday February 16, 2015
Description: 
	Networking in Containers and Container Clusters by Victor Marmol
February 2015

From netdev01.org:
 Containers have recently risen in popularity tremendously in the Linux world. Their promise of fast, light, isolated, and secure runtime and deployment appeals to many user space developers. One of the most important aspects of containers today is networking. Container networking configurations are almost as varied as there are container users, but there is a common emphasis on flexibility, performance, and security. Using Docker's libcontainer we will present and showcase many of the popular networking setups, their uses, and their problems today.

A second aspect we will explore are containers in clusters. Systems like Kubernetes manage containers across clusters of machines. Container-based applications in these clusters communicate almost exclusively through the network; discovery, linking, and synchronization happen all in the network. We will present and showcase the history, the setups, and the problems of networking in Kubernetes clusters. We will also cover common patterns of handling networking for multi-container applications in clusters. 

https://www.netdev01.org

This video is licensed under Creative Commons Attribution-ShareAlike 4.0 International license. Feel free to download and distribute.
Captions: 
	00:00:07,980 --> 00:00:11,380
alright we'll go ahead and get started

00:00:09,820 --> 00:00:14,680
given that apparently we're already late

00:00:11,380 --> 00:00:16,150
don't worry I'll keep it short so hi my

00:00:14,680 --> 00:00:16,810
name is Victor I'm a software engineer

00:00:16,150 --> 00:00:18,730
at Google

00:00:16,810 --> 00:00:20,170
I work in - in the containers

00:00:18,730 --> 00:00:22,210
infrastructure team I'm part of the team

00:00:20,170 --> 00:00:24,160
that runs all the containers across all

00:00:22,210 --> 00:00:26,140
machines at Google it's an interesting

00:00:24,160 --> 00:00:28,090
job but thankfully I only have to worry

00:00:26,140 --> 00:00:29,619
about the user space side so sorry for

00:00:28,090 --> 00:00:32,470
you guys that have to actually make the

00:00:29,619 --> 00:00:33,820
stuff underneath work given that I was

00:00:32,470 --> 00:00:37,540
asked to come here and talk a little bit

00:00:33,820 --> 00:00:39,040
about how we set up networking on our

00:00:37,540 --> 00:00:40,000
side on user space so I'll be talking a

00:00:39,040 --> 00:00:42,280
little bit about how we set up

00:00:40,000 --> 00:00:43,570
containers on the actual machines as

00:00:42,280 --> 00:00:45,489
well as how do we set up clusters of

00:00:43,570 --> 00:00:47,500
containers how do those things route and

00:00:45,489 --> 00:00:48,640
network to each other so the talk will

00:00:47,500 --> 00:00:49,840
be a little bit different from the other

00:00:48,640 --> 00:00:51,399
ones you've heard given that we'll be a

00:00:49,840 --> 00:00:53,320
lot less technical much more of a use

00:00:51,399 --> 00:00:54,700
case but hopefully it'll be useful

00:00:53,320 --> 00:00:56,680
material and please point out anything

00:00:54,700 --> 00:00:57,850
that we're doing incorrectly I'm sure

00:00:56,680 --> 00:01:00,820
that we're doing plenty of those things

00:00:57,850 --> 00:01:02,379
and we'd be happy to fix them so the

00:01:00,820 --> 00:01:04,270
first thing I want to talk about is what

00:01:02,379 --> 00:01:05,649
containers are not really because I

00:01:04,270 --> 00:01:08,050
don't expect anyone to know but because

00:01:05,649 --> 00:01:09,310
everybody has their own definition so in

00:01:08,050 --> 00:01:11,440
general we all agree that they're like

00:01:09,310 --> 00:01:13,659
these lightweight VMs or their allow

00:01:11,440 --> 00:01:15,760
some form of virtualization without the

00:01:13,659 --> 00:01:17,020
overhead of the hypervisor I generally

00:01:15,760 --> 00:01:19,119
just like to say that we're able to

00:01:17,020 --> 00:01:20,740
isolate the resources namespaces small

00:01:19,119 --> 00:01:24,700
systems and capabilities of a machine

00:01:20,740 --> 00:01:26,890
and show that one unique view to an

00:01:24,700 --> 00:01:28,360
application and so specifically today

00:01:26,890 --> 00:01:30,040
since this is a networking conference

00:01:28,360 --> 00:01:34,030
we'll talk about networking and the two

00:01:30,040 --> 00:01:35,200
big aspects of networking on after I

00:01:34,030 --> 00:01:37,570
guess there's three big aspects of

00:01:35,200 --> 00:01:39,790
networking when it comes to containers

00:01:37,570 --> 00:01:42,130
are two namespaces UTS primarily doing

00:01:39,790 --> 00:01:44,140
hostname isolation and the network

00:01:42,130 --> 00:01:47,290
namespace which gives you literally an

00:01:44,140 --> 00:01:49,119
entire network stack available to you

00:01:47,290 --> 00:01:52,360
and you can customize it however you'd

00:01:49,119 --> 00:01:54,640
like the third one is the C group the

00:01:52,360 --> 00:01:55,930
net C groups we don't really talk much

00:01:54,640 --> 00:01:58,149
about those today because they're

00:01:55,930 --> 00:01:59,439
actually not particularly used in the

00:01:58,149 --> 00:02:01,450
things that I'll be talking about today

00:01:59,439 --> 00:02:03,969
there are certain applications that do

00:02:01,450 --> 00:02:05,110
use them we do use them in Google but a

00:02:03,969 --> 00:02:08,200
lot of the open source stuff that we'll

00:02:05,110 --> 00:02:10,269
be talking about don't use it yet and so

00:02:08,200 --> 00:02:11,590
when I talk about containers today I'm

00:02:10,269 --> 00:02:13,360
really gonna be talking about docker and

00:02:11,590 --> 00:02:15,220
kind of everybody now is all excited

00:02:13,360 --> 00:02:16,720
about docker and everybody everything

00:02:15,220 --> 00:02:18,700
you hear about containers is docker and

00:02:16,720 --> 00:02:19,340
I think the primary reason for that is

00:02:18,700 --> 00:02:21,590
that Dockers

00:02:19,340 --> 00:02:23,629
very easy to package applications and

00:02:21,590 --> 00:02:25,400
file systems and ship those around and

00:02:23,629 --> 00:02:27,110
that turned out to be a problem that a

00:02:25,400 --> 00:02:28,670
lot of people had and while all the

00:02:27,110 --> 00:02:30,110
container people kind of you know went

00:02:28,670 --> 00:02:32,150
up to that and said well we can also do

00:02:30,110 --> 00:02:34,400
all these other things that many people

00:02:32,150 --> 00:02:36,080
don't quite know that they want yet but

00:02:34,400 --> 00:02:37,580
they're getting for free anyway so

00:02:36,080 --> 00:02:39,410
docker is an open source project it's

00:02:37,580 --> 00:02:41,120
really entirely and go you'll find that

00:02:39,410 --> 00:02:43,010
is a common trend amongst all the things

00:02:41,120 --> 00:02:45,230
I will talk about today and it promises

00:02:43,010 --> 00:02:46,879
to be hardware and platform agnostic so

00:02:45,230 --> 00:02:48,530
hard done a pretty good job of both

00:02:46,879 --> 00:02:50,840
they're actually porting it right now to

00:02:48,530 --> 00:02:51,829
arm and things like PPC so those will be

00:02:50,840 --> 00:02:54,049
interesting to see

00:02:51,829 --> 00:02:56,690
I will also be specifically talking

00:02:54,049 --> 00:02:59,510
about a lower layer of docker called Lib

00:02:56,690 --> 00:03:01,640
container so Lib container is similar to

00:02:59,510 --> 00:03:03,410
what you might see something like lxc it

00:03:01,640 --> 00:03:05,810
is a container runtime it implements

00:03:03,410 --> 00:03:07,790
containers on top of the kernel this is

00:03:05,810 --> 00:03:10,400
written also primarily in go with a

00:03:07,790 --> 00:03:12,920
bunch of stuff in C it is very very very

00:03:10,400 --> 00:03:14,959
interesting to try to write things right

00:03:12,920 --> 00:03:16,790
on top of the kernel lingo you quickly

00:03:14,959 --> 00:03:18,890
run into the limitations of the go

00:03:16,790 --> 00:03:22,489
runtime and then trying to circumvent

00:03:18,890 --> 00:03:23,720
those is an act of its own and so today

00:03:22,489 --> 00:03:25,069
when we talk about containers we'll be

00:03:23,720 --> 00:03:27,109
talking about docker container is built

00:03:25,069 --> 00:03:28,760
with lib container specifically and

00:03:27,109 --> 00:03:32,569
that's actually a container abstraction

00:03:28,760 --> 00:03:34,069
we'll be using and so networking in

00:03:32,569 --> 00:03:35,989
docker is actually relatively simple

00:03:34,069 --> 00:03:37,310
through time has gone more and more

00:03:35,989 --> 00:03:38,930
complicated and this is actually one of

00:03:37,310 --> 00:03:40,700
the parts where everybody kind of wants

00:03:38,930 --> 00:03:42,049
to have their own way that they set up

00:03:40,700 --> 00:03:43,609
their own networking and everybody's

00:03:42,049 --> 00:03:44,989
data center is different so we've

00:03:43,609 --> 00:03:47,019
actually run into a lot of issues and

00:03:44,989 --> 00:03:49,340
are working very heavily to make it

00:03:47,019 --> 00:03:52,970
customizable but in general the rough

00:03:49,340 --> 00:03:54,530
things that we allow today hostname as

00:03:52,970 --> 00:03:56,510
well as networking routes what network

00:03:54,530 --> 00:03:59,150
we mean is how we actually get exposed

00:03:56,510 --> 00:04:00,620
to the outside world how everybody can

00:03:59,150 --> 00:04:02,239
address the container and how the

00:04:00,620 --> 00:04:04,549
container exposes its services to the

00:04:02,239 --> 00:04:05,959
outside world as well as routes how do

00:04:04,549 --> 00:04:08,120
we actually route the traffic to and

00:04:05,959 --> 00:04:09,340
from the container and we'll talk about

00:04:08,120 --> 00:04:11,870
each of those in turn

00:04:09,340 --> 00:04:14,870
so docker a very roughly tries to give

00:04:11,870 --> 00:04:17,299
these large ideas of strategies so kind

00:04:14,870 --> 00:04:19,039
of it tries to give you pre-made

00:04:17,299 --> 00:04:20,989
networking setups that you can use

00:04:19,039 --> 00:04:23,270
there's four main ones that I'll talk

00:04:20,989 --> 00:04:26,330
about today so the first one is loopback

00:04:23,270 --> 00:04:27,860
this is sort of the no op strategy

00:04:26,330 --> 00:04:29,659
primarily used for containers that don't

00:04:27,860 --> 00:04:31,280
need a networking because it just has a

00:04:29,659 --> 00:04:33,260
loopback interface nothing else nothing

00:04:31,280 --> 00:04:36,020
else is exposed outside of the network

00:04:33,260 --> 00:04:38,240
the next one is the vs strategy so this

00:04:36,020 --> 00:04:39,680
is actually the most popular given that

00:04:38,240 --> 00:04:41,510
it is the default strategy so this is

00:04:39,680 --> 00:04:44,570
what most docker containers use today

00:04:41,510 --> 00:04:45,830
with a few exceptions and generally what

00:04:44,570 --> 00:04:47,660
it does is what we'll see on the right

00:04:45,830 --> 00:04:49,880
side of the screen where we create a VF

00:04:47,660 --> 00:04:51,920
pair we put one end of the pipe inside

00:04:49,880 --> 00:04:53,870
the container the other end outside of

00:04:51,920 --> 00:04:55,730
the namespaces and we cook it up to the

00:04:53,870 --> 00:04:58,160
docker bridge and this bridge is itself

00:04:55,730 --> 00:04:59,870
connected to yet zero or wherever you're

00:04:58,160 --> 00:05:01,670
getting your networking from and we'll

00:04:59,870 --> 00:05:03,800
talk a little bit more about what those

00:05:01,670 --> 00:05:05,390
things do one of the things that we have

00:05:03,800 --> 00:05:07,100
notice is that we have a significant

00:05:05,390 --> 00:05:09,350
performance degradation from this we've

00:05:07,100 --> 00:05:12,170
measured it to be about 50% of what some

00:05:09,350 --> 00:05:13,550
of the other strategies are today so for

00:05:12,170 --> 00:05:15,580
a lot of the users of docker they don't

00:05:13,550 --> 00:05:18,560
particularly care it's not a problem yet

00:05:15,580 --> 00:05:22,610
but we found it to be a big problem in a

00:05:18,560 --> 00:05:23,900
lot of our use cases and so by the way

00:05:22,610 --> 00:05:26,540
feel free to ask any questions any time

00:05:23,900 --> 00:05:28,670
so looking at the vs strategy in general

00:05:26,540 --> 00:05:30,920
looking at how connections coming in so

00:05:28,670 --> 00:05:32,330
again we have these this es 0 connects

00:05:30,920 --> 00:05:36,200
to the bridge and the bridge itself

00:05:32,330 --> 00:05:38,600
connected to the actual containers so by

00:05:36,200 --> 00:05:40,760
default all communication to the

00:05:38,600 --> 00:05:42,800
containers is blocked through all ports

00:05:40,760 --> 00:05:46,700
the only way that we ever get any

00:05:42,800 --> 00:05:48,830
information incoming is if the container

00:05:46,700 --> 00:05:50,780
itself asks for a port to be exposed and

00:05:48,830 --> 00:05:53,120
so will do is that will map a port from

00:05:50,780 --> 00:05:56,330
the container for example my port 80 to

00:05:53,120 --> 00:05:58,130
the hosts 6 for 73 and so any requests

00:05:56,330 --> 00:05:59,660
coming to the host on 6 4 7 3 actually

00:05:58,130 --> 00:06:01,610
gets exposed to the container otherwise

00:05:59,660 --> 00:06:04,790
the container has no way to accept

00:06:01,610 --> 00:06:06,980
incoming traffic going out it's a little

00:06:04,790 --> 00:06:09,260
different the the container pretty much

00:06:06,980 --> 00:06:10,940
masquerades as the host in order to be

00:06:09,260 --> 00:06:12,770
able to reach the rest of the internet

00:06:10,940 --> 00:06:14,060
if a container wants to talk to another

00:06:12,770 --> 00:06:16,310
container on the same machine that

00:06:14,060 --> 00:06:17,210
travel is actually itself blocked the

00:06:16,310 --> 00:06:18,440
only way that you address another

00:06:17,210 --> 00:06:20,330
container is the same way that you would

00:06:18,440 --> 00:06:21,770
address any ducking container on the

00:06:20,330 --> 00:06:24,710
network you would have to go through the

00:06:21,770 --> 00:06:26,540
host and then down through the map port

00:06:24,710 --> 00:06:28,160
all the way down and we'll talk a little

00:06:26,540 --> 00:06:31,190
bit about what we do in other places

00:06:28,160 --> 00:06:33,910
since this kind of is not a very good

00:06:31,190 --> 00:06:36,290
starting point for a lot of applications

00:06:33,910 --> 00:06:38,900
and so the last two strategies I wanted

00:06:36,290 --> 00:06:40,940
to talk about one of them is that net NS

00:06:38,900 --> 00:06:42,830
strategy which in essence is I want to

00:06:40,940 --> 00:06:44,990
use that guys in that word namespaces it

00:06:42,830 --> 00:06:46,370
allows you to share Network name spaces

00:06:44,990 --> 00:06:48,979
between containers

00:06:46,370 --> 00:06:50,870
sort of sounds innocuous but there's two

00:06:48,979 --> 00:06:51,949
very large use cases one which we will

00:06:50,870 --> 00:06:54,620
talk about the second half of the

00:06:51,949 --> 00:06:55,970
presentation and the big one is that you

00:06:54,620 --> 00:06:58,370
can actually say hey I want to use that

00:06:55,970 --> 00:07:00,590
guy's now remain space and that and

00:06:58,370 --> 00:07:02,449
those being the hosts not ruining space

00:07:00,590 --> 00:07:03,889
this is heavily used by a lot of people

00:07:02,449 --> 00:07:05,600
who don't want to pay that performance

00:07:03,889 --> 00:07:06,800
penalty because at that point you do get

00:07:05,600 --> 00:07:08,660
the native performance you come to

00:07:06,800 --> 00:07:10,190
expect although at this point you are

00:07:08,660 --> 00:07:12,110
pretty much exposing your container and

00:07:10,190 --> 00:07:13,430
all its ports you can get a lot of

00:07:12,110 --> 00:07:15,949
issues where two containers try to bind

00:07:13,430 --> 00:07:17,750
to the same port if you can manage your

00:07:15,949 --> 00:07:19,460
network that way it turns out pretty

00:07:17,750 --> 00:07:22,280
well if you don't have that much

00:07:19,460 --> 00:07:23,960
discipline it doesn't work as well the

00:07:22,280 --> 00:07:27,139
last strategy I wanted to talk about is

00:07:23,960 --> 00:07:28,280
VLAN both Mac VLAN as well as IP VLAN

00:07:27,139 --> 00:07:30,289
coming soon

00:07:28,280 --> 00:07:32,630
there was a talk earlier today about IP

00:07:30,289 --> 00:07:34,430
VLAN were particularly very very excited

00:07:32,630 --> 00:07:35,660
and we'll talk more about it on the

00:07:34,430 --> 00:07:37,639
second half as to why that's the case

00:07:35,660 --> 00:07:39,440
but this is something that's currently

00:07:37,639 --> 00:07:40,970
in the works and adding this actual

00:07:39,440 --> 00:07:44,800
driver this is something that's deemed

00:07:40,970 --> 00:07:47,690
necessary for terms of performance and

00:07:44,800 --> 00:07:49,460
so some of the things that the darker

00:07:47,690 --> 00:07:51,080
world is working on right now as I

00:07:49,460 --> 00:07:53,960
mentioned is definitely making things

00:07:51,080 --> 00:07:55,400
much more pluggable every other day we

00:07:53,960 --> 00:07:57,530
there is a complain about some

00:07:55,400 --> 00:07:59,870
networking strategy not being sufficient

00:07:57,530 --> 00:08:01,580
and so we're really working and trying

00:07:59,870 --> 00:08:02,900
to make that as a pluggable as possible

00:08:01,580 --> 00:08:05,450
so people can run their own networks the

00:08:02,900 --> 00:08:07,340
way they'd like to also performance as I

00:08:05,450 --> 00:08:09,800
mentioned before Mac via an IP VLAN or

00:08:07,340 --> 00:08:12,169
top of our list and a native support for

00:08:09,800 --> 00:08:14,510
a checkpoint or not so primarily using

00:08:12,169 --> 00:08:16,520
creo there's a lot of very late work

00:08:14,510 --> 00:08:18,889
into that I think we'll be getting that

00:08:16,520 --> 00:08:20,630
very soon which allows us to do other

00:08:18,889 --> 00:08:24,229
interesting things related to migration

00:08:20,630 --> 00:08:25,400
and such so the second half of the

00:08:24,229 --> 00:08:26,810
presentation that I want her to talk

00:08:25,400 --> 00:08:29,060
about something that's particularly

00:08:26,810 --> 00:08:30,949
interesting to me so this is networking

00:08:29,060 --> 00:08:32,959
container clusters when we talk about

00:08:30,949 --> 00:08:34,969
container clusters generally talk about

00:08:32,959 --> 00:08:36,200
clusters data centers things like that

00:08:34,969 --> 00:08:38,029
look like this so this is actually a

00:08:36,200 --> 00:08:39,919
picture inside one of Google's data

00:08:38,029 --> 00:08:42,169
centers just a very large row of

00:08:39,919 --> 00:08:43,789
machines cluster in general one set of

00:08:42,169 --> 00:08:46,880
aggregate compute that we can address

00:08:43,789 --> 00:08:48,680
and make usable for our needs usually

00:08:46,880 --> 00:08:50,839
very tightly packed in terms of network

00:08:48,680 --> 00:08:52,760
you have very fast connection between a

00:08:50,839 --> 00:08:54,410
those given machines this is actually a

00:08:52,760 --> 00:08:55,850
really nice picture to see at night if

00:08:54,410 --> 00:08:58,730
for whatever reason you're alone in a

00:08:55,850 --> 00:09:00,459
data center at night because all of

00:08:58,730 --> 00:09:05,859
these do have their own

00:09:00,459 --> 00:09:07,299
and it's a very nice sight and so Google

00:09:05,859 --> 00:09:08,920
has been running container closers for a

00:09:07,299 --> 00:09:11,350
very long time we usually say about ten

00:09:08,920 --> 00:09:13,420
eleven years have a sticker here of the

00:09:11,350 --> 00:09:14,470
10th anniversary of our cluster

00:09:13,420 --> 00:09:16,179
management system that manages

00:09:14,470 --> 00:09:18,129
containers and so we've been running

00:09:16,179 --> 00:09:19,869
containers in some form or another for a

00:09:18,129 --> 00:09:21,850
very long time and you know we think

00:09:19,869 --> 00:09:22,959
that sort of we we do it relatively well

00:09:21,850 --> 00:09:24,100
at least you know well enough to

00:09:22,959 --> 00:09:27,279
convince ourselves that we should talk

00:09:24,100 --> 00:09:28,720
about it publicly and so we want to talk

00:09:27,279 --> 00:09:30,429
about kubernetes so kubernetes is a

00:09:28,720 --> 00:09:32,529
project that we released open-source a

00:09:30,429 --> 00:09:34,329
little over six months ago what this is

00:09:32,529 --> 00:09:36,489
is it's builds on top of everything that

00:09:34,329 --> 00:09:37,839
we've done inside google of after

00:09:36,489 --> 00:09:39,189
running the assistance for such a long

00:09:37,839 --> 00:09:41,290
time we decided hey look there's these

00:09:39,189 --> 00:09:42,730
set of abstractions that we think are

00:09:41,290 --> 00:09:44,139
interesting there's a set of things that

00:09:42,730 --> 00:09:46,480
we think would be useful to the outside

00:09:44,139 --> 00:09:48,609
world let's talk about him and let's get

00:09:46,480 --> 00:09:50,889
people excited about this and so

00:09:48,609 --> 00:09:52,959
kubernetes is Greek for a helmsman or

00:09:50,889 --> 00:09:54,519
governor it was the only word that we

00:09:52,959 --> 00:09:57,220
could find that wasn't trademarked and

00:09:54,519 --> 00:09:59,679
still pronounceable so that's how we got

00:09:57,220 --> 00:10:01,959
the name so this is itself a container

00:09:59,679 --> 00:10:03,040
orchestrated this is a system a way that

00:10:01,959 --> 00:10:04,929
you run your systems with certain

00:10:03,040 --> 00:10:06,639
abstractions in order to manage large

00:10:04,929 --> 00:10:08,949
amounts of compute and be able to

00:10:06,639 --> 00:10:11,290
address scheduled and run workloads on

00:10:08,949 --> 00:10:14,259
top of it as I mentioned it's open

00:10:11,290 --> 00:10:17,199
source and also written and go and it's

00:10:14,259 --> 00:10:18,369
primarily targeted at the tag line we

00:10:17,199 --> 00:10:20,199
always say is that you want to manage

00:10:18,369 --> 00:10:22,029
your applications not your machines and

00:10:20,199 --> 00:10:23,290
the thing that we always try to say is

00:10:22,029 --> 00:10:25,269
about the difference between pet and

00:10:23,290 --> 00:10:26,649
cattle of like pet is something you take

00:10:25,269 --> 00:10:28,629
care if you give a name you're worried

00:10:26,649 --> 00:10:31,059
about cattle is something you shoot and

00:10:28,629 --> 00:10:32,709
kill when they get sick and so we try to

00:10:31,059 --> 00:10:34,389
think of our computers cattle as opposed

00:10:32,709 --> 00:10:36,339
to pet if one of them comes and goes

00:10:34,389 --> 00:10:37,839
that's ok the infrastructure will handle

00:10:36,339 --> 00:10:39,610
that and we'll take care of making sure

00:10:37,839 --> 00:10:41,829
that our system works in general instead

00:10:39,610 --> 00:10:43,959
of having to manually care and prune for

00:10:41,829 --> 00:10:46,989
each one of our instances and so in

00:10:43,959 --> 00:10:48,249
general at a 10,000 feet view this is

00:10:46,989 --> 00:10:50,709
what a kubernetes cluster looks like

00:10:48,249 --> 00:10:53,649
happy faces are now so we'll have some

00:10:50,709 --> 00:10:55,119
users on the left side trying to do

00:10:53,649 --> 00:10:57,579
useful work through whatever means we

00:10:55,119 --> 00:11:00,819
allow them API CLI UI on the right side

00:10:57,579 --> 00:11:02,649
we have a large number of of compute

00:11:00,819 --> 00:11:04,209
nodes each of these is running the

00:11:02,649 --> 00:11:06,970
kubernetes agent which we call the

00:11:04,209 --> 00:11:08,889
cubelet this agent is in talks with what

00:11:06,970 --> 00:11:11,649
we call the kubernetes master the master

00:11:08,889 --> 00:11:15,339
is a node or a replicated set of nodes

00:11:11,649 --> 00:11:17,139
that run a set of resources that allow

00:11:15,339 --> 00:11:18,850
it to run the actual cluster so in this

00:11:17,139 --> 00:11:20,980
example we have the API server actually

00:11:18,850 --> 00:11:22,389
exposes all the endpoints and it's what

00:11:20,980 --> 00:11:24,100
everybody talks to and then this

00:11:22,389 --> 00:11:26,740
scheduler which is a component that as

00:11:24,100 --> 00:11:28,749
the name implies schedules work units on

00:11:26,740 --> 00:11:29,649
each of the machines things that you

00:11:28,749 --> 00:11:31,689
won't see here are things like

00:11:29,649 --> 00:11:34,029
management of machines management of the

00:11:31,689 --> 00:11:35,709
actual scheduled units if any of them go

00:11:34,029 --> 00:11:38,050
down when you reschedule them when you

00:11:35,709 --> 00:11:41,860
make sure that the machines are up and

00:11:38,050 --> 00:11:43,240
running as we expect them to be and so

00:11:41,860 --> 00:11:44,499
there's two key abstractions in

00:11:43,240 --> 00:11:46,329
kubernetes that I want to talk about

00:11:44,499 --> 00:11:47,769
both because they're important to

00:11:46,329 --> 00:11:49,480
kubernetes as well as because they're

00:11:47,769 --> 00:11:51,399
the ones that are interesting in terms

00:11:49,480 --> 00:11:53,439
of networking so the first one of these

00:11:51,399 --> 00:11:56,110
is Potts and as the name would imply up

00:11:53,439 --> 00:11:58,449
you know Pete peas in a pod or a pot of

00:11:56,110 --> 00:11:59,769
oils it's a small group of containers

00:11:58,449 --> 00:12:01,540
it's a group of containers that are

00:11:59,769 --> 00:12:03,730
tightly knit so these are containers

00:12:01,540 --> 00:12:05,379
that you run on the same machine you Co

00:12:03,730 --> 00:12:07,509
schedule them so general we say they

00:12:05,379 --> 00:12:09,670
share the same fate they share the same

00:12:07,509 --> 00:12:11,290
resources so this wouldn't be like your

00:12:09,670 --> 00:12:12,579
front end and your back end this would

00:12:11,290 --> 00:12:15,220
be something like the example you have

00:12:12,579 --> 00:12:17,050
here where you have a web server and a

00:12:15,220 --> 00:12:18,939
file sinker so your web server serve

00:12:17,050 --> 00:12:20,709
static contents and we have something

00:12:18,939 --> 00:12:22,540
that grabs that static content from some

00:12:20,709 --> 00:12:24,399
distributed storage and periodically

00:12:22,540 --> 00:12:25,990
updates the web server so it's updating

00:12:24,399 --> 00:12:27,550
that these are two things that if they

00:12:25,990 --> 00:12:29,259
were not to both run at the same time

00:12:27,550 --> 00:12:31,329
together it wouldn't make sense to run

00:12:29,259 --> 00:12:32,829
them our printers and our backends we

00:12:31,329 --> 00:12:34,329
can load balance them separately and

00:12:32,829 --> 00:12:36,730
scale them separately so they don't

00:12:34,329 --> 00:12:38,170
necessarily need to run on a pond so

00:12:36,730 --> 00:12:39,999
this is the scheduling atom in

00:12:38,170 --> 00:12:41,740
kubernetes whenever we talk about

00:12:39,999 --> 00:12:44,139
anything that we run anything that we

00:12:41,740 --> 00:12:46,720
schedule it's a pond it's the thing that

00:12:44,139 --> 00:12:49,720
we always talk about and so things with

00:12:46,720 --> 00:12:51,879
within a within a pond do share a lot of

00:12:49,720 --> 00:12:53,579
things of what's relevant here is that

00:12:51,879 --> 00:12:56,259
they do share a network main space so

00:12:53,579 --> 00:12:57,910
within themselves they refer to their

00:12:56,259 --> 00:13:00,610
they can talk to each other as localhost

00:12:57,910 --> 00:13:04,269
they share the port address space and

00:13:00,610 --> 00:13:05,889
they share one single IP address which

00:13:04,269 --> 00:13:08,709
is writable and we'll talk a little bit

00:13:05,889 --> 00:13:11,649
more about that so probably the most

00:13:08,709 --> 00:13:14,230
interesting part about pods is that they

00:13:11,649 --> 00:13:15,910
themselves have each an IP and this is

00:13:14,230 --> 00:13:18,279
an IP that is routable by the network

00:13:15,910 --> 00:13:19,779
fabric on the data center so by default

00:13:18,279 --> 00:13:21,399
docker usually doesn't give you an IP

00:13:19,779 --> 00:13:23,679
you have to do weird things go through

00:13:21,399 --> 00:13:25,240
the host exporter port we've done that

00:13:23,679 --> 00:13:27,250
before we built that internally

00:13:25,240 --> 00:13:29,860
not to be a great idea you run into all

00:13:27,250 --> 00:13:32,110
kinds of issues where either depletion

00:13:29,860 --> 00:13:33,790
of pods reports scheduling of ports

00:13:32,110 --> 00:13:34,959
suddenly ports become a first-class

00:13:33,790 --> 00:13:36,430
thing where you have to put it

00:13:34,959 --> 00:13:38,589
everywhere all configuration has the

00:13:36,430 --> 00:13:41,080
specified ports all applications have to

00:13:38,589 --> 00:13:42,700
know that I need to take a port as an

00:13:41,080 --> 00:13:44,830
argument so it's just a monstrous

00:13:42,700 --> 00:13:46,240
disaster and if we were to do it again

00:13:44,830 --> 00:13:47,980
we would do it

00:13:46,240 --> 00:13:49,870
assigning a single IP per container

00:13:47,980 --> 00:13:50,410
which is what we chose to do with

00:13:49,870 --> 00:13:52,630
kubernetes

00:13:50,410 --> 00:13:53,890
so again all the nodes can talk to all

00:13:52,630 --> 00:13:56,470
the containers can talk to each other

00:13:53,890 --> 00:13:58,540
even across nodes without doing any form

00:13:56,470 --> 00:13:59,950
of math they are address a fully draw

00:13:58,540 --> 00:14:01,930
supply piece they can talk to each other

00:13:59,950 --> 00:14:03,730
but again these tend to be internal IP

00:14:01,930 --> 00:14:08,560
so not necessarily IPS that are exposed

00:14:03,730 --> 00:14:12,399
to the internet usually Tendai piece the

00:14:08,560 --> 00:14:14,620
question of how how we actually get at

00:14:12,399 --> 00:14:16,660
network from the public internet and

00:14:14,620 --> 00:14:18,910
actually have a public IP that you

00:14:16,660 --> 00:14:20,140
expose is kind of an ongoing interesting

00:14:18,910 --> 00:14:23,500
problem and we'll talk a little bit more

00:14:20,140 --> 00:14:25,959
about that so the second abstraction I

00:14:23,500 --> 00:14:27,339
want to talk about with services and the

00:14:25,959 --> 00:14:30,220
important thing to learn about services

00:14:27,339 --> 00:14:32,020
is why they exist at all so as I was

00:14:30,220 --> 00:14:34,089
talking about pods being these cattle

00:14:32,020 --> 00:14:36,579
that we just kill at any time we refer

00:14:34,089 --> 00:14:38,620
to positing ephemeral being one pot is

00:14:36,579 --> 00:14:40,480
replaceable for another pond given that

00:14:38,620 --> 00:14:42,459
they both have the same abstraction the

00:14:40,480 --> 00:14:43,630
same contract the same API but it

00:14:42,459 --> 00:14:45,940
doesn't matter whether it's this one or

00:14:43,630 --> 00:14:47,829
that one they may go away a machine my

00:14:45,940 --> 00:14:51,010
entirely go down and so we have some of

00:14:47,829 --> 00:14:53,320
the machine who brought up the pods or

00:14:51,010 --> 00:14:54,670
one of those pods may just be bad and

00:14:53,320 --> 00:14:55,329
we'll need to start to route our traffic

00:14:54,670 --> 00:14:58,270
to another one

00:14:55,329 --> 00:14:59,950
either way we we shouldn't we we argue

00:14:58,270 --> 00:15:02,440
that it is incorrect for us to address a

00:14:59,950 --> 00:15:04,690
single pod by its IP both because it

00:15:02,440 --> 00:15:06,610
might go away as well as it might run

00:15:04,690 --> 00:15:07,990
into issues and so we've created the

00:15:06,610 --> 00:15:10,000
service abstraction to do this so

00:15:07,990 --> 00:15:13,630
instead of you actually addressing upon

00:15:10,000 --> 00:15:16,120
who's again IP my go away or its essence

00:15:13,630 --> 00:15:18,130
my go away you address a service which

00:15:16,120 --> 00:15:19,720
is an abstraction similar to what you

00:15:18,130 --> 00:15:23,589
might have a load balancer in front of a

00:15:19,720 --> 00:15:25,690
set of pods so as the as we showed here

00:15:23,589 --> 00:15:28,029
you might have three pods behind this

00:15:25,690 --> 00:15:29,860
load balancer and then this little

00:15:28,029 --> 00:15:32,170
balancer will ahead again understand

00:15:29,860 --> 00:15:33,730
which nodes are happy which knows or not

00:15:32,170 --> 00:15:35,729
which ones are ready to serve which ones

00:15:33,730 --> 00:15:38,229
weren't already to serve and

00:15:35,729 --> 00:15:39,609
move the traffic accordingly the

00:15:38,229 --> 00:15:41,379
interesting thing about a service is

00:15:39,609 --> 00:15:45,009
that a service does have a stable IP

00:15:41,379 --> 00:15:47,109
this is an IP that unlike the IP of a

00:15:45,009 --> 00:15:49,209
pod is guaranteed to exist is guaranteed

00:15:47,109 --> 00:15:54,329
to continue and it's guaranteed to be

00:15:49,209 --> 00:15:54,329
used by all the clients and so the

00:15:55,079 --> 00:16:01,569
couple quick questions yes sir the first

00:15:58,029 --> 00:16:04,869
is you mentioned that pod IPS are

00:16:01,569 --> 00:16:06,579
routable and they each part has its own

00:16:04,869 --> 00:16:08,789
IP I assume that this is based off of

00:16:06,579 --> 00:16:10,479
the physical topology of the cluster

00:16:08,789 --> 00:16:12,309
yeah so we'll talk a little bit more

00:16:10,479 --> 00:16:16,209
about the the set up on this so there's

00:16:12,309 --> 00:16:17,739
well we have a few ways that you can

00:16:16,209 --> 00:16:19,449
actually get this done we do it with an

00:16:17,739 --> 00:16:20,979
SDN or if you have built in top of our

00:16:19,449 --> 00:16:24,249
fabric people have start to do with a

00:16:20,979 --> 00:16:26,019
UDP end cap system built on top so that

00:16:24,249 --> 00:16:28,269
actually was the first the hardest thing

00:16:26,019 --> 00:16:32,109
to get people onto our names oh cool and

00:16:28,269 --> 00:16:33,970
then the other question is the service

00:16:32,109 --> 00:16:36,249
IP so I imagine that they move a lot

00:16:33,970 --> 00:16:38,529
more if you're using the Sdn solution

00:16:36,249 --> 00:16:40,600
then is it the same address space or do

00:16:38,529 --> 00:16:42,339
you carve out different like a different

00:16:40,600 --> 00:16:44,259
prefix for all of your service IDs yeah

00:16:42,339 --> 00:16:45,639
so we will carve out a different prefix

00:16:44,259 --> 00:16:47,819
for the server side piece and so those

00:16:45,639 --> 00:16:50,079
will be guaranteed to be more standard

00:16:47,819 --> 00:16:53,559
and you know we'll go into a little more

00:16:50,079 --> 00:16:55,329
detail about it a little bit so back to

00:16:53,559 --> 00:16:57,579
services so the way that you would look

00:16:55,329 --> 00:16:58,989
at how connections are done with

00:16:57,579 --> 00:17:01,239
services it's it's a little interesting

00:16:58,989 --> 00:17:03,729
we're working on optimizing it but in

00:17:01,239 --> 00:17:05,289
essence we have some client I'm one pod

00:17:03,729 --> 00:17:07,120
on the kubernetes cluster and I want to

00:17:05,289 --> 00:17:10,240
talk to another don't want to talk to a

00:17:07,120 --> 00:17:11,649
service somehow I look up the IP of that

00:17:10,240 --> 00:17:14,350
service I'm gonna go ahead and hit it

00:17:11,649 --> 00:17:16,179
and then on the actual kubernetes node

00:17:14,350 --> 00:17:18,639
that I'm running on we actually do

00:17:16,179 --> 00:17:20,470
iptables dnat where we get oh you're

00:17:18,639 --> 00:17:22,209
actually wanting to talk to this thing

00:17:20,470 --> 00:17:23,589
which is we know as a service we're

00:17:22,209 --> 00:17:25,600
actually gonna reroute you to what we

00:17:23,589 --> 00:17:28,209
call the cube proxy this is a proxy

00:17:25,600 --> 00:17:31,090
running on every kubernetes node that is

00:17:28,209 --> 00:17:33,279
an enlightened proxy it knows oh this

00:17:31,090 --> 00:17:35,230
service is actually backed by these

00:17:33,279 --> 00:17:36,850
number of pods I'm gonna go ahead and

00:17:35,230 --> 00:17:38,320
route you to one of those so well the

00:17:36,850 --> 00:17:40,629
cube proxy does is that it places a

00:17:38,320 --> 00:17:42,100
watch on the API server what this means

00:17:40,629 --> 00:17:43,869
is that it knows the constituency of

00:17:42,100 --> 00:17:45,940
those services so it understands what

00:17:43,869 --> 00:17:48,190
pods are active and what pawns are ready

00:17:45,940 --> 00:17:49,370
to serve as part of this service and

00:17:48,190 --> 00:17:50,930
then it goes in

00:17:49,370 --> 00:17:52,910
routes between them today it does the

00:17:50,930 --> 00:17:53,840
super awesome super advanced round

00:17:52,910 --> 00:17:55,970
robbing

00:17:53,840 --> 00:17:59,030
load balancing so as you can see there's

00:17:55,970 --> 00:18:00,290
definitely some room to grow but this

00:17:59,030 --> 00:18:02,270
turns out to work very well in practice

00:18:00,290 --> 00:18:03,559
we haven't noticed a significant

00:18:02,270 --> 00:18:05,510
performance degradation although we

00:18:03,559 --> 00:18:07,040
haven't tried extensively well and the

00:18:05,510 --> 00:18:09,920
few tests that we have run it actually

00:18:07,040 --> 00:18:13,010
does is better because the proxy matches

00:18:09,920 --> 00:18:14,720
a lot of the packets so we're sure that

00:18:13,010 --> 00:18:17,330
it doesn't actually make it better

00:18:14,720 --> 00:18:20,300
but we haven't really seen a problem in

00:18:17,330 --> 00:18:21,559
practice today and so one of the

00:18:20,300 --> 00:18:23,870
questions that always comes up is how do

00:18:21,559 --> 00:18:26,240
you actually discover these servers IPs

00:18:23,870 --> 00:18:28,100
and of course we came up with two

00:18:26,240 --> 00:18:29,809
different solutions the first one we

00:18:28,100 --> 00:18:31,760
started with was clearly awesome and

00:18:29,809 --> 00:18:33,530
super scalable which was just to place a

00:18:31,760 --> 00:18:35,330
bunch of environment variables into the

00:18:33,530 --> 00:18:36,620
container to tell you exactly what

00:18:35,330 --> 00:18:38,570
things were so if you're looking for the

00:18:36,620 --> 00:18:40,429
kubernetes read-only service it's on

00:18:38,570 --> 00:18:43,670
this IP or the foo service it's on that

00:18:40,429 --> 00:18:45,890
IP so that we knew wasn't going to scale

00:18:43,670 --> 00:18:47,510
but at least it got us a foothold in the

00:18:45,890 --> 00:18:49,580
beginning right now we just finished

00:18:47,510 --> 00:18:51,620
rolling out an internal cluster DNS

00:18:49,580 --> 00:18:53,390
where we can actually you can actually

00:18:51,620 --> 00:18:55,280
do a look-up on the service by the

00:18:53,390 --> 00:18:57,590
service name and we'll return the IP

00:18:55,280 --> 00:18:59,570
this is the IP of the service not the IP

00:18:57,590 --> 00:19:01,820
of any of the pods underneath it the

00:18:59,570 --> 00:19:04,100
routing will still do the same way that

00:19:01,820 --> 00:19:06,020
we did before and again this tends to

00:19:04,100 --> 00:19:09,950
work very well in practice because the

00:19:06,020 --> 00:19:11,330
service IPS are stable IDs and so

00:19:09,950 --> 00:19:12,830
talking a little bit more to the

00:19:11,330 --> 00:19:14,660
question that was asked about the

00:19:12,830 --> 00:19:17,000
configurations that we have in general

00:19:14,660 --> 00:19:19,070
in kubernetes setting up this networking

00:19:17,000 --> 00:19:20,330
is slightly complicated in the beginning

00:19:19,070 --> 00:19:22,790
a lot of people didn't think that they

00:19:20,330 --> 00:19:25,610
could do it outside of the first initial

00:19:22,790 --> 00:19:27,910
implementation we had but then we we

00:19:25,610 --> 00:19:30,170
were able to get a couple new

00:19:27,910 --> 00:19:32,210
configurations coming in so the first

00:19:30,170 --> 00:19:34,250
one that we developed was based on on

00:19:32,210 --> 00:19:35,450
dromeda so Andromeda is Google Software

00:19:34,250 --> 00:19:37,190
Defined Networking this is what we

00:19:35,450 --> 00:19:39,800
actually expose as part of Google

00:19:37,190 --> 00:19:41,840
compute engine our compute or public

00:19:39,800 --> 00:19:43,370
compute services our public cloud and so

00:19:41,840 --> 00:19:46,640
this allows you to program to program

00:19:43,370 --> 00:19:48,500
the underlying fabric pretty rudimentary

00:19:46,640 --> 00:19:49,790
you can do sort of a few set of things

00:19:48,500 --> 00:19:51,590
but it turns out to be extremely

00:19:49,790 --> 00:19:53,210
powerful and practice and works well for

00:19:51,590 --> 00:19:54,800
us and the second option that's

00:19:53,210 --> 00:19:57,500
available is core OS who is a big

00:19:54,800 --> 00:19:59,840
partner in kubernetes wrote this UDP end

00:19:57,500 --> 00:20:02,300
cap or you this UDP end cap overlay

00:19:59,840 --> 00:20:03,020
network called flannel so I know tom was

00:20:02,300 --> 00:20:05,720
just here talking

00:20:03,020 --> 00:20:07,550
UTP Network UDP end cap so they did

00:20:05,720 --> 00:20:08,900
something similar and they they have

00:20:07,550 --> 00:20:12,710
even more ability to program the network

00:20:08,900 --> 00:20:14,870
than we do we being Andromeda so what we

00:20:12,710 --> 00:20:16,610
tend to do as I mentioned is that we do

00:20:14,870 --> 00:20:19,430
carve the Service IPS out of their own

00:20:16,610 --> 00:20:22,340
$10 dress space for the rest of it we

00:20:19,430 --> 00:20:23,870
actually give a slash 24 for each of the

00:20:22,340 --> 00:20:26,210
nodes so each of the nodes itself is a

00:20:23,870 --> 00:20:28,820
256 IPS that it can distribute to the

00:20:26,210 --> 00:20:30,680
containers hasn't yet been a problem we

00:20:28,820 --> 00:20:32,690
have yet to a lot of things break before

00:20:30,680 --> 00:20:35,300
you get anywhere close to 256 containers

00:20:32,690 --> 00:20:37,610
per node so so far has not been an issue

00:20:35,300 --> 00:20:40,040
but car how we actually carve out this

00:20:37,610 --> 00:20:41,270
address space has been an issue and the

00:20:40,040 --> 00:20:42,530
number of nodes that were able to scale

00:20:41,270 --> 00:20:45,650
in the cluster and so we're actively

00:20:42,530 --> 00:20:47,540
working on that other configurations

00:20:45,650 --> 00:20:49,460
that are not particularly well supported

00:20:47,540 --> 00:20:50,780
today but are in the works there's a

00:20:49,460 --> 00:20:53,060
couple of people working on OS based

00:20:50,780 --> 00:20:56,420
solutions and I guess Andromeda would be

00:20:53,060 --> 00:20:57,920
similar to that in some ways and there's

00:20:56,420 --> 00:21:01,010
also a few other companies doing

00:20:57,920 --> 00:21:02,900
different types of overlay networks

00:21:01,010 --> 00:21:05,960
built on top of that so we're also

00:21:02,900 --> 00:21:08,720
seeing how some of that ends up and so

00:21:05,960 --> 00:21:12,290
future work as I mentioned my gradable

00:21:08,720 --> 00:21:15,320
IPS so today we see that we carve out

00:21:12,290 --> 00:21:16,490
the the slash 24 spur machine so this

00:21:15,320 --> 00:21:19,040
works out great we can give each

00:21:16,490 --> 00:21:20,930
container an IP but this is not an IP I

00:21:19,040 --> 00:21:22,970
can migrate so once I can actually have

00:21:20,930 --> 00:21:24,530
my container be checkpoint restorable I

00:21:22,970 --> 00:21:26,750
can actually move it to another machine

00:21:24,530 --> 00:21:27,920
because I can't migrate at IP I can

00:21:26,750 --> 00:21:29,390
argue that I can kill the pod and

00:21:27,920 --> 00:21:31,340
started somewhere else but then some

00:21:29,390 --> 00:21:33,020
people get upset I just did all this

00:21:31,340 --> 00:21:34,850
computation and you just killed me can

00:21:33,020 --> 00:21:36,440
you please just moving so we're very

00:21:34,850 --> 00:21:38,930
much working on that and we don't have

00:21:36,440 --> 00:21:41,510
quite a solution today the second the

00:21:38,930 --> 00:21:43,250
third thing on the slide is real load

00:21:41,510 --> 00:21:45,500
balancing so again we today we just do

00:21:43,250 --> 00:21:47,720
very naive round-robin at least we know

00:21:45,500 --> 00:21:48,650
the constituency of our services but we

00:21:47,720 --> 00:21:50,270
definitely want to do something that

00:21:48,650 --> 00:21:51,920
understands the state of the cluster

00:21:50,270 --> 00:21:54,290
understands the utilization of the nodes

00:21:51,920 --> 00:21:56,750
well the nodes are healthy what needs to

00:21:54,290 --> 00:21:58,280
happen there and the the first point of

00:21:56,750 --> 00:21:59,990
the slide something I haven't touched at

00:21:58,280 --> 00:22:01,130
all on the presentation because it's

00:21:59,990 --> 00:22:02,840
something that we haven't quite had a

00:22:01,130 --> 00:22:05,030
problem with is actually network

00:22:02,840 --> 00:22:08,450
resource management so capping how much

00:22:05,030 --> 00:22:09,620
a container can put in can sense the

00:22:08,450 --> 00:22:11,240
network and receive from the network

00:22:09,620 --> 00:22:12,980
both doing this the level of a single

00:22:11,240 --> 00:22:14,630
node so we don't saturate the NIC as

00:22:12,980 --> 00:22:15,490
well as the level of the cluster so we

00:22:14,630 --> 00:22:17,350
don't overload the

00:22:15,490 --> 00:22:19,360
any any of the flows are just in the

00:22:17,350 --> 00:22:20,919
cluster this is all sort of future work

00:22:19,360 --> 00:22:22,450
and super dependent on your network

00:22:20,919 --> 00:22:23,799
topology so we're not particularly

00:22:22,450 --> 00:22:27,909
looking forward to addressing that one

00:22:23,799 --> 00:22:29,980
just yet with that this is a picture of

00:22:27,909 --> 00:22:32,200
it at night so it is quite interesting

00:22:29,980 --> 00:22:39,159
but if you do have any questions feel

00:22:32,200 --> 00:22:43,029
free to ask so initially you mentioned

00:22:39,159 --> 00:22:46,899
the Vth use case and mcfillin and you

00:22:43,029 --> 00:22:48,929
said that if the Vth with use case has a

00:22:46,899 --> 00:22:51,640
performance integration the aggregation

00:22:48,929 --> 00:22:55,059
which relates to what to the IP tables

00:22:51,640 --> 00:22:57,070
that involved so I'd be lying to you if

00:22:55,059 --> 00:22:58,240
I knew if I told you I knew off hand

00:22:57,070 --> 00:23:00,130
what it was

00:22:58,240 --> 00:23:03,970
can you go back to the slides that I

00:23:00,130 --> 00:23:08,200
also haven't managed to this question so

00:23:03,970 --> 00:23:10,240
he means a loop through the routing

00:23:08,200 --> 00:23:12,820
table so you basically want all the IP

00:23:10,240 --> 00:23:15,039
stack and what's the difference between

00:23:12,820 --> 00:23:17,470
the vs use case to the Mac villain and

00:23:15,039 --> 00:23:20,520
what how does IP villain can you explain

00:23:17,470 --> 00:23:24,130
it a bit more here in this context so in

00:23:20,520 --> 00:23:26,200
case any packets you send from the

00:23:24,130 --> 00:23:31,120
network namespace have to go through the

00:23:26,200 --> 00:23:34,240
IP forwarding path so that's why it's

00:23:31,120 --> 00:23:36,010
more expensive them to plan the packet

00:23:34,240 --> 00:23:39,100
Lian that you cut you exposed to the

00:23:36,010 --> 00:23:42,700
container width interface right and they

00:23:39,100 --> 00:23:44,770
so so you expose a pair of V so one leg

00:23:42,700 --> 00:23:47,350
isn't a network namespace and the other

00:23:44,770 --> 00:23:49,779
leg runs in the initial net network

00:23:47,350 --> 00:23:54,390
namespace that's why you need the

00:23:49,779 --> 00:23:54,390
routing yes I know I know

00:23:55,470 --> 00:24:01,139
well what Eric mentioned and also the

00:23:58,830 --> 00:24:03,299
fact that Vieth isn't Hardware so any

00:24:01,139 --> 00:24:11,009
the nice hardware offloads you you get

00:24:03,299 --> 00:24:13,230
on on a modern NIC don't exist thank you

00:24:11,009 --> 00:24:21,000
for asking me I'm more confused than I

00:24:13,230 --> 00:24:23,460
was before I asked the question expert

00:24:21,000 --> 00:24:26,490
mark villian offloads from hardware you

00:24:23,460 --> 00:24:28,980
go from the physical boom you land into

00:24:26,490 --> 00:24:31,950
the container if depending on the mode

00:24:28,980 --> 00:24:34,470
you set V if you have to send the packet

00:24:31,950 --> 00:24:37,289
through if to the stack then it gets

00:24:34,470 --> 00:24:46,820
copied over to the container so there's

00:24:37,289 --> 00:24:50,279
more steps ok maybe answer that question

00:24:46,820 --> 00:24:51,990
so your question was why not use it we

00:24:50,279 --> 00:25:00,299
simply haven't piped in support for yet

00:24:51,990 --> 00:25:07,980
there's no it was just main line a lot

00:25:00,299 --> 00:25:11,129
of this is to work another my

00:25:07,980 --> 00:25:12,899
presentation earlier so the thing is we

00:25:11,129 --> 00:25:14,820
cannot use Mac villain because we cannot

00:25:12,899 --> 00:25:17,879
explode the number of mac address on the

00:25:14,820 --> 00:25:21,990
network we are tied to using one micro

00:25:17,879 --> 00:25:24,379
rays per host that's why so part of that

00:25:21,990 --> 00:25:26,909
part of that is a limitation on our SDM

00:25:24,379 --> 00:25:33,840
part of the reason why we prefer IP

00:25:26,909 --> 00:25:35,759
belong to Mike together okay so looking

00:25:33,840 --> 00:25:38,009
at kubernetes for a couple of months of

00:25:35,759 --> 00:25:40,230
Red Hat and I noticed in your

00:25:38,009 --> 00:25:42,870
presentation as well it seemed like

00:25:40,230 --> 00:25:46,620
there's certainly at least a bias and

00:25:42,870 --> 00:25:49,529
maybe a total of towards ipv4 or maybe

00:25:46,620 --> 00:25:51,690
even a total totally ignoring ipv6

00:25:49,529 --> 00:25:53,279
within cooper's the kubernetes whereas

00:25:51,690 --> 00:25:55,230
over the past several days i've seen

00:25:53,279 --> 00:25:57,000
presentations from Tom and Eric maybe

00:25:55,230 --> 00:25:59,940
some other group and other Google folks

00:25:57,000 --> 00:26:01,649
and all they're talking about is ipv6

00:25:59,940 --> 00:26:02,910
and that seems like a disconnect I

00:26:01,649 --> 00:26:04,590
wonder if you could address that

00:26:02,910 --> 00:26:06,570
yes that's actually something we're

00:26:04,590 --> 00:26:09,030
working on right now I know the the

00:26:06,570 --> 00:26:11,700
excuse that we will use is that so we

00:26:09,030 --> 00:26:14,070
primarily develop on pump Andromeda and

00:26:11,700 --> 00:26:16,980
what's exposed today for GCE we don't

00:26:14,070 --> 00:26:19,080
really have a lot of support for ipv6

00:26:16,980 --> 00:26:21,000
just yet and so a lot of that is coming

00:26:19,080 --> 00:26:22,440
and sort of until we can't run it

00:26:21,000 --> 00:26:24,090
ourselves we're gonna have a hard time

00:26:22,440 --> 00:26:25,890
going outside of that but I do agree

00:26:24,090 --> 00:26:27,210
with you we do have a very heavy ipv4

00:26:25,890 --> 00:26:28,500
emphasis today but we're actually

00:26:27,210 --> 00:26:31,500
getting a lot of pushback on getting

00:26:28,500 --> 00:26:34,850
ipv6 very soon so I very much expect to

00:26:31,500 --> 00:26:34,850
see them changing in the near future i

00:26:41,900 --> 00:26:52,800
pv LAN works on ipv6 this is the primary

00:26:46,650 --> 00:26:55,800
target ipv4 is working so what question

00:26:52,800 --> 00:26:57,780
is if the service discovery is where you

00:26:55,800 --> 00:27:00,840
resolve the IP addresses of the source

00:26:57,780 --> 00:27:04,170
and destinations how do other tools like

00:27:00,840 --> 00:27:07,800
TC or any of the policy tools know what

00:27:04,170 --> 00:27:10,620
how to apply policies on do you also

00:27:07,800 --> 00:27:13,080
extend the endpoint kind of inside those

00:27:10,620 --> 00:27:14,850
policy layers maybe I'm not

00:27:13,080 --> 00:27:16,650
understanding your question I'm sorry so

00:27:14,850 --> 00:27:18,240
until the service discovery happens you

00:27:16,650 --> 00:27:21,660
don't really know what will be the fight

00:27:18,240 --> 00:27:23,280
to pull on the wire is that right so

00:27:21,660 --> 00:27:24,720
until you that you don't know what the

00:27:23,280 --> 00:27:28,140
destination is until after service is

00:27:24,720 --> 00:27:29,400
covering right so today the we have like

00:27:28,140 --> 00:27:30,750
the legacy applications I don't know

00:27:29,400 --> 00:27:32,880
anything about kubernetes and they run

00:27:30,750 --> 00:27:34,860
through the kubernetes proxy that turns

00:27:32,880 --> 00:27:36,330
out to work very well in practice we can

00:27:34,860 --> 00:27:38,850
route things without things being

00:27:36,330 --> 00:27:40,410
kubernetes native now you can do the

00:27:38,850 --> 00:27:41,550
same thing that the cube proxy does

00:27:40,410 --> 00:27:43,770
today where you can actually do the

00:27:41,550 --> 00:27:45,300
watching yourself and then directly

00:27:43,770 --> 00:27:47,400
connect one of the notes we don't block

00:27:45,300 --> 00:27:49,140
any of that from happening we actually

00:27:47,400 --> 00:27:50,820
so today we use the kubernetes proxy but

00:27:49,140 --> 00:27:52,470
we've been looking at just doing it

00:27:50,820 --> 00:27:54,150
straight through through IP table rules

00:27:52,470 --> 00:27:55,830
and there's a way to do it it's been

00:27:54,150 --> 00:27:58,110
there's plenty of other fires to fight

00:27:55,830 --> 00:27:59,370
for right now but yes this is definitely

00:27:58,110 --> 00:28:01,140
something you can do natively we won't

00:27:59,370 --> 00:28:04,740
ever block you actually talking to

00:28:01,140 --> 00:28:05,820
another node directly but we find most

00:28:04,740 --> 00:28:07,470
applications don't even want to bother

00:28:05,820 --> 00:28:09,510
with it and so that's why the proxy

00:28:07,470 --> 00:28:11,220
exists today for performance sensitive

00:28:09,510 --> 00:28:13,410
applications which is always a complaint

00:28:11,220 --> 00:28:15,520
we get they will do the the lookup some

00:28:13,410 --> 00:28:22,990
cells

00:28:15,520 --> 00:28:24,370
I could vote interested on you on that

00:28:22,990 --> 00:28:28,510
last ball point on the last slide

00:28:24,370 --> 00:28:32,170
resource management and obviously the

00:28:28,510 --> 00:28:34,480
difficulties translating intent into

00:28:32,170 --> 00:28:36,850
order a order operational constraints

00:28:34,480 --> 00:28:39,010
without being specific to the hardware

00:28:36,850 --> 00:28:41,050
or the infrastructure in general they

00:28:39,010 --> 00:28:44,260
already have an idea on how to capture

00:28:41,050 --> 00:28:46,810
that resource management constraints in

00:28:44,260 --> 00:28:49,510
ten patients um like that and how to

00:28:46,810 --> 00:28:53,530
translate that into infrastructure

00:28:49,510 --> 00:28:54,640
specific configuration yeah so I have a

00:28:53,530 --> 00:28:55,840
very sad answer that we don't

00:28:54,640 --> 00:28:57,940
unfortunately have any answers today

00:28:55,840 --> 00:29:00,130
this is an area that we know we have

00:28:57,940 --> 00:29:01,630
work to do because a lot of so we tend

00:29:00,130 --> 00:29:04,600
to not worry too much about the network

00:29:01,630 --> 00:29:06,130
but under certain a lot of other data

00:29:04,600 --> 00:29:08,170
centers do have an issue with networking

00:29:06,130 --> 00:29:11,580
and so that's something that we actually

00:29:08,170 --> 00:29:11,580
don't have much of an answer for today

00:29:18,060 --> 00:29:21,720
sorry this might be like a dupe and I

00:29:20,550 --> 00:29:24,300
can definitely just forget the code to

00:29:21,720 --> 00:29:26,340
figure out but your proxy that's a layer

00:29:24,300 --> 00:29:28,920
seven proxy just doing like regular TCP

00:29:26,340 --> 00:29:30,750
termination connection forwarding so is

00:29:28,920 --> 00:29:34,050
it application-specific at all does it

00:29:30,750 --> 00:29:41,940
like go into RPC protocol okay super

00:29:34,050 --> 00:29:44,610
general super simple nothing special I'm

00:29:41,940 --> 00:29:47,400
actually continuing the same this proxy

00:29:44,610 --> 00:29:50,700
question so that's a proxy in user space

00:29:47,400 --> 00:29:52,380
written and go right yes and so although

00:29:50,700 --> 00:29:53,880
the connections ala gtp connection

00:29:52,380 --> 00:29:55,620
actually established into itself on a

00:29:53,880 --> 00:29:56,610
local fuss and then it's a lot balanced

00:29:55,620 --> 00:30:00,600
in a round-robin

00:29:56,610 --> 00:30:02,670
way right so did you measure performance

00:30:00,600 --> 00:30:05,160
oh are you saying like at this point you

00:30:02,670 --> 00:30:06,600
don't care like oh yeah all the extra

00:30:05,160 --> 00:30:09,150
stuff that's happening we haven't done

00:30:06,600 --> 00:30:10,560
extensive performance tests we do expect

00:30:09,150 --> 00:30:12,000
there to be some degradation the simple

00:30:10,560 --> 00:30:13,530
test that we did actually perform better

00:30:12,000 --> 00:30:16,020
with the proxy because the proxy was

00:30:13,530 --> 00:30:17,490
doing batching of the packets but we

00:30:16,020 --> 00:30:20,340
don't actually expect those to be real

00:30:17,490 --> 00:30:22,890
numbers so in short we haven't really

00:30:20,340 --> 00:30:25,380
found in practice we haven't found an

00:30:22,890 --> 00:30:28,920
issue yet but we need to run more

00:30:25,380 --> 00:30:30,840
performance tests on that but again we

00:30:28,920 --> 00:30:32,580
do hope to change this so because

00:30:30,840 --> 00:30:34,380
performance that and that level has not

00:30:32,580 --> 00:30:36,630
yet been an issue we haven't addressed

00:30:34,380 --> 00:30:38,160
it but we do have plans again going

00:30:36,630 --> 00:30:40,590
straight through IP table rules that we

00:30:38,160 --> 00:30:42,360
should get rid of the processing itself

00:30:40,590 --> 00:30:43,680
and we should bring the performance down

00:30:42,360 --> 00:30:48,710
within an acceptable level

00:30:43,680 --> 00:30:48,710
I guess the performance back to

00:30:50,510 --> 00:30:53,600

YouTube URL: https://www.youtube.com/watch?v=Mpx-azJSmOE


