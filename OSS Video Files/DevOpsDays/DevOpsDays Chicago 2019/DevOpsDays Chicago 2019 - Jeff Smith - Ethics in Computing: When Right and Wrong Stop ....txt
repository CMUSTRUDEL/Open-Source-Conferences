Title: DevOpsDays Chicago 2019 - Jeff Smith - Ethics in Computing: When Right and Wrong Stop ...
Publication date: 2019-09-09
Playlist: DevOpsDays Chicago 2019
Description: 
	Jeff Smith - Ethics in Computing: When Right and Wrong Stop Being Obvious

Ethics in computing sounds like a fairly straight forward proposition at first glance. But when you begin to peel back the complicated layers at which technology and its implementation converge, its anything but simple. In this talk we’ll discuss some of the ethical and morale frameworks that we can apply to the computing industry. As we do that we’ll discuss and ask ourselves the questions

Exactly what is ethical or morale when it comes to technology?
What is the engineer’s responsibility in ethical and morale dilemmas involving the technology that they create?
What recourse should an engineer have at her disposal to challenge ethical lapses in her organization?
Is the market sufficiently capable of deciding what is right and wrong with regards to technology?
What are the limits of ethical guidelines as the apply to safety and security in systems?

These questions could never be answered in a brief talk and even if they could, I don’t have all the answers. But the goal is to promote conversation and critical thinking on the future of our industry and how we can continue to be proud of the work we create, in a time where technology moves to deeper into our lives. I would love for this talk to also lead into an Open Space discussion around themes brought up in the talk.
Captions: 
	00:00:13,410 --> 00:00:16,059
Jeff Smith - Ethics in Computing: When Right and Wrong Stop Being Obvious

00:00:16,059 --> 00:00:17,059
>>SASHA ROSENBAUM: Yes.

00:00:17,059 --> 00:00:19,539
So, with that, it is my privilege to introduce our first keynote speaker, Jeff Smith!

00:00:19,539 --> 00:00:20,539
[ Applause ]. >>Jeff Smith: It is early, I'm sitting here

00:00:20,539 --> 00:00:21,539
reaching for the microphone, forgetting I have a lav mic.

00:00:21,539 --> 00:00:22,539
How are you doing?

00:00:22,539 --> 00:00:23,539
A few people hung over, that's fine.

00:00:23,539 --> 00:00:28,050
I want to thank the DevOpsDays organizers for trusting me with the first keynote, because

00:00:28,050 --> 00:00:33,330
talking about ethics at 9:00 in the morning is a pretty gutsy move for a conference.

00:00:33,330 --> 00:00:36,400
Hopefully we don't screw it up too much.

00:00:36,400 --> 00:00:38,190
I don't have the answers in this talk, unfortunately.

00:00:38,190 --> 00:00:43,710
I have a bunch of questions, and the whole point of this is to start a conversation and

00:00:43,710 --> 00:00:50,440
a dialogue, to think about ethics and our responsibility as engineers in the technology

00:00:50,440 --> 00:00:55,460
we create and the things that we enable through the things that we create.

00:00:55,460 --> 00:00:59,219
For those that are seasoned DevOpsDays attendees, you are probably going to sit back and think,

00:00:59,219 --> 00:01:04,080
damn, this is the longest open space pitch I have seen in my life!

00:01:04,080 --> 00:01:05,440
Pretty much what it is.

00:01:05,440 --> 00:01:08,700
So, with that, I will start with a story.

00:01:08,700 --> 00:01:11,810
I will start with a story.

00:01:11,810 --> 00:01:12,810
Okay.

00:01:12,810 --> 00:01:19,210
I got a phone call from a friend, and we're all in tech, so we all have that friend that

00:01:19,210 --> 00:01:21,290
thinks, if you are in technology, you can do anything, right?

00:01:21,290 --> 00:01:27,459
So I get the phone call and I'm like, okay, something is wrong with her printer, I'm sure.

00:01:27,459 --> 00:01:28,459
Jeff!

00:01:28,459 --> 00:01:29,459
What?

00:01:29,459 --> 00:01:31,470
I need to know how to hack an iPod account!

00:01:31,470 --> 00:01:33,540
Okay.

00:01:33,540 --> 00:01:36,349
So a better problem than a printer.

00:01:36,349 --> 00:01:37,530
What's going on?

00:01:37,530 --> 00:01:39,369
I just need to hack an iPod account.

00:01:39,369 --> 00:01:41,120
Can you help me, can you teach me?

00:01:41,120 --> 00:01:43,860
Sure, I can teach you.

00:01:43,860 --> 00:01:47,230
Get a pad of paper and write this down.

00:01:47,230 --> 00:01:51,970
You are going to need to find one of your friends, a fast typist, make sure they have

00:01:51,970 --> 00:01:58,280
small hands, because both of you need to put your hands on the keyboard at the same time,

00:01:58,280 --> 00:02:00,890
because we are going to be doing some serious NCIS shit here!

00:02:00,890 --> 00:02:04,489
There is stuff flying over the screen and everything.

00:02:04,489 --> 00:02:07,490
She is not amused at all.

00:02:07,490 --> 00:02:09,000
Jeff, seriously.

00:02:09,000 --> 00:02:13,140
I need to know know how to hack an iPod account.

00:02:13,140 --> 00:02:14,140
What is going on?

00:02:14,140 --> 00:02:16,050
Talk to me.

00:02:16,050 --> 00:02:20,760
She said, my 16-year-old nephew left for the grocery store 20 hours ago.

00:02:20,760 --> 00:02:25,310
We have not seen him or heard from him, he is not answering his phone.

00:02:25,310 --> 00:02:33,240
We want to use the find my iPhone feature to find where he's at.

00:02:33,240 --> 00:02:34,240
Shit.

00:02:34,240 --> 00:02:38,050
So I started thinking about this and I said, you know, with two-factor authentication,

00:02:38,050 --> 00:02:42,720
you don't have another device he has, hacking an iPod account is probably going to be a

00:02:42,720 --> 00:02:43,930
pretty long endeavor.

00:02:43,930 --> 00:02:49,600
Maybe we can go another route, maybe you can contact the telecom company and have them

00:02:49,600 --> 00:02:51,580
ping his phone.

00:02:51,580 --> 00:02:54,440
She said, that is probably a better path.

00:02:54,440 --> 00:03:01,670
And something you don't know about me, I watch a lot of Law and Order, like, a lot.

00:03:01,670 --> 00:03:06,700
And while it is not technically legal advice, I know enough from Law and Order to know that

00:03:06,700 --> 00:03:13,081
the telecon company will not do it without a court order, you cannot get that without

00:03:13,081 --> 00:03:18,290
a missing persons report, cannot get that until the kid is gone for 24 hours.

00:03:18,290 --> 00:03:24,420
It is a kid, probably ran off with friends, but everyone is freaking out.

00:03:24,420 --> 00:03:31,380
They started the process, true to form, the cops drag their feet for 24 hours, telecom

00:03:31,380 --> 00:03:38,620
company drags their feet for 12 hours, and after 36 hours, they ping the phone.

00:03:38,620 --> 00:03:40,220
Two hours too late.

00:03:40,220 --> 00:03:47,110
A jogger find his car, he had an accident, fell into a ravine, and was trapped inside.

00:03:47,110 --> 00:03:57,560
So you have to think, like, could it have been different if they had gotten to him faster?

00:03:57,560 --> 00:03:58,830
How long was he trapped in the car?

00:03:58,830 --> 00:03:59,830
They don't know.

00:03:59,830 --> 00:04:04,160
Another question that the family does not want to ask, you don't want to know that the

00:04:04,160 --> 00:04:09,060
company you are sending $130 for for shitty service was complicit in your child's death.

00:04:09,060 --> 00:04:14,630
But, at the same time, we have to be careful of how we look at these things.

00:04:14,630 --> 00:04:20,099
It is easy for us to look from the perspective of the consequence.

00:04:20,099 --> 00:04:25,280
We do this in life, take action and evaluate it based on the consequence that came out

00:04:25,280 --> 00:04:26,280
of it.

00:04:26,280 --> 00:04:31,680
In this case, it was a terrible end, a tragedy, it could have just as easily been a jealous

00:04:31,680 --> 00:04:42,250
spouse, or a stalker, a garden variety type that's technically savvy.

00:04:42,250 --> 00:04:51,380
How do companies create rules that aren't beaten up when the consequences don't really

00:04:51,380 --> 00:04:57,400
go our way, and how do we evaluate right and wrong when we can only evaluate it against

00:04:57,400 --> 00:05:00,330
the outcome of a particular action?

00:05:00,330 --> 00:05:05,500
And it is becoming more and more important with all of the technology that is coming

00:05:05,500 --> 00:05:09,280
out, and all of the interesting things that it enables us to do, but all of the terrible

00:05:09,280 --> 00:05:10,860
accidents that can happen, too.

00:05:10,860 --> 00:05:15,811
My favorite is the Amazon echo that recorded a family's conversation and sent it to a person

00:05:15,811 --> 00:05:17,160
on their contact list.

00:05:17,160 --> 00:05:24,420
I love my Echo, I got my first alt a DevOpsDays, actually, that is scary, one thing you know

00:05:24,420 --> 00:05:29,440
you are talking trash, and then your boss has your conversation.

00:05:29,440 --> 00:05:38,520
One person enabled a suicide machine, which is great if a person has a terminally-ill

00:05:38,520 --> 00:05:43,630
condition that wants to die with dignity, you can think of the negative ramifications

00:05:43,630 --> 00:05:45,520
that happen from that.

00:05:45,520 --> 00:05:51,330
How do we evaluate, keep a ledger of what is good, what is bad, and who is the arbiter

00:05:51,330 --> 00:05:53,900
of those decisions?

00:05:53,900 --> 00:05:59,860
So, on that depressing note, good morning, I'm Jeff Smith, I'm the director of production

00:05:59,860 --> 00:06:10,540
operations at Centro, a digital company, and the irony of a guy in ag tech talking about

00:06:10,540 --> 00:06:13,610
ethics is not lost on me.

00:06:13,610 --> 00:06:23,140
I think we have Centro people here, too.

00:06:23,140 --> 00:06:25,510
Shout it out!

00:06:25,510 --> 00:06:27,099
[No response].

00:06:27,099 --> 00:06:35,699
Back me up, guys [ laughter ]. So it is a media company in Chicago, we is help people

00:06:35,699 --> 00:06:40,520
manage their ad campaigns, great company and culture.

00:06:40,520 --> 00:06:45,900
We are hiring, if you are interested in making a career change, I will be here both days

00:06:45,900 --> 00:06:47,850
of the conference.

00:06:47,850 --> 00:06:53,080
I'm working on a book, tentatively called real-world DevOps, probably will change 15

00:06:53,080 --> 00:06:54,080
more times.

00:06:54,080 --> 00:06:57,860
After I get Emily Freeman's book, I may cancel the project altogether.

00:06:57,860 --> 00:07:03,440
She nailed it, so I will just bow out of the conversation.

00:07:03,440 --> 00:07:05,870
So, another story.

00:07:05,870 --> 00:07:08,410
How many people have gotten on the get rid of Facebook campaign?

00:07:08,410 --> 00:07:13,640
Not as many as I thought, but still, that is hopeful.

00:07:13,640 --> 00:07:19,449
So one day, Facebook has one of their many breaches, or data leaks, or whatever.

00:07:19,449 --> 00:07:23,630
I was finally like, I'm done, I'm leaving Facebook.

00:07:23,630 --> 00:07:30,479
I was having lunch with a friend and she said, I sent you a thing, you didn't respond.

00:07:30,479 --> 00:07:34,620
And I was like, oh, I have to get a little hoity-toity about it.

00:07:34,620 --> 00:07:37,780
I'm not on Facebook anymore.

00:07:37,780 --> 00:07:38,780
Why?

00:07:38,780 --> 00:07:44,870
Don't you see the terrible things they are doing, the data they are leaking, the ethical

00:07:44,870 --> 00:07:46,770
violations they are doing?

00:07:46,770 --> 00:07:48,720
She says, eh.

00:07:48,720 --> 00:07:50,560
I like cat photos.

00:07:50,560 --> 00:07:53,139
I like News Feeds.

00:07:53,139 --> 00:07:56,750
I know that they are taking my data, but it is an even exchange for me.

00:07:56,750 --> 00:08:00,000
I get this fun stuff and it doesn't cost me anything.

00:08:00,000 --> 00:08:06,480
Is her perspective different from mine, is it more wrong or more right?

00:08:06,480 --> 00:08:08,150
Probably not.

00:08:08,150 --> 00:08:10,199
She has a different value proposition than I do.

00:08:10,199 --> 00:08:15,670
When I think about the things that Facebook has, it enrages me.

00:08:15,670 --> 00:08:21,110
Facebook was doing emotional studies, manipulating people's News Feeds to see alter their emotions,

00:08:21,110 --> 00:08:33,110
you were in experiments without knowing, that violates all ethical guidelines of test subjects.

00:08:33,110 --> 00:08:38,539
They ignored staff warnings about Cambridge Analytica.

00:08:38,539 --> 00:08:47,509
If you are in Facebook, what do you do if you bring up bad behavior to leadership and

00:08:47,509 --> 00:08:48,680
they don't do anything?

00:08:48,680 --> 00:08:52,399
What is your recourse, what do you do?

00:08:52,399 --> 00:08:55,660
You have to weigh it against the good that Facebook does.

00:08:55,660 --> 00:09:00,970
Over one billion causes were raised on Facebook.

00:09:00,970 --> 00:09:02,930
That is great in aggregate.

00:09:02,930 --> 00:09:09,220
If someone asked you a question, like, hey, man, I will donate $20,000 to cancer research

00:09:09,220 --> 00:09:12,390
if you tell me what your favorite snack flavor is.

00:09:12,390 --> 00:09:17,570
Great, I will give you that bit of information.

00:09:17,570 --> 00:09:18,570
Is it worth the trade-off?

00:09:18,570 --> 00:09:19,649
I don't know.

00:09:19,649 --> 00:09:26,050
But a billion dollars in causes is a lot, for something that, you know, you are really

00:09:26,050 --> 00:09:31,699
not actually physically giving up, even though you emotionally are.

00:09:31,699 --> 00:09:37,019
And then I get a lot out of it, too, in the micro level.

00:09:37,019 --> 00:09:38,939
My family, we are the only ones in Chicago.

00:09:38,939 --> 00:09:42,380
The rest is in New York and Georgia.

00:09:42,380 --> 00:09:45,459
So we get to share moments with them on Facebook.

00:09:45,459 --> 00:09:48,870
My daughter got a hamster this summer, when she was with my mother it was all she could

00:09:48,870 --> 00:09:49,880
talk about.

00:09:49,880 --> 00:09:53,209
I'm getting a hamster!

00:09:53,209 --> 00:09:57,410
She was so excited, my mom wanted to see that moment so badly and we did it on Facebook.

00:09:57,410 --> 00:09:58,410
She was able to see that.

00:09:58,410 --> 00:10:02,050
That is huge, that is powerful.

00:10:02,050 --> 00:10:06,740
But, again, how do we add up the legend?

00:10:06,740 --> 00:10:10,170
How do we know that these things that are happening are worth the costs?

00:10:10,170 --> 00:10:14,850
And the other interesting thing is, we are always thinking about it from the perspective

00:10:14,850 --> 00:10:16,860
of a western philosophy, right?

00:10:16,860 --> 00:10:19,940
But the internet is global!

00:10:19,940 --> 00:10:25,569
What do we do when we have an eastern set of philosophies, how do we take their viewpoints

00:10:25,569 --> 00:10:28,329
into account?

00:10:28,329 --> 00:10:29,879
What happens when they clash?

00:10:29,879 --> 00:10:32,420
Urban and rural, we see it in America.

00:10:32,420 --> 00:10:37,260
There's a disparity between what urban people feel is right and wrong, and what rural people

00:10:37,260 --> 00:10:40,529
feel is right and wrong.

00:10:40,529 --> 00:10:47,699
So how do we balance that, how do we figure it out: Is it ethical, is it moral, is it

00:10:47,699 --> 00:10:48,699
fair.

00:10:48,699 --> 00:10:53,550
As a group of us, we were having a conversation, four of us were organizers at DevOpsDays and

00:10:53,550 --> 00:10:58,540
got together and said, what can we do about ethics?

00:10:58,540 --> 00:11:01,899
Reading about it, it is complicated.

00:11:01,899 --> 00:11:05,949
You wouldn't believe that people were studying this for 1300 years.

00:11:05,949 --> 00:11:06,949
Weird.

00:11:06,949 --> 00:11:11,300
I thought we would be able to apply an algorithm to it and be done with it.

00:11:11,300 --> 00:11:21,199
We focused on the idea of consent, and in my world, you cannot talk about consent without

00:11:21,199 --> 00:11:23,759
talking about James Kirk.

00:11:23,759 --> 00:11:29,319
Weird segue, so stick with me for a second.

00:11:29,319 --> 00:11:34,119
Star Trek fan, and the episode taste of Armageddon.

00:11:34,119 --> 00:11:40,680
They are dispatched to a planet for a trade negotiation, but the planet has been at war

00:11:40,680 --> 00:11:46,500
for 500 years, 500 years they have been fighting each other, probably a wasteland.

00:11:46,500 --> 00:11:56,040
But the federation of America, they need to get -- basically space oil.

00:11:56,040 --> 00:11:59,850
They showed up, and the solar system has a beacon out there and says, do not enter.

00:11:59,850 --> 00:12:03,149
Enter at your risk, no visitors wanted.

00:12:03,149 --> 00:12:07,670
And Kirk said, fuck that, we're going anyways.

00:12:07,670 --> 00:12:12,910
They get there, and they are welcomed.

00:12:12,910 --> 00:12:14,379
Hey, guys, how are you doing?

00:12:14,379 --> 00:12:15,379
Good to see you.

00:12:15,379 --> 00:12:17,299
Yeah, c'mon down to the planet.

00:12:17,299 --> 00:12:18,299
Visit.

00:12:18,299 --> 00:12:20,949
So they get down there, it is a paradise.

00:12:20,949 --> 00:12:25,670
They are looking around, man, everything is clean, it doesn't look like a planet that

00:12:25,670 --> 00:12:27,739
has been at war for 500 years.

00:12:27,739 --> 00:12:28,949
What the hell is going on?

00:12:28,949 --> 00:12:36,730
So they start to ask about it, and Kirk finds out that the "war" is actually computer-simulated.

00:12:36,730 --> 00:12:43,369
So instead of an actual attack, they have agreed that we're never going to have peace.

00:12:43,369 --> 00:12:46,079
So let's try to make sure our culture survives.

00:12:46,079 --> 00:12:51,639
So they both create network computers that simulate their attacks.

00:12:51,639 --> 00:12:58,360
When a casualty is reported, the computer spits out who dies, and that person goes to

00:12:58,360 --> 00:13:03,310
a disintegration chamber in order to be killed, and put to death.

00:13:03,310 --> 00:13:05,970
Yikes.

00:13:05,970 --> 00:13:10,619
But the interesting thing is that everyone on the planet consents to it, they agree.

00:13:10,619 --> 00:13:13,009
They say, you know what?

00:13:13,009 --> 00:13:16,510
It is a lot better than having dinner and having your roof cave in on you because a

00:13:16,510 --> 00:13:17,559
bomb attack happened.

00:13:17,559 --> 00:13:20,850
Now I get a sticky note that says, man, you died.

00:13:20,850 --> 00:13:28,589
Wrap shit up with your family, report to the disintegration chamber in 24 hours.

00:13:28,589 --> 00:13:33,959
It is weird, but it works for them, compared to the alternative, where they had the death

00:13:33,959 --> 00:13:36,769
AND the destruction of their culture.

00:13:36,769 --> 00:13:41,240
How many people think this is a right, a moral thing?

00:13:41,240 --> 00:13:42,459
Show your hands.

00:13:42,459 --> 00:13:45,509
How many people are like, eh.

00:13:45,509 --> 00:13:46,509
Okay.

00:13:46,509 --> 00:13:49,629
A few of you.

00:13:49,629 --> 00:13:51,269
So I guess the rest of you think it is immoral.

00:13:51,269 --> 00:13:52,929
That is terrible, right?

00:13:52,929 --> 00:13:55,329
It is a tough line.

00:13:55,329 --> 00:13:59,989
So, you know, the original series didn't do the directive thing that much.

00:13:59,989 --> 00:14:03,069
So Kirk goes and starts kicking ass, like Kirk does.

00:14:03,069 --> 00:14:13,180
He is going to blow up the disintegration chambers, that is wrong, and there was a woman

00:14:13,180 --> 00:14:15,109
that he liked.

00:14:15,109 --> 00:14:16,850
Classic Kirk.

00:14:16,850 --> 00:14:27,559
But, like, if everyone is consented to it, why would Kirk impose his moral philosophy

00:14:27,559 --> 00:14:29,839
on these people?

00:14:29,839 --> 00:14:34,329
And that's something that we have to thing about in technology, too.

00:14:34,329 --> 00:14:39,059
As we create technology, the technology is inherently neutral for a lot of technologies.

00:14:39,059 --> 00:14:41,240
There is probably some that is pretty bad.

00:14:41,240 --> 00:14:43,549
But it is the application of that thing.

00:14:43,549 --> 00:14:51,279
And how do we apply our moral code to a group of people that might feel differently?

00:14:51,279 --> 00:14:52,279
[Whispering].

00:14:52,279 --> 00:15:03,579
I don't know, but -- we have to get to a cross road where we figure out, how do we deal with

00:15:03,579 --> 00:15:04,579
it?

00:15:04,579 --> 00:15:07,730
How many of you have encountered a situation that you thought was a little shady at work,

00:15:07,730 --> 00:15:10,750
but was not 100 percent sure what to do with it?

00:15:10,750 --> 00:15:15,369
Some of you are real lucky, don't leave.

00:15:15,369 --> 00:15:17,230
Don't leave.

00:15:17,230 --> 00:15:20,269
There's a lot of stuff, though, that is just a little dicy, but you don't know where to

00:15:20,269 --> 00:15:21,269
go with it.

00:15:21,269 --> 00:15:24,220
You don't go what to do with it, and you don't know if maybe it is just you, maybe it is

00:15:24,220 --> 00:15:25,970
me acting weird.

00:15:25,970 --> 00:15:31,480
So one of the things that comes to mind is user license agreements, everyone has seen

00:15:31,480 --> 00:15:35,769
those, how many of you have read them?

00:15:35,769 --> 00:15:36,769
This guy!

00:15:36,769 --> 00:15:38,070
I need to see you in an open space, sir.

00:15:38,070 --> 00:15:41,290
There are too many of them, I don't read them.

00:15:41,290 --> 00:15:46,279
In fact, I maybe thought about it again with the GDPR thing, you are prompted for cookies

00:15:46,279 --> 00:15:47,279
every site you go to.

00:15:47,279 --> 00:15:49,689
I don't read that shit anymore, accept, accept, accept, first born, whatever.

00:15:49,689 --> 00:15:56,459
Give me the news, I need the news!

00:15:56,459 --> 00:16:02,089
So that's a -- it has been traditionally accepted as, when we agree, when we click that button,

00:16:02,089 --> 00:16:05,329
we are consenting to the terrible things that they're going to do.

00:16:05,329 --> 00:16:08,069
Is that consent actually informed?

00:16:08,069 --> 00:16:09,069
And does that matter?

00:16:09,069 --> 00:16:10,069
Of course it matters.

00:16:10,069 --> 00:16:13,859
It matters in all types of areas within our society, right?

00:16:13,859 --> 00:16:16,889
People can't consent when they're drunk.

00:16:16,889 --> 00:16:21,059
Children can't consent for a lot of things, because they don't understand the implications

00:16:21,059 --> 00:16:22,790
of what it is they're consenting to.

00:16:22,790 --> 00:16:26,589
So informed consent is a thing.

00:16:26,589 --> 00:16:29,220
So how do we think about that with technology as well?

00:16:29,220 --> 00:16:32,619
So, as I said, I work in ad tech.

00:16:32,619 --> 00:16:39,720
And our CEO, Shawn Zucker, is a very thoughtful guy.

00:16:39,720 --> 00:16:46,480
We were having conversations about ethics and things, and he had a quote that I had

00:16:46,480 --> 00:16:47,480
to share.

00:16:47,480 --> 00:16:52,399
Government and entries have been adopting a libertarian viewpoint that it is the consumer's

00:16:52,399 --> 00:16:58,899
responsibility to know what they are accepting in terms of policies and terms of conditions.

00:16:58,899 --> 00:17:06,470
This is a ridiculous thinking, it is incredulous to assume that they know what the jargon means,

00:17:06,470 --> 00:17:13,070
and for every app they install they will read 10 pages of legalese.

00:17:13,070 --> 00:17:16,150
That is dead on, in my opinion.

00:17:16,150 --> 00:17:21,520
I scan my app to make sure they are not inviting themselves into my house.

00:17:21,520 --> 00:17:25,890
As long as they don't get a spare bedroom, I click okay.

00:17:25,890 --> 00:17:29,530
In line with this, I feel like we need to, in 5th grade English, make it easy for the

00:17:29,530 --> 00:17:34,230
customer to understand what they are accepting with a small, well-designed screen than 10

00:17:34,230 --> 00:17:36,240
pages of legalese.

00:17:36,240 --> 00:17:39,680
This works.

00:17:39,680 --> 00:17:44,260
If you are on the Android app store, if you have installed an app, I don't know if you

00:17:44,260 --> 00:17:48,140
do, it has been a while since I have been on there, you get a list of terrible shit

00:17:48,140 --> 00:17:51,860
this app is going to do.

00:17:51,860 --> 00:17:56,850
It is a note-taking app, why do they do this terrible stuff?

00:17:56,850 --> 00:17:59,300
No, I will get another app.

00:17:59,300 --> 00:18:05,730
The idea works, because we are using it and evaluating things like that today.

00:18:05,730 --> 00:18:11,160
So maybe consent is the lever that we need to toggle and making sure that even though

00:18:11,160 --> 00:18:15,620
we are doing something, like Nest, for example, when it had a microphone built into it, there

00:18:15,620 --> 00:18:17,990
were a ton of people that knew about it.

00:18:17,990 --> 00:18:19,930
A microphone just not show up.

00:18:19,930 --> 00:18:25,550
It is not the fact that the microphone was there, but people didn't know about it.

00:18:25,550 --> 00:18:31,250
We put microphones into our houses all the time, but we accepted and consented to it.

00:18:31,250 --> 00:18:36,940
And the idea that it is not different, because they didn't ask, is ludicrous.

00:18:36,940 --> 00:18:44,110
So going back to Kirk, the whole manifest destiny thing, there was one small piece of

00:18:44,110 --> 00:18:45,900
information that I left out.

00:18:45,900 --> 00:18:50,850
And since most of you that it was immoral, it will probably continue to be immoral.

00:18:50,850 --> 00:18:58,980
One of the things that I left out was that, when the enterprise entered orbit and broke

00:18:58,980 --> 00:19:06,390
-- and basically disregarded that message, they basically entered the conflict.

00:19:06,390 --> 00:19:09,560
They became legal combatants.

00:19:09,560 --> 00:19:12,331
When they arrived, there was an attack.

00:19:12,331 --> 00:19:19,770
And the simulated attack recorded the enterprise as destroyed, and all of its crew perished

00:19:19,770 --> 00:19:20,770
and died.

00:19:20,770 --> 00:19:24,770
And they had to report to a disintegration chamber within 24 hours.

00:19:24,770 --> 00:19:26,590
They consented, right?

00:19:26,590 --> 00:19:29,610
We gave them the warning.

00:19:29,610 --> 00:19:31,520
They ignored the warning, and they showed up.

00:19:31,520 --> 00:19:33,520
Was it informed consent, does that matter?

00:19:33,520 --> 00:19:35,510
I don't know.

00:19:35,510 --> 00:19:41,740
But when that part happens, it changes your view on whether this is okay or not.

00:19:41,740 --> 00:19:46,940
Because consenting has to be informed.

00:19:46,940 --> 00:19:52,660
And when we talk about this, it is easy to continue to, sort of, push up the off a little

00:19:52,660 --> 00:19:53,660
bit.

00:19:53,660 --> 00:19:58,190
You as the engineer knows what it is doing, but you push it to the product manager, who

00:19:58,190 --> 00:20:09,321
pushes it up to the strategists, and then to the C level, and then they are like, I

00:20:09,321 --> 00:20:11,190
don't know how this happened.

00:20:11,190 --> 00:20:17,510
The code showed up, I clicked the button and it generated a bunch of privacy generation

00:20:17,510 --> 00:20:18,510
shit?

00:20:18,510 --> 00:20:22,720
I thought it was scaffolding.

00:20:22,720 --> 00:20:30,330
But, at some point, we have to say, when do I take responsibility?

00:20:30,330 --> 00:20:32,890
When do I insert myself into the process?

00:20:32,890 --> 00:20:37,390
If you do that, where do you go?

00:20:37,390 --> 00:20:43,840
Your boss knows you are violating people's privacy, it is what they ask you to do, in

00:20:43,840 --> 00:20:45,570
not-so-certain terms.

00:20:45,570 --> 00:20:49,140
So with DevOps, we look at other industries.

00:20:49,140 --> 00:20:52,750
How do other places handle it, what do other industries do?

00:20:52,750 --> 00:20:55,640
The answer is not great me.

00:20:55,640 --> 00:20:56,660
Licensing.

00:20:56,660 --> 00:21:00,610
It is where we always come back to, right?

00:21:00,610 --> 00:21:08,330
The bar association, the medical association, these are organizations that say you are licensed

00:21:08,330 --> 00:21:10,470
to do what you do.

00:21:10,470 --> 00:21:14,850
And it works for them because they have some sort of body to go to, they have a set of

00:21:14,850 --> 00:21:19,490
concrete guidelines to follow that says, this is what we should be doing.

00:21:19,490 --> 00:21:23,900
In technology, that scares me.

00:21:23,900 --> 00:21:27,710
I think technology is an escape for a lot of underserved people.

00:21:27,710 --> 00:21:35,840
And licensing and regulation has served as artificial hurdles to getting into something,

00:21:35,840 --> 00:21:36,840
right?

00:21:36,840 --> 00:21:40,240
Women used to dominate programming as an industry.

00:21:40,240 --> 00:21:41,240
What happened?

00:21:41,240 --> 00:21:44,531
It is a professional thing, you have to have a degree.

00:21:44,531 --> 00:21:46,560
That's a barrier for women.

00:21:46,560 --> 00:21:52,350
Now we are struggling to get women back into a field that they owned not that long ago.

00:21:52,350 --> 00:21:56,780
So licensing isn't a great solution.

00:21:56,780 --> 00:22:00,740
But you have to recognize the appeal of it.

00:22:00,740 --> 00:22:03,780
These are the licensed professions in Illinois.

00:22:03,780 --> 00:22:10,250
So you're telling me my barber has to be licensed to make sure he doesn't screw up my feed.

00:22:10,250 --> 00:22:15,070
But this guy that programs a 745 landing algorithm?

00:22:15,070 --> 00:22:18,660
It is kind of scary, right?

00:22:18,660 --> 00:22:22,720
If you were to ask the populace, and the people that aren't in technology, they would be like,

00:22:22,720 --> 00:22:25,370
eh, that sounds kind of crazy, man.

00:22:25,370 --> 00:22:34,820
Every time I get in an elevator, I think, who programmed this thing, and are they licensed?

00:22:34,820 --> 00:22:39,960
A lot of these things start to intersect, because if you look at the medical profession,

00:22:39,960 --> 00:22:45,640
like we said before, you have this opportunity to lose license.

00:22:45,640 --> 00:22:48,550
If you do something wrong, I can prevent you from doing that.

00:22:48,550 --> 00:22:54,210
And not only that, there's a body and document that says, these are the things that are right,

00:22:54,210 --> 00:22:59,600
these are the things that are wrong, here are the ethical hazards.

00:22:59,600 --> 00:23:02,690
And there's a separate body that adjudicates that.

00:23:02,690 --> 00:23:09,050
So we said, well, what can we do that is sort of similar to that that doesn't involve the

00:23:09,050 --> 00:23:10,170
whole licensing piece?

00:23:10,170 --> 00:23:14,500
Because the other thing that we discovered, especially in advertising, is self-regulation

00:23:14,500 --> 00:23:16,490
can be difficult if it doesn't have teeth.

00:23:16,490 --> 00:23:21,720
In advertising, there's a group that is so powerful, the IEB, I cannot remember what

00:23:21,720 --> 00:23:23,020
it stands for.

00:23:23,020 --> 00:23:27,260
But it puts out guidelines for what is right and wrong.

00:23:27,260 --> 00:23:31,140
If you violate it, they say, no!

00:23:31,140 --> 00:23:33,990
Go on.

00:23:33,990 --> 00:23:38,270
That's your lashing.

00:23:38,270 --> 00:23:39,270
So it has to have teeth.

00:23:39,270 --> 00:23:41,630
So how do we give it teeth without having a licensing body?

00:23:41,630 --> 00:23:42,750
I don't know.

00:23:42,750 --> 00:23:48,700
The only thing I can come up with, you're a developer, ops person, you see something

00:23:48,700 --> 00:23:49,740
shady going on.

00:23:49,740 --> 00:23:52,760
You say, hmm, something suspect is going on.

00:23:52,760 --> 00:23:57,230
So what if we had a separate body?

00:23:57,230 --> 00:23:58,620
That was sort of like the IEB.

00:23:58,620 --> 00:24:05,360
And their job is to both amplify your complaints, and do a little public shaming, right?

00:24:05,360 --> 00:24:11,000
So you go to the ethics body and say, hey, I have a situation that seems suspect, what

00:24:11,000 --> 00:24:12,800
should I do about it?

00:24:12,800 --> 00:24:18,460
And the other half goes to the company and says, hey, this is bull shit.

00:24:18,460 --> 00:24:25,150
The company is like, sorry, our fault.

00:24:25,150 --> 00:24:27,490
We will fix it.

00:24:27,490 --> 00:24:29,480
Hmm, right?

00:24:29,480 --> 00:24:31,720
Could happen, probably not.

00:24:31,720 --> 00:24:33,760
And then the problem is solved, right?

00:24:33,760 --> 00:24:39,240
We have an ethical body that we agreed and empowered, and they approaching companies,

00:24:39,240 --> 00:24:44,120
and because the way the ethical body is structured, it is large enough that it wields some sort

00:24:44,120 --> 00:24:46,490
of market influence.

00:24:46,490 --> 00:24:51,800
If they don't respond, if they don't fix it, we go to the public shaming option.

00:24:51,800 --> 00:24:58,390
We go to popular sites like Live Journal, Geo Cities, put it out there.

00:24:58,390 --> 00:25:02,000
All 50 users know that you guys are doing some unethical stuff.

00:25:02,000 --> 00:25:03,430
Will it work?

00:25:03,430 --> 00:25:04,860
I don't know.

00:25:04,860 --> 00:25:11,430
I don't know, it is the only thing that I can think of that we can do ourselves that

00:25:11,430 --> 00:25:13,660
these companies might respond to.

00:25:13,660 --> 00:25:21,490
For all the ill that Facebook is doing, they are trying to respond in some way to all of

00:25:21,490 --> 00:25:24,200
the negative pressure.

00:25:24,200 --> 00:25:27,150
So maybe we can try to leverage that somehow, some way.

00:25:27,150 --> 00:25:32,120
I don't know if it works, but I would LOVE, love, love, to talk about it more in an open

00:25:32,120 --> 00:25:33,120
space.

00:25:33,120 --> 00:25:34,120
What are some of your ideas?

00:25:34,120 --> 00:25:37,320
What are you thinking, is this something that people care about, is it something that I

00:25:37,320 --> 00:25:39,510
worry about?

00:25:39,510 --> 00:25:41,960
Part of it is, it is a great starting point.

00:25:41,960 --> 00:25:45,360
As technology evolves, I'm worried about AI.

00:25:45,360 --> 00:25:50,840
Who is providing AI data, are we programatically encoding our biases?

00:25:50,840 --> 00:25:53,560
Women, you are cold in the office, why?

00:25:53,560 --> 00:25:56,900
Because they only studied men.

00:25:56,900 --> 00:25:59,150
That's why you are cold in the office.

00:25:59,150 --> 00:26:01,200
How do we make sure this is represented?

00:26:01,200 --> 00:26:10,230
Those are larger conversations, and with consent, that is an easy first step because we can

00:26:10,230 --> 00:26:15,450
all agree that if you are going to do something with my information, it would be nice that

00:26:15,450 --> 00:26:18,100
you told me you were going to do it.

00:26:18,100 --> 00:26:21,870
The other problem is, companies are not going to do it themselves as long as there is a

00:26:21,870 --> 00:26:24,230
bad actor doing the bad stuff.

00:26:24,230 --> 00:26:29,790
It is unfair if company A is saying, well, we can target Jeff, it is in his 40s, know

00:26:29,790 --> 00:26:30,790
how much he makes, two kids, a wife, loves Star Trek, and these guys know he loves fruit

00:26:30,790 --> 00:26:31,790
snacks.

00:26:31,790 --> 00:26:32,790
So they will violate until they bring the players together.

00:26:32,790 --> 00:26:33,790
So we will talk about this in open space.

00:26:33,790 --> 00:26:34,790
That's my time?

00:26:34,790 --> 00:26:35,790
Three minutes early.

00:26:35,790 --> 00:26:36,790
That's it for today, thank you for listening, hopefully I did not bore you with the ethics

00:26:36,790 --> 00:26:37,790
talk and we can continue this talk in the open spaces places this afternoon.

00:26:37,790 --> 00:26:38,790
Thank you.

00:26:38,790 --> 00:26:39,790
[ Applause ]. >>MATT STRATTON: Thank you.

00:26:39,790 --> 00:26:40,790
Thank you, Jeff, that was amazing, as always.

00:26:40,790 --> 00:26:41,790
So our next speaker is Heidi Waterhouse.

00:26:41,790 --> 00:26:42,790
So Heidi has spoken at DevOpsDays Chicago before, and is joining us again!

00:26:42,790 --> 00:26:43,790
Oh!

00:26:43,790 --> 00:26:44,790
Forgive what I just said.

00:26:44,790 --> 00:26:45,790
We are going to say thank you to our sponsors, and we have some sponsor pitch time!

00:26:45,790 --> 00:26:46,790
So, where are our sponsors?

00:26:46,790 --> 00:26:47,790
Don't you love the part where it says this is the 6th time we've done this?

00:26:47,790 --> 00:26:48,790
The 7th time.

00:26:48,790 --> 00:26:49,790
>>SASHA ROSENBAUM: We are three minutes early.

00:26:49,790 --> 00:26:50,790
>>MATT STRATTON: That's fine, it is probably fine.

00:26:50,790 --> 00:26:51,790
Nobody wants that.

00:26:51,790 --> 00:26:52,790
Nobody wants that, either.

00:26:52,790 --> 00:26:53,790
>>AUDIENCE MEMBER: (Off-mic comments).

00:26:53,790 --> 00:26:54,790
>>MATT STRATTON: Lindy hop?

00:26:54,790 --> 00:26:55,790
Sure, that is an evening event.

00:26:55,790 --> 00:26:56,790
>>SASHA ROSENBAUM: Now we stand here, waiting for sponsors.

00:26:56,790 --> 00:26:57,790
>>MATT STRATTON: We're not used to being early.

00:26:57,790 --> 00:26:58,790
>> What was your favorite part about that talk?

00:26:58,790 --> 00:26:59,790
>>MATT STRATTON: My favorite part about that talk, besides the Star Trek references, was

00:26:59,790 --> 00:27:00,790
the consent -- the slide about consent.

00:27:00,790 --> 00:27:01,790
That is super, duper key.

00:27:01,790 --> 00:27:02,790
And I think Jeff was right.

00:27:02,790 --> 00:27:03,790
Opening up and talking about ethics, it is an interesting way to start.

00:27:03,790 --> 00:27:04,790
But I thought it was very compelling.

00:27:04,790 --> 00:27:05,790
Sasha, what did you like?

00:27:05,790 --> 00:27:06,790
>>SASHA ROSENBAUM: I think we've been -- this talk came out of a conversation that happened

00:27:06,790 --> 00:27:07,790
two years ago, and a year ago in open spaces, which is part of the value of open spaces

00:27:07,790 --> 00:27:08,790
at this conference.

00:27:08,790 --> 00:27:09,790
And so this is just a thing that we started thinking about, right, that we don't have

00:27:09,790 --> 00:27:10,790
any kind of enforcement body, or any way to complain to anyone if board is doing something

00:27:10,790 --> 00:27:11,790
crappy, and most of us need to pay bills and cannot rage quit and go someplace else.

00:27:11,790 --> 00:27:12,790
Like Jeff said, companies don't have any reason to stop doing evil, or semi-evil things, because

00:27:12,790 --> 00:27:13,790
there is competition.

00:27:13,790 --> 00:27:14,790
So it is something that we all need to think about, how do we prevent the tech from doing

00:27:14,790 --> 00:27:15,790
evil things?

00:27:15,790 --> 00:27:16,790
>>MATT STRATTON: I think our sponsors are ready.

00:27:16,790 --> 00:27:17,790
They are over here.

00:27:17,790 --> 00:27:18,790
>>SASHA ROSENBAUM: Okay!

00:27:18,790 --> 00:27:19,790
>>MATT STRATTON: So our first sponsor is Rundeck!

00:27:19,790 --> 00:27:20,790
>> I'm from Rundeck, who likes getting -- everyone in this room deals with tickets.

00:27:20,790 --> 00:27:21,790
Who wants to get more tickets?

00:27:21,790 --> 00:27:22,790
There is usually one, and we have to have security haul them away.

00:27:22,790 --> 00:27:23,790
[High pitched voice] yea, me, more tickets!

00:27:23,790 --> 00:27:24,790
It is an open source platform, it started in 2009 and 2010.

00:27:24,790 --> 00:27:25,790
And the entire concept around doing self-service, there's a lot of ways that it can be powerful

00:27:25,790 --> 00:27:26,790
for you.

00:27:26,790 --> 00:27:27,790
One of the things that we're excited about is incident remediation.

00:27:27,790 --> 00:27:28,790
You get an alert From PagerDuty, or Vector Ops, the CAT ops thing is not working, you

00:27:28,790 --> 00:27:29,790
need to throw out the Gui, maybe it is the middle of the night, you get up and try to

00:27:29,790 --> 00:27:30,790
fix it.

00:27:30,790 --> 00:27:31,790
With Run Deck, we want you to democratize access to all of the end points, you can run

00:27:31,790 --> 00:27:32,790
automation against and the automation you have inside of your system and allow people

00:27:32,790 --> 00:27:33,790
outside of the organization to get things done so you no longer wait for people to notice

00:27:33,790 --> 00:27:34,790
the small ticket and that person is context switching, wasting time.

00:27:34,790 --> 00:27:35,790
So come to the booth, talk to the people there and get excited about Run Deck!

00:27:35,790 --> 00:27:36,790
>>MATT STRATTON: Next up, PagerDuty!

00:27:36,790 --> 00:27:37,790
Yay.

00:27:37,790 --> 00:27:38,790
[ Applause ]. >> All right.

00:27:38,790 --> 00:27:39,790
How is everybody doing?

00:27:39,790 --> 00:27:40,790
So on Pager Duty, we focus on realtime operations.

00:27:40,790 --> 00:27:41,790
So with on-call management, letting the users know what is happening, essentially getting

00:27:41,790 --> 00:27:42,790
the notifications to the right people, at the right time.

00:27:42,790 --> 00:27:43,790
Preventing issues before they happen.

00:27:43,790 --> 00:27:44,790
We are over next to Run Deck, come over and raffle to get this guy right here.

00:27:44,790 --> 00:27:45,790
You can take a picture, tweet it out, but if you have any questions, or want to learn

00:27:45,790 --> 00:27:46,790
more, definitely feel free to come by and hopefully we will be able to answer those

00:27:46,790 --> 00:27:47,790
for you.

00:27:47,790 --> 00:27:48,790
>>SASHA ROSENBAUM: Thank you!

00:27:48,790 --> 00:27:49,790
[ Applause ]. >>SASHA ROSENBAUM: A little-known fact, I

00:27:49,790 --> 00:27:50,790
won this twice, but I didn't take it either time.

00:27:50,790 --> 00:27:51,790
And this is fine.

00:27:51,790 --> 00:27:52,790
>>MATT STRATTON: Our next sponsor is F5!

00:27:52,790 --> 00:27:53,790
[ Applause ]. >> Thank you.

00:27:53,790 --> 00:27:54,790
Hey, guys.

00:27:54,790 --> 00:27:55,790
I'm Josh from F5 networks.

00:27:55,790 --> 00:27:56,790
I'm sure many of you don't know who we are, and what we do.

00:27:56,790 --> 00:27:57,790
We acquired a small company, called NGINX.

00:27:57,790 --> 00:27:58,790
We are a load balancer, here to support the community, and we don't give long speeches.

00:27:58,790 --> 00:27:59,790
Come by and check us out!

00:27:59,790 --> 00:28:00,790
>>SASHA ROSENBAUM: Thank you.

00:28:00,790 --> 00:28:01,790
>>MATT STRATTON: And our last sponsor for this break is Dynatrace!

00:28:01,790 --> 00:28:02,790
>> So you heard from Josh from F5, I'm the Josh from Dynatrace, I'm less eloquent, apologies

00:28:02,790 --> 00:28:03,790
in advance.

00:28:03,790 --> 00:28:04,790
Who has heard of Dynatrace?

00:28:04,790 --> 00:28:05,790
If you haven't, come on by.

00:28:05,790 --> 00:28:06,790
We have cool swag, raffling an Echo Show, we have shirts and stickers.

00:28:06,790 --> 00:28:07,790
Come and find out more about us, we are an application management market, we are the

00:28:07,790 --> 00:28:08,790
only AI -driven solution out there.

00:28:08,790 --> 00:28:09,790
C'mon by, talk to us.

00:28:09,790 --> 00:28:10,790
We would love to talk to you.

00:28:10,790 --> 00:28:11,790
And I will keep it at that!

00:28:11,790 --> 00:28:12,790
[ Applause ]. >>SASHA ROSENBAUM: Thank you.

00:28:12,790 --> 00:28:13,790
>>MATT STRATTON: Thank you.

00:28:13,790 --> 00:28:14,790
Thank you, sponsors.

00:28:14,790 --> 00:28:15,790
>>SASHA ROSENBAUM: All right.

00:28:15,790 --> 00:28:16,790
So, we can introduce the next wonderful speaker, Heidi Waterhouse!

00:28:16,790 --> 00:28:17,790
C'mon.

00:28:17,790 --> 00:28:17,791

YouTube URL: https://www.youtube.com/watch?v=_pY7h1m1ux0


