Title: DevOpsDays Chicago 2019 - Holly Allen - Service Ownership @Slack
Publication date: 2019-09-09
Playlist: DevOpsDays Chicago 2019
Description: 
	Holly Allen - Service Ownership @Slack

Last year the Slack development team and operations teams were living in different worlds. Development teams deployed to production over a hundred times a day, and a centralized operations team tried to fix things when they broke. The operations teams struggled to support systems they had not written. Heros and knowledge islands saved the day over and over. Post-incident meetings were poorly attended and did not encourage learning.

Slowly, then quickly, all that changed. Slack moved to teams of empowered developers on-call, with embedded SREs, safer production deployments, and actionable alerts. Post-incident meetings focus on learning, and meaningful analysis of incident patterns is done at all levels of the company.

In this talk you’ll hear all about the bumps and scrapes, triumphs and pitfalls of our journey from a centralized ops team to development teams that own the full lifecycle of their systems. It wasn’t easy, but it wasn’t impossible. Hopefully it will inspire you to try something radically different at your company too.
Captions: 
	00:00:13,190 --> 00:00:16,660
Holly Allen - Service Ownership @Slack >>Holly Allen: I'm Holly from Slack, I will

00:00:16,660 --> 00:00:18,700
talk about service ownership.

00:00:18,700 --> 00:00:28,980
I have been in development for 9 years, in development and leadership roles, and I have

00:00:28,980 --> 00:00:35,720
been able to take this technical skillset to a huge number of domains, such as biotech,

00:00:35,720 --> 00:00:41,880
publishing, government and entertainment, and now at Slack.

00:00:41,880 --> 00:00:47,699
So I have done a lot of things with computers, and learned a lot about different kinds of

00:00:47,699 --> 00:00:49,440
businesses and organizations.

00:00:49,440 --> 00:00:54,160
But I actually started my career as a mechanical engineer.

00:00:54,160 --> 00:01:01,030
I absolutely loved studying mechanical engineering, and after college, my first team was building

00:01:01,030 --> 00:01:03,359
a new kind of engine.

00:01:03,359 --> 00:01:08,580
We were in the design phase, I drew a lot of mechanical drawings, like this one, sent

00:01:08,580 --> 00:01:16,390
them for fabrication, came back, ran my tests, did the calculations, redesigned the parts,

00:01:16,390 --> 00:01:17,900
and repeat.

00:01:17,900 --> 00:01:23,640
I worked there for two and a half years, but the entire time was this really slow slog

00:01:23,640 --> 00:01:27,830
of designing, testing, and learning, each cycle months long.

00:01:27,830 --> 00:01:33,640
And, at the end, we didn't even have a product, or even a field-testable prototype, just a

00:01:33,640 --> 00:01:36,360
series of test units, like this one.

00:01:36,360 --> 00:01:39,450
And we were still just in the lab, not in the field.

00:01:39,450 --> 00:01:45,030
And at the same time, I was writing software to interface with the sensors for testing,

00:01:45,030 --> 00:01:48,640
and the fuel and air flow systems.

00:01:48,640 --> 00:01:52,590
And that technology grew a lot faster than the engine.

00:01:52,590 --> 00:02:00,880
I found it satisfying to write that software every day, a lot more satisfying than building

00:02:00,880 --> 00:02:02,620
the engine for years.

00:02:02,620 --> 00:02:07,900
I switched to engineering full time and have not looked back.

00:02:07,900 --> 00:02:13,129
It is not because I don't love making things, I absolutely do.

00:02:13,129 --> 00:02:17,450
It wasn't because I wanted to throw away four years of expensive, specialized education,

00:02:17,450 --> 00:02:19,730
because I didn't.

00:02:19,730 --> 00:02:24,710
But for me, mechanical engineering work was too slow.

00:02:24,710 --> 00:02:30,460
Any test would give you 10 ideas, but it takes you forever to work through them and find

00:02:30,460 --> 00:02:31,890
a great solution.

00:02:31,890 --> 00:02:37,960
Meanwhile on the product I was on, we didn't have product market fit, just test units.

00:02:37,960 --> 00:02:41,740
And with software development, everything was super fast.

00:02:41,740 --> 00:02:47,230
You can write code, test it immediately, and then fix it right away if it wasn't working.

00:02:47,230 --> 00:02:51,930
If you have good tests and user testing, you have confidence you are building the right

00:02:51,930 --> 00:02:56,870
thing every day.

00:02:56,870 --> 00:03:02,600
It was the difference between a fast and a slow cycle time around this loop, designing

00:03:02,600 --> 00:03:10,380
an about, code, a business process, try it out, measure results, learn from those results

00:03:10,380 --> 00:03:11,980
and try it again.

00:03:11,980 --> 00:03:15,000
And the faster you go around this loop, the better.

00:03:15,000 --> 00:03:21,490
I was exposed to all of these principles in college when studying the history of manufacturing.

00:03:21,490 --> 00:03:26,220
The Toyota production system, developed after World War II, revolutionized car manufacturing

00:03:26,220 --> 00:03:31,459
at the time and it was the precursor to what we call lean manufacturing.

00:03:31,459 --> 00:03:39,090
Traditional car manufacturing relies on stockpiles of parts and finished cars to fulfill orders,

00:03:39,090 --> 00:03:46,880
which means you had to predict a year in advance how much you need, and making and storing

00:03:46,880 --> 00:03:49,040
the part and cars.

00:03:49,040 --> 00:03:52,930
The Toyota production system designed out all of that.

00:03:52,930 --> 00:03:58,270
And the dealer triggers a new production at the plant.

00:03:58,270 --> 00:04:03,920
So the dealer is pulling a car from the plant to fill the empty spot on the lot, because

00:04:03,920 --> 00:04:05,340
they sold a car.

00:04:05,340 --> 00:04:12,810
And the plant starts making a car, and as they use the parts, they pull parts from the

00:04:12,810 --> 00:04:13,810
supplier.

00:04:13,810 --> 00:04:21,150
They track parts at the factory using physical cards called combun, and those cards stay

00:04:21,150 --> 00:04:22,900
with the parts.

00:04:22,900 --> 00:04:29,460
When the parts are used up, then you have a card with no parts and you place that on

00:04:29,460 --> 00:04:37,370
a combun board, and a card without parts represents an empty slot in the system that needs to

00:04:37,370 --> 00:04:38,770
be filled.

00:04:38,770 --> 00:04:45,300
So the cards are on the board, move right, and as the fabrication is completed, they

00:04:45,300 --> 00:04:48,300
exit the board and are attached to new parts.

00:04:48,300 --> 00:04:55,310
You can control the inventory in the system by limiting the cards you have.

00:04:55,310 --> 00:05:05,020
And a decade after I visited a factory and saw cards attached to parts, I started using

00:05:05,020 --> 00:05:09,570
combun in developing software on the agile team.

00:05:09,570 --> 00:05:14,050
And I thought, this is the exact same thing, I had no idea.

00:05:14,050 --> 00:05:20,270
And everything about lean manufacturing that I learned was combined with how to make software.

00:05:20,270 --> 00:05:28,949
And so the cards represent work to do, they throw left to right, and eliminating waste

00:05:28,949 --> 00:05:31,430
and reducing cycle them are the goals.

00:05:31,430 --> 00:05:41,830
Besides eliminating waste, a key concept in the production system is kai zen, continuous

00:05:41,830 --> 00:05:42,970
improvement.

00:05:42,970 --> 00:05:52,400
So line workers in a factory can improve their work stations, and they can reconfigure the

00:05:52,400 --> 00:05:57,710
factory to make things efficient, without involving management.

00:05:57,710 --> 00:06:03,440
Everyone at the factory is empowered to design and manage their work.

00:06:03,440 --> 00:06:10,630
Empowerment is key to it, otherwise improvements go through layers of approval and change slows

00:06:10,630 --> 00:06:12,670
down.

00:06:12,670 --> 00:06:22,240
So this all looks like the lean thinking we do in software.

00:06:22,240 --> 00:06:30,370
Like most of you, I have been through agile transformations, I have been a scrum master,

00:06:30,370 --> 00:06:40,120
attended thousands of scrums, groomed giant backlogs, and I have been in two or three

00:06:40,120 --> 00:06:48,060
lively debates about the best user story size is, do you include bugs and sprint planning.

00:06:48,060 --> 00:06:54,180
In my experience, a lot of teams practicing Agile are putting in a lot of effort to do

00:06:54,180 --> 00:06:59,639
it right and not really living the benefits.

00:06:59,639 --> 00:07:04,130
Organizing the work is great, and staying in sync with my co-workers with daily stand-up

00:07:04,130 --> 00:07:06,330
is awesome.

00:07:06,330 --> 00:07:09,590
Writing code every day is still great.

00:07:09,590 --> 00:07:14,230
Knowing that my code was compiling, passing tests, and giving the output I expected was

00:07:14,230 --> 00:07:16,740
exhilarating, but that is not enough.

00:07:16,740 --> 00:07:24,090
You need to shift to the user, learn in production, you need to test in production, like we learned

00:07:24,090 --> 00:07:25,090
yesterday.

00:07:25,090 --> 00:07:29,370
And all that execution has to add up to something.

00:07:29,370 --> 00:07:34,550
Too many agile teams I have been on feel like this, a fast dev team on a treadmill.

00:07:34,550 --> 00:07:40,330
You tell yourself you are doing well, the agile metrics show, you have short lead times,

00:07:40,330 --> 00:07:45,060
or accurate estimates, but you are not shipping.

00:07:45,060 --> 00:07:51,260
I'm not talking about the difference between scrum and lean, I have used all of those.

00:07:51,260 --> 00:07:54,479
And each of those can fail in this way.

00:07:54,479 --> 00:07:59,020
So personally, I have observed two things that differentiate teams that are delivering

00:07:59,020 --> 00:08:02,600
the right things fast, and teams that aren't.

00:08:02,600 --> 00:08:05,910
The first is executive dedication to learning.

00:08:05,910 --> 00:08:11,819
If your highest leaders are not committed to creating a learning and adapting organization,

00:08:11,819 --> 00:08:16,539
one that is fearless in the face of change, then no team in that org is going to succeed

00:08:16,539 --> 00:08:19,000
under those terms, either.

00:08:19,000 --> 00:08:21,680
And the second thing is high-trust teams.

00:08:21,680 --> 00:08:27,040
So high-trust teams can really dig into what is not going well, and suggest radical changes

00:08:27,040 --> 00:08:28,040
to make it better.

00:08:28,040 --> 00:08:32,320
They can push themselves to do that, over and over again.

00:08:32,320 --> 00:08:37,740
A high-trust team can execute the design, measure, learn cycle and make progress incredibly

00:08:37,740 --> 00:08:38,740
quickly.

00:08:38,740 --> 00:08:43,460
Too often, teams are not willing to measure and learn and try new things.

00:08:43,460 --> 00:08:48,740
It is a lot more comfortable to avoid conflict, not talk about the bigger issues.

00:08:48,740 --> 00:08:54,250
It is a lot more comfortable to avoid the pain of change, especially when that change

00:08:54,250 --> 00:08:55,630
might fail.

00:08:55,630 --> 00:09:01,370
So a lot of teams languish, not asking the hard questions, not really learning, if you

00:09:01,370 --> 00:09:03,850
are not learning and changing, you are going nowhere.

00:09:03,850 --> 00:09:04,850
All right.

00:09:04,850 --> 00:09:08,459
What does this have to do with Slack?

00:09:08,459 --> 00:09:12,890
So Slack launched in February 2014, and it grew really quickly.

00:09:12,890 --> 00:09:18,880
So, within five years, we grew to 10 million daily active users.

00:09:18,880 --> 00:09:24,130
We went from supporting really small teams to supporting some companies that have hundreds

00:09:24,130 --> 00:09:25,870
of thousands of users each.

00:09:25,870 --> 00:09:34,260
And, because Slack is a communication tool, people keep it open for nine hours a day actively

00:09:34,260 --> 00:09:36,350
using it.

00:09:36,350 --> 00:09:45,300
We went from 100 servers to over 15,000 servers in AWS, 25 data centers.

00:09:45,300 --> 00:09:52,270
And then, of course, we grew from 8 to 1600 employees in 10 international offices.

00:09:52,270 --> 00:10:00,170
So that's a ton of growth, and Slack really lives and breathes this lean thinking, and

00:10:00,170 --> 00:10:03,150
executive dedication learning is super high.

00:10:03,150 --> 00:10:07,990
And part of why that is, is Slack itself is a massive pivot.

00:10:07,990 --> 00:10:15,430
It started as a gaming company, and their game basically failed to make money fast enough.

00:10:15,430 --> 00:10:21,100
And they're looking at the end of the company, winding it down, and they're like, well, we

00:10:21,100 --> 00:10:27,070
have this internal chat program we wrote for ourselves to make it easier to make the game,

00:10:27,070 --> 00:10:30,010
and maybe that would do okay in the marketplace.

00:10:30,010 --> 00:10:34,410
And that kind of worked out.

00:10:34,410 --> 00:10:39,610
So what is great, from the very beginning, shipping code that is fast to users was a

00:10:39,610 --> 00:10:40,610
priority.

00:10:40,610 --> 00:10:45,510
So they set up continuous deployment systems, where any developer can push code to production

00:10:45,510 --> 00:10:50,610
in minutes, built-in experiment frameworks to test features and interface changes with

00:10:50,610 --> 00:10:58,790
slices of your user base, and we were always releasing major features, testing them with

00:10:58,790 --> 00:11:01,370
users along the way.

00:11:01,370 --> 00:11:05,700
And we're lucky enough to have a design and user research department that measures how

00:11:05,700 --> 00:11:07,490
users are experiencing Slack.

00:11:07,490 --> 00:11:09,339
So, great!

00:11:09,339 --> 00:11:13,649
But there is something, long these five years, that didn't really scale.

00:11:13,649 --> 00:11:18,580
That was the centralized operations team.

00:11:18,580 --> 00:11:24,810
Who is responsible for the monitoring and operation of a production application?

00:11:24,810 --> 00:11:27,060
There is no right answer to this question.

00:11:27,060 --> 00:11:32,900
But a centralized operations team was Slack's answer for a lot of years.

00:11:32,900 --> 00:11:39,540
One team to do your cloud instances and write all the Chef in terraform, manage the pages,

00:11:39,540 --> 00:11:46,970
the incidents, divide your labor into specialized areas.

00:11:46,970 --> 00:11:52,920
So the product developers are focusing on features and scale and architecture, and this

00:11:52,920 --> 00:11:55,930
model works well for a lot of companies.

00:11:55,930 --> 00:11:59,130
To be honest, it worked for Slack for a long time.

00:11:59,130 --> 00:12:03,600
In the early days, most Slack developers knew the whole code base.

00:12:03,600 --> 00:12:10,240
And, as Slack grew, ops engineers generally knew who to contact to get help.

00:12:10,240 --> 00:12:13,240
So ops was getting all the pages in the early days.

00:12:13,240 --> 00:12:19,510
The devs showed up when there was a problem and things were working out.

00:12:19,510 --> 00:12:26,830
As time went on, growth really meant that product development scaled faster than ops

00:12:26,830 --> 00:12:34,519
and, at one point, we were 20: 1 on product devs to op engineers.

00:12:34,519 --> 00:12:41,010
So how can operations reliably reach a developer when there's a problem?

00:12:41,010 --> 00:12:45,260
So gradually, the developers started to go on call.

00:12:45,260 --> 00:12:47,779
Just the most ultra-senior developers at first.

00:12:47,779 --> 00:12:52,000
There was a group of 8 to 12 engineers, basically.

00:12:52,000 --> 00:12:57,940
And they had this rotation, and they can be escalated, too, if something was beyond ops

00:12:57,940 --> 00:13:01,460
has power to fix.

00:13:01,460 --> 00:13:04,990
There were different thoughts and feelings happening.

00:13:04,990 --> 00:13:09,350
Ops was happy that developers were going to be available via pager duty escalation in

00:13:09,350 --> 00:13:12,529
some organized fashion to help.

00:13:12,529 --> 00:13:15,040
But some of the devs have never been on call in their whole life.

00:13:15,040 --> 00:13:20,000
They were scared and not confident it was going to work.

00:13:20,000 --> 00:13:27,620
And now we're at ops getting the first pages, but the ultra-senior devs are on call.

00:13:27,620 --> 00:13:33,590
And that worked for a little while and, you know, even if the dev on call didn't know

00:13:33,590 --> 00:13:38,620
how to fix something, you still knew who in the org did know.

00:13:38,620 --> 00:13:43,760
Call mode, she knows how this stuff works, she will probably answer.

00:13:43,760 --> 00:13:53,100
And Slack is a high-trust organization, and this goes on for a few years, and more engineers,

00:13:53,100 --> 00:13:58,070
more features, more systems, and more and more often, the on-call dev doesn't know how

00:13:58,070 --> 00:14:00,899
to fix the problem.

00:14:00,899 --> 00:14:07,350
So we ask this question again, how can operations reliably reach a developer when there's a

00:14:07,350 --> 00:14:11,660
problem, and on-call, reach the right developer?

00:14:11,660 --> 00:14:17,200
So in fall, 2017, most of the product developers went on call.

00:14:17,200 --> 00:14:23,470
Seven new pager rotations were created overnight, covering specific parts of Slack's infrastructure

00:14:23,470 --> 00:14:26,100
and product.

00:14:26,100 --> 00:14:29,360
The change management for this change was pretty bad.

00:14:29,360 --> 00:14:34,829
Ideally, people are included in the changes to their work, like empowered, continuous

00:14:34,829 --> 00:14:35,950
improvement.

00:14:35,950 --> 00:14:42,670
But this change came from the top, and it really dissempowered people.

00:14:42,670 --> 00:14:49,079
Ops was feeling pretty happy, because we had these new rotations, like search, front end,

00:14:49,079 --> 00:14:54,990
or back end, so you can reach somebody that can help you with the specific problem that

00:14:54,990 --> 00:14:56,070
you are seeing.

00:14:56,070 --> 00:15:00,340
But the devs were pretty surprised, oh my gosh, I'm on call now?

00:15:00,340 --> 00:15:03,750
What happened?

00:15:03,750 --> 00:15:09,490
But they were -- that fear was tempered by one of the bad aspects of these on-calls,

00:15:09,490 --> 00:15:13,860
which is that some of them are really big, all the front-end engineers or the back-end

00:15:13,860 --> 00:15:15,329
engineers.

00:15:15,329 --> 00:15:20,209
Some of these folks were on-call 2, 3, 4 times a year.

00:15:20,209 --> 00:15:23,170
And being on call is like anything else.

00:15:23,170 --> 00:15:29,750
You learn by doing it, and so if you are only on call a couple times a year, you are probably

00:15:29,750 --> 00:15:37,410
scared each time, because you don't get used to the sensation of being on call and, sort

00:15:37,410 --> 00:15:42,380
of, the life patterns you have to set up for it.

00:15:42,380 --> 00:15:49,490
And so, if you get paged, you don't know how to be an incident, either.

00:15:49,490 --> 00:15:54,260
Again, you are only in it three times a year.

00:15:54,260 --> 00:15:59,970
So at this point, fall 2017,ops is getting the first pages, all the senior devs are on

00:15:59,970 --> 00:16:05,279
call, and we are having 7 more targeted pager rotations.

00:16:05,279 --> 00:16:08,130
So we're evolving.

00:16:08,130 --> 00:16:11,680
At this point, we have dozens of production deployments every day.

00:16:11,680 --> 00:16:16,620
We have that great continuous deployment system that empowers the developers to push to production

00:16:16,620 --> 00:16:22,649
within minutes, which means that ops has to keep a really detailed understanding in their

00:16:22,649 --> 00:16:24,120
heads of the whole system.

00:16:24,120 --> 00:16:30,680
And you have to know which of the pager rotations to page, given what you are seeing.

00:16:30,680 --> 00:16:38,050
So the engineers are basically human routers, finding the pager rotation, or the specific

00:16:38,050 --> 00:16:44,230
people that need to help you, in any given incident.

00:16:44,230 --> 00:16:49,800
Slack keeps growing, more people, more systems, more code.

00:16:49,800 --> 00:16:55,570
Even with seven rotations, over time, it was a good chance that the dev who was paged didn't

00:16:55,570 --> 00:17:01,310
know about the sub-system having problems, that left the devs feeling like failures,

00:17:01,310 --> 00:17:04,860
and ops feeling really over burdened.

00:17:04,860 --> 00:17:10,490
And still end up calling people who weren't on call who were the ones who knew how something

00:17:10,490 --> 00:17:11,490
worked.

00:17:11,490 --> 00:17:17,889
So, in alerting organization, the post-incident meeting, or the post-mortem, like most of

00:17:17,889 --> 00:17:24,919
us call it, there's a chance in that meeting to learn about the unexpected complexities

00:17:24,919 --> 00:17:32,679
of the system, the nuance of how things fail, and really extract that learning from people

00:17:32,679 --> 00:17:34,970
who knew it best.

00:17:34,970 --> 00:17:42,519
But the problem at this point is at Slack, post-mortems were not a great place for learning.

00:17:42,519 --> 00:17:50,029
They were run by the ops engineers who were tired and over worked and didn't have the

00:17:50,029 --> 00:17:57,149
time, or the context, to prepare in the way that you need to, to make a really good post-mortem.

00:17:57,149 --> 00:18:02,259
So the post-mortems were not about learning, but creating lists of action items.

00:18:02,259 --> 00:18:08,359
And people didn't think that attending would be a great use of time, so the group that

00:18:08,359 --> 00:18:12,840
felt needed to attend, because they were directly involved in the incident attended and everyone

00:18:12,840 --> 00:18:16,879
else fell away.

00:18:16,879 --> 00:18:25,419
In fall 2017, operations got new leadership, we had a re-org and a mission change.

00:18:25,419 --> 00:18:29,739
And like any change re-org, you have a new way.

00:18:29,739 --> 00:18:34,350
So now operations is called service engineering.

00:18:34,350 --> 00:18:38,940
So we asked ourselves a new question, how do we ensure that the developers know that

00:18:38,940 --> 00:18:41,499
there's a problem?

00:18:41,499 --> 00:18:46,169
We decided that centralized operations was no longer the answer.

00:18:46,169 --> 00:18:52,740
Service ownership was the transformation, the idea that the dev teams write the code,

00:18:52,740 --> 00:18:56,669
owns the operations of that code, right down to getting the pages and running the incident

00:18:56,669 --> 00:18:58,279
response.

00:18:58,279 --> 00:19:02,609
So obviously there was a radical departure from Slack's past s and that level of change

00:19:02,609 --> 00:19:05,789
can be pretty uncomfortable.

00:19:05,789 --> 00:19:11,389
But we really, really leaned on the fact that Slack is a high-trust learning organization,

00:19:11,389 --> 00:19:14,129
so we really dug into that trust and got to work.

00:19:14,129 --> 00:19:21,519
So the idea was, service engineering would focus on providing tools, guidance to producing

00:19:21,519 --> 00:19:29,119
products, cloud platform, storage platform, and slowly push operational responsibility

00:19:29,119 --> 00:19:32,119
toward the dev teams.

00:19:32,119 --> 00:19:37,950
So what about those really high-stakes teams, the ones that really do need that support?

00:19:37,950 --> 00:19:40,429
We decided to create an SRE team.

00:19:40,429 --> 00:19:44,639
So SRE means a lot of different things at different places.

00:19:44,639 --> 00:19:52,259
At Slack, SRE are DevOps generalists, with high emotional intelligence and a mentoring

00:19:52,259 --> 00:19:58,049
capability, because they are skilled practitioners of DevOps and ambassadors of this new way

00:19:58,049 --> 00:19:59,049
of working.

00:19:59,049 --> 00:20:03,629
They are basically selling this way of working to these dev teams.

00:20:03,629 --> 00:20:13,080
So we embedded an SRE into these few select teams to increase operational maturity, the

00:20:13,080 --> 00:20:18,590
reliability of services, and this was really a grass roots effort from the SREs themselves.

00:20:18,590 --> 00:20:26,909
Management's role was to empower them, remove road blocks, and get out of the way.

00:20:26,909 --> 00:20:32,970
We are excited, we have success metrics, the team pairings, but the operations perk didn't

00:20:32,970 --> 00:20:34,379
go away.

00:20:34,379 --> 00:20:37,629
Now we have the SRE that are getting the first pages.

00:20:37,629 --> 00:20:40,320
We didn't change the alerting strategy first.

00:20:40,320 --> 00:20:50,570
SREs are getting pages dozens a week, there are dozens of production deployments a week.

00:20:50,570 --> 00:20:55,279
So how do we lower the operational burden on the SREs?

00:20:55,279 --> 00:21:01,109
They made a plan, we are going to categorize and re-route the existing paging alerts to

00:21:01,109 --> 00:21:02,571
the right teams so they can act on them.

00:21:02,571 --> 00:21:09,190
The non-existent central operations teams will not waste their time and energy anymore

00:21:09,190 --> 00:21:15,629
and will have it for their embedded teams.

00:21:15,629 --> 00:21:18,190
What does it look like?

00:21:18,190 --> 00:21:21,549
Teams should know what they own, what team owns what?

00:21:21,549 --> 00:21:32,879
This was a difficult question to answer, start with 1, who owns this stuff, and find ownership

00:21:32,879 --> 00:21:40,239
for all of these features and software that just had names of people who had worked on

00:21:40,239 --> 00:21:42,479
them last.

00:21:42,479 --> 00:21:47,979
And then we said, like, we defined a whole set of criteria for, like, what service ownership

00:21:47,979 --> 00:21:52,460
meant, and it means a lot of thing, but it also includes, like, you have to have at least

00:21:52,460 --> 00:21:56,919
one alerting health metric, latency, whatever is important for your feature.

00:21:56,919 --> 00:22:07,059
We started getting teams on-call ready, right-sizing them, we heard about pizza teams and starting

00:22:07,059 --> 00:22:12,000
to think about moving away from all front-end engineers being on the same rotation, and

00:22:12,000 --> 00:22:15,889
back-end engineers being on the same rotation.

00:22:15,889 --> 00:22:23,359
So the devs are like, this is scarry, we are on board, we need training, documentation,

00:22:23,359 --> 00:22:28,970
and maybe guard rails in the system so we don't mess stuff up.

00:22:28,970 --> 00:22:36,379
So the SREs start planning all of those things, and nothing changed, heh, because making progress

00:22:36,379 --> 00:22:44,429
on training and guardrails is really slow when you've got the site to keep up all day.

00:22:44,429 --> 00:22:50,500
So the dev teams are starting to work on their health checks, tuning, tweaking their alerts

00:22:50,500 --> 00:22:56,759
and channels to make sure it is just right, and the SREs are working on the training,

00:22:56,759 --> 00:22:59,279
guard rails, and automation.

00:22:59,279 --> 00:23:07,539
Everyone is working hard, but again, we are going nowhere, because we are aiming for perfection.

00:23:07,539 --> 00:23:13,979
So the SREs are looking every week at these alerts, and the vast majority are host-level

00:23:13,979 --> 00:23:22,950
work, out of memory, out of disk, they are paging operations for years and they are huge

00:23:22,950 --> 00:23:28,479
part of the alerting strategy and there was a lot of uncertainty about turning them off.

00:23:28,479 --> 00:23:34,399
So finally, testing the users, go to the dev teams, here are the alerts your dev teams

00:23:34,399 --> 00:23:36,690
would get, what do you think?

00:23:36,690 --> 00:23:45,379
And the dev teams were like, that is useless, and it was like coming out of the a fog, that

00:23:45,379 --> 00:23:51,450
is right, we shouldn't be running this in the first place, getting these alerts, we

00:23:51,450 --> 00:23:58,019
should throw away the hosts and re-provision the hosts and make sure the services can handle

00:23:58,019 --> 00:23:59,019
that.

00:23:59,019 --> 00:24:03,659
Great, so we started working on that.

00:24:03,659 --> 00:24:07,169
Guess what that looks like?

00:24:07,169 --> 00:24:10,610
We wanted to do all this automation first, we wanted to do it right.

00:24:10,610 --> 00:24:15,969
We knew it was possible, we could envision it in our minds.

00:24:15,969 --> 00:24:19,230
But, you know, we were still moving way too slow.

00:24:19,230 --> 00:24:23,749
And then this breakthrough came last fall.

00:24:23,749 --> 00:24:27,840
So there are these moments of organizational clarity that happened.

00:24:27,840 --> 00:24:33,700
For us, it was a push from senior leadership that reliability and fast incident response

00:24:33,700 --> 00:24:37,309
were literally the most important things that we can do in engineering.

00:24:37,309 --> 00:24:44,049
And so in this moment of intense clarity, we decided we're not going to waste this crisis,

00:24:44,049 --> 00:24:47,600
we're going to swallow our fears and take the plunge.

00:24:47,600 --> 00:24:53,399
So one afternoon, we turned off every single one of the low-level alerts.

00:24:53,399 --> 00:24:57,989
I walked to the desk of the dev teams and said, today is the day, you are going on call

00:24:57,989 --> 00:25:02,990
and turning on the alerts you have been carefully crafting for months now.

00:25:02,990 --> 00:25:09,830
On that day, with no pre-planning, the devs went on call for their alerts.

00:25:09,830 --> 00:25:13,580
And everything worked, everything was fine.

00:25:13,580 --> 00:25:17,659
There was really bad change management, once again.

00:25:17,659 --> 00:25:21,039
No comes plan, we had skipped way ahead on the timeline.

00:25:21,039 --> 00:25:27,289
But all the people affected had been working towards this for months, and they knew it

00:25:27,289 --> 00:25:28,289
was coming.

00:25:28,289 --> 00:25:33,489
And they were empowered to continue to change those alerts and their own on-call strategy

00:25:33,489 --> 00:25:37,529
after that day, because they more fumly owned these services now.

00:25:37,529 --> 00:25:42,309
And, ever since then, those SREs have fully dedicated themselves to the teams they are

00:25:42,309 --> 00:25:43,850
embedded in.

00:25:43,850 --> 00:25:53,669
Today, there are dozens of rotations, one for each team, we have alerts to tell us when

00:25:53,669 --> 00:25:55,580
services are failing.

00:25:55,580 --> 00:26:08,869
I see them learning, provisioning, decoupling, and teams dramatically improve the resilience

00:26:08,869 --> 00:26:10,960
of the their services.

00:26:10,960 --> 00:26:15,399
I'm not going to tell you everything that happened in the journey to service ownership,

00:26:15,399 --> 00:26:17,809
but we continue to make improvements.

00:26:17,809 --> 00:26:24,139
Teams of developrs fully own their systems, it is important that you are on call for your

00:26:24,139 --> 00:26:25,139
systems.

00:26:25,139 --> 00:26:31,899
These are a few thing we are working on, we are asking and challenging ourselves to improve

00:26:31,899 --> 00:26:35,659
the post-mortems.

00:26:35,659 --> 00:26:42,799
We hired experts, did training, post-incident analysis and investigation, we are still continuing

00:26:42,799 --> 00:26:45,070
to heavily invest in this area.

00:26:45,070 --> 00:26:51,169
And we also really want to have trained incident commanders for every incident, this skill

00:26:51,169 --> 00:26:54,960
is too centralized in engineering.

00:26:54,960 --> 00:27:00,779
So we have gotten a lot of trainings together, a lot of people trained up.

00:27:00,779 --> 00:27:09,750
We also added it to the engineer career ladder, and to, sort of, insentiveize people to participate.

00:27:09,750 --> 00:27:12,309
But it is falling mostly to service engineering.

00:27:12,309 --> 00:27:17,320
We've got ideas, it is major focus for us right now.

00:27:17,320 --> 00:27:23,499
Like most places, we ask ourselves, how can we make it easier to operate a service without

00:27:23,499 --> 00:27:31,159
specialized training, like Chef, so like most of you, we are building a Kubernetes platform,

00:27:31,159 --> 00:27:36,789
early signs are good.

00:27:36,789 --> 00:27:43,369
So we have made a lot of progress, there's a lot of work to do, Slack continues to be

00:27:43,369 --> 00:27:46,440
a high-trust organization.

00:27:46,440 --> 00:27:52,090
You can ask what is not working and suggest radical changes, so I know we are going to

00:27:52,090 --> 00:27:53,090
continue to make progress.

00:27:53,090 --> 00:28:02,230
So I wanted to leave you with one last thought, the Toyota production system eliminates waste,

00:28:02,230 --> 00:28:08,609
but the entire management philosophy as a company is designed around that way of working.

00:28:08,609 --> 00:28:13,970
Low inventory levels, which saves money, is just one really visible outcome.

00:28:13,970 --> 00:28:21,219
So some other businesses, when they saw how successful Toyota was, tried to lower inventory

00:28:21,219 --> 00:28:26,779
levels in isolation without understanding the philosophy, or making empowered working

00:28:26,779 --> 00:28:28,609
environments.

00:28:28,609 --> 00:28:31,309
And those projects failed.

00:28:31,309 --> 00:28:37,009
So imitating another company, or process, without understanding the underlying concepts

00:28:37,009 --> 00:28:38,229
doesn't work.

00:28:38,229 --> 00:28:43,950
What works for Toyota, Slack, or me, will not work for you, just like following the

00:28:43,950 --> 00:28:49,450
scrum process perfectly will not lead you to amazing results.

00:28:49,450 --> 00:28:53,269
The most important thing to know is what you are trying to accomplish and be willing to

00:28:53,269 --> 00:28:56,049
learn and try again and again.

00:28:56,049 --> 00:29:00,470
So I'm not saying that mechanical engineering is a bad profession, it just didn't work for

00:29:00,470 --> 00:29:01,470
me.

00:29:01,470 --> 00:29:06,619
I'm not saying that central operations can't work, it doesn't work for Slack anymore.

00:29:06,619 --> 00:29:13,039
Change is possible, no matter how hard or impossible it feels.

00:29:13,039 --> 00:29:18,820
So ask yourself what feels wrong about your work, and imagine a different future.

00:29:18,820 --> 00:29:23,679
And don't be paralyzed by doubt or perfection or uncertainty.

00:29:23,679 --> 00:29:27,269
You don't have to be ready to make a change.

00:29:27,269 --> 00:29:33,109
If you have the support of leadership, if you are in a high-trust environment, you are

00:29:33,109 --> 00:29:39,159
empowered to make change, and you can commit yourself to continuous improvement, then progress

00:29:39,159 --> 00:29:41,889
is inevitable.

00:29:41,889 --> 00:29:45,950
Succeed is the speed and skill with which you go around this loop.

00:29:45,950 --> 00:29:49,113
So design thoughtfully, measure ruthlessly, and learn faster.

00:29:49,113 --> 00:29:50,113
Thanks.

00:29:50,113 --> 00:29:51,113
[ Applause ]. Live captioning by Lindsay @stoker_lindsay

00:29:51,113 --> 00:29:52,113
at White Coat Captioning @whitecoatcapxg.

00:29:52,113 --> 00:29:53,113
>>SASHA ROSENBAUM: Thank you, Holly.

00:29:53,113 --> 00:29:54,113
And now we're going to have a much-anticipated break, and you can get some coffee and a bio

00:29:54,113 --> 00:29:55,113
break and whatever you need.

00:29:55,113 --> 00:29:56,113
And please be here at 10:40 for the next talk!

00:29:56,113 --> 00:29:57,113
And also, go visit sponsors.

00:29:57,113 --> 00:29:57,118

YouTube URL: https://www.youtube.com/watch?v=UJ6Otz7IFKE


