Title: How our ISP's firewall cost us a full day of the entire R&D - Lior Redlus - DevOpsDays Tel Aviv 2017
Publication date: 2017-11-23
Playlist: DevOpsDays Tel Aviv 2017
Description: 
	http://devopsdays.org/events/2017-tel-aviv/
Captions: 
	00:00:05,420 --> 00:00:11,280
okay hi everyone I'm going to present

00:00:08,370 --> 00:00:15,210
you a quick overview of how our ISP cost

00:00:11,280 --> 00:00:17,160
us like the whole day of our R&D team so

00:00:15,210 --> 00:00:19,680
about myself a little bit I'm 32 years

00:00:17,160 --> 00:00:20,609
old I have a master's degree in

00:00:19,680 --> 00:00:22,859
neuroscience in information processing

00:00:20,609 --> 00:00:26,519
and I am one of the co-founders and

00:00:22,859 --> 00:00:28,949
chief data scientist at core logics Cora

00:00:26,519 --> 00:00:31,439
logics is a machine learning powered

00:00:28,949 --> 00:00:33,120
scalable log analytics solutions which

00:00:31,439 --> 00:00:34,770
means that we have all the log

00:00:33,120 --> 00:00:36,989
management tools that you already use to

00:00:34,770 --> 00:00:39,090
the indexing querying filtering etc but

00:00:36,989 --> 00:00:41,309
on top of that we actually have core

00:00:39,090 --> 00:00:43,140
logics analytics which turns your data

00:00:41,309 --> 00:00:45,120
into policies and flows it gives you

00:00:43,140 --> 00:00:46,860
deep insights on your system it

00:00:45,120 --> 00:00:50,850
automatically detects production

00:00:46,860 --> 00:00:52,800
problems it can also find you what the

00:00:50,850 --> 00:00:55,050
changes in the behavior or software

00:00:52,800 --> 00:00:56,699
between different code deployments so

00:00:55,050 --> 00:01:00,480
get all of these when you get core

00:00:56,699 --> 00:01:02,430
logics three options to interact with

00:01:00,480 --> 00:01:04,049
your data one of them is the core logic

00:01:02,430 --> 00:01:05,670
dashboard which are our is our

00:01:04,049 --> 00:01:07,229
proprietary dashboard with the machine

00:01:05,670 --> 00:01:09,979
learning capabilities it's very simple

00:01:07,229 --> 00:01:12,900
much simpler than elastics Cabana but

00:01:09,979 --> 00:01:14,490
Cabana does give you all the rich query

00:01:12,900 --> 00:01:16,439
language and flexible visualizations

00:01:14,490 --> 00:01:19,049
which everyone is used to and love so

00:01:16,439 --> 00:01:20,549
much and it's a great add-on and of

00:01:19,049 --> 00:01:21,960
course if you are very technical I want

00:01:20,549 --> 00:01:24,090
all the aggregations that elasticsearch

00:01:21,960 --> 00:01:26,810
provides them it's also this way that

00:01:24,090 --> 00:01:29,880
you can get everything all kinds of

00:01:26,810 --> 00:01:31,439
slicing and dicing on your data so the

00:01:29,880 --> 00:01:33,720
product is good and the customers are

00:01:31,439 --> 00:01:36,570
happy and everything worked smoothly for

00:01:33,720 --> 00:01:39,240
months until one day we get a call one

00:01:36,570 --> 00:01:41,939
of our customers who send around half a

00:01:39,240 --> 00:01:44,040
terabyte per day and he says listen my

00:01:41,939 --> 00:01:47,189
heavier cabana dashboards are not

00:01:44,040 --> 00:01:49,579
loading and of course he wasn't happy

00:01:47,189 --> 00:01:53,100
and neither will we

00:01:49,579 --> 00:01:56,790
so no one can be happy when seeing this

00:01:53,100 --> 00:01:59,090
kind of error which we initially didn't

00:01:56,790 --> 00:02:01,619
know what to do with it but we could

00:01:59,090 --> 00:02:04,409
replicate it in our offices so we

00:02:01,619 --> 00:02:06,479
immediately began investigating so let's

00:02:04,409 --> 00:02:08,970
begin with understanding how Cabana is

00:02:06,479 --> 00:02:11,009
implemented in core logics we have our

00:02:08,970 --> 00:02:12,930
customer who's in the public domain and

00:02:11,009 --> 00:02:16,860
interacts with our Cabana

00:02:12,930 --> 00:02:20,760
the standard port 5 601 it gets taken by

00:02:16,860 --> 00:02:22,439
angularjs a component and the local host

00:02:20,760 --> 00:02:27,120
gets into the node.js component

00:02:22,439 --> 00:02:28,860
collectively known as Cabana this

00:02:27,120 --> 00:02:32,400
information is transferred through the

00:02:28,860 --> 00:02:34,829
query mechanism to what Cabana thinks is

00:02:32,400 --> 00:02:40,700
elasticsearch but there it sits our

00:02:34,829 --> 00:02:43,500
proprietary proxy which emulates Cabana

00:02:40,700 --> 00:02:45,569
emulates elastic search for Cabana it

00:02:43,500 --> 00:02:47,579
confines customers you only have access

00:02:45,569 --> 00:02:49,709
to their data alone and it parses

00:02:47,579 --> 00:02:52,379
queries enough to have various slavery

00:02:49,709 --> 00:02:56,459
restrictions and it is then transferred

00:02:52,379 --> 00:02:58,769
into our elastic cluster all these are

00:02:56,459 --> 00:03:01,159
sitting different doctor containers and

00:02:58,769 --> 00:03:04,349
the whole process is working as expected

00:03:01,159 --> 00:03:06,030
but what could have gone wrong there so

00:03:04,349 --> 00:03:07,829
we tried doing everything that

00:03:06,030 --> 00:03:09,840
interesting everything we can think of

00:03:07,829 --> 00:03:12,450
maybe some of the customers dashboard

00:03:09,840 --> 00:03:15,780
queries were not properly defined but

00:03:12,450 --> 00:03:17,400
everything was ok then we looked at is

00:03:15,780 --> 00:03:20,160
the elastic data maybe there were some

00:03:17,400 --> 00:03:20,549
corruption but everything was there ok

00:03:20,160 --> 00:03:22,919
again

00:03:20,549 --> 00:03:24,810
and what about our cabana proxy this is

00:03:22,919 --> 00:03:28,230
like our sweet spot that we developed

00:03:24,810 --> 00:03:31,019
maybe it had some overloading for the

00:03:28,230 --> 00:03:32,699
and for all the dashboard that he was

00:03:31,019 --> 00:03:35,099
loading we're not according to our

00:03:32,699 --> 00:03:39,269
monitoring maybe it was certain queries

00:03:35,099 --> 00:03:42,359
that we had a bug but none and that we

00:03:39,269 --> 00:03:44,069
could think of and then we sorted to

00:03:42,359 --> 00:03:46,530
docker maybe it was a docker container

00:03:44,069 --> 00:03:48,060
with different settings or one of the

00:03:46,530 --> 00:03:50,400
docker networking bugs that we

00:03:48,060 --> 00:03:53,760
experienced when using swarm the old

00:03:50,400 --> 00:03:58,109
swamp that was managing our cluster but

00:03:53,760 --> 00:04:00,629
everything seemed to work fine so why

00:03:58,109 --> 00:04:03,569
not finding we did find is that whenever

00:04:00,629 --> 00:04:05,760
we try to replicate this using when we

00:04:03,569 --> 00:04:07,260
are connected to our VPN everything

00:04:05,760 --> 00:04:09,389
disappeared all the problems disappeared

00:04:07,260 --> 00:04:12,299
everything what is expected so it was

00:04:09,389 --> 00:04:16,169
already late at night we decided to call

00:04:12,299 --> 00:04:20,099
it a day and just get a very very angry

00:04:16,169 --> 00:04:22,200
with all the situation but connecting

00:04:20,099 --> 00:04:24,360
the dots we returned home and we found

00:04:22,200 --> 00:04:26,180
out that all the dashboards subtly

00:04:24,360 --> 00:04:28,280
loaded when we try to do this

00:04:26,180 --> 00:04:32,479
each each and every one was from our

00:04:28,280 --> 00:04:35,539
homes the thing is the same ISP provider

00:04:32,479 --> 00:04:37,970
was serving our customer as in our

00:04:35,539 --> 00:04:41,180
offices but not at our homes which

00:04:37,970 --> 00:04:43,100
turned out to be our new suspect so

00:04:41,180 --> 00:04:45,919
returning the next day we tried to

00:04:43,100 --> 00:04:48,500
experiment with SSL this know as a cell

00:04:45,919 --> 00:04:52,220
cabana using the it standard port via

00:04:48,500 --> 00:04:54,919
the port 443 and try to add the Cabana

00:04:52,220 --> 00:04:57,410
scene into CloudFlare and the results

00:04:54,919 --> 00:04:59,660
were staggering I only have one

00:04:57,410 --> 00:05:02,449
screenshot of this we have a deeper

00:04:59,660 --> 00:05:04,699
technical analysis in our office but as

00:05:02,449 --> 00:05:07,490
you can see what is reported through

00:05:04,699 --> 00:05:08,990
home 14 seconds of loading when we're

00:05:07,490 --> 00:05:11,830
trying to do this without CloudFlare

00:05:08,990 --> 00:05:14,030
without SSL and understand on port and

00:05:11,830 --> 00:05:17,900
only two and a half seconds and many

00:05:14,030 --> 00:05:21,259
less requests then then on the other

00:05:17,900 --> 00:05:24,590
side of that comparison other things

00:05:21,259 --> 00:05:27,590
that we've noticed is that the requests

00:05:24,590 --> 00:05:29,479
were throttled and some of the check

00:05:27,590 --> 00:05:31,580
sums of the packet sizes did not match

00:05:29,479 --> 00:05:35,349
the package size itself so some kind of

00:05:31,580 --> 00:05:35,349
data loss was actually happening there

00:05:36,760 --> 00:05:43,550
the question what was the ISP and this

00:05:41,330 --> 00:05:49,699
this wind we discussed privately for a

00:05:43,550 --> 00:05:52,610
very large sum maybe so eventually we we

00:05:49,699 --> 00:05:54,530
sorted out to having cabana and transfer

00:05:52,610 --> 00:05:57,199
all the data through CloudFlare and port

00:05:54,530 --> 00:06:00,710
443 and SSL and it sorted out everything

00:05:57,199 --> 00:06:02,930
of course aside from losing the whole

00:06:00,710 --> 00:06:06,520
day of the whole R&D team and the

00:06:02,930 --> 00:06:08,389
conclusion is trust no one

00:06:06,520 --> 00:06:10,250
so if you have any further questions

00:06:08,389 --> 00:06:12,050
you're welcome to turn to me directly

00:06:10,250 --> 00:06:13,759
and if you want to use CoreLogic you

00:06:12,050 --> 00:06:16,220
have one month of free trial and you can

00:06:13,759 --> 00:06:22,259
see all the details in our website

00:06:16,220 --> 00:06:22,259

YouTube URL: https://www.youtube.com/watch?v=IDBYbxanqgM


