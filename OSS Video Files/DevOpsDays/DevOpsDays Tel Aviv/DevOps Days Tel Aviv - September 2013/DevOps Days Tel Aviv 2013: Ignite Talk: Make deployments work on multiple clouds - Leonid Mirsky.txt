Title: DevOps Days Tel Aviv 2013: Ignite Talk: Make deployments work on multiple clouds - Leonid Mirsky
Publication date: 2013-10-24
Playlist: DevOps Days Tel Aviv - September 2013
Description: 
	In this talk Leonid will share his experience of taking an AWS only deployment process and extending it to support both private and public clouds (Openstack and Rackspace). If you think that all clouds were born the same, think again! The road to a truly multi cloud provisioning isn't so simple. Leonid will provide some technical tips about what you should expect when you are getting outside of the AWS garden, as well as some practical technical examples of how you can overcome some common pitfalls.

Speaker:

Leonid Mirsky

Leonid has more than 10 years experience in different IT operations roles. Currently, as an Independent consultant, he is helping small to medium sized companies to implement DevOps and to realize the potential power of cloud computing.
Captions: 
	00:00:09,550 --> 00:00:14,300
okay so hi everyone my name is Leonard

00:00:12,710 --> 00:00:16,789
and today I'm going to talk about

00:00:14,300 --> 00:00:18,980
multi-cloud provisioning I would like to

00:00:16,789 --> 00:00:20,750
share some of my experience and doing

00:00:18,980 --> 00:00:25,220
multi-cloud provisioning projects at the

00:00:20,750 --> 00:00:26,710
start of codex plenty and just to kick

00:00:25,220 --> 00:00:31,010
things off there's probably multiple

00:00:26,710 --> 00:00:33,109
reasons why I wanna go multi-cloud one

00:00:31,010 --> 00:00:35,780
of them is disaster recovering the

00:00:33,109 --> 00:00:37,550
ability to provision your services on

00:00:35,780 --> 00:00:39,589
top of a different cloud in case your

00:00:37,550 --> 00:00:43,670
and main cloud have some availability

00:00:39,589 --> 00:00:46,639
issues also additionally and each cloud

00:00:43,670 --> 00:00:48,409
has its own characteristics so it can be

00:00:46,639 --> 00:00:50,239
a good thing to move some of your

00:00:48,409 --> 00:00:52,789
services to the cloud that better fits

00:00:50,239 --> 00:00:54,889
your needs but none of these reasons is

00:00:52,789 --> 00:00:58,010
YX plenty wanted to expand into

00:00:54,889 --> 00:01:02,510
multi-cloud and just in a couple of what

00:00:58,010 --> 00:01:04,580
explain T provides a duper set service

00:01:02,510 --> 00:01:06,260
when you don't need to know that open

00:01:04,580 --> 00:01:09,170
tournament center in order to process

00:01:06,260 --> 00:01:12,100
your big data so you actually can in

00:01:09,170 --> 00:01:14,690
visual way visualize your queries and

00:01:12,100 --> 00:01:16,520
explain to have a clusters architecture

00:01:14,690 --> 00:01:19,010
that look something similar to this when

00:01:16,520 --> 00:01:22,160
you new user one sadhu cluster there's a

00:01:19,010 --> 00:01:24,200
worker and built in Ruby and puppet

00:01:22,160 --> 00:01:27,260
which know how to process that request

00:01:24,200 --> 00:01:30,680
and provision a new class a dupe cluster

00:01:27,260 --> 00:01:33,520
on amazon for that user and the request

00:01:30,680 --> 00:01:35,780
was to extend that worker and

00:01:33,520 --> 00:01:37,940
functionality to provision this cluster

00:01:35,780 --> 00:01:40,160
on top of three additional providers

00:01:37,940 --> 00:01:43,730
software Rackspace and private

00:01:40,160 --> 00:01:45,830
installation of OpenStack so I just

00:01:43,730 --> 00:01:48,230
wanna go briefly on sound the challenges

00:01:45,830 --> 00:01:49,700
we had during that process in case there

00:01:48,230 --> 00:01:51,530
are some people here who are interested

00:01:49,700 --> 00:01:53,540
in multi-cloud provisioning it should

00:01:51,530 --> 00:01:57,230
provides a good taste of what to expect

00:01:53,540 --> 00:01:58,790
for from such a project the first thing

00:01:57,230 --> 00:02:01,340
we noticed is the lack of network

00:01:58,790 --> 00:02:04,580
automation when you request a new

00:02:01,340 --> 00:02:06,500
instance from amazon and you actually go

00:02:04,580 --> 00:02:08,810
that's an example you actually got

00:02:06,500 --> 00:02:10,610
everything configured for you from dns

00:02:08,810 --> 00:02:13,430
to order

00:02:10,610 --> 00:02:15,860
networking this is something that's not

00:02:13,430 --> 00:02:18,920
available on the other provide cloud

00:02:15,860 --> 00:02:20,480
providers we walked with on different

00:02:18,920 --> 00:02:24,050
degrees you probably will need to take

00:02:20,480 --> 00:02:25,820
care about the DNS yourself and to go

00:02:24,050 --> 00:02:28,700
into additional provisioning step 2 for

00:02:25,820 --> 00:02:30,830
example attach public IP it introduced

00:02:28,700 --> 00:02:32,930
some issues for us for example with our

00:02:30,830 --> 00:02:35,300
puppet configuration how we were using

00:02:32,930 --> 00:02:38,930
puppet so we solve that by using a

00:02:35,300 --> 00:02:41,390
third-party dinner service it's not

00:02:38,930 --> 00:02:42,860
always applicable in our case it was a

00:02:41,390 --> 00:02:44,750
good solution however you can also

00:02:42,860 --> 00:02:46,970
manage your own DNS if you don't mind

00:02:44,750 --> 00:02:50,770
the writing the automation and managing

00:02:46,970 --> 00:02:54,020
your own DNS on the additional cloud

00:02:50,770 --> 00:02:57,320
additionally we were using something

00:02:54,020 --> 00:02:59,870
called user data scripts that's when you

00:02:57,320 --> 00:03:02,840
request a new ec2 instance you actually

00:02:59,870 --> 00:03:04,580
can provide a shell script that will

00:03:02,840 --> 00:03:08,900
configure that instance for you at boot

00:03:04,580 --> 00:03:12,410
time and that supported not supported on

00:03:08,900 --> 00:03:14,420
all the providers we were using so we

00:03:12,410 --> 00:03:17,269
also couldn't eliminate the need of that

00:03:14,420 --> 00:03:19,549
so we needed to add again add some

00:03:17,269 --> 00:03:22,540
additional provisioning steps to to

00:03:19,549 --> 00:03:24,530
overcome that to emulate that behavior

00:03:22,540 --> 00:03:27,470
additionally it was very hard for us to

00:03:24,530 --> 00:03:29,540
find the same similar instant sizes

00:03:27,470 --> 00:03:31,250
across the providers so for example as

00:03:29,540 --> 00:03:33,950
you can see here the with the same CPU

00:03:31,250 --> 00:03:36,560
memory we actually got less storage so

00:03:33,950 --> 00:03:38,480
we could all over provision or add

00:03:36,560 --> 00:03:41,500
additional provision steps to attach

00:03:38,480 --> 00:03:44,180
additional storage but all of that is

00:03:41,500 --> 00:03:46,160
easily discoverable during the

00:03:44,180 --> 00:03:48,799
development process itself but we

00:03:46,160 --> 00:03:50,989
actually saw some behaviors which we

00:03:48,799 --> 00:03:53,630
couldn't see before we run the

00:03:50,989 --> 00:03:55,250
application in production one thing that

00:03:53,630 --> 00:03:58,430
is very interesting to discover is that

00:03:55,250 --> 00:04:00,320
each cloud has its own weak spots we

00:03:58,430 --> 00:04:03,440
were optimizing our provisioning process

00:04:00,320 --> 00:04:06,380
for amazon and it began to fare on

00:04:03,440 --> 00:04:08,480
different phases on different providers

00:04:06,380 --> 00:04:10,760
so actually we need to invest time for

00:04:08,480 --> 00:04:12,650
that additionally the performance

00:04:10,760 --> 00:04:14,660
between the provides various differently

00:04:12,650 --> 00:04:17,989
differently so as you can see in that

00:04:14,660 --> 00:04:20,599
graph from rivera blog and thank you

00:04:17,989 --> 00:04:22,600
guys but we actually did discover the

00:04:20,599 --> 00:04:24,070
same potential

00:04:22,600 --> 00:04:26,830
difference in average time in the api

00:04:24,070 --> 00:04:29,740
error rates so we needed to somehow

00:04:26,830 --> 00:04:31,720
overcome that we added the on/off

00:04:29,740 --> 00:04:34,510
switches for easily to switch on/off

00:04:31,720 --> 00:04:36,910
regions of some of the providers we saw

00:04:34,510 --> 00:04:40,210
they are not reliable in addition to

00:04:36,910 --> 00:04:43,600
that and we fine-tuned our retry

00:04:40,210 --> 00:04:46,330
mechanisms to better and work with the

00:04:43,600 --> 00:04:48,850
different providers so just to summarize

00:04:46,330 --> 00:04:51,040
I provided some of the challenges we had

00:04:48,850 --> 00:04:54,550
actually had much more especially in the

00:04:51,040 --> 00:04:56,200
networking department however if I need

00:04:54,550 --> 00:04:58,630
to take one thing from that experience

00:04:56,200 --> 00:05:00,910
is that despite the fact that all the

00:04:58,630 --> 00:05:03,730
providers seem to have the same basic

00:05:00,910 --> 00:05:05,740
functionality on paper when you try to

00:05:03,730 --> 00:05:09,040
move from amazon to and other providers

00:05:05,740 --> 00:05:11,380
you need to plan to invest to fill the

00:05:09,040 --> 00:05:14,050
gap of the automation and it's not an

00:05:11,380 --> 00:05:16,240
easy task you just need to plan for that

00:05:14,050 --> 00:05:18,190
it's not a configuration change the

00:05:16,240 --> 00:05:23,280
devil is in the details so thank you

00:05:18,190 --> 00:05:23,280

YouTube URL: https://www.youtube.com/watch?v=IfPAZCojMxU


