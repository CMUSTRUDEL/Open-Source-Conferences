Title: Statscraft Spotlight Ignite: 5 Monitoring Anti Patterns   and How to Avoid Them, Nati Cohen | Commun
Publication date: 2020-12-26
Playlist: Community Summit TLV 2020
Description: 
	During the past 15 years I was lucky to partake in building and monitoring various production systems. However, while sometimes the monitors and alerts we created were spot-on, and helped us mitigate future failures quickly, other times the dashboards we created were simply useless, and the alerts did nothing but make us miserable. In this talk we will review several common monitoring mistakes my peers and I repeatedly tend to lean towards. We will discuss why these are not the right things to do, and suggest several, hopefully better alternatives.
Captions: 
	00:00:02,880 --> 00:00:05,940
[Music]

00:00:23,439 --> 00:00:27,119
and

00:00:24,240 --> 00:00:28,560
from now we're going straight to our

00:00:27,119 --> 00:00:32,000
spotlight

00:00:28,560 --> 00:00:33,120
talk uh with natty cohen uh that's a

00:00:32,000 --> 00:00:35,520
very uh

00:00:33,120 --> 00:00:36,160
important uh topic and painful one for

00:00:35,520 --> 00:00:37,840
all of us

00:00:36,160 --> 00:00:39,360
about uh monitoring and the

00:00:37,840 --> 00:00:42,399
anti-patterns around that

00:00:39,360 --> 00:00:44,480
so nat is going to talk about five mon

00:00:42,399 --> 00:00:45,680
monitoring anti-patterns and how to

00:00:44,480 --> 00:00:47,840
avoid them

00:00:45,680 --> 00:00:49,680
uh we don't have nothing for uh for a

00:00:47,840 --> 00:00:59,840
live low but we'll see is

00:00:49,680 --> 00:00:59,840
recorded talk here we go

00:01:04,019 --> 00:01:13,529
[Music]

00:01:18,640 --> 00:01:21,920
um hello everyone my name is nathikon

00:01:21,360 --> 00:01:24,479
and

00:01:21,920 --> 00:01:24,960
for the past 15 years or so i've been

00:01:24,479 --> 00:01:27,280
building

00:01:24,960 --> 00:01:28,960
production systems and monitoring them

00:01:27,280 --> 00:01:31,280
and today i'd like to share with you

00:01:28,960 --> 00:01:32,720
some of the mistakes i've been doing and

00:01:31,280 --> 00:01:36,079
i've seen others doing

00:01:32,720 --> 00:01:39,439
around monitoring and alerting so

00:01:36,079 --> 00:01:40,159
you come to the office on monday and you

00:01:39,439 --> 00:01:42,079
found out

00:01:40,159 --> 00:01:43,920
find out that your group is starting to

00:01:42,079 --> 00:01:46,799
use an open source software

00:01:43,920 --> 00:01:48,240
and you got the exciting task of

00:01:46,799 --> 00:01:49,680
designing monitoring for it

00:01:48,240 --> 00:01:51,759
but how do you start monitoring

00:01:49,680 --> 00:01:54,079
something new

00:01:51,759 --> 00:01:56,000
so there are three common approaches the

00:01:54,079 --> 00:01:57,680
first one is the stock overflow approach

00:01:56,000 --> 00:02:00,799
where you just go to google and

00:01:57,680 --> 00:02:02,960
write how to monitor x and the second

00:02:00,799 --> 00:02:04,320
one is the rtfm approach where you open

00:02:02,960 --> 00:02:06,320
the manual and look at the

00:02:04,320 --> 00:02:08,239
metrics section and choose metrics from

00:02:06,320 --> 00:02:09,440
there and the last one is the reverse

00:02:08,239 --> 00:02:12,480
engineering approach

00:02:09,440 --> 00:02:13,360
where you open your monitoring tool look

00:02:12,480 --> 00:02:15,920
at the metrics

00:02:13,360 --> 00:02:18,879
that are being spewed out and choose

00:02:15,920 --> 00:02:21,680
metrics by name and volume

00:02:18,879 --> 00:02:23,040
however all these three approaches have

00:02:21,680 --> 00:02:25,120
problems

00:02:23,040 --> 00:02:27,440
the first problem is that you are going

00:02:25,120 --> 00:02:29,920
to use metrics that you don't really

00:02:27,440 --> 00:02:31,440
understand maybe not fully understand

00:02:29,920 --> 00:02:33,920
the metric or maybe not understanding

00:02:31,440 --> 00:02:36,879
the metric in the scope of your system

00:02:33,920 --> 00:02:38,560
the second problem is where you will be

00:02:36,879 --> 00:02:40,640
looking only at the software you

00:02:38,560 --> 00:02:41,840
you are monitoring and you will lose

00:02:40,640 --> 00:02:45,040
sight of the

00:02:41,840 --> 00:02:49,200
entire system um and there

00:02:45,040 --> 00:02:52,400
might be problems there as well so

00:02:49,200 --> 00:02:53,440
the approach i found working a little

00:02:52,400 --> 00:02:55,280
bit better

00:02:53,440 --> 00:02:57,760
is being explicit about your mental

00:02:55,280 --> 00:02:58,239
model for me it's drawing the entire

00:02:57,760 --> 00:03:01,280
system

00:02:58,239 --> 00:03:03,680
and your software within it be explicit

00:03:01,280 --> 00:03:06,080
about your clients and your dependencies

00:03:03,680 --> 00:03:07,360
use a methodology to choose the metrics

00:03:06,080 --> 00:03:09,840
you want to monitor

00:03:07,360 --> 00:03:12,159
maybe the use method or the red method

00:03:09,840 --> 00:03:13,440
or even the forgotten signals from the

00:03:12,159 --> 00:03:15,280
sre book

00:03:13,440 --> 00:03:16,959
and eventually once you know which

00:03:15,280 --> 00:03:19,200
metrics you want to monitor

00:03:16,959 --> 00:03:21,040
pull the metrics you want they might

00:03:19,200 --> 00:03:21,440
come from the software you're monitoring

00:03:21,040 --> 00:03:23,120
but

00:03:21,440 --> 00:03:25,280
they might also require some

00:03:23,120 --> 00:03:27,120
manipulation on these metrics

00:03:25,280 --> 00:03:28,959
they might come from your clients or

00:03:27,120 --> 00:03:29,920
your dependencies or your operating

00:03:28,959 --> 00:03:32,159
system

00:03:29,920 --> 00:03:33,120
and they might be missing so you might

00:03:32,159 --> 00:03:36,879
need to

00:03:33,120 --> 00:03:36,879
add a code to the software

00:03:37,040 --> 00:03:42,560
so uh you come to work the next day and

00:03:40,879 --> 00:03:43,760
you finished with the previous task but

00:03:42,560 --> 00:03:44,879
now you're getting pulled into a

00:03:43,760 --> 00:03:47,120
postmortem

00:03:44,879 --> 00:03:49,040
and in that postmortem you learn about

00:03:47,120 --> 00:03:50,959
an application crashing

00:03:49,040 --> 00:03:52,319
due to too many files too many open

00:03:50,959 --> 00:03:55,360
files there

00:03:52,319 --> 00:03:55,680
and the action items that came out from

00:03:55,360 --> 00:03:58,480
this

00:03:55,680 --> 00:03:59,200
postmortem are to increase the number of

00:03:58,480 --> 00:04:01,120
files

00:03:59,200 --> 00:04:02,239
and to alert on the number of files

00:04:01,120 --> 00:04:04,480
being used

00:04:02,239 --> 00:04:06,159
and this is our second anti-pattern

00:04:04,480 --> 00:04:07,120
where you are monitoring for software

00:04:06,159 --> 00:04:09,040
bugs

00:04:07,120 --> 00:04:10,159
and this might be okay for a very

00:04:09,040 --> 00:04:12,720
temporary fix

00:04:10,159 --> 00:04:13,599
but in the long run your on-call doesn't

00:04:12,720 --> 00:04:16,720
have much to do

00:04:13,599 --> 00:04:18,880
other than restart the service and you

00:04:16,720 --> 00:04:19,919
will eventually just add more and more

00:04:18,880 --> 00:04:22,320
and more alerts

00:04:19,919 --> 00:04:23,280
uh for every software bug you encounter

00:04:22,320 --> 00:04:26,400
encounter

00:04:23,280 --> 00:04:27,280
because there are uh more common uh than

00:04:26,400 --> 00:04:30,880
what we wanted

00:04:27,280 --> 00:04:33,440
to be uh what you can do instead

00:04:30,880 --> 00:04:34,560
is uh hopefully fix the bug and add

00:04:33,440 --> 00:04:36,639
regression tests

00:04:34,560 --> 00:04:38,560
in order to make sure it doesn't come

00:04:36,639 --> 00:04:40,639
back and also

00:04:38,560 --> 00:04:42,960
uh try to make sure that when you're

00:04:40,639 --> 00:04:45,440
adding alerts you're adding them for

00:04:42,960 --> 00:04:46,400
service degradation right or failure or

00:04:45,440 --> 00:04:49,040
slow breach

00:04:46,400 --> 00:04:51,440
and not for every specific type of

00:04:49,040 --> 00:04:51,440
failure

00:04:52,160 --> 00:04:56,000
so another day in the office and another

00:04:54,080 --> 00:04:58,479
postmortem that you're being pulled and

00:04:56,000 --> 00:04:59,520
pulled into and this time the problem is

00:04:58,479 --> 00:05:01,919
latency spiking

00:04:59,520 --> 00:05:03,199
due to increased traffic uh to a

00:05:01,919 --> 00:05:06,479
resource-heavy

00:05:03,199 --> 00:05:08,400
api endpoint and the action items from

00:05:06,479 --> 00:05:10,160
this postmortem are to add more

00:05:08,400 --> 00:05:13,520
resources and

00:05:10,160 --> 00:05:16,400
also to alert on cpu and ram usage

00:05:13,520 --> 00:05:17,440
which brings us to our third

00:05:16,400 --> 00:05:20,800
anti-pattern

00:05:17,440 --> 00:05:22,800
alerting on ambiguous properties and

00:05:20,800 --> 00:05:24,960
cpu and ram are classic ambiguous

00:05:22,800 --> 00:05:25,840
properties because they can be triggered

00:05:24,960 --> 00:05:27,520
due to

00:05:25,840 --> 00:05:29,600
another issue not the issue you're

00:05:27,520 --> 00:05:31,919
trying to monitor and

00:05:29,600 --> 00:05:33,039
they can also be triggered due to normal

00:05:31,919 --> 00:05:35,600
operation

00:05:33,039 --> 00:05:37,520
like traffic spike garbage collection or

00:05:35,600 --> 00:05:39,520
another os service

00:05:37,520 --> 00:05:41,840
and while your application is running

00:05:39,520 --> 00:05:44,320
normally

00:05:41,840 --> 00:05:45,280
instead what you probably want to do is

00:05:44,320 --> 00:05:47,440
to alert on

00:05:45,280 --> 00:05:50,000
issue specific metrics for example on

00:05:47,440 --> 00:05:52,400
the latency of api endpoints

00:05:50,000 --> 00:05:53,520
and also you want to continuously look

00:05:52,400 --> 00:05:56,800
at your alerts

00:05:53,520 --> 00:05:59,600
and remove or refine alerts that are

00:05:56,800 --> 00:05:59,600
false positive

00:06:00,319 --> 00:06:04,160
eventually you get to the weekend

00:06:02,960 --> 00:06:07,280
however

00:06:04,160 --> 00:06:09,680
on saturday at 2am you get a pager

00:06:07,280 --> 00:06:10,319
and the title of the pager is nodex is

00:06:09,680 --> 00:06:12,000
down

00:06:10,319 --> 00:06:13,919
and the description of the pager is

00:06:12,000 --> 00:06:15,919
exactly the same

00:06:13,919 --> 00:06:17,919
and this is the fourth anti-pattern

00:06:15,919 --> 00:06:19,919
right alerting without context and this

00:06:17,919 --> 00:06:22,880
one is pretty clear

00:06:19,919 --> 00:06:24,800
you get increased time for remediation

00:06:22,880 --> 00:06:27,919
and your on-call might end up

00:06:24,800 --> 00:06:30,000
fixing the wrong thing and

00:06:27,919 --> 00:06:32,400
on the worst case you get a secondary

00:06:30,000 --> 00:06:34,800
incident

00:06:32,400 --> 00:06:35,919
what you can do instead is add a lot of

00:06:34,800 --> 00:06:38,960
context into

00:06:35,919 --> 00:06:41,199
this description part of the pager uh

00:06:38,960 --> 00:06:42,960
which system is being impacted which

00:06:41,199 --> 00:06:45,919
customers are being impacted

00:06:42,960 --> 00:06:46,319
uh who else is being triggered at graphs

00:06:45,919 --> 00:06:49,039
on

00:06:46,319 --> 00:06:50,720
the larger time scope like for the last

00:06:49,039 --> 00:06:54,400
week how this metric looks

00:06:50,720 --> 00:06:57,360
and add uh graphs for other

00:06:54,400 --> 00:06:59,680
triggered alerts right and also add a

00:06:57,360 --> 00:07:02,880
lot of links add a link to your run book

00:06:59,680 --> 00:07:04,160
entry have a run runbook add links to

00:07:02,880 --> 00:07:06,000
the relevant dashboards the

00:07:04,160 --> 00:07:07,759
communication channels you want to tell

00:07:06,000 --> 00:07:10,960
people you are working on it

00:07:07,759 --> 00:07:14,240
and for your dependencies stoppage page

00:07:10,960 --> 00:07:15,120
and if this was too much so the short

00:07:14,240 --> 00:07:19,440
version is

00:07:15,120 --> 00:07:21,599
design your alerts for your 2am self

00:07:19,440 --> 00:07:24,000
and another day in the weekend then

00:07:21,599 --> 00:07:26,960
another pager goes off always at 2am

00:07:24,000 --> 00:07:29,039
because pagers always go off at 2am

00:07:26,960 --> 00:07:30,800
this time the title is nodex is down but

00:07:29,039 --> 00:07:32,880
the description is much better

00:07:30,800 --> 00:07:34,400
cluster y is in the graded state please

00:07:32,880 --> 00:07:37,680
speed up additional nodes

00:07:34,400 --> 00:07:40,800
and some context so this is a better

00:07:37,680 --> 00:07:43,759
pager but it's another anti-pattern

00:07:40,800 --> 00:07:44,080
you're alerting a human and the problems

00:07:43,759 --> 00:07:47,840
are

00:07:44,080 --> 00:07:50,400
the same as the previous anti-pattern

00:07:47,840 --> 00:07:51,440
but also this might lead to your own

00:07:50,400 --> 00:07:55,520
calls

00:07:51,440 --> 00:07:58,560
going burn out right

00:07:55,520 --> 00:08:01,520
going on burnout because we can't handle

00:07:58,560 --> 00:08:02,960
too many nights without sleep instead

00:08:01,520 --> 00:08:06,639
what i recommend you do

00:08:02,960 --> 00:08:07,440
is add autoremediation for these kind of

00:08:06,639 --> 00:08:10,080
failures

00:08:07,440 --> 00:08:10,560
and architect your environment your

00:08:10,080 --> 00:08:13,199
system

00:08:10,560 --> 00:08:14,800
in a way that can handle common failures

00:08:13,199 --> 00:08:16,960
like machine or vm failure

00:08:14,800 --> 00:08:17,919
like a data center failure or like

00:08:16,960 --> 00:08:21,599
increased

00:08:17,919 --> 00:08:23,599
traffic also look into doing game days

00:08:21,599 --> 00:08:25,599
or what the cool kids are calling chaos

00:08:23,599 --> 00:08:28,080
engineering in order to

00:08:25,599 --> 00:08:30,720
find out how your system is handling

00:08:28,080 --> 00:08:34,000
these kind of videos

00:08:30,720 --> 00:08:35,919
to recap uh we've seen a lot

00:08:34,000 --> 00:08:38,320
a lot of mistake i've done over the

00:08:35,919 --> 00:08:40,240
years but i'm really interested in

00:08:38,320 --> 00:08:41,039
hearing about the anti-patterns you

00:08:40,240 --> 00:08:44,399
found

00:08:41,039 --> 00:08:46,720
so shoot me a twit at nocot

00:08:44,399 --> 00:08:50,160
on twitter and thank you very much for

00:08:46,720 --> 00:08:50,160
listening and enjoy the rest of the

00:08:50,190 --> 00:08:54,500
[Music]

00:08:52,839 --> 00:08:59,080
conference

00:08:54,500 --> 00:08:59,080
[Music]

00:09:01,120 --> 00:09:03,200

YouTube URL: https://www.youtube.com/watch?v=d3CaZ-ncIUo


