Title: DevOpsDays Keynote: DataOps Why is it so hard?   Einat Orr, Treeverse | Community Summit TLV 2020
Publication date: 2020-12-26
Playlist: Community Summit TLV 2020
Description: 
	DataOps is harder than you might think. Supporting the operations of big data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, Data is big, so all systems are now becoming distributed. Data is mutable, and it's hard to create repeatable, automated pipelines. On top of that, the technology is evolving at high speed and change management is messy. In this talk, we will dive into those challenges, explore best practices, and develop empathy for those who manage big data environments.
Captions: 
	00:00:02,880 --> 00:00:05,940
[Music]

00:00:13,519 --> 00:00:17,440
as our first

00:00:14,559 --> 00:00:18,080
keynote speaker of the day i am going to

00:00:17,440 --> 00:00:21,199
introduce

00:00:18,080 --> 00:00:23,279
a nut ore from traversie

00:00:21,199 --> 00:00:24,480
and she's going to talk about data ops

00:00:23,279 --> 00:00:28,560
and why it is

00:00:24,480 --> 00:00:31,840
so hard so let me

00:00:28,560 --> 00:00:36,239
share that and we have

00:00:31,840 --> 00:00:38,320
yes so data ops why is that so hard

00:00:36,239 --> 00:00:39,760
well i'll explain in details in a minute

00:00:38,320 --> 00:00:43,200
but it's it's the data

00:00:39,760 --> 00:00:44,000
the data makes it so hard and we will

00:00:43,200 --> 00:00:46,719
see why it makes

00:00:44,000 --> 00:00:47,360
it so hard when you run this video hi

00:00:46,719 --> 00:00:48,800
everyone

00:00:47,360 --> 00:00:50,879
and thank you for having me and i'm

00:00:48,800 --> 00:00:54,239
really excited to be the one opening the

00:00:50,879 --> 00:00:56,719
uh community summit um

00:00:54,239 --> 00:01:00,160
and i'm really honored to be here i hope

00:00:56,719 --> 00:01:01,840
to hear your questions after the talk

00:01:00,160 --> 00:01:03,359
cool yeah so definitely do the questions

00:01:01,840 --> 00:01:05,840
after the talk so and that's

00:01:03,359 --> 00:01:07,600
could you maybe introduce us uh to your

00:01:05,840 --> 00:01:08,799
background in history and how did you

00:01:07,600 --> 00:01:12,479
came out to found

00:01:08,799 --> 00:01:15,920
a controversy reverse

00:01:12,479 --> 00:01:16,720
so yeah odd name i know we thought we

00:01:15,920 --> 00:01:18,570
were

00:01:16,720 --> 00:01:21,280
very i'm always getting my wrong so

00:01:18,570 --> 00:01:24,240
[Laughter]

00:01:21,280 --> 00:01:25,840
um so uh yeah so we were working at

00:01:24,240 --> 00:01:28,479
similar web

00:01:25,840 --> 00:01:31,439
which is a company with a great r d and

00:01:28,479 --> 00:01:34,240
a very interesting devops challenges

00:01:31,439 --> 00:01:36,479
and uh and there are a lot of challenges

00:01:34,240 --> 00:01:38,079
at similar web around data and we found

00:01:36,479 --> 00:01:40,880
that we were patching a lot of things

00:01:38,079 --> 00:01:42,960
because there was no conceptual solution

00:01:40,880 --> 00:01:44,159
to the problems that we had with data

00:01:42,960 --> 00:01:46,479
infrastructure

00:01:44,159 --> 00:01:47,200
and then when it was time to go to move

00:01:46,479 --> 00:01:49,759
on

00:01:47,200 --> 00:01:50,640
and then my co-founder oz katz and

00:01:49,759 --> 00:01:52,720
myself

00:01:50,640 --> 00:01:54,159
thought that introducing a solution that

00:01:52,720 --> 00:01:57,280
might assist in

00:01:54,159 --> 00:01:58,399
managing data is something that we can

00:01:57,280 --> 00:02:00,880
do and want to do

00:01:58,399 --> 00:02:01,840
because of our history at similar web

00:02:00,880 --> 00:02:05,200
originally i'm

00:02:01,840 --> 00:02:07,200
i'm a mathematician and therefore i also

00:02:05,200 --> 00:02:09,440
understand data science people

00:02:07,200 --> 00:02:11,599
and their needs and of course they are

00:02:09,440 --> 00:02:13,520
an important part of

00:02:11,599 --> 00:02:15,040
the teams that are being served by the

00:02:13,520 --> 00:02:18,319
data infrastructure

00:02:15,040 --> 00:02:21,920
and the data devops so

00:02:18,319 --> 00:02:23,280
it helped me it helped me understand the

00:02:21,920 --> 00:02:25,760
field and uh

00:02:23,280 --> 00:02:27,599
connect to their potential customers

00:02:25,760 --> 00:02:30,000
yeah it's always awesome when when

00:02:27,599 --> 00:02:31,360
the founder feels the pain before

00:02:30,000 --> 00:02:33,840
solving the problem

00:02:31,360 --> 00:02:36,000
so we're going to move on to and to the

00:02:33,840 --> 00:02:38,000
main keynote and i'm going to

00:02:36,000 --> 00:02:40,239
share the talk now and then we'll switch

00:02:38,000 --> 00:02:42,800
off back to continuing our discussion

00:02:40,239 --> 00:02:51,420
and have some questions also from from

00:02:42,800 --> 00:02:54,500
the floor

00:02:51,420 --> 00:02:54,500
[Music]

00:03:00,879 --> 00:03:04,080
hi everyone i'm excited to open the

00:03:02,959 --> 00:03:06,159
first ever

00:03:04,080 --> 00:03:07,760
community summit tel aviv where they

00:03:06,159 --> 00:03:10,959
talk about data ops

00:03:07,760 --> 00:03:12,800
why is it so hard so first a little

00:03:10,959 --> 00:03:16,000
about myself

00:03:12,800 --> 00:03:19,519
so to be honest i am not an ops person

00:03:16,000 --> 00:03:21,920
i am a mathematician i had my phd done

00:03:19,519 --> 00:03:24,879
in tel aviv university ages ago

00:03:21,920 --> 00:03:27,120
and since then i have led r d

00:03:24,879 --> 00:03:28,400
organizations in startup companies the

00:03:27,120 --> 00:03:30,640
latest was

00:03:28,400 --> 00:03:31,680
a cto role in similarweb which is a data

00:03:30,640 --> 00:03:33,280
company

00:03:31,680 --> 00:03:36,560
and of course whenever i developed

00:03:33,280 --> 00:03:39,360
algorithms i extensively used data

00:03:36,560 --> 00:03:40,400
and now i am a co-creator of an open

00:03:39,360 --> 00:03:42,480
source

00:03:40,400 --> 00:03:44,959
data infrastructure tool called lake

00:03:42,480 --> 00:03:48,000
fest that we will mention in a word

00:03:44,959 --> 00:03:50,640
during this talk let's begin

00:03:48,000 --> 00:03:51,440
so what are we going to talk about if we

00:03:50,640 --> 00:03:53,599
want to talk about

00:03:51,440 --> 00:03:55,920
why data ops is hard we should probably

00:03:53,599 --> 00:03:57,920
first define what data ops is

00:03:55,920 --> 00:04:00,000
after we do that we will talk about why

00:03:57,920 --> 00:04:02,319
it is hard we'll talk about data

00:04:00,000 --> 00:04:03,680
architecture we'll talk about the roles

00:04:02,319 --> 00:04:07,040
that people need to

00:04:03,680 --> 00:04:08,319
take in order to implement a good data

00:04:07,040 --> 00:04:11,200
environment

00:04:08,319 --> 00:04:12,000
we talk about data pipelines where those

00:04:11,200 --> 00:04:14,799
are pets

00:04:12,000 --> 00:04:15,840
or cattles and at the end we would look

00:04:14,799 --> 00:04:18,479
at a few tools that

00:04:15,840 --> 00:04:20,079
might be the light at the end of the end

00:04:18,479 --> 00:04:21,120
of the tunnel and solve some of our

00:04:20,079 --> 00:04:23,040
problems

00:04:21,120 --> 00:04:25,120
if we have time i'm sure we will we'll

00:04:23,040 --> 00:04:28,800
have some q a

00:04:25,120 --> 00:04:30,880
okay so what is data ops

00:04:28,800 --> 00:04:32,320
i've looked at some of the definitions

00:04:30,880 --> 00:04:35,120
and i kind of

00:04:32,320 --> 00:04:36,639
created my own but it is really based on

00:04:35,120 --> 00:04:39,919
what's out there

00:04:36,639 --> 00:04:40,320
on the web so data ops is a collection

00:04:39,919 --> 00:04:43,680
of

00:04:40,320 --> 00:04:44,320
technical practices workflows cultural

00:04:43,680 --> 00:04:47,440
norms

00:04:44,320 --> 00:04:49,840
and architectural patterns that enable

00:04:47,440 --> 00:04:52,240
rapid innovation and experimentation

00:04:49,840 --> 00:04:55,280
very important when you have research

00:04:52,240 --> 00:04:56,960
resilience and high quality monitoring

00:04:55,280 --> 00:04:57,919
and fast recovery once we're in

00:04:56,960 --> 00:04:59,520
production

00:04:57,919 --> 00:05:02,000
and of course collaboration across a

00:04:59,520 --> 00:05:05,600
complex array of people technologies

00:05:02,000 --> 00:05:08,080
and environments so what we actually

00:05:05,600 --> 00:05:09,440
see here is that we already have terms

00:05:08,080 --> 00:05:11,280
for those challenges

00:05:09,440 --> 00:05:13,199
that we already know from the ops world

00:05:11,280 --> 00:05:14,720
so creating an experimentation

00:05:13,199 --> 00:05:16,800
environment is the development

00:05:14,720 --> 00:05:19,680
environment we're expected to do

00:05:16,800 --> 00:05:21,600
whenever we provide ops to engineering

00:05:19,680 --> 00:05:25,039
resilience and high quality is

00:05:21,600 --> 00:05:26,160
our ci cd testing platforms staging

00:05:25,039 --> 00:05:28,320
environments

00:05:26,160 --> 00:05:30,720
and the monitoring is fast recovery is

00:05:28,320 --> 00:05:33,360
of course monitoring and fast recovery

00:05:30,720 --> 00:05:34,560
and since ops is a team that has a lot

00:05:33,360 --> 00:05:36,560
of interfaces

00:05:34,560 --> 00:05:37,759
a complex array of people that we need

00:05:36,560 --> 00:05:40,560
to work with is

00:05:37,759 --> 00:05:41,759
actually a regular challenge within the

00:05:40,560 --> 00:05:46,320
ops world

00:05:41,759 --> 00:05:49,600
so why is data ops different than ops

00:05:46,320 --> 00:05:50,320
well it's the data so we have another

00:05:49,600 --> 00:05:53,600
aspect

00:05:50,320 --> 00:05:56,240
and another challenge above

00:05:53,600 --> 00:05:56,720
managing the ops which is managing the

00:05:56,240 --> 00:05:59,360
data

00:05:56,720 --> 00:06:00,400
itself the fact we have this additional

00:05:59,360 --> 00:06:02,560
aspect

00:06:00,400 --> 00:06:04,560
not only brings another aspect but also

00:06:02,560 --> 00:06:05,360
makes the other aspects that we are used

00:06:04,560 --> 00:06:08,639
to and know

00:06:05,360 --> 00:06:11,520
from ops to be more complex let's go

00:06:08,639 --> 00:06:15,199
over this and see how

00:06:11,520 --> 00:06:16,880
so how is why is it so hard

00:06:15,199 --> 00:06:18,319
the first aspect is the data

00:06:16,880 --> 00:06:19,919
architecture

00:06:18,319 --> 00:06:22,479
what we can see is that there are no

00:06:19,919 --> 00:06:25,520
best practices

00:06:22,479 --> 00:06:28,319
this is a data lake architecture

00:06:25,520 --> 00:06:31,120
an extremely general one if you have a

00:06:28,319 --> 00:06:32,960
data architecture or about to build one

00:06:31,120 --> 00:06:34,639
you're not necessarily going to have all

00:06:32,960 --> 00:06:36,080
these components but you're going to

00:06:34,639 --> 00:06:38,000
have some of them

00:06:36,080 --> 00:06:39,840
let's go a little into the details for

00:06:38,000 --> 00:06:41,520
those of you who are not familiar with

00:06:39,840 --> 00:06:43,280
data applications

00:06:41,520 --> 00:06:44,960
because we would need this understanding

00:06:43,280 --> 00:06:47,199
later in the talk

00:06:44,960 --> 00:06:48,240
so on the left hand side we can see data

00:06:47,199 --> 00:06:52,400
sources

00:06:48,240 --> 00:06:54,960
those could be events data streams

00:06:52,400 --> 00:06:57,919
actually ingest infrastructure such as

00:06:54,960 --> 00:07:01,199
flink or kafka or spark streaming

00:06:57,919 --> 00:07:02,720
if you heard the names and operational

00:07:01,199 --> 00:07:05,919
data that is usually

00:07:02,720 --> 00:07:08,319
databases replication into

00:07:05,919 --> 00:07:09,599
an object storage or a data lake or a

00:07:08,319 --> 00:07:12,080
single source of truth

00:07:09,599 --> 00:07:13,680
whatever we choose to call it if we look

00:07:12,080 --> 00:07:16,800
at the bottom we can see

00:07:13,680 --> 00:07:17,520
a set of processing techniques such as

00:07:16,800 --> 00:07:21,039
spark

00:07:17,520 --> 00:07:22,960
and presto that also need orchestration

00:07:21,039 --> 00:07:26,160
tools such as airflow

00:07:22,960 --> 00:07:27,599
and capabilities of accessing the data

00:07:26,160 --> 00:07:30,479
through sql

00:07:27,599 --> 00:07:32,560
such as hive this is generally speaking

00:07:30,479 --> 00:07:34,880
the hadoop ecosystem

00:07:32,560 --> 00:07:36,080
what we can see on top is the analytics

00:07:34,880 --> 00:07:38,000
databases

00:07:36,080 --> 00:07:40,160
once upon a time that was the only

00:07:38,000 --> 00:07:40,880
component a data architecture used to

00:07:40,160 --> 00:07:43,440
have

00:07:40,880 --> 00:07:45,120
this is a not necessarily now a

00:07:43,440 --> 00:07:47,199
relational database

00:07:45,120 --> 00:07:48,160
but the logic of a relational database

00:07:47,199 --> 00:07:50,879
which means

00:07:48,160 --> 00:07:54,080
it presents tabular data in a way that

00:07:50,879 --> 00:07:56,080
could be queried by analysts using sql

00:07:54,080 --> 00:07:58,080
and we have a set of technologies today

00:07:56,080 --> 00:08:00,080
that allow that

00:07:58,080 --> 00:08:02,400
in some of them the deep storage is

00:08:00,080 --> 00:08:04,720
actually the object store

00:08:02,400 --> 00:08:06,479
and on the right hand side we have data

00:08:04,720 --> 00:08:07,199
visualization tools which used to be

00:08:06,479 --> 00:08:10,960
called

00:08:07,199 --> 00:08:12,960
bi tools such as tableau or looker

00:08:10,960 --> 00:08:14,639
that can either communicate directly

00:08:12,960 --> 00:08:17,840
with the storage or can work

00:08:14,639 --> 00:08:20,319
over an analytics engine and the last

00:08:17,840 --> 00:08:22,080
part we have is data exploration

00:08:20,319 --> 00:08:24,720
those are notebooks that are used by

00:08:22,080 --> 00:08:26,319
researchers in order to develop models

00:08:24,720 --> 00:08:28,960
over the data

00:08:26,319 --> 00:08:29,840
so if you have a data architecture you

00:08:28,960 --> 00:08:31,840
would have

00:08:29,840 --> 00:08:33,839
a subset of what we're seeing here you

00:08:31,840 --> 00:08:35,519
won't necessarily have an object stories

00:08:33,839 --> 00:08:39,039
you won't necessarily have events

00:08:35,519 --> 00:08:42,240
data but you would be choosing your own

00:08:39,039 --> 00:08:44,159
part of this piece and when you do that

00:08:42,240 --> 00:08:45,279
you would notice that those tools that

00:08:44,159 --> 00:08:48,560
are presented here

00:08:45,279 --> 00:08:50,720
overlap in their capabilities so if we

00:08:48,560 --> 00:08:54,000
go one level down there's no real

00:08:50,720 --> 00:08:57,680
good practice on how to choose which

00:08:54,000 --> 00:09:01,519
tools are optimal for your own challenge

00:08:57,680 --> 00:09:04,800
the ecosystem is based on

00:09:01,519 --> 00:09:06,000
open source tools some of them or most

00:09:04,800 --> 00:09:09,200
of them today

00:09:06,000 --> 00:09:09,600
have provided managed services which can

00:09:09,200 --> 00:09:12,480
take

00:09:09,600 --> 00:09:13,040
some of the offload of the ops effort if

00:09:12,480 --> 00:09:17,360
of course

00:09:13,040 --> 00:09:20,640
you can afford the services provided

00:09:17,360 --> 00:09:22,560
but this poses another challenge why is

00:09:20,640 --> 00:09:24,320
that a challenge because we have very

00:09:22,560 --> 00:09:25,760
strong players we can see here we're

00:09:24,320 --> 00:09:29,200
talking about aws

00:09:25,760 --> 00:09:31,680
confluent data bricks and snowflake

00:09:29,200 --> 00:09:33,440
four very strong players with completely

00:09:31,680 --> 00:09:35,920
different agendas

00:09:33,440 --> 00:09:37,040
the only thing they have in common is a

00:09:35,920 --> 00:09:40,320
philosophy of

00:09:37,040 --> 00:09:42,480
conquering all so aws would like you to

00:09:40,320 --> 00:09:45,519
have your entire data pipeline

00:09:42,480 --> 00:09:47,839
on aws using their own hosting

00:09:45,519 --> 00:09:49,279
confluent would tell you managing kafka

00:09:47,839 --> 00:09:51,680
in real time would tell you

00:09:49,279 --> 00:09:53,600
you don't need storage there's nothing

00:09:51,680 --> 00:09:56,240
there's no such thing as batch

00:09:53,600 --> 00:09:58,000
you can do everything in real time data

00:09:56,240 --> 00:09:59,120
bricks would tell you the entire world

00:09:58,000 --> 00:10:01,120
is spark

00:09:59,120 --> 00:10:03,360
and snowflake would tell you run your

00:10:01,120 --> 00:10:06,800
spark over snowflake

00:10:03,360 --> 00:10:08,800
which is actually a data warehouse

00:10:06,800 --> 00:10:09,920
and each and every one of them is

00:10:08,800 --> 00:10:12,240
actually pulling

00:10:09,920 --> 00:10:14,399
the open source or in snow flakings the

00:10:12,240 --> 00:10:17,360
closed source that they're managing

00:10:14,399 --> 00:10:19,440
to their own philosophy hurting the

00:10:17,360 --> 00:10:23,040
compatibility that does exist

00:10:19,440 --> 00:10:24,240
because the tools are naturally open

00:10:23,040 --> 00:10:27,279
source tools

00:10:24,240 --> 00:10:30,839
but somehow the compatibility is now

00:10:27,279 --> 00:10:32,160
limited let's dive a little deeper into

00:10:30,839 --> 00:10:35,680
that

00:10:32,160 --> 00:10:37,680
so when if we think about managing data

00:10:35,680 --> 00:10:39,920
we have those different layers we have

00:10:37,680 --> 00:10:40,320
the storage where data the data is saved

00:10:39,920 --> 00:10:42,880
we have

00:10:40,320 --> 00:10:44,959
the format over it which is how we save

00:10:42,880 --> 00:10:46,800
the data and how it is indexed

00:10:44,959 --> 00:10:48,240
we have the data frame within the

00:10:46,800 --> 00:10:50,480
compute application

00:10:48,240 --> 00:10:51,760
which is how it is saved in the memory

00:10:50,480 --> 00:10:53,279
of the compute system

00:10:51,760 --> 00:10:54,880
and then we have the compute systems

00:10:53,279 --> 00:10:58,480
that are running this is of course

00:10:54,880 --> 00:11:00,240
a very it's an approximation that only

00:10:58,480 --> 00:11:02,880
helps us understand that

00:11:00,240 --> 00:11:03,279
any mismatch in this in these layers

00:11:02,880 --> 00:11:06,480
could

00:11:03,279 --> 00:11:08,640
be a mismatch in trying and

00:11:06,480 --> 00:11:09,760
work with different tools together in

00:11:08,640 --> 00:11:12,720
the data environment

00:11:09,760 --> 00:11:14,480
so you can work but when you need

00:11:12,720 --> 00:11:17,519
throughput and performance

00:11:14,480 --> 00:11:19,440
you might find yourself with a lack of

00:11:17,519 --> 00:11:20,800
compatibility that allows you to get

00:11:19,440 --> 00:11:23,200
the right throughput and the right

00:11:20,800 --> 00:11:24,959
performance over several tools in

00:11:23,200 --> 00:11:26,959
parallel

00:11:24,959 --> 00:11:28,000
another best practice that is very hard

00:11:26,959 --> 00:11:29,600
to implement in data

00:11:28,000 --> 00:11:31,360
environments is the development

00:11:29,600 --> 00:11:33,760
environment and cicd

00:11:31,360 --> 00:11:36,240
which are critical for creating a

00:11:33,760 --> 00:11:38,720
resilient environment and high quality

00:11:36,240 --> 00:11:38,720
outputs

00:11:39,040 --> 00:11:42,880
we'll talk later about why it is so

00:11:40,800 --> 00:11:44,800
complex to create but since it is a best

00:11:42,880 --> 00:11:46,480
practice to have development environment

00:11:44,800 --> 00:11:50,160
and ci cd

00:11:46,480 --> 00:11:51,519
we are mentioning this in this part

00:11:50,160 --> 00:11:53,360
the second challenge that we have

00:11:51,519 --> 00:11:56,560
mentioned in the agenda

00:11:53,360 --> 00:11:57,360
is regarded to who's doing what whose

00:11:56,560 --> 00:12:01,519
role is it

00:11:57,360 --> 00:12:03,440
to do what and in data it is

00:12:01,519 --> 00:12:05,519
a little more complicated than usual

00:12:03,440 --> 00:12:07,360
let's see why

00:12:05,519 --> 00:12:09,440
okay so the first challenge is who is

00:12:07,360 --> 00:12:12,639
going to give us the requirements

00:12:09,440 --> 00:12:14,560
in regular software application we have

00:12:12,639 --> 00:12:16,560
a product manager that works together

00:12:14,560 --> 00:12:19,120
with an r d team

00:12:16,560 --> 00:12:20,800
using agile methodologies and it is the

00:12:19,120 --> 00:12:21,920
role of the product manager to bring the

00:12:20,800 --> 00:12:23,760
requirements

00:12:21,920 --> 00:12:25,279
and to make sure that the fit between

00:12:23,760 --> 00:12:28,800
the business the customer

00:12:25,279 --> 00:12:30,880
and the technology is actually met

00:12:28,800 --> 00:12:32,720
data applications more often than not

00:12:30,880 --> 00:12:35,360
are internal applications

00:12:32,720 --> 00:12:37,279
that serve the organization and the

00:12:35,360 --> 00:12:39,839
stakeholders within the organization who

00:12:37,279 --> 00:12:43,279
needs insights about the business

00:12:39,839 --> 00:12:45,360
in that case there is no product manager

00:12:43,279 --> 00:12:47,440
or it's not intuitive to some

00:12:45,360 --> 00:12:48,720
organizations to put a product manager

00:12:47,440 --> 00:12:50,800
in that position

00:12:48,720 --> 00:12:53,279
and hence we are missing requirements we

00:12:50,800 --> 00:12:55,519
can miss performance requirements

00:12:53,279 --> 00:12:57,920
we can miss accuracy requirements that

00:12:55,519 --> 00:13:00,720
are critical in order to understand

00:12:57,920 --> 00:13:02,240
how to manage the data we don't always

00:13:00,720 --> 00:13:03,920
understand the business logic

00:13:02,240 --> 00:13:05,360
behind the queries that are running in

00:13:03,920 --> 00:13:08,079
the data environments

00:13:05,360 --> 00:13:08,959
or what they mean to the business and we

00:13:08,079 --> 00:13:12,560
don't have

00:13:08,959 --> 00:13:13,839
a budget or a demand for a certain cost

00:13:12,560 --> 00:13:16,399
effectiveness

00:13:13,839 --> 00:13:17,760
how much data does it cost us to get to

00:13:16,399 --> 00:13:19,200
a certain accuracy

00:13:17,760 --> 00:13:21,920
and whether it is something that we want

00:13:19,200 --> 00:13:23,920
to do or money that we want to spend

00:13:21,920 --> 00:13:25,760
so we are kind of in the dark in a lot

00:13:23,920 --> 00:13:26,959
of gray areas taking a lot of

00:13:25,760 --> 00:13:29,839
considerations

00:13:26,959 --> 00:13:31,440
without clarity on their influence on

00:13:29,839 --> 00:13:33,519
the results that we have

00:13:31,440 --> 00:13:36,880
and on the goal that we are meant to

00:13:33,519 --> 00:13:36,880
provide the organization

00:13:36,959 --> 00:13:42,000
another challenge that we have in rules

00:13:39,600 --> 00:13:43,600
is related to a very old and important

00:13:42,000 --> 00:13:46,079
role called a dba

00:13:43,600 --> 00:13:47,600
so once upon a time when our data

00:13:46,079 --> 00:13:50,399
environment was just

00:13:47,600 --> 00:13:51,920
a great postgres database running on one

00:13:50,399 --> 00:13:54,160
machine

00:13:51,920 --> 00:13:56,320
and providing the organization with all

00:13:54,160 --> 00:13:57,920
the data that it needed because data was

00:13:56,320 --> 00:14:01,040
still small

00:13:57,920 --> 00:14:02,240
we had the ops running the machine we

00:14:01,040 --> 00:14:04,160
had the users

00:14:02,240 --> 00:14:05,360
whether they were analysts data

00:14:04,160 --> 00:14:08,880
scientists

00:14:05,360 --> 00:14:11,920
or engineers querying the data

00:14:08,880 --> 00:14:12,639
and we had a dba we had a person whose

00:14:11,920 --> 00:14:14,399
role was

00:14:12,639 --> 00:14:16,880
to make sure the internals of the

00:14:14,399 --> 00:14:20,399
database whether it's indexing

00:14:16,880 --> 00:14:23,839
schema or configuration

00:14:20,399 --> 00:14:27,040
actually optimizes the data querying

00:14:23,839 --> 00:14:29,360
that were provided by the users

00:14:27,040 --> 00:14:31,360
today we are not in the comfort of the

00:14:29,360 --> 00:14:33,760
relational database world

00:14:31,360 --> 00:14:36,240
but rather in a more complex environment

00:14:33,760 --> 00:14:39,519
so as data grew

00:14:36,240 --> 00:14:42,320
we found ourselves managing a lot of

00:14:39,519 --> 00:14:43,440
distributed systems whether it is the

00:14:42,320 --> 00:14:46,079
storage itself

00:14:43,440 --> 00:14:46,639
if you you chose to manage it yourself

00:14:46,079 --> 00:14:49,920
or if

00:14:46,639 --> 00:14:51,600
it although all those um applications

00:14:49,920 --> 00:14:53,680
that i mentioned earlier that run over

00:14:51,600 --> 00:14:56,959
the day star such as spark

00:14:53,680 --> 00:14:57,600
kafka or presto all of those or of

00:14:56,959 --> 00:14:59,920
course

00:14:57,600 --> 00:15:03,199
non-sql databases all of those are

00:14:59,920 --> 00:15:03,199
distributed systems

00:15:04,399 --> 00:15:08,240
that query the data we still have the

00:15:06,320 --> 00:15:10,800
engineers or the users

00:15:08,240 --> 00:15:11,519
that or the data scientists that query

00:15:10,800 --> 00:15:13,440
the data

00:15:11,519 --> 00:15:14,880
we still have the ops running those

00:15:13,440 --> 00:15:17,040
clusters

00:15:14,880 --> 00:15:19,519
but we no longer have that person in the

00:15:17,040 --> 00:15:22,320
middle who understands how

00:15:19,519 --> 00:15:23,519
indexing formats and configuration of

00:15:22,320 --> 00:15:26,639
those environments

00:15:23,519 --> 00:15:29,360
influences the type of queries or

00:15:26,639 --> 00:15:31,360
code that we run within those

00:15:29,360 --> 00:15:33,839
environments

00:15:31,360 --> 00:15:35,839
and this is actually something that we

00:15:33,839 --> 00:15:39,199
fill in as we go along

00:15:35,839 --> 00:15:41,759
so server developers who started

00:15:39,199 --> 00:15:43,040
creating data environments suddenly

00:15:41,759 --> 00:15:46,880
found themselves

00:15:43,040 --> 00:15:50,399
the dbas of a complex

00:15:46,880 --> 00:15:51,360
set of complex systems in other cases

00:15:50,399 --> 00:15:54,000
it's the ops

00:15:51,360 --> 00:15:55,680
people who found themselves needing also

00:15:54,000 --> 00:15:58,320
to become the dba

00:15:55,680 --> 00:15:59,199
of those environments the title for this

00:15:58,320 --> 00:16:02,480
has not yet

00:15:59,199 --> 00:16:05,759
been finalized but in the last

00:16:02,480 --> 00:16:07,040
uh i say a couple of years organizations

00:16:05,759 --> 00:16:08,880
are more aware this

00:16:07,040 --> 00:16:11,279
dysfunction is missing and they are

00:16:08,880 --> 00:16:13,519
trying to build the knowledge

00:16:11,279 --> 00:16:15,920
and to create the capabilities if you

00:16:13,519 --> 00:16:17,920
are a small organization and you're just

00:16:15,920 --> 00:16:20,000
going into this you probably don't have

00:16:17,920 --> 00:16:23,279
the knowledge

00:16:20,000 --> 00:16:24,079
so to sum it up we have a missing link

00:16:23,279 --> 00:16:26,959
here

00:16:24,079 --> 00:16:28,320
with that connects the capabilities or

00:16:26,959 --> 00:16:31,360
the configurations

00:16:28,320 --> 00:16:34,639
of those distributed systems with

00:16:31,360 --> 00:16:35,600
the querying needs of the company right

00:16:34,639 --> 00:16:38,000
now

00:16:35,600 --> 00:16:39,040
if we ask whose role is it in some

00:16:38,000 --> 00:16:41,279
organizations

00:16:39,040 --> 00:16:42,240
data engineers take this role in other

00:16:41,279 --> 00:16:45,279
organizations

00:16:42,240 --> 00:16:48,480
data ops take the role or infrastructure

00:16:45,279 --> 00:16:50,720
data infrastructure teams take the role

00:16:48,480 --> 00:16:52,639
it is very important to just to note

00:16:50,720 --> 00:16:54,480
that someone has to take this role

00:16:52,639 --> 00:16:58,639
because it's very critical

00:16:54,480 --> 00:16:58,639
for the success of a data environment

00:16:58,880 --> 00:17:05,199
so data pipelines are they a pet

00:17:02,399 --> 00:17:05,839
or a cattle it is a pet it is a pet

00:17:05,199 --> 00:17:07,679
because

00:17:05,839 --> 00:17:09,839
there's one machine running this

00:17:07,679 --> 00:17:10,720
database it is a mission critical

00:17:09,839 --> 00:17:12,559
database

00:17:10,720 --> 00:17:14,000
and we nurture it and we make sure it's

00:17:12,559 --> 00:17:16,319
very stable

00:17:14,000 --> 00:17:17,439
on the other hand for example a nosql

00:17:16,319 --> 00:17:20,160
database

00:17:17,439 --> 00:17:22,400
that runs on a cluster is a cattle

00:17:20,160 --> 00:17:25,600
because if one of the machines falls

00:17:22,400 --> 00:17:28,240
we simply replace it by another one and

00:17:25,600 --> 00:17:29,760
until we do that the cluster knows how

00:17:28,240 --> 00:17:32,960
to handle

00:17:29,760 --> 00:17:35,280
itself without the missing machine

00:17:32,960 --> 00:17:36,480
so let's look at this data pipeline

00:17:35,280 --> 00:17:40,080
example

00:17:36,480 --> 00:17:43,120
if we talk about kafka or spark

00:17:40,080 --> 00:17:45,520
or in many cases the analytical database

00:17:43,120 --> 00:17:48,640
itself and of course the storage

00:17:45,520 --> 00:17:51,039
those are cattles those are environments

00:17:48,640 --> 00:17:53,120
that are distributed systems with the

00:17:51,039 --> 00:17:55,840
capabilities of a cattle

00:17:53,120 --> 00:17:58,320
but when we want to run another version

00:17:55,840 --> 00:18:00,320
of our production data environment

00:17:58,320 --> 00:18:03,200
we're not looking at a machine that is

00:18:00,320 --> 00:18:05,760
running part of the kafka cluster

00:18:03,200 --> 00:18:07,679
we are actually looking at duplicating

00:18:05,760 --> 00:18:10,320
this entire pipeline

00:18:07,679 --> 00:18:12,320
and running it in parallel what we want

00:18:10,320 --> 00:18:16,080
to have is the ability to spin

00:18:12,320 --> 00:18:18,160
up the entire pipeline run an experiment

00:18:16,080 --> 00:18:23,039
on some of the code

00:18:18,160 --> 00:18:25,760
and hence we need to spin up a variety

00:18:23,039 --> 00:18:27,360
of distributed systems either managed by

00:18:25,760 --> 00:18:29,840
us or managed by others

00:18:27,360 --> 00:18:31,679
and we want to do that efficiently so

00:18:29,840 --> 00:18:34,640
this is an ops challenge

00:18:31,679 --> 00:18:35,840
that you all know but it does grow and

00:18:34,640 --> 00:18:38,880
becomes

00:18:35,840 --> 00:18:39,200
more complex because the systems that

00:18:38,880 --> 00:18:40,880
are

00:18:39,200 --> 00:18:43,760
actually creating our production

00:18:40,880 --> 00:18:46,320
pipeline are complex systems

00:18:43,760 --> 00:18:47,760
but this is not the only complexity that

00:18:46,320 --> 00:18:50,640
we are adding

00:18:47,760 --> 00:18:52,080
to the world we also need to manage the

00:18:50,640 --> 00:18:55,520
data

00:18:52,080 --> 00:18:56,000
and data also have the characteristic of

00:18:55,520 --> 00:18:59,600
being

00:18:56,000 --> 00:19:01,280
production data so where do we hold this

00:18:59,600 --> 00:19:03,440
production data we either

00:19:01,280 --> 00:19:06,080
hold it in an object storage this is

00:19:03,440 --> 00:19:08,960
what modern architectures do

00:19:06,080 --> 00:19:10,000
due to the cost effectiveness and high

00:19:08,960 --> 00:19:13,120
throughput

00:19:10,000 --> 00:19:16,000
of object storages but definitely we can

00:19:13,120 --> 00:19:18,240
find the need and save the data in a

00:19:16,000 --> 00:19:20,559
database a lot of organizations save the

00:19:18,240 --> 00:19:21,760
data in several places according to the

00:19:20,559 --> 00:19:24,240
need

00:19:21,760 --> 00:19:26,240
and the production data is the data

00:19:24,240 --> 00:19:29,840
which is the single source of truth

00:19:26,240 --> 00:19:33,520
that we want people to consume and trust

00:19:29,840 --> 00:19:36,799
and hence we guard it the way we guard

00:19:33,520 --> 00:19:38,080
any other component of production and it

00:19:36,799 --> 00:19:41,200
means that if we want

00:19:38,080 --> 00:19:42,400
to duplicate our data pipelines we also

00:19:41,200 --> 00:19:45,840
need a way

00:19:42,400 --> 00:19:48,320
to duplicate not necessarily physically

00:19:45,840 --> 00:19:49,760
the data that is running over it in the

00:19:48,320 --> 00:19:51,919
world of databases

00:19:49,760 --> 00:19:52,960
this is solved by creating a replication

00:19:51,919 --> 00:19:55,919
of the database

00:19:52,960 --> 00:19:57,120
we are actually physically replicating

00:19:55,919 --> 00:19:59,760
the data

00:19:57,120 --> 00:20:02,080
in the world of object storages we can

00:19:59,760 --> 00:20:04,240
also do that replication

00:20:02,080 --> 00:20:05,120
but because object storages don't

00:20:04,240 --> 00:20:08,080
provide

00:20:05,120 --> 00:20:09,919
the guarantees given by databases this

00:20:08,080 --> 00:20:12,400
could be a bit dangerous

00:20:09,919 --> 00:20:13,120
so let's dive into the object storages

00:20:12,400 --> 00:20:16,159
because they

00:20:13,120 --> 00:20:17,520
are a main component in every data

00:20:16,159 --> 00:20:19,600
architecture

00:20:17,520 --> 00:20:21,600
and understand although i highly

00:20:19,600 --> 00:20:22,400
recommend using of course in object

00:20:21,600 --> 00:20:24,960
storage

00:20:22,400 --> 00:20:26,480
what we need to be careful of so there

00:20:24,960 --> 00:20:28,960
is low visibility

00:20:26,480 --> 00:20:30,400
and manual management of this

00:20:28,960 --> 00:20:34,159
environment

00:20:30,400 --> 00:20:37,440
it doesn't help you efficiently manage

00:20:34,159 --> 00:20:38,720
or create resilient work because it's a

00:20:37,440 --> 00:20:41,360
shared folder

00:20:38,720 --> 00:20:43,840
in the logical management of it it is

00:20:41,360 --> 00:20:44,159
immutable it means we can replace data

00:20:43,840 --> 00:20:47,600
by

00:20:44,159 --> 00:20:51,280
other data or a file by another file

00:20:47,600 --> 00:20:53,200
but not necessarily change it

00:20:51,280 --> 00:20:55,520
there is no transactionality there's no

00:20:53,200 --> 00:20:56,720
guarantee of transactionality which goes

00:20:55,520 --> 00:21:00,080
very well with

00:20:56,720 --> 00:21:02,480
no cross-collection consistency and

00:21:00,080 --> 00:21:03,760
hence if we want isolation if we want to

00:21:02,480 --> 00:21:06,159
be working on our

00:21:03,760 --> 00:21:08,559
own copy of the data or we want a

00:21:06,159 --> 00:21:11,760
duplication of our data pipeline

00:21:08,559 --> 00:21:12,400
to be working on its own copy of the

00:21:11,760 --> 00:21:15,280
data

00:21:12,400 --> 00:21:16,960
we need to copy the data but if you copy

00:21:15,280 --> 00:21:20,080
and create a lot of copies

00:21:16,960 --> 00:21:22,640
in a manual manager managed

00:21:20,080 --> 00:21:26,000
environment clearly this is an

00:21:22,640 --> 00:21:29,120
error-prone situation

00:21:26,000 --> 00:21:31,200
and there is a lot of attention to that

00:21:29,120 --> 00:21:34,320
fact in the last

00:21:31,200 --> 00:21:36,080
few years or a growing attention

00:21:34,320 --> 00:21:37,679
part of the reason this is getting more

00:21:36,080 --> 00:21:41,200
attention now is that

00:21:37,679 --> 00:21:45,120
problems like performance and

00:21:41,200 --> 00:21:48,559
throughput of data infrastructure

00:21:45,120 --> 00:21:51,520
were solved or are sufficient to most

00:21:48,559 --> 00:21:51,919
organizations and manageability became

00:21:51,520 --> 00:21:54,480
now

00:21:51,919 --> 00:21:56,840
the biggest challenge so what we could

00:21:54,480 --> 00:21:58,880
see on the left-hand side is a set of

00:21:56,840 --> 00:22:02,480
tools

00:21:58,880 --> 00:22:04,400
for envelopes or in the ml ops world

00:22:02,480 --> 00:22:05,840
that provides solutions for people who

00:22:04,400 --> 00:22:09,840
actually manage

00:22:05,840 --> 00:22:12,240
and create ml models so we're looking at

00:22:09,840 --> 00:22:15,200
one application that allows

00:22:12,240 --> 00:22:16,559
replication of pipelines and versioning

00:22:15,200 --> 00:22:19,520
of data

00:22:16,559 --> 00:22:20,720
to allow efficient work for data

00:22:19,520 --> 00:22:25,280
scientists

00:22:20,720 --> 00:22:27,600
who build ml models

00:22:25,280 --> 00:22:28,559
so that is nice but we do with the data

00:22:27,600 --> 00:22:32,880
a lot more than

00:22:28,559 --> 00:22:34,000
just ml models so this solves only part

00:22:32,880 --> 00:22:38,159
of the problem

00:22:34,000 --> 00:22:41,600
what we see on the right hand side are

00:22:38,159 --> 00:22:44,720
solutions that aim towards

00:22:41,600 --> 00:22:45,760
giving the data itself characteristics

00:22:44,720 --> 00:22:47,760
that would

00:22:45,760 --> 00:22:49,520
help manage it no matter what the

00:22:47,760 --> 00:22:51,840
application is

00:22:49,520 --> 00:22:54,720
on the top we see data formats that run

00:22:51,840 --> 00:22:56,400
within the object storage and allows the

00:22:54,720 --> 00:23:00,080
object storage

00:22:56,400 --> 00:23:02,720
to provide mutability

00:23:00,080 --> 00:23:04,559
through those formats and also time

00:23:02,720 --> 00:23:07,039
travel within

00:23:04,559 --> 00:23:08,080
a table which is helpful for

00:23:07,039 --> 00:23:11,360
manageability

00:23:08,080 --> 00:23:11,360
and for um

00:23:12,720 --> 00:23:19,840
recovering from errors and

00:23:16,720 --> 00:23:20,720
on the bottom we see tools that actually

00:23:19,840 --> 00:23:23,039
allow you

00:23:20,720 --> 00:23:23,840
git like operations over your object

00:23:23,039 --> 00:23:27,039
storage

00:23:23,840 --> 00:23:30,720
so if now you have several

00:23:27,039 --> 00:23:34,000
versions or you can spin up another

00:23:30,720 --> 00:23:36,960
data pipeline from an ops perspective

00:23:34,000 --> 00:23:37,280
you can simply open a branch of the data

00:23:36,960 --> 00:23:40,559
and

00:23:37,280 --> 00:23:41,679
run this pipeline over a branch of the

00:23:40,559 --> 00:23:46,480
data

00:23:41,679 --> 00:23:49,600
providing a full isolated

00:23:46,480 --> 00:23:53,200
data pipeline for experimentation

00:23:49,600 --> 00:23:58,320
so this helps solve both parts

00:23:53,200 --> 00:23:58,320
of the puzzle to sum it up

00:23:58,640 --> 00:24:03,200
so data ops is actually devops for data

00:24:01,520 --> 00:24:05,840
and as we said it is

00:24:03,200 --> 00:24:06,720
more complex because of the aspect of

00:24:05,840 --> 00:24:09,279
the data

00:24:06,720 --> 00:24:09,919
that not only brings complexity by

00:24:09,279 --> 00:24:12,960
itself

00:24:09,919 --> 00:24:16,000
but also makes other aspects complex

00:24:12,960 --> 00:24:18,000
of course if you have big data on the

00:24:16,000 --> 00:24:20,159
upside we see a lot of work being

00:24:18,000 --> 00:24:21,200
done in the last 10 years to improve the

00:24:20,159 --> 00:24:24,960
situation

00:24:21,200 --> 00:24:28,240
including very strong players that bring

00:24:24,960 --> 00:24:31,919
viable solutions and on the

00:24:28,240 --> 00:24:32,559
data aspect there are now emerging tools

00:24:31,919 --> 00:24:36,000
that

00:24:32,559 --> 00:24:38,400
are helping you build resilient data

00:24:36,000 --> 00:24:40,240
pipelines and duplicate data pipelines

00:24:38,400 --> 00:24:42,640
for experimentation

00:24:40,240 --> 00:24:44,799
lakefs the projects that are the project

00:24:42,640 --> 00:24:48,000
that i am a co-creator of

00:24:44,799 --> 00:24:50,480
is one of those examples so check it out

00:24:48,000 --> 00:24:51,600
show us some love and give us a github

00:24:50,480 --> 00:25:01,840
star

00:24:51,600 --> 00:25:01,840
thank you very much

00:25:04,080 --> 00:25:08,640
thank you very much that was a really

00:25:06,400 --> 00:25:11,679
great uh

00:25:08,640 --> 00:25:15,679
session super interesting super exciting

00:25:11,679 --> 00:25:18,880
so what happened to the dbas

00:25:15,679 --> 00:25:23,440
where have they gone uh

00:25:18,880 --> 00:25:26,080
some of them are still out there hiding

00:25:23,440 --> 00:25:27,200
and helping organizations uh close the

00:25:26,080 --> 00:25:30,240
gap

00:25:27,200 --> 00:25:32,880
uh but they had to so uh of course

00:25:30,240 --> 00:25:33,360
in on relational databases in uh when

00:25:32,880 --> 00:25:35,279
they are

00:25:33,360 --> 00:25:36,400
widely used there are still dbas out

00:25:35,279 --> 00:25:38,720
there

00:25:36,400 --> 00:25:40,080
um and some of them have updated their

00:25:38,720 --> 00:25:42,000
capabilities to

00:25:40,080 --> 00:25:43,600
the distributed systems and then they

00:25:42,000 --> 00:25:44,960
are the ones closing the gap i was

00:25:43,600 --> 00:25:47,200
talking about

00:25:44,960 --> 00:25:48,240
but a lot of organizations uh who

00:25:47,200 --> 00:25:50,400
started uh

00:25:48,240 --> 00:25:52,080
without the knowledge because when you

00:25:50,400 --> 00:25:52,799
work with open source and at some point

00:25:52,080 --> 00:25:56,400
it became

00:25:52,799 --> 00:25:57,520
um an inspiration to for the developers

00:25:56,400 --> 00:26:00,880
to be the ops

00:25:57,520 --> 00:26:02,480
themselves so you just install spark and

00:26:00,880 --> 00:26:02,960
you start running it and it's really

00:26:02,480 --> 00:26:05,279
easy

00:26:02,960 --> 00:26:06,400
not really easy it's not easy at all but

00:26:05,279 --> 00:26:08,640
and you do it yourself

00:26:06,400 --> 00:26:10,880
and you're not aware of the gap and as

00:26:08,640 --> 00:26:14,880
you use it more and more the complexity

00:26:10,880 --> 00:26:16,960
uh hits you and uh then you realize

00:26:14,880 --> 00:26:18,799
that there are missing pieces between

00:26:16,960 --> 00:26:20,799
the capabilities of whoever you are that

00:26:18,799 --> 00:26:23,279
decided to start using it

00:26:20,799 --> 00:26:25,360
and the needs of the organization so i

00:26:23,279 --> 00:26:28,159
think it's kind of a fashion

00:26:25,360 --> 00:26:29,760
not to have a dba not the thought

00:26:28,159 --> 00:26:33,200
through a decision let's just

00:26:29,760 --> 00:26:33,760
kill the profession so you think there

00:26:33,200 --> 00:26:36,960
will be

00:26:33,760 --> 00:26:38,240
a sort of replacement for them or like

00:26:36,960 --> 00:26:40,960
the new version the next

00:26:38,240 --> 00:26:42,480
version of of of the dba something

00:26:40,960 --> 00:26:44,400
similar to what happened with with

00:26:42,480 --> 00:26:46,000
sysadmin role right that sort of

00:26:44,400 --> 00:26:48,720
transmitted transformed

00:26:46,000 --> 00:26:50,880
into devops which is a little different

00:26:48,720 --> 00:26:52,480
maybe a bit more complex practice

00:26:50,880 --> 00:26:53,919
so you think something is similar would

00:26:52,480 --> 00:26:56,960
happen

00:26:53,919 --> 00:26:59,840
in the data ops space

00:26:56,960 --> 00:27:01,760
so um yeah i'm pretty sure and it really

00:26:59,840 --> 00:27:03,440
depends on the paradigm that you choose

00:27:01,760 --> 00:27:04,559
for managing data so there's a new

00:27:03,440 --> 00:27:06,080
paradigm that is

00:27:04,559 --> 00:27:08,400
has a lot of talk around it called the

00:27:06,080 --> 00:27:10,320
data mesh so if you look at the data

00:27:08,400 --> 00:27:11,039
mesh concept you would be talking about

00:27:10,320 --> 00:27:13,360
having

00:27:11,039 --> 00:27:16,720
the ops team or the infrastructure team

00:27:13,360 --> 00:27:18,960
providing only a self-serving

00:27:16,720 --> 00:27:21,039
infrastructure while the knowledge about

00:27:18,960 --> 00:27:24,159
the data and hence the dba

00:27:21,039 --> 00:27:27,919
of the data would be done by the

00:27:24,159 --> 00:27:30,000
data engineers and our other paradigms

00:27:27,919 --> 00:27:31,679
are actually building data

00:27:30,000 --> 00:27:34,240
infrastructure teams

00:27:31,679 --> 00:27:34,960
that include not only the infrastructure

00:27:34,240 --> 00:27:38,000
itself but

00:27:34,960 --> 00:27:39,919
also the knowledge of the um

00:27:38,000 --> 00:27:41,440
of the data structuring and actually the

00:27:39,919 --> 00:27:43,200
dba rule

00:27:41,440 --> 00:27:45,279
which requires from those people to have

00:27:43,200 --> 00:27:48,720
a very strong business context

00:27:45,279 --> 00:27:49,600
so you have ops people with uh business

00:27:48,720 --> 00:27:52,480
context

00:27:49,600 --> 00:27:54,480
it's it's um it's a very unique

00:27:52,480 --> 00:27:55,520
requirement but it does exist in some

00:27:54,480 --> 00:27:58,960
organizations

00:27:55,520 --> 00:28:02,080
definitely definitely makes sense so

00:27:58,960 --> 00:28:03,120
let's touch a bit about data and data

00:28:02,080 --> 00:28:04,720
versioning so

00:28:03,120 --> 00:28:06,159
right right many many organizations are

00:28:04,720 --> 00:28:07,279
doing data versioning within the

00:28:06,159 --> 00:28:10,480
application itself

00:28:07,279 --> 00:28:14,159
right so how how do you see that

00:28:10,480 --> 00:28:14,159
that been happening in moving forward

00:28:15,440 --> 00:28:18,880
so the data is an asset of the entire

00:28:17,279 --> 00:28:21,840
organization and as you

00:28:18,880 --> 00:28:23,840
evolve a lot of data sets that were once

00:28:21,840 --> 00:28:26,640
used only by one application

00:28:23,840 --> 00:28:29,039
for analysis are being used for many

00:28:26,640 --> 00:28:31,679
applications in different ways

00:28:29,039 --> 00:28:33,919
so you should be pushing down the data

00:28:31,679 --> 00:28:35,840
versioning to an infrastructure level to

00:28:33,919 --> 00:28:36,960
something that is horizontal and covers

00:28:35,840 --> 00:28:38,559
the data

00:28:36,960 --> 00:28:40,480
rather than having it as something

00:28:38,559 --> 00:28:41,520
vertical that is used only by one

00:28:40,480 --> 00:28:43,279
application

00:28:41,520 --> 00:28:44,960
because if then you have you use another

00:28:43,279 --> 00:28:46,320
application whether it offers data

00:28:44,960 --> 00:28:49,919
versioning or not

00:28:46,320 --> 00:28:51,200
you cannot be efficiently communicating

00:28:49,919 --> 00:28:52,159
with the versioning of another

00:28:51,200 --> 00:28:54,000
application

00:28:52,159 --> 00:28:56,399
the language is different the api is

00:28:54,000 --> 00:28:57,120
different it would become a mess going

00:28:56,399 --> 00:28:58,880
forward

00:28:57,120 --> 00:29:00,320
but if the versioning is the quality of

00:28:58,880 --> 00:29:03,200
the data itself

00:29:00,320 --> 00:29:05,279
then it's very easy to use it on all

00:29:03,200 --> 00:29:08,159
applications which is what's in the last

00:29:05,279 --> 00:29:11,039
slide was the right-hand side

00:29:08,159 --> 00:29:13,520
formats and lake effect interesting and

00:29:11,039 --> 00:29:16,159
so you decided to make it versus

00:29:13,520 --> 00:29:17,679
traversing am i saying that correctly

00:29:16,159 --> 00:29:21,520
traverse

00:29:17,679 --> 00:29:23,840
traverse thank you it's a tree

00:29:21,520 --> 00:29:24,960
it's a tree because git is like miracle

00:29:23,840 --> 00:29:26,799
trees so we said

00:29:24,960 --> 00:29:28,960
tree and then we thought about

00:29:26,799 --> 00:29:29,679
traversing a tree you know you go over a

00:29:28,960 --> 00:29:31,360
tree

00:29:29,679 --> 00:29:33,360
and then it becomes reverse and there

00:29:31,360 --> 00:29:34,880
was no going back there

00:29:33,360 --> 00:29:36,720
so you decided to make it an open source

00:29:34,880 --> 00:29:38,399
project like how did that

00:29:36,720 --> 00:29:40,000
come about obviously as a startup it's

00:29:38,399 --> 00:29:40,799
it's really hard to make that decision i

00:29:40,000 --> 00:29:43,600
guess

00:29:40,799 --> 00:29:45,520
so super interested in hearing the

00:29:43,600 --> 00:29:49,440
thoughts and the process behind that

00:29:45,520 --> 00:29:51,840
yeah so so laker fest

00:29:49,440 --> 00:29:53,679
is the the project itself is called lake

00:29:51,840 --> 00:29:55,520
fest and it's it's open source

00:29:53,679 --> 00:29:58,080
because the ecosystem is open source

00:29:55,520 --> 00:30:00,399
when you look at big data infrastructure

00:29:58,080 --> 00:30:02,640
it's open source and the adoption of big

00:30:00,399 --> 00:30:05,919
data infrastructure is usually done by

00:30:02,640 --> 00:30:07,360
uh open source uh users and open source

00:30:05,919 --> 00:30:09,039
communities because

00:30:07,360 --> 00:30:10,880
you need first to have the data

00:30:09,039 --> 00:30:12,240
developers believe in the product and

00:30:10,880 --> 00:30:14,399
help you shape it

00:30:12,240 --> 00:30:15,520
and then you get the right product out

00:30:14,399 --> 00:30:17,279
there

00:30:15,520 --> 00:30:19,440
and then the business model is what is

00:30:17,279 --> 00:30:21,679
called an open core model where

00:30:19,440 --> 00:30:23,600
over this open source which will always

00:30:21,679 --> 00:30:26,240
be an open source in our case

00:30:23,600 --> 00:30:27,919
we think of lakefs as git so git will

00:30:26,240 --> 00:30:30,720
always be open source

00:30:27,919 --> 00:30:33,440
and we will be building over it a github

00:30:30,720 --> 00:30:35,440
which would have

00:30:33,440 --> 00:30:37,440
more enterprise features and

00:30:35,440 --> 00:30:40,559
manageability features over it

00:30:37,440 --> 00:30:41,360
uh that would be uh paid but laker fest

00:30:40,559 --> 00:30:43,120
as it is and

00:30:41,360 --> 00:30:44,880
a lot more that we're going to add to it

00:30:43,120 --> 00:30:46,640
is going to stay open source

00:30:44,880 --> 00:30:48,880
that would give us the penetration the

00:30:46,640 --> 00:30:51,200
adoption and of course the

00:30:48,880 --> 00:30:52,720
the great feedback and assistance of a

00:30:51,200 --> 00:30:55,840
community which is the

00:30:52,720 --> 00:30:58,880
the most important leverage you get from

00:30:55,840 --> 00:30:58,880
having open source

00:30:59,519 --> 00:31:03,120
and obviously that's a downloadable

00:31:02,320 --> 00:31:05,600
software

00:31:03,120 --> 00:31:06,640
and as we move forward obviously there

00:31:05,600 --> 00:31:08,720
are

00:31:06,640 --> 00:31:10,159
many as a service products that you can

00:31:08,720 --> 00:31:11,919
use out there so how do you see this

00:31:10,159 --> 00:31:14,640
space being involved you see like

00:31:11,919 --> 00:31:16,000
the data offset people they're mostly

00:31:14,640 --> 00:31:18,159
using something that they would be

00:31:16,000 --> 00:31:20,320
deploying and owning or do you see that

00:31:18,159 --> 00:31:22,159
going forward with a bunch of

00:31:20,320 --> 00:31:24,080
cloud services that are being available

00:31:22,159 --> 00:31:27,039
to them and they will be using them

00:31:24,080 --> 00:31:29,120
how do you see this space being evolved

00:31:27,039 --> 00:31:32,240
so it really depends on the industry

00:31:29,120 --> 00:31:34,240
um if you are a cloud user then yes you

00:31:32,240 --> 00:31:35,600
would probably be shifting to the cloud

00:31:34,240 --> 00:31:39,679
services

00:31:35,600 --> 00:31:42,000
because the um the overhead in cost

00:31:39,679 --> 00:31:43,279
is uh not big if you consider the work

00:31:42,000 --> 00:31:45,120
that you need to do

00:31:43,279 --> 00:31:47,360
if you compare it to the world the cost

00:31:45,120 --> 00:31:50,640
of people who need to actually

00:31:47,360 --> 00:31:52,480
uh run the systems locally if if or on

00:31:50,640 --> 00:31:53,440
the cloud instead of using the cloud

00:31:52,480 --> 00:31:56,000
services

00:31:53,440 --> 00:31:57,919
so yes i think uh most organizations

00:31:56,000 --> 00:31:59,360
using cloud would be moving to the cloud

00:31:57,919 --> 00:32:01,760
services

00:31:59,360 --> 00:32:03,039
still because cloud services offers so

00:32:01,760 --> 00:32:06,559
many options

00:32:03,039 --> 00:32:08,960
that overlap in so many ways so

00:32:06,559 --> 00:32:10,080
uh choosing the right the right set of

00:32:08,960 --> 00:32:13,039
tools and then

00:32:10,080 --> 00:32:14,320
managing these tools synchronized would

00:32:13,039 --> 00:32:16,880
still be a diverse

00:32:14,320 --> 00:32:17,519
devops effort that people would have to

00:32:16,880 --> 00:32:20,000
that

00:32:17,519 --> 00:32:21,760
data ops people would have to deal with

00:32:20,000 --> 00:32:23,120
and i don't suspect it's going to be

00:32:21,760 --> 00:32:24,720
easy i talked a little bit within the

00:32:23,120 --> 00:32:27,760
presentation about

00:32:24,720 --> 00:32:29,679
uh the different forces

00:32:27,760 --> 00:32:30,960
pulling this industry in different

00:32:29,679 --> 00:32:33,519
directions

00:32:30,960 --> 00:32:35,200
and this would still cause uh a

00:32:33,519 --> 00:32:38,880
challenge for anyone who wants to build

00:32:35,200 --> 00:32:41,279
an optimal system for their needs

00:32:38,880 --> 00:32:42,399
yeah and so you talked a little bit

00:32:41,279 --> 00:32:44,480
about costs

00:32:42,399 --> 00:32:45,600
can you share any like war story about

00:32:44,480 --> 00:32:48,480
costs and managing

00:32:45,600 --> 00:32:48,480
data at scale

00:32:48,880 --> 00:32:52,880
um i i guess that was just uh we were

00:32:52,080 --> 00:32:55,440
hosted on

00:32:52,880 --> 00:32:56,720
aws so i think that would just be

00:32:55,440 --> 00:33:00,320
stories about uh

00:32:56,720 --> 00:33:01,919
cost visibility on aws where

00:33:00,320 --> 00:33:03,679
if you're not very careful and you have

00:33:01,919 --> 00:33:04,559
not targeted everything and you don't

00:33:03,679 --> 00:33:09,360
have the right

00:33:04,559 --> 00:33:10,240
um i'd say standardization within the

00:33:09,360 --> 00:33:13,440
organization

00:33:10,240 --> 00:33:15,120
to constantly track your cost

00:33:13,440 --> 00:33:17,279
when you are running so large

00:33:15,120 --> 00:33:20,640
distributed systems

00:33:17,279 --> 00:33:23,279
uh then you would be hit by stop

00:33:20,640 --> 00:33:24,159
staying up that you didn't know of and

00:33:23,279 --> 00:33:27,600
it could cost

00:33:24,159 --> 00:33:30,480
a lot of money or even s3 costs that

00:33:27,600 --> 00:33:31,519
just although s3 is very cheap if you

00:33:30,480 --> 00:33:33,679
have a lot of data

00:33:31,519 --> 00:33:35,279
it is not cheap anymore and if data is

00:33:33,679 --> 00:33:36,799
constantly duplicated you would be

00:33:35,279 --> 00:33:37,840
finding yourself paying for those

00:33:36,799 --> 00:33:39,440
duplications

00:33:37,840 --> 00:33:41,039
and suddenly it's hundreds of thousands

00:33:39,440 --> 00:33:42,640
of dollars a year

00:33:41,039 --> 00:33:45,039
because of the mess that you were

00:33:42,640 --> 00:33:47,679
running over your data

00:33:45,039 --> 00:33:49,600
but uh i guess if you have visibility

00:33:47,679 --> 00:33:51,440
this is solved like a festival some of

00:33:49,600 --> 00:33:52,960
that by providing visibility over the

00:33:51,440 --> 00:33:56,640
data

00:33:52,960 --> 00:33:58,399
and over the compute it's for aws to

00:33:56,640 --> 00:33:59,440
provide this visibility and ease of

00:33:58,399 --> 00:34:00,799
management

00:33:59,440 --> 00:34:02,960
i think they're doing better than they

00:34:00,799 --> 00:34:04,399
did five years ago and should probably

00:34:02,960 --> 00:34:06,880
be doing much better

00:34:04,399 --> 00:34:07,440
about other cloud providers i i know

00:34:06,880 --> 00:34:09,760
less

00:34:07,440 --> 00:34:11,599
of how it's managed but i suspect it's

00:34:09,760 --> 00:34:13,200
similar

00:34:11,599 --> 00:34:15,440
yeah yeah i've heard so many worse

00:34:13,200 --> 00:34:17,359
stories of companies going and standing

00:34:15,440 --> 00:34:18,079
up new cloud projects and then suddenly

00:34:17,359 --> 00:34:20,000
receiving

00:34:18,079 --> 00:34:21,839
the bill for the first month being like

00:34:20,000 --> 00:34:23,839
so surprised

00:34:21,839 --> 00:34:26,079
this story so many times you need to

00:34:23,839 --> 00:34:29,359
establish standardization over

00:34:26,079 --> 00:34:32,560
cost tracking day one yeah

00:34:29,359 --> 00:34:34,399
yeah definitely all right enough thank

00:34:32,560 --> 00:34:36,320
you very much this has been super

00:34:34,399 --> 00:34:40,560
interesting thank you very much

00:34:36,320 --> 00:34:40,560
for the interesting talk and

00:34:43,800 --> 00:34:46,800

YouTube URL: https://www.youtube.com/watch?v=vUYj4mz2H34


