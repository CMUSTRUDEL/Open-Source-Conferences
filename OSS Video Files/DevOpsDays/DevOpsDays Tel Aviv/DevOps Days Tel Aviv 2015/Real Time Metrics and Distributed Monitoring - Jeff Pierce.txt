Title: Real Time Metrics and Distributed Monitoring - Jeff Pierce
Publication date: 2015-11-01
Playlist: DevOps Days Tel Aviv 2015
Description: 
	http://www.devopsdays.org/events/2015-telaviv/

Implementing the data store for the metrics, the ability to collect them, and the ability to retrieve them (Cassandra, collectd, statsd, cyanite, graphite-api, Grafana, cyanite-proxy). Implementing a distributed monitoring system -- get notified if something breaks without relying on a centralized system.

Presenting language plugins for statsd to get developers excited about adding statistics to their code for easier development planning.

Make the JSON dashboards are part of a project's code, so when code is updated, the dashboards are updated as well!

Showing some of Change.org's use cases, along with some of the pitfalls we ran into along the way.

Making monitoring not suck!

About the speaker - Jeff Pierce

Jeff Pierce has been working in systems administration and DevOps for over a decade, and has worked for companies such as Apple and Rackspace and consulted for Citigroup. He's been a part of enterprise organizations and small startups, and has experience scaling solutions for companies of all sizes.
Captions: 
	00:00:10,570 --> 00:00:16,750
Jeff Pierce which recently in addition

00:00:14,410 --> 00:00:19,810
to recently combining the powers of NGO

00:00:16,750 --> 00:00:22,960
radius and Cassandra for monitoring

00:00:19,810 --> 00:00:25,179
allergy to check his a get up page he is

00:00:22,960 --> 00:00:29,230
also a Technomancer do you know what

00:00:25,179 --> 00:00:30,939
techno Mansi is Technomancer so I look

00:00:29,230 --> 00:00:33,550
this one up in Wikipedia

00:00:30,939 --> 00:00:35,670
so it's techno Mansi any sufficiently

00:00:33,550 --> 00:00:38,670
advanced technology is indistinguishable

00:00:35,670 --> 00:00:41,260
from magic powers hmm

00:00:38,670 --> 00:00:43,840
specific uses of Technology powers

00:00:41,260 --> 00:00:48,220
include causing devices to malfunction

00:00:43,840 --> 00:00:50,710
and traveling to a cyber world so

00:00:48,220 --> 00:00:52,870
traveled from the barrier Bay Area San

00:00:50,710 --> 00:01:01,690
Francisco to Tel Aviv please welcome

00:00:52,870 --> 00:01:03,190
Jeff Pierce I mean it would be good

00:01:01,690 --> 00:01:06,759
DevOps engineer after all if I wasn't

00:01:03,190 --> 00:01:09,729
breaking things on a constant basis so

00:01:06,759 --> 00:01:13,020
hi I'm Jeff Pierce I'm from change.org

00:01:09,729 --> 00:01:15,820
and let me figure out my mouse here

00:01:13,020 --> 00:01:18,549
alright and my talk today is gonna be on

00:01:15,820 --> 00:01:19,899
how I got a real-time metrics in

00:01:18,549 --> 00:01:22,299
distributed monitoring system in place

00:01:19,899 --> 00:01:24,429
and what some of the pitfalls that we

00:01:22,299 --> 00:01:26,560
hit in our specific use case and also

00:01:24,429 --> 00:01:32,170
how to drive adoption of it among the

00:01:26,560 --> 00:01:34,270
different development teams so a little

00:01:32,170 --> 00:01:35,439
bit about me I am a senior DevOps

00:01:34,270 --> 00:01:38,140
engineer change.org

00:01:35,439 --> 00:01:39,850
that's my email address my github and my

00:01:38,140 --> 00:01:43,450
twitter handle please feel free to reach

00:01:39,850 --> 00:01:46,899
out I also also especially appreciate

00:01:43,450 --> 00:01:49,659
feedback on the talk a little more about

00:01:46,899 --> 00:01:52,780
me that is an official company headshot

00:01:49,659 --> 00:01:54,219
by the way I consulted for Citigroup on

00:01:52,780 --> 00:01:57,249
their high frequency trading servers I

00:01:54,219 --> 00:01:58,990
had stints at Apple and Rackspace and I

00:01:57,249 --> 00:02:01,270
am the project leader on Casa bong which

00:01:58,990 --> 00:02:05,170
is the tool that was described during my

00:02:01,270 --> 00:02:10,030
introduction so a little bit of

00:02:05,170 --> 00:02:13,150
background on our situation at

00:02:10,030 --> 00:02:13,930
change.org we are a global platform

00:02:13,150 --> 00:02:16,269
where people can

00:02:13,930 --> 00:02:17,890
start and win campaigns for change we

00:02:16,269 --> 00:02:19,540
want to give people the opportunity to

00:02:17,890 --> 00:02:22,689
make the change and become the change

00:02:19,540 --> 00:02:24,879
that they want to see in the world we

00:02:22,689 --> 00:02:26,560
have 120 million users worldwide it

00:02:24,879 --> 00:02:29,439
ticked over just about after that

00:02:26,560 --> 00:02:31,239
screenshot got taken and we have a

00:02:29,439 --> 00:02:32,920
rapidly expanding user base and

00:02:31,239 --> 00:02:34,780
engineering team we're hitting the point

00:02:32,920 --> 00:02:37,269
in scale where we want to go from a

00:02:34,780 --> 00:02:40,599
hundred million people to a billion we

00:02:37,269 --> 00:02:42,430
want to ramp up big and fast part of our

00:02:40,599 --> 00:02:44,500
traffic patterns and why we've had a lot

00:02:42,430 --> 00:02:48,010
of trouble with monitoring and metric

00:02:44,500 --> 00:02:50,139
solutions in the past is it's very

00:02:48,010 --> 00:02:52,030
unpredictable if a movement or a

00:02:50,139 --> 00:02:55,239
petition takes off it hits the news

00:02:52,030 --> 00:02:55,569
something like that we go up in a

00:02:55,239 --> 00:02:58,659
heartbeat

00:02:55,569 --> 00:03:01,599
an example is in Italy we had a petition

00:02:58,659 --> 00:03:03,370
recently about things being made in

00:03:01,599 --> 00:03:06,819
Italy that might not actually have been

00:03:03,370 --> 00:03:08,919
made in Italy and on a talk-show someone

00:03:06,819 --> 00:03:11,650
mentioned this petition and we went from

00:03:08,919 --> 00:03:15,340
a slow day to four or five times normal

00:03:11,650 --> 00:03:18,310
traffic immediately like no ramp up its

00:03:15,340 --> 00:03:21,569
the link got posted and then everyone's

00:03:18,310 --> 00:03:21,569
going to the site to sign this petition

00:03:22,349 --> 00:03:27,400
so we went to build something ourselves

00:03:25,569 --> 00:03:31,060
the first thing we ask ourself is well

00:03:27,400 --> 00:03:33,220
do we really have to build it I mean if

00:03:31,060 --> 00:03:35,019
you can use someone else's tools or

00:03:33,220 --> 00:03:38,290
something like that hey let's do it it's

00:03:35,019 --> 00:03:41,409
less work that we have to do but you

00:03:38,290 --> 00:03:43,209
know we just weren't able to find a

00:03:41,409 --> 00:03:44,709
solution in the limited amount of time

00:03:43,209 --> 00:03:46,540
that we had to come up with something

00:03:44,709 --> 00:03:51,310
that was going to meet our needs but we

00:03:46,540 --> 00:03:52,599
definitely tried one of the things was

00:03:51,310 --> 00:03:56,639
you weren't happy with the pricing on

00:03:52,599 --> 00:03:58,629
some especially for auto scaled hosts

00:03:56,639 --> 00:03:59,739
you know we weren't happy with the

00:03:58,629 --> 00:04:01,030
resolution of the stats you were

00:03:59,739 --> 00:04:02,530
capturing and said you know we were

00:04:01,030 --> 00:04:05,560
getting things at 30 seconds or a minute

00:04:02,530 --> 00:04:09,609
or five minutes and when we have the

00:04:05,560 --> 00:04:11,019
extreme spikes that are short term that

00:04:09,609 --> 00:04:16,150
was flattening out a lot of patterns

00:04:11,019 --> 00:04:18,310
that we wanted to see so why do we need

00:04:16,150 --> 00:04:20,259
the wandering distributed and these

00:04:18,310 --> 00:04:22,330
really high resolution metrics like for

00:04:20,259 --> 00:04:24,340
example most of the metrics that were

00:04:22,330 --> 00:04:26,500
capturing in production are coming in

00:04:24,340 --> 00:04:27,670
every six seconds and we have the

00:04:26,500 --> 00:04:30,520
ability to give them

00:04:27,670 --> 00:04:32,200
second while we're stress testing or for

00:04:30,520 --> 00:04:34,210
certain things where we want to be able

00:04:32,200 --> 00:04:38,620
to see things real-time for

00:04:34,210 --> 00:04:40,210
troubleshooting purposes but the reason

00:04:38,620 --> 00:04:42,370
why we need these is we really wanted

00:04:40,210 --> 00:04:45,850
these servers to monitor and provide

00:04:42,370 --> 00:04:47,830
metrics on themselves we want to see

00:04:45,850 --> 00:04:49,510
things as possible a few things as

00:04:47,830 --> 00:04:52,750
possible that were externally or

00:04:49,510 --> 00:04:55,180
centrally monitored because in a cloud

00:04:52,750 --> 00:04:59,620
world a centralized service is asking

00:04:55,180 --> 00:05:03,100
for failure if you're in AWS all of a

00:04:59,620 --> 00:05:08,170
sudden oops that instance went away it

00:05:03,100 --> 00:05:10,030
can happen we really you know as I said

00:05:08,170 --> 00:05:12,790
for us the high resolution metrics are

00:05:10,030 --> 00:05:14,890
just awesome gives us much deeper

00:05:12,790 --> 00:05:17,230
troubleshooting ability it lets us see

00:05:14,890 --> 00:05:21,160
patterns that weren't visible at the

00:05:17,230 --> 00:05:23,890
lower resolutions as I mentioned and it

00:05:21,160 --> 00:05:25,780
also lets us you know give a really

00:05:23,890 --> 00:05:28,630
slick presentation like any product even

00:05:25,780 --> 00:05:30,640
aimed internally when you're able to

00:05:28,630 --> 00:05:33,160
show off a great feature it really helps

00:05:30,640 --> 00:05:37,150
pick up internal adoption buy-in from

00:05:33,160 --> 00:05:39,040
management things of that nature it also

00:05:37,150 --> 00:05:41,440
gave us faster response times to outages

00:05:39,040 --> 00:05:43,780
one of the monitoring platforms that we

00:05:41,440 --> 00:05:48,610
were using that we had done externally

00:05:43,780 --> 00:05:50,410
prior to building our own there were

00:05:48,610 --> 00:05:52,540
cases where when we were testing this

00:05:50,410 --> 00:05:54,580
and had it on only a few our products

00:05:52,540 --> 00:05:58,180
there was an issue with it and we knew

00:05:54,580 --> 00:06:00,310
about it within you know 24 seconds of

00:05:58,180 --> 00:06:01,540
it happening it took four tics of our

00:06:00,310 --> 00:06:03,970
monitor system to figure out something

00:06:01,540 --> 00:06:05,850
was up we were already addressing it and

00:06:03,970 --> 00:06:08,440
almost had it fixed by the time our

00:06:05,850 --> 00:06:10,660
previous monitor which was still in

00:06:08,440 --> 00:06:11,830
place on a five minute scale even let us

00:06:10,660 --> 00:06:17,560
know that something was wrong and hit

00:06:11,830 --> 00:06:20,470
the pager and then finally we want to be

00:06:17,560 --> 00:06:23,140
able to auto scale on our own terms we

00:06:20,470 --> 00:06:26,350
want to use the six and metrics that we

00:06:23,140 --> 00:06:29,050
care about you know not just default

00:06:26,350 --> 00:06:30,460
ones like CPU or network traffic

00:06:29,050 --> 00:06:34,030
something like that we want to be able

00:06:30,460 --> 00:06:39,370
to drive that with the metrics that you

00:06:34,030 --> 00:06:41,849
know we care about if you were going

00:06:39,370 --> 00:06:44,020
over our traffic patterns

00:06:41,849 --> 00:06:45,729
sometimes that's going to be how many

00:06:44,020 --> 00:06:47,199
signatures are we getting because that's

00:06:45,729 --> 00:06:49,180
a separate service from just the front

00:06:47,199 --> 00:06:50,740
end just things of that nature we want

00:06:49,180 --> 00:06:55,270
to be able to really have fine-grained

00:06:50,740 --> 00:06:58,689
scaling so what else influenced our

00:06:55,270 --> 00:07:03,569
decision for going with our own setup

00:06:58,689 --> 00:07:05,830
well we were pretty understaffed we were

00:07:03,569 --> 00:07:09,310
getting a lot of engineers in the door

00:07:05,830 --> 00:07:11,229
and in the san francisco markets people

00:07:09,310 --> 00:07:12,879
who can do both systems encode are in

00:07:11,229 --> 00:07:15,729
very high demand and it was tough to get

00:07:12,879 --> 00:07:19,229
them in the door we would interview six

00:07:15,729 --> 00:07:22,599
seven people and give them offers and

00:07:19,229 --> 00:07:24,340
someone showered them with money or they

00:07:22,599 --> 00:07:25,689
wanted a higher profile position was

00:07:24,340 --> 00:07:29,229
tough to get people in the door so it

00:07:25,689 --> 00:07:31,779
was something that with that with that

00:07:29,229 --> 00:07:33,900
rapid growth we needed something that

00:07:31,779 --> 00:07:36,099
you know had a low implementation time

00:07:33,900 --> 00:07:37,719
mer civically it needs to be something

00:07:36,099 --> 00:07:40,900
that we were going to be able to put in

00:07:37,719 --> 00:07:42,960
immediately and it we that rely on the

00:07:40,900 --> 00:07:45,639
knowledge that the team already had

00:07:42,960 --> 00:07:48,729
especially not only does that reduce the

00:07:45,639 --> 00:07:50,500
time from coming up the idea and

00:07:48,729 --> 00:07:51,939
implementing it but again if you're

00:07:50,500 --> 00:07:55,000
using tools that your team is already

00:07:51,939 --> 00:07:57,250
familiar with it's going to drive

00:07:55,000 --> 00:07:59,979
adoption even if there's possibly a

00:07:57,250 --> 00:08:02,560
better tool out there but no one on your

00:07:59,979 --> 00:08:05,110
team knows it you definitely want to

00:08:02,560 --> 00:08:06,699
build your tools around the talent that

00:08:05,110 --> 00:08:08,620
is already on your team what they've

00:08:06,699 --> 00:08:11,680
used what they know and try to make that

00:08:08,620 --> 00:08:13,539
fit before going for something that

00:08:11,680 --> 00:08:14,800
might be a little bit better fit but no

00:08:13,539 --> 00:08:18,729
one knows about and will need to be

00:08:14,800 --> 00:08:20,110
trained on so and then finally we needed

00:08:18,729 --> 00:08:21,639
something that had relatively low

00:08:20,110 --> 00:08:26,169
maintenance and relatively easy

00:08:21,639 --> 00:08:28,740
scalability and with that we started to

00:08:26,169 --> 00:08:28,740
search for a solution

00:08:29,009 --> 00:08:38,709
sorry about that have a little bit of

00:08:30,909 --> 00:08:41,769
dry mouth one sect so our first attempt

00:08:38,709 --> 00:08:43,899
was let's make absolutely sure that we

00:08:41,769 --> 00:08:45,699
have to build this on our own and we

00:08:43,899 --> 00:08:51,640
started doing trials with some other

00:08:45,699 --> 00:08:52,839
providers of external metrics and in our

00:08:51,640 --> 00:08:55,160
case we were really unable to find a

00:08:52,839 --> 00:08:57,440
provider that met both the price or

00:08:55,160 --> 00:08:59,389
solution requirements we didn't get a

00:08:57,440 --> 00:09:00,769
chance to check out a whole lot of

00:08:59,389 --> 00:09:04,250
companies the time we're building this

00:09:00,769 --> 00:09:06,800
but the big issue that we had was that

00:09:04,250 --> 00:09:08,690
none of them really had reasonable

00:09:06,800 --> 00:09:10,970
pricing for the temporary auto-scale

00:09:08,690 --> 00:09:13,759
pooled host a lot of things that we saw

00:09:10,970 --> 00:09:15,110
was if a if a server was in their system

00:09:13,759 --> 00:09:17,029
even it only been up for a couple of

00:09:15,110 --> 00:09:18,529
hours we were still getting charged for

00:09:17,029 --> 00:09:22,819
that for an entire month or even the

00:09:18,529 --> 00:09:24,829
term of the contract and with all animai

00:09:22,819 --> 00:09:26,329
we said well let's see what we can come

00:09:24,829 --> 00:09:28,160
up with in-house let's see if we can

00:09:26,329 --> 00:09:30,589
beat that so we did at change Lord we

00:09:28,160 --> 00:09:32,389
call those spikes where we spend a week

00:09:30,589 --> 00:09:35,000
or two figuring out hey is this

00:09:32,389 --> 00:09:36,259
something we can do viably and if so

00:09:35,000 --> 00:09:40,160
then we go for the Minimum Viable

00:09:36,259 --> 00:09:42,649
Product on it and part of this means was

00:09:40,160 --> 00:09:44,569
we needed to define our requirements for

00:09:42,649 --> 00:09:47,870
what that satisfactory solution would be

00:09:44,569 --> 00:09:49,430
rather than just well we don't quite

00:09:47,870 --> 00:09:52,819
we're not quite getting what we want

00:09:49,430 --> 00:09:54,319
from someone externally so the

00:09:52,819 --> 00:09:56,180
requirements that we came up with for

00:09:54,319 --> 00:10:00,380
the do-it-yourself stack that's what DIY

00:09:56,180 --> 00:10:01,759
stands for is we've leverage tools like

00:10:00,380 --> 00:10:04,490
I said the team members were already

00:10:01,759 --> 00:10:06,259
familiar with in our case that happened

00:10:04,490 --> 00:10:08,029
to be the good old Etsy stack of

00:10:06,259 --> 00:10:12,230
collective stats tea and graphite

00:10:08,029 --> 00:10:14,060
thanks for that Etsy people also need to

00:10:12,230 --> 00:10:15,769
be again a relatively low maintenance we

00:10:14,060 --> 00:10:16,850
were very understaffed at the time the

00:10:15,769 --> 00:10:18,769
less time that we're spending

00:10:16,850 --> 00:10:21,019
maintaining this is the more time that

00:10:18,769 --> 00:10:22,759
we're actually working on deployments or

00:10:21,019 --> 00:10:24,500
converting more of our legacy

00:10:22,759 --> 00:10:27,709
infrastructure to something that's a

00:10:24,500 --> 00:10:28,970
more modern DevOps setup and then

00:10:27,709 --> 00:10:31,360
finally we needed something that was

00:10:28,970 --> 00:10:34,160
flexible resilient and distributed

00:10:31,360 --> 00:10:35,810
flexible meaning that there are multiple

00:10:34,160 --> 00:10:38,000
ways that we can try to implement it to

00:10:35,810 --> 00:10:38,959
really figure out what's the best fit

00:10:38,000 --> 00:10:41,420
for us

00:10:38,959 --> 00:10:43,970
resilient because if your monetary

00:10:41,420 --> 00:10:46,040
system goes down and there's no way you

00:10:43,970 --> 00:10:48,470
know you can't failover somewhere or

00:10:46,040 --> 00:10:51,019
it's not distributed out well now no

00:10:48,470 --> 00:10:52,309
one's watching your systems and you

00:10:51,019 --> 00:10:53,809
don't know what's going on we don't know

00:10:52,309 --> 00:10:59,540
what's going on you're already broken

00:10:53,809 --> 00:11:01,220
even if the site's running fine and then

00:10:59,540 --> 00:11:02,990
it really need to be cost competitive

00:11:01,220 --> 00:11:05,000
with the services we had tried

00:11:02,990 --> 00:11:07,120
externally and give us the higher

00:11:05,000 --> 00:11:08,480
resolution of stats we were looking for

00:11:07,120 --> 00:11:09,829
and then

00:11:08,480 --> 00:11:11,209
I said earlier we would find ones that

00:11:09,829 --> 00:11:13,370
had a really good price but the

00:11:11,209 --> 00:11:16,190
resolution was one minute or five

00:11:13,370 --> 00:11:19,610
minutes which wasn't good for our needs

00:11:16,190 --> 00:11:22,550
there were providers that we could get

00:11:19,610 --> 00:11:24,050
that higher resolution but we couldn't

00:11:22,550 --> 00:11:25,699
use their built-in checks for it we

00:11:24,050 --> 00:11:27,620
solved to write our own checks which is

00:11:25,699 --> 00:11:31,910
the majority of the time in this project

00:11:27,620 --> 00:11:34,630
anyway and then finally we wanted

00:11:31,910 --> 00:11:36,680
something that used as many parts and

00:11:34,630 --> 00:11:38,690
services that we already had in our

00:11:36,680 --> 00:11:40,190
infrastructure as possible which again

00:11:38,690 --> 00:11:42,560
that brings down our tool training time

00:11:40,190 --> 00:11:44,540
we make better use of our own limited

00:11:42,560 --> 00:11:46,310
time because hey we already have our

00:11:44,540 --> 00:11:50,570
conflict management cookbooks and stuff

00:11:46,310 --> 00:11:54,860
ready for that so what we ended up

00:11:50,570 --> 00:11:58,100
settling on is collecti and running

00:11:54,860 --> 00:11:59,750
stats D as a collective plugin as anyone

00:11:58,100 --> 00:12:01,940
here who's used it know it has a wealth

00:11:59,750 --> 00:12:05,209
of pre-made plugins for monitoring

00:12:01,940 --> 00:12:07,760
different services your system etc saves

00:12:05,209 --> 00:12:09,889
time for us we're running it on every

00:12:07,760 --> 00:12:12,470
server so every server is monitoring

00:12:09,889 --> 00:12:15,139
itself and then reporting out and then

00:12:12,470 --> 00:12:17,329
for the checks that collect he doesn't

00:12:15,139 --> 00:12:21,050
do we're basically using it as a super

00:12:17,329 --> 00:12:24,829
cron to run our own custom checks so

00:12:21,050 --> 00:12:29,540
that way they run every 6 seconds our in

00:12:24,829 --> 00:12:32,540
some cases every second we also use

00:12:29,540 --> 00:12:34,639
cyanides this acts as our carpet

00:12:32,540 --> 00:12:38,329
receiver it's also where we are querying

00:12:34,639 --> 00:12:40,819
for the statistics this uses Cassandra

00:12:38,329 --> 00:12:44,240
as its storage back-end uses elastic

00:12:40,819 --> 00:12:45,860
search to index the stat paths and we're

00:12:44,240 --> 00:12:47,720
already using both of those Cassandra we

00:12:45,860 --> 00:12:50,500
store a lot of our data in that does not

00:12:47,720 --> 00:12:52,910
have to be immediately correct or

00:12:50,500 --> 00:12:55,910
doesn't deal with finance or something

00:12:52,910 --> 00:12:59,060
like that elastic search is what we back

00:12:55,910 --> 00:13:00,350
our actual search on the page with so we

00:12:59,060 --> 00:13:02,060
already had knowledge on how to

00:13:00,350 --> 00:13:06,560
implement those services and run them

00:13:02,060 --> 00:13:08,750
well syenite also clustered very easily

00:13:06,560 --> 00:13:13,040
and it seemed pretty fast especially in

00:13:08,750 --> 00:13:14,630
the earlier releases of it and then we

00:13:13,040 --> 00:13:16,069
also instead of implementing full

00:13:14,630 --> 00:13:20,510
graphite we went with a stripped down

00:13:16,069 --> 00:13:21,740
graphite API it is stripped down it runs

00:13:20,510 --> 00:13:23,540
a lot faster

00:13:21,740 --> 00:13:26,090
what it doesn't give you is the

00:13:23,540 --> 00:13:28,640
dashboards or the composer or the nice

00:13:26,090 --> 00:13:29,900
pretty stats tree so obviously we're

00:13:28,640 --> 00:13:35,150
gonna need something that provides that

00:13:29,900 --> 00:13:38,840
way never go with graph on ax it is fast

00:13:35,150 --> 00:13:42,080
very responsive it has a really great

00:13:38,840 --> 00:13:45,980
feature set you know full mouse over on

00:13:42,080 --> 00:13:47,300
stats you're able adjusts I click on one

00:13:45,980 --> 00:13:49,330
area drag it over a little bit and will

00:13:47,300 --> 00:13:51,890
automatically zoom into that time frame

00:13:49,330 --> 00:13:53,750
integrates with a lot of different time

00:13:51,890 --> 00:13:55,670
series databases so even if we decide

00:13:53,750 --> 00:13:57,110
that what we've got now doesn't work we

00:13:55,670 --> 00:14:00,050
can still use for fauna as a front-end

00:13:57,110 --> 00:14:03,770
but the big one the biggest deal on it

00:14:00,050 --> 00:14:06,980
is that the dashboards for it are JSON

00:14:03,770 --> 00:14:08,900
its temporal JSON and as we're a web

00:14:06,980 --> 00:14:10,670
company that has a lot of developers who

00:14:08,900 --> 00:14:16,700
are very familiar with using JSON as a

00:14:10,670 --> 00:14:18,890
data structure that's a big deal because

00:14:16,700 --> 00:14:20,360
we want the developers to own the

00:14:18,890 --> 00:14:22,070
dashboards we don't want to have to do

00:14:20,360 --> 00:14:24,740
that in infrastructure in operations

00:14:22,070 --> 00:14:27,680
because frankly the devs know better

00:14:24,740 --> 00:14:29,960
what matters which stats are important

00:14:27,680 --> 00:14:31,910
what should be graphed so that we were

00:14:29,960 --> 00:14:33,440
not just collecting everything and then

00:14:31,910 --> 00:14:36,440
flooding ourselves with information that

00:14:33,440 --> 00:14:39,290
isn't useful so developers can set up

00:14:36,440 --> 00:14:43,070
their own monitors and manage that

00:14:39,290 --> 00:14:44,000
dashboard and it's a lot better than us

00:14:43,070 --> 00:14:46,040
trying to figure out and flooding

00:14:44,000 --> 00:14:48,770
ourselves with stats takes work off the

00:14:46,040 --> 00:14:51,020
plate of the dev ops team you know we're

00:14:48,770 --> 00:14:52,400
again we were understaffed and if

00:14:51,020 --> 00:14:56,510
developers can handle this

00:14:52,400 --> 00:14:57,890
it also you know it takes more work off

00:14:56,510 --> 00:15:01,310
us puts on then gives them increased

00:14:57,890 --> 00:15:02,930
ownership and we can check in the this

00:15:01,310 --> 00:15:05,360
dashboard with the app code itself

00:15:02,930 --> 00:15:07,460
instead of having to manage it through

00:15:05,360 --> 00:15:12,020
you know our deploy repository or our

00:15:07,460 --> 00:15:12,950
dev ops repository that is you know

00:15:12,020 --> 00:15:14,780
that's really great because now it's a

00:15:12,950 --> 00:15:16,490
visual reminder hey if the developer is

00:15:14,780 --> 00:15:18,740
updating something about this service or

00:15:16,490 --> 00:15:20,510
this application they need to update

00:15:18,740 --> 00:15:24,160
their dashboard too and it's with their

00:15:20,510 --> 00:15:24,160
it's there with the rest of their code

00:15:26,080 --> 00:15:30,020
not only that for dashboards that are

00:15:28,820 --> 00:15:32,690
pretty common amongst the different

00:15:30,020 --> 00:15:34,660
services like hey if we're gonna be

00:15:32,690 --> 00:15:36,579
using nginx as

00:15:34,660 --> 00:15:38,620
for a service well then we have a temp

00:15:36,579 --> 00:15:41,019
you know we can template out the checks

00:15:38,620 --> 00:15:42,399
that we care about in nginx and apply

00:15:41,019 --> 00:15:44,319
them to that service give it its own

00:15:42,399 --> 00:15:46,509
dashboard and we can just put that and

00:15:44,319 --> 00:15:49,329
change config as a custom library or

00:15:46,509 --> 00:15:53,410
just you know a template file it works

00:15:49,329 --> 00:15:54,519
out really well and then finally as I

00:15:53,410 --> 00:15:57,189
touch on in the beginning of this

00:15:54,519 --> 00:16:00,519
section the JSON is familiar format our

00:15:57,189 --> 00:16:02,410
devs and again when we're shooting for

00:16:00,519 --> 00:16:03,850
you know increased adoption rate because

00:16:02,410 --> 00:16:06,310
if you build a great tool and nobody

00:16:03,850 --> 00:16:08,620
uses it it's useless

00:16:06,310 --> 00:16:10,089
you've wasted time on it so the fact

00:16:08,620 --> 00:16:12,519
that we can go hey let's use something

00:16:10,089 --> 00:16:15,069
that we already know that our devs are

00:16:12,519 --> 00:16:19,720
in our familiar with and they picked up

00:16:15,069 --> 00:16:21,879
on it right away then here's a little

00:16:19,720 --> 00:16:23,649
architecture graph of what it all sort

00:16:21,879 --> 00:16:25,629
of looks like it had a high-level you

00:16:23,649 --> 00:16:26,920
know we have our app servers our central

00:16:25,629 --> 00:16:29,379
monitor for the stuff we do have to

00:16:26,920 --> 00:16:31,029
monitor centrally our stuff that gets

00:16:29,379 --> 00:16:33,279
our external stats like our web page

00:16:31,029 --> 00:16:35,019
speed around the world sends that over

00:16:33,279 --> 00:16:37,870
to psy and I indexes the stats in

00:16:35,019 --> 00:16:40,089
elasticsearch puts the actual stats

00:16:37,870 --> 00:16:43,180
themselves in Cassandra and then Griffin

00:16:40,089 --> 00:16:45,130
and graphite API query syenite whenever

00:16:43,180 --> 00:16:47,319
you have a dashboard request coming from

00:16:45,130 --> 00:16:48,639
anywhere and also I forgot to mention

00:16:47,319 --> 00:16:50,709
this other reason why we did go with

00:16:48,639 --> 00:16:53,709
Griffin ax is that hey we're a Google

00:16:50,709 --> 00:16:56,800
shop as far as we use Google Docs you

00:16:53,709 --> 00:17:00,730
know we have our logins done SSO with

00:16:56,800 --> 00:17:02,709
Google so there is a Google auth plugin

00:17:00,730 --> 00:17:04,839
for Griffin ax so that's even better

00:17:02,709 --> 00:17:06,939
there we don't have to manage that login

00:17:04,839 --> 00:17:08,949
it's not another thing if someone leaves

00:17:06,939 --> 00:17:10,209
the company then we disable their Google

00:17:08,949 --> 00:17:13,299
account and they can't log in anymore

00:17:10,209 --> 00:17:15,159
we not to worry about it so that's the

00:17:13,299 --> 00:17:20,500
metrics let's talk about the monitoring

00:17:15,159 --> 00:17:22,659
side of it real quick first thing we did

00:17:20,500 --> 00:17:24,640
was we're writing and running some

00:17:22,659 --> 00:17:27,939
simple scripts to query syenite to get a

00:17:24,640 --> 00:17:29,740
lot of our aggregated metrics and for a

00:17:27,939 --> 00:17:32,200
lot of the actual stuff we want to alert

00:17:29,740 --> 00:17:34,600
on and these accompany the systems

00:17:32,200 --> 00:17:37,330
checks from collect D that we really

00:17:34,600 --> 00:17:39,820
care about we use paid your duty for

00:17:37,330 --> 00:17:41,770
alerting and paging mainly because I

00:17:39,820 --> 00:17:44,799
can't build a better tool than that

00:17:41,770 --> 00:17:47,020
right now they've implemented that great

00:17:44,799 --> 00:17:49,070
I'm a big fan

00:17:47,020 --> 00:17:50,990
and then again we only want to use

00:17:49,070 --> 00:17:53,540
external monitoring it to check

00:17:50,990 --> 00:17:55,630
application wide or aggregate stats and

00:17:53,540 --> 00:17:58,580
also for things like site up/down

00:17:55,630 --> 00:18:01,460
obviously it works best if that's not in

00:17:58,580 --> 00:18:03,530
you know where your data center or your

00:18:01,460 --> 00:18:04,880
AWS region you want to make sure you're

00:18:03,530 --> 00:18:06,830
actually able to the outside world no

00:18:04,880 --> 00:18:10,760
matter what your stats internally say

00:18:06,830 --> 00:18:13,490
and then we want to use those external

00:18:10,760 --> 00:18:16,120
services as little as possible which

00:18:13,490 --> 00:18:18,410
again for site and servers up and down

00:18:16,120 --> 00:18:20,450
things that we can't implement better

00:18:18,410 --> 00:18:22,040
than an external vendor because again

00:18:20,450 --> 00:18:24,830
our time is precious and we're

00:18:22,040 --> 00:18:27,020
understaffed we need to get that done

00:18:24,830 --> 00:18:30,100
and then or there services that we can't

00:18:27,020 --> 00:18:32,510
beat for the price like page or duty and

00:18:30,100 --> 00:18:34,970
then finally one at temple as many

00:18:32,510 --> 00:18:36,860
checks as possible for each amount easy

00:18:34,970 --> 00:18:38,990
management by change control because

00:18:36,860 --> 00:18:40,970
again if a developer doesn't have to do

00:18:38,990 --> 00:18:43,340
a whole bunch of work and implement it

00:18:40,970 --> 00:18:45,110
every single time we're going to get

00:18:43,340 --> 00:18:47,720
quick adoption they're going to get

00:18:45,110 --> 00:18:49,430
excited about it and we are going to be

00:18:47,720 --> 00:18:54,220
able to roll this roll this internal

00:18:49,430 --> 00:18:56,630
product out and really make it shine so

00:18:54,220 --> 00:19:02,330
we need to see how are we gonna get

00:18:56,630 --> 00:19:06,080
developer buy-in in this situation first

00:19:02,330 --> 00:19:08,120
thing is we need to make it simple to

00:19:06,080 --> 00:19:12,230
add stats and monitors so that we get

00:19:08,120 --> 00:19:13,940
this high adoption rate in our case this

00:19:12,230 --> 00:19:16,430
was making portable code in the

00:19:13,940 --> 00:19:18,230
languages that we're using we started as

00:19:16,430 --> 00:19:20,930
a proof the concept that I just called

00:19:18,230 --> 00:19:22,790
monitor Lib it's an internal library for

00:19:20,930 --> 00:19:24,560
python that's where i write most of our

00:19:22,790 --> 00:19:27,290
checks and even though we're a ruby shop

00:19:24,560 --> 00:19:29,000
I learned on Python and it's again if

00:19:27,290 --> 00:19:31,400
I've got to implement something I want

00:19:29,000 --> 00:19:36,350
to use something that I know to get it

00:19:31,400 --> 00:19:38,750
out there fast so this saves time people

00:19:36,350 --> 00:19:41,540
developers don't have to evaluate a

00:19:38,750 --> 00:19:43,460
bunch of different implementations of a

00:19:41,540 --> 00:19:44,840
stats D client because we're going to

00:19:43,460 --> 00:19:47,030
provide it for them

00:19:44,840 --> 00:19:50,630
it helps dry up our code with dry being

00:19:47,030 --> 00:19:53,420
don't repeat yourself we only want our

00:19:50,630 --> 00:19:54,350
stats D client and the our page do

00:19:53,420 --> 00:19:57,620
declines things like that to be

00:19:54,350 --> 00:19:59,540
implemented once per codebase let them

00:19:57,620 --> 00:20:00,200
get let them import it makes it easier

00:19:59,540 --> 00:20:01,190
something does

00:20:00,200 --> 00:20:04,549
break we're not trying to troubleshoot

00:20:01,190 --> 00:20:08,210
four or five different clients here and

00:20:04,549 --> 00:20:10,460
then finally that gives us a very very

00:20:08,210 --> 00:20:13,669
standard and trainable implementation

00:20:10,460 --> 00:20:16,010
you know we get it done we show it off

00:20:13,669 --> 00:20:19,370
in demos we sit down with the

00:20:16,010 --> 00:20:21,289
development leads and we train it and

00:20:19,370 --> 00:20:26,899
now it's the same across all of our

00:20:21,289 --> 00:20:30,620
departments and then like I just

00:20:26,899 --> 00:20:32,929
mentioned demo the ease of use show how

00:20:30,620 --> 00:20:34,880
easy it is to use because we've done the

00:20:32,929 --> 00:20:37,220
legwork for you we provided the tools

00:20:34,880 --> 00:20:39,799
we've led the horse to water and I'll to

00:20:37,220 --> 00:20:41,029
do is show them how to drink which if

00:20:39,799 --> 00:20:46,159
you know developers showing them how to

00:20:41,029 --> 00:20:49,190
drink is not our task well at least for

00:20:46,159 --> 00:20:51,649
me and then finally the other thing you

00:20:49,190 --> 00:20:53,840
want to do is consult the you know the

00:20:51,649 --> 00:20:56,419
individual influential influential

00:20:53,840 --> 00:20:59,539
developers on the importance of getting

00:20:56,419 --> 00:21:01,429
these stats everywhere because you don't

00:20:59,539 --> 00:21:03,139
just want to go out and tell people how

00:21:01,429 --> 00:21:05,899
great it is and show them how great it

00:21:03,139 --> 00:21:08,450
is it's much more effective when you can

00:21:05,899 --> 00:21:09,889
get someone else to do this on your

00:21:08,450 --> 00:21:12,080
behalf because you've gotten them

00:21:09,889 --> 00:21:13,880
excited about this service and this goes

00:21:12,080 --> 00:21:17,380
for an external product or an internal

00:21:13,880 --> 00:21:19,580
product get people to evangelize it and

00:21:17,380 --> 00:21:22,039
those people are going to go out there

00:21:19,580 --> 00:21:23,960
your team leads there your principal

00:21:22,039 --> 00:21:25,789
architect these are the people that are

00:21:23,960 --> 00:21:28,490
your thought leaders within your own

00:21:25,789 --> 00:21:30,919
company and get them on board solicit

00:21:28,490 --> 00:21:33,529
their advice and then show them how they

00:21:30,919 --> 00:21:35,779
can go to their teams and get it in

00:21:33,529 --> 00:21:38,299
place get it running and have that

00:21:35,779 --> 00:21:40,190
support it makes a lot easier to

00:21:38,299 --> 00:21:44,320
implement new systems whether it's this

00:21:40,190 --> 00:21:46,909
or anything else you choose and finally

00:21:44,320 --> 00:21:48,889
once we've done all this we've gotten

00:21:46,909 --> 00:21:50,659
our developer buy-in we've evaluated

00:21:48,889 --> 00:21:52,610
what we need to do we've got an

00:21:50,659 --> 00:21:54,470
implementation in place it's been

00:21:52,610 --> 00:22:01,460
running for a while so what are we get

00:21:54,470 --> 00:22:03,710
from all this work well we have faster

00:22:01,460 --> 00:22:06,320
code by far we were able to find

00:22:03,710 --> 00:22:08,840
situations that were getting flattened

00:22:06,320 --> 00:22:12,289
out in our metrics that had a lower

00:22:08,840 --> 00:22:13,630
resolution to them where there would be

00:22:12,289 --> 00:22:16,870
spikes

00:22:13,630 --> 00:22:19,150
as far as response times went and other

00:22:16,870 --> 00:22:20,740
things of that nature where services

00:22:19,150 --> 00:22:22,810
were communicating as fast as possible

00:22:20,740 --> 00:22:25,420
in certain situations but the big one is

00:22:22,810 --> 00:22:28,030
of is for our some of our services we

00:22:25,420 --> 00:22:30,580
brought response times down by 25% or

00:22:28,030 --> 00:22:34,600
more on a lot of things and that's a big

00:22:30,580 --> 00:22:37,210
deal and here this is one of our

00:22:34,600 --> 00:22:40,000
dashboards it's you know a low traffic

00:22:37,210 --> 00:22:41,800
time period but we're really able to see

00:22:40,000 --> 00:22:44,310
you know how many if we got the up top

00:22:41,800 --> 00:22:46,570
the numbers for just quick looks at it

00:22:44,310 --> 00:22:47,650
then below we've got our graphs and a

00:22:46,570 --> 00:22:49,540
show off work you know we've got our

00:22:47,650 --> 00:22:52,000
median response times we've got our 95th

00:22:49,540 --> 00:22:54,580
percentile the reason why we have

00:22:52,000 --> 00:22:56,550
average there is historically that was a

00:22:54,580 --> 00:22:58,480
metric that people cared about and while

00:22:56,550 --> 00:23:00,400
average isn't really all that great

00:22:58,480 --> 00:23:02,770
again you give people something they're

00:23:00,400 --> 00:23:05,590
familiar with and they're gonna be more

00:23:02,770 --> 00:23:06,790
willing to adopt the new thing you're

00:23:05,590 --> 00:23:08,440
putting in front of them if there's

00:23:06,790 --> 00:23:13,900
still something there that looks like

00:23:08,440 --> 00:23:15,030
the old thing we've got faster and fewer

00:23:13,900 --> 00:23:17,890
rollbacks

00:23:15,030 --> 00:23:19,450
because one with these you know with

00:23:17,890 --> 00:23:22,210
their stats coming in faster than before

00:23:19,450 --> 00:23:25,270
you know if something does manage to get

00:23:22,210 --> 00:23:28,060
through our testing and our staging

00:23:25,270 --> 00:23:30,750
process things like that we know much

00:23:28,060 --> 00:23:33,160
faster if we need to roll that code back

00:23:30,750 --> 00:23:34,750
we're getting stats now in our staging

00:23:33,160 --> 00:23:36,250
environment that you know if we were

00:23:34,750 --> 00:23:39,040
using an external vendor was money that

00:23:36,250 --> 00:23:41,200
sometimes wasn't in the budget that will

00:23:39,040 --> 00:23:43,060
you know cheaply since it's already a

00:23:41,200 --> 00:23:44,560
planner for prod show the issues we

00:23:43,060 --> 00:23:47,590
weren't catching before there which

00:23:44,560 --> 00:23:49,810
helped again fewer roll backs there but

00:23:47,590 --> 00:23:51,760
not only that is when you're gathering

00:23:49,810 --> 00:23:54,640
metrics on everything you can also put

00:23:51,760 --> 00:23:56,200
your business KPI is in place and show

00:23:54,640 --> 00:23:59,290
you I mean by that here's our main

00:23:56,200 --> 00:24:00,850
deploy dashboard that decides when you

00:23:59,290 --> 00:24:03,400
know if we're available to deploy and

00:24:00,850 --> 00:24:05,980
what happens after a deployment and if

00:24:03,400 --> 00:24:07,600
you'll notice there the big graph front

00:24:05,980 --> 00:24:11,350
and center that's top and the left is

00:24:07,600 --> 00:24:13,090
KPIs that is all business this is our

00:24:11,350 --> 00:24:14,590
signatures and past me this was a slow

00:24:13,090 --> 00:24:16,900
time of the day when I took this because

00:24:14,590 --> 00:24:21,130
I was up late and doing a slide deck at

00:24:16,900 --> 00:24:22,090
the last minute but these are most of

00:24:21,130 --> 00:24:23,500
the stuff that were looking at here is

00:24:22,090 --> 00:24:25,540
business the only things that's really

00:24:23,500 --> 00:24:27,010
technical here are the Redis commands

00:24:25,540 --> 00:24:29,290
per second because for us

00:24:27,010 --> 00:24:31,780
with Redis goes way too high it has

00:24:29,290 --> 00:24:33,610
fallen over for us and then if we're

00:24:31,780 --> 00:24:36,100
getting a bunch of 500 you know 500

00:24:33,610 --> 00:24:37,690
class requests on production everything

00:24:36,100 --> 00:24:39,730
else there it's business based and we're

00:24:37,690 --> 00:24:41,530
able to see that stuff a lot faster to

00:24:39,730 --> 00:24:43,120
see them even if the code is good and

00:24:41,530 --> 00:24:45,190
we're not throwing errors if it's

00:24:43,120 --> 00:24:47,740
dropped our signature rate because we've

00:24:45,190 --> 00:24:49,660
made a change to one of our pages and we

00:24:47,740 --> 00:24:51,580
didn't catch it in earlier testing for

00:24:49,660 --> 00:24:53,380
some reason let's show an up front and

00:24:51,580 --> 00:24:55,030
saidar we're getting it real time and we

00:24:53,380 --> 00:24:56,710
can go hey we need to roll this back

00:24:55,030 --> 00:24:58,270
until we figure out what happened

00:24:56,710 --> 00:25:00,130
because if we're not getting signatures

00:24:58,270 --> 00:25:03,400
and we're not getting people to promote

00:25:00,130 --> 00:25:05,920
our petitions we're not one doing our

00:25:03,400 --> 00:25:07,660
job as change.org to try to make change

00:25:05,920 --> 00:25:10,030
and get people to sign on to movements

00:25:07,660 --> 00:25:11,590
but hey we're not making the revenue

00:25:10,030 --> 00:25:14,110
that we need to keep our doors open and

00:25:11,590 --> 00:25:16,870
keep making change so that is as

00:25:14,110 --> 00:25:18,670
important as the technical stuff that

00:25:16,870 --> 00:25:20,350
would cause a rollback like hey my

00:25:18,670 --> 00:25:25,150
response times went up or something like

00:25:20,350 --> 00:25:26,590
that now we've also finding our problem

00:25:25,150 --> 00:25:28,300
instances easier than before I'm sure

00:25:26,590 --> 00:25:30,640
you all know that when you bring up

00:25:28,300 --> 00:25:32,350
instances in the cloud a lot of times

00:25:30,640 --> 00:25:33,490
you're not you don't have that box to

00:25:32,350 --> 00:25:35,830
yourself what's it's a really large

00:25:33,490 --> 00:25:38,200
instance and you do have noisy neighbor

00:25:35,830 --> 00:25:41,320
syndrome especially for some vendors out

00:25:38,200 --> 00:25:43,330
there that use on that let you use

00:25:41,320 --> 00:25:45,190
unused resources on the host machine if

00:25:43,330 --> 00:25:48,010
no one else is using it well sometimes

00:25:45,190 --> 00:25:50,050
that doesn't get capped as fast as they

00:25:48,010 --> 00:25:51,370
would like so you'll have an issue where

00:25:50,050 --> 00:25:53,830
you got a noisy neighbor using a lot of

00:25:51,370 --> 00:25:55,780
resources and now we've got an instance

00:25:53,830 --> 00:25:58,180
that's responding 40 or 50 milliseconds

00:25:55,780 --> 00:26:00,700
slower than every other one and it makes

00:25:58,180 --> 00:26:02,530
it much easier to find this problem

00:26:00,700 --> 00:26:06,430
after we moved out code because we can

00:26:02,530 --> 00:26:07,630
sit here and we can break up our you

00:26:06,430 --> 00:26:10,780
know our servers per second and

00:26:07,630 --> 00:26:12,400
everything like that by you know the

00:26:10,780 --> 00:26:14,350
actual servers themselves and not just

00:26:12,400 --> 00:26:16,630
an aggregate and we're able to capture

00:26:14,350 --> 00:26:18,430
all these stats and set up easy

00:26:16,630 --> 00:26:20,140
visualizations to see that oh hey we

00:26:18,430 --> 00:26:22,810
just brought up a new host

00:26:20,140 --> 00:26:25,810
it's taking fewer requests and it's

00:26:22,810 --> 00:26:27,250
response time is much higher we need to

00:26:25,810 --> 00:26:29,860
kill that host because that's going to

00:26:27,250 --> 00:26:30,910
cause issues that are going to be

00:26:29,860 --> 00:26:36,070
visible externally to someone who's

00:26:30,910 --> 00:26:38,020
using the site you know we can also set

00:26:36,070 --> 00:26:39,880
up visualizations like this where hey

00:26:38,020 --> 00:26:41,289
this tells me everything I really need

00:26:39,880 --> 00:26:44,320
to know about

00:26:41,289 --> 00:26:47,769
about a system's overall health how much

00:26:44,320 --> 00:26:49,690
traffic that's pushing out the CPU usage

00:26:47,769 --> 00:26:50,889
load is there because while it's not

00:26:49,690 --> 00:26:53,649
particularly useful for troubleshooting

00:26:50,889 --> 00:27:00,759
its again people are used to seeing that

00:26:53,649 --> 00:27:02,200
and familiarity will help adoption so

00:27:00,759 --> 00:27:04,590
then the big thing that we got is we've

00:27:02,200 --> 00:27:07,330
also got faster and easier

00:27:04,590 --> 00:27:10,659
troubleshooting for even non instance

00:27:07,330 --> 00:27:13,659
items because in our case we throw us we

00:27:10,659 --> 00:27:15,009
throw us that whenever you would

00:27:13,659 --> 00:27:17,679
normally logging something if it would

00:27:15,009 --> 00:27:20,139
be logged in debug mode it needs to

00:27:17,679 --> 00:27:23,769
throw a stat and one of the big wins we

00:27:20,139 --> 00:27:25,929
get by that is we hardly crawl logs

00:27:23,769 --> 00:27:27,999
anymore especially on the production

00:27:25,929 --> 00:27:30,369
engineering team I haven't looked at a

00:27:27,999 --> 00:27:32,320
log for troubleshooting for our main

00:27:30,369 --> 00:27:34,989
front end or our back-end services in

00:27:32,320 --> 00:27:36,940
going on four or five months now because

00:27:34,989 --> 00:27:40,330
anything that I would really care about

00:27:36,940 --> 00:27:41,889
that's gonna be in that log is sending a

00:27:40,330 --> 00:27:43,659
stat it also means that we don't have to

00:27:41,889 --> 00:27:45,249
do a whole lot of central log collection

00:27:43,659 --> 00:27:48,249
it's another service that we've replaced

00:27:45,249 --> 00:27:51,609
by throwing a stat on everything it also

00:27:48,249 --> 00:27:52,840
means that now our logs can be easier to

00:27:51,609 --> 00:27:55,059
be read by machine we can implement in

00:27:52,840 --> 00:27:58,539
JSON because humans are rarely going to

00:27:55,059 --> 00:28:00,999
be reading them and that allows us to

00:27:58,539 --> 00:28:03,519
parse them for business intelligence or

00:28:00,999 --> 00:28:05,409
other reasons a lot faster without

00:28:03,519 --> 00:28:09,659
having to have separate logs for that as

00:28:05,409 --> 00:28:09,659
compared to the actual system themselves

00:28:10,769 --> 00:28:15,549
we can also visually see which instances

00:28:13,929 --> 00:28:18,399
and servers are acting up like I showed

00:28:15,549 --> 00:28:20,080
you so well the nice things is once you

00:28:18,399 --> 00:28:22,629
have a set of dashboards that you're

00:28:20,080 --> 00:28:25,479
viewing on a regular basis is you start

00:28:22,629 --> 00:28:28,509
to notice patterns once you've seen how

00:28:25,479 --> 00:28:30,429
your site looks for a while you can come

00:28:28,509 --> 00:28:32,229
in and you're coming in for the day you

00:28:30,429 --> 00:28:34,179
look at you go something's wrong there

00:28:32,229 --> 00:28:36,340
that patterns not right you might not

00:28:34,179 --> 00:28:39,309
know what it is it might not be bringing

00:28:36,340 --> 00:28:42,190
your site down but you can visually see

00:28:39,309 --> 00:28:43,299
that this is happening you need to check

00:28:42,190 --> 00:28:44,830
out something and then start your

00:28:43,299 --> 00:28:47,739
troubleshooting steps before the problem

00:28:44,830 --> 00:28:50,440
ever comes to a head and brings you down

00:28:47,739 --> 00:28:52,269
slows you down cost the company money

00:28:50,440 --> 00:28:54,560
stops them from achieving their mission

00:28:52,269 --> 00:28:57,600
something like that

00:28:54,560 --> 00:29:01,560
another nice thing about using Griffin

00:28:57,600 --> 00:29:04,290
ax is you're able to create dashboards

00:29:01,560 --> 00:29:06,450
really quick and really fast just for

00:29:04,290 --> 00:29:08,340
scratch purposes to compare multiple

00:29:06,450 --> 00:29:10,320
sets of stats I don't have to sit here

00:29:08,340 --> 00:29:11,730
and have 20 terminal windows open any

00:29:10,320 --> 00:29:15,240
more tracing connections all the way

00:29:11,730 --> 00:29:17,670
through my stack because I can throw

00:29:15,240 --> 00:29:19,530
together a couple of graphs for you know

00:29:17,670 --> 00:29:21,330
this is how my flow should be my front

00:29:19,530 --> 00:29:23,760
end is going to talk to petition service

00:29:21,330 --> 00:29:25,710
to get a petition to show to someone and

00:29:23,760 --> 00:29:28,140
it's going to talk to user service so if

00:29:25,710 --> 00:29:29,550
the person decides to sign is there and

00:29:28,140 --> 00:29:31,620
if they sign it's gonna go to signature

00:29:29,550 --> 00:29:32,760
service so all those all have dashboards

00:29:31,620 --> 00:29:35,310
of their own if I need to do

00:29:32,760 --> 00:29:37,860
troubleshooting along the path I can

00:29:35,310 --> 00:29:40,680
whip up a scratch dashboard that has all

00:29:37,860 --> 00:29:43,830
this stuff there and zoom in on it to

00:29:40,680 --> 00:29:45,240
very high resolution and take a look at

00:29:43,830 --> 00:29:47,520
what's going on see where this problem

00:29:45,240 --> 00:29:50,360
is coming out with again I don't have to

00:29:47,520 --> 00:29:56,840
hunt through logs I don't have to watch

00:29:50,360 --> 00:30:00,300
individual machines on a terminal and

00:29:56,840 --> 00:30:01,680
you could also use it to get stuff that

00:30:00,300 --> 00:30:04,590
you would normally go on a machine to

00:30:01,680 --> 00:30:06,120
check anyways like in this case this is

00:30:04,590 --> 00:30:08,580
just a dashboard that basically just

00:30:06,120 --> 00:30:11,130
implements the info command in Redis

00:30:08,580 --> 00:30:12,600
this is gonna give us our pid' for type

00:30:11,130 --> 00:30:16,140
there it's gonna let me know if it's

00:30:12,600 --> 00:30:17,880
Redis or if it's Redis Sentinel it's got

00:30:16,140 --> 00:30:20,670
the amount of connections are used

00:30:17,880 --> 00:30:24,330
memory how long our background saves are

00:30:20,670 --> 00:30:25,560
taking whether or not it's a master you

00:30:24,330 --> 00:30:27,390
know things of that nature where it's

00:30:25,560 --> 00:30:29,250
really quick to just get it at a glance

00:30:27,390 --> 00:30:31,440
where I don't have to go on a server to

00:30:29,250 --> 00:30:33,120
check it I don't have to look at a bunch

00:30:31,440 --> 00:30:34,830
of graphs I'm putting it all there in

00:30:33,120 --> 00:30:36,950
numerical format like I'd see it on the

00:30:34,830 --> 00:30:36,950
machine

00:30:37,100 --> 00:30:45,150
but the biggest win of all was that it

00:30:43,020 --> 00:30:47,280
heavily heavily increased the

00:30:45,150 --> 00:30:49,290
communication between our feature

00:30:47,280 --> 00:30:52,620
developers our product developers and

00:30:49,290 --> 00:30:55,020
the infrastructure team who are now our

00:30:52,620 --> 00:30:57,840
DevOps team and this is the goal of

00:30:55,020 --> 00:30:59,340
DevOps as a philosophy is increased

00:30:57,840 --> 00:31:03,480
communication breaking down those

00:30:59,340 --> 00:31:05,309
barriers and this is it's a huge win

00:31:03,480 --> 00:31:07,409
especially when we're

00:31:05,309 --> 00:31:10,110
start up we're trying to do this major

00:31:07,409 --> 00:31:12,179
scaling operation we're growing very

00:31:10,110 --> 00:31:13,070
fast so there's new faces in the office

00:31:12,179 --> 00:31:15,929
all the time

00:31:13,070 --> 00:31:17,999
communication is the biggest biggest

00:31:15,929 --> 00:31:20,129
priority that we have in order to pull

00:31:17,999 --> 00:31:23,700
this off in order to do this mission

00:31:20,129 --> 00:31:25,289
that we want to do the app developers

00:31:23,700 --> 00:31:26,789
themselves as I mentioned earlier they

00:31:25,289 --> 00:31:29,039
have an increased sense of ownership

00:31:26,789 --> 00:31:32,070
because they're choosing what stats to

00:31:29,039 --> 00:31:34,669
capture and which dashboards matter they

00:31:32,070 --> 00:31:37,169
feel like that they're getting to decide

00:31:34,669 --> 00:31:38,700
this is what we care about these are the

00:31:37,169 --> 00:31:41,100
stats that we care about as far as my

00:31:38,700 --> 00:31:43,049
application is concerned seems like a

00:31:41,100 --> 00:31:45,149
little thing but it's all things where

00:31:43,049 --> 00:31:47,519
when I talk to my developer that's the

00:31:45,149 --> 00:31:52,379
biggest feedback I get is I feel like I

00:31:47,519 --> 00:31:54,960
own it and then honestly when something

00:31:52,379 --> 00:31:57,450
is wrong like when it's a business KPI

00:31:54,960 --> 00:31:59,970
instead of something very very visible

00:31:57,450 --> 00:32:03,360
like the site is slow or the site is

00:31:59,970 --> 00:32:05,220
down it can be easier to accept it from

00:32:03,360 --> 00:32:07,350
a graph or stats than it can from the

00:32:05,220 --> 00:32:10,590
Oscarson because if you're very very

00:32:07,350 --> 00:32:12,179
invested as a developer in getting this

00:32:10,590 --> 00:32:14,909
code released and someone tells you

00:32:12,179 --> 00:32:16,080
there's a problem you don't always want

00:32:14,909 --> 00:32:19,710
to hear it especially you pulled a lot

00:32:16,080 --> 00:32:21,990
of late nights but it you know decreases

00:32:19,710 --> 00:32:24,240
friction to have these statistics there

00:32:21,990 --> 00:32:26,879
and leaves the lines communication open

00:32:24,240 --> 00:32:28,559
so your infrastructure team or any

00:32:26,879 --> 00:32:29,789
DevOps team isn't the bad guy saying

00:32:28,559 --> 00:32:33,059
you've got to roll this back I know

00:32:29,789 --> 00:32:36,059
you've worked hard on it there is an

00:32:33,059 --> 00:32:38,220
unforeseen circumstance here we're able

00:32:36,059 --> 00:32:40,259
to point the stats and not only go this

00:32:38,220 --> 00:32:41,850
is why it has to roll back but when you

00:32:40,259 --> 00:32:45,440
go back to test it this is what you need

00:32:41,850 --> 00:32:47,940
to look at look at it right here and

00:32:45,440 --> 00:32:50,490
that is again the biggest win from all

00:32:47,940 --> 00:32:54,330
of this is not so much that we've

00:32:50,490 --> 00:32:56,029
implemented a very nice statistic system

00:32:54,330 --> 00:32:59,490
or that we're doing our monitoring

00:32:56,029 --> 00:33:01,769
distributed but it's that the tool

00:32:59,490 --> 00:33:03,990
itself not just does its job but helps

00:33:01,769 --> 00:33:07,980
open communications lines and break down

00:33:03,990 --> 00:33:10,080
those departmental barriers because it's

00:33:07,980 --> 00:33:12,259
being designed to take advantage of

00:33:10,080 --> 00:33:16,049
people's skill sets on their teams and

00:33:12,259 --> 00:33:17,879
being able to like that just communicate

00:33:16,049 --> 00:33:18,429
with them stats are used for

00:33:17,879 --> 00:33:19,929
communication

00:33:18,429 --> 00:33:21,639
is communicating whether something is

00:33:19,929 --> 00:33:23,289
down communicating how much is something

00:33:21,639 --> 00:33:25,119
you're doing and not only that you can

00:33:23,289 --> 00:33:26,980
use it to actually work on your people

00:33:25,119 --> 00:33:29,350
communication as well getting that

00:33:26,980 --> 00:33:31,679
buy-in getting people on board is again

00:33:29,350 --> 00:33:35,799
this is a core philosophy of DevOps and

00:33:31,679 --> 00:33:57,129
with that winners ask questions did he

00:33:35,799 --> 00:33:59,259
all have any the question was what was

00:33:57,129 --> 00:34:02,679
some of the decisions that went into

00:33:59,259 --> 00:34:05,289
choosing syenite Cassandra won it was we

00:34:02,679 --> 00:34:07,389
already had Cassandra as a database in

00:34:05,289 --> 00:34:08,470
our system where we stored data so one

00:34:07,389 --> 00:34:10,450
of the things we were trying to do to

00:34:08,470 --> 00:34:11,859
get implementation time down was use

00:34:10,450 --> 00:34:13,839
stuff we were use even if it might not

00:34:11,859 --> 00:34:14,770
have been the absolute best tool for the

00:34:13,839 --> 00:34:16,480
job

00:34:14,770 --> 00:34:17,859
in this case I mentioned Casa bond

00:34:16,480 --> 00:34:19,720
earlier that's actually a replacement

00:34:17,859 --> 00:34:21,760
for syenite that we're working on I was

00:34:19,720 --> 00:34:22,929
actually hoping to release it to you all

00:34:21,760 --> 00:34:26,500
today but it needs a couple of weeks

00:34:22,929 --> 00:34:29,260
more work but we will be transitioning

00:34:26,500 --> 00:34:31,000
to that but that was a decision behind

00:34:29,260 --> 00:34:32,589
it and it was also these are tools that

00:34:31,000 --> 00:34:34,450
as we've broken it out more if we

00:34:32,589 --> 00:34:36,970
decided we didn't like one piece of it

00:34:34,450 --> 00:34:38,889
we could replace one piece and not have

00:34:36,970 --> 00:34:48,879
to rip out the entire stack and redesign

00:34:38,889 --> 00:34:50,799
it yeah the question was what kind of

00:34:48,879 --> 00:34:53,319
latency did we get on the dashboards it

00:34:50,799 --> 00:34:56,020
really depends on what's being aggregate

00:34:53,319 --> 00:34:58,180
aggregated in there one of the flaws of

00:34:56,020 --> 00:34:59,740
the set up that I showed is we were is

00:34:58,180 --> 00:35:01,660
the original goal is to try to do it

00:34:59,740 --> 00:35:03,609
without an extra carbon aggregation

00:35:01,660 --> 00:35:05,319
layer one of the reasons why we're

00:35:03,609 --> 00:35:07,930
moving the Kasbah was that didn't work

00:35:05,319 --> 00:35:09,609
and I don't want to have all of the

00:35:07,930 --> 00:35:11,170
carbon IRA Gators in there if I can

00:35:09,609 --> 00:35:12,670
actually do that where my stats are

00:35:11,170 --> 00:35:14,589
being collected

00:35:12,670 --> 00:35:16,180
so part of what Casa bond will do is

00:35:14,589 --> 00:35:17,619
actually route stats around to which

00:35:16,180 --> 00:35:21,280
instance should own them according to a

00:35:17,619 --> 00:35:23,650
Pearson hash but the actual latency that

00:35:21,280 --> 00:35:25,000
we see for dashboards that were very

00:35:23,650 --> 00:35:28,119
numerical based we're doing a lot of

00:35:25,000 --> 00:35:30,490
aggregations they'll pop up in under a

00:35:28,119 --> 00:35:32,200
second in a lot of cases some of the

00:35:30,490 --> 00:35:34,599
ones that are busier like if we're

00:35:32,200 --> 00:35:36,160
a whole lot of load that web statistics

00:35:34,599 --> 00:35:39,070
one that we showed it will occasionally

00:35:36,160 --> 00:35:40,750
timeout and that's one of the things

00:35:39,070 --> 00:35:42,130
we're trying to address going forward

00:35:40,750 --> 00:35:43,540
because again this isn't about how we

00:35:42,130 --> 00:35:46,420
did something awesome it was what was

00:35:43,540 --> 00:35:48,400
our process for getting a solution in

00:35:46,420 --> 00:35:50,410
place so there's definitely flaws with

00:35:48,400 --> 00:36:13,119
it and that's one of them and that's a

00:35:50,410 --> 00:36:14,560
really good question asked question was

00:36:13,119 --> 00:36:17,109
how much value we're seeing from people

00:36:14,560 --> 00:36:18,250
being able to view the patterns like I

00:36:17,109 --> 00:36:20,650
talked about and if we're considering

00:36:18,250 --> 00:36:23,170
some kind of automated solution the

00:36:20,650 --> 00:36:25,510
answer is yes that's actually gonna be

00:36:23,170 --> 00:36:27,160
next up on our plate once we have some

00:36:25,510 --> 00:36:30,430
other work that's been prioritized over

00:36:27,160 --> 00:36:35,740
at in our department done you can get

00:36:30,430 --> 00:36:38,560
dashboard fatigue yeah sorry yes to both

00:36:35,740 --> 00:36:40,720
of those questions yes the value is

00:36:38,560 --> 00:36:42,490
there because if you make it easy for

00:36:40,720 --> 00:36:44,710
someone to look at how their codes doing

00:36:42,490 --> 00:36:46,210
again they'll adopt it if they can look

00:36:44,710 --> 00:36:47,740
on the big screens when they walk in the

00:36:46,210 --> 00:36:49,750
office take a look at their app before

00:36:47,740 --> 00:36:51,220
they even sit down for the day and it's

00:36:49,750 --> 00:36:52,300
sitting there right by the restrooms so

00:36:51,220 --> 00:36:53,589
if they're walk by to go the restroom

00:36:52,300 --> 00:36:56,400
they look up at the stats on the way

00:36:53,589 --> 00:36:59,230
there there's a huge value from that and

00:36:56,400 --> 00:37:00,849
but no but you can get dashboard fatigue

00:36:59,230 --> 00:37:03,670
so one of things we are definitely

00:37:00,849 --> 00:37:07,270
looking at is getting some how to

00:37:03,670 --> 00:37:08,619
automate a system in place to view a lot

00:37:07,270 --> 00:37:11,290
of dashboards that we were not having to

00:37:08,619 --> 00:37:14,170
try to cycle through 40 or 50 dashboards

00:37:11,290 --> 00:37:15,730
on our monitors up front there's another

00:37:14,170 --> 00:37:22,300
question over here alright and this is

00:37:15,730 --> 00:37:24,910
the last question no we actually we

00:37:22,300 --> 00:37:26,380
don't have a knock right now we are not

00:37:24,910 --> 00:37:29,410
quite that big yet on the engineering

00:37:26,380 --> 00:37:32,170
team to have a dedicated NOC what we do

00:37:29,410 --> 00:37:34,810
in a lot of cases is the scripts that I

00:37:32,170 --> 00:37:37,270
was talking about that will query for

00:37:34,810 --> 00:37:39,730
our aggregate stats or stuff like that

00:37:37,270 --> 00:37:41,050
those will also those also act as our

00:37:39,730 --> 00:37:42,970
pages that they see something wrong

00:37:41,050 --> 00:37:45,550
either on the individual system level or

00:37:42,970 --> 00:37:48,000
in aggregate across

00:37:45,550 --> 00:37:51,220
product a service something like that

00:37:48,000 --> 00:37:52,420
they will actually page out and right

00:37:51,220 --> 00:37:54,280
now one of the things that we're trying

00:37:52,420 --> 00:37:56,110
to work on is get away from defining

00:37:54,280 --> 00:37:58,150
thresholds which doesn't work so great

00:37:56,110 --> 00:38:00,160
in an auto scaling world and move over

00:37:58,150 --> 00:38:03,040
to what's do off of a standard deviation

00:38:00,160 --> 00:38:05,050
or - off of historical traffic patterns

00:38:03,040 --> 00:38:06,250
and that's again the next step that one

00:38:05,050 --> 00:38:08,290
of things we want to build out over the

00:38:06,250 --> 00:38:11,200
next year to make this system more

00:38:08,290 --> 00:38:12,400
robust anyway I realize thank you all

00:38:11,200 --> 00:38:16,680
for listening and for the great

00:38:12,400 --> 00:38:16,680

YouTube URL: https://www.youtube.com/watch?v=8yErya52xTw


