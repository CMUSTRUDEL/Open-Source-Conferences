Title: Tools for Salvation - Bethany Macri
Publication date: 2015-11-01
Playlist: DevOps Days Tel Aviv 2015
Description: 
	http://www.devopsdays.org/events/2015-telaviv/

Failure is inevitable in software. But expensive failure is not. Tools can save you from expensive failure.

In this talk, Bethany will discuss data migration at Etsy and how it is a shining example of DevOps for the following reasons:

the data migration team acknowledged that failure was inevitable and planned for it by building tools,
the data migration team acted as its own developers, dbas and QA team, and
the team used continuous integration to its advantage while making infrastructural changes.
Bethany will show how expensive or catastrophic failure can be prevented by letting go of ego and using practical tooling.

About the speaker - Bethany Macri

Bethany studied Literature in college, then taught herself how to code. After attending The Recurse Center, she started working at Etsy on the Core Platform team.

Bethany currently spends her days migrating data, running and recording a podcast.
Captions: 
	00:00:09,380 --> 00:00:14,820
and etsy is located in New York and

00:00:12,360 --> 00:00:17,310
Brooklyn where all the cool hipsters are

00:00:14,820 --> 00:00:21,119
and it's well known for its company

00:00:17,310 --> 00:00:25,350
culture things like embracing failure

00:00:21,119 --> 00:00:28,560
and post mortems and you know monitor

00:00:25,350 --> 00:00:30,330
everything ah if you heard about stats d

00:00:28,560 --> 00:00:33,300
stuff like that they were one of the

00:00:30,330 --> 00:00:38,190
first you know to write about that and I

00:00:33,300 --> 00:00:40,829
present in endeavor plaisance and

00:00:38,190 --> 00:00:43,500
bethany's uh is here as she's a core

00:00:40,829 --> 00:00:46,379
platform developer at Etsy and cheese go

00:00:43,500 --> 00:00:49,950
going to tell us a few of the secrets of

00:00:46,379 --> 00:00:53,910
the data migration team which is a very

00:00:49,950 --> 00:00:56,430
difficult complex subject I'm excited to

00:00:53,910 --> 00:01:02,460
have you here today please help me

00:00:56,430 --> 00:01:04,379
welcome Bethenny good morning I'm

00:01:02,460 --> 00:01:06,030
Bethany Macri it's my first time in

00:01:04,379 --> 00:01:08,040
Israel I could not be happier to be here

00:01:06,030 --> 00:01:14,610
I actually didn't think I was going to

00:01:08,040 --> 00:01:16,140
make it here I flew on ll and and in New

00:01:14,610 --> 00:01:17,460
York they asked what i did and i said

00:01:16,140 --> 00:01:18,960
i'm a software engineer and they said oh

00:01:17,460 --> 00:01:20,580
did you study that in school and i said

00:01:18,960 --> 00:01:23,939
no i'm self-taught and they took my bags

00:01:20,580 --> 00:01:25,979
and escorted me to the gate so like a

00:01:23,939 --> 00:01:27,869
criminal but I'm not a criminal I'm a

00:01:25,979 --> 00:01:30,659
software engineer at etsy on the core

00:01:27,869 --> 00:01:34,229
platform team we are an infrastructure

00:01:30,659 --> 00:01:38,490
team so we kind of live in were a hybrid

00:01:34,229 --> 00:01:42,689
of development and operations so who has

00:01:38,490 --> 00:01:43,950
not heard of Etsy okay perfect so for

00:01:42,689 --> 00:01:47,130
those of you who have not heard of us

00:01:43,950 --> 00:01:48,720
we're a brooklyn-based company we enable

00:01:47,130 --> 00:01:50,610
people to sell their handmade and

00:01:48,720 --> 00:01:53,640
vintage goods online and make a living

00:01:50,610 --> 00:01:57,270
for themselves we were founded in 2005

00:01:53,640 --> 00:01:59,219
we now have over 700 employees we just

00:01:57,270 --> 00:02:02,399
went public in the spring so we're now a

00:01:59,219 --> 00:02:05,640
public company we have 32 million items

00:02:02,399 --> 00:02:08,429
for sale online currently we have 1.5

00:02:05,640 --> 00:02:10,259
million active users and we now have

00:02:08,429 --> 00:02:13,160
transactions in nearly every country in

00:02:10,259 --> 00:02:15,440
the world including Israel

00:02:13,160 --> 00:02:17,660
technically speaking for the most part

00:02:15,440 --> 00:02:21,650
we run a lamp stack does everyone know

00:02:17,660 --> 00:02:25,160
what that is okay um and we have a

00:02:21,650 --> 00:02:28,370
monolithic code base so not overly

00:02:25,160 --> 00:02:30,080
exciting in that respect what is

00:02:28,370 --> 00:02:32,150
exciting about Etsy and I think what

00:02:30,080 --> 00:02:35,330
sets us apart technically is that we

00:02:32,150 --> 00:02:38,050
practice continuous deployment and we're

00:02:35,330 --> 00:02:40,460
pretty good at it we push 35 times daily

00:02:38,050 --> 00:02:42,800
the thought behind continuous deployment

00:02:40,460 --> 00:02:45,470
is that a small change is easier to

00:02:42,800 --> 00:02:48,320
understand it's easier to test and it's

00:02:45,470 --> 00:02:52,280
easier to fix in fact it's so easy to

00:02:48,320 --> 00:02:54,200
detect a change when it's so small that

00:02:52,280 --> 00:02:56,210
when something breaks we're often able

00:02:54,200 --> 00:02:59,510
to roll forward with a fix rather than

00:02:56,210 --> 00:03:02,900
revert or back it out this is to say

00:02:59,510 --> 00:03:05,570
that our failures are kept small or if

00:03:02,900 --> 00:03:07,610
our small change does cause a

00:03:05,570 --> 00:03:09,740
catastrophic failure it's very easy to

00:03:07,610 --> 00:03:12,650
pinpoint you know exactly what caused

00:03:09,740 --> 00:03:15,830
the error we do not have release

00:03:12,650 --> 00:03:18,470
managers build managers or a QA team we

00:03:15,830 --> 00:03:20,870
do that all ourselves as engineers there

00:03:18,470 --> 00:03:23,540
are no barriers to pushing this is

00:03:20,870 --> 00:03:25,430
actually our former CTO s t-shirt

00:03:23,540 --> 00:03:27,290
company that he used to sell these

00:03:25,430 --> 00:03:30,470
t-shirts on etsy and this is our one of

00:03:27,290 --> 00:03:34,340
our aesthetics to borrow a term from the

00:03:30,470 --> 00:03:36,980
keynote speaker continuous deployment

00:03:34,340 --> 00:03:39,950
fosters and maybe even forces DevOps

00:03:36,980 --> 00:03:41,630
collaboration this is the ratio of

00:03:39,950 --> 00:03:45,260
software engineers to operations

00:03:41,630 --> 00:03:47,510
engineers at Etsy there is both a clear

00:03:45,260 --> 00:03:50,600
separation and shared responsibility

00:03:47,510 --> 00:03:53,360
between engineers and ops engineers for

00:03:50,600 --> 00:03:55,760
us DevOps is not only about having Devin

00:03:53,360 --> 00:03:57,560
ops be the same thing we have really

00:03:55,760 --> 00:04:01,060
good engineers we have really good

00:03:57,560 --> 00:04:03,230
operations engineers but we cooperate

00:04:01,060 --> 00:04:05,540
whereas typically you have engineering

00:04:03,230 --> 00:04:07,790
in one silo and operations in another

00:04:05,540 --> 00:04:10,180
silo and they don't talk or when they do

00:04:07,790 --> 00:04:13,100
talk it's contentious because

00:04:10,180 --> 00:04:14,989
development makes some feature or piece

00:04:13,100 --> 00:04:17,840
of infrastructure and lobs it over the

00:04:14,989 --> 00:04:21,109
gate to operations and operation says no

00:04:17,840 --> 00:04:25,740
you're insane and then development cease

00:04:21,109 --> 00:04:28,680
operations is blocking their deadline

00:04:25,740 --> 00:04:30,750
we have developers who are empowered to

00:04:28,680 --> 00:04:33,599
think about the operational implications

00:04:30,750 --> 00:04:36,000
of their code and we talked early we

00:04:33,599 --> 00:04:38,430
talked often we talked before deploy we

00:04:36,000 --> 00:04:43,560
talk after deploy and we perform game

00:04:38,430 --> 00:04:45,270
days with our operations engineers what

00:04:43,560 --> 00:04:47,699
underlies this cooperation and what I

00:04:45,270 --> 00:04:49,380
believe is the most important thing that

00:04:47,699 --> 00:04:52,080
sets etsy apart is our culture of

00:04:49,380 --> 00:04:54,060
blameless pneus that means we conduct

00:04:52,080 --> 00:04:56,729
blameless post mortems when something

00:04:54,060 --> 00:04:58,860
goes wrong there's no pointing fingers

00:04:56,729 --> 00:05:01,500
and yet this does not mean no

00:04:58,860 --> 00:05:03,690
responsibility is taken having what we

00:05:01,500 --> 00:05:05,310
call a just culture means that you're

00:05:03,690 --> 00:05:08,550
making effort to balance safety and

00:05:05,310 --> 00:05:10,949
accountability and that people who made

00:05:08,550 --> 00:05:12,599
the mistake are empowered and enabled to

00:05:10,949 --> 00:05:16,440
give a detailed account without fear or

00:05:12,599 --> 00:05:20,039
punishment of retribution now this does

00:05:16,440 --> 00:05:22,199
not mean that etsy has a cushy kumbaya

00:05:20,039 --> 00:05:24,030
environment rather we don't tolerate

00:05:22,199 --> 00:05:27,659
pointing fingers we don't tolerate

00:05:24,030 --> 00:05:30,360
 and we recognize that failure

00:05:27,659 --> 00:05:32,310
happens it's not a surprise there will

00:05:30,360 --> 00:05:35,039
be bugs this is a foregone conclusion

00:05:32,310 --> 00:05:37,560
and working with in working with complex

00:05:35,039 --> 00:05:41,699
systems we believe that failure is

00:05:37,560 --> 00:05:44,280
inevitable at the same time expensive

00:05:41,699 --> 00:05:46,680
failure is not inevitable we often ask

00:05:44,280 --> 00:05:48,960
ourselves how we can fail quickly how we

00:05:46,680 --> 00:05:50,940
can fail in a small way how we can fail

00:05:48,960 --> 00:05:52,050
in a way since we're an e-commerce site

00:05:50,940 --> 00:05:54,300
that doesn't prevent people from

00:05:52,050 --> 00:05:56,190
checking out or doesn't for example

00:05:54,300 --> 00:06:00,360
prevent people from making new accounts

00:05:56,190 --> 00:06:02,520
on our website one way in which we keep

00:06:00,360 --> 00:06:05,099
our failures small is by using tools and

00:06:02,520 --> 00:06:06,419
that's what I'll be discussing today how

00:06:05,099 --> 00:06:08,460
myself and a team of four other

00:06:06,419 --> 00:06:11,190
engineers use tools to complete a very

00:06:08,460 --> 00:06:14,310
daunting data migration project who has

00:06:11,190 --> 00:06:18,990
completed a data migration project okay

00:06:14,310 --> 00:06:21,030
so you know it sucks it blows and the

00:06:18,990 --> 00:06:23,190
reason it blows is because it's scary

00:06:21,030 --> 00:06:25,409
it's scary to lose data it's scary to

00:06:23,190 --> 00:06:27,780
corrupt data so you have to move safely

00:06:25,409 --> 00:06:30,479
and when you're working under a deadline

00:06:27,780 --> 00:06:31,550
as we were you have to move quickly as

00:06:30,479 --> 00:06:34,160
well

00:06:31,550 --> 00:06:37,130
so why did we want to complete this

00:06:34,160 --> 00:06:38,900
migration when we began in 2005 we

00:06:37,130 --> 00:06:41,980
started storing all of our data on a

00:06:38,900 --> 00:06:45,830
postgres primary secondary database

00:06:41,980 --> 00:06:47,840
around 2011 when our growth really

00:06:45,830 --> 00:06:49,490
started skyrocketing we realized that

00:06:47,840 --> 00:06:52,370
postgres could no longer work for us

00:06:49,490 --> 00:06:55,460
first it was complete single point of

00:06:52,370 --> 00:06:57,560
failure in our system secondly we had

00:06:55,460 --> 00:06:59,870
stored procedures in the postgres schema

00:06:57,560 --> 00:07:01,850
and they were very difficult to iterate

00:06:59,870 --> 00:07:05,480
on and as I mentioned before we're used

00:07:01,850 --> 00:07:08,570
to pushing code 35 times daily so it was

00:07:05,480 --> 00:07:12,680
in terms of our engineering culture it

00:07:08,570 --> 00:07:14,600
was a counter example of that the stored

00:07:12,680 --> 00:07:17,600
procedures were also very difficult if

00:07:14,600 --> 00:07:19,550
not impossible to test lastly and

00:07:17,600 --> 00:07:21,500
perhaps most importantly postgres would

00:07:19,550 --> 00:07:23,300
also cause problems as we tried to scale

00:07:21,500 --> 00:07:25,390
in fact we got to the point where we

00:07:23,300 --> 00:07:28,520
could no longer vertically scale the box

00:07:25,390 --> 00:07:31,850
our solution to this was to implement

00:07:28,520 --> 00:07:33,920
what we call my sequel shards the

00:07:31,850 --> 00:07:36,200
following is a simplified diagram of how

00:07:33,920 --> 00:07:41,020
the shards work so we have a PHP

00:07:36,200 --> 00:07:43,370
application a unique primary key is

00:07:41,020 --> 00:07:47,060
created on what we call a ticket server

00:07:43,370 --> 00:07:50,660
this provides a globally unique primary

00:07:47,060 --> 00:07:53,150
key we then go to a database called

00:07:50,660 --> 00:07:55,850
index by default index randomly assigns

00:07:53,150 --> 00:07:59,780
which shard the record will be stored on

00:07:55,850 --> 00:08:02,000
and finally we write to the shards

00:07:59,780 --> 00:08:04,760
you'll notice that each shard has side a

00:08:02,000 --> 00:08:06,080
and side B this is a primary primary set

00:08:04,760 --> 00:08:09,020
up and prevents a single point of

00:08:06,080 --> 00:08:11,480
failure this also allows us to do schema

00:08:09,020 --> 00:08:13,910
changes while maintaining without

00:08:11,480 --> 00:08:16,010
downtime so we take for example side a

00:08:13,910 --> 00:08:19,040
out apply the schema change to it while

00:08:16,010 --> 00:08:23,540
side B is in put side a is output side a

00:08:19,040 --> 00:08:25,520
back in and do the same for side B so I

00:08:23,540 --> 00:08:29,240
mentioned that this data migration was

00:08:25,520 --> 00:08:31,850
particularly daunting that is because it

00:08:29,240 --> 00:08:36,200
infected affected the entire etsy

00:08:31,850 --> 00:08:38,780
universe in terms of the code base we

00:08:36,200 --> 00:08:41,000
began this in 2011 and what remained at

00:08:38,780 --> 00:08:44,590
the beginning of 2015 were two tables

00:08:41,000 --> 00:08:44,590
receipts and transactions

00:08:44,630 --> 00:08:50,110
here's a short list of what receipts and

00:08:47,750 --> 00:08:53,930
transactions data effects at Etsy

00:08:50,110 --> 00:08:56,120
financial reports this is especially a

00:08:53,930 --> 00:08:58,310
touchy subject for us now as I mentioned

00:08:56,120 --> 00:09:01,130
we just went public so we do not want

00:08:58,310 --> 00:09:04,400
any of our financial reports reporting

00:09:01,130 --> 00:09:06,950
false information check out so the

00:09:04,400 --> 00:09:10,220
ability to create a receipt upon

00:09:06,950 --> 00:09:12,050
checkout can actually halt the check out

00:09:10,220 --> 00:09:14,920
so if you cannot create a receipt or a

00:09:12,050 --> 00:09:17,210
transaction you're not able to check out

00:09:14,920 --> 00:09:18,740
marking a receipt as paid or shipped

00:09:17,210 --> 00:09:20,900
this is something important to both the

00:09:18,740 --> 00:09:23,240
buyers and the sellers but more so to

00:09:20,900 --> 00:09:25,760
the buyer as they've often already put

00:09:23,240 --> 00:09:26,990
money down on the item and people get

00:09:25,760 --> 00:09:30,590
very sensitive when they've already

00:09:26,990 --> 00:09:32,270
purchased something email notifications

00:09:30,590 --> 00:09:34,760
as to when the item will be shipped and

00:09:32,270 --> 00:09:37,130
arrive a seller paying their bill and

00:09:34,760 --> 00:09:39,830
our search infrastructure and Big Data

00:09:37,130 --> 00:09:42,050
jobs that make recommendations based on

00:09:39,830 --> 00:09:45,050
prior purchases so as you can see the

00:09:42,050 --> 00:09:46,460
scope of this project in addition to

00:09:45,050 --> 00:09:47,990
just being a data migration where you

00:09:46,460 --> 00:09:51,230
don't want to lose data the scope of

00:09:47,990 --> 00:09:55,400
this project was enormous so we made a

00:09:51,230 --> 00:09:58,130
plan first we would move rights we call

00:09:55,400 --> 00:09:59,540
this teeing rights so at each place in

00:09:58,130 --> 00:10:01,310
the code base where we were writing to

00:09:59,540 --> 00:10:04,790
postgres we would also write to the my

00:10:01,310 --> 00:10:06,920
sequel shards secondly we would backfill

00:10:04,790 --> 00:10:09,980
data from postgres to the my sequel

00:10:06,920 --> 00:10:13,490
shards data that preceded or happened

00:10:09,980 --> 00:10:15,770
before was created before rather the

00:10:13,490 --> 00:10:17,210
teeing of the rights and finally we

00:10:15,770 --> 00:10:22,130
would move reads from looking to

00:10:17,210 --> 00:10:25,760
postgres to looking at the shards moving

00:10:22,130 --> 00:10:29,380
reads might sound simple it was not it

00:10:25,760 --> 00:10:32,420
was extremely complex and the reason is

00:10:29,380 --> 00:10:34,780
I'll first go back to this slide so

00:10:32,420 --> 00:10:37,220
you'll recall our sharted architecture

00:10:34,780 --> 00:10:39,470
moving rights here was complex because

00:10:37,220 --> 00:10:42,860
of this architecture this architecture

00:10:39,470 --> 00:10:45,680
requires a compound primary key because

00:10:42,860 --> 00:10:48,890
it stores the shard that the record is

00:10:45,680 --> 00:10:51,320
stored on and also the record itself in

00:10:48,890 --> 00:10:53,600
many cases some pretty gnarly and

00:10:51,320 --> 00:10:55,930
nuanced refactoring had to be done to

00:10:53,600 --> 00:10:58,470
accomplish this and the fear was that

00:10:55,930 --> 00:11:00,570
while we were trying to t

00:10:58,470 --> 00:11:03,290
to postgres and to the shards that we

00:11:00,570 --> 00:11:06,270
would actually lose the right to

00:11:03,290 --> 00:11:11,310
postgres and lose data or corrupt data

00:11:06,270 --> 00:11:13,890
and the shard what we call the shard

00:11:11,310 --> 00:11:16,290
afire so the number the shard number

00:11:13,890 --> 00:11:18,720
that the record was stored on was not

00:11:16,290 --> 00:11:21,600
always in scope or even in the class or

00:11:18,720 --> 00:11:24,660
the inheritance tree of the right

00:11:21,600 --> 00:11:28,050
location so that's what i mean by very

00:11:24,660 --> 00:11:30,600
nuanced gnarly refactoring so to get

00:11:28,050 --> 00:11:32,160
back to this slide the most important

00:11:30,600 --> 00:11:35,310
thing we had to ensure here is that we

00:11:32,160 --> 00:11:37,950
wouldn't clobber rights to master for

00:11:35,310 --> 00:11:42,450
this we used our feature framework this

00:11:37,950 --> 00:11:44,520
is an example of an if statement using

00:11:42,450 --> 00:11:46,950
our config flags in our feature

00:11:44,520 --> 00:11:49,230
framework they act as a knob for

00:11:46,950 --> 00:11:51,540
requests so we can enable a feature as

00:11:49,230 --> 00:11:54,180
the one I have here and ramp it up

00:11:51,540 --> 00:11:56,610
assigning it a nonzero number value in

00:11:54,180 --> 00:11:59,400
the config when the feature is enabled

00:11:56,610 --> 00:12:01,410
to a hundred percent all requests will

00:11:59,400 --> 00:12:05,460
go through the control flow underneath

00:12:01,410 --> 00:12:07,920
this line so for example if we were

00:12:05,460 --> 00:12:11,060
uncertain or feeling unconfident which

00:12:07,920 --> 00:12:14,160
was often because of the complexity of

00:12:11,060 --> 00:12:16,470
this project we would use config flags

00:12:14,160 --> 00:12:19,440
and we would ramp it up first to one

00:12:16,470 --> 00:12:22,260
percent and see if receipts were being

00:12:19,440 --> 00:12:26,190
still created and updated properly on

00:12:22,260 --> 00:12:28,920
postgres once we were sure that we were

00:12:26,190 --> 00:12:31,970
writing both to postgres and my sequel

00:12:28,920 --> 00:12:34,770
and all locations across the code base

00:12:31,970 --> 00:12:38,580
we were then ready to backfill and

00:12:34,770 --> 00:12:41,670
verify data we had to verify that each

00:12:38,580 --> 00:12:46,020
record in Postgres was also on my sequel

00:12:41,670 --> 00:12:48,720
and that all columns correspondent

00:12:46,020 --> 00:12:52,470
correctly we call this step back fill in

00:12:48,720 --> 00:12:56,150
verification for this we used gear man

00:12:52,470 --> 00:12:59,540
which is an asynchronous job a job queue

00:12:56,150 --> 00:13:02,580
I'll get back to this slide but first

00:12:59,540 --> 00:13:04,890
this step back fill and verification

00:13:02,580 --> 00:13:07,530
necessitated a lot of bulk querying for

00:13:04,890 --> 00:13:09,010
efficiency the danger here is that we

00:13:07,530 --> 00:13:12,400
would take down index

00:13:09,010 --> 00:13:13,780
if you hadn't noticed before this is our

00:13:12,400 --> 00:13:16,600
new single point of failure in our

00:13:13,780 --> 00:13:19,840
system and I hope that next year I'm

00:13:16,600 --> 00:13:22,630
able to give a talk about how we fixed

00:13:19,840 --> 00:13:24,250
that but currently this is a single

00:13:22,630 --> 00:13:28,390
point of failure in our system so if you

00:13:24,250 --> 00:13:30,490
take down index the site's down so for

00:13:28,390 --> 00:13:34,150
backfill and verification we didn't want

00:13:30,490 --> 00:13:36,760
to pummel index and risk-taking the site

00:13:34,150 --> 00:13:38,830
down because we wanted to do this data

00:13:36,760 --> 00:13:42,850
migration entirely online with no

00:13:38,830 --> 00:13:48,120
downtime so we use this asynchronous job

00:13:42,850 --> 00:13:51,670
queue as to not hit index every time we

00:13:48,120 --> 00:13:55,000
needed to look up a record on the shards

00:13:51,670 --> 00:13:57,610
so you can see here that this is the

00:13:55,000 --> 00:14:00,040
gear man architecture jobs are queued in

00:13:57,610 --> 00:14:02,560
a job server where they wait this is the

00:14:00,040 --> 00:14:04,390
asynchronous part and then once a job is

00:14:02,560 --> 00:14:07,810
released a worker process completes the

00:14:04,390 --> 00:14:10,240
job pummelling index with queries would

00:14:07,810 --> 00:14:11,650
likely take down the site so instead we

00:14:10,240 --> 00:14:13,690
wrote a gear man job that would query

00:14:11,650 --> 00:14:16,180
only so many records at a time and in

00:14:13,690 --> 00:14:20,880
that way we were able to rate limit the

00:14:16,180 --> 00:14:24,760
the hitting or the querying of index

00:14:20,880 --> 00:14:26,650
finally we moved reads this was a giant

00:14:24,760 --> 00:14:29,890
task and perhaps the most intimidating

00:14:26,650 --> 00:14:32,400
part of the project because accessing

00:14:29,890 --> 00:14:34,570
this data happened all over the codebase

00:14:32,400 --> 00:14:36,460
we also had to make sure that the

00:14:34,570 --> 00:14:38,920
methods in each of the finder classes

00:14:36,460 --> 00:14:41,680
were in parity this took a lot of

00:14:38,920 --> 00:14:44,410
careful auditing time implementing time

00:14:41,680 --> 00:14:46,960
and testing time we had to ensure that

00:14:44,410 --> 00:14:49,980
each query was returning the same data

00:14:46,960 --> 00:14:52,780
from both both postgres and the shards

00:14:49,980 --> 00:14:55,480
we started moving reads in July this

00:14:52,780 --> 00:14:57,100
past July and the project was the

00:14:55,480 --> 00:15:00,730
deadline for the project was the end of

00:14:57,100 --> 00:15:03,130
this September so the risk here was that

00:15:00,730 --> 00:15:05,650
because the scope and the number of read

00:15:03,130 --> 00:15:09,220
sites was so great that we wouldn't

00:15:05,650 --> 00:15:11,080
complete the project on time this is

00:15:09,220 --> 00:15:14,290
significant because we are an e-commerce

00:15:11,080 --> 00:15:16,510
site as I mentioned and we do not want

00:15:14,290 --> 00:15:20,920
significant changes happening to the

00:15:16,510 --> 00:15:21,550
site during us holiday shopping time

00:15:20,920 --> 00:15:23,810
which

00:15:21,550 --> 00:15:26,509
essentially starts in October and goes

00:15:23,810 --> 00:15:28,129
through the end of the year so by not

00:15:26,509 --> 00:15:30,709
finishing in September this would have

00:15:28,129 --> 00:15:35,300
pushed the project into 2016 which was a

00:15:30,709 --> 00:15:37,279
big no-no for us so we use spatch and

00:15:35,300 --> 00:15:39,680
this is the patch file and we applied it

00:15:37,279 --> 00:15:44,059
globally so this is not a perfect

00:15:39,680 --> 00:15:50,120
solution and the reason is very specific

00:15:44,059 --> 00:15:53,829
to xes architecture but we have in the

00:15:50,120 --> 00:15:58,720
sharded world in the my sequel world

00:15:53,829 --> 00:16:01,759
records stored by shop and by user and

00:15:58,720 --> 00:16:03,620
we call these variants so what would

00:16:01,759 --> 00:16:05,600
have been ideal is to have a mechanism

00:16:03,620 --> 00:16:08,420
that could detect whether a postgres

00:16:05,600 --> 00:16:10,790
receipt actually correspondent to a user

00:16:08,420 --> 00:16:15,019
receipt or a shop receipt on the shards

00:16:10,790 --> 00:16:20,360
this is completely a human task it could

00:16:15,019 --> 00:16:22,129
not be machine automated so we this is a

00:16:20,360 --> 00:16:24,610
pretty good solution it's not a perfect

00:16:22,129 --> 00:16:26,720
one but it ended up saving us and

00:16:24,610 --> 00:16:29,839
allowing us to finish the project on

00:16:26,720 --> 00:16:35,600
time the reason is that in this tea

00:16:29,839 --> 00:16:38,089
finder we built a tool to compare the

00:16:35,600 --> 00:16:40,610
results that were being returned from

00:16:38,089 --> 00:16:42,290
postgres and from my sequel and so we

00:16:40,610 --> 00:16:44,180
were able to pinpoint where in the

00:16:42,290 --> 00:16:47,930
codebase those results were not the same

00:16:44,180 --> 00:16:50,269
and to fix them individually I'll

00:16:47,930 --> 00:16:52,399
reiterate the doing this manually would

00:16:50,269 --> 00:16:57,319
have caused us to not finish the project

00:16:52,399 --> 00:16:58,759
on time doing these by hand so late in

00:16:57,319 --> 00:17:00,769
the project I think would have also

00:16:58,759 --> 00:17:02,959
caused a significant amount of

00:17:00,769 --> 00:17:05,230
engineering burnout again this was a

00:17:02,959 --> 00:17:08,030
nine-month data migration project and

00:17:05,230 --> 00:17:10,010
the longer as you know a long term

00:17:08,030 --> 00:17:12,919
project goes on the more at risk you are

00:17:10,010 --> 00:17:14,899
for engineering fatigue and burnout by

00:17:12,919 --> 00:17:16,970
using tools to automate and protect

00:17:14,899 --> 00:17:21,589
ourselves we largely avoided this kind

00:17:16,970 --> 00:17:23,780
of engineering burn out again at Etsy we

00:17:21,589 --> 00:17:27,049
acknowledge that failure is possible and

00:17:23,780 --> 00:17:29,059
even inevitable here are five etsy

00:17:27,049 --> 00:17:31,880
employees last Halloween dressed up as a

00:17:29,059 --> 00:17:34,190
sort-of-famous bug that we wrote that

00:17:31,880 --> 00:17:34,530
caused five stars in the etsy app to

00:17:34,190 --> 00:17:39,240
show

00:17:34,530 --> 00:17:40,500
as for stars and a horse when you

00:17:39,240 --> 00:17:43,260
acknowledge that failure is inevitable

00:17:40,500 --> 00:17:44,820
but that expensive failure is not you

00:17:43,260 --> 00:17:47,370
can actually celebrate your mistakes

00:17:44,820 --> 00:17:49,440
rather than being ashamed of them you

00:17:47,370 --> 00:17:51,240
can learn from them you can build a

00:17:49,440 --> 00:17:53,760
safer system you can protect yourself

00:17:51,240 --> 00:18:07,700
from expensive failure you can use tools

00:17:53,760 --> 00:18:07,700
for salvation thank you questions hi

00:18:26,100 --> 00:18:32,590
so the question was why did we not use

00:18:29,080 --> 00:18:34,990
and out-of-the-box solution as opposed

00:18:32,590 --> 00:18:37,750
to the shards and then the second

00:18:34,990 --> 00:18:40,990
question was sorry I'm really jet-lagged

00:18:37,750 --> 00:18:44,740
remind me how do we scale out so the

00:18:40,990 --> 00:18:46,480
first the answer is that we inherited a

00:18:44,740 --> 00:18:50,470
large number of engineers from Flickr

00:18:46,480 --> 00:18:53,260
and Flickr had a pretty funny and famous

00:18:50,470 --> 00:18:55,179
problem where during the two thousand

00:18:53,260 --> 00:18:59,890
eight election president is everyone

00:18:55,179 --> 00:19:02,340
familiar with Flickr okay okay during

00:18:59,890 --> 00:19:04,630
the 2008 u.s. presidential election

00:19:02,340 --> 00:19:07,000
President Obama created a flickr account

00:19:04,630 --> 00:19:09,910
and took the site down because it was so

00:19:07,000 --> 00:19:11,950
popular so the engineers from Flickr

00:19:09,910 --> 00:19:14,110
learned from that and brought their

00:19:11,950 --> 00:19:17,410
experience over to Etsy and what the

00:19:14,110 --> 00:19:20,770
shards allows is a reapportionment of

00:19:17,410 --> 00:19:22,120
the data and so that goes to your net

00:19:20,770 --> 00:19:23,679
speaks to your next question which is

00:19:22,120 --> 00:19:31,200
how do we scale we scale horizontally

00:19:23,679 --> 00:19:40,120
and the shards now my slide is a lie um

00:19:31,200 --> 00:19:44,340
let me go back to it so okay so ooh yeah

00:19:40,120 --> 00:19:47,919
this is a this is a total lie I'm sorry

00:19:44,340 --> 00:19:50,650
the reason it's a lie is that it make it

00:19:47,919 --> 00:19:53,799
makes it seem that the shards are one

00:19:50,650 --> 00:19:56,380
box one database per box we actually

00:19:53,799 --> 00:19:59,110
have logical shards so we have multiple

00:19:56,380 --> 00:20:03,520
databases per box and so we can scale by

00:19:59,110 --> 00:20:05,799
moving data around as we add boxes does

00:20:03,520 --> 00:20:11,190
that answer your question other

00:20:05,799 --> 00:20:11,190
questions yes

00:20:22,000 --> 00:20:44,120
yeah the question was why didn't we

00:20:42,200 --> 00:20:46,970
consider a consistent hashing algorithm

00:20:44,120 --> 00:20:48,170
rather than a compound primary key I

00:20:46,970 --> 00:20:56,200
think that would have been a better

00:20:48,170 --> 00:20:56,200
better way to go in hindsight yeah yes

00:20:57,490 --> 00:21:03,380
no the question was do we do cross char

00:21:01,370 --> 00:21:05,510
joins no we do not we try to keep our

00:21:03,380 --> 00:21:09,800
sequel as simple as possible it does

00:21:05,510 --> 00:21:12,880
happen in the codebase it's very rare we

00:21:09,800 --> 00:21:15,880
try to keep it as PK lookups globally

00:21:12,880 --> 00:21:15,880
yes

00:21:22,850 --> 00:21:27,690
so the question was where do we store

00:21:25,080 --> 00:21:30,240
files on the shards or some other place

00:21:27,690 --> 00:21:35,130
the answer is we have an entire photos

00:21:30,240 --> 00:21:37,290
stack those are actually stored in in

00:21:35,130 --> 00:21:42,230
our own database but not on the shards

00:21:37,290 --> 00:21:59,790
and eventually in s3 once they get older

00:21:42,230 --> 00:22:01,770
is that what you were asking yeah yes so

00:21:59,790 --> 00:22:04,770
the question is how did we make index

00:22:01,770 --> 00:22:09,270
not not a single point of failure we

00:22:04,770 --> 00:22:13,580
haven't yet no we hope to that's some of

00:22:09,270 --> 00:22:13,580
the work that will be doing in 2016 I

00:22:13,850 --> 00:22:19,200
don't know I think distributing index

00:22:16,440 --> 00:22:20,970
would be extremely complicated so that's

00:22:19,200 --> 00:22:22,890
why we haven't done it yet we we don't

00:22:20,970 --> 00:22:26,340
have any there's no clear road full

00:22:22,890 --> 00:22:33,179
forward do you have any ideas oh all

00:22:26,340 --> 00:22:35,510
right any more questions okay thank you

00:22:33,179 --> 00:22:35,510

YouTube URL: https://www.youtube.com/watch?v=4cLHns5a-Cw


