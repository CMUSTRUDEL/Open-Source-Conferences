Title: Who Watches the Watchmen - Arup Chakrabarti
Publication date: 2015-11-01
Playlist: DevOps Days Tel Aviv 2015
Description: 
	http://www.devopsdays.org/events/2015-telaviv/

At PagerDuty, we are building out our highly available service for managing incidents properly, but how does PagerDuty monitor PagerDuty? In this talk we will cover the strategies that PagerDuty uses to always make sure our service is always up and running.

About the speaker - Arup Chakrabarti

Arup has been working in the space of software operations since 2007. He started out at as an Operations Engineer at Amazon, helping to reduce customer defects with multiple teams for the Amazon Marketplace. Since then, he has managed and built operations teams at Amazon and Netflix to help improve availability and reliability. He currently works at PagerDuty, where he is part of the Infrastructure Engineering group.
Captions: 
	00:00:11,389 --> 00:00:20,910
our next talk is it's from route from

00:00:18,090 --> 00:00:23,360
pager duty and I want to say a few

00:00:20,910 --> 00:00:27,000
personal words about duty

00:00:23,360 --> 00:00:29,010
so you know when my phone rings and in

00:00:27,000 --> 00:00:31,890
the middle of the night it's usually

00:00:29,010 --> 00:00:35,490
pager duty so I fix whatever needs

00:00:31,890 --> 00:00:37,790
fixing which is the easy part the more

00:00:35,490 --> 00:00:41,280
difficult part is getting back to sleep

00:00:37,790 --> 00:00:43,680
so what I do is I close my eyes and I

00:00:41,280 --> 00:00:46,860
imagine how the world would be if pager

00:00:43,680 --> 00:00:49,290
duty had downtime would be a great place

00:00:46,860 --> 00:00:53,000
everybody's sound flippin sound in their

00:00:49,290 --> 00:00:56,520
bed you know dreaming great dreams and

00:00:53,000 --> 00:00:58,649
our next speaker is our root chakra

00:00:56,520 --> 00:01:01,620
party giacoppetti director of

00:00:58,649 --> 00:01:04,290
engineering pager duty and he's going to

00:01:01,620 --> 00:01:07,439
explain explain why my dream is not

00:01:04,290 --> 00:01:09,619
going to come true so please welcome a

00:01:07,439 --> 00:01:09,619
rube

00:01:21,680 --> 00:01:29,100
nope there we go haha there there's my

00:01:25,940 --> 00:01:30,930
boisterous voice so thank you for that

00:01:29,100 --> 00:01:33,570
welcome to ty thank you for the intro

00:01:30,930 --> 00:01:35,790
it's been fantastic being here in

00:01:33,570 --> 00:01:38,880
tel-aviv a lot of hummus a lot of

00:01:35,790 --> 00:01:42,060
shwarma so far so I'm enjoying myself

00:01:38,880 --> 00:01:43,680
for my first trip to Tel Aviv so the

00:01:42,060 --> 00:01:45,869
time of my talk is who watches the

00:01:43,680 --> 00:01:48,390
Watchmen and for those who don't get the

00:01:45,869 --> 00:01:50,640
reference the Watchmen is a 1985 graphic

00:01:48,390 --> 00:01:53,399
novel about a group of vigilante heroes

00:01:50,640 --> 00:01:55,080
that look over the world and the course

00:01:53,399 --> 00:01:56,759
run into corruption problems and then

00:01:55,080 --> 00:01:58,800
everyone starts asking well who the heck

00:01:56,759 --> 00:02:00,539
is watching the Watchmen and so that's

00:01:58,800 --> 00:02:02,520
what I'm gonna explain to you guys who

00:02:00,539 --> 00:02:05,550
watches over page duty to make sure that

00:02:02,520 --> 00:02:08,670
we are working properly as not a corrupt

00:02:05,550 --> 00:02:11,190
vigilante organization so I have my

00:02:08,670 --> 00:02:12,959
twitter handle up here and my email

00:02:11,190 --> 00:02:15,720
address if you do have feedback please

00:02:12,959 --> 00:02:17,730
feel free to send it my way please do

00:02:15,720 --> 00:02:20,310
not troll me during the talk feel free

00:02:17,730 --> 00:02:21,600
to do it afterwards though so first off

00:02:20,310 --> 00:02:23,850
what is paid ready to give a little bit

00:02:21,600 --> 00:02:28,620
of context actually quick show of hands

00:02:23,850 --> 00:02:30,420
who has heard of pager duty before okay

00:02:28,620 --> 00:02:32,340
but Wow more than I thought about three

00:02:30,420 --> 00:02:35,370
quarters okay so for folks that haven't

00:02:32,340 --> 00:02:37,079
asked your ops person and they will tell

00:02:35,370 --> 00:02:38,700
you basically we're an incident

00:02:37,079 --> 00:02:39,690
management service we provide on-call

00:02:38,700 --> 00:02:42,120
tooling things like scheduling

00:02:39,690 --> 00:02:43,920
escalation policies we integrate with a

00:02:42,120 --> 00:02:46,049
lot of existing monitoring tools and

00:02:43,920 --> 00:02:48,470
really our goal is to notify and alert

00:02:46,049 --> 00:02:51,329
the right person every single time so a

00:02:48,470 --> 00:02:54,660
better than a better graphic here is

00:02:51,329 --> 00:02:59,100
stuff happens we make magic and then we

00:02:54,660 --> 00:03:00,359
tell people via SMS phone email and push

00:02:59,100 --> 00:03:02,600
notifications that they are having a

00:03:00,359 --> 00:03:05,310
problem so why do we care about

00:03:02,600 --> 00:03:08,280
monitoring and alerting well honestly

00:03:05,310 --> 00:03:10,829
it's because you guys are customers care

00:03:08,280 --> 00:03:12,359
about monitoring and alerting so I get

00:03:10,829 --> 00:03:13,769
to refresh this graph every couple

00:03:12,359 --> 00:03:15,690
months when we get to see our highest

00:03:13,769 --> 00:03:18,870
traffic spikes so this was the leap

00:03:15,690 --> 00:03:21,480
second back in June and basically this

00:03:18,870 --> 00:03:23,549
spike of about 5x traffic happened to us

00:03:21,480 --> 00:03:26,880
in about five minutes so a lot of you

00:03:23,549 --> 00:03:30,300
had very very rough mornings our light

00:03:26,880 --> 00:03:32,069
our late nights when June was flipping

00:03:30,300 --> 00:03:33,280
over to July and we see it we actually

00:03:32,069 --> 00:03:34,750
see when the Internet

00:03:33,280 --> 00:03:37,420
is having broader problems because we

00:03:34,750 --> 00:03:39,400
get to capture graphs like this actually

00:03:37,420 --> 00:03:40,990
reference to the talks earlier this is

00:03:39,400 --> 00:03:43,030
our driving business metric this is the

00:03:40,990 --> 00:03:44,350
metric that tells us whether pager Duty

00:03:43,030 --> 00:03:45,670
is working properly or not at this

00:03:44,350 --> 00:03:48,160
flatlines that's when we know we're

00:03:45,670 --> 00:03:49,690
having a problem so what are we going to

00:03:48,160 --> 00:03:50,950
be talking about today of you know cover

00:03:49,690 --> 00:03:52,330
a little bit about what is paged you

00:03:50,950 --> 00:03:53,800
already talked a little bit about

00:03:52,330 --> 00:03:55,870
philosophies that we have towards

00:03:53,800 --> 00:03:57,459
monitoring and alerting then go into the

00:03:55,870 --> 00:03:59,230
actual tools that we use I'd like to get

00:03:57,459 --> 00:04:01,989
very specific about the actual vendors

00:03:59,230 --> 00:04:03,150
and tools that we've built also want to

00:04:01,989 --> 00:04:05,400
talk up a little bit about security

00:04:03,150 --> 00:04:07,570
monitoring and alerting that's kind of a

00:04:05,400 --> 00:04:10,300
versioning of you know a up-and-coming

00:04:07,570 --> 00:04:12,730
topic within within the industry is how

00:04:10,300 --> 00:04:14,350
to monitor security effectively then

00:04:12,730 --> 00:04:16,150
talk a little bit about distributed

00:04:14,350 --> 00:04:17,950
systems and how that changes your

00:04:16,150 --> 00:04:20,500
alerting talk about dependency

00:04:17,950 --> 00:04:22,570
management and dependency monitoring how

00:04:20,500 --> 00:04:24,160
we cheat using chef and configuration

00:04:22,570 --> 00:04:26,050
management how we validate alerts and

00:04:24,160 --> 00:04:28,900
then if we have time we'll have some Q&A

00:04:26,050 --> 00:04:31,330
as well at the end so a quick disclaimer

00:04:28,900 --> 00:04:33,430
I did not come up with everything I'm

00:04:31,330 --> 00:04:34,450
talking about today I get I have the

00:04:33,430 --> 00:04:36,310
privilege of working with a lot of

00:04:34,450 --> 00:04:38,169
brilliant people at page duty and in

00:04:36,310 --> 00:04:39,490
previous companies so I do know would

00:04:38,169 --> 00:04:41,050
pretend for a second that I came up with

00:04:39,490 --> 00:04:43,479
everything or built anything I'm very

00:04:41,050 --> 00:04:45,700
lucky another thing is that slides will

00:04:43,479 --> 00:04:47,050
be posted online - speaker deck probably

00:04:45,700 --> 00:04:48,610
later tonight or tomorrow so if you

00:04:47,050 --> 00:04:50,950
don't get to take a picture don't worry

00:04:48,610 --> 00:04:52,720
it'll all be posted and I will probably

00:04:50,950 --> 00:04:55,660
make some American cultural references

00:04:52,720 --> 00:04:58,210
so if there's a joke that does not is

00:04:55,660 --> 00:05:00,310
not funny I apologize it's because of my

00:04:58,210 --> 00:05:02,200
American cultural references so

00:05:00,310 --> 00:05:04,479
philosophy so the first one is using the

00:05:02,200 --> 00:05:06,940
right tool this is a mistake that I see

00:05:04,479 --> 00:05:09,310
a lot of companies make where they

00:05:06,940 --> 00:05:10,450
basically decide to use a single tool

00:05:09,310 --> 00:05:12,910
and they say oh yeah we have the

00:05:10,450 --> 00:05:14,560
monitoring they have one monitoring tool

00:05:12,910 --> 00:05:16,240
and that's enough and so they they make

00:05:14,560 --> 00:05:18,070
this mistake of just trying to shove

00:05:16,240 --> 00:05:19,660
everything in there and you know the

00:05:18,070 --> 00:05:21,250
example I use bring up is time series

00:05:19,660 --> 00:05:23,440
data versus of that data those are two

00:05:21,250 --> 00:05:25,210
very different kinds of events sources

00:05:23,440 --> 00:05:26,680
are data sources and you shouldn't be

00:05:25,210 --> 00:05:28,270
using the same monitoring tool for that

00:05:26,680 --> 00:05:30,190
event so you can model as discrete

00:05:28,270 --> 00:05:31,720
impulse functions that happen whereas

00:05:30,190 --> 00:05:35,590
time series data tend to have some

00:05:31,720 --> 00:05:36,610
seasonality to it or curves to it so

00:05:35,590 --> 00:05:39,850
that's why you should be using different

00:05:36,610 --> 00:05:41,890
tools for that avoid single host

00:05:39,850 --> 00:05:43,570
monitoring we've learned this painfully

00:05:41,890 --> 00:05:45,630
way too many times where we have a

00:05:43,570 --> 00:05:47,520
single box monitoring other

00:05:45,630 --> 00:05:49,770
and then that box goes away and guess

00:05:47,520 --> 00:05:51,780
what because the monitoring that before

00:05:49,770 --> 00:05:53,610
that box is also on that box now we've

00:05:51,780 --> 00:05:55,650
lost visibility across the entire sack

00:05:53,610 --> 00:05:57,960
so we always tell folks build in some

00:05:55,650 --> 00:06:00,450
redundancy to your alerting and to your

00:05:57,960 --> 00:06:02,550
monitoring another thing is you should

00:06:00,450 --> 00:06:05,010
alert on what customers care about now

00:06:02,550 --> 00:06:07,470
when I say customers I don't always mean

00:06:05,010 --> 00:06:10,140
end-users to say you're a DBA team and

00:06:07,470 --> 00:06:11,730
you know your end users of course

00:06:10,140 --> 00:06:13,320
customers of the entire company but your

00:06:11,730 --> 00:06:15,390
immediate customers might be your other

00:06:13,320 --> 00:06:17,010
software teams that you work with so you

00:06:15,390 --> 00:06:18,930
know maybe you want to monitor the slow

00:06:17,010 --> 00:06:20,610
query log or you want to monitor you

00:06:18,930 --> 00:06:22,680
know roll back transactions on your

00:06:20,610 --> 00:06:24,570
database but those are things that York

00:06:22,680 --> 00:06:25,860
immediate customers care about those are

00:06:24,570 --> 00:06:28,710
the things that you need to be alerting

00:06:25,860 --> 00:06:31,020
on another thing is to alert on high and

00:06:28,710 --> 00:06:32,880
low values basically this idea that you

00:06:31,020 --> 00:06:34,740
should be creating bands for all of your

00:06:32,880 --> 00:06:36,390
alerts so this is just a graph are sort

00:06:34,740 --> 00:06:39,930
of metrics I pull it's hard to see but

00:06:36,390 --> 00:06:43,710
basically this alert up here is looking

00:06:39,930 --> 00:06:45,630
at load when it goes above 0.5 then

00:06:43,710 --> 00:06:47,670
it'll trigger an alert but just as

00:06:45,630 --> 00:06:49,470
important if it goes below point 1

00:06:47,670 --> 00:06:51,690
trigger the alert as well because that's

00:06:49,470 --> 00:06:55,080
an indication of work not being done in

00:06:51,690 --> 00:06:56,460
the system make itself service this is

00:06:55,080 --> 00:06:58,320
something that I keep seeing over and

00:06:56,460 --> 00:06:59,760
over and over and over again is that if

00:06:58,320 --> 00:07:01,860
you make your alerting and monitoring

00:06:59,760 --> 00:07:02,970
tools hard to use your other development

00:07:01,860 --> 00:07:04,560
teams and software teams they're not

00:07:02,970 --> 00:07:05,940
going to use it they have to be able to

00:07:04,560 --> 00:07:08,220
go in and create their own alerts

00:07:05,940 --> 00:07:09,660
whether it's via config files a web UI

00:07:08,220 --> 00:07:10,980
whatever it is they have to be able to

00:07:09,660 --> 00:07:12,900
go in and do it themselves without

00:07:10,980 --> 00:07:14,420
talking to anyone else because you want

00:07:12,900 --> 00:07:16,650
to encourage them to do that

00:07:14,420 --> 00:07:18,930
another is validate that your alerts

00:07:16,650 --> 00:07:22,020
work this is hilarious how many times

00:07:18,930 --> 00:07:24,120
I've seen a post mortem where they'll be

00:07:22,020 --> 00:07:25,920
probably about 40 minute delay in the

00:07:24,120 --> 00:07:28,020
post mortem and the reason is because

00:07:25,920 --> 00:07:29,400
they they had an alert setup but they

00:07:28,020 --> 00:07:30,720
didn't actually validate that it worked

00:07:29,400 --> 00:07:32,370
they didn't validate that the alerts

00:07:30,720 --> 00:07:35,310
actually were triggering when there was

00:07:32,370 --> 00:07:37,170
an error condition and and that leads to

00:07:35,310 --> 00:07:40,290
a much longer time to resolution for

00:07:37,170 --> 00:07:41,550
that so let's get into the tools that we

00:07:40,290 --> 00:07:43,620
use so we use a new relic it's a

00:07:41,550 --> 00:07:45,630
application performance monitoring tool

00:07:43,620 --> 00:07:47,580
our management tool and APM as they call

00:07:45,630 --> 00:07:49,410
it and with new relic you get a lot of

00:07:47,580 --> 00:07:52,050
nice graphs so this is our

00:07:49,410 --> 00:07:54,870
our latency profile for one of our API

00:07:52,050 --> 00:07:56,850
endpoints and you get other nice graphs

00:07:54,870 --> 00:07:58,440
so this is I guess is app restarts by

00:07:56,850 --> 00:08:00,360
host so I guess we had three host

00:07:58,440 --> 00:08:02,850
restart and you get more graphs and you

00:08:00,360 --> 00:08:04,800
get wall clock time and more colors and

00:08:02,850 --> 00:08:08,070
blues and greens and browns oh and you

00:08:04,800 --> 00:08:09,600
get even lines and bars and and this is

00:08:08,070 --> 00:08:11,580
respond and throughput okay great and

00:08:09,600 --> 00:08:13,410
you get reports you get numbers that you

00:08:11,580 --> 00:08:15,180
can show to your finance team to your

00:08:13,410 --> 00:08:18,300
business team and more reports you get

00:08:15,180 --> 00:08:20,190
numbers and and very quickly you get

00:08:18,300 --> 00:08:22,860
overwhelmed with the amount of data that

00:08:20,190 --> 00:08:24,660
that that these APM's provide and so to

00:08:22,860 --> 00:08:25,920
me they're you know the pros of an APM

00:08:24,660 --> 00:08:28,410
especially when paid rudy was much

00:08:25,920 --> 00:08:30,870
smaller is that it was actually a really

00:08:28,410 --> 00:08:34,140
helpful tool to help us just explore it

00:08:30,870 --> 00:08:36,330
gave us a lot of data it told us just a

00:08:34,140 --> 00:08:37,650
lot of edge cases that we just would

00:08:36,330 --> 00:08:40,380
never have thought about right even had

00:08:37,650 --> 00:08:43,050
times in time to instrument new relics

00:08:40,380 --> 00:08:44,550
also great for for tracing transactions

00:08:43,050 --> 00:08:45,990
so it'll do transaction tracing

00:08:44,550 --> 00:08:47,880
basically from the load balancer to your

00:08:45,990 --> 00:08:50,250
app servers to your database you can see

00:08:47,880 --> 00:08:51,960
where a transaction is getting stuck or

00:08:50,250 --> 00:08:53,580
request is slowing down in your system

00:08:51,960 --> 00:08:55,140
and it gives a lot of data you know I

00:08:53,580 --> 00:08:56,670
showed you some of these reports and

00:08:55,140 --> 00:08:58,590
they kind of come for free you know you

00:08:56,670 --> 00:09:01,230
just swipe your credit card and you get

00:08:58,590 --> 00:09:03,420
a lot of data cons they can be overly

00:09:01,230 --> 00:09:05,520
prescriptive so as your business grows

00:09:03,420 --> 00:09:07,200
as you start to understand your customer

00:09:05,520 --> 00:09:09,090
behavior as you start to know the

00:09:07,200 --> 00:09:10,890
seasonality of your business

00:09:09,090 --> 00:09:12,540
New Relic is gonna tell you know here's

00:09:10,890 --> 00:09:14,460
the things you care about whereas again

00:09:12,540 --> 00:09:16,260
as you mature as a business you start to

00:09:14,460 --> 00:09:17,430
realize that's not always true you know

00:09:16,260 --> 00:09:19,980
new relics not always gonna be able to

00:09:17,430 --> 00:09:22,200
do that for you another thing is that it

00:09:19,980 --> 00:09:24,060
can be hard to tune and customize to

00:09:22,200 --> 00:09:25,830
your needs again going back to that it

00:09:24,060 --> 00:09:27,870
being overly prescriptive you want to

00:09:25,830 --> 00:09:29,430
add in custom metrics and counters it's

00:09:27,870 --> 00:09:31,590
a little bit harder in these services

00:09:29,430 --> 00:09:32,910
and again I say you know it's a both

00:09:31,590 --> 00:09:34,770
good and a bad thing and gives a lot of

00:09:32,910 --> 00:09:36,300
data it can be very overwhelming and

00:09:34,770 --> 00:09:38,310
kind of hard to figure out what's the

00:09:36,300 --> 00:09:42,030
signal from the noise in a lot of these

00:09:38,310 --> 00:09:44,430
tools so another tool we use is data dog

00:09:42,030 --> 00:09:47,400
and stats D so who's here familiar with

00:09:44,430 --> 00:09:49,320
stats D from Etsy ok about a third

00:09:47,400 --> 00:09:52,440
basically it's it's a stats daemon that

00:09:49,320 --> 00:09:54,030
you install on your servers it's UDP so

00:09:52,440 --> 00:09:56,400
you send your metrics to it it's

00:09:54,030 --> 00:09:57,230
fire-and-forget and its non-blocking so

00:09:56,400 --> 00:09:58,940
it's it's very

00:09:57,230 --> 00:10:01,130
performant and it's really easy to use

00:09:58,940 --> 00:10:02,870
so you're gonna slap counters gauges and

00:10:01,130 --> 00:10:04,850
histograms into your code and this is it

00:10:02,870 --> 00:10:06,890
like this is literally all it takes to

00:10:04,850 --> 00:10:10,130
instrument your application for

00:10:06,890 --> 00:10:11,720
collecting metrics this is data dog's UI

00:10:10,130 --> 00:10:12,860
so this is where our engineers are able

00:10:11,720 --> 00:10:14,720
to go in and create their own alerts

00:10:12,860 --> 00:10:16,370
this is just a set of alerts for

00:10:14,720 --> 00:10:18,530
Cassandra greedy node failures on

00:10:16,370 --> 00:10:20,210
specific boxes that again one not even

00:10:18,530 --> 00:10:23,030
our operations team set it up but one of

00:10:20,210 --> 00:10:24,920
our other engineering teams did and they

00:10:23,030 --> 00:10:26,270
have nice integration so the integrate

00:10:24,920 --> 00:10:27,860
with page eighty of course they send out

00:10:26,270 --> 00:10:29,660
emails they also integrate with slack so

00:10:27,860 --> 00:10:31,640
this is one of the engineers on my team

00:10:29,660 --> 00:10:33,590
Evan he's basically upgrading our

00:10:31,640 --> 00:10:36,470
Cassandra cluster and he's just showing

00:10:33,590 --> 00:10:38,090
the decline in CPU utilization that

00:10:36,470 --> 00:10:42,080
happened as effect which is really nice

00:10:38,090 --> 00:10:43,880
very customizable you can change it as

00:10:42,080 --> 00:10:45,890
you grow so as you realize oh wow here's

00:10:43,880 --> 00:10:47,360
a new API endpoint that we've set up

00:10:45,890 --> 00:10:49,310
that customers are starting to use you

00:10:47,360 --> 00:10:51,530
can easily instrument that pretty easily

00:10:49,310 --> 00:10:53,360
it's a self service which I really like

00:10:51,530 --> 00:10:55,250
that all of our engineering teams can go

00:10:53,360 --> 00:10:57,470
in themselves and they can create alerts

00:10:55,250 --> 00:11:01,220
set up new metrics do whatever they want

00:10:57,470 --> 00:11:03,140
with it we did have some problems when

00:11:01,220 --> 00:11:04,520
we were first setting it up we didn't

00:11:03,140 --> 00:11:05,600
have this is about two and half years

00:11:04,520 --> 00:11:07,520
ago we didn't have the most mature

00:11:05,600 --> 00:11:09,050
configuration management in place and

00:11:07,520 --> 00:11:10,760
this actually was part of the drivers

00:11:09,050 --> 00:11:11,990
for us setting up a better config

00:11:10,760 --> 00:11:13,370
management system was that we were

00:11:11,990 --> 00:11:15,830
trying to install this agent everywhere

00:11:13,370 --> 00:11:18,020
and doing it by hand was a complete pain

00:11:15,830 --> 00:11:20,060
so that actually drove us to set up a

00:11:18,020 --> 00:11:21,200
better configure management and another

00:11:20,060 --> 00:11:23,660
thing is that actually took us a little

00:11:21,200 --> 00:11:24,770
bit more time to ramp up teams on to it

00:11:23,660 --> 00:11:26,120
and the granted this is when their

00:11:24,770 --> 00:11:28,070
company was much smaller so teams

00:11:26,120 --> 00:11:30,470
weren't as familiar with this notion of

00:11:28,070 --> 00:11:32,780
counters engages now it's kind of built

00:11:30,470 --> 00:11:34,160
into the engineering workflow but we did

00:11:32,780 --> 00:11:35,920
have to do some training initially and

00:11:34,160 --> 00:11:39,260
then it kind of took care of itself

00:11:35,920 --> 00:11:41,960
sumit logic so we use it's a similar to

00:11:39,260 --> 00:11:44,000
Splunk we use it's basically a log

00:11:41,960 --> 00:11:45,350
aggregation service and we ship all of

00:11:44,000 --> 00:11:47,330
our critical logs off the disk

00:11:45,350 --> 00:11:48,770
immediately and off to sumo logic so

00:11:47,330 --> 00:11:51,050
like things like off blog syslog

00:11:48,770 --> 00:11:52,730
application logs and basically people

00:11:51,050 --> 00:11:54,140
can run queries against it later on so

00:11:52,730 --> 00:11:55,850
and you can set up alerts so something

00:11:54,140 --> 00:11:57,140
like hey tell me how many five hundreds

00:11:55,850 --> 00:11:59,590
have in the last ten minutes if there's

00:11:57,140 --> 00:12:01,880
more than you know 1% of requests then

00:11:59,590 --> 00:12:04,550
send off an email and trigger an alert

00:12:01,880 --> 00:12:05,600
it's somewhat self service so engineers

00:12:04,550 --> 00:12:07,100
can go in and create their own alerts

00:12:05,600 --> 00:12:08,880
but the initial setup actually has to be

00:12:07,100 --> 00:12:11,310
done by by chef for

00:12:08,880 --> 00:12:12,810
so that's that's how we do it and it's

00:12:11,310 --> 00:12:14,070
hard to use for real-time debugging

00:12:12,810 --> 00:12:16,770
unfortunately we find that there's a

00:12:14,070 --> 00:12:19,110
typical three to five minute lag so when

00:12:16,770 --> 00:12:21,420
you know craps broken on production

00:12:19,110 --> 00:12:23,070
you're trying to figure out what's going

00:12:21,420 --> 00:12:24,660
on unfortunately sometimes easier just

00:12:23,070 --> 00:12:26,610
tail logs directly on the server which

00:12:24,660 --> 00:12:27,960
is a complete anti-pattern and so this

00:12:26,610 --> 00:12:30,420
is a little bit frustrating that we have

00:12:27,960 --> 00:12:32,400
to wait for that delay so the the

00:12:30,420 --> 00:12:36,210
question that I always get is does pager

00:12:32,400 --> 00:12:42,120
duty use page or duty yes we do

00:12:36,210 --> 00:12:46,470
but I'll explain that in a second so we

00:12:42,120 --> 00:12:48,690
do use it we do dog food our own product

00:12:46,470 --> 00:12:49,920
we do use it we you know internally test

00:12:48,690 --> 00:12:51,540
things to make sure that it's you know

00:12:49,920 --> 00:12:55,110
up to our own standards before releasing

00:12:51,540 --> 00:12:57,020
it into the wild but we don't get to use

00:12:55,110 --> 00:12:59,190
it for high urgency high severity

00:12:57,020 --> 00:13:00,720
catastrophic type outages so we actually

00:12:59,190 --> 00:13:03,000
have to use backup alerting for that we

00:13:00,720 --> 00:13:05,340
actually is normally in ma notice for

00:13:03,000 --> 00:13:07,050
this and so I like these tools and that

00:13:05,340 --> 00:13:07,860
they're nice and simple I also hate

00:13:07,050 --> 00:13:11,130
these tools because they're nice and

00:13:07,860 --> 00:13:12,420
simple and they it's our backup alerting

00:13:11,130 --> 00:13:14,700
system and they're very naive in their

00:13:12,420 --> 00:13:16,530
health checks basically they they'll hit

00:13:14,700 --> 00:13:18,600
us what I thought they'd do what I call

00:13:16,530 --> 00:13:21,360
a shallow thing and they hit an endpoint

00:13:18,600 --> 00:13:23,700
and they say is it up or down and in

00:13:21,360 --> 00:13:25,170
order to make this more useful for us

00:13:23,700 --> 00:13:27,570
internally we actually have to build out

00:13:25,170 --> 00:13:29,610
what we call our status our basically an

00:13:27,570 --> 00:13:31,590
internal health check page and what this

00:13:29,610 --> 00:13:34,080
health check page does is it's exposed

00:13:31,590 --> 00:13:35,940
to the internet is exposed to Wormley

00:13:34,080 --> 00:13:38,370
and monitors and what happens is when

00:13:35,940 --> 00:13:40,140
normally our monitors hits that page it

00:13:38,370 --> 00:13:41,730
lightly touches each of our internal

00:13:40,140 --> 00:13:44,370
services and then the internal service

00:13:41,730 --> 00:13:46,830
is basically exposed if I in a healthy

00:13:44,370 --> 00:13:48,060
state or not and then that page gets

00:13:46,830 --> 00:13:50,190
rendered and if there's an unexpected

00:13:48,060 --> 00:13:52,680
value like hey I'm in an unhealthy state

00:13:50,190 --> 00:13:54,630
it then triggers a completely separate

00:13:52,680 --> 00:13:56,160
channel of alerting for our engineering

00:13:54,630 --> 00:13:58,020
teams thankfully again this doesn't

00:13:56,160 --> 00:14:00,330
happen very often but this is what we

00:13:58,020 --> 00:14:01,440
had to build out and/or make our status

00:14:00,330 --> 00:14:04,860
page and these health checks a lot

00:14:01,440 --> 00:14:09,120
smarter cron monitoring who here loves

00:14:04,860 --> 00:14:11,880
cron okay more it's more than good

00:14:09,120 --> 00:14:13,500
so kroehner this is a tool that we we

00:14:11,880 --> 00:14:14,550
open sourced earlier this year we were

00:14:13,500 --> 00:14:16,470
happy we had this problem where we have

00:14:14,550 --> 00:14:18,600
hundreds of the server's running cron

00:14:16,470 --> 00:14:20,070
jobs left and right and we had very

00:14:18,600 --> 00:14:21,870
little visibility of course you know the

00:14:20,070 --> 00:14:22,170
typical way of setting up monitoring

00:14:21,870 --> 00:14:23,519
around

00:14:22,170 --> 00:14:25,560
Cron's is just used the mail to

00:14:23,519 --> 00:14:27,180
attribute and that's very brittle when

00:14:25,560 --> 00:14:28,860
there's problems it'll just it'll spam

00:14:27,180 --> 00:14:30,600
you and that's not the the best way to

00:14:28,860 --> 00:14:32,100
find out that there's a problem so we

00:14:30,600 --> 00:14:34,440
wrote Khurana it's a very thin wrapper

00:14:32,100 --> 00:14:36,060
around our cron jobs and it tracks run

00:14:34,440 --> 00:14:37,950
times and exit codes that's it it's a

00:14:36,060 --> 00:14:40,860
very simple tool it doesn't do anything

00:14:37,950 --> 00:14:43,790
that fancy we've integrated it with data

00:14:40,860 --> 00:14:46,290
dog and it will work with any other

00:14:43,790 --> 00:14:48,089
metrics back-end that you want but

00:14:46,290 --> 00:14:50,430
basically we have you know this in this

00:14:48,089 --> 00:14:51,959
example here we have a case where we

00:14:50,430 --> 00:14:53,880
have a cron job that's running stats on

00:14:51,959 --> 00:14:56,250
NTP for us and it failed after eight

00:14:53,880 --> 00:14:57,899
seconds and what's nice here is that if

00:14:56,250 --> 00:15:00,600
one if this fails once I don't really

00:14:57,899 --> 00:15:02,160
care it's a single cron job will get run

00:15:00,600 --> 00:15:03,600
later nope no big deal but if this fails

00:15:02,160 --> 00:15:05,339
a hundred times

00:15:03,600 --> 00:15:08,010
then I care and I can set up a learning

00:15:05,339 --> 00:15:09,360
on that another nice thing is it tells

00:15:08,010 --> 00:15:11,430
me how long my database backups are

00:15:09,360 --> 00:15:12,870
taking those kinds of batch jobs that

00:15:11,430 --> 00:15:14,250
you know you don't realize your database

00:15:12,870 --> 00:15:16,949
backups and failing until you have to

00:15:14,250 --> 00:15:18,630
restore it and then you realize oh crap

00:15:16,949 --> 00:15:19,260
I haven't had a bad successful backup in

00:15:18,630 --> 00:15:20,820
three weeks

00:15:19,260 --> 00:15:22,139
hope my customers didn't care about the

00:15:20,820 --> 00:15:24,750
data they sent us in the last three

00:15:22,139 --> 00:15:26,550
weeks and so now I know that our backups

00:15:24,750 --> 00:15:28,589
are succeeding and I also know that

00:15:26,550 --> 00:15:31,260
they're taking about 1900 seconds which

00:15:28,589 --> 00:15:32,910
is five and a half hours and so that

00:15:31,260 --> 00:15:36,750
gives me visibility to how long my batch

00:15:32,910 --> 00:15:40,560
jobs are taken watchdog so this was a

00:15:36,750 --> 00:15:42,360
fascinating tool that we we built out so

00:15:40,560 --> 00:15:43,649
it actually started out as a functional

00:15:42,360 --> 00:15:46,380
test suite that we were building

00:15:43,649 --> 00:15:48,149
internally at page duty and as the

00:15:46,380 --> 00:15:49,589
complexity of our product grew we want

00:15:48,149 --> 00:15:50,970
to have something that kind of worked

00:15:49,589 --> 00:15:52,769
end to end and so we built up this

00:15:50,970 --> 00:15:55,170
functional test suite as part of our CI

00:15:52,769 --> 00:15:57,449
system and then our engineer start

00:15:55,170 --> 00:15:59,339
running in production and it takes about

00:15:57,449 --> 00:16:00,959
you know 10 to 15 minutes but they just

00:15:59,339 --> 00:16:02,970
run it constantly it's just constantly

00:16:00,959 --> 00:16:04,769
running and it's modeling user behavior

00:16:02,970 --> 00:16:06,420
and what's really interesting is we've

00:16:04,769 --> 00:16:09,000
turned our functional test suite into an

00:16:06,420 --> 00:16:10,709
operational monitoring tool and this is

00:16:09,000 --> 00:16:12,630
you know this is we're running the same

00:16:10,709 --> 00:16:14,100
bamboo nothing too fancy here basically

00:16:12,630 --> 00:16:15,269
here's a couple of things that were

00:16:14,100 --> 00:16:17,220
modeling incidents behavior

00:16:15,269 --> 00:16:19,290
notifications behavior user creation

00:16:17,220 --> 00:16:21,240
just these kind of common things that

00:16:19,290 --> 00:16:22,800
our customers do and we've actually

00:16:21,240 --> 00:16:24,839
caught some meaningful things one

00:16:22,800 --> 00:16:27,540
problem we caught was DNS caching and

00:16:24,839 --> 00:16:28,889
for some of our customers we saw that

00:16:27,540 --> 00:16:31,140
ourselves because we were modeling

00:16:28,889 --> 00:16:33,269
custom customer behavior and we saw some

00:16:31,140 --> 00:16:35,430
errors in DNS and because the

00:16:33,269 --> 00:16:37,080
infrastructure that runs these tests are

00:16:35,430 --> 00:16:39,630
please outside of our own infrastructure

00:16:37,080 --> 00:16:40,860
that's how we're able to see that hey

00:16:39,630 --> 00:16:42,600
maybe our customers are seeing this

00:16:40,860 --> 00:16:43,620
problem we actually run this in a couple

00:16:42,600 --> 00:16:45,720
of different places around the globe

00:16:43,620 --> 00:16:47,279
right now so it shows us when we're

00:16:45,720 --> 00:16:49,560
seeing DNS problems in other parts of

00:16:47,279 --> 00:16:51,149
the globe we found a bug with our online

00:16:49,560 --> 00:16:53,160
schema change tool and we've even seen

00:16:51,149 --> 00:16:54,480
problems with postfix as a result of

00:16:53,160 --> 00:16:56,250
this that we wouldn't have detected

00:16:54,480 --> 00:16:59,279
unless we had something I was modeling

00:16:56,250 --> 00:17:01,290
customer behavior security

00:16:59,279 --> 00:17:04,650
who here has to work on security

00:17:01,290 --> 00:17:08,339
problems every now and then not too many

00:17:04,650 --> 00:17:10,350
so I hate audits I added czar like the

00:17:08,339 --> 00:17:11,939
bane of my existence you know we we get

00:17:10,350 --> 00:17:14,030
we work with a lot of customers and and

00:17:11,939 --> 00:17:16,949
they always ask us for audit reports and

00:17:14,030 --> 00:17:19,079
it's it's a lot of manual effort it's a

00:17:16,949 --> 00:17:21,870
lot of you know filling out paperwork

00:17:19,079 --> 00:17:24,150
and I can't stand it so instead we

00:17:21,870 --> 00:17:26,280
automated it we basically took a lot of

00:17:24,150 --> 00:17:27,839
security policies a lot of the security

00:17:26,280 --> 00:17:30,390
audits other customers wanted from us

00:17:27,839 --> 00:17:32,880
and we built that into our security

00:17:30,390 --> 00:17:34,380
monitoring suite and the way that we

00:17:32,880 --> 00:17:36,000
think about this is the earlier that we

00:17:34,380 --> 00:17:37,710
can alert on these problems that earlier

00:17:36,000 --> 00:17:39,360
it is to fix the example that I always

00:17:37,710 --> 00:17:42,870
think of is let's say an engineer goes

00:17:39,360 --> 00:17:44,670
in and disables SSL for my sequel okay

00:17:42,870 --> 00:17:46,350
yeah yeah you didn't feel like debugging

00:17:44,670 --> 00:17:48,000
that stacktrace turn off SSL hey it

00:17:46,350 --> 00:17:49,740
works and then it gets rolled on the

00:17:48,000 --> 00:17:51,510
production and everything and then weeks

00:17:49,740 --> 00:17:53,190
later you're trying to figure out why

00:17:51,510 --> 00:17:55,140
traffic's being sent over to clear and

00:17:53,190 --> 00:17:56,250
it's hard to like correlate back to what

00:17:55,140 --> 00:17:57,750
the change was whereas if you're

00:17:56,250 --> 00:17:59,970
alerting on it earlier it's easier to

00:17:57,750 --> 00:18:01,770
fix because you know that there's a

00:17:59,970 --> 00:18:04,470
limited change set that led up to that

00:18:01,770 --> 00:18:05,970
problem the other point that I was like

00:18:04,470 --> 00:18:07,679
making with the slide is that I actually

00:18:05,970 --> 00:18:09,630
do believe there are parts of security

00:18:07,679 --> 00:18:11,130
that are measurable now security is like

00:18:09,630 --> 00:18:12,780
not like performance or things like that

00:18:11,130 --> 00:18:14,520
where you can easily show like well I

00:18:12,780 --> 00:18:16,770
improved it by 50 percent that's the

00:18:14,520 --> 00:18:18,450
hard but I do believe there are parts

00:18:16,770 --> 00:18:19,679
that are measurable from a security

00:18:18,450 --> 00:18:21,900
standpoint that you can operationalize

00:18:19,679 --> 00:18:24,600
collect metrics on and then alert on it

00:18:21,900 --> 00:18:26,610
so some examples of this so we use an

00:18:24,600 --> 00:18:28,470
intrusion detection system called OS sec

00:18:26,610 --> 00:18:30,210
and this guy's just constantly running

00:18:28,470 --> 00:18:31,650
all it does is it monitors logs for

00:18:30,210 --> 00:18:33,660
anomalous behavior and it checks them to

00:18:31,650 --> 00:18:37,679
directories so say we were to get routed

00:18:33,660 --> 00:18:39,990
on a box and and attacker were to you

00:18:37,679 --> 00:18:42,420
know muck with our IP tables rules or

00:18:39,990 --> 00:18:43,770
you know change user directories or

00:18:42,420 --> 00:18:46,500
something that this guy will trigger

00:18:43,770 --> 00:18:49,140
immediately we do port scanning via and

00:18:46,500 --> 00:18:51,480
map so we have we basically scan

00:18:49,140 --> 00:18:53,400
it's one through I think 65,000

00:18:51,480 --> 00:18:55,890
constantly is just running against the

00:18:53,400 --> 00:18:57,780
entire infrastructure quit funny story

00:18:55,890 --> 00:19:01,560
so AWS does not like if you do port

00:18:57,780 --> 00:19:02,910
scans and that's they so we only do it

00:19:01,560 --> 00:19:04,860
against our own infrastructure of course

00:19:02,910 --> 00:19:06,180
and we actually got a very angry letter

00:19:04,860 --> 00:19:08,430
from their network team saying hey

00:19:06,180 --> 00:19:10,020
you're scanning up other people's ports

00:19:08,430 --> 00:19:12,090
and we had to tell them no no it's it's

00:19:10,020 --> 00:19:13,710
our own stuff trust me we're only we're

00:19:12,090 --> 00:19:16,500
only bothering ourselves here but they

00:19:13,710 --> 00:19:18,870
they do monitor this apparently we also

00:19:16,500 --> 00:19:20,550
scrape our ipsec data so we encrypt all

00:19:18,870 --> 00:19:22,770
of our traffic from a node to node basis

00:19:20,550 --> 00:19:23,760
via IPSec at the transport layer and the

00:19:22,770 --> 00:19:26,130
way that we know that it's working

00:19:23,760 --> 00:19:28,050
properly is that we that we scrape the

00:19:26,130 --> 00:19:30,360
IPSec counters and we shove that into

00:19:28,050 --> 00:19:34,170
our metric store and we alert on that so

00:19:30,360 --> 00:19:36,510
if 80% sorry if ninety percent are lower

00:19:34,170 --> 00:19:39,090
that's our threshold for the traffic

00:19:36,510 --> 00:19:40,440
being encrypted so typically our encrypt

00:19:39,090 --> 00:19:41,790
our traffic that's being cryptid on a

00:19:40,440 --> 00:19:44,520
node to node base is somewhere between

00:19:41,790 --> 00:19:46,020
90 to 95 percent and that fluctuates a

00:19:44,520 --> 00:19:48,390
little bit just because like some of its

00:19:46,020 --> 00:19:49,620
over SSL so it's not over IPSec so it's

00:19:48,390 --> 00:19:52,110
still encrypted but it's not being done

00:19:49,620 --> 00:19:54,330
my IPSec whereas let's say an engineer

00:19:52,110 --> 00:19:55,950
goes in and and disables IPSec it'll

00:19:54,330 --> 00:19:57,510
actually trigger this alert because now

00:19:55,950 --> 00:19:59,760
we're sending traffic over the wire

00:19:57,510 --> 00:20:01,650
that's not being encrypted we also have

00:19:59,760 --> 00:20:03,360
a tool that we're gonna open source at

00:20:01,650 --> 00:20:04,590
some point called the EWS auditor and

00:20:03,360 --> 00:20:07,350
what this does is basically just

00:20:04,590 --> 00:20:09,120
monitoring the cloud trail logs and it

00:20:07,350 --> 00:20:11,040
looks for giant dips so like if someone

00:20:09,120 --> 00:20:12,870
were to hack our AWS account and muck

00:20:11,040 --> 00:20:14,400
with a bunch of iam settings we would

00:20:12,870 --> 00:20:16,500
see that immediately and it's just

00:20:14,400 --> 00:20:18,150
constantly running as well we also have

00:20:16,500 --> 00:20:20,160
you know an example of you know kind of

00:20:18,150 --> 00:20:22,170
measurable security we we actually look

00:20:20,160 --> 00:20:23,970
for deep data exfiltration so let's say

00:20:22,170 --> 00:20:25,530
an attacker were to get onto a database

00:20:23,970 --> 00:20:27,270
server of ours and run a select star

00:20:25,530 --> 00:20:29,370
that will show up in our slow query log

00:20:27,270 --> 00:20:30,750
and that immediately pages out to our

00:20:29,370 --> 00:20:33,510
engineer so we can go in and mitigate

00:20:30,750 --> 00:20:35,940
the attack distributed systems

00:20:33,510 --> 00:20:37,440
monitoring so this is something I see a

00:20:35,940 --> 00:20:40,110
lot of companies especially as they grow

00:20:37,440 --> 00:20:41,640
and they they start to implement more

00:20:40,110 --> 00:20:43,230
distributed systems to handle or load

00:20:41,640 --> 00:20:45,480
they really struggle going from the

00:20:43,230 --> 00:20:47,520
single node alerting and monitoring to

00:20:45,480 --> 00:20:49,830
kind of this idea of the cluster level

00:20:47,520 --> 00:20:51,960
monitoring so in distributed systems you

00:20:49,830 --> 00:20:53,880
don't care about a single server 13500

00:20:51,960 --> 00:20:56,280
you care about the overall number of

00:20:53,880 --> 00:20:57,750
500s you don't care about the number of

00:20:56,280 --> 00:20:59,790
Cassandra nodes going down you care

00:20:57,750 --> 00:21:01,620
about the percentage of Cassandra notes

00:20:59,790 --> 00:21:03,380
going down you don't care about latency

00:21:01,620 --> 00:21:05,490
single-server you care about the overall

00:21:03,380 --> 00:21:07,470
latency and you have to invest time and

00:21:05,490 --> 00:21:09,960
effort to measure these things I see a

00:21:07,470 --> 00:21:12,240
lot of companies where they they they

00:21:09,960 --> 00:21:14,040
effectively have local scrapers for

00:21:12,240 --> 00:21:15,809
they're like nginx logs or something

00:21:14,040 --> 00:21:17,910
like that and that's not the way you can

00:21:15,809 --> 00:21:19,260
it's not sustainable because if you get

00:21:17,910 --> 00:21:20,309
you know if you're successful business

00:21:19,260 --> 00:21:22,380
can have hundreds of thousands of

00:21:20,309 --> 00:21:23,760
servers and that's very brittle you want

00:21:22,380 --> 00:21:26,370
to be sending these things into a

00:21:23,760 --> 00:21:27,660
central alerting tool or monitoring tool

00:21:26,370 --> 00:21:30,300
so this is kind of the way that we

00:21:27,660 --> 00:21:31,920
visualize it the way our systems are

00:21:30,300 --> 00:21:34,140
deployed across West one West two

00:21:31,920 --> 00:21:35,910
integer and that everything goes into

00:21:34,140 --> 00:21:37,320
this kind of central monitoring system

00:21:35,910 --> 00:21:39,660
and then it hooks into page B and that's

00:21:37,320 --> 00:21:41,790
how we get alert about it a similar kind

00:21:39,660 --> 00:21:43,770
of idea for service level alerting so

00:21:41,790 --> 00:21:45,990
you know our services are deployed in a

00:21:43,770 --> 00:21:47,730
very that topology across multiple

00:21:45,990 --> 00:21:49,620
regions across multiple data centers and

00:21:47,730 --> 00:21:50,880
we try to leverage a central monitoring

00:21:49,620 --> 00:21:53,190
system again through them pipe into

00:21:50,880 --> 00:21:54,360
pager duty now I had this arrow in red

00:21:53,190 --> 00:21:56,400
here that we actually do up a couple

00:21:54,360 --> 00:21:58,340
services that reach directly into page

00:21:56,400 --> 00:22:00,809
duty and we rely on pager duties

00:21:58,340 --> 00:22:03,030
deduping functionality so like even if

00:22:00,809 --> 00:22:04,320
there is like an event storm where you

00:22:03,030 --> 00:22:06,420
know every single servers reporting the

00:22:04,320 --> 00:22:09,630
same exact thing we pipe that into page

00:22:06,420 --> 00:22:11,550
redirect links to get some auditing and

00:22:09,630 --> 00:22:13,170
and tracking of those events though

00:22:11,550 --> 00:22:14,370
honestly I still don't like doing that

00:22:13,170 --> 00:22:16,140
because I think a better way to do is to

00:22:14,370 --> 00:22:18,480
pipe it through the monitoring system

00:22:16,140 --> 00:22:21,809
and then go into pager duty so

00:22:18,480 --> 00:22:23,790
dependency monitoring so who here uses

00:22:21,809 --> 00:22:25,770
third-party vendors for anything is

00:22:23,790 --> 00:22:27,630
every in your own data set okay so

00:22:25,770 --> 00:22:31,050
everyone you know but ninety percent of

00:22:27,630 --> 00:22:32,820
us so we you know as an ops team at

00:22:31,050 --> 00:22:35,190
pager T we have a couple of dependencies

00:22:32,820 --> 00:22:37,050
DNS monitoring tools logging and you

00:22:35,190 --> 00:22:38,940
know the the constant problem for us is

00:22:37,050 --> 00:22:40,860
how do you know they're working properly

00:22:38,940 --> 00:22:43,080
you know how do you know that that you

00:22:40,860 --> 00:22:45,210
can trust them so there's some

00:22:43,080 --> 00:22:46,950
validation work you can do so for DNS

00:22:45,210 --> 00:22:48,960
we're constantly creating and deleting

00:22:46,950 --> 00:22:51,270
records just to validate okay yeah do

00:22:48,960 --> 00:22:53,070
DNS propagated properly where for our

00:22:51,270 --> 00:22:55,050
monitoring tools we can't really do more

00:22:53,070 --> 00:22:57,000
than a basic ping we kind of log in and

00:22:55,050 --> 00:22:58,200
we look at graphs we have automated

00:22:57,000 --> 00:23:00,660
checks that look at graphs like okay

00:22:58,200 --> 00:23:02,280
that that works properly again same with

00:23:00,660 --> 00:23:03,809
our logging we validate that the logs

00:23:02,280 --> 00:23:05,160
are being pushed will write to a log

00:23:03,809 --> 00:23:06,660
entry locally we'll wait a couple

00:23:05,160 --> 00:23:09,000
minutes and see that it's getting sent

00:23:06,660 --> 00:23:10,620
out there and then status pages are

00:23:09,000 --> 00:23:12,179
making this easier which is nice I find

00:23:10,620 --> 00:23:14,010
that status pages are getting better and

00:23:12,179 --> 00:23:16,800
better we're at an open space yesterday

00:23:14,010 --> 00:23:18,840
we were discussing this and and

00:23:16,800 --> 00:23:20,550
companies are getting more public when

00:23:18,840 --> 00:23:22,050
they're having and when they're having

00:23:20,550 --> 00:23:24,570
outage and being more transparent so

00:23:22,050 --> 00:23:26,070
that's nice but it's still imperfect so

00:23:24,570 --> 00:23:28,140
that's on the ops side now on the

00:23:26,070 --> 00:23:30,510
software stack for pager duty we also

00:23:28,140 --> 00:23:32,190
have dependencies we we use vendors for

00:23:30,510 --> 00:23:34,790
our email SMS phone and push

00:23:32,190 --> 00:23:37,410
notifications in Google and Apple and

00:23:34,790 --> 00:23:40,130
this is what keeps us up at night is

00:23:37,410 --> 00:23:44,520
where one of these vendors has a problem

00:23:40,130 --> 00:23:46,080
so quick story we were using we have an

00:23:44,520 --> 00:23:46,620
SMS provider we're using and they were

00:23:46,080 --> 00:23:48,780
quote-unquote

00:23:46,620 --> 00:23:51,090
up and that they were returning to

00:23:48,780 --> 00:23:53,850
hundreds to us but our customers were

00:23:51,090 --> 00:23:55,290
not actually getting the SMS that they

00:23:53,850 --> 00:23:58,170
signed up that they were paying us for

00:23:55,290 --> 00:23:59,940
and we found out in the worst possible

00:23:58,170 --> 00:24:02,340
way our customer actually had to call us

00:23:59,940 --> 00:24:04,650
and say hey I just tried to trigger an

00:24:02,340 --> 00:24:07,500
alert and expect it an SMS and didn't

00:24:04,650 --> 00:24:09,000
get one was a really really crappy

00:24:07,500 --> 00:24:10,740
feeling to have your customers tell you

00:24:09,000 --> 00:24:12,300
that you're having a problem so what

00:24:10,740 --> 00:24:15,179
actually happened here was this really

00:24:12,300 --> 00:24:16,800
strange edge case where t-mobile one of

00:24:15,179 --> 00:24:18,360
you know one of the large carriers and

00:24:16,800 --> 00:24:20,340
the you mobile carriers in the US

00:24:18,360 --> 00:24:22,320
they basically blocked our short code

00:24:20,340 --> 00:24:23,850
and when we called up t-mobile we said

00:24:22,320 --> 00:24:27,050
why the heck do you guys block our short

00:24:23,850 --> 00:24:29,160
code they basically said in in fraud and

00:24:27,050 --> 00:24:31,410
that was incredibly frustrating they

00:24:29,160 --> 00:24:33,330
reenable it and it was fine but that was

00:24:31,410 --> 00:24:35,130
something that was a kind of eye-opening

00:24:33,330 --> 00:24:37,320
moment for us that we realized that you

00:24:35,130 --> 00:24:39,450
know we we are paying these vendors but

00:24:37,320 --> 00:24:42,090
you have to verify that they're working

00:24:39,450 --> 00:24:43,710
as well so what we did was we built out

00:24:42,090 --> 00:24:45,210
what we call our end-to-end testing

00:24:43,710 --> 00:24:47,580
suite or as I like to call it how to

00:24:45,210 --> 00:24:51,060
abuse unlimited messaging plans are

00:24:47,580 --> 00:24:52,470
limited SMS plans and we test the four

00:24:51,060 --> 00:24:54,090
main carriers in the u.s. probably right

00:24:52,470 --> 00:24:56,130
now it's only working in the US and

00:24:54,090 --> 00:24:59,309
basically what we do is every minute we

00:24:56,130 --> 00:25:00,750
send an SMS through through the carriers

00:24:59,309 --> 00:25:02,040
through our providers and through the

00:25:00,750 --> 00:25:03,600
carriers to see if if they're working

00:25:02,040 --> 00:25:06,270
properly or not and we measure response

00:25:03,600 --> 00:25:09,480
times and it's a what we call our intent

00:25:06,270 --> 00:25:11,760
suite and this is the device lab it's

00:25:09,480 --> 00:25:13,410
very sophisticated it's so you can see

00:25:11,760 --> 00:25:17,130
the the router that these guys are

00:25:13,410 --> 00:25:18,750
hooked up to and it's actually much

00:25:17,130 --> 00:25:21,240
fancier now but I like this picture it

00:25:18,750 --> 00:25:24,150
because it shows how scrappy we are even

00:25:21,240 --> 00:25:25,840
has our old logo there and there's

00:25:24,150 --> 00:25:27,490
variability in

00:25:25,840 --> 00:25:29,289
numbers this is what's interesting for

00:25:27,490 --> 00:25:30,730
us these are the forming carries I'm not

00:25:29,289 --> 00:25:32,409
going to name who they are but if you

00:25:30,730 --> 00:25:35,529
are from the US you might recognize the

00:25:32,409 --> 00:25:37,510
colors and so there's a certain there's

00:25:35,529 --> 00:25:39,700
a certain yellow carrier that I would

00:25:37,510 --> 00:25:42,130
recommend you do not use if you're in

00:25:39,700 --> 00:25:44,559
the US or visiting the US so just stay

00:25:42,130 --> 00:25:46,330
away from the yellow care here in the US

00:25:44,559 --> 00:25:47,590
but there is variability and that's that

00:25:46,330 --> 00:25:50,289
was really interesting when we first

00:25:47,590 --> 00:25:51,789
implemented this that we you know we

00:25:50,289 --> 00:25:54,100
just naively assumed like oh of course

00:25:51,789 --> 00:25:55,809
all mobile carriers are the same no they

00:25:54,100 --> 00:25:57,250
actually have different SMS performance

00:25:55,809 --> 00:25:59,490
profiles and here's what's more

00:25:57,250 --> 00:26:02,380
interesting there's actually like

00:25:59,490 --> 00:26:04,390
seasonality or some sort of periodic

00:26:02,380 --> 00:26:06,520
functions here that you see too so we

00:26:04,390 --> 00:26:08,770
actually see at the top of the hour that

00:26:06,520 --> 00:26:10,270
the latency actually spikes and late

00:26:08,770 --> 00:26:12,820
tink it's bad enough sometimes it takes

00:26:10,270 --> 00:26:15,220
us up to seven to eight minutes to send

00:26:12,820 --> 00:26:17,320
a mess and this is something completely

00:26:15,220 --> 00:26:19,210
out of our control and the nice thing is

00:26:17,320 --> 00:26:21,309
now that we're measuring this we hand

00:26:19,210 --> 00:26:23,110
this data over to our support team so a

00:26:21,309 --> 00:26:24,490
customer ever contacted us saying like

00:26:23,110 --> 00:26:27,130
hey it took a little bit longer for me

00:26:24,490 --> 00:26:28,450
to get an SMS what gives they can look

00:26:27,130 --> 00:26:31,480
at this data and they could say like oh

00:26:28,450 --> 00:26:33,789
right that that yellow carrier was

00:26:31,480 --> 00:26:35,529
having a problem at that time and that's

00:26:33,789 --> 00:26:36,970
why you saw that delay and it's really

00:26:35,529 --> 00:26:40,659
empowering for our support team to have

00:26:36,970 --> 00:26:42,730
that data ready for our customers so how

00:26:40,659 --> 00:26:44,649
do we cheat how do we automate all this

00:26:42,730 --> 00:26:47,559
stuff away so we use chef at page 80

00:26:44,649 --> 00:26:49,570
where big chef proponents and you know

00:26:47,559 --> 00:26:51,279
everyone everyone in here of course is

00:26:49,570 --> 00:26:53,140
using a sophisticated configuration

00:26:51,279 --> 00:26:55,390
management sort of service so I don't

00:26:53,140 --> 00:26:57,159
have to lecture you guys on that we

00:26:55,390 --> 00:26:59,110
install all of our agents via chef

00:26:57,159 --> 00:27:00,279
we don't install by hand or anything

00:26:59,110 --> 00:27:02,140
it's basically any piece of

00:27:00,279 --> 00:27:03,520
infrastructure that comes up in our

00:27:02,140 --> 00:27:05,350
environment automatically gets these

00:27:03,520 --> 00:27:06,669
things for free our engineers can just

00:27:05,350 --> 00:27:08,169
do what they want with them they'll have

00:27:06,669 --> 00:27:10,059
to think about it

00:27:08,169 --> 00:27:12,850
our backup alerting tools are actually

00:27:10,059 --> 00:27:14,260
not automated that's on purpose because

00:27:12,850 --> 00:27:15,820
we're trying to keep it on completely

00:27:14,260 --> 00:27:18,220
separate infrastructure we try to

00:27:15,820 --> 00:27:21,100
decouple it as much as we can so we

00:27:18,220 --> 00:27:22,419
purposely have not automated it another

00:27:21,100 --> 00:27:24,159
problem that we run into is we actually

00:27:22,419 --> 00:27:25,600
haven't automated the cluster level

00:27:24,159 --> 00:27:26,950
alerts yet either is basically a

00:27:25,600 --> 00:27:29,049
limitation of the tooling that we're

00:27:26,950 --> 00:27:30,340
using that don't allow for for good

00:27:29,049 --> 00:27:33,549
automation though I think that's gonna

00:27:30,340 --> 00:27:35,919
get better in the next year or so so how

00:27:33,549 --> 00:27:37,450
do we validate that these alerts work so

00:27:35,919 --> 00:27:38,440
at pager day we you know we've talked

00:27:37,450 --> 00:27:40,090
about this in the past and

00:27:38,440 --> 00:27:41,740
you can find talks on it we have this

00:27:40,090 --> 00:27:44,169
thing called failure Friday that we do

00:27:41,740 --> 00:27:45,309
so it's every Friday we we all get in a

00:27:44,169 --> 00:27:47,679
room together for an hour and we just

00:27:45,309 --> 00:27:51,340
knock stuff over we just you know knock

00:27:47,679 --> 00:27:52,750
servers over processes and we see if the

00:27:51,340 --> 00:27:55,690
alerts work we actually leave all the

00:27:52,750 --> 00:27:57,340
alerts enabled and we actually see a lot

00:27:55,690 --> 00:27:59,500
of problems in our alerting where we'll

00:27:57,340 --> 00:28:01,600
knock over a process and leave it down

00:27:59,500 --> 00:28:04,570
and it's throwing errors and we don't

00:28:01,600 --> 00:28:06,370
get the alert and then we you know very

00:28:04,570 --> 00:28:08,080
quickly create the alert make sure it

00:28:06,370 --> 00:28:10,240
triggers and that validates that the

00:28:08,080 --> 00:28:11,919
alert is working properly if you're

00:28:10,240 --> 00:28:13,450
curious about the actual commands and

00:28:11,919 --> 00:28:17,350
failures that we inject you can go to

00:28:13,450 --> 00:28:18,399
our blog it has everything on there some

00:28:17,350 --> 00:28:21,070
of the things that we've learned from

00:28:18,399 --> 00:28:22,929
failure Friday my favorite was the first

00:28:21,070 --> 00:28:25,090
one we had the process monitoring

00:28:22,929 --> 00:28:26,559
commingled with the prep process running

00:28:25,090 --> 00:28:29,200
so when the app process died the

00:28:26,559 --> 00:28:31,000
monitoring died with it again bad idea

00:28:29,200 --> 00:28:33,070
because that way you'll never know if

00:28:31,000 --> 00:28:34,480
you know if all your web workers are

00:28:33,070 --> 00:28:37,210
dying you're not going to know because

00:28:34,480 --> 00:28:39,429
the monitoring is dying with it only

00:28:37,210 --> 00:28:40,870
localhost checks as opposed to using

00:28:39,429 --> 00:28:42,460
external health check so we had a bunch

00:28:40,870 --> 00:28:44,559
of services that we're using tools like

00:28:42,460 --> 00:28:46,539
Monnett or other process monitoring

00:28:44,559 --> 00:28:48,909
tools that relied on local host still

00:28:46,539 --> 00:28:50,799
being up well what happens when Amazon

00:28:48,909 --> 00:28:52,860
shuts down a box for you that's you know

00:28:50,799 --> 00:28:55,570
a service they provide apparently and

00:28:52,860 --> 00:28:56,740
then you're monitoring goes away as well

00:28:55,570 --> 00:28:58,960
so you need we needed to have better

00:28:56,740 --> 00:29:01,029
external health checks as well

00:28:58,960 --> 00:29:03,879
outbound network connection this was a

00:29:01,029 --> 00:29:05,950
fun one so we we've seen this within

00:29:03,879 --> 00:29:08,049
west us West one especially where

00:29:05,950 --> 00:29:10,720
they'll be complete network isolation

00:29:08,049 --> 00:29:13,809
for the entire region and because of

00:29:10,720 --> 00:29:16,240
that the the individual servers cannot

00:29:13,809 --> 00:29:18,039
issue out hey I'm in a degraded state

00:29:16,240 --> 00:29:19,419
because there's no outbound network

00:29:18,039 --> 00:29:21,639
connection so we had to build out again

00:29:19,419 --> 00:29:23,679
better external monitoring to tell us

00:29:21,639 --> 00:29:26,740
when when our servers our services were

00:29:23,679 --> 00:29:28,419
having problems and it's been really

00:29:26,740 --> 00:29:29,169
interesting just you know we've been

00:29:28,419 --> 00:29:30,700
running this for about almost

00:29:29,169 --> 00:29:32,139
two-and-a-half years now it's been

00:29:30,700 --> 00:29:34,299
really interesting seeing how teams

00:29:32,139 --> 00:29:35,500
leverage failure Friday from not only

00:29:34,299 --> 00:29:37,720
from a teamwork and collaboration

00:29:35,500 --> 00:29:39,070
standpoint but also from the technical

00:29:37,720 --> 00:29:42,429
making sure that our alerts work

00:29:39,070 --> 00:29:44,649
standpoint that's all I got

00:29:42,429 --> 00:29:46,269
we we are hiring we are starting to hire

00:29:44,649 --> 00:29:47,500
remote engineers so if anyone is

00:29:46,269 --> 00:29:50,440
interested in working on high

00:29:47,500 --> 00:29:51,470
availability problems please come get

00:29:50,440 --> 00:29:59,630
ahold of me and we should

00:29:51,470 --> 00:30:02,059
talk so thank you for listening yes

00:29:59,630 --> 00:30:17,030
question hold on somebody knows from the

00:30:02,059 --> 00:30:20,590
front line hi hi so how do you do

00:30:17,030 --> 00:30:24,350
on-call scheduling for your critical

00:30:20,590 --> 00:30:26,570
level alert yeah you're not using yeah

00:30:24,350 --> 00:30:29,360
it's a good question so right now the

00:30:26,570 --> 00:30:30,799
way we do it is for the critical ones

00:30:29,360 --> 00:30:35,030
where it really is a all-hands-on-deck

00:30:30,799 --> 00:30:36,919
we the on calls for that week of the

00:30:35,030 --> 00:30:40,370
individual teams they actually go in and

00:30:36,919 --> 00:30:41,900
they manually enter in the that that

00:30:40,370 --> 00:30:43,880
they need to be notified and again it's

00:30:41,900 --> 00:30:45,260
it's purposely not automated because

00:30:43,880 --> 00:30:47,179
we're trying to maintain that decoupling

00:30:45,260 --> 00:30:50,090
that works right now because we only

00:30:47,179 --> 00:30:51,530
have I think I won't say six or seven on

00:30:50,090 --> 00:30:53,480
call schedule but really once we get to

00:30:51,530 --> 00:30:54,799
like ten fifteen twenty it's not going

00:30:53,480 --> 00:30:56,630
to work anymore and so we have to find a

00:30:54,799 --> 00:30:59,299
better solution once we get to that size

00:30:56,630 --> 00:31:10,460
but we achieve because we're small right

00:30:59,299 --> 00:31:20,780
now oh sure let me go back to the

00:31:10,460 --> 00:31:23,840
beginning hi

00:31:20,780 --> 00:31:27,710
question here yeah I was mesmerised by

00:31:23,840 --> 00:31:31,760
your system for monitoring SMS delays

00:31:27,710 --> 00:31:34,820
and I wonder how are you planning to

00:31:31,760 --> 00:31:38,929
take the problem of different regions

00:31:34,820 --> 00:31:40,460
have carriers and that's yeah so no

00:31:38,929 --> 00:31:42,110
that's a great question so ideas that

00:31:40,460 --> 00:31:44,480
we're mulling around with right now so

00:31:42,110 --> 00:31:46,460
we have actually pretty global user base

00:31:44,480 --> 00:31:49,010
obviously a lot of folks in in Israel

00:31:46,460 --> 00:31:50,480
and because of our mobile app is also

00:31:49,010 --> 00:31:51,980
something people are downloading more

00:31:50,480 --> 00:31:53,360
and more now we're trying to figure out

00:31:51,980 --> 00:31:54,950
ways to leverage that so this would be

00:31:53,360 --> 00:31:56,870
definitely like an opt-in type thing

00:31:54,950 --> 00:32:00,919
that if if you want to help us out give

00:31:56,870 --> 00:32:02,720
us data on on your SMS that's one idea

00:32:00,919 --> 00:32:05,090
we have the other is we're

00:32:02,720 --> 00:32:06,620
to see more services that will actually

00:32:05,090 --> 00:32:08,720
manage mobile devices for you in

00:32:06,620 --> 00:32:09,890
different countries so that's what we're

00:32:08,720 --> 00:32:13,549
trying to figure out if we can just

00:32:09,890 --> 00:32:16,370
basically put put so we built a Android

00:32:13,549 --> 00:32:19,250
app that we use and take that ship it to

00:32:16,370 --> 00:32:20,659
Tel Aviv to London to sink wherever it

00:32:19,250 --> 00:32:23,000
is and just have those running as well

00:32:20,659 --> 00:32:24,470
in those other regions but right now a

00:32:23,000 --> 00:32:25,760
majority of our customer base is in the

00:32:24,470 --> 00:32:27,500
US and we haven't figured out a good way

00:32:25,760 --> 00:32:28,669
to do this international yet but that is

00:32:27,500 --> 00:32:34,070
something that we're actively trying to

00:32:28,669 --> 00:32:36,260
figure out okay I notice that most you

00:32:34,070 --> 00:32:37,970
know not all but most most of the

00:32:36,260 --> 00:32:41,270
monitoring tools that you have our

00:32:37,970 --> 00:32:42,320
external services yes I'm assuming that

00:32:41,270 --> 00:32:46,039
that's by design

00:32:42,320 --> 00:32:47,480
yeah and I'm also assuming the one the

00:32:46,039 --> 00:32:49,610
one question I have I'm sorry you get

00:32:47,480 --> 00:32:52,640
also pretty good pricing on that as well

00:32:49,610 --> 00:32:57,409
because it seems like a very costly way

00:32:52,640 --> 00:32:59,270
to go yeah so the it is costly but it's

00:32:57,409 --> 00:33:02,030
also not our core competency that's why

00:32:59,270 --> 00:33:03,260
we try to use and leverage vendors as

00:33:02,030 --> 00:33:06,049
much as possible because that's what

00:33:03,260 --> 00:33:10,700
they think about 24/7 of in terms of

00:33:06,049 --> 00:33:12,350
costly costliness it does stack up

00:33:10,700 --> 00:33:13,880
though honestly like we lean on our

00:33:12,350 --> 00:33:16,370
vendors very heavily for better

00:33:13,880 --> 00:33:18,650
discounts and pricing so it's one of

00:33:16,370 --> 00:33:21,169
those things where because we're not the

00:33:18,650 --> 00:33:22,610
biggest in terms of infrastructure yeah

00:33:21,169 --> 00:33:29,059
the vendors are a little bit more

00:33:22,610 --> 00:33:33,620
flexible with us yeah you talked about

00:33:29,059 --> 00:33:41,600
security all your data is on AWS yes aw

00:33:33,620 --> 00:33:47,960
signature yep the cloud I swear it's

00:33:41,600 --> 00:33:52,549
secure other questions actually I have

00:33:47,960 --> 00:33:57,110
one as well and they probably know this

00:33:52,549 --> 00:34:00,980
as well yeah like I'm sure you're moved

00:33:57,110 --> 00:34:02,630
to region yes no it's a good question so

00:34:00,980 --> 00:34:04,220
that's why we're a multi cloud as well

00:34:02,630 --> 00:34:05,870
so like our infrastructure we can just

00:34:04,220 --> 00:34:08,780
pick it up and and move it so it's

00:34:05,870 --> 00:34:11,510
multiple regions as well yeah but the

00:34:08,780 --> 00:34:14,210
SAS providers that you guys use are you

00:34:11,510 --> 00:34:15,790
sure like do you you check that they are

00:34:14,210 --> 00:34:18,010
close like cross

00:34:15,790 --> 00:34:19,810
cloud as well no they're not they're not

00:34:18,010 --> 00:34:21,940
actually most of them are AWS only so

00:34:19,810 --> 00:34:24,790
when AWS has an outage that's when we

00:34:21,940 --> 00:34:27,429
have internal run books that if there is

00:34:24,790 --> 00:34:29,200
like a global AWS outage that we can use

00:34:27,429 --> 00:34:30,429
to make sure that we can manually check

00:34:29,200 --> 00:34:33,550
our systems are running and that's why

00:34:30,429 --> 00:34:35,110
we also have our own internal tools as

00:34:33,550 --> 00:34:36,970
well a couple of those that I mentioned

00:34:35,110 --> 00:34:38,890
so like we look for you know I guess

00:34:36,970 --> 00:34:40,870
it's the idea of like monitoring and

00:34:38,890 --> 00:34:42,160
depth idea that like if we MA use a

00:34:40,870 --> 00:34:43,540
bunch of different tools and monitor a

00:34:42,160 --> 00:34:45,250
bunch of different areas there will be

00:34:43,540 --> 00:34:46,540
some overlaps so if one of them goes

00:34:45,250 --> 00:34:48,280
away it's okay

00:34:46,540 --> 00:34:50,770
but we do feel the pain when AWS has a

00:34:48,280 --> 00:34:52,840
larger multi-region outage we do feel

00:34:50,770 --> 00:34:54,460
that pain though it's basically what

00:34:52,840 --> 00:34:56,860
ends up happening is we find out about

00:34:54,460 --> 00:34:59,290
it because again our traffic skyrockets

00:34:56,860 --> 00:35:00,850
and then we see that our vendors are now

00:34:59,290 --> 00:35:02,800
having issues as well so we actually

00:35:00,850 --> 00:35:05,680
have to get some of our own calls to

00:35:02,800 --> 00:35:06,670
kind of look over the system while while

00:35:05,680 --> 00:35:08,890
the vendors are having out just

00:35:06,670 --> 00:35:11,230
thankfully it happens to me once a year

00:35:08,890 --> 00:35:13,680
it's something that's not happening

00:35:11,230 --> 00:35:21,190
often enough for me to worry about it

00:35:13,680 --> 00:35:24,340
Thanks we are now waiting for the tech

00:35:21,190 --> 00:35:28,150
Trek to come here okay so we will all

00:35:24,340 --> 00:35:31,900
stay here for our next talk the funny

00:35:28,150 --> 00:35:35,170
talk about git so it would probably take

00:35:31,900 --> 00:35:37,980
a few more minutes okay so just be

00:35:35,170 --> 00:35:37,980

YouTube URL: https://www.youtube.com/watch?v=3z6K1ev7VcM


