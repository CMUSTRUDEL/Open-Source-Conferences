Title: Machine Learning Concepts for Software Monitoring - Lior Redlus - DevOpsDays Tel Aviv 2016
Publication date: 2017-01-08
Playlist: DevOps Days Tel Aviv 2016
Description: 
	
Captions: 
	00:00:02,710 --> 00:00:10,269
[Music]

00:00:07,920 --> 00:00:13,420
for those of you who've been here for

00:00:10,269 --> 00:00:15,910
the last talk by Leon he talked about

00:00:13,420 --> 00:00:20,109
all these kinds of tools for monitoring

00:00:15,910 --> 00:00:21,699
and here we gonna touch about how the

00:00:20,109 --> 00:00:23,409
monitoring tools are currently not

00:00:21,699 --> 00:00:25,480
enough and what we can do with machine

00:00:23,409 --> 00:00:30,310
learning in order to enhance the

00:00:25,480 --> 00:00:33,899
monitoring of locks so about myself I'm

00:00:30,310 --> 00:00:36,070
31 years old scientist at heart I have a

00:00:33,899 --> 00:00:37,570
master's degree in neuroscience

00:00:36,070 --> 00:00:40,000
information processing from bar-ilan

00:00:37,570 --> 00:00:41,800
University and currently I am the

00:00:40,000 --> 00:00:45,609
co-founder and chief scientist at Quora

00:00:41,800 --> 00:00:49,239
logics so what did we do we do it in

00:00:45,609 --> 00:00:51,640
core logics projects is a machine land

00:00:49,239 --> 00:00:54,550
learning platform that analyzes your log

00:00:51,640 --> 00:00:56,469
records of course that the log

00:00:54,550 --> 00:00:58,149
management is already included we have

00:00:56,469 --> 00:00:59,679
the indexing the querying the filtering

00:00:58,149 --> 00:01:02,109
alerting you that you are used to in

00:00:59,679 --> 00:01:05,050
your current tools but on top of that we

00:01:02,109 --> 00:01:06,910
have an analytics platform there turns

00:01:05,050 --> 00:01:09,700
your data into patterns and flows and

00:01:06,910 --> 00:01:12,370
then we will give you deep insights on

00:01:09,700 --> 00:01:16,900
your system and of course automatically

00:01:12,370 --> 00:01:19,270
alerts you about production problems in

00:01:16,900 --> 00:01:22,870
this talk we will explore the challenges

00:01:19,270 --> 00:01:25,480
of software logs today we will take not

00:01:22,870 --> 00:01:29,200
so short in overview of machine learning

00:01:25,480 --> 00:01:31,540
of some of the use cases suggest a fully

00:01:29,200 --> 00:01:36,550
automatic algorithm for anomaly

00:01:31,540 --> 00:01:39,730
detection in logs so what do we use log

00:01:36,550 --> 00:01:42,310
records for debugging security

00:01:39,730 --> 00:01:45,820
compliance user analytics we all know

00:01:42,310 --> 00:01:47,980
that but specifically there are two use

00:01:45,820 --> 00:01:49,870
cases that standout production mode

00:01:47,980 --> 00:01:53,920
monitoring and production

00:01:49,870 --> 00:01:57,070
troubleshooting 70% of users of these

00:01:53,920 --> 00:01:59,480
systems use log records for production

00:01:57,070 --> 00:02:02,300
troubleshooting

00:01:59,480 --> 00:02:05,430
additionally open-source software and

00:02:02,300 --> 00:02:08,220
cloud enables companies to grow very

00:02:05,430 --> 00:02:11,340
very fast and their data and log records

00:02:08,220 --> 00:02:12,960
grow exponentially even small companies

00:02:11,340 --> 00:02:14,760
are generating enormous amounts of logs

00:02:12,960 --> 00:02:16,650
and they have trouble of actually

00:02:14,760 --> 00:02:22,920
finding what they want and the data

00:02:16,650 --> 00:02:25,230
grows exponentially the big data

00:02:22,920 --> 00:02:27,240
approach up until now and also in log

00:02:25,230 --> 00:02:29,310
management is to collect everything

00:02:27,240 --> 00:02:31,470
let's just dump everything into a single

00:02:29,310 --> 00:02:33,060
bucket and then we can do whatever we

00:02:31,470 --> 00:02:39,690
want when we need it and we'll know what

00:02:33,060 --> 00:02:43,230
to do or will we the assumption is that

00:02:39,690 --> 00:02:46,470
in the belief that the sufficiently

00:02:43,230 --> 00:02:49,350
large pile of horseshit we will find a

00:02:46,470 --> 00:02:52,140
pony just collect the data and you'll

00:02:49,350 --> 00:02:55,550
find something interesting in there the

00:02:52,140 --> 00:02:55,550
problem is that it is not that simple

00:02:57,170 --> 00:03:02,670
what we have now is that humans have to

00:03:00,900 --> 00:03:05,790
tackle these enormous amount amounts of

00:03:02,670 --> 00:03:07,620
data and this is a problem because we

00:03:05,790 --> 00:03:10,500
are very bad at identifying complex

00:03:07,620 --> 00:03:12,860
relationships in our systems we are not

00:03:10,500 --> 00:03:19,100
noticing small but important changes and

00:03:12,860 --> 00:03:19,100
also we also will never 100% in focus

00:03:19,580 --> 00:03:25,770
this causes the main issue that most of

00:03:23,040 --> 00:03:28,230
the time on investigating issues we're

00:03:25,770 --> 00:03:31,800
actually trying to find what the issue

00:03:28,230 --> 00:03:34,080
is what caused it and where is what what

00:03:31,800 --> 00:03:37,320
do we need to fix seventy percent of the

00:03:34,080 --> 00:03:40,670
time is is for finding the problem

00:03:37,320 --> 00:03:40,670
instead of fixing it

00:03:42,600 --> 00:03:46,750
we can condense all this into the

00:03:44,920 --> 00:03:47,710
problem that log management does not

00:03:46,750 --> 00:03:50,110
have a brain

00:03:47,710 --> 00:03:53,310
we need log management to think for us

00:03:50,110 --> 00:03:56,170
and to have all these correlations

00:03:53,310 --> 00:03:58,030
automatically recognized in order to

00:03:56,170 --> 00:04:01,000
give us where the problem is so we can

00:03:58,030 --> 00:04:04,030
focus on fixing Intel instead of taking

00:04:01,000 --> 00:04:07,150
70% of the time just to find where the

00:04:04,030 --> 00:04:13,240
problem is so let's give it a brain this

00:04:07,150 --> 00:04:15,430
is the log analytics but before you dive

00:04:13,240 --> 00:04:18,150
into it let's have a quick overview of

00:04:15,430 --> 00:04:21,850
machine learning

00:04:18,150 --> 00:04:24,040
what's machine learning it is a field of

00:04:21,850 --> 00:04:27,040
study that gives computers the ability

00:04:24,040 --> 00:04:29,500
to learn by themselves instead of being

00:04:27,040 --> 00:04:31,510
explicitly programmed this is by Arthur

00:04:29,500 --> 00:04:37,870
Samuel the pioneer of machine learning

00:04:31,510 --> 00:04:39,730
in 1959 in traditional coding what we do

00:04:37,870 --> 00:04:41,410
mainly is that we have a model of the

00:04:39,730 --> 00:04:45,030
world of how the world works in our head

00:04:41,410 --> 00:04:47,050
and then we write code that explicitly

00:04:45,030 --> 00:04:51,100
represents this model and behaves

00:04:47,050 --> 00:04:53,229
exactly like like we expected and then

00:04:51,100 --> 00:04:55,090
whenever the world changes or the model

00:04:53,229 --> 00:04:57,570
of our head changes we need to manually

00:04:55,090 --> 00:05:01,900
update the code in order to reflect that

00:04:57,570 --> 00:05:04,270
the concept of machine learning is that

00:05:01,900 --> 00:05:06,280
you have loose concepts of what the

00:05:04,270 --> 00:05:08,800
world behaves like and what you do is

00:05:06,280 --> 00:05:11,229
write code that can automatically learn

00:05:08,800 --> 00:05:14,530
a model of that world and then behave

00:05:11,229 --> 00:05:17,410
accordingly the exact behavior of the

00:05:14,530 --> 00:05:20,650
code is not always known but the general

00:05:17,410 --> 00:05:22,450
results are pretty pretty good and it

00:05:20,650 --> 00:05:25,740
can also be updated automatically as the

00:05:22,450 --> 00:05:27,790
model knows how to build the the

00:05:25,740 --> 00:05:32,500
representation of the world bytes by

00:05:27,790 --> 00:05:35,360
itself how well does it work

00:05:32,500 --> 00:05:37,220
first it is very quick much faster than

00:05:35,360 --> 00:05:41,500
what we can do and humans do and

00:05:37,220 --> 00:05:41,500
sometimes it even has better accuracy

00:05:42,490 --> 00:05:48,020
what types of machine learning do we

00:05:44,660 --> 00:05:50,540
have supervised machine learning deals

00:05:48,020 --> 00:05:53,210
with data that is clearly defined we

00:05:50,540 --> 00:05:56,840
have a right and wrong answers for each

00:05:53,210 --> 00:05:59,720
of our input data we let the machine

00:05:56,840 --> 00:06:03,350
learn from this right and wrong answers

00:05:59,720 --> 00:06:05,210
in order to predict what will be the the

00:06:03,350 --> 00:06:08,300
answer for data that is it is not seen

00:06:05,210 --> 00:06:13,130
yet the two main types are regression

00:06:08,300 --> 00:06:16,550
and classification they are both in both

00:06:13,130 --> 00:06:18,590
try to predict the outcome of a specific

00:06:16,550 --> 00:06:20,210
data point with regression using

00:06:18,590 --> 00:06:24,950
continuous values and with

00:06:20,210 --> 00:06:29,900
classification using classes of discrete

00:06:24,950 --> 00:06:33,440
values what is regression we can take

00:06:29,900 --> 00:06:37,610
the temperature of the day and data over

00:06:33,440 --> 00:06:40,190
the amount of yogurt sold in order to

00:06:37,610 --> 00:06:41,780
predict what the temperature is based on

00:06:40,190 --> 00:06:43,970
to the amount of yogurt that someone

00:06:41,780 --> 00:06:46,390
sold in the sword and a linear

00:06:43,970 --> 00:06:50,120
regression will give us a pretty good

00:06:46,390 --> 00:06:52,640
accuracy for this data set another

00:06:50,120 --> 00:06:54,560
example can be given the cups of coffees

00:06:52,640 --> 00:06:56,870
that is that are solved every 10 minutes

00:06:54,560 --> 00:06:59,000
and predict how many cups of coffee a

00:06:56,870 --> 00:07:00,550
coffee will be sold on any given time of

00:06:59,000 --> 00:07:02,870
the day

00:07:00,550 --> 00:07:05,180
linear regression does not always solve

00:07:02,870 --> 00:07:07,790
our problem as we can see the data does

00:07:05,180 --> 00:07:09,920
not behave in a linear way but a

00:07:07,790 --> 00:07:11,930
polynomial solution here gives a pretty

00:07:09,920 --> 00:07:15,940
good accuracy for how much cups of

00:07:11,930 --> 00:07:15,940
coffee are sold in every time of the day

00:07:16,430 --> 00:07:24,620
for classification can we identify the

00:07:21,190 --> 00:07:27,620
the type of an iris based on its leaf

00:07:24,620 --> 00:07:32,600
sizes we have three types of viruses and

00:07:27,620 --> 00:07:34,940
each one has two types of leaves the

00:07:32,600 --> 00:07:39,380
question is can we can we identify which

00:07:34,940 --> 00:07:42,800
kind of iris is it only by the size of

00:07:39,380 --> 00:07:44,660
the leaves that we have so we have four

00:07:42,800 --> 00:07:46,460
measurements the width and the length of

00:07:44,660 --> 00:07:49,550
the two types of leaves that each

00:07:46,460 --> 00:07:51,470
yourself and now we need to plot the

00:07:49,550 --> 00:07:54,169
data and see whether we can cluster

00:07:51,470 --> 00:07:55,820
these these units in order to predict

00:07:54,169 --> 00:08:00,139
could it correctly according to the leaf

00:07:55,820 --> 00:08:01,639
size Fisher's iris that set which is one

00:08:00,139 --> 00:08:04,720
of the most classic in machine learning

00:08:01,639 --> 00:08:07,520
it it has a hundred and fifty

00:08:04,720 --> 00:08:10,760
measurements of each of the three

00:08:07,520 --> 00:08:12,830
categories of irises and when we plot

00:08:10,760 --> 00:08:15,710
one of the sizes of the leaves the width

00:08:12,830 --> 00:08:17,599
against its length we can use a support

00:08:15,710 --> 00:08:20,630
vector machine one of the clustering

00:08:17,599 --> 00:08:23,690
algorithms in order to to paint the

00:08:20,630 --> 00:08:28,099
zones that we will predict each of the

00:08:23,690 --> 00:08:29,770
each of the types of an iris this method

00:08:28,099 --> 00:08:32,060
alone gives us seventy percent

00:08:29,770 --> 00:08:33,409
seventy-three percent accuracy which is

00:08:32,060 --> 00:08:37,339
pretty good for something that is

00:08:33,409 --> 00:08:39,130
completely automatic reinforcement

00:08:37,339 --> 00:08:42,289
learning is a different type of

00:08:39,130 --> 00:08:45,290
supervised learning and it is reward

00:08:42,289 --> 00:08:48,500
based which means that we need to define

00:08:45,290 --> 00:08:50,470
a set of rules that that shows us the

00:08:48,500 --> 00:08:54,560
interaction of the Machine and the world

00:08:50,470 --> 00:08:57,560
where each each action can be good or

00:08:54,560 --> 00:09:00,380
bad and will increase or decrease the

00:08:57,560 --> 00:09:03,980
reward that the machine gets and the

00:09:00,380 --> 00:09:06,290
machine tries to maximize this call it

00:09:03,980 --> 00:09:08,600
is used in both in game BOTS and in

00:09:06,290 --> 00:09:11,510
recommender systems and specifically if

00:09:08,600 --> 00:09:14,720
we go for a commando system we can build

00:09:11,510 --> 00:09:17,180
profiles for items and users we then can

00:09:14,720 --> 00:09:20,750
recommend an item to a user based on his

00:09:17,180 --> 00:09:22,700
previous preferences and we gain rewards

00:09:20,750 --> 00:09:25,820
when users click on these recommended

00:09:22,700 --> 00:09:27,800
items accordingly we can update the

00:09:25,820 --> 00:09:30,260
model all the time in order to

00:09:27,800 --> 00:09:34,510
have all the the correlations of new

00:09:30,260 --> 00:09:36,890
data and new users the most renown

00:09:34,510 --> 00:09:39,580
recommender systems are implemented in

00:09:36,890 --> 00:09:41,810
YouTube Netflix Amazon well you you get

00:09:39,580 --> 00:09:43,550
suggested of what the next movie you

00:09:41,810 --> 00:09:47,029
will see what the next show you should

00:09:43,550 --> 00:09:49,730
watch or what book you should buy but

00:09:47,029 --> 00:09:52,430
the general rule is that recommender

00:09:49,730 --> 00:09:55,610
system try to to offer similar things to

00:09:52,430 --> 00:09:57,200
similar users for example if you take

00:09:55,610 --> 00:10:00,080
Jim who likes sports

00:09:57,200 --> 00:10:04,339
and Jim likes to buy pizza salads and

00:10:00,080 --> 00:10:08,779
coke and we have Bob that also is into

00:10:04,339 --> 00:10:10,760
sports and likes pizza and and salads we

00:10:08,779 --> 00:10:13,250
need the our system to recognize that

00:10:10,760 --> 00:10:16,610
there are similar users and offer a coke

00:10:13,250 --> 00:10:20,029
also to Bob and this and there the

00:10:16,610 --> 00:10:22,339
percentage of success here will be quite

00:10:20,029 --> 00:10:28,459
high the more good the recommender

00:10:22,339 --> 00:10:30,649
system is unsupervised learning is a

00:10:28,459 --> 00:10:32,930
different kind and it detects a problem

00:10:30,649 --> 00:10:35,630
that we have with supervised machine

00:10:32,930 --> 00:10:38,000
learning the problem is that it gives

00:10:35,630 --> 00:10:41,209
good results but it requires us to have

00:10:38,000 --> 00:10:44,630
a lot of label data and label data is

00:10:41,209 --> 00:10:48,410
quite sparse and we don't always have

00:10:44,630 --> 00:10:52,550
the the good and bad answers for these

00:10:48,410 --> 00:10:54,350
data labeling for unlabeled data if we

00:10:52,550 --> 00:10:55,850
if you want to label the data it is a

00:10:54,350 --> 00:10:58,550
human effort which it takes a lot of

00:10:55,850 --> 00:11:00,140
time and a lot of energy in our

00:10:58,550 --> 00:11:01,520
supervised learning the Machine

00:11:00,140 --> 00:11:04,910
automatically recognizes the

00:11:01,520 --> 00:11:07,250
relationships in the data and it has no

00:11:04,910 --> 00:11:10,370
right or wrong answers it can it does it

00:11:07,250 --> 00:11:13,670
automatically and then we can you can

00:11:10,370 --> 00:11:15,470
use it in order to use our supervised

00:11:13,670 --> 00:11:17,480
learning algorithm on base of the

00:11:15,470 --> 00:11:21,770
features that we've learned unsupervised

00:11:17,480 --> 00:11:24,430
learning some approaches in unsupervised

00:11:21,770 --> 00:11:28,670
learning include clustering algorithms

00:11:24,430 --> 00:11:32,240
anomaly detection of rare events and the

00:11:28,670 --> 00:11:33,230
buzz word of the year deep learning well

00:11:32,240 --> 00:11:35,329
in deep learning

00:11:33,230 --> 00:11:38,720
the idea is to study from a lot of non

00:11:35,329 --> 00:11:39,480
labelled data learn highly nonlinear

00:11:38,720 --> 00:11:41,279
correlations

00:11:39,480 --> 00:11:44,220
you can then represent complex

00:11:41,279 --> 00:11:45,839
relationships inside our data and it

00:11:44,220 --> 00:11:49,560
gives surprisingly good results for many

00:11:45,839 --> 00:11:51,810
applications so we'll take an example

00:11:49,560 --> 00:11:53,399
about how deep learning works but we're

00:11:51,810 --> 00:11:55,440
not going to dive deep into the math

00:11:53,399 --> 00:11:57,410
because otherwise we'll spend the whole

00:11:55,440 --> 00:12:00,720
convention here speaking just about that

00:11:57,410 --> 00:12:05,699
so can we automatically cluster digits

00:12:00,720 --> 00:12:08,100
together the idea is to have we have a

00:12:05,699 --> 00:12:09,630
sixty thousand black and white twenty by

00:12:08,100 --> 00:12:12,570
twenty pixel images of handwritten

00:12:09,630 --> 00:12:16,949
digits and we want our computer to add

00:12:12,570 --> 00:12:19,170
to understand which of these images

00:12:16,949 --> 00:12:21,720
belongs to which representation of an

00:12:19,170 --> 00:12:23,760
image of it over digits we want it to

00:12:21,720 --> 00:12:25,589
recognize that these threes although

00:12:23,760 --> 00:12:29,760
they are not the same image they should

00:12:25,589 --> 00:12:32,610
be classified as threes so each image

00:12:29,760 --> 00:12:34,980
which is twenty by twenty pixels we

00:12:32,610 --> 00:12:39,029
flatten it to a four hundred fourteen

00:12:34,980 --> 00:12:42,600
floating point values vector we take

00:12:39,029 --> 00:12:45,540
this vector and then we can fit it into

00:12:42,600 --> 00:12:47,880
a neural network the neural network

00:12:45,540 --> 00:12:50,279
consists of an input layer where all

00:12:47,880 --> 00:12:53,819
these input values are inserted into

00:12:50,279 --> 00:12:56,790
this layer hidden layers over here and

00:12:53,819 --> 00:12:59,490
here we have several of them and they

00:12:56,790 --> 00:13:01,529
recognize features inside these images

00:12:59,490 --> 00:13:04,290
and the output layer is the only

00:13:01,529 --> 00:13:06,269
supervised thing that we have here we'll

00:13:04,290 --> 00:13:09,000
probably have four digits then

00:13:06,269 --> 00:13:10,589
supervised output layer and output

00:13:09,000 --> 00:13:12,930
neurons in the name in the output layer

00:13:10,589 --> 00:13:18,740
and then we can know from each neuron

00:13:12,930 --> 00:13:18,740
which which cluster the image came from

00:13:22,360 --> 00:13:27,160
after feeding the an image to to an

00:13:25,480 --> 00:13:28,899
input layer the neural network

00:13:27,160 --> 00:13:32,649
automatically learns the features of

00:13:28,899 --> 00:13:35,079
that layer and each neuron lights up

00:13:32,649 --> 00:13:38,019
when ink recognizes a certain feature in

00:13:35,079 --> 00:13:42,429
the layer that you know that is previous

00:13:38,019 --> 00:13:44,499
to it if we look at the first layer one

00:13:42,429 --> 00:13:46,689
neuron can actually recognize round

00:13:44,499 --> 00:13:50,410
edges the other one will recognize

00:13:46,689 --> 00:13:53,410
vertical lines and other may recognize

00:13:50,410 --> 00:13:55,689
diagonal lines etc and then these

00:13:53,410 --> 00:13:57,850
neurons will actually light up whenever

00:13:55,689 --> 00:14:00,699
they recognize such features in the

00:13:57,850 --> 00:14:05,139
previous layer and each layer learns

00:14:00,699 --> 00:14:07,839
higher representations of the layer that

00:14:05,139 --> 00:14:10,720
has that is before it so if we look at

00:14:07,839 --> 00:14:14,920
the whole process here we have the

00:14:10,720 --> 00:14:18,040
neural network after it is learned we

00:14:14,920 --> 00:14:23,829
give it one vector of representation of

00:14:18,040 --> 00:14:26,529
an image of one feeding it to the input

00:14:23,829 --> 00:14:28,989
layer we can now see that several

00:14:26,529 --> 00:14:30,999
features have have been recognized and

00:14:28,989 --> 00:14:34,149
they are leading up and they feed

00:14:30,999 --> 00:14:36,399
forward these these activation neurons

00:14:34,149 --> 00:14:39,759
on to the next layer and there and

00:14:36,399 --> 00:14:42,869
another feature is recognized until the

00:14:39,759 --> 00:14:45,279
output layer recognized that that these

00:14:42,869 --> 00:14:49,769
the features that the last layer is

00:14:45,279 --> 00:14:53,559
recognized feel feet the digit of one

00:14:49,769 --> 00:14:55,569
the whole this neural network actually

00:14:53,559 --> 00:14:57,819
is becoming became an expert in

00:14:55,569 --> 00:15:00,369
recognizing different features of how

00:14:57,819 --> 00:15:03,929
digits are represented in the world it

00:15:00,369 --> 00:15:07,869
can it can find that aids are actually

00:15:03,929 --> 00:15:10,569
have confined zeros that are stuck

00:15:07,869 --> 00:15:13,209
together and it will and many times find

00:15:10,569 --> 00:15:14,949
the aides and zeros may find themselves

00:15:13,209 --> 00:15:17,019
clustering the same features but the

00:15:14,949 --> 00:15:19,949
output layer will differentiate them by

00:15:17,019 --> 00:15:19,949
inviting these features

00:15:24,259 --> 00:15:26,799
each

00:15:27,290 --> 00:15:32,180
they they feed forward the vectors from

00:15:29,930 --> 00:15:33,920
one layer to another and each layer just

00:15:32,180 --> 00:15:40,130
recognizes different features of the

00:15:33,920 --> 00:15:43,940
previous layer so this layer basically

00:15:40,130 --> 00:15:47,209
yeah this this layer just found that

00:15:43,940 --> 00:15:49,819
these vectors when when these light up

00:15:47,209 --> 00:15:52,000
these these two neurons light up it

00:15:49,819 --> 00:15:54,740
causes this neuron to to activate

00:15:52,000 --> 00:15:56,990
because the features that it has found

00:15:54,740 --> 00:15:59,630
is the vertical line and another feature

00:15:56,990 --> 00:16:05,870
of something else which is a black

00:15:59,630 --> 00:16:09,079
surrounding for example yes

00:16:05,870 --> 00:16:13,089
but each each each number is either that

00:16:09,079 --> 00:16:16,089
is a feature it's a feature in its own

00:16:13,089 --> 00:16:16,089
okay

00:16:20,760 --> 00:16:23,330
it

00:16:25,320 --> 00:16:29,860
yeah

00:16:26,750 --> 00:16:29,860
that's the basic idea

00:16:31,550 --> 00:16:38,750
this method can actually get an amazing

00:16:34,640 --> 00:16:41,300
0.2% error rate which is amazing it is

00:16:38,750 --> 00:16:46,790
better than than what humans do on this

00:16:41,300 --> 00:16:48,290
data set okay so getting back to you log

00:16:46,790 --> 00:16:50,029
records how do we apply all these

00:16:48,290 --> 00:16:54,500
methods in order to improve our

00:16:50,029 --> 00:16:57,500
monitoring we want some to check out

00:16:54,500 --> 00:17:00,050
several problems one is that log data is

00:16:57,500 --> 00:17:02,360
very redundant we see a lot of instances

00:17:00,050 --> 00:17:04,459
of the same logs just with different

00:17:02,360 --> 00:17:08,300
parameters inside which we know they

00:17:04,459 --> 00:17:10,179
live of the same type but it's hard for

00:17:08,300 --> 00:17:14,209
a machine to recognize this

00:17:10,179 --> 00:17:16,520
additionally the the data is so abundant

00:17:14,209 --> 00:17:20,350
that finding the rare events and the

00:17:16,520 --> 00:17:22,520
important events is very hard

00:17:20,350 --> 00:17:25,699
additionally when we when we look at

00:17:22,520 --> 00:17:29,600
systems that users interact with or have

00:17:25,699 --> 00:17:31,580
complex components inside each action is

00:17:29,600 --> 00:17:35,210
represented by several log records and

00:17:31,580 --> 00:17:37,250
then looking at the flow of records we

00:17:35,210 --> 00:17:40,040
have many unrelated events just coming

00:17:37,250 --> 00:17:42,559
all designed from different actions and

00:17:40,040 --> 00:17:44,840
happening in the system and tracing it

00:17:42,559 --> 00:17:49,520
the complete sequence of locks of a of a

00:17:44,840 --> 00:17:52,790
single action is very hard the solution

00:17:49,520 --> 00:17:56,419
is to identify log prototypes or log

00:17:52,790 --> 00:17:59,300
templates cluster which logs represent

00:17:56,419 --> 00:18:02,690
an action alert one action are

00:17:59,300 --> 00:18:04,340
incomplete or anomalous and notify about

00:18:02,690 --> 00:18:05,990
new arrows which have never occurred

00:18:04,340 --> 00:18:09,080
before because now we know which

00:18:05,990 --> 00:18:11,929
prototype each log is and we can know

00:18:09,080 --> 00:18:14,960
that a new log arrow prototype has come

00:18:11,929 --> 00:18:16,970
and arrived to the system which you

00:18:14,960 --> 00:18:21,440
should be notified about and there is of

00:18:16,970 --> 00:18:27,140
course much more to see so a real-world

00:18:21,440 --> 00:18:30,440
example this is an occurrences graph of

00:18:27,140 --> 00:18:34,280
all the logs that are B to C system and

00:18:30,440 --> 00:18:37,700
it's each column here represents a

00:18:34,280 --> 00:18:41,970
different log type the frequency here is

00:18:37,700 --> 00:18:45,180
the ratio of how of this log type

00:18:41,970 --> 00:18:50,390
to all the other logs so this log comes

00:18:45,180 --> 00:18:50,390
with around 9.8 percent of all the time

00:18:51,170 --> 00:18:58,980
the color represents represents the the

00:18:56,280 --> 00:19:01,200
severity of the log record and we can

00:18:58,980 --> 00:19:02,670
see that the most then most frequent log

00:19:01,200 --> 00:19:05,390
records comprised of around sixty

00:19:02,670 --> 00:19:08,280
percent of the data cutting around here

00:19:05,390 --> 00:19:11,700
most of these log around between one and

00:19:08,280 --> 00:19:14,910
ten percent and a lot of the data is

00:19:11,700 --> 00:19:17,550
redundant we almost always miss all the

00:19:14,910 --> 00:19:19,980
the long tail that we see here now a

00:19:17,550 --> 00:19:22,440
good machine learning for for log

00:19:19,980 --> 00:19:25,560
management would give me statistics

00:19:22,440 --> 00:19:28,200
about these these are the my my core

00:19:25,560 --> 00:19:30,360
business in my system these are mostly

00:19:28,200 --> 00:19:34,440
info locks of reporting actions that

00:19:30,360 --> 00:19:36,600
users or machines are doing and it will

00:19:34,440 --> 00:19:38,130
also be able to alert me on these rare

00:19:36,600 --> 00:19:41,580
events which mostly comprised of

00:19:38,130 --> 00:19:47,130
critical errors and and warning logs

00:19:41,580 --> 00:19:48,810
that I'm I most likely missing so we'll

00:19:47,130 --> 00:19:53,000
have correlations from these ones and

00:19:48,810 --> 00:19:53,000
then statistics and alerts on these ones

00:19:53,920 --> 00:19:59,320
okay possible log analysis pipeline I'm

00:19:57,640 --> 00:20:05,710
going to warn you brace yourself

00:19:59,320 --> 00:20:08,980
math is coming so the idea is we want to

00:20:05,710 --> 00:20:10,780
cluster similar log records into log

00:20:08,980 --> 00:20:13,630
prototypes so we have these just

00:20:10,780 --> 00:20:17,290
different kinds of logs and different

00:20:13,630 --> 00:20:19,960
and similar logs which are which I still

00:20:17,290 --> 00:20:22,000
don't know they're of the same type so

00:20:19,960 --> 00:20:23,890
we need to find a distance metric to

00:20:22,000 --> 00:20:26,340
compare to log records and tell me that

00:20:23,890 --> 00:20:26,340
they are similar

00:20:26,520 --> 00:20:32,110
after that we create a new log template

00:20:29,410 --> 00:20:34,060
from these similar log records and then

00:20:32,110 --> 00:20:35,860
we can even identify the variables

00:20:34,060 --> 00:20:38,290
inside these log records in order to

00:20:35,860 --> 00:20:42,790
find statistics of how these variables

00:20:38,290 --> 00:20:45,580
react for example if you have these two

00:20:42,790 --> 00:20:49,510
log records creating tag on stream minus

00:20:45,580 --> 00:20:50,710
1 position 42 and log to everyone here

00:20:49,510 --> 00:20:52,570
in the room who's looking on this

00:20:50,710 --> 00:20:55,810
understand that this is the same log

00:20:52,570 --> 00:21:00,880
template but comparing these is of

00:20:55,810 --> 00:21:05,380
course giving us a false in in in a

00:21:00,880 --> 00:21:07,210
strictly code coding scheme a different

00:21:05,380 --> 00:21:10,090
solution may be to look at the the

00:21:07,210 --> 00:21:12,070
single words of each of these log

00:21:10,090 --> 00:21:14,470
records and compare them and then we can

00:21:12,070 --> 00:21:17,530
find that most of the words are actually

00:21:14,470 --> 00:21:20,800
similar and can be found in both logs

00:21:17,530 --> 00:21:22,510
but they I'm not sure if this is distant

00:21:20,800 --> 00:21:26,200
enough or close enough in order to be

00:21:22,510 --> 00:21:28,290
clustered together and this approach is

00:21:26,200 --> 00:21:31,390
also also poses the problem and

00:21:28,290 --> 00:21:33,340
comparing all these substrings of all

00:21:31,390 --> 00:21:37,840
the logs in my system is an extremely

00:21:33,340 --> 00:21:39,580
expensive operation so what are we going

00:21:37,840 --> 00:21:42,790
to do we're going to find a heuristic

00:21:39,580 --> 00:21:46,480
distance metrics that will take a log

00:21:42,790 --> 00:21:48,909
record break it into the words that it

00:21:46,480 --> 00:21:50,529
has and then

00:21:48,909 --> 00:21:54,159
through hashing of locality-sensitive

00:21:50,529 --> 00:21:57,369
hashing we will receive a binary

00:21:54,159 --> 00:22:00,159
representation of a log record and this

00:21:57,369 --> 00:22:01,929
binary representation can now be used in

00:22:00,159 --> 00:22:04,299
order to be compared to all the other

00:22:01,929 --> 00:22:06,759
logs by this very quick way of bitwise

00:22:04,299 --> 00:22:09,309
operations which are the quickest kinds

00:22:06,759 --> 00:22:11,379
of operations that a computer can do and

00:22:09,309 --> 00:22:13,570
now we only need to construct this

00:22:11,379 --> 00:22:15,279
listing once for each log and we can

00:22:13,570 --> 00:22:21,450
compare it many many times to many other

00:22:15,279 --> 00:22:24,369
log records this process gives us that

00:22:21,450 --> 00:22:26,950
these two logs are actually the same log

00:22:24,369 --> 00:22:29,559
prototype and they have two variables at

00:22:26,950 --> 00:22:30,450
these locations were the decent larges

00:22:29,559 --> 00:22:34,330
happen

00:22:30,450 --> 00:22:38,019
the result is debt of M log records we

00:22:34,330 --> 00:22:40,479
get n prototypes where n is much much

00:22:38,019 --> 00:22:42,820
smaller than M M is in the billions

00:22:40,479 --> 00:22:45,009
billions of log records that we our

00:22:42,820 --> 00:22:46,929
system creates every day but the total

00:22:45,009 --> 00:22:51,700
amounts of log prototypes that we

00:22:46,929 --> 00:22:54,149
actually create are 900 1000 for log for

00:22:51,700 --> 00:22:54,149
large systems

00:22:59,380 --> 00:23:03,610
after having our log prototype we can

00:23:02,140 --> 00:23:06,910
now collect the values that these

00:23:03,610 --> 00:23:09,160
variables have and find the distribution

00:23:06,910 --> 00:23:11,230
that best represent the behavior of

00:23:09,160 --> 00:23:14,020
these log records and these variables

00:23:11,230 --> 00:23:17,919
inside them after we find the

00:23:14,020 --> 00:23:20,230
distributions we can now define the the

00:23:17,919 --> 00:23:22,780
bands which are anomalous in the

00:23:20,230 --> 00:23:25,360
behavior of these logs and then alert on

00:23:22,780 --> 00:23:27,070
whenever these variables jump out of the

00:23:25,360 --> 00:23:32,320
normal behavior that we expect from this

00:23:27,070 --> 00:23:35,080
log record additionally we will want to

00:23:32,320 --> 00:23:37,179
find what sequence of logs represent an

00:23:35,080 --> 00:23:40,630
action in our system so how do we find

00:23:37,179 --> 00:23:42,549
sequences we want to find logs that

00:23:40,630 --> 00:23:45,780
correlate together that come together in

00:23:42,549 --> 00:23:49,539
order to find sequences that are related

00:23:45,780 --> 00:23:52,299
so Willis will assume that logs are

00:23:49,539 --> 00:23:54,370
independent that means that two logs can

00:23:52,299 --> 00:23:58,419
come one after another in a random in a

00:23:54,370 --> 00:24:00,850
random fashion and if we find logs that

00:23:58,419 --> 00:24:02,770
break this assumption then it is

00:24:00,850 --> 00:24:04,990
probably that because they are related

00:24:02,770 --> 00:24:07,780
together to actions that come one after

00:24:04,990 --> 00:24:09,520
another we'll will break our assumption

00:24:07,780 --> 00:24:14,049
of Independence because we will find

00:24:09,520 --> 00:24:18,970
them a lot of time this is called the G

00:24:14,049 --> 00:24:21,010
test and it is about the observed amount

00:24:18,970 --> 00:24:25,150
of occurrences that we found of

00:24:21,010 --> 00:24:27,159
sequences against the expected and it is

00:24:25,150 --> 00:24:29,590
a statistical test that can show us when

00:24:27,159 --> 00:24:31,210
have we have we broken and independence

00:24:29,590 --> 00:24:33,159
assumption in order to take these and

00:24:31,210 --> 00:24:38,159
understand that two logs are related or

00:24:33,159 --> 00:24:43,270
sequences related let's take it into a

00:24:38,159 --> 00:24:45,880
into a real-world example if I have a

00:24:43,270 --> 00:24:49,030
purchase request coming to my my system

00:24:45,880 --> 00:24:51,520
I then need to get the cart of that

00:24:49,030 --> 00:24:54,610
purchase from the database process that

00:24:51,520 --> 00:24:57,480
request authenticate the payment maybe

00:24:54,610 --> 00:24:59,980
update the BI system and another one

00:24:57,480 --> 00:25:02,169
send a response also to the client after

00:24:59,980 --> 00:25:03,610
I've processed it and then have probably

00:25:02,169 --> 00:25:04,920
have a lot of that marked this purchase

00:25:03,610 --> 00:25:09,100
is complete

00:25:04,920 --> 00:25:10,780
following the templates search that

00:25:09,100 --> 00:25:13,780
we've that we've defined in the

00:25:10,780 --> 00:25:16,270
beginning we actually have here not

00:25:13,780 --> 00:25:18,730
eight but seven different kinds of logs

00:25:16,270 --> 00:25:21,790
because Update VI system 1 and system 2

00:25:18,730 --> 00:25:27,430
is actually very similar log that it's

00:25:21,790 --> 00:25:29,350
just reported in the same way and now we

00:25:27,430 --> 00:25:32,280
want to count all the sequences of

00:25:29,350 --> 00:25:35,350
length 2 we'll call them two sequences

00:25:32,280 --> 00:25:37,320
l1 the log template 1 and log templates

00:25:35,350 --> 00:25:41,920
you the poor chase and gate card are

00:25:37,320 --> 00:25:44,560
frequent sequence to sequence and we

00:25:41,920 --> 00:25:48,130
expect to never see all or almost never

00:25:44,560 --> 00:25:50,650
see sequence 2 sequences which compose

00:25:48,130 --> 00:25:53,260
compares or poochie's request and

00:25:50,650 --> 00:25:55,780
authenticate payment these will happen

00:25:53,260 --> 00:25:58,330
rarely only when two actions happen one

00:25:55,780 --> 00:26:01,540
after another and these and the actions

00:25:58,330 --> 00:26:04,210
come into the into the same time where

00:26:01,540 --> 00:26:05,500
one poor chase has just begun and

00:26:04,210 --> 00:26:07,180
another push is already authenticated

00:26:05,500 --> 00:26:11,260
the payment so we'll find them one after

00:26:07,180 --> 00:26:15,970
another but mostly l1 and l2 will be the

00:26:11,260 --> 00:26:18,520
frequent sequences that we find after

00:26:15,970 --> 00:26:20,680
mapping all the possible two sequences

00:26:18,520 --> 00:26:23,350
that we have in our system we need to

00:26:20,680 --> 00:26:26,590
normalize their scores which means that

00:26:23,350 --> 00:26:29,620
we found a thousand times the l1 and l2

00:26:26,590 --> 00:26:32,830
and other sequences we found several

00:26:29,620 --> 00:26:36,400
times but much lower we'll subtract the

00:26:32,830 --> 00:26:39,370
average of these occurrences and divided

00:26:36,400 --> 00:26:42,700
by the variance and now we have a

00:26:39,370 --> 00:26:47,800
normalized score of how frequent or how

00:26:42,700 --> 00:26:49,450
related these sequences of logs are when

00:26:47,800 --> 00:26:51,850
we have the two the normalized two

00:26:49,450 --> 00:26:54,660
sequences we can now take each of the

00:26:51,850 --> 00:26:57,700
sequences that we found here and try to

00:26:54,660 --> 00:27:00,040
enlighten them to a three sequence add

00:26:57,700 --> 00:27:03,930
another log record and then try to find

00:27:00,040 --> 00:27:03,930
if they are still frequent in our system

00:27:04,340 --> 00:27:09,960
when repeating the process we're taking

00:27:07,140 --> 00:27:12,360
K sequence and trying to convert it into

00:27:09,960 --> 00:27:14,970
K plus-1 sequence and we're doing this

00:27:12,360 --> 00:27:16,230
as long as the G tests course tells us

00:27:14,970 --> 00:27:18,570
that we're still breaking the

00:27:16,230 --> 00:27:21,780
independence assumption and the the

00:27:18,570 --> 00:27:25,080
nominal score of the K sequence is is

00:27:21,780 --> 00:27:29,640
still smaller than the normalized score

00:27:25,080 --> 00:27:32,130
of the k plus 1 in this way we receive

00:27:29,640 --> 00:27:36,630
all the the sequences of length K which

00:27:32,130 --> 00:27:39,150
are correlated in our system and now

00:27:36,630 --> 00:27:41,550
each action is represented inside our

00:27:39,150 --> 00:27:43,650
system as sequences that we can follow

00:27:41,550 --> 00:27:45,300
and we can find statistics about them we

00:27:43,650 --> 00:27:50,820
can look into the variables that they

00:27:45,300 --> 00:27:55,500
have etc so if we look at the log

00:27:50,820 --> 00:27:57,600
sequence a sigma 3 sequence with ratio 1

00:27:55,500 --> 00:28:00,240
1 1 would say that we expect that each

00:27:57,600 --> 00:28:03,570
log prototype has the same occurrences

00:28:00,240 --> 00:28:07,710
of all the others in our example here we

00:28:03,570 --> 00:28:10,140
have seven seven sequence with one log

00:28:07,710 --> 00:28:13,050
record showing twice so we expect the

00:28:10,140 --> 00:28:18,420
ratio to act of this log to be twice as

00:28:13,050 --> 00:28:21,030
much as the others when these sequences

00:28:18,420 --> 00:28:23,880
are now broken we need to count how much

00:28:21,030 --> 00:28:28,130
how many times they are broken and if

00:28:23,880 --> 00:28:31,470
these are broken by a certain ratio and

00:28:28,130 --> 00:28:33,030
the p-value the statistical significance

00:28:31,470 --> 00:28:35,280
of this is high enough then we will

00:28:33,030 --> 00:28:37,290
alert that there is anomaly because no

00:28:35,280 --> 00:28:39,150
one is interested in a sequence that is

00:28:37,290 --> 00:28:42,420
broken once it is probably just a

00:28:39,150 --> 00:28:45,420
network malfunction or or something or a

00:28:42,420 --> 00:28:46,920
different user name/password that wasn't

00:28:45,420 --> 00:28:49,080
in certain right there are many

00:28:46,920 --> 00:28:51,570
sequences that break all the time but if

00:28:49,080 --> 00:28:53,130
it goes to a significant level then you

00:28:51,570 --> 00:28:55,110
probably have an error in your system

00:28:53,130 --> 00:28:58,290
and you you will be notified that in

00:28:55,110 --> 00:29:01,170
real time additionally software is

00:28:58,290 --> 00:29:02,850
constantly changing which means that we

00:29:01,170 --> 00:29:06,180
will be updating the model all the time

00:29:02,850 --> 00:29:09,510
and as long as these sequences continue

00:29:06,180 --> 00:29:13,040
to come and there is much more that we

00:29:09,510 --> 00:29:13,040
have we haven't explored here

00:29:13,190 --> 00:29:17,640
for summary everyone will analyze their

00:29:15,840 --> 00:29:19,380
big data with machine learning there is

00:29:17,640 --> 00:29:22,560
no other option we just have too much

00:29:19,380 --> 00:29:24,720
data and we cannot handle it however it

00:29:22,560 --> 00:29:26,580
is hard to done to do by yourself but if

00:29:24,720 --> 00:29:28,500
you do it it is extremely rewarding and

00:29:26,580 --> 00:29:30,540
we can customize it to exactly what you

00:29:28,500 --> 00:29:33,180
want and what you need and most

00:29:30,540 --> 00:29:34,920
importantly you can focus on the

00:29:33,180 --> 00:29:37,200
products that you want to build and you

00:29:34,920 --> 00:29:39,830
want to support instead of the bugs that

00:29:37,200 --> 00:29:39,830
are inside them

00:29:41,300 --> 00:29:45,420
thank you if you have any further

00:29:43,170 --> 00:29:47,700
questions you can come I'm available

00:29:45,420 --> 00:29:50,070
here and you can send me an email and

00:29:47,700 --> 00:29:52,680
visit our website to see how we

00:29:50,070 --> 00:29:54,330
implemented such a method and what we

00:29:52,680 --> 00:29:56,570
can do with Korra logics for your log

00:29:54,330 --> 00:30:01,700
records

00:29:56,570 --> 00:30:01,700

YouTube URL: https://www.youtube.com/watch?v=8EOTZnN_V8g


