Title: Exactly Once Delivery is a Harsh Mistress - Natan Silnitsy
Publication date: 2020-01-13
Playlist: DevOpsDays Tel Aviv 2019
Description: 
	Is Exactly Once Delivery a pipe dream? Recent versions of Kafka have claimed they have made it a reality.

In this talk I will go over the basic theory of messaging in distributed systems, the different message delivery guarantees and the protocols that implement them.

I will focus on exactly once delivery guarantees and the way Kafka implements it with transaction based messaging protocol between the producer and the consumer.
Including a discussion of the latency/throughput trade-offs, resource utilisation and its overall shortcomings.

Finally, I will show how it has helped power Wix's event-driven data-streaming and data-storage Infrastructure we hope to open source soon.
This infrastructure greatly simplifies building and maintaining services in a highly distributed production environment.
It lets developers focus on business logic instead of keep making sure their code is idempotent and fault-tolerant.
Captions: 
	00:00:00,440 --> 00:00:03,580
[Music]

00:00:04,520 --> 00:00:09,360
okay I would like to start with a quick

00:00:07,500 --> 00:00:13,190
question who is familiar with the raise

00:00:09,360 --> 00:00:16,199
of hand with how Kafka mr. Drucker works

00:00:13,190 --> 00:00:18,960
okay good I'm glad

00:00:16,199 --> 00:00:20,789
so welcome everyone my name is Natasha

00:00:18,960 --> 00:00:23,900
netsuke and I'm a back-end

00:00:20,789 --> 00:00:27,090
infrastructure developer at wix.com and

00:00:23,900 --> 00:00:29,460
part of the team that builds libraries

00:00:27,090 --> 00:00:32,070
and all kinds of tools on top of the

00:00:29,460 --> 00:00:34,380
Kafka message broker and we've recently

00:00:32,070 --> 00:00:37,320
started incorporating exactly ones

00:00:34,380 --> 00:00:40,350
delivery into our system so if I will be

00:00:37,320 --> 00:00:42,870
a good opportunity to share how exactly

00:00:40,350 --> 00:00:47,219
this Kafka implement exactly one's

00:00:42,870 --> 00:00:50,219
delivery and why is it so difficult so

00:00:47,219 --> 00:00:53,610
at weeks we of course have a distributed

00:00:50,219 --> 00:00:57,690
system comprising roughly 1,400 micro

00:00:53,610 --> 00:01:01,530
services and a lot of the flows include

00:00:57,690 --> 00:01:05,909
the event-driven style for instance when

00:01:01,530 --> 00:01:08,689
you do site editing or publishing as

00:01:05,909 --> 00:01:12,030
which is a website building platform and

00:01:08,689 --> 00:01:17,369
we do it like I said using Kafka we have

00:01:12,030 --> 00:01:20,610
over 850 million messages every day now

00:01:17,369 --> 00:01:23,909
I wanted to explain the difficulty

00:01:20,610 --> 00:01:27,630
without doing exactly once using the

00:01:23,909 --> 00:01:30,090
classic ecommerce flow and one one

00:01:27,630 --> 00:01:32,220
reason is that which as of course

00:01:30,090 --> 00:01:34,770
ecommerce websites but the other one is

00:01:32,220 --> 00:01:37,530
that ecommerce is a very familiar way

00:01:34,770 --> 00:01:40,560
for the audience to understand these

00:01:37,530 --> 00:01:42,990
concepts so let's say we have purchased

00:01:40,560 --> 00:01:45,590
completed the notification for service

00:01:42,990 --> 00:01:49,110
and then we want to update the inventory

00:01:45,590 --> 00:01:52,920
to reflect that the items that were in

00:01:49,110 --> 00:01:56,729
the order need to be reduced in number

00:01:52,920 --> 00:01:59,250
and we're dealing with some legacy API

00:01:56,729 --> 00:02:03,780
where you can only update one item at a

00:01:59,250 --> 00:02:07,200
time okay so we go through all the items

00:02:03,780 --> 00:02:11,190
in the inventory in the order side and

00:02:07,200 --> 00:02:13,140
then we want to update them but doing

00:02:11,190 --> 00:02:13,950
all this exactly once can be quite

00:02:13,140 --> 00:02:17,459
difficult

00:02:13,950 --> 00:02:19,890
I mean we could need to deal here with

00:02:17,459 --> 00:02:22,830
some failure and some errors and

00:02:19,890 --> 00:02:25,530
restarts and reach rights and then we

00:02:22,830 --> 00:02:28,140
can get to a situation where okay let's

00:02:25,530 --> 00:02:29,760
say we managed the first item and reduce

00:02:28,140 --> 00:02:32,160
the number and then we go through the

00:02:29,760 --> 00:02:34,830
second item reduce the number but then

00:02:32,160 --> 00:02:38,190
something blows up and we start over we

00:02:34,830 --> 00:02:41,310
try then we can potentially get to a

00:02:38,190 --> 00:02:44,849
state where the inventory has completely

00:02:41,310 --> 00:02:45,420
incorrect information so how do we solve

00:02:44,849 --> 00:02:51,530
that

00:02:45,420 --> 00:02:56,640
so you can maybe use some database and

00:02:51,530 --> 00:02:59,190
handle the diversion updates sorry using

00:02:56,640 --> 00:03:02,040
some versions to to manage these updates

00:02:59,190 --> 00:03:03,900
of the inventory and to do some

00:03:02,040 --> 00:03:06,019
optimistic locking with the data was

00:03:03,900 --> 00:03:07,980
something like that so that's a lot of

00:03:06,019 --> 00:03:10,410
management and boilerplate that you will

00:03:07,980 --> 00:03:13,590
need to do on the application level but

00:03:10,410 --> 00:03:15,810
I think using Kafka and the exactly ones

00:03:13,590 --> 00:03:17,910
semantics that Kafka offers can be a

00:03:15,810 --> 00:03:21,359
better approach and we'll see a little

00:03:17,910 --> 00:03:23,250
bit more about this with this talk so

00:03:21,359 --> 00:03:26,359
achieving exactly ones delivery in

00:03:23,250 --> 00:03:28,950
distributed systems is really not easy

00:03:26,359 --> 00:03:32,609
first of all the message delivery of the

00:03:28,950 --> 00:03:35,220
network is unreliable we can mitigate

00:03:32,609 --> 00:03:38,070
this issue a little bit by introducing a

00:03:35,220 --> 00:03:41,370
message broker and then once the message

00:03:38,070 --> 00:03:43,560
is produced and put inside the broker we

00:03:41,370 --> 00:03:45,989
at least know that it will not be lost

00:03:43,560 --> 00:03:48,200
anymore and we could have all kinds of

00:03:45,989 --> 00:03:50,579
issues with the consumers and producers

00:03:48,200 --> 00:03:54,329
but at least we know the message is not

00:03:50,579 --> 00:03:58,230
lost and we can handle it now I'll do a

00:03:54,329 --> 00:03:59,519
very quick review of how Kafka works I

00:03:58,230 --> 00:04:02,670
know that most of you are familiar with

00:03:59,519 --> 00:04:05,340
it so you send messages in Kafka over

00:04:02,670 --> 00:04:08,130
two specific topics and these topics are

00:04:05,340 --> 00:04:08,880
divided into partitions so partitions

00:04:08,130 --> 00:04:12,900
are basically

00:04:08,880 --> 00:04:15,900
append-only logs where you add new

00:04:12,900 --> 00:04:17,910
messages at the end and it's very high

00:04:15,900 --> 00:04:18,329
performance because usually you'll do

00:04:17,910 --> 00:04:20,280
that

00:04:18,329 --> 00:04:24,990
in memory and sequentially it's quite

00:04:20,280 --> 00:04:26,380
easy and then you read it sequentially

00:04:24,990 --> 00:04:29,170
as well

00:04:26,380 --> 00:04:31,300
and you can scale up the sequential

00:04:29,170 --> 00:04:32,980
processing by just introducing more

00:04:31,300 --> 00:04:34,960
partitions the more partitions you have

00:04:32,980 --> 00:04:38,770
the higher degree of parallelism and

00:04:34,960 --> 00:04:41,230
scale so of course we see here an

00:04:38,770 --> 00:04:43,900
example of Kafka producer producing new

00:04:41,230 --> 00:04:46,290
messages at the very end and then the

00:04:43,900 --> 00:04:49,150
Kafka consumer reads messages

00:04:46,290 --> 00:04:51,610
sequentially like I said once it's done

00:04:49,150 --> 00:04:55,690
processing some message it will commit

00:04:51,610 --> 00:04:58,180
the message to the partition and if

00:04:55,690 --> 00:05:00,100
there's some problems we start or

00:04:58,180 --> 00:05:02,680
rebalancing of the partitions among the

00:05:00,100 --> 00:05:05,530
consumers then we pick up where we left

00:05:02,680 --> 00:05:10,030
the first message that is uncommitted in

00:05:05,530 --> 00:05:12,220
this case message with offset 3 ok like

00:05:10,030 --> 00:05:14,770
I said very short overview of how Kafka

00:05:12,220 --> 00:05:18,160
works with the messages run over

00:05:14,770 --> 00:05:21,340
partitions now let's look at which

00:05:18,160 --> 00:05:24,280
options we have for message delivery so

00:05:21,340 --> 00:05:27,400
we have three basic options at least

00:05:24,280 --> 00:05:30,900
once delivery at most once and exactly

00:05:27,400 --> 00:05:33,790
once if we look at the at least once

00:05:30,900 --> 00:05:37,030
that means for the Kafka producer that

00:05:33,790 --> 00:05:38,680
will retry on every failure because we

00:05:37,030 --> 00:05:42,670
don't want to get to a situation where

00:05:38,680 --> 00:05:44,230
we miss some message right if a site if

00:05:42,670 --> 00:05:46,110
you want to notify the side was created

00:05:44,230 --> 00:05:49,240
you really don't want to miss that

00:05:46,110 --> 00:05:50,710
notification so if there's some problem

00:05:49,240 --> 00:05:54,700
with acknowledgement from the Kafka

00:05:50,710 --> 00:05:56,770
broker then we just retry and we can end

00:05:54,700 --> 00:06:00,490
up with duplicate messages on the

00:05:56,770 --> 00:06:03,640
partition now over on the consumer side

00:06:00,490 --> 00:06:06,460
we will and if you want to do at least

00:06:03,640 --> 00:06:09,640
once we will process the message before

00:06:06,460 --> 00:06:11,320
we commit it back to the partition now

00:06:09,640 --> 00:06:13,480
of course if there's some issue here

00:06:11,320 --> 00:06:16,300
then we can end up with double multiple

00:06:13,480 --> 00:06:18,280
processes because once we start up again

00:06:16,300 --> 00:06:21,580
we will always go with the first

00:06:18,280 --> 00:06:23,680
uncommitted message now with at most

00:06:21,580 --> 00:06:26,980
once with a consumer it's the flip side

00:06:23,680 --> 00:06:30,940
we first commit a message and only then

00:06:26,980 --> 00:06:32,560
do the processing so this will be

00:06:30,940 --> 00:06:34,960
helpful for scenarios where you don't

00:06:32,560 --> 00:06:37,450
care if some messages are dropped and

00:06:34,960 --> 00:06:39,700
lost you can think about the situation

00:06:37,450 --> 00:06:42,460
with your sending user

00:06:39,700 --> 00:06:44,560
some text message reminders about

00:06:42,460 --> 00:06:46,120
something and you really don't want to

00:06:44,560 --> 00:06:49,390
annoy them you don't want them to get to

00:06:46,120 --> 00:06:51,070
placate messages for nothing and if once

00:06:49,390 --> 00:06:52,000
in a while they don't get a reminder

00:06:51,070 --> 00:06:58,090
it's not a big deal

00:06:52,000 --> 00:07:00,250
so here if we retry it means that if we

00:06:58,090 --> 00:07:00,940
start over we will start from the next

00:07:00,250 --> 00:07:03,280
message

00:07:00,940 --> 00:07:07,020
so no double processing of the same

00:07:03,280 --> 00:07:09,970
message but you can get to message lost

00:07:07,020 --> 00:07:11,650
but classically what we really want to

00:07:09,970 --> 00:07:13,810
achieve without all this mess exact

00:07:11,650 --> 00:07:16,540
exactly once right we want to process

00:07:13,810 --> 00:07:18,220
these messages exactly once by the

00:07:16,540 --> 00:07:22,510
consumer we don't want to worry about it

00:07:18,220 --> 00:07:24,880
but we live in a real world with the cat

00:07:22,510 --> 00:07:28,960
fur and everything else and it's really

00:07:24,880 --> 00:07:32,560
hard to achieve actually in theory it's

00:07:28,960 --> 00:07:34,390
probably impossible to to achieve there

00:07:32,560 --> 00:07:37,060
is the famous thought experiment called

00:07:34,390 --> 00:07:39,790
two generals problem where we have two

00:07:37,060 --> 00:07:42,190
generals in this case Alice and Bob who

00:07:39,790 --> 00:07:44,470
are trying to coordinate an attack over

00:07:42,190 --> 00:07:47,320
when between them there is the enemy in

00:07:44,470 --> 00:07:49,690
the middle so let's see they want to

00:07:47,320 --> 00:07:51,730
coordinate an attack on when to attack

00:07:49,690 --> 00:07:53,890
this enemy so Alice is sending a

00:07:51,730 --> 00:07:56,980
messenger over with the time of the

00:07:53,890 --> 00:08:00,100
attack over to Bob but unfortunately the

00:07:56,980 --> 00:08:02,850
enemy intercepted the messenger and Alex

00:08:00,100 --> 00:08:05,890
didn't receive an acknowledgment okay

00:08:02,850 --> 00:08:08,770
Alice will send another messenger and

00:08:05,890 --> 00:08:10,840
let's say even that this time Bob

00:08:08,770 --> 00:08:13,060
received this messenger and he agrees

00:08:10,840 --> 00:08:15,850
for the time of the attack and he sends

00:08:13,060 --> 00:08:18,970
his own acknowledgment messenger and

00:08:15,850 --> 00:08:22,000
let's say this time even Alice received

00:08:18,970 --> 00:08:24,640
this messenger with acknowledgment but

00:08:22,000 --> 00:08:27,400
we had a problem here right how this bad

00:08:24,640 --> 00:08:31,300
know that Alice received the

00:08:27,400 --> 00:08:34,930
acknowledgment okay maybe we'll have

00:08:31,300 --> 00:08:37,960
Alice send an acknowledgment messenger

00:08:34,930 --> 00:08:39,700
that she got the acknowledgment but you

00:08:37,960 --> 00:08:42,880
think about it we kinda end up here with

00:08:39,700 --> 00:08:45,690
some and a slope and catch-22 where we

00:08:42,880 --> 00:08:49,180
can never have Alice and Bob actually

00:08:45,690 --> 00:08:51,490
agreeing for the time of the attack and

00:08:49,180 --> 00:08:53,260
knowing that both sides know about it so

00:08:51,490 --> 00:08:55,360
you can think about this

00:08:53,260 --> 00:08:57,970
and the terms of microservices right we

00:08:55,360 --> 00:09:00,310
have Ellis service and Bob service and

00:08:57,970 --> 00:09:03,820
Alice is trying to send a message over

00:09:00,310 --> 00:09:05,970
to Bob service but we have the enemy in

00:09:03,820 --> 00:09:09,370
the middle which it could be network

00:09:05,970 --> 00:09:11,440
failures some hardware failure maybe

00:09:09,370 --> 00:09:14,440
even the Kafka message broker is down

00:09:11,440 --> 00:09:17,320
the moment so we cannot guarantee that

00:09:14,440 --> 00:09:20,670
the message is actually delivered and

00:09:17,320 --> 00:09:24,780
acknowledged one time exactly once

00:09:20,670 --> 00:09:28,570
so is exactly once actually impossible

00:09:24,780 --> 00:09:31,360
Kafka does offer some solution for the

00:09:28,570 --> 00:09:36,120
problem and and let's look at exactly

00:09:31,360 --> 00:09:39,670
how they do that so we have this classic

00:09:36,120 --> 00:09:43,330
message passing flow here we start off

00:09:39,670 --> 00:09:45,900
with some HTTP endpoint and we get some

00:09:43,330 --> 00:09:50,110
requests we put it through the message

00:09:45,900 --> 00:09:52,990
producer and put this in topic a then in

00:09:50,110 --> 00:09:53,530
the middle we have both a consumer and a

00:09:52,990 --> 00:09:56,580
producer

00:09:53,530 --> 00:09:59,110
so it's let's call it a processor and

00:09:56,580 --> 00:10:02,290
this processor will read the message

00:09:59,110 --> 00:10:06,220
from topic a do some processing and put

00:10:02,290 --> 00:10:10,060
it into topic B then we have the

00:10:06,220 --> 00:10:13,840
consumer at the very end that will need

00:10:10,060 --> 00:10:15,970
to see the processed message and we'll

00:10:13,840 --> 00:10:18,190
do some side effect outside of Kafka

00:10:15,970 --> 00:10:20,520
let's say right into database and let's

00:10:18,190 --> 00:10:23,860
call it the this consumer and observer

00:10:20,520 --> 00:10:27,130
okay so we have this flow and we want to

00:10:23,860 --> 00:10:29,710
introduce exactly one semantics here so

00:10:27,130 --> 00:10:30,790
let's start with the Kafka producer how

00:10:29,710 --> 00:10:33,820
we can we make sure that it only

00:10:30,790 --> 00:10:35,010
produces one message at a time well this

00:10:33,820 --> 00:10:38,950
is actually quite simple

00:10:35,010 --> 00:10:41,400
we Kafka has the possibility to

00:10:38,950 --> 00:10:43,750
introduce an idempotent producer

00:10:41,400 --> 00:10:46,510
idempotency means that no matter how

00:10:43,750 --> 00:10:49,170
many times we try we never change the

00:10:46,510 --> 00:10:51,430
state of the system so you just turn on

00:10:49,170 --> 00:10:54,310
idempotence here and it will

00:10:51,430 --> 00:10:58,210
automatically attach an offset and index

00:10:54,310 --> 00:11:00,520
to the message and then if the Kafka

00:10:58,210 --> 00:11:02,770
broker notices that the producer for

00:11:00,520 --> 00:11:05,530
some reason is sending the message again

00:11:02,770 --> 00:11:06,610
with the same offset it knows it's a

00:11:05,530 --> 00:11:09,100
duplicate

00:11:06,610 --> 00:11:10,380
it will discard this message okay pretty

00:11:09,100 --> 00:11:12,820
simple

00:11:10,380 --> 00:11:15,399
things get a little bit more complicated

00:11:12,820 --> 00:11:17,800
for the remainder of the flow we have

00:11:15,399 --> 00:11:21,640
the processor in the middle and then the

00:11:17,800 --> 00:11:24,220
observer at the end here what Kafka

00:11:21,640 --> 00:11:27,579
tries to do is introduce a transaction

00:11:24,220 --> 00:11:29,589
between them in the sense that only once

00:11:27,579 --> 00:11:32,079
the transaction is completed the

00:11:29,589 --> 00:11:35,440
observer will actually witness the

00:11:32,079 --> 00:11:37,050
messages and start consuming them so

00:11:35,440 --> 00:11:40,750
let's see how does that work

00:11:37,050 --> 00:11:43,510
so you introduce an offset in a similar

00:11:40,750 --> 00:11:45,880
fashion to the idempotent producer for

00:11:43,510 --> 00:11:48,640
each message and also you have markers

00:11:45,880 --> 00:11:52,570
to when the transaction begins ends and

00:11:48,640 --> 00:11:54,430
in the middle etcetera so what does the

00:11:52,570 --> 00:11:56,399
processor need to do we'll start by

00:11:54,430 --> 00:11:59,470
polling the consumer reading the message

00:11:56,399 --> 00:12:02,260
we begin a transaction we're marking the

00:11:59,470 --> 00:12:05,339
transaction has and has begun we're

00:12:02,260 --> 00:12:09,190
sending the message over to topic B and

00:12:05,339 --> 00:12:12,310
then we send the offset as well so we

00:12:09,190 --> 00:12:15,250
know that it's not a duplicate and also

00:12:12,310 --> 00:12:17,529
importantly we commit the transaction so

00:12:15,250 --> 00:12:21,279
only now the observer can actually read

00:12:17,529 --> 00:12:24,670
message C now let's say what we had some

00:12:21,279 --> 00:12:29,079
issue before and maybe the messages were

00:12:24,670 --> 00:12:31,110
sent but the but the offsets were failed

00:12:29,079 --> 00:12:33,910
it was a failure sending the offsets

00:12:31,110 --> 00:12:37,899
okay and maybe the processor needed to

00:12:33,910 --> 00:12:40,899
restart you know it happens so in this

00:12:37,899 --> 00:12:43,449
situation the processor will wake up and

00:12:40,899 --> 00:12:45,699
understand that there was a transaction

00:12:43,449 --> 00:12:48,550
going on in the middle but it didn't

00:12:45,699 --> 00:12:51,130
finish okay so you can just cancel it

00:12:48,550 --> 00:12:52,990
and start all over again and all this

00:12:51,130 --> 00:12:56,040
time the observer haven't yet seen

00:12:52,990 --> 00:13:00,310
message C and we're good

00:12:56,040 --> 00:13:04,149
that was a very high-level explanation

00:13:00,310 --> 00:13:06,190
of how this works a very important note

00:13:04,149 --> 00:13:08,079
here to make is that there is

00:13:06,190 --> 00:13:10,510
performance overhead for using exactly

00:13:08,079 --> 00:13:11,800
once in Kafka you have more management

00:13:10,510 --> 00:13:14,589
and accounting to do for these

00:13:11,800 --> 00:13:17,440
transactions so you may decide to

00:13:14,589 --> 00:13:20,589
amortize this cost and add more messages

00:13:17,440 --> 00:13:23,529
to each one transaction once you do that

00:13:20,589 --> 00:13:26,050
you may end up with a problem because

00:13:23,529 --> 00:13:28,110
you can remember that only once all the

00:13:26,050 --> 00:13:30,519
messages in the transaction arrive

00:13:28,110 --> 00:13:33,639
successfully can you start reading them

00:13:30,519 --> 00:13:37,059
so you may introduce a latency here so

00:13:33,639 --> 00:13:38,649
as a user of this ability with Kafka you

00:13:37,059 --> 00:13:42,819
have to fine-tune the amount of messages

00:13:38,649 --> 00:13:44,470
you put into each transaction and it's

00:13:42,819 --> 00:13:46,809
important to also note that there is no

00:13:44,470 --> 00:13:49,420
end-to-end guarantee for exactly once

00:13:46,809 --> 00:13:51,519
delivery here right we have the HTTP

00:13:49,420 --> 00:13:53,829
endpoint Kafka doesn't know about that

00:13:51,519 --> 00:13:55,930
maybe there'll be some duplicate

00:13:53,829 --> 00:13:57,519
requests here you need to handle that on

00:13:55,930 --> 00:13:59,829
the application level Kafka won't help

00:13:57,519 --> 00:14:02,110
you there and also with the side effect

00:13:59,829 --> 00:14:04,420
at the end of this process you want to

00:14:02,110 --> 00:14:07,269
write a database and you need to make

00:14:04,420 --> 00:14:09,189
sure the updates are idempotent with

00:14:07,269 --> 00:14:11,829
yourself in a database with some locking

00:14:09,189 --> 00:14:13,540
mechanism because Kafka doesn't know

00:14:11,829 --> 00:14:18,160
about that it can guarantee anything

00:14:13,540 --> 00:14:20,079
with the DB failures and in reality all

00:14:18,160 --> 00:14:22,269
the this that I've explained is much

00:14:20,079 --> 00:14:25,000
more complex of course than a few slides

00:14:22,269 --> 00:14:26,889
there's a short time we have there's

00:14:25,000 --> 00:14:30,189
two-phase commit the transaction

00:14:26,889 --> 00:14:32,079
coordinator and there's a transaction

00:14:30,189 --> 00:14:34,029
log that keeps track of where we are

00:14:32,079 --> 00:14:36,639
with the transaction so if the service

00:14:34,029 --> 00:14:39,040
restarts it knows if it maybe need to

00:14:36,639 --> 00:14:41,800
abort the transaction and also there's

00:14:39,040 --> 00:14:44,500
an interesting case where the service

00:14:41,800 --> 00:14:46,660
instance suddenly gets stuck so we

00:14:44,500 --> 00:14:49,600
decide to spin up a new service instance

00:14:46,660 --> 00:14:52,209
and then we have some zombie State with

00:14:49,600 --> 00:14:54,579
the original service that it decides to

00:14:52,209 --> 00:14:57,370
wake up and we don't want to have double

00:14:54,579 --> 00:14:58,809
processing of the messages right so

00:14:57,370 --> 00:15:01,689
Kafka handles that as well and

00:14:58,809 --> 00:15:04,620
introduces epochs and transactional IDs

00:15:01,689 --> 00:15:06,970
you know to fence against these zombies

00:15:04,620 --> 00:15:09,970
so a lot of things are going on behind

00:15:06,970 --> 00:15:12,250
the scenes one caveat I want to mention

00:15:09,970 --> 00:15:16,019
here is that in terms of resource

00:15:12,250 --> 00:15:18,790
utilization there's some issue here with

00:15:16,019 --> 00:15:20,649
you want to of course work with

00:15:18,790 --> 00:15:24,610
transaction as many partitions as you

00:15:20,649 --> 00:15:27,009
can but with exactly once you will need

00:15:24,610 --> 00:15:31,120
to have one producer for each of these

00:15:27,009 --> 00:15:33,220
partitions so the scaling is not that

00:15:31,120 --> 00:15:34,209
great with exact ones at the moment but

00:15:33,220 --> 00:15:36,129
confluent the

00:15:34,209 --> 00:15:37,990
Berni behind Casca are working on

00:15:36,129 --> 00:15:42,610
solving this and hopefully will be

00:15:37,990 --> 00:15:45,220
pretty soon available okay so let's look

00:15:42,610 --> 00:15:48,279
back at the e-commerce flow we saw in

00:15:45,220 --> 00:15:52,720
the beginning now we can use the Kafka

00:15:48,279 --> 00:15:55,269
exactly once semantics and just put all

00:15:52,720 --> 00:15:57,610
of these inventory updates in the same

00:15:55,269 --> 00:15:59,740
Kafka transaction and it doesn't matter

00:15:57,610 --> 00:16:02,319
if you put these messages in different

00:15:59,740 --> 00:16:04,059
partitions or different topics even all

00:16:02,319 --> 00:16:07,509
of them are part of the same transaction

00:16:04,059 --> 00:16:10,059
and then the service at the end will

00:16:07,509 --> 00:16:12,730
only start handling this transaction

00:16:10,059 --> 00:16:14,769
once all of these requests and I come to

00:16:12,730 --> 00:16:18,279
it but of course you will still need to

00:16:14,769 --> 00:16:21,449
do a deduplication on the DB side of

00:16:18,279 --> 00:16:24,759
things because that's out of Kafka scope

00:16:21,449 --> 00:16:27,429
so to summarize exactly once delivery is

00:16:24,759 --> 00:16:29,819
like the holy grail of message delivery

00:16:27,429 --> 00:16:33,759
over the network so tough nut to crack

00:16:29,819 --> 00:16:36,399
it's complex to implement and it's a bit

00:16:33,759 --> 00:16:37,929
complex to use like I showed you and it

00:16:36,399 --> 00:16:41,290
requires fine tuning to get the

00:16:37,929 --> 00:16:43,720
performance right and crucial for

00:16:41,290 --> 00:16:45,759
achieving atomic actions and distributes

00:16:43,720 --> 00:16:48,069
assist systems on the infrastructure

00:16:45,759 --> 00:16:50,319
level and not let the application

00:16:48,069 --> 00:16:52,509
developers get bogged down in all of

00:16:50,319 --> 00:16:55,389
these details with the messages passing

00:16:52,509 --> 00:16:58,209
through your system so thank you very

00:16:55,389 --> 00:17:00,790
much if you want to check out more

00:16:58,209 --> 00:17:02,920
detail the way guys implement exactly

00:17:00,790 --> 00:17:04,860
once you have all these links and

00:17:02,920 --> 00:17:08,169
resources and I put them out on

00:17:04,860 --> 00:17:10,270
SlideShare with this link so you can

00:17:08,169 --> 00:17:12,760
check out all these resources you can

00:17:10,270 --> 00:17:15,970
also follow me on medium and Twitter to

00:17:12,760 --> 00:17:18,339
know all about what fix Wix is doing

00:17:15,970 --> 00:17:20,079
with the Kafka and data streaming and

00:17:18,339 --> 00:17:23,079
the cool libraries and tools you want to

00:17:20,079 --> 00:17:24,309
open-source thank you very much and if

00:17:23,079 --> 00:17:26,529
you have more questions you can see me

00:17:24,309 --> 00:17:29,459
later and let's move on to the great

00:17:26,529 --> 00:17:29,459
next speaker

00:17:29,740 --> 00:17:37,680
[Applause]

00:17:32,540 --> 00:17:37,680

YouTube URL: https://www.youtube.com/watch?v=7O_UC_i1XY0


