Title: I want a GPU - Regev Golan
Publication date: 2020-01-12
Playlist: DevOpsDays Tel Aviv 2019
Description: 
	If you have a data science team working with you, and havenâ€™t yet required to provide them a GPU (Graphical Processing Unit) production cluster, surely it's going to happen soon and you want to know how to respond and what will it take from you.
In this talk I'll make an intro to GPU basing on my experience building a production auto-scalable cluster of GPUs for inference of a deep learning model.
We'll understand what is a GPU, why is that important, what does it mean to scale, monitor and develop using this exciting infra.
Captions: 
	00:00:00,440 --> 00:00:03,580
[Music]

00:00:05,029 --> 00:00:11,429
okay hi everyone great to be here in

00:00:08,940 --> 00:00:14,280
this awesome conference and say thank

00:00:11,429 --> 00:00:16,800
you for attending this talk about a GPU

00:00:14,280 --> 00:00:21,000
I promise it will going to be

00:00:16,800 --> 00:00:23,430
interesting and practical so let's get

00:00:21,000 --> 00:00:26,820
started the nice to meet you my name is

00:00:23,430 --> 00:00:30,840
Raghav grande and I work at Howard Koh

00:00:26,820 --> 00:00:33,210
for the past three years at house Co we

00:00:30,840 --> 00:00:36,090
are using artificial intelligence to

00:00:33,210 --> 00:00:38,840
make hiring efficient and fell for the

00:00:36,090 --> 00:00:38,840
fortune 500

00:00:39,500 --> 00:00:44,280
previously to house Co I was at the IDF

00:00:42,329 --> 00:00:47,430
in the detergent scopes for six years as

00:00:44,280 --> 00:00:50,219
a big data engineer and I also has a

00:00:47,430 --> 00:00:54,989
bachelor's degree and now a master's for

00:00:50,219 --> 00:00:58,079
computer science we're here to talk

00:00:54,989 --> 00:01:01,739
about the growing computing needs needed

00:00:58,079 --> 00:01:03,840
for a a I so the world is seeing a

00:01:01,739 --> 00:01:05,909
artificial intelligence deep learning

00:01:03,840 --> 00:01:08,130
machine learning everywhere there are so

00:01:05,909 --> 00:01:15,240
many startups and industries that rely

00:01:08,130 --> 00:01:19,159
on this exciting technology this is just

00:01:15,240 --> 00:01:23,850
a picture of autonomous driving em and

00:01:19,159 --> 00:01:27,210
the thing about AI is that you hear a

00:01:23,850 --> 00:01:30,930
lot about GPU graphical processing unit

00:01:27,210 --> 00:01:33,810
this is actually in that pictures the

00:01:30,930 --> 00:01:38,610
Google's version of of a GPU the tons of

00:01:33,810 --> 00:01:40,500
processing unit and M this specific

00:01:38,610 --> 00:01:42,750
hardware is designed to make at

00:01:40,500 --> 00:01:48,049
efficient turn intelligence one runs

00:01:42,750 --> 00:01:51,090
faster it it uses a parallel computing

00:01:48,049 --> 00:01:54,409
capabilities which are thousands times

00:01:51,090 --> 00:01:59,189
more efficient than CPU for parallel

00:01:54,409 --> 00:02:02,670
processing the thing is you can't simply

00:01:59,189 --> 00:02:06,509
ask for a GPU choose in your cloud

00:02:02,670 --> 00:02:10,500
provider GPU instance and think it will

00:02:06,509 --> 00:02:11,090
all be okay and this is the story I am

00:02:10,500 --> 00:02:15,440
here

00:02:11,090 --> 00:02:18,140
talk about the high school justice so at

00:02:15,440 --> 00:02:20,540
high school with some of the stuff we do

00:02:18,140 --> 00:02:23,690
is natural language processing we

00:02:20,540 --> 00:02:25,880
basically have a document with lot of

00:02:23,690 --> 00:02:28,660
text in it and we need to extract

00:02:25,880 --> 00:02:31,060
entities and insights from the text to

00:02:28,660 --> 00:02:35,120
understand their words and some

00:02:31,060 --> 00:02:38,630
sentences and then to do all kinds of

00:02:35,120 --> 00:02:41,959
queries and the processing on top of

00:02:38,630 --> 00:02:46,160
that so it used to take us 20 seconds

00:02:41,959 --> 00:02:48,890
and before we change to work with GPU

00:02:46,160 --> 00:02:52,910
and again I'm not from Nvidia or I don't

00:02:48,890 --> 00:02:56,540
have any interest in in that just to

00:02:52,910 --> 00:02:59,180
share a how we solved a technological

00:02:56,540 --> 00:03:02,870
challenge in our data science group and

00:02:59,180 --> 00:03:07,269
for me as a DevOps for that project so

00:03:02,870 --> 00:03:12,500
we knew that natural language processing

00:03:07,269 --> 00:03:16,010
might be better with using GPUs so we

00:03:12,500 --> 00:03:18,890
started our journey in that path for the

00:03:16,010 --> 00:03:22,540
goal of lowering from 20 seconds per dog

00:03:18,890 --> 00:03:25,549
to much less to be able to scale and and

00:03:22,540 --> 00:03:28,730
previously to that we had many instances

00:03:25,549 --> 00:03:31,700
of that self is also locally on on a lot

00:03:28,730 --> 00:03:36,350
of server and wanted to never done to

00:03:31,700 --> 00:03:38,239
have this capability of dog dog human

00:03:36,350 --> 00:03:40,700
processing as a service and to be able

00:03:38,239 --> 00:03:44,720
to scale the organization and sales

00:03:40,700 --> 00:03:48,079
support the sales so as a DevOps

00:03:44,720 --> 00:03:53,060
engineer you might think this is the

00:03:48,079 --> 00:03:55,670
Amazon a instance a page so you go ahead

00:03:53,060 --> 00:03:58,010
and you want to choose the GPU instance

00:03:55,670 --> 00:04:00,319
day and you think this will be over with

00:03:58,010 --> 00:04:03,650
that suddenly you see this column over

00:04:00,319 --> 00:04:07,639
the GPU memory so wait what does it mean

00:04:03,650 --> 00:04:12,049
another memory I have my CPU my ram what

00:04:07,639 --> 00:04:15,319
will 16 gigs be enough and then it gets

00:04:12,049 --> 00:04:17,299
even more complicated when you see that

00:04:15,319 --> 00:04:18,769
there are machines with multiple GPUs

00:04:17,299 --> 00:04:21,530
inside of them

00:04:18,769 --> 00:04:24,090
so what should I choose for machines of

00:04:21,530 --> 00:04:27,990
one GPU one machine of

00:04:24,090 --> 00:04:29,910
these are very expensive cell very

00:04:27,990 --> 00:04:34,050
expensive cell service

00:04:29,910 --> 00:04:37,950
it usually cost 10 times more as a

00:04:34,050 --> 00:04:40,860
normal easy to instance so a this is the

00:04:37,950 --> 00:04:45,090
part when you need to understand the GPU

00:04:40,860 --> 00:04:47,340
and using this infrastructure and it it

00:04:45,090 --> 00:04:49,139
means changes in the entire DevOps like

00:04:47,340 --> 00:04:51,840
a cycle you need to develop differently

00:04:49,139 --> 00:04:54,630
to operate it differently to monitor to

00:04:51,840 --> 00:04:57,990
solicit and this is what I am here to

00:04:54,630 --> 00:05:00,960
talk about today and the world stories

00:04:57,990 --> 00:05:03,990
we experienced when were working with

00:05:00,960 --> 00:05:09,540
this infrastructure so let's just go

00:05:03,990 --> 00:05:12,330
ahead and start talking about em I I

00:05:09,540 --> 00:05:15,360
want to immediately share our stack and

00:05:12,330 --> 00:05:19,050
what we have what we are using

00:05:15,360 --> 00:05:21,120
so using GPUs minimized the response

00:05:19,050 --> 00:05:23,250
time the processing time for one

00:05:21,120 --> 00:05:26,729
document from twenty seconds to one

00:05:23,250 --> 00:05:32,600
second it's it's a normal improvement

00:05:26,729 --> 00:05:36,780
when using this power and we are using a

00:05:32,600 --> 00:05:40,500
PI touch which is the Python library for

00:05:36,780 --> 00:05:43,320
working with GPU we are using a model

00:05:40,500 --> 00:05:46,080
called built from Google it's an open

00:05:43,320 --> 00:05:48,630
source and the reason I mention that is

00:05:46,080 --> 00:05:53,280
because belt is a very fat model it

00:05:48,630 --> 00:05:56,340
supports 110 languages and have a 170

00:05:53,280 --> 00:05:59,760
million parameters it's a very heavy and

00:05:56,340 --> 00:06:02,910
and compute intensive but mother it

00:05:59,760 --> 00:06:05,070
assumes you will walk with GPU it it

00:06:02,910 --> 00:06:08,880
doesn't make sense a lot to work with

00:06:05,070 --> 00:06:11,820
this size of model without a chip GPU to

00:06:08,880 --> 00:06:14,430
deploy it in production we use we are in

00:06:11,820 --> 00:06:19,530
the Amazon Cloud and we use the Amazon

00:06:14,430 --> 00:06:24,389
Elastic container service what is

00:06:19,530 --> 00:06:26,699
important to know and I highly recommend

00:06:24,389 --> 00:06:28,650
to to read online there are M

00:06:26,699 --> 00:06:30,870
there are a lot of good information

00:06:28,650 --> 00:06:36,389
about understanding the internals of the

00:06:30,870 --> 00:06:39,090
GPU gpo's has basically they are

00:06:36,389 --> 00:06:42,409
optimized for many parallel tasks a very

00:06:39,090 --> 00:06:46,919
simple parallel task like a summary and

00:06:42,409 --> 00:06:50,219
multiplication of the vectors and they

00:06:46,919 --> 00:06:55,080
they hi they are highly relevant for

00:06:50,219 --> 00:06:58,250
deep neural networks because you need to

00:06:55,080 --> 00:07:02,270
do these actions in parallel or on a

00:06:58,250 --> 00:07:06,000
large amount of vectors and features

00:07:02,270 --> 00:07:09,000
also one last point is the different

00:07:06,000 --> 00:07:12,509
types I am mentioning only the GPU the

00:07:09,000 --> 00:07:15,870
graphical processing unit but also there

00:07:12,509 --> 00:07:18,150
are a basic application specific

00:07:15,870 --> 00:07:22,199
integrated circuit a circuit these are

00:07:18,150 --> 00:07:26,580
like a Google snip tip you however

00:07:22,199 --> 00:07:29,849
designed for deep learning as opposed to

00:07:26,580 --> 00:07:34,050
GP which is taken from the graphical and

00:07:29,849 --> 00:07:37,610
this display a word and was customized

00:07:34,050 --> 00:07:42,889
for deep learning basically all the

00:07:37,610 --> 00:07:45,569
matrix multiplied with multiplications

00:07:42,889 --> 00:07:47,669
also there is the FPGA which is a

00:07:45,569 --> 00:07:52,440
relatively new it's also a very

00:07:47,669 --> 00:07:56,610
efficient hardware but it is not as

00:07:52,440 --> 00:08:00,300
flexible so when you work with deep

00:07:56,610 --> 00:08:02,039
learning they I would say the main usage

00:08:00,300 --> 00:08:07,680
is GPU but there are also these other

00:08:02,039 --> 00:08:10,199
types I am using only the GPU as a just

00:08:07,680 --> 00:08:15,029
as the topic but there are other

00:08:10,199 --> 00:08:20,339
possibilities I like to talk about

00:08:15,029 --> 00:08:24,479
history when I examining technology so a

00:08:20,339 --> 00:08:28,439
GPUs are not a just a bus or a over

00:08:24,479 --> 00:08:31,150
trend they are here and a being used in

00:08:28,439 --> 00:08:33,880
deep learning for the past 10 years

00:08:31,150 --> 00:08:38,230
since 2009 when two researchers from

00:08:33,880 --> 00:08:45,520
Stanford approved that deep learning can

00:08:38,230 --> 00:08:48,130
be highly accelerated using GPUs in 2007

00:08:45,520 --> 00:08:51,400
two years before that Nvidia released

00:08:48,130 --> 00:08:54,640
CUDA which was the first a programming

00:08:51,400 --> 00:08:59,500
model for GPU computing since then there

00:08:54,640 --> 00:09:04,420
are many others but CUDA is very widely

00:08:59,500 --> 00:09:07,840
used and known so we are now starting to

00:09:04,420 --> 00:09:09,790
talk about the the world's top stories

00:09:07,840 --> 00:09:11,560
as as I call them and let's start with

00:09:09,790 --> 00:09:14,680
some code where engineers would like to

00:09:11,560 --> 00:09:19,750
see code so this is the hello world form

00:09:14,680 --> 00:09:22,090
a PI thought to use a GPU and what I

00:09:19,750 --> 00:09:24,760
wanted to emphasize here that using this

00:09:22,090 --> 00:09:28,960
specific framework at the programmer

00:09:24,760 --> 00:09:32,110
must be a well to the fact that there is

00:09:28,960 --> 00:09:35,080
a GPU underneath it it reminds us like

00:09:32,110 --> 00:09:39,520
the time well you should have thought

00:09:35,080 --> 00:09:42,820
about whether my CP with 32 bits of 664

00:09:39,520 --> 00:09:45,840
you actually need to load the data this

00:09:42,820 --> 00:09:49,090
simple example you take a vector of X

00:09:45,840 --> 00:09:52,020
numbers and you want to add once to all

00:09:49,090 --> 00:09:55,720
of the vector so you actually need to a

00:09:52,020 --> 00:09:59,770
torch dot ones like and load it to the

00:09:55,720 --> 00:10:02,800
device of the GPU only then you are

00:09:59,770 --> 00:10:07,480
loading also another vector and only

00:10:02,800 --> 00:10:10,450
after all the data is was loaded to form

00:10:07,480 --> 00:10:16,150
the ramp to the GPUs memory and you pay

00:10:10,450 --> 00:10:18,100
for this new movie and H shift then when

00:10:16,150 --> 00:10:22,990
you summarize the two vectors you will

00:10:18,100 --> 00:10:26,910
get the improvement and you will see the

00:10:22,990 --> 00:10:26,910
result in a very fast a time

00:10:27,440 --> 00:10:34,829
memo is a big deal with walking with

00:10:30,180 --> 00:10:37,500
dipping GPUs we saw arrows that you tend

00:10:34,829 --> 00:10:41,610
to forget about them a segmentation

00:10:37,500 --> 00:10:44,519
fault and out of memory this out to

00:10:41,610 --> 00:10:48,930
example of issues the right one there is

00:10:44,519 --> 00:10:51,540
a deep dive calculation about how many

00:10:48,930 --> 00:10:54,050
bytes will I use I will need to take the

00:10:51,540 --> 00:10:58,649
model and the size of the input and how

00:10:54,050 --> 00:11:02,040
will I optimize it and we spent a lot of

00:10:58,649 --> 00:11:06,779
time on this area if you are just now

00:11:02,040 --> 00:11:10,670
starting what start work with GPUs be

00:11:06,779 --> 00:11:12,500
sure to spare some time for this step

00:11:10,670 --> 00:11:17,670
[Applause]

00:11:12,500 --> 00:11:21,180
the memory management of GPU is not as

00:11:17,670 --> 00:11:25,470
convenient as the RAM you don't have the

00:11:21,180 --> 00:11:27,899
swap space that can help if the memory

00:11:25,470 --> 00:11:29,790
gets too big you don't have such a thing

00:11:27,899 --> 00:11:32,880
in GPU it it doesn't make sense because

00:11:29,790 --> 00:11:36,980
the GPU needs its own memory to be able

00:11:32,880 --> 00:11:42,240
to send it to the processing cost and

00:11:36,980 --> 00:11:43,829
you you will need to be patient and have

00:11:42,240 --> 00:11:46,550
some time in the bargain both in

00:11:43,829 --> 00:11:53,009
production and in the research for this

00:11:46,550 --> 00:11:55,949
wall the second thing we stumble with we

00:11:53,009 --> 00:11:59,120
work with big models if you will now try

00:11:55,949 --> 00:12:02,550
to Google about that you have like 5

00:11:59,120 --> 00:12:05,910
gigabytes of aim of a model that you

00:12:02,550 --> 00:12:08,610
have computed you now want to run the

00:12:05,910 --> 00:12:11,699
inference the processing of the input of

00:12:08,610 --> 00:12:14,189
it in production so a docker and

00:12:11,699 --> 00:12:17,399
containers are not that optimized to

00:12:14,189 --> 00:12:20,029
work in in such a big size you want your

00:12:17,399 --> 00:12:24,360
a container to be as small as possible

00:12:20,029 --> 00:12:25,850
and the thing to understand here we used

00:12:24,360 --> 00:12:30,110
to have mesh

00:12:25,850 --> 00:12:32,470
and have on them a several containers we

00:12:30,110 --> 00:12:34,790
are not aware about how many container

00:12:32,470 --> 00:12:36,470
containers will be on that machine in

00:12:34,790 --> 00:12:39,230
GPUs it's different

00:12:36,470 --> 00:12:41,540
the your codes need to be aware about

00:12:39,230 --> 00:12:44,810
how much memory will it have any way to

00:12:41,540 --> 00:12:49,070
plan for it so for instance in the house

00:12:44,810 --> 00:12:52,180
caucus we have a cluster of easy tooth

00:12:49,070 --> 00:12:56,120
just the the minimum is two for highly

00:12:52,180 --> 00:13:00,260
available we chose one GPU per instance

00:12:56,120 --> 00:13:04,370
just to make things simpler we optimized

00:13:00,260 --> 00:13:08,330
our memory usage to to be able to cope

00:13:04,370 --> 00:13:11,690
with only that the 16 gigabytes of

00:13:08,330 --> 00:13:13,550
memory that one GPU machine has and we

00:13:11,690 --> 00:13:16,190
are running one container per instance

00:13:13,550 --> 00:13:20,170
this is an anti-pattern for containers

00:13:16,190 --> 00:13:26,330
again but when you need a production

00:13:20,170 --> 00:13:28,490
inference deeply learning a service this

00:13:26,330 --> 00:13:31,820
is what we chose it makes things simpler

00:13:28,490 --> 00:13:33,920
and it does the job it can scale to

00:13:31,820 --> 00:13:39,580
whatever the size and load we have

00:13:33,920 --> 00:13:43,160
thanks to a Amazon in the ICS and the

00:13:39,580 --> 00:13:45,980
model is packed inside that container if

00:13:43,160 --> 00:13:48,260
the containers becomes big we have

00:13:45,980 --> 00:13:51,130
containers of 8 gigabytes only the

00:13:48,260 --> 00:13:55,940
container itself but it's working and

00:13:51,130 --> 00:13:59,350
also when the service loads it sinks the

00:13:55,940 --> 00:14:06,980
part of the models that it doesn't have

00:13:59,350 --> 00:14:10,570
we talk about micro services and monnel

00:14:06,980 --> 00:14:13,940
it and the the real case is that our

00:14:10,570 --> 00:14:16,400
services are not so decoupled then one

00:14:13,940 --> 00:14:18,470
cell service using a lot of models and

00:14:16,400 --> 00:14:21,110
doing a lot of work and it's easier for

00:14:18,470 --> 00:14:23,420
the data science team to walk like like

00:14:21,110 --> 00:14:27,890
that so you must be able as devops to

00:14:23,420 --> 00:14:31,410
also support it and so this is how we

00:14:27,890 --> 00:14:34,660
are running the second it works great

00:14:31,410 --> 00:14:39,610
and but this was also an issue that

00:14:34,660 --> 00:14:43,360
needs to to have some sort in it I have

00:14:39,610 --> 00:14:46,689
two more points to talk about monitoring

00:14:43,360 --> 00:14:50,679
so this is the basic tool that our data

00:14:46,689 --> 00:14:55,019
science used when creating the models

00:14:50,679 --> 00:14:59,050
they simply walk locally on the servers

00:14:55,019 --> 00:15:01,269
with the nvidia SMI you see here in the

00:14:59,050 --> 00:15:04,660
example that all four GPUs of that

00:15:01,269 --> 00:15:07,809
machine are utilized and what are the

00:15:04,660 --> 00:15:09,999
processes that uses the memory this is

00:15:07,809 --> 00:15:12,339
the default see light tool for me in a

00:15:09,999 --> 00:15:13,749
video and also if you are running in

00:15:12,339 --> 00:15:16,449
production you probably have some kind

00:15:13,749 --> 00:15:19,990
of Prometheus so you can install the

00:15:16,449 --> 00:15:23,589
exporter for to to get those map metrics

00:15:19,990 --> 00:15:26,259
and in terms of what to monitor so there

00:15:23,589 --> 00:15:28,720
are a lot of metrics to look at GPU

00:15:26,259 --> 00:15:32,189
temperature for instance I found the

00:15:28,720 --> 00:15:35,529
most useful is simply the GPU and memory

00:15:32,189 --> 00:15:39,759
utilization the top ten operations and

00:15:35,529 --> 00:15:42,730
the service metrics by dogs you can also

00:15:39,759 --> 00:15:46,949
scale your cluster and make sure you

00:15:42,730 --> 00:15:50,369
hold the stress you put against it I'm

00:15:46,949 --> 00:15:53,610
talking about the cloud providers so I

00:15:50,369 --> 00:15:56,709
thought that google has a lot of

00:15:53,610 --> 00:15:59,439
provisioning and profiling tools for GPU

00:15:56,709 --> 00:16:05,199
I feel that they are more advanced in

00:15:59,439 --> 00:16:09,329
that town and the Amazon has just

00:16:05,199 --> 00:16:12,009
recently announced on their own like TPU

00:16:09,329 --> 00:16:15,910
instance type which are optimized for

00:16:12,009 --> 00:16:18,009
the player learning day in fun so whilst

00:16:15,910 --> 00:16:20,679
checking out if you have the option to

00:16:18,009 --> 00:16:25,559
choose well worth checking out both this

00:16:20,679 --> 00:16:28,509
is my advice so just to one final point

00:16:25,559 --> 00:16:32,110
when you have your application it will

00:16:28,509 --> 00:16:33,790
work both on CPU and GPU you want to

00:16:32,110 --> 00:16:36,600
take advantage of your money GPU

00:16:33,790 --> 00:16:38,770
machines has also cause of CPU and

00:16:36,600 --> 00:16:41,860
memory in them so

00:16:38,770 --> 00:16:45,970
you will need to take care of it of that

00:16:41,860 --> 00:16:48,310
all and also to be able to take out

00:16:45,970 --> 00:16:51,959
parts and place them on their own

00:16:48,310 --> 00:16:55,690
instances one of the topics that we are

00:16:51,959 --> 00:17:01,690
also examining is having like a GPU

00:16:55,690 --> 00:17:05,559
cloud and making a lot of services use a

00:17:01,690 --> 00:17:08,130
simple GPU deep learning inference and

00:17:05,559 --> 00:17:12,130
then on top of that doing their own

00:17:08,130 --> 00:17:14,079
calculations so this day I don't have

00:17:12,130 --> 00:17:18,250
enough time to elaborate on that

00:17:14,079 --> 00:17:20,770
but there are also many steps you can do

00:17:18,250 --> 00:17:23,170
to improve your efficiency

00:17:20,770 --> 00:17:26,610
so to summarize with we've talked about

00:17:23,170 --> 00:17:31,630
those for short a wall stall stories a

00:17:26,610 --> 00:17:35,440
and the main point that I want to infer

00:17:31,630 --> 00:17:37,720
that it literally one more exciting

00:17:35,440 --> 00:17:40,510
world in the area of DevOps when we feel

00:17:37,720 --> 00:17:43,120
that we are empowering day I it has a

00:17:40,510 --> 00:17:48,790
lot of impact to know and understand at

00:17:43,120 --> 00:17:51,420
this unique power that's it I hope you

00:17:48,790 --> 00:17:56,460
enjoyed and have a beautiful day

00:17:51,420 --> 00:18:01,599
[Applause]

00:17:56,460 --> 00:18:01,599

YouTube URL: https://www.youtube.com/watch?v=GcDB7iurNHI


