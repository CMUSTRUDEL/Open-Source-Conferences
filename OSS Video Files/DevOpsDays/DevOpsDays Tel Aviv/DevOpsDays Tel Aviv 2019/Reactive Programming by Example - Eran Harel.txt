Title: Reactive Programming by Example - Eran Harel
Publication date: 2020-01-12
Playlist: DevOpsDays Tel Aviv 2019
Description: 
	The reactive manifesto is meant to guide you in building Responsive, Resilient, Elastic (scalable), and Message Driven systems.

But these are all bombastic words which are quite meaningless without a good context or good examples.

This talk will walk you through a story of improving a real life service, bringing it to perform well, and link the steps to the reactive manifesto cornerstones.
Captions: 
	00:00:00,030 --> 00:00:11,150
oh hi everybody my name is Annabelle and

00:00:08,010 --> 00:00:14,610
I am up slice platform group architect

00:00:11,150 --> 00:00:16,440
this is my twitter handle so feel free

00:00:14,610 --> 00:00:19,800
to get in contact if you have any

00:00:16,440 --> 00:00:21,840
questions after this talk I've been I've

00:00:19,800 --> 00:00:24,960
been writing software professionally for

00:00:21,840 --> 00:00:28,050
about 19 years now and I always loved

00:00:24,960 --> 00:00:32,759
solving scale issues and over the years

00:00:28,050 --> 00:00:35,640
I was wondered what's the important

00:00:32,759 --> 00:00:38,489
qualities of services that deal with

00:00:35,640 --> 00:00:42,300
high concurrency high throughput but

00:00:38,489 --> 00:00:44,789
still remain resilient and robust even

00:00:42,300 --> 00:00:46,890
when things start failing around them

00:00:44,789 --> 00:00:49,170
well put it in other words I was looking

00:00:46,890 --> 00:00:52,370
for the patterns that help you build

00:00:49,170 --> 00:00:55,789
such services and at some point I

00:00:52,370 --> 00:00:58,530
stumbled across the reactive manifesto

00:00:55,789 --> 00:01:03,600
so who is familiar with the reactive

00:00:58,530 --> 00:01:05,790
manifesto what a few nice so the

00:01:03,600 --> 00:01:10,229
reactive manifesto is a document that

00:01:05,790 --> 00:01:13,680
defines the core principles of reactive

00:01:10,229 --> 00:01:16,409
systems and it was released at 2013 and

00:01:13,680 --> 00:01:18,570
and the reason for publishing it was

00:01:16,409 --> 00:01:22,009
that application requirements have

00:01:18,570 --> 00:01:25,040
changed dramatically during these years

00:01:22,009 --> 00:01:27,540
runtime environment changed dramatically

00:01:25,040 --> 00:01:30,030
tighter so let's go introduce low

00:01:27,540 --> 00:01:32,310
latency is higher throughputs high

00:01:30,030 --> 00:01:35,520
availability linear scalability and

00:01:32,310 --> 00:01:38,280
quite a few buzzwords tools and

00:01:35,520 --> 00:01:41,509
techniques emerged in the industry at

00:01:38,280 --> 00:01:45,200
that time by various organizations and

00:01:41,509 --> 00:01:49,680
there was a need for a common vocabulary

00:01:45,200 --> 00:01:52,680
so the reactive manifesto describes the

00:01:49,680 --> 00:01:55,560
reactive what reactive applications are

00:01:52,680 --> 00:01:58,619
and defines them through four high level

00:01:55,560 --> 00:02:02,250
traits resilience responsiveness

00:01:58,619 --> 00:02:06,960
resilience elasticity and message driven

00:02:02,250 --> 00:02:09,959
architecture and I know it all sounds

00:02:06,960 --> 00:02:12,750
like the usual bunch of buzzwords so in

00:02:09,959 --> 00:02:13,590
this talk I'll try to explain using my

00:02:12,750 --> 00:02:16,830
own words

00:02:13,590 --> 00:02:20,250
what these concepts mean and why they're

00:02:16,830 --> 00:02:23,190
so important and I'll demonstrate how

00:02:20,250 --> 00:02:28,140
these concepts can be applied in real

00:02:23,190 --> 00:02:30,660
life so responsiveness is the

00:02:28,140 --> 00:02:33,900
cornerstone of usability responsiveness

00:02:30,660 --> 00:02:36,930
is what what we're trying to achieve

00:02:33,900 --> 00:02:40,680
here really and it means that the system

00:02:36,930 --> 00:02:43,410
responds at a timely manner if it's all

00:02:40,680 --> 00:02:47,930
possible and responsiveness responsive

00:02:43,410 --> 00:02:51,239
systems system that provide predictive

00:02:47,930 --> 00:02:54,269
predictable bounds and reasonably short

00:02:51,239 --> 00:02:56,940
response times and this consistent

00:02:54,269 --> 00:03:00,510
behavior simplifies the error detection

00:02:56,940 --> 00:03:03,569
and handling and in general responsive

00:03:00,510 --> 00:03:07,170
systems are what makes your users come

00:03:03,569 --> 00:03:10,950
back to your service right and in

00:03:07,170 --> 00:03:13,170
general responsive systems yeah so to

00:03:10,950 --> 00:03:16,319
become to become responsive a system

00:03:13,170 --> 00:03:20,510
needs to be both resilient and elastic

00:03:16,319 --> 00:03:24,030
in resilience means that the system

00:03:20,510 --> 00:03:27,150
stays responsive even at the face of

00:03:24,030 --> 00:03:30,450
failure and resilience is achieved by

00:03:27,150 --> 00:03:33,690
means of replication which means having

00:03:30,450 --> 00:03:37,200
several copies of your components

00:03:33,690 --> 00:03:38,850
basically by means of isolation which

00:03:37,200 --> 00:03:40,890
means the coupling between your

00:03:38,850 --> 00:03:43,620
components between your senders and

00:03:40,890 --> 00:03:47,010
receivers and using some sort of

00:03:43,620 --> 00:03:49,940
location transparency and by delegation

00:03:47,010 --> 00:03:53,700
which basically means letting other

00:03:49,940 --> 00:03:57,150
components handle tasks for us in an

00:03:53,700 --> 00:04:00,180
unsynchronized fashion and when we use

00:03:57,150 --> 00:04:03,480
these techniques you get systems that

00:04:00,180 --> 00:04:09,180
are easier to understand to extend the

00:04:03,480 --> 00:04:12,389
test and to evolve and elasticity means

00:04:09,180 --> 00:04:14,760
that the system stays responsive under

00:04:12,389 --> 00:04:17,760
vying workloads meaning the system

00:04:14,760 --> 00:04:20,639
scales up and down as the load on a

00:04:17,760 --> 00:04:23,250
system changes in order to meet the

00:04:20,639 --> 00:04:27,060
required throughput which in turn means

00:04:23,250 --> 00:04:29,639
you cannot have any contention points

00:04:27,060 --> 00:04:34,800
or centralized bottlenecks in your

00:04:29,639 --> 00:04:37,290
system and the technique that we use in

00:04:34,800 --> 00:04:39,450
order to achieve resilience and

00:04:37,290 --> 00:04:42,510
elasticity is called message driven

00:04:39,450 --> 00:04:45,180
architecture it means that your system

00:04:42,510 --> 00:04:47,910
relies on a synchronous message passing

00:04:45,180 --> 00:04:50,540
messages are sent to location

00:04:47,910 --> 00:04:53,310
transparent recipient some other

00:04:50,540 --> 00:04:56,639
recipients are basically handlers which

00:04:53,310 --> 00:05:00,560
either react to incoming messages or

00:04:56,639 --> 00:05:03,380
remain idle meaning we only consume

00:05:00,560 --> 00:05:06,690
resources when we are actually active

00:05:03,380 --> 00:05:09,419
and this order design is what enables us

00:05:06,690 --> 00:05:11,850
to establish clear boundaries between

00:05:09,419 --> 00:05:14,310
the components and achieve loose

00:05:11,850 --> 00:05:17,700
coupling and location transparency we

00:05:14,310 --> 00:05:20,190
talked about earlier and it also allows

00:05:17,700 --> 00:05:23,340
us to apply to easily apply load

00:05:20,190 --> 00:05:26,490
balancing and flow of control or put it

00:05:23,340 --> 00:05:29,370
in other words it's a technique that we

00:05:26,490 --> 00:05:34,919
use in order to become both resilient

00:05:29,370 --> 00:05:37,470
and elastic now large systems are

00:05:34,919 --> 00:05:40,940
composed of smaller ones which means

00:05:37,470 --> 00:05:44,580
that in order to preserve the reactive

00:05:40,940 --> 00:05:47,850
qualities of our system we must apply

00:05:44,580 --> 00:05:52,380
these principles in all layers of our

00:05:47,850 --> 00:05:55,919
architecture everywhere alright so this

00:05:52,380 --> 00:05:58,289
is the end of the theoretical part and I

00:05:55,919 --> 00:05:58,919
know it still sounds like a bunch of

00:05:58,289 --> 00:06:02,130
buzzwords

00:05:58,919 --> 00:06:05,160
so I'd like to dive into a real-life use

00:06:02,130 --> 00:06:07,350
case and I will demonstrate how we can

00:06:05,160 --> 00:06:11,400
apply at least some of these concepts

00:06:07,350 --> 00:06:14,340
and before I begin I'd like to stress

00:06:11,400 --> 00:06:17,460
out that this case study demonstrates a

00:06:14,340 --> 00:06:20,930
solution to a specific requirements and

00:06:17,460 --> 00:06:24,000
you may have to apply other techniques

00:06:20,930 --> 00:06:29,100
for your own systems or put it in other

00:06:24,000 --> 00:06:31,889
words there is no silver bullet right so

00:06:29,100 --> 00:06:36,330
this is devops days and I bet you all of

00:06:31,889 --> 00:06:38,580
your metrics right so let's discuss the

00:06:36,330 --> 00:06:39,690
tale of scaling or matrix delivery

00:06:38,580 --> 00:06:42,270
system

00:06:39,690 --> 00:06:47,220
so at a fictional company I used to work

00:06:42,270 --> 00:06:49,770
for we use graphite to store matrix and

00:06:47,220 --> 00:06:52,920
the system looked roughly like this

00:06:49,770 --> 00:06:57,660
diagram and all of our services had a

00:06:52,920 --> 00:07:01,080
graphite reporter in them it's the blue

00:06:57,660 --> 00:07:04,560
powder which would send matrix directly

00:07:01,080 --> 00:07:07,910
to the graphite relay once per minute

00:07:04,560 --> 00:07:12,090
and this strategy held reasonably well

00:07:07,910 --> 00:07:15,210
until we reach around 500,000 matrix per

00:07:12,090 --> 00:07:17,250
minute per minute and at this point the

00:07:15,210 --> 00:07:20,760
graphite really started dropping matrix

00:07:17,250 --> 00:07:22,320
mainly due to its inability to handle

00:07:20,760 --> 00:07:26,760
the high rate of IO

00:07:22,320 --> 00:07:29,760
interrupts that was like five six seven

00:07:26,760 --> 00:07:32,940
years ago I don't remember and someone

00:07:29,760 --> 00:07:35,340
quickly implemented a solution we

00:07:32,940 --> 00:07:36,030
introduced RabbitMQ and log stash into

00:07:35,340 --> 00:07:39,630
the system

00:07:36,030 --> 00:07:41,970
and the metric reporter would then write

00:07:39,630 --> 00:07:45,060
to the log stash agent running on

00:07:41,970 --> 00:07:47,370
localhost logs logs - published the

00:07:45,060 --> 00:07:49,020
metrics to Robert him kill and on the

00:07:47,370 --> 00:07:51,060
other end log start consumed their

00:07:49,020 --> 00:07:56,070
metrics and publish them to graphite and

00:07:51,060 --> 00:07:58,440
guess what log stash may walk okay in

00:07:56,070 --> 00:08:02,030
pull mode but it simply doesn't work

00:07:58,440 --> 00:08:07,320
that well in push mode and it crushed it

00:08:02,030 --> 00:08:12,360
ridiculously low request weights so kids

00:08:07,320 --> 00:08:15,270
don't do this at home seriously alright

00:08:12,360 --> 00:08:17,550
so at this point someone I play I played

00:08:15,270 --> 00:08:19,830
with the idea of replacing that local

00:08:17,550 --> 00:08:22,830
log stash agent with a service tower

00:08:19,830 --> 00:08:25,080
wrote myself and the problem was that

00:08:22,830 --> 00:08:27,660
the log stash consumer but we still

00:08:25,080 --> 00:08:30,960
hurting a system was way too slow and it

00:08:27,660 --> 00:08:33,270
caused RabbitMQ to hang you to the queue

00:08:30,960 --> 00:08:36,240
becoming too long which stole the matrix

00:08:33,270 --> 00:08:39,200
publisher bringing the entire matrix

00:08:36,240 --> 00:08:43,979
delivery system to a halt so sad

00:08:39,200 --> 00:08:46,080
and before you say I do know that Kafka

00:08:43,979 --> 00:08:48,750
can solve some of these issues and we

00:08:46,080 --> 00:08:51,540
have a customer committed here in the

00:08:48,750 --> 00:08:54,900
crowd I'm going to be very cautious

00:08:51,540 --> 00:08:57,660
the problem with Kafka is that when you

00:08:54,900 --> 00:09:00,300
deal with metrics is that freshness is

00:08:57,660 --> 00:09:03,360
actually more important than durability

00:09:00,300 --> 00:09:07,170
and Kafka is tuned for throughput mainly

00:09:03,360 --> 00:09:09,300
not for low latency loss Kafka Kafka

00:09:07,170 --> 00:09:11,880
based solutions tend to be more

00:09:09,300 --> 00:09:16,830
expensive at least in terms of hardware

00:09:11,880 --> 00:09:19,860
networking and storage so instead of

00:09:16,830 --> 00:09:23,010
using a queueing system I realized that

00:09:19,860 --> 00:09:25,980
we what we need to do is to introduce a

00:09:23,010 --> 00:09:28,260
service that will sit on top of the

00:09:25,980 --> 00:09:31,830
graphite relays and protect them from

00:09:28,260 --> 00:09:34,140
the roaring crowd all trying to right

00:09:31,830 --> 00:09:37,110
masses of masses of metrics into the

00:09:34,140 --> 00:09:42,960
system and we call this service Gruffalo

00:09:37,110 --> 00:09:46,110
and this Russian skate world but the

00:09:42,960 --> 00:09:48,450
single cob one relay was still a

00:09:46,110 --> 00:09:51,990
bottleneck and he was sink still a

00:09:48,450 --> 00:09:54,510
single point of failure but luckily now

00:09:51,990 --> 00:09:56,480
with gar follow in place it's a lot

00:09:54,510 --> 00:10:01,710
easier to start implementing

00:09:56,480 --> 00:10:03,840
responsiveness features so in the next

00:10:01,710 --> 00:10:07,170
iteration we added several carbone

00:10:03,840 --> 00:10:09,660
relays we replicated them and gruffalo

00:10:07,170 --> 00:10:13,010
performed client-side load balancing on

00:10:09,660 --> 00:10:16,260
top of the relays now this greatly

00:10:13,010 --> 00:10:18,630
increase the carbon relays capacity and

00:10:16,260 --> 00:10:21,900
availability but we still discovered

00:10:18,630 --> 00:10:24,240
more and more interesting issues we had

00:10:21,900 --> 00:10:28,620
to take curve as the volume of matrix

00:10:24,240 --> 00:10:31,590
grew over time right so before we go on

00:10:28,620 --> 00:10:34,290
let's review what this module called

00:10:31,590 --> 00:10:38,070
Gruffalo is the first of all it's open

00:10:34,290 --> 00:10:40,800
source and the main role of Gruffalo is

00:10:38,070 --> 00:10:42,990
to protect graphite and it does so by

00:10:40,800 --> 00:10:46,200
utilizing several strategies first

00:10:42,990 --> 00:10:47,990
strategy is batching this is quite

00:10:46,200 --> 00:10:50,550
similar to the cough cough strategy of

00:10:47,990 --> 00:10:54,990
batching messages in order to increase

00:10:50,550 --> 00:10:58,410
the throughput gruffalo is built on top

00:10:54,990 --> 00:11:01,350
of neti neti is a low level message

00:10:58,410 --> 00:11:03,300
driven networking library it allows us

00:11:01,350 --> 00:11:04,579
to write network services and protocols

00:11:03,300 --> 00:11:07,759
that handle mass

00:11:04,579 --> 00:11:11,689
massive throughput and very high

00:11:07,759 --> 00:11:14,839
concurrency levels and one other role of

00:11:11,689 --> 00:11:20,209
Gruffalo was to replicate metrics

00:11:14,839 --> 00:11:22,100
between clusters between regions so the

00:11:20,209 --> 00:11:24,439
deployment of The Gruffalo Service looks

00:11:22,100 --> 00:11:28,480
roughly like this the server decoupled

00:11:24,439 --> 00:11:31,879
in each region the the multiple clients

00:11:28,480 --> 00:11:34,009
writing the metrics to The Gruffalo

00:11:31,879 --> 00:11:36,350
Service once per minute there are

00:11:34,009 --> 00:11:40,480
several decoupled instances of Gruffalo

00:11:36,350 --> 00:11:44,959
each can receive those metrics Gruffalo

00:11:40,480 --> 00:11:47,689
batches the metrics and send batches to

00:11:44,959 --> 00:11:51,910
the Carbone relay and Gough loss or

00:11:47,689 --> 00:11:55,999
replicates between the remote regions

00:11:51,910 --> 00:11:59,360
now we have quite a few metric line

00:11:55,999 --> 00:12:02,929
types in our system but for the most

00:11:59,360 --> 00:12:06,470
part metric publishers send metrics to

00:12:02,929 --> 00:12:09,379
graphite once per minute and for each

00:12:06,470 --> 00:12:14,839
batch they open a new connection to

00:12:09,379 --> 00:12:17,470
graphite write their metrics to one by

00:12:14,839 --> 00:12:20,480
one to the to the relay and the flash

00:12:17,470 --> 00:12:21,889
immediately after each write and when

00:12:20,480 --> 00:12:25,549
they're done they close the connection

00:12:21,889 --> 00:12:28,040
so the graphite targets not a Gruffalo

00:12:25,549 --> 00:12:30,949
service in this context they don't have

00:12:28,040 --> 00:12:33,410
to maintain an open connection to all

00:12:30,949 --> 00:12:35,329
clients at all time but they do have to

00:12:33,410 --> 00:12:42,019
deal with these unpunched

00:12:35,329 --> 00:12:45,799
flushes ok so let's let's dive into how

00:12:42,019 --> 00:12:47,089
the Gruffalo service is designed so the

00:12:45,799 --> 00:12:49,399
gravel service is built the top of

00:12:47,089 --> 00:12:52,790
Nettie and that is basically an event

00:12:49,399 --> 00:12:55,850
loop with a pipeline of handlers the

00:12:52,790 --> 00:12:58,910
couple hands us and our inbound pipeline

00:12:55,850 --> 00:13:02,600
looks roughly like this the the first

00:12:58,910 --> 00:13:06,230
handler is in idle state and its role is

00:13:02,600 --> 00:13:08,439
to detect bogus or flaky clients and

00:13:06,230 --> 00:13:12,470
disconnect them when to prevent

00:13:08,439 --> 00:13:15,439
connection leaks next handler cuts the

00:13:12,470 --> 00:13:17,830
metrics lines according to the protocol

00:13:15,439 --> 00:13:20,320
basically this is done by a new

00:13:17,830 --> 00:13:23,980
the limiter for the line protocol and

00:13:20,320 --> 00:13:28,060
the next handler batches these line

00:13:23,980 --> 00:13:30,700
metric buffers the last handler is in

00:13:28,060 --> 00:13:33,130
charge of publishing these batches and

00:13:30,700 --> 00:13:36,820
usually by using the internal graphite

00:13:33,130 --> 00:13:40,870
client and the graphite client is also

00:13:36,820 --> 00:13:43,260
native based and for each carbon relay

00:13:40,870 --> 00:13:46,480
we have a pipeline that looks like so

00:13:43,260 --> 00:13:49,270
the first handler is an idle straight

00:13:46,480 --> 00:13:51,130
handler again it's always to detect when

00:13:49,270 --> 00:13:54,190
a target relay is disconnect and

00:13:51,130 --> 00:13:56,710
gracefully this prevents us from hanging

00:13:54,190 --> 00:13:58,090
in all sorts of really sad scenarios

00:13:56,710 --> 00:14:00,550
I'll talk about this in a bit

00:13:58,090 --> 00:14:03,070
and the second handler is in charge of

00:14:00,550 --> 00:14:06,040
connecting and reconnecting to the

00:14:03,070 --> 00:14:10,740
targets and performed throttling back

00:14:06,040 --> 00:14:14,310
pressure etc now we actually hold a

00:14:10,740 --> 00:14:17,020
client graphite relay cluster and

00:14:14,310 --> 00:14:19,620
perform client-side load balancing on

00:14:17,020 --> 00:14:22,840
top of that cluster so if we zoom out

00:14:19,620 --> 00:14:24,400
looks awfully like this diagram and the

00:14:22,840 --> 00:14:27,190
client performs round-robin load

00:14:24,400 --> 00:14:32,950
balancing and retries between the

00:14:27,190 --> 00:14:34,570
graphite relays targets now the

00:14:32,950 --> 00:14:37,710
connection to the carbene really

00:14:34,570 --> 00:14:40,180
sometimes gate is connected gracefully

00:14:37,710 --> 00:14:44,260
ungracefully in can happen from all

00:14:40,180 --> 00:14:48,430
sorts of reasons for example a

00:14:44,260 --> 00:14:50,800
deployment network blip etc but we have

00:14:48,430 --> 00:14:54,160
server relays in each cluster so the

00:14:50,800 --> 00:14:56,890
client can make best for attempt to find

00:14:54,160 --> 00:15:01,660
an alternative relay can still publish a

00:14:56,890 --> 00:15:04,180
matrix tour now we all know here that

00:15:01,660 --> 00:15:06,760
the network is unreliable and especially

00:15:04,180 --> 00:15:09,700
especially over the one these

00:15:06,760 --> 00:15:13,420
connections will happen and timeouts

00:15:09,700 --> 00:15:16,240
will occur all the time so a client

00:15:13,420 --> 00:15:19,270
detects these issues and reconnects to

00:15:16,240 --> 00:15:21,820
the downed relays as soon as they come

00:15:19,270 --> 00:15:26,920
back up and 4dr

00:15:21,820 --> 00:15:29,260
and durability reasons we replicate the

00:15:26,920 --> 00:15:31,329
matrix to two different regions this is

00:15:29,260 --> 00:15:35,350
actually the most shallow

00:15:31,329 --> 00:15:38,079
part of the service replicating over the

00:15:35,350 --> 00:15:40,649
one at this rate can be quite tricky and

00:15:38,079 --> 00:15:44,139
at some point it becomes even almost

00:15:40,649 --> 00:15:49,059
impossible but this is what makes a life

00:15:44,139 --> 00:15:52,540
more interesting right so one of the

00:15:49,059 --> 00:15:54,699
issues we discovered was that grace for

00:15:52,540 --> 00:15:57,579
grace less disconnections do happen and

00:15:54,699 --> 00:16:01,329
it can happen due to human error power

00:15:57,579 --> 00:16:03,939
outage for other generators tropical

00:16:01,329 --> 00:16:05,980
storms you know all these things that

00:16:03,939 --> 00:16:10,569
shouldn't happen but still somehow

00:16:05,980 --> 00:16:13,509
happen it an unreasonable rate and when

00:16:10,569 --> 00:16:16,209
it happens the TCP stack fails to detect

00:16:13,509 --> 00:16:20,040
the that the connection is down and

00:16:16,209 --> 00:16:22,749
simply sits the idle waiting for an ACK

00:16:20,040 --> 00:16:25,839
what happens next is that the connection

00:16:22,749 --> 00:16:29,230
becomes unrightable but it still seems

00:16:25,839 --> 00:16:31,299
to be connected so a system hangs

00:16:29,230 --> 00:16:35,829
waiting to be able to write and

00:16:31,299 --> 00:16:38,199
everything just hangs and shut down and

00:16:35,829 --> 00:16:41,860
what we did to solve this was to add a

00:16:38,199 --> 00:16:45,489
timeout now occurs when the connection

00:16:41,860 --> 00:16:47,679
is idle for more than a few seconds and

00:16:45,489 --> 00:16:51,369
we simply close that connection brutally

00:16:47,679 --> 00:16:53,529
and let the client reconnect that when

00:16:51,369 --> 00:16:57,069
the targets goes back up or becomes

00:16:53,529 --> 00:17:01,049
available again and if you want to

00:16:57,069 --> 00:17:04,230
simulate this scenario which you should

00:17:01,049 --> 00:17:07,959
it's very easy to do so with IP tables

00:17:04,230 --> 00:17:11,010
you simply block like the port just be

00:17:07,959 --> 00:17:17,339
careful not to lock yourself out like I

00:17:11,010 --> 00:17:19,809
once did more than once right so

00:17:17,339 --> 00:17:24,909
although we got rid of that thing that

00:17:19,809 --> 00:17:27,819
we call a queue in our system turns out

00:17:24,909 --> 00:17:30,029
there are still queues everywhere there

00:17:27,819 --> 00:17:32,860
are queues in the network interface the

00:17:30,029 --> 00:17:35,830
backlog queues there are queues in the

00:17:32,860 --> 00:17:38,320
net event loop the for the inbound and

00:17:35,830 --> 00:17:42,039
outbound messages there are queues in

00:17:38,320 --> 00:17:43,870
each and every device our messages go

00:17:42,039 --> 00:17:44,830
through in the network and outside the

00:17:43,870 --> 00:17:47,830
network

00:17:44,830 --> 00:17:51,100
and the problem with these cues are is

00:17:47,830 --> 00:17:53,500
that when we're not aware of the

00:17:51,100 --> 00:17:55,870
existence of the cue or when we don't

00:17:53,500 --> 00:17:59,650
have control over the length of the

00:17:55,870 --> 00:18:03,700
queue and under certain load conditions

00:17:59,650 --> 00:18:07,740
we may we will run out of resources and

00:18:03,700 --> 00:18:11,500
become unresponsive or even crush and

00:18:07,740 --> 00:18:15,430
another problem is that queues increased

00:18:11,500 --> 00:18:18,670
latency and basically your service

00:18:15,430 --> 00:18:21,700
latency will be the service processing

00:18:18,670 --> 00:18:25,090
time plus the time spent in a queue and

00:18:21,700 --> 00:18:27,850
as a side note many times we tend to

00:18:25,090 --> 00:18:29,710
measure just the internal service

00:18:27,850 --> 00:18:33,580
processing time and we totally ignore

00:18:29,710 --> 00:18:36,880
all the time spent in a backlog which

00:18:33,580 --> 00:18:42,700
practically leaves us blind to latency

00:18:36,880 --> 00:18:45,790
issues in our system luckily there are

00:18:42,700 --> 00:18:49,090
several strategies we can implement in

00:18:45,790 --> 00:18:51,600
order to avoid that queue buildup first

00:18:49,090 --> 00:18:56,110
strategy is called back pressure

00:18:51,600 --> 00:18:59,260
it means signaling the clients that we

00:18:56,110 --> 00:19:02,500
are currently unable to handle the

00:18:59,260 --> 00:19:05,920
requests or put it in other words push

00:19:02,500 --> 00:19:08,320
the clients back second strategy is

00:19:05,920 --> 00:19:11,890
called load shedding which in our

00:19:08,320 --> 00:19:14,110
context basically means dropping some of

00:19:11,890 --> 00:19:16,840
the metrics on the floor all of the

00:19:14,110 --> 00:19:21,280
metrics on the floor and third option is

00:19:16,840 --> 00:19:24,330
spooling which means temporarily storing

00:19:21,280 --> 00:19:27,610
the metrics we can handle at the moment

00:19:24,330 --> 00:19:31,180
somewhere else it may be publish them

00:19:27,610 --> 00:19:34,810
later when we can and these last two

00:19:31,180 --> 00:19:38,440
techniques basically mean reducing the

00:19:34,810 --> 00:19:41,530
SLA as we either lose data or delayed

00:19:38,440 --> 00:19:44,080
and what we said about metrics is that

00:19:41,530 --> 00:19:46,810
we do prefer freshness over here and

00:19:44,080 --> 00:19:51,700
yeah crushing is not really an option

00:19:46,810 --> 00:19:53,590
okay so the technique which shows for

00:19:51,700 --> 00:19:56,020
the graph or service is to apply back

00:19:53,590 --> 00:19:57,740
pressure this strategy means that the

00:19:56,020 --> 00:19:59,500
server will not

00:19:57,740 --> 00:20:03,140
request in an uncontrolled fashion

00:19:59,500 --> 00:20:05,960
instead it communicates the state of the

00:20:03,140 --> 00:20:08,240
server to the clients allowing them to

00:20:05,960 --> 00:20:12,770
slow down or to choose another target

00:20:08,240 --> 00:20:17,390
and allowing the system to add more

00:20:12,770 --> 00:20:20,270
resources if possible how do we

00:20:17,390 --> 00:20:23,480
implement back pressure so gruffalo is

00:20:20,270 --> 00:20:27,170
written on top of Nettie and Nettie

00:20:23,480 --> 00:20:29,390
provides us with an event a tells us

00:20:27,170 --> 00:20:32,660
when the outbound channel has become

00:20:29,390 --> 00:20:36,830
unrightable in when this happens we

00:20:32,660 --> 00:20:40,010
close all the inbound channels and we

00:20:36,830 --> 00:20:43,370
stop accepting new connections now this

00:20:40,010 --> 00:20:46,429
technique works but we found it's not

00:20:43,370 --> 00:20:49,340
fast enough especially on the load and

00:20:46,429 --> 00:20:51,970
the server is in fact already under

00:20:49,340 --> 00:20:56,300
stress when this event occurs

00:20:51,970 --> 00:20:59,750
so we also added a throttling mechanism

00:20:56,300 --> 00:21:03,200
based on the number of outstanding

00:20:59,750 --> 00:21:06,470
messages there is the number of messages

00:21:03,200 --> 00:21:10,670
that we sent but the a sync operation

00:21:06,470 --> 00:21:13,490
did not complete yet for then when a

00:21:10,670 --> 00:21:16,580
certain threshold is exceeded we apply

00:21:13,490 --> 00:21:20,300
but pressure and when we go below a

00:21:16,580 --> 00:21:24,559
low-water mark we then remove the back

00:21:20,300 --> 00:21:26,750
pressure and this way we can put bounds

00:21:24,559 --> 00:21:29,350
on the amount of data that we can

00:21:26,750 --> 00:21:33,610
actually hold in whomp

00:21:29,350 --> 00:21:36,350
licked in one connection so clients

00:21:33,610 --> 00:21:40,690
normally close the connection gracefully

00:21:36,350 --> 00:21:43,790
but when they don't for example when the

00:21:40,690 --> 00:21:46,250
client process crushed leaving half open

00:21:43,790 --> 00:21:49,100
sockets when the network cable gets

00:21:46,250 --> 00:21:52,429
disconnected when the data center is on

00:21:49,100 --> 00:21:55,550
fire it expired a long time for the

00:21:52,429 --> 00:21:58,940
receiving end of our service to detect

00:21:55,550 --> 00:22:02,540
this even when we use TCP keepalive so

00:21:58,940 --> 00:22:06,169
to avoid wasting precious resources on

00:22:02,540 --> 00:22:09,230
bogus or problematic clients where the

00:22:06,169 --> 00:22:11,300
timeouts the detect when a client

00:22:09,230 --> 00:22:13,970
connection becomes idle

00:22:11,300 --> 00:22:20,840
and we can simply close this connection

00:22:13,970 --> 00:22:24,400
when this happens yeah auto-scaling so

00:22:20,840 --> 00:22:27,170
when the loads on a service increases

00:22:24,400 --> 00:22:29,900
we'd like to be able to deploy more

00:22:27,170 --> 00:22:33,140
instances and spread the load between

00:22:29,900 --> 00:22:37,430
them and at UPS fire we implemented auto

00:22:33,140 --> 00:22:40,550
scaling based on metrics when we go

00:22:37,430 --> 00:22:43,880
above or below certain threshold we add

00:22:40,550 --> 00:22:48,160
or remove spots or on-demand hosts

00:22:43,880 --> 00:22:51,290
accordingly then there are several

00:22:48,160 --> 00:22:54,260
multiple strategies we can use to

00:22:51,290 --> 00:22:58,280
implement load balancing on top of these

00:22:54,260 --> 00:23:02,090
servers that we added we can use cloud

00:22:58,280 --> 00:23:05,240
provider based load balancer like LB

00:23:02,090 --> 00:23:09,110
this works it can be quite expensive for

00:23:05,240 --> 00:23:12,020
high traffic services it also feels a

00:23:09,110 --> 00:23:14,450
bit absurd to put a proxy on top of a

00:23:12,020 --> 00:23:17,120
proxy ruffalo is some kind of a proxy

00:23:14,450 --> 00:23:19,490
it's really ridiculous to put another

00:23:17,120 --> 00:23:22,850
proxy on top of it we can use a layer

00:23:19,490 --> 00:23:25,310
for load balance this is quite cheap it

00:23:22,850 --> 00:23:28,370
works well for network services it

00:23:25,310 --> 00:23:30,110
doesn't always spread the load evenly on

00:23:28,370 --> 00:23:33,770
all targets really depending on the

00:23:30,110 --> 00:23:37,250
implementation and on your ops team we

00:23:33,770 --> 00:23:40,220
can use the NS this works it's kind of

00:23:37,250 --> 00:23:42,100
hard to get it right at scale and

00:23:40,220 --> 00:23:45,320
another option is to implement

00:23:42,100 --> 00:23:49,310
client-side all balancing which at least

00:23:45,320 --> 00:23:52,520
for my experience works best for high

00:23:49,310 --> 00:23:55,390
throughput low latency services but

00:23:52,520 --> 00:23:59,780
requires quite a significant

00:23:55,390 --> 00:24:03,200
programmatic effort on your end so and

00:23:59,780 --> 00:24:07,910
do note that no matter which strategy we

00:24:03,200 --> 00:24:13,100
choose the actual targets locations need

00:24:07,910 --> 00:24:14,690
to be transparent to the clients okay to

00:24:13,100 --> 00:24:17,780
summarize

00:24:14,690 --> 00:24:21,710
we talked about what being responsive

00:24:17,780 --> 00:24:24,860
means and we explained what resilience

00:24:21,710 --> 00:24:28,190
and resiliency and elasticity means and

00:24:24,860 --> 00:24:30,740
we mentioned that in order to be

00:24:28,190 --> 00:24:34,640
responsive your system needs to be both

00:24:30,740 --> 00:24:37,580
resilient and elastic and we explained

00:24:34,640 --> 00:24:40,160
that we should be using a message driven

00:24:37,580 --> 00:24:43,400
architecture in order to achieve

00:24:40,160 --> 00:24:46,370
resilience and elasticity we also showed

00:24:43,400 --> 00:24:49,370
that all these how all these bombastic

00:24:46,370 --> 00:24:53,090
words can actually be applied in real

00:24:49,370 --> 00:24:56,210
life and we also stated that and

00:24:53,090 --> 00:25:00,200
demonstrated how these principles can

00:24:56,210 --> 00:25:02,840
and should be applied everywhere in all

00:25:00,200 --> 00:25:04,310
layers of our system and this is

00:25:02,840 --> 00:25:07,130
actually quite a small system when you

00:25:04,310 --> 00:25:10,370
have you know a large system you do need

00:25:07,130 --> 00:25:14,240
to apply it everywhere and so after all

00:25:10,370 --> 00:25:16,820
this walk I can scale of the system that

00:25:14,240 --> 00:25:19,220
our system has to deal with is about

00:25:16,820 --> 00:25:22,130
three hundred thousand metrics per

00:25:19,220 --> 00:25:25,360
second per region which translates to

00:25:22,130 --> 00:25:27,830
around 18 million matrix per minute each

00:25:25,360 --> 00:25:29,870
instance of a Gruffalo can actually deal

00:25:27,830 --> 00:25:35,230
with a lot more than that so we do sleep

00:25:29,870 --> 00:25:38,930
well at night right so one last point

00:25:35,230 --> 00:25:43,640
have you noticed how I haven't mentioned

00:25:38,930 --> 00:25:52,250
kubernetes even once throughout this

00:25:43,640 --> 00:25:55,550
session do you know why that word as

00:25:52,250 --> 00:25:58,610
well because kubernetes will not solve

00:25:55,550 --> 00:26:01,460
your design issues it's just a system

00:25:58,610 --> 00:26:04,760
that may help you but it's not magical

00:26:01,460 --> 00:26:07,160
right you will still have to do some

00:26:04,760 --> 00:26:10,220
proper engineering work in order to get

00:26:07,160 --> 00:26:12,110
everything to work at scale so please

00:26:10,220 --> 00:26:15,590
people please stop talking about

00:26:12,110 --> 00:26:19,820
kubernetes talk about engineering thank

00:26:15,590 --> 00:26:21,020
you yes if Gruffalo is still active and

00:26:19,820 --> 00:26:23,900
maintained yeah it's still active

00:26:21,020 --> 00:26:26,660
maintain it used to be I developed it

00:26:23,900 --> 00:26:27,790
when I worked for our brand so there's a

00:26:26,660 --> 00:26:30,700
broader

00:26:27,790 --> 00:26:32,590
code there but I it because they

00:26:30,700 --> 00:26:35,410
were very unresponsive in letting me

00:26:32,590 --> 00:26:38,260
commit stuff so it's under my name at

00:26:35,410 --> 00:26:43,660
the moment maybe I'll move it to

00:26:38,260 --> 00:26:54,790
absorber is maintained here did I use

00:26:43,660 --> 00:26:58,450
kubernetes really so with push back to

00:26:54,790 --> 00:27:04,180
the client the client can push data what

00:26:58,450 --> 00:27:08,680
happens responsibility if if the clients

00:27:04,180 --> 00:27:11,620
when when I perform pushback and suppose

00:27:08,680 --> 00:27:13,750
the clients can't do anything if it's a

00:27:11,620 --> 00:27:15,190
usual user yeah at least you get some

00:27:13,750 --> 00:27:18,460
indication that the system is under

00:27:15,190 --> 00:27:21,670
stress but normally it's another service

00:27:18,460 --> 00:27:24,850
and this service can be can have some

00:27:21,670 --> 00:27:27,700
sort of a circuit breaker it can have

00:27:24,850 --> 00:27:29,860
other targets it can have I don't know

00:27:27,700 --> 00:27:33,310
other options to deal with the stress it

00:27:29,860 --> 00:27:35,410
can slow down okay you have options but

00:27:33,310 --> 00:27:38,560
the most important part of blood

00:27:35,410 --> 00:27:40,210
pressure is that you know that the

00:27:38,560 --> 00:27:42,970
target that you currently dealing with

00:27:40,210 --> 00:27:44,680
is under stress it's signals and you

00:27:42,970 --> 00:27:47,110
have options to do something it doesn't

00:27:44,680 --> 00:27:49,450
just crash and leaving you with an error

00:27:47,110 --> 00:27:52,380
so the error is actually contained

00:27:49,450 --> 00:27:58,729
somewhere Thanks

00:27:52,380 --> 00:27:58,729
[Applause]

00:27:58,880 --> 00:28:00,940

YouTube URL: https://www.youtube.com/watch?v=Fg1SJufaHOs


