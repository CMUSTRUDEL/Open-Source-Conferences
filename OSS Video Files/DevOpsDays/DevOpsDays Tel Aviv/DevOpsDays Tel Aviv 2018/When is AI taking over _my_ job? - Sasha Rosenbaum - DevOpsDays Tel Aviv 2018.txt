Title: When is AI taking over _my_ job? - Sasha Rosenbaum - DevOpsDays Tel Aviv 2018
Publication date: 2019-01-03
Playlist: DevOpsDays Tel Aviv 2018
Description: 
	There are a lot of misconceptions regarding AI. People are often not worried enough, or worried about the wrong things. I would love the audience to experience an aha moment with regards to what AI is capable of, and what the future might hold for us.
Captions: 
	00:00:01,980 --> 00:00:18,440
[Music]

00:00:19,550 --> 00:00:43,469
these are just created by a machine

00:00:23,900 --> 00:00:46,079
machine okay hi everybody so I would

00:00:43,469 --> 00:00:48,059
like to introduce myself real quick I

00:00:46,079 --> 00:00:49,680
apologize if I look back into the slides

00:00:48,059 --> 00:00:51,390
I just don't like to stand in front of a

00:00:49,680 --> 00:00:54,780
podium so much

00:00:51,390 --> 00:00:57,930
so I'm Sasha Rosa mom Ivan has a global

00:00:54,780 --> 00:00:59,850
black belt they work for myself I'm all

00:00:57,930 --> 00:01:00,359
soaked organizer of DevOps Taoiseach

00:00:59,850 --> 00:01:02,399
kaavo

00:01:00,359 --> 00:01:03,690
so it's an awesome conference kind of

00:01:02,399 --> 00:01:07,680
like this one you should all come

00:01:03,690 --> 00:01:09,990
especially anyway and I wrote a book

00:01:07,680 --> 00:01:12,420
plus you're on admin functions it's

00:01:09,990 --> 00:01:16,830
about 80% outdated right now so you can

00:01:12,420 --> 00:01:19,110
still get it on Amazon anyway um so

00:01:16,830 --> 00:01:21,119
before I started the conversation I want

00:01:19,110 --> 00:01:23,640
to say what I'm not gonna talk about so

00:01:21,119 --> 00:01:25,890
I am NOT gonna talk about terminator

00:01:23,640 --> 00:01:29,369
coming to town and you know humanoid

00:01:25,890 --> 00:01:31,439
machines taking over that is because

00:01:29,369 --> 00:01:34,409
it's very unlikely scenario it's

00:01:31,439 --> 00:01:36,840
probably not gonna happen but I am going

00:01:34,409 --> 00:01:39,869
to briefly mention the possibility of

00:01:36,840 --> 00:01:43,320
superhuman intelligence because it is

00:01:39,869 --> 00:01:45,149
actually kind of relevant to the talk so

00:01:43,320 --> 00:01:48,149
I want to start with a couple of

00:01:45,149 --> 00:01:50,009
definitions and for those of you who are

00:01:48,149 --> 00:01:51,780
familiar these things just bear with me

00:01:50,009 --> 00:01:54,720
because it's important to say that we

00:01:51,780 --> 00:01:58,170
speak common language and understand the

00:01:54,720 --> 00:02:00,479
same thing when we say something so the

00:01:58,170 --> 00:02:02,630
first one is intelligence so what is

00:02:00,479 --> 00:02:04,880
intelligence this is like the easy

00:02:02,630 --> 00:02:07,520
the shortest definition I can give it is

00:02:04,880 --> 00:02:11,420
the ability to accomplish complex goals

00:02:07,520 --> 00:02:14,090
right so then next we move to artificial

00:02:11,420 --> 00:02:16,340
intelligence which is the ability of a

00:02:14,090 --> 00:02:18,349
machine to perform tasks commonly

00:02:16,340 --> 00:02:21,319
associated with a human with a human

00:02:18,349 --> 00:02:24,140
intelligence so if I can do something

00:02:21,319 --> 00:02:26,330
that you know we typically say only

00:02:24,140 --> 00:02:28,190
people can do if a machine can do that

00:02:26,330 --> 00:02:30,410
we call it artificial intelligence this

00:02:28,190 --> 00:02:32,450
is a very broad definition so anything

00:02:30,410 --> 00:02:35,599
that you know an animal could possibly

00:02:32,450 --> 00:02:37,250
do and then we have the concept of

00:02:35,599 --> 00:02:39,769
machine learning again the pros

00:02:37,250 --> 00:02:41,540
definition we can give it is that it's a

00:02:39,769 --> 00:02:44,180
science of getting computers to act

00:02:41,540 --> 00:02:47,239
without being explicitly programmed now

00:02:44,180 --> 00:02:49,849
this is important right because if we

00:02:47,239 --> 00:02:54,560
look back at just about you know 80

00:02:49,849 --> 00:02:56,959
years ago Ada Lovelace who is the person

00:02:54,560 --> 00:02:59,599
who's the programming language ADA is

00:02:56,959 --> 00:03:01,819
named after right she said something to

00:02:59,599 --> 00:03:03,590
make said this is not an exact quote but

00:03:01,819 --> 00:03:06,230
that computers can only do with their

00:03:03,590 --> 00:03:08,000
program to do and this objection is

00:03:06,230 --> 00:03:10,400
still being used today in fact you know

00:03:08,000 --> 00:03:12,950
by American Congress saying that hey you

00:03:10,400 --> 00:03:14,780
know computers are only doing what a

00:03:12,950 --> 00:03:16,310
programmer needs help at all it's do but

00:03:14,780 --> 00:03:18,769
now when we talk about machine learning

00:03:16,310 --> 00:03:21,319
it's explicitly about what she was doing

00:03:18,769 --> 00:03:23,739
what we did not program them to do it's

00:03:21,319 --> 00:03:25,639
about machines learning from data

00:03:23,739 --> 00:03:27,920
recognizing patterns and then

00:03:25,639 --> 00:03:30,170
implementing something based on these

00:03:27,920 --> 00:03:33,139
patterns so in many cases it's a black

00:03:30,170 --> 00:03:34,850
box so people don't know what what the

00:03:33,139 --> 00:03:37,790
why the machine is actually making the

00:03:34,850 --> 00:03:40,100
choices it's making right so before we

00:03:37,790 --> 00:03:42,230
go any further I like you guys to help

00:03:40,100 --> 00:03:45,970
me out a little bit so everybody who

00:03:42,230 --> 00:03:45,970
can't stand up can you please stand up

00:03:46,329 --> 00:03:51,560
thank you

00:03:47,840 --> 00:03:53,120
and then can you touch your nose okay

00:03:51,560 --> 00:03:55,160
cool thanks very much

00:03:53,120 --> 00:03:57,410
now I just think you want to volunteer

00:03:55,160 --> 00:04:00,530
to come up here and explain to me

00:03:57,410 --> 00:04:03,380
exactly how you did that so how did your

00:04:00,530 --> 00:04:04,910
brain understand the command and then

00:04:03,380 --> 00:04:07,160
how did you bring to and submit it to

00:04:04,910 --> 00:04:09,620
your muscles and which muscles did you

00:04:07,160 --> 00:04:14,030
exactly use to lift up your hand and

00:04:09,620 --> 00:04:16,950
touch she knows no okay

00:04:14,030 --> 00:04:19,590
maybe somebody know so the truth is that

00:04:16,950 --> 00:04:25,080
you don't know and I don't know either

00:04:19,590 --> 00:04:27,270
right but you know Thursday this is the

00:04:25,080 --> 00:04:30,840
video of computer simulation learning to

00:04:27,270 --> 00:04:33,210
walk and it's obviously facing some

00:04:30,840 --> 00:04:35,580
obstacles some curl let's see here I

00:04:33,210 --> 00:04:37,230
don't know but the truth is that like

00:04:35,580 --> 00:04:40,470
this is what's really happening today

00:04:37,230 --> 00:04:42,140
with machines with robots and just

00:04:40,470 --> 00:04:45,150
regular machines that are deciding

00:04:42,140 --> 00:04:46,320
algorithmically on things they don't

00:04:45,150 --> 00:04:48,690
they're not being explicitly programmed

00:04:46,320 --> 00:04:51,630
by humans who tell it okay if I throw

00:04:48,690 --> 00:04:53,040
the Box at this angle that you need to

00:04:51,630 --> 00:04:54,600
ship slightly to the left

00:04:53,040 --> 00:04:56,310
in fact this robe is alert this

00:04:54,600 --> 00:04:57,750
simulation is learning to walk pretty

00:04:56,310 --> 00:05:01,440
much like humans do right through

00:04:57,750 --> 00:05:04,470
practice so this is what artificial

00:05:01,440 --> 00:05:05,610
intelligence is all about and then this

00:05:04,470 --> 00:05:07,410
is a lesson finition

00:05:05,610 --> 00:05:09,360
I'm gonna read it cuz it's really long

00:05:07,410 --> 00:05:11,490
super human artificial general

00:05:09,360 --> 00:05:13,530
intelligence is the ability of a machine

00:05:11,490 --> 00:05:17,580
to accomplish any goal as well or better

00:05:13,530 --> 00:05:20,750
than humans so if a machine can outdo me

00:05:17,580 --> 00:05:23,490
on every single task right manual or

00:05:20,750 --> 00:05:25,290
intellectual or whatever if it can do as

00:05:23,490 --> 00:05:26,970
well or better than me then we have

00:05:25,290 --> 00:05:31,350
super super human artificial

00:05:26,970 --> 00:05:33,660
intelligence right so okay but the truth

00:05:31,350 --> 00:05:36,030
is that it's not the first sense so like

00:05:33,660 --> 00:05:37,410
there's a lot of theories and whether or

00:05:36,030 --> 00:05:40,800
not superhuman intelligence is gonna

00:05:37,410 --> 00:05:42,930
happen but in fact I claim that we need

00:05:40,800 --> 00:05:45,540
to start worrying about it before it

00:05:42,930 --> 00:05:48,750
happens so a long long before that right

00:05:45,540 --> 00:05:51,030
because so what we have right now is a

00:05:48,750 --> 00:05:53,370
lot of machines that can do very narrow

00:05:51,030 --> 00:05:55,290
tasks right so if I'm giving some

00:05:53,370 --> 00:05:57,870
examples I have to start with this one

00:05:55,290 --> 00:06:00,450
so this was Garry Kasparov and it was

00:05:57,870 --> 00:06:05,940
playing he was playing chess against IBM

00:06:00,450 --> 00:06:08,820
Steve blue don't remember the year 97 so

00:06:05,940 --> 00:06:10,740
you know in case a year before the

00:06:08,820 --> 00:06:12,480
computer one Garry Kasparov said a

00:06:10,740 --> 00:06:14,640
computer is no match for a human can

00:06:12,480 --> 00:06:16,500
never win win in chess just requires you

00:06:14,640 --> 00:06:19,410
know higher level things thinking and

00:06:16,500 --> 00:06:20,249
then this happened um that of course

00:06:19,410 --> 00:06:24,419
this happen

00:06:20,249 --> 00:06:27,959
seven 2015 a computer one against gold

00:06:24,419 --> 00:06:31,379
player and in fact it did so by using a

00:06:27,959 --> 00:06:33,629
strategy that humans didn't use so it

00:06:31,379 --> 00:06:35,609
learns from human examples but then it

00:06:33,629 --> 00:06:38,759
devised a strategy that was not common

00:06:35,609 --> 00:06:41,879
through and among the players but this

00:06:38,759 --> 00:06:44,189
is a games right if we look at for

00:06:41,879 --> 00:06:47,309
instance Microsoft cognitive services we

00:06:44,189 --> 00:06:48,989
surpass human parity in speech to text

00:06:47,309 --> 00:06:51,839
so now like before to talk about a

00:06:48,989 --> 00:06:53,729
conference and have a little text coming

00:06:51,839 --> 00:06:55,619
up cite subtitles it's done by a machine

00:06:53,729 --> 00:06:57,839
and something better than a human could

00:06:55,619 --> 00:06:59,969
do it we surpass human error you

00:06:57,839 --> 00:07:03,419
translating Chinese to English which was

00:06:59,969 --> 00:07:05,309
no small fit and of course so when I

00:07:03,419 --> 00:07:07,199
went to school in Madras he was

00:07:05,309 --> 00:07:09,209
considered a super hard problem and

00:07:07,199 --> 00:07:11,969
people were like debating computers can

00:07:09,209 --> 00:07:14,999
ever like dream across on it was humans

00:07:11,969 --> 00:07:16,679
it was considering a very hard task now

00:07:14,999 --> 00:07:19,049
as far as they try suppose my computer

00:07:16,679 --> 00:07:21,119
and my phone to unlock by recognizing me

00:07:19,049 --> 00:07:22,949
and orders and many countries are

00:07:21,119 --> 00:07:24,389
exploring the possibility of doing the

00:07:22,949 --> 00:07:26,969
same thing right

00:07:24,389 --> 00:07:29,069
so the interesting point about this it's

00:07:26,969 --> 00:07:32,909
a lot of AI researchers are complaining

00:07:29,069 --> 00:07:34,889
about once the task is accomplished it's

00:07:32,909 --> 00:07:36,839
considered table stakes like people like

00:07:34,889 --> 00:07:39,149
whoa corns my form recognized my face

00:07:36,839 --> 00:07:40,860
that's not a problem at all but it used

00:07:39,149 --> 00:07:43,649
to be considered very hard clones it's

00:07:40,860 --> 00:07:45,389
to Seoul right now what are the

00:07:43,649 --> 00:07:47,219
implications of these things like I said

00:07:45,389 --> 00:07:48,689
this isn't that nowhere near general

00:07:47,219 --> 00:07:50,939
intelligence we're talking about very

00:07:48,689 --> 00:07:54,360
narrow least narrow not only designed

00:07:50,939 --> 00:07:55,979
tasks that computers can do maybe as

00:07:54,360 --> 00:07:57,749
well maybe a little bit better maybe a

00:07:55,979 --> 00:07:59,819
little bit less well than humans right

00:07:57,749 --> 00:08:02,729
but so okay

00:07:59,819 --> 00:08:05,129
major firms in the u.s. now are

00:08:02,729 --> 00:08:06,599
implementing computers to do the

00:08:05,129 --> 00:08:08,429
drive-throughs because like we said

00:08:06,599 --> 00:08:10,639
speech-to-text mr. fretts you mean

00:08:08,429 --> 00:08:13,379
parodies so if you drive through I

00:08:10,639 --> 00:08:16,079
mentioned fast food chain and you order

00:08:13,379 --> 00:08:17,969
your food um it actually makes more

00:08:16,079 --> 00:08:20,129
sense for me to employ computer and to

00:08:17,969 --> 00:08:22,860
translate what order you actually gave

00:08:20,129 --> 00:08:24,869
me um then of course we have tellers

00:08:22,860 --> 00:08:26,849
they don't have to look like this but a

00:08:24,869 --> 00:08:28,319
lot of stores will have self checkout

00:08:26,849 --> 00:08:30,320
and they're implementing more and more

00:08:28,319 --> 00:08:32,780
of these things right so

00:08:30,320 --> 00:08:34,760
in Texas this guy remembers much better

00:08:32,780 --> 00:08:36,890
than a human where everything is stored

00:08:34,760 --> 00:08:39,740
and before the stock on something right

00:08:36,890 --> 00:08:41,570
so it's actually very efficient of

00:08:39,740 --> 00:08:43,910
course you can order whatever you want

00:08:41,570 --> 00:08:45,830
and talk to you support and it might not

00:08:43,910 --> 00:08:48,860
actually being a human in fact I think

00:08:45,830 --> 00:08:51,140
in the u.s. specifically most of the

00:08:48,860 --> 00:08:53,180
first year support is now not human and

00:08:51,140 --> 00:08:56,180
it's bots that are getting Priestley

00:08:53,180 --> 00:08:58,970
better this is an interesting example

00:08:56,180 --> 00:09:01,040
because we don't even think about it but

00:08:58,970 --> 00:09:03,800
all the recommendations in our lives

00:09:01,040 --> 00:09:06,110
I get being guided by machines so

00:09:03,800 --> 00:09:09,080
everything where do you eat what males

00:09:06,110 --> 00:09:12,020
you read what shoes you buy what

00:09:09,080 --> 00:09:13,910
vacations you look at everything in the

00:09:12,020 --> 00:09:16,010
world it's being recommended by machines

00:09:13,910 --> 00:09:17,960
who store information about who you are

00:09:16,010 --> 00:09:20,300
what you like they remember it much

00:09:17,960 --> 00:09:23,200
better than the human can right and they

00:09:20,300 --> 00:09:27,350
actually persuade you to purchase things

00:09:23,200 --> 00:09:29,420
this is like actually surpassing you

00:09:27,350 --> 00:09:31,430
know humans it's so many levels and we

00:09:29,420 --> 00:09:33,200
don't even pay attention to that our

00:09:31,430 --> 00:09:36,790
life is over anything guided by machines

00:09:33,200 --> 00:09:39,890
right so if we look at us job markets so

00:09:36,790 --> 00:09:41,780
officer do the study in 2013 I think

00:09:39,890 --> 00:09:45,080
these numbers would be probably actually

00:09:41,780 --> 00:09:47,240
higher today but in 2013 they said how

00:09:45,080 --> 00:09:50,270
likely is a job to be replaced to be

00:09:47,240 --> 00:09:52,670
computerized away right so we look at

00:09:50,270 --> 00:09:54,140
clerks it's so different types of they

00:09:52,670 --> 00:09:56,630
recognize different types of clerks

00:09:54,140 --> 00:09:59,570
different types of cashiers one type of

00:09:56,630 --> 00:10:02,240
the contents I don't know so in any case

00:09:59,570 --> 00:10:05,450
this is like eighty five to ninety four

00:10:02,240 --> 00:10:07,760
percent for many of the occupations now

00:10:05,450 --> 00:10:11,080
if you look at us you're a statistic by

00:10:07,760 --> 00:10:13,970
a patient there is a very big

00:10:11,080 --> 00:10:16,160
coincidence of this like so like sales

00:10:13,970 --> 00:10:18,050
and related occupations employs fifteen

00:10:16,160 --> 00:10:19,610
million people and according to some

00:10:18,050 --> 00:10:22,100
comments more like 30 million people

00:10:19,610 --> 00:10:24,680
drivers employ about 30 million people

00:10:22,100 --> 00:10:26,300
in the US so we're potentially so if we

00:10:24,680 --> 00:10:28,100
do implement self-driving cars we're

00:10:26,300 --> 00:10:29,960
talking about taking out you know I

00:10:28,100 --> 00:10:32,660
don't know ten percent of jobs in the US

00:10:29,960 --> 00:10:33,970
yeah this is great even if I'm not a

00:10:32,660 --> 00:10:37,899
driver then surely

00:10:33,970 --> 00:10:40,449
influence my life now I am NOT a driver

00:10:37,899 --> 00:10:43,389
like I said I'm sort of not a few here

00:10:40,449 --> 00:10:45,699
probably as a driver and you're probably

00:10:43,389 --> 00:10:47,139
not a cashmere so you you may say okay I

00:10:45,699 --> 00:10:52,000
don't have to worry about this I'm Way

00:10:47,139 --> 00:10:53,410
smarter so of course we can talk sense

00:10:52,000 --> 00:10:54,459
for a dialogue phase I'm gonna say

00:10:53,410 --> 00:10:56,670
infrastructure as code

00:10:54,459 --> 00:10:59,709
I mean we're automating jobs away yeh

00:10:56,670 --> 00:11:02,170
but this is not a I write most commonly

00:10:59,709 --> 00:11:03,819
this is not even time by AI is just done

00:11:02,170 --> 00:11:05,290
by automation so it is actually

00:11:03,819 --> 00:11:09,790
programmed to do what's that for you

00:11:05,290 --> 00:11:12,160
what you know somebody told it to do now

00:11:09,790 --> 00:11:15,399
we can move into a different space which

00:11:12,160 --> 00:11:16,839
is like okay deep code is a piece of

00:11:15,399 --> 00:11:19,779
software that's sitting out there they

00:11:16,839 --> 00:11:21,430
can it's like a gravely for your code it

00:11:19,779 --> 00:11:23,500
can make accommodations it looks at

00:11:21,430 --> 00:11:25,959
other like open source samples and it

00:11:23,500 --> 00:11:27,550
goes like okay you know you can you can

00:11:25,959 --> 00:11:30,970
improve your code by solving this in

00:11:27,550 --> 00:11:33,819
this issue is sort of like lint for the

00:11:30,970 --> 00:11:36,370
code that you write and this is a I know

00:11:33,819 --> 00:11:38,019
if we look at so this is a micro product

00:11:36,370 --> 00:11:40,059
of international you have your

00:11:38,019 --> 00:11:42,250
accommodations engine which is called as

00:11:40,059 --> 00:11:44,350
your adviser it advises you on many

00:11:42,250 --> 00:11:46,209
different things on how to improve your

00:11:44,350 --> 00:11:49,000
clouds appointments one of the things

00:11:46,209 --> 00:11:51,160
that it does it recommends - to you -

00:11:49,000 --> 00:11:53,559
for instance index certain columns in a

00:11:51,160 --> 00:11:55,480
database index on started call them in a

00:11:53,559 --> 00:11:57,850
database or you can search and perform

00:11:55,480 --> 00:12:00,189
better we've had a lot of examples would

00:11:57,850 --> 00:12:02,589
solve the problems for people this is a

00:12:00,189 --> 00:12:05,259
job in the industry it's called a DBA

00:12:02,589 --> 00:12:07,540
those people are used to be paid a lot

00:12:05,259 --> 00:12:10,240
of money we're gradually just taking

00:12:07,540 --> 00:12:13,779
them out by you know introducing smart a

00:12:10,240 --> 00:12:15,850
ice another thesis offs are out there

00:12:13,779 --> 00:12:18,160
it's called Hugh is earned it takes your

00:12:15,850 --> 00:12:20,079
wireframes and your like different

00:12:18,160 --> 00:12:22,660
Photoshop pictures and it translates

00:12:20,079 --> 00:12:25,839
them into HTML and CSS it does it pretty

00:12:22,660 --> 00:12:27,200
well that's another job that means it's

00:12:25,839 --> 00:12:29,120
called a front

00:12:27,200 --> 00:12:31,130
signer those people spend hours usually

00:12:29,120 --> 00:12:33,020
trying to render CSS and the way that

00:12:31,130 --> 00:12:36,440
makes sense for the way your friend so a

00:12:33,020 --> 00:12:38,990
but if you had any does this is an

00:12:36,440 --> 00:12:41,570
effort that was said by Microsoft at the

00:12:38,990 --> 00:12:43,610
University of Cambridge and they build a

00:12:41,570 --> 00:12:45,650
little simple program that looks at

00:12:43,610 --> 00:12:48,140
again at all those open source code

00:12:45,650 --> 00:12:50,720
samples and it belts programs thank you

00:12:48,140 --> 00:12:53,120
so you know how you this come to simple

00:12:50,720 --> 00:12:55,820
competitions for coding that you need to

00:12:53,120 --> 00:12:57,170
solve some sort of problem so this thing

00:12:55,820 --> 00:12:59,600
does pretty well in these competitions

00:12:57,170 --> 00:13:01,490
again it's pretty basic it's solving

00:12:59,600 --> 00:13:04,970
little problems right now it's not like

00:13:01,490 --> 00:13:07,280
super out there but the thing is when

00:13:04,970 --> 00:13:08,870
these things when this thing gets good

00:13:07,280 --> 00:13:12,200
it's actually gonna be better than me

00:13:08,870 --> 00:13:14,240
because it doesn't sleep doesn't eat it

00:13:12,200 --> 00:13:17,960
can be clung to a hundred servers and it

00:13:14,240 --> 00:13:23,020
never forgets to no invulnerability so

00:13:17,960 --> 00:13:26,270
yeah so this is a huntsman make a

00:13:23,020 --> 00:13:29,120
metaphor for this it's like a yeah so if

00:13:26,270 --> 00:13:31,310
you imagine human competence as this

00:13:29,120 --> 00:13:34,550
landscape with like valleys and

00:13:31,310 --> 00:13:36,500
mountains AI is slowly just it's like a

00:13:34,550 --> 00:13:38,600
water flooding the landscape right I'm

00:13:36,500 --> 00:13:40,790
said no not in over this so I feel

00:13:38,600 --> 00:13:42,950
pretty comfortable but eventually it

00:13:40,790 --> 00:13:45,110
kind of gets there little by little by

00:13:42,950 --> 00:13:48,530
replacing will pieces of different

00:13:45,110 --> 00:13:50,390
competencies and then another thing

00:13:48,530 --> 00:13:52,730
that's important to mention it's is the

00:13:50,390 --> 00:13:54,830
rate of progress so everybody probably

00:13:52,730 --> 00:13:56,480
heard of the Moore's Law and the fact

00:13:54,830 --> 00:13:58,520
that we're doubling computer power every

00:13:56,480 --> 00:14:00,200
year in fact it happens not just in

00:13:58,520 --> 00:14:02,840
computers but it definitely happens with

00:14:00,200 --> 00:14:04,310
computers this is exponential relative

00:14:02,840 --> 00:14:06,530
progress we've had this in many

00:14:04,310 --> 00:14:08,180
industries recently since we started

00:14:06,530 --> 00:14:11,420
sharing knowledge effective way around

00:14:08,180 --> 00:14:15,110
planned and expeditionary of progress it

00:14:11,420 --> 00:14:18,020
so intuitively we think that tomorrow is

00:14:15,110 --> 00:14:19,520
gonna look a bit like yesterday right so

00:14:18,020 --> 00:14:22,190
in the reign of progress it's going to

00:14:19,520 --> 00:14:23,660
be just as slow as it was or just as

00:14:22,190 --> 00:14:25,880
fast as it was yesterday

00:14:23,660 --> 00:14:27,500
right but now this is no longer true its

00:14:25,880 --> 00:14:29,300
exponential rate of progress which looks

00:14:27,500 --> 00:14:31,840
like this it you know it starts slowly

00:14:29,300 --> 00:14:35,060
and then it's like asymptotically

00:14:31,840 --> 00:14:36,490
blowing us up right so you know at one

00:14:35,060 --> 00:14:38,350
point when you get to like

00:14:36,490 --> 00:14:40,840
then you get to a million of these

00:14:38,350 --> 00:14:43,330
bacteria really professes you only if I

00:14:40,840 --> 00:14:44,800
found to represent exponential rate of

00:14:43,330 --> 00:14:47,920
progress but this is what it looks like

00:14:44,800 --> 00:14:49,870
an end rate of adoption I was talking to

00:14:47,920 --> 00:14:52,030
a co-worker of mine about self-driving

00:14:49,870 --> 00:14:53,500
cars and he was like well you know I

00:14:52,030 --> 00:14:55,450
have a couple of friends and they have

00:14:53,500 --> 00:14:57,490
self-driving cars and you know they

00:14:55,450 --> 00:14:59,170
don't trust them I don't think like even

00:14:57,490 --> 00:15:01,090
if it was out tomorrow I don't think

00:14:59,170 --> 00:15:02,920
people would trust them so it's just

00:15:01,090 --> 00:15:06,820
like wanting to do something for this

00:15:02,920 --> 00:15:08,800
this is 1900 it's at Fifth Avenue in New

00:15:06,820 --> 00:15:11,590
York City there's one single car

00:15:08,800 --> 00:15:13,000
everything else is horses I mean if you

00:15:11,590 --> 00:15:16,630
don't see the picture you can google it

00:15:13,000 --> 00:15:18,460
it's gonna come anyway this is 1913

00:15:16,630 --> 00:15:20,920
there's no horses in this street that's

00:15:18,460 --> 00:15:21,970
the same street it's old cars and that

00:15:20,920 --> 00:15:24,490
was the beginning of the last century

00:15:21,970 --> 00:15:27,340
right so we move a little bit faster

00:15:24,490 --> 00:15:30,100
today than we did then so if you have

00:15:27,340 --> 00:15:32,560
any doubts if your employer has an

00:15:30,100 --> 00:15:34,030
advantage in employing a machine over

00:15:32,560 --> 00:15:35,740
you you're probably gonna choose a

00:15:34,030 --> 00:15:38,080
machine because another thing that they

00:15:35,740 --> 00:15:40,510
don't do is they don't don't beat taxes

00:15:38,080 --> 00:15:44,730
for a point and machine as opposed to

00:15:40,510 --> 00:15:47,680
human so uh what do we do now

00:15:44,730 --> 00:15:53,890
so the first thing it's important don't

00:15:47,680 --> 00:15:55,780
panic maybe branch out anyway so which

00:15:53,890 --> 00:15:58,200
jobs will linger it's also important

00:15:55,780 --> 00:16:00,370
thing so jobs involving perception

00:15:58,200 --> 00:16:02,350
manipulation creative intelligence and

00:16:00,370 --> 00:16:06,160
social intelligence which is why I love

00:16:02,350 --> 00:16:08,590
my job right involves all of these but

00:16:06,160 --> 00:16:10,180
we probably all of us in the industry so

00:16:08,590 --> 00:16:12,340
this is the same Oscars study doing

00:16:10,180 --> 00:16:16,420
doing on the numbers for engineers

00:16:12,340 --> 00:16:18,430
scientists therapists and then software

00:16:16,420 --> 00:16:20,110
developers right so if you these

00:16:18,430 --> 00:16:22,570
professions that are very unlikely to be

00:16:20,110 --> 00:16:25,780
replaced any time in the near future so

00:16:22,570 --> 00:16:27,790
we keep us safer around our industry

00:16:25,780 --> 00:16:30,100
again this is range because there are

00:16:27,790 --> 00:16:32,440
different types of engineers now we

00:16:30,100 --> 00:16:34,330
could of course over you on dates and we

00:16:32,440 --> 00:16:35,830
can say ok you know we're gonna break

00:16:34,330 --> 00:16:37,540
the machines right here prohibit AI

00:16:35,830 --> 00:16:40,000
we're going to say okay we don't want to

00:16:37,540 --> 00:16:43,150
move forward but honestly like selfishly

00:16:40,000 --> 00:16:45,070
speaking it's a bad idea so resistance

00:16:43,150 --> 00:16:47,500
is a bad idea for multiple reasons so I

00:16:45,070 --> 00:16:49,540
could bring up a lot of different things

00:16:47,500 --> 00:16:50,980
here a lot of different problems but I'm

00:16:49,540 --> 00:16:53,650
going to talk about the pensions problem

00:16:50,980 --> 00:16:55,450
which is in Israel you still probably

00:16:53,650 --> 00:16:57,640
don't have that because the rate of

00:16:55,450 --> 00:17:00,070
population growth is pretty high but in

00:16:57,640 --> 00:17:04,060
many countries like in Western Europe

00:17:00,070 --> 00:17:06,040
and in the u.s. is really dropping so a

00:17:04,060 --> 00:17:08,800
lot of people aren't concerned but how

00:17:06,040 --> 00:17:11,620
are we gonna feel feed like the aging

00:17:08,800 --> 00:17:14,050
population right so when there's not

00:17:11,620 --> 00:17:15,640
enough young kids working how are we

00:17:14,050 --> 00:17:16,930
going to feed the ancient population and

00:17:15,640 --> 00:17:18,970
a lot of people think it's about money

00:17:16,930 --> 00:17:21,100
right it's so it's like okay if we save

00:17:18,970 --> 00:17:23,320
more money we can feed more people but

00:17:21,100 --> 00:17:25,300
in actuality that's not the only problem

00:17:23,320 --> 00:17:29,080
with it so I'm just going to go through

00:17:25,300 --> 00:17:30,790
this let's say I'm inserted island and I

00:17:29,080 --> 00:17:33,310
need to feed the families so I'm like

00:17:30,790 --> 00:17:35,260
okay I'm guessing apples now ok confute

00:17:33,310 --> 00:17:37,210
family if you want one person awesome

00:17:35,260 --> 00:17:40,360
now I can I ran out of the resources

00:17:37,210 --> 00:17:42,940
right so going ok maybe there's another

00:17:40,360 --> 00:17:45,010
person on this island and you can also

00:17:42,940 --> 00:17:46,930
gather the apples that's great I'm gonna

00:17:45,010 --> 00:17:49,930
pay him some sort of currency that he

00:17:46,930 --> 00:17:51,670
values whatever this and then he's gonna

00:17:49,930 --> 00:17:54,250
give me the elbows okay I can feed two

00:17:51,670 --> 00:17:56,410
more people now awesome right we can ran

00:17:54,250 --> 00:17:59,530
out of everything all right we're not

00:17:56,410 --> 00:18:01,900
employing technology so I can probably

00:17:59,530 --> 00:18:03,820
produce more resources he can also

00:18:01,900 --> 00:18:06,760
produce more resources okay we couldn't

00:18:03,820 --> 00:18:09,280
want people now no matter how much

00:18:06,760 --> 00:18:11,680
currency you have like I could have

00:18:09,280 --> 00:18:14,080
status of currency and I'd have centers

00:18:11,680 --> 00:18:16,630
and machines we only have two people to

00:18:14,080 --> 00:18:18,850
work the fields right so we can't do it

00:18:16,630 --> 00:18:21,190
anymore we can't feed so right now okay

00:18:18,850 --> 00:18:22,630
we're wrong here because these people

00:18:21,190 --> 00:18:24,220
are going to start because there's

00:18:22,630 --> 00:18:27,070
nobody to work we can't work more than

00:18:24,220 --> 00:18:29,560
24 hours who's we to sleep right so the

00:18:27,070 --> 00:18:32,200
only so all the best socially relevant

00:18:29,560 --> 00:18:34,060
at this moment right now probably in

00:18:32,200 --> 00:18:36,310
student reduce technology they can work

00:18:34,060 --> 00:18:38,620
better that needs last term in to power

00:18:36,310 --> 00:18:40,330
ed right so we're making actually feed

00:18:38,620 --> 00:18:44,260
page and population or whatever sick

00:18:40,330 --> 00:18:46,180
population whether it is kids this is

00:18:44,260 --> 00:18:48,280
not the only problem we can face right

00:18:46,180 --> 00:18:49,280
so this is a quote if we don't provide

00:18:48,280 --> 00:18:51,320
technology the

00:18:49,280 --> 00:18:54,650
question isn't whether if you mentor

00:18:51,320 --> 00:18:56,960
will go extinct but nearly how this is

00:18:54,650 --> 00:18:59,360
an opinion by Mike's max tegmark but I

00:18:56,960 --> 00:19:01,700
happen to agree with it because in the

00:18:59,360 --> 00:19:04,400
long run if we take like a millennia we

00:19:01,700 --> 00:19:06,200
are gonna face master and Silas is gonna

00:19:04,400 --> 00:19:07,940
run out of power like we're gonna kill

00:19:06,200 --> 00:19:09,890
ourselves with a nuclear war whatever it

00:19:07,940 --> 00:19:12,620
happens right we can face a lot of

00:19:09,890 --> 00:19:15,020
problems global warming um right that

00:19:12,620 --> 00:19:16,910
can kill of humanity potentially if we

00:19:15,020 --> 00:19:18,590
don't improve our technology so not

00:19:16,910 --> 00:19:20,210
improving technology is probably a bad

00:19:18,590 --> 00:19:22,790
idea in the long run may be a good idea

00:19:20,210 --> 00:19:26,300
in 100 years but probably not beyond

00:19:22,790 --> 00:19:29,150
that right so and if we look at opinions

00:19:26,300 --> 00:19:30,710
of what's going to happen so there are

00:19:29,150 --> 00:19:32,750
people who say that superhuman

00:19:30,710 --> 00:19:35,960
artificial general intelligence not

00:19:32,750 --> 00:19:37,640
possible right there's a camp there's

00:19:35,960 --> 00:19:40,130
people who say that it will be achieved

00:19:37,640 --> 00:19:41,000
like tomorrow seriously like in the next

00:19:40,130 --> 00:19:43,850
10 20 years

00:19:41,000 --> 00:19:45,830
and that's another cat that's you know

00:19:43,850 --> 00:19:48,170
and there's a lot of people in there I

00:19:45,830 --> 00:19:50,540
mean like researchers and scientists and

00:19:48,170 --> 00:19:52,220
then a lot of people think that we only

00:19:50,540 --> 00:19:54,800
need to worry about machines they're

00:19:52,220 --> 00:19:56,750
conscious and that you know Robin that

00:19:54,800 --> 00:19:59,450
wants to kill you or something or like

00:19:56,750 --> 00:20:04,000
lock you up or whatever the problem is

00:19:59,450 --> 00:20:07,970
that goal isn't equal purpose so Janice

00:20:04,000 --> 00:20:10,370
heat-seeking missile um it probably

00:20:07,970 --> 00:20:11,810
doesn't personally hate you right and it

00:20:10,370 --> 00:20:13,310
doesn't have any kind of general

00:20:11,810 --> 00:20:14,750
intelligence it doesn't have any

00:20:13,310 --> 00:20:17,150
understanding and it doesn't have a

00:20:14,750 --> 00:20:19,910
purpose it has a goal if you happen to

00:20:17,150 --> 00:20:21,500
meet the goal you have a problem it's

00:20:19,910 --> 00:20:24,740
not gonna help you that it doesn't care

00:20:21,500 --> 00:20:27,920
about you right so machines don't have

00:20:24,740 --> 00:20:30,200
to care about you or physically want to

00:20:27,920 --> 00:20:32,420
or actually want to harm you to cause

00:20:30,200 --> 00:20:34,220
you problems if they just have to not

00:20:32,420 --> 00:20:36,140
care that they are causing your problems

00:20:34,220 --> 00:20:38,960
while they're achieving the goal that

00:20:36,140 --> 00:20:40,670
they're set to achieve which is why it's

00:20:38,960 --> 00:20:42,620
really important when we build the eyes

00:20:40,670 --> 00:20:45,110
and we're all in this field right we are

00:20:42,620 --> 00:20:46,840
actively helping people develop a eyes

00:20:45,110 --> 00:20:48,700
whether you work on a

00:20:46,840 --> 00:20:51,190
now or not you probably implemented

00:20:48,700 --> 00:20:52,660
something that has the Mallards maybe

00:20:51,190 --> 00:20:54,640
it's linear regression maybe it's deep

00:20:52,660 --> 00:20:56,260
learning whatever it is you're probably

00:20:54,640 --> 00:20:58,810
helping the industry of build the next

00:20:56,260 --> 00:21:01,120
AI okay this is why it's really

00:20:58,810 --> 00:21:02,650
important to consider ethics when you're

00:21:01,120 --> 00:21:05,950
actually building artificial

00:21:02,650 --> 00:21:08,500
intelligence so this was a real

00:21:05,950 --> 00:21:11,170
revelation for me actually we've talked

00:21:08,500 --> 00:21:13,240
a lot about how humans are biased and

00:21:11,170 --> 00:21:15,460
like we hate different races we hate

00:21:13,240 --> 00:21:17,380
different genders we discriminate

00:21:15,460 --> 00:21:18,940
against people whatever it is right we

00:21:17,380 --> 00:21:22,750
assume something without knowing

00:21:18,940 --> 00:21:24,220
something anything about the subject but

00:21:22,750 --> 00:21:26,430
bias is actually not a property of

00:21:24,220 --> 00:21:29,230
humans it's a property of information

00:21:26,430 --> 00:21:31,180
right what does this mean there's too

00:21:29,230 --> 00:21:33,100
much information in the world right

00:21:31,180 --> 00:21:35,260
I can't treat every single human

00:21:33,100 --> 00:21:37,330
differently I can't treat every single

00:21:35,260 --> 00:21:41,380
case differently I can't get out of the

00:21:37,330 --> 00:21:43,210
house and decide every day on everything

00:21:41,380 --> 00:21:45,490
I want to do right what am I going to

00:21:43,210 --> 00:21:47,590
wear wait how we're going to eat if I

00:21:45,490 --> 00:21:49,450
make all these decisions since one offs

00:21:47,590 --> 00:21:53,050
I would be considering the decision

00:21:49,450 --> 00:21:54,580
forever right so I have to jail eyes and

00:21:53,050 --> 00:21:56,770
stereotype and then I have to have

00:21:54,580 --> 00:21:59,920
you're a six they lead me to a certain

00:21:56,770 --> 00:22:01,840
path of action so this happens to humans

00:21:59,920 --> 00:22:04,960
it also happens to artificial

00:22:01,840 --> 00:22:07,270
intelligence so in fact I'm not gonna go

00:22:04,960 --> 00:22:11,980
through all these sample of all these

00:22:07,270 --> 00:22:13,960
examples in like super detail but we're

00:22:11,980 --> 00:22:16,030
already facing some of these and these

00:22:13,960 --> 00:22:17,470
are again natural intelligence it's not

00:22:16,030 --> 00:22:19,930
algorithms that are writing people's

00:22:17,470 --> 00:22:22,330
lives so we have facial recognition

00:22:19,930 --> 00:22:24,430
software that's being used for law a law

00:22:22,330 --> 00:22:26,410
enforcement and there's been identified

00:22:24,430 --> 00:22:28,570
race and gender bias and that in that

00:22:26,410 --> 00:22:31,330
artificial intelligence it's being used

00:22:28,570 --> 00:22:35,890
by police in the United States another

00:22:31,330 --> 00:22:39,520
thing is like in 2015 so Google ran like

00:22:35,890 --> 00:22:41,950
an N then wanted to hire new CEOs Bank

00:22:39,520 --> 00:22:45,760
and so it was displayed almost

00:22:41,950 --> 00:22:48,940
exclusively to men why because CEOs are

00:22:45,760 --> 00:22:51,100
almost exclusively men so it ran the you

00:22:48,940 --> 00:22:52,530
know on existing data and it said hey

00:22:51,100 --> 00:22:54,780
okay I know

00:22:52,530 --> 00:22:57,830
seola's like and that's whom couldn't

00:22:54,780 --> 00:23:00,540
talk to you when i display this ad so

00:22:57,830 --> 00:23:03,840
then another example is like so we did

00:23:00,540 --> 00:23:07,080
yeah we did tied Microsoft released our

00:23:03,840 --> 00:23:09,810
Twitter and she's been trolled

00:23:07,080 --> 00:23:12,840
effectively in so she became a racing I

00:23:09,810 --> 00:23:15,450
don't know all sorts of bad things and

00:23:12,840 --> 00:23:20,670
 in a span of 16 hours and she had

00:23:15,450 --> 00:23:23,130
to be shut down and then yeah so there

00:23:20,670 --> 00:23:24,650
was a pipe all this again used by US law

00:23:23,130 --> 00:23:27,090
enforcement is great

00:23:24,650 --> 00:23:30,590
this is offer predicted how likely

00:23:27,090 --> 00:23:33,270
people are to commit crimes and then

00:23:30,590 --> 00:23:35,280
it's been noticed that it points the

00:23:33,270 --> 00:23:37,470
police effectively to the same

00:23:35,280 --> 00:23:39,390
neighborhoods over and over and over

00:23:37,470 --> 00:23:41,280
again whether or not crimes happen there

00:23:39,390 --> 00:23:43,220
because it looks at previous data and

00:23:41,280 --> 00:23:47,900
goes like ah this is a bad neighborhood

00:23:43,220 --> 00:23:50,910
awesome so and then okay compass is a

00:23:47,900 --> 00:23:52,740
again this was predicting it's this is

00:23:50,910 --> 00:23:55,230
again this is running our lives right

00:23:52,740 --> 00:23:56,760
today and people don't think about it

00:23:55,230 --> 00:23:59,970
when they implemented in the beginning

00:23:56,760 --> 00:24:02,000
so this they predicted whether or not a

00:23:59,970 --> 00:24:04,740
person is likely to commit another crime

00:24:02,000 --> 00:24:08,460
so when when it judge looked at the

00:24:04,740 --> 00:24:10,260
person because there was a software

00:24:08,460 --> 00:24:12,510
algorithm that said okay this person is

00:24:10,260 --> 00:24:13,920
sixty percent likely likely heard of

00:24:12,510 --> 00:24:14,340
committing another crime in the next

00:24:13,920 --> 00:24:16,920
year

00:24:14,340 --> 00:24:20,220
guess what it was highly correlated with

00:24:16,920 --> 00:24:22,080
race no surprise there right I'm and

00:24:20,220 --> 00:24:25,230
then okay what's my last one it's a

00:24:22,080 --> 00:24:27,210
version sighs okay I don't know I'm

00:24:25,230 --> 00:24:32,280
gonna get rid this anyway oh yeah it was

00:24:27,210 --> 00:24:35,940
something about the Facebook giving up

00:24:32,280 --> 00:24:38,370
another so the point is this is already

00:24:35,940 --> 00:24:40,680
happening the algorithm that decides if

00:24:38,370 --> 00:24:43,710
you're going to have a mortgage is run

00:24:40,680 --> 00:24:45,330
by mmm it's run by all-male during like

00:24:43,710 --> 00:24:46,650
maybe when your aggression or maybe

00:24:45,330 --> 00:24:49,530
something a little bit smarter than that

00:24:46,650 --> 00:24:51,660
and it decides based on where are you

00:24:49,530 --> 00:24:53,639
from what gender you are it like what

00:24:51,660 --> 00:24:56,429
ensure you're working and if

00:24:53,639 --> 00:24:58,349
has bias against you you probably want

00:24:56,429 --> 00:25:01,679
to avoid it and you don't want to keep

00:24:58,349 --> 00:25:05,159
using these other lose truly lights they

00:25:01,679 --> 00:25:08,249
decide things that can happen to you so

00:25:05,159 --> 00:25:11,579
basically the point of that was build a

00:25:08,249 --> 00:25:12,929
is responsibly okay and this is my

00:25:11,579 --> 00:25:16,909
lights light and that's a problem

00:25:12,929 --> 00:25:16,909
because we have a lot of time left so

00:25:18,889 --> 00:25:23,099
resources I'm just going to match her so

00:25:20,909 --> 00:25:26,279
if you do nothing else to read this book

00:25:23,099 --> 00:25:27,959
its max tegmark and it's being human in

00:25:26,279 --> 00:25:32,609
the age of artificial intelligence light

00:25:27,959 --> 00:25:33,929
3.0 so he talks about album artificial

00:25:32,609 --> 00:25:35,639
intelligence super humanity fish

00:25:33,929 --> 00:25:37,169
montages where do we go from here and

00:25:35,639 --> 00:25:41,999
different scenarios of what's going to

00:25:37,169 --> 00:25:43,799
happen none of them are good so and then

00:25:41,999 --> 00:25:46,200
there's a lot of other different people

00:25:43,799 --> 00:25:48,539
that you know have some useful reading

00:25:46,200 --> 00:25:56,999
on the subject so now I have a lot of

00:25:48,539 --> 00:25:58,889
time for questions yes so it's like

00:25:56,999 --> 00:26:02,759
something that's I've been reading about

00:25:58,889 --> 00:26:04,859
myself there's two moral machines by MIT

00:26:02,759 --> 00:26:05,700
that's helping the self-driving cars and

00:26:04,859 --> 00:26:07,709
things like that

00:26:05,700 --> 00:26:09,690
make moral decisions because this

00:26:07,709 --> 00:26:10,799
machine it is a little bit more

00:26:09,690 --> 00:26:13,320
perspective on that because it's

00:26:10,799 --> 00:26:15,749
something that I'm very very yes so

00:26:13,320 --> 00:26:17,820
building moral machines it's a problem

00:26:15,749 --> 00:26:19,499
because defining morality is a problem

00:26:17,820 --> 00:26:21,479
right because people in different

00:26:19,499 --> 00:26:23,190
countries and different populations have

00:26:21,479 --> 00:26:29,399
different perceptions and morality so

00:26:23,190 --> 00:26:31,679
one example from Kaplan boys say like if

00:26:29,399 --> 00:26:34,529
a person from know it I think this

00:26:31,679 --> 00:26:37,679
remote postural anyway if a person from

00:26:34,529 --> 00:26:39,869
Taliban creates a superhuman

00:26:37,679 --> 00:26:42,450
intelligence can be a very different

00:26:39,869 --> 00:26:45,659
goals than the persons around them say

00:26:42,450 --> 00:26:47,219
Switzerland right the you took it the

00:26:45,659 --> 00:26:49,440
best-case scenario that these people

00:26:47,219 --> 00:26:51,659
perceive is completely different so when

00:26:49,440 --> 00:26:53,039
we define morality one of the problems

00:26:51,659 --> 00:26:54,929
is that morality is different to

00:26:53,039 --> 00:26:58,529
different people the other problem is

00:26:54,929 --> 00:27:00,629
that defining morality is difficult you

00:26:58,529 --> 00:27:02,339
can't create a decision tree and say

00:27:00,629 --> 00:27:03,559
okay if this then that

00:27:02,339 --> 00:27:05,929
right because you have

00:27:03,559 --> 00:27:08,960
too many options here so like in the

00:27:05,929 --> 00:27:11,269
case of South Darby car in fact if it

00:27:08,960 --> 00:27:12,559
knows so you're driving you get into an

00:27:11,269 --> 00:27:14,659
accident and you have two kids in the

00:27:12,559 --> 00:27:16,399
car and it knows a turns to the left

00:27:14,659 --> 00:27:18,350
it's gonna kill one kid and it's just an

00:27:16,399 --> 00:27:20,360
array it's gonna kill them that okay was

00:27:18,350 --> 00:27:22,100
supposed to do and literally you know

00:27:20,360 --> 00:27:23,659
sad because it ran the simulation it

00:27:22,100 --> 00:27:27,950
knows that it's gonna happen so that's

00:27:23,659 --> 00:27:34,190
not easy answers to answer eight easy

00:27:27,950 --> 00:27:35,809
questions yes so it is quantum computing

00:27:34,190 --> 00:27:37,970
mature enough to be considering

00:27:35,809 --> 00:27:40,309
something viable I don't think there's

00:27:37,970 --> 00:27:42,980
right now one of computing that's close

00:27:40,309 --> 00:27:44,629
enough to being implemented because they

00:27:42,980 --> 00:27:46,850
have a lot of restrictions on what they

00:27:44,629 --> 00:27:48,799
can do right now but the thing is like

00:27:46,850 --> 00:27:50,899
we we could potentially have a

00:27:48,799 --> 00:27:53,690
breakthrough that makes things a lot

00:27:50,899 --> 00:27:55,909
faster that nobody anticipates and one

00:27:53,690 --> 00:27:58,039
of the scariest is this is that if it

00:27:55,909 --> 00:28:00,019
happens really quickly we have no time

00:27:58,039 --> 00:28:02,539
to adapt to it right it's essentially

00:28:00,019 --> 00:28:03,649
and Islanders like ok nuclear war just

00:28:02,539 --> 00:28:05,419
happened what do we do now

00:28:03,649 --> 00:28:08,119
right so if you suddenly have a

00:28:05,419 --> 00:28:10,970
superhuman intelligence said can decide

00:28:08,119 --> 00:28:12,110
everything in the world like and you

00:28:10,970 --> 00:28:13,789
don't know what goal is it was

00:28:12,110 --> 00:28:16,490
improvement with you have a huge problem

00:28:13,789 --> 00:28:21,769
a make sure that you do have no problem

00:28:16,490 --> 00:28:25,610
we can always unplug no sir yes and then

00:28:21,769 --> 00:28:31,039
get into matrix or whatever scenario you

00:28:25,610 --> 00:28:34,450
can't always a sluggish I don't have a

00:28:31,039 --> 00:28:34,450
question second generation of

00:28:34,950 --> 00:28:39,510
so we have all these families developing

00:28:37,650 --> 00:28:41,190
these technologies and they are taking

00:28:39,510 --> 00:28:42,840
our lives and they should actually

00:28:41,190 --> 00:28:45,660
affect the gosset South America agenda

00:28:42,840 --> 00:28:54,390
chose after the car take the decision to

00:28:45,660 --> 00:28:56,760
kill the key to the left yeah so I think

00:28:54,390 --> 00:29:00,420
it's not where it needs to be so there

00:28:56,760 --> 00:29:02,220
say I'm like an ethically I initiated

00:29:00,420 --> 00:29:04,200
oaf and no letter that a bunch of

00:29:02,220 --> 00:29:07,550
research or science and they you know

00:29:04,200 --> 00:29:10,680
there's not work on trying to create foi

00:29:07,550 --> 00:29:13,410
but you know if you watch congress

00:29:10,680 --> 00:29:14,550
questioning big firms you understand

00:29:13,410 --> 00:29:17,550
that they don't understand what

00:29:14,550 --> 00:29:19,440
technology does necessarily so it's

00:29:17,550 --> 00:29:22,230
ineffective for them to create laws

00:29:19,440 --> 00:29:24,000
around this so i don't think it's where

00:29:22,230 --> 00:29:26,640
it needs to be I think we have a problem

00:29:24,000 --> 00:29:29,190
with creating a scola law so around AI

00:29:26,640 --> 00:29:34,950
implementations and it's also another

00:29:29,190 --> 00:29:36,330
point is you share in the risk of what's

00:29:34,950 --> 00:29:38,760
happening but you don't necessarily

00:29:36,330 --> 00:29:41,520
share in the profits of what's happening

00:29:38,760 --> 00:29:43,350
so if Google implements now very who

00:29:41,520 --> 00:29:46,290
they can profit from it you don't profit

00:29:43,350 --> 00:29:48,930
from it necessarily but if it has a bad

00:29:46,290 --> 00:29:50,430
outcome to your like potentially deadly

00:29:48,930 --> 00:29:56,430
outcome would have a better outcome in

00:29:50,430 --> 00:29:58,770
your life as well right I think so the

00:29:56,430 --> 00:30:00,270
question is how a company's solving the

00:29:58,770 --> 00:30:02,550
liability issue in the legal issue

00:30:00,270 --> 00:30:04,950
around a eyes that can potentially have

00:30:02,550 --> 00:30:06,150
a dangerous impact and specifically in

00:30:04,950 --> 00:30:07,890
the case of self-driving cars

00:30:06,150 --> 00:30:09,690
I think there's answers to that and

00:30:07,890 --> 00:30:11,100
that's probably the biggest reason we

00:30:09,690 --> 00:30:12,960
don't have self-driving cars driving

00:30:11,100 --> 00:30:14,790
everywhere because they're scared of

00:30:12,960 --> 00:30:18,120
potential impact of even a single

00:30:14,790 --> 00:30:20,250
accident happening like who's liable and

00:30:18,120 --> 00:30:22,740
and what's gonna happen like if the

00:30:20,250 --> 00:30:25,380
perception of people is that you know

00:30:22,740 --> 00:30:27,900
self-driving car is not safe so stuff

00:30:25,380 --> 00:30:30,630
like that and again it's all very new

00:30:27,900 --> 00:30:33,330
and the problem is our legal system is

00:30:30,630 --> 00:30:35,220
to reacting to things that happened over

00:30:33,330 --> 00:30:36,960
decades the changes that happen over

00:30:35,220 --> 00:30:39,140
decades and we essentially talking about

00:30:36,960 --> 00:30:46,230
changes that can happen in a year

00:30:39,140 --> 00:30:48,540
be very afraid don't wait but I'll see

00:30:46,230 --> 00:30:50,640
it like my I think my biggest point is

00:30:48,540 --> 00:30:52,440
like first of all don't don't think that

00:30:50,640 --> 00:30:55,200
AI is not coming for you to talk because

00:30:52,440 --> 00:30:56,700
I think it is eventually and second when

00:30:55,200 --> 00:30:59,760
you're building AI ask yourself a

00:30:56,700 --> 00:31:01,470
question how can be like bias against

00:30:59,760 --> 00:31:03,780
you or how you negatively impact

00:31:01,470 --> 00:31:06,000
people's lives because people build this

00:31:03,780 --> 00:31:07,470
thing that's you know easiest to build

00:31:06,000 --> 00:31:09,120
oh okay I'm playing with us in the

00:31:07,470 --> 00:31:11,370
algorithm and the student predictions is

00:31:09,120 --> 00:31:15,920
great but we have to think about how

00:31:11,370 --> 00:31:15,920
we're approaching a bigger problem here

00:31:16,490 --> 00:31:26,150
[Applause]

00:31:21,020 --> 00:31:26,150

YouTube URL: https://www.youtube.com/watch?v=It3QUin5K1o


