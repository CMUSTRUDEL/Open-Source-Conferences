Title: DevOpsDays Boston 2019 - Seeing RED by Dave McAllister
Publication date: 2019-10-11
Playlist: DevOpsDays Boston 2019
Description: 
	Seeing RED: Why multi-dimensional monitoring matters by Dave McAllister

RED (Rates, Errors, Duration) was a spinoff from Google’s Golden Signals designed for monitoring microservices. However, RED use has clearly demonstrated that the applicability is applicable to any services-based architecture.

With RED, unlike the modern belief in observability, your architecture is watched from aspects of multiple dimensions. You receive alerts and indications not just from anomalies, but also from headache alerts. By seeing multiple dimensions of concerns, be they failures in service or activity to close to the edge of capability, these combined monitors and deep-dive, focused access get you to your root cause faster, with less false positives and quicker resolution.

Duration speaks of time. In particular, time to send and receive a specific request. And both client-side and server-side are important in meeting your customers’ desires.

Often this is relegated to distributed request tracing. And while each individual trace is important, so is the sum of the latency experienced across the entire environment. A High Latency Trigger(HLT) in one spot can cause a cascading HLT across the entire system.

In today’s instant gratification world, slow response is bad. So you need to identify not only the headache (alert) but identify and narrow to the culprit as fast as possible.

Errors are just what it says, problems that cause an incorrect, incomplete or unexpected result. They are failures in the code, bugs that appear under production loads (or under peak loads). Errors require rapid response. They require point specific responses. And while errors can be measured as a metric, you will need to dive deep into the structure and get to your root cause as soon as possible. If your business is dependent on the system that failed, ASAP might not even be fast enough.

And BTW, errors can also result in signal anomalies in rate and duration.

Not responding to errors rapidly and specifically result in lost business, unhappy users and stressed DevOps/SRE teams.

Rate, or traffic, can be considered as the number of requests on the network and system. This is the communication pipeline for your apps and your enemy is peak load. While you could think of this as bandwidth, to quote John Mashey, “Bandwidth costs money”. Are you hitting peak loads that exceed your bandwidth?

Rate includes lots of items. High-level ones are like HTTP, SOAP, REST. But keep in mind that it’s not just the web, it’s things like middleware messaging/queuing, API calls. It can also be overhead of control structures like service meshes. Any environment that fails on peak traffic is a suspect for Rate monitoring

With rate breaches, you need to move quickly from the metric measurements to a specific cause, be they in your own services or external factors. And since loads may be temporal, you need to be able to dig into the potential causes after the fact. Just a metric isn’t enough, you’ll need detailed info as well.

With RED, while there can be many alerts, it follows that there are two major categories that apply broadly, anomaly alerts and headache alerts.

Rate is most often an anomaly alert. You’re alerting on something that has crossed a threshold, usually a percentage value. The risk here is that the alert doesn’t get processed in real time. Duration and metrics are most accurately retrieved via logs (after all, a distributed trace is really just a specialized log entry).

As a caveat, anomaly alerts are new, and often do not consider seasonality. They require aggregation, especially when going for the root cause. This is where visualization for humans is vital, as we are still pretty unmatched at pattern extrapolation.

Also, keep in mind that too low may be as indicative as too high. Low saturation at normal high volume times is a definite flag.

Duration crosses the streams, being both an anomaly alert and a headache alert. It’s worthwhile to keep an eye on the total duration/latency of a request as well as on crucial services. While it may be difficult, being able to see duration from the client through the app environment is incredibly useful.

Errors and to a point duration are both headache alerts. Something needs fixing NOW, so get it done. Here you need the alert as fast as possible. You need the information that shows where it occurred, when it occurred and what was going on. Again, log aggregation and threaded displays are vital; sometimes the thing that caused a failure isn’t the thing that failed.

So RED gives you the framework to build alerting, monitoring and analysis into a flexible structure to meet the emerging needs of services-based architectures as well as give you the capability to grow as your environment scales.

Come learn how to make RED something you want to see every day.

#DevOpsDays #DevOpsDaysBoston
Captions: 
	00:00:01,480 --> 00:00:22,630
Oh, so, first of all, good afternoon and thanks for letting me come here and talk to you.

00:00:22,630 --> 00:00:26,960
The quick thing is first of all, this is the perfect theater stage for me because this

00:00:26,960 --> 00:00:29,119
is one third of my personal library.

00:00:29,119 --> 00:00:30,390
It's about 2500 books.

00:00:30,390 --> 00:00:34,149
I feel perfectly aligned with this here.

00:00:34,149 --> 00:00:35,280
I'm out of California.

00:00:35,280 --> 00:00:37,410
I'm owned by three cats.

00:00:37,410 --> 00:00:39,030
[ Laughter ] And a wife.

00:00:39,030 --> 00:00:40,030
Yes.

00:00:40,030 --> 00:00:41,030
In this of that.

00:00:41,030 --> 00:00:45,140
I have been open source before, was open source starting with Linux.

00:00:45,140 --> 00:00:49,809
And two years ago was named as one of the top pioneers in open source space.

00:00:49,809 --> 00:00:55,710
And what I'm going to do a little bit is talk about Red.

00:00:55,710 --> 00:01:00,359
Red is a concept that talks about you can approach looking at your monitoring structures

00:01:00,359 --> 00:01:02,859
and why you need to go deeper inside of here.

00:01:02,859 --> 00:01:07,550
But I want to start with a simple quote out of Sherlock Holmes, of course, you see, but

00:01:07,550 --> 00:01:09,070
you do not observe.

00:01:09,070 --> 00:01:13,800
If you're only looking at one level or one dimension, you're missing the entire picture

00:01:13,800 --> 00:01:19,380
inside of here and missing the ability to dye deep and look at the fidelity inside that

00:01:19,380 --> 00:01:20,380
have.

00:01:20,380 --> 00:01:23,930
So, to talk about this, we need to talk a bit about observability.

00:01:23,930 --> 00:01:29,330
And start off with the fact that observability is a signal to noise problem.

00:01:29,330 --> 00:01:30,580
But it's not.

00:01:30,580 --> 00:01:34,330
It's treated as a signal not noise problem.

00:01:34,330 --> 00:01:37,580
And there are basically two ways of reducing noise.

00:01:37,580 --> 00:01:39,560
The first one is very simple.

00:01:39,560 --> 00:01:41,780
Don't send as many signals into it.

00:01:41,780 --> 00:01:45,580
And that will give you very clean signal that is coming in.

00:01:45,580 --> 00:01:48,640
But it doesn't let you have the rest of the information inside of here.

00:01:48,640 --> 00:01:53,590
The other one is, bring all the signals in and filter around the backend.

00:01:53,590 --> 00:01:58,760
And that will help you clarify the signal while still allowing you to get the data that's

00:01:58,760 --> 00:01:59,860
inside of here.

00:01:59,860 --> 00:02:04,220
There's a fairly famous encryption scheme where they would send messages and in the

00:02:04,220 --> 00:02:07,310
front of the message, there's a little bleep!

00:02:07,310 --> 00:02:11,959
And all the bleep was, sounds like noise to everybody else, it was the actually encoded

00:02:11,959 --> 00:02:12,959
message.

00:02:12,959 --> 00:02:18,140
So, if you didn't save that piece, you had no chance of ever decoding what was going

00:02:18,140 --> 00:02:19,550
on inside of that.

00:02:19,550 --> 00:02:22,230
So, it's a signal, not noise problem.

00:02:22,230 --> 00:02:25,030
Not a signal to noise problem.

00:02:25,030 --> 00:02:28,379
observability, talked about a lot these days.

00:02:28,379 --> 00:02:34,260
But really comes down to what is your system doing based on what you can see.

00:02:34,260 --> 00:02:38,069
And it allows you to infer the internal behaviors.

00:02:38,069 --> 00:02:42,620
You can't actually know what's happening inside of that at the same point in time.

00:02:42,620 --> 00:02:47,890
But the thing you have to remember is observability is design that lets you answer an unknown

00:02:47,890 --> 00:02:48,890
question.

00:02:48,890 --> 00:02:50,400
What's known as the unknown unknowns.

00:02:50,400 --> 00:02:54,620
The thing use need to know long before you ever realize you need to know them.

00:02:54,620 --> 00:02:58,150
And, it consists of a lot of places.

00:02:58,150 --> 00:03:04,280
There's logs, monitoring and tracing.

00:03:04,280 --> 00:03:11,799
And it has to pass service bound arise, Kubernetes, for instance, adds four layers of infrastructure

00:03:11,799 --> 00:03:13,269
to your environment.

00:03:13,269 --> 00:03:15,980
You need to get all that data in one place inside here.

00:03:15,980 --> 00:03:19,040
And honestly, anything that slows you down is bald.

00:03:19,040 --> 00:03:21,209
Sorry, we think really fast.

00:03:21,209 --> 00:03:24,569
If you want to wait to figure out what you're going to think about next, it's not a good

00:03:24,569 --> 00:03:25,569
concept.

00:03:25,569 --> 00:03:28,400
So, it's not just monitoring.

00:03:28,400 --> 00:03:29,650
I'm going to talk about monitoring.

00:03:29,650 --> 00:03:32,079
But observability is more than just monitoring.

00:03:32,079 --> 00:03:36,700
The problem with observability is we haven't set up what we need to observe.

00:03:36,700 --> 00:03:39,489
And there are lots of potential signals that come into play here.

00:03:39,489 --> 00:03:44,549
The big three, logs, traces, metrics inside of here.

00:03:44,549 --> 00:03:46,170
But there can be things like stack traces.

00:03:46,170 --> 00:03:47,170
There can be core dumps.

00:03:47,170 --> 00:03:48,989
There can be all sorts of stuff.

00:03:48,989 --> 00:03:56,520
We have a customer with believe it or not who feeds their Twitter feed into it because

00:03:56,520 --> 00:04:03,230
their customers always know something is going wrong long before it ever goes wrong.

00:04:03,230 --> 00:04:07,760
And they actually watch for customers going, is something wrong?

00:04:07,760 --> 00:04:12,440
And that triggers them it triggers an alert and they have to go out and find what's happening.

00:04:12,440 --> 00:04:13,810
Signals are every static.

00:04:13,810 --> 00:04:17,230
So, they're constantly moving and they're constantly changing.

00:04:17,230 --> 00:04:25,240
So, you'll also hear a lot about observability is around events.

00:04:25,240 --> 00:04:26,470
Everything is an event.

00:04:26,470 --> 00:04:30,390
For those of you who are unfortunate enough earlier on, you got it hear me do Hamlet's

00:04:30,390 --> 00:04:38,690
soliloquy talk about poetry, and the rest of y'all, it never happened.

00:04:38,690 --> 00:04:42,130
Events are only important if they are observed and recorded.

00:04:42,130 --> 00:04:45,200
And so, you have to be able to do that.

00:04:45,200 --> 00:04:46,200
Metrics are great.

00:04:46,200 --> 00:04:47,360
They're compact, they're efficient.

00:04:47,360 --> 00:04:51,570
And by gosh, we as people can pattern match with the best of them.

00:04:51,570 --> 00:04:53,640
We extrapolate.

00:04:53,640 --> 00:04:54,810
My degree is in statistics.

00:04:54,810 --> 00:04:56,800
I do lie for a living for this.

00:04:56,800 --> 00:04:57,800
For this.

00:04:57,800 --> 00:05:00,490
But nevertheless, metrics are great, extrapolation is bad.

00:05:00,490 --> 00:05:02,410
But we're good at this.

00:05:02,410 --> 00:05:03,850
Logs are full fidelity.

00:05:03,850 --> 00:05:06,160
They tell you everything that's going on.

00:05:06,160 --> 00:05:11,200
But they're bulky and they can be hard to deal with inside of here.

00:05:11,200 --> 00:05:14,490
Most of the places, you can hear things like I'm indexing things.

00:05:14,490 --> 00:05:20,130
If you don't index it, it's hard because we have so much volume in here.

00:05:20,130 --> 00:05:21,870
And traces are cool.

00:05:21,870 --> 00:05:28,760
They make logs of a little bitty storage system in the average trace in a microservices environment

00:05:28,760 --> 00:05:36,410
with, the average request can be produced an extra 50 to 100 additional spans.

00:05:36,410 --> 00:05:44,280
five or ten log issues is going to be closer to 25 to 300.

00:05:44,280 --> 00:05:46,520
Those are what we look into today.

00:05:46,520 --> 00:05:49,580
We are stepping into RED.

00:05:49,580 --> 00:05:55,180
RED is golden signals out of Google.

00:05:55,180 --> 00:05:57,940
The Google SRE handbook talks about this.

00:05:57,940 --> 00:06:07,800
And Tom Wilkie with WeaveWorks at that time comes up with the service oriented architectures.

00:06:07,800 --> 00:06:10,110
That's not the only limiting factor.

00:06:10,110 --> 00:06:14,680
It is great for anything that is request driven.

00:06:14,680 --> 00:06:19,800
Any services based or request driven architecture, RED is a great fit for.

00:06:19,800 --> 00:06:27,340
It does not work well for batch jobs or completely 100% streaming type jobs or monolithic architectures.

00:06:27,340 --> 00:06:31,490
It's really designed for how you keep track of things through the entire life cycle, entire

00:06:31,490 --> 00:06:34,370
process of what's going on.

00:06:34,370 --> 00:06:40,180
So, RED starts taking monitoring to a new level, what I call the Chuck Norris model

00:06:40,180 --> 00:06:41,180
inside of here.

00:06:41,180 --> 00:06:47,740
And it gives the duration of the functionality to recognize the issue as well as to recognize

00:06:47,740 --> 00:06:48,750
the root cause.

00:06:48,750 --> 00:06:53,360
You start looking at these things and we have metrics that fit into those, and we have the

00:06:53,360 --> 00:06:55,320
log capability inside of here.

00:06:55,320 --> 00:06:59,910
And traces across the boundaries inside of here.

00:06:59,910 --> 00:07:03,730
Likewise, you can find the problems like I was talking about with Twitter feeds.

00:07:03,730 --> 00:07:05,430
likewise in stack traces.

00:07:05,430 --> 00:07:12,300
All those different things come together and manage to make this part of the overall process.

00:07:12,300 --> 00:07:13,350
Sorry.

00:07:13,350 --> 00:07:18,590
A week ago I was in Sydney so my voice is kind of shot at this point in time for this.

00:07:18,590 --> 00:07:20,340
So, let's talk about rate.

00:07:20,340 --> 00:07:21,880
Rate is the first part of this.

00:07:21,880 --> 00:07:25,610
It's the size and number of requests going across your system.

00:07:25,610 --> 00:07:30,820
And rate is really great because it will give you an overall picture of what's going on

00:07:30,820 --> 00:07:36,250
while allowing you to disaggregate into what is happening with any specific piece of that

00:07:36,250 --> 00:07:37,250
rate structure.

00:07:37,250 --> 00:07:43,500
So, now I can look at not only do I have all these I have got hundreds of calls going through,

00:07:43,500 --> 00:07:49,230
but that can also start evolving down and say this specific call took this much time.

00:07:49,230 --> 00:07:51,690
Or this specific call generated this activity.

00:07:51,690 --> 00:07:56,830
And so, start looking at those various things inside of here and rate gives me those things.

00:07:56,830 --> 00:08:01,540
And it also provides a measurement for the overhead.

00:08:01,540 --> 00:08:02,990
If you're using a service mesh, for instance.

00:08:02,990 --> 00:08:09,940
There's an overhead involved in using Service Mesh Istio, Envoy, Linkerd, Concord, any of

00:08:09,940 --> 00:08:11,130
those pieces.

00:08:11,130 --> 00:08:20,560
Any environment that can fail when you hit peep is a good candidate for a rate oriented

00:08:20,560 --> 00:08:21,750
structure.

00:08:21,750 --> 00:08:27,510
So, if you were going to have something where the speed and performance matters, you need

00:08:27,510 --> 00:08:29,710
to keep track of rate.

00:08:29,710 --> 00:08:32,349
So, standard Google survey.

00:08:32,349 --> 00:08:36,060
5 seconds in eCommerce is a lost sale.

00:08:36,060 --> 00:08:41,689
10 seconds, someone who will go and complain to their friends about how slow you are.

00:08:41,689 --> 00:08:44,329
That's the type of environment we live in.

00:08:44,329 --> 00:08:49,640
People are expecting to see results come back in half a second, max.

00:08:49,640 --> 00:08:55,389
The other number they don't always quote is 3.2 seconds is 68% of all people will abandon

00:08:55,389 --> 00:08:56,660
a shopping cart.

00:08:56,660 --> 00:08:58,420
3.2 seconds.

00:08:58,420 --> 00:09:04,060
You need to look at the rate, but not only the rate piece, but the individual accesses.

00:09:04,060 --> 00:09:07,050
This is the bandwidth curve.

00:09:07,050 --> 00:09:11,579
The two curve pieces.

00:09:11,579 --> 00:09:18,970
This is, I think, my current summation and this is my maximum inside of here.

00:09:18,970 --> 00:09:21,040
But that doesn't tell me much.

00:09:21,040 --> 00:09:27,949
And so, I need to be able to do the next down, RED sets it up, it's metrics that are de aggregated

00:09:27,949 --> 00:09:29,139
here.

00:09:29,139 --> 00:09:35,470
What is causing the pieces and where is the data coming from and how to lack at the server

00:09:35,470 --> 00:09:42,769
level, the application level, even down to the path level, what is happening with my

00:09:42,769 --> 00:09:44,089
rate structures?

00:09:44,089 --> 00:09:49,149
And what it will do, for instance, it will identify interesting things.

00:09:49,149 --> 00:09:50,779
I can see which one.

00:09:50,779 --> 00:09:52,509
Probably don't have it here.

00:09:52,509 --> 00:09:55,160
My focus is probably a lot worse than y'alls.

00:09:55,160 --> 00:09:59,959
This one says image render, this one says image, and this one says hone.

00:09:59,959 --> 00:10:04,490
And I look at this, for instance, over here, image one, two, and image one.

00:10:04,490 --> 00:10:05,490
I would expect images.

00:10:05,490 --> 00:10:11,040
Why is the web server 6 so big?

00:10:11,040 --> 00:10:12,040
And that's a flag.

00:10:12,040 --> 00:10:16,620
That's a signal that is kind of given to you that something is going on.

00:10:16,620 --> 00:10:22,079
And if you continue down here, I didn't put them all in here, I have two web servers,

00:10:22,079 --> 00:10:26,779
or three web servers that are really big and I have three web servers that are really small.

00:10:26,779 --> 00:10:30,449
And that tells me that our load balancer has got problems.

00:10:30,449 --> 00:10:34,129
And generally speaking, it's a configuration problem at this point in time.

00:10:34,129 --> 00:10:39,009
So, RED can easily tell you what's happening at that level without you having to do a whole

00:10:39,009 --> 00:10:43,010
bunch of additional research or bring in other pieces.

00:10:43,010 --> 00:10:45,709
Starts from the top, drill into it.

00:10:45,709 --> 00:10:48,309
Errors, obvious.

00:10:48,309 --> 00:10:51,740
Incorrect, incomplete or unexpected results.

00:10:51,740 --> 00:10:53,440
Can be code violations.

00:10:53,440 --> 00:10:55,470
Can be production peak loads.

00:10:55,470 --> 00:10:57,129
It can be communication headaches.

00:10:57,129 --> 00:11:00,100
It can be lots of different pieces that it happen inside of here.

00:11:00,100 --> 00:11:03,110
Areas need rapid response.

00:11:03,110 --> 00:11:07,040
If things are broken, you got to fix it.

00:11:07,040 --> 00:11:10,230
Usually requires very point specific responses.

00:11:10,230 --> 00:11:16,300
It's not something that says, oh, gosh, the problem is in my app server cluster.

00:11:16,300 --> 00:11:19,259
My appear server cluster is seven app servers.

00:11:19,259 --> 00:11:20,769
It's not in my app server.

00:11:20,769 --> 00:11:24,829
It's where is it happening with, what's mapping and start digging into it.

00:11:24,829 --> 00:11:29,769
And we start looking at the logs.

00:11:29,769 --> 00:11:32,480
Start looking at the data inside of here.

00:11:32,480 --> 00:11:34,829
You need this deep dive fidelity.

00:11:34,829 --> 00:11:40,769
RED sets up the ability to look at this also from a complex viewpoint that says my error

00:11:40,769 --> 00:11:43,660
rate across my system is doing this.

00:11:43,660 --> 00:11:48,019
It compares it to the information about how things were working last week or last month

00:11:48,019 --> 00:11:49,940
or last hour inside of here.

00:11:49,940 --> 00:11:55,189
But, again, has that tame capability of driving it because, again, I can disaggregate stuff

00:11:55,189 --> 00:11:56,519
inside of this.

00:11:56,519 --> 00:12:01,189
And generally speaking, when it happens, you got to respond as fast as possible.

00:12:01,189 --> 00:12:06,149
This is what wakes people gets pagers going off in the middle of the night.

00:12:06,149 --> 00:12:11,200
My team, for instance, when we look at this, when an alert goes off for our system, the

00:12:11,200 --> 00:12:15,939
DevOps on call gets paged, the DevOps team gets a Slack notification and the DevOps manager

00:12:15,939 --> 00:12:18,560
gets an email all at the same time.

00:12:18,560 --> 00:12:22,050
Because these are things that cost business.

00:12:22,050 --> 00:12:25,980
So, using this as an error example here.

00:12:25,980 --> 00:12:29,730
I can see that I've got some interesting peaks that are harping inside of here.

00:12:29,730 --> 00:12:34,569
Again, doing the same breakdown, I can see what's causing that peak to happen.

00:12:34,569 --> 00:12:36,050
Decolored all these things.

00:12:36,050 --> 00:12:40,940
But I've got this orange peak which is right here.

00:12:40,940 --> 00:12:44,619
And that's actually probably the biggest single contributor to this model.

00:12:44,619 --> 00:12:46,379
I can then start drilling down.

00:12:46,379 --> 00:12:49,519
I can look at what it was week over week.

00:12:49,519 --> 00:12:55,149
Because I have a standard set of things I am measuring with the ability to find the

00:12:55,149 --> 00:13:00,069
things I didn't even think of to begin with, RED gives me a space to start.

00:13:00,069 --> 00:13:07,240
And RED gives me the ability to set something up and adapt to the needs insider hoof.

00:13:07,240 --> 00:13:08,240
Not responding.

00:13:08,240 --> 00:13:10,519
I'm at a DevOps conference.

00:13:10,519 --> 00:13:13,889
Not responding is what makes us unhappy.

00:13:13,889 --> 00:13:14,889
People yell at us.

00:13:14,889 --> 00:13:17,629
When things breaking with people yell at us.

00:13:17,629 --> 00:13:20,569
Duration, it's about the time.

00:13:20,569 --> 00:13:26,779
And you will hear about this in distributed request tracing, or distributed tracing.

00:13:26,779 --> 00:13:28,920
There are lots of things.

00:13:28,920 --> 00:13:35,209
Most distributed retracing, distribute request tracing is based on the server side.

00:13:35,209 --> 00:13:41,139
The client side is if not as important, possibly even more important than that.

00:13:41,139 --> 00:13:44,750
Because, again, that response time curve comes into play here.

00:13:44,750 --> 00:13:51,269
It brings the events into order so you now know what order things happened in and it

00:13:51,269 --> 00:13:56,749
can help you understand what the pathway through your system looked like.

00:13:56,749 --> 00:14:02,959
And that, honestly, will tell you things like, so, do I even know it's slow?

00:14:02,959 --> 00:14:08,480
Last or two weeks ago, we had this really interesting problem that showed up with my

00:14:08,480 --> 00:14:10,430
company's production product.

00:14:10,430 --> 00:14:16,070
All of a sudden the request coming into in our system started slowing down.

00:14:16,070 --> 00:14:17,970
Our bandwidth started dropping.

00:14:17,970 --> 00:14:18,970
This is unusual.

00:14:18,970 --> 00:14:20,269
It doesn't usually happen.

00:14:20,269 --> 00:14:25,319
It's actually fairly constant or going up slightly at all times.

00:14:25,319 --> 00:14:26,319
Nothing else reported.

00:14:26,319 --> 00:14:29,060
We had a bandwidth issue showing up here.

00:14:29,060 --> 00:14:36,100
What happens was an update to a service in our system invalidated one of the configuration

00:14:36,100 --> 00:14:41,720
controls which allowed us to control what happened when the queue filled up.

00:14:41,720 --> 00:14:46,970
And what happened was, the ingest engine basically filled its queue of 160 things.

00:14:46,970 --> 00:14:53,980
Added it to the queuing system, which filled its queue of 160 things and told the load

00:14:53,980 --> 00:14:55,819
balancer keep sending them to me.

00:14:55,819 --> 00:14:58,379
Nothing was going through.

00:14:58,379 --> 00:15:03,839
Root cause is an issue.

00:15:03,839 --> 00:15:11,379
We had one customer who was sending us lots of transactions with very small data rates.

00:15:11,379 --> 00:15:14,300
The little tiny data pieces inside of here.

00:15:14,300 --> 00:15:19,620
So, those pieces were backing up because the processing time doesn't really matter that

00:15:19,620 --> 00:15:20,699
much in terms of size.

00:15:20,699 --> 00:15:21,699
It's pretty fast.

00:15:21,699 --> 00:15:23,610
No matter if it's big or little.

00:15:23,610 --> 00:15:28,500
But because we had so much coming in here that this thing was overflowing.

00:15:28,500 --> 00:15:34,709
And the configuration change that showed up via this duration issue was actually being

00:15:34,709 --> 00:15:37,839
caused by another completely different problem.

00:15:37,839 --> 00:15:39,559
It wasn't even broken code.

00:15:39,559 --> 00:15:42,220
It was a broken configuration file.

00:15:42,220 --> 00:15:46,239
And so, duration give use interesting aspects inside that.

00:15:46,239 --> 00:15:49,740
And it can be in different ways.

00:15:49,740 --> 00:15:53,509
I started with the distributed traces model up here.

00:15:53,509 --> 00:15:57,379
There are three common distributed tracing things.

00:15:57,379 --> 00:15:58,800
There are lots of them.

00:15:58,800 --> 00:16:03,369
But you'll hear open tracing, open census and open telemetry.

00:16:03,369 --> 00:16:06,949
Which is the joining between open tracing and open census.

00:16:06,949 --> 00:16:10,810
Open tracing is with the Cloud Data Compute Foundation.

00:16:10,810 --> 00:16:13,899
Open census is out of Google.

00:16:13,899 --> 00:16:19,689
And open telemetry has an organization that will probably end up inside of CNCF as well.

00:16:19,689 --> 00:16:23,559
But you can see real quick there was a problem here.

00:16:23,559 --> 00:16:28,600
You can see quickly what happens in your metrics that there's a long tail issue that's here.

00:16:28,600 --> 00:16:32,850
And if you want to, you can see every single transaction and how long it took.

00:16:32,850 --> 00:16:34,980
Here is one that's out of the ordinary.

00:16:34,980 --> 00:16:38,129
There here, you can start grilling boo into it, the pieces.

00:16:38,129 --> 00:16:43,079
The reason you're not seeing the distributed trace model, as I said earlier, distributed

00:16:43,079 --> 00:16:52,279
tracing blows the amount of data out the window for a for this request with open tracing,

00:16:52,279 --> 00:17:00,100
there's about 128 traces that are generated for basically eight lines of log files.

00:17:00,100 --> 00:17:03,369
So, each piece matters.

00:17:03,369 --> 00:17:08,120
Here's the nice trick if you're in this space and you want to get into open or you want

00:17:08,120 --> 00:17:13,990
to get into tracing easily, if you're in microservices or a service oriented model, look at service

00:17:13,990 --> 00:17:15,150
meshes.

00:17:15,150 --> 00:17:21,970
All the service meshes in open source are already wired for distributed tracing.

00:17:21,970 --> 00:17:26,520
And that will give you the ability to see what's out in between service endpoints which

00:17:26,520 --> 00:17:29,770
is actually, again, probably more important.

00:17:29,770 --> 00:17:34,000
Communication pathways and microservices take up the majority of the time.

00:17:34,000 --> 00:17:39,980
Microservices being written as small as necessary tend to not be the stumbling block in terms

00:17:39,980 --> 00:17:41,590
of time.

00:17:41,590 --> 00:17:44,010
So, why RED?

00:17:44,010 --> 00:17:49,750
Well, interestingly enough, one of the early things was, I did spend ten years as a soccer

00:17:49,750 --> 00:17:52,470
rep.

00:17:52,470 --> 00:17:54,610
Anybody who plays soccer, I'm sorry.

00:17:54,610 --> 00:17:58,280
But seeing RED is has a meaning to me as well.

00:17:58,280 --> 00:17:59,880
But it's easy to remember.

00:17:59,880 --> 00:18:07,090
So, it's companions, golden signals or USE, utilization, saturation and errors are in

00:18:07,090 --> 00:18:08,549
that space.

00:18:08,549 --> 00:18:11,679
RED fits my world of microservices well.

00:18:11,679 --> 00:18:14,899
It fits this request based model extremely well.

00:18:14,899 --> 00:18:21,950
It drives a standard and consistency without locking you rigidly into any activity.

00:18:21,950 --> 00:18:28,120
You can build whatever you want to within in framework of your duration for this.

00:18:28,120 --> 00:18:31,880
And pretty much drive forward from that viewpoint.

00:18:31,880 --> 00:18:34,780
Start small, start with the basics.

00:18:34,780 --> 00:18:40,710
And then add to it as you discovered things that are unique around your application space.

00:18:40,710 --> 00:18:45,039
Almost every third party package will fall into this RED category as long as, again,

00:18:45,039 --> 00:18:47,570
their request or service is driven inside of here.

00:18:47,570 --> 00:18:51,049
So, it's not a major headache.

00:18:51,049 --> 00:18:55,440
Interesting enough, one of the things that it does help with a lot is automation of these

00:18:55,440 --> 00:18:56,440
functionalities.

00:18:56,440 --> 00:19:03,299
Because I am now looking at a standard environment, I can automate the same capabilities when

00:19:03,299 --> 00:19:07,020
I'm looking at the materials and information that RED is telling me.

00:19:07,020 --> 00:19:12,769
And, I no longer am worried about what happens with this team here and how they produce code

00:19:12,769 --> 00:19:15,440
or this team here and how they produce metrics.

00:19:15,440 --> 00:19:20,010
I now have a common basis that allows me to automate that access together.

00:19:20,010 --> 00:19:28,399
And honestly, what we have found, and what I've heard, is that everyone has focused on

00:19:28,399 --> 00:19:30,250
this is user happiness.

00:19:30,250 --> 00:19:32,590
If things slow down, you have unhappy users.

00:19:32,590 --> 00:19:36,110
If things aren't working, you have unhappy users, wherever they come from.

00:19:36,110 --> 00:19:40,559
And RED give use quick proxy, a quick look at the model.

00:19:40,559 --> 00:19:46,470
Maybe my users aren't happy, maybe I should look at this.

00:19:46,470 --> 00:19:53,890
Coming up, two types of alerts, anomaly or headache alerts.

00:19:53,890 --> 00:20:00,029
Anomaly is I have crossed a boundary or dropped below a boundary.

00:20:00,029 --> 00:20:01,600
Headaches are it broke.

00:20:01,600 --> 00:20:03,399
Go fix it.

00:20:03,399 --> 00:20:08,940
The issue with RED and the false metrics piece that RED can get you is something that shows

00:20:08,940 --> 00:20:13,440
up as an headache or an anomaly may be caused by something totally different.

00:20:13,440 --> 00:20:18,360
So, for instance, rate can be caused by an error problem.

00:20:18,360 --> 00:20:20,980
My earlier example for that.

00:20:20,980 --> 00:20:24,590
Bandwidth can end up as an error problem because I have a slow consumer problem and I can no

00:20:24,590 --> 00:20:27,880
longer get the data because my bandwidth is out there.

00:20:27,880 --> 00:20:31,700
Duration, it could show up as error or rate.

00:20:31,700 --> 00:20:36,990
So, rate crosses the boundaries and you need to be careful that you're not hitting a false

00:20:36,990 --> 00:20:40,250
positive indicator when it's not there.

00:20:40,250 --> 00:20:46,450
So, as a quick example, this is a RED chart, rate error duration.

00:20:46,450 --> 00:20:50,399
Broken down by the service this is my current backend.

00:20:50,399 --> 00:20:55,149
There's structure pieces and queue pieces.

00:20:55,149 --> 00:21:01,200
And we're indicating the change in reflection of the error pieces inside much here.

00:21:01,200 --> 00:21:03,830
Showing you what happened over the last week over week basis.

00:21:03,830 --> 00:21:07,190
It's a huge amount of information in a very quick.

00:21:07,190 --> 00:21:13,250
The darker the reds, the more the errors are showing up.

00:21:13,250 --> 00:21:18,190
There's a pail green not showing up that's basically saying things are getting better.

00:21:18,190 --> 00:21:21,549
So, rate has an easy way of being expressed.

00:21:21,549 --> 00:21:26,549
A visual method for expressing it and so forth.

00:21:26,549 --> 00:21:28,410
There is a zen around this.

00:21:28,410 --> 00:21:30,639
And this is the observability zen.

00:21:30,639 --> 00:21:33,299
Of the customer, is it working or not?

00:21:33,299 --> 00:21:34,960
Is it fast enough?

00:21:34,960 --> 00:21:36,100
That's pretty much it.

00:21:36,100 --> 00:21:39,140
For the people who are keeping it working, there's more than that.

00:21:39,140 --> 00:21:43,549
There's the latency, there's the rates, there's the concern currencies and how you're doing

00:21:43,549 --> 00:21:46,680
on system components and how you're responding to it.

00:21:46,680 --> 00:21:52,080
So, philosophy, I ran a Stanford philosophy.

00:21:52,080 --> 00:21:56,990
You cannot get them to agree on anything.

00:21:56,990 --> 00:22:00,780
Informs is not an answer.

00:22:00,780 --> 00:22:04,070
It's powerful, but not sufficient.

00:22:04,070 --> 00:22:09,190
Measure metrics can be aggregated, but not disaggregated.

00:22:09,190 --> 00:22:11,630
Your job is to look at the work, not the service.

00:22:11,630 --> 00:22:16,350
Look at how the work affects the service.

00:22:16,350 --> 00:22:18,610
And your goal is not observability.

00:22:18,610 --> 00:22:20,000
That's an attribute.

00:22:20,000 --> 00:22:23,799
Your goal is meantime to recognition and meantime to resolution.

00:22:23,799 --> 00:22:26,020
RED will help you get there.

00:22:26,020 --> 00:22:28,740
Short, observability is more than monitoring.

00:22:28,740 --> 00:22:31,409
But we start with monitoring in this of here.

00:22:31,409 --> 00:22:36,340
RED can work in strange and mysterious ways to give you all sorts of information.

00:22:36,340 --> 00:22:41,100
Be careful that the information you're looking at is the information you expect to see here.

00:22:41,100 --> 00:22:43,240
And use all your signals.

00:22:43,240 --> 00:22:48,139
If you've got Twitter feeds, figure out how to bring them in and use them as a signal

00:22:48,139 --> 00:22:49,640
moving forward for observability.

00:22:49,640 --> 00:22:53,481
And I think I have a few minutes for questions.

00:22:53,481 --> 00:22:54,929
[ Applause ] >> Now, I know the questions are out there.

00:22:54,929 --> 00:22:56,429
Who has got questions?

00:22:56,429 --> 00:22:58,679
DAVE: Do I have an audience?

00:22:58,679 --> 00:23:00,940
>> Make sure you have a mic.

00:23:00,940 --> 00:23:02,059
It's been recorded.

00:23:02,059 --> 00:23:05,380
AUDIENCE: Do you have suggestions on best way to interface this into an existing model?

00:23:05,380 --> 00:23:08,880
DAVE: The best way repeating your questions to pick it up on the mic, how do we integrate

00:23:08,880 --> 00:23:10,190
into an existing model?

00:23:10,190 --> 00:23:17,880
The exists models can be APMs, log management, the existing models can be tail, graph, any

00:23:17,880 --> 00:23:19,690
of those things.

00:23:19,690 --> 00:23:24,019
Interestingly enough, there are two ways to approach this, one is a straightforward metrics

00:23:24,019 --> 00:23:25,019
model.

00:23:25,019 --> 00:23:29,870
Bring the in the metrics.

00:23:29,870 --> 00:23:32,450
Doing this from scratch, looking heavily at Prometheus.

00:23:32,450 --> 00:23:36,110
It's a standard way of producing metrics.

00:23:36,110 --> 00:23:43,410
Looking at bringing it into an APM model, you're pretty much there.

00:23:43,410 --> 00:23:47,039
You need to sort the data into a different way for this.

00:23:47,039 --> 00:23:52,440
If it's not, if you're not with that particular type of model here, there are a number of

00:23:52,440 --> 00:23:58,590
times that will pull out RED from almost every single third party package.

00:23:58,590 --> 00:24:00,549
And so, take a look at what your packages are.

00:24:00,549 --> 00:24:06,179
And grab, you know grab your favorite Graphana instance and I bet you can go to the RED model

00:24:06,179 --> 00:24:10,419
headache is you're not doing the drill in unless you do the fidelity up roll.

00:24:10,419 --> 00:24:11,419
That's the challenge.

00:24:11,419 --> 00:24:13,789
Come on, I can't sing and dance for the next five minutes.

00:24:13,789 --> 00:24:19,590
I'm going to phrase this: how does that relate to the telemetry?

00:24:19,590 --> 00:24:21,480
They are used a lot.

00:24:21,480 --> 00:24:23,380
We hear signals and observability.

00:24:23,380 --> 00:24:29,370
The concept of observability from the old machine days came out of the telemetry model.

00:24:29,370 --> 00:24:36,809
And it's pretty much the same concept applied to application digital involvement.

00:24:36,809 --> 00:24:39,930
It's really scarily close for that.

00:24:39,930 --> 00:24:46,480
So, you know, the if you've ever worked on an log computer, this would have made your

00:24:46,480 --> 00:24:48,100
hair stand on end, but you would have loved it.

00:24:48,100 --> 00:24:49,100
>> Any questions?

00:24:49,100 --> 00:24:50,100
Over here?

00:24:50,100 --> 00:24:51,100
This is pretty dense.

00:24:51,100 --> 00:24:52,100
And you did it pretty quickly.

00:24:52,100 --> 00:24:53,100
So DAVE: They told me I had 25 minutes and I

00:24:53,100 --> 00:24:54,100
hit 22: 38.

00:24:54,100 --> 00:24:55,100
>> Can you two into more detail?

00:24:55,100 --> 00:24:59,059
Are you directly involved in they finally picked a name, open tracing?

00:24:59,059 --> 00:25:06,039
DAVE: Open tracing actually started with oh, shoot.

00:25:06,039 --> 00:25:09,429
Another company and was given to the CNCF.

00:25:09,429 --> 00:25:16,320
It's been a CNCF incubating project since I want to say 2016, 2017.

00:25:16,320 --> 00:25:17,590
We got involved.

00:25:17,590 --> 00:25:22,429
I'm fascinated by distributing tracing until I look at the amount of data it produces.

00:25:22,429 --> 00:25:25,720
To do a talk on distributed tracing as well.

00:25:25,720 --> 00:25:31,350
Distributed tracing, the difference between those three I talked about, tracing, census

00:25:31,350 --> 00:25:38,850
and telemetry, tracing defined sort of the API of how you got traces.

00:25:38,850 --> 00:25:39,850
Okay?

00:25:39,850 --> 00:25:45,649
Open census defined both an API and an engine to actually show you and drive down the traces.

00:25:45,649 --> 00:25:52,330
And telemetry is to bind the APIs together so people who have written open tracing stuff

00:25:52,330 --> 00:25:53,870
don't have to rewrite.

00:25:53,870 --> 00:25:56,159
So, those are the three categories.

00:25:56,159 --> 00:26:00,550
In general, just so my tracing hat on for a minute here.

00:26:00,550 --> 00:26:06,789
In general, it takes about a year to really successfully instrument for open tracing.

00:26:06,789 --> 00:26:12,519
And that's why I heavily recommend if you can use something like Istio, Istio has almost

00:26:12,519 --> 00:26:14,130
virtual lie no overhead.

00:26:14,130 --> 00:26:15,580
Use it.

00:26:15,580 --> 00:26:17,429
It's painful to go through traces.

00:26:17,429 --> 00:26:26,450
And we did this with our product and it was massively painful to do this because the tracing

00:26:26,450 --> 00:26:27,840
has some interesting characteristics.

00:26:27,840 --> 00:26:31,990
The spans, the tracing IDs and all those things get set up for you.

00:26:31,990 --> 00:26:36,259
But how you read them and analyze them becomes more and more of a challenge.

00:26:36,259 --> 00:26:37,259
>> One last check.

00:26:37,259 --> 00:26:38,259
Any other questions?

00:26:38,259 --> 00:26:39,259
DAVE: Just one right here.

00:26:39,259 --> 00:26:40,259
AUDIENCE: Do you have one top strategy, I guess, for weeding out red herrings in

00:26:40,259 --> 00:26:41,259
DAVE: Oh, god.

00:26:41,259 --> 00:26:44,429
Do I have one top strategy for weeding out the red herrings.

00:26:44,429 --> 00:26:47,360
Block your computer in a closed room and unplug it.

00:26:47,360 --> 00:26:51,830
That gets rid of almost every red hearing you have ever seen.

00:26:51,830 --> 00:26:55,040
Here is the way I recommend.

00:26:55,040 --> 00:27:01,230
Generally speaking were you are going to see either an anomaly alert crossing a boundary

00:27:01,230 --> 00:27:02,570
or you're going to see an error.

00:27:02,570 --> 00:27:03,809
A headache alert.

00:27:03,809 --> 00:27:06,210
You're not going to see a lot of the other alerts happening.

00:27:06,210 --> 00:27:10,580
Very few people are doing tracing alerts at this time.

00:27:10,580 --> 00:27:16,340
If you see a headache alert, ignore everything else and find out what caused that alert.

00:27:16,340 --> 00:27:21,460
The headache here, and the reason the secondary headache, especially my space and microservices,

00:27:21,460 --> 00:27:24,780
the error can occur other here and the cause is over here.

00:27:24,780 --> 00:27:31,210
So, for instance, in our microservices world we add an annotation every time we add a service

00:27:31,210 --> 00:27:34,059
that we can back trace to.

00:27:34,059 --> 00:27:35,299
No big deal.

00:27:35,299 --> 00:27:39,200
Timestamp, this changed so we can track back to it.

00:27:39,200 --> 00:27:44,000
It's quite often that they show up as rate problems, unfortunately.

00:27:44,000 --> 00:27:47,280
Rate problems basically say, oh, something doesn't look like.

00:27:47,280 --> 00:27:50,440
Not that something's wrong.

00:27:50,440 --> 00:27:54,500
The next thing I do is I split all of my code, my things out, and I look if anything is out

00:27:54,500 --> 00:27:55,649
of whack.

00:27:55,649 --> 00:27:56,649
Pattern match.

00:27:56,649 --> 00:28:00,289
If something is out of whack, focus on that.

00:28:00,289 --> 00:28:09,500
Web 2, 6 and 4 or 3, that's causing the rate problem here.

00:28:09,500 --> 00:28:13,100
It's running, it's not the way I want them to run.

00:28:13,100 --> 00:28:18,040
I drilled town into that and looked down into the rate functionality.

00:28:18,040 --> 00:28:19,350
Okay.

00:28:19,350 --> 00:28:23,389
What's happen something I had a rate picture that said my overall rate is looking like

00:28:23,389 --> 00:28:24,389
this.

00:28:24,389 --> 00:28:27,399
And this guy is eating a lot of it.

00:28:27,399 --> 00:28:33,210
It was the experience model, check your load balancer because your load balancer is probably

00:28:33,210 --> 00:28:35,519
screwed up.

00:28:35,519 --> 00:28:36,690
That's kind of it.

00:28:36,690 --> 00:28:41,850
A lot of this right now is just pure investigation.

00:28:41,850 --> 00:28:45,950
Hard, you know, code scene investigation, if you will.

00:28:45,950 --> 00:28:51,210
By the way, I want this on the back of a jacket with lettering for this.

00:28:51,210 --> 00:28:59,470
But nevertheless, a lot of it comes down to experience and rate gives you the ability

00:28:59,470 --> 00:29:02,880
to recognize things in a exon environment.

00:29:02,880 --> 00:29:07,809
If you move from one to another, you're not going to have to figure out what's good and

00:29:07,809 --> 00:29:08,809
bad.

00:29:08,809 --> 00:29:09,809
You know.

00:29:09,809 --> 00:29:13,590
But there's no magic trick that I can think of to do this.

00:29:13,590 --> 00:29:16,730
I have great demos that can drill you through in 30 seconds.

00:29:16,730 --> 00:29:22,490
But tell you right now, they're demos.

00:29:22,490 --> 00:29:27,150
>> We've got a few more minutes to the next session, Jesse Butler, talking about imposter

00:29:27,150 --> 00:29:28,150
syndrome.

00:29:28,150 --> 00:29:29,860
Are you around?

00:29:29,860 --> 00:29:32,950
DAVE: I will be around the rest of the day.

00:29:32,950 --> 00:29:33,950
Come find me.

00:29:33,950 --> 00:29:37,009
If you have questions glad to chat with you.

00:29:37,009 --> 00:29:39,650
About anything that you want.

00:29:39,650 --> 00:29:40,799

YouTube URL: https://www.youtube.com/watch?v=1O6hO8YLDwA


