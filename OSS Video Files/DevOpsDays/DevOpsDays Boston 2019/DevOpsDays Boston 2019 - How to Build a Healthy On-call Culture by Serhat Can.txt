Title: DevOpsDays Boston 2019 - How to Build a Healthy On-call Culture by Serhat Can
Publication date: 2019-10-11
Playlist: DevOpsDays Boston 2019
Description: 
	How to Build a Healthy On-call Culture by Serhat Can

Raise your hand if you enjoy being buried in alerts or woken up at 2 am? (Yeah… thought so.) Ever-rising customer expectations around high availability and performance put massive pressure on the teams who develop and support SaaS products. And teams are literally losing sleep over it.

Until outages and other incidents are a thing of the past, organizations need to invest in a way of dealing with them that won’t lead to burn-out. In this session, you’ll learn how to combine the latest tooling with DevOps practices in the pursuit of a sustainable incident response workflow. It’s all about transparency, actionable alerts, resilience, and learning from each incident.

#DevOpsDays #DevOpsDaysBoston
Captions: 
	00:00:16,509 --> 00:00:17,509
There are DevOpsDays all other the world.

00:00:17,509 --> 00:00:18,509
You should go visit one.

00:00:18,509 --> 00:00:19,509
He is going to focus on Opsgenie.

00:00:19,509 --> 00:00:20,509
I think many of you might get a page during this conference.

00:00:20,509 --> 00:00:21,509
I apologize if you do.

00:00:21,509 --> 00:00:22,509
He is trying to make your life better.

00:00:22,509 --> 00:00:23,509
And he's also an AWS community hero.

00:00:23,509 --> 00:00:24,509
And without further ado, fake it away.

00:00:24,509 --> 00:00:25,509
SERHAT: Hello, everyone.

00:00:25,509 --> 00:00:26,509
Welcome.

00:00:26,509 --> 00:00:27,509
[ Applause ] Thanks.

00:00:27,509 --> 00:00:28,509
Pretty excited to be here.

00:00:28,509 --> 00:00:29,509
So, this is my second time if Boston.

00:00:29,509 --> 00:00:30,509
Boston is one of my favorite cities in the US.

00:00:30,509 --> 00:00:31,509
I live in Istanbul.

00:00:31,509 --> 00:00:32,509
Again, I'm part of the DevOpsDays Core team.

00:00:32,509 --> 00:00:33,509
I'm also helping the local team here back in Istanbul.

00:00:33,509 --> 00:00:34,509
I'm pretty interested in I want to talk about AWS.

00:00:34,509 --> 00:00:35,509
I will be here during the whole day.

00:00:35,509 --> 00:00:36,509
But today we're going to talk about on call.

00:00:36,509 --> 00:00:37,509
So, my favorite topic.

00:00:37,509 --> 00:00:38,509
So, before we start, I want to ask you a couple of questions.

00:00:38,509 --> 00:00:40,279
So, are you working in an organization that expects you to be on call?

00:00:40,279 --> 00:00:41,279
Awesome.

00:00:41,279 --> 00:00:45,288
So, do you think on call sucks?

00:00:45,288 --> 00:00:46,638
Anyone?

00:00:46,638 --> 00:00:47,989
Okay.

00:00:47,989 --> 00:00:51,459
Well, probably does suck at a certain point.

00:00:51,459 --> 00:00:52,459
I agree.

00:00:52,459 --> 00:00:57,959
I have been on call for more than a year before moving.

00:00:57,959 --> 00:01:02,789
I have been an engineer for six years, again, before moving into this job.

00:01:02,789 --> 00:01:09,059
And I agree with you, but I think there are a lot of practical solutions that we can we

00:01:09,059 --> 00:01:13,279
can apply to reduce the stress and burnout of on call.

00:01:13,279 --> 00:01:16,569
And why is always more important than why.

00:01:16,569 --> 00:01:21,469
I want to start with talking about why on call matters first.

00:01:21,469 --> 00:01:25,769
And then why having better on call practices matters.

00:01:25,769 --> 00:01:34,949
So, first, so, I could give you different examples from the biggest providers I have

00:01:34,949 --> 00:01:39,839
a list of them as your AWS they all fail all the time.

00:01:39,839 --> 00:01:42,519
They bring that with them.

00:01:42,519 --> 00:01:46,869
But I want to give you an example from a different from a different example.

00:01:46,869 --> 00:01:49,489
I want to give you a different example.

00:01:49,489 --> 00:01:52,059
The service that our lives depend on.

00:01:52,059 --> 00:01:53,059
911.

00:01:53,059 --> 00:01:56,059
So, the service that needs to be operating all the time.

00:01:56,059 --> 00:01:59,969
These examples are from the United States.

00:01:59,969 --> 00:02:07,979
In 2014 on a June morning in Washington, 911's IT operations manager got a page.

00:02:07,979 --> 00:02:11,439
There was someone trying to reach out to 911, but he couldn't.

00:02:11,439 --> 00:02:14,708
He was able to reach out by borrowing someone else's phone.

00:02:14,708 --> 00:02:18,589
This was happening for a specific telecom provider.

00:02:18,589 --> 00:02:21,869
They could not.

00:02:21,869 --> 00:02:27,798
And it was only solved 12 hours later without a response.

00:02:27,798 --> 00:02:37,419
And last year in 2018, there was a more serious incident.

00:02:37,419 --> 00:02:44,858
The data center the cloud provider that these companies, these cell providers used had a

00:02:44,858 --> 00:02:51,738
big outage and it brought down the 911 wireless calls for over a cay and they could not do

00:02:51,738 --> 00:02:52,958
anything.

00:02:52,958 --> 00:02:54,169
So...

00:02:54,169 --> 00:03:01,389
[ Laughter ] We already agree to this one.

00:03:01,389 --> 00:03:05,668
Obviously it is not funny if you can't go without your laptop, if you wake up at 2:00

00:03:05,668 --> 00:03:06,668
a.m. in the morning.

00:03:06,668 --> 00:03:11,119
But there is more to this than just being on call.

00:03:11,119 --> 00:03:13,449
And there is research on this.

00:03:13,449 --> 00:03:20,798
People from the professors from the University of Hamburg did the research with 122 people

00:03:20,798 --> 00:03:23,048
from 13 organizations.

00:03:23,048 --> 00:03:28,968
And they collected data survey over a period of four days during which they were required

00:03:28,968 --> 00:03:31,218
to be on call outside of business hours.

00:03:31,218 --> 00:03:35,018
And during four days which they were not.

00:03:35,018 --> 00:03:39,768
And they also collected daily cortisol levels from 51 of them.

00:03:39,768 --> 00:03:47,268
And the results show that on call, even if there is no alert, on call isn't leisure time.

00:03:47,268 --> 00:03:54,238
And there are significant effects of extended work availability on the daily start of the

00:03:54,238 --> 00:03:58,809
day mood and cortisol awakening response.

00:03:58,809 --> 00:04:06,389
And this help us invest more in on call and take actions to ensure that we put enough

00:04:06,389 --> 00:04:10,089
resources to make on call better.

00:04:10,089 --> 00:04:15,539
And as a software industry, we became a lot better since the last decade, since the foundation

00:04:15,539 --> 00:04:20,308
of DevOps ten years ago in Ghent.

00:04:20,308 --> 00:04:23,778
And there was the DevOpsDays.

00:04:23,778 --> 00:04:31,338
So, we started making some changes using DevOps principles and tooling.

00:04:31,338 --> 00:04:35,329
One of the most important parts comes with the CI/CD solutions.

00:04:35,329 --> 00:04:44,849
According to a survey that our team did, over 500 people, 57% of teams report fewer outages

00:04:44,849 --> 00:04:51,719
in the inbox after they adopted the CI/CD solutions.

00:04:51,719 --> 00:04:53,539
But there was something different.

00:04:53,539 --> 00:04:59,009
We had the saying, this sentence basically changed how we operate software today.

00:04:59,009 --> 00:05:02,338
You build it, you run it.

00:05:02,338 --> 00:05:10,969
And for us, at Opsgenie, Opsgenie is not part of Atlassian, but when I joined two years

00:05:10,969 --> 00:05:15,859
ago, we had 15 people in the team and only two technical cofounders were on call.

00:05:15,859 --> 00:05:19,669
They knew the ins and outs.

00:05:19,669 --> 00:05:22,439
They could take on the on call.

00:05:22,439 --> 00:05:23,439
This was fine.

00:05:23,439 --> 00:05:29,849
But as we scaled up to 80 people today just for the Opsgenie team, it didn't work out.

00:05:29,849 --> 00:05:32,489
We started putting the person on call.

00:05:32,489 --> 00:05:36,259
Because one of the reasons is they get a lot of the others, they had a lot of services.

00:05:36,259 --> 00:05:41,259
And they didn't know all the details anymore.

00:05:41,259 --> 00:05:44,659
So, there are different versions of the on call.

00:05:44,659 --> 00:05:48,789
And as you know, they follow a slightly different approach.

00:05:48,789 --> 00:05:55,819
They have the on call, but for services that are not extremely reliable, for services that

00:05:55,819 --> 00:06:01,109
are stable enough, they have been going for a long time, let's say Google Maps, they are

00:06:01,109 --> 00:06:05,839
on for a long time, the on call is the SRE's job.

00:06:05,839 --> 00:06:14,849
But the SRE team can give it back to the developers if the on call is doesn't if they don't meet

00:06:14,849 --> 00:06:17,148
the service levels.

00:06:17,148 --> 00:06:22,558
And I think developers on call is the most important part of this how to do on call.

00:06:22,558 --> 00:06:27,889
And there are three important things that we should do.

00:06:27,889 --> 00:06:31,238
Three benefits of putting developers on call.

00:06:31,238 --> 00:06:37,609
And the first one is we have like ever increasing demands around reliability, security and applications.

00:06:37,609 --> 00:06:43,268
And developers are the most qualified people who can solve problems faster.

00:06:43,268 --> 00:06:48,268
And the second one is when we talk about DevOps, we usually talk about this.

00:06:48,268 --> 00:06:53,719
So, basically when we put developers, we remove the conflicting incentives, right?

00:06:53,719 --> 00:06:56,458
Everyone is responsible for what is running in production.

00:06:56,458 --> 00:07:00,969
And the outcome is better testing, documentation and alerting.

00:07:00,969 --> 00:07:04,249
And overall on call.

00:07:04,249 --> 00:07:09,268
And the last benefit of developers on call, I think this is sometimes overlooked, is the

00:07:09,268 --> 00:07:13,189
alignment between development teams and management.

00:07:13,189 --> 00:07:18,308
Because it used to be only a couple people from operations were on call.

00:07:18,308 --> 00:07:21,419
If something happens on production, they were to blame.

00:07:21,419 --> 00:07:23,599
And managers considered them responsible.

00:07:23,599 --> 00:07:28,859
But now management can easily see what are the effects of on call on the time developers

00:07:28,859 --> 00:07:30,229
spent.

00:07:30,229 --> 00:07:34,118
Not creating new features, but working on alerts.

00:07:34,118 --> 00:07:38,569
They're now able to better see the importance of reliability work.

00:07:38,569 --> 00:07:44,449
So, they can prioritize reliability work easily.

00:07:44,449 --> 00:07:50,159
So, now I want to share a personal story to you.

00:07:50,159 --> 00:07:53,219
This happened about two years ago.

00:07:53,219 --> 00:07:57,058
So, me and my friend was working on an issue.

00:07:57,058 --> 00:07:59,379
So, it was kind of a big change.

00:07:59,379 --> 00:08:05,458
So, for those of you who don't know, Opsgenie is an alerting and on call management solution.

00:08:05,458 --> 00:08:06,638
Alerting is important.

00:08:06,638 --> 00:08:11,148
When you log on to Opsgenie, you see the alerts page.

00:08:11,148 --> 00:08:15,609
Our change was Elasticsearch.

00:08:15,609 --> 00:08:17,868
Most of you use that.

00:08:17,868 --> 00:08:22,449
It was a code change.

00:08:22,449 --> 00:08:25,999
We did all the tests we did.

00:08:25,999 --> 00:08:34,929
Question couldn't do it Monday, it was around 5:30:00 p.m. on Friday.

00:08:34,929 --> 00:08:43,209
If you remember one thing, you don't send changes at 5:30:00 p.m. on Friday.

00:08:43,209 --> 00:08:45,668
So, just going over the incident.

00:08:45,668 --> 00:08:48,859
So, we could not catch, unfortunately, this problem.

00:08:48,859 --> 00:08:50,989
With automated monitoring.

00:08:50,989 --> 00:08:55,859
Customers report a problem, there was an inconsistency on the UI, they acknowledge.

00:08:55,859 --> 00:08:57,179
They see it's not closed.

00:08:57,179 --> 00:09:03,049
They see they used others from Elasticsearch.

00:09:03,049 --> 00:09:05,409
Customers report the problem.

00:09:05,409 --> 00:09:11,629
So, customers access from Slack, paging at alerting team.

00:09:11,629 --> 00:09:16,449
So, on call checks the Jira issues.

00:09:16,449 --> 00:09:18,429
We are now sending the feature.

00:09:18,429 --> 00:09:19,579
And it is me as a responder.

00:09:19,579 --> 00:09:22,099
I get paged as well.

00:09:22,099 --> 00:09:28,939
And after like 15 minutes of figuring out figuring out what is wrong with this one,

00:09:28,939 --> 00:09:33,939
we could not, with so, we bring in the incident response team, two technical cofounders, two

00:09:33,939 --> 00:09:35,149
senior engineers.

00:09:35,149 --> 00:09:37,879
We started working on the issue.

00:09:37,879 --> 00:09:44,759
At this moment, we knew we had an option to disable Elasticsearch cluster.

00:09:44,759 --> 00:09:47,299
We had two clusters at the time.

00:09:47,299 --> 00:09:51,369
Whatever the decision was, we are reading from that one.

00:09:51,369 --> 00:09:57,149
We decided with the two clusters always to distribute additional alerts because there

00:09:57,149 --> 00:10:00,809
were more going on to one cluster.

00:10:00,809 --> 00:10:02,529
But it stopped the problem.

00:10:02,529 --> 00:10:10,209
So, after a lot of debugging, we found a bug in the code.

00:10:10,209 --> 00:10:13,309
Basically it was about like the nature of Elasticsearch.

00:10:13,309 --> 00:10:16,889
We did not consider in just one code one part of the code.

00:10:16,889 --> 00:10:18,949
We sent the fix.

00:10:18,949 --> 00:10:20,549
But there were still inconsistencies.

00:10:20,549 --> 00:10:25,059
So, it was about 2:00 a.m. in the morning, we went back home, ran the data and fixed

00:10:25,059 --> 00:10:29,549
the other states and went back home.

00:10:29,549 --> 00:10:39,249
As you can imagine, there were a lot of lessons learned from that experience.

00:10:39,249 --> 00:10:42,009
Now I'm going over some of them.

00:10:42,009 --> 00:10:44,478
But I'm to the just gonna talk about this incident.

00:10:44,478 --> 00:10:46,838
I'm also gonna talk about some of our successes.

00:10:46,838 --> 00:10:49,259
We usually miss them, we usually don't talk about them.

00:10:49,259 --> 00:10:54,959
But talking about them was also important for us to figure out like what can we do better

00:10:54,959 --> 00:10:58,449
next time?

00:10:58,449 --> 00:11:06,269
Last week Andrew Clay Shafer was in Istanbul, I love this quote, you haven't learned anything

00:11:06,269 --> 00:11:09,269
until you change your behavior.

00:11:09,269 --> 00:11:11,219
You're seeing all these lessons learned here.

00:11:11,219 --> 00:11:16,049
If you don't change anything in your company, it's did doesn't mean anything.

00:11:16,049 --> 00:11:21,468
So, the first thing I want to point out is heroism is not sustainable.

00:11:21,468 --> 00:11:26,360
If you are just a couple of people in your company, that can work to some point, but

00:11:26,360 --> 00:11:28,299
even Iron Man needs backup, right?

00:11:28,299 --> 00:11:30,359
So, it doesn't scale up.

00:11:30,359 --> 00:11:34,399
And obviously the first thing is to put developers on call.

00:11:34,399 --> 00:11:36,439
We talked about that.

00:11:36,439 --> 00:11:42,399
Best practices, don't put an engineer on call more than a week.

00:11:42,399 --> 00:11:44,789
Improvements in your rotations.

00:11:44,789 --> 00:11:50,468
There are improvement in this one.

00:11:50,468 --> 00:11:52,908
You need to make it easy to call for help.

00:11:52,908 --> 00:11:57,028
In this case, the customer success team knew how to page the on call team.

00:11:57,028 --> 00:12:03,019
They didn't need to go to Slack and say who to call if there was a problem with the alerting

00:12:03,019 --> 00:12:04,019
page.

00:12:04,019 --> 00:12:07,299
It was written in the documents, they knew who they needed to call.

00:12:07,299 --> 00:12:14,879
And once the alert is created, the alerting team on call engineer is there selecting my

00:12:14,879 --> 00:12:19,989
name and I get a call from the preferred notification channel which is important.

00:12:19,989 --> 00:12:22,939
They don't need to know who is on call at that point, which is important.

00:12:22,939 --> 00:12:28,949
And you need to have three step escalation paths.

00:12:28,949 --> 00:12:30,999
If the alerting team wasn't available.

00:12:30,999 --> 00:12:38,559
Thaw knew it was a high priority because they were able to replicate the problem, so, if

00:12:38,559 --> 00:12:43,629
the escalation couldn't reach out to the on call person, in five minutes it will page

00:12:43,629 --> 00:12:46,019
the whole alerting team, for example.

00:12:46,019 --> 00:12:48,909
Because it was a high priority alert for us.

00:12:48,909 --> 00:12:52,989
And one of the things that I think we should be talking about is arranging development

00:12:52,989 --> 00:12:56,098
duties during on call based on pager load.

00:12:56,098 --> 00:13:04,009
So, we as we started doing on call with the putting developers on call, we decided that

00:13:04,009 --> 00:13:11,329
half of the developer's time, they will be able to work on the new features.

00:13:11,329 --> 00:13:16,119
But it didn't work out well for a while, so, we decided to basically don't expect the new

00:13:16,119 --> 00:13:17,159
feature development.

00:13:17,159 --> 00:13:20,129
And just work on the alerts, improve them.

00:13:20,129 --> 00:13:23,228
Just go and talk with the team if needed.

00:13:23,228 --> 00:13:26,039
Just work on fixing your issues.

00:13:26,039 --> 00:13:27,559
And this worked out pretty well for some time.

00:13:27,559 --> 00:13:35,098
Then we went back to the developers and the future development.

00:13:35,098 --> 00:13:39,538
It's based on your team, based on your maturity level.

00:13:39,538 --> 00:13:40,759
Decide what's best for your team.

00:13:40,759 --> 00:13:44,708
There's no one truth or silver bullet.

00:13:44,708 --> 00:13:49,369
I suggest assigning on call time early to the engineer making the deployment.

00:13:49,369 --> 00:13:52,959
This is not something we did at the time.

00:13:52,959 --> 00:13:56,779
And most of the problems we have, we have them because there is a change.

00:13:56,779 --> 00:14:02,718
If something changes, there's a better chance there is going to be something broken.

00:14:02,718 --> 00:14:08,788
And if you assign the on call temporarily like 15 minutes, for example, to the engineer,

00:14:08,788 --> 00:14:15,749
probably if there's an issue, probably he or she knows like the best.

00:14:15,749 --> 00:14:21,968
So, the second one is, alerting.

00:14:21,968 --> 00:14:27,759
I mean, alerting can easily become your best friend or worst friend.

00:14:27,759 --> 00:14:29,259
Enemy.

00:14:29,259 --> 00:14:32,218
So, an example comes from here.

00:14:32,218 --> 00:14:40,079
In 2010, a hospital here, a patient died after alarms signaling a critical event went unnoticed

00:14:40,079 --> 00:14:41,639
by 10 nurses.

00:14:41,639 --> 00:14:47,359
I mean, the reason, basically, there were too many alerts coming through.

00:14:47,359 --> 00:14:50,449
They started not caring about them anymore.

00:14:50,449 --> 00:14:54,629
They basically if there is an alert, it's probably a false one.

00:14:54,629 --> 00:14:56,739
They basically skipped that.

00:14:56,739 --> 00:14:58,588
And they turned off the alerts.

00:14:58,588 --> 00:14:59,588
And there were a lot of unheard alerts.

00:14:59,588 --> 00:15:04,639
So, the result was a patient died.

00:15:04,639 --> 00:15:07,159
I mean, it doesn't have to be this way.

00:15:07,159 --> 00:15:08,478
Like someone dying.

00:15:08,478 --> 00:15:11,869
But obviously our apps are important for our business.

00:15:11,869 --> 00:15:16,549
You know, they basically what we are doing in this job.

00:15:16,549 --> 00:15:17,958
So, they're also pretty important.

00:15:17,958 --> 00:15:22,979
That is why we need to proactively fight against alert fatigue.

00:15:22,979 --> 00:15:26,419
And my suggestion is no alert is better than a lot of alerts.

00:15:26,419 --> 00:15:32,999
If you are getting a lot of alerts, look at them and start creating alerts that are actually

00:15:32,999 --> 00:15:34,489
important for you.

00:15:34,489 --> 00:15:38,179
And make the distinction between tickets and alerts very clear.

00:15:38,179 --> 00:15:42,599
Because like if, like, a failing backup job doesn't mean anything.

00:15:42,599 --> 00:15:44,679
It can wait until morning.

00:15:44,679 --> 00:15:45,778
Create a Jira ticket.

00:15:45,778 --> 00:15:51,158
Or create an alert if you want to see, because it's important for you, you want to see in

00:15:51,158 --> 00:15:53,249
the morning, do that, make an alert.

00:15:53,249 --> 00:15:58,459
But don't page the on call engineer for that alert.

00:15:58,459 --> 00:16:00,549
And you need to add context on alerts.

00:16:00,549 --> 00:16:07,968
So, in this case, in this example, in this incident, we did not catch the problem with

00:16:07,968 --> 00:16:14,929
the bugs, you need to catch them before customer impacts if possible.

00:16:14,929 --> 00:16:20,429
In this case, we did not, but we try to add details, sometimes get the logs and attach

00:16:20,429 --> 00:16:26,389
it back on to the alert so we reduce the context between going between the logging tool and

00:16:26,389 --> 00:16:29,449
the monitoring tool and the alerting tool.

00:16:29,449 --> 00:16:35,958
We have where you just click if you want to increase the capacity in the instance to your

00:16:35,958 --> 00:16:40,049
local answer, you can do it in one click.

00:16:40,049 --> 00:16:46,799
You don't need to go into the management console and -- for example.

00:16:46,799 --> 00:16:53,549
And one of the things we started doing after this one was identifying and reviewing repeating

00:16:53,549 --> 00:16:54,549
alerts.

00:16:54,549 --> 00:17:01,039
The way we do it, we get this data from Opsgenie and we basically create send a message to

00:17:01,039 --> 00:17:03,999
Slack every day if there is a repeating alert.

00:17:03,999 --> 00:17:08,259
We mention it to the team so they can check basically if you want to remove it, remove

00:17:08,259 --> 00:17:10,359
it, or if you want to deal with it, deal with it.

00:17:10,359 --> 00:17:15,449
This is also important.

00:17:15,449 --> 00:17:19,229
And we talk about blameless postmortems.

00:17:19,229 --> 00:17:26,779
I'm not going to talk about blameless postmortems today, I'm going to talk about blameless culture.

00:17:26,779 --> 00:17:32,789
Because thinking about from, for example, this example, if when we call the cofounders

00:17:32,789 --> 00:17:37,369
or senior engineers, if they were to say, like, you worked on this for two weeks, how

00:17:37,369 --> 00:17:42,159
did you miss this?

00:17:42,159 --> 00:17:45,579
It doesn't matter if it's a blameless postmortem.

00:17:45,579 --> 00:17:51,869
We have to try to avoid blame as much as possible.

00:17:51,869 --> 00:17:54,198
And one time this happened to me as well.

00:17:54,198 --> 00:17:59,979
One of the engineers tried to send this new feature, and just, you know, two weeks in

00:17:59,979 --> 00:18:04,899
a row he caused a partial outage.

00:18:04,899 --> 00:18:08,489
But when I went when something like that happens, I always think about two things.

00:18:08,489 --> 00:18:12,219
I always think about why we need to be blameless.

00:18:12,219 --> 00:18:15,869
And there I think, again, two things that are most important here.

00:18:15,869 --> 00:18:19,039
The first one is basically blameful comments help no one.

00:18:19,039 --> 00:18:25,819
I mean, it will either just cause people to be scared of making changes.

00:18:25,819 --> 00:18:29,558
Because most of the time, people who make the changes, they are the ones taking the

00:18:29,558 --> 00:18:30,688
risks.

00:18:30,688 --> 00:18:33,239
They might be scared, they might be hiding things.

00:18:33,239 --> 00:18:38,639
They might not be willing to work hard enough after some point.

00:18:38,639 --> 00:18:43,209
So, that is why we need to assume good intentions.

00:18:43,209 --> 00:18:49,669
And basically go and talk with him or her before making public blameful comments.

00:18:49,669 --> 00:18:56,509
This might for example, if an on call engineer misses a lot of alerts and you get the alerts

00:18:56,509 --> 00:19:05,109
during the night, you might say this person is not doing his or her job very well.

00:19:05,109 --> 00:19:06,849
You might feel the blame.

00:19:06,849 --> 00:19:12,989
So, in that case, again, just go talk with him or her and I'm sure you can find a better

00:19:12,989 --> 00:19:13,989
way.

00:19:13,989 --> 00:19:17,619
And the second one is people are not the root cause of incidents.

00:19:17,619 --> 00:19:22,948
Just thinking about this huge incident with AWS, you know, estuary being down, the whole

00:19:22,948 --> 00:19:23,948
time.

00:19:23,948 --> 00:19:30,999
If you read the postmortem, the report, there is a section, an engineer was basically using

00:19:30,999 --> 00:19:40,359
it with the wrong flag and not causing not using that blameful kind of word, but basically

00:19:40,359 --> 00:19:44,919
saying an engineer used a wrong flag and it brought it down.

00:19:44,919 --> 00:19:46,999
And currently that's not the problem.

00:19:46,999 --> 00:19:50,359
AWS didn't have the necessary checks, they didn't automate it very well.

00:19:50,359 --> 00:19:51,359
All that kind of stuff.

00:19:51,359 --> 00:19:57,829
You have to look at the real issues behind this.

00:19:57,829 --> 00:20:05,979
And the next one is, embracing open or what people call sometimes is transparency.

00:20:05,979 --> 00:20:13,119
And the open means being vulnerable, transparent, willing to fail in front of others.

00:20:13,119 --> 00:20:25,869
I mean, being open is key to understand and because sharing knowledge makes everyone smarter.

00:20:25,869 --> 00:20:29,029
We need to encourage people to speak up.

00:20:29,029 --> 00:20:33,229
They might not feel comfortable speaking up with a lot of people.

00:20:33,229 --> 00:20:35,649
Maybe just spoke with them in person that would be great.

00:20:35,649 --> 00:20:40,479
But also everyone in your team, whether they are happy with the on call schedules, whether

00:20:40,479 --> 00:20:42,269
they are affecting their life.

00:20:42,269 --> 00:20:43,629
Basically talk with them.

00:20:43,629 --> 00:20:46,109
And encourage them to speak up.

00:20:46,109 --> 00:20:52,548
We meet with the team every two weeks just to discuss on call and alerts specifically.

00:20:52,548 --> 00:20:55,969
And we need to make information accessible to everyone.

00:20:55,969 --> 00:21:00,649
The information probably should be about on call places, for example.

00:21:00,649 --> 00:21:03,448
Those should be available for anyone to see.

00:21:03,448 --> 00:21:05,649
Anyone to make comments and change.

00:21:05,649 --> 00:21:08,829
And, of course, you have a version control system.

00:21:08,829 --> 00:21:14,799
So, some of the questions we should be answering in this post is, for example, are juniors

00:21:14,799 --> 00:21:16,099
on call at nights?

00:21:16,099 --> 00:21:21,629
We need to know this before hiring energies they need to know.

00:21:21,629 --> 00:21:26,389
Or if there's on call during the night, is there flexibility to work from home the next

00:21:26,389 --> 00:21:29,329
day or start the day later than usual?

00:21:29,329 --> 00:21:37,099
Or are engineers available on call time, or maximum how many times in a month will an

00:21:37,099 --> 00:21:38,649
engineer be on call?

00:21:38,649 --> 00:21:42,179
Questions like this need to be answered so everyone has a better understanding of how

00:21:42,179 --> 00:21:45,249
on call works.

00:21:45,249 --> 00:21:50,419
So, the best way to predict the future is to create it.

00:21:50,419 --> 00:21:54,739
That is why we need to practice incident response, we need to train our people.

00:21:54,739 --> 00:21:58,448
We need to do what we can to get them ready for on call.

00:21:58,448 --> 00:22:02,159
This adds to the stress a lot.

00:22:02,159 --> 00:22:06,548
And there are different practices.

00:22:06,548 --> 00:22:09,558
And the one I really like, I really recommend, is shadowing.

00:22:09,558 --> 00:22:15,789
And in my little poll with like 80 people, it's not that big, but the results are, like,

00:22:15,789 --> 00:22:19,668
if I asked, do you use shadowing for on call or onboarding?

00:22:19,668 --> 00:22:25,969
And the result is 4 6% yes, that is what?

00:22:25,969 --> 00:22:29,668
If they know it, they use it.

00:22:29,668 --> 00:22:33,298
That's why we need to talk about the practices more and more.

00:22:33,298 --> 00:22:38,739
Shadowing, basically, for those who don't know, put an inexperienced engineer with an

00:22:38,739 --> 00:22:40,829
experienced engineer on call.

00:22:40,829 --> 00:22:44,469
They start getting the alerts and work on the issues at the same time.

00:22:44,469 --> 00:22:50,009
Ideally this is during the business hours so they can work together in the same area

00:22:50,009 --> 00:22:52,039
so it's easier.

00:22:52,039 --> 00:22:54,309
It's less stressful for the inexperienced engineer.

00:22:54,309 --> 00:22:55,619
They work together.

00:22:55,619 --> 00:22:58,808
It's like pair programming.

00:22:58,808 --> 00:23:01,879
We also have what we call game days.

00:23:01,879 --> 00:23:04,369
So, we can have, again, step by step instructions.

00:23:04,369 --> 00:23:11,948
Kind of a game, and people come together and simulate it and try to solve the problem and

00:23:11,948 --> 00:23:14,459
get to the end.

00:23:14,459 --> 00:23:17,938
And also, I personally I'm a fan of podcasts.

00:23:17,938 --> 00:23:20,119
I listen to a lot of podcasts.

00:23:20,119 --> 00:23:26,479
If you are interested, listen to the on call nightmare podcast, it's a great one.

00:23:26,479 --> 00:23:30,039
And we have team play books.

00:23:30,039 --> 00:23:35,599
With team play books you can measure your team's health about incident response, on

00:23:35,599 --> 00:23:37,729
call, there are different playbooks for different teams.

00:23:37,729 --> 00:23:41,909
I suggest you check them out as well.

00:23:41,909 --> 00:23:46,849
And one last thing that I think is very important, compensate for on call.

00:23:46,849 --> 00:23:49,558
I mean, most of Europe this is mandatory.

00:23:49,558 --> 00:23:53,149
I don't think it is in the US.

00:23:53,149 --> 00:23:58,099
But I hear it's becoming more and more the practice in our industry.

00:23:58,099 --> 00:24:00,139
So, compensate for on call.

00:24:00,139 --> 00:24:04,339
We used to do, like I think this is okay.

00:24:04,339 --> 00:24:09,339
We used to do for five days of on call, we give one day off and don't expect want engineer

00:24:09,339 --> 00:24:10,369
to work at all.

00:24:10,369 --> 00:24:13,409
Or we could just pay for the extra time.

00:24:13,409 --> 00:24:14,719
Especially for the extra time.

00:24:14,719 --> 00:24:16,808
Outside of business hours.

00:24:16,808 --> 00:24:18,729
This is also possible.

00:24:18,729 --> 00:24:25,229
And the reason, of course, is the on call isn't leisure time as we talked in the beginning.

00:24:25,229 --> 00:24:31,279
We talked about several important things throughout this presentation.

00:24:31,279 --> 00:24:36,659
I think the key, the real key, is giving developers basically asking developers to become on call

00:24:36,659 --> 00:24:39,479
so they give them on call responsibilities.

00:24:39,479 --> 00:24:40,479
Start paging them.

00:24:40,479 --> 00:24:42,369
They feel the pain.

00:24:42,369 --> 00:24:45,709
They start doing a better job and distributing responsibilities.

00:24:45,709 --> 00:24:51,059
So, we don't basically cause our Ops engineers to burn out.

00:24:51,059 --> 00:24:55,089
And we need to create sustainable rotations and clear escalation paths so people feel

00:24:55,089 --> 00:24:56,089
safer.

00:24:56,089 --> 00:25:00,939
People can make changes for the rotations.

00:25:00,939 --> 00:25:04,568
So, make them available for everyone.

00:25:04,568 --> 00:25:07,329
Be open and share knowledge within the company.

00:25:07,329 --> 00:25:09,119
But in the teams.

00:25:09,119 --> 00:25:13,999
This is also especially important with the team label because they know what is best

00:25:13,999 --> 00:25:14,999
for them.

00:25:14,999 --> 00:25:21,129
If you weren't thinking about a strict policy like that will be applied throughout your

00:25:21,129 --> 00:25:23,639
company, a big one especially, it might be a problem.

00:25:23,639 --> 00:25:29,209
So, basically empower your teams, a lot of them to make some decisions for them.

00:25:29,209 --> 00:25:34,139
Also, again, create a blameless culture, not just postmortems.

00:25:34,139 --> 00:25:37,989
Which is very important.

00:25:37,989 --> 00:25:43,319
Embrace effective alerting practices.

00:25:43,319 --> 00:25:44,849
And practice incident response.

00:25:44,849 --> 00:25:48,049
Training is an important part of on call.

00:25:48,049 --> 00:25:51,979
And finally, compensate for on call.

00:25:51,979 --> 00:25:56,629
And as I told you before, on call by definition is stressful and can lead to stress and burnout

00:25:56,629 --> 00:25:57,629
for people.

00:25:57,629 --> 00:26:05,859
Apply these practice wills drive good results in making on call better for your organization.

00:26:05,859 --> 00:26:11,829
And I want to finish my presentation with a code they really like from the site reliability

00:26:11,829 --> 00:26:13,659
workbook from Google.

00:26:13,659 --> 00:26:20,759
They say, we never achieve reliability at the expense of on call of engineering staff.

00:26:20,759 --> 00:26:21,759
Thank you very.

00:26:21,759 --> 00:26:29,448
[ Applause ] >> Thank you so much, Serhat.

00:26:29,448 --> 00:26:32,069
We so, we have a couple minutes for questions.

00:26:32,069 --> 00:26:33,069
I just wanted to do a quick intro to questions.

00:26:33,069 --> 00:26:34,069
One, we do like to use microphones.

00:26:34,069 --> 00:26:35,069
Anybody in a red or white shirt, that's myself and Phoenix in the room today will have a

00:26:35,069 --> 00:26:36,069
microphone.

00:26:36,069 --> 00:26:37,069
Who will like to be the first Q&A person?

00:26:37,069 --> 00:26:38,069
Point and get the microphone.

00:26:38,069 --> 00:26:39,069
Here we go.

00:26:39,069 --> 00:26:45,188
AUDIENCE: You said you had switched back and forth between having on call engineers do

00:26:45,188 --> 00:26:46,709
feature work or just do Ops work.

00:26:46,709 --> 00:26:54,129
Did you come up with any kinds of rule of thumb for when a team should use which of

00:26:54,129 --> 00:26:55,129
approach?

00:26:55,129 --> 00:26:56,409
It depends on the team, you said.

00:26:56,409 --> 00:26:59,829
SERHAT: If we get a lot of others, we talk with energies but we also have reports.

00:26:59,829 --> 00:27:05,188
We know if a team gets a lot of other farce certain period of time how many are in which

00:27:05,188 --> 00:27:07,519
priority label.

00:27:07,519 --> 00:27:12,339
There's a lot of alerts that are low priority, probably get some of them.

00:27:12,339 --> 00:27:18,709
And that means we have to we can give some time for the on call engineer to work on those.

00:27:18,709 --> 00:27:23,408
And also, we are able to easily see the burned down chart, like on call basically can't do

00:27:23,408 --> 00:27:24,829
any feature work.

00:27:24,829 --> 00:27:26,169
It's a good sign.

00:27:26,169 --> 00:27:33,609
After two weeks, after two we see on call spends all of his or her time working on the

00:27:33,609 --> 00:27:35,749
alerts, that's a good sign for us.

00:27:35,749 --> 00:27:39,829
And it's not just on call engineer's job.

00:27:39,829 --> 00:27:44,798
Obviously we didn't expect on call to fix the alerting problems, it's more deep, maybe.

00:27:44,798 --> 00:27:50,229
But they should talk with the team and figure out the best approaches.

00:27:50,229 --> 00:27:53,499
AUDIENCE: Thank you for the session.

00:27:53,499 --> 00:27:55,679
I had two questions.

00:27:55,679 --> 00:28:03,929
First is what do you guys use for creating your playbooks in office?

00:28:03,929 --> 00:28:05,959
Playbooks, different names.

00:28:05,959 --> 00:28:10,829
SERHAT: Right now we keep them in Confluence.

00:28:10,829 --> 00:28:12,249
AUDIENCE: Okay.

00:28:12,249 --> 00:28:13,899
That makes sense.

00:28:13,899 --> 00:28:16,379
SERHAT: But, yeah.

00:28:16,379 --> 00:28:18,389
Yeah, for now it's Confluence.

00:28:18,389 --> 00:28:23,009
But there's going to be an integration between Confluence and on call.

00:28:23,009 --> 00:28:25,339
AUDIENCE: I missed the second part?

00:28:25,339 --> 00:28:32,489
SERHAT: There's going to be an integration between Confluence and on call.

00:28:32,489 --> 00:28:37,479
For a lot of others, we have the round book, directly on on call.

00:28:37,479 --> 00:28:43,678
But for some not very critical alerts, you might have a link to the Confluence page as

00:28:43,678 --> 00:28:44,678
well.

00:28:44,678 --> 00:28:49,779
The version control, it's easier to update if you have a lot of others.

00:28:49,779 --> 00:28:53,009
AUDIENCE: My other question was about automated escalations.

00:28:53,009 --> 00:28:58,288
There are a few open source license tools.

00:28:58,288 --> 00:29:06,349
Is there any recommendation from your side?

00:29:06,349 --> 00:29:07,409
SERHAT: Opsgenie.

00:29:07,409 --> 00:29:09,019
AUDIENCE: Thank you.

00:29:09,019 --> 00:29:12,298
>> We have time for one more question.

00:29:12,298 --> 00:29:15,808
AUDIENCE: Yeah.

00:29:15,808 --> 00:29:22,928
So, when talking about alerting culture and just policies around it, when it comes to

00:29:22,928 --> 00:29:27,959
how an engineer is notified with an alert, letting them define that themselves versus

00:29:27,959 --> 00:29:34,999
company mandated policies, where do you fall on that and why?

00:29:34,999 --> 00:29:39,908
SERHAT: You mean diagnose like first getting a let's say a push notification, whether or

00:29:39,908 --> 00:29:42,568
not you fall back to a phone call or SERHAT: Okay.

00:29:42,568 --> 00:29:48,428
So, our approach is so, we have two things for this one.

00:29:48,428 --> 00:29:55,739
So, we basically give the maybe let this let the on call engineer choose whatever they

00:29:55,739 --> 00:29:57,039
want, first.

00:29:57,039 --> 00:30:05,039
But for very high priority, priority one level alerts, we have a global notification rule.

00:30:05,039 --> 00:30:08,879
Call and send SMS and send a push notification.

00:30:08,879 --> 00:30:13,139
We can't do that globally, we can't enforce this.

00:30:13,139 --> 00:30:16,609
But our approach is let engineers do whatever they want.

00:30:16,609 --> 00:30:22,298
If they want to get a push notification, they can, if they want to get SMS, they can.

00:30:22,298 --> 00:30:26,019
But were certain alerts, we don't want the engineer to miss something.

00:30:26,019 --> 00:30:27,829
That can happen.

00:30:27,829 --> 00:30:35,658
That's why we have global policies that could change the notifications in Opsgenie.

00:30:35,658 --> 00:30:36,719
>> Thank you so much.

00:30:36,719 --> 00:30:39,158
One more round of applause.

00:30:39,158 --> 00:30:39,348

YouTube URL: https://www.youtube.com/watch?v=dfKOiXh6Q50


