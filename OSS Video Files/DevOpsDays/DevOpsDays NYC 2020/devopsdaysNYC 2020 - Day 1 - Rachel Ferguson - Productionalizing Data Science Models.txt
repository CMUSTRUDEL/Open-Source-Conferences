Title: devopsdaysNYC 2020 - Day 1 - Rachel Ferguson - Productionalizing Data Science Models
Publication date: 2020-03-30
Playlist: DevOpsDays NYC 2020
Description: 
	Data science is a research-heavy discipline concerned with staying on the cusp of technological advances. Scientists rapidly experiment with cutting-edge techniques and approaches, often coding in highly interactive, but highly unstructured, mediums like Jupyter notebooks.

How can a team integrate this experimental scripting and model handoff into a stable, defendable product?
Captions: 
	00:00:14,840 --> 00:00:21,350
hello all I'm excited to be today's

00:00:17,690 --> 00:00:22,760
first ignite speaker so I am also I'm

00:00:21,350 --> 00:00:25,189
going to be speaking about production

00:00:22,760 --> 00:00:26,869
Eliza data science models which is

00:00:25,189 --> 00:00:29,330
taking data science scripts and

00:00:26,869 --> 00:00:32,419
converting them into stable production

00:00:29,330 --> 00:00:34,850
code so my name is Rachel Ferguson I

00:00:32,419 --> 00:00:37,550
work as a data science consultant in

00:00:34,850 --> 00:00:40,730
Charlotte North Carolina and my main

00:00:37,550 --> 00:00:43,040
role is facilitating the domain

00:00:40,730 --> 00:00:46,699
knowledge transition between data

00:00:43,040 --> 00:00:49,880
scientists and software engineers so

00:00:46,699 --> 00:00:52,130
your project starts your data scientists

00:00:49,880 --> 00:00:54,620
are so excited they start analyzing data

00:00:52,130 --> 00:00:56,809
they have all these predictions and they

00:00:54,620 --> 00:00:58,219
have a good one so they are so excited

00:00:56,809 --> 00:01:00,320
they give you their model files their

00:00:58,219 --> 00:01:02,480
metrics and their notebook with all the

00:01:00,320 --> 00:01:04,790
code they wrote the problem is they're

00:01:02,480 --> 00:01:08,390
not done yet because notebook code is

00:01:04,790 --> 00:01:09,560
not finished code this is an example of

00:01:08,390 --> 00:01:13,070
something you could write in a notebook

00:01:09,560 --> 00:01:16,460
you can save this checkpoint it commits

00:01:13,070 --> 00:01:18,800
it all good the issue you put this in

00:01:16,460 --> 00:01:21,319
any text editor anything with linting

00:01:18,800 --> 00:01:24,020
and you get a million one errors it's

00:01:21,319 --> 00:01:26,690
very thick with airs so notebooks are

00:01:24,020 --> 00:01:28,729
great but they are extremely flexible to

00:01:26,690 --> 00:01:32,690
the point where you can put anything you

00:01:28,729 --> 00:01:34,520
want in one circle is to take this

00:01:32,690 --> 00:01:36,890
notebook file and then we're going to

00:01:34,520 --> 00:01:39,740
document separate integrate and then

00:01:36,890 --> 00:01:41,720
hopefully if we have time enhance the

00:01:39,740 --> 00:01:48,349
code a little bit using your developers

00:01:41,720 --> 00:01:49,970
skill set so first documentation there's

00:01:48,349 --> 00:01:52,580
nothing better for documentation than

00:01:49,970 --> 00:01:56,240
get any code that your project relies on

00:01:52,580 --> 00:01:58,399
should be in source control but the

00:01:56,240 --> 00:02:00,830
problem is notebooks are notoriously

00:01:58,399 --> 00:02:04,429
very very bad for source control they're

00:02:00,830 --> 00:02:07,940
very thick with images and very hard so

00:02:04,429 --> 00:02:10,759
like control so what we're gonna do is

00:02:07,940 --> 00:02:13,010
convert our notebook into a pure code

00:02:10,759 --> 00:02:15,110
file usually Python and then we'll also

00:02:13,010 --> 00:02:17,930
save any static outputs the data

00:02:15,110 --> 00:02:21,290
scientists had in shared Drive so they

00:02:17,930 --> 00:02:22,819
have access to it next your data science

00:02:21,290 --> 00:02:25,430
is going to take that Python file and

00:02:22,819 --> 00:02:27,319
write a million comments in it all the

00:02:25,430 --> 00:02:29,269
contexts they possibly can

00:02:27,319 --> 00:02:31,099
in case we lose this resource going

00:02:29,269 --> 00:02:33,650
forward we want to have all their

00:02:31,099 --> 00:02:35,329
information in their head then we are

00:02:33,650 --> 00:02:37,819
going to track down all the data they

00:02:35,329 --> 00:02:39,139
used in experimentation time data

00:02:37,819 --> 00:02:41,810
scientists like to use a lot of

00:02:39,139 --> 00:02:44,180
information outside your company we need

00:02:41,810 --> 00:02:45,739
to know what these are how they got them

00:02:44,180 --> 00:02:48,650
and how we can persist them going

00:02:45,739 --> 00:02:51,620
forward so taking all of the extra data

00:02:48,650 --> 00:02:54,379
we also need to store it somewhere we

00:02:51,620 --> 00:02:56,419
need to have a shared fault deterrent

00:02:54,379 --> 00:02:58,249
storage area with all this extra

00:02:56,419 --> 00:03:02,030
information so we need can recreate

00:02:58,249 --> 00:03:04,159
their models if we need to and now we

00:03:02,030 --> 00:03:05,719
need to separate oftentimes you'll get a

00:03:04,159 --> 00:03:08,480
notebook that is just a single notebook

00:03:05,719 --> 00:03:09,739
everything's in it all in one line so

00:03:08,480 --> 00:03:11,090
we're going to take that and we're going

00:03:09,739 --> 00:03:14,299
to separate it into separate different

00:03:11,090 --> 00:03:16,129
files usually there is a data processing

00:03:14,299 --> 00:03:18,409
section of the notebook and then a

00:03:16,129 --> 00:03:19,819
modeling section of the notebook we're

00:03:18,409 --> 00:03:22,370
going to take that and set bring in two

00:03:19,819 --> 00:03:24,620
separate files a separate file for every

00:03:22,370 --> 00:03:27,139
data set cleaning a separate file for

00:03:24,620 --> 00:03:29,659
the model creation training main and

00:03:27,139 --> 00:03:32,329
then if it's used a separate file for

00:03:29,659 --> 00:03:33,590
that prediction file and then we're

00:03:32,329 --> 00:03:34,959
gonna take those separate files and

00:03:33,590 --> 00:03:37,459
we're going to module eyes it more

00:03:34,959 --> 00:03:39,409
usually again it's just freeform thought

00:03:37,459 --> 00:03:41,780
so we're going to take that and we're

00:03:39,409 --> 00:03:44,060
gonna make little functions and build

00:03:41,780 --> 00:03:47,959
upon that so we can much more easily

00:03:44,060 --> 00:03:50,120
integrate all of their stuff and that

00:03:47,959 --> 00:03:52,069
goes along with this this is optional so

00:03:50,120 --> 00:03:53,509
sometimes data scientists like to stay

00:03:52,069 --> 00:03:55,400
in their own world and just provide

00:03:53,509 --> 00:03:57,409
api's where you access their outputs

00:03:55,400 --> 00:03:59,900
sometimes you will actually integrate

00:03:57,409 --> 00:04:04,519
the data itself or the modeling itself

00:03:59,900 --> 00:04:06,079
into your ETL and app repos but if we

00:04:04,519 --> 00:04:07,819
can get all this done and all these test

00:04:06,079 --> 00:04:10,909
cases done and all of our code still

00:04:07,819 --> 00:04:13,009
stable we can begin to enhance which is

00:04:10,909 --> 00:04:16,009
using your developer skill set to

00:04:13,009 --> 00:04:18,590
enhance the data scientist aspects so we

00:04:16,009 --> 00:04:20,389
can do automated metric collection using

00:04:18,590 --> 00:04:22,490
those metrics we can define thresholds

00:04:20,389 --> 00:04:24,440
so when we do deployments for data

00:04:22,490 --> 00:04:25,909
science we can set automatic thresholds

00:04:24,440 --> 00:04:28,610
where they're automatically rejected if

00:04:25,909 --> 00:04:30,050
they don't meet it and your developers

00:04:28,610 --> 00:04:32,630
are likely more comfortable with

00:04:30,050 --> 00:04:34,520
implementing Auto ml kind of solutions

00:04:32,630 --> 00:04:36,199
so once they have an understanding of

00:04:34,520 --> 00:04:37,969
where designs are coming from and their

00:04:36,199 --> 00:04:40,280
context they can get in to look outside

00:04:37,969 --> 00:04:42,760
to more generic solutions that can

00:04:40,280 --> 00:04:46,580
fried benchmarks for your own company

00:04:42,760 --> 00:04:48,590
but yeah my overall arching is data

00:04:46,580 --> 00:04:50,240
science doesn't stop at a notebook your

00:04:48,590 --> 00:04:52,040
data scientists need to be integrated

00:04:50,240 --> 00:04:54,770
with the process afterwards in writing

00:04:52,040 --> 00:04:58,970
test cases and defining the stableness

00:04:54,770 --> 00:05:00,970
around their code but yeah thank you it

00:04:58,970 --> 00:05:02,340
was great being the first talk today

00:05:00,970 --> 00:05:04,620
[Music]

00:05:02,340 --> 00:05:17,920
[Applause]

00:05:04,620 --> 00:05:17,920

YouTube URL: https://www.youtube.com/watch?v=ZRqgvmfGOuo


