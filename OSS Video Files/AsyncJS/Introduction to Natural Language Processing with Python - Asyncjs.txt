Title: Introduction to Natural Language Processing with Python - Asyncjs
Publication date: 2016-07-14
Playlist: AsyncJS
Description: 
	In this talk, Jess Bowden introduces the area of NLP (Natural Language Processing) and a basic introduction of its principles. She uses Python and some of its fundamental NLP packages, such as NLTK, to illustrate examples and topics, demonstrating how to get started with processing and analysing Natural Languages. She also looks at what NLP can be used for, a broad overview of the sub-topics, and how to get yourself started with a demo project.

▼ Links mentioned in the talk ▼

http://www.nltk.org/
https://github.com/yyuu/pyenv
https://github.com/yyuu/pyenv-virtualenv
http://jupyter.org/

▼ Speaker ▼

https://twitter.com/jessicambowden

▼ Event ▼

This talk was part of AsyncJS in May (https://asyncjs.com/introduction-to-nlp-with-python/).

▼ Transcript ▼

https://blog.pusher.com/introduction-to-natural-language-processing-with-python/

▼ Video by Pusher ▼

Pusher is a hosted service with APIs, developer tools and open source libraries that greatly simplify integrating real-time functionality into web and mobile applications. 

Pusher will automatically scale when required, removing all the pain of setting up and maintaining a secure, realtime infrastructure. 

Pusher is already trusted to do so by thousands of developers and companies like GitHub, MailChimp, the Financial Times, Buffer and many more. 

Getting started takes just a few seconds: simply go to pusher.com and create a free account. Happy hacking!

▼ More from Pusher ▼

Subscribe to Pusher: https://www.youtube.com/c/pusherrealtime?sub_confirmation=1
Asyncjs playlist: https://www.youtube.com/playlist?list=PL8xuokhAnn4pHB_R1KRz4_NEgPCUotyb8
Captions: 
	00:00:06,790 --> 00:00:11,240
so today I'm going to be talking about

00:00:08,740 --> 00:00:12,830
natural language processing specifically

00:00:11,240 --> 00:00:16,160
with Python just a bit of an

00:00:12,830 --> 00:00:17,449
introduction so yeah this is what will

00:00:16,160 --> 00:00:19,010
be an overview of what we'll be talking

00:00:17,449 --> 00:00:20,900
about today so I'm going to give a

00:00:19,010 --> 00:00:23,300
little introduction to who I am in case

00:00:20,900 --> 00:00:25,579
you don't know me then a bit of an

00:00:23,300 --> 00:00:28,369
introduction to what natural language

00:00:25,579 --> 00:00:31,099
processing is and what's going on in

00:00:28,369 --> 00:00:32,930
case you don't know and then to why I

00:00:31,099 --> 00:00:35,180
think that you should be using Python

00:00:32,930 --> 00:00:36,800
for NLP some people might disagree

00:00:35,180 --> 00:00:40,219
that's fine

00:00:36,800 --> 00:00:42,859
and then an introduction to like a crash

00:00:40,219 --> 00:00:44,809
course on what the syntax is in Python

00:00:42,859 --> 00:00:46,550
just like some things that are a bit

00:00:44,809 --> 00:00:49,550
different you might not know about sorry

00:00:46,550 --> 00:00:51,370
if you do and then I'll be looking at

00:00:49,550 --> 00:00:56,600
sort of like preparing your data for

00:00:51,370 --> 00:00:59,690
building prototypes and how to get low

00:00:56,600 --> 00:01:02,570
data in Python then a little look at how

00:00:59,690 --> 00:01:05,960
to explore and analyze data just like

00:01:02,570 --> 00:01:07,220
thin things like tokenizing and then

00:01:05,960 --> 00:01:10,070
I'll be looking a little sent a couple

00:01:07,220 --> 00:01:10,970
of little sentiment based projects that

00:01:10,070 --> 00:01:14,180
you could hopefully play around with

00:01:10,970 --> 00:01:17,870
yourself and then looking at some more

00:01:14,180 --> 00:01:19,580
advanced things perhaps so yeah like I

00:01:17,870 --> 00:01:22,520
said I'm Jessica I work at Brown watch

00:01:19,580 --> 00:01:24,800
that down said that just start on the

00:01:22,520 --> 00:01:27,590
data science team I've been there for up

00:01:24,800 --> 00:01:31,520
two years now and that's my twitter

00:01:27,590 --> 00:01:33,200
handle and so yeah natural language

00:01:31,520 --> 00:01:37,400
processing is a really really broad

00:01:33,200 --> 00:01:40,880
topic I'll be trying to cover some basic

00:01:37,400 --> 00:01:42,640
techniques today so it covers like some

00:01:40,880 --> 00:01:44,450
topics like machine translation

00:01:42,640 --> 00:01:47,200
summarizing blocks of text like

00:01:44,450 --> 00:01:50,180
something that got big like some lis

00:01:47,200 --> 00:01:52,490
which is a terrible name spam detection

00:01:50,180 --> 00:01:58,880
sentiment analysis or a couple more like

00:01:52,490 --> 00:02:01,340
really big fields yeah so I think

00:01:58,880 --> 00:02:05,180
pythons great which is the main language

00:02:01,340 --> 00:02:07,370
I use for programming now it's really

00:02:05,180 --> 00:02:10,069
readable and so it makes to like make

00:02:07,370 --> 00:02:14,200
really fast prototypes and it's got like

00:02:10,069 --> 00:02:16,420
really rich support for text analysis

00:02:14,200 --> 00:02:18,670
strings and less

00:02:16,420 --> 00:02:22,930
there's loads of like great available

00:02:18,670 --> 00:02:25,300
NLP libraries like NLT k-space e-text

00:02:22,930 --> 00:02:27,250
blob and there's also some really great

00:02:25,300 --> 00:02:30,100
pausing libraries and I've also just

00:02:27,250 --> 00:02:33,660
added a couple of tools I like using if

00:02:30,100 --> 00:02:35,890
you want to have a looking at sparetime

00:02:33,660 --> 00:02:36,850
so now I'm going to do a little bit of a

00:02:35,890 --> 00:02:40,390
crash course in case you're not familiar

00:02:36,850 --> 00:02:42,130
with Python sorry if you are so the

00:02:40,390 --> 00:02:45,550
first thing to know is that python has

00:02:42,130 --> 00:02:47,260
no brackets for separating your lines of

00:02:45,550 --> 00:02:49,000
text pretty sure they're doing

00:02:47,260 --> 00:02:52,450
JavaScript sorry oh my JavaScript is

00:02:49,000 --> 00:02:55,150
awful and it's really yeah dependent on

00:02:52,450 --> 00:02:58,180
white space for indenting and separating

00:02:55,150 --> 00:03:00,280
new lines there are generally no

00:02:58,180 --> 00:03:03,370
semicolons so whilst this block of sex

00:03:00,280 --> 00:03:05,610
would still run it's not very pythonic

00:03:03,370 --> 00:03:08,620
and you should avoid it because

00:03:05,610 --> 00:03:10,120
semicolons are actually used to separate

00:03:08,620 --> 00:03:11,950
multiple statements like that are used

00:03:10,120 --> 00:03:13,840
on the same line so you might use it

00:03:11,950 --> 00:03:19,420
like that or if you're importing

00:03:13,840 --> 00:03:24,239
multiple lines but even then it's sort

00:03:19,420 --> 00:03:28,080
of avoided strings over in like that and

00:03:24,239 --> 00:03:30,840
you can format strings like that with

00:03:28,080 --> 00:03:32,739
curly braces and the format function

00:03:30,840 --> 00:03:36,280
pretty similar to other languages I

00:03:32,739 --> 00:03:39,310
think and then just a couple on data

00:03:36,280 --> 00:03:41,620
structures so less so the equivalent in

00:03:39,310 --> 00:03:44,590
javascript is arrays you define them

00:03:41,620 --> 00:03:48,310
with square brackets and you iterate

00:03:44,590 --> 00:03:49,840
through them as follows but a nice thing

00:03:48,310 --> 00:03:51,459
to note is that strings and python work

00:03:49,840 --> 00:03:53,430
a bit like lists you can just iterate

00:03:51,459 --> 00:03:55,330
through them and slice them like that

00:03:53,430 --> 00:03:59,860
which is the thing you'll probably see

00:03:55,330 --> 00:04:02,080
me using quite a lot you've also got

00:03:59,860 --> 00:04:04,870
list comprehensions which is just allows

00:04:02,080 --> 00:04:09,430
you to perform some like if conditionals

00:04:04,870 --> 00:04:11,440
on some data and return it into a list

00:04:09,430 --> 00:04:16,390
so this here is just like getting all

00:04:11,440 --> 00:04:18,970
the ones which are even numbers in the

00:04:16,390 --> 00:04:20,410
range of 0 to 10 and that's just I

00:04:18,970 --> 00:04:25,020
thought I'd show the equivalent of an

00:04:20,410 --> 00:04:25,020
actual for loop a lot more verbose

00:04:25,260 --> 00:04:28,210
dictionaries really similar to JSON

00:04:27,760 --> 00:04:29,830
blobs

00:04:28,210 --> 00:04:31,569
it's just key value storage

00:04:29,830 --> 00:04:34,900
so yeah they look really similar to

00:04:31,569 --> 00:04:38,229
Jason and you can just when you actually

00:04:34,900 --> 00:04:40,360
read in Jason in Python it just often

00:04:38,229 --> 00:04:42,190
comes back as dictionaries anyway he

00:04:40,360 --> 00:04:44,979
access the values for keys like this and

00:04:42,190 --> 00:04:47,380
it's right you in them as follows

00:04:44,979 --> 00:04:50,199
it's also worth noting that this is

00:04:47,380 --> 00:04:53,470
called unpacking variables I don't think

00:04:50,199 --> 00:04:55,630
you get it you can do that now okay well

00:04:53,470 --> 00:05:01,630
I don't know about anything else she can

00:04:55,630 --> 00:05:02,650
I don't know about new JavaScript and

00:05:01,630 --> 00:05:04,509
you can also do dictionary

00:05:02,650 --> 00:05:05,800
comprehensions in Python which pretty

00:05:04,509 --> 00:05:08,199
cool and you can also do set

00:05:05,800 --> 00:05:11,139
comprehensions so just here you're just

00:05:08,199 --> 00:05:14,770
iterating through the by using the

00:05:11,139 --> 00:05:16,630
dictionary we defined here and selecting

00:05:14,770 --> 00:05:22,270
all the ones where the first letter of

00:05:16,630 --> 00:05:26,250
the key begins with J pretty nice and

00:05:22,270 --> 00:05:28,419
then the last but not least data

00:05:26,250 --> 00:05:29,949
structure sets which are just like

00:05:28,419 --> 00:05:34,050
really similar to lists but they're

00:05:29,949 --> 00:05:34,050
unordered and they've got no duplicates

00:05:34,590 --> 00:05:40,870
and on the last note the comparing

00:05:38,530 --> 00:05:43,599
values and comparing objects in python

00:05:40,870 --> 00:05:45,669
is as follows so you compare values with

00:05:43,599 --> 00:05:48,520
double equals and objects with the key

00:05:45,669 --> 00:05:53,740
word is and in case you didn't know the

00:05:48,520 --> 00:05:55,180
null keyword is just none in python and

00:05:53,740 --> 00:05:56,560
i also include a couple of links to

00:05:55,180 --> 00:05:59,110
coding style in case any of you are

00:05:56,560 --> 00:06:01,810
interested in making your code super

00:05:59,110 --> 00:06:03,960
pythonic and really annoying with peph

00:06:01,810 --> 00:06:03,960
eight

00:06:04,090 --> 00:06:12,849
so yeah just a little intro to getting

00:06:07,509 --> 00:06:16,270
started with NLP and python in case you

00:06:12,849 --> 00:06:19,060
need to that's how you open and read

00:06:16,270 --> 00:06:23,500
text files from a local file and then

00:06:19,060 --> 00:06:25,030
like this for the online files in case

00:06:23,500 --> 00:06:28,750
you need to read an online text files to

00:06:25,030 --> 00:06:30,639
process and then i'm going to do a

00:06:28,750 --> 00:06:33,849
little introduction to NLT k which is a

00:06:30,639 --> 00:06:37,659
really popular NLP library and Python

00:06:33,849 --> 00:06:39,400
it's quite old and it's not often

00:06:37,659 --> 00:06:40,990
updated now but it's really great for

00:06:39,400 --> 00:06:42,669
educational purposes which is why I'm

00:06:40,990 --> 00:06:43,479
introducing it here it's got like loads

00:06:42,669 --> 00:06:45,969
of

00:06:43,479 --> 00:06:48,189
it's got its got free book included it's

00:06:45,969 --> 00:06:53,710
got like loads of open datasets that

00:06:48,189 --> 00:06:54,909
free you can just use so it's great so

00:06:53,710 --> 00:06:58,240
the first thing I want to go over is

00:06:54,909 --> 00:07:00,849
tokenizing so tokenizing is where you

00:06:58,240 --> 00:07:04,210
just split your document up into like

00:07:00,849 --> 00:07:06,909
logical chunks which is usually group

00:07:04,210 --> 00:07:08,939
like broken up by sentences so if I

00:07:06,909 --> 00:07:12,580
wanted to tokenize the first line of

00:07:08,939 --> 00:07:13,629
from isin alice in wonderland' we'd end

00:07:12,580 --> 00:07:16,330
up looking like this if i use the

00:07:13,629 --> 00:07:18,069
default and ltk tokenizer and it just

00:07:16,330 --> 00:07:22,870
breaks it up on it looks like

00:07:18,069 --> 00:07:25,150
punctuation and spaces the next thing is

00:07:22,870 --> 00:07:27,009
stammers and limit eyes errs they

00:07:25,150 --> 00:07:30,099
basically just reduce words - they're

00:07:27,009 --> 00:07:33,270
like normalized form so like Ann would

00:07:30,099 --> 00:07:35,889
become be and cars would become car

00:07:33,270 --> 00:07:38,099
that's how you use a stemmer and this is

00:07:35,889 --> 00:07:40,870
how you the lemma tiser in air okay so

00:07:38,099 --> 00:07:43,300
they look like they do pretty much the

00:07:40,870 --> 00:07:46,120
same thing but stammers are more naive

00:07:43,300 --> 00:07:49,689
and they don't like they don't analyze

00:07:46,120 --> 00:07:51,550
the text like a lemma ties it us but

00:07:49,689 --> 00:07:54,639
they're a lot faster so if you just want

00:07:51,550 --> 00:07:56,830
to chunk your text and just have it in a

00:07:54,639 --> 00:07:58,689
comparable format then you're better off

00:07:56,830 --> 00:08:00,550
using lemma Tice's if you just want to

00:07:58,689 --> 00:08:02,409
cluster the text the similar text in

00:08:00,550 --> 00:08:04,599
some way but like so you'll notice

00:08:02,409 --> 00:08:07,330
things like the e if Alice has just been

00:08:04,599 --> 00:08:09,159
chopped off but that's not plural just

00:08:07,330 --> 00:08:13,749
because it's got an e on the end same

00:08:09,159 --> 00:08:21,189
with like Louis but Oh Carol as well as

00:08:13,749 --> 00:08:23,800
really crap limit izing is like just the

00:08:21,189 --> 00:08:27,639
same as stemming so it's still like

00:08:23,800 --> 00:08:29,199
reducing it to its like normal form this

00:08:27,639 --> 00:08:31,870
one doesn't work so well because I

00:08:29,199 --> 00:08:35,079
haven't added in like the part of speech

00:08:31,870 --> 00:08:37,180
that it is shall come to later but it

00:08:35,079 --> 00:08:38,979
just basically considers the context and

00:08:37,180 --> 00:08:40,810
it doesn't just like do it naively and

00:08:38,979 --> 00:08:43,289
go through and just chop off where it

00:08:40,810 --> 00:08:43,289
sees an S

00:08:45,470 --> 00:08:55,730
ya know just from the end and then ya

00:08:53,870 --> 00:08:58,880
select plurals and then lemma tithers

00:08:55,730 --> 00:09:00,140
consider the context okay so now I'm

00:08:58,880 --> 00:09:06,380
going to look at exploring and analyzing

00:09:00,140 --> 00:09:07,940
data so the first thing that's quite fun

00:09:06,380 --> 00:09:10,280
that you can do without a is explore the

00:09:07,940 --> 00:09:12,650
frequency distribution so we can try

00:09:10,280 --> 00:09:14,660
find out which are the most importance

00:09:12,650 --> 00:09:20,650
in our text so to do this we can just

00:09:14,660 --> 00:09:23,270
use the freak dist package from NLT k

00:09:20,650 --> 00:09:25,940
run it against our set of individual

00:09:23,270 --> 00:09:30,830
tokens from item on land and extract the

00:09:25,940 --> 00:09:34,310
top 25 most common most informative and

00:09:30,830 --> 00:09:37,100
most common tokens from the text so

00:09:34,310 --> 00:09:39,970
that's what it looks like it's kind of

00:09:37,100 --> 00:09:43,990
not very informative because it's kept

00:09:39,970 --> 00:09:48,670
Connor's and punctuation and stop words

00:09:43,990 --> 00:09:51,170
so it's just full of rubbish really but

00:09:48,670 --> 00:09:53,270
they've been included because they are

00:09:51,170 --> 00:09:55,250
like evenly distributed throughout the

00:09:53,270 --> 00:09:57,980
text so it makes logical sense but it's

00:09:55,250 --> 00:10:00,370
not very useful for us so we can insert

00:09:57,980 --> 00:10:03,200
a look at the opposite which are the

00:10:00,370 --> 00:10:05,120
yeah the ones that aren't frequently

00:10:03,200 --> 00:10:06,800
occurring at all but again I've never

00:10:05,120 --> 00:10:08,750
heard of Brandi and Alice in Wonderland

00:10:06,800 --> 00:10:17,200
so it doesn't really tell me about about

00:10:08,750 --> 00:10:19,580
the text so instead we could look at the

00:10:17,200 --> 00:10:24,890
still in a frequency distribution but

00:10:19,580 --> 00:10:28,250
maybe looking for longer words which is

00:10:24,890 --> 00:10:32,900
definitely more useful so like Griffin

00:10:28,250 --> 00:10:37,550
and creatures and mushroom what's that

00:10:32,900 --> 00:10:41,380
one yeah so they're more informative

00:10:37,550 --> 00:10:41,380
words but perhaps not quite what we want

00:10:42,280 --> 00:10:48,170
and the next part on Tourette's part of

00:10:44,630 --> 00:10:51,310
speech tagging which is also known as

00:10:48,170 --> 00:10:53,750
pulse tagging so it's where you extract

00:10:51,310 --> 00:10:56,270
given a sentence or something like that

00:10:53,750 --> 00:10:57,110
what whether they're like each token is

00:10:56,270 --> 00:11:01,640
a verb or an

00:10:57,110 --> 00:11:05,240
adjectives or just bit punctuation so

00:11:01,640 --> 00:11:09,320
this is how we tokenize using nrt k and

00:11:05,240 --> 00:11:10,430
it returned and this is using so loads

00:11:09,320 --> 00:11:13,220
of different libraries have their own

00:11:10,430 --> 00:11:16,640
like versions of how they represent the

00:11:13,220 --> 00:11:19,940
tags which is really annoying so this

00:11:16,640 --> 00:11:21,350
one uses the porter stemmer tag set so

00:11:19,940 --> 00:11:25,310
like i don't know them all off the top

00:11:21,350 --> 00:11:29,839
of my head but nnp is like a proper noun

00:11:25,310 --> 00:11:32,560
and then vb is obviously a verb and then

00:11:29,839 --> 00:11:36,440
we've got like prepositions nouns

00:11:32,560 --> 00:11:38,089
conjunctions using a frequency

00:11:36,440 --> 00:11:43,010
distribution and from learning how we

00:11:38,089 --> 00:11:44,810
can post our sentences we can consider

00:11:43,010 --> 00:11:47,140
the frequency distribution of the types

00:11:44,810 --> 00:11:50,149
of tags throughout Alice and wonderland

00:11:47,140 --> 00:11:52,459
so again not very interesting the most

00:11:50,149 --> 00:11:54,470
common is nouns injunctions determiners

00:11:52,459 --> 00:12:01,610
prepositions it's kind of what you'd

00:11:54,470 --> 00:12:05,680
expect but then from that we could look

00:12:01,610 --> 00:12:08,269
at try find more interesting words again

00:12:05,680 --> 00:12:10,220
it was difficult earlier so we could

00:12:08,269 --> 00:12:14,390
this is looking more informative already

00:12:10,220 --> 00:12:15,980
so we can this go through the most

00:12:14,390 --> 00:12:18,500
common ones we're looking at before and

00:12:15,980 --> 00:12:21,949
extracting the proper nouns so offers

00:12:18,500 --> 00:12:24,019
the Alice queen but it's included a

00:12:21,949 --> 00:12:26,839
bunch of punctuation you can't really

00:12:24,019 --> 00:12:28,070
know why that's happened without like

00:12:26,839 --> 00:12:31,399
going through and analyzing the

00:12:28,070 --> 00:12:33,920
individual sentences but the NL CK pause

00:12:31,399 --> 00:12:36,140
tiger is not amazing by any means and

00:12:33,920 --> 00:12:42,560
this is on like properly written text so

00:12:36,140 --> 00:12:44,269
it would just fall apart on tweets so

00:12:42,560 --> 00:12:46,310
yeah now I want to have a little look at

00:12:44,269 --> 00:12:48,470
sentiment projects sort of working on

00:12:46,310 --> 00:12:53,660
the building blocks of what we have a

00:12:48,470 --> 00:12:55,190
look at before tokenization and so one

00:12:53,660 --> 00:12:56,810
of the common approaches to sentiment

00:12:55,190 --> 00:12:59,209
analysis while it's not super clever is

00:12:56,810 --> 00:12:59,630
rule-based which is exactly what it

00:12:59,209 --> 00:13:02,410
sounds like

00:12:59,630 --> 00:13:07,270
matching finding rules in our text and

00:13:02,410 --> 00:13:10,100
to find out the polarity of the text and

00:13:07,270 --> 00:13:10,850
so I've just stolen a bunch of these

00:13:10,100 --> 00:13:14,449
from

00:13:10,850 --> 00:13:18,050
is because we're monster but I can

00:13:14,449 --> 00:13:19,940
analyze so building from what we have

00:13:18,050 --> 00:13:23,000
for is I'm going to take the tokenizer

00:13:19,940 --> 00:13:26,209
and split one of our reviews the its

00:13:23,000 --> 00:13:29,360
Captain America into tokens and then

00:13:26,209 --> 00:13:32,420
pause tag them and turn those so that's

00:13:29,360 --> 00:13:35,750
we can see here and then I've just built

00:13:32,420 --> 00:13:37,130
a list of handcrafted rules and that's

00:13:35,750 --> 00:13:43,880
definitely not the way you do it in real

00:13:37,130 --> 00:13:46,430
life but it's good enough for this so

00:13:43,880 --> 00:13:49,610
the first way we can do it is go through

00:13:46,430 --> 00:13:51,110
and look at the repeat so the first

00:13:49,610 --> 00:13:52,790
review very entertaining and a for

00:13:51,110 --> 00:13:57,800
tighter production of Marvel's recent

00:13:52,790 --> 00:14:00,380
output so go through and if I find one

00:13:57,800 --> 00:14:03,680
of the words that's been in our list of

00:14:00,380 --> 00:14:06,319
words increment the count the score by

00:14:03,680 --> 00:14:08,779
one or if I find it in the negative

00:14:06,319 --> 00:14:10,610
reviews in the negative rule story I

00:14:08,779 --> 00:14:13,610
want to decrement it by one so a really

00:14:10,610 --> 00:14:17,360
simple approach but and it just outputs

00:14:13,610 --> 00:14:19,670
zero because well there was entertain

00:14:17,360 --> 00:14:23,689
but it didn't find that because we

00:14:19,670 --> 00:14:27,920
didn't lemma ties it so next up we can

00:14:23,689 --> 00:14:29,959
add Lammert ization but there's an awful

00:14:27,920 --> 00:14:34,220
lot to go through so I've added it to

00:14:29,959 --> 00:14:36,199
just look at adjectives and do the same

00:14:34,220 --> 00:14:38,000
again and it's found entertaining

00:14:36,199 --> 00:14:41,329
because we had to entertain in the rule

00:14:38,000 --> 00:14:44,899
set so we can just build upon it like

00:14:41,329 --> 00:14:50,120
this and then you can improve it further

00:14:44,899 --> 00:14:52,459
maybe by looking at words that increment

00:14:50,120 --> 00:14:55,939
the meaning of things like if it's

00:14:52,459 --> 00:14:57,459
really great or very great or two

00:14:55,939 --> 00:15:00,380
brilliant

00:14:57,459 --> 00:15:03,380
might be even better I don't know tweets

00:15:00,380 --> 00:15:07,009
are terrible these days so and then it

00:15:03,380 --> 00:15:08,810
from here we can see that very is it's

00:15:07,009 --> 00:15:10,970
very entertaining so it increases the

00:15:08,810 --> 00:15:13,880
score even more it's just like this

00:15:10,970 --> 00:15:15,620
isn't a great approach but it's just an

00:15:13,880 --> 00:15:19,160
example of like roughly how rule-based

00:15:15,620 --> 00:15:20,569
approach could work but then you could

00:15:19,160 --> 00:15:23,209
take it even further and just build upon

00:15:20,569 --> 00:15:24,620
it so you could in a similar way we had

00:15:23,209 --> 00:15:26,210
words increment

00:15:24,620 --> 00:15:30,380
seeing it like very we could add

00:15:26,210 --> 00:15:31,850
modifiers for words like not or we could

00:15:30,380 --> 00:15:46,640
add things that decrement it like oh

00:15:31,850 --> 00:15:49,730
it's a little bit good what for a

00:15:46,640 --> 00:15:53,360
rule-based approach like I you get

00:15:49,730 --> 00:15:54,260
people to like markup themselves I guess

00:15:53,360 --> 00:15:55,940
so

00:15:54,260 --> 00:15:58,460
get people to mark off a bunch of them

00:15:55,940 --> 00:16:01,120
and then you can go well if it if it

00:15:58,460 --> 00:16:04,640
agrees with them I'm probably right and

00:16:01,120 --> 00:16:06,350
but which brings me sort of onto a knife

00:16:04,640 --> 00:16:11,740
based sentiment analysis which is a bit

00:16:06,350 --> 00:16:20,600
more sophisticated I suppose which is a

00:16:11,740 --> 00:16:24,710
my words of film yeah it's a I will just

00:16:20,600 --> 00:16:26,780
move on words are not coming okay so

00:16:24,710 --> 00:16:29,740
this is how you can build a super simple

00:16:26,780 --> 00:16:34,040
knife based classifier with NLT K and

00:16:29,740 --> 00:16:37,160
NLT k9 phase works by using training

00:16:34,040 --> 00:16:39,110
data so I've gone and find a found bunch

00:16:37,160 --> 00:16:41,120
of tweets which are already marked up so

00:16:39,110 --> 00:16:44,150
it will be loads of tweets that someone

00:16:41,120 --> 00:16:46,580
has hand annotated for a very long time

00:16:44,150 --> 00:16:52,730
and said this one's positive this one's

00:16:46,580 --> 00:16:54,500
negative and it's very exhausting and so

00:16:52,730 --> 00:16:56,540
then I can go through and I can split my

00:16:54,500 --> 00:16:59,090
data into training data and testing data

00:16:56,540 --> 00:17:00,920
so training data is what the classifier

00:16:59,090 --> 00:17:02,930
will use and testing data is so I can

00:17:00,920 --> 00:17:11,030
just see later if it's worked as much

00:17:02,930 --> 00:17:13,070
like as well as I hoped and so oh yeah

00:17:11,030 --> 00:17:14,900
that's just the polarity is just like

00:17:13,070 --> 00:17:17,400
the positive or negative for some reason

00:17:14,900 --> 00:17:22,580
the person sees four instead of one

00:17:17,400 --> 00:17:22,580
oh it's so strange and I understand that

00:17:25,460 --> 00:17:35,480
yeah I they just he's four and zero I

00:17:28,620 --> 00:17:35,480
don't know why yeah it seems so obvious

00:17:36,770 --> 00:17:45,060
and so yeah we split our data and I've

00:17:41,940 --> 00:17:47,580
just processed it so that it's in a

00:17:45,060 --> 00:17:49,710
format that makes easier for each tweet

00:17:47,580 --> 00:17:54,410
if it's supposed to tweet I've put it in

00:17:49,710 --> 00:17:56,880
a tuple with the tweet and the sentiment

00:17:54,410 --> 00:18:01,490
in a list called either positive or

00:17:56,880 --> 00:18:03,750
negative so this is how it looks now

00:18:01,490 --> 00:18:06,420
this is a sample of like the negative

00:18:03,750 --> 00:18:15,860
and positive it's just the tweet and the

00:18:06,420 --> 00:18:20,060
sentiment um and now just ridiculous Oh

00:18:15,860 --> 00:18:23,160
how's it ever going to learn now

00:18:20,060 --> 00:18:24,390
breaking each down to a bag of words and

00:18:23,160 --> 00:18:26,820
just make sure it's kept with the

00:18:24,390 --> 00:18:28,800
sentiment for now but removing those

00:18:26,820 --> 00:18:30,150
that are like really small words because

00:18:28,800 --> 00:18:37,500
they're not going to be informative to

00:18:30,150 --> 00:18:40,740
us this gives us an actual bag of words

00:18:37,500 --> 00:18:45,180
for all the tweets so before we had just

00:18:40,740 --> 00:18:47,520
a group for we still knew whether they

00:18:45,180 --> 00:18:49,710
were positive and negative so we keep

00:18:47,520 --> 00:18:51,540
track of them from before but now we've

00:18:49,710 --> 00:18:55,770
just got like an honor an ambiguous bag

00:18:51,540 --> 00:19:01,010
of words for all of the tweets which

00:18:55,770 --> 00:19:02,790
lets us build this so now building a

00:19:01,010 --> 00:19:04,710
frequency distribution like we did

00:19:02,790 --> 00:19:08,910
before so we can find out the most

00:19:04,710 --> 00:19:11,640
informative features from from this from

00:19:08,910 --> 00:19:15,180
this group of words and we go through

00:19:11,640 --> 00:19:18,450
and extract these features so that when

00:19:15,180 --> 00:19:20,070
we're given a doc so sorry so that when

00:19:18,450 --> 00:19:24,300
we're given a document we can find if

00:19:20,070 --> 00:19:26,370
any of the features match up with sorry

00:19:24,300 --> 00:19:27,960
so we pass it in a document like all

00:19:26,370 --> 00:19:30,450
rock stars back home while some of us

00:19:27,960 --> 00:19:31,200
freshen up others watch em magic Lakers

00:19:30,450 --> 00:19:34,080
game then we'll settle

00:19:31,200 --> 00:19:35,310
reader suite and rock in Florida okay so

00:19:34,080 --> 00:19:38,970
we split that off and then we can go

00:19:35,310 --> 00:19:42,050
through and find out if any of our any

00:19:38,970 --> 00:19:44,780
anything in this tweet is matched with

00:19:42,050 --> 00:19:46,740
things from our training data so we can

00:19:44,780 --> 00:19:49,890
ultimately end up finding out whether

00:19:46,740 --> 00:19:59,040
it's a ultimately classifier using the

00:19:49,890 --> 00:20:02,550
classifier and then from that training

00:19:59,040 --> 00:20:05,220
set we just built we can build a classic

00:20:02,550 --> 00:20:06,480
natural classifier and this is an output

00:20:05,220 --> 00:20:09,030
of what it thinks is the most

00:20:06,480 --> 00:20:12,900
informative features so the features

00:20:09,030 --> 00:20:17,370
which bear the most weight too for what

00:20:12,900 --> 00:20:19,470
we the classifier we just built so if

00:20:17,370 --> 00:20:22,020
something contains cancer it's probably

00:20:19,470 --> 00:20:24,300
like it's a twelve to one possibility

00:20:22,020 --> 00:20:25,620
that it's negative whereas if it

00:20:24,300 --> 00:20:27,450
contains love is it ten to one

00:20:25,620 --> 00:20:28,830
possibility that it's positive because

00:20:27,450 --> 00:20:34,200
that's what it learnt from the data and

00:20:28,830 --> 00:20:36,210
rightly so and but so now we've got this

00:20:34,200 --> 00:20:39,180
we can classify some tweets we put aside

00:20:36,210 --> 00:20:41,250
at the beginning so we can extract one

00:20:39,180 --> 00:20:43,680
of the positive tweets and classify it

00:20:41,250 --> 00:20:45,450
with our new classifier and it

00:20:43,680 --> 00:20:48,680
classifies the positive one is positive

00:20:45,450 --> 00:20:48,680
and the negative one is negative

00:20:51,560 --> 00:20:54,990
yes

00:20:52,770 --> 00:20:56,850
so that's that's been classified as

00:20:54,990 --> 00:20:59,250
positive and it's was marked up by a

00:20:56,850 --> 00:21:02,100
person as positive I haven't done like a

00:20:59,250 --> 00:21:04,050
thorough investigation on all 100 tweets

00:21:02,100 --> 00:21:06,480
I put aside I probably should but I

00:21:04,050 --> 00:21:08,610
think they only put about one hundred

00:21:06,480 --> 00:21:11,580
tweets aside and that's probably not

00:21:08,610 --> 00:21:13,980
enough to so naive Bayes is entirely

00:21:11,580 --> 00:21:17,460
depending on how much data for it which

00:21:13,980 --> 00:21:24,240
also makes it quite hard to see how well

00:21:17,460 --> 00:21:25,650
it's yeah you sorry yeah the more data

00:21:24,240 --> 00:21:27,870
you give to a nice very classified the

00:21:25,650 --> 00:21:29,940
better it will perform but obviously

00:21:27,870 --> 00:21:31,770
then you've got the alternative of

00:21:29,940 --> 00:21:34,080
having to market RAZR data and if

00:21:31,770 --> 00:21:36,480
something doesn't classify correctly as

00:21:34,080 --> 00:21:38,130
we'll see later it's kind of hard to see

00:21:36,480 --> 00:21:39,840
where it's gone wrong and you just have

00:21:38,130 --> 00:21:41,190
to throw more data at it and it if it

00:21:39,840 --> 00:21:43,320
comes across a feature it hasn't seen

00:21:41,190 --> 00:21:44,669
before it's not it's not going to be our

00:21:43,320 --> 00:21:49,169
classifier or it's going to cut

00:21:44,669 --> 00:21:50,940
quiet incorrectly so now I thought I'd

00:21:49,169 --> 00:21:52,379
just do a little demo on like so we can

00:21:50,940 --> 00:21:54,899
look at maybe the sentiment that

00:21:52,379 --> 00:21:58,649
co-occurs with with along with Smiley's

00:21:54,899 --> 00:22:06,210
using the classifier we've already built

00:21:58,649 --> 00:22:11,159
you know loading in tweets this is a lot

00:22:06,210 --> 00:22:12,389
of code right I'm not going to go

00:22:11,159 --> 00:22:13,619
through and explain all of this because

00:22:12,389 --> 00:22:16,129
it'd be really boring but basically

00:22:13,619 --> 00:22:19,440
these are the Unicode ranges for emojis

00:22:16,129 --> 00:22:20,850
and they're in like two different ranges

00:22:19,440 --> 00:22:24,960
which is why they've had to be compiled

00:22:20,850 --> 00:22:27,119
separately so it just finds any of those

00:22:24,960 --> 00:22:29,609
ranges in a tweet I'm going through all

00:22:27,119 --> 00:22:34,700
of this and then go through and

00:22:29,609 --> 00:22:34,700
classifies a tweet and if a tweet exists

00:22:35,059 --> 00:22:42,119
it finds the emoji in our dictionary and

00:22:39,749 --> 00:22:48,359
increments one to it or increments one

00:22:42,119 --> 00:22:50,129
to it if it's a negative tweet and then

00:22:48,359 --> 00:22:54,570
that's the result of all our emojis

00:22:50,129 --> 00:22:57,480
below but it will be a bit more useful

00:22:54,570 --> 00:22:58,859
in a graph so I don't think this is

00:22:57,480 --> 00:23:00,749
going to be super informative but we can

00:22:58,859 --> 00:23:03,690
have a look anyway well it's not bad

00:23:00,749 --> 00:23:07,590
so the crying emoji appears far more

00:23:03,690 --> 00:23:11,429
often with negative tweets then then

00:23:07,590 --> 00:23:15,389
half then pills with positive tweets the

00:23:11,429 --> 00:23:16,799
happy emoji appears more or less an

00:23:15,389 --> 00:23:18,779
equal number of times of both which

00:23:16,799 --> 00:23:23,700
makes me think that it's not a very good

00:23:18,779 --> 00:23:26,609
classifier and the crying the joyful

00:23:23,700 --> 00:23:29,999
tweet that moji is appearing with

00:23:26,609 --> 00:23:31,139
negative farm on the positive so I don't

00:23:29,999 --> 00:23:33,840
think this class before I hadn't

00:23:31,139 --> 00:23:37,489
anywhere near enough data or maybe

00:23:33,840 --> 00:23:37,489
people around really weird tweets

00:23:38,450 --> 00:23:50,210
ah no because I don't think well that I

00:23:48,740 --> 00:23:57,410
don't think there were any emojis in the

00:23:50,210 --> 00:23:59,840
data set I had this is just sorry this

00:23:57,410 --> 00:24:02,330
is just separate data that I found which

00:23:59,840 --> 00:24:04,430
I just gathered myself to try maximize

00:24:02,330 --> 00:24:05,690
the amount of emojis that I could get

00:24:04,430 --> 00:24:09,410
back but there weren't many and then

00:24:05,690 --> 00:24:10,670
tested it against the so I've not used

00:24:09,410 --> 00:24:17,600
the training data I've not been nice to

00:24:10,670 --> 00:24:22,280
it it's just completely new data yes

00:24:17,600 --> 00:24:25,310
okay yeah

00:24:22,280 --> 00:24:27,590
so there's yeah there's still a lot

00:24:25,310 --> 00:24:30,680
going on beyond NLT Kay like it's quite

00:24:27,590 --> 00:24:32,120
a limited library really and I hope that

00:24:30,680 --> 00:24:35,120
some of these demos have given you an

00:24:32,120 --> 00:24:37,310
idea of what you can go away and do if

00:24:35,120 --> 00:24:39,560
you're actually interested in going like

00:24:37,310 --> 00:24:42,710
far beyond and RTK there's a lot of

00:24:39,560 --> 00:24:45,050
interesting and better faster libraries

00:24:42,710 --> 00:24:46,820
about at the moment like there's a

00:24:45,050 --> 00:24:49,040
Python I recalled Spacey which is really

00:24:46,820 --> 00:24:50,840
cool which has like built-in named

00:24:49,040 --> 00:24:55,910
entity recognition and tokenization

00:24:50,840 --> 00:24:58,000
which is far superior to NRT KS and you

00:24:55,910 --> 00:24:59,720
might have heard of the Google's

00:24:58,000 --> 00:25:02,060
dependency files that they came out

00:24:59,720 --> 00:25:03,950
recently which is open source I think so

00:25:02,060 --> 00:25:05,330
there's a lot more things you can look

00:25:03,950 --> 00:25:06,710
at that a lot more relevant but I hope

00:25:05,330 --> 00:25:09,560
this is that sort of giving you an idea

00:25:06,710 --> 00:25:14,330
of getting started and it's not too hard

00:25:09,560 --> 00:25:17,590
to really party Mac parts of face hot no

00:25:14,330 --> 00:25:17,590
party m'q party face

00:25:17,980 --> 00:25:25,730
and it's not get help okay you can this

00:25:23,720 --> 00:25:27,769
is just a Jupiter notebook so you can

00:25:25,730 --> 00:25:31,450
just run it yourself and it should be

00:25:27,769 --> 00:25:31,450

YouTube URL: https://www.youtube.com/watch?v=IMKweOTFjXw


