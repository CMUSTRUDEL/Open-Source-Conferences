Title: Building a K8s Operator for a Distributed Database – Natalie Pistunovich
Publication date: 2021-04-28
Playlist: DevX Conf
Description: 
	How did we build a k8s operator that allows 100% up time for a high availability high workload database? Operating a distributed high load, high throughput database in the cloud comes with several interesting challenges. In order to manage real-time serving of mission critical workloads at 100% availability we developed a Kubernetes operator that handles the operational complexities.

We needed to handle the following requirements:

Apply live patches
Replace live cluster with tens of nodes
Handle degraded/crashed nodes
Under these conditions:

High Availability
remain 100% online with no down time
Operate under very high workloads and traffic
Manage replicated records across different hardware failure groups (rack awareness)
Due to its stateful nature and the type of workloads that are usually handled, cluster management and recovery are non-trivial. We are using the Operators API to handle that complexity and control the clusters from within Kubernetes. In this talk we’ll cover the steps we took to plan and execute and the challenges we faced and share the best practices.

About DevX Conf:
DevX Conf is a two-day virtual conference. It's a space where creators collaborate, listen, discuss and declutter our workflows, toolchains, and minds. To improve developer experience. Organized by Gitpod.
https://devxconf.org/
Captions: 
	00:00:13,120 --> 00:00:16,080
hi everyone

00:00:14,400 --> 00:00:18,000
welcome to the talk about building a

00:00:16,080 --> 00:00:18,880
kubernetes operator for a distributed

00:00:18,000 --> 00:00:23,840
database

00:00:18,880 --> 00:00:26,240
at the very first devx conf exciting

00:00:23,840 --> 00:00:28,720
my name is natalie vistanovic i'm a

00:00:26,240 --> 00:00:31,760
developer advocate lead at aerospike

00:00:28,720 --> 00:00:33,840
a google developer expert for co an open

00:00:31,760 --> 00:00:36,399
ai developer ambassador

00:00:33,840 --> 00:00:37,200
and i organized the berlin user groups

00:00:36,399 --> 00:00:40,160
for go

00:00:37,200 --> 00:00:41,200
and women techmakers and the conferences

00:00:40,160 --> 00:00:45,200
goprocon europe

00:00:41,200 --> 00:00:46,800
cloud9 hey ai and besides berlin

00:00:45,200 --> 00:00:48,640
you're welcome to follow me on twitter

00:00:46,800 --> 00:00:50,879
at nataliepiss

00:00:48,640 --> 00:00:52,800
and if you learn anything new or

00:00:50,879 --> 00:00:53,840
interesting from this talk which i hope

00:00:52,800 --> 00:00:57,440
you do

00:00:53,840 --> 00:01:01,520
please tweet and tag me but also

00:00:57,440 --> 00:01:01,520
tag the conference at the vex conf

00:01:01,600 --> 00:01:06,080
so that's on our agenda today we'll talk

00:01:04,640 --> 00:01:08,000
what are

00:01:06,080 --> 00:01:09,200
kubernetes operators we'll talk about

00:01:08,000 --> 00:01:11,680
that

00:01:09,200 --> 00:01:13,920
then we'll talk a little bit about what

00:01:11,680 --> 00:01:16,000
is aerospike so we can understand

00:01:13,920 --> 00:01:17,680
better the high level design of it which

00:01:16,000 --> 00:01:19,119
i'll introduce you to

00:01:17,680 --> 00:01:20,960
and we'll focus on some of the

00:01:19,119 --> 00:01:21,520
engineering challenges we faced when we

00:01:20,960 --> 00:01:28,240
developed

00:01:21,520 --> 00:01:31,280
ours so what is a kubernetes operator

00:01:28,240 --> 00:01:33,119
kubernetes is designed for automation

00:01:31,280 --> 00:01:36,079
matter of fact it offers it out of the

00:01:33,119 --> 00:01:39,360
box and you can use it to automate

00:01:36,079 --> 00:01:41,520
deploying and running workloads and also

00:01:39,360 --> 00:01:44,000
how kubernetes does that

00:01:41,520 --> 00:01:45,920
is something you can automate the core

00:01:44,000 --> 00:01:49,119
of the kubernetes control plane

00:01:45,920 --> 00:01:49,119
is the api server

00:01:49,600 --> 00:01:55,119
it exposes an http api that lets users

00:01:53,040 --> 00:01:56,719
different parts of the cluster and the

00:01:55,119 --> 00:02:00,159
external components

00:01:56,719 --> 00:02:03,119
communicate with each other operators

00:02:00,159 --> 00:02:04,880
are clients of the kubernetes api they

00:02:03,119 --> 00:02:09,039
act as controllers

00:02:04,880 --> 00:02:11,440
for a custom resource

00:02:09,039 --> 00:02:13,200
in kubernetes controllers are control

00:02:11,440 --> 00:02:14,319
loops that watch the state of your

00:02:13,200 --> 00:02:17,360
cluster

00:02:14,319 --> 00:02:19,920
and then they make or request a change

00:02:17,360 --> 00:02:21,440
where needed with the purpose of moving

00:02:19,920 --> 00:02:24,720
the current cluster state

00:02:21,440 --> 00:02:27,760
closer to the desired state a controller

00:02:24,720 --> 00:02:30,000
tracks at least one kubernetes resource

00:02:27,760 --> 00:02:30,000
type

00:02:30,160 --> 00:02:34,800
in our example operator is a controller

00:02:33,040 --> 00:02:37,360
for a custom resource

00:02:34,800 --> 00:02:39,519
a custom resource is an object that

00:02:37,360 --> 00:02:42,160
extends the kubernetes api

00:02:39,519 --> 00:02:43,120
or allows you to introduce your api into

00:02:42,160 --> 00:02:46,879
the project

00:02:43,120 --> 00:02:47,920
or a cluster a crd a custom resource

00:02:46,879 --> 00:02:50,720
definition

00:02:47,920 --> 00:02:51,440
is a file that defines your own object

00:02:50,720 --> 00:02:54,080
and it

00:02:51,440 --> 00:02:55,120
lets the api server handle the entire

00:02:54,080 --> 00:02:57,840
life cycle

00:02:55,120 --> 00:02:59,519
for example aerospike is a database and

00:02:57,840 --> 00:03:01,760
it's a custom resource

00:02:59,519 --> 00:03:03,440
our engineers built an operator for it

00:03:01,760 --> 00:03:05,680
because it's not part of the kubernetes

00:03:03,440 --> 00:03:07,360
ecosystem

00:03:05,680 --> 00:03:09,840
the operator pattern is used for

00:03:07,360 --> 00:03:12,239
automating repeatable tasks

00:03:09,840 --> 00:03:14,560
and it combines the custom resources and

00:03:12,239 --> 00:03:17,280
the custom controllers

00:03:14,560 --> 00:03:18,959
basically it's meant to take the human

00:03:17,280 --> 00:03:20,879
out of the equation

00:03:18,959 --> 00:03:24,800
because it's boring and when you do

00:03:20,879 --> 00:03:24,800
boring things you make mistakes

00:03:26,159 --> 00:03:30,879
was that confusing in case you didn't

00:03:28,879 --> 00:03:32,799
write your own operator by now or

00:03:30,879 --> 00:03:34,000
didn't dive so much into the operator

00:03:32,799 --> 00:03:36,319
that you're using

00:03:34,000 --> 00:03:40,239
maybe that was a little bit confusing so

00:03:36,319 --> 00:03:42,319
one more time for the people in the back

00:03:40,239 --> 00:03:43,519
we start with a user that tells that it

00:03:42,319 --> 00:03:45,840
wants to do things

00:03:43,519 --> 00:03:46,959
so it sends a command to the kubernetes

00:03:45,840 --> 00:03:51,120
cluster

00:03:46,959 --> 00:03:53,599
the api server exposes this http api

00:03:51,120 --> 00:03:55,439
that as we said it lets the end users

00:03:53,599 --> 00:03:57,680
the different parts of the cluster

00:03:55,439 --> 00:04:00,480
and the external components communicate

00:03:57,680 --> 00:04:02,799
with each other

00:04:00,480 --> 00:04:03,920
creates pods to host the application

00:04:02,799 --> 00:04:07,680
instances

00:04:03,920 --> 00:04:10,640
and each pod is tied to a node

00:04:07,680 --> 00:04:12,799
a cluster is a set of node machines or

00:04:10,640 --> 00:04:15,280
worker machines that are called nodes

00:04:12,799 --> 00:04:17,759
and they run containerized apps every

00:04:15,280 --> 00:04:20,160
cluster has at least one worker node

00:04:17,759 --> 00:04:21,040
and let's say in our example we have n

00:04:20,160 --> 00:04:24,960
pods

00:04:21,040 --> 00:04:26,840
tied to n nodes they're created by the

00:04:24,960 --> 00:04:29,759
deployment

00:04:26,840 --> 00:04:32,240
object deployments are used

00:04:29,759 --> 00:04:33,360
for stateless applications like web

00:04:32,240 --> 00:04:35,520
servers

00:04:33,360 --> 00:04:37,840
pods that are deployed by a deployment

00:04:35,520 --> 00:04:39,840
are identical and interchangeable

00:04:37,840 --> 00:04:42,880
and they're created in a random order

00:04:39,840 --> 00:04:45,600
with random hashes in their pod names

00:04:42,880 --> 00:04:47,199
on the other hand a stateful set is used

00:04:45,600 --> 00:04:50,080
for stateful application

00:04:47,199 --> 00:04:51,199
as the name might suggest when dealing

00:04:50,080 --> 00:04:53,040
with databases

00:04:51,199 --> 00:04:54,880
you definitely want to have the stateful

00:04:53,040 --> 00:04:57,120
set because you want to persist and

00:04:54,880 --> 00:04:59,440
store the data

00:04:57,120 --> 00:05:00,320
pods that are deployed by stateful set

00:04:59,440 --> 00:05:02,320
are not

00:05:00,320 --> 00:05:04,160
identical they each have their own

00:05:02,320 --> 00:05:05,199
identity and that's kept between

00:05:04,160 --> 00:05:10,560
restarts

00:05:05,199 --> 00:05:13,039
and each can be addressed individually

00:05:10,560 --> 00:05:15,039
an abstract way to expose an application

00:05:13,039 --> 00:05:18,720
running on a set of pods

00:05:15,039 --> 00:05:21,840
is a network service is our next object

00:05:18,720 --> 00:05:24,160
and the last one is config map

00:05:21,840 --> 00:05:26,320
that's an api object that is used to

00:05:24,160 --> 00:05:29,199
store a non-confidential data

00:05:26,320 --> 00:05:31,039
in key value pairs pods can consume

00:05:29,199 --> 00:05:32,000
these config maps as environment

00:05:31,039 --> 00:05:34,560
variables

00:05:32,000 --> 00:05:36,960
as command line arguments or as

00:05:34,560 --> 00:05:38,960
configuration files in a volume

00:05:36,960 --> 00:05:41,039
a config map allows you to decouple

00:05:38,960 --> 00:05:43,280
environment specific configuration

00:05:41,039 --> 00:05:46,000
from your container image so the

00:05:43,280 --> 00:05:49,039
application is pretty portable

00:05:46,000 --> 00:05:52,720
many more objects in this ecosystem but

00:05:49,039 --> 00:05:56,080
this is what we need for our example

00:05:52,720 --> 00:05:58,800
of course we also need a controller

00:05:56,080 --> 00:06:00,160
as we said this is making sure that if

00:05:58,800 --> 00:06:02,000
there is a difference between the

00:06:00,160 --> 00:06:03,120
current state and the desired state of

00:06:02,000 --> 00:06:05,919
the cluster

00:06:03,120 --> 00:06:09,360
this will act to make sure that it's

00:06:05,919 --> 00:06:12,240
getting towards the desired state

00:06:09,360 --> 00:06:15,039
and a controller tracks at least one

00:06:12,240 --> 00:06:17,440
resource type

00:06:15,039 --> 00:06:19,680
then there is a custom resource for

00:06:17,440 --> 00:06:22,560
example our database

00:06:19,680 --> 00:06:25,120
and this one is controlled by an

00:06:22,560 --> 00:06:25,120
operator

00:06:26,000 --> 00:06:30,800
both concepts of controller and operator

00:06:28,639 --> 00:06:33,120
they represent a pattern

00:06:30,800 --> 00:06:34,000
it does not involve a language specific

00:06:33,120 --> 00:06:36,319
implementation or

00:06:34,000 --> 00:06:38,160
framework so if you want to write a

00:06:36,319 --> 00:06:39,680
controller or an operator

00:06:38,160 --> 00:06:41,280
you can do it in any language of your

00:06:39,680 --> 00:06:43,840
choice you just need to follow the

00:06:41,280 --> 00:06:43,840
convention

00:06:44,880 --> 00:06:50,800
so to summarize this in a writing form

00:06:48,400 --> 00:06:51,759
you can say that a kubernetes operator

00:06:50,800 --> 00:06:53,360
minimizes

00:06:51,759 --> 00:06:54,960
manual deploying and life cycle

00:06:53,360 --> 00:06:56,800
management

00:06:54,960 --> 00:06:59,039
you can expect it to do things like

00:06:56,800 --> 00:07:02,160
complex resource management

00:06:59,039 --> 00:07:03,680
scale the cluster up and down and do

00:07:02,160 --> 00:07:05,520
things like version upgrade and

00:07:03,680 --> 00:07:07,280
downgrade

00:07:05,520 --> 00:07:08,800
you can use it for configuration

00:07:07,280 --> 00:07:12,639
management and

00:07:08,800 --> 00:07:15,520
for monitoring as we said take out

00:07:12,639 --> 00:07:16,400
the human from the equation to minimize

00:07:15,520 --> 00:07:20,160
mistakes

00:07:16,400 --> 00:07:20,160
and automate the boring things

00:07:20,960 --> 00:07:27,599
so what will our operator manage

00:07:24,560 --> 00:07:30,720
let's talk a little bit about aerospike

00:07:27,599 --> 00:07:32,800
it's a nosql database and it implements

00:07:30,720 --> 00:07:35,840
a hybrid memory architecture

00:07:32,800 --> 00:07:38,960
where the index is purely in memory so

00:07:35,840 --> 00:07:40,880
not persisted and the data is stored

00:07:38,960 --> 00:07:44,879
only on a persisted storage

00:07:40,880 --> 00:07:46,560
ssd for example and it reads directly

00:07:44,879 --> 00:07:49,120
from the disk

00:07:46,560 --> 00:07:50,160
the disk ion is not required to access

00:07:49,120 --> 00:07:52,160
the index

00:07:50,160 --> 00:07:53,599
and this enables a predictable

00:07:52,160 --> 00:07:56,840
performance

00:07:53,599 --> 00:07:58,160
the slas with the clients are pretty

00:07:56,840 --> 00:08:02,080
strict

00:07:58,160 --> 00:08:04,400
of data in sub milliseconds you can have

00:08:02,080 --> 00:08:07,919
strong consistency which you probably

00:08:04,400 --> 00:08:07,919
heard of from the cap theorem

00:08:08,160 --> 00:08:14,479
and there is transactional guarantees

00:08:12,479 --> 00:08:15,680
so the database transactions provide

00:08:14,479 --> 00:08:19,280
asset guarantees

00:08:15,680 --> 00:08:21,759
if needed this is why huge clients

00:08:19,280 --> 00:08:22,879
for example banks or others from the

00:08:21,759 --> 00:08:26,000
finance industry

00:08:22,879 --> 00:08:28,000
and other industries are using it

00:08:26,000 --> 00:08:29,039
other features that will be relevant for

00:08:28,000 --> 00:08:32,640
our operator

00:08:29,039 --> 00:08:34,959
and i want to mention is rack management

00:08:32,640 --> 00:08:36,880
so this allows you to store different

00:08:34,959 --> 00:08:38,240
replicas of records on different

00:08:36,880 --> 00:08:40,719
hardware failure groups

00:08:38,240 --> 00:08:41,680
you do this for resilience and

00:08:40,719 --> 00:08:45,919
multi-cluster

00:08:41,680 --> 00:08:45,919
cross data center replication setup

00:08:46,880 --> 00:08:52,640
multi-site is a setup where the nodes

00:08:50,160 --> 00:08:53,680
comprising a single cluster are

00:08:52,640 --> 00:08:56,080
distributed across

00:08:53,680 --> 00:08:57,360
sites it can be a physical rack in a

00:08:56,080 --> 00:09:00,240
data center

00:08:57,360 --> 00:09:00,800
an entire data center an availability

00:09:00,240 --> 00:09:04,640
zone

00:09:00,800 --> 00:09:06,560
in a cloud region or others basically

00:09:04,640 --> 00:09:08,160
the cluster is stretched across regions

00:09:06,560 --> 00:09:09,760
cloud providers and it expands

00:09:08,160 --> 00:09:12,399
horizontally

00:09:09,760 --> 00:09:13,519
this uses synchronous replication to

00:09:12,399 --> 00:09:16,240
deliver a global

00:09:13,519 --> 00:09:18,480
distributed transaction capability and

00:09:16,240 --> 00:09:20,560
the update speed is only limited by the

00:09:18,480 --> 00:09:23,440
speed of light

00:09:20,560 --> 00:09:24,720
it can also go asynchronous and in that

00:09:23,440 --> 00:09:28,000
case we'll use the cross

00:09:24,720 --> 00:09:28,320
data center application setup that uses

00:09:28,000 --> 00:09:30,240
the

00:09:28,320 --> 00:09:32,480
asynchronous replication to connect to

00:09:30,240 --> 00:09:35,040
clusters that are located at different

00:09:32,480 --> 00:09:37,360
geographically distributed sites

00:09:35,040 --> 00:09:38,959
it can extend that data infrastructure

00:09:37,360 --> 00:09:42,640
to any number of clusters

00:09:38,959 --> 00:09:46,480
easily so

00:09:42,640 --> 00:09:46,480
the aerospike kubernetes operator

00:09:46,800 --> 00:09:51,760
the high level design of it well it's

00:09:49,839 --> 00:09:53,920
driven by a single custom resource

00:09:51,760 --> 00:09:57,680
see our and it conforms with the

00:09:53,920 --> 00:10:00,959
operator custom resource definition crt

00:09:57,680 --> 00:10:01,519
the cluster specs cover things like the

00:10:00,959 --> 00:10:04,320
size

00:10:01,519 --> 00:10:05,920
so number of nodes per cluster and the

00:10:04,320 --> 00:10:09,600
resource allocation request

00:10:05,920 --> 00:10:12,720
for example cpu per node and it handles

00:10:09,600 --> 00:10:14,480
the complete aerospike configurations so

00:10:12,720 --> 00:10:16,320
yes it has the yaml version of the

00:10:14,480 --> 00:10:18,000
aerospike server configuration

00:10:16,320 --> 00:10:19,920
and it converts from the amole-based

00:10:18,000 --> 00:10:22,959
ones to the aerospike

00:10:19,920 --> 00:10:25,839
language configuration it also handles

00:10:22,959 --> 00:10:26,480
security configuration tls and user

00:10:25,839 --> 00:10:30,720
management

00:10:26,480 --> 00:10:32,480
and so on and we discussed a little bit

00:10:30,720 --> 00:10:34,000
the features of aerospike

00:10:32,480 --> 00:10:35,519
and those introduce some special

00:10:34,000 --> 00:10:38,720
considerations for

00:10:35,519 --> 00:10:40,720
things that it should do so you can

00:10:38,720 --> 00:10:42,640
of course expect it to deploy the

00:10:40,720 --> 00:10:45,440
database clusters

00:10:42,640 --> 00:10:46,079
and it manages all the things cycle

00:10:45,440 --> 00:10:48,160
management

00:10:46,079 --> 00:10:49,200
for example the database cluster scale

00:10:48,160 --> 00:10:51,040
up and down

00:10:49,200 --> 00:10:53,519
server version open upgrade and

00:10:51,040 --> 00:10:54,480
downgrade the air spy configuration

00:10:53,519 --> 00:10:56,560
management

00:10:54,480 --> 00:10:58,160
the rack awareness management the

00:10:56,560 --> 00:11:01,279
cluster access control

00:10:58,160 --> 00:11:04,800
management and the setup

00:11:01,279 --> 00:11:08,320
that you need for the multi-cluster xdr

00:11:04,800 --> 00:11:11,760
and as we said it spans across many

00:11:08,320 --> 00:11:14,079
different types of servers in many

00:11:11,760 --> 00:11:16,720
different geographical zones and

00:11:14,079 --> 00:11:18,000
hardware or cloud or multi-cloud so lots

00:11:16,720 --> 00:11:21,440
of specifics

00:11:18,000 --> 00:11:25,120
and it's really great to automate and

00:11:21,440 --> 00:11:28,079
it also handles the monitoring

00:11:25,120 --> 00:11:29,600
so when building it we faced all sorts

00:11:28,079 --> 00:11:32,160
of engineering challenges

00:11:29,600 --> 00:11:33,760
and i'll tell you about three of them

00:11:32,160 --> 00:11:37,040
first one being the persistent

00:11:33,760 --> 00:11:40,560
data so each pod

00:11:37,040 --> 00:11:42,320
has a dedicated storage and as we said

00:11:40,560 --> 00:11:45,440
it must be persistent because we are

00:11:42,320 --> 00:11:48,959
talking about the database here

00:11:45,440 --> 00:11:50,240
so the logic is that first time you're

00:11:48,959 --> 00:11:52,480
assigned a volume

00:11:50,240 --> 00:11:53,519
you need to wipe it just like with

00:11:52,480 --> 00:11:55,360
computer memory

00:11:53,519 --> 00:11:57,040
you cannot assume that the storage is

00:11:55,360 --> 00:12:01,200
going to be empty or full of

00:11:57,040 --> 00:12:02,399
nonsense but if it's a volume of a

00:12:01,200 --> 00:12:05,040
restarting put

00:12:02,399 --> 00:12:06,320
you don't want to wipe it when you

00:12:05,040 --> 00:12:08,399
change the configuration

00:12:06,320 --> 00:12:10,000
the pod restarts so of course you want

00:12:08,399 --> 00:12:11,760
to keep using the data or

00:12:10,000 --> 00:12:13,760
if something went wrong and the pod has

00:12:11,760 --> 00:12:14,320
to restart you still want to be able to

00:12:13,760 --> 00:12:18,160
use it

00:12:14,320 --> 00:12:18,160
so don't touch that data

00:12:18,240 --> 00:12:24,480
but there is no kubernetes way

00:12:21,680 --> 00:12:25,839
of telling if it's a restarted pod or a

00:12:24,480 --> 00:12:28,399
new pod

00:12:25,839 --> 00:12:29,120
because you probably updated the image

00:12:28,399 --> 00:12:32,480
and well

00:12:29,120 --> 00:12:34,800
all metrics are set to zero what are you

00:12:32,480 --> 00:12:34,800
gonna do

00:12:36,399 --> 00:12:42,959
a flag is the solution

00:12:40,240 --> 00:12:44,800
we add a flag using init containers

00:12:42,959 --> 00:12:47,680
these are the containers that you run

00:12:44,800 --> 00:12:49,519
before your containers run and that's

00:12:47,680 --> 00:12:52,320
how you edit the devices

00:12:49,519 --> 00:12:54,560
this is where you do wipe the data and

00:12:52,320 --> 00:12:57,440
in order not to wipe the data twice

00:12:54,560 --> 00:12:58,639
on the restart for example the operator

00:12:57,440 --> 00:13:01,440
makes

00:12:58,639 --> 00:13:02,800
for each research a cr in which it

00:13:01,440 --> 00:13:06,320
creates a single tone

00:13:02,800 --> 00:13:09,040
instance upon initialization basically

00:13:06,320 --> 00:13:09,519
when you wipe the data for the first

00:13:09,040 --> 00:13:12,639
time

00:13:09,519 --> 00:13:13,519
you add the flag and then if you do a

00:13:12,639 --> 00:13:15,279
pod restart

00:13:13,519 --> 00:13:16,720
you're just going to look in the config

00:13:15,279 --> 00:13:18,320
and you're going to see whether the flag

00:13:16,720 --> 00:13:20,880
already exists there

00:13:18,320 --> 00:13:23,920
and if it does exist there you know not

00:13:20,880 --> 00:13:23,920
to wipe the data

00:13:24,800 --> 00:13:35,839
the next challenge is what happens

00:13:28,720 --> 00:13:35,839
during a rolling update

00:13:36,480 --> 00:13:42,720
so let's say that something happened

00:13:39,760 --> 00:13:44,000
and you need to the operator started the

00:13:42,720 --> 00:13:46,079
rolling update

00:13:44,000 --> 00:13:47,040
in the middle you realize this is not

00:13:46,079 --> 00:13:50,000
what you want to do

00:13:47,040 --> 00:13:51,440
and you need to abort so you see this is

00:13:50,000 --> 00:13:54,320
printed on the screen

00:13:51,440 --> 00:13:54,720
update on node 1 complete update on node

00:13:54,320 --> 00:13:57,279
00:13:54,720 --> 00:14:00,240
complete same for node 3 then the

00:13:57,279 --> 00:14:02,480
realization to a board hits you

00:14:00,240 --> 00:14:03,279
what happens what are you gonna see in

00:14:02,480 --> 00:14:06,399
update on

00:14:03,279 --> 00:14:06,800
node it sounds pretty straightforward

00:14:06,399 --> 00:14:08,959
right

00:14:06,800 --> 00:14:11,279
you want it to stop everything and just

00:14:08,959 --> 00:14:12,639
reapply the old version to nodes one two

00:14:11,279 --> 00:14:15,040
three

00:14:12,639 --> 00:14:15,760
but if you send to the operator the

00:14:15,040 --> 00:14:19,600
command

00:14:15,760 --> 00:14:20,560
update all 10 nodes it makes sense that

00:14:19,600 --> 00:14:24,160
it will first

00:14:20,560 --> 00:14:27,360
complete 4 5 6 7 8 9 10 and then get

00:14:24,160 --> 00:14:30,639
the revert command

00:14:27,360 --> 00:14:32,639
and in specific use cases

00:14:30,639 --> 00:14:35,680
when you have a lot of traffic or when

00:14:32,639 --> 00:14:37,680
things can go wrong from the new setup

00:14:35,680 --> 00:14:39,199
you want it to stop immediately you

00:14:37,680 --> 00:14:43,040
cannot take those

00:14:39,199 --> 00:14:43,040
few additional changes

00:14:43,199 --> 00:14:48,720
so that's why we implemented the

00:14:46,480 --> 00:14:49,760
setup in the way that after every

00:14:48,720 --> 00:14:52,720
operation

00:14:49,760 --> 00:14:53,920
it requeues the reconciliation request

00:14:52,720 --> 00:14:58,240
basically

00:14:53,920 --> 00:15:00,560
the operator is asking the api now what

00:14:58,240 --> 00:15:02,560
this way it completed updating node

00:15:00,560 --> 00:15:05,120
number three to the new version

00:15:02,560 --> 00:15:06,079
and the next step would be what do i do

00:15:05,120 --> 00:15:09,360
now

00:15:06,079 --> 00:15:11,040
and the api says apply old version to

00:15:09,360 --> 00:15:13,519
nodes one two three

00:15:11,040 --> 00:15:14,240
so instead of saying updating new

00:15:13,519 --> 00:15:17,279
version

00:15:14,240 --> 00:15:21,199
on node 4 it will say updating

00:15:17,279 --> 00:15:23,839
version on node 1. hooray

00:15:21,199 --> 00:15:26,000
this is what you wanted it to do and to

00:15:23,839 --> 00:15:26,880
make things even more efficient the

00:15:26,000 --> 00:15:29,680
operator

00:15:26,880 --> 00:15:30,480
requests delay in the response for

00:15:29,680 --> 00:15:32,160
example

00:15:30,480 --> 00:15:34,639
let's say the node is in the middle of a

00:15:32,160 --> 00:15:37,519
migration and it knows that it will take

00:15:34,639 --> 00:15:38,240
some time then the operator tells the

00:15:37,519 --> 00:15:41,519
api

00:15:38,240 --> 00:15:42,160
now what but don't respond just yet hold

00:15:41,519 --> 00:15:44,639
on

00:15:42,160 --> 00:15:46,000
a few seconds and this long until i

00:15:44,639 --> 00:15:48,639
finish the migration

00:15:46,000 --> 00:15:49,759
because i will not be able to abort the

00:15:48,639 --> 00:15:53,120
migration for this

00:15:49,759 --> 00:15:54,959
one specific node in the middle and

00:15:53,120 --> 00:15:56,399
your decision might change by the time

00:15:54,959 --> 00:15:59,680
your response

00:15:56,399 --> 00:16:00,800
so it tells the the operator tells the

00:15:59,680 --> 00:16:03,759
api

00:16:00,800 --> 00:16:04,240
wait a few seconds and then respond and

00:16:03,759 --> 00:16:06,399
then

00:16:04,240 --> 00:16:07,440
when the migration of the current node

00:16:06,399 --> 00:16:09,440
is done the

00:16:07,440 --> 00:16:11,120
operator gets the most up-to-date

00:16:09,440 --> 00:16:15,120
response

00:16:11,120 --> 00:16:15,120
and this sounds very trivial

00:16:15,600 --> 00:16:20,560
probably when you dive into the problem

00:16:18,399 --> 00:16:23,600
long enough and when you see how

00:16:20,560 --> 00:16:26,560
different operators are doing this this

00:16:23,600 --> 00:16:27,519
very obvious solution looks slightly

00:16:26,560 --> 00:16:31,120
less obvious

00:16:27,519 --> 00:16:34,560
and more of an aha in a

00:16:31,120 --> 00:16:34,560
type of a hindsight setup

00:16:35,120 --> 00:16:42,079
the third challenge is what happens when

00:16:37,839 --> 00:16:42,079
you go to a really really large scale

00:16:42,480 --> 00:16:46,320
so cloud is great for prototyping and as

00:16:45,600 --> 00:16:49,519
you go

00:16:46,320 --> 00:16:51,360
bigger it gets a little pricey

00:16:49,519 --> 00:16:53,199
and we have customers with half a

00:16:51,360 --> 00:16:55,680
trillion objects

00:16:53,199 --> 00:16:57,440
to give you an understanding of what is

00:16:55,680 --> 00:17:00,079
half a trillion

00:16:57,440 --> 00:17:00,639
imagine that one dollar buys you three

00:17:00,079 --> 00:17:02,880
and a half

00:17:00,639 --> 00:17:05,039
objects now this is a little bit of a

00:17:02,880 --> 00:17:07,039
funny thing to say because an object is

00:17:05,039 --> 00:17:09,039
kind of an entry in the database

00:17:07,039 --> 00:17:11,120
and one dollar won't buy you three and a

00:17:09,039 --> 00:17:12,880
half entries but let's say for our

00:17:11,120 --> 00:17:16,480
understanding

00:17:12,880 --> 00:17:20,240
then half a trillion is how much

00:17:16,480 --> 00:17:20,240
jeff bezos can afford

00:17:20,799 --> 00:17:23,839
this is a client of ours and some

00:17:22,799 --> 00:17:26,480
clients have

00:17:23,839 --> 00:17:27,120
requirements for petabytes of traffic

00:17:26,480 --> 00:17:29,919
and

00:17:27,120 --> 00:17:32,880
all handled in sub milliseconds so the

00:17:29,919 --> 00:17:34,720
scale can get really big

00:17:32,880 --> 00:17:36,480
then there's a problem that the cloud is

00:17:34,720 --> 00:17:39,679
not homogeneous

00:17:36,480 --> 00:17:40,000
there is a promise of a minimum cpu but

00:17:39,679 --> 00:17:41,919
it

00:17:40,000 --> 00:17:43,919
doesn't mean that all the cpus are like

00:17:41,919 --> 00:17:45,760
that it means that the minimal cpu is

00:17:43,919 --> 00:17:46,400
like this but it can be that the other

00:17:45,760 --> 00:17:49,280
cpu

00:17:46,400 --> 00:17:49,280
is more than that

00:17:50,080 --> 00:17:54,480
aerospike because of its architecture is

00:17:52,880 --> 00:17:57,919
disk heavy

00:17:54,480 --> 00:18:00,000
sharing the aisle means that

00:17:57,919 --> 00:18:02,000
you definitely get a slice but it's hard

00:18:00,000 --> 00:18:03,840
to cap the size of it

00:18:02,000 --> 00:18:06,240
this can mean that the machine might

00:18:03,840 --> 00:18:08,480
respond slowly to messages

00:18:06,240 --> 00:18:10,480
in a distributed database there are a

00:18:08,480 --> 00:18:12,400
lot of messages going on

00:18:10,480 --> 00:18:13,600
there is a synchronization that is going

00:18:12,400 --> 00:18:15,120
on all the time

00:18:13,600 --> 00:18:17,039
you want to make sure that all the

00:18:15,120 --> 00:18:18,320
copies know which one is the most

00:18:17,039 --> 00:18:20,400
up-to-date one

00:18:18,320 --> 00:18:23,760
and well you want communication with

00:18:20,400 --> 00:18:23,760
endpoints like the heartbeat

00:18:24,559 --> 00:18:30,880
then there is the networking and

00:18:28,080 --> 00:18:31,200
the when the network is not private you

00:18:30,880 --> 00:18:33,280
get

00:18:31,200 --> 00:18:34,400
a slice of it but you have to share it

00:18:33,280 --> 00:18:37,039
right

00:18:34,400 --> 00:18:40,320
and if you have noisy neighbors they can

00:18:37,039 --> 00:18:42,480
drive your performance down

00:18:40,320 --> 00:18:43,520
not to talk about the cascading effect

00:18:42,480 --> 00:18:45,440
of any of these

00:18:43,520 --> 00:18:46,960
interruptions that's really hard to

00:18:45,440 --> 00:18:49,679
predict

00:18:46,960 --> 00:18:51,280
so as a slide says this is a situation

00:18:49,679 --> 00:18:54,640
that once you arrive to it

00:18:51,280 --> 00:18:56,320
an operator alone will not solve this

00:18:54,640 --> 00:18:58,400
the solution that some of our clients

00:18:56,320 --> 00:19:01,600
are doing is

00:18:58,400 --> 00:19:03,360
private cloud when you reach a really

00:19:01,600 --> 00:19:04,960
really really large scale

00:19:03,360 --> 00:19:06,880
get the whole host and split the

00:19:04,960 --> 00:19:09,840
resources internally

00:19:06,880 --> 00:19:11,919
and of course budget for some over

00:19:09,840 --> 00:19:14,320
capacity

00:19:11,919 --> 00:19:15,120
when your client is snap for example in

00:19:14,320 --> 00:19:17,120
their scale

00:19:15,120 --> 00:19:18,400
you do want to read from the client not

00:19:17,120 --> 00:19:20,960
from the master

00:19:18,400 --> 00:19:21,840
so be aware max your communication to a

00:19:20,960 --> 00:19:23,840
local one

00:19:21,840 --> 00:19:25,600
because latency matters a lot at the

00:19:23,840 --> 00:19:27,760
scale

00:19:25,600 --> 00:19:29,440
the kubernetes operator is great in the

00:19:27,760 --> 00:19:31,520
setup

00:19:29,440 --> 00:19:32,799
think about this kubernetes started

00:19:31,520 --> 00:19:35,840
inside google in their

00:19:32,799 --> 00:19:36,400
private cloud so of course the operator

00:19:35,840 --> 00:19:39,840
will work

00:19:36,400 --> 00:19:39,840
great in such a setup as well

00:19:40,880 --> 00:19:46,400
and to recap

00:19:44,080 --> 00:19:47,919
we talked about the kubernetes operators

00:19:46,400 --> 00:19:48,960
and how they control the custom

00:19:47,919 --> 00:19:50,960
resources

00:19:48,960 --> 00:19:52,000
and we understood a little bit the

00:19:50,960 --> 00:19:54,559
architecture and

00:19:52,000 --> 00:19:55,280
how those things work with each other

00:19:54,559 --> 00:19:57,440
and then

00:19:55,280 --> 00:19:59,200
i told you a little bit about the

00:19:57,440 --> 00:20:01,039
challenges we had building the operator

00:19:59,200 --> 00:20:03,840
for a distributed database

00:20:01,039 --> 00:20:05,039
and the takeaways that you can take home

00:20:03,840 --> 00:20:07,760
keep the data

00:20:05,039 --> 00:20:09,280
upon a pod restart because you are using

00:20:07,760 --> 00:20:11,440
a database

00:20:09,280 --> 00:20:12,559
be able to revert a rolling update

00:20:11,440 --> 00:20:15,200
immediately

00:20:12,559 --> 00:20:16,240
don't wait until the update is run on

00:20:15,200 --> 00:20:19,200
all the nodes

00:20:16,240 --> 00:20:20,080
have an option to stop it in the middle

00:20:19,200 --> 00:20:22,480
and

00:20:20,080 --> 00:20:23,840
be aware that when you go on a really

00:20:22,480 --> 00:20:26,080
really large scale

00:20:23,840 --> 00:20:27,919
you might have new problems and the

00:20:26,080 --> 00:20:32,320
solution might be bigger

00:20:27,919 --> 00:20:34,799
and of course automate automate automate

00:20:32,320 --> 00:20:36,240
the source code is of course open source

00:20:34,799 --> 00:20:39,600
available on github

00:20:36,240 --> 00:20:42,720
and you're very welcome to read learn

00:20:39,600 --> 00:20:44,240
contribute i'm happy to chat about this

00:20:42,720 --> 00:20:46,799
very much

00:20:44,240 --> 00:20:47,760
and again if you learned anything

00:20:46,799 --> 00:20:49,520
interesting

00:20:47,760 --> 00:20:51,039
please tweet at the conference please

00:20:49,520 --> 00:20:58,640
tweet at me

00:20:51,039 --> 00:20:58,640

YouTube URL: https://www.youtube.com/watch?v=VUGBq0ME2Qc


