Title: OSDC 2017 | Taming the Modern Data Center by Seth Vargo
Publication date: 2017-06-02
Playlist: OSDC 2017 | Open Source Data Center Conference
Description: 
	Today we are plagued by hundreds of choices when architecting a modern data center. Should our machines be virtual or physical? Should we use containers or Docker? Should we use a public cloud provider or a private cloud provider? Which configuration management tool is best to use? What about IaaS, PaaS, and SaaS? It would be manageable if these were binary choices; however, we often find ourselves in a hybrid environment.
Captions: 
	00:00:09,549 --> 00:00:16,660
okay welcome back so next speaker is the

00:00:13,419 --> 00:00:18,099
Fargo from Hoshi Corp talking about

00:00:16,660 --> 00:00:24,640
taming the modern data center so please

00:00:18,099 --> 00:00:25,629
give a warm applause success thank you

00:00:24,640 --> 00:00:29,919
can everyone hear me

00:00:25,629 --> 00:00:31,960
pretty loud so welcome today I want to

00:00:29,919 --> 00:00:34,899
talk to you about taming the modern data

00:00:31,960 --> 00:00:36,640
center so my name is Seth and the

00:00:34,899 --> 00:00:37,930
director of technical advocacy at a

00:00:36,640 --> 00:00:39,309
company called Hoshi Corp how many

00:00:37,930 --> 00:00:42,430
people here have heard of Hoshi court

00:00:39,309 --> 00:00:43,989
before awesome I've been in how she grew

00:00:42,430 --> 00:00:45,430
up for almost three years now and I've

00:00:43,989 --> 00:00:47,290
been doing this the whole time and the

00:00:45,430 --> 00:00:49,170
first time I said that like one person

00:00:47,290 --> 00:00:52,570
raised their hand and they worked for us

00:00:49,170 --> 00:00:53,620
so things have gotten a lot better for

00:00:52,570 --> 00:00:55,690
those of you that aren't familiar with

00:00:53,620 --> 00:00:57,880
Hoshi Corp more an open source company

00:00:55,690 --> 00:01:00,489
we make open source tools so it all

00:00:57,880 --> 00:01:02,230
started with vagrant we also make you

00:01:00,489 --> 00:01:05,950
know Packer serve console terraform

00:01:02,230 --> 00:01:08,860
vault and nomad you might recognize some

00:01:05,950 --> 00:01:10,240
of these logos up here but today I'm

00:01:08,860 --> 00:01:12,549
going to talk about three of those tools

00:01:10,240 --> 00:01:13,990
but before we talk about technology or

00:01:12,549 --> 00:01:16,090
code or anything like that I want to

00:01:13,990 --> 00:01:18,479
take a step back and try to answer the

00:01:16,090 --> 00:01:21,880
question of like why are we here

00:01:18,479 --> 00:01:23,259
not why are we in this room but why are

00:01:21,880 --> 00:01:25,179
we in the state like why are you at this

00:01:23,259 --> 00:01:27,159
conference what problems do you have and

00:01:25,179 --> 00:01:30,039
what technology are we trying to build

00:01:27,159 --> 00:01:31,600
to solve those problems and to do that

00:01:30,039 --> 00:01:33,700
it's important to talk about the

00:01:31,600 --> 00:01:37,149
evolution of the data center or the DC

00:01:33,700 --> 00:01:38,770
so we have this data center and we'll

00:01:37,149 --> 00:01:42,670
define that in a little bit but how do

00:01:38,770 --> 00:01:46,029
we get here well and say like you know

00:01:42,670 --> 00:01:48,880
the 1980s or 1990s the data center was

00:01:46,029 --> 00:01:51,939
very easy to visualize and it was very

00:01:48,880 --> 00:01:54,579
tangible it was a physical server in a

00:01:51,939 --> 00:01:56,499
physical room that you could walk up to

00:01:54,579 --> 00:01:58,719
and touch you could plug things in you

00:01:56,499 --> 00:02:00,939
could put in a CD or a DVD or a thumb

00:01:58,719 --> 00:02:04,209
drive or a floppy disk it was a physical

00:02:00,939 --> 00:02:04,869
machine and it had a name and it had a

00:02:04,209 --> 00:02:07,689
caretaker

00:02:04,869 --> 00:02:10,239
we called them sysadmin and that was

00:02:07,689 --> 00:02:12,340
their job and some of these computers

00:02:10,239 --> 00:02:14,170
were the size of a football stadium or a

00:02:12,340 --> 00:02:16,950
soccer stadium they're very large and

00:02:14,170 --> 00:02:19,600
some of them were you know not as big

00:02:16,950 --> 00:02:23,349
and then we start entering kind of the

00:02:19,600 --> 00:02:25,180
era of every Enterprise big companies

00:02:23,349 --> 00:02:27,159
having their own data center so

00:02:25,180 --> 00:02:28,150
computers get a little bit smaller we

00:02:27,159 --> 00:02:30,159
apply Moore's law

00:02:28,150 --> 00:02:32,319
now servers or you know they can fit in

00:02:30,159 --> 00:02:34,749
a rack they still require a lot of

00:02:32,319 --> 00:02:36,639
energy a lot of cooling but big

00:02:34,749 --> 00:02:39,209
enterprises can afford to build their

00:02:36,639 --> 00:02:43,209
own data centers so we start seeing the

00:02:39,209 --> 00:02:44,769
whole ciccolo and the retail Colo start

00:02:43,209 --> 00:02:47,680
come up with all of these individual

00:02:44,769 --> 00:02:49,810
data centers and then very quickly these

00:02:47,680 --> 00:02:51,400
organizations realize that they weren't

00:02:49,810 --> 00:02:53,799
getting utilization out of those servers

00:02:51,400 --> 00:02:56,139
you buy a server from you know son or

00:02:53,799 --> 00:02:58,629
Dell and it has at the time maybe four

00:02:56,139 --> 00:03:01,480
gigs of RAM but your application is only

00:02:58,629 --> 00:03:03,879
required like 55 kilobytes of RAM so you

00:03:01,480 --> 00:03:05,919
had this massive over-provisioning of

00:03:03,879 --> 00:03:07,930
resources and we didn't have a good way

00:03:05,919 --> 00:03:10,750
to run multiple applications on the same

00:03:07,930 --> 00:03:12,849
machine so we needed an isolation layer

00:03:10,750 --> 00:03:15,969
and this is where you start seeing like

00:03:12,849 --> 00:03:17,590
Xen and VMware come into the picture so

00:03:15,969 --> 00:03:19,900
we start seeing hypervisors where we can

00:03:17,590 --> 00:03:22,780
run multiple operating systems multiple

00:03:19,900 --> 00:03:25,359
virtual machines on one physical machine

00:03:22,780 --> 00:03:26,980
but this added some complexity we now

00:03:25,359 --> 00:03:28,780
have overlay networks we have the

00:03:26,980 --> 00:03:30,009
network within the virtualized appliance

00:03:28,780 --> 00:03:32,530
but then we also have the physical

00:03:30,009 --> 00:03:34,750
network and we start seeing machines are

00:03:32,530 --> 00:03:36,549
no longer connected via serial cables we

00:03:34,750 --> 00:03:38,739
start seeing a lot of you know layer 3

00:03:36,549 --> 00:03:41,590
layer 5 layer 7 networking start to come

00:03:38,739 --> 00:03:43,120
into play and then more recently we've

00:03:41,590 --> 00:03:46,449
entered the era of the containerized

00:03:43,120 --> 00:03:48,579
workflow where VMS gave us operating

00:03:46,449 --> 00:03:50,229
system and kernel bubble isolation but

00:03:48,579 --> 00:03:52,000
we didn't need that much we didn't need

00:03:50,229 --> 00:03:54,189
a whole new operating system we just

00:03:52,000 --> 00:03:55,239
needed containerization and you know

00:03:54,189 --> 00:03:57,310
containers have been around for a long

00:03:55,239 --> 00:04:00,099
time like LXE is probably one of the

00:03:57,310 --> 00:04:02,049
oldest technologies out there but you

00:04:00,099 --> 00:04:04,419
know darker created the proliferation of

00:04:02,049 --> 00:04:05,769
these containers and we start seeing you

00:04:04,419 --> 00:04:07,659
know applications packaged deploying

00:04:05,769 --> 00:04:10,919
containers the schedulers all of the

00:04:07,659 --> 00:04:13,299
stuff is really rapidly approaching and

00:04:10,919 --> 00:04:15,219
then to add additional complexity to

00:04:13,299 --> 00:04:17,859
that we don't manage all of our

00:04:15,219 --> 00:04:21,099
infrastructure anymore so 20 years ago

00:04:17,859 --> 00:04:22,870
your database your caching server your

00:04:21,099 --> 00:04:24,460
application code was all in-house it was

00:04:22,870 --> 00:04:27,159
all in the one data center or multiple

00:04:24,460 --> 00:04:30,430
data centers now we have all of these

00:04:27,159 --> 00:04:32,169
services we have DNS as a service we

00:04:30,430 --> 00:04:34,150
have post grass's of service we have

00:04:32,169 --> 00:04:35,830
cloud databases as a service CDN to the

00:04:34,150 --> 00:04:37,010
service so instead of all of those

00:04:35,830 --> 00:04:40,190
things being in-house

00:04:37,010 --> 00:04:42,920
we've moved into specialized services

00:04:40,190 --> 00:04:46,130
that are external to our own data center

00:04:42,920 --> 00:04:47,330
and then the rise of cloud technologies

00:04:46,130 --> 00:04:50,000
hasn't helped that at all

00:04:47,330 --> 00:04:52,940
so some organizations have a physical

00:04:50,000 --> 00:04:54,680
data center and a cloud-based data

00:04:52,940 --> 00:04:56,180
center or multi cloud-based data centers

00:04:54,680 --> 00:04:57,500
and they're trying to peer traffic

00:04:56,180 --> 00:04:58,970
between them and they're trying to route

00:04:57,500 --> 00:05:01,340
users to different data centers

00:04:58,970 --> 00:05:03,740
and it turns out that if you look at

00:05:01,340 --> 00:05:05,960
pretty much any organization that's been

00:05:03,740 --> 00:05:08,330
around for longer than 10 years they

00:05:05,960 --> 00:05:11,150
have something that looks like this they

00:05:08,330 --> 00:05:12,860
have a mix of physical machines in a

00:05:11,150 --> 00:05:14,570
physical data center some of them are

00:05:12,860 --> 00:05:16,730
running VMs and some of those VMs have

00:05:14,570 --> 00:05:18,470
containers on them and they might be

00:05:16,730 --> 00:05:20,300
like kind of moving to cloud or they

00:05:18,470 --> 00:05:21,920
might not be you know they might be

00:05:20,300 --> 00:05:23,570
using Heroku for some stuff and they

00:05:21,920 --> 00:05:25,970
might be using an RDS database for some

00:05:23,570 --> 00:05:28,250
other stuff and really what it boils

00:05:25,970 --> 00:05:31,520
down to is that there's so much

00:05:28,250 --> 00:05:33,950
complexity here no longer do we have one

00:05:31,520 --> 00:05:35,560
physical machine with a serial port that

00:05:33,950 --> 00:05:38,540
we can physically walk up to and touch

00:05:35,560 --> 00:05:40,940
instead we have compute and we have

00:05:38,540 --> 00:05:42,410
storage and we have abstractions of

00:05:40,940 --> 00:05:45,860
these components that we used to be able

00:05:42,410 --> 00:05:48,470
to reason about and we have a mixture of

00:05:45,860 --> 00:05:50,090
all the asses your I asked your PS your

00:05:48,470 --> 00:05:51,800
CS your software infrastructure and

00:05:50,090 --> 00:05:53,210
platform as a service so these are

00:05:51,800 --> 00:05:54,860
things not only just like the cloud

00:05:53,210 --> 00:05:56,900
providers but also very specialized

00:05:54,860 --> 00:05:59,380
services things that just provide DNS or

00:05:56,900 --> 00:06:02,630
just provide application runtimes

00:05:59,380 --> 00:06:07,990
so with all this added complexity we had

00:06:02,630 --> 00:06:11,360
to find a way to tame this previously

00:06:07,990 --> 00:06:13,910
before the modern cloud era we have this

00:06:11,360 --> 00:06:16,220
thing called the a pod cycle so acquire

00:06:13,910 --> 00:06:18,830
provision update and delete a server and

00:06:16,220 --> 00:06:19,790
if you think about maybe 20 years ago

00:06:18,830 --> 00:06:22,640
for those of you that have been in

00:06:19,790 --> 00:06:25,370
industry for a while this process was

00:06:22,640 --> 00:06:27,740
very time consuming to acquire a new

00:06:25,370 --> 00:06:31,130
physical server someone had to get on

00:06:27,740 --> 00:06:34,160
the phone or send a fax those are still

00:06:31,130 --> 00:06:35,930
things faxes and acquire a server you

00:06:34,160 --> 00:06:37,580
had to talk to a procurement department

00:06:35,930 --> 00:06:39,500
you had to allocate funds and you had to

00:06:37,580 --> 00:06:42,800
send a purchase order to one of the

00:06:39,500 --> 00:06:46,610
vendors like Dell or IBM or son to get

00:06:42,800 --> 00:06:48,710
physical machines and that process took

00:06:46,610 --> 00:06:50,210
a really long time and then after you

00:06:48,710 --> 00:06:50,900
got that machine once it shipped to you

00:06:50,210 --> 00:06:53,199
in like

00:06:50,900 --> 00:06:56,030
to eight weeks via you know FedEx or UPS

00:06:53,199 --> 00:06:58,430
someone would have to unbox it take the

00:06:56,030 --> 00:07:00,560
cellophane off put it in a rack plug it

00:06:58,430 --> 00:07:02,030
in connect it to the network provision

00:07:00,560 --> 00:07:04,930
the initial set of software and users

00:07:02,030 --> 00:07:07,160
and then from there there's kind of this

00:07:04,930 --> 00:07:09,110
infinite life cycle of updating that

00:07:07,160 --> 00:07:10,850
server making sure that the security

00:07:09,110 --> 00:07:12,650
patches are up-to-date making sure that

00:07:10,850 --> 00:07:14,180
the users are there making sure that

00:07:12,650 --> 00:07:15,910
applications have the correct packages

00:07:14,180 --> 00:07:18,110
for their dependencies in run time and

00:07:15,910 --> 00:07:19,669
then at some point that server needs to

00:07:18,110 --> 00:07:21,169
be decommissioned or destroyed maybe

00:07:19,669 --> 00:07:23,270
it's getting repurposed move to another

00:07:21,169 --> 00:07:25,190
data center you know installing into our

00:07:23,270 --> 00:07:27,260
operating system or maybe it's just time

00:07:25,190 --> 00:07:28,970
to trash it claim the loss on the taxes

00:07:27,260 --> 00:07:32,180
and you know newer technology newer

00:07:28,970 --> 00:07:35,630
server comes in the rack and that

00:07:32,180 --> 00:07:37,280
process took a really long time there

00:07:35,630 --> 00:07:40,430
was a study about the NRDC that said the

00:07:37,280 --> 00:07:41,810
average acquisition from time that you

00:07:40,430 --> 00:07:42,979
realize you need a new server until you

00:07:41,810 --> 00:07:44,750
could run an application on it was

00:07:42,979 --> 00:07:48,500
somewhere between three and four months

00:07:44,750 --> 00:07:51,530
back in I think 1985 so you're talking

00:07:48,500 --> 00:07:52,550
weeks to acquire days to provision you

00:07:51,530 --> 00:07:54,259
know an infinite amount of time to

00:07:52,550 --> 00:07:57,789
update and then days to decommission the

00:07:54,259 --> 00:08:00,199
server but presently we have this era of

00:07:57,789 --> 00:08:03,020
Elastic Compute and all of the services

00:08:00,199 --> 00:08:05,690
so things like cloud technologies and

00:08:03,020 --> 00:08:09,199
Elastic Compute took the acquisition and

00:08:05,690 --> 00:08:12,169
delete cycle from weeks and days down to

00:08:09,199 --> 00:08:13,909
really minutes and seconds so what

00:08:12,169 --> 00:08:16,039
previously took six weeks to acquire a

00:08:13,909 --> 00:08:18,320
server can now be done in one API call

00:08:16,039 --> 00:08:19,340
to Google or Amazon and even if you're

00:08:18,320 --> 00:08:21,050
spinning up one of their largest

00:08:19,340 --> 00:08:24,620
instances it can be done in you know

00:08:21,050 --> 00:08:26,539
five or ten minutes similarly it's one

00:08:24,620 --> 00:08:28,750
API called to destroy an instance you're

00:08:26,539 --> 00:08:31,010
no longer using that compute power great

00:08:28,750 --> 00:08:32,750
but because of that we don't have to we

00:08:31,010 --> 00:08:34,640
don't think about things in terms of

00:08:32,750 --> 00:08:37,000
physical machines anymore instead we

00:08:34,640 --> 00:08:42,610
think about things in terms of RAM or

00:08:37,000 --> 00:08:47,150
disk i/o or CPU cycles or GPU cycles and

00:08:42,610 --> 00:08:49,190
this move has shifted our thinking and

00:08:47,150 --> 00:08:52,730
company's thinking's from capital

00:08:49,190 --> 00:08:54,410
expenses to operational expenses and

00:08:52,730 --> 00:08:56,480
this is where sometimes you'll hear

00:08:54,410 --> 00:08:59,900
organizations say oh the cloud is too

00:08:56,480 --> 00:09:02,600
expensive and that's because prior to

00:08:59,900 --> 00:09:04,590
the cloud you would acquire a server and

00:09:02,600 --> 00:09:06,330
your organization could

00:09:04,590 --> 00:09:08,520
right off the cost of that server over

00:09:06,330 --> 00:09:11,130
time you could depreciate that physical

00:09:08,520 --> 00:09:13,140
virtual or physical appliance as a

00:09:11,130 --> 00:09:16,080
write-off on your taxes you could

00:09:13,140 --> 00:09:19,470
depreciate it over time by switching to

00:09:16,080 --> 00:09:21,120
cloud technologies you now have

00:09:19,470 --> 00:09:23,460
operational expenses you have a monthly

00:09:21,120 --> 00:09:25,170
bill that might be less than the total

00:09:23,460 --> 00:09:26,370
cost of a server but you can't write

00:09:25,170 --> 00:09:28,890
that off on your taxes you're just

00:09:26,370 --> 00:09:30,570
paying for a service and this was a big

00:09:28,890 --> 00:09:33,030
shift in the paradigm and that's why we

00:09:30,570 --> 00:09:37,320
also started seeing even additional

00:09:33,030 --> 00:09:39,930
specialized services configuration

00:09:37,320 --> 00:09:41,580
management also saw a lot of increase in

00:09:39,930 --> 00:09:43,260
this area we see tools like puppet

00:09:41,580 --> 00:09:45,600
shafts ansible and salt really leading

00:09:43,260 --> 00:09:47,930
the space here the configuration

00:09:45,600 --> 00:09:50,220
management also includes things like -

00:09:47,930 --> 00:09:51,990
configuration management is still you

00:09:50,220 --> 00:09:54,300
know yum install our app - get installs

00:09:51,990 --> 00:09:57,480
the basics of config management and that

00:09:54,300 --> 00:10:00,000
basic level of automation improved the

00:09:57,480 --> 00:10:01,710
provision and update cycle so instead of

00:10:00,000 --> 00:10:04,290
someone copying and pasting commands

00:10:01,710 --> 00:10:05,610
from a wiki or manually typing commands

00:10:04,290 --> 00:10:08,610
into a machine to provision we could

00:10:05,610 --> 00:10:10,320
codify that capture it in code using you

00:10:08,610 --> 00:10:12,810
know tool like puppet or chef or ansible

00:10:10,320 --> 00:10:14,370
or salt to just kick off a command walk

00:10:12,810 --> 00:10:15,960
away and come back on the servers

00:10:14,370 --> 00:10:18,120
provisioned and you have guarantees that

00:10:15,960 --> 00:10:19,530
that server matches because of the the

00:10:18,120 --> 00:10:22,500
guarantees the configuration management

00:10:19,530 --> 00:10:24,090
provides so that's look at the provision

00:10:22,500 --> 00:10:26,010
and update cycle from you know

00:10:24,090 --> 00:10:28,050
previously days down to minutes or

00:10:26,010 --> 00:10:29,640
seconds you know a quick CM update on an

00:10:28,050 --> 00:10:31,560
already provisioned system might take

00:10:29,640 --> 00:10:35,040
two or three seconds everything's up to

00:10:31,560 --> 00:10:37,830
date great and this led to the

00:10:35,040 --> 00:10:40,170
proliferation of what I call specialized

00:10:37,830 --> 00:10:42,860
comm I don't think that's a real website

00:10:40,170 --> 00:10:46,430
it might be but it's the idea that

00:10:42,860 --> 00:10:50,910
individual companies are forming around

00:10:46,430 --> 00:10:55,560
very specific and very specialized tasks

00:10:50,910 --> 00:11:00,390
and you can consume those tasks via AP

00:10:55,560 --> 00:11:02,370
is but those AP is add complexity so

00:11:00,390 --> 00:11:03,840
when we think about the traditional

00:11:02,370 --> 00:11:05,100
modern data center or even think about

00:11:03,840 --> 00:11:07,470
your own applications and your own

00:11:05,100 --> 00:11:09,260
organizations you probably have a mix of

00:11:07,470 --> 00:11:11,700
physical machines virtual machines

00:11:09,260 --> 00:11:14,220
containers and you're also communicating

00:11:11,700 --> 00:11:16,070
with third party ap is it's very

00:11:14,220 --> 00:11:18,390
unlikely that anyone in this room

00:11:16,070 --> 00:11:19,830
manages their own DNS servers

00:11:18,390 --> 00:11:22,770
you probably delegate that to something

00:11:19,830 --> 00:11:25,530
like din or DN simple unless you're a

00:11:22,770 --> 00:11:27,540
DNS provider it's very unlikely than any

00:11:25,530 --> 00:11:30,120
of you unless you work for a major cloud

00:11:27,540 --> 00:11:31,350
provider are your own CDN you're

00:11:30,120 --> 00:11:33,660
probably offloading that to something

00:11:31,350 --> 00:11:36,390
like Akamai er fastly who specialized in

00:11:33,660 --> 00:11:37,950
those technologies but when you think

00:11:36,390 --> 00:11:40,020
about all of these dependencies and you

00:11:37,950 --> 00:11:41,670
start adding de la data layers and

00:11:40,020 --> 00:11:44,760
applications and containers on top of

00:11:41,670 --> 00:11:46,410
them it very quickly becomes difficult

00:11:44,760 --> 00:11:47,850
to reason about and then when you think

00:11:46,410 --> 00:11:49,890
about larger enterprises that have

00:11:47,850 --> 00:11:52,110
multiple data centers that span multiple

00:11:49,890 --> 00:11:54,060
countries and continents and they have

00:11:52,110 --> 00:12:02,790
to network between them and somehow sync

00:11:54,060 --> 00:12:04,410
data it starts to feel like this so it's

00:12:02,790 --> 00:12:09,120
important to take a step back and ask

00:12:04,410 --> 00:12:11,310
like why why did we make VMs why do we

00:12:09,120 --> 00:12:13,080
put applications and containers why is

00:12:11,310 --> 00:12:15,240
Amazon a cloud provider why is Google

00:12:13,080 --> 00:12:19,650
why is as your wise digital or why why

00:12:15,240 --> 00:12:23,310
and to do that I like to hypothesize

00:12:19,650 --> 00:12:25,320
that the goal from the institution of

00:12:23,310 --> 00:12:26,990
computers has been to effectively

00:12:25,320 --> 00:12:29,610
deliver and maintain applications

00:12:26,990 --> 00:12:32,790
whether your application is a punch card

00:12:29,610 --> 00:12:35,670
that you feed into a Fortran system to

00:12:32,790 --> 00:12:38,730
get a quick calculation or Redis or a

00:12:35,670 --> 00:12:40,620
rails app or a go app the goal has

00:12:38,730 --> 00:12:44,700
always been the same to effectively

00:12:40,620 --> 00:12:46,230
deliver and maintain applications to put

00:12:44,700 --> 00:12:48,560
it another way we want to move very

00:12:46,230 --> 00:12:51,360
quickly and not break things

00:12:48,560 --> 00:12:54,180
so as quick as possible I want to get

00:12:51,360 --> 00:12:56,970
code from my local development laptop or

00:12:54,180 --> 00:12:59,720
machine in front of a customer or in

00:12:56,970 --> 00:13:05,070
front of a user with minimal risk

00:12:59,720 --> 00:13:07,740
maximum testing and little thought so

00:13:05,070 --> 00:13:10,230
this is really the breakdown of the

00:13:07,740 --> 00:13:13,650
application lifecycle you want to

00:13:10,230 --> 00:13:14,940
provision infrastructure resources it

00:13:13,650 --> 00:13:17,370
whether it's a cloud or a physical data

00:13:14,940 --> 00:13:19,350
center you need to secure those

00:13:17,370 --> 00:13:22,050
infrastructure resources whether it's

00:13:19,350 --> 00:13:23,460
using TLS on the network layer or you

00:13:22,050 --> 00:13:25,290
know encryption within the applications

00:13:23,460 --> 00:13:26,610
themselves and then you need to run

00:13:25,290 --> 00:13:28,860
those applications whether you're using

00:13:26,610 --> 00:13:31,260
a tool like running it on bare metal

00:13:28,860 --> 00:13:31,860
like upstart or systemd or you're using

00:13:31,260 --> 00:13:34,140
you know a

00:13:31,860 --> 00:13:36,420
kubernetes are no matter Miso's to run

00:13:34,140 --> 00:13:38,010
the scheduled environment and then you

00:13:36,420 --> 00:13:40,290
also want parity with production and

00:13:38,010 --> 00:13:41,880
once it's in production if you have a

00:13:40,290 --> 00:13:44,190
microservices or in an architecture you

00:13:41,880 --> 00:13:46,440
need a way to network and discover all

00:13:44,190 --> 00:13:48,390
of these tools and that's really the

00:13:46,440 --> 00:13:49,920
hashey court vision is we make these

00:13:48,390 --> 00:13:52,140
open-source tools to solve these very

00:13:49,920 --> 00:13:55,620
specific problems and if you've ever

00:13:52,140 --> 00:13:57,300
read the Tao of Hoshi Corp excuse me we

00:13:55,620 --> 00:13:59,010
make and believe in the UNIX philosophy

00:13:57,300 --> 00:14:00,600
which means we build a tool and that

00:13:59,010 --> 00:14:02,790
tool does one thing and it does it very

00:14:00,600 --> 00:14:04,500
well you would never expect the LS

00:14:02,790 --> 00:14:07,140
command to create a file and you would

00:14:04,500 --> 00:14:09,269
never expect you know the cat command to

00:14:07,140 --> 00:14:10,920
restart your computer and the same thing

00:14:09,269 --> 00:14:12,990
is true here we build service discovery

00:14:10,920 --> 00:14:14,730
technologies that discover services they

00:14:12,990 --> 00:14:16,260
don't provision infrastructure and we

00:14:14,730 --> 00:14:18,300
build infrastructure provisioning tools

00:14:16,260 --> 00:14:20,339
that don't secure your infrastructure

00:14:18,300 --> 00:14:22,680
because we have a security tool that is

00:14:20,339 --> 00:14:25,680
responsible for that and all of these

00:14:22,680 --> 00:14:27,209
are completely free and open-source but

00:14:25,680 --> 00:14:30,079
today I want to focus on three of them

00:14:27,209 --> 00:14:33,170
and then tomorrow I'm talking about volt

00:14:30,079 --> 00:14:36,660
first I want to talk about terraform so

00:14:33,170 --> 00:14:38,519
why did we make terraform well terraform

00:14:36,660 --> 00:14:39,810
is designed to answer the questions that

00:14:38,519 --> 00:14:41,930
I've been posing in front of you which

00:14:39,810 --> 00:14:44,339
is how do I provision these resources

00:14:41,930 --> 00:14:47,430
terraform answers the acquisition and

00:14:44,339 --> 00:14:49,260
delete cycle of the aphid process how do

00:14:47,430 --> 00:14:51,209
I provision things that's more than

00:14:49,260 --> 00:14:55,079
compute though I need storage any

00:14:51,209 --> 00:14:56,459
network how do I get those things how do

00:14:55,079 --> 00:14:58,589
I manage the life cycle of those

00:14:56,459 --> 00:15:00,810
resources over time there are a lot of

00:14:58,589 --> 00:15:03,750
great tools out there the provision

00:15:00,810 --> 00:15:05,399
infrastructure but then they're done but

00:15:03,750 --> 00:15:07,589
we learned very quickly through our own

00:15:05,399 --> 00:15:09,209
findings on our own infrastructure use

00:15:07,589 --> 00:15:10,440
cases that infrastructure is not a

00:15:09,209 --> 00:15:12,750
static thing you don't click a button

00:15:10,440 --> 00:15:14,970
and walk away instead you're constantly

00:15:12,750 --> 00:15:16,980
iterating building new images deleting

00:15:14,970 --> 00:15:19,140
images upgrading downsizing to meet the

00:15:16,980 --> 00:15:20,579
needs of your customer base so we needed

00:15:19,140 --> 00:15:22,890
a tool that could support that workflow

00:15:20,579 --> 00:15:24,390
not just provision infrastructure but

00:15:22,890 --> 00:15:27,990
manage the lifecycle of those

00:15:24,390 --> 00:15:31,079
infrastructure resources over time and

00:15:27,990 --> 00:15:33,720
then how do i balance the different

00:15:31,079 --> 00:15:35,250
service providers the big buzzword right

00:15:33,720 --> 00:15:37,260
now is hybrid cloud and multi cloud

00:15:35,250 --> 00:15:38,519
right no one wants to go all Amazon

00:15:37,260 --> 00:15:40,110
because they might compete with Amazon

00:15:38,519 --> 00:15:42,690
Sunday or Amazon could jack up their

00:15:40,110 --> 00:15:44,040
prices and then you're stuck and you

00:15:42,690 --> 00:15:45,270
know no one wants to go all Google or

00:15:44,040 --> 00:15:47,160
all I sure for the same reason

00:15:45,270 --> 00:15:49,680
in so a lot of big organizations are

00:15:47,160 --> 00:15:52,680
going hybrid cloud and many of the cloud

00:15:49,680 --> 00:15:54,360
providers have their own tool for

00:15:52,680 --> 00:15:56,250
presenting infrastructure popular one is

00:15:54,360 --> 00:15:59,610
like conformation or deployment manager

00:15:56,250 --> 00:16:01,410
but these tools don't support multi

00:15:59,610 --> 00:16:04,080
cloud or hybrid clouds they don't let

00:16:01,410 --> 00:16:05,340
you link up your ec2 instance to

00:16:04,080 --> 00:16:10,410
communicate with your Google storage

00:16:05,340 --> 00:16:13,050
back-end terraform does how do I enforce

00:16:10,410 --> 00:16:14,760
policy across all these resources so how

00:16:13,050 --> 00:16:17,610
do I make sure that the permissions are

00:16:14,760 --> 00:16:19,170
correct how can I test that and then

00:16:17,610 --> 00:16:22,830
most importantly how can i aughtta mate

00:16:19,170 --> 00:16:24,450
it because running commands in a CLI to

00:16:22,830 --> 00:16:25,410
make every single little change isn't

00:16:24,450 --> 00:16:27,500
sustainable

00:16:25,410 --> 00:16:30,300
you can't automate it you can't test it

00:16:27,500 --> 00:16:34,500
so Tara forms goal is to provide a

00:16:30,300 --> 00:16:36,540
single workflow with a unified view that

00:16:34,500 --> 00:16:38,670
uses this concept of infrastructure as

00:16:36,540 --> 00:16:39,900
code they cannot only provision

00:16:38,670 --> 00:16:42,540
infrastructure but manage that

00:16:39,900 --> 00:16:44,640
infrastructure over time that's capable

00:16:42,540 --> 00:16:46,800
of doing really simple stuff like

00:16:44,640 --> 00:16:49,530
spinning up a blog but is also capable

00:16:46,800 --> 00:16:52,110
of spinning up huge multiple into your

00:16:49,530 --> 00:16:54,960
multi cloud applications almost a single

00:16:52,110 --> 00:16:56,400
command so for those of you that have

00:16:54,960 --> 00:16:58,860
never seen Tara form configuration

00:16:56,400 --> 00:17:01,230
before this is what it looks like it's a

00:16:58,860 --> 00:17:03,690
very declarative syntax and what we're

00:17:01,230 --> 00:17:06,030
doing here in this example is creating a

00:17:03,690 --> 00:17:07,920
digital ocean droplet which is like an

00:17:06,030 --> 00:17:10,020
ec2 instance or a compute image on

00:17:07,920 --> 00:17:11,550
Google and we're assigning at a DNS

00:17:10,020 --> 00:17:13,709
record so you can imagine this is like

00:17:11,550 --> 00:17:15,450
spinning up my personal blog I have a

00:17:13,709 --> 00:17:17,790
digitalocean droplet and I point you

00:17:15,450 --> 00:17:22,530
know South Fargo comm at that

00:17:17,790 --> 00:17:24,270
digitalocean droplet size each stanza is

00:17:22,530 --> 00:17:28,140
highly declarative and self-contained

00:17:24,270 --> 00:17:30,540
and terraform under the hood actually

00:17:28,140 --> 00:17:32,880
uses a lot of academia it's a basis and

00:17:30,540 --> 00:17:34,170
its roots are in graph theory and those

00:17:32,880 --> 00:17:35,640
of you that came to my training course

00:17:34,170 --> 00:17:37,590
yesterday know what I'm talking about

00:17:35,640 --> 00:17:39,570
but there's a lot of academics under the

00:17:37,590 --> 00:17:41,340
hood and terraform builds a dependency

00:17:39,570 --> 00:17:43,080
graph a directed acyclic graph under the

00:17:41,340 --> 00:17:45,510
hood for modeling all these dependencies

00:17:43,080 --> 00:17:46,980
and handling the order of operations but

00:17:45,510 --> 00:17:48,510
as a user you don't have to think about

00:17:46,980 --> 00:17:50,250
that because terraform has this really

00:17:48,510 --> 00:17:52,440
great built-in syntax called the

00:17:50,250 --> 00:17:55,380
interpolation syntax which uses this

00:17:52,440 --> 00:17:56,940
dollar curly brace syntax here which

00:17:55,380 --> 00:17:58,470
lets you reference values from other

00:17:56,940 --> 00:18:00,679
resources

00:17:58,470 --> 00:18:03,630
that simple process builds the graph

00:18:00,679 --> 00:18:05,669
this example here tells terraform to

00:18:03,630 --> 00:18:07,890
create the digitalocean droplet before

00:18:05,669 --> 00:18:09,330
it creates the DNS record and if we

00:18:07,890 --> 00:18:11,730
think about that that makes sense

00:18:09,330 --> 00:18:13,559
I can't create a DNS record and assign

00:18:11,730 --> 00:18:15,929
it an IP address until after I have that

00:18:13,559 --> 00:18:17,909
IP address and cloud providers don't

00:18:15,929 --> 00:18:22,110
assign me an IP address until after the

00:18:17,909 --> 00:18:23,700
resource has been created as you can see

00:18:22,110 --> 00:18:26,039
the configurations designed to be human

00:18:23,700 --> 00:18:28,049
friendly but it supports JSON as well so

00:18:26,039 --> 00:18:30,120
if you want to write in JSON or if you

00:18:28,049 --> 00:18:31,230
don't like the terraform configuration

00:18:30,120 --> 00:18:34,289
format you're free to do that

00:18:31,230 --> 00:18:35,520
it is very VCS friendly this was

00:18:34,289 --> 00:18:37,020
something that was thought about from

00:18:35,520 --> 00:18:39,059
the very beginning when you're capturing

00:18:37,020 --> 00:18:41,870
something is code you want to be able to

00:18:39,059 --> 00:18:44,309
review that code in a meaningful manner

00:18:41,870 --> 00:18:46,110
but really what it does is it captures

00:18:44,309 --> 00:18:48,299
your entire infrastructure all your

00:18:46,110 --> 00:18:50,460
dependencies from scratch in a single

00:18:48,299 --> 00:18:52,919
text file so you can collaborate as a

00:18:50,460 --> 00:18:54,330
team you can you know ship these to

00:18:52,919 --> 00:18:56,789
different organizations you can publish

00:18:54,330 --> 00:18:58,409
them publicly to spin up different

00:18:56,789 --> 00:19:00,179
environments in the last talk in this

00:18:58,409 --> 00:19:02,159
room someone talked about how they were

00:19:00,179 --> 00:19:04,020
using terraform to spin up kubernetes

00:19:02,159 --> 00:19:04,950
clusters it's just one example of one of

00:19:04,020 --> 00:19:06,990
the many things you can do with

00:19:04,950 --> 00:19:08,580
terraform once you build it once though

00:19:06,990 --> 00:19:10,200
you can deliver that to customers over

00:19:08,580 --> 00:19:13,590
and over and over again without the need

00:19:10,200 --> 00:19:15,240
to change or iterate or update the

00:19:13,590 --> 00:19:17,520
reason that we can support this broad

00:19:15,240 --> 00:19:19,980
range of technologies is that terraform

00:19:17,520 --> 00:19:21,900
has these providers or plugins that

00:19:19,980 --> 00:19:24,809
support a single integration point and

00:19:21,900 --> 00:19:26,909
provide a resource so terraform is

00:19:24,809 --> 00:19:29,340
broken up into three main parts core

00:19:26,909 --> 00:19:32,130
plugins and then the upstream api is or

00:19:29,340 --> 00:19:34,650
the providers themselves the providers

00:19:32,130 --> 00:19:37,710
implement a simple create read update

00:19:34,650 --> 00:19:39,390
and delete API to communicate with these

00:19:37,710 --> 00:19:44,039
third-party services like Amazon or

00:19:39,390 --> 00:19:45,480
Google or fastly they're pluggable but

00:19:44,039 --> 00:19:47,159
more recently we've started to see the

00:19:45,480 --> 00:19:50,039
terraform can manage anything with an

00:19:47,159 --> 00:19:51,750
API so I recently wrote a blog post on

00:19:50,039 --> 00:19:54,270
how you can use terraform to manage your

00:19:51,750 --> 00:19:56,429
github teams and permissions and labels

00:19:54,270 --> 00:19:58,020
on repositories that's not

00:19:56,429 --> 00:19:59,929
infrastructure right that's not compute

00:19:58,020 --> 00:20:01,980
that's not storage that's github repos

00:19:59,929 --> 00:20:04,799
but imagine being able to have a

00:20:01,980 --> 00:20:07,530
declarative text file that said all of

00:20:04,799 --> 00:20:09,750
these people in my organization have

00:20:07,530 --> 00:20:11,340
access to this repo and when someone

00:20:09,750 --> 00:20:12,340
joins your company as a new employee a

00:20:11,340 --> 00:20:14,230
new engineer

00:20:12,340 --> 00:20:17,230
they submit a pull request to that

00:20:14,230 --> 00:20:19,210
repository to add themselves to the

00:20:17,230 --> 00:20:21,460
necessary permissions and teams that

00:20:19,210 --> 00:20:24,190
they need their manager approves clicks

00:20:21,460 --> 00:20:26,500
merge and they inherit access the next

00:20:24,190 --> 00:20:29,350
time the terraform runs completely

00:20:26,500 --> 00:20:32,710
automated no clicking no API calls from

00:20:29,350 --> 00:20:36,580
a human all automated all with a single

00:20:32,710 --> 00:20:39,640
command terraform apply there are over

00:20:36,580 --> 00:20:41,610
65 built-in providers for infrastructure

00:20:39,640 --> 00:20:44,799
and API providers and Counting and

00:20:41,610 --> 00:20:46,240
because the provider system is a plug-in

00:20:44,799 --> 00:20:48,460
based system there are third-party

00:20:46,240 --> 00:20:51,029
providers which are not inside terraform

00:20:48,460 --> 00:20:53,200
core that you can install on your own

00:20:51,029 --> 00:20:55,059
and I know a lot of people are thinking

00:20:53,200 --> 00:20:57,130
like oh well I'm on like legacy

00:20:55,059 --> 00:20:58,690
infrastructure I can't none of this will

00:20:57,130 --> 00:21:00,789
work for me it's terrible I'm going to

00:20:58,690 --> 00:21:02,679
start napping well we just announced

00:21:00,789 --> 00:21:04,750
support for Oracle and Oracle enterprise

00:21:02,679 --> 00:21:07,570
and Oracle public cloud and terraform

00:21:04,750 --> 00:21:09,850
supports VMware vCenter and vSphere so

00:21:07,570 --> 00:21:11,890
it's not just cloud technologies it's

00:21:09,850 --> 00:21:13,510
anything with an API so if you're stuck

00:21:11,890 --> 00:21:15,340
on legacy infrastructure and you want

00:21:13,510 --> 00:21:18,669
the new hotness terraform might be able

00:21:15,340 --> 00:21:20,799
to do that for you but the terraform

00:21:18,669 --> 00:21:23,649
plan is where some of the other power

00:21:20,799 --> 00:21:25,899
comes from this is a preprocessor which

00:21:23,649 --> 00:21:27,880
tells you what will happen before it

00:21:25,899 --> 00:21:31,179
does and this is something that's really

00:21:27,880 --> 00:21:33,039
unique to terraform especially at its

00:21:31,179 --> 00:21:35,020
inception other tools have followed suit

00:21:33,039 --> 00:21:37,149
but terraform provides a dry run it

00:21:35,020 --> 00:21:38,440
tells you what it's going to do here we

00:21:37,149 --> 00:21:40,870
can see that it's going to create a

00:21:38,440 --> 00:21:43,870
digital ocean droplet that's what the

00:21:40,870 --> 00:21:45,520
plus sign indicates and it fills in any

00:21:43,870 --> 00:21:48,700
of the attributes it knows so this is

00:21:45,520 --> 00:21:50,649
creating a CentOS 5 32-bit image I don't

00:21:48,700 --> 00:21:54,730
know why anyone would use a 32-bit image

00:21:50,649 --> 00:21:56,740
anymore but there it is you'll also see

00:21:54,730 --> 00:21:59,350
a number of these what look like HTML

00:21:56,740 --> 00:22:00,850
computed tags these are values that the

00:21:59,350 --> 00:22:02,890
cloud provider or the infrastructure

00:22:00,850 --> 00:22:04,960
provider gives back we don't know them

00:22:02,890 --> 00:22:07,720
in advance so some examples there like

00:22:04,960 --> 00:22:11,409
the status whether it has you know an

00:22:07,720 --> 00:22:12,640
ipv4 or an ipv6 address and then if we

00:22:11,409 --> 00:22:14,799
scroll down a little bit we see the

00:22:12,640 --> 00:22:18,309
interpolation is right in the output as

00:22:14,799 --> 00:22:21,039
well the terraform plan shows us what

00:22:18,309 --> 00:22:24,039
will happen and it also explains certain

00:22:21,039 --> 00:22:25,299
actions what do I mean by that well if

00:22:24,039 --> 00:22:26,169
any of you worked with Amazon or Google

00:22:25,299 --> 00:22:28,059
before you know

00:22:26,169 --> 00:22:31,330
if you change the base image like the

00:22:28,059 --> 00:22:33,429
ami ID you you can't do that as an

00:22:31,330 --> 00:22:34,960
online upgrade imagine switching from

00:22:33,429 --> 00:22:36,909
Linux to Windows and trying to do that

00:22:34,960 --> 00:22:38,409
online on the same machine it's not

00:22:36,909 --> 00:22:40,179
possible the bits are simply different

00:22:38,409 --> 00:22:43,149
and the same is true for the cloud

00:22:40,179 --> 00:22:45,489
providers with terraform it'll actually

00:22:43,149 --> 00:22:47,289
identify those instances where it needs

00:22:45,489 --> 00:22:48,669
to delete and recreate an instance and

00:22:47,289 --> 00:22:50,859
it'll tell you very clearly in the

00:22:48,669 --> 00:22:52,659
output so you can minimize downtime by

00:22:50,859 --> 00:22:54,700
recognizing the operations that have to

00:22:52,659 --> 00:22:58,239
take place to successfully complete the

00:22:54,700 --> 00:23:00,519
run previously operators had to divine

00:22:58,239 --> 00:23:02,649
this change and if you know the history

00:23:00,519 --> 00:23:04,149
of terraform at all you'll know that one

00:23:02,649 --> 00:23:06,100
of the main reasons we build terraform

00:23:04,149 --> 00:23:08,139
is that previously our SAS product was

00:23:06,100 --> 00:23:10,509
running entirely on Amazon we made a

00:23:08,139 --> 00:23:12,159
change to conformation we couldn't see

00:23:10,509 --> 00:23:13,779
the rollout effect and it caused over

00:23:12,159 --> 00:23:15,789
six hours of downtime because a change

00:23:13,779 --> 00:23:17,799
to a V PC resulted in all of our

00:23:15,789 --> 00:23:19,899
instances being destroyed and then

00:23:17,799 --> 00:23:22,720
recreated and that was impossible to

00:23:19,899 --> 00:23:24,369
visualize at the time so terraform gives

00:23:22,720 --> 00:23:26,230
you a lot of visibility a lot of control

00:23:24,369 --> 00:23:27,429
over your infrastructure but there's

00:23:26,230 --> 00:23:30,369
still a decent amount of uncertainty

00:23:27,429 --> 00:23:32,559
we've solved the acquisition and delete

00:23:30,369 --> 00:23:34,299
cycle but there's still that

00:23:32,559 --> 00:23:36,129
configuration management component

00:23:34,299 --> 00:23:37,869
provisioning servers at runtime and

00:23:36,129 --> 00:23:41,499
we're subject to runtime related

00:23:37,869 --> 00:23:43,600
failures so what does tomorrow's

00:23:41,499 --> 00:23:45,609
infrastructure look like well at hash

00:23:43,600 --> 00:23:47,289
your Corp we preach immutable

00:23:45,609 --> 00:23:50,440
infrastructure how many people here are

00:23:47,289 --> 00:23:53,549
using immutable infrastructure yeah

00:23:50,440 --> 00:23:56,320
you're not it's actually not possible

00:23:53,549 --> 00:23:59,049
true immutable infrastructure means that

00:23:56,320 --> 00:24:00,669
you sign and checksum every bit of

00:23:59,049 --> 00:24:02,679
memory that runs on your machine so

00:24:00,669 --> 00:24:04,419
unless you work for like the government

00:24:02,679 --> 00:24:05,919
it's highly unlikely that anyone in this

00:24:04,419 --> 00:24:08,259
room is actually deploying a mutable

00:24:05,919 --> 00:24:09,700
infrastructure however you might be

00:24:08,259 --> 00:24:12,279
following a mutable infrastructure

00:24:09,700 --> 00:24:14,019
paradigms or patterns and that's really

00:24:12,279 --> 00:24:14,769
great so what is a mutable

00:24:14,019 --> 00:24:17,350
infrastructure

00:24:14,769 --> 00:24:20,559
well mutable infrastructure or

00:24:17,350 --> 00:24:22,989
changeable infrastructure mutates over

00:24:20,559 --> 00:24:25,330
time so we run something like Shaffer

00:24:22,989 --> 00:24:27,249
puppet or ansible or salt as a service

00:24:25,330 --> 00:24:28,840
on the running machines in the cluster

00:24:27,249 --> 00:24:31,090
so the production servers are running

00:24:28,840 --> 00:24:32,889
let's just use chef as an example

00:24:31,090 --> 00:24:35,049
they're running the chef agent the chef

00:24:32,889 --> 00:24:38,499
client agent and they run you know every

00:24:35,049 --> 00:24:40,000
30 minutes over time as we push our

00:24:38,499 --> 00:24:44,080
cookbook changes new recipe

00:24:40,000 --> 00:24:45,940
etc the divergence happens perhaps we

00:24:44,080 --> 00:24:47,590
have a slight network blip where one

00:24:45,940 --> 00:24:51,280
package doesn't get updated on one of

00:24:47,590 --> 00:24:53,800
our 100 machines or perhaps one user

00:24:51,280 --> 00:24:56,440
just failed to create random computer

00:24:53,800 --> 00:24:58,780
mistake and we might be alerted to that

00:24:56,440 --> 00:25:00,910
but over time what happens in a mutable

00:24:58,780 --> 00:25:02,860
infrastructure is the level of

00:25:00,910 --> 00:25:05,100
confidence that your machines are the

00:25:02,860 --> 00:25:09,520
same or have reached convergence

00:25:05,100 --> 00:25:12,850
decreases over time similarly the

00:25:09,520 --> 00:25:15,670
consistency between machines decreases

00:25:12,850 --> 00:25:17,820
over time in fact a recent study showed

00:25:15,670 --> 00:25:21,400
that on an average infrastructure over

00:25:17,820 --> 00:25:23,320
30 days 10% of machines will be

00:25:21,400 --> 00:25:24,520
divergent meaning they won't be the same

00:25:23,320 --> 00:25:26,050
even though they're using the same

00:25:24,520 --> 00:25:29,350
configuration management tooling and

00:25:26,050 --> 00:25:31,800
over 90 days almost 65% of those

00:25:29,350 --> 00:25:36,070
machines will be different unless

00:25:31,800 --> 00:25:38,800
they're reprovision from scratch so

00:25:36,070 --> 00:25:41,980
immutable infrastructure looks like this

00:25:38,800 --> 00:25:45,520
everything the same 100% of the time so

00:25:41,980 --> 00:25:47,530
how do we achieve that we know that a

00:25:45,520 --> 00:25:49,780
mutable infrastructure is faster you

00:25:47,530 --> 00:25:51,370
just launch a pre-built image it has all

00:25:49,780 --> 00:25:53,500
your software all your users everything

00:25:51,370 --> 00:25:56,320
on it pre-configured but how do we get

00:25:53,500 --> 00:25:57,670
in on there in the first place we can

00:25:56,320 --> 00:25:59,290
put immutable infrastructure and things

00:25:57,670 --> 00:26:01,000
like auto scaling technologies so that

00:25:59,290 --> 00:26:02,860
we can very quickly scale without human

00:26:01,000 --> 00:26:04,540
intervention we don't have to wait 30

00:26:02,860 --> 00:26:06,010
minutes for puppet or chef to finish

00:26:04,540 --> 00:26:08,170
their configuration management run if

00:26:06,010 --> 00:26:09,550
all of a sudden your website is on the

00:26:08,170 --> 00:26:12,310
front page of hacker news and you're

00:26:09,550 --> 00:26:14,650
trying to scale you don't have 30

00:26:12,310 --> 00:26:17,590
minutes to provision new servers you

00:26:14,650 --> 00:26:18,850
have maybe 30 seconds before then evil

00:26:17,590 --> 00:26:21,910
people on how con you to start telling

00:26:18,850 --> 00:26:23,740
you how terrible you are immutable

00:26:21,910 --> 00:26:26,020
infrastructure allows for greater parity

00:26:23,740 --> 00:26:27,190
we can actually check some those images

00:26:26,020 --> 00:26:29,380
and we know the thing that we're

00:26:27,190 --> 00:26:31,000
launching but the challenge is that

00:26:29,380 --> 00:26:34,380
immutable infrastructure requires

00:26:31,000 --> 00:26:37,720
automation it needs automation and

00:26:34,380 --> 00:26:39,610
that's why we built packer so packer

00:26:37,720 --> 00:26:42,160
is another open source tool that builds

00:26:39,610 --> 00:26:43,240
automated machine images and I know what

00:26:42,160 --> 00:26:45,640
many of you are thinking here they're

00:26:43,240 --> 00:26:50,590
like machine images those are gross

00:26:45,640 --> 00:26:52,630
nobody wants those but why like why as

00:26:50,590 --> 00:26:53,560
an industry have we traditionally been

00:26:52,630 --> 00:26:57,340
against machine

00:26:53,560 --> 00:27:00,460
images and the answer is simple they

00:26:57,340 --> 00:27:03,190
used to be the way golden images as they

00:27:00,460 --> 00:27:05,890
were called or these quarterly unchanged

00:27:03,190 --> 00:27:09,150
and blessed images that the sysadmin and

00:27:05,890 --> 00:27:11,920
ops team manages inside the data center

00:27:09,150 --> 00:27:13,120
so why like they used to be the way why

00:27:11,920 --> 00:27:15,100
did we move away from them

00:27:13,120 --> 00:27:16,660
well the changes were very slow and

00:27:15,100 --> 00:27:19,000
frustrating and there were some culture

00:27:16,660 --> 00:27:20,350
aspects here as well we had lots of

00:27:19,000 --> 00:27:22,060
teams trying to deploy their

00:27:20,350 --> 00:27:23,560
applications on the same server so

00:27:22,060 --> 00:27:26,260
they're fighting over packaged versions

00:27:23,560 --> 00:27:28,740
we weren't using virtualized technology

00:27:26,260 --> 00:27:30,430
or containerized technology at the time

00:27:28,740 --> 00:27:31,900
and we didn't have advanced

00:27:30,430 --> 00:27:33,820
configuration management tooling so a

00:27:31,900 --> 00:27:36,250
lot of this stuff was done manually so

00:27:33,820 --> 00:27:37,930
this admins would schedule some time and

00:27:36,250 --> 00:27:39,370
they would reprovision the new servers

00:27:37,930 --> 00:27:42,700
and if something broke it would take

00:27:39,370 --> 00:27:45,250
weeks to fix the tooling wasn't as

00:27:42,700 --> 00:27:47,830
mature as it is today we didn't have

00:27:45,250 --> 00:27:49,360
chef puppet ansible and salt the best

00:27:47,830 --> 00:27:52,030
thing at the time was cfengine

00:27:49,360 --> 00:27:54,130
and most companies weren't using it they

00:27:52,030 --> 00:27:56,680
were just using bash or wiki's with copy

00:27:54,130 --> 00:28:00,250
and paste modern configuration

00:27:56,680 --> 00:28:02,080
management changed all of that chef

00:28:00,250 --> 00:28:06,130
puppet ansible salt right Assange ones

00:28:02,080 --> 00:28:07,750
deploy in a million times but ops

00:28:06,130 --> 00:28:10,180
without machine images is like

00:28:07,750 --> 00:28:12,190
applications without binaries what do I

00:28:10,180 --> 00:28:13,660
mean by that and I'm not trying to say

00:28:12,190 --> 00:28:16,210
that config management is dead or it's

00:28:13,660 --> 00:28:17,860
terrible I'm trying to say that config

00:28:16,210 --> 00:28:20,980
management belongs at a different layer

00:28:17,860 --> 00:28:23,530
of the stack let's look at a typical

00:28:20,980 --> 00:28:25,630
application lifecycle like a go binary

00:28:23,530 --> 00:28:28,120
rust binary Java binary you have some

00:28:25,630 --> 00:28:31,030
source code and you compile that source

00:28:28,120 --> 00:28:32,470
code into a binary and that binary might

00:28:31,030 --> 00:28:37,720
contain some dynamically linked

00:28:32,470 --> 00:28:40,480
libraries lebay Lib B lib C if one of

00:28:37,720 --> 00:28:43,360
those binaries fails to link updated to

00:28:40,480 --> 00:28:45,760
a new version changes in API the binary

00:28:43,360 --> 00:28:47,290
fails to build and that binary fails

00:28:45,760 --> 00:28:48,850
maybe it's in Jenkins or maybe it's on

00:28:47,290 --> 00:28:50,170
your local laptop but it fails to build

00:28:48,850 --> 00:28:52,360
and it never makes its way in front of a

00:28:50,170 --> 00:28:55,660
customer never makes its way to a user

00:28:52,360 --> 00:28:57,190
because the build failed now let's

00:28:55,660 --> 00:28:59,890
compare that to the mutable server

00:28:57,190 --> 00:29:02,200
lifecycle we have a base server like a

00:28:59,890 --> 00:29:04,120
base Red Hat or a bun to server and

00:29:02,200 --> 00:29:06,509
we're trying to get to a ready server or

00:29:04,120 --> 00:29:08,519
ready state well the knee

00:29:06,509 --> 00:29:10,109
for packages and networking and

00:29:08,519 --> 00:29:12,209
configuration management hasn't gone

00:29:10,109 --> 00:29:13,859
away that the challenge still exists

00:29:12,209 --> 00:29:14,940
even if you're running in a scheduled

00:29:13,859 --> 00:29:17,279
architecture with something like

00:29:14,940 --> 00:29:19,259
kubernetes or no mat or mesas you still

00:29:17,279 --> 00:29:21,209
need basic users you don't want to run

00:29:19,259 --> 00:29:22,589
everything as route you know you still

00:29:21,209 --> 00:29:23,879
need the basic configuration management

00:29:22,589 --> 00:29:26,940
make sure your packages are up to date

00:29:23,879 --> 00:29:28,409
etc in the mutable lifecycle we have

00:29:26,940 --> 00:29:31,259
something like you know puppet or chef

00:29:28,409 --> 00:29:33,690
running as an agent every 30 minutes and

00:29:31,259 --> 00:29:35,159
if something breaks a package is

00:29:33,690 --> 00:29:37,559
unavailable the apt cache was

00:29:35,159 --> 00:29:40,199
unreachable or you know something

00:29:37,559 --> 00:29:41,639
happens at the network layer where all

00:29:40,199 --> 00:29:43,379
of a sudden you know we can't access

00:29:41,639 --> 00:29:45,449
something we have a temporary network

00:29:43,379 --> 00:29:46,979
outage or really bad latency where our

00:29:45,449 --> 00:29:48,869
packages can't update or we can't talk

00:29:46,979 --> 00:29:50,489
to a third party service or we have a

00:29:48,869 --> 00:29:52,199
bug in our cm it made it through tests

00:29:50,489 --> 00:29:53,849
but we forgot to test this one pathway

00:29:52,199 --> 00:29:57,989
and our configuration management crashes

00:29:53,849 --> 00:30:00,269
all of these result in a unhealthy ready

00:29:57,989 --> 00:30:02,069
server so it's right up there at the top

00:30:00,269 --> 00:30:04,199
this is an unhealthy ready server it's

00:30:02,069 --> 00:30:07,079
not ready but it's in the path of

00:30:04,199 --> 00:30:08,909
downtime this is in front of customers

00:30:07,079 --> 00:30:12,259
it might be receiving traffic it might

00:30:08,909 --> 00:30:14,519
not but it's in the path of downtime

00:30:12,259 --> 00:30:17,429
let's compare that to the machine image

00:30:14,519 --> 00:30:19,469
or the immutable server lifecycle we

00:30:17,429 --> 00:30:24,419
have a base server and a ready server

00:30:19,469 --> 00:30:26,639
and it just works because that base

00:30:24,419 --> 00:30:27,629
server has all of our packages all of

00:30:26,639 --> 00:30:30,659
our users all of our configuration

00:30:27,629 --> 00:30:33,749
management baked into it so how did we

00:30:30,659 --> 00:30:36,389
get there well that's what Packer does

00:30:33,749 --> 00:30:38,819
Packer embraces configuration management

00:30:36,389 --> 00:30:41,940
it uses the same chef recipe is the same

00:30:38,819 --> 00:30:43,709
puppet module is the same forget what

00:30:41,940 --> 00:30:45,690
ansible calls them ansible playbooks

00:30:43,709 --> 00:30:48,659
uses the same ansible playbooks that

00:30:45,690 --> 00:30:51,329
you're already using but moves failure

00:30:48,659 --> 00:30:54,179
from runtime in front of customers in

00:30:51,329 --> 00:30:56,909
production to build time so you run

00:30:54,179 --> 00:31:00,239
Packer in a tool like Jenkins or your CI

00:30:56,909 --> 00:31:03,749
server to output artifacts and those

00:31:00,239 --> 00:31:05,159
artifacts can be Amazon am hires or

00:31:03,749 --> 00:31:07,049
Google compute images digital ocean

00:31:05,159 --> 00:31:09,690
droplets or even things like docker

00:31:07,049 --> 00:31:11,339
containers or vagrant boxes you can

00:31:09,690 --> 00:31:13,199
enforce parity with your different

00:31:11,339 --> 00:31:14,609
environments you build an ami once and

00:31:13,199 --> 00:31:16,019
then you can launch it in multiple

00:31:14,609 --> 00:31:18,149
different environments production

00:31:16,019 --> 00:31:19,980
staging etc and you can even create

00:31:18,149 --> 00:31:21,690
parity with development

00:31:19,980 --> 00:31:23,010
so your local developers might be using

00:31:21,690 --> 00:31:24,809
something like vagrant or docker for

00:31:23,010 --> 00:31:26,940
local development part of your build

00:31:24,809 --> 00:31:29,309
process can be to output one of those

00:31:26,940 --> 00:31:30,870
images that they consume and run locally

00:31:29,309 --> 00:31:32,700
they use their same editor or their same

00:31:30,870 --> 00:31:34,650
everything but they're running in the

00:31:32,700 --> 00:31:36,059
same or as close to the same production

00:31:34,650 --> 00:31:39,480
infrastructure as possible to reduce

00:31:36,059 --> 00:31:42,600
those bugs and reduce the the number of

00:31:39,480 --> 00:31:45,750
integrations there but this introduced

00:31:42,600 --> 00:31:47,610
some new challenges so one of the

00:31:45,750 --> 00:31:48,900
biggest challenges and one of the common

00:31:47,610 --> 00:31:52,290
questions I get here is like oh but I

00:31:48,900 --> 00:31:53,820
rely on you know the chef server or the

00:31:52,290 --> 00:31:55,919
puppet server to do things like service

00:31:53,820 --> 00:31:58,140
discovery how do I tell my database

00:31:55,919 --> 00:32:00,030
where my app is and how do I tell my app

00:31:58,140 --> 00:32:02,850
or my databases I use chef search for

00:32:00,030 --> 00:32:05,419
that and my answer to that is that it

00:32:02,850 --> 00:32:08,669
didn't belong there in the first place

00:32:05,419 --> 00:32:09,900
and I think that configuration

00:32:08,669 --> 00:32:12,510
management trying to do service

00:32:09,900 --> 00:32:15,690
discovery is the same as LS trying to

00:32:12,510 --> 00:32:17,549
create a file the architecture was not

00:32:15,690 --> 00:32:19,320
designed for that we don't have edge

00:32:17,549 --> 00:32:20,970
triggering or push notifications and

00:32:19,320 --> 00:32:23,940
configuration management because it's a

00:32:20,970 --> 00:32:25,650
poll based agent you're restricted to a

00:32:23,940 --> 00:32:27,210
convergence interval and change sets are

00:32:25,650 --> 00:32:28,950
really difficult to visualize you can't

00:32:27,210 --> 00:32:31,049
think about the rollout impact there's

00:32:28,950 --> 00:32:33,510
no tool to show you what the rollout

00:32:31,049 --> 00:32:35,490
impacts going to be and this isn't you

00:32:33,510 --> 00:32:36,990
know a bad mark against cm tools I think

00:32:35,490 --> 00:32:39,510
that's just not what they were designed

00:32:36,990 --> 00:32:43,070
to do their architectural II the wrong

00:32:39,510 --> 00:32:46,770
choice and that's why we built console

00:32:43,070 --> 00:32:50,040
so console again fully open source tool

00:32:46,770 --> 00:32:52,500
has four major components service

00:32:50,040 --> 00:32:55,049
discovery health checking key value and

00:32:52,500 --> 00:32:57,620
this ability to be multi data center and

00:32:55,049 --> 00:33:00,960
I want to break that down piece by piece

00:32:57,620 --> 00:33:03,210
so what is service discovery well simply

00:33:00,960 --> 00:33:05,100
put service discovery is this idea that

00:33:03,210 --> 00:33:08,100
in a micro service-oriented architecture

00:33:05,100 --> 00:33:10,650
you have services in a traditional model

00:33:08,100 --> 00:33:12,179
if everything was local your order

00:33:10,650 --> 00:33:13,710
processing service and your data

00:33:12,179 --> 00:33:15,299
processing service and your credit card

00:33:13,710 --> 00:33:18,240
processing service we're all in the same

00:33:15,299 --> 00:33:19,860
app it was all of the same process but

00:33:18,240 --> 00:33:21,299
when you move to micro services or you

00:33:19,860 --> 00:33:24,210
want to scale those components

00:33:21,299 --> 00:33:25,860
individually now you have multiple

00:33:24,210 --> 00:33:27,780
instances of order processing multiple

00:33:25,860 --> 00:33:29,460
instances of credit card processing or

00:33:27,780 --> 00:33:31,860
perhaps you're offloading your data

00:33:29,460 --> 00:33:33,660
processing to a third party service like

00:33:31,860 --> 00:33:35,040
a cloud technology

00:33:33,660 --> 00:33:37,140
you need to be able to address those

00:33:35,040 --> 00:33:40,170
services they need an IP address or a

00:33:37,140 --> 00:33:42,840
DNS address service discovery provides

00:33:40,170 --> 00:33:48,060
that it's the basis for how you address

00:33:42,840 --> 00:33:50,160
services in console we have two primary

00:33:48,060 --> 00:33:52,770
service discovery interfaces that

00:33:50,160 --> 00:33:55,920
operate off of the same catalog the

00:33:52,770 --> 00:33:57,960
first is the DNS interface the DNS

00:33:55,920 --> 00:34:00,300
interface is zero-touch and requires no

00:33:57,960 --> 00:34:02,640
application changes so if you have a

00:34:00,300 --> 00:34:04,800
legacy application like a Haskell or

00:34:02,640 --> 00:34:06,210
Java or you know something COBOL that

00:34:04,800 --> 00:34:07,710
was written a long time ago that nobody

00:34:06,210 --> 00:34:10,610
is really maintaining it just kind of

00:34:07,710 --> 00:34:12,919
works don't touch it you can still use

00:34:10,610 --> 00:34:16,560
because you can rely on the kernel level

00:34:12,919 --> 00:34:18,630
resolution of DNS entries to do service

00:34:16,560 --> 00:34:20,010
discovery your application has no idea

00:34:18,630 --> 00:34:22,580
that it's actually talking to micro

00:34:20,010 --> 00:34:25,380
services it's just hitting a DNS address

00:34:22,580 --> 00:34:27,330
for more modern applications you can get

00:34:25,380 --> 00:34:33,210
finer grained control and rich metadata

00:34:27,330 --> 00:34:34,470
by query and consoles HTTP API console

00:34:33,210 --> 00:34:37,050
allows for discovery of both internal

00:34:34,470 --> 00:34:39,270
and external services what do I mean by

00:34:37,050 --> 00:34:40,919
that well an internal service is very

00:34:39,270 --> 00:34:43,230
simple it's things within your V PC or

00:34:40,919 --> 00:34:44,850
within your data center but what about

00:34:43,230 --> 00:34:47,310
external services like what if you're

00:34:44,850 --> 00:34:49,050
using a hosted database service it would

00:34:47,310 --> 00:34:51,270
still be great if you could talk to that

00:34:49,050 --> 00:34:53,580
as if it were a local service identified

00:34:51,270 --> 00:34:55,770
as a local service without hard coding a

00:34:53,580 --> 00:34:58,950
really long RDS URL everywhere in your

00:34:55,770 --> 00:34:59,970
configs and more so when that changes

00:34:58,950 --> 00:35:02,640
you want to be able to update it

00:34:59,970 --> 00:35:04,170
instantaneously console supports that

00:35:02,640 --> 00:35:06,900
was the internal external service

00:35:04,170 --> 00:35:09,660
catalogs so what does the DNS interface

00:35:06,900 --> 00:35:13,080
look like well it's very simple the

00:35:09,660 --> 00:35:15,090
console suffix is the TLD and anything

00:35:13,080 --> 00:35:17,880
you query on the dot console suffix just

00:35:15,090 --> 00:35:19,650
like calm or greedy you gets delegated

00:35:17,880 --> 00:35:22,530
to console which is running in a

00:35:19,650 --> 00:35:24,420
client-server relationship here I'm

00:35:22,530 --> 00:35:27,840
querying for services because I'm using

00:35:24,420 --> 00:35:30,360
the dot service dot console TLD and I'm

00:35:27,840 --> 00:35:31,980
querying for all services named web so

00:35:30,360 --> 00:35:34,230
services have a name we call this a

00:35:31,980 --> 00:35:36,030
logical service name so I might have a

00:35:34,230 --> 00:35:37,830
service named order processing and there

00:35:36,030 --> 00:35:40,650
might be fifteen or twenty instances of

00:35:37,830 --> 00:35:41,880
those well the console DNS service is

00:35:40,650 --> 00:35:44,310
going to do is it's going to randomize

00:35:41,880 --> 00:35:47,640
round-robin me3 IP addresses at a time

00:35:44,310 --> 00:35:48,870
for all of the healthy services

00:35:47,640 --> 00:35:51,060
so we'll talk about that in a second but

00:35:48,870 --> 00:35:53,250
the health checking is deeply integrated

00:35:51,060 --> 00:35:56,960
with the service discovery so you never

00:35:53,250 --> 00:35:56,960
route traffic to an unhealthy host

00:35:57,530 --> 00:36:02,700
here's an example of using the HTTP API

00:36:00,300 --> 00:36:04,560
you'll get back a big JSON blob that

00:36:02,700 --> 00:36:05,760
includes a ton of metadata most of which

00:36:04,560 --> 00:36:07,140
isn't actually relevant to your

00:36:05,760 --> 00:36:09,270
application unless you're using some of

00:36:07,140 --> 00:36:11,130
consoles internals but you'll be able to

00:36:09,270 --> 00:36:12,600
build rich metadata rich load-balancing

00:36:11,130 --> 00:36:14,280
or do things like prepared queries

00:36:12,600 --> 00:36:16,770
perhaps you want to build a query that

00:36:14,280 --> 00:36:20,450
says use the data center that is closest

00:36:16,770 --> 00:36:22,950
to me has the lowest latency to me but

00:36:20,450 --> 00:36:27,990
failover to the next closest one if that

00:36:22,950 --> 00:36:30,180
one's not available the next Punk

00:36:27,990 --> 00:36:33,510
component of console is health checking

00:36:30,180 --> 00:36:35,070
as I said before unhealthy services are

00:36:33,510 --> 00:36:36,210
removed from the discovery layer and

00:36:35,070 --> 00:36:38,910
this is something that's really unique

00:36:36,210 --> 00:36:41,660
to console is that we tightly couple

00:36:38,910 --> 00:36:44,310
health checking and service discovery

00:36:41,660 --> 00:36:46,050
you might have a health checking

00:36:44,310 --> 00:36:49,230
solution now like you know Nagios or

00:36:46,050 --> 00:36:50,190
sensu that is occasionally checking into

00:36:49,230 --> 00:36:52,680
your nodes and making sure they're

00:36:50,190 --> 00:36:54,060
healthy and using heartbeats etc but at

00:36:52,680 --> 00:36:54,660
the end of the day if a node is

00:36:54,060 --> 00:36:56,220
unhealthy

00:36:54,660 --> 00:36:57,750
someone has to remove it from the load

00:36:56,220 --> 00:37:00,450
balancer someone has to go repair it

00:36:57,750 --> 00:37:02,400
manually with console health checking

00:37:00,450 --> 00:37:04,740
it's integrated with a service discovery

00:37:02,400 --> 00:37:07,560
layer and the DNS just simply doesn't

00:37:04,740 --> 00:37:08,610
return results that are unhealthy so

00:37:07,560 --> 00:37:10,830
what that means is if you tell your

00:37:08,610 --> 00:37:13,290
application talk to database service

00:37:10,830 --> 00:37:16,890
start console and for some reason one of

00:37:13,290 --> 00:37:18,780
the the database followers is down to

00:37:16,890 --> 00:37:20,640
health check is failing console just

00:37:18,780 --> 00:37:22,620
won't route traffic to it your

00:37:20,640 --> 00:37:24,150
application just won't route traffic to

00:37:22,620 --> 00:37:26,220
it because it won't be returned as one

00:37:24,150 --> 00:37:27,510
of those DNS results so your application

00:37:26,220 --> 00:37:29,610
doesn't know how to check if the

00:37:27,510 --> 00:37:31,320
database is healthy console does but by

00:37:29,610 --> 00:37:34,920
using the DNS interface it just doesn't

00:37:31,320 --> 00:37:36,240
route traffic there automatically for

00:37:34,920 --> 00:37:38,040
applications that are using the API

00:37:36,240 --> 00:37:40,110
endpoints the API very clearly

00:37:38,040 --> 00:37:41,850
identifies if a node or service is

00:37:40,110 --> 00:37:44,640
healthy or unhealthy so you can use

00:37:41,850 --> 00:37:45,840
really complex build complex tooling

00:37:44,640 --> 00:37:48,630
around that to determine whether you

00:37:45,840 --> 00:37:51,540
want to route or not console also

00:37:48,630 --> 00:37:54,690
supports a trinary state so try as in

00:37:51,540 --> 00:37:57,000
three most monitoring solutions have two

00:37:54,690 --> 00:37:58,740
states passing and failing console has a

00:37:57,000 --> 00:38:00,060
third which is warning and this is

00:37:58,740 --> 00:38:00,859
really useful because things that are

00:38:00,060 --> 00:38:03,019
warning

00:38:00,859 --> 00:38:04,759
do not trigger alerts but get removed

00:38:03,019 --> 00:38:07,099
from service discovery so why might that

00:38:04,759 --> 00:38:08,509
be useful well let's say you have an

00:38:07,099 --> 00:38:11,599
order processing application it's very

00:38:08,509 --> 00:38:14,269
memory intensive uses a lot of memory to

00:38:11,599 --> 00:38:15,890
do it's order processing you can create

00:38:14,269 --> 00:38:18,589
a health checking console that says ok

00:38:15,890 --> 00:38:20,959
when memory is at 85% usage

00:38:18,589 --> 00:38:24,019
worn and when is it you know 95 percent

00:38:20,959 --> 00:38:26,569
usage alert or fail when memory reaches

00:38:24,019 --> 00:38:28,279
85% console will remove that from the

00:38:26,569 --> 00:38:30,319
discovery layer meaning it won't receive

00:38:28,279 --> 00:38:32,690
new jobs so it has an opportunity to

00:38:30,319 --> 00:38:34,190
clear its cues because hopefully if your

00:38:32,690 --> 00:38:37,059
service is well-behaved the memory usage

00:38:34,190 --> 00:38:40,309
will go down as it clears out its cues

00:38:37,059 --> 00:38:42,289
then once that application is healthy

00:38:40,309 --> 00:38:43,700
again which is entirely automated

00:38:42,289 --> 00:38:45,559
console is going to continue checking it

00:38:43,700 --> 00:38:47,809
it'll just get reacted to the load

00:38:45,559 --> 00:38:49,729
balancer so in this way you get

00:38:47,809 --> 00:38:52,579
self-healing self-describing

00:38:49,729 --> 00:38:54,079
infrastructure no one has to be woken up

00:38:52,579 --> 00:38:55,819
in the middle of the night just because

00:38:54,079 --> 00:38:59,799
one application server got a little bit

00:38:55,819 --> 00:38:59,799
too much memory it will self heal

00:38:59,829 --> 00:39:04,279
someone out of left field is this

00:39:01,999 --> 00:39:05,630
integration with a key value store every

00:39:04,279 --> 00:39:07,670
time I talk about console people are

00:39:05,630 --> 00:39:09,529
like oh you violated the unit philosophy

00:39:07,670 --> 00:39:11,900
because a key value store is not at all

00:39:09,529 --> 00:39:15,349
related to service discovery but it

00:39:11,900 --> 00:39:17,359
actually is the key value store is a

00:39:15,349 --> 00:39:19,940
highly available storage mechanism for

00:39:17,359 --> 00:39:21,229
configuration and feature flags when

00:39:19,940 --> 00:39:22,910
you're using configuration management

00:39:21,229 --> 00:39:24,499
you might rely on something like chef

00:39:22,910 --> 00:39:26,029
puppet Hansel or salt to be writing out

00:39:24,499 --> 00:39:29,420
your application config store data base

00:39:26,029 --> 00:39:31,880
configs whatever it might be and at 30

00:39:29,420 --> 00:39:33,289
minute pol interval that's not enough

00:39:31,880 --> 00:39:34,029
time to propagate a change throughout

00:39:33,289 --> 00:39:36,380
the system

00:39:34,029 --> 00:39:38,209
instead you want something that's highly

00:39:36,380 --> 00:39:40,640
available an edge triggered like

00:39:38,209 --> 00:39:42,859
consoles key value store that the moment

00:39:40,640 --> 00:39:45,229
a change happens in configuration it

00:39:42,859 --> 00:39:47,809
immediately propagates to all servers in

00:39:45,229 --> 00:39:49,099
the system who are watching that key so

00:39:47,809 --> 00:39:51,319
this allows you to have a feature flag

00:39:49,099 --> 00:39:53,359
that instantaneously triggers across

00:39:51,319 --> 00:39:55,569
your entire fleet without waiting for a

00:39:53,359 --> 00:39:57,319
big CM process to come through it

00:39:55,569 --> 00:40:00,739
supports this using what's called

00:39:57,319 --> 00:40:02,930
blocking queries that allow us to push

00:40:00,739 --> 00:40:04,549
out changes instantaneously to

00:40:02,930 --> 00:40:06,739
applications or services that are

00:40:04,549 --> 00:40:08,809
watching for those changes and then it

00:40:06,739 --> 00:40:10,729
integrates with optional ACL so if you

00:40:08,809 --> 00:40:13,369
have particular data that is sensitive

00:40:10,729 --> 00:40:14,760
you can prevent people using Ackles and

00:40:13,369 --> 00:40:17,070
tokens to retrieve that info

00:40:14,760 --> 00:40:18,630
Meishan at that pass but more

00:40:17,070 --> 00:40:21,180
importantly the key value store is the

00:40:18,630 --> 00:40:22,680
primitive basis for automation and

00:40:21,180 --> 00:40:25,830
orchestration which we're talking about

00:40:22,680 --> 00:40:28,380
in a second so what does the kV store

00:40:25,830 --> 00:40:31,260
look like well you can write or use the

00:40:28,380 --> 00:40:32,490
API to integrate with the kV store so

00:40:31,260 --> 00:40:35,130
here's an example of putting some data

00:40:32,490 --> 00:40:38,370
into the KB store here I'm putting the

00:40:35,130 --> 00:40:40,530
value bar at the key foo and here I'm

00:40:38,370 --> 00:40:42,990
getting that data back out so console kV

00:40:40,530 --> 00:40:46,020
getsu so very simple very

00:40:42,990 --> 00:40:48,330
straightforward easy to use there's also

00:40:46,020 --> 00:40:52,980
again the API if you're not using the

00:40:48,330 --> 00:40:55,140
console CLI directly in console also

00:40:52,980 --> 00:40:56,700
supports multiple data centers so at

00:40:55,140 --> 00:40:58,400
house your Corp we define a data center

00:40:56,700 --> 00:41:01,470
as a highly high-bandwidth low-latency

00:40:58,400 --> 00:41:06,300
collection of machines so like a V PC is

00:41:01,470 --> 00:41:08,040
our definition of a data center console

00:41:06,300 --> 00:41:09,450
usually secures them the local data

00:41:08,040 --> 00:41:11,250
center but if for some reason you need

00:41:09,450 --> 00:41:13,620
to query another data center you can do

00:41:11,250 --> 00:41:15,060
that and we have an open source UI that

00:41:13,620 --> 00:41:18,150
lets you see the health of all your data

00:41:15,060 --> 00:41:21,510
centers in one view so for some reason

00:41:18,150 --> 00:41:23,490
let's say hypothetically my local data

00:41:21,510 --> 00:41:25,590
center is down I could delegate a query

00:41:23,490 --> 00:41:27,390
to my Singapore data center or to my

00:41:25,590 --> 00:41:29,520
Germany data center for example because

00:41:27,390 --> 00:41:35,120
that's geographically closest and we can

00:41:29,520 --> 00:41:35,120
do this again via the DNS or HTTP API s

00:41:36,260 --> 00:41:39,840
but what's really great about console

00:41:38,430 --> 00:41:41,580
are the things that are built on top of

00:41:39,840 --> 00:41:43,290
it so in addition to your own

00:41:41,580 --> 00:41:46,170
integrations console has a number of

00:41:43,290 --> 00:41:50,250
first-class citizens for events execs

00:41:46,170 --> 00:41:51,930
and watches the kV Store by being highly

00:41:50,250 --> 00:41:54,360
available in supporting the cap theorem

00:41:51,930 --> 00:41:56,730
allows us to build really powerful

00:41:54,360 --> 00:41:58,410
orchestration tools we can do things

00:41:56,730 --> 00:42:00,390
like implement client-side leader

00:41:58,410 --> 00:42:01,740
election without the need to understand

00:42:00,390 --> 00:42:03,140
how leader election works and all the

00:42:01,740 --> 00:42:05,880
different consensus algorithms out there

00:42:03,140 --> 00:42:08,850
we can build a distributed locking and

00:42:05,880 --> 00:42:10,890
event system and all of these approaches

00:42:08,850 --> 00:42:13,310
are proven to scale to thousands and

00:42:10,890 --> 00:42:15,750
even hundreds of thousands of agents

00:42:13,310 --> 00:42:17,250
some of the largest console clusters out

00:42:15,750 --> 00:42:19,170
there are a hundred and fifty thousand

00:42:17,250 --> 00:42:21,390
two hundred thousand nodes and they're

00:42:19,170 --> 00:42:23,880
running at really high capacity

00:42:21,390 --> 00:42:25,590
consistently without failure it's a very

00:42:23,880 --> 00:42:28,290
battle-tested technology the day's

00:42:25,590 --> 00:42:31,080
production ready so here's

00:42:28,290 --> 00:42:33,180
examples of the orchestration layer once

00:42:31,080 --> 00:42:34,590
you move into a micro service-oriented

00:42:33,180 --> 00:42:36,930
architecture you might need to do

00:42:34,590 --> 00:42:38,190
something like tail the logs across all

00:42:36,930 --> 00:42:39,930
of your services because you haven't

00:42:38,190 --> 00:42:42,000
implemented distributed logging yet or

00:42:39,930 --> 00:42:43,860
you might need to reboot all your

00:42:42,000 --> 00:42:45,240
particular application servers to do a

00:42:43,860 --> 00:42:47,700
kernel upgrade and you need to do that

00:42:45,240 --> 00:42:49,140
level of orchestration so instead of SS

00:42:47,700 --> 00:42:51,480
Aging in every machine and manually

00:42:49,140 --> 00:42:53,910
running commands you can use consuls

00:42:51,480 --> 00:42:55,980
execs and Watchers and built-in locking

00:42:53,910 --> 00:42:58,710
primitives to orchestrate this by

00:42:55,980 --> 00:43:00,540
running one command and walking away so

00:42:58,710 --> 00:43:02,130
here's an example of an event execution

00:43:00,540 --> 00:43:04,530
that allows me to do application

00:43:02,130 --> 00:43:06,300
deployments so I might have an event

00:43:04,530 --> 00:43:08,520
called deploy that pulls down a

00:43:06,300 --> 00:43:12,030
particular sha from git or github and

00:43:08,520 --> 00:43:14,100
then executes the deploy script on my

00:43:12,030 --> 00:43:17,040
servers I can create a watch for that

00:43:14,100 --> 00:43:19,770
deploy and that deploys receives the

00:43:17,040 --> 00:43:22,140
payload in that script or I could

00:43:19,770 --> 00:43:23,970
manually execute my own deploy using the

00:43:22,140 --> 00:43:29,160
console exec command all three of these

00:43:23,970 --> 00:43:30,320
are basically the same thing so what

00:43:29,160 --> 00:43:33,090
about security

00:43:30,320 --> 00:43:34,980
well console uses a gossip protocol

00:43:33,090 --> 00:43:36,840
that's primarily over UDP and if you

00:43:34,980 --> 00:43:39,060
know anything about UDP that's not

00:43:36,840 --> 00:43:42,900
really secure especially if you're in a

00:43:39,060 --> 00:43:45,990
shared environment UDP like multicast er

00:43:42,900 --> 00:43:48,600
M cast is anyone on the network can

00:43:45,990 --> 00:43:51,030
listen so console has the ability to

00:43:48,600 --> 00:43:53,490
encrypt that traffic using an symmetric

00:43:51,030 --> 00:43:55,500
key so all of the agents share a key

00:43:53,490 --> 00:43:56,790
they encrypt the traffic so anyone who's

00:43:55,500 --> 00:43:58,950
on the network who doesn't have that key

00:43:56,790 --> 00:44:00,410
would be unable to see that traffic so

00:43:58,950 --> 00:44:03,000
we have encryption at the gossip layer

00:44:00,410 --> 00:44:05,870
but we also have encryption at the HTTP

00:44:03,000 --> 00:44:07,800
layer so you can put you know a TLS

00:44:05,870 --> 00:44:09,660
requirement in front of console in

00:44:07,800 --> 00:44:11,520
addition to being able to just use TLS

00:44:09,660 --> 00:44:12,960
and make sure that the server is

00:44:11,520 --> 00:44:15,480
validated you can also require

00:44:12,960 --> 00:44:17,700
client-side TLS certificates to verify

00:44:15,480 --> 00:44:20,490
that each agent can talk to the console

00:44:17,700 --> 00:44:22,080
server so by using a common certificate

00:44:20,490 --> 00:44:23,490
authority like an intermediate or self

00:44:22,080 --> 00:44:26,130
sign CA that's installed on all the

00:44:23,490 --> 00:44:27,630
hosts you can verify that anyone

00:44:26,130 --> 00:44:30,450
communicating with consoles signed by

00:44:27,630 --> 00:44:32,070
that same central CA and then as I

00:44:30,450 --> 00:44:33,840
mentioned before we have a really

00:44:32,070 --> 00:44:36,180
advanced ACL and token-based

00:44:33,840 --> 00:44:39,450
authentication system that provides for

00:44:36,180 --> 00:44:40,650
really massive scale and one at a

00:44:39,450 --> 00:44:41,430
massive scale I am talking about

00:44:40,650 --> 00:44:42,930
hundreds of

00:44:41,430 --> 00:44:44,760
thousands of nodes pushing you know

00:44:42,930 --> 00:44:46,950
petabytes or terabytes of data through

00:44:44,760 --> 00:44:51,900
the system so not just a couple hundred

00:44:46,950 --> 00:44:53,670
notes so to kind of summarize we have

00:44:51,900 --> 00:44:55,680
three technologies we've talked about

00:44:53,670 --> 00:44:57,000
here today we started with terraform

00:44:55,680 --> 00:44:58,650
which is a tool for managing the

00:44:57,000 --> 00:45:00,980
complexity of the modern data center all

00:44:58,650 --> 00:45:03,660
these api is all these upstream tools

00:45:00,980 --> 00:45:06,450
then we talked about packer which is our

00:45:03,660 --> 00:45:08,369
tool for immutable infrastructure packer

00:45:06,450 --> 00:45:11,099
moves configuration management and

00:45:08,369 --> 00:45:12,900
failures from run time to build time by

00:45:11,099 --> 00:45:14,819
using automation to output multiple

00:45:12,900 --> 00:45:17,490
machine images like Amazon a.m. eyes or

00:45:14,819 --> 00:45:18,780
docker containers but then once we move

00:45:17,490 --> 00:45:21,270
to this immediate infrastructure

00:45:18,780 --> 00:45:22,920
paradigm we need a tool to be able to

00:45:21,270 --> 00:45:24,690
orchestrate across these machines and

00:45:22,920 --> 00:45:27,180
that's what console does it provides

00:45:24,690 --> 00:45:31,200
service discovery and orchestration at

00:45:27,180 --> 00:45:33,720
the data center layer as I said before

00:45:31,200 --> 00:45:37,440
all of these tools completely open

00:45:33,720 --> 00:45:39,540
source how do we make money well we saw

00:45:37,440 --> 00:45:42,240
turns out you can't build a business by

00:45:39,540 --> 00:45:43,950
giving everything away for free so how

00:45:42,240 --> 00:45:45,690
do we make money well we sell enterprise

00:45:43,950 --> 00:45:47,790
versions of the software that include

00:45:45,690 --> 00:45:49,829
additional functionality that we think

00:45:47,790 --> 00:45:53,250
only benefits enterprise users so the

00:45:49,829 --> 00:45:54,750
fortune 500 or fortune 100 companies we

00:45:53,250 --> 00:45:56,099
have lots of companies who are using the

00:45:54,750 --> 00:45:57,930
open source tools successfully and we

00:45:56,099 --> 00:45:59,339
never intentionally hold something back

00:45:57,930 --> 00:46:01,049
from open source because we think it'll

00:45:59,339 --> 00:46:02,430
make money things that you're going to

00:46:01,049 --> 00:46:04,140
see in the enterprise tools are going to

00:46:02,430 --> 00:46:06,329
be things like Active Directory and LDAP

00:46:04,140 --> 00:46:07,950
integration things that you know at a

00:46:06,329 --> 00:46:09,390
medium-size startup or even a large

00:46:07,950 --> 00:46:10,859
company you're not going to start to see

00:46:09,390 --> 00:46:12,380
a lot but when you reach an enterprise

00:46:10,859 --> 00:46:14,160
with hundreds of thousands of employees

00:46:12,380 --> 00:46:18,410
that's where you start seeing that

00:46:14,160 --> 00:46:21,030
enterprise features which are paid more

00:46:18,410 --> 00:46:23,730
importantly everything we do is

00:46:21,030 --> 00:46:25,020
completely dog food that a thing do

00:46:23,730 --> 00:46:28,770
people know what that means here or is

00:46:25,020 --> 00:46:30,780
that an English thing so dog food is

00:46:28,770 --> 00:46:34,260
this idea that we use all of our tools

00:46:30,780 --> 00:46:36,480
internally we have a SAS product for

00:46:34,260 --> 00:46:38,869
terraform that product is deployed by

00:46:36,480 --> 00:46:41,220
terraform and it's deployed on to nomad

00:46:38,869 --> 00:46:43,470
everything we do at Corp runs on

00:46:41,220 --> 00:46:46,200
our tooling our developers use vagrant

00:46:43,470 --> 00:46:47,579
locally and sometimes we run our own pre

00:46:46,200 --> 00:46:49,260
releases in our own beta versions

00:46:47,579 --> 00:46:51,030
internally before we ever publish it

00:46:49,260 --> 00:46:52,700
publicly so that means when you're

00:46:51,030 --> 00:46:55,020
getting the software that you download

00:46:52,700 --> 00:46:56,430
we've already tested it a more

00:46:55,020 --> 00:46:58,770
running in production we've already

00:46:56,430 --> 00:47:00,810
found all the bugs and it's not just us

00:46:58,770 --> 00:47:04,200
we have hundreds of customers who are

00:47:00,810 --> 00:47:07,800
using this in production everyday so

00:47:04,200 --> 00:47:19,170
with that I'm done are there any

00:47:07,800 --> 00:47:21,450
questions other questions come on I'll

00:47:19,170 --> 00:47:25,980
ask a question says do you have stickers

00:47:21,450 --> 00:47:29,510
I do have stickers it was a great

00:47:25,980 --> 00:47:29,510
question by the was yeah

00:47:29,630 --> 00:47:36,990
any questions around or anything else

00:47:34,530 --> 00:47:41,730
sorry kirb is doing I think this is able

00:47:36,990 --> 00:47:43,100
to answer that questions as well thank

00:47:41,730 --> 00:47:47,940
god

00:47:43,100 --> 00:47:49,650
thanks for talk how do you run terraform

00:47:47,940 --> 00:47:55,860
internally like how do you trigger the

00:47:49,650 --> 00:47:57,750
terraform runs very carefully so we use

00:47:55,860 --> 00:48:00,390
our paid product which is terraform

00:47:57,750 --> 00:48:02,130
enterprise it's a either a hosted or

00:48:00,390 --> 00:48:04,800
on-premise but it's a basically

00:48:02,130 --> 00:48:06,180
terraform as a service so instead of a

00:48:04,800 --> 00:48:08,120
developer running terraform locally

00:48:06,180 --> 00:48:09,750
which is totally valid totally scalable

00:48:08,120 --> 00:48:11,700
terraform enterprise gives us

00:48:09,750 --> 00:48:13,080
collaboration so we can say like oh this

00:48:11,700 --> 00:48:15,000
change looks good this change doesn't it

00:48:13,080 --> 00:48:16,620
gives us a full history of changes it

00:48:15,000 --> 00:48:18,450
also lets us control the terraform

00:48:16,620 --> 00:48:19,890
version so we can opt into like

00:48:18,450 --> 00:48:21,780
pre-release versions on terraform

00:48:19,890 --> 00:48:24,000
enterprise even if the binary isn't

00:48:21,780 --> 00:48:25,620
published yet but it's a permission

00:48:24,000 --> 00:48:27,300
based model as well so we have you know

00:48:25,620 --> 00:48:29,400
only people on our ops team can actually

00:48:27,300 --> 00:48:31,950
make op space changes but each you know

00:48:29,400 --> 00:48:33,390
we have a team that works on Baker and

00:48:31,950 --> 00:48:34,440
they can control like a subset of the

00:48:33,390 --> 00:48:35,670
infrastructure and that's what you get

00:48:34,440 --> 00:48:36,690
out of styrofoam Enterprise that you

00:48:35,670 --> 00:48:44,790
don't get out of running terror from

00:48:36,690 --> 00:48:48,930
locally okay thanks thanks for the talk

00:48:44,790 --> 00:48:50,160
would that be telephone version 1.0 so

00:48:48,930 --> 00:48:52,830
the question is will it be when will

00:48:50,160 --> 00:48:54,900
terraform p1l this is a question we get

00:48:52,830 --> 00:48:58,020
asked a lot especially in customer

00:48:54,900 --> 00:49:01,860
meetings you know people think that 1.0

00:48:58,020 --> 00:49:03,930
is something special at hatchet corp we

00:49:01,860 --> 00:49:05,850
don't really follow semver for our

00:49:03,930 --> 00:49:07,350
products we follow semantic versioning

00:49:05,850 --> 00:49:08,869
for our libraries but not for our

00:49:07,350 --> 00:49:11,940
products

00:49:08,869 --> 00:49:13,109
and it's always interesting to hear

00:49:11,940 --> 00:49:14,970
people ask this question because if you

00:49:13,109 --> 00:49:19,590
look at Microsoft Office

00:49:14,970 --> 00:49:21,000
it started at 13.5 like to some

00:49:19,590 --> 00:49:23,160
organizations version numbers mean a lot

00:49:21,000 --> 00:49:26,700
into others they don't and to us a major

00:49:23,160 --> 00:49:31,950
version of 1.0 is our guarantee that the

00:49:26,700 --> 00:49:35,310
api will not change for us 0.2 is api

00:49:31,950 --> 00:49:36,390
stability so when you hit 0 to 0 that is

00:49:35,310 --> 00:49:37,290
whenever you can run this in production

00:49:36,390 --> 00:49:39,060
it's ready to go

00:49:37,290 --> 00:49:42,150
it's not going to crash it's not going

00:49:39,060 --> 00:49:44,190
to panic but you might not get the

00:49:42,150 --> 00:49:45,930
logging and the metrics and the analysis

00:49:44,190 --> 00:49:48,330
in the event of like something going on

00:49:45,930 --> 00:49:49,230
when you hit 0.3 dot 0 that's where we

00:49:48,330 --> 00:49:50,970
think like this thing is

00:49:49,230 --> 00:49:53,550
production-ready and you can use it at

00:49:50,970 --> 00:49:55,170
scale everything beyond that is trying

00:49:53,550 --> 00:49:58,800
to identify the landscape of like what

00:49:55,170 --> 00:50:01,320
makes Consular terraformer or packer 1.0

00:49:58,800 --> 00:50:03,210
and for Packer we just hit 1 dot oh and

00:50:01,320 --> 00:50:05,250
that was when we we identified what we

00:50:03,210 --> 00:50:08,160
think is the feature set the complete

00:50:05,250 --> 00:50:10,080
feature set and for terraform for us to

00:50:08,160 --> 00:50:12,240
reach one dot oh we need a really stable

00:50:10,080 --> 00:50:13,830
provider plug-in API and that's

00:50:12,240 --> 00:50:16,440
something we're working on now we need a

00:50:13,830 --> 00:50:17,490
way for plugins like providers and

00:50:16,440 --> 00:50:19,260
provisioners to be able to iterate

00:50:17,490 --> 00:50:21,089
separately from terraform score and

00:50:19,260 --> 00:50:22,349
that's something our team is actively

00:50:21,089 --> 00:50:24,359
focused on it's conversation I'm

00:50:22,349 --> 00:50:26,300
actually a part of to try to build that

00:50:24,359 --> 00:50:28,800
ecosystem out and once we get to that

00:50:26,300 --> 00:50:30,390
terraform can very quickly go to 1.0 it

00:50:28,800 --> 00:50:32,670
then becomes a question of when does the

00:50:30,390 --> 00:50:35,070
AWS provider go to 1.0 when does the

00:50:32,670 --> 00:50:36,630
terraform Google provider go to 1.0 and

00:50:35,070 --> 00:50:38,609
those are different questions because a

00:50:36,630 --> 00:50:40,440
lot of those particularly for you know

00:50:38,609 --> 00:50:41,550
Google and Amazon is they're still

00:50:40,440 --> 00:50:43,260
making products and they're going to

00:50:41,550 --> 00:50:45,869
continue making products so if we make a

00:50:43,260 --> 00:50:49,440
1.0 how do we handle a situation where

00:50:45,869 --> 00:50:51,480
Amazon deprecates an API because that

00:50:49,440 --> 00:50:53,010
breaks our API promise so to a certain

00:50:51,480 --> 00:50:55,530
extent we're also waiting for stability

00:50:53,010 --> 00:50:57,119
upstream in terraform for console we

00:50:55,530 --> 00:50:59,160
have a much clearer path to 1.0 so

00:50:57,119 --> 00:51:02,490
you'll probably see that this year early

00:50:59,160 --> 00:51:05,670
next year but my recommendation is don't

00:51:02,490 --> 00:51:08,609
don't let 1.0 hold you back it's not an

00:51:05,670 --> 00:51:09,230
indication of stability question over

00:51:08,609 --> 00:51:12,089
here

00:51:09,230 --> 00:51:13,830
thank you for my career talk I have a

00:51:12,089 --> 00:51:15,410
question if you have a typical

00:51:13,830 --> 00:51:18,480
infrastructure let's say that as

00:51:15,410 --> 00:51:20,560
terraform and then spins up say some VMs

00:51:18,480 --> 00:51:22,870
which then inside of them

00:51:20,560 --> 00:51:25,270
puppet or something like that how would

00:51:22,870 --> 00:51:27,820
you build the scenario where something

00:51:25,270 --> 00:51:29,590
in those VMs produces some data which

00:51:27,820 --> 00:51:31,330
you want to feed back into the

00:51:29,590 --> 00:51:34,300
definition of your terraform

00:51:31,330 --> 00:51:37,960
infrastructure for example you know some

00:51:34,300 --> 00:51:40,210
value that indicates their load or some

00:51:37,960 --> 00:51:41,710
other thing you want to then cause pair

00:51:40,210 --> 00:51:43,540
form to have more of something or less

00:51:41,710 --> 00:51:47,140
of something or do something entirely

00:51:43,540 --> 00:51:48,520
that feedback mechanism so the question

00:51:47,140 --> 00:51:50,920
is basically how do I get information

00:51:48,520 --> 00:51:53,620
back in to terraform from the things I

00:51:50,920 --> 00:51:57,040
provision there's a few ways to do that

00:51:53,620 --> 00:52:00,700
the most common one is using the console

00:51:57,040 --> 00:52:02,350
provider so if you're running console on

00:52:00,700 --> 00:52:03,670
these machines you can actually write to

00:52:02,350 --> 00:52:05,970
console of key value store from the

00:52:03,670 --> 00:52:08,800
machine so you write - you know the key

00:52:05,970 --> 00:52:11,940
terraform slash instance 1 slash load

00:52:08,800 --> 00:52:15,100
with a value of 0.08 or something right

00:52:11,940 --> 00:52:16,510
terraform has a resource that's whether

00:52:15,100 --> 00:52:18,970
it's called a data source which is a

00:52:16,510 --> 00:52:21,100
read-only resource for query and console

00:52:18,970 --> 00:52:22,840
so it can then reach in to your data

00:52:21,100 --> 00:52:24,820
center via like a bastion host and read

00:52:22,840 --> 00:52:27,940
from that console data store to pull

00:52:24,820 --> 00:52:30,160
that data out and act on it so when that

00:52:27,940 --> 00:52:32,410
data changes in the VM will go into

00:52:30,160 --> 00:52:33,400
console and then console causing form

00:52:32,410 --> 00:52:35,470
applied

00:52:33,400 --> 00:52:37,810
you can't trigger it that way because

00:52:35,470 --> 00:52:40,420
the data is kind of it's like a one-way

00:52:37,810 --> 00:52:43,180
one-way you would have to be running

00:52:40,420 --> 00:52:45,220
terraform on some type of interval so

00:52:43,180 --> 00:52:49,210
you'll run terraform every 10 minutes to

00:52:45,220 --> 00:52:50,650
trigger that change okay thank you it

00:52:49,210 --> 00:52:54,420
sounds like what you're asking is how do

00:52:50,650 --> 00:52:57,550
I build my own auto scaling based off of

00:52:54,420 --> 00:52:59,590
data and that's currently like an

00:52:57,550 --> 00:53:04,450
unsolved problem the a lot of people are

00:52:59,590 --> 00:53:06,910
trying to solve and thanks for the talk

00:53:04,450 --> 00:53:09,640
and short question for the and service

00:53:06,910 --> 00:53:14,100
discovery in console showed an example

00:53:09,640 --> 00:53:17,110
with console as a key of e is this fixed

00:53:14,100 --> 00:53:18,700
so the question is is dot console vi

00:53:17,110 --> 00:53:22,060
know everything in console is

00:53:18,700 --> 00:53:25,720
configurable so if you wanted to run it

00:53:22,060 --> 00:53:28,780
it banana you could run it at dot banana

00:53:25,720 --> 00:53:31,450
it also supports recursos so you can

00:53:28,780 --> 00:53:33,160
query everything to console it'll you

00:53:31,450 --> 00:53:34,119
can you can set it as like your vocal

00:53:33,160 --> 00:53:36,400
DNS server

00:53:34,119 --> 00:53:40,900
and anything it can't resolve it can

00:53:36,400 --> 00:53:43,809
forward to like 8.8.4.4 em quick

00:53:40,900 --> 00:53:51,210
follow-up and have you applied for the

00:53:43,809 --> 00:53:54,240
TLD dot console now because we've made

00:53:51,210 --> 00:53:57,940
we've had the experience with utter in

00:53:54,240 --> 00:54:01,509
sorry internal domains which got bought

00:53:57,940 --> 00:54:03,849
as a generic EOD and broke many customer

00:54:01,509 --> 00:54:05,619
infrastructures I think that's a good

00:54:03,849 --> 00:54:07,869
question or a good point and something

00:54:05,619 --> 00:54:09,160
you know I'll raise the team it's a

00:54:07,869 --> 00:54:14,170
pretty hefty investment I think it's

00:54:09,160 --> 00:54:15,789
like two million dollars but the way you

00:54:14,170 --> 00:54:17,980
generally architect console that would

00:54:15,789 --> 00:54:20,559
probably never be an issue because the

00:54:17,980 --> 00:54:22,029
general installation paradigm is you

00:54:20,559 --> 00:54:23,619
always run a local agent so it's a

00:54:22,029 --> 00:54:25,599
client-server but the the agent the

00:54:23,619 --> 00:54:28,450
client always runs locally and then you

00:54:25,599 --> 00:54:30,940
use a tool like DNS mask to delegate all

00:54:28,450 --> 00:54:33,190
queries to local host so they would

00:54:30,940 --> 00:54:35,079
actually hit console before hitting like

00:54:33,190 --> 00:54:37,150
resolve comms or anything like that so

00:54:35,079 --> 00:54:39,160
even if someone did purchase that TLD it

00:54:37,150 --> 00:54:41,140
wouldn't resolve outside of that and

00:54:39,160 --> 00:54:44,410
less console didn't resolve then it

00:54:41,140 --> 00:54:54,999
would hit that TLD other questions

00:54:44,410 --> 00:54:59,499
I was wondering you're highly dependent

00:54:54,999 --> 00:55:02,009
somehow key providers in terraform how

00:54:59,499 --> 00:55:11,170
do you ensure and the compatibility

00:55:02,009 --> 00:55:13,210
especially something changes and I'm

00:55:11,170 --> 00:55:16,559
just wondering how big has it go back to

00:55:13,210 --> 00:55:20,380
leave so we just crossed 100 employees

00:55:16,559 --> 00:55:23,680
like three days ago so that was cool a

00:55:20,380 --> 00:55:25,329
cool milestone for the providers in

00:55:23,680 --> 00:55:28,180
terraform specifically because the

00:55:25,329 --> 00:55:30,789
surface area is infinite like it is

00:55:28,180 --> 00:55:31,989
actually infinite we have a few

00:55:30,789 --> 00:55:33,460
different models there are people

00:55:31,989 --> 00:55:35,200
internally at hash record whose full

00:55:33,460 --> 00:55:37,900
time job is to work on terraform core

00:55:35,200 --> 00:55:40,920
they work on the graph theory the config

00:55:37,900 --> 00:55:44,489
parsing everything that is not providers

00:55:40,920 --> 00:55:46,930
we then have provider teams so we have

00:55:44,489 --> 00:55:47,920
two and a half full-time engineers who

00:55:46,930 --> 00:55:51,609
work entirely on

00:55:47,920 --> 00:55:55,599
Mazon we also have agreements with

00:55:51,609 --> 00:55:58,780
companies like Oracle and Google where

00:55:55,599 --> 00:56:00,849
they will provide us resources so Google

00:55:58,780 --> 00:56:02,829
for example there's a person at Google

00:56:00,849 --> 00:56:04,869
our name's Dana Willow she works for

00:56:02,829 --> 00:56:07,809
Google Google signs our paycheck but she

00:56:04,869 --> 00:56:09,430
works full-time on Tara form and she is

00:56:07,809 --> 00:56:12,220
kind of the owner and the curator for

00:56:09,430 --> 00:56:13,240
all of the Google cloud resources and we

00:56:12,220 --> 00:56:14,980
have a really great symbiotic

00:56:13,240 --> 00:56:16,240
relationship with them because Google

00:56:14,980 --> 00:56:17,589
supports that project it's something

00:56:16,240 --> 00:56:19,329
that they push forward in their

00:56:17,589 --> 00:56:21,790
customers etc so they want to give us

00:56:19,329 --> 00:56:25,180
resources other things our community

00:56:21,790 --> 00:56:27,369
maintained so for example DN simple

00:56:25,180 --> 00:56:30,010
which is a DNS provider that's entirely

00:56:27,369 --> 00:56:32,410
maintained by their company they do

00:56:30,010 --> 00:56:33,819
everything they manage it we just you

00:56:32,410 --> 00:56:35,680
know merge their pull request because we

00:56:33,819 --> 00:56:39,010
trust that they know their API better

00:56:35,680 --> 00:56:40,150
than we do so it really depends on what

00:56:39,010 --> 00:56:42,460
you're talking about for the major cloud

00:56:40,150 --> 00:56:44,680
providers as your we have a slightly

00:56:42,460 --> 00:56:47,230
different working relationship with with

00:56:44,680 --> 00:56:49,119
Azure they give us code but they don't

00:56:47,230 --> 00:56:51,309
maintain it over time so we do like the

00:56:49,119 --> 00:56:52,839
data to issue triage and then they'll

00:56:51,309 --> 00:56:54,940
occasionally come in and do like a mass

00:56:52,839 --> 00:56:56,619
update of you know new api's that are

00:56:54,940 --> 00:56:59,920
available and like changes to their SDK

00:56:56,619 --> 00:57:01,359
so it really depends but we have like a

00:56:59,920 --> 00:57:07,480
pretty good sustainability model in

00:57:01,359 --> 00:57:09,880
place so um how can i implement my own

00:57:07,480 --> 00:57:11,290
like provided for terraformers it's like

00:57:09,880 --> 00:57:13,180
integrated to the source code of

00:57:11,290 --> 00:57:14,740
terraform where can i build some kind of

00:57:13,180 --> 00:57:17,740
library taken some kind of binary that

00:57:14,740 --> 00:57:18,940
then gets called by Tara forms so a good

00:57:17,740 --> 00:57:20,470
question the question is how do i build

00:57:18,940 --> 00:57:22,150
my own maybe I have my own cloud

00:57:20,470 --> 00:57:24,670
provider or my own technology and I want

00:57:22,150 --> 00:57:27,069
to integrate with Tara form first your

00:57:24,670 --> 00:57:28,660
API has to support for operations create

00:57:27,069 --> 00:57:30,280
read update and delete if it only

00:57:28,660 --> 00:57:31,960
supports read you're creating what's

00:57:30,280 --> 00:57:35,410
called a data source but that's an

00:57:31,960 --> 00:57:37,809
implementation detail I Tara forms

00:57:35,410 --> 00:57:40,480
plugin model is a G RPC server client

00:57:37,809 --> 00:57:42,160
model so you build your plug-in in any

00:57:40,480 --> 00:57:43,569
language you want but it's easiest to

00:57:42,160 --> 00:57:44,829
write and go because that's the language

00:57:43,569 --> 00:57:47,049
the Tara form is written in and there's

00:57:44,829 --> 00:57:48,880
lots of examples you build the plug-in

00:57:47,049 --> 00:57:50,890
using what's called helper schema this

00:57:48,880 --> 00:57:52,329
is kind of a little thing but you build

00:57:50,890 --> 00:57:53,650
a helper schema you say these are my

00:57:52,329 --> 00:57:55,809
inputs these are my outputs these are

00:57:53,650 --> 00:57:57,549
the API calls I want and then you build

00:57:55,809 --> 00:58:00,840
it just a standard go binary with go

00:57:57,549 --> 00:58:02,400
build then you put it in a

00:58:00,840 --> 00:58:03,750
directory that's either adjacent to

00:58:02,400 --> 00:58:05,430
where you're running terraform or in a

00:58:03,750 --> 00:58:07,440
local terraform directory on your

00:58:05,430 --> 00:58:09,540
machine and terraform picks it up so if

00:58:07,440 --> 00:58:12,270
you name your binary terraform -

00:58:09,540 --> 00:58:14,550
provider banana it'll pick up anything

00:58:12,270 --> 00:58:17,700
prefixed with banana when you register

00:58:14,550 --> 00:58:19,350
that it's possible to write providers in

00:58:17,700 --> 00:58:20,720
other languages it's just a little bit

00:58:19,350 --> 00:58:23,100
harder because you have to re-implement

00:58:20,720 --> 00:58:25,770
some of the internals of terraform in

00:58:23,100 --> 00:58:27,000
your plugin then from distribution you

00:58:25,770 --> 00:58:29,040
just distribute that binary to your

00:58:27,000 --> 00:58:30,360
customers or to your users now there are

00:58:29,040 --> 00:58:32,730
things on the road map for terraform

00:58:30,360 --> 00:58:33,900
like a terraform plug-in install command

00:58:32,730 --> 00:58:35,610
that would automate that process a

00:58:33,900 --> 00:58:36,750
little bit more but it's very easy to

00:58:35,610 --> 00:58:39,090
write your own terraform plugin and

00:58:36,750 --> 00:58:44,510
there's actually a guide that yours

00:58:39,090 --> 00:58:49,970
truly just wrote the other day I will

00:58:44,510 --> 00:58:53,130
throw this up here not this one this one

00:58:49,970 --> 00:58:55,200
this is a like a step-by-step guide of

00:58:53,130 --> 00:58:56,430
writing your own provider you know it's

00:58:55,200 --> 00:58:58,260
a little bit long but there's a lot of

00:58:56,430 --> 00:58:59,790
terraform internals here but it gives

00:58:58,260 --> 00:59:02,130
you some code samples of like okay

00:58:59,790 --> 00:59:03,300
here's how you serve the plugin here's

00:59:02,130 --> 00:59:05,250
the naming conventions that i just

00:59:03,300 --> 00:59:08,190
talked about and then you know here's a

00:59:05,250 --> 00:59:12,000
resource just defines like a create read

00:59:08,190 --> 00:59:16,020
update and delete function yeah over

00:59:12,000 --> 00:59:19,260
here so the last question you say you do

00:59:16,020 --> 00:59:23,010
the feeding but for example why isn't

00:59:19,260 --> 00:59:24,600
the nomads pregnant from supported back

00:59:23,010 --> 00:59:28,980
office in the community supported

00:59:24,600 --> 00:59:31,470
plugins why is the nomads the nomads

00:59:28,980 --> 00:59:35,100
plug-in in the room a provider in

00:59:31,470 --> 00:59:37,800
telephone is community supported it's

00:59:35,100 --> 00:59:41,490
not we support it it's probably just not

00:59:37,800 --> 00:59:43,560
documented correctly we we maintain all

00:59:41,490 --> 00:59:46,110
of the hash coupons so console fall and

00:59:43,560 --> 00:59:48,990
Nomad we also maintain the kubernetes

00:59:46,110 --> 00:59:51,090
ones now it's possible that

00:59:48,990 --> 00:59:53,070
documentation is just out of date

00:59:51,090 --> 00:59:55,290
okay thank you it might have been

00:59:53,070 --> 00:59:57,810
originally community contributed which

00:59:55,290 --> 00:59:59,520
is why it might have that atop but we

00:59:57,810 --> 01:00:05,040
everything that has our name on it we

00:59:59,520 --> 01:00:06,960
maintain okay so yeah we're done time is

01:00:05,040 --> 01:00:07,340
over thank you very much again for the

01:00:06,960 --> 01:00:13,179
greater

01:00:07,340 --> 01:00:13,179

YouTube URL: https://www.youtube.com/watch?v=ytdMANuKIsM


