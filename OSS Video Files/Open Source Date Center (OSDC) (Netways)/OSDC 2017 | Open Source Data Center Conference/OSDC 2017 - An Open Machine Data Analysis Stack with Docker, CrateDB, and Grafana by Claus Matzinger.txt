Title: OSDC 2017 - An Open Machine Data Analysis Stack with Docker, CrateDB, and Grafana by Claus Matzinger
Publication date: 2017-05-31
Playlist: OSDC 2017 | Open Source Data Center Conference
Description: 
	Predictive analytics, Internet of Things, Industry 4.0 - everybody has heard them at least once, but what do real installations look like? How can containerized Microservices help deployment and increase productivity? Claus from Crate.io will answer any and all of these questions and show real world examples with a stack based on Raspberry Pis, Grafana, Docker, and Rust.
Captions: 
	00:00:09,719 --> 00:00:18,460
welcome back now the last hawk arm

00:00:14,369 --> 00:00:21,009
what's clouds matching eyes about data

00:00:18,460 --> 00:00:25,090
analysis data analyzers was opened with

00:00:21,009 --> 00:00:36,520
an exhaust Ecch so please welcome him

00:00:25,090 --> 00:00:39,430
and have fun hi alright so don't let

00:00:36,520 --> 00:00:42,820
yourself be confused by the title it is

00:00:39,430 --> 00:00:44,260
the same content it's yeah as a

00:00:42,820 --> 00:00:47,800
different title sometimes

00:00:44,260 --> 00:00:50,310
so yeah it's about databases and data

00:00:47,800 --> 00:00:54,910
analysis and sensor data analysis and

00:00:50,310 --> 00:00:57,430
ultimately that's about myself I'm Klaus

00:00:54,910 --> 00:00:59,590
I work at a company called cratered IO

00:00:57,430 --> 00:01:02,950
and I've been here for roughly two years

00:00:59,590 --> 00:01:05,320
which is almost you know almost half its

00:01:02,950 --> 00:01:08,890
life time or even a bit more since it

00:01:05,320 --> 00:01:12,729
was founded in 2013 and it's a start-up

00:01:08,890 --> 00:01:15,939
that's based out of Austria well then

00:01:12,729 --> 00:01:21,369
and yeah and or it has grown to Berlin

00:01:15,939 --> 00:01:23,619
and San Francisco it's basically from

00:01:21,369 --> 00:01:27,429
dong-bin hence the nice mountain themes

00:01:23,619 --> 00:01:31,869
and yeah you can see the landscape every

00:01:27,429 --> 00:01:33,909
time throughout the presentation I yeah

00:01:31,869 --> 00:01:37,090
also do a lot of stuff with raspberry

00:01:33,909 --> 00:01:39,240
PI's and therefore I brought one and it

00:01:37,090 --> 00:01:42,039
will be featured in the live demo and

00:01:39,240 --> 00:01:44,529
yeah I do you know a lot of rust

00:01:42,039 --> 00:01:46,299
programming if you know what if you know

00:01:44,529 --> 00:01:48,340
the language that's very nice and it

00:01:46,299 --> 00:01:50,889
also will the whole demo will feature a

00:01:48,340 --> 00:01:53,919
rust and raspberry pi's if you have any

00:01:50,889 --> 00:01:56,499
questions now or later or at some point

00:01:53,919 --> 00:01:59,229
on the lower left some does my twitter

00:01:56,499 --> 00:02:04,149
handle you can send me messages or tweet

00:01:59,229 --> 00:02:06,909
at me and you know ask me things at

00:02:04,149 --> 00:02:08,890
create I I did a lot of support and

00:02:06,909 --> 00:02:12,190
field engineering and basically like

00:02:08,890 --> 00:02:14,560
pre-sales consulting so I got a lot out

00:02:12,190 --> 00:02:16,690
in the field and I saw what people did

00:02:14,560 --> 00:02:20,770
with their sensors and with their

00:02:16,690 --> 00:02:23,410
different things data really like IOT

00:02:20,770 --> 00:02:25,120
data and yeah this is basically a

00:02:23,410 --> 00:02:29,620
talk about the pattern that we have seen

00:02:25,120 --> 00:02:31,420
that is very common in that space so but

00:02:29,620 --> 00:02:34,870
first let's talk a bit about machine

00:02:31,420 --> 00:02:37,840
data machine data is yeah data is not

00:02:34,870 --> 00:02:40,980
produced by humans and essentially what

00:02:37,840 --> 00:02:43,870
that means is it's growing a lot since

00:02:40,980 --> 00:02:45,400
machines are you know slowly taking over

00:02:43,870 --> 00:02:50,440
the world and it's predicted to be

00:02:45,400 --> 00:02:52,870
around 40 set abides by 2020 which if

00:02:50,440 --> 00:02:55,390
you if you look at the growth rate is

00:02:52,870 --> 00:02:59,830
like the growth rate is five times

00:02:55,390 --> 00:03:02,290
between five five times in the last five

00:02:59,830 --> 00:03:06,160
years basically between it what it was

00:03:02,290 --> 00:03:10,750
between 2015 and 2010 so it grows a lot

00:03:06,160 --> 00:03:13,420
and it's going to be huge so but what is

00:03:10,750 --> 00:03:14,830
machine data reading and what basically

00:03:13,420 --> 00:03:17,880
what are the characteristics of it and

00:03:14,830 --> 00:03:21,100
the characteristics mainly are that it's

00:03:17,880 --> 00:03:23,530
large and it has a lot a high insulate

00:03:21,100 --> 00:03:25,830
you alike it's streaming from something

00:03:23,530 --> 00:03:30,130
that produces a lot of data so sensors

00:03:25,830 --> 00:03:31,959
devices locks anything like that and the

00:03:30,130 --> 00:03:33,910
data looks very different from each

00:03:31,959 --> 00:03:37,750
device so it could be you know anything

00:03:33,910 --> 00:03:40,020
from pictures from IP cameras to simple

00:03:37,750 --> 00:03:43,990
sense of values that feels like a float

00:03:40,020 --> 00:03:47,560
value or JSON itself yeah it could be

00:03:43,990 --> 00:03:49,930
anything really in addition to that the

00:03:47,560 --> 00:03:52,570
data you want to basically know about

00:03:49,930 --> 00:03:55,780
the data when it happens so it's it's

00:03:52,570 --> 00:03:57,280
it's deprecating quite quickly so if you

00:03:55,780 --> 00:03:59,140
for example if your house is burning

00:03:57,280 --> 00:04:01,360
down you want to know right now and the

00:03:59,140 --> 00:04:05,680
sensor should tell you right now and not

00:04:01,360 --> 00:04:06,580
after you know some analysis to a

00:04:05,680 --> 00:04:09,250
streaming

00:04:06,580 --> 00:04:11,860
stream processing thing ran through and

00:04:09,250 --> 00:04:15,520
it took five hours to basically - yeah

00:04:11,860 --> 00:04:17,320
finish however it's not only that you

00:04:15,520 --> 00:04:19,810
don't do one when you have like a larger

00:04:17,320 --> 00:04:22,510
production environment for you know for

00:04:19,810 --> 00:04:23,890
manufacturing things you probably also

00:04:22,510 --> 00:04:25,510
want to know when your machines are

00:04:23,890 --> 00:04:29,280
breaking down so you want to basically

00:04:25,510 --> 00:04:32,350
sieve a trend for example in temperature

00:04:29,280 --> 00:04:35,740
that you know indicates your machines

00:04:32,350 --> 00:04:37,330
are kind of dying over a long period of

00:04:35,740 --> 00:04:38,620
time so at the same time as you

00:04:37,330 --> 00:04:41,650
want to have basically the data that's

00:04:38,620 --> 00:04:43,689
happening really at this very second

00:04:41,650 --> 00:04:45,669
it's best you also want to know what's

00:04:43,689 --> 00:04:49,539
happening order what has happened over

00:04:45,669 --> 00:04:52,000
the last year yeah and on top of that

00:04:49,539 --> 00:04:55,120
it's usually growing a lot so you know

00:04:52,000 --> 00:04:58,479
interest adding for example increasing

00:04:55,120 --> 00:05:02,469
the log level of on some on some server

00:04:58,479 --> 00:05:04,330
or just doubling or yeah it's easily or

00:05:02,469 --> 00:05:06,969
just doubling the read frequency of

00:05:04,330 --> 00:05:09,969
sensor will just also double the amount

00:05:06,969 --> 00:05:12,099
of data and insert rate your you're

00:05:09,969 --> 00:05:15,509
going to have to worry about so

00:05:12,099 --> 00:05:20,110
exponential growth is also a thing here

00:05:15,509 --> 00:05:22,509
but yeah this this is all manageable and

00:05:20,110 --> 00:05:24,639
the basically the scenarios and the

00:05:22,509 --> 00:05:27,009
whole things where you can find this is

00:05:24,639 --> 00:05:30,569
yeah the Internet of Things of course

00:05:27,009 --> 00:05:33,699
different sensors cameras and whatnot

00:05:30,569 --> 00:05:36,310
wearables and gadgets as well as logs

00:05:33,699 --> 00:05:39,580
and monitoring of server farms and you

00:05:36,310 --> 00:05:43,389
know I'm not any distributed application

00:05:39,580 --> 00:05:45,370
really industry 4.0 which is which has

00:05:43,389 --> 00:05:48,699
been you know a thing for the last I

00:05:45,370 --> 00:05:51,819
don't know five years and it's maybe

00:05:48,699 --> 00:05:53,740
taking off this year so it's basically

00:05:51,819 --> 00:05:57,039
about production lines and automating

00:05:53,740 --> 00:05:58,509
the most of the processes there and as

00:05:57,039 --> 00:06:00,639
well as vehicle data from you know

00:05:58,509 --> 00:06:04,120
location data over predictive

00:06:00,639 --> 00:06:06,000
maintenance and things like that at

00:06:04,120 --> 00:06:09,580
great we have a few customers that do

00:06:06,000 --> 00:06:12,639
yeah sort of that like that for example

00:06:09,580 --> 00:06:15,339
cricketer clicked live dot IO which do

00:06:12,639 --> 00:06:17,889
basically fleet management in in

00:06:15,339 --> 00:06:19,960
Singapore I think and they have like a

00:06:17,889 --> 00:06:22,779
huge taxi company as a customer so they

00:06:19,960 --> 00:06:26,349
track a lot of vehicles and also can

00:06:22,779 --> 00:06:28,839
determine what the route efficiency they

00:06:26,349 --> 00:06:31,509
can reconstruct accidents they can do

00:06:28,839 --> 00:06:35,430
very predictive maintenance and send

00:06:31,509 --> 00:06:38,800
cars to repair before they break down

00:06:35,430 --> 00:06:40,690
but that also has them operate on a

00:06:38,800 --> 00:06:43,539
level of two thousand data points per

00:06:40,690 --> 00:06:45,940
car per second and they have to enter

00:06:43,539 --> 00:06:49,080
have to get down what you know what's

00:06:45,940 --> 00:06:49,080
really valuable about it

00:06:49,110 --> 00:06:54,979
similarly room monitor which too does

00:06:52,380 --> 00:06:58,169
something similar but in apartment so

00:06:54,979 --> 00:07:01,940
while the these things don't move around

00:06:58,169 --> 00:07:03,810
and don't have as many I don't know

00:07:01,940 --> 00:07:07,500
devices in them that can break down

00:07:03,810 --> 00:07:09,330
easily they have other other yeah

00:07:07,500 --> 00:07:11,819
maintenance needs like for example the

00:07:09,330 --> 00:07:14,069
noise level or if heating is on and at

00:07:11,819 --> 00:07:15,180
the same time the window is open so

00:07:14,069 --> 00:07:17,810
things like that to make it more

00:07:15,180 --> 00:07:22,349
efficient to make it just more yeah

00:07:17,810 --> 00:07:27,240
effective in in doing for like hotel

00:07:22,349 --> 00:07:30,300
rooms and renting out apartments so yeah

00:07:27,240 --> 00:07:32,490
as the last customer that does something

00:07:30,300 --> 00:07:35,490
very different in the whole spaces

00:07:32,490 --> 00:07:37,550
skyhigh networks they do these are a

00:07:35,490 --> 00:07:39,750
cloud access security broker which

00:07:37,550 --> 00:07:44,909
essentially means they are logging all

00:07:39,750 --> 00:07:47,159
the cloud logins and interactions you do

00:07:44,909 --> 00:07:48,900
for example as a company and then they

00:07:47,159 --> 00:07:50,460
find out basically do unsupervised

00:07:48,900 --> 00:07:53,580
learning on top of that to find out

00:07:50,460 --> 00:07:58,470
what's the baseline of things like you

00:07:53,580 --> 00:08:01,050
logging in from 9:00 to 5:00 in in a

00:07:58,470 --> 00:08:03,990
European time soon for example and then

00:08:01,050 --> 00:08:08,580
all of a sudden you're there in a very

00:08:03,990 --> 00:08:11,969
weird hour in Asia or the u.s. all of a

00:08:08,580 --> 00:08:14,639
sudden and that could be and yeah worthy

00:08:11,969 --> 00:08:16,409
of an alert for example you have a la or

00:08:14,639 --> 00:08:19,380
day of a large volume of inches today of

00:08:16,409 --> 00:08:22,469
a production cluster that's roughly 60

00:08:19,380 --> 00:08:24,810
nodes of crate running and they have a

00:08:22,469 --> 00:08:29,130
couple of thousand TCP connections

00:08:24,810 --> 00:08:31,830
concurrently to handle all of that so

00:08:29,130 --> 00:08:34,409
yeah all of that has something in common

00:08:31,830 --> 00:08:36,419
which is their architecture and their

00:08:34,409 --> 00:08:38,599
architecture is something that we have

00:08:36,419 --> 00:08:43,110
figured out over the last years

00:08:38,599 --> 00:08:45,089
basically large data needs to be or

00:08:43,110 --> 00:08:48,240
needs to be handled in a scalable way

00:08:45,089 --> 00:08:50,670
which means usually people do micro

00:08:48,240 --> 00:08:53,880
services so they deploy stuff in

00:08:50,670 --> 00:08:56,790
containers for example they have to get

00:08:53,880 --> 00:08:58,380
some flexibility on their deployment

00:08:56,790 --> 00:09:02,370
itself so they can just exchange

00:08:58,380 --> 00:09:02,680
different processing pipeline items for

00:09:02,370 --> 00:09:05,710
exam

00:09:02,680 --> 00:09:08,020
or just yeah add more of the same thing

00:09:05,710 --> 00:09:11,589
in order to just get more of the

00:09:08,020 --> 00:09:14,230
performance scene but they usually

00:09:11,589 --> 00:09:16,540
struggle with is to put databases in

00:09:14,230 --> 00:09:19,120
containers and they basically anything

00:09:16,540 --> 00:09:24,310
that state the state fold usually proves

00:09:19,120 --> 00:09:26,709
to be sort of a problem yeah but let's

00:09:24,310 --> 00:09:28,410
look an example at an example so this is

00:09:26,709 --> 00:09:34,720
a common use case we have encountered

00:09:28,410 --> 00:09:38,440
and as I said before we have yeah this

00:09:34,720 --> 00:09:40,360
is as a pattern we had three three

00:09:38,440 --> 00:09:42,660
components which is usually some kind of

00:09:40,360 --> 00:09:45,940
sensor some kind of input and top here

00:09:42,660 --> 00:09:47,680
some kind of consumer that consumes this

00:09:45,940 --> 00:09:49,570
data and saves it to a database for

00:09:47,680 --> 00:09:51,670
example and some kind of visualize it

00:09:49,570 --> 00:09:54,459
and outputs it again to something that

00:09:51,670 --> 00:09:57,550
visualizer is yeah could be anything

00:09:54,459 --> 00:10:01,600
that does some does stuff with data like

00:09:57,550 --> 00:10:07,510
machine learning pipeline or similar so

00:10:01,600 --> 00:10:10,630
this is what what some yeah development

00:10:07,510 --> 00:10:13,420
environments usually look like so you

00:10:10,630 --> 00:10:15,820
have a couple of sensors here on top as

00:10:13,420 --> 00:10:18,370
well as a load balancer that does

00:10:15,820 --> 00:10:21,490
reverse proxying and maybe on the finish

00:10:18,370 --> 00:10:24,180
where you get the TLS encryption there

00:10:21,490 --> 00:10:27,070
and then you have three stacks of

00:10:24,180 --> 00:10:29,589
consumer and visualizer and this could

00:10:27,070 --> 00:10:33,670
be for example a single a single node or

00:10:29,589 --> 00:10:36,240
a single machine and three of them means

00:10:33,670 --> 00:10:39,670
you can easily achieve high availability

00:10:36,240 --> 00:10:42,130
so but there's something essentially

00:10:39,670 --> 00:10:45,700
missing something essential missing from

00:10:42,130 --> 00:10:47,950
this stack which is the data storage and

00:10:45,700 --> 00:10:51,550
with the data storage at all over all of

00:10:47,950 --> 00:10:56,260
a sudden because it is a bit more yeah

00:10:51,550 --> 00:10:59,140
more complex mainly this means first of

00:10:56,260 --> 00:11:00,880
all we have what we usually encounter is

00:10:59,140 --> 00:11:04,209
something like a lambda architecture

00:11:00,880 --> 00:11:06,430
which means now you add message queue in

00:11:04,209 --> 00:11:08,980
order to synchronize between your two

00:11:06,430 --> 00:11:12,070
storage layers one of them is basically

00:11:08,980 --> 00:11:14,589
usually an sequel database and the other

00:11:12,070 --> 00:11:16,610
thing is a fast storage layer which is

00:11:14,589 --> 00:11:18,829
in no sequel database which is

00:11:16,610 --> 00:11:21,040
known for scaling well but maybe not

00:11:18,829 --> 00:11:24,819
having as nice as a query language or

00:11:21,040 --> 00:11:29,809
not having a yeah it's not as robust as

00:11:24,819 --> 00:11:33,860
a traditional sequel database so in

00:11:29,809 --> 00:11:35,600
order to connect these what the consumer

00:11:33,860 --> 00:11:38,170
instance usually have to has to do is

00:11:35,600 --> 00:11:40,910
each of them has to have a connection to

00:11:38,170 --> 00:11:45,049
to this message queue which can which

00:11:40,910 --> 00:11:46,999
probably is its own host again and the

00:11:45,049 --> 00:11:49,879
visualizer on the other hand has to

00:11:46,999 --> 00:11:52,549
connect to to your sequel database and

00:11:49,879 --> 00:11:56,839
to your no sequel database to provide

00:11:52,549 --> 00:11:58,759
effective output of yeah of charts and

00:11:56,839 --> 00:12:04,639
I'll give you an overview of all the

00:11:58,759 --> 00:12:08,809
data so in order to achieve this usually

00:12:04,639 --> 00:12:11,860
the this results in a lot of network

00:12:08,809 --> 00:12:15,379
connections at the same time to a very

00:12:11,860 --> 00:12:19,579
few number of hosts or between SKU

00:12:15,379 --> 00:12:21,529
number or post so that's where usually

00:12:19,579 --> 00:12:24,110
the problems arise because the problems

00:12:21,529 --> 00:12:27,379
are you have a bottleneck on the network

00:12:24,110 --> 00:12:32,209
or you very much reliant on the network

00:12:27,379 --> 00:12:33,799
so you can basically yeah it is sort of

00:12:32,209 --> 00:12:35,600
a single point of failure as well as

00:12:33,799 --> 00:12:37,279
your message queue which some of them

00:12:35,600 --> 00:12:40,429
has been known not to scale very well

00:12:37,279 --> 00:12:43,790
and depending on where you do this in

00:12:40,429 --> 00:12:46,509
cloud environments this might be we are

00:12:43,790 --> 00:12:50,029
happening very often or or yeah or

00:12:46,509 --> 00:12:52,879
sometimes not at all but it's definitely

00:12:50,029 --> 00:12:55,399
definitely a risk the digit take care of

00:12:52,879 --> 00:12:59,569
and again they provide some other

00:12:55,399 --> 00:13:01,730
problems with for example of security in

00:12:59,569 --> 00:13:03,649
access control so basically every if

00:13:01,730 --> 00:13:05,809
everyone needs access or some people

00:13:03,649 --> 00:13:07,939
need access to the different hosts there

00:13:05,809 --> 00:13:10,790
you have to manage all of a sudden this

00:13:07,939 --> 00:13:12,669
is H keys and things like that you use a

00:13:10,790 --> 00:13:15,049
lot of different technologies which is

00:13:12,669 --> 00:13:18,350
sometimes doesn't allow you to build

00:13:15,049 --> 00:13:20,110
routine and expertise of the yeah of the

00:13:18,350 --> 00:13:22,699
team the tough stuff there and

00:13:20,110 --> 00:13:25,069
maintenance you know with all the

00:13:22,699 --> 00:13:29,240
different components involved it's like

00:13:25,069 --> 00:13:30,350
finding the finding the yeah the weak

00:13:29,240 --> 00:13:36,170
link sometime

00:13:30,350 --> 00:13:38,330
really hard so what we have basically

00:13:36,170 --> 00:13:40,430
extracted from that is that needs to be

00:13:38,330 --> 00:13:42,020
a database that works well for micro

00:13:40,430 --> 00:13:44,300
services that gives you a shared nothing

00:13:42,020 --> 00:13:47,060
architecture that basically means you

00:13:44,300 --> 00:13:49,040
can deploy multi and multitude of those

00:13:47,060 --> 00:13:53,390
and they don't have to be configured to

00:13:49,040 --> 00:13:55,430
do a certain thing so did they have that

00:13:53,390 --> 00:13:57,200
authorized replication built-in that has

00:13:55,430 --> 00:13:59,660
say in defaults in order to keep the

00:13:57,200 --> 00:14:01,580
configuration management low and to be

00:13:59,660 --> 00:14:03,350
resilient as well as cross-functional in

00:14:01,580 --> 00:14:08,300
order to avoid just adding another thing

00:14:03,350 --> 00:14:13,160
to your stack so does that mean we have

00:14:08,300 --> 00:14:16,340
another database there and there's there

00:14:13,160 --> 00:14:18,860
are a lot of them though so there's good

00:14:16,340 --> 00:14:20,780
man to choose from and so we said yeah

00:14:18,860 --> 00:14:23,600
let's create another one and it's called

00:14:20,780 --> 00:14:26,270
crate DB it's of course an open source

00:14:23,600 --> 00:14:30,680
database and yeah it can be found in a

00:14:26,270 --> 00:14:34,520
docker hub of course and yeah this is

00:14:30,680 --> 00:14:40,580
what some of our fans say about it for

00:14:34,520 --> 00:14:43,310
example and yeah crazy B itself is in

00:14:40,580 --> 00:14:46,340
fact to shared-nothing database it or

00:14:43,310 --> 00:14:47,750
shared nothing architecture it includes

00:14:46,340 --> 00:14:50,540
partitioning and order charting and

00:14:47,750 --> 00:14:53,540
replication all these things

00:14:50,540 --> 00:14:55,460
it's the configuration we try to keep it

00:14:53,540 --> 00:14:58,790
as simple as possible in order to get it

00:14:55,460 --> 00:15:01,250
to get to people yeah to get it easy to

00:14:58,790 --> 00:15:04,850
manage essentially and easy to scale it

00:15:01,250 --> 00:15:08,470
has the ability to work with structured

00:15:04,850 --> 00:15:12,280
and unstructured data and it gives you a

00:15:08,470 --> 00:15:16,280
sequel in order to handle all of those

00:15:12,280 --> 00:15:18,950
so how does it work underneath it's

00:15:16,280 --> 00:15:21,920
basically a disk based index and so it

00:15:18,950 --> 00:15:24,350
utilizes the Linux and mapped caching or

00:15:21,920 --> 00:15:27,170
the page caching and in memory which

00:15:24,350 --> 00:15:30,260
means that it's you know basically on

00:15:27,170 --> 00:15:33,980
disk but still in memory within memory

00:15:30,260 --> 00:15:35,630
speeds it works at sharp level so these

00:15:33,980 --> 00:15:38,480
are the units of data that are basically

00:15:35,630 --> 00:15:40,610
moved around or operated on and the

00:15:38,480 --> 00:15:44,000
query execution engine basically pushes

00:15:40,610 --> 00:15:46,399
each query down to to the shard level

00:15:44,000 --> 00:15:48,860
which means that everybody can work at

00:15:46,399 --> 00:15:53,720
the same time depending on the data they

00:15:48,860 --> 00:15:56,540
have so looking at it from from above

00:15:53,720 --> 00:16:00,079
this would be a simple example of a

00:15:56,540 --> 00:16:03,620
table that has three charts and on three

00:16:00,079 --> 00:16:07,819
nodes so they with one group vacation so

00:16:03,620 --> 00:16:10,610
it this is one table really and this

00:16:07,819 --> 00:16:14,149
gives the ability to just lose one node

00:16:10,610 --> 00:16:19,639
and still be fully operational and have

00:16:14,149 --> 00:16:22,399
a full copy of the data so we're looking

00:16:19,639 --> 00:16:24,379
at it a bit more and well from the

00:16:22,399 --> 00:16:27,589
component perspective let's say this is

00:16:24,379 --> 00:16:31,040
it follows biggity posters wire protocol

00:16:27,589 --> 00:16:35,420
as one of the import input ports as well

00:16:31,040 --> 00:16:38,120
as HTTP each of the nodes is able to

00:16:35,420 --> 00:16:40,579
parse and plan and execute the queries

00:16:38,120 --> 00:16:43,850
and basically you know ask the other

00:16:40,579 --> 00:16:45,410
nodes to do stuff for it in order to get

00:16:43,850 --> 00:16:48,889
the query yeah

00:16:45,410 --> 00:16:50,959
process underneath we for clustering we

00:16:48,889 --> 00:16:54,019
use elastic search clustering algorithm

00:16:50,959 --> 00:16:59,209
and the state storage of course this the

00:16:54,019 --> 00:17:02,120
scene so this is how we can transform

00:16:59,209 --> 00:17:04,459
the setup into something better and

00:17:02,120 --> 00:17:06,799
something that we have seen and in fact

00:17:04,459 --> 00:17:08,750
it was also something similar and

00:17:06,799 --> 00:17:12,559
independently was presented by one of

00:17:08,750 --> 00:17:15,949
our customers last week as a different

00:17:12,559 --> 00:17:19,640
conference and essentially this means

00:17:15,949 --> 00:17:23,329
putting a database onto each each host

00:17:19,640 --> 00:17:25,309
in to your other micro services and this

00:17:23,329 --> 00:17:29,659
is what this is it gives you the ability

00:17:25,309 --> 00:17:31,730
to just connect my local host now with

00:17:29,659 --> 00:17:35,510
each service basically is on the same

00:17:31,730 --> 00:17:41,390
note as or is in the same machine as the

00:17:35,510 --> 00:17:42,950
database node and this is also a set up

00:17:41,390 --> 00:17:46,850
that can be copied so each of those

00:17:42,950 --> 00:17:49,669
stacks can just be copied and added to

00:17:46,850 --> 00:17:52,640
the to this service on the service level

00:17:49,669 --> 00:17:54,890
and this would effectively increase the

00:17:52,640 --> 00:17:58,039
cluster and by having the database only

00:17:54,890 --> 00:17:59,779
communicate over the network it reduces

00:17:58,039 --> 00:18:01,730
the network traffic basically to

00:17:59,779 --> 00:18:04,549
whatever the network does whatever the

00:18:01,730 --> 00:18:08,389
database does this reduces the tech

00:18:04,549 --> 00:18:10,999
stack significantly and gives you also

00:18:08,389 --> 00:18:14,029
the ability in theory to just go in and

00:18:10,999 --> 00:18:19,879
look at the live data if someone wanted

00:18:14,029 --> 00:18:23,840
to yeah um the and this is now a

00:18:19,879 --> 00:18:25,549
horizontally scalable thing and yeah it

00:18:23,840 --> 00:18:28,460
just effectively means that there's no

00:18:25,549 --> 00:18:31,749
single point of failure yeah and there's

00:18:28,460 --> 00:18:35,230
no queue or something less moving parts

00:18:31,749 --> 00:18:37,609
at all and what is also lets do is to

00:18:35,230 --> 00:18:39,919
isolate the database that it only can

00:18:37,609 --> 00:18:43,159
that you can only access it from within

00:18:39,919 --> 00:18:45,499
the machine and removes on top of that

00:18:43,159 --> 00:18:49,879
removes a bunch of hosts that you have

00:18:45,499 --> 00:18:53,179
to buy or provision yeah it also lets

00:18:49,879 --> 00:18:55,309
you add just a regular additional node

00:18:53,179 --> 00:18:57,710
for example to connect some kind of

00:18:55,309 --> 00:19:00,379
analytic stack some kind of machining

00:18:57,710 --> 00:19:02,570
learning thing maybe in order to yeah

00:19:00,379 --> 00:19:07,129
handle or do a different kind of

00:19:02,570 --> 00:19:09,230
workload all right but this has been a

00:19:07,129 --> 00:19:12,679
lot of talking I'm now going to show you

00:19:09,230 --> 00:19:15,259
the whole thing in and for real

00:19:12,679 --> 00:19:17,659
essentially this is what it's going to

00:19:15,259 --> 00:19:20,179
look like so there's going to be the

00:19:17,659 --> 00:19:21,830
Raspberry Pi which is this one I'm

00:19:20,179 --> 00:19:25,429
connected to a temperature sensor that

00:19:21,830 --> 00:19:30,230
inserts data onto a load balancer that

00:19:25,429 --> 00:19:32,570
is essentially Dorcas warm on to three

00:19:30,230 --> 00:19:34,489
nodes and each of those nodes have will

00:19:32,570 --> 00:19:37,669
have server component that's called Eden

00:19:34,489 --> 00:19:41,629
this is basically I think that I created

00:19:37,669 --> 00:19:43,850
myself it's a wait yeah it receives data

00:19:41,629 --> 00:19:45,649
by a rest and pointers authentication

00:19:43,850 --> 00:19:49,580
and inserts it into the database under

00:19:45,649 --> 00:19:51,499
the same host so this is deployed on

00:19:49,580 --> 00:19:55,549
every host so these are again maithili

00:19:51,499 --> 00:19:57,919
hosts and the database itself will also

00:19:55,549 --> 00:20:00,739
run on the same host graph on a-- is

00:19:57,919 --> 00:20:01,519
only running on one host in order to

00:20:00,739 --> 00:20:04,309
yeah

00:20:01,519 --> 00:20:08,869
this is this is scheduled by Daka swarm

00:20:04,309 --> 00:20:11,340
and will will be might be moved around

00:20:08,869 --> 00:20:14,009
so it's not strictly on this one

00:20:11,340 --> 00:20:16,980
yeah and addition to that I will add

00:20:14,009 --> 00:20:18,870
some random number generator because the

00:20:16,980 --> 00:20:20,639
Raspberry Pi itself even though it reads

00:20:18,870 --> 00:20:22,559
like every 100 milliseconds the

00:20:20,639 --> 00:20:29,360
temperature it doesn't generate an

00:20:22,559 --> 00:20:29,360
amazing amount of data so um to start

00:20:31,940 --> 00:20:41,730
can we read that so this is this is an I

00:20:39,629 --> 00:20:44,759
will do the deployment with ansible so

00:20:41,730 --> 00:20:48,179
this is a regular ansible file

00:20:44,759 --> 00:20:50,419
really it's a llamo depending if you

00:20:48,179 --> 00:20:54,749
have seen that it's basically executing

00:20:50,419 --> 00:20:58,860
SSH commands by a Python connection on

00:20:54,749 --> 00:21:01,350
the server so this is all setup so it

00:20:58,860 --> 00:21:03,059
gives you it creates a bunch of

00:21:01,350 --> 00:21:03,539
different folders in order to mount them

00:21:03,059 --> 00:21:08,129
later on

00:21:03,539 --> 00:21:10,429
so um yeah this is all the regular stuff

00:21:08,129 --> 00:21:12,749
and then what it really does is it

00:21:10,429 --> 00:21:17,549
installs the docker stack

00:21:12,749 --> 00:21:21,509
yeah mol file and creates basically that

00:21:17,549 --> 00:21:24,110
from the template so from the template

00:21:21,509 --> 00:21:29,669
the template looks like that so it's a

00:21:24,110 --> 00:21:33,960
docker stack version 3.2 file or dr.

00:21:29,669 --> 00:21:38,159
Campos really it deploys one instance of

00:21:33,960 --> 00:21:39,480
one service of crate DB and this is

00:21:38,159 --> 00:21:42,480
based on our image

00:21:39,480 --> 00:21:46,320
yeah our image on the docker hub and use

00:21:42,480 --> 00:21:48,950
the utilizers version 2 - which is which

00:21:46,320 --> 00:21:52,490
has been released to testing I think

00:21:48,950 --> 00:21:55,889
yesterday or the day before yesterday

00:21:52,490 --> 00:21:57,840
the environment educator said you

00:21:55,889 --> 00:22:00,450
created a crazy bit easy to configure

00:21:57,840 --> 00:22:04,710
the environment variable is is

00:22:00,450 --> 00:22:07,740
calculated like that which means if you

00:22:04,710 --> 00:22:11,009
have more than 31 gigabytes of memory

00:22:07,740 --> 00:22:13,740
you will have it will set it to 32

00:22:11,009 --> 00:22:16,529
gigabytes otherwise it is just less

00:22:13,740 --> 00:22:21,259
you'll take you get half of the

00:22:16,529 --> 00:22:24,210
available memory for for your heap size

00:22:21,259 --> 00:22:25,110
which is essentially just the space

00:22:24,210 --> 00:22:29,970
where

00:22:25,110 --> 00:22:34,679
career results are compiled in the

00:22:29,970 --> 00:22:36,240
command itself is is yeah it passes in

00:22:34,679 --> 00:22:38,429
all the configuration options that are

00:22:36,240 --> 00:22:41,250
required in this case they will it will

00:22:38,429 --> 00:22:43,980
just set the number of expected notes in

00:22:41,250 --> 00:22:45,870
order to tell the database how many

00:22:43,980 --> 00:22:48,780
notes there should be so in order so it

00:22:45,870 --> 00:22:55,380
knows when it's broken essentially so

00:22:48,780 --> 00:22:57,360
this is generated by by ansible and they

00:22:55,380 --> 00:22:59,429
recover after notes and the minimum

00:22:57,360 --> 00:23:00,809
master notes are both a quorum and also

00:22:59,429 --> 00:23:03,660
generated by ansible

00:23:00,809 --> 00:23:05,970
so this is since we have three notes we

00:23:03,660 --> 00:23:08,880
will have three notes this will both be

00:23:05,970 --> 00:23:13,440
set to two so this is what the minimum

00:23:08,880 --> 00:23:16,049
requirements for electing a master yeah

00:23:13,440 --> 00:23:17,880
is you know after that it will form a

00:23:16,049 --> 00:23:20,490
cluster so it is at least two nodes have

00:23:17,880 --> 00:23:26,010
to be present for a master or for a

00:23:20,490 --> 00:23:29,130
cluster to form then we said the unicast

00:23:26,010 --> 00:23:32,549
host to create DB which is something

00:23:29,130 --> 00:23:35,460
that is also provided by dhaka swarming

00:23:32,549 --> 00:23:37,500
is depending on the this is basically

00:23:35,460 --> 00:23:40,980
the service name so this is DNS round

00:23:37,500 --> 00:23:44,280
robin internally it will just get new IP

00:23:40,980 --> 00:23:49,380
address every time it carries the this

00:23:44,280 --> 00:23:52,620
DNS address so the cluster name is also

00:23:49,380 --> 00:23:54,870
called he ross con but that that doesn't

00:23:52,620 --> 00:23:58,169
really matter I think in reality it's

00:23:54,870 --> 00:24:00,929
even different so it and it binds the

00:23:58,169 --> 00:24:03,750
network to to the site which means to

00:24:00,929 --> 00:24:06,059
all external interfaces which is okay

00:24:03,750 --> 00:24:07,890
because it's in a docker container then

00:24:06,059 --> 00:24:10,799
we map in the volume from the outside

00:24:07,890 --> 00:24:12,090
world to have it written right to disk

00:24:10,799 --> 00:24:15,179
so we know where the data is located

00:24:12,090 --> 00:24:17,540
really we set it to deployment mode

00:24:15,179 --> 00:24:22,200
global which means one instance per node

00:24:17,540 --> 00:24:25,230
and the endpoint Mon doesn't point mode

00:24:22,200 --> 00:24:27,990
as I said it is DNS round robin the

00:24:25,230 --> 00:24:31,380
placement constraint is yeah just a

00:24:27,990 --> 00:24:33,570
simple label to add type if we had

00:24:31,380 --> 00:24:37,580
different types of service or we for

00:24:33,570 --> 00:24:43,879
example use the raspberry pi's as well

00:24:37,580 --> 00:24:47,059
so yeah then we deploy gravano add that

00:24:43,879 --> 00:24:50,360
to the to the services that we provide

00:24:47,059 --> 00:24:55,460
Pravana is a dashboard tool and it's

00:24:50,360 --> 00:24:58,879
very nice to easily drag and drop some

00:24:55,460 --> 00:24:59,840
dashboards and create as a datasource

00:24:58,879 --> 00:25:02,989
plugin for it

00:24:59,840 --> 00:25:05,690
this is this one exposes port 3000

00:25:02,989 --> 00:25:07,429
insults to create data source plugin and

00:25:05,690 --> 00:25:10,039
it runs in replicator mode and even

00:25:07,429 --> 00:25:12,950
server is something that is available

00:25:10,039 --> 00:25:17,269
yeah also on the docker hub and but also

00:25:12,950 --> 00:25:22,330
it will be deployed after after create

00:25:17,269 --> 00:25:26,440
DB and it will expose the port 6200 and

00:25:22,330 --> 00:25:29,720
map its configuration file into that

00:25:26,440 --> 00:25:34,789
yeah quickly for the inventory so these

00:25:29,720 --> 00:25:37,789
are the three hosts that we work on this

00:25:34,789 --> 00:25:41,989
is of course in valid configuration but

00:25:37,789 --> 00:25:46,519
this ad for the ads basically lets the

00:25:41,989 --> 00:25:49,580
generation of the config file here if

00:25:46,519 --> 00:25:58,789
that's going to be there that's why it's

00:25:49,580 --> 00:26:00,739
there and yeah so this is what the end

00:25:58,789 --> 00:26:04,700
result of one end result looks like so

00:26:00,739 --> 00:26:07,039
this is Gravano and as we can see I

00:26:04,700 --> 00:26:09,529
plugged in the temperature the

00:26:07,039 --> 00:26:12,619
temperature sensor earlier when I when I

00:26:09,529 --> 00:26:15,019
started basically setting up so we can

00:26:12,619 --> 00:26:17,330
see that this thing here basically you

00:26:15,019 --> 00:26:19,309
know increased in temperature all the

00:26:17,330 --> 00:26:23,899
time and it's now the temperature sensor

00:26:19,309 --> 00:26:26,119
is now sitting here yeah in front of my

00:26:23,899 --> 00:26:27,950
notebook and it's paint has been warming

00:26:26,119 --> 00:26:33,259
up I can also hold it and will be

00:26:27,950 --> 00:26:35,539
probably even warmer so yeah this is not

00:26:33,259 --> 00:26:38,539
this is not a lot going on it refreshes

00:26:35,539 --> 00:26:40,789
every five seconds we can this is now

00:26:38,539 --> 00:26:44,749
every five seconds and new query running

00:26:40,789 --> 00:26:46,909
in to integrate so just to show you how

00:26:44,749 --> 00:26:49,879
a career like that looks or what a

00:26:46,909 --> 00:26:51,350
career like that looks like it's quite

00:26:49,879 --> 00:26:54,550
simple and

00:26:51,350 --> 00:26:59,420
basically what the Griffin datasource

00:26:54,550 --> 00:27:04,550
gives us so yeah it's fairly simple

00:26:59,420 --> 00:27:07,820
sequel so what we are going to do next

00:27:04,550 --> 00:27:10,520
is just look quickly at the admin UI so

00:27:07,820 --> 00:27:13,850
of course create comes with an admin UI

00:27:10,520 --> 00:27:18,050
and this is a new testing version

00:27:13,850 --> 00:27:20,500
version two up here and this is why I

00:27:18,050 --> 00:27:23,870
couldn't buy my Enterprise license yet

00:27:20,500 --> 00:27:25,940
and yeah we have the India of one table

00:27:23,870 --> 00:27:28,280
that already has some data in it which

00:27:25,940 --> 00:27:32,840
is entered a temperature data from my

00:27:28,280 --> 00:27:36,680
living room and the table setup looks

00:27:32,840 --> 00:27:39,470
like that so it it as as egg-crate

00:27:36,680 --> 00:27:44,870
supports basically all the fancy new

00:27:39,470 --> 00:27:47,990
sequel data types like object which will

00:27:44,870 --> 00:27:50,810
be here so in this case we have we want

00:27:47,990 --> 00:27:53,480
to be flexible about the number of yam

00:27:50,810 --> 00:27:56,660
values and the data types of the values

00:27:53,480 --> 00:27:59,380
one we want to store so we have this the

00:27:56,660 --> 00:28:06,230
data from this sensor is essentially a

00:27:59,380 --> 00:28:08,270
so air pressure and temperature data so

00:28:06,230 --> 00:28:10,940
this is an object that has different

00:28:08,270 --> 00:28:15,140
keys in it and the keys are for this

00:28:10,940 --> 00:28:17,210
example very much what what we want to

00:28:15,140 --> 00:28:21,070
put in there right so it's either

00:28:17,210 --> 00:28:27,830
parameter temperature or a random number

00:28:21,070 --> 00:28:30,620
and then again the the ultimate data

00:28:27,830 --> 00:28:33,050
types here are basically string or float

00:28:30,620 --> 00:28:37,070
or whatever data type you know from Java

00:28:33,050 --> 00:28:41,060
so it's a more open kind of yeah

00:28:37,070 --> 00:28:43,400
environment here we add some metadata to

00:28:41,060 --> 00:28:46,640
each row which means that each row has

00:28:43,400 --> 00:28:48,380
an agent name and the role on which this

00:28:46,640 --> 00:28:51,100
yeah very common problem if you have

00:28:48,380 --> 00:28:53,810
multiple sensors on different devices

00:28:51,100 --> 00:28:56,600
they you know create a huge path and

00:28:53,810 --> 00:28:59,360
they are very hard to find not in

00:28:56,600 --> 00:29:01,910
reality but on the junk to map to

00:28:59,360 --> 00:29:04,820
reality then we have a generated column

00:29:01,910 --> 00:29:07,820
which extracts the month every

00:29:04,820 --> 00:29:10,820
time from from the timestamp we provide

00:29:07,820 --> 00:29:12,950
and by extracting the month each time

00:29:10,820 --> 00:29:16,399
this let's see lets us create a petition

00:29:12,950 --> 00:29:21,620
which means that we have one sort of one

00:29:16,399 --> 00:29:24,220
virtual table per column per month which

00:29:21,620 --> 00:29:26,809
lets us it gives us some handle on

00:29:24,220 --> 00:29:29,409
deprecating the data quickly and for

00:29:26,809 --> 00:29:32,269
example removing replicas from older

00:29:29,409 --> 00:29:34,070
from all the values and all the tables

00:29:32,269 --> 00:29:36,320
all the months and stuff that we don't

00:29:34,070 --> 00:29:38,690
need anymore maybe want to save space on

00:29:36,320 --> 00:29:40,809
or we can just you know drop all the

00:29:38,690 --> 00:29:43,850
older petitions and it will

00:29:40,809 --> 00:29:46,850
automatically also improve query speeds

00:29:43,850 --> 00:29:52,129
based on the yeah the data size that we

00:29:46,850 --> 00:29:55,279
are basically selecting there yeah the

00:29:52,129 --> 00:29:58,940
data itself is literally a hundred

00:29:55,279 --> 00:30:01,429
millisecond sensor readings from yeah

00:29:58,940 --> 00:30:05,690
somewhere near the floor of my living

00:30:01,429 --> 00:30:08,179
room and it's it's yeah it's last four

00:30:05,690 --> 00:30:09,830
months with some interruptions but it

00:30:08,179 --> 00:30:13,100
gives us essentially a thirty two

00:30:09,830 --> 00:30:18,519
million rows with via roughly two

00:30:13,100 --> 00:30:22,250
gigabyte of data the cluster itself are

00:30:18,519 --> 00:30:23,899
three nodes they are hosted on packet

00:30:22,250 --> 00:30:27,529
dotnet which is basically a bare metal

00:30:23,899 --> 00:30:29,330
cloud hoster and each of them has four

00:30:27,529 --> 00:30:32,809
cores and their internal atoms so they

00:30:29,330 --> 00:30:34,460
are not terribly fast they come with

00:30:32,809 --> 00:30:38,570
eight gigabytes of memory and the

00:30:34,460 --> 00:30:42,320
necessity yeah we provided half the

00:30:38,570 --> 00:30:44,509
memory to the heap and this is basically

00:30:42,320 --> 00:30:51,049
this is the hardware setup we are

00:30:44,509 --> 00:30:53,960
working with here alright so yeah this

00:30:51,049 --> 00:30:55,909
is this is refreshing every five seconds

00:30:53,960 --> 00:31:00,289
running the queries and we want to have

00:30:55,909 --> 00:31:03,110
now a bit more of yeah maybe more more

00:31:00,289 --> 00:31:06,440
things to do there and maybe increase

00:31:03,110 --> 00:31:08,059
the load a little bit so I'm now going

00:31:06,440 --> 00:31:09,860
to run the Python script it essentially

00:31:08,059 --> 00:31:13,360
generates a random number and inserts

00:31:09,860 --> 00:31:18,190
that every yeah as fast as it can and

00:31:13,360 --> 00:31:18,190
yeah as you can see this start

00:31:19,799 --> 00:31:26,470
yeah this starts here and sends over a

00:31:23,110 --> 00:31:30,220
thousand items every yeah I don't know

00:31:26,470 --> 00:31:33,370
but roughly every second I guess and

00:31:30,220 --> 00:31:36,940
this will at some point show up here as

00:31:33,370 --> 00:31:40,330
you can see the data is fairly real-time

00:31:36,940 --> 00:31:41,830
available it we can switch it here maybe

00:31:40,330 --> 00:31:47,320
to something like the last five minutes

00:31:41,830 --> 00:31:51,399
it would make more sense and getting we

00:31:47,320 --> 00:31:52,059
can just have it insert data so it sends

00:31:51,399 --> 00:31:55,750
yeah

00:31:52,059 --> 00:32:00,279
every roughly every second as a thousand

00:31:55,750 --> 00:32:03,669
inserts going on and we can see here

00:32:00,279 --> 00:32:05,559
this is the thing that's so these are

00:32:03,669 --> 00:32:07,240
always live queries when I just zoom in

00:32:05,559 --> 00:32:09,250
here or maybe some month these are

00:32:07,240 --> 00:32:12,370
always queries when that's going into

00:32:09,250 --> 00:32:15,940
the database there so we wanna maybe

00:32:12,370 --> 00:32:19,270
look back to the last 50 minutes and it

00:32:15,940 --> 00:32:21,309
should at some point there should be a

00:32:19,270 --> 00:32:27,880
little bit more coming the depends on

00:32:21,309 --> 00:32:35,399
the course on the yeah on the grouping

00:32:27,880 --> 00:32:39,460
interval so we can now basically do some

00:32:35,399 --> 00:32:42,159
so this gives us unfortunately a fairly

00:32:39,460 --> 00:32:45,640
limited handle on what we want to do

00:32:42,159 --> 00:32:49,029
because the data basically this is only

00:32:45,640 --> 00:32:52,450
you know to visualize data so we could

00:32:49,029 --> 00:32:55,360
also just run regular queries while this

00:32:52,450 --> 00:33:00,909
is inserting and maybe just find out for

00:32:55,360 --> 00:33:04,659
example what are the what happened for

00:33:00,909 --> 00:33:07,210
example in in the spikes from average

00:33:04,659 --> 00:33:13,330
temperatures here so as you can see this

00:33:07,210 --> 00:33:16,299
is a rather normal sequel query and we

00:33:13,330 --> 00:33:18,370
will go here and basically paste this in

00:33:16,299 --> 00:33:21,220
and see what the temperatures spikes in

00:33:18,370 --> 00:33:27,010
March were so this is a sub select um I

00:33:21,220 --> 00:33:30,190
think yeah which gets the data from that

00:33:27,010 --> 00:33:33,730
is smaller than the less than the

00:33:30,190 --> 00:33:36,250
average that the total average in March

00:33:33,730 --> 00:33:37,960
and as you can see March there was you

00:33:36,250 --> 00:33:41,110
know the the average is always sometimes

00:33:37,960 --> 00:33:42,549
around 21 degrees and what we want to

00:33:41,110 --> 00:33:44,500
find out this is was there something

00:33:42,549 --> 00:33:46,179
weird happening maybe or was there

00:33:44,500 --> 00:33:48,309
something what yeah

00:33:46,179 --> 00:33:49,629
or did we open the window all enough

00:33:48,309 --> 00:33:53,860
because that's where basically

00:33:49,629 --> 00:33:56,289
temperatures bike go down and yeah we

00:33:53,860 --> 00:34:01,000
can see that there's some you know some

00:33:56,289 --> 00:34:03,850
some of the sensors from pi/3 basically

00:34:01,000 --> 00:34:05,200
this is the timestamp or the day that

00:34:03,850 --> 00:34:07,659
something has happened is the minimum

00:34:05,200 --> 00:34:10,329
temperature of a day and here are 15

00:34:07,659 --> 00:34:12,609
degrees and yeah it's really it's fair

00:34:10,329 --> 00:34:17,379
this would be very normal and fairly

00:34:12,609 --> 00:34:21,960
expectable to do we can also just remove

00:34:17,379 --> 00:34:21,960
it and maybe see what what the whole

00:34:22,800 --> 00:34:29,230
month or the whole data safe assess and

00:34:26,260 --> 00:34:33,010
of course this is now suffering a bit

00:34:29,230 --> 00:34:35,050
under the inserts and then we can see

00:34:33,010 --> 00:34:41,369
that sometimes in buildings where it's

00:34:35,050 --> 00:34:44,940
really cold and other than that it's

00:34:41,369 --> 00:34:48,879
quite fine so yeah this is basically a

00:34:44,940 --> 00:34:52,800
temperature malfunctioning of the sensor

00:34:48,879 --> 00:34:57,940
if you run to instance at the same time

00:34:52,800 --> 00:35:02,950
cool so this should have been just sint

00:34:57,940 --> 00:35:04,810
more more in there um yeah but this is

00:35:02,950 --> 00:35:07,300
still inserting data as you can see this

00:35:04,810 --> 00:35:11,170
is still running nicely we can now just

00:35:07,300 --> 00:35:12,970
go and maybe just you know since we are

00:35:11,170 --> 00:35:20,829
highly highly available we should be

00:35:12,970 --> 00:35:23,109
able to turn off the node easily which

00:35:20,829 --> 00:35:25,420
can be done by just talking stopping

00:35:23,109 --> 00:35:27,869
docker since it should that's the that's

00:35:25,420 --> 00:35:30,670
what the software-defined network and

00:35:27,869 --> 00:35:35,470
whole thing basically holds together

00:35:30,670 --> 00:35:38,859
I guess the is a good word and you know

00:35:35,470 --> 00:35:43,300
doctor is not stopped this should still

00:35:38,859 --> 00:35:48,040
show us data eventually

00:35:43,300 --> 00:35:51,540
and yeah the cluster has now ranked

00:35:48,040 --> 00:35:54,570
flanked by one node it's still the data

00:35:51,540 --> 00:35:56,040
the health is critical for some reason

00:35:54,570 --> 00:36:01,270
[Music]

00:35:56,040 --> 00:36:09,460
yeah and it should show that it

00:36:01,270 --> 00:36:11,730
continues clearing that's what it should

00:36:09,460 --> 00:36:11,730
do

00:36:15,810 --> 00:36:30,440
yeah the class is still there so you can

00:36:21,180 --> 00:36:30,440
come back up the entity here alright so

00:36:31,460 --> 00:36:38,310
yeah I guess that's it so what else

00:36:36,030 --> 00:36:42,720
could we do after that

00:36:38,310 --> 00:36:44,580
s entity yeah we can scale up more we

00:36:42,720 --> 00:36:47,700
can just deploy those things and

00:36:44,580 --> 00:36:51,660
multiple services multiple devices like

00:36:47,700 --> 00:36:54,030
you know um give it away as a yeah as a

00:36:51,660 --> 00:36:55,860
docker container and we could change

00:36:54,030 --> 00:36:59,880
your castration tool just use something

00:36:55,860 --> 00:37:02,580
more reliable and proven maybe then

00:36:59,880 --> 00:37:04,970
docker swarm use something you know

00:37:02,580 --> 00:37:09,870
something that that we have more and

00:37:04,970 --> 00:37:11,610
more experience with and we could add

00:37:09,870 --> 00:37:14,490
services on top of all of that like

00:37:11,610 --> 00:37:16,800
machine learning or do some data

00:37:14,490 --> 00:37:19,830
roll-ups as I said maybe for the past

00:37:16,800 --> 00:37:24,540
month and yeah of course add backups and

00:37:19,830 --> 00:37:26,990
make it more more refined service but

00:37:24,540 --> 00:37:30,420
what it is essentially is yeah the

00:37:26,990 --> 00:37:33,990
OpenStack that you know it can be

00:37:30,420 --> 00:37:37,830
connected to and from some yeah why a

00:37:33,990 --> 00:37:39,630
sequel and for example via JDBC it gives

00:37:37,830 --> 00:37:41,580
you the ability to use sequel and just

00:37:39,630 --> 00:37:44,160
run queries that we are interested in

00:37:41,580 --> 00:37:46,850
and it should integrate well with there

00:37:44,160 --> 00:37:49,520
all kinds of legacy applications and

00:37:46,850 --> 00:37:52,560
available tools that are already there

00:37:49,520 --> 00:37:56,240
it's also horizontally scalable since

00:37:52,560 --> 00:37:58,950
this is yeah this is what the whole

00:37:56,240 --> 00:38:01,920
containerization is all about and it

00:37:58,950 --> 00:38:07,560
will increase concurrency by adding

00:38:01,920 --> 00:38:11,790
nodes yeah so thank you for your

00:38:07,560 --> 00:38:15,270
attention I now I will open does so well

00:38:11,790 --> 00:38:17,130
you can ask questions now and you can

00:38:15,270 --> 00:38:22,370
check out our github repositories or

00:38:17,130 --> 00:38:22,370
Twitter and yeah thank you

00:38:22,770 --> 00:38:24,830

YouTube URL: https://www.youtube.com/watch?v=RetejQBxlf4


