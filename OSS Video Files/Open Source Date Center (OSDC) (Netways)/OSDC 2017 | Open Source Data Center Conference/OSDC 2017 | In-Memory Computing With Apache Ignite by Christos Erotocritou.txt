Title: OSDC 2017 | In-Memory Computing With Apache Ignite by Christos Erotocritou
Publication date: 2017-05-31
Playlist: OSDC 2017 | Open Source Data Center Conference
Description: 
	Apache Ignite is an integrated and distributed In-Memory Data Fabric for computing and transacting on large-scale data sets in real-time, orders of magnitude faster than possible with traditional disk-based or flash technologies. It is designed to easily power both existing and new applications in a distributed, massively parallel architecture on affordable, industry-standard hardware. Apache Ignite addresses today's Fast Data and Big Data needs by providing a comprehensive in-memory data fabric, which includes a data grid with SQL and transactional capabilities, in-memory streaming, an in-memory file system, and more.
Captions: 
	00:00:09,940 --> 00:00:15,980
with our first talk

00:00:13,040 --> 00:00:17,840
attach ignite it will be a short talk

00:00:15,980 --> 00:00:21,410
about 30 minutes because it has to catch

00:00:17,840 --> 00:00:24,770
the slide so this is your stage thank

00:00:21,410 --> 00:00:27,470
you Sam Mike's working is okay so uh

00:00:24,770 --> 00:00:28,790
good to see you guys here hopefully we

00:00:27,470 --> 00:00:31,820
made it we didn't make it yesterday but

00:00:28,790 --> 00:00:33,170
we made it today in the morning so so

00:00:31,820 --> 00:00:35,149
basically I'm crystals I've been working

00:00:33,170 --> 00:00:36,739
in the prefer grid gain actually I've

00:00:35,149 --> 00:00:39,079
been working there for just over a year

00:00:36,739 --> 00:00:41,270
now I've been in the space of kind of

00:00:39,079 --> 00:00:42,829
in-memory computing for a while but five

00:00:41,270 --> 00:00:46,280
or six years already with kind of

00:00:42,829 --> 00:00:49,490
previous kind of jobs working in similar

00:00:46,280 --> 00:00:51,920
companies so I really would like to

00:00:49,490 --> 00:00:53,630
present today's kind of present ignite

00:00:51,920 --> 00:00:55,430
what it is you know what is about what

00:00:53,630 --> 00:00:57,740
we kind of use it what's different use

00:00:55,430 --> 00:00:59,690
cases and kind of give you a rundown of

00:00:57,740 --> 00:01:01,730
the different features that it kind of

00:00:59,690 --> 00:01:02,900
has and how perhaps you could give you

00:01:01,730 --> 00:01:05,810
an idea or inspiration on how you

00:01:02,900 --> 00:01:06,950
perhaps you can use it so yeah like I

00:01:05,810 --> 00:01:08,600
said we're going to talk about you know

00:01:06,950 --> 00:01:09,740
the Apache ignite project what's it

00:01:08,600 --> 00:01:12,380
about what's the relationship between

00:01:09,740 --> 00:01:15,079
grid gain what would we do with the

00:01:12,380 --> 00:01:17,270
project and then more kind of in depth

00:01:15,079 --> 00:01:19,909
about the plot from this kind of few

00:01:17,270 --> 00:01:22,370
distinct features or groups of features

00:01:19,909 --> 00:01:24,140
I would say so we'll talk about a few

00:01:22,370 --> 00:01:26,090
concepts so how it works what the

00:01:24,140 --> 00:01:28,070
fundamentals are and then twelve into

00:01:26,090 --> 00:01:29,810
the in-memory data grid side of things

00:01:28,070 --> 00:01:31,790
how we could do some processing around

00:01:29,810 --> 00:01:33,890
there and of course have a look at the

00:01:31,790 --> 00:01:36,920
compute grid the service grid and some

00:01:33,890 --> 00:01:38,659
streaming aspects that we can do

00:01:36,920 --> 00:01:40,340
hopefully I mean I've got also Hadoop

00:01:38,659 --> 00:01:41,830
and spark integration in there there's a

00:01:40,340 --> 00:01:45,229
couple of slides that talk about our

00:01:41,830 --> 00:01:46,490
integration with herd upin and spark if

00:01:45,229 --> 00:01:48,229
you're interested in that one we'll see

00:01:46,490 --> 00:01:50,030
how the time goes I'll try to run

00:01:48,229 --> 00:01:52,280
through the slides hopefully I'm not

00:01:50,030 --> 00:01:54,020
rushed too much and if we get some time

00:01:52,280 --> 00:01:57,500
to do that and then get some questions

00:01:54,020 --> 00:01:59,479
in the end that would be great okay so

00:01:57,500 --> 00:02:02,240
yeah what is ignite

00:01:59,479 --> 00:02:04,220
pretty much the best kind of explanation

00:02:02,240 --> 00:02:06,260
I'd like to give is the puzzle is a

00:02:04,220 --> 00:02:08,479
really good a really good picture I

00:02:06,260 --> 00:02:10,159
think it describes a lot so what is

00:02:08,479 --> 00:02:12,260
trying to kind of demonstrate here it's

00:02:10,159 --> 00:02:14,030
it's more of a platform so it's a group

00:02:12,260 --> 00:02:15,860
or a collection of different features

00:02:14,030 --> 00:02:17,930
that all share one common thing they all

00:02:15,860 --> 00:02:20,390
work in memory and they all

00:02:17,930 --> 00:02:22,099
fundamentally work to give you high

00:02:20,390 --> 00:02:23,719
performance and then distributed

00:02:22,099 --> 00:02:26,690
computing so that's about kind of

00:02:23,719 --> 00:02:29,090
transacting a large data set in real and

00:02:26,690 --> 00:02:31,520
near-real-time so I will talk a couple

00:02:29,090 --> 00:02:33,260
of use cases and how that works so but

00:02:31,520 --> 00:02:35,930
as an idea of what it is it's a

00:02:33,260 --> 00:02:37,940
different features all built in memory

00:02:35,930 --> 00:02:43,190
or using in memory kind of technology to

00:02:37,940 --> 00:02:47,000
be able to service very kind of high

00:02:43,190 --> 00:02:49,730
performance use cases okay so the Apache

00:02:47,000 --> 00:02:51,560
ignite project so the kind of the

00:02:49,730 --> 00:02:54,590
history here or what really happened is

00:02:51,560 --> 00:02:57,380
basically back in 2007 we have two

00:02:54,590 --> 00:02:58,640
fellows came out of Russia they moved to

00:02:57,380 --> 00:03:00,350
San Francisco

00:02:58,640 --> 00:03:02,330
they built kind of a grid and what it

00:03:00,350 --> 00:03:04,310
was a distributed computer it so that's

00:03:02,330 --> 00:03:07,250
the idea to be able to execute different

00:03:04,310 --> 00:03:10,030
jobs on a group of servers so that was

00:03:07,250 --> 00:03:12,440
the initial kind of deployment or or

00:03:10,030 --> 00:03:13,970
implementation of a patch ignite and

00:03:12,440 --> 00:03:15,860
what was called at the time grid game so

00:03:13,970 --> 00:03:18,890
these were called gradient at the time

00:03:15,860 --> 00:03:20,630
fast-track that about you know 2014

00:03:18,890 --> 00:03:21,980
after adding a whole lot of features

00:03:20,630 --> 00:03:23,660
that we're going to talk about we

00:03:21,980 --> 00:03:25,340
basically took the product we

00:03:23,660 --> 00:03:26,660
open-source it gave all the code away

00:03:25,340 --> 00:03:29,510
and we created what is called apache

00:03:26,660 --> 00:03:31,100
ignite so exciting project so there's a

00:03:29,510 --> 00:03:32,540
couple of things around that it's mature

00:03:31,100 --> 00:03:35,120
codebase right it's been around for

00:03:32,540 --> 00:03:37,750
quite a while we've had a couple of not

00:03:35,120 --> 00:03:40,040
a couple but a significant number of

00:03:37,750 --> 00:03:42,680
professional engagements and enterprise

00:03:40,040 --> 00:03:44,330
users they're using this so the code is

00:03:42,680 --> 00:03:45,470
really good I mean then the numbers

00:03:44,330 --> 00:03:47,630
because themselves I'm not sure you can

00:03:45,470 --> 00:03:49,760
read the text there but I was just

00:03:47,630 --> 00:03:51,590
taking this from-from open hub I did a

00:03:49,760 --> 00:03:53,420
comparison and what I did in 2016

00:03:51,590 --> 00:03:55,010
because I had this slide before but the

00:03:53,420 --> 00:03:56,540
whole idea was just kind of to show how

00:03:55,010 --> 00:03:58,670
active the project is now in the

00:03:56,540 --> 00:04:00,230
open-source community right what's

00:03:58,670 --> 00:04:01,850
important there is it was the

00:04:00,230 --> 00:04:04,370
second-fastest project to graduate after

00:04:01,850 --> 00:04:05,959
spark so sparks product quite a popular

00:04:04,370 --> 00:04:07,760
technology we see it around with data

00:04:05,959 --> 00:04:09,980
scientists and a lot of other use cases

00:04:07,760 --> 00:04:12,410
so I think it's quite impressive to see

00:04:09,980 --> 00:04:14,000
the same kind of momentum behind another

00:04:12,410 --> 00:04:15,830
open-source project I wouldn't say that

00:04:14,000 --> 00:04:17,239
they would live in a similar world but

00:04:15,830 --> 00:04:20,060
we cater for different types of use

00:04:17,239 --> 00:04:21,380
cases you can see there's a couple of

00:04:20,060 --> 00:04:23,480
numbers there you know there's a growing

00:04:21,380 --> 00:04:25,400
number of contributors so you know it's

00:04:23,480 --> 00:04:29,090
moving forward there's a lot of activity

00:04:25,400 --> 00:04:30,740
as a lot of interaction with the

00:04:29,090 --> 00:04:32,750
community and you know of course feel

00:04:30,740 --> 00:04:34,340
free join the user list join the dev

00:04:32,750 --> 00:04:36,350
list join the discussion there's a lot

00:04:34,340 --> 00:04:38,840
of stuff going on if you're a coder and

00:04:36,350 --> 00:04:40,039
you enjoy these kind of things of course

00:04:38,840 --> 00:04:40,670
there's all these simple tickets that

00:04:40,039 --> 00:04:42,560
you can take them

00:04:40,670 --> 00:04:45,590
bored and be part of the project as you

00:04:42,560 --> 00:04:46,970
want okay so I mean what is this thing

00:04:45,590 --> 00:04:49,840
you know what do we use it what are a

00:04:46,970 --> 00:04:52,130
couple of use cases I guess so I've seen

00:04:49,840 --> 00:04:54,680
the stuff that would generally kind of

00:04:52,130 --> 00:04:56,420
focus on commercially with with grid

00:04:54,680 --> 00:04:57,980
gain around the technologies I've seen

00:04:56,420 --> 00:04:59,690
is usually around you know FinTech and

00:04:57,980 --> 00:05:03,320
financial services those are the guys

00:04:59,690 --> 00:05:05,300
that really kind of demand this types of

00:05:03,320 --> 00:05:07,550
technology so usually you get like

00:05:05,300 --> 00:05:08,990
automated trading systems or you know of

00:05:07,550 --> 00:05:11,360
high-frequency trading systems

00:05:08,990 --> 00:05:13,970
reconciliation engines risk calculation

00:05:11,360 --> 00:05:16,750
engines all the kind of stuff that you

00:05:13,970 --> 00:05:19,280
get front and back office as well if

00:05:16,750 --> 00:05:20,900
also got you know other verticals as

00:05:19,280 --> 00:05:22,250
well that you can use igniting you see

00:05:20,900 --> 00:05:25,430
how it fits in and all of different

00:05:22,250 --> 00:05:27,560
things online gaming is another one you

00:05:25,430 --> 00:05:29,690
know doing another use case of doing the

00:05:27,560 --> 00:05:31,280
leaderboards as you know all the data

00:05:29,690 --> 00:05:33,500
comes in and then we have to calculate

00:05:31,280 --> 00:05:35,240
that in real time travel and e-commerce

00:05:33,500 --> 00:05:36,860
is another one that I've seen a lot

00:05:35,240 --> 00:05:39,320
lately so that's kind of doing real-time

00:05:36,860 --> 00:05:41,870
search engines or real-time pricing

00:05:39,320 --> 00:05:43,670
grids of you know different reservations

00:05:41,870 --> 00:05:45,080
let's say of hotels or flies or

00:05:43,670 --> 00:05:47,350
different comparisons that you want to

00:05:45,080 --> 00:05:49,700
do so that's all about you know kind of

00:05:47,350 --> 00:05:51,830
having the ability to execute a search

00:05:49,700 --> 00:05:53,840
on a cluster that's that's big and also

00:05:51,830 --> 00:05:56,120
have the ability to linearly scale that

00:05:53,840 --> 00:05:58,160
cluster as you know more mobile phones

00:05:56,120 --> 00:06:01,910
and more users get online and start to

00:05:58,160 --> 00:06:04,010
use these services right and of course

00:06:01,910 --> 00:06:05,930
now more kind of reason I would say is

00:06:04,010 --> 00:06:08,570
we're moving more into the big data

00:06:05,930 --> 00:06:10,400
world so I think that's there's a lot of

00:06:08,570 --> 00:06:12,470
stuff being added stuff that's on the on

00:06:10,400 --> 00:06:13,820
the roadmap that will align there so one

00:06:12,470 --> 00:06:15,560
of the things of course is having the

00:06:13,820 --> 00:06:17,480
ability to use our sequel engine to

00:06:15,560 --> 00:06:19,790
query this data which lines very well to

00:06:17,480 --> 00:06:22,190
kind of big data and not only that we're

00:06:19,790 --> 00:06:24,350
now adding if not already added I'm

00:06:22,190 --> 00:06:26,210
pretty sure in ignite 2.0 we've got the

00:06:24,350 --> 00:06:27,650
machine learning libraries similar to

00:06:26,210 --> 00:06:31,970
spark and of course we're extending that

00:06:27,650 --> 00:06:33,710
as version 2.1 comes out so now you see

00:06:31,970 --> 00:06:35,390
like I said this is a platform and

00:06:33,710 --> 00:06:36,980
collection of features so it's not about

00:06:35,390 --> 00:06:39,290
using all of them it's about you know

00:06:36,980 --> 00:06:43,400
picking the right mix for your use case

00:06:39,290 --> 00:06:44,750
right so what is grid gaining what's the

00:06:43,400 --> 00:06:47,180
relationship right now you know we get a

00:06:44,750 --> 00:06:50,120
given our code away would we actually do

00:06:47,180 --> 00:06:51,710
a lot of time we spend evangelizing

00:06:50,120 --> 00:06:53,600
about a patch signal you know talking

00:06:51,710 --> 00:06:54,200
and preaching about and ensuring they

00:06:53,600 --> 00:06:55,940
can't

00:06:54,200 --> 00:06:58,250
but as a commercial entity what we

00:06:55,940 --> 00:07:00,710
really do is pretty much provide an

00:06:58,250 --> 00:07:02,060
enterprise build of a patch ignite so if

00:07:00,710 --> 00:07:04,790
you're going to move into production you

00:07:02,060 --> 00:07:06,290
want to really take this into you know

00:07:04,790 --> 00:07:08,300
the next level mission-critical

00:07:06,290 --> 00:07:10,400
applications which is really what we're

00:07:08,300 --> 00:07:11,630
geared for as ignite then that's

00:07:10,400 --> 00:07:12,770
something that we know a great game can

00:07:11,630 --> 00:07:14,660
support and provide you with these

00:07:12,770 --> 00:07:16,010
builds of course there's a couple of

00:07:14,660 --> 00:07:19,520
different components in the enterprise

00:07:16,010 --> 00:07:22,280
edition that we'll talk which basically

00:07:19,520 --> 00:07:26,000
are more catered for the enterprising

00:07:22,280 --> 00:07:28,700
like Big Data cross-site replication

00:07:26,000 --> 00:07:30,950
added security added auditing and stuff

00:07:28,700 --> 00:07:32,630
like that so the whole idea that you get

00:07:30,950 --> 00:07:34,370
behind is you get a group of experts you

00:07:32,630 --> 00:07:35,960
know that created ignite that are still

00:07:34,370 --> 00:07:38,840
maintaining ignite and that can help you

00:07:35,960 --> 00:07:43,670
drive your project into production and

00:07:38,840 --> 00:07:45,860
manage it as well okay back to the

00:07:43,670 --> 00:07:47,870
puzzle I like this kind of picture so I

00:07:45,860 --> 00:07:50,000
think what I try to get it demonstrated

00:07:47,870 --> 00:07:52,070
here is just you know have a couple of

00:07:50,000 --> 00:07:54,020
pop ups from each of these let's say you

00:07:52,070 --> 00:07:56,030
know pieces of the puzzle and explain a

00:07:54,020 --> 00:07:57,350
couple of the stuff that we see so like

00:07:56,030 --> 00:07:58,940
I said you're not meant to use all of

00:07:57,350 --> 00:08:00,800
these at once but it's kind of

00:07:58,940 --> 00:08:02,570
combination perhaps so usually with a

00:08:00,800 --> 00:08:04,160
data grid you would see stuff like in a

00:08:02,570 --> 00:08:06,980
web session clustering so I've got a

00:08:04,160 --> 00:08:08,780
number of servers I've got a number of

00:08:06,980 --> 00:08:10,490
users accessing this they have their

00:08:08,780 --> 00:08:11,930
cookies what you can really do for

00:08:10,490 --> 00:08:14,000
example with ignite is usually there's a

00:08:11,930 --> 00:08:16,010
distributed caching layer to store a

00:08:14,000 --> 00:08:18,500
shared session cookies across and be

00:08:16,010 --> 00:08:20,060
able to access that from any service so

00:08:18,500 --> 00:08:22,850
you don't need to have sticky proxies as

00:08:20,060 --> 00:08:24,440
your connections come in right one use

00:08:22,850 --> 00:08:26,240
case of course you can also use it for

00:08:24,440 --> 00:08:28,430
caching requests and responses when

00:08:26,240 --> 00:08:30,800
you're building a let's say a REST API

00:08:28,430 --> 00:08:33,979
or some sort of service that you want to

00:08:30,800 --> 00:08:35,810
expose and if you've got a back-end

00:08:33,979 --> 00:08:37,370
database let's saying you need a faster

00:08:35,810 --> 00:08:39,040
way to do that

00:08:37,370 --> 00:08:43,729
then using a cache which certainly

00:08:39,040 --> 00:08:45,740
certainly help again in-memory sequel so

00:08:43,729 --> 00:08:47,630
now it's getting a lot more popular the

00:08:45,740 --> 00:08:50,060
ability to have in-memory database and

00:08:47,630 --> 00:08:52,310
that's for reasons of pure speed right

00:08:50,060 --> 00:08:53,660
so again the sequel API that's something

00:08:52,310 --> 00:08:58,970
that you can do with the data grid as

00:08:53,660 --> 00:09:00,830
well if I move on a little bit to the

00:08:58,970 --> 00:09:02,420
compute grid so that's your traditional

00:09:00,830 --> 00:09:04,550
kind of use cases of high-performance

00:09:02,420 --> 00:09:06,590
computing so different jobs come in like

00:09:04,550 --> 00:09:07,709
mentioned before risk calculations

00:09:06,590 --> 00:09:09,720
engines

00:09:07,709 --> 00:09:12,689
or pricing grids that you want to do in

00:09:09,720 --> 00:09:15,569
real time risk analysis and stuff like

00:09:12,689 --> 00:09:18,869
that service grids so that's pretty much

00:09:15,569 --> 00:09:20,850
the ability to run what you think of

00:09:18,869 --> 00:09:22,589
services now but in memory right so you

00:09:20,850 --> 00:09:24,329
can have a client and a server and you

00:09:22,589 --> 00:09:26,490
can have your interface and your in your

00:09:24,329 --> 00:09:28,829
implementation we can do exactly that or

00:09:26,490 --> 00:09:30,809
follow that exact paradigm but handle it

00:09:28,829 --> 00:09:33,439
in memory and add some resilience on top

00:09:30,809 --> 00:09:36,809
of that and scalability in real time

00:09:33,439 --> 00:09:39,720
streaming so like I said that one mostly

00:09:36,809 --> 00:09:41,759
lines into Big Data that's about having

00:09:39,720 --> 00:09:43,709
the ability to ingest streams of data

00:09:41,759 --> 00:09:44,429
into the grid and then process it in

00:09:43,709 --> 00:09:46,679
real time

00:09:44,429 --> 00:09:49,949
at the bottom bits of the puzzle I think

00:09:46,679 --> 00:09:51,720
it's slightly not I wouldn't say not

00:09:49,949 --> 00:09:54,119
interesting enough but I think slightly

00:09:51,720 --> 00:09:55,769
more low-level stuff that you use in

00:09:54,119 --> 00:09:56,970
your in your implementation so

00:09:55,769 --> 00:09:58,139
clustering that's the next kind of

00:09:56,970 --> 00:10:00,929
slides we're going to talk about which

00:09:58,139 --> 00:10:02,970
is basically how the nodes configure how

00:10:00,929 --> 00:10:05,790
they create this distributed platform or

00:10:02,970 --> 00:10:08,420
fabric and we also have an in-memory

00:10:05,790 --> 00:10:10,589
file system that you can use again or

00:10:08,420 --> 00:10:14,179
messaging between the nodes that we can

00:10:10,589 --> 00:10:16,679
communicate or even triggering certain

00:10:14,179 --> 00:10:21,209
processing or certain business logics

00:10:16,679 --> 00:10:22,709
with the trigger of different events and

00:10:21,209 --> 00:10:25,889
of course data structure is just kind of

00:10:22,709 --> 00:10:29,910
more low-level Java structures that you

00:10:25,889 --> 00:10:32,339
can use for different ideas that you

00:10:29,910 --> 00:10:33,959
have in your in your architecture and

00:10:32,339 --> 00:10:37,620
we'll get into more detail in each of

00:10:33,959 --> 00:10:39,480
these ok so let's talk about the

00:10:37,620 --> 00:10:44,120
clustering so what it isn't and how does

00:10:39,480 --> 00:10:47,339
it how does it kind of work out so think

00:10:44,120 --> 00:10:50,670
think of it as a collection of nodes so

00:10:47,339 --> 00:10:53,339
actually that pretty much cluster

00:10:50,670 --> 00:10:55,860
together and they provide you with

00:10:53,339 --> 00:10:57,779
pretty much a distributed data store so

00:10:55,860 --> 00:10:59,970
that could be key value access it could

00:10:57,779 --> 00:11:01,980
be sequel access it could be

00:10:59,970 --> 00:11:03,959
transactional access so what we can do

00:11:01,980 --> 00:11:06,629
in ignite is take data distribute it

00:11:03,959 --> 00:11:08,100
across servers partition it or replicate

00:11:06,629 --> 00:11:09,839
it however you will and we'll talk about

00:11:08,100 --> 00:11:11,819
the different modes and then provide you

00:11:09,839 --> 00:11:13,799
with different types of ap is very

00:11:11,819 --> 00:11:15,899
flexible ap is for you to access and

00:11:13,799 --> 00:11:18,240
process that data and the whole idea of

00:11:15,899 --> 00:11:20,819
ignite is keeping this data in memory

00:11:18,240 --> 00:11:21,580
where is the fastest place where you can

00:11:20,819 --> 00:11:23,320
process the

00:11:21,580 --> 00:11:26,019
and also accessing it in the most

00:11:23,320 --> 00:11:28,060
flexible way right and not only that but

00:11:26,019 --> 00:11:29,920
also processing it with the right

00:11:28,060 --> 00:11:32,110
approach which means now as we go into

00:11:29,920 --> 00:11:34,240
the computer it later on you see it's

00:11:32,110 --> 00:11:36,310
all about sending the processing to the

00:11:34,240 --> 00:11:38,290
data and not actually reading the data

00:11:36,310 --> 00:11:39,940
and doing the processing so it might

00:11:38,290 --> 00:11:41,950
sound like a database or a distributed

00:11:39,940 --> 00:11:44,380
store in the beginning and that's what

00:11:41,950 --> 00:11:47,190
you know usually people start to use it

00:11:44,380 --> 00:11:49,390
as but once you understand the

00:11:47,190 --> 00:11:51,010
processing or the computing framework

00:11:49,390 --> 00:11:52,209
that runs on top of Ignite that's when

00:11:51,010 --> 00:12:00,250
you really understand how to leverage

00:11:52,209 --> 00:12:02,200
this technology okay so got some

00:12:00,250 --> 00:12:04,320
definitions here and I've already thrown

00:12:02,200 --> 00:12:06,579
a couple of these different words around

00:12:04,320 --> 00:12:08,110
I've mentioned clusters so whenever I

00:12:06,579 --> 00:12:10,029
say kind of an ignite cluster that's

00:12:08,110 --> 00:12:12,399
pretty much a group of ignite nodes so

00:12:10,029 --> 00:12:14,470
as we start up that all kind of work

00:12:12,399 --> 00:12:17,019
together to accomplish accomplish this

00:12:14,470 --> 00:12:19,810
this features right so an igniter node

00:12:17,019 --> 00:12:22,690
is pretty much a java process it runs in

00:12:19,810 --> 00:12:25,209
the JVM right and all it does it starts

00:12:22,690 --> 00:12:27,459
up there's two kind of concepts what we

00:12:25,209 --> 00:12:29,950
call an ignite client and an ignite

00:12:27,459 --> 00:12:32,680
server and these are exactly the same

00:12:29,950 --> 00:12:34,690
process many of them can live on on the

00:12:32,680 --> 00:12:36,850
same server right or even in the same

00:12:34,690 --> 00:12:40,120
JVM so a lot of the times you can start

00:12:36,850 --> 00:12:42,670
up multiple ignite nodes within the same

00:12:40,120 --> 00:12:44,620
JVM to test your your topology right so

00:12:42,670 --> 00:12:47,770
imagine this is can be a distributed

00:12:44,620 --> 00:12:50,440
grid of or a distributed cluster of

00:12:47,770 --> 00:12:53,079
nodes having the ability to test locally

00:12:50,440 --> 00:12:57,520
and deploy this local is very important

00:12:53,079 --> 00:12:59,470
you know for kind of as you can increase

00:12:57,520 --> 00:13:01,390
momentum to go into production having to

00:12:59,470 --> 00:13:05,410
test on remote servers and starting that

00:13:01,390 --> 00:13:08,050
that can be a real pain so like I said

00:13:05,410 --> 00:13:10,029
the same idea with ignite nodes clients

00:13:08,050 --> 00:13:11,740
and servers pretty much exactly the same

00:13:10,029 --> 00:13:13,600
process the only difference with the

00:13:11,740 --> 00:13:15,880
server is that it can store data and

00:13:13,600 --> 00:13:17,680
when we say store data imagine like a

00:13:15,880 --> 00:13:20,980
distributed hash map or a containers

00:13:17,680 --> 00:13:22,510
that can store some entries right the

00:13:20,980 --> 00:13:25,660
difference with a client it can not

00:13:22,510 --> 00:13:28,120
store entries so it can access a server

00:13:25,660 --> 00:13:30,040
and access those entries and read them

00:13:28,120 --> 00:13:33,940
and do some processing right but not

00:13:30,040 --> 00:13:35,260
really store any data both ignite ignite

00:13:33,940 --> 00:13:36,880
clients and server

00:13:35,260 --> 00:13:38,350
can participate in what we call

00:13:36,880 --> 00:13:40,180
computing so when we mentioned the

00:13:38,350 --> 00:13:43,420
compute grid and processing that right

00:13:40,180 --> 00:13:47,280
so either of these kind of two processes

00:13:43,420 --> 00:13:50,140
which are very similar can execute these

00:13:47,280 --> 00:13:57,070
these jobs as you will task that you

00:13:50,140 --> 00:13:59,320
issue yeah so so you can understand

00:13:57,070 --> 00:14:00,880
there's a kind of a clock what's

00:13:59,320 --> 00:14:02,740
important to understand here and perhaps

00:14:00,880 --> 00:14:03,940
the picture is a little bit deceiving is

00:14:02,740 --> 00:14:06,460
like I said the clients and the server's

00:14:03,940 --> 00:14:09,490
they all join the cluster so what's

00:14:06,460 --> 00:14:11,080
important therefore if you want to build

00:14:09,490 --> 00:14:13,030
let's say a service here is about

00:14:11,080 --> 00:14:14,650
accessing your client is that your

00:14:13,030 --> 00:14:17,260
client is already part of the cluster

00:14:14,650 --> 00:14:18,820
and any topology changes so any server

00:14:17,260 --> 00:14:21,040
that leaves the cluster or any client

00:14:18,820 --> 00:14:23,290
that leaves the cluster everyone's aware

00:14:21,040 --> 00:14:25,360
of this so everyone all the nodes in the

00:14:23,290 --> 00:14:27,310
server all the clients and and servers

00:14:25,360 --> 00:14:29,740
in the cluster we see we call they are

00:14:27,310 --> 00:14:31,930
topology aware so any changes and this

00:14:29,740 --> 00:14:34,090
is important because once we move into

00:14:31,930 --> 00:14:35,230
data partitioning and it's very

00:14:34,090 --> 00:14:37,390
important to understand how your data

00:14:35,230 --> 00:14:39,130
moves around and how you process that

00:14:37,390 --> 00:14:41,080
and that aligns very well when we said

00:14:39,130 --> 00:14:47,710
how do you actually process your data in

00:14:41,080 --> 00:14:49,510
the best way okay so move into the kind

00:14:47,710 --> 00:14:52,570
of the most fundamental piece I would

00:14:49,510 --> 00:14:54,130
say of the feature sets or you know the

00:14:52,570 --> 00:14:56,020
range of features which is pretty much

00:14:54,130 --> 00:14:57,640
the in memory data grid right so there's

00:14:56,020 --> 00:15:01,720
the ability to store the data and access

00:14:57,640 --> 00:15:05,860
it like we said so what does that look

00:15:01,720 --> 00:15:07,780
like so imagine we have four nodes right

00:15:05,860 --> 00:15:09,310
four machines or four nodes that I

00:15:07,780 --> 00:15:11,170
started my same machine therefore server

00:15:09,310 --> 00:15:13,450
nodes and I want to store some data

00:15:11,170 --> 00:15:14,920
the first mode that you can use is what

00:15:13,450 --> 00:15:17,470
we call a replicated cache or a

00:15:14,920 --> 00:15:20,080
replicated data set right so that means

00:15:17,470 --> 00:15:21,610
every node in your cluster or every

00:15:20,080 --> 00:15:22,630
server node in your cluster will have it

00:15:21,610 --> 00:15:24,970
exactly the same piece of information

00:15:22,630 --> 00:15:27,460
right so imagine like a sequel server or

00:15:24,970 --> 00:15:29,200
or a standard database that we just

00:15:27,460 --> 00:15:31,120
replicate and you can have access to

00:15:29,200 --> 00:15:33,700
that data on any server this could be

00:15:31,120 --> 00:15:35,620
great for read heavy use cases right so

00:15:33,700 --> 00:15:39,850
imagine the scenario when I said before

00:15:35,620 --> 00:15:41,320
about the travel industry where you want

00:15:39,850 --> 00:15:43,390
to do like real-time search and very

00:15:41,320 --> 00:15:45,070
fast search usually the data is going to

00:15:43,390 --> 00:15:47,740
be minimal you know it's not big data

00:15:45,070 --> 00:15:48,990
there's a as the working data set that

00:15:47,740 --> 00:15:50,760
you work which is which are

00:15:48,990 --> 00:15:52,950
hotels and the availability for example

00:15:50,760 --> 00:15:55,290
and the flights usually that data can be

00:15:52,950 --> 00:15:56,730
replicated so that's a good idea where

00:15:55,290 --> 00:15:58,440
you can have a fully replicated cache

00:15:56,730 --> 00:15:59,910
every server that you add or every you

00:15:58,440 --> 00:16:01,440
know that you add pretty much you get a

00:15:59,910 --> 00:16:04,709
replication of that data set and then

00:16:01,440 --> 00:16:07,140
you increase your capacity by X by

00:16:04,709 --> 00:16:08,459
adding this server right the other mode

00:16:07,140 --> 00:16:10,740
that you can rel is what we call a

00:16:08,459 --> 00:16:12,360
partitioned cache and if you've kind of

00:16:10,740 --> 00:16:14,339
worked with other kind of distributed

00:16:12,360 --> 00:16:15,750
databases before like Cassandra and and

00:16:14,339 --> 00:16:17,279
and stuff like that you will you

00:16:15,750 --> 00:16:20,100
understand what the partitioning works

00:16:17,279 --> 00:16:22,050
like so the whole idea here is that we

00:16:20,100 --> 00:16:23,850
split the data set so in this scenario

00:16:22,050 --> 00:16:26,310
if I have four servers and have a

00:16:23,850 --> 00:16:29,310
distributed partitioned cache what will

00:16:26,310 --> 00:16:32,310
happen is each server we get a portion

00:16:29,310 --> 00:16:35,459
of those keys right so if I had let's

00:16:32,310 --> 00:16:36,839
say 20 keys that I want to insert or 20

00:16:35,459 --> 00:16:39,990
rows if you want to imagine in a

00:16:36,839 --> 00:16:42,959
database each server would have five of

00:16:39,990 --> 00:16:44,970
those rows or five of those keys if I

00:16:42,959 --> 00:16:46,980
wanted to access that through a client

00:16:44,970 --> 00:16:49,830
what will happen is when I will request

00:16:46,980 --> 00:16:51,990
key one I will be read the client

00:16:49,830 --> 00:16:54,750
request is redirected to server one if

00:16:51,990 --> 00:16:57,060
you want key to for example go to server

00:16:54,750 --> 00:16:59,160
two it goes to key five it will go back

00:16:57,060 --> 00:17:00,149
to server one so what we do is almost

00:16:59,160 --> 00:17:01,500
like a round robin

00:17:00,149 --> 00:17:03,750
so that's the distribution of the data

00:17:01,500 --> 00:17:05,670
that the way we partition the data this

00:17:03,750 --> 00:17:08,040
is fully pluggable and of course if

00:17:05,670 --> 00:17:09,270
you've got usually you don't want to

00:17:08,040 --> 00:17:11,189
change it and you want to stick with a

00:17:09,270 --> 00:17:12,890
default which is the partitioning

00:17:11,189 --> 00:17:15,870
strategy but if you do and you want

00:17:12,890 --> 00:17:17,880
change to change it then there's a lot

00:17:15,870 --> 00:17:21,059
of these options what we can also

00:17:17,880 --> 00:17:23,730
support is a near cache so remember as

00:17:21,059 --> 00:17:26,610
we have these on the data on the server

00:17:23,730 --> 00:17:27,959
a client can deploy in your cache so I

00:17:26,610 --> 00:17:30,510
know I mentioned before you can't store

00:17:27,959 --> 00:17:33,300
data on the client but you can create

00:17:30,510 --> 00:17:35,490
what we call a small near cache where

00:17:33,300 --> 00:17:37,860
some of that data is pushed to the

00:17:35,490 --> 00:17:40,200
client you operate locally is the client

00:17:37,860 --> 00:17:41,850
application and then any data changes

00:17:40,200 --> 00:17:44,940
are reflected directly to the server

00:17:41,850 --> 00:17:46,890
right so that's really helpful when you

00:17:44,940 --> 00:17:49,260
want to push stuff downstream right so

00:17:46,890 --> 00:17:51,000
perhaps rather than having to pull or

00:17:49,260 --> 00:17:53,520
read from the server you want to operate

00:17:51,000 --> 00:17:55,620
locally and you there's third-party

00:17:53,520 --> 00:17:57,570
systems or other users changing in the

00:17:55,620 --> 00:18:00,710
changing the data and then you get a

00:17:57,570 --> 00:18:00,710
push notification of that

00:18:00,740 --> 00:18:05,419
so the one thing we probably going to

00:18:02,809 --> 00:18:08,360
ask my dimension here is there's the

00:18:05,419 --> 00:18:09,470
concept of primaries and backups so all

00:18:08,360 --> 00:18:11,630
this time I was kind of talking about

00:18:09,470 --> 00:18:14,630
the primaries the idea there is that you

00:18:11,630 --> 00:18:16,429
can specify for every key an amount of

00:18:14,630 --> 00:18:18,230
backups right so there in this picture

00:18:16,429 --> 00:18:19,700
I've got a one on one representation

00:18:18,230 --> 00:18:21,620
that means for every primary key I have

00:18:19,700 --> 00:18:23,779
one backup key now what will end up

00:18:21,620 --> 00:18:25,580
happening is that backup key will end up

00:18:23,779 --> 00:18:27,980
on a separate server right it will not

00:18:25,580 --> 00:18:29,450
be on the same node so imagine this

00:18:27,980 --> 00:18:32,149
scenario where I lose any of these one

00:18:29,450 --> 00:18:33,710
servers I still have all my data set so

00:18:32,149 --> 00:18:35,000
what will happen in ignite in real time

00:18:33,710 --> 00:18:36,830
is once you lose one of these servers

00:18:35,000 --> 00:18:38,299
you have what we call real-time

00:18:36,830 --> 00:18:39,860
rebalancing right the data is going to

00:18:38,299 --> 00:18:42,140
move around and it's going to rebalance

00:18:39,860 --> 00:18:45,350
and then now you have three three nodes

00:18:42,140 --> 00:18:48,169
to work with you reduced kind of let's

00:18:45,350 --> 00:18:50,029
say capacity but still no loss in data

00:18:48,169 --> 00:18:51,440
set if I lost two of these servers in

00:18:50,029 --> 00:18:53,419
this scenario I would lose some of my

00:18:51,440 --> 00:18:55,490
data set right because I don't have that

00:18:53,419 --> 00:18:59,840
enough backups if I had two backups for

00:18:55,490 --> 00:19:01,760
each node then my redundancy goes up the

00:18:59,840 --> 00:19:03,020
thing is that to keep in mind that of

00:19:01,760 --> 00:19:04,880
course as you add backups you're

00:19:03,020 --> 00:19:09,289
duplicating your data sets right so

00:19:04,880 --> 00:19:10,899
that's how it works and what is

00:19:09,289 --> 00:19:13,520
important here is the real-time

00:19:10,899 --> 00:19:16,190
scalability and distribution or

00:19:13,520 --> 00:19:18,049
rebalancing of the cluster so you can

00:19:16,190 --> 00:19:19,789
remove a cluster in real time and still

00:19:18,049 --> 00:19:21,200
have availability so I mean if you're

00:19:19,789 --> 00:19:22,850
familiar with the cap theorem which is

00:19:21,200 --> 00:19:25,820
consistency availability and partition

00:19:22,850 --> 00:19:28,190
tolerance the idea is you can only pick

00:19:25,820 --> 00:19:31,100
two out of the three right so if you

00:19:28,190 --> 00:19:32,840
want availability and eventual

00:19:31,100 --> 00:19:35,630
consistency then you can do that in a

00:19:32,840 --> 00:19:37,760
guy right so you can have it to be

00:19:35,630 --> 00:19:39,649
highly available not block any access to

00:19:37,760 --> 00:19:41,090
the cluster and do real-time rebalancing

00:19:39,649 --> 00:19:44,690
if you're working with transactional

00:19:41,090 --> 00:19:47,929
data and you really care about having a

00:19:44,690 --> 00:19:49,549
firm let's say data consistency then

00:19:47,929 --> 00:19:52,039
what you can switch on is the

00:19:49,549 --> 00:19:54,529
transactionality and brew and drop the

00:19:52,039 --> 00:19:56,510
availability for data consistency right

00:19:54,529 --> 00:19:58,789
so what that will means if server leaves

00:19:56,510 --> 00:20:00,500
or server enters and there's rebalancing

00:19:58,789 --> 00:20:02,450
happening in the cluster then the

00:20:00,500 --> 00:20:04,190
cluster access is blocked for many

00:20:02,450 --> 00:20:07,250
clients to make any changes until the

00:20:04,190 --> 00:20:09,940
rebalancing complete so the main idea or

00:20:07,250 --> 00:20:12,409
the main thing to take from this is that

00:20:09,940 --> 00:20:14,000
it can be as transactional as possible

00:20:12,409 --> 00:20:14,659
and consistent with the data as possible

00:20:14,000 --> 00:20:16,820
or you can be

00:20:14,659 --> 00:20:19,669
flexible as possible always you know the

00:20:16,820 --> 00:20:22,220
more consistent and the more you tighten

00:20:19,669 --> 00:20:23,509
it up the slower you get I guess or the

00:20:22,220 --> 00:20:25,429
more you degrade in terms of performance

00:20:23,509 --> 00:20:28,669
the more you loosen it up the faster you

00:20:25,429 --> 00:20:31,129
go so that's the main key point I would

00:20:28,669 --> 00:20:32,809
say here so and the ability to split

00:20:31,129 --> 00:20:35,659
your data into different modes partition

00:20:32,809 --> 00:20:37,009
it or or replicate it and of course you

00:20:35,659 --> 00:20:38,929
can have a mixture of these caches right

00:20:37,009 --> 00:20:40,789
so you can imagine the cache as a table

00:20:38,929 --> 00:20:42,830
as you will in the database and you can

00:20:40,789 --> 00:20:49,399
have a mixture of replicated caches or

00:20:42,830 --> 00:20:52,279
partitioned caches okay so I mean we did

00:20:49,399 --> 00:20:54,019
say we're a JVM based kind of technology

00:20:52,279 --> 00:20:56,869
so you would assume we use the heap that

00:20:54,019 --> 00:20:58,249
can be problematic with GCS fear not we

00:20:56,869 --> 00:21:00,559
have what we call the off heap

00:20:58,249 --> 00:21:03,409
implementation so that's the ability for

00:21:00,559 --> 00:21:06,109
us to manage to reignite the data

00:21:03,409 --> 00:21:09,440
outside the JVM so this can be very

00:21:06,109 --> 00:21:10,879
helpful in terms of GC spikes and we're

00:21:09,440 --> 00:21:13,129
talking about big datasets right you

00:21:10,879 --> 00:21:15,259
know I was talking about last week when

00:21:13,129 --> 00:21:18,080
actually this week were we're we're mid

00:21:15,259 --> 00:21:19,489
Madrid working on some stuff we're

00:21:18,080 --> 00:21:21,649
talking about three terabytes of memory

00:21:19,489 --> 00:21:23,960
right so that's a that's a lot of data

00:21:21,649 --> 00:21:25,849
that we have to work with having to run

00:21:23,960 --> 00:21:28,129
that in the JVM that will require a lot

00:21:25,849 --> 00:21:29,779
of JVM nodes and you know you cannot go

00:21:28,129 --> 00:21:31,340
beyond a certain heap size without

00:21:29,779 --> 00:21:35,359
having problems so off you can really

00:21:31,340 --> 00:21:37,389
help in that in that sense right that's

00:21:35,359 --> 00:21:41,119
the default version Sorrell McKnight 2.0

00:21:37,389 --> 00:21:42,320
the default mode going forward is off

00:21:41,119 --> 00:21:44,090
heap I know you're going to say what

00:21:42,320 --> 00:21:46,999
about serialization costs and stuff like

00:21:44,090 --> 00:21:48,529
that that's all handled once you start

00:21:46,999 --> 00:21:49,879
getting into ignite you understand

00:21:48,529 --> 00:21:52,129
there's some concepts of what we call

00:21:49,879 --> 00:21:54,739
the binding marshal ER or the ability to

00:21:52,129 --> 00:21:56,539
keep data serialized already so we not

00:21:54,739 --> 00:21:58,999
keep any classes on the server most of

00:21:56,539 --> 00:22:00,710
the time so you can operate basically

00:21:58,999 --> 00:22:02,330
against a map and the serialized map

00:22:00,710 --> 00:22:04,099
with that data so you pay the

00:22:02,330 --> 00:22:10,129
serialization cost on the client if you

00:22:04,099 --> 00:22:11,690
need to so the one thing I probably

00:22:10,129 --> 00:22:14,450
didn't mention is the ability we say

00:22:11,690 --> 00:22:16,190
here of heap indexes but once we store

00:22:14,450 --> 00:22:18,229
data and memory remember we also have

00:22:16,190 --> 00:22:20,690
the sequel API and I'll talk about that

00:22:18,229 --> 00:22:22,489
in just a moment that's it basically you

00:22:20,690 --> 00:22:24,109
can also have indexes that live in

00:22:22,489 --> 00:22:26,419
memory and index it cannot only just

00:22:24,109 --> 00:22:29,919
live in the in JVM heap but they can

00:22:26,419 --> 00:22:29,919
also be in off heap right

00:22:30,639 --> 00:22:35,029
so datacenter replication so this is a

00:22:33,320 --> 00:22:36,859
gradient enterprise kind of features

00:22:35,029 --> 00:22:39,169
just to give you a kind of a sense what

00:22:36,859 --> 00:22:41,599
what's in the commercial edition and

00:22:39,169 --> 00:22:43,580
what not in the open-source so that's

00:22:41,599 --> 00:22:45,109
the ability remember of clusters that we

00:22:43,580 --> 00:22:46,580
describe right now you know with range

00:22:45,109 --> 00:22:47,899
of partition caches and replicated

00:22:46,580 --> 00:22:49,700
caches what if you want to do that

00:22:47,899 --> 00:22:51,379
across datacenters that's something that

00:22:49,700 --> 00:22:53,179
you can get with with grid gain what we

00:22:51,379 --> 00:22:55,580
call the DR replication plugin which

00:22:53,179 --> 00:22:58,039
will allow you to replicate your exact

00:22:55,580 --> 00:22:59,539
topology over to somewhere else or

00:22:58,039 --> 00:23:01,509
another data center what you can do is

00:22:59,539 --> 00:23:03,859
control how this replication happens how

00:23:01,509 --> 00:23:05,809
conflict-resolution happens how network

00:23:03,859 --> 00:23:06,889
split scenarios occur there's a lot of

00:23:05,809 --> 00:23:10,190
funky stuff that you can actually

00:23:06,889 --> 00:23:11,809
leverage in there so external

00:23:10,190 --> 00:23:13,820
persistence we did say we're we're in

00:23:11,809 --> 00:23:17,209
memory and we store stuff in memory so

00:23:13,820 --> 00:23:18,950
the idea there is you're probably going

00:23:17,209 --> 00:23:20,869
to say okay with if I switch it off I'm

00:23:18,950 --> 00:23:22,639
probably going to lose all my data what

00:23:20,869 --> 00:23:24,950
you can do is actually integrate with an

00:23:22,639 --> 00:23:26,239
external persistent store and that's the

00:23:24,950 --> 00:23:28,489
ability to have read and write through

00:23:26,239 --> 00:23:30,079
so usually what you what we have in a

00:23:28,489 --> 00:23:32,119
lot of our deployments is we'll have a

00:23:30,079 --> 00:23:33,649
lot of historic data a lot of you know

00:23:32,119 --> 00:23:35,929
long term data sitting in the database

00:23:33,649 --> 00:23:37,940
and all of warm data or hot data sitting

00:23:35,929 --> 00:23:40,429
in the cache or in memory right that's

00:23:37,940 --> 00:23:42,320
the that's the stuff you want to the

00:23:40,429 --> 00:23:43,879
most recently used data that you want to

00:23:42,320 --> 00:23:46,820
operate against you don't want to waste

00:23:43,879 --> 00:23:48,769
all your memory to start the serve data

00:23:46,820 --> 00:23:50,779
that you know not used so frequently

00:23:48,769 --> 00:23:56,419
memory can be expensive although it's

00:23:50,779 --> 00:23:58,070
it's cheaper nowadays right another use

00:23:56,419 --> 00:23:59,869
for this is the is the ability to

00:23:58,070 --> 00:24:01,039
basically recover so you want to restart

00:23:59,869 --> 00:24:02,839
your grid and you want to reload your

00:24:01,039 --> 00:24:04,339
data you bring it off of the database

00:24:02,839 --> 00:24:07,309
and the database can be a tube can be

00:24:04,339 --> 00:24:09,619
cassandra it could be can be an AR

00:24:07,309 --> 00:24:11,749
DBMS or any other kind of JDBC but say

00:24:09,619 --> 00:24:13,429
interface if it's something custom then

00:24:11,749 --> 00:24:15,469
you can always implement a custom cache

00:24:13,429 --> 00:24:17,659
store interface and and hook it up with

00:24:15,469 --> 00:24:20,179
your own kind of stuff and what's nifty

00:24:17,659 --> 00:24:22,399
here there's if you're using a our DBMS

00:24:20,179 --> 00:24:24,499
is a nifty little mapping wizard that

00:24:22,399 --> 00:24:27,559
basically will connect over JDBC extract

00:24:24,499 --> 00:24:30,079
all the tables configurations create the

00:24:27,559 --> 00:24:31,669
relevant poachers and then create the

00:24:30,079 --> 00:24:33,379
connection directly with ignite so you

00:24:31,669 --> 00:24:35,690
would operate through ignite rather

00:24:33,379 --> 00:24:37,519
directly to your database so the idea is

00:24:35,690 --> 00:24:38,989
having an easy way to just slice ignite

00:24:37,519 --> 00:24:42,220
in between without having to have too

00:24:38,989 --> 00:24:44,409
much code change so

00:24:42,220 --> 00:24:46,690
cash api's how do we actually access the

00:24:44,409 --> 00:24:47,889
data there's a range of stuff like I

00:24:46,690 --> 00:24:50,740
said I think this is one of the key

00:24:47,889 --> 00:24:52,450
points of the technology is the number

00:24:50,740 --> 00:24:54,730
of ways that you can access this data so

00:24:52,450 --> 00:24:57,100
it can have scan queries that's pretty

00:24:54,730 --> 00:24:59,710
much when you issue a scan query we scan

00:24:57,100 --> 00:25:02,379
every row of the data with a certain

00:24:59,710 --> 00:25:04,509
predicate that you with you you give

00:25:02,379 --> 00:25:06,129
let's say you know salary over a

00:25:04,509 --> 00:25:08,470
thousand or something like that we

00:25:06,129 --> 00:25:10,539
evaluate the predicate for each of those

00:25:08,470 --> 00:25:12,009
rows so can be quite expensive but a lot

00:25:10,539 --> 00:25:14,590
of times you need to do some some full

00:25:12,009 --> 00:25:16,330
scans right you can do text queries so

00:25:14,590 --> 00:25:17,679
if you familiar with Lucene and the

00:25:16,330 --> 00:25:19,149
scene style indexing that's kind of

00:25:17,679 --> 00:25:20,950
something that's used behind the scenes

00:25:19,149 --> 00:25:22,629
so you can do full-text search on the

00:25:20,950 --> 00:25:25,419
keys and on the values right so you can

00:25:22,629 --> 00:25:27,220
do a full search there you can drive

00:25:25,419 --> 00:25:28,690
this configuration or this schema

00:25:27,220 --> 00:25:30,970
through annotation so you can plug them

00:25:28,690 --> 00:25:33,549
into your project classes or you can use

00:25:30,970 --> 00:25:35,169
xml configuration to do it or you can

00:25:33,549 --> 00:25:37,980
even use a java config well so you're

00:25:35,169 --> 00:25:40,059
starting up to configure this stuff

00:25:37,980 --> 00:25:41,230
sequel queries so that's really

00:25:40,059 --> 00:25:43,389
important that's actually something that

00:25:41,230 --> 00:25:46,419
we're very really investing in and

00:25:43,389 --> 00:25:49,539
that's one of the I mean you can never

00:25:46,419 --> 00:25:52,889
be fully sequel compatible but we are an

00:25:49,539 --> 00:25:54,970
c99 sequel compatible so you can usually

00:25:52,889 --> 00:25:56,769
execute most of the queries that you're

00:25:54,970 --> 00:25:59,139
running with a standard kind of you know

00:25:56,769 --> 00:26:01,090
sequel database you should be able to

00:25:59,139 --> 00:26:03,309
just execute them and ignite and

00:26:01,090 --> 00:26:05,139
actually what we've well I'll leave it

00:26:03,309 --> 00:26:08,139
for the for the next line what we've got

00:26:05,139 --> 00:26:10,840
is got all the aggregation supports of

00:26:08,139 --> 00:26:12,929
group bys filters sorting cross cache

00:26:10,840 --> 00:26:16,029
joins distributed cache joins

00:26:12,929 --> 00:26:17,860
distributed non co-located joins so all

00:26:16,029 --> 00:26:20,080
that stuff is really support and very

00:26:17,860 --> 00:26:21,340
very very fast and not only supported

00:26:20,080 --> 00:26:23,370
like I said it's growing it's one of the

00:26:21,340 --> 00:26:25,419
important investments that we're making

00:26:23,370 --> 00:26:28,179
there's a couple of other interfaces

00:26:25,419 --> 00:26:31,750
like you can connect by a REST API or a

00:26:28,179 --> 00:26:33,399
memcache client or JDBC driver right so

00:26:31,750 --> 00:26:35,230
you can just access the data the

00:26:33,399 --> 00:26:38,429
in-memory stuff with a JDBC driver so

00:26:35,230 --> 00:26:40,690
rather than using the native AP is

00:26:38,429 --> 00:26:42,899
another one that the recent addition and

00:26:40,690 --> 00:26:46,480
that's because of our sequel kind of in

00:26:42,899 --> 00:26:47,980
feature set is the ODBC driver right so

00:26:46,480 --> 00:26:49,600
that's really cool because you can use

00:26:47,980 --> 00:26:51,850
something like a bi tool and then just

00:26:49,600 --> 00:26:54,519
connect straight to the grid and

00:26:51,850 --> 00:26:55,900
visualize your data that's coming in so

00:26:54,519 --> 00:27:01,890
you can use like tableau or

00:26:55,900 --> 00:27:04,990
and stuff like that to access do they do

00:27:01,890 --> 00:27:06,790
okay so sequel support a little bit more

00:27:04,990 --> 00:27:08,470
like I said I think we've covered most

00:27:06,790 --> 00:27:11,500
of the stuff here what's important is

00:27:08,470 --> 00:27:13,990
we've added DML and we're adding video

00:27:11,500 --> 00:27:16,840
dbl so that's basically right now you

00:27:13,990 --> 00:27:19,390
can do everything in terms of reading

00:27:16,840 --> 00:27:24,610
data to the JDBC and ODBC driver and

00:27:19,390 --> 00:27:26,320
directly you can do modification but you

00:27:24,610 --> 00:27:27,670
can't do creation so there's a slight

00:27:26,320 --> 00:27:30,670
bit that's missing right now and that's

00:27:27,670 --> 00:27:33,700
what we're working on to add by the end

00:27:30,670 --> 00:27:35,410
of q2 actually in ignite so the idea is

00:27:33,700 --> 00:27:38,380
there that you should be able to operate

00:27:35,410 --> 00:27:40,300
against ignite or your in-memory data

00:27:38,380 --> 00:27:42,070
set without having to write any code

00:27:40,300 --> 00:27:44,530
really right and just go through the

00:27:42,070 --> 00:27:50,320
ODBC or JDBC driver and do exactly what

00:27:44,530 --> 00:27:52,929
you were doing with your database so

00:27:50,320 --> 00:27:54,790
okay so sequel API like I said one of

00:27:52,929 --> 00:27:56,500
the strong points the other one that's

00:27:54,790 --> 00:27:57,970
really really strong I would say and

00:27:56,500 --> 00:28:00,340
technologies if you want to compare it

00:27:57,970 --> 00:28:02,200
with other competitors out there sequel

00:28:00,340 --> 00:28:05,679
one and transactions would be the other

00:28:02,200 --> 00:28:07,840
right so confirm very strong consistent

00:28:05,679 --> 00:28:10,270
background in terms of data consistency

00:28:07,840 --> 00:28:12,880
so you can have fully transactional or

00:28:10,270 --> 00:28:14,860
atomic or atomic operations this can be

00:28:12,880 --> 00:28:16,780
cross cash transactions to remember if

00:28:14,860 --> 00:28:18,880
we have like multiple caches or tables

00:28:16,780 --> 00:28:20,460
if you will you can have transactions

00:28:18,880 --> 00:28:22,929
that span a number of different caches

00:28:20,460 --> 00:28:25,900
you can have optimistic or pessimistic

00:28:22,929 --> 00:28:28,690
modes and of course a bunch of different

00:28:25,900 --> 00:28:30,610
kind of isolation levels like read

00:28:28,690 --> 00:28:34,179
repeated repeatable reads or repeats

00:28:30,610 --> 00:28:35,950
serializable and stuff like that based

00:28:34,179 --> 00:28:38,460
on some some benchmarks you know in some

00:28:35,950 --> 00:28:40,870
editions each I think it's about 50%

00:28:38,460 --> 00:28:42,460
faster in terms of transactions most of

00:28:40,870 --> 00:28:44,670
the other stuff that's out there right

00:28:42,460 --> 00:28:44,670
now

00:28:51,610 --> 00:28:56,570
okay so Java structure I think I'll skip

00:28:55,279 --> 00:28:59,480
this slide that's pretty much just

00:28:56,570 --> 00:29:02,120
saying there's a the different features

00:28:59,480 --> 00:29:03,710
that we can use in ignite in terms of

00:29:02,120 --> 00:29:06,110
the lower-level stuff so you not only

00:29:03,710 --> 00:29:08,150
have a distributed kind of map and queue

00:29:06,110 --> 00:29:09,380
you have a set you have a queue you have

00:29:08,150 --> 00:29:19,789
a sequence that you can use in the

00:29:09,380 --> 00:29:21,380
distributed executor service right so

00:29:19,789 --> 00:29:24,080
continuous query so we did say we can

00:29:21,380 --> 00:29:26,029
store the data in memory and we can

00:29:24,080 --> 00:29:29,960
access it but actually we can do is

00:29:26,029 --> 00:29:35,960
deploy a continuous query so that's the

00:29:29,960 --> 00:29:37,970
ability to really create let's say

00:29:35,960 --> 00:29:41,029
different filters and evaluate based on

00:29:37,970 --> 00:29:42,830
those filters so the idea is let's say

00:29:41,029 --> 00:29:45,289
you're listening on specific datasets

00:29:42,830 --> 00:29:47,899
that are coming in and then you evaluate

00:29:45,289 --> 00:29:49,340
based on a certain filter and then based

00:29:47,899 --> 00:29:51,620
on that you trigger some business logic

00:29:49,340 --> 00:29:53,600
so it could be like new business objects

00:29:51,620 --> 00:29:56,720
coming in that have a specific field

00:29:53,600 --> 00:29:58,970
that has a specific let's say status and

00:29:56,720 --> 00:30:00,590
then it says new so what you can do is

00:29:58,970 --> 00:30:03,260
have a query that's listening or a

00:30:00,590 --> 00:30:05,000
listener that picks this up and executes

00:30:03,260 --> 00:30:07,460
some business logic based on this input

00:30:05,000 --> 00:30:08,840
right the whole idea here is that you

00:30:07,460 --> 00:30:10,429
can create change some different

00:30:08,840 --> 00:30:12,620
continuous queries can create different

00:30:10,429 --> 00:30:16,010
kind of event chains and process it in a

00:30:12,620 --> 00:30:18,230
different kind of way of course I'm

00:30:16,010 --> 00:30:20,000
trying to run through most of these

00:30:18,230 --> 00:30:21,620
things quite fast and it's probably you

00:30:20,000 --> 00:30:23,929
know too early into a lot of the

00:30:21,620 --> 00:30:27,529
information but just kind of conscious

00:30:23,929 --> 00:30:29,929
of time that we need to go so we've got

00:30:27,529 --> 00:30:32,570
about 15 minutes to cover that so that

00:30:29,929 --> 00:30:36,049
was pretty much the the data grid I

00:30:32,570 --> 00:30:37,909
would say and that's kind of an overview

00:30:36,049 --> 00:30:39,500
of course like I said feel free to check

00:30:37,909 --> 00:30:44,740
out the commentation there's a lot more

00:30:39,500 --> 00:30:44,740
rich than just those few features right

00:30:44,770 --> 00:30:51,679
okay so the compute grid very important

00:30:49,070 --> 00:30:53,899
I think you know how this this kind of

00:30:51,679 --> 00:30:56,780
integrated together but just small gap

00:30:53,899 --> 00:30:58,309
before we go on the streaming so what's

00:30:56,780 --> 00:30:59,539
the perception of scream is streaming

00:30:58,309 --> 00:31:02,419
and ignite that's the ability like I

00:30:59,539 --> 00:31:03,859
guess like I said before store data but

00:31:02,419 --> 00:31:05,690
access it and what you can do in terms

00:31:03,859 --> 00:31:08,169
of streaming is create what we call

00:31:05,690 --> 00:31:11,749
event windows so that's the ability to

00:31:08,169 --> 00:31:14,659
define the eviction policies of your

00:31:11,749 --> 00:31:16,820
different data objects and then query

00:31:14,659 --> 00:31:19,369
against that event window so imagine if

00:31:16,820 --> 00:31:21,350
these events coming in their clicks on

00:31:19,369 --> 00:31:22,970
the website and you set the eviction

00:31:21,350 --> 00:31:25,039
policy of each of these events to one

00:31:22,970 --> 00:31:27,019
hour what you can do then later is use a

00:31:25,039 --> 00:31:29,299
sequel query and clear that and say ok

00:31:27,019 --> 00:31:31,759
give me the top 10 clicks or an hour

00:31:29,299 --> 00:31:32,989
because only in that sliding window or

00:31:31,759 --> 00:31:35,450
that event window we've only been

00:31:32,989 --> 00:31:37,149
dealing with that one hour then that's

00:31:35,450 --> 00:31:39,289
where you you will get your results

00:31:37,149 --> 00:31:40,519
based on that if you want to do it over

00:31:39,289 --> 00:31:43,159
three hours then you have to create

00:31:40,519 --> 00:31:51,700
another sliding window based on a three

00:31:43,159 --> 00:31:55,100
hour time frame right okay so client and

00:31:51,700 --> 00:31:57,350
servers versus affinity colocation so

00:31:55,100 --> 00:31:59,029
the new concept I guess here is affinity

00:31:57,350 --> 00:32:03,369
colocation how that's different with

00:31:59,029 --> 00:32:05,450
what we we currently do I guess so

00:32:03,369 --> 00:32:08,840
traditionally if you see on the left

00:32:05,450 --> 00:32:10,369
hand side we have what we call the data

00:32:08,840 --> 00:32:11,809
nodes so your database and then a

00:32:10,369 --> 00:32:13,519
processing node so traditionally you

00:32:11,809 --> 00:32:15,799
would read data and then process it and

00:32:13,519 --> 00:32:17,269
the processing node right heavy costs I

00:32:15,799 --> 00:32:18,710
mean if the datasets are big you have to

00:32:17,269 --> 00:32:21,590
move a lot of data over the wire and

00:32:18,710 --> 00:32:23,419
then do the processing but we can

00:32:21,590 --> 00:32:24,710
actually do is what we almost like a

00:32:23,419 --> 00:32:27,590
MapReduce I'm not sure if you're

00:32:24,710 --> 00:32:31,249
familiar with kind of Hadoop and that

00:32:27,590 --> 00:32:34,279
kind of paradigm or that idea but the

00:32:31,249 --> 00:32:35,809
whole idea there is to send the job to

00:32:34,279 --> 00:32:37,970
the servers to the processing nodes

00:32:35,809 --> 00:32:39,950
rather than reading the data you send

00:32:37,970 --> 00:32:42,230
the processing to the data and you do

00:32:39,950 --> 00:32:44,840
that operation now the easiest way to

00:32:42,230 --> 00:32:46,639
think about this is a MapReduce where

00:32:44,840 --> 00:32:48,529
you just broadcast you work with a

00:32:46,639 --> 00:32:50,389
subset of the data and then you read

00:32:48,529 --> 00:32:53,269
back so imagine if those processing

00:32:50,389 --> 00:32:56,029
nodes and data nodes at the same time

00:32:53,269 --> 00:32:57,980
they hold the data partition it as we

00:32:56,029 --> 00:33:01,460
talked before about partitioning the

00:32:57,980 --> 00:33:03,649
cache and now they process only

00:33:01,460 --> 00:33:05,629
individual subsets of the data so it

00:33:03,649 --> 00:33:07,999
could be for example a change operation

00:33:05,629 --> 00:33:10,519
right so imagine I want to increase

00:33:07,999 --> 00:33:12,859
someone's you know salary or benefits by

00:33:10,519 --> 00:33:14,509
X amount right usually would have to

00:33:12,859 --> 00:33:16,010
read that changes and then write it

00:33:14,509 --> 00:33:18,800
again what you can do is create

00:33:16,010 --> 00:33:22,400
closure or a task or a job or Java

00:33:18,800 --> 00:33:24,380
executable and then broadcast that to

00:33:22,400 --> 00:33:26,600
all the servers now what will happen is

00:33:24,380 --> 00:33:28,520
each server will perform that operation

00:33:26,600 --> 00:33:30,140
on the subset of the data that is

00:33:28,520 --> 00:33:33,530
holding right and then give you the

00:33:30,140 --> 00:33:34,880
result back that you would get now this

00:33:33,530 --> 00:33:37,070
gets more interesting because that's

00:33:34,880 --> 00:33:43,010
kind of a full broadcast let's say right

00:33:37,070 --> 00:33:46,880
you can also be more specific in where

00:33:43,010 --> 00:33:50,300
you direct that processing so and that's

00:33:46,880 --> 00:33:51,920
what we call a data affinity access or

00:33:50,300 --> 00:33:53,570
affinity processing right what we

00:33:51,920 --> 00:33:57,200
dimension in the in the compute grid is

00:33:53,570 --> 00:33:59,330
in the in the data grid when you

00:33:57,200 --> 00:34:01,760
partition the data you can decide on

00:33:59,330 --> 00:34:04,130
which key you use to partition the data

00:34:01,760 --> 00:34:06,410
app so usually can be the ID of an

00:34:04,130 --> 00:34:08,389
object so what object with ID 1 will end

00:34:06,410 --> 00:34:11,210
up on the server 1 object with ID 2 ends

00:34:08,389 --> 00:34:12,530
up on server 2 now if you've got two

00:34:11,210 --> 00:34:14,330
different caches or two different

00:34:12,530 --> 00:34:17,270
entities that are related right think

00:34:14,330 --> 00:34:19,010
person and address right so a person and

00:34:17,270 --> 00:34:20,570
his address always related I mean ok

00:34:19,010 --> 00:34:22,669
sometimes people might have the same

00:34:20,570 --> 00:34:24,110
address but let's let's take for this

00:34:22,669 --> 00:34:28,220
instance that it's a one-to-one mapping

00:34:24,110 --> 00:34:30,860
what you would do is insert the person

00:34:28,220 --> 00:34:33,440
with its ID and then insert the address

00:34:30,860 --> 00:34:35,480
with the affinity key of the person ID

00:34:33,440 --> 00:34:38,450
so what that would guarantee is that the

00:34:35,480 --> 00:34:40,370
address object and the employee object

00:34:38,450 --> 00:34:43,190
or customer object will end up on the

00:34:40,370 --> 00:34:45,050
physical same physical node so that

00:34:43,190 --> 00:34:47,540
means if you then want to do a joint or

00:34:45,050 --> 00:34:51,110
process and say ok give me this person's

00:34:47,540 --> 00:34:53,389
address joined up the request goes to

00:34:51,110 --> 00:34:55,280
the server that has the data and then

00:34:53,389 --> 00:34:56,990
the data is served back so that avoids

00:34:55,280 --> 00:34:59,270
kind of distributed calls around the

00:34:56,990 --> 00:35:02,230
grid where data is whole is held

00:34:59,270 --> 00:35:05,930
everywhere so the the idea here is to

00:35:02,230 --> 00:35:08,690
have a strategy when you're splitting up

00:35:05,930 --> 00:35:10,640
your data to split it up in a correct

00:35:08,690 --> 00:35:13,310
way across different entities that you

00:35:10,640 --> 00:35:16,640
position or collect them together or

00:35:13,310 --> 00:35:18,830
group them together with they as you

00:35:16,640 --> 00:35:21,650
would say they had similar functions

00:35:18,830 --> 00:35:26,270
right it sounds simpler than it actually

00:35:21,650 --> 00:35:27,830
is and it's certainly where it becomes

00:35:26,270 --> 00:35:29,660
difficult not only when you're trying to

00:35:27,830 --> 00:35:31,099
partition the data

00:35:29,660 --> 00:35:32,750
also when you try to partition it and

00:35:31,099 --> 00:35:34,460
then try to think how do you process

00:35:32,750 --> 00:35:37,309
that so I think partitioning the data

00:35:34,460 --> 00:35:40,339
and having islands clusters of partition

00:35:37,309 --> 00:35:42,170
data is okay when you start to think

00:35:40,339 --> 00:35:43,940
about how you process that also in a

00:35:42,170 --> 00:35:45,650
balanced way that's when complexity

00:35:43,940 --> 00:35:49,130
increases so the merging of the compute

00:35:45,650 --> 00:35:53,349
grid and the data grid very powerful but

00:35:49,130 --> 00:35:53,349
can get very complex very fast right

00:35:54,609 --> 00:35:59,180
that's it the community and we are out

00:35:57,619 --> 00:36:02,809
there to help you and support you guys

00:35:59,180 --> 00:36:04,640
with all of this stuff a cool thing

00:36:02,809 --> 00:36:06,890
about the computing a compute grid which

00:36:04,640 --> 00:36:10,730
I didn't mention here the whole idea is

00:36:06,890 --> 00:36:12,380
you execute closures or Java executables

00:36:10,730 --> 00:36:13,819
right or Java colobus that's just a

00:36:12,380 --> 00:36:16,940
piece of code that you can write and

00:36:13,819 --> 00:36:18,890
then you can fire off to each to all the

00:36:16,940 --> 00:36:20,539
servers to specific server or to a range

00:36:18,890 --> 00:36:22,460
of service that they want to execute now

00:36:20,539 --> 00:36:24,049
this can be any piece of code you know

00:36:22,460 --> 00:36:25,309
could be even restarting the server it

00:36:24,049 --> 00:36:27,140
could be deleting all the data it could

00:36:25,309 --> 00:36:29,839
be processing a part of the data could

00:36:27,140 --> 00:36:31,970
be anything and what's cool about that

00:36:29,839 --> 00:36:34,220
is you got zero deployment that's the

00:36:31,970 --> 00:36:36,410
ability to you know spin up your cluster

00:36:34,220 --> 00:36:37,760
write your class your task on the client

00:36:36,410 --> 00:36:39,799
side and then fired off to the grid

00:36:37,760 --> 00:36:41,990
without having to load the classes on

00:36:39,799 --> 00:36:43,579
the on the server side we have what we

00:36:41,990 --> 00:36:45,500
call a distributed class loader so as

00:36:43,579 --> 00:36:47,630
you fire the tasks it gets distributed

00:36:45,500 --> 00:36:49,220
across the cluster they all execute the

00:36:47,630 --> 00:36:51,049
task and then and then the class is

00:36:49,220 --> 00:36:53,059
dumped and you can have a new version of

00:36:51,049 --> 00:36:55,490
the same task right so really cool stuff

00:36:53,059 --> 00:37:01,099
again to kind of increase your momentum

00:36:55,490 --> 00:37:02,720
of development the last kind of bitter I

00:37:01,099 --> 00:37:04,730
think with the implementation of how it

00:37:02,720 --> 00:37:05,720
works is the in-memory service grid so I

00:37:04,730 --> 00:37:07,700
think those are the most kind of

00:37:05,720 --> 00:37:10,010
important part data grid compute grid

00:37:07,700 --> 00:37:13,099
service grid right so service grid

00:37:10,010 --> 00:37:16,430
pretty much allows you to run within

00:37:13,099 --> 00:37:19,369
each of these process or nodes or deploy

00:37:16,430 --> 00:37:21,589
what we call a service implementation or

00:37:19,369 --> 00:37:24,770
a service instance right so that's

00:37:21,589 --> 00:37:27,109
pretty much a client has the interface

00:37:24,770 --> 00:37:29,270
and the server has the implementation

00:37:27,109 --> 00:37:31,130
what this could be is actually you can

00:37:29,270 --> 00:37:32,930
deploy a service that exposes a number

00:37:31,130 --> 00:37:35,630
of different service calls and then

00:37:32,930 --> 00:37:37,819
makes a sequel call or a sequel API use

00:37:35,630 --> 00:37:39,049
the sequel API to access the data to

00:37:37,819 --> 00:37:40,690
access the cache you can do some

00:37:39,049 --> 00:37:42,980
transactions it can do some compute

00:37:40,690 --> 00:37:43,400
tasks but the whole ability the whole

00:37:42,980 --> 00:37:45,890
idea

00:37:43,400 --> 00:37:47,690
is that your shield shielding or

00:37:45,890 --> 00:37:49,510
creating this you know service layer

00:37:47,690 --> 00:37:51,500
around this rather than giving access

00:37:49,510 --> 00:37:55,220
directly to the clients to operate

00:37:51,500 --> 00:37:57,170
against the grid you can create your own

00:37:55,220 --> 00:37:59,109
service similar it's kind of exposing it

00:37:57,170 --> 00:38:02,390
your own kind of API on top of it right

00:37:59,109 --> 00:38:05,029
the the good thing about that is the

00:38:02,390 --> 00:38:06,710
resilience that you can add so you can

00:38:05,029 --> 00:38:09,049
have a node single thing what we call so

00:38:06,710 --> 00:38:11,140
a service the same service would be

00:38:09,049 --> 00:38:13,400
deployed on every single node so as you

00:38:11,140 --> 00:38:16,569
increment your the number of servers and

00:38:13,400 --> 00:38:18,619
you bring new nodes in then this kind of

00:38:16,569 --> 00:38:20,180
automatically increases the same way you

00:38:18,619 --> 00:38:22,579
can remove it or you can have a cluster

00:38:20,180 --> 00:38:24,740
singleton which is one instance of the

00:38:22,579 --> 00:38:29,990
servants running on the whole cluster if

00:38:24,740 --> 00:38:31,670
you kill any of this service or this

00:38:29,990 --> 00:38:35,260
there's a problem with this node or this

00:38:31,670 --> 00:38:39,619
service then we restart this node right

00:38:35,260 --> 00:38:45,680
and I think that's pretty much it I'll

00:38:39,619 --> 00:38:50,359
try to run very quickly with the Hadoop

00:38:45,680 --> 00:38:52,329
stuff so the idea here is that with

00:38:50,359 --> 00:38:54,859
Hadoop what we provide it's pretty much

00:38:52,329 --> 00:38:56,660
if you've used it before there's what we

00:38:54,859 --> 00:38:58,910
call a hard OOP accelerator component to

00:38:56,660 --> 00:39:01,039
ignite which is basically allowing you

00:38:58,910 --> 00:39:02,390
to execute the MapReduce where closed or

00:39:01,039 --> 00:39:04,819
the jobs that you would execute before

00:39:02,390 --> 00:39:06,589
but to ignite without any code change so

00:39:04,819 --> 00:39:08,869
what you do is you install ignite you

00:39:06,589 --> 00:39:10,279
configure it do to work with it just a

00:39:08,869 --> 00:39:12,619
configuration level and then you can

00:39:10,279 --> 00:39:14,240
execute your your old jobs no problem

00:39:12,619 --> 00:39:15,950
so that will speed up a lot of the stuff

00:39:14,240 --> 00:39:17,839
that you since we're operating in memory

00:39:15,950 --> 00:39:20,450
and of course since we're using our own

00:39:17,839 --> 00:39:22,369
computational framework but I think you

00:39:20,450 --> 00:39:24,410
know more more interesting or more kind

00:39:22,369 --> 00:39:27,319
of popular nowadays is the kind of spark

00:39:24,410 --> 00:39:29,660
integration which is basically what we

00:39:27,319 --> 00:39:32,359
call a memory shared rdd's so that's

00:39:29,660 --> 00:39:35,420
using an ignite cache as a shared RDD

00:39:32,359 --> 00:39:37,640
for spark so you cannot spark workers

00:39:35,420 --> 00:39:40,520
each with spark jobs now traditionally

00:39:37,640 --> 00:39:42,440
spark would have its own RDD but these

00:39:40,520 --> 00:39:43,880
are immutable and not visible to any of

00:39:42,440 --> 00:39:45,890
these other service if you use what we

00:39:43,880 --> 00:39:48,710
call an ignite RDD then you can make

00:39:45,890 --> 00:39:50,750
this available to all your jobs to all

00:39:48,710 --> 00:39:52,789
your spark jobs and all your workers in

00:39:50,750 --> 00:39:54,589
your cluster right and of course you can

00:39:52,789 --> 00:39:57,140
speed it up with some in-memory indexes

00:39:54,589 --> 00:40:01,160
like we said with the sequel

00:39:57,140 --> 00:40:04,190
and share the states across so yeah I

00:40:01,160 --> 00:40:06,170
mean I guess JVM products or

00:40:04,190 --> 00:40:08,900
implementation can run anywhere I'm

00:40:06,170 --> 00:40:10,280
pretty much that runs the JVM the idea

00:40:08,900 --> 00:40:12,710
here in terms of cloud deployment and

00:40:10,280 --> 00:40:14,870
integration is the one bit to keep in

00:40:12,710 --> 00:40:17,600
mind is it can pretty much run anywhere

00:40:14,870 --> 00:40:20,540
and we have you know support support for

00:40:17,600 --> 00:40:21,950
most of these stocks it's about

00:40:20,540 --> 00:40:23,120
discovery right when these nodes are

00:40:21,950 --> 00:40:24,620
going to come together and they need to

00:40:23,120 --> 00:40:27,860
discover together it's about providing

00:40:24,620 --> 00:40:30,980
the right kind of infrastructure to do

00:40:27,860 --> 00:40:32,540
that for example in Amazon we use s3 to

00:40:30,980 --> 00:40:35,270
do the lookups right because we don't

00:40:32,540 --> 00:40:37,730
have access to the network details so

00:40:35,270 --> 00:40:40,160
but anyway yeah idea is that you can

00:40:37,730 --> 00:40:42,980
access or deploy ignite to any of these

00:40:40,160 --> 00:40:45,320
kind of clouds working on your own

00:40:42,980 --> 00:40:49,100
laptop working in the same JVM working

00:40:45,320 --> 00:40:51,530
on your own physical servers the ideas

00:40:49,100 --> 00:40:56,600
is commodity hardware running wherever

00:40:51,530 --> 00:40:57,260
you you can and I think that's pretty

00:40:56,600 --> 00:40:59,590
much it

00:40:57,260 --> 00:41:01,310
wasn't too much but hopefully was

00:40:59,590 --> 00:41:07,150
educating

00:41:01,310 --> 00:41:07,150

YouTube URL: https://www.youtube.com/watch?v=A4joAZIKvCs


