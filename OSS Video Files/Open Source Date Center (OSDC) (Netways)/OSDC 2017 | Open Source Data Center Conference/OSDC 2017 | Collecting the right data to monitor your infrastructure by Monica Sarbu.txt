Title: OSDC 2017 | Collecting the right data to monitor your infrastructure by Monica Sarbu
Publication date: 2017-06-02
Playlist: OSDC 2017 | Open Source Data Center Conference
Description: 
	In the world of containers and microservices, where your infrastructure consists of thousands of containers that are changing continuously, monitoring your infrastructure become a challenge. At the beginning, you collect the logs from all your servers to help you understand when there is a failure in your system, but logs are not always available, think of when the service is down. To prevent a failure, you would need to monitor the status of your services, and the health of the server where the service is running. In a distributed environment where the microservices communicate between them via APIs, itâ€™s important to be able to visualize the traffic exchanged between your microservices for troubleshooting purposes. This talk will present how you can use the open source tools and in particular the Elastic Beats to offer a broad visibility into your network by collecting different kinds of operational data from all your services into a central point in Elasticsearch, and then build Dashboards with Kibana.
Captions: 
	00:00:09,820 --> 00:00:13,820
welcome back everybody

00:00:12,440 --> 00:00:17,140
[Music]

00:00:13,820 --> 00:00:20,119
bigger is monika sorrow from elastic and

00:00:17,140 --> 00:00:22,790
actually the first creator of the first

00:00:20,119 --> 00:00:24,439
beat I guess yes yeah she will talk

00:00:22,790 --> 00:00:35,239
about collecting the right data to

00:00:24,439 --> 00:00:41,589
monitor your infrastructure I think the

00:00:35,239 --> 00:00:44,570
microphone yeah now works hi everyone so

00:00:41,589 --> 00:00:46,820
let's say you want to monitor your

00:00:44,570 --> 00:00:52,190
infrastructure your services your

00:00:46,820 --> 00:00:56,059
applications and your containers I think

00:00:52,190 --> 00:00:58,699
it's a bit challenging to set up a

00:00:56,059 --> 00:01:00,859
monitoring system because there are so

00:00:58,699 --> 00:01:03,020
many there there is a lot of data that

00:01:00,859 --> 00:01:05,750
you can collect and there are so many

00:01:03,020 --> 00:01:08,350
tools out there it is very difficult for

00:01:05,750 --> 00:01:11,780
you to know which one you need

00:01:08,350 --> 00:01:17,049
so this talk ends to guide you four

00:01:11,780 --> 00:01:20,810
different types of monitoring data and

00:01:17,049 --> 00:01:24,590
their use case but before we start I

00:01:20,810 --> 00:01:27,500
start presenting I start talking about

00:01:24,590 --> 00:01:32,090
this let me say a few words about myself

00:01:27,500 --> 00:01:35,560
so I'm burning monitoring tools for over

00:01:32,090 --> 00:01:38,299
10 years now initially I started

00:01:35,560 --> 00:01:41,290
actually I was working for a start-up

00:01:38,299 --> 00:01:44,030
here in Berlin doing a monitoring

00:01:41,290 --> 00:01:50,299
solution for voice over IP networks I

00:01:44,030 --> 00:01:53,149
have a bit of echo and starting with

00:01:50,299 --> 00:01:56,930
like four years ago I started beat

00:01:53,149 --> 00:02:01,040
thanks I started that is a more

00:01:56,930 --> 00:02:04,939
generic monitoring solution it's open

00:02:01,040 --> 00:02:09,860
source and currently I'm working for

00:02:04,939 --> 00:02:12,410
elastic and I'm leading the big team so

00:02:09,860 --> 00:02:14,690
I think everyone knows about elastic so

00:02:12,410 --> 00:02:17,360
elastic is the company behind elastic

00:02:14,690 --> 00:02:21,079
search can you say a few words about

00:02:17,360 --> 00:02:23,359
pizza we have I'm using the pitch in

00:02:21,079 --> 00:02:26,180
plural because there is not a single

00:02:23,359 --> 00:02:27,470
project it's a family of lightweight

00:02:26,180 --> 00:02:30,770
zippers that are called

00:02:27,470 --> 00:02:33,380
all kinds of operational data and send

00:02:30,770 --> 00:02:38,360
them to elasticsearch these are open

00:02:33,380 --> 00:02:41,420
sewers they are written in golang and we

00:02:38,360 --> 00:02:44,350
have a bid for each type of data it

00:02:41,420 --> 00:02:48,680
collects for example we have packet bit

00:02:44,350 --> 00:02:51,860
for natural data we have magic bit that

00:02:48,680 --> 00:02:54,770
fetches metrics from different services

00:02:51,860 --> 00:02:59,090
we have five we that collects a lot from

00:02:54,770 --> 00:03:02,120
your servers and we have we log bit for

00:02:59,090 --> 00:03:04,580
windows event logs and harp it for

00:03:02,120 --> 00:03:06,470
uptime monitoring we are doing a bit

00:03:04,580 --> 00:03:09,320
more than that in the sense that we are

00:03:06,470 --> 00:03:12,190
building a platform that makes it easier

00:03:09,320 --> 00:03:17,090
for for our users to build their own

00:03:12,190 --> 00:03:24,650
Beach a proof is that are over 40

00:03:17,090 --> 00:03:27,470
community beach created so beacon can be

00:03:24,650 --> 00:03:30,020
used for multiple use cases for example

00:03:27,470 --> 00:03:33,320
there is application for application

00:03:30,020 --> 00:03:35,690
monitoring for network monitoring for

00:03:33,320 --> 00:03:39,800
service monitoring for system monitoring

00:03:35,690 --> 00:03:44,270
and even for security I like to go I

00:03:39,800 --> 00:03:49,130
love to go on next floor to take one by

00:03:44,270 --> 00:03:54,850
one and tell you what data we think is

00:03:49,130 --> 00:03:57,860
important to collect for each use case

00:03:54,850 --> 00:04:01,360
so let's start with application

00:03:57,860 --> 00:04:05,330
monitoring I think the first thing you

00:04:01,360 --> 00:04:11,180
will do to monitor your application will

00:04:05,330 --> 00:04:14,090
be to collect the logs so for that you

00:04:11,180 --> 00:04:16,250
can use file bit to collect the logs

00:04:14,090 --> 00:04:20,060
from your application and if your

00:04:16,250 --> 00:04:23,750
application write locks in a JSON format

00:04:20,060 --> 00:04:26,000
then it's easy in the sense that you the

00:04:23,750 --> 00:04:30,919
logs already passed you don't have to do

00:04:26,000 --> 00:04:34,130
anything but if your application doesn't

00:04:30,919 --> 00:04:37,190
write the lock in a JSON format you have

00:04:34,130 --> 00:04:41,090
to do the parsing so you probably have

00:04:37,190 --> 00:04:43,430
to write some growth letters to pass

00:04:41,090 --> 00:04:45,770
the log lines that are generated by your

00:04:43,430 --> 00:04:47,900
application and there are two ways how

00:04:45,770 --> 00:04:50,900
you can do that using the elastic stack

00:04:47,900 --> 00:04:53,540
one way would be to use the ingest not

00:04:50,900 --> 00:04:55,880
plug-in which is a plugin for elastic

00:04:53,540 --> 00:04:58,970
search and the other option will be to

00:04:55,880 --> 00:05:05,090
use log stash and in the second case you

00:04:58,970 --> 00:05:11,300
will have to send all the events through

00:05:05,090 --> 00:05:14,740
log stash to elastic search based on

00:05:11,300 --> 00:05:17,389
your logs you can also get metrics

00:05:14,740 --> 00:05:21,139
imagine you have a function that is

00:05:17,389 --> 00:05:23,720
logging a line and you can count for

00:05:21,139 --> 00:05:26,750
example how many times that function was

00:05:23,720 --> 00:05:29,780
called it's true that this is an option

00:05:26,750 --> 00:05:32,500
but it generates you quite a lot of log

00:05:29,780 --> 00:05:36,260
life another approach would be to

00:05:32,500 --> 00:05:43,430
instrument your code and this way you

00:05:36,260 --> 00:05:47,060
can get more you can get metrics so one

00:05:43,430 --> 00:05:51,140
option will be to use the client

00:05:47,060 --> 00:05:54,080
libraries offer by Pro materials I think

00:05:51,140 --> 00:05:57,650
they have official support for go Java

00:05:54,080 --> 00:05:59,960
Scala Python sinker and so on but they

00:05:57,650 --> 00:06:03,140
also have a bunch of others are from

00:05:59,960 --> 00:06:05,479
community so the way it works is that

00:06:03,140 --> 00:06:08,120
you can instrument your application

00:06:05,479 --> 00:06:13,520
using these client libraries and then

00:06:08,120 --> 00:06:16,610
they expose their custom metrics over

00:06:13,520 --> 00:06:20,090
HTTP and then you can use metric beads

00:06:16,610 --> 00:06:22,100
that are written by us to pull together

00:06:20,090 --> 00:06:26,750
the metrics and then put them pre

00:06:22,100 --> 00:06:28,580
elasticsearch if your application is

00:06:26,750 --> 00:06:31,070
written in Java then you can use your

00:06:28,580 --> 00:06:34,880
locker you can install it as an agent on

00:06:31,070 --> 00:06:38,750
your JVM and then it provides it gets

00:06:34,880 --> 00:06:41,060
your JMX metrics and expose them and

00:06:38,750 --> 00:06:45,800
then you can use metric bit to fetch

00:06:41,060 --> 00:06:48,680
them and push them to elasticsearch if

00:06:45,800 --> 00:06:54,229
your application is written in goal then

00:06:48,680 --> 00:06:54,980
you can use x-bar package and to

00:06:54,229 --> 00:07:00,020
instrument your

00:06:54,980 --> 00:07:02,960
code and the metric bit so x-bar exposes

00:07:00,020 --> 00:07:07,280
some customers over HTTP and Metra bit

00:07:02,960 --> 00:07:14,720
fetish metrics on them and and insert

00:07:07,280 --> 00:07:18,200
them in elastic search so there is a go

00:07:14,720 --> 00:07:21,050
module in metric bit that does that and

00:07:18,200 --> 00:07:26,870
for example if we have a go application

00:07:21,050 --> 00:07:29,450
that just import the X bar the package

00:07:26,870 --> 00:07:33,110
then you will get by default some

00:07:29,450 --> 00:07:35,870
metrics about heap garbage collector

00:07:33,110 --> 00:07:39,050
detail census like this so this is a

00:07:35,870 --> 00:07:41,480
dashboard this is a Cabana dashboard

00:07:39,050 --> 00:07:44,690
that comes with this goal modeling

00:07:41,480 --> 00:07:47,210
metric bit and gives you details about

00:07:44,690 --> 00:07:52,520
the hip and the grip garbage collection

00:07:47,210 --> 00:07:54,830
garbage collector so if your application

00:07:52,520 --> 00:07:58,430
is running on the windows

00:07:54,830 --> 00:08:01,730
Broly you are interested in collecting

00:07:58,430 --> 00:08:04,610
the application event logs and of that

00:08:01,730 --> 00:08:06,290
you can use a window bit it's very easy

00:08:04,610 --> 00:08:10,270
just have to say that you are

00:08:06,290 --> 00:08:10,270
interesting in the application event log

00:08:11,530 --> 00:08:20,420
if your application is running in

00:08:14,540 --> 00:08:24,860
kubernetes then sorry

00:08:20,420 --> 00:08:28,550
then you need to in order to monitor

00:08:24,860 --> 00:08:32,960
your application you need to install the

00:08:28,550 --> 00:08:36,280
bit on each node and you can reach the

00:08:32,960 --> 00:08:40,570
event that is generated by the bit with

00:08:36,280 --> 00:08:45,790
metadata from kubernetes pause by using

00:08:40,570 --> 00:08:49,010
processor that is called kubernetes and

00:08:45,790 --> 00:08:51,890
this way you can get details about the

00:08:49,010 --> 00:08:54,500
pond name is Vance Labor's well this is

00:08:51,890 --> 00:08:57,920
why this is important is that this way

00:08:54,500 --> 00:09:00,560
you can basically correlate all the

00:08:57,920 --> 00:09:02,480
events are generating by bits like for

00:09:00,560 --> 00:09:06,320
example metrics and logs you can

00:09:02,480 --> 00:09:08,310
correlate them with with the kubernetes

00:09:06,320 --> 00:09:11,070
spot so you know

00:09:08,310 --> 00:09:17,520
which port generated the logs or those

00:09:11,070 --> 00:09:21,150
or the matrix if your application is

00:09:17,520 --> 00:09:23,790
running on cloud then you can use a

00:09:21,150 --> 00:09:25,110
similar processor is called a date ed

00:09:23,790 --> 00:09:28,350
cloud with a date

00:09:25,110 --> 00:09:32,779
that gives you some details about the

00:09:28,350 --> 00:09:36,660
cloud and we have we support a few

00:09:32,779 --> 00:09:40,620
clouds here is Google cloud Amazon Web

00:09:36,660 --> 00:09:44,550
Services digitalocean and then it's a

00:09:40,620 --> 00:09:50,700
ten-cent cloud and Alibaba cloud those

00:09:44,550 --> 00:09:52,680
two were a community contribution so

00:09:50,700 --> 00:09:55,320
applications are not standalone they

00:09:52,680 --> 00:09:58,560
usually integrate with other services

00:09:55,320 --> 00:10:02,430
for example with databases like by

00:09:58,560 --> 00:10:05,520
sequel memcache or with key value stores

00:10:02,430 --> 00:10:07,740
like Reddy's or queuing system like

00:10:05,520 --> 00:10:11,610
Kafka so probably you also want to

00:10:07,740 --> 00:10:14,010
monitor the services and the first thing

00:10:11,610 --> 00:10:16,980
you probably would do is to get the

00:10:14,010 --> 00:10:22,320
locks from your service so for most

00:10:16,980 --> 00:10:26,480
common services 5-bit has modules that

00:10:22,320 --> 00:10:30,470
gives you an out-of-the-box experience

00:10:26,480 --> 00:10:35,780
because they include everything you need

00:10:30,470 --> 00:10:39,210
to collect parts and visualize your logs

00:10:35,780 --> 00:10:42,480
so in order to monitor a service like

00:10:39,210 --> 00:10:46,530
here you can see the command line that

00:10:42,480 --> 00:10:48,660
you can use you specify the service that

00:10:46,530 --> 00:10:50,030
you want to monitor or the list of

00:10:48,660 --> 00:10:54,720
services that you want to monitor

00:10:50,030 --> 00:10:58,260
separated by comma and then you will

00:10:54,720 --> 00:11:01,170
basically instantly you'll be able to

00:10:58,260 --> 00:11:03,570
have a test port like this one where you

00:11:01,170 --> 00:11:06,300
can monitor for example in this case you

00:11:03,570 --> 00:11:11,130
can monitor Apache and as you can see

00:11:06,300 --> 00:11:18,270
here you will get the logs from a bash

00:11:11,130 --> 00:11:21,560
part so for example I can see the map

00:11:18,270 --> 00:11:25,250
with the unique IP that access that

00:11:21,560 --> 00:11:28,029
patchy or the response code of tutorials

00:11:25,250 --> 00:11:31,100
or the browsers that are used or

00:11:28,029 --> 00:11:33,290
operating systems entries like this so

00:11:31,100 --> 00:11:35,740
all this information were extracted from

00:11:33,290 --> 00:11:35,740
the locks

00:11:39,030 --> 00:11:44,720
[Music]

00:11:40,720 --> 00:11:49,190
similarly file bit modules you can also

00:11:44,720 --> 00:11:52,970
have magical modules so for most common

00:11:49,190 --> 00:11:56,839
services like for example Apache read

00:11:52,970 --> 00:11:59,720
this my sequel memcache and so on you

00:11:56,839 --> 00:12:02,300
get a module in metric beads that

00:11:59,720 --> 00:12:06,710
interrogate periodically those services

00:12:02,300 --> 00:12:09,770
and such metric from them in addition we

00:12:06,710 --> 00:12:13,040
also have more generic modules in fabric

00:12:09,770 --> 00:12:18,500
for example this HTTP module that can be

00:12:13,040 --> 00:12:20,660
used when there is no support so in case

00:12:18,500 --> 00:12:24,160
you want to monitor a service for which

00:12:20,660 --> 00:12:30,020
there is no support and your service

00:12:24,160 --> 00:12:33,680
exposes metrics over HTTP so if you want

00:12:30,020 --> 00:12:36,050
to monitor docker then you can also

00:12:33,680 --> 00:12:39,110
there are two options basically one

00:12:36,050 --> 00:12:43,220
option is to in in order to get some

00:12:39,110 --> 00:12:45,220
docker container information so there

00:12:43,220 --> 00:12:48,620
are two options one option would be to

00:12:45,220 --> 00:12:52,160
interrogate directly the docker API and

00:12:48,620 --> 00:12:55,310
the second option is to use C group and

00:12:52,160 --> 00:12:59,930
C groups reach directly similar

00:12:55,310 --> 00:13:02,750
information from /proc the advantage of

00:12:59,930 --> 00:13:06,860
C group is that you can use you can use

00:13:02,750 --> 00:13:09,770
it to all kinds of container technology

00:13:06,860 --> 00:13:11,810
not only for docker but a disadvantage

00:13:09,770 --> 00:13:15,520
is that you don't get the name of the

00:13:11,810 --> 00:13:21,680
containers sometimes is a bit

00:13:15,520 --> 00:13:24,770
frustrating so here is an example here

00:13:21,680 --> 00:13:27,500
is a screenshot of a dashboard of the

00:13:24,770 --> 00:13:30,140
dashboard that comes with docker module

00:13:27,500 --> 00:13:32,660
in magic bit and you can see here it

00:13:30,140 --> 00:13:35,390
gives you some information about your

00:13:32,660 --> 00:13:39,529
token containers for example

00:13:35,390 --> 00:13:44,000
you can beside the idea of the container

00:13:39,529 --> 00:13:47,149
labels that are used and the name of the

00:13:44,000 --> 00:13:49,820
container you can also get the CPU usage

00:13:47,149 --> 00:13:53,360
the memory usage the disk i/o

00:13:49,820 --> 00:13:56,269
information you can also get information

00:13:53,360 --> 00:13:59,089
about you the status of your docker

00:13:56,269 --> 00:14:02,690
containers for example which are running

00:13:59,089 --> 00:14:05,240
which are stopped as if like this you

00:14:02,690 --> 00:14:06,920
can get details about the traffic the

00:14:05,240 --> 00:14:12,380
network traffic that is exchanged

00:14:06,920 --> 00:14:16,760
between your docker containers what else

00:14:12,380 --> 00:14:19,760
you can also for example get details

00:14:16,760 --> 00:14:22,519
about the image that was used when

00:14:19,760 --> 00:14:26,720
starting the docker container every

00:14:22,519 --> 00:14:31,640
slide is a similar approach is for

00:14:26,720 --> 00:14:34,459
kubernetes so you can monitor you can

00:14:31,640 --> 00:14:38,230
get metrics about the running containers

00:14:34,459 --> 00:14:42,380
or in the available port but using the

00:14:38,230 --> 00:14:45,140
kubernetes module in metric bit and the

00:14:42,380 --> 00:14:47,600
kubernetes module comes with a dashboard

00:14:45,140 --> 00:14:51,970
like this one here is a screen shot and

00:14:47,600 --> 00:14:55,490
you can see you can get information like

00:14:51,970 --> 00:15:01,130
the name of the port or the name spaces

00:14:55,490 --> 00:15:08,630
or the labels or the CPU usage on memory

00:15:01,130 --> 00:15:14,390
usage the next network i/o and things

00:15:08,630 --> 00:15:18,380
like this right so as you see we collect

00:15:14,390 --> 00:15:21,079
quite a lot of metrics and I think the

00:15:18,380 --> 00:15:24,170
most important thing is to be able to

00:15:21,079 --> 00:15:28,399
have a nice way to visualize those

00:15:24,170 --> 00:15:31,279
metrics because everyone likes to see in

00:15:28,399 --> 00:15:35,390
a UI forum write a matrix not a number

00:15:31,279 --> 00:15:40,029
so for this way we just released it with

00:15:35,390 --> 00:15:45,529
cabana 5 for time series visual builder

00:15:40,029 --> 00:15:48,720
that is that offers you a way to

00:15:45,529 --> 00:15:51,350
visualize time series and

00:15:48,720 --> 00:15:54,449
is meant to be used with met repeat data

00:15:51,350 --> 00:15:57,750
so met ribbit do the data that are

00:15:54,449 --> 00:16:03,000
generated by met repeat and I would like

00:15:57,750 --> 00:16:06,000
to take an example here of how you can

00:16:03,000 --> 00:16:08,180
use this time series visual builder for

00:16:06,000 --> 00:16:11,970
example here I want to create a

00:16:08,180 --> 00:16:16,769
visualization where you can see read

00:16:11,970 --> 00:16:20,639
verses right from the Mexico traffic so

00:16:16,769 --> 00:16:23,639
you consider write all the the insert

00:16:20,639 --> 00:16:26,430
Mexican insert commands update commands

00:16:23,639 --> 00:16:29,129
delete commands and the reach are

00:16:26,430 --> 00:16:36,000
basically so select the Mexico select

00:16:29,129 --> 00:16:39,839
commands so first we defined yeah first

00:16:36,000 --> 00:16:42,209
we define the average of each of this

00:16:39,839 --> 00:16:45,180
for example so let's start with rights

00:16:42,209 --> 00:16:47,699
right so first we define it 1 the

00:16:45,180 --> 00:16:50,189
average of the mask you delete comments

00:16:47,699 --> 00:16:52,620
and then the average of the my sequel

00:16:50,189 --> 00:16:54,870
insert commands and then average of the

00:16:52,620 --> 00:16:57,750
update commands and then we do a

00:16:54,870 --> 00:17:01,050
calculation and in order to to do a

00:16:57,750 --> 00:17:04,470
calculation basically you can define a

00:17:01,050 --> 00:17:09,689
variable for each and then use a

00:17:04,470 --> 00:17:12,150
painless script to sum it up and after

00:17:09,689 --> 00:17:14,970
we have this calculation which which is

00:17:12,150 --> 00:17:19,439
the sum of all these commands then we

00:17:14,970 --> 00:17:23,900
need to make a derivative out of it why

00:17:19,439 --> 00:17:27,409
a derivative because the number of

00:17:23,900 --> 00:17:30,240
delete commands for example is a

00:17:27,409 --> 00:17:33,650
ever-growing counter and we don't want

00:17:30,240 --> 00:17:38,669
to see the total value we want to see

00:17:33,650 --> 00:17:41,450
the rate per second so that's why we

00:17:38,669 --> 00:17:48,320
need to use a derivative here and

00:17:41,450 --> 00:17:51,780
similar with which we define the average

00:17:48,320 --> 00:17:54,659
for the number of select commands and

00:17:51,780 --> 00:17:57,799
then we apply a derivative amazingly

00:17:54,659 --> 00:18:02,510
basically the result is this where he

00:17:57,799 --> 00:18:02,510
both on the same chart

00:18:05,850 --> 00:18:13,000
going to be bad so this is only a small

00:18:10,300 --> 00:18:16,180
subset of what this time series

00:18:13,000 --> 00:18:19,750
interpreter can do here for example can

00:18:16,180 --> 00:18:23,140
do top n can be couch couch you see and

00:18:19,750 --> 00:18:27,000
things like this so if you want to learn

00:18:23,140 --> 00:18:31,150
more about this there is a cool demo

00:18:27,000 --> 00:18:34,300
that are done by my colleague Chris

00:18:31,150 --> 00:18:39,450
during the elastic on so there is you

00:18:34,300 --> 00:18:41,980
can you can see it online so

00:18:39,450 --> 00:18:44,560
applications your applications and your

00:18:41,980 --> 00:18:47,230
services are running on on different

00:18:44,560 --> 00:18:50,560
operating systems so I think it makes

00:18:47,230 --> 00:18:53,260
sense to also monitor as a operating

00:18:50,560 --> 00:18:56,410
system level in order to see how your

00:18:53,260 --> 00:19:01,900
hard resources are used and a good way

00:18:56,410 --> 00:19:05,710
to do that is by monitoring fetching

00:19:01,900 --> 00:19:08,500
metrics from your system and we have in

00:19:05,710 --> 00:19:11,020
metric need we have system modules that

00:19:08,500 --> 00:19:14,430
gives information about the CPU usage

00:19:11,020 --> 00:19:18,720
memory usage the load the disk i/o

00:19:14,430 --> 00:19:22,930
filesystem network starts meaning like

00:19:18,720 --> 00:19:26,130
how much network traffic is exchanged on

00:19:22,930 --> 00:19:31,540
an interface and per process information

00:19:26,130 --> 00:19:33,610
secure information and one of the

00:19:31,540 --> 00:19:36,550
dashboards that comes with this system

00:19:33,610 --> 00:19:41,440
module inmate ribbit look looks like

00:19:36,550 --> 00:19:43,120
this one so it's this is built with time

00:19:41,440 --> 00:19:46,870
series is your builder that I was

00:19:43,120 --> 00:19:50,080
telling you about earlier and here as

00:19:46,870 --> 00:19:55,330
you can see yeah you can see all kinds

00:19:50,080 --> 00:20:01,090
of rivers the CPU usage load and this

00:19:55,330 --> 00:20:03,790
guy off excited so only those poor you

00:20:01,090 --> 00:20:06,700
will be interested in collecting the

00:20:03,790 --> 00:20:13,030
windows system event logs and you can do

00:20:06,700 --> 00:20:15,130
that with we love it so now let's go to

00:20:13,030 --> 00:20:19,800
the network level so I think

00:20:15,130 --> 00:20:22,030
is very important to look at a

00:20:19,800 --> 00:20:28,030
communication that is exchanged between

00:20:22,030 --> 00:20:30,550
your applications your servers and to do

00:20:28,030 --> 00:20:32,860
that you can use packet bit that is

00:20:30,550 --> 00:20:35,320
listening on the network traffic then

00:20:32,860 --> 00:20:38,170
decode the upper layer protocols create

00:20:35,320 --> 00:20:41,020
a request is a response then correlate

00:20:38,170 --> 00:20:44,320
them into transactions and then pushes

00:20:41,020 --> 00:20:47,950
them to elasticsearch so this way you

00:20:44,320 --> 00:20:51,190
can see the communication between for

00:20:47,950 --> 00:20:55,810
example your web server and my sequel

00:20:51,190 --> 00:20:58,870
and so on and in case the traffic is

00:20:55,810 --> 00:21:01,120
encrypted or there is no protocol it

00:20:58,870 --> 00:21:05,400
still we don't have support for that

00:21:01,120 --> 00:21:08,680
protocols then you can still see some

00:21:05,400 --> 00:21:16,780
some some data about the traffic

00:21:08,680 --> 00:21:20,170
exchange another way of monitoring your

00:21:16,780 --> 00:21:22,990
system is using its uptime monitoring so

00:21:20,170 --> 00:21:28,000
this means that you can print a service

00:21:22,990 --> 00:21:31,030
from outside to check its health so a

00:21:28,000 --> 00:21:36,670
heartbeat you you give a list of hosts

00:21:31,030 --> 00:21:40,600
and then heartbeat will ping them one by

00:21:36,670 --> 00:21:42,310
one over ICP I hit be basically the

00:21:40,600 --> 00:21:51,260
protocol that is used by the ping

00:21:42,310 --> 00:21:53,400
command using using TCP or TLS Connect

00:21:51,260 --> 00:21:57,070
[Music]

00:21:53,400 --> 00:22:00,120
basically this is similar with you

00:21:57,070 --> 00:22:04,750
running telnet hostname and then port

00:22:00,120 --> 00:22:08,250
but it's also support ALS or HTTP where

00:22:04,750 --> 00:22:14,350
you basically specify a list of URLs and

00:22:08,250 --> 00:22:18,220
a heartbeat will send HTTP requests to

00:22:14,350 --> 00:22:21,130
each of these URLs and will interpret

00:22:18,220 --> 00:22:24,340
the HTTP response we get the response

00:22:21,130 --> 00:22:28,690
code and also be able to is also able to

00:22:24,340 --> 00:22:31,349
look at the response

00:22:28,690 --> 00:22:39,070
examples if you want to match something

00:22:31,349 --> 00:22:44,379
in the in the in the response hybrid is

00:22:39,070 --> 00:22:46,840
able to create some matrix to collect

00:22:44,379 --> 00:22:49,840
our matrix around cheat times for

00:22:46,840 --> 00:22:52,799
example for DNS resolve for TCP connect

00:22:49,840 --> 00:22:57,729
for TLS handshake a fix like this and

00:22:52,799 --> 00:22:59,979
here you can see there is a screenshot

00:22:57,729 --> 00:23:03,220
of the dashboard that comes with

00:22:59,979 --> 00:23:08,679
heartbeat and my favorite part is a heat

00:23:03,220 --> 00:23:12,009
map here so on the x-axis is you have

00:23:08,679 --> 00:23:14,769
time on the Y you have the response time

00:23:12,009 --> 00:23:18,669
and each of these this packet is the

00:23:14,769 --> 00:23:21,729
number of query discrete in that so I

00:23:18,669 --> 00:23:26,849
think this is a cool visualization for

00:23:21,729 --> 00:23:26,849
visual visualizing the performance data

00:23:27,090 --> 00:23:37,450
so so far we spoke so many so much about

00:23:34,169 --> 00:23:41,979
what data we can collect or using a

00:23:37,450 --> 00:23:47,099
logging and monitoring tools and I think

00:23:41,979 --> 00:23:50,019
what people don't realize is that using

00:23:47,099 --> 00:23:53,950
another interesting use case that you

00:23:50,019 --> 00:23:58,269
can have with logging and monitoring

00:23:53,950 --> 00:24:03,970
solution is security so our purpose is

00:23:58,269 --> 00:24:06,700
to be able to discover bridges using a

00:24:03,970 --> 00:24:09,849
logging and monitoring solution and this

00:24:06,700 --> 00:24:12,159
as soon as possible because I think this

00:24:09,849 --> 00:24:18,720
is what makes the difference between a

00:24:12,159 --> 00:24:21,519
catastrophic in a minor incident and

00:24:18,720 --> 00:24:25,629
next I would like to go through a few

00:24:21,519 --> 00:24:30,159
data sources that I think are important

00:24:25,629 --> 00:24:34,989
to monitor in the case of for the

00:24:30,159 --> 00:24:42,190
security use case so first is our flocks

00:24:34,989 --> 00:24:45,680
so for each SSH login you will see

00:24:42,190 --> 00:24:47,810
this inner in the authentification logs

00:24:45,680 --> 00:24:52,190
and you working for somebody there is a

00:24:47,810 --> 00:24:54,860
user that is trying to do an SSA login

00:24:52,190 --> 00:24:57,500
you will be able to see if he used the

00:24:54,860 --> 00:25:00,020
password or if you use the public key

00:24:57,500 --> 00:25:03,830
and in case he used the public key

00:25:00,020 --> 00:25:10,760
you'll be able also to see the the

00:25:03,830 --> 00:25:13,760
public key that he used also you'll be

00:25:10,760 --> 00:25:16,610
able to see the IP from where the users

00:25:13,760 --> 00:25:19,670
try to log in and based on that of IP

00:25:16,610 --> 00:25:20,400
you'll be able to get the location of

00:25:19,670 --> 00:25:22,630
the user

00:25:20,400 --> 00:25:25,400
[Music]

00:25:22,630 --> 00:25:29,500
okay so first to the latents are also

00:25:25,400 --> 00:25:32,600
important so when this is useful because

00:25:29,500 --> 00:25:35,870
you want to know when the hackers want

00:25:32,600 --> 00:25:40,250
to upgrade their privileges using the

00:25:35,870 --> 00:25:44,120
sudo command and also important are when

00:25:40,250 --> 00:25:47,750
a user is added or or when a group is

00:25:44,120 --> 00:25:50,000
added so in case there is a user that

00:25:47,750 --> 00:25:56,270
was created that it's not expected so

00:25:50,000 --> 00:25:58,630
you know to take some action so one of

00:25:56,270 --> 00:26:01,220
the dashboard that is comes with

00:25:58,630 --> 00:26:05,090
identification so identification looks

00:26:01,220 --> 00:26:09,830
is basically part of a module in inside

00:26:05,090 --> 00:26:12,620
bit and it comes with this dashboard for

00:26:09,830 --> 00:26:15,970
educational you know temp and I think my

00:26:12,620 --> 00:26:20,810
favorite is this tag cloud from here

00:26:15,970 --> 00:26:25,150
where you can see what are the most user

00:26:20,810 --> 00:26:28,910
names that are used by hackers to try to

00:26:25,150 --> 00:26:33,110
SSH into the box and obviously the most

00:26:28,910 --> 00:26:35,540
use one is root using cabaÃ±as maps

00:26:33,110 --> 00:26:39,200
you'll be able to also see from where

00:26:35,540 --> 00:26:47,090
those attackers were doing the exercise

00:26:39,200 --> 00:26:50,660
okay so on Windows you you're probably

00:26:47,090 --> 00:26:53,630
interesting in looking at a logon feller

00:26:50,660 --> 00:26:54,980
event from the security van log and you

00:26:53,630 --> 00:26:59,419
can do that we're using

00:26:54,980 --> 00:27:02,000
we love it and here is a screenshot from

00:26:59,419 --> 00:27:06,020
the dashboard that comes with that and

00:27:02,000 --> 00:27:08,510
here you can see for example yeah the

00:27:06,020 --> 00:27:11,210
logon failures no these are the

00:27:08,510 --> 00:27:17,570
successful logins or the failed logins

00:27:11,210 --> 00:27:20,809
and things like this another interesting

00:27:17,570 --> 00:27:24,440
thing that to look at is this oddity I'm

00:27:20,809 --> 00:27:27,580
sure many of you are familiar of this so

00:27:24,440 --> 00:27:32,179
it's a demon that audits every fiscal

00:27:27,580 --> 00:27:38,380
into the Linux kernel and write the

00:27:32,179 --> 00:27:43,370
record in in lakhs it's available since

00:27:38,380 --> 00:27:48,440
the Linux kernel to 614 you hang on a

00:27:43,370 --> 00:27:51,590
second and you can do things like you

00:27:48,440 --> 00:27:54,650
can see when the users when a user

00:27:51,590 --> 00:27:57,980
accesses the files you can see when the

00:27:54,650 --> 00:28:02,690
new connection is created or you can see

00:27:57,980 --> 00:28:06,830
when a new process is created so the

00:28:02,690 --> 00:28:10,130
advantage here is that it creates quite

00:28:06,830 --> 00:28:12,890
a lot of traffic and this is because the

00:28:10,130 --> 00:28:16,850
lineal Saudis framework creates multiple

00:28:12,890 --> 00:28:20,419
messages for each auditable event for

00:28:16,850 --> 00:28:23,960
example the rename Cisco creates eight

00:28:20,419 --> 00:28:28,179
messages for the same event so I think

00:28:23,960 --> 00:28:30,770
it will be more valuable if those

00:28:28,179 --> 00:28:33,860
messages will be correlated together

00:28:30,770 --> 00:28:37,220
into a single event so that's why we are

00:28:33,860 --> 00:28:42,650
now working on an alternative for Audi D

00:28:37,220 --> 00:28:45,320
as a module in metric bit net buffers

00:28:42,650 --> 00:28:49,270
all these messages and correlated

00:28:45,320 --> 00:28:52,429
together together into one event the

00:28:49,270 --> 00:28:55,400
first version is already marching master

00:28:52,429 --> 00:29:03,020
so feel free to try it if you are

00:28:55,400 --> 00:29:05,780
curious okay so another thing too that

00:29:03,020 --> 00:29:07,530
is interesting to look at is the running

00:29:05,780 --> 00:29:11,180
processes

00:29:07,530 --> 00:29:14,280
in order to look for suspicious commands

00:29:11,180 --> 00:29:16,640
right so you can instantly do that with

00:29:14,280 --> 00:29:20,520
the system module each metric bit

00:29:16,640 --> 00:29:25,110
because it shows you all the tells about

00:29:20,520 --> 00:29:30,900
each process and society's network

00:29:25,110 --> 00:29:33,090
connections so here you can see when so

00:29:30,900 --> 00:29:35,280
probably here you are interested to see

00:29:33,090 --> 00:29:39,090
when a process creates an about

00:29:35,280 --> 00:29:44,310
connection or when a process is leading

00:29:39,090 --> 00:29:46,830
a listening one is listening and this

00:29:44,310 --> 00:29:50,180
works by pulling the kernel for TCP

00:29:46,830 --> 00:29:54,140
socket and then correlating them with

00:29:50,180 --> 00:29:54,140
with the running processes

00:29:55,160 --> 00:30:02,400
Yanis tunneling DNS tunneling I don't

00:29:58,950 --> 00:30:04,980
know if you're familiar with it works by

00:30:02,400 --> 00:30:09,960
including some data in the jnanis

00:30:04,980 --> 00:30:13,830
request or DNS response and the client

00:30:09,960 --> 00:30:20,000
create a DNS request to a domain that is

00:30:13,830 --> 00:30:26,550
forwarded to a malicious entity

00:30:20,000 --> 00:30:31,640
alterative domain name and then how do

00:30:26,550 --> 00:30:38,520
you so there are quite many ways to to

00:30:31,640 --> 00:30:40,170
to track this malicious less a data so I

00:30:38,520 --> 00:30:43,020
will sorry whatever what I forgot to

00:30:40,170 --> 00:30:45,900
mention is that when the DNS request

00:30:43,020 --> 00:30:50,070
reaches the malicious authoritive domain

00:30:45,900 --> 00:30:53,790
name then it adds the user DNS response

00:30:50,070 --> 00:30:58,140
the malicious data so there are multiple

00:30:53,790 --> 00:31:02,490
ways how you can check that and one way

00:30:58,140 --> 00:31:09,030
that we are doing here is that we are

00:31:02,490 --> 00:31:15,780
looking for the unique I host names that

00:31:09,030 --> 00:31:20,040
are created for a domain and there is a

00:31:15,780 --> 00:31:20,940
I don't have it a screenshot but there

00:31:20,040 --> 00:31:23,820
is a blog

00:31:20,940 --> 00:31:28,980
about this so if you are interested in

00:31:23,820 --> 00:31:32,370
more details and you can see you can see

00:31:28,980 --> 00:31:34,440
more details about this there so you can

00:31:32,370 --> 00:31:38,220
detect that respectively I forgot to

00:31:34,440 --> 00:31:42,270
mention because packet bit it has a DNS

00:31:38,220 --> 00:31:49,260
module that is able to monitor the DNS

00:31:42,270 --> 00:31:52,590
traffic so in conclusion we I show you

00:31:49,260 --> 00:31:59,280
that there are so many data that you can

00:31:52,590 --> 00:32:02,310
collect and for example locks metrics

00:31:59,280 --> 00:32:06,330
events and so on and then you have all

00:32:02,310 --> 00:32:08,550
this data into elastic search into

00:32:06,330 --> 00:32:10,590
elastic search cluster and then by

00:32:08,550 --> 00:32:13,710
yelling that on top of it you can have

00:32:10,590 --> 00:32:16,230
other applications that are using that

00:32:13,710 --> 00:32:18,690
data for example you have some

00:32:16,230 --> 00:32:21,110
visualizations for sample qivana or a

00:32:18,690 --> 00:32:24,630
different UI that visualize your data

00:32:21,110 --> 00:32:28,290
you can do reporting based on that on

00:32:24,630 --> 00:32:31,830
the data you can have alerts you can

00:32:28,290 --> 00:32:35,160
create alerts for example you want to be

00:32:31,830 --> 00:32:36,770
alerted when a metric reaches a certain

00:32:35,160 --> 00:32:39,770
threshold

00:32:36,770 --> 00:32:43,260
another interesting application is

00:32:39,770 --> 00:32:50,610
anomaly detection so you can build a

00:32:43,260 --> 00:32:54,260
path and when most of these are part of

00:32:50,610 --> 00:32:58,920
our expect offering so it's a commercial

00:32:54,260 --> 00:33:01,260
there are commercial products yeah

00:32:58,920 --> 00:33:01,860
that's that's all thank you for your

00:33:01,260 --> 00:33:06,750
attention

00:33:01,860 --> 00:33:09,690
here are my slides at this URL so now if

00:33:06,750 --> 00:33:12,390
we have questions I think I was a bit

00:33:09,690 --> 00:33:14,880
too fast and I finish a bit earlier yeah

00:33:12,390 --> 00:33:17,640
we have plenty of time for questions so

00:33:14,880 --> 00:33:21,090
or if you want to know anything else

00:33:17,640 --> 00:33:23,510
that manager can can answer don't

00:33:21,090 --> 00:33:23,510
hesitate

00:33:32,510 --> 00:33:41,130
talk about scalability I wonder what's

00:33:36,180 --> 00:33:45,510
the biggest installation that's quite

00:33:41,130 --> 00:33:47,580
big I can't give a proper numbers

00:33:45,510 --> 00:33:49,740
because we have big customers for

00:33:47,580 --> 00:33:58,260
example eBay is using this quite a lot

00:33:49,740 --> 00:34:00,630
and yeah in quite big numbers so I can't

00:33:58,260 --> 00:34:04,230
give you the exact numbers but they are

00:34:00,630 --> 00:34:09,149
really huge a couple of terabytes every

00:34:04,230 --> 00:34:16,200
I don't know I don't want to give some

00:34:09,149 --> 00:34:18,360
wrong numbers but there is quite high so

00:34:16,200 --> 00:34:34,380
I'm not sure if I answer the exact

00:34:18,360 --> 00:34:36,179
question so you are you saying - and you

00:34:34,380 --> 00:34:39,960
would want to implement everything you

00:34:36,179 --> 00:34:42,630
showed what size of storage between and

00:34:39,960 --> 00:34:51,149
when would you run into which problems

00:34:42,630 --> 00:34:53,550
because I can't believe that yeah so the

00:34:51,149 --> 00:34:55,770
audience are quite lightweight because

00:34:53,550 --> 00:35:00,180
there isn't go say they can push a lot

00:34:55,770 --> 00:35:02,070
of data it's up to you to to say what

00:35:00,180 --> 00:35:06,300
data you are interested in so you can

00:35:02,070 --> 00:35:08,820
filter at the agent level and then it's

00:35:06,300 --> 00:35:10,500
up to elastic search to deal with all

00:35:08,820 --> 00:35:21,030
the storage so we have quite big

00:35:10,500 --> 00:35:23,670
deployment a big customers and the the

00:35:21,030 --> 00:35:26,310
scalability is on a elastic search side

00:35:23,670 --> 00:35:32,400
and I think it's very mature in this

00:35:26,310 --> 00:35:34,790
sense so yeah I'm not sure did I answer

00:35:32,400 --> 00:35:34,790
your question

00:35:37,170 --> 00:35:42,490
so maybe can you tell us something about

00:35:40,090 --> 00:35:44,740
the future of the beach project will

00:35:42,490 --> 00:35:47,410
would there be any new official beats

00:35:44,740 --> 00:35:48,750
coming soon or what other plans for the

00:35:47,410 --> 00:35:51,790
future

00:35:48,750 --> 00:35:57,310
yeah so we want to extend quite a lot in

00:35:51,790 --> 00:36:01,510
this in the security space and we also

00:35:57,310 --> 00:36:04,960
want to exclude too so we started with

00:36:01,510 --> 00:36:08,140
kubernetes and we want to also be mature

00:36:04,960 --> 00:36:10,600
and in the sense that to offer a

00:36:08,140 --> 00:36:13,510
complete monitoring solution for

00:36:10,600 --> 00:36:16,570
kubernetes we have a radio first version

00:36:13,510 --> 00:36:21,880
was just merged I don't know call month

00:36:16,570 --> 00:36:24,430
ago so the the kubernetes then we are

00:36:21,880 --> 00:36:27,090
not planning to extend our Beach so

00:36:24,430 --> 00:36:31,620
because we already have quite many Beach

00:36:27,090 --> 00:36:35,050
so the new things that we want to

00:36:31,620 --> 00:36:39,390
basically add will be in the Korean

00:36:35,050 --> 00:36:42,820
Beach all right so I think Lux - is

00:36:39,390 --> 00:36:45,130
that's part of what Pete's already do so

00:36:42,820 --> 00:36:48,610
there are some inputs that do the same

00:36:45,130 --> 00:36:53,500
things that to beat so the question is

00:36:48,610 --> 00:36:55,360
will beats one day yeah replace locks -

00:36:53,500 --> 00:36:57,810
because currently you can already send

00:36:55,360 --> 00:37:00,340
your data directly to the elasticsearch

00:36:57,810 --> 00:37:04,000
yeah that's a good question I get this

00:37:00,340 --> 00:37:07,770
question quite often to be honest so we

00:37:04,000 --> 00:37:10,420
are not here to replace locks -

00:37:07,770 --> 00:37:12,880
obviously we are you know for working

00:37:10,420 --> 00:37:18,040
for the same company listen like this so

00:37:12,880 --> 00:37:21,700
it's true that in some you'll is in some

00:37:18,040 --> 00:37:26,590
cases unique locks - for example let's

00:37:21,700 --> 00:37:29,050
say you want to to send your data to

00:37:26,590 --> 00:37:32,620
lock the velocity search and also to s3

00:37:29,050 --> 00:37:35,980
in this case you have to use locks -

00:37:32,620 --> 00:37:40,980
right so we don't want to basically

00:37:35,980 --> 00:37:44,410
extend our output and focus more on on

00:37:40,980 --> 00:37:47,020
collecting the data and not not so much

00:37:44,410 --> 00:37:50,170
in adding more and more and more output

00:37:47,020 --> 00:37:51,500
and that loss to do that also another

00:37:50,170 --> 00:37:55,970
thing that

00:37:51,500 --> 00:38:00,320
worth mentioning is is that logstash has

00:37:55,970 --> 00:38:02,300
persistent Ewing and Venus is very huge

00:38:00,320 --> 00:38:06,860
in the sense that now there are quite

00:38:02,300 --> 00:38:11,060
many deployments with Kafka and the idea

00:38:06,860 --> 00:38:14,630
is to use log stash and don't need to

00:38:11,060 --> 00:38:17,050
have Kafka in your deployment all right

00:38:14,630 --> 00:38:21,800
so do you have any more questions

00:38:17,050 --> 00:38:24,260
otherwise I would have one more so if

00:38:21,800 --> 00:38:27,770
there is there any attempt to somehow

00:38:24,260 --> 00:38:30,650
make use in peace so like when when

00:38:27,770 --> 00:38:34,070
something is blocking to actually queue

00:38:30,650 --> 00:38:36,440
the events a client-side yes so we are

00:38:34,070 --> 00:38:43,430
working on a solution hopefully for the

00:38:36,440 --> 00:38:45,920
next major release six zero and we call

00:38:43,430 --> 00:38:49,790
it pulling to disk so it's it's

00:38:45,920 --> 00:38:53,390
important because in case for example

00:38:49,790 --> 00:38:59,150
log stash is down and we cannot send the

00:38:53,390 --> 00:39:01,510
events so we can yeah do a bit of

00:38:59,150 --> 00:39:04,400
buffering on on our side

00:39:01,510 --> 00:39:08,990
so yeah let's say oh you you mentioned

00:39:04,400 --> 00:39:13,880
six version 6.0 yeah yeah usually we

00:39:08,990 --> 00:39:16,910
don't give a date or we don't promise

00:39:13,880 --> 00:39:18,710
anything because we are trying to make

00:39:16,910 --> 00:39:21,500
it as soon as possible and if we don't

00:39:18,710 --> 00:39:23,390
make it and we we have the freedom to

00:39:21,500 --> 00:39:26,090
say that we are postponing for the next

00:39:23,390 --> 00:39:28,160
release all right thank you very much

00:39:26,090 --> 00:39:34,340
morning he had a question if we have

00:39:28,160 --> 00:39:38,390
time sure sorry one question regarding

00:39:34,340 --> 00:39:40,910
the bushel builder is it the visual

00:39:38,390 --> 00:39:44,180
builder in cubanos is it possible to use

00:39:40,910 --> 00:39:48,440
different time series databases or is it

00:39:44,180 --> 00:39:51,020
just for metric beam yes you can use I

00:39:48,440 --> 00:39:54,260
mean it's working on top of

00:39:51,020 --> 00:39:58,670
elasticsearch so any data that you have

00:39:54,260 --> 00:40:01,490
as a time series it in elasticsearch is

00:39:58,670 --> 00:40:04,549
working so I said for a metric bit

00:40:01,490 --> 00:40:06,589
because in order

00:40:04,549 --> 00:40:08,900
to encourage people to use it together

00:40:06,589 --> 00:40:11,209
with the metric bid data but it can be

00:40:08,900 --> 00:40:19,039
used for any kind of time series that

00:40:11,209 --> 00:40:23,209
you have but in situ elasticsearch do we

00:40:19,039 --> 00:40:24,769
have any more questions all right Monica

00:40:23,209 --> 00:40:26,260
thank you very much for being here thank

00:40:24,769 --> 00:40:26,600
you very much for inviting

00:40:26,260 --> 00:40:31,460
[Music]

00:40:26,600 --> 00:40:31,460

YouTube URL: https://www.youtube.com/watch?v=64YOrj-jRu4


