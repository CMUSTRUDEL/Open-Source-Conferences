Title: OSDC 2017 | Automating Kubernetes Cluster Operations with Operators by Timo Derstappen
Publication date: 2017-06-01
Playlist: OSDC 2017 | Open Source Data Center Conference
Description: 
	At Giant Swarm, we manage Kubernetes clusters for customers 24/7, both on-premises and in the cloud. That means we do not just set something up and hand it over, but we actually take care that itâ€™s operational and up-to-date at all times.
In this talk Timo explains how Giant Swarm are using Operators to codify all operational tasks of managing Kubernetes cluster and distributed applications on top. The operators manage PKI infrastructures, networks, VMs and storage both on-premises and in the cloud. There have been a lots of challenges and learnings in the past year and Timo would like to share them with you.
Captions: 
	00:00:09,710 --> 00:00:14,809
we start into the day with stimulus that

00:00:13,070 --> 00:00:17,720
I'm talking about automating kubernetes

00:00:14,809 --> 00:00:20,740
thanks for coming over and yeah it used

00:00:17,720 --> 00:00:25,340
h hi everybody

00:00:20,740 --> 00:00:29,660
yeah that's a very long title and so in

00:00:25,340 --> 00:00:32,930
fact I'm I'm talking about this running

00:00:29,660 --> 00:00:41,120
kubernetes in kubernetes and who of you

00:00:32,930 --> 00:00:44,269
is using kubernetes okay who have like

00:00:41,120 --> 00:00:49,190
played with kubernetes or at least knows

00:00:44,269 --> 00:00:54,019
a little bit about it okay good so yeah

00:00:49,190 --> 00:00:56,390
what is this talk about I want to tell

00:00:54,019 --> 00:00:59,030
you a little bit about how we provision

00:00:56,390 --> 00:01:03,070
communities and what we learned in the

00:00:59,030 --> 00:01:07,820
last one and a half years doing this and

00:01:03,070 --> 00:01:10,610
and since we're using human it is to

00:01:07,820 --> 00:01:13,460
provision kubernetes there's also some

00:01:10,610 --> 00:01:16,220
advanced usage of communities itself but

00:01:13,460 --> 00:01:20,480
it's not about giving an introduction

00:01:16,220 --> 00:01:23,960
into kubernetes concepts and yeah

00:01:20,480 --> 00:01:26,380
obviously is not using kubernetes or

00:01:23,960 --> 00:01:31,610
deploying your own applications on

00:01:26,380 --> 00:01:35,240
kubernetes but since some of the things

00:01:31,610 --> 00:01:39,110
are quite advanced please raise your

00:01:35,240 --> 00:01:43,370
hand ask with in the middle of the talk

00:01:39,110 --> 00:01:46,159
let's sometimes is a little bit

00:01:43,370 --> 00:01:50,030
complicated to explain so just raise

00:01:46,159 --> 00:01:52,100
your voice and ask so I give you a

00:01:50,030 --> 00:01:55,760
little bit background what we're doing

00:01:52,100 --> 00:02:00,890
at joins form so we manage communities

00:01:55,760 --> 00:02:03,049
clusters 24/7 for our customers so we're

00:02:00,890 --> 00:02:05,090
not only just provisioning them and then

00:02:03,049 --> 00:02:07,869
hand it over to the customers

00:02:05,090 --> 00:02:12,739
we are also on call for those

00:02:07,869 --> 00:02:14,989
environments and we do this in data

00:02:12,739 --> 00:02:19,569
centers on bare metal but also in the

00:02:14,989 --> 00:02:22,599
cloud mainly on AWS at the moment and

00:02:19,569 --> 00:02:24,819
yeah we have this

00:02:22,599 --> 00:02:27,340
we have our own data center in Frankfort

00:02:24,819 --> 00:02:31,030
and where we actually hand out

00:02:27,340 --> 00:02:33,700
kubernetes clusters to customers as a

00:02:31,030 --> 00:02:35,500
service so it's our machines and

00:02:33,700 --> 00:02:40,569
everything we are just responsible for

00:02:35,500 --> 00:02:42,579
the whole infrastructure and important

00:02:40,569 --> 00:02:44,170
thing about this is that we hand out

00:02:42,579 --> 00:02:47,170
like vanilla kubernetes

00:02:44,170 --> 00:02:49,420
clusters that that are you have full

00:02:47,170 --> 00:02:52,150
root access to the clusters it's not

00:02:49,420 --> 00:02:54,340
that it's like some like platform as a

00:02:52,150 --> 00:02:56,200
service where we down the API

00:02:54,340 --> 00:02:58,510
and you're not allowed to do sort

00:02:56,200 --> 00:03:00,489
privilege containers or things like that

00:02:58,510 --> 00:03:02,590
so you get a full kubernetes

00:03:00,489 --> 00:03:06,579
and you can use the full communities as

00:03:02,590 --> 00:03:08,590
you can like with your own communities

00:03:06,579 --> 00:03:10,870
so the reference for art for us as

00:03:08,590 --> 00:03:13,419
always if it runs on mini cube on your

00:03:10,870 --> 00:03:15,970
laptop it should run in our communities

00:03:13,419 --> 00:03:20,849
as well and it shouldn't be like the

00:03:15,970 --> 00:03:25,870
giant swarm version of kubernetes so I

00:03:20,849 --> 00:03:29,049
don't really like to say that is like

00:03:25,870 --> 00:03:31,870
this is the giant swarm distribution of

00:03:29,049 --> 00:03:34,180
kubernetes because we're just using the

00:03:31,870 --> 00:03:36,400
standard things that are coming from the

00:03:34,180 --> 00:03:39,430
community that have been accepted by the

00:03:36,400 --> 00:03:42,459
community and that are in most of the

00:03:39,430 --> 00:03:45,549
kubernetes clusters and we support them

00:03:42,459 --> 00:03:50,349
and it's not that we have like our own

00:03:45,549 --> 00:03:53,079
layer on top of kubernetes and we also

00:03:50,349 --> 00:03:57,479
do this on premises in the data centers

00:03:53,079 --> 00:04:01,900
of our customers so we install our stack

00:03:57,479 --> 00:04:05,409
in a data center of a of a company and

00:04:01,900 --> 00:04:10,620
then we maintain this stack we we update

00:04:05,409 --> 00:04:14,079
it and we are on call for it and so if

00:04:10,620 --> 00:04:17,229
an enterprise wants to have a community

00:04:14,079 --> 00:04:20,979
environment we also do this inside their

00:04:17,229 --> 00:04:25,630
data centers and all the tooling

00:04:20,979 --> 00:04:30,760
everything I'm talking about is open

00:04:25,630 --> 00:04:34,700
source so the things we do to provision

00:04:30,760 --> 00:04:38,090
communities clusters it's all on github

00:04:34,700 --> 00:04:42,800
it's not downloadable as a package and

00:04:38,090 --> 00:04:45,140
then you just run it at the moment but

00:04:42,800 --> 00:04:49,070
still all those bits and pieces are out

00:04:45,140 --> 00:04:52,130
there and yeah we given this kind of

00:04:49,070 --> 00:04:54,830
talks to encourage people also to look

00:04:52,130 --> 00:05:00,940
into it and there are also a bunch of

00:04:54,830 --> 00:05:04,040
other companies working with us on those

00:05:00,940 --> 00:05:07,070
components and we're close to the

00:05:04,040 --> 00:05:11,030
community to actually see how we can

00:05:07,070 --> 00:05:14,330
improve the lifecycle management of of

00:05:11,030 --> 00:05:16,660
kubernetes clusters so that it's

00:05:14,330 --> 00:05:19,940
actually something that goes upstream

00:05:16,660 --> 00:05:22,760
that every kubernetes installation would

00:05:19,940 --> 00:05:27,410
benefit from that it can manage itself

00:05:22,760 --> 00:05:29,350
and stuff like that so a little bit

00:05:27,410 --> 00:05:31,430
about the motivation why we're

00:05:29,350 --> 00:05:36,830
provisioning multiple kubernetes

00:05:31,430 --> 00:05:39,520
clusters so in like in in a dream

00:05:36,830 --> 00:05:41,600
scenario you would have just one

00:05:39,520 --> 00:05:44,660
communities cluster in your data center

00:05:41,600 --> 00:05:48,190
and you can schedule all your workloads

00:05:44,660 --> 00:05:51,020
on this cluster that would be ideal but

00:05:48,190 --> 00:05:54,380
we figured out that lots of companies

00:05:51,020 --> 00:05:55,910
are not there yet so most of the

00:05:54,380 --> 00:06:00,110
company's wants to run multiple

00:05:55,910 --> 00:06:04,070
communities clusters a they they have

00:06:00,110 --> 00:06:07,640
teams that work with a no financial diet

00:06:04,070 --> 00:06:10,400
data privacy stuff and they want to have

00:06:07,640 --> 00:06:15,280
them separated from other teams that are

00:06:10,400 --> 00:06:19,040
doing some web applications and and and

00:06:15,280 --> 00:06:21,290
also they're they're used to having

00:06:19,040 --> 00:06:25,190
environments for pre prod prod and then

00:06:21,290 --> 00:06:27,140
separate them from death and test and

00:06:25,190 --> 00:06:33,650
they don't want to have these workloads

00:06:27,140 --> 00:06:34,970
mixed in a cluster yet and and also

00:06:33,650 --> 00:06:36,650
there's always the need if you have a

00:06:34,970 --> 00:06:39,200
built pipeline that integrates with the

00:06:36,650 --> 00:06:41,330
kubernetes api that you have like a

00:06:39,200 --> 00:06:43,790
testbed where you can spin up the

00:06:41,330 --> 00:06:45,950
cluster with a new version and test out

00:06:43,790 --> 00:06:47,580
your built pipeline does it still work

00:06:45,950 --> 00:06:49,949
with the new kubernetes version

00:06:47,580 --> 00:06:52,530
and stuff like that so we also make this

00:06:49,949 --> 00:06:54,780
easy to spin up a new cluster with a

00:06:52,530 --> 00:07:00,120
different version and test everything

00:06:54,780 --> 00:07:03,360
and yeah so we figured out that people

00:07:00,120 --> 00:07:06,389
must come to things in their own time in

00:07:03,360 --> 00:07:09,590
their own way for their own reason or

00:07:06,389 --> 00:07:13,560
they never truly come and all because

00:07:09,590 --> 00:07:19,919
with kubernetes has role-based access to

00:07:13,560 --> 00:07:22,319
narrow down the machine to machine and

00:07:19,919 --> 00:07:25,770
the user to machine communication within

00:07:22,319 --> 00:07:31,080
a cluster and this is fairly new but

00:07:25,770 --> 00:07:36,150
still the direction is clear that that

00:07:31,080 --> 00:07:41,180
there's a lot of security features and a

00:07:36,150 --> 00:07:46,560
lot of new concepts in in in kubernetes

00:07:41,180 --> 00:07:48,840
as a container platform that maybe at

00:07:46,560 --> 00:07:52,979
some point will also be accepted by

00:07:48,840 --> 00:07:56,479
security in those companies but they

00:07:52,979 --> 00:07:59,190
need to gain trust that for instance

00:07:56,479 --> 00:08:04,500
something like helical and the network

00:07:59,190 --> 00:08:06,930
policies as a dynamic firewall are doing

00:08:04,500 --> 00:08:09,479
the same thing as your old like

00:08:06,930 --> 00:08:12,449
appliances that are manually configured

00:08:09,479 --> 00:08:16,469
where you have your zones and you you

00:08:12,449 --> 00:08:19,860
separate your application and your

00:08:16,469 --> 00:08:22,229
presentation and your data so that's

00:08:19,860 --> 00:08:23,669
where the where those people in the data

00:08:22,229 --> 00:08:28,949
centers are coming from that's what

00:08:23,669 --> 00:08:31,740
they're used to and obviously the

00:08:28,949 --> 00:08:34,380
technology is quite young so it just

00:08:31,740 --> 00:08:37,399
needs some time to gain trust to

00:08:34,380 --> 00:08:41,760
understand how it works to see how the

00:08:37,399 --> 00:08:44,579
concepts make sense and so we encourage

00:08:41,760 --> 00:08:49,620
people to to use all this in their

00:08:44,579 --> 00:08:51,630
clusters and we're helping them to you

00:08:49,620 --> 00:08:56,459
know we even had a row base access in

00:08:51,630 --> 00:09:00,779
1.5 clusters running and it was just a

00:08:56,459 --> 00:09:01,560
little bit more work and to do and yeah

00:09:00,779 --> 00:09:04,399
and

00:09:01,560 --> 00:09:08,730
those people using the cluster's learn

00:09:04,399 --> 00:09:11,930
learn about those features and gain more

00:09:08,730 --> 00:09:15,149
trust so what I'm talking about now is

00:09:11,930 --> 00:09:17,610
internally called giant neatest because

00:09:15,149 --> 00:09:22,759
it's the kubernetes that spawns

00:09:17,610 --> 00:09:22,759
vanitas's for our customers and for our

00:09:23,149 --> 00:09:29,670
clients and the motivation is quite

00:09:27,300 --> 00:09:33,920
obvious we are providing kubernetes

00:09:29,670 --> 00:09:37,350
because we think it makes sense to

00:09:33,920 --> 00:09:38,939
deploy application that way and the

00:09:37,350 --> 00:09:42,509
kubernetes has the right building blocks

00:09:38,939 --> 00:09:45,680
to do so and why should we then use

00:09:42,509 --> 00:09:48,120
ansible to provision communities again

00:09:45,680 --> 00:09:52,649
because we don't want to do that anymore

00:09:48,120 --> 00:09:54,360
and so we came up with the idea to use

00:09:52,649 --> 00:09:56,370
actually kubernetes to provision

00:09:54,360 --> 00:09:58,470
communities we still have this

00:09:56,370 --> 00:10:01,680
chicken-egg problem that we need to

00:09:58,470 --> 00:10:05,490
provision the first cluster and we have

00:10:01,680 --> 00:10:08,129
some interval scripts for spinning up

00:10:05,490 --> 00:10:11,519
the first node in an environment at the

00:10:08,129 --> 00:10:15,000
moment and also on AWS we're using some

00:10:11,519 --> 00:10:17,339
terraform so there's still some of the

00:10:15,000 --> 00:10:20,629
configuration management tools that you

00:10:17,339 --> 00:10:23,069
probably know but once this is up

00:10:20,629 --> 00:10:25,410
everything is automated inside

00:10:23,069 --> 00:10:31,100
kubernetes itself and there's no

00:10:25,410 --> 00:10:35,220
instable anymore and so short definition

00:10:31,100 --> 00:10:37,680
because it's confusing this is a

00:10:35,220 --> 00:10:41,759
shortcut for giant it is and that always

00:10:37,680 --> 00:10:43,980
means the host kubernetes cluster the

00:10:41,759 --> 00:10:49,019
underlying the mother of the kubernetes

00:10:43,980 --> 00:10:50,970
is and and the this is the kubernetes if

00:10:49,019 --> 00:10:52,709
i've been talking about kubernetes it's

00:10:50,970 --> 00:10:54,689
always the tenant cluster a guest

00:10:52,709 --> 00:10:58,189
cluster that we're providing for our

00:10:54,689 --> 00:11:01,829
customers and don't confuse it with

00:10:58,189 --> 00:11:05,730
cuban eddies that's what the people from

00:11:01,829 --> 00:11:08,759
the yes call it and it's a nice

00:11:05,730 --> 00:11:16,759
restaurant you can go to right you can

00:11:08,759 --> 00:11:16,759
take out some food so architecture

00:11:17,340 --> 00:11:23,800
yeah as I said a little bit confusing

00:11:19,690 --> 00:11:29,680
sometimes so there are just three roles

00:11:23,800 --> 00:11:34,600
in speaking to our control plane which

00:11:29,680 --> 00:11:36,970
is we who provide the host cluster and

00:11:34,600 --> 00:11:41,170
the tooling so we're the journey this

00:11:36,970 --> 00:11:44,370
Edmonds then somebody is the cluster

00:11:41,170 --> 00:11:47,860
admin so somebody creates a cluster on

00:11:44,370 --> 00:11:50,350
our system and for instance you can

00:11:47,860 --> 00:11:55,120
create key pairs and then hand them out

00:11:50,350 --> 00:11:58,030
to developers who wants to use to the

00:11:55,120 --> 00:12:00,820
cluster and then yeah the developer is

00:11:58,030 --> 00:12:03,790
the cluster user that actually put stuff

00:12:00,820 --> 00:12:07,120
on the on the cluster or maybe that's

00:12:03,790 --> 00:12:10,860
the build pipeline and then next to it

00:12:07,120 --> 00:12:14,620
you have the actual nodes running as a

00:12:10,860 --> 00:12:16,990
application zone so you have a

00:12:14,620 --> 00:12:20,170
separation of this is controlling the

00:12:16,990 --> 00:12:23,430
infrastructure some monitoring's in here

00:12:20,170 --> 00:12:27,190
as well we have some services that are

00:12:23,430 --> 00:12:29,400
reusing vault and we're using a

00:12:27,190 --> 00:12:34,330
distributed file system called quo bite

00:12:29,400 --> 00:12:36,820
and we're using yeah some other like

00:12:34,330 --> 00:12:41,110
networking and those kind of services

00:12:36,820 --> 00:12:44,500
that we're running here and we have our

00:12:41,110 --> 00:12:46,240
own API and our own services that allow

00:12:44,500 --> 00:12:51,220
the cluster admin to actually create the

00:12:46,240 --> 00:12:53,860
cluster and and here we have the like

00:12:51,220 --> 00:12:57,280
dump nodes running where people can

00:12:53,860 --> 00:12:59,770
schedule their workloads on and so if

00:12:57,280 --> 00:13:03,370
you blow this up this looks a little bit

00:12:59,770 --> 00:13:06,040
more like this so here are the clusters

00:13:03,370 --> 00:13:08,650
the different ones and traffic comes

00:13:06,040 --> 00:13:10,750
from here from the outside and you can

00:13:08,650 --> 00:13:13,540
separate this from like traffic who has

00:13:10,750 --> 00:13:16,360
access to the control plane and stuff

00:13:13,540 --> 00:13:19,930
and you have some dashboards here and

00:13:16,360 --> 00:13:21,640
some other tools and yeah but it doesn't

00:13:19,930 --> 00:13:24,130
really matter and then if you if you

00:13:21,640 --> 00:13:27,939
blow this up here as well it looks like

00:13:24,130 --> 00:13:30,159
this so that's the tenant cluster

00:13:27,939 --> 00:13:34,449
cluster and like a typical architecture

00:13:30,159 --> 00:13:37,449
of a communities cluster where you have

00:13:34,449 --> 00:13:40,089
like a controller scheduler and you can

00:13:37,449 --> 00:13:42,579
run some dashboards on it you have the

00:13:40,089 --> 00:13:46,479
API server and that these are the the

00:13:42,579 --> 00:13:48,279
workers the worker nodes where you

00:13:46,479 --> 00:13:51,759
schedule you workload on and then you

00:13:48,279 --> 00:13:54,129
can use you know cubes ETL you need

00:13:51,759 --> 00:13:57,819
registry those kind of things and then

00:13:54,129 --> 00:14:00,970
inside the the workers usually also have

00:13:57,819 --> 00:14:02,859
like an engine X controller and you have

00:14:00,970 --> 00:14:06,849
an ingress controller we're using the

00:14:02,859 --> 00:14:10,989
nginx controller and you would have cube

00:14:06,849 --> 00:14:14,019
DNS and a couple of other things running

00:14:10,989 --> 00:14:18,609
in there like the network policy

00:14:14,019 --> 00:14:20,709
controller and so on so within our set

00:14:18,609 --> 00:14:22,419
up and the master is completely managed

00:14:20,709 --> 00:14:25,929
and nobody has access to the master

00:14:22,419 --> 00:14:27,729
itself but on the on the workers as I

00:14:25,929 --> 00:14:29,859
said you can start privileged containers

00:14:27,729 --> 00:14:35,019
and you have like full access and to it

00:14:29,859 --> 00:14:38,739
and yeah and so yeah why are we doing

00:14:35,019 --> 00:14:41,289
this again yeah we just want to fully

00:14:38,739 --> 00:14:45,759
automate the complexity I just shown

00:14:41,289 --> 00:14:49,449
I've just sewn so there lots of bits and

00:14:45,759 --> 00:14:52,989
pieces in the infrastructure lots of

00:14:49,449 --> 00:14:55,059
tools that also have independent an

00:14:52,989 --> 00:14:57,249
independent life cycle so there it's

00:14:55,059 --> 00:14:58,179
like a new release of calico and a new

00:14:57,249 --> 00:15:02,169
release of flaneur

00:14:58,179 --> 00:15:04,749
and the vol changes and covert and and

00:15:02,169 --> 00:15:08,439
whatnot and kubernetes and so there are

00:15:04,749 --> 00:15:10,029
lots of bits and pieces in there and we

00:15:08,439 --> 00:15:13,539
want to just want to have this fully

00:15:10,029 --> 00:15:15,699
automate it and also have something that

00:15:13,539 --> 00:15:18,959
takes care of the of the life cycle of

00:15:15,699 --> 00:15:22,389
those components while they are running

00:15:18,959 --> 00:15:24,279
so not only just provision them and then

00:15:22,389 --> 00:15:26,769
you know hand it over

00:15:24,279 --> 00:15:29,229
but also mention the life cycle so

00:15:26,769 --> 00:15:31,689
having frequent updates in your cluster

00:15:29,229 --> 00:15:34,659
and not like every six months or we do

00:15:31,689 --> 00:15:36,660
this like huge upgrade of kubernetes now

00:15:34,659 --> 00:15:41,819
and then everything fails and

00:15:36,660 --> 00:15:44,459
so yeah we were just doing all this

00:15:41,819 --> 00:15:49,380
complexity to take away complexity from

00:15:44,459 --> 00:15:52,170
the developers and yeah one thing that

00:15:49,380 --> 00:15:54,990
is really important to us there is that

00:15:52,170 --> 00:15:57,480
we have full immutability so we're using

00:15:54,990 --> 00:16:02,540
Korres for the underlying house but also

00:15:57,480 --> 00:16:06,000
for the tenant clusters we're creating

00:16:02,540 --> 00:16:09,360
and yeah we have like a continuous

00:16:06,000 --> 00:16:12,149
pipeline where we build our images and

00:16:09,360 --> 00:16:15,870
then push the images to registry and

00:16:12,149 --> 00:16:20,910
then also apply those changes to

00:16:15,870 --> 00:16:23,899
kubernetes automatically so that means

00:16:20,910 --> 00:16:26,730
for our services that happens on the

00:16:23,899 --> 00:16:29,190
Chinese's on the underlying kubernetes

00:16:26,730 --> 00:16:33,000
so our services are running in there and

00:16:29,190 --> 00:16:35,190
then if somebody upgrades component you

00:16:33,000 --> 00:16:39,089
know it's automate automate Akili

00:16:35,190 --> 00:16:44,329
deployed and but we also do this for the

00:16:39,089 --> 00:16:48,990
tenant clusters and yeah that gives us

00:16:44,329 --> 00:16:53,699
reproducible builds we can create the

00:16:48,990 --> 00:16:58,850
same build forever we can create the

00:16:53,699 --> 00:17:02,040
same deployment at any point in time and

00:16:58,850 --> 00:17:04,439
it's not something that just lives and

00:17:02,040 --> 00:17:07,559
is like item potent lead patched all the

00:17:04,439 --> 00:17:10,799
time and then you didn't in this library

00:17:07,559 --> 00:17:13,650
or that library and slowly everything

00:17:10,799 --> 00:17:17,159
diverged from what what you would create

00:17:13,650 --> 00:17:19,289
if you create it now from scratch so you

00:17:17,159 --> 00:17:21,600
can also like easily automate like spin

00:17:19,289 --> 00:17:26,549
up a cluster do some testing tear it

00:17:21,600 --> 00:17:28,610
down and do it again so yeah and every

00:17:26,549 --> 00:17:31,049
manual change on the systems is a firmer

00:17:28,610 --> 00:17:34,260
ephemeral because we're also kind of

00:17:31,049 --> 00:17:36,510
managing what should not be on the

00:17:34,260 --> 00:17:38,010
machines because if you have like some

00:17:36,510 --> 00:17:41,610
scripts that go to your machines and

00:17:38,010 --> 00:17:44,760
install something you don't probably

00:17:41,610 --> 00:17:46,799
don't remove things that shouldn't be

00:17:44,760 --> 00:17:50,550
installed I mean who's doing that with

00:17:46,799 --> 00:17:54,150
an table removing every package

00:17:50,550 --> 00:17:56,940
shouldn't be on your machines I didn't

00:17:54,150 --> 00:17:59,760
do that as well as a sensible but if you

00:17:56,940 --> 00:18:04,110
if you have everything immutable and you

00:17:59,760 --> 00:18:07,370
frequently change your images you

00:18:04,110 --> 00:18:11,100
frequently change the machines the

00:18:07,370 --> 00:18:12,570
containers at some point manual changes

00:18:11,100 --> 00:18:15,570
will just be gone

00:18:12,570 --> 00:18:19,440
so everybody who does something needs to

00:18:15,570 --> 00:18:23,280
persist it somewhere within the stack

00:18:19,440 --> 00:18:25,320
yeah and mean time to recovery those

00:18:23,280 --> 00:18:27,600
clusters are pretty fast so if you

00:18:25,320 --> 00:18:29,010
create this this cluster in under under

00:18:27,600 --> 00:18:33,330
a minute you have a fully working

00:18:29,010 --> 00:18:35,310
kubernetes cluster and and yet that's

00:18:33,330 --> 00:18:39,810
also important so if hits the fan

00:18:35,310 --> 00:18:45,720
you know still able to recover quite

00:18:39,810 --> 00:18:50,370
fast and yeah some detail about the

00:18:45,720 --> 00:18:53,730
networking so I mean how are we doing

00:18:50,370 --> 00:18:56,070
this with creating kubernetes clusters

00:18:53,730 --> 00:19:05,150
with kubernetes on bare metal so we

00:18:56,070 --> 00:19:07,800
actually start KVM a VMS in plots so

00:19:05,150 --> 00:19:11,400
just imagine creating a deployment in

00:19:07,800 --> 00:19:13,470
kubernetes that has a replica set that

00:19:11,400 --> 00:19:17,340
says okay I want to have three workers

00:19:13,470 --> 00:19:21,350
and then the kubernetes takes care of

00:19:17,340 --> 00:19:26,630
creating this creating three containers

00:19:21,350 --> 00:19:30,390
with chemo tooling in that start VMs and

00:19:26,630 --> 00:19:33,930
then inside the VM we have again chorus

00:19:30,390 --> 00:19:36,570
and form the cluster across those nodes

00:19:33,930 --> 00:19:39,260
and this sounds a little bit weird but

00:19:36,570 --> 00:19:42,000
you know the container is just a package

00:19:39,260 --> 00:19:44,040
that we use to have the chemo tooling

00:19:42,000 --> 00:19:48,680
because chorus doesn't have that and

00:19:44,040 --> 00:19:53,640
then it's just spinning up a VM process

00:19:48,680 --> 00:19:57,870
inside the container namespace so

00:19:53,640 --> 00:20:01,200
there's this like not really overhead

00:19:57,870 --> 00:20:03,969
involved there and I mean the KPM

00:20:01,200 --> 00:20:10,499
overhead obviously and

00:20:03,969 --> 00:20:14,529
and it's quite handy for us to do so and

00:20:10,499 --> 00:20:17,499
so we're using flammer

00:20:14,529 --> 00:20:19,179
to connect an actress's out we're not

00:20:17,499 --> 00:20:23,739
using the Flannan server anymore

00:20:19,179 --> 00:20:27,099
there was a experimental feature we're

00:20:23,739 --> 00:20:29,979
just using this line here only

00:20:27,099 --> 00:20:32,829
so we're connecting the VMS outside of

00:20:29,979 --> 00:20:34,599
the VM through flannel so usually if you

00:20:32,829 --> 00:20:36,759
have a community set up you would use a

00:20:34,599 --> 00:20:39,639
flannel in a different way but we're

00:20:36,759 --> 00:20:42,099
using it to to actually connect the VMS

00:20:39,639 --> 00:20:47,529
across the host we're using a via clan

00:20:42,099 --> 00:20:50,799
backend in in flannel and so they will

00:20:47,529 --> 00:20:53,919
be in the same vni and and can talk to

00:20:50,799 --> 00:20:57,039
each other and then inside and then

00:20:53,919 --> 00:20:59,829
inside we use in calico to collect to

00:20:57,039 --> 00:21:05,339
connect all the containers between each

00:20:59,829 --> 00:21:08,589
other and and then you can have like a

00:21:05,339 --> 00:21:11,709
different tenant on the same physical

00:21:08,589 --> 00:21:15,309
machines that are also connected through

00:21:11,709 --> 00:21:19,149
a different flannel bridge and have

00:21:15,309 --> 00:21:22,989
calico inside so flannel does the tunnel

00:21:19,149 --> 00:21:26,049
for the for the VMS since it's outside

00:21:22,989 --> 00:21:31,959
and nobody inside the VM can change that

00:21:26,049 --> 00:21:37,359
and calico is there to connect a

00:21:31,959 --> 00:21:41,499
container from one house to the other we

00:21:37,359 --> 00:21:46,749
also do so every component in

00:21:41,499 --> 00:21:50,669
communities has TLS obviously and so we

00:21:46,749 --> 00:21:54,669
create a root CA per cluster

00:21:50,669 --> 00:21:57,940
automatically in vault vault in the PKI

00:21:54,669 --> 00:22:00,249
back-end that you mount then you can

00:21:57,940 --> 00:22:03,190
create your root CA and then you can

00:22:00,249 --> 00:22:05,940
create a role and tokens for for those

00:22:03,190 --> 00:22:10,329
roles that are allowed to issue

00:22:05,940 --> 00:22:12,779
certificates of certain domains and you

00:22:10,329 --> 00:22:15,909
know TTLs and stuff like that and

00:22:12,779 --> 00:22:17,680
instance we're able to do this we're in

00:22:15,909 --> 00:22:21,340
an automated way we're

00:22:17,680 --> 00:22:23,500
rotating them everyday we did this for a

00:22:21,340 --> 00:22:26,410
while it was like a couple of minutes

00:22:23,500 --> 00:22:30,280
and we found a lot of box so it was a

00:22:26,410 --> 00:22:34,920
nice resilience test doing so but you

00:22:30,280 --> 00:22:38,530
know one day works very well and

00:22:34,920 --> 00:22:42,600
actually we need us doesn't check

00:22:38,530 --> 00:22:42,600
revocation this so

00:22:43,299 --> 00:22:48,940
rotating shortly makes sense because you

00:22:46,989 --> 00:22:50,679
cannot revoke a certificate and because

00:22:48,940 --> 00:22:55,119
the kubernetes common components

00:22:50,679 --> 00:22:59,139
wouldn't check the revocation list for

00:22:55,119 --> 00:23:01,269
performance reasons and and to avoid

00:22:59,139 --> 00:23:03,220
that it fails and then after one day

00:23:01,269 --> 00:23:06,100
you're there you're screwed

00:23:03,220 --> 00:23:08,860
actually we're currently using the TTL

00:23:06,100 --> 00:23:14,409
of three days so it's a little bit of

00:23:08,860 --> 00:23:17,220
time to get the certificate set up if I

00:23:14,409 --> 00:23:21,460
don't know volt say it's four on a day

00:23:17,220 --> 00:23:23,710
we're not screwed and most of the

00:23:21,460 --> 00:23:25,299
kubernetes components have mutual TLS

00:23:23,710 --> 00:23:29,919
even so you have client and server

00:23:25,299 --> 00:23:35,700
certificate and this is also the Aria

00:23:29,919 --> 00:23:39,580
that can be used in authentication as a

00:23:35,700 --> 00:23:43,629
basis for our back so the role based

00:23:39,580 --> 00:23:45,730
access depends on the common name or the

00:23:43,629 --> 00:23:47,919
organization in the certificate so you

00:23:45,730 --> 00:23:51,279
the common name is your like user and

00:23:47,919 --> 00:23:55,600
the organization is your group that you

00:23:51,279 --> 00:24:01,119
belong to in in the in the role model of

00:23:55,600 --> 00:24:06,100
kubernetes and so it's it's also used

00:24:01,119 --> 00:24:09,639
for for our back there so yeah operators

00:24:06,100 --> 00:24:11,590
so we went through some iterations with

00:24:09,639 --> 00:24:13,989
our platform so in the beginning we have

00:24:11,590 --> 00:24:16,749
been using Korres for for quite some

00:24:13,989 --> 00:24:20,470
time and we have been using fleet very

00:24:16,749 --> 00:24:23,799
heavily and so we in the beginning it

00:24:20,470 --> 00:24:25,659
was the easiest thing for us to put like

00:24:23,799 --> 00:24:30,220
system D units with the kubernetes

00:24:25,659 --> 00:24:33,749
components or with this like docker

00:24:30,220 --> 00:24:37,119
containers so it's been empty ends on

00:24:33,749 --> 00:24:40,210
complete and we created the clusters

00:24:37,119 --> 00:24:41,859
like that and then we thought okay it

00:24:40,210 --> 00:24:43,809
would be better to ditch fleet

00:24:41,859 --> 00:24:47,470
completely where the own users of fleet

00:24:43,809 --> 00:24:50,350
and nobody else is using that anymore so

00:24:47,470 --> 00:24:53,470
let's move on so why not using

00:24:50,350 --> 00:24:55,629
kubernetes for it so we started out just

00:24:53,470 --> 00:24:56,950
writing our llaman files for cluster and

00:24:55,629 --> 00:24:58,840
then the

00:24:56,950 --> 00:25:00,730
being like keep cydia create and then

00:24:58,840 --> 00:25:06,940
you know all the bits and pieces of a

00:25:00,730 --> 00:25:09,910
cluster were created and at some point

00:25:06,940 --> 00:25:14,590
we started automating this because we

00:25:09,910 --> 00:25:17,920
didn't want to yeah have like templating

00:25:14,590 --> 00:25:21,340
a config management anymore so we

00:25:17,920 --> 00:25:24,100
started writing operators and operators

00:25:21,340 --> 00:25:27,400
so apparatus is a term that actually

00:25:24,100 --> 00:25:33,370
Korres coined the alternative name would

00:25:27,400 --> 00:25:35,290
be custom controller so it's basically

00:25:33,370 --> 00:25:39,910
about writing your own kubernetes

00:25:35,290 --> 00:25:42,520
controller and we're doing this to you

00:25:39,910 --> 00:25:44,770
know do all the things automatically

00:25:42,520 --> 00:25:51,220
that we have been doing manually as

00:25:44,770 --> 00:25:54,940
operators and the goal is managing

00:25:51,220 --> 00:25:56,800
desire state you say I want to have a

00:25:54,940 --> 00:25:58,870
cluster with these in these nodes and

00:25:56,800 --> 00:26:04,440
then the operator takes care to

00:25:58,870 --> 00:26:07,030
transform that to the actual state so

00:26:04,440 --> 00:26:09,340
that's how kubernetes works in general

00:26:07,030 --> 00:26:12,160
so you say I want to have three parts

00:26:09,340 --> 00:26:14,410
and then community is the internal

00:26:12,160 --> 00:26:16,150
controller takes care that actually

00:26:14,410 --> 00:26:18,340
there are three parts and if you kill

00:26:16,150 --> 00:26:23,310
one we need this will take care to just

00:26:18,340 --> 00:26:26,560
spin up a new one and so for doing this

00:26:23,310 --> 00:26:29,800
the API server is quite dump

00:26:26,560 --> 00:26:33,100
it's just crud operations so you have

00:26:29,800 --> 00:26:35,860
like a key value store so you create a

00:26:33,100 --> 00:26:38,410
resource in the API server and then

00:26:35,860 --> 00:26:41,620
there's something else that watches the

00:26:38,410 --> 00:26:44,320
API and he's like okay I have like five

00:26:41,620 --> 00:26:47,770
ports okay now has six pots and then it

00:26:44,320 --> 00:26:52,090
does a reconciliation loop with the

00:26:47,770 --> 00:26:54,670
existing things and C is okay I see five

00:26:52,090 --> 00:26:58,720
parts so I need to spin up one more so

00:26:54,670 --> 00:27:02,200
that's that's the loop controller is

00:26:58,720 --> 00:27:04,830
always in watching the API seeing what's

00:27:02,200 --> 00:27:09,160
define the desired state and then

00:27:04,830 --> 00:27:12,160
listing what's actually there and then

00:27:09,160 --> 00:27:14,799
trying to apply the difference that also

00:27:12,160 --> 00:27:19,150
means removing one if you define it

00:27:14,799 --> 00:27:22,150
should be one less and and to do your

00:27:19,150 --> 00:27:24,039
own extension of the kubernetes api it's

00:27:22,150 --> 00:27:27,970
quite simple to create a third-party

00:27:24,039 --> 00:27:30,220
resource so kubernetes comes with a

00:27:27,970 --> 00:27:33,160
bunch of resources predefined that

00:27:30,220 --> 00:27:35,530
support services jobs daemon sets

00:27:33,160 --> 00:27:39,160
replica set deployments

00:27:35,530 --> 00:27:40,870
Ingres lots of concepts that are in

00:27:39,160 --> 00:27:43,929
communities where kubernetes has a

00:27:40,870 --> 00:27:45,669
controller for or ingress is a little

00:27:43,929 --> 00:27:48,190
bit different where there are external

00:27:45,669 --> 00:27:51,010
controllers for it but you can also

00:27:48,190 --> 00:27:54,340
create your own resource so for instance

00:27:51,010 --> 00:27:58,049
I don't know if you want to write a shop

00:27:54,340 --> 00:28:00,970
system on kubernetes you could create

00:27:58,049 --> 00:28:03,220
the third-party resource called shop and

00:28:00,970 --> 00:28:05,950
you put some attributes in this

00:28:03,220 --> 00:28:08,950
third-party resource just think of it as

00:28:05,950 --> 00:28:12,760
like a database schema that you create

00:28:08,950 --> 00:28:15,370
so it would be like what kind of theme

00:28:12,760 --> 00:28:18,820
that what kind of URL what's the title

00:28:15,370 --> 00:28:20,950
of the shop things like that put them in

00:28:18,820 --> 00:28:22,840
the third-party resource and then write

00:28:20,950 --> 00:28:26,020
your own controller that is okay

00:28:22,840 --> 00:28:28,240
somebody created a new shop so I need to

00:28:26,020 --> 00:28:31,630
do like create this part this part in

00:28:28,240 --> 00:28:34,780
this part and so my shop incidence this

00:28:31,630 --> 00:28:37,419
is created that's what you can do with

00:28:34,780 --> 00:28:40,840
items once like that's what you can do

00:28:37,419 --> 00:28:44,260
with third-party resources you can

00:28:40,840 --> 00:28:45,610
easily extend the the kubernetes api but

00:28:44,260 --> 00:28:49,950
you also have to write your own

00:28:45,610 --> 00:28:56,730
controller for it and so what we did is

00:28:49,950 --> 00:28:59,590
we created a KBM operator that defines a

00:28:56,730 --> 00:29:02,860
cluster third-party resource in

00:28:59,590 --> 00:29:05,440
kubernetes and so it has some attributes

00:29:02,860 --> 00:29:08,530
how a cluster looks like and then the

00:29:05,440 --> 00:29:11,770
KVM operator actually translates the

00:29:08,530 --> 00:29:14,049
cluster definition into the actual pots

00:29:11,770 --> 00:29:15,700
that need to be running i mean it's

00:29:14,049 --> 00:29:17,770
there's more to it than pots there's

00:29:15,700 --> 00:29:21,070
ingress and then some jobs and their

00:29:17,770 --> 00:29:23,250
services and stuff like that so it would

00:29:21,070 --> 00:29:26,770
just create all the necessary

00:29:23,250 --> 00:29:29,200
community's native resources so

00:29:26,770 --> 00:29:31,170
translating our abstract model of a

00:29:29,200 --> 00:29:35,650
cluster into the like the real

00:29:31,170 --> 00:29:40,090
community's concept and we did the same

00:29:35,650 --> 00:29:42,640
thing for AWS with the AWS operator -

00:29:40,090 --> 00:29:45,520
you can still do like cube CTL create

00:29:42,640 --> 00:29:49,470
cluster so you have a cluster Yammer

00:29:45,520 --> 00:29:52,360
where you define your cluster and and

00:29:49,470 --> 00:29:55,000
the AWS operator would actually do the

00:29:52,360 --> 00:29:58,030
written installation loop with the AWS

00:29:55,000 --> 00:30:00,820
API so it checks with the kubernetes api

00:29:58,030 --> 00:30:05,380
a how many clusters should be there and

00:30:00,820 --> 00:30:07,360
then check with the AWS api are the

00:30:05,380 --> 00:30:13,330
clusters really there and created and

00:30:07,360 --> 00:30:16,090
are other components there and so the

00:30:13,330 --> 00:30:18,550
workflow looks like this so the cluster

00:30:16,090 --> 00:30:20,590
admin creates you know and ignore our

00:30:18,550 --> 00:30:24,340
services here because in the end we're

00:30:20,590 --> 00:30:26,950
writing the PPO which is the third party

00:30:24,340 --> 00:30:28,930
object the resource is the schema you

00:30:26,950 --> 00:30:31,510
define and then the object is the

00:30:28,930 --> 00:30:35,830
instance like the role you insert insert

00:30:31,510 --> 00:30:38,890
into your database and so we are so here

00:30:35,830 --> 00:30:41,590
you see everything is empty this is the

00:30:38,890 --> 00:30:46,540
host cluster community state and then

00:30:41,590 --> 00:30:49,570
our services right at EPO to the

00:30:46,540 --> 00:30:55,030
kubernetes api so we create a cluster

00:30:49,570 --> 00:30:59,650
object a CA object and multiple objects

00:30:55,030 --> 00:31:03,460
for certificates and then our operators

00:30:59,650 --> 00:31:10,630
so you can see the CA operator will pick

00:31:03,460 --> 00:31:16,150
up the CA TPO and talk to vault and

00:31:10,630 --> 00:31:18,670
create the backend in vault and creates

00:31:16,150 --> 00:31:25,990
the root CA creates the necessary roles

00:31:18,670 --> 00:31:30,790
and and then the sort operator is able

00:31:25,990 --> 00:31:35,050
to to issue the certificates based on

00:31:30,790 --> 00:31:36,180
the list that was given in the search t

00:31:35,050 --> 00:31:39,780
POS

00:31:36,180 --> 00:31:44,100
and then the KVM operator is able to

00:31:39,780 --> 00:31:46,680
actually create the cluster itself and

00:31:44,100 --> 00:31:48,930
then you see like from here to here here

00:31:46,680 --> 00:31:52,650
you see you have no parts and suddenly

00:31:48,930 --> 00:31:55,820
you have flannel running a master

00:31:52,650 --> 00:32:00,630
running workers running inside the host

00:31:55,820 --> 00:32:03,900
kubernetes and yeah similar on AWS but

00:32:00,630 --> 00:32:07,140
then you don't get pots in here but

00:32:03,900 --> 00:32:13,080
actually the ad RDS operator talks to

00:32:07,140 --> 00:32:16,320
the AWS api and typical things that are

00:32:13,080 --> 00:32:20,870
in those cluster definitions are you

00:32:16,320 --> 00:32:24,210
know configuration and versions and so

00:32:20,870 --> 00:32:26,010
yeah it's a it's a huge list of llaman

00:32:24,210 --> 00:32:29,790
attributes that didn't fit on my screen

00:32:26,010 --> 00:32:33,990
so sorry for not giving you some some

00:32:29,790 --> 00:32:36,230
examples but yeah it's basically what

00:32:33,990 --> 00:32:40,200
you would do in a templating

00:32:36,230 --> 00:32:42,240
configuration management as well most of

00:32:40,200 --> 00:32:43,830
the stuff is just holding contact maps

00:32:42,240 --> 00:32:48,600
in in kubernetes

00:32:43,830 --> 00:32:50,430
so if no in this is this is in the in

00:32:48,600 --> 00:32:52,440
the TPR but then it's translated into

00:32:50,430 --> 00:32:56,390
conflict maps and secrets and all these

00:32:52,440 --> 00:33:00,240
native components and in communities and

00:32:56,390 --> 00:33:03,480
you know similar for for the certificate

00:33:00,240 --> 00:33:07,490
TPR so we have a TTL you have a common

00:33:03,480 --> 00:33:07,490
name you have some alternative names and

00:33:07,730 --> 00:33:18,290
yeah questions because we're nearly end

00:33:14,550 --> 00:33:18,290
to my talk yeah

00:33:21,259 --> 00:33:28,639
yeah it didn't make sense to have like

00:33:23,969 --> 00:33:28,639
nested virtualization in ec2 instance

00:33:31,189 --> 00:33:34,189
yes

00:33:39,080 --> 00:33:42,590
other questions

00:33:43,640 --> 00:33:50,790
yeah so one learning for us was yeah we

00:33:48,330 --> 00:33:54,210
should use very small operators and not

00:33:50,790 --> 00:33:57,690
built to two big operators that do a lot

00:33:54,210 --> 00:34:00,570
of things I mean the CA the certain

00:33:57,690 --> 00:34:05,220
operator and the KVM operators are

00:34:00,570 --> 00:34:08,159
already a nice separation and for

00:34:05,220 --> 00:34:11,490
instance currently doing some DNS stuff

00:34:08,159 --> 00:34:16,470
inside the operator and we would like to

00:34:11,490 --> 00:34:19,200
separate that out into into a DNS

00:34:16,470 --> 00:34:24,270
operator that is then like if you create

00:34:19,200 --> 00:34:25,710
a DNS GPO then you have an interface and

00:34:24,270 --> 00:34:28,050
then you have like the DNS operator

00:34:25,710 --> 00:34:31,260
talking to CloudFlare talking to route

00:34:28,050 --> 00:34:35,850
53 to whatever you want to talk to

00:34:31,260 --> 00:34:38,970
actually create a DNS entry that would

00:34:35,850 --> 00:34:41,130
help and not having this inside of the

00:34:38,970 --> 00:34:43,040
KVM operator and then again inside the

00:34:41,130 --> 00:34:47,220
AWS operator but slightly different

00:34:43,040 --> 00:34:50,850
those kind of things one colleague wrote

00:34:47,220 --> 00:34:53,190
a Pingdom operator so for every ingress

00:34:50,850 --> 00:34:56,820
in the cluster the Pingdom operator

00:34:53,190 --> 00:34:59,520
creates a check and Pingdom which is

00:34:56,820 --> 00:35:02,040
quite nice and it's like nearly instant

00:34:59,520 --> 00:35:04,380
and you remove the ingress like the room

00:35:02,040 --> 00:35:06,530
remove your service from the cluster and

00:35:04,380 --> 00:35:08,820
it's instantly removed from Pingdom and

00:35:06,530 --> 00:35:11,400
that's quite nice and you have this nice

00:35:08,820 --> 00:35:13,190
decoupling of things where you have

00:35:11,400 --> 00:35:15,240
something that you know takes care of

00:35:13,190 --> 00:35:20,880
translating this I mean it doesn't need

00:35:15,240 --> 00:35:23,070
to be a Pingdom specific definition it's

00:35:20,880 --> 00:35:25,590
just based on the ingress resource that

00:35:23,070 --> 00:35:27,690
is anyway in your cluster so the the

00:35:25,590 --> 00:35:30,630
Pingdom operator just watches the

00:35:27,690 --> 00:35:32,670
ingresses that are anyway there and then

00:35:30,630 --> 00:35:34,200
talks to Pingdom and creates a somebody

00:35:32,670 --> 00:35:39,330
you could do that with Eddie are any

00:35:34,200 --> 00:35:44,060
other monitoring system as well and yeah

00:35:39,330 --> 00:35:48,710
so keep them keep them small have like a

00:35:44,060 --> 00:35:48,710
a focus of that

00:35:48,969 --> 00:35:53,799
operator makes sense

00:35:51,339 --> 00:35:55,299
we also finding more and more that we

00:35:53,799 --> 00:35:57,099
have some boilerplate code in the

00:35:55,299 --> 00:36:02,940
operators you're always doing the same

00:35:57,099 --> 00:36:08,349
thing in in the operators and so we are

00:36:02,940 --> 00:36:12,670
putting some of our code into a library

00:36:08,349 --> 00:36:15,130
at the moment or we have a library based

00:36:12,670 --> 00:36:16,569
on go kit for our micro service it's

00:36:15,130 --> 00:36:18,519
called micro kit and we want to do the

00:36:16,569 --> 00:36:22,390
same thing for operators it's only

00:36:18,519 --> 00:36:24,849
having operator kit and and one

00:36:22,390 --> 00:36:27,549
important thing I think that's the most

00:36:24,849 --> 00:36:31,029
important thing with upstream kubernetes

00:36:27,549 --> 00:36:33,009
is the the whole community is heading

00:36:31,029 --> 00:36:34,930
into having self hosted kubernetes

00:36:33,009 --> 00:36:38,019
clusters and that's also very important

00:36:34,930 --> 00:36:40,059
for us so self hosted communities means

00:36:38,019 --> 00:36:42,519
another type of inception that would

00:36:40,059 --> 00:36:45,640
mean the components that form kubernetes

00:36:42,519 --> 00:36:48,849
would run inside of kubernetes as well

00:36:45,640 --> 00:36:50,769
so inside the same communities so you

00:36:48,849 --> 00:36:52,749
spin up a kubernetes cluster and then

00:36:50,769 --> 00:36:54,819
the bits and pieces that form the

00:36:52,749 --> 00:36:58,059
cluster are actually parts inside of the

00:36:54,819 --> 00:37:01,749
cluster so you could do a rolling update

00:36:58,059 --> 00:37:05,650
of cubelet and every kubernetes cluster

00:37:01,749 --> 00:37:08,799
is able to do that with itself so that

00:37:05,650 --> 00:37:11,109
would be easy for us to just delegate

00:37:08,799 --> 00:37:14,680
that to the clusters so if the outer

00:37:11,109 --> 00:37:18,910
cluster says our clusters back this says

00:37:14,680 --> 00:37:22,989
okay we want to now have like 1.7 1.6 .

00:37:18,910 --> 00:37:26,200
3 then you change this in the cluster

00:37:22,989 --> 00:37:28,599
TPR in our cluster operator the KVM

00:37:26,200 --> 00:37:31,299
operator would just delegate this to the

00:37:28,599 --> 00:37:33,640
inner cluster and say like hey you

00:37:31,299 --> 00:37:36,960
should upgrade your your part and please

00:37:33,640 --> 00:37:41,680
do a rolling update of your of your

00:37:36,960 --> 00:37:44,079
couplets and that's much easier and it's

00:37:41,680 --> 00:37:46,559
something that every communities

00:37:44,079 --> 00:37:50,469
installation would benefit from so

00:37:46,559 --> 00:37:52,839
currently the the cig lifecycle is

00:37:50,469 --> 00:37:56,700
working on making this default in cube

00:37:52,839 --> 00:38:00,430
admin having self hosted clusters and

00:37:56,700 --> 00:38:03,610
yeah we also working on this

00:38:00,430 --> 00:38:06,220
and so that's kind of important to

00:38:03,610 --> 00:38:11,130
achieve to make the lifecycle of

00:38:06,220 --> 00:38:11,130
communities clusters easier in general

00:38:11,700 --> 00:38:25,440
yeah thank you are there any questions

00:38:29,010 --> 00:38:35,400
so what does the current work will look

00:38:31,990 --> 00:38:38,290
like when you upgrade kubernetes

00:38:35,400 --> 00:38:40,170
currently we are not upgrading in place

00:38:38,290 --> 00:38:42,610
that's something we're working on

00:38:40,170 --> 00:38:45,490
especially also because we don't want to

00:38:42,610 --> 00:38:47,200
invent something separate from the

00:38:45,490 --> 00:38:49,570
community that doesn't make sense for us

00:38:47,200 --> 00:38:53,460
so we want to do that with the community

00:38:49,570 --> 00:38:56,530
to have a nice way to upgrade clusters

00:38:53,460 --> 00:38:58,750
so at the moment is you create a new

00:38:56,530 --> 00:39:00,370
cluster with a new version that's the

00:38:58,750 --> 00:39:04,470
current state then move your apps

00:39:00,370 --> 00:39:08,590
automatically yes yeah yeah I mean we we

00:39:04,470 --> 00:39:11,080
we did quite a lot of rolling updates of

00:39:08,590 --> 00:39:14,710
clusters as well but the operator

00:39:11,080 --> 00:39:18,100
version of our system doesn't yet

00:39:14,710 --> 00:39:21,190
support it so in the older versions of

00:39:18,100 --> 00:39:23,950
the platform which is rolling nodes so

00:39:21,190 --> 00:39:26,550
we're creating a new image like a new

00:39:23,950 --> 00:39:29,980
container image with the chemo tooling

00:39:26,550 --> 00:39:32,710
that has a templating for the cloud

00:39:29,980 --> 00:39:34,720
context so we're starting the KVM with

00:39:32,710 --> 00:39:38,140
core s and then we're changing out the

00:39:34,720 --> 00:39:41,050
the cloud config and then it starts at

00:39:38,140 --> 00:39:42,790
the new node with a new version and we

00:39:41,050 --> 00:39:44,530
could do the same thing with with

00:39:42,790 --> 00:39:47,230
operators and we're probably doing

00:39:44,530 --> 00:39:51,940
something like that but we also want to

00:39:47,230 --> 00:39:53,440
see what's going on upstream to have the

00:39:51,940 --> 00:39:56,530
goal is definitely to have self hosted

00:39:53,440 --> 00:39:58,210
clusters and not to invest too much time

00:39:56,530 --> 00:40:01,030
into tooling around that will be

00:39:58,210 --> 00:40:02,710
obsolete in a couple of months hopefully

00:40:01,030 --> 00:40:06,630
it's a couple of months it could also

00:40:02,710 --> 00:40:09,310
take more than a year you don't know so

00:40:06,630 --> 00:40:12,800
yeah our customers definitely have the

00:40:09,310 --> 00:40:17,080
needs to to upgrade and and it sometimes

00:40:12,800 --> 00:40:17,080
depends also on what are they running

00:40:17,330 --> 00:40:25,060
not very many are running a lot of state

00:40:20,450 --> 00:40:32,420
in containers at the moment thank you

00:40:25,060 --> 00:40:38,180
more questions come on I know it's early

00:40:32,420 --> 00:40:40,940
but anyway also for him we have plenty

00:40:38,180 --> 00:40:43,940
of time I have some coffee yeah

00:40:40,940 --> 00:40:49,640
any questions somebody needs a wake-up

00:40:43,940 --> 00:40:52,250
call I don't think that I can force

00:40:49,640 --> 00:40:54,500
somebody into what it would like to how

00:40:52,250 --> 00:40:56,060
are you running company I mean there are

00:40:54,500 --> 00:40:59,690
three people running kubernetes

00:40:56,060 --> 00:41:06,650
what are you using to provision I can

00:40:59,690 --> 00:41:08,330
ask questions yeah

00:41:06,650 --> 00:41:10,400
knocking we're running kubernetes on

00:41:08,330 --> 00:41:12,740
bare metal and we have our own the metal

00:41:10,400 --> 00:41:14,660
provisioning system I think that we

00:41:12,740 --> 00:41:18,560
wrote ourselves and that we are

00:41:14,660 --> 00:41:21,350
currently running to basically control

00:41:18,560 --> 00:41:24,080
all of operations so it's your own pixie

00:41:21,350 --> 00:41:26,180
boot system or it's a bit more than that

00:41:24,080 --> 00:41:28,070
it's actually too much on a single thing

00:41:26,180 --> 00:41:29,300
there is a system called server DB and

00:41:28,070 --> 00:41:32,630
there's a thing called Nemo

00:41:29,300 --> 00:41:35,150
so a DB does all of data center

00:41:32,630 --> 00:41:36,560
operations inclusive inclusive asset

00:41:35,150 --> 00:41:37,340
management and stuff it's not just

00:41:36,560 --> 00:41:40,550
fixable

00:41:37,340 --> 00:41:41,900
and Nemo does all network operations and

00:41:40,550 --> 00:41:45,020
that is everything from router and

00:41:41,900 --> 00:41:47,510
switch configuration to again line

00:41:45,020 --> 00:41:50,990
management and stuff like that it's two

00:41:47,510 --> 00:41:53,020
very huge awfully written giant PI's and

00:41:50,990 --> 00:41:55,550
jungle applications that basically

00:41:53,020 --> 00:41:58,400
control the entire data center but

00:41:55,550 --> 00:42:00,110
nobody does anything manually we are

00:41:58,400 --> 00:42:02,300
just clicking the web interface or

00:42:00,110 --> 00:42:04,340
talking to the jungle REST API to do

00:42:02,300 --> 00:42:08,390
stuff which is pretty convenient and

00:42:04,340 --> 00:42:10,760
inside that we then run also or various

00:42:08,390 --> 00:42:16,210
Cuban I discussed us there's more about

00:42:10,760 --> 00:42:16,210
that later today in my top ten cool

00:42:16,440 --> 00:42:20,670
okay we already hit the marketing part

00:42:21,060 --> 00:42:27,910
well done whoo edge is running

00:42:23,380 --> 00:42:31,599
communities it was one over there right

00:42:27,910 --> 00:42:34,180
with the experience I don't know I mean

00:42:31,599 --> 00:42:39,880
you can also say with a pain you're set

00:42:34,180 --> 00:42:43,840
up yeah we I mean we we also have our

00:42:39,880 --> 00:42:47,200
own PC boots system that we're using to

00:42:43,840 --> 00:42:49,410
spin up chorus and we also open source

00:42:47,200 --> 00:42:53,200
that there's also match box from chorus

00:42:49,410 --> 00:42:56,710
that does something like that where you

00:42:53,200 --> 00:43:00,190
can yeah provision your machines with

00:42:56,710 --> 00:43:03,849
Korres images and have some templating

00:43:00,190 --> 00:43:07,150
around cloud configs because you need to

00:43:03,849 --> 00:43:10,270
have services configured that spin up

00:43:07,150 --> 00:43:12,070
the cluster what we're trying to do at

00:43:10,270 --> 00:43:14,650
the moment is to reduce that to a

00:43:12,070 --> 00:43:17,590
minimum so we have been using system D

00:43:14,650 --> 00:43:19,210
with fleet heavily and you know we we

00:43:17,590 --> 00:43:21,160
did all this but at the moment we're

00:43:19,210 --> 00:43:25,030
trying to reduce the footprint in the

00:43:21,160 --> 00:43:28,810
cloud config or in ignition to have like

00:43:25,030 --> 00:43:30,820
as as low as possible in there and to

00:43:28,810 --> 00:43:32,710
have something like cube admin actually

00:43:30,820 --> 00:43:35,070
taking care of creating the cluster

00:43:32,710 --> 00:43:38,260
because that's what's in in the upstream

00:43:35,070 --> 00:43:40,869
that's used in the upstream community so

00:43:38,260 --> 00:43:43,030
if we create a cluster with cube and

00:43:40,869 --> 00:43:46,480
it's more likely to be a cluster that is

00:43:43,030 --> 00:43:48,400
common to the community and and that's

00:43:46,480 --> 00:43:51,849
our goal to have a vanilla kubernetes

00:43:48,400 --> 00:43:55,330
cluster that is not you know different

00:43:51,849 --> 00:43:59,109
from lots of other installations and and

00:43:55,330 --> 00:44:04,839
that would also you know reduce some of

00:43:59,109 --> 00:44:06,640
the like Jason and or Yammer the

00:44:04,839 --> 00:44:09,130
definition we have in our cloud context

00:44:06,640 --> 00:44:13,599
that makes sometimes makes it a little

00:44:09,130 --> 00:44:16,210
bit complicated you manage you mentioned

00:44:13,599 --> 00:44:17,770
a Pingdom monitoring before for ingress

00:44:16,210 --> 00:44:20,710
ingress --is that are automatically

00:44:17,770 --> 00:44:25,270
added to to Pingdom and what i wondering

00:44:20,710 --> 00:44:27,940
is if there is something that monitors

00:44:25,270 --> 00:44:28,840
your deployments and services and maybe

00:44:27,940 --> 00:44:31,720
parts or

00:44:28,840 --> 00:44:34,480
even containers for their availability

00:44:31,720 --> 00:44:37,420
state so I know there's lots of people

00:44:34,480 --> 00:44:39,580
using Prometheus for kubernetes

00:44:37,420 --> 00:44:44,260
monitoring and it's I think it's already

00:44:39,580 --> 00:44:48,430
somehow included but is there anything

00:44:44,260 --> 00:44:49,930
or is there even the need to monitor the

00:44:48,430 --> 00:44:52,540
availability of all these components

00:44:49,930 --> 00:44:57,160
that are building the cluster building

00:44:52,540 --> 00:45:00,600
the application yet I mean it's like

00:44:57,160 --> 00:45:04,600
doing unit tests and integration tests

00:45:00,600 --> 00:45:06,880
so you need to have an end-to-end test

00:45:04,600 --> 00:45:10,630
through your stack from the outside in

00:45:06,880 --> 00:45:12,970
my opinion and and you also need to

00:45:10,630 --> 00:45:15,670
monitor inside to actually see what

00:45:12,970 --> 00:45:18,880
smaller things are breaking that gives

00:45:15,670 --> 00:45:21,150
you more insight so we are usually doing

00:45:18,880 --> 00:45:24,760
both and we also have like a test bot in

00:45:21,150 --> 00:45:26,440
running inside the clusters and also in

00:45:24,760 --> 00:45:29,350
the host cluster that is able to

00:45:26,440 --> 00:45:31,900
continuously create a cluster and check

00:45:29,350 --> 00:45:33,880
a few things and does it still work that

00:45:31,900 --> 00:45:36,370
you can create a cluster those kind of

00:45:33,880 --> 00:45:40,870
things and so having end-to-end test

00:45:36,370 --> 00:45:45,070
integration tests that makes make sense

00:45:40,870 --> 00:45:48,360
next to having monitoring on internally

00:45:45,070 --> 00:45:51,190
inside I mean you could also do just

00:45:48,360 --> 00:45:53,200
monitor your money your inside

00:45:51,190 --> 00:45:55,630
monitoring system from the outside and

00:45:53,200 --> 00:46:00,130
check that it's available and then test

00:45:55,630 --> 00:46:02,800
internally but yeah we also do doing

00:46:00,130 --> 00:46:04,360
this for endpoints that are should be

00:46:02,800 --> 00:46:11,650
available to the outside that we are

00:46:04,360 --> 00:46:14,740
monitoring them from the outside okay

00:46:11,650 --> 00:46:16,720
yeah permisos is not included in in

00:46:14,740 --> 00:46:20,710
Combinator's but promises as part of the

00:46:16,720 --> 00:46:24,480
cloud native foundation as also so

00:46:20,710 --> 00:46:31,350
there's fluent e for shipping locks

00:46:24,480 --> 00:46:34,720
there's open tracing for me toys and

00:46:31,350 --> 00:46:38,130
there have been some recent additions

00:46:34,720 --> 00:46:38,130
linka D and

00:46:38,380 --> 00:46:44,950
I don't know but a couple of projects

00:46:41,050 --> 00:46:50,820
and these together form the cloud native

00:46:44,950 --> 00:46:54,100
stack so usually our expectation is that

00:46:50,820 --> 00:46:57,820
projects that are in the foundation also

00:46:54,100 --> 00:47:02,740
fit better with kubernetes than other

00:46:57,820 --> 00:47:04,150
solutions that's not always true but but

00:47:02,740 --> 00:47:11,950
at least they are like in a family

00:47:04,150 --> 00:47:13,960
somehow ok I think we are through it so

00:47:11,950 --> 00:47:18,240
thank you Thank You Timo

00:47:13,960 --> 00:47:18,240

YouTube URL: https://www.youtube.com/watch?v=gxwFctaBlOk


