Title: OSDC 2019: Fast log management for your infrastructure by Nicolas Frankel
Publication date: 2019-05-21
Playlist: OSDC 2019 | Open Source Data Center Conference
Description: 
	So, youâ€™ve migrated your application to Reactive Microservices to get the last ounce of performance from your servers. Still want more? Perhaps you forgot about the logs: logs can be one of the few roadblocks on the road to ultimate performance. At Exoscale, we faced the same challenges as everyone: the application produces logs, and they need to be stored in our log storage â€“ Elasticsearch, with the minimum of fuss and the fastest way possible. In this talk, Iâ€™ll show you some insider tips and tricks taken from our experience put you on the track toward fast(er) log management. Without revealing too much, it involves async loggers, JSON, Kafka and Logstash.

OPNsense is an open source and easy-to-use FreeBSD based firewall and routing platform. 2018 â€“ three years after OPNsense started as a fork of pfSenseÂ® and m0n0wall â€“ OPNsense brings the rich feature set of commercial offerings with the benefits of open and verifiable sources. A strong focus on security and code quality drives the development of the project. The modern and intuitive web interface makes configuring firewall rules funny ðŸ™‚
In this talk, Thomas will outline OPNsenseâ€™s FreeBSD-based architecture and how you can take advantage of additional features using OPNsense plugins. He will also show how to initially setup an OPNsense firewall, and how you use datacenter-features like High Availability & Hardware Failover or Dual Uplinks.
Open (source) makes sense â€“ also for your firewall ðŸ™‚

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:01,140 --> 00:00:10,550
[Music]

00:00:16,039 --> 00:00:21,119
thanks guy I'm happy to be here and

00:00:19,260 --> 00:00:23,580
thank you to be here for this talk about

00:00:21,119 --> 00:00:29,400
like how can you do like log ingestion

00:00:23,580 --> 00:00:32,369
Faust's I'm Nicola Frankel I have been a

00:00:29,400 --> 00:00:36,719
developer how many developers are in

00:00:32,369 --> 00:00:39,360
this room Oh still a few hands okay I

00:00:36,719 --> 00:00:42,600
was thinking a lot of new people would

00:00:39,360 --> 00:00:44,550
be like Ops teams but that's great

00:00:42,600 --> 00:00:47,579
I've been a developer architect in the

00:00:44,550 --> 00:00:49,739
Java ecosystem for a long time but six

00:00:47,579 --> 00:00:52,110
months ago I decided to change my career

00:00:49,739 --> 00:00:55,190
and thought that it would be much better

00:00:52,110 --> 00:00:58,410
to be paid to like go to conferences

00:00:55,190 --> 00:01:03,180
meet people have beers that that's a

00:00:58,410 --> 00:01:05,489
good way to be paid on even earlier on I

00:01:03,180 --> 00:01:07,920
was always interested in the other side

00:01:05,489 --> 00:01:10,080
of the wall you know where basically you

00:01:07,920 --> 00:01:12,630
throw software ads and you hope people

00:01:10,080 --> 00:01:16,259
somehow manage it so I'm yes I'm pretty

00:01:12,630 --> 00:01:19,350
like DevOps minded I work for a company

00:01:16,259 --> 00:01:21,570
called eggs or scale and at exascale we

00:01:19,350 --> 00:01:23,220
are actually providing you with like

00:01:21,570 --> 00:01:25,710
cloud computing and cloud storage

00:01:23,220 --> 00:01:31,229
services who here is already using the

00:01:25,710 --> 00:01:35,909
cloud introduction few hands of the room

00:01:31,229 --> 00:01:43,860
who is using Amazon okay who is using

00:01:35,909 --> 00:01:45,210
Google Microsoft's exascale that's what

00:01:43,860 --> 00:01:49,200
I thought

00:01:45,210 --> 00:01:53,700
guys do you know about the cloud act who

00:01:49,200 --> 00:01:57,659
knows about the cloud act no one two

00:01:53,700 --> 00:01:59,369
people three people and not probably

00:01:57,659 --> 00:02:01,979
those that are using the cloud the cloud

00:01:59,369 --> 00:02:04,439
act is the US law and it's an

00:02:01,979 --> 00:02:06,869
extraterritorial law which means that it

00:02:04,439 --> 00:02:10,920
applies outside of the US which is very

00:02:06,869 --> 00:02:12,690
funny like gdpr and actually it states

00:02:10,920 --> 00:02:13,470
that if you are contracting with an

00:02:12,690 --> 00:02:17,040
American

00:02:13,470 --> 00:02:19,680
funny that means that legally the

00:02:17,040 --> 00:02:22,050
government the US government can access

00:02:19,680 --> 00:02:27,900
the data center wherever it's located in

00:02:22,050 --> 00:02:31,020
the world change the new lights to you

00:02:27,900 --> 00:02:34,410
use of the clouds I know it's not your

00:02:31,020 --> 00:02:37,140
problem but if you are working for a

00:02:34,410 --> 00:02:40,860
company that handles data and that

00:02:37,140 --> 00:02:43,560
customer give like this data to the

00:02:40,860 --> 00:02:46,020
company trusting them to handle it can

00:02:43,560 --> 00:02:48,660
create a lot of issues I will really

00:02:46,020 --> 00:02:50,730
recommend that you like go to your legal

00:02:48,660 --> 00:02:53,610
department check about the cloud act and

00:02:50,730 --> 00:02:56,010
probably change to like a European cloud

00:02:53,610 --> 00:02:58,620
provider we are not the only one we are

00:02:56,010 --> 00:02:59,209
one of them but be very careful about

00:02:58,620 --> 00:03:03,320
that

00:02:59,209 --> 00:03:09,209
okay back to log management's who can

00:03:03,320 --> 00:03:12,270
not understand this stuff this is what

00:03:09,209 --> 00:03:16,980
you do even if you are not a developer

00:03:12,270 --> 00:03:20,130
this is what you do generally right you

00:03:16,980 --> 00:03:23,090
do it back this is Java who here doesn't

00:03:20,130 --> 00:03:26,970
on the same job at all

00:03:23,090 --> 00:03:34,760
okay what's your preferred language no

00:03:26,970 --> 00:03:37,830
long which you are agnostic okay - okay

00:03:34,760 --> 00:03:41,040
Python it will be hard for me because I

00:03:37,830 --> 00:03:43,500
don't know Python but okay this is a

00:03:41,040 --> 00:03:47,489
debug statements and basically you have

00:03:43,500 --> 00:03:49,739
like the string and you have the

00:03:47,489 --> 00:03:52,200
computation right this is what you

00:03:49,739 --> 00:03:56,790
expect to have in your logs is card

00:03:52,200 --> 00:03:59,310
price will be something this is what

00:03:56,790 --> 00:04:02,150
developers generally do and they are

00:03:59,310 --> 00:04:06,239
blissfully unaware of how it works

00:04:02,150 --> 00:04:10,590
because what they believe is that here I

00:04:06,239 --> 00:04:13,680
have a debug so what happens if my log

00:04:10,590 --> 00:04:19,350
level is above debug is it will be

00:04:13,680 --> 00:04:21,530
ignored who thinks the same yeah that's

00:04:19,350 --> 00:04:21,530
normal

00:04:22,479 --> 00:04:32,060
however it's not the case and I have a

00:04:27,410 --> 00:04:34,759
little program that does it so I've

00:04:32,060 --> 00:04:37,340
created logger with a self 4G blah blah

00:04:34,759 --> 00:04:40,610
blah I mean the important post is this

00:04:37,340 --> 00:04:43,520
one I have this logger dot debug and

00:04:40,610 --> 00:04:47,539
then I have my message and then I check

00:04:43,520 --> 00:04:54,320
how long it will like be for me to log

00:04:47,539 --> 00:04:56,389
this everybody agrees with that no

00:04:54,320 --> 00:04:58,430
issues just tell me

00:04:56,389 --> 00:05:01,990
really I'm if you if something is not

00:04:58,430 --> 00:05:01,990
fine let's let's discuss it

00:05:11,060 --> 00:05:21,590
five seconds took me five seconds to run

00:05:15,410 --> 00:05:25,700
that and if I check the defaults stuff

00:05:21,590 --> 00:05:28,880
I'm in full level so I'm above debug so

00:05:25,700 --> 00:05:30,680
debug statesman's and actually it's the

00:05:28,880 --> 00:05:35,300
case the debug statement was never

00:05:30,680 --> 00:05:41,930
written but it took me five seconds who

00:05:35,300 --> 00:05:42,680
can tell me why they don't be shy no

00:05:41,930 --> 00:05:47,980
worries

00:05:42,680 --> 00:05:47,980
I say a lot of crap myself yeah sure

00:05:57,880 --> 00:06:00,880
perfect

00:06:02,920 --> 00:06:09,260
so here I've been working in e-commerce

00:06:06,710 --> 00:06:12,590
when you compute the price of a court

00:06:09,260 --> 00:06:14,720
it's not a it's a value because you need

00:06:12,590 --> 00:06:17,060
to apply the discount you need to know

00:06:14,720 --> 00:06:19,400
who is logged in if they I mean there

00:06:17,060 --> 00:06:21,710
are a lot of computation here I simulate

00:06:19,400 --> 00:06:24,620
a long computation the computation takes

00:06:21,710 --> 00:06:26,930
time and it's not guarded by the debug

00:06:24,620 --> 00:06:30,470
it's not worried at all by the debug

00:06:26,930 --> 00:06:33,530
actually so whatever you logs are doing

00:06:30,470 --> 00:06:39,410
if you are doing computation inside of

00:06:33,530 --> 00:06:42,220
logging it will slow you down so how do

00:06:39,410 --> 00:06:42,220
we handle that

00:06:48,040 --> 00:06:59,890
this is how we enter that I put a gourd

00:06:54,910 --> 00:07:02,890
myself if lager is debug enabled then

00:06:59,890 --> 00:07:06,190
bla bla bla in that case since the

00:07:02,890 --> 00:07:08,260
logger is only info then it's not debug

00:07:06,190 --> 00:07:11,500
enabled and then what is inside the

00:07:08,260 --> 00:07:19,110
block won't get executed right that's

00:07:11,500 --> 00:07:19,110
right just to be sure now

00:07:22,920 --> 00:07:33,280
instance everybody agrees with that so

00:07:31,480 --> 00:07:37,810
if you're a developer that's what you

00:07:33,280 --> 00:07:41,620
are meant to do every time we've no

00:07:37,810 --> 00:07:46,320
error like hey what happens if I do that

00:07:41,620 --> 00:07:46,320
because I mean we are only human being

00:07:47,400 --> 00:07:54,040
it can happen if every time you need to

00:07:52,120 --> 00:07:58,420
write a log statements you need to water

00:07:54,040 --> 00:08:01,090
it then you need to like be sure that

00:07:58,420 --> 00:08:06,520
it's in sync probably it won't be in

00:08:01,090 --> 00:08:09,040
sync yeah yeah then you are back to the

00:08:06,520 --> 00:08:11,830
square one because actually yeah you are

00:08:09,040 --> 00:08:14,230
warned enabled but it will be of course

00:08:11,830 --> 00:08:18,490
it's not the only stuff that you can do

00:08:14,230 --> 00:08:22,270
when you can do the opposite and then

00:08:18,490 --> 00:08:23,800
you lost a log at all so this is the

00:08:22,270 --> 00:08:32,969
kind of stuff that might happen if you

00:08:23,800 --> 00:08:34,200
do it like that but we can do better

00:08:32,969 --> 00:08:35,550
actually

00:08:34,200 --> 00:08:38,550
[Music]

00:08:35,550 --> 00:08:38,550
sorry

00:08:42,469 --> 00:08:50,040
since Java aids and already languages

00:08:47,639 --> 00:08:53,730
like Python or JavaScript alloyed since

00:08:50,040 --> 00:08:56,970
ages you don't need to pass the

00:08:53,730 --> 00:08:59,370
computation results to a log statement

00:08:56,970 --> 00:09:03,449
you just need to pass the computation

00:08:59,370 --> 00:09:09,240
itself so what you could do is you could

00:09:03,449 --> 00:09:13,709
write a lazy logger that wraps a logger

00:09:09,240 --> 00:09:16,680
and instead of accepting the result of

00:09:13,709 --> 00:09:23,660
the computation accepts a computation

00:09:16,680 --> 00:09:27,839
and then inside you can go audit and

00:09:23,660 --> 00:09:30,569
then only then if it's debug enabled

00:09:27,839 --> 00:09:34,079
then you call the computation and you

00:09:30,569 --> 00:09:36,029
execute the computation and if you call

00:09:34,079 --> 00:09:39,930
it in Java it's not super nice in other

00:09:36,029 --> 00:09:45,029
languages it's a bit nicer so you can

00:09:39,930 --> 00:09:50,879
have this kind of like notation and if I

00:09:45,029 --> 00:09:54,860
execute that it's not as good as a no op

00:09:50,879 --> 00:10:00,860
stuff but it's still pretty good and

00:09:54,860 --> 00:10:04,319
with zero risk of errors so you just had

00:10:00,860 --> 00:10:07,639
the main logger API and you just provide

00:10:04,319 --> 00:10:07,639
the lazy log or API

00:10:16,250 --> 00:10:20,790
that's the first lesson so it starts

00:10:19,200 --> 00:10:27,650
from the developer point of view you

00:10:20,790 --> 00:10:27,650
should only do lazy logging second point

00:10:27,770 --> 00:10:31,800
developers they don't like to think

00:10:29,820 --> 00:10:35,400
about it but actually we are dealing

00:10:31,800 --> 00:10:37,950
with the physical world we are dealing

00:10:35,400 --> 00:10:40,950
with hard drive and when you log in

00:10:37,950 --> 00:10:45,330
general you are writing something to a

00:10:40,950 --> 00:10:48,600
file and now it depends where your file

00:10:45,330 --> 00:10:52,410
is located if your file system is

00:10:48,600 --> 00:10:57,420
implemented on the SSD it might be very

00:10:52,410 --> 00:11:00,110
fast on the standard hard drive it must

00:10:57,420 --> 00:11:06,540
be slower on the network file system

00:11:00,110 --> 00:11:08,190
definitely it's going to be so on

00:11:06,540 --> 00:11:10,680
one side you have the developer world

00:11:08,190 --> 00:11:12,780
and everything is fine and it needs to

00:11:10,680 --> 00:11:15,390
do stuff on the other side you have the

00:11:12,780 --> 00:11:20,690
the apps word the infra world and you

00:11:15,390 --> 00:11:20,690
also need as an infra guy to handle that

00:11:20,990 --> 00:11:29,910
however we can cope with that in general

00:11:26,670 --> 00:11:32,390
what your logging framework does and so

00:11:29,910 --> 00:11:38,070
we get back to the developer world is

00:11:32,390 --> 00:11:41,760
you open the stream you write bytes you

00:11:38,070 --> 00:11:45,930
close the stream opening the stream

00:11:41,760 --> 00:11:48,150
takes time and you do it for every log

00:11:45,930 --> 00:11:50,160
statements you open the stream you write

00:11:48,150 --> 00:11:52,520
the blight close the stream you open the

00:11:50,160 --> 00:11:58,530
well-understood

00:11:52,520 --> 00:12:02,880
it could be very great if we could open

00:11:58,530 --> 00:12:06,020
the stream right many many many log

00:12:02,880 --> 00:12:10,760
statement and then close the stream and

00:12:06,020 --> 00:12:14,550
that's all a synchronous logging and

00:12:10,760 --> 00:12:17,850
it's not a developer problem it's not

00:12:14,550 --> 00:12:21,240
like an infrastructure problem it's just

00:12:17,850 --> 00:12:22,740
it's somewhere in the middle I don't

00:12:21,240 --> 00:12:24,530
know if you have those kind of people

00:12:22,740 --> 00:12:28,230
who are called integrators or

00:12:24,530 --> 00:12:29,279
configurators or whatever they are the

00:12:28,230 --> 00:12:32,100
one who configures

00:12:29,279 --> 00:12:36,269
software in every environment and they

00:12:32,100 --> 00:12:40,050
are the one that should do that by

00:12:36,269 --> 00:12:42,660
default you do synchronous log in but

00:12:40,050 --> 00:12:46,319
most frameworks they allow you to do

00:12:42,660 --> 00:12:52,589
asynchronous logging here I have a

00:12:46,319 --> 00:12:56,790
framework called log bank and this is

00:12:52,589 --> 00:12:59,100
how I do in synchronous logging so

00:12:56,790 --> 00:13:05,399
instead of writing directly to the file

00:12:59,100 --> 00:13:07,019
I just wrap my like well it's not here

00:13:05,399 --> 00:13:09,389
it shouldn't be STD out it should be

00:13:07,019 --> 00:13:13,499
fine sorry I just wrapped the stuff into

00:13:09,389 --> 00:13:16,259
an async appender and I'm using the I

00:13:13,499 --> 00:13:17,850
think appender to write it down it's not

00:13:16,259 --> 00:13:36,779
very interesting the running of that

00:13:17,850 --> 00:13:38,610
stuff but you understand it however now

00:13:36,779 --> 00:13:41,610
you start if you start doing a

00:13:38,610 --> 00:13:42,839
synchronous logging then you need to

00:13:41,610 --> 00:13:44,790
understand how it works basically

00:13:42,839 --> 00:13:48,029
because then there will be a queue of

00:13:44,790 --> 00:13:50,399
logging statements and the queue again

00:13:48,029 --> 00:13:54,050
we are bound to the physical world we

00:13:50,399 --> 00:13:57,930
are bound to memory it must have a size

00:13:54,050 --> 00:14:00,870
yeah it's it's always problem of

00:13:57,930 --> 00:14:03,720
trade-offs and I'm thinking so you need

00:14:00,870 --> 00:14:05,850
to configure the queue size and sometime

00:14:03,720 --> 00:14:10,230
you might think okay but my my queue

00:14:05,850 --> 00:14:16,410
starts like being newly complete what is

00:14:10,230 --> 00:14:18,509
better I keep every message and when my

00:14:16,410 --> 00:14:21,629
queue is fuel I start discarding them or

00:14:18,509 --> 00:14:24,360
when my queue starts being fuel then I

00:14:21,629 --> 00:14:27,230
discard the log levels that are below a

00:14:24,360 --> 00:14:29,459
certain threshold because the like error

00:14:27,230 --> 00:14:36,480
messages there are much more important

00:14:29,459 --> 00:14:38,699
than the debug messages so log back

00:14:36,480 --> 00:14:41,819
allows you to do that to say hey you can

00:14:38,699 --> 00:14:42,840
discard when your queue is let's say 80%

00:14:41,819 --> 00:14:46,200
few Lukens

00:14:42,840 --> 00:14:48,210
discounting because those log messages

00:14:46,200 --> 00:14:52,290
are much more important than debug

00:14:48,210 --> 00:14:57,270
statements and then there is this issue

00:14:52,290 --> 00:15:00,150
of reliability what is better to

00:14:57,270 --> 00:15:03,029
actually lose log message because your

00:15:00,150 --> 00:15:10,170
queue is full or to write them anyway

00:15:03,029 --> 00:15:10,670
and then start blocking again it's up to

00:15:10,170 --> 00:15:13,230
you

00:15:10,670 --> 00:15:17,450
it depends exactly I've been a

00:15:13,230 --> 00:15:17,450
consultant the answer it depends yes

00:15:23,930 --> 00:15:29,820
yeah so what the gentleman says yeah if

00:15:27,630 --> 00:15:33,120
you are using a synchronous logging then

00:15:29,820 --> 00:15:35,820
you have no reliability because if your

00:15:33,120 --> 00:15:38,400
program crashes for any reason then your

00:15:35,820 --> 00:15:40,290
G VM or your JavaScript engine or

00:15:38,400 --> 00:15:48,660
whatever goes down you lose everything

00:15:40,290 --> 00:15:52,200
in memory ok next point the log message

00:15:48,660 --> 00:15:55,380
is not the most important stuff it's

00:15:52,200 --> 00:15:57,960
worth noting if you don't get metadata

00:15:55,380 --> 00:16:00,660
associated with it one of the most

00:15:57,960 --> 00:16:05,190
important metadata that you need to have

00:16:00,660 --> 00:16:09,630
is timestamp yeah I mean it's stupid but

00:16:05,190 --> 00:16:11,790
it needs just to be told again log level

00:16:09,630 --> 00:16:14,310
is also an important metadata because

00:16:11,790 --> 00:16:17,220
afterwards you might need to search

00:16:14,310 --> 00:16:19,890
through all I don't know like error

00:16:17,220 --> 00:16:21,960
statements thread name class name

00:16:19,890 --> 00:16:24,240
whatever you name it you can have a lot

00:16:21,960 --> 00:16:28,580
of metadata that is associated with

00:16:24,240 --> 00:16:33,900
every log statement some of them are

00:16:28,580 --> 00:16:36,000
very expensive to get you can have like

00:16:33,900 --> 00:16:38,400
log back allows you to add the line

00:16:36,000 --> 00:16:41,520
number associated so basically you can

00:16:38,400 --> 00:16:43,680
trace the log statements to a line

00:16:41,520 --> 00:16:47,130
number in your source code which is very

00:16:43,680 --> 00:16:49,380
very cool and a lot of developers might

00:16:47,130 --> 00:16:50,670
want to have that because yeah I mean

00:16:49,380 --> 00:16:54,440
what's better than to debug in

00:16:50,670 --> 00:16:54,440
production but

00:16:55,860 --> 00:17:02,620
actually again there is a trade off like

00:16:59,230 --> 00:17:05,020
it's easier to debug afterwards but then

00:17:02,620 --> 00:17:07,959
getting this line number is a very

00:17:05,020 --> 00:17:10,600
expensive operation and then if you need

00:17:07,959 --> 00:17:15,850
to do it at every lock statement it will

00:17:10,600 --> 00:17:18,910
slow down your logs a lot so what we can

00:17:15,850 --> 00:17:21,309
do is you can still have the line number

00:17:18,910 --> 00:17:23,620
but you cannot you shouldn't compute it

00:17:21,309 --> 00:17:26,110
you you can write it explicitly in your

00:17:23,620 --> 00:17:28,179
log message and then we get back to the

00:17:26,110 --> 00:17:36,580
problem that it might give you wrong

00:17:28,179 --> 00:17:39,190
advice if you write it explicitly if you

00:17:36,580 --> 00:17:40,990
just have a bunch of logs especially

00:17:39,190 --> 00:17:43,480
nowadays where you might have like

00:17:40,990 --> 00:17:47,620
10,000 of instances it's completely

00:17:43,480 --> 00:17:50,170
useless it reminds me a story where I

00:17:47,620 --> 00:17:56,470
was working for a public administration

00:17:50,170 --> 00:17:58,840
and they had backup in place that the

00:17:56,470 --> 00:18:01,600
process to do backup the backup every

00:17:58,840 --> 00:18:05,850
week or every day once we needed to

00:18:01,600 --> 00:18:08,559
access the backup we couldn't restore it

00:18:05,850 --> 00:18:12,940
because they never tried that actually

00:18:08,559 --> 00:18:15,880
the restore process was working so if

00:18:12,940 --> 00:18:17,800
you just tore log somewhere it's fine

00:18:15,880 --> 00:18:20,530
but in general that's not very useful

00:18:17,800 --> 00:18:22,780
what you want to do is you want to

00:18:20,530 --> 00:18:25,300
aggregate the logs in like in a

00:18:22,780 --> 00:18:30,940
centralized place and you need to search

00:18:25,300 --> 00:18:32,679
for them you need to query them and you

00:18:30,940 --> 00:18:34,510
might know about elastic search you mind

00:18:32,679 --> 00:18:39,970
about splines you might know about grey

00:18:34,510 --> 00:18:41,820
loggers are all great products and in

00:18:39,970 --> 00:18:47,350
those products in general you add

00:18:41,820 --> 00:18:49,330
additional metadata the file because

00:18:47,350 --> 00:18:51,070
probably you don't have any single log

00:18:49,330 --> 00:18:53,169
file you have multiple log files might

00:18:51,070 --> 00:18:56,710
be like your web server file your

00:18:53,169 --> 00:18:58,750
applications have a file whatever the

00:18:56,710 --> 00:19:01,960
hosts lower comes from is super

00:18:58,750 --> 00:19:05,490
important and if you are using the cloud

00:19:01,960 --> 00:19:05,490
zone might be important

00:19:05,910 --> 00:19:13,060
also the the environments that you have

00:19:11,230 --> 00:19:15,190
like dedicated tags for each of your

00:19:13,060 --> 00:19:18,040
environments whether its development its

00:19:15,190 --> 00:19:24,600
staging its pre-production is important

00:19:18,040 --> 00:19:27,580
those are like required metadata and so

00:19:24,600 --> 00:19:31,210
when you agree jate those logs you might

00:19:27,580 --> 00:19:33,430
want to search with those metadata in

00:19:31,210 --> 00:19:40,210
mind like find me all the production

00:19:33,430 --> 00:19:45,960
logs that are like Java related and so

00:19:40,210 --> 00:19:50,530
comes the issue when do you actually

00:19:45,960 --> 00:19:53,080
index the logs because elasticsearch

00:19:50,530 --> 00:19:56,560
which I happen to be most familiar with

00:19:53,080 --> 00:19:58,720
is the no sequel data store and as you

00:19:56,560 --> 00:20:05,800
know no sequel data store they don't

00:19:58,720 --> 00:20:10,780
have any schema yeah yeah that was a

00:20:05,800 --> 00:20:13,960
joke sorry I was not clear enough of

00:20:10,780 --> 00:20:16,690
course every every data store has a

00:20:13,960 --> 00:20:18,850
schema the thing is the good side is

00:20:16,690 --> 00:20:20,830
that you don't need to like think about

00:20:18,850 --> 00:20:23,230
it beforehand you let you can let

00:20:20,830 --> 00:20:25,840
elasticsearch you send you documents and

00:20:23,230 --> 00:20:28,210
then by default it will try to be smart

00:20:25,840 --> 00:20:31,330
with the JSON and actually create a

00:20:28,210 --> 00:20:35,110
schema out of it or just like augment

00:20:31,330 --> 00:20:38,220
the existing schema and if you want to

00:20:35,110 --> 00:20:41,860
be very very fast then you just send

00:20:38,220 --> 00:20:44,080
like queries you don't think about the

00:20:41,860 --> 00:20:49,480
schema you let elasticsearch does this

00:20:44,080 --> 00:20:51,430
magical stuff and when you query it's

00:20:49,480 --> 00:20:54,430
going to take a lot of time because

00:20:51,430 --> 00:20:56,500
actually you don't know what the schema

00:20:54,430 --> 00:20:58,720
will be if it will be consistent or not

00:20:56,500 --> 00:21:02,710
and you will need to handle all that

00:20:58,720 --> 00:21:05,320
crap and if something that you want it

00:21:02,710 --> 00:21:08,490
to be structure is unstructured that

00:21:05,320 --> 00:21:13,180
mean that you will revert to do like

00:21:08,490 --> 00:21:16,060
select from like something which is like

00:21:13,180 --> 00:21:19,080
very very bad and that's the time when

00:21:16,060 --> 00:21:23,140
you will need to be fast

00:21:19,080 --> 00:21:25,750
so actually either you ingest your logs

00:21:23,140 --> 00:21:28,750
very very fast and then when you query

00:21:25,750 --> 00:21:33,070
it's going to be super slow or you

00:21:28,750 --> 00:21:36,160
accept that when you send the logs it's

00:21:33,070 --> 00:21:39,040
at the time that you index it's going to

00:21:36,160 --> 00:21:43,530
be slower but when you actually need to

00:21:39,040 --> 00:21:43,530
query then it's going to be much faster

00:21:44,400 --> 00:21:48,970
it can be seen as a trade off but to be

00:21:47,170 --> 00:21:54,900
honest in my opinion there is no real

00:21:48,970 --> 00:21:57,820
choice but I mean it's up to you and

00:21:54,900 --> 00:22:01,120
again I'm more familiar with the elastic

00:21:57,820 --> 00:22:04,210
stack so I will talk about the elastic

00:22:01,120 --> 00:22:08,650
stack architecture so you have file bits

00:22:04,210 --> 00:22:12,190
that like reads the log files and then

00:22:08,650 --> 00:22:15,130
sends it to log stash and lost log stash

00:22:12,190 --> 00:22:19,270
can already like try to process it and

00:22:15,130 --> 00:22:22,630
then send it into like a JSON format to

00:22:19,270 --> 00:22:27,480
elasticsearch who is familiar with

00:22:22,630 --> 00:22:30,760
elasticsearch well no okay that's fine

00:22:27,480 --> 00:22:32,890
so this is the general like architecture

00:22:30,760 --> 00:22:36,150
yeah you've seen that I am I was an

00:22:32,890 --> 00:22:39,760
architect I can do like UML diagrams so

00:22:36,150 --> 00:22:41,770
it's a good sign and everything

00:22:39,760 --> 00:22:43,480
everything here works because it's on

00:22:41,770 --> 00:22:47,830
the dash under the whiteboard that

00:22:43,480 --> 00:22:50,680
that's very fine really I'm not joking

00:22:47,830 --> 00:22:53,050
it works very well so first you have the

00:22:50,680 --> 00:22:55,300
the the the creation time so the

00:22:53,050 --> 00:22:57,880
application produces the log on the file

00:22:55,300 --> 00:23:00,640
and then you have the ingestion time

00:22:57,880 --> 00:23:02,740
where fire bit will read the file get

00:23:00,640 --> 00:23:04,660
the log and send the log to log stash

00:23:02,740 --> 00:23:06,910
and the still log stash will transform

00:23:04,660 --> 00:23:09,850
the log into JSON and then will say send

00:23:06,910 --> 00:23:15,250
JSON to elasticsearch so this is the

00:23:09,850 --> 00:23:19,150
general workflow who here has already

00:23:15,250 --> 00:23:21,970
been using log stash for real yeah again

00:23:19,150 --> 00:23:26,920
like more than half the room log stash

00:23:21,970 --> 00:23:28,990
leader is been in Ruby when you think

00:23:26,920 --> 00:23:31,300
about Ruby do you think this is the like

00:23:28,990 --> 00:23:33,230
the most performant part of your

00:23:31,300 --> 00:23:36,060
infrastructure

00:23:33,230 --> 00:23:43,020
I'm not a Reba developer so I troll a

00:23:36,060 --> 00:23:45,600
lot but it's not really wrong and so log

00:23:43,020 --> 00:23:47,670
stash is super good so you send it this

00:23:45,600 --> 00:23:49,800
kind of stuff then you have this like

00:23:47,670 --> 00:23:53,640
croc pattern and then it spits you out

00:23:49,800 --> 00:23:56,100
this J'son of course you you need to

00:23:53,640 --> 00:23:58,590
write the grog pattern and I mean rockin

00:23:56,100 --> 00:24:01,950
creating the grog pattern is is a skill

00:23:58,590 --> 00:24:05,100
in itself it is not super fun most of

00:24:01,950 --> 00:24:07,500
the time however once you get it it's

00:24:05,100 --> 00:24:11,010
it's quite convenient but the problem is

00:24:07,500 --> 00:24:12,960
it takes time and if you are using the

00:24:11,010 --> 00:24:15,270
elasticsearch the elastic stack in

00:24:12,960 --> 00:24:19,320
production then log stash might be one

00:24:15,270 --> 00:24:21,750
of like your congestion nodes then you

00:24:19,320 --> 00:24:25,650
need to scale it and it's not super

00:24:21,750 --> 00:24:28,470
great but when you think about it who

00:24:25,650 --> 00:24:30,780
told you that you need its log stash at

00:24:28,470 --> 00:24:33,990
all you need log stash to do the

00:24:30,780 --> 00:24:41,520
grokking for you but why can't you

00:24:33,990 --> 00:24:43,770
produce JSON directly actually it will

00:24:41,520 --> 00:24:46,650
make the log harder for me to read on

00:24:43,770 --> 00:24:48,810
the server but really are you really

00:24:46,650 --> 00:24:52,320
connecting to the server and reading the

00:24:48,810 --> 00:24:56,190
log there most people don't do that

00:24:52,320 --> 00:25:04,520
anymore so you can directly create the

00:24:56,190 --> 00:25:04,520
JSON yourself like that and guess what

00:25:13,200 --> 00:25:18,480
it's the same it's the same like stuff I

00:25:16,470 --> 00:25:22,049
didn't use a sink or whatever because

00:25:18,480 --> 00:25:25,020
it's just not my point here but I can

00:25:22,049 --> 00:25:33,929
weave the log statements directly after

00:25:25,020 --> 00:25:35,760
Jason it just needs a default like

00:25:33,929 --> 00:25:38,280
templating that is a bit different than

00:25:35,760 --> 00:25:40,559
one you expect but basically the pattern

00:25:38,280 --> 00:25:44,970
itself you can create it in JSON

00:25:40,559 --> 00:25:49,160
directly one of the gentlemen is super

00:25:44,970 --> 00:25:49,160
happy here now I'm afraid what did I say

00:25:56,720 --> 00:26:03,750
if you have some time I can check on the

00:26:00,720 --> 00:26:05,640
Internet and then check it how you can I

00:26:03,750 --> 00:26:07,799
can produce it but probably here it's

00:26:05,640 --> 00:26:11,250
because I use this pattern because the

00:26:07,799 --> 00:26:25,710
pattern I used again I used before not

00:26:11,250 --> 00:26:31,110
an easel timestamp believe me you can do

00:26:25,710 --> 00:26:33,750
whatever you want you can actually I

00:26:31,110 --> 00:26:37,230
have like I have the date and I have the

00:26:33,750 --> 00:26:41,610
time I separated them but actually it's

00:26:37,230 --> 00:26:44,910
because well the reason is I didn't want

00:26:41,610 --> 00:26:49,650
to I'm a dry kind of guys and don't

00:26:44,910 --> 00:26:53,280
repeat myself and actually yeah don't

00:26:49,650 --> 00:26:56,640
repeat yourself it's dry right it's

00:26:53,280 --> 00:27:00,480
because this is what is produced and

00:26:56,640 --> 00:27:02,760
what afterwards like logstash produces

00:27:00,480 --> 00:27:06,299
out of it so you have the date itself

00:27:02,760 --> 00:27:08,940
but then you can also have like again

00:27:06,299 --> 00:27:11,340
duplicated information that or index

00:27:08,940 --> 00:27:15,299
themself it depends you can choose a

00:27:11,340 --> 00:27:21,840
pattern that rocks your boat but it's a

00:27:15,299 --> 00:27:25,250
good question so now we are much faster

00:27:21,840 --> 00:27:26,830
because 5-bit is super super fast and

00:27:25,250 --> 00:27:30,630
elasticsearch can be

00:27:26,830 --> 00:27:33,789
scaled up to your heart's content and

00:27:30,630 --> 00:27:36,070
you don't have these like logstash stuff

00:27:33,789 --> 00:27:39,039
in between that just sits there to do

00:27:36,070 --> 00:27:44,080
the poor thing and that's pretty crazy

00:27:39,039 --> 00:27:47,289
you creates like a string and you

00:27:44,080 --> 00:27:54,100
transform it into Jason whereas you

00:27:47,289 --> 00:27:57,269
can't just create Jason there are two

00:27:54,100 --> 00:28:04,269
stuff that I also want to mentioned

00:27:57,269 --> 00:28:06,220
first is lock files really if you are

00:28:04,269 --> 00:28:09,639
using docker you know that you don't

00:28:06,220 --> 00:28:11,470
need to actually produce log files you

00:28:09,639 --> 00:28:16,570
can just change the driver to send

00:28:11,470 --> 00:28:19,860
events so what I told you about where

00:28:16,570 --> 00:28:22,179
you write your file or your log on

00:28:19,860 --> 00:28:27,039
doesn't apply anymore it's only

00:28:22,179 --> 00:28:29,440
networking and memory much faster but

00:28:27,039 --> 00:28:31,960
then if something happens then we get

00:28:29,440 --> 00:28:40,059
back to the problem that everything is

00:28:31,960 --> 00:28:45,309
lost second is I told you some metadata

00:28:40,059 --> 00:28:49,990
is hard to get and is expensive but when

00:28:45,309 --> 00:28:53,080
you miss it then you'll feel naked in

00:28:49,990 --> 00:28:57,399
that case I would advise you to think

00:28:53,080 --> 00:29:00,820
about like hot reloads for your logging

00:28:57,399 --> 00:29:03,639
framework because then by default it can

00:29:00,820 --> 00:29:07,720
be very fast and then you just change

00:29:03,639 --> 00:29:10,000
the configuration on the fly and because

00:29:07,720 --> 00:29:11,919
of the hot reloading feature then it

00:29:10,000 --> 00:29:14,139
will get slower but when you have a

00:29:11,919 --> 00:29:19,210
problem at least we you will have the

00:29:14,139 --> 00:29:22,110
Demeter data that you need those are

00:29:19,210 --> 00:29:22,110
things to consider

00:29:22,529 --> 00:29:31,720
so at exascale how do we log we don't

00:29:27,760 --> 00:29:35,110
use file beads we use syslog-ng and we

00:29:31,720 --> 00:29:39,360
have Kafka in the middle although we

00:29:35,110 --> 00:29:43,620
just spits out Jason this is a

00:29:39,360 --> 00:29:47,130
or architecture and the reason is why

00:29:43,620 --> 00:29:49,770
you just told us that is just like five

00:29:47,130 --> 00:29:51,799
beads elastics tag and then no logstash

00:29:49,770 --> 00:29:55,110
and then you put kafka in the middle

00:29:51,799 --> 00:29:57,240
true so the reason for that is we have

00:29:55,110 --> 00:30:00,170
syslog because basically when we started

00:29:57,240 --> 00:30:02,990
that then file bit was not that

00:30:00,170 --> 00:30:06,630
production ready

00:30:02,990 --> 00:30:08,220
so why chain I mean you you know if it

00:30:06,630 --> 00:30:12,030
works don't touch it

00:30:08,220 --> 00:30:16,470
so we still keep syslog-ng besides our I

00:30:12,030 --> 00:30:18,299
mean our InfraGard much more used to

00:30:16,470 --> 00:30:20,429
this kind of stuff so for them it's more

00:30:18,299 --> 00:30:24,720
natural than learning another tool file

00:30:20,429 --> 00:30:26,400
bit and then with send stuff to Kafka in

00:30:24,720 --> 00:30:29,240
between before sending them to

00:30:26,400 --> 00:30:32,040
elasticsearch we send them to Kafka and

00:30:29,240 --> 00:30:34,950
because Kafka is just a data store we

00:30:32,040 --> 00:30:37,440
need something to get the stuff out of

00:30:34,950 --> 00:30:40,080
Kafka and send it to elasticsearch and

00:30:37,440 --> 00:30:42,570
that is the job of locks - in that case

00:30:40,080 --> 00:30:47,130
locks - doesn't transform anything but

00:30:42,570 --> 00:30:50,220
it's just like you know any not an ETL

00:30:47,130 --> 00:30:52,919
because it doesn't transform but you

00:30:50,220 --> 00:30:55,169
just take and put take input so why do

00:30:52,919 --> 00:30:58,400
we send the stuff to Kafka we have two

00:30:55,169 --> 00:31:01,230
reasons for that so like historically

00:30:58,400 --> 00:31:03,990
elasticsearch was not like super

00:31:01,230 --> 00:31:07,620
reliable and sometimes you needed to do

00:31:03,990 --> 00:31:09,150
stuff like restart it and if you just

00:31:07,620 --> 00:31:10,890
send your stuff to alas this to

00:31:09,150 --> 00:31:13,290
elasticsearch during the time when when

00:31:10,890 --> 00:31:16,760
it reloads basically then all your age

00:31:13,290 --> 00:31:20,669
nodes they start buffering the logs and

00:31:16,760 --> 00:31:22,020
so your whole data center is starts

00:31:20,669 --> 00:31:24,690
getting trouble like the memory

00:31:22,020 --> 00:31:27,210
consumption like is huge and it's better

00:31:24,690 --> 00:31:30,330
to have this centralized stuff another

00:31:27,210 --> 00:31:32,790
reason for that is we are doing like

00:31:30,330 --> 00:31:35,780
complex event processing so basically

00:31:32,790 --> 00:31:39,090
the billing of the data center is made

00:31:35,780 --> 00:31:41,400
well the data must be centralized in

00:31:39,090 --> 00:31:43,230
Kafka so it's better to have like one

00:31:41,400 --> 00:31:46,080
single point where everything every

00:31:43,230 --> 00:31:48,059
event is aggregated and so we are just

00:31:46,080 --> 00:31:49,740
sending everything in Kafka even if it

00:31:48,059 --> 00:31:53,570
doesn't matter in the billing or in

00:31:49,740 --> 00:31:58,500
other like business decisions

00:31:53,570 --> 00:32:00,840
so this is a summary first if you are

00:31:58,500 --> 00:32:02,490
developer you don't pass the result of

00:32:00,840 --> 00:32:05,130
the computation to the logging framework

00:32:02,490 --> 00:32:07,110
you just passed the computation itself

00:32:05,130 --> 00:32:11,280
and then you have a centralized point

00:32:07,110 --> 00:32:13,830
where your ward with the log level the

00:32:11,280 --> 00:32:15,660
second is if you are an integrator if

00:32:13,830 --> 00:32:17,550
you are an infrastructure guy well you

00:32:15,660 --> 00:32:21,330
need to consider where you are longing

00:32:17,550 --> 00:32:26,490
to what like Vologda in the in the real

00:32:21,330 --> 00:32:30,600
world third is if speed is more

00:32:26,490 --> 00:32:34,650
important than reliability then consider

00:32:30,600 --> 00:32:39,390
going a synchronous and even in that

00:32:34,650 --> 00:32:41,250
case where like you can consider still

00:32:39,390 --> 00:32:42,990
reliability you can still go as

00:32:41,250 --> 00:32:46,740
synchronous and then configure your

00:32:42,990 --> 00:32:49,890
logging framework to skip events if they

00:32:46,740 --> 00:32:52,190
are like a certain level below a certain

00:32:49,890 --> 00:32:52,190
level

00:32:52,850 --> 00:32:58,770
don't use expensive metadata and that's

00:32:56,520 --> 00:33:01,679
every logging framework tell you this is

00:32:58,770 --> 00:33:04,260
expensive metadata to get so don't use

00:33:01,679 --> 00:33:06,690
them but in case you need them at some

00:33:04,260 --> 00:33:08,429
point then think about hot reloading

00:33:06,690 --> 00:33:10,350
again you lose a bit of performance

00:33:08,429 --> 00:33:12,360
because you have a thread that is runs

00:33:10,350 --> 00:33:15,330
that runs in the background that watches

00:33:12,360 --> 00:33:20,640
the configuration file but is just in

00:33:15,330 --> 00:33:23,280
case no alternative yes schema on right

00:33:20,640 --> 00:33:25,559
is slower but actually when you search

00:33:23,280 --> 00:33:28,350
which is basically what you really want

00:33:25,559 --> 00:33:30,330
it's going to be much faster so in that

00:33:28,350 --> 00:33:33,450
case that's not really the trade-off

00:33:30,330 --> 00:33:35,880
what if you really want to be fast then

00:33:33,450 --> 00:33:38,880
go that way but afterwards your search

00:33:35,880 --> 00:33:42,150
will be like crawling and then of course

00:33:38,880 --> 00:33:45,480
don't transform you logs just send the

00:33:42,150 --> 00:33:47,880
result that you want to whatever like if

00:33:45,480 --> 00:33:49,890
you are using elastic search then cengiz

00:33:47,880 --> 00:33:51,360
and directly or I don't know how ghrelin

00:33:49,890 --> 00:33:53,660
goes plunk work but probably they might

00:33:51,360 --> 00:33:57,920
be the same just send what you're

00:33:53,660 --> 00:33:57,920
centralized logging pattern expects

00:34:01,070 --> 00:34:05,730
might be stupid to say it but if you

00:34:03,600 --> 00:34:09,300
want very fast logs then it's everyone's

00:34:05,730 --> 00:34:10,980
responsibility the developer of course

00:34:09,300 --> 00:34:15,020
because it's the one producing the logs

00:34:10,980 --> 00:34:18,359
but also the ops guy the architects

00:34:15,020 --> 00:34:20,970
everybody must be involved that that's

00:34:18,359 --> 00:34:26,010
cross-cutting concern that crud that

00:34:20,970 --> 00:34:29,550
cuts across all organization and of

00:34:26,010 --> 00:34:32,070
course it depends you can be very very

00:34:29,550 --> 00:34:34,159
fast you know if you want to have to be

00:34:32,070 --> 00:34:39,830
very very fast you have no monitoring

00:34:34,159 --> 00:34:39,830
because it's just like slows you down

00:34:41,600 --> 00:34:47,940
really when I talks about some

00:34:45,060 --> 00:34:51,980
monitoring tools the question was but

00:34:47,940 --> 00:34:54,750
how less performance will my system be

00:34:51,980 --> 00:34:56,850
so the answer is is it better to get

00:34:54,750 --> 00:35:00,930
false blindfolded or to just slow down a

00:34:56,850 --> 00:35:03,390
bit and to know where you drive so the

00:35:00,930 --> 00:35:07,470
speed in itself is not very important

00:35:03,390 --> 00:35:09,600
the question is what do you want do you

00:35:07,470 --> 00:35:12,570
want a reliable system if you are

00:35:09,600 --> 00:35:16,160
sending logs to build your customer your

00:35:12,570 --> 00:35:18,780
must be pretty sure of those are like

00:35:16,160 --> 00:35:22,260
transactional and you cannot just send

00:35:18,780 --> 00:35:26,730
like that's Billings to your customer in

00:35:22,260 --> 00:35:29,010
one way or another it's not good so of

00:35:26,730 --> 00:35:32,250
course you want to be fast but consider

00:35:29,010 --> 00:35:37,470
what trade-offs you need to to trade

00:35:32,250 --> 00:35:40,140
them for so thanks for your attention

00:35:37,470 --> 00:35:43,320
you can read more about my blog which is

00:35:40,140 --> 00:35:46,109
more like programming stuff the DevOps

00:35:43,320 --> 00:35:48,750
stuff is written on my company's blog

00:35:46,109 --> 00:35:52,790
and you can follow me on Twitter and now

00:35:48,750 --> 00:35:52,790
I guess we have some time for questions

00:35:55,330 --> 00:36:10,120
thank you yeah questions thank you hi

00:36:04,960 --> 00:36:13,630
sorry the last slide with your whole

00:36:10,120 --> 00:36:15,250
pipeline you had Kafka going into

00:36:13,630 --> 00:36:18,460
elasticsearch did you say that there's

00:36:15,250 --> 00:36:21,700
logstash in between or look actually

00:36:18,460 --> 00:36:24,610
it's a it's a UML diagram sequence so it

00:36:21,700 --> 00:36:27,400
must always be on left to right the the

00:36:24,610 --> 00:36:30,190
arrow must be on left to right so yes I

00:36:27,400 --> 00:36:32,410
have lock stash in between but it must

00:36:30,190 --> 00:36:34,720
sit there because log stash will read

00:36:32,410 --> 00:36:41,670
stuff from Kafka and send it to

00:36:34,720 --> 00:36:41,670
elasticsearch right

00:36:42,070 --> 00:36:54,670
I do okay then follow up in your

00:36:51,490 --> 00:36:58,530
experience which is the most fragile

00:36:54,670 --> 00:37:02,860
part of this like what is has failed

00:36:58,530 --> 00:37:05,560
most often well regarding the context of

00:37:02,860 --> 00:37:08,650
exascale this was elasticsearch that was

00:37:05,560 --> 00:37:10,690
failing most of the time that Putin

00:37:08,650 --> 00:37:14,430
Kafka in the middle actually increased

00:37:10,690 --> 00:37:14,430
the reliability of the whole system

00:37:14,640 --> 00:37:22,330
and it was the restarts or what cost

00:37:19,810 --> 00:37:24,670
elasticsearch that you know that

00:37:22,330 --> 00:37:26,080
elasticsearch is based on the gvm and at

00:37:24,670 --> 00:37:28,000
some points I don't know if it was

00:37:26,080 --> 00:37:29,350
misconfigure or whatever but we had like

00:37:28,000 --> 00:37:32,470
stop the words events

00:37:29,350 --> 00:37:34,690
so basically when elasticsearch like

00:37:32,470 --> 00:37:37,210
tries to do like the garbage collection

00:37:34,690 --> 00:37:39,280
it might run for like 10 seconds 20

00:37:37,210 --> 00:37:41,560
seconds whatever and again when you have

00:37:39,280 --> 00:37:44,830
in this state then file bit cannot send

00:37:41,560 --> 00:37:47,950
the events then the load on every h node

00:37:44,830 --> 00:37:49,600
increasing and bla bla bla so Kafka in

00:37:47,950 --> 00:37:55,890
that case helped us a lot

00:37:49,600 --> 00:37:55,890
coop thanks thanks more questions

00:37:57,810 --> 00:38:02,740
when there are no questions that means

00:38:01,060 --> 00:38:04,690
either two things I've been extremely

00:38:02,740 --> 00:38:08,290
clear that was the best presentation

00:38:04,690 --> 00:38:10,570
ever or it's the crap of the day and you

00:38:08,290 --> 00:38:15,880
will always remembered it as well so I

00:38:10,570 --> 00:38:17,740
may be afraid of the second option it

00:38:15,880 --> 00:38:21,390
was the first time I did this talk I

00:38:17,740 --> 00:38:24,070
would be very very grateful if you like

00:38:21,390 --> 00:38:26,650
sends me or I don't know if there will

00:38:24,070 --> 00:38:30,040
be feedback forms or whatever yeah we'll

00:38:26,650 --> 00:38:32,170
be okay if you if you like not only

00:38:30,040 --> 00:38:34,270
great at me because grades especially if

00:38:32,170 --> 00:38:36,580
it's a good grade it's always nice but

00:38:34,270 --> 00:38:38,800
especially if you like wrote in the

00:38:36,580 --> 00:38:40,900
comments what stuff we're not clear what

00:38:38,800 --> 00:38:43,090
you you dislike what could be improved

00:38:40,900 --> 00:38:46,170
because then from the next session I

00:38:43,090 --> 00:38:49,140
could perhaps improve a bit

00:38:46,170 --> 00:38:54,730
thank you very much then thanks again

00:38:49,140 --> 00:38:54,730
[Applause]

00:38:55,940 --> 00:39:01,930
[Music]

00:38:59,869 --> 00:39:01,930

YouTube URL: https://www.youtube.com/watch?v=xzu245IqDbA


