Title: OSDC 2019: Tick Tock: What the heck is time-series data? by Tanay Pant
Publication date: 2019-05-21
Playlist: OSDC 2019 | Open Source Data Center Conference
Description: 
	The rise of IoT and smart infrastructure has led to the generation of massive amounts of complex data. In this session, we will talk about time-series data, the challenges of working with time series data, ingestion of this data using data from NYC cabs and running real time queries to gather insights. By the end of the session, we will have an understanding of what time-series data is, how to build streaming data pipelines for massive time series data using Flink, Kafka and CrateDB, and visualising all this data with the help of a dashboard.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:01,140 --> 00:00:10,550
[Music]

00:00:15,020 --> 00:00:21,930
Thanks hello everyone

00:00:18,119 --> 00:00:28,019
guten tag how are all of you doing good

00:00:21,930 --> 00:00:29,730
and are you enjoying or SDC cool thank

00:00:28,019 --> 00:00:31,800
you very much for attending my talk on

00:00:29,730 --> 00:00:34,140
tick-tock what the heck is time series

00:00:31,800 --> 00:00:36,210
data before we go any further I would

00:00:34,140 --> 00:00:38,129
like to know the demographics of this

00:00:36,210 --> 00:00:43,320
room a little bit so how many of you are

00:00:38,129 --> 00:00:45,360
working as back end engineers okay how

00:00:43,320 --> 00:00:50,460
many of you are working as data

00:00:45,360 --> 00:00:53,010
engineers data scientists perhaps okay

00:00:50,460 --> 00:00:56,610
and how many of you have worked with

00:00:53,010 --> 00:01:05,760
databases before okay most most of you

00:00:56,610 --> 00:01:07,409
that's good which ones what are the

00:01:05,760 --> 00:01:20,250
databases have you guys worked with

00:01:07,409 --> 00:01:22,710
before good okay today we will talk

00:01:20,250 --> 00:01:25,799
about a couple of topics which include

00:01:22,710 --> 00:01:27,600
time series data what it is and how its

00:01:25,799 --> 00:01:30,869
workload differs from other forms of

00:01:27,600 --> 00:01:33,060
data different use cases where time

00:01:30,869 --> 00:01:36,090
series data is being used widely these

00:01:33,060 --> 00:01:38,759
days then we have how crazy B helps in

00:01:36,090 --> 00:01:40,619
dealing with machine data and what are

00:01:38,759 --> 00:01:43,350
the technical aspects that make it so

00:01:40,619 --> 00:01:45,149
efficient then we are going to shortly

00:01:43,350 --> 00:01:47,040
talk about creative e running on

00:01:45,149 --> 00:01:49,950
Microsoft Azure and the different things

00:01:47,040 --> 00:01:52,500
that you can do with it and finally a

00:01:49,950 --> 00:01:54,479
case study about a smart factory that we

00:01:52,500 --> 00:01:56,670
have been working with over the past few

00:01:54,479 --> 00:02:00,060
years and then we'll create some machine

00:01:56,670 --> 00:02:02,100
data my name is Stan upon't and I

00:02:00,060 --> 00:02:04,590
currently work as a developer advocate

00:02:02,100 --> 00:02:06,869
at create i/o I love working with time

00:02:04,590 --> 00:02:09,599
series data especially machine and

00:02:06,869 --> 00:02:11,310
sensor data I'm a tech speaker for

00:02:09,599 --> 00:02:13,290
Mozilla and volunteer to their various

00:02:11,310 --> 00:02:13,830
projects in my spare time I have

00:02:13,290 --> 00:02:16,290
published

00:02:13,830 --> 00:02:18,060
on Firefox OS building a virtual

00:02:16,290 --> 00:02:20,700
assistant for Raspberry Pi and web-based

00:02:18,060 --> 00:02:22,470
virtual reality I'm also an intern

00:02:20,700 --> 00:02:24,840
software innovator in the field of IOT

00:02:22,470 --> 00:02:26,490
and towards this end of this

00:02:24,840 --> 00:02:28,650
presentation I will share my Twitter

00:02:26,490 --> 00:02:30,810
handle and my email so if you have any

00:02:28,650 --> 00:02:32,760
questions even after the event or if you

00:02:30,810 --> 00:02:35,790
have any interesting project ideas I'd

00:02:32,760 --> 00:02:37,830
be happy to collaborate so let's get

00:02:35,790 --> 00:02:40,670
started that's the million dollar

00:02:37,830 --> 00:02:43,740
question what is time series data and

00:02:40,670 --> 00:02:46,560
most importantly how does it differ from

00:02:43,740 --> 00:02:49,830
other forms of data to answer this

00:02:46,560 --> 00:02:52,800
question imagine a sensor and this

00:02:49,830 --> 00:02:55,980
sensor is sending streams of data over a

00:02:52,800 --> 00:02:58,170
period of time if you were to plot this

00:02:55,980 --> 00:03:01,410
data then one of this axis in the

00:02:58,170 --> 00:03:03,840
visualization would be time and as

00:03:01,410 --> 00:03:06,270
compared to other workloads this data is

00:03:03,840 --> 00:03:09,660
not usually added as an update to the

00:03:06,270 --> 00:03:11,760
database time series data is added as an

00:03:09,660 --> 00:03:14,870
insert and this is the primary way of

00:03:11,760 --> 00:03:17,340
adding it to a time series database and

00:03:14,870 --> 00:03:19,560
time series databases basically

00:03:17,340 --> 00:03:22,050
introduce efficiencies by treating time

00:03:19,560 --> 00:03:24,840
as a first-class citizen and this allows

00:03:22,050 --> 00:03:27,090
you to intuitive into all the aspects of

00:03:24,840 --> 00:03:29,550
your operation having this rich amount

00:03:27,090 --> 00:03:32,040
of historical data such as analyzing the

00:03:29,550 --> 00:03:35,580
past monitoring the present and also

00:03:32,040 --> 00:03:38,220
predicting the future now that we have

00:03:35,580 --> 00:03:40,830
an idea of what time series data is what

00:03:38,220 --> 00:03:44,100
it looks like let's play a game that I

00:03:40,830 --> 00:03:46,140
like to call time series or not so does

00:03:44,100 --> 00:03:52,080
the data visualized by this graph do you

00:03:46,140 --> 00:03:57,950
know what time series data yes no no

00:03:52,080 --> 00:04:05,310
correct awesome time series or not yes

00:03:57,950 --> 00:04:07,160
one last one time series or not yes now

00:04:05,310 --> 00:04:09,930
if you take an abstract look at

00:04:07,160 --> 00:04:12,060
different use cases of time series and

00:04:09,930 --> 00:04:14,090
the kind of data generated you can

00:04:12,060 --> 00:04:17,220
categorize them roughly into two types

00:04:14,090 --> 00:04:19,320
so first of all we have IT and system

00:04:17,220 --> 00:04:21,000
monitoring and this can be termed as the

00:04:19,320 --> 00:04:23,700
traditional use case of Time series

00:04:21,000 --> 00:04:25,890
databases so if you look at its

00:04:23,700 --> 00:04:27,540
characteristics you can say that it has

00:04:25,890 --> 00:04:30,480
tens of hundreds

00:04:27,540 --> 00:04:32,880
of metrics or sensors it has real-time

00:04:30,480 --> 00:04:35,460
write functionality reality of complex

00:04:32,880 --> 00:04:38,580
queries and data volumes that often fall

00:04:35,460 --> 00:04:41,760
in gigabytes so in flux DB is a really

00:04:38,580 --> 00:04:44,880
good example for this kind of therefore

00:04:41,760 --> 00:04:47,460
database in this category and then we

00:04:44,880 --> 00:04:49,410
have industrial sensor data and this is

00:04:47,460 --> 00:04:51,480
an emerging sector that I feel has not

00:04:49,410 --> 00:04:53,610
been talked about enough so this is

00:04:51,480 --> 00:04:55,920
usually characterized through hundreds

00:04:53,610 --> 00:04:58,380
of thousands of sensors or metrics and

00:04:55,920 --> 00:05:00,270
real-time queries that are under highly

00:04:58,380 --> 00:05:03,210
concurrent load and mass access

00:05:00,270 --> 00:05:05,910
gigabytes and to up to hundreds of

00:05:03,210 --> 00:05:07,910
terabytes of data and creed DB is an

00:05:05,910 --> 00:05:09,840
especially good fit for this category

00:05:07,910 --> 00:05:13,050
there are some other characteristics

00:05:09,840 --> 00:05:15,300
that separate the two use cases like

00:05:13,050 --> 00:05:17,730
projects that fall under the last

00:05:15,300 --> 00:05:20,220
category tend to be internal IT projects

00:05:17,730 --> 00:05:22,950
and the data and the database itself is

00:05:20,220 --> 00:05:24,330
often not that mission-critical and the

00:05:22,950 --> 00:05:26,430
projects that fall in the latter

00:05:24,330 --> 00:05:28,740
category are often mission-critical

00:05:26,430 --> 00:05:31,670
they are transformative and enabled

00:05:28,740 --> 00:05:34,740
real-time data driven decision systems

00:05:31,670 --> 00:05:38,820
have anybody used create DB before or

00:05:34,740 --> 00:05:41,370
maybe heard of it no no one okay so

00:05:38,820 --> 00:05:43,620
let's start off by going into the heart

00:05:41,370 --> 00:05:46,500
of the honeycomb to the core technology

00:05:43,620 --> 00:05:48,480
and talk about what cray DB is and what

00:05:46,500 --> 00:05:51,300
makes it stand apart from the various of

00:05:48,480 --> 00:05:53,400
the databases in its segment so cray DB

00:05:51,300 --> 00:05:55,170
is a new kind of distributed sequel

00:05:53,400 --> 00:05:57,990
database that is best suited for

00:05:55,170 --> 00:06:00,510
handling industrial sensor data due to

00:05:57,990 --> 00:06:02,910
its ease of use and ability to work with

00:06:00,510 --> 00:06:04,380
many different terabytes of data with

00:06:02,910 --> 00:06:07,350
thousands of different sensor data

00:06:04,380 --> 00:06:10,410
structures so crazy be uniquely combines

00:06:07,350 --> 00:06:11,820
sequel with no sequel benefits and it

00:06:10,410 --> 00:06:14,340
operates on a shared nothing

00:06:11,820 --> 00:06:16,610
architecture credibly supports

00:06:14,340 --> 00:06:21,000
distributed sequel with full-text search

00:06:16,610 --> 00:06:24,240
geospatial queries and also aggregations

00:06:21,000 --> 00:06:25,890
and various nodes in a create DB cluster

00:06:24,240 --> 00:06:29,340
coordinate seamlessly with each other

00:06:25,890 --> 00:06:31,680
and the execution of write and query

00:06:29,340 --> 00:06:35,340
operations are automatically distributed

00:06:31,680 --> 00:06:37,760
throughout the nodes in the cluster so

00:06:35,340 --> 00:06:40,050
crazy be has columnar caches for

00:06:37,760 --> 00:06:41,190
real-time in-memory sequel query

00:06:40,050 --> 00:06:43,380
performance

00:06:41,190 --> 00:06:45,870
so real-time databases usually require

00:06:43,380 --> 00:06:48,120
all the data to fit in the main memory

00:06:45,870 --> 00:06:50,370
but that limits how much data you can

00:06:48,120 --> 00:06:53,160
manage our solution for real time

00:06:50,370 --> 00:06:55,680
performance is without data volume

00:06:53,160 --> 00:06:58,020
limitations is to implement memory

00:06:55,680 --> 00:07:01,170
resident columnar filled caches on each

00:06:58,020 --> 00:07:03,540
node so the caches tell the query engine

00:07:01,170 --> 00:07:06,720
whether there are records on that node

00:07:03,540 --> 00:07:09,030
that meet the query criteria and where

00:07:06,720 --> 00:07:11,210
the records are located so this is all

00:07:09,030 --> 00:07:13,770
performed at in-memory speed and

00:07:11,210 --> 00:07:16,350
distributed query before distributing

00:07:13,770 --> 00:07:18,690
query processing also contributes to

00:07:16,350 --> 00:07:20,940
fast performance and a query planner

00:07:18,690 --> 00:07:23,490
that makes really smart decisions about

00:07:20,940 --> 00:07:26,820
which nodes are best suited to perform

00:07:23,490 --> 00:07:29,160
different aggregations and joints and it

00:07:26,820 --> 00:07:31,530
has machine data specific features with

00:07:29,160 --> 00:07:34,770
a cloud native architecture which allows

00:07:31,530 --> 00:07:39,210
it to run seamlessly on cloud on the

00:07:34,770 --> 00:07:40,590
edge or on-premise let's take a quick

00:07:39,210 --> 00:07:44,460
look at some of the salient features

00:07:40,590 --> 00:07:46,200
which may create dB what it is like so

00:07:44,460 --> 00:07:48,360
create DB is quite simple to install

00:07:46,200 --> 00:07:50,640
you can spawn an instance of creativi

00:07:48,360 --> 00:07:54,090
with a single line on your terminal or

00:07:50,640 --> 00:07:56,700
on docker create DB is in a size equal

00:07:54,090 --> 00:07:58,950
compatible including joins and Postgres

00:07:56,700 --> 00:08:00,810
via which makes integration with tools

00:07:58,950 --> 00:08:02,970
such as graph on are really easy with

00:08:00,810 --> 00:08:06,270
the post classifier and this also

00:08:02,970 --> 00:08:08,250
prevents log in it has a distributed

00:08:06,270 --> 00:08:11,130
query engine which supports full text

00:08:08,250 --> 00:08:13,200
queries and is real-time and it supports

00:08:11,130 --> 00:08:14,490
all kind of data including structured

00:08:13,200 --> 00:08:17,310
data Jason

00:08:14,490 --> 00:08:20,940
time series data geospatial data and

00:08:17,310 --> 00:08:23,220
even blob tables it runs on commodity

00:08:20,940 --> 00:08:25,530
hardware and economical instances and

00:08:23,220 --> 00:08:27,440
it's really easy to scale out this

00:08:25,530 --> 00:08:29,580
architecture is really well suited to

00:08:27,440 --> 00:08:31,590
containerization meaning a cluster

00:08:29,580 --> 00:08:34,650
running on kubernetes can be scaled up

00:08:31,590 --> 00:08:36,419
or down just using a cube CTL scale come

00:08:34,650 --> 00:08:39,570
on so scale up operations can take

00:08:36,419 --> 00:08:42,000
minutes instead of ours shutting

00:08:39,570 --> 00:08:44,460
replication and rebalancing of data as

00:08:42,000 --> 00:08:46,830
the cluster size changes is automated

00:08:44,460 --> 00:08:48,780
and finally these features prevent

00:08:46,830 --> 00:08:51,000
lock-in and allow you to interface with

00:08:48,780 --> 00:08:54,900
your favorite tools running wherever you

00:08:51,000 --> 00:08:57,360
prefer so our motto is put

00:08:54,900 --> 00:08:59,400
shindaita to work and everyday millions

00:08:57,360 --> 00:09:01,530
and billions of data points are

00:08:59,400 --> 00:09:03,990
generated by sensors smart factories

00:09:01,530 --> 00:09:06,660
health industry security applications

00:09:03,990 --> 00:09:09,090
and so on and this data needs to be

00:09:06,660 --> 00:09:11,160
ingested in real time and this data

00:09:09,090 --> 00:09:14,190
tends to be complex and can be in many

00:09:11,160 --> 00:09:16,500
different structures and forms this data

00:09:14,190 --> 00:09:18,720
has valuable insights about the

00:09:16,500 --> 00:09:21,210
operation which can be obtained from it

00:09:18,720 --> 00:09:24,450
by performing analytics on the data in

00:09:21,210 --> 00:09:27,060
real time queries like aggregation and

00:09:24,450 --> 00:09:29,130
time series analysis along with machine

00:09:27,060 --> 00:09:31,710
learning or predictive analytical models

00:09:29,130 --> 00:09:33,750
are often applied on this data and the

00:09:31,710 --> 00:09:36,420
insights produced by these analytical

00:09:33,750 --> 00:09:38,430
operations can be used to take action to

00:09:36,420 --> 00:09:40,650
improve the efficiency and functioning

00:09:38,430 --> 00:09:42,620
of these various industries hence

00:09:40,650 --> 00:09:45,300
causing significant cost reductions so

00:09:42,620 --> 00:09:46,950
the differentiating factor that should

00:09:45,300 --> 00:09:49,200
be kept in mind as compared to other

00:09:46,950 --> 00:09:50,910
time series use cases is that these

00:09:49,200 --> 00:09:53,160
real-time queries are under highly

00:09:50,910 --> 00:09:55,230
concurrent load for stream processing or

00:09:53,160 --> 00:09:57,450
machine learning and must access

00:09:55,230 --> 00:10:01,260
gigabytes to hundreds of terabytes of

00:09:57,450 --> 00:10:03,680
data and it's a machine data world and

00:10:01,260 --> 00:10:06,930
machine data means explosive growth in

00:10:03,680 --> 00:10:09,720
2018 we saw a boom in the number of IOT

00:10:06,930 --> 00:10:12,120
devices and their use cases there was an

00:10:09,720 --> 00:10:14,100
increase in the use of IOT and sensors

00:10:12,120 --> 00:10:16,950
in different contexts such as smart

00:10:14,100 --> 00:10:19,050
factories and smart cities and the total

00:10:16,950 --> 00:10:21,960
number of IOT devices is moving towards

00:10:19,050 --> 00:10:25,080
20 billion by the year 2020 and every

00:10:21,960 --> 00:10:27,270
second another 127 new devices are being

00:10:25,080 --> 00:10:29,850
connected to the Internet as an example

00:10:27,270 --> 00:10:31,860
take a Boeing 787 which generates about

00:10:29,850 --> 00:10:34,860
half a terabyte of data during a single

00:10:31,860 --> 00:10:36,390
flight and if we talk about the plastic

00:10:34,860 --> 00:10:39,000
manufacturing industry for example

00:10:36,390 --> 00:10:41,430
there's a wide array of data being

00:10:39,000 --> 00:10:43,920
produced per bottle and then there's

00:10:41,430 --> 00:10:46,530
data per production line with about 10

00:10:43,920 --> 00:10:48,620
production lines per plant and this

00:10:46,530 --> 00:10:51,570
chair data is being generated by around

00:10:48,620 --> 00:10:53,850
950 sensor types and that's just one

00:10:51,570 --> 00:10:56,670
factory and this particular company that

00:10:53,850 --> 00:10:59,940
I'm talking about has 180 factories in

00:10:56,670 --> 00:11:02,940
around 45 countries that means data on

00:10:59,940 --> 00:11:05,370
billions of bottles so we will talk more

00:11:02,940 --> 00:11:07,020
about this company in a while and look

00:11:05,370 --> 00:11:08,790
take a look at the data generated by it

00:11:07,020 --> 00:11:11,700
so crazy bee base

00:11:08,790 --> 00:11:14,190
excels at handling velocity volume and

00:11:11,700 --> 00:11:17,130
diversity of huge industrial time-series

00:11:14,190 --> 00:11:19,530
use cases for example industrial sensors

00:11:17,130 --> 00:11:21,630
often increase the frequency of their

00:11:19,530 --> 00:11:24,420
measurements when values exceed the

00:11:21,630 --> 00:11:27,000
expected thresholds and with multiple

00:11:24,420 --> 00:11:29,820
sensors cascading failures can cause in

00:11:27,000 --> 00:11:32,390
huge data injection spikes and kratom is

00:11:29,820 --> 00:11:34,280
able to handle this these spikes without

00:11:32,390 --> 00:11:36,990
sacrificing any query performance

00:11:34,280 --> 00:11:38,820
meaning that your reporting and analysis

00:11:36,990 --> 00:11:40,100
tools won't fail just when you need them

00:11:38,820 --> 00:11:43,470
the most

00:11:40,100 --> 00:11:45,900
this for host of complex data needs to

00:11:43,470 --> 00:11:48,720
be handled in real time let's talk about

00:11:45,900 --> 00:11:50,160
the next layer of the honeycomb that is

00:11:48,720 --> 00:11:52,920
built on the core credible technology

00:11:50,160 --> 00:11:56,190
and is called create a big cloud also

00:11:52,920 --> 00:12:00,180
known as cray DB on Microsoft Azure NES

00:11:56,190 --> 00:12:04,530
your users here by any chance just -

00:12:00,180 --> 00:12:06,660
okay crazy McCloud is a scalable sequel

00:12:04,530 --> 00:12:10,170
service hosted on Azure and operated

00:12:06,660 --> 00:12:11,910
24/7 by experts at Crate IO it is ideal

00:12:10,170 --> 00:12:14,580
for industrial time series data

00:12:11,910 --> 00:12:17,280
processing and other IOT and machine

00:12:14,580 --> 00:12:19,320
data specific workloads so some of the

00:12:17,280 --> 00:12:21,630
salient features of trade DB clowder

00:12:19,320 --> 00:12:23,400
it has real-time query performance for

00:12:21,630 --> 00:12:26,610
partitioning parallel processing

00:12:23,400 --> 00:12:29,790
in-memory columnar indexes which enabled

00:12:26,610 --> 00:12:32,490
real time complex analytics and AI not

00:12:29,790 --> 00:12:35,430
just time series aggregates and it has

00:12:32,490 --> 00:12:38,310
really good integration with as your IOT

00:12:35,430 --> 00:12:39,270
ecosystem with built-in interfaces do as

00:12:38,310 --> 00:12:43,170
your IOT hub

00:12:39,270 --> 00:12:46,410
mqtt Postgres fire protocol JDBC rest

00:12:43,170 --> 00:12:48,540
and a native dotnet core driver and it

00:12:46,410 --> 00:12:50,730
is a fully managed cloud service which

00:12:48,540 --> 00:12:53,400
is secured scaled from the silicon up

00:12:50,730 --> 00:12:57,330
and operated 24/7 by create IO on

00:12:53,400 --> 00:12:59,220
Microsoft Azure now as promised I would

00:12:57,330 --> 00:13:02,220
like to share with you a story of one of

00:12:59,220 --> 00:13:04,280
our customers named al plow have anybody

00:13:02,220 --> 00:13:07,530
of you heard of them before

00:13:04,280 --> 00:13:09,690
no ok let me introduce you to them so

00:13:07,530 --> 00:13:11,430
most of the people in this world have

00:13:09,690 --> 00:13:12,180
their products in their hand at least

00:13:11,430 --> 00:13:15,450
once a week

00:13:12,180 --> 00:13:18,120
be it coca-cola Head & Shoulders or dove

00:13:15,450 --> 00:13:20,190
al plow is really well known in the

00:13:18,120 --> 00:13:22,559
consumer packaged goods market for its

00:13:20,190 --> 00:13:25,409
innovative packaging system what

00:13:22,559 --> 00:13:26,819
closures and injection-molded butts and

00:13:25,409 --> 00:13:29,669
we have been working with them over the

00:13:26,819 --> 00:13:32,309
past three years they have sensors for

00:13:29,669 --> 00:13:34,349
temperature pressure blending ratios and

00:13:32,309 --> 00:13:36,629
other settings that need to be really

00:13:34,349 --> 00:13:38,729
precise on every machine and when these

00:13:36,629 --> 00:13:39,479
settings are right high quality products

00:13:38,729 --> 00:13:41,669
are the result

00:13:39,479 --> 00:13:44,429
and when these settings are off quality

00:13:41,669 --> 00:13:47,129
suffers and production costs increase in

00:13:44,429 --> 00:13:48,959
the past discovering issues involve

00:13:47,129 --> 00:13:51,419
multiple people to be continuously

00:13:48,959 --> 00:13:53,609
moving around the factory floor checking

00:13:51,419 --> 00:13:55,379
up on the LED machines on each LED

00:13:53,609 --> 00:13:58,289
lights on each machine to see if it's

00:13:55,379 --> 00:14:00,419
blinking or not and as production lines

00:13:58,289 --> 00:14:02,459
are laid over the entire factory floor

00:14:00,419 --> 00:14:06,509
and these factories do tend to be quite

00:14:02,459 --> 00:14:08,729
large in size distances between machines

00:14:06,509 --> 00:14:10,759
can be considerable meaning that this

00:14:08,729 --> 00:14:13,979
was really slow and labor-intensive

00:14:10,759 --> 00:14:16,019
operation additionally the latency

00:14:13,979 --> 00:14:18,329
introduced by having to walk around that

00:14:16,019 --> 00:14:20,669
factory floor meant that products were

00:14:18,329 --> 00:14:23,159
often affected by a defect even before

00:14:20,669 --> 00:14:24,899
the error was spotted by the operator so

00:14:23,159 --> 00:14:28,049
this resulted in loss of valuable

00:14:24,899 --> 00:14:30,119
machine time and also loss of raw

00:14:28,049 --> 00:14:32,309
material as defective material gets

00:14:30,119 --> 00:14:36,029
produced unless the problems are fixed

00:14:32,309 --> 00:14:37,859
so I'll plus market entry in 2001 was

00:14:36,029 --> 00:14:42,269
followed by years of tremendous growth

00:14:37,859 --> 00:14:44,339
in 2015 they were already operating 14

00:14:42,269 --> 00:14:47,009
manufacturing plants with more than

00:14:44,339 --> 00:14:49,019
thousand employees in USA so this with

00:14:47,009 --> 00:14:51,539
this growth came the challenge of hiring

00:14:49,019 --> 00:14:53,039
and training enough personnel to run the

00:14:51,539 --> 00:14:55,619
different complex manufacturing

00:14:53,039 --> 00:14:57,479
processes and because modern

00:14:55,619 --> 00:14:59,519
manufacturing machines each have their

00:14:57,479 --> 00:15:02,299
own different complex interfaces

00:14:59,519 --> 00:15:05,339
every plant required specially trained

00:15:02,299 --> 00:15:09,089
experts for every machine on every shift

00:15:05,339 --> 00:15:11,970
and as a result of rapid growth database

00:15:09,089 --> 00:15:14,789
performance Rd created and response time

00:15:11,970 --> 00:15:17,039
for queries got longer and any analytics

00:15:14,789 --> 00:15:19,469
work on the data was taking more and

00:15:17,039 --> 00:15:21,629
more time real-time actionable guidance

00:15:19,469 --> 00:15:24,629
on the production flow was no longer

00:15:21,629 --> 00:15:27,029
possible so create i/o build a proof of

00:15:24,629 --> 00:15:29,189
concept on the top of cray DB cloud and

00:15:27,029 --> 00:15:31,499
enable them to drive technological

00:15:29,189 --> 00:15:33,629
transformations so I would like to share

00:15:31,499 --> 00:15:35,669
with you now how the machine data from

00:15:33,629 --> 00:15:36,240
ultra looks like and then we'll do some

00:15:35,669 --> 00:15:52,320
queries

00:15:36,240 --> 00:15:59,339
over the machine data is it visible in

00:15:52,320 --> 00:16:02,370
the back ok so this is what admin UI of

00:15:59,339 --> 00:16:03,750
cray DB looks like so this is the front

00:16:02,370 --> 00:16:06,810
page and you can see basic information

00:16:03,750 --> 00:16:09,630
such as health of the cluster replicated

00:16:06,810 --> 00:16:11,399
data how much data is available how many

00:16:09,630 --> 00:16:14,010
records are there in the database so we

00:16:11,399 --> 00:16:15,930
have 2.7 billion records here and how

00:16:14,010 --> 00:16:19,410
many nodes are present in this cluster

00:16:15,930 --> 00:16:21,149
so we have a 5 node cluster here and we

00:16:19,410 --> 00:16:23,670
have different panels for different

00:16:21,149 --> 00:16:26,640
functions so we have a console planner

00:16:23,670 --> 00:16:30,660
here for entering our queries on the

00:16:26,640 --> 00:16:32,250
cluster and we have here the tables

00:16:30,660 --> 00:16:34,770
section which basically shows you the

00:16:32,250 --> 00:16:37,800
different tables that are present so

00:16:34,770 --> 00:16:40,020
let's take a look at this and understand

00:16:37,800 --> 00:16:42,720
how this whole operation and of the Data

00:16:40,020 --> 00:16:44,490
Platform works so to show you how this

00:16:42,720 --> 00:16:48,270
works I would like to take you through

00:16:44,490 --> 00:16:50,520
the flow of the data so if we go down

00:16:48,270 --> 00:16:53,279
here you will see that we have a couple

00:16:50,520 --> 00:16:56,399
of different tables named raw underscore

00:16:53,279 --> 00:16:59,130
something so basically what happens is

00:16:56,399 --> 00:17:01,140
each plant out of these 180 plants so we

00:16:59,130 --> 00:17:01,649
are implemented in around 18 of them

00:17:01,140 --> 00:17:04,530
right now

00:17:01,649 --> 00:17:07,350
so each plant sends raw data from their

00:17:04,530 --> 00:17:11,640
sensors to create dB and it creates a

00:17:07,350 --> 00:17:13,530
table like this and these these are

00:17:11,640 --> 00:17:15,990
basically different names for different

00:17:13,530 --> 00:17:18,329
plants so if you see here this is this

00:17:15,990 --> 00:17:21,050
data is coming from the Florida plant we

00:17:18,329 --> 00:17:21,050
have data coming

00:17:32,650 --> 00:17:46,970
yeah I think I should be horrible

00:17:38,900 --> 00:17:49,130
without this let's try again and then we

00:17:46,970 --> 00:17:51,559
have here for example data coming in

00:17:49,130 --> 00:17:54,440
from the McDonald plant so let's open

00:17:51,559 --> 00:17:56,780
one of these tables so this will show

00:17:54,440 --> 00:17:59,780
you here the health of the table they're

00:17:56,780 --> 00:18:01,700
configured shards so it's creamy creates

00:17:59,780 --> 00:18:05,090
a default of four shots unless specified

00:18:01,700 --> 00:18:08,150
otherwise and we have total records of

00:18:05,090 --> 00:18:09,920
two or twenty 4.6 million data points so

00:18:08,150 --> 00:18:13,070
this shows the different partitions and

00:18:09,920 --> 00:18:18,200
the schema so let's query this data and

00:18:13,070 --> 00:18:20,710
see what it actually looks like so here

00:18:18,200 --> 00:18:24,020
we have only four columns we have just

00:18:20,710 --> 00:18:26,540
party table which is a partition with

00:18:24,020 --> 00:18:30,350
the help of month then we have a payload

00:18:26,540 --> 00:18:33,050
which is basically a JSON object and it

00:18:30,350 --> 00:18:35,150
has nestings within it there's a topic

00:18:33,050 --> 00:18:38,500
and then there is the timestamp of when

00:18:35,150 --> 00:18:41,870
the data was inserted into create DB so

00:18:38,500 --> 00:18:44,420
let's take a look at this payload here

00:18:41,870 --> 00:18:47,450
you can see it's basically just a JSON

00:18:44,420 --> 00:18:50,090
object and it has really different

00:18:47,450 --> 00:18:52,730
information about the machine that it is

00:18:50,090 --> 00:18:55,160
coming from and this part also gives us

00:18:52,730 --> 00:18:58,760
some other kinds of information like

00:18:55,160 --> 00:19:00,650
okay this is coming from a plant that's

00:18:58,760 --> 00:19:03,110
the name of the plant it's coming from

00:19:00,650 --> 00:19:05,679
the production line one on the a side

00:19:03,110 --> 00:19:08,120
and the number of sensor is nine and

00:19:05,679 --> 00:19:10,370
this is the topic that we are seeing

00:19:08,120 --> 00:19:13,750
here is portal underscore counts let's

00:19:10,370 --> 00:19:19,460
take a look at a different topic and

00:19:13,750 --> 00:19:21,530
have a look at its object so if you take

00:19:19,460 --> 00:19:24,350
a look at this you would realize that

00:19:21,530 --> 00:19:27,950
this is not exactly similar in structure

00:19:24,350 --> 00:19:29,800
to the one above and this is made

00:19:27,950 --> 00:19:32,630
possible with something called dynamic

00:19:29,800 --> 00:19:35,150
object typing with the help of great DB

00:19:32,630 --> 00:19:36,920
so even if different data points

00:19:35,150 --> 00:19:38,660
entering the same column have a

00:19:36,920 --> 00:19:41,809
different structure you can still place

00:19:38,660 --> 00:19:43,669
them within a single table so once we

00:19:41,809 --> 00:19:44,540
have this raw data what create DB is

00:19:43,669 --> 00:19:47,210
data pad for

00:19:44,540 --> 00:19:49,810
does is it takes this data and it starts

00:19:47,210 --> 00:19:52,370
a process of enrichment of this data and

00:19:49,810 --> 00:19:55,910
what it basically does is it takes

00:19:52,370 --> 00:19:58,370
different nested objects it creates a

00:19:55,910 --> 00:20:00,740
new table for each of them a new column

00:19:58,370 --> 00:20:03,140
for each of them and it makes it

00:20:00,740 --> 00:20:05,660
strongly typed so once these objects are

00:20:03,140 --> 00:20:07,490
strongly typed they are even much more

00:20:05,660 --> 00:20:08,450
performant oncreate DB when we run

00:20:07,490 --> 00:20:13,150
queries on them

00:20:08,450 --> 00:20:16,010
so this enrichment process gives us

00:20:13,150 --> 00:20:19,300
gives us new table which is called

00:20:16,010 --> 00:20:21,620
metric table and the data from different

00:20:19,300 --> 00:20:23,540
raw data from different factories is

00:20:21,620 --> 00:20:27,080
being then enriched and fed into this

00:20:23,540 --> 00:20:29,720
table so let's take a look at this table

00:20:27,080 --> 00:20:31,280
and see what it looks like so as you can

00:20:29,720 --> 00:20:33,950
see we have we end up with much more

00:20:31,280 --> 00:20:37,370
columns than before and most of them are

00:20:33,950 --> 00:20:41,000
null because maybe this these records

00:20:37,370 --> 00:20:43,550
did not have the kind of nesting that

00:20:41,000 --> 00:20:45,200
was present in other objects and this

00:20:43,550 --> 00:20:50,000
doesn't affect the performance of

00:20:45,200 --> 00:20:51,980
querying on krei dB so let's run some

00:20:50,000 --> 00:20:54,260
queries on this so I prepared some

00:20:51,980 --> 00:21:00,320
queries beforehand I'll quickly copy

00:20:54,260 --> 00:21:03,710
pasted them so this is just a very basic

00:21:00,320 --> 00:21:06,110
operation we are doing a average of a

00:21:03,710 --> 00:21:10,280
nested object from this table we are

00:21:06,110 --> 00:21:12,500
type is vision vision rich X module so

00:21:10,280 --> 00:21:15,260
you're gonna execute this query and it's

00:21:12,500 --> 00:21:19,070
running over about 1.2 billion records

00:21:15,260 --> 00:21:22,700
and it gives us a result in just 4.2

00:21:19,070 --> 00:21:24,590
seconds let's take since we were talking

00:21:22,700 --> 00:21:30,250
about the use case of smart factories

00:21:24,590 --> 00:21:33,610
let's do a more suitable query on this

00:21:30,250 --> 00:21:36,730
so what this query does is it basically

00:21:33,610 --> 00:21:39,410
tells me all the alarms that went off or

00:21:36,730 --> 00:21:40,850
what the unit they were these alarms

00:21:39,410 --> 00:21:44,030
were coming from and what was the

00:21:40,850 --> 00:21:45,530
Machine model and for some reason I put

00:21:44,030 --> 00:21:48,470
some constraints on this data like

00:21:45,530 --> 00:21:50,030
machine model should be be mu 2200 the

00:21:48,470 --> 00:21:53,390
type should be alarmed then I also asked

00:21:50,030 --> 00:21:55,340
it to be chromed and ordered so this is

00:21:53,390 --> 00:21:56,900
a sub second query here again and this

00:21:55,340 --> 00:21:58,460
gives us the result that we were looking

00:21:56,900 --> 00:22:00,320
for

00:21:58,460 --> 00:22:06,350
let's take a look at one last query

00:22:00,320 --> 00:22:09,019
before moving on so what we are looking

00:22:06,350 --> 00:22:10,999
for here is I want to look so this is

00:22:09,019 --> 00:22:13,009
some of the things that are really used

00:22:10,999 --> 00:22:15,470
by I'll play in their messaging and

00:22:13,009 --> 00:22:17,990
alerting system so here we are looking

00:22:15,470 --> 00:22:20,419
for production line and then we are

00:22:17,990 --> 00:22:22,009
looking for the downtime reason when the

00:22:20,419 --> 00:22:24,740
downtime of a particular component

00:22:22,009 --> 00:22:27,169
started how long it when it was finished

00:22:24,740 --> 00:22:29,659
how long it lasted and how old it is in

00:22:27,169 --> 00:22:32,149
terms of ours and then we have a few

00:22:29,659 --> 00:22:34,159
constraints on this data that you want

00:22:32,149 --> 00:22:37,220
the type to be downtime location should

00:22:34,159 --> 00:22:39,499
be the mcdonough plant that downtime

00:22:37,220 --> 00:22:41,659
should not be null and then I I also

00:22:39,499 --> 00:22:44,899
asked it to be ordered by timestamp in a

00:22:41,659 --> 00:22:47,029
descending fashion and it's again a sub

00:22:44,899 --> 00:22:50,690
second query it takes just two 0.2

00:22:47,029 --> 00:22:52,610
seconds to run over 1.2 billion records

00:22:50,690 --> 00:22:54,259
of data and here we get information that

00:22:52,610 --> 00:22:56,869
we were looking for about the downtime

00:22:54,259 --> 00:22:59,360
when it started how long did the

00:22:56,869 --> 00:23:02,600
downtime last and how old is the

00:22:59,360 --> 00:23:05,240
downtime and if we take a look at the

00:23:02,600 --> 00:23:07,460
cluster panel of this admin UI you will

00:23:05,240 --> 00:23:11,090
see that we are running over just five

00:23:07,460 --> 00:23:13,970
nodes each running over 15 gigs of ram

00:23:11,090 --> 00:23:16,460
with each CPU core processors and with

00:23:13,970 --> 00:23:19,549
just this we are able to able to achieve

00:23:16,460 --> 00:23:22,669
subsequent query performance over a

00:23:19,549 --> 00:23:25,220
large amount of data so every plant that

00:23:22,669 --> 00:23:28,100
was using create DB saw improvements in

00:23:25,220 --> 00:23:30,889
the net equipment efficiency with some

00:23:28,100 --> 00:23:32,809
efficiencies above 90% so that is a

00:23:30,889 --> 00:23:34,669
metric that they were using for how

00:23:32,809 --> 00:23:37,460
productive or how efficient their

00:23:34,669 --> 00:23:39,830
factories are and every plant also saw a

00:23:37,460 --> 00:23:43,279
reduction in manual process adjustments

00:23:39,830 --> 00:23:45,919
as much as by 50% and reducing the

00:23:43,279 --> 00:23:48,350
number of manual adjustment stabilizes

00:23:45,919 --> 00:23:50,720
the process and significantly improves

00:23:48,350 --> 00:23:53,289
production efficiency so the result was

00:23:50,720 --> 00:23:56,539
a six-figure saving just in 2018 and

00:23:53,289 --> 00:23:58,909
currently they make about 2.5 million

00:23:56,539 --> 00:24:00,769
queries per day and work hasn't even

00:23:58,909 --> 00:24:05,299
finished yet it is estimated to increase

00:24:00,769 --> 00:24:06,649
about 18 times by the end of 2020 so

00:24:05,299 --> 00:24:09,289
with this I would like to conclude

00:24:06,649 --> 00:24:11,840
today's session on tik-tok what the heck

00:24:09,289 --> 00:24:12,380
is time series data so today we learned

00:24:11,840 --> 00:24:14,660
about

00:24:12,380 --> 00:24:17,240
I'm serious data its broad use cases

00:24:14,660 --> 00:24:20,450
create DB for Machine data use cases

00:24:17,240 --> 00:24:22,760
credi be on Azure and finally did a case

00:24:20,450 --> 00:24:25,190
study on how huge amounts of time series

00:24:22,760 --> 00:24:27,290
data is handled by a smart factory like

00:24:25,190 --> 00:24:30,080
I'll plow if you'd like to play around

00:24:27,290 --> 00:24:31,910
with create DB clusters on s your feel

00:24:30,080 --> 00:24:34,280
free to get in touch with me and we will

00:24:31,910 --> 00:24:37,100
set you up so we are in this booming age

00:24:34,280 --> 00:24:39,650
of data so why not put it to work to

00:24:37,100 --> 00:24:41,360
make our lives easier safer and better I

00:24:39,650 --> 00:24:49,340
look forward to seeing what you do with

00:24:41,360 --> 00:24:54,610
this thank you so thank you very much

00:24:49,340 --> 00:24:54,610
tonight there are any questions

00:24:55,120 --> 00:25:01,430
so after first processing of the data so

00:24:59,060 --> 00:25:05,300
the nested objects all those null

00:25:01,430 --> 00:25:08,540
columns do they actually use still space

00:25:05,300 --> 00:25:11,360
in terms of yes setting some sort of a

00:25:08,540 --> 00:25:14,090
null bit somewhere in memory or on disk

00:25:11,360 --> 00:25:17,000
or anything because in terms of my

00:25:14,090 --> 00:25:20,000
sequel would still reserve the space of

00:25:17,000 --> 00:25:23,210
the column like character whatsoever so

00:25:20,000 --> 00:25:27,830
do they use that space if it is null or

00:25:23,210 --> 00:25:31,700
not so they do not use any any extra

00:25:27,830 --> 00:25:33,770
space and otherwise that would be a lot

00:25:31,700 --> 00:25:36,780
of extra data that would be held by

00:25:33,770 --> 00:25:40,070
welcome to my sequel

00:25:36,780 --> 00:25:43,590
and another question so you're offering

00:25:40,070 --> 00:25:45,990
hosted services on prayer just on Azure

00:25:43,590 --> 00:25:50,010
or do you have something like also on

00:25:45,990 --> 00:25:52,800
other cloud public cloud platforms

00:25:50,010 --> 00:25:54,930
currently we are only offering hosted

00:25:52,800 --> 00:25:58,230
services on Azure but we have a couple

00:25:54,930 --> 00:26:00,570
of other major major providers in our

00:25:58,230 --> 00:26:04,260
roadmap as well so probably later on in

00:26:00,570 --> 00:26:05,700
this year but you can always use if you

00:26:04,260 --> 00:26:07,830
want to host your own clusters that's

00:26:05,700 --> 00:26:12,630
also possible but for now yes one yes

00:26:07,830 --> 00:26:23,760
your so questions answered yep any

00:26:12,630 --> 00:26:28,280
further questions okay okay hi thanks

00:26:23,760 --> 00:26:30,570
for the talk would you personally

00:26:28,280 --> 00:26:33,000
recommend this also for kind of the

00:26:30,570 --> 00:26:38,510
first type of metrics we were talking

00:26:33,000 --> 00:26:42,000
about from just the normal computer CPUs

00:26:38,510 --> 00:26:44,310
so like it really depends on your use

00:26:42,000 --> 00:26:46,350
case so when creatively started off like

00:26:44,310 --> 00:26:48,930
we we still have a lot of different

00:26:46,350 --> 00:26:52,080
clients who are not per se in machine

00:26:48,930 --> 00:26:53,760
data scenario like for example we

00:26:52,080 --> 00:26:55,920
started working with McAfee and they

00:26:53,760 --> 00:26:57,870
have also huge streams of data coming in

00:26:55,920 --> 00:27:01,350
right like people submitting and know

00:26:57,870 --> 00:27:03,600
different kinds of files for virus scans

00:27:01,350 --> 00:27:06,020
and so on and they I think they were

00:27:03,600 --> 00:27:08,820
using Postgres sequel before that and

00:27:06,020 --> 00:27:10,650
deploying a great cluster help them take

00:27:08,820 --> 00:27:13,950
down their costs and machinery by about

00:27:10,650 --> 00:27:18,360
75% so I would really say quite depends

00:27:13,950 --> 00:27:22,560
on how how how much data is to be

00:27:18,360 --> 00:27:25,470
ingested and yeah like that's I think

00:27:22,560 --> 00:27:28,170
that's one of the major points so how

00:27:25,470 --> 00:27:31,140
heavy our data intensive the processes

00:27:28,170 --> 00:27:31,680
are and on the cluster that you just

00:27:31,140 --> 00:27:35,070
amount

00:27:31,680 --> 00:27:38,190
what's the storage capacity of it you

00:27:35,070 --> 00:27:41,540
said the CPU and memory but what stuck I

00:27:38,190 --> 00:27:47,640
am not sure about it I can check it with

00:27:41,540 --> 00:27:49,230
roughly so I think they have about I'm

00:27:47,640 --> 00:27:50,700
not sure if I'm entirely correct but

00:27:49,230 --> 00:27:53,820
about five TVs

00:27:50,700 --> 00:28:00,720
of storage capacity on each machine but

00:27:53,820 --> 00:28:03,780
I will check that thanks so I just

00:28:00,720 --> 00:28:07,430
checked the clock we have time we would

00:28:03,780 --> 00:28:11,860
have time for another question anyone ok

00:28:07,430 --> 00:28:15,050
so I say thank you very much to deny

00:28:11,860 --> 00:28:15,050
[Applause]

00:28:17,860 --> 00:28:21,950

YouTube URL: https://www.youtube.com/watch?v=8AEMjKbcOy4


