Title: OSDC 2019: Introducing Kudo – Kubernetes Operators the easy way by Matt Javis
Publication date: 2019-05-21
Playlist: OSDC 2019 | Open Source Data Center Conference
Description: 
	Kubernetes Operators are the next phase of the journey towards automating complex applications in containers. Many Operators that exist today handle initial deployment, but they don’t provide automation for tasks like binary upgrades, configuration updates, and failure recovery. Implementing a production-grade controller for a complex workload typically requires thousands of lines of code and many months of development. As a result, the quality of operators that are available today varies. In DC/OS, the DC/OS commons SDK enables anyone to build service automation for DC/OS using just a declarative spec in most cases. The Kudo project can now leverage this set of automation expertise to enable automated creation of operators for Kubernetes. In this talk I’ll introduce the Kudo project, talk through the conceptual similarities between frameworks and operators, and demo the creation of a Kubernetes operator using Kudo.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:01,140 --> 00:00:10,550
[Music]

00:00:14,050 --> 00:00:17,349
[Applause]

00:00:18,949 --> 00:00:25,340
there we go so this is me I'm Director

00:00:22,140 --> 00:00:28,199
of Community a company called mesosphere

00:00:25,340 --> 00:00:31,320
mesosphere build cluster orchestration

00:00:28,199 --> 00:00:33,870
software we work around historically

00:00:31,320 --> 00:00:39,989
around Apache mazes but also now around

00:00:33,870 --> 00:00:41,760
kubernetes we have two big engineering

00:00:39,989 --> 00:00:47,309
offices one in San Francisco and one in

00:00:41,760 --> 00:00:48,539
Hamburg so just start by setting the

00:00:47,309 --> 00:00:50,899
scene a little bit about what we're

00:00:48,539 --> 00:00:53,940
gonna be be looking at today so

00:00:50,899 --> 00:00:56,579
kubernetes and most orchestration

00:00:53,940 --> 00:01:00,329
systems built around the idea of

00:00:56,579 --> 00:01:02,940
declarative state work very well for

00:01:00,329 --> 00:01:04,979
stateless applications and a stateless

00:01:02,940 --> 00:01:06,630
application for those of you are

00:01:04,979 --> 00:01:09,290
familiar with this space is a program

00:01:06,630 --> 00:01:13,049
that doesn't save any client data

00:01:09,290 --> 00:01:15,750
generated in one session to use it in

00:01:13,049 --> 00:01:17,939
the next session with that client each

00:01:15,750 --> 00:01:20,130
sessions carried out as if it was the

00:01:17,939 --> 00:01:22,290
first time and responses aren't

00:01:20,130 --> 00:01:24,540
dependent on data from the previous

00:01:22,290 --> 00:01:27,119
session so what that means in practice

00:01:24,540 --> 00:01:29,610
is that we can start and stop these

00:01:27,119 --> 00:01:31,170
stateless applications and when we start

00:01:29,610 --> 00:01:35,700
them they'll just keep they'll just

00:01:31,170 --> 00:01:36,840
start up and start executing and in

00:01:35,700 --> 00:01:38,610
general we'd run this kind of

00:01:36,840 --> 00:01:41,670
application behind a load balancer and

00:01:38,610 --> 00:01:44,390
we can scale the state stateless apps up

00:01:41,670 --> 00:01:47,610
and down by adding or removing instances

00:01:44,390 --> 00:01:49,710
and we can also deploy new versions of

00:01:47,610 --> 00:01:53,040
them very easily by basically just

00:01:49,710 --> 00:01:54,810
replacing all of the running instances

00:01:53,040 --> 00:01:57,350
and we can have various ordering

00:01:54,810 --> 00:02:00,000
strategies for performing those tasks

00:01:57,350 --> 00:02:01,950
Bluegreen deployments for example for

00:02:00,000 --> 00:02:03,509
testing new versions and but

00:02:01,950 --> 00:02:06,270
fundamentally the management of them is

00:02:03,509 --> 00:02:09,780
kind of binary they're either running or

00:02:06,270 --> 00:02:11,760
they're not and stateless applications

00:02:09,780 --> 00:02:13,680
are kind of intrinsically intertwined

00:02:11,760 --> 00:02:16,620
with the move towards

00:02:13,680 --> 00:02:18,780
microservices architectures where we

00:02:16,620 --> 00:02:21,510
connect together many very simple

00:02:18,780 --> 00:02:25,590
applications where each have a very well

00:02:21,510 --> 00:02:27,450
bound its function and also to the

00:02:25,590 --> 00:02:28,709
adoption of container technologies to

00:02:27,450 --> 00:02:30,510
manage them because they're

00:02:28,709 --> 00:02:32,819
fundamentally well suited to those

00:02:30,510 --> 00:02:35,849
patterns that systems like kubernetes

00:02:32,819 --> 00:02:38,010
give us so we simply declare that we

00:02:35,849 --> 00:02:40,950
want a specific number of an application

00:02:38,010 --> 00:02:43,079
to exist and and kubernetes will create

00:02:40,950 --> 00:02:44,249
them for us and again it's kind of

00:02:43,079 --> 00:02:46,379
binary you know they're either running

00:02:44,249 --> 00:02:48,959
or they're not we don't have to worry

00:02:46,379 --> 00:02:53,449
about any complex operations to

00:02:48,959 --> 00:02:53,449
configure them before we start deploying

00:02:53,989 --> 00:02:59,370
so if all applications were stateless

00:02:57,030 --> 00:03:01,889
then the world of application deployment

00:02:59,370 --> 00:03:03,480
would be super simple and you'd be

00:03:01,889 --> 00:03:06,840
forgiven for thinking if you read the

00:03:03,480 --> 00:03:09,139
tech press that that that's the case

00:03:06,840 --> 00:03:11,099
everywhere in the area of micro services

00:03:09,139 --> 00:03:14,400
but unfortunately that's not actually

00:03:11,099 --> 00:03:17,639
true the second class of applications

00:03:14,400 --> 00:03:19,799
that we deal with a state full and here

00:03:17,639 --> 00:03:23,549
the app is retaining data or State

00:03:19,799 --> 00:03:25,680
across its entire lifecycle and you know

00:03:23,549 --> 00:03:29,549
older more monolithic applications like

00:03:25,680 --> 00:03:30,989
SQL databases fit into this paradigm but

00:03:29,549 --> 00:03:33,720
there are so many architectures where

00:03:30,989 --> 00:03:36,629
the individual elements making up a

00:03:33,720 --> 00:03:38,400
particular service are are clustered

00:03:36,629 --> 00:03:40,609
together in some way they're sharing

00:03:38,400 --> 00:03:42,900
some kind of data between them and

00:03:40,609 --> 00:03:45,209
typically these kind of applications

00:03:42,900 --> 00:03:48,599
have a set of lifecycle States which

00:03:45,209 --> 00:03:51,509
require logical ordering of actions in

00:03:48,599 --> 00:03:53,759
order to maintain operation in general

00:03:51,509 --> 00:03:55,889
we can't just start and stop things and

00:03:53,759 --> 00:03:58,280
expect the overall service to continue

00:03:55,889 --> 00:04:01,259
to operate or continue to be performant

00:03:58,280 --> 00:04:04,049
and we might need to wait for data to

00:04:01,259 --> 00:04:06,120
rebalance we might there might be

00:04:04,049 --> 00:04:08,519
multiple steps involved in an upgrade

00:04:06,120 --> 00:04:11,189
process and that applies not only to

00:04:08,519 --> 00:04:14,509
upgrading but to scaling to failure

00:04:11,189 --> 00:04:16,859
scenarios and all these operational

00:04:14,509 --> 00:04:20,669
strategies require domain-specific

00:04:16,859 --> 00:04:22,320
knowledge to manage them so they're kind

00:04:20,669 --> 00:04:25,080
of unique to the application those

00:04:22,320 --> 00:04:26,849
operations for a Cassandra cluster are

00:04:25,080 --> 00:04:27,270
different from what they might be for a

00:04:26,849 --> 00:04:33,560
CAF

00:04:27,270 --> 00:04:35,879
first or all for an HDFS deployment an

00:04:33,560 --> 00:04:37,379
kubernetes was originally very focused

00:04:35,879 --> 00:04:39,240
on the stateless application

00:04:37,379 --> 00:04:40,470
architecture as I said earlier and one

00:04:39,240 --> 00:04:42,720
of the fundamental features of

00:04:40,470 --> 00:04:46,050
kubernetes is that this scheduler can

00:04:42,720 --> 00:04:47,819
move pods around between agents and this

00:04:46,050 --> 00:04:51,449
works fine for stateless applications

00:04:47,819 --> 00:04:53,400
but not so well for stateful which tend

00:04:51,449 --> 00:04:57,270
to like their storage in their network

00:04:53,400 --> 00:04:58,909
not to change during operation unless we

00:04:57,270 --> 00:05:03,960
handle that in a very specific

00:04:58,909 --> 00:05:07,229
operational way so in order to try and

00:05:03,960 --> 00:05:09,599
handle the the complexities of stateful

00:05:07,229 --> 00:05:14,310
applications and kubernetes added this

00:05:09,599 --> 00:05:18,810
concept of stateful sets which adds the

00:05:14,310 --> 00:05:20,130
idea of stable network and storage but

00:05:18,810 --> 00:05:22,080
this doesn't really solve all of our

00:05:20,130 --> 00:05:23,789
problems because kubernetes still

00:05:22,080 --> 00:05:29,400
fundamentally doesn't know what's

00:05:23,789 --> 00:05:31,889
happening inside the pods and so as an

00:05:29,400 --> 00:05:34,770
example of this we can consider a

00:05:31,889 --> 00:05:36,990
rolling upgrade of nodes in a kubernetes

00:05:34,770 --> 00:05:40,560
cluster and let's imagine we have a

00:05:36,990 --> 00:05:42,770
replicated master slave database of some

00:05:40,560 --> 00:05:46,490
description running in a stateful set

00:05:42,770 --> 00:05:49,440
the node running the master upgrades and

00:05:46,490 --> 00:05:52,680
reboot so we lose we may lose some

00:05:49,440 --> 00:05:55,349
transactions at that point and at this

00:05:52,680 --> 00:05:58,740
point this this our imaginary database

00:05:55,349 --> 00:06:03,060
cluster may trigger a re-election and

00:05:58,740 --> 00:06:05,389
elect a new master so when the master

00:06:03,060 --> 00:06:08,969
the original master pod is rescheduled

00:06:05,389 --> 00:06:11,520
it comes back up and into a cluster with

00:06:08,969 --> 00:06:13,169
a new master which you may be unaware of

00:06:11,520 --> 00:06:15,870
and that might cause issues in itself

00:06:13,169 --> 00:06:18,569
but even if it then settles back into

00:06:15,870 --> 00:06:20,389
its role as a slave kubernetes sees all

00:06:18,569 --> 00:06:22,680
the all the nodes are running again and

00:06:20,389 --> 00:06:25,139
starts to roll out an upgrade to another

00:06:22,680 --> 00:06:28,889
node which may well hit our new master

00:06:25,139 --> 00:06:33,630
and so on so how happy is this database

00:06:28,889 --> 00:06:35,490
likely to be during this process and the

00:06:33,630 --> 00:06:39,990
point here is that the domain knowledge

00:06:35,490 --> 00:06:41,190
of how to safely do a rolling upgrade

00:06:39,990 --> 00:06:42,750
when you've got things

00:06:41,190 --> 00:06:45,060
database thrusters running isn't

00:06:42,750 --> 00:06:48,960
captured by that concept of stateful

00:06:45,060 --> 00:06:53,310
sets so how do we automate and package

00:06:48,960 --> 00:06:55,800
that knowledge up so the emerging

00:06:53,310 --> 00:06:58,620
solution to that problem is the operator

00:06:55,800 --> 00:07:02,190
pattern and operators basically

00:06:58,620 --> 00:07:05,550
encapsulate operational tasks in code

00:07:02,190 --> 00:07:07,430
and that code can be run either when an

00:07:05,550 --> 00:07:10,590
API is invoked it can be run

00:07:07,430 --> 00:07:12,920
automatically it can be run on a

00:07:10,590 --> 00:07:16,680
schedule so we can orchestrate

00:07:12,920 --> 00:07:21,030
application lifestyle actions using the

00:07:16,680 --> 00:07:22,080
kubernetes api so fundamentally they

00:07:21,030 --> 00:07:24,510
encode all that domain-specific

00:07:22,080 --> 00:07:26,250
knowledge about the lifecycle actions

00:07:24,510 --> 00:07:28,710
required for the application how do i

00:07:26,250 --> 00:07:31,020
scale it how do i upgrade it how do i

00:07:28,710 --> 00:07:33,030
deal with failure scenarios and they

00:07:31,020 --> 00:07:38,880
achieve all of this using native

00:07:33,030 --> 00:07:40,800
kubernetes api so what our operators

00:07:38,880 --> 00:07:43,740
well when we install an operator we

00:07:40,800 --> 00:07:46,740
generally get the operator itself which

00:07:43,740 --> 00:07:48,300
is the kind of code that manages the

00:07:46,740 --> 00:07:49,350
life cycle of that particular thing and

00:07:48,300 --> 00:07:53,520
that's where all that domain-specific

00:07:49,350 --> 00:07:55,770
knowledge is is encapsulated and we'll

00:07:53,520 --> 00:07:58,530
also likely get a bunch of custom

00:07:55,770 --> 00:08:00,480
resource definitions see our DS and

00:07:58,530 --> 00:08:04,790
custom resource definitions extend the

00:08:00,480 --> 00:08:08,430
kubernetes api to add new resource types

00:08:04,790 --> 00:08:10,140
so once we've got that type we can put

00:08:08,430 --> 00:08:12,600
some data into it give it to kubernetes

00:08:10,140 --> 00:08:14,640
and kubernetes can then instantiate an

00:08:12,600 --> 00:08:17,669
instance of it in this example here

00:08:14,640 --> 00:08:22,260
we're looking at some code to

00:08:17,669 --> 00:08:23,550
instantiate a MySQL cluster but the

00:08:22,260 --> 00:08:26,400
important thing here is that each of

00:08:23,550 --> 00:08:29,030
these operators is unique it's

00:08:26,400 --> 00:08:34,169
purpose-built for each specific

00:08:29,030 --> 00:08:36,510
application and up until now there have

00:08:34,169 --> 00:08:39,810
been two main ways of building operators

00:08:36,510 --> 00:08:42,990
the first is the operator framework this

00:08:39,810 --> 00:08:45,210
originated at core OS and then and then

00:08:42,990 --> 00:08:48,240
came in to Red Hat when when core OS

00:08:45,210 --> 00:08:52,470
were acquired as mainly developed by by

00:08:48,240 --> 00:08:54,390
Red Hat and IBM and you can build

00:08:52,470 --> 00:08:55,500
operators just use the ansible and helm

00:08:54,390 --> 00:08:59,190
Chah

00:08:55,500 --> 00:09:01,860
but there's also a go SDK when you want

00:08:59,190 --> 00:09:05,160
to encapsulate complex lifecycle actions

00:09:01,860 --> 00:09:07,740
with with ansible or or helm generally

00:09:05,160 --> 00:09:09,480
those those operators will have some

00:09:07,740 --> 00:09:11,670
limitations in their functionality

00:09:09,480 --> 00:09:13,320
because of that the underlying

00:09:11,670 --> 00:09:17,070
technology that's been used to build

00:09:13,320 --> 00:09:20,760
them so most complex operators are built

00:09:17,070 --> 00:09:23,010
using the go SDK and then there's the

00:09:20,760 --> 00:09:25,260
COO builder project that's part of the

00:09:23,010 --> 00:09:27,480
API machinery sitting in the kubernetes

00:09:25,260 --> 00:09:30,720
project and that's focused really on

00:09:27,480 --> 00:09:33,300
extending the kubernetes api again you

00:09:30,720 --> 00:09:36,210
need to write a lot of go to build using

00:09:33,300 --> 00:09:38,670
this and there are some gaps in the in

00:09:36,210 --> 00:09:44,040
the lifecycle coverage for for operators

00:09:38,670 --> 00:09:46,290
done in that way but the point is that

00:09:44,040 --> 00:09:48,660
building operators can be really complex

00:09:46,290 --> 00:09:51,930
and it requires a lot of knowledge about

00:09:48,660 --> 00:09:55,470
how kubernetes works internally it also

00:09:51,930 --> 00:09:57,300
requires very deep go expertise as well

00:09:55,470 --> 00:09:59,610
as all that domain-specific knowledge

00:09:57,300 --> 00:10:02,790
about the particular application you're

00:09:59,610 --> 00:10:04,470
going to write an operator for so for

00:10:02,790 --> 00:10:07,830
for a lot of organizations who are

00:10:04,470 --> 00:10:10,200
looking to to do orchestration in this

00:10:07,830 --> 00:10:12,210
way using operators you know you you

00:10:10,200 --> 00:10:13,800
might well have to bring in substantial

00:10:12,210 --> 00:10:16,500
engineering resource in order to

00:10:13,800 --> 00:10:19,020
actually build one yourself and and I

00:10:16,500 --> 00:10:20,490
guess that the the kinds of tasks that

00:10:19,020 --> 00:10:24,240
you're gonna want to automate are also

00:10:20,490 --> 00:10:26,460
fairly variable you know automating

00:10:24,240 --> 00:10:28,830
adding a replica to it to a database

00:10:26,460 --> 00:10:30,990
cluster is pretty straightforward but

00:10:28,830 --> 00:10:33,930
how do you fix things like a right ahead

00:10:30,990 --> 00:10:35,610
log or something like that um so the

00:10:33,930 --> 00:10:37,680
engineering effort that goes into making

00:10:35,610 --> 00:10:39,870
really high quality ones is is pretty

00:10:37,680 --> 00:10:43,860
considerable and as an example of this

00:10:39,870 --> 00:10:46,020
one of the major no SQL vendors has

00:10:43,860 --> 00:10:49,260
built a extremely comprehensive operator

00:10:46,020 --> 00:10:51,330
and it's about 40,000 lines of go and it

00:10:49,260 --> 00:10:54,300
took over one man year of effort to

00:10:51,330 --> 00:10:56,580
build this thing so you know you can see

00:10:54,300 --> 00:10:58,920
that's a very big investment for anybody

00:10:56,580 --> 00:11:01,590
to make even for a software vendor to

00:10:58,920 --> 00:11:03,450
make and even something like the HCD

00:11:01,590 --> 00:11:07,530
operator you know Etsy DS got a fairly

00:11:03,450 --> 00:11:10,520
simple life cycle and and that's around

00:11:07,530 --> 00:11:10,520
9,000 lines of

00:11:11,600 --> 00:11:17,700
so um historically at mesosphere we've

00:11:15,270 --> 00:11:20,550
we've worked in the field of Apache

00:11:17,700 --> 00:11:22,260
maize oz and the story in Mesa surrounds

00:11:20,550 --> 00:11:25,350
stateful applications this is slightly

00:11:22,260 --> 00:11:28,020
different this is the what the

00:11:25,350 --> 00:11:30,300
architecture of a mazes cluster looks

00:11:28,020 --> 00:11:32,400
like we have a bunch of agents that are

00:11:30,300 --> 00:11:34,230
running on your on your infrastructure

00:11:32,400 --> 00:11:36,270
at the bottom we have a number of

00:11:34,230 --> 00:11:38,880
masters only which one of which is in

00:11:36,270 --> 00:11:42,510
control at any one time we have a

00:11:38,880 --> 00:11:44,910
zookeeper cluster that's managing leader

00:11:42,510 --> 00:11:46,560
election for if my master fails and then

00:11:44,910 --> 00:11:49,170
we have what's called frameworks running

00:11:46,560 --> 00:11:50,160
on the top of these and so what makes

00:11:49,170 --> 00:11:52,320
makes us different

00:11:50,160 --> 00:11:55,890
in one sense from kubernetes is this

00:11:52,320 --> 00:11:59,700
idea of two level scheduling that the

00:11:55,890 --> 00:12:02,730
the maysa masters themselves are really

00:11:59,700 --> 00:12:04,890
only concerned with resource allocations

00:12:02,730 --> 00:12:07,290
so they're offering up resources to the

00:12:04,890 --> 00:12:09,450
frameworks the frameworks then decide

00:12:07,290 --> 00:12:12,060
whether they want to accept or decline

00:12:09,450 --> 00:12:15,150
that particular piece of resource that's

00:12:12,060 --> 00:12:17,610
been offered up from the cluster and the

00:12:15,150 --> 00:12:19,050
the frameworks consists of a scheduler

00:12:17,610 --> 00:12:20,850
which is the bit that's running at the

00:12:19,050 --> 00:12:22,740
top and then an executor that actually

00:12:20,850 --> 00:12:25,770
is the thing that launches workloads

00:12:22,740 --> 00:12:28,040
um but the key thing here is that that

00:12:25,770 --> 00:12:30,690
application logic that domain-specific

00:12:28,040 --> 00:12:34,650
application logic in Maysles has always

00:12:30,690 --> 00:12:38,340
been encoded in the scheduler separating

00:12:34,650 --> 00:12:39,780
out those two concerns and so that

00:12:38,340 --> 00:12:42,510
scheduler model is known as non

00:12:39,780 --> 00:12:44,250
monolithic and the mesas master doesn't

00:12:42,510 --> 00:12:45,870
need to know about all of the things it

00:12:44,250 --> 00:12:48,300
just needs to know about what resources

00:12:45,870 --> 00:12:50,520
are being offered up from below and the

00:12:48,300 --> 00:12:54,930
application specific knowledge is held

00:12:50,520 --> 00:12:56,340
in the application scheduler so that

00:12:54,930 --> 00:12:58,800
that means that those application

00:12:56,340 --> 00:13:02,340
schedulers have always been able to

00:12:58,800 --> 00:13:07,650
handle complex lifecycle actions for

00:13:02,340 --> 00:13:09,210
stateful stateful workloads but mesas

00:13:07,650 --> 00:13:12,300
frameworks are also incredibly hard to

00:13:09,210 --> 00:13:15,060
write they require in exactly the same

00:13:12,300 --> 00:13:17,400
way as as as kubernetes operators they

00:13:15,060 --> 00:13:19,110
require very deep knowledge of the

00:13:17,400 --> 00:13:22,050
internals of maze offs how to achieve

00:13:19,110 --> 00:13:24,149
specific things so um

00:13:22,050 --> 00:13:26,010
mesosphere we spent a lot of time

00:13:24,149 --> 00:13:29,339
looking at how to simplify that process

00:13:26,010 --> 00:13:33,450
to allow developers to more easily build

00:13:29,339 --> 00:13:35,160
mazes frameworks and it occurred to us

00:13:33,450 --> 00:13:38,370
after looking at lots of schedule

00:13:35,160 --> 00:13:40,200
implementations that there was actually

00:13:38,370 --> 00:13:42,750
quite a lot of commonality between the

00:13:40,200 --> 00:13:46,170
types of actions that are encapsulated

00:13:42,750 --> 00:13:49,380
into schedulers to manage application

00:13:46,170 --> 00:13:52,850
life style a life cycle and so that led

00:13:49,380 --> 00:13:55,920
to the creation of the DTS Commons SDK

00:13:52,850 --> 00:13:59,190
and which provided a layer of

00:13:55,920 --> 00:14:04,700
abstraction for developing frameworks

00:13:59,190 --> 00:14:08,550
and basically allows you to define a

00:14:04,700 --> 00:14:12,360
framework mostly in amel and one of the

00:14:08,550 --> 00:14:14,760
core concepts within the SDK is the idea

00:14:12,360 --> 00:14:17,010
of plans that are basically sequences of

00:14:14,760 --> 00:14:20,510
events which define a particular

00:14:17,010 --> 00:14:23,190
lifecycle event so a lot of plans are

00:14:20,510 --> 00:14:25,770
you know you can consider the structure

00:14:23,190 --> 00:14:27,450
of them to be fairly generic and for

00:14:25,770 --> 00:14:29,579
example deployment you know there's

00:14:27,450 --> 00:14:30,839
always going to be a deployment plan

00:14:29,579 --> 00:14:33,930
there's always going to be an upgrade

00:14:30,839 --> 00:14:38,160
plan and and so those are defined in the

00:14:33,930 --> 00:14:43,250
SDK - to remove the necessity for for

00:14:38,160 --> 00:14:45,480
everybody - to reimplemented framework

00:14:43,250 --> 00:14:47,880
so when we start to look at the operator

00:14:45,480 --> 00:14:51,870
landscape it seemed like this was a

00:14:47,880 --> 00:14:53,700
pretty familiar problem space we saw

00:14:51,870 --> 00:14:55,649
developers having to implement the same

00:14:53,700 --> 00:14:58,380
steps a lot of complexity being

00:14:55,649 --> 00:15:02,329
expressed in a number of lines of code

00:14:58,380 --> 00:15:05,430
and in in engineering time to implement

00:15:02,329 --> 00:15:08,610
so we took what we learnt from that DCOs

00:15:05,430 --> 00:15:11,880
Commons SDK and created the Kudo project

00:15:08,610 --> 00:15:15,149
with the aim of simplifying operator

00:15:11,880 --> 00:15:17,370
development so kudos an acronym for the

00:15:15,149 --> 00:15:20,160
kubernetes universal declarative

00:15:17,370 --> 00:15:23,450
operator and so rather than implementing

00:15:20,160 --> 00:15:27,089
a custom operator for each application

00:15:23,450 --> 00:15:29,670
cuda provides a universal operator with

00:15:27,089 --> 00:15:33,990
this concept of plans built into it so

00:15:29,670 --> 00:15:35,910
allows a operator developers to define a

00:15:33,990 --> 00:15:38,550
lot of the code

00:15:35,910 --> 00:15:40,200
Kamel it started out as being built on

00:15:38,550 --> 00:15:44,010
top of cou builder it's a fully open

00:15:40,200 --> 00:15:46,350
source project so you can also think of

00:15:44,010 --> 00:15:49,680
it as a kind of a way of simplifying cou

00:15:46,350 --> 00:15:52,080
builder based and development and you

00:15:49,680 --> 00:15:55,410
know we're it's in the early stages but

00:15:52,080 --> 00:15:58,380
we're actively working to build to build

00:15:55,410 --> 00:16:04,920
community around this and hoping to to

00:15:58,380 --> 00:16:06,540
push it into a CNC F sandbox project so

00:16:04,920 --> 00:16:09,750
in comparison to the other ways of

00:16:06,540 --> 00:16:11,690
building operators Kudo can create

00:16:09,750 --> 00:16:14,250
complete operators without needing any

00:16:11,690 --> 00:16:18,900
either particularly deep knowledge of

00:16:14,250 --> 00:16:21,870
kubernetes or writing any code just by

00:16:18,900 --> 00:16:24,840
defining those lifecycle phases and

00:16:21,870 --> 00:16:27,540
actions in Yama we can orchestrate those

00:16:24,840 --> 00:16:29,520
those lifecycle stages using the built

00:16:27,540 --> 00:16:33,420
in custom resource definitions which

00:16:29,520 --> 00:16:36,060
kudos applies and kudos ultimately just

00:16:33,420 --> 00:16:38,220
kubernetes there's no DSL to learn

00:16:36,060 --> 00:16:39,570
there's no proprietary API is there's no

00:16:38,220 --> 00:16:43,380
external datastore

00:16:39,570 --> 00:16:46,230
it's just kubernetes api so as a result

00:16:43,380 --> 00:16:48,380
it's a lot easier to learn and it

00:16:46,230 --> 00:16:51,840
supports all the native kubernetes

00:16:48,380 --> 00:16:54,690
tooling things like customized for for

00:16:51,840 --> 00:16:55,830
merging amyl together cube cuttle you

00:16:54,690 --> 00:16:57,450
know it's all just managed in the same

00:16:55,830 --> 00:16:59,370
way as you manage any other things

00:16:57,450 --> 00:17:02,370
within your kubernetes clusters and it

00:16:59,370 --> 00:17:03,960
maps to the same resource model that

00:17:02,370 --> 00:17:09,990
you're used to using with other

00:17:03,960 --> 00:17:12,120
kubernetes resources so there's a few

00:17:09,990 --> 00:17:14,580
core concepts that we need to understand

00:17:12,120 --> 00:17:17,970
and when we talk about what kudo is um

00:17:14,580 --> 00:17:19,470
firstly a framework so this is the kind

00:17:17,970 --> 00:17:22,350
of high-level description of a

00:17:19,470 --> 00:17:24,120
deployable service its representative

00:17:22,350 --> 00:17:27,750
represented as a custom resource

00:17:24,120 --> 00:17:31,010
definition object to CRD and we can see

00:17:27,750 --> 00:17:36,060
an example here of the the Kafka

00:17:31,010 --> 00:17:38,700
operator and then we have this this idea

00:17:36,060 --> 00:17:41,160
of framework version and that represents

00:17:38,700 --> 00:17:43,500
the particular implementation of a

00:17:41,160 --> 00:17:45,870
framework so it's possible to to have

00:17:43,500 --> 00:17:47,670
installed multiple framework versions of

00:17:45,870 --> 00:17:49,380
a particular framework you may want to

00:17:47,670 --> 00:17:51,690
choose pick and choose but

00:17:49,380 --> 00:17:53,820
in different versions of a particular

00:17:51,690 --> 00:17:56,880
piece of software and this is really

00:17:53,820 --> 00:18:01,380
where all of the where all of the the

00:17:56,880 --> 00:18:03,720
smarts are defined and the purpose of

00:18:01,380 --> 00:18:06,240
the framework version is really to give

00:18:03,720 --> 00:18:09,750
kudo the details that it needs to become

00:18:06,240 --> 00:18:13,260
an operator for that particular that

00:18:09,750 --> 00:18:16,170
particular application so as the

00:18:13,260 --> 00:18:18,600
operator will execute a plan to

00:18:16,170 --> 00:18:21,420
establish all the instances of the calf

00:18:18,600 --> 00:18:23,940
components in the cluster as you know as

00:18:21,420 --> 00:18:25,590
we asked for them and you know in this

00:18:23,940 --> 00:18:28,770
this example here we'll look at some

00:18:25,590 --> 00:18:30,150
some code a little bit later on but you

00:18:28,770 --> 00:18:32,010
can see there that we're gonna make sure

00:18:30,150 --> 00:18:34,710
there's three brokers for each instance

00:18:32,010 --> 00:18:37,530
and we have some we're defining which

00:18:34,710 --> 00:18:39,750
version of caf-co we want and we have

00:18:37,530 --> 00:18:43,260
some parameters and and stuff like that

00:18:39,750 --> 00:18:47,940
and then in terms of plans here we have

00:18:43,260 --> 00:18:50,310
just a single deployment plan and then

00:18:47,940 --> 00:18:52,800
finally nested into all of that is this

00:18:50,310 --> 00:18:55,260
instance CRD and the instance is the

00:18:52,800 --> 00:18:56,930
actual thing itself so it's a CR D to

00:18:55,260 --> 00:19:04,500
represent a particular application

00:18:56,930 --> 00:19:09,780
instantiation of a framework version so

00:19:04,500 --> 00:19:12,600
plans are how how Kudo frameworks convey

00:19:09,780 --> 00:19:14,670
progress through particular service

00:19:12,600 --> 00:19:17,520
management operations like deployment

00:19:14,670 --> 00:19:19,470
like scaling and a plan is kind of got

00:19:17,520 --> 00:19:23,000
this three-step hierarchy a plan is

00:19:19,470 --> 00:19:27,900
comprised of phases and then phases are

00:19:23,000 --> 00:19:29,790
comprised of of steps and we kind of

00:19:27,900 --> 00:19:33,570
collectively refer to all this stuff as

00:19:29,790 --> 00:19:35,910
elements and the status of the execution

00:19:33,570 --> 00:19:40,200
of a particular plan is captured through

00:19:35,910 --> 00:19:43,020
another CRD a plan execution CRD and the

00:19:40,200 --> 00:19:44,910
phases of the plan determine what their

00:19:43,020 --> 00:19:47,790
status is based on the steps that

00:19:44,910 --> 00:19:50,850
they've executed and then the overall

00:19:47,790 --> 00:19:53,070
planned status is defined by the by the

00:19:50,850 --> 00:19:55,620
status of the phases and then that's

00:19:53,070 --> 00:19:58,080
that captured in that plan execution CRD

00:19:55,620 --> 00:19:59,630
and we'll see that a little bit later I

00:19:58,080 --> 00:20:02,480
think

00:19:59,630 --> 00:20:04,610
and then we have this concept of

00:20:02,480 --> 00:20:06,800
deployment strategy and and this is

00:20:04,610 --> 00:20:10,280
about which the way in which a

00:20:06,800 --> 00:20:13,220
particular plan or step is executed and

00:20:10,280 --> 00:20:17,030
by default these can kind of be serial

00:20:13,220 --> 00:20:19,100
or or parallel so you know obviously

00:20:17,030 --> 00:20:20,900
that what that says is either I can

00:20:19,100 --> 00:20:23,900
execute all of those steps at the same

00:20:20,900 --> 00:20:26,090
time or I have to wait for four steps to

00:20:23,900 --> 00:20:32,540
complete before I start trying to

00:20:26,090 --> 00:20:35,960
execute the next one and a trigger is

00:20:32,540 --> 00:20:39,920
what happens when a parameter is updated

00:20:35,960 --> 00:20:42,230
in a in an instance object it's it's

00:20:39,920 --> 00:20:44,480
part of the framework version and it

00:20:42,230 --> 00:20:47,030
gives you just more customization for

00:20:44,480 --> 00:20:49,760
what actions you want to happen in this

00:20:47,030 --> 00:20:52,340
case we're looking over here at the the

00:20:49,760 --> 00:20:54,910
trigger for the change to a backup file

00:20:52,340 --> 00:20:59,210
and the trigger is going to trigger a

00:20:54,910 --> 00:21:05,390
backup step which is going to redo that

00:20:59,210 --> 00:21:07,130
particular backup task so if we have a

00:21:05,390 --> 00:21:09,820
closer look at what the the yama looks

00:21:07,130 --> 00:21:11,870
like hopefully everybody can see that

00:21:09,820 --> 00:21:14,480
here's an example of framework version

00:21:11,870 --> 00:21:16,730
and you know as I said earlier this is

00:21:14,480 --> 00:21:19,580
really where the magic happens for a

00:21:16,730 --> 00:21:21,740
particular version of a framework so we

00:21:19,580 --> 00:21:24,020
could see the plans for the lifecycle

00:21:21,740 --> 00:21:27,230
phases that are being that are being

00:21:24,020 --> 00:21:29,600
defined here for the deploy plan the

00:21:27,230 --> 00:21:31,310
strategy is serial so each of those

00:21:29,600 --> 00:21:33,710
tasks is going to be is going to be

00:21:31,310 --> 00:21:35,030
executed serially and we could see in

00:21:33,710 --> 00:21:36,680
this example we've got a couple of other

00:21:35,030 --> 00:21:39,590
plans we've got a back-up plan and we've

00:21:36,680 --> 00:21:42,230
got a restore plan and it's also worth

00:21:39,590 --> 00:21:45,290
noting here that tasks are defined

00:21:42,230 --> 00:21:47,000
outside of this of this gamal this

00:21:45,290 --> 00:21:49,940
particular yanil hierarchy so you can

00:21:47,000 --> 00:21:57,910
reuse them across across different plans

00:21:49,940 --> 00:22:00,680
and here's an example of a set of tasks

00:21:57,910 --> 00:22:05,440
defining what the resources which gamal

00:22:00,680 --> 00:22:05,440
applies to to each of those tasks and

00:22:05,770 --> 00:22:10,190
although the the patchy section we

00:22:08,090 --> 00:22:12,350
haven't yet implemented but that's going

00:22:10,190 --> 00:22:13,300
to be on the roadmap that that you could

00:22:12,350 --> 00:22:16,220
kind of money

00:22:13,300 --> 00:22:18,770
Mangiamo all together for a particular

00:22:16,220 --> 00:22:20,900
maybe adding custom configuration for

00:22:18,770 --> 00:22:22,730
your environment and and then you get

00:22:20,900 --> 00:22:25,190
those things executed at the same time

00:22:22,730 --> 00:22:27,710
and that will kind of let you separate

00:22:25,190 --> 00:22:30,520
out site-specific organization specific

00:22:27,710 --> 00:22:32,750
and configuration from perhaps a default

00:22:30,520 --> 00:22:38,330
upstream configuration for a particular

00:22:32,750 --> 00:22:40,690
framework and then in the framework

00:22:38,330 --> 00:22:42,940
version we also define configuration

00:22:40,690 --> 00:22:46,190
information about the application itself

00:22:42,940 --> 00:22:49,820
in this example here we've got some

00:22:46,190 --> 00:22:51,830
parameters we give a parameter we can

00:22:49,820 --> 00:22:53,900
give it a default value those can then

00:22:51,830 --> 00:22:55,270
be overwrite overridden at the instance

00:22:53,900 --> 00:22:59,350
level when we define the particular

00:22:55,270 --> 00:23:05,080
instantiation of that application and

00:22:59,350 --> 00:23:09,140
you can see that we can refer to these

00:23:05,080 --> 00:23:12,530
two the the parameters that we defined

00:23:09,140 --> 00:23:17,150
further on in our spec using templating

00:23:12,530 --> 00:23:20,360
this is moustache templating and here's

00:23:17,150 --> 00:23:22,280
an example of an instance and we could

00:23:20,360 --> 00:23:25,250
see we're referencing which framework

00:23:22,280 --> 00:23:27,050
version we want to use so as I said

00:23:25,250 --> 00:23:28,310
before you could have multiple framework

00:23:27,050 --> 00:23:30,470
versions of a particular framework

00:23:28,310 --> 00:23:32,090
installed in your cluster and decide

00:23:30,470 --> 00:23:34,010
when you instantiate an application

00:23:32,090 --> 00:23:35,510
which one you want to use and we're

00:23:34,010 --> 00:23:37,070
defining some parameters that we're

00:23:35,510 --> 00:23:44,810
going to override from our framework

00:23:37,070 --> 00:23:48,620
version and CUDA also includes a CLI

00:23:44,810 --> 00:23:50,890
extension to to cout kettle and which

00:23:48,620 --> 00:23:55,280
allows us to run Kudo specific commands

00:23:50,890 --> 00:23:59,480
natively inside inside coop kettle so we

00:23:55,280 --> 00:24:02,960
can look at at the excuse me at the

00:23:59,480 --> 00:24:04,940
status of particular plan executions we

00:24:02,960 --> 00:24:07,250
can actually install frameworks directly

00:24:04,940 --> 00:24:10,010
from the Kudo upstream github

00:24:07,250 --> 00:24:11,630
repositories again this is in very

00:24:10,010 --> 00:24:16,550
active development so things are being

00:24:11,630 --> 00:24:18,880
added to this all the time so we're

00:24:16,550 --> 00:24:22,310
gonna look at some some actual Kudo

00:24:18,880 --> 00:24:23,630
operators in a minute but I wanted to

00:24:22,310 --> 00:24:26,690
share a couple of things that are likely

00:24:23,630 --> 00:24:29,960
to be to be coming in the next version

00:24:26,690 --> 00:24:32,810
of kudo firstly we have this concept of

00:24:29,960 --> 00:24:35,930
dynamics er DS so the idea here is that

00:24:32,810 --> 00:24:39,800
CR DS can be added to a particular

00:24:35,930 --> 00:24:42,320
framework on the fly via another known

00:24:39,800 --> 00:24:44,600
set of kudos er DS so the controller

00:24:42,320 --> 00:24:46,580
doesn't need to know when you install

00:24:44,600 --> 00:24:49,160
that framework about every single action

00:24:46,580 --> 00:24:53,090
that that particular framework is going

00:24:49,160 --> 00:24:55,790
to do so it might be as a as an operator

00:24:53,090 --> 00:24:57,770
you know as a an operator who's a person

00:24:55,790 --> 00:25:00,500
operating a cluster you want to change

00:24:57,770 --> 00:25:03,620
our particular plan is executed or you

00:25:00,500 --> 00:25:07,340
want to change ordering or add entirely

00:25:03,620 --> 00:25:09,530
new sets of actions and you may want to

00:25:07,340 --> 00:25:11,360
change what what things trigger which

00:25:09,530 --> 00:25:15,560
which thing and you'll be able to do

00:25:11,360 --> 00:25:17,750
that through dynamic CR DS and what that

00:25:15,560 --> 00:25:19,220
kind of means is a slight change to to

00:25:17,750 --> 00:25:22,400
how it's structured at the minute in

00:25:19,220 --> 00:25:27,940
that operations themselves move up to

00:25:22,400 --> 00:25:31,360
become a first-class object a kind and

00:25:27,940 --> 00:25:33,380
that effectively means you can trigger

00:25:31,360 --> 00:25:36,650
operational actions just by

00:25:33,380 --> 00:25:40,520
instantiating a particular object into

00:25:36,650 --> 00:25:41,900
the cluster and we're also doing some

00:25:40,520 --> 00:25:46,520
work around the concept of framework

00:25:41,900 --> 00:25:48,890
extensions and so because the frameworks

00:25:46,520 --> 00:25:51,770
themselves are just defined in Yama we

00:25:48,890 --> 00:25:53,300
can add functionality to a framework and

00:25:51,770 --> 00:25:55,460
have extensions to that framework

00:25:53,300 --> 00:25:58,760
maintained separately and then merge

00:25:55,460 --> 00:26:01,220
together at runtime so some examples of

00:25:58,760 --> 00:26:04,940
that may be platform specific extensions

00:26:01,220 --> 00:26:08,030
to a particular particular framework or

00:26:04,940 --> 00:26:09,950
then proprietary internal extensions

00:26:08,030 --> 00:26:14,950
that are maintained separately within an

00:26:09,950 --> 00:26:14,950
organization or tied to a specific user

00:26:15,910 --> 00:26:20,660
and one of the problems we'd also like

00:26:18,710 --> 00:26:22,880
to improve on and this is not this is

00:26:20,660 --> 00:26:26,870
not unique to Takuto in any way it's a

00:26:22,880 --> 00:26:28,670
kind of a problem space for kubernetes

00:26:26,870 --> 00:26:31,940
generally as kubernetes gets more and

00:26:28,670 --> 00:26:33,980
more complicated is that Yamal is fairly

00:26:31,940 --> 00:26:36,380
terrible to work with you know

00:26:33,980 --> 00:26:40,669
especially when you start creating

00:26:36,380 --> 00:26:44,239
complex objects for complex operators

00:26:40,669 --> 00:26:46,970
using using llamo you could clearly end

00:26:44,239 --> 00:26:49,669
up with a lot of Y Amal there and it

00:26:46,970 --> 00:26:52,190
rapidly becomes pretty unreadable and

00:26:49,669 --> 00:26:55,580
kuda uses customized which is another

00:26:52,190 --> 00:26:57,499
kubernetes project under the hood to

00:26:55,580 --> 00:26:59,929
patch in and template y Amal so we're

00:26:57,499 --> 00:27:03,320
kind of exposed to some of the problems

00:26:59,929 --> 00:27:05,029
that exist in in in in customize itself

00:27:03,320 --> 00:27:08,239
you know merge strategies can be quite

00:27:05,029 --> 00:27:10,970
complicated and mustache templating on

00:27:08,239 --> 00:27:13,070
top of on top of customized can also be

00:27:10,970 --> 00:27:15,590
fairly confusing customizer was

00:27:13,070 --> 00:27:17,480
originally built as a CLI tool so it

00:27:15,590 --> 00:27:19,669
doesn't quite map into the paradigm that

00:27:17,480 --> 00:27:24,109
that Kudo is trying to do to use it

00:27:19,669 --> 00:27:27,950
inside an application and so one of the

00:27:24,109 --> 00:27:32,570
emerging solutions to all this yeah

00:27:27,950 --> 00:27:35,690
Melin in kubernetes is q.q stands for

00:27:32,570 --> 00:27:37,850
configure unify execute it's a project

00:27:35,690 --> 00:27:40,909
that originally originated at Google and

00:27:37,850 --> 00:27:43,009
it's basically a kind of JSON superset

00:27:40,909 --> 00:27:45,259
that was influenced by their internal

00:27:43,009 --> 00:27:48,649
configuration language for borg which is

00:27:45,259 --> 00:27:51,470
the the internal container Orchestrator

00:27:48,649 --> 00:27:54,350
that google use and in practice it ends

00:27:51,470 --> 00:27:59,690
up looking a bit more like a DSL than

00:27:54,350 --> 00:28:01,279
than than Yammer and using Q we should

00:27:59,690 --> 00:28:04,460
be able to get to this kind of thing

00:28:01,279 --> 00:28:07,609
where we can define define an operator

00:28:04,460 --> 00:28:10,129
much more simply and it looks a lot like

00:28:07,609 --> 00:28:12,619
nice to JSON I when I look at this stuff

00:28:10,129 --> 00:28:14,239
this reminds me a lot of puppet so you

00:28:12,619 --> 00:28:15,799
know I think it's a lot easier to look

00:28:14,239 --> 00:28:20,299
at that kind of code than it is to look

00:28:15,799 --> 00:28:22,129
at massive massive sheets of of Yama and

00:28:20,299 --> 00:28:23,889
so here's a more complete example you

00:28:22,129 --> 00:28:27,320
probably can't read that from there but

00:28:23,889 --> 00:28:29,269
you know basically that kind of that

00:28:27,320 --> 00:28:35,600
would be two or three times as long if

00:28:29,269 --> 00:28:37,549
we had to define it all in Yama so he's

00:28:35,600 --> 00:28:40,460
exciting bit let's see whether this

00:28:37,549 --> 00:28:43,179
works and just give me two seconds to

00:28:40,460 --> 00:28:43,179
just switch this around

00:28:55,830 --> 00:29:03,210
let's see where the demo gods are

00:28:57,750 --> 00:29:06,680
shining on us today all right Mira okay

00:29:03,210 --> 00:29:06,680
it's looking quite good

00:29:14,070 --> 00:29:17,430
okay how's that exercise for everyone

00:29:16,170 --> 00:29:28,560
you see we're gonna make that a bit

00:29:17,430 --> 00:29:32,040
bigger right so so here I've got mini

00:29:28,560 --> 00:29:33,750
cube I'm running on my on my laptop you

00:29:32,040 --> 00:29:38,040
shouldn't have anything running in it

00:29:33,750 --> 00:29:42,450
it's completely empty and we can see if

00:29:38,040 --> 00:29:44,790
we if we look at frameworks that

00:29:42,450 --> 00:29:46,920
kubernetes right now just doesn't have a

00:29:44,790 --> 00:29:51,140
clue what a framework is so the first

00:29:46,920 --> 00:29:54,630
thing we need to do is to to install our

00:29:51,140 --> 00:29:57,390
frameworks er DS or kudos er DS which

00:29:54,630 --> 00:30:01,020
will effectively instantiate those

00:29:57,390 --> 00:30:02,910
things within the kubernetes cluster and

00:30:01,020 --> 00:30:05,070
so here you know those are the four

00:30:02,910 --> 00:30:07,260
things that we talked about plan

00:30:05,070 --> 00:30:10,800
executions instances frameworks and

00:30:07,260 --> 00:30:14,670
framework versions and so um and then we

00:30:10,800 --> 00:30:16,860
also want to to run our operator I'm

00:30:14,670 --> 00:30:20,970
just running this locally from my from

00:30:16,860 --> 00:30:24,120
my local github check out but at this

00:30:20,970 --> 00:30:27,540
point now if I run my get frameworks

00:30:24,120 --> 00:30:31,110
again so now kubernetes knows that a

00:30:27,540 --> 00:30:34,770
thing like a framework actually exists

00:30:31,110 --> 00:30:38,900
and so at this point we can start to we

00:30:34,770 --> 00:30:41,700
can start to actually do some stuff so

00:30:38,900 --> 00:30:44,370
we're going to just install a zookeeper

00:30:41,700 --> 00:30:49,770
cluster to start with and you know if we

00:30:44,370 --> 00:30:51,900
look at the Yamal for this so that's the

00:30:49,770 --> 00:30:53,910
the framework Djamel again it's super

00:30:51,900 --> 00:30:58,050
simple just says basically that this

00:30:53,910 --> 00:31:00,500
applies to the kudo api and so we can

00:30:58,050 --> 00:31:00,500
just do

00:31:08,990 --> 00:31:14,960
and now we know that we've got a

00:31:11,029 --> 00:31:21,320
zookeeper framework installed and if we

00:31:14,960 --> 00:31:24,710
look at version dot yeah more this is

00:31:21,320 --> 00:31:26,419
obviously a lot more stuff in here so

00:31:24,710 --> 00:31:30,740
we've got a bunch of parameters that are

00:31:26,419 --> 00:31:32,330
defined that we can then use later and

00:31:30,740 --> 00:31:35,270
we can use them when we define an

00:31:32,330 --> 00:31:37,460
instance with setting out what our

00:31:35,270 --> 00:31:39,470
templates are here so we've got our

00:31:37,460 --> 00:31:42,320
services Djamel we've got our pod

00:31:39,470 --> 00:31:44,600
disruption budget yeah Mille and we've

00:31:42,320 --> 00:31:51,279
got our stateful set which is going to

00:31:44,600 --> 00:31:56,779
give us our our zookeeper cluster and

00:31:51,279 --> 00:31:59,450
then down here it's where we start to

00:31:56,779 --> 00:32:02,090
get into the kudos specific stuff so

00:31:59,450 --> 00:32:04,039
we're defining for each of our tasks

00:32:02,090 --> 00:32:06,320
that we want to we're going to want to

00:32:04,039 --> 00:32:08,960
run and which of those yeah Mille files

00:32:06,320 --> 00:32:11,299
are going to be are going to be used to

00:32:08,960 --> 00:32:12,919
handle that task and then right at the

00:32:11,299 --> 00:32:15,649
bottom here we've got what our plans are

00:32:12,919 --> 00:32:18,080
so here we can see our deploy plan it's

00:32:15,649 --> 00:32:20,029
got a serial strategy but it's only got

00:32:18,080 --> 00:32:24,080
it's only got a single phase there

00:32:20,029 --> 00:32:26,179
anyway but the the phase can actually be

00:32:24,080 --> 00:32:29,470
implemented in parallel and then we have

00:32:26,179 --> 00:32:29,470
a validation plan as well

00:32:38,650 --> 00:32:47,920
right so install that one and then when

00:32:46,120 --> 00:32:49,090
we want an actual zookeeper cluster

00:32:47,920 --> 00:32:52,750
we're going to use the zookeeper

00:32:49,090 --> 00:32:54,870
instance yeah mo and that you can see

00:32:52,750 --> 00:32:59,980
we're just overriding their the the

00:32:54,870 --> 00:33:03,840
number of the zookeeper CPU setting and

00:32:59,980 --> 00:33:03,840
so hopefully if we know

00:33:11,340 --> 00:33:16,539
and so we can see there that what's

00:33:13,450 --> 00:33:18,809
happening is that the kubernetes is now

00:33:16,539 --> 00:33:21,220
deploying those those zookeeper pods

00:33:18,809 --> 00:33:35,820
that's going to build us a zookeeper

00:33:21,220 --> 00:33:35,820
cluster just give that a second okay

00:33:44,300 --> 00:33:48,960
so we can think we've now got an

00:33:46,230 --> 00:33:53,250
instance of our particular zookeeper

00:33:48,960 --> 00:33:55,430
framework and we're now going to install

00:33:53,250 --> 00:34:02,480
Kafka cluster as well calf car obviously

00:33:55,430 --> 00:34:02,480
relies on on on zookeeper so

00:34:19,100 --> 00:34:24,550
so we can see Kubiak's now knows about

00:34:21,350 --> 00:34:24,550
our Kafka framework

00:34:34,700 --> 00:34:40,309
and it's now starting to deploy

00:34:36,849 --> 00:34:46,579
hopefully a 3 node 3 broker kafka

00:34:40,309 --> 00:34:49,210
cluster I think my internet connection

00:34:46,579 --> 00:34:49,210
so it's slowing

00:35:03,690 --> 00:35:08,700
let's come back to that one in a second

00:35:05,910 --> 00:35:13,440
and so the other example that have got

00:35:08,700 --> 00:35:15,750
here is for my sequel framework and this

00:35:13,440 --> 00:35:21,480
might single framework has a some

00:35:15,750 --> 00:35:26,130
slightly more complicated plans for both

00:35:21,480 --> 00:35:28,740
backup and restore and so we can start

00:35:26,130 --> 00:35:30,720
up my sequel we can take a backup using

00:35:28,740 --> 00:35:33,480
the kubernetes api s and then restore

00:35:30,720 --> 00:35:42,780
that back up and back into that that

00:35:33,480 --> 00:35:44,640
particular my sequel instance enough CPU

00:35:42,780 --> 00:35:49,049
and RAM in my mini cube to deploy all

00:35:44,640 --> 00:35:51,450
this at the same time so we can see

00:35:49,049 --> 00:35:53,309
there that that that's deployed the

00:35:51,450 --> 00:36:01,650
framework framework version and instance

00:35:53,309 --> 00:36:03,329
all together in a single Yama file we

00:36:01,650 --> 00:36:06,150
can see our calf gate clusters now now

00:36:03,329 --> 00:36:10,789
up and running and we're just waiting

00:36:06,150 --> 00:36:10,789
for our minus QL now

00:36:35,609 --> 00:36:40,109
okay so that's all up and running now so

00:36:38,489 --> 00:36:43,219
at this point I want to use my cheat

00:36:40,109 --> 00:36:46,950
sheet for this because I've got some

00:36:43,219 --> 00:36:48,719
variables that I need to sit so I'm just

00:36:46,950 --> 00:36:55,709
gonna set and then var there to make it

00:36:48,719 --> 00:36:58,200
easy and in that in that MySQL

00:36:55,709 --> 00:37:02,999
deployment we've just got a single table

00:36:58,200 --> 00:37:06,619
this example table and we're just gonna

00:37:02,999 --> 00:37:06,619
stick some data in there

00:37:10,700 --> 00:37:13,480
okay

00:37:21,080 --> 00:37:28,280
right so there we can see we've got

00:37:22,760 --> 00:37:31,540
three rows in that in that table so now

00:37:28,280 --> 00:37:37,280
we're going to take a backup of of that

00:37:31,540 --> 00:37:39,380
and I've got another gamal here that's

00:37:37,280 --> 00:37:42,140
just a plan execution and it's just

00:37:39,380 --> 00:37:45,220
going to execute the back-up plan for

00:37:42,140 --> 00:37:45,220
that mysql framework

00:37:54,600 --> 00:38:00,620
and we can see that starting to run and

00:37:57,120 --> 00:38:00,620
now that's completed

00:38:06,790 --> 00:38:12,280
so they have just deleted all of the all

00:38:10,270 --> 00:38:14,380
of the stuff from that from that

00:38:12,280 --> 00:38:18,400
particular table so you can see it's

00:38:14,380 --> 00:38:21,670
completely empty and now I've got a

00:38:18,400 --> 00:38:27,600
similar plan execution yeah mph or the

00:38:21,670 --> 00:38:27,600
restore so we'll just apply that

00:38:34,200 --> 00:38:41,050
and that's already already got I already

00:38:37,599 --> 00:38:44,339
happened and now if we do that we should

00:38:41,050 --> 00:38:44,339
see the data back there again okay

00:38:47,310 --> 00:38:59,470
so so we really like other people to get

00:38:57,609 --> 00:39:01,150
involved in in Kudo if any of this has

00:38:59,470 --> 00:39:03,700
been interesting there's a whole bunch

00:39:01,150 --> 00:39:08,680
of community resources we have the CUDA

00:39:03,700 --> 00:39:11,800
website at kudo dev and you can see the

00:39:08,680 --> 00:39:13,990
the code on on github we also have the

00:39:11,800 --> 00:39:17,440
hash kudo channel on the kubernetes

00:39:13,990 --> 00:39:19,599
slack and there is a mailing list and we

00:39:17,440 --> 00:39:21,910
also have a weekly community meeting

00:39:19,599 --> 00:39:24,910
which is held on zoom all the details

00:39:21,910 --> 00:39:26,050
for that are on the cuda website so

00:39:24,910 --> 00:39:30,510
thank you very much for listening

00:39:26,050 --> 00:39:30,510
i'm happy to to answer any questions

00:39:31,110 --> 00:39:39,369
[Applause]

00:39:35,730 --> 00:39:41,859
okay thank you again but I really liked

00:39:39,369 --> 00:39:46,060
the naming of your MacBook I have to

00:39:41,859 --> 00:39:48,420
admit okay are there any questions in

00:39:46,060 --> 00:39:48,420
the audience

00:39:50,520 --> 00:39:57,640
none so far I can see well I guess

00:39:55,630 --> 00:39:59,920
things met thank you we'll see you at

00:39:57,640 --> 00:40:03,990
the evening event yeah hopefully yep

00:39:59,920 --> 00:40:03,990
yeah nice thanks folks

00:40:04,300 --> 00:40:09,439
[Applause]

00:40:12,010 --> 00:40:16,099

YouTube URL: https://www.youtube.com/watch?v=qAUmRfbd300


