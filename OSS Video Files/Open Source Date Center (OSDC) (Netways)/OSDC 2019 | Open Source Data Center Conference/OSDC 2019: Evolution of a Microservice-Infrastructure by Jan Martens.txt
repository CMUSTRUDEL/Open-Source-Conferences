Title: OSDC 2019: Evolution of a Microservice-Infrastructure by Jan Martens
Publication date: 2019-05-21
Playlist: OSDC 2019 | Open Source Data Center Conference
Description: 
	This talk is about our journey from Nginx & Docker Swarm to Traefik & Nomad. With the increase of load & traffic on our container-environment over the time, we experienced some issues that were unnoticed when built the environment. Due to the way we dynamically configured Nginx with Consul-Template we started to experience a lot of dropped keepalive connections and connection resets. Also the traffic wasnâ€™t distributed evenly throughout our container-infrastructure which led to single instances receiving most of the load. This is why made the decision to change to a reverse-proxy that can get dynamically configured. We were aware of the shortcommings of Docker Swarm (standalone) and seeked for a tool that would allow us to distribute containers more evenly without totally reconstructing everything and provide us with self healing capabilities. Performing these changes under the hood, transparent for our developers, was one of our key objectives.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:01,140 --> 00:00:10,550
[Music]

00:00:15,680 --> 00:00:19,859
hello good morning everyone can you hear

00:00:18,330 --> 00:00:20,699
me all right to the microphone working

00:00:19,859 --> 00:00:22,439
great

00:00:20,699 --> 00:00:26,730
cool so first talk of today very

00:00:22,439 --> 00:00:29,580
exciting so my name is young Martin sir

00:00:26,730 --> 00:00:30,570
I work as a DevOps engineer at River

00:00:29,580 --> 00:00:33,149
digital

00:00:30,570 --> 00:00:36,090
last year my colleague Paul was already

00:00:33,149 --> 00:00:39,090
here and gave a talk about our journey

00:00:36,090 --> 00:00:41,520
from a shop monolith towards a micro

00:00:39,090 --> 00:00:44,430
service environment where he explained a

00:00:41,520 --> 00:00:46,230
lot about the architectural decisions

00:00:44,430 --> 00:00:48,539
that we made in how we build micro

00:00:46,230 --> 00:00:50,850
services and today I would like to

00:00:48,539 --> 00:00:52,890
continue on the top this topic a little

00:00:50,850 --> 00:00:54,899
bit and explain to you how does this

00:00:52,890 --> 00:00:58,260
environment actually look like that we

00:00:54,899 --> 00:01:00,300
built how do we do deployments how do we

00:00:58,260 --> 00:01:03,480
do requests routing in our environment

00:01:00,300 --> 00:01:06,119
and tell you about some of the problems

00:01:03,480 --> 00:01:08,310
that we experienced once our environment

00:01:06,119 --> 00:01:11,960
got bigger and bigger and how we

00:01:08,310 --> 00:01:20,040
hopefully are always solve them later on

00:01:11,960 --> 00:01:22,350
so does this clicker work no yes so the

00:01:20,040 --> 00:01:25,040
company I work at David digital is part

00:01:22,350 --> 00:01:27,570
of the so-called rave group which is a

00:01:25,040 --> 00:01:30,860
holding that is much more than just a

00:01:27,570 --> 00:01:33,479
supermarket so last year we had over 75

00:01:30,860 --> 00:01:36,570
billion euros and turnover we have more

00:01:33,479 --> 00:01:41,939
than 300 45,000 employees and all over

00:01:36,570 --> 00:01:44,280
Europe and about over 15,000 shops we do

00:01:41,939 --> 00:01:47,310
not only work in food ray type but we

00:01:44,280 --> 00:01:50,670
also have companies that to tourism and

00:01:47,310 --> 00:01:52,290
DIY markets so here I can see a list of

00:01:50,670 --> 00:01:54,750
all the companies that actually belong

00:01:52,290 --> 00:01:57,030
to the labor groups or River of course

00:01:54,750 --> 00:02:00,240
they are touristic penny markets

00:01:57,030 --> 00:02:03,450
toom DIY markets people knockoff Villa

00:02:00,240 --> 00:02:05,399
and not yes so actually it's quite quite

00:02:03,450 --> 00:02:07,950
a lot of companies that belong to the

00:02:05,399 --> 00:02:11,150
little group and then there of course we

00:02:07,950 --> 00:02:13,080
as really digital we were founded in

00:02:11,150 --> 00:02:15,660
2014 to

00:02:13,080 --> 00:02:18,480
move the digitalization of the river

00:02:15,660 --> 00:02:22,110
group forward and we ourselves we have

00:02:18,480 --> 00:02:24,690
offices in Cologne here in Berlin in M&R

00:02:22,110 --> 00:02:29,130
which is in eastern Germany and in Sofia

00:02:24,690 --> 00:02:32,400
in Bulgaria so what do we as really to

00:02:29,130 --> 00:02:33,150
tell actually run we are responsible for

00:02:32,400 --> 00:02:35,600
the river

00:02:33,150 --> 00:02:38,840
online shop the rave website and the

00:02:35,600 --> 00:02:42,180
food delivery service that comes with it

00:02:38,840 --> 00:02:45,810
this is our main business also the

00:02:42,180 --> 00:02:48,000
mobile apps are programmed by us where

00:02:45,810 --> 00:02:51,210
you can also order your groceries from

00:02:48,000 --> 00:02:52,980
your mobile phone so before we get

00:02:51,210 --> 00:02:54,780
started with our environment I will

00:02:52,980 --> 00:02:57,810
first show you a little bit of our

00:02:54,780 --> 00:03:01,080
history which is quite interesting and

00:02:57,810 --> 00:03:04,290
also responsible for some of the

00:03:01,080 --> 00:03:07,830
problems we experienced later on so as I

00:03:04,290 --> 00:03:08,850
already said we were founded in June of

00:03:07,830 --> 00:03:12,450
00:03:08,850 --> 00:03:14,430
back at the time we took over a shop

00:03:12,450 --> 00:03:18,780
monolith that was built by an external

00:03:14,430 --> 00:03:21,150
partner and this at the time was running

00:03:18,780 --> 00:03:24,000
reasonably so people could order their

00:03:21,150 --> 00:03:27,060
stuff it would get delivered and things

00:03:24,000 --> 00:03:30,240
were generally well but we were tasked

00:03:27,060 --> 00:03:33,180
with how to actually scale this delivery

00:03:30,240 --> 00:03:35,700
service throughout Germany so that's why

00:03:33,180 --> 00:03:39,180
we hired our first platform architect in

00:03:35,700 --> 00:03:41,070
the end of 2014 who thought about how we

00:03:39,180 --> 00:03:43,410
how we could do this how can we stay

00:03:41,070 --> 00:03:45,900
scale this delivery service throughout

00:03:43,410 --> 00:03:49,050
Germany and the solution that we came up

00:03:45,900 --> 00:03:52,440
with it with at the time was ok let's

00:03:49,050 --> 00:03:54,500
let's do micro services so we hope that

00:03:52,440 --> 00:04:00,600
by using micro services we can really

00:03:54,500 --> 00:04:05,880
scale fast and deliver our product

00:04:00,600 --> 00:04:11,730
quickly with micro services we

00:04:05,880 --> 00:04:14,370
introduced containers in 2015 so we

00:04:11,730 --> 00:04:16,560
decided ok let's let's use docker and

00:04:14,370 --> 00:04:18,780
des let's use chakra swarm to actually

00:04:16,560 --> 00:04:21,450
deploy our micro services in our

00:04:18,780 --> 00:04:23,669
environment the first product that

00:04:21,450 --> 00:04:27,060
actually went live using only micro

00:04:23,669 --> 00:04:29,580
services was our shopping app in 2016

00:04:27,060 --> 00:04:32,940
back at the time there were about 20

00:04:29,580 --> 00:04:35,340
teams involved and yeah it was running

00:04:32,940 --> 00:04:40,200
solely on our new micro services which

00:04:35,340 --> 00:04:42,690
was very nice and at the end of 2016 we

00:04:40,200 --> 00:04:45,930
were already about 30 development teams

00:04:42,690 --> 00:04:51,810
and decided okay let's split up a little

00:04:45,930 --> 00:04:54,120
bit in different domains so we decided

00:04:51,810 --> 00:04:55,860
okay let's split our environment in

00:04:54,120 --> 00:04:57,750
three parts we would split it in

00:04:55,860 --> 00:05:00,590
everything ecommerce related so

00:04:57,750 --> 00:05:03,660
everything that concerns itself with

00:05:00,590 --> 00:05:07,080
ordering in the shop and so on then we

00:05:03,660 --> 00:05:08,880
had a fulfillment part that programs

00:05:07,080 --> 00:05:10,950
everything when it comes to actually

00:05:08,880 --> 00:05:14,550
delivering your groceries to your home

00:05:10,950 --> 00:05:24,090
and we also split up a big data part

00:05:14,550 --> 00:05:26,490
that does big data stuff also in 2017 we

00:05:24,090 --> 00:05:28,950
then had the launch of our revenue a

00:05:26,490 --> 00:05:32,070
marketplace as in better which was super

00:05:28,950 --> 00:05:34,230
nice because we disallowed external

00:05:32,070 --> 00:05:37,950
partners to also sell their products in

00:05:34,230 --> 00:05:40,740
our webshop so if you search our webshop

00:05:37,950 --> 00:05:44,010
you can sometimes find products from

00:05:40,740 --> 00:05:45,540
Butler's for example that sell their

00:05:44,010 --> 00:05:48,600
products through our online shop and

00:05:45,540 --> 00:05:52,920
they will get delivered by our delivery

00:05:48,600 --> 00:05:55,410
drivers as well and in the end of 2018

00:05:52,920 --> 00:05:57,840
we launched our first food fulfillment

00:05:55,410 --> 00:06:00,840
center 2.0 in the north of cologne which

00:05:57,840 --> 00:06:03,440
was which is partly automated which was

00:06:00,840 --> 00:06:05,700
a huge step forward for us because our

00:06:03,440 --> 00:06:08,640
1.0 fulfillment centers were all

00:06:05,700 --> 00:06:11,850
manually labor and now we have a partly

00:06:08,640 --> 00:06:15,840
automated solution which can which

00:06:11,850 --> 00:06:20,930
actually is responsible for all the

00:06:15,840 --> 00:06:23,280
deliveries in the room of Cologne so

00:06:20,930 --> 00:06:25,410
service wise this looked a little bit

00:06:23,280 --> 00:06:28,290
like this so as I said in the beginning

00:06:25,410 --> 00:06:29,910
we were only one two teams with one

00:06:28,290 --> 00:06:30,510
service and then we really really

00:06:29,910 --> 00:06:35,340
quickly

00:06:30,510 --> 00:06:38,669
scared up to almost 50 teams in 2018

00:06:35,340 --> 00:06:40,270
running in total about 270 micro

00:06:38,669 --> 00:06:44,320
services

00:06:40,270 --> 00:06:47,260
due to this rather fast growth we were

00:06:44,320 --> 00:06:49,510
not really able to establish this some

00:06:47,260 --> 00:06:51,940
kind of DevOps culture so we still have

00:06:49,510 --> 00:06:54,880
dedicated platform teams running the

00:06:51,940 --> 00:06:57,160
whole thing the whole thing and I work

00:06:54,880 --> 00:07:01,060
in the platform team that runs our

00:06:57,160 --> 00:07:02,830
e-commerce environment so that's what we

00:07:01,060 --> 00:07:04,720
are going to talk that's what I'm going

00:07:02,830 --> 00:07:07,510
to talk about today how does this

00:07:04,720 --> 00:07:08,880
ecommerce environment look like and how

00:07:07,510 --> 00:07:13,539
does it work

00:07:08,880 --> 00:07:16,090
so to recap a little bit we are

00:07:13,539 --> 00:07:19,060
operating a custom docker environment

00:07:16,090 --> 00:07:22,199
consisting of all of these tools so we

00:07:19,060 --> 00:07:25,600
have docker for containers docker swarm

00:07:22,199 --> 00:07:27,250
for actually scheduling them we use a

00:07:25,600 --> 00:07:31,169
Hershey cob console for service

00:07:27,250 --> 00:07:34,060
discovery and also console template for

00:07:31,169 --> 00:07:37,060
configuration rendering we rely heavily

00:07:34,060 --> 00:07:42,550
on elasticsearch slack we use we used

00:07:37,060 --> 00:07:45,220
nginx as a reverse proxy this purplish

00:07:42,550 --> 00:07:49,720
logo down there is actually the DNS mask

00:07:45,220 --> 00:07:54,099
logo I didn't know that one before what

00:07:49,720 --> 00:07:55,599
they we said well and all our virtual

00:07:54,099 --> 00:07:59,080
machines or bare metal servers are

00:07:55,599 --> 00:08:02,349
running Debian and yeah at the time in

00:07:59,080 --> 00:08:04,630
the end of 2018 basically everything was

00:08:02,349 --> 00:08:06,940
cool our environment worked very nice

00:08:04,630 --> 00:08:08,949
developers could bring cold life and

00:08:06,940 --> 00:08:12,039
everything was well or at least we

00:08:08,949 --> 00:08:13,870
thought it was and also yeah the shop

00:08:12,039 --> 00:08:15,789
model is was still running in some parts

00:08:13,870 --> 00:08:18,760
so didn't really managed to get rid of

00:08:15,789 --> 00:08:24,039
this completely but we were working on

00:08:18,760 --> 00:08:25,720
it so how does the architecture of this

00:08:24,039 --> 00:08:29,590
environment look like this is of course

00:08:25,720 --> 00:08:32,260
a very basic high-level overview we have

00:08:29,590 --> 00:08:35,169
dedicated ingress nodes that run an

00:08:32,260 --> 00:08:37,089
engine eeks whose configuration gets

00:08:35,169 --> 00:08:40,089
written dynamically by console template

00:08:37,089 --> 00:08:42,279
and they are responsible for routing

00:08:40,089 --> 00:08:45,339
external host names into our environment

00:08:42,279 --> 00:08:48,100
so basically after a few hops your

00:08:45,339 --> 00:08:50,520
requests or every day will end up on

00:08:48,100 --> 00:08:53,980
these ingress nodes which will then

00:08:50,520 --> 00:08:56,470
forward them through to the

00:08:53,980 --> 00:08:59,079
corresponding containers then we have

00:08:56,470 --> 00:09:02,649
docker dedicated docker host or worker

00:08:59,079 --> 00:09:05,920
notes that also run an engine X which is

00:09:02,649 --> 00:09:09,279
configured dynamically and there we only

00:09:05,920 --> 00:09:13,630
route internal host names and of course

00:09:09,279 --> 00:09:16,630
they run the containers themselves then

00:09:13,630 --> 00:09:20,550
we have a set of console server or

00:09:16,630 --> 00:09:25,089
master nodes that act as console masters

00:09:20,550 --> 00:09:27,699
and also as sorry as swarm masters so

00:09:25,089 --> 00:09:29,949
from there on we start all of our

00:09:27,699 --> 00:09:32,560
deployments from there everything is

00:09:29,949 --> 00:09:34,930
more less centrally managed and they

00:09:32,560 --> 00:09:37,060
also act as a DNS service for our

00:09:34,930 --> 00:09:39,250
service discovery so if you want to

00:09:37,060 --> 00:09:42,190
address a service you ask the Khans room

00:09:39,250 --> 00:09:44,139
as a service for the DNS records and it

00:09:42,190 --> 00:09:46,420
will then give you the corresponding I

00:09:44,139 --> 00:09:49,209
piece of the docker hosts where your

00:09:46,420 --> 00:09:51,070
service might be running and then of

00:09:49,209 --> 00:09:54,820
course we have some some other managed

00:09:51,070 --> 00:09:57,610
services like Kafka databases the

00:09:54,820 --> 00:10:00,850
elastic search steak from Ecco fauna and

00:09:57,610 --> 00:10:03,040
what else is needed for a team to run

00:10:00,850 --> 00:10:07,180
their service properly but we were not

00:10:03,040 --> 00:10:09,279
concerned with this today one word about

00:10:07,180 --> 00:10:11,920
Kafka if you're interested in that I

00:10:09,279 --> 00:10:13,779
invite you to have a look at the talk of

00:10:11,920 --> 00:10:16,000
my colleague port from last year you

00:10:13,779 --> 00:10:17,769
will find it in the OCC archive from

00:10:16,000 --> 00:10:20,170
last year and he talked a great deal

00:10:17,769 --> 00:10:22,209
about how we do eventing with Kafka so

00:10:20,170 --> 00:10:27,040
if this is interesting for you please

00:10:22,209 --> 00:10:32,110
have a look so how do we do deployments

00:10:27,040 --> 00:10:33,910
in our environment we do blue/green

00:10:32,110 --> 00:10:35,620
deployments or if you don't know the

00:10:33,910 --> 00:10:38,589
name it's also called active inactive

00:10:35,620 --> 00:10:41,470
deployments so a service is usually

00:10:38,589 --> 00:10:44,019
deployed in two versions of which only

00:10:41,470 --> 00:10:46,779
one can ever be active so only one of

00:10:44,019 --> 00:10:50,709
these versions will get requests routed

00:10:46,779 --> 00:10:53,589
to it and receive all the traffic these

00:10:50,709 --> 00:10:56,529
colors get started as an independent set

00:10:53,589 --> 00:10:59,500
of containers per color and in the

00:10:56,529 --> 00:11:01,209
background we use docker compose in

00:10:59,500 --> 00:11:05,040
combination with chakra swarm to start

00:11:01,209 --> 00:11:05,040
these sets of containers per color

00:11:06,350 --> 00:11:11,390
teams can deploy themselves so we

00:11:09,170 --> 00:11:14,420
provide them with centrally managed

00:11:11,390 --> 00:11:17,720
deployment jobs which they can use at

00:11:14,420 --> 00:11:20,510
their will to deploy at any time in our

00:11:17,720 --> 00:11:22,520
environments they can perform colors

00:11:20,510 --> 00:11:24,710
which is per service so there is no

00:11:22,520 --> 00:11:27,380
global state that now green or blue is

00:11:24,710 --> 00:11:29,120
active but they can decide for their

00:11:27,380 --> 00:11:33,530
service or for every of their services

00:11:29,120 --> 00:11:36,620
when they switch the color both colors

00:11:33,530 --> 00:11:38,930
actually get the same DNS record

00:11:36,620 --> 00:11:42,950
assigned so if I have my service running

00:11:38,930 --> 00:11:45,580
in a blue color and in green color and I

00:11:42,950 --> 00:11:49,160
ask console for a DNS record it will

00:11:45,580 --> 00:11:50,990
give me the IP of all hosts where this

00:11:49,160 --> 00:11:55,940
service is running in no matter which

00:11:50,990 --> 00:11:59,150
color which leads me to the next no it

00:11:55,940 --> 00:12:01,160
doesn't first we first let's have a look

00:11:59,150 --> 00:12:04,130
at how this deployment process work

00:12:01,160 --> 00:12:06,380
works and then afterwards I will talk a

00:12:04,130 --> 00:12:07,960
little bit about how the request routing

00:12:06,380 --> 00:12:10,970
works so how can we actually

00:12:07,960 --> 00:12:12,830
differentiate where our requests get

00:12:10,970 --> 00:12:17,110
routed to if we have the same DNS

00:12:12,830 --> 00:12:20,720
records for both service colors so a

00:12:17,110 --> 00:12:22,610
team typically has its own Jenkins

00:12:20,720 --> 00:12:25,940
instance which you can see on the upper

00:12:22,610 --> 00:12:30,440
left the centralized deployment tools

00:12:25,940 --> 00:12:34,130
that we use is written in ansible on the

00:12:30,440 --> 00:12:35,840
right hand side you can see console or

00:12:34,130 --> 00:12:38,510
the console master service and next to

00:12:35,840 --> 00:12:40,430
it a very simplified version of the

00:12:38,510 --> 00:12:43,240
console key value store with where we

00:12:40,430 --> 00:12:46,040
store some metadata about the service

00:12:43,240 --> 00:12:49,430
that we start and on the bottom you can

00:12:46,040 --> 00:12:52,430
see here also very simplified three

00:12:49,430 --> 00:12:54,410
docker hosts that are all running an

00:12:52,430 --> 00:12:57,830
engineer's instance and some service one

00:12:54,410 --> 00:13:02,930
and two one in blue and one in green

00:12:57,830 --> 00:13:05,600
every time so if I wanted to deploy a

00:13:02,930 --> 00:13:08,590
new service for example a basket service

00:13:05,600 --> 00:13:10,070
that handles everything basket related

00:13:08,590 --> 00:13:13,430
what would I do

00:13:10,070 --> 00:13:16,370
I will start the deployment playbook and

00:13:13,430 --> 00:13:18,020
say let's give it a description of my

00:13:16,370 --> 00:13:21,050
service so what is the name of my

00:13:18,020 --> 00:13:23,480
dr. image what is my service name should

00:13:21,050 --> 00:13:26,540
it be available externally and so on and

00:13:23,480 --> 00:13:29,930
so forth and start this playbook against

00:13:26,540 --> 00:13:33,830
the console service they would then as a

00:13:29,930 --> 00:13:36,730
first step start an instance or as many

00:13:33,830 --> 00:13:40,250
instances as I like of my basket service

00:13:36,730 --> 00:13:42,860
this was rather simple so it would dr.

00:13:40,250 --> 00:13:44,450
swarm would simply look okay which

00:13:42,860 --> 00:13:46,520
docker host has the least amount of

00:13:44,450 --> 00:13:48,649
containers running and simply start as

00:13:46,520 --> 00:13:51,770
many containers there and today this was

00:13:48,649 --> 00:13:56,390
even and only then distribute them host

00:13:51,770 --> 00:13:58,010
by host actually if no service was if no

00:13:56,390 --> 00:14:01,670
instance of my service was running

00:13:58,010 --> 00:14:04,220
beforehand we default to always deploy

00:14:01,670 --> 00:14:07,459
to the blue color this is why destroyers

00:14:04,220 --> 00:14:10,010
gets now started in the color blue so

00:14:07,459 --> 00:14:13,730
afterwards as the next step we would

00:14:10,010 --> 00:14:16,279
update the console key value store in

00:14:13,730 --> 00:14:17,839
this case because we always deploy to

00:14:16,279 --> 00:14:20,899
the inactive color we would of course

00:14:17,839 --> 00:14:22,550
right into the key value store okay the

00:14:20,899 --> 00:14:25,970
active color for my newly deployed

00:14:22,550 --> 00:14:29,420
service should be green this will then

00:14:25,970 --> 00:14:32,029
automatically trigger a reload of all of

00:14:29,420 --> 00:14:35,870
our engineers instances so konso

00:14:32,029 --> 00:14:38,720
template actually watches the state of

00:14:35,870 --> 00:14:41,060
the console service catalog and this

00:14:38,720 --> 00:14:43,670
console key value store and as soon as

00:14:41,060 --> 00:14:46,700
it detects changes it will automatically

00:14:43,670 --> 00:14:50,300
render the con and unix configuration on

00:14:46,700 --> 00:14:53,000
all hosts not only on these docker nodes

00:14:50,300 --> 00:14:57,470
but also simultaneously on our ingress

00:14:53,000 --> 00:15:01,070
nodes if I'm now certain that my service

00:14:57,470 --> 00:15:02,690
is running good and well I can start a

00:15:01,070 --> 00:15:05,779
second deployment playbook which will

00:15:02,690 --> 00:15:08,930
simply update the console key value

00:15:05,779 --> 00:15:11,000
store again and tell set the now active

00:15:08,930 --> 00:15:13,130
color to blue which will then again

00:15:11,000 --> 00:15:16,459
trigger every load of all nginx

00:15:13,130 --> 00:15:20,050
instances and from that point on I can

00:15:16,459 --> 00:15:22,130
reach my service and all of the blue

00:15:20,050 --> 00:15:27,050
instances that I just started will

00:15:22,130 --> 00:15:30,770
receive live traffic so here it is again

00:15:27,050 --> 00:15:31,820
in all its glory the very simplified

00:15:30,770 --> 00:15:36,230
diploma

00:15:31,820 --> 00:15:40,250
pipeline that we built so now how do I

00:15:36,230 --> 00:15:45,020
actually address the service that I that

00:15:40,250 --> 00:15:47,950
I just started so as I said both colors

00:15:45,020 --> 00:15:50,060
will receive the same DNS record because

00:15:47,950 --> 00:15:53,870
internally they are registered as the

00:15:50,060 --> 00:16:01,460
same service in the console service

00:15:53,870 --> 00:16:04,610
catalog so if I query console for the IP

00:16:01,460 --> 00:16:07,760
addresses of my service it will as

00:16:04,610 --> 00:16:10,100
already set give me all I P addresses of

00:16:07,760 --> 00:16:12,950
the service not only the host where

00:16:10,100 --> 00:16:15,560
negative instances running so we have to

00:16:12,950 --> 00:16:18,200
at some point different shade where to

00:16:15,560 --> 00:16:20,000
route our requests and we do this with

00:16:18,200 --> 00:16:24,920
the help of this nginx instance it is

00:16:20,000 --> 00:16:26,900
running on every worker node and we dare

00:16:24,920 --> 00:16:29,390
differentiate depending on the port that

00:16:26,900 --> 00:16:31,130
you used so let's have a look at how

00:16:29,390 --> 00:16:34,010
this looks on the left hand side you see

00:16:31,130 --> 00:16:36,620
me and on the right hand side those are

00:16:34,010 --> 00:16:40,310
the cons of service again and in the

00:16:36,620 --> 00:16:42,590
bottom the simplified rocker hosts so if

00:16:40,310 --> 00:16:44,570
I want to reach my new basket service

00:16:42,590 --> 00:16:46,730
that I just deployed I first asked

00:16:44,570 --> 00:16:49,040
console for the records it will tell me

00:16:46,730 --> 00:16:54,260
ok yeah it's running on docker host 1

00:16:49,040 --> 00:16:57,500
and 2 and then if I send a request on

00:16:54,260 --> 00:17:02,060
port 80 to docker host number one in

00:16:57,500 --> 00:17:04,430
this case the nginx will forward this

00:17:02,060 --> 00:17:08,420
request to the currently active color of

00:17:04,430 --> 00:17:11,570
my service so everything I throw an at

00:17:08,420 --> 00:17:14,959
port 80 will always route to the active

00:17:11,570 --> 00:17:17,390
version of my service so in this case

00:17:14,959 --> 00:17:21,290
it's not running on docker host 1 this

00:17:17,390 --> 00:17:23,570
this is why engine extent forwards it to

00:17:21,290 --> 00:17:26,990
the container running on the cross - and

00:17:23,570 --> 00:17:29,000
if I want it for some reason to reach

00:17:26,990 --> 00:17:31,370
the inactive color I could also do that

00:17:29,000 --> 00:17:33,830
for testing purposes for example and

00:17:31,370 --> 00:17:37,010
then I would just use port number 90 and

00:17:33,830 --> 00:17:41,720
nginx will then simply route my request

00:17:37,010 --> 00:17:44,510
to the green color of my service and in

00:17:41,720 --> 00:17:45,650
this case it also will always favor

00:17:44,510 --> 00:17:48,080
locally running

00:17:45,650 --> 00:17:50,120
docker container over a container that

00:17:48,080 --> 00:17:55,280
is running somewhere else to simply skip

00:17:50,120 --> 00:17:59,630
unnecessary network ops so here again

00:17:55,280 --> 00:18:02,650
the whole process how it works

00:17:59,630 --> 00:18:05,180
this recording by itself was working

00:18:02,650 --> 00:18:09,290
reasonably well so this was actually

00:18:05,180 --> 00:18:12,110
very solid but as you could or hopefully

00:18:09,290 --> 00:18:14,780
could see in the beginning slides our

00:18:12,110 --> 00:18:17,900
environment and our company grew quite a

00:18:14,780 --> 00:18:19,760
bit in in this few years and therefore

00:18:17,900 --> 00:18:23,830
our environment also grew quite a bit

00:18:19,760 --> 00:18:25,940
and in the beginning this request

00:18:23,830 --> 00:18:28,190
process that I just explained to you was

00:18:25,940 --> 00:18:30,440
working very well but the bigger our

00:18:28,190 --> 00:18:32,240
environment got the more problems we

00:18:30,440 --> 00:18:37,400
actually experienced with with the way

00:18:32,240 --> 00:18:39,230
we configure nginx because we saw an

00:18:37,400 --> 00:18:40,820
increasing amount of requests that

00:18:39,230 --> 00:18:43,910
actually never reached their

00:18:40,820 --> 00:18:45,410
destinations and if we tried to use

00:18:43,910 --> 00:18:47,510
keepalive connections towards our

00:18:45,410 --> 00:18:51,020
services they would get dropped very

00:18:47,510 --> 00:18:54,620
very fast and this always happened

00:18:51,020 --> 00:18:57,830
during deployments of any service but in

00:18:54,620 --> 00:18:59,360
worst cases every five seconds or so

00:18:57,830 --> 00:19:02,780
even if there were no deployments

00:18:59,360 --> 00:19:05,900
running and the underlying reason for

00:19:02,780 --> 00:19:08,390
this was that every time as I told you

00:19:05,900 --> 00:19:10,640
there was a state change in the console

00:19:08,390 --> 00:19:13,330
service catalog or a konso key value

00:19:10,640 --> 00:19:15,890
store konso template would immediately

00:19:13,330 --> 00:19:18,770
rerender this nginx configuration

00:19:15,890 --> 00:19:20,720
everywhere at the same time meaning all

00:19:18,770 --> 00:19:23,180
of our engineer instances in the

00:19:20,720 --> 00:19:24,830
environment but reload at the exact same

00:19:23,180 --> 00:19:28,430
time because engine eeks

00:19:24,830 --> 00:19:30,740
in the way we configured it was not able

00:19:28,430 --> 00:19:31,220
to gracefully handle configuration

00:19:30,740 --> 00:19:35,900
reloads

00:19:31,220 --> 00:19:38,600
so this was not so it's not so good

00:19:35,900 --> 00:19:40,400
because this would actually lead to

00:19:38,600 --> 00:19:43,430
in-flight requests that never reached

00:19:40,400 --> 00:19:46,700
the destinations to these keepalive

00:19:43,430 --> 00:19:51,620
connections that get dropped and so on

00:19:46,700 --> 00:19:53,990
so we with total selves okay how how can

00:19:51,620 --> 00:19:57,140
we solve this how because this was a

00:19:53,990 --> 00:19:59,570
huge problem for us how can we what can

00:19:57,140 --> 00:20:01,850
we do so we decided ok

00:19:59,570 --> 00:20:05,470
let's look for a different reverse proxy

00:20:01,850 --> 00:20:08,119
depth is actually able to get

00:20:05,470 --> 00:20:10,399
configuration real or dynamically and in

00:20:08,119 --> 00:20:13,399
the best case it will also be very nice

00:20:10,399 --> 00:20:15,919
if it can be configured to initely so

00:20:13,399 --> 00:20:18,669
our hope was that we could maybe also

00:20:15,919 --> 00:20:23,029
get rid of konso template with this and

00:20:18,669 --> 00:20:26,929
could simply point our rear what's it

00:20:23,029 --> 00:20:28,879
called reverse proxy to some data store

00:20:26,929 --> 00:20:31,570
for example econ so a service catalog

00:20:28,879 --> 00:20:34,149
and it will would generate routes

00:20:31,570 --> 00:20:37,399
automatically at least that was our hope

00:20:34,149 --> 00:20:41,529
what we wanted to have so there were

00:20:37,399 --> 00:20:44,299
some alternatives that we had a look at

00:20:41,529 --> 00:20:46,359
the first thing we looked at was envoy

00:20:44,299 --> 00:20:50,119
proxy or envoi I don't know how to

00:20:46,359 --> 00:20:53,749
pronounce it which you might know if you

00:20:50,119 --> 00:20:55,729
have to do with Sto or ambassador and

00:20:53,749 --> 00:20:59,389
kubernetes or if you had a look at

00:20:55,729 --> 00:21:03,169
console connect these all these products

00:20:59,389 --> 00:21:06,379
use invoke proxy to do their service

00:21:03,169 --> 00:21:09,320
routing but for us it quickly turned out

00:21:06,379 --> 00:21:11,690
that this was way too complex so envoi

00:21:09,320 --> 00:21:13,940
is a very cool product where you can

00:21:11,690 --> 00:21:16,220
build a lot of very fancy stuff with it

00:21:13,940 --> 00:21:20,749
but for us it was just too complex to

00:21:16,220 --> 00:21:23,330
set up so we disregarded this one we

00:21:20,749 --> 00:21:26,929
also had a look at Fabio Elbe which are

00:21:23,330 --> 00:21:29,479
Fabio Elbe which is a load balancer that

00:21:26,929 --> 00:21:32,269
was originally developed by eBay and

00:21:29,479 --> 00:21:35,090
which can be configured dynamically from

00:21:32,269 --> 00:21:37,009
the console service catalog so this was

00:21:35,090 --> 00:21:40,099
a very interesting candidate for us

00:21:37,009 --> 00:21:43,309
because as I said we wanted ideally to

00:21:40,099 --> 00:21:45,979
have our loop and configure itself but

00:21:43,309 --> 00:21:49,399
Fabio was missing some other features

00:21:45,979 --> 00:21:52,159
that we wanted so we also skipped that

00:21:49,399 --> 00:21:54,830
and then we come to our last candidate

00:21:52,159 --> 00:21:59,259
with of course the nicest logo of them

00:21:54,830 --> 00:22:02,179
all or not depending if you like offers

00:21:59,259 --> 00:22:05,809
traffic they call themselves the cloud

00:22:02,179 --> 00:22:09,109
native at rudra and this is actually

00:22:05,809 --> 00:22:11,389
built by design to configure itself

00:22:09,109 --> 00:22:13,440
dynamically so you can point traffic at

00:22:11,389 --> 00:22:17,250
a plethora of different

00:22:13,440 --> 00:22:19,380
backends and it can generate routes all

00:22:17,250 --> 00:22:22,230
by itself so for example you could point

00:22:19,380 --> 00:22:24,510
it at your local doctor ap iPod or your

00:22:22,230 --> 00:22:27,210
console service catalog you can point it

00:22:24,510 --> 00:22:30,060
at your kubernetes cluster and from

00:22:27,210 --> 00:22:32,520
there it can via some meta text that you

00:22:30,060 --> 00:22:37,230
have to provide general write routes all

00:22:32,520 --> 00:22:39,690
by itself this is super cool and it also

00:22:37,230 --> 00:22:42,630
has some nice other features that I will

00:22:39,690 --> 00:22:44,900
talk about now so we chose traffic as I

00:22:42,630 --> 00:22:48,150
said it can be dynamically configurable

00:22:44,900 --> 00:22:50,040
if you choose to not configure

00:22:48,150 --> 00:22:55,110
dynamically but use good old config

00:22:50,040 --> 00:22:56,670
files it can also hot reload them so if

00:22:55,110 --> 00:22:58,380
this file changes on disk

00:22:56,670 --> 00:23:00,150
traffic will automatically update its

00:22:58,380 --> 00:23:05,780
route and doesn't have to be reloaded

00:23:00,150 --> 00:23:09,510
which is also a big plus it has a lot of

00:23:05,780 --> 00:23:12,420
metrics which is super nice which I will

00:23:09,510 --> 00:23:15,150
show you in a few slides it has also a

00:23:12,420 --> 00:23:17,550
very nice web interface we can see the

00:23:15,150 --> 00:23:20,100
route set gets configured and it also

00:23:17,550 --> 00:23:23,970
runs as a single go binary so it is very

00:23:20,100 --> 00:23:26,610
very easy to set up and use so how did

00:23:23,970 --> 00:23:30,390
we go about actually changing our

00:23:26,610 --> 00:23:33,530
reverse proxy during live traffic what

00:23:30,390 --> 00:23:36,540
we saw here again you can see the

00:23:33,530 --> 00:23:38,520
simplified request routing diagram and

00:23:36,540 --> 00:23:41,130
what we did we would first install a

00:23:38,520 --> 00:23:43,740
traffic alongside nginx on all of our

00:23:41,130 --> 00:23:45,630
docker hosts and later also on all of

00:23:43,740 --> 00:23:49,200
our ingress nodes and we would at first

00:23:45,630 --> 00:23:51,390
have listen on a different part so for

00:23:49,200 --> 00:23:54,600
reactive port for example we used port

00:23:51,390 --> 00:23:57,300
number 10,080 instead of port 80 this

00:23:54,600 --> 00:23:58,860
way we could really thoroughly test all

00:23:57,300 --> 00:24:02,520
the routes that traffic would generate

00:23:58,860 --> 00:24:04,730
and at that point we would also see ok

00:24:02,520 --> 00:24:07,950
our wish to get rid of konso template

00:24:04,730 --> 00:24:10,260
sadly was impossible because of the

00:24:07,950 --> 00:24:12,540
decision that we made to actually have

00:24:10,260 --> 00:24:15,710
both colors of a service used to same

00:24:12,540 --> 00:24:18,510
DNS record this certainly made it

00:24:15,710 --> 00:24:20,400
impossible for us to use the dynamic

00:24:18,510 --> 00:24:23,370
configuration the traffic provides you

00:24:20,400 --> 00:24:27,390
with so we still use console template

00:24:23,370 --> 00:24:29,760
there but now we

00:24:27,390 --> 00:24:32,190
can have the configuration reloaded

00:24:29,760 --> 00:24:35,370
dynamically so once we were sure that

00:24:32,190 --> 00:24:39,450
everything worked we simply switch to

00:24:35,370 --> 00:24:41,790
port mapping host by host we could roll

00:24:39,450 --> 00:24:43,800
this back in case of emergency so if

00:24:41,790 --> 00:24:46,170
anything didn't work we could simply

00:24:43,800 --> 00:24:49,140
roll back our cannery deployment on

00:24:46,170 --> 00:24:52,050
single hosts fix a problem and try it

00:24:49,140 --> 00:24:54,540
again this worked super super nice for

00:24:52,050 --> 00:24:56,790
us and once we were certain that

00:24:54,540 --> 00:24:59,280
everything worked and after a reasonably

00:24:56,790 --> 00:25:02,880
amount of time we then simply

00:24:59,280 --> 00:25:04,520
uninstalled nginx and here had our

00:25:02,880 --> 00:25:07,350
reverse proxy change so this was

00:25:04,520 --> 00:25:11,670
surprisingly simple but very effective

00:25:07,350 --> 00:25:14,130
for us and now was traffic yeah all of

00:25:11,670 --> 00:25:16,020
these problems that I described more or

00:25:14,130 --> 00:25:18,720
less immediately went away so keep alive

00:25:16,020 --> 00:25:21,000
connections now really work which is

00:25:18,720 --> 00:25:23,610
super nice day the connection even holds

00:25:21,000 --> 00:25:26,790
if I redeploy the service that I'm

00:25:23,610 --> 00:25:28,590
connected to then for a short amount of

00:25:26,790 --> 00:25:31,530
time traffic will simply respond bits

00:25:28,590 --> 00:25:33,810
4/4 or whatever arrow is correct in that

00:25:31,530 --> 00:25:35,610
case but my keepalive connection will

00:25:33,810 --> 00:25:39,810
always hold it will not get dropped this

00:25:35,610 --> 00:25:43,110
is super nice because internally traffic

00:25:39,810 --> 00:25:47,100
decouples the listening on pause and the

00:25:43,110 --> 00:25:50,670
actual routing to the backends also we

00:25:47,100 --> 00:25:52,320
have almost real-time data about the

00:25:50,670 --> 00:25:54,800
response times or for services I will

00:25:52,320 --> 00:25:58,350
show you this in I think the next slide

00:25:54,800 --> 00:26:00,150
which is super nice so we built very

00:25:58,350 --> 00:26:02,370
nice graph on our dashboards on top of

00:26:00,150 --> 00:26:04,770
this where we connect we see in almost

00:26:02,370 --> 00:26:06,870
real-time how our services respond how

00:26:04,770 --> 00:26:10,290
fast they respond what kind of error

00:26:06,870 --> 00:26:12,870
codes we throw how was requests we

00:26:10,290 --> 00:26:15,600
receive and so on which is a huge plus

00:26:12,870 --> 00:26:18,210
for us because with nginx previously we

00:26:15,600 --> 00:26:20,940
had no metrics whatsoever so we were

00:26:18,210 --> 00:26:22,800
more or less flying blind we didn't know

00:26:20,940 --> 00:26:25,620
how fast all services where or how many

00:26:22,800 --> 00:26:28,890
arrows withdrew so this was a very nice

00:26:25,620 --> 00:26:31,110
added bonus for us and also traffic

00:26:28,890 --> 00:26:32,730
white writes very rich access locks in

00:26:31,110 --> 00:26:35,700
JSON format which are super nice to

00:26:32,730 --> 00:26:38,520
parse and provide you with a lot of lot

00:26:35,700 --> 00:26:40,260
of insights so here you can see the

00:26:38,520 --> 00:26:41,460
promise profile dashboard I took a

00:26:40,260 --> 00:26:43,860
screenshot

00:26:41,460 --> 00:26:46,800
some days ago in the evening so you can

00:26:43,860 --> 00:26:49,230
see all services are healthy which is

00:26:46,800 --> 00:26:51,360
always nice to see you can see the

00:26:49,230 --> 00:26:54,510
accumulated response time of all our

00:26:51,360 --> 00:26:57,420
services which updates every time from

00:26:54,510 --> 00:26:59,850
resource scrapes the traffic metrics so

00:26:57,420 --> 00:27:01,980
at that point was 11 milliseconds for

00:26:59,850 --> 00:27:05,100
our internal services which I guess is

00:27:01,980 --> 00:27:07,320
quite nice you can see the return code

00:27:05,100 --> 00:27:09,600
rate on the right hand side you can see

00:27:07,320 --> 00:27:11,880
the total request by service you can

00:27:09,600 --> 00:27:13,980
also see the black sheep on the left

00:27:11,880 --> 00:27:18,480
hand side so the slowest one at that

00:27:13,980 --> 00:27:21,360
time took 10 seconds to respond yeah so

00:27:18,480 --> 00:27:23,970
a lot of metrics very nice if you like

00:27:21,360 --> 00:27:27,630
metrics and I guess you do it's

00:27:23,970 --> 00:27:30,780
definitely worth it alone for 40 edit

00:27:27,630 --> 00:27:33,960
metrics here you can see a screenshot of

00:27:30,780 --> 00:27:35,490
this web interface that I just talked

00:27:33,960 --> 00:27:38,550
about so on the left hand side you can

00:27:35,490 --> 00:27:41,760
see the font ends as they are called

00:27:38,550 --> 00:27:45,480
so which hosts header do I listen to

00:27:41,760 --> 00:27:47,490
which parts do i which pots a map to

00:27:45,480 --> 00:27:49,800
restaurant end and then to which

00:27:47,490 --> 00:27:52,050
back-end toss this front and route is

00:27:49,800 --> 00:27:54,570
requests and on the right hand side you

00:27:52,050 --> 00:27:59,910
can see the actual back ends with a few

00:27:54,570 --> 00:28:03,380
service the different weight and some of

00:27:59,910 --> 00:28:08,880
the options that you can configure there

00:28:03,380 --> 00:28:11,940
so we changed our reverse proxy got rid

00:28:08,880 --> 00:28:14,610
of those problems and then we thought oh

00:28:11,940 --> 00:28:15,990
why well let's just change another core

00:28:14,610 --> 00:28:18,210
part of our infrastructure as well

00:28:15,990 --> 00:28:21,740
because why not it seemed to work out

00:28:18,210 --> 00:28:24,930
just fine and tokus warm for us was also

00:28:21,740 --> 00:28:29,670
a lot of pain often time so we decided

00:28:24,930 --> 00:28:31,710
ok let's let's go ahead and also look

00:28:29,670 --> 00:28:33,510
for a difference there because we also

00:28:31,710 --> 00:28:35,580
had a lot of problems that only got

00:28:33,510 --> 00:28:38,310
worse with the amount of services that

00:28:35,580 --> 00:28:41,190
were deployed in our environment because

00:28:38,310 --> 00:28:44,130
as I already mentioned a little bit the

00:28:41,190 --> 00:28:46,170
container spread of the very old

00:28:44,130 --> 00:28:49,380
standalone Dockers for version that we

00:28:46,170 --> 00:28:52,050
were using was very poor to put it

00:28:49,380 --> 00:28:54,410
nicely so we could end up with the

00:28:52,050 --> 00:28:57,110
situation that all contain

00:28:54,410 --> 00:29:01,880
of a service that was were deployed were

00:28:57,110 --> 00:29:05,120
started on the same host and it was not

00:29:01,880 --> 00:29:07,160
easy or it wasn't possible at all to do

00:29:05,120 --> 00:29:11,720
it automatically to move those

00:29:07,160 --> 00:29:14,690
containers to other hosts also we had no

00:29:11,720 --> 00:29:17,570
self-healing with this Turkish warm

00:29:14,690 --> 00:29:19,450
concept that we built so failed

00:29:17,570 --> 00:29:22,820
containers would get restarted which was

00:29:19,450 --> 00:29:25,610
reasonably nice but death was about it

00:29:22,820 --> 00:29:29,390
so if if a node went down for whatever

00:29:25,610 --> 00:29:31,280
reason toka swarm would not start all

00:29:29,390 --> 00:29:33,890
containers that run on this node

00:29:31,280 --> 00:29:36,020
somewhere else they were simply lost and

00:29:33,890 --> 00:29:39,470
we had to cope with this somehow so this

00:29:36,020 --> 00:29:41,990
was a huge problem and also there was no

00:29:39,470 --> 00:29:43,370
way for us to automatically drain a node

00:29:41,990 --> 00:29:45,710
if we had to take it down for

00:29:43,370 --> 00:29:47,660
maintenance reasons for example so we

00:29:45,710 --> 00:29:51,530
had to come up with a very tedious

00:29:47,660 --> 00:29:53,510
manual process to actually rescale every

00:29:51,530 --> 00:29:56,720
container that was running on that host

00:29:53,510 --> 00:29:58,880
somewhere else in the hope that it would

00:29:56,720 --> 00:30:01,130
work before we could take this note down

00:29:58,880 --> 00:30:05,290
so this was a huge pain for us and we

00:30:01,130 --> 00:30:08,420
decided okay why not switch it as well

00:30:05,290 --> 00:30:10,640
and there we looked for a different

00:30:08,420 --> 00:30:13,670
Orchestrator that could provide us with

00:30:10,640 --> 00:30:16,670
proper feeling so for example in case of

00:30:13,670 --> 00:30:18,920
a note outage it would be nice and we

00:30:16,670 --> 00:30:23,060
wanted to be able to ensure a proper

00:30:18,920 --> 00:30:25,700
container spread so what were the

00:30:23,060 --> 00:30:29,300
options on the table of course we could

00:30:25,700 --> 00:30:30,590
also always use the now integrated swarm

00:30:29,300 --> 00:30:33,200
mode that comes with docker

00:30:30,590 --> 00:30:36,050
but we were kind of hesitant to do that

00:30:33,200 --> 00:30:38,180
because our experience with the

00:30:36,050 --> 00:30:41,060
standalone swarm wasn't that nice and we

00:30:38,180 --> 00:30:42,980
decided ok let's just not do that and

00:30:41,060 --> 00:30:47,000
then there was of course the kubernetes

00:30:42,980 --> 00:30:50,840
option to run it by yourself or to use I

00:30:47,000 --> 00:30:54,380
managed provider like Rancher for this

00:30:50,840 --> 00:30:57,290
but it turned out that this was way to a

00:30:54,380 --> 00:30:59,690
huge step for us so we needed to change

00:30:57,290 --> 00:31:04,430
a lot in our environment to make this

00:30:59,690 --> 00:31:06,170
work and seeing that we ideally wanted

00:31:04,430 --> 00:31:08,330
to make this switch of our container

00:31:06,170 --> 00:31:10,220
Orchestrator completely and transfer

00:31:08,330 --> 00:31:13,220
and not only to our customers but also

00:31:10,220 --> 00:31:16,010
to our developers as well we said okay

00:31:13,220 --> 00:31:19,640
no let's not do kubernetes and that's

00:31:16,010 --> 00:31:25,490
why we decided we take a look at Hershey

00:31:19,640 --> 00:31:27,980
Corp no matter how chic of nomad is as

00:31:25,490 --> 00:31:30,200
the name might suggest developed by the

00:31:27,980 --> 00:31:33,320
same company that already developed

00:31:30,200 --> 00:31:36,110
console which we rely on heavily it has

00:31:33,320 --> 00:31:40,070
a seamless console integration which is

00:31:36,110 --> 00:31:43,399
super nice so you can basically start

00:31:40,070 --> 00:31:45,289
your normal program and tell it here is

00:31:43,399 --> 00:31:48,799
your console server please connect there

00:31:45,289 --> 00:31:51,200
and then it almost works instantly you

00:31:48,799 --> 00:31:54,440
don't have to do a lot of setting up

00:31:51,200 --> 00:31:57,049
this worked almost out of the box

00:31:54,440 --> 00:32:00,139
without a lot of configuration this was

00:31:57,049 --> 00:32:02,299
super nice it actually has proper safe

00:32:00,139 --> 00:32:04,580
feeling we thoroughly tested it so we

00:32:02,299 --> 00:32:06,860
could shoot down a note and it would

00:32:04,580 --> 00:32:10,010
immediately start up this containers

00:32:06,860 --> 00:32:13,480
everywhere else as we hoped it would it

00:32:10,010 --> 00:32:18,730
has been pecking enabled by default so

00:32:13,480 --> 00:32:22,639
contrary to the AutoCAD swarm if you

00:32:18,730 --> 00:32:24,250
yeah do not ensure that your containers

00:32:22,639 --> 00:32:27,200
are spread everywhere it will try to

00:32:24,250 --> 00:32:30,529
utilize the resources of your machines

00:32:27,200 --> 00:32:32,120
to the maximum which is especially nice

00:32:30,529 --> 00:32:34,460
if you are operating in a cloud

00:32:32,120 --> 00:32:37,399
environment where every megahertz costs

00:32:34,460 --> 00:32:39,679
more or less it is again a single go

00:32:37,399 --> 00:32:41,659
binary which was very easy to set up and

00:32:39,679 --> 00:32:43,850
it also has a very nice weapon surface

00:32:41,659 --> 00:32:45,980
where you can see the history of your

00:32:43,850 --> 00:32:48,409
jobs that you start with Nomad you can

00:32:45,980 --> 00:32:49,940
see what might have been reason why your

00:32:48,409 --> 00:32:52,399
containers crashed you can see locks

00:32:49,940 --> 00:32:55,669
directly in the web interface and so on

00:32:52,399 --> 00:33:00,950
so this is again a very nice added

00:32:55,669 --> 00:33:05,330
benefit how did we go on to change this

00:33:00,950 --> 00:33:07,429
in live traffic so this process was

00:33:05,330 --> 00:33:08,929
pretty similar to the way we would

00:33:07,429 --> 00:33:12,200
change the reverse proxy in the first

00:33:08,929 --> 00:33:14,269
place so we due to the fact that we had

00:33:12,200 --> 00:33:16,370
this centrally managed ensberg playbooks

00:33:14,269 --> 00:33:19,730
that developer said to you or have to

00:33:16,370 --> 00:33:21,650
use when deploying a service we simply

00:33:19,730 --> 00:33:24,410
change display books

00:33:21,650 --> 00:33:25,730
at some point to say okay if I now want

00:33:24,410 --> 00:33:29,630
to redeploy a service

00:33:25,730 --> 00:33:32,360
sorry if I want to redeploy my basket

00:33:29,630 --> 00:33:35,330
service it will first stop the old

00:33:32,360 --> 00:33:38,059
version running in raucous form and then

00:33:35,330 --> 00:33:40,280
start a new version running and normal I

00:33:38,059 --> 00:33:42,800
hope you can see the difference with a

00:33:40,280 --> 00:33:44,870
different shape and greenish color this

00:33:42,800 --> 00:33:48,250
Shila indicate that this is now a

00:33:44,870 --> 00:33:50,390
container started and docker swarm

00:33:48,250 --> 00:33:54,020
yeah and that was basically it

00:33:50,390 --> 00:33:57,230
so the cool very cool thing was that you

00:33:54,020 --> 00:34:01,250
can run docker swarm and Nomad alongside

00:33:57,230 --> 00:34:03,170
on on a docker host and Nomad doesn't

00:34:01,250 --> 00:34:05,780
really care about any workload that it

00:34:03,170 --> 00:34:08,510
doesn't start itself so you can

00:34:05,780 --> 00:34:10,580
gradually move stuff over from swamp to

00:34:08,510 --> 00:34:13,159
Nomad and no Matt will only care about

00:34:10,580 --> 00:34:14,929
everything that it started itself so

00:34:13,159 --> 00:34:19,280
they wouldn't get into each other's way

00:34:14,929 --> 00:34:21,919
which was super nice and then at some

00:34:19,280 --> 00:34:23,510
point after about I think it took us

00:34:21,919 --> 00:34:26,240
three weeks in our production

00:34:23,510 --> 00:34:29,810
environment to get all teams to redeploy

00:34:26,240 --> 00:34:32,330
their service there we had this

00:34:29,810 --> 00:34:34,940
situation that every container was

00:34:32,330 --> 00:34:39,429
started with Nomad and we could then

00:34:34,940 --> 00:34:42,620
stop the old akka swarm and be done with

00:34:39,429 --> 00:34:45,280
with this migration as well so as you

00:34:42,620 --> 00:34:48,770
can see again not rocket science but

00:34:45,280 --> 00:34:51,610
yeah it worked very well for us in the

00:34:48,770 --> 00:34:56,840
long run we are very very happy with

00:34:51,610 --> 00:34:59,750
with the way Nomad works of course there

00:34:56,840 --> 00:35:02,630
are also some benefits so we can in the

00:34:59,750 --> 00:35:07,070
job definition files of Nomad ensure a

00:35:02,630 --> 00:35:09,470
proper container spread so actually we

00:35:07,070 --> 00:35:12,320
can we can turn nomads okay please

00:35:09,470 --> 00:35:15,050
deploy all of my how many instances I

00:35:12,320 --> 00:35:17,720
want to start on different hosts and if

00:35:15,050 --> 00:35:20,150
it cannot do it this it will fail the

00:35:17,720 --> 00:35:23,030
deployment this is super nice

00:35:20,150 --> 00:35:27,140
it has self-healing in case of outage as

00:35:23,030 --> 00:35:29,990
I already mentioned we now have nice

00:35:27,140 --> 00:35:32,990
access control list so we can limit the

00:35:29,990 --> 00:35:34,320
access to who can start containers and

00:35:32,990 --> 00:35:36,270
who cannot

00:35:34,320 --> 00:35:38,660
you can inspect the job history in a web

00:35:36,270 --> 00:35:41,570
interface and again it has a lot of

00:35:38,660 --> 00:35:46,170
metrics that you can use as you will see

00:35:41,570 --> 00:35:50,460
in the next few slides also you are not

00:35:46,170 --> 00:35:52,530
limited to docker with Nomad I don't

00:35:50,460 --> 00:35:56,400
know about your relationship with docker

00:35:52,530 --> 00:36:02,160
but for us it's sometimes love and hate

00:35:56,400 --> 00:36:05,790
so it doesn't always work the way that

00:36:02,160 --> 00:36:07,920
we wanted to and Nomad actually has

00:36:05,790 --> 00:36:10,860
first-class support for running your

00:36:07,920 --> 00:36:13,980
workloads with rocket or with Alexi and

00:36:10,860 --> 00:36:15,780
if you want you are not even limited to

00:36:13,980 --> 00:36:18,630
containers with Nomad so it also

00:36:15,780 --> 00:36:21,800
provides a way for you to simply deploy

00:36:18,630 --> 00:36:25,650
jar files or simply execute binaries and

00:36:21,800 --> 00:36:27,300
it can also if you want scheduled

00:36:25,650 --> 00:36:29,370
virtual machines for you so those are

00:36:27,300 --> 00:36:34,620
very interesting features that we are

00:36:29,370 --> 00:36:37,470
actually looking into if we maybe change

00:36:34,620 --> 00:36:39,390
something there so very interesting and

00:36:37,470 --> 00:36:41,550
with this again you don't have to do

00:36:39,390 --> 00:36:43,170
have to change anything if you want to

00:36:41,550 --> 00:36:45,030
use this okay of course you would have

00:36:43,170 --> 00:36:48,180
to install rocket for example on your

00:36:45,030 --> 00:36:50,130
house but you simply change a line in

00:36:48,180 --> 00:36:51,870
your job definition and that's it and

00:36:50,130 --> 00:36:53,370
Nomad starts the container with rocket

00:36:51,870 --> 00:36:59,010
and not with docker so this is super

00:36:53,370 --> 00:37:02,130
nice and very promising alternative so

00:36:59,010 --> 00:37:05,460
here you can see the web interface of

00:37:02,130 --> 00:37:07,140
Nomad as I already taught you well you

00:37:05,460 --> 00:37:09,600
can see a lot of details so how many

00:37:07,140 --> 00:37:11,580
tokkan tanners are still acute how many

00:37:09,600 --> 00:37:13,640
are currently starting up how many are

00:37:11,580 --> 00:37:17,880
running you can see locks you can see

00:37:13,640 --> 00:37:20,250
resources used and so on which is very

00:37:17,880 --> 00:37:22,590
nice not only for us but also for our

00:37:20,250 --> 00:37:24,390
developers so they can see what how

00:37:22,590 --> 00:37:27,720
their service behaves what resources

00:37:24,390 --> 00:37:31,020
there it is using and access to loccs

00:37:27,720 --> 00:37:33,540
directly in a web interface again it has

00:37:31,020 --> 00:37:35,280
a lot of metrics so we kind of like

00:37:33,540 --> 00:37:38,100
building ref another spot so we also

00:37:35,280 --> 00:37:39,630
build one for no matter which shows you

00:37:38,100 --> 00:37:43,860
in this case for example at the amount

00:37:39,630 --> 00:37:46,740
of running containers the used CPU for

00:37:43,860 --> 00:37:47,490
your allocations to use memory you can

00:37:46,740 --> 00:37:50,369
see the

00:37:47,490 --> 00:37:52,890
located and vice versus two used memory

00:37:50,369 --> 00:37:55,230
of your host so I'm not sure if you can

00:37:52,890 --> 00:37:58,140
see it in the back but the the red bar

00:37:55,230 --> 00:38:00,900
on the left hand side is the allocated

00:37:58,140 --> 00:38:03,810
memory and this yellow line which is

00:38:00,900 --> 00:38:06,600
about 50% of an SD actually used memory

00:38:03,810 --> 00:38:09,420
so there's a lot to improve there but

00:38:06,600 --> 00:38:13,380
it's definitely nice to be able to see

00:38:09,420 --> 00:38:17,130
something like this and also we built

00:38:13,380 --> 00:38:19,950
our own like service overview service

00:38:17,130 --> 00:38:22,710
for developers where they can have a

00:38:19,950 --> 00:38:24,510
look at everything that they need for

00:38:22,710 --> 00:38:26,640
their service and here with with the

00:38:24,510 --> 00:38:30,290
help of these metrics we now also have

00:38:26,640 --> 00:38:34,170
the real-time or almost real-time

00:38:30,290 --> 00:38:37,380
display of the resource usage we turn to

00:38:34,170 --> 00:38:39,810
the extra migration we could show which

00:38:37,380 --> 00:38:42,510
Orchestrator was currently active for a

00:38:39,810 --> 00:38:44,520
given color so we could have this

00:38:42,510 --> 00:38:45,930
situation that for example the blue

00:38:44,520 --> 00:38:48,119
color and the top was still running

00:38:45,930 --> 00:38:51,480
swarmed and you would see a swarm logo

00:38:48,119 --> 00:38:53,760
there next to the inactive bar and on

00:38:51,480 --> 00:38:55,380
the bottom you would already have no

00:38:53,760 --> 00:38:57,510
Matt run in for example you could see

00:38:55,380 --> 00:39:00,090
this directly in a web interface which

00:38:57,510 --> 00:39:02,250
helped not only us a lot to see about

00:39:00,090 --> 00:39:05,369
the status of the migration but also our

00:39:02,250 --> 00:39:07,800
developers to to see how it's going how

00:39:05,369 --> 00:39:10,380
their service is behaving also with

00:39:07,800 --> 00:39:13,109
direct links to access locks in the

00:39:10,380 --> 00:39:15,420
elasticsearch take for example and a

00:39:13,109 --> 00:39:21,810
list of these service events generated

00:39:15,420 --> 00:39:27,960
by nomads so that's it what did we learn

00:39:21,810 --> 00:39:30,780
in all of this process what helps you a

00:39:27,960 --> 00:39:33,840
lot definitely is to have centralized

00:39:30,780 --> 00:39:36,720
deployment two sets or playbox or

00:39:33,840 --> 00:39:39,810
whatever because with this we could

00:39:36,720 --> 00:39:42,119
perform a simple change and can could be

00:39:39,810 --> 00:39:43,859
sure that the next deployment that was

00:39:42,119 --> 00:39:47,160
started as soon as we merge to change

00:39:43,859 --> 00:39:49,890
would actually use it directly this was

00:39:47,160 --> 00:39:52,320
super helpful for us and this was the

00:39:49,890 --> 00:39:55,080
main reason why we could actually

00:39:52,320 --> 00:39:57,540
migrate in in an amount of three weeks

00:39:55,080 --> 00:40:01,380
in our production environment from swarm

00:39:57,540 --> 00:40:03,200
to no met with without any real

00:40:01,380 --> 00:40:07,109
work that had to be done by the teams

00:40:03,200 --> 00:40:09,299
all they had to do was to come up with a

00:40:07,109 --> 00:40:12,989
memory limit for their service because

00:40:09,299 --> 00:40:17,039
in difference to the swarm that we were

00:40:12,989 --> 00:40:19,829
using nomads and enforces a service

00:40:17,039 --> 00:40:22,319
memory limit but this was about

00:40:19,829 --> 00:40:24,420
everything the teams had to do so we had

00:40:22,319 --> 00:40:26,640
to bring them to change a simple value

00:40:24,420 --> 00:40:30,239
in their service description and then

00:40:26,640 --> 00:40:33,150
redeploy this was very very nice for us

00:40:30,239 --> 00:40:37,109
and also what's helped a lot was to do

00:40:33,150 --> 00:40:41,640
these kenri like changes in order in our

00:40:37,109 --> 00:40:43,710
environment so to really make sure that

00:40:41,640 --> 00:40:48,029
this change can always be rolled back

00:40:43,710 --> 00:40:50,609
without a lot of work so we actually had

00:40:48,029 --> 00:40:52,349
some issues with it in the actual

00:40:50,609 --> 00:40:55,019
process and then we could simply

00:40:52,349 --> 00:40:58,979
rollback and everything was fine this

00:40:55,019 --> 00:41:00,749
helped us a lot so and what do you learn

00:40:58,979 --> 00:41:04,289
please please don't hate me but you

00:41:00,749 --> 00:41:07,710
might not need the kubernetes it might

00:41:04,289 --> 00:41:10,529
might be enough to look for a simpler

00:41:07,710 --> 00:41:13,529
alternative that works well enough for

00:41:10,529 --> 00:41:15,599
you so this is definitely one one big

00:41:13,529 --> 00:41:17,369
point that we took away from it you

00:41:15,599 --> 00:41:19,349
don't really need to use kubernetes if

00:41:17,369 --> 00:41:22,950
you don't want to there are other

00:41:19,349 --> 00:41:25,019
alternatives out there try to keep your

00:41:22,950 --> 00:41:27,950
your architecture pluggable this helps

00:41:25,019 --> 00:41:31,229
you a lot in the long run so try not to

00:41:27,950 --> 00:41:33,660
yeah intervene this is right word I

00:41:31,229 --> 00:41:36,569
don't know try to keep things as

00:41:33,660 --> 00:41:40,019
separated as possible so in the long run

00:41:36,569 --> 00:41:43,349
you can change a component if you have

00:41:40,019 --> 00:41:45,690
issues with it so I think he the only

00:41:43,349 --> 00:41:48,269
thing that we definitely cannot change

00:41:45,690 --> 00:41:51,479
is using console as a service discovery

00:41:48,269 --> 00:41:53,640
tool because we use it everywhere but I

00:41:51,479 --> 00:41:57,180
think pretty much everything else we we

00:41:53,640 --> 00:42:01,940
can with reasonable effort change doing

00:41:57,180 --> 00:42:07,009
our live traffic so this helps you a lot

00:42:01,940 --> 00:42:09,239
it can be difficult to to care about

00:42:07,009 --> 00:42:12,420
memory limits all of a sudden and you

00:42:09,239 --> 00:42:15,330
can suddenly find out that your

00:42:12,420 --> 00:42:18,510
resources are in the in fact finite

00:42:15,330 --> 00:42:20,820
and then you have to run around to all

00:42:18,510 --> 00:42:22,710
teams and tell them hey why is your

00:42:20,820 --> 00:42:25,470
docker container using 8 gigabytes of

00:42:22,710 --> 00:42:28,950
RAM can you please lower it because now

00:42:25,470 --> 00:42:32,220
no one else can deploy so this was this

00:42:28,950 --> 00:42:34,230
was funny at some points and can

00:42:32,220 --> 00:42:34,860
surprise you if you didn't have to deal

00:42:34,230 --> 00:42:39,360
with it

00:42:34,860 --> 00:42:41,910
beforehand and also one one thing that

00:42:39,360 --> 00:42:44,490
we learned the hard way is distributed

00:42:41,910 --> 00:42:45,960
systems can be very hard and don't fit

00:42:44,490 --> 00:42:49,110
with the cluster state of your

00:42:45,960 --> 00:42:51,330
distributed system we actually managed

00:42:49,110 --> 00:42:54,090
to completely shut down one of our

00:42:51,330 --> 00:42:56,460
development environments when we had the

00:42:54,090 --> 00:42:59,520
idea that hey let's let's change the

00:42:56,460 --> 00:43:03,000
cluster state on the on the hard drive

00:42:59,520 --> 00:43:08,160
what what can happen yeah a lot

00:43:03,000 --> 00:43:12,450
apparently so yeah that's that are our

00:43:08,160 --> 00:43:14,250
learnings and that's it thanks goes to

00:43:12,450 --> 00:43:17,280
my team to mock me with the greatest

00:43:14,250 --> 00:43:19,410
logo of them all and thank you a lot for

00:43:17,280 --> 00:43:21,000
your attention I hope you found this

00:43:19,410 --> 00:43:24,950
interesting if you have any questions

00:43:21,000 --> 00:43:32,700
please ask I will answer them gladly

00:43:24,950 --> 00:43:32,700
[Applause]

00:43:37,880 --> 00:43:43,270
do we have bad experiences with

00:43:39,530 --> 00:43:47,900
kubernetes yeah we actually we tried

00:43:43,270 --> 00:43:49,520
using it instead of Nomad for about a

00:43:47,900 --> 00:43:51,770
week we would try to get a

00:43:49,520 --> 00:43:53,450
proof-of-concept working and it just

00:43:51,770 --> 00:43:55,580
wouldn't work without a reasonable

00:43:53,450 --> 00:44:01,100
effort that's why we we decided against

00:43:55,580 --> 00:44:02,360
it hello hey thank you for your talk I

00:44:01,100 --> 00:44:04,550
just had a question

00:44:02,360 --> 00:44:06,410
yeah we talked a bunch about console

00:44:04,550 --> 00:44:08,510
yeah it seemed that you actually use

00:44:06,410 --> 00:44:10,100
that to build other stuff and you depend

00:44:08,510 --> 00:44:12,860
on it yeah I'm actually curious about

00:44:10,100 --> 00:44:15,320
how you set up and manage over time

00:44:12,860 --> 00:44:16,880
console itself you have any tricks or if

00:44:15,320 --> 00:44:19,520
you have a script or how you actually

00:44:16,880 --> 00:44:22,130
yeah the config management for it yeah

00:44:19,520 --> 00:44:25,910
we we use ansible for for our config

00:44:22,130 --> 00:44:28,790
management and basically all all the

00:44:25,910 --> 00:44:32,480
play books that we use include a console

00:44:28,790 --> 00:44:35,210
task so for example if I install the

00:44:32,480 --> 00:44:38,450
elasticsearch stake there's a task that

00:44:35,210 --> 00:44:42,940
it creates a it installs console as a

00:44:38,450 --> 00:44:45,140
client on the host and will register the

00:44:42,940 --> 00:44:49,790
elasticsearch servers in console by

00:44:45,140 --> 00:44:51,280
itself and the registering of services

00:44:49,790 --> 00:44:54,740
into kwang-soo

00:44:51,280 --> 00:44:56,450
we with swarm we we did this with a

00:44:54,740 --> 00:45:00,170
third party tool that is called

00:44:56,450 --> 00:45:03,260
registrate err so this is a simple

00:45:00,170 --> 00:45:05,900
docker container that you have to map

00:45:03,260 --> 00:45:08,030
your docker socket - and then it

00:45:05,900 --> 00:45:09,770
inspects the environment variables of

00:45:08,030 --> 00:45:13,010
all your containers that are started in

00:45:09,770 --> 00:45:14,350
depending on a given set of variables

00:45:13,010 --> 00:45:17,750
that you put into our environment

00:45:14,350 --> 00:45:20,320
variables it will then register this

00:45:17,750 --> 00:45:25,220
container as a service in console and

00:45:20,320 --> 00:45:28,070
with nomad we could simply define this

00:45:25,220 --> 00:45:30,320
in the job file itself so because nobody

00:45:28,070 --> 00:45:32,150
has this first-class support for konso

00:45:30,320 --> 00:45:34,910
you can simply tell in your job

00:45:32,150 --> 00:45:36,920
definition please register my container

00:45:34,910 --> 00:45:41,350
or my job as a konso service on the

00:45:36,920 --> 00:45:41,350
Dysport and that's then it's done

00:45:47,390 --> 00:45:55,230
of distributed data centers mm-hmm

00:45:51,119 --> 00:45:58,470
is something similar existing in nomad

00:45:55,230 --> 00:46:02,160
as well yeah so that you can also

00:45:58,470 --> 00:46:05,549
somehow define like okay this container

00:46:02,160 --> 00:46:07,170
needs to be running five times per data

00:46:05,549 --> 00:46:09,480
center or something you can actually

00:46:07,170 --> 00:46:11,579
define quotas per data center for

00:46:09,480 --> 00:46:13,680
example you can say okay thirty percent

00:46:11,579 --> 00:46:16,640
in this data center the seventy in the

00:46:13,680 --> 00:46:19,680
rest or whatever you want you can also

00:46:16,640 --> 00:46:22,680
define limits on for example TCP you

00:46:19,680 --> 00:46:25,380
type the kernel version and so on

00:46:22,680 --> 00:46:28,770
there's a lot it you can configure and

00:46:25,380 --> 00:46:47,130
also depending on the data center so is

00:46:28,770 --> 00:46:50,279
it also true I do we have any PCI

00:46:47,130 --> 00:46:53,579
related services on the same cluster so

00:46:50,279 --> 00:46:56,789
with payments yeah because of singing of

00:46:53,579 --> 00:46:58,920
the PCI requirements you have and we

00:46:56,789 --> 00:47:00,990
have at the moment just normal virtual

00:46:58,920 --> 00:47:04,470
house because it is a requirement and

00:47:00,990 --> 00:47:09,000
we're asking us if it is also possible

00:47:04,470 --> 00:47:13,440
to have CPC are requirements in talker

00:47:09,000 --> 00:47:15,390
like things yeah we we do have payment

00:47:13,440 --> 00:47:18,000
processing services as well running in

00:47:15,390 --> 00:47:23,940
there if this answers your question I'm

00:47:18,000 --> 00:47:29,520
not really sure so yeah we do payments

00:47:23,940 --> 00:47:31,970
as well with micro services any more

00:47:29,520 --> 00:47:31,970
questions

00:47:39,290 --> 00:47:45,349
i regarding the in s issues that you

00:47:43,160 --> 00:47:48,349
have that you have the name dns name for

00:47:45,349 --> 00:47:52,060
every service for blue and green did you

00:47:48,349 --> 00:47:58,339
try to work with text with that stuff to

00:47:52,060 --> 00:48:01,460
separate it so we in console itself we

00:47:58,339 --> 00:48:03,170
we separate them with text so I don't

00:48:01,460 --> 00:48:04,940
know if all of you know console but if

00:48:03,170 --> 00:48:07,580
you were a guesser a container in the

00:48:04,940 --> 00:48:10,310
cons were service discovery you can give

00:48:07,580 --> 00:48:13,880
it as a set of text that you can

00:48:10,310 --> 00:48:16,070
differentiate with it so every container

00:48:13,880 --> 00:48:20,060
that is started in the green color gets

00:48:16,070 --> 00:48:22,160
a green tech with its service definition

00:48:20,060 --> 00:48:25,010
and all blue containers get a blue check

00:48:22,160 --> 00:48:27,920
and based on this tech we then generate

00:48:25,010 --> 00:48:31,550
different routes and of course you could

00:48:27,920 --> 00:48:33,470
also do look up space on this tech this

00:48:31,550 --> 00:48:35,810
is super nice feature of konso you can

00:48:33,470 --> 00:48:38,930
prefix the tech or any tech you give

00:48:35,810 --> 00:48:42,320
your container your your service

00:48:38,930 --> 00:48:44,869
instances and it will then only cons

00:48:42,320 --> 00:48:47,570
will then only give you all the hosts

00:48:44,869 --> 00:48:49,490
where the service with this tech is

00:48:47,570 --> 00:48:52,849
running so we definitely looked into

00:48:49,490 --> 00:48:56,050
this but we couldn't really figure out

00:48:52,849 --> 00:49:00,320
how to create active and inactive text

00:48:56,050 --> 00:49:03,710
without basically more or less

00:49:00,320 --> 00:49:05,900
redeploying on color change and this was

00:49:03,710 --> 00:49:07,910
something we didn't want to do because

00:49:05,900 --> 00:49:10,609
the way we currently do this by just

00:49:07,910 --> 00:49:12,290
simply switching a value in the cons of

00:49:10,609 --> 00:49:14,900
key value store we have an atomic change

00:49:12,290 --> 00:49:17,540
that triggers a configuration reload

00:49:14,900 --> 00:49:19,910
everywhere and we couldn't get this

00:49:17,540 --> 00:49:25,190
working with trying to use active and

00:49:19,910 --> 00:49:27,230
inactive text yeah do you have the need

00:49:25,190 --> 00:49:29,599
for persistent data in your cluster or

00:49:27,230 --> 00:49:32,839
is everything stateless so everything is

00:49:29,599 --> 00:49:36,260
stateless if a service needs to store

00:49:32,839 --> 00:49:40,580
data it has its own post curse database

00:49:36,260 --> 00:49:43,070
usually and also if it has data that it

00:49:40,580 --> 00:49:45,980
shares with different services it

00:49:43,070 --> 00:49:48,859
produces them on a Kafka topic if this

00:49:45,980 --> 00:49:51,320
interests you I can again recommend the

00:49:48,859 --> 00:49:54,080
talk of Paul from last year where he

00:49:51,320 --> 00:49:55,940
talks about the same exact problem so

00:49:54,080 --> 00:50:01,070
how do we share data which we in

00:49:55,940 --> 00:50:07,010
stateless microservices any more

00:50:01,070 --> 00:50:09,120
questions note them thank you again

00:50:07,010 --> 00:50:15,230
YUM yeah thank you a lot

00:50:09,120 --> 00:50:15,230
[Applause]

00:50:16,560 --> 00:50:20,650

YouTube URL: https://www.youtube.com/watch?v=I3RpW0Lh948


