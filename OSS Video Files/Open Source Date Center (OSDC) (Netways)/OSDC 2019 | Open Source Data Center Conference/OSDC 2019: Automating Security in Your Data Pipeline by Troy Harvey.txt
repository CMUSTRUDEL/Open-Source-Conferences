Title: OSDC 2019: Automating Security in Your Data Pipeline by Troy Harvey
Publication date: 2019-05-21
Playlist: OSDC 2019 | Open Source Data Center Conference
Description: 
	Carta helps companies manage and secure their cap table and equity plans. Highly sensitive data. And in a post-GDPR world, data engineers play a critical role in protecting data and limiting access at each step in a data pipeline. In this session, Troy will walk through the steps that Carta’s data team has taken to secure the data pipeline using open source tools. You will leave with a checklist of things to consider when building a data lake, data warehouse, or deploying a data orchestration system. Some of the technologies covered include Apache Airflow, dbt, Docker, S3, Redshift, and Looker. Become a better steward of your customer’s data.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:01,140 --> 00:00:10,550
[Music]

00:00:13,250 --> 00:00:16,679
[Applause]

00:00:16,970 --> 00:00:21,840
all right I'm glad you all glad

00:00:20,550 --> 00:00:22,140
everybody made it off the boat last

00:00:21,840 --> 00:00:24,420
night

00:00:22,140 --> 00:00:27,210
here's safely I like seeing the coffee

00:00:24,420 --> 00:00:28,380
mugs at 2:30 it helps gives me more

00:00:27,210 --> 00:00:33,360
confidence and you're not gonna fall

00:00:28,380 --> 00:00:34,980
asleep I'm going to do a quiz in a

00:00:33,360 --> 00:00:37,110
little game so I'll let everybody sit

00:00:34,980 --> 00:00:40,980
down so you get to participate you have

00:00:37,110 --> 00:00:42,120
an equal opportunity how many Americans

00:00:40,980 --> 00:00:45,510
are in the room raise your hand if

00:00:42,120 --> 00:00:54,000
you're American sweet okay everybody can

00:00:45,510 --> 00:00:56,100
play okay so if you can tell me which

00:00:54,000 --> 00:00:57,629
one of these states is Kentucky you have

00:00:56,100 --> 00:00:59,070
to raise your hand first one raise their

00:00:57,629 --> 00:01:07,250
hand I'll give you a t-shirt a card a

00:00:59,070 --> 00:01:12,360
t-shirt no not to you

00:01:07,250 --> 00:01:13,560
no not fun you should just everybody

00:01:12,360 --> 00:01:14,189
just start just raise your hand and just

00:01:13,560 --> 00:01:18,060
take it yes

00:01:14,189 --> 00:01:28,080
no not for three he's got it

00:01:18,060 --> 00:01:32,020
are you uh what size exhale I think you

00:01:28,080 --> 00:01:34,659
have a neck so yeah

00:01:32,020 --> 00:01:38,680
I have two more does anybody here's a

00:01:34,659 --> 00:01:45,250
large what's a large you got it medium

00:01:38,680 --> 00:01:49,770
any mediums there we go all right so I'm

00:01:45,250 --> 00:01:52,420
a I'm a data engineer working for Carta

00:01:49,770 --> 00:01:55,479
remotely I work for Kentucky so that's

00:01:52,420 --> 00:01:56,979
that's the reason for the quiz work from

00:01:55,479 --> 00:01:59,170
Kentucky I should say a remote worker

00:01:56,979 --> 00:02:02,470
from Kentucky for Carta

00:01:59,170 --> 00:02:03,640
fun fact Kentucky Fried Chicken y'all

00:02:02,470 --> 00:02:05,500
I'm sure you all know about Kentucky

00:02:03,640 --> 00:02:08,950
Fried Chicken my grandma actually met

00:02:05,500 --> 00:02:11,140
Colonel Sanders a few times so that's a

00:02:08,950 --> 00:02:12,190
picture of my grandma with Colonel

00:02:11,140 --> 00:02:13,510
Sanders in Kentucky

00:02:12,190 --> 00:02:14,860
they're not eating chicken so I don't

00:02:13,510 --> 00:02:19,959
really know what's going on in the

00:02:14,860 --> 00:02:21,069
picture but so anyway before I get on to

00:02:19,959 --> 00:02:22,720
the talk I just want to tell you a

00:02:21,069 --> 00:02:23,799
little bit about my employer about Carta

00:02:22,720 --> 00:02:25,959
they sent me here all the way from

00:02:23,799 --> 00:02:28,569
Kentucky to come and talk about data

00:02:25,959 --> 00:02:31,209
engineering super excited to announce

00:02:28,569 --> 00:02:34,150
that we just did our close our Series E

00:02:31,209 --> 00:02:38,650
funding round andreessen horowitz is

00:02:34,150 --> 00:02:40,630
leading that round and so of course you

00:02:38,650 --> 00:02:42,850
know the next slide is we're hiring and

00:02:40,630 --> 00:02:47,079
I would love to talk to any of you who

00:02:42,850 --> 00:02:51,489
are interested in working stateside this

00:02:47,079 --> 00:02:52,930
is the data team at Carta so most of the

00:02:51,489 --> 00:02:55,660
things I'm going to be talking about in

00:02:52,930 --> 00:02:57,340
this talk were built by this team this

00:02:55,660 --> 00:03:00,040
is a highly distributed team we've got

00:02:57,340 --> 00:03:04,180
folks in the Bay Area San Francisco

00:03:00,040 --> 00:03:08,260
Seattle obviously Kentucky Brazil have a

00:03:04,180 --> 00:03:10,420
big team in Brazil North Carolina New

00:03:08,260 --> 00:03:12,160
York so we're kind of spread out all

00:03:10,420 --> 00:03:18,220
over the place using slack all the time

00:03:12,160 --> 00:03:20,920
like I'm sure all of you are so the

00:03:18,220 --> 00:03:24,459
title of the talk is automating security

00:03:20,920 --> 00:03:25,840
in your data pipeline and then you know

00:03:24,459 --> 00:03:28,329
the title of this conference open source

00:03:25,840 --> 00:03:30,639
data center conference so I was kind of

00:03:28,329 --> 00:03:33,370
thinking about the ways that we've used

00:03:30,639 --> 00:03:36,489
open source tools in our data pipeline

00:03:33,370 --> 00:03:39,430
at Carta and especially ways that we've

00:03:36,489 --> 00:03:42,459
instrumented privacy and security inside

00:03:39,430 --> 00:03:44,730
of our data pipeline so I probably could

00:03:42,459 --> 00:03:49,870
have titled the talk

00:03:44,730 --> 00:03:50,680
automating data pipeline privacy most of

00:03:49,870 --> 00:03:54,190
what I'm talking about is actually

00:03:50,680 --> 00:03:56,890
privacy related it's ideas around how to

00:03:54,190 --> 00:04:01,200
default to protecting our customers data

00:03:56,890 --> 00:04:04,140
and offline systems and analytic systems

00:04:01,200 --> 00:04:07,150
so I'm going to briefly talk about

00:04:04,140 --> 00:04:09,280
regulatory and ethical reasons why this

00:04:07,150 --> 00:04:11,500
is important to the data team at Carta

00:04:09,280 --> 00:04:13,240
and in addition to that probably the

00:04:11,500 --> 00:04:16,630
more interesting part to this to this

00:04:13,240 --> 00:04:19,600
room is some of the engineering

00:04:16,630 --> 00:04:21,220
challenges and tactics and ways that we

00:04:19,600 --> 00:04:26,980
can build tooling that will protect our

00:04:21,220 --> 00:04:29,290
customers data so three big ideas in the

00:04:26,980 --> 00:04:33,040
talk my goal is for you to take away

00:04:29,290 --> 00:04:35,620
these key points number one is kind of a

00:04:33,040 --> 00:04:38,650
strange intro but privacy has a has odd

00:04:35,620 --> 00:04:39,340
and kind of interesting history from

00:04:38,650 --> 00:04:42,130
what I've read

00:04:39,340 --> 00:04:45,160
number two is that privacy first systems

00:04:42,130 --> 00:04:47,140
are designed by professionals

00:04:45,160 --> 00:04:50,110
professionals with ethical behavior with

00:04:47,140 --> 00:04:51,850
ethics and three what I consider to be

00:04:50,110 --> 00:04:54,490
the fun part that you know we get to

00:04:51,850 --> 00:04:57,760
write tools there's a lot of opportunity

00:04:54,490 --> 00:05:00,070
from what I've seen too for custom

00:04:57,760 --> 00:05:02,170
tooling for new open-source tools that

00:05:00,070 --> 00:05:08,080
need to be built around security and

00:05:02,170 --> 00:05:10,690
privacy so why should you care like

00:05:08,080 --> 00:05:14,650
what's why why would this talk be

00:05:10,690 --> 00:05:16,750
interesting to you so I what I've

00:05:14,650 --> 00:05:19,030
observed and you can tell me if this is

00:05:16,750 --> 00:05:22,360
if this is wrong but I think what's

00:05:19,030 --> 00:05:23,860
happened is the past maybe 10 past

00:05:22,360 --> 00:05:26,890
decade probably some of you in this room

00:05:23,860 --> 00:05:29,919
in the past decade have spent a big

00:05:26,890 --> 00:05:31,660
chunk of your career taking kind of

00:05:29,919 --> 00:05:35,080
antiquated systems administration

00:05:31,660 --> 00:05:37,150
practices and building automated tooling

00:05:35,080 --> 00:05:39,669
basically I've heard several of you say

00:05:37,150 --> 00:05:43,810
the phrase I want to be able to automate

00:05:39,669 --> 00:05:44,950
my my job away and any mundane tasks or

00:05:43,810 --> 00:05:47,530
any manual work I'm gonna build

00:05:44,950 --> 00:05:49,300
automation for that the second trend

00:05:47,530 --> 00:05:51,970
I've seen and this is what caused me to

00:05:49,300 --> 00:05:54,040
leave full-stack programming and become

00:05:51,970 --> 00:05:56,610
a data engineer the second trend I've

00:05:54,040 --> 00:05:56,610
seen is

00:05:57,580 --> 00:06:01,480
the hype cycle around machine learning

00:05:59,830 --> 00:06:03,730
and artificial intelligence you see it

00:06:01,480 --> 00:06:09,070
on TV it's it's everywhere

00:06:03,730 --> 00:06:12,850
and companies were just hiring data

00:06:09,070 --> 00:06:14,890
sciences machine learning engineers you

00:06:12,850 --> 00:06:16,990
know the incubators were cranking out

00:06:14,890 --> 00:06:21,190
data's on new data science programs and

00:06:16,990 --> 00:06:23,800
so I think what happened was these data

00:06:21,190 --> 00:06:25,660
data Sciences show up at companies and

00:06:23,800 --> 00:06:27,700
the infrastructure is not in place that

00:06:25,660 --> 00:06:29,320
they need to do their job so there's

00:06:27,700 --> 00:06:32,080
there is no automation there is no

00:06:29,320 --> 00:06:33,340
tooling they show up at companies and

00:06:32,080 --> 00:06:35,470
the companies like okay we need you to

00:06:33,340 --> 00:06:37,120
deploy this to this kubernetes cluster

00:06:35,470 --> 00:06:41,130
and they decide to so I don't even know

00:06:37,120 --> 00:06:44,380
what you're talking about so I guess

00:06:41,130 --> 00:06:45,970
what I'm trying to say is that I think

00:06:44,380 --> 00:06:47,650
there's a huge opportunity if you are

00:06:45,970 --> 00:06:50,140
interested in machine learning or

00:06:47,650 --> 00:06:53,290
artificial intelligence for your skill

00:06:50,140 --> 00:06:55,900
set to basically I'm pitching you to

00:06:53,290 --> 00:06:57,280
become a data engineer or to start to

00:06:55,900 --> 00:06:59,710
get more involved in data engineering

00:06:57,280 --> 00:07:01,960
inner organizations because your skill

00:06:59,710 --> 00:07:03,760
set is extremely valuable and extremely

00:07:01,960 --> 00:07:08,790
needed and data organizations where

00:07:03,760 --> 00:07:08,790
probably some of you have seen a mess

00:07:13,710 --> 00:07:20,920
and of course I have to acknowledge the

00:07:16,540 --> 00:07:23,020
kind of the elephant in the room the

00:07:20,920 --> 00:07:26,940
fact that an American is here talking

00:07:23,020 --> 00:07:29,380
about privacy is it's kind of absurd I

00:07:26,940 --> 00:07:31,330
don't have anything to say it's just

00:07:29,380 --> 00:07:35,950
it's it is what it is we're doing our

00:07:31,330 --> 00:07:38,200
best I've tried so earlier this month

00:07:35,950 --> 00:07:41,680
Zuckerberg took the stage at the f8

00:07:38,200 --> 00:07:44,290
conference any sort of you know

00:07:41,680 --> 00:07:46,300
announced that we've done a terrible job

00:07:44,290 --> 00:07:47,920
of privacy and the future going forward

00:07:46,300 --> 00:07:49,840
we're going to put our customers privacy

00:07:47,920 --> 00:07:55,570
first I'm sure you all have opinions

00:07:49,840 --> 00:07:57,810
about how genuine that statement is so

00:07:55,570 --> 00:08:01,870
anyway there's massive external pressure

00:07:57,810 --> 00:08:04,570
from regulators and journalists and the

00:08:01,870 --> 00:08:06,130
tech community too that led to this

00:08:04,570 --> 00:08:08,230
happening to him doing this to this

00:08:06,130 --> 00:08:10,570
makeup on the on stage at the Facebook

00:08:08,230 --> 00:08:12,840
conference but it did give me thinking

00:08:10,570 --> 00:08:12,840
about

00:08:13,890 --> 00:08:18,970
you know the nature of privacy and

00:08:16,740 --> 00:08:22,000
especially at working for a company in

00:08:18,970 --> 00:08:25,510
Silicon Valley why are we doing this

00:08:22,000 --> 00:08:27,370
like what what is kind of what's what's

00:08:25,510 --> 00:08:28,690
the history of privacy and now that

00:08:27,370 --> 00:08:31,240
we're starting to develop our own laws

00:08:28,690 --> 00:08:34,810
that kind of copy the GDP are like what

00:08:31,240 --> 00:08:38,050
does this look like so let's do a fun

00:08:34,810 --> 00:08:40,090
little detour into the history of

00:08:38,050 --> 00:08:46,150
privacy in the Western world and you can

00:08:40,090 --> 00:08:48,820
tell me where I'm wrong so as you can

00:08:46,150 --> 00:08:51,370
imagine the religion plays a massive

00:08:48,820 --> 00:08:53,860
role in the development of privacy in

00:08:51,370 --> 00:08:58,150
the Western world one of the earliest

00:08:53,860 --> 00:09:01,420
places that you see this concept is in

00:08:58,150 --> 00:09:02,770
in Judaism and the Torah and in the book

00:09:01,420 --> 00:09:05,260
of Genesis where you have this story

00:09:02,770 --> 00:09:07,000
that everybody's hurried about Adam and

00:09:05,260 --> 00:09:08,860
Eve eat the fruit they get keeps out of

00:09:07,000 --> 00:09:12,130
the garden but I think the part that's

00:09:08,860 --> 00:09:14,560
not in the the story but they like the

00:09:12,130 --> 00:09:16,330
kids storybooks is the dark parts

00:09:14,560 --> 00:09:18,460
obviously you know so the dark part is

00:09:16,330 --> 00:09:22,240
there's this death that happens in the

00:09:18,460 --> 00:09:24,820
story where some the creator in the

00:09:22,240 --> 00:09:28,470
story slaughters animals and then uses

00:09:24,820 --> 00:09:32,190
the the the furs or the skins to create

00:09:28,470 --> 00:09:34,270
coverings for for these people so I

00:09:32,190 --> 00:09:35,800
don't know it seems like one of the

00:09:34,270 --> 00:09:43,990
earliest places where maybe the word

00:09:35,800 --> 00:09:47,890
private parts was invented could be so

00:09:43,990 --> 00:09:50,740
isolation is another kind of strange

00:09:47,890 --> 00:09:54,490
development in human history humans are

00:09:50,740 --> 00:09:58,750
tribal species mammals species and small

00:09:54,490 --> 00:10:01,750
communities and isolation is kind of a

00:09:58,750 --> 00:10:04,270
dangerous or discouraged behavior in

00:10:01,750 --> 00:10:05,680
tribal societies so if you're you know

00:10:04,270 --> 00:10:07,210
isolating yourself separate yourself

00:10:05,680 --> 00:10:11,950
from the tribe you could be susceptible

00:10:07,210 --> 00:10:15,730
to to risk and even to this day in our

00:10:11,950 --> 00:10:20,170
culture we we still have kind of these

00:10:15,730 --> 00:10:23,320
phrases antisocial behavior you know

00:10:20,170 --> 00:10:25,270
things that we recognize isolation as

00:10:23,320 --> 00:10:29,380
kind of being like the lone

00:10:25,270 --> 00:10:31,390
or hermit or wills we have a phrase

00:10:29,380 --> 00:10:32,860
called odd-man-out things that kind of

00:10:31,390 --> 00:10:38,620
separate people who tend to isolate

00:10:32,860 --> 00:10:41,890
themselves before the in-home fireplace

00:10:38,620 --> 00:10:44,470
was developed family dwellings typically

00:10:41,890 --> 00:10:47,080
didn't even have walls so you had a

00:10:44,470 --> 00:10:50,740
family all living together in a small

00:10:47,080 --> 00:10:51,870
house or a hut all in one room and you

00:10:50,740 --> 00:10:55,000
know sometimes that would that would be

00:10:51,870 --> 00:10:59,070
guests or extended family sharing a

00:10:55,000 --> 00:10:59,070
single bed together in one big open room

00:10:59,460 --> 00:11:04,480
so the idea of having private bedrooms

00:11:02,020 --> 00:11:07,600
with individuals having their own room

00:11:04,480 --> 00:11:12,880
their own bed is really a modern concept

00:11:07,600 --> 00:11:14,260
and beds were actually a luxury item to

00:11:12,880 --> 00:11:18,310
have not you know not everybody has a

00:11:14,260 --> 00:11:19,720
bed and would typically be shared by a

00:11:18,310 --> 00:11:22,330
bunch of people in a home so you'd have

00:11:19,720 --> 00:11:23,950
a you know maybe extended family or

00:11:22,330 --> 00:11:26,230
visitors when they would come to your

00:11:23,950 --> 00:11:28,180
house would would be sharing a bed with

00:11:26,230 --> 00:11:30,160
you and you actually see this the place

00:11:28,180 --> 00:11:34,840
where you typically see this written

00:11:30,160 --> 00:11:36,690
down in history is how Co sleeping and

00:11:34,840 --> 00:11:38,980
sharing beds with guests and visitors

00:11:36,690 --> 00:11:45,550
contributed to the spread of disease and

00:11:38,980 --> 00:11:48,070
the plagues another place we see

00:11:45,550 --> 00:11:50,680
isolation so in the Middle Ages monks

00:11:48,070 --> 00:11:57,250
begin this practice of isolation and

00:11:50,680 --> 00:11:59,290
it's probably traces back to Jesus being

00:11:57,250 --> 00:12:00,610
in the wilderness for forty days and

00:11:59,290 --> 00:12:05,020
then on the Senate in the Sermon on the

00:12:00,610 --> 00:12:07,180
Mount he says to go like when you pray

00:12:05,020 --> 00:12:08,230
to me go into your room by yourself and

00:12:07,180 --> 00:12:11,340
don't let anybody else know what you're

00:12:08,230 --> 00:12:14,860
doing so it's kind of this antisocial

00:12:11,340 --> 00:12:17,320
behavior begins of isolation so this is

00:12:14,860 --> 00:12:21,250
a picture of be high up beehive huts on

00:12:17,320 --> 00:12:25,000
skellig michael and you may have seen

00:12:21,250 --> 00:12:33,700
these in a in a recent low-budget

00:12:25,000 --> 00:12:36,930
low-budget indie film anybody anybody

00:12:33,700 --> 00:12:36,930
excited for the new Star Wars movie

00:12:37,660 --> 00:12:51,130
no Star Wars fans man yeah so our modern

00:12:47,920 --> 00:12:51,880
right to privacy definitely has roots in

00:12:51,130 --> 00:12:56,019
antiquity

00:12:51,880 --> 00:12:58,240
however you look at it before public and

00:12:56,019 --> 00:13:00,130
private laws were separated when that

00:12:58,240 --> 00:13:03,699
happened in Roman law there's this idea

00:13:00,130 --> 00:13:06,579
of the actio inning areum I have no idea

00:13:03,699 --> 00:13:08,920
how to say that a law that so the idea

00:13:06,579 --> 00:13:12,670
of this law was that it starts to

00:13:08,920 --> 00:13:14,769
protect who a person is rather than just

00:13:12,670 --> 00:13:16,589
their possessions and what they have so

00:13:14,769 --> 00:13:19,089
it's the beginning of this idea of

00:13:16,589 --> 00:13:25,329
individuals a concept of individual

00:13:19,089 --> 00:13:28,509
personal rights and then if you jump

00:13:25,329 --> 00:13:31,750
ahead to 1890 there's these two lawyers

00:13:28,509 --> 00:13:34,930
in Boston Massachusetts in the u.s. who

00:13:31,750 --> 00:13:37,029
write publishes article in the Harvard

00:13:34,930 --> 00:13:38,500
Law Review and this is a part I had to

00:13:37,029 --> 00:13:41,139
include when I came across this because

00:13:38,500 --> 00:13:42,610
one of the lawyers is from my city and I

00:13:41,139 --> 00:13:45,269
see his name all over the city but I had

00:13:42,610 --> 00:13:49,569
no idea that he was connected with this

00:13:45,269 --> 00:13:50,889
so Dorothy glances she argues that this

00:13:49,569 --> 00:13:52,269
article that they wrote and they

00:13:50,889 --> 00:13:53,620
published in the Harvard Law Review had

00:13:52,269 --> 00:13:55,990
actually created a new chapter in

00:13:53,620 --> 00:13:58,810
American law and the title of the

00:13:55,990 --> 00:14:02,130
article they wrote the right to privacy

00:13:58,810 --> 00:14:04,630
in 1890 and the Harvard Law Review and

00:14:02,130 --> 00:14:06,610
the author's their names were Samuel

00:14:04,630 --> 00:14:10,810
Warren and Louis Brandeis so there were

00:14:06,610 --> 00:14:13,839
two law firm partners samuel warren he

00:14:10,810 --> 00:14:17,290
was a son he was the wealthy one he was

00:14:13,839 --> 00:14:19,899
the son of a wealthy paper manufacturer

00:14:17,290 --> 00:14:22,930
and so the funny part of the story is

00:14:19,899 --> 00:14:25,180
that it seems like the motivation his

00:14:22,930 --> 00:14:29,740
motivation at least for writing this

00:14:25,180 --> 00:14:32,050
paper is his daughter's wedding the

00:14:29,740 --> 00:14:34,300
local newspaper had this society section

00:14:32,050 --> 00:14:35,949
I think is called the Boston The

00:14:34,300 --> 00:14:39,610
Saturday Evening Gazette and they have

00:14:35,949 --> 00:14:42,610
this society section where they

00:14:39,610 --> 00:14:44,139
published all the details about his

00:14:42,610 --> 00:14:45,910
daughter's wedding and like when you

00:14:44,139 --> 00:14:47,500
know everybody's wearing the guest list

00:14:45,910 --> 00:14:49,689
with the food they had basically kind of

00:14:47,500 --> 00:14:50,769
exposing the lavishness of this of this

00:14:49,689 --> 00:14:52,959
wedding

00:14:50,769 --> 00:14:55,720
and so he was really angry about the

00:14:52,959 --> 00:14:57,819
fact that the newspaper was publicizing

00:14:55,720 --> 00:15:01,389
his his details and he felt like it was

00:14:57,819 --> 00:15:04,179
an intrusion into his family's private

00:15:01,389 --> 00:15:06,999
life so that's what led to him exploring

00:15:04,179 --> 00:15:08,199
this idea and then the other lawyer the

00:15:06,999 --> 00:15:11,199
guy that's from louisville kentucky

00:15:08,199 --> 00:15:13,540
louis brandeis he he was not wealthy

00:15:11,199 --> 00:15:16,779
he's a second generation immigrant from

00:15:13,540 --> 00:15:20,019
a jewish immigrant family from prague

00:15:16,779 --> 00:15:21,759
and it looks like apparently Brandeis

00:15:20,019 --> 00:15:26,829
actually wrote the bulk of the article

00:15:21,759 --> 00:15:28,540
and so his motivations for being

00:15:26,829 --> 00:15:31,239
involved with this obviously his buddy

00:15:28,540 --> 00:15:32,889
his partner in the law firm but he was

00:15:31,239 --> 00:15:36,999
just kind of obsessed with the invention

00:15:32,889 --> 00:15:39,639
of of legal theory so anyway in their

00:15:36,999 --> 00:15:41,319
article what they argue is that these

00:15:39,639 --> 00:15:42,819
are the three you know big categories

00:15:41,319 --> 00:15:44,199
that we that we talk about when we talk

00:15:42,819 --> 00:15:47,279
about individual rights the right to

00:15:44,199 --> 00:15:50,439
life liberty and prosperity and property

00:15:47,279 --> 00:15:55,600
but what they outlined in their article

00:15:50,439 --> 00:15:57,279
is these two new subsets of of legal

00:15:55,600 --> 00:15:59,199
concepts underneath the right to life

00:15:57,279 --> 00:16:00,699
the first one is the right to be let

00:15:59,199 --> 00:16:05,529
alone in the second is the right to

00:16:00,699 --> 00:16:06,610
privacy so what is the right to privacy

00:16:05,529 --> 00:16:09,749
what is that

00:16:06,610 --> 00:16:13,569
Brandeis argues that it's the right to

00:16:09,749 --> 00:16:15,549
decide if that which is there shall be

00:16:13,569 --> 00:16:17,559
given to the public so that for

00:16:15,549 --> 00:16:20,069
something about my private life to be

00:16:17,559 --> 00:16:21,999
made public it requires my consent and

00:16:20,069 --> 00:16:25,509
that's I mean obviously that's a huge

00:16:21,999 --> 00:16:27,639
we're now consent nobody has the right

00:16:25,509 --> 00:16:30,369
to publish something that I've created

00:16:27,639 --> 00:16:33,220
or that I own without my consent and

00:16:30,369 --> 00:16:35,740
then you know so this is this is kind of

00:16:33,220 --> 00:16:38,490
the end of this part but for from there

00:16:35,740 --> 00:16:41,439
you have a technology the newspaper

00:16:38,490 --> 00:16:44,230
telephone television all these

00:16:41,439 --> 00:16:46,720
technologies that expedite the

00:16:44,230 --> 00:16:49,540
development of privacy law in the

00:16:46,720 --> 00:16:54,129
Western world and now until today we

00:16:49,540 --> 00:16:57,639
have the GD P R and more recently the

00:16:54,129 --> 00:17:01,209
California equivalent the CCPA so I

00:16:57,639 --> 00:17:03,279
don't know if I don't know I don't know

00:17:01,209 --> 00:17:04,570
if you can trace trace back this you

00:17:03,279 --> 00:17:07,660
know these lawyers in eighteen

00:17:04,570 --> 00:17:09,610
the GDP are all the way back to fur

00:17:07,660 --> 00:17:11,260
coats in the Garden of Eden does it can

00:17:09,610 --> 00:17:14,230
anybody think of anything older than the

00:17:11,260 --> 00:17:17,320
Garden of Eden that could be like the

00:17:14,230 --> 00:17:19,510
invention of privacy I don't know I

00:17:17,320 --> 00:17:21,750
couldn't find anything it's a strange

00:17:19,510 --> 00:17:21,750
thought

00:17:32,890 --> 00:17:39,580
so my second big idea in this talk is

00:17:35,320 --> 00:17:41,890
about ethics systems that take privacy

00:17:39,580 --> 00:17:44,980
seriously are designed by ethical

00:17:41,890 --> 00:17:47,140
professionals Marc Andreessen this is a

00:17:44,980 --> 00:17:48,549
picture of Marc Andreessen I didn't just

00:17:47,140 --> 00:17:51,040
include him just because he was involved

00:17:48,549 --> 00:17:53,230
in our funding round I genuinely liked

00:17:51,040 --> 00:17:56,950
this quote so he likes to say that

00:17:53,230 --> 00:17:58,179
software is eating the world and I

00:17:56,950 --> 00:18:02,580
believe I believe it's true I mean we

00:17:58,179 --> 00:18:06,250
see it everywhere but if that is true

00:18:02,580 --> 00:18:10,000
then a car to a company like Carta that

00:18:06,250 --> 00:18:14,850
I work for so we're a FinTech company

00:18:10,000 --> 00:18:17,470
what we do is we build software that

00:18:14,850 --> 00:18:21,309
automates the management of your cap

00:18:17,470 --> 00:18:25,750
tables allows company to manage employee

00:18:21,309 --> 00:18:31,270
equity we also build tools for VC firms

00:18:25,750 --> 00:18:34,929
and boards of companies so we're

00:18:31,270 --> 00:18:39,580
replacing eating our sufferers eating

00:18:34,929 --> 00:18:42,870
away at traditional paper processes you

00:18:39,580 --> 00:18:47,530
know legal processes legal documents

00:18:42,870 --> 00:18:49,390
banking and finance processes and so we

00:18:47,530 --> 00:18:51,970
believe that if we're going to replace

00:18:49,390 --> 00:18:55,150
these with better tools that we also

00:18:51,970 --> 00:18:59,500
need to be raising the bar in terms of

00:18:55,150 --> 00:19:04,360
our ethics and the privacy systems that

00:18:59,500 --> 00:19:05,740
we build and build into our software so

00:19:04,360 --> 00:19:07,720
one place you see this concept is

00:19:05,740 --> 00:19:11,559
actually in GDP in GDP our and it's

00:19:07,720 --> 00:19:13,960
called privacy by design so I believe in

00:19:11,559 --> 00:19:15,940
a Carta I think we believe that audit

00:19:13,960 --> 00:19:19,720
defensibility is too low of a bar when

00:19:15,940 --> 00:19:23,830
it comes to our customers privacy so the

00:19:19,720 --> 00:19:26,200
idea is what does it mean to to treat

00:19:23,830 --> 00:19:27,580
our customers with the dignity and

00:19:26,200 --> 00:19:31,330
respect they deserve when we build

00:19:27,580 --> 00:19:36,610
software what kind of software can we

00:19:31,330 --> 00:19:39,429
build as a company that by default

00:19:36,610 --> 00:19:43,570
protects our customers privacy and the

00:19:39,429 --> 00:19:46,419
default mode favors our customers last

00:19:43,570 --> 00:19:47,049
slide about about ethics and legal

00:19:46,419 --> 00:19:51,940
concept

00:19:47,049 --> 00:19:54,999
just a quick overview of gdpr and and

00:19:51,940 --> 00:19:57,879
CCPA so they take the idea of the right

00:19:54,999 --> 00:19:59,950
to privacy and dignity of customers and

00:19:57,879 --> 00:20:05,739
they put it in these laws put this into

00:19:59,950 --> 00:20:07,179
law so before I moved on to our move on

00:20:05,739 --> 00:20:09,970
to our tactics and tools that we're

00:20:07,179 --> 00:20:12,340
using at Carta just gonna remind you of

00:20:09,970 --> 00:20:15,460
some of the things that these that we

00:20:12,340 --> 00:20:16,960
put into law so our customers have the

00:20:15,460 --> 00:20:19,509
right to know what data we're collecting

00:20:16,960 --> 00:20:22,720
and storing about them who it's being

00:20:19,509 --> 00:20:25,149
sold to and and they have the right to

00:20:22,720 --> 00:20:26,769
ask us to erase their data or sometimes

00:20:25,149 --> 00:20:31,059
you'll see it be being called the right

00:20:26,769 --> 00:20:33,820
to be forgotten customers also have the

00:20:31,059 --> 00:20:35,470
right to assume that when analysts and

00:20:33,820 --> 00:20:40,419
data scientists are using their data

00:20:35,470 --> 00:20:43,019
that they're using personally

00:20:40,419 --> 00:20:47,139
identifiable information that's been

00:20:43,019 --> 00:20:49,239
scrubbed or pseudo anonymized so

00:20:47,139 --> 00:20:51,669
regulators are really forcing Silicon

00:20:49,239 --> 00:20:55,389
Valley to mature and our

00:20:51,669 --> 00:20:59,350
responsibilities as data engineers and

00:20:55,389 --> 00:21:01,149
systems engineers now have more ethical

00:20:59,350 --> 00:21:04,389
considerations we see that in our gate

00:21:01,149 --> 00:21:06,460
commits and github we see it in design

00:21:04,389 --> 00:21:09,519
documents more and more comments and

00:21:06,460 --> 00:21:11,759
more and more interactions with legal

00:21:09,519 --> 00:21:11,759
teams

00:21:23,720 --> 00:21:31,480
so the third section of this talk is the

00:21:26,379 --> 00:21:35,450
more interesting part fun part for me

00:21:31,480 --> 00:21:38,990
the idea that we can build tools we can

00:21:35,450 --> 00:21:40,309
automate away privacy and that's been

00:21:38,990 --> 00:21:42,639
one of the most interesting things I've

00:21:40,309 --> 00:21:46,429
discovered in data engineering is just

00:21:42,639 --> 00:21:50,000
how many tools still need to be built in

00:21:46,429 --> 00:21:52,009
this area so our goal is building

00:21:50,000 --> 00:21:53,450
analytical systems that automate the

00:21:52,009 --> 00:21:57,019
protection of our customer data and

00:21:53,450 --> 00:22:00,710
reduce the potential for for human error

00:21:57,019 --> 00:22:02,629
in data pipelines or for data leaks of

00:22:00,710 --> 00:22:07,309
sensitive data that could happen on in

00:22:02,629 --> 00:22:11,389
data pipelines how many of you have seen

00:22:07,309 --> 00:22:15,710
diagrams like this yeah this one is you

00:22:11,389 --> 00:22:17,330
know the the big data landscape this is

00:22:15,710 --> 00:22:20,779
one of the most frustrating parts of

00:22:17,330 --> 00:22:26,029
beginning an analytics project is the

00:22:20,779 --> 00:22:29,480
the tools are overwhelming so we at

00:22:26,029 --> 00:22:31,700
Carta we looked at at least 30 vendor

00:22:29,480 --> 00:22:33,769
and open source tools for what we were

00:22:31,700 --> 00:22:36,259
going to use to build our our offline

00:22:33,769 --> 00:22:40,419
analytics systems kind of the second

00:22:36,259 --> 00:22:44,389
generation of our offline systems and

00:22:40,419 --> 00:22:46,100
the key the the main takeaway that I had

00:22:44,389 --> 00:22:47,330
from and I can see some of the names of

00:22:46,100 --> 00:22:50,629
the companies that we talked to you on

00:22:47,330 --> 00:22:53,870
this chart but the main takeaway I had

00:22:50,629 --> 00:22:56,960
from the audits and the conversations

00:22:53,870 --> 00:22:58,700
that we had with vendors and the

00:22:56,960 --> 00:23:02,720
inspections into open source tools that

00:22:58,700 --> 00:23:04,580
we did was that some of the like biggest

00:23:02,720 --> 00:23:09,620
companies with the biggest marketing

00:23:04,580 --> 00:23:23,210
budgets the most VC backing when you

00:23:09,620 --> 00:23:24,740
looked at their stock report yeah when

00:23:23,210 --> 00:23:27,490
you when you looked at their their stock

00:23:24,740 --> 00:23:27,490
reports and

00:23:30,590 --> 00:23:56,720
maybe I'm too far away the software

00:23:41,360 --> 00:23:59,980
architecture should I swap out okay yeah

00:23:56,720 --> 00:23:59,980
I don't know that's totally fine

00:24:09,250 --> 00:24:12,250
test

00:24:14,789 --> 00:24:21,499
so you know we're doing vendor audits

00:24:17,369 --> 00:24:23,070
looking at Sauk Reports architecture

00:24:21,499 --> 00:24:26,459
getting there

00:24:23,070 --> 00:24:27,989
what's it called signing NDA's so you

00:24:26,459 --> 00:24:30,089
can see the software architecture behind

00:24:27,989 --> 00:24:31,469
the scenes and the big takeaway for me

00:24:30,089 --> 00:24:33,419
was that some of the biggest companies

00:24:31,469 --> 00:24:36,059
had an absolute mess behind the scenes

00:24:33,419 --> 00:24:37,709
in terms of their security and then some

00:24:36,059 --> 00:24:40,739
of the smaller you know maybe newer

00:24:37,709 --> 00:24:42,509
companies had really rock-solid

00:24:40,739 --> 00:24:44,519
architecture they thought through all

00:24:42,509 --> 00:24:48,929
the security considerations privacy

00:24:44,519 --> 00:24:51,419
first design and so yeah I just

00:24:48,929 --> 00:24:52,709
encourage you to when you're when you're

00:24:51,419 --> 00:24:57,539
talking to vendors I'm looking at these

00:24:52,709 --> 00:25:00,599
open source projects definitely do the

00:24:57,539 --> 00:25:09,929
diligence to go through and look at how

00:25:00,599 --> 00:25:12,059
the systems are architected when we're

00:25:09,929 --> 00:25:15,359
talking about customer data really the

00:25:12,059 --> 00:25:16,829
security posture of your weakest vendor

00:25:15,359 --> 00:25:18,839
and that could be the weakest vendor in

00:25:16,829 --> 00:25:24,299
your data pipeline is the security

00:25:18,839 --> 00:25:25,709
posture of the entire organization so

00:25:24,299 --> 00:25:29,579
I'm going to camp out on this slide for

00:25:25,709 --> 00:25:31,619
a minute what what tools did we use in

00:25:29,579 --> 00:25:34,139
our data pipeline I kind of want to walk

00:25:31,619 --> 00:25:36,469
through just our design our architecture

00:25:34,139 --> 00:25:38,729
for how we built our analytics systems

00:25:36,469 --> 00:25:41,339
it's kinda it's you know it's obviously

00:25:38,729 --> 00:25:43,289
constantly evolving and what I'm showing

00:25:41,339 --> 00:25:46,940
is just a snapshot of what we have in

00:25:43,289 --> 00:25:50,519
production but we are using a mixture of

00:25:46,940 --> 00:25:54,079
vendor tools and open source tools in

00:25:50,519 --> 00:25:57,690
our VPC on AWS

00:25:54,079 --> 00:26:00,239
we chose terraform for building

00:25:57,690 --> 00:26:01,619
infrastructure I know lots of terraform

00:26:00,239 --> 00:26:04,739
users are at the conference we're big

00:26:01,619 --> 00:26:06,839
fans of it we're using Apache air flow

00:26:04,739 --> 00:26:11,909
we have any Apache airflow users out

00:26:06,839 --> 00:26:14,009
there few that does workflow management

00:26:11,909 --> 00:26:17,940
reason Rancher for a container

00:26:14,009 --> 00:26:21,629
orchestration on AWS redshift is our

00:26:17,940 --> 00:26:26,879
data warehouse we have a data Lake in s3

00:26:21,629 --> 00:26:27,910
and then on the kind of the end of the

00:26:26,879 --> 00:26:29,890
pipe we have

00:26:27,910 --> 00:26:32,860
looker for data visualization and

00:26:29,890 --> 00:26:36,510
business intelligence and some of our

00:26:32,860 --> 00:26:36,510
data scientists are also using Jupiter

00:26:36,840 --> 00:26:41,140
so it's it's a busy diagram let me take

00:26:39,850 --> 00:26:46,600
a minute to walk through the different

00:26:41,140 --> 00:26:48,550
parts of this pipeline so again this is

00:26:46,600 --> 00:26:52,860
offline this is our offline analytic

00:26:48,550 --> 00:26:55,300
systems on the left here you see

00:26:52,860 --> 00:26:57,700
basically all of our upstream systems

00:26:55,300 --> 00:26:59,740
are transactional databases and the

00:26:57,700 --> 00:27:01,960
different cloud services just Anna kind

00:26:59,740 --> 00:27:04,210
of a random sample here of cloud

00:27:01,960 --> 00:27:10,270
services that run car.com and our

00:27:04,210 --> 00:27:13,030
production application Apache airflow so

00:27:10,270 --> 00:27:16,780
we're if you're not familiar with Apache

00:27:13,030 --> 00:27:19,780
airflow it's similar to Spotify Luigi

00:27:16,780 --> 00:27:25,710
project or a newer one is kubernetes our

00:27:19,780 --> 00:27:30,520
go for workflow management we use it to

00:27:25,710 --> 00:27:33,160
run Python scripts docker containers

00:27:30,520 --> 00:27:36,640
bash scripts anything that needs to be

00:27:33,160 --> 00:27:39,540
scheduled or orchestrated and the black

00:27:36,640 --> 00:27:42,340
list here is just a really simple

00:27:39,540 --> 00:27:45,220
concept that privacy concept that we've

00:27:42,340 --> 00:27:47,260
added into our data pipeline it's just a

00:27:45,220 --> 00:27:52,960
it's really just a Python data

00:27:47,260 --> 00:27:55,030
dictionary and github and the basic idea

00:27:52,960 --> 00:27:57,310
with the black list is that this is a

00:27:55,030 --> 00:27:59,400
black list of columns and fields that

00:27:57,310 --> 00:28:03,030
should never make it from our online

00:27:59,400 --> 00:28:09,700
transactional systems into our

00:28:03,030 --> 00:28:14,160
analytical data VPC we're using Amazon

00:28:09,700 --> 00:28:16,600
kms for key management airflow tags are

00:28:14,160 --> 00:28:18,850
responsible for encrypting some of the

00:28:16,600 --> 00:28:21,430
data that comes from our production

00:28:18,850 --> 00:28:26,580
systems and so it can get airflow can

00:28:21,430 --> 00:28:30,580
get data keys from kms to do that and

00:28:26,580 --> 00:28:32,230
then the JSON schema you might not be

00:28:30,580 --> 00:28:37,380
able to see that but right above s3

00:28:32,230 --> 00:28:40,240
we're using JSON schema files and

00:28:37,380 --> 00:28:40,500
storing those right next to our data and

00:28:40,240 --> 00:28:43,190
our

00:28:40,500 --> 00:28:46,530
three buckets so that allows us to

00:28:43,190 --> 00:28:49,620
manage schema evolution as the upstream

00:28:46,530 --> 00:28:51,870
objects and tables change the schema

00:28:49,620 --> 00:28:59,790
just evolves along with that and we

00:28:51,870 --> 00:29:04,650
store that in s3 oh yeah I'm not hot

00:28:59,790 --> 00:29:07,220
enough and then the the end users of our

00:29:04,650 --> 00:29:10,110
data pipeline all the way to the right

00:29:07,220 --> 00:29:14,010
they're using data scientists are using

00:29:10,110 --> 00:29:15,810
Jupiter and building machine learning

00:29:14,010 --> 00:29:18,600
models that can be deployed back into

00:29:15,810 --> 00:29:20,400
production when we have data analysts

00:29:18,600 --> 00:29:24,360
that are using looker is anyone using

00:29:20,400 --> 00:29:28,110
looker it's using a looker okay it's

00:29:24,360 --> 00:29:31,260
similar to meta base or tableau

00:29:28,110 --> 00:29:34,320
it's used by yeah it's used by analysts

00:29:31,260 --> 00:29:35,670
and data scientists so yeah that's a

00:29:34,320 --> 00:29:38,010
high level of all the tools we're using

00:29:35,670 --> 00:29:40,440
now let me dig into some of the places

00:29:38,010 --> 00:29:46,140
in this pipeline where we've automated

00:29:40,440 --> 00:29:47,640
privacy and security first I'm going to

00:29:46,140 --> 00:29:51,480
show you a little bit more about the

00:29:47,640 --> 00:29:54,030
tool DBT it was at the very bottom I

00:29:51,480 --> 00:30:00,570
didn't mention it at the very bottom of

00:29:54,030 --> 00:30:04,410
this law DBT is a and E let's see ELT

00:30:00,570 --> 00:30:08,130
yeah extract load transform so the

00:30:04,410 --> 00:30:11,130
purpose of DBT is to do extract load

00:30:08,130 --> 00:30:13,080
transform and it's really one of the

00:30:11,130 --> 00:30:15,990
main tools that's enabled us to switch

00:30:13,080 --> 00:30:18,660
from kind of doing ETL over to ELT and

00:30:15,990 --> 00:30:21,090
this is just a screenshot of the DBT

00:30:18,660 --> 00:30:22,530
homepage I wanted to call this out and

00:30:21,090 --> 00:30:25,110
give it its own slide because I think

00:30:22,530 --> 00:30:27,810
this tool is really underrated and it

00:30:25,110 --> 00:30:29,700
adds a lot of automation to your data

00:30:27,810 --> 00:30:31,980
pipeline and it really improves business

00:30:29,700 --> 00:30:35,310
processes when you're working with data

00:30:31,980 --> 00:30:40,140
analysts basically short version of that

00:30:35,310 --> 00:30:42,300
is it saves you a lot of time so it

00:30:40,140 --> 00:30:44,970
stands for data build tool and it allows

00:30:42,300 --> 00:30:46,680
you to compose and organize sequel so

00:30:44,970 --> 00:30:48,929
analysts can begin to compose and

00:30:46,680 --> 00:30:53,280
organize their sequel for transforming

00:30:48,929 --> 00:30:54,240
data in in the data warehouse at Cardo

00:30:53,280 --> 00:30:57,090
we don't have

00:30:54,240 --> 00:30:59,190
big data we're not like a social media

00:30:57,090 --> 00:31:01,980
company our data is very complex for a

00:30:59,190 --> 00:31:05,280
financial financial services company and

00:31:01,980 --> 00:31:08,600
so it's extremely important for us to

00:31:05,280 --> 00:31:12,240
have a tool like this that allows us to

00:31:08,600 --> 00:31:14,790
land really complex data in our in our

00:31:12,240 --> 00:31:18,510
data Lake in our data warehouse and then

00:31:14,790 --> 00:31:20,280
to make sense of it for our team of

00:31:18,510 --> 00:31:21,540
analysts across the business to be able

00:31:20,280 --> 00:31:24,360
to make sense of that data and organize

00:31:21,540 --> 00:31:27,420
it in a way that can be checked in to

00:31:24,360 --> 00:31:31,290
get and that's the big advantage of DBT

00:31:27,420 --> 00:31:34,220
is that it allows your analysts to adopt

00:31:31,290 --> 00:31:37,290
an engineering workflow so their sequel

00:31:34,220 --> 00:31:38,760
statements can be checked in to get do

00:31:37,290 --> 00:31:39,929
pull requests on them all the

00:31:38,760 --> 00:31:42,030
traditional things that we do it

00:31:39,929 --> 00:31:45,660
basically combines sequin Jinja to allow

00:31:42,030 --> 00:31:49,940
them to do that it also has a feature

00:31:45,660 --> 00:31:54,480
for dinner generating data dictionary so

00:31:49,940 --> 00:31:56,460
it can generate documentation for your

00:31:54,480 --> 00:31:58,559
models and diagrams like this one so

00:31:56,460 --> 00:32:02,270
this is a data lineage diagram that

00:31:58,559 --> 00:32:05,490
shows at the far right you have a

00:32:02,270 --> 00:32:10,200
finance team customer billing details

00:32:05,490 --> 00:32:12,570
model and all of the models that are

00:32:10,200 --> 00:32:22,650
upstream that are used to build that the

00:32:12,570 --> 00:32:26,390
data data lineage for that model so we

00:32:22,650 --> 00:32:28,860
use Apache airflow to to run DBT for us

00:32:26,390 --> 00:32:30,809
air flow like I said as a workflow

00:32:28,860 --> 00:32:40,980
manager is originally written by Airbnb

00:32:30,809 --> 00:32:44,190
now it's under Apache and sorry an air

00:32:40,980 --> 00:32:47,610
flow terminology this what we're looking

00:32:44,190 --> 00:32:50,670
at here is a job or an air flow

00:32:47,610 --> 00:32:53,160
terminology would be a dag and every

00:32:50,670 --> 00:32:55,470
green box in the dag is an air flow

00:32:53,160 --> 00:32:57,210
operator or an air flow task so some

00:32:55,470 --> 00:33:00,870
individual piece of work that air flow

00:32:57,210 --> 00:33:04,320
is orchestrating in this tag and because

00:33:00,870 --> 00:33:08,100
DBT is just a command-line tool we can

00:33:04,320 --> 00:33:11,820
have one of these operators

00:33:08,100 --> 00:33:13,740
be responsible for running dbt and a

00:33:11,820 --> 00:33:20,340
docker container that has all of our DBT

00:33:13,740 --> 00:33:24,679
code inside of it I wanted to call up

00:33:20,340 --> 00:33:27,510
this project - it's the open-source

00:33:24,679 --> 00:33:30,330
project from Puckle Puckett well docker

00:33:27,510 --> 00:33:32,220
airflow so this is we basically use and

00:33:30,330 --> 00:33:33,899
copy all their configuration settings

00:33:32,220 --> 00:33:36,889
for running air flow and production

00:33:33,899 --> 00:33:40,830
we've just only slightly customized it

00:33:36,889 --> 00:33:43,500
so highly recommend this project and if

00:33:40,830 --> 00:33:46,799
anybody is considering either moving

00:33:43,500 --> 00:33:50,700
airflow into a docker eyes set up on

00:33:46,799 --> 00:33:52,200
kubernetes or starting a new airflow

00:33:50,700 --> 00:33:54,179
project you should definitely check this

00:33:52,200 --> 00:33:57,179
out before you deploy airflow into

00:33:54,179 --> 00:33:59,250
production and we'd love to talk to you

00:33:57,179 --> 00:34:00,690
about you know configuring it and

00:33:59,250 --> 00:34:07,470
setting out because we spent some time

00:34:00,690 --> 00:34:12,240
doing that last year so when air flow

00:34:07,470 --> 00:34:14,040
does run DBT the only thing that you can

00:34:12,240 --> 00:34:16,770
count on is that distributed systems are

00:34:14,040 --> 00:34:18,800
going to fail right so this is what it

00:34:16,770 --> 00:34:22,490
looks like when it fails in slack

00:34:18,800 --> 00:34:25,859
airflow runs the DBT docker container

00:34:22,490 --> 00:34:29,359
things break and we need to know about

00:34:25,859 --> 00:34:32,490
that so airflow is also responsible for

00:34:29,359 --> 00:34:34,859
notifying our team and posting in our

00:34:32,490 --> 00:34:39,260
slack channel as soon as DBT fails and

00:34:34,859 --> 00:34:44,569
because the data build tool produces

00:34:39,260 --> 00:34:46,919
detailed run results we can extract

00:34:44,569 --> 00:34:48,389
exactly where it failed what models

00:34:46,919 --> 00:34:52,190
failed what was the error message that

00:34:48,389 --> 00:34:52,190
occurred when we when it failed

00:34:55,970 --> 00:35:02,390
we're also using DBT to detect new

00:34:59,599 --> 00:35:05,839
potentially sensitive columns that

00:35:02,390 --> 00:35:07,510
arrive in in our data VPC from our

00:35:05,839 --> 00:35:11,210
upstream data sources

00:35:07,510 --> 00:35:13,910
DBT has a data testing framework that's

00:35:11,210 --> 00:35:16,300
built-in it's really simple but it's

00:35:13,910 --> 00:35:19,849
really powerful when you pair it with

00:35:16,300 --> 00:35:21,170
the metadata tables that you have and in

00:35:19,849 --> 00:35:24,319
any of your data warehouses like

00:35:21,170 --> 00:35:26,000
redshift so I mentioned earlier I

00:35:24,319 --> 00:35:28,400
mentioned the blacklist that we have of

00:35:26,000 --> 00:35:30,740
fields and columns in that Python

00:35:28,400 --> 00:35:34,250
dictionary and obviously one of the main

00:35:30,740 --> 00:35:36,740
problems with having static dictionary

00:35:34,250 --> 00:35:41,300
like that and get is that it gets stale

00:35:36,740 --> 00:35:43,730
so our tables and upstream objects are

00:35:41,300 --> 00:35:46,270
constantly changing so this in this

00:35:43,730 --> 00:35:50,420
example the blacklisted fields are

00:35:46,270 --> 00:35:52,640
Salesforce fields if you look at account

00:35:50,420 --> 00:35:54,859
the Billings Street field and Salesforce

00:35:52,640 --> 00:36:00,560
is blacklisted and should not come into

00:35:54,859 --> 00:36:02,150
our offline systems but if new fields

00:36:00,560 --> 00:36:06,260
are added to that account

00:36:02,150 --> 00:36:12,170
maybe password or date of birth then

00:36:06,260 --> 00:36:14,240
this list is immediately stale so we can

00:36:12,170 --> 00:36:17,480
automate checking for sensitive columns

00:36:14,240 --> 00:36:22,130
just by adding a DB t-test and the basic

00:36:17,480 --> 00:36:27,319
idea behind a DB t-test is that you just

00:36:22,130 --> 00:36:31,210
write a sequel statement and if that

00:36:27,319 --> 00:36:36,020
sequel statement returns zero records an

00:36:31,210 --> 00:36:38,690
empty result set then the test passes so

00:36:36,020 --> 00:36:41,690
the idea is that any record that's

00:36:38,690 --> 00:36:43,819
returned by this by this query is an

00:36:41,690 --> 00:36:46,609
offending record it's a record that's

00:36:43,819 --> 00:36:48,859
causing a problem and so that makes it

00:36:46,609 --> 00:36:52,010
really easy to debug because when you

00:36:48,859 --> 00:36:54,470
need to find out what data is causing a

00:36:52,010 --> 00:36:57,020
problem or in this case what columns

00:36:54,470 --> 00:36:59,030
might potentially be sensitive columns

00:36:57,020 --> 00:37:03,980
that have showed up and are in our data

00:36:59,030 --> 00:37:05,839
VPC then we can just run this query and

00:37:03,980 --> 00:37:07,849
see what new columns have arrived and

00:37:05,839 --> 00:37:09,420
what neucom's we need to go back to the

00:37:07,849 --> 00:37:11,490
team back to the business and say

00:37:09,420 --> 00:37:13,799
hey is this sensitive data that we need

00:37:11,490 --> 00:37:22,740
to encrypt should we blacklisted it how

00:37:13,799 --> 00:37:24,150
do we need to handle this and to run

00:37:22,740 --> 00:37:28,349
these tests it's really simple it's just

00:37:24,150 --> 00:37:31,890
a single command DB t-test and then you

00:37:28,349 --> 00:37:34,950
can see the results here so the tests we

00:37:31,890 --> 00:37:36,480
ran it completed with one error and the

00:37:34,950 --> 00:37:41,160
and the tests that we were looking at

00:37:36,480 --> 00:37:43,380
before so pretend that it failed assert

00:37:41,160 --> 00:37:44,490
no sensitive columns and redshift so

00:37:43,380 --> 00:37:46,109
we're testing to see if there's any

00:37:44,490 --> 00:37:47,910
sensitive columns and redshift and we

00:37:46,109 --> 00:37:50,910
got back three results so that means

00:37:47,910 --> 00:37:52,619
that there's three new columns that have

00:37:50,910 --> 00:38:01,140
arrived in redshift that might be

00:37:52,619 --> 00:38:04,799
sensitive that we need to look at so I'm

00:38:01,140 --> 00:38:07,920
curious how many of you have left left a

00:38:04,799 --> 00:38:13,680
company and months later you still had

00:38:07,920 --> 00:38:18,390
access to get repos ssh credentials has

00:38:13,680 --> 00:38:21,390
that is that ever happened anyone yeah

00:38:18,390 --> 00:38:24,900
it's a problem it's a really difficult

00:38:21,390 --> 00:38:26,400
problem to solve so one thing that we've

00:38:24,900 --> 00:38:30,450
been working on a card is trying to

00:38:26,400 --> 00:38:33,540
automate access control so when new

00:38:30,450 --> 00:38:36,630
members join the team new services need

00:38:33,540 --> 00:38:38,599
access to like our data warehouse or a

00:38:36,630 --> 00:38:41,099
data Lake or people leave the company

00:38:38,599 --> 00:38:44,220
any one of those events is going to

00:38:41,099 --> 00:38:47,309
require things to happen access levels

00:38:44,220 --> 00:38:50,280
to be you know revoked access levels to

00:38:47,309 --> 00:38:52,530
change so our security team has built a

00:38:50,280 --> 00:38:55,710
system called gatekeeper and the goal of

00:38:52,530 --> 00:38:58,589
this is to give us a common workflow and

00:38:55,710 --> 00:39:02,069
a common API across the company that we

00:38:58,589 --> 00:39:05,280
can use to for each team to be

00:39:02,069 --> 00:39:07,109
responsible for what off-boarding looks

00:39:05,280 --> 00:39:08,640
like when team members are rotated and

00:39:07,109 --> 00:39:11,220
what onboarding looks like when someone

00:39:08,640 --> 00:39:13,260
new joins the team so those tools

00:39:11,220 --> 00:39:15,680
it's an unfortunately it's a work in

00:39:13,260 --> 00:39:18,299
progress and it's not open source

00:39:15,680 --> 00:39:21,299
maybe next year when I come I can I can

00:39:18,299 --> 00:39:22,500
show you this but having a consistent

00:39:21,299 --> 00:39:25,290
way to

00:39:22,500 --> 00:39:29,070
on-boarding and off-boarding team

00:39:25,290 --> 00:39:32,870
members has I've seen it be proven to be

00:39:29,070 --> 00:39:32,870
essential for protecting customer data

00:39:33,950 --> 00:39:43,050
so yesterday Anton gave a really great

00:39:38,100 --> 00:39:49,650
talk about terraform best practices he

00:39:43,050 --> 00:39:52,380
said it was a it was a little sad

00:39:49,650 --> 00:39:54,360
because he didn't he didn't say anything

00:39:52,380 --> 00:39:55,920
about terraform Enterprise and we're

00:39:54,360 --> 00:39:58,140
using terraform Enterprise and so I was

00:39:55,920 --> 00:40:00,240
curious about you know how how to use

00:39:58,140 --> 00:40:02,100
that in the best way possible but it was

00:40:00,240 --> 00:40:07,260
an excellent talk and we're big users of

00:40:02,100 --> 00:40:09,540
terraform at Carta we we also use it in

00:40:07,260 --> 00:40:13,620
a really kind of microscopic way inside

00:40:09,540 --> 00:40:16,230
of our offline systems to provision

00:40:13,620 --> 00:40:19,170
access to our data Lake to data

00:40:16,230 --> 00:40:20,940
scientist and new machine learning

00:40:19,170 --> 00:40:22,620
services that are coming online and

00:40:20,940 --> 00:40:24,540
going into production to be able to

00:40:22,620 --> 00:40:28,740
restrict access their access to specific

00:40:24,540 --> 00:40:35,280
buckets inside of our s3 data Lake so

00:40:28,740 --> 00:40:42,390
this example shows how a module a

00:40:35,280 --> 00:40:45,510
terraform module that creates a new

00:40:42,390 --> 00:40:47,490
service account user and then the policy

00:40:45,510 --> 00:40:49,830
that that user has that grants them

00:40:47,490 --> 00:40:56,220
access to just one single bucket inside

00:40:49,830 --> 00:40:59,250
of our s3 data Lake we do something

00:40:56,220 --> 00:41:01,800
really similar with red shift for red

00:40:59,250 --> 00:41:04,590
shift access we're using reusing a

00:41:01,800 --> 00:41:08,070
different tool they there there are some

00:41:04,590 --> 00:41:11,090
community terraform modules where you

00:41:08,070 --> 00:41:14,970
can manage red shift grants and revokes

00:41:11,090 --> 00:41:17,220
through terraform but we're using a tool

00:41:14,970 --> 00:41:19,530
called sequel migrate so a sequel

00:41:17,220 --> 00:41:22,530
migrate another shoutout for a great

00:41:19,530 --> 00:41:25,410
open source tool it's written in go it's

00:41:22,530 --> 00:41:28,260
it's a database migrations tool like a

00:41:25,410 --> 00:41:31,260
limbic or like django migrations

00:41:28,260 --> 00:41:32,730
eautiful or maybe Flyway DB you've

00:41:31,260 --> 00:41:35,240
probably all seen at least one of those

00:41:32,730 --> 00:41:35,240
projects

00:41:35,430 --> 00:41:40,710
we've customized it just a little bit to

00:41:37,710 --> 00:41:43,230
support Jinja template the Jinja

00:41:40,710 --> 00:41:45,569
templating language and you can see

00:41:43,230 --> 00:41:47,400
we're like in this example we have a

00:41:45,569 --> 00:41:48,839
password that's coming from a ginger

00:41:47,400 --> 00:41:51,869
template that we can dynamically

00:41:48,839 --> 00:41:54,150
generate but the point is that sequel

00:41:51,869 --> 00:41:59,630
migrate allows us to completely rebuild

00:41:54,150 --> 00:41:59,630
our redshift data warehouse from scratch

00:41:59,960 --> 00:42:06,690
the migration in this example creates a

00:42:04,410 --> 00:42:10,619
redshift service account and grants

00:42:06,690 --> 00:42:14,039
access scoped to a single schema in DBT

00:42:10,619 --> 00:42:16,019
so we're creating a new known entity

00:42:14,039 --> 00:42:18,329
service and that known entity service

00:42:16,019 --> 00:42:20,369
should only be allowed to use the data

00:42:18,329 --> 00:42:26,069
that's in this one specific schema in

00:42:20,369 --> 00:42:29,490
our data warehouse so probably to take

00:42:26,069 --> 00:42:32,420
away from this slide is that data

00:42:29,490 --> 00:42:36,950
migrations are not just for web apps

00:42:32,420 --> 00:42:42,180
they can be used with great advantage to

00:42:36,950 --> 00:42:45,839
to build data warehouses to it's it's

00:42:42,180 --> 00:42:49,950
kind of like the I know you've all heard

00:42:45,839 --> 00:42:52,589
the pets versus cattle analogy what I

00:42:49,950 --> 00:42:54,599
see what I've seen in talking to other

00:42:52,589 --> 00:42:56,549
data engineers is that data warehouses

00:42:54,599 --> 00:43:00,599
are typically treated as pets within

00:42:56,549 --> 00:43:04,499
companies as anyone else seen that yeah

00:43:00,599 --> 00:43:06,420
yeah so it you you have this data

00:43:04,499 --> 00:43:08,249
warehouse and you maybe have a small

00:43:06,420 --> 00:43:10,049
team or maybe even one person that has

00:43:08,249 --> 00:43:13,880
kind of administrative access and they

00:43:10,049 --> 00:43:18,569
carefully groom the data warehouse so

00:43:13,880 --> 00:43:21,239
with a tool like sequel migrate anything

00:43:18,569 --> 00:43:23,430
that changes in that data warehouse has

00:43:21,239 --> 00:43:24,869
to go through code review github just

00:43:23,430 --> 00:43:26,910
all the traditional processes that you

00:43:24,869 --> 00:43:30,210
do just like you're building a piece of

00:43:26,910 --> 00:43:32,160
software so you can begin to treat a

00:43:30,210 --> 00:43:35,329
data warehouse as cattle you can tear it

00:43:32,160 --> 00:43:35,329
down you can rebuild it from scratch

00:43:39,259 --> 00:43:45,509
another tricky part of securing a data

00:43:42,509 --> 00:43:47,130
pipeline is pseudonymity and this is

00:43:45,509 --> 00:43:48,090
something that we're still working on at

00:43:47,130 --> 00:43:50,610
Carta

00:43:48,090 --> 00:43:53,130
so today we have these really coarse

00:43:50,610 --> 00:43:55,440
black lists about data that's that's not

00:43:53,130 --> 00:43:58,050
allowed in our in our V PC and we have

00:43:55,440 --> 00:44:00,350
binary access controls on sensitive

00:43:58,050 --> 00:44:03,000
columns that we're using terraform and

00:44:00,350 --> 00:44:08,520
in access and redshift that we're using

00:44:03,000 --> 00:44:10,770
sequel migrated to control but if we

00:44:08,520 --> 00:44:13,260
have the consent of our customers to use

00:44:10,770 --> 00:44:15,510
for our analysts to use data to improve

00:44:13,260 --> 00:44:18,480
products or for our data scientists to

00:44:15,510 --> 00:44:21,750
use data to build machine learning

00:44:18,480 --> 00:44:26,460
models and improve our products we

00:44:21,750 --> 00:44:30,860
should be able to separate the full data

00:44:26,460 --> 00:44:33,150
set from data that is restricted and

00:44:30,860 --> 00:44:35,630
separate the restricted data from the

00:44:33,150 --> 00:44:41,190
non identical identifiable data and

00:44:35,630 --> 00:44:43,140
grant access to privileged used users

00:44:41,190 --> 00:44:49,170
data scientists and analysts that can

00:44:43,140 --> 00:44:50,340
have access to that anonymized data so I

00:44:49,170 --> 00:44:51,960
want to talk about some of the

00:44:50,340 --> 00:44:55,040
techniques that you could use for

00:44:51,960 --> 00:44:58,500
pseudonymity and student autumn ization

00:44:55,040 --> 00:45:01,890
in your data pipeline the first one is

00:44:58,500 --> 00:45:04,830
to obfuscate the data so obfuscation is

00:45:01,890 --> 00:45:06,840
the process of scrambling or mixing up

00:45:04,830 --> 00:45:08,730
the data the advantage of this is that

00:45:06,840 --> 00:45:11,100
it's it's really easy to do and there's

00:45:08,730 --> 00:45:13,800
built-in support for it in most

00:45:11,100 --> 00:45:16,320
programming languages this example on

00:45:13,800 --> 00:45:20,720
the screen is a Python example of how

00:45:16,320 --> 00:45:23,160
you could do it and the disadvantage is

00:45:20,720 --> 00:45:24,870
like in this example we're taking the

00:45:23,160 --> 00:45:28,140
word Mark Zuckerberg and then scrambling

00:45:24,870 --> 00:45:30,990
it up and it produces this string at the

00:45:28,140 --> 00:45:33,690
bottom so it's just like a puzzle that

00:45:30,990 --> 00:45:35,910
you saw when you're a kid and the

00:45:33,690 --> 00:45:38,760
disadvantage is that it's possible to

00:45:35,910 --> 00:45:41,130
take that text and unscramble it and get

00:45:38,760 --> 00:45:48,080
the real extract the real data from that

00:45:41,130 --> 00:45:51,780
text another technique is masking

00:45:48,080 --> 00:45:54,960
masking is typically used to obscure

00:45:51,780 --> 00:45:59,870
part of the data but leave just enough

00:45:54,960 --> 00:46:02,010
of the data too untouched to be able to

00:45:59,870 --> 00:46:04,890
perform some basic verification

00:46:02,010 --> 00:46:07,050
Shinzon it so everybody seen this

00:46:04,890 --> 00:46:12,690
technique used with a credit card where

00:46:07,050 --> 00:46:14,010
you you know obscure the first part of

00:46:12,690 --> 00:46:18,060
the credit card and just leave the last

00:46:14,010 --> 00:46:20,100
four and so the advantage of this is

00:46:18,060 --> 00:46:22,950
that it's really simple for the owner of

00:46:20,100 --> 00:46:25,920
the data to do some basic verifications

00:46:22,950 --> 00:46:28,230
on their on their card number in this

00:46:25,920 --> 00:46:32,180
example without having to see the full

00:46:28,230 --> 00:46:37,200
full data but the disadvantage is that

00:46:32,180 --> 00:46:38,340
some of some of the data is still stored

00:46:37,200 --> 00:46:40,320
and clear-text

00:46:38,340 --> 00:46:41,970
so in this example you still have some

00:46:40,320 --> 00:46:44,280
of the customers real data you stuff

00:46:41,970 --> 00:46:54,510
that last four of their credit card

00:46:44,280 --> 00:46:56,910
number in clear-text tokenization is

00:46:54,510 --> 00:47:00,290
another technique another technique used

00:46:56,910 --> 00:47:05,310
for anonymizing data tokenizing strings

00:47:00,290 --> 00:47:08,040
of data as a way to protect data at rest

00:47:05,310 --> 00:47:09,380
without having to encrypt the data so

00:47:08,040 --> 00:47:12,630
it's another really simple technique

00:47:09,380 --> 00:47:14,670
faker is a really popular Python library

00:47:12,630 --> 00:47:20,040
anybody has anyone use that for tat for

00:47:14,670 --> 00:47:21,840
a unit test or yeah yeah it's a tip I've

00:47:20,040 --> 00:47:24,900
seen it typically used in testing

00:47:21,840 --> 00:47:28,230
frameworks but it's a possible you know

00:47:24,900 --> 00:47:31,140
to use it in this context for replacing

00:47:28,230 --> 00:47:33,480
real data so phone numbers email

00:47:31,140 --> 00:47:35,970
addresses you can use fake faker to

00:47:33,480 --> 00:47:38,700
generate fake data to replace that real

00:47:35,970 --> 00:47:41,580
data and the advantage of this is that

00:47:38,700 --> 00:47:45,630
it hides sensitive information from

00:47:41,580 --> 00:47:46,980
downstream systems like in in our case

00:47:45,630 --> 00:47:50,370
and looker in our business intelligence

00:47:46,980 --> 00:47:54,570
and data visualization tools that data

00:47:50,370 --> 00:47:56,760
is hidden but it's not going to explode

00:47:54,570 --> 00:47:58,920
when it when it hits downstream because

00:47:56,760 --> 00:48:00,720
the data looks really similar to a real

00:47:58,920 --> 00:48:05,300
name or real address or real phone

00:48:00,720 --> 00:48:08,820
number but the problem with this is

00:48:05,300 --> 00:48:11,130
there's no way for a privileged user who

00:48:08,820 --> 00:48:12,369
does have access to say like our finance

00:48:11,130 --> 00:48:15,640
team if they

00:48:12,369 --> 00:48:18,269
access to the billing address but it's

00:48:15,640 --> 00:48:20,799
been run through faker and faker has

00:48:18,269 --> 00:48:22,630
generated a fake billing address there's

00:48:20,799 --> 00:48:33,549
no way for them to recover the actual

00:48:22,630 --> 00:48:45,970
real data they might know what maybe

00:48:33,549 --> 00:48:48,309
this is from so this is a technique

00:48:45,970 --> 00:48:50,769
called data blurring and you've all seen

00:48:48,309 --> 00:48:52,960
this before so the idea is that you take

00:48:50,769 --> 00:48:54,759
some subset of the data and you create

00:48:52,960 --> 00:48:56,559
like a really coarse approximation of

00:48:54,759 --> 00:48:58,660
the values I've actually used data

00:48:56,559 --> 00:49:00,700
blurring and a couple places to to kind

00:48:58,660 --> 00:49:04,150
of like block out screenshots in this

00:49:00,700 --> 00:49:06,700
presentation and another common example

00:49:04,150 --> 00:49:08,920
where you'll see it is license plate

00:49:06,700 --> 00:49:09,400
numbers so he said the movie it's dumb

00:49:08,920 --> 00:49:11,529
and Dumberer

00:49:09,400 --> 00:49:18,579
so this is a license plate from Dumb and

00:49:11,529 --> 00:49:20,140
Dumber the advantage is that you were

00:49:18,579 --> 00:49:22,390
able to like even if you had seen this

00:49:20,140 --> 00:49:24,869
image without the light without the

00:49:22,390 --> 00:49:27,999
license plate you probably could have

00:49:24,869 --> 00:49:30,309
told me what movie it was from so you

00:49:27,999 --> 00:49:32,109
can still see 95% of the image and just

00:49:30,309 --> 00:49:38,049
like a tiny little percentages is

00:49:32,109 --> 00:49:39,789
blurred out but the disadvantage is is I

00:49:38,049 --> 00:49:44,369
didn't I didn't realize this but you

00:49:39,789 --> 00:49:47,549
could you could reverse engineer this

00:49:44,369 --> 00:49:52,660
there's a great blog post that's

00:49:47,549 --> 00:49:53,499
referenced in my slides where save

00:49:52,660 --> 00:49:55,690
you're trying to reverse engineer

00:49:53,499 --> 00:49:57,009
blurred license plates you can generate

00:49:55,690 --> 00:50:00,640
all the possible license plate

00:49:57,009 --> 00:50:04,059
combinations blur those and then compare

00:50:00,640 --> 00:50:06,279
that to your to your master image and

00:50:04,059 --> 00:50:08,529
try to find which which ones could

00:50:06,279 --> 00:50:10,450
potentially match with image so there's

00:50:08,529 --> 00:50:13,029
disadvantages to using this technique

00:50:10,450 --> 00:50:19,539
don't do that with my slides by the way

00:50:13,029 --> 00:50:20,859
on my sensitive URLs were probably so

00:50:19,539 --> 00:50:23,140
encryption is the one that I really want

00:50:20,859 --> 00:50:25,180
to talk about the encryption is a

00:50:23,140 --> 00:50:25,390
technique that we're using at Carta and

00:50:25,180 --> 00:50:28,930
our

00:50:25,390 --> 00:50:31,720
to pipeline one common thing that you

00:50:28,930 --> 00:50:34,390
hear that came up over and over again

00:50:31,720 --> 00:50:37,480
when we talked to vendors and talked and

00:50:34,390 --> 00:50:41,650
looked at their Sauk reports was oh yeah

00:50:37,480 --> 00:50:44,080
we use encryption at risk that's

00:50:41,650 --> 00:50:46,000
provided by us that's provided by Amazon

00:50:44,080 --> 00:50:48,520
or provided by asher provided by a

00:50:46,000 --> 00:50:49,930
google cloud and that's that is great I

00:50:48,520 --> 00:50:54,640
mean it's great to have that turned on

00:50:49,930 --> 00:50:57,250
but it's it's really insufficient when

00:50:54,640 --> 00:50:59,170
you're talking about restricted access

00:50:57,250 --> 00:51:03,760
to restricted data within an

00:50:59,170 --> 00:51:06,520
organization so we'd like to be able to

00:51:03,760 --> 00:51:08,980
encrypt a subset of sensitive data as it

00:51:06,520 --> 00:51:11,350
moves from from those production systems

00:51:08,980 --> 00:51:13,570
over to the analytic systems like in

00:51:11,350 --> 00:51:16,630
this example say we want to encrypt

00:51:13,570 --> 00:51:20,230
birthdate as it moves from online to

00:51:16,630 --> 00:51:22,630
offline systems and we want the data to

00:51:20,230 --> 00:51:24,790
remain encrypted through the entire data

00:51:22,630 --> 00:51:27,220
pipeline and only allow it to be

00:51:24,790 --> 00:51:29,830
decrypted by a small number of

00:51:27,220 --> 00:51:31,630
authorized users people that have access

00:51:29,830 --> 00:51:35,100
to the encryption keys and people that

00:51:31,630 --> 00:51:37,420
and also have access to the data

00:51:35,100 --> 00:51:39,100
obviously the advantage of encryption

00:51:37,420 --> 00:51:40,690
over some of the other techniques we've

00:51:39,100 --> 00:51:43,030
talked about is that the original data

00:51:40,690 --> 00:51:44,410
can be recovered so if you have that if

00:51:43,030 --> 00:51:47,290
you have access to the data you have

00:51:44,410 --> 00:51:51,070
access to the key you can decrypt that

00:51:47,290 --> 00:51:52,480
data and get the original value but it

00:51:51,070 --> 00:51:57,700
comes with disadvantages and trade-offs

00:51:52,480 --> 00:51:59,410
it adds a lot of complexity to to your

00:51:57,700 --> 00:52:00,940
data pipeline and there's a bunch of

00:51:59,410 --> 00:52:01,960
trade-offs when it comes to how you're

00:52:00,940 --> 00:52:04,470
going to implement your encryption

00:52:01,960 --> 00:52:06,850
scheme whether you're going to use a

00:52:04,470 --> 00:52:08,740
symmetric or symmetric encryption and

00:52:06,850 --> 00:52:11,140
are you gonna use hashing cobalt or you

00:52:08,740 --> 00:52:17,830
can use kms there's lots of things to

00:52:11,140 --> 00:52:21,520
consider so right now we're using the

00:52:17,830 --> 00:52:24,160
AWS key management service it allows you

00:52:21,520 --> 00:52:26,260
to automate key creation and key

00:52:24,160 --> 00:52:28,240
rotation and gives you lots of options

00:52:26,260 --> 00:52:32,170
for how how you want to do that and what

00:52:28,240 --> 00:52:36,490
scheme you want to use so we can use

00:52:32,170 --> 00:52:38,589
terraform to create a customer master

00:52:36,490 --> 00:52:41,349
key in kms

00:52:38,589 --> 00:52:44,200
and then in our airflow tags we can use

00:52:41,349 --> 00:52:46,809
like a tool like bottle three library

00:52:44,200 --> 00:52:49,960
like bottle three to generate a new data

00:52:46,809 --> 00:52:51,880
key so that's what bottle is doing here

00:52:49,960 --> 00:52:56,979
we get the bottle client and then we run

00:52:51,880 --> 00:53:00,670
generate data key so in this example the

00:52:56,979 --> 00:53:02,319
key is not the master customer master

00:53:00,670 --> 00:53:05,739
key is not stored in our environment

00:53:02,319 --> 00:53:06,549
variables so there's no you're not given

00:53:05,739 --> 00:53:10,390
you're not having to worry about

00:53:06,549 --> 00:53:13,809
handling storing your master key we

00:53:10,390 --> 00:53:17,259
strictly just stored the the kms Amazon

00:53:13,809 --> 00:53:19,239
resource number and get the master key

00:53:17,259 --> 00:53:22,180
using that and then generate a data key

00:53:19,239 --> 00:53:24,369
and the data key is never stored in air

00:53:22,180 --> 00:53:27,329
flow it's never written to disk it's

00:53:24,369 --> 00:53:27,329
just used for the encryption

00:53:39,640 --> 00:53:47,480
at Cardo we are heavy heavy users of

00:53:43,790 --> 00:53:51,050
Postgres Postgres comes with a PG crypto

00:53:47,480 --> 00:53:54,380
module that provides you with a variety

00:53:51,050 --> 00:53:56,660
of encryption methods so say we wanted

00:53:54,380 --> 00:53:59,510
to encrypt that birthday column in the

00:53:56,660 --> 00:54:01,340
users table and we want to do that at

00:53:59,510 --> 00:54:03,860
query time when we're when we're

00:54:01,340 --> 00:54:08,690
ingesting data from the users table and

00:54:03,860 --> 00:54:11,600
bringing it into s3 or redshift the PG

00:54:08,690 --> 00:54:15,470
crypto extension has a symmetric

00:54:11,600 --> 00:54:15,860
encryption function yeah you can see it

00:54:15,470 --> 00:54:21,380
right here

00:54:15,860 --> 00:54:25,790
so PGP sim encrypt we can encrypt the

00:54:21,380 --> 00:54:27,140
birthdate column and pass in the

00:54:25,790 --> 00:54:30,800
encryption key obviously it wouldn't be

00:54:27,140 --> 00:54:33,200
hard-coded in your in your code get the

00:54:30,800 --> 00:54:35,570
encryption key out of AWS kms and

00:54:33,200 --> 00:54:38,120
encrypt that data and the image here is

00:54:35,570 --> 00:54:42,260
that we can push that work of encryption

00:54:38,120 --> 00:54:47,600
up into our standby Postgres database

00:54:42,260 --> 00:54:54,320
instances the data will net will never

00:54:47,600 --> 00:54:56,690
arrive in our data VPC and clear text so

00:54:54,320 --> 00:54:58,940
for these sensitive encrypting columns

00:54:56,690 --> 00:55:01,580
the goal is to leave that data encrypted

00:54:58,940 --> 00:55:03,860
from the time it arrives in the data VPC

00:55:01,580 --> 00:55:05,540
in that data pipeline and all the way

00:55:03,860 --> 00:55:09,350
through the pipeline and then justa

00:55:05,540 --> 00:55:11,360
crypt it and the last mile decrypt it at

00:55:09,350 --> 00:55:15,650
run time I guess you could say like when

00:55:11,360 --> 00:55:18,860
when it's being retrieved most of our

00:55:15,650 --> 00:55:21,350
data access is happening against these

00:55:18,860 --> 00:55:23,930
systems is happening in looker in that

00:55:21,350 --> 00:55:28,460
VI data visualization tool then we run

00:55:23,930 --> 00:55:30,580
that on prim inside of our V PC so

00:55:28,460 --> 00:55:33,560
lickers connected directly to our

00:55:30,580 --> 00:55:37,820
redshift data warehouse and just like

00:55:33,560 --> 00:55:39,050
Postgres we can use some encryption

00:55:37,820 --> 00:55:41,660
functions and take advantage of

00:55:39,050 --> 00:55:44,630
encryption inside of redshift redshift

00:55:41,660 --> 00:55:47,060
doesn't have the PG crypto module so it

00:55:44,630 --> 00:55:48,680
can't just use the same symmetric

00:55:47,060 --> 00:55:50,599
encryption functions there

00:55:48,680 --> 00:55:52,400
but it does have support for

00:55:50,599 --> 00:55:54,380
user-defined function Python

00:55:52,400 --> 00:55:57,740
user-defined functions so we can just

00:55:54,380 --> 00:56:01,039
write a function like this one AES

00:55:57,740 --> 00:56:04,910
decrypt that can decrypt that birthday

00:56:01,039 --> 00:56:07,730
column so this is just one way that you

00:56:04,910 --> 00:56:09,950
could encrypt data at the very beginning

00:56:07,730 --> 00:56:17,200
of your data pipeline and then decrypt

00:56:09,950 --> 00:56:21,920
it at runtime at the very end but

00:56:17,200 --> 00:56:23,839
choosing to do this to do this technique

00:56:21,920 --> 00:56:26,210
to encrypt individual columns and

00:56:23,839 --> 00:56:30,140
encrypt subsets of data is definitely

00:56:26,210 --> 00:56:33,230
complicated so encrypting personally

00:56:30,140 --> 00:56:35,299
identifiable information like this like

00:56:33,230 --> 00:56:37,869
the birthdate example comes with a long

00:56:35,299 --> 00:56:41,450
list of decisions because now you've got

00:56:37,869 --> 00:56:45,470
your organization has to manage keys so

00:56:41,450 --> 00:56:50,089
if you use kms you need to decide am I

00:56:45,470 --> 00:56:52,789
gonna let kms buy to do the default load

00:56:50,089 --> 00:56:55,880
rotate my master key annually is that

00:56:52,789 --> 00:56:57,740
enough or do I need to do manual key

00:56:55,880 --> 00:57:02,359
rotation do we want to rotate our key

00:56:57,740 --> 00:57:05,240
every day is it even acceptable to have

00:57:02,359 --> 00:57:09,490
a single master key for all this data so

00:57:05,240 --> 00:57:12,380
then you have to ask yourself should we

00:57:09,490 --> 00:57:14,299
should we create different master keys

00:57:12,380 --> 00:57:17,410
for different groups of users like the

00:57:14,299 --> 00:57:19,849
finance team has a master key and

00:57:17,410 --> 00:57:22,549
marketing has a master key dependent

00:57:19,849 --> 00:57:29,150
only those subsets of data can be

00:57:22,549 --> 00:57:31,369
decrypted or another consideration is

00:57:29,150 --> 00:57:34,190
can we install encryption keys and our

00:57:31,369 --> 00:57:37,400
bi tools behind the scenes are we going

00:57:34,190 --> 00:57:39,289
to ask our analyst to have to learn

00:57:37,400 --> 00:57:42,170
about keys so we're going to push work

00:57:39,289 --> 00:57:45,079
down into our end users and be like yeah

00:57:42,170 --> 00:57:47,059
we need you to get the key from this

00:57:45,079 --> 00:57:48,950
system and plug it in here when you're

00:57:47,059 --> 00:57:51,559
trying to retrieve this certain subset

00:57:48,950 --> 00:57:54,799
of data or is there a way that we as

00:57:51,559 --> 00:57:58,150
data engineers can build the system and

00:57:54,799 --> 00:58:02,299
automate that process of updating keys

00:57:58,150 --> 00:58:05,479
and then under what circumstances

00:58:02,299 --> 00:58:06,799
you need to re-encrypt the data so when

00:58:05,479 --> 00:58:08,689
a team member leaves like we were

00:58:06,799 --> 00:58:10,489
talking about earlier and they have

00:58:08,689 --> 00:58:12,650
access to sensitive data in the company

00:58:10,489 --> 00:58:16,429
how are you gonna handle that are you

00:58:12,650 --> 00:58:20,209
gonna rotate the keys when only certain

00:58:16,429 --> 00:58:21,859
people leave when anyone leaves and is

00:58:20,209 --> 00:58:25,489
rotating the key enough do you need to

00:58:21,859 --> 00:58:28,579
go back and re encrypt certain subsets

00:58:25,489 --> 00:58:30,199
of data encrypt all the data so a column

00:58:28,579 --> 00:58:32,089
level encryption like this it comes with

00:58:30,199 --> 00:58:33,709
a lot of complications a lot of design

00:58:32,089 --> 00:58:37,390
decisions that you would need to hash

00:58:33,709 --> 00:58:37,390
out if you decide to take on the scheme

00:58:41,199 --> 00:58:45,380
so just a quick quickly summarize what

00:58:43,699 --> 00:58:48,289
I've been talking about the three big

00:58:45,380 --> 00:58:50,900
ideas from this talk number one that

00:58:48,289 --> 00:58:53,660
privacy has a strange history I think

00:58:50,900 --> 00:58:55,609
and fortunately we're living at the

00:58:53,660 --> 00:58:58,279
point in history where we have things

00:58:55,609 --> 00:59:00,829
like GD P R and C CPA and it's just

00:58:58,279 --> 00:59:03,229
accepted that it's that privacy is a

00:59:00,829 --> 00:59:06,109
human right that we need to build

00:59:03,229 --> 00:59:07,670
systems to respect this second point is

00:59:06,109 --> 00:59:11,209
that we are responsible for doing that

00:59:07,670 --> 00:59:13,099
we wrestle professionals who customers

00:59:11,209 --> 00:59:16,160
have entrusted with their data and we're

00:59:13,099 --> 00:59:19,249
responsible for it and then the last

00:59:16,160 --> 00:59:22,819
part is that there's lots of tools that

00:59:19,249 --> 00:59:25,069
need to be built for this I showed you

00:59:22,819 --> 00:59:28,849
some examples of tools that we've had

00:59:25,069 --> 00:59:30,920
the custom build certain cases we

00:59:28,849 --> 00:59:32,839
couldn't find or the vendor tool was too

00:59:30,920 --> 00:59:34,459
expensive that would accomplish what we

00:59:32,839 --> 00:59:37,759
needed to or the vendor tool did too

00:59:34,459 --> 00:59:40,249
much lots of opportunities for new small

00:59:37,759 --> 00:59:45,079
single purpose open source tools to be

00:59:40,249 --> 00:59:46,489
built and we can do that we can automate

00:59:45,079 --> 00:59:51,979
the pain out of taking care of our

00:59:46,489 --> 00:59:54,499
customer data so my name is Troy Harvey

00:59:51,979 --> 00:59:56,390
I'm a data engineer at Carta and I just

00:59:54,499 --> 00:59:58,490
wanted to thank you all for your time

00:59:56,390 --> 01:00:05,500
and attention

00:59:58,490 --> 01:00:05,500
[Applause]

01:00:06,580 --> 01:00:10,670

YouTube URL: https://www.youtube.com/watch?v=V_tVuDPWKSw


