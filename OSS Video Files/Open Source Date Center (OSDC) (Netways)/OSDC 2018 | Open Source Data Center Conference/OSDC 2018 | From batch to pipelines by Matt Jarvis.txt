Title: OSDC 2018 | From batch to pipelines by Matt Jarvis
Publication date: 2018-06-22
Playlist: OSDC 2018 | Open Source Data Center Conference
Description: 
	– why Apache Mesos and DC/OS are a solution for emerging patterns in data processing

Apache Mesos is a distributed system for running other distributed systems, often described as a distributed kernel. It’s in use at massive scale at some of the worlds largest companies like Netflix, Uber and Yelp, abstracting entire data centres of hardware to allow for workloads to be distributed efficiently. DC/OS is an open source distribution of Mesos, which adds all the functionality to run Mesos in production across any substrate, both on-premise and in the cloud. In this talk, I’ll introduce both Mesos and DC/OS and talk about how they work under the hood, and what the benefits are of running these new kinds of systems for emerging cloud native workloads.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Google+: https://plus.google.com/+netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:00,540 --> 00:00:18,000
[Music]

00:00:13,490 --> 00:00:19,680
this is this is me Matt Jarvis I work at

00:00:18,000 --> 00:00:25,200
a company called mesosphere in San

00:00:19,680 --> 00:00:26,820
Francisco mesosphere builds software to

00:00:25,200 --> 00:00:27,989
orchestrate clusters and that's kind of

00:00:26,820 --> 00:00:30,029
what I'm going to be talking about in

00:00:27,989 --> 00:00:31,980
this talk today you know I've been

00:00:30,029 --> 00:00:33,690
around open-source software for a pretty

00:00:31,980 --> 00:00:36,030
long time as you can probably tell from

00:00:33,690 --> 00:00:38,760
the gray hair and I've worked in

00:00:36,030 --> 00:00:40,200
development of distributed systems and

00:00:38,760 --> 00:00:41,910
in operating these things so I've seen

00:00:40,200 --> 00:00:45,179
quite a few trends come and go over the

00:00:41,910 --> 00:00:47,309
last over the last few years I'm gonna

00:00:45,179 --> 00:00:48,929
start with a bit of setting the scene

00:00:47,309 --> 00:00:50,940
I'm gonna play a little fast and loose

00:00:48,929 --> 00:00:53,730
with the history of computing in the

00:00:50,940 --> 00:00:56,460
interests of telling a better story so

00:00:53,730 --> 00:01:01,320
please feel free to come and hit me

00:00:56,460 --> 00:01:03,839
afterwards so in the beginning there was

00:01:01,320 --> 00:01:08,760
batch processing mainframes like this

00:01:03,839 --> 00:01:11,580
fine IBM model running single workloads

00:01:08,760 --> 00:01:14,159
to start with and you know in that model

00:01:11,580 --> 00:01:15,780
we got very high utilization rates we

00:01:14,159 --> 00:01:18,390
can generally choose when we're in a

00:01:15,780 --> 00:01:20,430
schedule when to run things so allowing

00:01:18,390 --> 00:01:22,079
for shifting workloads to quieter times

00:01:20,430 --> 00:01:24,540
and the basic idea of this is that we

00:01:22,079 --> 00:01:26,820
take a bunch of data we do some stuff to

00:01:24,540 --> 00:01:29,670
it we return the results and rinse and

00:01:26,820 --> 00:01:33,090
repeat and so it's very transactional

00:01:29,670 --> 00:01:35,100
very well-defined and what that looks

00:01:33,090 --> 00:01:37,290
like from an application perspective is

00:01:35,100 --> 00:01:39,719
this highly monolithic stack where we

00:01:37,290 --> 00:01:41,939
have hardware we have an operating

00:01:39,719 --> 00:01:43,710
system running on that hardware and we

00:01:41,939 --> 00:01:46,799
have a single application that that

00:01:43,710 --> 00:01:49,320
operating system is designed to run and

00:01:46,799 --> 00:01:51,270
as we move through the mini computer era

00:01:49,320 --> 00:01:53,579
to the workstation that pattern remained

00:01:51,270 --> 00:01:55,619
fairly similar and the machines to do it

00:01:53,579 --> 00:01:58,020
got a bit smaller the i/o got a bit

00:01:55,619 --> 00:02:00,600
quicker the storage got a little bit

00:01:58,020 --> 00:02:04,020
bigger not much bigger in in comparison

00:02:00,600 --> 00:02:05,759
to today's machines and in the 70s and

00:02:04,020 --> 00:02:08,070
80s we start to see this emergence of

00:02:05,759 --> 00:02:09,720
this client-server model a little

00:02:08,070 --> 00:02:11,480
machine or lots of little machines

00:02:09,720 --> 00:02:13,440
talking to a big machine but again the

00:02:11,480 --> 00:02:14,850
processing paradigms that we're talking

00:02:13,440 --> 00:02:17,100
about there weren't really that

00:02:14,850 --> 00:02:19,380
different in general was still taking

00:02:17,100 --> 00:02:21,780
some data running processing against

00:02:19,380 --> 00:02:22,730
that data on a single machine inside a

00:02:21,780 --> 00:02:25,019
single box

00:02:22,730 --> 00:02:26,760
but we might push those results

00:02:25,019 --> 00:02:28,739
somewhere else or we might pull our

00:02:26,760 --> 00:02:30,209
starting data from somewhere but

00:02:28,739 --> 00:02:34,590
fundamentally we're still within that

00:02:30,209 --> 00:02:36,360
single box and then in the late 90s and

00:02:34,590 --> 00:02:38,430
early 2000s we start to have this very

00:02:36,360 --> 00:02:40,079
massive growth in the internet and in

00:02:38,430 --> 00:02:43,049
order to deal with these this

00:02:40,079 --> 00:02:44,760
unprecedented transactional volume the

00:02:43,049 --> 00:02:46,590
early web scale companies start to build

00:02:44,760 --> 00:02:48,840
these huge fleets of off-the-shelf

00:02:46,590 --> 00:02:50,459
hardware and the major problem that all

00:02:48,840 --> 00:02:52,709
of those companies were facing was how

00:02:50,459 --> 00:02:56,670
to make most efficient use of those

00:02:52,709 --> 00:02:58,620
computing clusters and so we started to

00:02:56,670 --> 00:03:00,660
see some fairly big paradigm shifts in

00:02:58,620 --> 00:03:02,579
application architecture that are pretty

00:03:00,660 --> 00:03:04,230
familiar to most of us these days and

00:03:02,579 --> 00:03:07,049
one of those new approaches was

00:03:04,230 --> 00:03:09,720
MapReduce programming technique for

00:03:07,049 --> 00:03:13,650
distributed systems which splits tasks

00:03:09,720 --> 00:03:15,239
into two phases and then we we spread

00:03:13,650 --> 00:03:16,650
that out across the cluster operate on

00:03:15,239 --> 00:03:19,200
the smaller bits and pull it all

00:03:16,650 --> 00:03:21,599
together at the end and that originated

00:03:19,200 --> 00:03:23,760
from a inside Google and then was

00:03:21,599 --> 00:03:26,280
generalized most famously into things

00:03:23,760 --> 00:03:28,919
like Hadoop and so now we could all

00:03:26,280 --> 00:03:32,340
parallel eyes processing operations

00:03:28,919 --> 00:03:33,989
across big clusters of servers but even

00:03:32,340 --> 00:03:38,220
in this model was still performing

00:03:33,989 --> 00:03:40,829
effectively a batch operation and the

00:03:38,220 --> 00:03:43,410
issue with that first generation is that

00:03:40,829 --> 00:03:45,450
we were dedicating computing resources

00:03:43,410 --> 00:03:48,030
to each of these single frameworks so we

00:03:45,450 --> 00:03:50,220
subdivide our fleet into various silos

00:03:48,030 --> 00:03:52,709
and those silos are all doing one

00:03:50,220 --> 00:03:54,810
particular kind of work and this is

00:03:52,709 --> 00:03:58,400
generally pretty inefficient in terms of

00:03:54,810 --> 00:04:01,769
resource utilization and we have a

00:03:58,400 --> 00:04:04,410
problem where when we lose machines out

00:04:01,769 --> 00:04:05,970
of out of those out of those individual

00:04:04,410 --> 00:04:08,730
silos it's very difficult for us to

00:04:05,970 --> 00:04:10,500
manage around failure so we end up over

00:04:08,730 --> 00:04:12,449
provisioning to deal with both peaks and

00:04:10,500 --> 00:04:15,090
troughs and to deal with failure

00:04:12,449 --> 00:04:16,650
scenarios and the other problem with

00:04:15,090 --> 00:04:19,289
static partitioning like that is that

00:04:16,650 --> 00:04:22,440
workloads are almost never uniform they

00:04:19,289 --> 00:04:23,699
neither time or resources so we have

00:04:22,440 --> 00:04:25,650
certain workloads that may be

00:04:23,699 --> 00:04:27,510
long-running but not take much resource

00:04:25,650 --> 00:04:29,760
but we have other workloads that are

00:04:27,510 --> 00:04:32,010
highly spiky and we have some they're

00:04:29,760 --> 00:04:34,289
very short-lived and again we have to

00:04:32,010 --> 00:04:36,090
over-provision to deal with that with

00:04:34,289 --> 00:04:38,430
that with those patterns

00:04:36,090 --> 00:04:41,760
a much better approach would be to

00:04:38,430 --> 00:04:43,770
dynamically share resources across our

00:04:41,760 --> 00:04:45,900
entire cluster which would give us much

00:04:43,770 --> 00:04:48,290
higher utilization and a much more

00:04:45,900 --> 00:04:53,130
efficient use of of all of our machines

00:04:48,290 --> 00:04:56,840
and around the same time we start to see

00:04:53,130 --> 00:04:59,070
the the kind of big wave of

00:04:56,840 --> 00:05:01,229
virtualization and virtualization is not

00:04:59,070 --> 00:05:02,460
really that newer technology and I mean

00:05:01,229 --> 00:05:05,910
in some ways you could say it's been

00:05:02,460 --> 00:05:08,310
around since the mainframe era but

00:05:05,910 --> 00:05:10,139
around that kind of time we have a very

00:05:08,310 --> 00:05:11,190
big growth in virtualization and you

00:05:10,139 --> 00:05:12,870
know we're all familiar with what

00:05:11,190 --> 00:05:14,580
virtualization looks like we have a

00:05:12,870 --> 00:05:16,500
hardware platform we have a hypervisor

00:05:14,580 --> 00:05:18,120
and then we have a bunch of virtual

00:05:16,500 --> 00:05:22,470
machines that are running on those on

00:05:18,120 --> 00:05:24,720
that hypervisor and what that does to us

00:05:22,470 --> 00:05:26,630
conceptually is we now have many many

00:05:24,720 --> 00:05:29,789
more machines that we can run them and

00:05:26,630 --> 00:05:32,090
we can more efficiently use those those

00:05:29,789 --> 00:05:34,110
Hardware resources but we also add

00:05:32,090 --> 00:05:35,760
complexity we have a lot more machines

00:05:34,110 --> 00:05:37,740
to manage we have a lot more operating

00:05:35,760 --> 00:05:43,289
systems we have a lot more applications

00:05:37,740 --> 00:05:45,660
and that kind of that that's split in

00:05:43,289 --> 00:05:48,090
the infrastructure layer led us to start

00:05:45,660 --> 00:05:49,740
to subdivide our applications into

00:05:48,090 --> 00:05:52,830
smaller and smaller units the kind of

00:05:49,740 --> 00:05:54,510
rise of micro services matching that sub

00:05:52,830 --> 00:05:56,520
division at the infrastructure layer and

00:05:54,510 --> 00:05:58,770
micro services we know we're all

00:05:56,520 --> 00:06:00,750
designed to be these small autonomous

00:05:58,770 --> 00:06:05,030
things that work together doing one

00:06:00,750 --> 00:06:09,090
thing and doing that one thing well and

00:06:05,030 --> 00:06:11,099
now we've added that complexity the the

00:06:09,090 --> 00:06:13,050
micro services stuff gives us it gives

00:06:11,099 --> 00:06:15,210
us those benefits of being able to scale

00:06:13,050 --> 00:06:17,490
our applications more efficiently but

00:06:15,210 --> 00:06:19,260
what it also does is caused us further

00:06:17,490 --> 00:06:22,020
problems because now when we look at our

00:06:19,260 --> 00:06:23,880
application stack we have our virtual

00:06:22,020 --> 00:06:25,710
machines we have our operating system

00:06:23,880 --> 00:06:28,080
running on the virtual machines and then

00:06:25,710 --> 00:06:30,360
we have many more applications running

00:06:28,080 --> 00:06:32,070
inside those operating systems if we

00:06:30,360 --> 00:06:34,050
start to make any changes to that

00:06:32,070 --> 00:06:35,789
operating system those applications are

00:06:34,050 --> 00:06:37,320
all relying on the same libraries and

00:06:35,789 --> 00:06:42,000
that's potentially going to cause

00:06:37,320 --> 00:06:44,219
problems when we make changes to one or

00:06:42,000 --> 00:06:45,750
many of those applications running on

00:06:44,219 --> 00:06:47,370
our machine so it's very difficult to

00:06:45,750 --> 00:06:49,710
manage and we've got a lot of problems

00:06:47,370 --> 00:06:52,199
with dependencies

00:06:49,710 --> 00:06:53,789
and so in the last few years we've

00:06:52,199 --> 00:06:56,520
started to see widespread adoption of

00:06:53,789 --> 00:06:58,830
container technology and again you know

00:06:56,520 --> 00:07:00,990
container technology for anybody that's

00:06:58,830 --> 00:07:02,820
been around in the UNIX world for for

00:07:00,990 --> 00:07:04,919
for very long you know that those

00:07:02,820 --> 00:07:06,690
container like technologies have been

00:07:04,919 --> 00:07:09,210
around for probably twenty years or so

00:07:06,690 --> 00:07:11,580
but we've started to see very widespread

00:07:09,210 --> 00:07:13,229
adoption of that and you know it's

00:07:11,580 --> 00:07:16,889
probably important to differentiate here

00:07:13,229 --> 00:07:20,060
between the idea of a container as a

00:07:16,889 --> 00:07:23,210
packaging format and the process of

00:07:20,060 --> 00:07:26,130
containerization by which an application

00:07:23,210 --> 00:07:31,139
maintains process isolation and those

00:07:26,130 --> 00:07:34,380
the the the the the process by which the

00:07:31,139 --> 00:07:36,030
the application remains isolated here

00:07:34,380 --> 00:07:37,800
are actually all functions of the linux

00:07:36,030 --> 00:07:39,599
kernel you know containers are not a

00:07:37,800 --> 00:07:40,830
thing is a very famous quote these days

00:07:39,599 --> 00:07:43,139
they're just a bunch of stuff that

00:07:40,830 --> 00:07:45,449
happens in the kernel and you know lots

00:07:43,139 --> 00:07:48,630
of people get can get confused between

00:07:45,449 --> 00:07:50,400
the packaging formats like docker and

00:07:48,630 --> 00:07:52,889
what's actually happening there the

00:07:50,400 --> 00:07:54,840
operating system-level but you know the

00:07:52,889 --> 00:07:57,539
rise of docker really standardized the

00:07:54,840 --> 00:08:01,440
idea of a packaging format and that's

00:07:57,539 --> 00:08:03,419
very important because we end up moving

00:08:01,440 --> 00:08:05,639
from that that scenario where we've got

00:08:03,419 --> 00:08:07,680
dependencies on system libraries to a

00:08:05,639 --> 00:08:08,970
scenario where now our applications are

00:08:07,680 --> 00:08:12,570
packaged together with all their

00:08:08,970 --> 00:08:14,610
dependencies and so we can deploy on top

00:08:12,570 --> 00:08:16,409
of our container runtime all of our

00:08:14,610 --> 00:08:17,880
applications and they're not relying on

00:08:16,409 --> 00:08:19,530
things that are in the operating system

00:08:17,880 --> 00:08:20,490
so we can change stuff in there and

00:08:19,530 --> 00:08:24,270
nothing's gonna happen to our

00:08:20,490 --> 00:08:26,780
applications in theory um you know when

00:08:24,270 --> 00:08:29,610
we first started using containers I

00:08:26,780 --> 00:08:31,139
think that there was a bit of an uphill

00:08:29,610 --> 00:08:33,390
struggle for people to get their heads

00:08:31,139 --> 00:08:34,709
around the best patterns of how we use

00:08:33,390 --> 00:08:36,060
these things you know lots of people

00:08:34,709 --> 00:08:38,339
were like how do I shell into my

00:08:36,060 --> 00:08:40,500
container how do I update things in it

00:08:38,339 --> 00:08:43,440
and and I think we're we're now reaching

00:08:40,500 --> 00:08:45,390
the point where people of have accepted

00:08:43,440 --> 00:08:47,550
the idea that can the the best part of

00:08:45,390 --> 00:08:49,380
containers is that they're immutable so

00:08:47,550 --> 00:08:52,020
when we create them we never change them

00:08:49,380 --> 00:08:54,300
if we if we want to update our container

00:08:52,020 --> 00:08:57,270
we rebuild it we kill the running ones

00:08:54,300 --> 00:09:00,950
and we we we start replacement ones that

00:08:57,270 --> 00:09:00,950
are that have our changes in

00:09:01,050 --> 00:09:05,070
and so at this point we've got a whole

00:09:03,180 --> 00:09:06,899
bunch of benefits from containers we can

00:09:05,070 --> 00:09:09,269
deploy them really rapidly they start up

00:09:06,899 --> 00:09:11,160
very fast we've kind of solved our

00:09:09,269 --> 00:09:13,560
dependency problems somewhat in that

00:09:11,160 --> 00:09:15,750
we're now packaging all of our libraries

00:09:13,560 --> 00:09:18,300
and dependencies together with our

00:09:15,750 --> 00:09:20,250
application and we get benefits of

00:09:18,300 --> 00:09:21,360
things like image repositories so we

00:09:20,250 --> 00:09:23,670
don't need to create everything from

00:09:21,360 --> 00:09:25,620
scratch because we can pull stuff in

00:09:23,670 --> 00:09:28,470
from from repositories we can build on

00:09:25,620 --> 00:09:31,410
other people's work but the big problem

00:09:28,470 --> 00:09:33,089
at this point becomes how do we schedule

00:09:31,410 --> 00:09:35,310
our containers across our whole fleet

00:09:33,089 --> 00:09:37,350
how do we know where they are how do we

00:09:35,310 --> 00:09:43,290
know how do we connect them together and

00:09:37,350 --> 00:09:46,050
what happens if machine dies and so the

00:09:43,290 --> 00:09:49,470
the solution to those problems is the

00:09:46,050 --> 00:09:52,620
idea of container orchestrations an

00:09:49,470 --> 00:09:54,390
overlying system that handles where we

00:09:52,620 --> 00:09:57,360
place containers handles a lot of the

00:09:54,390 --> 00:09:59,250
the underlying plumbing and you know

00:09:57,360 --> 00:10:01,740
whilst this introduces a lot of

00:09:59,250 --> 00:10:05,100
capabilities to us it also introduces a

00:10:01,740 --> 00:10:06,899
lot of complexity and so what container

00:10:05,100 --> 00:10:09,450
orchestrate is generally consists of is

00:10:06,899 --> 00:10:11,760
these kind of three pillars we have

00:10:09,450 --> 00:10:13,980
container scheduling how we have

00:10:11,760 --> 00:10:16,370
resource management and we have service

00:10:13,980 --> 00:10:18,899
management so from the scheduling side

00:10:16,370 --> 00:10:21,209
orchestrators generally are keeping

00:10:18,899 --> 00:10:24,180
track of where we place individual

00:10:21,209 --> 00:10:27,959
containers how do we scale our

00:10:24,180 --> 00:10:30,480
applications horizontally how do we

00:10:27,959 --> 00:10:32,070
resurrect things if things die and how

00:10:30,480 --> 00:10:33,899
do we do upgrades and all that kind of

00:10:32,070 --> 00:10:37,920
stuff and then from the resource

00:10:33,899 --> 00:10:40,020
management side we are interested in our

00:10:37,920 --> 00:10:41,670
overall resource utilization across our

00:10:40,020 --> 00:10:43,290
fleet how do we allocate that most

00:10:41,670 --> 00:10:46,050
effectively and that can be you know

00:10:43,290 --> 00:10:48,779
hardware stuff like memory and CPU but

00:10:46,050 --> 00:10:51,660
also things like IP addresses and ports

00:10:48,779 --> 00:10:53,699
and other artifacts we might need to run

00:10:51,660 --> 00:10:56,640
our containers and then on the service

00:10:53,699 --> 00:10:59,040
management side this is about

00:10:56,640 --> 00:11:01,500
connections between things so which

00:10:59,040 --> 00:11:03,839
which which things need to run before

00:11:01,500 --> 00:11:06,120
other things how do we tell whether

00:11:03,839 --> 00:11:08,550
service is ready or not for it for

00:11:06,120 --> 00:11:12,180
things to connect to it and how do we

00:11:08,550 --> 00:11:13,890
group things together and so once we had

00:11:12,180 --> 00:11:15,010
that container Orchestrator layer a

00:11:13,890 --> 00:11:17,470
really abstract

00:11:15,010 --> 00:11:18,910
away all of the stuff at the bottom so

00:11:17,470 --> 00:11:21,100
right from all of our machine

00:11:18,910 --> 00:11:24,630
infrastructure all of our container

00:11:21,100 --> 00:11:27,540
runtimes our operating systems are all

00:11:24,630 --> 00:11:30,610
abstracted away and we're just left with

00:11:27,540 --> 00:11:33,730
our container orchestration layer that

00:11:30,610 --> 00:11:35,860
treats our underlying infrastructure as

00:11:33,730 --> 00:11:40,320
just pools of resources where machines

00:11:35,860 --> 00:11:40,320
where where applications can be deployed

00:11:40,950 --> 00:11:46,720
but that doesn't really solve all of our

00:11:43,660 --> 00:11:48,880
problems because in the last few years

00:11:46,720 --> 00:11:51,070
we've seen emergence of new classes of

00:11:48,880 --> 00:11:54,010
applications which are not just about

00:11:51,070 --> 00:11:56,080
containers micro-services and these new

00:11:54,010 --> 00:11:58,660
architectures look much more like

00:11:56,080 --> 00:12:01,060
pipelines than that traditional batch

00:11:58,660 --> 00:12:03,670
computing paradigm that we we looked at

00:12:01,060 --> 00:12:05,200
earlier we have events being generated

00:12:03,670 --> 00:12:08,080
by something we might have something

00:12:05,200 --> 00:12:10,090
ingesting those events we probably want

00:12:08,080 --> 00:12:12,700
to do some analysis on that data we may

00:12:10,090 --> 00:12:13,930
want to store it persistently and we

00:12:12,700 --> 00:12:16,420
probably want to do something with the

00:12:13,930 --> 00:12:18,580
results whether that's visualizing it or

00:12:16,420 --> 00:12:22,200
triggering other actions and there's a

00:12:18,580 --> 00:12:24,250
bunch of of things happening in in

00:12:22,200 --> 00:12:26,770
enterprises that are really driving

00:12:24,250 --> 00:12:29,980
these kind of architectures and things

00:12:26,770 --> 00:12:31,510
like real-time advertising which we're

00:12:29,980 --> 00:12:33,400
all familiar with the instant you look

00:12:31,510 --> 00:12:36,490
at something on on Google you're then

00:12:33,400 --> 00:12:40,000
bombarded by adverts for it in financial

00:12:36,490 --> 00:12:42,190
services detecting fraud very quickly in

00:12:40,000 --> 00:12:44,200
in things like credit card transactions

00:12:42,190 --> 00:12:46,420
and then the rise of things like

00:12:44,200 --> 00:12:51,460
Internet of Things were many many more

00:12:46,420 --> 00:12:54,100
producers of data and one of the really

00:12:51,460 --> 00:12:56,200
common patterns that that we mesosphere

00:12:54,100 --> 00:12:58,390
see for this kind of pipeline is what

00:12:56,200 --> 00:13:02,020
people have talked about as the smack

00:12:58,390 --> 00:13:05,350
stack this an acronym stands for spark

00:13:02,020 --> 00:13:07,150
mezzos Acker Cassandra and Kafka and

00:13:05,350 --> 00:13:09,070
we'll come back to maze us in a minute

00:13:07,150 --> 00:13:10,810
but those other applications are all

00:13:09,070 --> 00:13:13,930
part of a class of cloud native

00:13:10,810 --> 00:13:16,900
applications developed specifically for

00:13:13,930 --> 00:13:19,270
this kind of fast data world many of

00:13:16,900 --> 00:13:22,180
them were developed inside big web scale

00:13:19,270 --> 00:13:24,150
companies and then open sourced and so

00:13:22,180 --> 00:13:26,500
now they are available to all of us

00:13:24,150 --> 00:13:28,570
despite their acronym they're actually a

00:13:26,500 --> 00:13:31,240
ton of different variations on this but

00:13:28,570 --> 00:13:33,010
they they all map fairly neatly onto

00:13:31,240 --> 00:13:34,990
that pipeline paradigm with a bunch of

00:13:33,010 --> 00:13:39,580
microservices to kind of interact with

00:13:34,990 --> 00:13:41,500
with the different tools now all of

00:13:39,580 --> 00:13:44,200
those applications are complex

00:13:41,500 --> 00:13:46,720
distributed systems in themselves so the

00:13:44,200 --> 00:13:48,070
requirement for deploying and operating

00:13:46,720 --> 00:13:50,530
those systems have become increasingly

00:13:48,070 --> 00:13:53,230
complex they all have different

00:13:50,530 --> 00:13:55,420
requirements for placement how about

00:13:53,230 --> 00:13:57,520
data locality where do I need to be how

00:13:55,420 --> 00:14:00,280
do I need to be close to my storage how

00:13:57,520 --> 00:14:02,470
do i scale how do I upgrade and we're

00:14:00,280 --> 00:14:04,030
still trying to maintain maximum

00:14:02,470 --> 00:14:06,490
efficiency in terms of resource

00:14:04,030 --> 00:14:10,330
utilization across that entire fleet of

00:14:06,490 --> 00:14:13,120
hardware and one of the most mature

00:14:10,330 --> 00:14:14,590
approaches to this problem and one

00:14:13,120 --> 00:14:17,290
that's particularly well suited to that

00:14:14,590 --> 00:14:19,570
idea of running multiple distributed

00:14:17,290 --> 00:14:23,140
systems within a single substrate is

00:14:19,570 --> 00:14:26,680
apache mozo's and you can think of maze

00:14:23,140 --> 00:14:28,720
us as a distributed kernel so a lot like

00:14:26,680 --> 00:14:32,080
the Linux kernel it's responsible for

00:14:28,720 --> 00:14:33,670
low-level task scheduling and but you

00:14:32,080 --> 00:14:35,320
can also think of it as a distributed

00:14:33,670 --> 00:14:41,050
system for running other distributed

00:14:35,320 --> 00:14:43,630
systems maze off is massively scalable

00:14:41,050 --> 00:14:46,180
and battle-hardened in production to

00:14:43,630 --> 00:14:48,250
multiple tens of thousands of nodes and

00:14:46,180 --> 00:14:49,990
it's in production at some of the

00:14:48,250 --> 00:14:50,920
world's biggest Internet services so if

00:14:49,990 --> 00:14:52,990
you've used Twitter

00:14:50,920 --> 00:14:56,830
if you've used Netflix if you've used

00:14:52,990 --> 00:14:58,240
Yelp then at some point your interaction

00:14:56,830 --> 00:15:02,410
with those services has passed through

00:14:58,240 --> 00:15:03,880
mazes um to talk a little bit about the

00:15:02,410 --> 00:15:06,040
history of mazes so makes us was a

00:15:03,880 --> 00:15:07,930
started out life as a research project

00:15:06,040 --> 00:15:11,200
at the University of California Berkeley

00:15:07,930 --> 00:15:15,310
and the original team who wrote maze us

00:15:11,200 --> 00:15:17,790
also wrote Apache spark as a example

00:15:15,310 --> 00:15:20,890
framework for running on top of maze O's

00:15:17,790 --> 00:15:24,880
and Ben Heineman who's one of the

00:15:20,890 --> 00:15:26,830
founders of Earth mesosphere had friends

00:15:24,880 --> 00:15:29,410
who were working at Twitter and he went

00:15:26,830 --> 00:15:32,350
into to Twitter to give a talk about

00:15:29,410 --> 00:15:35,230
about maze us and at the time Twitter

00:15:32,350 --> 00:15:37,690
were we're experiencing a lot of scaling

00:15:35,230 --> 00:15:40,420
problems and for those of you old enough

00:15:37,690 --> 00:15:41,950
to remember you remain a room this image

00:15:40,420 --> 00:15:43,870
to fail whale

00:15:41,950 --> 00:15:45,520
which Twitter used to put up on their

00:15:43,870 --> 00:15:47,710
website whenever the service went down

00:15:45,520 --> 00:15:50,560
and this was a very regular site on on

00:15:47,710 --> 00:15:54,610
Twitter site during the the early years

00:15:50,560 --> 00:15:57,610
of them scaling and so been and Twitter

00:15:54,610 --> 00:15:59,620
eventually decided to to implement maize

00:15:57,610 --> 00:16:03,460
Asin and really that's why you don't see

00:15:59,620 --> 00:16:05,020
that fail well anymore that research

00:16:03,460 --> 00:16:08,590
group then published a pretty famous

00:16:05,020 --> 00:16:11,530
distributed systems paper about about

00:16:08,590 --> 00:16:14,620
maize us and and maize us then went and

00:16:11,530 --> 00:16:17,590
into the Apache foundation as a as an

00:16:14,620 --> 00:16:18,940
Apache project and you know made us is

00:16:17,590 --> 00:16:22,210
under the governance of the Apache

00:16:18,940 --> 00:16:24,510
foundation so there are many different

00:16:22,210 --> 00:16:27,850
contributors to the to the maize us

00:16:24,510 --> 00:16:31,570
source code and a bit further down the

00:16:27,850 --> 00:16:35,280
line been and a couple of his friends

00:16:31,570 --> 00:16:38,230
started mesosphere to actually build a

00:16:35,280 --> 00:16:39,700
project on top of mazes to make mazes

00:16:38,230 --> 00:16:43,870
easier to use which we'll talk about in

00:16:39,700 --> 00:16:45,580
a second and so what the other

00:16:43,870 --> 00:16:47,740
interesting thing to point out is that

00:16:45,580 --> 00:16:49,360
you know in this in this world of

00:16:47,740 --> 00:16:51,280
large-scale cluster orchestration maze

00:16:49,360 --> 00:16:55,210
us was kind of building on stuff that

00:16:51,280 --> 00:16:56,890
did exist ideas that had already existed

00:16:55,210 --> 00:16:58,780
inside some of the really big web scale

00:16:56,890 --> 00:17:01,330
companies some of those companies who I

00:16:58,780 --> 00:17:04,030
said at the start were the first ones to

00:17:01,330 --> 00:17:06,310
really struggle with this concept of how

00:17:04,030 --> 00:17:08,680
do I make most efficient use of very

00:17:06,310 --> 00:17:11,020
large computing clusters so inside

00:17:08,680 --> 00:17:13,300
Google and Facebook there had been work

00:17:11,020 --> 00:17:16,630
done already but in in a proprietary

00:17:13,300 --> 00:17:18,970
sense and and the the the original maze

00:17:16,630 --> 00:17:21,370
us group were in touch with those folks

00:17:18,970 --> 00:17:24,160
as well about you know the best way to

00:17:21,370 --> 00:17:25,780
overcome some of the challenges that the

00:17:24,160 --> 00:17:29,830
both Google and Facebook had seen we're

00:17:25,780 --> 00:17:31,810
trying to build these kind of systems so

00:17:29,830 --> 00:17:34,990
back to maze offs and the key

00:17:31,810 --> 00:17:37,060
differentiator between maze us and all

00:17:34,990 --> 00:17:39,130
other kind of cluster orchestration

00:17:37,060 --> 00:17:42,640
systems is that maze us has this concept

00:17:39,130 --> 00:17:44,200
of of two-level scheduling and I'll talk

00:17:42,640 --> 00:17:46,630
about that in a second I'll just talk

00:17:44,200 --> 00:17:48,460
through what the the base architecture

00:17:46,630 --> 00:17:51,160
of what I am what amazed us cluster

00:17:48,460 --> 00:17:53,830
looks like so we have a zookeeper

00:17:51,160 --> 00:17:55,100
cluster the zookeeper cluster is there

00:17:53,830 --> 00:17:58,730
to

00:17:55,100 --> 00:18:00,679
election of masters it's also there to

00:17:58,730 --> 00:18:02,600
do some small key value stuff but it's

00:18:00,679 --> 00:18:04,970
mainly use for praxis election of

00:18:02,600 --> 00:18:07,789
masters will then have a number of

00:18:04,970 --> 00:18:10,759
masters normally in a odd number so that

00:18:07,789 --> 00:18:13,279
we avoid the problem of split braining

00:18:10,759 --> 00:18:15,679
in the cluster if we lose master nodes

00:18:13,279 --> 00:18:17,779
and one of those master nodes is elected

00:18:15,679 --> 00:18:19,850
as the leader and then we all have a

00:18:17,779 --> 00:18:22,880
number of slaves these are actually

00:18:19,850 --> 00:18:27,110
called agents in in modern parlance this

00:18:22,880 --> 00:18:29,899
this image is slightly old and those

00:18:27,110 --> 00:18:31,730
slaves will effectively be all of your

00:18:29,899 --> 00:18:37,039
hardware will be running that that's

00:18:31,730 --> 00:18:38,750
those slave processes and the the only

00:18:37,039 --> 00:18:41,570
thing that a master really cares about

00:18:38,750 --> 00:18:43,789
is making offers to what Moses calls

00:18:41,570 --> 00:18:47,289
frameworks which are effectively other

00:18:43,789 --> 00:18:50,590
applications and a framework has its own

00:18:47,289 --> 00:18:53,120
scheduler so the mazes master is not

00:18:50,590 --> 00:18:55,070
deciding where to place workloads all

00:18:53,120 --> 00:18:56,929
the mazes master is doing is saying I

00:18:55,070 --> 00:19:01,340
have this resource available do you want

00:18:56,929 --> 00:19:03,649
it and this is a concept called non

00:19:01,340 --> 00:19:06,440
monolithic scheduling and in distributed

00:19:03,649 --> 00:19:08,779
systems it's it's pretty much validated

00:19:06,440 --> 00:19:11,480
as the way to scale to very big cluster

00:19:08,779 --> 00:19:12,889
sizes and because the the master doesn't

00:19:11,480 --> 00:19:15,559
need to know about the placement of

00:19:12,889 --> 00:19:18,190
every single workload on the cluster so

00:19:15,559 --> 00:19:22,009
a framework has a scheduler and it has

00:19:18,190 --> 00:19:24,590
executors and executors run on the on

00:19:22,009 --> 00:19:27,950
the agents at the bottom on your actual

00:19:24,590 --> 00:19:30,590
hardware and those executors are tasked

00:19:27,950 --> 00:19:32,299
with actually doing work so if I need to

00:19:30,590 --> 00:19:34,820
spawn a particular process from a

00:19:32,299 --> 00:19:37,460
framework that's what the executor will

00:19:34,820 --> 00:19:39,649
do and that communication all happens

00:19:37,460 --> 00:19:42,259
through the master so the framework

00:19:39,649 --> 00:19:45,470
scheduler will will accept a resource

00:19:42,259 --> 00:19:51,169
offer and then cause that that executed

00:19:45,470 --> 00:19:53,000
to actually execute the task and so what

00:19:51,169 --> 00:19:56,870
does that mean for our datacenter is a

00:19:53,000 --> 00:19:58,370
very broad brush sense is that you know

00:19:56,870 --> 00:20:00,169
what with this is a slightly different

00:19:58,370 --> 00:20:01,730
view that static partitioning on the

00:20:00,169 --> 00:20:03,649
left where we see you know single

00:20:01,730 --> 00:20:07,010
machines or groups of machines allocated

00:20:03,649 --> 00:20:08,930
to to a particular workload what happens

00:20:07,010 --> 00:20:11,540
in May zoast is that those work

00:20:08,930 --> 00:20:14,360
to multiplex together onto individual

00:20:11,540 --> 00:20:18,080
machines and so we get much higher

00:20:14,360 --> 00:20:20,180
density and the the those those

00:20:18,080 --> 00:20:23,390
processes are isolated from each other

00:20:20,180 --> 00:20:28,070
sometimes by in a docker container sense

00:20:23,390 --> 00:20:30,320
but in other in other ways just by Mesa

00:20:28,070 --> 00:20:32,180
natively handling the stuff in the

00:20:30,320 --> 00:20:34,310
kernel to separate out those processes

00:20:32,180 --> 00:20:36,560
may this actually predates docker so

00:20:34,310 --> 00:20:38,360
mais us has its own concept to contain

00:20:36,560 --> 00:20:43,160
arises which we'll talk about in a

00:20:38,360 --> 00:20:45,680
second so made us this primarily API

00:20:43,160 --> 00:20:48,380
driven does have a UI but as you can see

00:20:45,680 --> 00:20:50,480
it's not massively user-friendly it's a

00:20:48,380 --> 00:20:51,740
bunch of view id's and stuff I can't

00:20:50,480 --> 00:20:53,390
really tell what's going on in my

00:20:51,740 --> 00:20:55,130
cluster and there's a bunch of things

00:20:53,390 --> 00:20:57,860
that will come on to later that that

00:20:55,130 --> 00:21:01,070
made us doesn't give you natively and we

00:20:57,860 --> 00:21:05,150
don't really have much much real insight

00:21:01,070 --> 00:21:07,070
there and for the same reasons that that

00:21:05,150 --> 00:21:09,200
their main source is that powerful

00:21:07,070 --> 00:21:10,970
abstraction it's also pretty complicated

00:21:09,200 --> 00:21:14,480
to use in production and there are a

00:21:10,970 --> 00:21:16,760
bunch of stuff that you need to scaffold

00:21:14,480 --> 00:21:20,110
on top of it in order to give you that

00:21:16,760 --> 00:21:22,490
full scope of what we expect from modern

00:21:20,110 --> 00:21:24,560
cluster orchestrators in terms of

00:21:22,490 --> 00:21:28,000
operational stuff like monitoring and

00:21:24,560 --> 00:21:30,260
logging and deployment automation

00:21:28,000 --> 00:21:31,820
service discovery load balancing all

00:21:30,260 --> 00:21:34,340
that kind of stuff and most of those

00:21:31,820 --> 00:21:37,610
those early web scale users developed

00:21:34,340 --> 00:21:41,060
their own systems in house on top of

00:21:37,610 --> 00:21:43,460
maze oaths but mesosphere was really

00:21:41,060 --> 00:21:45,650
formed to kind of democratize that so

00:21:43,460 --> 00:21:47,810
that you know companies and users who

00:21:45,650 --> 00:21:50,920
didn't have access to a team of 100

00:21:47,810 --> 00:21:53,960
engineers just to work on the on the

00:21:50,920 --> 00:21:56,900
scaffold around mazes to be able to to

00:21:53,960 --> 00:21:58,670
have something to use and so DC OS which

00:21:56,900 --> 00:22:01,180
stands for data center operating system

00:21:58,670 --> 00:22:05,540
is basically a distribution it made us

00:22:01,180 --> 00:22:07,700
along with the UI CLI and all the bits

00:22:05,540 --> 00:22:09,500
that you need to run may sauce in

00:22:07,700 --> 00:22:12,290
production environments monitoring

00:22:09,500 --> 00:22:13,540
security and most importantly a catalog

00:22:12,290 --> 00:22:15,770
of pre-configured

00:22:13,540 --> 00:22:18,440
applications you know and by

00:22:15,770 --> 00:22:21,859
applications I mean services /

00:22:18,440 --> 00:22:24,599
frameworks that are

00:22:21,859 --> 00:22:26,999
encoding best operational practice for

00:22:24,599 --> 00:22:28,889
these distributed systems that run on

00:22:26,999 --> 00:22:30,710
top of DCs and we'll talk about that in

00:22:28,889 --> 00:22:33,450
a second

00:22:30,710 --> 00:22:35,190
so d2 s is an is an open source project

00:22:33,450 --> 00:22:38,399
mesosphere do have an Enterprise version

00:22:35,190 --> 00:22:42,330
add some stuff that enterprises tend to

00:22:38,399 --> 00:22:47,669
care about like integration with with

00:22:42,330 --> 00:22:49,830
enterprise all systems and some

00:22:47,669 --> 00:22:51,629
additional security features a deeper

00:22:49,830 --> 00:22:53,159
are back but the the open source project

00:22:51,629 --> 00:22:55,919
is fully functional

00:22:53,159 --> 00:22:58,649
it's all the things that are available

00:22:55,919 --> 00:23:02,009
for you to use there it's not limited in

00:22:58,649 --> 00:23:04,679
any way and it's it's basically an

00:23:02,009 --> 00:23:06,059
umbrella project for a whole bunch of

00:23:04,679 --> 00:23:10,919
other stuff around 30 different

00:23:06,059 --> 00:23:12,779
open-source projects so when we look at

00:23:10,919 --> 00:23:14,099
DC OS from an architecture perspective

00:23:12,779 --> 00:23:16,019
this is kind of what it looks like so

00:23:14,099 --> 00:23:20,339
we've got may's offset at that bottom

00:23:16,019 --> 00:23:23,009
layer and then on top of on top of Mazur

00:23:20,339 --> 00:23:24,570
so as I said we've got some native

00:23:23,009 --> 00:23:26,729
container orchestration which I'll talk

00:23:24,570 --> 00:23:28,619
about in a minute a whole bunch of

00:23:26,729 --> 00:23:31,289
security features monitoring and

00:23:28,619 --> 00:23:33,149
operations and then ways for you to

00:23:31,289 --> 00:23:36,210
interact with the system so the UI the

00:23:33,149 --> 00:23:40,799
CLI the API and then at the top we have

00:23:36,210 --> 00:23:43,710
a catalog of applications that are super

00:23:40,799 --> 00:23:45,599
easy to deploy and then we can run DTS

00:23:43,710 --> 00:23:48,629
pretty much anywhere so it runs on on

00:23:45,599 --> 00:23:51,149
bare metal on virtual machines you can

00:23:48,629 --> 00:23:54,330
really on private cloud and you know a

00:23:51,149 --> 00:24:00,690
run on your public cloud provider of

00:23:54,330 --> 00:24:03,450
choice and we generally see about 60 70

00:24:00,690 --> 00:24:07,289
% of DTS users are really about running

00:24:03,450 --> 00:24:08,879
this on Prem because gives you that kind

00:24:07,289 --> 00:24:10,889
of public cloud flexibility without

00:24:08,879 --> 00:24:13,080
actually being in public cloud in the

00:24:10,889 --> 00:24:15,210
enterprise project there's actually a

00:24:13,080 --> 00:24:16,619
whole bunch of hybrid cloud features

00:24:15,210 --> 00:24:17,820
where you can do cloud bursting and all

00:24:16,619 --> 00:24:23,580
that kind of stuff but I'm not going to

00:24:17,820 --> 00:24:25,169
talk about that so from a from a UI

00:24:23,580 --> 00:24:27,599
perspective we could see straight away

00:24:25,169 --> 00:24:29,780
the DC OS gives us a much clearer

00:24:27,599 --> 00:24:32,990
picture of what's going on in our

00:24:29,780 --> 00:24:35,360
in our cluster it's it's much more

00:24:32,990 --> 00:24:37,280
focused on the operational things that

00:24:35,360 --> 00:24:39,560
we need to see to see whether whether

00:24:37,280 --> 00:24:41,090
what we're trying to do is is working

00:24:39,560 --> 00:24:42,980
correctly so we can see in this view

00:24:41,090 --> 00:24:44,720
we've got there the health of the

00:24:42,980 --> 00:24:48,080
applications that are running within our

00:24:44,720 --> 00:24:50,630
our cluster and you've you've through

00:24:48,080 --> 00:24:52,430
the CLI and API you've got access to all

00:24:50,630 --> 00:24:54,730
of the functionality so it's very easy

00:24:52,430 --> 00:24:59,390
to automate very easy to integrate with

00:24:54,730 --> 00:25:03,590
and so the the catalog which is one of

00:24:59,390 --> 00:25:07,480
the key kind of functions of of d2s is I

00:25:03,590 --> 00:25:11,020
think up to a I think this 50 or 60

00:25:07,480 --> 00:25:13,340
applications prepackaged in there and

00:25:11,020 --> 00:25:15,320
that we have this this distinction

00:25:13,340 --> 00:25:17,470
between certified and community so the

00:25:15,320 --> 00:25:20,780
certified ones are in the mesosphere QA

00:25:17,470 --> 00:25:22,220
chain so we're testing nose and the but

00:25:20,780 --> 00:25:24,440
there are a whole bunch of other stuff

00:25:22,220 --> 00:25:28,010
in there that's that's submitted by the

00:25:24,440 --> 00:25:30,080
community in most cases the the the

00:25:28,010 --> 00:25:32,240
complex things like caf-co and elastic

00:25:30,080 --> 00:25:34,010
and stuff the the the folks who are

00:25:32,240 --> 00:25:35,870
building and maintaining those packages

00:25:34,010 --> 00:25:39,380
are actually the the creators of that

00:25:35,870 --> 00:25:42,550
software so when you deploy something

00:25:39,380 --> 00:25:45,680
like Kafka from within DCs then it's um

00:25:42,550 --> 00:25:47,450
it's LLL give you straightaway a working

00:25:45,680 --> 00:25:51,320
clustered version of that that

00:25:47,450 --> 00:25:53,180
particular application and we can deploy

00:25:51,320 --> 00:25:55,430
in a number of ways we can do it just

00:25:53,180 --> 00:25:57,860
through the through the UI in most cases

00:25:55,430 --> 00:25:58,880
we can accept the defaults and you're

00:25:57,860 --> 00:26:00,170
gonna have something that's going to

00:25:58,880 --> 00:26:04,420
work straight out of the box and

00:26:00,170 --> 00:26:06,560
nothing's hidden from you all of the the

00:26:04,420 --> 00:26:09,080
configuration elements even within the

00:26:06,560 --> 00:26:10,280
UI are available to you to change and

00:26:09,080 --> 00:26:13,160
it's literally just a case of clicking

00:26:10,280 --> 00:26:15,070
the button but in general more people

00:26:13,160 --> 00:26:18,320
are more likely to deploy things

00:26:15,070 --> 00:26:21,710
programmatically using the the CLI so we

00:26:18,320 --> 00:26:24,140
can define a song like this and then we

00:26:21,710 --> 00:26:26,540
can run a single command DCOs package

00:26:24,140 --> 00:26:29,600
install our application name with

00:26:26,540 --> 00:26:35,930
options to give it a config file and

00:26:29,600 --> 00:26:37,700
deploy applications like that and coming

00:26:35,930 --> 00:26:41,179
back to the idea that that I said

00:26:37,700 --> 00:26:43,130
earlier about these aren't applications

00:26:41,179 --> 00:26:45,260
in the typical sense that

00:26:43,130 --> 00:26:48,170
you know we might install a package in

00:26:45,260 --> 00:26:52,340
our operating system because most of

00:26:48,170 --> 00:26:53,990
these these applications have their own

00:26:52,340 --> 00:26:57,500
framework scheduler and the framework

00:26:53,990 --> 00:26:59,510
scheduler is doing more than just patent

00:26:57,500 --> 00:27:00,920
unpacking the application is actively

00:26:59,510 --> 00:27:03,590
managing the lifecycle of that

00:27:00,920 --> 00:27:05,240
application so if we lose an element and

00:27:03,590 --> 00:27:06,920
lose a particular piece of that

00:27:05,240 --> 00:27:09,470
application then it's going to be

00:27:06,920 --> 00:27:11,600
respawned and we can we can have a look

00:27:09,470 --> 00:27:15,260
at some of the some of the things that

00:27:11,600 --> 00:27:17,600
that typically an application package

00:27:15,260 --> 00:27:20,750
will provide through using this plan

00:27:17,600 --> 00:27:23,150
list the command and and generally these

00:27:20,750 --> 00:27:27,110
these these services that are built

00:27:23,150 --> 00:27:29,480
using the the using our SDK have the

00:27:27,110 --> 00:27:32,210
this this plan command which will show

00:27:29,480 --> 00:27:34,550
you the typical kind of operational

00:27:32,210 --> 00:27:37,430
things that the framework scheduler can

00:27:34,550 --> 00:27:40,850
do if you need to so it's typically

00:27:37,430 --> 00:27:43,010
things like scaling like upgrading or

00:27:40,850 --> 00:27:45,740
recovering from from our hardware

00:27:43,010 --> 00:27:47,510
failure and when we look a bit deeper

00:27:45,740 --> 00:27:49,430
into the plan these are these are all

00:27:47,510 --> 00:27:54,320
from the from the kubernetes package

00:27:49,430 --> 00:27:58,400
that that is in the in the catalog we

00:27:54,320 --> 00:28:00,530
can look at the the ordering that that

00:27:58,400 --> 00:28:02,150
the scheduler will use in order to do a

00:28:00,530 --> 00:28:04,610
particular task so this is the

00:28:02,150 --> 00:28:06,140
employment of kubernetes it's cut off at

00:28:04,610 --> 00:28:09,200
the bottom because this is a huge long

00:28:06,140 --> 00:28:10,940
long thing here but we can see that it

00:28:09,200 --> 00:28:12,980
knows about the order that it needs to

00:28:10,940 --> 00:28:15,200
do things in so when we deploy our

00:28:12,980 --> 00:28:16,820
kubernetes cluster he knows that it

00:28:15,200 --> 00:28:19,430
needs to bring up a net CD cluster to

00:28:16,820 --> 00:28:21,590
start with it can only do that's the HCD

00:28:19,430 --> 00:28:23,510
part serially because the sed cluster

00:28:21,590 --> 00:28:25,550
needs to actually talk to its talk to

00:28:23,510 --> 00:28:27,470
each other but then we can also see that

00:28:25,550 --> 00:28:30,350
it knows that certain elements of this

00:28:27,470 --> 00:28:32,330
plan it can do in parallel so it's

00:28:30,350 --> 00:28:34,670
encoding all of these things this

00:28:32,330 --> 00:28:39,110
operational knowledge of how you might

00:28:34,670 --> 00:28:40,940
do particular tasks so when we're

00:28:39,110 --> 00:28:42,830
dealing with with orchestration systems

00:28:40,940 --> 00:28:44,990
one of the most critical elements is

00:28:42,830 --> 00:28:48,620
service discovery you know we might have

00:28:44,990 --> 00:28:50,420
lots of machines we might have lots of

00:28:48,620 --> 00:28:53,060
applications how do we know where our

00:28:50,420 --> 00:28:55,610
applications are particularly when we're

00:28:53,060 --> 00:28:56,840
working in environments where IPs and

00:28:55,610 --> 00:28:58,940
ports may be dying

00:28:56,840 --> 00:29:02,840
Italy allocated so they can be pretty

00:28:58,940 --> 00:29:05,000
much anything and our actual services

00:29:02,840 --> 00:29:07,250
themselves may be dynamically changing

00:29:05,000 --> 00:29:09,260
so if we lose a particular node those

00:29:07,250 --> 00:29:12,590
containers maybe respawn somewhere else

00:29:09,260 --> 00:29:14,779
and so it's very difficult in this

00:29:12,590 --> 00:29:16,669
picture here for us to know how does my

00:29:14,779 --> 00:29:20,600
client application know where my service

00:29:16,669 --> 00:29:22,730
applications are and so the typical

00:29:20,600 --> 00:29:27,799
model for how we manage this is service

00:29:22,730 --> 00:29:30,350
discovery so when our services start up

00:29:27,799 --> 00:29:33,950
they will register with with some

00:29:30,350 --> 00:29:35,179
central registry mechanism and we might

00:29:33,950 --> 00:29:37,370
have something like a load balancer

00:29:35,179 --> 00:29:40,010
which is querying that registry and

00:29:37,370 --> 00:29:41,330
updating itself all the time as those

00:29:40,010 --> 00:29:44,120
particular elements of that application

00:29:41,330 --> 00:29:45,770
change and so then our client only needs

00:29:44,120 --> 00:29:47,570
to know about where our load balancer is

00:29:45,770 --> 00:29:49,370
and it doesn't really care about where

00:29:47,570 --> 00:29:52,460
any of these services happen to be at a

00:29:49,370 --> 00:29:55,480
particular point in time and in this

00:29:52,460 --> 00:29:58,370
u.s. we have two mechanisms for for

00:29:55,480 --> 00:30:00,740
service discovery the first one is it's

00:29:58,370 --> 00:30:04,039
called missus DNS and you know as the

00:30:00,740 --> 00:30:06,470
name suggests its DNS based so every

00:30:04,039 --> 00:30:10,010
element of every application every task

00:30:06,470 --> 00:30:13,419
that Mesa spawns gets allocated a DNS

00:30:10,010 --> 00:30:16,250
entry in this particular format and

00:30:13,419 --> 00:30:18,230
clearly when you have multiple instances

00:30:16,250 --> 00:30:19,730
of a particular service you're going to

00:30:18,230 --> 00:30:23,059
get some basic round-robin load

00:30:19,730 --> 00:30:26,270
balancing there but the complexity here

00:30:23,059 --> 00:30:28,760
is is that we need to know the port of

00:30:26,270 --> 00:30:31,279
that that particular task or container

00:30:28,760 --> 00:30:33,500
may be may be running on and so any

00:30:31,279 --> 00:30:35,960
application that wants to use maysa DNS

00:30:33,500 --> 00:30:37,669
to resolve things needs to know not only

00:30:35,960 --> 00:30:39,679
how to look up an a record which pretty

00:30:37,669 --> 00:30:42,789
much everything does but how to look up

00:30:39,679 --> 00:30:45,409
an SRV record for that to get that port

00:30:42,789 --> 00:30:47,450
depending on your DNS configuration DNS

00:30:45,409 --> 00:30:49,250
can also be slow to update so if we've

00:30:47,450 --> 00:30:52,580
got very dynamic environments that might

00:30:49,250 --> 00:30:54,980
be somewhat of a downside so the second

00:30:52,580 --> 00:30:59,720
mechanism is this mechanism called named

00:30:54,980 --> 00:31:01,669
VIPs and this is it gives it allocates

00:30:59,720 --> 00:31:04,130
this name-based virtual IP this very

00:31:01,669 --> 00:31:05,690
long string here and this is integrated

00:31:04,130 --> 00:31:07,370
directly with the Linux kernel with the

00:31:05,690 --> 00:31:10,010
connection tracking table in the Linux

00:31:07,370 --> 00:31:13,610
kernel using ipbs

00:31:10,010 --> 00:31:15,530
so the Anna gossips between the agents

00:31:13,610 --> 00:31:19,060
so that they're all constantly updated

00:31:15,530 --> 00:31:21,680
about where particular things are living

00:31:19,060 --> 00:31:23,210
so this gives us very fast update times

00:31:21,680 --> 00:31:26,930
somewhere in the region of 100

00:31:23,210 --> 00:31:30,940
milliseconds and it provides east-west

00:31:26,930 --> 00:31:33,380
low balancing ie inside our cluster and

00:31:30,940 --> 00:31:36,800
the other key mechanism we need is load

00:31:33,380 --> 00:31:38,360
balancing so when in micro services

00:31:36,800 --> 00:31:39,680
architectures typically a service is

00:31:38,360 --> 00:31:41,030
going to be made up of a whole bunch of

00:31:39,680 --> 00:31:42,050
different things we need something

00:31:41,030 --> 00:31:46,700
that's going to sit in front of those

00:31:42,050 --> 00:31:48,650
things and direct that traffic so the

00:31:46,700 --> 00:31:51,920
basic load balancing DCOs is called

00:31:48,650 --> 00:31:54,050
marathon lb marathon lb is based on H a

00:31:51,920 --> 00:31:55,910
proxy I'm sure many in this room are

00:31:54,050 --> 00:31:58,280
familiar with H a proxy it's pretty much

00:31:55,910 --> 00:32:01,550
industrial-strength open source load

00:31:58,280 --> 00:32:03,920
balancer and in in marathon elby's case

00:32:01,550 --> 00:32:07,100
it's integrated directly with the with

00:32:03,920 --> 00:32:08,390
the configuration of the system so if

00:32:07,100 --> 00:32:12,050
you've configured your app to use

00:32:08,390 --> 00:32:15,910
marathon lb then when instances are

00:32:12,050 --> 00:32:18,560
spawned or or go away they they will be

00:32:15,910 --> 00:32:22,240
automatically added or removed from the

00:32:18,560 --> 00:32:25,610
from the marathon RB configurations and

00:32:22,240 --> 00:32:27,560
this Meriton lb can be north-south so it

00:32:25,610 --> 00:32:29,780
can be traffic coming in and out of your

00:32:27,560 --> 00:32:32,420
cluster or you can use it east-west

00:32:29,780 --> 00:32:36,920
inside for applications to talk to other

00:32:32,420 --> 00:32:39,980
applications so storage is probably one

00:32:36,920 --> 00:32:41,680
of the most controversial areas I guess

00:32:39,980 --> 00:32:44,450
in in containers right now and

00:32:41,680 --> 00:32:46,460
containers work very well for stateless

00:32:44,450 --> 00:32:48,770
services where we don't care about what

00:32:46,460 --> 00:32:51,260
anything any state that that container

00:32:48,770 --> 00:32:53,170
happens to have if it blows away then

00:32:51,260 --> 00:32:56,090
you know we just start another one and

00:32:53,170 --> 00:32:58,880
but the reality is that the most service

00:32:56,090 --> 00:33:01,010
is not stateless even when we think we

00:32:58,880 --> 00:33:04,340
hope they might be lots of things turn

00:33:01,010 --> 00:33:05,870
out to really care about state um even

00:33:04,340 --> 00:33:07,940
in some of the cloud native frameworks

00:33:05,870 --> 00:33:10,070
where we've got replication between

00:33:07,940 --> 00:33:12,680
nodes people might choose to make them

00:33:10,070 --> 00:33:16,520
stateful in order to avoid long recovery

00:33:12,680 --> 00:33:18,050
times bye-bye things moving about so as

00:33:16,520 --> 00:33:19,880
I'm sure some of you are aware there is

00:33:18,050 --> 00:33:22,290
an emerging standard in this space the

00:33:19,880 --> 00:33:23,730
container storage interface and

00:33:22,290 --> 00:33:26,280
mesosphere a part of the core team

00:33:23,730 --> 00:33:29,940
developing that and that's integrated

00:33:26,280 --> 00:33:32,610
into the into the latest release of DC

00:33:29,940 --> 00:33:36,230
OS and and we kind of have three other

00:33:32,610 --> 00:33:39,000
mechanisms for for doing for doing

00:33:36,230 --> 00:33:41,550
storage for containers clearly stateless

00:33:39,000 --> 00:33:42,930
that you know we just allocated pretty

00:33:41,550 --> 00:33:44,370
bitter than the file system for a

00:33:42,930 --> 00:33:46,860
particular container and that's blown

00:33:44,370 --> 00:33:49,530
away when we lose that container made us

00:33:46,860 --> 00:33:52,610
can do local persistent storage so you

00:33:49,530 --> 00:33:55,800
can allocate a drive or part of a drive

00:33:52,610 --> 00:33:57,330
to a particular container and you know

00:33:55,800 --> 00:33:58,890
that that's going to work okay if we

00:33:57,330 --> 00:34:01,560
lose the container but clearly if we

00:33:58,890 --> 00:34:03,630
lose the agent that persistent volumes

00:34:01,560 --> 00:34:06,300
hosted on then then that's not going to

00:34:03,630 --> 00:34:09,419
be great you can integrate with rex ray

00:34:06,300 --> 00:34:11,370
which is a Dell EMC thing I'm not quite

00:34:09,419 --> 00:34:13,919
sure what the what the current

00:34:11,370 --> 00:34:15,810
maintenance state of Rex ray is but that

00:34:13,919 --> 00:34:20,190
enables you to integrate with external

00:34:15,810 --> 00:34:23,750
storage things like sans and there are

00:34:20,190 --> 00:34:26,520
also a bunch of kind of pretty popular

00:34:23,750 --> 00:34:29,000
third-party things port works is one

00:34:26,520 --> 00:34:32,550
that this particularly we see a

00:34:29,000 --> 00:34:34,290
particularly large bunch of users for

00:34:32,550 --> 00:34:36,270
and those gives you persistent

00:34:34,290 --> 00:34:37,379
distributed volumes across your cluster

00:34:36,270 --> 00:34:40,050
so you don't need to worry about losing

00:34:37,379 --> 00:34:41,669
individual agents the import works

00:34:40,050 --> 00:34:47,340
they're actually distributed all across

00:34:41,669 --> 00:34:52,080
your cluster and networking again is

00:34:47,340 --> 00:34:53,700
kind of a little bit of a difficult

00:34:52,080 --> 00:34:55,590
topic I guess you know everyone wants to

00:34:53,700 --> 00:34:57,960
do networking in in different ways

00:34:55,590 --> 00:34:59,850
everyone's got opinions on on which way

00:34:57,960 --> 00:35:05,000
they think things should be hooked up

00:34:59,850 --> 00:35:07,620
together from the DCOs perspective our

00:35:05,000 --> 00:35:09,390
our position is to be as an opinionated

00:35:07,620 --> 00:35:12,210
as possible and to let people plug

00:35:09,390 --> 00:35:17,400
whatever they want in and so whilst we

00:35:12,210 --> 00:35:19,650
do have integrated VX land based virtual

00:35:17,400 --> 00:35:23,880
networks so you can have basic layer two

00:35:19,650 --> 00:35:25,770
separation on the wire we support the

00:35:23,880 --> 00:35:27,000
container network interface standards so

00:35:25,770 --> 00:35:30,630
if you want to plug in things like

00:35:27,000 --> 00:35:34,520
calico or open contrail then that's you

00:35:30,630 --> 00:35:34,520
can do that and they just use CNO

00:35:35,890 --> 00:35:42,160
so lots of people make the comparison

00:35:38,920 --> 00:35:43,840
between medicine and kubernetes and its

00:35:42,160 --> 00:35:45,940
really the wrong level to make that

00:35:43,840 --> 00:35:48,400
comparison at Mesa operates at a lower

00:35:45,940 --> 00:35:51,990
level than kubernetes does as hopefully

00:35:48,400 --> 00:35:55,060
as has become clear from from this talk

00:35:51,990 --> 00:35:57,790
marathon is the native container

00:35:55,060 --> 00:36:00,960
Orchestrator that ships with with GCS

00:35:57,790 --> 00:36:03,580
and you can probably think that the

00:36:00,960 --> 00:36:06,970
marathon is analogous to kubernetes as

00:36:03,580 --> 00:36:08,680
opposed to to maze us being so marathon

00:36:06,970 --> 00:36:12,040
handles all the requirements for

00:36:08,680 --> 00:36:15,400
spawning containerized micro services

00:36:12,040 --> 00:36:18,880
onto DCs agents it supports a whole

00:36:15,400 --> 00:36:20,980
bunch of stuff like pods GPUs health

00:36:18,880 --> 00:36:23,740
checking and you know if things die

00:36:20,980 --> 00:36:26,950
Marathon will restart them a marathons

00:36:23,740 --> 00:36:28,540
basic job is whatever you tell it to do

00:36:26,950 --> 00:36:31,060
in terms of an application it's going to

00:36:28,540 --> 00:36:32,710
try its best to make sure that pattern

00:36:31,060 --> 00:36:40,270
that you've defined is going to continue

00:36:32,710 --> 00:36:42,520
to to exist so the there are most people

00:36:40,270 --> 00:36:44,320
are more familiar with in the in the

00:36:42,520 --> 00:36:47,770
container kind of terminology with

00:36:44,320 --> 00:36:49,990
running docker containers and so DCOs

00:36:47,770 --> 00:36:53,070
provides a whole range of ways that we

00:36:49,990 --> 00:36:56,650
can just basically run docker containers

00:36:53,070 --> 00:36:58,870
we support what Mesa says this concept

00:36:56,650 --> 00:37:01,600
of different container eise's and you

00:36:58,870 --> 00:37:03,940
know again this is because of predating

00:37:01,600 --> 00:37:06,820
some of the some of the newer stuff that

00:37:03,940 --> 00:37:08,500
made us as always had the idea that you

00:37:06,820 --> 00:37:12,100
could plug in different ways of

00:37:08,500 --> 00:37:13,390
container izing applications and so the

00:37:12,100 --> 00:37:15,190
first way that you can run docker

00:37:13,390 --> 00:37:17,080
containers is using the docker container

00:37:15,190 --> 00:37:20,590
Iser which effectively runs docker

00:37:17,080 --> 00:37:23,230
natively on each of the agents and so

00:37:20,590 --> 00:37:24,370
when something like marathon wants to

00:37:23,230 --> 00:37:26,590
run a docker container

00:37:24,370 --> 00:37:28,540
it'll hand that straight over to the

00:37:26,590 --> 00:37:30,280
docker run time and docker runtime will

00:37:28,540 --> 00:37:34,930
handle all the stuff about actually

00:37:30,280 --> 00:37:37,690
running those containers one of the

00:37:34,930 --> 00:37:40,870
problems that that that we saw

00:37:37,690 --> 00:37:43,810
particularly at scale was problems with

00:37:40,870 --> 00:37:45,880
with the docker engine and so the mesas

00:37:43,810 --> 00:37:47,500
content native container Iser was been

00:37:45,880 --> 00:37:48,660
extended to this thing called the

00:37:47,500 --> 00:37:51,480
universal container

00:37:48,660 --> 00:37:54,809
time so that you don't need to actually

00:37:51,480 --> 00:37:58,230
run docker on that layer at all in

00:37:54,809 --> 00:38:00,270
reusing UCR UCR understands all the

00:37:58,230 --> 00:38:02,369
stuff about how docker containers work

00:38:00,270 --> 00:38:04,230
natively so it understands docker

00:38:02,369 --> 00:38:06,930
registries it understands the docker

00:38:04,230 --> 00:38:09,480
container format and so we can spawn

00:38:06,930 --> 00:38:14,849
docker containers natively without using

00:38:09,480 --> 00:38:17,579
docker at all and then we've also added

00:38:14,849 --> 00:38:20,010
kubernetes as a framework natively

00:38:17,579 --> 00:38:22,260
running on May's O's and really that's

00:38:20,010 --> 00:38:24,150
about abstracting away all the

00:38:22,260 --> 00:38:27,170
operational complexity of running

00:38:24,150 --> 00:38:30,660
kubernetes if you look at something like

00:38:27,170 --> 00:38:32,480
Kelsey Hightower's kubernetes the hard

00:38:30,660 --> 00:38:35,339
way you know you've got something like

00:38:32,480 --> 00:38:37,950
20-plus discrete steps for how you

00:38:35,339 --> 00:38:40,380
actually deploy kubernetes cluster and

00:38:37,950 --> 00:38:41,819
the one of the emerging patterns in

00:38:40,380 --> 00:38:46,130
kubernetes is that people are running

00:38:41,819 --> 00:38:49,260
lots and lots of clusters because I

00:38:46,130 --> 00:38:51,240
think the kubernetes work because the

00:38:49,260 --> 00:38:53,520
kubernetes scheduler actually keeps

00:38:51,240 --> 00:38:55,650
track of where all the applications are

00:38:53,520 --> 00:38:58,020
as well the idea that you could run very

00:38:55,650 --> 00:39:00,059
very very large single kubernetes

00:38:58,020 --> 00:39:04,140
clusters is never going to work so we

00:39:00,059 --> 00:39:06,000
see lots of users who have tens or even

00:39:04,140 --> 00:39:07,980
hundreds of individual kubernetes

00:39:06,000 --> 00:39:09,869
clusters so if we can imagine that from

00:39:07,980 --> 00:39:12,539
an operational perspective how do we

00:39:09,869 --> 00:39:14,099
deploy and manage all of those different

00:39:12,539 --> 00:39:17,160
kubernetes clusters you can imagine that

00:39:14,099 --> 00:39:19,829
it's it's extremely complex even if

00:39:17,160 --> 00:39:21,539
we're using conflict management or

00:39:19,829 --> 00:39:24,059
whatever to deploy them that's still a

00:39:21,539 --> 00:39:28,020
whole ton of code that you've got to

00:39:24,059 --> 00:39:32,599
manage you've got a update um so with

00:39:28,020 --> 00:39:36,450
with the kubernetes framework in in DCOs

00:39:32,599 --> 00:39:38,670
it's a single CLI command to install a

00:39:36,450 --> 00:39:40,559
highly available kubernetes cluster as I

00:39:38,670 --> 00:39:42,960
said before if we lose element so that

00:39:40,559 --> 00:39:45,180
we can I mean we do demos where we go in

00:39:42,960 --> 00:39:48,690
and actually kill individual Kubla it's

00:39:45,180 --> 00:39:50,430
kill EDD peers and the framework knows

00:39:48,690 --> 00:39:53,190
that needs to have those running so it's

00:39:50,430 --> 00:39:55,770
going to restart them it's unmodified

00:39:53,190 --> 00:39:58,619
kubernetes upstream so I think we're

00:39:55,770 --> 00:40:00,750
aiming to be something like two weeks

00:39:58,619 --> 00:40:02,040
behind each of the kubernetes releases

00:40:00,750 --> 00:40:04,170
in terms of

00:40:02,040 --> 00:40:05,700
of upgrading the framework and there's

00:40:04,170 --> 00:40:08,190
no extra patches or anything is

00:40:05,700 --> 00:40:10,260
completely unmodified so you interact

00:40:08,190 --> 00:40:11,430
with kubernetes and eCos exactly is how

00:40:10,260 --> 00:40:13,560
you would interact with kubernetes

00:40:11,430 --> 00:40:16,020
anywhere so coop coal and all that kind

00:40:13,560 --> 00:40:21,090
of stuff is is exactly the same we

00:40:16,020 --> 00:40:23,000
expose the API endpoints directly so

00:40:21,090 --> 00:40:25,800
coming back to some of the operational

00:40:23,000 --> 00:40:28,590
features this is a different view of the

00:40:25,800 --> 00:40:30,120
DSOs dashboard and I think this one kind

00:40:28,590 --> 00:40:33,120
of gives you a better picture of like

00:40:30,120 --> 00:40:36,150
how you can see the the entire operation

00:40:33,120 --> 00:40:38,400
of your fleet from one from one

00:40:36,150 --> 00:40:40,110
particular pane of glass and this is the

00:40:38,400 --> 00:40:41,880
whole idea is simplifying this

00:40:40,110 --> 00:40:45,000
operational view across all of this

00:40:41,880 --> 00:40:47,010
complexity so from this view we can see

00:40:45,000 --> 00:40:49,230
all of our resources that's all of our

00:40:47,010 --> 00:40:51,000
resources in our entire cluster we could

00:40:49,230 --> 00:40:53,250
see what tasks we've got running we can

00:40:51,000 --> 00:40:55,500
see what's the state of individual

00:40:53,250 --> 00:40:57,750
components within the DCOs cluster

00:40:55,500 --> 00:41:00,060
itself and what's the state of the

00:40:57,750 --> 00:41:06,240
services that we have running in the dcs

00:41:00,060 --> 00:41:10,800
cluster and there are a few other things

00:41:06,240 --> 00:41:13,550
that we provide in order to simplify

00:41:10,800 --> 00:41:17,030
that that that operational complexity

00:41:13,550 --> 00:41:20,400
the first one is that the frameworks

00:41:17,030 --> 00:41:23,280
will generally have integration with the

00:41:20,400 --> 00:41:26,040
dcs CLI in UI so that rather than having

00:41:23,280 --> 00:41:28,350
to switch out between different CL eyes

00:41:26,040 --> 00:41:31,650
to achieve common operational tasks

00:41:28,350 --> 00:41:34,200
we'll have for example a CLI extension

00:41:31,650 --> 00:41:36,000
where we can just use the dcs CLI in

00:41:34,200 --> 00:41:39,030
order to do that stuff so this is an

00:41:36,000 --> 00:41:41,550
example of very familiar I'm sure

00:41:39,030 --> 00:41:43,860
through anybody who's Kafka of creating

00:41:41,550 --> 00:41:46,350
a topic in Kafka right which so instead

00:41:43,860 --> 00:41:49,050
of switching to the Kafka CLI the DTR

00:41:46,350 --> 00:41:51,090
CLI all configures itself and you can

00:41:49,050 --> 00:41:53,760
just use the dcs CLI to do a lot of that

00:41:51,090 --> 00:41:57,960
operational stuff and then where we

00:41:53,760 --> 00:41:59,670
might have web interfaces that are that

00:41:57,960 --> 00:42:01,110
are provided by a particular framework

00:41:59,670 --> 00:42:03,780
particular application that we've

00:42:01,110 --> 00:42:04,920
deployed into our cluster we will

00:42:03,780 --> 00:42:08,250
generally make those frameworks

00:42:04,920 --> 00:42:10,800
available for those UI is available from

00:42:08,250 --> 00:42:12,510
the dcs UI so there'll be a little hover

00:42:10,800 --> 00:42:14,280
button next to that service you can

00:42:12,510 --> 00:42:16,140
click that button and that UI for that

00:42:14,280 --> 00:42:20,020
service will

00:42:16,140 --> 00:42:22,150
we'll spin up and that's done by

00:42:20,020 --> 00:42:25,150
proxying those things through what we

00:42:22,150 --> 00:42:27,130
call the admin route or which is a proxy

00:42:25,150 --> 00:42:28,660
that allows us to do traffic into the

00:42:27,130 --> 00:42:30,790
cluster because obviously those things

00:42:28,660 --> 00:42:32,799
are deployed inside our cluster and as a

00:42:30,790 --> 00:42:37,390
user we may not have direct access to

00:42:32,799 --> 00:42:39,369
them from a log perspective all of the

00:42:37,390 --> 00:42:42,069
logs for any task that's running on our

00:42:39,369 --> 00:42:46,380
cluster all aggregated they're available

00:42:42,069 --> 00:42:50,230
via the UI the API all the CLI and

00:42:46,380 --> 00:42:52,390
there's what we see general as a pattern

00:42:50,230 --> 00:42:55,240
is is that people want to integrate with

00:42:52,390 --> 00:42:57,220
things like elk upstream with the

00:42:55,240 --> 00:42:59,829
elasticsearch log stash and Cabana and

00:42:57,220 --> 00:43:01,210
so those logs are all available as files

00:42:59,829 --> 00:43:03,280
on the disk so that you can run

00:43:01,210 --> 00:43:05,140
something like file beat and just pick

00:43:03,280 --> 00:43:07,299
up every single task that's running

00:43:05,140 --> 00:43:10,270
within your cluster and then from the

00:43:07,299 --> 00:43:13,540
from the API we've got the logs endpoint

00:43:10,270 --> 00:43:15,099
and from the CLI you can do a bunch of

00:43:13,540 --> 00:43:16,390
stuff there's there's a ton more of

00:43:15,099 --> 00:43:18,220
these but this is just to give you an

00:43:16,390 --> 00:43:20,500
idea if we want to see the log of an

00:43:18,220 --> 00:43:22,510
individual task we can just use the dcs

00:43:20,500 --> 00:43:25,180
task log or if we want to see the logs

00:43:22,510 --> 00:43:27,040
of a particular node we can use the ID

00:43:25,180 --> 00:43:30,790
of that node and get all of those logs

00:43:27,040 --> 00:43:32,829
from a single command a metrics is a

00:43:30,790 --> 00:43:35,230
pretty similar story so again

00:43:32,829 --> 00:43:37,000
everything's aggregated and available

00:43:35,230 --> 00:43:39,730
through the through all of those

00:43:37,000 --> 00:43:43,000
mechanisms custom applications can

00:43:39,730 --> 00:43:46,990
publish their own custom metrics into

00:43:43,000 --> 00:43:48,730
the metrics API using a stats D endpoint

00:43:46,990 --> 00:43:53,200
that's available to all containers and

00:43:48,730 --> 00:43:55,540
we have Native Prometheus endpoints on

00:43:53,200 --> 00:43:57,280
each agent so you don't need to install

00:43:55,540 --> 00:43:59,799
any additional Prometheus stuff to make

00:43:57,280 --> 00:44:03,280
that work and in fact if you if you

00:43:59,799 --> 00:44:05,980
deploy Prometheus from the catalog into

00:44:03,280 --> 00:44:07,750
your DCs cluster it'll pre configure

00:44:05,980 --> 00:44:09,940
itself to pick up all the previous

00:44:07,750 --> 00:44:12,099
endpoints that you've got running in

00:44:09,940 --> 00:44:13,510
that cluster so it's literally you know

00:44:12,099 --> 00:44:17,530
that will that will set you up and

00:44:13,510 --> 00:44:18,910
you're good to go so kind of coming to

00:44:17,530 --> 00:44:21,790
the end there's a bunch of ways that you

00:44:18,910 --> 00:44:22,900
can that you can try it out if this has

00:44:21,790 --> 00:44:25,540
been something that you might be

00:44:22,900 --> 00:44:27,430
interested in we have d cirrus vagrant

00:44:25,540 --> 00:44:28,920
project and github so you can spin it up

00:44:27,430 --> 00:44:30,960
in vagrant the

00:44:28,920 --> 00:44:34,880
to us docker project as well we can spin

00:44:30,960 --> 00:44:37,860
up locally in docker and there are

00:44:34,880 --> 00:44:41,880
automation scripts for all of your cloud

00:44:37,860 --> 00:44:44,190
platforms of choice there's a bunch of

00:44:41,880 --> 00:44:46,830
ways that you can interact with our

00:44:44,190 --> 00:44:49,290
community so DTS dot io is our community

00:44:46,830 --> 00:44:52,230
website we have a very active slack

00:44:49,290 --> 00:44:53,730
community chat DCOs dot io there's a

00:44:52,230 --> 00:44:56,220
whole bunch of stuff you can look at on

00:44:53,730 --> 00:44:58,400
github and I'm happy to take any

00:44:56,220 --> 00:44:58,400
questions

00:44:59,060 --> 00:45:06,000
[Applause]

00:45:17,360 --> 00:45:23,030
DC OS support multiple tenants does it

00:45:23,120 --> 00:45:33,630
so in the in the the Enterprise version

00:45:29,400 --> 00:45:36,840
will have multi-tenancy the open-source

00:45:33,630 --> 00:45:40,700
version of the minute is is single

00:45:36,840 --> 00:45:43,770
tenant from that perspective in the the

00:45:40,700 --> 00:45:46,860
Enterprise version currently has very

00:45:43,770 --> 00:45:48,930
sophisticated our back so you can you

00:45:46,860 --> 00:45:50,940
can lock individual users down to

00:45:48,930 --> 00:45:53,460
particular things but yeah effectively

00:45:50,940 --> 00:45:56,480
the minute it's a super user interface

00:45:53,460 --> 00:45:56,480
in that in that sense

00:46:16,730 --> 00:46:20,600
sorry can you repeat the question

00:46:23,150 --> 00:46:26,150
yep

00:46:29,510 --> 00:46:35,760
so so the question was how how the

00:46:34,260 --> 00:46:38,130
things generally translate through

00:46:35,760 --> 00:46:43,650
Tomatoes and I think you know probably

00:46:38,130 --> 00:46:45,450
the answer to that is that they the the

00:46:43,650 --> 00:46:49,380
kubernetes deployment itself is

00:46:45,450 --> 00:46:52,110
completely is is isolated within its own

00:46:49,380 --> 00:46:53,580
thing right so you know whatever's

00:46:52,110 --> 00:46:55,860
happening within kubernetes is just

00:46:53,580 --> 00:46:58,670
happening within kubernetes you have

00:46:55,860 --> 00:47:00,750
network pathways externally from

00:46:58,670 --> 00:47:02,400
applications running into in in

00:47:00,750 --> 00:47:04,460
kubernetes or applications running

00:47:02,400 --> 00:47:06,660
natively in dcs can talk to each other

00:47:04,460 --> 00:47:08,640
but anything you do in kubernetes is

00:47:06,660 --> 00:47:15,270
exactly the same as you would do it in

00:47:08,640 --> 00:47:19,860
kubernetes anywhere else exactly yeah

00:47:15,270 --> 00:47:21,660
yeah I mean the really cool stuff that's

00:47:19,860 --> 00:47:23,700
coming in the next releases of the

00:47:21,660 --> 00:47:25,800
kubernetes framework is being able to do

00:47:23,700 --> 00:47:29,190
I the minute we only run a single

00:47:25,800 --> 00:47:31,380
kubernetes cluster within a single DCOs

00:47:29,190 --> 00:47:33,870
cluster and the next release which is in

00:47:31,380 --> 00:47:36,720
about two weeks will have the beginnings

00:47:33,870 --> 00:47:39,390
of running multiple kubernetes clusters

00:47:36,720 --> 00:47:41,310
within a single DCOs cluster and the the

00:47:39,390 --> 00:47:42,900
aim of that in another couple of

00:47:41,310 --> 00:47:44,610
versions time will be you actually been

00:47:42,900 --> 00:47:47,760
packing them so you'll be able to have

00:47:44,610 --> 00:47:49,980
you know effectively cubelets from

00:47:47,760 --> 00:47:53,280
different Kublai's clusters packed into

00:47:49,980 --> 00:47:54,930
the into the same the same DCs agents

00:47:53,280 --> 00:47:56,750
it's actually quite a complex problem

00:47:54,930 --> 00:47:59,550
because the ports and the minute but

00:47:56,750 --> 00:48:01,290
that's the end the end goal because you

00:47:59,550 --> 00:48:03,150
know that's what we see from from users

00:48:01,290 --> 00:48:04,500
that folks really want to do because

00:48:03,150 --> 00:48:07,580
this thing and wanting to run multiple

00:48:04,500 --> 00:48:07,580
Cabernets clusters

00:48:10,000 --> 00:48:13,690
any more questions

00:48:20,350 --> 00:48:23,350
yep

00:48:25,180 --> 00:48:30,560
well I said a lot of those packages are

00:48:27,770 --> 00:48:35,540
created by the companies who who

00:48:30,560 --> 00:48:46,250
produced that software yeah well that's

00:48:35,540 --> 00:48:47,600
okay we can take that offline perhaps

00:48:46,250 --> 00:48:48,980
we're doing new in-house for that

00:48:47,600 --> 00:48:50,840
particular one I mean I know for things

00:48:48,980 --> 00:49:05,390
like Kafka confluent actually made that

00:48:50,840 --> 00:49:16,520
package I mean I think you're actually a

00:49:05,390 --> 00:49:18,500
partner of ours so alright then thank

00:49:16,520 --> 00:49:20,790
you very much Matt for giving us these

00:49:18,500 --> 00:49:23,890
insights thanks for being here

00:49:20,790 --> 00:49:25,950
[Applause]

00:49:23,890 --> 00:49:25,950
you

00:49:33,410 --> 00:49:41,360
[Music]

00:49:39,540 --> 00:49:44,260
you

00:49:41,360 --> 00:49:44,260

YouTube URL: https://www.youtube.com/watch?v=GS0m0ZgwMIg


