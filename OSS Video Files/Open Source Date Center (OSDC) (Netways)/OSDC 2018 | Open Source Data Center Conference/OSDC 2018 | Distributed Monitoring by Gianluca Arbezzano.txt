Title: OSDC 2018 | Distributed Monitoring by Gianluca Arbezzano
Publication date: 2018-06-22
Playlist: OSDC 2018 | Open Source Data Center Conference
Description: 
	Modern software development is increasingly taking a “microservice” approach that has resulted in an explosion of complexity at the network level. We have more applications running distributed across different datacenters. Distributed tracing, events, and metrics are essential for observing and understanding modern microservice architectures.
This talk is a deep dive on how to monitor your distributed system. You will get tools, methodologies, and experiences that will help you to realize what your applications expose and how to get value out from all these information.
Gianluca Arbezzano, SRE at InfluxData will share how to monitor a distributed system, how to switch from a more traditional monitoring approach to observability. Stay focused on the server’s role and not on the hostname because it’s not really important anymore, our servers or containers are fast moving part and it’s easy to detach it from the right in case of trouble than call the server by name as a cute puppet. How to design a SLO for your core services and now to iterate on them. Instrument your services with tracing using tools like Zipkin or Jaeger to measure latency between in your network.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Google+: https://plus.google.com/+netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:00,540 --> 00:00:03,369
[Music]

00:00:01,310 --> 00:00:03,369
you

00:00:11,750 --> 00:00:16,000
[Music]

00:00:13,049 --> 00:00:19,720
thank you for the introduction and thank

00:00:16,000 --> 00:00:24,130
you to be here for we my session and I'm

00:00:19,720 --> 00:00:26,190
here to try to share how we understand

00:00:24,130 --> 00:00:30,510
or at least try to understand the cows

00:00:26,190 --> 00:00:34,420
in Free State I mean how we can try to

00:00:30,510 --> 00:00:36,970
do it together so I try to split these

00:00:34,420 --> 00:00:41,199
stalls in two different mains area one

00:00:36,970 --> 00:00:44,200
is a more traditional one that it's more

00:00:41,199 --> 00:00:47,110
about how you can use the influx to be

00:00:44,200 --> 00:00:49,739
inclusive and the other tools that we

00:00:47,110 --> 00:00:53,530
made to instrument and monitor a

00:00:49,739 --> 00:00:55,750
traditional infrastructure because we

00:00:53,530 --> 00:00:58,870
are at the ospc so I need to do

00:00:55,750 --> 00:01:01,270
something with DC involved so that's why

00:00:58,870 --> 00:01:03,190
I went for a more traditional part of

00:01:01,270 --> 00:01:06,250
the topic and another one is about

00:01:03,190 --> 00:01:08,229
distributed monitoring and tracing just

00:01:06,250 --> 00:01:11,020
because I discovered that it can be a

00:01:08,229 --> 00:01:13,540
really useful tools that developers can

00:01:11,020 --> 00:01:17,619
have to understand what's going on in a

00:01:13,540 --> 00:01:19,720
distributed system so I think that if

00:01:17,619 --> 00:01:22,330
you are able to give to the developer

00:01:19,720 --> 00:01:24,540
these kind of tools they can try to make

00:01:22,330 --> 00:01:27,790
to be less chaotic so I'm a developer

00:01:24,540 --> 00:01:30,180
I'm a friendly one so I'm trying to fill

00:01:27,790 --> 00:01:32,860
the gap between developers and ops

00:01:30,180 --> 00:01:38,170
that's the purpose of the topic so be

00:01:32,860 --> 00:01:41,710
friendly with me at the end if I can't

00:01:38,170 --> 00:01:44,110
go yes so as I said I'm a software

00:01:41,710 --> 00:01:46,180
engineer I work in in fixed data and I'm

00:01:44,110 --> 00:01:48,430
almost passionate about all the stacks

00:01:46,180 --> 00:01:50,350
maybe not the one that involved colors

00:01:48,430 --> 00:01:52,119
in UI because I'm really bad but the

00:01:50,350 --> 00:01:55,689
rest is kind of good and currently I'm

00:01:52,119 --> 00:01:57,970
working go I'm open source developer and

00:01:55,689 --> 00:02:00,729
I like to share what I do so that's why

00:01:57,970 --> 00:02:02,710
I'm a doctor captain and see s the CN CF

00:02:00,729 --> 00:02:05,920
ambassador because it's kind of part of

00:02:02,710 --> 00:02:08,799
my learning brochure or process of my

00:02:05,920 --> 00:02:11,110
nature to share and trying to speak with

00:02:08,799 --> 00:02:14,170
people about what what I'm doing

00:02:11,110 --> 00:02:17,680
I work as a Sri influx data so and maybe

00:02:14,170 --> 00:02:19,569
I mainly take care about scalability in

00:02:17,680 --> 00:02:22,069
our cloud offering because other than

00:02:19,569 --> 00:02:26,450
develop open source store we also

00:02:22,069 --> 00:02:29,209
an enterprise license and cloud product

00:02:26,450 --> 00:02:31,700
that we used to set in 50 B and all the

00:02:29,209 --> 00:02:35,000
stack has a source as other service

00:02:31,700 --> 00:02:39,079
I write vlogs and I'm active on Twitter

00:02:35,000 --> 00:02:40,549
so if you just send me something I'm

00:02:39,079 --> 00:02:42,379
gonna house her at some point I love

00:02:40,549 --> 00:02:45,379
traveling I grow my vegetables and they

00:02:42,379 --> 00:02:48,980
cook so here it was in New York counting

00:02:45,379 --> 00:02:50,540
em lambs a lot of them trying to keep

00:02:48,980 --> 00:02:53,510
the situation quiet and monitor the

00:02:50,540 --> 00:02:57,319
situation here I was making gnocchi at

00:02:53,510 --> 00:02:59,780
home very good here was in Cuba holiday

00:02:57,319 --> 00:03:02,540
few weeks ago a really good place and

00:02:59,780 --> 00:03:04,609
here I was at the SRS trying to figure

00:03:02,540 --> 00:03:10,579
out how big a tree can be and that was

00:03:04,609 --> 00:03:12,230
really big just to make a quick

00:03:10,579 --> 00:03:14,269
introduction and give you some context

00:03:12,230 --> 00:03:17,180
about what influence data does in fixed

00:03:14,269 --> 00:03:18,919
data is a company a start-up based in

00:03:17,180 --> 00:03:21,079
San Francisco and we have a couple of

00:03:18,919 --> 00:03:23,329
open source tools that can help you to

00:03:21,079 --> 00:03:25,819
manage time series data what time series

00:03:23,329 --> 00:03:28,389
data are everything that has a time and

00:03:25,819 --> 00:03:31,579
everything where the time matters if you

00:03:28,389 --> 00:03:32,989
are ISDN company if you have sensors and

00:03:31,579 --> 00:03:35,419
you try are trying to monitor

00:03:32,989 --> 00:03:37,609
temperature and you don't have data you

00:03:35,419 --> 00:03:39,470
need to date isn't at the time when the

00:03:37,609 --> 00:03:43,129
temperature happened it's gonna be a use

00:03:39,470 --> 00:03:45,069
useful I use less the same for logs if

00:03:43,129 --> 00:03:48,109
you have logs you don't have time

00:03:45,069 --> 00:03:49,940
something weird and even for metrics

00:03:48,109 --> 00:03:52,280
that come from services CPU memory

00:03:49,940 --> 00:03:54,440
Network everything needs to have a

00:03:52,280 --> 00:03:56,269
timestamp and without a timestamp it

00:03:54,440 --> 00:03:59,060
doesn't have sense and there are a

00:03:56,269 --> 00:04:00,799
couple of features that you need when

00:03:59,060 --> 00:04:03,379
you are working with time series like

00:04:00,799 --> 00:04:05,389
retention policy or aggregation or or

00:04:03,379 --> 00:04:09,109
math and you have a you need to have a

00:04:05,389 --> 00:04:11,269
efficient time series so the the first

00:04:09,109 --> 00:04:13,489
project influx data released was

00:04:11,269 --> 00:04:15,650
inflexibly a time series database it

00:04:13,489 --> 00:04:18,650
it's open source and you can use it

00:04:15,650 --> 00:04:20,780
after that we discovered that the time

00:04:18,650 --> 00:04:22,669
series the database wasn't enough people

00:04:20,780 --> 00:04:25,120
was having the same were struggling with

00:04:22,669 --> 00:04:28,039
the same problems all over the time so

00:04:25,120 --> 00:04:30,110
we had the Telegraph Telegraph is our

00:04:28,039 --> 00:04:32,389
collector agent so you can take it and

00:04:30,110 --> 00:04:35,310
store it in every in all your hosts and

00:04:32,389 --> 00:04:37,800
it gets metrics out from everywhere

00:04:35,310 --> 00:04:40,650
it has a lot of plugins like more than

00:04:37,800 --> 00:04:42,960
100 and whatever so a lot of them and

00:04:40,650 --> 00:04:44,310
you can write them and submit them to

00:04:42,960 --> 00:04:46,620
the open source project they will be

00:04:44,310 --> 00:04:50,160
merged and you can start to collect

00:04:46,620 --> 00:04:52,710
metrics from new services like docker or

00:04:50,160 --> 00:04:56,390
from container itself or you call for

00:04:52,710 --> 00:05:00,000
more traditional sources like Prague or

00:04:56,390 --> 00:05:02,640
a lot of other services I'm a sequel

00:05:00,000 --> 00:05:05,850
progress so you have your application

00:05:02,640 --> 00:05:08,010
you collect all the data in telegraph

00:05:05,850 --> 00:05:11,280
you get all the data from your host and

00:05:08,010 --> 00:05:13,650
you store the data in flexi be good now

00:05:11,280 --> 00:05:16,710
another project that we had to the stack

00:05:13,650 --> 00:05:18,870
was cronograph a UI to match all the

00:05:16,710 --> 00:05:21,960
tick all the stacks so even capacitor

00:05:18,870 --> 00:05:26,670
Telegraph in 50 B can be measured from

00:05:21,960 --> 00:05:29,550
chronograph with we saw few thoughts

00:05:26,670 --> 00:05:31,380
about ago that Ravana is also a really

00:05:29,550 --> 00:05:33,780
powerful dashboard that supports multi

00:05:31,380 --> 00:05:37,440
sources so you can use infra CB with

00:05:33,780 --> 00:05:40,290
Ravana there capacitor is our framework

00:05:37,440 --> 00:05:43,050
to process data and to make alerts so

00:05:40,290 --> 00:05:45,210
you can stream data from in from infancy

00:05:43,050 --> 00:05:48,420
B and create our alerts based on

00:05:45,210 --> 00:05:50,340
thresholds like CPU memory usage or you

00:05:48,420 --> 00:05:52,610
can combine them together or you can

00:05:50,340 --> 00:05:55,590
have more complicated flow for example

00:05:52,610 --> 00:05:57,480
capacitor support a pool based model has

00:05:55,590 --> 00:05:59,190
primitives so you can configure it to

00:05:57,480 --> 00:06:02,190
grab information from your application

00:05:59,190 --> 00:06:04,500
or from your exporters and it will store

00:06:02,190 --> 00:06:06,390
them in infancy B so it's fully

00:06:04,500 --> 00:06:09,630
compatible with the promise with the

00:06:06,390 --> 00:06:12,420
primitive scraper and that's it I'm not

00:06:09,630 --> 00:06:14,790
gonna I'm gonna work on them later so

00:06:12,420 --> 00:06:16,980
this is was just a kind of overview and

00:06:14,790 --> 00:06:19,110
context what I mean when I think about

00:06:16,980 --> 00:06:21,000
distributed system micro services is

00:06:19,110 --> 00:06:24,630
just one of the distributed system that

00:06:21,000 --> 00:06:27,090
you can work with work with it's

00:06:24,630 --> 00:06:29,160
probably the one of the famous one but

00:06:27,090 --> 00:06:31,680
even queues AHA distribute can be

00:06:29,160 --> 00:06:34,230
distributed so you can have workers and

00:06:31,680 --> 00:06:36,420
queues usually the queue system usually

00:06:34,230 --> 00:06:40,260
are in a availability zone they they are

00:06:36,420 --> 00:06:42,750
across regions across data centers - so

00:06:40,260 --> 00:06:45,480
it's a it's a distribution it's a

00:06:42,750 --> 00:06:46,890
distributed system - even a single

00:06:45,480 --> 00:06:48,420
application with multi tresses

00:06:46,890 --> 00:06:48,840
distributed is not distributed across

00:06:48,420 --> 00:06:51,360
net

00:06:48,840 --> 00:06:58,580
works but it is distributed across tress

00:06:51,360 --> 00:07:01,110
it's also distributed and do like more

00:06:58,580 --> 00:07:04,290
distributed system is big and spread

00:07:01,110 --> 00:07:07,070
across containers and data centers and

00:07:04,290 --> 00:07:09,389
more is complicated another factor to

00:07:07,070 --> 00:07:12,990
understand how complicated it can be

00:07:09,389 --> 00:07:16,040
it's it's containers the first component

00:07:12,990 --> 00:07:21,720
is whatever yeah all these stores

00:07:16,040 --> 00:07:23,729
accelerated how how much we spread our

00:07:21,720 --> 00:07:25,710
processes around the globe because with

00:07:23,729 --> 00:07:27,750
kubernetes it looks everything like a

00:07:25,710 --> 00:07:29,639
big server and you deploy containers

00:07:27,750 --> 00:07:32,520
that look like processes and they go and

00:07:29,639 --> 00:07:34,200
come it's it's like a it's like a really

00:07:32,520 --> 00:07:36,690
chaotic environment where that's what

00:07:34,200 --> 00:07:39,210
cars come from and so it's so chaotic

00:07:36,690 --> 00:07:40,620
that sometimes people just take these

00:07:39,210 --> 00:07:43,680
technologies without really think about

00:07:40,620 --> 00:07:47,940
why and blame yourself if you did that

00:07:43,680 --> 00:07:49,889
because now you are in the cows another

00:07:47,940 --> 00:07:52,620
way to understand how complex a

00:07:49,889 --> 00:07:54,389
distributed system is is to look at one

00:07:52,620 --> 00:07:57,030
request and understand how many

00:07:54,389 --> 00:07:58,800
application is trying to hit and you

00:07:57,030 --> 00:08:02,070
will discover that is hitting a lot more

00:07:58,800 --> 00:08:05,460
services that you figure it out so even

00:08:02,070 --> 00:08:08,550
in a biggest really big microservices

00:08:05,460 --> 00:08:11,370
environment when there are 200 micro

00:08:08,550 --> 00:08:14,039
services involved you really lost track

00:08:11,370 --> 00:08:16,200
about who is calling what and it's also

00:08:14,039 --> 00:08:17,639
really hard to understand how to delete

00:08:16,200 --> 00:08:20,370
a service from the chain because you

00:08:17,639 --> 00:08:25,979
don't know if it's seat cold or not so

00:08:20,370 --> 00:08:29,099
it's complicated logs are really useless

00:08:25,979 --> 00:08:30,720
in this kind of situation because you

00:08:29,099 --> 00:08:32,610
don't have a stream anymore you have a

00:08:30,720 --> 00:08:35,039
lot of them you need to aggregate them

00:08:32,610 --> 00:08:37,589
and you need to collect them together

00:08:35,039 --> 00:08:39,839
and trying to understand which log come

00:08:37,589 --> 00:08:44,430
from which are castes because that's the

00:08:39,839 --> 00:08:47,310
complicated part now in very also event

00:08:44,430 --> 00:08:49,320
and matrix are hard to understand in a

00:08:47,310 --> 00:08:50,610
complex distributed system because you

00:08:49,320 --> 00:08:52,350
don't know where where they come from

00:08:50,610 --> 00:08:55,920
and they need to be correlated together

00:08:52,350 --> 00:08:57,750
and you need to almost rebuild every

00:08:55,920 --> 00:08:59,490
caste from the beginning to the end and

00:08:57,750 --> 00:09:01,170
understand what's happening in the

00:08:59,490 --> 00:09:04,060
middle

00:09:01,170 --> 00:09:06,190
so you can do that anymore you can

00:09:04,060 --> 00:09:08,380
stream and tailor logs and look at it

00:09:06,190 --> 00:09:10,450
really fast and try to understand if

00:09:08,380 --> 00:09:11,920
it's too fast or too slow I was doing

00:09:10,450 --> 00:09:13,089
that it scenarios you can just follow

00:09:11,920 --> 00:09:15,459
update and see me

00:09:13,089 --> 00:09:18,100
this is sauce so we are this so slow

00:09:15,459 --> 00:09:19,899
something's happening and you're kind of

00:09:18,100 --> 00:09:21,220
you try to detect the problem this

00:09:19,899 --> 00:09:23,110
doesn't work anymore because you have

00:09:21,220 --> 00:09:26,339
too many streams you don't know where to

00:09:23,110 --> 00:09:30,190
look so I'm sorry

00:09:26,339 --> 00:09:33,250
so tracing is all about aggregating logs

00:09:30,190 --> 00:09:36,660
and metrics using IDs usually and the ID

00:09:33,250 --> 00:09:39,279
is called trace ID and it's the

00:09:36,660 --> 00:09:41,410
identification for a specific request so

00:09:39,279 --> 00:09:43,330
let's say that you have an engine X in

00:09:41,410 --> 00:09:45,790
front of your application and whenever

00:09:43,330 --> 00:09:48,250
the caste hits your engine X it had a

00:09:45,790 --> 00:09:50,470
trace ID and this trace it is propagated

00:09:48,250 --> 00:09:53,310
to the network until it comes back and

00:09:50,470 --> 00:09:56,800
the idea is that you can query your

00:09:53,310 --> 00:10:00,970
analytics magic tool and it will be able

00:09:56,800 --> 00:10:03,640
to give you a trace this is a trace I

00:10:00,970 --> 00:10:06,040
made it this is piece of art that I made

00:10:03,640 --> 00:10:08,709
and as you can see there is time over

00:10:06,040 --> 00:10:11,290
there and the list of of application and

00:10:08,709 --> 00:10:14,800
there is an engine XR out service and

00:10:11,290 --> 00:10:16,899
API authentication again and my sequel

00:10:14,800 --> 00:10:19,089
so the idea is that all these

00:10:16,899 --> 00:10:22,180
information every time that somebody

00:10:19,089 --> 00:10:24,040
calls one of your services the request

00:10:22,180 --> 00:10:26,020
is floating to all these services and

00:10:24,040 --> 00:10:28,480
you need to correlate them together to

00:10:26,020 --> 00:10:31,240
understand when it breaks or how long it

00:10:28,480 --> 00:10:34,779
takes and who is the service that is

00:10:31,240 --> 00:10:39,279
required that is blocked in the request

00:10:34,779 --> 00:10:40,899
to go over so one of the problem making

00:10:39,279 --> 00:10:43,029
this is that we write application in a

00:10:40,899 --> 00:10:46,270
lot of languages and so the

00:10:43,029 --> 00:10:47,980
instrumentation tools can be hard

00:10:46,270 --> 00:10:51,370
because it need to be across languages

00:10:47,980 --> 00:10:53,560
and it also need to be across steam

00:10:51,370 --> 00:10:55,360
because at the end of the story you need

00:10:53,560 --> 00:10:56,890
to have a single trace so if every

00:10:55,360 --> 00:10:58,899
application and every team and every

00:10:56,890 --> 00:11:02,980
language is trace in a different way you

00:10:58,899 --> 00:11:06,450
are you may be won't be able to get a

00:11:02,980 --> 00:11:09,459
trace and it will be useless

00:11:06,450 --> 00:11:11,290
so distributed tracing and open tracing

00:11:09,459 --> 00:11:13,570
is one of the standard that you can

00:11:11,290 --> 00:11:15,790
follow is promoted by the CNC

00:11:13,570 --> 00:11:17,830
so the cloud energy foundation the same

00:11:15,790 --> 00:11:20,160
foundation that helps kubernetes

00:11:17,830 --> 00:11:22,750
kubernetes promoted permit use and

00:11:20,160 --> 00:11:25,950
continuity and other popular cloud

00:11:22,750 --> 00:11:29,650
projects to grow they promoted this

00:11:25,950 --> 00:11:32,260
standard and it's good for a couple of

00:11:29,650 --> 00:11:36,880
reasons one of one of the reason is that

00:11:32,260 --> 00:11:40,240
it is split your tracing tools from your

00:11:36,880 --> 00:11:43,000
application so you can you embed the

00:11:40,240 --> 00:11:47,790
open tracing SDK that it's made in a lot

00:11:43,000 --> 00:11:51,820
of different languages like Java C go

00:11:47,790 --> 00:11:54,640
JavaScript and all of them fight on you

00:11:51,820 --> 00:11:57,370
you have the open tracing SDK you inject

00:11:54,640 --> 00:11:59,800
the tracer and you instrument your

00:11:57,370 --> 00:12:01,180
application with the open tracing API so

00:11:59,800 --> 00:12:03,760
if you need to change the tracer you

00:12:01,180 --> 00:12:05,440
just change the tracer on top and your

00:12:03,760 --> 00:12:08,020
instrumentation code will be the same

00:12:05,440 --> 00:12:10,080
that's good if to to avoid the vendor

00:12:08,020 --> 00:12:12,640
lock-in and all this kind of stuff so

00:12:10,080 --> 00:12:14,140
you have your application instrumented

00:12:12,640 --> 00:12:18,070
and they are speaking with the open

00:12:14,140 --> 00:12:21,220
tracing API and you can use a lot of

00:12:18,070 --> 00:12:25,090
open source or other service traces to

00:12:21,220 --> 00:12:27,550
store your events so zip bikinis one

00:12:25,090 --> 00:12:30,130
open-source license as a service yeah it

00:12:27,550 --> 00:12:33,430
is open source and it's Ingo jeepneys in

00:12:30,130 --> 00:12:37,270
Java up - in Eastern so there are a lot

00:12:33,430 --> 00:12:39,100
of them even as a service one so all

00:12:37,270 --> 00:12:41,560
this application and all these companies

00:12:39,100 --> 00:12:44,700
are using tracing and open tracing so

00:12:41,560 --> 00:12:49,090
dr. supported Red Hat in OpenShift

00:12:44,700 --> 00:12:53,530
and giant has it a couple of use cases

00:12:49,090 --> 00:12:56,470
and it it's maybe two or three years old

00:12:53,530 --> 00:13:00,100
I mean Q was last year maybe three now

00:12:56,470 --> 00:13:01,450
if I'm counting good and there are

00:13:00,100 --> 00:13:03,070
already a lot of company and

00:13:01,450 --> 00:13:04,570
implementation because it this is a

00:13:03,070 --> 00:13:06,550
common problem that when you design

00:13:04,570 --> 00:13:09,450
micro services and distributed system

00:13:06,550 --> 00:13:12,400
you have and you need to figure out

00:13:09,450 --> 00:13:16,060
Amazon has a service it is called x-ray

00:13:12,400 --> 00:13:18,310
that does that has a service and so

00:13:16,060 --> 00:13:20,140
that's why I think that in a data center

00:13:18,310 --> 00:13:22,990
maybe this kind of service can be good

00:13:20,140 --> 00:13:25,000
and to provide to the user as I said

00:13:22,990 --> 00:13:26,590
before a lot of implementation are

00:13:25,000 --> 00:13:30,960
already available

00:13:26,590 --> 00:13:33,940
so Express is a jes framework LPC has it

00:13:30,960 --> 00:13:39,000
javis on his DK go Ruby Python

00:13:33,940 --> 00:13:39,000
objective-c and so on so it's a solid

00:13:39,210 --> 00:13:45,880
standard and tools that you can use and

00:13:42,160 --> 00:13:47,890
promote when you start a tracer then

00:13:45,880 --> 00:13:49,750
when you start to trace your application

00:13:47,890 --> 00:13:51,610
the problem is the high cardinality

00:13:49,750 --> 00:13:54,370
because you are almost creating an ID

00:13:51,610 --> 00:13:56,800
for every guest and usually developers

00:13:54,370 --> 00:13:59,050
and people are using that trace to look

00:13:56,800 --> 00:14:01,510
up information from the storage so that

00:13:59,050 --> 00:14:03,580
tag that release ID that trace ID is

00:14:01,510 --> 00:14:05,950
index set so the cardinality for

00:14:03,580 --> 00:14:10,270
database can grow so you need to take a

00:14:05,950 --> 00:14:12,550
good one so this is many the problem

00:14:10,270 --> 00:14:16,210
around tracing how to have a optimized

00:14:12,550 --> 00:14:18,700
storage for that influx DB we are doing

00:14:16,210 --> 00:14:20,680
some tests and luckily we saw that our

00:14:18,700 --> 00:14:24,900
developer and our support team doesn't

00:14:20,680 --> 00:14:27,610
look for traces for too long it usually

00:14:24,900 --> 00:14:30,550
we usually look for traces for the last

00:14:27,610 --> 00:14:32,410
week because maybe somebody you send us

00:14:30,550 --> 00:14:34,990
a ticket and say this request ID failed

00:14:32,410 --> 00:14:36,100
can you figure it out or we are testing

00:14:34,990 --> 00:14:39,250
and we need to understand what's going

00:14:36,100 --> 00:14:42,280
on so it's really the life cycle is

00:14:39,250 --> 00:14:43,900
really short so we can in infancy be

00:14:42,280 --> 00:14:47,290
there is a feature color retention

00:14:43,900 --> 00:14:49,600
policy that you can set your custom date

00:14:47,290 --> 00:14:51,520
and after that time all the data will be

00:14:49,600 --> 00:14:53,860
cleaned so we have a retention policy in

00:14:51,520 --> 00:14:56,680
one week and Tracy's older than what in

00:14:53,860 --> 00:15:00,430
a week are automatically removed from

00:14:56,680 --> 00:15:03,040
database and what you can do is you can

00:15:00,430 --> 00:15:07,050
have a procedure that down sample data

00:15:03,040 --> 00:15:11,650
and store them in a long-term storage

00:15:07,050 --> 00:15:13,900
so with yeah we said yeah we said a week

00:15:11,650 --> 00:15:15,910
of retention policy and after that we

00:15:13,900 --> 00:15:19,720
remove the old one and we use capacitor

00:15:15,910 --> 00:15:21,970
to down sample traces and keep what we

00:15:19,720 --> 00:15:27,400
need from them in a separate long long

00:15:21,970 --> 00:15:29,580
long time storage it usually simply be

00:15:27,400 --> 00:15:33,390
just different ways

00:15:29,580 --> 00:15:35,459
so this is an example in go-to as you

00:15:33,390 --> 00:15:37,769
can see there are two different packages

00:15:35,459 --> 00:15:40,200
one is called zip kin and what is called

00:15:37,769 --> 00:15:42,029
open tracing and I put this light

00:15:40,200 --> 00:15:44,820
together to show you the idea about

00:15:42,029 --> 00:15:47,040
having the tracer that is different from

00:15:44,820 --> 00:15:48,959
the instrumentation code that you will

00:15:47,040 --> 00:15:50,850
have in your application so the idea is

00:15:48,959 --> 00:15:52,350
that when you need your application you

00:15:50,850 --> 00:15:55,110
create the tracer then you inject a

00:15:52,350 --> 00:15:56,880
tracer in open tracing and after that

00:15:55,110 --> 00:15:58,560
you don't need to care about the tracing

00:15:56,880 --> 00:16:01,050
anymore in your application you have the

00:15:58,560 --> 00:16:04,350
open tracing one and you use it forever

00:16:01,050 --> 00:16:06,089
never if you need to change the tracer

00:16:04,350 --> 00:16:08,279
for some reason just to change the

00:16:06,089 --> 00:16:12,470
tracer in your main configuration and

00:16:08,279 --> 00:16:15,480
move over that's how you create a span

00:16:12,470 --> 00:16:17,910
in your code so you get the open trade

00:16:15,480 --> 00:16:19,829
you get the open tracing library you get

00:16:17,910 --> 00:16:22,890
the tracer and you start a new span

00:16:19,829 --> 00:16:27,420
every line inside the trace is a span so

00:16:22,890 --> 00:16:30,690
a trace is made of spans and when your

00:16:27,420 --> 00:16:34,380
function finish on when you need to

00:16:30,690 --> 00:16:37,290
close the trace you just in go you can

00:16:34,380 --> 00:16:38,940
defer it so it's the last stuff the last

00:16:37,290 --> 00:16:42,000
function that is executed when a

00:16:38,940 --> 00:16:46,350
function and or you can just end it

00:16:42,000 --> 00:16:51,360
manually so that that's it about the

00:16:46,350 --> 00:16:53,880
open tracing part and it's a standard

00:16:51,360 --> 00:16:58,230
that it's available and you can read it

00:16:53,880 --> 00:17:00,690
on github and in that side site so I put

00:16:58,230 --> 00:17:04,709
together a slides about how do we couple

00:17:00,690 --> 00:17:07,380
slides about how do we create a tracing

00:17:04,709 --> 00:17:10,230
infrastructure in influx data using our

00:17:07,380 --> 00:17:12,780
tools and that's the idea you have all

00:17:10,230 --> 00:17:15,089
your application over there and you need

00:17:12,780 --> 00:17:17,550
to instrument them currently the problem

00:17:15,089 --> 00:17:19,199
is the instrumentation because it's hard

00:17:17,550 --> 00:17:22,949
you need to keep developers and give

00:17:19,199 --> 00:17:24,419
them the ability to instrument it what

00:17:22,949 --> 00:17:28,050
we did with the SDKs it's not too

00:17:24,419 --> 00:17:32,750
complicated and yeah that's that's the

00:17:28,050 --> 00:17:35,340
breaking point usually with we have a

00:17:32,750 --> 00:17:37,620
our collector is called a telegraph and

00:17:35,340 --> 00:17:40,640
as I said you before it exposed a lot of

00:17:37,620 --> 00:17:42,870
plugins to collect metrics from

00:17:40,640 --> 00:17:43,360
different system one of them is called

00:17:42,870 --> 00:17:46,030
zip

00:17:43,360 --> 00:17:49,030
in plugging that expose a compatible API

00:17:46,030 --> 00:17:51,160
with Zipkin so what what you can do is

00:17:49,030 --> 00:17:53,679
to instrument your application to use

00:17:51,160 --> 00:17:56,350
the zip kinis DKA able to store all the

00:17:53,679 --> 00:17:58,630
metrics to Telegraph Telegraph will

00:17:56,350 --> 00:18:00,670
collect them bunch of them usually by

00:17:58,630 --> 00:18:02,620
default it's ten second and every 10

00:18:00,670 --> 00:18:05,260
seconds it's send metrics to inference

00:18:02,620 --> 00:18:06,580
it be this is really good for each

00:18:05,260 --> 00:18:07,900
availability because you can have a

00:18:06,580 --> 00:18:10,210
bunch of Telegraph they don't have

00:18:07,900 --> 00:18:11,799
States so if you're going to lose if one

00:18:10,210 --> 00:18:14,679
of them is going away you are just

00:18:11,799 --> 00:18:19,150
losing ten seconds data in the worst

00:18:14,679 --> 00:18:20,980
case so it's not super critical and yeah

00:18:19,150 --> 00:18:23,799
you can send them to improve DB on in

00:18:20,980 --> 00:18:25,929
our case we are sending them to

00:18:23,799 --> 00:18:26,890
capacitor and I'm going to explain you

00:18:25,929 --> 00:18:32,500
why later

00:18:26,890 --> 00:18:33,940
so zip key yeah Telegraph supports a lot

00:18:32,500 --> 00:18:35,620
of plugins and there is a long

00:18:33,940 --> 00:18:38,470
configuration that you can create to say

00:18:35,620 --> 00:18:41,830
to him expose these or look for this my

00:18:38,470 --> 00:18:43,809
sequel over there so it's all well

00:18:41,830 --> 00:18:47,500
documented and this is a screenshot from

00:18:43,809 --> 00:18:49,090
the github did Kim plugin so you just

00:18:47,500 --> 00:18:50,919
need to put that stuff in your

00:18:49,090 --> 00:18:56,799
configuration item and it will magically

00:18:50,919 --> 00:19:01,330
expose HTTP port on 94 11 compatible

00:18:56,799 --> 00:19:03,100
with Zipkin we put a capacitor in the

00:19:01,330 --> 00:19:04,960
middle between in Flook CB and telegraph

00:19:03,100 --> 00:19:08,850
because we was collecting so many traces

00:19:04,960 --> 00:19:11,590
it we wasn't able to store them or the

00:19:08,850 --> 00:19:15,100
our criticality was growing so much that

00:19:11,590 --> 00:19:17,500
was critical and we have up we have a

00:19:15,100 --> 00:19:21,610
particular path that is the right one

00:19:17,500 --> 00:19:24,010
that is the route that all our clients

00:19:21,610 --> 00:19:26,140
are calling to store metrics and there

00:19:24,010 --> 00:19:28,990
are like there are too many requests to

00:19:26,140 --> 00:19:32,100
be trace at all of them so we decided to

00:19:28,990 --> 00:19:35,860
put a long sampler in the middle to get

00:19:32,100 --> 00:19:37,720
currently we are getting that 20% of the

00:19:35,860 --> 00:19:40,480
traces but it's a dynamic configuration

00:19:37,720 --> 00:19:42,520
that we can change and what I was trying

00:19:40,480 --> 00:19:44,740
to say here is that you should have a

00:19:42,520 --> 00:19:48,880
smart down sampler at some point in your

00:19:44,740 --> 00:19:51,130
chain to dynamically configure what you

00:19:48,880 --> 00:19:53,740
are happy to store and what you are not

00:19:51,130 --> 00:19:56,740
to you aren't happy to store because if

00:19:53,740 --> 00:19:58,390
you have I frequency API request store

00:19:56,740 --> 00:20:00,400
of them can be critical and maybe can

00:19:58,390 --> 00:20:04,240
also be not really important from a

00:20:00,400 --> 00:20:07,480
feature perspective so have a long

00:20:04,240 --> 00:20:09,309
sampler can be good and at the hand we

00:20:07,480 --> 00:20:12,390
store all the points in infancy be so

00:20:09,309 --> 00:20:15,730
they are stored there and safe

00:20:12,390 --> 00:20:17,980
we don't have a UI so we decided to go

00:20:15,730 --> 00:20:20,580
with out with other UI currently just to

00:20:17,980 --> 00:20:24,760
give to our developer the ability to

00:20:20,580 --> 00:20:26,290
trace and to query and usually my

00:20:24,760 --> 00:20:28,059
colleagues are really good writing query

00:20:26,290 --> 00:20:31,090
for influenza B so they don't have a lot

00:20:28,059 --> 00:20:32,620
of problems but I just decided to say

00:20:31,090 --> 00:20:35,080
that now because if you're looking for

00:20:32,620 --> 00:20:37,630
UI there isn't yet but the idea is that

00:20:35,080 --> 00:20:39,580
from our use cases we will be able to at

00:20:37,630 --> 00:20:43,420
least have some prototype working at

00:20:39,580 --> 00:20:45,490
some point so and this I built this

00:20:43,420 --> 00:20:47,350
example we didn't fix to be because I'm

00:20:45,490 --> 00:20:50,050
comfortable with with the tools and I'm

00:20:47,350 --> 00:20:53,800
using them but you can directly use

00:20:50,050 --> 00:20:56,140
Zipkin or Jaeger or another tracer they

00:20:53,800 --> 00:20:59,230
usually supports multiple backends so I

00:20:56,140 --> 00:21:01,390
know for from my direct experience that

00:20:59,230 --> 00:21:03,580
zip team support Cassandra and

00:21:01,390 --> 00:21:05,830
elasticsearch probably also others I

00:21:03,580 --> 00:21:08,050
don't know and the same for Jaeger they

00:21:05,830 --> 00:21:13,420
supports elasticsearch and Cassandra so

00:21:08,050 --> 00:21:17,920
you can go with them so part of the

00:21:13,420 --> 00:21:20,050
process these tools matrix log traces

00:21:17,920 --> 00:21:23,650
are all about understanding application

00:21:20,050 --> 00:21:26,559
in system and the process that I learn

00:21:23,650 --> 00:21:28,690
and I try to keep my colleagues in the

00:21:26,559 --> 00:21:30,640
loop is the is this one so you

00:21:28,690 --> 00:21:32,559
instrument your application to observe

00:21:30,640 --> 00:21:35,110
the metrics that you are able to get on

00:21:32,559 --> 00:21:37,890
your service system to understand how

00:21:35,110 --> 00:21:40,690
many memories or CPU you are getting

00:21:37,890 --> 00:21:43,570
when you observe them you try to down

00:21:40,690 --> 00:21:45,429
sample and aggregate data and when you

00:21:43,570 --> 00:21:50,110
aggregate them you can take action so

00:21:45,429 --> 00:21:52,120
make alerts or automation on it the

00:21:50,110 --> 00:21:54,550
aggregation is really important for what

00:21:52,120 --> 00:21:56,650
I discover because if you look at a in a

00:21:54,550 --> 00:21:58,510
if you look at a complex system from

00:21:56,650 --> 00:22:00,250
only one matrix you're gonna have

00:21:58,510 --> 00:22:03,370
trouble and usually troubles means that

00:22:00,250 --> 00:22:05,230
you are get paged for nothing because if

00:22:03,370 --> 00:22:06,850
the CPU is growing but for some reason

00:22:05,230 --> 00:22:09,970
your system is able to balance itself

00:22:06,850 --> 00:22:13,660
why you need to be paged so I think

00:22:09,970 --> 00:22:15,700
you the secret is here is to is really

00:22:13,660 --> 00:22:17,710
that you instrument your service you get

00:22:15,700 --> 00:22:19,150
everything you need from everything you

00:22:17,710 --> 00:22:22,090
can from your servers and from your

00:22:19,150 --> 00:22:23,350
application you wash how it behaves you

00:22:22,090 --> 00:22:26,110
understand how your application are

00:22:23,350 --> 00:22:29,740
working together from that you are able

00:22:26,110 --> 00:22:31,810
to take action and others if you don't

00:22:29,740 --> 00:22:35,410
do that you're gonna wake up in the

00:22:31,810 --> 00:22:37,240
night how do you keep developers in the

00:22:35,410 --> 00:22:40,180
loop they should deploy their services

00:22:37,240 --> 00:22:43,330
and with all the tools that we saw today

00:22:40,180 --> 00:22:44,920
primitives or terraform kubernetes they

00:22:43,330 --> 00:22:47,890
are able to that to do that I'm a

00:22:44,920 --> 00:22:49,930
developer and I deploy my code and if

00:22:47,890 --> 00:22:53,590
they do that they should be also take

00:22:49,930 --> 00:22:57,070
care about it and be on call for the

00:22:53,590 --> 00:22:58,960
part that represents the capability it's

00:22:57,070 --> 00:23:01,270
that part is really hard because you

00:22:58,960 --> 00:23:04,180
don't know right it's hard to split what

00:23:01,270 --> 00:23:05,350
and who calls where but it's something

00:23:04,180 --> 00:23:07,210
that you figure out with the right

00:23:05,350 --> 00:23:09,460
metrics and with the right observability

00:23:07,210 --> 00:23:11,560
on the services and order on the

00:23:09,460 --> 00:23:16,840
application so I'll keep developers in

00:23:11,560 --> 00:23:18,910
the loop it's really important and we we

00:23:16,840 --> 00:23:22,120
are trying to present our services when

00:23:18,910 --> 00:23:24,640
we when we write them and we write

00:23:22,120 --> 00:23:27,010
capacitor alerts inside the same

00:23:24,640 --> 00:23:29,590
repository and we apply them in this way

00:23:27,010 --> 00:23:32,440
they will also be involved in something

00:23:29,590 --> 00:23:35,370
that is not just the code that they are

00:23:32,440 --> 00:23:38,140
writing but it's also the operational

00:23:35,370 --> 00:23:40,120
like point of view also because they are

00:23:38,140 --> 00:23:41,950
the developers so they know that what

00:23:40,120 --> 00:23:44,980
it's critical of what should be critical

00:23:41,950 --> 00:23:47,770
and they can expose the right metrics

00:23:44,980 --> 00:23:50,170
out from the application then and we can

00:23:47,770 --> 00:23:51,940
use that matrix and we can correlate

00:23:50,170 --> 00:23:53,890
that matrix with the metrics that are

00:23:51,940 --> 00:23:55,420
coming from the server's the from the

00:23:53,890 --> 00:23:57,390
server's from from the containers where

00:23:55,420 --> 00:24:01,260
it's running

00:23:57,390 --> 00:24:02,980
yeah keep everyone in the loop and

00:24:01,260 --> 00:24:05,290
responsible for what's happening in

00:24:02,980 --> 00:24:11,770
production because that's where the

00:24:05,290 --> 00:24:13,330
funny stuff happens and yeah that's fine

00:24:11,770 --> 00:24:14,920
it I learned a lot of stuff running my

00:24:13,330 --> 00:24:20,440
code in production so I think it can be

00:24:14,920 --> 00:24:23,170
variable for everyone and as we heard a

00:24:20,440 --> 00:24:23,710
lot everything that we expose everybody

00:24:23,170 --> 00:24:26,020
right

00:24:23,710 --> 00:24:27,370
was on API so developer can use them

00:24:26,020 --> 00:24:30,669
even from an infrastructure perspective

00:24:27,370 --> 00:24:31,240
we should the expose ap is that people

00:24:30,669 --> 00:24:35,860
can use

00:24:31,240 --> 00:24:39,100
I saw follow with a talk from Kelsey a

00:24:35,860 --> 00:24:41,919
tower few months ago and he was

00:24:39,100 --> 00:24:43,840
developing deploying is came a scalable

00:24:41,919 --> 00:24:46,480
capability inside the application itself

00:24:43,840 --> 00:24:48,789
using the kubernetes api maybe that is

00:24:46,480 --> 00:24:51,220
it's it's too much but it's definitely

00:24:48,789 --> 00:24:53,980
an idea if we expose api we are able to

00:24:51,220 --> 00:24:55,950
do magic stuff for example create run

00:24:53,980 --> 00:24:58,929
time alerts with capacitor is the same

00:24:55,950 --> 00:25:02,380
that the same example that we saw before

00:24:58,929 --> 00:25:06,850
with terraform and page of duty we are

00:25:02,380 --> 00:25:08,380
doing similar stuff without the without

00:25:06,850 --> 00:25:10,390
tariffs or more with capacitor and a

00:25:08,380 --> 00:25:15,399
bunch of api calls because it exposed

00:25:10,390 --> 00:25:17,529
api so it's simple so how they introduce

00:25:15,399 --> 00:25:22,120
introduce you before server containers

00:25:17,529 --> 00:25:24,490
in vm are really dynamic and they are

00:25:22,120 --> 00:25:27,279
and they change how frequently we deploy

00:25:24,490 --> 00:25:31,870
code and how spread around data centers

00:25:27,279 --> 00:25:33,640
we are and for this reason we can name

00:25:31,870 --> 00:25:36,279
them anymore so they don't they don't

00:25:33,640 --> 00:25:39,669
are pets or animals that we have in our

00:25:36,279 --> 00:25:44,559
house they go and come yeah that's sad

00:25:39,669 --> 00:25:46,779
but it's true and often is it's it's

00:25:44,559 --> 00:25:49,529
really simple when you have a problem to

00:25:46,779 --> 00:25:52,659
just remove the server from the

00:25:49,529 --> 00:25:54,490
production pool replace it and figure

00:25:52,659 --> 00:25:56,380
out it later why it's not working

00:25:54,490 --> 00:25:58,510
because if you are in a really dynamic

00:25:56,380 --> 00:26:01,179
environment you don't need to keep a

00:25:58,510 --> 00:26:02,860
server that is key that it's failing for

00:26:01,179 --> 00:26:05,710
too long you just remove it from the

00:26:02,860 --> 00:26:07,960
pool the autoscaler will add a new one

00:26:05,710 --> 00:26:10,480
and you troubleshoot the problem in that

00:26:07,960 --> 00:26:14,020
servers later so this is a practice that

00:26:10,480 --> 00:26:20,440
it can be follow and it helps you to try

00:26:14,020 --> 00:26:22,390
your automation yeah yeah hazard was

00:26:20,440 --> 00:26:24,880
yeah the server doesn't have name

00:26:22,390 --> 00:26:27,240
usually the it has role and in

00:26:24,880 --> 00:26:30,789
kubernetes they are called label so you

00:26:27,240 --> 00:26:33,070
attach a label to to your nodes and you

00:26:30,789 --> 00:26:36,789
create your topology for example you can

00:26:33,070 --> 00:26:37,570
label servers that has SS SS the

00:26:36,789 --> 00:26:39,820
mountains

00:26:37,570 --> 00:26:42,370
and you can mount servers that are

00:26:39,820 --> 00:26:44,350
running that requires SSD for example in

00:26:42,370 --> 00:26:46,270
50 B is that intense intensive

00:26:44,350 --> 00:26:48,690
application because it's a database so

00:26:46,270 --> 00:26:51,730
you need a SSD and you can say

00:26:48,690 --> 00:26:55,030
kubernetes deployed these containers

00:26:51,730 --> 00:26:57,460
only in server that has SSD so they your

00:26:55,030 --> 00:27:01,480
servers doesn't have name it has rows

00:26:57,460 --> 00:27:03,220
and features that you can that you can

00:27:01,480 --> 00:27:05,340
use to identify where to deploy an

00:27:03,220 --> 00:27:05,340
application

00:27:06,010 --> 00:27:11,590
yeah the DevOps is a is an attitude it's

00:27:09,610 --> 00:27:14,140
not something that you like it's not a

00:27:11,590 --> 00:27:16,240
role we I think that we are also sort of

00:27:14,140 --> 00:27:18,850
engineers some of them are more close to

00:27:16,240 --> 00:27:21,370
the servers and automation compared with

00:27:18,850 --> 00:27:25,630
others that maybe likes more something

00:27:21,370 --> 00:27:28,090
else like e-commerce or whatever but is

00:27:25,630 --> 00:27:32,850
more like an attitude so it's something

00:27:28,090 --> 00:27:36,790
that can grow and can be injected or

00:27:32,850 --> 00:27:43,420
these two new developer so you just

00:27:36,790 --> 00:27:45,490
require some time I'm back on a more

00:27:43,420 --> 00:27:48,010
traditional use case and doesn't involve

00:27:45,490 --> 00:27:50,610
traces it's not the really oldest

00:27:48,010 --> 00:27:54,940
possible because it involves kubernetes

00:27:50,610 --> 00:27:59,140
but let's imagine that every node is a

00:27:54,940 --> 00:28:01,930
server and Rize kubernetes and it's part

00:27:59,140 --> 00:28:04,030
of a kubernetes cluster how do we

00:28:01,930 --> 00:28:06,160
monitor this in improved data is using

00:28:04,030 --> 00:28:10,710
Telegraph we deploy a daemon set demon

00:28:06,160 --> 00:28:13,840
set is a kubernetes resources that it's

00:28:10,710 --> 00:28:16,090
it's designed to deploy agents because

00:28:13,840 --> 00:28:18,850
you can say deployed this demon set in

00:28:16,090 --> 00:28:20,800
every server in my cluster so what

00:28:18,850 --> 00:28:23,050
what's happening here you're saying okay

00:28:20,800 --> 00:28:26,200
deploy one telegraph in every server in

00:28:23,050 --> 00:28:28,420
my cluster and Telegraph is configured

00:28:26,200 --> 00:28:30,970
user usually to take all the metrics

00:28:28,420 --> 00:28:33,730
from your system memory CPU network disk

00:28:30,970 --> 00:28:35,860
and whatever you can takes plus the

00:28:33,730 --> 00:28:37,660
docker in docker information so we can

00:28:35,860 --> 00:28:38,290
manage the docker socket and see what's

00:28:37,660 --> 00:28:42,070
going on

00:28:38,290 --> 00:28:45,190
it also takes matrix from the cubelet

00:28:42,070 --> 00:28:47,260
itself so we can grab information from

00:28:45,190 --> 00:28:48,000
all the kubernetes process that are

00:28:47,260 --> 00:28:51,710
running

00:28:48,000 --> 00:28:54,480
so now you have one Telegraph for every

00:28:51,710 --> 00:28:56,010
servers and you are ready getting a lot

00:28:54,480 --> 00:28:58,680
of good information from your system

00:28:56,010 --> 00:29:01,880
because you know almost everything that

00:28:58,680 --> 00:29:04,440
is running inside and how its behaving

00:29:01,880 --> 00:29:06,740
other than that in an in a separate

00:29:04,440 --> 00:29:08,700
namespace or you can have it if your

00:29:06,740 --> 00:29:11,190
infrastructure is really good and you

00:29:08,700 --> 00:29:13,890
are looking for high availability you

00:29:11,190 --> 00:29:16,110
should have your monitoring namespace in

00:29:13,890 --> 00:29:18,210
another data center or replicated in

00:29:16,110 --> 00:29:20,010
another place because it can be in your

00:29:18,210 --> 00:29:21,810
same infrastructure because if your

00:29:20,010 --> 00:29:23,760
infrastructure go down with your monitor

00:29:21,810 --> 00:29:25,080
you don't know that it's down you don't

00:29:23,760 --> 00:29:28,190
wake up but it's not a real good

00:29:25,080 --> 00:29:30,450
experience for all the other people so

00:29:28,190 --> 00:29:32,070
this is just an example so you have your

00:29:30,450 --> 00:29:34,140
monitor infrastructure your monitor

00:29:32,070 --> 00:29:36,240
namespace within 50 B and chronograph

00:29:34,140 --> 00:29:38,820
for your UI and you have a telegraph in

00:29:36,240 --> 00:29:40,860
every node so your developers start to

00:29:38,820 --> 00:29:43,020
deploy application and how do you manage

00:29:40,860 --> 00:29:45,320
your application you can there are

00:29:43,020 --> 00:29:47,700
different ways so you can instrument

00:29:45,320 --> 00:29:49,710
capacitor or if you have for me to use

00:29:47,700 --> 00:29:51,840
parameters can grab them from all the

00:29:49,710 --> 00:29:54,710
places but usually what I what I learned

00:29:51,840 --> 00:29:58,410
is that every application has its own

00:29:54,710 --> 00:30:00,600
way to tell about itself and to expose

00:29:58,410 --> 00:30:02,970
metrics so some of them are new

00:30:00,600 --> 00:30:05,160
application and they are exposing the

00:30:02,970 --> 00:30:07,680
parameters format with Slash metrics so

00:30:05,160 --> 00:30:12,000
they are super easy to grab but other of

00:30:07,680 --> 00:30:14,880
them are more like creeping around that

00:30:12,000 --> 00:30:17,520
and you can do that so or even if you

00:30:14,880 --> 00:30:20,820
have a high intensive application and

00:30:17,520 --> 00:30:23,310
you need to load every point you can

00:30:20,820 --> 00:30:26,520
have a pre aggregation that permit use

00:30:23,310 --> 00:30:29,040
and exporter can be your final choice so

00:30:26,520 --> 00:30:30,870
you may new maybe need a UDP collector

00:30:29,040 --> 00:30:34,020
that can get high frequency metrics and

00:30:30,870 --> 00:30:36,360
store them in freeze to be anticipate so

00:30:34,020 --> 00:30:39,810
Telegraph has give you the ability to do

00:30:36,360 --> 00:30:42,900
all these kind of tricks and what we do

00:30:39,810 --> 00:30:48,600
in kubernetes language is called sandbox

00:30:42,900 --> 00:30:50,760
so in every port a pod is a you deploy

00:30:48,600 --> 00:30:53,640
your application inside a pod and a pod

00:30:50,760 --> 00:30:54,860
can contain more containers and not just

00:30:53,640 --> 00:30:57,750
the one that you are deploying

00:30:54,860 --> 00:30:59,850
every container inside the same pod are

00:30:57,750 --> 00:31:01,770
sharing the network namespace so

00:30:59,850 --> 00:31:03,480
basically you can call them using

00:31:01,770 --> 00:31:06,360
localist and they look like they are

00:31:03,480 --> 00:31:08,400
together that's really a powerful

00:31:06,360 --> 00:31:10,140
feature and we use it for Telegraph's

00:31:08,400 --> 00:31:12,270
because we deploy a telegraph inside

00:31:10,140 --> 00:31:14,910
inside every part close to our

00:31:12,270 --> 00:31:16,860
application and based on the neon the

00:31:14,910 --> 00:31:20,730
needs for that application we are able

00:31:16,860 --> 00:31:25,440
to configure the collector in the best

00:31:20,730 --> 00:31:28,080
way so for example for etcd that is the

00:31:25,440 --> 00:31:31,710
guy over there you see the expose of

00:31:28,080 --> 00:31:34,980
slash metrics primitives like scraping

00:31:31,710 --> 00:31:37,200
entry point so you can just we just

00:31:34,980 --> 00:31:39,710
configured telegraph with the primitives

00:31:37,200 --> 00:31:42,180
input to scrape the metrics from the

00:31:39,710 --> 00:31:44,550
entry point but if you have an

00:31:42,180 --> 00:31:46,710
application like another application

00:31:44,550 --> 00:31:50,270
that doesn't expose that capability you

00:31:46,710 --> 00:31:55,350
can maybe expose UDP socket you can

00:31:50,270 --> 00:31:58,380
deploy Telegraph plugging that list into

00:31:55,350 --> 00:31:59,790
a socket and read the metrics from your

00:31:58,380 --> 00:32:04,680
application so it's really really

00:31:59,790 --> 00:32:06,480
flexible in this way when you are

00:32:04,680 --> 00:32:11,360
designed when you are a distributed

00:32:06,480 --> 00:32:14,460
system you don't really care about

00:32:11,360 --> 00:32:18,030
memory or this kind of stuff from my

00:32:14,460 --> 00:32:21,540
perspective we care about the state of

00:32:18,030 --> 00:32:24,270
the application and events because as I

00:32:21,540 --> 00:32:26,220
said before memory CPU they are going

00:32:24,270 --> 00:32:29,550
and coming my container sees is going

00:32:26,220 --> 00:32:31,140
away if it reach if it gets too much

00:32:29,550 --> 00:32:33,750
memory because the scatterer is going to

00:32:31,140 --> 00:32:37,980
kick us could kick it away so I don't

00:32:33,750 --> 00:32:41,340
really care what I care is that my event

00:32:37,980 --> 00:32:44,040
so if somebody is asking me if I

00:32:41,340 --> 00:32:46,710
received a create user event I need to

00:32:44,040 --> 00:32:48,900
be sure that that event is stored and is

00:32:46,710 --> 00:32:52,230
safe if my application crash it should

00:32:48,900 --> 00:32:54,120
be able to get the event again that's

00:32:52,230 --> 00:32:56,600
what we call the resilient application

00:32:54,120 --> 00:33:00,230
so they can fail come to in come again

00:32:56,600 --> 00:33:03,390
so what we are really monitoring for is

00:33:00,230 --> 00:33:05,330
the event in our application it's not

00:33:03,390 --> 00:33:08,430
really nothing more than that

00:33:05,330 --> 00:33:10,380
how many of them unable to solve many of

00:33:08,430 --> 00:33:14,160
them I'm losing in the middle of

00:33:10,380 --> 00:33:16,620
something and so on

00:33:14,160 --> 00:33:18,690
we don't care us we also care about

00:33:16,620 --> 00:33:19,440
datum that data is a complete different

00:33:18,690 --> 00:33:22,230
topic

00:33:19,440 --> 00:33:24,240
so I hope you guys spend a lot of time

00:33:22,230 --> 00:33:25,679
on that because where that data

00:33:24,240 --> 00:33:28,049
warehouse is something that I don't know

00:33:25,679 --> 00:33:30,750
about so I just know that we care about

00:33:28,049 --> 00:33:32,850
data so I'm just going these slides to

00:33:30,750 --> 00:33:36,230
remember you that all we care about data

00:33:32,850 --> 00:33:40,289
to how this is an example about how

00:33:36,230 --> 00:33:42,630
re-try can really better try can be aid

00:33:40,289 --> 00:33:48,120
in your system so this is an example of

00:33:42,630 --> 00:33:50,520
trace took by zip King and when you are

00:33:48,120 --> 00:33:53,490
designing resilient application if

00:33:50,520 --> 00:33:56,460
you're doing that in a not really good

00:33:53,490 --> 00:33:59,220
way you have a retry in every API

00:33:56,460 --> 00:34:01,710
request and if it fails you try again

00:33:59,220 --> 00:34:04,919
maybe for 10 times maybe for 10 minutes

00:34:01,710 --> 00:34:08,609
maybe I don't know for it's it's only

00:34:04,919 --> 00:34:11,070
it's on the developer to set this this

00:34:08,609 --> 00:34:12,990
resilient methodology but if you have a

00:34:11,070 --> 00:34:15,350
problem and all your requests are

00:34:12,990 --> 00:34:18,690
failing your application is just

00:34:15,350 --> 00:34:23,220
retrying and retry and return again so

00:34:18,690 --> 00:34:25,470
this is a really long trace with 156

00:34:23,220 --> 00:34:27,270
plans so almost every science is already

00:34:25,470 --> 00:34:29,369
try is trying to do the same stuff again

00:34:27,270 --> 00:34:31,260
because it fails and it wasn't able to

00:34:29,369 --> 00:34:35,220
understand it it was a failure for other

00:34:31,260 --> 00:34:37,500
ISM and the request took more than 10

00:34:35,220 --> 00:34:40,889
minutes and probably somebody keyed at

00:34:37,500 --> 00:34:42,899
some point so when you when you listen

00:34:40,889 --> 00:34:44,820
about resilient application try to

00:34:42,899 --> 00:34:47,190
investigate about what it means because

00:34:44,820 --> 00:34:51,869
if it means that a try and it's not

00:34:47,190 --> 00:34:55,440
going to work so let's summarize

00:34:51,869 --> 00:34:57,270
everything monitor distributed system is

00:34:55,440 --> 00:34:58,770
hard so if you are migrating from a

00:34:57,270 --> 00:35:01,740
traditional infrastructure to a

00:34:58,770 --> 00:35:04,130
distributed this tribute at one also

00:35:01,740 --> 00:35:06,710
think about how to monitor it how to

00:35:04,130 --> 00:35:09,000
recover how to understand your

00:35:06,710 --> 00:35:11,580
applications in your system because it's

00:35:09,000 --> 00:35:13,380
going to be different

00:35:11,580 --> 00:35:16,650
open tracing and distributed tracing is

00:35:13,380 --> 00:35:20,240
one of the tools that you you can use to

00:35:16,650 --> 00:35:22,619
have a better visibility on your system

00:35:20,240 --> 00:35:25,160
keep people in the loop because they are

00:35:22,619 --> 00:35:28,140
making the application and they know

00:35:25,160 --> 00:35:31,079
probably something more about

00:35:28,140 --> 00:35:31,650
why it doesn't work and the Bobst is an

00:35:31,079 --> 00:35:33,989
attitude

00:35:31,650 --> 00:35:36,869
so let's grow it together with our

00:35:33,989 --> 00:35:41,910
colleagues and servers containers and

00:35:36,869 --> 00:35:44,880
processes our pet not are not brevity's

00:35:41,910 --> 00:35:47,670
was my my brain and trying to convert me

00:35:44,880 --> 00:35:52,079
but they go and come so don't look at

00:35:47,670 --> 00:35:55,289
them in you know too serious way look

00:35:52,079 --> 00:35:59,819
for application stated events and listen

00:35:55,289 --> 00:36:02,910
to your application have fun so when we

00:35:59,819 --> 00:36:05,249
are looking for a when we analyze a

00:36:02,910 --> 00:36:08,130
system the data are just one of the

00:36:05,249 --> 00:36:11,220
problem that's what I try to say during

00:36:08,130 --> 00:36:12,720
my talk you can you can collect all the

00:36:11,220 --> 00:36:14,609
data of the world but if you are not

00:36:12,720 --> 00:36:16,680
able to understand them or to collect

00:36:14,609 --> 00:36:19,170
them together or to keep them together

00:36:16,680 --> 00:36:20,730
or to design smart alerts because you

00:36:19,170 --> 00:36:25,910
don't know how your application behaves

00:36:20,730 --> 00:36:28,319
all these data are gonna be useless so

00:36:25,910 --> 00:36:30,569
to answer your question need to have a

00:36:28,319 --> 00:36:32,579
good collection metric collection but

00:36:30,569 --> 00:36:36,239
also a good alerting a good downsampling

00:36:32,579 --> 00:36:40,530
application and aggregation tools so

00:36:36,239 --> 00:36:46,819
that's it for me thank you

00:36:40,530 --> 00:36:54,349
[Applause]

00:36:46,819 --> 00:36:54,349
q Qian Luka are there any questions so

00:36:57,859 --> 00:37:04,289
so if you have no you aren't you see

00:37:01,980 --> 00:37:08,809
yeah Luka if you have no UI to see what

00:37:04,289 --> 00:37:08,809
you get from your capacitor is it right

00:37:11,029 --> 00:37:18,269
currently we like we are using that that

00:37:15,480 --> 00:37:21,029
infrastructure because we are trying a

00:37:18,269 --> 00:37:25,410
new set of tools and we are developing

00:37:21,029 --> 00:37:28,980
new a new system so that's why we are

00:37:25,410 --> 00:37:31,230
also trying this tracing methodology and

00:37:28,980 --> 00:37:34,549
we are tracing our infrastructure so we

00:37:31,230 --> 00:37:38,239
are using you are directly creating our

00:37:34,549 --> 00:37:41,880
database impossible supports SQL

00:37:38,239 --> 00:37:44,100
compatible language so we use almost ICS

00:37:41,880 --> 00:37:48,120
all queries to get a single trace with

00:37:44,100 --> 00:37:50,400
all the span and we correlate them it's

00:37:48,120 --> 00:37:55,200
not I hope this won't be the final

00:37:50,400 --> 00:37:58,500
solution for us but that's what we have

00:37:55,200 --> 00:38:01,980
and if you if you're looking for a

00:37:58,500 --> 00:38:06,870
complete kind of solution as I say there

00:38:01,980 --> 00:38:09,420
are 15 on Yaeger that has a nice UI and

00:38:06,870 --> 00:38:12,240
you can store them store all your points

00:38:09,420 --> 00:38:15,390
in in zip clean it will use Cassandra to

00:38:12,240 --> 00:38:18,060
keep them safe and query them via UI so

00:38:15,390 --> 00:38:20,400
it would be if we work you'd hold some

00:38:18,060 --> 00:38:24,390
slides before you write them with

00:38:20,400 --> 00:38:26,460
Telegraph into influx TVC right yeah

00:38:24,390 --> 00:38:28,050
Telegraph is our collector so you get

00:38:26,460 --> 00:38:30,870
all the metrics inside the collector

00:38:28,050 --> 00:38:37,050
named tweens the way to visualize them

00:38:30,870 --> 00:38:41,310
was in flux we yeah you can you can do

00:38:37,050 --> 00:38:43,830
that but I don't name to to create a

00:38:41,310 --> 00:38:44,820
trace you need a proper graph so I don't

00:38:43,830 --> 00:38:47,790
know if you're a fan I had that

00:38:44,820 --> 00:38:50,370
capability if it has let me know I will

00:38:47,790 --> 00:38:52,320
use it but yeah I don't think there is

00:38:50,370 --> 00:38:54,990
so you can't do that but that's probably

00:38:52,320 --> 00:38:58,380
the way to go so contributing we worked

00:38:54,990 --> 00:39:00,120
out with Agrafena people to win our

00:38:58,380 --> 00:39:02,960
integration with in flux so maybe the

00:39:00,120 --> 00:39:07,020
next step would be to have a trace there

00:39:02,960 --> 00:39:09,900
okay any questions left okay then

00:39:07,020 --> 00:39:11,970
Gianluca thank you thank you

00:39:09,900 --> 00:39:14,030
[Applause]

00:39:11,970 --> 00:39:14,030
you

00:39:21,310 --> 00:39:31,959

YouTube URL: https://www.youtube.com/watch?v=AHZP9Ivh0uI


