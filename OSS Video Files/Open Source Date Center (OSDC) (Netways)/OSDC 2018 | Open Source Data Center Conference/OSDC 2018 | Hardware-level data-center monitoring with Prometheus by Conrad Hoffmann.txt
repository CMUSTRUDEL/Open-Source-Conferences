Title: OSDC 2018 | Hardware-level data-center monitoring with Prometheus by Conrad Hoffmann
Publication date: 2018-06-22
Playlist: OSDC 2018 | Open Source Data Center Conference
Description: 
	SoundCloud - Prometheus. However, when it comes down to hardware, getting data into Prometheus isn’t always straight-forward. In this talk, I will provide a look into how we managed to port all our infrastructure monitoring – including SNMP, IPMI and more – to Prometheus, and even improve it along the way.

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Google+: https://plus.google.com/+netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:00,540 --> 00:00:03,420
[Music]

00:00:01,330 --> 00:00:03,420
you

00:00:11,750 --> 00:00:15,179
[Music]

00:00:13,129 --> 00:00:17,009
thank you very much and good morning

00:00:15,179 --> 00:00:20,239
everyone excited to have you all here

00:00:17,009 --> 00:00:23,789
and I hope you've all had enough coffee

00:00:20,239 --> 00:00:25,890
my name is Konrad and I work in one of

00:00:23,789 --> 00:00:28,919
the infrastructure teams at SoundCloud

00:00:25,890 --> 00:00:31,680
here in Berlin and that SoundCloud we

00:00:28,919 --> 00:00:33,329
operate a sizable data center and we

00:00:31,680 --> 00:00:36,510
also use a lot of open source software

00:00:33,329 --> 00:00:39,329
so I was actually bit disappointed that

00:00:36,510 --> 00:00:40,829
we just this year found out about this

00:00:39,329 --> 00:00:43,500
very conference right here at our

00:00:40,829 --> 00:00:45,390
doorstep however now that we did we

00:00:43,500 --> 00:00:47,760
figured maybe we even have a bit to

00:00:45,390 --> 00:00:49,530
contribute here because not only do we

00:00:47,760 --> 00:00:52,680
use a lot of open source software we

00:00:49,530 --> 00:00:54,809
occasionally also create some and one of

00:00:52,680 --> 00:00:57,270
the more noteworthy occasions in the

00:00:54,809 --> 00:00:59,309
recent past or as someone actually

00:00:57,270 --> 00:01:01,469
phrased it over a beer last night one of

00:00:59,309 --> 00:01:03,210
our more successful products is

00:01:01,469 --> 00:01:07,890
certainly the Prometheus monitoring

00:01:03,210 --> 00:01:11,009
system so as a company we're still

00:01:07,890 --> 00:01:13,229
heavily invested in the Prometheus

00:01:11,009 --> 00:01:17,310
project even though it has by now grown

00:01:13,229 --> 00:01:20,429
up to become a CN CF project and as such

00:01:17,310 --> 00:01:22,560
we recently made a move to migrate all

00:01:20,429 --> 00:01:26,719
our infrastructure monitoring from

00:01:22,560 --> 00:01:29,130
various other systems to Prometheus and

00:01:26,719 --> 00:01:30,689
we thought that is something that others

00:01:29,130 --> 00:01:33,600
might be interested in as well because

00:01:30,689 --> 00:01:35,069
Prometheus has traditionally been very

00:01:33,600 --> 00:01:37,049
much focused on application level

00:01:35,069 --> 00:01:38,939
monitoring so this talk is a bit about

00:01:37,049 --> 00:01:40,529
this migration and what we did and how

00:01:38,939 --> 00:01:43,770
we did it and the challenges you'll

00:01:40,529 --> 00:01:44,310
still face along the way so let's dive

00:01:43,770 --> 00:01:45,959
right in

00:01:44,310 --> 00:01:48,060
I'll first give you an introduction to

00:01:45,959 --> 00:01:49,529
our data center just so you know what we

00:01:48,060 --> 00:01:52,200
are up against in terms of monitoring

00:01:49,529 --> 00:01:54,359
and the scale we operate at I'll then

00:01:52,200 --> 00:01:57,509
give you a very brief introduction to

00:01:54,359 --> 00:01:59,310
Prometheus which would rightfully be a

00:01:57,509 --> 00:02:03,450
talk of its own so it'll also be a bit

00:01:59,310 --> 00:02:05,669
dense focus on the most relevant most

00:02:03,450 --> 00:02:09,000
relevant aspects for this presentation

00:02:05,669 --> 00:02:10,410
and don't worry if some of it doesn't

00:02:09,000 --> 00:02:12,239
immediately make sense because it's

00:02:10,410 --> 00:02:14,190
mostly intended to give you the right

00:02:12,239 --> 00:02:17,190
Google keywords to follow up later on if

00:02:14,190 --> 00:02:18,780
you like in the main part of the talk I

00:02:17,190 --> 00:02:21,300
go through all the exporters and

00:02:18,780 --> 00:02:23,310
integrations that we used and partially

00:02:21,300 --> 00:02:25,620
even create it along the way

00:02:23,310 --> 00:02:27,480
to achieve this migration and then as is

00:02:25,620 --> 00:02:30,569
customary I'll give you an Outlook and

00:02:27,480 --> 00:02:32,670
summary kind of thing so let's start

00:02:30,569 --> 00:02:34,709
with our data center it goes by the name

00:02:32,670 --> 00:02:37,260
of AMS five and as you can probably

00:02:34,709 --> 00:02:39,390
already infer it is located in the

00:02:37,260 --> 00:02:41,850
beautiful city of Amsterdam and I'm

00:02:39,390 --> 00:02:43,980
telling you this just to emphasize that

00:02:41,850 --> 00:02:46,170
if something breaks we don't just go to

00:02:43,980 --> 00:02:48,330
the data center and fix it we have to

00:02:46,170 --> 00:02:50,180
tell a team of remote hand engineers

00:02:48,330 --> 00:02:53,610
employed by the data center operator

00:02:50,180 --> 00:02:56,250
precisely what to do to fix or replace

00:02:53,610 --> 00:02:59,340
Hardware for us so we really have really

00:02:56,250 --> 00:03:02,550
rely on monitoring to know what's wrong

00:02:59,340 --> 00:03:04,590
in the first place in this data center

00:03:02,550 --> 00:03:06,900
we have a total or at least at the time

00:03:04,590 --> 00:03:08,910
I created this presentation of twenty

00:03:06,900 --> 00:03:10,590
one hundred and eighteen service and I'm

00:03:08,910 --> 00:03:12,660
giving you this oddly specific number

00:03:10,590 --> 00:03:14,640
because as a little teaser if you look

00:03:12,660 --> 00:03:17,120
closely this is actually a screenshot of

00:03:14,640 --> 00:03:20,400
a graph but more on that later

00:03:17,120 --> 00:03:22,890
this service is spread across 56 recs

00:03:20,400 --> 00:03:25,650
and this is the actual data center not

00:03:22,890 --> 00:03:27,329
stock photos we have run about 200

00:03:25,650 --> 00:03:29,549
network devices like routers switches

00:03:27,329 --> 00:03:32,370
and firewalls to connect all those and

00:03:29,549 --> 00:03:35,100
speaking of connectivity we have for

00:03:32,370 --> 00:03:39,180
peering partners - for generic internet

00:03:35,100 --> 00:03:41,640
connectivity and we have quite huge

00:03:39,180 --> 00:03:45,000
direct pairings with both AWS and Google

00:03:41,640 --> 00:03:46,910
because we've been a hybrid cloud and

00:03:45,000 --> 00:03:50,130
on-premises company for a long time and

00:03:46,910 --> 00:03:51,630
heavily rely on these links and yes we

00:03:50,130 --> 00:03:53,460
know some of our cabling looks better

00:03:51,630 --> 00:03:54,829
than others

00:03:53,460 --> 00:03:58,769
[Music]

00:03:54,829 --> 00:04:01,109
so all this needs to be monitored how

00:03:58,769 --> 00:04:04,019
did we do this in the past well when I

00:04:01,109 --> 00:04:06,000
joined SoundCloud you could find mention

00:04:04,019 --> 00:04:08,850
of all these monitoring related systems

00:04:06,000 --> 00:04:10,650
in our infrastructure code and to be

00:04:08,850 --> 00:04:12,470
fair you could say that I singer and

00:04:10,650 --> 00:04:15,600
Nagios are basically the same thing and

00:04:12,470 --> 00:04:17,700
also munion we actually had removed

00:04:15,600 --> 00:04:20,190
already by the time we started the move

00:04:17,700 --> 00:04:22,229
to prometheus however you know you'd

00:04:20,190 --> 00:04:23,910
find mentions of this and might be a

00:04:22,229 --> 00:04:27,300
little confusing for example for new

00:04:23,910 --> 00:04:33,510
hires as to what is being monitored were

00:04:27,300 --> 00:04:35,580
and how so what I personally find a

00:04:33,510 --> 00:04:37,080
little more confusing than just the mere

00:04:35,580 --> 00:04:39,599
presence of these systems

00:04:37,080 --> 00:04:41,580
is how they were being used because if

00:04:39,599 --> 00:04:44,909
you break down the job of monitoring

00:04:41,580 --> 00:04:47,520
into three distinct parts so data

00:04:44,909 --> 00:04:50,039
collection visualization and alerting

00:04:47,520 --> 00:04:51,840
you'll see that some of these systems

00:04:50,039 --> 00:04:54,960
cover the entire spectrum while others

00:04:51,840 --> 00:04:57,900
don't and rely on yet other tools to do

00:04:54,960 --> 00:05:00,780
that job for them so to give you a kind

00:04:57,900 --> 00:05:03,120
of a worst case example that you could

00:05:00,780 --> 00:05:06,689
encounter back then is you get paged

00:05:03,120 --> 00:05:08,669
pager Duty says it's an Argus alert so

00:05:06,689 --> 00:05:10,020
you think now yours then realize all

00:05:08,669 --> 00:05:12,240
right it's actually called I think I

00:05:10,020 --> 00:05:14,099
know so you go to Reisinger we are

00:05:12,240 --> 00:05:16,440
seeing a check but failed actually

00:05:14,099 --> 00:05:18,300
checks the value and graphite so let's

00:05:16,440 --> 00:05:20,550
look at the graphite graph huh it looks

00:05:18,300 --> 00:05:22,680
odd where does the data come from oh it

00:05:20,550 --> 00:05:25,379
actually comes from ganglia so I mean

00:05:22,680 --> 00:05:27,330
this is kind of bit exaggerated maybe

00:05:25,379 --> 00:05:29,759
but if you get woken up in the middle of

00:05:27,330 --> 00:05:31,110
the night and you have to follow through

00:05:29,759 --> 00:05:33,900
such a chain of systems

00:05:31,110 --> 00:05:37,490
it might make you know it takes longer

00:05:33,900 --> 00:05:40,020
than it maybe should in in an emergency

00:05:37,490 --> 00:05:43,229
so when this whole Prometheus thing

00:05:40,020 --> 00:05:46,050
started to take off we were of course

00:05:43,229 --> 00:05:48,449
wondering would it maybe be possible to

00:05:46,050 --> 00:05:51,960
just you know replace all of this with

00:05:48,449 --> 00:05:54,419
just Prometheus and whenever someone

00:05:51,960 --> 00:05:56,129
presents such a list to you and suggests

00:05:54,419 --> 00:05:58,740
to maybe you replace it with just one

00:05:56,129 --> 00:06:01,969
thing usually someone and rightfully so

00:05:58,740 --> 00:06:04,349
we'll bring up the glorious xkcd comic

00:06:01,969 --> 00:06:06,719
in case you don't know it it's called

00:06:04,349 --> 00:06:08,940
how standards proliferate and it says

00:06:06,719 --> 00:06:11,909
situation there are 14 competing

00:06:08,940 --> 00:06:13,979
standards and then someone says 14

00:06:11,909 --> 00:06:15,900
ridiculous we need to develop one

00:06:13,979 --> 00:06:17,909
Universal Center that covers everyone's

00:06:15,900 --> 00:06:20,639
use case and someone else agrees and

00:06:17,909 --> 00:06:23,430
then in the end you have soon situations

00:06:20,639 --> 00:06:24,839
there are 15 competing standards now

00:06:23,430 --> 00:06:29,129
there's a very important lesson in this

00:06:24,839 --> 00:06:31,199
strip and that is if you were interested

00:06:29,129 --> 00:06:33,360
in doing such a migration yourself or

00:06:31,199 --> 00:06:35,039
actually any kind of emigration of this

00:06:33,360 --> 00:06:37,259
kind you'd better not do it

00:06:35,039 --> 00:06:39,150
half-heartedly because there's a real

00:06:37,259 --> 00:06:41,190
danger you end up in a situation that is

00:06:39,150 --> 00:06:45,180
even worse or more complex than the one

00:06:41,190 --> 00:06:48,479
you started out at nevertheless spoiler

00:06:45,180 --> 00:06:50,700
alert it worked pretty well for us so I

00:06:48,479 --> 00:06:52,590
will revisit this list at

00:06:50,700 --> 00:06:56,660
the end of this talk to show you the

00:06:52,590 --> 00:06:59,810
situation we ended up win all right

00:06:56,660 --> 00:07:03,150
let's take a brief look at prometheus

00:06:59,810 --> 00:07:04,680
like I said might be a little dense

00:07:03,150 --> 00:07:06,660
don't worry too much about some of the

00:07:04,680 --> 00:07:09,300
details and there will be some specific

00:07:06,660 --> 00:07:11,580
details nevertheless if you get the

00:07:09,300 --> 00:07:13,320
general idea reading for example the

00:07:11,580 --> 00:07:15,860
Prometheus documentation should

00:07:13,320 --> 00:07:20,370
immediately fill in the gaps for you

00:07:15,860 --> 00:07:21,840
that said let's start out lightly if I

00:07:20,370 --> 00:07:25,590
had to make a marketing slide for

00:07:21,840 --> 00:07:27,540
Prometheus this would be it what we as

00:07:25,590 --> 00:07:30,990
the infrastructure team were looking for

00:07:27,540 --> 00:07:33,180
in prometheus is a reliable scalable yet

00:07:30,990 --> 00:07:34,950
flexible real-time monitoring and

00:07:33,180 --> 00:07:36,960
alerting system that is easy to

00:07:34,950 --> 00:07:40,170
integrate with as many things as

00:07:36,960 --> 00:07:42,480
possible now lucky for you this isn't a

00:07:40,170 --> 00:07:44,820
marketing talk or only sort of actually

00:07:42,480 --> 00:07:46,710
so let's actually look at some more

00:07:44,820 --> 00:07:51,180
detail at each of the points that I

00:07:46,710 --> 00:07:53,760
highlighted reliability important to

00:07:51,180 --> 00:07:56,040
point out is that Prometheus operates in

00:07:53,760 --> 00:07:58,530
a pool based manner where one such pool

00:07:56,040 --> 00:08:01,800
of data is referred to as a scrape a

00:07:58,530 --> 00:08:03,330
term I'll be using henceforth for that

00:08:01,800 --> 00:08:05,160
to work in the first place the

00:08:03,330 --> 00:08:06,990
Prometheus server needs to know about a

00:08:05,160 --> 00:08:10,050
list of targets that it's supposed to

00:08:06,990 --> 00:08:12,420
scrape now this might seem like

00:08:10,050 --> 00:08:14,160
additional configuration overhead but

00:08:12,420 --> 00:08:17,010
it's actually worth it because you can

00:08:14,160 --> 00:08:20,070
use some semi dynamic mechanisms to

00:08:17,010 --> 00:08:23,280
define your targets like for example DNS

00:08:20,070 --> 00:08:25,560
or service discovery and if you use the

00:08:23,280 --> 00:08:27,660
same mechanisms that your services use

00:08:25,560 --> 00:08:29,880
to talk to each other which you should

00:08:27,660 --> 00:08:31,560
it's actually really nice because the

00:08:29,880 --> 00:08:33,510
Prometheus server will immediately have

00:08:31,560 --> 00:08:35,460
the very same view of your

00:08:33,510 --> 00:08:38,099
infrastructure as all your services and

00:08:35,460 --> 00:08:40,170
if something that should be responding

00:08:38,099 --> 00:08:41,880
isn't responding or not producing any

00:08:40,170 --> 00:08:44,520
metrics you'll immediately know

00:08:41,880 --> 00:08:46,830
something's wrong on top of that

00:08:44,520 --> 00:08:49,320
Prometheus has built in meta meta

00:08:46,830 --> 00:08:51,720
monitoring so you can easily and

00:08:49,320 --> 00:08:54,980
extensively monitor Prometheus with

00:08:51,720 --> 00:08:57,120
Prometheus which is also really nice and

00:08:54,980 --> 00:08:59,460
redundancy is super easy

00:08:57,120 --> 00:09:01,890
for example our team simply has two

00:08:59,460 --> 00:09:03,910
servers that scrape the same targets and

00:09:01,890 --> 00:09:07,090
in case one breaks which

00:09:03,910 --> 00:09:09,370
DNS failover to the other one also very

00:09:07,090 --> 00:09:14,290
nice for example to test major version

00:09:09,370 --> 00:09:16,870
upgrades scalability prometheus is very

00:09:14,290 --> 00:09:19,000
performant and especially efficient on

00:09:16,870 --> 00:09:20,320
disk space I'm just going to put that

00:09:19,000 --> 00:09:23,710
out here for now I have some actual

00:09:20,320 --> 00:09:26,470
numbers later on it also scales really

00:09:23,710 --> 00:09:28,450
well to available resources probably

00:09:26,470 --> 00:09:31,980
partly thanks to being written in gold

00:09:28,450 --> 00:09:36,610
but also thanks to being written well

00:09:31,980 --> 00:09:38,500
you can see an H top screenshot below if

00:09:36,610 --> 00:09:40,480
you put some load on the server the load

00:09:38,500 --> 00:09:42,310
distributes really nicely across all

00:09:40,480 --> 00:09:44,950
available course the service doing

00:09:42,310 --> 00:09:46,390
nothing but running from e theist that

00:09:44,950 --> 00:09:49,450
said it also doesn't always use that

00:09:46,390 --> 00:09:50,890
much CPU because in this situation we

00:09:49,450 --> 00:09:54,090
actually put some load on it it's just

00:09:50,890 --> 00:09:56,590
sure how well it distributes

00:09:54,090 --> 00:09:59,170
nevertheless should your resources on

00:09:56,590 --> 00:10:01,900
one server not surface any more it is

00:09:59,170 --> 00:10:04,510
very easy to scale horizontally for

00:10:01,900 --> 00:10:07,960
example at SoundCloud again every team

00:10:04,510 --> 00:10:10,540
has their server or their servers mostly

00:10:07,960 --> 00:10:12,610
and the reason it is so easy to scale

00:10:10,540 --> 00:10:14,920
out horizontally is in case you do have

00:10:12,610 --> 00:10:17,620
any cross-cutting concerns it is very

00:10:14,920 --> 00:10:20,350
easy to aggregate certain metrics from

00:10:17,620 --> 00:10:23,970
multiple servers back into one server by

00:10:20,350 --> 00:10:23,970
using the built-in Federation feature

00:10:25,020 --> 00:10:30,790
flexibility now Prometheus employs what

00:10:29,230 --> 00:10:33,580
it refers to as a multi-dimensional

00:10:30,790 --> 00:10:36,220
label based data model what this means

00:10:33,580 --> 00:10:39,040
is that every metric is defined by a

00:10:36,220 --> 00:10:42,970
metric name and an arbitrary set of key

00:10:39,040 --> 00:10:46,030
value pairs the labels also by a value

00:10:42,970 --> 00:10:47,890
of course and to be exact a timestamp

00:10:46,030 --> 00:10:49,780
though you're rarely concerned was a

00:10:47,890 --> 00:10:52,000
timestamp because thanks to being pool

00:10:49,780 --> 00:10:55,510
based this is all handled internally by

00:10:52,000 --> 00:10:57,820
the Prometheus server in this model the

00:10:55,510 --> 00:11:00,430
all data points that have an identical

00:10:57,820 --> 00:11:03,160
metric name and labels format I'm

00:11:00,430 --> 00:11:05,400
serious and on top of this you have a

00:11:03,160 --> 00:11:09,310
powerful query language called prompt QL

00:11:05,400 --> 00:11:11,500
that allows for yeah easy queries and

00:11:09,310 --> 00:11:14,050
aggregation of metrics based on these

00:11:11,500 --> 00:11:17,410
labels now that's a bit theoretical so

00:11:14,050 --> 00:11:18,879
let's look at a very tiny example

00:11:17,410 --> 00:11:22,170
assuming you have some target to scrape

00:11:18,879 --> 00:11:24,610
for example an HTTP load balancer and

00:11:22,170 --> 00:11:26,440
assuming this load balancer were to

00:11:24,610 --> 00:11:28,240
expose the following metrics now this is

00:11:26,440 --> 00:11:30,699
the actual data format so this is

00:11:28,240 --> 00:11:33,399
something that could be actually scraped

00:11:30,699 --> 00:11:35,889
by a Prometheus server you see four

00:11:33,399 --> 00:11:38,949
metrics here each with the same metric

00:11:35,889 --> 00:11:41,829
name in white and they each have the

00:11:38,949 --> 00:11:44,230
back end in the code label however was a

00:11:41,829 --> 00:11:48,279
different permutation of values so they

00:11:44,230 --> 00:11:50,290
actually four distinct metrics if you

00:11:48,279 --> 00:11:52,509
scrape this small example it was a

00:11:50,290 --> 00:11:55,420
Prometheus server you could then run the

00:11:52,509 --> 00:11:57,790
query below on the server and what it

00:11:55,420 --> 00:12:00,069
would do is give you the sum of all the

00:11:57,790 --> 00:12:01,810
metrics with a given metric name and

00:12:00,069 --> 00:12:02,560
where the back-end label matches the

00:12:01,810 --> 00:12:04,600
value foo

00:12:02,560 --> 00:12:06,519
so in the example above would be the

00:12:04,600 --> 00:12:07,379
first two query at the first two metrics

00:12:06,519 --> 00:12:10,930
sorry

00:12:07,379 --> 00:12:12,550
you could also omit any labels in the

00:12:10,930 --> 00:12:14,680
query and it would give you the sum of

00:12:12,550 --> 00:12:16,689
all four metrics or you could use two

00:12:14,680 --> 00:12:18,639
labels or just the code label and the

00:12:16,689 --> 00:12:21,459
point is you can use the labels in any

00:12:18,639 --> 00:12:28,740
way you seem fit and it will be equally

00:12:21,459 --> 00:12:31,240
performant so ease of integration as

00:12:28,740 --> 00:12:33,189
we've just seen the data format is

00:12:31,240 --> 00:12:34,959
text-based so should you be in the

00:12:33,189 --> 00:12:38,259
situation to need to create some kind of

00:12:34,959 --> 00:12:41,740
script or other tools that create such

00:12:38,259 --> 00:12:43,689
data it is pretty easy to achieve from

00:12:41,740 --> 00:12:45,490
easiest scrapes I actually just HTTP

00:12:43,689 --> 00:12:48,720
requests so should you have to implement

00:12:45,490 --> 00:12:51,399
that it's also pretty straightforward

00:12:48,720 --> 00:12:53,319
much more importantly you rarely

00:12:51,399 --> 00:12:55,720
actually have to do this because many

00:12:53,319 --> 00:12:58,060
many integrations already exist and in

00:12:55,720 --> 00:13:00,250
fact that image on the right is actually

00:12:58,060 --> 00:13:02,500
a screenshot from the Prometheus website

00:13:00,250 --> 00:13:04,209
with the list of existing integrations

00:13:02,500 --> 00:13:06,180
so it's just to show you that there's

00:13:04,209 --> 00:13:08,230
quite a few already

00:13:06,180 --> 00:13:10,720
even if whatever you would like to

00:13:08,230 --> 00:13:12,610
monitor is not in that list the

00:13:10,720 --> 00:13:14,559
Prometheus project provides excellent

00:13:12,610 --> 00:13:16,660
tooling and libraries to create new ones

00:13:14,559 --> 00:13:22,149
and we'll also see later on just how

00:13:16,660 --> 00:13:25,300
easy that really is now the main part of

00:13:22,149 --> 00:13:28,290
this talk will be about the integrations

00:13:25,300 --> 00:13:30,820
that we used to migrate all our

00:13:28,290 --> 00:13:34,070
monitoring to Prometheus so

00:13:30,820 --> 00:13:36,950
let's look at some detail about how such

00:13:34,070 --> 00:13:39,470
an integration is usually designed in

00:13:36,950 --> 00:13:41,330
the simplest case you write an

00:13:39,470 --> 00:13:43,520
application and you use the existing

00:13:41,330 --> 00:13:45,649
libraries for example to instrument your

00:13:43,520 --> 00:13:48,080
application so that it can be scraped by

00:13:45,649 --> 00:13:50,570
Prometheus and returns the metrics

00:13:48,080 --> 00:13:51,649
you're interested in however this

00:13:50,570 --> 00:13:53,060
doesn't do anything for us in

00:13:51,649 --> 00:13:55,070
infrastructure because we don't want our

00:13:53,060 --> 00:13:58,279
money to applications we want a money to

00:13:55,070 --> 00:14:00,380
a service and network here luckily

00:13:58,279 --> 00:14:01,790
there's a second type of integration

00:14:00,380 --> 00:14:04,670
which is usually referred to as

00:14:01,790 --> 00:14:06,830
exporters these exporters are standalone

00:14:04,670 --> 00:14:08,899
applications that you run on some host

00:14:06,830 --> 00:14:12,010
that you're interested in that you want

00:14:08,899 --> 00:14:14,720
to monitor and this in exporters will

00:14:12,010 --> 00:14:17,300
for you open endpoint that promises can

00:14:14,720 --> 00:14:19,580
scrape and upon being scraped they

00:14:17,300 --> 00:14:21,740
collect the data they are designed to

00:14:19,580 --> 00:14:23,870
collect in this example I'm using the

00:14:21,740 --> 00:14:26,120
node exporter which exposes operating

00:14:23,870 --> 00:14:29,080
system level statistics like memory

00:14:26,120 --> 00:14:31,399
usage and CPU usage for unix-like

00:14:29,080 --> 00:14:35,450
systems so everything you could get out

00:14:31,399 --> 00:14:37,190
of proc for example now that's already

00:14:35,450 --> 00:14:39,770
pretty valuable of course for

00:14:37,190 --> 00:14:41,600
instruction monitoring but what about

00:14:39,770 --> 00:14:44,870
network gear that we can't run arbitrary

00:14:41,600 --> 00:14:48,290
applications on for this there's another

00:14:44,870 --> 00:14:51,200
type of exporter which upon being

00:14:48,290 --> 00:14:53,270
scraped we actually employ some other

00:14:51,200 --> 00:14:56,420
protocol that it was designed to employ

00:14:53,270 --> 00:14:58,580
to talk to your network here very common

00:14:56,420 --> 00:15:02,150
example is of course the SNMP exporter

00:14:58,580 --> 00:15:05,089
which when you scrape it it will request

00:15:02,150 --> 00:15:07,339
from a router all its current metrics

00:15:05,089 --> 00:15:09,320
via SNMP transform that into the

00:15:07,339 --> 00:15:11,050
Prometheus data model and return it to

00:15:09,320 --> 00:15:14,600
the server

00:15:11,050 --> 00:15:16,610
now for these exporters it's important

00:15:14,600 --> 00:15:19,070
to point out that you actually use the

00:15:16,610 --> 00:15:21,470
same exporter instance to monitor all

00:15:19,070 --> 00:15:23,600
your routers nevertheless there's still

00:15:21,470 --> 00:15:26,209
a one-to-one relationship of scrapes and

00:15:23,600 --> 00:15:27,920
targets so that means in this case your

00:15:26,209 --> 00:15:30,459
Prometheus server still knows this

00:15:27,920 --> 00:15:33,529
router a and this router B and when

00:15:30,459 --> 00:15:40,490
scraping it will tell the exporter which

00:15:33,529 --> 00:15:41,450
router it is intending to scrape so now

00:15:40,490 --> 00:15:43,100
that we've handle this

00:15:41,450 --> 00:15:44,959
there's two more important things for

00:15:43,100 --> 00:15:49,290
monitoring that we haven't covered

00:15:44,959 --> 00:15:51,390
one is alerting a lot built into

00:15:49,290 --> 00:15:54,029
Prometheus and can be defined in

00:15:51,390 --> 00:15:56,970
prometheus configuration however they

00:15:54,029 --> 00:15:59,160
don't do much just on their own for this

00:15:56,970 --> 00:16:01,230
there's a separate application that is

00:15:59,160 --> 00:16:04,829
however part of the official Prometheus

00:16:01,230 --> 00:16:06,570
project called a lot manager and it very

00:16:04,829 --> 00:16:09,000
much just what you'd expect of an

00:16:06,570 --> 00:16:11,880
application by that name it manages your

00:16:09,000 --> 00:16:13,980
alerts you can silence alerts and what's

00:16:11,880 --> 00:16:16,290
pretty fancy is you can do a lot

00:16:13,980 --> 00:16:19,320
grouping and routing based on the

00:16:16,290 --> 00:16:21,089
Prometheus labels which is very handy it

00:16:19,320 --> 00:16:23,970
also comes with a clustering mode built

00:16:21,089 --> 00:16:25,529
in so it offers high availability out of

00:16:23,970 --> 00:16:29,760
the box which is of course very

00:16:25,529 --> 00:16:32,240
important for alerting now just as

00:16:29,760 --> 00:16:34,709
important as alerting is of course

00:16:32,240 --> 00:16:37,230
dashboards and graphs so we haven't seen

00:16:34,709 --> 00:16:38,970
those yet and actually the Prometheus

00:16:37,230 --> 00:16:41,250
project is not concerned with them and

00:16:38,970 --> 00:16:43,680
the official recommendation is to use

00:16:41,250 --> 00:16:46,950
graph ahna for all your dashboarding and

00:16:43,680 --> 00:16:49,200
data visualization needs griffin has a

00:16:46,950 --> 00:16:51,990
totally separate open source project but

00:16:49,200 --> 00:16:53,610
an awesome one and it displays data from

00:16:51,990 --> 00:16:55,589
many many different data sources

00:16:53,610 --> 00:16:57,600
Prometheus just being one of them so

00:16:55,589 --> 00:16:59,250
even if you're not interested into

00:16:57,600 --> 00:17:01,860
Prometheus you should probably check out

00:16:59,250 --> 00:17:04,740
graph annum and what's so nice about it

00:17:01,860 --> 00:17:06,540
being so versatile is two things first

00:17:04,740 --> 00:17:08,699
of all if you attempt such a migration

00:17:06,540 --> 00:17:12,179
from one monitoring system to another

00:17:08,699 --> 00:17:14,970
you can easily have graphs from one

00:17:12,179 --> 00:17:17,040
source and from the old source and the

00:17:14,970 --> 00:17:19,470
new source on the same dashboard and you

00:17:17,040 --> 00:17:23,550
know make sure it all adds up and it's

00:17:19,470 --> 00:17:27,030
what you expect it to be and the other

00:17:23,550 --> 00:17:28,800
thing is that there's certain data you

00:17:27,030 --> 00:17:31,950
should never ever attempt to put into

00:17:28,800 --> 00:17:34,440
Prometheus like locks for example even

00:17:31,950 --> 00:17:37,260
though though I'm sure someone somewhere

00:17:34,440 --> 00:17:38,669
has already tried that for this you'd

00:17:37,260 --> 00:17:40,230
probably have something like your

00:17:38,669 --> 00:17:42,480
elasticsearch cluster or some other

00:17:40,230 --> 00:17:44,760
database and graph and I will happily

00:17:42,480 --> 00:17:46,050
display data from there as well so you

00:17:44,760 --> 00:17:48,059
can easily have a dashboard that

00:17:46,050 --> 00:17:50,220
displays Prometheus data side-by-side

00:17:48,059 --> 00:17:54,640
with for example relevant log snippets

00:17:50,220 --> 00:17:59,590
from the same time more stuff like this

00:17:54,640 --> 00:18:02,200
all right so now you have a rough idea

00:17:59,590 --> 00:18:04,900
of what we are monitoring and how

00:18:02,200 --> 00:18:08,650
Prometheus works so let's take a quick

00:18:04,900 --> 00:18:10,840
look at what all integrations we use and

00:18:08,650 --> 00:18:13,299
as I already mentioned partially created

00:18:10,840 --> 00:18:16,540
along the way when trying to get rid of

00:18:13,299 --> 00:18:18,070
all the other systems and for each

00:18:16,540 --> 00:18:22,960
integration I'll talk about I'll give

00:18:18,070 --> 00:18:25,870
you some a little tip or trick in case

00:18:22,960 --> 00:18:30,520
you might venture this way on your own

00:18:25,870 --> 00:18:32,950
that you might find useful so let's

00:18:30,520 --> 00:18:35,559
start simple I already mentioned the

00:18:32,950 --> 00:18:38,169
node exporter and that it exports

00:18:35,559 --> 00:18:39,600
operating system and partially I guess

00:18:38,169 --> 00:18:42,610
you could call it hardware level metrics

00:18:39,600 --> 00:18:43,690
for running systems so you have a server

00:18:42,610 --> 00:18:45,730
it's up and running

00:18:43,690 --> 00:18:47,860
you run the node exporter on it you can

00:18:45,730 --> 00:18:51,370
scrape it with Prometheus and you get a

00:18:47,860 --> 00:18:53,260
ton of data out of the box first node

00:18:51,370 --> 00:18:56,290
exporter mostly replaced ganglia and

00:18:53,260 --> 00:19:00,610
also some icinga checks some custom

00:18:56,290 --> 00:19:02,830
checks that we wrote and what's not

00:19:00,610 --> 00:19:06,030
worried about it is it comes with a ton

00:19:02,830 --> 00:19:08,230
of collectors built-in so different

00:19:06,030 --> 00:19:10,750
plugins if you will that collect

00:19:08,230 --> 00:19:12,400
different type of data and one should

00:19:10,750 --> 00:19:14,620
point out that only some of them are

00:19:12,400 --> 00:19:15,940
enabled by default so if you were going

00:19:14,620 --> 00:19:17,919
to use it you should read the

00:19:15,940 --> 00:19:19,630
documentation to make sure all the

00:19:17,919 --> 00:19:21,970
collectors you might be interested in

00:19:19,630 --> 00:19:24,010
I actually enabled theirs awesome stuff

00:19:21,970 --> 00:19:26,679
like for example extended TCP metrics

00:19:24,010 --> 00:19:29,770
that can be very helpful to debug things

00:19:26,679 --> 00:19:32,169
at times also I don't know much about it

00:19:29,770 --> 00:19:34,419
but I'm told if you want to monitor

00:19:32,169 --> 00:19:37,570
Windows service you should use the WMI

00:19:34,419 --> 00:19:43,090
export instead which is roughly the same

00:19:37,570 --> 00:19:46,570
thing just on Windows hosts so this

00:19:43,090 --> 00:19:48,760
brings me to pro tip number one the node

00:19:46,570 --> 00:19:50,470
exporter has one specific collector that

00:19:48,760 --> 00:19:54,040
is a bit special and it's called the

00:19:50,470 --> 00:19:56,620
text file collector and that one is as

00:19:54,040 --> 00:19:58,799
awesome as it is simple because all it

00:19:56,620 --> 00:20:00,820
does is it reads the text file or

00:19:58,799 --> 00:20:04,000
actually several text files if you

00:20:00,820 --> 00:20:06,880
wanted to in the data format that you've

00:20:04,000 --> 00:20:08,490
seen previously on the slides so the one

00:20:06,880 --> 00:20:11,580
that is actually pretty simple to create

00:20:08,490 --> 00:20:13,080
and it will take the metrics from the

00:20:11,580 --> 00:20:15,540
text file and just return them to the

00:20:13,080 --> 00:20:17,670
Prometheus server very simple idea but

00:20:15,540 --> 00:20:20,580
it makes for great integration point to

00:20:17,670 --> 00:20:22,530
add your custom metrics there's actually

00:20:20,580 --> 00:20:25,740
some example scripts even in the Nordic

00:20:22,530 --> 00:20:28,710
sport github repository that collect

00:20:25,740 --> 00:20:31,500
rate controller data for example or

00:20:28,710 --> 00:20:34,860
smartphone data and expose that to

00:20:31,500 --> 00:20:38,160
Prometheus we also use it for some cron

00:20:34,860 --> 00:20:40,560
job results and we also have a certain

00:20:38,160 --> 00:20:42,810
amount of our chef's data so from our

00:20:40,560 --> 00:20:46,620
configuration management system that we

00:20:42,810 --> 00:20:52,380
expose in Prometheus this way very

00:20:46,620 --> 00:20:55,680
simple to use but very powerful next

00:20:52,380 --> 00:20:58,020
exporter is the black box exporter this

00:20:55,680 --> 00:21:00,630
one is a bit special as in that you can

00:20:58,020 --> 00:21:03,180
tell the exporter to send a probe in a

00:21:00,630 --> 00:21:06,450
protocol of your liking to a random

00:21:03,180 --> 00:21:09,930
target the probe types supported or the

00:21:06,450 --> 00:21:14,670
protocol supported a DNS HTTP /s I

00:21:09,930 --> 00:21:16,620
simply ping and TCP connect and you can

00:21:14,670 --> 00:21:18,990
already see that for example the ICMP

00:21:16,620 --> 00:21:21,930
ping this is a prime candidate to

00:21:18,990 --> 00:21:24,870
replace smoke ping which we did and I

00:21:21,930 --> 00:21:27,780
talk about that in a second another

00:21:24,870 --> 00:21:31,470
thing that we found really noteworthy is

00:21:27,780 --> 00:21:34,800
that it also monitors for the HTTP probe

00:21:31,470 --> 00:21:37,170
type TLS certificate expiry so you can

00:21:34,800 --> 00:21:39,990
actually easily create alerts that will

00:21:37,170 --> 00:21:42,270
notify you when your SSL certificates

00:21:39,990 --> 00:21:48,440
are about to expire which is really

00:21:42,270 --> 00:21:51,090
handy so smoking replacement now if you

00:21:48,440 --> 00:21:55,230
if you look at your typical smoke being

00:21:51,090 --> 00:21:58,320
set up then smoke ping has some interval

00:21:55,230 --> 00:22:00,600
usually around 2 minutes and every 2

00:21:58,320 --> 00:22:04,290
minutes it'll send a number of pings say

00:22:00,600 --> 00:22:06,450
20 and it will then give you aggregate

00:22:04,290 --> 00:22:09,270
statistics for the timing and potential

00:22:06,450 --> 00:22:10,890
packet loss of these 20 pings now the

00:22:09,270 --> 00:22:12,900
blackbox exporter is designed

00:22:10,890 --> 00:22:17,040
differently upon each scrape it will

00:22:12,900 --> 00:22:20,070
only send a single probe now to kind of

00:22:17,040 --> 00:22:21,420
bend that into what we wanted to we just

00:22:20,070 --> 00:22:22,140
decided we're going to scrape it really

00:22:21,420 --> 00:22:23,520
really fast

00:22:22,140 --> 00:22:25,380
so we said a scrape interval of five

00:22:23,520 --> 00:22:28,170
seconds so we actually have one ping

00:22:25,380 --> 00:22:29,070
every five seconds and if you take that

00:22:28,170 --> 00:22:30,330
over two minutes

00:22:29,070 --> 00:22:32,370
you actually have roughly the same

00:22:30,330 --> 00:22:34,800
amount of data that we previously had

00:22:32,370 --> 00:22:36,270
been smoking and in fact we have even

00:22:34,800 --> 00:22:40,890
more data because we have the exact

00:22:36,270 --> 00:22:42,090
timing information for every probe so

00:22:40,890 --> 00:22:45,480
you can already see where this is going

00:22:42,090 --> 00:22:47,070
once you have that it is very easy to

00:22:45,480 --> 00:22:49,650
create alerts that alert you when the

00:22:47,070 --> 00:22:54,570
target is down or you encounter a

00:22:49,650 --> 00:22:56,640
certain amount of packet loss and then

00:22:54,570 --> 00:22:59,130
given the data you can use some of the

00:22:56,640 --> 00:23:01,410
aggregation functions that Prometheus

00:22:59,130 --> 00:23:04,080
offers and slap them in to grow fauna

00:23:01,410 --> 00:23:05,910
and you easily get a graph for example

00:23:04,080 --> 00:23:08,280
like this so the white one is an

00:23:05,910 --> 00:23:11,430
original smoke ping graph and the black

00:23:08,280 --> 00:23:14,400
one is a graph on a graph we created

00:23:11,430 --> 00:23:17,640
from our smoke ping replacement data and

00:23:14,400 --> 00:23:19,830
it doesn't look quite as smoky I guess

00:23:17,640 --> 00:23:22,170
but it nevertheless conveys the same

00:23:19,830 --> 00:23:23,760
data and you could actually you actually

00:23:22,170 --> 00:23:26,100
have even more precise data if you

00:23:23,760 --> 00:23:27,630
wanted to but yeah we don't look at the

00:23:26,100 --> 00:23:31,290
graph so much we're more interested in

00:23:27,630 --> 00:23:34,440
the alerts of course so there goes

00:23:31,290 --> 00:23:37,170
smoking and that brings me to pro tip

00:23:34,440 --> 00:23:39,360
number two if you think you might bring

00:23:37,170 --> 00:23:41,580
you any value scrape more and scrape

00:23:39,360 --> 00:23:44,940
faster because for Prometheus that is

00:23:41,580 --> 00:23:47,430
totally not a problem our server scrapes

00:23:44,940 --> 00:23:50,130
around 1 million metrics from over 5,000

00:23:47,430 --> 00:23:51,990
targets and the scrape interval is 10

00:23:50,130 --> 00:23:55,020
seconds for most of them as you've just

00:23:51,990 --> 00:23:57,300
seen some even 5 seconds and granted

00:23:55,020 --> 00:23:59,760
some a little longer and we have a data

00:23:57,300 --> 00:24:02,610
retention time of 50 days on that server

00:23:59,760 --> 00:24:07,620
and it only utilizes 250 gigabytes of

00:24:02,610 --> 00:24:09,240
disk space which is nothing so if you

00:24:07,620 --> 00:24:13,460
think more data might help you you

00:24:09,240 --> 00:24:13,460
should probably go ahead and just add in

00:24:14,240 --> 00:24:19,590
the next exporter the SNMP exporter also

00:24:18,270 --> 00:24:23,580
already mentioned that in one of the

00:24:19,590 --> 00:24:26,760
examples what it does is use SNMP to

00:24:23,580 --> 00:24:29,040
monitor all your network devices and for

00:24:26,760 --> 00:24:32,880
us that mostly replaced what we were

00:24:29,040 --> 00:24:34,980
using tactile form what's not worry

00:24:32,880 --> 00:24:35,940
about it is it's it's somewhat of a pain

00:24:34,980 --> 00:24:38,460
to configure

00:24:35,940 --> 00:24:40,710
but I have to point out this is not the

00:24:38,460 --> 00:24:43,500
exporters Ford of course but more as an

00:24:40,710 --> 00:24:45,120
MP's fault we're still trying to figure

00:24:43,500 --> 00:24:48,350
out some of the more vendor specific

00:24:45,120 --> 00:24:50,790
parts in our as an MP tree but

00:24:48,350 --> 00:24:54,000
nevertheless the exporter actually comes

00:24:50,790 --> 00:24:56,520
with some some tools to help you set up

00:24:54,000 --> 00:24:59,730
the Prometheus configuration so it does

00:24:56,520 --> 00:25:01,850
its best to mitigate that once you get

00:24:59,730 --> 00:25:06,180
through the process you actually get

00:25:01,850 --> 00:25:08,580
awesome data and awesome graphs almost

00:25:06,180 --> 00:25:10,680
right away it's very easy to create nice

00:25:08,580 --> 00:25:13,770
dashboards with graph on ax just some

00:25:10,680 --> 00:25:15,530
example I would say it's even more

00:25:13,770 --> 00:25:23,970
powerful than the graphs you get from

00:25:15,530 --> 00:25:25,320
cacti but there's a big but first cactus

00:25:23,970 --> 00:25:28,020
killer feature was certainly the weather

00:25:25,320 --> 00:25:31,290
map and the weather map is actually

00:25:28,020 --> 00:25:34,890
standalone open source project that just

00:25:31,290 --> 00:25:37,130
acts as a plugin to cacti however it has

00:25:34,890 --> 00:25:40,380
very powerful and very dense

00:25:37,130 --> 00:25:42,660
visualizations of your network so

00:25:40,380 --> 00:25:46,230
naturally we wanted to have something to

00:25:42,660 --> 00:25:49,490
replace this and as it turns out graph

00:25:46,230 --> 00:25:53,250
on ax actually has a diagram panel type

00:25:49,490 --> 00:25:55,460
that you know sort of does diagrams so

00:25:53,250 --> 00:25:59,100
we figured maybe we can give this a try

00:25:55,460 --> 00:26:03,150
however yeah we haven't quite gotten it

00:25:59,100 --> 00:26:05,400
to be what we want it to be yet you can

00:26:03,150 --> 00:26:06,810
see an example here it's it still

00:26:05,400 --> 00:26:09,090
contains a lot of information but it's

00:26:06,810 --> 00:26:11,370
absolutely we can't get it to be as

00:26:09,090 --> 00:26:15,660
dense as the weather map and also it's

00:26:11,370 --> 00:26:20,190
unidirectional so yeah it's not quite

00:26:15,660 --> 00:26:21,360
what we want yet so here goes if any of

00:26:20,190 --> 00:26:23,570
the weather map people are in this

00:26:21,360 --> 00:26:27,390
audience or ever see this on video maybe

00:26:23,570 --> 00:26:29,730
the weather map is already a program to

00:26:27,390 --> 00:26:31,440
support multiple data sources so if we

00:26:29,730 --> 00:26:33,210
could have a Prometheus data source for

00:26:31,440 --> 00:26:33,840
the weather map plugin that would be

00:26:33,210 --> 00:26:36,420
amazing

00:26:33,840 --> 00:26:37,740
and obviously we'd also send graph on an

00:26:36,420 --> 00:26:41,520
integration but I think that is actually

00:26:37,740 --> 00:26:43,710
the easier part so yeah if anyone wants

00:26:41,520 --> 00:26:45,540
to beef up their open source Karma I

00:26:43,710 --> 00:26:48,320
think this would be an amazing amazing

00:26:45,540 --> 00:26:48,320
project

00:26:48,880 --> 00:26:54,310
so next pro tip somewhat related to this

00:26:52,420 --> 00:26:56,520
exporter why not build a dedicated

00:26:54,310 --> 00:26:58,990
long-term from with your server

00:26:56,520 --> 00:27:01,150
long-term data is not a primary concern

00:26:58,990 --> 00:27:03,190
for Prometheus but there's very simply

00:27:01,150 --> 00:27:04,960
workaround actually you just build a

00:27:03,190 --> 00:27:06,760
separate server from the one you're

00:27:04,960 --> 00:27:09,250
using for your primary monitoring and

00:27:06,760 --> 00:27:10,600
you only scrape a very few selected

00:27:09,250 --> 00:27:13,090
metrics that you think might be

00:27:10,600 --> 00:27:14,740
interesting for long term and then it'll

00:27:13,090 --> 00:27:16,990
use very little disk space so you just

00:27:14,740 --> 00:27:19,840
yank the retention time way up things

00:27:16,990 --> 00:27:21,130
like two years for example if you do

00:27:19,840 --> 00:27:23,020
that make sure you do backups because

00:27:21,130 --> 00:27:27,550
otherwise if something happens you lose

00:27:23,020 --> 00:27:30,610
all your data but if you do and you have

00:27:27,550 --> 00:27:33,280
the data we think it'd be very very

00:27:30,610 --> 00:27:34,960
useful we're thinking in this case for

00:27:33,280 --> 00:27:36,880
example about the aggregate outgoing

00:27:34,960 --> 00:27:39,820
bandwidth from our data center or the

00:27:36,880 --> 00:27:42,190
bandwidth that we use peering with the

00:27:39,820 --> 00:27:44,140
cloud providers for example to figure

00:27:42,190 --> 00:27:46,510
out do we need to buy more links do we

00:27:44,140 --> 00:27:48,720
need to upgrade the links my kind of

00:27:46,510 --> 00:27:48,720
stuff

00:27:49,410 --> 00:27:53,830
the next exporter it's going to be the

00:27:52,180 --> 00:27:55,180
Collins exporter however want to make

00:27:53,830 --> 00:27:59,230
sure everyone knows what Collins

00:27:55,180 --> 00:28:01,480
actually is it calls itself an

00:27:59,230 --> 00:28:03,100
infrastructure management system it's I

00:28:01,480 --> 00:28:06,070
think it's more often referred to as I

00:28:03,100 --> 00:28:08,110
Pam and it is an open source project

00:28:06,070 --> 00:28:10,060
started by tumblr you can find it on

00:28:08,110 --> 00:28:12,670
github and what it gives you is kind of

00:28:10,060 --> 00:28:14,590
a server inventory and classification

00:28:12,670 --> 00:28:16,720
system for example you can say this

00:28:14,590 --> 00:28:20,530
server is an application server this

00:28:16,720 --> 00:28:23,260
server is a database server and you get

00:28:20,530 --> 00:28:25,570
a basic kind of lifecycle management so

00:28:23,260 --> 00:28:28,060
for example the service being used the

00:28:25,570 --> 00:28:30,460
server is not being used right now this

00:28:28,060 --> 00:28:34,260
server is broken and maintenance so

00:28:30,460 --> 00:28:36,250
that's what you get from Collins now

00:28:34,260 --> 00:28:38,710
whenever we wanted to know something

00:28:36,250 --> 00:28:41,920
about the data we had in Collins we

00:28:38,710 --> 00:28:44,080
usually add a bunch of scripts to

00:28:41,920 --> 00:28:46,630
retrieve it for us and we would run

00:28:44,080 --> 00:28:49,840
those occasionally and look at the data

00:28:46,630 --> 00:28:52,930
and figure out maybe we need to buy some

00:28:49,840 --> 00:28:56,800
new service you know but that wasn't

00:28:52,930 --> 00:28:58,810
actually working out too well so we

00:28:56,800 --> 00:29:01,240
naturally we figured we should get this

00:28:58,810 --> 00:29:02,620
data into prometheus and there wasn't a

00:29:01,240 --> 00:29:04,750
Collins exporter

00:29:02,620 --> 00:29:06,309
we wrote one and we actually just open

00:29:04,750 --> 00:29:12,040
sourced it so you can find it on our

00:29:06,309 --> 00:29:13,890
github orc and yeah as I mentioned for

00:29:12,040 --> 00:29:17,080
us it replaces a bunch of scripts and

00:29:13,890 --> 00:29:20,440
now we have this fancy dashboard which

00:29:17,080 --> 00:29:22,720
is maybe a bit overkill but you can see

00:29:20,440 --> 00:29:24,550
the amount of service we have broken

00:29:22,720 --> 00:29:28,030
down by class which ones are being used

00:29:24,550 --> 00:29:29,559
which ones are currently broken and the

00:29:28,030 --> 00:29:30,910
dashboard itself is a bit overkill but

00:29:29,559 --> 00:29:33,130
it's just more to illustrate the point

00:29:30,910 --> 00:29:36,270
the important part is that now we are

00:29:33,130 --> 00:29:39,429
systematically collecting this data and

00:29:36,270 --> 00:29:41,020
more importantly this for us is also

00:29:39,429 --> 00:29:43,000
another prime candidate for long term

00:29:41,020 --> 00:29:45,520
storage so just imagine you have this

00:29:43,000 --> 00:29:46,000
very exact data over the course of two

00:29:45,520 --> 00:29:48,550
years

00:29:46,000 --> 00:29:52,750
we're very helpful for capacity planning

00:29:48,550 --> 00:29:55,000
for your data center so this brings me

00:29:52,750 --> 00:29:57,309
to pro tip number four you should write

00:29:55,000 --> 00:30:00,490
your own integrations it's super easy

00:29:57,309 --> 00:30:04,510
the collins export is written in go it's

00:30:00,490 --> 00:30:07,450
a single source fire was only 264 lines

00:30:04,510 --> 00:30:10,210
that includes boilerplate whitespace and

00:30:07,450 --> 00:30:12,610
comments and everything which is of

00:30:10,210 --> 00:30:14,980
course due to their being really amazing

00:30:12,610 --> 00:30:16,660
libraries provided by the Prometheus

00:30:14,980 --> 00:30:19,540
project that make it very simple to

00:30:16,660 --> 00:30:21,610
write such an integration so if there's

00:30:19,540 --> 00:30:23,020
something that no integration exists for

00:30:21,610 --> 00:30:24,730
you should really consider investing

00:30:23,020 --> 00:30:27,400
just a tiny bit of effort to create one

00:30:24,730 --> 00:30:30,070
and if you do maybe even consider open

00:30:27,400 --> 00:30:35,860
source and open sourcing it to help

00:30:30,070 --> 00:30:39,160
others benefit next exporter IPMI

00:30:35,860 --> 00:30:40,870
exporter now they actually is already an

00:30:39,160 --> 00:30:44,200
IP my exporter listed on the premises

00:30:40,870 --> 00:30:47,620
web page however that one is one of the

00:30:44,200 --> 00:30:49,330
kind that is running on host and as such

00:30:47,620 --> 00:30:52,570
obviously only works when the host is up

00:30:49,330 --> 00:30:55,720
and running what we wanted is to have

00:30:52,570 --> 00:30:59,920
enough and access to the IP my metrics

00:30:55,720 --> 00:31:02,620
even for hosts that are turned off so we

00:30:59,920 --> 00:31:04,210
wrote yet another IP my exporter and we

00:31:02,620 --> 00:31:06,850
also just open source then so you can

00:31:04,210 --> 00:31:08,950
also find in our github orc and this one

00:31:06,850 --> 00:31:11,710
works much like there's an MP exporter

00:31:08,950 --> 00:31:15,190
where this is running on one host on our

00:31:11,710 --> 00:31:16,480
network and we use that host to scrape

00:31:15,190 --> 00:31:21,820
the baseboard management

00:31:16,480 --> 00:31:23,620
controllers of all the entire fleet and

00:31:21,820 --> 00:31:25,360
this obviously works regardless of

00:31:23,620 --> 00:31:29,860
whether the host is currently powered on

00:31:25,360 --> 00:31:32,980
or not the IP are exporters of course

00:31:29,860 --> 00:31:35,590
not very you know exciting because

00:31:32,980 --> 00:31:37,360
there's not much to graph a dashboard we

00:31:35,590 --> 00:31:39,160
mostly use it for alerting whenever any

00:31:37,360 --> 00:31:43,799
Hardware breaks like fans or power

00:31:39,160 --> 00:31:46,990
supplies so yeah but nevertheless

00:31:43,799 --> 00:31:50,500
there's another Pro type to be found

00:31:46,990 --> 00:31:53,440
here and that is prometheus is primarily

00:31:50,500 --> 00:31:55,480
or primarily only designed to handle

00:31:53,440 --> 00:31:57,460
numeric data however sometimes

00:31:55,480 --> 00:31:59,290
especially if you maybe write your own

00:31:57,460 --> 00:32:02,080
integrations you find yourself in a

00:31:59,290 --> 00:32:04,120
situation where you have certain pieces

00:32:02,080 --> 00:32:06,340
of information that just aren't numeric

00:32:04,120 --> 00:32:09,190
but you'd really really like to get them

00:32:06,340 --> 00:32:11,470
into Prometheus now luckily there's

00:32:09,190 --> 00:32:14,860
labels infirmities which are strings so

00:32:11,470 --> 00:32:17,470
there's certainly a way to do it however

00:32:14,860 --> 00:32:20,860
a word of caution you should always do

00:32:17,470 --> 00:32:23,080
this with great care so in case of the

00:32:20,860 --> 00:32:25,380
ipmi exporter for example we were very

00:32:23,080 --> 00:32:27,520
much interested in the firmware revision

00:32:25,380 --> 00:32:29,470
that is running on the base board

00:32:27,520 --> 00:32:33,040
management controllers so we can you

00:32:29,470 --> 00:32:34,809
know for upgrades and stuff so what we

00:32:33,040 --> 00:32:37,210
did and this is a common approach that

00:32:34,809 --> 00:32:38,860
some other exporters take as well we

00:32:37,210 --> 00:32:43,059
export the static time series

00:32:38,860 --> 00:32:45,250
so this metric IPMI BMC info will always

00:32:43,059 --> 00:32:48,250
have a value of 1 there's no information

00:32:45,250 --> 00:32:51,250
in that actual value we just use the

00:32:48,250 --> 00:32:53,710
labels to have access to the firmware

00:32:51,250 --> 00:32:57,010
revision and in this case the

00:32:53,710 --> 00:32:58,720
manufacturer ID when you do this be

00:32:57,010 --> 00:33:00,460
cautious because if the labels change

00:32:58,720 --> 00:33:03,040
too often this were very much confused

00:33:00,460 --> 00:33:05,559
Prometheus but for certain information

00:33:03,040 --> 00:33:07,750
if the the benefit outweighs the risks

00:33:05,559 --> 00:33:11,350
here this is something that is super

00:33:07,750 --> 00:33:14,290
handy a similar example this time from

00:33:11,350 --> 00:33:15,549
the collins exporter a very similar

00:33:14,290 --> 00:33:19,240
situation is when you have certain

00:33:15,549 --> 00:33:22,799
states and if you have states and you

00:33:19,240 --> 00:33:25,659
have to encode these as a number then

00:33:22,799 --> 00:33:28,870
yeah you often get there maybe we should

00:33:25,659 --> 00:33:30,130
use a magic number don't it might become

00:33:28,870 --> 00:33:34,920
very complicated

00:33:30,130 --> 00:33:38,380
another approach is using labels and

00:33:34,920 --> 00:33:40,390
Static time-series again and if you want

00:33:38,380 --> 00:33:42,640
to know more about the details you

00:33:40,390 --> 00:33:45,430
should read the readme of the collins

00:33:42,640 --> 00:33:47,650
exporter wear it nicely explains how

00:33:45,430 --> 00:33:49,630
these states are being represented and

00:33:47,650 --> 00:33:51,550
how it makes certain foods very easy

00:33:49,630 --> 00:33:53,440
because you can actually use the state

00:33:51,550 --> 00:33:57,610
name in labels which is much easier to

00:33:53,440 --> 00:33:59,680
remember than any magic number again

00:33:57,610 --> 00:34:01,680
where you benefit from this do it but do

00:33:59,680 --> 00:34:05,320
it with caution

00:34:01,680 --> 00:34:08,379
ok so now that we've gone through all

00:34:05,320 --> 00:34:09,820
this we all of a sudden have all this

00:34:08,379 --> 00:34:14,169
data that we previously had in

00:34:09,820 --> 00:34:16,149
completely isolated systems in one

00:34:14,169 --> 00:34:18,490
single system and so we can actually

00:34:16,149 --> 00:34:20,320
start merging the data from all these

00:34:18,490 --> 00:34:24,129
systems and this is where it kind of

00:34:20,320 --> 00:34:26,080
becomes fun and this also kind of

00:34:24,129 --> 00:34:27,370
becomes a little complex very easily so

00:34:26,080 --> 00:34:29,830
I have a somewhat contrived example

00:34:27,370 --> 00:34:31,960
that's just intended to tease you a

00:34:29,830 --> 00:34:35,050
little bit about what you could possibly

00:34:31,960 --> 00:34:38,649
do if you are in that situation where

00:34:35,050 --> 00:34:41,350
you just can watch all this data so the

00:34:38,649 --> 00:34:44,440
example I'll be using is here is imagine

00:34:41,350 --> 00:34:46,720
we want to find out all the servers of a

00:34:44,440 --> 00:34:48,639
certain type that are not running the

00:34:46,720 --> 00:34:52,990
latest revision of the base board

00:34:48,639 --> 00:34:55,210
management controller somewhere so

00:34:52,990 --> 00:34:57,580
what's not running the latest firmware

00:34:55,210 --> 00:35:00,550
revision this is very easy to figure out

00:34:57,580 --> 00:35:02,530
it's kind of the same example we just

00:35:00,550 --> 00:35:05,680
used previously we just query for all

00:35:02,530 --> 00:35:08,530
the hosts that don't run what we know to

00:35:05,680 --> 00:35:10,450
be the latest revision the result will

00:35:08,530 --> 00:35:12,160
actually be a whole list of things but

00:35:10,450 --> 00:35:15,490
I'm just including this one example here

00:35:12,160 --> 00:35:20,260
that we'll get back to in in every query

00:35:15,490 --> 00:35:22,360
basically on the other hand it is very

00:35:20,260 --> 00:35:24,640
easy to find out or service of a certain

00:35:22,360 --> 00:35:27,120
type I'm going to use the type f2 here

00:35:24,640 --> 00:35:31,060
as an example which is just some

00:35:27,120 --> 00:35:33,670
classification so again this query will

00:35:31,060 --> 00:35:38,200
return a whole list of things namely all

00:35:33,670 --> 00:35:40,270
app to service but again it'll include

00:35:38,200 --> 00:35:42,850
the one I'll be using as example here

00:35:40,270 --> 00:35:43,850
and you see it also because we also

00:35:42,850 --> 00:35:45,590
expose that

00:35:43,850 --> 00:35:47,600
in Collins it will contain a label

00:35:45,590 --> 00:35:50,810
called IP my address where it has the

00:35:47,600 --> 00:35:53,600
same value as the instance label from

00:35:50,810 --> 00:35:55,100
the IP my exporter now you can already

00:35:53,600 --> 00:35:57,650
see where this going we want to merge

00:35:55,100 --> 00:35:59,510
this so but to be able to do this we

00:35:57,650 --> 00:36:01,730
actually need to make sure both key and

00:35:59,510 --> 00:36:03,080
value of the label are the same so we're

00:36:01,730 --> 00:36:05,990
just going to use the label replace

00:36:03,080 --> 00:36:08,120
function in from here and replace the

00:36:05,990 --> 00:36:10,610
instance label in the IP my matrix with

00:36:08,120 --> 00:36:12,200
the ipmi address label and you end up

00:36:10,610 --> 00:36:13,700
with an IP my metric that all of a

00:36:12,200 --> 00:36:16,250
sudden not only is a firmware revision

00:36:13,700 --> 00:36:19,400
but also the IP my address this time in

00:36:16,250 --> 00:36:21,380
a label called IP my address so once you

00:36:19,400 --> 00:36:24,080
have that you can easily merge these two

00:36:21,380 --> 00:36:27,470
metrics using the grouping functionality

00:36:24,080 --> 00:36:30,260
in from QL and you'll end up with a

00:36:27,470 --> 00:36:31,610
result like this where you all of a

00:36:30,260 --> 00:36:34,280
sudden have a metric that doesn't have a

00:36:31,610 --> 00:36:35,990
name but has all the labels you're

00:36:34,280 --> 00:36:40,220
interested in namely the firmware is

00:36:35,990 --> 00:36:41,690
revision but also the type of server now

00:36:40,220 --> 00:36:43,160
again if you were to do this on your

00:36:41,690 --> 00:36:45,170
entire server inventory this would

00:36:43,160 --> 00:36:47,780
actually be a list now of all the

00:36:45,170 --> 00:36:50,600
servers that are of course up to and are

00:36:47,780 --> 00:36:52,370
not running the latest firmware now to

00:36:50,600 --> 00:36:54,890
keep going you could just group again

00:36:52,370 --> 00:36:57,050
for example on the current state because

00:36:54,890 --> 00:36:59,780
maybe you want to know how these servers

00:36:57,050 --> 00:37:02,600
actually being used at the moment and so

00:36:59,780 --> 00:37:05,240
for this very example it would all of a

00:37:02,600 --> 00:37:07,310
sudden add another label status and yes

00:37:05,240 --> 00:37:09,890
this service currently allocated so it's

00:37:07,310 --> 00:37:12,380
being used and you will keep going and

00:37:09,890 --> 00:37:15,320
going for example I mentioned earlier

00:37:12,380 --> 00:37:17,330
that we also have some of our chef data

00:37:15,320 --> 00:37:21,320
in from easiest so I could actually

00:37:17,330 --> 00:37:25,160
start looking at what chef roles is the

00:37:21,320 --> 00:37:28,910
server currently operating at this guy

00:37:25,160 --> 00:37:31,160
is a limit basically so even if this

00:37:28,910 --> 00:37:34,580
example may not seem entirely useful for

00:37:31,160 --> 00:37:36,680
you it's just kind of intended to get

00:37:34,580 --> 00:37:39,290
you thinking about what you maybe could

00:37:36,680 --> 00:37:42,170
do if you have all this data in a single

00:37:39,290 --> 00:37:44,750
data source all right

00:37:42,170 --> 00:37:47,240
so as promised I'm going to get back to

00:37:44,750 --> 00:37:48,800
our list of monitoring systems and as

00:37:47,240 --> 00:37:53,240
you can see we've actually gotten rid of

00:37:48,800 --> 00:37:55,460
quite a few of them I have a just a

00:37:53,240 --> 00:37:56,930
little cross across that see because

00:37:55,460 --> 00:37:57,480
we're not actually using it for

00:37:56,930 --> 00:38:00,330
instruction

00:37:57,480 --> 00:38:01,920
monitoring anymore however we as the

00:38:00,330 --> 00:38:04,290
infrastructure team have to keep it

00:38:01,920 --> 00:38:07,350
running for some very legacy services

00:38:04,290 --> 00:38:10,590
that we need to support so yeah kind of

00:38:07,350 --> 00:38:12,840
thing of it this being gone graphite and

00:38:10,590 --> 00:38:14,100
cloud watch our little special so

00:38:12,840 --> 00:38:16,619
they're probably going to say around for

00:38:14,100 --> 00:38:22,830
a while but I'll talk about that on the

00:38:16,619 --> 00:38:24,990
next slide everything else gone so to

00:38:22,830 --> 00:38:27,630
look at the breakdown that I had earlier

00:38:24,990 --> 00:38:29,790
this is now super simple and super

00:38:27,630 --> 00:38:31,170
consistent so we still have cloud watch

00:38:29,790 --> 00:38:34,440
because cloud watch is a little special

00:38:31,170 --> 00:38:36,060
in this list of monitoring systems but

00:38:34,440 --> 00:38:38,400
for everything else we just have from

00:38:36,060 --> 00:38:40,290
easiest for the data we dashboard

00:38:38,400 --> 00:38:42,540
everything in graph on a-- and all the

00:38:40,290 --> 00:38:47,119
alerting goes through a lot manager so

00:38:42,540 --> 00:38:49,950
super simple super consistent very nice

00:38:47,119 --> 00:38:53,160
so what's the deal with cloud watch here

00:38:49,950 --> 00:38:54,750
and also graphite there is actually a

00:38:53,160 --> 00:38:56,970
cloud watch exporter for prometheus

00:38:54,750 --> 00:38:58,619
however a cloud watch is a little

00:38:56,970 --> 00:39:01,470
special as in the lead only guarantees

00:38:58,619 --> 00:39:04,740
convergence of your metrics once they're

00:39:01,470 --> 00:39:06,000
15 minutes so it that is fundamentally

00:39:04,740 --> 00:39:08,460
incompatible with how Prometheus

00:39:06,000 --> 00:39:11,450
operates in Prometheus it scrapes it

00:39:08,460 --> 00:39:14,869
well you take that value it's read-only

00:39:11,450 --> 00:39:17,550
afterwards so you can't ever change this

00:39:14,869 --> 00:39:19,080
so the cloud watch exporter for

00:39:17,550 --> 00:39:21,150
communities actually does this curious

00:39:19,080 --> 00:39:24,320
thing where it'll only collect metrics

00:39:21,150 --> 00:39:29,250
that are already 15 minutes old which is

00:39:24,320 --> 00:39:30,810
not great you could argue okay but you

00:39:29,250 --> 00:39:32,970
could just use Grapher name because it

00:39:30,810 --> 00:39:35,070
has a cloud watch data source and look

00:39:32,970 --> 00:39:37,350
at everything in graph Anna at least we

00:39:35,070 --> 00:39:39,840
don't do that either because actually

00:39:37,350 --> 00:39:42,210
each query to cloud card watch incurs

00:39:39,840 --> 00:39:43,530
the cost so if you have dashboard

00:39:42,210 --> 00:39:45,690
monitors in your office I've been

00:39:43,530 --> 00:39:48,450
running they're constantly creating

00:39:45,690 --> 00:39:50,340
costs so what we do here is that will

00:39:48,450 --> 00:39:53,250
keep graphite around for graphite works

00:39:50,340 --> 00:39:55,440
a little more nicely with cloud watch so

00:39:53,250 --> 00:39:57,869
we stuff all our cloud watch data into

00:39:55,440 --> 00:40:02,600
graphite and then use that to show um

00:39:57,869 --> 00:40:06,570
dashboards that just as a side thing

00:40:02,600 --> 00:40:09,119
so let's come to a quick summary and

00:40:06,570 --> 00:40:11,290
outlook if you only in here for the

00:40:09,119 --> 00:40:15,510
summary you can wake up now

00:40:11,290 --> 00:40:18,040
is it working yes I would say it is

00:40:15,510 --> 00:40:21,070
totally was it worth it

00:40:18,040 --> 00:40:22,930
yes I would also say it very much is so

00:40:21,070 --> 00:40:24,850
the most interesting question is of

00:40:22,930 --> 00:40:28,060
course why was it worth it

00:40:24,850 --> 00:40:29,859
now as you've seen I said hopefully

00:40:28,060 --> 00:40:32,320
showing you there are many many many

00:40:29,859 --> 00:40:34,450
integrations available out of the box

00:40:32,320 --> 00:40:36,490
and even if the one you're looking for

00:40:34,450 --> 00:40:39,310
isn't there it's pretty easy to write

00:40:36,490 --> 00:40:42,130
one so the effort that was involved in

00:40:39,310 --> 00:40:45,340
this whole undertaking was actually

00:40:42,130 --> 00:40:46,780
given the scope rather little I would

00:40:45,340 --> 00:40:49,660
say I mean it was still a lot of effort

00:40:46,780 --> 00:40:53,170
but we expected things to be a little

00:40:49,660 --> 00:40:55,510
worse the good thing is that in the

00:40:53,170 --> 00:40:58,540
process both quality and quantity of our

00:40:55,510 --> 00:41:01,030
monitoring has increased significantly I

00:40:58,540 --> 00:41:03,820
would say we added new stuff things we

00:41:01,030 --> 00:41:05,320
hadn't monitored before we now have

00:41:03,820 --> 00:41:10,840
everything in a single data source we

00:41:05,320 --> 00:41:12,430
can correlate things it's very nice on

00:41:10,840 --> 00:41:14,140
top of that monitoring and alerting has

00:41:12,430 --> 00:41:15,970
become much more consistent so it's much

00:41:14,140 --> 00:41:18,760
easier for new hires to get acquainted

00:41:15,970 --> 00:41:20,650
with everything and I already mentioned

00:41:18,760 --> 00:41:23,500
that it's easy to merge the data sources

00:41:20,650 --> 00:41:25,540
and the probably most important part the

00:41:23,500 --> 00:41:27,609
most important point I would like to

00:41:25,540 --> 00:41:29,350
make about this and this may not be

00:41:27,609 --> 00:41:31,359
interesting for you is infrastructure

00:41:29,350 --> 00:41:33,700
people but if you look at it from an

00:41:31,359 --> 00:41:36,760
organization point of view of a company

00:41:33,700 --> 00:41:38,380
point of view all the points I made are

00:41:36,760 --> 00:41:40,450
actually true across the entire

00:41:38,380 --> 00:41:43,780
organization not just for us

00:41:40,450 --> 00:41:45,600
infrastructure folks so whenever someone

00:41:43,780 --> 00:41:47,980
who was for example previously an

00:41:45,600 --> 00:41:49,660
application developer joins an

00:41:47,980 --> 00:41:53,710
infrastructure team or maybe the other

00:41:49,660 --> 00:41:56,050
way around or whatnot they're facing the

00:41:53,710 --> 00:41:58,570
same stack for the entire monitoring

00:41:56,050 --> 00:42:00,160
spectrum and it's very easy for them to

00:41:58,570 --> 00:42:04,960
get started and figure out where to go

00:42:00,160 --> 00:42:06,670
things for example even if maybe the

00:42:04,960 --> 00:42:09,220
infrastructure people aren't around

00:42:06,670 --> 00:42:13,150
developers know exactly where to silence

00:42:09,220 --> 00:42:19,030
alert where to look for graphs yeah very

00:42:13,150 --> 00:42:20,530
powerful very helpful some outlook error

00:42:19,030 --> 00:42:22,720
you talked a bit about long-term storage

00:42:20,530 --> 00:42:24,050
and that it's not a primary concern for

00:42:22,720 --> 00:42:25,760
Prometheus

00:42:24,050 --> 00:42:27,500
however I also already gave you the

00:42:25,760 --> 00:42:30,520
simple workaround that is probably

00:42:27,500 --> 00:42:34,250
already good enough for most use cases

00:42:30,520 --> 00:42:35,990
however if you want to get more into the

00:42:34,250 --> 00:42:38,380
details there there's actually also a

00:42:35,990 --> 00:42:41,180
remote readwrite interface in Prometheus

00:42:38,380 --> 00:42:43,190
that you could use for example to write

00:42:41,180 --> 00:42:47,000
out historical data to other time

00:42:43,190 --> 00:42:49,250
serious databases for example and there

00:42:47,000 --> 00:42:53,210
are some new features in Prometheus 2.0

00:42:49,250 --> 00:42:55,880
that facilitate external solutions for

00:42:53,210 --> 00:42:58,070
such long-term storage and if you're

00:42:55,880 --> 00:42:59,150
interested in such things then you

00:42:58,070 --> 00:43:01,520
should probably check out the famous

00:42:59,150 --> 00:43:04,970
project an open source project that

00:43:01,520 --> 00:43:09,380
builds on these new features and allows

00:43:04,970 --> 00:43:12,680
for all kinds of things also possibly of

00:43:09,380 --> 00:43:14,120
interest there is an open matrix working

00:43:12,680 --> 00:43:18,970
group that is trying to establish a

00:43:14,120 --> 00:43:22,660
standard and this standard is very much

00:43:18,970 --> 00:43:26,380
influenced by if not even based on

00:43:22,660 --> 00:43:30,290
Prometheus or the Prometheus data format

00:43:26,380 --> 00:43:32,990
so if this were to become an actual

00:43:30,290 --> 00:43:36,260
standard of course you could be hoping

00:43:32,990 --> 00:43:38,630
to see even more adoption of Prometheus

00:43:36,260 --> 00:43:40,970
or in this case open metrics maybe

00:43:38,630 --> 00:43:45,590
hopefully maybe even network device

00:43:40,970 --> 00:43:47,540
vendors who knows one can dream if

00:43:45,590 --> 00:43:50,060
you're interested in short such things

00:43:47,540 --> 00:43:53,630
the working group is operating on github

00:43:50,060 --> 00:44:00,130
and everyone is welcome to participate

00:43:53,630 --> 00:44:04,700
so that actually brings me to the end

00:44:00,130 --> 00:44:07,270
probably pretty fast sorry about that

00:44:04,700 --> 00:44:07,270
thank you

00:44:11,850 --> 00:44:16,860
but I guess it means we have more time

00:44:14,350 --> 00:44:24,850
for questions should you have any

00:44:16,860 --> 00:44:27,720
absolutely so are there any questions hi

00:44:24,850 --> 00:44:30,940
so on the topic of standard

00:44:27,720 --> 00:44:34,480
proliferation how is open metrics

00:44:30,940 --> 00:44:36,430
relating to the metrics 2.0 initiative

00:44:34,480 --> 00:44:38,920
that the graphite people had a couple of

00:44:36,430 --> 00:44:42,430
years ago at least and it maybe died

00:44:38,920 --> 00:44:46,750
right now I don't know um I don't know

00:44:42,430 --> 00:44:48,550
to be honest I am NOT very involved with

00:44:46,750 --> 00:44:52,990
that working group I just felt it was

00:44:48,550 --> 00:44:55,960
necessary to point it out if I guess I

00:44:52,990 --> 00:44:57,850
mean first of all it obviously it was

00:44:55,960 --> 00:45:00,340
initiated out of the Prometheus

00:44:57,850 --> 00:45:03,850
ecosystem so I'm not even sure those

00:45:00,340 --> 00:45:06,520
people themselves are aware of this

00:45:03,850 --> 00:45:12,880
other working group yeah

00:45:06,520 --> 00:45:15,280
I do not know sorry I have a couple of

00:45:12,880 --> 00:45:17,980
questions the first one is how do you

00:45:15,280 --> 00:45:20,380
live exporters to the physical machines

00:45:17,980 --> 00:45:22,780
and I believe that you have some network

00:45:20,380 --> 00:45:24,670
devices so if you deliver some which

00:45:22,780 --> 00:45:28,000
borders to the natural devices also it

00:45:24,670 --> 00:45:30,610
would be interesting to listen to sorry

00:45:28,000 --> 00:45:34,480
could you repeat the last part oh yeah

00:45:30,610 --> 00:45:38,520
so if you deliver the exporters to the

00:45:34,480 --> 00:45:42,970
network devices how you deserve okay no

00:45:38,520 --> 00:45:44,620
yeah sorry we don't if so if I

00:45:42,970 --> 00:45:46,480
understand the question correctly we

00:45:44,620 --> 00:45:49,120
don't do that I'm sorry if I gave you

00:45:46,480 --> 00:45:52,630
the impression so how it works is the

00:45:49,120 --> 00:45:56,560
for example the SNMP exporter runs on a

00:45:52,630 --> 00:46:00,220
regular server in our network that we

00:45:56,560 --> 00:46:02,850
dedicate to running this exporter it

00:46:00,220 --> 00:46:06,070
just has special network network

00:46:02,850 --> 00:46:10,870
configuration that allows network access

00:46:06,070 --> 00:46:14,080
to the monitoring ports of the network

00:46:10,870 --> 00:46:16,720
devices so when Prometheus scrapes

00:46:14,080 --> 00:46:20,890
this exporter it will tell the exporter

00:46:16,720 --> 00:46:24,100
I now want the data from router X it

00:46:20,890 --> 00:46:27,310
will then initiate a network connection

00:46:24,100 --> 00:46:30,070
to router X get the data from that

00:46:27,310 --> 00:46:32,290
router transform it into the Prometheus

00:46:30,070 --> 00:46:34,000
data model and return it to the server

00:46:32,290 --> 00:46:36,580
so there's no additional software

00:46:34,000 --> 00:46:39,070
running on the routers we just use the

00:46:36,580 --> 00:46:41,920
existing interfaces of the network

00:46:39,070 --> 00:46:45,370
devices how do you deliver in node

00:46:41,920 --> 00:46:49,570
exporter actually to the normal servers

00:46:45,370 --> 00:46:51,580
I mean is it docker container or do you

00:46:49,570 --> 00:46:55,240
use some kind of pre-built dated in our

00:46:51,580 --> 00:46:57,040
packages in the system all yeah we just

00:46:55,240 --> 00:46:58,740
installed the binary and this is all

00:46:57,040 --> 00:47:06,330
done by our configuration management

00:46:58,740 --> 00:47:06,330
okay thanks more questions

00:47:08,910 --> 00:47:13,690
so you mentioned at the beginning that

00:47:11,350 --> 00:47:16,810
there are timestamps when I'm running in

00:47:13,690 --> 00:47:19,120
a hi available setup to both instances

00:47:16,810 --> 00:47:23,590
sink that timestamps or do the recent

00:47:19,120 --> 00:47:24,610
gaps when one of the nodes crashed since

00:47:23,590 --> 00:47:27,190
our monitoring data

00:47:24,610 --> 00:47:30,450
you mean if I every London service

00:47:27,190 --> 00:47:33,370
monitoring the same thing no they will

00:47:30,450 --> 00:47:34,720
they will have slightly different views

00:47:33,370 --> 00:47:39,550
I mean you scrape in ten second

00:47:34,720 --> 00:47:41,440
intervals on both of them and I mean

00:47:39,550 --> 00:47:43,540
statistically it's very unlikely that

00:47:41,440 --> 00:47:46,290
they scribe at the same time so most

00:47:43,540 --> 00:47:50,380
likely they just have slightly different

00:47:46,290 --> 00:47:52,840
data points if you wanted to you could

00:47:50,380 --> 00:47:56,860
use the Federation feature instead which

00:47:52,840 --> 00:47:58,690
would actually which would include

00:47:56,860 --> 00:48:04,150
linking the timestamp to the other

00:47:58,690 --> 00:48:05,380
server but we don't do that I mean not

00:48:04,150 --> 00:48:10,900
sure if you would see any well you're

00:48:05,380 --> 00:48:13,440
not but you could do it any other

00:48:10,900 --> 00:48:13,440
question

00:48:15,320 --> 00:48:22,820
okay so thank you for now again Thank

00:48:19,650 --> 00:48:22,820
You Conrad thank you

00:48:24,460 --> 00:48:45,150

YouTube URL: https://www.youtube.com/watch?v=CZ8bTA9Lemg


