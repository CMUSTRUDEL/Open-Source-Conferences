Title: OSDC 2018 | Monitoring Kubernetes at Scale by Monica Sarbu
Publication date: 2018-06-22
Playlist: OSDC 2018 | Open Source Data Center Conference
Description: 
	Kubernetes is changing the game in the data centre, but also in the monitoring and troubleshooting landscape. Static tools and vertically scalable TSDBs are no longer fit for the job. Large-scale dynamic infrastructures require scalable dynamic monitoring.
This talk presents how the Elastic Stack collects logs, metrics, and APM traces from the applications running in Kubernetes:
   – Collect application logs, metrics and enhance them with Kubernetes metadata
   – Collect application metrics from Prometheus endpoints
   – Collect Kubernetes metrics
   – Collect application performance traces (APM)
   – Autodiscover new pods and monitor them based on their type
   – Control the monitoring via Kubernetes annotations
   – Use Kibana as a single looking glass to visualize the collected data

NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de
Blog: http://blog.netways.de
Webinare: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Google+: https://plus.google.com/+netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh

https://www.frametraxx.de/
Captions: 
	00:00:00,540 --> 00:00:03,420
[Music]

00:00:01,330 --> 00:00:03,420
you

00:00:11,750 --> 00:00:16,139
[Music]

00:00:13,279 --> 00:00:18,570
so I'm Monica Szabo and I'm a proud

00:00:16,139 --> 00:00:22,710
software engineer and working and

00:00:18,570 --> 00:00:25,259
elastic currently I'm leading a team

00:00:22,710 --> 00:00:30,800
that is responsible for for beating

00:00:25,259 --> 00:00:33,000
locks - elastic is the company behind

00:00:30,800 --> 00:00:35,340
elastics open-source projects like

00:00:33,000 --> 00:00:38,280
elastic search keep on our log stash and

00:00:35,340 --> 00:00:41,400
beat and today I'm going to talk about

00:00:38,280 --> 00:00:43,350
how to monitor kubernetes using the

00:00:41,400 --> 00:00:46,320
elastic stack but first let's see what

00:00:43,350 --> 00:00:49,380
the elastic stack is so elastic stack

00:00:46,320 --> 00:00:52,710
consists of this of open-source tools

00:00:49,380 --> 00:00:56,520
like beats elastic search cabana and log

00:00:52,710 --> 00:00:58,829
stash these are lightweight shippers

00:00:56,520 --> 00:01:01,079
they are written in go and if you stole

00:00:58,829 --> 00:01:03,059
them as an agent on your server they

00:01:01,079 --> 00:01:05,280
collect all kinds of operational data

00:01:03,059 --> 00:01:08,340
and then insert them to elastic search

00:01:05,280 --> 00:01:11,219
and then you can use Cubana on top of

00:01:08,340 --> 00:01:13,439
elastic search to visualize the data you

00:01:11,219 --> 00:01:15,959
can also put a log stash in the picture

00:01:13,439 --> 00:01:19,319
if you want to enhance your data that

00:01:15,959 --> 00:01:27,029
you collected with data from external

00:01:19,319 --> 00:01:30,060
databases for example so--but to use a

00:01:27,029 --> 00:01:35,279
plural because it's consists of a series

00:01:30,060 --> 00:01:37,979
of products we have a bit for each to

00:01:35,279 --> 00:01:40,229
solve a particular use case we start

00:01:37,979 --> 00:01:42,450
with back a bit to collect a network

00:01:40,229 --> 00:01:44,429
data and then we have file which to

00:01:42,450 --> 00:01:47,219
collect the locks from your servers and

00:01:44,429 --> 00:01:49,109
then we have met repeat that collects

00:01:47,219 --> 00:01:51,450
the metrics and the way it does it it

00:01:49,109 --> 00:01:53,819
interrogate periodically external

00:01:51,450 --> 00:01:56,429
services fetches metrics from them and

00:01:53,819 --> 00:01:59,099
then insert them to elastic search for

00:01:56,429 --> 00:02:01,979
Windows lovers we have in window feeds

00:01:59,099 --> 00:02:04,529
that collect the windows event logs we

00:02:01,979 --> 00:02:06,810
have heartbeat for uptime monitoring and

00:02:04,529 --> 00:02:08,099
the way it works is that it sends

00:02:06,810 --> 00:02:11,130
periodically pings

00:02:08,099 --> 00:02:14,640
to servers to see if they're up or down

00:02:11,130 --> 00:02:17,340
and also can generate HTTP requests to

00:02:14,640 --> 00:02:20,370
services to see if services up and

00:02:17,340 --> 00:02:22,980
running and the newest addition to the

00:02:20,370 --> 00:02:26,370
bid family is audit with

00:02:22,980 --> 00:02:30,599
which is an alternative for oddity but

00:02:26,370 --> 00:02:35,010
is built-in it's better integrated with

00:02:30,599 --> 00:02:38,040
the elastic stack and this one is for

00:02:35,010 --> 00:02:41,720
security use cases all of them are built

00:02:38,040 --> 00:02:45,769
on the bit platform as we call it lib it

00:02:41,720 --> 00:02:49,560
so in the last three years Pizza

00:02:45,769 --> 00:02:52,080
experienced that a steep growth there

00:02:49,560 --> 00:02:56,250
were more than 50 million downloads

00:02:52,080 --> 00:02:59,730
cumulative downloads and in the last

00:02:56,250 --> 00:03:06,590
year in beach we introduced a lot of

00:02:59,730 --> 00:03:10,160
features to make it easier to monitor

00:03:06,590 --> 00:03:10,160
kubernetes with them

00:03:10,260 --> 00:03:16,079
so I would like to take to use this

00:03:13,470 --> 00:03:18,420
presentation to tell you about what

00:03:16,079 --> 00:03:22,319
features we introduced in piece to make

00:03:18,420 --> 00:03:24,560
it easier to monitor kubernetes before

00:03:22,319 --> 00:03:28,430
that I would like to take a moment and

00:03:24,560 --> 00:03:31,560
go through all the challenges that

00:03:28,430 --> 00:03:34,650
assist the monitoring system has when it

00:03:31,560 --> 00:03:36,739
has to monitor kubernetes and the first

00:03:34,650 --> 00:03:40,230
one and the most important one is that

00:03:36,739 --> 00:03:43,260
in in kubernetes everything is a moving

00:03:40,230 --> 00:03:45,540
target so because it's easier to create

00:03:43,260 --> 00:03:48,239
and delete pods

00:03:45,540 --> 00:03:50,790
whenever you need it so it doesn't make

00:03:48,239 --> 00:03:53,069
sense for the monitoring solution to

00:03:50,790 --> 00:03:55,200
have a static of figuration so the

00:03:53,069 --> 00:03:58,680
configuration needs to be dynamic needs

00:03:55,200 --> 00:04:00,720
to be able to adjust the configuration

00:03:58,680 --> 00:04:04,650
every time something happens in your

00:04:00,720 --> 00:04:09,750
infrastructure if a container is it

00:04:04,650 --> 00:04:13,260
started or or or deleted because it's

00:04:09,750 --> 00:04:15,690
such a dynamic infrastructure it makes

00:04:13,260 --> 00:04:19,289
sense to have to offer a high-level

00:04:15,690 --> 00:04:22,940
overview of of your infrastructure so

00:04:19,289 --> 00:04:25,080
you can easily detect when something is

00:04:22,940 --> 00:04:27,690
something is wrong in your

00:04:25,080 --> 00:04:30,870
infrastructure and then you can also

00:04:27,690 --> 00:04:33,419
have the it will be nice also I have the

00:04:30,870 --> 00:04:36,120
possibility to drill down and see more

00:04:33,419 --> 00:04:42,780
details about where

00:04:36,120 --> 00:04:46,560
where the problem was caused another

00:04:42,780 --> 00:04:50,010
problem is a scalability so income in

00:04:46,560 --> 00:04:52,860
kubernetes is common to run thousands of

00:04:50,010 --> 00:04:56,280
services and when you scale it the

00:04:52,860 --> 00:04:58,710
number of services grow so you need to

00:04:56,280 --> 00:05:01,110
have a monitoring solution that Hitler's

00:04:58,710 --> 00:05:10,410
are able to scale as well together with

00:05:01,110 --> 00:05:16,800
your infrastructure so if you if you

00:05:10,410 --> 00:05:19,080
want to convince developers to to deploy

00:05:16,800 --> 00:05:21,750
their application into into your

00:05:19,080 --> 00:05:23,970
kubernetes infrastructure you need to

00:05:21,750 --> 00:05:26,130
provide them with a set of tools to make

00:05:23,970 --> 00:05:28,410
it easier for them to monitor the

00:05:26,130 --> 00:05:32,010
applications and probably you need to

00:05:28,410 --> 00:05:34,289
collect as much data as possible about

00:05:32,010 --> 00:05:37,500
that application so you will end up

00:05:34,289 --> 00:05:43,860
collecting lakhs metrics as well as

00:05:37,500 --> 00:05:46,500
traces now let's see how you can monitor

00:05:43,860 --> 00:05:50,039
kubernetes infrastructure with Pete and

00:05:46,500 --> 00:05:55,680
then we also tell you how we solve these

00:05:50,039 --> 00:05:59,760
challenges in the pits so first you want

00:05:55,680 --> 00:06:02,460
to monitor kubernetes itself and for

00:05:59,760 --> 00:06:05,490
that you need to you can use the

00:06:02,460 --> 00:06:08,280
kubernetes module in metric bit and you

00:06:05,490 --> 00:06:10,349
can get nan metrics from Q ballot you

00:06:08,280 --> 00:06:13,919
can get state matrix from cube state

00:06:10,349 --> 00:06:17,280
state matrix kubernetes events and other

00:06:13,919 --> 00:06:21,360
metrics from kubernetes api so I won't

00:06:17,280 --> 00:06:23,310
go into details you write another but

00:06:21,360 --> 00:06:26,190
this is not enough right you need to

00:06:23,310 --> 00:06:28,139
also monitor the applications and the

00:06:26,190 --> 00:06:31,800
services that are running inside Cooper

00:06:28,139 --> 00:06:34,560
netis so you want to get the logs the

00:06:31,800 --> 00:06:36,990
metrics and the traces from your

00:06:34,560 --> 00:06:39,360
application and services and then push

00:06:36,990 --> 00:06:41,789
them into elasticsearch and the big

00:06:39,360 --> 00:06:45,000
advantage of having all these free into

00:06:41,789 --> 00:06:47,370
the same same central point is that you

00:06:45,000 --> 00:06:49,380
are able to correlate it for example

00:06:47,370 --> 00:06:52,590
you'll be able to see

00:06:49,380 --> 00:06:55,530
for a port or for a particular container

00:06:52,590 --> 00:06:59,040
or for a particular namespace you can

00:06:55,530 --> 00:07:01,200
see an overview of your service you can

00:06:59,040 --> 00:07:05,160
see the locks that are coming from that

00:07:01,200 --> 00:07:11,580
service or or you can also see the

00:07:05,160 --> 00:07:14,060
traces so first let's start with how you

00:07:11,580 --> 00:07:17,250
can collect the locks from your service

00:07:14,060 --> 00:07:20,520
so we have this docker input in valve it

00:07:17,250 --> 00:07:23,880
that understands what the format of the

00:07:20,520 --> 00:07:27,720
locks that are returned by docker if you

00:07:23,880 --> 00:07:30,900
are using the JSON driver and what it

00:07:27,720 --> 00:07:33,300
does is that it gets it tries to get the

00:07:30,900 --> 00:07:35,910
locks of the container from from the

00:07:33,300 --> 00:07:40,770
Depot default path and then also is

00:07:35,910 --> 00:07:43,920
trying to parse a bit the the lock

00:07:40,770 --> 00:07:45,690
message and extract the timestamp so

00:07:43,920 --> 00:07:48,300
this time said that you extracted from

00:07:45,690 --> 00:07:51,270
the locks is basically the time when the

00:07:48,300 --> 00:07:54,360
lock was created so you can push it or

00:07:51,270 --> 00:07:57,360
you can have an elastic search this time

00:07:54,360 --> 00:08:01,670
when the lock was created and not the

00:07:57,360 --> 00:08:04,470
time when the lock was read by by 5 bit

00:08:01,670 --> 00:08:09,660
which is very important right we have

00:08:04,470 --> 00:08:12,750
the same the right time in such a

00:08:09,660 --> 00:08:15,150
dynamic infrastructure is important to

00:08:12,750 --> 00:08:17,850
know from where the law these locks are

00:08:15,150 --> 00:08:22,200
coming from and for that we can make use

00:08:17,850 --> 00:08:25,710
of the metadata and in in this we have a

00:08:22,200 --> 00:08:28,500
series of metadata processors we have

00:08:25,710 --> 00:08:31,290
add cloud metadata that basically

00:08:28,500 --> 00:08:34,110
enriches all the events that are

00:08:31,290 --> 00:08:39,450
collectively beats with cloud metadata

00:08:34,110 --> 00:08:40,890
like availability zone region and

00:08:39,450 --> 00:08:44,160
provider and so on

00:08:40,890 --> 00:08:47,040
a docker metadata that enriches all the

00:08:44,160 --> 00:08:49,590
data that are coming from beach locks

00:08:47,040 --> 00:08:52,500
metrics and traces and reaches them with

00:08:49,590 --> 00:08:55,710
docker metadata like the docker ID'd

00:08:52,500 --> 00:08:58,740
labels and the name of the container and

00:08:55,710 --> 00:09:02,790
so on and also at kubernetes metadata

00:08:58,740 --> 00:09:06,000
that enriches the events are coming

00:09:02,790 --> 00:09:08,610
beats with kubernetes method a not like

00:09:06,000 --> 00:09:11,640
the port the name of the port the

00:09:08,610 --> 00:09:15,690
namespace the labels annotations and so

00:09:11,640 --> 00:09:19,050
on so you can enable one or multiple

00:09:15,690 --> 00:09:22,830
ones at a at a time and here is an

00:09:19,050 --> 00:09:28,080
example where you enable ad cloud

00:09:22,830 --> 00:09:30,480
metadata and add kubernetes metadata so

00:09:28,080 --> 00:09:33,390
as you can see here in this event that

00:09:30,480 --> 00:09:37,740
is generated by by the bit you have the

00:09:33,390 --> 00:09:39,960
section on top you have the panetta

00:09:37,740 --> 00:09:42,570
section where you have the name of the

00:09:39,960 --> 00:09:45,480
pod the the name of the container the

00:09:42,570 --> 00:09:48,420
namespace and the labels and also for

00:09:45,480 --> 00:09:51,450
for the cloud metadata you have the name

00:09:48,420 --> 00:09:54,680
of the provider in this case is Google

00:09:51,450 --> 00:10:02,460
Cloud and then the instance name and

00:09:54,680 --> 00:10:04,530
availability zone and so on after you

00:10:02,460 --> 00:10:06,450
collect all the locks you also have to

00:10:04,530 --> 00:10:09,450
understand the logs you need to parse

00:10:06,450 --> 00:10:12,660
the logs and for that we have introduced

00:10:09,450 --> 00:10:15,740
file bit modules that basically are

00:10:12,660 --> 00:10:20,750
prepackaged configuration that contains

00:10:15,740 --> 00:10:23,910
configuration file for file bit but also

00:10:20,750 --> 00:10:26,460
pipeline configuration that you can use

00:10:23,910 --> 00:10:28,950
to parse locks in ingest not in

00:10:26,460 --> 00:10:31,560
elasticsearch they also contain

00:10:28,950 --> 00:10:35,150
dashboards that you can use to visualize

00:10:31,560 --> 00:10:38,970
the data that you collect and also ml

00:10:35,150 --> 00:10:41,580
configurations so basically they contain

00:10:38,970 --> 00:10:43,620
everything everything you need to have

00:10:41,580 --> 00:10:46,380
an out-of-the-box experience for

00:10:43,620 --> 00:10:51,150
collecting the logs from a particular

00:10:46,380 --> 00:10:53,220
service and we have support for we edit

00:10:51,150 --> 00:10:55,800
the file with modules for the most

00:10:53,220 --> 00:10:58,980
common services out there like Apache

00:10:55,800 --> 00:11:04,890
and Jenny's cough cup was released we

00:10:58,980 --> 00:11:08,390
already and so on similar we can collect

00:11:04,890 --> 00:11:11,340
the metrics from services so with that

00:11:08,390 --> 00:11:14,220
you can use metric with modules that are

00:11:11,340 --> 00:11:15,860
similar with farming by modules but the

00:11:14,220 --> 00:11:18,829
difference is that the module

00:11:15,860 --> 00:11:20,899
sorry are interrogating external

00:11:18,829 --> 00:11:25,010
services and fetches metric from them

00:11:20,899 --> 00:11:28,730
and then they also contain dashboard ml

00:11:25,010 --> 00:11:31,070
configuration and so on so we have

00:11:28,730 --> 00:11:34,670
metric new modules for the most common

00:11:31,070 --> 00:11:38,149
services for example web servers Apache

00:11:34,670 --> 00:11:43,029
and genex cues like Kafka a distributing

00:11:38,149 --> 00:11:46,490
queue data source stores like my sequel

00:11:43,029 --> 00:11:51,260
MongoDB graphite and so on custom

00:11:46,490 --> 00:11:59,209
application like J max Jolokia memcache

00:11:51,260 --> 00:12:03,320
the HR proceeds keeper and so on the

00:11:59,209 --> 00:12:06,410
first data type is our traces so

00:12:03,320 --> 00:12:08,390
basically this is when you instrument

00:12:06,410 --> 00:12:12,890
your application and this is done by

00:12:08,390 --> 00:12:15,950
elastic APM in elastic and previously

00:12:12,890 --> 00:12:19,550
was called op it was a company that was

00:12:15,950 --> 00:12:22,810
acquired by elastic last year and they

00:12:19,550 --> 00:12:26,959
created this APM server that basically

00:12:22,810 --> 00:12:30,440
collects traces from all the agents it's

00:12:26,959 --> 00:12:33,170
based on the beach platform so that we

00:12:30,440 --> 00:12:36,260
call it lipid so it it comes with

00:12:33,170 --> 00:12:38,630
benefits from this metadata that we

00:12:36,260 --> 00:12:42,320
collect so basically you could use at

00:12:38,630 --> 00:12:45,589
kubernetes metadata to enrich your

00:12:42,320 --> 00:12:48,410
traces with the poor name with all the

00:12:45,589 --> 00:12:51,709
Cooper native metallic metadata so you

00:12:48,410 --> 00:12:56,300
can identify from which container or

00:12:51,709 --> 00:13:01,120
from which namespace this this trace is

00:12:56,300 --> 00:13:06,350
coming from they have a few agents

00:13:01,120 --> 00:13:08,870
Python nodejs ruby Ram Java which has an

00:13:06,350 --> 00:13:11,390
alpha version and we are working on a go

00:13:08,870 --> 00:13:16,209
version and the most important for most

00:13:11,390 --> 00:13:16,209
of you is that it's Apache 2 license

00:13:19,800 --> 00:13:25,090
so if you are instrumenting your

00:13:22,180 --> 00:13:27,850
application from materials then you can

00:13:25,090 --> 00:13:31,210
also use the primitives module in metric

00:13:27,850 --> 00:13:33,220
bid to pull the the metrics from your

00:13:31,210 --> 00:13:39,310
application and then pull them to

00:13:33,220 --> 00:13:42,430
elastic search now let's talk a bit

00:13:39,310 --> 00:13:46,020
about how to deploy file bitumen to be

00:13:42,430 --> 00:13:51,220
in order to monitor your kubernetes and

00:13:46,020 --> 00:13:53,590
so you can use you can deploy metric bit

00:13:51,220 --> 00:13:56,470
and and file bit as daemon sets that

00:13:53,590 --> 00:14:01,650
make sure you have a single file bit and

00:13:56,470 --> 00:14:04,390
a single metric bit run on each node and

00:14:01,650 --> 00:14:08,560
from my point of view this is a scalable

00:14:04,390 --> 00:14:13,030
solution because it pulls locally the

00:14:08,560 --> 00:14:14,890
data to Manik that monitors all the all

00:14:13,030 --> 00:14:18,520
the posts that are running on that node

00:14:14,890 --> 00:14:22,660
so when you when you want to add more

00:14:18,520 --> 00:14:25,330
nodes then you basically need to install

00:14:22,660 --> 00:14:29,430
another demo set with five bit and much

00:14:25,330 --> 00:14:29,430
of it and then that's it right

00:14:30,210 --> 00:14:35,770
we provide manifest file sample manifest

00:14:33,730 --> 00:14:39,960
files to deploy file bit and metric bit

00:14:35,770 --> 00:14:43,630
as demo set and here is an example of

00:14:39,960 --> 00:14:50,080
file bit my manifest file that you can

00:14:43,630 --> 00:14:53,580
download and then change it to two for

00:14:50,080 --> 00:14:57,070
example you can change the elasticsearch

00:14:53,580 --> 00:14:59,340
URL and things like this and then you

00:14:57,070 --> 00:15:03,100
can use the cubes in here create command

00:14:59,340 --> 00:15:11,220
with the manifest file to create a file

00:15:03,100 --> 00:15:14,260
pretty much set like I said previously

00:15:11,220 --> 00:15:14,560
so all these manifest files that I show

00:15:14,260 --> 00:15:18,700
you

00:15:14,560 --> 00:15:20,500
they contain the pit configuration in

00:15:18,700 --> 00:15:23,410
that in the previous case file build

00:15:20,500 --> 00:15:28,990
configuration so and I and I mentioned

00:15:23,410 --> 00:15:32,830
earlier in my presentation that that it

00:15:28,990 --> 00:15:35,050
makes sense to have the bit have but I

00:15:32,830 --> 00:15:41,130
in configuration so let's see how we

00:15:35,050 --> 00:15:41,130
solve this in how we solve this in bits

00:15:42,810 --> 00:15:47,890
so we have introduced this out of this

00:15:45,550 --> 00:15:50,770
novel feature and the way it works is

00:15:47,890 --> 00:15:53,560
that it watches for kubernetes events

00:15:50,770 --> 00:15:56,590
and when a new container is started in

00:15:53,560 --> 00:16:01,320
this case in this example that has a

00:15:56,590 --> 00:16:05,980
kubernetes namespace coupe system then

00:16:01,320 --> 00:16:10,000
it starts it enables the docker docker

00:16:05,980 --> 00:16:11,590
in put in in falmouth to monic to fetch

00:16:10,000 --> 00:16:14,500
all the locks from that specific

00:16:11,590 --> 00:16:16,840
container and music as you can see here

00:16:14,500 --> 00:16:19,720
the configuration file is passed as a

00:16:16,840 --> 00:16:24,670
template so you have you can see that

00:16:19,720 --> 00:16:27,330
this placeholder here basically is this

00:16:24,670 --> 00:16:33,900
place or here placeholder here basically

00:16:27,330 --> 00:16:33,900
is overreaching with the container ID

00:16:34,290 --> 00:16:40,420
besides having the configuration file in

00:16:38,280 --> 00:16:42,790
besides having the configuration

00:16:40,420 --> 00:16:46,300
template in the configuration file of

00:16:42,790 --> 00:16:48,190
the bit you can also have the

00:16:46,300 --> 00:16:51,490
configuration template in the

00:16:48,190 --> 00:16:55,600
annotations of the pot or in the doctor

00:16:51,490 --> 00:16:59,110
labels so basically you can specify in

00:16:55,600 --> 00:17:03,870
kubernetes series of annotations and

00:16:59,110 --> 00:17:07,930
then you just have to do is to enable

00:17:03,870 --> 00:17:12,880
this hints here and then it will search

00:17:07,930 --> 00:17:15,730
for Co elastic dot locks and when it

00:17:12,880 --> 00:17:19,209
finds it then it enables the nginx

00:17:15,730 --> 00:17:22,990
module in file with in order to fetch in

00:17:19,209 --> 00:17:27,010
this case access and error locks similar

00:17:22,990 --> 00:17:29,890
with matrix searches for geo elastic dot

00:17:27,010 --> 00:17:32,590
matrix and everytime is find it that

00:17:29,890 --> 00:17:38,890
then it enables an Jennings module in

00:17:32,590 --> 00:17:43,860
metric PT and here have a small demo

00:17:38,890 --> 00:17:46,060
about how to use auto discover feature

00:17:43,860 --> 00:17:47,830
hi everyone

00:17:46,060 --> 00:17:49,600
they were single zero leaves and last

00:17:47,830 --> 00:17:52,030
acquits have native support for

00:17:49,600 --> 00:17:54,850
monitoring kubernetes and applications

00:17:52,030 --> 00:17:56,770
running inside kubernetes this is one of

00:17:54,850 --> 00:17:59,650
the most comprehensive monitoring

00:17:56,770 --> 00:18:03,400
solution for kubernetes as we handle

00:17:59,650 --> 00:18:06,040
locks metrics and ABM data all enhance

00:18:03,400 --> 00:18:09,010
in kubernetes metadata and perfectly

00:18:06,040 --> 00:18:11,710
integrated with kubernetes in our

00:18:09,010 --> 00:18:13,810
documentation will provide sample

00:18:11,710 --> 00:18:16,870
manifest files that you can use to

00:18:13,810 --> 00:18:19,450
deploy file bit and metric bit as daemon

00:18:16,870 --> 00:18:22,300
set which means that you will have a

00:18:19,450 --> 00:18:26,220
file bit and a major bit instance on

00:18:22,300 --> 00:18:29,380
each node in Cubana

00:18:26,220 --> 00:18:32,490
let's now deploy a file bitumen set by

00:18:29,380 --> 00:18:34,930
using the cube CTL create command a

00:18:32,490 --> 00:18:36,430
critical feature for any monitoring

00:18:34,930 --> 00:18:38,560
solution for kubernetes

00:18:36,430 --> 00:18:40,930
is to be able to dynamically adjust

00:18:38,560 --> 00:18:42,970
their configuration when something

00:18:40,930 --> 00:18:44,680
happens in your cluster because you

00:18:42,970 --> 00:18:46,930
don't want to change the configuration

00:18:44,680 --> 00:18:48,790
of the monitoring solution each time

00:18:46,930 --> 00:18:51,760
when you Porter application is deployed

00:18:48,790 --> 00:18:54,340
in kubernetes in business is solved by

00:18:51,760 --> 00:18:57,550
Auto discovery feature that allows you

00:18:54,340 --> 00:19:00,100
to automatically start new modules when

00:18:57,550 --> 00:19:02,590
new kubernetes ports are created and

00:19:00,100 --> 00:19:04,740
here is an example of the file bit

00:19:02,590 --> 00:19:07,300
manifest file

00:19:04,740 --> 00:19:09,760
this contains an example of the

00:19:07,300 --> 00:19:12,310
autodiscover configuration in this case

00:19:09,760 --> 00:19:15,850
fiber subscribes to docker events and

00:19:12,310 --> 00:19:18,460
looks for new pots with nginx image and

00:19:15,850 --> 00:19:21,460
if such pod is starting on a given node

00:19:18,460 --> 00:19:23,680
then 5-bit initiates a module for

00:19:21,460 --> 00:19:26,110
collecting and parsing the nginx locks

00:19:23,680 --> 00:19:28,840
as you can see here the module

00:19:26,110 --> 00:19:34,810
configuration is a template in which you

00:19:28,840 --> 00:19:37,030
can use the kubernetes metadata now

00:19:34,810 --> 00:19:39,220
let's get the engine experiment to five

00:19:37,030 --> 00:19:45,190
pots and for that I'm going to use a

00:19:39,220 --> 00:19:47,140
cube CTL scale command if we switch the

00:19:45,190 --> 00:19:49,090
Cabana then we'll be able to see that

00:19:47,140 --> 00:19:50,890
file build has already sent the logs

00:19:49,090 --> 00:19:53,050
from all the ports that are running in

00:19:50,890 --> 00:19:56,140
kubernetes and if we search for auto

00:19:53,050 --> 00:19:58,630
discover then we'll be able to see that

00:19:56,140 --> 00:19:59,779
file be detected the new port running

00:19:58,630 --> 00:20:02,330
Ingenix

00:19:59,779 --> 00:20:04,460
meet and has started an engine X modulo

00:20:02,330 --> 00:20:07,219
to get the logs directly from the toker

00:20:04,460 --> 00:20:09,710
contact this means that the engine X

00:20:07,219 --> 00:20:11,299
locks are already passed similarly we

00:20:09,710 --> 00:20:13,759
can collect the metrics from the

00:20:11,299 --> 00:20:16,729
engineer service using the engine X

00:20:13,759 --> 00:20:19,369
module in metric P and then we can have

00:20:16,729 --> 00:20:21,649
a dashboard like this one that includes

00:20:19,369 --> 00:20:25,690
information collected from logs and

00:20:21,649 --> 00:20:25,690
metrics thanks for watching

00:20:27,340 --> 00:20:35,229
ok so in this presentation - what you

00:20:30,979 --> 00:20:38,539
have seen is that I used the Cubana

00:20:35,229 --> 00:20:43,639
visualizations generic ones to visualize

00:20:38,539 --> 00:20:45,859
kubernetes kubernetes infrastructure so

00:20:43,639 --> 00:20:48,889
I think it makes sense for for

00:20:45,859 --> 00:20:51,769
kubernetes infrastructure to have more

00:20:48,889 --> 00:20:53,450
specific visualizations and I mentioned

00:20:51,769 --> 00:20:56,059
this at the beginning that I think it

00:20:53,450 --> 00:20:59,029
makes sense to have to offer a high

00:20:56,059 --> 00:21:01,279
level overview of of your kubernetes

00:20:59,029 --> 00:21:04,580
infrastructure so recently we are

00:21:01,279 --> 00:21:06,799
working on on on this map that we are

00:21:04,580 --> 00:21:11,299
calling the waffle map because it looks

00:21:06,799 --> 00:21:15,679
like a waffle where each of this square

00:21:11,299 --> 00:21:18,529
is a pot and you can group it by name

00:21:15,679 --> 00:21:21,799
space pod name container name and so on

00:21:18,529 --> 00:21:24,889
basically you can group it by any field

00:21:21,799 --> 00:21:28,820
that exists in the event that is created

00:21:24,889 --> 00:21:31,309
by the beach you can also filter by for

00:21:28,820 --> 00:21:34,929
example if you want to filter for all

00:21:31,309 --> 00:21:39,619
the pots that have a specific name space

00:21:34,929 --> 00:21:43,609
and what's also important is that the

00:21:39,619 --> 00:21:48,950
color of this of each square is

00:21:43,609 --> 00:21:51,799
determined by biometric so here is CPU

00:21:48,950 --> 00:21:55,399
usage the metric but it can be anything

00:21:51,799 --> 00:21:59,029
any metric that is part of the of the

00:21:55,399 --> 00:22:02,419
event events from that are generated by

00:21:59,029 --> 00:22:05,089
the beach so this is like looks like in

00:22:02,419 --> 00:22:09,559
real world right but usually you will

00:22:05,089 --> 00:22:12,500
have like squares like orange or green

00:22:09,559 --> 00:22:15,860
or or

00:22:12,500 --> 00:22:19,670
depending on on the color or the value

00:22:15,860 --> 00:22:21,710
of the CP usage in this case so I would

00:22:19,670 --> 00:22:28,220
like to show you a small video of how

00:22:21,710 --> 00:22:32,300
this works so basically as you can see

00:22:28,220 --> 00:22:34,640
here this is the waffle map and you if

00:22:32,300 --> 00:22:37,190
you click on one of the waffle map of

00:22:34,640 --> 00:22:40,309
the square you can see more details

00:22:37,190 --> 00:22:43,100
about that particular part so you can

00:22:40,309 --> 00:22:45,710
see not only metrics but also you can

00:22:43,100 --> 00:22:52,610
see the other locks that are coming from

00:22:45,710 --> 00:22:55,429
that particular pot also here it was

00:22:52,610 --> 00:22:57,410
cost now we are going to cook to

00:22:55,429 --> 00:22:59,330
kubernetes and here you'll be able to

00:22:57,410 --> 00:23:01,690
group by for example it was named space

00:22:59,330 --> 00:23:06,200
group by and group by container name and

00:23:01,690 --> 00:23:08,780
also you are able to filter by the

00:23:06,200 --> 00:23:13,309
container name and if we filter for file

00:23:08,780 --> 00:23:16,280
bit then we'll be able to see all the

00:23:13,309 --> 00:23:19,580
pots that are part of this file bit and

00:23:16,280 --> 00:23:29,590
here as you can see we can we can see

00:23:19,580 --> 00:23:33,950
metrics but also locks I think that's it

00:23:29,590 --> 00:23:45,340
now if you have any questions thanks for

00:23:33,950 --> 00:23:45,340
listening Thank You Monica any questions

00:23:52,900 --> 00:23:58,730
do you have some sort of multi-tenancy

00:23:55,940 --> 00:24:03,620
support for people with different

00:23:58,730 --> 00:24:06,530
privileges this is something that it's

00:24:03,620 --> 00:24:09,410
we are working on on this on the Cabana

00:24:06,530 --> 00:24:11,150
side but so what you've seen I didn't I

00:24:09,410 --> 00:24:15,020
forgot to mention the most important

00:24:11,150 --> 00:24:18,380
thing is that this the waffle map that

00:24:15,020 --> 00:24:20,919
you saw is just a prototype so we didn't

00:24:18,380 --> 00:24:24,240
release it yet but we are working on it

00:24:20,919 --> 00:24:34,890
so of course it doesn't have

00:24:24,240 --> 00:24:43,290
advanced features for now some questions

00:24:34,890 --> 00:24:46,680
left so maybe one question show us that

00:24:43,290 --> 00:24:52,800
you can run the five bit also add a

00:24:46,680 --> 00:24:57,320
talker in within kubernetes so advise to

00:24:52,800 --> 00:25:02,310
run the monitoring file within the same

00:24:57,320 --> 00:25:09,090
part or communities deployment so you

00:25:02,310 --> 00:25:12,030
have a file bit and a metric bit for

00:25:09,090 --> 00:25:14,310
each for each node that is monitoring

00:25:12,030 --> 00:25:35,100
all the ports that are in that in that

00:25:14,310 --> 00:25:37,350
in that node question that is in terms

00:25:35,100 --> 00:25:39,600
of so what it showed is very specific to

00:25:37,350 --> 00:25:40,020
how the docket even logs its content

00:25:39,600 --> 00:25:41,970
logs

00:25:40,020 --> 00:25:43,980
so do you know how it actually is going

00:25:41,970 --> 00:25:45,300
to change once they move to cry or

00:25:43,980 --> 00:25:49,470
something like that has run time for

00:25:45,300 --> 00:26:01,500
communities that's a good question

00:25:49,470 --> 00:26:04,380
unfortunately I have an answer here okay

00:26:01,500 --> 00:26:06,000
then thank you Monica

00:26:04,380 --> 00:26:08,400
yeah thank you very much and if you have

00:26:06,000 --> 00:26:10,610
more questions yeah feel free to to come

00:26:08,400 --> 00:26:14,990
to ask me in private

00:26:10,610 --> 00:26:35,160
[Applause]

00:26:14,990 --> 00:26:35,160

YouTube URL: https://www.youtube.com/watch?v=X27-ysCm2cQ


