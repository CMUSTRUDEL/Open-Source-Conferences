Title: OSDC 2015: David Norton | InfluxDB   Scalable Metrics Made Easy
Publication date: 2015-04-29
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	There are many applications that need an easy way to store and analyze periodic measurements, whether it's system stats from multiple data centers or a Saturday afternoon project. Learn how InfluxDB can be used to store and analyze metrics as your system scales.
This session will be an introduction to InfluxDB, an open-source, distributed, time series database. It's written in Golang and compiles to a single executable, making deployment simple. We'll see how a fresh install of a single instance can be up and running in minutes. We'll look at its SQL-like query syntax, designed to reduce the learning curve coming from other SQL databases. And, we'll see how to add nodes to create a cluster as a system grows.
Captions: 
	00:01:37,469 --> 00:01:48,670
fitness tracking device happy in flux

00:01:41,950 --> 00:01:51,549
giving users are all individual boxes

00:01:48,670 --> 00:01:57,819
and collect a bunch of those things

00:01:51,549 --> 00:01:59,970
together put in the database let's talk

00:01:57,819 --> 00:01:59,970
about

00:04:28,630 --> 00:04:53,419
this is usually the problem that is it

00:04:45,710 --> 00:04:55,940
doesn't scale very well so for example

00:04:53,419 --> 00:04:59,240
of that in the metrics use case you have

00:04:55,940 --> 00:05:00,979
10 votes very small number of hosts 100

00:04:59,240 --> 00:05:04,750
measurements per host is also a fairly

00:05:00,979 --> 00:05:11,169
small number collected every 10 seconds

00:05:04,750 --> 00:05:11,169
365 3.1 billion records

00:05:39,010 --> 00:05:42,430
we don't have to

00:06:16,979 --> 00:06:19,979
or

00:09:18,279 --> 00:09:20,339
you

00:11:56,860 --> 00:12:02,899
hello testing Oh so I've been on mute

00:12:00,199 --> 00:12:12,350
the whole time you guys want me to start

00:12:02,899 --> 00:12:16,279
over all right on the back oh okay all

00:12:12,350 --> 00:12:19,279
right so no external dependencies those

00:12:16,279 --> 00:12:21,589
shared libraries no no third-party

00:12:19,279 --> 00:12:24,230
software to install you know no other

00:12:21,589 --> 00:12:27,819
database is underneath anything like

00:12:24,230 --> 00:12:27,819
that it's all rolled into one

00:12:28,720 --> 00:12:36,829
distributed and scalable easy to use and

00:12:33,709 --> 00:12:38,329
install maintain and we'll we're going

00:12:36,829 --> 00:12:41,300
to go through we'll we'll see this in a

00:12:38,329 --> 00:12:43,100
little bit i'm going to demo an install

00:12:41,300 --> 00:12:46,160
so we can see just how hard it is we've

00:12:43,100 --> 00:12:47,779
all heard that story before that from

00:12:46,160 --> 00:12:50,990
the sales guy that it's really easy to

00:12:47,779 --> 00:12:52,519
install news and then you get it get it

00:12:50,990 --> 00:12:56,240
home and it's actually quite a bit of

00:12:52,519 --> 00:13:00,379
trouble open source of course i wouldn't

00:12:56,240 --> 00:13:03,889
be here but it's MIT licensed and it's

00:13:00,379 --> 00:13:05,660
written and go if anybody's a talk for

00:13:03,889 --> 00:13:08,980
another time but a great another great

00:13:05,660 --> 00:13:14,059
language from google open source as well

00:13:08,980 --> 00:13:15,740
so let's look at the data model so at

00:13:14,059 --> 00:13:17,360
the top this isn't really part of the

00:13:15,740 --> 00:13:20,029
data model but i just i put it in here

00:13:17,360 --> 00:13:22,309
too so we can show how it all works

00:13:20,029 --> 00:13:25,490
you've got the influx damon that runs

00:13:22,309 --> 00:13:26,809
that manages everything you've got one

00:13:25,490 --> 00:13:28,699
of these running on and you know each

00:13:26,809 --> 00:13:32,119
node whether you've got one note or of

00:13:28,699 --> 00:13:37,189
plots of nodes and it can manage one or

00:13:32,119 --> 00:13:40,459
more databases and then inside of each

00:13:37,189 --> 00:13:41,929
database we have measurements and those

00:13:40,459 --> 00:13:45,230
are the things that we looked at kind of

00:13:41,929 --> 00:13:47,779
like before you know CPU loads amount of

00:13:45,230 --> 00:13:48,999
memory used this usage it could be

00:13:47,779 --> 00:13:54,980
anything you want to measure

00:13:48,999 --> 00:13:59,179
temperatures and then we have tags this

00:13:54,980 --> 00:14:00,499
is metadata this tells you the things

00:13:59,179 --> 00:14:02,660
that you're you know where are the

00:14:00,499 --> 00:14:05,089
things you're storing came from or any

00:14:02,660 --> 00:14:07,220
kind of information about it and in

00:14:05,089 --> 00:14:09,740
something important to note is that in

00:14:07,220 --> 00:14:11,810
in flux measurements and tags or

00:14:09,740 --> 00:14:14,840
all indexed so any search on a

00:14:11,810 --> 00:14:19,390
measurement or a tag or a tag value the

00:14:14,840 --> 00:14:19,390
searches are all all very quick or cheap

00:14:20,080 --> 00:14:27,560
and we have a series hence the time

00:14:24,110 --> 00:14:32,630
series which in influx a series means a

00:14:27,560 --> 00:14:35,720
measurement plus a unique tag set so in

00:14:32,630 --> 00:14:37,100
this example we have like there's a

00:14:35,720 --> 00:14:41,150
pointer on here I don't know where it is

00:14:37,100 --> 00:14:43,640
it anyway so we're collecting CPU

00:14:41,150 --> 00:14:49,310
measurements from Germany in the western

00:14:43,640 --> 00:14:50,810
region or we could be collecting CPU

00:14:49,310 --> 00:14:52,370
measurements from Germany in the eastern

00:14:50,810 --> 00:14:55,250
region so either one of those little

00:14:52,370 --> 00:15:01,370
streams of data would be a unique time

00:14:55,250 --> 00:15:05,150
series then inside of a series we have

00:15:01,370 --> 00:15:07,420
points and the points are just the

00:15:05,150 --> 00:15:13,610
values the actual data values themselves

00:15:07,420 --> 00:15:15,590
with their time stamps now the example

00:15:13,610 --> 00:15:18,440
little table data over there shows just

00:15:15,590 --> 00:15:20,390
one value column and that's that's

00:15:18,440 --> 00:15:22,760
common in time series just to have one

00:15:20,390 --> 00:15:26,120
you know a time stamp and a single value

00:15:22,760 --> 00:15:30,980
I like CPU measurement we do support

00:15:26,120 --> 00:15:33,170
multiple fields off the top of my head I

00:15:30,980 --> 00:15:35,000
don't I can't think of a use case that

00:15:33,170 --> 00:15:36,220
that I know of that anybody's actually

00:15:35,000 --> 00:15:39,290
using in the field I'm sure there are

00:15:36,220 --> 00:15:41,960
but it could be maybe if you're storing

00:15:39,290 --> 00:15:44,300
event like mouseevent clicks you can

00:15:41,960 --> 00:15:47,720
store x and y coordinate of where the

00:15:44,300 --> 00:15:53,480
cliq occurred but more typically we just

00:15:47,720 --> 00:15:55,700
see this a single value so how many of

00:15:53,480 --> 00:16:00,350
these time series can-can the database

00:15:55,700 --> 00:16:04,340
handle the answer right now is we don't

00:16:00,350 --> 00:16:10,250
I don't know but the goal for the

00:16:04,340 --> 00:16:13,190
upcoming version 09 releases is 1.2

00:16:10,250 --> 00:16:18,560
million value or points per second

00:16:13,190 --> 00:16:21,050
written across 1.2 million series so I

00:16:18,560 --> 00:16:23,000
know for a fact that we can handle 1.2

00:16:21,050 --> 00:16:23,640
million series my laptop will easily do

00:16:23,000 --> 00:16:27,180
that

00:16:23,640 --> 00:16:28,530
a problem we're not there yet on the

00:16:27,180 --> 00:16:32,460
number of points per second we're

00:16:28,530 --> 00:16:34,790
somewhere around 450 k we've done we're

00:16:32,460 --> 00:16:38,820
mostly concentrating on getting features

00:16:34,790 --> 00:16:40,050
up to a certain base level and then

00:16:38,820 --> 00:16:41,760
we'll hit the low hanging fruit on

00:16:40,050 --> 00:16:44,790
performance and I think we'll pretty

00:16:41,760 --> 00:16:53,550
easily get up to the 1.2 million points

00:16:44,790 --> 00:16:55,710
per minute goal so let's talk about data

00:16:53,550 --> 00:16:57,630
retention and replication so these are a

00:16:55,710 --> 00:16:58,770
couple of the problems that we talked

00:16:57,630 --> 00:17:01,530
about that you know if you're building

00:16:58,770 --> 00:17:02,730
on top of an existing distributed

00:17:01,530 --> 00:17:08,910
database that you're going to have to

00:17:02,730 --> 00:17:10,670
deal with yourself as the data age is

00:17:08,910 --> 00:17:12,650
out you know you want to down sample it

00:17:10,670 --> 00:17:15,089
store it in a lower resolution

00:17:12,650 --> 00:17:21,570
longer-term storage or age it out

00:17:15,089 --> 00:17:25,640
completely an influx DB we have what are

00:17:21,570 --> 00:17:31,140
called retention policies and this makes

00:17:25,640 --> 00:17:33,420
managing retention very easy when you

00:17:31,140 --> 00:17:36,870
write data it's always written to a

00:17:33,420 --> 00:17:40,110
retention policy and when you create a

00:17:36,870 --> 00:17:43,520
database every database has to have a

00:17:40,110 --> 00:17:48,410
default retention policy if you don't

00:17:43,520 --> 00:17:51,150
it'll create one for you by default and

00:17:48,410 --> 00:17:53,130
if you if you just write data in and you

00:17:51,150 --> 00:17:54,780
don't specify a particular one it writes

00:17:53,130 --> 00:17:57,500
it gets written into the default and the

00:17:54,780 --> 00:18:01,140
default maintains or retains data

00:17:57,500 --> 00:18:02,640
indefinitely so you definitely know you

00:18:01,140 --> 00:18:05,460
want to be careful you don't want to

00:18:02,640 --> 00:18:07,590
throw that into production and just

00:18:05,460 --> 00:18:09,420
start beaming tons of data at it because

00:18:07,590 --> 00:18:11,340
it's it's never going to go away you're

00:18:09,420 --> 00:18:16,320
going to quickly probably overflow your

00:18:11,340 --> 00:18:19,920
disks you can create as many retention

00:18:16,320 --> 00:18:23,250
policies as you want you can enter a

00:18:19,920 --> 00:18:26,520
retention policy is just it's a duration

00:18:23,250 --> 00:18:31,260
and and we'll talk about in a second is

00:18:26,520 --> 00:18:33,150
a replication factor we also handle

00:18:31,260 --> 00:18:36,210
replication through retention policy so

00:18:33,150 --> 00:18:37,410
you tell it you know create a Creative

00:18:36,210 --> 00:18:42,060
retention policy that's

00:18:37,410 --> 00:18:44,670
one hour in duration on database foo and

00:18:42,060 --> 00:18:46,440
its replication factor is you know if

00:18:44,670 --> 00:18:47,850
I'm running on my laptop my replication

00:18:46,440 --> 00:18:49,380
factor is going to be one because I'm

00:18:47,850 --> 00:18:51,060
not replicating it anywhere but if you

00:18:49,380 --> 00:18:53,070
know if you're running on a three node

00:18:51,060 --> 00:19:03,600
cluster maybe you you set your

00:18:53,070 --> 00:19:05,910
replication two three and so that's I it

00:19:03,600 --> 00:19:09,120
seems like a really short discussion of

00:19:05,910 --> 00:19:10,530
it but it's about that simple we will

00:19:09,120 --> 00:19:12,690
look at i'll show you how you create a

00:19:10,530 --> 00:19:15,240
retention policy and and once you write

00:19:12,690 --> 00:19:21,990
data into it that's it the database

00:19:15,240 --> 00:19:26,310
handles in the background cleaning up so

00:19:21,990 --> 00:19:28,650
continuous queries these are used for

00:19:26,310 --> 00:19:31,410
down sampling or aggregating data on the

00:19:28,650 --> 00:19:35,550
fly so you can create a if you have a

00:19:31,410 --> 00:19:38,010
you know a high-res stream of CPU

00:19:35,550 --> 00:19:41,190
measurements coming in on your dashboard

00:19:38,010 --> 00:19:44,820
you want to look at the the averages you

00:19:41,190 --> 00:19:47,040
know over a period of time you can

00:19:44,820 --> 00:19:48,510
create a continuous query that will you

00:19:47,040 --> 00:19:50,850
know that could it could be an expensive

00:19:48,510 --> 00:19:54,180
query maybe maybe not depending on what

00:19:50,850 --> 00:19:56,010
exactly you're computing but you can you

00:19:54,180 --> 00:19:58,650
can create a continuous query it will

00:19:56,010 --> 00:20:01,530
create the roll-up or the aggregation on

00:19:58,650 --> 00:20:03,120
the fly for you in the background and so

00:20:01,530 --> 00:20:05,850
that does a couple things one is it's

00:20:03,120 --> 00:20:07,500
you know it's doing it on the fly the

00:20:05,850 --> 00:20:09,930
other is is that for expensive

00:20:07,500 --> 00:20:12,240
operations they get done periodically

00:20:09,930 --> 00:20:15,870
and then everybody just grabs that

00:20:12,240 --> 00:20:19,430
result so it doesn't get recomputed over

00:20:15,870 --> 00:20:21,930
and over you know stored in the database

00:20:19,430 --> 00:20:29,070
once you create them they just they live

00:20:21,930 --> 00:20:30,330
there and run on their own so I'm

00:20:29,070 --> 00:20:36,660
covering most of it without hitting the

00:20:30,330 --> 00:20:38,340
clicker here all right so let's talk

00:20:36,660 --> 00:20:43,740
about actually writing data into the

00:20:38,340 --> 00:20:45,840
database we use HTTP as a transport for

00:20:43,740 --> 00:20:49,050
writing it in you can just see you just

00:20:45,840 --> 00:20:50,220
hit the the right endpoint and give it

00:20:49,050 --> 00:20:52,620
some data

00:20:50,220 --> 00:20:56,130
in the dash deeper am there if you were

00:20:52,620 --> 00:21:02,039
using curl to post it we currently use

00:20:56,130 --> 00:21:06,510
json as our format you can see let's see

00:21:02,039 --> 00:21:07,919
it you can see up at the top you know

00:21:06,510 --> 00:21:11,669
you tell it what database you're writing

00:21:07,919 --> 00:21:13,980
into here we're writing into a retention

00:21:11,669 --> 00:21:16,380
policy name default probably not the

00:21:13,980 --> 00:21:19,830
best choice of names for the for my

00:21:16,380 --> 00:21:22,559
slide here but if you if you create a

00:21:19,830 --> 00:21:27,450
database it actually creates a policy

00:21:22,559 --> 00:21:28,950
name to default so even if we didn't

00:21:27,450 --> 00:21:31,350
specify that here it would have still

00:21:28,950 --> 00:21:32,730
written into the one name default or

00:21:31,350 --> 00:21:34,799
would actually whatever the whatever

00:21:32,730 --> 00:21:37,440
policy was set as the default for the

00:21:34,799 --> 00:21:41,460
database then we have a list of points

00:21:37,440 --> 00:21:43,020
the name there you see cpu load short is

00:21:41,460 --> 00:21:45,900
what we were looking at called the

00:21:43,020 --> 00:21:48,960
measurement so that would be you know

00:21:45,900 --> 00:21:50,190
steps taken cpu load temperature

00:21:48,960 --> 00:21:54,090
whatever the thing is that you're

00:21:50,190 --> 00:21:59,220
measuring and then the tags for where

00:21:54,090 --> 00:22:04,400
the data came from timestamp and in the

00:21:59,220 --> 00:22:06,870
field values it's mostly schema-less

00:22:04,400 --> 00:22:10,140
except that once you write in a field

00:22:06,870 --> 00:22:11,789
value it takes the type from it the

00:22:10,140 --> 00:22:17,309
first time and then it expects to always

00:22:11,789 --> 00:22:19,980
get for that field that type of data so

00:22:17,309 --> 00:22:23,669
queria ng data we have a query endpoint

00:22:19,980 --> 00:22:27,059
and you pass in a queue pram with the

00:22:23,669 --> 00:22:29,880
query that you want to run and the

00:22:27,059 --> 00:22:33,470
language itself is sequel like we didn't

00:22:29,880 --> 00:22:36,539
try to stick to any particular sequel

00:22:33,470 --> 00:22:39,870
language standard we just borrowed very

00:22:36,539 --> 00:22:41,250
heavily from it and in places where it

00:22:39,870 --> 00:22:47,850
didn't make sense in the time series

00:22:41,250 --> 00:22:50,880
world we adapted it to fit so here's an

00:22:47,850 --> 00:22:52,860
example query if you've never seen in

00:22:50,880 --> 00:22:53,940
flux before but you've worked on sequel

00:22:52,860 --> 00:22:56,820
this probably still looks pretty

00:22:53,940 --> 00:23:00,390
familiar we're just selecting value from

00:22:56,820 --> 00:23:04,200
cpu load short we're at AG equals some

00:23:00,390 --> 00:23:09,570
value and then we get back a result

00:23:04,200 --> 00:23:12,929
in JSON format you could get back one or

00:23:09,570 --> 00:23:22,679
more series and a result in this case

00:23:12,929 --> 00:23:24,450
we've we've just got one series and so I

00:23:22,679 --> 00:23:26,610
mean as far as just writing data in and

00:23:24,450 --> 00:23:33,990
getting it out again it's it's just that

00:23:26,610 --> 00:23:36,750
simple something I was hoping to show

00:23:33,990 --> 00:23:39,419
but i don't it some visual dashboards

00:23:36,750 --> 00:23:41,120
but we've been going through anybody

00:23:39,419 --> 00:23:44,750
that keeps up with a project knows that

00:23:41,120 --> 00:23:48,649
own i know has been a major change from

00:23:44,750 --> 00:23:51,659
from 08 basically a rewrite from scratch

00:23:48,649 --> 00:23:55,460
there's probably four or five lines of

00:23:51,659 --> 00:23:59,309
the old code left i don't know but so

00:23:55,460 --> 00:24:01,679
Griffin ax definitely works with 08 and

00:23:59,309 --> 00:24:04,230
it will work very soon with 09 I

00:24:01,679 --> 00:24:05,220
couldn't get it to work for the demo not

00:24:04,230 --> 00:24:06,600
well enough that I felt comfortable

00:24:05,220 --> 00:24:09,419
getting up in front of people with

00:24:06,600 --> 00:24:11,130
anyway but it'll be working again soon

00:24:09,419 --> 00:24:14,720
and so anyway Griffin ax if you haven't

00:24:11,130 --> 00:24:17,309
used it is a really nice third-party

00:24:14,720 --> 00:24:19,980
graph or you know visually dashboard

00:24:17,309 --> 00:24:23,730
application that works with with influx

00:24:19,980 --> 00:24:27,750
it knows how to talk to end flux and

00:24:23,730 --> 00:24:31,309
show you what's in the data and allow

00:24:27,750 --> 00:24:34,320
you to create visualizations of it

00:24:31,309 --> 00:24:36,600
alright so this is going to be the

00:24:34,320 --> 00:24:38,970
unscripted part of this thing of the

00:24:36,600 --> 00:24:40,380
demo here or the talk we're going to

00:24:38,970 --> 00:24:43,950
actually go through and install we'll

00:24:40,380 --> 00:24:46,889
look at config and we'll we'll see how

00:24:43,950 --> 00:24:51,110
you would will write data to the

00:24:46,889 --> 00:24:51,110
database discover and query the data

00:24:52,519 --> 00:24:57,090
everyone see this okay you guys in the

00:24:55,620 --> 00:25:02,370
back read that or do I may need to make

00:24:57,090 --> 00:25:04,710
the font bigger good okay so what I've

00:25:02,370 --> 00:25:07,350
got here is a I'm going to run this I'm

00:25:04,710 --> 00:25:11,809
going to fire up a docker real quick and

00:25:07,350 --> 00:25:11,809
just to show you what's in this docker

00:25:15,520 --> 00:25:20,120
we're going to just pull a basic you

00:25:18,140 --> 00:25:22,460
bunt to image and we're going to add in

00:25:20,120 --> 00:25:25,040
that Deb you install package which I

00:25:22,460 --> 00:25:28,040
built you can you know do a dub you get

00:25:25,040 --> 00:25:31,730
off of our website to grab the latest

00:25:28,040 --> 00:25:33,710
release candidate 409 and then and then

00:25:31,730 --> 00:25:34,820
after you do that w get the process

00:25:33,710 --> 00:25:37,130
would be just like what you're seeing

00:25:34,820 --> 00:25:42,220
here you basically w get it and you run

00:25:37,130 --> 00:25:42,220
it that's it so let's see that happen

00:25:42,490 --> 00:25:54,980
I'm probably going to have to get some

00:25:45,140 --> 00:25:57,350
expert here to tell me the right so I'm

00:25:54,980 --> 00:26:04,630
going to export a couple of ports here

00:25:57,350 --> 00:26:10,250
that influx uses 8086 for the data

00:26:04,630 --> 00:26:15,620
interface and then port 8083 for the

00:26:10,250 --> 00:26:18,470
admin interface the admin interface is

00:26:15,620 --> 00:26:23,630
kind of an optional thing but we'll take

00:26:18,470 --> 00:26:25,870
a look at it let's see make this bigger

00:26:23,630 --> 00:26:25,870
too

00:26:38,470 --> 00:26:47,429
alright so we're in we have our Deb

00:26:42,970 --> 00:26:47,429
install package there so we'll run that

00:26:50,549 --> 00:26:58,630
all right so no going to get coffee

00:26:52,690 --> 00:27:01,720
while in flux installs is done if we go

00:26:58,630 --> 00:27:04,590
look at the directory to see what we

00:27:01,720 --> 00:27:04,590
actually put there

00:27:14,070 --> 00:27:19,770
so you can see you know the thing at the

00:27:17,190 --> 00:27:22,170
top is a command-line interface then

00:27:19,770 --> 00:27:25,350
there's the daemon and a couple on the

00:27:22,170 --> 00:27:27,390
knit script so there's not much there I

00:27:25,350 --> 00:27:32,730
mean truly no external dependencies on

00:27:27,390 --> 00:27:34,350
anything else let's fire it up I mean we

00:27:32,730 --> 00:27:37,200
could at this point we could start it

00:27:34,350 --> 00:27:40,410
through your normal you know whatever

00:27:37,200 --> 00:27:44,210
you're managing Damon's with but we'll

00:27:40,410 --> 00:27:44,210
we'll run it from the command line here

00:27:54,110 --> 00:27:59,870
so I've just launched it with actually

00:27:56,809 --> 00:28:00,950
with no configuration file at all when

00:27:59,870 --> 00:28:02,420
you run it when you run it from the

00:28:00,950 --> 00:28:04,910
command line like this it just ran with

00:28:02,420 --> 00:28:05,929
an internal default config obviously you

00:28:04,910 --> 00:28:09,860
wouldn't install and run it in

00:28:05,929 --> 00:28:12,440
production this way probably but as far

00:28:09,860 --> 00:28:15,530
as experimenting with it seeing if you

00:28:12,440 --> 00:28:17,270
like it getting up to speed doing a lot

00:28:15,530 --> 00:28:19,780
of development work against it this

00:28:17,270 --> 00:28:19,780
works great

00:28:30,800 --> 00:28:33,460
oops

00:28:34,060 --> 00:28:40,630
so that guy is running so let's um I've

00:28:39,040 --> 00:28:42,670
got a little app that will actually

00:28:40,630 --> 00:28:57,630
before we do this let me bring up

00:28:42,670 --> 00:29:03,040
something else we go to we go to the

00:28:57,630 --> 00:29:05,920
admin port 8083 that we exported come to

00:29:03,040 --> 00:29:08,890
this this is just a real quick little

00:29:05,920 --> 00:29:10,810
admin app that you can or admin

00:29:08,890 --> 00:29:16,660
interface that you can use to play

00:29:10,810 --> 00:29:18,970
around with it connect right now we have

00:29:16,660 --> 00:29:21,190
no databases which is not a surprise we

00:29:18,970 --> 00:29:26,620
just installed it if we want to create a

00:29:21,190 --> 00:29:31,210
new database tell it so we've created a

00:29:26,620 --> 00:29:38,410
database now if we go explore data in

00:29:31,210 --> 00:29:39,910
that database obviously we haven't done

00:29:38,410 --> 00:29:44,110
anything yet so we haven't we have no

00:29:39,910 --> 00:29:45,760
data to explore so what I'm going to do

00:29:44,110 --> 00:29:49,240
now is I've got a little app that I

00:29:45,760 --> 00:29:51,760
wrote that uses our library our go

00:29:49,240 --> 00:29:55,660
library to interface with it we have a

00:29:51,760 --> 00:29:59,620
number of different language libraries

00:29:55,660 --> 00:30:05,620
bindings out there we have go of course

00:29:59,620 --> 00:30:06,730
we have Java Ruby Python there's I don't

00:30:05,620 --> 00:30:08,380
there's a there's a bunch more there's

00:30:06,730 --> 00:30:11,170
probably twice that many more out there

00:30:08,380 --> 00:30:13,360
that I'm forgetting about some of those

00:30:11,170 --> 00:30:17,860
will probably need some work to come up

00:30:13,360 --> 00:30:19,750
to speed 409 but what they will so I'm

00:30:17,860 --> 00:30:24,010
going to run this little test app that's

00:30:19,750 --> 00:30:26,820
just going to put some data in there and

00:30:24,010 --> 00:30:26,820
remember what I named it

00:30:39,059 --> 00:30:42,059
there

00:30:43,820 --> 00:30:52,040
so it's just ticking off data into the

00:30:47,750 --> 00:30:54,500
database 5,000 points at a time it says

00:30:52,040 --> 00:30:57,440
there at the top 3200 series that we're

00:30:54,500 --> 00:30:59,150
writing alright so I've got that thing

00:30:57,440 --> 00:31:04,610
putting data in there for us to play

00:30:59,150 --> 00:31:06,680
with open up another tab here and this

00:31:04,610 --> 00:31:08,810
time we'll we'll go in and look at it

00:31:06,680 --> 00:31:10,580
actually will go back through the web

00:31:08,810 --> 00:31:13,340
interface here and now if we rerun our

00:31:10,580 --> 00:31:17,290
show measurements now you can see we

00:31:13,340 --> 00:31:17,290
have some some measurements to look at

00:31:20,920 --> 00:31:28,430
show you this also there's some basic

00:31:26,210 --> 00:31:29,600
Diagnostics that you can run just to you

00:31:28,430 --> 00:31:36,800
know see what's going on with the

00:31:29,600 --> 00:31:41,600
database let's go back to our

00:31:36,800 --> 00:31:43,070
measurements alright so we see these

00:31:41,600 --> 00:31:45,170
things that we're collecting so maybe we

00:31:43,070 --> 00:31:47,390
want to one of the big things like I was

00:31:45,170 --> 00:31:50,210
talking about with Griffin is is data

00:31:47,390 --> 00:31:53,720
discovery when you've got a million

00:31:50,210 --> 00:31:57,500
series and you know thousands of tags or

00:31:53,720 --> 00:31:59,830
thousands of possible tag values just

00:31:57,500 --> 00:32:03,920
knowing what's even out there to look at

00:31:59,830 --> 00:32:05,570
is a problem in itself and then there's

00:32:03,920 --> 00:32:09,610
also the possibility that there are gaps

00:32:05,570 --> 00:32:12,230
in time you may be collecting CPU data

00:32:09,610 --> 00:32:14,300
for this for this window of time but

00:32:12,230 --> 00:32:16,190
maybe not for this window of time so if

00:32:14,300 --> 00:32:17,780
you go to query it you know maybe maybe

00:32:16,190 --> 00:32:20,660
there's nothing there at that particular

00:32:17,780 --> 00:32:23,390
period so one of the things is you know

00:32:20,660 --> 00:32:24,920
discovering what's available for this

00:32:23,390 --> 00:32:27,010
window of time that you're interested in

00:32:24,920 --> 00:32:30,080
you know if some anomaly happened at

00:32:27,010 --> 00:32:31,580
four o'clock yesterday you know you

00:32:30,080 --> 00:32:34,820
first need to go see what data do you

00:32:31,580 --> 00:32:36,380
have from four o'clock yesterday and

00:32:34,820 --> 00:32:39,020
this that can also come into play if

00:32:36,380 --> 00:32:42,800
you're collecting metrics from from VMs

00:32:39,020 --> 00:32:45,470
or you know docker images that are

00:32:42,800 --> 00:32:50,030
running those things are they come and

00:32:45,470 --> 00:32:52,190
go so we know we're collecting these

00:32:50,030 --> 00:32:56,200
measurements let's see what we know

00:32:52,190 --> 00:32:56,200
about these measurements we can say show

00:32:56,260 --> 00:33:23,690
tag tags from cpu up no sure I'm

00:33:12,649 --> 00:33:26,299
forgetting my own language now that's

00:33:23,690 --> 00:33:35,260
your phone tags expected continuing with

00:33:26,299 --> 00:33:35,260
state of these know it would be show

00:33:47,130 --> 00:34:00,060
tied keys there we go so we can see that

00:33:55,710 --> 00:34:02,340
we're collecting tag keys from with with

00:34:00,060 --> 00:34:07,920
a with a name country data center

00:34:02,340 --> 00:34:10,679
region and server and so then we would

00:34:07,920 --> 00:34:12,780
wonder what values are we collecting in

00:34:10,679 --> 00:34:23,550
those individual tags so we can ship we

00:34:12,780 --> 00:34:31,250
can say show tag values from cpu with

00:34:23,550 --> 00:34:33,899
key good got that one right at least

00:34:31,250 --> 00:34:37,560
okay so we can see we're collecting cpu

00:34:33,899 --> 00:34:40,159
measurements for these countries and

00:34:37,560 --> 00:34:43,350
there were some other things other key

00:34:40,159 --> 00:34:45,870
or other tag keys there we could also if

00:34:43,350 --> 00:34:49,429
you wanted to query for multiple keys at

00:34:45,870 --> 00:34:49,429
the same time you could say

00:34:49,550 --> 00:35:02,550
country-region a server I think was the

00:34:58,650 --> 00:35:04,020
other then it'll spit out a list of all

00:35:02,550 --> 00:35:05,700
the different you know for for the

00:35:04,020 --> 00:35:07,020
country tag we're collecting these

00:35:05,700 --> 00:35:10,470
things for the data center we're

00:35:07,020 --> 00:35:17,340
collecting these four regions and then

00:35:10,470 --> 00:35:18,780
servers alright so I'm going to just to

00:35:17,340 --> 00:35:26,310
show it i'm going to switch over to the

00:35:18,780 --> 00:35:29,000
command line interface so we're

00:35:26,310 --> 00:35:29,000
connected to it

00:35:35,799 --> 00:35:41,230
and let's actually query some data so

00:35:44,980 --> 00:35:51,500
now we're writing data in you know kind

00:35:49,760 --> 00:35:52,849
of fast and not super fast so you don't

00:35:51,500 --> 00:35:55,670
you know you don't want to select from a

00:35:52,849 --> 00:35:58,309
huge range of time or will be watching

00:35:55,670 --> 00:36:03,770
data scroll by for the rest of the

00:35:58,309 --> 00:36:11,119
presentation so let's say time is

00:36:03,770 --> 00:36:14,950
greater than now minus one second so we

00:36:11,119 --> 00:36:14,950
just spit out you know a ton of values

00:36:19,420 --> 00:36:24,279
we could count the values that we spit

00:36:22,250 --> 00:36:24,279
out

00:36:34,060 --> 00:36:42,170
maybe with cpu measurement that might be

00:36:39,619 --> 00:36:44,119
more interesting to look at and then we

00:36:42,170 --> 00:36:46,869
can you know limit our search by by the

00:36:44,119 --> 00:36:46,869
tags of course

00:36:57,540 --> 00:37:00,690
we can

00:37:04,750 --> 00:37:16,580
so that's for the last second let's see

00:37:08,020 --> 00:37:19,720
we might be interested in let's see the

00:37:16,580 --> 00:37:19,720
last 30 seconds

00:37:35,210 --> 00:37:39,740
so we just so we just ask for the CPU

00:37:37,760 --> 00:37:44,960
measurement the average CPU measurement

00:37:39,740 --> 00:37:46,580
and for the last minute grouped in or

00:37:44,960 --> 00:37:50,740
the last 30 seconds rather I think

00:37:46,580 --> 00:37:53,030
grouped in and blocks of 10 seconds and

00:37:50,740 --> 00:37:55,580
you can see there with the results that

00:37:53,030 --> 00:38:00,619
come back we get back the name of the

00:37:55,580 --> 00:38:05,349
measurement and we get back the unique

00:38:00,619 --> 00:38:05,349
tag sets from which that data came

00:38:08,500 --> 00:38:12,740
another really useful feature in time

00:38:10,880 --> 00:38:17,690
series is being able to use regular

00:38:12,740 --> 00:38:19,720
expressions and your queries say for

00:38:17,690 --> 00:38:19,720
instance

00:38:43,830 --> 00:38:56,280
can everyone still see at the bottom of

00:38:45,710 --> 00:39:01,080
the screen let's see and we know that we

00:38:56,280 --> 00:39:02,460
have servers that end in 000 something

00:39:01,080 --> 00:39:04,170
and maybe we only want to see the values

00:39:02,460 --> 00:39:06,930
from odd-numbered servers or something

00:39:04,170 --> 00:39:09,450
crazy obviously not a good real world

00:39:06,930 --> 00:39:15,180
use case but just to show the feature

00:39:09,450 --> 00:39:18,090
here we can say we're server and then we

00:39:15,180 --> 00:39:20,940
have a regular expression operator see

00:39:18,090 --> 00:39:24,150
the equals and then the tilde and we

00:39:20,940 --> 00:39:26,190
write regular expressions this way so we

00:39:24,150 --> 00:39:38,990
wanted to add numbers at the end of the

00:39:26,190 --> 00:39:38,990
line so 1 3 5 7 and 9 know what did I do

00:39:42,580 --> 00:39:56,980
oh thank you and may be here we have

00:39:52,960 --> 00:40:13,420
it's possible we have no values now

00:39:56,980 --> 00:40:19,500
there we go so there you can see we're

00:40:13,420 --> 00:40:22,320
only selecting from odd-numbered servers

00:40:19,500 --> 00:40:25,030
you can use regular expressions in

00:40:22,320 --> 00:40:26,710
aquaria in the where clause or you can

00:40:25,030 --> 00:40:28,870
use them in the from clause also if you

00:40:26,710 --> 00:40:39,190
wanted I don't the data that I've got

00:40:28,870 --> 00:40:46,390
here is not great for it we could you

00:40:39,190 --> 00:40:56,760
know select from sunni make this we

00:40:46,390 --> 00:40:56,760
could say let's see

00:40:57,770 --> 00:41:04,670
again this isn't great data for this

00:40:59,780 --> 00:41:06,800
example but we could say T or rx and you

00:41:04,670 --> 00:41:19,130
know select from series names or from

00:41:06,800 --> 00:41:21,320
measurement names that way let's see so

00:41:19,130 --> 00:41:22,370
that's all the examples I can think of

00:41:21,320 --> 00:41:24,410
off the top of my head I mean it's

00:41:22,370 --> 00:41:25,670
there's there's a lot more to it more

00:41:24,410 --> 00:41:30,260
than I have we have time to go through

00:41:25,670 --> 00:41:35,750
today let's take a take a quick look at

00:41:30,260 --> 00:41:37,880
the configuration file i don't think i

00:41:35,750 --> 00:41:44,330
have them installed on here see you less

00:41:37,880 --> 00:41:47,410
Oh actually I dropped out of the it's

00:41:44,330 --> 00:41:47,410
not the window I want anyway

00:42:04,700 --> 00:42:08,690
so it does when we were running we

00:42:07,190 --> 00:42:10,579
started it up from the command line in

00:42:08,690 --> 00:42:12,680
it so the run that way I didn't

00:42:10,579 --> 00:42:15,079
explicitly tell it to go use the config

00:42:12,680 --> 00:42:18,410
file so it just ran from the config it's

00:42:15,079 --> 00:42:20,060
in memory or built-in default config if

00:42:18,410 --> 00:42:22,490
had we actually launched the service it

00:42:20,060 --> 00:42:26,390
would run basically the same config you

00:42:22,490 --> 00:42:32,030
know installed in at least on Ubuntu

00:42:26,390 --> 00:42:36,349
system and in the normal config

00:42:32,030 --> 00:42:37,670
directory so we have you know pretty

00:42:36,349 --> 00:42:38,869
simple basically the stuff you would

00:42:37,670 --> 00:42:41,089
expect to be able to set you know

00:42:38,869 --> 00:42:49,250
addresses to bind to you can set the

00:42:41,089 --> 00:42:54,500
ports where the data comes in we do

00:42:49,250 --> 00:42:58,609
report anonymous usage statistics if you

00:42:54,500 --> 00:43:00,349
but you can shut that off basically

00:42:58,609 --> 00:43:01,790
that's just we're just trying to get an

00:43:00,349 --> 00:43:07,820
idea of how many people are actually

00:43:01,790 --> 00:43:09,859
installing and running it then there's

00:43:07,820 --> 00:43:11,390
some configuration options for join URLs

00:43:09,859 --> 00:43:16,940
if you're going to if you're going to

00:43:11,390 --> 00:43:18,550
run it in a cluster you would set up and

00:43:16,940 --> 00:43:23,150
I don't have enough time to go into

00:43:18,550 --> 00:43:26,180
setting up a cluster but we have broker

00:43:23,150 --> 00:43:29,990
nodes and data nodes typically you'll

00:43:26,180 --> 00:43:33,560
have a small number of brokers and a

00:43:29,990 --> 00:43:35,690
larger number of data nodes kind of as

00:43:33,560 --> 00:43:38,180
the name implies of data nodes is where

00:43:35,690 --> 00:43:43,339
the data lives and that's where the

00:43:38,180 --> 00:43:44,450
rights end up going into we you could

00:43:43,339 --> 00:43:46,400
see that we didn't do any sort of

00:43:44,450 --> 00:43:48,619
authentication while we were doing

00:43:46,400 --> 00:43:52,220
queries that by default it comes up with

00:43:48,619 --> 00:43:54,140
it disabled but you can can enable with

00:43:52,220 --> 00:43:59,960
authentication create users set

00:43:54,140 --> 00:44:03,170
passwords and that kind of thing then

00:43:59,960 --> 00:44:04,730
there's the admin interface config

00:44:03,170 --> 00:44:08,170
settings that was how we got to the

00:44:04,730 --> 00:44:08,170
little web page that's embedded in it

00:44:13,990 --> 00:44:21,950
alright so we have some plugins support

00:44:18,530 --> 00:44:25,910
for like graphite collect the open TSD

00:44:21,950 --> 00:44:28,580
be and there's there's more coming we're

00:44:25,910 --> 00:44:35,240
working on we're just starting to work

00:44:28,580 --> 00:44:40,450
on more collection some config settings

00:44:35,240 --> 00:44:45,290
for broker nodes data nodes logging

00:44:40,450 --> 00:44:47,600
snapshotting and that's that's it I mean

00:44:45,290 --> 00:44:50,150
there's really not a lot to it you

00:44:47,600 --> 00:44:52,190
install it come in tweak some config

00:44:50,150 --> 00:44:56,570
settings there's documentation on the

00:44:52,190 --> 00:44:58,820
website you know please if you if you're

00:44:56,570 --> 00:45:00,740
interested in playing around with it go

00:44:58,820 --> 00:45:02,390
through the config file go through the

00:45:00,740 --> 00:45:05,000
docs if there's anything missing let us

00:45:02,390 --> 00:45:07,550
know or if it anything confusing that we

00:45:05,000 --> 00:45:09,770
could do better in the docs and of

00:45:07,550 --> 00:45:15,350
course it's open source we we welcome

00:45:09,770 --> 00:45:18,830
PRS we have it's a pretty active project

00:45:15,350 --> 00:45:20,870
it's been open source developments been

00:45:18,830 --> 00:45:23,060
going for just over two years now we've

00:45:20,870 --> 00:45:26,840
had about 5,000 commits and about a

00:45:23,060 --> 00:45:29,360
hundred contributors a little over 100

00:45:26,840 --> 00:45:31,190
contributors so it's a pre active

00:45:29,360 --> 00:45:35,260
project and we do you know we're very

00:45:31,190 --> 00:45:39,020
friendly we're not not going to get any

00:45:35,260 --> 00:45:40,490
ranting torvalds emails back from us you

00:45:39,020 --> 00:45:43,990
know we we really do welcome and

00:45:40,490 --> 00:45:43,990
encourage everybody to help us out

00:45:45,970 --> 00:45:50,690
that's all I'll cover for demo unless

00:45:48,770 --> 00:45:53,000
anybody has any specific questions about

00:45:50,690 --> 00:45:57,250
query language or anything while we're

00:45:53,000 --> 00:45:57,250
in here okay

00:46:08,320 --> 00:46:21,460
so thank you that's it and we do have a

00:46:19,420 --> 00:46:34,390
few minutes if anybody has questions in

00:46:21,460 --> 00:46:36,100
general about influx I think if you go

00:46:34,390 --> 00:46:38,320
actually so that's a that's a really

00:46:36,100 --> 00:46:40,690
good question if you just hit n flux DB

00:46:38,320 --> 00:46:43,890
website and click on the document link

00:46:40,690 --> 00:46:46,120
it takes you to the 08 documentation

00:46:43,890 --> 00:46:49,300
yeah and you'll see that up in the URL

00:46:46,120 --> 00:46:52,420
it'll say okay up there and the

00:46:49,300 --> 00:46:56,830
clustering there was definitely not

00:46:52,420 --> 00:47:00,430
production level we're getting pretty

00:46:56,830 --> 00:47:03,490
close with 09 the release hasn't come

00:47:00,430 --> 00:47:06,940
out yet but we've got one of our larger

00:47:03,490 --> 00:47:09,340
users that we work with on a daily basis

00:47:06,940 --> 00:47:11,890
and we're we're getting the clustering

00:47:09,340 --> 00:47:13,780
stabilized so that should be pretty soon

00:47:11,890 --> 00:47:16,510
you can go in there may be a better way

00:47:13,780 --> 00:47:18,460
to get to it but I always just click on

00:47:16,510 --> 00:47:20,580
the docks link and then change the OA to

00:47:18,460 --> 00:47:22,390
an 09 if you want to get to the 09

00:47:20,580 --> 00:47:31,120
documentation I'm sure there's a better

00:47:22,390 --> 00:47:33,490
way what's that oh really hmm ok bad

00:47:31,120 --> 00:47:37,710
advice I'll see you after the talk and

00:47:33,490 --> 00:47:37,710
we'll figure it out another one

00:47:47,350 --> 00:47:53,990
so the question is is I'm sorry yes so

00:47:51,710 --> 00:47:56,330
the question is is do we have any right

00:47:53,990 --> 00:48:03,340
buffering going on was that it was a

00:47:56,330 --> 00:48:07,130
more to it or was that there's no

00:48:03,340 --> 00:48:08,900
there's no buffering going on so then we

00:48:07,130 --> 00:48:12,200
encourage and maybe I don't fully

00:48:08,900 --> 00:48:14,870
understand the question but if for

00:48:12,200 --> 00:48:17,000
performance we we encourage you to batch

00:48:14,870 --> 00:48:20,720
the rights up and send many points in

00:48:17,000 --> 00:48:22,220
one in a single right you're you're the

00:48:20,720 --> 00:48:24,230
performance will be or the throughput

00:48:22,220 --> 00:48:26,960
would be much higher if you're writing

00:48:24,230 --> 00:48:29,690
you know a thousand or 5,000 points at a

00:48:26,960 --> 00:48:32,690
time as opposed to writing single values

00:48:29,690 --> 00:48:35,440
at a time is that does that answer your

00:48:32,690 --> 00:48:35,440
question or no

00:48:44,570 --> 00:48:50,750
oh no rights can come in through any any

00:48:48,800 --> 00:48:55,520
node and then it will figure out where

00:48:50,750 --> 00:48:58,220
the right needs to be routed to so as

00:48:55,520 --> 00:48:59,600
you know as part of the scaling you know

00:48:58,220 --> 00:49:02,030
we're the performance numbers that I was

00:48:59,600 --> 00:49:04,330
talking about earlier 1.2 million points

00:49:02,030 --> 00:49:09,470
per minute is with a single node and

00:49:04,330 --> 00:49:11,690
then we expect that the scale hopefully

00:49:09,470 --> 00:49:14,510
linearly with with nodes as close as we

00:49:11,690 --> 00:49:18,350
can get so you know we would we like to

00:49:14,510 --> 00:49:24,400
have 10 nodes writing 12 million points

00:49:18,350 --> 00:49:24,400
per minute more questions

00:49:33,410 --> 00:49:38,280
okay so the question is could I give an

00:49:36,330 --> 00:49:45,300
example of how the retention policies

00:49:38,280 --> 00:49:47,490
work let me real quick well I probably

00:49:45,300 --> 00:49:53,850
don't have time to do it basically you

00:49:47,490 --> 00:49:57,810
would you would go in say let's say that

00:49:53,850 --> 00:50:01,830
you had CPU readings collected every 10

00:49:57,810 --> 00:50:04,410
seconds and you wanted to create a and

00:50:01,830 --> 00:50:06,420
you're only holding that for you know a

00:50:04,410 --> 00:50:08,670
week those readings because you're

00:50:06,420 --> 00:50:10,590
collecting them so fast but let's say

00:50:08,670 --> 00:50:13,680
for a longer term you wanted to move

00:50:10,590 --> 00:50:15,900
that into like longer term retention

00:50:13,680 --> 00:50:17,190
policy you could say go into the query

00:50:15,900 --> 00:50:23,310
language you know on the command line

00:50:17,190 --> 00:50:26,640
and say create retention policy CPU six

00:50:23,310 --> 00:50:31,320
months you give it a name like that CPU

00:50:26,640 --> 00:50:35,970
six months on database foo duration

00:50:31,320 --> 00:50:37,710
equal 6 months 6 mm would be six more

00:50:35,970 --> 00:50:42,840
six captain whatever does I've forgotten

00:50:37,710 --> 00:50:44,520
the units for it there and then a

00:50:42,840 --> 00:50:45,960
replication factory and that's it so you

00:50:44,520 --> 00:50:49,140
give it that one command and now you've

00:50:45,960 --> 00:50:52,950
created a retention policy named CPU six

00:50:49,140 --> 00:50:57,050
months and so you could you can select

00:50:52,950 --> 00:50:59,340
from your higher precision into that one

00:50:57,050 --> 00:51:02,840
or you could write a continuous query

00:50:59,340 --> 00:51:05,820
that automatically down samples the data

00:51:02,840 --> 00:51:08,730
into it like you compute the the mean or

00:51:05,820 --> 00:51:12,620
the average of the CPUs for that period

00:51:08,730 --> 00:51:14,580
of time into the longer retention policy

00:51:12,620 --> 00:51:17,580
and I don't know if that answers your

00:51:14,580 --> 00:51:21,080
question under the covers a retention

00:51:17,580 --> 00:51:26,330
policy the shards actually belonged to

00:51:21,080 --> 00:51:28,530
the the retention policy so when the

00:51:26,330 --> 00:51:30,540
collector in the background is going

00:51:28,530 --> 00:51:34,170
going around every now and then cleaning

00:51:30,540 --> 00:51:36,950
up retention you know data retention it

00:51:34,170 --> 00:51:39,510
looks at the retention policy and says

00:51:36,950 --> 00:51:43,140
okay this retention policies up for

00:51:39,510 --> 00:51:45,150
collection what shards does it own and

00:51:43,140 --> 00:51:46,619
then it goes out and cleans up those

00:51:45,150 --> 00:51:53,359
you know that data is old so it cleans

00:51:46,619 --> 00:51:53,359
them up that is that

00:51:57,940 --> 00:52:28,990
uh bit dependency yes no no you don't

00:52:23,589 --> 00:52:30,819
have to do that more questions are we

00:52:28,990 --> 00:52:39,849
hid one we got a few more minutes do we

00:52:30,819 --> 00:52:46,450
have any further questions the question

00:52:39,849 --> 00:52:50,109
is is there a way to export the data off

00:52:46,450 --> 00:52:53,250
the top of my head I don't know in what

00:52:50,109 --> 00:52:53,250
type of format like

00:53:01,500 --> 00:53:06,670
not yet I and I don't there's nothing in

00:53:04,630 --> 00:53:17,320
the immediate plans for that that I know

00:53:06,670 --> 00:53:20,500
of you can is the next question you're

00:53:17,320 --> 00:53:23,710
going to ask me how that works no you

00:53:20,500 --> 00:53:32,500
can I actually don't know if that works

00:53:23,710 --> 00:53:34,210
yet though you can do it though yeah yes

00:53:32,500 --> 00:53:43,510
but the answer is yes that is definitely

00:53:34,210 --> 00:53:49,770
going to work yeah i hope i hope that

00:53:43,510 --> 00:53:49,770
works in 09 anymore

00:54:14,630 --> 00:54:24,690
so the question is are all the the

00:54:18,780 --> 00:54:27,600
inputs for data the same speed the

00:54:24,690 --> 00:54:31,550
answer in 08 was definitely not the

00:54:27,600 --> 00:54:34,830
reason being the collect d for instance

00:54:31,550 --> 00:54:40,010
came through a custom interface that

00:54:34,830 --> 00:54:40,010
skipped over the Jason deserialization

00:54:41,870 --> 00:54:48,270
in 09 I'm not sure I didn't I didn't I

00:54:46,590 --> 00:54:50,070
didn't personally make that change and I

00:54:48,270 --> 00:54:53,310
haven't looked at the collecti the

00:54:50,070 --> 00:54:56,370
collecti was a contribution from from

00:54:53,310 --> 00:54:58,800
the open source community in 08 one of

00:54:56,370 --> 00:55:02,550
our guys I think went in and made it

00:54:58,800 --> 00:55:04,200
work in 09 also but I know that that

00:55:02,550 --> 00:55:06,450
would have taken probably a significant

00:55:04,200 --> 00:55:09,210
amount of work when he did it I don't

00:55:06,450 --> 00:55:11,490
know if he changed it to go through the

00:55:09,210 --> 00:55:13,890
the JSON interface and if he did then

00:55:11,490 --> 00:55:19,200
then the speed is going to be about the

00:55:13,890 --> 00:55:21,240
same if he left it as a binary interface

00:55:19,200 --> 00:55:23,850
in some way then then it's still going

00:55:21,240 --> 00:55:28,170
to be much faster I know from profiling

00:55:23,850 --> 00:55:30,690
the database that we spend on at least

00:55:28,170 --> 00:55:32,490
on my laptop and on kind of typical

00:55:30,690 --> 00:55:34,740
hardware about fifty percent of the

00:55:32,490 --> 00:55:38,010
processing power right now is spent

00:55:34,740 --> 00:55:39,810
deserializing Jason when you're when

00:55:38,010 --> 00:55:42,030
you're so that's low hanging fruit mean

00:55:39,810 --> 00:55:44,760
the good news is is that's that's like

00:55:42,030 --> 00:55:47,040
this big glaring you know or big shiny

00:55:44,760 --> 00:55:49,470
piece of low-hanging fruit for

00:55:47,040 --> 00:55:51,510
performance for us and we you know we

00:55:49,470 --> 00:55:56,240
know that we've got a number of ideas

00:55:51,510 --> 00:55:56,240
about how to make it go quicker so

00:56:04,980 --> 00:56:12,370
well I'm not another right I mean for

00:56:09,670 --> 00:56:15,340
from a performance point of view binary

00:56:12,370 --> 00:56:17,950
is quicker I'm not sure if it supports a

00:56:15,340 --> 00:56:24,400
binary interface in 09 it depends on how

00:56:17,950 --> 00:56:30,640
that conversion was done a couple more

00:56:24,400 --> 00:56:33,190
minutes anybody else maybe I have one

00:56:30,640 --> 00:56:36,010
last question sure when using graphite

00:56:33,190 --> 00:56:39,700
web we have a lot of mathematical

00:56:36,010 --> 00:56:42,430
functions we can use to transform our

00:56:39,700 --> 00:56:44,440
data points does influx to be provide

00:56:42,430 --> 00:56:47,110
something like that I'm glad you asked

00:56:44,440 --> 00:56:48,970
that question and I don't know all the

00:56:47,110 --> 00:56:54,480
functions the graphite provides but

00:56:48,970 --> 00:56:56,890
there are we have things like you know

00:56:54,480 --> 00:56:58,870
derivative and I don't actually I don't

00:56:56,890 --> 00:57:03,130
think derivatives implemented yet but I

00:56:58,870 --> 00:57:06,360
think it will be by 09 and derivatives

00:57:03,130 --> 00:57:10,030
and percentiles and psalms and means and

00:57:06,360 --> 00:57:12,730
many statistical is that what you're

00:57:10,030 --> 00:57:17,650
asking those types of functions yes yes

00:57:12,730 --> 00:57:20,980
and when we rewrote it 409 we one of the

00:57:17,650 --> 00:57:22,960
problems in 08 was with the architecture

00:57:20,980 --> 00:57:24,490
of it it was it's fairly difficult to

00:57:22,960 --> 00:57:26,980
add some of those things we struggled

00:57:24,490 --> 00:57:29,050
anybody that used 08 a lot knows that we

00:57:26,980 --> 00:57:31,450
struggled a lot with derivatives trying

00:57:29,050 --> 00:57:35,320
to get that to work right you know do

00:57:31,450 --> 00:57:37,180
you some people want a derivative of you

00:57:35,320 --> 00:57:39,970
know from just from the from this point

00:57:37,180 --> 00:57:42,370
to the last point and some people want a

00:57:39,970 --> 00:57:44,380
derivative from the first point in this

00:57:42,370 --> 00:57:46,540
bucket in the last point of the previous

00:57:44,380 --> 00:57:48,280
bucket some people wanted the derivative

00:57:46,540 --> 00:57:51,010
from the first point in the bucket to

00:57:48,280 --> 00:57:53,380
the end of the bucket you know when

00:57:51,010 --> 00:57:55,300
you're grouping by time so the old

00:57:53,380 --> 00:57:57,220
engine didn't handle that it was not

00:57:55,300 --> 00:57:59,680
easy to write that code in the old

00:57:57,220 --> 00:58:02,620
engine the new engine handles this much

00:57:59,680 --> 00:58:04,450
better so i think the number of

00:58:02,620 --> 00:58:09,750
functions that we support will will grow

00:58:04,450 --> 00:58:09,750
pretty quickly as once we get 09 out

00:58:12,480 --> 00:58:21,910
so the question is when will 09 be

00:58:15,100 --> 00:58:24,940
released today is April 22nd I think the

00:58:21,910 --> 00:58:30,460
goal is to have it out at at the end of

00:58:24,940 --> 00:58:32,470
april so I have to check you know check

00:58:30,460 --> 00:58:36,160
with home base and see how the guys have

00:58:32,470 --> 00:58:38,740
done the last few days since I left it's

00:58:36,160 --> 00:58:41,980
an aggressive goal the end of april but

00:58:38,740 --> 00:58:44,910
we are getting close our main thing is

00:58:41,980 --> 00:58:47,530
trying to get clustering stabilized and

00:58:44,910 --> 00:58:55,510
distributed queries that was another

00:58:47,530 --> 00:58:58,180
topic i didn't cover yeah okay I think

00:58:55,510 --> 00:59:01,920
that's it thank you everybody coming

00:58:58,180 --> 00:59:01,920

YouTube URL: https://www.youtube.com/watch?v=slTNVGUxyb4


