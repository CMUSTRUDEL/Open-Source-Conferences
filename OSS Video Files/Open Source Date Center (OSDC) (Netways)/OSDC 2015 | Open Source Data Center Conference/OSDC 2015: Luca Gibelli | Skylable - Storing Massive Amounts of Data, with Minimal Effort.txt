Title: OSDC 2015: Luca Gibelli | Skylable - Storing Massive Amounts of Data, with Minimal Effort
Publication date: 2015-04-30
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	Skylable provides an object storage solution which you can run on your own hardware. It provides data replication, deduplication, client-side encryption, and built-in data integrity checks.
Skylable scales easily to hundreds of TBs and it can be deployed in literally 2 minutes by anyone with a minimal experience of Linux administration.
The installation and setup process is straightforward and requires very few dependencies.
Skylable is fully compatible with Amazon S3 APIs. It also offers a more advanced RESTful protocol called SX, which solves many limitations of the S3 protocol, most importantly the ability to upload/download files in parallel from multiple hosts and true support for resuming transfers.
The software is released under the GPL licence, the client library (libsx) is LGPL.
This talk is about the ease of use of Skylable, a comparison of S3 vs SX and the real-world advantage of doing deduplication and multiplexed transfers.
Captions: 
	00:00:06,500 --> 00:00:12,770
okay so I guess we can start first of

00:00:10,880 --> 00:00:17,210
all thanks for coming and look at you

00:00:12,770 --> 00:00:20,500
badly founder of scalable we're doing a

00:00:17,210 --> 00:00:24,619
distributed object storage open source

00:00:20,500 --> 00:00:27,080
so today's presentation is about the

00:00:24,619 --> 00:00:29,090
limits of storing massive amounts of

00:00:27,080 --> 00:00:33,710
data in a traditional storage area

00:00:29,090 --> 00:00:36,890
network and the limits of using the s3

00:00:33,710 --> 00:00:41,059
protocol or an s3 compatible cloud i

00:00:36,890 --> 00:00:43,190
will show you why scatter ball sex is

00:00:41,059 --> 00:00:47,750
better what we do better what we tried

00:00:43,190 --> 00:00:51,680
to fix and hopefully succeeded and some

00:00:47,750 --> 00:00:54,770
use cases for our software so first of

00:00:51,680 --> 00:00:56,629
all a bit about our background the team

00:00:54,770 --> 00:01:02,000
behind the scalable is the same team

00:00:56,629 --> 00:01:04,489
that founded clamavi in 2002 2003 there

00:01:02,000 --> 00:01:08,270
are the forefront but all four founders

00:01:04,489 --> 00:01:12,200
come from clamavi you probably know

00:01:08,270 --> 00:01:15,340
clamav it's the most famous and widely

00:01:12,200 --> 00:01:19,490
use the antivirus open-source antivirus

00:01:15,340 --> 00:01:23,240
it's deployed on millions of servers

00:01:19,490 --> 00:01:25,939
protecting billions of mailboxes it's

00:01:23,240 --> 00:01:29,570
used by Google for Gmail for Google

00:01:25,939 --> 00:01:32,150
Drive it's used even by Facebook to scan

00:01:29,570 --> 00:01:34,820
the pictures that we upload to your

00:01:32,150 --> 00:01:39,829
facebook account to find if you're using

00:01:34,820 --> 00:01:42,710
steganography to hide some pirated

00:01:39,829 --> 00:01:48,470
software or nasty stuff into your images

00:01:42,710 --> 00:01:51,649
to leverage facebook CDN it we were

00:01:48,470 --> 00:01:56,210
acquired by 2018 2007 by sourcefire

00:01:51,649 --> 00:02:00,079
which later in 2012 2013 was acquired by

00:01:56,210 --> 00:02:01,820
cisco so now clamavi is maintained and

00:02:00,079 --> 00:02:04,100
developed by Cisco they made a

00:02:01,820 --> 00:02:06,530
commitment to keep it open source and

00:02:04,100 --> 00:02:08,179
keep the development active so you don't

00:02:06,530 --> 00:02:11,630
have to worry if you're running clamavi

00:02:08,179 --> 00:02:18,440
on your servers it will not go away

00:02:11,630 --> 00:02:20,480
anytime soon so from we made when we

00:02:18,440 --> 00:02:22,010
gotta quit when satisfied gotta park

00:02:20,480 --> 00:02:25,640
battery before sort of I've got acquired

00:02:22,010 --> 00:02:27,549
by Cisco we decided to move to a new

00:02:25,640 --> 00:02:30,230
project to do something new after

00:02:27,549 --> 00:02:32,930
developing clamavi for 10 years we felt

00:02:30,230 --> 00:02:37,370
that it was time to look around and find

00:02:32,930 --> 00:02:41,480
other problems to solve so we went we

00:02:37,370 --> 00:02:46,390
found it scalable with a mission to

00:02:41,480 --> 00:02:49,220
develop an easy-to-use and secure

00:02:46,390 --> 00:02:52,459
distributed the storage we bootstrapped

00:02:49,220 --> 00:02:55,160
for one year and about one year one year

00:02:52,459 --> 00:02:57,140
and also later we got to sit around by

00:02:55,160 --> 00:03:00,319
sandstone capital you might know

00:02:57,140 --> 00:03:04,549
sandstone capital if you are into graph

00:03:00,319 --> 00:03:06,859
databases so graph databases are that

00:03:04,549 --> 00:03:10,250
thing that tells you you like this so

00:03:06,859 --> 00:03:14,030
you may like also order on for example

00:03:10,250 --> 00:03:15,980
on netflix or when you go on linkedin

00:03:14,030 --> 00:03:18,470
and you are connected to somebody and

00:03:15,980 --> 00:03:20,329
you see you are connected to this guy so

00:03:18,470 --> 00:03:23,590
you might know also this person so all

00:03:20,329 --> 00:03:28,190
that stuff comes from graph database and

00:03:23,590 --> 00:03:33,919
the most famous one the which is also

00:03:28,190 --> 00:03:40,130
open source is neo4j which is also was

00:03:33,919 --> 00:03:42,560
also founded by sandstone so how did it

00:03:40,130 --> 00:03:45,260
happen how we moved why did we move from

00:03:42,560 --> 00:03:47,900
cyber security from the antivirus market

00:03:45,260 --> 00:03:49,669
to the storage market well one of the

00:03:47,900 --> 00:03:52,250
most pressing issue that we had when

00:03:49,669 --> 00:03:54,769
we're developing clamavi was storing the

00:03:52,250 --> 00:03:58,130
malware so we're getting around 100

00:03:54,769 --> 00:04:02,810
gigabytes of solid 10 gigabytes of new

00:03:58,130 --> 00:04:05,030
unique malware per day so every time we

00:04:02,810 --> 00:04:07,489
made a database update with the new

00:04:05,030 --> 00:04:11,299
signatures we had to analyze older all

00:04:07,489 --> 00:04:14,329
the signatures and also the old the old

00:04:11,299 --> 00:04:16,130
malware samples and the new ones and we

00:04:14,329 --> 00:04:21,169
added to do it quickly because we had to

00:04:16,130 --> 00:04:25,070
react quickly sorry the Wi-Fi is taking

00:04:21,169 --> 00:04:28,610
over we had react quickly to new

00:04:25,070 --> 00:04:31,789
outbreaks of malware so we added to have

00:04:28,610 --> 00:04:34,430
a fast access to a huge collection of

00:04:31,789 --> 00:04:38,030
malware and that was art

00:04:34,430 --> 00:04:40,880
so we started with netapp storage area

00:04:38,030 --> 00:04:44,900
network which was which soon became too

00:04:40,880 --> 00:04:49,040
small so it took almost 12 months to get

00:04:44,900 --> 00:04:50,690
an upgraded model so we simply realized

00:04:49,040 --> 00:04:53,210
that it was not working out and

00:04:50,690 --> 00:04:55,850
especially speed was not that great it

00:04:53,210 --> 00:04:57,800
was not as fast as we wanted so we

00:04:55,850 --> 00:05:00,110
always said the problem that we were

00:04:57,800 --> 00:05:01,850
adding more shelves more shells but you

00:05:00,110 --> 00:05:04,460
can add more shelves up to some point

00:05:01,850 --> 00:05:06,980
right at some point you just have to buy

00:05:04,460 --> 00:05:10,970
the newer model you cannot keep adding

00:05:06,980 --> 00:05:13,370
floors and so we started looking around

00:05:10,970 --> 00:05:17,420
for open source solution for distributed

00:05:13,370 --> 00:05:21,380
storage and we try the most of them and

00:05:17,420 --> 00:05:24,830
we were not really happy with the

00:05:21,380 --> 00:05:28,610
results most of them were complicated to

00:05:24,830 --> 00:05:31,670
set up and to maintain some of them were

00:05:28,610 --> 00:05:34,370
not well most of them were also not fast

00:05:31,670 --> 00:05:36,830
enough for our needs so we started

00:05:34,370 --> 00:05:39,830
analyzing the implementation decisions

00:05:36,830 --> 00:05:42,710
of each project since we were open

00:05:39,830 --> 00:05:46,460
source developers and the software walls

00:05:42,710 --> 00:05:48,230
open source we add a look at how where

00:05:46,460 --> 00:05:53,450
were the bottlenecks were were the

00:05:48,230 --> 00:05:56,090
problems and we gained some know-how in

00:05:53,450 --> 00:05:58,280
the field while developing clamavi but

00:05:56,090 --> 00:06:03,050
simply with the entertainment back then

00:05:58,280 --> 00:06:05,330
to to fix them or to help with the

00:06:03,050 --> 00:06:07,610
development of that project so when we

00:06:05,330 --> 00:06:14,690
decided to start a new project we add

00:06:07,610 --> 00:06:17,180
some background in the storage arena and

00:06:14,690 --> 00:06:20,690
we used it we decided to start something

00:06:17,180 --> 00:06:22,370
new because we thought that the the

00:06:20,690 --> 00:06:27,440
software that was ready available I'd

00:06:22,370 --> 00:06:31,100
made some assumptions that were not

00:06:27,440 --> 00:06:34,220
really needed for our use case so we we

00:06:31,100 --> 00:06:36,320
started looking at how some projects

00:06:34,220 --> 00:06:38,030
were doing things and why they had come

00:06:36,320 --> 00:06:40,010
up with that design and we realized that

00:06:38,030 --> 00:06:43,100
some of those things were not really

00:06:40,010 --> 00:06:44,720
requirement for our use case we asked a

00:06:43,100 --> 00:06:46,250
few questions around and we found out

00:06:44,720 --> 00:06:48,320
that there were other people with the

00:06:46,250 --> 00:06:50,540
same use cases us

00:06:48,320 --> 00:06:53,330
that didn't need those assumptions as

00:06:50,540 --> 00:06:55,460
well so what were those assumption well

00:06:53,330 --> 00:06:59,410
it's all about the cap theorem the club

00:06:55,460 --> 00:07:03,620
cheering as you probably already know

00:06:59,410 --> 00:07:05,360
says that you cannot have all of these

00:07:03,620 --> 00:07:07,250
things you cannot have consistency and

00:07:05,360 --> 00:07:08,930
availability and partition tolerance at

00:07:07,250 --> 00:07:15,440
the same time you have to choose two of

00:07:08,930 --> 00:07:17,150
them and what are the different choices

00:07:15,440 --> 00:07:21,230
that you can eat the most common well

00:07:17,150 --> 00:07:26,150
block storage chooses CA consistency and

00:07:21,230 --> 00:07:29,360
availability it allows to create it's

00:07:26,150 --> 00:07:33,320
mainly treated to make structured to

00:07:29,360 --> 00:07:36,610
store structure at the data it turned it

00:07:33,320 --> 00:07:39,260
as a consequence of guaranteeing

00:07:36,610 --> 00:07:44,240
consistency and availability it has to

00:07:39,260 --> 00:07:47,050
be complex and it's also often well

00:07:44,240 --> 00:07:50,660
expensive in terms of time and resources

00:07:47,050 --> 00:07:53,780
to run file system storage also

00:07:50,660 --> 00:07:59,830
guarantees consistency and availability

00:07:53,780 --> 00:08:03,140
so similar story object storage is that

00:07:59,830 --> 00:08:04,820
aims at guaranteeing availability and

00:08:03,140 --> 00:08:09,520
partition tolerance so the idea is that

00:08:04,820 --> 00:08:13,850
you sacrifice consistency and get

00:08:09,520 --> 00:08:16,480
something that is always available and

00:08:13,850 --> 00:08:20,930
which can sustain the dead of the node

00:08:16,480 --> 00:08:23,030
so where is object storage most useful

00:08:20,930 --> 00:08:25,330
well when you have to store really huge

00:08:23,030 --> 00:08:28,840
amount of data and structure at the data

00:08:25,330 --> 00:08:34,010
and when you want to use inexpensive

00:08:28,840 --> 00:08:35,360
hardware so typically you get with

00:08:34,010 --> 00:08:38,450
object storage you get the scalability

00:08:35,360 --> 00:08:40,969
durability low price and it's best

00:08:38,450 --> 00:08:44,960
suited for backups storing log files

00:08:40,969 --> 00:08:51,260
long-term archival documents very large

00:08:44,960 --> 00:08:52,790
five media files so typically when you

00:08:51,260 --> 00:08:55,250
start a new project the first thing that

00:08:52,790 --> 00:08:56,960
you think is I need a large file system

00:08:55,250 --> 00:09:00,110
and I need a larger storage I'd need a

00:08:56,960 --> 00:09:02,030
large file system but this question is a

00:09:00,110 --> 00:09:04,190
give is not even given

00:09:02,030 --> 00:09:07,100
much thought usually and this in my in

00:09:04,190 --> 00:09:10,310
our opinion was a big mistake so you

00:09:07,100 --> 00:09:12,080
should focus on rhe your requirements

00:09:10,310 --> 00:09:13,790
and ask yourself do I really need all

00:09:12,080 --> 00:09:16,820
the features that come with the file

00:09:13,790 --> 00:09:18,980
systems with five systems or can I

00:09:16,820 --> 00:09:21,800
survive with an object storage because

00:09:18,980 --> 00:09:27,590
that makes a huge difference in the long

00:09:21,800 --> 00:09:32,510
term again in terms of costs running

00:09:27,590 --> 00:09:38,420
costs and also in terms of complexity of

00:09:32,510 --> 00:09:40,550
the setup so and what are the instead of

00:09:38,420 --> 00:09:42,530
the arguments against using computer

00:09:40,550 --> 00:09:44,150
storage well it's not that easy to use

00:09:42,530 --> 00:09:47,600
an object storage for your project

00:09:44,150 --> 00:09:50,030
because it requires support at the

00:09:47,600 --> 00:09:53,900
application layer so you need to arrange

00:09:50,030 --> 00:09:55,640
an aerial application to use a few

00:09:53,900 --> 00:09:58,010
operations you are not you don't have

00:09:55,640 --> 00:09:59,750
the flexibility of a file system so the

00:09:58,010 --> 00:10:02,630
typical operation that you can perform

00:09:59,750 --> 00:10:06,110
on on your virtual storage are put at

00:10:02,630 --> 00:10:08,780
least delete you have to give up on

00:10:06,110 --> 00:10:11,000
append so you cannot simply modify a

00:10:08,780 --> 00:10:14,270
file or append the data to a file not

00:10:11,000 --> 00:10:17,660
easily you have to give up on locking so

00:10:14,270 --> 00:10:19,670
you cannot rely on the file system to

00:10:17,660 --> 00:10:22,880
tell you if another application is

00:10:19,670 --> 00:10:27,200
obsessing your files and you lose file

00:10:22,880 --> 00:10:29,570
hierarchy so you all the namespace is

00:10:27,200 --> 00:10:31,640
flat when you want to use an object

00:10:29,570 --> 00:10:33,910
storage you see directories but they are

00:10:31,640 --> 00:10:36,800
not really directories Joe they're just

00:10:33,910 --> 00:10:39,770
fake director is made by extending the

00:10:36,800 --> 00:10:44,840
name by accepting slashes in the file

00:10:39,770 --> 00:10:47,060
name right so the listing volume of an

00:10:44,840 --> 00:10:49,250
object storage takes a lot of time

00:10:47,060 --> 00:10:51,260
typically because you are actually

00:10:49,250 --> 00:10:53,420
listing all the files in that volume

00:10:51,260 --> 00:10:56,410
even if you are listing a subdirectory

00:10:53,420 --> 00:11:04,970
because again there is no concept of

00:10:56,410 --> 00:11:07,250
directories so now what what happens

00:11:04,970 --> 00:11:09,590
when so what happens when you use an

00:11:07,250 --> 00:11:11,840
object storage as we said there is no

00:11:09,590 --> 00:11:15,220
concept of appending the file and there

00:11:11,840 --> 00:11:19,839
is no concept of

00:11:15,220 --> 00:11:23,920
of sorry there is no concept of a

00:11:19,839 --> 00:11:26,350
pending file and no locking so what

00:11:23,920 --> 00:11:28,420
happens when you use a typical object

00:11:26,350 --> 00:11:33,069
storage all the style has three like

00:11:28,420 --> 00:11:37,990
obvious storage well you run the risk of

00:11:33,069 --> 00:11:39,610
losing a cop a specific provision of

00:11:37,990 --> 00:11:41,920
your file unless you enable the

00:11:39,610 --> 00:11:43,540
revisions obviously because you might

00:11:41,920 --> 00:11:47,379
write tweet from two different

00:11:43,540 --> 00:11:50,680
applications and you have to always

00:11:47,379 --> 00:11:52,750
upload the whole file so if you just

00:11:50,680 --> 00:11:55,899
change one bite in the file you have to

00:11:52,750 --> 00:11:58,839
overwrite the whole file this means you

00:11:55,899 --> 00:12:03,250
consume more bandwidth and it takes more

00:11:58,839 --> 00:12:07,060
time s3 does some s3 implementation do

00:12:03,250 --> 00:12:11,410
the duplication server side so when you

00:12:07,060 --> 00:12:13,120
actually store the file server side when

00:12:11,410 --> 00:12:16,959
you store multiple revisions you just

00:12:13,120 --> 00:12:21,250
take a little space you don't you don't

00:12:16,959 --> 00:12:23,470
write a full copy of the new revision of

00:12:21,250 --> 00:12:28,000
the file but you still have to transfer

00:12:23,470 --> 00:12:32,139
it completely right so we with our

00:12:28,000 --> 00:12:33,730
protocol SX we improve this with we

00:12:32,139 --> 00:12:36,389
improve the situation with the client

00:12:33,730 --> 00:12:39,699
side the duplication which means that if

00:12:36,389 --> 00:12:42,850
even a portion any portion of a file of

00:12:39,699 --> 00:12:45,459
an object sorry is already stored on the

00:12:42,850 --> 00:12:48,250
cluster you will never transfer it again

00:12:45,459 --> 00:12:50,680
so it's already there the server when

00:12:48,250 --> 00:12:53,350
the client has to upload a file it first

00:12:50,680 --> 00:12:55,269
ask the server do you already have this

00:12:53,350 --> 00:12:57,670
block do you already have this part of

00:12:55,269 --> 00:13:00,399
the file if yes I will not transfer it i

00:12:57,670 --> 00:13:03,550
will just send you the ash of that block

00:13:00,399 --> 00:13:06,100
so that you can put it in life in the

00:13:03,550 --> 00:13:08,889
map of my file but I don't have to

00:13:06,100 --> 00:13:11,230
upload it again so this translates into

00:13:08,889 --> 00:13:14,740
savings in terms of bandwidth time and

00:13:11,230 --> 00:13:19,959
also this space and how does it help

00:13:14,740 --> 00:13:23,009
with the lack of the append operation on

00:13:19,959 --> 00:13:25,149
orbiter storage well it effectively

00:13:23,009 --> 00:13:28,899
implements a kind of a panda because

00:13:25,149 --> 00:13:32,410
when you want to just modify a portion

00:13:28,899 --> 00:13:33,999
the file you just reuploaded off the

00:13:32,410 --> 00:13:36,009
whole file but the plaster will tell you

00:13:33,999 --> 00:13:37,720
that that part of the file that you

00:13:36,009 --> 00:13:42,670
didn't modify is already there and you

00:13:37,720 --> 00:13:45,220
will not transfer it again it helps with

00:13:42,670 --> 00:13:47,529
locking because you can keep as many

00:13:45,220 --> 00:13:51,249
revisions of the file as you want and

00:13:47,529 --> 00:13:53,709
they will take very little space on the

00:13:51,249 --> 00:13:55,689
actual storage because you will just

00:13:53,709 --> 00:14:02,399
store the differences between different

00:13:55,689 --> 00:14:08,290
revisions of the file yeah this is so

00:14:02,399 --> 00:14:11,410
how do we position scalable compared to

00:14:08,290 --> 00:14:13,059
our competitors well the thing that we

00:14:11,410 --> 00:14:15,429
didn't like normal competitors is

00:14:13,059 --> 00:14:17,819
anticipated our where that the most of

00:14:15,429 --> 00:14:22,420
them were complex to install and run

00:14:17,819 --> 00:14:26,889
they only supported the s3 protocol and

00:14:22,420 --> 00:14:32,199
the diva didn't have high transfers

00:14:26,889 --> 00:14:36,040
resumes incremental resumes to turn off

00:14:32,199 --> 00:14:39,970
Wi-Fi and none of them supported the

00:14:36,040 --> 00:14:41,740
duplication natively so yeah there were

00:14:39,970 --> 00:14:44,800
some attempts to integrate the

00:14:41,740 --> 00:14:47,290
duplication by using the butter FS which

00:14:44,800 --> 00:14:50,050
is really not stable not production

00:14:47,290 --> 00:14:52,749
ready especially it was not production

00:14:50,050 --> 00:14:55,149
ready for short two years ago recently

00:14:52,749 --> 00:14:58,899
they made some improvements but still

00:14:55,149 --> 00:15:03,220
it's quite risky to use so we built all

00:14:58,899 --> 00:15:05,559
these we made the setup of scalable as

00:15:03,220 --> 00:15:08,829
simple as possible our aim was to make a

00:15:05,559 --> 00:15:11,050
trip step installation process something

00:15:08,829 --> 00:15:17,860
that even a junior sysadmin could do in

00:15:11,050 --> 00:15:20,370
one minute and implement and add the

00:15:17,860 --> 00:15:25,689
duplication so these were our main

00:15:20,370 --> 00:15:27,490
differentiator so just to give you an

00:15:25,689 --> 00:15:31,990
idea of how simple it is to deploy

00:15:27,490 --> 00:15:33,850
scalable this is the type the only

00:15:31,990 --> 00:15:36,819
questions that you will be asked when

00:15:33,850 --> 00:15:39,129
you do a deployment so you start by

00:15:36,819 --> 00:15:41,589
setting up the first node on the first

00:15:39,129 --> 00:15:42,460
and when you so all nodes are created

00:15:41,589 --> 00:15:44,590
equal right

00:15:42,460 --> 00:15:46,780
the questions that you need to answer

00:15:44,590 --> 00:15:48,760
when you create the first node of the

00:15:46,780 --> 00:15:51,070
clusters are different from the others

00:15:48,760 --> 00:15:53,380
but all create all nodes are created

00:15:51,070 --> 00:15:56,170
equal so there is no concept of primary

00:15:53,380 --> 00:15:59,320
and secondary obviously and there is

00:15:56,170 --> 00:16:01,690
even no concept of prophecy nodes in

00:15:59,320 --> 00:16:03,340
storage nodes chunk nodes which you

00:16:01,690 --> 00:16:05,920
might be familiar if you have tried the

00:16:03,340 --> 00:16:12,100
surface with leads are the face for

00:16:05,920 --> 00:16:14,320
example so are we yes weaved so all

00:16:12,100 --> 00:16:17,140
nodes are created equal which means that

00:16:14,320 --> 00:16:19,650
it's simpler simpler to manage and to

00:16:17,140 --> 00:16:22,120
scale out because you don't have to

00:16:19,650 --> 00:16:25,600
think about creating a layer of

00:16:22,120 --> 00:16:28,320
prophecies a layer of storage nodes you

00:16:25,600 --> 00:16:31,870
just keep setting up notes the same way

00:16:28,320 --> 00:16:33,820
all the time so the first node when you

00:16:31,870 --> 00:16:35,680
set up the first node you just install

00:16:33,820 --> 00:16:38,580
of our package we have package

00:16:35,680 --> 00:16:43,170
repositories for the linux or majors

00:16:38,580 --> 00:16:50,170
distribution from centos-5 to center 6-7

00:16:43,170 --> 00:16:56,410
debian stable testing Ubuntu from 1204

00:16:50,170 --> 00:17:00,280
to the latest we support the Omnius if

00:16:56,410 --> 00:17:06,220
you like ZFS we and so on opensolaris

00:17:00,280 --> 00:17:09,850
illumos I mean we support the BSD yeah

00:17:06,220 --> 00:17:12,610
that's it we also run on multiple

00:17:09,850 --> 00:17:16,810
architectures so you can install a

00:17:12,610 --> 00:17:19,060
scalable on x86 commodity servers you

00:17:16,810 --> 00:17:21,670
can install it on Raspberry Pi on a

00:17:19,060 --> 00:17:23,550
cubby board we try to be the cluster

00:17:21,670 --> 00:17:27,430
with a couple of cubby board we got

00:17:23,550 --> 00:17:29,590
about four megabytes per second from

00:17:27,430 --> 00:17:31,960
each node which is not bad if you take

00:17:29,590 --> 00:17:35,650
into consideration that we were using SD

00:17:31,960 --> 00:17:43,230
cards as the backend storage and the

00:17:35,650 --> 00:17:43,230
cubby board is really cheap node so we

00:17:43,680 --> 00:17:55,630
we support the armor MIPS and x86 yes ah

00:17:51,550 --> 00:17:56,350
and also PowerPC if the software is

00:17:55,630 --> 00:17:58,809
written

00:17:56,350 --> 00:18:01,450
lynci so it's much faster than other

00:17:58,809 --> 00:18:05,470
similar software for example written in

00:18:01,450 --> 00:18:09,580
Python and it made it also very portable

00:18:05,470 --> 00:18:13,660
I mean we ported the same client also to

00:18:09,580 --> 00:18:15,490
Android in like a couple of hours never

00:18:13,660 --> 00:18:18,700
tried writing anything for Android but

00:18:15,490 --> 00:18:20,830
we got a request for that so we ported

00:18:18,700 --> 00:18:22,840
the libra sex our client library to

00:18:20,830 --> 00:18:25,450
android in a couple of hours so that

00:18:22,840 --> 00:18:27,580
that's the advantage when you start you

00:18:25,450 --> 00:18:29,559
know when you take the hard way when you

00:18:27,580 --> 00:18:32,230
instead of going the easy way with

00:18:29,559 --> 00:18:34,480
Python which seems easy but then you

00:18:32,230 --> 00:18:37,870
face the problems later especially

00:18:34,480 --> 00:18:40,450
performance programs we took the hard

00:18:37,870 --> 00:18:43,210
way with C by developing this in see it

00:18:40,450 --> 00:18:46,990
took some time took some more well much

00:18:43,210 --> 00:18:51,039
longer than we expected to develop it

00:18:46,990 --> 00:18:56,530
but it's paying off so back to the

00:18:51,039 --> 00:18:59,140
installation I diverse so to set up an

00:18:56,530 --> 00:19:03,309
old you just run a single comment sx

00:18:59,140 --> 00:19:06,760
setup you have to choose a dns name for

00:19:03,309 --> 00:19:09,460
the cluster i called it as X dot food

00:19:06,760 --> 00:19:11,559
com you have to store to choose the

00:19:09,460 --> 00:19:14,620
directory where you want to put the

00:19:11,559 --> 00:19:17,289
actual data you have to choose how much

00:19:14,620 --> 00:19:20,980
space you want to allocate on this node

00:19:17,289 --> 00:19:22,960
and the IP address and then of course

00:19:20,980 --> 00:19:25,150
you are asked if this is the first node

00:19:22,960 --> 00:19:27,850
of the cluster you just push enter the

00:19:25,150 --> 00:19:30,210
default value confirm all the settings

00:19:27,850 --> 00:19:32,770
and you have the claw at the first node

00:19:30,210 --> 00:19:36,929
of the cluster up and running which is

00:19:32,770 --> 00:19:36,929
already let me

00:19:43,180 --> 00:19:49,870
okay which is already usable you can

00:19:46,840 --> 00:19:51,730
already start uploading data because you

00:19:49,870 --> 00:19:54,700
can only create volumes with replicas

00:19:51,730 --> 00:19:56,610
one because you have only one node so

00:19:54,700 --> 00:19:58,390
this works differently from other

00:19:56,610 --> 00:20:01,290
projects which you might be familiar

00:19:58,390 --> 00:20:06,040
with like react CS which by default

00:20:01,290 --> 00:20:09,060
starts with creates it a replica tree

00:20:06,040 --> 00:20:13,210
volume even if you have like two nodes

00:20:09,060 --> 00:20:14,830
whatever reason so initially you can

00:20:13,210 --> 00:20:16,960
create volume solid with the replica one

00:20:14,830 --> 00:20:19,570
and start uploading data and then you

00:20:16,960 --> 00:20:21,640
can keep raw in the cluster you can add

00:20:19,570 --> 00:20:23,950
more nodes and create volumes with

00:20:21,640 --> 00:20:27,660
either replica counts so this is the

00:20:23,950 --> 00:20:30,250
setup of the second and following nodes

00:20:27,660 --> 00:20:33,280
again you have to enter the cluster name

00:20:30,250 --> 00:20:35,350
as X dot food com the directory where

00:20:33,280 --> 00:20:39,010
you want to store the data the size of

00:20:35,350 --> 00:20:42,340
the node the P address of this node the

00:20:39,010 --> 00:20:44,950
IP address of any other node of the

00:20:42,340 --> 00:20:48,430
cluster not necessary the first any

00:20:44,950 --> 00:20:51,970
other existing node and admin key which

00:20:48,430 --> 00:20:55,480
is the actual authentication that is

00:20:51,970 --> 00:20:58,390
used to connect to join the cluster and

00:20:55,480 --> 00:21:03,880
that's it then if you use self-signed

00:20:58,390 --> 00:21:05,980
certificates you will be shown a the

00:21:03,880 --> 00:21:10,800
finger bit of this certificate for

00:21:05,980 --> 00:21:13,330
better security so you can you can

00:21:10,800 --> 00:21:16,060
verify that you are connecting to this

00:21:13,330 --> 00:21:17,740
node to the real cluster accept the

00:21:16,060 --> 00:21:19,840
certificate and you are done you have

00:21:17,740 --> 00:21:22,480
joined the node what happens at this

00:21:19,840 --> 00:21:25,180
point if you have already uploaded some

00:21:22,480 --> 00:21:27,370
data to the cluster it will start

00:21:25,180 --> 00:21:30,480
automatically a rebalance process which

00:21:27,370 --> 00:21:33,220
will redistribute the data

00:21:30,480 --> 00:21:36,310
proportionally on all nodes depending on

00:21:33,220 --> 00:21:38,890
the size of the nodes so this is very

00:21:36,310 --> 00:21:43,000
important we use consistent hashing like

00:21:38,890 --> 00:21:45,370
most products so this means that you can

00:21:43,000 --> 00:21:47,620
have nodes of different sizes you can

00:21:45,370 --> 00:21:50,320
have a node with 10 terabytes and two

00:21:47,620 --> 00:21:55,060
nodes with five terabytes and you will

00:21:50,320 --> 00:21:57,070
have 10 terabytes replica to of usable

00:21:55,060 --> 00:21:58,750
space so you don't need to create

00:21:57,070 --> 00:22:00,870
notes all equal they can be different

00:21:58,750 --> 00:22:04,320
and they will store a proportional

00:22:00,870 --> 00:22:06,789
amount of data to the size of the node

00:22:04,320 --> 00:22:08,169
the balance process as I said that is

00:22:06,789 --> 00:22:10,360
totally transparent to you you don't

00:22:08,169 --> 00:22:12,940
have to worry about it you when you join

00:22:10,360 --> 00:22:16,840
the node it will start automatically and

00:22:12,940 --> 00:22:20,009
it will redistribute the data making

00:22:16,840 --> 00:22:24,399
sure that all the replica count is

00:22:20,009 --> 00:22:28,240
satisfied so what are the main benefits

00:22:24,399 --> 00:22:30,190
of SX cluster we took the best of the

00:22:28,240 --> 00:22:33,399
three things that we really liked one is

00:22:30,190 --> 00:22:35,919
our sink incremental transfers one is s3

00:22:33,399 --> 00:22:39,220
so we made all of our protocol restful

00:22:35,919 --> 00:22:41,200
so it's a totally proxy friendly you

00:22:39,220 --> 00:22:44,500
don't have to worry about anything it

00:22:41,200 --> 00:22:47,230
uses https by default so you don't have

00:22:44,500 --> 00:22:49,450
to worry about encryption if you have

00:22:47,230 --> 00:22:52,960
used Swift if you have set up a

00:22:49,450 --> 00:22:54,669
distributed Swift cluster you probably

00:22:52,960 --> 00:22:57,429
face the problem of authentication

00:22:54,669 --> 00:23:00,580
writings with the all chunks or all

00:22:57,429 --> 00:23:03,159
storage nodes communicate in securely

00:23:00,580 --> 00:23:05,529
with the proxy nodes they communicate in

00:23:03,159 --> 00:23:07,509
clear text and last time I checked they

00:23:05,529 --> 00:23:10,870
didn't even have authentication in place

00:23:07,509 --> 00:23:13,179
so the documentation at least until the

00:23:10,870 --> 00:23:16,559
release in January the one that I

00:23:13,179 --> 00:23:19,929
checked out suggested to setup a VPN

00:23:16,559 --> 00:23:22,419
over which the Swift nodes should

00:23:19,929 --> 00:23:24,669
communicate this complicates things a

00:23:22,419 --> 00:23:26,529
lot because you have to make the VPN

00:23:24,669 --> 00:23:28,659
redundant make sure that it's always

00:23:26,529 --> 00:23:30,789
available if you want the storage to be

00:23:28,659 --> 00:23:32,789
always available so we solve this

00:23:30,789 --> 00:23:37,379
problem by making everything

00:23:32,789 --> 00:23:42,490
authenticated and encrypted by default

00:23:37,379 --> 00:23:44,710
we use proxy friendly protocol contrary

00:23:42,490 --> 00:23:48,100
to Swift which requires opening a

00:23:44,710 --> 00:23:52,629
thousand ports many of them are dynamic

00:23:48,100 --> 00:23:55,059
ports and we made the most important

00:23:52,629 --> 00:23:58,149
improvement that we made compared to s3

00:23:55,059 --> 00:24:00,519
is that we made the speed grow linearly

00:23:58,149 --> 00:24:02,649
with the number of nodes so thanks to

00:24:00,519 --> 00:24:05,350
consistent hashing the data is

00:24:02,649 --> 00:24:07,779
distributed across all nodes so when you

00:24:05,350 --> 00:24:10,340
download or upload the data you are

00:24:07,779 --> 00:24:12,140
talking to all nodes in parallel

00:24:10,340 --> 00:24:15,409
up to a certain limit that you can

00:24:12,140 --> 00:24:17,090
choose so if you want to talk to five

00:24:15,409 --> 00:24:18,620
nodes you can set the limited to five

00:24:17,090 --> 00:24:22,250
nodes but by default it will use all

00:24:18,620 --> 00:24:28,370
nodes this means that you get better

00:24:22,250 --> 00:24:31,070
speed better and that you can actually

00:24:28,370 --> 00:24:33,529
grow the speed there is no single

00:24:31,070 --> 00:24:35,960
bottleneck when you use a proxy node

00:24:33,529 --> 00:24:37,940
when use an s3 and the point you are

00:24:35,960 --> 00:24:39,380
limited by the bandwidth for the single

00:24:37,940 --> 00:24:42,049
transfer you are limited by the

00:24:39,380 --> 00:24:46,220
bandwidth of that node with a sexy you

00:24:42,049 --> 00:24:48,409
you don't have such limited so we are

00:24:46,220 --> 00:24:52,899
almost running out of time so I will go

00:24:48,409 --> 00:24:56,710
quickly through the use cases that

00:24:52,899 --> 00:24:59,630
summer well some of our users found a

00:24:56,710 --> 00:25:02,620
cluster useful for some projects that we

00:24:59,630 --> 00:25:06,399
didn't even some of them we didn't even

00:25:02,620 --> 00:25:10,399
foresee but anyway let's go through them

00:25:06,399 --> 00:25:12,230
so the most obvious use case is a s3

00:25:10,399 --> 00:25:17,510
compatible storage so dropping

00:25:12,230 --> 00:25:21,770
replacement for AWS s3 the nice thing is

00:25:17,510 --> 00:25:24,620
that you get a local s3 cloud which is

00:25:21,770 --> 00:25:27,860
obviously faster than uploading to a

00:25:24,620 --> 00:25:30,140
remote cloud it's very useful in it's

00:25:27,860 --> 00:25:36,140
used for example by 3s data center in

00:25:30,140 --> 00:25:38,690
Poland which to allow backing up and

00:25:36,140 --> 00:25:40,870
restoring servers from a local network

00:25:38,690 --> 00:25:47,299
at a gigabit speed instead of using

00:25:40,870 --> 00:25:52,159
Amazon s3 CDN this is if you like image

00:25:47,299 --> 00:25:55,940
sharing services IMG y dot org is using

00:25:52,159 --> 00:25:59,029
scalable to store and distribute insert

00:25:55,940 --> 00:26:02,899
the images so basically can create a

00:25:59,029 --> 00:26:07,279
network of Essex nodes distributed even

00:26:02,899 --> 00:26:11,029
geographically and serve data even over

00:26:07,279 --> 00:26:14,179
HTTP HTTPS directly without using the

00:26:11,029 --> 00:26:18,080
Essex proto but by using just plain HTTP

00:26:14,179 --> 00:26:20,539
HTTPS so you can effectively create if

00:26:18,080 --> 00:26:24,650
you create for example in cluster of

00:26:20,539 --> 00:26:26,690
four nodes and put and distributed them

00:26:24,650 --> 00:26:30,140
graphically you have effectively created

00:26:26,690 --> 00:26:32,210
then you use a replica for volume you

00:26:30,140 --> 00:26:33,800
have effectively created the CDN because

00:26:32,210 --> 00:26:36,130
each location will have a copy of the

00:26:33,800 --> 00:26:39,680
file and will be serving the files

00:26:36,130 --> 00:26:45,260
independently and the nice thing is that

00:26:39,680 --> 00:26:46,930
you don't have to wait to for the city

00:26:45,260 --> 00:26:49,400
and to be synchronized because when you

00:26:46,930 --> 00:26:52,900
upload the file to the cluster its

00:26:49,400 --> 00:26:55,730
replicated synchronously to all nodes

00:26:52,900 --> 00:26:58,760
another use case is an hospital in it

00:26:55,730 --> 00:27:02,390
actually a couple of hospitals in Italy

00:26:58,760 --> 00:27:06,020
which are using scalable as storage back

00:27:02,390 --> 00:27:08,390
and for the storage system for their

00:27:06,020 --> 00:27:11,140
parks system the picture archiving

00:27:08,390 --> 00:27:14,780
communication system the park system is

00:27:11,140 --> 00:27:18,110
in case you're not aware it's the one

00:27:14,780 --> 00:27:20,840
that stores your ex x-rays or MRIs we

00:27:18,110 --> 00:27:25,210
are talking about typically 100

00:27:20,840 --> 00:27:29,120
gigabytes per exam pepper per patient so

00:27:25,210 --> 00:27:32,060
it's a lot of data to store there is an

00:27:29,120 --> 00:27:35,080
open source software for the which

00:27:32,060 --> 00:27:38,270
basically implements the DCIM protocol

00:27:35,080 --> 00:27:40,460
which talks to the x-rays which is the

00:27:38,270 --> 00:27:43,730
one used to talk to the x-rays machine

00:27:40,460 --> 00:27:48,740
MRI machines it's called the DCM for CH

00:27:43,730 --> 00:27:51,770
e and this hospitals well the IT company

00:27:48,740 --> 00:27:55,910
of this hospital developed a the glue a

00:27:51,770 --> 00:27:58,850
plug-in for DCM fourth key to store the

00:27:55,910 --> 00:28:01,550
data in scalable so they have a multi

00:27:58,850 --> 00:28:05,660
tiering policy which allows to have some

00:28:01,550 --> 00:28:10,640
exams stored locally and some exam

00:28:05,660 --> 00:28:14,650
instead stored in a remote location on a

00:28:10,640 --> 00:28:18,130
long-term archival cheaper storage and

00:28:14,650 --> 00:28:20,360
so this satisfies us all the ISO

00:28:18,130 --> 00:28:22,880
certification required to have that

00:28:20,360 --> 00:28:25,550
always available even for the disaster

00:28:22,880 --> 00:28:28,370
recovery and the nice thing is that we

00:28:25,550 --> 00:28:32,710
also support in scalable we also support

00:28:28,370 --> 00:28:36,560
permissions with a quite fine grade way

00:28:32,710 --> 00:28:38,600
so you can choose to write once and read

00:28:36,560 --> 00:28:41,480
many policy so

00:28:38,600 --> 00:28:44,870
basically what what the hospital needed

00:28:41,480 --> 00:28:48,860
was the to forbid to modify an existing

00:28:44,870 --> 00:28:50,900
exam so once the MRI or the x-ray exam

00:28:48,860 --> 00:28:54,169
is stored in the cluster it cannot be

00:28:50,900 --> 00:28:59,809
edited in any way and we fit perfectly

00:28:54,169 --> 00:29:02,660
in this use case another use case which

00:28:59,809 --> 00:29:07,010
we really like which is gaining momentum

00:29:02,660 --> 00:29:10,309
now is our white label cloud drive so on

00:29:07,010 --> 00:29:13,130
top of scalable over the Essex cluster

00:29:10,309 --> 00:29:16,450
we build a cloud drive application which

00:29:13,130 --> 00:29:20,030
is available for the windows OS X Linux

00:29:16,450 --> 00:29:23,030
Android iOS is pending approval by a

00:29:20,030 --> 00:29:25,039
poster by App Store and even a web

00:29:23,030 --> 00:29:28,400
interface so probably you are familiar

00:29:25,039 --> 00:29:32,000
with on cloud we built an alternative to

00:29:28,400 --> 00:29:34,220
own cloud why is it's better than on

00:29:32,000 --> 00:29:36,980
glad we're on cloud still requires you

00:29:34,220 --> 00:29:39,409
to use separated the system for the

00:29:36,980 --> 00:29:44,630
storage which can be an s3 storage or a

00:29:39,409 --> 00:29:47,330
local file system but it's basically

00:29:44,630 --> 00:29:49,820
unclouded relies heavily on a database

00:29:47,330 --> 00:29:51,559
right and that database becomes a single

00:29:49,820 --> 00:29:53,960
point of failure or the bottleneck if

00:29:51,559 --> 00:29:55,850
you want to look at another way so it's

00:29:53,960 --> 00:29:57,980
quite difficult to scale or it's on

00:29:55,850 --> 00:29:59,929
telly on cloud the only thing that you

00:29:57,980 --> 00:30:01,730
can do is a sharding which is what they

00:29:59,929 --> 00:30:05,030
do also so far as I know in their

00:30:01,730 --> 00:30:07,789
enterprise product but again shouting in

00:30:05,030 --> 00:30:13,100
my from my point of view is a bit like

00:30:07,789 --> 00:30:16,159
cheating and it brings saw so many a lot

00:30:13,100 --> 00:30:19,100
of complexity and many problems with it

00:30:16,159 --> 00:30:21,830
so with scalable you do you don't need

00:30:19,100 --> 00:30:24,590
to do sharding obviously you have no

00:30:21,830 --> 00:30:26,080
middleware which does act as a Bach

00:30:24,590 --> 00:30:29,000
bottleneck single point of failure

00:30:26,080 --> 00:30:31,610
everything is totally distributed the

00:30:29,000 --> 00:30:34,520
cloud drive application talks directly

00:30:31,610 --> 00:30:36,950
to the cluster where all nodes as I said

00:30:34,520 --> 00:30:39,049
are created equal we support the

00:30:36,950 --> 00:30:40,640
client-side encryption I talked to the

00:30:39,049 --> 00:30:42,220
unclouded guys they don't want to

00:30:40,640 --> 00:30:44,990
implement akai the side encryption

00:30:42,220 --> 00:30:48,190
because they want to didn't want to

00:30:44,990 --> 00:30:50,450
implement it in the web interface so

00:30:48,190 --> 00:30:52,250
instead we said okay if you enable

00:30:50,450 --> 00:30:55,790
client-side encryption you

00:30:52,250 --> 00:31:01,010
not use the web interface in good grief

00:30:55,790 --> 00:31:02,810
and so we implemented client encryption

00:31:01,010 --> 00:31:05,930
which is a very important feature if you

00:31:02,810 --> 00:31:08,420
cannot trust the company or the person

00:31:05,930 --> 00:31:11,210
that administer your servers because the

00:31:08,420 --> 00:31:13,850
data is encrypted before it's uploaded

00:31:11,210 --> 00:31:16,240
to the Essex cluster so nobody can have

00:31:13,850 --> 00:31:18,920
access to your data except you and

00:31:16,240 --> 00:31:22,280
finally compared to one cloud we support

00:31:18,920 --> 00:31:24,290
incremental transfers so when when you

00:31:22,280 --> 00:31:27,410
if you modify a part of a file in on

00:31:24,290 --> 00:31:29,690
cloud then it will upload the whole file

00:31:27,410 --> 00:31:32,210
because it uses the web dev protocol

00:31:29,690 --> 00:31:34,280
with scalable you only upload and

00:31:32,210 --> 00:31:36,740
transfer the differences this is very

00:31:34,280 --> 00:31:39,110
important especially when you are using

00:31:36,740 --> 00:31:42,800
the mobile clients it saves a lot of

00:31:39,110 --> 00:31:46,240
time bandwidth battery everything but

00:31:42,800 --> 00:31:49,130
it's also useful when using large files

00:31:46,240 --> 00:31:51,740
gigabyte many give multiple gigabytes

00:31:49,130 --> 00:31:54,380
files on a desktop because we don't

00:31:51,740 --> 00:31:56,510
crowd uploading a 4 gigabytes I mean

00:31:54,380 --> 00:31:58,700
managing a 4 gigabyte file is quite

00:31:56,510 --> 00:32:00,290
problematic because you have to every

00:31:58,700 --> 00:32:02,660
time you modified any part of it you

00:32:00,290 --> 00:32:04,820
have to upload all four gigabytes then

00:32:02,660 --> 00:32:08,140
we support the revisions but unclouded

00:32:04,820 --> 00:32:13,160
z22 and the duplication comes for free

00:32:08,140 --> 00:32:14,840
so I run out of time so we just say we

00:32:13,160 --> 00:32:18,020
have us an enterprise edition which is

00:32:14,840 --> 00:32:20,500
our business model right which supports

00:32:18,020 --> 00:32:24,530
into the ldap Active Directory

00:32:20,500 --> 00:32:27,830
two-factor authentication we have web

00:32:24,530 --> 00:32:30,980
management console and obviously we

00:32:27,830 --> 00:32:32,960
offer commercial support for our open

00:32:30,980 --> 00:32:36,610
source addition we offer support on our

00:32:32,960 --> 00:32:41,330
mailing list so I encourage you to try

00:32:36,610 --> 00:32:47,060
scalable it's obviously it's licensed

00:32:41,330 --> 00:32:49,100
under the GPL GPL 2 plus and so try it

00:32:47,060 --> 00:32:54,290
out and subscribe to our mailing list

00:32:49,100 --> 00:32:58,190
and well if you encounter any problem we

00:32:54,290 --> 00:33:00,440
will help you out so yeah and three

00:32:58,190 --> 00:33:03,240
minutes late so I guess there is no time

00:33:00,440 --> 00:33:08,340
for the Persian time or can we ask a few

00:33:03,240 --> 00:33:14,600
all right all right okay so any

00:33:08,340 --> 00:33:14,600
questions yep

00:33:19,960 --> 00:33:24,400
mm-hmm okay so yeah the duplication is

00:33:22,720 --> 00:33:27,390
clustered wise can you please repeat the

00:33:24,400 --> 00:33:30,040
question for the ground oh yeah we won

00:33:27,390 --> 00:33:33,160
okay so the question is is the

00:33:30,040 --> 00:33:38,740
duplication global across all users yes

00:33:33,160 --> 00:33:42,400
it's cluster wide and it it was

00:33:38,740 --> 00:33:45,130
difficult to implement so the question

00:33:42,400 --> 00:33:48,010
is also does it how does it scale it was

00:33:45,130 --> 00:33:52,510
the hardest part of the project so when

00:33:48,010 --> 00:33:56,500
we started a project in 2012 more or

00:33:52,510 --> 00:33:59,260
less we thought that it wouldn't be

00:33:56,500 --> 00:34:01,390
sword so we started with exotic by

00:33:59,260 --> 00:34:03,070
implementing the duplication and we

00:34:01,390 --> 00:34:05,710
thought that we will be done in one year

00:34:03,070 --> 00:34:10,360
it took two years and Dolph to get it

00:34:05,710 --> 00:34:15,250
right so it was really difficult but we

00:34:10,360 --> 00:34:17,770
sorted it out it's it it scales well

00:34:15,250 --> 00:34:20,680
from our perspective with while we we

00:34:17,770 --> 00:34:23,620
tried to so far the largest cluster that

00:34:20,680 --> 00:34:26,080
we had was a 400 terabytes so I cannot

00:34:23,620 --> 00:34:30,040
talk about the petabyte scale yet but

00:34:26,080 --> 00:34:39,550
the 400 terabyte cluster was just fine

00:34:30,040 --> 00:34:41,920
and the the duplication is done on with

00:34:39,550 --> 00:34:45,190
a dynamic dynamic block size so

00:34:41,920 --> 00:34:47,950
basically all that I stored with a

00:34:45,190 --> 00:34:51,160
different block size depending on the

00:34:47,950 --> 00:34:52,810
total size of the file so either it's a

00:34:51,160 --> 00:34:56,140
totally customizable so you can just

00:34:52,810 --> 00:35:02,320
pass a flag to configure and change the

00:34:56,140 --> 00:35:04,660
default we just set some what we believe

00:35:02,320 --> 00:35:09,190
are saying defaults but the truth is

00:35:04,660 --> 00:35:15,160
that there is no one-size-fits-all rule

00:35:09,190 --> 00:35:18,220
right so you just have to to adjust

00:35:15,160 --> 00:35:21,430
those settings if you want to optimize

00:35:18,220 --> 00:35:24,630
for performances so typically if I

00:35:21,430 --> 00:35:24,630
remember correctly we have

00:35:24,930 --> 00:35:37,900
is 60 sorry a 4k kilobyte block size for

00:35:31,150 --> 00:35:40,480
files under 64 k then we have 64 K a

00:35:37,900 --> 00:35:42,340
block size for files under one mag and

00:35:40,480 --> 00:35:45,600
then we have a one megabyte block size

00:35:42,340 --> 00:35:49,200
for files bigger than 100 Meg's

00:35:45,600 --> 00:35:52,540
something like that so it's totally

00:35:49,200 --> 00:35:58,780
configurable the reason we did this is

00:35:52,540 --> 00:36:03,160
that when you have a small file you are

00:35:58,780 --> 00:36:07,510
more likely to get a decent

00:36:03,160 --> 00:36:12,190
deduplication factor if the block size

00:36:07,510 --> 00:36:16,300
is small but for large files sorry we

00:36:12,190 --> 00:36:18,250
had really big performance issues but if

00:36:16,300 --> 00:36:20,140
we're using when we're using small block

00:36:18,250 --> 00:36:22,540
size so we can amp up with the idea of a

00:36:20,140 --> 00:36:26,410
dynamic block size tool to fix that so

00:36:22,540 --> 00:36:29,070
when you are using gigabyte files but we

00:36:26,410 --> 00:36:32,200
tested even a single I mean we were able

00:36:29,070 --> 00:36:35,440
quite easily to store in scalable a

00:36:32,200 --> 00:36:38,800
single 20 terabyte file that was not a

00:36:35,440 --> 00:36:49,720
problem at all using the one megabyte

00:36:38,800 --> 00:36:52,680
block size so aha for the game okay so

00:36:49,720 --> 00:36:52,680
any other questions yeah

00:36:55,720 --> 00:37:02,120
okay so right now we require using our

00:37:00,590 --> 00:37:05,660
sorry the question is how do you handle

00:37:02,120 --> 00:37:08,720
multiple our drives per node so right

00:37:05,660 --> 00:37:13,510
now we require a ride can be a software

00:37:08,720 --> 00:37:16,580
I hardware ride on each node to store

00:37:13,510 --> 00:37:19,640
and we rely on the file system to store

00:37:16,580 --> 00:37:22,900
our data so we basically the block size

00:37:19,640 --> 00:37:27,380
the various blogs that we store are

00:37:22,900 --> 00:37:30,770
aggregated into a what we call a

00:37:27,380 --> 00:37:33,490
container a data container and then we

00:37:30,770 --> 00:37:36,740
have some techniques to recover the

00:37:33,490 --> 00:37:39,170
unused space to compact these containers

00:37:36,740 --> 00:37:42,950
so basically what you will find out is

00:37:39,170 --> 00:37:47,180
that you have to mount the right volume

00:37:42,950 --> 00:37:50,810
the right partition put to scalable on

00:37:47,180 --> 00:37:53,450
it and let me input a scalpel storage on

00:37:50,810 --> 00:37:56,180
it and you will find like if remember

00:37:53,450 --> 00:38:00,050
correctly like twenty fives or something

00:37:56,180 --> 00:38:06,740
like that huge files that keep growing

00:38:00,050 --> 00:38:09,620
as your storage usage grows the ideal

00:38:06,740 --> 00:38:13,130
setup obviously is to use a LVN to rely

00:38:09,620 --> 00:38:17,680
on LVN so that you can if you add more

00:38:13,130 --> 00:38:17,680
our drives you can extend the partition

00:38:28,529 --> 00:38:34,420
yes yes the well it's not a big problem

00:38:32,469 --> 00:38:36,309
at least in our production environment

00:38:34,420 --> 00:38:41,410
it didn't turn out to be a big problem

00:38:36,309 --> 00:38:44,950
because we had you know the replica

00:38:41,410 --> 00:38:47,769
takes care of any downtime so you you

00:38:44,950 --> 00:38:50,529
will never run a replica one set up

00:38:47,769 --> 00:38:55,799
nobody does that the typical use case is

00:38:50,529 --> 00:38:55,799
with the replica tree so you can get

00:38:58,799 --> 00:39:03,130
even if one note goes down because you

00:39:01,509 --> 00:39:05,619
have to do a filesystem check or

00:39:03,130 --> 00:39:07,630
whatever you use you can still use the

00:39:05,619 --> 00:39:14,739
cluster transparently so it's not a big

00:39:07,630 --> 00:39:17,589
problem the the so far the useful we

00:39:14,739 --> 00:39:19,960
didn't find any problem also it's quite

00:39:17,589 --> 00:39:21,969
standard I mean other project most

00:39:19,960 --> 00:39:31,239
projects that i tried are doing the same

00:39:21,969 --> 00:39:32,950
with the exception of self yeah so we we

00:39:31,239 --> 00:39:35,140
did a lot of testing in this sense and

00:39:32,950 --> 00:39:40,859
I'll sorry the question is a which file

00:39:35,140 --> 00:39:45,759
system is recommended and the for the

00:39:40,859 --> 00:39:50,109
storage we did a lot of testing and in

00:39:45,759 --> 00:39:54,460
the end that turned out that the x4 was

00:39:50,109 --> 00:39:56,400
the best option for us I know that there

00:39:54,460 --> 00:39:59,019
might be different opinions on this but

00:39:56,400 --> 00:40:00,969
this is what we came up we did it we do

00:39:59,019 --> 00:40:04,420
most of all I mean I think that it also

00:40:00,969 --> 00:40:06,910
depends a lot on to be honest on the

00:40:04,420 --> 00:40:10,839
kind of artwork that you use so we do

00:40:06,910 --> 00:40:16,509
most of our testing on the dialer 720

00:40:10,839 --> 00:40:20,229
something on that but I think it it

00:40:16,509 --> 00:40:23,289
really depends on your setup on the

00:40:20,229 --> 00:40:25,569
exact machine that you are using I'm

00:40:23,289 --> 00:40:28,299
sure that if we did repeat the test a 10

00:40:25,569 --> 00:40:31,890
times will get each one will have

00:40:28,299 --> 00:40:34,839
different different opinions on this and

00:40:31,890 --> 00:40:37,509
it's your resolution only meant for

00:40:34,839 --> 00:40:39,999
storing files like a simple faster

00:40:37,509 --> 00:40:42,249
prakruti for example also use it to

00:40:39,999 --> 00:40:44,799
export block devices to integrate it to

00:40:42,249 --> 00:40:47,589
a nebula OpenStack or any virtualization

00:40:44,799 --> 00:40:48,819
okay so right now we have basically it

00:40:47,589 --> 00:40:50,619
so the question I know you add the

00:40:48,819 --> 00:40:56,609
microphone and ahead the only time that

00:40:50,619 --> 00:40:56,609
I remembered the other my girlfriend so

00:40:57,389 --> 00:41:03,219
right now we have basically four

00:40:59,709 --> 00:41:06,669
products available one is a SX cluster

00:41:03,219 --> 00:41:12,669
which is the actual storage distributed

00:41:06,669 --> 00:41:15,399
storage which can I mean it wasn't it

00:41:12,669 --> 00:41:17,079
can be abused for many things let's say

00:41:15,399 --> 00:41:19,989
we try to make it as flexible as

00:41:17,079 --> 00:41:23,289
possible unfortunately block storage

00:41:19,989 --> 00:41:25,389
doesn't really fit in the picture so I

00:41:23,289 --> 00:41:28,599
don't see it happening we are more

00:41:25,389 --> 00:41:31,919
targeted for the secondary storage so we

00:41:28,599 --> 00:41:34,899
already have actually a use case with

00:41:31,919 --> 00:41:41,289
with a company here in Germany who is

00:41:34,899 --> 00:41:44,259
using who's using which is using as a

00:41:41,289 --> 00:41:46,349
girl SS cluster with our s3 interface

00:41:44,259 --> 00:41:50,019
which is called libera three as a

00:41:46,349 --> 00:41:56,130
secondary storage for citrix cloud

00:41:50,019 --> 00:41:59,769
platform and we also have in the working

00:41:56,130 --> 00:42:03,279
SXF s which is a file system it's really

00:41:59,769 --> 00:42:06,849
promising it's still beta so don't use

00:42:03,279 --> 00:42:09,669
it in production we started developing

00:42:06,849 --> 00:42:13,509
it in February so it's on the couple of

00:42:09,669 --> 00:42:15,549
months of work obviously SX cluster as I

00:42:13,509 --> 00:42:21,489
said has been in development for almost

00:42:15,549 --> 00:42:24,999
three years now and we didn't get any

00:42:21,489 --> 00:42:28,449
bug report since even minor ones since

00:42:24,999 --> 00:42:32,979
December last year so it has reached a

00:42:28,449 --> 00:42:35,949
pretty mature state as accessing status

00:42:32,979 --> 00:42:39,729
is still a lot of work to do it works

00:42:35,949 --> 00:42:42,219
really well as in read read only mode

00:42:39,729 --> 00:42:44,739
for now it's incredibly fast because

00:42:42,219 --> 00:42:46,689
basically when you want fi system you

00:42:44,739 --> 00:42:50,289
are reading in parallel from all nodes

00:42:46,689 --> 00:42:51,490
that you and you can really see that you

00:42:50,289 --> 00:42:53,230
keep adding notes

00:42:51,490 --> 00:42:55,240
to the cluster and the speed just keep

00:42:53,230 --> 00:42:58,240
growing growing in the end you are

00:42:55,240 --> 00:43:02,590
always limited by the network so as fast

00:42:58,240 --> 00:43:04,690
as your network can be and we are still

00:43:02,590 --> 00:43:07,330
writing on the we're still working on

00:43:04,690 --> 00:43:14,250
improving the writing performance which

00:43:07,330 --> 00:43:18,280
is quite poor we support for now only

00:43:14,250 --> 00:43:22,000
Linux a success on Linux we have plans

00:43:18,280 --> 00:43:26,830
to extend the support to windows so the

00:43:22,000 --> 00:43:30,940
idea that we add actually which might

00:43:26,830 --> 00:43:35,619
like is to integrate also a sex drive

00:43:30,940 --> 00:43:38,320
with the Essex FS so it's the what some

00:43:35,619 --> 00:43:39,940
people call the infinite drive so that

00:43:38,320 --> 00:43:43,330
is that you get a cloud drive

00:43:39,940 --> 00:43:46,240
application that you can run on your

00:43:43,330 --> 00:43:49,000
desktop for example and for each volume

00:43:46,240 --> 00:43:50,740
which you can think of as a directory

00:43:49,000 --> 00:43:53,440
after all for each directory you can

00:43:50,740 --> 00:43:56,109
choose if you want to keep a directory a

00:43:53,440 --> 00:43:57,730
copy of the directory local so you want

00:43:56,109 --> 00:44:00,280
it to be available when you are offline

00:43:57,730 --> 00:44:02,800
or if you want to just be able to access

00:44:00,280 --> 00:44:05,710
it when you are online so the use case

00:44:02,800 --> 00:44:09,190
is typically people that store terabytes

00:44:05,710 --> 00:44:12,910
of pictures and movies family movies in

00:44:09,190 --> 00:44:16,990
the cloud drive but they travel with the

00:44:12,910 --> 00:44:19,330
macbook air which has 256 gigabytes of

00:44:16,990 --> 00:44:21,760
SSD and they simply cannot synchronize

00:44:19,330 --> 00:44:24,900
them but they still want to have access

00:44:21,760 --> 00:44:27,640
to them while they are on the door and

00:44:24,900 --> 00:44:30,730
currently this feature as far as i know

00:44:27,640 --> 00:44:35,859
is only supported by one drive in

00:44:30,730 --> 00:44:38,200
windows 10 even not not in the other

00:44:35,859 --> 00:44:40,690
version it was dropped because it goes

00:44:38,200 --> 00:44:42,340
to some problems and our idea is to

00:44:40,690 --> 00:44:44,230
implemented is in a sex drive so you

00:44:42,340 --> 00:44:46,480
will be able to choose which director

00:44:44,230 --> 00:44:50,220
you want to synchronize locally in which

00:44:46,480 --> 00:44:54,369
one you want to obsess over the network

00:44:50,220 --> 00:44:57,609
so yeah anyway 22 sorry I went a little

00:44:54,369 --> 00:45:01,150
further than the question was but yeah

00:44:57,609 --> 00:45:05,490
for a so about the we have them talking

00:45:01,150 --> 00:45:08,410
about open stock we don't we we are

00:45:05,490 --> 00:45:11,440
working on Java library Java client

00:45:08,410 --> 00:45:16,630
library for our protocol for the Essex

00:45:11,440 --> 00:45:19,630
protocol and we will be working also on

00:45:16,630 --> 00:45:27,760
a Python library and when that is done

00:45:19,630 --> 00:45:29,170
we plan to create a project on the fur

00:45:27,760 --> 00:45:32,110
coat oh it's called the OpenStack

00:45:29,170 --> 00:45:34,860
development website and register as an

00:45:32,110 --> 00:45:39,130
experimental project for OpenStack and

00:45:34,860 --> 00:45:50,440
integrate into it but we will only

00:45:39,130 --> 00:46:00,220
target the secondary storage yeah you

00:45:50,440 --> 00:46:03,310
can choose so the question is does does

00:46:00,220 --> 00:46:08,320
a scalpel store all data on every node

00:46:03,310 --> 00:46:11,800
the same data on every node know the the

00:46:08,320 --> 00:46:16,270
idea of consistent hashing is that the

00:46:11,800 --> 00:46:21,460
father object is divided into many

00:46:16,270 --> 00:46:23,860
blocks and then you basically create a

00:46:21,460 --> 00:46:26,500
map of these blocks which doesn't

00:46:23,860 --> 00:46:29,650
require an index so the map is the

00:46:26,500 --> 00:46:32,260
result of a function so you put the edge

00:46:29,650 --> 00:46:35,530
of the block into the function and you

00:46:32,260 --> 00:46:37,900
get the position of the block ok so this

00:46:35,530 --> 00:46:41,680
way you can have a file split across

00:46:37,900 --> 00:46:44,080
multiple nodes then you want a replica

00:46:41,680 --> 00:46:48,340
so you take this map and you just shift

00:46:44,080 --> 00:46:53,800
it by some amount of positions and this

00:46:48,340 --> 00:46:56,620
way the block that in the first map was

00:46:53,800 --> 00:46:59,320
on note one with the shift will go on

00:46:56,620 --> 00:47:04,510
for example or node tree right and this

00:46:59,320 --> 00:47:08,950
is how you get the replica so to

00:47:04,510 --> 00:47:13,840
simplify the the answer the date is that

00:47:08,950 --> 00:47:16,660
if you have a replica three-volume and

00:47:13,840 --> 00:47:19,079
you sorry a three node cluster with a

00:47:16,660 --> 00:47:22,900
replica to volume

00:47:19,079 --> 00:47:26,559
the data will be the mean one file will

00:47:22,900 --> 00:47:29,950
be one copy of the fight would be on the

00:47:26,559 --> 00:47:32,260
first two nodes and another copy of the

00:47:29,950 --> 00:47:38,910
file will be on the second and third

00:47:32,260 --> 00:47:38,910
node okay daveed exactly except yeah

00:47:41,039 --> 00:47:49,650
other questions yeah

00:48:09,460 --> 00:48:15,830
okay so the question is how do the

00:48:12,080 --> 00:48:18,620
clients find the data on the cluster how

00:48:15,830 --> 00:48:21,650
do they find the location of the blocks

00:48:18,620 --> 00:48:25,640
and how do they communicate with the

00:48:21,650 --> 00:48:30,470
with the target nodes so the way what

00:48:25,640 --> 00:48:33,800
protocol works is this we first the so

00:48:30,470 --> 00:48:36,890
we expect in a DNS record with a

00:48:33,800 --> 00:48:39,310
round-robin of all nodes don't we scared

00:48:36,890 --> 00:48:45,020
the DNS record doesn't have to be

00:48:39,310 --> 00:48:47,450
updated continuously so the client will

00:48:45,020 --> 00:48:54,080
detect automatically if one node of the

00:48:47,450 --> 00:48:56,390
of that DNS record is down is that as

00:48:54,080 --> 00:48:59,150
being kicked out of the cluster so you

00:48:56,390 --> 00:49:00,740
don't have to continuously keep the DNS

00:48:59,150 --> 00:49:03,320
record updated because otherwise you

00:49:00,740 --> 00:49:08,930
with you will have the problem again the

00:49:03,320 --> 00:49:11,990
problem of H a right and so the client

00:49:08,930 --> 00:49:14,540
queries the DNS record gets a list of

00:49:11,990 --> 00:49:17,570
nodes you don't have to provide all the

00:49:14,540 --> 00:49:20,570
nodes just a few of them is enough to to

00:49:17,570 --> 00:49:23,420
bootstrap the request let's say the

00:49:20,570 --> 00:49:26,870
client will connect to each node to the

00:49:23,420 --> 00:49:31,190
first node in the list it checks if it

00:49:26,870 --> 00:49:34,640
is a live node or not then it asks where

00:49:31,190 --> 00:49:38,600
is the volume so the volume that I'm

00:49:34,640 --> 00:49:41,270
trying to access who has the metadata

00:49:38,600 --> 00:49:43,510
information for this volume right so the

00:49:41,270 --> 00:49:49,760
node each every node in the cluster

00:49:43,510 --> 00:49:53,540
nodes knows which nodes no old the

00:49:49,760 --> 00:49:56,090
volume metadata so they will quare they

00:49:53,540 --> 00:50:00,080
will get a list of we call them volume

00:49:56,090 --> 00:50:04,700
nodes for that volume and they will

00:50:00,080 --> 00:50:06,890
query directly those nodes and when the

00:50:04,700 --> 00:50:11,510
query those notes the

00:50:06,890 --> 00:50:14,180
they get the list of blocks of the ashes

00:50:11,510 --> 00:50:17,150
of the blocks which make up the object

00:50:14,180 --> 00:50:20,059
and once they have the edge they can

00:50:17,150 --> 00:50:23,059
just put a dash through the consistent

00:50:20,059 --> 00:50:26,930
hashing function and get the whole map

00:50:23,059 --> 00:50:29,900
of the distribution of the file so to

00:50:26,930 --> 00:50:34,609
make it to give you the big picture okay

00:50:29,900 --> 00:50:38,029
over how it works in the cluster the

00:50:34,609 --> 00:50:40,220
files to like the previous question was

00:50:38,029 --> 00:50:43,279
about how the files are distributed with

00:50:40,220 --> 00:50:47,329
replicas right so it's all nodes contain

00:50:43,279 --> 00:50:51,500
a portion of the objects right but only

00:50:47,329 --> 00:50:53,150
a few nodes so for replica to it would

00:50:51,500 --> 00:50:58,160
be two notes for replica tree it will be

00:50:53,150 --> 00:51:00,500
three nodes no the block list of an

00:50:58,160 --> 00:51:03,039
object so if you choose a replica to

00:51:00,500 --> 00:51:07,579
there will be two nodes which know that

00:51:03,039 --> 00:51:15,769
object a as five blocks and the ashes of

00:51:07,579 --> 00:51:18,470
the block are the AV da DB rubber all

00:51:15,769 --> 00:51:25,190
the foolish right the sha-1 we use a

00:51:18,470 --> 00:51:26,630
sha-1 and once you have the ash of the

00:51:25,190 --> 00:51:28,430
blocks you can just pass them to a

00:51:26,630 --> 00:51:31,009
function and the function will give you

00:51:28,430 --> 00:51:35,779
the map will tell you exactly on which

00:51:31,009 --> 00:51:38,589
nodes are the blocks okay then thank you

00:51:35,779 --> 00:51:38,589

YouTube URL: https://www.youtube.com/watch?v=MqfSfMx8S7c


