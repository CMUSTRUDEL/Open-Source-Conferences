Title: OSDC 2015: Georg Schoenberger | Linux Performance Profiling and Monitoring
Publication date: 2015-04-30
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	Nowadays system administrators have great choices when it comes down to performance profiling and monitoring. The challenge is to pick the ppropriate tool and interpret their results correctly.
This talk is a chance to take a tour through various performance profiling and benchmarking tools, focusing on their benefit for every sysadmin. The topics will range from simple application profiling over sysstat utilities to low-level tracing methods. Besides traditional Linux methods a short glance at MySQL and Linux containers will be taken, too, as they are widely spread technologies.
At the end the goal is to gather reference points to look at, if you are faced with performance problems. Take the chance to close your knowledge gaps and learn how to get the most out of your system.
Captions: 
	00:00:06,500 --> 00:00:12,830
hello everybody welcome to the

00:00:11,450 --> 00:00:15,500
i will look at i would like to introduce

00:00:12,830 --> 00:00:17,779
you georg Schoenberger who will give us

00:00:15,500 --> 00:00:24,380
a talk about linux performance profiling

00:00:17,779 --> 00:00:28,040
and monitoring thanks for the

00:00:24,380 --> 00:00:30,739
introduction and another time I'll

00:00:28,040 --> 00:00:32,989
everybody i hope you enjoyed the lunch

00:00:30,739 --> 00:00:35,870
and i hope also that you're not too

00:00:32,989 --> 00:00:38,690
tired now to follow my talk about linux

00:00:35,870 --> 00:00:41,480
performance profiling and monitoring we

00:00:38,690 --> 00:00:44,570
have quite a tough broken the next hour

00:00:41,480 --> 00:00:48,470
and i hope i will not overrun too much

00:00:44,570 --> 00:00:51,250
but if you do if it is too boring just

00:00:48,470 --> 00:00:55,610
raise your hand and say okay next slide

00:00:51,250 --> 00:00:58,910
so i am working for the thomas crown

00:00:55,610 --> 00:01:01,870
oggi a super manufacturer in bavaria who

00:00:58,910 --> 00:01:05,269
knows us so who knows the thomas panicky

00:01:01,870 --> 00:01:09,649
yes massa who has service from us yes

00:01:05,269 --> 00:01:11,930
nice to ok raise your hands up up we

00:01:09,649 --> 00:01:13,520
also have a well-known knowledgebase

00:01:11,930 --> 00:01:14,810
Artemis plain week is a who knows to

00:01:13,520 --> 00:01:17,689
Thomas when Ricky maybe more people

00:01:14,810 --> 00:01:20,240
knows that thanks a lot please give us

00:01:17,689 --> 00:01:23,420
any feedback if you have just about

00:01:20,240 --> 00:01:28,189
comments to improve our articles we are

00:01:23,420 --> 00:01:30,319
always open for feedback this is the

00:01:28,189 --> 00:01:32,060
program for the next hour we start with

00:01:30,319 --> 00:01:36,020
collecting some statistics about our

00:01:32,060 --> 00:01:38,810
Linux system these are in general basic

00:01:36,020 --> 00:01:41,240
tools everybody knows a lot of a lot of

00:01:38,810 --> 00:01:43,939
stats tools then we watch some online

00:01:41,240 --> 00:01:47,119
statistics with the top page tool so we

00:01:43,939 --> 00:01:49,549
watched we watch life resource usage

00:01:47,119 --> 00:01:52,429
then we do some tracing with performance

00:01:49,549 --> 00:01:55,069
and f trees and also explain what flame

00:01:52,429 --> 00:02:00,049
graphs are and we then if we have time

00:01:55,069 --> 00:02:02,569
we go further to Lexi containers and if

00:02:00,049 --> 00:02:07,520
there's time at the end we will cover

00:02:02,569 --> 00:02:10,009
some my sequel topics so okay let's

00:02:07,520 --> 00:02:12,290
start right now we have to hurry up so

00:02:10,009 --> 00:02:15,410
the first thing our statistics I think

00:02:12,290 --> 00:02:17,060
everybody has used a lot of these tools

00:02:15,410 --> 00:02:21,620
mentioned on this picture this picture

00:02:17,060 --> 00:02:24,980
is from brant Greg famous performance

00:02:21,620 --> 00:02:28,970
guy so Brandon brekkie uses a lot of

00:02:24,980 --> 00:02:33,080
is that which is from solaris and PST

00:02:28,970 --> 00:02:35,239
systems we will focus on the tools

00:02:33,080 --> 00:02:39,170
highlighted here in this orange arm

00:02:35,239 --> 00:02:41,420
circles so estrace have just kicked out

00:02:39,170 --> 00:02:43,040
of the slides because i do not have time

00:02:41,420 --> 00:02:46,250
to cover that but i think everybody

00:02:43,040 --> 00:02:48,440
knows estrace so this is a basic tool we

00:02:46,250 --> 00:02:51,170
will cover all this tattoos iostat

00:02:48,440 --> 00:02:53,360
netstat and pit that in these debts all

00:02:51,170 --> 00:02:57,590
the things highlighted here and also

00:02:53,360 --> 00:02:59,329
that the top life usage tools this is

00:02:57,590 --> 00:03:01,310
not the number of minutes i'm going to

00:02:59,329 --> 00:03:03,500
overrun this talk this is the number of

00:03:01,310 --> 00:03:08,120
tools will i mentioned in my talk and i

00:03:03,500 --> 00:03:10,280
put this light um every to every five or

00:03:08,120 --> 00:03:12,230
six lights just to know how much tools

00:03:10,280 --> 00:03:13,760
we have covered yet so take a deep

00:03:12,230 --> 00:03:15,799
breath fiction series slide and we do

00:03:13,760 --> 00:03:18,530
some simply do some pause and we take a

00:03:15,799 --> 00:03:21,680
deep breath and just 22 relaxant and

00:03:18,530 --> 00:03:23,810
cover the next tools the first thing i

00:03:21,680 --> 00:03:26,060
want to to mention this I'm pleased that

00:03:23,810 --> 00:03:29,150
this is a good tool to get an overview

00:03:26,060 --> 00:03:31,310
how well balanced our algorithms in your

00:03:29,150 --> 00:03:35,450
system so you get an overview of all the

00:03:31,310 --> 00:03:39,109
usage of the cpu cores your your system

00:03:35,450 --> 00:03:41,480
has and not only the real physical see

00:03:39,109 --> 00:03:43,489
bukas but also the hyper threats your

00:03:41,480 --> 00:03:45,799
system is using so if you have two cores

00:03:43,489 --> 00:03:48,859
and to hyper threads per socket and you

00:03:45,799 --> 00:03:50,599
see for course with MP stats so if you

00:03:48,859 --> 00:03:52,250
if you have a lot of course then and

00:03:50,599 --> 00:03:54,799
peace that output will will get long

00:03:52,250 --> 00:03:56,630
there yes this is the first overview we

00:03:54,799 --> 00:04:00,880
can see most of the course the idol here

00:03:56,630 --> 00:04:00,880
so there's nothing going on in my system

00:04:00,970 --> 00:04:08,230
there is some resource usage on our

00:04:04,040 --> 00:04:10,730
system encore one as you can see the

00:04:08,230 --> 00:04:13,760
usage is not the rebalance in the system

00:04:10,730 --> 00:04:16,450
because three cores are idle course zero

00:04:13,760 --> 00:04:18,650
co2 in co 3 are idle but there's some

00:04:16,450 --> 00:04:20,900
yes some performance tests running on

00:04:18,650 --> 00:04:23,120
core 0 this is what's basically a few

00:04:20,900 --> 00:04:24,860
performance tests during semi oh and as

00:04:23,120 --> 00:04:27,470
we can see we have some my oh wait here

00:04:24,860 --> 00:04:30,800
the cpu is idling because it is waiting

00:04:27,470 --> 00:04:34,370
for 20 being done on the on the disk

00:04:30,800 --> 00:04:36,890
drives we will cover that later on when

00:04:34,370 --> 00:04:38,870
when i'm coming to aerostat what this

00:04:36,890 --> 00:04:41,500
number means for us

00:04:38,870 --> 00:04:44,810
with also the assistant the user

00:04:41,500 --> 00:04:47,090
research its user says okay how much

00:04:44,810 --> 00:04:49,460
time are CPU time is spent in user code

00:04:47,090 --> 00:04:51,890
and how much time we spend in system

00:04:49,460 --> 00:04:54,200
code this is the percent this column

00:04:51,890 --> 00:04:56,150
okay we have twenty three percent in

00:04:54,200 --> 00:05:00,470
running a kernel code and eleven percent

00:04:56,150 --> 00:05:02,930
running in user application code we am

00:05:00,470 --> 00:05:04,370
stat is also a tool with kids with gift

00:05:02,930 --> 00:05:06,890
some high level statistics about the

00:05:04,370 --> 00:05:09,440
virtual memory usage so you get a lot of

00:05:06,890 --> 00:05:12,470
counters a lot of summary counters about

00:05:09,440 --> 00:05:15,880
to currently used a memory usage we have

00:05:12,470 --> 00:05:18,620
to swap usage in kilobytes we have the

00:05:15,880 --> 00:05:20,540
amount of remember we have the amount of

00:05:18,620 --> 00:05:25,210
used buffers in cash that this is a good

00:05:20,540 --> 00:05:28,250
summary and this is also a good tool to

00:05:25,210 --> 00:05:30,590
to collect historical data so you can

00:05:28,250 --> 00:05:33,800
see how well your system is using memory

00:05:30,590 --> 00:05:36,830
usage and you also have to collect to to

00:05:33,800 --> 00:05:39,470
get an overview of how your system is

00:05:36,830 --> 00:05:43,370
using memory we also have blogs in a

00:05:39,470 --> 00:05:47,270
blog out statistics so how my system is

00:05:43,370 --> 00:05:49,310
using the block devices underneath there

00:05:47,270 --> 00:05:52,400
is again a few tests running years or

00:05:49,310 --> 00:05:55,940
blocks out is a write tests and blocks

00:05:52,400 --> 00:05:58,430
in is a retest in the system column you

00:05:55,940 --> 00:06:00,470
also get information about interrupts

00:05:58,430 --> 00:06:02,810
and context switches this is mainly

00:06:00,470 --> 00:06:06,260
interesting for algorithms running on

00:06:02,810 --> 00:06:13,040
your cpu and at the end we have some cpu

00:06:06,260 --> 00:06:14,930
timers wait times and steal times the

00:06:13,040 --> 00:06:17,000
memory statistics they correlate with

00:06:14,930 --> 00:06:18,650
the output from the well-known free

00:06:17,000 --> 00:06:20,420
programs that everyone has called free

00:06:18,650 --> 00:06:22,400
sometimes on this system to see how much

00:06:20,420 --> 00:06:24,620
memory is free how much memory is

00:06:22,400 --> 00:06:26,810
available okay the first time you think

00:06:24,620 --> 00:06:29,060
okay that's not very much memory free of

00:06:26,810 --> 00:06:31,880
my system but you have to consider that

00:06:29,060 --> 00:06:35,510
a lot of memory is buffered and cached

00:06:31,880 --> 00:06:37,700
and buffer buffer cache are raw disk

00:06:35,510 --> 00:06:40,520
blocks like filesystem meter data that

00:06:37,700 --> 00:06:42,920
is used by the kernel and the cash r is

00:06:40,520 --> 00:06:45,170
memory used for real data informations

00:06:42,920 --> 00:06:48,680
of pages with actually user content in

00:06:45,170 --> 00:06:51,020
it and if you subtract that from the

00:06:48,680 --> 00:06:52,700
free memory then we have nearly few

00:06:51,020 --> 00:06:55,340
gigabytes of free ram but

00:06:52,700 --> 00:06:59,920
a lot of memory is used for catching

00:06:55,340 --> 00:06:59,920
actual user data so that's good to know

00:07:01,750 --> 00:07:07,000
you also have process related fields in

00:07:04,490 --> 00:07:10,490
vmstat these are the first two columns

00:07:07,000 --> 00:07:13,040
column B and column are so r is the

00:07:10,490 --> 00:07:15,350
number of runnable processes currently

00:07:13,040 --> 00:07:17,300
running or waiting for runtime if this

00:07:15,350 --> 00:07:19,520
is high then you have an indicator for

00:07:17,300 --> 00:07:21,860
cpu saturation so if there is a high

00:07:19,520 --> 00:07:25,220
number of a process is currently waiting

00:07:21,860 --> 00:07:27,920
for cpu time that maybe you have to put

00:07:25,220 --> 00:07:29,960
some workers in your system and b is the

00:07:27,920 --> 00:07:31,640
number of processes in an or

00:07:29,960 --> 00:07:34,100
interruptible sleep and this is mostly

00:07:31,640 --> 00:07:36,440
if the process is awaiting for i/o so if

00:07:34,100 --> 00:07:38,840
you have a high number of processes

00:07:36,440 --> 00:07:41,570
waiting an uninterruptible sleep then

00:07:38,840 --> 00:07:45,320
maybe your block devices over saturated

00:07:41,570 --> 00:07:47,090
you can check with the PS command in

00:07:45,320 --> 00:07:48,920
which kernel function your process are

00:07:47,090 --> 00:07:51,470
currently sleeping so this is a nice

00:07:48,920 --> 00:07:52,940
tool with PSO everybody knows yes but

00:07:51,470 --> 00:07:54,830
what I like a lot is that you can

00:07:52,940 --> 00:07:58,580
customize the columns you used you have

00:07:54,830 --> 00:08:01,400
the whn parameter here and you can grab

00:07:58,580 --> 00:08:04,130
for the X 4 columns and then you get ok

00:08:01,400 --> 00:08:05,630
my processes are doing some file right

00:08:04,130 --> 00:08:09,110
so there's some fire rights going on in

00:08:05,630 --> 00:08:11,930
the system and this this processes are

00:08:09,110 --> 00:08:15,680
in an uninterruptible state waiting for

00:08:11,930 --> 00:08:18,550
the x 45 rights you can see that in the

00:08:15,680 --> 00:08:20,900
enterprise I states from the PS output

00:08:18,550 --> 00:08:22,400
so if you have any questions just raise

00:08:20,900 --> 00:08:24,350
your hand and ask directly because

00:08:22,400 --> 00:08:29,120
afterwards it's hard to remember what

00:08:24,350 --> 00:08:31,490
was on slide X epsilon you can also

00:08:29,120 --> 00:08:33,620
generate some blood from vmstat I really

00:08:31,490 --> 00:08:35,690
like plots a lot everybody knows new

00:08:33,620 --> 00:08:38,120
blood sometimes it is hard to use so

00:08:35,690 --> 00:08:40,400
there are a lot of automatic scripts are

00:08:38,120 --> 00:08:43,070
degenerates and blood from the tools and

00:08:40,400 --> 00:08:45,230
if you want to use we amstead plotted

00:08:43,070 --> 00:08:49,760
and you can just call the m's that and

00:08:45,230 --> 00:08:51,980
from we am stat text 0 file that you you

00:08:49,760 --> 00:08:55,360
captured and then you can generate some

00:08:51,980 --> 00:08:58,010
blood this is again a blood from a few

00:08:55,360 --> 00:09:00,590
performance tests first doing some right

00:08:58,010 --> 00:09:03,890
testing in other words doing some random

00:09:00,590 --> 00:09:07,030
retest and I've drawn here interrupts

00:09:03,890 --> 00:09:07,030
and context switches

00:09:13,389 --> 00:09:18,110
obviously most of the time we are not

00:09:15,620 --> 00:09:22,009
satisfied with some reason overviews so

00:09:18,110 --> 00:09:25,220
we really want to know what is pit 1959

00:09:22,009 --> 00:09:27,170
doing ok so what program can you use to

00:09:25,220 --> 00:09:32,089
to know to get a more information about

00:09:27,170 --> 00:09:34,519
your process IDs any ideas estrace ok

00:09:32,089 --> 00:09:38,870
yes you can trace a currently running

00:09:34,519 --> 00:09:45,079
pit that's true pit stop ok that's the

00:09:38,870 --> 00:09:47,449
correct answer a so plus so kids that

00:09:45,079 --> 00:09:51,290
report sadistics for the task currently

00:09:47,449 --> 00:09:54,350
managed by the kernel and you have two

00:09:51,290 --> 00:09:56,449
reports available if your task is CPU

00:09:54,350 --> 00:09:59,569
pounds and you can identify the peak

00:09:56,449 --> 00:10:02,240
activity with the cpu report you can

00:09:59,569 --> 00:10:04,399
further sorry it was the wrong button

00:10:02,240 --> 00:10:07,250
you can first caught up in batch mode

00:10:04,399 --> 00:10:09,259
and sort by the CPU column make a check

00:10:07,250 --> 00:10:11,930
ok there is some python script running

00:10:09,259 --> 00:10:14,660
and it is using ninety-six percent of my

00:10:11,930 --> 00:10:16,579
cpu and it's consuming some memory but

00:10:14,660 --> 00:10:19,759
you want to know more about that passing

00:10:16,579 --> 00:10:22,279
script then you can call pit start with

00:10:19,759 --> 00:10:25,759
the process ed you get you've got with

00:10:22,279 --> 00:10:28,250
top then you can check the utilization

00:10:25,759 --> 00:10:31,069
report and we will call that one time

00:10:28,250 --> 00:10:33,410
and then you can see ok there's a magic

00:10:31,069 --> 00:10:35,870
matrix multiplication running in patent

00:10:33,410 --> 00:10:38,139
they're all done and user application

00:10:35,870 --> 00:10:41,720
code so if you have one hundred percent

00:10:38,139 --> 00:10:44,600
user resource utilization and no kernel

00:10:41,720 --> 00:10:47,990
code invoke involved and it is using one

00:10:44,600 --> 00:10:49,670
hundred percent of CPU and you can even

00:10:47,990 --> 00:10:51,500
check the comment line arguments from

00:10:49,670 --> 00:10:54,279
this pattern script running there so

00:10:51,500 --> 00:10:58,880
pizza this really a nice tool because if

00:10:54,279 --> 00:11:01,819
your pit is doing some I opened some o

00:10:58,880 --> 00:11:04,389
boundary and opened workload then you

00:11:01,819 --> 00:11:08,660
can also generate a device report and

00:11:04,389 --> 00:11:11,449
you first check with amp is that ok

00:11:08,660 --> 00:11:14,300
there's something running on my cpu

00:11:11,449 --> 00:11:17,600
encore one this is the same slide after

00:11:14,300 --> 00:11:19,880
we've seen before at the MPS that slide

00:11:17,600 --> 00:11:21,950
but you want to know okay which

00:11:19,880 --> 00:11:24,830
this is actually calling I'm causing our

00:11:21,950 --> 00:11:27,590
weight on my system you check the device

00:11:24,830 --> 00:11:29,660
report with pizza then you can see okay

00:11:27,590 --> 00:11:33,500
there's a random write tests running we

00:11:29,660 --> 00:11:35,330
feel and you can also check the kilobyte

00:11:33,500 --> 00:11:37,690
written to the device so this is nice

00:11:35,330 --> 00:11:40,700
about fits that these two reports and

00:11:37,690 --> 00:11:42,590
pits that is also reveals the command

00:11:40,700 --> 00:11:49,040
and they are all currently running on

00:11:42,590 --> 00:11:51,020
your system we've heard about we entered

00:11:49,040 --> 00:11:52,760
that before which is a summary tool and

00:11:51,020 --> 00:11:55,040
we also want to get know about the

00:11:52,760 --> 00:11:58,220
currently running task i'm a system how

00:11:55,040 --> 00:12:02,180
much memory is a specific pit consuming

00:11:58,220 --> 00:12:06,100
and you can do that with pits that tool

00:12:02,180 --> 00:12:10,430
to you called you call the device

00:12:06,100 --> 00:12:12,620
specific report for a given pit and you

00:12:10,430 --> 00:12:14,810
can check ok this pit is using this

00:12:12,620 --> 00:12:17,270
amount of this version of memory now and

00:12:14,810 --> 00:12:20,600
is also generating some minor or major

00:12:17,270 --> 00:12:23,510
page faults major page faults require

00:12:20,600 --> 00:12:25,190
operation minor page faults not and if

00:12:23,510 --> 00:12:26,690
you have a lot of major page force then

00:12:25,190 --> 00:12:30,620
it's a good indicator that you need some

00:12:26,690 --> 00:12:32,510
more memory of your system and ok so

00:12:30,620 --> 00:12:34,600
this is the same column as you get with

00:12:32,510 --> 00:12:41,390
the top there you also have a percent

00:12:34,600 --> 00:12:45,080
memory column I oh that is a doula liked

00:12:41,390 --> 00:12:47,510
a lot I've also written a short article

00:12:45,080 --> 00:12:51,410
about arose that in the formula known

00:12:47,510 --> 00:12:53,390
admin magazine you get a 0 subsystem

00:12:51,410 --> 00:12:56,150
statistics without that and you have a

00:12:53,390 --> 00:12:58,220
CPU or a device utilization report and

00:12:56,150 --> 00:13:00,410
without any argument when you call it is

00:12:58,220 --> 00:13:02,630
that you get the summary counters since

00:13:00,410 --> 00:13:06,410
boot but you can step you can skip that

00:13:02,630 --> 00:13:08,840
with the y option so just just running a

00:13:06,410 --> 00:13:12,380
toast that gives you the number since

00:13:08,840 --> 00:13:17,960
boot okay that's not very very much

00:13:12,380 --> 00:13:19,610
running on this system ok so I'm going

00:13:17,960 --> 00:13:26,090
to do this I want to show this on our

00:13:19,610 --> 00:13:29,690
life to you we've roasted I'm just at

00:13:26,090 --> 00:13:32,150
the top left terminal I'm starting a

00:13:29,690 --> 00:13:33,890
random write tests just producing some

00:13:32,150 --> 00:13:36,960
bloat

00:13:33,890 --> 00:13:39,690
okay they are now running for threats

00:13:36,960 --> 00:13:43,830
i'm just to me and I'm calling a toast

00:13:39,690 --> 00:13:46,080
at here and we see okay I'm just

00:13:43,830 --> 00:13:49,500
stopping this year we've now produced

00:13:46,080 --> 00:13:51,779
some oh wait with we feel so forty

00:13:49,500 --> 00:13:53,850
percent of iobit you have to keep in

00:13:51,779 --> 00:13:56,220
mind that this is a summary of all your

00:13:53,850 --> 00:13:57,690
process SF so we have seen before that

00:13:56,220 --> 00:14:00,380
we've am pissed that we have four

00:13:57,690 --> 00:14:03,330
processors so this is their oh wait of

00:14:00,380 --> 00:14:06,120
the average of orbit over all processes

00:14:03,330 --> 00:14:10,920
we've got in the system I'm starting the

00:14:06,120 --> 00:14:14,520
test again and I'm calling and please

00:14:10,920 --> 00:14:17,250
turn now and we can see that our

00:14:14,520 --> 00:14:19,920
workload is actually running only um

00:14:17,250 --> 00:14:22,290
okay there's a it's actually running

00:14:19,920 --> 00:14:24,420
only encore on course here was a with

00:14:22,290 --> 00:14:27,600
unquote one sorry and the other

00:14:24,420 --> 00:14:28,680
processor course are idle so with am

00:14:27,600 --> 00:14:30,660
pleased that we can checked okay

00:14:28,680 --> 00:14:32,940
actually the workload is only running on

00:14:30,660 --> 00:14:34,140
one cpu without that you don't get this

00:14:32,940 --> 00:14:37,440
information because you have to average

00:14:34,140 --> 00:14:39,930
of all cpus so that is when we use am

00:14:37,440 --> 00:14:43,710
pissed at four and what i want to say is

00:14:39,930 --> 00:14:46,850
that you can't always rely on I oh wait

00:14:43,710 --> 00:14:50,280
time because I oh wait time is a subject

00:14:46,850 --> 00:14:52,250
subsection of CPU idle time so I'm going

00:14:50,280 --> 00:14:56,370
to show this to you again we start

00:14:52,250 --> 00:15:00,960
random read a random write test I'm

00:14:56,370 --> 00:15:03,360
calling it 0 start here but if I start

00:15:00,960 --> 00:15:04,830
at the same time a CPU bound tests on

00:15:03,360 --> 00:15:13,530
the same color I'm doing this with task

00:15:04,830 --> 00:15:19,560
set this is running now then we can see

00:15:13,530 --> 00:15:23,670
that oh sorry that our i alway time goes

00:15:19,560 --> 00:15:26,310
down to 0 so we have a I opened workload

00:15:23,670 --> 00:15:28,830
running on cpu core 1 and we have an

00:15:26,310 --> 00:15:32,220
cpu-bound workload running on the same

00:15:28,830 --> 00:15:34,620
core then a cpu don't produces any idle

00:15:32,220 --> 00:15:37,500
time because the two tasks are running

00:15:34,620 --> 00:15:39,270
on the same core so you can't always

00:15:37,500 --> 00:15:41,459
rely on that oh wait time because it is

00:15:39,270 --> 00:15:44,670
a subsection of CPU at all time that's

00:15:41,459 --> 00:15:46,890
important to keep in mind because most

00:15:44,670 --> 00:15:47,820
of the users just take a look at the

00:15:46,890 --> 00:15:49,440
Oviatt okay

00:15:47,820 --> 00:15:51,240
nowhere oh wait then I don't have a

00:15:49,440 --> 00:15:53,700
problem with my with my block devices

00:15:51,240 --> 00:15:58,260
but that not that's not true in every

00:15:53,700 --> 00:16:00,330
case this is what I've basically

00:15:58,260 --> 00:16:02,790
highlighted on this slide so I'll wait

00:16:00,330 --> 00:16:04,230
but if we're under cpu-bound task on the

00:16:02,790 --> 00:16:08,270
same coordinate over it goes down to

00:16:04,230 --> 00:16:08,270
zero this is important to keep in mind

00:16:08,510 --> 00:16:15,210
the next thing I want to notice is the

00:16:12,020 --> 00:16:17,910
percent of utilization a device reports

00:16:15,210 --> 00:16:20,610
with iostat if you read the manwich

00:16:17,910 --> 00:16:23,010
which we should do in for the tool if we

00:16:20,610 --> 00:16:25,200
use the tool and then it says ok for

00:16:23,010 --> 00:16:28,020
devices doing requests in parallel such

00:16:25,200 --> 00:16:31,170
as raid arrays or SSDs this number does

00:16:28,020 --> 00:16:33,720
not reflect the performance limits does

00:16:31,170 --> 00:16:37,040
anyone know what utilization reports

00:16:33,720 --> 00:16:44,430
from iostat can anyone explain that

00:16:37,040 --> 00:16:47,310
money it is basically basically the time

00:16:44,430 --> 00:16:50,010
the CPUs pants while waiting on requests

00:16:47,310 --> 00:16:52,350
being served on the device so if we

00:16:50,010 --> 00:16:55,770
issue one request to the device the CPU

00:16:52,350 --> 00:16:58,980
is waiting ok the device is now being

00:16:55,770 --> 00:17:02,940
served in and gets a went back from the

00:16:58,980 --> 00:17:05,310
device driver but if we issue five

00:17:02,940 --> 00:17:07,470
requests and the device can do that in

00:17:05,310 --> 00:17:11,459
parallel then it gets information back

00:17:07,470 --> 00:17:13,470
ok the requests are being done but the

00:17:11,459 --> 00:17:15,660
CPU has spent the same time while

00:17:13,470 --> 00:17:18,420
issuing just one request so if our

00:17:15,660 --> 00:17:20,370
device can conserve more requests in

00:17:18,420 --> 00:17:22,230
parallel the neutralization is not a

00:17:20,370 --> 00:17:25,880
good indicator it's not a good

00:17:22,230 --> 00:17:28,470
performance indicator of doing a short

00:17:25,880 --> 00:17:31,980
calculation here in theory when my

00:17:28,470 --> 00:17:35,420
device can mend my device reports

00:17:31,980 --> 00:17:38,940
forty-four percent utilization while

00:17:35,420 --> 00:17:41,550
reporting over 30,000 I ups then in

00:17:38,940 --> 00:17:44,610
theory if I just calculate this just a

00:17:41,550 --> 00:17:47,460
simple calculation it will serve 24,000

00:17:44,610 --> 00:17:49,410
I apps with 99 percent but if you do the

00:17:47,460 --> 00:17:52,050
actual performance test then you will

00:17:49,410 --> 00:17:54,930
see that you can double the I ups to

00:17:52,050 --> 00:17:57,540
over 40,000 apps but you get 99 percent

00:17:54,930 --> 00:18:00,330
utilization so with SSDs Android devices

00:17:57,540 --> 00:18:01,680
you cannot rely on the utilization

00:18:00,330 --> 00:18:03,480
column of iostat

00:18:01,680 --> 00:18:06,930
this is the second thing you really have

00:18:03,480 --> 00:18:09,120
to keep in mind with iostat so for four

00:18:06,930 --> 00:18:11,490
devices you having requests in parallel

00:18:09,120 --> 00:18:18,240
this utilization column is not a good

00:18:11,490 --> 00:18:20,040
indicator I've told you now that what

00:18:18,240 --> 00:18:22,680
are not good indicators for performance

00:18:20,040 --> 00:18:25,650
with that and I want to you to show what

00:18:22,680 --> 00:18:28,890
are good indicators we have to consider

00:18:25,650 --> 00:18:31,200
that I oath that reports numbers from

00:18:28,890 --> 00:18:33,840
the block layer so there's the blocker

00:18:31,200 --> 00:18:35,430
this is our Linux dodged active Ram who

00:18:33,840 --> 00:18:37,860
knows this theorem who has already

00:18:35,430 --> 00:18:39,960
taking a look at it one person okay nice

00:18:37,860 --> 00:18:43,440
we also have some posters if you want

00:18:39,960 --> 00:18:46,370
that at our booth and just just grab one

00:18:43,440 --> 00:18:49,890
and be happy with our storagetek diagram

00:18:46,370 --> 00:18:52,020
so we want to take a closer look at the

00:18:49,890 --> 00:18:55,080
statistics given from for the block

00:18:52,020 --> 00:18:57,660
layer from iostat and the first one is

00:18:55,080 --> 00:19:00,180
the average request sighs and this is

00:18:57,660 --> 00:19:02,880
the average queue length they're all

00:19:00,180 --> 00:19:06,780
scheduler issues to the disparate cue

00:19:02,880 --> 00:19:09,420
from the driver so if we can dispatch

00:19:06,780 --> 00:19:12,780
more requests to the device driver in

00:19:09,420 --> 00:19:15,390
the same time that our block devices

00:19:12,780 --> 00:19:19,380
performing better because we can serve

00:19:15,390 --> 00:19:21,390
more requests but you have to to keep in

00:19:19,380 --> 00:19:24,180
mind that you have to look at this both

00:19:21,390 --> 00:19:25,920
indicators so you have to take a look at

00:19:24,180 --> 00:19:28,500
the average queue length and you also

00:19:25,920 --> 00:19:31,800
have to take a look at the average time

00:19:28,500 --> 00:19:34,470
the requests needs being served if you

00:19:31,800 --> 00:19:36,180
can serve more requests but also on the

00:19:34,470 --> 00:19:38,340
average waiting time increases a lot

00:19:36,180 --> 00:19:41,760
then you don't have a better device

00:19:38,340 --> 00:19:44,400
because I'm both are increasing but if

00:19:41,760 --> 00:19:46,830
the average queue size is increases and

00:19:44,400 --> 00:19:48,870
the waiting time stays the same then

00:19:46,830 --> 00:19:51,540
your device can serve more requests in

00:19:48,870 --> 00:19:54,540
the same time so these two numbers and

00:19:51,540 --> 00:19:57,480
together are a good indicator for good

00:19:54,540 --> 00:19:59,310
performance of your block device I've

00:19:57,480 --> 00:20:01,770
just some desert in this sentence here

00:19:59,310 --> 00:20:03,120
serving more requests while the waiting

00:20:01,770 --> 00:20:04,800
time is not increasing is a good

00:20:03,120 --> 00:20:07,620
performance indicator so these two

00:20:04,800 --> 00:20:11,480
numbers are from the extended device

00:20:07,620 --> 00:20:11,480
utilization reboot of iOS that

00:20:18,029 --> 00:20:27,840
just take a look at the time so we have

00:20:22,570 --> 00:20:33,820
done for of near 25 comment line tools

00:20:27,840 --> 00:20:37,049
and just take a breath it will not get

00:20:33,820 --> 00:20:39,370
the detailed now on the next few slides

00:20:37,049 --> 00:20:42,340
the first are the next to Livan to

00:20:39,370 --> 00:20:45,909
present to you is the T start command

00:20:42,340 --> 00:20:49,679
line tool and these copines several

00:20:45,909 --> 00:20:52,720
classic tools like vmstat or MP stat and

00:20:49,679 --> 00:20:55,600
the nice thing is that it uses color so

00:20:52,720 --> 00:20:58,029
everybody loves colors and it also has a

00:20:55,600 --> 00:20:59,559
plug-in concept you can call plugins you

00:20:58,029 --> 00:21:01,539
can read about the plugins in the man

00:20:59,559 --> 00:21:03,480
page and for example you can call that

00:21:01,539 --> 00:21:06,970
top memorable game then you get the

00:21:03,480 --> 00:21:09,009
process a DS using them using the most

00:21:06,970 --> 00:21:12,269
memory in your system this is mr.

00:21:09,009 --> 00:21:15,970
Chester Schwartz lied about this that if

00:21:12,269 --> 00:21:18,549
you are interested in network bandwidth

00:21:15,970 --> 00:21:20,289
you can use nixed at nicks that is as

00:21:18,549 --> 00:21:23,799
far as I know also written by Brent and

00:21:20,289 --> 00:21:27,100
Craig and good indicators for your

00:21:23,799 --> 00:21:29,379
network bandwidth are the set duration

00:21:27,100 --> 00:21:31,720
column the utilization column and the

00:21:29,379 --> 00:21:34,840
drops so if you had a lot of if you have

00:21:31,720 --> 00:21:37,000
a lot of drops I'm concerning the TCP

00:21:34,840 --> 00:21:39,610
report from next that then maybe your

00:21:37,000 --> 00:21:42,190
network card is oversaturated and it has

00:21:39,610 --> 00:21:45,269
to drop some packets then you should

00:21:42,190 --> 00:21:48,610
take a look at your network speed and

00:21:45,269 --> 00:21:50,919
set the risk saturation and util are

00:21:48,610 --> 00:21:53,590
always depending on the speed and the

00:21:50,919 --> 00:21:55,840
depeche mode of your network card and

00:21:53,590 --> 00:21:57,700
the good thing about saturation is that

00:21:55,840 --> 00:21:59,559
it also takes drops and errors into

00:21:57,700 --> 00:22:02,769
account so if you have a lot of errors

00:21:59,559 --> 00:22:07,870
and drops then maybe your network card

00:22:02,769 --> 00:22:10,899
is always a g-rated just the short

00:22:07,870 --> 00:22:12,610
question who has some chronic clue about

00:22:10,899 --> 00:22:16,210
his systems data who is collecting

00:22:12,610 --> 00:22:18,370
system data every day one person ok I

00:22:16,210 --> 00:22:22,179
think who is losing graphite who is

00:22:18,370 --> 00:22:24,399
using that ok I'm not very familiar with

00:22:22,179 --> 00:22:26,770
graphite but I've heard a lot of good

00:22:24,399 --> 00:22:30,240
things about it I'm just

00:22:26,770 --> 00:22:32,890
mentioning here as simple a simple

00:22:30,240 --> 00:22:34,630
implementation on how to collect some

00:22:32,890 --> 00:22:36,700
statistics about the system if you just

00:22:34,630 --> 00:22:39,220
want a quick heck to collect some

00:22:36,700 --> 00:22:42,040
statistics then you can use the system

00:22:39,220 --> 00:22:44,230
archive reporter and this is an easy way

00:22:42,040 --> 00:22:46,630
to get some Chronicle data from your

00:22:44,230 --> 00:22:49,030
system just install that and be sure to

00:22:46,630 --> 00:22:52,450
activate the data collector and the

00:22:49,030 --> 00:22:55,570
essay 1 and essay to crunch ups and then

00:22:52,450 --> 00:22:57,670
you can basically collect all the things

00:22:55,570 --> 00:23:00,850
the system archive free board covers

00:22:57,670 --> 00:23:03,430
this is another nice picture done by

00:23:00,850 --> 00:23:05,080
Brandon and it's an overview about what

00:23:03,430 --> 00:23:08,920
you can get with the system arc of

00:23:05,080 --> 00:23:12,880
reporter um who knows Castle who has

00:23:08,920 --> 00:23:15,400
used it and who hates it okay it's

00:23:12,880 --> 00:23:19,090
java-based is unfortunately the chapel

00:23:15,400 --> 00:23:22,840
paste but it's a simple way to check

00:23:19,090 --> 00:23:24,790
what's going on in your system if you

00:23:22,840 --> 00:23:27,430
have problems with character encoding

00:23:24,790 --> 00:23:30,700
and number formats then you can use the

00:23:27,430 --> 00:23:33,430
LC or a variant where we're able to

00:23:30,700 --> 00:23:35,920
ensure that you have the POSIX format in

00:23:33,430 --> 00:23:39,520
your numbers and your date formats in

00:23:35,920 --> 00:23:41,890
the castle output this is just and then

00:23:39,520 --> 00:23:45,340
you can open this file the output file

00:23:41,890 --> 00:23:47,890
you can open with Kaiser this is a quick

00:23:45,340 --> 00:23:52,720
way to to inspect the numbers collected

00:23:47,890 --> 00:23:54,790
by the system archive reporter I've

00:23:52,720 --> 00:23:58,450
mentioned the brook fighting system here

00:23:54,790 --> 00:24:00,640
that to make clear that the Brock file

00:23:58,450 --> 00:24:03,760
system knows everything all the tools we

00:24:00,640 --> 00:24:06,850
have looked now or we have looked now

00:24:03,760 --> 00:24:08,920
are using our collecting the numbers

00:24:06,850 --> 00:24:10,630
from the Brock file system so if you're

00:24:08,920 --> 00:24:13,870
not sure about if your tools are correct

00:24:10,630 --> 00:24:17,230
then you have to look at the brook if I

00:24:13,870 --> 00:24:18,970
just them directly and I did not know

00:24:17,230 --> 00:24:21,190
that there's a Brock info command who

00:24:18,970 --> 00:24:25,210
did who did who knows that command to

00:24:21,190 --> 00:24:29,470
Brock info no one okay it is a comment

00:24:25,210 --> 00:24:31,510
line tool to to output the content of

00:24:29,470 --> 00:24:34,630
the Brock fighters name in a formatted

00:24:31,510 --> 00:24:36,850
way so that's that's a nice tool I did

00:24:34,630 --> 00:24:39,850
not know that just found it while

00:24:36,850 --> 00:24:42,510
preparing for the talk so the proc file

00:24:39,850 --> 00:24:42,510
system knows everything

00:24:45,419 --> 00:24:51,909
in the next section we will watch some

00:24:49,809 --> 00:24:54,909
tools on lines we will do some life

00:24:51,909 --> 00:24:56,679
watching attitudes well top knows

00:24:54,909 --> 00:25:00,970
everybody knows top and everybody's

00:24:56,679 --> 00:25:04,240
using top and then saying okay you get

00:25:00,970 --> 00:25:07,059
pair process metrics for top you can

00:25:04,240 --> 00:25:10,120
also check the 15 and 50 minute load

00:25:07,059 --> 00:25:13,299
average with top and put default the top

00:25:10,120 --> 00:25:15,190
output is sorted by the CPU usage okay

00:25:13,299 --> 00:25:17,409
there's some intrusion into action

00:25:15,190 --> 00:25:20,710
system running OSX and it is consuming

00:25:17,409 --> 00:25:24,279
some cpu time and also top itself is

00:25:20,710 --> 00:25:29,640
consuming some resources you also have

00:25:24,279 --> 00:25:32,860
information about memory usage and the

00:25:29,640 --> 00:25:35,440
first column you get the top is good and

00:25:32,860 --> 00:25:37,480
what is the total size of the virtual

00:25:35,440 --> 00:25:39,760
memory for the process available and

00:25:37,480 --> 00:25:43,240
this number is also including not

00:25:39,760 --> 00:25:45,039
already mapped heaps there are for

00:25:43,240 --> 00:25:48,299
example not already mapped heap memory

00:25:45,039 --> 00:25:53,409
were also swapped out memory so this is

00:25:48,299 --> 00:25:57,010
in most cases a lot higher than the real

00:25:53,409 --> 00:26:00,220
physical memory usage of the process the

00:25:57,010 --> 00:26:01,840
resident memory is how many blocks are

00:26:00,220 --> 00:26:03,399
allocated and mapped to the address

00:26:01,840 --> 00:26:06,700
space so this is the resident resident

00:26:03,399 --> 00:26:11,950
memory but this memory also includes the

00:26:06,700 --> 00:26:14,110
shared memory and we know that processes

00:26:11,950 --> 00:26:16,590
who share the same libraries also can

00:26:14,110 --> 00:26:19,419
share the same memory so the M mapped

00:26:16,590 --> 00:26:22,750
memory can be used by multiple process

00:26:19,419 --> 00:26:25,630
in parallel and the real memory used by

00:26:22,750 --> 00:26:28,870
the process is somehow between residents

00:26:25,630 --> 00:26:31,299
my nose shape memory and resident itself

00:26:28,870 --> 00:26:34,149
and this is called an anonymous memory

00:26:31,299 --> 00:26:37,950
the actual memory the process really

00:26:34,149 --> 00:26:40,390
uses so normally this is called this is

00:26:37,950 --> 00:26:43,480
increased by malok so if you call malloc

00:26:40,390 --> 00:26:46,120
then the resident memory increases you

00:26:43,480 --> 00:26:48,340
can also get that by checking this death

00:26:46,120 --> 00:26:50,140
am file in the Brock file system as we

00:26:48,340 --> 00:26:51,490
have learned before the stack five are

00:26:50,140 --> 00:26:54,850
the Brock file system knows everything

00:26:51,490 --> 00:26:58,720
these are the three counters

00:26:54,850 --> 00:27:01,900
or virtual resident and shared but not

00:26:58,720 --> 00:27:05,200
in kilobyte but in pages so is there a

00:27:01,900 --> 00:27:10,150
number of pages so normally a page is

00:27:05,200 --> 00:27:12,460
four kilobyte what do we have to

00:27:10,150 --> 00:27:14,560
consider when we use top top can consume

00:27:12,460 --> 00:27:17,650
resources on its own you have seen that

00:27:14,560 --> 00:27:19,930
in the output of top before but you can

00:27:17,650 --> 00:27:23,440
also customize the view with top you can

00:27:19,930 --> 00:27:25,810
add new fields you can only check the

00:27:23,440 --> 00:27:27,760
processes running under specific users

00:27:25,810 --> 00:27:31,660
you can kill a process and you can also

00:27:27,760 --> 00:27:33,520
be nice a process Ettie you also have to

00:27:31,660 --> 00:27:36,280
consider that top can make short living

00:27:33,520 --> 00:27:39,760
processes because it is refreshing its

00:27:36,280 --> 00:27:41,920
output just I think every second and the

00:27:39,760 --> 00:27:44,770
next question is you have high cpu usage

00:27:41,920 --> 00:27:47,950
on your system but what to do now ok so

00:27:44,770 --> 00:27:50,440
a hundred percent of CPUs usage is not

00:27:47,950 --> 00:27:54,910
enough for us we will cover that in the

00:27:50,440 --> 00:27:56,890
tracing part afterwards hdac poet vanced

00:27:54,910 --> 00:28:01,060
amazing top again using color so who

00:27:56,890 --> 00:28:04,690
loves colors can use H top and you can

00:28:01,060 --> 00:28:08,380
also do some interacting with the

00:28:04,690 --> 00:28:11,440
processes by using your by using your f

00:28:08,380 --> 00:28:13,330
patterns like sorting or reen icing or

00:28:11,440 --> 00:28:15,580
even killing like you can do it with the

00:28:13,330 --> 00:28:18,790
normal top but you also have some

00:28:15,580 --> 00:28:24,250
graphical output therefore how cpu usage

00:28:18,790 --> 00:28:26,650
is going on on your system now again

00:28:24,250 --> 00:28:31,140
doing some block leah statistics who has

00:28:26,650 --> 00:28:33,940
already used i/o top ok most of you

00:28:31,140 --> 00:28:36,960
again we are taking a look at some

00:28:33,940 --> 00:28:39,670
random it's some random right testing

00:28:36,960 --> 00:28:41,920
the command running here is doing some

00:28:39,670 --> 00:28:46,030
random right testing and issuing some

00:28:41,920 --> 00:28:47,860
rights to our block device and we can

00:28:46,030 --> 00:28:52,420
check that in real time so that's nice

00:28:47,860 --> 00:28:53,800
about the odor if you just just want to

00:28:52,420 --> 00:28:56,710
know what's going on with your block

00:28:53,800 --> 00:28:59,800
devices then you call I autofit the

00:28:56,710 --> 00:29:01,060
first it's the first tool you call when

00:28:59,800 --> 00:29:02,500
you want to know something about your

00:29:01,060 --> 00:29:07,450
block devices so that's really nice I

00:29:02,500 --> 00:29:08,650
ought up again with bandwidth life you

00:29:07,450 --> 00:29:11,770
said you can use

00:29:08,650 --> 00:29:13,510
if top or networks so if w her get

00:29:11,770 --> 00:29:17,920
statistics for interface and with

00:29:13,510 --> 00:29:19,990
netflix you get them pair process so

00:29:17,920 --> 00:29:22,300
there's a typo need sorry about that so

00:29:19,990 --> 00:29:24,820
the process there's some Firefox running

00:29:22,300 --> 00:29:27,100
here and some sh h connection and with

00:29:24,820 --> 00:29:28,330
networks you get that process and open

00:29:27,100 --> 00:29:30,400
interface sometimes you want to know

00:29:28,330 --> 00:29:32,680
which process is doing io on my network

00:29:30,400 --> 00:29:37,720
and not how much is done so that's nice

00:29:32,680 --> 00:29:41,130
with networks okay nearly half time now

00:29:37,720 --> 00:29:44,790
we have covered more than 13 tools now

00:29:41,130 --> 00:29:47,590
yes these tools are known by the most

00:29:44,790 --> 00:29:52,090
most of our system administrators this

00:29:47,590 --> 00:29:55,780
is or who has already known all the

00:29:52,090 --> 00:29:57,040
tools I've presented now I hope there's

00:29:55,780 --> 00:30:01,180
something new for you okay there's one

00:29:57,040 --> 00:30:04,390
nice so ever that I hope I hope you're

00:30:01,180 --> 00:30:06,220
not too bored too much and so maybe I

00:30:04,390 --> 00:30:12,010
can resent you something new with the

00:30:06,220 --> 00:30:14,050
tracing partner so tracing our profiles

00:30:12,010 --> 00:30:17,020
about usage characteristics in the

00:30:14,050 --> 00:30:20,230
system you count Pacific samples and

00:30:17,020 --> 00:30:22,270
events and you count objects and then

00:30:20,230 --> 00:30:24,850
you collect statistics and you make

00:30:22,270 --> 00:30:27,340
statistics about these objects and the

00:30:24,850 --> 00:30:28,930
next slides will focus on system

00:30:27,340 --> 00:30:33,330
profiling with after its and performance

00:30:28,930 --> 00:30:37,210
and we will also get from time to time

00:30:33,330 --> 00:30:38,830
22 determines and trace points or lines

00:30:37,210 --> 00:30:44,980
of Colonel encode we were defined you

00:30:38,830 --> 00:30:48,880
went F trace has been part in the linux

00:30:44,980 --> 00:30:52,420
kernel scenes 2.6 point on 27 since 2008

00:30:48,880 --> 00:30:54,910
and you can check with a phrase what is

00:30:52,420 --> 00:30:59,110
going on inside the coral and the common

00:30:54,910 --> 00:31:02,200
task is a is to to trace events and to

00:30:59,110 --> 00:31:04,600
see what my process is doing in the

00:31:02,200 --> 00:31:06,040
kernel and we've f2 is configured

00:31:04,600 --> 00:31:07,990
already in the colonel the only thing

00:31:06,040 --> 00:31:12,340
you need is the debug filesystem and

00:31:07,990 --> 00:31:15,580
then you can use f trace so most users

00:31:12,340 --> 00:31:18,070
complain okay there is no simple example

00:31:15,580 --> 00:31:23,400
on how to use F trace but i can show you

00:31:18,070 --> 00:31:23,400
that it's no magic behind so I'm sorry

00:31:24,090 --> 00:31:32,920
so this is a short best script you can

00:31:28,180 --> 00:31:36,240
use to to trace your application to

00:31:32,920 --> 00:31:41,320
trace a single command so at the first

00:31:36,240 --> 00:31:44,620
you you specify the process ID you want

00:31:41,320 --> 00:31:48,240
to use for tracing bye-bye echoing the

00:31:44,620 --> 00:31:51,370
process ID to set F to a spit in the

00:31:48,240 --> 00:31:53,740
next step you stayed okay I want to use

00:31:51,370 --> 00:31:55,300
the function tracer so I want to to

00:31:53,740 --> 00:31:57,460
trace the function the colonel is using

00:31:55,300 --> 00:31:59,740
for the process ID this is done by

00:31:57,460 --> 00:32:02,590
echoing function to the current tracer

00:31:59,740 --> 00:32:05,160
file then you turn and choice it on then

00:32:02,590 --> 00:32:07,870
you execute the command with the exact

00:32:05,160 --> 00:32:12,100
if the exit statement and then you turn

00:32:07,870 --> 00:32:19,210
racing off so I'm just calling this now

00:32:12,100 --> 00:32:25,840
okay i'm using the date command here and

00:32:19,210 --> 00:32:27,700
then we can sorry oops here so this is a

00:32:25,840 --> 00:32:31,480
lot of lines in it this file this is the

00:32:27,700 --> 00:32:32,980
trace file the trace file now contains

00:32:31,480 --> 00:32:34,990
the trace about the command we have

00:32:32,980 --> 00:32:37,060
traced now and this is the f trace

00:32:34,990 --> 00:32:40,830
output and these are a lot of lines we

00:32:37,060 --> 00:32:43,660
can we will learn some graphical

00:32:40,830 --> 00:32:46,420
improvement on how to interpret these

00:32:43,660 --> 00:32:49,060
lots of samples by using Flame graphs

00:32:46,420 --> 00:32:52,900
afterwards so if you want to know just

00:32:49,060 --> 00:32:54,940
ok what two scores and what kernel

00:32:52,900 --> 00:32:57,940
functions is my project using you can

00:32:54,940 --> 00:33:09,040
achieve that with F trees and these are

00:32:57,940 --> 00:33:11,440
just for five lines of bash there's also

00:33:09,040 --> 00:33:14,050
an easy a command-line interface to all

00:33:11,440 --> 00:33:16,300
the rock up to all the files in the

00:33:14,050 --> 00:33:18,360
debug file system and this command line

00:33:16,300 --> 00:33:22,450
interface is called traced command who

00:33:18,360 --> 00:33:25,120
has already used phrase command person

00:33:22,450 --> 00:33:27,070
okay nice and then you don't have to

00:33:25,120 --> 00:33:28,600
figure round with the files in the debug

00:33:27,070 --> 00:33:30,640
file system but just called race command

00:33:28,600 --> 00:33:34,560
and you can start the trace command you

00:33:30,640 --> 00:33:34,560
can filter for functions and so on

00:33:34,990 --> 00:33:40,510
the perfect Wentz and the perv tools and

00:33:37,540 --> 00:33:42,820
where formerly known as permanent

00:33:40,510 --> 00:33:45,940
performance counters for linux and they

00:33:42,820 --> 00:33:51,220
also get a lot of updates in the kernel

00:33:45,940 --> 00:33:52,990
version 4.1 and with the perfect ants

00:33:51,220 --> 00:33:56,170
you can count cpu performance counters

00:33:52,990 --> 00:33:59,650
but you can also collect statistics

00:33:56,170 --> 00:34:03,790
about static trace points k probes and

00:33:59,650 --> 00:34:06,940
you probes and the perfect ones and the

00:34:03,790 --> 00:34:09,820
perf tools are included in the linux

00:34:06,940 --> 00:34:11,800
tools common package and this is true

00:34:09,820 --> 00:34:14,590
for Ubuntu I don't know if just the same

00:34:11,800 --> 00:34:17,080
for Debian or but at least in a bundle

00:34:14,590 --> 00:34:18,600
you can just install the tools common

00:34:17,080 --> 00:34:23,320
package then you have all the proof

00:34:18,600 --> 00:34:25,810
commands available perf list is the

00:34:23,320 --> 00:34:27,760
first command we will look at I have

00:34:25,810 --> 00:34:31,659
highlighted the hardware events here we

00:34:27,760 --> 00:34:33,760
can see ok we can check how many CPU

00:34:31,659 --> 00:34:35,860
cycles or how many instructions my

00:34:33,760 --> 00:34:39,669
application is using so if I call a

00:34:35,860 --> 00:34:41,800
command then I can record with the perf

00:34:39,669 --> 00:34:44,679
tools how many instructions my

00:34:41,800 --> 00:34:46,840
application is doing and this is not

00:34:44,679 --> 00:34:48,909
only limited to Hardware vents but it

00:34:46,840 --> 00:34:50,679
also includes static trace point slide

00:34:48,909 --> 00:34:53,649
we have learned that before with F

00:34:50,679 --> 00:34:55,990
traced and there are nearly 2,000 it

00:34:53,649 --> 00:34:59,260
went saleable if you call proof list at

00:34:55,990 --> 00:35:00,760
least this is for current colonel in you

00:34:59,260 --> 00:35:02,800
wonder I don't know if that's the same

00:35:00,760 --> 00:35:04,869
for older kernels so there are a lot of

00:35:02,800 --> 00:35:07,600
hard we went you can collect a new

00:35:04,869 --> 00:35:14,310
consumer with the perv command line

00:35:07,600 --> 00:35:19,330
tools if you are not satisfied with the

00:35:14,310 --> 00:35:23,109
predefined hardware events by the perf

00:35:19,330 --> 00:35:25,780
tools you can dive deeper into the

00:35:23,109 --> 00:35:28,690
hardware counters your CPU provides you

00:35:25,780 --> 00:35:31,359
just have to check the manufacturers

00:35:28,690 --> 00:35:35,560
documentation for your specific CPU

00:35:31,359 --> 00:35:37,450
that's at least true for Intel it

00:35:35,560 --> 00:35:39,040
documents the raw hardware counters I

00:35:37,450 --> 00:35:42,250
don't know if that's also true for other

00:35:39,040 --> 00:35:45,850
javea vendors and then you can use

00:35:42,250 --> 00:35:48,610
so-called on Raw CPU masks you can use

00:35:45,850 --> 00:35:50,470
with their proof commands um and

00:35:48,610 --> 00:35:53,740
there's also a nice way to found the

00:35:50,470 --> 00:35:57,130
rock counters with the lip pfm I'm

00:35:53,740 --> 00:36:00,160
library you can see that if you grab for

00:35:57,130 --> 00:36:03,580
the raw CPU mask then poof list allows

00:36:00,160 --> 00:36:05,380
some raw counters and you can get that

00:36:03,580 --> 00:36:07,420
with the with the library of just

00:36:05,380 --> 00:36:09,370
mentioned here you can say okay I'm

00:36:07,420 --> 00:36:11,620
calling sure we went into and I want to

00:36:09,370 --> 00:36:14,140
know something about my last level cache

00:36:11,620 --> 00:36:15,790
misses my application produces then you

00:36:14,140 --> 00:36:19,600
can grab for the last level cache misses

00:36:15,790 --> 00:36:23,170
and then you can use that mask for the

00:36:19,600 --> 00:36:25,660
check you went by lip f lip pfm for and

00:36:23,170 --> 00:36:30,730
then you get the raw CPU mask and you

00:36:25,660 --> 00:36:33,100
can use that cpu counter that raw mask

00:36:30,730 --> 00:36:35,350
in conjunction with the proof stop

00:36:33,100 --> 00:36:38,620
command and then you can uncheck the raw

00:36:35,350 --> 00:36:41,470
cpu counters but you have to keep in

00:36:38,620 --> 00:36:44,500
mind that this masks are different for

00:36:41,470 --> 00:36:49,410
every cpu architecture you have to check

00:36:44,500 --> 00:36:49,410
the documentation for your specific cpu

00:36:50,820 --> 00:36:56,980
and what we've got here that we can now

00:36:54,280 --> 00:37:01,540
collect the last level cache misses for

00:36:56,980 --> 00:37:03,100
our application with this raw mask I've

00:37:01,540 --> 00:37:06,460
mentioned before that perf also has

00:37:03,100 --> 00:37:09,190
traced functionalities we can grep here

00:37:06,460 --> 00:37:13,120
for the trace when's defined by the perf

00:37:09,190 --> 00:37:14,830
tools you can check trace points in the

00:37:13,120 --> 00:37:16,830
file system you can check trace points

00:37:14,830 --> 00:37:21,000
in the block Leah and you can also

00:37:16,830 --> 00:37:24,310
record the number of cisco's your your

00:37:21,000 --> 00:37:26,500
your process or your your program is

00:37:24,310 --> 00:37:30,220
doing I've done that for a simple hello

00:37:26,500 --> 00:37:33,070
world command here written in C and you

00:37:30,220 --> 00:37:35,710
can say okay just printing out hello

00:37:33,070 --> 00:37:38,230
world using is using eight Cisco so

00:37:35,710 --> 00:37:40,390
that's that's a simple way with with

00:37:38,230 --> 00:37:42,430
perf you can check okay how many

00:37:40,390 --> 00:37:44,710
syscalls is my application actually

00:37:42,430 --> 00:37:47,170
doing and that's not only limited to

00:37:44,710 --> 00:37:48,730
cisco's but as I've said it he'll also

00:37:47,170 --> 00:37:50,980
that's true for a file system over a

00:37:48,730 --> 00:37:57,910
block layer or whatever you can check

00:37:50,980 --> 00:38:01,540
that nearly two thousand events I've

00:37:57,910 --> 00:38:02,350
collected a summary of the of the events

00:38:01,540 --> 00:38:03,850
done for

00:38:02,350 --> 00:38:06,580
trix multiplication written in pattern

00:38:03,850 --> 00:38:08,320
here and that's an easy way to compare

00:38:06,580 --> 00:38:11,470
multiple algorithms how well they

00:38:08,320 --> 00:38:13,840
perform on your cpu so you can collect

00:38:11,470 --> 00:38:15,490
an overview of statistics for your

00:38:13,840 --> 00:38:18,490
program with the perf step command and

00:38:15,490 --> 00:38:20,170
you can see ok how many context context

00:38:18,490 --> 00:38:23,350
switches how many instructions how many

00:38:20,170 --> 00:38:25,780
branch mrs. are there on my on my cpu

00:38:23,350 --> 00:38:28,090
for a specific algorithm and if you have

00:38:25,780 --> 00:38:30,010
another argument you think that performs

00:38:28,090 --> 00:38:34,900
better than you can compare these values

00:38:30,010 --> 00:38:37,360
by collecting the perv stab you wins ok

00:38:34,900 --> 00:38:38,890
a good indicators also the instructions

00:38:37,360 --> 00:38:40,240
per cycle so if more if you have more

00:38:38,890 --> 00:38:41,980
instructions per cycle than your

00:38:40,240 --> 00:38:47,710
algorithm is performing better on on the

00:38:41,980 --> 00:38:50,530
cpu we've proved record you can not only

00:38:47,710 --> 00:38:53,230
print the perfect ones add two to

00:38:50,530 --> 00:38:55,570
standard out but you can also record

00:38:53,230 --> 00:38:57,550
these samples to a file which then can

00:38:55,570 --> 00:38:59,830
be post possessed with pro free put and

00:38:57,550 --> 00:39:02,220
then you can filter for events you can

00:38:59,830 --> 00:39:04,690
filter for on specific functions and

00:39:02,220 --> 00:39:07,780
what is nice about perfect code is that

00:39:04,690 --> 00:39:11,260
you that you can also can record so

00:39:07,780 --> 00:39:14,350
called on congress which is a history or

00:39:11,260 --> 00:39:16,030
a call stack from about your of your

00:39:14,350 --> 00:39:21,550
functions you are using in your program

00:39:16,030 --> 00:39:23,890
a nice way to record all events that are

00:39:21,550 --> 00:39:27,700
currently running on your system is to

00:39:23,890 --> 00:39:29,680
use paracord in in conjunction with the

00:39:27,700 --> 00:39:33,100
minus a switch which says okay i want to

00:39:29,680 --> 00:39:35,170
record on all cpus available and we also

00:39:33,100 --> 00:39:37,090
want to collect call graphs and then i'm

00:39:35,170 --> 00:39:39,520
just using this lip comment and all

00:39:37,090 --> 00:39:42,880
events are our collected currently

00:39:39,520 --> 00:39:45,070
running on your system and as you can

00:39:42,880 --> 00:39:46,690
see if these numbers this 45 seconds

00:39:45,070 --> 00:39:49,360
this is quite a high number of samples

00:39:46,690 --> 00:39:52,240
so nearly 11 hundred thousand samples

00:39:49,360 --> 00:39:54,100
for just five seconds and as you can

00:39:52,240 --> 00:39:56,200
imagine this there are quite a lot of

00:39:54,100 --> 00:40:00,700
samples if you called it if we call it

00:39:56,200 --> 00:40:03,940
longer and with perf report you can then

00:40:00,700 --> 00:40:09,730
display a profile about a former about

00:40:03,940 --> 00:40:14,940
running record and okay this is a little

00:40:09,730 --> 00:40:16,059
smaller but this is a report about

00:40:14,940 --> 00:40:18,369
collect

00:40:16,059 --> 00:40:22,089
dude record from a DD command just

00:40:18,369 --> 00:40:25,900
copying one megabyte from def 02 my

00:40:22,089 --> 00:40:30,369
block device and I'm edit some nice

00:40:25,900 --> 00:40:32,439
options to the perf report I want I said

00:40:30,369 --> 00:40:36,880
okay I want to show the cpu utilization

00:40:32,439 --> 00:40:39,549
my program spent in in Sue's calls and

00:40:36,880 --> 00:40:41,979
encounter functions and this was an

00:40:39,549 --> 00:40:46,029
encrypted file systems okay some time

00:40:41,979 --> 00:40:49,650
was spent in encrypting the actual data

00:40:46,029 --> 00:40:52,199
ok this is equipped FS so the default

00:40:49,650 --> 00:40:55,599
encrypted file system used by Ubuntu and

00:40:52,199 --> 00:40:58,839
then we can see ok also some time has

00:40:55,599 --> 00:41:01,989
been spent in there in the actual riped

00:40:58,839 --> 00:41:04,329
of TV so this is nice if you if you want

00:41:01,989 --> 00:41:07,719
to know where did my program spend its

00:41:04,329 --> 00:41:10,779
CPU time in which in which function this

00:41:07,719 --> 00:41:16,809
is just a quick Hector to check what

00:41:10,779 --> 00:41:18,880
your application is doing Brendan greg

00:41:16,809 --> 00:41:22,059
has also written some nice tools to show

00:41:18,880 --> 00:41:25,479
what is possible with with F Chris and

00:41:22,059 --> 00:41:27,880
the performance some good examples are I

00:41:25,479 --> 00:41:29,619
owe snoop which shows that i owe access

00:41:27,880 --> 00:41:32,349
for your blog devices including its

00:41:29,619 --> 00:41:35,859
latency so you can check what latency

00:41:32,349 --> 00:41:38,170
the command is producing you can check

00:41:35,859 --> 00:41:40,119
with cast at how well your linux page

00:41:38,170 --> 00:41:43,949
cache is used and i want to show you

00:41:40,119 --> 00:41:48,279
that these are really simple bash script

00:41:43,949 --> 00:41:53,229
ok so we are checking this is other

00:41:48,279 --> 00:41:56,259
proof tools and all what is done here is

00:41:53,229 --> 00:42:02,289
that we have we collect events about

00:41:56,259 --> 00:42:03,939
this for this for Colonel counters so we

00:42:02,289 --> 00:42:05,259
are measuring the cache access we are

00:42:03,939 --> 00:42:07,509
measuring the cash whites and we are

00:42:05,259 --> 00:42:11,140
measuring some page statistics and with

00:42:07,509 --> 00:42:13,539
this for functions we can do some

00:42:11,140 --> 00:42:16,779
calculation and say okay the page case

00:42:13,539 --> 00:42:19,089
is used that while on my system these

00:42:16,779 --> 00:42:22,749
are not so this batch script is not very

00:42:19,089 --> 00:42:27,759
long you see that that's all and if we

00:42:22,749 --> 00:42:29,800
call cash that here then it says ok with

00:42:27,759 --> 00:42:31,390
that amount of hits with that

00:42:29,800 --> 00:42:34,330
amount of mrs. that amount of dirty

00:42:31,390 --> 00:42:37,330
pages in my page cache and buffers and

00:42:34,330 --> 00:42:40,300
the cash usage we have learned before

00:42:37,330 --> 00:42:43,930
with pits that and MPs are and free am

00:42:40,300 --> 00:42:53,170
stat so these are nice examples what can

00:42:43,930 --> 00:42:56,710
be done with the girth tools we've done

00:42:53,170 --> 00:42:59,560
now nearly 20 tools so let's go on to

00:42:56,710 --> 00:43:02,140
the to the final few tools um who knows

00:42:59,560 --> 00:43:05,560
what a flame graph is who can explain a

00:43:02,140 --> 00:43:08,730
frame breath okay that's one person just

00:43:05,560 --> 00:43:13,330
short some words about the flame breath

00:43:08,730 --> 00:43:16,210
yes I'll you've seen them okay so this

00:43:13,330 --> 00:43:19,720
is a nice nice graphic about flame

00:43:16,210 --> 00:43:22,840
graphs so flame graph so my dad this is

00:43:19,720 --> 00:43:24,960
my working day and processing the emails

00:43:22,840 --> 00:43:29,500
and I'm doing some back fixing and

00:43:24,960 --> 00:43:31,570
they're from the upper layers are always

00:43:29,500 --> 00:43:34,630
on call traces so this is actually a

00:43:31,570 --> 00:43:38,500
back trace of the of the commands issued

00:43:34,630 --> 00:43:41,440
and at the top at the top level is where

00:43:38,500 --> 00:43:44,050
the CPU time is spent so at this level

00:43:41,440 --> 00:43:47,800
the actual CPU time is spent the colors

00:43:44,050 --> 00:43:50,200
are chosen by random and the the width

00:43:47,800 --> 00:43:51,820
of the boxes are the samples collected

00:43:50,200 --> 00:43:53,890
so the longer the box is the more

00:43:51,820 --> 00:43:56,830
samples from this function have been

00:43:53,890 --> 00:43:58,570
collected and yes so the actually work

00:43:56,830 --> 00:44:08,830
we do is back fixing verifying issues

00:43:58,570 --> 00:44:10,510
and cooking and testing a lot and as we

00:44:08,830 --> 00:44:13,780
have heard before with perf record we

00:44:10,510 --> 00:44:16,870
can also record the call graphs so we

00:44:13,780 --> 00:44:21,280
add the minus G option here for our DD

00:44:16,870 --> 00:44:23,560
command then we take that record as

00:44:21,280 --> 00:44:27,190
input for perl script first script can

00:44:23,560 --> 00:44:30,340
format and output a perf record and we

00:44:27,190 --> 00:44:32,320
can bet pipe to the flame graph v2 a

00:44:30,340 --> 00:44:35,020
flame graph script also done by Brandon

00:44:32,320 --> 00:44:37,060
and collapse it in the first step in the

00:44:35,020 --> 00:44:40,690
next step we can produce an

00:44:37,060 --> 00:44:43,210
and in picture as SVG picture which is

00:44:40,690 --> 00:44:45,280
also interactive so if you roll over

00:44:43,210 --> 00:44:47,020
your mouth or your function then you can

00:44:45,280 --> 00:44:49,750
see some more information that you can

00:44:47,020 --> 00:44:51,340
also dive in you Jeff just to open the

00:44:49,750 --> 00:44:53,560
file with Firefox and then you can do

00:44:51,340 --> 00:44:55,600
that interactively so this is a nice

00:44:53,560 --> 00:44:58,210
frame graph about our DD command and

00:44:55,600 --> 00:45:00,430
again we see that we have used an

00:44:58,210 --> 00:45:02,920
encrypted file system so we've done son

00:45:00,430 --> 00:45:06,880
encryption here so there's will cpu

00:45:02,920 --> 00:45:10,480
usage for filesystem encryption and then

00:45:06,880 --> 00:45:12,850
we also have the the real data access

00:45:10,480 --> 00:45:14,830
from our duty commander this is just a

00:45:12,850 --> 00:45:17,440
simple command and as you can see here

00:45:14,830 --> 00:45:20,350
in this simple example this can be this

00:45:17,440 --> 00:45:23,650
can become quite complicated if you have

00:45:20,350 --> 00:45:25,990
more samples and if you have a lot of

00:45:23,650 --> 00:45:27,550
work doing on your cpu but with these

00:45:25,990 --> 00:45:29,500
flame graphs you can see okay if you

00:45:27,550 --> 00:45:32,050
have a lot of picks up and a lot of if

00:45:29,500 --> 00:45:35,620
the occult races are getting more and

00:45:32,050 --> 00:45:46,000
more then you can just get a nice view

00:45:35,620 --> 00:45:49,600
of where your cpu time is spent i'm

00:45:46,000 --> 00:45:53,410
stating here lexi is some special topic

00:45:49,600 --> 00:45:56,230
because we will leave now the the

00:45:53,410 --> 00:45:59,080
environment of traditional command line

00:45:56,230 --> 00:46:03,850
interface tools and go over to lexi and

00:45:59,080 --> 00:46:08,530
my sick poor counters um who is using

00:46:03,850 --> 00:46:12,040
lexion here okay well that's quite

00:46:08,530 --> 00:46:17,530
surprising not more people who is using

00:46:12,040 --> 00:46:21,010
docker okay not more and lexie

00:46:17,530 --> 00:46:23,590
containers are using two features

00:46:21,010 --> 00:46:26,020
provided by a linux kernel not only

00:46:23,590 --> 00:46:28,600
these two but most of these two the

00:46:26,020 --> 00:46:31,420
first one hour see groups see groups

00:46:28,600 --> 00:46:34,690
petition tasks and create a hierarchical

00:46:31,420 --> 00:46:37,080
group of isolated resources so see

00:46:34,690 --> 00:46:40,900
groups on group things together and

00:46:37,080 --> 00:46:43,990
namespaces namespaces create a future

00:46:40,900 --> 00:46:47,920
processes that they think okay to have

00:46:43,990 --> 00:46:49,630
an isolated on resource so if a process

00:46:47,920 --> 00:46:50,530
in is a namespace then you can think

00:46:49,630 --> 00:46:53,590
okay a half

00:46:50,530 --> 00:46:55,180
an isolated resource but in reality is

00:46:53,590 --> 00:46:58,480
sharing the resource with with other

00:46:55,180 --> 00:47:00,460
processes in other namespaces and each

00:46:58,480 --> 00:47:03,430
continuous and the same kernel running

00:47:00,460 --> 00:47:05,470
on the host and some like about Lexi

00:47:03,430 --> 00:47:08,080
their native performance because there

00:47:05,470 --> 00:47:09,970
is no real hypervisor between the day

00:47:08,080 --> 00:47:13,750
hardware and actual running virtual

00:47:09,970 --> 00:47:16,500
machines virtual machines is is maybe

00:47:13,750 --> 00:47:19,840
not the right term here because it's

00:47:16,500 --> 00:47:23,640
it's somehow some lightweight virtual

00:47:19,840 --> 00:47:26,020
machine and we've see groups you can

00:47:23,640 --> 00:47:30,070
divide your system into several

00:47:26,020 --> 00:47:35,620
subsystems you can define CPU sets which

00:47:30,070 --> 00:47:38,260
let you divide your CPU time in multiple

00:47:35,620 --> 00:47:40,630
parts so you can say okay my sequel like

00:47:38,260 --> 00:47:43,660
sea container can use forty percent of

00:47:40,630 --> 00:47:46,870
CPU sets so you can divide your CPU

00:47:43,660 --> 00:47:49,630
resource you can also do that with the

00:47:46,870 --> 00:47:53,140
block Yossi group so you can say okay my

00:47:49,630 --> 00:47:55,720
lexy container a can only use a hundred

00:47:53,140 --> 00:47:59,500
of megabyte for concerning the bandwidth

00:47:55,720 --> 00:48:02,530
for block device a and you can also

00:47:59,500 --> 00:48:07,000
specify cpu core painting you can say ok

00:48:02,530 --> 00:48:10,900
my sequel container one can only use cpu

00:48:07,000 --> 00:48:12,970
core ones 224 that's nice we've see

00:48:10,900 --> 00:48:16,390
groups and there are also some see

00:48:12,970 --> 00:48:18,130
groups subsystems for memory and some

00:48:16,390 --> 00:48:22,990
more you can check that in the kernel

00:48:18,130 --> 00:48:26,530
documentation we have one running linux

00:48:22,990 --> 00:48:29,230
container here is its name is ubuntu one

00:48:26,530 --> 00:48:32,410
and if you check its counters with the

00:48:29,230 --> 00:48:36,190
lexi info command you can see ok they

00:48:32,410 --> 00:48:38,350
are numbers about the cpu usage number

00:48:36,190 --> 00:48:40,230
about the block i use usage and numbers

00:48:38,350 --> 00:48:42,730
about the memory usage and the

00:48:40,230 --> 00:48:44,560
containers actually using 30 bytes of

00:48:42,730 --> 00:48:47,710
memory that's quite a small footprint

00:48:44,560 --> 00:48:51,580
and it has not used as if you very much

00:48:47,710 --> 00:48:53,890
but um i have i wanted to know where do

00:48:51,580 --> 00:48:55,720
these numbers come from and i've

00:48:53,890 --> 00:48:57,820
prepared a small table about that so the

00:48:55,720 --> 00:49:02,500
cpu you search is actually coming from

00:48:57,820 --> 00:49:04,090
the cpu account MC group this is the use

00:49:02,500 --> 00:49:06,910
its parameter in the sea

00:49:04,090 --> 00:49:11,170
bua can see group this is CP usage the

00:49:06,910 --> 00:49:14,560
block io usage has its origin in the

00:49:11,170 --> 00:49:16,720
block IOC group so there's a sub

00:49:14,560 --> 00:49:20,320
parameter called throttled IOC always

00:49:16,720 --> 00:49:22,090
bites the memory usage is the memory

00:49:20,320 --> 00:49:24,670
usage in bytes thats the memories you

00:49:22,090 --> 00:49:27,850
group that's not very surprising and you

00:49:24,670 --> 00:49:31,360
can also check the core memory used by

00:49:27,850 --> 00:49:33,430
the container and the link statistics

00:49:31,360 --> 00:49:34,990
desist sorry this is the last one this

00:49:33,430 --> 00:49:37,330
are the links that did sick they come

00:49:34,990 --> 00:49:38,800
from the Susa Fez interface so this is

00:49:37,330 --> 00:49:41,050
not from a SI group but from the sea

00:49:38,800 --> 00:49:43,900
surface interface and there's even more

00:49:41,050 --> 00:49:47,080
there see groups are providing you can

00:49:43,900 --> 00:49:49,360
check memories that these are detailed

00:49:47,080 --> 00:49:52,690
information like we get them from the

00:49:49,360 --> 00:49:54,910
block vmstat file the memory fail count

00:49:52,690 --> 00:49:57,970
is also interesting this is the number

00:49:54,910 --> 00:50:00,160
of times the container has hit its

00:49:57,970 --> 00:50:02,140
memory limits so if you specify one

00:50:00,160 --> 00:50:05,350
gigabyte memory limit and the container

00:50:02,140 --> 00:50:08,770
hits that limit then memory fail count

00:50:05,350 --> 00:50:14,190
is increased and you can also check the

00:50:08,770 --> 00:50:17,050
CPUs the container is using that's just

00:50:14,190 --> 00:50:19,840
just a note that's a nice value to check

00:50:17,050 --> 00:50:21,580
with nagios or a singer because then you

00:50:19,840 --> 00:50:23,200
know that your container is consuming

00:50:21,580 --> 00:50:25,990
too much memory and you have to increase

00:50:23,200 --> 00:50:31,840
that memory we have to to watch fail

00:50:25,990 --> 00:50:35,080
count there is also a top command for

00:50:31,840 --> 00:50:37,150
Lexi and that is called Lexi top and

00:50:35,080 --> 00:50:38,860
with that you can get an overview about

00:50:37,150 --> 00:50:40,840
the current running containers in your

00:50:38,860 --> 00:50:44,890
system you can check the CPU the blocker

00:50:40,840 --> 00:50:47,140
oh and the memory usage and just one

00:50:44,890 --> 00:50:50,100
note at the end about Lexi traditional

00:50:47,140 --> 00:50:54,940
tools without like CFS and do not work

00:50:50,100 --> 00:50:57,970
does anyone know why not so when you

00:50:54,940 --> 00:50:59,740
call free in a container is that really

00:50:57,970 --> 00:51:04,720
the memory usage the container has

00:50:59,740 --> 00:51:06,850
actually or does anyone know that who is

00:51:04,720 --> 00:51:09,070
using their containers who is using

00:51:06,850 --> 00:51:12,210
Alexi you raise your hand again okay so

00:51:09,070 --> 00:51:15,220
if you call free in a container you I

00:51:12,210 --> 00:51:17,829
don't know okay so if you call free

00:51:15,220 --> 00:51:20,140
inside a container inside Lexi

00:51:17,829 --> 00:51:23,109
you don't get the correct numbers I have

00:51:20,140 --> 00:51:26,170
specified a memory limit of 30 megabyte

00:51:23,109 --> 00:51:29,950
here for the container called ubuntu one

00:51:26,170 --> 00:51:32,259
and if we call free inside the container

00:51:29,950 --> 00:51:36,130
so this is inside the container then it

00:51:32,259 --> 00:51:38,380
reports free reports okay there are 287

00:51:36,130 --> 00:51:40,709
megabytes free but that's not correct

00:51:38,380 --> 00:51:46,619
but as you've seen before I've specified

00:51:40,709 --> 00:51:49,359
actually 30 megabytes so this is because

00:51:46,619 --> 00:51:51,609
the broke file system from the host is

00:51:49,359 --> 00:51:54,789
not correctly mapped into the container

00:51:51,609 --> 00:51:56,979
itself so we need some kind of bind

00:51:54,789 --> 00:51:59,200
mounting the Brock file system inside

00:51:56,979 --> 00:52:03,729
the container and this is currently done

00:51:59,200 --> 00:52:05,229
by leg CFS so like CFS ensures that the

00:52:03,729 --> 00:52:07,749
Brock file system has the correct

00:52:05,229 --> 00:52:10,299
numbers inside the container if you have

00:52:07,749 --> 00:52:12,309
like CFS installed and mount it then and

00:52:10,299 --> 00:52:15,969
you call free inside the container then

00:52:12,309 --> 00:52:17,920
you get the correct numbers so just keep

00:52:15,969 --> 00:52:19,569
that in mind that you need like CFS for

00:52:17,920 --> 00:52:25,839
calling traditional tools inside the

00:52:19,569 --> 00:52:28,859
container the last section is about my

00:52:25,839 --> 00:52:32,799
sequel these are just to slide to think

00:52:28,859 --> 00:52:35,849
so if you deal with my sequel you do not

00:52:32,799 --> 00:52:40,449
come around the percona tools and some

00:52:35,849 --> 00:52:42,039
percona monitoring plugins pakona

00:52:40,449 --> 00:52:44,019
provides a lots of useful information

00:52:42,039 --> 00:52:47,619
and lots of good tools concerning my

00:52:44,019 --> 00:52:49,719
sequel and if you have a running my

00:52:47,619 --> 00:52:51,519
sequel server in the first step you want

00:52:49,719 --> 00:52:54,640
to gain or weight a short summary you

00:52:51,519 --> 00:52:57,640
can use PT mais my sequel summary then

00:52:54,640 --> 00:52:59,920
you get some you get a very long list of

00:52:57,640 --> 00:53:02,079
detailed information what is interesting

00:52:59,920 --> 00:53:04,329
here is maybe they're currently running

00:53:02,079 --> 00:53:07,869
my sequel circuit so what secret are you

00:53:04,329 --> 00:53:10,359
using what socket are you using and you

00:53:07,869 --> 00:53:12,640
can also check their currently running

00:53:10,359 --> 00:53:15,039
user and the time the server is running

00:53:12,640 --> 00:53:17,319
and to host name and the databases you

00:53:15,039 --> 00:53:21,789
have cities there are 15 databases at

00:53:17,319 --> 00:53:25,569
this host and if you want some extended

00:53:21,789 --> 00:53:29,259
status for them my sequel server and you

00:53:25,569 --> 00:53:31,120
can get it with that my sequel admin

00:53:29,259 --> 00:53:34,030
command and we

00:53:31,120 --> 00:53:36,280
during the extended status and as I've

00:53:34,030 --> 00:53:38,050
highlighted here with the word count in

00:53:36,280 --> 00:53:40,930
fact I'm counting the lines here you can

00:53:38,050 --> 00:53:42,910
see that have more than 300 count as

00:53:40,930 --> 00:53:45,010
your available and the good thing is

00:53:42,910 --> 00:53:47,650
that these counters can be wanted toward

00:53:45,010 --> 00:53:51,040
with their PMP check my secret status

00:53:47,650 --> 00:53:52,690
plugin so I really recommend to take a

00:53:51,040 --> 00:53:55,690
look at this campus and if there are

00:53:52,690 --> 00:53:57,580
several counters you want to to monitor

00:53:55,690 --> 00:53:59,620
then just install this plugin and say

00:53:57,580 --> 00:54:02,770
okay I want to to monitor this and this

00:53:59,620 --> 00:54:05,830
counter from the mask my sequel admin

00:54:02,770 --> 00:54:09,460
command the slow query logo is also

00:54:05,830 --> 00:54:12,400
quite common um the slow query log is

00:54:09,460 --> 00:54:14,830
logging my sequel commands exceeding a

00:54:12,400 --> 00:54:17,560
specific one time so if you specify ok

00:54:14,830 --> 00:54:20,740
five seconds for the slow query log

00:54:17,560 --> 00:54:22,900
every query hitting this limit gets

00:54:20,740 --> 00:54:27,030
locked in the log file it is off by

00:54:22,900 --> 00:54:31,270
default and it ignores the query cache

00:54:27,030 --> 00:54:33,220
but it is also an easy way to log all

00:54:31,270 --> 00:54:36,130
the queries currently running on your my

00:54:33,220 --> 00:54:38,470
sequel server because if you specify 04

00:54:36,130 --> 00:54:40,690
along every time that all queries gets

00:54:38,470 --> 00:54:43,780
locked in the query and then you can

00:54:40,690 --> 00:54:46,300
generate a nice we've report with the PT

00:54:43,780 --> 00:54:49,990
crew attaches command and this command

00:54:46,300 --> 00:54:52,720
processes to slow log and produces a

00:54:49,990 --> 00:54:54,490
detailed report about your slow query

00:54:52,720 --> 00:54:57,730
log and also prints a lot of statistics

00:54:54,490 --> 00:55:04,480
about your queries run at your my my

00:54:57,730 --> 00:55:09,990
sequel server and the statements are

00:55:04,480 --> 00:55:13,930
paid default sorted by response time and

00:55:09,990 --> 00:55:16,420
there's also one interesting indicator

00:55:13,930 --> 00:55:19,660
about your my sequel statements this is

00:55:16,420 --> 00:55:24,150
the variation two main parameter and

00:55:19,660 --> 00:55:24,150
this is the deviation to the mean time

00:55:24,180 --> 00:55:31,510
normally statements need to take so if

00:55:28,000 --> 00:55:33,130
this variation to mean parameters we are

00:55:31,510 --> 00:55:38,530
we hadn't is a good indicator that you

00:55:33,130 --> 00:55:40,840
need to improve that statement last but

00:55:38,530 --> 00:55:43,030
not least there's also a life analysis

00:55:40,840 --> 00:55:44,590
tool for statements currently running on

00:55:43,030 --> 00:55:44,829
your my sequel server and this is called

00:55:44,590 --> 00:55:48,969
in

00:55:44,829 --> 00:55:51,999
top it is not only for in Adobe but

00:55:48,969 --> 00:55:53,799
mostly used to it and if you call in the

00:55:51,999 --> 00:55:55,749
top then you get a live view about the

00:55:53,799 --> 00:55:57,759
currently running statements at your my

00:55:55,749 --> 00:56:00,400
sequel server that's really nice if you

00:55:57,759 --> 00:56:02,140
have some high cpu utilization for from

00:56:00,400 --> 00:56:03,670
your my sequel server and you want to

00:56:02,140 --> 00:56:05,619
check okay so what is currently running

00:56:03,670 --> 00:56:10,239
which statements are currently running

00:56:05,619 --> 00:56:17,079
there that's really a good point to

00:56:10,239 --> 00:56:18,459
start so that's all and I'm looking

00:56:17,079 --> 00:56:23,789
forward to answer some question now I

00:56:18,459 --> 00:56:56,109
hope we are in time yes that's good to

00:56:23,789 --> 00:57:01,719
any questions okay I must admit I feel

00:56:56,109 --> 00:57:04,779
sorry about okay so the question was if

00:57:01,719 --> 00:57:07,809
there is any nice tool to to measure

00:57:04,779 --> 00:57:10,630
fibre channel performance or but I'm

00:57:07,809 --> 00:57:21,670
sorry must admit that I did not get in

00:57:10,630 --> 00:57:23,799
touch with fibre channel yet so okay the

00:57:21,670 --> 00:57:25,869
ball does anyone has any experience with

00:57:23,799 --> 00:57:32,170
fibre channel performance measuring in

00:57:25,869 --> 00:57:34,869
here look it so maybe we can get into

00:57:32,170 --> 00:57:37,769
detail that later on babies I'm sorry

00:57:34,869 --> 00:57:37,769
yes okay

00:57:40,460 --> 00:58:12,480
yes very nice okay yes um I provided a

00:57:53,910 --> 00:58:14,700
slide about that but yes thanks for

00:58:12,480 --> 00:58:17,369
adding that yes that's very nice that's

00:58:14,700 --> 00:58:24,060
true that performance scheme is default

00:58:17,369 --> 00:58:25,859
on with 5.6 and the older use profiling

00:58:24,060 --> 00:58:27,900
commands also becoming deprecated with

00:58:25,859 --> 00:58:29,940
that version so my squirrel is really

00:58:27,900 --> 00:58:33,420
pushing forward to use the performance

00:58:29,940 --> 00:58:35,940
scheme and it is as you've said is a

00:58:33,420 --> 00:58:38,460
structured way also in SQL that you

00:58:35,940 --> 00:58:40,950
query tables about performance events

00:58:38,460 --> 00:58:42,660
running performance events and record it

00:58:40,950 --> 00:58:44,520
in your my sequel server so I think

00:58:42,660 --> 00:58:47,490
there are more than 50 tables in the

00:58:44,520 --> 00:59:06,050
performance scheme I don't there are

00:58:47,490 --> 00:59:06,050
many yes yes can add it's nice

00:59:14,300 --> 00:59:19,800
I've read a lot about the PS helper but

00:59:17,280 --> 00:59:22,770
I must admit the death not used it in

00:59:19,800 --> 00:59:24,900
productive environment but the tables

00:59:22,770 --> 00:59:27,780
and diffuse created by PS helper looked

00:59:24,900 --> 00:59:30,150
very promising so there are just one

00:59:27,780 --> 00:59:33,270
table from the PS helper you have schema

00:59:30,150 --> 00:59:35,880
table statistics and you get events like

00:59:33,270 --> 00:59:37,260
was fetched about a specific table so

00:59:35,880 --> 00:59:44,970
this is really nice really nice

00:59:37,260 --> 00:59:47,970
logistics about yeah and we have

00:59:44,970 --> 00:59:49,650
upgraded to 5.6 about two weeks ago but

00:59:47,970 --> 00:59:51,570
we have currently turned off the

00:59:49,650 --> 00:59:53,700
performance team because we had not time

00:59:51,570 --> 00:59:56,330
to take a closer look at it but we will

00:59:53,700 --> 01:00:02,960
do that I think in the next few months

00:59:56,330 --> 01:00:02,960
thanks about adding that not a question

01:00:12,530 --> 01:00:21,690
okay I'm as as far as as I demand you

01:00:17,730 --> 01:00:23,180
can I'm dares and okay sorry I'm just do

01:00:21,690 --> 01:00:27,930
it again so the question is about

01:00:23,180 --> 01:00:31,140
monitoring and and profiling routing and

01:00:27,930 --> 01:00:34,800
and bridging and so stuff in the colonel

01:00:31,140 --> 01:00:39,180
I think there are events from F trees

01:00:34,800 --> 01:00:42,780
you can you can check which which

01:00:39,180 --> 01:00:45,000
functions concerning the network the

01:00:42,780 --> 01:00:46,560
network stack in the in the colonel yeah

01:00:45,000 --> 01:00:49,170
I think there are some trace plans you

01:00:46,560 --> 01:00:56,850
can do it with a phrase as far as I have

01:00:49,170 --> 01:00:58,910
that in mind good many more just one

01:00:56,850 --> 01:00:58,910
more

01:01:45,280 --> 01:02:13,750
saket buffer overrun do you you talking

01:01:48,220 --> 01:02:15,040
about that um parameter okay okay thanks

01:02:13,750 --> 01:02:17,260
for nothing that i will take a closer

01:02:15,040 --> 01:02:22,500
look at that maybe include that in my in

01:02:17,260 --> 01:02:22,500
my slides thanks

01:02:27,100 --> 01:02:33,260
so no more questions then thanks for

01:02:31,190 --> 01:02:36,440
your attention and just one note we have

01:02:33,260 --> 01:02:38,660
a void code issue that almost went away

01:02:36,440 --> 01:02:40,910
so if you participate in any open source

01:02:38,660 --> 01:02:43,790
project then you can come to our booth

01:02:40,910 --> 01:02:47,390
and talk with my colleague stir about

01:02:43,790 --> 01:02:51,560
that almost winterboard you can win some

01:02:47,390 --> 01:02:53,600
I think you can win some money do too by

01:02:51,560 --> 01:02:56,120
Thomas quince over if you have an open

01:02:53,600 --> 01:02:59,660
source project that is interesting girls

01:02:56,120 --> 01:03:03,260
I think also parent is again in the in

01:02:59,660 --> 01:03:04,850
the jewelry this year again so if you

01:03:03,260 --> 01:03:06,560
participate an open source project and

01:03:04,850 --> 01:03:07,850
come to our booth and get more

01:03:06,560 --> 01:03:09,770
information about the thomas crown of

01:03:07,850 --> 01:03:11,390
work this year so thanks for your

01:03:09,770 --> 01:03:13,690
attention to and maybe see you later

01:03:11,390 --> 01:03:13,690

YouTube URL: https://www.youtube.com/watch?v=7GuFAs10vik


