Title: OSDC 2015: John Spray | The Ceph Storage System
Publication date: 2015-04-30
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	Ceph is an open source distributed object store and file system that provides excellent performance, reliability and scalability.
In this presentation, the Ceph architecture will be explained, attendees will be introduced to the block, object and file interfaces to Ceph.
Captions: 
	00:00:06,630 --> 00:00:10,800
welcome back I hope you

00:00:11,010 --> 00:00:15,980
now is talking trans play and he will

00:00:14,080 --> 00:00:21,510
talk

00:00:15,980 --> 00:00:26,730
thank you so today's talk is going to be

00:00:21,510 --> 00:00:30,090
a deep introduction to SEF followed by

00:00:26,730 --> 00:00:32,510
an update on the latest development in

00:00:30,090 --> 00:00:34,769
the the recently released hammer version

00:00:32,510 --> 00:00:37,230
so I'm going to talk about what's f is

00:00:34,769 --> 00:00:39,480
for those who don't know how it works

00:00:37,230 --> 00:00:41,760
how it stores your data to guarantee

00:00:39,480 --> 00:00:43,620
resilience and redundancy how your

00:00:41,760 --> 00:00:46,559
applications can talk to SEF and then

00:00:43,620 --> 00:00:50,370
finally what's new before we go any

00:00:46,559 --> 00:00:54,089
further quick show of hands who already

00:00:50,370 --> 00:00:57,780
knows what Seth is good most people who

00:00:54,089 --> 00:01:00,089
is already using Seth ok good number of

00:00:57,780 --> 00:01:01,589
people so this will be interesting for

00:01:00,089 --> 00:01:04,949
at least some of you but a few of you

00:01:01,589 --> 00:01:08,790
can correct me if I'm wrong so SEF is a

00:01:04,949 --> 00:01:12,240
highly available data store it's free

00:01:08,790 --> 00:01:15,900
software it's lgpl and it's all lgpl

00:01:12,240 --> 00:01:18,119
there are no proprietary add-ons it's a

00:01:15,900 --> 00:01:19,770
comparatively old project it's been

00:01:18,119 --> 00:01:23,430
going for 10 years it was originally

00:01:19,770 --> 00:01:25,350
started by sage while as a high

00:01:23,430 --> 00:01:26,700
performance computing file system but

00:01:25,350 --> 00:01:29,730
it's become a lot more versatile than

00:01:26,700 --> 00:01:32,640
that since then it's available for use

00:01:29,730 --> 00:01:35,280
as a block device as an s3 compatible

00:01:32,640 --> 00:01:37,470
objects tool or as a file system and the

00:01:35,280 --> 00:01:39,750
place that you see it most today is in

00:01:37,470 --> 00:01:41,760
private clouds where people either need

00:01:39,750 --> 00:01:45,330
a store for their virtual machine images

00:01:41,760 --> 00:01:50,100
or as3 compatible object store for their

00:01:45,330 --> 00:01:53,490
application workloads more generally SEF

00:01:50,100 --> 00:01:55,710
is what I think most people want in a

00:01:53,490 --> 00:01:57,960
storage system it's a piece of software

00:01:55,710 --> 00:02:00,960
where you feed disks in at one end and

00:01:57,960 --> 00:02:02,640
you get usable storage out the other end

00:02:00,960 --> 00:02:04,890
it makes sure you don't lose your data

00:02:02,640 --> 00:02:07,500
it doesn't require babysitting to do

00:02:04,890 --> 00:02:09,269
that and you can run it on more or less

00:02:07,500 --> 00:02:10,950
any Linux platform you want it's

00:02:09,269 --> 00:02:16,110
portable and occasionally it's been run

00:02:10,950 --> 00:02:19,110
on non Linux platforms as well object

00:02:16,110 --> 00:02:21,840
storage is provided by the radars

00:02:19,110 --> 00:02:24,060
gateway which is shortened rgw on the

00:02:21,840 --> 00:02:26,550
left-hand side of this slide so that's

00:02:24,060 --> 00:02:28,530
compatible with s3 that's how most

00:02:26,550 --> 00:02:30,150
people use it but you can also use

00:02:28,530 --> 00:02:33,410
compatible interface there if that's

00:02:30,150 --> 00:02:37,319
what your applications want it supports

00:02:33,410 --> 00:02:39,600
geo replication so you can have a master

00:02:37,319 --> 00:02:40,830
and slave data center or region to

00:02:39,600 --> 00:02:44,400
ensure you have multiple copies of your

00:02:40,830 --> 00:02:46,530
data as well the block storage or the

00:02:44,400 --> 00:02:48,450
virtual desk is provided by the Raiders

00:02:46,530 --> 00:02:51,420
block device or rbd that's in the middle

00:02:48,450 --> 00:02:53,010
and it's a fairly advanced network block

00:02:51,420 --> 00:02:55,290
device implementation that includes

00:02:53,010 --> 00:02:57,390
snapshots copy-on-write clones and that

00:02:55,290 --> 00:03:00,000
kind of thing and finally the file

00:02:57,390 --> 00:03:02,790
system on the right hand side is a POSIX

00:03:00,000 --> 00:03:05,940
compatible shared file system similar to

00:03:02,790 --> 00:03:10,019
an NFS filer but a lot more scalable

00:03:05,940 --> 00:03:13,590
than the average one so now I want to

00:03:10,019 --> 00:03:14,970
talk about how that all works so those

00:03:13,590 --> 00:03:16,650
are the three components again in a

00:03:14,970 --> 00:03:18,300
slightly different form and in this

00:03:16,650 --> 00:03:20,819
diagram there is the layer along the

00:03:18,300 --> 00:03:23,940
bottom as well which is ray das and

00:03:20,819 --> 00:03:26,160
that's the underlying object storage

00:03:23,940 --> 00:03:28,620
platform that supports all of the

00:03:26,160 --> 00:03:31,170
different application interfaces so

00:03:28,620 --> 00:03:33,150
whether you're using rgw rbd or the ceph

00:03:31,170 --> 00:03:36,690
file system your data is ultimately

00:03:33,150 --> 00:03:38,610
being stored by Ray dose rate or stands

00:03:36,690 --> 00:03:41,100
for reliable autonomous distributed

00:03:38,610 --> 00:03:42,390
object store it's reliable because it

00:03:41,100 --> 00:03:44,130
will keep more than one copy of your

00:03:42,390 --> 00:03:46,049
data and make sure that those copies are

00:03:44,130 --> 00:03:47,480
maintained in the case of failures its

00:03:46,049 --> 00:03:50,040
autonomous because it doesn't require

00:03:47,480 --> 00:03:52,019
administrator interaction to do that

00:03:50,040 --> 00:03:54,540
process it will notice failures and fix

00:03:52,019 --> 00:03:56,850
them itself it's distributed and that it

00:03:54,540 --> 00:03:58,799
scales linearly you can use it on

00:03:56,850 --> 00:04:01,230
anything from tends to tens of thousands

00:03:58,799 --> 00:04:06,000
of drives and of course it's an object

00:04:01,230 --> 00:04:07,859
store a Raiders cluster consists of two

00:04:06,000 --> 00:04:10,799
types of service usually running on

00:04:07,859 --> 00:04:14,250
separate servers Oh SDS which are object

00:04:10,799 --> 00:04:17,340
storage demons and monitors so OS DS are

00:04:14,250 --> 00:04:20,160
essentially your disks and their job is

00:04:17,340 --> 00:04:23,370
to serve data to clients to receive

00:04:20,160 --> 00:04:25,890
reads and to service rights as well they

00:04:23,370 --> 00:04:28,919
talk to each other as well as clients so

00:04:25,890 --> 00:04:31,050
all of that replication and self-healing

00:04:28,919 --> 00:04:35,160
happens on a peer-to-peer basis between

00:04:31,050 --> 00:04:37,380
OSD s monitors you usually have three of

00:04:35,160 --> 00:04:40,349
them perhaps five you need an odd number

00:04:37,380 --> 00:04:42,390
of them because they form a quorum using

00:04:40,349 --> 00:04:44,940
the pack sauce protocol

00:04:42,390 --> 00:04:47,280
and they maintain the most important

00:04:44,940 --> 00:04:49,950
state of your system they tell the

00:04:47,280 --> 00:04:51,570
system which OS dsr available which are

00:04:49,950 --> 00:04:56,790
up which are down and take care of

00:04:51,570 --> 00:05:01,380
managing that so object storage demons

00:04:56,790 --> 00:05:03,600
themselves are C++ applications that sit

00:05:01,380 --> 00:05:05,490
on top of a disk but between the OSD

00:05:03,600 --> 00:05:09,180
service and the disk is usually a local

00:05:05,490 --> 00:05:10,980
file system by default that's XFS it can

00:05:09,180 --> 00:05:13,380
also be something else you can use a

00:05:10,980 --> 00:05:14,820
file system of your choice butter FS is

00:05:13,380 --> 00:05:16,530
quite interesting for some people

00:05:14,820 --> 00:05:19,650
because of its copy-on-write snapshot

00:05:16,530 --> 00:05:21,390
capability although it has the stability

00:05:19,650 --> 00:05:23,160
issues that butter FS generally does so

00:05:21,390 --> 00:05:25,650
most people are using XFS in production

00:05:23,160 --> 00:05:27,960
in the most recent versions of SEF it's

00:05:25,650 --> 00:05:30,860
also possible to use experimental non

00:05:27,960 --> 00:05:33,540
file system back ends at that layer so

00:05:30,860 --> 00:05:36,300
you may have heard of leveldb or rocks

00:05:33,540 --> 00:05:38,430
TV some of these emerging key value

00:05:36,300 --> 00:05:40,890
stores get very good performance out of

00:05:38,430 --> 00:05:42,660
solid-state devices and so surf supports

00:05:40,890 --> 00:05:46,320
using those as well in an experimental

00:05:42,660 --> 00:05:47,910
capacity from the applications point of

00:05:46,320 --> 00:05:51,150
view it sees all of those Oh SDS and

00:05:47,910 --> 00:05:53,610
monitors in a big group and it addresses

00:05:51,150 --> 00:05:55,080
them collectively so applications don't

00:05:53,610 --> 00:05:56,850
think about I'm going to talk to this

00:05:55,080 --> 00:05:59,730
OSD or I'm going to talk to this monitor

00:05:56,850 --> 00:06:02,250
they address the cluster as a whole and

00:05:59,730 --> 00:06:05,480
all of the mechanisms for deciding which

00:06:02,250 --> 00:06:07,800
server to talk to are hidden inside seff

00:06:05,480 --> 00:06:10,710
applications initially only need to know

00:06:07,800 --> 00:06:12,300
the address of a monitor to learn about

00:06:10,710 --> 00:06:15,900
the addresses of all the rest of the

00:06:12,300 --> 00:06:17,370
cluster so within that cluster the most

00:06:15,900 --> 00:06:21,210
important decision we have to make is

00:06:17,370 --> 00:06:23,190
where to put the data there are a

00:06:21,210 --> 00:06:24,540
variety of ways of doing that the most

00:06:23,190 --> 00:06:26,790
traditional one is to have a metadata

00:06:24,540 --> 00:06:29,970
server that would tell you for a given

00:06:26,790 --> 00:06:32,520
file or s3 object or disk image what

00:06:29,970 --> 00:06:34,170
server it should be stored on the

00:06:32,520 --> 00:06:36,810
problem with having a metadata server is

00:06:34,170 --> 00:06:39,780
it quickly becomes a bottleneck if you

00:06:36,810 --> 00:06:43,590
have a very large number of servers and

00:06:39,780 --> 00:06:45,060
a single metadata server then your your

00:06:43,590 --> 00:06:48,270
linear scalability is limited by your

00:06:45,060 --> 00:06:49,860
metadata it also adds latency because

00:06:48,270 --> 00:06:52,950
you have to do two looks for everything

00:06:49,860 --> 00:06:54,420
that you want to find preferable to a

00:06:52,950 --> 00:06:56,009
metadata server is to do some kind of

00:06:54,420 --> 00:06:57,509
calculated placement so

00:06:56,009 --> 00:07:00,270
a simple example of this and this is

00:06:57,509 --> 00:07:02,460
what some systems do is to take the ID

00:07:00,270 --> 00:07:05,520
of a file or a task image or so on and

00:07:02,460 --> 00:07:07,289
hash it and look at the low few bits of

00:07:05,520 --> 00:07:10,710
the hash or if we imagine it as an

00:07:07,289 --> 00:07:12,899
alphabet the first letter and use that

00:07:10,710 --> 00:07:15,330
to decide where in a range of servers

00:07:12,899 --> 00:07:19,249
the data should be placed that works

00:07:15,330 --> 00:07:22,199
quite well for certain simple use cases

00:07:19,249 --> 00:07:24,240
where it doesn't work so well is when

00:07:22,199 --> 00:07:26,969
you need to pick multiple locations for

00:07:24,240 --> 00:07:28,620
data because you need a smart way of

00:07:26,969 --> 00:07:30,270
doing that you don't necessarily want to

00:07:28,620 --> 00:07:32,430
put things on two discs in the same

00:07:30,270 --> 00:07:33,599
server or conversely if you have a

00:07:32,430 --> 00:07:34,740
performance requirement you don't

00:07:33,599 --> 00:07:37,339
necessarily want to put them on opposite

00:07:34,740 --> 00:07:39,809
ends of the data center and it also

00:07:37,339 --> 00:07:41,819
doesn't necessarily cope well when you

00:07:39,809 --> 00:07:44,520
add and remove systems and you have to

00:07:41,819 --> 00:07:48,059
move these boundaries between areas of

00:07:44,520 --> 00:07:52,469
the hash space so in sev there's an

00:07:48,059 --> 00:07:54,419
algorithm called crush and this is the

00:07:52,469 --> 00:07:57,659
algorithm which allows us to quickly

00:07:54,419 --> 00:07:59,159
decide for a given piece of data where

00:07:57,659 --> 00:08:01,699
it should live and more importantly on

00:07:59,159 --> 00:08:03,839
which multiple servers should it live

00:08:01,699 --> 00:08:06,599
the qualities that an algorithm should

00:08:03,839 --> 00:08:09,779
have to perform this purpose are that it

00:08:06,599 --> 00:08:11,039
should have a repeatable distribution we

00:08:09,779 --> 00:08:12,509
need to make sure that when we run the

00:08:11,039 --> 00:08:14,849
calculation again we'll find at the same

00:08:12,509 --> 00:08:17,759
place we put it also to have a stable

00:08:14,849 --> 00:08:19,139
mapping so if we add or remove drives we

00:08:17,759 --> 00:08:20,939
won't shuffle a lot of data back and

00:08:19,139 --> 00:08:23,309
forth will move more or less the minimal

00:08:20,939 --> 00:08:25,830
amount of data we have to to adapt to

00:08:23,309 --> 00:08:28,589
drives coming and going and it should

00:08:25,830 --> 00:08:30,389
also be topology aware so the placement

00:08:28,589 --> 00:08:32,940
algorithm needs to be making intelligent

00:08:30,389 --> 00:08:35,789
decisions about whether to put copies of

00:08:32,940 --> 00:08:38,849
data in different racks in different

00:08:35,789 --> 00:08:42,510
hosts rather than just putting them on

00:08:38,849 --> 00:08:45,329
different disks so visually this is the

00:08:42,510 --> 00:08:47,959
job that's being done by crush some data

00:08:45,329 --> 00:08:50,730
comes in it's split up into multiple

00:08:47,959 --> 00:08:52,290
objects at the radar slayer so these

00:08:50,730 --> 00:08:54,930
become Rados objects and the different

00:08:52,290 --> 00:08:57,060
colored blocks on the right hand side

00:08:54,930 --> 00:08:59,610
you can see the different OS DS the

00:08:57,060 --> 00:09:03,319
squares and each block in this example

00:08:59,610 --> 00:09:03,319
is going to two different OS DS

00:09:03,889 --> 00:09:09,360
importantly we do it in a topology aware

00:09:06,720 --> 00:09:11,220
way so the in

00:09:09,360 --> 00:09:12,930
to crush isn't just the idea of an

00:09:11,220 --> 00:09:16,050
object it's something called the crush

00:09:12,930 --> 00:09:19,950
map which is a structure that SEF users

00:09:16,050 --> 00:09:22,500
can define to say this drive is in this

00:09:19,950 --> 00:09:25,380
host is in this rack is in this chassis

00:09:22,500 --> 00:09:27,800
is in this data center and so on my

00:09:25,380 --> 00:09:31,589
default we create three copies of data

00:09:27,800 --> 00:09:33,870
and we put each copy on a different host

00:09:31,589 --> 00:09:35,899
and we detect which drives around which

00:09:33,870 --> 00:09:38,459
hosts by just looking at the host names

00:09:35,899 --> 00:09:40,200
so it's a quick calculation we don't

00:09:38,459 --> 00:09:42,630
have to do it we don't want to be

00:09:40,200 --> 00:09:46,230
wasting a lot of time recalculating this

00:09:42,630 --> 00:09:48,690
and when you want to customize it there

00:09:46,230 --> 00:09:50,690
is a simple language that you can use to

00:09:48,690 --> 00:09:53,760
specify what we call a crush rule and

00:09:50,690 --> 00:09:55,950
this is this is the entire syntax of the

00:09:53,760 --> 00:09:58,200
language and it essentially lets you

00:09:55,950 --> 00:10:02,040
specify a series of steps along the

00:09:58,200 --> 00:10:04,500
lines of pick one rack okay now pick two

00:10:02,040 --> 00:10:07,950
drives okay now pick another rack now

00:10:04,500 --> 00:10:10,130
pick one drive and so on as well as i'm

00:10:07,950 --> 00:10:12,120
specifying where in the topology of your

00:10:10,130 --> 00:10:14,130
servers and racks and so on you put

00:10:12,120 --> 00:10:16,949
things you can also choose to define a

00:10:14,130 --> 00:10:18,959
crush map that puts things like SSDs and

00:10:16,949 --> 00:10:20,250
hard drives or higher performance and

00:10:18,959 --> 00:10:21,660
lower performance hard drives into

00:10:20,250 --> 00:10:24,240
different regions of the map and you

00:10:21,660 --> 00:10:26,820
could do interesting things like put the

00:10:24,240 --> 00:10:28,829
primary copy of a data on an SSD and

00:10:26,820 --> 00:10:30,870
also put a copy on a spinning disk

00:10:28,829 --> 00:10:34,410
somewhere else that's a fairly contrived

00:10:30,870 --> 00:10:37,820
example but it's possible and finally

00:10:34,410 --> 00:10:41,850
just to complete your terminology of SEF

00:10:37,820 --> 00:10:44,640
in the previous diagram those little

00:10:41,850 --> 00:10:47,100
colored blocks of objects we don't

00:10:44,640 --> 00:10:50,250
actually decide on an object by object

00:10:47,100 --> 00:10:52,290
basis where to put things we could it

00:10:50,250 --> 00:10:55,470
would be scalable but it would not be

00:10:52,290 --> 00:10:57,360
efficient so what we do is we divide up

00:10:55,470 --> 00:10:59,910
the overall object space into so-called

00:10:57,360 --> 00:11:02,420
placement groups and we apply the crush

00:10:59,910 --> 00:11:06,269
algorithm to each placement group and

00:11:02,420 --> 00:11:08,820
typically a system will have a couple of

00:11:06,269 --> 00:11:12,029
hundred of these per OSD so they're

00:11:08,820 --> 00:11:14,579
fairly numerous things and those are the

00:11:12,029 --> 00:11:17,490
placement groups are the granularity at

00:11:14,579 --> 00:11:19,440
which we do data management so if we

00:11:17,490 --> 00:11:21,990
have lost some data and we need to re

00:11:19,440 --> 00:11:23,130
replicate it rather than having to worry

00:11:21,990 --> 00:11:24,900
about finding

00:11:23,130 --> 00:11:27,180
a million objects we know which

00:11:24,900 --> 00:11:32,040
placement group was lost and we re

00:11:27,180 --> 00:11:35,820
replicate that and placement groups form

00:11:32,040 --> 00:11:39,150
a pool and so pools are just a logical

00:11:35,820 --> 00:11:40,680
data management concept within safe so

00:11:39,150 --> 00:11:43,350
within your safe cluster you say I would

00:11:40,680 --> 00:11:45,330
like a pool the pool will contain some

00:11:43,350 --> 00:11:49,140
thousands of placement groups which

00:11:45,330 --> 00:11:51,420
again simply virtual system a generic

00:11:49,140 --> 00:11:54,000
things and then objects are assigned by

00:11:51,420 --> 00:11:55,610
their ID hash into a placement group and

00:11:54,000 --> 00:11:57,990
when you want to write the object you

00:11:55,610 --> 00:11:59,310
use the crush algorithm to work out

00:11:57,990 --> 00:12:01,250
where the placement group lives and

00:11:59,310 --> 00:12:03,270
that's where you'll find the object

00:12:01,250 --> 00:12:04,500
don't worry if you didn't completely

00:12:03,270 --> 00:12:06,390
follow that or if I didn't explain it

00:12:04,500 --> 00:12:08,100
very well it's not essential to the

00:12:06,390 --> 00:12:10,260
understanding of the system it just

00:12:08,100 --> 00:12:11,310
comes up subsequently if you're looking

00:12:10,260 --> 00:12:12,870
at surf and you think what are these

00:12:11,310 --> 00:12:17,550
placement group things that's where they

00:12:12,870 --> 00:12:20,480
are the way that resilience is

00:12:17,550 --> 00:12:23,790
implemented in CF is initially a

00:12:20,480 --> 00:12:25,770
peer-to-peer heartbeat between the OS DS

00:12:23,790 --> 00:12:27,780
in the system so they will constantly be

00:12:25,770 --> 00:12:30,240
checking if their neighbors are still

00:12:27,780 --> 00:12:32,280
alive or not and if they're not they'll

00:12:30,240 --> 00:12:35,670
send a message to a monitor server to

00:12:32,280 --> 00:12:38,370
say I believe my neighbor is dead the

00:12:35,670 --> 00:12:40,740
monitor server will require several of

00:12:38,370 --> 00:12:42,360
these messages so if one guy is falsely

00:12:40,740 --> 00:12:43,680
claiming that all his neighbors are dead

00:12:42,360 --> 00:12:46,200
and his neighbors are saying no I'm

00:12:43,680 --> 00:12:48,360
still alive and the monitors will cope

00:12:46,200 --> 00:12:50,700
with that and and realize that his

00:12:48,360 --> 00:12:52,440
neighbors are not dead but after maybe

00:12:50,700 --> 00:12:54,390
three or four different OS DS have all

00:12:52,440 --> 00:12:57,270
said I think this particular guy is dead

00:12:54,390 --> 00:13:00,000
the monitors will decide that he is

00:12:57,270 --> 00:13:02,010
indeed dead and at that point they

00:13:00,000 --> 00:13:04,680
update their state to indicate that he

00:13:02,010 --> 00:13:06,510
is down which means that the OSD

00:13:04,680 --> 00:13:08,820
probably still exists he's not available

00:13:06,510 --> 00:13:10,770
right now and at that point client

00:13:08,820 --> 00:13:13,110
workload will start going to the other

00:13:10,770 --> 00:13:15,000
replicas of that data so if you had

00:13:13,110 --> 00:13:19,230
three copies of the data and you lose

00:13:15,000 --> 00:13:21,060
one or one goes down then the other two

00:13:19,230 --> 00:13:23,610
will continue providing normal

00:13:21,060 --> 00:13:25,770
operations in the default settings if

00:13:23,610 --> 00:13:28,320
you then lose a second copy then it will

00:13:25,770 --> 00:13:30,180
go into a read-only mode because if you

00:13:28,320 --> 00:13:32,160
did any rights then we wouldn't be able

00:13:30,180 --> 00:13:35,190
to make sure that we'd reliably persist

00:13:32,160 --> 00:13:36,510
at those rights those thresholds are

00:13:35,190 --> 00:13:36,990
configurable you can have as many copies

00:13:36,510 --> 00:13:38,279
as you

00:13:36,990 --> 00:13:39,899
monton you can change the rules about

00:13:38,279 --> 00:13:43,440
how many copies have to be available to

00:13:39,899 --> 00:13:45,480
service rate after some period of time

00:13:43,440 --> 00:13:46,890
the default is five minutes but you will

00:13:45,480 --> 00:13:48,930
want to configure this depending on the

00:13:46,890 --> 00:13:52,529
size and nature of your system the

00:13:48,930 --> 00:13:54,630
monitors mark the OSD out this

00:13:52,529 --> 00:13:57,779
terminology gets confusing the up/down

00:13:54,630 --> 00:13:59,310
in/out concept but the distinction is

00:13:57,779 --> 00:14:01,529
that when something's down it's simply

00:13:59,310 --> 00:14:03,690
unavailable to serve data whereas when

00:14:01,529 --> 00:14:06,450
it's out we need to start making new

00:14:03,690 --> 00:14:07,880
copies of that data or elsewhere and by

00:14:06,450 --> 00:14:10,740
default that happens after five minutes

00:14:07,880 --> 00:14:12,330
the most exciting thing about this

00:14:10,740 --> 00:14:14,130
process other than all the housekeeping

00:14:12,330 --> 00:14:17,130
stuff is that when we start making those

00:14:14,130 --> 00:14:20,310
new copies it happens on an altar all

00:14:17,130 --> 00:14:23,430
basis across the cluster if I go back to

00:14:20,310 --> 00:14:25,980
the diagram with the colors on it you'll

00:14:23,430 --> 00:14:28,170
notice that the the red block in the top

00:14:25,980 --> 00:14:30,149
left is next to a blue block but the red

00:14:28,170 --> 00:14:33,270
block down here is next to a green block

00:14:30,149 --> 00:14:35,220
so that's what in storage jog and you

00:14:33,270 --> 00:14:38,610
call that D clustered placement of data

00:14:35,220 --> 00:14:40,470
and that means that when we later want

00:14:38,610 --> 00:14:43,709
to rebuild all the data that used to be

00:14:40,470 --> 00:14:45,870
on an OSD which is now dead it's that

00:14:43,709 --> 00:14:49,529
data that we want is all over the

00:14:45,870 --> 00:14:52,079
cluster and we're not having to stream

00:14:49,529 --> 00:14:53,250
the data from a particular hard drive so

00:14:52,079 --> 00:14:55,800
we get much much better recovery

00:14:53,250 --> 00:14:58,079
bandwidth than in a trade system if you

00:14:55,800 --> 00:14:59,670
have four terabyte drives in a raid 6

00:14:58,079 --> 00:15:01,500
array I'm sure a lot of you have had the

00:14:59,670 --> 00:15:03,540
experience of having a drive fail and

00:15:01,500 --> 00:15:05,459
it's got an a nail-biting experience you

00:15:03,540 --> 00:15:07,260
know waiting for a day or two for it to

00:15:05,459 --> 00:15:11,040
rebuild when you're worried about your

00:15:07,260 --> 00:15:13,890
degraded system so beyond simple puts

00:15:11,040 --> 00:15:14,820
and gets of objects raid us can do a lot

00:15:13,890 --> 00:15:17,399
more than you would expect from a

00:15:14,820 --> 00:15:20,940
typical object store we can partially

00:15:17,399 --> 00:15:23,149
update objects we can store key value

00:15:20,940 --> 00:15:26,970
dictionaries within a particular object

00:15:23,149 --> 00:15:29,220
we can do snapshots of the objects and

00:15:26,970 --> 00:15:30,720
we can also use it for messaging so we

00:15:29,220 --> 00:15:33,300
have a mechanism called watch notify

00:15:30,720 --> 00:15:36,209
that allows you to create an object and

00:15:33,300 --> 00:15:38,279
have some clients subscribe to the

00:15:36,209 --> 00:15:41,180
object and subsequently have somebody

00:15:38,279 --> 00:15:43,079
else send notifications via that object

00:15:41,180 --> 00:15:44,310
that's the kind of thing that would be

00:15:43,079 --> 00:15:45,870
very interesting if you had an

00:15:44,310 --> 00:15:47,459
application built on liberate us and

00:15:45,870 --> 00:15:50,310
that needed some sort of queuing or

00:15:47,459 --> 00:15:50,790
notification we use it internally for

00:15:50,310 --> 00:15:53,130
the house

00:15:50,790 --> 00:15:56,100
keeping in some of the other layers that

00:15:53,130 --> 00:15:58,890
we have on top of liberate us and you

00:15:56,100 --> 00:16:02,100
can also customize it so in addition to

00:15:58,890 --> 00:16:04,530
the right read and so on transactions

00:16:02,100 --> 00:16:06,210
the radar supports by default you can

00:16:04,530 --> 00:16:08,910
add code in what's called ray das

00:16:06,210 --> 00:16:10,890
classes load it into the OSD and then

00:16:08,910 --> 00:16:12,780
call it remotely so if you have some

00:16:10,890 --> 00:16:14,730
interesting manipulation that is

00:16:12,780 --> 00:16:17,310
specific to your application maybe you

00:16:14,730 --> 00:16:19,560
want to compress and decompress objects

00:16:17,310 --> 00:16:21,540
maybe you want to remotely grep an

00:16:19,560 --> 00:16:26,100
object on an OSD you could implement

00:16:21,540 --> 00:16:27,900
that as a ratos object class just a

00:16:26,100 --> 00:16:29,940
quick note about choosing hardware I

00:16:27,900 --> 00:16:32,610
work for a software company so we don't

00:16:29,940 --> 00:16:35,700
sell hardware so we tend to give fairly

00:16:32,610 --> 00:16:38,610
loose recommendations but the key idea

00:16:35,700 --> 00:16:40,560
is that the cost of having these

00:16:38,610 --> 00:16:42,600
multiple replicas of data should be

00:16:40,560 --> 00:16:45,330
mitigated by using comparatively cheap

00:16:42,600 --> 00:16:48,390
hardware so don't go out and buy a raid

00:16:45,330 --> 00:16:50,280
controller don't go out and buy a treat

00:16:48,390 --> 00:16:52,320
a big traditional storage chassis you

00:16:50,280 --> 00:16:53,790
want commodity servers with cheap hard

00:16:52,320 --> 00:16:58,410
drives in a lot of people even just use

00:16:53,790 --> 00:17:00,750
Saturday is and you can get a lot of the

00:16:58,410 --> 00:17:03,870
performance back that you might lose by

00:17:00,750 --> 00:17:07,770
using slower drives by using an SSD

00:17:03,870 --> 00:17:10,290
specifically for the journal on our OS

00:17:07,770 --> 00:17:12,270
DS so osts do data journaling which

00:17:10,290 --> 00:17:14,460
means by default if you have an x FS

00:17:12,270 --> 00:17:16,260
hard drive acting's on OSD every piece

00:17:14,460 --> 00:17:18,390
of data that you right to it will go on

00:17:16,260 --> 00:17:20,670
there twice once on the journal and once

00:17:18,390 --> 00:17:22,020
on the desk clearly that's undesirable

00:17:20,670 --> 00:17:24,300
to double your bet you're right

00:17:22,020 --> 00:17:25,920
bandwidth consumption like that and the

00:17:24,300 --> 00:17:29,670
way that most people build safe clusters

00:17:25,920 --> 00:17:31,890
is to have an SSD that takes that first

00:17:29,670 --> 00:17:33,270
right with very low latency makes it

00:17:31,890 --> 00:17:34,950
persistent and allows the rest of the

00:17:33,270 --> 00:17:37,560
system to continue gets you a good

00:17:34,950 --> 00:17:39,840
latency number and then subsequently

00:17:37,560 --> 00:17:42,870
flush from that journal on to the

00:17:39,840 --> 00:17:44,550
spinning disk and the typical ratio that

00:17:42,870 --> 00:17:47,130
you use when building a system is to

00:17:44,550 --> 00:17:49,020
have an SSD with for hard drives

00:17:47,130 --> 00:17:51,090
something like that but it's very

00:17:49,020 --> 00:17:52,470
approximate and it depends on what hard

00:17:51,090 --> 00:17:54,420
drives you are using what SSDs you're

00:17:52,470 --> 00:17:55,950
using and so on that SSD is not

00:17:54,420 --> 00:17:57,630
mandatory but it's the typical way

00:17:55,950 --> 00:18:00,270
people deploy seff if they care about

00:17:57,630 --> 00:18:03,300
latency at all make sure you have enough

00:18:00,270 --> 00:18:04,080
cpu and ram SEF will use a lot more cpu

00:18:03,300 --> 00:18:05,519
and ram the

00:18:04,080 --> 00:18:08,220
traditional for example filer

00:18:05,519 --> 00:18:11,610
application because when it's doing

00:18:08,220 --> 00:18:13,350
things like re replicating data there's

00:18:11,610 --> 00:18:16,320
there's just a lot more going on and

00:18:13,350 --> 00:18:17,940
I'll come on later to the erasure coding

00:18:16,320 --> 00:18:21,510
feature of surf and that requires a lot

00:18:17,940 --> 00:18:23,909
more CPU as well and finally think about

00:18:21,510 --> 00:18:26,220
the ratios in your system think about

00:18:23,909 --> 00:18:29,100
the ratio of reads to rights think about

00:18:26,220 --> 00:18:33,000
the ratio of storage capacity to

00:18:29,100 --> 00:18:35,610
bandwidth accessing that storage so do

00:18:33,000 --> 00:18:39,450
you need more smaller drives or fewer

00:18:35,610 --> 00:18:41,309
bigger drives do you need a relatively

00:18:39,450 --> 00:18:43,740
little disk bandwidth and a lot of RAM

00:18:41,309 --> 00:18:45,029
for caching reads or are you doing

00:18:43,740 --> 00:18:46,950
streaming rights and you don't care

00:18:45,029 --> 00:18:49,110
about reads at all and that's just

00:18:46,950 --> 00:18:55,080
different for every system so take that

00:18:49,110 --> 00:18:58,139
into account so I mentioned the three

00:18:55,080 --> 00:18:59,480
different ways of accessing SF I'll talk

00:18:58,139 --> 00:19:02,070
about those a little bit more detail now

00:18:59,480 --> 00:19:05,190
LBD the Raiders block device gives you a

00:19:02,070 --> 00:19:07,380
virtual disk this is essentially a

00:19:05,190 --> 00:19:09,299
software Sam we don't generally call it

00:19:07,380 --> 00:19:12,139
that but it's that's where it would fit

00:19:09,299 --> 00:19:16,230
into the traditional storage landscape

00:19:12,139 --> 00:19:19,230
so in our BD we take these disk images

00:19:16,230 --> 00:19:22,080
that we expose to clients which might

00:19:19,230 --> 00:19:25,230
appear as / dev / RVD whatever it looks

00:19:22,080 --> 00:19:30,269
just like a drive but under the hood

00:19:25,230 --> 00:19:32,490
each byte extent within that image

00:19:30,269 --> 00:19:34,799
within some configurable chunking by

00:19:32,490 --> 00:19:36,929
default 4 megabytes is a separate Radel

00:19:34,799 --> 00:19:38,909
subject and that's going to get written

00:19:36,929 --> 00:19:41,100
to a different OSD now there in the

00:19:38,909 --> 00:19:43,380
radar system what that means is you can

00:19:41,100 --> 00:19:45,929
get really phenomenally good bandwidth

00:19:43,380 --> 00:19:47,130
if you're sending a lot of rights to one

00:19:45,929 --> 00:19:49,019
of these images and you've got a large

00:19:47,130 --> 00:19:51,330
number of 0 SDS then you're really

00:19:49,019 --> 00:19:55,289
limited only by the network bandwidth

00:19:51,330 --> 00:19:57,510
from the client so it's pretty fast it's

00:19:55,289 --> 00:20:00,690
fairly feature-rich snapshots are very

00:19:57,510 --> 00:20:04,470
useful snapshots themselves relatively

00:20:00,690 --> 00:20:06,179
obvious feature but on top of snapshots

00:20:04,470 --> 00:20:09,000
we have an import export feature which

00:20:06,179 --> 00:20:10,409
lets you take diffs of a disk image

00:20:09,000 --> 00:20:13,309
that's very useful if you have multiple

00:20:10,409 --> 00:20:15,870
data centers and you want to keep a

00:20:13,309 --> 00:20:17,320
asynchronous backup of some of your

00:20:15,870 --> 00:20:20,110
images you can use snapshot

00:20:17,320 --> 00:20:22,960
and on the diffs I it's called RVD

00:20:20,110 --> 00:20:24,610
important rbd export to implement that

00:20:22,960 --> 00:20:28,929
and a lot of people have very simple

00:20:24,610 --> 00:20:30,580
scripts that do that it's also a more or

00:20:28,929 --> 00:20:32,470
less a default part of typical

00:20:30,580 --> 00:20:34,299
open-source virtualization stacks so

00:20:32,470 --> 00:20:35,710
it's an OpenStack it's in CloudStack

00:20:34,299 --> 00:20:38,259
it's an open that beurettes and proxmox

00:20:35,710 --> 00:20:39,610
the chances are whoever you're using for

00:20:38,259 --> 00:20:43,779
your open source virtualization has

00:20:39,610 --> 00:20:45,549
already heard about bt and that's just a

00:20:43,779 --> 00:20:47,200
visual representation of that so the

00:20:45,549 --> 00:20:49,480
virtual machine is sitting on top of a

00:20:47,200 --> 00:20:51,730
disk is sitting on top of while is being

00:20:49,480 --> 00:20:54,009
managed by a hypervisor and then that

00:20:51,730 --> 00:20:57,370
write or read workload is getting spread

00:20:54,009 --> 00:21:01,179
out across the radar cluster next up

00:20:57,370 --> 00:21:02,590
radars gateway so this is from an

00:21:01,179 --> 00:21:05,049
applications point of view the the

00:21:02,590 --> 00:21:08,340
simplest one it's providing a relatively

00:21:05,049 --> 00:21:11,620
straightforward s very compatible API

00:21:08,340 --> 00:21:13,179
until recently it was most of the s3 API

00:21:11,620 --> 00:21:16,570
and we're more complete than we used to

00:21:13,179 --> 00:21:19,659
be we have versioning now and it has all

00:21:16,570 --> 00:21:21,490
of the accounting type management and

00:21:19,659 --> 00:21:22,600
the user management permissions and so

00:21:21,490 --> 00:21:24,610
on that you need for a system like that

00:21:22,600 --> 00:21:26,799
and the way that Raiders gateway works

00:21:24,610 --> 00:21:29,529
is to have a series of web servers

00:21:26,799 --> 00:21:31,330
sitting in front of your radars cluster

00:21:29,529 --> 00:21:34,240
so you can have as many of these as you

00:21:31,330 --> 00:21:35,769
want and they coordinate with each other

00:21:34,240 --> 00:21:37,509
to make sure that they're not treading

00:21:35,769 --> 00:21:39,490
on the same objects at the same time and

00:21:37,509 --> 00:21:41,590
they do that using the aforementioned

00:21:39,490 --> 00:21:44,190
radar swatch notify mechanism so they do

00:21:41,590 --> 00:21:47,440
it's Caleb Lee and so you have a

00:21:44,190 --> 00:21:49,870
separate cluster of radars gateway web

00:21:47,440 --> 00:21:51,639
servers acting as a front end to your

00:21:49,870 --> 00:21:53,559
underlying ray das cluster which is

00:21:51,639 --> 00:21:55,720
doing all the storage and those web

00:21:53,559 --> 00:21:57,519
servers themselves aren't storing

00:21:55,720 --> 00:21:59,950
anything there more or less stateless

00:21:57,519 --> 00:22:01,000
and so they can come and go you don't

00:21:59,950 --> 00:22:04,809
have to worry too much about the

00:22:01,000 --> 00:22:08,860
reliability of those finally set of s

00:22:04,809 --> 00:22:13,389
the file system right so Seth FS you can

00:22:08,860 --> 00:22:15,429
mount it on a client machine the client

00:22:13,389 --> 00:22:17,649
software force ffs is actually part of

00:22:15,429 --> 00:22:20,710
the mainline linux kernel so the chances

00:22:17,649 --> 00:22:23,620
are you already have this ffs client and

00:22:20,710 --> 00:22:25,269
once you've mounted it in / month / f or

00:22:23,620 --> 00:22:27,070
wherever it looks just like a normal

00:22:25,269 --> 00:22:28,690
file system and it has the same POSIX

00:22:27,070 --> 00:22:30,850
semantics that a local file system would

00:22:28,690 --> 00:22:33,070
have the trick is

00:22:30,850 --> 00:22:34,600
it sends its data directly to raid us so

00:22:33,070 --> 00:22:36,460
in the same way that an rbd image can

00:22:34,600 --> 00:22:38,260
stripe its data across all of those Oh

00:22:36,460 --> 00:22:42,130
SDS and get very good bandwidth you get

00:22:38,260 --> 00:22:43,990
the same capability with CFS in order to

00:22:42,130 --> 00:22:46,060
provide POSIX semantics we introduced an

00:22:43,990 --> 00:22:50,110
additional service the metadata server

00:22:46,060 --> 00:22:52,480
or MDS but it's important to just

00:22:50,110 --> 00:22:54,370
clarify the data does not all flow via

00:22:52,480 --> 00:22:56,740
that and the object lookups don't flow

00:22:54,370 --> 00:23:00,190
via that the metadata server is purely

00:22:56,740 --> 00:23:01,960
for the POSIX file system metadata so

00:23:00,190 --> 00:23:04,570
for example the inode the directory

00:23:01,960 --> 00:23:06,460
entry the directory and so on once

00:23:04,570 --> 00:23:08,260
you've got a file open all of your

00:23:06,460 --> 00:23:10,780
rights from the client go straight to

00:23:08,260 --> 00:23:13,120
the underlying radar Slayer and you get

00:23:10,780 --> 00:23:15,430
the same fabulous bandwidth and so on

00:23:13,120 --> 00:23:17,920
and that's what that looks like the

00:23:15,430 --> 00:23:19,630
clients are are talking to metadata

00:23:17,920 --> 00:23:21,550
servers and data servers and in that

00:23:19,630 --> 00:23:23,650
cluster at the bottom there I've in

00:23:21,550 --> 00:23:25,870
addition to the MS and the OSD suc the

00:23:23,650 --> 00:23:28,030
MDS is in there as well they're managed

00:23:25,870 --> 00:23:29,860
by the monitor service as well so in the

00:23:28,030 --> 00:23:31,840
same way that the monitors keep track of

00:23:29,860 --> 00:23:35,290
which OS these are up and down they do

00:23:31,840 --> 00:23:37,960
that for the MD asses as well surfer

00:23:35,290 --> 00:23:40,480
thessaly goes a little bit beyond what a

00:23:37,960 --> 00:23:43,480
typical POSIX file system can do so we

00:23:40,480 --> 00:23:46,420
have subdirectory snapshots you can take

00:23:43,480 --> 00:23:49,180
a snapshot of a particular directory you

00:23:46,420 --> 00:23:51,460
get recursive statistics so if you want

00:23:49,180 --> 00:23:54,160
to know how much data is in a particular

00:23:51,460 --> 00:23:55,600
folder by default if you use D you on a

00:23:54,160 --> 00:23:56,890
Linux system it's actually going down

00:23:55,600 --> 00:23:59,410
into all of the files in there and

00:23:56,890 --> 00:24:02,680
touching all of them to calculate that

00:23:59,410 --> 00:24:04,780
in cephus we keep that data to hand so

00:24:02,680 --> 00:24:06,970
if you have the right user space tools

00:24:04,780 --> 00:24:09,190
it will be able to look that data up

00:24:06,970 --> 00:24:11,140
directly and that's especially important

00:24:09,190 --> 00:24:12,970
in a distributed file system because if

00:24:11,140 --> 00:24:16,000
you've ever used something like lustre

00:24:12,970 --> 00:24:18,040
with a big directory and you type LS

00:24:16,000 --> 00:24:20,020
dash l quite often you'll find it

00:24:18,040 --> 00:24:21,250
actually takes a while because it has to

00:24:20,020 --> 00:24:22,810
go and talk to a lot of different places

00:24:21,250 --> 00:24:24,550
in the system to get all that metadata

00:24:22,810 --> 00:24:27,730
together so recursive statistics can

00:24:24,550 --> 00:24:29,020
really speed that up and this is not so

00:24:27,730 --> 00:24:30,280
much a pose X file system feature but

00:24:29,020 --> 00:24:31,750
it's something that's quite important to

00:24:30,280 --> 00:24:33,730
mention and you can have multiple

00:24:31,750 --> 00:24:36,130
metadata servers so we get away from

00:24:33,730 --> 00:24:43,380
that bottle necking issue that some

00:24:36,130 --> 00:24:45,049
systems have CFS is not part of the

00:24:43,380 --> 00:24:47,659
officially

00:24:45,049 --> 00:24:49,489
products that most of the enterprise

00:24:47,659 --> 00:24:51,499
vendors providing safe distributions are

00:24:49,489 --> 00:24:53,779
providing at the moment the reason for

00:24:51,499 --> 00:24:57,440
that is that the recovery tools aren't

00:24:53,779 --> 00:24:58,999
done yet so the file system check and

00:24:57,440 --> 00:25:01,340
the ability to repair systems after

00:24:58,999 --> 00:25:03,980
something's gone wrong on there yet so

00:25:01,340 --> 00:25:06,200
the system the file system works but on

00:25:03,980 --> 00:25:07,669
the off chance that you you hit a bug or

00:25:06,200 --> 00:25:10,190
you have a data corruption issue or

00:25:07,669 --> 00:25:11,779
something like that you have quite a

00:25:10,190 --> 00:25:14,960
difficult time at the moment so that's

00:25:11,779 --> 00:25:17,090
the priority that we're work those of us

00:25:14,960 --> 00:25:19,009
working on surf of s myself included are

00:25:17,090 --> 00:25:21,559
working on getting those recovery tools

00:25:19,009 --> 00:25:23,769
and those consistency checks online so

00:25:21,559 --> 00:25:27,230
that people can make use of CFS for more

00:25:23,769 --> 00:25:30,470
production sensitive workloads and all

00:25:27,230 --> 00:25:32,119
of that complexity aside if you already

00:25:30,470 --> 00:25:33,649
have a radio slusser and you're using

00:25:32,119 --> 00:25:36,259
our SEF deploy tool to set up your

00:25:33,649 --> 00:25:39,350
system setting up CFS is actually

00:25:36,259 --> 00:25:41,539
incredibly simple you create an MDS

00:25:39,350 --> 00:25:43,309
using safe deployed you create a couple

00:25:41,539 --> 00:25:45,440
of radio spools one for your metadata

00:25:43,309 --> 00:25:47,840
room for your data and then you also get

00:25:45,440 --> 00:25:49,879
to create you a new FS and finally you

00:25:47,840 --> 00:25:51,379
mount it on a client and like I said the

00:25:49,879 --> 00:25:55,989
chances are your client machine already

00:25:51,379 --> 00:25:58,789
has the software on it so I've covered a

00:25:55,989 --> 00:26:00,139
ratos the underlying mechanism that we

00:25:58,789 --> 00:26:02,409
use to provide the storage and the

00:26:00,139 --> 00:26:04,970
resilience I've covered the different

00:26:02,409 --> 00:26:07,850
interfaces that you have two radars for

00:26:04,970 --> 00:26:10,159
your applications I'm going to go on and

00:26:07,850 --> 00:26:12,320
talk about some other slightly more

00:26:10,159 --> 00:26:13,519
advanced topics but before I do I just

00:26:12,320 --> 00:26:14,720
wanted to ask you for any questions

00:26:13,519 --> 00:26:20,799
about what I've covered so far about

00:26:14,720 --> 00:26:23,799
terminology and that kind of thing okay

00:26:20,799 --> 00:26:23,799
yes

00:26:26,500 --> 00:26:30,610
so the question is what about access

00:26:28,390 --> 00:26:32,830
control list since ffs yeah that's an

00:26:30,610 --> 00:26:35,320
area that needs some work so different

00:26:32,830 --> 00:26:38,290
systems different platforms whether

00:26:35,320 --> 00:26:40,450
you're using NFS or samer and so on have

00:26:38,290 --> 00:26:42,430
different requirements in that area and

00:26:40,450 --> 00:26:44,410
a related thing is support for selinux

00:26:42,430 --> 00:26:47,470
and that kind of thing so we have

00:26:44,410 --> 00:26:49,840
extended attributes in surface and we'd

00:26:47,470 --> 00:26:52,750
have the whole standard UNIX permissions

00:26:49,840 --> 00:26:56,100
mechanism in CFS but in terms of the

00:26:52,750 --> 00:27:07,960
specific akal capability for specific

00:26:56,100 --> 00:27:12,700
platforms there is similar if needed so

00:27:07,960 --> 00:27:14,380
so ok so that's that's a great question

00:27:12,700 --> 00:27:16,120
i should clarify that the question is

00:27:14,380 --> 00:27:18,390
can i mount surfer fest for multiple

00:27:16,120 --> 00:27:22,630
clients at once and the answer is yes

00:27:18,390 --> 00:27:25,480
that that's the the sort of technical

00:27:22,630 --> 00:27:28,780
definition of shared in the context of

00:27:25,480 --> 00:27:30,190
file systems so in file system jargon a

00:27:28,780 --> 00:27:34,630
local file system would be something

00:27:30,190 --> 00:27:36,850
like ext4 and you could have a remotely

00:27:34,630 --> 00:27:38,020
accessed ext4 filesystem if you had I

00:27:36,850 --> 00:27:40,180
scuzzy in the middle or something like

00:27:38,020 --> 00:27:41,980
that whereas a shared file system is

00:27:40,180 --> 00:27:44,350
something where multiple clients can

00:27:41,980 --> 00:27:46,060
mount it at the same time and do

00:27:44,350 --> 00:27:48,310
operations in it at the same time and

00:27:46,060 --> 00:27:50,260
they will be consistent so having two

00:27:48,310 --> 00:27:52,240
processes running on separate hosts

00:27:50,260 --> 00:27:54,670
using two different mounts has the same

00:27:52,240 --> 00:27:56,250
POSIX consistency guarantees as having

00:27:54,670 --> 00:28:01,000
two processes running on one machine

00:27:56,250 --> 00:28:04,450
looking at the same map somehow

00:28:01,000 --> 00:28:06,630
somewhere in locker dot CC yes yes at

00:28:04,450 --> 00:28:06,630
the back

00:28:34,909 --> 00:28:41,599
okay so the question is what about small

00:28:37,369 --> 00:28:43,070
files in surface you're right it's an

00:28:41,599 --> 00:28:45,830
Achilles heel of distributed file

00:28:43,070 --> 00:28:47,419
systems generally small files support

00:28:45,830 --> 00:28:49,849
and things like you know luster and

00:28:47,419 --> 00:28:51,649
gluster the you know the established

00:28:49,849 --> 00:28:57,259
players in this area in the open source

00:28:51,649 --> 00:29:02,090
area to a certain extent it's inevitable

00:28:57,259 --> 00:29:04,580
that you you pay a latency cost for each

00:29:02,090 --> 00:29:06,679
metadata operation that you do and you

00:29:04,580 --> 00:29:09,529
hopefully amor ties that with some

00:29:06,679 --> 00:29:12,470
bandwidth afterwards and in small files

00:29:09,529 --> 00:29:14,570
that equations working against you so in

00:29:12,470 --> 00:29:20,840
surfers there are a couple of things

00:29:14,570 --> 00:29:23,989
that we do one is that the clients cash

00:29:20,840 --> 00:29:26,450
a lot of metadata and that includes a

00:29:23,989 --> 00:29:29,659
fair bit of intelligence about whether

00:29:26,450 --> 00:29:33,349
they have a complete directory or not so

00:29:29,659 --> 00:29:35,119
in many cases when creating files we

00:29:33,349 --> 00:29:38,149
don't have to do a round trip to the MBS

00:29:35,119 --> 00:29:41,330
each time a client can know i created

00:29:38,149 --> 00:29:44,659
this directory i have a complete picture

00:29:41,330 --> 00:29:46,820
of it in my cash and so when i can go

00:29:44,659 --> 00:29:49,039
ahead and create a file rather without

00:29:46,820 --> 00:29:52,849
having to ask the MBS for permission the

00:29:49,039 --> 00:29:54,679
other thing we do is in lining so for

00:29:52,849 --> 00:29:55,999
very small files and this is an optional

00:29:54,679 --> 00:29:58,970
feature that you can enable because it's

00:29:55,999 --> 00:30:01,009
not a good thing for all workloads you

00:29:58,970 --> 00:30:03,919
can say the file is below a certain size

00:30:01,009 --> 00:30:07,220
and so instead of sending it to its own

00:30:03,919 --> 00:30:10,070
radar subject out there in the cluster I

00:30:07,220 --> 00:30:13,729
will send it with my metadata operation

00:30:10,070 --> 00:30:17,090
to the MDS and the MDS will store it

00:30:13,729 --> 00:30:18,919
along with the inode in the same place

00:30:17,090 --> 00:30:21,259
the MDS stores it so in that instance

00:30:18,919 --> 00:30:24,229
you you have a lot fewer objects in your

00:30:21,259 --> 00:30:25,489
raiders cluster as a result but you are

00:30:24,229 --> 00:30:27,200
you know it's a compromise you're

00:30:25,489 --> 00:30:30,229
sending more data by your MDS at that

00:30:27,200 --> 00:30:33,470
point so there's no silver bullet for

00:30:30,229 --> 00:30:34,759
small files in a distributed system but

00:30:33,470 --> 00:30:37,479
we do have a couple mechanisms in place

00:30:34,759 --> 00:30:37,479
for dealing with that

00:31:13,550 --> 00:31:19,200
um so I'm not a Gloucester expert so I

00:31:16,740 --> 00:31:20,610
the question is do we have something

00:31:19,200 --> 00:31:24,360
equivalent to a feature where Gloucester

00:31:20,610 --> 00:31:25,980
uses a local volume for caching I mean

00:31:24,360 --> 00:31:28,050
we could we could take that offline and

00:31:25,980 --> 00:31:29,220
talk about exactly what that is in

00:31:28,050 --> 00:31:33,300
Gloucester because I'm not familiar with

00:31:29,220 --> 00:31:34,800
it but in general there are some tricks

00:31:33,300 --> 00:31:37,020
that you might do in a system like

00:31:34,800 --> 00:31:40,560
cluster where you have the files stored

00:31:37,020 --> 00:31:42,510
directly inside a brick that wouldn't

00:31:40,560 --> 00:31:43,920
necessarily apply to SEF where files are

00:31:42,510 --> 00:31:47,250
translated into objects that are stored

00:31:43,920 --> 00:31:48,690
across a whole cluster so I would doubt

00:31:47,250 --> 00:31:50,700
there would be a one-to-one mapping of

00:31:48,690 --> 00:32:00,720
that method but we can talk about it if

00:31:50,700 --> 00:32:03,870
you like ok so I want to introduce are

00:32:00,720 --> 00:32:05,550
not just the basic default settings

00:32:03,870 --> 00:32:07,620
force F but a little bit of the extra

00:32:05,550 --> 00:32:09,320
stuff that it can do and some of this

00:32:07,620 --> 00:32:11,820
stuff it hasn't been capable of doing

00:32:09,320 --> 00:32:13,520
since before relatively recent releases

00:32:11,820 --> 00:32:15,960
so this is comparatively new stuff

00:32:13,520 --> 00:32:17,850
erasure coding and cash tearing was part

00:32:15,960 --> 00:32:20,910
of the Firefly release of SEF that came

00:32:17,850 --> 00:32:24,150
out in spring of last year so a little

00:32:20,910 --> 00:32:26,340
under a year ago which in in sort of

00:32:24,150 --> 00:32:30,150
enterprise software terms is incredibly

00:32:26,340 --> 00:32:32,430
recent but we're doing like 22 stable

00:32:30,150 --> 00:32:36,630
releases a year so that's already two

00:32:32,430 --> 00:32:40,650
releases old for us so firstly cash

00:32:36,630 --> 00:32:44,040
tearing this allows you to take 27 have

00:32:40,650 --> 00:32:46,920
to SEF pools to ray das pools I only

00:32:44,040 --> 00:32:49,680
sets ffs there I'm obsessed with it and

00:32:46,920 --> 00:32:51,750
use one as a cash to the other so the

00:32:49,680 --> 00:32:53,790
reason you would want to do that is that

00:32:51,750 --> 00:32:56,070
you can use crush rules to place a

00:32:53,790 --> 00:32:58,830
particular pool on two particular

00:32:56,070 --> 00:33:01,260
storage that means you can have an SSD

00:32:58,830 --> 00:33:03,210
pool and a hard drive pool and use the

00:33:01,260 --> 00:33:08,070
SSD pool as a cash to the hard drive

00:33:03,210 --> 00:33:11,040
pool that's pretty useful if you have

00:33:08,070 --> 00:33:12,930
the right kind of data set for it if all

00:33:11,040 --> 00:33:14,700
of your data is comparatively hot data

00:33:12,930 --> 00:33:16,530
then this won't do very much for you

00:33:14,700 --> 00:33:18,870
because you know you'll constantly be

00:33:16,530 --> 00:33:23,460
flashing your cash but if you have a

00:33:18,870 --> 00:33:25,980
fair bit of warm to cold data like old

00:33:23,460 --> 00:33:28,260
versions of disc images for example

00:33:25,980 --> 00:33:30,090
snapshots of disk images then you can

00:33:28,260 --> 00:33:33,630
use this to make sure those end up

00:33:30,090 --> 00:33:35,100
getting migrated down to spinning disks

00:33:33,630 --> 00:33:37,169
while all the stuff that you're

00:33:35,100 --> 00:33:42,780
accessing day-to-day might remain on

00:33:37,169 --> 00:33:46,760
your SSD tier so the the kind of caveat

00:33:42,780 --> 00:33:50,910
with this is that that cash is a

00:33:46,760 --> 00:33:53,910
persistent cash that's doing synchronous

00:33:50,910 --> 00:33:55,590
i/o so it's it's a normal set pool

00:33:53,910 --> 00:33:57,299
moralizing you the same kind of

00:33:55,590 --> 00:33:59,460
resilience guarantees as you get with

00:33:57,299 --> 00:34:01,860
any other SEF pool and that means that

00:33:59,460 --> 00:34:05,220
operations like promoting something to

00:34:01,860 --> 00:34:07,830
the cash from your you're backing pool

00:34:05,220 --> 00:34:10,020
are comparatively expensive because you

00:34:07,830 --> 00:34:12,649
have to write this resilient redundant

00:34:10,020 --> 00:34:15,060
copy of your data into the cash i

00:34:12,649 --> 00:34:16,350
mentioned that not to discourage you

00:34:15,060 --> 00:34:18,290
from using cash tearing it's an

00:34:16,350 --> 00:34:20,879
incredibly useful and powerful feature

00:34:18,290 --> 00:34:22,919
but just to clarify that it's not a

00:34:20,879 --> 00:34:25,740
capsule it's not something you want to

00:34:22,919 --> 00:34:26,970
enable on all systems because it's it

00:34:25,740 --> 00:34:32,970
depends on your work loading it has

00:34:26,970 --> 00:34:34,649
those caveats the feature that goes best

00:34:32,970 --> 00:34:37,230
hand in hand with cash tearing is

00:34:34,649 --> 00:34:40,290
erasure coding so raise your coding is a

00:34:37,230 --> 00:34:44,250
piece of a piece of jargon which

00:34:40,290 --> 00:34:46,379
essentially means raid sex or raid 5 so

00:34:44,250 --> 00:34:50,040
this is where you take a piece of data

00:34:46,379 --> 00:34:53,490
and you break it up into M data chunks

00:34:50,040 --> 00:34:56,550
and K parity chunks so for example if

00:34:53,490 --> 00:35:01,580
you have a 10 disc raid 6 array then you

00:34:56,550 --> 00:35:04,350
have 8 M equals 8 and K equals 2 so

00:35:01,580 --> 00:35:06,510
erasure coding ancef takes that model

00:35:04,350 --> 00:35:09,060
and applies it across the Raiders

00:35:06,510 --> 00:35:11,940
cluster instead of having three copies

00:35:09,060 --> 00:35:16,170
of one piece of data we break the data

00:35:11,940 --> 00:35:20,160
up into n plus K chunks and we scatter

00:35:16,170 --> 00:35:23,250
those across the cluster the important

00:35:20,160 --> 00:35:24,600
thing about erasure coding is that it

00:35:23,250 --> 00:35:26,340
gives you much more efficient use of

00:35:24,600 --> 00:35:28,650
your underlying storage the replication

00:35:26,340 --> 00:35:31,140
so if you do threeway replication that's

00:35:28,650 --> 00:35:34,109
a two hundred percent overhead in terms

00:35:31,140 --> 00:35:35,550
of desks which you can make financially

00:35:34,109 --> 00:35:37,170
you can make a certain amount of that

00:35:35,550 --> 00:35:38,790
back by the fact that you didn't have to

00:35:37,170 --> 00:35:39,810
buy expensive disks or raid controllers

00:35:38,790 --> 00:35:41,910
to begin with but

00:35:39,810 --> 00:35:43,350
still kind of hurts for some workloads

00:35:41,910 --> 00:35:46,230
if you've got really large amounts of

00:35:43,350 --> 00:35:48,720
bulk data so Reggie coding is the

00:35:46,230 --> 00:35:50,130
solution to that within SEF that you can

00:35:48,720 --> 00:35:52,680
use a rager coding for those workloads

00:35:50,130 --> 00:35:54,000
that have large amounts of bulk data

00:35:52,680 --> 00:35:56,040
that you don't necessarily access that

00:35:54,000 --> 00:36:01,020
often but you'd still use replication

00:35:56,040 --> 00:36:06,000
for your day to day near line data if

00:36:01,020 --> 00:36:08,430
Reggie coding isn't just a little more

00:36:06,000 --> 00:36:09,630
expensive to do than replication I mean

00:36:08,430 --> 00:36:11,550
I don't think actually called that out

00:36:09,630 --> 00:36:14,580
so so a rager coding is a calculation

00:36:11,550 --> 00:36:16,350
it's mathematics it has a cpu load

00:36:14,580 --> 00:36:18,210
associated with it and you're also

00:36:16,350 --> 00:36:19,890
breaking the data up into more chunks

00:36:18,210 --> 00:36:22,860
you're talking to more different oh SDS

00:36:19,890 --> 00:36:26,640
and in a distributed system you have a

00:36:22,860 --> 00:36:29,400
weakest link latency effect so you your

00:36:26,640 --> 00:36:31,020
latency of an operation will be the

00:36:29,400 --> 00:36:32,760
latency of whichever OSD responded

00:36:31,020 --> 00:36:34,560
slowest so statistically if you're

00:36:32,760 --> 00:36:38,790
writing two moro sd's your latency tends

00:36:34,560 --> 00:36:41,700
to get a little bit worse you also find

00:36:38,790 --> 00:36:43,500
that what you can do with an erasure

00:36:41,700 --> 00:36:44,670
coded pool in CF is a little bit more

00:36:43,500 --> 00:36:48,750
limited than what you can do with a

00:36:44,670 --> 00:36:51,570
replicated pull your raid controller in

00:36:48,750 --> 00:36:54,150
your traditional server will let you

00:36:51,570 --> 00:36:58,080
modify the data however you want to late

00:36:54,150 --> 00:37:00,330
you go to an arbitrary bite and right of

00:36:58,080 --> 00:37:02,780
one instead of a zero but under the hood

00:37:00,330 --> 00:37:04,800
the reason raid controllers are

00:37:02,780 --> 00:37:06,960
comparatively expensive specialized

00:37:04,800 --> 00:37:09,510
pieces of silicon is that what it's

00:37:06,960 --> 00:37:12,120
having to do to do that is to read that

00:37:09,510 --> 00:37:14,010
look back modify it chunk it up again

00:37:12,120 --> 00:37:15,780
encode it again and write it back to

00:37:14,010 --> 00:37:17,520
disks which is why when you're doing

00:37:15,780 --> 00:37:19,350
high-performance applications with a an

00:37:17,520 --> 00:37:20,940
enterprise raid controller you are

00:37:19,350 --> 00:37:23,310
frequently advised to do things like

00:37:20,940 --> 00:37:26,310
four megabyte aligned rights it's

00:37:23,310 --> 00:37:29,280
because of this so in SEF rather than

00:37:26,310 --> 00:37:32,070
just letting you do that and having you

00:37:29,280 --> 00:37:33,630
take the hit if you're unwary we

00:37:32,070 --> 00:37:35,730
restrict what you can do with an erasure

00:37:33,630 --> 00:37:39,720
coded pool we don't let you modify

00:37:35,730 --> 00:37:41,970
things within it the way we make that

00:37:39,720 --> 00:37:44,910
into a usable system is to combine cash

00:37:41,970 --> 00:37:49,590
cheering and erasure coding so you can

00:37:44,910 --> 00:37:53,160
put a replicated pool that will use as a

00:37:49,590 --> 00:37:55,510
cash on top of an erasure coded pool

00:37:53,160 --> 00:37:57,460
so you're paying a two hundred percent

00:37:55,510 --> 00:37:59,500
overhead in terms of storage space in

00:37:57,460 --> 00:38:01,990
your replicated pool but you're going to

00:37:59,500 --> 00:38:03,940
get really nice consistent latency

00:38:01,990 --> 00:38:05,799
numbers and you're going to be able to

00:38:03,940 --> 00:38:08,079
do everything with it that you could do

00:38:05,799 --> 00:38:11,410
with any other pool so you can run LG

00:38:08,079 --> 00:38:12,940
WRB DCFS on top of that pool and then

00:38:11,410 --> 00:38:15,849
any data which doesn't get touched or a

00:38:12,940 --> 00:38:19,869
long time gets migrated transparently

00:38:15,849 --> 00:38:21,490
within SEF to the cold pool and there

00:38:19,869 --> 00:38:22,630
are configurable casual settings here

00:38:21,490 --> 00:38:24,400
for how many objects you want in your

00:38:22,630 --> 00:38:25,569
hot tier and how long things have to be

00:38:24,400 --> 00:38:29,309
there before they can get my greater

00:38:25,569 --> 00:38:32,609
than so on for a little workloads that

00:38:29,309 --> 00:38:34,740
gives you a really great balance of

00:38:32,609 --> 00:38:36,910
efficiency of use of storage and

00:38:34,740 --> 00:38:39,970
flexibility and speed for the stuff that

00:38:36,910 --> 00:38:42,609
you access often and this can all

00:38:39,970 --> 00:38:44,829
coexist with any other stuff you have on

00:38:42,609 --> 00:38:47,109
your staff cluster so you don't have to

00:38:44,829 --> 00:38:48,849
decide up front my whole set Buster is

00:38:47,109 --> 00:38:50,529
going to be like this this is a pool by

00:38:48,849 --> 00:38:51,910
pool thing and you can have pools

00:38:50,529 --> 00:38:52,900
addressing different bits of physical

00:38:51,910 --> 00:38:54,519
hardware so you have a lot of

00:38:52,900 --> 00:38:56,109
flexibility and there are a lot of

00:38:54,519 --> 00:38:58,470
different ways to why this stuff

00:38:56,109 --> 00:39:00,930
together but this is a typical one and

00:38:58,470 --> 00:39:02,799
just to give you an idea of what the

00:39:00,930 --> 00:39:04,839
administrative overhead is of setting

00:39:02,799 --> 00:39:06,880
something like this up well this is a

00:39:04,839 --> 00:39:09,670
few commands more than it took to set up

00:39:06,880 --> 00:39:11,980
a SEF FS file system but it's still

00:39:09,670 --> 00:39:15,849
fairly simple for something that

00:39:11,980 --> 00:39:17,710
powerful under the hood we're creating a

00:39:15,849 --> 00:39:20,349
rager code profile where we're setting

00:39:17,710 --> 00:39:23,109
our K and M so unlike a lot of legacy

00:39:20,349 --> 00:39:25,200
raid controllers this is completely

00:39:23,109 --> 00:39:28,599
customizable you can pick your ratio

00:39:25,200 --> 00:39:29,859
some ratios are more efficient than

00:39:28,599 --> 00:39:32,710
others based on the underlying

00:39:29,859 --> 00:39:34,630
mathematics that's going on but you can

00:39:32,710 --> 00:39:37,869
do things like having you know m equals

00:39:34,630 --> 00:39:40,630
10 and K equals 2 or or M equals ten and

00:39:37,869 --> 00:39:42,789
K equals 10 and get to a position where

00:39:40,630 --> 00:39:44,680
you can tolerate a very large number of

00:39:42,789 --> 00:39:48,369
losses before you would have lost data

00:39:44,680 --> 00:39:49,990
at the cost of more storage space and at

00:39:48,369 --> 00:39:51,900
the cost of having to do more

00:39:49,990 --> 00:39:54,220
mathematics when you write something and

00:39:51,900 --> 00:39:56,259
then similarly to what we did with CFS

00:39:54,220 --> 00:39:57,970
we create a pool for the cold here we

00:39:56,259 --> 00:40:00,309
create a pool for the cash tier and we

00:39:57,970 --> 00:40:02,019
used a few more commands to associate

00:40:00,309 --> 00:40:05,650
them with each other and from that point

00:40:02,019 --> 00:40:07,200
onwards any application that access to

00:40:05,650 --> 00:40:10,060
the tier that I called cash

00:40:07,200 --> 00:40:12,430
would be entirely unaware that there was

00:40:10,060 --> 00:40:14,770
also a cold erasure coded pool sitting

00:40:12,430 --> 00:40:16,120
beneath it so you have a total

00:40:14,770 --> 00:40:21,970
abstraction beneath the cached here in

00:40:16,120 --> 00:40:27,340
that case finally I want to talk about

00:40:21,970 --> 00:40:30,190
what's new so SEF not 94 came out I

00:40:27,340 --> 00:40:32,410
think last week very recently it's

00:40:30,190 --> 00:40:35,050
called hammer so we have an alphabetical

00:40:32,410 --> 00:40:36,580
naming system where we're picking the

00:40:35,050 --> 00:40:40,000
names of different cephalopods and

00:40:36,580 --> 00:40:42,400
squids and octopuses and so on so the

00:40:40,000 --> 00:40:44,230
next one after that is infernalis which

00:40:42,400 --> 00:40:45,910
i think is a great name for a piece of

00:40:44,230 --> 00:40:49,990
software it sounds very heavy metal I

00:40:45,910 --> 00:40:51,970
think so in hammer I'm just going to run

00:40:49,990 --> 00:40:54,280
through what's new in each component so

00:40:51,970 --> 00:40:56,530
in the underlying radars layer the the

00:40:54,280 --> 00:40:58,390
main new thing is performance so we've

00:40:56,530 --> 00:41:00,370
had a lot of great contributions from

00:40:58,390 --> 00:41:02,230
companies like sandisk who are

00:41:00,370 --> 00:41:06,670
interested in running safe on their

00:41:02,230 --> 00:41:09,250
high-end a solid state and in some of

00:41:06,670 --> 00:41:11,980
the cases non-volatile memory storage so

00:41:09,250 --> 00:41:14,110
they're interested in I ops that's

00:41:11,980 --> 00:41:15,730
involved a lot of riorca tech ting of

00:41:14,110 --> 00:41:17,320
bits of the software so there are parts

00:41:15,730 --> 00:41:19,000
of SEF that were designed around the

00:41:17,320 --> 00:41:20,830
idea of spinning disks where you had a

00:41:19,000 --> 00:41:22,000
lot of latency from the disk to play

00:41:20,830 --> 00:41:25,090
with you don't have to worry too much

00:41:22,000 --> 00:41:26,980
about your latency and software but that

00:41:25,090 --> 00:41:28,990
millet latency matters a lot more when

00:41:26,980 --> 00:41:30,940
you're trying to send hundreds of

00:41:28,990 --> 00:41:33,790
thousands of I ops to a single PCI

00:41:30,940 --> 00:41:37,090
Express flash device there is a new

00:41:33,790 --> 00:41:38,980
crush setting in in Hammer called straw

00:41:37,090 --> 00:41:43,000
too you don't need to worry too much

00:41:38,980 --> 00:41:45,850
about that but that's an incremental

00:41:43,000 --> 00:41:47,590
improvement on some corner cases where

00:41:45,850 --> 00:41:48,490
you're adding or removing disks and you

00:41:47,590 --> 00:41:50,550
might have been doing more data

00:41:48,490 --> 00:41:54,610
migration than was otherwise necessary

00:41:50,550 --> 00:41:56,710
and cash steering has a new setting that

00:41:54,610 --> 00:41:58,530
will allow you to do reads straight

00:41:56,710 --> 00:42:00,670
through into the erasure coded tier

00:41:58,530 --> 00:42:03,160
without having to promote things to the

00:42:00,670 --> 00:42:04,540
cashier so for some workloads where you

00:42:03,160 --> 00:42:06,490
just periodically need to do a single

00:42:04,540 --> 00:42:08,710
right of something without taking that

00:42:06,490 --> 00:42:09,670
hit of promoting it you can get much

00:42:08,710 --> 00:42:13,810
better performance now and you could

00:42:09,670 --> 00:42:17,850
before rbd has had efficiency

00:42:13,810 --> 00:42:20,740
improvements as well so we maintain a

00:42:17,850 --> 00:42:22,660
map now of which parts of an image have

00:42:20,740 --> 00:42:24,280
ever been written to so that when we do

00:42:22,660 --> 00:42:25,600
things like exporting and importing and

00:42:24,280 --> 00:42:28,390
cloning we can do a much more efficient

00:42:25,600 --> 00:42:30,460
version of that there is new locking in

00:42:28,390 --> 00:42:33,730
our BD if you have multiple servers that

00:42:30,460 --> 00:42:35,290
try and access the same rbd image that

00:42:33,730 --> 00:42:37,450
is rarely what you actually want it to

00:42:35,290 --> 00:42:39,640
happen if two servers try and mount the

00:42:37,450 --> 00:42:41,890
same ext4 filesystem on a shared block

00:42:39,640 --> 00:42:45,210
device bad things tend to happen so

00:42:41,890 --> 00:42:47,500
that's safer than it used to be in

00:42:45,210 --> 00:42:49,690
radars gateway we have new support for

00:42:47,500 --> 00:42:51,550
the s3 object versioning API so that's a

00:42:49,690 --> 00:42:53,970
that's a neat feature that Amazon have

00:42:51,550 --> 00:42:56,260
in their s3 implementation that lets you

00:42:53,970 --> 00:42:58,300
implicitly create new versions of

00:42:56,260 --> 00:43:00,640
objects every time somebody writes it

00:42:58,300 --> 00:43:02,910
essentially makes your s3 bucket version

00:43:00,640 --> 00:43:05,890
control so we support that now and

00:43:02,910 --> 00:43:08,650
there's also been another efficiency

00:43:05,890 --> 00:43:11,050
improvement in our GW that previously

00:43:08,650 --> 00:43:12,250
there were some scalability limits on

00:43:11,050 --> 00:43:13,720
how many objects you could have in a

00:43:12,250 --> 00:43:16,540
single bucket before things started

00:43:13,720 --> 00:43:18,160
slowing down and that's been removed by

00:43:16,540 --> 00:43:22,150
shutting up some of our metadata across

00:43:18,160 --> 00:43:25,210
multiple objects finally I'm closest to

00:43:22,150 --> 00:43:27,160
my heart sefa vests the SEF file system

00:43:25,210 --> 00:43:29,770
has seen a lot of work over the time

00:43:27,160 --> 00:43:32,369
between Firefly which was the previous

00:43:29,770 --> 00:43:36,700
long-term supported release and Hammer

00:43:32,369 --> 00:43:39,310
so mainly this is about resilience it's

00:43:36,700 --> 00:43:42,880
about hardening and it's also about

00:43:39,310 --> 00:43:44,680
usability so people who have used

00:43:42,880 --> 00:43:46,180
distributed file systems before might

00:43:44,680 --> 00:43:48,250
have come across the cases where

00:43:46,180 --> 00:43:50,830
something goes wrong maybe there's a

00:43:48,250 --> 00:43:52,420
server not responding or something's

00:43:50,830 --> 00:43:54,580
gone wrong internally and the way you

00:43:52,420 --> 00:43:56,590
find out about it all too often is that

00:43:54,580 --> 00:43:58,660
you go into a directory and type LS and

00:43:56,590 --> 00:44:00,790
it just sits there because it can't

00:43:58,660 --> 00:44:03,280
service that request or it's waiting for

00:44:00,790 --> 00:44:05,290
something to happen so we've added a

00:44:03,280 --> 00:44:07,930
bunch of health checks and diagnostics

00:44:05,290 --> 00:44:10,890
to surface so that it will now be able

00:44:07,930 --> 00:44:14,260
to proactively tell the administrator

00:44:10,890 --> 00:44:16,480
that oh this client isn't responding to

00:44:14,260 --> 00:44:17,950
an RPC that I sent it or this client has

00:44:16,480 --> 00:44:20,590
a behavior which indicates you've hit

00:44:17,950 --> 00:44:23,800
bug 768 you might want to restart that

00:44:20,590 --> 00:44:26,080
guy that kind of thing we've also added

00:44:23,800 --> 00:44:29,109
some recovery tools this is a work in

00:44:26,080 --> 00:44:31,690
progress so the metadata servers and CFS

00:44:29,109 --> 00:44:33,940
have a journal as well as their main

00:44:31,690 --> 00:44:34,599
store for the metadata and where we are

00:44:33,940 --> 00:44:36,190
at the moment

00:44:34,599 --> 00:44:38,589
that we have a bunch of the tools for

00:44:36,190 --> 00:44:40,089
checking and recovering journals if

00:44:38,589 --> 00:44:42,789
something goes wrong that's in hammer

00:44:40,089 --> 00:44:44,289
and at the moment where we're working on

00:44:42,789 --> 00:44:45,910
the recovery tools for the other stuff

00:44:44,289 --> 00:44:50,440
which will make it into a subsequent SEF

00:44:45,910 --> 00:44:53,859
release there's a new online scrub for

00:44:50,440 --> 00:44:55,779
the metadata so checking that if we have

00:44:53,859 --> 00:44:57,430
an inode in our metadata pool that the

00:44:55,779 --> 00:44:58,869
corresponding data exists and has the

00:44:57,430 --> 00:45:00,279
size we think it in the data pool and

00:44:58,869 --> 00:45:01,720
that kind of thing that's the kind of

00:45:00,279 --> 00:45:03,400
thing that a filesystem check might do

00:45:01,720 --> 00:45:06,210
in a legacy file system in a local file

00:45:03,400 --> 00:45:08,619
system but we have to do that online

00:45:06,210 --> 00:45:10,239
because the size of these systems we're

00:45:08,619 --> 00:45:12,940
talking about multi patter bite systems

00:45:10,239 --> 00:45:14,470
doing that offline means taking your

00:45:12,940 --> 00:45:18,190
system offline for an unacceptably long

00:45:14,470 --> 00:45:20,619
period of time a little bit of

00:45:18,190 --> 00:45:21,849
housekeeping stuff so filling up a SEF

00:45:20,619 --> 00:45:23,349
file system used to be kind of an

00:45:21,849 --> 00:45:25,749
interesting experience in terms of how

00:45:23,349 --> 00:45:27,369
it would respond to that it has more it

00:45:25,749 --> 00:45:29,499
has safer handling for that now and

00:45:27,369 --> 00:45:31,269
that's mirrored in a increase in the

00:45:29,499 --> 00:45:33,789
amount of QA we've done with CFS as well

00:45:31,269 --> 00:45:35,349
so there are a lot more tests force F of

00:45:33,789 --> 00:45:37,779
s now it's in the progress of being

00:45:35,349 --> 00:45:41,349
stabilized and there's also new support

00:45:37,779 --> 00:45:44,160
for client quotas so CFS has a kernel

00:45:41,349 --> 00:45:46,359
client it also has a fuse client these

00:45:44,160 --> 00:45:47,799
quotas are currently only supported in a

00:45:46,359 --> 00:45:50,890
fuse client and they're not completely

00:45:47,799 --> 00:45:53,589
strict a client can slightly over run

00:45:50,890 --> 00:45:54,700
its quota but that could be useful if

00:45:53,589 --> 00:45:57,849
you have a use case where you've got

00:45:54,700 --> 00:45:58,960
multiple users and you want a general

00:45:57,849 --> 00:46:05,200
sense that they're not going to be able

00:45:58,960 --> 00:46:08,769
to have too much data finally here are

00:46:05,200 --> 00:46:10,059
some links fee to get involved come to

00:46:08,769 --> 00:46:12,099
the mailing list come to the IC channel

00:46:10,059 --> 00:46:13,809
like I said like everything else at this

00:46:12,099 --> 00:46:15,940
conference is completely open source so

00:46:13,809 --> 00:46:18,700
we rely on you we rely on your feedback

00:46:15,940 --> 00:46:19,960
we rely on your testing you know try not

00:46:18,700 --> 00:46:21,339
to swear too much when you're giving us

00:46:19,960 --> 00:46:25,089
the feedback but if something's broken

00:46:21,339 --> 00:46:27,819
you know we'd like to know and next

00:46:25,089 --> 00:46:30,309
Tuesday here in Berlin is the next SEF

00:46:27,819 --> 00:46:33,670
de Berlin so if you're free next Tuesday

00:46:30,309 --> 00:46:36,190
go to that link and there are still

00:46:33,670 --> 00:46:37,509
tickets available I think so I'll turn

00:46:36,190 --> 00:46:40,499
it over to questions now thanks very

00:46:37,509 --> 00:46:40,499
much yes

00:46:41,390 --> 00:46:47,670
I'm sorry didn't catch that so the

00:46:46,590 --> 00:46:53,220
question is is there a roadmap for

00:46:47,670 --> 00:46:54,570
surface no that doesn't mean that there

00:46:53,220 --> 00:46:58,230
doesn't mean we're not doing it what it

00:46:54,570 --> 00:47:01,440
means is that we're not we're not saying

00:46:58,230 --> 00:47:03,440
it will be done by this date the main

00:47:01,440 --> 00:47:07,590
sort of marker in the road for us is

00:47:03,440 --> 00:47:10,020
having those recovery tools in place so

00:47:07,590 --> 00:47:12,630
it's it's less about claiming that there

00:47:10,020 --> 00:47:15,690
will be zero bugs because all software

00:47:12,630 --> 00:47:16,980
has bugs and surfer has already for the

00:47:15,690 --> 00:47:20,700
last couple of years hasn't had a

00:47:16,980 --> 00:47:22,920
serious data loss bug anyway it's more

00:47:20,700 --> 00:47:25,230
about getting to a position where if

00:47:22,920 --> 00:47:27,900
there was a bug hypothetically that

00:47:25,230 --> 00:47:30,560
damaged someone's data we would be able

00:47:27,900 --> 00:47:33,240
to recover from that without having to

00:47:30,560 --> 00:47:35,130
send one of our developers on site to

00:47:33,240 --> 00:47:36,930
manually hex edit things you know back

00:47:35,130 --> 00:47:40,080
into the right state so but that's

00:47:36,930 --> 00:47:41,700
that's our kind of goal and that's

00:47:40,080 --> 00:47:47,030
something that's being you know being

00:47:41,700 --> 00:47:47,030
worked on right now yes

00:48:04,740 --> 00:48:10,270
right so the question is does does the

00:48:07,960 --> 00:48:12,220
OSD do check something and you're quite

00:48:10,270 --> 00:48:13,540
correct that's left either to the

00:48:12,220 --> 00:48:17,560
underlying application at the underlying

00:48:13,540 --> 00:48:21,460
desk or to the application what the OS

00:48:17,560 --> 00:48:22,810
DS do do is scrub so if there are 30 s

00:48:21,460 --> 00:48:25,330
DS with a copy of a particular piece of

00:48:22,810 --> 00:48:28,090
data then they will periodically compare

00:48:25,330 --> 00:48:30,220
notes about that data so we have two

00:48:28,090 --> 00:48:33,460
types of scrub there's scrub and there's

00:48:30,220 --> 00:48:36,250
deep scrub and the default scrub which

00:48:33,460 --> 00:48:38,920
happens I think daily by default is a

00:48:36,250 --> 00:48:40,810
metadata check that they agree about the

00:48:38,920 --> 00:48:42,310
version of things and that they have the

00:48:40,810 --> 00:48:44,200
same number of objects in a placement

00:48:42,310 --> 00:48:46,990
group and that kind of thing and then

00:48:44,200 --> 00:48:48,850
the deep scrub is where each OSD holding

00:48:46,990 --> 00:48:50,350
a replica of a placement group will go

00:48:48,850 --> 00:48:53,590
down to the disk and read every single

00:48:50,350 --> 00:48:57,580
bite and calculate a checksum on the fly

00:48:53,590 --> 00:48:59,230
and then compare those checksums so we

00:48:57,580 --> 00:49:02,410
don't you check something on the fly but

00:48:59,230 --> 00:49:04,000
we do periodically scrub the data and if

00:49:02,410 --> 00:49:06,130
you have three copies and one of them

00:49:04,000 --> 00:49:08,760
disagrees with the other two then we can

00:49:06,130 --> 00:49:08,760
recover from that

00:49:20,119 --> 00:49:27,119
mm-hmm so the question is if you might

00:49:23,969 --> 00:49:29,309
if you change the crush algorithm to

00:49:27,119 --> 00:49:32,459
straw too after installing hammer can

00:49:29,309 --> 00:49:35,699
you do a live migration of that so you

00:49:32,459 --> 00:49:38,099
can you don't necessarily want to if you

00:49:35,699 --> 00:49:40,349
change the algorithm in use that will

00:49:38,099 --> 00:49:43,890
change the calculated locations for the

00:49:40,349 --> 00:49:46,289
data which will result in movement so

00:49:43,890 --> 00:49:49,249
most people on live running production

00:49:46,289 --> 00:49:51,209
systems don't choose to take that hit

00:49:49,249 --> 00:49:52,739
because obviously it generates a whole

00:49:51,209 --> 00:49:55,199
load of i/o workload on your system and

00:49:52,739 --> 00:49:57,719
if your system is running happily with

00:49:55,199 --> 00:50:02,420
an existing algorithm then it's not

00:49:57,719 --> 00:50:02,420
really worthwhile to move to the new one

00:50:17,419 --> 00:50:24,419
not quite so you can apply this

00:50:21,419 --> 00:50:26,279
algorithm on a pool by pool basis so if

00:50:24,419 --> 00:50:28,079
you had some new OS DS and you decided

00:50:26,279 --> 00:50:29,640
that the data you put on those iced teas

00:50:28,079 --> 00:50:31,319
you would put empty in a different pool

00:50:29,640 --> 00:50:33,569
then you could choose to use that

00:50:31,319 --> 00:50:35,880
algorithm with in that pool but you

00:50:33,569 --> 00:50:39,509
can't apply the algorithm on a per

00:50:35,880 --> 00:50:52,699
object basis or a per disk basis it's

00:50:39,509 --> 00:51:08,039
it's a purple thing ok ok any questions

00:50:52,699 --> 00:51:09,390
come on one more it's the yep so the

00:51:08,039 --> 00:51:13,079
question is is it standard tap three

00:51:09,390 --> 00:51:16,159
copies of the data it's the default it's

00:51:13,079 --> 00:51:19,319
completely customizable in general I

00:51:16,159 --> 00:51:21,659
mean I've seen people using 2 i've seen

00:51:19,319 --> 00:51:24,479
people using four but i think most

00:51:21,659 --> 00:51:27,229
people are using three it's kind of

00:51:24,479 --> 00:51:30,569
accepted as a reasonable trade-off for

00:51:27,229 --> 00:51:33,329
replicated systems to as to is pretty

00:51:30,569 --> 00:51:42,119
risky double disk failures on a larger

00:51:33,329 --> 00:51:46,099
system they're not rare it will tell you

00:51:42,119 --> 00:51:46,099
and then it will be your problem

00:51:47,300 --> 00:51:57,870
yes yep the question is how do you

00:51:56,550 --> 00:52:00,660
monitor self and get metrics out of it

00:51:57,870 --> 00:52:02,580
so you have a choice of tools what's F

00:52:00,660 --> 00:52:05,370
comes with by default is just command

00:52:02,580 --> 00:52:06,840
line tools so you there is a SEF status

00:52:05,370 --> 00:52:12,120
command that will give you an overview

00:52:06,840 --> 00:52:14,880
of your system and there are also each

00:52:12,120 --> 00:52:16,800
demon in the system has what we call an

00:52:14,880 --> 00:52:19,290
admin socket that you can send commands

00:52:16,800 --> 00:52:23,730
to to ask it for things like performance

00:52:19,290 --> 00:52:27,510
counters on top of that there is a

00:52:23,730 --> 00:52:29,700
diamond plugin so diamond is a Python

00:52:27,510 --> 00:52:32,930
package that will send statistics to

00:52:29,700 --> 00:52:35,820
graphite diamond has a safe module and

00:52:32,930 --> 00:52:37,170
that knows how to talk to our services

00:52:35,820 --> 00:52:38,670
and take the statistics and send them

00:52:37,170 --> 00:52:40,620
home however there are a lot of

00:52:38,670 --> 00:52:43,740
statistics so make sure you have a good

00:52:40,620 --> 00:52:46,520
graphite server before you were you try

00:52:43,740 --> 00:52:50,310
switching that on finally there is

00:52:46,520 --> 00:52:54,990
calamari which is a monitoring and

00:52:50,310 --> 00:52:56,460
management web application that's I

00:52:54,990 --> 00:52:59,580
think there should be a new version

00:52:56,460 --> 00:53:00,690
coming out soon so far the the packaging

00:52:59,580 --> 00:53:02,780
of that has been a little challenging

00:53:00,690 --> 00:53:05,850
because it has a lot of dependencies and

00:53:02,780 --> 00:53:07,830
but if you go to our website is on there

00:53:05,850 --> 00:53:10,200
and there's a SEF calamari mailing list

00:53:07,830 --> 00:53:11,520
for that as well so you have a number of

00:53:10,200 --> 00:53:14,640
different and they're also a couple of

00:53:11,520 --> 00:53:16,410
other third-party GUI web apps and the

00:53:14,640 --> 00:53:17,670
other people have written that I can't

00:53:16,410 --> 00:53:22,880
remember the name of any of the moment

00:53:17,670 --> 00:53:22,880
but you have a choice yep

00:53:50,090 --> 00:53:55,560
so the question is would it would it be

00:53:53,790 --> 00:53:57,840
sensible to run something like graphite

00:53:55,560 --> 00:54:00,870
or MySQL and on top of staff for those

00:53:57,840 --> 00:54:05,790
type of workloads yes you can guess

00:54:00,870 --> 00:54:07,560
people do you will never get the same

00:54:05,790 --> 00:54:11,100
latency out of a network system that you

00:54:07,560 --> 00:54:13,470
get out of a local system so it depends

00:54:11,100 --> 00:54:14,850
I mean it sounds like a get out all the

00:54:13,470 --> 00:54:16,680
time but it really depends on the

00:54:14,850 --> 00:54:18,480
workload graphite is actually a really

00:54:16,680 --> 00:54:20,040
interesting example I don't know anyone

00:54:18,480 --> 00:54:23,370
running graphite on top of our BD at the

00:54:20,040 --> 00:54:25,470
moment but I have looked at the i/o pans

00:54:23,370 --> 00:54:28,200
that come out of graphite and it's an

00:54:25,470 --> 00:54:31,650
example of an application that will pile

00:54:28,200 --> 00:54:33,720
I OS into your page cache and then

00:54:31,650 --> 00:54:36,600
gradually drain them out the page cache

00:54:33,720 --> 00:54:38,040
onto desk and I would expect that would

00:54:36,600 --> 00:54:41,820
actually probably work pretty well on

00:54:38,040 --> 00:54:44,790
Seth if you if you tried it but it yeah

00:54:41,820 --> 00:54:47,220
it if you need the latency over local

00:54:44,790 --> 00:54:49,320
SSD for your database you need a local

00:54:47,220 --> 00:55:00,030
SSD and no network system will ever give

00:54:49,320 --> 00:55:03,660
you the same performance but yeah I mean

00:55:00,030 --> 00:55:05,490
it's so so not at not all sands are

00:55:03,660 --> 00:55:08,010
created equal and not all safe clusters

00:55:05,490 --> 00:55:09,540
are created equal so the performance you

00:55:08,010 --> 00:55:12,600
get out of your self cluster is going to

00:55:09,540 --> 00:55:15,840
depend what hardware you buy so if you

00:55:12,600 --> 00:55:19,110
if you buy you know some nice fast low

00:55:15,840 --> 00:55:21,030
latency 10 gig nicks and you put all

00:55:19,110 --> 00:55:24,060
your data on SSDs and you have nice fast

00:55:21,030 --> 00:55:27,150
CPUs then you will get pretty good

00:55:24,060 --> 00:55:29,910
performance whether it will be faster or

00:55:27,150 --> 00:55:32,730
not than a nun content it's an over

00:55:29,910 --> 00:55:34,620
fibre channel my money would be on the

00:55:32,730 --> 00:55:37,370
sand but the surf cluster would probably

00:55:34,620 --> 00:55:37,370
be better value for money

00:55:40,430 --> 00:55:48,680
oh absolutely yeah the the cab the

00:55:45,830 --> 00:55:50,750
caveats around serif s do not apply in

00:55:48,680 --> 00:55:54,500
any way to the rest of staff so RVD and

00:55:50,750 --> 00:55:56,570
I'll GW are entirely ready for

00:55:54,500 --> 00:55:58,670
production yes yeah there's actually a

00:55:56,570 --> 00:56:02,260
really good case study that just came

00:55:58,670 --> 00:56:04,580
out last week for a google for it yahoo

00:56:02,260 --> 00:56:06,560
published a developer blog about how

00:56:04,580 --> 00:56:09,020
they're using safe for their internal

00:56:06,560 --> 00:56:11,710
data storage so they're using radar

00:56:09,020 --> 00:56:14,660
skateway along with airasia coded pools

00:56:11,710 --> 00:56:16,430
and they have a number of Seth clusters

00:56:14,660 --> 00:56:19,010
that they're using as pubs I think each

00:56:16,430 --> 00:56:21,410
pod is three petabytes something like

00:56:19,010 --> 00:56:27,890
that so it's a really nice case study of

00:56:21,410 --> 00:56:31,340
SEF being used at scale okay any

00:56:27,890 --> 00:56:36,830
questions then one more question for my

00:56:31,340 --> 00:56:41,450
side is it possible to UM to control the

00:56:36,830 --> 00:56:44,200
load executed by buy dips clubbing so a

00:56:41,450 --> 00:56:47,390
duplicitous whining my my newest is

00:56:44,200 --> 00:56:52,670
under load and the next flight is on the

00:56:47,390 --> 00:56:55,100
other all right so in the most i think

00:56:52,670 --> 00:56:57,350
this made it into hammer there is some

00:56:55,100 --> 00:56:59,000
io prioritization that would d

00:56:57,350 --> 00:57:01,850
prioritize deep scrubs compared with

00:56:59,000 --> 00:57:04,550
client i oh there is there are also some

00:57:01,850 --> 00:57:08,060
patches for more intelligence scheduling

00:57:04,550 --> 00:57:10,880
of deep scrubs so traditionally it was a

00:57:08,060 --> 00:57:13,880
if you haven't been deep scrubbed for a

00:57:10,880 --> 00:57:16,010
week then deep scrub which meant a week

00:57:13,880 --> 00:57:18,320
after installing your cluster everything

00:57:16,010 --> 00:57:20,180
would decide it needed to deep scrub so

00:57:18,320 --> 00:57:21,890
it's a bit smarter than that now but

00:57:20,180 --> 00:57:24,230
even in that case it's not as bad as it

00:57:21,890 --> 00:57:26,090
sounds because there's also a per OSD

00:57:24,230 --> 00:57:28,490
setting of how many pgs it will scrub at

00:57:26,090 --> 00:57:30,470
one time and which by default i think is

00:57:28,490 --> 00:57:34,880
is one deep scrub at a time something

00:57:30,470 --> 00:57:37,220
like that so it is already somewhat

00:57:34,880 --> 00:57:38,660
configurable and it's but it will become

00:57:37,220 --> 00:57:39,920
more configurable as well because that

00:57:38,660 --> 00:57:45,730
has been a pain point for some

00:57:39,920 --> 00:57:45,730
configurations back

00:57:54,110 --> 00:57:57,540
okay so the question is is there a way

00:57:56,100 --> 00:58:00,770
to warm up a cached here before I sent

00:57:57,540 --> 00:58:05,580
my client traffic to it there are

00:58:00,770 --> 00:58:07,110
explicit commands for for demoting and

00:58:05,580 --> 00:58:08,340
flushing things from cache tears i'm not

00:58:07,110 --> 00:58:10,620
sure if there's an explicit way of

00:58:08,340 --> 00:58:13,380
promoting things in advance either but

00:58:10,620 --> 00:58:15,210
ordinarily you would have the way the

00:58:13,380 --> 00:58:20,100
data was written to begin with would

00:58:15,210 --> 00:58:21,870
have been via the cached here so the way

00:58:20,100 --> 00:58:24,090
you put the data in your erasure coated

00:58:21,870 --> 00:58:25,530
pool to begin with was via the cash so

00:58:24,090 --> 00:58:39,570
it would already be in the cash unless

00:58:25,530 --> 00:58:41,430
it had fallen out okay so all right so

00:58:39,570 --> 00:58:43,860
you're you're adding you're adding a

00:58:41,430 --> 00:58:47,030
cash pool to an existing back in pool I

00:58:43,860 --> 00:58:47,030
would have to get back to you on that

00:58:48,830 --> 00:59:00,630
yes yes the the rules get a little bit

00:58:58,560 --> 00:59:02,640
more complicated so you can have

00:59:00,630 --> 00:59:05,550
write-back cache tears you can also have

00:59:02,640 --> 00:59:08,250
read only cash tears you can definitely

00:59:05,550 --> 00:59:10,110
have multiple read-only cash tears for a

00:59:08,250 --> 00:59:11,940
single backing pool so that's the kind

00:59:10,110 --> 00:59:14,280
of natural use case if you if you wanted

00:59:11,940 --> 00:59:17,970
a cached here in each data center or

00:59:14,280 --> 00:59:19,800
something like that having multiple

00:59:17,970 --> 00:59:21,810
write-back cache tears probably doesn't

00:59:19,800 --> 00:59:24,060
make sense from a consistency point of

00:59:21,810 --> 00:59:27,210
view because in order to maintain

00:59:24,060 --> 00:59:28,410
consistency each cash pool would have to

00:59:27,210 --> 00:59:31,470
be talking to the other ones and we

00:59:28,410 --> 00:59:34,320
don't do that so yes you can have

00:59:31,470 --> 00:59:35,640
multiple cash tears but having multiple

00:59:34,320 --> 00:59:37,920
write-back cache is on a single back in

00:59:35,640 --> 00:59:39,120
pool may not in general make sense

00:59:37,920 --> 00:59:43,890
unless you have a workload that's

00:59:39,120 --> 00:59:48,350
tolerant of inconsistency okay the times

00:59:43,890 --> 00:59:48,350
over now our thang you've ever match

00:59:48,990 --> 00:59:51,050

YouTube URL: https://www.youtube.com/watch?v=qHLXBEkT5Ww


