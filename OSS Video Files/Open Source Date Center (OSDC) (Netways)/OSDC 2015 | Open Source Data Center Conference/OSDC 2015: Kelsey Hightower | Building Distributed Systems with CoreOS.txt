Title: OSDC 2015: Kelsey Hightower | Building Distributed Systems with CoreOS
Publication date: 2015-04-29
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	Building distributed systems is hard, but with the right components just about anyone can get started. At the heart of any distributed system is the underlying infrastructure, which often includes a collection of servers, a central configuration and lock service, and a scheduler to manage your workloads.
CoreOS provides all of these components starting with the base OS, CoreOS Linux; a minimal OS optimized for running Linux containers such as Rocket and Docker. Next is the central key value store, etcd, which provides shared configuration, service discovery, and a cluster wide lock service built on top of the Raft consensus algorithm for high-availability. Finally, fleet is a distributed init system that ties it all together and provides low level workload scheduling. fleet is often used to install higher level workload distribution systems such as Mesos or Kubernetes.
All of these projects are open source and part of growing community. Come learn how they work and how you can get involved.
Captions: 
	00:00:07,000 --> 00:00:12,650
okay it's nice to see you back here in

00:00:09,800 --> 00:00:14,730
this Trek our next session and our next

00:00:12,650 --> 00:00:17,430
speaker is Kelsey hide-hole

00:00:14,730 --> 00:00:20,130
or chorus and as you all know blockers

00:00:17,430 --> 00:00:22,410
all over the place and he will show us a

00:00:20,130 --> 00:00:26,449
live demo of Korres using Coburn

00:00:22,410 --> 00:00:28,410
eighties and as well as docker all right

00:00:26,449 --> 00:00:29,970
we're gonna try something different i'm

00:00:28,410 --> 00:00:31,560
going to reintroduce myself and then

00:00:29,970 --> 00:00:33,120
when i come up we're all going to crap

00:00:31,560 --> 00:00:34,589
really really loud to get the speaker

00:00:33,120 --> 00:00:36,150
really hyped up and energetic we're

00:00:34,589 --> 00:00:38,489
gonna try that okay i'm gonna do it

00:00:36,150 --> 00:00:39,809
again all right so you guys are all

00:00:38,489 --> 00:00:41,730
awake right so our next speaker is

00:00:39,809 --> 00:00:43,350
kelsey Hightower who flew all the way

00:00:41,730 --> 00:00:45,000
over here from Portland Oregon the

00:00:43,350 --> 00:00:46,440
United States to give you this demo and

00:00:45,000 --> 00:00:48,059
worked on the slides all night and then

00:00:46,440 --> 00:00:57,239
get any sleep we're going to give them a

00:00:48,059 --> 00:00:59,039
warm welcome to OS DC thank you alright

00:00:57,239 --> 00:01:01,739
so I heard earlier that eighty percent

00:00:59,039 --> 00:01:04,860
of you are running like data centers

00:01:01,739 --> 00:01:06,330
with real servers right is that true are

00:01:04,860 --> 00:01:07,440
you guys give yourself a big round of

00:01:06,330 --> 00:01:10,610
applause because you have like real

00:01:07,440 --> 00:01:14,580
servers not like virtual unicorn

00:01:10,610 --> 00:01:16,289
machines in the cloud so we're going to

00:01:14,580 --> 00:01:18,240
talk about building distributed systems

00:01:16,289 --> 00:01:20,580
with core OS we're also going to attempt

00:01:18,240 --> 00:01:24,720
to stand up all the components that you

00:01:20,580 --> 00:01:26,280
would do inside of your data center was

00:01:24,720 --> 00:01:28,619
that what's that we're okay all right

00:01:26,280 --> 00:01:30,289
all right all right so what I did right

00:01:28,619 --> 00:01:33,270
before you guys walked in here is I

00:01:30,289 --> 00:01:36,030
pixie booted five virtual machines on my

00:01:33,270 --> 00:01:37,470
laptop to kind of give us a very similar

00:01:36,030 --> 00:01:38,880
environment that if you have bare metal

00:01:37,470 --> 00:01:40,080
machines you're going to probably pick

00:01:38,880 --> 00:01:43,349
seed them hopefully you're not writing

00:01:40,080 --> 00:01:45,149
around a data center with CDs in 2015

00:01:43,349 --> 00:01:47,420
there's a much better way of doing this

00:01:45,149 --> 00:01:49,289
you can you get do something like pixie

00:01:47,420 --> 00:01:52,950
so hopefully you guys know about that

00:01:49,289 --> 00:01:55,440
all right so a couple of goals today one

00:01:52,950 --> 00:01:57,360
is to provide an overview of distributed

00:01:55,440 --> 00:02:01,050
systems and distributed infrastructure

00:01:57,360 --> 00:02:04,259
more concretely and then a live demo of

00:02:01,050 --> 00:02:05,610
Korra Wes and Cooper Nettie's right so

00:02:04,259 --> 00:02:07,530
the goal here is like we have all of

00:02:05,610 --> 00:02:09,270
these puzzle pieces like there's tools

00:02:07,530 --> 00:02:11,160
like Etsy d there's docker there's

00:02:09,270 --> 00:02:12,629
operating systems but what's the point

00:02:11,160 --> 00:02:14,250
of having all these tools right the

00:02:12,629 --> 00:02:16,230
endgame is to actually build something

00:02:14,250 --> 00:02:17,160
and build something useful so we're

00:02:16,230 --> 00:02:19,170
going to see with all the LEGO pieces

00:02:17,160 --> 00:02:21,239
look like connected together and what

00:02:19,170 --> 00:02:22,530
you can build with that and then I want

00:02:21,239 --> 00:02:24,209
to demystify and spark your imagination

00:02:22,530 --> 00:02:26,370
right so a lot of people think these

00:02:24,209 --> 00:02:28,830
systems like uber Nettie's the board

00:02:26,370 --> 00:02:31,050
paper was released right last week

00:02:28,830 --> 00:02:33,390
from google so Borg is a very

00:02:31,050 --> 00:02:35,250
intelligent cluster management system

00:02:33,390 --> 00:02:37,050
that google used to run their physical

00:02:35,250 --> 00:02:40,290
data centers and cooper Nettie's

00:02:37,050 --> 00:02:42,690
represents a open-source edition of that

00:02:40,290 --> 00:02:44,250
and lessons learned but the system is

00:02:42,690 --> 00:02:47,240
not as complicated as what people think

00:02:44,250 --> 00:02:49,950
way to try to show and break that down

00:02:47,240 --> 00:02:52,160
to get your frame of thinking correct I

00:02:49,950 --> 00:02:54,630
want you to ask yourself this question

00:02:52,160 --> 00:02:57,240
how would you design your data center if

00:02:54,630 --> 00:02:58,560
you could never log in right so a lot of

00:02:57,240 --> 00:03:01,590
our tools that we have today are

00:02:58,560 --> 00:03:04,410
basically automating the processes that

00:03:01,590 --> 00:03:06,090
we do today right we log into a server

00:03:04,410 --> 00:03:08,430
and we type a bunch of commands to add

00:03:06,090 --> 00:03:10,590
users and services and check on things

00:03:08,430 --> 00:03:13,590
and what we've done over the years it

00:03:10,590 --> 00:03:15,540
just automate that process right so yay

00:03:13,590 --> 00:03:17,760
for automation but we just took the

00:03:15,540 --> 00:03:20,310
workflow that we had and this wraps up

00:03:17,760 --> 00:03:21,720
in on top of it so imagine if you could

00:03:20,310 --> 00:03:23,459
never log into your server would you

00:03:21,720 --> 00:03:26,910
build it a different way I think the

00:03:23,459 --> 00:03:28,640
answer is yes so we're going to talk

00:03:26,910 --> 00:03:30,780
about distributed systems really quick

00:03:28,640 --> 00:03:33,360
how many of you familiar like with

00:03:30,780 --> 00:03:35,790
distributed systems right something you

00:03:33,360 --> 00:03:38,489
may have degrees in it that's cool I

00:03:35,790 --> 00:03:40,470
don't the way I like to describe a

00:03:38,489 --> 00:03:42,150
distributed system it's a collection of

00:03:40,470 --> 00:03:44,310
machines usually connected over a

00:03:42,150 --> 00:03:47,280
network that act as a single machine

00:03:44,310 --> 00:03:50,280
right an idea here is that you don't

00:03:47,280 --> 00:03:51,989
keep track of all the host names you

00:03:50,280 --> 00:03:54,120
don't try to log into all the servers

00:03:51,989 --> 00:03:56,010
and you want your ssh key everywhere you

00:03:54,120 --> 00:03:58,140
want to have that collection of machines

00:03:56,010 --> 00:04:00,780
was like a single machine that's where

00:03:58,140 --> 00:04:02,970
things get really really hard that

00:04:00,780 --> 00:04:04,709
single machine view needs to be scalable

00:04:02,970 --> 00:04:06,780
so if we were talking about a single

00:04:04,709 --> 00:04:08,940
machine you should be able to plug in a

00:04:06,780 --> 00:04:11,130
new cpu and get more cpu capacity and

00:04:08,940 --> 00:04:12,900
hot-plug memory one of the machines are

00:04:11,130 --> 00:04:14,760
still on this is very popular in the

00:04:12,900 --> 00:04:16,650
mainframe world but you can also do this

00:04:14,760 --> 00:04:17,820
with a collection of servers if we can

00:04:16,650 --> 00:04:20,609
adopt some of the distributed computing

00:04:17,820 --> 00:04:22,470
principles highly available machine goes

00:04:20,609 --> 00:04:24,270
down maybe a new one is provisioned

00:04:22,470 --> 00:04:26,479
automatically or maybe the service right

00:04:24,270 --> 00:04:28,890
on that machine can move somewhere else

00:04:26,479 --> 00:04:30,900
fault-tolerant this was really confusing

00:04:28,890 --> 00:04:33,479
for a lot of people fault tolerance in

00:04:30,900 --> 00:04:36,000
my opinion is you have a bad actor on

00:04:33,479 --> 00:04:37,919
the network in it joined and it starts

00:04:36,000 --> 00:04:39,300
doing something destructive you got to

00:04:37,919 --> 00:04:41,010
be able to protect against pats our

00:04:39,300 --> 00:04:42,710
service is up but it can't communicate

00:04:41,010 --> 00:04:44,300
with everyone else we have

00:04:42,710 --> 00:04:46,220
to have some protections in place and

00:04:44,300 --> 00:04:48,290
we'll see what etsy d does with the rack

00:04:46,220 --> 00:04:49,730
protocol to prevent some of that stuff

00:04:48,290 --> 00:04:52,100
from happening and then we need

00:04:49,730 --> 00:04:54,230
consistency so the consistency here is

00:04:52,100 --> 00:04:57,410
the idea that we interact with this

00:04:54,230 --> 00:04:59,360
cluster like a single machine we try not

00:04:57,410 --> 00:05:01,700
to enter back in 50 different tools that

00:04:59,360 --> 00:05:03,830
gets really confusing and you just start

00:05:01,700 --> 00:05:08,360
just hacking things together we want a

00:05:03,830 --> 00:05:10,040
consistent view of the cluster now there

00:05:08,360 --> 00:05:12,610
are some downsides there's pros and cons

00:05:10,040 --> 00:05:14,870
to distribute systems the pros are

00:05:12,610 --> 00:05:16,370
there's proven solutions for fault

00:05:14,870 --> 00:05:18,860
tolerance there's people that have

00:05:16,370 --> 00:05:20,840
researched the stuff in detail written

00:05:18,860 --> 00:05:22,280
white papers about it and now we're

00:05:20,840 --> 00:05:24,650
moving to an error where those white

00:05:22,280 --> 00:05:26,930
papers are actually bits that you can

00:05:24,650 --> 00:05:28,670
download and run in your own system

00:05:26,930 --> 00:05:30,740
right so this is we're moving past the

00:05:28,670 --> 00:05:33,170
theory phase and get into the actual

00:05:30,740 --> 00:05:34,310
implementation phase and some people

00:05:33,170 --> 00:05:38,150
have been doing this for a decade or

00:05:34,310 --> 00:05:40,010
more it's also your best shot at 99.999%

00:05:38,150 --> 00:05:41,270
uptime right some of you probably have

00:05:40,010 --> 00:05:43,250
sales reps that put that in the

00:05:41,270 --> 00:05:44,480
contracts and you look at them like you

00:05:43,250 --> 00:05:46,460
know we're going to be paying everyone

00:05:44,480 --> 00:05:48,770
refunds like half a year you know that

00:05:46,460 --> 00:05:51,020
right how many people know how much

00:05:48,770 --> 00:05:56,000
downtime you can have before you missed

00:05:51,020 --> 00:05:57,680
five nights five minutes right so if

00:05:56,000 --> 00:05:59,750
you're if you have like a bunch of

00:05:57,680 --> 00:06:01,250
system admins right and they're all on

00:05:59,750 --> 00:06:04,940
call maybe your fancy you have something

00:06:01,250 --> 00:06:06,710
like pagerduty and server goes down they

00:06:04,940 --> 00:06:08,240
get a call they wake up really quick and

00:06:06,710 --> 00:06:10,400
they fix it you're like whew four

00:06:08,240 --> 00:06:11,990
minutes 32 seconds that means you have

00:06:10,400 --> 00:06:14,990
about 28 seconds for the rest of the

00:06:11,990 --> 00:06:16,490
year before you miss five nines right so

00:06:14,990 --> 00:06:19,820
you kind of need a system that has just

00:06:16,490 --> 00:06:22,220
the stuff built into it right so the

00:06:19,820 --> 00:06:24,200
contours to increase complexity meaning

00:06:22,220 --> 00:06:26,030
you have to understand what you have

00:06:24,200 --> 00:06:28,970
right a lot of vendors say it's easy to

00:06:26,030 --> 00:06:30,380
use no it's not when it breaks is not

00:06:28,970 --> 00:06:32,030
only going to break is going to break

00:06:30,380 --> 00:06:33,800
really really fast and it's going to

00:06:32,030 --> 00:06:35,390
break everything at the same time

00:06:33,800 --> 00:06:38,060
because going to be consistent to be

00:06:35,390 --> 00:06:40,400
consistently broken across all of your

00:06:38,060 --> 00:06:41,810
data center but we build software to

00:06:40,400 --> 00:06:43,970
guard against that so we know it's going

00:06:41,810 --> 00:06:46,550
to fail listen it's going to fail

00:06:43,970 --> 00:06:47,840
guaranteed at some point maybe you won't

00:06:46,550 --> 00:06:51,200
be working there when it fails but it

00:06:47,840 --> 00:06:53,060
will there's a steep learning curve here

00:06:51,200 --> 00:06:54,920
so when I started to get into some of

00:06:53,060 --> 00:06:56,280
this stuff especially at core OS you

00:06:54,920 --> 00:06:57,900
know in our slack channel

00:06:56,280 --> 00:06:58,950
get about you know do we what kind of

00:06:57,900 --> 00:07:00,330
vector clock are we going to use for

00:06:58,950 --> 00:07:02,040
this particular implementation like

00:07:00,330 --> 00:07:03,810
Victor clock you know the Apple watch

00:07:02,040 --> 00:07:06,600
just came out is this is this is a new

00:07:03,810 --> 00:07:08,130
thing no like this stuff is really

00:07:06,600 --> 00:07:09,960
really hard but when you think about it

00:07:08,130 --> 00:07:11,790
and you break it down to its basic

00:07:09,960 --> 00:07:14,370
elements it's actually pretty pretty

00:07:11,790 --> 00:07:16,250
straightforward and then the software

00:07:14,370 --> 00:07:18,360
must be designed to handle failures a

00:07:16,250 --> 00:07:20,580
lot of people have been kicking the can

00:07:18,360 --> 00:07:22,860
down the road right you gotta databases

00:07:20,580 --> 00:07:24,419
like yeah just don't touch the database

00:07:22,860 --> 00:07:26,669
right so these are single points of

00:07:24,419 --> 00:07:27,990
failure and people are just getting lazy

00:07:26,669 --> 00:07:29,580
in some cases like you know what we're

00:07:27,990 --> 00:07:31,919
not going to fix it we have all these

00:07:29,580 --> 00:07:34,020
systems we hired they keep it running it

00:07:31,919 --> 00:07:36,600
goes down they bring it back up you know

00:07:34,020 --> 00:07:39,419
nice compliment but it's much better if

00:07:36,600 --> 00:07:41,070
you want to get to this 59 world is that

00:07:39,419 --> 00:07:42,660
you need to have your systems you know

00:07:41,070 --> 00:07:44,850
handle failures I'll give you a great

00:07:42,660 --> 00:07:46,860
example have an application developer

00:07:44,850 --> 00:07:48,840
they have an app that needs a database

00:07:46,860 --> 00:07:50,700
this is simple right you boot the

00:07:48,840 --> 00:07:53,340
application and it can't find this

00:07:50,700 --> 00:07:56,250
database what should the app to the app

00:07:53,340 --> 00:07:58,380
 seg fault I just don't crash

00:07:56,250 --> 00:07:59,700
and die you can actually just look for

00:07:58,380 --> 00:08:01,320
your database and just log that you

00:07:59,700 --> 00:08:03,570
can't find it because we could be in

00:08:01,320 --> 00:08:05,400
maintenance mode right it could be that

00:08:03,570 --> 00:08:06,510
we just started up out of order but

00:08:05,400 --> 00:08:07,919
there's no reason for you to have a

00:08:06,510 --> 00:08:09,840
heart failure because your database

00:08:07,919 --> 00:08:11,460
isn't there if you just take a few

00:08:09,840 --> 00:08:12,930
seconds to protect against this then no

00:08:11,460 --> 00:08:14,669
one needs to bring things up in order

00:08:12,930 --> 00:08:15,810
anymore just bring things up and then

00:08:14,669 --> 00:08:19,169
they just kind of figure things out on

00:08:15,810 --> 00:08:21,570
their own all right so there's building

00:08:19,169 --> 00:08:22,680
blocks to distribute systems I'm going

00:08:21,570 --> 00:08:25,440
to talk about things that are very high

00:08:22,680 --> 00:08:27,030
level there's containers and reason why

00:08:25,440 --> 00:08:29,310
i say containers here is because i'm so

00:08:27,030 --> 00:08:31,710
happy we're not talking about our pm's

00:08:29,310 --> 00:08:34,440
app star balls shell scripts fabric

00:08:31,710 --> 00:08:35,940
scripts java Ruby Python these are

00:08:34,440 --> 00:08:37,950
things that are not that important much

00:08:35,940 --> 00:08:41,219
larger scheme of things you just have an

00:08:37,950 --> 00:08:43,740
application some bits that need traffic

00:08:41,219 --> 00:08:45,810
some data that's the only thing that's

00:08:43,740 --> 00:08:48,570
important it's not exciting anymore to

00:08:45,810 --> 00:08:50,580
spin your life making packages so people

00:08:48,570 --> 00:08:53,120
still get excited about it but this is

00:08:50,580 --> 00:08:55,980
not the end game for what people do

00:08:53,120 --> 00:08:57,600
containers don't run in mythical land

00:08:55,980 --> 00:08:59,430
for some people they do they run on

00:08:57,600 --> 00:09:02,430
operating systems you still need those

00:08:59,430 --> 00:09:04,020
and then we need to network right these

00:09:02,430 --> 00:09:05,850
things need to talk to each other right

00:09:04,020 --> 00:09:08,640
so we'll talk about the network model

00:09:05,850 --> 00:09:10,230
and then we need consensus this is

00:09:08,640 --> 00:09:12,840
important right so

00:09:10,230 --> 00:09:13,800
you have a couple of services on the

00:09:12,840 --> 00:09:15,960
network and they need to share

00:09:13,800 --> 00:09:18,120
information with each other in many

00:09:15,960 --> 00:09:20,070
cases you want them all to agree on what

00:09:18,120 --> 00:09:21,450
the information is this is a lot

00:09:20,070 --> 00:09:23,460
different from everyone getting the same

00:09:21,450 --> 00:09:26,220
value at the same time this is more like

00:09:23,460 --> 00:09:28,230
if we agree at this particular moment in

00:09:26,220 --> 00:09:30,930
time that the database password is foo

00:09:28,230 --> 00:09:32,790
we should all agree that that's the

00:09:30,930 --> 00:09:34,740
password at that time what you don't

00:09:32,790 --> 00:09:36,330
want is two people to think the password

00:09:34,740 --> 00:09:39,510
is the same thing at the same time

00:09:36,330 --> 00:09:41,010
that's a problem and then a scheduler

00:09:39,510 --> 00:09:44,730
how many people are familiar with the

00:09:41,010 --> 00:09:47,010
scheduler so all you actually probably

00:09:44,730 --> 00:09:48,570
have a scheduler right you have like

00:09:47,010 --> 00:09:50,970
these guys they're like the system mins

00:09:48,570 --> 00:09:53,040
and they know where all the servers are

00:09:50,970 --> 00:09:55,170
in the infrastructure and you ask them

00:09:53,040 --> 00:09:56,850
to deploy app they usually have this

00:09:55,170 --> 00:10:00,540
database that they keep we call it a

00:09:56,850 --> 00:10:03,510
spreadsheet and they take your app they

00:10:00,540 --> 00:10:06,290
do a lookup in the database associate it

00:10:03,510 --> 00:10:09,150
to a machine and schedule that workload

00:10:06,290 --> 00:10:10,860
there they're pretty inefficient they

00:10:09,150 --> 00:10:12,630
like they want money they want time off

00:10:10,860 --> 00:10:13,980
you got to feed them but that's most

00:10:12,630 --> 00:10:16,260
people scheduler it's not really

00:10:13,980 --> 00:10:18,420
automated and it's very specialized to a

00:10:16,260 --> 00:10:20,010
small group of people so what we want to

00:10:18,420 --> 00:10:21,420
do in these schedulers for these cluster

00:10:20,010 --> 00:10:23,010
managers you want to take those people

00:10:21,420 --> 00:10:24,870
and all the knowledge that they have and

00:10:23,010 --> 00:10:26,370
we want to build it into the tool and

00:10:24,870 --> 00:10:28,260
let the tool just do these things right

00:10:26,370 --> 00:10:30,480
what app find a server put it on the

00:10:28,260 --> 00:10:32,370
server record where you put it update a

00:10:30,480 --> 00:10:34,080
database that's not a spreadsheet maybe

00:10:32,370 --> 00:10:35,610
you could if you wanted to but that's

00:10:34,080 --> 00:10:37,290
what we want in an automated scheduler

00:10:35,610 --> 00:10:41,220
right let's let the computers through

00:10:37,290 --> 00:10:43,140
the hard part a bit of level setting

00:10:41,220 --> 00:10:44,340
first what is the container right so a

00:10:43,140 --> 00:10:46,560
lot of people were introduced to

00:10:44,340 --> 00:10:48,060
containers via docker this is actually a

00:10:46,560 --> 00:10:50,840
pretty good thing is that dr. make

00:10:48,060 --> 00:10:53,130
containers very easy to use but

00:10:50,840 --> 00:10:55,590
containers just some technology and a

00:10:53,130 --> 00:10:57,780
linux kernel right so when we talk about

00:10:55,590 --> 00:10:59,640
containers in the classical sense or in

00:10:57,780 --> 00:11:00,930
the sense of a cluster manager we're

00:10:59,640 --> 00:11:03,420
really talking about application

00:11:00,930 --> 00:11:05,220
containers right application containers

00:11:03,420 --> 00:11:06,900
are different from system containers if

00:11:05,220 --> 00:11:08,730
any of you have experience with like web

00:11:06,900 --> 00:11:10,380
hosting you've probably been using

00:11:08,730 --> 00:11:12,270
containers for a very long time to give

00:11:10,380 --> 00:11:13,860
people like three dollar web hosting

00:11:12,270 --> 00:11:15,780
like five years ago how can you give

00:11:13,860 --> 00:11:17,130
people three dollar web hosting let's go

00:11:15,780 --> 00:11:18,990
you're taking a machine and you're using

00:11:17,130 --> 00:11:20,850
containers just give them an isolated

00:11:18,990 --> 00:11:23,760
view of that server so you can make

00:11:20,850 --> 00:11:25,530
really cheap machines that way

00:11:23,760 --> 00:11:26,610
in this docker community and what we're

00:11:25,530 --> 00:11:28,170
talking about with Cooper Nettie's we're

00:11:26,610 --> 00:11:29,610
talking about application containers in

00:11:28,170 --> 00:11:31,740
a difference with those is we don't

00:11:29,610 --> 00:11:34,230
expect you to log into your application

00:11:31,740 --> 00:11:36,810
via SSH we want you to kind of package

00:11:34,230 --> 00:11:39,510
things maybe one app like engine X or

00:11:36,810 --> 00:11:41,910
your PHP app in per container and they

00:11:39,510 --> 00:11:43,140
should be lightweight unfortunately live

00:11:41,910 --> 00:11:44,810
people are getting their experience

00:11:43,140 --> 00:11:47,970
building the container with like from

00:11:44,810 --> 00:11:48,960
2gig operating system and then a bunch

00:11:47,970 --> 00:11:50,850
of statements to build up your

00:11:48,960 --> 00:11:52,320
application and you end up with these

00:11:50,850 --> 00:11:54,210
two gig containers that's just way too

00:11:52,320 --> 00:11:55,680
big there's slightly better ways of

00:11:54,210 --> 00:11:58,800
doing this and we'll talk about that

00:11:55,680 --> 00:12:00,510
later in application plus its

00:11:58,800 --> 00:12:03,240
dependencies equals an image it's just a

00:12:00,510 --> 00:12:05,310
root filesystem packaging the tarball

00:12:03,240 --> 00:12:08,190
usually and a config file on how to

00:12:05,310 --> 00:12:11,010
execute that particular image and then

00:12:08,190 --> 00:12:13,290
the runtime managers like dr. rocket and

00:12:11,010 --> 00:12:14,700
there's some other ones Alexei there's

00:12:13,290 --> 00:12:16,980
your runtime environments that handle

00:12:14,700 --> 00:12:19,080
some configuration for you right see

00:12:16,980 --> 00:12:20,310
groups and namespaces it can automate

00:12:19,080 --> 00:12:21,450
creating those things for you so you

00:12:20,310 --> 00:12:26,340
don't have to specify them yourself

00:12:21,450 --> 00:12:27,450
that's where the container is and this

00:12:26,340 --> 00:12:30,240
talk we're going to talk about core OS

00:12:27,450 --> 00:12:32,810
Linux so chorus came out about a year

00:12:30,240 --> 00:12:35,160
and a half ago and it was optimized for

00:12:32,810 --> 00:12:37,200
containers from the get-go and what does

00:12:35,160 --> 00:12:39,840
that mean we basically started with

00:12:37,200 --> 00:12:42,360
Linux system d and we picked a container

00:12:39,840 --> 00:12:44,640
runtime and said that's it that's all we

00:12:42,360 --> 00:12:46,050
need to get this done and a lot of

00:12:44,640 --> 00:12:47,700
people were taken aback by this by

00:12:46,050 --> 00:12:49,770
saying wow you know I want to put my

00:12:47,700 --> 00:12:51,750
Emacs on there and my web browser is

00:12:49,770 --> 00:12:54,150
like no you can't we don't want you to

00:12:51,750 --> 00:12:56,160
do that and the analogy I use is like if

00:12:54,150 --> 00:12:58,800
you familiar with like a gaming console

00:12:56,160 --> 00:13:00,810
like a Playstation the developer that

00:12:58,800 --> 00:13:02,700
builds the games that you play on the

00:13:00,810 --> 00:13:05,250
PlayStation they don't develop it on a

00:13:02,700 --> 00:13:07,740
Playstation they develop it on a big

00:13:05,250 --> 00:13:09,570
strong dev machine and they render the

00:13:07,740 --> 00:13:11,880
video game and when they're done that

00:13:09,570 --> 00:13:14,190
disk or that downloadable image that's

00:13:11,880 --> 00:13:16,650
the artifact that you run on the

00:13:14,190 --> 00:13:18,720
PlayStation so why are we taking general

00:13:16,650 --> 00:13:20,760
purpose machines and putting them into

00:13:18,720 --> 00:13:22,980
production right we should probably have

00:13:20,760 --> 00:13:24,930
something that's like purpose-built just

00:13:22,980 --> 00:13:27,270
to run the application not develop

00:13:24,930 --> 00:13:29,630
applications so GCC on the server is

00:13:27,270 --> 00:13:31,940
probably not the best idea in the world

00:13:29,630 --> 00:13:34,770
there's no package management core OS

00:13:31,940 --> 00:13:36,660
can't install anything extra you could

00:13:34,770 --> 00:13:37,350
use docker or rocket as your package

00:13:36,660 --> 00:13:39,380
manager but that's

00:13:37,350 --> 00:13:44,820
the goal there to run your application

00:13:39,380 --> 00:13:46,080
automatic updates so in crow s the file

00:13:44,820 --> 00:13:48,390
system where all the apps are installed

00:13:46,080 --> 00:13:50,940
or read-only you can't install anything

00:13:48,390 --> 00:13:52,290
there and we don't want you to depend on

00:13:50,940 --> 00:13:54,510
anything there there's no Ruby there's

00:13:52,290 --> 00:13:56,940
no Python there's no java so what this

00:13:54,510 --> 00:13:58,770
means is that it's not our fault if you

00:13:56,940 --> 00:14:00,510
depended on something on core OS because

00:13:58,770 --> 00:14:02,340
we told you not to and since we have

00:14:00,510 --> 00:14:04,350
this contract with the user we're free

00:14:02,340 --> 00:14:05,670
to upgrade in the background so as your

00:14:04,350 --> 00:14:07,260
servers running we're periodically

00:14:05,670 --> 00:14:08,910
checking for updates very similar to the

00:14:07,260 --> 00:14:11,370
way chrome does it matter of fact almost

00:14:08,910 --> 00:14:14,220
exactly the way chrome web browser does

00:14:11,370 --> 00:14:16,440
it we use the Omaha protocol we see an

00:14:14,220 --> 00:14:18,270
update we install it to a second

00:14:16,440 --> 00:14:20,130
partition if any of you familiar with

00:14:18,270 --> 00:14:21,930
like networking gear all out of the gear

00:14:20,130 --> 00:14:23,970
does the very same thing and then we

00:14:21,930 --> 00:14:26,340
reboot into the second partition so we

00:14:23,970 --> 00:14:28,680
do this atomic update we keep all your

00:14:26,340 --> 00:14:30,420
applications and data on the readwrite

00:14:28,680 --> 00:14:36,450
partitions that's your world that's your

00:14:30,420 --> 00:14:38,340
data separated from the OS itself this

00:14:36,450 --> 00:14:41,550
is another point point Korres is rolling

00:14:38,340 --> 00:14:44,970
releases so for instance Colonel 319 is

00:14:41,550 --> 00:14:47,160
the stable Colonel right now we roll 319

00:14:44,970 --> 00:14:48,960
when for Daddo becomes stable in a

00:14:47,160 --> 00:14:51,300
couple weeks here we roll for that oh

00:14:48,960 --> 00:14:56,610
right so we keep up to date with these

00:14:51,300 --> 00:14:58,980
things all right etsy d XE d is a

00:14:56,610 --> 00:15:02,580
distributed key value store very similar

00:14:58,980 --> 00:15:05,610
to console but we shrunk it way down and

00:15:02,580 --> 00:15:06,840
all it does is key value it gives us the

00:15:05,610 --> 00:15:08,610
pem motives to do things like

00:15:06,840 --> 00:15:10,710
distributed locks that you can build

00:15:08,610 --> 00:15:12,840
leader election on top of and the goal

00:15:10,710 --> 00:15:16,860
of SCD is to store your configuration

00:15:12,840 --> 00:15:18,630
data that's it we build a lot of tools

00:15:16,860 --> 00:15:20,010
on top of this because this is a problem

00:15:18,630 --> 00:15:22,050
that you have when you have a cluster

00:15:20,010 --> 00:15:24,150
manager or something that manage network

00:15:22,050 --> 00:15:26,160
links you need to store the routing

00:15:24,150 --> 00:15:27,870
tables the state of the cluster

00:15:26,160 --> 00:15:30,450
somewhere and we happen to do that in

00:15:27,870 --> 00:15:32,220
LCD and we use at CD and we want the

00:15:30,450 --> 00:15:35,310
consensus provided the strong guarantees

00:15:32,220 --> 00:15:36,870
provided by NCD many so that we can have

00:15:35,310 --> 00:15:43,830
the whole cluster agree on what values

00:15:36,870 --> 00:15:46,470
are at a specific time flannel final is

00:15:43,830 --> 00:15:48,330
our X ed back Network fabric so we could

00:15:46,470 --> 00:15:50,820
call this sdn if you want to do some

00:15:48,330 --> 00:15:51,240
Enterprise buzzwords but what flannel

00:15:50,820 --> 00:15:53,850
does

00:15:51,240 --> 00:15:55,620
as it to automate the configuration of

00:15:53,850 --> 00:15:58,020
docker containers across multiple hosts

00:15:55,620 --> 00:15:59,580
so they can talk to each other this is

00:15:58,020 --> 00:16:02,220
the networking model required by Cooper

00:15:59,580 --> 00:16:04,680
Nettie's everything on a cluster the

00:16:02,220 --> 00:16:06,540
host and the container are actually

00:16:04,680 --> 00:16:09,330
first-class citizens they all have

00:16:06,540 --> 00:16:11,130
addressable IP addresses and the

00:16:09,330 --> 00:16:12,480
benefits of doing that is that we don't

00:16:11,130 --> 00:16:14,339
have to treat servers different from the

00:16:12,480 --> 00:16:15,930
application it's really great when you

00:16:14,339 --> 00:16:18,649
start doing application tracing to know

00:16:15,930 --> 00:16:21,600
exactly what app is producing that data

00:16:18,649 --> 00:16:22,950
there's optional overlay networking so

00:16:21,600 --> 00:16:25,170
if you're in a bare-metal data center

00:16:22,950 --> 00:16:27,990
you may not want like the UDP overlay

00:16:25,170 --> 00:16:29,550
right why would you want to do a lot of

00:16:27,990 --> 00:16:31,020
networking stuff in user space that's

00:16:29,550 --> 00:16:32,610
way too slow you're running a data

00:16:31,020 --> 00:16:35,040
center for a reason you want performance

00:16:32,610 --> 00:16:36,630
so in those cases final supports

00:16:35,040 --> 00:16:38,850
multiple backends we can go on

00:16:36,630 --> 00:16:41,279
allocation mode and host gateway and in

00:16:38,850 --> 00:16:43,350
those modes we just configure the subnet

00:16:41,279 --> 00:16:46,200
so you give us a / 8 we chop that up

00:16:43,350 --> 00:16:48,690
into smaller / 24 so that we can assign

00:16:46,200 --> 00:16:50,670
to the host and allocate ip's two

00:16:48,690 --> 00:16:51,810
containers from then on out and that

00:16:50,670 --> 00:16:53,040
allows you just to integrate with your

00:16:51,810 --> 00:16:55,170
standard gear so if you have some

00:16:53,040 --> 00:16:58,440
juniper gear some cisco gear you can

00:16:55,170 --> 00:17:00,029
have you can just do some normal layer 3

00:16:58,440 --> 00:17:02,339
routing to get traffic to flow between

00:17:00,029 --> 00:17:04,589
the cluster with no encapsulation on top

00:17:02,339 --> 00:17:05,640
if unfortunately you're in like a cloud

00:17:04,589 --> 00:17:07,530
environment where you can't necessarily

00:17:05,640 --> 00:17:09,689
deal with the l2 layer to networking

00:17:07,530 --> 00:17:12,630
stuff and we do have some overlay stuff

00:17:09,689 --> 00:17:14,400
you can go UDP or VX plan to kind of

00:17:12,630 --> 00:17:15,929
give you this way of like in a public

00:17:14,400 --> 00:17:19,800
cloud building your own private network

00:17:15,929 --> 00:17:23,030
on top and then we'll talk about fleet

00:17:19,800 --> 00:17:25,079
so fleet is a highly available

00:17:23,030 --> 00:17:27,839
distributed in its system based on

00:17:25,079 --> 00:17:30,540
system d right we talked to the single

00:17:27,839 --> 00:17:33,059
endpoint and it schedules units on a

00:17:30,540 --> 00:17:35,840
machine so without fleet you would log

00:17:33,059 --> 00:17:38,400
into a server yum apt-get install apache

00:17:35,840 --> 00:17:39,750
maybe make a service file for it if

00:17:38,400 --> 00:17:42,780
you're using system d put it in the

00:17:39,750 --> 00:17:45,690
right place and do service CTL apache

00:17:42,780 --> 00:17:47,340
start or the other way around and fleet

00:17:45,690 --> 00:17:49,559
just says forget all that just talk to

00:17:47,340 --> 00:17:50,970
fleet it'll pick a server for you or you

00:17:49,559 --> 00:17:53,130
can be specific and say I want to server

00:17:50,970 --> 00:17:55,230
with this ID to run this particular unit

00:17:53,130 --> 00:17:57,210
and fleet basically lets you do this at

00:17:55,230 --> 00:17:59,669
a cluster level versus the command line

00:17:57,210 --> 00:18:02,540
and it also serves as the machine

00:17:59,669 --> 00:18:02,540
database for core OS

00:18:03,600 --> 00:18:10,920
so let's look at fleet in action so like

00:18:07,480 --> 00:18:10,920
again I provision a bunch of machines

00:18:12,270 --> 00:18:16,240
alright so we have four machines on the

00:18:14,470 --> 00:18:18,250
network those are their IP addresses and

00:18:16,240 --> 00:18:20,080
they have some metadata to just call

00:18:18,250 --> 00:18:22,660
Cooper Nettie's what you don't see in

00:18:20,080 --> 00:18:25,750
this output is the NCD server at CD is a

00:18:22,660 --> 00:18:27,460
special snowflake it is the it is the

00:18:25,750 --> 00:18:29,620
source of true for the entire cluster

00:18:27,460 --> 00:18:31,510
don't schedule no more workloads on it

00:18:29,620 --> 00:18:35,230
so I haven't run on different vm that's

00:18:31,510 --> 00:18:37,120
not part of the work of pull baidu dash

00:18:35,230 --> 00:18:39,160
l you'll see that the machines these are

00:18:37,120 --> 00:18:41,530
the machine IDs that when a machine boot

00:18:39,160 --> 00:18:46,270
systemd assigns it a unique ID and we

00:18:41,530 --> 00:18:48,040
can adjust machines that way now we want

00:18:46,270 --> 00:18:50,680
to stand up a more advanced scheduler so

00:18:48,040 --> 00:18:52,810
we're going to do is use fleet to deploy

00:18:50,680 --> 00:18:53,830
parts of Cooper Nettie's so the first

00:18:52,810 --> 00:18:55,300
thing we need to do is set up some

00:18:53,830 --> 00:18:57,070
networking between this machine so we

00:18:55,300 --> 00:18:58,660
have these four machines and if we were

00:18:57,070 --> 00:18:59,710
to deploy containers now on them they

00:18:58,660 --> 00:19:01,630
won't be able to talk to each other

00:18:59,710 --> 00:19:03,610
across those this is mainly because of

00:19:01,630 --> 00:19:05,890
the default knock dr. networking model

00:19:03,610 --> 00:19:07,870
there's a bridge virtual interfaces and

00:19:05,890 --> 00:19:09,700
a random IP that isn't rattleball in the

00:19:07,870 --> 00:19:13,000
network so we need to solve that problem

00:19:09,700 --> 00:19:14,920
so what we can do is use unit files so

00:19:13,000 --> 00:19:16,300
one unit file we have is the final unit

00:19:14,920 --> 00:19:18,190
file this looks just like a normal

00:19:16,300 --> 00:19:20,080
system d unit file what we're seeing

00:19:18,190 --> 00:19:23,170
here is that we're going to use etsy d

00:19:20,080 --> 00:19:25,270
to store the configuration of final

00:19:23,170 --> 00:19:27,040
meaning here's the subnet that I want

00:19:25,270 --> 00:19:28,840
here's the starting in range of that

00:19:27,040 --> 00:19:31,650
subnet and I want to use the host

00:19:28,840 --> 00:19:34,120
gateway network configuration meaning

00:19:31,650 --> 00:19:36,400
flannel will just update the local

00:19:34,120 --> 00:19:37,840
routing table on each Linux system so

00:19:36,400 --> 00:19:40,210
that way we know where the next hop is

00:19:37,840 --> 00:19:42,820
for range of containers right this is

00:19:40,210 --> 00:19:44,650
what flannel will do for us and then

00:19:42,820 --> 00:19:46,990
down there we have the execute start and

00:19:44,650 --> 00:19:48,850
basically saying hey final startup you

00:19:46,990 --> 00:19:50,620
handle the iptables entries and no

00:19:48,850 --> 00:19:53,110
there's where etsy d is on the network

00:19:50,620 --> 00:19:54,700
and then we're here's some fleet

00:19:53,110 --> 00:19:56,560
metadata so what we want to do here is

00:19:54,700 --> 00:19:58,750
like we need to run on every single

00:19:56,560 --> 00:19:59,980
server global equals true so we don't

00:19:58,750 --> 00:20:02,380
need to keep track of our machines that

00:19:59,980 --> 00:20:04,180
those 10,000 machines that means all of

00:20:02,380 --> 00:20:05,980
the machines on the cluster that match

00:20:04,180 --> 00:20:08,740
that metadata so everyone that is roll

00:20:05,980 --> 00:20:12,040
Cooper Nettie's will get final installed

00:20:08,740 --> 00:20:16,060
so let's install it so we'll do fleet

00:20:12,040 --> 00:20:17,049
start units flannel so that's it and now

00:20:16,060 --> 00:20:18,879
every machine on the

00:20:17,049 --> 00:20:25,600
cluster should be running final and we

00:20:18,879 --> 00:20:28,539
can verify that list units all right so

00:20:25,600 --> 00:20:30,669
now every machine is running final is up

00:20:28,539 --> 00:20:31,989
and running so now we've done a bit of

00:20:30,669 --> 00:20:33,820
work they should have all checked into

00:20:31,989 --> 00:20:35,889
NCD figured out what their sudden that

00:20:33,820 --> 00:20:38,590
should be and configure themselves and

00:20:35,889 --> 00:20:41,950
we can validate that really quick SADC

00:20:38,590 --> 00:20:47,799
tlls and we do a recursive look up on

00:20:41,950 --> 00:20:49,690
that all right so let's just do this

00:20:47,799 --> 00:20:51,850
really quick so each of those servers

00:20:49,690 --> 00:20:53,799
came up independently found ltd figured

00:20:51,850 --> 00:20:56,289
out what their subnet was wrote it to

00:20:53,799 --> 00:20:57,759
etsy d and this becomes a route entry

00:20:56,289 --> 00:21:00,609
for all the other servers right so we

00:20:57,759 --> 00:21:02,919
can do NCD get and we can get one of

00:21:00,609 --> 00:21:04,779
these entries so each server on its own

00:21:02,919 --> 00:21:07,149
independently writes this value internet

00:21:04,779 --> 00:21:09,249
CD and what happens is all the other

00:21:07,149 --> 00:21:11,649
final listeners are watching for this

00:21:09,249 --> 00:21:13,299
namespace for any changes the new server

00:21:11,649 --> 00:21:14,950
shows up and needs to update as local

00:21:13,299 --> 00:21:18,100
route table so it knows how to route to

00:21:14,950 --> 00:21:20,169
those containers right now the next

00:21:18,100 --> 00:21:21,700
thing we need is something to execute

00:21:20,169 --> 00:21:23,590
our containers this could be rocket this

00:21:21,700 --> 00:21:25,659
could be docker this could be lattice

00:21:23,590 --> 00:21:27,220
from Cloud Foundry could be anything lxc

00:21:25,659 --> 00:21:30,480
we're going to use docker in this case

00:21:27,220 --> 00:21:32,889
so very same thing i have a dr unit file

00:21:30,480 --> 00:21:34,210
and here's the doctor the unifi looks

00:21:32,889 --> 00:21:36,909
like i'm going to run doc in a daemon

00:21:34,210 --> 00:21:38,739
mode i'm going to use an overlay storage

00:21:36,909 --> 00:21:40,119
driver i'm going to listen on the local

00:21:38,739 --> 00:21:42,730
socket because i don't want to actually

00:21:40,119 --> 00:21:44,619
deal with soccer over the remote api

00:21:42,730 --> 00:21:47,109
there's a lot of people know right now

00:21:44,619 --> 00:21:48,940
they have their doctor remote api just

00:21:47,109 --> 00:21:50,470
open on the public internet that is like

00:21:48,940 --> 00:21:53,100
so much fun you just like deploy your

00:21:50,470 --> 00:21:55,090
Bitcoin miners to their servers for free

00:21:53,100 --> 00:21:57,429
so thank you guys for doing that if

00:21:55,090 --> 00:21:59,379
you're part of that crowd thank you and

00:21:57,429 --> 00:22:01,029
then we have a sick we have a registry

00:21:59,379 --> 00:22:03,039
actually writing on my laptop right so

00:22:01,029 --> 00:22:05,440
now I have a go implementation of the

00:22:03,039 --> 00:22:07,119
Dockers new registry for dr. 16 that

00:22:05,440 --> 00:22:10,210
just came out so I have it run it as a

00:22:07,119 --> 00:22:12,039
service on my Mac so I'm going to allow

00:22:10,210 --> 00:22:13,299
insecure access to that so it's like

00:22:12,039 --> 00:22:14,470
behind the firewall in the data center

00:22:13,299 --> 00:22:16,359
you'll notice that I don't have any

00:22:14,470 --> 00:22:19,059
internet access on my laptop on purpose

00:22:16,359 --> 00:22:20,710
and then we have this thing here where

00:22:19,059 --> 00:22:22,570
we say we want the final subnet so we're

00:22:20,710 --> 00:22:23,889
going to have flannel tell us what our

00:22:22,570 --> 00:22:26,019
sudden that range is because the way

00:22:23,889 --> 00:22:28,990
doctor works every time I machine joined

00:22:26,019 --> 00:22:30,610
or dr. spins one up it gives it an IP

00:22:28,990 --> 00:22:32,200
address right

00:22:30,610 --> 00:22:33,730
then we're going to set our empty you to

00:22:32,200 --> 00:22:36,400
kind of line that up and we need to also

00:22:33,730 --> 00:22:38,410
run on every machine so let's I'll

00:22:36,400 --> 00:22:42,700
deploy that so we'll do fleet start

00:22:38,410 --> 00:22:45,549
units docker right so at this point we

00:22:42,700 --> 00:22:48,100
now have fleet and docker running on

00:22:45,549 --> 00:22:49,630
every machine in the cluster now the

00:22:48,100 --> 00:22:52,000
nice thing about Global's equals true we

00:22:49,630 --> 00:22:54,130
add 10 more machines later fleets going

00:22:52,000 --> 00:22:55,360
to automatically just deploy all the

00:22:54,130 --> 00:22:57,250
global units we don't have to take any

00:22:55,360 --> 00:22:59,140
more action this is like declaring all

00:22:57,250 --> 00:23:03,040
of them even though we don't know what

00:22:59,140 --> 00:23:04,780
all of them are right now all right so

00:23:03,040 --> 00:23:07,450
now we kind of got the foundation in

00:23:04,780 --> 00:23:09,070
place now let's talk up so that fleets

00:23:07,450 --> 00:23:11,500
really good at that they can schedule

00:23:09,070 --> 00:23:13,690
you know low-level things and replace

00:23:11,500 --> 00:23:14,919
systemd at a cluster level but doesn't

00:23:13,690 --> 00:23:16,390
have all the smarts that are really

00:23:14,919 --> 00:23:18,549
advanced scheduler has right it doesn't

00:23:16,390 --> 00:23:20,230
have there's not a lot of dimensions

00:23:18,549 --> 00:23:22,630
into the way it schedules doesn't take

00:23:20,230 --> 00:23:25,750
CPU performs into consideration memory

00:23:22,630 --> 00:23:28,360
usage into consideration that's where

00:23:25,750 --> 00:23:29,530
Cooper Nettie's comes in so Cooper

00:23:28,360 --> 00:23:31,510
nannies and all these things are open

00:23:29,530 --> 00:23:34,090
source project this is an open source

00:23:31,510 --> 00:23:36,610
project from google this project is

00:23:34,090 --> 00:23:39,400
based on lessons learned from operating

00:23:36,610 --> 00:23:41,140
borg at scale at Google for the last

00:23:39,400 --> 00:23:44,080
decade they just produce a white paper

00:23:41,140 --> 00:23:45,370
on borg and how it works and cooper

00:23:44,080 --> 00:23:47,799
neighs is a basically an open-source

00:23:45,370 --> 00:23:49,570
implementation of of this global

00:23:47,799 --> 00:23:51,309
scheduler that sits inside the data

00:23:49,570 --> 00:23:53,290
center a lot of people refer to this as

00:23:51,309 --> 00:23:57,520
a data center colonel some people made

00:23:53,290 --> 00:23:59,169
to describe these as like a data center

00:23:57,520 --> 00:24:01,000
operating system i kind of like the

00:23:59,169 --> 00:24:03,429
colonel analogy mainly because you still

00:24:01,000 --> 00:24:04,480
need user land around this kernel write

00:24:03,429 --> 00:24:06,370
this thing knows how to schedule

00:24:04,480 --> 00:24:08,110
resources two things give you views on

00:24:06,370 --> 00:24:09,549
these things but you still need to build

00:24:08,110 --> 00:24:10,780
some tolling to integrate into the rest

00:24:09,549 --> 00:24:12,370
of your stack right if you have some

00:24:10,780 --> 00:24:15,700
real networking gear you're gonna have

00:24:12,370 --> 00:24:18,340
to plumb those things together so it's a

00:24:15,700 --> 00:24:20,770
container manager scheduler and service

00:24:18,340 --> 00:24:22,179
discovery that part is important because

00:24:20,770 --> 00:24:24,190
since service discovery is built into

00:24:22,179 --> 00:24:26,020
the cluster manager you don't have to

00:24:24,190 --> 00:24:27,669
build any sidekick toolings when you

00:24:26,020 --> 00:24:29,950
start to point containers to tell them

00:24:27,669 --> 00:24:31,750
you know register them somewhere kuber

00:24:29,950 --> 00:24:34,210
navies handles that it started the

00:24:31,750 --> 00:24:35,980
container it knows where it is and it

00:24:34,210 --> 00:24:38,800
does a really intelligent service

00:24:35,980 --> 00:24:40,690
discovery mechanisms by using labels so

00:24:38,800 --> 00:24:42,880
labels mean you don't have to commit to

00:24:40,690 --> 00:24:44,440
the name of this service right now so

00:24:42,880 --> 00:24:46,660
you may deploy a service and say hey

00:24:44,440 --> 00:24:49,000
is a web service it serves credit card

00:24:46,660 --> 00:24:51,430
data and then you'll be in jail but that

00:24:49,000 --> 00:24:53,170
could be one of your services what curb

00:24:51,430 --> 00:24:54,580
Renee's does says let's not hard code

00:24:53,170 --> 00:24:55,960
things like that let's just give it a

00:24:54,580 --> 00:24:58,240
set of labels that we can add and remove

00:24:55,960 --> 00:25:00,640
later and then we can just classify a

00:24:58,240 --> 00:25:02,080
service by a collection of labels this

00:25:00,640 --> 00:25:03,850
is really really powerful when you start

00:25:02,080 --> 00:25:05,500
talking about service discovery on the

00:25:03,850 --> 00:25:08,080
fly we can adjust the service discovery

00:25:05,500 --> 00:25:10,510
based on just labels we can find things

00:25:08,080 --> 00:25:14,260
based on labels and we'll see this in a

00:25:10,510 --> 00:25:17,320
demo everything is apt I driven there is

00:25:14,260 --> 00:25:20,410
no language it's strictly Jason that you

00:25:17,320 --> 00:25:22,510
say what your intent is urban edge works

00:25:20,410 --> 00:25:24,850
it out if there's something Cabernets

00:25:22,510 --> 00:25:26,380
can't do then we need to add that logic

00:25:24,850 --> 00:25:28,090
into the components that make up Cooper

00:25:26,380 --> 00:25:30,520
Nettie's we're not going to give you the

00:25:28,090 --> 00:25:33,280
power saying for this in this if this do

00:25:30,520 --> 00:25:34,960
this none of that purely declarative you

00:25:33,280 --> 00:25:37,600
say what your intent is we enforce that

00:25:34,960 --> 00:25:39,220
intent there's agents that monitor for

00:25:37,600 --> 00:25:41,410
state changes on all these machines

00:25:39,220 --> 00:25:43,360
there'll be agents that watch the API

00:25:41,410 --> 00:25:44,680
server to figure out what should I be

00:25:43,360 --> 00:25:46,780
doing what applications should I be

00:25:44,680 --> 00:25:51,310
running is there any IPS that I need to

00:25:46,780 --> 00:25:53,560
proxy for that sort of thing at a high

00:25:51,310 --> 00:25:56,140
level urban areas has these four main

00:25:53,560 --> 00:25:57,820
resources there's nodes the systems that

00:25:56,140 --> 00:26:01,120
provide resources that we can schedule

00:25:57,820 --> 00:26:03,820
too there's pods and a pod is a

00:26:01,120 --> 00:26:05,290
collection of containers so if any of

00:26:03,820 --> 00:26:06,850
you dealt with docker before you kind of

00:26:05,290 --> 00:26:08,890
specify hey here's a docker container

00:26:06,850 --> 00:26:11,170
that doesn't work but we all know that

00:26:08,890 --> 00:26:13,630
an application is usually composed of

00:26:11,170 --> 00:26:16,570
multiple containers you may need engine

00:26:13,630 --> 00:26:18,520
X your Ruby application and maybe this

00:26:16,570 --> 00:26:20,410
thing that does migrations by looking at

00:26:18,520 --> 00:26:22,510
the file system or sharing code with a

00:26:20,410 --> 00:26:24,580
ruby app so what we need to do is deploy

00:26:22,510 --> 00:26:26,680
those things as a collection as a single

00:26:24,580 --> 00:26:28,960
unit and that's what a pod represents is

00:26:26,680 --> 00:26:31,480
a single unit of one or more containers

00:26:28,960 --> 00:26:33,340
we can take that single unit and we can

00:26:31,480 --> 00:26:35,680
give it to the scheduler ensure that

00:26:33,340 --> 00:26:37,330
their clothes scheduled to the system

00:26:35,680 --> 00:26:39,450
together versus breaking them down and

00:26:37,330 --> 00:26:41,590
then having to figure out where they are

00:26:39,450 --> 00:26:43,300
replication controllers are interesting

00:26:41,590 --> 00:26:45,610
they're like the auto scalars for your

00:26:43,300 --> 00:26:47,590
application these things take a pot

00:26:45,610 --> 00:26:50,050
template you describe what this

00:26:47,590 --> 00:26:52,300
collection of services look like and it

00:26:50,050 --> 00:26:54,490
keeps track of how many you want you

00:26:52,300 --> 00:26:56,260
want two of them its job is to keep

00:26:54,490 --> 00:26:57,910
track of those by looking at labels of

00:26:56,260 --> 00:27:00,160
pods on the network there

00:26:57,910 --> 00:27:02,440
too few it adds more there's too many

00:27:00,160 --> 00:27:04,240
and reduces some that way of someone

00:27:02,440 --> 00:27:06,340
spends something up out of bound by

00:27:04,240 --> 00:27:07,690
talking to the API directly the

00:27:06,340 --> 00:27:09,790
replication controller will notice this

00:27:07,690 --> 00:27:12,490
and kill that thing to keep you at the

00:27:09,790 --> 00:27:14,680
desired state and in their services

00:27:12,490 --> 00:27:17,530
services is the high level primitive for

00:27:14,680 --> 00:27:20,170
service discovery so what you do is you

00:27:17,530 --> 00:27:22,630
basically assign a VIP it gets an IP

00:27:20,170 --> 00:27:24,700
that all the services can use you can

00:27:22,630 --> 00:27:26,350
you also use DNS but in this case I want

00:27:24,700 --> 00:27:29,110
to talk about an IP address by default

00:27:26,350 --> 00:27:31,690
and this IP addresses associated with

00:27:29,110 --> 00:27:34,210
the lookup and the API service so if a

00:27:31,690 --> 00:27:36,190
node hits an IP Cooper nays will do a

00:27:34,210 --> 00:27:39,040
lookup to find all the positive match a

00:27:36,190 --> 00:27:41,440
set of labels get those IPS import and

00:27:39,040 --> 00:27:42,870
then proxy the traffic for you so then

00:27:41,440 --> 00:27:45,010
you can freely move things around

00:27:42,870 --> 00:27:48,970
without trying to orchestrate the whole

00:27:45,010 --> 00:27:51,730
world hears what a pod definitional

00:27:48,970 --> 00:27:53,950
looks like it's declarative syntax just

00:27:51,730 --> 00:27:56,650
Jason this is a pot object I'm going to

00:27:53,950 --> 00:27:58,420
use the v1 beta 3 API I have some

00:27:56,650 --> 00:28:01,000
metadata the name of the app is this

00:27:58,420 --> 00:28:03,850
awesome app that I they'll probably some

00:28:01,000 --> 00:28:05,980
labels like it's in production here's

00:28:03,850 --> 00:28:08,830
the spec so this thing doesn't have any

00:28:05,980 --> 00:28:12,340
volumes and it is composed of only one

00:28:08,830 --> 00:28:14,650
container and the image name conforms to

00:28:12,340 --> 00:28:16,360
the doctor spec where you put the

00:28:14,650 --> 00:28:18,400
registry and the namespace of that

00:28:16,360 --> 00:28:20,200
container and then we specify some port

00:28:18,400 --> 00:28:21,640
and an image policy and there's some

00:28:20,200 --> 00:28:23,980
other things you can do around policy

00:28:21,640 --> 00:28:25,840
now every user in the cluster can get

00:28:23,980 --> 00:28:27,520
their own namespace so you want to say

00:28:25,840 --> 00:28:29,200
hey here's your view of the world maybe

00:28:27,520 --> 00:28:31,690
that namespace is testing another

00:28:29,200 --> 00:28:33,490
namespace is a QA or it could just be

00:28:31,690 --> 00:28:35,680
namespace based on department whatever

00:28:33,490 --> 00:28:39,640
you want to do these objects landed in

00:28:35,680 --> 00:28:41,500
namespace default by default and Cooper

00:28:39,640 --> 00:28:46,420
nays will enforce that this pot lives

00:28:41,500 --> 00:28:48,130
somewhere all right here's the

00:28:46,420 --> 00:28:50,320
components that make up Cooper Nettie's

00:28:48,130 --> 00:28:53,830
it needs etsy d this is where it stores

00:28:50,320 --> 00:28:55,990
its cluster state and he's an API server

00:28:53,830 --> 00:28:58,030
everything goes through the API server

00:28:55,990 --> 00:28:59,980
when you make changes to the cluster you

00:28:58,030 --> 00:29:02,080
go through the API server all the other

00:28:59,980 --> 00:29:03,550
components they watch the API server

00:29:02,080 --> 00:29:05,950
they don't know anything about NCD of

00:29:03,550 --> 00:29:07,240
the back end then we have these

00:29:05,950 --> 00:29:08,740
replication controllers these are

00:29:07,240 --> 00:29:10,660
standalone binaries that moment in the

00:29:08,740 --> 00:29:11,740
cluster and their job is to talk to the

00:29:10,660 --> 00:29:12,850
API server

00:29:11,740 --> 00:29:15,280
figure out is there anything it needs to

00:29:12,850 --> 00:29:16,750
do then we have a scheduler so the

00:29:15,280 --> 00:29:18,880
schedule is a global thing that when you

00:29:16,750 --> 00:29:21,490
submit work pods or replication

00:29:18,880 --> 00:29:24,270
controllers its job is to find the best

00:29:21,490 --> 00:29:27,250
fit in the cluster to run this thing and

00:29:24,270 --> 00:29:30,010
then we have a proxy the process unique

00:29:27,250 --> 00:29:32,380
it runs on every host and its job is to

00:29:30,010 --> 00:29:34,300
help with service discovery when you use

00:29:32,380 --> 00:29:36,670
one of those virtual IPS they're not

00:29:34,300 --> 00:29:38,290
actually a tie to a Linux Atlantic's

00:29:36,670 --> 00:29:40,690
interface or anything it's actually

00:29:38,290 --> 00:29:43,150
iptables rule so when we see a

00:29:40,690 --> 00:29:45,280
destination IP that matches that we

00:29:43,150 --> 00:29:47,500
actually proxy it to the proxy server

00:29:45,280 --> 00:29:49,900
running it does a dynamic query to

00:29:47,500 --> 00:29:52,780
Cooper Nettie's to see what labels match

00:29:49,900 --> 00:29:55,030
that IP and then starts to do proxy of

00:29:52,780 --> 00:29:57,010
the eyepiece and then we have the

00:29:55,030 --> 00:29:58,630
couplet itself this is the thing that

00:29:57,010 --> 00:30:01,210
runs on each server each worker node

00:29:58,630 --> 00:30:02,770
that knows how to talk to dr. and do the

00:30:01,210 --> 00:30:04,690
things that's been told by the scheduler

00:30:02,770 --> 00:30:07,270
right so everyone has their role in life

00:30:04,690 --> 00:30:09,490
you can pull one of these things out and

00:30:07,270 --> 00:30:10,929
roll your own so I've written little

00:30:09,490 --> 00:30:13,300
miniature schedulers and miniature

00:30:10,929 --> 00:30:14,980
controllers that do things like hey do

00:30:13,300 --> 00:30:16,780
not even allow work to be scheduled

00:30:14,980 --> 00:30:18,340
until someone closes this ticket in JIRA

00:30:16,780 --> 00:30:19,600
right you do whatever you want with

00:30:18,340 --> 00:30:20,980
these things they're all pluggable just

00:30:19,600 --> 00:30:24,910
pull them out all you have to do is talk

00:30:20,980 --> 00:30:26,440
to the API server alright so let's

00:30:24,910 --> 00:30:28,510
provision Cooper Nettie so right now we

00:30:26,440 --> 00:30:30,760
have fleet running we have final cross

00:30:28,510 --> 00:30:32,920
container networking going on and we

00:30:30,760 --> 00:30:36,250
have dr. in place let's spend up Cooper

00:30:32,920 --> 00:30:38,679
Nettie's all right so we're going to use

00:30:36,250 --> 00:30:41,200
fleet to do this again what we want to

00:30:38,679 --> 00:30:43,690
do first is I'm going to pin NCD to a

00:30:41,200 --> 00:30:46,630
specific machine mainly because it has

00:30:43,690 --> 00:30:49,630
persistent data right so what I'm going

00:30:46,630 --> 00:30:50,650
to do now is list all the machines now

00:30:49,630 --> 00:30:51,670
there's a different way we actually do

00:30:50,650 --> 00:30:53,830
this in production we don't pick a

00:30:51,670 --> 00:30:56,230
specific machine we just let it schedule

00:30:53,830 --> 00:30:58,150
anywhere and we have the other dependent

00:30:56,230 --> 00:30:59,679
services following but i'm going to pick

00:30:58,150 --> 00:31:01,510
a specific machine to show you that you

00:30:59,679 --> 00:31:03,640
can actually do this so we'll pick the

00:31:01,510 --> 00:31:05,590
one with the oddest IP address 20 will

00:31:03,640 --> 00:31:07,150
take us machine ID what we're going to

00:31:05,590 --> 00:31:10,570
do is we're going to look at the cooper

00:31:07,150 --> 00:31:14,200
Nettie's etsy d and we're going to sign

00:31:10,570 --> 00:31:15,730
it to a specific machine so when the

00:31:14,200 --> 00:31:16,809
fleet scheduler looks like this is going

00:31:15,730 --> 00:31:19,090
to say all right what rules are here

00:31:16,809 --> 00:31:21,550
needs to be a cobra navies node and it

00:31:19,090 --> 00:31:25,549
needs to match this machine ID and this

00:31:21,550 --> 00:31:29,600
is just say SED system d unit file I

00:31:25,549 --> 00:31:33,539
so what we'll do now is a fleet start

00:31:29,600 --> 00:31:34,350
the Cooper Nettie's at CD server so it's

00:31:33,539 --> 00:31:35,879
going through is making this

00:31:34,350 --> 00:31:38,669
determination and I know it's going to

00:31:35,879 --> 00:31:42,139
land on that 20 because it has to write

00:31:38,669 --> 00:31:45,119
I've told it to so it's there now and

00:31:42,139 --> 00:31:47,730
then we can look at our units and we'll

00:31:45,119 --> 00:31:50,059
see that HCD is now running on this

00:31:47,730 --> 00:31:53,989
machine and one nice thing about fleet

00:31:50,059 --> 00:31:57,570
you can do things like fleet journal

00:31:53,989 --> 00:32:00,239
khoob at CD and we can look at the logs

00:31:57,570 --> 00:32:01,950
real time on the server right you don't

00:32:00,239 --> 00:32:03,840
have to log into the server you know we

00:32:01,950 --> 00:32:06,090
can have fleet proxy the journal on all

00:32:03,840 --> 00:32:07,470
our machines for so now we know that SED

00:32:06,090 --> 00:32:09,659
actually starting is actually running

00:32:07,470 --> 00:32:13,379
the next thing we want to do is deploy

00:32:09,659 --> 00:32:16,109
the other components now we'll start the

00:32:13,379 --> 00:32:17,909
API server but here i just want fleet to

00:32:16,109 --> 00:32:20,399
figure out where ever coupe su deal and

00:32:17,909 --> 00:32:24,179
it just put this there right wherever

00:32:20,399 --> 00:32:25,799
that is right if SED were to move fleet

00:32:24,179 --> 00:32:27,989
will also d schedule these things and

00:32:25,799 --> 00:32:31,039
move them to wherever NCD lips so we're

00:32:27,989 --> 00:32:36,929
just going to spend these up with fleet

00:32:31,039 --> 00:32:39,119
so fleet start units coo API server and

00:32:36,929 --> 00:32:43,230
i know it's going to go in that 20

00:32:39,119 --> 00:32:44,580
because that's where SED is all right

00:32:43,230 --> 00:32:45,869
and then we want to do the same thing we

00:32:44,580 --> 00:32:48,269
need the controller manager what is

00:32:45,869 --> 00:32:49,679
another binary on the network and its

00:32:48,269 --> 00:32:51,090
job is to handle those replication

00:32:49,679 --> 00:32:52,980
controller objects that we create until

00:32:51,090 --> 00:32:54,690
the cluster that should go to the same

00:32:52,980 --> 00:32:57,960
place because it relies on it and that

00:32:54,690 --> 00:32:59,940
one had a dependency on the API server

00:32:57,960 --> 00:33:04,649
and the next thing we need to do is

00:32:59,940 --> 00:33:06,389
deploy our scheduler so we'll deploy our

00:33:04,649 --> 00:33:09,210
scheduler the same thing fleets donuts

00:33:06,389 --> 00:33:11,759
work all right so now we have our

00:33:09,210 --> 00:33:14,399
scheduler replication controller the API

00:33:11,759 --> 00:33:17,369
server now the final visit we need to do

00:33:14,399 --> 00:33:19,109
is get the proxy on every machine and we

00:33:17,369 --> 00:33:20,879
need to get the couplet on every machine

00:33:19,109 --> 00:33:23,489
so we have something back and talk to

00:33:20,879 --> 00:33:25,889
dr. and these are very much the same as

00:33:23,489 --> 00:33:30,149
the global units we saw before so we'll

00:33:25,889 --> 00:33:32,999
just start those start units and we want

00:33:30,149 --> 00:33:36,119
to do the proxy so the coup proxy so we

00:33:32,999 --> 00:33:39,030
start that every machine and then if you

00:33:36,119 --> 00:33:41,100
look at this list

00:33:39,030 --> 00:33:42,270
it's all right they're all there so we

00:33:41,100 --> 00:33:43,890
have a bunch of stuff now running the

00:33:42,270 --> 00:33:49,170
cluster and a nice thing about this

00:33:43,890 --> 00:33:52,290
setup is that all the ones that say

00:33:49,170 --> 00:33:58,770
global the proxy service let's go ahead

00:33:52,290 --> 00:34:02,220
and add one more fleet start units coo

00:33:58,770 --> 00:34:03,900
couplet all right so thats everywhere so

00:34:02,220 --> 00:34:05,460
now when we list files all the ones that

00:34:03,900 --> 00:34:06,810
are global we can just keep adding

00:34:05,460 --> 00:34:08,220
servers to this cluster and they're

00:34:06,810 --> 00:34:09,480
going to always get the global one we

00:34:08,220 --> 00:34:11,310
don't have to go back and update this

00:34:09,480 --> 00:34:14,429
because new servers showed up this is

00:34:11,310 --> 00:34:17,970
really powerful alright so now that we

00:34:14,429 --> 00:34:19,470
have all the components in place we can

00:34:17,970 --> 00:34:22,770
actually start doing things with Cooper

00:34:19,470 --> 00:34:24,870
Nettie's so I can use a coop CTL command

00:34:22,770 --> 00:34:27,770
to do things like figure out how many

00:34:24,870 --> 00:34:29,970
pods are running there's none by default

00:34:27,770 --> 00:34:33,480
Kerber neighs puts a few services in

00:34:29,970 --> 00:34:35,159
place so we know where the api server is

00:34:33,480 --> 00:34:36,720
so when we deployed things into Cooper

00:34:35,159 --> 00:34:38,490
natives we can use environment variables

00:34:36,720 --> 00:34:40,610
that will be injected with all the

00:34:38,490 --> 00:34:42,510
services in your particular namespace

00:34:40,610 --> 00:34:45,540
right and the next thing we can do is

00:34:42,510 --> 00:34:46,620
look at the nodes there's no nodes even

00:34:45,540 --> 00:34:49,290
though we have all these nodes ready

00:34:46,620 --> 00:34:51,480
nothing happened now one way I can add

00:34:49,290 --> 00:34:53,669
nodes is also using the API some people

00:34:51,480 --> 00:34:56,010
prefer this method so I could just make

00:34:53,669 --> 00:34:58,110
an API call with this block of Jason I

00:34:56,010 --> 00:35:01,200
can say hey this note has this IP

00:34:58,110 --> 00:35:02,700
address and it has these labels maybe

00:35:01,200 --> 00:35:05,010
that makes sense but a larger scale you

00:35:02,700 --> 00:35:06,900
want something to automate this so what

00:35:05,010 --> 00:35:09,600
I've done is I've written a really quick

00:35:06,900 --> 00:35:11,850
app called coop register and what coop

00:35:09,600 --> 00:35:13,860
register does it talks to fleet let's

00:35:11,850 --> 00:35:17,100
look at the unit file ford coupe

00:35:13,860 --> 00:35:18,840
register so it talks to fleet over our

00:35:17,100 --> 00:35:22,130
unix socket that's local to the machine

00:35:18,840 --> 00:35:24,480
and then what it does is it says fleet

00:35:22,130 --> 00:35:26,640
anything that has metadata role equals

00:35:24,480 --> 00:35:29,010
cooper Nettie's give me that list of

00:35:26,640 --> 00:35:31,050
servers and their IP addresses and then

00:35:29,010 --> 00:35:32,940
i'm going to hit their health check port

00:35:31,050 --> 00:35:35,370
so incur bearnaise everything has a

00:35:32,940 --> 00:35:37,590
health check port or an endpoint call

00:35:35,370 --> 00:35:39,870
health z we hit that endpoint and if it

00:35:37,590 --> 00:35:41,850
comes back healthy or register it to the

00:35:39,870 --> 00:35:43,440
API server I'm going to automatically

00:35:41,850 --> 00:35:45,390
register so that way we keep adding

00:35:43,440 --> 00:35:46,770
machines we just automatically grow the

00:35:45,390 --> 00:35:48,750
cluster without any manual intervention

00:35:46,770 --> 00:35:50,910
so I'm just going to run this is a very

00:35:48,750 --> 00:35:52,320
small little go app it just runs and

00:35:50,910 --> 00:35:53,280
does this thing and we're going to make

00:35:52,320 --> 00:35:55,230
sure we go with

00:35:53,280 --> 00:36:01,220
where the API server is just to make my

00:35:55,230 --> 00:36:05,100
life easy so we'll do fleet start units

00:36:01,220 --> 00:36:06,540
khoob register and this kind of just

00:36:05,100 --> 00:36:08,070
shows how easy it is to integrate with

00:36:06,540 --> 00:36:10,710
the crew barnaise API this is about a

00:36:08,070 --> 00:36:13,200
two hundred line little go service but I

00:36:10,710 --> 00:36:15,180
built against the API and it just works

00:36:13,200 --> 00:36:19,830
so what we can do now is look at the

00:36:15,180 --> 00:36:29,220
logs for this guy journal for a cube

00:36:19,830 --> 00:36:30,720
register not coupe CTL fleet alright so

00:36:29,220 --> 00:36:32,430
its up and running and we see in this

00:36:30,720 --> 00:36:34,110
log that it registered for machines for

00:36:32,430 --> 00:36:36,060
us and it just stays on the network

00:36:34,110 --> 00:36:39,270
watching for changes anything shows up

00:36:36,060 --> 00:36:41,160
or goes away and we'll remove them so at

00:36:39,270 --> 00:36:45,030
this point we should be able to see all

00:36:41,160 --> 00:36:47,190
of our nodes there we go so all the

00:36:45,030 --> 00:36:48,960
nodes are now ready on the network ready

00:36:47,190 --> 00:36:54,060
for work Cooper neighs is now in control

00:36:48,960 --> 00:36:56,160
of this particular cluster so before we

00:36:54,060 --> 00:36:57,980
deploy applications let's set up a real

00:36:56,160 --> 00:37:00,870
world example that we want to use here

00:36:57,980 --> 00:37:02,850
so what I've done is already I've pixie

00:37:00,870 --> 00:37:04,740
some machines some virtual machines so I

00:37:02,850 --> 00:37:06,210
burnt the firmware on the Nick cards on

00:37:04,740 --> 00:37:08,760
these vm to kind of treat them like

00:37:06,210 --> 00:37:10,920
virtual bare metal machines we have some

00:37:08,760 --> 00:37:13,020
hot cross host networking in place we

00:37:10,920 --> 00:37:15,570
also have Coburn eighties ready to go so

00:37:13,020 --> 00:37:17,100
now if you're in the ops department you

00:37:15,570 --> 00:37:18,390
rack the machines you get all the

00:37:17,100 --> 00:37:20,220
networking right and you may also

00:37:18,390 --> 00:37:21,870
install the cluster manager maybe

00:37:20,220 --> 00:37:24,030
implement some policies and credentials

00:37:21,870 --> 00:37:25,140
and that's the end of your job you

00:37:24,030 --> 00:37:27,300
monitor things that make sure that's

00:37:25,140 --> 00:37:29,610
always up then you kind of delegate

00:37:27,300 --> 00:37:31,560
power to people to start specifying

00:37:29,610 --> 00:37:33,720
configs that they can talk to Cooper

00:37:31,560 --> 00:37:37,260
Nettie's with right kind of self service

00:37:33,720 --> 00:37:39,180
mode here here's a sample app called PG

00:37:37,260 --> 00:37:41,220
view very simple app it talks to

00:37:39,180 --> 00:37:43,410
postgres figures out all the

00:37:41,220 --> 00:37:45,920
capabilities of postgres and reports it

00:37:43,410 --> 00:37:48,390
back right post grass info as a service

00:37:45,920 --> 00:37:49,950
we have some requirements it needs to be

00:37:48,390 --> 00:37:51,300
horizontally scalable so we start with

00:37:49,950 --> 00:37:54,600
one we want to be able to scale up to

00:37:51,300 --> 00:37:57,690
five ten instances of this thing we want

00:37:54,600 --> 00:38:00,030
a dedicated memcache per instance a PG

00:37:57,690 --> 00:38:01,470
view meaning I want my own I don't want

00:38:00,030 --> 00:38:03,890
to share one with anyone else I want to

00:38:01,470 --> 00:38:06,150
be able to cash my results on localhost

00:38:03,890 --> 00:38:06,930
VMM cash that means i need to be

00:38:06,150 --> 00:38:09,540
scheduled together

00:38:06,930 --> 00:38:11,730
and then I need access to the Postgres

00:38:09,540 --> 00:38:13,319
database right like we don't necessarily

00:38:11,730 --> 00:38:15,480
want to give this thing a file to say

00:38:13,319 --> 00:38:17,220
hey here's where postgres is I just want

00:38:15,480 --> 00:38:19,920
you to inject and tell me where postgres

00:38:17,220 --> 00:38:21,089
is and then we want this thing to do you

00:38:19,920 --> 00:38:24,000
know that's our automated service

00:38:21,089 --> 00:38:26,910
discovery and then we want zero downtown

00:38:24,000 --> 00:38:29,940
downtime application upgrades that's

00:38:26,910 --> 00:38:31,859
really really hard but with the proxy we

00:38:29,940 --> 00:38:33,240
actually get we actually get a head

00:38:31,859 --> 00:38:36,000
start on actually trying to make that

00:38:33,240 --> 00:38:38,819
happen here's what the app looks like

00:38:36,000 --> 00:38:40,740
really quick and we do a curl to it and

00:38:38,819 --> 00:38:42,630
we give it a bit of RPC in this case we

00:38:40,740 --> 00:38:45,300
call sequel features we get this

00:38:42,630 --> 00:38:47,460
response back so if you were born a

00:38:45,300 --> 00:38:51,359
while ago these are some really hot new

00:38:47,460 --> 00:38:54,270
languages for you and postgres still

00:38:51,359 --> 00:38:56,160
supports them all for some reason all

00:38:54,270 --> 00:38:57,869
right so let's get on with it so our

00:38:56,160 --> 00:38:59,910
goal is to deploy this right so now your

00:38:57,869 --> 00:39:02,099
operator you have Cooper Nettie's and

00:38:59,910 --> 00:39:03,480
you give it to your developers let's

00:39:02,099 --> 00:39:05,609
just go through the flow of deploying

00:39:03,480 --> 00:39:07,349
that application and also upgrading the

00:39:05,609 --> 00:39:09,329
application and doing rolling upgrades

00:39:07,349 --> 00:39:11,700
of it so what we'll do is we'll start

00:39:09,329 --> 00:39:13,559
with the pod so we need a pod first and

00:39:11,700 --> 00:39:15,660
a pot is like a single tendency we only

00:39:13,559 --> 00:39:18,119
want one of these right containers don't

00:39:15,660 --> 00:39:19,980
make your services like automatically

00:39:18,119 --> 00:39:21,180
clusterware highly available they just

00:39:19,980 --> 00:39:23,010
give you something that can move around

00:39:21,180 --> 00:39:24,780
really quick and probably fail really

00:39:23,010 --> 00:39:27,359
quick but in this case we have a

00:39:24,780 --> 00:39:29,160
postgres pod we're going to give it a

00:39:27,359 --> 00:39:30,980
volume meaning we want a path on the

00:39:29,160 --> 00:39:33,180
host in this case this could have been a

00:39:30,980 --> 00:39:36,059
if you're in a cloud you can take a

00:39:33,180 --> 00:39:38,099
virtual disk or persistent disk and

00:39:36,059 --> 00:39:39,809
mount it to the host first and then

00:39:38,099 --> 00:39:40,770
schedule container there you can

00:39:39,809 --> 00:39:42,660
actually integrate with all those things

00:39:40,770 --> 00:39:45,450
in Coober Nettie's to keep this simple

00:39:42,660 --> 00:39:48,230
we're going to say we specify we need a

00:39:45,450 --> 00:39:50,400
host path that has that destination and

00:39:48,230 --> 00:39:52,559
then we're also wanted to describe the

00:39:50,400 --> 00:39:55,049
container we want this container to pull

00:39:52,559 --> 00:39:56,609
from its this registry on my local

00:39:55,049 --> 00:39:58,680
machine on poor 5000 we want the

00:39:56,609 --> 00:40:01,829
postgres container and then the postgres

00:39:58,680 --> 00:40:03,450
container has that port and then down

00:40:01,829 --> 00:40:05,250
here we're in checking some environment

00:40:03,450 --> 00:40:07,170
variables don't do this in production

00:40:05,250 --> 00:40:08,880
this is like me putting the password for

00:40:07,170 --> 00:40:10,589
postgres in here just to make my life

00:40:08,880 --> 00:40:12,630
convenient there's better ways of doing

00:40:10,589 --> 00:40:14,579
this normally I wrap things and Etsy d

00:40:12,630 --> 00:40:15,990
pull things down and configure it but in

00:40:14,579 --> 00:40:18,240
this case this container is going to

00:40:15,990 --> 00:40:20,870
take on that password for the postgres

00:40:18,240 --> 00:40:23,940
user and then down here i'm referring

00:40:20,870 --> 00:40:26,250
to the mount point described earlier and

00:40:23,940 --> 00:40:28,860
by mounting that into the container at

00:40:26,250 --> 00:40:30,990
runtime and then down here we have some

00:40:28,860 --> 00:40:32,520
basic policy stuff so we're going to do

00:40:30,990 --> 00:40:37,050
now is we're going to tell Cooper

00:40:32,520 --> 00:40:41,910
Nettie's to deploy this pot so we do

00:40:37,050 --> 00:40:45,630
clube CTL we want to create pods

00:40:41,910 --> 00:40:48,600
postgres right so now the postgres pod

00:40:45,630 --> 00:40:53,310
has been created so we do coop CTL get

00:40:48,600 --> 00:40:54,900
pods so its impending mode because it's

00:40:53,310 --> 00:40:56,430
actually downloading the container from

00:40:54,900 --> 00:41:00,660
the doctor registry running on my laptop

00:40:56,430 --> 00:41:02,400
and now it's running right but we don't

00:41:00,660 --> 00:41:04,410
want to use this pot IP so every IP

00:41:02,400 --> 00:41:06,240
every pot in the Cooper neighs cluster

00:41:04,410 --> 00:41:07,860
gets an IP from that range that final

00:41:06,240 --> 00:41:09,240
setup but we don't want to use this

00:41:07,860 --> 00:41:10,770
directly and we don't want to give it to

00:41:09,240 --> 00:41:15,120
anyone directly right we want to give

00:41:10,770 --> 00:41:18,570
them a service IP instead so what we'll

00:41:15,120 --> 00:41:20,430
do is we'll create a service for here's

00:41:18,570 --> 00:41:22,980
what a service looks like week in dec

00:41:20,430 --> 00:41:26,070
declarative you would say hey here's the

00:41:22,980 --> 00:41:27,060
port we don't know what the IP is yet

00:41:26,070 --> 00:41:29,310
because we're going to get it from the

00:41:27,060 --> 00:41:31,500
service portal so the crew barnaise API

00:41:29,310 --> 00:41:33,330
has a range they could be public ip's or

00:41:31,500 --> 00:41:35,580
they could be private IPS that will

00:41:33,330 --> 00:41:37,290
allocate when people create a service so

00:41:35,580 --> 00:41:39,360
we have a block sitting there if I

00:41:37,290 --> 00:41:41,400
submit this job is going to give me an

00:41:39,360 --> 00:41:43,620
IP back and it's going to be bound to

00:41:41,400 --> 00:41:46,080
this port and it's going to look for any

00:41:43,620 --> 00:41:48,030
containers that have those labels only

00:41:46,080 --> 00:41:50,850
those labels so if it has 10 of those

00:41:48,030 --> 00:41:53,130
labels these two will match and then we

00:41:50,850 --> 00:41:54,720
will proxy data to it you can also do

00:41:53,130 --> 00:41:56,130
sticky sessions or sticky affinity if

00:41:54,720 --> 00:41:57,660
you want to be a multiple you want

00:41:56,130 --> 00:42:00,150
clients to stick to the one thing land

00:41:57,660 --> 00:42:02,730
on versus round-robin you can do that so

00:42:00,150 --> 00:42:09,450
we're going to create a service so clube

00:42:02,730 --> 00:42:11,940
CTL create services postgres so now we

00:42:09,450 --> 00:42:16,260
have a service and when we look at this

00:42:11,940 --> 00:42:20,730
service get services you'll see that we

00:42:16,260 --> 00:42:24,900
got this VIP virtual IP from the 10 dot

00:42:20,730 --> 00:42:26,460
10 / 16 range now for a lot of you that

00:42:24,900 --> 00:42:27,450
are familiar with networking there is no

00:42:26,460 --> 00:42:30,630
way I'm going to be able to reach this

00:42:27,450 --> 00:42:34,500
from my mac i'm on this l to network 192

00:42:30,630 --> 00:42:37,290
dot 6a / 6 16 networks or the containers

00:42:34,500 --> 00:42:40,530
I can't hit that IP right now if I try

00:42:37,290 --> 00:42:46,020
right so if I were to do this psql dash

00:42:40,530 --> 00:42:51,650
H what was this IP let's grab it let's

00:42:46,020 --> 00:42:51,650
try to hit the side p dash you postgres

00:42:52,430 --> 00:42:57,270
we can't hit it alright so this is where

00:42:55,110 --> 00:42:58,650
basic routing comes in so what you do at

00:42:57,270 --> 00:43:00,630
your higher level parts of the cluster

00:42:58,650 --> 00:43:02,310
you would add some rounds now normally

00:43:00,630 --> 00:43:05,100
what we do in this particular model will

00:43:02,310 --> 00:43:06,840
low balance this IP to wherever the pods

00:43:05,100 --> 00:43:09,210
live or just all the servers and let the

00:43:06,840 --> 00:43:12,060
proxy server give us the next hop to

00:43:09,210 --> 00:43:13,530
where it actually runs so what I'm going

00:43:12,060 --> 00:43:14,790
to do now is just a door out really

00:43:13,530 --> 00:43:16,890
quick so i have this thing they're just

00:43:14,790 --> 00:43:19,860
little script that adds a static route

00:43:16,890 --> 00:43:22,320
for me so this static route is anything

00:43:19,860 --> 00:43:23,910
going to the 10 10 network I'm just go

00:43:22,320 --> 00:43:25,050
to this host here doesn't matter what

00:43:23,910 --> 00:43:27,180
hosting the cluster because it was a

00:43:25,050 --> 00:43:29,040
proxy that will just send me to the next

00:43:27,180 --> 00:43:31,410
hop right so I just need to just wrap my

00:43:29,040 --> 00:43:32,970
traffic somewhere within the cluster so

00:43:31,410 --> 00:43:39,600
now that I have my route in place we can

00:43:32,970 --> 00:43:44,760
try this again and it works what's the

00:43:39,600 --> 00:43:46,440
password there is host its core OS so

00:43:44,760 --> 00:43:48,510
now we're into the my sequel day our

00:43:46,440 --> 00:43:51,990
Postgres database and then we have a

00:43:48,510 --> 00:43:53,910
service IP for it so now how do you use

00:43:51,990 --> 00:43:56,790
this service IP well it's look really

00:43:53,910 --> 00:43:59,280
quick at the PG View application github

00:43:56,790 --> 00:44:01,290
comes height RPG view and we'll just

00:43:59,280 --> 00:44:04,170
look at you know ignore all this source

00:44:01,290 --> 00:44:06,450
code but here's where I look at my

00:44:04,170 --> 00:44:08,090
credentials I can say postgres

00:44:06,450 --> 00:44:10,710
underscore service host and port

00:44:08,090 --> 00:44:14,430
whenever you deploy a pod with Cooper

00:44:10,710 --> 00:44:16,710
Nettie's it injects all the server's IP

00:44:14,430 --> 00:44:18,540
is import in your namespace so anything

00:44:16,710 --> 00:44:19,740
that you authorize the sea will just

00:44:18,540 --> 00:44:22,110
give you the username and password how

00:44:19,740 --> 00:44:23,460
to get there there's also configuration

00:44:22,110 --> 00:44:25,320
management built into Cooper Nettie so

00:44:23,460 --> 00:44:27,120
you could say when I deploy this Cooper

00:44:25,320 --> 00:44:29,190
this container give it this

00:44:27,120 --> 00:44:30,660
configuration settings and then what

00:44:29,190 --> 00:44:33,450
Cooper neighs will do is two options it

00:44:30,660 --> 00:44:35,760
will either mount a temporary volume

00:44:33,450 --> 00:44:37,590
into the space that you specify so that

00:44:35,760 --> 00:44:39,240
your app can just look on this for a

00:44:37,590 --> 00:44:41,610
file that has all the stuff that you

00:44:39,240 --> 00:44:43,410
want in it or allow your app to just

00:44:41,610 --> 00:44:46,170
make a HTTP call to grab the

00:44:43,410 --> 00:44:48,330
configuration we're just going to use

00:44:46,170 --> 00:44:49,920
those service environment variables

00:44:48,330 --> 00:44:50,970
so now what we want to do is we'll use a

00:44:49,920 --> 00:44:52,800
pod this time we're going to a

00:44:50,970 --> 00:44:54,600
replication controller so here's a

00:44:52,800 --> 00:44:56,370
replication controller for the PG view

00:44:54,600 --> 00:44:59,370
app this is a stable version of the app

00:44:56,370 --> 00:45:01,080
at version 1 now this was a bit more

00:44:59,370 --> 00:45:02,430
complicated mainly because we're going

00:45:01,080 --> 00:45:06,840
to have a high level controller that

00:45:02,430 --> 00:45:09,210
we're describing and that controller has

00:45:06,840 --> 00:45:11,520
a specification mainly we want one of

00:45:09,210 --> 00:45:13,890
these things on the on the cluster

00:45:11,520 --> 00:45:18,240
network at a time here's a selector

00:45:13,890 --> 00:45:20,280
we're going to watch for all pods that

00:45:18,240 --> 00:45:22,790
have those labels and that's the only

00:45:20,280 --> 00:45:25,380
thing that this controller cares about

00:45:22,790 --> 00:45:27,900
here's a template on how to build pods

00:45:25,380 --> 00:45:30,810
so if you see that there's too few pods

00:45:27,900 --> 00:45:33,120
use this template give them these labels

00:45:30,810 --> 00:45:34,980
when you create it in this case we have

00:45:33,120 --> 00:45:37,020
multiple containers so the first

00:45:34,980 --> 00:45:39,030
container is our pgv you app that comes

00:45:37,020 --> 00:45:41,070
from our doctor registry it listens on

00:45:39,030 --> 00:45:43,770
port 80 the nice thing about company

00:45:41,070 --> 00:45:45,240
says every container has an IP we don't

00:45:43,770 --> 00:45:48,000
have pork clashes so if everyone wants

00:45:45,240 --> 00:45:50,160
to listen on port 80 so be it there's no

00:45:48,000 --> 00:45:51,510
clashes that you have to deal with we

00:45:50,160 --> 00:45:53,640
can also give this a resource limit

00:45:51,510 --> 00:45:55,320
these are arbitrary these are these are

00:45:53,640 --> 00:45:57,780
a bit weird because with see groups on

00:45:55,320 --> 00:45:59,850
Linux if you say hey 1,000 shares of a

00:45:57,780 --> 00:46:02,310
cpu and someone says 10,000 shares of

00:45:59,850 --> 00:46:04,500
cpu everything gets relative so you got

00:46:02,310 --> 00:46:05,910
to be careful how you use these and then

00:46:04,500 --> 00:46:08,280
here's the memcache container that we

00:46:05,910 --> 00:46:10,080
also want deploy in the same pot and

00:46:08,280 --> 00:46:12,320
memcache will just come from the same

00:46:10,080 --> 00:46:14,970
doctor registry and it will listen on

00:46:12,320 --> 00:46:16,950
that particular port and we also give it

00:46:14,970 --> 00:46:19,440
some limitations here Cooper neighs is a

00:46:16,950 --> 00:46:20,490
bit smart about scheduling things it

00:46:19,440 --> 00:46:22,980
will take some of those things into

00:46:20,490 --> 00:46:27,240
account if you register a node will say

00:46:22,980 --> 00:46:28,830
only 1000 CPU units when you get to that

00:46:27,240 --> 00:46:31,250
limit no more pods will be scheduled

00:46:28,830 --> 00:46:34,020
there it does some basic bin packing

00:46:31,250 --> 00:46:35,250
alright so here's a pod so now what

00:46:34,020 --> 00:46:37,380
we'll do is instead of launching those

00:46:35,250 --> 00:46:38,880
pods you know one by one we'll just

00:46:37,380 --> 00:46:41,100
launched the replication controller and

00:46:38,880 --> 00:46:43,590
let it manage the number of pods in the

00:46:41,100 --> 00:46:47,670
cluster for us so we'll do coop CTL

00:46:43,590 --> 00:46:49,410
crate dash f we want to do replication

00:46:47,670 --> 00:46:53,190
controllers and then we're going to do

00:46:49,410 --> 00:46:55,080
the PG view stable v 1 controller so now

00:46:53,190 --> 00:46:58,470
that's up and running so now we can do

00:46:55,080 --> 00:47:01,530
coop CTL get replication controllers and

00:46:58,470 --> 00:47:02,190
here we have we have one in play this

00:47:01,530 --> 00:47:04,620
thing wants

00:47:02,190 --> 00:47:09,830
one replica of this so if everything is

00:47:04,620 --> 00:47:12,750
working right what we should see is a

00:47:09,830 --> 00:47:15,090
pot being created and this pot kind gets

00:47:12,750 --> 00:47:16,530
a random name here with and what we do

00:47:15,090 --> 00:47:18,450
is we see that is actually running that

00:47:16,530 --> 00:47:21,300
means doctor has pull both the PG view

00:47:18,450 --> 00:47:22,920
app and memcache and if everything is

00:47:21,300 --> 00:47:24,750
working with service discovery it should

00:47:22,920 --> 00:47:26,250
also have the right IP import to

00:47:24,750 --> 00:47:29,340
postgres because we use the service fit

00:47:26,250 --> 00:47:31,410
here so again we don't want to give

00:47:29,340 --> 00:47:34,050
people the IP to our pods so we're going

00:47:31,410 --> 00:47:37,140
to create a service for PG view because

00:47:34,050 --> 00:47:38,430
we may scale this thing so the service

00:47:37,140 --> 00:47:40,770
look pretty much the same as we saw

00:47:38,430 --> 00:47:43,350
before so we won't look at it but we'll

00:47:40,770 --> 00:47:48,270
do is just create a new service for pgp

00:47:43,350 --> 00:47:51,750
you now this service says is very well

00:47:48,270 --> 00:47:53,700
let's look at it this one is a little

00:47:51,750 --> 00:47:55,140
bit loose it says that it only cares

00:47:53,700 --> 00:47:57,630
about things in production there an MPG

00:47:55,140 --> 00:47:59,640
view right this is good because now if

00:47:57,630 --> 00:48:01,080
we come up with another controller that

00:47:59,640 --> 00:48:02,460
has a different set of values may be

00:48:01,080 --> 00:48:05,160
mainly for a new version of the

00:48:02,460 --> 00:48:07,590
application this same proxy can be used

00:48:05,160 --> 00:48:10,860
to send traffic to both since they match

00:48:07,590 --> 00:48:13,050
the set of labels all right so now that

00:48:10,860 --> 00:48:18,240
we have the proxy in place we should get

00:48:13,050 --> 00:48:22,410
a new VIP and here it is so here's a PG

00:48:18,240 --> 00:48:24,180
view we have a VIP and now we can test

00:48:22,410 --> 00:48:27,030
to see if this works so we should be

00:48:24,180 --> 00:48:30,210
able to do curl get this thing make an

00:48:27,030 --> 00:48:32,610
RPC request let's just get the version

00:48:30,210 --> 00:48:35,300
really quick all right so we know we

00:48:32,610 --> 00:48:39,210
have one dot o of the application and

00:48:35,300 --> 00:48:41,490
also let's test to see if the sequel is

00:48:39,210 --> 00:48:44,130
working so we have a sequel features API

00:48:41,490 --> 00:48:46,800
endpoint so we hit this on by design i

00:48:44,130 --> 00:48:48,930
introduce a 10-second sleep to mimic a

00:48:46,800 --> 00:48:50,820
very slow query the curry comes back

00:48:48,930 --> 00:48:52,950
it's a lot of data and we do it again

00:48:50,820 --> 00:48:56,070
memcache kicks in and now it's fast

00:48:52,950 --> 00:48:57,540
right so our applications working using

00:48:56,070 --> 00:49:00,030
the database it found it on its own

00:48:57,540 --> 00:49:02,040
we're doing our thing so let's see what

00:49:00,030 --> 00:49:06,780
customers see right so customers are

00:49:02,040 --> 00:49:09,060
using this application now true do that

00:49:06,780 --> 00:49:10,290
and then we'll sleep for a minute so

00:49:09,060 --> 00:49:13,530
we're just going to see what customers

00:49:10,290 --> 00:49:15,450
see using an app over time all right so

00:49:13,530 --> 00:49:15,750
customers are hitting postgrads we're

00:49:15,450 --> 00:49:18,240
happy

00:49:15,750 --> 00:49:20,010
we got one instance of the app it start

00:49:18,240 --> 00:49:21,900
getting popular now your mom's using the

00:49:20,010 --> 00:49:25,020
service for some odd reason she deployed

00:49:21,900 --> 00:49:29,190
postgres that's pretty cool of your

00:49:25,020 --> 00:49:30,930
mom's using postgres so you're hitting

00:49:29,190 --> 00:49:34,560
the service it's doing this thing and

00:49:30,930 --> 00:49:36,060
now what we want to do is a scale it up

00:49:34,560 --> 00:49:37,770
right you've earned the right to have

00:49:36,060 --> 00:49:40,620
more than one instance in our cluster

00:49:37,770 --> 00:49:42,180
sweet so how do we resize it well we

00:49:40,620 --> 00:49:44,250
could update that definition and check

00:49:42,180 --> 00:49:45,840
it in or we can actually just talk to

00:49:44,250 --> 00:49:47,760
urban aims directly and say hey for that

00:49:45,840 --> 00:49:52,800
controller I want you to resize it for

00:49:47,760 --> 00:49:54,960
me so let's do khoob CTL and we're going

00:49:52,800 --> 00:49:56,780
to use the resize command and then we

00:49:54,960 --> 00:50:00,360
want to increase the number of replicas

00:49:56,780 --> 00:50:03,470
23 from one and then we want to do this

00:50:00,360 --> 00:50:08,670
for replication controllers pgv you

00:50:03,470 --> 00:50:10,650
stable v1 yes all right so it's resized

00:50:08,670 --> 00:50:14,010
so what does that mean so now if it's

00:50:10,650 --> 00:50:15,930
been resized so one thing you do is you

00:50:14,010 --> 00:50:18,150
see that the definition the spec inside

00:50:15,930 --> 00:50:19,560
Kobernus has changed to three right now

00:50:18,150 --> 00:50:21,300
this is where the replication controller

00:50:19,560 --> 00:50:22,860
step sentence as well the state has

00:50:21,300 --> 00:50:25,440
changed real time we don't have to do

00:50:22,860 --> 00:50:27,330
any pushes is do any full-time watch the

00:50:25,440 --> 00:50:31,650
state has changed now the state has

00:50:27,330 --> 00:50:32,550
changed our job is to make its own so

00:50:31,650 --> 00:50:34,260
now we have three of those things

00:50:32,550 --> 00:50:36,750
running on the network they're all up

00:50:34,260 --> 00:50:37,980
and running now and they've all done the

00:50:36,750 --> 00:50:39,990
same things and they spread themselves

00:50:37,980 --> 00:50:41,490
out across the host we don't really care

00:50:39,990 --> 00:50:43,530
because we're not keeping track we've

00:50:41,490 --> 00:50:45,840
just implemented the definition you

00:50:43,530 --> 00:50:47,370
wanted three you get three using the

00:50:45,840 --> 00:50:49,950
same template same version running

00:50:47,370 --> 00:50:51,900
where's our customers see it just does

00:50:49,950 --> 00:50:54,990
this thing the proxies handling things

00:50:51,900 --> 00:50:57,210
for us so the proxy will only add things

00:50:54,990 --> 00:50:58,800
to us back in as they come alive if you

00:50:57,210 --> 00:51:00,480
have a health implant on your apps and

00:50:58,800 --> 00:51:02,430
services it will check that health in

00:51:00,480 --> 00:51:04,290
point before adding it to this proxy

00:51:02,430 --> 00:51:06,780
pool so the customer doesn't see any

00:51:04,290 --> 00:51:08,490
changes here so now life is good right

00:51:06,780 --> 00:51:10,920
you have three of these things and now

00:51:08,490 --> 00:51:13,320
you want to upgrade to two dot oh all

00:51:10,920 --> 00:51:15,120
right so I don't trust anyone writing

00:51:13,320 --> 00:51:16,470
code to get it right the first time so

00:51:15,120 --> 00:51:18,180
we're going to do we're gonna give you a

00:51:16,470 --> 00:51:19,410
canary we can give you one of the new

00:51:18,180 --> 00:51:21,690
things i'm just going to mix it with the

00:51:19,410 --> 00:51:24,330
existing traffic to see if it works if

00:51:21,690 --> 00:51:26,550
it works will consider rolling out that

00:51:24,330 --> 00:51:28,110
new version across the cluster so one

00:51:26,550 --> 00:51:29,590
very popular pattern for doing this is

00:51:28,110 --> 00:51:30,880
the canary pattern

00:51:29,590 --> 00:51:32,650
right so we're going to do is use a

00:51:30,880 --> 00:51:34,720
canary controller that looks just like a

00:51:32,650 --> 00:51:37,900
other controller but one difference it

00:51:34,720 --> 00:51:40,090
uses a new version of the image that we

00:51:37,900 --> 00:51:41,260
want to deploy and we also tell this new

00:51:40,090 --> 00:51:47,230
controller that it should only be

00:51:41,260 --> 00:51:49,270
concerned about things things inside of

00:51:47,230 --> 00:51:53,470
its controller so have a new label here

00:51:49,270 --> 00:51:55,270
so we'll do PG view BG view and then

00:51:53,470 --> 00:51:56,410
we'll do the canary controller and

00:51:55,270 --> 00:51:59,530
here's the difference with the canary

00:51:56,410 --> 00:52:01,990
controller it only cares about things

00:51:59,530 --> 00:52:04,270
that have this thing called a new value

00:52:01,990 --> 00:52:06,250
call track equals canary the other one

00:52:04,270 --> 00:52:07,720
cared about track equals production so

00:52:06,250 --> 00:52:09,550
this they won't get confused and start

00:52:07,720 --> 00:52:11,380
fighting each other for ownership of

00:52:09,550 --> 00:52:12,610
those pods and the only thing we have

00:52:11,380 --> 00:52:15,220
different here is just the version of

00:52:12,610 --> 00:52:16,540
the app will do to that oh here so we're

00:52:15,220 --> 00:52:18,520
ready you know we're going to go ahead

00:52:16,540 --> 00:52:20,440
and do this canary you can see if this

00:52:18,520 --> 00:52:22,810
thing will work well so we do coo CTO

00:52:20,440 --> 00:52:24,190
we'll just do create by the way you can

00:52:22,810 --> 00:52:25,300
do all this stuff with the API if you

00:52:24,190 --> 00:52:27,670
want i'm just using the command line

00:52:25,300 --> 00:52:29,680
make it super easy for myself so we'll

00:52:27,670 --> 00:52:32,890
do a replication controller and then

00:52:29,680 --> 00:52:34,450
we'll just do the canary one all right

00:52:32,890 --> 00:52:36,340
so now the canary controller is up and

00:52:34,450 --> 00:52:39,390
then we've told it look we just need one

00:52:36,340 --> 00:52:43,510
instance of 20 and it makes it happen

00:52:39,390 --> 00:52:45,970
sokku CTL get pods and we see the fourth

00:52:43,510 --> 00:52:47,590
member in play at the top track canary

00:52:45,970 --> 00:52:50,950
but look at the container that enjoys

00:52:47,590 --> 00:52:52,750
writing to dot 0 we go over to this

00:52:50,950 --> 00:52:55,660
machine knows what the customers

00:52:52,750 --> 00:52:58,000
experiencing now today shows up so 25

00:52:55,660 --> 00:53:00,490
percent of our traffic now is getting a

00:52:58,000 --> 00:53:03,490
tu dedo app served right so no downtime

00:53:00,490 --> 00:53:05,170
no talking to two different teams things

00:53:03,490 --> 00:53:08,320
are working we haven't blew up the world

00:53:05,170 --> 00:53:11,920
hasn't blown up yet we're good so now we

00:53:08,320 --> 00:53:13,240
want to roll the entire cluster now

00:53:11,920 --> 00:53:15,400
somebody may be asking like how does

00:53:13,240 --> 00:53:17,460
this thing actually proxy right because

00:53:15,400 --> 00:53:20,290
remember we're using that set of labels

00:53:17,460 --> 00:53:22,240
so if we do let's just do let's say you

00:53:20,290 --> 00:53:23,920
just want this customers like look I

00:53:22,240 --> 00:53:25,570
only want the new features I don't want

00:53:23,920 --> 00:53:26,860
to see this old stuff well how can we do

00:53:25,570 --> 00:53:29,320
that well we could just create another

00:53:26,860 --> 00:53:32,320
service right so if we look at services

00:53:29,320 --> 00:53:34,360
will see that there's a PG view canary

00:53:32,320 --> 00:53:36,180
service and then the set of labels it

00:53:34,360 --> 00:53:38,530
matches is a little bit more specific

00:53:36,180 --> 00:53:41,290
things that have track equals canary

00:53:38,530 --> 00:53:43,079
well I proxy for so let's see what

00:53:41,290 --> 00:53:48,029
happens if we create this guy

00:53:43,079 --> 00:53:50,640
create dash f services PG view stable

00:53:48,029 --> 00:53:55,979
canary so now that we've added this one

00:53:50,640 --> 00:53:57,900
we get khoob CTL get services well the

00:53:55,979 --> 00:53:59,700
slash so we get a new thing come on

00:53:57,900 --> 00:54:01,559
saying oh four track will give people

00:53:59,700 --> 00:54:03,989
this vid so we can make a new dns entry

00:54:01,559 --> 00:54:05,549
pointing at this particular VIP and then

00:54:03,989 --> 00:54:12,630
come over here and s'alright customer

00:54:05,549 --> 00:54:16,559
curl you know hit this thing RPC is just

00:54:12,630 --> 00:54:18,150
look at the version its to that oh so

00:54:16,559 --> 00:54:19,829
everyone hitting that endpoint will get

00:54:18,150 --> 00:54:21,450
the dynamic label cray that's a little

00:54:19,829 --> 00:54:23,430
bit more specific and only see these

00:54:21,450 --> 00:54:25,200
pots to me that's really really powerful

00:54:23,430 --> 00:54:27,779
we're not saying this is a service we're

00:54:25,200 --> 00:54:29,910
saying this collection of things is a

00:54:27,779 --> 00:54:31,769
service so later on down the road to one

00:54:29,910 --> 00:54:33,660
adds a new set of things that need to

00:54:31,769 --> 00:54:36,029
live in and provide this service you can

00:54:33,660 --> 00:54:37,469
add and remove a label real time this

00:54:36,029 --> 00:54:39,359
also has a lot of power if you want to

00:54:37,469 --> 00:54:41,130
debug something let's say something is

00:54:39,359 --> 00:54:43,019
happening in production you don't want

00:54:41,130 --> 00:54:45,180
to turn off the machine or container

00:54:43,019 --> 00:54:46,680
doing its work you can remove a label

00:54:45,180 --> 00:54:49,109
which will cause the replication

00:54:46,680 --> 00:54:51,180
controller say LOL i'm missing one it'll

00:54:49,109 --> 00:54:52,289
spit up a new one and ignore this other

00:54:51,180 --> 00:54:54,239
one because it doesn't have all the

00:54:52,289 --> 00:54:57,150
labels to meet the requirement and now

00:54:54,239 --> 00:54:59,160
that particular container is offline but

00:54:57,150 --> 00:55:01,109
still up and running you're free to log

00:54:59,160 --> 00:55:05,579
in and do whatever you want with that

00:55:01,109 --> 00:55:07,079
alright alright so now that we have our

00:55:05,579 --> 00:55:08,430
canary running we have this other

00:55:07,079 --> 00:55:11,509
service for people that want to use to

00:55:08,430 --> 00:55:13,890
data and now we need to roll the cluster

00:55:11,509 --> 00:55:16,589
to the new version across the board

00:55:13,890 --> 00:55:18,119
right so there's a bunch of API calls

00:55:16,589 --> 00:55:20,009
you can do to orchestrate this for you

00:55:18,119 --> 00:55:21,630
but there's also things like update

00:55:20,009 --> 00:55:23,819
controllers that you can write so you

00:55:21,630 --> 00:55:26,789
may say do my update at this time of day

00:55:23,819 --> 00:55:28,259
and then if there's any failures put

00:55:26,789 --> 00:55:29,849
back the old one you can do whatever you

00:55:28,259 --> 00:55:31,400
want here I'm just going to show a very

00:55:29,849 --> 00:55:34,699
simple one that's built until the tool

00:55:31,400 --> 00:55:37,769
so coupe CTL we're going to do a rolling

00:55:34,699 --> 00:55:44,359
update and then we're going to give it

00:55:37,769 --> 00:55:49,109
an update period of three seconds and

00:55:44,359 --> 00:55:52,739
then we want to update tpg view PG view

00:55:49,109 --> 00:55:53,819
stable that should be one controller and

00:55:52,739 --> 00:55:57,499
we want to give it a new one

00:55:53,819 --> 00:55:57,499
specification mainly the v2

00:55:58,119 --> 00:56:06,079
my PG view stable v2 all right so what's

00:56:03,890 --> 00:56:08,720
happening here now is our command line

00:56:06,079 --> 00:56:10,819
tool is orchestrating a couple of API

00:56:08,720 --> 00:56:12,890
endpoints and the reason why they're all

00:56:10,819 --> 00:56:15,020
separated in API there's no way we can

00:56:12,890 --> 00:56:16,280
know the workflow for everyone but the

00:56:15,020 --> 00:56:17,599
workflows are rich enough to allow you

00:56:16,280 --> 00:56:19,849
to do this so what this is doing is

00:56:17,599 --> 00:56:22,160
taking both replication controllers on

00:56:19,849 --> 00:56:24,109
one of them d incrementing is replica

00:56:22,160 --> 00:56:26,059
count and the other one increasing this

00:56:24,109 --> 00:56:28,040
replica count right so we can just kind

00:56:26,059 --> 00:56:29,450
of make the switch one dies the other

00:56:28,040 --> 00:56:31,280
replication controller brings another

00:56:29,450 --> 00:56:34,130
one online and we just do this at the

00:56:31,280 --> 00:56:36,050
same time until it's complete and what

00:56:34,130 --> 00:56:38,809
are the customers seeing here right

00:56:36,050 --> 00:56:40,339
we're not dropping packets a little bit

00:56:38,809 --> 00:56:43,430
more than half of our traffic is now all

00:56:40,339 --> 00:56:45,980
running to that oh and we just keep

00:56:43,430 --> 00:56:48,170
going and keep going and eventually we

00:56:45,980 --> 00:56:50,329
should see to that Oh across the board

00:56:48,170 --> 00:56:51,950
for all customers that's it we've rolled

00:56:50,329 --> 00:56:53,900
the full cluster everyone's been

00:56:51,950 --> 00:56:58,400
upgraded to the new version of the

00:56:53,900 --> 00:57:01,480
application and with that that's the end

00:56:58,400 --> 00:57:01,480
of the demo thank you

00:57:06,920 --> 00:57:21,349
I guess we'll take questions okay so he

00:57:20,030 --> 00:57:22,849
wants me to kill it the question is can

00:57:21,349 --> 00:57:25,160
you kill one of the containers and you

00:57:22,849 --> 00:57:26,990
want to see it bring it back right

00:57:25,160 --> 00:57:32,210
because it got really hand wave ease in

00:57:26,990 --> 00:57:34,400
it it was like wow you're 22 this guy

00:57:32,210 --> 00:57:36,230
was it here hand waving don't don't do

00:57:34,400 --> 00:57:41,450
that on Twitter ok so what will happen

00:57:36,230 --> 00:57:43,430
is coo CTL get two pods right so you

00:57:41,450 --> 00:57:46,849
want to kill a pot right so let's take

00:57:43,430 --> 00:57:53,119
this pod this guy sorry he just let me

00:57:46,849 --> 00:57:54,950
him cool CTL delete a pod and we're

00:57:53,119 --> 00:57:56,990
going to delete that one all right it's

00:57:54,950 --> 00:57:58,940
gone right but the thing is the

00:57:56,990 --> 00:58:03,890
replication controllers like don't

00:57:58,940 --> 00:58:05,270
matter coming back so it's pending so we

00:58:03,890 --> 00:58:06,410
have to schedule a new one because it

00:58:05,270 --> 00:58:09,170
doesn't meet our definition of

00:58:06,410 --> 00:58:10,760
requirements and then and now it's

00:58:09,170 --> 00:58:12,890
running again right so someone deletes

00:58:10,760 --> 00:58:15,230
one of the pods the controller says you

00:58:12,890 --> 00:58:17,569
said three I see two and it puts it back

00:58:15,230 --> 00:58:19,609
right so it just enforce the state all

00:58:17,569 --> 00:58:20,990
the time now if we were mid-flight there

00:58:19,609 --> 00:58:23,089
we would probably drop the packet I mean

00:58:20,990 --> 00:58:25,490
you know you killed the thing but you

00:58:23,089 --> 00:58:27,859
can have hooks in Cooper days so when we

00:58:25,490 --> 00:58:29,930
delete a pod we can have some hooks to

00:58:27,859 --> 00:58:31,490
you know call on the way out maybe one

00:58:29,930 --> 00:58:33,200
of the hooks as well wait until I finish

00:58:31,490 --> 00:58:35,150
this connection I just won't take

00:58:33,200 --> 00:58:36,740
anymore and then maybe needs to clean up

00:58:35,150 --> 00:58:39,109
a little bit will allow you to do that

00:58:36,740 --> 00:58:40,849
in Cooper daddy's all right so you can

00:58:39,109 --> 00:58:47,119
delete pods and the same thing happens

00:58:40,849 --> 00:58:48,980
if you remove a server all huh yeah so

00:58:47,119 --> 00:58:50,569
there's signal handling so we do sin a

00:58:48,980 --> 00:58:53,900
sick term to the machine but we also

00:58:50,569 --> 00:58:57,559
explicitly you can define other actions

00:58:53,900 --> 00:58:59,059
that need to happen inside the spec for

00:58:57,559 --> 00:59:02,690
your particular pot like do this other

00:58:59,059 --> 00:59:04,930
thing over here right good question here

00:59:02,690 --> 00:59:04,930
then

00:59:13,740 --> 00:59:18,630
does it matter it works with ipv6 as

00:59:16,590 --> 00:59:20,100
well so do you want to look at the rules

00:59:18,630 --> 00:59:22,020
really quick i said i wouldn't login to

00:59:20,100 --> 00:59:24,150
the server but the talk is over we're

00:59:22,020 --> 00:59:26,850
now allowed to login to the server okay

00:59:24,150 --> 00:59:28,200
alright so i want you guys to call me

00:59:26,850 --> 00:59:31,530
out like do you log into the server i

00:59:28,200 --> 00:59:32,910
saw you so let's just do that now well

00:59:31,530 --> 00:59:34,770
log into one of the servers it doesn't

00:59:32,910 --> 00:59:36,300
matter which one because the the proxies

00:59:34,770 --> 00:59:38,820
is the thing that manages the IP tables

00:59:36,300 --> 00:59:41,490
on these things so we log into this

00:59:38,820 --> 00:59:43,580
thing and then we can do a man my IP

00:59:41,490 --> 00:59:46,830
tables foo is rusty so I'm off I

00:59:43,580 --> 00:59:51,330
apologize we want to look at the the net

00:59:46,830 --> 00:59:55,260
table IP table and that'd be stuck if

00:59:51,330 --> 01:00:02,670
there's only one of them where's my

00:59:55,260 --> 01:00:05,460
table yeah here we go we don't want to

01:00:02,670 --> 01:00:07,380
resolve these names okay so what we have

01:00:05,460 --> 01:00:08,700
here is you see all these like rules so

01:00:07,380 --> 01:00:10,890
there's the Cooper Nettie services

01:00:08,700 --> 01:00:13,080
here's our postgres one post grass PG

01:00:10,890 --> 01:00:16,050
view and what it does is so none of

01:00:13,080 --> 01:00:18,900
these IPS in this tent range if you

01:00:16,050 --> 01:00:21,060
notice none of them are assigned to any

01:00:18,900 --> 01:00:24,150
of the interfaces right they're not

01:00:21,060 --> 01:00:25,440
there you don't see flannel there either

01:00:24,150 --> 01:00:28,740
because finals not doing any

01:00:25,440 --> 01:00:32,100
encapsulation so what happens is when

01:00:28,740 --> 01:00:33,600
iptables execute we just have this we

01:00:32,100 --> 01:00:35,100
filter the rule comes the packet comes

01:00:33,600 --> 01:00:37,290
in we see what the destination header

01:00:35,100 --> 01:00:39,869
looks like we send that destination

01:00:37,290 --> 01:00:42,150
header to the proxy on a random port

01:00:39,869 --> 01:00:44,700
over here so in this case the proxy is

01:00:42,150 --> 01:00:47,369
binding to port 45,000 blah blah blah

01:00:44,700 --> 01:00:49,680
and it will say anything that comes into

01:00:47,369 --> 01:00:51,840
this destination IP is going to go to

01:00:49,680 --> 01:00:54,270
Cooper Nettie's and say hey this IP came

01:00:51,840 --> 01:00:56,940
in could you give me the pods that are

01:00:54,270 --> 01:01:00,330
that belong to it and do its proxy that

01:00:56,940 --> 01:01:01,530
way right so everything happens here and

01:01:00,330 --> 01:01:04,109
the nice thing about this particular

01:01:01,530 --> 01:01:06,840
pattern if you have a higher level like

01:01:04,109 --> 01:01:08,550
router f5 or something like this you can

01:01:06,840 --> 01:01:10,050
tell f5 with the next hops are for these

01:01:08,550 --> 01:01:12,450
services right so I have some

01:01:10,050 --> 01:01:13,920
integrations I've done where you can

01:01:12,450 --> 01:01:16,530
build custom plugins I've done this for

01:01:13,920 --> 01:01:18,359
engine X and lua where the backends you

01:01:16,530 --> 01:01:19,950
know you see it IP come in you can have

01:01:18,359 --> 01:01:22,440
a dynamically look up the back ends and

01:01:19,950 --> 01:01:25,020
cashed in in engine X so it's really

01:01:22,440 --> 01:01:27,320
really flexible here cool there's a

01:01:25,020 --> 01:01:27,320
question here

01:01:48,310 --> 01:01:53,000
yeah so let me go back to the slide see

01:01:51,740 --> 01:01:55,880
there's something you have to do too

01:01:53,000 --> 01:01:57,230
right so your job is you got to handle

01:01:55,880 --> 01:01:58,849
the failure right so a lot of people

01:01:57,230 --> 01:02:00,109
that are operating in distributed

01:01:58,849 --> 01:02:01,760
environments they understand this very

01:02:00,109 --> 01:02:04,190
use case i'm doing this database

01:02:01,760 --> 01:02:06,650
migration was the responsibility of the

01:02:04,190 --> 01:02:08,210
app if this happens right maybe when you

01:02:06,650 --> 01:02:09,680
start your database migration you can

01:02:08,210 --> 01:02:11,839
inform other people using something like

01:02:09,680 --> 01:02:13,490
etsy d and say hey migrations on their

01:02:11,839 --> 01:02:16,010
way you need to go in maintenance mode

01:02:13,490 --> 01:02:18,470
and don't process any rights or reads or

01:02:16,010 --> 01:02:20,630
something like this right and maybe

01:02:18,470 --> 01:02:22,190
handle the failure if you are migration

01:02:20,630 --> 01:02:23,570
and you get clipped right there's

01:02:22,190 --> 01:02:25,880
databases that handle this better than

01:02:23,570 --> 01:02:27,320
others but you're in that situation you

01:02:25,880 --> 01:02:29,359
have to handle that orchestration like

01:02:27,320 --> 01:02:30,770
that is not a cluster manager is

01:02:29,359 --> 01:02:32,359
concerned that you chose to use

01:02:30,770 --> 01:02:35,420
something like my sequel to postgres

01:02:32,359 --> 01:02:37,130
that can't handle being migrated or

01:02:35,420 --> 01:02:38,839
moved and it has to have downtime this

01:02:37,130 --> 01:02:41,089
way right there's patterns for doing

01:02:38,839 --> 01:02:42,560
these things but good bearnaise of no

01:02:41,089 --> 01:02:44,510
concern that's a different level of the

01:02:42,560 --> 01:02:45,740
stack so there are people that have some

01:02:44,510 --> 01:02:47,720
tooling around this there's a come to

01:02:45,740 --> 01:02:49,520
like call compose i/o and allow the

01:02:47,720 --> 01:02:51,380
patterns you see for this particular use

01:02:49,520 --> 01:02:52,970
case you can introduce a proxy in front

01:02:51,380 --> 01:02:55,250
of your database that takes all the

01:02:52,970 --> 01:02:56,510
connections that proxy you can tell like

01:02:55,250 --> 01:02:59,780
hey we're going into maintenance mode

01:02:56,510 --> 01:03:01,700
block connections or block craze maybe

01:02:59,780 --> 01:03:03,349
send them to the caching layer do your

01:03:01,700 --> 01:03:05,240
migration and then bring things back

01:03:03,349 --> 01:03:07,910
online so that way you can respond with

01:03:05,240 --> 01:03:09,109
a database is out of service your apps

01:03:07,910 --> 01:03:10,760
should be built in a way that it can

01:03:09,109 --> 01:03:12,050
handle that happening periodically and

01:03:10,760 --> 01:03:13,550
you're free to do your database

01:03:12,050 --> 01:03:15,430
migration there that's going to be

01:03:13,550 --> 01:03:18,290
different for every company depending on

01:03:15,430 --> 01:03:20,240
what their needs are that should not be

01:03:18,290 --> 01:03:21,859
encapsulated in Coober Nettie's right

01:03:20,240 --> 01:03:23,690
maybe you could write a controller that

01:03:21,859 --> 01:03:25,400
does it right you can have a smarter

01:03:23,690 --> 01:03:27,170
controller that says oh that's a

01:03:25,400 --> 01:03:28,880
database thing I'm going to go through

01:03:27,170 --> 01:03:31,400
all of these steps before I do anything

01:03:28,880 --> 01:03:35,020
that controller you can just pull it out

01:03:31,400 --> 01:03:38,089
then before we go 1901 cooper Nettie's

01:03:35,020 --> 01:03:40,790
the goal is to have you have the ability

01:03:38,089 --> 01:03:42,710
to put custom objects in Coober Nettie's

01:03:40,790 --> 01:03:44,810
to write your own controllers for

01:03:42,710 --> 01:03:46,550
example the open shift team has written

01:03:44,810 --> 01:03:48,470
their own deployment controller that's

01:03:46,550 --> 01:03:50,210
really smart about handling multi-tier

01:03:48,470 --> 01:03:51,950
deployments and then they can have their

01:03:50,210 --> 01:03:52,890
own Jason Bob to describe that

01:03:51,950 --> 01:03:54,750
multi-tiered appoint

01:03:52,890 --> 01:03:57,030
and then they launch their own binary

01:03:54,750 --> 01:03:58,770
that will watch for multi-tier

01:03:57,030 --> 01:04:00,690
deployments you can do the same thing

01:03:58,770 --> 01:04:02,760
for database orchestration right so

01:04:00,690 --> 01:04:04,140
you'll pricey and if you do it you can

01:04:02,760 --> 01:04:05,970
share with the community and then people

01:04:04,140 --> 01:04:08,840
can just run it in their cluster thank

01:04:05,970 --> 01:04:08,840

YouTube URL: https://www.youtube.com/watch?v=bU9Uh4ihgR4


