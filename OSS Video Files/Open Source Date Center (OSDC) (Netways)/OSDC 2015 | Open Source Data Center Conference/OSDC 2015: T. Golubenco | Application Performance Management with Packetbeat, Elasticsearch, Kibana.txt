Title: OSDC 2015: T. Golubenco | Application Performance Management with Packetbeat, Elasticsearch, Kibana
Publication date: 2015-04-30
Playlist: OSDC 2015 | Open Source Data Center Conference
Description: 
	Typical Application Performance Management (APM) solutions like New Relic, AppDynamics or Compuware are closed source because they require significant up-front development efforts. Due to the recent advances in open source storage and analytics technologies, it is now feasible to create a generic APM that is more applicable, more flexible and easier to integrate than the commercial alternatives.
The talk will present the requirements for a good open source APM solution and the proposed Packetbeat architecture to meet those requirements. It will demo our automation scripts for installing the complete Packetbeat system and it will show how to use it to monitor the performance of an example Django application. It will also introduce Bonito, a new web interface for the Packetbeat data which is complementary to Kibana. Finally, the talk will explain the differences in overhead and features when compared to the commercially available tools.
Captions: 
	00:00:07,010 --> 00:00:14,760
okay welcome back to the

00:00:10,110 --> 00:00:16,529
today and yeah welcome see you hi

00:00:14,760 --> 00:00:19,200
everyone thank you for coming to my talk

00:00:16,529 --> 00:00:22,140
can you hear me okay I think yes right

00:00:19,200 --> 00:00:24,090
yeah all right so I'm going to talk

00:00:22,140 --> 00:00:26,279
about the packet bit elasticsearch in

00:00:24,090 --> 00:00:29,910
cabana I think elasticsearch in cabana

00:00:26,279 --> 00:00:32,489
you you all know i think but i'm curious

00:00:29,910 --> 00:00:35,340
who here has heard of packet bit before

00:00:32,489 --> 00:00:40,230
this conference Oh quite a few hands

00:00:35,340 --> 00:00:43,530
great that's cool still I'm going to do

00:00:40,230 --> 00:00:45,990
a pretty basic introduction to eat right

00:00:43,530 --> 00:00:48,930
because lots of people don't know it so

00:00:45,990 --> 00:00:51,150
what is pack a bit anyway there are

00:00:48,930 --> 00:00:54,270
actually a few ways in which you could

00:00:51,150 --> 00:00:57,660
describe it the the tag line we have on

00:00:54,270 --> 00:01:00,329
our website is open source application

00:00:57,660 --> 00:01:02,570
monitoring which is quite generic right

00:01:00,329 --> 00:01:05,159
i mean it's clearly opensuse open source

00:01:02,570 --> 00:01:09,299
and it's something about application

00:01:05,159 --> 00:01:13,140
monitoring but that sort of just sets

00:01:09,299 --> 00:01:16,590
the scene of of what it is so digging a

00:01:13,140 --> 00:01:18,360
bit deeper the reason we even started

00:01:16,590 --> 00:01:21,450
the packet bit project was that we

00:01:18,360 --> 00:01:23,880
wanted to do something that makes it

00:01:21,450 --> 00:01:26,970
easier to monitor and troubleshoot

00:01:23,880 --> 00:01:30,270
distributed applications by distributed

00:01:26,970 --> 00:01:31,619
applications I actually mean almost all

00:01:30,270 --> 00:01:34,799
applications because almost all

00:01:31,619 --> 00:01:37,490
applications today are somehow

00:01:34,799 --> 00:01:39,869
distributed right you have at least the

00:01:37,490 --> 00:01:42,510
application server and the web server

00:01:39,869 --> 00:01:44,939
and a database server but I also mean

00:01:42,510 --> 00:01:46,890
things like microservices in micro

00:01:44,939 --> 00:01:50,250
service architecture where you have lots

00:01:46,890 --> 00:01:52,170
of lots of small components interacting

00:01:50,250 --> 00:01:55,049
with each other and at some point it

00:01:52,170 --> 00:01:57,030
becomes difficult to troubleshoot when

00:01:55,049 --> 00:02:02,040
something goes wrong in in production

00:01:57,030 --> 00:02:05,490
right end and we fought a good way to

00:02:02,040 --> 00:02:08,159
start this this this attempting to

00:02:05,490 --> 00:02:10,319
making a better troubleshooting product

00:02:08,159 --> 00:02:11,819
for this rooted application is to look

00:02:10,319 --> 00:02:13,349
at the communication between these

00:02:11,819 --> 00:02:15,030
components right because if it's a

00:02:13,349 --> 00:02:16,560
distributed system automatically you

00:02:15,030 --> 00:02:19,140
have communication between these

00:02:16,560 --> 00:02:21,989
components and looking into this

00:02:19,140 --> 00:02:24,000
communication will reveal interesting

00:02:21,989 --> 00:02:27,210
things so another way

00:02:24,000 --> 00:02:29,460
to to explain what pack abilities is

00:02:27,210 --> 00:02:31,320
like distributed Wireshark we've a lot

00:02:29,460 --> 00:02:37,110
more analytics features and we're going

00:02:31,320 --> 00:02:39,300
to see some of those and then also we

00:02:37,110 --> 00:02:42,120
will have features like we can give you

00:02:39,300 --> 00:02:45,209
late interior partitions and response

00:02:42,120 --> 00:02:47,310
time analyzes and can tell you where

00:02:45,209 --> 00:02:49,440
things which transactions break and how

00:02:47,310 --> 00:02:51,090
many transactions are in total so there

00:02:49,440 --> 00:02:52,890
are lots of these features that are

00:02:51,090 --> 00:02:55,500
usually considering the in the

00:02:52,890 --> 00:02:59,489
application performance management area

00:02:55,500 --> 00:03:02,010
so to say alright and my favorite way of

00:02:59,489 --> 00:03:05,610
explaining what packet bitties is by by

00:03:02,010 --> 00:03:07,830
explaining how how it works so I already

00:03:05,610 --> 00:03:10,200
indicated maybe a little bit also from

00:03:07,830 --> 00:03:12,900
the name that it works by capturing the

00:03:10,200 --> 00:03:16,620
traffic the traffic between your your

00:03:12,900 --> 00:03:19,170
servers and it follows the TCP streams

00:03:16,620 --> 00:03:21,840
and the codes application layer

00:03:19,170 --> 00:03:24,360
protocols of things like HTTP MySQL

00:03:21,840 --> 00:03:27,780
PostgreSQL ready sentries RPC these are

00:03:24,360 --> 00:03:29,760
the ones we have so far and it looks for

00:03:27,780 --> 00:03:33,840
what seems to be a request for example

00:03:29,760 --> 00:03:35,640
an HTTP request and then in the same TCP

00:03:33,840 --> 00:03:37,730
connection it waits for the response

00:03:35,640 --> 00:03:41,940
right so we can capture things like

00:03:37,730 --> 00:03:46,019
response time the URLs response codes

00:03:41,940 --> 00:03:50,510
and then things like this right alright

00:03:46,019 --> 00:03:50,510
I'm going to switch to a quick demo now

00:03:51,080 --> 00:03:59,489
just of the command line stuff alright

00:03:56,250 --> 00:04:02,850
so the way it works is that you install

00:03:59,489 --> 00:04:04,560
an agent that's one possibility you can

00:04:02,850 --> 00:04:10,110
install an agent and we provide the

00:04:04,560 --> 00:04:11,820
debian and RPM packages it's also the

00:04:10,110 --> 00:04:13,799
agent is written in go so it's actually

00:04:11,820 --> 00:04:15,600
quite easy to deploy and it doesn't

00:04:13,799 --> 00:04:17,579
consume that no more much memory at

00:04:15,600 --> 00:04:20,100
least compared to if it would be written

00:04:17,579 --> 00:04:21,840
in Java and so on so that's one

00:04:20,100 --> 00:04:25,800
possibility and other possibility is to

00:04:21,840 --> 00:04:28,680
use new reports or type devices if you

00:04:25,800 --> 00:04:30,750
are on premise and this brings you some

00:04:28,680 --> 00:04:32,520
advantages I mean you don't have to

00:04:30,750 --> 00:04:34,940
install an agent so that's that's pretty

00:04:32,520 --> 00:04:34,940
good

00:04:35,660 --> 00:04:41,480
okay so as normally it runs in a in a

00:04:39,290 --> 00:04:44,660
demon mod like you have it here it has a

00:04:41,480 --> 00:04:48,200
configuration file I'm going to open it

00:04:44,660 --> 00:04:55,880
to quickly open it do you see this good

00:04:48,200 --> 00:04:58,820
enough okay so the the bottom side is

00:04:55,880 --> 00:05:01,460
more interesting you you can configure

00:04:58,820 --> 00:05:03,440
on which interfaces to listen so where

00:05:01,460 --> 00:05:07,090
to capture the packets from you can just

00:05:03,440 --> 00:05:09,710
use any you can set some environmental

00:05:07,090 --> 00:05:12,800
variables like the agent named the

00:05:09,710 --> 00:05:15,260
server on it is some some things it can

00:05:12,800 --> 00:05:17,570
also deduce it by itself and you can

00:05:15,260 --> 00:05:20,210
apply tags right so I just put here that

00:05:17,570 --> 00:05:22,940
let's imagine this the server is part

00:05:20,210 --> 00:05:24,650
owner of a particular logical services

00:05:22,940 --> 00:05:28,130
service that you might have and that

00:05:24,650 --> 00:05:29,510
says naturally another ground server and

00:05:28,130 --> 00:05:32,690
then you just need to define the

00:05:29,510 --> 00:05:34,850
protocols that you're interested in so

00:05:32,690 --> 00:05:37,880
I'm interesting in HTTP on all these

00:05:34,850 --> 00:05:39,920
ports and in postgresql on on this

00:05:37,880 --> 00:05:43,250
standard port and there are some options

00:05:39,920 --> 00:05:45,860
you can set for each protocols will see

00:05:43,250 --> 00:05:50,030
that later for now this is kind of the

00:05:45,860 --> 00:05:56,210
default configuration and let me stop it

00:05:50,030 --> 00:06:02,780
from from from background so that I can

00:05:56,210 --> 00:06:04,910
start it in in for a ground and with a

00:06:02,780 --> 00:06:11,030
bit of debugging and now I will just

00:06:04,910 --> 00:06:12,800
create a sample request and response if

00:06:11,030 --> 00:06:17,270
the Wi-Fi house I think there will be

00:06:12,800 --> 00:06:20,810
actually two requests here because it

00:06:17,270 --> 00:06:24,470
first gets a field to redirect and then

00:06:20,810 --> 00:06:26,419
the actual google page whatever and you

00:06:24,470 --> 00:06:30,470
can see that immediately it printed here

00:06:26,419 --> 00:06:35,140
json object for each of these requests

00:06:30,470 --> 00:06:38,240
reply pairs with what's going on and

00:06:35,140 --> 00:06:42,620
it's is that it's a it's a it's a get

00:06:38,240 --> 00:06:45,020
request one on this path and response

00:06:42,620 --> 00:06:48,590
code was 302 let's say the phrase is

00:06:45,020 --> 00:06:49,580
also here and then we also captured some

00:06:48,590 --> 00:06:54,979
measurements like the

00:06:49,580 --> 00:06:56,960
response time it's here and then we also

00:06:54,979 --> 00:06:58,819
have some things about the environment

00:06:56,960 --> 00:07:02,030
in which we sold this transaction right

00:06:58,819 --> 00:07:05,629
so like for example the the IP address

00:07:02,030 --> 00:07:07,430
is involved the ports and the tags that

00:07:05,629 --> 00:07:13,000
tags that I set in the configuration

00:07:07,430 --> 00:07:16,159
file so life is service and Enver grunt

00:07:13,000 --> 00:07:18,530
all right and and of course you could

00:07:16,159 --> 00:07:22,400
easily get this also from the locks

00:07:18,530 --> 00:07:24,650
that's that's that's pretty obvious but

00:07:22,400 --> 00:07:26,629
we can do it for other protocols as well

00:07:24,650 --> 00:07:28,490
so not necessarily for HTTP and we will

00:07:26,629 --> 00:07:30,949
see later that in some cases is more

00:07:28,490 --> 00:07:34,120
convenient to use the logs in other

00:07:30,949 --> 00:07:39,680
cases it might be more convenient to use

00:07:34,120 --> 00:07:42,289
packet data so let's also create a few

00:07:39,680 --> 00:07:47,810
transactions with postgres so I'm just

00:07:42,289 --> 00:07:56,360
going to start the client here okay this

00:07:47,810 --> 00:07:59,330
works and we'll select for example like

00:07:56,360 --> 00:08:03,409
this and you can see on the right that

00:07:59,330 --> 00:08:05,389
it creates a it creates requests every

00:08:03,409 --> 00:08:08,000
time I do something it's actually

00:08:05,389 --> 00:08:10,550
interesting that even before I pressed

00:08:08,000 --> 00:08:12,860
enter I press tab a few times and in

00:08:10,550 --> 00:08:15,110
order for that autocomplete to work it

00:08:12,860 --> 00:08:18,250
queried the server right so we can see

00:08:15,110 --> 00:08:21,229
here this is the final select that I did

00:08:18,250 --> 00:08:24,349
and this was probably from a from a tab

00:08:21,229 --> 00:08:26,449
command that I pressed so you can see

00:08:24,349 --> 00:08:29,810
that you can capture this way things

00:08:26,449 --> 00:08:33,440
that you would maybe not expect to let's

00:08:29,810 --> 00:08:36,320
also create a an error so let me drop a

00:08:33,440 --> 00:08:38,360
table that doesn't exist right it

00:08:36,320 --> 00:08:40,820
doesn't exist so you can see here that

00:08:38,360 --> 00:08:44,600
we recognize that it's an error status

00:08:40,820 --> 00:08:47,050
error and we have some information in

00:08:44,600 --> 00:08:49,850
the response packet from from from

00:08:47,050 --> 00:08:53,630
postgresql about what's wrong right sir

00:08:49,850 --> 00:08:57,069
yeah that table doesn't exist and I have

00:08:53,630 --> 00:09:01,880
to eat chance to use my favorite

00:08:57,069 --> 00:09:03,290
postgres command which is sleep and it

00:09:01,880 --> 00:09:07,759
slipped for

00:09:03,290 --> 00:09:10,670
and you can see it here the response

00:09:07,759 --> 00:09:16,940
time is three seconds and 3 milliseconds

00:09:10,670 --> 00:09:21,860
a bit longer all right are there any

00:09:16,940 --> 00:09:28,220
questions so far all right then we can

00:09:21,860 --> 00:09:30,230
we can move on with the slides a bit all

00:09:28,220 --> 00:09:32,209
right now imagine you have this these

00:09:30,230 --> 00:09:34,459
these agents installed in all your

00:09:32,209 --> 00:09:36,980
infrastructure zorn all relevant servers

00:09:34,459 --> 00:09:41,000
so they capture everything that's that's

00:09:36,980 --> 00:09:43,069
that's going on in your network and they

00:09:41,000 --> 00:09:45,410
all generate these JSON objects which

00:09:43,069 --> 00:09:48,069
can imagine can become quite a lot of

00:09:45,410 --> 00:09:50,750
data and of course the next question is

00:09:48,069 --> 00:09:53,269
what do we do with this large amount of

00:09:50,750 --> 00:09:59,540
data I would even dare to call it big

00:09:53,269 --> 00:10:02,329
data and yeah they're the traditional

00:09:59,540 --> 00:10:07,040
way that maybe I would have done a year

00:10:02,329 --> 00:10:09,139
or two ago would be to to think okay

00:10:07,040 --> 00:10:12,139
what metrics do we need to extract from

00:10:09,139 --> 00:10:14,089
from from this JSON objects let's say

00:10:12,139 --> 00:10:18,170
I'm thinking about response time

00:10:14,089 --> 00:10:20,540
percentiles or how many requests are per

00:10:18,170 --> 00:10:22,730
server and so on what are the top 10

00:10:20,540 --> 00:10:26,089
hosts things like this I will have to

00:10:22,730 --> 00:10:27,620
think about beforehand about all the all

00:10:26,089 --> 00:10:30,860
the things that I want to extract from

00:10:27,620 --> 00:10:33,670
the data and then write some code to to

00:10:30,860 --> 00:10:38,029
to actually implement this matrix right

00:10:33,670 --> 00:10:40,670
and then I'll need to to store them the

00:10:38,029 --> 00:10:42,740
matrix in a time series that database

00:10:40,670 --> 00:10:45,620
probably so I don't know like in

00:10:42,740 --> 00:10:48,290
graffiti or in flux eb or or some other

00:10:45,620 --> 00:10:51,260
some other but because we were

00:10:48,290 --> 00:10:54,110
interested in in troubleshooting we also

00:10:51,260 --> 00:10:55,850
want to store the transactions the

00:10:54,110 --> 00:10:57,860
actual transactions what's going on so

00:10:55,850 --> 00:11:00,199
that then when you see a graph you can

00:10:57,860 --> 00:11:05,300
actually drill down with two into what's

00:11:00,199 --> 00:11:07,610
what's wrong so for that you also would

00:11:05,300 --> 00:11:09,199
need to store the transaction and you do

00:11:07,610 --> 00:11:14,029
that I don't know mysql postgresql

00:11:09,199 --> 00:11:17,150
whatever the problem I already said this

00:11:14,029 --> 00:11:18,620
with this also from the implementation

00:11:17,150 --> 00:11:20,450
point of view this makes drilling down

00:11:18,620 --> 00:11:23,750
difficult right you'll need to somehow

00:11:20,450 --> 00:11:26,420
mark into the transactions what metrics

00:11:23,750 --> 00:11:28,370
they have associated so then you can you

00:11:26,420 --> 00:11:30,380
can implement this drill down and then

00:11:28,370 --> 00:11:33,140
features like top ten methods with

00:11:30,380 --> 00:11:36,230
errors are difficult to implement so I

00:11:33,140 --> 00:11:38,150
mean you can do it but it's pretty hard

00:11:36,230 --> 00:11:42,110
to maintain and I know this pretty well

00:11:38,150 --> 00:11:43,970
because I did this at a previous company

00:11:42,110 --> 00:11:46,190
where we did a monitoring product for

00:11:43,970 --> 00:11:48,260
voice over IP networks and it was this

00:11:46,190 --> 00:11:51,350
pretty crazy multi-threaded see

00:11:48,260 --> 00:11:53,390
application it was very fast but it got

00:11:51,350 --> 00:11:56,690
pretty difficult to maintain over time

00:11:53,390 --> 00:11:59,000
and for each new feature we need to add

00:11:56,690 --> 00:12:01,490
we needed to add more code and more code

00:11:59,000 --> 00:12:07,940
and things to just step out of handles a

00:12:01,490 --> 00:12:10,490
little bit right so the newer way let's

00:12:07,940 --> 00:12:14,870
say or our solution to this is to use

00:12:10,490 --> 00:12:16,820
some some of the good open source

00:12:14,870 --> 00:12:19,190
project that already exists in this area

00:12:16,820 --> 00:12:24,680
of analytics and the one that we chose

00:12:19,190 --> 00:12:27,770
is the elastic search or the ALK stack

00:12:24,680 --> 00:12:30,350
so its location cabana and the way it

00:12:27,770 --> 00:12:31,460
works is that the packet bit agents

00:12:30,350 --> 00:12:34,790
which you have installed on your

00:12:31,460 --> 00:12:37,490
application servers will insert those

00:12:34,790 --> 00:12:39,410
JSON objects into elasticsearch the

00:12:37,490 --> 00:12:42,110
middle part here with ready San logstash

00:12:39,410 --> 00:12:43,730
is actually optional you could configure

00:12:42,110 --> 00:12:47,870
the agents to write directly in the

00:12:43,730 --> 00:12:51,140
elastic search if you want but you yeah

00:12:47,870 --> 00:12:52,760
this is the way the better way let's say

00:12:51,140 --> 00:12:54,470
because then you get a bit of buffering

00:12:52,760 --> 00:12:57,260
there in case the elastic search is busy

00:12:54,470 --> 00:12:59,360
or whatever and you also have the

00:12:57,260 --> 00:13:01,880
opportunity to use log stash to modify

00:12:59,360 --> 00:13:04,280
the data if you need to add a tag if you

00:13:01,880 --> 00:13:05,900
need to rename a field whatever you have

00:13:04,280 --> 00:13:07,910
the opportunity to do that with log

00:13:05,900 --> 00:13:13,160
stash and then you have the Cabana web

00:13:07,910 --> 00:13:17,660
interface alright why why did you choose

00:13:13,160 --> 00:13:21,020
to use the ALK because it was it's

00:13:17,660 --> 00:13:22,910
already proven to to scale and perform

00:13:21,020 --> 00:13:27,110
well with logs there are there are many

00:13:22,910 --> 00:13:30,649
many users of this tag and our use cases

00:13:27,110 --> 00:13:35,119
is in many ways similar with the

00:13:30,649 --> 00:13:37,339
with the with the locks k is actually

00:13:35,119 --> 00:13:39,139
right i mean logs are also usually they

00:13:37,339 --> 00:13:43,309
get into a json object and they get

00:13:39,139 --> 00:13:45,829
inserted into the database and so on and

00:13:43,309 --> 00:13:48,800
the thing that attracted us to this

00:13:45,829 --> 00:13:51,529
solution is that the the flow is simple

00:13:48,800 --> 00:13:54,920
right you have the agents they create

00:13:51,529 --> 00:13:57,410
data you have optionally logstash to

00:13:54,920 --> 00:13:59,749
transport it you have elasticsearch to

00:13:57,410 --> 00:14:01,790
index it and then keep on ax to

00:13:59,749 --> 00:14:04,160
visualize right it's a very simple float

00:14:01,790 --> 00:14:07,129
and when you're doing a monitoring

00:14:04,160 --> 00:14:09,050
product that this is quite an important

00:14:07,129 --> 00:14:12,259
feature especially if it's open source

00:14:09,050 --> 00:14:14,240
and you want people to adopt it it needs

00:14:12,259 --> 00:14:16,759
to be easy to understand and easy to

00:14:14,240 --> 00:14:18,350
follow what's going on for for when

00:14:16,759 --> 00:14:20,660
things don't work you want to quickly

00:14:18,350 --> 00:14:23,829
check with it is it really not working

00:14:20,660 --> 00:14:27,439
or it's just the monitoring system

00:14:23,829 --> 00:14:28,939
reporting things weirdly so then you

00:14:27,439 --> 00:14:31,639
need to have something easy to

00:14:28,939 --> 00:14:37,639
understand to see quickly what's what's

00:14:31,639 --> 00:14:40,249
wrong and then you don't have to pre pre

00:14:37,639 --> 00:14:41,990
create the matrix right so especially in

00:14:40,249 --> 00:14:44,809
troubleshooting it's important that you

00:14:41,990 --> 00:14:47,029
can explore the data right you can you

00:14:44,809 --> 00:14:49,279
can start from whatever dashboard you

00:14:47,029 --> 00:14:52,220
have and then choose a different filter

00:14:49,279 --> 00:14:54,679
and so on so that you can get down to

00:14:52,220 --> 00:15:00,740
two what's wrong right without having to

00:14:54,679 --> 00:15:02,990
add code to create new matrix okay the

00:15:00,740 --> 00:15:05,899
ad hoc troubleshooting analytics which

00:15:02,990 --> 00:15:10,100
gabbana the queue manager is a nice web

00:15:05,899 --> 00:15:13,579
interface for doing this I I will go

00:15:10,100 --> 00:15:15,529
through their mobile bit later drilling

00:15:13,579 --> 00:15:18,529
down to the problematic transaction is

00:15:15,529 --> 00:15:21,379
very easy because every every

00:15:18,529 --> 00:15:23,749
visualization of a graph every metric is

00:15:21,379 --> 00:15:26,209
based on the actual data so it's very

00:15:23,749 --> 00:15:28,429
easy to just use the same filter or the

00:15:26,209 --> 00:15:30,429
same search to get the actual data and

00:15:28,429 --> 00:15:33,290
that's very useful for troubleshooting

00:15:30,429 --> 00:15:35,720
and things like top and features are

00:15:33,290 --> 00:15:37,910
also trivially by using elastic search

00:15:35,720 --> 00:15:40,279
aggregations so it's just an elastic

00:15:37,910 --> 00:15:45,370
search feature which you can use for for

00:15:40,279 --> 00:15:45,370
free right

00:15:45,430 --> 00:15:52,010
and and then it's easy to slice based on

00:15:50,240 --> 00:15:55,340
different dimension what I mean by that

00:15:52,010 --> 00:15:57,770
is let's say you want you have the top

00:15:55,340 --> 00:16:01,970
and host but you also want to see the

00:15:57,770 --> 00:16:04,250
top and URLs or the top and services or

00:16:01,970 --> 00:16:06,290
these are different dimensions right and

00:16:04,250 --> 00:16:08,810
you can you can easily slice the data by

00:16:06,290 --> 00:16:12,770
by one or the other and about new ones

00:16:08,810 --> 00:16:22,580
whatever okay we can switch to to the

00:16:12,770 --> 00:16:25,810
demo again so i have i have Kabana

00:16:22,580 --> 00:16:29,180
running here and if i if i refresh i

00:16:25,810 --> 00:16:33,170
should see their the request that i just

00:16:29,180 --> 00:16:36,050
did right so in reversing order there

00:16:33,170 --> 00:16:41,480
was the the slip command which which

00:16:36,050 --> 00:16:44,450
took three seconds the error here you

00:16:41,480 --> 00:16:47,690
can see that the status is error and

00:16:44,450 --> 00:16:49,280
then whatever what I what did I do can't

00:16:47,690 --> 00:16:53,140
even see that was one that actually took

00:16:49,280 --> 00:16:57,410
quite long which is interesting and

00:16:53,140 --> 00:17:01,900
these are the HTTP requests that ID to

00:16:57,410 --> 00:17:05,180
google.com and the rest word test runs

00:17:01,900 --> 00:17:11,270
let me know switch to some data I

00:17:05,180 --> 00:17:18,320
created yesterday to the to show other

00:17:11,270 --> 00:17:20,240
things right so here you can see again

00:17:18,320 --> 00:17:26,720
what what protocols we support so

00:17:20,240 --> 00:17:32,060
postgresql HTTP ok let me zoom into here

00:17:26,720 --> 00:17:34,270
we also support read this and thrift RPC

00:17:32,060 --> 00:17:36,440
which is interesting also for this

00:17:34,270 --> 00:17:38,450
microservices case you can have

00:17:36,440 --> 00:17:41,690
microservices talking three-star

00:17:38,450 --> 00:17:45,320
passivity in them and then we can show

00:17:41,690 --> 00:17:48,350
those right and you can search of course

00:17:45,320 --> 00:17:52,240
and you can do structured search so

00:17:48,350 --> 00:17:54,980
let's say I search for type my sequel

00:17:52,240 --> 00:17:57,600
and then usually what I would be

00:17:54,980 --> 00:18:00,390
interested in is to look for error

00:17:57,600 --> 00:18:04,610
actions right so these are these are all

00:18:00,390 --> 00:18:07,410
okay generally speaking but i can set

00:18:04,610 --> 00:18:10,080
okay let's set a filter by my school as

00:18:07,410 --> 00:18:16,140
well and I can do this contextually here

00:18:10,080 --> 00:18:20,340
i can set status is not okay and I have

00:18:16,140 --> 00:18:23,340
them here this I don't need right so

00:18:20,340 --> 00:18:25,980
that the filters are set here and I now

00:18:23,340 --> 00:18:29,940
I can see only the errors and what's

00:18:25,980 --> 00:18:31,860
interesting here is that the way we do

00:18:29,940 --> 00:18:35,000
it with packet bit and this is a bit

00:18:31,860 --> 00:18:37,679
different from logs is that we unify the

00:18:35,000 --> 00:18:40,169
format from all these protocols right so

00:18:37,679 --> 00:18:42,120
the status is is not just my sequel

00:18:40,169 --> 00:18:46,530
status is the status for any type of

00:18:42,120 --> 00:18:50,250
transaction so when I filter for for

00:18:46,530 --> 00:18:53,820
error like let's disable this my sequel

00:18:50,250 --> 00:18:57,090
one I get really old errors regardless

00:18:53,820 --> 00:19:01,049
of the protocols so you see they are

00:18:57,090 --> 00:19:03,570
drift and so on okay and you I can also

00:19:01,049 --> 00:19:06,270
see you see the chart here updates every

00:19:03,570 --> 00:19:12,419
time I change something so they are like

00:19:06,270 --> 00:19:17,720
200 errors per minute generally speaking

00:19:12,419 --> 00:19:20,909
and if I enable the my sequel filter

00:19:17,720 --> 00:19:22,620
then they are only like around 20 or

00:19:20,909 --> 00:19:24,900
something lightly so you can quickly get

00:19:22,620 --> 00:19:31,640
an idea of the size of the problem let's

00:19:24,900 --> 00:19:35,370
say and yeah in this in this case you

00:19:31,640 --> 00:19:38,549
notice that most of them has this bug 66

00:19:35,370 --> 00:19:41,669
in them like a keyword so I can search

00:19:38,549 --> 00:19:44,250
for that just to be sure if these are

00:19:41,669 --> 00:19:47,970
these are really old all the all the

00:19:44,250 --> 00:19:50,880
arrows that I have so yeah it's a search

00:19:47,970 --> 00:19:54,000
engine after all elastic search so you

00:19:50,880 --> 00:19:56,400
can search by any field you can also

00:19:54,000 --> 00:19:59,760
search by the way by an IP addresses and

00:19:56,400 --> 00:20:02,760
IP address so i can do like I don't know

00:19:59,760 --> 00:20:06,210
what I used here like like this right it

00:20:02,760 --> 00:20:08,510
whoops okay maybe I don't even use that

00:20:06,210 --> 00:20:08,510
one

00:20:11,130 --> 00:20:17,730
alright so yeah now it will search the

00:20:15,040 --> 00:20:21,460
IP in all fields that are relevant and

00:20:17,730 --> 00:20:23,260
keep on for is quite nice that it also

00:20:21,460 --> 00:20:25,630
highlights right it highlights the

00:20:23,260 --> 00:20:28,120
matches so you can quickly see where it

00:20:25,630 --> 00:20:39,330
saw it it will it will also work is if

00:20:28,120 --> 00:20:42,370
it sees it in the message all right yeah

00:20:39,330 --> 00:20:45,190
now let's go a little bit to the

00:20:42,370 --> 00:20:58,660
dashboards I know there was something

00:20:45,190 --> 00:21:00,850
else back here ya go help ok so the

00:20:58,660 --> 00:21:05,679
question is if we support encrypted

00:21:00,850 --> 00:21:09,630
protocols no that's a disadvantage there

00:21:05,679 --> 00:21:13,590
are some potential solutions for that

00:21:09,630 --> 00:21:16,450
one of them that some some commercial

00:21:13,590 --> 00:21:20,650
competitors let's say do is that you

00:21:16,450 --> 00:21:23,290
give them the the certificates and they

00:21:20,650 --> 00:21:25,840
will decrypt the data on the fly so that

00:21:23,290 --> 00:21:28,330
would be a possibility I'm not that much

00:21:25,840 --> 00:21:33,040
of a fan of it because I don't know if

00:21:28,330 --> 00:21:35,410
this is really nice thing to do another

00:21:33,040 --> 00:21:41,590
possibility that we would prefer is to

00:21:35,410 --> 00:21:43,570
have a proxy like you know you usually

00:21:41,590 --> 00:21:46,120
want this when you want to monitor for

00:21:43,570 --> 00:21:47,440
parties right so you use an API from I

00:21:46,120 --> 00:21:49,330
don't know let's say from Twitter and

00:21:47,440 --> 00:21:51,429
you want to monitor that right and then

00:21:49,330 --> 00:21:53,559
you could use a Twitter that essentially

00:21:51,429 --> 00:21:56,110
logs the transactions but it will load

00:21:53,559 --> 00:22:00,040
them with the payloads as well so we

00:21:56,110 --> 00:22:01,690
like you would having in front like you

00:22:00,040 --> 00:22:05,350
would have some packet bit so this would

00:22:01,690 --> 00:22:07,420
be the solutions that we have but yeah

00:22:05,350 --> 00:22:09,309
as I said there are there are cases

00:22:07,420 --> 00:22:11,170
where it's more convenient to just use

00:22:09,309 --> 00:22:13,360
the logs right like for example if you

00:22:11,170 --> 00:22:16,840
have an ng anak server that's just serve

00:22:13,360 --> 00:22:18,610
statics file and you have the the logs

00:22:16,840 --> 00:22:21,370
from it is just more convenient to use

00:22:18,610 --> 00:22:22,690
those than to use wire data right but

00:22:21,370 --> 00:22:24,760
they are also

00:22:22,690 --> 00:22:26,350
cases where it's more convenient to use

00:22:24,760 --> 00:22:28,810
wire data I will go through the

00:22:26,350 --> 00:22:32,140
advantages and disadvantages later on in

00:22:28,810 --> 00:22:34,540
the talk actually yeah so something else

00:22:32,140 --> 00:22:38,860
I wanted to say a hit to show here is

00:22:34,540 --> 00:22:42,340
that you can also filter for things not

00:22:38,860 --> 00:22:45,790
containing something search for things

00:22:42,340 --> 00:22:51,820
not containing something and why isn't

00:22:45,790 --> 00:22:54,460
this work okay this this doesn't contain

00:22:51,820 --> 00:22:56,830
but I thought I have this filters here

00:22:54,460 --> 00:23:00,220
right so it works exactly liking google

00:22:56,830 --> 00:23:01,360
right you say- something and it will

00:23:00,220 --> 00:23:07,020
search for everything that doesn't

00:23:01,360 --> 00:23:10,630
contain that those keywords yeah and

00:23:07,020 --> 00:23:13,930
another thing to mention here in the

00:23:10,630 --> 00:23:16,600
configuration file is that you cannot

00:23:13,930 --> 00:23:20,710
enable more more data so by default we

00:23:16,600 --> 00:23:24,550
we we don't store that much data because

00:23:20,710 --> 00:23:28,990
of privacy con concerns so you wouldn't

00:23:24,550 --> 00:23:31,170
want to accidentally you know put in I

00:23:28,990 --> 00:23:34,420
don't know bank account data or sewn

00:23:31,170 --> 00:23:37,120
into elasticsearch so by default we

00:23:34,420 --> 00:23:40,240
leave this these this send requests

00:23:37,120 --> 00:23:45,070
understand response to to false but you

00:23:40,240 --> 00:23:50,410
can you can switch them to true and then

00:23:45,070 --> 00:23:57,820
we will record the whole payload let's

00:23:50,410 --> 00:23:59,770
do a quick example of that one it was

00:23:57,820 --> 00:24:02,280
for HTTP so I have to do this again

00:23:59,770 --> 00:24:02,280
again

00:24:06,240 --> 00:24:10,860
and you see there's it is more here now

00:24:08,940 --> 00:24:12,630
you have all the headers you still don't

00:24:10,860 --> 00:24:16,320
have the body it's a different option

00:24:12,630 --> 00:24:19,950
where you can select which bodies you

00:24:16,320 --> 00:24:22,110
want to save by by by content type so

00:24:19,950 --> 00:24:25,559
you can say for example that you want to

00:24:22,110 --> 00:24:28,020
store everything that's Jason but you

00:24:25,559 --> 00:24:30,240
don't want to save like pictures and so

00:24:28,020 --> 00:24:33,440
on right which is just it depends what

00:24:30,240 --> 00:24:38,010
makes sense in your in your use case

00:24:33,440 --> 00:24:40,980
okay and now let's get to what I think

00:24:38,010 --> 00:24:45,809
it's more interesting or at least nicer

00:24:40,980 --> 00:24:48,960
to show that the dashboards that we have

00:24:45,809 --> 00:24:53,270
so packet beat the project sort of comes

00:24:48,960 --> 00:24:55,740
with a few predefined Cabana dashboards

00:24:53,270 --> 00:24:58,500
just to make it easier for you to start

00:24:55,740 --> 00:25:00,540
right so it does common common

00:24:58,500 --> 00:25:03,809
visualizations that you would typically

00:25:00,540 --> 00:25:07,170
expect from from one from an ATM type of

00:25:03,809 --> 00:25:11,220
product so you have here this is the

00:25:07,170 --> 00:25:17,190
truth let the twist dashboard let me go

00:25:11,220 --> 00:25:22,850
to the befo one loading alright so you

00:25:17,190 --> 00:25:25,640
have the the total total amount of HTTP

00:25:22,850 --> 00:25:28,620
transactions and the total amount of

00:25:25,640 --> 00:25:31,500
database transaction right postgres and

00:25:28,620 --> 00:25:35,880
SQL the same for for for cash flow

00:25:31,500 --> 00:25:38,070
readies and RPC we could also do here

00:25:35,880 --> 00:25:40,380
the the sum of the response times so

00:25:38,070 --> 00:25:43,470
then you could see like in a single

00:25:40,380 --> 00:25:45,900
visualization where the time is mostly

00:25:43,470 --> 00:25:47,850
spent like if it's spent in application

00:25:45,900 --> 00:25:50,910
layer or if it spent in the database

00:25:47,850 --> 00:25:55,070
layer and so on actually I will show

00:25:50,910 --> 00:25:58,830
this later if we have time then we have

00:25:55,070 --> 00:26:00,600
response times per sentence this is also

00:25:58,830 --> 00:26:05,250
an elastic search aggregation which we

00:26:00,600 --> 00:26:09,080
make use of you can tell it to two okay

00:26:05,250 --> 00:26:12,270
on this set of on the set of

00:26:09,080 --> 00:26:14,309
measurements show me the percentage

00:26:12,270 --> 00:26:18,720
rights of the seventy five percent till

00:26:14,309 --> 00:26:20,160
or the ninety-nine percent which I think

00:26:18,720 --> 00:26:23,730
you all know what it means in

00:26:20,160 --> 00:26:29,100
is that 99% of the of the transactions

00:26:23,730 --> 00:26:32,810
are faster than this right and then

00:26:29,100 --> 00:26:36,720
error rate right so what percentage of

00:26:32,810 --> 00:26:39,600
of transactions are errors the

00:26:36,720 --> 00:26:42,900
evolutionary of errors the latency

00:26:39,600 --> 00:26:46,470
diagram this is also interesting so you

00:26:42,900 --> 00:26:48,570
can get here you can see that well most

00:26:46,470 --> 00:26:52,470
of the transactions are below 20

00:26:48,570 --> 00:26:56,100
milliseconds write error codes and so on

00:26:52,470 --> 00:26:59,700
and now let me switch to the to the

00:26:56,100 --> 00:27:03,270
thrift RPC performance one nope this is

00:26:59,700 --> 00:27:07,370
what I wanted to do to also show here we

00:27:03,270 --> 00:27:11,040
have a few of this example top 10

00:27:07,370 --> 00:27:17,340
widgets or visualizations right so here

00:27:11,040 --> 00:27:19,470
they are the top 10 or top 5 RPC methods

00:27:17,340 --> 00:27:23,190
used in the system right so you have

00:27:19,470 --> 00:27:26,310
things if calculate and so on and you

00:27:23,190 --> 00:27:29,910
can also have the top 10 or the top

00:27:26,310 --> 00:27:32,130
whatever that have errors right so by

00:27:29,910 --> 00:27:34,800
our binder number of errors top 10 by

00:27:32,130 --> 00:27:37,170
the number of errors so it's quickly you

00:27:34,800 --> 00:27:42,150
get a quick overview of what's going

00:27:37,170 --> 00:27:44,580
wrong and here you have the slowest one

00:27:42,150 --> 00:27:49,230
right by average response time so we can

00:27:44,580 --> 00:27:52,260
also do that and you can do this also

00:27:49,230 --> 00:27:56,670
for like for example for my signal for

00:27:52,260 --> 00:28:00,450
postgresql things line is right so this

00:27:56,670 --> 00:28:02,280
is the the my sequel one okay another

00:28:00,450 --> 00:28:05,850
thing that i wanted to show so all this

00:28:02,280 --> 00:28:09,090
was Kabana for if you go for example to

00:28:05,850 --> 00:28:12,270
our demo site i won on packet be calm

00:28:09,090 --> 00:28:14,460
you'll see that we we still have Kabana

00:28:12,270 --> 00:28:17,700
free there and the reason is that we

00:28:14,460 --> 00:28:20,460
added this this this widget to to keep

00:28:17,700 --> 00:28:23,670
won a free we call it a forced layout

00:28:20,460 --> 00:28:25,080
but it it's it's it's it's a it's I

00:28:23,670 --> 00:28:27,600
think it's a good widget because it

00:28:25,080 --> 00:28:30,060
shows you the exactly the communication

00:28:27,600 --> 00:28:31,950
right so you can see here how the

00:28:30,060 --> 00:28:32,610
traffic is coming right to have this

00:28:31,950 --> 00:28:35,760
come

00:28:32,610 --> 00:28:39,179
from unknown IP addresses going to ng

00:28:35,760 --> 00:28:42,900
anak servers and from ng annex to two

00:28:39,179 --> 00:28:45,210
other application server and you I hear

00:28:42,900 --> 00:28:48,120
it's configured to use the the server

00:28:45,210 --> 00:28:51,179
names but you could also do it by IP

00:28:48,120 --> 00:28:52,950
addresses biological service by whatever

00:28:51,179 --> 00:28:55,230
field you want right the widget is

00:28:52,950 --> 00:28:58,260
pretty flexible and you have here also

00:28:55,230 --> 00:29:02,059
the the vagrant stuff which is probably

00:28:58,260 --> 00:29:06,090
due to the test that that I just did

00:29:02,059 --> 00:29:08,790
probably right and I was saying that

00:29:06,090 --> 00:29:11,700
this is we only have in cabana free for

00:29:08,790 --> 00:29:13,740
now unfortunately we're looking for the

00:29:11,700 --> 00:29:16,260
best way to implement this in cabana for

00:29:13,740 --> 00:29:21,690
as well Sookie wanna for will will

00:29:16,260 --> 00:29:23,880
support plugins which means that you

00:29:21,690 --> 00:29:27,360
have people like us can create the new

00:29:23,880 --> 00:29:30,030
widgets for it but the plug-in interface

00:29:27,360 --> 00:29:33,480
is not yet ready rights a big warning

00:29:30,030 --> 00:29:36,419
when you when you open it in the readme

00:29:33,480 --> 00:29:38,429
file in the in the in the gig harbor

00:29:36,419 --> 00:29:44,250
repo saying don't use this yet because

00:29:38,429 --> 00:29:50,190
it's not stable okay so actually what we

00:29:44,250 --> 00:29:52,530
did then was to work on on on our own

00:29:50,190 --> 00:29:56,400
new interface but i'll talk about that

00:29:52,530 --> 00:29:59,880
later now because we're pretty good with

00:29:56,400 --> 00:30:02,460
the time let's create a visualization a

00:29:59,880 --> 00:30:06,000
new visualization just to show you how

00:30:02,460 --> 00:30:10,380
that works and the one that i want to

00:30:06,000 --> 00:30:13,169
create is I don't know if you know New

00:30:10,380 --> 00:30:15,179
Relic one of the first thing first

00:30:13,169 --> 00:30:18,030
things that you see when you log in is

00:30:15,179 --> 00:30:20,100
this what I said earlier the amount of

00:30:18,030 --> 00:30:22,440
time spent in each layer of your

00:30:20,100 --> 00:30:25,679
application let's say so let's try to do

00:30:22,440 --> 00:30:31,130
that just pick the default search here

00:30:25,679 --> 00:30:37,260
and the aggregation that i will use is

00:30:31,130 --> 00:30:40,010
the sum of response times right and i

00:30:37,260 --> 00:30:40,010
can apply that

00:30:40,800 --> 00:30:56,340
okay what did I do wrong my gay

00:30:52,530 --> 00:31:00,770
refreshing probably doesn't help hey

00:30:56,340 --> 00:31:03,770
what if I do some of something else

00:31:00,770 --> 00:31:03,770
count

00:31:07,310 --> 00:31:14,390
okay let me check the filter is correct

00:31:14,930 --> 00:31:23,100
maybe this apply somehow ok I'll try

00:31:21,660 --> 00:31:31,110
once more if it doesn't work doesn't

00:31:23,100 --> 00:31:32,880
work let's do a new search I think yeah

00:31:31,110 --> 00:31:35,310
I think what I did wrong was that I just

00:31:32,880 --> 00:31:37,530
tried to do it too early and it was not

00:31:35,310 --> 00:31:39,780
he didn't have all the information that

00:31:37,530 --> 00:31:41,340
you need so I also need to set what what

00:31:39,780 --> 00:31:44,340
we want to show on the x-axis or

00:31:41,340 --> 00:31:49,140
obviously so I want the the time on the

00:31:44,340 --> 00:31:52,710
x-axis so in Cuban I you that means you

00:31:49,140 --> 00:31:58,740
will want a date histogram by field

00:31:52,710 --> 00:32:02,040
timestamp right so now we have the the

00:31:58,740 --> 00:32:03,840
total amount of of off time spent in

00:32:02,040 --> 00:32:07,710
your application that you need for that

00:32:03,840 --> 00:32:09,600
it's not that important it's in in

00:32:07,710 --> 00:32:12,030
milliseconds but it's just the total

00:32:09,600 --> 00:32:15,180
amount so it's not that relevant but

00:32:12,030 --> 00:32:19,890
then what you can do is add a sub

00:32:15,180 --> 00:32:22,820
aggregation and here we'll split it area

00:32:19,890 --> 00:32:27,170
so we create one of these tech charts

00:32:22,820 --> 00:32:30,780
and the suburb irrigation will do terms

00:32:27,170 --> 00:32:33,660
because I want to separate this by by

00:32:30,780 --> 00:32:36,960
the type by the type of the transaction

00:32:33,660 --> 00:32:39,240
like this and if I apply it now now you

00:32:36,960 --> 00:32:45,540
have that yeah it's cool looks quite

00:32:39,240 --> 00:32:47,400
nice you have the time spent in each

00:32:45,540 --> 00:32:50,130
layer of your application that's that

00:32:47,400 --> 00:32:58,260
that's the idea all right any questions

00:32:50,130 --> 00:33:01,950
so far all right continuing with the

00:32:58,260 --> 00:33:04,980
presentation then I'll skip this because

00:33:01,950 --> 00:33:07,740
it's just what i already showed you so

00:33:04,980 --> 00:33:11,730
let's talk now about the pros and cons

00:33:07,740 --> 00:33:14,930
of where they died versus other ways of

00:33:11,730 --> 00:33:17,030
getting data about your your

00:33:14,930 --> 00:33:20,660
your transactions sort of the things

00:33:17,030 --> 00:33:22,520
that happens in your in your systems so

00:33:20,660 --> 00:33:24,830
one important advantage is that it

00:33:22,520 --> 00:33:27,140
captures things that you wouldn't expect

00:33:24,830 --> 00:33:30,140
for most other things you need to do

00:33:27,140 --> 00:33:33,530
something to get this data right like

00:33:30,140 --> 00:33:35,960
four logs you need to well at least

00:33:33,530 --> 00:33:41,930
enable them in your application in you

00:33:35,960 --> 00:33:44,990
need to write the the right logs and and

00:33:41,930 --> 00:33:47,660
so on right but with with wire data if

00:33:44,990 --> 00:33:49,310
something happens and it's in a protocol

00:33:47,660 --> 00:33:54,620
that we can understand then it will for

00:33:49,310 --> 00:33:57,530
sure show up right another thing is that

00:33:54,620 --> 00:34:02,480
you don't need to to do any kind of of

00:33:57,530 --> 00:34:04,220
code changes and not generally don't

00:34:02,480 --> 00:34:07,010
touch the application at all right so

00:34:04,220 --> 00:34:09,950
with with with tools like New Relic or f

00:34:07,010 --> 00:34:14,900
dynamics you have to run an agent in

00:34:09,950 --> 00:34:16,880
your inner in your jvm for example but

00:34:14,900 --> 00:34:22,400
with where they die we're completely

00:34:16,880 --> 00:34:25,550
passive right this means if you if you

00:34:22,400 --> 00:34:28,070
troubleshoot a system that's that's

00:34:25,550 --> 00:34:32,630
monitored by by packet bit or whatever

00:34:28,070 --> 00:34:34,370
where data system you need only minimal

00:34:32,630 --> 00:34:37,460
knowledge about what the application is

00:34:34,370 --> 00:34:39,440
doing right you don't in other tools you

00:34:37,460 --> 00:34:41,270
need to know more about the internals of

00:34:39,440 --> 00:34:44,210
application but here because the

00:34:41,270 --> 00:34:46,610
protocols are kind of universal UK you

00:34:44,210 --> 00:34:50,810
can understand and use it without

00:34:46,610 --> 00:34:52,850
without that much knowledge there's no

00:34:50,810 --> 00:34:57,350
latency overhead because we work on a

00:34:52,850 --> 00:35:00,590
copy of the traffic so yeah that's

00:34:57,350 --> 00:35:02,930
important with logs the overhead is is

00:35:00,590 --> 00:35:06,050
minimal so we probably wouldn't care

00:35:02,930 --> 00:35:09,550
about that too much but with a PM tools

00:35:06,050 --> 00:35:11,900
the the latency overhead can be can be

00:35:09,550 --> 00:35:16,040
significant in some cases so you have to

00:35:11,900 --> 00:35:19,160
be careful there and then if you use

00:35:16,040 --> 00:35:22,370
like you know if you are on premise so

00:35:19,160 --> 00:35:25,130
you don't use the agent but you use tap

00:35:22,370 --> 00:35:27,650
points or mirror ports to to extract the

00:35:25,130 --> 00:35:28,730
traffic from the network then you are

00:35:27,650 --> 00:35:32,600
you don't need

00:35:28,730 --> 00:35:34,220
the sort of CPU or memory consumption on

00:35:32,600 --> 00:35:36,710
your application server so that's

00:35:34,220 --> 00:35:39,290
extremely passive right you could have a

00:35:36,710 --> 00:35:41,180
system you could have a fully

00:35:39,290 --> 00:35:44,390
proprietary system like an appliance

00:35:41,180 --> 00:35:46,730
that speaks HTTP and you would still be

00:35:44,390 --> 00:35:50,180
able to monitor it this way right and

00:35:46,730 --> 00:35:52,160
measure its performance and so on but

00:35:50,180 --> 00:35:55,609
there are some some disadvantages as

00:35:52,160 --> 00:35:58,880
well first there can be like tons of

00:35:55,609 --> 00:36:02,000
that data will i'll talk a bit later

00:35:58,880 --> 00:36:04,310
about how can we can address this i'm

00:36:02,000 --> 00:36:07,040
saying it can be a lot of data because

00:36:04,310 --> 00:36:10,280
imagine you have a ready server it does

00:36:07,040 --> 00:36:13,070
one hundred thousand requests per second

00:36:10,280 --> 00:36:17,390
only like a single normal instance of

00:36:13,070 --> 00:36:19,520
Redis and you will need to store all of

00:36:17,390 --> 00:36:23,750
those in elasticsearch can imagine this

00:36:19,520 --> 00:36:27,740
gets this gets to a large number quite

00:36:23,750 --> 00:36:32,600
quickly compared to low processing it

00:36:27,740 --> 00:36:34,130
does consume somehow more CPU because we

00:36:32,600 --> 00:36:35,480
need to do sniffing which it doesn't

00:36:34,130 --> 00:36:37,190
come for free because we need to copy

00:36:35,480 --> 00:36:39,350
the package from from from the colonel

00:36:37,190 --> 00:36:43,970
and we need to parse the protocols front

00:36:39,350 --> 00:36:47,060
so it's a bit a bit more CPU usage there

00:36:43,970 --> 00:36:50,510
are privacy concerns as I as I said you

00:36:47,060 --> 00:36:53,780
it could be that you you log things that

00:36:50,510 --> 00:36:57,080
you didn't intend to we also have a plan

00:36:53,780 --> 00:36:59,420
about how to improve that doesn't work

00:36:57,080 --> 00:37:02,030
with ink with encrypt protocols this was

00:36:59,420 --> 00:37:05,000
already a question it we've discussed it

00:37:02,030 --> 00:37:08,210
and doesn't work for your own protocols

00:37:05,000 --> 00:37:12,260
right so if you use a if you decide to

00:37:08,210 --> 00:37:14,150
develop your own protocol packet it

00:37:12,260 --> 00:37:18,820
cannot understand it unless you

00:37:14,150 --> 00:37:21,340
contribute the the parsing go to it

00:37:18,820 --> 00:37:23,930
right so these were the pros and cons

00:37:21,340 --> 00:37:27,470
let me know if you can think of others

00:37:23,930 --> 00:37:32,720
pros or cons or if you have any other

00:37:27,470 --> 00:37:37,130
questions overall now okay we go on so I

00:37:32,720 --> 00:37:42,200
wanted to go a bit into what our what

00:37:37,130 --> 00:37:43,760
our our future plans so

00:37:42,200 --> 00:37:46,880
first of all more protocols the ones

00:37:43,760 --> 00:37:50,750
available today are HTTP MySQL

00:37:46,880 --> 00:37:55,790
PostgreSQL ready centrist RPC and we

00:37:50,750 --> 00:37:59,119
want to work like soon on the ones on

00:37:55,790 --> 00:38:03,079
the right so dns and memcache will will

00:37:59,119 --> 00:38:05,060
probably be quite easy to do there are

00:38:03,079 --> 00:38:08,450
lots of people requesting all sorts of

00:38:05,060 --> 00:38:12,920
database protocols like MongoDB riffing

00:38:08,450 --> 00:38:14,810
to be my oracle microsoft SQL these are

00:38:12,920 --> 00:38:19,099
generally more complex right so it will

00:38:14,810 --> 00:38:22,460
probably take us more time excel Mel RPC

00:38:19,099 --> 00:38:28,130
json-rpc are also useful again if you

00:38:22,460 --> 00:38:32,060
have some sort of SOA or whatever andia

00:38:28,130 --> 00:38:37,640
if you have any suggestions let me know

00:38:32,060 --> 00:38:41,260
we could do MQTT for example right and

00:38:37,640 --> 00:38:43,790
we are also working on creating a

00:38:41,260 --> 00:38:47,380
contributors guide for adding a new

00:38:43,790 --> 00:38:50,300
protocols it's not ready yet but

00:38:47,380 --> 00:38:53,720
hopefully this will will make it easy

00:38:50,300 --> 00:38:57,069
enough to do to add your support support

00:38:53,720 --> 00:39:00,200
for whatever things you need yourself

00:38:57,069 --> 00:39:03,200
all right another thing this this

00:39:00,200 --> 00:39:05,690
relates with the problem that that the

00:39:03,200 --> 00:39:10,910
wire data can generate really large

00:39:05,690 --> 00:39:12,890
amounts of of objects the way you think

00:39:10,910 --> 00:39:17,000
a pretty good solution is to have

00:39:12,890 --> 00:39:20,290
sampling which means that we would only

00:39:17,000 --> 00:39:24,680
store parts of it and we will record

00:39:20,290 --> 00:39:27,470
what what the sampling rate is for each

00:39:24,680 --> 00:39:30,200
transactions that we saw restore so this

00:39:27,470 --> 00:39:32,630
will balance a bit troubleshooting

00:39:30,200 --> 00:39:35,930
convenience with the hardware

00:39:32,630 --> 00:39:37,520
requirements right so how much this pace

00:39:35,930 --> 00:39:41,270
you want to give to elastic searches

00:39:37,520 --> 00:39:44,450
essentially of course if you have all

00:39:41,270 --> 00:39:47,030
the transactions then troubleshooting is

00:39:44,450 --> 00:39:49,520
is a pleasure because you can find

00:39:47,030 --> 00:39:52,520
whatever went wrong whenever we went

00:39:49,520 --> 00:39:55,130
wrong but if that's not economically

00:39:52,520 --> 00:39:56,030
feasible then you can use sampling and

00:39:55,130 --> 00:39:58,780
the way

00:39:56,030 --> 00:40:01,730
we want to do it is to have this

00:39:58,780 --> 00:40:04,760
flexible so you could do it for example

00:40:01,730 --> 00:40:06,950
different for per protocol so you could

00:40:04,760 --> 00:40:09,410
say that hey I want all the database

00:40:06,950 --> 00:40:12,290
requests but the reddest request which

00:40:09,410 --> 00:40:16,360
are really a lot of them only sample

00:40:12,290 --> 00:40:22,190
them or sample all reddy's get requests

00:40:16,360 --> 00:40:23,990
but write all the all the reddest

00:40:22,190 --> 00:40:25,700
requests that are actually modifying the

00:40:23,990 --> 00:40:30,200
data right that could that could also be

00:40:25,700 --> 00:40:35,030
useful by status code as well like store

00:40:30,200 --> 00:40:40,130
all the errors and only sample all the

00:40:35,030 --> 00:40:42,950
successes by response time so we could

00:40:40,130 --> 00:40:44,630
guarantee that we restore all all

00:40:42,950 --> 00:40:49,880
transactions that are taking longer than

00:40:44,630 --> 00:40:52,730
usual or finish line is right the next

00:40:49,880 --> 00:40:56,390
thing is a string of Fisk Asian this is

00:40:52,730 --> 00:40:59,900
to address the privacy concerns so

00:40:56,390 --> 00:41:02,270
because we parse parse these things we

00:40:59,900 --> 00:41:05,200
can also part the the SQL queries for

00:41:02,270 --> 00:41:09,380
example and wherever we find a stream

00:41:05,200 --> 00:41:11,330
string we would replace it with with

00:41:09,380 --> 00:41:14,080
something like like this like in the

00:41:11,330 --> 00:41:18,500
first example to door is replaced with

00:41:14,080 --> 00:41:20,990
s8 because it's a string and it has five

00:41:18,500 --> 00:41:23,930
letters and the next power of two is

00:41:20,990 --> 00:41:28,220
eight right so you'd you would use the

00:41:23,930 --> 00:41:30,830
next power of 82 to have s and the

00:41:28,220 --> 00:41:35,060
number and and the same with with the

00:41:30,830 --> 00:41:36,710
with the ID I did a mistake there it

00:41:35,060 --> 00:41:41,240
should be n for because it should also

00:41:36,710 --> 00:41:43,970
use the next power of off to the

00:41:41,240 --> 00:41:47,030
advantage of this is not only that it

00:41:43,970 --> 00:41:49,100
helps with with with making sure you

00:41:47,030 --> 00:41:52,310
don't store anything that you don't want

00:41:49,100 --> 00:41:54,830
to store but also when you have top and

00:41:52,310 --> 00:41:57,020
charts they would be automatically

00:41:54,830 --> 00:41:59,480
better right because if you have you

00:41:57,020 --> 00:42:01,700
have imagined you have a select query

00:41:59,480 --> 00:42:05,600
that's always the same but the first in

00:42:01,700 --> 00:42:07,580
one differs in one parameter then in the

00:42:05,600 --> 00:42:09,349
top query this will only show up as one

00:42:07,580 --> 00:42:12,229
right so it will make

00:42:09,349 --> 00:42:14,930
for a better widget for that this idea

00:42:12,229 --> 00:42:18,259
by the way is taken from from this book

00:42:14,930 --> 00:42:22,880
the mud mature optimization handbook by

00:42:18,259 --> 00:42:26,900
Carlos bueno it's it's a really good

00:42:22,880 --> 00:42:30,019
book from Carlos bueno is the Facebook

00:42:26,900 --> 00:42:32,210
engineer and it describes the system

00:42:30,019 --> 00:42:34,670
that's actually quite quite similar with

00:42:32,210 --> 00:42:37,160
packet bit in some ways so I do

00:42:34,670 --> 00:42:39,440
recommend this book and it's also free

00:42:37,160 --> 00:42:43,069
so it's a you can just download it as an

00:42:39,440 --> 00:42:46,339
ebook if you if you just google it hey I

00:42:43,069 --> 00:42:50,049
totally commented to read nap alright

00:42:46,339 --> 00:42:54,529
and I almost introduced this too early

00:42:50,049 --> 00:42:56,599
so bonito is the it's it's a UI we're

00:42:54,529 --> 00:42:59,749
working on but it's not yet released

00:42:56,599 --> 00:43:02,839
it's not yet ready the what we want to

00:42:59,749 --> 00:43:05,559
it to be is complementary to keep Anna

00:43:02,839 --> 00:43:09,890
that means it will be an interface

00:43:05,559 --> 00:43:12,769
that's more more focused on application

00:43:09,890 --> 00:43:15,920
performance and you can still use the

00:43:12,769 --> 00:43:19,839
very generic bana interface to create

00:43:15,920 --> 00:43:22,700
your own visualizations and so on

00:43:19,839 --> 00:43:25,099
hopefully at some point bonito will will

00:43:22,700 --> 00:43:29,869
become a Cabana for plug-in just to make

00:43:25,099 --> 00:43:32,900
it very easy to use for you and to

00:43:29,869 --> 00:43:39,140
switch between them for now we're

00:43:32,900 --> 00:43:40,819
developing in a acetone project I can

00:43:39,140 --> 00:43:47,089
show you that as well because we have

00:43:40,819 --> 00:43:49,569
time I just need to generate a bit of

00:43:47,089 --> 00:43:49,569
data

00:43:51,880 --> 00:44:04,340
all right let's do this okay so the idea

00:43:59,840 --> 00:44:06,020
the idea here in this view this is also

00:44:04,340 --> 00:44:10,940
inspired by somewhere if you know the

00:44:06,020 --> 00:44:14,090
the historic dashboard from netflix they

00:44:10,940 --> 00:44:16,520
also have this this green planet we call

00:44:14,090 --> 00:44:21,560
them which gives you the relative size

00:44:16,520 --> 00:44:24,590
so the idea here is to to have a quick

00:44:21,560 --> 00:44:28,280
way of comparing different dimensions

00:44:24,590 --> 00:44:33,770
right so a dimension as i said could be

00:44:28,280 --> 00:44:38,120
a logical service or it could be the

00:44:33,770 --> 00:44:41,150
host or it could be urls methods

00:44:38,120 --> 00:44:43,250
whatever right and the point is that you

00:44:41,150 --> 00:44:45,500
can easily put them all on the same on

00:44:43,250 --> 00:44:47,780
the same page and this is from the UI

00:44:45,500 --> 00:44:53,600
point of view is designed to scale to

00:44:47,780 --> 00:44:56,570
large number of of things and for each

00:44:53,600 --> 00:45:00,860
of them with we show some some some key

00:44:56,570 --> 00:45:03,770
metrics like the total volume some sub

00:45:00,860 --> 00:45:06,410
metric like how many hosts they are in

00:45:03,770 --> 00:45:08,930
the service for example the response

00:45:06,410 --> 00:45:13,630
time percentiles error rates and things

00:45:08,930 --> 00:45:16,670
like this and you can reorder this

00:45:13,630 --> 00:45:18,550
bye-bye for example by volume and all

00:45:16,670 --> 00:45:21,290
the big ones are at the beginning

00:45:18,550 --> 00:45:28,940
alphabetically by a particular response

00:45:21,290 --> 00:45:32,390
time and so on all right yeah and it

00:45:28,940 --> 00:45:35,600
will also have it will make it easy then

00:45:32,390 --> 00:45:36,980
to drill down to a particular dashboard

00:45:35,600 --> 00:45:39,560
for it and then to the actual

00:45:36,980 --> 00:45:42,560
transactions but this is not is not yet

00:45:39,560 --> 00:45:50,570
ready actually I just wanted to give you

00:45:42,560 --> 00:45:54,380
a preview all right anything else ok a

00:45:50,570 --> 00:45:57,170
little bit about how to deploy pack a

00:45:54,380 --> 00:46:00,650
bit if you decide you you want to give

00:45:57,170 --> 00:46:02,720
it a try so we have a pretty easy to

00:46:00,650 --> 00:46:04,870
follow getting started guide on our

00:46:02,720 --> 00:46:07,460
website which

00:46:04,870 --> 00:46:10,250
wilden's which will work you through

00:46:07,460 --> 00:46:14,870
installing packet bit and elastic search

00:46:10,250 --> 00:46:17,300
and keep on manually but they also have

00:46:14,870 --> 00:46:20,390
a what we call packet be deployed which

00:46:17,300 --> 00:46:25,490
is an ansible project which may which

00:46:20,390 --> 00:46:27,350
makes this automate it yeah and if you

00:46:25,490 --> 00:46:30,710
use already use ansible in your in your

00:46:27,350 --> 00:46:32,720
in your environment you could probably

00:46:30,710 --> 00:46:35,540
easily adopt this otherwise you could

00:46:32,720 --> 00:46:37,280
still use the used packet be deployed to

00:46:35,540 --> 00:46:40,070
set up like a proof of concept and

00:46:37,280 --> 00:46:44,330
whatever and then use puppet or whatever

00:46:40,070 --> 00:46:48,770
you used to to do the real thing the

00:46:44,330 --> 00:46:51,800
nice thing about ansible is that it

00:46:48,770 --> 00:46:54,190
makes it makes it easy to support like

00:46:51,800 --> 00:46:56,780
multiple elasticsearch nodes and

00:46:54,190 --> 00:46:59,210
different configurations and you only

00:46:56,780 --> 00:47:01,040
need to edit this house fast to add your

00:46:59,210 --> 00:47:04,040
host and then you run this command and

00:47:01,040 --> 00:47:06,080
it will get everything run executed for

00:47:04,040 --> 00:47:07,880
you automatically so I think it's a good

00:47:06,080 --> 00:47:11,090
fit for these kind of projects where you

00:47:07,880 --> 00:47:13,310
just want to set a quick POC still

00:47:11,090 --> 00:47:16,610
spinning on multiple on multiple

00:47:13,310 --> 00:47:21,620
machines all right that was deploying

00:47:16,610 --> 00:47:25,370
and that's pretty much all from me so if

00:47:21,620 --> 00:47:28,040
you want to keep up with the project you

00:47:25,370 --> 00:47:30,140
can do that on on Twitter or pack a bit

00:47:28,040 --> 00:47:33,410
or you can also follow me if you prefer

00:47:30,140 --> 00:47:37,010
not following a brand you can also

00:47:33,410 --> 00:47:39,020
subscribe 22 announcement mailing list

00:47:37,010 --> 00:47:42,070
essentially a newsletter but don't worry

00:47:39,020 --> 00:47:46,100
we won't spam you with marketing stuff

00:47:42,070 --> 00:47:47,870
and of course on github yep that was for

00:47:46,100 --> 00:47:50,020
me if there are any questions let me

00:47:47,870 --> 00:47:50,020
know

00:48:00,190 --> 00:48:39,829
after so the question is if we couldn't

00:48:36,589 --> 00:48:42,500
just use wireshark decoders to do the

00:48:39,829 --> 00:48:45,140
parsing and then we will have all the

00:48:42,500 --> 00:48:50,690
thousands of of protocols that wireshark

00:48:45,140 --> 00:48:53,510
supports so that might actually be a an

00:48:50,690 --> 00:48:56,510
option the the reason we didn't really

00:48:53,510 --> 00:48:58,880
consider it so far is that it will

00:48:56,510 --> 00:49:01,520
probably be not fast enough to keep up

00:48:58,880 --> 00:49:07,609
with the with the with the traffic

00:49:01,520 --> 00:49:09,650
speeds I mean if you open a 2 gigabyte

00:49:07,609 --> 00:49:13,279
file in wireshark it will take a little

00:49:09,650 --> 00:49:17,109
bit about until it's loaded we can be

00:49:13,279 --> 00:49:20,089
kind of faster than that because we can

00:49:17,109 --> 00:49:22,760
we can parse only the only like the

00:49:20,089 --> 00:49:25,400
headers that we need for example not not

00:49:22,760 --> 00:49:28,460
really every small thing they're like

00:49:25,400 --> 00:49:30,349
the decoders do in wireshark and then we

00:49:28,460 --> 00:49:31,970
could we have the choice of not parsing

00:49:30,349 --> 00:49:36,559
everything right so if we look at

00:49:31,970 --> 00:49:38,359
something and say well this is indeed I

00:49:36,559 --> 00:49:40,640
don't know my sequel message but it's

00:49:38,359 --> 00:49:42,200
not a MySQL message of interest let's

00:49:40,640 --> 00:49:45,500
say it's a heartbeat or something like

00:49:42,200 --> 00:49:47,299
this we can just keep those and just

00:49:45,500 --> 00:49:50,240
look at things that are interesting to

00:49:47,299 --> 00:49:53,890
us right so so that's the reason we did

00:49:50,240 --> 00:49:56,890
didn't didn't just use wireshark plugins

00:49:53,890 --> 00:49:56,890
right

00:50:07,609 --> 00:50:14,490
we do yeah we don't do that yet but it

00:50:11,460 --> 00:50:16,950
would yeah we could yeah sure we could

00:50:14,490 --> 00:50:19,740
do have like IP addresses sports and

00:50:16,950 --> 00:50:23,970
things like this we don't do that today

00:50:19,740 --> 00:50:33,810
but I imagine we could there was a

00:50:23,970 --> 00:50:35,640
question here right so there's another

00:50:33,810 --> 00:50:38,340
project that's actually also quite

00:50:35,640 --> 00:50:43,140
similar to packet bit in the sense that

00:50:38,340 --> 00:50:44,700
also uses packet capturing and also

00:50:43,140 --> 00:50:48,090
stores things in elasticsearch it's

00:50:44,700 --> 00:50:52,650
called Moloch I think so Moloch is more

00:50:48,090 --> 00:50:55,050
is more on this use case of of storing

00:50:52,650 --> 00:50:58,849
everything and then looking for for

00:50:55,050 --> 00:51:01,650
particular pickup and particular packets

00:50:58,849 --> 00:51:03,990
while we are more focused on getting

00:51:01,650 --> 00:51:06,060
application data right so we try to get

00:51:03,990 --> 00:51:08,010
as fast as possible to the application

00:51:06,060 --> 00:51:10,470
layer and provide interesting things

00:51:08,010 --> 00:51:12,630
about about those about the application

00:51:10,470 --> 00:51:15,440
layer I think there was also a question

00:51:12,630 --> 00:51:15,440
in the back yeah

00:51:41,750 --> 00:51:50,010
right so more most people use it all

00:51:45,770 --> 00:51:56,970
right okay so the question was let me

00:51:50,010 --> 00:52:00,510
see how i summarize it so yeah the

00:51:56,970 --> 00:52:02,609
question is there are lots of lots of

00:52:00,510 --> 00:52:05,339
the things that we show today they are

00:52:02,609 --> 00:52:08,010
cool and so on but they could also be

00:52:05,339 --> 00:52:11,119
accomplished in in different ways by by

00:52:08,010 --> 00:52:15,420
using logs or by using something else

00:52:11,119 --> 00:52:19,680
and how are the current users of the

00:52:15,420 --> 00:52:23,940
project using it and it's their most of

00:52:19,680 --> 00:52:25,920
them are using it yet to in in and in

00:52:23,940 --> 00:52:27,839
systems where they don't have full

00:52:25,920 --> 00:52:31,020
control of what's going on right so they

00:52:27,839 --> 00:52:34,320
don't they cannot easily modify logs for

00:52:31,020 --> 00:52:38,310
example what what logs are written or

00:52:34,320 --> 00:52:41,040
yeah for whatever reason they were

00:52:38,310 --> 00:52:45,990
looking for another solution and they

00:52:41,040 --> 00:52:47,670
found us and then it's easy with packet

00:52:45,990 --> 00:52:50,240
big because they can just install the

00:52:47,670 --> 00:52:52,500
agent and they and they get this data so

00:52:50,240 --> 00:52:55,770
at least for now it's a lot about

00:52:52,500 --> 00:52:57,869
convenience I think in the future you

00:52:55,770 --> 00:53:05,220
can also differentiate by different

00:52:57,869 --> 00:53:10,790
features and so on okay think that was

00:53:05,220 --> 00:53:10,790

YouTube URL: https://www.youtube.com/watch?v=sGvfrA5FI5E


