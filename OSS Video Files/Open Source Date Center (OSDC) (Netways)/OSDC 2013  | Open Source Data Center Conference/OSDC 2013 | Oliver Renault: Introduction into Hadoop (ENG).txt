Title: OSDC 2013 | Oliver Renault: Introduction into Hadoop (ENG)
Publication date: 2015-01-12
Playlist: OSDC 2013  | Open Source Data Center Conference
Description: 
	Wie setzt man Hadoop in einem Rechenzentrum ein und wie verwaltet und überwacht man es? Olivier Renault wird Ihnen beim Kickstart helfen, indem er von seinen Erfahrungen, die er mit dem Hortonworks Team beim Deployment im Yahoo Data Center und anderen Umgebungen gemacht hat. In diesem Vortrag wir er ebenfalls Ambari vorstellen, eine Open Source Management Lösung von Apache, welche den Einsatz und das Monitoring von Hadoop sehr vereinfacht.
Captions: 
	00:00:04,160 --> 00:00:07,120
no

00:00:04,960 --> 00:00:09,040
from hortonworks he will give you an

00:00:07,120 --> 00:00:11,759
introduction to hadoop

00:00:09,040 --> 00:00:12,559
have fun thank you very much so as you

00:00:11,759 --> 00:00:14,559
mentioned i'm

00:00:12,559 --> 00:00:16,160
olivier i work as a solution engineer

00:00:14,559 --> 00:00:17,600
for autonomous uh

00:00:16,160 --> 00:00:19,760
i've journaled tone works at the

00:00:17,600 --> 00:00:21,840
beginning of this year i'm

00:00:19,760 --> 00:00:22,800
myself still a bit discovering hadoop so

00:00:21,840 --> 00:00:24,080
bear with me

00:00:22,800 --> 00:00:25,359
that's why we're doing only an intro

00:00:24,080 --> 00:00:26,960
maybe next year we'll do something a bit

00:00:25,359 --> 00:00:29,920
more advanced but here we go

00:00:26,960 --> 00:00:31,519
so before i jump into adobe quickly i'm

00:00:29,920 --> 00:00:32,640
just going to quickly present with

00:00:31,519 --> 00:00:34,640
autumn works and

00:00:32,640 --> 00:00:35,920
to present where it was with autumn

00:00:34,640 --> 00:00:37,600
works what we

00:00:35,920 --> 00:00:38,960
like to do is to look a bit about the

00:00:37,600 --> 00:00:42,320
history of hadoop

00:00:38,960 --> 00:00:44,800
so hadoop got created in 2005 in

00:00:42,320 --> 00:00:46,480
yahoo okay based upon the white paper

00:00:44,800 --> 00:00:48,559
which was written on by google about

00:00:46,480 --> 00:00:50,000
how to process a large amount of data in

00:00:48,559 --> 00:00:52,960
parallel okay

00:00:50,000 --> 00:00:54,079
so it was created by a guy and his team

00:00:52,960 --> 00:00:56,559
the guy's name

00:00:54,079 --> 00:00:59,520
eric baldoch willa and uh we know him

00:00:56,559 --> 00:01:01,440
more most often under the name of e14

00:00:59,520 --> 00:01:02,559
because by the wheel has got 14 letters

00:01:01,440 --> 00:01:06,080
effectively

00:01:02,559 --> 00:01:07,760
and they started in 2005 they released

00:01:06,080 --> 00:01:09,520
it as an open source project in the

00:01:07,760 --> 00:01:13,520
apache software foundation

00:01:09,520 --> 00:01:15,680
in 2006. and pretty much

00:01:13,520 --> 00:01:18,240
they've developed this project along

00:01:15,680 --> 00:01:21,200
with yao along inside yahoo

00:01:18,240 --> 00:01:21,840
in from 2006 it got picked up by other

00:01:21,200 --> 00:01:24,479
companies

00:01:21,840 --> 00:01:25,600
such as facebook such as twitter and so

00:01:24,479 --> 00:01:28,479
on and so forth and it gets

00:01:25,600 --> 00:01:29,360
started to have some growth outside just

00:01:28,479 --> 00:01:33,520
the year is just

00:01:29,360 --> 00:01:34,640
yahoo and pretty much in 2008 just

00:01:33,520 --> 00:01:37,680
looked at

00:01:34,640 --> 00:01:39,680
their growth of a deep and so that 2008

00:01:37,680 --> 00:01:41,840
they were reaching numbers of

00:01:39,680 --> 00:01:43,920
cluster which were around 20 not 30

00:01:41,840 --> 00:01:45,520
nodes and they looked and they're

00:01:43,920 --> 00:01:47,280
starting to see that within yahoo the

00:01:45,520 --> 00:01:48,640
usage for hadoop was starting to grow

00:01:47,280 --> 00:01:50,320
and grow and grow even further and it

00:01:48,640 --> 00:01:51,920
was going to grow

00:01:50,320 --> 00:01:54,000
to a large scale so they started to

00:01:51,920 --> 00:01:58,159
focus in how to operate a cluster

00:01:54,000 --> 00:02:00,240
at scale just for so you know right now

00:01:58,159 --> 00:02:01,759
the amount of ad node within yahoo is

00:02:00,240 --> 00:02:05,520
around 75 000

00:02:01,759 --> 00:02:08,319
okay the cluster for yahoo is 4000 node

00:02:05,520 --> 00:02:10,160
on adobe 1.0 and they're starting to run

00:02:08,319 --> 00:02:13,120
adip 2.0 on a cluster

00:02:10,160 --> 00:02:15,040
is 15 000 node liner for them okay so

00:02:13,120 --> 00:02:15,840
when you start having this kind of scale

00:02:15,040 --> 00:02:17,520
you need to

00:02:15,840 --> 00:02:18,879
think about are you going to operate and

00:02:17,520 --> 00:02:21,120
how are you going to manage it at this

00:02:18,879 --> 00:02:24,160
kind of volume

00:02:21,120 --> 00:02:27,760
in 2011 uh

00:02:24,160 --> 00:02:28,080
yeah hadoop got used even further and

00:02:27,760 --> 00:02:29,599
the

00:02:28,080 --> 00:02:31,680
star the world of the enterprise started

00:02:29,599 --> 00:02:34,239
to get interested within hadoop

00:02:31,680 --> 00:02:35,599
and so autumn works got created at this

00:02:34,239 --> 00:02:37,280
time and it's pretty much

00:02:35,599 --> 00:02:38,640
so eric baldwich wheeler which was at

00:02:37,280 --> 00:02:41,440
the beginning which left

00:02:38,640 --> 00:02:43,599
plus 24 is a kia deep engineer and

00:02:41,440 --> 00:02:45,120
created our tone works okay the focus is

00:02:43,599 --> 00:02:46,800
to arrive into stability

00:02:45,120 --> 00:02:49,200
and provide enterprise distribution

00:02:46,800 --> 00:02:52,239
effectively so alton works

00:02:49,200 --> 00:02:53,760
got we do freezing we still develop

00:02:52,239 --> 00:02:55,680
quite extensively

00:02:53,760 --> 00:02:57,360
we distribute and we put what people

00:02:55,680 --> 00:02:59,360
that's what we provide we provide an

00:02:57,360 --> 00:03:00,959
arduino distribution which we call hdp

00:02:59,360 --> 00:03:03,519
alternate data platform

00:03:00,959 --> 00:03:05,519
and after that we support we work with

00:03:03,519 --> 00:03:07,680
few partners

00:03:05,519 --> 00:03:09,440
one of them is a customer of ours and

00:03:07,680 --> 00:03:12,400
this is still is yahoo

00:03:09,440 --> 00:03:14,080
but it's also a partner for us so it's

00:03:12,400 --> 00:03:15,760
also developing partners

00:03:14,080 --> 00:03:17,360
so we work with them along with on the

00:03:15,760 --> 00:03:19,440
features and on the roadmap and they

00:03:17,360 --> 00:03:21,440
also give us access to

00:03:19,440 --> 00:03:23,360
their cluster in order to do our qa

00:03:21,440 --> 00:03:24,640
testing okay so before we launch

00:03:23,360 --> 00:03:27,120
uh distribution we've got the

00:03:24,640 --> 00:03:29,440
opportunity to test our distribution

00:03:27,120 --> 00:03:30,480
on the 4000 node cluster of yahoo so

00:03:29,440 --> 00:03:31,599
when we say that we test our

00:03:30,480 --> 00:03:33,360
distribution at scale

00:03:31,599 --> 00:03:36,560
we actually do test it at fairly large

00:03:33,360 --> 00:03:39,280
scale we've got also a

00:03:36,560 --> 00:03:40,319
a company which is well famous for this

00:03:39,280 --> 00:03:42,799
open source

00:03:40,319 --> 00:03:43,760
microsoft that's also one of our

00:03:42,799 --> 00:03:45,680
partners

00:03:43,760 --> 00:03:46,799
it's kind of strange to see microsoft

00:03:45,680 --> 00:03:48,959
coming into uh

00:03:46,799 --> 00:03:50,640
into the picture but microsoft tried to

00:03:48,959 --> 00:03:52,239
bring hadoop for we were

00:03:50,640 --> 00:03:54,640
for a year and a half they tried to port

00:03:52,239 --> 00:03:55,439
hadoop onto on top of video windows i do

00:03:54,640 --> 00:03:57,760
being just

00:03:55,439 --> 00:03:58,480
java you know write it once and run it

00:03:57,760 --> 00:04:01,040
everywhere

00:03:58,480 --> 00:04:01,840
it should just work they struggle and

00:04:01,040 --> 00:04:04,000
they came to

00:04:01,840 --> 00:04:05,760
us for us for some help what's

00:04:04,000 --> 00:04:06,400
interesting is that now microsoft is

00:04:05,760 --> 00:04:08,959
also

00:04:06,400 --> 00:04:09,840
providing quite a lot of input on how to

00:04:08,959 --> 00:04:12,159
optimize

00:04:09,840 --> 00:04:13,920
some of the work which has been done so

00:04:12,159 --> 00:04:15,360
they are trying to arrive to the table

00:04:13,920 --> 00:04:16,400
and they're trying to arrive to you as a

00:04:15,360 --> 00:04:18,160
community as well

00:04:16,400 --> 00:04:19,919
we then have territory and rackspace in

00:04:18,160 --> 00:04:22,720
the cloud

00:04:19,919 --> 00:04:24,240
then uh the way of seeing why autumn

00:04:22,720 --> 00:04:25,360
works is contributing quite a lot of the

00:04:24,240 --> 00:04:27,840
adobe project is that

00:04:25,360 --> 00:04:29,360
the green bar that's element of a line

00:04:27,840 --> 00:04:30,639
of code that we've contributed to the

00:04:29,360 --> 00:04:32,720
project so yes we are

00:04:30,639 --> 00:04:34,880
every developer on this project the

00:04:32,720 --> 00:04:36,639
second the second contributor is yahoo

00:04:34,880 --> 00:04:39,600
so when i was saying that yahoo is still

00:04:36,639 --> 00:04:40,880
a development partner for us that's uh

00:04:39,600 --> 00:04:42,160
alton works yaou

00:04:40,880 --> 00:04:44,400
and then the third player which is

00:04:42,160 --> 00:04:46,639
claria okay so we we're quite heavy on

00:04:44,400 --> 00:04:49,600
this part

00:04:46,639 --> 00:04:50,000
as part of hdp what do you get but you

00:04:49,600 --> 00:04:53,199
get

00:04:50,000 --> 00:04:56,000
obviously hadoop and hadoop it's made of

00:04:53,199 --> 00:04:57,520
a distributed fire system hdfs and a

00:04:56,000 --> 00:04:58,400
parallel processing engine which is

00:04:57,520 --> 00:05:00,560
called mapreduce

00:04:58,400 --> 00:05:02,560
okay so that's what we call the adobe

00:05:00,560 --> 00:05:04,400
call you've got now also something which

00:05:02,560 --> 00:05:05,919
is called web hdfs which is a rest api

00:05:04,400 --> 00:05:09,280
in order to bring your data

00:05:05,919 --> 00:05:10,720
onto the hdfs5 system and with adobe 2

00:05:09,280 --> 00:05:12,560
what we're all waiting for

00:05:10,720 --> 00:05:14,160
it's something called yarn yet another

00:05:12,560 --> 00:05:15,840
resource negotiator

00:05:14,160 --> 00:05:17,280
and the beauty of yarn is that it's

00:05:15,840 --> 00:05:19,919
going to

00:05:17,280 --> 00:05:20,639
take away from just being a map reduced

00:05:19,919 --> 00:05:22,400
solution

00:05:20,639 --> 00:05:24,080
we are going to be able to do multiple

00:05:22,400 --> 00:05:25,199
other applications on top of that so i

00:05:24,080 --> 00:05:27,280
don't know if you've heard

00:05:25,199 --> 00:05:28,240
from storm from twitter which is a

00:05:27,280 --> 00:05:30,560
streaming engine

00:05:28,240 --> 00:05:31,919
effectively with yarn we will be able to

00:05:30,560 --> 00:05:34,479
run

00:05:31,919 --> 00:05:35,520
a mapreduce and on the same cluster run

00:05:34,479 --> 00:05:36,720
a storm cluster

00:05:35,520 --> 00:05:38,800
okay so that's going to be fairly

00:05:36,720 --> 00:05:41,520
interesting then we provide

00:05:38,800 --> 00:05:42,720
as you probably know the name node in

00:05:41,520 --> 00:05:45,759
adobe 1.0

00:05:42,720 --> 00:05:47,600
is a single point of failure so as part

00:05:45,759 --> 00:05:49,440
of our solution we also provide

00:05:47,600 --> 00:05:51,440
an hsa solution for the name node we've

00:05:49,440 --> 00:05:53,120
got disaster recovery snapshot mirroring

00:05:51,440 --> 00:05:56,080
and so on and so forth

00:05:53,120 --> 00:05:57,280
what we do is that everything we do is

00:05:56,080 --> 00:05:58,720
actually under 100

00:05:57,280 --> 00:06:00,400
open source so everything we do we give

00:05:58,720 --> 00:06:01,840
it back to the community okay so it's

00:06:00,400 --> 00:06:04,000
not something which is

00:06:01,840 --> 00:06:05,360
it's developed by us but then it's

00:06:04,000 --> 00:06:06,800
available for everyone else so that's

00:06:05,360 --> 00:06:08,720
pretty good you can go and download it

00:06:06,800 --> 00:06:10,560
directly from the website okay

00:06:08,720 --> 00:06:13,520
so there's a lot of data services around

00:06:10,560 --> 00:06:16,000
the on top of hadoop because

00:06:13,520 --> 00:06:17,280
mapreduce is initially some written in

00:06:16,000 --> 00:06:19,039
java okay so you need to write

00:06:17,280 --> 00:06:21,039
your you map producing java most of the

00:06:19,039 --> 00:06:22,080
time i don't know about your javascript

00:06:21,039 --> 00:06:24,319
but mine are

00:06:22,080 --> 00:06:25,520
equivalent to none so thank god for that

00:06:24,319 --> 00:06:26,720
there is some other options which are

00:06:25,520 --> 00:06:28,639
available and

00:06:26,720 --> 00:06:30,080
two which jump to mine are big and hive

00:06:28,639 --> 00:06:30,479
and we will speak about pigeon have in a

00:06:30,080 --> 00:06:32,800
bit

00:06:30,479 --> 00:06:34,479
in a bit but there is also h base which

00:06:32,800 --> 00:06:37,039
is available it's a nosql database

00:06:34,479 --> 00:06:39,280
okay and freeman's group flume is when

00:06:37,039 --> 00:06:42,800
you want to stream some log file

00:06:39,280 --> 00:06:44,160
into hdfs and scoop is to be able to

00:06:42,800 --> 00:06:47,759
export some data from

00:06:44,160 --> 00:06:51,440
a standard rdbms database into hadoop

00:06:47,759 --> 00:06:53,759
and export from hadoop into another dbms

00:06:51,440 --> 00:06:55,520
database

00:06:53,759 --> 00:06:57,520
then we've got some operational services

00:06:55,520 --> 00:06:59,599
and that's on barry and uzi so i will do

00:06:57,520 --> 00:07:01,440
a demonstration on barry a bit later

00:06:59,599 --> 00:07:02,639
and use a scheduling engine see it as a

00:07:01,440 --> 00:07:04,720
crown job if you want

00:07:02,639 --> 00:07:06,240
you can schedule all of your pig and

00:07:04,720 --> 00:07:07,680
high job and your mapreduce job

00:07:06,240 --> 00:07:10,400
to happen in a certain manner so when

00:07:07,680 --> 00:07:11,919
the data arrive in hdfs you can just

00:07:10,400 --> 00:07:13,599
take some action based upon who's the

00:07:11,919 --> 00:07:16,240
effective

00:07:13,599 --> 00:07:17,840
and all of that is what is hdp run on

00:07:16,240 --> 00:07:20,160
top of linux

00:07:17,840 --> 00:07:21,599
run on top of microsoft as well that's

00:07:20,160 --> 00:07:24,080
part another

00:07:21,599 --> 00:07:25,360
we run in the cloud in the vm as an

00:07:24,080 --> 00:07:27,520
appliance okay

00:07:25,360 --> 00:07:29,360
and in terms of marketing globe that's

00:07:27,520 --> 00:07:32,160
finished all right

00:07:29,360 --> 00:07:33,280
so uh quickly an overview of hadoop so

00:07:32,160 --> 00:07:35,599
alpha hadoop so first

00:07:33,280 --> 00:07:37,120
it all started at google okay so google

00:07:35,599 --> 00:07:38,479
needed a way of doing page ranking

00:07:37,120 --> 00:07:40,560
so they wanted to see all of the link

00:07:38,479 --> 00:07:41,840
and how the links are going in between

00:07:40,560 --> 00:07:44,000
the different web pages

00:07:41,840 --> 00:07:45,280
in order to when we do a search provide

00:07:44,000 --> 00:07:46,400
us with something which is relevant to

00:07:45,280 --> 00:07:50,720
our search effectively

00:07:46,400 --> 00:07:54,160
all right a way of looking at it

00:07:50,720 --> 00:07:56,400
is to that's pretty much

00:07:54,160 --> 00:07:57,919
the idea of map so google wrote is this

00:07:56,400 --> 00:08:00,000
white paper called mapreduce simplified

00:07:57,919 --> 00:08:02,879
data processing on large cluster

00:08:00,000 --> 00:08:04,639
they put it out and pretty much that's

00:08:02,879 --> 00:08:06,080
the id so you've got two faces your

00:08:04,639 --> 00:08:06,800
first mapping and then you're reducing

00:08:06,080 --> 00:08:10,319
it so

00:08:06,800 --> 00:08:12,319
if you look with an example we've got

00:08:10,319 --> 00:08:14,319
we're going to look for all of the url

00:08:12,319 --> 00:08:16,720
where there is a book for the word books

00:08:14,319 --> 00:08:18,000
okay so we are looking at this url when

00:08:16,720 --> 00:08:20,960
we find the word books

00:08:18,000 --> 00:08:21,680
we lo we linking all of the url all

00:08:20,960 --> 00:08:24,639
together so

00:08:21,680 --> 00:08:25,440
you've got books in this url and on that

00:08:24,639 --> 00:08:27,479
url

00:08:25,440 --> 00:08:28,720
and that's pretty much what the map

00:08:27,479 --> 00:08:31,280
recipe

00:08:28,720 --> 00:08:32,959
map reduce concept is all about okay is

00:08:31,280 --> 00:08:34,479
to be able to do some key value kind of

00:08:32,959 --> 00:08:37,519
solution

00:08:34,479 --> 00:08:40,640
the with this kind of

00:08:37,519 --> 00:08:42,240
work what now people are able to do is

00:08:40,640 --> 00:08:42,880
for example to look at all of the web

00:08:42,240 --> 00:08:45,360
log

00:08:42,880 --> 00:08:46,800
which are being generated when people

00:08:45,360 --> 00:08:48,640
are using your website

00:08:46,800 --> 00:08:50,880
and based upon the information that of

00:08:48,640 --> 00:08:52,320
this web blog being able for example to

00:08:50,880 --> 00:08:54,560
do recommendation engine

00:08:52,320 --> 00:08:57,279
okay so you've all gone to amazon you've

00:08:54,560 --> 00:08:59,920
all bought from ebay or from whatever

00:08:57,279 --> 00:09:01,360
and when you choose when you search for

00:08:59,920 --> 00:09:05,200
something there will always

00:09:01,360 --> 00:09:06,640
be uh some stuff that you search for and

00:09:05,200 --> 00:09:08,720
then some other products which are

00:09:06,640 --> 00:09:11,600
fairly relevant to your search

00:09:08,720 --> 00:09:12,800
an example is that for example uh my dad

00:09:11,600 --> 00:09:15,920
is a fisherman

00:09:12,800 --> 00:09:17,920
so i look for some at some point to buy

00:09:15,920 --> 00:09:20,480
a rod for his birthday

00:09:17,920 --> 00:09:22,000
and i didn't boast the fishing rod so

00:09:20,480 --> 00:09:23,760
far we will

00:09:22,000 --> 00:09:25,440
ebay was convinced i was really much

00:09:23,760 --> 00:09:27,920
into fishing and that were

00:09:25,440 --> 00:09:29,600
pushing me a lot of advertising about

00:09:27,920 --> 00:09:30,880
you know either the string for my

00:09:29,600 --> 00:09:32,640
fishing rod or

00:09:30,880 --> 00:09:34,160
the special boots to go fishing or

00:09:32,640 --> 00:09:35,600
whatever it was you know

00:09:34,160 --> 00:09:37,279
fortunately i was actually not really

00:09:35,600 --> 00:09:38,399
that much into fishing myself but anyway

00:09:37,279 --> 00:09:42,880
that's another story

00:09:38,399 --> 00:09:45,920
okay so um

00:09:42,880 --> 00:09:46,880
the idea for hadoop is that so you got

00:09:45,920 --> 00:09:48,320
the map

00:09:46,880 --> 00:09:50,640
you've got the shuffle phase and then

00:09:48,320 --> 00:09:53,120
you've got the radius the shuffle phase

00:09:50,640 --> 00:09:54,240
is this phase where we are linking all

00:09:53,120 --> 00:09:56,720
of the books together

00:09:54,240 --> 00:09:57,839
the beauty of this mapreduce paradigm is

00:09:56,720 --> 00:10:00,080
that you can work

00:09:57,839 --> 00:10:01,279
into a lot of system at the same time

00:10:00,080 --> 00:10:02,000
you can it's parallel processing

00:10:01,279 --> 00:10:05,680
effectively

00:10:02,000 --> 00:10:06,959
okay so hadoop

00:10:05,680 --> 00:10:08,800
has come made of two components

00:10:06,959 --> 00:10:10,000
initially hdfs and mapreduce so let's

00:10:08,800 --> 00:10:12,800
have a quick look at

00:10:10,000 --> 00:10:14,320
how hdfs work effectively well that's a

00:10:12,800 --> 00:10:15,920
cluster so we've got some nodes which

00:10:14,320 --> 00:10:17,040
are the master node and on the on the

00:10:15,920 --> 00:10:19,279
side on the

00:10:17,040 --> 00:10:20,399
blue node and then we've got a lot of

00:10:19,279 --> 00:10:22,480
data nodes

00:10:20,399 --> 00:10:24,160
which are in a rack and hadoop's got

00:10:22,480 --> 00:10:25,680
this knowledge of rack awareness as well

00:10:24,160 --> 00:10:27,360
so you will know where your

00:10:25,680 --> 00:10:29,440
nodes are located and it will make sure

00:10:27,360 --> 00:10:31,360
that you do blocks are not on the same

00:10:29,440 --> 00:10:33,600
physical box or within the same physical

00:10:31,360 --> 00:10:35,120
racks in order to uh

00:10:33,600 --> 00:10:37,440
make sure that if you stuff crash it

00:10:35,120 --> 00:10:41,200
doesn't work so that's the hdfs part

00:10:37,440 --> 00:10:42,880
so hdfs it's got it's uh first it's a

00:10:41,200 --> 00:10:43,600
distributed fire system so we don't have

00:10:42,880 --> 00:10:45,120
a sun

00:10:43,600 --> 00:10:47,920
at the back end we're using some of the

00:10:45,120 --> 00:10:50,160
pizza box in order to have that

00:10:47,920 --> 00:10:51,040
some key assumption hardware failure is

00:10:50,160 --> 00:10:54,079
unknown

00:10:51,040 --> 00:10:57,040
okay so the fire system itself

00:10:54,079 --> 00:10:58,720
is planning for hardware failure it's

00:10:57,040 --> 00:11:00,399
made for ice throughput

00:10:58,720 --> 00:11:02,399
and it's an app and only file system so

00:11:00,399 --> 00:11:04,240
it's something which is really good

00:11:02,399 --> 00:11:05,519
uh you write it once and you read it

00:11:04,240 --> 00:11:06,959
many times

00:11:05,519 --> 00:11:09,680
and what you're trying to what we're

00:11:06,959 --> 00:11:10,320
trying to do is uh hdfs and mapreduce is

00:11:09,680 --> 00:11:12,720
to have

00:11:10,320 --> 00:11:13,600
the computing power to be close to the

00:11:12,720 --> 00:11:16,079
data

00:11:13,600 --> 00:11:17,440
okay so that's one of the important part

00:11:16,079 --> 00:11:19,200
of it

00:11:17,440 --> 00:11:20,880
when you run an hdfs you've got few

00:11:19,200 --> 00:11:22,399
components and there's a when you look

00:11:20,880 --> 00:11:23,839
at the

00:11:22,399 --> 00:11:25,760
you've got first the name node and the

00:11:23,839 --> 00:11:26,800
name node is effectively holding in

00:11:25,760 --> 00:11:28,160
memory

00:11:26,800 --> 00:11:30,959
all of the information of where your

00:11:28,160 --> 00:11:32,399
data is located and what i said that was

00:11:30,959 --> 00:11:34,240
that is a single point of failure that's

00:11:32,399 --> 00:11:35,440
kind of an issue because when you were

00:11:34,240 --> 00:11:37,519
losing your name node

00:11:35,440 --> 00:11:38,880
your data wasn't lost but you are not

00:11:37,519 --> 00:11:41,920
able to access it

00:11:38,880 --> 00:11:43,279
which you know if you feel like

00:11:41,920 --> 00:11:45,680
some clusters that i know of where

00:11:43,279 --> 00:11:47,040
you've got five petabytes of data

00:11:45,680 --> 00:11:49,279
suddenly you can't access with five

00:11:47,040 --> 00:11:50,560
petabytes this can't be an issue

00:11:49,279 --> 00:11:52,079
then you've got another one which is

00:11:50,560 --> 00:11:53,440
called the data node and that's another

00:11:52,079 --> 00:11:55,200
effectively the worker node

00:11:53,440 --> 00:11:56,880
okay so the one which where this you

00:11:55,200 --> 00:11:57,839
store your data and you

00:11:56,880 --> 00:12:00,480
it's where you're going to do your map

00:11:57,839 --> 00:12:02,079
reduce processing zipping every so often

00:12:00,480 --> 00:12:03,680
the name node in order to make sure that

00:12:02,079 --> 00:12:05,200
you tell them that is there alive and

00:12:03,680 --> 00:12:08,160
they will also report the blocks that

00:12:05,200 --> 00:12:09,839
they've got available on top of them

00:12:08,160 --> 00:12:11,440
and then we've got a third node which is

00:12:09,839 --> 00:12:14,800
called the secondary name node

00:12:11,440 --> 00:12:16,639
and m is only making sure that the

00:12:14,800 --> 00:12:18,959
image of the name nodes of the

00:12:16,639 --> 00:12:22,480
information of all of the blocks whereas

00:12:18,959 --> 00:12:24,560
where all the block located is correct

00:12:22,480 --> 00:12:26,000
it's a really fairly bad name because

00:12:24,560 --> 00:12:27,200
when i first started

00:12:26,000 --> 00:12:29,440
to work with adobe i thought our

00:12:27,200 --> 00:12:30,880
secondary name node was the failover

00:12:29,440 --> 00:12:33,680
name node effectively

00:12:30,880 --> 00:12:34,480
which is not okay so don't get confused

00:12:33,680 --> 00:12:37,200
with the name

00:12:34,480 --> 00:12:38,880
it's not a fail of the name node is

00:12:37,200 --> 00:12:40,480
something else

00:12:38,880 --> 00:12:42,079
so when we say that it's uh something

00:12:40,480 --> 00:12:44,079
which replicates itself

00:12:42,079 --> 00:12:46,160
but that's pretty much how it works okay

00:12:44,079 --> 00:12:48,720
so when you want to copy a file

00:12:46,160 --> 00:12:50,240
it goes first the file goes first so

00:12:48,720 --> 00:12:51,200
when you the client goes to the name no

00:12:50,240 --> 00:12:53,760
then say okay

00:12:51,200 --> 00:12:54,560
i want to copy this file to your hdfs

00:12:53,760 --> 00:12:56,480
right

00:12:54,560 --> 00:12:58,079
so the name node says sure you're going

00:12:56,480 --> 00:13:00,720
to work with server a

00:12:58,079 --> 00:13:02,079
so the client then sends the data to

00:13:00,720 --> 00:13:03,920
server a

00:13:02,079 --> 00:13:05,519
and straight away okay but we need also

00:13:03,920 --> 00:13:08,000
to replicate this block

00:13:05,519 --> 00:13:09,839
onto typically it's a replication of

00:13:08,000 --> 00:13:10,800
free you can change your application as

00:13:09,839 --> 00:13:12,160
you want but typically it's a

00:13:10,800 --> 00:13:13,920
replication of free

00:13:12,160 --> 00:13:15,839
and it's going to then copy the block

00:13:13,920 --> 00:13:18,000
onto two other servers

00:13:15,839 --> 00:13:19,440
okay and that's how it works so if you

00:13:18,000 --> 00:13:21,519
lose one server

00:13:19,440 --> 00:13:22,959
or if you even if you lose two servers

00:13:21,519 --> 00:13:24,880
the name node is going to detect that

00:13:22,959 --> 00:13:25,120
you've lost as you've got a block which

00:13:24,880 --> 00:13:27,360
is

00:13:25,120 --> 00:13:28,720
under replicated and will make sure that

00:13:27,360 --> 00:13:31,200
it's copying

00:13:28,720 --> 00:13:32,959
this block to two other extra servers

00:13:31,200 --> 00:13:34,480
okay

00:13:32,959 --> 00:13:36,800
if for example it's just that you've

00:13:34,480 --> 00:13:38,079
rebooted the node it doesn't matter you

00:13:36,800 --> 00:13:39,040
will have copied the block and you will

00:13:38,079 --> 00:13:41,760
have the block

00:13:39,040 --> 00:13:43,360
full time if you only want to have three

00:13:41,760 --> 00:13:44,240
if you've decided that you want a copy

00:13:43,360 --> 00:13:46,399
on your free

00:13:44,240 --> 00:13:47,839
the next time that you need some space

00:13:46,399 --> 00:13:48,320
you will get rid of one of your block

00:13:47,839 --> 00:13:51,440
which is

00:13:48,320 --> 00:13:55,360
over replicated if you are okay

00:13:51,440 --> 00:13:55,360
any question regarding the hdfs part

00:13:55,440 --> 00:13:57,680
no

00:13:58,560 --> 00:14:02,240
so then the second part is the map

00:13:59,920 --> 00:14:05,360
reduce okay

00:14:02,240 --> 00:14:06,000
so mapreduce it's a framework for

00:14:05,360 --> 00:14:08,480
developing

00:14:06,000 --> 00:14:09,760
a distributed application across large

00:14:08,480 --> 00:14:12,240
amount of data

00:14:09,760 --> 00:14:12,959
and the idea is really to have the

00:14:12,240 --> 00:14:16,240
processing

00:14:12,959 --> 00:14:17,760
to happen really close to your data

00:14:16,240 --> 00:14:20,480
okay so as we've seen the block is

00:14:17,760 --> 00:14:22,480
replicated on top of freenode by default

00:14:20,480 --> 00:14:24,160
so we will when we're going to try to do

00:14:22,480 --> 00:14:26,560
a mapreduce action

00:14:24,160 --> 00:14:27,600
we're going to send it to the to the

00:14:26,560 --> 00:14:29,920
server

00:14:27,600 --> 00:14:30,720
which got a copy of the data available

00:14:29,920 --> 00:14:33,040
with them

00:14:30,720 --> 00:14:34,959
okay so we are trying to not have to

00:14:33,040 --> 00:14:35,920
copy the data across the network or

00:14:34,959 --> 00:14:37,120
anything like that

00:14:35,920 --> 00:14:39,760
we're always going to try to send it

00:14:37,120 --> 00:14:42,000
where it goes same thing for the

00:14:39,760 --> 00:14:43,680
mapreduce we've got a few components

00:14:42,000 --> 00:14:44,079
we've got a job tracker and the job

00:14:43,680 --> 00:14:47,440
tracker

00:14:44,079 --> 00:14:49,040
is going is uh old is the one who is

00:14:47,440 --> 00:14:50,160
going to decide which node is going to

00:14:49,040 --> 00:14:51,279
work

00:14:50,160 --> 00:14:55,360
and the task tracker which is

00:14:51,279 --> 00:14:55,360
effectively going to do the job okay

00:14:55,519 --> 00:14:58,720
the job tracker so when you send a job

00:14:57,680 --> 00:15:01,519
the job tracker

00:14:58,720 --> 00:15:03,120
go to the name node say which block we

00:15:01,519 --> 00:15:04,160
want to work on this type of the undo's

00:15:03,120 --> 00:15:07,199
data

00:15:04,160 --> 00:15:08,880
which servers do you have the block

00:15:07,199 --> 00:15:10,720
and that's where it's going to then send

00:15:08,880 --> 00:15:11,600
it to the data or to that task tracker

00:15:10,720 --> 00:15:15,279
without the data

00:15:11,600 --> 00:15:17,600
okay that's pretty much

00:15:15,279 --> 00:15:19,680
the mapreduce phase okay so we've got

00:15:17,600 --> 00:15:22,240
first the mapper phase which happened

00:15:19,680 --> 00:15:23,760
and we've got the shuffle okay and then

00:15:22,240 --> 00:15:25,120
after that when you finish your circle

00:15:23,760 --> 00:15:27,440
you use a reducer

00:15:25,120 --> 00:15:29,040
be careful the shuffle phase it's really

00:15:27,440 --> 00:15:31,440
network intensive

00:15:29,040 --> 00:15:32,720
so because we've got rack awareness in

00:15:31,440 --> 00:15:34,480
adobe so you can say

00:15:32,720 --> 00:15:36,000
this cluster is in rack a and that

00:15:34,480 --> 00:15:38,480
clustering is in rugby

00:15:36,000 --> 00:15:40,480
often for ha purposes we're thinking

00:15:38,480 --> 00:15:41,839
amazing great i'm going to put one rack

00:15:40,480 --> 00:15:44,160
in data center one

00:15:41,839 --> 00:15:46,399
another rack in data center two and i

00:15:44,160 --> 00:15:46,800
will get i have ability by splitting my

00:15:46,399 --> 00:15:50,240
uh

00:15:46,800 --> 00:15:52,880
two racks into data centers

00:15:50,240 --> 00:15:54,639
and it's kind of work until you get on

00:15:52,880 --> 00:15:56,399
to a large amount of data

00:15:54,639 --> 00:15:58,399
and you try to do the shuffle across the

00:15:56,399 --> 00:15:58,880
network and you're bringing down the

00:15:58,399 --> 00:16:00,480
pipe

00:15:58,880 --> 00:16:01,839
which is joining you to data center

00:16:00,480 --> 00:16:02,959
effectively you're going to flood the

00:16:01,839 --> 00:16:05,279
pipe so

00:16:02,959 --> 00:16:06,320
the advice for now is don't do it

00:16:05,279 --> 00:16:08,160
apparently they

00:16:06,320 --> 00:16:09,920
are working on doing something in order

00:16:08,160 --> 00:16:12,639
to fix this kind of problem

00:16:09,920 --> 00:16:16,000
okay that's pretty much what we've seen

00:16:12,639 --> 00:16:16,000
earlier so i'm not going to go to that

00:16:16,720 --> 00:16:22,240
we so as i said earlier mapreduce is

00:16:20,320 --> 00:16:23,680
something that you write in java most of

00:16:22,240 --> 00:16:25,199
the time you can also write

00:16:23,680 --> 00:16:27,279
an option to write it in python but it's

00:16:25,199 --> 00:16:28,800
most commonly done in java

00:16:27,279 --> 00:16:30,480
there is two other tools which have been

00:16:28,800 --> 00:16:33,600
developed

00:16:30,480 --> 00:16:35,920
but pig has been developed yahoo and

00:16:33,600 --> 00:16:38,240
has been developed at facebook in order

00:16:35,920 --> 00:16:40,240
to give something to the analysts

00:16:38,240 --> 00:16:42,240
we are not java expert and still make

00:16:40,240 --> 00:16:45,440
sense of the data that they've got

00:16:42,240 --> 00:16:48,720
okay so

00:16:45,440 --> 00:16:50,079
big it's a scripting language okay

00:16:48,720 --> 00:16:52,320
it provides something that they call the

00:16:50,079 --> 00:16:54,800
pickling latin and it's a data flow

00:16:52,320 --> 00:16:59,279
scripting language kind of thing

00:16:54,800 --> 00:17:01,680
so why you speak but

00:16:59,279 --> 00:17:02,320
so what do you say you've got some files

00:17:01,680 --> 00:17:04,400
you want to

00:17:02,320 --> 00:17:05,600
merge two different type of file and

00:17:04,400 --> 00:17:07,839
take the top five

00:17:05,600 --> 00:17:10,640
most visited website for the aid for a

00:17:07,839 --> 00:17:14,400
group of age 18-25

00:17:10,640 --> 00:17:15,839
if you do it in map reduce

00:17:14,400 --> 00:17:19,120
that's the amount of code that you need

00:17:15,839 --> 00:17:20,640
to write okay so as i say what 170 line

00:17:19,120 --> 00:17:22,240
of code four hours to write it

00:17:20,640 --> 00:17:23,679
if it's me it's probably five days but

00:17:22,240 --> 00:17:27,360
you know

00:17:23,679 --> 00:17:27,360
i'm a java coder with google so

00:17:27,919 --> 00:17:34,559
in pig it's a it's nine line of code

00:17:31,520 --> 00:17:36,320
okay so what people tend to say that you

00:17:34,559 --> 00:17:39,760
use

00:17:36,320 --> 00:17:41,520
75 of the time you use pig

00:17:39,760 --> 00:17:43,840
and when you can't you speak then you're

00:17:41,520 --> 00:17:46,480
going to go and move to something else

00:17:43,840 --> 00:17:48,320
pig also can be extended by some what we

00:17:46,480 --> 00:17:50,080
call the user-defined function

00:17:48,320 --> 00:17:52,400
and this user-defined function can be in

00:17:50,080 --> 00:17:54,320
any languages so they can be in python

00:17:52,400 --> 00:17:57,760
they can be in whatever you want to

00:17:54,320 --> 00:18:01,120
okay so it's really quite convenient

00:17:57,760 --> 00:18:03,280
to work with so

00:18:01,120 --> 00:18:04,320
the idea behind ping is that my produce

00:18:03,280 --> 00:18:06,559
was too low

00:18:04,320 --> 00:18:07,600
and sql was too high my sql is fairly

00:18:06,559 --> 00:18:08,799
straightforward but if you don't know i

00:18:07,600 --> 00:18:12,080
guess it's maybe true

00:18:08,799 --> 00:18:15,600
okay so the big latin is here to

00:18:12,080 --> 00:18:17,600
to go around this issue uh

00:18:15,600 --> 00:18:18,799
there is no need for a schema you can do

00:18:17,600 --> 00:18:21,360
join you can do sort

00:18:18,799 --> 00:18:22,160
and you've got as i mentioned this udf

00:18:21,360 --> 00:18:24,160
we tend to use

00:18:22,160 --> 00:18:26,160
pig a lot when you want to do etl okay

00:18:24,160 --> 00:18:28,720
so there's a lot of usage for hadoop

00:18:26,160 --> 00:18:29,919
where we store a lot of new data source

00:18:28,720 --> 00:18:32,000
that our source could be

00:18:29,919 --> 00:18:34,480
a upload could be the email could be

00:18:32,000 --> 00:18:37,919
text form from different uh

00:18:34,480 --> 00:18:40,000
query or whatever and we speak we tend

00:18:37,919 --> 00:18:41,919
to do a lot of etl of cleaning the data

00:18:40,000 --> 00:18:43,679
merging it together and maybe we're

00:18:41,919 --> 00:18:46,960
going to then export it back to

00:18:43,679 --> 00:18:48,559
another rdbms or to something else okay

00:18:46,960 --> 00:18:51,039
piggies make of freezing so we've got

00:18:48,559 --> 00:18:54,080
the pig latin which is a the language

00:18:51,039 --> 00:18:55,600
you've got a shell which is called grand

00:18:54,080 --> 00:18:57,520
i know the it people they like to have

00:18:55,600 --> 00:18:58,960
fun and so it's big it's grant and

00:18:57,520 --> 00:19:00,240
everything it's normal

00:18:58,960 --> 00:19:02,320
and then we've got what we call the

00:19:00,240 --> 00:19:05,600
piggy bank which is a way for sharing

00:19:02,320 --> 00:19:07,360
uh udf okay so there is quite a lot of

00:19:05,600 --> 00:19:08,960
udf which have been defined

00:19:07,360 --> 00:19:10,640
and that you can go directly and

00:19:08,960 --> 00:19:11,760
basically use them directly out of the

00:19:10,640 --> 00:19:13,600
pigment

00:19:11,760 --> 00:19:15,679
okay do you have any question regarding

00:19:13,600 --> 00:19:18,720
pig

00:19:15,679 --> 00:19:19,360
no it's all clear with pig so we've got

00:19:18,720 --> 00:19:22,799
this friend

00:19:19,360 --> 00:19:25,919
which is called ive okay and

00:19:22,799 --> 00:19:29,120
ive it's pretty much an

00:19:25,919 --> 00:19:31,039
sql-like interface okay so it's

00:19:29,120 --> 00:19:33,039
the idea was to become an enterprise at

00:19:31,039 --> 00:19:35,120
our house of hadoop

00:19:33,039 --> 00:19:36,559
i'm not sure we get there with that with

00:19:35,120 --> 00:19:37,120
but that's pretty much the id it's going

00:19:36,559 --> 00:19:39,120
to be

00:19:37,120 --> 00:19:41,440
an sql like interface that you can use

00:19:39,120 --> 00:19:44,080
so it's called ifql

00:19:41,440 --> 00:19:45,440
it's based on sql 92 specification it's

00:19:44,080 --> 00:19:48,400
not unsysqueable but

00:19:45,440 --> 00:19:50,000
it's getting there it's pretty much like

00:19:48,400 --> 00:19:51,919
a relational database so you're going to

00:19:50,000 --> 00:19:53,679
be able to do you know a select star

00:19:51,919 --> 00:19:56,080
from blah blah blah

00:19:53,679 --> 00:19:58,000
joined by and everything okay there we

00:19:56,080 --> 00:20:00,400
go here's a different typical

00:19:58,000 --> 00:20:02,080
i mean if you're speed familiar with sql

00:20:00,400 --> 00:20:03,679
you will probably recognize

00:20:02,080 --> 00:20:05,520
some standard statement because there is

00:20:03,679 --> 00:20:08,720
nothing new there efficient

00:20:05,520 --> 00:20:10,080
okay so

00:20:08,720 --> 00:20:11,760
something which is interesting is that

00:20:10,080 --> 00:20:12,400
in most organization you're probably

00:20:11,760 --> 00:20:14,400
going to use

00:20:12,400 --> 00:20:16,880
pig and you're going to also use i've

00:20:14,400 --> 00:20:18,400
it's not either one or the other

00:20:16,880 --> 00:20:20,480
it's something that you can use together

00:20:18,400 --> 00:20:23,520
in parallel okay

00:20:20,480 --> 00:20:24,880
so uh is a good choice when you want to

00:20:23,520 --> 00:20:27,280
query this data and especially if you're

00:20:24,880 --> 00:20:30,320
already familiar with sql

00:20:27,280 --> 00:20:33,280
pig is was saying earlier is pretty much

00:20:30,320 --> 00:20:36,400
good for doing a lot of etl

00:20:33,280 --> 00:20:37,679
okay i'm just realizing that i'm going

00:20:36,400 --> 00:20:38,880
quite fast on my presentation so i'm

00:20:37,679 --> 00:20:40,159
just going to go back quickly

00:20:38,880 --> 00:20:42,480
and i'm going to speak about some fuse

00:20:40,159 --> 00:20:44,080
mode so uh

00:20:42,480 --> 00:20:45,679
then we've got something which we call h

00:20:44,080 --> 00:20:47,120
catalog and as pig

00:20:45,679 --> 00:20:49,440
knife can be used exactly in the same

00:20:47,120 --> 00:20:52,320
organization we

00:20:49,440 --> 00:20:55,200
have got a way of having the metadata

00:20:52,320 --> 00:20:57,679
shared across pigeon hive user because

00:20:55,200 --> 00:20:59,039
by definition when you copy data into

00:20:57,679 --> 00:21:00,799
hadoop it's schema-less

00:20:59,039 --> 00:21:03,520
okay it's on some unstructured data that

00:21:00,799 --> 00:21:05,600
you put into adoption schemeless so

00:21:03,520 --> 00:21:07,120
sometimes when you were having when you

00:21:05,600 --> 00:21:07,919
were working with pig you were defining

00:21:07,120 --> 00:21:10,720
your

00:21:07,919 --> 00:21:11,679
your field with a certain name and the

00:21:10,720 --> 00:21:13,760
iv user would

00:21:11,679 --> 00:21:14,720
define them with another sort of name

00:21:13,760 --> 00:21:16,799
and the idea of

00:21:14,720 --> 00:21:18,720
catalog is to provide a metadata

00:21:16,799 --> 00:21:19,760
directory where you store all of this

00:21:18,720 --> 00:21:22,320
information so

00:21:19,760 --> 00:21:23,120
when your big user is speaking to uh

00:21:22,320 --> 00:21:24,799
when you

00:21:23,120 --> 00:21:26,240
pick a knife use exactly the same

00:21:24,799 --> 00:21:28,320
metadata information so

00:21:26,240 --> 00:21:29,840
when you want to share information

00:21:28,320 --> 00:21:31,679
across two different team

00:21:29,840 --> 00:21:33,760
using two different one or the other

00:21:31,679 --> 00:21:36,000
tool at least you've got a way of having

00:21:33,760 --> 00:21:40,159
this information being shared easily

00:21:36,000 --> 00:21:42,640
okay and then we've got also a

00:21:40,159 --> 00:21:44,799
freelance group that i mentioned earlier

00:21:42,640 --> 00:21:47,440
so let's go back to the

00:21:44,799 --> 00:21:48,000
something which is maybe more admin like

00:21:47,440 --> 00:21:50,960
which is

00:21:48,000 --> 00:21:53,280
on barry okay so on barry is a

00:21:50,960 --> 00:21:56,559
deployment mechanism which is used

00:21:53,280 --> 00:21:58,320
to deploy uh apache to deploy a

00:21:56,559 --> 00:21:59,919
autonomous data platform it's an open

00:21:58,320 --> 00:22:02,000
source project you can deploy vania

00:21:59,919 --> 00:22:04,720
apache as well with that

00:22:02,000 --> 00:22:04,720
and the idea of

00:22:06,000 --> 00:22:10,480
the id behind onbary that you're going

00:22:08,080 --> 00:22:11,919
first you use it to install your cluster

00:22:10,480 --> 00:22:13,840
then you will be able to manage your

00:22:11,919 --> 00:22:15,039
cluster and finally you will be able to

00:22:13,840 --> 00:22:16,720
monitor your

00:22:15,039 --> 00:22:19,039
not your cluster but also monitor your

00:22:16,720 --> 00:22:20,159
job on top of that okay so it's also a

00:22:19,039 --> 00:22:23,120
way for you to

00:22:20,159 --> 00:22:24,000
push configuration across different uh

00:22:23,120 --> 00:22:25,600
platform

00:22:24,000 --> 00:22:27,840
and modify your configuration from one

00:22:25,600 --> 00:22:30,080
single place so of course you can do

00:22:27,840 --> 00:22:32,000
that all with uh puppet and chef and

00:22:30,080 --> 00:22:33,280
on sibo and we've all heard about it for

00:22:32,000 --> 00:22:35,120
the last two days

00:22:33,280 --> 00:22:36,720
but that's you know that's one of the

00:22:35,120 --> 00:22:38,799
way actually underneath

00:22:36,720 --> 00:22:40,240
on barry is using puppet in order to uh

00:22:38,799 --> 00:22:43,360
to do that effectively

00:22:40,240 --> 00:22:45,120
but that's another issue okay

00:22:43,360 --> 00:22:46,640
the beauty of hambury is that he's got

00:22:45,120 --> 00:22:48,559
also a

00:22:46,640 --> 00:22:49,919
rest api which means that if you've got

00:22:48,559 --> 00:22:51,360
another solution that you want to

00:22:49,919 --> 00:22:53,919
integrate with onbari

00:22:51,360 --> 00:22:55,360
you can do that through the rest api by

00:22:53,919 --> 00:22:59,039
default

00:22:55,360 --> 00:23:02,320
ombari is coming with a monitoring

00:22:59,039 --> 00:23:05,600
and uh performance monitoring and also a

00:23:02,320 --> 00:23:06,240
monitoring solution on embedded as part

00:23:05,600 --> 00:23:07,679
of it

00:23:06,240 --> 00:23:09,679
and the two solutions that we're using

00:23:07,679 --> 00:23:11,840
is ganglia and nagios

00:23:09,679 --> 00:23:14,000
okay so ganglia and nagios are installed

00:23:11,840 --> 00:23:15,679
by default by on very effectively

00:23:14,000 --> 00:23:17,039
if you want to use another performance

00:23:15,679 --> 00:23:18,720
monitoring solution

00:23:17,039 --> 00:23:20,840
you know it's really easy to actually

00:23:18,720 --> 00:23:22,000
get back the information out of the rest

00:23:20,840 --> 00:23:24,799
api

00:23:22,000 --> 00:23:26,559
for example we uh so as i mentioned

00:23:24,799 --> 00:23:28,480
we've got partnership with terra data

00:23:26,559 --> 00:23:30,720
and with microsoft

00:23:28,480 --> 00:23:32,799
and teradata and microsoft do have

00:23:30,720 --> 00:23:34,559
system management solution as well

00:23:32,799 --> 00:23:36,559
and when they implement the system

00:23:34,559 --> 00:23:38,720
management solution they don't change

00:23:36,559 --> 00:23:39,840
everything to work with autonomous what

00:23:38,720 --> 00:23:41,919
they do is actually

00:23:39,840 --> 00:23:43,760
they modify they integrate through the

00:23:41,919 --> 00:23:44,320
umbra interface so they don't present on

00:23:43,760 --> 00:23:46,960
barry

00:23:44,320 --> 00:23:48,320
to the end user but they are getting all

00:23:46,960 --> 00:23:52,159
of the information

00:23:48,320 --> 00:23:52,799
from verification okay ombari got

00:23:52,159 --> 00:23:56,400
created

00:23:52,799 --> 00:24:00,880
based upon the experience of the team at

00:23:56,400 --> 00:24:00,880
yao when they were using

00:24:01,919 --> 00:24:05,120
basically that's pretty much uh it used

00:24:04,320 --> 00:24:07,279
to be called

00:24:05,120 --> 00:24:08,960
hadoop management console something like

00:24:07,279 --> 00:24:12,080
and that's a bit of a rewrite

00:24:08,960 --> 00:24:14,720
of what they used to have before okay so

00:24:12,080 --> 00:24:17,360
just to show you that you don't really

00:24:14,720 --> 00:24:19,440
need to have a lot of skill in order to

00:24:17,360 --> 00:24:22,159
install a cluster we're going to install

00:24:19,440 --> 00:24:22,159
one together

00:24:31,360 --> 00:24:34,400
so uh what i've done there is that i've

00:24:33,039 --> 00:24:35,840
installed an onbari server

00:24:34,400 --> 00:24:38,320
and installing an onbari server is

00:24:35,840 --> 00:24:40,400
extremely complex it consists of saying

00:24:38,320 --> 00:24:43,760
yummy install on barry server

00:24:40,400 --> 00:24:45,840
so that's kind of taking skill already

00:24:43,760 --> 00:24:47,919
and then the next step is to say ombari

00:24:45,840 --> 00:24:49,360
server space setup

00:24:47,919 --> 00:24:50,960
and you've got two stupid questions

00:24:49,360 --> 00:24:52,799
which are being asked

00:24:50,960 --> 00:24:54,159
and both you can keep the default one of

00:24:52,799 --> 00:24:56,320
them is do you want to modify the

00:24:54,159 --> 00:24:59,120
default password for the database

00:24:56,320 --> 00:25:00,159
you can choose yes or no and the other

00:24:59,120 --> 00:25:03,120
one is do you

00:25:00,159 --> 00:25:04,640
uh accept the oracle license for the jdk

00:25:03,120 --> 00:25:05,440
because he's going and downloaded the

00:25:04,640 --> 00:25:07,520
jdk

00:25:05,440 --> 00:25:08,640
uh the sanjelike sorry the era called

00:25:07,520 --> 00:25:10,320
jdk

00:25:08,640 --> 00:25:12,320
out of the oracle website and you've got

00:25:10,320 --> 00:25:13,919
to press yes in order to validate it

00:25:12,320 --> 00:25:15,520
okay so that's pretty much what i've

00:25:13,919 --> 00:25:17,440
just done in order to arrive to this

00:25:15,520 --> 00:25:18,480
page i know then i did on vari server

00:25:17,440 --> 00:25:21,200
start

00:25:18,480 --> 00:25:22,880
again that's taking quite rocket science

00:25:21,200 --> 00:25:26,880
then i've gone on to the

00:25:22,880 --> 00:25:28,400
the url log with the default credential

00:25:26,880 --> 00:25:30,080
and i'm alright i'm presented with this

00:25:28,400 --> 00:25:33,440
page okay so i'm being asked

00:25:30,080 --> 00:25:34,640
to give a name for my cluster i'm quite

00:25:33,440 --> 00:25:36,480
imaginative so

00:25:34,640 --> 00:25:38,000
i'll just use the name of my company

00:25:36,480 --> 00:25:40,080
normally i've got more imagination but

00:25:38,000 --> 00:25:41,360
last night i kind of lost some funerals

00:25:40,080 --> 00:25:43,279
i don't know what i did last night but

00:25:41,360 --> 00:25:46,320
you know

00:25:43,279 --> 00:25:48,000
uh then

00:25:46,320 --> 00:25:49,360
we are being asked for the list of holes

00:25:48,000 --> 00:25:53,760
that i want to install

00:25:49,360 --> 00:25:55,760
uh my cluster on okay so

00:25:53,760 --> 00:25:56,960
because i'm lazy i've already created

00:25:55,760 --> 00:26:00,640
the list and i'm just going to

00:25:56,960 --> 00:26:03,120
copy and paste so i've

00:26:00,640 --> 00:26:03,120
here we go

00:26:04,720 --> 00:26:08,559
so i've defined i've got fuels there

00:26:07,279 --> 00:26:10,559
which i've written okay

00:26:08,559 --> 00:26:12,000
so i've given them some name which are

00:26:10,559 --> 00:26:13,360
making sense like name node and trap

00:26:12,000 --> 00:26:16,240
tracker and so on and so forth

00:26:13,360 --> 00:26:17,520
it doesn't matter but here we go all of

00:26:16,240 --> 00:26:19,840
myos got already uh

00:26:17,520 --> 00:26:21,840
an ssh key which is being provided if

00:26:19,840 --> 00:26:23,200
you don't want to have a chair ssh key

00:26:21,840 --> 00:26:24,960
that's fine you take a bit more time you

00:26:23,200 --> 00:26:27,039
need to first install the agent

00:26:24,960 --> 00:26:29,120
on top of each node in manually so it's

00:26:27,039 --> 00:26:31,760
going to be a yummy install on variation

00:26:29,120 --> 00:26:33,200
again it takes a bit of skill but there

00:26:31,760 --> 00:26:34,720
in my case i already share

00:26:33,200 --> 00:26:36,880
the ssh key because it's actually

00:26:34,720 --> 00:26:38,960
running on the cloud so by default

00:26:36,880 --> 00:26:40,320
i've got one which is uh just need to

00:26:38,960 --> 00:26:41,279
use the right one here we go that's the

00:26:40,320 --> 00:26:45,440
wrong one

00:26:41,279 --> 00:26:48,720
there we go right

00:26:45,440 --> 00:26:50,799
and then i can

00:26:48,720 --> 00:26:52,640
if i've got a local repository for my uh

00:26:50,799 --> 00:26:56,320
on for my packages for my autumn

00:26:52,640 --> 00:26:58,480
packages then i can select it here

00:26:56,320 --> 00:27:00,240
and define the different way it's

00:26:58,480 --> 00:27:01,760
located it's not my case

00:27:00,240 --> 00:27:03,279
and if i want to have a custom place to

00:27:01,760 --> 00:27:05,760
install my jdk

00:27:03,279 --> 00:27:06,960
again i can modify it here not uh it's

00:27:05,760 --> 00:27:08,559
not my case again

00:27:06,960 --> 00:27:11,120
okay so i'm just going to register and

00:27:08,559 --> 00:27:12,720
confirm and it's going to go to every

00:27:11,120 --> 00:27:14,880
single node

00:27:12,720 --> 00:27:16,240
and make sure you can speak with it and

00:27:14,880 --> 00:27:19,679
start the ambarian

00:27:16,240 --> 00:27:21,760
on top of it okay

00:27:19,679 --> 00:27:22,799
so it's a demo it's live so normally it

00:27:21,760 --> 00:27:24,399
should break and we should have a big

00:27:22,799 --> 00:27:28,799
red failure at some point but you know

00:27:24,399 --> 00:27:28,799
let's cross finger for five minutes um

00:27:29,200 --> 00:27:35,120
so i've even as you can see i've also

00:27:32,640 --> 00:27:36,080
embedded as part of it the uh ombari

00:27:35,120 --> 00:27:38,000
server itself

00:27:36,080 --> 00:27:40,080
as part of the cluster okay because it's

00:27:38,000 --> 00:27:41,520
also going to become part of the claim

00:27:40,080 --> 00:27:43,120
luckily he's saying that he's able to

00:27:41,520 --> 00:27:46,320
speak with all of the node

00:27:43,120 --> 00:27:49,840
so then i'm being presented with a list

00:27:46,320 --> 00:27:49,840
of components

00:27:54,399 --> 00:27:58,960
okay i'm being presented with a list of

00:27:57,039 --> 00:28:01,919
components that i may want to install

00:27:58,960 --> 00:28:02,559
okay so i've got hdfs i've got my

00:28:01,919 --> 00:28:04,240
previews

00:28:02,559 --> 00:28:06,000
fair enough i kind of need this too if i

00:28:04,240 --> 00:28:08,799
want to do some adobe stuff

00:28:06,000 --> 00:28:10,320
uh then i can as i mentioned i've got

00:28:08,799 --> 00:28:11,279
nigels and ganglia which are available

00:28:10,320 --> 00:28:13,039
by default

00:28:11,279 --> 00:28:14,399
and all of the geophone components are

00:28:13,039 --> 00:28:16,320
there okay

00:28:14,399 --> 00:28:18,000
i'm not going to uh browser i'm just

00:28:16,320 --> 00:28:19,200
going to press next and that's pretty

00:28:18,000 --> 00:28:21,120
much it

00:28:19,200 --> 00:28:23,120
then i'm going to suggest based upon the

00:28:21,120 --> 00:28:24,080
amount of servers that i'm going to

00:28:23,120 --> 00:28:27,279
install my

00:28:24,080 --> 00:28:27,950
audio cluster on how may i how do i want

00:28:27,279 --> 00:28:29,440
to

00:28:27,950 --> 00:28:32,559
[Music]

00:28:29,440 --> 00:28:35,279
to spread my services okay so there

00:28:32,559 --> 00:28:37,120
i only have uh seven hosts apparently so

00:28:35,279 --> 00:28:37,679
it's going to try to put as much as

00:28:37,120 --> 00:28:41,039
possible

00:28:37,679 --> 00:28:42,640
on freenode but if i add let's say 200

00:28:41,039 --> 00:28:43,279
hosts it will try to split much more the

00:28:42,640 --> 00:28:45,039
services

00:28:43,279 --> 00:28:46,559
in order to have something which is much

00:28:45,039 --> 00:28:48,559
more scalable in the future

00:28:46,559 --> 00:28:50,559
okay so it's got a bit of intelligence

00:28:48,559 --> 00:28:52,159
part of it the only intelligence it

00:28:50,559 --> 00:28:53,679
doesn't have is that for example is

00:28:52,159 --> 00:28:56,000
chosen for my

00:28:53,679 --> 00:28:57,600
uh for one of my servers something which

00:28:56,000 --> 00:28:58,000
is called data node one which is not

00:28:57,600 --> 00:29:00,240
exactly

00:28:58,000 --> 00:29:01,520
what i was hoping for but anyway it

00:29:00,240 --> 00:29:05,120
doesn't really matter

00:29:01,520 --> 00:29:07,919
it's just a name so then we press next

00:29:05,120 --> 00:29:08,960
and it's asking me where do i do i want

00:29:07,919 --> 00:29:10,399
to have hdfs

00:29:08,960 --> 00:29:13,039
installed on all of the components yes

00:29:10,399 --> 00:29:14,960
or no and as i've also said that i

00:29:13,039 --> 00:29:16,799
wanted to have a hbase install is

00:29:14,960 --> 00:29:18,480
suggesting to install a region server on

00:29:16,799 --> 00:29:26,080
every single data node

00:29:18,480 --> 00:29:31,679
whatever i'm going to say next

00:29:26,080 --> 00:29:31,679
okay i've got some issues with my

00:29:38,840 --> 00:29:41,840
mastery

00:29:44,480 --> 00:29:48,240
either is a mouse or it's the internet

00:29:46,480 --> 00:29:51,679
but

00:29:48,240 --> 00:29:51,679
one of the two is not too happy

00:29:52,840 --> 00:29:57,520
hello

00:29:54,960 --> 00:29:58,799
so now i arrive to the to the last spot

00:29:57,520 --> 00:30:00,159
and i can review all of the different

00:29:58,799 --> 00:30:02,320
configuration files and i can

00:30:00,159 --> 00:30:04,000
pass some additional information on top

00:30:02,320 --> 00:30:07,600
of it so for example there

00:30:04,000 --> 00:30:10,480
i mean the hdfs configuration

00:30:07,600 --> 00:30:11,679
we tend to have a typical adobe server

00:30:10,480 --> 00:30:14,159
for data node we'll have

00:30:11,679 --> 00:30:14,799
a lot of different disk this disk won't

00:30:14,159 --> 00:30:16,480
be ready

00:30:14,799 --> 00:30:18,159
because we're trying to spread the i o

00:30:16,480 --> 00:30:19,760
across all of the local disk at the same

00:30:18,159 --> 00:30:20,640
time because we said that we were happy

00:30:19,760 --> 00:30:23,279
to lose a disk

00:30:20,640 --> 00:30:25,120
because hdfs is resilient by itself so i

00:30:23,279 --> 00:30:27,360
don't need to have red actually

00:30:25,120 --> 00:30:29,039
by having all of the i o going on at the

00:30:27,360 --> 00:30:30,880
same in parallel to all of the disks

00:30:29,039 --> 00:30:32,720
i'm going to have a better speed in

00:30:30,880 --> 00:30:34,720
order to access my disk

00:30:32,720 --> 00:30:36,480
that by having a red device effectively

00:30:34,720 --> 00:30:38,880
okay so

00:30:36,480 --> 00:30:39,760
if i had what we what will typically

00:30:38,880 --> 00:30:42,240
happen is that

00:30:39,760 --> 00:30:42,799
if i've got a lot of local disk each

00:30:42,240 --> 00:30:44,880
disk

00:30:42,799 --> 00:30:47,520
that's not on the name node actually it

00:30:44,880 --> 00:30:50,640
will be on the data node

00:30:47,520 --> 00:30:52,880
here we go each disk will be a different

00:30:50,640 --> 00:30:54,240
a specific mount point and we will try

00:30:52,880 --> 00:30:56,799
to mount we will

00:30:54,240 --> 00:30:58,159
uh mention all of the man point in this

00:30:56,799 --> 00:30:59,679
folder for example

00:30:58,159 --> 00:31:01,279
there pretty much if you're familiar

00:30:59,679 --> 00:31:05,360
with the hdfs site

00:31:01,279 --> 00:31:07,279
dot xml you think you can find hdfsi.com

00:31:05,360 --> 00:31:08,880
you can find all of the information that

00:31:07,279 --> 00:31:10,399
you will find

00:31:08,880 --> 00:31:12,000
whether you find the configuration file

00:31:10,399 --> 00:31:15,679
available from the web interface

00:31:12,000 --> 00:31:18,960
okay here we go

00:31:15,679 --> 00:31:19,519
that's for example for hdfs and it's the

00:31:18,960 --> 00:31:21,760
same for

00:31:19,519 --> 00:31:22,880
exactly all of the different options

00:31:21,760 --> 00:31:27,039
which are available

00:31:22,880 --> 00:31:29,039
into the into the solution okay there is

00:31:27,039 --> 00:31:30,320
as you can see there is some red point

00:31:29,039 --> 00:31:32,080
and the right point just means that i've

00:31:30,320 --> 00:31:34,559
got some input which is expected for me

00:31:32,080 --> 00:31:36,080
before it's allowing me to go first

00:31:34,559 --> 00:31:38,240
i'm going to stop here because there is

00:31:36,080 --> 00:31:40,480
no need for us to sit through and

00:31:38,240 --> 00:31:41,679
start for an installation of an onbari

00:31:40,480 --> 00:31:43,039
server it's going to go

00:31:41,679 --> 00:31:44,799
quickly it's going to take 15 minutes

00:31:43,039 --> 00:31:46,159
you know to install all of the packages

00:31:44,799 --> 00:31:47,360
and so on and so forth

00:31:46,159 --> 00:31:48,880
one of the beauty is that when he

00:31:47,360 --> 00:31:50,399
finished to install before it starts the

00:31:48,880 --> 00:31:52,960
service he's got what they call

00:31:50,399 --> 00:31:54,559
uh what we call a smoke solution and so

00:31:52,960 --> 00:31:56,399
before it starts hdfs

00:31:54,559 --> 00:31:58,559
it's going to run a test to make sure

00:31:56,399 --> 00:32:00,080
that the sdfs is what no it started gfs

00:31:58,559 --> 00:32:02,320
and after he started it you're going to

00:32:00,080 --> 00:32:04,159
test to make sure that sdfs is working

00:32:02,320 --> 00:32:05,519
then we'll try to start mapreduce and do

00:32:04,159 --> 00:32:06,799
exactly the same we will launch a small

00:32:05,519 --> 00:32:07,440
test to make sure that mapreduce is

00:32:06,799 --> 00:32:09,279
working

00:32:07,440 --> 00:32:11,120
the same with hp the same with if the

00:32:09,279 --> 00:32:14,159
same with everything okay

00:32:11,120 --> 00:32:17,360
so i've got one which i'm

00:32:14,159 --> 00:32:20,159
already running there okay

00:32:17,360 --> 00:32:21,120
so that's uh but that's an embarrassed

00:32:20,159 --> 00:32:23,039
server

00:32:21,120 --> 00:32:25,200
which is monitoring a cluster already

00:32:23,039 --> 00:32:26,720
okay so it's a 12 node cluster that

00:32:25,200 --> 00:32:29,279
we've got running in

00:32:26,720 --> 00:32:29,919
on the cloud i believe okay and from

00:32:29,279 --> 00:32:31,600
here

00:32:29,919 --> 00:32:33,120
i've got access to all of the different

00:32:31,600 --> 00:32:34,240
services which are available on the top

00:32:33,120 --> 00:32:37,600
of my cloud okay

00:32:34,240 --> 00:32:39,840
so i can click on hdfs for example and

00:32:37,600 --> 00:32:41,279
it will tell me what is my name node

00:32:39,840 --> 00:32:44,720
what the my second

00:32:41,279 --> 00:32:45,760
name node is how much storage do i have

00:32:44,720 --> 00:32:50,880
got a variable

00:32:45,760 --> 00:32:54,480
what is the current memory usage for the

00:32:50,880 --> 00:32:57,039
for the hdfs part effectively so

00:32:54,480 --> 00:32:58,559
it can be we're trying to provide you

00:32:57,039 --> 00:32:59,360
with some metrics which are relevant to

00:32:58,559 --> 00:33:01,840
you so this

00:32:59,360 --> 00:33:03,519
metrics are actually stored in ganglia

00:33:01,840 --> 00:33:07,519
and we're just displaying them

00:33:03,519 --> 00:33:10,320
into in the interface okay you can also

00:33:07,519 --> 00:33:11,760
from this space each uh so the name node

00:33:10,320 --> 00:33:13,120
the task tracker the job tracker they

00:33:11,760 --> 00:33:15,279
all have a standalone web

00:33:13,120 --> 00:33:17,679
web user interface and from here you can

00:33:15,279 --> 00:33:19,600
go and go directly onto the web user

00:33:17,679 --> 00:33:22,840
interface of the data node for example

00:33:19,600 --> 00:33:24,799
or the name node to see what's happening

00:33:22,840 --> 00:33:28,960
okay

00:33:24,799 --> 00:33:28,960
so i can do that for uh

00:33:33,039 --> 00:33:38,000
i'm using okay i can do that for my hdfs

00:33:36,320 --> 00:33:39,760
but i can also do it for my produce for

00:33:38,000 --> 00:33:42,240
ivan 4h catalog okay

00:33:39,760 --> 00:33:43,120
there is something else which is

00:33:42,240 --> 00:33:45,120
interesting

00:33:43,120 --> 00:33:46,480
we've got what we call a neat map okay

00:33:45,120 --> 00:33:49,440
and a neat map will

00:33:46,480 --> 00:33:51,360
quickly give you a view of isu cluster

00:33:49,440 --> 00:33:52,000
doing for example there i can see that

00:33:51,360 --> 00:33:54,080
in this space

00:33:52,000 --> 00:33:55,200
i've got no issue everything is green

00:33:54,080 --> 00:33:58,159
and it's all good

00:33:55,200 --> 00:33:59,679
when i look at the memory usage i can

00:33:58,159 --> 00:34:00,720
see that i've got some server which are

00:33:59,679 --> 00:34:02,880
slightly more

00:34:00,720 --> 00:34:04,399
used in term of memory so i can have a

00:34:02,880 --> 00:34:06,000
quick look at what's up what's running

00:34:04,399 --> 00:34:08,480
on this server and why is it

00:34:06,000 --> 00:34:10,159
busy and is it normal that it's so busy

00:34:08,480 --> 00:34:11,040
well we can see that actually the server

00:34:10,159 --> 00:34:13,599
is running

00:34:11,040 --> 00:34:15,119
ganglia nagios the name node so one of

00:34:13,599 --> 00:34:17,200
the most important component

00:34:15,119 --> 00:34:18,399
okay and it's all on one server we

00:34:17,200 --> 00:34:20,159
should probably have spread it into

00:34:18,399 --> 00:34:20,960
multiple seven we won't have this issue

00:34:20,159 --> 00:34:24,000
about memory

00:34:20,960 --> 00:34:26,240
okay but it's a quick you know imagine

00:34:24,000 --> 00:34:28,639
you've got now a really large cluster

00:34:26,240 --> 00:34:30,159
and you when you look at the disk space

00:34:28,639 --> 00:34:31,040
you've got one node which is saying okay

00:34:30,159 --> 00:34:33,440
actually i'm

00:34:31,040 --> 00:34:34,879
copying a lot of data to this node it's

00:34:33,440 --> 00:34:36,399
a great way of seeing that you may have

00:34:34,879 --> 00:34:38,000
an issue and you may want to

00:34:36,399 --> 00:34:39,760
change something or you've got a problem

00:34:38,000 --> 00:34:42,320
okay so it's a good visualization tool

00:34:39,760 --> 00:34:42,320
effectively

00:34:42,480 --> 00:34:48,720
then every single service can be a stop

00:34:46,079 --> 00:34:50,720
or start directly from here okay so i

00:34:48,720 --> 00:34:53,040
can stop it i can start it

00:34:50,720 --> 00:34:54,480
i can put the node i can buy because

00:34:53,040 --> 00:34:56,000
that's what i was mentioning here we can

00:34:54,480 --> 00:34:58,160
run a smoke test so that's to make sure

00:34:56,000 --> 00:34:59,599
that my hdfs solution is still working

00:34:58,160 --> 00:35:02,079
and still behaving in exactly the same

00:34:59,599 --> 00:35:05,040
fashions i was expecting

00:35:02,079 --> 00:35:06,000
um and so that's the different

00:35:05,040 --> 00:35:08,400
information of my

00:35:06,000 --> 00:35:09,520
hdfs on the right hand side we've got

00:35:08,400 --> 00:35:12,640
the nagios alert

00:35:09,520 --> 00:35:13,920
okay so if we add an issue now so it's

00:35:12,640 --> 00:35:15,920
some predefined

00:35:13,920 --> 00:35:17,119
nagio salad which are available there

00:35:15,920 --> 00:35:19,280
and if we had an issue

00:35:17,119 --> 00:35:20,880
it will be brought back here as you can

00:35:19,280 --> 00:35:24,400
see there is never an issue with

00:35:20,880 --> 00:35:24,400
autumn works it's all green amazing

00:35:25,680 --> 00:35:28,839
and then we've got again some of the

00:35:27,520 --> 00:35:31,760
matrix which are

00:35:28,839 --> 00:35:34,079
relevant for the hdfs part for example

00:35:31,760 --> 00:35:34,079
okay

00:35:34,800 --> 00:35:38,720
if i want to modify my configuration of

00:35:37,200 --> 00:35:40,800
hdfs

00:35:38,720 --> 00:35:43,599
again it's something you can do quite

00:35:40,800 --> 00:35:45,920
easily through the web interface

00:35:43,599 --> 00:35:47,359
most of the time it's not most of the

00:35:45,920 --> 00:35:50,000
time actually it's something that we

00:35:47,359 --> 00:35:51,680
require to have a restart so we normally

00:35:50,000 --> 00:35:53,440
ask you to first stop the services

00:35:51,680 --> 00:35:56,800
modify configuration file and then

00:35:53,440 --> 00:35:56,800
restart the services okay

00:35:57,760 --> 00:36:02,160
so that's the part about how do you

00:35:59,599 --> 00:36:03,760
manage and how do you configure your

00:36:02,160 --> 00:36:05,440
your host otherwise that's again the

00:36:03,760 --> 00:36:07,440
host in your different view

00:36:05,440 --> 00:36:08,880
but if i click on one physical host for

00:36:07,440 --> 00:36:10,720
example i also have the performance

00:36:08,880 --> 00:36:12,400
metric coming out of ganglia

00:36:10,720 --> 00:36:14,640
just for this physical loss before what

00:36:12,400 --> 00:36:17,200
we are looking at where the cluster

00:36:14,640 --> 00:36:19,280
of nodes working together there we are

00:36:17,200 --> 00:36:20,800
looking at just a metric on one physical

00:36:19,280 --> 00:36:22,960
loss

00:36:20,800 --> 00:36:24,079
but that's all about the performance and

00:36:22,960 --> 00:36:24,880
there is something else which is

00:36:24,079 --> 00:36:28,400
available from

00:36:24,880 --> 00:36:30,079
barry which is a different job which i'm

00:36:28,400 --> 00:36:31,119
running within my organization so

00:36:30,079 --> 00:36:33,119
there's a different

00:36:31,119 --> 00:36:34,240
mapreduce job which are being run within

00:36:33,119 --> 00:36:36,079
the organization

00:36:34,240 --> 00:36:37,760
so for example you can see that i've got

00:36:36,079 --> 00:36:38,560
a job there which is a mapreduce code

00:36:37,760 --> 00:36:40,240
i've got a i

00:36:38,560 --> 00:36:42,320
script which i've been running i've got

00:36:40,240 --> 00:36:44,000
a pig and so on and so forth

00:36:42,320 --> 00:36:45,359
how much data did i took in how much

00:36:44,000 --> 00:36:47,520
data did i output it

00:36:45,359 --> 00:36:50,800
and so on something which is interesting

00:36:47,520 --> 00:36:52,240
to look at is also the swimlane okay so

00:36:50,800 --> 00:36:54,000
something i forgot to mention when i

00:36:52,240 --> 00:36:54,560
spoke about pig and ive is that pigeon

00:36:54,000 --> 00:36:56,240
knife

00:36:54,560 --> 00:36:57,920
they are when you're creating a big

00:36:56,240 --> 00:36:58,480
script or when you're writing a knife

00:36:57,920 --> 00:37:00,160
query

00:36:58,480 --> 00:37:02,079
what they're first doing is actually

00:37:00,160 --> 00:37:03,599
translating this big script as it's

00:37:02,079 --> 00:37:06,480
translating the site query

00:37:03,599 --> 00:37:07,359
into effectively some mapreduce job okay

00:37:06,480 --> 00:37:10,560
so

00:37:07,359 --> 00:37:11,839
when you look from the name node

00:37:10,560 --> 00:37:14,720
sorry from the job tracker user

00:37:11,839 --> 00:37:17,359
interface your big script has become

00:37:14,720 --> 00:37:18,880
has become five different uh map reduced

00:37:17,359 --> 00:37:19,680
job so it's sometimes quite hard to

00:37:18,880 --> 00:37:21,920
track

00:37:19,680 --> 00:37:23,200
which what happened over this time there

00:37:21,920 --> 00:37:24,079
what we're presenting is that we're

00:37:23,200 --> 00:37:25,520
representing

00:37:24,079 --> 00:37:27,200
uh i think there's a big script that's

00:37:25,520 --> 00:37:28,000
done is that i've clicked on we're

00:37:27,200 --> 00:37:31,200
representing

00:37:28,000 --> 00:37:32,320
those five uh this this job which have

00:37:31,200 --> 00:37:34,480
been

00:37:32,320 --> 00:37:35,359
so representing my peak script which has

00:37:34,480 --> 00:37:37,280
been split

00:37:35,359 --> 00:37:38,400
into this five job okay so we can see

00:37:37,280 --> 00:37:41,359
that for example

00:37:38,400 --> 00:37:42,400
that was my two uh map phase and then i

00:37:41,359 --> 00:37:44,480
had a

00:37:42,400 --> 00:37:45,599
probably a shuffle and then some

00:37:44,480 --> 00:37:48,560
different uh

00:37:45,599 --> 00:37:49,520
radio space after it okay i can have a

00:37:48,560 --> 00:37:52,240
look

00:37:49,520 --> 00:37:54,000
at here but i can also look at where did

00:37:52,240 --> 00:37:56,880
i spend my time okay so

00:37:54,000 --> 00:37:58,880
that was first the mapping the shuffling

00:37:56,880 --> 00:38:01,839
and then the review step

00:37:58,880 --> 00:38:03,440
it's it's got all of these story as well

00:38:01,839 --> 00:38:05,040
so if you've got a job which which

00:38:03,440 --> 00:38:06,880
you're running regularly you can also

00:38:05,040 --> 00:38:08,880
check that the performance are

00:38:06,880 --> 00:38:10,720
not being degraded no are not

00:38:08,880 --> 00:38:12,560
degradation okay that you always have

00:38:10,720 --> 00:38:14,000
the same performance

00:38:12,560 --> 00:38:15,920
and to get you get all of this

00:38:14,000 --> 00:38:16,880
information directly from this point

00:38:15,920 --> 00:38:19,839
effectively

00:38:16,880 --> 00:38:19,839
okay

00:38:22,000 --> 00:38:27,119
and as let's see something something

00:38:25,760 --> 00:38:29,359
else that we're going to add

00:38:27,119 --> 00:38:31,040
is the adobe user experience so i don't

00:38:29,359 --> 00:38:33,119
know if you're familiar with you

00:38:31,040 --> 00:38:34,160
you give you with a small web interface

00:38:33,119 --> 00:38:36,400
which lets you

00:38:34,160 --> 00:38:38,320
rag write big script which lets you

00:38:36,400 --> 00:38:39,200
write i've which let you do quite a lot

00:38:38,320 --> 00:38:42,079
of thing

00:38:39,200 --> 00:38:43,440
and with you it's it's uh it's going to

00:38:42,079 --> 00:38:44,720
be a new tab that you're going to have

00:38:43,440 --> 00:38:46,560
available here

00:38:44,720 --> 00:38:48,240
and you will be able to effectively have

00:38:46,560 --> 00:38:48,720
a you know an editor to write your big

00:38:48,240 --> 00:38:50,960
script

00:38:48,720 --> 00:38:52,800
with some syntax highlighting and so on

00:38:50,960 --> 00:38:54,160
and so forth okay so it makes it fairly

00:38:52,800 --> 00:38:56,240
easy and straightforward

00:38:54,160 --> 00:38:57,760
to learn how to or to write big script

00:38:56,240 --> 00:38:59,359
at least

00:38:57,760 --> 00:39:00,880
there is just maybe one last thing that

00:38:59,359 --> 00:39:02,960
i would like if you

00:39:00,880 --> 00:39:04,480
want to learn a bit more about hadoop

00:39:02,960 --> 00:39:06,320
and how to use hadoop there is something

00:39:04,480 --> 00:39:08,079
that auto works does quite well

00:39:06,320 --> 00:39:09,680
we've got a sandbox which is a

00:39:08,079 --> 00:39:12,079
standalone virtual machine

00:39:09,680 --> 00:39:13,040
that you can go and download from from

00:39:12,079 --> 00:39:14,640
our website

00:39:13,040 --> 00:39:17,040
and as part of this sandbox you've got a

00:39:14,640 --> 00:39:18,480
standalone uh adobe

00:39:17,040 --> 00:39:20,480
not cluster because it's a standalone

00:39:18,480 --> 00:39:21,440
node but you've got an adobe environment

00:39:20,480 --> 00:39:23,359
running

00:39:21,440 --> 00:39:25,119
and you also have some tutorials so on

00:39:23,359 --> 00:39:26,960
the left hand side you've got

00:39:25,119 --> 00:39:28,880
the tutorial where you can you know

00:39:26,960 --> 00:39:30,320
we're telling you what to do with pig

00:39:28,880 --> 00:39:31,839
and how to use pig and how to do use

00:39:30,320 --> 00:39:32,880
life and introducing you to this kind of

00:39:31,839 --> 00:39:34,960
tools

00:39:32,880 --> 00:39:36,079
and on the right you've got this hadoop

00:39:34,960 --> 00:39:38,079
user experience

00:39:36,079 --> 00:39:39,680
which let you do it effectively on the

00:39:38,079 --> 00:39:41,040
same time okay

00:39:39,680 --> 00:39:45,280
do you have any question regarding

00:39:41,040 --> 00:39:45,280
onbari or regarding hadoop effectively

00:39:48,160 --> 00:39:53,440
do you use yourself hadoop or have you

00:39:50,480 --> 00:39:57,839
did any of you started using hadoop

00:39:53,440 --> 00:39:57,839
you do of that surprise surprise

00:39:58,800 --> 00:40:03,760
no any question regarding hadoop on

00:40:01,119 --> 00:40:03,760
barry or

00:40:04,240 --> 00:40:06,560
nope

00:40:09,200 --> 00:40:19,839

YouTube URL: https://www.youtube.com/watch?v=y6NA099Tb_w


