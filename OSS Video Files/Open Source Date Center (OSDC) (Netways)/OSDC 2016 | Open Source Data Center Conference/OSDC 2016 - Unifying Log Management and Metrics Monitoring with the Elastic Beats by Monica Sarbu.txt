Title: OSDC 2016 - Unifying Log Management and Metrics Monitoring with the Elastic Beats by Monica Sarbu
Publication date: 2016-05-02
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	The Beats are a friendly army of lightweight agents that installed on your servers capture operational data and ship it to Elasticsearch for analysis. They are open source, written in Golang, and maintained by Elastic, the company behind Elasticsearch, Logstash, and Kibana.
This talk will present the first three Beats: Topbeat for system level metrics, Filebeat for log files and Packetbeat for wire data. It will also demonstrate how to combine them with Logstash and Kibana in one advanced monitoring solution, unifying log management, metrics monitoring and system stats. Finally, you will learn how to create a new Beat from scratch using Golang and the libbeat framework to capture any type of information and ship it to Elasticsearch.
Captions: 
	00:00:12,030 --> 00:00:17,190
thank you hi everyone so I'm here to

00:00:15,090 --> 00:00:19,619
talk about unifying locks and metrics in

00:00:17,190 --> 00:00:21,720
velocity beads and I would like to start

00:00:19,619 --> 00:00:25,529
by asking you a few questions how many

00:00:21,720 --> 00:00:28,500
of you are using elasticsearch oh that's

00:00:25,529 --> 00:00:31,380
quite many how many how many of you are

00:00:28,500 --> 00:00:35,720
using elastic bits to ingest data to

00:00:31,380 --> 00:00:39,900
elasticsearch okay that's quite good

00:00:35,720 --> 00:00:43,110
okay then I will start by saying a few

00:00:39,900 --> 00:00:44,399
words about myself so I'm Tim Reid at

00:00:43,110 --> 00:00:47,370
elastic beach

00:00:44,399 --> 00:00:50,250
I'm also software engineer so I code

00:00:47,370 --> 00:00:53,790
from time to time I join elastic one

00:00:50,250 --> 00:00:56,460
year ago and today I will talk to you

00:00:53,790 --> 00:00:59,430
all so I will start by telling you about

00:00:56,460 --> 00:01:02,879
the beach so feet are lightweight

00:00:59,430 --> 00:01:05,899
shippers that collected sheep all kinds

00:01:02,879 --> 00:01:09,780
of operational data and send it to

00:01:05,899 --> 00:01:11,789
elasticsearch let's take one by one bits

00:01:09,780 --> 00:01:13,590
are lightweight shippers what I mean by

00:01:11,789 --> 00:01:15,720
that is that they are lightweight

00:01:13,590 --> 00:01:18,900
applications they are written in go long

00:01:15,720 --> 00:01:23,330
there install is an agent on your server

00:01:18,900 --> 00:01:26,400
and they have the random dependencies

00:01:23,330 --> 00:01:30,540
they collect all kinds of operational

00:01:26,400 --> 00:01:34,259
data so we have file bits to collect the

00:01:30,540 --> 00:01:37,830
logs we have will not be to collect

00:01:34,259 --> 00:01:40,920
windows event logs we have top it to

00:01:37,830 --> 00:01:44,610
collect system statistics like CPU usage

00:01:40,920 --> 00:01:46,860
disk usage memory usage and so on we

00:01:44,610 --> 00:01:50,909
have packet bits to collect insights

00:01:46,860 --> 00:01:53,909
from your network traffic and we are

00:01:50,909 --> 00:01:56,780
working currently in releasing metric

00:01:53,909 --> 00:02:00,810
bit that collect metrics from external

00:01:56,780 --> 00:02:04,430
services after they collect all this

00:02:00,810 --> 00:02:07,170
data they ship the data to elasticsearch

00:02:04,430 --> 00:02:09,390
so because the Vantage of using

00:02:07,170 --> 00:02:12,150
elasticsearch is that you don't have to

00:02:09,390 --> 00:02:12,570
anticipate what matrix you need in the

00:02:12,150 --> 00:02:14,880
future

00:02:12,570 --> 00:02:17,459
you just push the raw data into

00:02:14,880 --> 00:02:20,129
elasticsearch and then you can create

00:02:17,459 --> 00:02:24,660
the metrics on the fly when you need

00:02:20,129 --> 00:02:27,960
them so they

00:02:24,660 --> 00:02:30,570
plastic stack it was previously called

00:02:27,960 --> 00:02:32,820
L stack now is called elastic stack it

00:02:30,570 --> 00:02:35,760
contains the injure side the logstash

00:02:32,820 --> 00:02:37,800
and bits that are pushing the data that

00:02:35,760 --> 00:02:40,260
are collecting the data and pushing the

00:02:37,800 --> 00:02:43,220
data to elasticsearch and then on top of

00:02:40,260 --> 00:02:46,290
it you can use Cabana to visualize it

00:02:43,220 --> 00:02:50,280
but today now in my talk I will talk

00:02:46,290 --> 00:02:52,430
mostly about the beach so packet bid is

00:02:50,280 --> 00:02:54,990
the first bit that we created it

00:02:52,430 --> 00:02:57,960
captures insights from the network

00:02:54,990 --> 00:03:00,690
packet the way packet with work is by

00:02:57,960 --> 00:03:03,360
sniffing the network traffic what

00:03:00,690 --> 00:03:05,790
sniffing means is that you get a copy of

00:03:03,360 --> 00:03:08,670
your network packet and this can be done

00:03:05,790 --> 00:03:13,530
as operating system level or hardwood

00:03:08,670 --> 00:03:16,380
level using mirror port film you know

00:03:13,530 --> 00:03:20,790
report functionality of switches or

00:03:16,380 --> 00:03:23,760
dedicated tap devices and packet bid is

00:03:20,790 --> 00:03:27,210
completely passive this means that you

00:03:23,760 --> 00:03:29,640
have zero latency overhead and if the

00:03:27,210 --> 00:03:31,860
application that is monitoring your

00:03:29,640 --> 00:03:37,470
network dies then your infrastructure is

00:03:31,860 --> 00:03:40,470
not affected packet read is not the

00:03:37,470 --> 00:03:44,310
first open source tool that is using

00:03:40,470 --> 00:03:46,890
sniffing Wireshark and TCP dump are the

00:03:44,310 --> 00:03:49,290
most well known ones and also there are

00:03:46,890 --> 00:03:54,390
other intrusion detection systems like

00:03:49,290 --> 00:03:57,239
snort they use wired data and they are

00:03:54,390 --> 00:03:59,940
used especially in the security domain

00:03:57,239 --> 00:04:01,830
for troubleshooting network issues for

00:03:59,940 --> 00:04:04,410
troubleshooting applications that are

00:04:01,830 --> 00:04:09,150
using standard protocols and for

00:04:04,410 --> 00:04:11,370
performance analytics yeah let's see how

00:04:09,150 --> 00:04:14,010
you will do if you want to monitor your

00:04:11,370 --> 00:04:16,890
servers in your network using open

00:04:14,010 --> 00:04:20,100
source tools so an option will be to

00:04:16,890 --> 00:04:24,210
associate to each of these servers then

00:04:20,100 --> 00:04:26,940
start a trace using TCP dump and then

00:04:24,210 --> 00:04:30,090
collect all the traces for all the

00:04:26,940 --> 00:04:32,550
servers to a common place then merge all

00:04:30,090 --> 00:04:34,919
these traces together into one trace and

00:04:32,550 --> 00:04:36,410
only then you can visualize you will for

00:04:34,919 --> 00:04:37,770
Wireshark

00:04:36,410 --> 00:04:42,330
Inc

00:04:37,770 --> 00:04:44,490
as you have sorry in case you have quite

00:04:42,330 --> 00:04:48,449
many servers this might be a challenge

00:04:44,490 --> 00:04:51,780
right so this is a problem that packet

00:04:48,449 --> 00:04:56,460
basalt impact of it you can you are able

00:04:51,780 --> 00:05:04,319
to visualize your network packets your

00:04:56,460 --> 00:05:07,770
traffic using in real-time so so the way

00:05:04,319 --> 00:05:10,069
pega tree works is by capturing the

00:05:07,770 --> 00:05:12,990
network traffic decoding the network

00:05:10,069 --> 00:05:15,690
packet then correlate the requests or

00:05:12,990 --> 00:05:18,090
response into transactions and extract

00:05:15,690 --> 00:05:23,340
measurements and then send them through

00:05:18,090 --> 00:05:27,000
elastic search so we have quite many

00:05:23,340 --> 00:05:30,900
availability colors in packet bit HTTP

00:05:27,000 --> 00:05:33,919
in my sequel PostgreSQL ready strips RPC

00:05:30,900 --> 00:05:39,930
memcache and also from the community our

00:05:33,919 --> 00:05:44,699
MongoDB ICMP DNS mqp NFS and also you

00:05:39,930 --> 00:05:46,710
can adhere your own decoder in the

00:05:44,699 --> 00:05:49,500
configuration file of the key bit you

00:05:46,710 --> 00:05:52,460
just need to specify what interface to

00:05:49,500 --> 00:05:55,770
sniff here in this example is any device

00:05:52,460 --> 00:05:59,099
but this works only on Linux and also

00:05:55,770 --> 00:06:02,539
for each protocol you have to specify on

00:05:59,099 --> 00:06:06,029
each port to listen for the traffic

00:06:02,539 --> 00:06:08,340
after you start packet with then it will

00:06:06,029 --> 00:06:10,289
step it will start sending the data to

00:06:08,340 --> 00:06:12,090
elasticsearch and only then you can

00:06:10,289 --> 00:06:14,909
visualize the data of kibana

00:06:12,090 --> 00:06:18,870
here is an example of a dashboard that

00:06:14,909 --> 00:06:22,319
is using data that is visualizing the

00:06:18,870 --> 00:06:25,229
data that are coming from packet bit we

00:06:22,319 --> 00:06:27,870
are offering a set of sample dashboards

00:06:25,229 --> 00:06:32,340
that you can use as a starting point for

00:06:27,870 --> 00:06:35,550
your customized dashboards let's zoom in

00:06:32,340 --> 00:06:38,340
to each of these widgets so here you see

00:06:35,550 --> 00:06:42,150
the map is this is a client location map

00:06:38,340 --> 00:06:44,430
and you can see here that most of the

00:06:42,150 --> 00:06:49,320
HTTP requests are coming from Berlin

00:06:44,430 --> 00:06:53,190
actually because this is a it's a it's a

00:06:49,320 --> 00:06:55,770
traffic that is generated from below

00:06:53,190 --> 00:06:59,060
another widget that you will see our web

00:06:55,770 --> 00:07:03,410
transactions database transactions

00:06:59,060 --> 00:07:09,750
response time repetition over time and

00:07:03,410 --> 00:07:11,550
per sentence another widget interesting

00:07:09,750 --> 00:07:15,090
widgets that you can have slide a

00:07:11,550 --> 00:07:18,930
latency histogram and also the errors

00:07:15,090 --> 00:07:22,980
for my sequel an HTTP transactions and

00:07:18,930 --> 00:07:28,770
here you can see maybe you can't see but

00:07:22,980 --> 00:07:32,010
here you can see that there is a pike so

00:07:28,770 --> 00:07:34,110
the number of HTTP transactions and the

00:07:32,010 --> 00:07:37,980
number of Mexico transaction it's a bit

00:07:34,110 --> 00:07:41,190
higher than usual so if we want to just

00:07:37,980 --> 00:07:46,170
see what's happening in in this period

00:07:41,190 --> 00:07:51,350
of time we can just select this to this

00:07:46,170 --> 00:07:54,360
area and then we'll be able to see

00:07:51,350 --> 00:07:57,360
details about the transactions that are

00:07:54,360 --> 00:08:00,810
in that specific time frame and also you

00:07:57,360 --> 00:08:05,430
can see in Cabana the time frame is also

00:08:00,810 --> 00:08:08,060
changed another knife things that you

00:08:05,430 --> 00:08:12,540
can do in Cabana is that you can have

00:08:08,060 --> 00:08:16,110
these top processes the top HTTP query

00:08:12,540 --> 00:08:21,000
story and so you can see here for

00:08:16,110 --> 00:08:23,130
example you can see how many get HTTP

00:08:21,000 --> 00:08:28,440
queries of this type the word and the

00:08:23,130 --> 00:08:31,080
number of them and so on in the

00:08:28,440 --> 00:08:33,590
discovery tab of cabana you'll be able

00:08:31,080 --> 00:08:36,750
to see all the transactions that are

00:08:33,590 --> 00:08:39,240
collected by a packet bit and if you

00:08:36,750 --> 00:08:42,270
click on one of them then you'll be able

00:08:39,240 --> 00:08:44,520
to see more details about it so here for

00:08:42,270 --> 00:08:50,520
example there is a my sequel transaction

00:08:44,520 --> 00:08:53,220
and you can see the the type as a method

00:08:50,520 --> 00:08:55,350
of my secret transaction it was a select

00:08:53,220 --> 00:08:58,830
you can see if it was an error or not

00:08:55,350 --> 00:09:03,780
you can see how many fields are returned

00:08:58,830 --> 00:09:05,100
and how many rows are returned in case

00:09:03,780 --> 00:09:10,440
there was a

00:09:05,100 --> 00:09:13,470
HTTP transaction then you can see if it

00:09:10,440 --> 00:09:17,010
was a you can see the response code you

00:09:13,470 --> 00:09:19,590
can see the contact length of a response

00:09:17,010 --> 00:09:22,620
and things like this and what is

00:09:19,590 --> 00:09:24,330
important or what's important to note is

00:09:22,620 --> 00:09:27,450
that also you can see this client

00:09:24,330 --> 00:09:33,320
location that is calculated based on the

00:09:27,450 --> 00:09:40,950
client IP that can be used to calculate

00:09:33,320 --> 00:09:43,170
where is the client location map packet

00:09:40,950 --> 00:09:45,330
bit is also able to show you the flows

00:09:43,170 --> 00:09:47,430
not only the transaction and this is

00:09:45,330 --> 00:09:49,980
useful in case spec if it doesn't have

00:09:47,430 --> 00:09:53,100
support for a certain protocol or is

00:09:49,980 --> 00:09:57,270
using TLS and what this means that it

00:09:53,100 --> 00:09:59,970
gets data from about IP TCP UDP layers

00:09:57,270 --> 00:10:02,790
and data like the number of packets

00:09:59,970 --> 00:10:05,580
returns missions inter arrival time and

00:10:02,790 --> 00:10:07,320
for for using flows you just need to

00:10:05,580 --> 00:10:12,420
configure the flow section in the

00:10:07,320 --> 00:10:15,990
configuration file under this under the

00:10:12,420 --> 00:10:18,210
discovery pay on the discovery top of

00:10:15,990 --> 00:10:22,470
cabana you'll be able to see all the

00:10:18,210 --> 00:10:25,260
flows in real time that are flowing in

00:10:22,470 --> 00:10:27,750
your network and if we select on one of

00:10:25,260 --> 00:10:31,170
them on of the flow you could see more

00:10:27,750 --> 00:10:33,690
details about it here for example we can

00:10:31,170 --> 00:10:36,960
see for each direction so because the

00:10:33,690 --> 00:10:39,810
flow has both two directions right we

00:10:36,960 --> 00:10:42,570
can see for each direction a couple of

00:10:39,810 --> 00:10:46,310
statistics like the number of bytes or

00:10:42,570 --> 00:10:46,310
the number of packets and so on

00:10:48,260 --> 00:10:55,320
so a monitoring solution won't be

00:10:51,030 --> 00:10:58,860
complete without having to gather the

00:10:55,320 --> 00:11:02,010
logs so we have file bit that is a

00:10:58,860 --> 00:11:04,680
simple log folder that sends all the log

00:11:02,010 --> 00:11:07,200
lines to elasticsearch is the successor

00:11:04,680 --> 00:11:09,780
of log stash folder that was written by

00:11:07,200 --> 00:11:16,140
Jordan Cecil the creator of log stash

00:11:09,780 --> 00:11:18,000
and it's it was a kind of popular so it

00:11:16,140 --> 00:11:18,690
there are quite many users that are

00:11:18,000 --> 00:11:24,060
still using

00:11:18,690 --> 00:11:25,950
los - Florida but the the locks - the

00:11:24,060 --> 00:11:28,620
locks - Tim didn't find the time to

00:11:25,950 --> 00:11:32,100
maintain this project so we decided to

00:11:28,620 --> 00:11:34,140
take the the code from Los - order did

00:11:32,100 --> 00:11:37,620
do some improvement and release it at

00:11:34,140 --> 00:11:41,310
5:00 so if you are using loss - folder

00:11:37,620 --> 00:11:43,920
and you want to upgrade to five it it

00:11:41,310 --> 00:11:46,770
might be interesting to you for you to

00:11:43,920 --> 00:11:51,270
look at the migration guide to have in

00:11:46,770 --> 00:11:53,580
our documentation a file which gives the

00:11:51,270 --> 00:11:56,610
same warranties and locks - further

00:11:53,580 --> 00:11:59,850
never loses a lock line and this can be

00:11:56,610 --> 00:12:02,730
done because file bid is reading the log

00:11:59,850 --> 00:12:08,130
files line by line and riparian

00:12:02,730 --> 00:12:10,080
remembers how far is read five reasons

00:12:08,130 --> 00:12:12,870
the localized as they are without

00:12:10,080 --> 00:12:16,440
parsing them so who is doing the parsing

00:12:12,870 --> 00:12:19,710
then so there are actually two options

00:12:16,440 --> 00:12:21,690
one option is to use locks - so in this

00:12:19,710 --> 00:12:25,140
scenario you will see that file bit is

00:12:21,690 --> 00:12:28,020
sending the the locks and parts to log

00:12:25,140 --> 00:12:31,850
stash and locks - is using filters like

00:12:28,020 --> 00:12:35,490
grog geoip mutate - parcel locks and

00:12:31,850 --> 00:12:38,250
then sends the parse locks to

00:12:35,490 --> 00:12:40,860
elasticsearch or to other systems and

00:12:38,250 --> 00:12:43,320
it's like because in unlock - there is

00:12:40,860 --> 00:12:45,650
the filters are quite flexible in the

00:12:43,320 --> 00:12:48,840
sense that you can group them by

00:12:45,650 --> 00:12:54,750
conditioners and you can also create

00:12:48,840 --> 00:12:57,150
your own custom filters in Ruby another

00:12:54,750 --> 00:12:59,790
option is to use the in just not plug-in

00:12:57,150 --> 00:13:01,589
of elasticsearch that is available

00:12:59,790 --> 00:13:05,220
starting with four five zero alpha one

00:13:01,589 --> 00:13:07,200
and in this scenario five bit is sending

00:13:05,220 --> 00:13:09,720
the on part lock so directly to

00:13:07,200 --> 00:13:14,220
elasticsearch and you have to configure

00:13:09,720 --> 00:13:18,720
via REST API processors and filters like

00:13:14,220 --> 00:13:22,380
Joe IP mutate in Grogg similar with the

00:13:18,720 --> 00:13:24,570
ones that you have in in lock - this

00:13:22,380 --> 00:13:27,410
scenario is easier in the sense that are

00:13:24,570 --> 00:13:27,410
less moving parts

00:13:27,960 --> 00:13:31,680
now let's have a look to the

00:13:29,670 --> 00:13:33,660
configuration file of file bit

00:13:31,680 --> 00:13:35,910
so in fact wait you just have to

00:13:33,660 --> 00:13:38,610
configure a list of respecters that

00:13:35,910 --> 00:13:41,279
fetch the data so for each prospector

00:13:38,610 --> 00:13:45,959
you define an input type if it was a

00:13:41,279 --> 00:13:48,390
lock or is if there is thunder in as an

00:13:45,959 --> 00:13:51,450
input and also a list of paths from

00:13:48,390 --> 00:13:53,970
where to to get the locks and in

00:13:51,450 --> 00:13:59,850
addition you have to also specify the

00:13:53,970 --> 00:14:03,060
encoding and manifold is playing after

00:13:59,850 --> 00:14:04,830
you after fight with is starting sending

00:14:03,060 --> 00:14:08,160
the data the lastly search you'll be

00:14:04,830 --> 00:14:10,980
able to see them in Cabana all the lock

00:14:08,160 --> 00:14:14,100
lies in the discovery page and if I

00:14:10,980 --> 00:14:18,089
click on one of them then I can I'm able

00:14:14,100 --> 00:14:20,510
to see more details about they the lock

00:14:18,089 --> 00:14:24,120
lines for example here in the message

00:14:20,510 --> 00:14:29,310
you'll be able to see exactly the lock

00:14:24,120 --> 00:14:34,080
line and of course this is a this is a

00:14:29,310 --> 00:14:36,750
sample lock line and Riaan reality the

00:14:34,080 --> 00:14:39,900
local eyes look more complicated than in

00:14:36,750 --> 00:14:43,529
this example for example in java is

00:14:39,900 --> 00:14:46,830
common to have exceptions on multiple

00:14:43,529 --> 00:14:49,529
lines and this is the default behavior

00:14:46,830 --> 00:14:54,380
won't work so for this we have the

00:14:49,529 --> 00:14:57,660
multi-line functionality that gathers

00:14:54,380 --> 00:15:03,630
multiple localize together into one line

00:14:57,660 --> 00:15:06,770
and you can group them together by using

00:15:03,630 --> 00:15:10,730
a regular expression here you can define

00:15:06,770 --> 00:15:21,450
under the patterns a regular expression

00:15:10,730 --> 00:15:24,540
young and here is an example of a

00:15:21,450 --> 00:15:27,680
multi-line so here you will be able to

00:15:24,540 --> 00:15:30,589
see so if multi-line is configured then

00:15:27,680 --> 00:15:37,350
the lock line will include the whole

00:15:30,589 --> 00:15:40,800
java exception now is more and more

00:15:37,350 --> 00:15:41,520
common for applications to use structure

00:15:40,800 --> 00:15:45,300
law

00:15:41,520 --> 00:15:48,720
what this means is that your application

00:15:45,300 --> 00:15:50,550
logs are written in a JSON format and of

00:15:48,720 --> 00:15:53,300
course there are quite many advantages

00:15:50,550 --> 00:15:56,880
right because you don't have to choose

00:15:53,300 --> 00:15:59,220
what data to include in your log line

00:15:56,880 --> 00:16:01,820
you can just dump the the entire

00:15:59,220 --> 00:16:04,800
structure and that's quite nice and

00:16:01,820 --> 00:16:06,570
another thing is that you don't have to

00:16:04,800 --> 00:16:11,010
parse your logs because they are already

00:16:06,570 --> 00:16:13,770
parsed and so I'd like to show you an

00:16:11,010 --> 00:16:17,880
example here so you will see that the

00:16:13,770 --> 00:16:20,460
log line I mean the the event that you

00:16:17,880 --> 00:16:29,930
see in Cabana also includes the JSON

00:16:20,460 --> 00:16:32,760
object and here you can see it better so

00:16:29,930 --> 00:16:35,010
your application is dumping a lot of

00:16:32,760 --> 00:16:38,160
local log lines and maybe you are not

00:16:35,010 --> 00:16:40,530
interesting to index all of those log

00:16:38,160 --> 00:16:44,790
lines in elastic search so for this we

00:16:40,530 --> 00:16:46,680
have basic filtering so you can drop the

00:16:44,790 --> 00:16:49,080
log lines that you are not interesting

00:16:46,680 --> 00:16:55,800
in for example you can say that I want

00:16:49,080 --> 00:16:58,260
to index only the warnings or errors in

00:16:55,800 --> 00:17:01,530
a lousy search and I'm not interesting

00:16:58,260 --> 00:17:10,800
in in the debug messages and also you

00:17:01,530 --> 00:17:13,650
can specify what files to drop after we

00:17:10,800 --> 00:17:16,610
release the file bit we got quite many

00:17:13,650 --> 00:17:20,490
requests to add to add the functionality

00:17:16,610 --> 00:17:24,390
together a Windows Event log so for this

00:17:20,490 --> 00:17:26,640
we created we locked it so we'll know if

00:17:24,390 --> 00:17:28,980
it has the same similar idea we filed it

00:17:26,640 --> 00:17:32,790
in the sense that sense the Windows

00:17:28,980 --> 00:17:36,210
Event log some parts to elasticsearch so

00:17:32,790 --> 00:17:38,880
you need to have you need to use logs -

00:17:36,210 --> 00:17:41,430
either locks - oranges not to parse them

00:17:38,880 --> 00:17:44,490
it gives the same warranties as file

00:17:41,430 --> 00:17:46,920
with never loses the lock line and the

00:17:44,490 --> 00:17:50,430
configuration file is cuadrado simple

00:17:46,920 --> 00:17:52,900
you just need to specify what event logs

00:17:50,430 --> 00:17:57,220
you are interested in

00:17:52,900 --> 00:18:00,460
and here you can see an example of a

00:17:57,220 --> 00:18:02,650
dashboard that's for example here you

00:18:00,460 --> 00:18:06,610
can see the evolution ins of event logs

00:18:02,650 --> 00:18:10,530
and overtime and you can also see how

00:18:06,610 --> 00:18:17,940
many information or how many warning

00:18:10,530 --> 00:18:21,850
levels or the world and so on another

00:18:17,940 --> 00:18:24,640
another a simple OB that we created but

00:18:21,850 --> 00:18:28,570
rather powerful is top it top it

00:18:24,640 --> 00:18:31,360
collects system statistics metrics so

00:18:28,570 --> 00:18:34,300
the idea behind it is very simple it's

00:18:31,360 --> 00:18:36,210
like the unique stop common but instead

00:18:34,300 --> 00:18:38,980
of printing out on the screen all the

00:18:36,210 --> 00:18:42,940
statistics is send them periodically to

00:18:38,980 --> 00:18:46,420
elasticsearch and it's different in the

00:18:42,940 --> 00:18:49,300
sense that it works also on Windows so

00:18:46,420 --> 00:18:52,780
it collects system-wide statistics like

00:18:49,300 --> 00:18:55,540
system load total CPU usage swap and

00:18:52,780 --> 00:19:00,580
memory usage per process statistics like

00:18:55,540 --> 00:19:02,590
the state of the process the name the

00:19:00,580 --> 00:19:03,100
command line that was used to start the

00:19:02,590 --> 00:19:07,320
process

00:19:03,100 --> 00:19:11,170
the PID is the CPU and the memory usage

00:19:07,320 --> 00:19:14,740
also gathers disk usage in statistics

00:19:11,170 --> 00:19:20,200
like the available is used free space

00:19:14,740 --> 00:19:22,450
and mounting points in the configuration

00:19:20,200 --> 00:19:25,720
file of Tobit you just just have to

00:19:22,450 --> 00:19:27,880
specify how often to send the system

00:19:25,720 --> 00:19:30,700
statistics and one process what

00:19:27,880 --> 00:19:33,100
processes you want to earth monitor and

00:19:30,700 --> 00:19:38,200
in addition you have to specify what

00:19:33,100 --> 00:19:40,390
kind of data you are interested in here

00:19:38,200 --> 00:19:45,640
is an example of a dashboard that called

00:19:40,390 --> 00:19:48,580
that that shows you the data that is

00:19:45,640 --> 00:19:52,960
collected from top it and for example if

00:19:48,580 --> 00:19:54,370
you if you configure different topics to

00:19:52,960 --> 00:19:56,800
send the take pass the same

00:19:54,370 --> 00:20:00,130
elasticsearch then you will be able to

00:19:56,800 --> 00:20:02,710
see in one place statistics from

00:20:00,130 --> 00:20:06,160
different servers so here for example I

00:20:02,710 --> 00:20:06,610
have three servers and I can see also a

00:20:06,160 --> 00:20:11,380
few

00:20:06,610 --> 00:20:15,880
tell us about each of them and if you

00:20:11,380 --> 00:20:19,270
can also see the system load statistics

00:20:15,880 --> 00:20:22,660
for example here are two servers and we

00:20:19,270 --> 00:20:28,929
can see we can compare one of each other

00:20:22,660 --> 00:20:33,910
so memory usage statistics oh yeah

00:20:28,929 --> 00:20:38,170
memory usage graph and CPU usage disk

00:20:33,910 --> 00:20:40,990
usage and this one is my favorite

00:20:38,170 --> 00:20:44,320
because he looks a similar with what the

00:20:40,990 --> 00:20:49,240
top command prints out on the screen so

00:20:44,320 --> 00:20:53,020
you can see what process is consumed

00:20:49,240 --> 00:20:55,809
most of the CPU usage Cosmos of the cpu

00:20:53,020 --> 00:20:58,780
so the first one will be the one that

00:20:55,809 --> 00:21:04,929
consumer most of the CPU so in this case

00:20:58,780 --> 00:21:09,100
is 300% and also you can see you can

00:21:04,929 --> 00:21:11,950
build graphs based on the data that you

00:21:09,100 --> 00:21:22,679
collected per processes purposes then is

00:21:11,950 --> 00:21:25,570
memory usage CPU usage and so on so we

00:21:22,679 --> 00:21:28,270
we collect all this information about

00:21:25,570 --> 00:21:31,570
system statistics but do you we wanted

00:21:28,270 --> 00:21:34,570
to extend more the type of metrics that

00:21:31,570 --> 00:21:37,510
we collect so we decided to start

00:21:34,570 --> 00:21:41,250
working on a metric bit that collects a

00:21:37,510 --> 00:21:44,080
different type of metrics from different

00:21:41,250 --> 00:21:47,710
from different earth systems so how

00:21:44,080 --> 00:21:50,919
metric bit works is by it interrogated

00:21:47,710 --> 00:21:53,470
periodically external systems get some

00:21:50,919 --> 00:21:56,590
metrics push correlate them together

00:21:53,470 --> 00:22:00,730
into documents and then pushes them to

00:21:56,590 --> 00:22:04,330
elasticsearch the metric bit is

00:22:00,730 --> 00:22:07,870
pluggable in the sense that you can add

00:22:04,330 --> 00:22:10,780
modules to it so it created so far an

00:22:07,870 --> 00:22:13,929
Apache module a magical module already

00:22:10,780 --> 00:22:16,990
is module and a system module and here

00:22:13,929 --> 00:22:20,530
you are we are happy to receive

00:22:16,990 --> 00:22:25,240
contributions from you with more mod

00:22:20,530 --> 00:22:26,980
for magic pigs format repeats right also

00:22:25,240 --> 00:22:29,230
it's important to note that metric

00:22:26,980 --> 00:22:32,800
petite also can be also used as a

00:22:29,230 --> 00:22:36,750
library so what this means that you can

00:22:32,800 --> 00:22:41,680
create your own beat that make use of

00:22:36,750 --> 00:22:44,830
metric bit as an infrastructure and for

00:22:41,680 --> 00:22:50,140
for this we created an example bit that

00:22:44,830 --> 00:22:53,260
has one module and uses metric bit as a

00:22:50,140 --> 00:22:56,560
library and in addition can also make

00:22:53,260 --> 00:23:02,110
use of the other built in modules from

00:22:56,560 --> 00:23:03,670
metric bit yes Alice is the difference

00:23:02,110 --> 00:23:06,730
between a metric bit mode you will

00:23:03,670 --> 00:23:10,780
understand a little bit so the metric

00:23:06,730 --> 00:23:13,480
bit modules are available under elastic

00:23:10,780 --> 00:23:15,580
bit github repository or while the

00:23:13,480 --> 00:23:19,660
standalone bits are available in their

00:23:15,580 --> 00:23:22,000
own repository the magic bit modules are

00:23:19,660 --> 00:23:24,340
officially supported by elastic while

00:23:22,000 --> 00:23:26,740
the standalone bits are supported by the

00:23:24,340 --> 00:23:29,020
community and the suspend alone which

00:23:26,740 --> 00:23:35,380
they have them maintained errs their own

00:23:29,020 --> 00:23:37,420
users and they are released cycle so I

00:23:35,380 --> 00:23:40,750
show you how many bits that we created

00:23:37,420 --> 00:23:43,420
together all kinds of operational data

00:23:40,750 --> 00:23:45,910
but what we did actually it was more

00:23:43,420 --> 00:23:48,370
than that we created a platform to make

00:23:45,910 --> 00:23:54,730
it easier to build custom bits on top of

00:23:48,370 --> 00:24:01,030
it so it created the lipid and on top of

00:23:54,730 --> 00:24:03,850
it you can easily build new bits just a

00:24:01,030 --> 00:24:07,600
few words a bit about this library lipid

00:24:03,850 --> 00:24:09,910
is written in go in go long and provides

00:24:07,600 --> 00:24:12,370
common functionality for all the bits

00:24:09,910 --> 00:24:14,190
for like for example it provides common

00:24:12,370 --> 00:24:17,640
functionality for handling the

00:24:14,190 --> 00:24:20,770
configuration files for handling

00:24:17,640 --> 00:24:23,650
command-line arguments for longing and

00:24:20,770 --> 00:24:30,700
also make sure the reliable sends out

00:24:23,650 --> 00:24:33,550
the data so far we show you that all the

00:24:30,700 --> 00:24:34,210
bits are sending data to log stash or to

00:24:33,550 --> 00:24:36,940
elastics

00:24:34,210 --> 00:24:43,059
but it can also send data to Kafka

00:24:36,940 --> 00:24:46,749
already based on the lipid library many

00:24:43,059 --> 00:24:48,639
community beats were created so the

00:24:46,749 --> 00:24:50,950
rural community beats so there are

00:24:48,639 --> 00:24:53,320
standalone projects they are written in

00:24:50,950 --> 00:24:56,379
golang and because they are used they

00:24:53,320 --> 00:24:58,929
are using this lipid then they can only

00:24:56,379 --> 00:25:01,840
concentrate on collecting the data and

00:24:58,929 --> 00:25:06,399
let little bit worried about how to send

00:25:01,840 --> 00:25:11,740
the data and they are trying usually to

00:25:06,399 --> 00:25:13,749
solve a specific use case yeah less is

00:25:11,740 --> 00:25:16,480
the difference between an official a bit

00:25:13,749 --> 00:25:19,450
and a community beat so now official

00:25:16,480 --> 00:25:22,149
beat is the one that lies in the elastic

00:25:19,450 --> 00:25:25,809
bit github repository it's officially

00:25:22,149 --> 00:25:28,659
supported while the community beats are

00:25:25,809 --> 00:25:31,059
in their own repository and have their

00:25:28,659 --> 00:25:38,409
own maintainer users and their release

00:25:31,059 --> 00:25:40,840
cycle so far mostly almost 20 community

00:25:38,409 --> 00:25:42,519
beats were created and I'm really

00:25:40,840 --> 00:25:45,480
surprised to see that because we

00:25:42,519 --> 00:25:48,369
released little bit a few months ago and

00:25:45,480 --> 00:25:50,919
actually the slide is kind of old I

00:25:48,369 --> 00:25:53,529
created it like two days ago and there

00:25:50,919 --> 00:25:54,909
were other community beats that were

00:25:53,529 --> 00:25:56,740
created in the mean time and

00:25:54,909 --> 00:25:59,049
unfortunately I couldn't fit those on

00:25:56,740 --> 00:26:03,220
the slide so I have to make two flies

00:25:59,049 --> 00:26:06,190
next time so let me highlight a few of

00:26:03,220 --> 00:26:09,460
the beats that the community created one

00:26:06,190 --> 00:26:12,159
of them is ping bit what pain league

00:26:09,460 --> 00:26:15,789
does is sends periodically ICMP pings

00:26:12,159 --> 00:26:20,440
and - a list of hosts and then records

00:26:15,789 --> 00:26:22,990
the round-trip time and he can also send

00:26:20,440 --> 00:26:32,320
the pings over UDP he can also resolve

00:26:22,990 --> 00:26:34,450
DNS and so on another interesting bidder

00:26:32,320 --> 00:26:37,299
that I would like to show you it's exact

00:26:34,450 --> 00:26:40,240
bit what exactly does is run it's

00:26:37,299 --> 00:26:43,450
running a periodical air command and

00:26:40,240 --> 00:26:46,480
then sends the standard out and the

00:26:43,450 --> 00:26:50,050
standard error - elasticsearch

00:26:46,480 --> 00:26:52,090
and I think this this bit is it's it's

00:26:50,050 --> 00:26:54,670
kind of valuable in the sense that

00:26:52,090 --> 00:26:57,070
imagine you can write your application

00:26:54,670 --> 00:26:59,230
in in the program in language that you

00:26:57,070 --> 00:27:02,440
want for example perverse or whatever

00:26:59,230 --> 00:27:08,380
and then you can index data to

00:27:02,440 --> 00:27:10,300
elasticsearch another interesting bill

00:27:08,380 --> 00:27:14,560
that I would like to show it's docker

00:27:10,300 --> 00:27:19,210
bit dr. B that is used for docker

00:27:14,560 --> 00:27:22,600
monitoring it's using the docker API to

00:27:19,210 --> 00:27:26,800
export per container statistics like the

00:27:22,600 --> 00:27:33,940
CPU memory disk network i/o access locks

00:27:26,800 --> 00:27:37,260
and so on and another interesting bit is

00:27:33,940 --> 00:27:41,710
not just ejected that execute

00:27:37,260 --> 00:27:43,900
periodically not just plugins and sends

00:27:41,710 --> 00:27:45,850
alerts warning or critical to

00:27:43,900 --> 00:27:48,060
elasticsearch and also send some

00:27:45,850 --> 00:27:54,330
performance data to elasticsearch and

00:27:48,060 --> 00:27:54,330
here is how the events are generated so

00:27:56,040 --> 00:28:05,890
we are always encouraging our users to

00:28:00,040 --> 00:28:07,690
build new bits to solve a use case in in

00:28:05,890 --> 00:28:12,700
the area where they are working on

00:28:07,690 --> 00:28:15,970
because I believe those people can do a

00:28:12,700 --> 00:28:18,220
better job building the beat than us

00:28:15,970 --> 00:28:24,070
because they are the expert in that area

00:28:18,220 --> 00:28:28,450
so we are always happy to help you with

00:28:24,070 --> 00:28:31,810
creating a new bit and for that we we

00:28:28,450 --> 00:28:34,150
made it even easier to build a bit so we

00:28:31,810 --> 00:28:36,580
created this bit generator that

00:28:34,150 --> 00:28:39,700
generates the boiler code plate for you

00:28:36,580 --> 00:28:45,220
it's using cookie cutter and you just

00:28:39,700 --> 00:28:48,370
need to give path of you o arguments

00:28:45,220 --> 00:28:51,820
like the name of the project and the

00:28:48,370 --> 00:28:57,790
github account and your full name and

00:28:51,820 --> 00:29:00,470
then yeah the pit will be there more in

00:28:57,790 --> 00:29:04,620
addition to this we are also

00:29:00,470 --> 00:29:08,130
offering away or we are helping you to

00:29:04,620 --> 00:29:10,230
build the packages for your own bit so

00:29:08,130 --> 00:29:12,870
for this we are offering the same tools

00:29:10,230 --> 00:29:16,920
that we are using to build as official

00:29:12,870 --> 00:29:21,540
bit and they can also be executed in in

00:29:16,920 --> 00:29:24,660
from Travis so we collected all this

00:29:21,540 --> 00:29:27,300
data the bits we collect this a

00:29:24,660 --> 00:29:31,230
different data right matrix locks

00:29:27,300 --> 00:29:34,560
transaction system statistics flows and

00:29:31,230 --> 00:29:37,860
so on and our goal is to visualize the

00:29:34,560 --> 00:29:41,340
debt this data with a single UI will

00:29:37,860 --> 00:29:43,980
keep a nominee so for example if I want

00:29:41,340 --> 00:29:47,850
to monitor my sequel then I'll be able

00:29:43,980 --> 00:29:51,180
to get my secret statistics from the my

00:29:47,850 --> 00:29:54,690
sequel module of my tribute locks like

00:29:51,180 --> 00:29:58,470
slow queries from file bit and also the

00:29:54,690 --> 00:30:03,510
queries from packet bit for example if I

00:29:58,470 --> 00:30:06,690
want to monitor the web server then and

00:30:03,510 --> 00:30:10,560
imagine where we contain some icicle in

00:30:06,690 --> 00:30:12,720
Apache it's just an example right you I

00:30:10,560 --> 00:30:14,820
will be able to get the statistics about

00:30:12,720 --> 00:30:17,430
my sickle and the pass from metric bit

00:30:14,820 --> 00:30:19,740
I'll be able to get the slow queries and

00:30:17,430 --> 00:30:21,660
Apache logs from file bit and from

00:30:19,740 --> 00:30:24,120
package with I'll be able to get the

00:30:21,660 --> 00:30:26,640
transactions the HTTP transaction and

00:30:24,120 --> 00:30:31,250
the Mexico transactions and I'll be able

00:30:26,640 --> 00:30:33,690
to visualize them with a single view

00:30:31,250 --> 00:30:38,310
that was everything thank you for

00:30:33,690 --> 00:30:39,510
listening if you want to hear more about

00:30:38,310 --> 00:30:42,300
log stash

00:30:39,510 --> 00:30:43,500
don't miss the ingest locks with style

00:30:42,300 --> 00:30:47,220
by parry

00:30:43,500 --> 00:30:49,320
tomorrow in the same room at 12 now I

00:30:47,220 --> 00:30:54,410
think we have a few minutes for

00:30:49,320 --> 00:30:54,410
questions thank you

00:31:00,179 --> 00:31:12,010
so any questions hello

00:31:09,429 --> 00:31:16,059
you're now adding a lot of possibilities

00:31:12,010 --> 00:31:19,020
to add performance more data into qivana

00:31:16,059 --> 00:31:21,520
are you also planning to extend

00:31:19,020 --> 00:31:23,500
possibilities to inspect these data like

00:31:21,520 --> 00:31:28,240
you have with crow fauna or graphite

00:31:23,500 --> 00:31:30,820
that you can yeah so the question is if

00:31:28,240 --> 00:31:32,919
we write oh I was I would say Labiche or

00:31:30,820 --> 00:31:37,000
to this the question make it shorter so

00:31:32,919 --> 00:31:40,210
if we are planning to extend Cubana to

00:31:37,000 --> 00:31:42,520
visualize easier the metrics right this

00:31:40,210 --> 00:31:48,150
was a question yes we we are planning

00:31:42,520 --> 00:31:53,220
that but we don't have any official

00:31:48,150 --> 00:31:53,220
story around it but yes definitely

00:31:54,690 --> 00:32:04,140
any more questions so one question from

00:32:01,240 --> 00:32:07,540
Isaac Kent timeline be used as

00:32:04,140 --> 00:32:09,669
visualization part for those metrics yes

00:32:07,540 --> 00:32:14,620
sir this was an option that we're

00:32:09,669 --> 00:32:17,320
thinking about and we realized that

00:32:14,620 --> 00:32:19,720
timeline it's a bit more difficult at

00:32:17,320 --> 00:32:22,120
the moment as it is right now to be used

00:32:19,720 --> 00:32:28,360
for visualizing the matrix so we are

00:32:22,120 --> 00:32:34,809
planning to do another another

00:32:28,360 --> 00:32:38,710
visualization app in cabana to be meant

00:32:34,809 --> 00:32:43,360
for the matrix thank you I hope I

00:32:38,710 --> 00:32:48,120
answered your question any more

00:32:43,360 --> 00:32:48,120
questions yeah

00:32:51,130 --> 00:32:56,890
so the question is what about impact on

00:32:54,669 --> 00:32:59,380
performance on the machines for example

00:32:56,890 --> 00:33:02,559
if I have multiple beats for locked

00:32:59,380 --> 00:33:05,860
files and top beats and so and when I

00:33:02,559 --> 00:33:09,370
have a really short interval for example

00:33:05,860 --> 00:33:11,919
one second or whatever or I have a high

00:33:09,370 --> 00:33:14,770
load on the system or big lock file

00:33:11,919 --> 00:33:19,179
straight files that are written so what

00:33:14,770 --> 00:33:23,919
is the impact if I use this beats on a

00:33:19,179 --> 00:33:27,220
machine this is is it high or is it low

00:33:23,919 --> 00:33:31,090
or other metrics grabbed direct from

00:33:27,220 --> 00:33:32,980
from kernel so here there are two

00:33:31,090 --> 00:33:38,260
questions so one is related to the locks

00:33:32,980 --> 00:33:42,700
right so the you are referring about

00:33:38,260 --> 00:33:48,909
five B right especially or in general in

00:33:42,700 --> 00:33:50,559
general so there is so this basically

00:33:48,909 --> 00:33:52,780
how we call it they are lightweight

00:33:50,559 --> 00:33:58,419
shippers so this means they don't

00:33:52,780 --> 00:33:59,830
consume a lot of CPU and memory because

00:33:58,419 --> 00:34:01,990
they don't want they don't interfere

00:33:59,830 --> 00:34:08,649
with your application that are running

00:34:01,990 --> 00:34:12,190
on yours on your machine so for example

00:34:08,649 --> 00:34:14,500
if your application is writing a lot of

00:34:12,190 --> 00:34:16,720
log files and you are not interesting

00:34:14,500 --> 00:34:21,340
about all the log files then you can

00:34:16,720 --> 00:34:23,169
filter to get only the log files that

00:34:21,340 --> 00:34:26,320
you are interested in and then of course

00:34:23,169 --> 00:34:27,669
you are reducing the amount of data that

00:34:26,320 --> 00:34:34,750
you are in the scene in the

00:34:27,669 --> 00:34:37,720
elasticsearch so I will say it depends

00:34:34,750 --> 00:34:42,210
it depends how many locks you have but

00:34:37,720 --> 00:34:47,800
in general we didn't have problems with

00:34:42,210 --> 00:34:57,030
with with the beach or consuming more

00:34:47,800 --> 00:35:01,619
CPU and so so you are always scan final

00:34:57,030 --> 00:35:01,619
balance right I mean it's

00:35:03,710 --> 00:35:08,210
yeah just a little short questions in

00:35:06,350 --> 00:35:12,020
case of for example of a network failure

00:35:08,210 --> 00:35:14,650
how would the Beats clients or the

00:35:12,020 --> 00:35:17,300
agents react what they queue all the

00:35:14,650 --> 00:35:20,720
collected information statistics and so

00:35:17,300 --> 00:35:22,460
on and then as soon as the connection is

00:35:20,720 --> 00:35:25,520
the network connection is established

00:35:22,460 --> 00:35:29,570
again we'll push the data or will be the

00:35:25,520 --> 00:35:32,030
year collecting and fairness yeah so it

00:35:29,570 --> 00:35:34,850
depends on the beat for example file bit

00:35:32,030 --> 00:35:40,400
has this never loses a log line so this

00:35:34,850 --> 00:35:44,240
means that the network keep busy then it

00:35:40,400 --> 00:35:48,020
stops it doesn't do anything Inc in in

00:35:44,240 --> 00:35:58,850
case of any tries later in case of pakka

00:35:48,020 --> 00:36:01,160
bit it drops it drops the packet do you

00:35:58,850 --> 00:36:04,100
have any more questions so it's to

00:36:01,160 --> 00:36:05,390
enhance a bit your question so the way

00:36:04,100 --> 00:36:10,700
how it works

00:36:05,390 --> 00:36:15,320
file bit so for each lock line that is

00:36:10,700 --> 00:36:15,830
sent out its wait for an ack from from

00:36:15,320 --> 00:36:19,850
logstash

00:36:15,830 --> 00:36:23,740
and in case the ack was not received and

00:36:19,850 --> 00:36:23,740
is trying to retransmit the message

00:36:28,330 --> 00:36:35,030
regarding packet beat and if it runs on

00:36:32,810 --> 00:36:37,700
a central component let's say firewall

00:36:35,030 --> 00:36:41,120
you have very huge amounts of traffic

00:36:37,700 --> 00:36:44,120
and how does the packet beat impact on

00:36:41,120 --> 00:36:46,820
the network traffic in itself so you

00:36:44,120 --> 00:36:54,620
don't see your packet the packet in your

00:36:46,820 --> 00:37:02,900
packet bit packet and so on yeah that's

00:36:54,620 --> 00:37:04,760
an interesting question I mean yeah I

00:37:02,900 --> 00:37:09,140
don't know I don't know how what to

00:37:04,760 --> 00:37:11,330
answer to this one actually I mean if

00:37:09,140 --> 00:37:14,470
you have a firewall you cannot see the

00:37:11,330 --> 00:37:14,470
network traffic so

00:37:23,260 --> 00:37:29,890
maybe I didn't understand them maybe I

00:37:26,590 --> 00:37:33,700
wasn't clear enough and if I was to see

00:37:29,890 --> 00:37:38,620
a lot of network traffic and it's a

00:37:33,700 --> 00:37:39,310
perfect place to see what's really on

00:37:38,620 --> 00:37:44,560
your network

00:37:39,310 --> 00:37:49,930
okay so packet beads will create a huge

00:37:44,560 --> 00:37:52,690
amount of packets itself so what's the

00:37:49,930 --> 00:37:55,540
performance of packet B let's say if you

00:37:52,690 --> 00:37:58,540
have a gig of the traffic coming through

00:37:55,540 --> 00:38:01,660
your firewall how much traffic will pick

00:37:58,540 --> 00:38:05,500
it itself generate on this you mean how

00:38:01,660 --> 00:38:07,720
much so you mean by how much through

00:38:05,500 --> 00:38:11,620
traffic packet they generate you mean by

00:38:07,720 --> 00:38:16,900
how much data take it with exports the

00:38:11,620 --> 00:38:20,140
elasticsearch right yes I mean yeah it

00:38:16,900 --> 00:38:22,480
depends on the amount of traffic that

00:38:20,140 --> 00:38:26,310
you have but as I said earlier you can

00:38:22,480 --> 00:38:29,380
if you if you if there is a problem in

00:38:26,310 --> 00:38:31,480
in the amount of data that packet with

00:38:29,380 --> 00:38:36,040
these ports you can also use the

00:38:31,480 --> 00:38:41,320
filtering to reduce the number of data

00:38:36,040 --> 00:38:43,480
the packet with these ports but so we

00:38:41,320 --> 00:38:46,720
did some performance tests while a while

00:38:43,480 --> 00:38:48,700
ago and service parts were not

00:38:46,720 --> 00:38:52,510
officially of course and the results are

00:38:48,700 --> 00:38:54,940
like packet bit was consuming 5% of the

00:38:52,510 --> 00:39:04,900
whole box so just to give you an idea

00:38:54,940 --> 00:39:08,970
it's very lightweight cheaper okay more

00:39:04,900 --> 00:39:08,970
questions yeah

00:39:13,000 --> 00:39:20,060
just in addition to the first question

00:39:15,260 --> 00:39:22,340
here is it like possible to to add to

00:39:20,060 --> 00:39:24,410
configure an interval in which if you

00:39:22,340 --> 00:39:27,140
don't need real-time monitoring in which

00:39:24,410 --> 00:39:29,330
the data is being sent to your unlocks

00:39:27,140 --> 00:39:31,820
their server so that you don't produce

00:39:29,330 --> 00:39:34,160
packet traffic all the time but you can

00:39:31,820 --> 00:39:35,990
like summarize them on on your client

00:39:34,160 --> 00:39:39,340
and then just send it every five minutes

00:39:35,990 --> 00:39:39,340
for example if you don't need real-time

00:39:39,849 --> 00:39:45,890
yeah I understand your question so you

00:39:43,160 --> 00:39:49,520
can for some piece for example for Tobit

00:39:45,890 --> 00:39:53,000
you can configure the period of time how

00:39:49,520 --> 00:39:59,200
often to send the data for the others no

00:39:53,000 --> 00:40:04,040
for packet bit and file bit no because

00:39:59,200 --> 00:40:16,880
they are meant to to be for seeing yours

00:40:04,040 --> 00:40:19,460
a traffic in real time so for for

00:40:16,880 --> 00:40:26,300
example take a bit it sends you sent out

00:40:19,460 --> 00:40:28,640
the transaction so I don't imagine how

00:40:26,300 --> 00:40:30,500
you will combine them together for

00:40:28,640 --> 00:40:33,070
example what we are using right right

00:40:30,500 --> 00:40:36,080
now we so in a last research you can

00:40:33,070 --> 00:40:38,570
specify use a bulk api of velocity

00:40:36,080 --> 00:40:42,530
search and insert multiple documents in

00:40:38,570 --> 00:40:46,910
batches maybe this is something that you

00:40:42,530 --> 00:40:49,910
are asking about but I don't know in

00:40:46,910 --> 00:40:50,390
packages for example I don't maybe I

00:40:49,910 --> 00:40:52,310
don't

00:40:50,390 --> 00:40:54,670
see right now I don't know how you will

00:40:52,310 --> 00:41:00,020
be able to combine multiple transactions

00:40:54,670 --> 00:41:02,660
into one transaction and send yeah I

00:41:00,020 --> 00:41:04,820
mean they depends on the pit right for

00:41:02,660 --> 00:41:06,680
some bits for example for matrix this is

00:41:04,820 --> 00:41:09,230
something that you can do and also for

00:41:06,680 --> 00:41:12,619
two bit for so for matrix in general you

00:41:09,230 --> 00:41:15,500
can combine let's say you have five

00:41:12,619 --> 00:41:18,170
minutes of matrix and you can combine

00:41:15,500 --> 00:41:20,810
them together and send them once every

00:41:18,170 --> 00:41:25,730
five minutes this again you can do but

00:41:20,810 --> 00:41:38,090
yeah so for the depends on the so yeah I

00:41:25,730 --> 00:41:43,100
hope I answer your question also just a

00:41:38,090 --> 00:41:45,980
small question has Beats or based on

00:41:43,100 --> 00:41:47,870
that elasticsearch then an option for

00:41:45,980 --> 00:41:49,700
all those metrics data to have some kind

00:41:47,870 --> 00:41:53,390
of retention like in graphite that you

00:41:49,700 --> 00:41:56,330
can just I would say not have all the

00:41:53,390 --> 00:42:03,740
real-time data in the dump but then

00:41:56,330 --> 00:42:05,990
after some time some data author oh yeah

00:42:03,740 --> 00:42:07,880
we don't have it for the moment but I

00:42:05,990 --> 00:42:23,690
think it's a really great feature thank

00:42:07,880 --> 00:42:28,520
you how how does file pete integrate

00:42:23,690 --> 00:42:34,430
with log file rotation yeah so file bit

00:42:28,520 --> 00:42:36,590
is doing look for rotation I I don't

00:42:34,430 --> 00:42:39,260
know exactly what you mean by that so

00:42:36,590 --> 00:42:41,060
he's doing no file rotation you mean

00:42:39,260 --> 00:42:43,340
that the so we had some problems

00:42:41,060 --> 00:42:46,400
regarding this log file rotation maybe

00:42:43,340 --> 00:42:49,520
this is maybe you can enhance it well

00:42:46,400 --> 00:42:51,740
typically I don't want a single log file

00:42:49,520 --> 00:42:53,960
to contain all the history I would

00:42:51,740 --> 00:42:55,940
rotate it say once a day or once a week

00:42:53,960 --> 00:42:59,720
or once a month it depends on the volume

00:42:55,940 --> 00:43:03,590
and what if file beat is watching this

00:42:59,720 --> 00:43:05,660
log file how does it switch on to the

00:43:03,590 --> 00:43:07,970
next segment or how does it handle the

00:43:05,660 --> 00:43:10,460
route how does it cope with the rotation

00:43:07,970 --> 00:43:13,700
so that it will read the next log file

00:43:10,460 --> 00:43:17,360
if I switch the chart if the file was

00:43:13,700 --> 00:43:19,340
just switched and rotated I'm not 100

00:43:17,360 --> 00:43:22,280
cents sure about this because I didn't

00:43:19,340 --> 00:43:24,590
call myself this part but we are

00:43:22,280 --> 00:43:27,770
supporting from what I know file log

00:43:24,590 --> 00:43:31,610
file rotation I know that it was a

00:43:27,770 --> 00:43:33,150
difficult challenging feature to

00:43:31,610 --> 00:43:35,850
implement

00:43:33,150 --> 00:43:39,570
so unfortunate I cannot give you more

00:43:35,850 --> 00:43:42,930
details about that but just to be clear

00:43:39,570 --> 00:43:45,300
so file bit is not doing any change on

00:43:42,930 --> 00:43:59,160
the box file bit is just reading the

00:43:45,300 --> 00:44:02,100
yeah log lines is there any chance we

00:43:59,160 --> 00:44:06,570
see a puppet module for it like there is

00:44:02,100 --> 00:44:09,000
already a pop an official one or a

00:44:06,570 --> 00:44:15,060
community-based one there is a community

00:44:09,000 --> 00:44:17,220
one yes God is there any plan to go the

00:44:15,060 --> 00:44:22,040
next level like for elasticsearch or

00:44:17,220 --> 00:44:24,540
locks - the puppet modules yes though

00:44:22,040 --> 00:44:28,680
actually we are planning that for a long

00:44:24,540 --> 00:44:31,290
time but unfortunately we didn't we

00:44:28,680 --> 00:44:34,020
didn't manage to to complete the task

00:44:31,290 --> 00:44:38,400
but we are planning that I don't know

00:44:34,020 --> 00:44:40,590
exactly when it will be ready yeah but

00:44:38,400 --> 00:44:46,830
there is a community one already you

00:44:40,590 --> 00:44:51,210
know pattern module ok so one one last

00:44:46,830 --> 00:44:53,460
one anyone no more questions

00:44:51,210 --> 00:44:55,080
okay Monica thank you very much for a

00:44:53,460 --> 00:44:57,200
great time thank you thanks for being

00:44:55,080 --> 00:44:57,200

YouTube URL: https://www.youtube.com/watch?v=pBG6ntutsoA


