Title: OSDC 2016 - Mesos and the Architecture of the New Datacenter by Jörg Schad
Publication date: 2016-05-03
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	Apache Mesos has the ability to run on every private and cloud instance, anywhere. In this talk, Jörg Schad (Software Developer at Mesosphere) will explain the momentum behind the “single computer” abstraction that has put Mesos at the center of one of the most exciting architecture shifts in recent information technology history. He will explain how Mesos is enabling application developers and devops to redefine their responsibilities and shorten the amount of time it takes to write and ship production code. Jörg will outline how Mesos is empowering the new class of “datacenter developers” to program directly against datacenter resources, and draw correlations to how the Linux revolutionized the server industry.
Captions: 
	00:00:12,330 --> 00:00:24,640
okay welcome back next is your chart

00:00:16,030 --> 00:00:27,490
with measles enjoy it so now you should

00:00:24,640 --> 00:00:31,779
be able to hear me yeah welcome to my

00:00:27,490 --> 00:00:35,620
talk about measles and D cos or open D

00:00:31,779 --> 00:00:39,850
cos maybe just a quick poll how many of

00:00:35,620 --> 00:00:45,190
you know mrs. how many have used it so

00:00:39,850 --> 00:00:46,960
far pretty cool I going to stay I gonna

00:00:45,190 --> 00:00:50,320
give like a high-level introduction

00:00:46,960 --> 00:00:52,570
chris is like just a sub set of the

00:00:50,320 --> 00:00:54,760
entire audience so I hope you don't feel

00:00:52,570 --> 00:00:55,840
bored at that part later on it's

00:00:54,760 --> 00:00:57,670
probably also going to be more

00:00:55,840 --> 00:01:00,100
interesting for you when it goes to like

00:00:57,670 --> 00:01:04,540
the surrounding environment around me

00:01:00,100 --> 00:01:07,659
sirs with D cos maybe just let me

00:01:04,540 --> 00:01:09,630
introduce myself my name is York I work

00:01:07,659 --> 00:01:13,030
at mesosphere and I'm basically a

00:01:09,630 --> 00:01:15,659
committer at sea open source project

00:01:13,030 --> 00:01:19,719
missiles at so at the core of D cos

00:01:15,659 --> 00:01:21,249
before that I worked on databases quite

00:01:19,719 --> 00:01:23,049
a lot which I really like this

00:01:21,249 --> 00:01:25,929
conference because it basically combines

00:01:23,049 --> 00:01:27,909
what I'm currently doing like cloud

00:01:25,929 --> 00:01:32,950
infrastructure architecture and

00:01:27,909 --> 00:01:36,880
databases yeah and let's start by

00:01:32,950 --> 00:01:40,179
missiles is a good idea so old data

00:01:36,880 --> 00:01:42,009
centers and what we actually see also

00:01:40,179 --> 00:01:45,639
still at a lot of customer sites is

00:01:42,009 --> 00:01:48,009
aesthetic partitioning of workloads and

00:01:45,639 --> 00:01:49,899
therefore cluster nodes so each of those

00:01:48,009 --> 00:01:51,880
boxes it's not necessarily like a

00:01:49,899 --> 00:01:54,249
physical box it can be like a virtual

00:01:51,880 --> 00:01:56,609
machine or something but what we often

00:01:54,249 --> 00:01:58,959
see when we also go to customers

00:01:56,609 --> 00:02:00,700
implementing measures we see that they

00:01:58,959 --> 00:02:02,590
basically have aesthetically partition

00:02:00,700 --> 00:02:05,259
cluster some part is used for flings

00:02:02,590 --> 00:02:07,149
some part is used for Cassandra some

00:02:05,259 --> 00:02:09,369
percent for the user facing rails

00:02:07,149 --> 00:02:12,060
applications some for data analytics

00:02:09,369 --> 00:02:15,190
with spark and then some caching and

00:02:12,060 --> 00:02:17,770
that actually causes some issues so for

00:02:15,190 --> 00:02:20,470
example when we have notes failing all

00:02:17,770 --> 00:02:23,319
of the sudden there's a wreck failure in

00:02:20,470 --> 00:02:24,560
the rails part of my cluster the rails

00:02:23,319 --> 00:02:26,810
petition

00:02:24,560 --> 00:02:28,849
and now as an operator I would have to

00:02:26,810 --> 00:02:32,330
go and manually move machines over

00:02:28,849 --> 00:02:35,810
deploying rails which is one thing you I

00:02:32,330 --> 00:02:38,630
at least I don't want to do a second

00:02:35,810 --> 00:02:42,260
problem with aesthetic partitioning is

00:02:38,630 --> 00:02:44,150
the efficiency so with a lot of

00:02:42,260 --> 00:02:46,640
applications there's a lot of churn

00:02:44,150 --> 00:02:50,300
through out today so for example I want

00:02:46,640 --> 00:02:52,489
to run data analytics at night my user

00:02:50,300 --> 00:02:54,709
facing applications are more active over

00:02:52,489 --> 00:02:56,750
the day and by having the static

00:02:54,709 --> 00:02:59,090
partitioning I'm actually wasting a lot

00:02:56,750 --> 00:03:01,940
of resources Chris I basically I always

00:02:59,090 --> 00:03:03,739
have to provision freezer max load which

00:03:01,940 --> 00:03:08,000
could be at a totally different time and

00:03:03,739 --> 00:03:10,459
I'm not very flexible so this leads to

00:03:08,000 --> 00:03:12,530
overall poor utilization and I would

00:03:10,459 --> 00:03:14,420
what I'm i personally would actually

00:03:12,530 --> 00:03:17,209
like to prefer is that I can actually

00:03:14,420 --> 00:03:20,030
mix that up so that I can flexibly and

00:03:17,209 --> 00:03:23,030
dynamically partition my cluster for the

00:03:20,030 --> 00:03:26,030
different workloads and this is the one

00:03:23,030 --> 00:03:28,130
more motivation for missiles the other

00:03:26,030 --> 00:03:31,130
big trend we're seeing is like that a

00:03:28,130 --> 00:03:33,820
lot of stuff gets run in containers how

00:03:31,130 --> 00:03:37,640
many are you of you are using containers

00:03:33,820 --> 00:03:40,880
for playing or four in production okay

00:03:37,640 --> 00:03:43,069
so especially like with trends towards

00:03:40,880 --> 00:03:45,079
microservices a lot of people are

00:03:43,069 --> 00:03:47,620
interested in containers and we're going

00:03:45,079 --> 00:03:52,819
to see a little bit of that in a second

00:03:47,620 --> 00:04:04,970
so for me my calendar is telling me

00:03:52,819 --> 00:04:07,609
something sorry so for me I really like

00:04:04,970 --> 00:04:09,530
dr. because dr. makes it really easy to

00:04:07,609 --> 00:04:14,750
run stuff on my laptop so I can easily

00:04:09,530 --> 00:04:16,549
spin up a container but I can just spin

00:04:14,750 --> 00:04:18,859
up this one container on my cluster as

00:04:16,549 --> 00:04:21,039
soon as i get to managing several

00:04:18,859 --> 00:04:24,139
containers it actually gets heart

00:04:21,039 --> 00:04:26,030
because how does i find each other how

00:04:24,139 --> 00:04:27,740
can i combine them how can i make sure

00:04:26,030 --> 00:04:29,990
that they're always enough containers

00:04:27,740 --> 00:04:32,720
running so basically as soon as I want

00:04:29,990 --> 00:04:35,710
to move from a doctor on my laptop to

00:04:32,720 --> 00:04:37,639
it's like darker in a big cluster I

00:04:35,710 --> 00:04:39,560
usually

00:04:37,639 --> 00:04:42,139
it becomes a lot harder so for example

00:04:39,560 --> 00:04:45,199
till recently it was like impossible

00:04:42,139 --> 00:04:48,199
just upgrades dr. demon itself without

00:04:45,199 --> 00:04:51,229
killing all containers that's like one

00:04:48,199 --> 00:04:54,080
of the problems so this is like the

00:04:51,229 --> 00:04:59,090
other motivation for my sauce its

00:04:54,080 --> 00:05:01,520
container orchestration where does mrs.

00:04:59,090 --> 00:05:05,539
actually originated from so it was

00:05:01,520 --> 00:05:08,360
actually in 2009 there was a class

00:05:05,539 --> 00:05:11,659
project by several PhD students at

00:05:08,360 --> 00:05:14,150
Berkeley and I had this idea initially

00:05:11,659 --> 00:05:17,930
called Nexus which then turned into

00:05:14,150 --> 00:05:20,120
missus and that was actually given as a

00:05:17,930 --> 00:05:22,159
tech talk to twitter how many of you

00:05:20,120 --> 00:05:25,699
remember this Twitter fail whale when

00:05:22,159 --> 00:05:28,310
Twitter wasn't available for exams I was

00:05:25,699 --> 00:05:30,319
one of the main motivations to have

00:05:28,310 --> 00:05:33,529
something where they can much faster

00:05:30,319 --> 00:05:36,560
deploy something into production and

00:05:33,529 --> 00:05:38,930
also be flexible in terms of failures so

00:05:36,560 --> 00:05:40,729
for initially for Twitter it was really

00:05:38,930 --> 00:05:42,889
really hard if they had a new version of

00:05:40,729 --> 00:05:44,839
the software it took him like two weeks

00:05:42,889 --> 00:05:47,379
to bring it from development into their

00:05:44,839 --> 00:05:50,060
def system into their production system

00:05:47,379 --> 00:05:52,129
which was something they really wanted

00:05:50,060 --> 00:05:55,939
to improve upon to decrease the

00:05:52,129 --> 00:05:59,750
occurrences of this fail whale yeah then

00:05:55,939 --> 00:06:01,639
in 2010 they actually released like the

00:05:59,750 --> 00:06:03,740
technical report from Berkeley about

00:06:01,639 --> 00:06:06,740
missiles together with like the first

00:06:03,740 --> 00:06:09,229
implementations and then soon after in

00:06:06,740 --> 00:06:12,020
December it actually moved into Apache

00:06:09,229 --> 00:06:14,419
incubation as an incubation project and

00:06:12,020 --> 00:06:19,819
as of right now we are like a top level

00:06:14,419 --> 00:06:22,729
Apache project last week we actually

00:06:19,819 --> 00:06:25,099
also open sourced a lot of stuff around

00:06:22,729 --> 00:06:26,750
mesos so you're going to see also

00:06:25,099 --> 00:06:28,639
throughout the presentation we kind of

00:06:26,750 --> 00:06:31,039
view the view missiles as like the

00:06:28,639 --> 00:06:33,620
kernel of the operating system for your

00:06:31,039 --> 00:06:37,099
data center and then the data center

00:06:33,620 --> 00:06:40,129
around with like monitoring applications

00:06:37,099 --> 00:06:44,080
that's what we call DC us and that's

00:06:40,129 --> 00:06:44,080
what we actually open source last week

00:06:44,139 --> 00:06:51,480
the technology so from it a technology

00:06:48,800 --> 00:06:53,880
point why is that messing up

00:06:51,480 --> 00:06:58,290
in the right corner from a technology

00:06:53,880 --> 00:07:00,840
point missus is actually resource

00:06:58,290 --> 00:07:02,880
sharing so it's about the core

00:07:00,840 --> 00:07:05,640
technology is about resource allocations

00:07:02,880 --> 00:07:08,400
in big clusters but the vision is

00:07:05,640 --> 00:07:10,020
actually so that you can share resources

00:07:08,400 --> 00:07:13,980
between different frameworks and your

00:07:10,020 --> 00:07:16,020
clusters as we saw initially the vision

00:07:13,980 --> 00:07:18,390
is actually something more as a vision

00:07:16,020 --> 00:07:20,400
is the second paper on the right so the

00:07:18,390 --> 00:07:24,030
data center actually needs an operating

00:07:20,400 --> 00:07:29,280
system to control the entire data center

00:07:24,030 --> 00:07:32,940
as we are moving from like mainframes to

00:07:29,280 --> 00:07:35,100
a client pcs over virtual machines and

00:07:32,940 --> 00:07:36,390
now I actually the form factor for which

00:07:35,100 --> 00:07:38,820
you also want to develop applications

00:07:36,390 --> 00:07:43,200
it's usually for many companies it's a

00:07:38,820 --> 00:07:45,810
data center yeah as mentioned Apache

00:07:43,200 --> 00:07:49,080
mrs. it's a top-level Apache project as

00:07:45,810 --> 00:07:52,440
the name says and it's actually used by

00:07:49,080 --> 00:07:55,080
many companies on the right and actually

00:07:52,440 --> 00:07:57,420
also had quite big scale so this is off

00:07:55,080 --> 00:07:59,130
often for me this is a difference for

00:07:57,420 --> 00:08:01,770
example to cure benitez if you want to

00:07:59,130 --> 00:08:03,840
do container orchestration Maysles

00:08:01,770 --> 00:08:05,760
actually scales to ten thousands of

00:08:03,840 --> 00:08:08,550
nodes so it's also likes the largest

00:08:05,760 --> 00:08:10,260
clusters running and this is actually

00:08:08,550 --> 00:08:13,200
it's quite fault tolerant and

00:08:10,260 --> 00:08:14,760
battle-tested it's also being an open

00:08:13,200 --> 00:08:17,850
source project this also has a downside

00:08:14,760 --> 00:08:20,370
for any patch we want to introduce it's

00:08:17,850 --> 00:08:22,260
a really long review cycle so

00:08:20,370 --> 00:08:24,390
introducing new features it usually

00:08:22,260 --> 00:08:27,000
takes a little bit longer because all

00:08:24,390 --> 00:08:29,280
the Twitter guys they're really afraid

00:08:27,000 --> 00:08:31,050
that anything might break their cluster

00:08:29,280 --> 00:08:33,300
so they're really cautious and what gets

00:08:31,050 --> 00:08:37,950
merged into Z mazes core and what

00:08:33,300 --> 00:08:40,140
doesn't it's also as mentioned this

00:08:37,950 --> 00:08:42,210
vision of C data center operating

00:08:40,140 --> 00:08:46,170
systems also to have an SDK to build

00:08:42,210 --> 00:08:48,840
distributed apps and we're going to see

00:08:46,170 --> 00:08:52,170
that in a second a little with in more

00:08:48,840 --> 00:08:54,090
detail and also of the container

00:08:52,170 --> 00:08:57,930
orchestration topic we saw initially

00:08:54,090 --> 00:08:59,910
it's a big issue for many customers or

00:08:57,930 --> 00:09:01,880
big motivation for many customers to use

00:08:59,910 --> 00:09:04,850
missiles

00:09:01,880 --> 00:09:07,280
missus under the hood it's a two-level

00:09:04,850 --> 00:09:11,060
scheduler a two-level scheduler means

00:09:07,280 --> 00:09:13,910
and the decision where to run an

00:09:11,060 --> 00:09:16,010
application it's taking a two parts down

00:09:13,910 --> 00:09:18,050
there you see allocation pod which is

00:09:16,010 --> 00:09:21,470
actually the mezzos part and this part

00:09:18,050 --> 00:09:23,540
will decide what framework on top or

00:09:21,470 --> 00:09:26,840
what scheduler on top receives what

00:09:23,540 --> 00:09:30,290
resources so to see like a more concrete

00:09:26,840 --> 00:09:32,990
example we could have several schedulers

00:09:30,290 --> 00:09:34,820
on top so for example marathon is one

00:09:32,990 --> 00:09:36,500
framework used for container

00:09:34,820 --> 00:09:40,670
orchestration so it makes it really easy

00:09:36,500 --> 00:09:43,420
to run to deploy docker containers for

00:09:40,670 --> 00:09:47,060
example saying you always want 100

00:09:43,420 --> 00:09:48,650
instances of your jinxed container

00:09:47,060 --> 00:09:51,100
running because this is it something you

00:09:48,650 --> 00:09:53,690
can easily express by using marathon

00:09:51,100 --> 00:09:56,300
myriad is yet another framework and

00:09:53,690 --> 00:09:58,610
myriad is actually used for running yarn

00:09:56,300 --> 00:10:01,160
on top of me so so if you still have

00:09:58,610 --> 00:10:04,400
like a young cluster you can run that on

00:10:01,160 --> 00:10:06,560
top of mesos same as you could do for

00:10:04,400 --> 00:10:09,260
example with Q Benitez this is like one

00:10:06,560 --> 00:10:14,600
of the nice features of this two-level

00:10:09,260 --> 00:10:16,880
scheduling hierarchy and those schedule

00:10:14,600 --> 00:10:19,070
ocean tops they're basically responsible

00:10:16,880 --> 00:10:21,860
for deciding what they want to do with

00:10:19,070 --> 00:10:24,560
resources so resources they are offered

00:10:21,860 --> 00:10:27,080
here down there by the slaves or we

00:10:24,560 --> 00:10:30,500
recently renamed them to agents to be

00:10:27,080 --> 00:10:32,540
politically correct and what's happening

00:10:30,500 --> 00:10:36,230
is actually that one of those agents

00:10:32,540 --> 00:10:38,270
it's going to advertise a resources to

00:10:36,230 --> 00:10:42,560
the master it's going to say i get 10

00:10:38,270 --> 00:10:45,260
cpus idle here and 10 gigs of ram mais

00:10:42,560 --> 00:10:48,830
us master do you want to use that then

00:10:45,260 --> 00:10:51,440
the mezzos master the allocated logic is

00:10:48,830 --> 00:10:54,710
actually responsible for deciding which

00:10:51,440 --> 00:10:57,680
frameworks turn it is so it's an

00:10:54,710 --> 00:11:00,620
algorithm called drf dominant resource

00:10:57,680 --> 00:11:03,410
fairness talk to me about later if you

00:11:00,620 --> 00:11:06,200
want to know more details and in this

00:11:03,410 --> 00:11:07,820
case it would basically say it's as he

00:11:06,200 --> 00:11:10,250
turn for the marathon schedule lights

00:11:07,820 --> 00:11:11,870
under its fair share so I first going to

00:11:10,250 --> 00:11:16,220
offer those resources to the marathon

00:11:11,870 --> 00:11:18,440
scheduler and then it's the job of the

00:11:16,220 --> 00:11:21,230
teamwork or the framework scheduler to

00:11:18,440 --> 00:11:23,900
say I want to use those ten CPUs you

00:11:21,230 --> 00:11:26,960
just offered or I just want to use five

00:11:23,900 --> 00:11:29,900
of those CPUs and started task there if

00:11:26,960 --> 00:11:31,880
it does so it's going to tell the master

00:11:29,900 --> 00:11:38,270
and the master will actually then

00:11:31,880 --> 00:11:42,260
deploys a task to Z agent node and there

00:11:38,270 --> 00:11:43,910
we have yet two instances so first of

00:11:42,260 --> 00:11:46,040
all there's the executor which is kind

00:11:43,910 --> 00:11:48,670
of like the watchdog it's going to start

00:11:46,040 --> 00:11:50,750
to test its going to monitor the task

00:11:48,670 --> 00:11:52,760
usually if you're writing your first

00:11:50,750 --> 00:11:55,100
framework or your first exposure with

00:11:52,760 --> 00:11:57,020
missiles you don't even see that because

00:11:55,100 --> 00:11:59,120
their default implementation so you

00:11:57,020 --> 00:12:00,830
don't have to touch up on that and then

00:11:59,120 --> 00:12:03,020
there is the actual tasks they could be

00:12:00,830 --> 00:12:05,180
a one-to-one mapping of executor to task

00:12:03,020 --> 00:12:11,600
or an executor could actually run

00:12:05,180 --> 00:12:13,820
several tasks and the agent it's going

00:12:11,600 --> 00:12:18,790
to report its current state back to the

00:12:13,820 --> 00:12:24,950
master so if something happens like the

00:12:18,790 --> 00:12:27,110
Z executor dies the agent gets their

00:12:24,950 --> 00:12:29,150
maintenance primitive so for example you

00:12:27,110 --> 00:12:32,120
as an administrator you can say I want

00:12:29,150 --> 00:12:34,760
to take this note offline and replace

00:12:32,120 --> 00:12:36,380
the hard disk or do maintenance if that

00:12:34,760 --> 00:12:39,500
comes it's going to tell the master I

00:12:36,380 --> 00:12:41,360
going to drain those task that's again

00:12:39,500 --> 00:12:43,160
reported back to the scheduler and the

00:12:41,360 --> 00:12:45,440
scheduler can actually decide whether it

00:12:43,160 --> 00:12:46,700
wants to restart those tests or whether

00:12:45,440 --> 00:12:50,630
it's for example in the case of

00:12:46,700 --> 00:12:52,340
maintenance it's okay to be off to have

00:12:50,630 --> 00:12:54,080
this task and not running for the next

00:12:52,340 --> 00:12:56,420
ten minutes but then it's most likely

00:12:54,080 --> 00:12:59,150
going to be back so this is like the

00:12:56,420 --> 00:13:01,070
framework logic and there will be

00:12:59,150 --> 00:13:02,930
actually keeping some mezzos master

00:13:01,070 --> 00:13:05,030
really simple so it's just doing the

00:13:02,930 --> 00:13:07,010
allocation between frameworks and that's

00:13:05,030 --> 00:13:13,040
why we actually gets this massive scale

00:13:07,010 --> 00:13:16,040
to several ten thousands of nodes as

00:13:13,040 --> 00:13:18,290
mentioned before this question was a

00:13:16,040 --> 00:13:20,360
running yarn as your cluster scheduler

00:13:18,290 --> 00:13:22,400
which people often which people usually

00:13:20,360 --> 00:13:26,150
do if they do like data analytics with

00:13:22,400 --> 00:13:27,710
spark or Hadoop but the interesting part

00:13:26,150 --> 00:13:29,750
is actually that spark was initially

00:13:27,710 --> 00:13:32,300
written as a use

00:13:29,750 --> 00:13:35,240
mlk's to show how easy it is to write

00:13:32,300 --> 00:13:37,550
frameworks for missiles but by now you

00:13:35,240 --> 00:13:39,710
can run it on both yarn and mezzos and

00:13:37,550 --> 00:13:41,740
in standalone it's not a decision

00:13:39,710 --> 00:13:44,360
whether you have to do either of those

00:13:41,740 --> 00:13:46,550
by this two-level strategy you can

00:13:44,360 --> 00:13:48,620
actually run it on top which is quite

00:13:46,550 --> 00:13:51,020
useful because you can scale up and

00:13:48,620 --> 00:13:54,020
scale down your young cluster you can

00:13:51,020 --> 00:13:56,030
have several multi-tenant young clusters

00:13:54,020 --> 00:13:57,770
so you can basically spin up on your

00:13:56,030 --> 00:14:00,050
physical clusters you can come up with

00:13:57,770 --> 00:14:02,360
several young clusters for different

00:14:00,050 --> 00:14:07,910
teams quite easily and scale them up and

00:14:02,360 --> 00:14:10,610
down depending on the need as mentioned

00:14:07,910 --> 00:14:14,210
and as we've seen also with the spark

00:14:10,610 --> 00:14:15,860
example the other vision for missus is

00:14:14,210 --> 00:14:18,230
to really make it easy to write

00:14:15,860 --> 00:14:21,080
distributed applications during my PhD

00:14:18,230 --> 00:14:23,660
we did like several class projects and

00:14:21,080 --> 00:14:26,240
it basically it's always like the same I

00:14:23,660 --> 00:14:28,370
need to detect which notice offline I

00:14:26,240 --> 00:14:32,390
need to decide where I want to run a

00:14:28,370 --> 00:14:34,130
task and this is like this common

00:14:32,390 --> 00:14:36,710
functionality which you basically need

00:14:34,130 --> 00:14:39,620
for any distributed application that's

00:14:36,710 --> 00:14:45,140
one of the ideas of mazes to put that

00:14:39,620 --> 00:14:48,110
into the mezzos API and that actually

00:14:45,140 --> 00:14:50,390
allows you to just or more you still

00:14:48,110 --> 00:14:52,160
have to write some boiler code but you

00:14:50,390 --> 00:14:53,780
really focus on the application logic

00:14:52,160 --> 00:14:56,900
and not so much like the data center

00:14:53,780 --> 00:14:58,430
parts like it's a node reachable is it

00:14:56,900 --> 00:15:01,130
just a network petition but it's going

00:14:58,430 --> 00:15:04,100
to be back in a few seconds and it

00:15:01,130 --> 00:15:05,960
actually when we have a new engineer

00:15:04,100 --> 00:15:08,270
starting on his first day we let them

00:15:05,960 --> 00:15:09,770
write a framework it most of them

00:15:08,270 --> 00:15:11,660
finished in the first day some of them

00:15:09,770 --> 00:15:14,930
take two days but it's really really

00:15:11,660 --> 00:15:17,390
easy to write your own framework Chris

00:15:14,930 --> 00:15:19,910
you basically you just have to consider

00:15:17,390 --> 00:15:21,920
those two sides as we've seen before

00:15:19,910 --> 00:15:24,890
there's a scheduler side which is like

00:15:21,920 --> 00:15:27,650
on top of the Maysles master on top of

00:15:24,890 --> 00:15:29,839
the abstraction layer which is like the

00:15:27,650 --> 00:15:32,510
framework part and that framework part

00:15:29,839 --> 00:15:35,240
it basically it has to decide I get

00:15:32,510 --> 00:15:37,550
those ten CPUs offered what do I do with

00:15:35,240 --> 00:15:40,490
them do I need them don't I need them do

00:15:37,550 --> 00:15:43,580
I just need a part and as a second part

00:15:40,490 --> 00:15:46,220
is it has to react on task events so

00:15:43,580 --> 00:15:48,740
what do i do if that task has failed for

00:15:46,220 --> 00:15:51,350
some reason what do I need to do if that

00:15:48,740 --> 00:15:54,560
test is lost for some reason Vickers for

00:15:51,350 --> 00:15:56,960
examples and node has gone offline the

00:15:54,560 --> 00:15:59,390
second part which is optional is the

00:15:56,960 --> 00:16:01,640
stuff actually running on the individual

00:15:59,390 --> 00:16:04,280
nodes on the individual agents which is

00:16:01,640 --> 00:16:07,130
the executor and the executor is usually

00:16:04,280 --> 00:16:10,160
responsible for life cycle management so

00:16:07,130 --> 00:16:13,910
it could automatically restart tests its

00:16:10,160 --> 00:16:19,310
owns for example and monitoring those

00:16:13,910 --> 00:16:20,480
tasks usually or for the basic scenarios

00:16:19,310 --> 00:16:23,540
you don't have to write your own

00:16:20,480 --> 00:16:25,670
executor because there's like two common

00:16:23,540 --> 00:16:29,990
executors there's a command executor

00:16:25,670 --> 00:16:32,420
which allows you to run like any command

00:16:29,990 --> 00:16:35,420
underneath it can be like just sleep

00:16:32,420 --> 00:16:39,560
task it can be any unix command you

00:16:35,420 --> 00:16:41,480
could do java something even though with

00:16:39,560 --> 00:16:45,640
java i would actually recommend you you

00:16:41,480 --> 00:16:48,770
see docker executor and not rely on the

00:16:45,640 --> 00:16:50,660
JRE installed on the actual notes so

00:16:48,770 --> 00:16:53,540
that brings us to the second approach

00:16:50,660 --> 00:16:58,580
darker executor the darker executor lets

00:16:53,540 --> 00:17:01,640
you run any darker image and actually

00:16:58,580 --> 00:17:04,040
within you let's move from darker to

00:17:01,640 --> 00:17:06,170
more like universal container Iser which

00:17:04,040 --> 00:17:08,150
is experimental right now you can

00:17:06,170 --> 00:17:12,530
actually run docker containers without

00:17:08,150 --> 00:17:14,380
using docker so it's basically our own

00:17:12,530 --> 00:17:18,080
implementation of the open-source

00:17:14,380 --> 00:17:19,670
container interface and so you can

00:17:18,080 --> 00:17:22,160
actually run them without using the

00:17:19,670 --> 00:17:23,630
doctor demon the stalker executor still

00:17:22,160 --> 00:17:26,060
uses like the darker demon so it

00:17:23,630 --> 00:17:28,910
supports all options you can do or you

00:17:26,060 --> 00:17:32,060
could dream up with stalker if you're

00:17:28,910 --> 00:17:33,890
not so focused on darker but like on

00:17:32,060 --> 00:17:37,190
general containers and you also want to

00:17:33,890 --> 00:17:41,150
run like rocket containers oci or

00:17:37,190 --> 00:17:42,710
anything else you can actually try out

00:17:41,150 --> 00:17:44,360
the universal container either I

00:17:42,710 --> 00:17:45,860
wouldn't use it in production right now

00:17:44,360 --> 00:17:48,670
which is why I didn't put it here

00:17:45,860 --> 00:17:51,560
because it's still experimental

00:17:48,670 --> 00:17:53,930
if now you say you just want to try out

00:17:51,560 --> 00:17:55,880
and write your own framework or just

00:17:53,930 --> 00:17:58,340
look at how its implemented that's

00:17:55,880 --> 00:18:01,040
actually a nice project called rent ler

00:17:58,340 --> 00:18:05,720
so it's rendering rep trawler basically

00:18:01,040 --> 00:18:07,700
has one task for crawling the web and

00:18:05,720 --> 00:18:10,100
one then from rendering a nice image how

00:18:07,700 --> 00:18:12,260
everything is related and that's just

00:18:10,100 --> 00:18:14,140
like a demo framework to show how to

00:18:12,260 --> 00:18:16,910
write schedulers and how to write

00:18:14,140 --> 00:18:19,400
executor it's actually there are

00:18:16,910 --> 00:18:21,260
different implementations for most

00:18:19,400 --> 00:18:25,670
languages you can actually come up with

00:18:21,260 --> 00:18:27,380
Java pison C++ go and it's I think

00:18:25,670 --> 00:18:28,970
that's a really good starting point if

00:18:27,380 --> 00:18:34,370
you want to try it out yourself and just

00:18:28,970 --> 00:18:36,770
write like your own framework there are

00:18:34,370 --> 00:18:41,180
two parts I would like to go into little

00:18:36,770 --> 00:18:44,240
more detail it's basically showing like

00:18:41,180 --> 00:18:46,610
common problems people are having with

00:18:44,240 --> 00:18:48,880
Maysles aware i personally also find the

00:18:46,610 --> 00:18:53,560
challenges in the open source / da and

00:18:48,880 --> 00:18:57,260
one is like container networking the

00:18:53,560 --> 00:18:59,360
vision of as we said before of mazes is

00:18:57,260 --> 00:19:02,270
to just I don't care in which actual

00:18:59,360 --> 00:19:04,520
note down there my container or my task

00:19:02,270 --> 00:19:06,110
is running which is cool because I'm

00:19:04,520 --> 00:19:08,120
really abstracting away from the

00:19:06,110 --> 00:19:10,670
individual machines but on the other

00:19:08,120 --> 00:19:12,320
hand I end up with a problem how can

00:19:10,670 --> 00:19:15,320
they communicate how can they actually

00:19:12,320 --> 00:19:17,090
discover the different containers they

00:19:15,320 --> 00:19:21,680
need if I have applications which are

00:19:17,090 --> 00:19:24,740
composed of several containers and the

00:19:21,680 --> 00:19:26,960
old way which was okay for Twitter was

00:19:24,740 --> 00:19:29,540
that actually each of those agents just

00:19:26,960 --> 00:19:32,000
had one IP address that meant each

00:19:29,540 --> 00:19:34,340
container just gets its own port or set

00:19:32,000 --> 00:19:38,420
of ports which made the service

00:19:34,340 --> 00:19:40,730
discovery problem really hard so first

00:19:38,420 --> 00:19:42,590
of all of course port conflicts of I've

00:19:40,730 --> 00:19:46,790
like a web service which really wants to

00:19:42,590 --> 00:19:49,460
run on port 80 security if I have

00:19:46,790 --> 00:19:51,650
production services and test services

00:19:49,460 --> 00:19:54,740
running on the same agent they could

00:19:51,650 --> 00:19:56,900
influence each other or reach each other

00:19:54,740 --> 00:20:00,590
on the network of course also

00:19:56,900 --> 00:20:01,400
performance issues and yeah as mentioned

00:20:00,590 --> 00:20:03,740
this

00:20:01,400 --> 00:20:06,230
this problem of service discovery how do

00:20:03,740 --> 00:20:07,910
I figure out where my Cassandra

00:20:06,230 --> 00:20:09,920
container is running on my database

00:20:07,910 --> 00:20:11,690
containers running I want to use and

00:20:09,920 --> 00:20:14,780
there's actually there's actually

00:20:11,690 --> 00:20:18,380
there's a number of solutions but all of

00:20:14,780 --> 00:20:19,880
them actually go into the direction of

00:20:18,380 --> 00:20:21,440
Software Defined Networking where's

00:20:19,880 --> 00:20:23,870
actually believes there's a talk later

00:20:21,440 --> 00:20:27,080
on about that and so it actually

00:20:23,870 --> 00:20:31,100
segregates the traffic between different

00:20:27,080 --> 00:20:34,790
applications so that means ABCD if

00:20:31,100 --> 00:20:38,000
they're different tasks running on

00:20:34,790 --> 00:20:39,740
potentially different nodes and now with

00:20:38,000 --> 00:20:42,740
Network isolation with a network

00:20:39,740 --> 00:20:45,500
isolator in missiles it's a module I can

00:20:42,740 --> 00:20:48,260
actually say only a and be allowed to

00:20:45,500 --> 00:20:51,110
talk to each other and only DNC allowed

00:20:48,260 --> 00:20:52,970
to talk to each other which really

00:20:51,110 --> 00:20:55,850
relieves a lot of those problems it's

00:20:52,970 --> 00:20:57,980
easier to find them oh yeah and each of

00:20:55,850 --> 00:21:00,110
them just has its own IP address by

00:20:57,980 --> 00:21:01,970
using some network isolation so this is

00:21:00,110 --> 00:21:03,500
like a really nice feature and we're

00:21:01,970 --> 00:21:07,430
seeing that a lot of the communities

00:21:03,500 --> 00:21:11,450
also driving this way of more defining

00:21:07,430 --> 00:21:18,260
the network inside there inside missiles

00:21:11,450 --> 00:21:19,850
or by using missiles it's the solution

00:21:18,260 --> 00:21:21,380
here this is like one of the many

00:21:19,850 --> 00:21:23,660
solutions actually quite interesting

00:21:21,380 --> 00:21:25,990
because they just basically rewrite

00:21:23,660 --> 00:21:28,700
iptables two routes a container traffic

00:21:25,990 --> 00:21:31,550
and then just advertise the routes and

00:21:28,700 --> 00:21:34,520
that's actually pretty neat and pretty

00:21:31,550 --> 00:21:39,020
easy solution which works on any Linux

00:21:34,520 --> 00:21:41,050
system the second part is if we go over

00:21:39,020 --> 00:21:44,450
to when we have persistent and

00:21:41,050 --> 00:21:47,180
persistence and our frameworks like this

00:21:44,450 --> 00:21:48,710
scheduling and also containers if for

00:21:47,180 --> 00:21:50,990
example if you work with communities

00:21:48,710 --> 00:21:52,940
it's really easy if you're dealing with

00:21:50,990 --> 00:21:54,830
stateless app stateless apps you're just

00:21:52,940 --> 00:21:56,780
going to kill them you can restart them

00:21:54,830 --> 00:22:00,680
on any other host everything is fine if

00:21:56,780 --> 00:22:02,930
I just have my web server which gets the

00:22:00,680 --> 00:22:05,360
state from some other system it's really

00:22:02,930 --> 00:22:07,490
a easy to scale it up and down but it

00:22:05,360 --> 00:22:10,580
really becomes more challenging as soon

00:22:07,490 --> 00:22:12,680
as I work with persistence so for

00:22:10,580 --> 00:22:14,780
example if I want to implement a

00:22:12,680 --> 00:22:15,260
distributed file system on top of mazes

00:22:14,780 --> 00:22:17,600
if

00:22:15,260 --> 00:22:22,760
one in implemented distributed database

00:22:17,600 --> 00:22:25,310
on top of measles and one of the main

00:22:22,760 --> 00:22:27,950
problems is imagine you have your task

00:22:25,310 --> 00:22:31,040
which has some state so could be like

00:22:27,950 --> 00:22:33,410
the dis rooted file system could be the

00:22:31,040 --> 00:22:36,020
database or just any other task which

00:22:33,410 --> 00:22:40,700
wants to persist some state which it

00:22:36,020 --> 00:22:43,370
would preferably even if it dies I gain

00:22:40,700 --> 00:22:46,340
access to again the first problem is

00:22:43,370 --> 00:22:49,010
that as we're abstracting away from all

00:22:46,340 --> 00:22:52,630
the agents underneath I'm actually and I

00:22:49,010 --> 00:22:55,970
wouldn't be guaranteed to receive

00:22:52,630 --> 00:22:57,890
resources from the same agent again as

00:22:55,970 --> 00:23:00,380
mentioned before the agent is just going

00:22:57,890 --> 00:23:02,260
to tell the master master hey here ten

00:23:00,380 --> 00:23:05,060
CPUs what do you want to do with that

00:23:02,260 --> 00:23:09,590
and the Masters and decides who he will

00:23:05,060 --> 00:23:11,810
offer it to and so there's a concept

00:23:09,590 --> 00:23:14,180
called dynamic reservations and dynamic

00:23:11,810 --> 00:23:17,570
reservations basically the first time I

00:23:14,180 --> 00:23:20,090
get resources offered i can say i want

00:23:17,570 --> 00:23:22,220
to own those resources whatever happens

00:23:20,090 --> 00:23:25,520
if said no dice if I as a framework

00:23:22,220 --> 00:23:27,370
scheduler die and restart I don't want

00:23:25,520 --> 00:23:30,290
those resources offered to another

00:23:27,370 --> 00:23:33,590
framework so I want to own them for

00:23:30,290 --> 00:23:35,390
right now until I release them so they

00:23:33,590 --> 00:23:38,000
bruh I'm guaranteed to get resources

00:23:35,390 --> 00:23:39,770
again offered from the same agent the

00:23:38,000 --> 00:23:43,790
second problem is that actually by

00:23:39,770 --> 00:23:46,640
default for isolation purposes the test

00:23:43,790 --> 00:23:48,320
sandbox so where he can persist data to

00:23:46,640 --> 00:23:50,660
is actually garbage collected and

00:23:48,320 --> 00:23:52,640
there's also called primitive and

00:23:50,660 --> 00:23:54,860
missiles called persistent volume where

00:23:52,640 --> 00:23:56,900
I can say I want a persistent volume

00:23:54,860 --> 00:23:58,520
which shouldn't be cleaned up and which

00:23:56,900 --> 00:24:04,910
also shouldn't be in the sandbox for

00:23:58,520 --> 00:24:06,980
other reasons the second interesting

00:24:04,910 --> 00:24:10,760
topic if we talking about persistence

00:24:06,980 --> 00:24:13,790
its external volumes so a lot of people

00:24:10,760 --> 00:24:18,200
in the mazes community they wanted to

00:24:13,790 --> 00:24:20,780
have a storage on which was necessarily

00:24:18,200 --> 00:24:23,720
on the same notes mezzos was running on

00:24:20,780 --> 00:24:27,080
so for example like EBS volumes in ec

00:24:23,720 --> 00:24:28,999
two which would be which they wanted to

00:24:27,080 --> 00:24:31,069
access from the cluster so

00:24:28,999 --> 00:24:34,249
not necessarily mezzos managed so

00:24:31,069 --> 00:24:37,039
Maysles is not going to say hey here 10

00:24:34,249 --> 00:24:39,829
gigabytes of EBS storage but they

00:24:37,039 --> 00:24:42,309
basically want to be able to use it so

00:24:39,829 --> 00:24:45,229
for example I stalker volumes and

00:24:42,309 --> 00:24:48,169
there's like something nice from the

00:24:45,229 --> 00:24:51,139
open source part from EMC which is a

00:24:48,169 --> 00:24:52,999
volume driver isolator module that's how

00:24:51,139 --> 00:24:56,599
it's called in docker and that actually

00:24:52,999 --> 00:24:58,999
allows you to just write an app

00:24:56,599 --> 00:25:01,879
definition a nap definition is something

00:24:58,999 --> 00:25:04,519
you start up a task which on marathon in

00:25:01,879 --> 00:25:07,729
this case and I can basically just

00:25:04,519 --> 00:25:09,499
specify some magic variables and those

00:25:07,729 --> 00:25:11,239
magic variables what's going to happen

00:25:09,499 --> 00:25:14,659
it's automatically going to create an

00:25:11,239 --> 00:25:16,729
EBS volume or ask a light o or whatever

00:25:14,659 --> 00:25:21,259
as a back-end series for the volume

00:25:16,729 --> 00:25:24,019
driver and I actually like this project

00:25:21,259 --> 00:25:25,909
so much I don't like the magic variables

00:25:24,019 --> 00:25:28,489
here but i like it because it actually

00:25:25,909 --> 00:25:30,229
shows that using this modular structure

00:25:28,489 --> 00:25:32,329
we are having in mazes you can actually

00:25:30,229 --> 00:25:35,899
extend it for so many different purposes

00:25:32,329 --> 00:25:38,179
so anyone can write its own isolated

00:25:35,899 --> 00:25:40,999
module this isolation module is like

00:25:38,179 --> 00:25:44,619
storage isolated module the previous one

00:25:40,999 --> 00:25:46,999
it's like a network isolation module so

00:25:44,619 --> 00:25:52,179
anyone can just come up with its own

00:25:46,999 --> 00:25:52,179
isolated modules for its own purposes

00:25:52,629 --> 00:26:02,359
this brings us to like the next step

00:25:55,269 --> 00:26:04,429
DCOs and actually yeah so mrs. itself we

00:26:02,359 --> 00:26:06,229
viewed as a kernel the kernel which is

00:26:04,429 --> 00:26:08,599
basically like in a traditional Linux

00:26:06,229 --> 00:26:11,779
system it gives you like these cysts

00:26:08,599 --> 00:26:14,119
calls to launch a task it gives you some

00:26:11,779 --> 00:26:16,839
updates but you need a lot of stuff

00:26:14,119 --> 00:26:19,999
around for a proper operating system and

00:26:16,839 --> 00:26:22,069
this is we actually we have built it as

00:26:19,999 --> 00:26:25,369
a company together with many partners

00:26:22,069 --> 00:26:27,859
but we decided to open source it I said

00:26:25,369 --> 00:26:32,299
last week and so it's actually it's

00:26:27,859 --> 00:26:34,489
about 30 i think we now have 34 open

00:26:32,299 --> 00:26:36,379
source projects and there plus like

00:26:34,489 --> 00:26:39,469
roadmaps design together with many

00:26:36,379 --> 00:26:42,770
partners which actually open sores the

00:26:39,469 --> 00:26:47,090
entire ecosystem around it so this

00:26:42,770 --> 00:26:50,600
includes for example monitoring this

00:26:47,090 --> 00:26:52,670
includes different applications so

00:26:50,600 --> 00:26:54,710
different frameworks which are running

00:26:52,670 --> 00:27:00,500
on top which make it really easy to use

00:26:54,710 --> 00:27:02,480
it and also just like security anything

00:27:00,500 --> 00:27:05,110
you need like basically around missiles

00:27:02,480 --> 00:27:08,270
to actually run it in production so

00:27:05,110 --> 00:27:10,610
mazes itself it's it's actually it's

00:27:08,270 --> 00:27:12,410
easy to set up but to really run it

00:27:10,610 --> 00:27:14,690
safely in production you need like a

00:27:12,410 --> 00:27:17,720
team like for example in Twitter and

00:27:14,690 --> 00:27:19,790
Twitter they have around 5i sing only by

00:27:17,720 --> 00:27:22,840
now engineers running that but it's not

00:27:19,790 --> 00:27:26,570
like a one-man job to set up like this

00:27:22,840 --> 00:27:29,540
amaze us in a cluster and with PCOS is

00:27:26,570 --> 00:27:31,250
actually possible that one person just

00:27:29,540 --> 00:27:33,170
tries to Dallas and sets it up for a

00:27:31,250 --> 00:27:35,690
large cluster because it actually just

00:27:33,170 --> 00:27:37,850
has all those best practices by many

00:27:35,690 --> 00:27:42,400
partners everyone who contributed to the

00:27:37,850 --> 00:27:48,320
open source project in build into DC OS

00:27:42,400 --> 00:27:58,700
and they're right I would actually like

00:27:48,320 --> 00:28:00,650
to switch to a short demo so I just

00:27:58,700 --> 00:28:06,380
before that I just started up a demo

00:28:00,650 --> 00:28:08,210
cluster actually on ec2 and this is like

00:28:06,380 --> 00:28:12,710
the main interface if you lock in choose

00:28:08,210 --> 00:28:14,560
a UI of D cos this is what you're going

00:28:12,710 --> 00:28:18,200
to see and what I find really

00:28:14,560 --> 00:28:21,230
interesting or what I find like the core

00:28:18,200 --> 00:28:23,660
value is that usually you're not really

00:28:21,230 --> 00:28:26,750
focused about notes so notice basically

00:28:23,660 --> 00:28:29,810
it's just like a small widget here but

00:28:26,750 --> 00:28:31,520
is an operator what do I care about when

00:28:29,810 --> 00:28:33,110
do I want to be woken up at night I

00:28:31,520 --> 00:28:38,180
don't want to be working up at night

00:28:33,110 --> 00:28:40,280
just because one node field I want to be

00:28:38,180 --> 00:28:43,160
woken up if for example might ask

00:28:40,280 --> 00:28:45,260
failure rate goes up or if my cluster is

00:28:43,160 --> 00:28:47,990
full so if i have like ninety-five

00:28:45,260 --> 00:28:51,170
percent cpu a location that's telling me

00:28:47,990 --> 00:28:55,730
hey something is wrong i should do

00:28:51,170 --> 00:28:56,519
something and this is like what's really

00:28:55,730 --> 00:28:59,479
driving

00:28:56,519 --> 00:29:03,299
dcs communities is making it easy for

00:28:59,479 --> 00:29:05,070
operators to run such cluster and yet

00:29:03,299 --> 00:29:08,369
the second part i probably care about

00:29:05,070 --> 00:29:09,899
our all my services healthy so in this

00:29:08,369 --> 00:29:12,570
cluster actually just installed

00:29:09,899 --> 00:29:14,339
Cassandra up front Chris Cassandra

00:29:12,570 --> 00:29:16,099
installation takes like 10 minutes I

00:29:14,339 --> 00:29:19,999
didn't want to do it in the presentation

00:29:16,099 --> 00:29:22,619
and so what I want to see are all the

00:29:19,999 --> 00:29:28,259
services I'm running on my cluster as a

00:29:22,619 --> 00:29:30,979
healthy this service installation is

00:29:28,259 --> 00:29:35,940
actually something pretty cool because

00:29:30,979 --> 00:29:39,119
anyone can contribute new packages to D

00:29:35,940 --> 00:29:41,929
cos it's basically it's like you have on

00:29:39,119 --> 00:29:44,519
your iPhone you have your App Store and

00:29:41,929 --> 00:29:45,959
this is universe it's called it's kind

00:29:44,519 --> 00:29:49,799
of like the app store for your cluster

00:29:45,959 --> 00:29:51,919
so I can just go here I don't know how

00:29:49,799 --> 00:29:55,739
many of you have installed like

00:29:51,919 --> 00:29:57,179
databases or anything on a cluster and

00:29:55,739 --> 00:30:00,929
especially if you want to try out

00:29:57,179 --> 00:30:02,519
something so again like being a student

00:30:00,929 --> 00:30:04,469
we wanted to benchmark different

00:30:02,519 --> 00:30:06,629
database systems and it took us like one

00:30:04,469 --> 00:30:10,349
afternoon to set up each of them and

00:30:06,629 --> 00:30:12,509
here it's actually really easy i can

00:30:10,349 --> 00:30:14,129
just pick HDFS it's going to take a

00:30:12,509 --> 00:30:16,829
while and let me just check that i have

00:30:14,129 --> 00:30:23,909
enough free resources should work good

00:30:16,829 --> 00:30:30,269
so i can actually just i can actually

00:30:23,909 --> 00:30:32,849
just go here install HDFS I can do like

00:30:30,269 --> 00:30:34,259
there's like a default best installation

00:30:32,849 --> 00:30:36,809
which is just going to put it in your

00:30:34,259 --> 00:30:41,039
cluster you can also go to like advanced

00:30:36,809 --> 00:30:43,289
settings and configure it that being

00:30:41,039 --> 00:30:45,989
said the advanced settings is actually

00:30:43,289 --> 00:30:48,149
it's generating adjacent file and if i'm

00:30:45,989 --> 00:30:50,969
running a production cluster let me just

00:30:48,149 --> 00:30:54,389
hit click install if i'm running a

00:30:50,969 --> 00:30:57,149
production cluster i wouldn't use this

00:30:54,389 --> 00:30:59,639
default universe packages so if i'm

00:30:57,149 --> 00:31:02,700
running a production cluster i usually i

00:30:59,639 --> 00:31:04,529
want to have this those Jason files and

00:31:02,700 --> 00:31:07,289
we're going to see them see how we can

00:31:04,529 --> 00:31:09,749
actually deploy them also by the CLI in

00:31:07,289 --> 00:31:10,410
a second and I really want to version

00:31:09,749 --> 00:31:13,110
them

00:31:10,410 --> 00:31:14,670
because any changes I always want to

00:31:13,110 --> 00:31:16,710
know what's running in my cluster so

00:31:14,670 --> 00:31:18,570
that's what we're seeing like with a lot

00:31:16,710 --> 00:31:20,430
of customers which they at definitions

00:31:18,570 --> 00:31:22,500
they basically someone writes them

00:31:20,430 --> 00:31:25,620
someone replies them to the cluster so

00:31:22,500 --> 00:31:27,420
that would be like one advice really

00:31:25,620 --> 00:31:29,070
really make sure that you version all

00:31:27,420 --> 00:31:30,810
the stuff since you have some defined

00:31:29,070 --> 00:31:32,910
place and some provenance that you

00:31:30,810 --> 00:31:35,880
always know what applications are

00:31:32,910 --> 00:31:38,040
running in your cluster but for demo

00:31:35,880 --> 00:31:42,870
reasons this is actually quite cool to

00:31:38,040 --> 00:31:44,700
just be able to go here click and if I

00:31:42,870 --> 00:31:47,100
want to be more advanced I can either go

00:31:44,700 --> 00:31:49,950
to the Advanced tab here or I can go

00:31:47,100 --> 00:31:52,410
into more detail by specifying the Jason

00:31:49,950 --> 00:31:54,930
found myself which I could also copy

00:31:52,410 --> 00:31:59,700
from from here for example as a basic

00:31:54,930 --> 00:32:02,510
template real brief here just the

00:31:59,700 --> 00:32:05,460
difference between selected packages and

00:32:02,510 --> 00:32:09,210
community packages there are some

00:32:05,460 --> 00:32:12,320
certification requirements so we want to

00:32:09,210 --> 00:32:15,060
make sure that to differentiate what

00:32:12,320 --> 00:32:17,790
services we recommend to be used in

00:32:15,060 --> 00:32:19,410
production and which ones not because

00:32:17,790 --> 00:32:21,930
they are just not tested that far and

00:32:19,410 --> 00:32:25,020
those selected packages there's actually

00:32:21,930 --> 00:32:29,640
it's also documented in the dcs project

00:32:25,020 --> 00:32:31,800
there's certification criteria and if

00:32:29,640 --> 00:32:34,590
you match those certification criteria

00:32:31,800 --> 00:32:37,800
you basically become a selected package

00:32:34,590 --> 00:32:40,110
and but anyone can also just come up

00:32:37,800 --> 00:32:43,980
with a community package here and then

00:32:40,110 --> 00:32:45,720
if you want to graduate as a selected

00:32:43,980 --> 00:32:50,490
package you and just need to go through

00:32:45,720 --> 00:32:53,460
the certification process and actually

00:32:50,490 --> 00:32:55,620
anyone it's really easy to set up your

00:32:53,460 --> 00:32:58,920
own repository for this app store it's

00:32:55,620 --> 00:33:01,530
basically just you can either clone a

00:32:58,920 --> 00:33:03,960
get up project provided in the dcs

00:33:01,530 --> 00:33:06,630
project or you can actually you can also

00:33:03,960 --> 00:33:10,530
just come up with a folder which has

00:33:06,630 --> 00:33:12,540
certain structure but so also for if you

00:33:10,530 --> 00:33:14,670
run it like internally and you inside

00:33:12,540 --> 00:33:18,750
your organization it's really easy to

00:33:14,670 --> 00:33:20,160
just spin it up your own apps so if

00:33:18,750 --> 00:33:22,050
you're developing your own apps you

00:33:20,160 --> 00:33:24,330
don't want to share with the entire

00:33:22,050 --> 00:33:26,610
universe with anyone else

00:33:24,330 --> 00:33:28,830
which I would still recommend Christie

00:33:26,610 --> 00:33:31,049
open-source idea but if you don't want

00:33:28,830 --> 00:33:37,620
to do that you can also just set it up

00:33:31,049 --> 00:33:39,990
as your own instance so we see here we

00:33:37,620 --> 00:33:43,130
actually see the different services we

00:33:39,990 --> 00:33:45,809
seeing that HDFS just turned healthy

00:33:43,130 --> 00:33:48,929
this is kind of sad cause i wanted to

00:33:45,809 --> 00:33:52,669
show it's still installing but let me

00:33:48,929 --> 00:33:55,470
just show use a marathon service

00:33:52,669 --> 00:34:00,120
marathon let's actually open it in a new

00:33:55,470 --> 00:34:04,260
window marathon is that before it's both

00:34:00,120 --> 00:34:06,330
used for a container orchestration but

00:34:04,260 --> 00:34:11,190
for us it's actually it's also it's like

00:34:06,330 --> 00:34:13,109
the init system of the cluster so by

00:34:11,190 --> 00:34:15,149
default marathon is going to always

00:34:13,109 --> 00:34:18,750
ensure that i have a certain number of

00:34:15,149 --> 00:34:20,580
instances running and whenever they fail

00:34:18,750 --> 00:34:23,339
marathon is automatically going to

00:34:20,580 --> 00:34:28,770
restart them and make sure i always have

00:34:23,339 --> 00:34:32,129
this number of instances running and to

00:34:28,770 --> 00:34:37,230
show that so let's actually deploy a new

00:34:32,129 --> 00:34:40,220
app mezzos as you seen before it has its

00:34:37,230 --> 00:34:44,899
origins by twitter so what could we do

00:34:40,220 --> 00:34:54,679
else and deploy like a twitter clone

00:34:44,899 --> 00:34:54,679
less is that actually readable

00:35:00,020 --> 00:35:07,109
so this is just the app definition and

00:35:04,760 --> 00:35:08,640
it actually down there has a lot of

00:35:07,109 --> 00:35:10,319
values which are used for the load

00:35:08,640 --> 00:35:12,720
balancer so you don't have to be too

00:35:10,319 --> 00:35:14,849
concerned about them but this is could

00:35:12,720 --> 00:35:17,430
also be like adjacent file you used to

00:35:14,849 --> 00:35:20,220
deploy your services on top of there so

00:35:17,430 --> 00:35:35,640
this would be something as said earlier

00:35:20,220 --> 00:35:37,770
I would actually version so and dcs it

00:35:35,640 --> 00:35:40,559
also comes with like a CLI which I

00:35:37,770 --> 00:35:43,470
installed earlier and I can so I can

00:35:40,559 --> 00:35:46,079
just do D cos I want to install a new

00:35:43,470 --> 00:35:50,760
marathon app so i do maris on tap Add

00:35:46,079 --> 00:35:53,510
and now I can just add this I hope just

00:35:50,760 --> 00:35:53,510
commanders right

00:35:57,930 --> 00:36:08,190
oh sure does that help yep that looks

00:36:04,530 --> 00:36:10,559
good let's switch back to the cluster

00:36:08,190 --> 00:36:14,430
you are we can actually do all the stuff

00:36:10,559 --> 00:36:16,800
also we see a CLI but it's much harder

00:36:14,430 --> 00:36:19,230
to see so yeah that looks good the

00:36:16,800 --> 00:36:24,300
Twitter app Twitter was in name for it

00:36:19,230 --> 00:36:26,069
it's running and deployed and here we

00:36:24,300 --> 00:36:27,630
actually currently have three running

00:36:26,069 --> 00:36:32,849
instances so that was one of the

00:36:27,630 --> 00:36:35,579
parameters which we put in there let's

00:36:32,849 --> 00:36:38,280
see whether this is now available yes

00:36:35,579 --> 00:36:40,770
it's now available maybe like two

00:36:38,280 --> 00:36:43,140
sentences for the structure of this

00:36:40,770 --> 00:36:47,430
application so this application it's

00:36:43,140 --> 00:36:50,760
like a simple a simple container which I

00:36:47,430 --> 00:36:52,559
can scale up and scale down and the part

00:36:50,760 --> 00:36:54,569
about the marathon load balancers

00:36:52,559 --> 00:36:58,890
actually the load balancer interface in

00:36:54,569 --> 00:37:00,660
front of it which lets me skip which

00:36:58,890 --> 00:37:02,670
basically defines the end point where it

00:37:00,660 --> 00:37:05,160
can reach the app and then there's

00:37:02,670 --> 00:37:12,569
Cassandra used as the distributed data

00:37:05,160 --> 00:37:16,020
store for for this application so let's

00:37:12,569 --> 00:37:17,880
just go here so we're seeing there's

00:37:16,020 --> 00:37:19,559
three instant orphan let's actually

00:37:17,880 --> 00:37:30,540
let's do something first let's first

00:37:19,559 --> 00:37:36,089
tweet jerk hi Oh SDC tweet this of

00:37:30,540 --> 00:37:37,950
course is ok but it's but it's there I'm

00:37:36,089 --> 00:37:39,990
not sure what you should write no voice

00:37:37,950 --> 00:37:44,549
but yeah we see the tweet is there it

00:37:39,990 --> 00:37:47,089
gets stored in Cassandra and so we can

00:37:44,549 --> 00:37:50,250
actually run like our own little Twitter

00:37:47,089 --> 00:37:53,369
being Twitter we're now hitting a

00:37:50,250 --> 00:37:55,710
problem which is a those three instance

00:37:53,369 --> 00:37:59,339
they are not enough anymore to scale my

00:37:55,710 --> 00:38:04,700
service so I actually want to scale up

00:37:59,339 --> 00:38:04,700
from three to say six

00:38:05,670 --> 00:38:15,160
and now marathon it starts deploying

00:38:09,880 --> 00:38:18,010
them usually the first deploy on one

00:38:15,160 --> 00:38:19,869
node if you're running darker it takes

00:38:18,010 --> 00:38:21,099
takes a little bit longer because it

00:38:19,869 --> 00:38:23,859
actually it's pulling the docker

00:38:21,099 --> 00:38:25,630
container but like see further so if

00:38:23,859 --> 00:38:27,700
first time if you're deploying like a

00:38:25,630 --> 00:38:31,210
doctor appt don't be scared if it takes

00:38:27,700 --> 00:38:33,430
a while because it first needs to pull

00:38:31,210 --> 00:38:36,910
the docker image and then actually

00:38:33,430 --> 00:38:40,329
deploy it so yeah now we scaled up our

00:38:36,910 --> 00:38:45,280
service and the load balancer actually

00:38:40,329 --> 00:38:47,440
has in the background more tweet or yeah

00:38:45,280 --> 00:38:57,160
more front and applications to choose

00:38:47,440 --> 00:39:03,369
from and maybe just real brief and yes

00:38:57,160 --> 00:39:05,410
as said I'm really just using Z I what

00:39:03,369 --> 00:39:07,599
for demos is actually it's quite cool

00:39:05,410 --> 00:39:10,180
and also to get a feeling for it it's

00:39:07,599 --> 00:39:15,099
really helpful to just play around with

00:39:10,180 --> 00:39:18,849
the different different options I have

00:39:15,099 --> 00:39:22,000
so just showing how we can do different

00:39:18,849 --> 00:39:29,260
stuff I can simply deploy like a sleep

00:39:22,000 --> 00:39:33,010
command just a dummy task and we don't

00:39:29,260 --> 00:39:37,599
need that much memory for it and we want

00:39:33,010 --> 00:39:40,530
two instances running or set so now I'm

00:39:37,599 --> 00:39:43,660
simply deploying like any Linux command

00:39:40,530 --> 00:39:46,720
using the command executor we've seen

00:39:43,660 --> 00:39:48,579
before but i can also i can hear pick

00:39:46,720 --> 00:39:52,030
docker images i can give a different

00:39:48,579 --> 00:39:54,069
parameters i can force it to pull it so

00:39:52,030 --> 00:39:58,270
i actually have a lot of options

00:39:54,069 --> 00:40:01,869
available and then if i've configured

00:39:58,270 --> 00:40:04,299
all of this and I feel yeah this is kind

00:40:01,869 --> 00:40:09,569
of okay I can go to the Jason view and

00:40:04,299 --> 00:40:09,569
actually take care of Z Jason

00:40:12,349 --> 00:40:20,309
okay back and now as we deployed more

00:40:18,299 --> 00:40:22,920
apps if we're going back to the

00:40:20,309 --> 00:40:25,349
dashboard we're seeing that the CPU

00:40:22,920 --> 00:40:31,109
allocation increased memory allocation

00:40:25,349 --> 00:40:33,450
increased and I also in this sense I

00:40:31,109 --> 00:40:35,609
have see i have several ways of

00:40:33,450 --> 00:40:39,030
controlling that so i don't want that

00:40:35,609 --> 00:40:42,059
like some like intern for example

00:40:39,030 --> 00:40:46,069
probably scales up this tweet up to

00:40:42,059 --> 00:40:49,079
10,000 so my entire cluster is unusable

00:40:46,069 --> 00:40:51,780
so what i usually do in production

00:40:49,079 --> 00:40:55,559
settings is something called quota so i

00:40:51,780 --> 00:40:58,410
can basically say each role or each

00:40:55,559 --> 00:41:01,740
entity is just allowed to have that many

00:40:58,410 --> 00:41:03,660
resources and the other part which i can

00:41:01,740 --> 00:41:06,150
i can basically also do the reverse i

00:41:03,660 --> 00:41:09,720
can set a maximum limit but i can also

00:41:06,150 --> 00:41:12,240
set a minimum limit i can say my HDFS

00:41:09,720 --> 00:41:14,790
server is running in my cluster I wanted

00:41:12,240 --> 00:41:16,680
always name nodes have enough resources

00:41:14,790 --> 00:41:19,369
available whatever happens in the

00:41:16,680 --> 00:41:23,130
cluster so I can set it like a minimal

00:41:19,369 --> 00:41:26,160
resource guarantee guaranteeing that

00:41:23,130 --> 00:41:28,559
always so these many resources are going

00:41:26,160 --> 00:41:31,380
to be available for the HDFS cluster and

00:41:28,559 --> 00:41:33,540
if too many notes are failing if

00:41:31,380 --> 00:41:36,180
something else happens in the cluster

00:41:33,540 --> 00:41:39,140
what will actually happen is that he is

00:41:36,180 --> 00:41:42,150
going to shut down other applications

00:41:39,140 --> 00:41:44,130
which don't have a quota to just to I

00:41:42,150 --> 00:41:51,559
always ensure those mission critical

00:41:44,130 --> 00:41:51,559
services that they are running okay I

00:41:53,450 --> 00:41:57,380
wish faster than I expected

00:42:02,609 --> 00:42:08,799
so yeah this again is the universe

00:42:05,109 --> 00:42:10,690
service as mentioned and yeah actually I

00:42:08,799 --> 00:42:13,749
was quicker with my presentation that I

00:42:10,690 --> 00:42:23,609
hoped but this gives us just more time

00:42:13,749 --> 00:42:27,099
for questions so any questions yeah hmm

00:42:23,609 --> 00:42:31,839
and could you explain a little bit more

00:42:27,099 --> 00:42:36,309
about that how do you avoid that traffic

00:42:31,839 --> 00:42:41,979
from all your physical machine okay it's

00:42:36,309 --> 00:42:44,559
like in the same subnet and so basically

00:42:41,979 --> 00:42:48,279
each application gets its own IP and

00:42:44,559 --> 00:42:50,019
then what their different solutions but

00:42:48,279 --> 00:42:53,650
what's the Calico solution is going to

00:42:50,019 --> 00:43:02,680
do it's creating IP table rules who can

00:42:53,650 --> 00:43:05,710
talk to whom basically so on on a per

00:43:02,680 --> 00:43:09,579
note level so you can just imagine

00:43:05,710 --> 00:43:12,069
there's like a imagine like a framework

00:43:09,579 --> 00:43:14,710
running and the framework knows who can

00:43:12,069 --> 00:43:17,259
talk to whom and that's actually going

00:43:14,710 --> 00:43:19,950
to create the IP table rules on a per

00:43:17,259 --> 00:43:19,950
node basis

00:43:34,130 --> 00:43:41,070
what can can you define the problem

00:43:37,170 --> 00:43:46,110
you're meaning that both containers are

00:43:41,070 --> 00:43:49,080
running on the same instance or yeah so

00:43:46,110 --> 00:43:51,450
currently that's so that would be for

00:43:49,080 --> 00:43:55,830
example I would be the pot concept incur

00:43:51,450 --> 00:43:57,660
benitez right so pots and cabrini to say

00:43:55,830 --> 00:44:01,760
also they share like a basically

00:43:57,660 --> 00:44:05,490
localhost and a common network and in

00:44:01,760 --> 00:44:06,870
mace in Marathon actually right now what

00:44:05,490 --> 00:44:09,390
you have to do you have to define a

00:44:06,870 --> 00:44:12,300
label so the first task running would

00:44:09,390 --> 00:44:13,890
say to label and then we know that we

00:44:12,300 --> 00:44:17,820
can only schedule the second container

00:44:13,890 --> 00:44:19,680
there but there's soon going to be as a

00:44:17,820 --> 00:44:21,420
concept of pots as well and then you

00:44:19,680 --> 00:44:26,850
would just define they both need to run

00:44:21,420 --> 00:44:31,100
on the same node and therefore I they

00:44:26,850 --> 00:44:34,590
can share this a mounted volume in

00:44:31,100 --> 00:44:37,110
general what we would recommend is to

00:44:34,590 --> 00:44:40,080
always back up those volumes by some

00:44:37,110 --> 00:44:42,590
distributed file system anyhow so that

00:44:40,080 --> 00:44:45,030
it doesn't matter too much on which node

00:44:42,590 --> 00:44:48,060
for example if the app is failing and

00:44:45,030 --> 00:44:57,300
then restarting on which node it's

00:44:48,060 --> 00:45:00,870
restarting yeah hi can I formulate how

00:44:57,300 --> 00:45:03,600
some something like impact zones with

00:45:00,870 --> 00:45:06,000
Bezos so I can have two data centers and

00:45:03,600 --> 00:45:09,540
say some application has to run in each

00:45:06,000 --> 00:45:14,340
data center or at least two instances in

00:45:09,540 --> 00:45:16,110
in one wreck or something like that this

00:45:14,340 --> 00:45:18,270
is this is a concept which isn't like in

00:45:16,110 --> 00:45:21,800
mrs. core but it's like it's a marathon

00:45:18,270 --> 00:45:29,780
constraint you could specify that you're

00:45:21,800 --> 00:45:29,780
so marathon lets you specify constraints

00:45:35,130 --> 00:45:43,200
so I can basically specify constraints

00:45:38,550 --> 00:45:43,200
so if I'm coming up here with a new app

00:45:45,270 --> 00:45:50,050
as it under optional here I can

00:45:48,670 --> 00:45:52,599
basically I can specify constraints

00:45:50,050 --> 00:45:54,990
which could be I want the difference

00:45:52,599 --> 00:45:58,450
instances of this app running on

00:45:54,990 --> 00:46:00,550
different notes so that the host name is

00:45:58,450 --> 00:46:03,579
unique I can also specify that they

00:46:00,550 --> 00:46:15,900
running in unique how do to college

00:46:03,579 --> 00:46:20,920
disaster zones or impact zones yeah hi

00:46:15,900 --> 00:46:24,040
resource wise you are running java to

00:46:20,920 --> 00:46:26,650
connect java to java and java being java

00:46:24,040 --> 00:46:28,119
it's quite heavy on resources so does it

00:46:26,650 --> 00:46:30,910
mean that you need to have separate

00:46:28,119 --> 00:46:34,359
infrastructure in order to operate your

00:46:30,910 --> 00:46:40,900
own infrastructure or how how does it

00:46:34,359 --> 00:46:43,060
scale in which ok which part which java

00:46:40,900 --> 00:46:45,160
part needs to take which travelpod secor

00:46:43,060 --> 00:46:47,200
of mrs. that you wait for example

00:46:45,160 --> 00:46:49,619
zookeeper right this one is connecting

00:46:47,200 --> 00:46:52,270
the Masters to slaves it's java-based

00:46:49,619 --> 00:46:54,520
both masters and slaves are also

00:46:52,270 --> 00:46:58,329
java-based and when you run on top of

00:46:54,520 --> 00:47:01,180
the sleep master and slave there c++

00:46:58,329 --> 00:47:07,839
later not java oh cool that I wasn't

00:47:01,180 --> 00:47:12,160
aware of that ok thank you I'm my daily

00:47:07,839 --> 00:47:23,290
job with C++ programming so i'm not a

00:47:12,160 --> 00:47:25,569
big java program any more questions no

00:47:23,290 --> 00:47:28,380
everyone is waiting for the coffee so

00:47:25,569 --> 00:47:28,380
yeah thank you again

00:47:29,680 --> 00:47:31,740

YouTube URL: https://www.youtube.com/watch?v=Qc-D4Gl4aHI


