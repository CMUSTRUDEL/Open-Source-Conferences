Title: OSDC 2016: Tuning Linux for your Database by Colin Charles
Publication date: 2016-05-04
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	Many operations folk know that performance varies depending on using one of the many Linux filesystems like EXT4 or XFS. They also know of the schedulers available, they see the OOM killer coming and more. However, appropriate configuration is necessary when you're running your databases at scale.
Learn best practices for Linux performance tuning for MariaDB/MySQL (where MyISAM uses the operating system cache, and InnoDB maintains its own aggressive buffer pool), as well as PostgreSQL and MongoDB (more dependent on the operating system). Topics that will be covered include: filesystems, swap and memory management, I/O scheduler settings, using and understanding the tools available (like iostat/vmstat/etc), practical kernel configuration, profiling your database, and using RAID and LVM.
There is a focus on bare metal as well as configuring your cloud instances in.
Learn from practical examples from the trenches.
Captions: 
	00:00:11,710 --> 00:00:18,300
okay we are back on track I hope you all

00:00:14,460 --> 00:00:21,939
enjoyed your lunch so we will continue

00:00:18,300 --> 00:00:28,560
with the talk about linux tuning for

00:00:21,939 --> 00:00:34,870
databases with colin charts thank you

00:00:28,560 --> 00:00:41,790
good afternoon I'm guessing all of you

00:00:34,870 --> 00:00:45,699
run linux for in production yes no ah

00:00:41,790 --> 00:00:48,190
freebsd okay so there are obviously some

00:00:45,699 --> 00:00:52,420
things that change for freebsd that are

00:00:48,190 --> 00:00:54,489
not quite Lynn eccentric but hopefully

00:00:52,420 --> 00:00:59,949
you also still learn some things from

00:00:54,489 --> 00:01:04,750
the talk do all of you run databases yes

00:00:59,949 --> 00:01:11,730
yes okay um maybe mysql ecosystem

00:01:04,750 --> 00:01:23,159
databases okay postgres okay page base

00:01:11,730 --> 00:01:27,490
Hadoop ecosystem Reddy's react

00:01:23,159 --> 00:01:34,329
anything else I didn't mention Oracle oh

00:01:27,490 --> 00:01:37,119
yeah so they make an operating system

00:01:34,329 --> 00:01:41,710
and processes for your oracle database

00:01:37,119 --> 00:01:43,780
combined Exadata okay so i'm going to

00:01:41,710 --> 00:01:48,520
focus on open source databases in this

00:01:43,780 --> 00:01:51,850
talk I'm Colin I work at unready be

00:01:48,520 --> 00:01:55,390
server and I've been in the MySQL wool

00:01:51,850 --> 00:01:59,200
for quite some time doing mysql before

00:01:55,390 --> 00:02:01,719
and also fedora and openoffice this is

00:01:59,200 --> 00:02:04,180
my rough agenda for what we plan to

00:02:01,719 --> 00:02:06,729
cover today how many here are actually

00:02:04,180 --> 00:02:12,280
using some kind of cloud provider beat

00:02:06,729 --> 00:02:13,780
amazon or Rackspace okay so most of you

00:02:12,280 --> 00:02:16,860
are still running bare metal in data

00:02:13,780 --> 00:02:18,960
centers excellent

00:02:16,860 --> 00:02:21,150
yeah I'm going to talk to you about IO

00:02:18,960 --> 00:02:23,790
file systems and a lot of this is

00:02:21,150 --> 00:02:28,890
obviously most of this is Linux Pacific

00:02:23,790 --> 00:02:32,640
and not PSD related so obviously the

00:02:28,890 --> 00:02:34,410
focus of the talk is Linux and I'm sure

00:02:32,640 --> 00:02:36,240
if you've been a Linux user for a long

00:02:34,410 --> 00:02:39,720
time you also realize that all linux

00:02:36,240 --> 00:02:41,250
distributions make a difference things

00:02:39,720 --> 00:02:46,320
are different in the Debian world how

00:02:41,250 --> 00:02:49,190
many here use a database system okay how

00:02:46,320 --> 00:02:56,489
many people here use an RPM based system

00:02:49,190 --> 00:03:00,660
most of the rest of you okay so you will

00:02:56,489 --> 00:03:03,200
see that even versions of Linux

00:03:00,660 --> 00:03:08,550
distributions tend to change things

00:03:03,200 --> 00:03:11,250
while rel 5 is still in support though

00:03:08,550 --> 00:03:12,870
very limited support or centos-5 so to

00:03:11,250 --> 00:03:17,720
speak you can also you know obviously

00:03:12,870 --> 00:03:21,000
use k 2 and k Thun Thun d as well as

00:03:17,720 --> 00:03:23,430
mattoon d that you see inside of rel 7

00:03:21,000 --> 00:03:25,110
it's all about tuning profiles and you

00:03:23,430 --> 00:03:27,709
can achieve this in debian as well by

00:03:25,110 --> 00:03:30,720
tuning things like Colonel scheduling

00:03:27,709 --> 00:03:33,239
granularities you can also change the

00:03:30,720 --> 00:03:35,430
scheduler yourself you can also you know

00:03:33,239 --> 00:03:38,220
play with the vm dirty ratios yourself

00:03:35,430 --> 00:03:41,700
and you can do things like you know set

00:03:38,220 --> 00:03:44,850
vm dirty background ratio to start

00:03:41,700 --> 00:03:46,470
flashing to disk as well as vm dirty

00:03:44,850 --> 00:03:49,820
ratio in general which obviously

00:03:46,470 --> 00:03:53,489
requires you to have synchronous i/o

00:03:49,820 --> 00:03:55,830
operations and this can all be given to

00:03:53,489 --> 00:03:58,650
you via monitoring things like proc

00:03:55,830 --> 00:04:01,470
vmstat so having some kind of monitoring

00:03:58,650 --> 00:04:02,970
in place for linux is extremely

00:04:01,470 --> 00:04:06,840
important to know how to get the most

00:04:02,970 --> 00:04:09,690
out of your OS and of course if you

00:04:06,840 --> 00:04:11,100
happen to use santo srl it becomes a

00:04:09,690 --> 00:04:13,230
little easier thanks to just having

00:04:11,100 --> 00:04:15,989
these preset profiles that you can use

00:04:13,230 --> 00:04:18,959
and get away with throughput performance

00:04:15,989 --> 00:04:21,900
on santa y7 seems to work pretty well

00:04:18,959 --> 00:04:23,190
for most database servers even if you

00:04:21,900 --> 00:04:26,690
happen to be running this database

00:04:23,190 --> 00:04:26,690
server inside of ec2

00:04:28,170 --> 00:04:35,070
so since many of you are doing bad metal

00:04:31,530 --> 00:04:37,440
stuff it's probably worth thinking that

00:04:35,070 --> 00:04:41,990
you know do you think about CPU memory

00:04:37,440 --> 00:04:44,820
and then I Oh database servers obviously

00:04:41,990 --> 00:04:47,010
very much more i/o bound than you would

00:04:44,820 --> 00:04:50,610
think and they require a lot more i/o

00:04:47,010 --> 00:04:53,820
capacity and memory and then only CPU

00:04:50,610 --> 00:04:55,860
but it turns out that today you can buy

00:04:53,820 --> 00:04:58,800
devices that can give you lots and lots

00:04:55,860 --> 00:05:01,530
of i/o like a fusion io device that can

00:04:58,800 --> 00:05:04,590
give you 120 thousand I 0 per second and

00:05:01,530 --> 00:05:07,890
then you'll find that your database

00:05:04,590 --> 00:05:10,710
actually cannot max use all that I oh so

00:05:07,890 --> 00:05:13,410
MySQL in the earlier days would maybe

00:05:10,710 --> 00:05:15,360
top out at 40,000 io per second and this

00:05:13,410 --> 00:05:17,730
is obviously improved over time and the

00:05:15,360 --> 00:05:19,980
way to work around this is of course to

00:05:17,730 --> 00:05:23,970
start up multiple instances and then

00:05:19,980 --> 00:05:25,650
make use of all your IO so we've we've

00:05:23,970 --> 00:05:27,600
reached a stage where I 0 is is

00:05:25,650 --> 00:05:29,220
important and you can now afford to get

00:05:27,600 --> 00:05:32,100
I oh that actually outperforms your

00:05:29,220 --> 00:05:35,370
database but obviously the next thing is

00:05:32,100 --> 00:05:38,610
memory that's also fairly important if

00:05:35,370 --> 00:05:40,290
you use something like Amazon you also

00:05:38,610 --> 00:05:43,620
are able to get something known as

00:05:40,290 --> 00:05:45,660
provisioned I ops or P I ops and that

00:05:43,620 --> 00:05:47,340
gives you provisioned I ops for up to

00:05:45,660 --> 00:05:49,470
three terabytes of storage and they will

00:05:47,340 --> 00:05:52,170
guarantee you that / database instance

00:05:49,470 --> 00:05:54,290
will get up to 30,000 I ups and this is

00:05:52,170 --> 00:05:58,200
depending on how much you'd like to pay

00:05:54,290 --> 00:06:00,810
as well now when it comes to things like

00:05:58,200 --> 00:06:03,720
queries and you're doing you know scans

00:06:00,810 --> 00:06:07,590
of large tables or even index cans or

00:06:03,720 --> 00:06:11,220
even doing changes to a row you will

00:06:07,590 --> 00:06:14,070
realize that changing sometimes even 256

00:06:11,220 --> 00:06:17,100
bytes in the row may actually call up an

00:06:14,070 --> 00:06:20,700
update of up to 8 kilobytes in page size

00:06:17,100 --> 00:06:23,880
this can actually be fairly sub optimal

00:06:20,700 --> 00:06:26,820
for you and depending on the kind of

00:06:23,880 --> 00:06:28,980
data you can now tune your inner DB page

00:06:26,820 --> 00:06:32,130
sizes previously you would only have 16

00:06:28,980 --> 00:06:35,850
kilobytes but with MySQL 57 and Maria

00:06:32,130 --> 00:06:38,460
db10 one you can tune this to basically

00:06:35,850 --> 00:06:40,740
go up to 64 kilobyte pages and the page

00:06:38,460 --> 00:06:41,849
sizes that you're using for DB should

00:06:40,740 --> 00:06:43,830
also probably match

00:06:41,849 --> 00:06:46,679
sizes that you're using on your file

00:06:43,830 --> 00:06:49,559
system generally speaking if you happen

00:06:46,679 --> 00:06:51,899
to be using this on something like ec2

00:06:49,559 --> 00:06:53,339
where you really that's the only one

00:06:51,899 --> 00:06:56,610
amazon is the only one that gives UPI

00:06:53,339 --> 00:06:58,110
ops we find that p provisioned I ops is

00:06:56,610 --> 00:07:00,119
much better than raiding things yourself

00:06:58,110 --> 00:07:03,300
because you can take your provisioned I

00:07:00,119 --> 00:07:05,939
ops up and down dynamically toss helping

00:07:03,300 --> 00:07:07,589
you save money the other thing that's

00:07:05,939 --> 00:07:09,240
probably important to remember is you

00:07:07,589 --> 00:07:11,219
always want to be durable when you're

00:07:09,240 --> 00:07:15,269
buying servers you tend to buy battery

00:07:11,219 --> 00:07:19,229
back RAID storage and today you can run

00:07:15,269 --> 00:07:21,389
your mysql in a fully durable mode which

00:07:19,229 --> 00:07:23,939
which basically means that after every

00:07:21,389 --> 00:07:27,419
transaction commits you should actually

00:07:23,939 --> 00:07:29,759
call F sync but I've seen calls on Linux

00:07:27,419 --> 00:07:31,830
and probably many other OSS extremely

00:07:29,759 --> 00:07:33,869
expensive which is why it actually

00:07:31,830 --> 00:07:37,889
reduces performance tremendously so

00:07:33,869 --> 00:07:40,289
since MySQL 56 and Maria db5 3 onwards

00:07:37,889 --> 00:07:41,729
there is what is known as group comment

00:07:40,289 --> 00:07:43,979
in the binary log so when you turn on

00:07:41,729 --> 00:07:46,649
the bin log and you turn on sync bit log

00:07:43,979 --> 00:07:48,479
equals 180 be flashy log a transaction

00:07:46,649 --> 00:07:50,610
committee call swan you actually don't

00:07:48,479 --> 00:07:52,199
see adverse effects because now instead

00:07:50,610 --> 00:07:54,779
now there's an algorithm that will

00:07:52,199 --> 00:07:57,269
actually just commit three of more

00:07:54,779 --> 00:07:59,490
parallel running queries inside as one F

00:07:57,269 --> 00:08:03,449
sync call as opposed to making 3 f's in

00:07:59,490 --> 00:08:05,189
calls or 10 f scene calls so again I

00:08:03,449 --> 00:08:09,589
think the most important thing here is

00:08:05,189 --> 00:08:09,589
to also use modern database software

00:08:11,509 --> 00:08:18,779
storage there's plenty of storage

00:08:15,059 --> 00:08:23,999
available today SAS discs are pretty

00:08:18,779 --> 00:08:27,149
common now if you happen to be using a

00:08:23,999 --> 00:08:30,360
spinning disk the benefits for this may

00:08:27,149 --> 00:08:32,639
be for streaming data there are some

00:08:30,360 --> 00:08:35,300
engines like scale DB that can benefit

00:08:32,639 --> 00:08:38,669
from you using large quantities of disk

00:08:35,300 --> 00:08:41,519
you may also happen to have some some

00:08:38,669 --> 00:08:45,029
some form of storage that is either wire

00:08:41,519 --> 00:08:46,889
Nazz or using something like drbd nasa's

00:08:45,029 --> 00:08:49,350
bring in the problems of having things

00:08:46,889 --> 00:08:51,209
like latency you have locking syncing

00:08:49,350 --> 00:08:53,779
issues so you definitely want to test

00:08:51,209 --> 00:08:53,779
your reliability

00:08:54,420 --> 00:09:01,570
overall SSDs SSDs I would definitely

00:08:58,810 --> 00:09:04,660
come back to later because i have a

00:09:01,570 --> 00:09:05,890
slight dedicated to that and then and

00:09:04,660 --> 00:09:09,310
this is also true for things like

00:09:05,890 --> 00:09:12,580
fusion-io and nvm FS or nvme based

00:09:09,310 --> 00:09:13,990
devices generally when you're measuring

00:09:12,580 --> 00:09:16,540
I oh you want to look at your

00:09:13,990 --> 00:09:18,550
performance numbers around I oh and

00:09:16,540 --> 00:09:21,339
sometimes you may see very good average

00:09:18,550 --> 00:09:24,940
performance but extremely lousy variants

00:09:21,339 --> 00:09:26,560
so some measure with granularity you

00:09:24,940 --> 00:09:29,709
don't want to be having you know spikes

00:09:26,560 --> 00:09:31,360
that go down up down because average

00:09:29,709 --> 00:09:32,830
performance may be good but the more

00:09:31,360 --> 00:09:34,450
granular you look at it you may actually

00:09:32,830 --> 00:09:38,529
be seeing that you're just doing nothing

00:09:34,450 --> 00:09:41,050
at points in time and generally speaking

00:09:38,529 --> 00:09:43,360
when it comes to i/o and storage

00:09:41,050 --> 00:09:45,220
efficiency you want to make sure you're

00:09:43,360 --> 00:09:47,260
also having a lot less right

00:09:45,220 --> 00:09:50,440
amplification to ensure your storage

00:09:47,260 --> 00:09:52,060
will last a lot longer than you expect i

00:09:50,440 --> 00:09:53,470
mean people buy hardware thinking

00:09:52,060 --> 00:09:55,630
they're going to last maybe three years

00:09:53,470 --> 00:09:57,640
but if you can guarantee five years

00:09:55,630 --> 00:10:00,760
about time that's actually extremely

00:09:57,640 --> 00:10:03,670
good and of course storage is also

00:10:00,760 --> 00:10:05,980
important from a space amplification

00:10:03,670 --> 00:10:09,100
perspective because you want to use less

00:10:05,980 --> 00:10:12,730
storage and more and more back-end

00:10:09,100 --> 00:10:14,890
storage engines today have have

00:10:12,730 --> 00:10:18,490
compression right so in ODB has

00:10:14,890 --> 00:10:20,110
compression toc pdb has compression if

00:10:18,490 --> 00:10:23,440
you happen to be using rocks DB that

00:10:20,110 --> 00:10:24,910
also has compression so many so you

00:10:23,440 --> 00:10:26,680
should be thinking about compression a

00:10:24,910 --> 00:10:34,930
lot especially when you're buying more

00:10:26,680 --> 00:10:38,350
expensive SSD or nvm ebays devices when

00:10:34,930 --> 00:10:41,860
it comes to schedulers or i/o elevators

00:10:38,350 --> 00:10:44,529
as they can be called you have things

00:10:41,860 --> 00:10:47,140
like the cfq which is a process q where

00:10:44,529 --> 00:10:49,360
each process will be will get a fixed

00:10:47,140 --> 00:10:52,600
time slice based on the priority of the

00:10:49,360 --> 00:10:55,839
process it is that it is generally the

00:10:52,600 --> 00:10:58,839
default their ships it's generally good

00:10:55,839 --> 00:11:00,880
for storage like SATA based disks so

00:10:58,839 --> 00:11:02,890
again you may use this when you end up

00:11:00,880 --> 00:11:05,450
using something like scale DB which is

00:11:02,890 --> 00:11:08,930
you know target the time series data

00:11:05,450 --> 00:11:13,250
there is also the no op which is aimed

00:11:08,930 --> 00:11:16,700
typically for SSD storage first in first

00:11:13,250 --> 00:11:19,910
out then this deadline which is

00:11:16,700 --> 00:11:22,660
something most database servers installs

00:11:19,910 --> 00:11:25,640
tend to end up using because this is

00:11:22,660 --> 00:11:27,800
this gives you two queues for device so

00:11:25,640 --> 00:11:29,600
you have 11 queue for reading and one

00:11:27,800 --> 00:11:32,180
queue for writing and the i/o is

00:11:29,600 --> 00:11:36,470
dispatched generally based on time spent

00:11:32,180 --> 00:11:39,470
inside of a query depending on what kind

00:11:36,470 --> 00:11:41,930
of linux you're using there may be an

00:11:39,470 --> 00:11:44,030
anticipatory scheduler which I believe

00:11:41,930 --> 00:11:46,970
has been removed down the line as well

00:11:44,030 --> 00:11:51,440
you probably want to forget that even

00:11:46,970 --> 00:11:54,710
exists generally you want to test and

00:11:51,440 --> 00:11:56,120
benchmark based on your workloads you

00:11:54,710 --> 00:11:58,430
obviously want to buy the best quality

00:11:56,120 --> 00:12:01,400
hardware you can afford so good quality

00:11:58,430 --> 00:12:06,650
right controller and obviously battery

00:12:01,400 --> 00:12:08,450
back makes a lot of sense yeah so again

00:12:06,650 --> 00:12:10,520
if you use something like k2 nantoon d

00:12:08,450 --> 00:12:13,280
the it has profiles that will actually

00:12:10,520 --> 00:12:14,990
turn it on failing which you can just

00:12:13,280 --> 00:12:19,550
turn it on by doing at boots I'm

00:12:14,990 --> 00:12:22,880
elevator a equals deadline you can also

00:12:19,550 --> 00:12:24,530
turn it on by just echo and generally

00:12:22,880 --> 00:12:27,170
why we say deadline at least in the

00:12:24,530 --> 00:12:29,660
MySQL world we see that cfq will

00:12:27,170 --> 00:12:32,000
serialize right operations whereas

00:12:29,660 --> 00:12:35,120
deadlines or even know up will continue

00:12:32,000 --> 00:12:37,220
to scale with growing workloads so the

00:12:35,120 --> 00:12:40,460
more threads you have it actually

00:12:37,220 --> 00:12:42,140
continues scaling and the MySQL model

00:12:40,460 --> 00:12:45,380
has always been one thread per

00:12:42,140 --> 00:12:47,030
connection so yes we do have things like

00:12:45,380 --> 00:12:49,160
a thread pool nowadays but you still

00:12:47,030 --> 00:12:51,470
open up many many threads and you will

00:12:49,160 --> 00:12:59,920
have many many connections in production

00:12:51,470 --> 00:13:02,990
presumably so a simple elevator

00:12:59,920 --> 00:13:04,490
benchmark where the first one had inner

00:13:02,990 --> 00:13:07,580
DB flash log a transaction commits

00:13:04,490 --> 00:13:09,980
equals zero and this is when you when we

00:13:07,580 --> 00:13:16,330
used MySQL without group commit so it is

00:13:09,980 --> 00:13:18,110
done against a MySQL 55 instance and

00:13:16,330 --> 00:13:19,400
obviously this has improved tremendously

00:13:18,110 --> 00:13:21,500
now in the

00:13:19,400 --> 00:13:23,990
that you should see zero difference when

00:13:21,500 --> 00:13:27,920
using the deadline with or without

00:13:23,990 --> 00:13:29,210
transaction commits equals zero but you

00:13:27,920 --> 00:13:31,460
will actually notice that there's a huge

00:13:29,210 --> 00:13:35,360
difference between the first and the

00:13:31,460 --> 00:13:37,750
third so deadline for all intents and

00:13:35,360 --> 00:13:41,780
purposes is exactly what you're after

00:13:37,750 --> 00:13:44,390
today and this was done with eight disc

00:13:41,780 --> 00:13:50,630
16 gigs of ram and it caused so fairly

00:13:44,390 --> 00:13:54,260
old oldish machine when it comes to file

00:13:50,630 --> 00:13:58,430
systems you probably want to choose

00:13:54,260 --> 00:14:01,730
something like EXT 4 or X of S they're

00:13:58,430 --> 00:14:05,890
very safe bets and they have many other

00:14:01,730 --> 00:14:08,000
people using these file systems at scale

00:14:05,890 --> 00:14:11,900
one file system you definitely don't

00:14:08,000 --> 00:14:14,840
want to use is hfs+ if you happen to

00:14:11,900 --> 00:14:18,560
have lots of money and buy Mac servers

00:14:14,840 --> 00:14:23,120
or Mac minis hfs+ is not right for most

00:14:18,560 --> 00:14:25,010
database workloads they're obviously

00:14:23,120 --> 00:14:27,260
mount options that you may think about

00:14:25,010 --> 00:14:30,050
using and it's important to remember

00:14:27,260 --> 00:14:32,090
that I owe barriers I used to basically

00:14:30,050 --> 00:14:33,830
make sure that the journal will write to

00:14:32,090 --> 00:14:36,230
your is actually written to the file

00:14:33,830 --> 00:14:38,960
system so the advantage of this is

00:14:36,230 --> 00:14:41,150
naturally that your data is safe but

00:14:38,960 --> 00:14:43,580
then you think about the fact that you

00:14:41,150 --> 00:14:47,810
don't actually end up using caches from

00:14:43,580 --> 00:14:50,390
that as well so quick benchmarks so when

00:14:47,810 --> 00:14:54,740
you do a very large dump of nearly two

00:14:50,390 --> 00:14:57,560
million sq L rows with various enabled

00:14:54,740 --> 00:15:00,170
you'll see something like 500 27 seconds

00:14:57,560 --> 00:15:01,550
for that but if you turn off barriers so

00:15:00,170 --> 00:15:03,620
you took you bond with the no Barry

00:15:01,550 --> 00:15:06,440
option you'll actually notice this takes

00:15:03,620 --> 00:15:09,430
something like 21 seconds so you can

00:15:06,440 --> 00:15:12,590
actually test this yourself as well and

00:15:09,430 --> 00:15:16,130
when you look at in ODB's flashing rate

00:15:12,590 --> 00:15:17,330
you'll see that it can go you can vary

00:15:16,130 --> 00:15:19,670
from you know something like 80

00:15:17,330 --> 00:15:22,730
megabytes a second instead of four

00:15:19,670 --> 00:15:25,970
megabytes a second so just having the

00:15:22,730 --> 00:15:27,830
barrier and the no Barry option in your

00:15:25,970 --> 00:15:31,250
in your test can be quite useful for

00:15:27,830 --> 00:15:33,279
this sort of purpose you can keep your

00:15:31,250 --> 00:15:35,690
data and indexes on a single volume

00:15:33,279 --> 00:15:37,639
but if you but if you think that

00:15:35,690 --> 00:15:39,560
separating things make sense you can

00:15:37,639 --> 00:15:41,950
separate your binary logs as well as

00:15:39,560 --> 00:15:44,300
your your data files if you want to and

00:15:41,950 --> 00:15:46,760
and this is especially useful in case

00:15:44,300 --> 00:15:48,920
you end up losing a disk but generally

00:15:46,760 --> 00:15:51,860
speaking I would just keep the data and

00:15:48,920 --> 00:15:53,630
indexes on a single volume and this is

00:15:51,860 --> 00:15:55,370
probably important also if you end up

00:15:53,630 --> 00:15:58,010
using talki DB which actually creates

00:15:55,370 --> 00:16:01,100
many many many many files inside of a

00:15:58,010 --> 00:16:06,220
directory as opposed to inner DB where

00:16:01,100 --> 00:16:06,220
now the default is one file per table

00:16:06,730 --> 00:16:11,240
definitely you you should know all your

00:16:09,320 --> 00:16:16,220
mount options mount options change over

00:16:11,240 --> 00:16:18,079
time as well ZFS is another cool file

00:16:16,220 --> 00:16:21,740
system that is just started shipping in

00:16:18,079 --> 00:16:24,500
the in ubuntu maybe last week it's very

00:16:21,740 --> 00:16:26,600
familiar for the bsd folk in fact they

00:16:24,500 --> 00:16:30,560
are even tools around the postgres of

00:16:26,600 --> 00:16:33,350
all that depend on ZFS we don't see many

00:16:30,560 --> 00:16:35,690
people and using that FS in production

00:16:33,350 --> 00:16:37,370
besides the bsd folk because you know

00:16:35,690 --> 00:16:40,449
it's for the longest time we're not sure

00:16:37,370 --> 00:16:43,730
how legal it is to be in the linux world

00:16:40,449 --> 00:16:45,399
but like i said the postgres tools that

00:16:43,730 --> 00:16:48,560
can benefit from it you can use

00:16:45,399 --> 00:16:50,870
Zarephath snapshot so my LVN backup will

00:16:48,560 --> 00:16:55,070
actually also generally be able to

00:16:50,870 --> 00:16:56,930
create ZFS snapshots for you but in

00:16:55,070 --> 00:17:00,019
linux we've yet to really see much

00:16:56,930 --> 00:17:02,320
production use of this and it's probably

00:17:00,019 --> 00:17:07,939
also worth mentioning butter FS is

00:17:02,320 --> 00:17:12,770
another fairly interesting file system

00:17:07,939 --> 00:17:15,890
kind of based on ZFS but again we don't

00:17:12,770 --> 00:17:17,569
see many people actually running this in

00:17:15,890 --> 00:17:19,339
production we know people having test

00:17:17,569 --> 00:17:21,380
test workloads but definitely not

00:17:19,339 --> 00:17:23,360
production workloads so if you are going

00:17:21,380 --> 00:17:26,689
to play with more modern file systems

00:17:23,360 --> 00:17:30,049
that are not ext4 or XFS maybe you will

00:17:26,689 --> 00:17:36,470
take good backups that you know you can

00:17:30,049 --> 00:17:40,640
restore yeah and of course you can you

00:17:36,470 --> 00:17:44,140
know use iostat to to measure the i/o

00:17:40,640 --> 00:17:44,140
operations on your file system

00:17:45,140 --> 00:17:52,950
now when it comes to SSD and flash

00:17:50,299 --> 00:17:55,470
you'll actually realize that the older

00:17:52,950 --> 00:17:57,690
hardware raid controllers were more or

00:17:55,470 --> 00:17:59,669
less optimized to reduce random i/o and

00:17:57,690 --> 00:18:02,549
you generally do not expect this that

00:17:59,669 --> 00:18:04,500
support so many I ups but today you

00:18:02,549 --> 00:18:07,230
obviously have much better raid

00:18:04,500 --> 00:18:11,220
controllers so running something modern

00:18:07,230 --> 00:18:15,539
also makes sense as these obviously give

00:18:11,220 --> 00:18:17,039
you a very low latency and flash flash

00:18:15,539 --> 00:18:18,840
is getting a lot more popular right I

00:18:17,039 --> 00:18:22,559
mean you're getting a lot more I ups

00:18:18,840 --> 00:18:25,200
than SSD it's got a lot lot less latency

00:18:22,559 --> 00:18:27,150
compared to SSD I mean I think even if

00:18:25,200 --> 00:18:28,919
you buy a mac laptop today you're

00:18:27,150 --> 00:18:32,309
basically getting flash as opposed to

00:18:28,919 --> 00:18:34,710
SSD now the flash obviously varies in

00:18:32,309 --> 00:18:37,799
type in quantity and in type of quality

00:18:34,710 --> 00:18:39,440
I'll flash so it's well worth taking a

00:18:37,799 --> 00:18:42,150
look at the type of flash are getting

00:18:39,440 --> 00:18:44,400
fusion-io probably brought the first

00:18:42,150 --> 00:18:46,950
generation of flash devices that you

00:18:44,400 --> 00:18:50,030
would buy for server use but now you can

00:18:46,950 --> 00:18:52,740
get plenty of them from the nvm Express

00:18:50,030 --> 00:18:57,870
non-volatile memory is basically what

00:18:52,740 --> 00:19:00,720
flashes fusion-io actually we've done

00:18:57,870 --> 00:19:03,390
work with them and modern versions of

00:19:00,720 --> 00:19:06,240
MySQL Maria DB as well as pakona server

00:19:03,390 --> 00:19:08,039
actually allowed to disable the innodb

00:19:06,240 --> 00:19:09,630
double write buffer I mean the rubble

00:19:08,039 --> 00:19:13,350
write buffer works as advertised it

00:19:09,630 --> 00:19:15,600
basically writes twice but this

00:19:13,350 --> 00:19:18,059
obviously increases the amplification on

00:19:15,600 --> 00:19:20,270
in terms of rights but if you use

00:19:18,059 --> 00:19:22,350
fusion-io because it has this direct FS

00:19:20,270 --> 00:19:24,480
capability it allows you to just write

00:19:22,350 --> 00:19:25,980
once so you can disable the double write

00:19:24,480 --> 00:19:27,900
buffer so we will actually we will

00:19:25,980 --> 00:19:30,030
detect it upon startup and disable the

00:19:27,900 --> 00:19:31,860
double write buffer but this also means

00:19:30,030 --> 00:19:34,260
that you have to be running what is

00:19:31,860 --> 00:19:36,840
known as direct FS a fusion io based

00:19:34,260 --> 00:19:39,260
file system which presumably nobody here

00:19:36,840 --> 00:19:42,320
is running this anybody run directive s

00:19:39,260 --> 00:19:48,320
no does anybody have fusion-io devices

00:19:42,320 --> 00:19:50,880
okay and you're running XFS on them okay

00:19:48,320 --> 00:19:52,350
yeah so more or less people are buying

00:19:50,880 --> 00:19:54,750
fusion-io devices but they are

00:19:52,350 --> 00:19:57,960
definitely not running direct FS because

00:19:54,750 --> 00:19:58,590
nobody likes running better file system

00:19:57,960 --> 00:20:03,179
so I'm guessing

00:19:58,590 --> 00:20:05,010
in production when it comes to using

00:20:03,179 --> 00:20:06,600
things like SSDs in flash as I mentioned

00:20:05,010 --> 00:20:08,250
earlier as well you you will now start

00:20:06,600 --> 00:20:12,510
thinking quite heavily about compression

00:20:08,250 --> 00:20:14,309
also one thing worth mentioning is that

00:20:12,510 --> 00:20:18,600
the double write buffer obviously rights

00:20:14,309 --> 00:20:21,600
twice but Percona in their release of

00:20:18,600 --> 00:20:23,370
MySQL 57 has introduced something new as

00:20:21,600 --> 00:20:24,539
well so if you end up using percona

00:20:23,370 --> 00:20:26,250
server you'll see that there's something

00:20:24,539 --> 00:20:29,580
known as a parallel double write buffer

00:20:26,250 --> 00:20:32,789
where you actually get a private buffer

00:20:29,580 --> 00:20:35,460
pool for each and every instance so you

00:20:32,789 --> 00:20:37,500
like the regular double write buffer is

00:20:35,460 --> 00:20:38,909
is shared between all the buffer pool

00:20:37,500 --> 00:20:43,250
instances and all the flash' threads

00:20:38,909 --> 00:20:46,500
inside of inner DB but having something

00:20:43,250 --> 00:20:48,450
you know private may actually be will

00:20:46,500 --> 00:20:51,059
actually possibly boost performance for

00:20:48,450 --> 00:20:55,350
you and this is currently only available

00:20:51,059 --> 00:20:57,179
in percona server 57 from maybe two or

00:20:55,350 --> 00:21:00,450
three months ago so it's still

00:20:57,179 --> 00:21:11,010
relatively new it's not made its way to

00:21:00,450 --> 00:21:13,679
any other mysql already be yet oh yeah

00:21:11,010 --> 00:21:16,200
so always remember to look at the size

00:21:13,679 --> 00:21:19,080
of page divide that by the size of rope

00:21:16,200 --> 00:21:21,720
and you have a fairly good understanding

00:21:19,080 --> 00:21:24,179
of what the right back of the page would

00:21:21,720 --> 00:21:27,059
be when it comes to even modifying rose

00:21:24,179 --> 00:21:36,360
and this is again where you may want to

00:21:27,059 --> 00:21:39,360
then change your page sizes for nodb up

00:21:36,360 --> 00:21:43,710
here I did not include raid 6 or 8 50

00:21:39,360 --> 00:21:47,730
and generally speaking you probably will

00:21:43,710 --> 00:21:49,409
end up having raids for your database

00:21:47,730 --> 00:21:52,350
server because you care for the data

00:21:49,409 --> 00:21:55,350
underneath it and this is obviously true

00:21:52,350 --> 00:21:58,409
for MySQL this is also probably true

00:21:55,350 --> 00:22:00,840
very much for postgres but it is it is

00:21:58,409 --> 00:22:05,520
definitely not true if you end up having

00:22:00,840 --> 00:22:07,440
to run HDFS a Hadoop we always say raid

00:22:05,520 --> 00:22:09,809
your database service but you don't do

00:22:07,440 --> 00:22:12,179
that for Hadoop because the nodes it

00:22:09,809 --> 00:22:14,340
nodes should never ever run as rate I do

00:22:12,179 --> 00:22:17,039
itself actually orchestrates data

00:22:14,340 --> 00:22:20,369
redundancy across all of its nodes so

00:22:17,039 --> 00:22:21,899
the only nodes that should actually end

00:22:20,369 --> 00:22:24,240
up having raid maybe Hadoop's

00:22:21,899 --> 00:22:27,809
masternodes but never any of its other

00:22:24,240 --> 00:22:30,029
nodes so the whole advice of raiding

00:22:27,809 --> 00:22:31,799
your database servers that you will see

00:22:30,029 --> 00:22:33,869
commonly on the internet will probably

00:22:31,799 --> 00:22:38,460
not apply on Hadoop workloads and this

00:22:33,869 --> 00:22:40,019
is generally true for a lot of bits and

00:22:38,460 --> 00:22:43,139
pieces of advice you end up finding on

00:22:40,019 --> 00:22:45,509
internet because sometimes it works for

00:22:43,139 --> 00:22:47,580
one person and it may not work for you

00:22:45,509 --> 00:22:54,690
or it may not work for most people but

00:22:47,580 --> 00:23:02,970
they have really good cuckoo juice so do

00:22:54,690 --> 00:23:05,309
people here use lvm okay so lvm is great

00:23:02,970 --> 00:23:07,740
because obviously not only do you easily

00:23:05,309 --> 00:23:10,020
expand your disk by adding more physical

00:23:07,740 --> 00:23:13,740
disks your son

00:23:10,020 --> 00:23:15,720
getting snapshot backups which is

00:23:13,740 --> 00:23:18,540
extremely useful in the MySQL world

00:23:15,720 --> 00:23:20,310
though generally now you are wondering

00:23:18,540 --> 00:23:22,440
why do you really need snapshot backups

00:23:20,310 --> 00:23:24,390
you can already do logical backups at my

00:23:22,440 --> 00:23:27,090
SQL dump we can do physical backups at

00:23:24,390 --> 00:23:29,490
extra backup but maybe you're using

00:23:27,090 --> 00:23:34,200
engines that don't use either of this

00:23:29,490 --> 00:23:35,940
and then you end up using lvm but they

00:23:34,200 --> 00:23:40,320
are obviously slapshot penalties with

00:23:35,940 --> 00:23:42,150
lvm each extra lvm snapshot will give

00:23:40,320 --> 00:23:47,810
you a two to three percent performance

00:23:42,150 --> 00:23:51,750
right penalty but your first snapshot

00:23:47,810 --> 00:23:54,150
depending on the way your data is

00:23:51,750 --> 00:23:56,790
structured can give you up to a forty

00:23:54,150 --> 00:23:59,330
percent loss in panel in performance

00:23:56,790 --> 00:24:04,080
penalty this is actually pretty huge and

00:23:59,330 --> 00:24:06,990
a good way to test this is to go and try

00:24:04,080 --> 00:24:10,730
RDS you can you can fire up one RDS and

00:24:06,990 --> 00:24:13,470
you can fly up one RDS multi a multi AZ

00:24:10,730 --> 00:24:16,580
or you can do this in ec to basically

00:24:13,470 --> 00:24:19,170
yourself and you'll actually see

00:24:16,580 --> 00:24:23,850
snapshot penalties are actually quite

00:24:19,170 --> 00:24:26,970
high Hadoop again does not matter

00:24:23,850 --> 00:24:28,920
recommend that you use Mel vm because it

00:24:26,970 --> 00:24:33,960
has latency and it really is a

00:24:28,920 --> 00:24:35,850
bottleneck taki DB this was used to be

00:24:33,960 --> 00:24:38,760
the best way to backup taki DB because

00:24:35,850 --> 00:24:41,760
hey you didn't have an enterprise backup

00:24:38,760 --> 00:24:44,070
tool that you wanted to pay for but talk

00:24:41,760 --> 00:24:48,080
to you backup is now also open source so

00:24:44,070 --> 00:24:48,080
again maybe you don't need to use lvm

00:24:48,560 --> 00:24:54,210
MongoDB so a famous use case for this

00:24:51,630 --> 00:24:56,610
was for the pars team they actually

00:24:54,210 --> 00:25:01,080
create consistent snapshots and they use

00:24:56,610 --> 00:25:03,060
EBS snapshots for mongodb so it's worth

00:25:01,080 --> 00:25:05,340
noting that when you do an lvm snapshot

00:25:03,060 --> 00:25:09,240
you will actually lock your manga d

00:25:05,340 --> 00:25:11,970
instance so if you use lvm on top of

00:25:09,240 --> 00:25:13,800
something like EBS you may end up

00:25:11,970 --> 00:25:17,250
realizing that you can do things like

00:25:13,800 --> 00:25:20,520
pre warming after a snapshot has

00:25:17,250 --> 00:25:22,860
happened so always remember that lvm is

00:25:20,520 --> 00:25:23,490
useful but it doesn't come for free so

00:25:22,860 --> 00:25:26,070
take it

00:25:23,490 --> 00:25:31,740
all snapshot penalties measure your

00:25:26,070 --> 00:25:35,790
snapshot penalties now when it comes to

00:25:31,740 --> 00:25:38,040
memory I'm sure many of you use modern

00:25:35,790 --> 00:25:40,470
servers that have that there are new

00:25:38,040 --> 00:25:43,170
Mackay pible generally you should have

00:25:40,470 --> 00:25:44,429
at least the last didn't the next five

00:25:43,170 --> 00:25:47,850
minutes of your working set sitting

00:25:44,429 --> 00:25:50,070
inside of memory and you know Numa is

00:25:47,850 --> 00:25:52,410
where memories addressed on a per CPU

00:25:50,070 --> 00:25:54,140
basis so your local operations on a CPU

00:25:52,410 --> 00:25:59,790
will obviously cause lots of swapping

00:25:54,140 --> 00:26:02,640
even if the memory is available Numa

00:25:59,790 --> 00:26:05,460
interleaving is something that Oracle

00:26:02,640 --> 00:26:08,600
the database actually has but for a very

00:26:05,460 --> 00:26:12,090
long time mysql MongoDB didn't have so

00:26:08,600 --> 00:26:16,140
when you run today MySQL on a fairly

00:26:12,090 --> 00:26:18,630
large system where Valley Lodge is maybe

00:26:16,140 --> 00:26:21,300
you have 64 gigs of RAM which is maybe

00:26:18,630 --> 00:26:23,370
not so large to you now you may want to

00:26:21,300 --> 00:26:27,620
obviously have new more control in to

00:26:23,370 --> 00:26:30,990
leave equals all and then startup mysqld

00:26:27,620 --> 00:26:32,910
this is especially true when you all

00:26:30,990 --> 00:26:34,890
right let's again say you have 64

00:26:32,910 --> 00:26:37,170
gigabytes of RAM and you have a fairly

00:26:34,890 --> 00:26:39,390
large innodb buffer pool maybe forty

00:26:37,170 --> 00:26:41,520
eight gigabytes over time linux will

00:26:39,390 --> 00:26:45,090
actually decide to swap out large

00:26:41,520 --> 00:26:46,350
amounts of your memory and appear that

00:26:45,090 --> 00:26:53,490
it's under memory pressure despite

00:26:46,350 --> 00:26:56,010
actually having no memory pressure in

00:26:53,490 --> 00:26:58,010
Postgres they have a solution for this

00:26:56,010 --> 00:27:00,600
it's called the zone reclaim mode

00:26:58,010 --> 00:27:02,790
postgres actually allows you to you can

00:27:00,600 --> 00:27:05,070
actually just set vm zone reclaim mode

00:27:02,790 --> 00:27:09,480
equals 0 and postgres will continue

00:27:05,070 --> 00:27:11,550
working as well but in the mysql world

00:27:09,480 --> 00:27:14,100
this is what you're looking for and to

00:27:11,550 --> 00:27:17,960
be fair percona server 56 onwards as

00:27:14,100 --> 00:27:21,270
well as maria DB 10 1 onwards actually

00:27:17,960 --> 00:27:23,610
has functionality to control your

00:27:21,270 --> 00:27:25,260
service when you are using Numa so that

00:27:23,610 --> 00:27:29,190
you don't actually end up having swap

00:27:25,260 --> 00:27:30,570
insanity so to speak and this is this is

00:27:29,190 --> 00:27:32,010
important because you you know you're

00:27:30,570 --> 00:27:33,540
getting on what you're getting a modern

00:27:32,010 --> 00:27:36,059
server and then you're finding out that

00:27:33,540 --> 00:27:38,600
it doesn't perform as well as may be an

00:27:36,059 --> 00:27:38,600
old server

00:27:39,740 --> 00:27:46,610
mysql for example uses memory for things

00:27:43,110 --> 00:27:50,160
like session buffers it uses things for

00:27:46,610 --> 00:27:51,780
statistics inside of metadata in locking

00:27:50,160 --> 00:27:53,340
stats as well and if you use things like

00:27:51,780 --> 00:27:55,200
the user starts it actually counts

00:27:53,340 --> 00:27:58,200
indexed adds user starts client starts

00:27:55,200 --> 00:28:05,460
and so forth obviously MySQL else uses

00:27:58,200 --> 00:28:08,040
it to cache data when it comes to where

00:28:05,460 --> 00:28:10,350
else its memory used you'll find that

00:28:08,040 --> 00:28:12,840
with regards to MySQL you have obviously

00:28:10,350 --> 00:28:15,500
got the idea of having a binary and

00:28:12,840 --> 00:28:18,810
relay log so memory gets used there

00:28:15,500 --> 00:28:20,700
mysql completely real my eyes am if you

00:28:18,810 --> 00:28:22,380
are still using it for whatever reason

00:28:20,700 --> 00:28:26,220
will actually completely rely on the

00:28:22,380 --> 00:28:27,780
file system cache so it uses the OS as

00:28:26,220 --> 00:28:30,810
opposed to creating its own buffer pool

00:28:27,780 --> 00:28:33,240
if you use postgres it has the concept

00:28:30,810 --> 00:28:35,850
of having a right a right to head log

00:28:33,240 --> 00:28:38,310
and well so getting more and more common

00:28:35,850 --> 00:28:40,110
in the evenin the MySQL especially if

00:28:38,310 --> 00:28:43,410
you start using something like my rocks

00:28:40,110 --> 00:28:47,940
and rocks DB they also have the concept

00:28:43,410 --> 00:28:49,980
of a writer headlock and of course you

00:28:47,940 --> 00:28:51,690
can do something like free dash mm to

00:28:49,980 --> 00:28:54,600
find out how much memory you actually

00:28:51,690 --> 00:28:56,580
really have show engine innodb status is

00:28:54,600 --> 00:28:58,290
obviously going away in the future so

00:28:56,580 --> 00:29:01,200
you will not have to make use of what is

00:28:58,290 --> 00:29:04,010
known as the performance schema inside

00:29:01,200 --> 00:29:06,690
of mysql 57 and going forward

00:29:04,010 --> 00:29:09,240
performance schema also does not come

00:29:06,690 --> 00:29:12,150
for free all that instrumentation

00:29:09,240 --> 00:29:14,820
actually will give you some actual use

00:29:12,150 --> 00:29:16,650
of loss in performance and this can be

00:29:14,820 --> 00:29:18,150
anywhere between three five seven

00:29:16,650 --> 00:29:20,610
percent it's been measured differently

00:29:18,150 --> 00:29:24,110
by different folk so again you

00:29:20,610 --> 00:29:24,110
definitely want to look at that

00:29:26,900 --> 00:29:32,730
transparent huge pages so if you have an

00:29:30,390 --> 00:29:35,340
app like a database that has a lot of

00:29:32,730 --> 00:29:37,230
memory access and you may actually

00:29:35,340 --> 00:29:39,840
realize you can obtain improvements by

00:29:37,230 --> 00:29:42,120
using very large pages because you

00:29:39,840 --> 00:29:47,420
actually reduce the looking inside the

00:29:42,120 --> 00:29:49,830
TLB the translation lookaside buffer and

00:29:47,420 --> 00:29:51,800
you actually access more of this data

00:29:49,830 --> 00:29:54,810
inside your waffle

00:29:51,800 --> 00:29:59,550
solaris for example has extremely large

00:29:54,810 --> 00:30:01,560
pages 256 megs that are not mistaken but

00:29:59,550 --> 00:30:04,140
love Linux this more or less means that

00:30:01,560 --> 00:30:06,870
you end up handing anywhere between two

00:30:04,140 --> 00:30:12,810
to Meg's or one gigabyte of a page on a

00:30:06,870 --> 00:30:16,380
malach and people set huge pages to via

00:30:12,810 --> 00:30:21,060
vm and our huge pages mysql generally

00:30:16,380 --> 00:30:25,590
may benefit from huge pages but MongoDB

00:30:21,060 --> 00:30:27,780
doesn't neither does reddy's now I would

00:30:25,590 --> 00:30:31,140
personally turn off transparent huge

00:30:27,780 --> 00:30:34,080
pages because fuel on say even save BSD

00:30:31,140 --> 00:30:37,680
this is not something you you're good at

00:30:34,080 --> 00:30:41,400
doing and if you use an engine like taco

00:30:37,680 --> 00:30:44,280
DB or even talk ukv memory tracking

00:30:41,400 --> 00:30:47,220
actually does not work which will

00:30:44,280 --> 00:30:50,250
actually lead to overcome it and why why

00:30:47,220 --> 00:30:52,470
does this happen because they don't use

00:30:50,250 --> 00:30:55,790
the standard matlock library that you

00:30:52,470 --> 00:31:00,950
are familiar with they use je malach and

00:30:55,790 --> 00:31:03,600
jml our calls up m advice instead of of

00:31:00,950 --> 00:31:05,190
just regular malach so the

00:31:03,600 --> 00:31:08,430
implementation of dealing with memory

00:31:05,190 --> 00:31:09,930
fragmentation inside of j Malik will

00:31:08,430 --> 00:31:13,080
actually mean that you need to turn off

00:31:09,930 --> 00:31:16,280
transparent huge pages now this this

00:31:13,080 --> 00:31:18,420
brings up a very interesting difference

00:31:16,280 --> 00:31:21,420
when it comes to using something like

00:31:18,420 --> 00:31:24,330
Maria DB a modern Percona server that

00:31:21,420 --> 00:31:26,330
ships with Aki DB and mysql because if

00:31:24,330 --> 00:31:29,010
you're running just stock Oracle MySQL

00:31:26,330 --> 00:31:31,530
you can actually have transparent huge

00:31:29,010 --> 00:31:34,170
pages on but if you're running maria DB

00:31:31,530 --> 00:31:36,270
or modern pakona server you'll actually

00:31:34,170 --> 00:31:38,460
have to turn this off because of the

00:31:36,270 --> 00:31:42,810
taki DB engine that is included inside

00:31:38,460 --> 00:31:46,110
of these servers jayma lock will also

00:31:42,810 --> 00:31:50,030
bring other fun issues like it makes it

00:31:46,110 --> 00:31:53,010
extremely hard to work on freebsd and

00:31:50,030 --> 00:31:55,020
there are open bugs to fix this it's

00:31:53,010 --> 00:31:57,560
more the malach library that needs to be

00:31:55,020 --> 00:31:57,560
ported

00:31:59,100 --> 00:32:05,880
now when it comes to sloppiness the

00:32:03,450 --> 00:32:08,549
internet is all for saying turn off swap

00:32:05,880 --> 00:32:11,190
when you have a database server that's

00:32:08,549 --> 00:32:13,020
all fine and dandy except that in most

00:32:11,190 --> 00:32:15,299
modern Linux's this will also mean that

00:32:13,020 --> 00:32:17,340
the other memory killer will come and it

00:32:15,299 --> 00:32:19,289
will kill your database server then

00:32:17,340 --> 00:32:23,070
you'll have to start it up again and all

00:32:19,289 --> 00:32:24,630
your buffers will be cold so vm

00:32:23,070 --> 00:32:26,610
sloppiness equals zero actually will

00:32:24,630 --> 00:32:31,169
call the OM killer and it turns out it

00:32:26,610 --> 00:32:32,880
was a bug in the colonel so if you see

00:32:31,169 --> 00:32:34,350
if you actually happen to run in a DB

00:32:32,880 --> 00:32:36,539
flash lakay transaction commits equals

00:32:34,350 --> 00:32:38,370
zero because you thought that it would

00:32:36,539 --> 00:32:41,010
give you better performance you would

00:32:38,370 --> 00:32:42,630
actually lose uncommitted transactions

00:32:41,010 --> 00:32:46,679
or transactions that were not committed

00:32:42,630 --> 00:32:50,940
to the log yet and this this is

00:32:46,679 --> 00:32:54,510
extremely bad so basically the to get

00:32:50,940 --> 00:32:57,419
the old method of VM sloppiness equals

00:32:54,510 --> 00:33:01,320
zero back again the new way to do it is

00:32:57,419 --> 00:33:06,080
to say vm sloppiness equals one the

00:33:01,320 --> 00:33:08,429
default if Ilan mistaken is set to 60 so

00:33:06,080 --> 00:33:13,230
basically if you have a modern kernel

00:33:08,429 --> 00:33:15,590
version 3.5 hour or thanks to set to ral

00:33:13,230 --> 00:33:18,210
they back both of it also to a 26 kernel

00:33:15,590 --> 00:33:20,640
you definitely want to make sure that

00:33:18,210 --> 00:33:22,890
you have vm sloppiness equals one to

00:33:20,640 --> 00:33:24,630
ensure that you get the minimum amount

00:33:22,890 --> 00:33:26,700
of swapping without disabling it

00:33:24,630 --> 00:33:30,480
entirely this is extremely important

00:33:26,700 --> 00:33:32,700
otherwise you will have servers you will

00:33:30,480 --> 00:33:35,340
have mysqld that just disappears and you

00:33:32,700 --> 00:33:36,960
will be extremely unhappy especially if

00:33:35,340 --> 00:33:40,169
you are running with a 48 giga byte

00:33:36,960 --> 00:33:41,850
buffer pool and it just went 2-0 warming

00:33:40,169 --> 00:33:44,250
that up could take quite some time

00:33:41,850 --> 00:33:48,179
though now there are multiple tricks

00:33:44,250 --> 00:33:52,650
that you can use liars a interest schema

00:33:48,179 --> 00:33:55,130
file replication in 57 or even Maria DB

00:33:52,650 --> 00:33:55,130
10 0

00:33:57,160 --> 00:34:02,960
for when it comes to CPUs you definitely

00:34:00,530 --> 00:34:05,570
you can definitely set it to performance

00:34:02,960 --> 00:34:07,970
and if you use tune dear admin it'll

00:34:05,570 --> 00:34:10,010
actually set it to the floor burns by by

00:34:07,970 --> 00:34:11,750
default you probably want to disable

00:34:10,010 --> 00:34:15,770
power save mode because you're not

00:34:11,750 --> 00:34:18,860
running this on a laptop you need to

00:34:15,770 --> 00:34:21,860
install CPU Frick utils which is not

00:34:18,860 --> 00:34:25,190
necessarily part of the distribution

00:34:21,860 --> 00:34:27,380
installed by default and you definitely

00:34:25,190 --> 00:34:29,510
want to configure your BIOS and biases

00:34:27,380 --> 00:34:31,840
are changing nowadays you also have all

00:34:29,510 --> 00:34:34,430
these graphical biases so make sure you

00:34:31,840 --> 00:34:40,460
ensure that you hand over power control

00:34:34,430 --> 00:34:41,900
to do s when we're doing lots of testing

00:34:40,460 --> 00:34:44,360
especially when it comes to galera

00:34:41,900 --> 00:34:47,450
cluster or application we end up using a

00:34:44,360 --> 00:34:48,920
lot of TC because you can actually show

00:34:47,450 --> 00:34:51,920
as well as change traffic control

00:34:48,920 --> 00:34:56,330
settings this is a great tool for 4 from

00:34:51,920 --> 00:34:59,210
a benchmarking standpoint you definitely

00:34:56,330 --> 00:35:03,860
can also use drop watch which is a

00:34:59,210 --> 00:35:06,230
kernel drop packet monitor you can

00:35:03,860 --> 00:35:10,430
afford the best ethernet so use use the

00:35:06,230 --> 00:35:12,140
best so you get more bandwidth trunking

00:35:10,430 --> 00:35:15,250
and bonding is also still something you

00:35:12,140 --> 00:35:17,420
connect you can still do for a che and

00:35:15,250 --> 00:35:20,570
something that you may want to turn on

00:35:17,420 --> 00:35:22,730
why not saving TCP metrics is that you

00:35:20,570 --> 00:35:26,540
know by default tcp itself will save

00:35:22,730 --> 00:35:30,110
various connection metrics in the route

00:35:26,540 --> 00:35:32,300
cash when the connection closes so if

00:35:30,110 --> 00:35:35,960
you turn this off you may actually see

00:35:32,300 --> 00:35:38,090
some performance improvement so you

00:35:35,960 --> 00:35:40,010
don't actually catch anything when the

00:35:38,090 --> 00:35:42,530
connections are closed and this

00:35:40,010 --> 00:35:48,440
generally saves probably the last 1000

00:35:42,530 --> 00:35:51,410
or so connections give or take you

00:35:48,440 --> 00:35:52,760
probably want to have the app filter

00:35:51,410 --> 00:35:56,510
especially when you're using something

00:35:52,760 --> 00:35:59,120
like again galera because a database

00:35:56,510 --> 00:36:01,280
server usually has to network interface

00:35:59,120 --> 00:36:03,470
cards so if your queries are loaded on

00:36:01,280 --> 00:36:05,870
your private land you never ever want

00:36:03,470 --> 00:36:08,060
the traffic to accidentally go through

00:36:05,870 --> 00:36:10,300
the public land so you turn on the air

00:36:08,060 --> 00:36:10,300
filter

00:36:10,500 --> 00:36:17,340
the idea for having jumbo frames makes

00:36:14,310 --> 00:36:22,440
sense when you're using things like

00:36:17,340 --> 00:36:25,530
Cassandra HBase NDB cluster if you

00:36:22,440 --> 00:36:28,560
happen to use drbd for the raid over

00:36:25,530 --> 00:36:31,830
Ethernet to ensure you have a extra copy

00:36:28,560 --> 00:36:35,970
of your data on a passive master turning

00:36:31,830 --> 00:36:38,700
on jumbo frames actually makes sense we

00:36:35,970 --> 00:36:41,190
don't see a huge benefit for oltp

00:36:38,700 --> 00:36:43,290
workloads but we definitely see a

00:36:41,190 --> 00:36:45,930
benefit for data warehousing workloads

00:36:43,290 --> 00:36:48,810
so if you happen to use infinity be info

00:36:45,930 --> 00:36:51,480
bright as well again jumbo frames

00:36:48,810 --> 00:36:59,550
actually tend to work quite well for you

00:36:51,480 --> 00:37:00,780
or the new Maria DB columnstore there

00:36:59,550 --> 00:37:05,670
are few other things you may end up

00:37:00,780 --> 00:37:08,460
wanting to tune like like increasing the

00:37:05,670 --> 00:37:11,090
kernel packet buffer reducing TCP

00:37:08,460 --> 00:37:13,290
timeouts when connections have closed

00:37:11,090 --> 00:37:16,160
TCP Finn timeout basically will

00:37:13,290 --> 00:37:19,320
determine how long to wait for a feed

00:37:16,160 --> 00:37:21,420
defaults are not necessarily as sensible

00:37:19,320 --> 00:37:25,650
when it comes to a server so you

00:37:21,420 --> 00:37:29,670
probably want to change them if you

00:37:25,650 --> 00:37:32,910
happen to be running galera cluster it

00:37:29,670 --> 00:37:36,690
is always worth noting that galera will

00:37:32,910 --> 00:37:38,670
write as fast as your fastest node well

00:37:36,690 --> 00:37:41,130
as fast as your slowest node so if you

00:37:38,670 --> 00:37:43,650
happen to have three two very good

00:37:41,130 --> 00:37:46,080
machines and then you have your you have

00:37:43,650 --> 00:37:48,510
this laptop as your third node rights

00:37:46,080 --> 00:37:51,630
are going to be as fast as as this

00:37:48,510 --> 00:37:55,200
laptop and then you also need to tune

00:37:51,630 --> 00:37:57,150
the EVS protocol which is there for auto

00:37:55,200 --> 00:38:00,660
eviction purposes of getting rid of

00:37:57,150 --> 00:38:02,730
members of the cluster and you need to

00:38:00,660 --> 00:38:04,410
tune the send windows and the user send

00:38:02,730 --> 00:38:08,820
windows to something large by default

00:38:04,410 --> 00:38:11,190
this is actually not tuned as well if

00:38:08,820 --> 00:38:13,800
you obviously run galera you can route

00:38:11,190 --> 00:38:16,590
the gcom traffic as galera doesn't only

00:38:13,800 --> 00:38:19,930
require you to have port 3306 open it

00:38:16,590 --> 00:38:22,660
also requires to have a G com node open

00:38:19,930 --> 00:38:24,580
gee comport and you can route all g comm

00:38:22,660 --> 00:38:26,740
traffic so that's the gallery cluster

00:38:24,580 --> 00:38:28,900
communication traffic to a second

00:38:26,740 --> 00:38:30,430
network interface card so you actually

00:38:28,900 --> 00:38:35,860
keep your main network interface card

00:38:30,430 --> 00:38:40,750
for your transactions I saw that

00:38:35,860 --> 00:38:42,520
includes this year largely because was

00:38:40,750 --> 00:38:44,440
chatting on IRC a couple days ago and a

00:38:42,520 --> 00:38:48,220
guy was like hey I can't get any core

00:38:44,440 --> 00:38:50,590
dubs mmm okay well it's because you

00:38:48,220 --> 00:38:52,030
didn't set your resource limits but

00:38:50,590 --> 00:38:54,040
that's more more important than just

00:38:52,030 --> 00:38:56,880
getting core dumps is that if you end up

00:38:54,040 --> 00:39:00,100
having to use something like MongoDB

00:38:56,880 --> 00:39:02,200
 d for example requires one file

00:39:00,100 --> 00:39:04,120
descriptor for each data file in use as

00:39:02,200 --> 00:39:06,880
well as one file descriptor for each

00:39:04,120 --> 00:39:09,460
journal file in use when the journal is

00:39:06,880 --> 00:39:11,140
being used and when you have replica

00:39:09,460 --> 00:39:14,380
sets which is again one of MongoDB

00:39:11,140 --> 00:39:16,240
claims to fame you have to remember that

00:39:14,380 --> 00:39:18,130
each and every d instance will

00:39:16,240 --> 00:39:21,700
maintain a connection to all the other

00:39:18,130 --> 00:39:24,040
members of the set so a default default

00:39:21,700 --> 00:39:26,770
resource limits are extremely low for

00:39:24,040 --> 00:39:29,950
mongodb to actually just get working at

00:39:26,770 --> 00:39:33,040
scale so you definitely want to turn

00:39:29,950 --> 00:39:34,630
this up and of course when mice qld

00:39:33,040 --> 00:39:40,450
crashes it's nice to get those core

00:39:34,630 --> 00:39:43,330
dumps you definitely want to also tune

00:39:40,450 --> 00:39:45,700
your database I mean tuning Linux is all

00:39:43,330 --> 00:39:48,070
fun but you also need to find all those

00:39:45,700 --> 00:39:50,200
slow and crappy queries so in the MySQL

00:39:48,070 --> 00:39:53,230
world SPT query digests it'll pause the

00:39:50,200 --> 00:39:56,620
slow query dog in the MongoDB world SPT

00:39:53,230 --> 00:40:00,760
query digest dash that will parse

00:39:56,620 --> 00:40:03,160
the slow query log you obviously want to

00:40:00,760 --> 00:40:06,640
monitor things like errors that are sent

00:40:03,160 --> 00:40:08,410
back to the client so Maria DB ships are

00:40:06,640 --> 00:40:10,360
plugging that will actually log all of

00:40:08,410 --> 00:40:12,130
these errors and you will actually be

00:40:10,360 --> 00:40:14,740
able to look at them to see if they're

00:40:12,130 --> 00:40:16,060
things that you can improve and of

00:40:14,740 --> 00:40:17,800
course you want to configure your

00:40:16,060 --> 00:40:21,640
database it turns out that when you

00:40:17,800 --> 00:40:23,440
install a database the Mitel CNF is for

00:40:21,640 --> 00:40:26,380
the base configure right the base config

00:40:23,440 --> 00:40:28,360
is assumed that you have 512 megs of ram

00:40:26,380 --> 00:40:31,870
so you can start this up in a cloud

00:40:28,360 --> 00:40:34,890
instance imagine having the base config

00:40:31,870 --> 00:40:34,890
that actually sucks

00:40:36,539 --> 00:40:41,890
virtualization is extremely popular

00:40:38,799 --> 00:40:43,779
today I'm almost certain even if you're

00:40:41,890 --> 00:40:48,579
running on bad metal you may be using

00:40:43,779 --> 00:40:54,279
this data bases of course run best on

00:40:48,579 --> 00:40:56,650
all hardware so one of the key things to

00:40:54,279 --> 00:40:59,589
remember is guess run as processes

00:40:56,650 --> 00:41:02,890
inside the user space on a host so you

00:40:59,589 --> 00:41:04,569
basically get your virtual CPUs but it's

00:41:02,890 --> 00:41:06,819
also it's worth remembering that guests

00:41:04,569 --> 00:41:09,130
will inherit cuddle features like huge

00:41:06,819 --> 00:41:11,319
pages and numerous so make sure you you

00:41:09,130 --> 00:41:13,510
know turned off huge face huge pages for

00:41:11,319 --> 00:41:17,559
example when you want to run for

00:41:13,510 --> 00:41:20,230
governor selvam ready be native aio is

00:41:17,559 --> 00:41:23,859
used over threaded today so that's nae

00:41:20,230 --> 00:41:28,170
today I 0 is asynchronous i/o nodb use

00:41:23,859 --> 00:41:30,519
native aio equals 1 is now the default

00:41:28,170 --> 00:41:33,970
we have seen instances where the

00:41:30,519 --> 00:41:36,970
hypervisor actually lies that the

00:41:33,970 --> 00:41:41,259
virtual machine didn't have sink so you

00:41:36,970 --> 00:41:43,779
can actually lose data these hypervisors

00:41:41,259 --> 00:41:48,160
tend to be more of more commercial

00:41:43,779 --> 00:41:52,809
variety hypervisors the open source will

00:41:48,160 --> 00:41:55,660
understand not to lie on the next hosts

00:41:52,809 --> 00:41:58,569
if you happen to use VMware it does not

00:41:55,660 --> 00:42:00,190
use unbuffered I oh so because it's not

00:41:58,569 --> 00:42:02,950
safe or supported across all Linux

00:42:00,190 --> 00:42:05,680
versions that VMware supports so

00:42:02,950 --> 00:42:08,619
basically vmware's hosted products on

00:42:05,680 --> 00:42:11,109
Linux will always use buffered I oh now

00:42:08,619 --> 00:42:14,259
how many people here use VMware for

00:42:11,109 --> 00:42:18,460
virtualization wow that's quite a lot of

00:42:14,259 --> 00:42:19,839
you so VMware actually besides what I

00:42:18,460 --> 00:42:22,000
just mentioned they actually have fairly

00:42:19,839 --> 00:42:24,970
lengthy documentation on how they differ

00:42:22,000 --> 00:42:31,240
from what you may be familiar with like

00:42:24,970 --> 00:42:35,730
Zeno kvm containers are getting

00:42:31,240 --> 00:42:38,950
extremely popular as well Cooper Nerys

00:42:35,730 --> 00:42:42,819
so you can today go to V test at i/o and

00:42:38,950 --> 00:42:47,620
roll out without any issue kubin at ease

00:42:42,819 --> 00:42:49,420
with docker and and mysql in

00:42:47,620 --> 00:42:50,950
since this all murrieta be instances and

00:42:49,420 --> 00:42:52,720
have it running in Google's cloud the

00:42:50,950 --> 00:42:56,290
only issue here is you obviously have to

00:42:52,720 --> 00:42:58,390
give them your credit card number but

00:42:56,290 --> 00:43:00,190
it's actually made it extremely easy so

00:42:58,390 --> 00:43:01,930
a good example of what the test can do

00:43:00,190 --> 00:43:04,180
is it can actually do shouting for you

00:43:01,930 --> 00:43:06,250
and that's how you for example YouTube

00:43:04,180 --> 00:43:09,250
itself maintained charting because every

00:43:06,250 --> 00:43:11,740
time you make one connection to mysql it

00:43:09,250 --> 00:43:13,270
can use anywhere between maybe 200 26

00:43:11,740 --> 00:43:15,640
kilobytes of RAM to up to three

00:43:13,270 --> 00:43:18,100
megabytes of RAM for that connection

00:43:15,640 --> 00:43:20,020
whereas vitesse actually reduces that

00:43:18,100 --> 00:43:30,150
cost of the connection to down to 32

00:43:20,020 --> 00:43:32,650
kilo kilobyte dizon blobs besides that I

00:43:30,150 --> 00:43:36,130
would definitely do some benchmarking

00:43:32,650 --> 00:43:37,390
yourself to actually realize what kind

00:43:36,130 --> 00:43:41,860
of performance losses you're actually

00:43:37,390 --> 00:43:43,690
seeing we've seen obviously some over

00:43:41,860 --> 00:43:45,760
some overhead and performance loss and

00:43:43,690 --> 00:43:49,060
we see less for read-only workloads and

00:43:45,760 --> 00:43:52,780
I guess containers like darker are great

00:43:49,060 --> 00:43:55,270
for you to be trying things out on your

00:43:52,780 --> 00:44:02,970
dev machine but maybe it's not quite

00:43:55,270 --> 00:44:07,540
there yet for production ec2 for one has

00:44:02,970 --> 00:44:09,580
instance limitations if you happen to

00:44:07,540 --> 00:44:12,700
use hosted versions like RDS you want to

00:44:09,580 --> 00:44:17,020
also watch your costs it's worth noting

00:44:12,700 --> 00:44:20,680
that MySQL and Maria DB on ec2 are

00:44:17,020 --> 00:44:23,220
cheaper than postgres on each sorry on

00:44:20,680 --> 00:44:25,660
RDS are cheaper than postgres on RDS I

00:44:23,220 --> 00:44:28,300
don't know why it costs more to host

00:44:25,660 --> 00:44:30,880
postgres but you you pay a postgres

00:44:28,300 --> 00:44:35,190
penalty to run it on RDS get it

00:44:30,880 --> 00:44:37,890
maintained and managed by amazon we see

00:44:35,190 --> 00:44:40,060
with regards to EBS a lot of

00:44:37,890 --> 00:44:43,950
unpredictable i/o performance so if you

00:44:40,060 --> 00:44:48,850
are running a database node either use

00:44:43,950 --> 00:44:54,580
rate n raid 0 or pay for provision die

00:44:48,850 --> 00:44:58,090
ops we find that as people grow they

00:44:54,580 --> 00:45:00,700
give up RDS and they switch to using

00:44:58,090 --> 00:45:01,980
regular easy to where you can now

00:45:00,700 --> 00:45:04,690
install

00:45:01,980 --> 00:45:07,840
whatever you feel like it on on that and

00:45:04,690 --> 00:45:13,510
have a DBA maintain it today RDS of

00:45:07,840 --> 00:45:15,340
course now gives you a mysql 575 655 it

00:45:13,510 --> 00:45:18,940
also gives you more ATP silver 10 but

00:45:15,340 --> 00:45:21,520
not 10 10 postgres but you may want to

00:45:18,940 --> 00:45:23,560
run percona server or some modern

00:45:21,520 --> 00:45:26,760
version of maria db10 one for example

00:45:23,560 --> 00:45:31,030
and none of this is available in RTS

00:45:26,760 --> 00:45:33,220
when you are loading data it is a

00:45:31,030 --> 00:45:36,130
postgres instance on RDS you definitely

00:45:33,220 --> 00:45:39,310
want to turn off backup retention also

00:45:36,130 --> 00:45:42,820
when you're loading data to RDS disable

00:45:39,310 --> 00:45:44,440
multiple az's this can be for all

00:45:42,820 --> 00:45:47,340
databases because you don't want to be

00:45:44,440 --> 00:45:50,320
writing to a passive master somewhere

00:45:47,340 --> 00:45:54,270
you might as well get the full

00:45:50,320 --> 00:45:54,270
performance of actually loading the data

00:45:56,460 --> 00:46:05,110
don't disable f sync really don't don't

00:46:02,080 --> 00:46:06,400
even disable in DB flash log I trans I

00:46:05,110 --> 00:46:10,560
didn't commit especially when you're

00:46:06,400 --> 00:46:10,560
running on Hardware you do not control

00:46:12,420 --> 00:46:16,630
they obviously some interesting reasons

00:46:14,980 --> 00:46:18,400
as to why you may want to run ready to

00:46:16,630 --> 00:46:21,880
be epic owner server instead of stock

00:46:18,400 --> 00:46:23,770
mysql 41 there's a thread pool and since

00:46:21,880 --> 00:46:26,800
we're here talking about performance and

00:46:23,770 --> 00:46:30,100
tuning a thread is you already if i

00:46:26,800 --> 00:46:32,380
mentioned earlier mysql as a 11 thread

00:46:30,100 --> 00:46:34,900
per connection model if you have many

00:46:32,380 --> 00:46:36,340
cheap short running connections this

00:46:34,900 --> 00:46:38,920
actually opens up threads it's actually

00:46:36,340 --> 00:46:40,240
end up ends up using up memory have a

00:46:38,920 --> 00:46:42,580
thread pool that you can actually just

00:46:40,240 --> 00:46:44,500
pull the threads together and constantly

00:46:42,580 --> 00:46:48,820
reuse those those connections and the

00:46:44,500 --> 00:46:51,610
thread pool works natively on all linux

00:46:48,820 --> 00:46:55,330
distributions bsd as well as windows

00:46:51,610 --> 00:46:57,310
actually so the thread pool is available

00:46:55,330 --> 00:46:59,830
in mysql but it's only available in a

00:46:57,310 --> 00:47:01,780
enterprise release of mysql but i'm sure

00:46:59,830 --> 00:47:03,010
we're all here for open source so again

00:47:01,780 --> 00:47:05,470
Marie to be in procurements ever have

00:47:03,010 --> 00:47:07,300
that new my interleaving is extremely

00:47:05,470 --> 00:47:09,850
important when you have large amounts of

00:47:07,300 --> 00:47:13,180
memory so you that's probably another

00:47:09,850 --> 00:47:14,950
good thing having faster nodvd restarts

00:47:13,180 --> 00:47:16,900
apparel double write buffer and in

00:47:14,950 --> 00:47:20,230
earlier versions extra DB also had the

00:47:16,900 --> 00:47:22,839
idea of a nerdy be fake ranges so if you

00:47:20,230 --> 00:47:24,640
happen to have a crash and you need to

00:47:22,839 --> 00:47:27,609
start up you can just run fake changes

00:47:24,640 --> 00:47:29,500
to do all the ddl operations and then

00:47:27,609 --> 00:47:31,930
roll them back so you'll actually just

00:47:29,500 --> 00:47:35,020
warm up your bubble and that that's

00:47:31,930 --> 00:47:36,700
actually pretty useful and of course the

00:47:35,020 --> 00:47:37,990
new parallel double write buffer is

00:47:36,700 --> 00:47:40,810
something that could be also quite

00:47:37,990 --> 00:47:43,359
interesting to look at and of course you

00:47:40,810 --> 00:47:45,820
you you can run something like galera

00:47:43,359 --> 00:47:50,050
cluster as well you don't only have to

00:47:45,820 --> 00:47:56,260
run you know MySQL or the art or wait

00:47:50,050 --> 00:47:58,770
for group application and so forth when

00:47:56,260 --> 00:48:02,619
it comes to data center stuff we've seen

00:47:58,770 --> 00:48:04,119
bad batches of of network cards bad

00:48:02,619 --> 00:48:06,700
batches of discs I'm sure you've seen

00:48:04,119 --> 00:48:10,540
just bad batches of stuff in general so

00:48:06,700 --> 00:48:13,300
and you may find that bad batches

00:48:10,540 --> 00:48:18,070
actually cause random bugs that show up

00:48:13,300 --> 00:48:19,930
as well so well worth checking to make

00:48:18,070 --> 00:48:24,329
sure that it's not your hardware that

00:48:19,930 --> 00:48:29,550
sucks it is it does actually a problem

00:48:24,329 --> 00:48:31,960
when you run Hadoop or HBase on easy to

00:48:29,550 --> 00:48:34,690
this is this really is a network of you

00:48:31,960 --> 00:48:38,710
salad so we don't recommend you to do

00:48:34,690 --> 00:48:40,569
that you also want to make sure things

00:48:38,710 --> 00:48:45,420
like name resolution always work in your

00:48:40,569 --> 00:48:49,750
network you want to have bonding but

00:48:45,420 --> 00:48:51,400
beyond that the major thing you want to

00:48:49,750 --> 00:48:53,890
change when you're running Hadoop HBase

00:48:51,400 --> 00:48:55,990
or even cassandra is to basically

00:48:53,890 --> 00:48:59,770
increase your you limits and also know

00:48:55,990 --> 00:49:03,250
that you don't want to really run this

00:48:59,770 --> 00:49:07,619
in the cloud generally speaking it's

00:49:03,250 --> 00:49:11,770
extremely extremely noisy on the network

00:49:07,619 --> 00:49:16,170
many other databases also give you

00:49:11,770 --> 00:49:18,970
tuning manuals and things like neo4j

00:49:16,170 --> 00:49:20,770
which at which I don't which basically

00:49:18,970 --> 00:49:22,510
just says again just change the you

00:49:20,770 --> 00:49:25,920
limits nothing doesn't really nothing

00:49:22,510 --> 00:49:29,890
else that you need to do to get going

00:49:25,920 --> 00:49:31,690
rioc has generally very similar tuning

00:49:29,890 --> 00:49:33,250
when it comes to the network so pretty

00:49:31,690 --> 00:49:36,190
much what I talked about earlier will

00:49:33,250 --> 00:49:38,800
generally work for yak they recommend

00:49:36,190 --> 00:49:40,329
India Docks to disable swap but as I

00:49:38,800 --> 00:49:42,730
have already said you definitely don't

00:49:40,329 --> 00:49:46,240
want to disable swap at least set it to

00:49:42,730 --> 00:49:48,339
one the mount options in the reactor

00:49:46,240 --> 00:49:52,030
wall again are extremely similar to what

00:49:48,339 --> 00:49:54,280
you do in mysql and they do specifically

00:49:52,030 --> 00:49:56,170
state state that they would benefit from

00:49:54,280 --> 00:49:58,210
a deadline scheduler mysql doesn't

00:49:56,170 --> 00:50:00,880
really state what you benefit from it's

00:49:58,210 --> 00:50:03,310
actually okay with using whatever but

00:50:00,880 --> 00:50:06,460
obviously people benchmark to actually

00:50:03,310 --> 00:50:10,420
realize that it's better to use deadline

00:50:06,460 --> 00:50:13,660
I hear a lot of people love to say that

00:50:10,420 --> 00:50:15,640
they should just disable selinux you

00:50:13,660 --> 00:50:18,790
really don't want to do that you find

00:50:15,640 --> 00:50:21,130
selinux extremely hard to understand Red

00:50:18,790 --> 00:50:23,050
Hat even made this nice coloring book to

00:50:21,130 --> 00:50:26,980
help you understand i see linux they've

00:50:23,050 --> 00:50:29,200
made it accessible in fact when we ship

00:50:26,980 --> 00:50:33,040
maria DB we ensure that it works with

00:50:29,200 --> 00:50:34,839
selinux we have seen people complain at

00:50:33,040 --> 00:50:37,930
least three on the forums a while back

00:50:34,839 --> 00:50:40,119
that app alma on a boon to will actually

00:50:37,930 --> 00:50:42,089
call system d issues on maury DB server

00:50:40,119 --> 00:50:47,470
basically a Palmer will come in to kill

00:50:42,089 --> 00:50:50,020
the Maria DB server this usually happens

00:50:47,470 --> 00:50:52,270
because someone decided to compile their

00:50:50,020 --> 00:50:54,040
own software and then packaged it up

00:50:52,270 --> 00:50:56,020
themselves and which point they should

00:50:54,040 --> 00:51:00,369
also learn to package up a par more

00:50:56,020 --> 00:51:02,220
stuff we ship everything in a way that

00:51:00,369 --> 00:51:04,480
should generally just work for you and

00:51:02,220 --> 00:51:06,940
we also ship inside the Linux

00:51:04,480 --> 00:51:08,290
distributions so naturally we test

00:51:06,940 --> 00:51:11,170
against things like s Linux and a

00:51:08,290 --> 00:51:13,089
polymer to make sure that they work so

00:51:11,170 --> 00:51:17,010
if you happen to want to compile stuff

00:51:13,089 --> 00:51:17,010
yourself you should definitely fix it

00:51:17,700 --> 00:51:25,510
there plenty of tools that you probably

00:51:21,130 --> 00:51:27,790
are familiar with I have not even put up

00:51:25,510 --> 00:51:29,500
there something like poor man's profiler

00:51:27,790 --> 00:51:32,710
which basically ends up making use of

00:51:29,500 --> 00:51:35,410
things like GD be there are obviously

00:51:32,710 --> 00:51:37,960
many mysql only related tools that are

00:51:35,410 --> 00:51:38,650
quite handy i would say the percona

00:51:37,960 --> 00:51:41,740
toolkit has

00:51:38,650 --> 00:51:44,890
a lot of them that could be useful many

00:51:41,740 --> 00:51:46,930
of those tools have been sort of vaguely

00:51:44,890 --> 00:51:51,630
portlet to work on the manga world by

00:51:46,930 --> 00:51:54,120
independent members of the community GDB

00:51:51,630 --> 00:51:56,710
surprisingly enough is extremely useful

00:51:54,120 --> 00:51:58,600
it's also an extremely useful to and you

00:51:56,710 --> 00:52:01,390
want to like switch options in say a

00:51:58,600 --> 00:52:05,110
running mysqld without restarting a

00:52:01,390 --> 00:52:08,500
server I won't bore you with those

00:52:05,110 --> 00:52:10,330
details or encourage such behavior but

00:52:08,500 --> 00:52:12,370
I'm sure if you search the internet you

00:52:10,330 --> 00:52:16,570
will find some good information about

00:52:12,370 --> 00:52:18,610
that when it comes to benchmarking there

00:52:16,570 --> 00:52:23,260
plenty of benchmark tools that we end up

00:52:18,610 --> 00:52:25,360
always using this bench suspect is not

00:52:23,260 --> 00:52:26,950
just useful for benchmarking mysql it's

00:52:25,360 --> 00:52:29,530
also useful for benchmarking your

00:52:26,950 --> 00:52:32,710
systems in general so to speak you can

00:52:29,530 --> 00:52:34,690
benchmark like file io e as well you can

00:52:32,710 --> 00:52:36,850
also use something like Bonnie + + for

00:52:34,690 --> 00:52:41,170
that but you can compare the Banu + + +

00:52:36,850 --> 00:52:44,080
6 bench results link bench has a more

00:52:41,170 --> 00:52:46,720
realistic social networking workload no

00:52:44,080 --> 00:52:49,030
surprise it comes from facebook so it

00:52:46,720 --> 00:52:52,440
works not only for MySQL but also for

00:52:49,030 --> 00:52:55,960
mongodb if you happen to use the other

00:52:52,440 --> 00:52:57,760
no SQL big data stores the Yahoo cloud

00:52:55,960 --> 00:53:00,280
serving benchmark is actually quite good

00:52:57,760 --> 00:53:03,490
as well something you may want to take a

00:53:00,280 --> 00:53:06,550
look at and if you want to compare what

00:53:03,490 --> 00:53:09,640
clouds look like Google's perfect eight

00:53:06,550 --> 00:53:11,400
bench marker will allow you to once you

00:53:09,640 --> 00:53:14,470
have accounts would say as your and

00:53:11,400 --> 00:53:15,820
Google and Amazon and so forth will

00:53:14,470 --> 00:53:18,820
allow you to compare the performance

00:53:15,820 --> 00:53:21,210
that you tend to get from the instances

00:53:18,820 --> 00:53:24,690
that you're that you're getting on these

00:53:21,210 --> 00:53:28,590
platforms and it's true Google's

00:53:24,690 --> 00:53:31,960
performance I mean if you measure hosted

00:53:28,590 --> 00:53:35,530
mysql on I've done this for Amazon

00:53:31,960 --> 00:53:38,260
Rackspace as well as Google Google's

00:53:35,530 --> 00:53:41,590
hardware and provisioning time and so

00:53:38,260 --> 00:53:43,690
forth is extremely like the best one

00:53:41,590 --> 00:53:46,600
wonders what they have behind there and

00:53:43,690 --> 00:53:48,370
I think maybe in 2012 they released some

00:53:46,600 --> 00:53:49,930
information about what the hardware was

00:53:48,370 --> 00:53:54,359
like then

00:53:49,930 --> 00:53:54,359
I can only imagine what it is like now

00:53:54,690 --> 00:54:00,309
there are also plenty of fairly good

00:53:57,339 --> 00:54:04,180
books that one might want to read high

00:54:00,309 --> 00:54:07,450
performance mysql while it doesn't cover

00:54:04,180 --> 00:54:10,720
mysql 56 and 57 so some of the premises

00:54:07,450 --> 00:54:12,609
there which state mysql shortcomings may

00:54:10,720 --> 00:54:16,390
have already been fixed and 56 and 57

00:54:12,609 --> 00:54:17,890
really has fixed quite love that the

00:54:16,390 --> 00:54:21,309
other one is systems performance tuning

00:54:17,890 --> 00:54:23,589
that's also extremely good I personally

00:54:21,309 --> 00:54:26,980
enjoy many of our friend and Greg's

00:54:23,589 --> 00:54:28,690
talks because they go extremely in-depth

00:54:26,980 --> 00:54:34,660
they also allow you to look at things

00:54:28,690 --> 00:54:37,900
like DTrace and so forth most likely our

00:54:34,660 --> 00:54:40,500
vendor also provides many of these

00:54:37,900 --> 00:54:44,109
tuning guides that you can end up using

00:54:40,500 --> 00:54:45,700
again if you look at red hat or sent to

00:54:44,109 --> 00:54:47,470
us you don't end up having to read these

00:54:45,700 --> 00:54:50,319
guides because most of them just say

00:54:47,470 --> 00:54:55,059
look you soon d and it generally just

00:54:50,319 --> 00:55:03,240
works my slides don't have any branding

00:54:55,059 --> 00:55:03,240
so high from maria DB corporation and

00:55:03,540 --> 00:55:11,230
with that think we're 54 minutes into

00:55:07,960 --> 00:55:15,130
this i'm open to answer your questions

00:55:11,230 --> 00:55:21,990
and i will repeat the questions for the

00:55:15,130 --> 00:55:21,990
video does anybody have a question

00:55:25,050 --> 00:55:32,109
my old colleague has a question yes I

00:55:28,059 --> 00:55:36,190
call it so fun I hope so when discussing

00:55:32,109 --> 00:55:38,920
file systems I missed a remark about XFS

00:55:36,190 --> 00:55:41,109
dynamic pre education sighs in my

00:55:38,920 --> 00:55:43,960
opinion it's extremely important to have

00:55:41,109 --> 00:55:45,880
that under control because otherwise you

00:55:43,960 --> 00:55:47,710
will waste lots of disk space and not

00:55:45,880 --> 00:55:51,599
get it back until you stop your mysql

00:55:47,710 --> 00:55:54,430
instance any other databases think yep

00:55:51,599 --> 00:55:56,440
XFS dynamic pre allocation size and this

00:55:54,430 --> 00:55:58,359
was actually more a problem with earlier

00:55:56,440 --> 00:56:00,790
versions of XFS rather than current

00:55:58,359 --> 00:56:03,730
versions of XFS I don't see this as a

00:56:00,790 --> 00:56:04,960
huge issue with XFS that you get in sent

00:56:03,730 --> 00:56:11,710
over seven but are you seeing

00:56:04,960 --> 00:56:13,869
differently we have seen issues with it

00:56:11,710 --> 00:56:17,559
and I've blocked that you should set

00:56:13,869 --> 00:56:19,990
amount option to avoid that fair enough

00:56:17,559 --> 00:56:35,309
so we should we should refer to York's

00:56:19,990 --> 00:56:35,309
blog or is it d okay yep okay question

00:57:07,180 --> 00:57:10,970
yep so question is if you run the

00:57:09,590 --> 00:57:13,400
database in the vm and you also

00:57:10,970 --> 00:57:15,680
obviously have lvm inside there and you

00:57:13,400 --> 00:57:19,190
also have a file system naturally either

00:57:15,680 --> 00:57:21,860
X fso ext4 what is the performance

00:57:19,190 --> 00:57:25,390
impact the performance impact inside the

00:57:21,860 --> 00:57:27,850
cake kvm is also very similar to the

00:57:25,390 --> 00:57:30,740
disc level snapshots that you'll do on

00:57:27,850 --> 00:57:34,760
unban metal so the first snapshot is

00:57:30,740 --> 00:57:36,950
extremely expensive and you know I said

00:57:34,760 --> 00:57:39,440
we see forty percent but we've also seen

00:57:36,950 --> 00:57:41,870
worst case first snapshots be you know

00:57:39,440 --> 00:57:43,610
much higher than that and that's the

00:57:41,870 --> 00:57:46,370
that's the first snapshot but every

00:57:43,610 --> 00:57:51,290
other following snapshot is actually not

00:57:46,370 --> 00:57:53,270
so bad so again it has no effect or if

00:57:51,290 --> 00:57:55,220
you happen to be running it inside of a

00:57:53,270 --> 00:57:57,050
virtualization platform but of course if

00:57:55,220 --> 00:57:59,000
you end up you know giving it extremely

00:57:57,050 --> 00:58:00,740
small amounts of memory you may see

00:57:59,000 --> 00:58:03,080
memory pressure and then you actually

00:58:00,740 --> 00:58:05,150
get worse much worse performance as well

00:58:03,080 --> 00:58:07,940
but that would be the same if you happen

00:58:05,150 --> 00:58:11,870
to run it on on bad metal with with very

00:58:07,940 --> 00:58:14,090
little memory as well but yeah lvm

00:58:11,870 --> 00:58:17,920
snapshots are today probably not the

00:58:14,090 --> 00:58:20,600
best way to get a mysql a backup because

00:58:17,920 --> 00:58:23,000
dump can give you a logical backup extra

00:58:20,600 --> 00:58:25,310
backup and give you physical backup and

00:58:23,000 --> 00:58:27,980
hot back up for talking to talk you back

00:58:25,310 --> 00:58:30,800
up and also do something similar so

00:58:27,980 --> 00:58:33,110
there there really isn't much much need

00:58:30,800 --> 00:58:35,240
for you to end up having to use our VM

00:58:33,110 --> 00:58:38,750
snapshot so I think lvm snapshots of a

00:58:35,240 --> 00:58:40,760
very good solution for maybe the last

00:58:38,750 --> 00:58:43,430
you know maybe five years ago or

00:58:40,760 --> 00:58:46,850
actually even even longer i think my LVN

00:58:43,430 --> 00:58:50,770
backup is a tool that started in may be

00:58:46,850 --> 00:58:50,770
the early 2000s or mid 2000s

00:58:56,120 --> 00:59:00,190

YouTube URL: https://www.youtube.com/watch?v=UIJCg-QPn08


