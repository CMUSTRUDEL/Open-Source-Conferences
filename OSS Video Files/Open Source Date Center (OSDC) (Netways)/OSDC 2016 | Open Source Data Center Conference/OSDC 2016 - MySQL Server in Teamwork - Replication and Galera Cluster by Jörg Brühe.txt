Title: OSDC 2016 - MySQL Server in Teamwork - Replication and Galera Cluster by JÃ¶rg BrÃ¼he
Publication date: 2016-05-02
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	Quite often, a single MySQL instance isn't enough, it rather should be several. The typical reasons are increased throughput, by distributing the load on more than one node, or high availability, by avoiding a single point of failure.
These goals can be achieved by setting up replication among several servers running MySQL, or by combining them to form a Galera cluster.
This talk will describe both approaches, compare them to each other, and assist with the decision which of these approaches will fit better to the user's goals and situation.
Captions: 
	00:00:13,760 --> 00:00:21,120
from dual is a Swiss company delivering

00:00:17,190 --> 00:00:24,779
services around my SQL services means we

00:00:21,120 --> 00:00:28,050
do consulting we provide support to

00:00:24,779 --> 00:00:30,210
discuss with customers why the system

00:00:28,050 --> 00:00:32,430
does not behave the way they expect it

00:00:30,210 --> 00:00:34,680
to whether it's correct and the

00:00:32,430 --> 00:00:37,590
expectations are wrong or otherwise

00:00:34,680 --> 00:00:39,980
whether it's a bug which we then discuss

00:00:37,590 --> 00:00:45,690
with the manufacturer we provide

00:00:39,980 --> 00:00:47,910
training around MySQL and yeah that's

00:00:45,690 --> 00:00:52,829
the consulting I think I mentioned

00:00:47,910 --> 00:00:57,239
already when I talk about my SQL here I

00:00:52,829 --> 00:00:59,910
refer to any variant of my SQL so it may

00:00:57,239 --> 00:01:03,180
be the original as delivered by Oracle

00:00:59,910 --> 00:01:07,470
it might be at the branch done by

00:01:03,180 --> 00:01:10,500
picanha and called Peconic picanha

00:01:07,470 --> 00:01:14,490
server it might be the fork done by

00:01:10,500 --> 00:01:18,020
maria DB called MariaDB and it could be

00:01:14,490 --> 00:01:21,210
either with or without Galliera cluster

00:01:18,020 --> 00:01:24,540
so for me and in the context of this

00:01:21,210 --> 00:01:29,420
talk MySQL is the generic term covering

00:01:24,540 --> 00:01:33,270
all variants as said from dual is

00:01:29,420 --> 00:01:36,360
dealing with it and delivering services

00:01:33,270 --> 00:01:40,350
a member of the relevant open source

00:01:36,360 --> 00:01:44,570
companies or organizations be the in

00:01:40,350 --> 00:01:47,700
Germany on this in Switzerland I

00:01:44,570 --> 00:01:50,310
personally have been active in the

00:01:47,700 --> 00:01:53,939
database management system development

00:01:50,310 --> 00:01:56,009
scene since I finished University so I

00:01:53,939 --> 00:01:58,020
have been involved not on the

00:01:56,009 --> 00:01:59,850
application side not on the application

00:01:58,020 --> 00:02:03,329
programming side but rather on the

00:01:59,850 --> 00:02:06,030
vendor side providing first a different

00:02:03,329 --> 00:02:09,420
database system which was then later

00:02:06,030 --> 00:02:12,060
cancelled decades ago and then join

00:02:09,420 --> 00:02:16,260
through MySQL build team spent several

00:02:12,060 --> 00:02:19,730
years there so those who followed MySQL

00:02:16,260 --> 00:02:22,669
release notes might have seen my name I

00:02:19,730 --> 00:02:26,350
then worked as a database administrator

00:02:22,669 --> 00:02:29,080
with larger website

00:02:26,350 --> 00:02:31,510
in Berlin and then joined from dual

00:02:29,080 --> 00:02:35,370
where I'm working in support in

00:02:31,510 --> 00:02:37,660
consulting and we'll add training to it

00:02:35,370 --> 00:02:41,890
conferences are not yet counted as a

00:02:37,660 --> 00:02:47,440
separate activity well that's my

00:02:41,890 --> 00:02:51,400
background that's what I bring and what

00:02:47,440 --> 00:02:54,520
makes me talk about replication and

00:02:51,400 --> 00:02:57,310
Calleja cluster so what the contents

00:02:54,520 --> 00:03:01,270
will discuss about the MySQL server

00:02:57,310 --> 00:03:03,790
architecture so that you know what's

00:03:01,270 --> 00:03:07,560
happening where and how the various

00:03:03,790 --> 00:03:12,060
components interact with each other will

00:03:07,560 --> 00:03:15,180
specially cover the MySQL Binh LOC and

00:03:12,060 --> 00:03:19,570
from based on that we will discuss how

00:03:15,180 --> 00:03:22,420
replication is implemented and what

00:03:19,570 --> 00:03:25,690
Galliera cluster does different we will

00:03:22,420 --> 00:03:29,140
then compare those two approaches and we

00:03:25,690 --> 00:03:31,840
will have some examples about

00:03:29,140 --> 00:03:33,940
circumstances when either the other one

00:03:31,840 --> 00:03:40,180
or the other approach is a better fit

00:03:33,940 --> 00:03:43,600
for the customer for the user I'll

00:03:40,180 --> 00:03:45,550
discuss concepts I am NOT going to into

00:03:43,600 --> 00:03:48,340
details or even a commands and

00:03:45,550 --> 00:03:51,070
configuration parameters so I'll try to

00:03:48,340 --> 00:03:56,950
show you the forest not the individual

00:03:51,070 --> 00:04:00,610
trees the basis is the previous GA

00:03:56,950 --> 00:04:05,080
version of MySQL it is 5-2 it's 5.6 I'm

00:04:00,610 --> 00:04:08,410
not yet discussing my SQL 5.7 in the

00:04:05,080 --> 00:04:12,160
main part of the talk I'll at the end

00:04:08,410 --> 00:04:15,370
I'll explain to you why as I said it's

00:04:12,160 --> 00:04:18,970
valid for Percona and MariaDB what I

00:04:15,370 --> 00:04:22,120
described here is does not include the

00:04:18,970 --> 00:04:25,120
so called NDB cluster of MySQL the

00:04:22,120 --> 00:04:28,090
special variant especially for telco

00:04:25,120 --> 00:04:30,760
companies and I'm not going to discuss

00:04:28,090 --> 00:04:33,449
the so-called embeddedness mysql where

00:04:30,760 --> 00:04:35,639
the application and the server code

00:04:33,449 --> 00:04:38,520
linked into a single process because

00:04:35,639 --> 00:04:44,099
that's fit for neither replication nor

00:04:38,520 --> 00:04:47,400
for Galliera cluster yeah and if there's

00:04:44,099 --> 00:04:50,309
anything unclear and you've got a

00:04:47,400 --> 00:04:53,219
question about facts please put it as

00:04:50,309 --> 00:04:55,979
soon as possible it's there's no use in

00:04:53,219 --> 00:04:59,029
me continuing and some of you having

00:04:55,979 --> 00:05:02,249
lost the track and the connection so

00:04:59,029 --> 00:05:02,909
factual questions please as soon as

00:05:02,249 --> 00:05:06,659
possible

00:05:02,909 --> 00:05:09,229
discussion at the end as I announced at

00:05:06,659 --> 00:05:12,479
the first chapter is the MySQL

00:05:09,229 --> 00:05:15,990
architecture in general MySQL is a

00:05:12,479 --> 00:05:19,860
typical client-server system so it does

00:05:15,990 --> 00:05:22,110
have one or more clients typically on

00:05:19,860 --> 00:05:25,949
different machines but might be running

00:05:22,110 --> 00:05:29,009
on the same machine which somehow either

00:05:25,949 --> 00:05:31,919
locally or via the network communicate

00:05:29,009 --> 00:05:34,789
with the MySQL server process and the

00:05:31,919 --> 00:05:37,770
MySQL server process is the only thing

00:05:34,789 --> 00:05:41,430
accessing the disks the clients better

00:05:37,770 --> 00:05:44,370
do not try to do that individually or

00:05:41,430 --> 00:05:46,919
themselves it would be it would create a

00:05:44,370 --> 00:05:49,680
horrible mess but that's a standard for

00:05:46,919 --> 00:05:52,469
any client server system the server

00:05:49,680 --> 00:05:55,499
itself is multi-threaded one thread per

00:05:52,469 --> 00:05:59,069
currently active connection processing

00:05:55,499 --> 00:06:02,550
the command sent by the client at the

00:05:59,069 --> 00:06:05,520
other end of the connection and disk of

00:06:02,550 --> 00:06:08,819
course includes rotating elements or if

00:06:05,520 --> 00:06:11,610
you want it really fast and are prepared

00:06:08,819 --> 00:06:17,580
to spend the money solid-state doesn't

00:06:11,610 --> 00:06:20,430
matter functioning in large setups

00:06:17,580 --> 00:06:23,849
it's very customary to have the disks in

00:06:20,430 --> 00:06:26,879
as on device which has its own

00:06:23,849 --> 00:06:29,789
advantages and its own drawbacks I'm not

00:06:26,879 --> 00:06:33,180
going to discuss them here I trust you

00:06:29,789 --> 00:06:35,819
are all aware of that well of the

00:06:33,180 --> 00:06:39,689
decision process to pick the one or the

00:06:35,819 --> 00:06:44,430
other so the MySQL server internally

00:06:39,689 --> 00:06:46,790
looks like this we have the client

00:06:44,430 --> 00:06:49,670
somewhere I'm sorry about the

00:06:46,790 --> 00:06:52,460
red color combination so the color the

00:06:49,670 --> 00:06:55,070
client is a separate process to

00:06:52,460 --> 00:06:59,240
particularly several clients of course

00:06:55,070 --> 00:07:01,160
and whatever they sent first is handled

00:06:59,240 --> 00:07:04,040
by the connection manager to make sure

00:07:01,160 --> 00:07:06,950
it's going to the proper thread the user

00:07:04,040 --> 00:07:10,160
is authenticated the commands are

00:07:06,950 --> 00:07:12,980
dispatched queries or select queries

00:07:10,160 --> 00:07:15,560
might be served from the query cache if

00:07:12,980 --> 00:07:20,450
the results didn't change since last

00:07:15,560 --> 00:07:22,940
time if not or any other request is

00:07:20,450 --> 00:07:27,680
passed and then given to the optimizer

00:07:22,940 --> 00:07:31,190
for the best way to execute execution

00:07:27,680 --> 00:07:36,410
path access control is of course checked

00:07:31,190 --> 00:07:40,150
and the table manager then picks the

00:07:36,410 --> 00:07:42,980
proper table using some caches

00:07:40,150 --> 00:07:45,200
preferably so that need not do so many

00:07:42,980 --> 00:07:47,180
disk operations and then it's passed

00:07:45,200 --> 00:07:50,120
through a handler interface and there

00:07:47,180 --> 00:07:52,250
are several different handlers depending

00:07:50,120 --> 00:07:54,980
on the mechanism used to implement the

00:07:52,250 --> 00:07:58,360
table what's interesting us here

00:07:54,980 --> 00:08:01,670
currently is in ODB and nothing else

00:07:58,360 --> 00:08:04,730
everything else is a special solution

00:08:01,670 --> 00:08:08,780
and at the moment we talked about Kali

00:08:04,730 --> 00:08:15,160
Ava cluster or we can do only with in

00:08:08,780 --> 00:08:19,880
ODB tables I'll like to tell you why no

00:08:15,160 --> 00:08:24,140
that's the very high level tour of the

00:08:19,880 --> 00:08:26,660
MySQL server architecture let's talk

00:08:24,140 --> 00:08:29,870
about the bin lock because this is where

00:08:26,660 --> 00:08:32,090
it becomes interesting as I said this

00:08:29,870 --> 00:08:36,410
Nava is multi-threaded so we have got

00:08:32,090 --> 00:08:40,690
two or more user threads each executing

00:08:36,410 --> 00:08:43,700
the commands sent by a single client and

00:08:40,690 --> 00:08:47,510
the commands are then given to the

00:08:43,700 --> 00:08:50,260
proper table handler in practice a table

00:08:47,510 --> 00:08:54,440
handlers are nodb and Maya or myosin

00:08:50,260 --> 00:08:55,820
others do not have a high usage number

00:08:54,440 --> 00:08:57,440
that's why I did not mention them here

00:08:55,820 --> 00:09:03,980
explicitly

00:08:57,440 --> 00:09:06,050
now before the command is given to the

00:09:03,980 --> 00:09:09,620
table handle before it passes the

00:09:06,050 --> 00:09:13,639
handler interface the lowest part of the

00:09:09,620 --> 00:09:17,689
general SQL layer is the generation of a

00:09:13,639 --> 00:09:21,170
pin lock entry the bin log means it's a

00:09:17,689 --> 00:09:24,620
collection of all changes which are done

00:09:21,170 --> 00:09:27,470
to either the data of the installation

00:09:24,620 --> 00:09:29,689
of the instance or to the schema so any

00:09:27,470 --> 00:09:32,959
create table alter table insert update

00:09:29,689 --> 00:09:36,949
delete would find its way into the bin

00:09:32,959 --> 00:09:40,850
lock and be written down there that's

00:09:36,949 --> 00:09:44,350
not so much needed for recovery after a

00:09:40,850 --> 00:09:48,019
crash but rather it's needed if you want

00:09:44,350 --> 00:09:51,170
to check if you later want to check what

00:09:48,019 --> 00:09:54,110
was done when who did it and especially

00:09:51,170 --> 00:09:59,329
it is needed for replication as I will

00:09:54,110 --> 00:10:01,670
soon show below that interface at the

00:09:59,329 --> 00:10:04,759
file layer there is the table Handler

00:10:01,670 --> 00:10:07,639
and the table and implements the XS

00:10:04,759 --> 00:10:09,680
through the data on disk does the

00:10:07,639 --> 00:10:13,149
recovery of the crash in the case of

00:10:09,680 --> 00:10:16,250
nodb if needed and stuff like this as

00:10:13,149 --> 00:10:20,060
those of you who deal with MySQL a bit

00:10:16,250 --> 00:10:23,060
or who have more experience with MySQL

00:10:20,060 --> 00:10:26,600
as those of you know my item itself has

00:10:23,060 --> 00:10:28,670
not crash safe that's one of the nasty

00:10:26,600 --> 00:10:30,410
properties of the Maya ISM engine and

00:10:28,670 --> 00:10:34,610
one of the reasons why you typically

00:10:30,410 --> 00:10:43,370
would not want to use it from a few

00:10:34,610 --> 00:10:47,139
exceptions granted okay so the important

00:10:43,370 --> 00:10:50,389
thing is that the bin lock covers all

00:10:47,139 --> 00:10:53,240
changes includes all changes and nothing

00:10:50,389 --> 00:10:57,079
else it does not log select statements

00:10:53,240 --> 00:11:03,030
or any values returned it just records

00:10:57,079 --> 00:11:06,570
the changes now what is this good for

00:11:03,030 --> 00:11:10,080
it means that in the bin lock we we know

00:11:06,570 --> 00:11:16,290
or we do log whatever was done to the

00:11:10,080 --> 00:11:21,440
data so data or schema and so this

00:11:16,290 --> 00:11:24,720
allows us to do recovery if we ever

00:11:21,440 --> 00:11:28,680
suffer a crash we could start from an

00:11:24,720 --> 00:11:31,140
old snapshot and then apply all entries

00:11:28,680 --> 00:11:34,590
of the bin lock which were generated

00:11:31,140 --> 00:11:37,980
after that snapshot and so do point in

00:11:34,590 --> 00:11:40,320
time recovery we are not limited to the

00:11:37,980 --> 00:11:43,830
previous or the next snapshot but we can

00:11:40,320 --> 00:11:46,040
reach any point in between as long as we

00:11:43,830 --> 00:11:49,170
have got the bin log entries available

00:11:46,040 --> 00:11:52,170
it is independent of the table handler

00:11:49,170 --> 00:11:56,250
so whether it's my eyes amor inner DB or

00:11:52,170 --> 00:11:59,160
whatever engine does not matter it can

00:11:56,250 --> 00:12:03,000
be written in three different statements

00:11:59,160 --> 00:12:05,280
sorry in three different formats the

00:12:03,000 --> 00:12:08,100
traditional format is the statement

00:12:05,280 --> 00:12:12,180
format which means the SQL command is

00:12:08,100 --> 00:12:15,510
written in plain text the new format is

00:12:12,180 --> 00:12:17,750
row which means the old and new data are

00:12:15,510 --> 00:12:21,720
locked but not the statement text and

00:12:17,750 --> 00:12:24,780
there is the mixed as an in-between

00:12:21,720 --> 00:12:28,170
level the difference becomes important

00:12:24,780 --> 00:12:32,250
because when you have your statement

00:12:28,170 --> 00:12:36,150
might contain something which needs to

00:12:32,250 --> 00:12:39,120
be evaluated while it is executed so

00:12:36,150 --> 00:12:41,880
when you say insert a row and the one

00:12:39,120 --> 00:12:45,480
value is a timestamp which you provide

00:12:41,880 --> 00:12:48,180
by the now function then of course it

00:12:45,480 --> 00:12:51,720
matters whether you're executed just now

00:12:48,180 --> 00:12:54,840
or in five minutes and the row format is

00:12:51,720 --> 00:12:56,940
essential to avoid any different

00:12:54,840 --> 00:13:01,140
contents which might be caused by

00:12:56,940 --> 00:13:03,930
functions like this so the row format is

00:13:01,140 --> 00:13:08,100
the only one which really guarantees

00:13:03,930 --> 00:13:10,260
that if you apply the bin log later you

00:13:08,100 --> 00:13:13,230
end up with the same result and the data

00:13:10,260 --> 00:13:15,840
as you did originally with statement

00:13:13,230 --> 00:13:16,800
format this can't happen

00:13:15,840 --> 00:13:20,100
Oh with the

00:13:16,800 --> 00:13:23,879
the data will differ the bin lock itself

00:13:20,100 --> 00:13:26,369
is written in segments the segment size

00:13:23,879 --> 00:13:29,040
can be configured and one when one

00:13:26,369 --> 00:13:31,709
segment has reached its size it is

00:13:29,040 --> 00:13:35,999
closed and a new segment has started

00:13:31,709 --> 00:13:42,929
they are numbered and this is pretty

00:13:35,999 --> 00:13:45,779
interesting very soon the bin lock is

00:13:42,929 --> 00:13:49,139
the basis for replication and that's the

00:13:45,779 --> 00:13:54,319
reason why I did describe it in such

00:13:49,139 --> 00:13:57,119
detail replication means we want to do

00:13:54,319 --> 00:14:00,360
changes which were done on one instance

00:13:57,119 --> 00:14:02,910
we want to repeat them propagate them to

00:14:00,360 --> 00:14:07,290
another instance and apply them there

00:14:02,910 --> 00:14:12,779
too so if both instances start from the

00:14:07,290 --> 00:14:15,420
same content and I log all changes done

00:14:12,779 --> 00:14:18,179
on a master instance and send this

00:14:15,420 --> 00:14:21,480
locked with the slave instance and apply

00:14:18,179 --> 00:14:25,319
the changes there I will end up with the

00:14:21,480 --> 00:14:30,299
same contents that's the basic principle

00:14:25,319 --> 00:14:33,509
of MySQL replication so we have

00:14:30,299 --> 00:14:36,420
applications clients that communicate

00:14:33,509 --> 00:14:40,170
with one one node typically called at a

00:14:36,420 --> 00:14:43,769
master node and the master writes a bin

00:14:40,170 --> 00:14:46,769
lock that contains all changes and the

00:14:43,769 --> 00:14:49,829
slave starts from the same initial State

00:14:46,769 --> 00:14:52,139
when we set it up we simply transferred

00:14:49,829 --> 00:14:56,279
a snapshot of the data from master to

00:14:52,139 --> 00:15:00,740
slave and installed it there now the

00:14:56,279 --> 00:15:06,089
master as I said logs all changes and

00:15:00,740 --> 00:15:09,149
the slave asks the changes to pass a

00:15:06,089 --> 00:15:12,299
sari asks the master to pass all changes

00:15:09,149 --> 00:15:15,509
to the slave the slave fetches all

00:15:12,299 --> 00:15:17,910
changes or in other words fetches the

00:15:15,509 --> 00:15:21,600
complete bin log contents from the

00:15:17,910 --> 00:15:26,970
master and applies those changes locally

00:15:21,600 --> 00:15:29,399
on the slave node this is running

00:15:26,970 --> 00:15:30,600
asynchronous depending on the relative

00:15:29,399 --> 00:15:32,759
speeds of master

00:15:30,600 --> 00:15:34,920
slave and if the commands are really

00:15:32,759 --> 00:15:38,639
complicated or the slave hardware is

00:15:34,920 --> 00:15:41,610
slow and it's disk is heavily loaded or

00:15:38,639 --> 00:15:45,149
stuff like this then we may have a delay

00:15:41,610 --> 00:15:49,500
in replication so the data on the slave

00:15:45,149 --> 00:15:51,630
may have the contents which the master

00:15:49,500 --> 00:15:54,660
had some seconds or maybe even some

00:15:51,630 --> 00:15:58,620
minutes before and in advance we can't

00:15:54,660 --> 00:16:00,930
tell how much this delay might be that's

00:15:58,620 --> 00:16:05,670
one of the weaknesses of MySQL

00:16:00,930 --> 00:16:08,970
replication the other thing is if any

00:16:05,670 --> 00:16:12,060
command on the slave behaves different

00:16:08,970 --> 00:16:16,829
than it did on the master then this

00:16:12,060 --> 00:16:20,910
slave stops replication typical example

00:16:16,829 --> 00:16:23,399
I do an insert into a table and on the

00:16:20,910 --> 00:16:27,449
master it's always fine on the slave I

00:16:23,399 --> 00:16:27,990
suffer from the placate key how could

00:16:27,449 --> 00:16:30,990
that happen

00:16:27,990 --> 00:16:32,819
well quite simple because somebody

00:16:30,990 --> 00:16:35,910
modified the data on the slave and

00:16:32,819 --> 00:16:38,970
already enter edit that record this

00:16:35,910 --> 00:16:43,709
can't be prevented in general now what

00:16:38,970 --> 00:16:47,459
would an insert command do if it hits a

00:16:43,709 --> 00:16:49,110
duplicate key situation the typical

00:16:47,459 --> 00:16:51,870
reaction would be the insert command

00:16:49,110 --> 00:16:54,149
throws an error and it's the client

00:16:51,870 --> 00:16:58,069
applications responsibility to deal with

00:16:54,149 --> 00:17:00,480
it error no problem well known any

00:16:58,069 --> 00:17:03,930
application that was programmed decently

00:17:00,480 --> 00:17:06,900
can do that now if that happens on the

00:17:03,930 --> 00:17:10,049
slave the slave doesn't have any

00:17:06,900 --> 00:17:12,959
connection for the client the slave

00:17:10,049 --> 00:17:14,730
cannot return an error to the client

00:17:12,959 --> 00:17:16,620
application because the client

00:17:14,730 --> 00:17:19,110
application is not connected to the

00:17:16,620 --> 00:17:23,720
slave it was connected to the master

00:17:19,110 --> 00:17:26,010
only so the slave cannot return an error

00:17:23,720 --> 00:17:28,760
on the other hand it can't simply

00:17:26,010 --> 00:17:33,289
continue which would be wrong either

00:17:28,760 --> 00:17:36,600
so all the slaves can do is stop

00:17:33,289 --> 00:17:39,179
replication replicating throw an error

00:17:36,600 --> 00:17:42,850
and call for the database administrator

00:17:39,179 --> 00:17:46,240
for exists for assistance and help

00:17:42,850 --> 00:17:48,010
yes you can do that but if it happens in

00:17:46,240 --> 00:17:49,510
the middle of the night most of you and

00:17:48,010 --> 00:17:57,100
your colleagues won't be very happy

00:17:49,510 --> 00:18:00,100
about it looking at it in a diagram this

00:17:57,100 --> 00:18:02,230
is what happens we have the clients

00:18:00,100 --> 00:18:05,230
connected to the master not shown here

00:18:02,230 --> 00:18:07,840
the master writes a pin lock there is a

00:18:05,230 --> 00:18:11,530
separate thread to which the slave

00:18:07,840 --> 00:18:15,550
connects and which it asks please send

00:18:11,530 --> 00:18:18,460
me all changes on a slave side these

00:18:15,550 --> 00:18:21,400
changes are first written into a relay

00:18:18,460 --> 00:18:25,540
knock on disk and then read by another

00:18:21,400 --> 00:18:28,750
thread the SQL thread which applies the

00:18:25,540 --> 00:18:34,060
transactions one after the other and so

00:18:28,750 --> 00:18:37,570
does the changes on a slave typically

00:18:34,060 --> 00:18:40,810
you would consider the slave to also

00:18:37,570 --> 00:18:44,080
write a bin lock and you would configure

00:18:40,810 --> 00:18:48,280
it to log the updates which itself

00:18:44,080 --> 00:18:52,570
receives as a slave and if you do that

00:18:48,280 --> 00:18:56,310
then this bin log here will contain all

00:18:52,570 --> 00:19:00,340
the changes the slave ever received and

00:18:56,310 --> 00:19:03,490
if you have a proper set up and down to

00:19:00,340 --> 00:19:07,360
any local changes on the slave but just

00:19:03,490 --> 00:19:11,290
apply replication then the contents of

00:19:07,360 --> 00:19:14,260
both bin locks will be the same I set

00:19:11,290 --> 00:19:16,390
the contents not necessarily the split

00:19:14,260 --> 00:19:18,010
into a segments because you could

00:19:16,390 --> 00:19:20,050
configure them different say with

00:19:18,010 --> 00:19:25,450
different segment size you could

00:19:20,050 --> 00:19:28,090
manually is you purge assign a flush

00:19:25,450 --> 00:19:32,080
command to switch to the next segment or

00:19:28,090 --> 00:19:34,240
stuff like this so segment numbers and

00:19:32,080 --> 00:19:39,420
offsets in the segment may differ but

00:19:34,240 --> 00:19:41,140
the contents will still be the same and

00:19:39,420 --> 00:19:44,080
what will you do that

00:19:41,140 --> 00:19:47,470
all that for well the typical use is

00:19:44,080 --> 00:19:49,900
high availability if you keep your data

00:19:47,470 --> 00:19:53,410
in both a master and the slave node

00:19:49,900 --> 00:19:55,260
which don't share anything please do not

00:19:53,410 --> 00:19:57,330
let them share the disk do not

00:19:55,260 --> 00:20:01,230
them work on the same Sun unit for

00:19:57,330 --> 00:20:04,320
example if you have your data on both

00:20:01,230 --> 00:20:07,470
instances then either instance may crash

00:20:04,320 --> 00:20:11,610
and your data will still be available so

00:20:07,470 --> 00:20:13,890
you have a high availability the next

00:20:11,610 --> 00:20:17,429
advantage is you have got your data

00:20:13,890 --> 00:20:20,640
available in the second instance so you

00:20:17,429 --> 00:20:25,020
could send any read command to that

00:20:20,640 --> 00:20:27,809
second instance and let that provide the

00:20:25,020 --> 00:20:30,270
values especially when you've got very

00:20:27,809 --> 00:20:35,309
complicated commands joins across many

00:20:30,270 --> 00:20:38,010
tables repo generation about huge number

00:20:35,309 --> 00:20:40,590
of rows searching for the customer with

00:20:38,010 --> 00:20:43,919
the highest order volume or whatever

00:20:40,590 --> 00:20:47,220
have you then this is um this may be a

00:20:43,919 --> 00:20:50,220
significant load on your database server

00:20:47,220 --> 00:20:53,220
and you would prefer to have that on

00:20:50,220 --> 00:20:55,770
read-only slave and not have it on the

00:20:53,220 --> 00:21:00,980
master node which is also used to

00:20:55,770 --> 00:21:07,049
process all changes and and are all data

00:21:00,980 --> 00:21:09,690
so we have high availability we support

00:21:07,049 --> 00:21:13,320
a higher rate load also called a scale

00:21:09,690 --> 00:21:16,380
out we of course have a geographic

00:21:13,320 --> 00:21:19,110
redundancy if the two instances are

00:21:16,380 --> 00:21:22,080
located in different data centers so you

00:21:19,110 --> 00:21:25,500
would make them independent of say loss

00:21:22,080 --> 00:21:28,290
of power of a fire or flooding or stuff

00:21:25,500 --> 00:21:30,870
like this provided second the second

00:21:28,290 --> 00:21:35,669
data center is really separate and

00:21:30,870 --> 00:21:39,270
independent you can use such read-only

00:21:35,669 --> 00:21:42,290
instances especially for backups and for

00:21:39,270 --> 00:21:46,620
complicated reports report generation

00:21:42,290 --> 00:21:50,000
you can if you want to configure your

00:21:46,620 --> 00:21:50,000
replication to add

00:21:50,970 --> 00:21:57,750
intentional delay which is especially

00:21:54,910 --> 00:22:00,370
interesting if you want to prepare about

00:21:57,750 --> 00:22:05,220
against what my boss calls an oops

00:22:00,370 --> 00:22:08,140
theory the sudden drop table blahblah

00:22:05,220 --> 00:22:10,480
which which would would of course also

00:22:08,140 --> 00:22:12,960
be propagated to the slave and the table

00:22:10,480 --> 00:22:15,640
would begun on both instances but if you

00:22:12,960 --> 00:22:18,040
configure your application for an

00:22:15,640 --> 00:22:20,800
intentional delay of say 60 minutes

00:22:18,040 --> 00:22:23,140
there are good chances you discover it

00:22:20,800 --> 00:22:26,710
and stop replication and so save your

00:22:23,140 --> 00:22:32,760
data and avoid the need to do recovery

00:22:26,710 --> 00:22:36,490
and you can in replication do filtering

00:22:32,760 --> 00:22:42,190
filtering in a replication is possible

00:22:36,490 --> 00:22:45,130
can be configured is sometimes not

00:22:42,190 --> 00:22:48,010
really implemented correctly I have run

00:22:45,130 --> 00:22:51,340
across some bugs in that and reported

00:22:48,010 --> 00:22:54,220
them at the end in the example section

00:22:51,340 --> 00:22:55,960
I'll have you I will show you one case

00:22:54,220 --> 00:23:01,080
where this filtering is really

00:22:55,960 --> 00:23:06,280
interesting and worthwhile returning to

00:23:01,080 --> 00:23:10,620
replication it is not limited to a

00:23:06,280 --> 00:23:13,990
single level so you can have a cascade

00:23:10,620 --> 00:23:16,620
one master replicating to a first slave

00:23:13,990 --> 00:23:19,960
and that's safe using the same mechanism

00:23:16,620 --> 00:23:23,050
replicating to a second slave why is it

00:23:19,960 --> 00:23:26,830
interesting well especially if you want

00:23:23,050 --> 00:23:28,900
to have your data on a huge number of

00:23:26,830 --> 00:23:32,530
slaves because you have an enormous

00:23:28,900 --> 00:23:35,170
number of reads then you don't want to

00:23:32,530 --> 00:23:37,630
connect say dozens or even hundreds of

00:23:35,170 --> 00:23:39,610
slaves to a single master you would

00:23:37,630 --> 00:23:42,390
bring it down to its knees just by

00:23:39,610 --> 00:23:44,920
sending the bin lock to so many slaves

00:23:42,390 --> 00:23:50,620
so you would have an intermediate slave

00:23:44,920 --> 00:23:53,410
for better fan out in any case the rape

00:23:50,620 --> 00:23:55,720
recommendation is that on the slave you

00:23:53,410 --> 00:23:58,810
configure it to be a read-only instance

00:23:55,720 --> 00:24:01,330
and of course also to lock it updates

00:23:58,810 --> 00:24:03,920
otherwise propagating the application

00:24:01,330 --> 00:24:06,500
further would not work

00:24:03,920 --> 00:24:10,910
you can have multiple slaves on a single

00:24:06,500 --> 00:24:14,870
master but I would not go to really high

00:24:10,910 --> 00:24:18,830
numbers so two to five slaves is okay 20

00:24:14,870 --> 00:24:21,830
is in most cases too much somewhere in

00:24:18,830 --> 00:24:25,750
between there is a limit depending on

00:24:21,830 --> 00:24:30,710
the load and on the frequency of changes

00:24:25,750 --> 00:24:32,840
now when you can get a cascade from a

00:24:30,710 --> 00:24:36,650
master to a first and to a second-level

00:24:32,840 --> 00:24:40,940
slave you can fold no sorry

00:24:36,650 --> 00:24:44,390
coming this one yeah

00:24:40,940 --> 00:24:47,960
the bin lock sorry about that the bin

00:24:44,390 --> 00:24:51,770
lock is identified by the log segment

00:24:47,960 --> 00:24:56,510
fire name and the position in there and

00:24:51,770 --> 00:24:59,690
the replication is configured by giving

00:24:56,510 --> 00:25:02,510
the slave the information where it

00:24:59,690 --> 00:25:05,450
should replicate from so passing it the

00:25:02,510 --> 00:25:10,450
master name the porter use user password

00:25:05,450 --> 00:25:14,150
etc etc and starting from mysql 5.6

00:25:10,450 --> 00:25:17,840
there are there is a new concept of

00:25:14,150 --> 00:25:20,300
global transaction ID which makes a set

00:25:17,840 --> 00:25:22,790
up of replication definitely easier

00:25:20,300 --> 00:25:26,300
because you need not deal with a

00:25:22,790 --> 00:25:29,230
position anymore global transaction ID

00:25:26,300 --> 00:25:33,770
is bring the feature of auto position

00:25:29,230 --> 00:25:37,820
which makes it a bit easier as I said

00:25:33,770 --> 00:25:40,910
you can cascade replication so you can

00:25:37,820 --> 00:25:45,790
even fold back that replication to the

00:25:40,910 --> 00:25:49,970
original master master first slave

00:25:45,790 --> 00:25:53,380
replicated replicating onward to node

00:25:49,970 --> 00:25:56,860
which happens to be the master instance

00:25:53,380 --> 00:26:01,010
now this looks like an infinite circle

00:25:56,860 --> 00:26:05,030
no it isn't it is not infinite because

00:26:01,010 --> 00:26:08,120
any transaction in the bin lock contains

00:26:05,030 --> 00:26:11,930
the server ID of the machine on which it

00:26:08,120 --> 00:26:15,170
was originally started and this server

00:26:11,930 --> 00:26:17,220
ID is not changing its propagated

00:26:15,170 --> 00:26:21,000
propagating onwards through

00:26:17,220 --> 00:26:24,270
application and typically you configure

00:26:21,000 --> 00:26:29,610
your machines such that the master will

00:26:24,270 --> 00:26:33,900
not apply any change which originated on

00:26:29,610 --> 00:26:41,640
itself so this is where you prevent the

00:26:33,900 --> 00:26:44,610
circle from closing what's a huge

00:26:41,640 --> 00:26:47,640
problem in a situation like this is if

00:26:44,610 --> 00:26:51,030
on both machines you do changes and

00:26:47,640 --> 00:26:54,539
those changes are overlapping because

00:26:51,030 --> 00:26:58,260
then they will both fail on the

00:26:54,539 --> 00:27:02,789
receiving machine and replication will

00:26:58,260 --> 00:27:05,970
break and your DBA will call for a pay

00:27:02,789 --> 00:27:13,289
raise for being awoken in the middle of

00:27:05,970 --> 00:27:16,850
the night this set up has just shown is

00:27:13,289 --> 00:27:19,799
called a master master replication and

00:27:16,850 --> 00:27:21,419
opinions whether master master is good

00:27:19,799 --> 00:27:24,840
or bad differ

00:27:21,419 --> 00:27:28,200
I know colleagues who say master master

00:27:24,840 --> 00:27:31,080
replication is really bad down to it my

00:27:28,200 --> 00:27:33,090
personal opinion is master master

00:27:31,080 --> 00:27:35,700
application is perfectly okay if you

00:27:33,090 --> 00:27:37,559
know what you are doing and if you have

00:27:35,700 --> 00:27:39,990
are configuring your application

00:27:37,559 --> 00:27:43,650
properly and if you especially if you

00:27:39,990 --> 00:27:45,900
avoid overlapping changes but it's not

00:27:43,650 --> 00:27:51,960
the subject of this talk to go into that

00:27:45,900 --> 00:27:56,640
in more detail anything else well yes

00:27:51,960 --> 00:28:00,750
what many people are not aware a really

00:27:56,640 --> 00:28:04,320
big installation of replication MySQL

00:28:00,750 --> 00:28:07,049
replication is booking.com and with big

00:28:04,320 --> 00:28:10,650
installation I mean booking comm has a

00:28:07,049 --> 00:28:14,549
mass has master instances to which some

00:28:10,650 --> 00:28:16,530
100 slaves are connected not on the

00:28:14,549 --> 00:28:19,710
first level but on second or even third

00:28:16,530 --> 00:28:23,760
level but they really serve 100 slaves

00:28:19,710 --> 00:28:27,539
from one master because they need to

00:28:23,760 --> 00:28:29,170
support such I read a load when people

00:28:27,539 --> 00:28:34,330
are looking for Akuma

00:28:29,170 --> 00:28:36,460
proposals and if you want to go into

00:28:34,330 --> 00:28:40,060
replication and replication architecture

00:28:36,460 --> 00:28:42,930
in more detail and read about what's

00:28:40,060 --> 00:28:46,150
possible even more complicated

00:28:42,930 --> 00:28:49,720
architectures than just shown then I

00:28:46,150 --> 00:28:52,380
recommend a block or series of blog

00:28:49,720 --> 00:28:55,480
entries written by Jose Eber maxia

00:28:52,380 --> 00:28:58,390
August last year because he has shown

00:28:55,480 --> 00:29:01,330
several replication architectures and

00:28:58,390 --> 00:29:04,150
how to do it what to avoid and stuff

00:29:01,330 --> 00:29:06,180
like this recommended reading for those

00:29:04,150 --> 00:29:11,260
who want to do more than a single

00:29:06,180 --> 00:29:16,710
master-slave replication so much about

00:29:11,260 --> 00:29:16,710
replication any general any questions

00:29:21,710 --> 00:29:24,710
yeah

00:29:32,489 --> 00:29:41,320
you know the easiest way is to skip the

00:29:38,609 --> 00:29:44,830
transaction that did not apply on the

00:29:41,320 --> 00:29:48,700
slave but if of course it depends on the

00:29:44,830 --> 00:29:50,830
under on the cause for the trouble so

00:29:48,700 --> 00:29:54,070
the typical example would be duplicate

00:29:50,830 --> 00:29:56,349
key and the new insert will fail well a

00:29:54,070 --> 00:29:59,229
thousand met doesn't really matter that

00:29:56,349 --> 00:30:02,559
much because the record is already

00:29:59,229 --> 00:30:05,289
present of course you can ask both the

00:30:02,559 --> 00:30:07,239
slave and the master directly do you

00:30:05,289 --> 00:30:10,089
have that record what does it look like

00:30:07,239 --> 00:30:18,849
if it's the same already you would skip

00:30:10,089 --> 00:30:21,489
the insert if it's different yes if you

00:30:18,849 --> 00:30:24,820
skip the offending transaction and then

00:30:21,489 --> 00:30:29,940
let replication continue it will resume

00:30:24,820 --> 00:30:33,580
it will proceed as fast as possible and

00:30:29,940 --> 00:30:35,739
assuming that you are not operating your

00:30:33,580 --> 00:30:40,719
machine at the limit it will sooner or

00:30:35,739 --> 00:30:43,450
later keep have have got have processed

00:30:40,719 --> 00:30:46,809
the delay and then they will be in sync

00:30:43,450 --> 00:30:50,379
as far as replication is in sync there

00:30:46,809 --> 00:31:02,969
is always a small delay but should not

00:30:50,379 --> 00:31:02,969
matter in that

00:31:07,250 --> 00:31:12,000
and that came through your best chat

00:31:10,080 --> 00:31:15,269
with me as you please repeat the

00:31:12,000 --> 00:31:18,059
question okay if if it's not just a

00:31:15,269 --> 00:31:20,940
single row that's causing the conflict

00:31:18,059 --> 00:31:24,750
but if say a complete table was dropped

00:31:20,940 --> 00:31:28,080
on the slave by accident then you've got

00:31:24,750 --> 00:31:31,080
two things to do the one is to make sure

00:31:28,080 --> 00:31:33,570
that doesn't happen again by controlling

00:31:31,080 --> 00:31:36,210
your slavery only or revoking privileges

00:31:33,570 --> 00:31:39,059
or educating your users or whatever is

00:31:36,210 --> 00:31:42,450
appropriate and the other is you would

00:31:39,059 --> 00:31:44,639
really have to set it up again if a

00:31:42,450 --> 00:31:48,240
table was dropped on the slave you would

00:31:44,639 --> 00:31:50,820
at least need to transfer that table

00:31:48,240 --> 00:31:53,250
again and then start resuming of

00:31:50,820 --> 00:31:57,350
application which is of course a problem

00:31:53,250 --> 00:32:00,450
it may be a you might he would prop

00:31:57,350 --> 00:32:02,730
transfer the new state contents of the

00:32:00,450 --> 00:32:05,220
table to the slave which might be

00:32:02,730 --> 00:32:08,010
changed compared to the moment it was

00:32:05,220 --> 00:32:11,789
originally exist and said etc etc so

00:32:08,010 --> 00:32:14,610
this is not a trivial thing the savers

00:32:11,789 --> 00:32:16,940
but on the other hand also the slowest

00:32:14,610 --> 00:32:19,679
approach would be to do a total recovery

00:32:16,940 --> 00:32:22,110
transfer snapshot from the master to the

00:32:19,679 --> 00:32:24,389
slave together with a bin lock position

00:32:22,110 --> 00:32:27,240
which is valid at the moment this

00:32:24,389 --> 00:32:30,240
snapshot was taken and let replication

00:32:27,240 --> 00:32:32,639
resumed from that that's possible but it

00:32:30,240 --> 00:32:36,029
takes some time transferring an

00:32:32,639 --> 00:32:38,630
individual table as faster but a bit

00:32:36,029 --> 00:32:38,630
more complicated

00:32:43,590 --> 00:32:51,850
so there's no metrics which that assures

00:32:46,360 --> 00:32:53,620
that you just tell the the slave to roll

00:32:51,850 --> 00:32:57,250
up to the same state as the master so

00:32:53,620 --> 00:33:00,070
fix all differences or something you

00:32:57,250 --> 00:33:03,370
can't do that because if the slave does

00:33:00,070 --> 00:33:05,970
not have the original data you don't

00:33:03,370 --> 00:33:09,160
have a way to do that there are

00:33:05,970 --> 00:33:11,200
third-party tools like the percona tools

00:33:09,160 --> 00:33:14,680
would help which would help you were

00:33:11,200 --> 00:33:17,470
doing it but using them and then

00:33:14,680 --> 00:33:20,680
resuming your application isn't isn't

00:33:17,470 --> 00:33:24,970
any easier the problem is you would get

00:33:20,680 --> 00:33:27,670
the new contents of the master and put

00:33:24,970 --> 00:33:31,810
it into a point in a replication history

00:33:27,670 --> 00:33:37,690
which is some some time back and so he

00:33:31,810 --> 00:33:39,790
would he would put new contents at the

00:33:37,690 --> 00:33:43,570
place where old commands are applied and

00:33:39,790 --> 00:33:48,070
this is will again introduce at the risk

00:33:43,570 --> 00:33:50,410
of replication breaking but that's why

00:33:48,070 --> 00:33:53,920
it is so important to configure your

00:33:50,410 --> 00:33:57,220
slave instance to be read-only and with

00:33:53,920 --> 00:34:00,280
MySQL 5.7 there is eveness which was

00:33:57,220 --> 00:34:03,340
calling it a super read-only which means

00:34:00,280 --> 00:34:06,490
even a user who has got super privileges

00:34:03,340 --> 00:34:10,740
cannot modify data on the slave so

00:34:06,490 --> 00:34:10,740
that's maybe important

00:34:27,220 --> 00:34:33,230
the fastest way is to use your

00:34:30,710 --> 00:34:37,070
networking functions you would assign a

00:34:33,230 --> 00:34:39,290
virtual IP address and configure either

00:34:37,070 --> 00:34:41,270
the one or the other master node to

00:34:39,290 --> 00:34:43,400
react to it to respond to that IP

00:34:41,270 --> 00:34:45,770
address and your clients would use the

00:34:43,400 --> 00:34:53,990
virtual IP address that's the fastest

00:34:45,770 --> 00:34:57,770
way of doing it okay so that's about

00:34:53,990 --> 00:35:01,280
replication now let's continue to the

00:34:57,770 --> 00:35:04,490
other approach as I said replication

00:35:01,280 --> 00:35:07,910
does have some drawbacks the first is

00:35:04,490 --> 00:35:10,400
it's a third corners the second is it's

00:35:07,910 --> 00:35:12,170
asymmetrical you would configure at

00:35:10,400 --> 00:35:14,600
least one of the nodes to be read-only

00:35:12,170 --> 00:35:18,560
even if you are running a master master

00:35:14,600 --> 00:35:21,380
replication so you would make sure that

00:35:18,560 --> 00:35:24,440
only on one node right commands are

00:35:21,380 --> 00:35:27,470
processed because if you don't do that

00:35:24,440 --> 00:35:30,010
and write on both nodes on multiple

00:35:27,470 --> 00:35:34,450
nodes in parallel you may cause breakage

00:35:30,010 --> 00:35:38,570
depending on which data are generated if

00:35:34,450 --> 00:35:41,690
one of the nodes crashes then for high

00:35:38,570 --> 00:35:44,690
availability you will need to do some

00:35:41,690 --> 00:35:47,030
fail over so that declines use the other

00:35:44,690 --> 00:35:49,340
node as we just discussed say by

00:35:47,030 --> 00:35:51,380
switching the virtual IP but might be

00:35:49,340 --> 00:35:53,210
any other situation depending on which

00:35:51,380 --> 00:35:55,880
node is crashing if your slave is

00:35:53,210 --> 00:35:58,340
crashing the clients need not switch but

00:35:55,880 --> 00:36:00,530
you might want to set up another slave

00:35:58,340 --> 00:36:02,840
or change your application architecture

00:36:00,530 --> 00:36:05,710
if it's a bit more than a single master

00:36:02,840 --> 00:36:09,410
with a single slave in any case each

00:36:05,710 --> 00:36:12,650
individual node is a single point of

00:36:09,410 --> 00:36:18,260
failure for the slaves connected to that

00:36:12,650 --> 00:36:21,650
node so if a single node that has Scott

00:36:18,260 --> 00:36:23,750
slaves breaks down then you will need to

00:36:21,650 --> 00:36:27,130
do some change in the structure of

00:36:23,750 --> 00:36:29,650
replication and doing such change

00:36:27,130 --> 00:36:33,550
dynamically is a bit complicated because

00:36:29,650 --> 00:36:37,870
you need to handle the new setup point

00:36:33,550 --> 00:36:43,420
of replication etc etc so it is possible

00:36:37,870 --> 00:36:46,630
it can be done it is done sorry it is

00:36:43,420 --> 00:36:51,640
done adjust consider booking comm but it

00:36:46,630 --> 00:36:55,420
may be complicated we would like to have

00:36:51,640 --> 00:36:57,400
something better definitely we always

00:36:55,420 --> 00:37:01,150
have whooshes we always want to improve

00:36:57,400 --> 00:37:03,940
or want the situation to improve so we

00:37:01,150 --> 00:37:07,210
would prefer if the system were

00:37:03,940 --> 00:37:10,690
operating synchronously about across our

00:37:07,210 --> 00:37:14,290
notes if it was symmetrical and we could

00:37:10,690 --> 00:37:16,780
write on any note we would prefer if the

00:37:14,290 --> 00:37:19,000
system would handle the con or would

00:37:16,780 --> 00:37:23,260
analyze and handle conflicts and not

00:37:19,000 --> 00:37:25,720
leave it to the DBA we want high

00:37:23,260 --> 00:37:27,550
availability to be reached by the system

00:37:25,720 --> 00:37:30,850
just continuing to work without any

00:37:27,550 --> 00:37:34,620
structure change and we want the system

00:37:30,850 --> 00:37:38,400
to handle the entry or exit of notes

00:37:34,620 --> 00:37:43,660
dynamically without the DBA intervening

00:37:38,400 --> 00:37:48,310
as I said we always have wishes enter

00:37:43,660 --> 00:37:51,490
Galliera cluster galava cluster is again

00:37:48,310 --> 00:37:54,520
a client-server situation you have

00:37:51,490 --> 00:37:57,730
applications connecting through some

00:37:54,520 --> 00:38:01,840
network typically by a load balancing

00:37:57,730 --> 00:38:05,560
mechanism to several nodes of mysql

00:38:01,840 --> 00:38:07,990
servers you can have the load balancing

00:38:05,560 --> 00:38:09,400
mechanism with a single or with a

00:38:07,990 --> 00:38:13,570
traditional master-slave replication

00:38:09,400 --> 00:38:15,370
also of course but with Calleja cluster

00:38:13,570 --> 00:38:17,200
it's pretty essential because Galera

00:38:15,370 --> 00:38:19,990
cluster as high a way is about high

00:38:17,200 --> 00:38:24,700
availability and you want to avoid

00:38:19,990 --> 00:38:26,560
manual action in case one node breaks

00:38:24,700 --> 00:38:28,960
down so you want to have an automated

00:38:26,560 --> 00:38:32,050
load balancing or switchover mechanism

00:38:28,960 --> 00:38:36,820
that's why I have it in this picture but

00:38:32,050 --> 00:38:40,970
did not have it before now every node

00:38:36,820 --> 00:38:45,140
has its own local disk and

00:38:40,970 --> 00:38:49,520
do not share anything what they have

00:38:45,140 --> 00:38:53,410
however is a replication layer which

00:38:49,520 --> 00:38:57,500
connects the code in all those nodes and

00:38:53,410 --> 00:39:01,609
whenever any of the nodes is processing

00:38:57,500 --> 00:39:05,150
a data change then it does communicate

00:39:01,609 --> 00:39:09,560
with its fellow nodes whether that

00:39:05,150 --> 00:39:12,080
change is possible or not because as I

00:39:09,560 --> 00:39:14,869
wrote on the previous slide we want to

00:39:12,080 --> 00:39:17,930
have automated call conflict analysis on

00:39:14,869 --> 00:39:21,830
conflict handling and this is why we

00:39:17,930 --> 00:39:25,280
need this communication here preferably

00:39:21,830 --> 00:39:27,619
we would use a dedicated network just

00:39:25,280 --> 00:39:29,750
for the backbone so that it does not

00:39:27,619 --> 00:39:33,380
suffer from overload when too many

00:39:29,750 --> 00:39:35,990
clients connect etc etc but that's more

00:39:33,380 --> 00:39:39,400
of performance decision and architecture

00:39:35,990 --> 00:39:43,070
decision not needed on principle you

00:39:39,400 --> 00:39:47,060
know and as I said it's shared-nothing

00:39:43,070 --> 00:39:50,839
although these are functionally local to

00:39:47,060 --> 00:39:55,160
the notes it is symmetrical in that all

00:39:50,839 --> 00:39:57,589
nodes hold all data it's not a split of

00:39:55,160 --> 00:40:06,140
the data contents across several nodes

00:39:57,589 --> 00:40:11,119
it's a fully symmetrical approach now as

00:40:06,140 --> 00:40:13,580
I said it is based on nodb because for

00:40:11,119 --> 00:40:16,609
conflict handling potential conflict

00:40:13,580 --> 00:40:19,310
handling it must be a transaction based

00:40:16,609 --> 00:40:22,790
tabling engine and my azam does not

00:40:19,310 --> 00:40:28,820
handle transactions so the data must be

00:40:22,790 --> 00:40:31,190
kept in a no DV it also transfers any

00:40:28,820 --> 00:40:35,240
changes done to the users to the

00:40:31,190 --> 00:40:37,550
privileges etc these are as several of

00:40:35,240 --> 00:40:41,869
you know still kept in my eyes and

00:40:37,550 --> 00:40:44,869
database but they are transferred via

00:40:41,869 --> 00:40:49,099
the by command level so the command

00:40:44,869 --> 00:40:53,530
modifying users grant or Evo create user

00:40:49,099 --> 00:40:53,530
abuser etc these commands are replicated

00:40:53,950 --> 00:41:03,099
the transfer of changes happens on

00:40:58,119 --> 00:41:06,000
commit only not before it is nearly

00:41:03,099 --> 00:41:10,930
synchronous and it is highly efficient

00:41:06,000 --> 00:41:14,829
as a result the whole cluster Galliera

00:41:10,930 --> 00:41:17,349
cluster provides high availability in a

00:41:14,829 --> 00:41:21,700
symmetrical approach as I said all nodes

00:41:17,349 --> 00:41:23,950
hold all data you don't lose

00:41:21,700 --> 00:41:26,349
transactions even if you have forgot the

00:41:23,950 --> 00:41:28,420
conflict ok the victim transaction of

00:41:26,349 --> 00:41:30,819
course is rolled back but any

00:41:28,420 --> 00:41:33,030
transaction that is successful is

00:41:30,819 --> 00:41:35,770
applied on all nodes and the cluster

00:41:33,030 --> 00:41:39,190
even if one of the nodes is currently

00:41:35,770 --> 00:41:44,410
down or out of reach it will later apply

00:41:39,190 --> 00:41:48,520
the transaction to again the cluster

00:41:44,410 --> 00:41:53,170
approach brings you reach scale out and

00:41:48,520 --> 00:41:55,720
it also slightly increases the white the

00:41:53,170 --> 00:41:59,490
right performance note that I do not use

00:41:55,720 --> 00:42:02,349
the term right scale out anybody who

00:41:59,490 --> 00:42:05,260
would promise you're right scale out in

00:42:02,349 --> 00:42:08,290
a Galliera cluster would be lying the

00:42:05,260 --> 00:42:10,960
problem is every node holds all data so

00:42:08,290 --> 00:42:14,109
every node must process all right

00:42:10,960 --> 00:42:17,589
commands and if you have say dozens of

00:42:14,109 --> 00:42:20,319
nodes obviously you can't have every

00:42:17,589 --> 00:42:23,290
node process right commands at full

00:42:20,319 --> 00:42:25,180
speed and then irately also do the right

00:42:23,290 --> 00:42:26,980
commands off of the other nodes

00:42:25,180 --> 00:42:31,119
that's impossible somewhere there's a

00:42:26,980 --> 00:42:31,810
limit what your hardware can do and

00:42:31,119 --> 00:42:35,050
galia

00:42:31,810 --> 00:42:37,500
dance dynamically handle the entry or

00:42:35,050 --> 00:42:41,619
the addition or the removal of notes and

00:42:37,500 --> 00:42:44,829
Kalia dance take care of the necessary

00:42:41,619 --> 00:42:46,510
data transfer that's important you do

00:42:44,829 --> 00:42:49,839
not need the DBA

00:42:46,510 --> 00:42:53,609
to manually handle the initial data

00:42:49,839 --> 00:42:57,900
initial setup of an of a new node that's

00:42:53,609 --> 00:42:57,900
automatically done by Galia

00:42:58,840 --> 00:43:04,940
the typical question which people ask is

00:43:02,210 --> 00:43:09,940
how can it be so fast how can it be

00:43:04,940 --> 00:43:14,270
nearly synchronous well as I said as

00:43:09,940 --> 00:43:17,290
long as a transaction is executing

00:43:14,270 --> 00:43:21,260
individual commands insert update delete

00:43:17,290 --> 00:43:23,690
there are handled locally on the single

00:43:21,260 --> 00:43:27,830
node to which the trends the client is

00:43:23,690 --> 00:43:30,230
connected and eventually the trend the

00:43:27,830 --> 00:43:34,640
client will issue a commit comma command

00:43:30,230 --> 00:43:37,730
and in that moment this node or the

00:43:34,640 --> 00:43:40,400
software on this node will send the

00:43:37,730 --> 00:43:46,880
whole transaction event to all other

00:43:40,400 --> 00:43:52,130
nodes in the cluster for this it uses

00:43:46,880 --> 00:43:54,470
the bid lock code not by writing the bin

00:43:52,130 --> 00:43:56,540
log part by at least creating bin log

00:43:54,470 --> 00:43:57,410
entries that's why you have to configure

00:43:56,540 --> 00:44:00,920
bin locked

00:43:57,410 --> 00:44:04,430
former to row in a kalila cluster but

00:44:00,920 --> 00:44:06,890
they are not it is not using the typical

00:44:04,430 --> 00:44:09,710
mechanism of writing the bin lock to

00:44:06,890 --> 00:44:12,170
file and sending it to the other node

00:44:09,710 --> 00:44:17,690
and writing to relay log etc but what

00:44:12,170 --> 00:44:21,590
always causes those delays how but the

00:44:17,690 --> 00:44:25,070
bid knock entry is transported across

00:44:21,590 --> 00:44:28,790
the network say as as piggyback on the

00:44:25,070 --> 00:44:32,930
commit note and the other node is now

00:44:28,790 --> 00:44:36,619
checking do I have any change that's in

00:44:32,930 --> 00:44:40,580
conflict this check can be done pretty

00:44:36,619 --> 00:44:44,930
cheap because it's done by checking the

00:44:40,580 --> 00:44:48,140
primary keys so the tables involved need

00:44:44,930 --> 00:44:50,869
to have a primary key and in the commit

00:44:48,140 --> 00:44:53,480
entry the primary key of all modified

00:44:50,869 --> 00:44:57,050
rows is transferred and the other node

00:44:53,480 --> 00:44:59,930
checks did do I have any local command

00:44:57,050 --> 00:45:02,359
modifying which just more any trans

00:44:59,930 --> 00:45:07,480
local transaction which already modified

00:45:02,359 --> 00:45:11,410
them let's assume it it not in that case

00:45:07,480 --> 00:45:16,250
the node replies with an okay

00:45:11,410 --> 00:45:19,640
message the originating node does the

00:45:16,250 --> 00:45:23,390
commit the transaction is finished the

00:45:19,640 --> 00:45:26,720
client is informed the other nodes apply

00:45:23,390 --> 00:45:29,869
the bin lock entry which they got here

00:45:26,720 --> 00:45:32,180
piggybacked which is takes a bit more

00:45:29,869 --> 00:45:35,780
than the commit so you have got very

00:45:32,180 --> 00:45:39,320
small time window in which the nodes on

00:45:35,780 --> 00:45:41,119
to not have the same contents and after

00:45:39,320 --> 00:45:43,250
that they are answering this is

00:45:41,119 --> 00:45:46,310
definitely much shorter at this time

00:45:43,250 --> 00:45:50,869
then typically the bin Locker

00:45:46,310 --> 00:45:53,840
application would need and after that

00:45:50,869 --> 00:45:57,200
they are in sync and the transaction has

00:45:53,840 --> 00:46:01,930
been applied on all notes you see the

00:45:57,200 --> 00:46:01,930
source I did not draw the picture myself

00:46:05,020 --> 00:46:14,840
well nothing is really perfect however

00:46:09,230 --> 00:46:15,560
much we want it to be so there are some

00:46:14,840 --> 00:46:18,650
drawbacks

00:46:15,560 --> 00:46:22,090
first of all Galera is an addition to

00:46:18,650 --> 00:46:27,950
the original MySQL sources which also

00:46:22,090 --> 00:46:31,220
means some delay currently MySQL 5.7 is

00:46:27,950 --> 00:46:33,950
GA is production quality as declared by

00:46:31,220 --> 00:46:37,220
Oracle but there is not yet a girli are

00:46:33,950 --> 00:46:39,950
based on MySQL 5.7 available that's one

00:46:37,220 --> 00:46:41,030
of the reasons why you is 5.6 as the

00:46:39,950 --> 00:46:46,450
base for this talk

00:46:41,030 --> 00:46:46,450
Galia on mysql 5.7 is work in progress

00:46:46,810 --> 00:46:57,140
the second is if you have really very

00:46:52,790 --> 00:46:59,090
many modifications to a small number of

00:46:57,140 --> 00:47:03,260
rows you will have a high number of

00:46:59,090 --> 00:47:06,740
conflicts now when there is a conflict

00:47:03,260 --> 00:47:09,380
of transactions the victim transaction

00:47:06,740 --> 00:47:14,660
will be rolled back and must be repeated

00:47:09,380 --> 00:47:17,119
which means work is lost to avoid that

00:47:14,660 --> 00:47:20,840
it's highly preferable that if you have

00:47:17,119 --> 00:47:23,600
hotspots you only always modify them on

00:47:20,840 --> 00:47:24,170
the same note because then the local

00:47:23,600 --> 00:47:27,620
locking

00:47:24,170 --> 00:47:30,130
in ODB will help and this is much

00:47:27,620 --> 00:47:32,510
cheaper and much faster than a

00:47:30,130 --> 00:47:38,240
distributed conflict and the rollback of

00:47:32,510 --> 00:47:43,400
the whole transaction even you need at

00:47:38,240 --> 00:47:46,160
least three notes and if your database

00:47:43,400 --> 00:47:48,740
is really large then the initial set up

00:47:46,160 --> 00:47:50,510
will take some time well it will take

00:47:48,740 --> 00:47:57,230
the similar time of replication no

00:47:50,510 --> 00:48:07,400
difference there let's again look a bit

00:47:57,230 --> 00:48:11,210
at the flow of control the client

00:48:07,400 --> 00:48:15,430
accesses one server sends the commits

00:48:11,210 --> 00:48:19,580
finally this is replicated to all hovers

00:48:15,430 --> 00:48:22,370
all notes to the what Collier our code

00:48:19,580 --> 00:48:25,820
ership the company called certification

00:48:22,370 --> 00:48:29,090
and it is guaranteed that all nodes will

00:48:25,820 --> 00:48:32,660
come to the same result so either it is

00:48:29,090 --> 00:48:34,550
okay which means on the originating now

00:48:32,660 --> 00:48:36,650
the commit will be done and on the

00:48:34,550 --> 00:48:39,440
others it will be applied and committed

00:48:36,650 --> 00:48:42,320
or it is not okay which means it will be

00:48:39,440 --> 00:48:45,920
rolled back on the originating node and

00:48:42,320 --> 00:48:52,880
it will just be ignored on any other

00:48:45,920 --> 00:48:53,270
node so much about Galliera at a high

00:48:52,880 --> 00:48:58,520
level

00:48:53,270 --> 00:49:02,690
Collier a cluster now let's compare the

00:48:58,520 --> 00:49:05,090
two approaches because both provide or

00:49:02,690 --> 00:49:06,430
want to provide similar features they

00:49:05,090 --> 00:49:10,490
both want to provide high availability

00:49:06,430 --> 00:49:13,400
and they both want to provide read scale

00:49:10,490 --> 00:49:15,770
out so from that point of view they are

00:49:13,400 --> 00:49:20,960
functionally pretty similar to each

00:49:15,770 --> 00:49:23,060
other however there are the details as I

00:49:20,960 --> 00:49:26,180
said both provide redundancy both

00:49:23,060 --> 00:49:29,000
provide high availability scale-out both

00:49:26,180 --> 00:49:31,160
provide instances which you can use

00:49:29,000 --> 00:49:33,770
dedicated for reports for analysis for

00:49:31,160 --> 00:49:37,900
backups and both make the data available

00:49:33,770 --> 00:49:37,900
locally save for branch offices

00:49:38,260 --> 00:49:46,280
now what's the difference replication is

00:49:42,680 --> 00:49:48,890
standard Galia is an add-on application

00:49:46,280 --> 00:49:53,270
can use all table engines Galera is

00:49:48,890 --> 00:49:56,120
limited to in OTB application is upwards

00:49:53,270 --> 00:49:59,560
compatible for one version saying 5.5 to

00:49:56,120 --> 00:50:02,420
5.6 or five six two five seven Galera

00:49:59,560 --> 00:50:05,180
prefers to have the same versions on all

00:50:02,420 --> 00:50:08,840
notes except for the moment of an

00:50:05,180 --> 00:50:12,070
upgrade of course replication starts

00:50:08,840 --> 00:50:15,020
from two notes Galaga from three

00:50:12,070 --> 00:50:17,570
replication does fail over Galliera

00:50:15,020 --> 00:50:21,710
provides high availability without

00:50:17,570 --> 00:50:22,930
changes communication is slightly

00:50:21,710 --> 00:50:26,570
different

00:50:22,930 --> 00:50:28,670
especially Galera cannot filter it's

00:50:26,570 --> 00:50:37,180
always complete it's always a fully

00:50:28,670 --> 00:50:39,980
symmetrical situation when you have

00:50:37,180 --> 00:50:44,710
several notes you have slightly

00:50:39,980 --> 00:50:48,680
increased worid capacity beyond Galera

00:50:44,710 --> 00:50:52,790
if all rights go to a single master then

00:50:48,680 --> 00:50:54,950
you in any case local conflicts will be

00:50:52,790 --> 00:50:57,860
handled by nodb the statement will

00:50:54,950 --> 00:51:00,560
return an error that statement will fail

00:50:57,860 --> 00:51:02,750
the a client will do it different or

00:51:00,560 --> 00:51:08,660
repeat or whatever's appropriate that's

00:51:02,750 --> 00:51:11,750
fine if you write to several notes in

00:51:08,660 --> 00:51:15,410
parallel you may have a distributed

00:51:11,750 --> 00:51:18,410
conflict Galera will have all that fine

00:51:15,410 --> 00:51:20,770
it will do a rollback for all but one of

00:51:18,410 --> 00:51:25,190
the conflicting transactions

00:51:20,770 --> 00:51:27,940
replication will break down so that's

00:51:25,190 --> 00:51:31,370
one of the very important differences

00:51:27,940 --> 00:51:36,680
Galera does handle a distributed

00:51:31,370 --> 00:51:39,290
conflict without the DBA intervening now

00:51:36,680 --> 00:51:42,350
assume your communication is broken for

00:51:39,290 --> 00:51:46,760
whatever reason network are capable

00:51:42,350 --> 00:51:51,320
stone or whatever in any case after a

00:51:46,760 --> 00:51:54,020
short interruption replication resumes

00:51:51,320 --> 00:51:57,770
Galia does what's called an incremental

00:51:54,020 --> 00:52:00,920
transfer it transfers the changes that

00:51:57,770 --> 00:52:04,810
have accumulated in the meantime no real

00:52:00,920 --> 00:52:09,260
difference with a long interruption

00:52:04,810 --> 00:52:12,710
replication still resumes but Galera may

00:52:09,260 --> 00:52:16,850
need a full transfer a snapshot a state

00:52:12,710 --> 00:52:20,330
transfer if it's cash for the recent

00:52:16,850 --> 00:52:24,980
changes has been exhausted so with a

00:52:20,330 --> 00:52:29,140
really long interruption Galliera may

00:52:24,980 --> 00:52:29,140
may need some time to resume operation

00:52:33,340 --> 00:52:39,020
it depends on you on on your note if you

00:52:36,890 --> 00:52:40,880
say if you have one insert per hour you

00:52:39,020 --> 00:52:42,800
can you can have and all that

00:52:40,880 --> 00:52:45,440
interruption for days maybe even for

00:52:42,800 --> 00:52:47,690
week if you have if you've got a

00:52:45,440 --> 00:52:49,670
thousand inserts per second the things

00:52:47,690 --> 00:52:52,700
are different so there is a formula

00:52:49,670 --> 00:52:55,360
available on our web that shows how much

00:52:52,700 --> 00:52:58,760
you would have to configure your Calera

00:52:55,360 --> 00:53:06,410
change cash in order to survive which

00:52:58,760 --> 00:53:10,280
which on which time of breakdown when

00:53:06,410 --> 00:53:13,550
you add notes or remove notes Galia does

00:53:10,280 --> 00:53:16,310
handle that automatically if you add a

00:53:13,550 --> 00:53:19,520
note Galia will automatically do the

00:53:16,310 --> 00:53:23,890
full transfer yes it takes some time but

00:53:19,520 --> 00:53:26,780
it works without the DBA doing it and

00:53:23,890 --> 00:53:29,720
depending on the transfer method one of

00:53:26,780 --> 00:53:32,690
the other note providing the data may be

00:53:29,720 --> 00:53:35,480
blocked that's also one of the reasons

00:53:32,690 --> 00:53:40,100
why you need at least three notes the

00:53:35,480 --> 00:53:43,820
one reason is that all cluster solutions

00:53:40,100 --> 00:53:46,760
including Galera operate by some way of

00:53:43,820 --> 00:53:49,850
majority consensus of voting so of two

00:53:46,760 --> 00:53:51,830
nodes one may fall down the other two

00:53:49,850 --> 00:53:53,150
will still see each other they know we

00:53:51,830 --> 00:53:56,330
are two of three we have got the

00:53:53,150 --> 00:54:00,440
majority we may continue and the other

00:53:56,330 --> 00:54:02,930
reason if is if that failed node comes

00:54:00,440 --> 00:54:04,880
up again and needs a full transfer it

00:54:02,930 --> 00:54:06,890
will ask one

00:54:04,880 --> 00:54:09,829
the other note notes to provide the data

00:54:06,890 --> 00:54:11,599
this may block the other note depending

00:54:09,829 --> 00:54:13,940
on the method chosen you can configure

00:54:11,599 --> 00:54:17,450
it I won't handle that in detail here

00:54:13,940 --> 00:54:19,789
and if only the third note will remain

00:54:17,450 --> 00:54:22,789
available for clients so the system will

00:54:19,789 --> 00:54:30,579
continue to work but of course at

00:54:22,789 --> 00:54:34,789
reduced capacity you know any

00:54:30,579 --> 00:54:37,490
distributed solution is limited by

00:54:34,789 --> 00:54:40,339
what's known as the cap theorem be it

00:54:37,490 --> 00:54:42,470
replication be it Galliera cluster you

00:54:40,339 --> 00:54:46,430
can't have all good properties at the

00:54:42,470 --> 00:54:50,089
same time just neither neither

00:54:46,430 --> 00:54:54,769
replication nor Galera can win against

00:54:50,089 --> 00:54:58,099
that as I said - curve 5/7 is in

00:54:54,769 --> 00:55:02,480
progress as as far as ghanima contour is

00:54:58,099 --> 00:55:06,529
concerned replication is getting some

00:55:02,480 --> 00:55:08,359
new features Oracle is working on group

00:55:06,529 --> 00:55:12,019
replication which it is the effect

00:55:08,359 --> 00:55:14,299
though repeating Galia cluster work and

00:55:12,019 --> 00:55:17,930
doing it an Oracle but Galera is

00:55:14,299 --> 00:55:19,700
definitely more advanced than a group

00:55:17,930 --> 00:55:21,680
application group replication for

00:55:19,700 --> 00:55:26,359
example does not yet handle schema

00:55:21,680 --> 00:55:32,380
changes okay I promised you a comparison

00:55:26,359 --> 00:55:39,500
or some examples when what not to choose

00:55:32,380 --> 00:55:43,009
as I said Galia is working by majority

00:55:39,500 --> 00:55:47,480
principle also called quorum which means

00:55:43,009 --> 00:55:51,079
if see if a note is isolated and knows

00:55:47,480 --> 00:55:54,589
I'm one and we were three before well

00:55:51,079 --> 00:55:58,490
one cannot win against the other two my

00:55:54,589 --> 00:56:01,579
data may be stale I will stop working by

00:55:58,490 --> 00:56:04,759
design intentionally it will not even

00:56:01,579 --> 00:56:07,819
provide data because they might be a

00:56:04,759 --> 00:56:09,980
would and obsolete you can change that

00:56:07,819 --> 00:56:12,789
by configuration but it's rare that this

00:56:09,980 --> 00:56:12,789
is a good approach

00:56:13,070 --> 00:56:23,180
and of course yeah when that isolation

00:56:19,690 --> 00:56:25,220
holds for too long then there is a risk

00:56:23,180 --> 00:56:27,710
that the system might switch to the

00:56:25,220 --> 00:56:32,330
snapshot transfer which is a slow

00:56:27,710 --> 00:56:35,750
operation for large data replication is

00:56:32,330 --> 00:56:39,320
different replication just accumulate

00:56:35,750 --> 00:56:42,230
some log segments and this slave will

00:56:39,320 --> 00:56:45,680
eventually ask for them which means

00:56:42,230 --> 00:56:48,560
replication is more tolerant as far as

00:56:45,680 --> 00:56:51,740
communication failure is concerned then

00:56:48,560 --> 00:56:56,290
Galliera cluster it's easier because it

00:56:51,740 --> 00:57:01,880
will in general not need full transfer

00:56:56,290 --> 00:57:05,120
as I said replication can do filtering a

00:57:01,880 --> 00:57:08,230
real-world example we have a customer

00:57:05,120 --> 00:57:11,470
with head offices in Germany and

00:57:08,230 --> 00:57:15,830
factories distributed across the world

00:57:11,470 --> 00:57:20,240
and they want the orders to be given to

00:57:15,830 --> 00:57:22,940
the factory by database means but each

00:57:20,240 --> 00:57:26,570
factory should only receive the orders

00:57:22,940 --> 00:57:30,020
intended for that factory you can do

00:57:26,570 --> 00:57:32,000
that by replicating and by filtering you

00:57:30,020 --> 00:57:35,660
cannot do that in Galia because gelila

00:57:32,000 --> 00:57:38,330
is always symmetrical so if you want to

00:57:35,660 --> 00:57:41,270
have such a selective propagation

00:57:38,330 --> 00:57:47,030
selective transfer you need traditional

00:57:41,270 --> 00:57:52,310
replication and you will add filters on

00:57:47,030 --> 00:57:55,460
the other hand if you have a conflict as

00:57:52,310 --> 00:57:58,100
I explained before replication will need

00:57:55,460 --> 00:58:03,670
administrator action Galera will handle

00:57:58,100 --> 00:58:03,670
it all by itself advantage Calera

00:58:03,760 --> 00:58:11,840
hotspot whether you are using a

00:58:07,970 --> 00:58:15,050
replication or Galera make sure that all

00:58:11,840 --> 00:58:17,480
changes to to what data are done on the

00:58:15,050 --> 00:58:21,070
same node because then you will profit

00:58:17,480 --> 00:58:24,260
from inner dbe's transaction facilities

00:58:21,070 --> 00:58:26,720
local role or row level locking etc

00:58:24,260 --> 00:58:29,330
cetera it's definitely much faster than

00:58:26,720 --> 00:58:31,700
having a distributed conflict especially

00:58:29,330 --> 00:58:35,990
than having a conflict which the DBA

00:58:31,700 --> 00:58:40,130
would resolve high-availability

00:58:35,990 --> 00:58:44,930
as I said Galera will handle it all by

00:58:40,130 --> 00:58:49,390
themselves by itself and it it is faster

00:58:44,930 --> 00:58:52,250
and replicating and it does not need a

00:58:49,390 --> 00:58:54,890
change of the master/slave communication

00:58:52,250 --> 00:58:59,660
structure so it's easier to have high

00:58:54,890 --> 00:59:02,630
availability in Galliera provided your

00:58:59,660 --> 00:59:07,810
setup is suitable say you don't want to

00:59:02,630 --> 00:59:11,900
fader no that's the high level

00:59:07,810 --> 00:59:15,440
comparison when to use which ought to

00:59:11,900 --> 00:59:21,800
avoid which and that's what I prepared

00:59:15,440 --> 00:59:23,750
for you any questions we are running out

00:59:21,800 --> 00:59:27,440
of time so we don't have really time for

00:59:23,750 --> 00:59:30,400
medication what's the license i'm valera

00:59:27,440 --> 00:59:30,400
GPL

00:59:37,820 --> 00:59:42,720
you mentioned that for avoiding hot

00:59:40,650 --> 00:59:45,300
spots you should target rights to a

00:59:42,720 --> 00:59:50,490
singing old yes how does that interact

00:59:45,300 --> 00:59:53,910
with load-balancing by proper

00:59:50,490 --> 00:59:57,930
configuration sorry I can't give you any

00:59:53,910 --> 01:00:00,150
any better answer with traditional

00:59:57,930 --> 01:00:02,460
replication you definitely don't want to

01:00:00,150 --> 01:00:04,890
have load balancing across everybody I'm

01:00:02,460 --> 01:00:07,350
cross all notes you want to direct all

01:00:04,890 --> 01:00:12,060
notes all rights to a single note with

01:00:07,350 --> 01:00:14,100
Cali huh you could direct your rights to

01:00:12,060 --> 01:00:17,400
different notes provided they do not

01:00:14,100 --> 01:00:19,530
conflict well if they conflict you get

01:00:17,400 --> 01:00:21,420
transsexual back you want to avoid that

01:00:19,530 --> 01:00:25,950
it would it would work but would big

01:00:21,420 --> 01:00:28,350
would be slow but you should configure

01:00:25,950 --> 01:00:32,100
your load balancing or you should

01:00:28,350 --> 01:00:35,310
configure your clients such that changes

01:00:32,100 --> 01:00:39,180
affecting the same conflict become

01:00:35,310 --> 01:00:41,790
complex sorry say it changes you could

01:00:39,180 --> 01:00:44,700
have say order data and customer data

01:00:41,790 --> 01:00:47,820
and employee data in the same system you

01:00:44,700 --> 01:00:51,000
would add customers on node one manage

01:00:47,820 --> 01:00:54,570
employees on no to handle orders on node

01:00:51,000 --> 01:00:57,600
three and you would set it up this way

01:00:54,570 --> 01:01:01,770
and this would then also cover for

01:00:57,600 --> 01:01:03,510
hotspots okay thank you very much for

01:01:01,770 --> 01:01:05,880
other questions please catch you up

01:01:03,510 --> 01:01:08,720
cooler at the coffee break yes sorry I

01:01:05,880 --> 01:01:08,720

YouTube URL: https://www.youtube.com/watch?v=YYDVHe30Pt8


