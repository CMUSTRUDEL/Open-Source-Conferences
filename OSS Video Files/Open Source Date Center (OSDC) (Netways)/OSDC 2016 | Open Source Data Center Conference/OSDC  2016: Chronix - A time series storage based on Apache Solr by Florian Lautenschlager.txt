Title: OSDC  2016: Chronix - A time series storage based on Apache Solr by Florian Lautenschlager
Publication date: 2016-05-04
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	How to store billions of time series points and access them within a few milliseconds? Chronix!
Chronix is a young but mature open source project that allows one for example to store about 15 GB (csv) of time series in 238 MB with average query times of 21 ms. Chronix is built on top of Apache Solr a bulletproof distributed NoSQL database with impressive search capabilities. In this code-intense session we show how Chronix achieves its efficiency in both respects by means of an ideal chunking, by selecting the best compression technique, by enhancing the stored data with (pre-computed) attributes, and by specialized query functions.
Captions: 
	00:00:11,860 --> 00:00:16,119
it's a pleasure for me to introduce

00:00:13,870 --> 00:00:21,189
Robin lager who will be wasn't talked

00:00:16,119 --> 00:00:25,360
about conics DB thank for introduction

00:00:21,189 --> 00:00:28,300
um welcome everyone to the last talk of

00:00:25,360 --> 00:00:32,890
the conference chronics a fast and

00:00:28,300 --> 00:00:35,020
efficient time series database well it's

00:00:32,890 --> 00:00:39,760
a very small audience so things that

00:00:35,020 --> 00:00:41,200
you're still sitting on your chairs I'm

00:00:39,760 --> 00:00:45,910
the founder of the open source project

00:00:41,200 --> 00:00:50,399
chronics and I'm also a guest researcher

00:00:45,910 --> 00:00:53,590
at the University along a new back and

00:00:50,399 --> 00:00:58,390
work as a software engineer at : unique

00:00:53,590 --> 00:01:03,670
and in my research I want to find ways

00:00:58,390 --> 00:01:08,140
to reduce the time that is needed to

00:01:03,670 --> 00:01:11,830
find a box and fixing them and for that

00:01:08,140 --> 00:01:14,320
we're using time series and the time

00:01:11,830 --> 00:01:18,000
series represent the resource consumed

00:01:14,320 --> 00:01:20,980
consumption of a system or a cluster and

00:01:18,000 --> 00:01:24,310
we have a tool called software Iike gear

00:01:20,980 --> 00:01:27,510
and for example one can see the memory

00:01:24,310 --> 00:01:30,730
consumption of two different hosts and

00:01:27,510 --> 00:01:37,930
once the one can see that here's a spike

00:01:30,730 --> 00:01:41,530
on early up 38 but that is not the topic

00:01:37,930 --> 00:01:44,650
of the talk the topic of the talk is the

00:01:41,530 --> 00:01:48,340
underlying storage or time series data

00:01:44,650 --> 00:01:50,440
base that is that allows to build such a

00:01:48,340 --> 00:01:53,710
tool and I want to start with the

00:01:50,440 --> 00:01:56,230
question how to store 68 billion time

00:01:53,710 --> 00:02:00,160
correlated data objects on your laptop

00:01:56,230 --> 00:02:05,910
computer and retrieve any point within a

00:02:00,160 --> 00:02:12,300
few milliseconds any ideas what could be

00:02:05,910 --> 00:02:16,300
approached for that a simple approach

00:02:12,300 --> 00:02:19,840
could be worth and relational database

00:02:16,300 --> 00:02:23,230
management system and a simple model

00:02:19,840 --> 00:02:25,069
like that we have a time series we have

00:02:23,230 --> 00:02:28,040
a table for the data objects

00:02:25,069 --> 00:02:30,739
whatever attributes the metric name and

00:02:28,040 --> 00:02:34,629
the logical measurement serious to group

00:02:30,739 --> 00:02:34,629
time series that belong to a measurement

00:02:37,030 --> 00:02:46,389
this approach is not rocket science and

00:02:40,780 --> 00:02:50,150
hands one has slow import one need a

00:02:46,389 --> 00:02:51,829
huge amount of hard drive space the

00:02:50,150 --> 00:02:56,930
retrieval of the time series is also

00:02:51,829 --> 00:03:00,709
fast slow and one half one has limited

00:02:56,930 --> 00:03:05,510
scalability due to relational database

00:03:00,709 --> 00:03:08,030
management systems and of course one has

00:03:05,510 --> 00:03:12,709
missing query functions for time series

00:03:08,030 --> 00:03:17,180
data and in my free time I love to go go

00:03:12,709 --> 00:03:21,349
sailing and if I want to explain how

00:03:17,180 --> 00:03:29,689
that approach felt I found a nice

00:03:21,349 --> 00:03:35,930
picture but what you do in such a

00:03:29,689 --> 00:03:38,720
situation it's simply it's very simple

00:03:35,930 --> 00:03:42,799
we only have three key ideas which

00:03:38,720 --> 00:03:46,129
combined solve the problem and the three

00:03:42,799 --> 00:03:49,819
key ideas are first if you have a time

00:03:46,129 --> 00:03:53,030
series with about 68 billion data

00:03:49,819 --> 00:03:55,819
objects split them up into chunks with

00:03:53,030 --> 00:03:59,750
the same size for example split them up

00:03:55,819 --> 00:04:04,669
into 1 million chunks and each chunk

00:03:59,750 --> 00:04:06,949
contains 68,000 data objects then

00:04:04,669 --> 00:04:11,810
compress these chunks to reduce the

00:04:06,949 --> 00:04:16,250
storage demand and this the third key

00:04:11,810 --> 00:04:18,500
idea is to build such a record who isn't

00:04:16,250 --> 00:04:20,349
starting an to describing the chunk the

00:04:18,500 --> 00:04:23,719
first point in the last point and

00:04:20,349 --> 00:04:26,620
arbitrary attributes that describe the

00:04:23,719 --> 00:04:30,500
chunk so one can find a chunk by

00:04:26,620 --> 00:04:38,360
queueing after for example a host called

00:04:30,500 --> 00:04:41,750
le app 37 and the reason that this

00:04:38,360 --> 00:04:44,270
approach is a success story of that we

00:04:41,750 --> 00:04:49,009
are able to store 68 billion data

00:04:44,270 --> 00:04:52,729
objects in 32 gigabytes and retrieve any

00:04:49,009 --> 00:04:54,229
point within a few milliseconds and I've

00:04:52,729 --> 00:04:58,580
done this measurement on my laptop

00:04:54,229 --> 00:05:02,750
computer no server computer just a small

00:04:58,580 --> 00:05:05,240
laptop computer and we are able to

00:05:02,750 --> 00:05:11,690
search on the attributes without loading

00:05:05,240 --> 00:05:15,289
all the binary data which also get gains

00:05:11,690 --> 00:05:17,949
first faster queries and every single on

00:05:15,289 --> 00:05:27,229
runs on a laptop computer and many more

00:05:17,949 --> 00:05:29,240
reasons so that's all and it's the last

00:05:27,229 --> 00:05:31,669
talk I have to go to to Nuremberg

00:05:29,240 --> 00:05:38,960
tomorrow have a meeting in the morning

00:05:31,669 --> 00:05:42,620
so are there any questions nope just

00:05:38,960 --> 00:05:45,229
kidding' what comes next

00:05:42,620 --> 00:05:47,509
I want to introduce what a time series

00:05:45,229 --> 00:05:52,580
what is a time series data base typical

00:05:47,509 --> 00:05:55,639
features and stuff and why did we choose

00:05:52,580 --> 00:05:59,960
a patch isola and are there alternatives

00:05:55,639 --> 00:06:02,930
I want to introduce the architecture of

00:05:59,960 --> 00:06:06,620
chronics and I want to share what is

00:06:02,930 --> 00:06:14,569
needed to speed up chronics to an idiot

00:06:06,620 --> 00:06:17,389
performance for short definitions a data

00:06:14,569 --> 00:06:21,319
object is a pair of time step and value

00:06:17,389 --> 00:06:26,180
and in my definition the value could be

00:06:21,319 --> 00:06:33,490
any kind of object for example you can

00:06:26,180 --> 00:06:38,800
also store a trace or a lock or threaten

00:06:33,490 --> 00:06:41,960
whatever you want and not only scholars

00:06:38,800 --> 00:06:44,509
the second definition is simply the time

00:06:41,960 --> 00:06:47,539
series itself time series is a ordered

00:06:44,509 --> 00:06:51,349
list chronological order list of data

00:06:47,539 --> 00:06:52,100
objects of one value type definitions

00:06:51,349 --> 00:06:56,000
three

00:06:52,100 --> 00:06:57,650
what is a junk as chronics heavily works

00:06:56,000 --> 00:06:59,840
with chunks

00:06:57,650 --> 00:07:03,080
it's a chronological order part of a

00:06:59,840 --> 00:07:04,580
time series a while and definition for

00:07:03,080 --> 00:07:06,860
simply says what is the time series

00:07:04,580 --> 00:07:10,790
database times each database is a

00:07:06,860 --> 00:07:13,040
database that is optimized or

00:07:10,790 --> 00:07:21,260
specialized to the obvious time series

00:07:13,040 --> 00:07:23,570
data that's all a few features of Time

00:07:21,260 --> 00:07:28,640
series database I have arranged them in

00:07:23,570 --> 00:07:30,730
four groups we have data management it's

00:07:28,640 --> 00:07:35,360
well known that time series data base

00:07:30,730 --> 00:07:39,860
rapidly grows two gigabyte terabytes of

00:07:35,360 --> 00:07:43,150
data so hands 1 1 need strategies to

00:07:39,860 --> 00:07:46,940
deal with that for example round-robin

00:07:43,150 --> 00:07:51,290
storages one has to count down sample

00:07:46,940 --> 00:07:56,650
all time series on compress the data or

00:07:51,290 --> 00:07:59,000
do a compaction like TST TST videos

00:07:56,650 --> 00:08:02,420
performs an operation all this the

00:07:59,000 --> 00:08:05,500
second group and time series have a

00:08:02,420 --> 00:08:09,650
special special characteristic that

00:08:05,500 --> 00:08:13,430
updates are rare so especially in the

00:08:09,650 --> 00:08:17,240
case said your monitory system you're

00:08:13,430 --> 00:08:22,820
doing no updates to your data typically

00:08:17,240 --> 00:08:26,210
and inserts are always additive we have

00:08:22,820 --> 00:08:29,690
we need fast inserts as typically

00:08:26,210 --> 00:08:33,380
nowadays systems are distributed and

00:08:29,690 --> 00:08:37,510
produced tons of metrics per second so

00:08:33,380 --> 00:08:41,510
one have to store them fastly and even

00:08:37,510 --> 00:08:44,690
for an online query processing one it

00:08:41,510 --> 00:08:48,430
also fast Retrievers time search

00:08:44,690 --> 00:08:52,690
databases are typically distributed and

00:08:48,430 --> 00:09:00,080
have an high efficient per note that's

00:08:52,690 --> 00:09:03,260
the wish and there is no need of acid

00:09:00,080 --> 00:09:04,150
but the data should be consistent and in

00:09:03,260 --> 00:09:10,000
a cluster

00:09:04,150 --> 00:09:12,010
as mentioned attributes are important to

00:09:10,000 --> 00:09:16,600
describe the time series to give them a

00:09:12,010 --> 00:09:22,480
context for example then the time series

00:09:16,600 --> 00:09:24,760
from that measurement are based or DD

00:09:22,480 --> 00:09:29,260
are collected during a load test for

00:09:24,760 --> 00:09:35,529
example or you have to define which

00:09:29,260 --> 00:09:37,720
hosts which process which type or even

00:09:35,529 --> 00:09:41,710
for the data objects itself so what is

00:09:37,720 --> 00:09:43,450
the scale the unit and where does the

00:09:41,710 --> 00:09:46,779
data come from for example is it

00:09:43,450 --> 00:09:53,610
collected from the operating system or

00:09:46,779 --> 00:09:56,860
from genex for example and and

00:09:53,610 --> 00:10:00,700
furthermore to work with the data will

00:09:56,860 --> 00:10:03,160
need specific time series functions for

00:10:00,700 --> 00:10:07,350
example money one need statistics like

00:10:03,160 --> 00:10:11,740
aggregations minimum maximum or

00:10:07,350 --> 00:10:12,630
transformations to time windows time

00:10:11,740 --> 00:10:17,080
shiftings

00:10:12,630 --> 00:10:20,350
or even resample the data and my opinion

00:10:17,080 --> 00:10:23,640
a time series database also deliver high

00:10:20,350 --> 00:10:27,930
level functions to detect outliers or

00:10:23,640 --> 00:10:32,350
trends or do a similarity search find

00:10:27,930 --> 00:10:35,290
similar time series and if you're

00:10:32,350 --> 00:10:38,470
interested in some more features there's

00:10:35,290 --> 00:10:42,370
a good blog post it's not very friendly

00:10:38,470 --> 00:10:48,820
to read but I think you will get the

00:10:42,370 --> 00:10:50,589
slides and you can find that block there

00:10:48,820 --> 00:10:53,500
are many times your is databases out

00:10:50,589 --> 00:10:58,480
there implementing one or more or even

00:10:53,500 --> 00:11:00,630
all features I've listed a few of them

00:10:58,480 --> 00:11:03,100
for example the rrdtool

00:11:00,630 --> 00:11:07,180
which is mainly used in traditional

00:11:03,100 --> 00:11:09,370
monitoring systems or graphite that uses

00:11:07,180 --> 00:11:12,459
some of the concepts and puts some sugar

00:11:09,370 --> 00:11:16,360
on on it more functions data

00:11:12,459 --> 00:11:17,670
visualization in flux DB which has a

00:11:16,360 --> 00:11:22,440
very handy

00:11:17,670 --> 00:11:29,070
language that is similar to SQL open TST

00:11:22,440 --> 00:11:32,730
be based on on HBase and Hadoop or even

00:11:29,070 --> 00:11:34,950
from meter so it's either it's not an

00:11:32,730 --> 00:11:38,960
old link to side view it was the first

00:11:34,950 --> 00:11:42,390
time series database used in scientific

00:11:38,960 --> 00:11:44,570
for scientific reasons or a Cairo city

00:11:42,390 --> 00:11:49,290
which is a rewrite of open T STP and

00:11:44,570 --> 00:11:55,650
Netflix Atlas Android and diamantina DB

00:11:49,290 --> 00:11:57,720
and of course chronics so why did we

00:11:55,650 --> 00:12:01,920
create our own time series database

00:11:57,720 --> 00:12:05,160
instead of using one of these the answer

00:12:01,920 --> 00:12:11,880
is simple the tool I've shown on this on

00:12:05,160 --> 00:12:15,960
the second slide have a few that's been

00:12:11,880 --> 00:12:18,270
around for a good few years and there

00:12:15,960 --> 00:12:21,000
was no time series database that

00:12:18,270 --> 00:12:23,190
complies our requirements and certainly

00:12:21,000 --> 00:12:25,890
there is no one today so that's the

00:12:23,190 --> 00:12:29,430
reason why I stand here and tell you

00:12:25,890 --> 00:12:33,660
something a lot chronics and what are

00:12:29,430 --> 00:12:35,630
what are our requirements of course we

00:12:33,660 --> 00:12:41,010
want fast write and query performance

00:12:35,630 --> 00:12:43,410
well what does Apache Solr delivers it's

00:12:41,010 --> 00:12:48,030
well-known that Apache Solr and even the

00:12:43,410 --> 00:12:51,000
underlining Lusine is very fast we want

00:12:48,030 --> 00:12:54,150
that the database runs on a laptop

00:12:51,000 --> 00:12:57,260
computer that is also achieve a bolus

00:12:54,150 --> 00:13:03,540
Apache Solr as it runs even embedded

00:12:57,260 --> 00:13:05,880
standalone or distributed as the whole

00:13:03,540 --> 00:13:08,700
database should run on a laptop computer

00:13:05,880 --> 00:13:12,150
we need that the data is stored highly

00:13:08,700 --> 00:13:14,820
efficient and you see in the index

00:13:12,150 --> 00:13:21,480
itself is very small has a field

00:13:14,820 --> 00:13:23,330
compression built-in so check we want to

00:13:21,480 --> 00:13:28,710
store arbitrary attributes

00:13:23,330 --> 00:13:30,230
it's also possible and we need or want a

00:13:28,710 --> 00:13:35,990
query language

00:13:30,230 --> 00:13:39,680
for describing complex terms on the

00:13:35,990 --> 00:13:43,880
attributes for example we want to do

00:13:39,680 --> 00:13:47,930
white card searches we want to combine

00:13:43,880 --> 00:13:50,440
terms using boolean operators and all

00:13:47,930 --> 00:13:57,650
that comes with the solid clear language

00:13:50,440 --> 00:14:00,890
and if you build the tool on a platform

00:13:57,650 --> 00:14:04,550
or a for the tool like like a chi solar

00:14:00,890 --> 00:14:08,900
you have to check if the community is

00:14:04,550 --> 00:14:12,790
large and if the application has an

00:14:08,900 --> 00:14:17,870
active development and Lua side works

00:14:12,790 --> 00:14:21,100
maintains Apache Solr and it's as the

00:14:17,870 --> 00:14:23,780
name suggests and Apache project and

00:14:21,100 --> 00:14:25,370
there are so many more features that

00:14:23,780 --> 00:14:27,170
comes with Apache Solr

00:14:25,370 --> 00:14:31,370
even in the latest version they

00:14:27,170 --> 00:14:37,090
introduced a native SQL support so you

00:14:31,370 --> 00:14:41,350
can use JDBC to query for data on

00:14:37,090 --> 00:14:45,710
harness Apache Solr cluster and an

00:14:41,350 --> 00:14:48,260
alternative to Apache Solr is elastic

00:14:45,710 --> 00:14:52,550
search elastic search is also based on

00:14:48,260 --> 00:14:56,090
the seen the product are quite similar

00:14:52,550 --> 00:14:59,870
so if you want to replace a patchy solar

00:14:56,090 --> 00:15:06,410
in our approach one should choose

00:14:59,870 --> 00:15:10,580
elastic search well let's dig deeper

00:15:06,410 --> 00:15:13,340
into the internals of chronics that's

00:15:10,580 --> 00:15:16,430
the architecture from a high-level point

00:15:13,340 --> 00:15:19,840
of view we have four stages the first

00:15:16,430 --> 00:15:23,320
stage is called semantic compression

00:15:19,840 --> 00:15:23,320
semantic compression

00:15:25,720 --> 00:15:32,800
I have the objective to reduce the

00:15:29,769 --> 00:15:36,939
points of data and even if accuracy is

00:15:32,800 --> 00:15:40,959
lost and that's the point why the first

00:15:36,939 --> 00:15:43,470
stage is optional and if one applies the

00:15:40,959 --> 00:15:47,949
semantic compression algorithm like

00:15:43,470 --> 00:15:50,199
Fourier transformation or averaging one

00:15:47,949 --> 00:15:53,129
has to ensure that the query asks the

00:15:50,199 --> 00:15:58,149
data can deal with the lost information

00:15:53,129 --> 00:16:01,300
the second stage called attributes and

00:15:58,149 --> 00:16:04,569
chunks splits for example 1 million

00:16:01,300 --> 00:16:08,050
points into 100 chunks and each chunk

00:16:04,569 --> 00:16:12,519
contains one contains ten thousand ten

00:16:08,050 --> 00:16:14,560
thousand points and it also calculates

00:16:12,519 --> 00:16:20,220
the attributes that describe the chunk

00:16:14,560 --> 00:16:23,410
and put them together into a logical

00:16:20,220 --> 00:16:26,769
record that is stored that is later is

00:16:23,410 --> 00:16:31,480
stored in the record storage and the

00:16:26,769 --> 00:16:34,269
third stage called basic confessin

00:16:31,480 --> 00:16:37,889
compression uses standard techniques

00:16:34,269 --> 00:16:41,319
like cheetah to compress the data and

00:16:37,889 --> 00:16:44,769
from time series data one can easily

00:16:41,319 --> 00:16:49,240
achieve compression rates of about 69

00:16:44,769 --> 00:16:52,350
percent the last stage is the

00:16:49,240 --> 00:16:56,439
multi-dimensional storage that means

00:16:52,350 --> 00:16:58,089
that the attributes remain untouched the

00:16:56,439 --> 00:17:00,309
data remains untouched and the

00:16:58,089 --> 00:17:03,399
attributes are represented as dimensions

00:17:00,309 --> 00:17:06,539
so each attribute is a dimension and

00:17:03,399 --> 00:17:09,250
could be searched within the storage and

00:17:06,539 --> 00:17:14,079
one can see that the record is the key

00:17:09,250 --> 00:17:20,770
data element of tronics

00:17:14,079 --> 00:17:24,270
and it's simply a key value map the data

00:17:20,770 --> 00:17:28,630
as mentioned before it contains the

00:17:24,270 --> 00:17:33,090
binary large object of our data object

00:17:28,630 --> 00:17:36,039
so if you want to store a time series a

00:17:33,090 --> 00:17:39,370
typical time series of timestamp numeric

00:17:36,039 --> 00:17:41,080
value you can use

00:17:39,370 --> 00:17:43,120
you have only you can use it the typical

00:17:41,080 --> 00:17:45,910
time series or if you want to store

00:17:43,120 --> 00:17:50,290
calls and and exceptions for example

00:17:45,910 --> 00:17:53,890
which are traces you can store them also

00:17:50,290 --> 00:17:58,300
in a binary large object so chronics has

00:17:53,890 --> 00:18:02,530
no information about what is inside the

00:17:58,300 --> 00:18:05,110
chunk or you can use you can store any

00:18:02,530 --> 00:18:08,830
kind of complex data you can imagine

00:18:05,110 --> 00:18:11,200
that half a time dimension of course we

00:18:08,830 --> 00:18:14,590
have some technical fields that comes

00:18:11,200 --> 00:18:16,809
with Apache Solr for example the IDE and

00:18:14,590 --> 00:18:19,870
the version and we have two required

00:18:16,809 --> 00:18:22,120
fields describing this the first point

00:18:19,870 --> 00:18:24,850
and chunk and the last point of the

00:18:22,120 --> 00:18:27,670
chunk called start and end and as

00:18:24,850 --> 00:18:30,370
mentioned before we can store optional

00:18:27,670 --> 00:18:33,070
attributes they make the chunk

00:18:30,370 --> 00:18:36,940
searchable they are indexed and can also

00:18:33,070 --> 00:18:41,920
contain pre calculated value values for

00:18:36,940 --> 00:18:44,290
example if you have an query that we for

00:18:41,920 --> 00:18:49,570
example the maximum and you do that

00:18:44,290 --> 00:18:54,400
query every day and you can think about

00:18:49,570 --> 00:18:57,820
if you pre calculated value so that the

00:18:54,400 --> 00:19:00,340
query can access that pre calculated

00:18:57,820 --> 00:19:03,179
value of the chunk instead of loading

00:19:00,340 --> 00:19:05,650
all the data from the hard drive

00:19:03,179 --> 00:19:09,840
decompressing it and deserializing it

00:19:05,650 --> 00:19:14,640
and with that one can speed up queries

00:19:09,840 --> 00:19:17,320
and it's also important that one has

00:19:14,640 --> 00:19:20,400
typical functions to work with time

00:19:17,320 --> 00:19:23,050
series data and in chronics there I

00:19:20,400 --> 00:19:25,210
split them up into aggregations and

00:19:23,050 --> 00:19:27,270
analysis and there are typical

00:19:25,210 --> 00:19:31,540
aggregations like the minimum maximum

00:19:27,270 --> 00:19:35,250
percentage get the bottom or top and

00:19:31,540 --> 00:19:39,429
values above or below a value or a

00:19:35,250 --> 00:19:42,550
calculate the range the value range of a

00:19:39,429 --> 00:19:46,420
time series and there are also high

00:19:42,550 --> 00:19:47,920
level analysis to query all time series

00:19:46,420 --> 00:19:51,820
that have a positive strength for

00:19:47,920 --> 00:19:53,230
example have an outlier or use fast and

00:19:51,820 --> 00:19:55,090
any time warping

00:19:53,230 --> 00:19:58,870
to search for time series that are

00:19:55,090 --> 00:20:02,770
similar and you can also do a

00:19:58,870 --> 00:20:07,750
vectorization means for example if you

00:20:02,770 --> 00:20:09,760
have a client with your tiny display you

00:20:07,750 --> 00:20:13,540
can do a server-side vectorization to

00:20:09,760 --> 00:20:16,480
reduce the data amount that's so that

00:20:13,540 --> 00:20:22,510
the client can handle the time series

00:20:16,480 --> 00:20:26,950
data so what if if you have a time

00:20:22,510 --> 00:20:29,980
series that has a specialized forum that

00:20:26,950 --> 00:20:33,280
only you have and you want that chronic

00:20:29,980 --> 00:20:35,320
stores that time series in that case you

00:20:33,280 --> 00:20:41,320
have to implement an interface with two

00:20:35,320 --> 00:20:47,049
methods called from and - and you get an

00:20:41,320 --> 00:20:49,150
binary time serious chunk which is the

00:20:47,049 --> 00:20:52,470
record the representation of the record

00:20:49,150 --> 00:20:57,580
it's a map of key value and with the

00:20:52,470 --> 00:21:00,030
compressed data plop and all attributes

00:20:57,580 --> 00:21:03,520
so you only have to describe how to

00:21:00,030 --> 00:21:07,510
convert that binary time series into

00:21:03,520 --> 00:21:11,740
your special time series and vice versa

00:21:07,510 --> 00:21:13,750
and with chronics we have implemented or

00:21:11,740 --> 00:21:17,410
we have a time series framework called

00:21:13,750 --> 00:21:19,540
chronics cassiopeia that come can be

00:21:17,410 --> 00:21:23,380
used within with chronic sore without

00:21:19,540 --> 00:21:26,650
chronics and it comes with three time

00:21:23,380 --> 00:21:31,960
series types of course the numeric time

00:21:26,650 --> 00:21:36,760
series with doubles one can store items

00:21:31,960 --> 00:21:41,650
or even as traces dumps from from s

00:21:36,760 --> 00:21:45,610
traces for example - the idea is that if

00:21:41,650 --> 00:21:50,440
you detect high CPU load you can also

00:21:45,610 --> 00:21:53,799
call chronic soraka's chronics for give

00:21:50,440 --> 00:22:00,340
me the most frequent system calls in

00:21:53,799 --> 00:22:03,610
that duration in that time range so if

00:22:00,340 --> 00:22:06,430
you want to execute chronics on your on

00:22:03,610 --> 00:22:09,700
your laptop computer

00:22:06,430 --> 00:22:13,030
that's the execution view you have your

00:22:09,700 --> 00:22:17,110
computer you need Java Runtime

00:22:13,030 --> 00:22:21,330
environment and version 8 and chronics

00:22:17,110 --> 00:22:25,170
and as one can see chronics is only a

00:22:21,330 --> 00:22:27,880
solar server with three custom plugins

00:22:25,170 --> 00:22:32,880
when as the query handler that

00:22:27,880 --> 00:22:37,150
implements all the functions one half

00:22:32,880 --> 00:22:43,750
the chronics response writer that allows

00:22:37,150 --> 00:22:49,300
one to typically solar reports if one

00:22:43,750 --> 00:22:57,120
uses the HTTP HTTP interface uses the

00:22:49,300 --> 00:22:57,120
Chasen and the data is utilized space 64

00:22:57,390 --> 00:23:06,730
but in the case that I wanted say I want

00:23:01,150 --> 00:23:10,840
to use graph Anna as my dashboard I need

00:23:06,730 --> 00:23:16,930
a way to say chronics give me the data

00:23:10,840 --> 00:23:20,830
this utilized and decompressed and this

00:23:16,930 --> 00:23:23,340
is the deep part of of the response

00:23:20,830 --> 00:23:25,450
writer response right deserialize

00:23:23,340 --> 00:23:28,480
decompresses and this realizes the data

00:23:25,450 --> 00:23:32,680
and serialize its chasing and of course

00:23:28,480 --> 00:23:35,350
one can use binary in binary and we have

00:23:32,680 --> 00:23:38,190
a small retention plugin that deletes

00:23:35,350 --> 00:23:44,110
time serious that are already defined

00:23:38,190 --> 00:23:45,670
threshold and chronics is young open

00:23:44,110 --> 00:23:51,430
source project and we only have one

00:23:45,670 --> 00:23:56,880
native client or in in Java and I want

00:23:51,430 --> 00:23:56,880
you to show how to use that to client

00:23:57,390 --> 00:24:04,210
one first half to define a connection to

00:24:01,840 --> 00:24:09,430
solar using this the standard solar API

00:24:04,210 --> 00:24:12,960
and a chronics client the first argument

00:24:09,430 --> 00:24:15,370
is the converter

00:24:12,960 --> 00:24:18,730
cassiopeia simple converter is the

00:24:15,370 --> 00:24:22,810
default for time series of

00:24:18,730 --> 00:24:27,400
of numeric values and one have to define

00:24:22,810 --> 00:24:32,560
the chronic solar storage that queries

00:24:27,400 --> 00:24:36,670
solar and thus the conversion and so on

00:24:32,560 --> 00:24:38,920
currently we have a second

00:24:36,670 --> 00:24:41,170
implementation or we have three

00:24:38,920 --> 00:24:46,090
implementations of the chronics of the

00:24:41,170 --> 00:24:53,140
storage interface even for solar for

00:24:46,090 --> 00:24:55,960
leucine and for spark and one have to

00:24:53,140 --> 00:25:01,240
define a function to group the records

00:24:55,960 --> 00:25:04,360
and reduce them and one can use any

00:25:01,240 --> 00:25:09,100
combination of the attributes to group

00:25:04,360 --> 00:25:12,700
the chunks so if you want to query the

00:25:09,100 --> 00:25:14,890
dual range query I want to get all

00:25:12,700 --> 00:25:18,400
metrics whose metric name contents load

00:25:14,890 --> 00:25:23,110
we define as standard a standard solar

00:25:18,400 --> 00:25:26,440
query define the expression and call

00:25:23,110 --> 00:25:31,420
stream on the client and the result is a

00:25:26,440 --> 00:25:34,660
Java 8 stream of Time series and if you

00:25:31,420 --> 00:25:37,570
want to do some aggregations we simply

00:25:34,660 --> 00:25:42,790
add we use the filter query mechanism

00:25:37,570 --> 00:25:44,830
and we pass our expression and that

00:25:42,790 --> 00:25:46,960
example we want to get the maximum the

00:25:44,830 --> 00:25:49,900
minimum we want to count the time series

00:25:46,960 --> 00:25:53,050
and we want assign difference of the

00:25:49,900 --> 00:25:54,820
time series that means it's the first

00:25:53,050 --> 00:25:57,970
value is 20 and the last value of the

00:25:54,820 --> 00:26:02,290
time series is minus 100 the scientific

00:25:57,970 --> 00:26:09,910
answers - ad and the way how we stream

00:26:02,290 --> 00:26:14,290
does the data is similar to above self a

00:26:09,910 --> 00:26:21,810
small screen cast of of how chronics

00:26:14,290 --> 00:26:21,810
performs and I hope who can be the input

00:26:23,790 --> 00:26:36,610
okay so I've pasted in a range query for

00:26:33,720 --> 00:26:42,870
with an define start and you find the

00:26:36,610 --> 00:26:49,600
end and for one specific time series

00:26:42,870 --> 00:26:53,620
load average and the query took 12

00:26:49,600 --> 00:26:56,050
milliseconds in that case we replaced

00:26:53,620 --> 00:26:59,110
the end with now which is the key a

00:26:56,050 --> 00:27:02,380
special keyword used by the loose in

00:26:59,110 --> 00:27:06,100
date math we can also subscribe to years

00:27:02,380 --> 00:27:09,700
as you can read the data is old but not

00:27:06,100 --> 00:27:14,290
too old so - three years returns an

00:27:09,700 --> 00:27:18,900
empty clearing so we're going to replace

00:27:14,290 --> 00:27:23,320
the end with the formal date string and

00:27:18,900 --> 00:27:26,440
what we now do is to a white card query

00:27:23,320 --> 00:27:30,490
so we want on metrics whose metric name

00:27:26,440 --> 00:27:38,590
contains load that was three and the

00:27:30,490 --> 00:27:42,550
fury time was 40 milliseconds and we can

00:27:38,590 --> 00:27:45,640
also express the query with boolean

00:27:42,550 --> 00:27:48,340
operator so we want all metrics all

00:27:45,640 --> 00:27:52,450
times you reset I have a max or an

00:27:48,340 --> 00:27:55,890
average in its name and what we're now

00:27:52,450 --> 00:27:59,200
going to do is cause an aggregation on

00:27:55,890 --> 00:28:02,320
all time series that matches it clearly

00:27:59,200 --> 00:28:04,690
above for example want the maximum of

00:28:02,320 --> 00:28:07,180
the maximum the maximum of the minimum

00:28:04,690 --> 00:28:09,970
and the maximum of the average when also

00:28:07,180 --> 00:28:13,690
can pass a further aggregation we want

00:28:09,970 --> 00:28:18,490
the maximum and the minimum will get six

00:28:13,690 --> 00:28:20,500
time series for the sweep they only

00:28:18,490 --> 00:28:25,920
three the three time series from the

00:28:20,500 --> 00:28:30,130
career buff now it has an outlier

00:28:25,920 --> 00:28:32,740
detection to two chronics delivers or

00:28:30,130 --> 00:28:34,960
time series that have an outlier the

00:28:32,740 --> 00:28:36,429
query itself took 51 milliseconds but

00:28:34,960 --> 00:28:43,809
the charting engine is

00:28:36,429 --> 00:28:47,190
very slow and took else 11 seconds so

00:28:43,809 --> 00:28:49,690
and the last analysis clearly is doing

00:28:47,190 --> 00:28:54,549
similarity search using fast dynamic

00:28:49,690 --> 00:28:57,340
time warping and we want to find all

00:28:54,549 --> 00:29:02,619
time series that are similar to load

00:28:57,340 --> 00:29:07,240
maximum and well okay it's not so

00:29:02,619 --> 00:29:15,749
special that the minimum and the average

00:29:07,240 --> 00:29:21,759
load is similar to the maximum load but

00:29:15,749 --> 00:29:24,490
okay but that why did I choose that

00:29:21,759 --> 00:29:27,279
small data set that's the data that

00:29:24,490 --> 00:29:31,179
shipped with the release so everyone who

00:29:27,279 --> 00:29:33,669
wants to play with chronics and repeat

00:29:31,179 --> 00:29:39,610
that could download the latest release

00:29:33,669 --> 00:29:43,179
and execute queries and using the chaves

00:29:39,610 --> 00:29:51,009
chaves example to visualize and expedite

00:29:43,179 --> 00:29:53,559
explores the time series data so if you

00:29:51,009 --> 00:29:57,340
want a more powerful way to work with

00:29:53,559 --> 00:30:02,230
time series data you can use a chronic

00:29:57,340 --> 00:30:05,649
cloud you can use a spark cluster and an

00:30:02,230 --> 00:30:11,220
arbitrary work band for example Apaches

00:30:05,649 --> 00:30:15,820
Zeppelin and as mentioned before

00:30:11,220 --> 00:30:19,629
chronics is simply an Apache Solr with

00:30:15,820 --> 00:30:23,619
three custom plugins so we can use the

00:30:19,629 --> 00:30:29,649
standard cluster mechanism of Apache

00:30:23,619 --> 00:30:33,809
Solr and if you have set up a spark

00:30:29,649 --> 00:30:39,539
cluster you can use chronic spark that

00:30:33,809 --> 00:30:44,350
delivers a spark a chronic spark context

00:30:39,539 --> 00:30:46,960
that connects spark with chronics so you

00:30:44,350 --> 00:30:50,200
can query data out of chronics and

00:30:46,960 --> 00:30:56,769
process in a spark cluster

00:30:50,200 --> 00:30:59,739
and furthermore you can convert the

00:30:56,769 --> 00:31:02,710
chronic spark context into a spark SQL

00:30:59,739 --> 00:31:09,509
context which allows you to use the SQL

00:31:02,710 --> 00:31:14,739
features of spark and on that you can

00:31:09,509 --> 00:31:17,529
use sapling to to play with the data or

00:31:14,739 --> 00:31:20,109
you can implement a plain java

00:31:17,529 --> 00:31:25,629
application or a scalar based

00:31:20,109 --> 00:31:29,739
application and that's the second code

00:31:25,629 --> 00:31:33,399
slide and the last Co slide what have

00:31:29,739 --> 00:31:36,359
you what has to be done to bring them

00:31:33,399 --> 00:31:40,409
all together excluding all the

00:31:36,359 --> 00:31:45,119
infrastructure stuff you have to setup

00:31:40,409 --> 00:31:47,980
chronics and you have to set up spark

00:31:45,119 --> 00:31:51,340
this Chavez park context you have to set

00:31:47,980 --> 00:31:56,649
up the chronic spark context and an SQL

00:31:51,340 --> 00:32:01,090
context and if you want to do the same

00:31:56,649 --> 00:32:04,179
as in the former an example we want all

00:32:01,090 --> 00:32:06,999
time serious that would have huge matrix

00:32:04,179 --> 00:32:09,940
contains load we define this Olek theory

00:32:06,999 --> 00:32:14,139
and we call on this chronic spark

00:32:09,940 --> 00:32:18,039
context query chronics chunks which

00:32:14,139 --> 00:32:20,850
indifference to the single solar

00:32:18,039 --> 00:32:24,549
instance or a single chronics instance

00:32:20,850 --> 00:32:28,570
delivers chunks for for a higher

00:32:24,549 --> 00:32:33,249
parallelism and when use for that we use

00:32:28,570 --> 00:32:34,720
to chronics solar cloud storage and then

00:32:33,249 --> 00:32:39,399
we can play with the data for example

00:32:34,720 --> 00:32:43,419
it's mentioned we can convert the r DD

00:32:39,399 --> 00:32:48,070
to a data set to use the SQL features we

00:32:43,419 --> 00:32:55,799
can call mean maximum or get an iterator

00:32:48,070 --> 00:32:58,359
over the time series data so let's I

00:32:55,799 --> 00:33:01,179
want to show you how to tune chronic

00:32:58,359 --> 00:33:03,760
sand or what we did to tune chronics to

00:33:01,179 --> 00:33:09,340
gain its best performance

00:33:03,760 --> 00:33:11,980
salt Christ can be used with various

00:33:09,340 --> 00:33:16,360
compression techniques and chunk sizes

00:33:11,980 --> 00:33:19,720
and it's clearly that these are the two

00:33:16,360 --> 00:33:23,470
tuning parameters called t4 compression

00:33:19,720 --> 00:33:27,790
technique and c4 chunk size and we have

00:33:23,470 --> 00:33:30,820
used a data set from three real-world

00:33:27,790 --> 00:33:35,040
projects about 15 gigabytes of time

00:33:30,820 --> 00:33:41,070
series data in in raw series in raw data

00:33:35,040 --> 00:33:44,080
uncompressed that's about 500 million

00:33:41,070 --> 00:33:47,770
points in about 15 thousand time series

00:33:44,080 --> 00:33:50,710
and we have collected typical queries

00:33:47,770 --> 00:33:53,350
that are asked for example with tools

00:33:50,710 --> 00:33:56,080
like the soft Waker key asking for

00:33:53,350 --> 00:34:00,550
different time ranges with different

00:33:56,080 --> 00:34:04,210
recurrence for example a times a query

00:34:00,550 --> 00:34:08,010
asking for for a for one week of data is

00:34:04,210 --> 00:34:10,389
asked 10 times while a query

00:34:08,010 --> 00:34:18,520
asking for three months is only asked

00:34:10,389 --> 00:34:20,440
twice to represent a real Q mix and then

00:34:18,520 --> 00:34:24,429
we have measured the compression rate

00:34:20,440 --> 00:34:27,450
with the different compression

00:34:24,429 --> 00:34:31,090
techniques and different chunk sizes and

00:34:27,450 --> 00:34:35,500
even the total time that is needed to

00:34:31,090 --> 00:34:37,210
answer the ninety-two queries with the

00:34:35,500 --> 00:34:40,480
compression techniques and with

00:34:37,210 --> 00:34:43,270
different chunks sizes and that's are

00:34:40,480 --> 00:34:47,230
the results we first have measured the

00:34:43,270 --> 00:34:50,409
compression rate so we have imported the

00:34:47,230 --> 00:34:53,800
15 gigabytes of time series data with

00:34:50,409 --> 00:34:56,730
different chunk sizes and that's one can

00:34:53,800 --> 00:34:58,870
see nearly all compression rates

00:34:56,730 --> 00:35:04,810
compression techniques achieve high

00:34:58,870 --> 00:35:08,050
compression rates but what what is more

00:35:04,810 --> 00:35:12,190
important for us is the query time did

00:35:08,050 --> 00:35:15,190
the overall query performance and it's

00:35:12,190 --> 00:35:17,440
shown in the second chart and this chart

00:35:15,190 --> 00:35:21,790
is missing the is

00:35:17,440 --> 00:35:26,470
missing beats - and except for example

00:35:21,790 --> 00:35:31,300
these two algorithms our times slower

00:35:26,470 --> 00:35:35,410
than GTA pelted for or snappy and one

00:35:31,300 --> 00:35:39,160
candy that gzip has the best overall

00:35:35,410 --> 00:35:44,369
performance in query time with a chunk

00:35:39,160 --> 00:35:49,510
size of 128 kilobytes so the default

00:35:44,369 --> 00:35:53,680
parameter values are cheap flip and 128

00:35:49,510 --> 00:35:56,800
kilobytes and for all that are more

00:35:53,680 --> 00:36:01,359
interested in in detuning and what we

00:35:56,800 --> 00:36:05,470
did you can check out our research paper

00:36:01,359 --> 00:36:08,650
about chronics and of course we have

00:36:05,470 --> 00:36:12,910
compared chronics to related time series

00:36:08,650 --> 00:36:17,400
databases we have evaluated in flux TB

00:36:12,910 --> 00:36:21,369
graphite open TST B and Kairos TP and

00:36:17,400 --> 00:36:26,200
all databases are config configured to

00:36:21,369 --> 00:36:31,089
run as a single single node that's a

00:36:26,200 --> 00:36:35,700
important point as one may say open csdb

00:36:31,089 --> 00:36:41,140
is optimized for for distribution with

00:36:35,700 --> 00:36:44,369
tens of of nodes and and TS DB clients

00:36:41,140 --> 00:36:48,040
well but what we want to measure is the

00:36:44,369 --> 00:36:51,030
performance on a single node so we have

00:36:48,040 --> 00:36:55,119
imported if the 15 gigabytes of data and

00:36:51,030 --> 00:36:59,770
chronics needs 237 megabytes to import

00:36:55,119 --> 00:37:03,670
15 gigabytes of data and the related

00:36:59,770 --> 00:37:11,920
time series data base is need for 284

00:37:03,670 --> 00:37:16,000
times more space of course and chronics

00:37:11,920 --> 00:37:18,839
needs less space that's right what we

00:37:16,000 --> 00:37:22,900
have done what you have done measured is

00:37:18,839 --> 00:37:24,460
the query time for the 92 queries we

00:37:22,900 --> 00:37:27,400
have imported the data to all databases

00:37:24,460 --> 00:37:30,910
and we have asked all databases of all

00:37:27,400 --> 00:37:34,240
that queries and chronics is

00:37:30,910 --> 00:37:37,200
fifty to ninety one percent faster than

00:37:34,240 --> 00:37:43,150
the ability to take the databases and

00:37:37,200 --> 00:37:46,570
the further important measurement is the

00:37:43,150 --> 00:37:51,520
memory footprint of the databases so we

00:37:46,570 --> 00:37:53,500
have measured three interest points of

00:37:51,520 --> 00:37:56,350
interests and shortly after the start of

00:37:53,500 --> 00:37:59,200
the time series database the maximum

00:37:56,350 --> 00:38:04,450
during the import and the maximum during

00:37:59,200 --> 00:38:07,630
the query plan on that graph at is the

00:38:04,450 --> 00:38:10,420
best time to restate the base we have

00:38:07,630 --> 00:38:13,230
thumbed up all the three values and it

00:38:10,420 --> 00:38:17,800
takes nine hundred twenty six megabytes

00:38:13,230 --> 00:38:20,860
chronics needs 1.5 gigabytes and the

00:38:17,800 --> 00:38:26,010
remaining three needs sixteen to thirty

00:38:20,860 --> 00:38:30,310
nine gigabytes of time self of memory

00:38:26,010 --> 00:38:34,660
well and know that this slide is it's a

00:38:30,310 --> 00:38:35,020
summary and the hard facts are in the

00:38:34,660 --> 00:38:40,900
paper

00:38:35,020 --> 00:38:45,640
so again if one is interested I suggest

00:38:40,900 --> 00:38:47,980
you to read the research paper called

00:38:45,640 --> 00:38:50,950
chronic sufficient storage storage and

00:38:47,980 --> 00:38:54,010
query of operational time series data of

00:38:50,950 --> 00:38:58,960
Tangiers the state is currently

00:38:54,010 --> 00:39:06,580
submitted so I have no idea of if if it

00:38:58,960 --> 00:39:13,300
will be accepted so hopefully so now

00:39:06,580 --> 00:39:16,110
it's your turn so are there any

00:39:13,300 --> 00:39:19,030
questions I have five minutes - too fast

00:39:16,110 --> 00:39:24,330
but are there any questions about

00:39:19,030 --> 00:39:24,330

YouTube URL: https://www.youtube.com/watch?v=pbs3VgwrGWY


