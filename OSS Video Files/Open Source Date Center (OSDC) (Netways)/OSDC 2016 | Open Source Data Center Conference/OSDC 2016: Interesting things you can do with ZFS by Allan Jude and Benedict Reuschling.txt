Title: OSDC 2016: Interesting things you can do with ZFS by Allan Jude and Benedict Reuschling
Publication date: 2016-05-04
Playlist: OSDC 2016 | Open Source Data Center Conference
Description: 
	ZFS is the next generation filesystem originally developed at Sun Microsystems. Available under the CDDL, it uniquely combines volume manager and filesystem into a powerful storage management solution for Unix systems. Regardless of big or small storage requirements. ZFS offers features, for free, that are usually found only in costly enterprise storage solutions. This talk will introduce ZFS and give an overview of its features like snapshots and rollback, compression, deduplication as well as replication. We will demonstrate how these features can make a difference in the datacenter, giving administrators the power and flexibility to adapt to changing storage requirements.
Real world examples of ZFS being used in production for video streaming, virtualization, archival, and research are shown to illustrate the concepts. The talk is intended for people considering ZFS for their data storage needs and those who are interested in the features ZFS provides.
Captions: 
	00:00:11,540 --> 00:00:16,490
okay good morning everybody welcome to

00:00:14,570 --> 00:00:19,070
our first talk for today we're

00:00:16,490 --> 00:00:21,529
introducing today Julian Allen which

00:00:19,070 --> 00:00:26,320
show us interesting things which you can

00:00:21,529 --> 00:00:30,410
do with ZFS so let's go

00:00:26,320 --> 00:00:32,270
microphone that would help yes so my

00:00:30,410 --> 00:00:33,980
name is Alan Jude and I'm a FreeBSD

00:00:32,270 --> 00:00:36,940
developer and I've been working as a

00:00:33,980 --> 00:00:42,559
freebie system in for about 13 years now

00:00:36,940 --> 00:00:44,930
and I got involved with ZFS in 2011 for

00:00:42,559 --> 00:00:49,760
my video streaming company to store a

00:00:44,930 --> 00:00:53,120
large quantity of video and I also do

00:00:49,760 --> 00:00:57,080
two weekly podcasts one about PSD and

00:00:53,120 --> 00:00:59,089
one about being assisted min and have

00:00:57,080 --> 00:01:06,650
contributed various bits and pieces to

00:00:59,089 --> 00:01:09,369
FreeBSD and set of hey yeah welcome

00:01:06,650 --> 00:01:11,930
thank you thanks for having me and

00:01:09,369 --> 00:01:14,479
valedictorians my name and I will be

00:01:11,930 --> 00:01:17,570
speaking with together with Alan Joo

00:01:14,479 --> 00:01:20,900
today about ZFS so my background is more

00:01:17,570 --> 00:01:22,550
in academia and I'm using ZFS on a big

00:01:20,900 --> 00:01:26,630
data cluster at our computer science

00:01:22,550 --> 00:01:28,820
department in Germany here and I'm also

00:01:26,630 --> 00:01:30,410
involved in the FreeBSD project I am

00:01:28,820 --> 00:01:32,870
meant for people into the documentation

00:01:30,410 --> 00:01:34,670
project that we have people like Alan

00:01:32,870 --> 00:01:38,630
Jude are welcome additions to our team

00:01:34,670 --> 00:01:40,640
and I'm also a member of the board of

00:01:38,630 --> 00:01:42,650
directors of the FreeBSD Foundation who

00:01:40,640 --> 00:01:46,010
not also supports the FreeBSD project

00:01:42,650 --> 00:01:48,080
but also sponsors CFS developer summits

00:01:46,010 --> 00:01:53,750
and we thought this would be a good

00:01:48,080 --> 00:01:58,450
venue to show you what ZFS can do so who

00:01:53,750 --> 00:02:01,370
here has is familiar with ZFS already

00:01:58,450 --> 00:02:04,100
okay so we'll do it kind of a brief

00:02:01,370 --> 00:02:06,790
overview to kind of get you up to speed

00:02:04,100 --> 00:02:10,429
if you're not that familiar with it so

00:02:06,790 --> 00:02:14,510
it's a pooled storage system so rather

00:02:10,429 --> 00:02:16,519
than your typical having to use a volume

00:02:14,510 --> 00:02:17,810
manager to take your multiple disks in

00:02:16,519 --> 00:02:19,430
an array and make them appear as one

00:02:17,810 --> 00:02:21,829
logical disk so that you can put a file

00:02:19,430 --> 00:02:24,130
system on it because most file systems

00:02:21,829 --> 00:02:26,920
can only understand one disk

00:02:24,130 --> 00:02:29,470
ZFS allows you to take a number of discs

00:02:26,920 --> 00:02:31,900
and then abstract the filesystem from it

00:02:29,470 --> 00:02:35,020
so it's an integrated volume manager and

00:02:31,900 --> 00:02:36,250
file system so that the free space of

00:02:35,020 --> 00:02:38,350
each of your file system is actually

00:02:36,250 --> 00:02:40,360
shared from the pool so instead of

00:02:38,350 --> 00:02:41,830
having to decide that this file system

00:02:40,360 --> 00:02:43,780
will be this percentage of your

00:02:41,830 --> 00:02:46,300
available space and this file system

00:02:43,780 --> 00:02:48,010
will be the rest by partitioning you

00:02:46,300 --> 00:02:50,380
create as many file systems as you want

00:02:48,010 --> 00:02:52,540
and they all share the free space and

00:02:50,380 --> 00:02:54,670
use it up as they need and you can

00:02:52,540 --> 00:02:56,380
expand it by just adding more disks and

00:02:54,670 --> 00:02:58,570
the space is available immediately you

00:02:56,380 --> 00:03:03,400
don't have to try to resize your logical

00:02:58,570 --> 00:03:06,220
volume or resize the file systems it

00:03:03,400 --> 00:03:07,720
also supports it was designed to do away

00:03:06,220 --> 00:03:09,790
with the idea of file systems having

00:03:07,720 --> 00:03:11,080
limits you know if you've been doing

00:03:09,790 --> 00:03:12,430
this long enough you remember when you

00:03:11,080 --> 00:03:14,710
couldn't have a file bigger than four

00:03:12,430 --> 00:03:16,960
gigabytes on a file system or when you

00:03:14,710 --> 00:03:19,840
you know every file system that exists

00:03:16,960 --> 00:03:21,570
now has some maximum size that we could

00:03:19,840 --> 00:03:27,820
probably reach within a couple of years

00:03:21,570 --> 00:03:32,230
but ZFS uses 128-bit pointers so it can

00:03:27,820 --> 00:03:34,960
handle up to 256 data bytes which is 256

00:03:32,230 --> 00:03:39,490
million petabytes or 20 56 million

00:03:34,960 --> 00:03:41,920
million gigabytes so it won't run out of

00:03:39,490 --> 00:03:44,140
the ability to expand with current

00:03:41,920 --> 00:03:46,990
technology having enough hard drives to

00:03:44,140 --> 00:03:48,730
actually fill as that FS pool would the

00:03:46,990 --> 00:03:50,020
amount of energy required would boil all

00:03:48,730 --> 00:03:53,890
the water and all the oceans on the

00:03:50,020 --> 00:03:57,360
planet so you're not going to make it

00:03:53,890 --> 00:04:01,870
too big at this point it also supports

00:03:57,360 --> 00:04:04,240
its own version of raid with distributed

00:04:01,870 --> 00:04:06,340
parity in three different flavors raid

00:04:04,240 --> 00:04:08,740
Zed one which is similar to raid 5 but

00:04:06,340 --> 00:04:10,900
with distributed parity and Zed to a

00:04:08,740 --> 00:04:12,610
game similar to raid 6 which distributed

00:04:10,900 --> 00:04:15,550
parity but it can actually do triple

00:04:12,610 --> 00:04:17,650
parity raid as well now that hard drives

00:04:15,550 --> 00:04:19,989
are so begging you know sakes or 10 or

00:04:17,650 --> 00:04:21,850
12 terabyte hard drives the time it

00:04:19,989 --> 00:04:24,490
takes to resilvered from a drive failing

00:04:21,850 --> 00:04:26,380
can be so long that the chance of

00:04:24,490 --> 00:04:28,630
another drive failing is high enough

00:04:26,380 --> 00:04:32,140
that you could actually lose the raid

00:04:28,630 --> 00:04:34,900
array and so by having triple parity you

00:04:32,140 --> 00:04:36,409
know some study from a university and

00:04:34,900 --> 00:04:37,819
Israel found that with

00:04:36,409 --> 00:04:41,169
triple parity there's no way to

00:04:37,819 --> 00:04:49,969
guarantee 99 or five nines of uptime

00:04:41,169 --> 00:04:51,379
without having at least triple parity so

00:04:49,969 --> 00:04:53,749
a little bit about the history it was

00:04:51,379 --> 00:04:57,499
originally started at Sun about fifteen

00:04:53,749 --> 00:04:59,179
years ago and just over ten years ago it

00:04:57,499 --> 00:05:02,209
became production ready and they open

00:04:59,179 --> 00:05:05,629
sourced it and it's been going since

00:05:02,209 --> 00:05:08,330
then when Oracle acquired Sun they

00:05:05,629 --> 00:05:10,489
closed source their version of it and

00:05:08,330 --> 00:05:13,129
went off in their own direction and the

00:05:10,489 --> 00:05:17,389
open ZFS project was founded and has

00:05:13,129 --> 00:05:20,059
continued development for illumos which

00:05:17,389 --> 00:05:23,239
is a fork of open Solaris but it's also

00:05:20,059 --> 00:05:26,119
since been ported to FreeBSD in 2008 and

00:05:23,239 --> 00:05:27,919
then there's the ZFS on Linux project

00:05:26,119 --> 00:05:30,909
from Lawrence Livermore National Labs in

00:05:27,919 --> 00:05:34,309
the US that has ported it to Linux and

00:05:30,909 --> 00:05:35,499
the purpose of the opens edifice project

00:05:34,309 --> 00:05:38,569
is to make sure those three different

00:05:35,499 --> 00:05:40,610
ports of it maintain compatibility

00:05:38,569 --> 00:05:42,559
between each other so you can actually

00:05:40,610 --> 00:05:44,779
export a pool from the Solaris machine

00:05:42,559 --> 00:05:47,209
and import it on a Linux machine and the

00:05:44,779 --> 00:05:49,309
file system just works like the entire

00:05:47,209 --> 00:05:50,719
raid array you can just move it if

00:05:49,309 --> 00:05:52,279
you've ever tried to move a raid array

00:05:50,719 --> 00:05:55,629
between two different hardware RAID

00:05:52,279 --> 00:06:00,289
cards you know how big of a deal that is

00:05:55,629 --> 00:06:02,269
and one of the ironic things is that as

00:06:00,289 --> 00:06:04,999
edifis at the proprietaries that FS from

00:06:02,269 --> 00:06:06,829
oracle is only just recently got one of

00:06:04,999 --> 00:06:09,589
the first feet-first features that was

00:06:06,829 --> 00:06:12,139
available in the open-source version

00:06:09,589 --> 00:06:15,019
that was maintained after oracle closed

00:06:12,139 --> 00:06:16,909
their source about four years ago which

00:06:15,019 --> 00:06:18,829
was the transparent compression using a

00:06:16,909 --> 00:06:20,929
new compression algorithm or a newer

00:06:18,829 --> 00:06:24,110
faster compression algorithm called LVAD

00:06:20,929 --> 00:06:25,969
for so it's interesting to watch there

00:06:24,110 --> 00:06:27,559
be a proprietary fork but it has less

00:06:25,969 --> 00:06:29,149
features than the open source version

00:06:27,559 --> 00:06:30,649
where it's usually the other way around

00:06:29,149 --> 00:06:32,089
you know it's like pay extra and you get

00:06:30,649 --> 00:06:34,610
these features on top of the open source

00:06:32,089 --> 00:06:36,860
one but more work has gone into the open

00:06:34,610 --> 00:06:40,279
source one benefit I hear you want since

00:06:36,860 --> 00:06:46,129
the split oh and there's also a port to

00:06:40,279 --> 00:06:48,439
Mac OS X if you have a Mac so like I

00:06:46,129 --> 00:06:50,260
said it combines the concept of the file

00:06:48,439 --> 00:06:52,210
system in the volume manager

00:06:50,260 --> 00:06:54,370
what makes a big difference here is that

00:06:52,210 --> 00:06:55,990
now the file system actually realizes

00:06:54,370 --> 00:06:58,060
that it's using more than one disk and

00:06:55,990 --> 00:07:02,140
it can optimize as IO to actually

00:06:58,060 --> 00:07:04,180
saturate all of those discs and it

00:07:02,140 --> 00:07:06,430
doesn't you're not having to fool the

00:07:04,180 --> 00:07:08,410
file system by presenting one big

00:07:06,430 --> 00:07:12,090
logical volume from your raid manager or

00:07:08,410 --> 00:07:14,740
something to pretend to just be one disk

00:07:12,090 --> 00:07:16,600
but what are the major other things

00:07:14,740 --> 00:07:18,370
about ZFS is that it does check

00:07:16,600 --> 00:07:20,710
something so every time you write data

00:07:18,370 --> 00:07:22,510
to the disk it calculates a checksum and

00:07:20,710 --> 00:07:25,030
stores that in its metadata along with

00:07:22,510 --> 00:07:28,510
the file and when it reads it it checks

00:07:25,030 --> 00:07:31,960
that checksum and if it's not the same

00:07:28,510 --> 00:07:33,850
then it will use the raid parity or your

00:07:31,960 --> 00:07:37,510
mirrors or whatever redundancy you have

00:07:33,850 --> 00:07:40,060
to restore the data or if not it will

00:07:37,510 --> 00:07:43,300
return a read error any other file

00:07:40,060 --> 00:07:45,400
system if the disk has just gone bad and

00:07:43,300 --> 00:07:47,800
is returning incorrect data or bit rot

00:07:45,400 --> 00:07:49,690
or a flipped bit or whatever the file

00:07:47,800 --> 00:07:52,420
system won't detect that and you'll just

00:07:49,690 --> 00:07:53,860
get back bad data you can imagine having

00:07:52,420 --> 00:07:55,600
a database and you're reading from it

00:07:53,860 --> 00:07:57,580
and a bit gets flipped and all of a

00:07:55,600 --> 00:07:59,310
sudden this users not an admin anymore

00:07:57,580 --> 00:08:01,360
or isn't it man when there shouldn't be

00:07:59,310 --> 00:08:04,930
and you probably don't want that to

00:08:01,360 --> 00:08:06,730
happen or it's not in this set of slides

00:08:04,930 --> 00:08:08,470
but another one they show what happens

00:08:06,730 --> 00:08:10,660
to a JPEG if you flip one bit in the

00:08:08,470 --> 00:08:16,660
middle of the JPEG and all sudden your

00:08:10,660 --> 00:08:18,040
you know family photo is ruined it also

00:08:16,660 --> 00:08:20,800
has like I mentioned transparent

00:08:18,040 --> 00:08:22,900
compression so as you write data into

00:08:20,800 --> 00:08:25,030
the buffer to be written to the disk it

00:08:22,900 --> 00:08:26,440
can be compressed and that way when you

00:08:25,030 --> 00:08:29,620
write it to the disk it takes up less

00:08:26,440 --> 00:08:31,450
space but that's not the only advantage

00:08:29,620 --> 00:08:32,710
to it especially in a database where you

00:08:31,450 --> 00:08:35,050
have a lot of text-based information

00:08:32,710 --> 00:08:37,060
that compress as well it can increase

00:08:35,050 --> 00:08:39,400
your throughput to writing to the disk

00:08:37,060 --> 00:08:42,099
and your standard hard drive can only do

00:08:39,400 --> 00:08:44,470
100 or 150 megabytes a second but if you

00:08:42,099 --> 00:08:46,240
have data that compresses two to one all

00:08:44,470 --> 00:08:49,060
the sudden you can write 300 megabytes

00:08:46,240 --> 00:08:51,100
of data to the disk in one second so you

00:08:49,060 --> 00:08:53,500
can actually lower the latency of your

00:08:51,100 --> 00:08:54,610
database by compression but the

00:08:53,500 --> 00:08:55,660
application doesn't have to know that

00:08:54,610 --> 00:08:57,450
that's happening right it's all

00:08:55,660 --> 00:08:59,710
transparent inside the filesystem and

00:08:57,450 --> 00:09:01,810
there a selection of different

00:08:59,710 --> 00:09:03,460
algorithms you can use so you can trade

00:09:01,810 --> 00:09:05,290
off the space versus how much

00:09:03,460 --> 00:09:07,389
compression

00:09:05,290 --> 00:09:11,350
but it also has a feature called early

00:09:07,389 --> 00:09:13,899
abort so that if you write a file like a

00:09:11,350 --> 00:09:15,939
tar file that's already compressed it

00:09:13,899 --> 00:09:17,589
will realize this and just write it

00:09:15,939 --> 00:09:19,120
uncompressed instead of spinning the CPU

00:09:17,589 --> 00:09:21,399
trying to compress you know an

00:09:19,120 --> 00:09:26,980
incompatible video file or something

00:09:21,399 --> 00:09:29,949
like that um the big difference is that

00:09:26,980 --> 00:09:31,810
once you don't have separate file

00:09:29,949 --> 00:09:33,069
systems anymore that are you know when

00:09:31,810 --> 00:09:34,899
you have to create partitions for each

00:09:33,069 --> 00:09:37,029
file system where you've statically

00:09:34,899 --> 00:09:38,350
allocating the amount of space it allows

00:09:37,029 --> 00:09:41,319
you to create a much larger number of

00:09:38,350 --> 00:09:44,560
file systems and through that be able to

00:09:41,319 --> 00:09:46,060
tune each one in a different way so ZFS

00:09:44,560 --> 00:09:48,009
maintains a set of properties on each

00:09:46,060 --> 00:09:49,750
file system that you can tweak so you

00:09:48,009 --> 00:09:52,180
can turn compression on and off for each

00:09:49,750 --> 00:09:53,889
individual file system and create as

00:09:52,180 --> 00:09:56,649
many file systems as you need to tune

00:09:53,889 --> 00:09:58,990
your workload the right way you can also

00:09:56,649 --> 00:10:00,519
adjust things like the block size so for

00:09:58,990 --> 00:10:01,149
example because that if s is

00:10:00,519 --> 00:10:04,029
copy-on-write

00:10:01,149 --> 00:10:05,620
you also get snapshots for free so you

00:10:04,029 --> 00:10:08,470
can take a snapshot of a dataset at any

00:10:05,620 --> 00:10:10,240
time and you have exactly how that

00:10:08,470 --> 00:10:12,100
dataset looked at the second you took

00:10:10,240 --> 00:10:14,620
the snapshot but you can keep writing

00:10:12,100 --> 00:10:17,829
and maintain both versions with the

00:10:14,620 --> 00:10:22,389
space not with a doubling the amount of

00:10:17,829 --> 00:10:23,949
space you're using and it has a

00:10:22,389 --> 00:10:26,949
delegation system which we'll talk about

00:10:23,949 --> 00:10:29,160
later so that you can allow certain

00:10:26,949 --> 00:10:34,990
operations to be done by non-root users

00:10:29,160 --> 00:10:37,750
at your discretion so yeah that was a

00:10:34,990 --> 00:10:39,220
good overview we're now going to show a

00:10:37,750 --> 00:10:42,100
little bit about how it's actually

00:10:39,220 --> 00:10:45,550
implemented or more like how the actual

00:10:42,100 --> 00:10:48,250
architecture is so on the right side

00:10:45,550 --> 00:10:50,620
here we have the old world the

00:10:48,250 --> 00:10:53,230
traditional file systems with a storage

00:10:50,620 --> 00:10:56,740
manager or a proprietary software raid

00:10:53,230 --> 00:10:59,589
or something like your lbm comes to mind

00:10:56,740 --> 00:11:01,389
or your software raid that you have on

00:10:59,589 --> 00:11:04,300
your operating system or also a

00:11:01,389 --> 00:11:07,509
proprietary Hardware card that does

00:11:04,300 --> 00:11:09,639
right for you so in that model we have a

00:11:07,509 --> 00:11:13,750
couple of disks that can be combined

00:11:09,639 --> 00:11:16,029
into a volume but as you are system

00:11:13,750 --> 00:11:17,650
admins you probably all know this once

00:11:16,029 --> 00:11:19,960
you set up the system and

00:11:17,650 --> 00:11:22,180
by that time during the install you

00:11:19,960 --> 00:11:24,730
already have to decide how big each

00:11:22,180 --> 00:11:26,440
individual partition has to be so we

00:11:24,730 --> 00:11:28,420
have to decide oh this is my home

00:11:26,440 --> 00:11:30,220
directory or the home directories for

00:11:28,420 --> 00:11:33,339
the users that are going to be on it

00:11:30,220 --> 00:11:36,160
later on Mars should have this size or

00:11:33,339 --> 00:11:38,170
user has a couple of gigabytes but you

00:11:36,160 --> 00:11:40,690
have to do it at the beginning of you

00:11:38,170 --> 00:11:42,940
install and the problem is you notice

00:11:40,690 --> 00:11:45,760
probably during the runtime of the

00:11:42,940 --> 00:11:48,940
system you figure out oh I should have

00:11:45,760 --> 00:11:52,450
put more gigabytes into var because

00:11:48,940 --> 00:11:54,460
there's a lot of log files going on with

00:11:52,450 --> 00:11:57,250
ZFS you don't have to do the initial

00:11:54,460 --> 00:12:01,510
partitioning ZFS does use a different

00:11:57,250 --> 00:12:03,880
approach uses in datasets so in ZFS all

00:12:01,510 --> 00:12:07,300
disk that you combine into a storage

00:12:03,880 --> 00:12:11,830
pool AZ pool they all share their

00:12:07,300 --> 00:12:13,390
complete storage capacity to the ZFS

00:12:11,830 --> 00:12:18,279
datasets that are on top of that and

00:12:13,390 --> 00:12:21,550
that way you can decide later on that a

00:12:18,279 --> 00:12:24,430
specific data set should have a specific

00:12:21,550 --> 00:12:27,040
quota or a reservation but all the data

00:12:24,430 --> 00:12:29,350
sets that you create like user data set

00:12:27,040 --> 00:12:31,720
of our data set or a home data set all

00:12:29,350 --> 00:12:34,240
of these data sets share the complete

00:12:31,720 --> 00:12:36,310
storage that your disks provide and you

00:12:34,240 --> 00:12:39,370
don't have to decide upfront which

00:12:36,310 --> 00:12:41,980
partition sizes you are going to assign

00:12:39,370 --> 00:12:44,740
and you can do that during the runtime

00:12:41,980 --> 00:12:48,490
of the system that's very flexible and

00:12:44,740 --> 00:12:51,279
also it gives you things like snapshots

00:12:48,490 --> 00:12:53,890
and clones and compression for free

00:12:51,279 --> 00:12:55,839
basically some vendors charge you extra

00:12:53,890 --> 00:12:57,910
for these features but on ZFS it's

00:12:55,839 --> 00:13:00,910
everything available in one big package

00:12:57,910 --> 00:13:04,209
and it's free and you can use that

00:13:00,910 --> 00:13:06,250
basically right away and by activating

00:13:04,209 --> 00:13:08,560
certain features as properties on these

00:13:06,250 --> 00:13:10,240
data sets you can decide ok this data

00:13:08,560 --> 00:13:13,029
set should have compression or that

00:13:10,240 --> 00:13:16,150
other data set should have a couple of

00:13:13,029 --> 00:13:18,880
snapshots for the benefit of having the

00:13:16,150 --> 00:13:23,529
ability to rollback to a certain state

00:13:18,880 --> 00:13:25,780
of the filesystem so yeah that's already

00:13:23,529 --> 00:13:28,930
a big difference between a traditional

00:13:25,780 --> 00:13:31,259
storage and a system and what ZFS

00:13:28,930 --> 00:13:34,300
provides the next one thanks

00:13:31,259 --> 00:13:36,339
GFS is also very focused on data

00:13:34,300 --> 00:13:38,920
integrity keeping your data safe and

00:13:36,339 --> 00:13:42,040
providing you with the actual original

00:13:38,920 --> 00:13:44,740
data like 15 years after you store that

00:13:42,040 --> 00:13:47,319
data in the same state that it was when

00:13:44,740 --> 00:13:49,149
it was stored so some long-term archival

00:13:47,319 --> 00:13:52,540
systems have the problem of you store

00:13:49,149 --> 00:13:55,600
like certain data set on it or a certain

00:13:52,540 --> 00:13:58,839
data amount and after 10 years or 20

00:13:55,600 --> 00:14:00,519
years you read it again and during that

00:13:58,839 --> 00:14:03,579
read there are some errors encountered

00:14:00,519 --> 00:14:05,699
because there were bit flips or cosmic

00:14:03,579 --> 00:14:11,290
rays caused some changes in the magnetic

00:14:05,699 --> 00:14:13,499
storage and you're then faced with a

00:14:11,290 --> 00:14:17,920
problem of how can I recover these files

00:14:13,499 --> 00:14:20,949
ZFS uses checksumming and different

00:14:17,920 --> 00:14:22,889
levels of redundancy like raid levels to

00:14:20,949 --> 00:14:25,990
ensure that the data is not only

00:14:22,889 --> 00:14:29,100
consistent but it can also detect errors

00:14:25,990 --> 00:14:32,410
that are happening using bit flips and

00:14:29,100 --> 00:14:35,860
when that happens ZFS has the ability to

00:14:32,410 --> 00:14:39,610
restore these damaged blocks on the disk

00:14:35,860 --> 00:14:44,439
and heal itself will have a short

00:14:39,610 --> 00:14:47,529
example later on how this works so this

00:14:44,439 --> 00:14:49,059
is a view of the old way so probably

00:14:47,529 --> 00:14:51,550
most of you know this already you have

00:14:49,059 --> 00:14:55,059
your storage array let's say we have a

00:14:51,550 --> 00:14:57,249
small mirror here with two disks and one

00:14:55,059 --> 00:15:00,269
day you read a couple of blocks from the

00:14:57,249 --> 00:15:02,410
disk or your application does that and

00:15:00,269 --> 00:15:05,529
there's a corrupt block for whatever

00:15:02,410 --> 00:15:08,339
reason maybe the disk is dying or there

00:15:05,529 --> 00:15:10,899
are some changes that go unnoticed so

00:15:08,339 --> 00:15:13,449
one of the disks is faulty and the other

00:15:10,899 --> 00:15:17,290
one is working properly the problem with

00:15:13,449 --> 00:15:19,540
that is usually Murphy's Law it's going

00:15:17,290 --> 00:15:22,779
to be the really is going to happen from

00:15:19,540 --> 00:15:24,550
the disk at is the damaged one and the

00:15:22,779 --> 00:15:27,720
corrupt block gets sent up to the

00:15:24,550 --> 00:15:30,429
application and the application will

00:15:27,720 --> 00:15:33,579
throw errors I cannot read the database

00:15:30,429 --> 00:15:35,829
or a file corrupt or something and users

00:15:33,579 --> 00:15:37,629
wonder and scratch their heads or u.s.

00:15:35,829 --> 00:15:39,309
system administrators sit there and try

00:15:37,629 --> 00:15:41,529
to figure out whatever the error in the

00:15:39,309 --> 00:15:44,620
application is while the actual error is

00:15:41,529 --> 00:15:47,530
down at the actual storage

00:15:44,620 --> 00:15:49,420
device the the worst case is that the

00:15:47,530 --> 00:15:51,130
application actually makes a decision

00:15:49,420 --> 00:15:52,750
based on the incorrect data and does the

00:15:51,130 --> 00:15:54,040
wrong thing ya rabb might written

00:15:52,750 --> 00:15:56,070
actually telling you that something went

00:15:54,040 --> 00:15:58,720
wrong exactly so it might process that

00:15:56,070 --> 00:16:00,900
damaged data and stores it again and

00:15:58,720 --> 00:16:04,330
then you have a real mess going on

00:16:00,900 --> 00:16:06,010
so in CFS we have the same situation or

00:16:04,330 --> 00:16:08,200
could have the same situation you have a

00:16:06,010 --> 00:16:10,510
damaged disk that happens all the time

00:16:08,200 --> 00:16:13,150
unfortunately but ZFS has the ability to

00:16:10,510 --> 00:16:16,690
not only detect that but also correct it

00:16:13,150 --> 00:16:19,060
so ZFS when every each block that is

00:16:16,690 --> 00:16:21,820
written it stores as a checksum beside

00:16:19,060 --> 00:16:23,590
the block and validates itself against

00:16:21,820 --> 00:16:28,630
the blocks that are on top of that and

00:16:23,590 --> 00:16:32,140
below it in the ZFS tree that way it can

00:16:28,630 --> 00:16:35,050
validate that it's still properly stored

00:16:32,140 --> 00:16:36,250
on the disk so let's say we have a

00:16:35,050 --> 00:16:38,620
checksum error

00:16:36,250 --> 00:16:40,540
so what ZFS is able to do then it goes

00:16:38,620 --> 00:16:43,330
to the mirror partner the other disk and

00:16:40,540 --> 00:16:45,940
checks its set checksum and when it

00:16:43,330 --> 00:16:49,060
detects that the other checksum is the

00:16:45,940 --> 00:16:51,310
correct one then it does two things one

00:16:49,060 --> 00:16:53,380
it sends the correct data up to the

00:16:51,310 --> 00:16:55,600
application so that the application can

00:16:53,380 --> 00:16:58,090
read it and process it accordingly but

00:16:55,600 --> 00:17:01,750
it also has the ability to heal the

00:16:58,090 --> 00:17:04,570
damaged disks by correcting the checksum

00:17:01,750 --> 00:17:07,000
under this one that's not the proper one

00:17:04,570 --> 00:17:10,690
by copying its own checksum to the

00:17:07,000 --> 00:17:13,870
mirror partner so that way both discs

00:17:10,690 --> 00:17:16,450
still have the original checksum from

00:17:13,870 --> 00:17:19,900
the file you originally saved so if your

00:17:16,450 --> 00:17:24,310
disk is dying ZFS has the ability to for

00:17:19,900 --> 00:17:26,500
a couple of moments or for a time to

00:17:24,310 --> 00:17:28,720
correct first this but you still need to

00:17:26,500 --> 00:17:32,410
figure out whether the disk is dying and

00:17:28,720 --> 00:17:35,080
replace it but in opposite to the

00:17:32,410 --> 00:17:37,180
example on the previous slide it doesn't

00:17:35,080 --> 00:17:39,160
give the damaged data up to the

00:17:37,180 --> 00:17:42,490
application so the application will

00:17:39,160 --> 00:17:45,880
always get correct data from ZFS if you

00:17:42,490 --> 00:17:47,770
have enough redundancy so ZFS is very

00:17:45,880 --> 00:17:50,410
powerful in that keeping your data safe

00:17:47,770 --> 00:17:52,810
in the long term and providing you with

00:17:50,410 --> 00:17:55,930
the actual data that you originally

00:17:52,810 --> 00:17:57,580
stored it also keeps a set of error

00:17:55,930 --> 00:17:58,480
counters so you can actually see if a

00:17:57,580 --> 00:18:00,100
disk is is

00:17:58,480 --> 00:18:05,049
throwing checksum errors or having read

00:18:00,100 --> 00:18:06,520
and write errors so it unlike a hardware

00:18:05,049 --> 00:18:08,380
RAID which will try to hide those errors

00:18:06,520 --> 00:18:10,570
from you until it's too late it will

00:18:08,380 --> 00:18:11,919
make sure that you're actually the

00:18:10,570 --> 00:18:13,780
errors are passed all the way up to the

00:18:11,919 --> 00:18:15,809
operating system from the hardware so

00:18:13,780 --> 00:18:17,919
that you can keep track of them yeah

00:18:15,809 --> 00:18:19,990
because there's a lot of stuff that can

00:18:17,919 --> 00:18:22,990
go wrong you might have faulty cables or

00:18:19,990 --> 00:18:26,320
your firmware driver of your write card

00:18:22,990 --> 00:18:27,610
is having some issues there's an honesty

00:18:26,320 --> 00:18:30,549
yeah you're right

00:18:27,610 --> 00:18:32,860
SSDs are still a little bit faulty or

00:18:30,549 --> 00:18:34,570
prone to errors so there's a lot of

00:18:32,860 --> 00:18:38,559
stuff that could go wrong in the whole

00:18:34,570 --> 00:18:42,940
aisle chain coming from your DRAM down

00:18:38,559 --> 00:18:44,980
to your actual storage media so to in

00:18:42,940 --> 00:18:47,290
order to help you detect those and

00:18:44,980 --> 00:18:50,919
correct them ZFS keeps snapshots as I

00:18:47,290 --> 00:18:54,340
told you but ZFS doesn't have a concept

00:18:50,919 --> 00:18:57,309
of file system check so no fsck that you

00:18:54,340 --> 00:18:59,140
need to run it can run the file system

00:18:57,309 --> 00:19:01,450
is because it's copy-on-write the file

00:18:59,140 --> 00:19:03,340
system is always consistent either the

00:19:01,450 --> 00:19:05,530
change happened or didn't and so there's

00:19:03,340 --> 00:19:09,190
never a need for a file system check

00:19:05,530 --> 00:19:11,740
right yeah so imagine you're running a

00:19:09,190 --> 00:19:14,160
file system check on a multi terabyte

00:19:11,740 --> 00:19:17,230
storage array that takes a lot of time

00:19:14,160 --> 00:19:19,600
ZFS does it basically in the background

00:19:17,230 --> 00:19:22,990
using a different approach called scrub

00:19:19,600 --> 00:19:25,929
so what it does is a periodic check that

00:19:22,990 --> 00:19:28,630
you can set yourself in intervals that

00:19:25,929 --> 00:19:30,130
you can define and scrubs basically go

00:19:28,630 --> 00:19:31,929
through each block and check the

00:19:30,130 --> 00:19:34,809
checksum whether this is still the

00:19:31,929 --> 00:19:37,150
correct one and if it isn't correct then

00:19:34,809 --> 00:19:41,590
it will try it self-healing capabilities

00:19:37,150 --> 00:19:45,460
to use the mirror partner or your write

00:19:41,590 --> 00:19:49,230
Z to reconstruct the proper checksum and

00:19:45,460 --> 00:19:52,660
copy it to the damaged ones you can run

00:19:49,230 --> 00:19:55,240
ZFS scrubs during off-hours

00:19:52,660 --> 00:19:57,040
but it doesn't take your storage offline

00:19:55,240 --> 00:19:59,740
you can still access and read and write

00:19:57,040 --> 00:20:02,799
from it while the checksum calculation

00:19:59,740 --> 00:20:06,580
is running and the scrub is basically a

00:20:02,799 --> 00:20:08,980
bit you know CPU hungry by doing all

00:20:06,580 --> 00:20:10,120
these checksum calculations but it's

00:20:08,980 --> 00:20:12,600
still better to have the storage

00:20:10,120 --> 00:20:16,179
available while the checks

00:20:12,600 --> 00:20:17,860
operation is running right so every read

00:20:16,179 --> 00:20:20,710
is checksum when you read it and make

00:20:17,860 --> 00:20:22,270
sure it's not corrupted but if you have

00:20:20,710 --> 00:20:24,340
data you're storing long term you don't

00:20:22,270 --> 00:20:25,780
read very frequently the scrub who's

00:20:24,340 --> 00:20:28,210
basically a Patrol read that goes

00:20:25,780 --> 00:20:29,710
through and checks that files that your

00:20:28,210 --> 00:20:32,409
aren't accessing frequently are still

00:20:29,710 --> 00:20:34,990
intact as well because if you detect the

00:20:32,409 --> 00:20:36,490
bit flip sooner it less chance that

00:20:34,990 --> 00:20:40,390
it'll happen on both discs and you won't

00:20:36,490 --> 00:20:42,340
have the ability to recover the data so

00:20:40,390 --> 00:20:45,429
Allen already mentioned the copy and

00:20:42,340 --> 00:20:48,700
write capabilities of ZFS so ZFS will

00:20:45,429 --> 00:20:50,530
never store the same block again on the

00:20:48,700 --> 00:20:52,840
so let's say you have an editor open or

00:20:50,530 --> 00:20:55,330
an application write something to a log

00:20:52,840 --> 00:20:57,520
file all the new writes will not go into

00:20:55,330 --> 00:21:00,490
that original file or the original block

00:20:57,520 --> 00:21:02,710
on disk but to a separate one which is

00:21:00,490 --> 00:21:05,260
the copy on write feature so in this

00:21:02,710 --> 00:21:07,390
example we always start from a

00:21:05,260 --> 00:21:10,960
consistent state and move the file

00:21:07,390 --> 00:21:12,789
system to another consistent state by

00:21:10,960 --> 00:21:14,860
let's say you're doing some rights over

00:21:12,789 --> 00:21:17,590
there your application writes data or

00:21:14,860 --> 00:21:19,840
your logging system or your archival

00:21:17,590 --> 00:21:22,710
storage so the blue blocks are the ones

00:21:19,840 --> 00:21:25,840
that are being changed at the moment and

00:21:22,710 --> 00:21:29,380
that change causes metadata to be

00:21:25,840 --> 00:21:31,659
written and this will cause this tree to

00:21:29,380 --> 00:21:34,600
be updated but it will never overwrite

00:21:31,659 --> 00:21:37,600
the yellow or orange ones because it's

00:21:34,600 --> 00:21:40,330
copying right it will never break or

00:21:37,600 --> 00:21:42,549
overwrite your current blocks that way

00:21:40,330 --> 00:21:44,230
when your power goes out you will never

00:21:42,549 --> 00:21:46,270
have an inconsistent state of the file

00:21:44,230 --> 00:21:48,510
system because either the change has

00:21:46,270 --> 00:21:51,400
happened or it has not happened yet and

00:21:48,510 --> 00:21:54,850
once all these metadata rights have

00:21:51,400 --> 00:21:57,580
happened then a new version of this ZFS

00:21:54,850 --> 00:22:00,549
tree is going to be written and that way

00:21:57,580 --> 00:22:03,250
you can have a consistent filesystem

00:22:00,549 --> 00:22:04,870
state each time it moves from consistent

00:22:03,250 --> 00:22:06,190
state to consistent state if it gets cut

00:22:04,870 --> 00:22:07,990
off in the middle it just basically

00:22:06,190 --> 00:22:12,400
rollback the transaction like in your

00:22:07,990 --> 00:22:15,419
database yeah

00:22:12,400 --> 00:22:18,789
what this gives us as an added bonus is

00:22:15,419 --> 00:22:20,710
snapshots very cheap snapshots because

00:22:18,789 --> 00:22:23,080
they don't require that much operations

00:22:20,710 --> 00:22:26,200
because you already do it basically in

00:22:23,080 --> 00:22:28,120
the background without knowing so

00:22:26,200 --> 00:22:30,010
you probably know snapshots that you can

00:22:28,120 --> 00:22:33,250
have an image of your current state of

00:22:30,010 --> 00:22:36,040
the file system ZFS supports millions of

00:22:33,250 --> 00:22:38,410
snapshots there's actually no limit that

00:22:36,040 --> 00:22:40,780
you could reach in a certain amount of

00:22:38,410 --> 00:22:42,820
time you can basically do snapshots

00:22:40,780 --> 00:22:45,430
every second if you are so inclined but

00:22:42,820 --> 00:22:47,320
it will only store the actual

00:22:45,430 --> 00:22:50,140
differences between the first snapshot

00:22:47,320 --> 00:22:51,160
and the state that the snapshot has that

00:22:50,140 --> 00:22:55,450
when you take it

00:22:51,160 --> 00:22:58,000
so using that model let's say I've

00:22:55,450 --> 00:22:59,680
changed some files here the metadata

00:22:58,000 --> 00:23:01,810
gets updated and it writes the uber

00:22:59,680 --> 00:23:04,450
block if you're taking a snapshot that

00:23:01,810 --> 00:23:06,070
old copy isn't thrown away or being

00:23:04,450 --> 00:23:08,560
recycled but it is kept it because

00:23:06,070 --> 00:23:10,600
that's your actual snapshot and if you

00:23:08,560 --> 00:23:12,550
take the next one the next version of

00:23:10,600 --> 00:23:15,160
this file system tree is being kept

00:23:12,550 --> 00:23:17,980
but ZFS only stores the actual Delta

00:23:15,160 --> 00:23:20,680
between those files so if there weren't

00:23:17,980 --> 00:23:24,190
many changes between those two versions

00:23:20,680 --> 00:23:25,750
then it doesn't take much disk space but

00:23:24,190 --> 00:23:32,080
if there were a lot of disk space

00:23:25,750 --> 00:23:34,090
changes then the snapshot will grow so

00:23:32,080 --> 00:23:37,060
here's the first example of how to

00:23:34,090 --> 00:23:40,810
create snapshot with CFS commands you

00:23:37,060 --> 00:23:42,610
immediately see that ZFS is very you

00:23:40,810 --> 00:23:45,550
know simple to use because it only has

00:23:42,610 --> 00:23:48,250
like two programs that you need to run

00:23:45,550 --> 00:23:51,550
or commands that you need to know ZFS

00:23:48,250 --> 00:23:55,120
and zpool so CFS the first one this one

00:23:51,550 --> 00:23:59,140
lists our current ZFS datasets that we

00:23:55,120 --> 00:24:01,720
have so I have a pool called tank and um

00:23:59,140 --> 00:24:04,330
under that one there's a dataset called

00:24:01,720 --> 00:24:06,400
home and I want to make a snapshot of

00:24:04,330 --> 00:24:11,260
that home directory because that's where

00:24:06,400 --> 00:24:15,190
all my important users are so you can

00:24:11,260 --> 00:24:18,070
see the backup one that I named it the

00:24:15,190 --> 00:24:20,410
ad basically separates the dataset the

00:24:18,070 --> 00:24:23,290
snapshot name from the dataset and you

00:24:20,410 --> 00:24:25,060
see the used capacity is zero because

00:24:23,290 --> 00:24:28,060
there weren't any changes between that

00:24:25,060 --> 00:24:31,360
snapshot and the previous one you

00:24:28,060 --> 00:24:34,660
already see also that on the available

00:24:31,360 --> 00:24:37,510
column the whole storage array gives the

00:24:34,660 --> 00:24:39,490
whole storage to all datasets that you

00:24:37,510 --> 00:24:41,289
have on your pool

00:24:39,490 --> 00:24:43,539
and so now I'm making some changes maybe

00:24:41,289 --> 00:24:46,649
a user store some files in there or

00:24:43,539 --> 00:24:50,049
you're doing backups or whatever

00:24:46,649 --> 00:24:51,789
workload that you have on your array and

00:24:50,049 --> 00:24:54,429
now you take a snack a second snapshot

00:24:51,789 --> 00:24:56,559
because you want to save that state and

00:24:54,429 --> 00:24:59,620
now when you look at the ZFS list output

00:24:56,559 --> 00:25:02,289
you can see that only the data changes

00:24:59,620 --> 00:25:03,909
that were happened between the first

00:25:02,289 --> 00:25:06,760
snapshot and the second one were

00:25:03,909 --> 00:25:09,220
recorded in this column and 27 megabytes

00:25:06,760 --> 00:25:12,429
in this case your mileage may vary but

00:25:09,220 --> 00:25:14,649
it's only the 27 or the actual

00:25:12,429 --> 00:25:17,919
difference between your last snapshot

00:25:14,649 --> 00:25:21,760
and first one not the full 31 gigabytes

00:25:17,919 --> 00:25:23,260
or 984 that we have here yeah so in the

00:25:21,760 --> 00:25:25,570
end you end up with three complete

00:25:23,260 --> 00:25:27,460
copies of your 31 gigabytes of data in

00:25:25,570 --> 00:25:29,140
the different states that you can access

00:25:27,460 --> 00:25:31,779
at any time but you only consumed an

00:25:29,140 --> 00:25:34,559
extra 27 megabytes of space to have all

00:25:31,779 --> 00:25:34,559
three copies in there

00:25:34,740 --> 00:25:40,000
so yeah let's say disaster happens you

00:25:37,870 --> 00:25:43,960
have a bad day it's Monday morning you

00:25:40,000 --> 00:25:47,380
had not coffee yet and your tap key is

00:25:43,960 --> 00:25:49,630
broken basically and you do an RM RF and

00:25:47,380 --> 00:25:51,940
if you're not careful your tab key broke

00:25:49,630 --> 00:25:55,750
and it doesn't expand home anymore you

00:25:51,940 --> 00:25:59,440
only wanted to delete one user and then

00:25:55,750 --> 00:26:01,419
suddenly you realize oh my home

00:25:59,440 --> 00:26:03,549
directories are all gone and suddenly

00:26:01,419 --> 00:26:05,260
two things happen you start to panic and

00:26:03,549 --> 00:26:06,669
your phone starts to ring because users

00:26:05,260 --> 00:26:10,510
are missing their home directories and

00:26:06,669 --> 00:26:14,830
the files in there what the ZFS to the

00:26:10,510 --> 00:26:17,890
rescue you do regular snapshots and that

00:26:14,830 --> 00:26:20,140
way you can save today by basically

00:26:17,890 --> 00:26:23,860
rolling back to the last-known state

00:26:20,140 --> 00:26:27,190
word RM RF didn't happen and that way

00:26:23,860 --> 00:26:29,230
you can quickly restore your data CFS

00:26:27,190 --> 00:26:33,669
rollback is super fast it takes a couple

00:26:29,230 --> 00:26:35,679
of seconds and you can answer the phone

00:26:33,669 --> 00:26:39,580
and say hey I already fixed the issue

00:26:35,679 --> 00:26:41,980
and now I getting some coffee so it's a

00:26:39,580 --> 00:26:43,750
very simple command just type ZFS

00:26:41,980 --> 00:26:46,450
rollback and then the snapshot name you

00:26:43,750 --> 00:26:49,299
want to roll back to and it will GU go

00:26:46,450 --> 00:26:50,919
back to that snapshot date or the stay

00:26:49,299 --> 00:26:54,389
that the file system was in during that

00:26:50,919 --> 00:26:54,389
when the snapshot was taken

00:26:54,810 --> 00:26:59,430
right so talking a bit more about the

00:26:57,910 --> 00:27:02,770
compression features that are built-in

00:26:59,430 --> 00:27:07,360
so you can basically as the data is

00:27:02,770 --> 00:27:09,070
being written into the ZFS buffer it can

00:27:07,360 --> 00:27:10,060
be compressed if it's being written to a

00:27:09,070 --> 00:27:12,700
data set that where you've enabled

00:27:10,060 --> 00:27:15,370
compression and then when it gets

00:27:12,700 --> 00:27:17,380
written to disk so you can end up with a

00:27:15,370 --> 00:27:19,030
higher throughput then you would

00:27:17,380 --> 00:27:20,290
normally get because you've confessed

00:27:19,030 --> 00:27:21,850
the large amount of data into a smaller

00:27:20,290 --> 00:27:23,140
amount and can write it faster than

00:27:21,850 --> 00:27:25,240
would be possible to write the original

00:27:23,140 --> 00:27:28,570
amount but you also get that back when

00:27:25,240 --> 00:27:29,920
you read so you know if you read 100

00:27:28,570 --> 00:27:32,140
megabytes of compressed data and it

00:27:29,920 --> 00:27:34,290
expands to 200 megabytes you've just

00:27:32,140 --> 00:27:38,140
read twice as much per second as well

00:27:34,290 --> 00:27:39,040
and it's automatically done transparent

00:27:38,140 --> 00:27:40,420
to the application so the application

00:27:39,040 --> 00:27:42,610
doesn't have to know about it

00:27:40,420 --> 00:27:45,250
and then there's a selection of

00:27:42,610 --> 00:27:47,590
different compression algorithms but the

00:27:45,250 --> 00:27:49,990
newer lz4 is recommended for almost

00:27:47,590 --> 00:27:54,340
everything because even on a laptop

00:27:49,990 --> 00:27:57,070
processor it can compress about 1.5

00:27:54,340 --> 00:27:59,020
gigabytes per second per core so in

00:27:57,070 --> 00:28:01,420
unless you're writing more than that on

00:27:59,020 --> 00:28:03,340
your laptop which is probably dead full

00:28:01,420 --> 00:28:05,350
you're not going to saturate your cpu

00:28:03,340 --> 00:28:07,450
with the compression and then for

00:28:05,350 --> 00:28:10,030
decompression it can do over 4 gigabytes

00:28:07,450 --> 00:28:12,370
per second per core again on the modest

00:28:10,030 --> 00:28:14,080
to point something gigahertz laptop with

00:28:12,370 --> 00:28:16,390
only 2 cores you know you put it on a

00:28:14,080 --> 00:28:18,250
Xeon with 32 cards and you're going to

00:28:16,390 --> 00:28:21,730
be able to more than keep up with your

00:28:18,250 --> 00:28:23,920
disks but as I mentioned it also has the

00:28:21,730 --> 00:28:25,210
early abort feature so it realizes when

00:28:23,920 --> 00:28:25,900
it's trying to confess something that's

00:28:25,210 --> 00:28:27,640
uncompressible

00:28:25,900 --> 00:28:29,350
or where the compression gain is not

00:28:27,640 --> 00:28:31,330
going to be enough to offset the cpu

00:28:29,350 --> 00:28:33,580
cost and it will skip compression on

00:28:31,330 --> 00:28:39,940
that file and write or at that block and

00:28:33,580 --> 00:28:41,950
write it uncompressed so for example one

00:28:39,940 --> 00:28:43,870
of the data sets that Benedict works

00:28:41,950 --> 00:28:44,910
with is called movie lens it's basically

00:28:43,870 --> 00:28:48,190
ratings

00:28:44,910 --> 00:28:50,320
for every movie ever or something so

00:28:48,190 --> 00:28:52,900
much yeah pretty much yeah so it's a

00:28:50,320 --> 00:28:55,480
large data set but it's mostly textual

00:28:52,900 --> 00:28:58,570
data and so it compresses quite well so

00:28:55,480 --> 00:29:01,510
in this case we will set it up and

00:28:58,570 --> 00:29:04,180
create data sets and compare the

00:29:01,510 --> 00:29:05,770
different compression settings so in

00:29:04,180 --> 00:29:06,500
this case I will create a new data set

00:29:05,770 --> 00:29:08,990
and set

00:29:06,500 --> 00:29:10,159
compression option to gzip and then

00:29:08,990 --> 00:29:14,029
create a second one where I've turned

00:29:10,159 --> 00:29:15,379
compression off and then rewrite the or

00:29:14,029 --> 00:29:18,289
we can take a look and we can see that

00:29:15,379 --> 00:29:20,179
we have one where the compression is set

00:29:18,289 --> 00:29:22,279
to gzip and one where it's off and

00:29:20,179 --> 00:29:23,779
currently our compression ratio is one

00:29:22,279 --> 00:29:27,320
to one because we haven't written any

00:29:23,779 --> 00:29:30,340
data yet but now we copy all the movie

00:29:27,320 --> 00:29:32,809
lens data into these two data sets and

00:29:30,340 --> 00:29:34,639
now we see that on the one with

00:29:32,809 --> 00:29:37,789
confession turned off our compression

00:29:34,639 --> 00:29:39,919
ratio is again one to one but on the

00:29:37,789 --> 00:29:42,379
where we've enabled gzip we now have

00:29:39,919 --> 00:29:46,360
stored that data 5.3 seven times more

00:29:42,379 --> 00:29:50,659
efficiently and so in the end we used

00:29:46,360 --> 00:29:53,870
673 megabytes of space instead of 3.5 to

00:29:50,659 --> 00:29:55,850
gigabytes and you can get the

00:29:53,870 --> 00:30:02,120
compression of that magnitude for most

00:29:55,850 --> 00:30:04,909
databases as well and then there's

00:30:02,120 --> 00:30:07,279
inheritance so because you can create

00:30:04,909 --> 00:30:09,740
datasets that are children of other data

00:30:07,279 --> 00:30:10,940
sets and on down very deep you don't

00:30:09,740 --> 00:30:13,639
want to have to manage all these

00:30:10,940 --> 00:30:15,340
settings on each individual data set so

00:30:13,639 --> 00:30:17,960
there's inheritance if we create a

00:30:15,340 --> 00:30:20,299
separate data set called back up under

00:30:17,960 --> 00:30:22,730
gzip it automatically inherits the

00:30:20,299 --> 00:30:26,029
setting from its parent although you can

00:30:22,730 --> 00:30:27,679
still override it locally if you want so

00:30:26,029 --> 00:30:30,919
this gives the administrator the ability

00:30:27,679 --> 00:30:33,200
to really define how they want the whole

00:30:30,919 --> 00:30:34,399
tree so you can easily turn confession

00:30:33,200 --> 00:30:36,740
on for everything at the root of your

00:30:34,399 --> 00:30:38,750
tree and then just decide to disable it

00:30:36,740 --> 00:30:41,929
from one specific workload on one

00:30:38,750 --> 00:30:43,460
specific data set and then if you want

00:30:41,929 --> 00:30:45,500
to restore the setting if you use the

00:30:43,460 --> 00:30:48,350
inherit command and tell it which

00:30:45,500 --> 00:30:50,690
property it will then take the property

00:30:48,350 --> 00:30:52,009
from its parent instead of erasing

00:30:50,690 --> 00:30:57,200
whatever local setting you might have

00:30:52,009 --> 00:31:00,740
said yeah another feature is

00:30:57,200 --> 00:31:03,409
deduplication so we're storing more and

00:31:00,740 --> 00:31:05,899
more data each day and ZFS has the

00:31:03,409 --> 00:31:08,570
capability of checking whether you are

00:31:05,899 --> 00:31:12,759
storing multiple copies of your data on

00:31:08,570 --> 00:31:14,929
your z pool how do they do that

00:31:12,759 --> 00:31:17,600
relatively easy because they already

00:31:14,929 --> 00:31:20,299
keep checksum of each block that you're

00:31:17,600 --> 00:31:22,549
storing and ZFS deduplication is

00:31:20,299 --> 00:31:24,349
block-level based so what they do is

00:31:22,549 --> 00:31:26,809
they check each incoming block and when

00:31:24,349 --> 00:31:28,940
you activate it application and if that

00:31:26,809 --> 00:31:31,549
block check some matches another block

00:31:28,940 --> 00:31:33,440
then that block gets replaced by a

00:31:31,549 --> 00:31:36,799
pointer which is a couple of bytes in

00:31:33,440 --> 00:31:39,739
length so that way you can store or save

00:31:36,799 --> 00:31:41,419
a lot of disk space by just referencing

00:31:39,739 --> 00:31:43,879
blocks that are already there with other

00:31:41,419 --> 00:31:46,459
blocks so imagine you're storing a

00:31:43,879 --> 00:31:49,070
couple of like research file that we do

00:31:46,459 --> 00:31:51,799
at our university and each user copies

00:31:49,070 --> 00:31:54,889
that original research data set into

00:31:51,799 --> 00:31:57,739
their home directory each one basically

00:31:54,889 --> 00:32:00,399
blows up the storage because everyone

00:31:57,739 --> 00:32:03,139
copies the same data into his own

00:32:00,399 --> 00:32:06,919
directories but the ZFS deduplication

00:32:03,139 --> 00:32:10,039
available it can detect that this is a

00:32:06,919 --> 00:32:13,070
multiple of the original block that are

00:32:10,039 --> 00:32:15,039
stored on the array and then it just

00:32:13,070 --> 00:32:17,209
replaces those with links to the

00:32:15,039 --> 00:32:20,479
original data set which saves a lot of

00:32:17,209 --> 00:32:23,239
space we have a short example here on

00:32:20,479 --> 00:32:25,969
the next slide which shows this so

00:32:23,239 --> 00:32:29,869
switch that one yep thanks so in this

00:32:25,969 --> 00:32:32,389
example we show a couple of information

00:32:29,869 --> 00:32:34,190
about our z pool the name it's called

00:32:32,389 --> 00:32:37,070
movie lens because that's our current

00:32:34,190 --> 00:32:39,109
project and the size the allocation and

00:32:37,070 --> 00:32:41,629
the free space and there's also a column

00:32:39,109 --> 00:32:45,349
here that tells you the DTaP ratio which

00:32:41,629 --> 00:32:47,899
is the column to the right here and now

00:32:45,349 --> 00:32:48,589
we're looking at how much space our

00:32:47,899 --> 00:32:51,950
movie lens

00:32:48,589 --> 00:32:53,450
original has so it's like six hundred

00:32:51,950 --> 00:32:54,649
seventy three megabytes at the moment

00:32:53,450 --> 00:32:56,329
that's all that's that's the compressed

00:32:54,649 --> 00:32:58,190
version so you can actually do the

00:32:56,329 --> 00:32:59,659
compressed version yeah combined those

00:32:58,190 --> 00:33:02,479
up all the way yeah

00:32:59,659 --> 00:33:05,029
so in this example we have like three

00:33:02,479 --> 00:33:07,190
users on each one copies that original

00:33:05,029 --> 00:33:11,139
data set into his own home directory or

00:33:07,190 --> 00:33:14,539
in his work directory and once that done

00:33:11,139 --> 00:33:16,999
ZFS when it activated the heat

00:33:14,539 --> 00:33:18,979
application it is able to detect oh it

00:33:16,999 --> 00:33:20,719
has three copies of that original data

00:33:18,979 --> 00:33:23,809
set already because the check sums match

00:33:20,719 --> 00:33:26,659
and that way we achieve a bit

00:33:23,809 --> 00:33:29,539
application ratio of three and you can

00:33:26,659 --> 00:33:31,099
see in the allocation column that no

00:33:29,539 --> 00:33:34,250
additional disk space was used because

00:33:31,099 --> 00:33:36,650
only a couple of bytes were used for

00:33:34,250 --> 00:33:39,410
the link to the other copies of the file

00:33:36,650 --> 00:33:41,840
system or the actual file system blocks

00:33:39,410 --> 00:33:44,180
and the other interesting things this

00:33:41,840 --> 00:33:46,820
also meant you avoided doing those

00:33:44,180 --> 00:33:48,830
rights to disk so your disk throughput

00:33:46,820 --> 00:33:50,720
and been with was saved for this and so

00:33:48,830 --> 00:33:53,510
this copy operation was faster than it

00:33:50,720 --> 00:33:54,410
would have been normally and it left the

00:33:53,510 --> 00:33:59,000
bandwidth available to other

00:33:54,410 --> 00:34:00,710
applications on your system so it works

00:33:59,000 --> 00:34:02,180
very well for virtual machines you know

00:34:00,710 --> 00:34:04,400
if you have a large array of virtual

00:34:02,180 --> 00:34:06,770
machines stored on your set of s pool

00:34:04,400 --> 00:34:08,270
you know they all have the same blocks

00:34:06,770 --> 00:34:10,700
for the operating system you know at

00:34:08,270 --> 00:34:12,500
least some share of most of your virtual

00:34:10,700 --> 00:34:14,179
machines is going to be the same because

00:34:12,500 --> 00:34:20,960
usually you run the same operating

00:34:14,179 --> 00:34:23,179
system on most of them yeah so as cool

00:34:20,960 --> 00:34:26,000
as deduplication might be you have to be

00:34:23,179 --> 00:34:27,770
careful because all these comparisons of

00:34:26,000 --> 00:34:31,490
the checksum have to be stored somewhere

00:34:27,770 --> 00:34:34,010
and ZFS is already hungry for memory

00:34:31,490 --> 00:34:36,710
you should give CFS as much memory as

00:34:34,010 --> 00:34:37,910
you can but also should leave some space

00:34:36,710 --> 00:34:39,410
for your operating system and your

00:34:37,910 --> 00:34:42,830
actual other applications that are

00:34:39,410 --> 00:34:45,530
running so ZFS when you activate the

00:34:42,830 --> 00:34:48,290
DITA plication you also have to store

00:34:45,530 --> 00:34:50,240
the actual deduplication table DDT into

00:34:48,290 --> 00:34:52,480
main memory because it has to make these

00:34:50,240 --> 00:34:55,880
comparisons of all these checksums

00:34:52,480 --> 00:34:58,360
that's why it's going to be it's

00:34:55,880 --> 00:35:01,460
difficult to recommend deduplication

00:34:58,360 --> 00:35:02,840
there might be workloads that are well

00:35:01,460 --> 00:35:06,110
worth it because they're used multiple

00:35:02,840 --> 00:35:08,120
times on the same storage array but also

00:35:06,110 --> 00:35:09,830
comes up with the benefit or with the

00:35:08,120 --> 00:35:13,180
drawback that you have to give it

00:35:09,830 --> 00:35:15,860
additional memory to process these

00:35:13,180 --> 00:35:18,470
references in the deduct table well in

00:35:15,860 --> 00:35:21,710
particular basically it's a giant list

00:35:18,470 --> 00:35:23,570
of sha-256 hashes for each block and so

00:35:21,710 --> 00:35:25,040
when a new write comes in it has to be

00:35:23,570 --> 00:35:26,630
compared against the whole list if the

00:35:25,040 --> 00:35:28,670
list doesn't fit in RAM and has to go to

00:35:26,630 --> 00:35:30,710
disk then you have to read the whole

00:35:28,670 --> 00:35:32,510
thing off disk every time you want to do

00:35:30,710 --> 00:35:34,430
a write and your performance will

00:35:32,510 --> 00:35:36,650
plummet but yeah as long as you have

00:35:34,430 --> 00:35:40,580
enough RAM to keep the table in memory

00:35:36,650 --> 00:35:42,500
it won't be an issue you need about 4

00:35:40,580 --> 00:35:44,720
gigabytes of RAM 4 terabyte of storage

00:35:42,500 --> 00:35:47,030
that you want to D do but it can really

00:35:44,720 --> 00:35:47,690
depend on your block size you'll get

00:35:47,030 --> 00:35:49,790
better deed

00:35:47,690 --> 00:35:52,430
uses smaller block size like 4 or 8

00:35:49,790 --> 00:35:53,839
kilobytes but that means there's that

00:35:52,430 --> 00:35:55,730
many more hashes for the same amount of

00:35:53,839 --> 00:35:59,390
storage whereas if you used larger

00:35:55,730 --> 00:36:03,319
blocks you'll only get the D dupe win

00:35:59,390 --> 00:36:05,450
from having if that whole block is the

00:36:03,319 --> 00:36:08,990
same but you'll have fewer blocks and

00:36:05,450 --> 00:36:11,270
it'll take less memory yeah so remember

00:36:08,990 --> 00:36:12,710
earlier we I told you that there are

00:36:11,270 --> 00:36:15,410
basically two commands if you have to

00:36:12,710 --> 00:36:17,930
know ZFS and Z pool that was actually a

00:36:15,410 --> 00:36:19,640
lie there is a third command but most

00:36:17,930 --> 00:36:24,650
users should not touch that one because

00:36:19,640 --> 00:36:27,470
it's the ZFS debugger ztb and that one

00:36:24,650 --> 00:36:30,560
can simulate the DTaP location gains

00:36:27,470 --> 00:36:34,010
that you might have for a given data set

00:36:30,560 --> 00:36:35,690
so you are unsure whether you want to

00:36:34,010 --> 00:36:38,359
activate it application or not because

00:36:35,690 --> 00:36:41,450
of the memory the additional memory

00:36:38,359 --> 00:36:42,770
needs then ZFS has the ability to

00:36:41,450 --> 00:36:45,140
simulate the actual heat application

00:36:42,770 --> 00:36:48,470
gains for a certain storage pool that

00:36:45,140 --> 00:36:50,510
you can give it and in this case it

00:36:48,470 --> 00:36:52,609
calculates that it would not be

00:36:50,510 --> 00:36:54,109
beneficial to activate it application on

00:36:52,609 --> 00:36:58,010
this pool because there aren't that many

00:36:54,109 --> 00:37:02,000
copies available and uses that using the

00:36:58,010 --> 00:37:04,579
ref counts of the actual check sums that

00:37:02,000 --> 00:37:06,829
it calculated from the blocks but there

00:37:04,579 --> 00:37:10,310
are other workloads or situations where

00:37:06,829 --> 00:37:12,619
you have a lot of copies multiple store

00:37:10,310 --> 00:37:16,640
multiple times on your storage array and

00:37:12,619 --> 00:37:20,720
in these cases the application is very

00:37:16,640 --> 00:37:23,960
beneficial for you so we initially we're

00:37:20,720 --> 00:37:26,210
excited by that feature but we recommend

00:37:23,960 --> 00:37:28,069
nowadays to first activate compression

00:37:26,210 --> 00:37:30,050
and then check out whether deduplication

00:37:28,069 --> 00:37:32,560
is beneficial to you because we have

00:37:30,050 --> 00:37:35,150
always there always gains with using

00:37:32,560 --> 00:37:37,760
compression but deduplication is kind of

00:37:35,150 --> 00:37:40,520
a two-edged sword yeah there's some

00:37:37,760 --> 00:37:42,109
ongoing work to make the deduplication

00:37:40,520 --> 00:37:45,170
table smarter to be able to handle the

00:37:42,109 --> 00:37:47,510
situation where your list of hashes is

00:37:45,170 --> 00:37:51,589
outgrowing your amount of memory but

00:37:47,510 --> 00:37:53,660
that's still ongoing work all right so

00:37:51,589 --> 00:37:56,560
one of the more important things the set

00:37:53,660 --> 00:37:58,880
of s is the ability to actually delegate

00:37:56,560 --> 00:38:00,230
commands to users so that you don't have

00:37:58,880 --> 00:38:01,490
to do as much of the work as the storage

00:38:00,230 --> 00:38:03,890
administrator but

00:38:01,490 --> 00:38:05,600
so you have to protect yourselves from

00:38:03,890 --> 00:38:07,550
the users because once you give them the

00:38:05,600 --> 00:38:10,310
ability to do things they will do things

00:38:07,550 --> 00:38:13,369
they shouldn't do so in this particular

00:38:10,310 --> 00:38:15,800
case we want to give a user the ability

00:38:13,369 --> 00:38:17,240
to manage their own snapshots all right

00:38:15,800 --> 00:38:18,980
so we create a separate data set for

00:38:17,240 --> 00:38:21,440
each user for their home directory and

00:38:18,980 --> 00:38:23,720
we can delegate to them with ZFS allow

00:38:21,440 --> 00:38:25,340
so we allow the user Alice to create

00:38:23,720 --> 00:38:27,890
snapshots and roll back those snapshots

00:38:25,340 --> 00:38:31,010
on their data set but their dates that

00:38:27,890 --> 00:38:32,690
only and then if we do the ZFS allowed

00:38:31,010 --> 00:38:34,790
command it shows us what they're allowed

00:38:32,690 --> 00:38:37,550
to do so now this user would be able to

00:38:34,790 --> 00:38:39,920
take snapshots of the home directory

00:38:37,550 --> 00:38:41,119
before they do something that might mess

00:38:39,920 --> 00:38:43,970
up their data where they won't want to

00:38:41,119 --> 00:38:45,890
rewind and then they have the ability to

00:38:43,970 --> 00:38:49,660
do the rollback themselves as well but

00:38:45,890 --> 00:38:49,660
they can't touch any other data sets ah

00:38:49,810 --> 00:38:55,070
and so you can see here if we switch to

00:38:52,820 --> 00:38:56,660
the user Alice and run the command and

00:38:55,070 --> 00:38:59,900
we can see that they've created the

00:38:56,660 --> 00:39:03,680
snapshot but you know snapshots have a

00:38:59,900 --> 00:39:05,210
cost and if you have thousands and

00:39:03,680 --> 00:39:06,770
thousands of them when you try to run

00:39:05,210 --> 00:39:09,500
those NFS Liscomb and it'll be slower

00:39:06,770 --> 00:39:11,180
and so you can enforce a limit on how

00:39:09,500 --> 00:39:13,550
many snapshots a user is allowed to have

00:39:11,180 --> 00:39:14,750
so they can't basically do it denial of

00:39:13,550 --> 00:39:17,480
service against your system by just

00:39:14,750 --> 00:39:18,920
creating snapshots in a loop and so

00:39:17,480 --> 00:39:20,630
we've limited the number of snapshots

00:39:18,920 --> 00:39:22,310
Alice can create and if they try to

00:39:20,630 --> 00:39:25,250
create too many then they get the

00:39:22,310 --> 00:39:26,900
permission denied error so you can limit

00:39:25,250 --> 00:39:29,570
the number of file systems or the number

00:39:26,900 --> 00:39:32,030
of snapshots that can be created as

00:39:29,570 --> 00:39:33,560
children of a certain data set so when

00:39:32,030 --> 00:39:38,930
you delegate permission to it you can

00:39:33,560 --> 00:39:41,359
still have the limit the interesting

00:39:38,930 --> 00:39:43,460
thing is there's actually delegation of

00:39:41,359 --> 00:39:45,800
those permissions as well or inheritance

00:39:43,460 --> 00:39:47,720
of the delegated permissions so if you

00:39:45,800 --> 00:39:49,700
give the user the ability to create a

00:39:47,720 --> 00:39:51,950
dataset under their home directory to

00:39:49,700 --> 00:39:54,350
create a second file system where they

00:39:51,950 --> 00:39:55,760
can have different settings then it

00:39:54,350 --> 00:40:01,070
automatically inherits the permissions

00:39:55,760 --> 00:40:04,730
they had on the parent but you can also

00:40:01,070 --> 00:40:07,670
set basically a sticky bits type of

00:40:04,730 --> 00:40:09,080
permissions so that rather than

00:40:07,670 --> 00:40:11,619
inheriting the permissions from the

00:40:09,080 --> 00:40:13,760
parent that will get certain permissions

00:40:11,619 --> 00:40:15,500
only if they were the one that created

00:40:13,760 --> 00:40:19,370
the data set

00:40:15,500 --> 00:40:21,500
so an example here so we want to allow

00:40:19,370 --> 00:40:23,900
Alice to create a mountain new data sets

00:40:21,500 --> 00:40:25,820
on her home directory but we don't want

00:40:23,900 --> 00:40:28,580
to allow Alice to actually destroy her

00:40:25,820 --> 00:40:31,460
home directory but by creating this

00:40:28,580 --> 00:40:33,770
create time permission of destroy it

00:40:31,460 --> 00:40:36,620
means that Alice will gain the ability

00:40:33,770 --> 00:40:38,780
to destroy any data that she creates but

00:40:36,620 --> 00:40:42,350
not actually getting the ability to

00:40:38,780 --> 00:40:44,450
destroy her home directory dataset and

00:40:42,350 --> 00:40:46,820
so only the data set she creates can she

00:40:44,450 --> 00:40:48,980
get rid of but her other permissions

00:40:46,820 --> 00:40:50,630
like the ability to create new data sets

00:40:48,980 --> 00:40:52,520
and mounts are automatically inherited

00:40:50,630 --> 00:40:54,320
so when she creates a new data set

00:40:52,520 --> 00:40:56,810
she'll have the ability to destroy that

00:40:54,320 --> 00:41:00,100
but also inherit the ability to create

00:40:56,810 --> 00:41:03,080
and mount children of that new data set

00:41:00,100 --> 00:41:07,040
and so you can see with that system you

00:41:03,080 --> 00:41:09,470
can easily delegate the permissions and

00:41:07,040 --> 00:41:12,410
create stuff another example is you

00:41:09,470 --> 00:41:13,610
could create a dataset called projects

00:41:12,410 --> 00:41:15,170
or something and every user would have

00:41:13,610 --> 00:41:17,180
the ability to create new datasets in

00:41:15,170 --> 00:41:20,180
there but only the person who created

00:41:17,180 --> 00:41:22,460
the data set would be able to destroy it

00:41:20,180 --> 00:41:24,980
or you can also delegate to groups

00:41:22,460 --> 00:41:26,930
instead so this would allow you to

00:41:24,980 --> 00:41:28,790
create a dataset where all users have

00:41:26,930 --> 00:41:30,680
permissions to read stuff from every

00:41:28,790 --> 00:41:33,170
project but only the project leader on

00:41:30,680 --> 00:41:35,180
each project can you know create or

00:41:33,170 --> 00:41:36,500
destroy datasets and or manage the

00:41:35,180 --> 00:41:42,440
snapshots or whatever you wanted to

00:41:36,500 --> 00:41:44,750
allow the other things NFS has this

00:41:42,440 --> 00:41:46,970
integration with the container system so

00:41:44,750 --> 00:41:49,520
on Solaris with zones and on FreeBSD

00:41:46,970 --> 00:41:51,740
with jails you can actually delegate a

00:41:49,520 --> 00:41:54,380
dataset into the container

00:41:51,740 --> 00:41:56,030
so now root in that container has full

00:41:54,380 --> 00:41:58,280
access to that data set as if it was

00:41:56,030 --> 00:42:00,260
their own pool and so they can do

00:41:58,280 --> 00:42:02,090
whatever commands they want inside there

00:42:00,260 --> 00:42:04,490
and create more datasets and change the

00:42:02,090 --> 00:42:08,090
mount points and and set any properties

00:42:04,490 --> 00:42:11,360
they want but it automatically protects

00:42:08,090 --> 00:42:15,080
the air makes it so that data set cannot

00:42:11,360 --> 00:42:17,120
be used on the host system so that you

00:42:15,080 --> 00:42:18,800
know the malicious user in the container

00:42:17,120 --> 00:42:21,650
creates a new data set and sets the

00:42:18,800 --> 00:42:23,480
mount point to /et see if that ever got

00:42:21,650 --> 00:42:25,190
mounted on your host system that would

00:42:23,480 --> 00:42:27,080
replace your password file with the one

00:42:25,190 --> 00:42:28,400
they've chosen and now they get a

00:42:27,080 --> 00:42:30,859
cessation and

00:42:28,400 --> 00:42:33,049
Brut your system or whatever so it has

00:42:30,859 --> 00:42:34,880
built-in support for dealing with

00:42:33,049 --> 00:42:37,130
containers and also isolating those data

00:42:34,880 --> 00:42:39,470
sets so that they're not accessible from

00:42:37,130 --> 00:42:43,069
the host system in such a way that they

00:42:39,470 --> 00:42:44,960
could be used as an attack vector it

00:42:43,069 --> 00:42:48,589
doesn't work on Linux yet because

00:42:44,960 --> 00:42:50,210
there's no with C groups and so on they

00:42:48,589 --> 00:42:51,859
don't actually have the concept of a

00:42:50,210 --> 00:42:53,900
container just the namespace for a

00:42:51,859 --> 00:42:55,539
specific application so it doesn't quite

00:42:53,900 --> 00:42:58,249
work yet but there's ongoing work to

00:42:55,539 --> 00:43:03,740
have something of this nature work on

00:42:58,249 --> 00:43:05,930
Linux as well but it becomes a big deal

00:43:03,740 --> 00:43:08,299
when you can delegate a data set a

00:43:05,930 --> 00:43:10,220
completely separate filesystem for the

00:43:08,299 --> 00:43:11,749
container to have but you can set a

00:43:10,220 --> 00:43:14,869
quota on it so they can only use so much

00:43:11,749 --> 00:43:16,640
space or you can allow them to create as

00:43:14,869 --> 00:43:19,309
many sub file systems as they they

00:43:16,640 --> 00:43:20,900
require but you can also a gain the

00:43:19,309 --> 00:43:22,609
limit on the number of file systems or

00:43:20,900 --> 00:43:29,990
snapshots they can create is

00:43:22,609 --> 00:43:32,059
automatically set up for you yeah

00:43:29,990 --> 00:43:33,739
so once you've delegated the permission

00:43:32,059 --> 00:43:35,630
into the container route in there has

00:43:33,739 --> 00:43:37,730
complete control over that sub data set

00:43:35,630 --> 00:43:39,559
except for the ability to override the

00:43:37,730 --> 00:43:41,539
quotas that you set for them on the

00:43:39,559 --> 00:43:44,499
amount of space or the number of file

00:43:41,539 --> 00:43:44,499
systems or snapshots

00:43:44,559 --> 00:43:49,579
yes EFS also has another feature called

00:43:47,390 --> 00:43:52,849
easy realization so you can serialize

00:43:49,579 --> 00:43:55,099
all your storage to another system so

00:43:52,849 --> 00:43:58,430
what it can do is you can say send this

00:43:55,099 --> 00:44:00,079
data set snapshot over the network or to

00:43:58,430 --> 00:44:03,019
another pool on your local system and

00:44:00,079 --> 00:44:04,910
receive it there and store it as being a

00:44:03,019 --> 00:44:07,730
copy of the original pool so you can

00:44:04,910 --> 00:44:09,559
have standby systems by periodically

00:44:07,730 --> 00:44:11,630
sending your snapshots over to a

00:44:09,559 --> 00:44:13,579
secondary system and when that first

00:44:11,630 --> 00:44:14,420
original system breaks or is not

00:44:13,579 --> 00:44:17,029
available anymore

00:44:14,420 --> 00:44:19,549
you can fire up the second system

00:44:17,029 --> 00:44:23,900
without worrying that the file systems

00:44:19,549 --> 00:44:27,289
are not in sync so basically you can

00:44:23,900 --> 00:44:29,599
take a snapshot and which is all the box

00:44:27,289 --> 00:44:32,359
and serialize it into a stream and send

00:44:29,599 --> 00:44:34,279
it over a pipe it's completely one-way

00:44:32,359 --> 00:44:36,259
so it doesn't require any communication

00:44:34,279 --> 00:44:37,999
back from the other side so you don't

00:44:36,259 --> 00:44:40,130
actually have to receive it at that time

00:44:37,999 --> 00:44:42,080
you can do is that if I send serialize

00:44:40,130 --> 00:44:43,460
the data and write it to a tape

00:44:42,080 --> 00:44:45,560
maybe decide to restore it a couple of

00:44:43,460 --> 00:44:47,330
years from now or you could pipe it

00:44:45,560 --> 00:44:49,730
directly into the set of s receive and

00:44:47,330 --> 00:44:50,990
recreate that filesystem on the same

00:44:49,730 --> 00:44:53,390
machine or another machine over the

00:44:50,990 --> 00:44:57,170
network or whatever you want to do and

00:44:53,390 --> 00:44:58,790
it also supports incremental snapshots

00:44:57,170 --> 00:45:00,320
as well so you can send only the

00:44:58,790 --> 00:45:02,240
difference between one snapshot and a

00:45:00,320 --> 00:45:04,820
second snapshot right so we have an

00:45:02,240 --> 00:45:07,280
example here we have two pools in this

00:45:04,820 --> 00:45:08,870
case our primary pool which is called

00:45:07,280 --> 00:45:11,120
tank you can give it any name you want

00:45:08,870 --> 00:45:13,190
it's just a very common example here and

00:45:11,120 --> 00:45:15,310
you have a backup pool that stores all

00:45:13,190 --> 00:45:18,800
your backup from the original pool and

00:45:15,310 --> 00:45:21,410
when we list our snap shots using ZFS

00:45:18,800 --> 00:45:24,740
lists that - te snapshot and only lists

00:45:21,410 --> 00:45:27,290
our snapshots and we see that tank has a

00:45:24,740 --> 00:45:29,660
snapshot name called backup one normally

00:45:27,290 --> 00:45:32,810
you would give it a more descriptive

00:45:29,660 --> 00:45:35,780
name like a date or some specifics that

00:45:32,810 --> 00:45:39,170
are internal to your project or your

00:45:35,780 --> 00:45:43,040
storage and then you can say ZFS sent

00:45:39,170 --> 00:45:45,860
and use that snapshot name and it writes

00:45:43,040 --> 00:45:47,690
into the pipe and ZFS receives on the

00:45:45,860 --> 00:45:51,680
other side reads from that pipe and

00:45:47,690 --> 00:45:54,260
stores it in your backup pool but since

00:45:51,680 --> 00:45:58,730
you can use a pipe you can also type it

00:45:54,260 --> 00:46:00,680
into SSH and move that data set to

00:45:58,730 --> 00:46:03,410
another file or to another host and

00:46:00,680 --> 00:46:05,630
that's nice having this replication over

00:46:03,410 --> 00:46:07,970
an encryption so that people aren't able

00:46:05,630 --> 00:46:10,940
to read on the wire and see what kind of

00:46:07,970 --> 00:46:15,890
data you're transmitting because SSH

00:46:10,940 --> 00:46:17,600
provides the whole encryption layer and

00:46:15,890 --> 00:46:19,280
you can also do incremental backups so

00:46:17,600 --> 00:46:22,760
you don't have to send everything over

00:46:19,280 --> 00:46:25,010
the wire each time since CFS has the

00:46:22,760 --> 00:46:27,170
ability to detect what are the

00:46:25,010 --> 00:46:29,600
difference between each snapshot it only

00:46:27,170 --> 00:46:30,770
sends those differences over the wire so

00:46:29,600 --> 00:46:34,160
and if there aren't many differences

00:46:30,770 --> 00:46:35,660
then the transmission isn't going to be

00:46:34,160 --> 00:46:37,250
very long yeah

00:46:35,660 --> 00:46:39,140
the big difference over something like

00:46:37,250 --> 00:46:40,640
our sink where you're going to stat each

00:46:39,140 --> 00:46:42,710
file in your you know walk the entire

00:46:40,640 --> 00:46:44,660
directory tree at each file see if the

00:46:42,710 --> 00:46:46,790
timestamps match and if not have to

00:46:44,660 --> 00:46:48,230
check some each block of the files until

00:46:46,790 --> 00:46:51,440
you find the differences and then send a

00:46:48,230 --> 00:46:53,720
delta ZFS because those are doing the

00:46:51,440 --> 00:46:55,520
copy-on-write in its metadata it knows

00:46:53,720 --> 00:46:58,160
for each block on the disk

00:46:55,520 --> 00:46:59,690
block was last updated and so when you

00:46:58,160 --> 00:47:02,900
ask it to do recommend

00:46:59,690 --> 00:47:04,130
incremental replication it basically

00:47:02,900 --> 00:47:06,080
finds every block that's been modified

00:47:04,130 --> 00:47:08,510
between the start time and the end time

00:47:06,080 --> 00:47:10,970
the dates of those two snapshots and

00:47:08,510 --> 00:47:14,510
then just sends those over the pipe and

00:47:10,970 --> 00:47:16,310
because it's all one way it doesn't have

00:47:14,510 --> 00:47:18,200
to worry about communication with the

00:47:16,310 --> 00:47:20,600
other side so it writes to the pipe as

00:47:18,200 --> 00:47:23,690
fast as they can and so it can easily

00:47:20,600 --> 00:47:25,490
saturate a 10 gigabit network uplink as

00:47:23,690 --> 00:47:29,210
long as your disks can keep up

00:47:25,490 --> 00:47:30,830
so the replication is very very fast and

00:47:29,210 --> 00:47:32,840
designed to basically saturate your

00:47:30,830 --> 00:47:37,010
network or your disks whichever runs out

00:47:32,840 --> 00:47:39,020
of bandwidth first so it you know when

00:47:37,010 --> 00:47:41,630
you compare it to running our sync over

00:47:39,020 --> 00:47:43,670
a data set of 10 terabytes it can take

00:47:41,630 --> 00:47:45,740
you know if you only have a couple of

00:47:43,670 --> 00:47:47,930
gigabytes of changes set FS will finish

00:47:45,740 --> 00:47:50,150
in a few seconds whereas our sync is

00:47:47,930 --> 00:47:51,380
going to take 12 or more hours just

00:47:50,150 --> 00:47:53,390
because it has to stat through all the

00:47:51,380 --> 00:47:55,070
files especially if there are a lot of

00:47:53,390 --> 00:47:56,660
small files so you know that slows

00:47:55,070 --> 00:47:58,850
Arsenic down even more

00:47:56,660 --> 00:48:00,830
but ZFS isn't looking at the files it's

00:47:58,850 --> 00:48:03,530
only looking at the blocks on the on the

00:48:00,830 --> 00:48:05,090
disk and so it doesn't matter if it's

00:48:03,530 --> 00:48:06,830
one really large file or a bunch of

00:48:05,090 --> 00:48:12,500
small files because at full speed either

00:48:06,830 --> 00:48:14,780
way yeah that's the example for the

00:48:12,500 --> 00:48:18,800
incremental snapshot but what you can

00:48:14,780 --> 00:48:22,609
also do since CFS is or open CFS is a

00:48:18,800 --> 00:48:24,560
open system and you can rely on it and

00:48:22,609 --> 00:48:26,090
look inside it it's not a proprietary

00:48:24,560 --> 00:48:28,820
thing where you have to call the vendor

00:48:26,090 --> 00:48:31,220
if there's something going wrong you can

00:48:28,820 --> 00:48:34,520
look inside all these streams so in this

00:48:31,220 --> 00:48:36,710
example we have a demo snapshot and then

00:48:34,520 --> 00:48:38,720
we make some changes to a text file open

00:48:36,710 --> 00:48:41,359
source datacenter Conference txc and

00:48:38,720 --> 00:48:43,730
then we create a separate snapshot after

00:48:41,359 --> 00:48:45,800
that so now we have only our text file

00:48:43,730 --> 00:48:51,170
in as a delta between those two

00:48:45,800 --> 00:48:53,590
snapshots and there is a flag called - I

00:48:51,170 --> 00:48:56,030
- ZFS sent which tells us to

00:48:53,590 --> 00:48:57,740
incrementally send these snapshots only

00:48:56,030 --> 00:48:59,300
the differences yeah so we're only

00:48:57,740 --> 00:49:01,250
sending the block sort of change between

00:48:59,300 --> 00:49:03,020
that a snapshot we created in the B

00:49:01,250 --> 00:49:05,300
snapshot and that should only be our

00:49:03,020 --> 00:49:07,340
echo to the one file yeah and we send it

00:49:05,300 --> 00:49:09,230
to a local file called difft of Z sent

00:49:07,340 --> 00:49:11,300
because we want to look inside

00:49:09,230 --> 00:49:14,480
to see whether everything is in there

00:49:11,300 --> 00:49:16,190
that we are expecting and there's yet

00:49:14,480 --> 00:49:19,280
another command called Jetstream dump

00:49:16,190 --> 00:49:22,570
which is been written to debug these ZFS

00:49:19,280 --> 00:49:26,630
streams and in this case we pipe our

00:49:22,570 --> 00:49:29,840
difference into it and after some header

00:49:26,630 --> 00:49:31,520
files which tells you basically when

00:49:29,840 --> 00:49:33,619
this snapshot was created and a

00:49:31,520 --> 00:49:36,650
difference between those and the fools

00:49:33,619 --> 00:49:39,170
that are involved it can give you a hex

00:49:36,650 --> 00:49:42,200
dump which contains our original OST C

00:49:39,170 --> 00:49:46,400
string so you already can inspect what

00:49:42,200 --> 00:49:48,440
data are going to be transmitted and it

00:49:46,400 --> 00:49:50,690
actually internally is using globally

00:49:48,440 --> 00:49:52,040
unique IDs for the snapshots so if you

00:49:50,690 --> 00:49:54,109
end up with two snapshots that have the

00:49:52,040 --> 00:49:55,880
same name but would have different

00:49:54,109 --> 00:49:58,130
contents it will detect that and do the

00:49:55,880 --> 00:49:59,600
right thing instead of you know getting

00:49:58,130 --> 00:50:01,250
very confused because oh you already

00:49:59,600 --> 00:50:06,740
have that snapshot but it's actually

00:50:01,250 --> 00:50:08,810
different so before we get into more of

00:50:06,740 --> 00:50:10,430
the some of the upcoming features that

00:50:08,810 --> 00:50:17,030
are being added to ZFS does anybody have

00:50:10,430 --> 00:50:19,760
any questions we know it's a lot of

00:50:17,030 --> 00:50:21,680
stuff but it's very exciting just having

00:50:19,760 --> 00:50:23,990
all these features in one file system

00:50:21,680 --> 00:50:26,600
and you probably will look at other file

00:50:23,990 --> 00:50:30,109
systems and say oh they don't have that

00:50:26,600 --> 00:50:31,400
many features like that CFS has because

00:50:30,109 --> 00:50:33,980
one of the other features it has is

00:50:31,400 --> 00:50:36,050
what's called as Evol and basically

00:50:33,980 --> 00:50:37,369
instead of creating a file system you

00:50:36,050 --> 00:50:39,320
can actually create a virtual block

00:50:37,369 --> 00:50:42,080
device that's just backed by the pooled

00:50:39,320 --> 00:50:44,510
storage uh and those are very great for

00:50:42,080 --> 00:50:47,480
exporting as I scuzzy targets or for

00:50:44,510 --> 00:50:49,700
creating as backing for virtual machines

00:50:47,480 --> 00:50:53,210
it's NFS also has integration with NFS

00:50:49,700 --> 00:50:56,180
to share data size over NFS v3 or v4

00:50:53,210 --> 00:51:04,540
full ACLs and everything else you might

00:50:56,180 --> 00:51:04,540
want to do no questions yes

00:51:10,809 --> 00:51:16,880
hi let's support some kind of it's a

00:51:15,829 --> 00:51:19,459
okay

00:51:16,880 --> 00:51:23,719
some kind of shrinking my pool storage

00:51:19,459 --> 00:51:25,759
oh I'd say yeah I have too many storages

00:51:23,719 --> 00:51:32,269
in my in my pool and I want to get rid

00:51:25,759 --> 00:51:35,359
of some H CDs yeah so if you used mirror

00:51:32,269 --> 00:51:38,119
sets to do this or anything you stripe

00:51:35,359 --> 00:51:41,209
together then a new feature which is

00:51:38,119 --> 00:51:43,669
available upstream that will land in

00:51:41,209 --> 00:51:47,059
opens NFS probably in the next three or

00:51:43,669 --> 00:51:48,979
four months is vdf removal so when you

00:51:47,059 --> 00:51:50,829
add groups of hard drives to your pool

00:51:48,979 --> 00:51:53,509
they're called virtual devices or V dev

00:51:50,829 --> 00:51:55,159
if it's a mirror or a stripe you'll be

00:51:53,509 --> 00:51:58,999
able to remove it now

00:51:55,159 --> 00:52:01,039
but what but if it's a raid transform

00:51:58,999 --> 00:52:03,079
you won't be able to because internally

00:52:01,039 --> 00:52:04,640
the raid transforms don't actually know

00:52:03,079 --> 00:52:06,679
where the blocks start and end because

00:52:04,640 --> 00:52:09,709
of the variable size and distributed

00:52:06,679 --> 00:52:11,059
parity so if you use mirror sets or

00:52:09,709 --> 00:52:12,949
stripes then yes you will be able to

00:52:11,059 --> 00:52:16,900
actually shrink the pool but if you're

00:52:12,949 --> 00:52:16,900
using raid Zed you will not be able to

00:52:17,439 --> 00:52:22,369
write so some of the other features one

00:52:20,179 --> 00:52:24,409
of the ones that just landed is those

00:52:22,369 --> 00:52:26,259
replication streams because it's you're

00:52:24,409 --> 00:52:28,999
writing to a pipe and it's only one way

00:52:26,259 --> 00:52:30,679
if the pipe gets interrupted you know

00:52:28,999 --> 00:52:33,439
you're doing a connection over SSH and

00:52:30,679 --> 00:52:35,689
it gets interrupted previously you had

00:52:33,439 --> 00:52:36,949
to start the replication over because on

00:52:35,689 --> 00:52:39,380
the sender side they had no way of

00:52:36,949 --> 00:52:42,199
knowing what the if the receiver was

00:52:39,380 --> 00:52:43,459
even receiving it right because it was a

00:52:42,199 --> 00:52:45,349
one-way pipe so you could write it to a

00:52:43,459 --> 00:52:47,419
tape and not receive it until five years

00:52:45,349 --> 00:52:50,809
later it obviously can't wait for the

00:52:47,419 --> 00:52:53,150
receive path and so they vented the

00:52:50,809 --> 00:52:55,729
system to allow you to resume basically

00:52:53,150 --> 00:52:58,339
on the receiving side you set an extra

00:52:55,729 --> 00:52:59,809
flag and it will keep the partial data

00:52:58,339 --> 00:53:02,689
instead of throwing it away when the

00:52:59,809 --> 00:53:06,019
stream is interrupted and then it sets a

00:53:02,689 --> 00:53:07,969
new property which is this cookie and

00:53:06,019 --> 00:53:10,219
you take that cookie from the receiving

00:53:07,969 --> 00:53:12,319
system and give it to the sending system

00:53:10,219 --> 00:53:14,890
as the an extra parameter to the send

00:53:12,319 --> 00:53:17,179
command and it contains the data of

00:53:14,890 --> 00:53:20,530
which block how far through the

00:53:17,179 --> 00:53:21,610
replication stream you were yeah which

00:53:20,530 --> 00:53:23,590
data set you were sending which

00:53:21,610 --> 00:53:25,300
snapshots and how far through the

00:53:23,590 --> 00:53:26,890
replication you were so we can

00:53:25,300 --> 00:53:29,470
fast-forward and just resume from that

00:53:26,890 --> 00:53:32,260
spot so that was a big enhancement and

00:53:29,470 --> 00:53:36,430
that's available today in open ZFS

00:53:32,260 --> 00:53:38,370
starting with freebsd 10.3 it also has

00:53:36,430 --> 00:53:40,420
the replication record checksum

00:53:38,370 --> 00:53:44,320
previously when there wasn't the ability

00:53:40,420 --> 00:53:46,870
to resume Zedd of his sentence dream was

00:53:44,320 --> 00:53:49,960
one checksum at the very end across the

00:53:46,870 --> 00:53:52,480
entire set of data which obviously

00:53:49,960 --> 00:53:53,950
wouldn't work if you allowed resuming so

00:53:52,480 --> 00:53:56,200
they split it up so that each individual

00:53:53,950 --> 00:53:59,290
record is checked summed as it's being

00:53:56,200 --> 00:54:01,630
received and if a checksum doesn't match

00:53:59,290 --> 00:54:04,480
it interrupts the receive stream and

00:54:01,630 --> 00:54:06,340
then you can just resume it and deal

00:54:04,480 --> 00:54:07,750
with you know why your network was

00:54:06,340 --> 00:54:09,910
corrupting your stream as you were

00:54:07,750 --> 00:54:10,570
sending it and again that's available

00:54:09,910 --> 00:54:13,870
today

00:54:10,570 --> 00:54:16,600
there's also they found that while the

00:54:13,870 --> 00:54:18,070
send command was very optimized in with

00:54:16,600 --> 00:54:20,020
saturate your network oftentimes the

00:54:18,070 --> 00:54:22,420
received command wasn't keeping your

00:54:20,020 --> 00:54:24,280
hard drives as busy as it could and so

00:54:22,420 --> 00:54:26,890
they've a new prefetch feature there

00:54:24,280 --> 00:54:29,830
where it fetches the metadata it's going

00:54:26,890 --> 00:54:31,720
to need to update to receive and

00:54:29,830 --> 00:54:33,640
basically uses a queue to dispatch more

00:54:31,720 --> 00:54:38,080
work at once and and improve the

00:54:33,640 --> 00:54:41,410
performance of the replication one of

00:54:38,080 --> 00:54:43,750
the upcoming features is ZFS has what's

00:54:41,410 --> 00:54:46,150
called the adaptive replacement cache so

00:54:43,750 --> 00:54:48,520
in memory when it's doing its file

00:54:46,150 --> 00:54:50,230
system cache instead of a regular LRU

00:54:48,520 --> 00:54:51,400
like you would see with you know the

00:54:50,230 --> 00:54:56,130
buffer cache on most operating systems

00:54:51,400 --> 00:54:58,420
on ZFS the Ark actually maintains two

00:54:56,130 --> 00:55:00,520
separate parts of the cache one is for

00:54:58,420 --> 00:55:01,840
most frequently used files and then

00:55:00,520 --> 00:55:04,630
another one for most recently used

00:55:01,840 --> 00:55:07,330
similar to the LRU the importance of

00:55:04,630 --> 00:55:10,000
this is if you do some operation like a

00:55:07,330 --> 00:55:11,980
table scan on a database or a backup of

00:55:10,000 --> 00:55:15,460
your system where you go through and

00:55:11,980 --> 00:55:17,800
read every single file an LRU type cache

00:55:15,460 --> 00:55:19,270
is going to just keep rolling over

00:55:17,800 --> 00:55:21,490
because you're inserting new data and

00:55:19,270 --> 00:55:24,040
not reading a file a second time during

00:55:21,490 --> 00:55:26,470
a backup so it's Oedipus's

00:55:24,040 --> 00:55:28,540
Ark you have the most frequently used

00:55:26,470 --> 00:55:32,380
cache that's not going to get it blown

00:55:28,540 --> 00:55:34,270
away by this large scan operation but

00:55:32,380 --> 00:55:37,180
it's actually for lists

00:55:34,270 --> 00:55:39,610
because it keeps a blacklist or a ghost

00:55:37,180 --> 00:55:41,860
list for both the most frequently used

00:55:39,610 --> 00:55:43,900
and recently used it remembers which

00:55:41,860 --> 00:55:46,120
files were in the cache by kept falling

00:55:43,900 --> 00:55:47,800
out and basically blacklists them to

00:55:46,120 --> 00:55:49,540
allow some other data to have a chance

00:55:47,800 --> 00:55:51,520
to get into the cache where it might

00:55:49,540 --> 00:55:57,190
actually get used enough to remain in

00:55:51,520 --> 00:55:58,630
the cache and the new feature is that if

00:55:57,190 --> 00:56:00,010
you're going if the data that's coming

00:55:58,630 --> 00:56:01,390
in is going to be written to the disk

00:56:00,010 --> 00:56:03,610
compressed because you've enabled that

00:56:01,390 --> 00:56:05,950
setting it will actually compress it in

00:56:03,610 --> 00:56:09,310
RAM and keep the cached copy compressed

00:56:05,950 --> 00:56:12,040
and we'll decompress it as it's used

00:56:09,310 --> 00:56:14,200
again and so that with this you can

00:56:12,040 --> 00:56:16,950
actually especially with databases keep

00:56:14,200 --> 00:56:20,560
more of your working set in your ramp

00:56:16,950 --> 00:56:22,480
and get much better performance this was

00:56:20,560 --> 00:56:24,550
built by a company called del phix that

00:56:22,480 --> 00:56:28,570
does a database virtualization appliance

00:56:24,550 --> 00:56:31,290
and they had a data set a database that

00:56:28,570 --> 00:56:33,820
was a 1.2 terabytes

00:56:31,290 --> 00:56:35,290
but the system they had to run it only

00:56:33,820 --> 00:56:37,270
had seven hundred and sixty eight gigs

00:56:35,290 --> 00:56:40,600
of ram because that was the most that

00:56:37,270 --> 00:56:42,850
the motherboard would support and they

00:56:40,600 --> 00:56:45,190
were basically stuck at a performance

00:56:42,850 --> 00:56:47,860
bottleneck because SSDs weren't fast

00:56:45,190 --> 00:56:49,720
enough for their workload and so with

00:56:47,860 --> 00:56:52,810
this compression feature they managed to

00:56:49,720 --> 00:56:57,280
compress that 1.2 terabyte of database

00:56:52,810 --> 00:56:59,230
to 460 gigabytes and so now it all fits

00:56:57,280 --> 00:57:02,770
in RAM with lots of room to grow in

00:56:59,230 --> 00:57:04,300
there 768 Yates and so this more than

00:57:02,770 --> 00:57:07,630
tripled their performance in their

00:57:04,300 --> 00:57:09,310
database by just keeping you know when

00:57:07,630 --> 00:57:11,200
you read a compressed block off the disk

00:57:09,310 --> 00:57:12,700
you keep it compressed in the cache and

00:57:11,200 --> 00:57:15,430
RAM and only decompress it as it's

00:57:12,700 --> 00:57:18,280
access there's a small cache of I think

00:57:15,430 --> 00:57:19,720
256 megabytes of blocks you know if

00:57:18,280 --> 00:57:21,370
you're reading the same block really

00:57:19,720 --> 00:57:24,130
rapidly you won't decompress at multiple

00:57:21,370 --> 00:57:26,470
times but like I said because elves at 4

00:57:24,130 --> 00:57:30,280
can decompress at 4 gigabytes per second

00:57:26,470 --> 00:57:32,950
per core on any modest machine you know

00:57:30,280 --> 00:57:36,120
the decompressing the block that stored

00:57:32,950 --> 00:57:39,460
in RAM is not going to be a bottleneck

00:57:36,120 --> 00:57:42,820
another one is there's the l2 arc so

00:57:39,460 --> 00:57:50,320
they say oh yes so

00:57:42,820 --> 00:57:52,150
I look not one and I also wrote a book

00:57:50,320 --> 00:57:54,310
on ZFS if you're interested in learning

00:57:52,150 --> 00:57:57,550
more on how to use it it's at Zetas

00:57:54,310 --> 00:58:05,620
facebook.com and question still some

00:57:57,550 --> 00:58:09,280
final questions we were short what do

00:58:05,620 --> 00:58:13,090
you think about Ubuntu releasing these

00:58:09,280 --> 00:58:17,380
FS on Linux directly within its

00:58:13,090 --> 00:58:19,480
distribution channels yeah we did we

00:58:17,380 --> 00:58:22,240
were doing this in FreeBSD since 2008

00:58:19,480 --> 00:58:25,060
but our license isn't in conflict as a

00:58:22,240 --> 00:58:26,800
ZFS person I'm all for it you know the

00:58:25,060 --> 00:58:29,830
more said of s is awesome and everybody

00:58:26,800 --> 00:58:32,140
should get to use it as a FreeBSD

00:58:29,830 --> 00:58:34,090
developer well I prefer to keep it as a

00:58:32,140 --> 00:58:37,810
FreeBSD feature not let Linux have it

00:58:34,090 --> 00:58:41,020
but in the end I think ZFS is too good

00:58:37,810 --> 00:58:44,830
to deny all of Linux access to it but

00:58:41,020 --> 00:58:46,900
I'm not a lawyer that question will be

00:58:44,830 --> 00:58:48,910
settled someday and hopefully come up

00:58:46,900 --> 00:58:51,370
with the right answer because it gives

00:58:48,910 --> 00:58:53,380
you the benefit of you know the ZFS cent

00:58:51,370 --> 00:58:55,870
feature it should transfer not only your

00:58:53,380 --> 00:58:58,600
pools but also between operating system

00:58:55,870 --> 00:59:02,410
and it ZFS is intelligent enough to also

00:58:58,600 --> 00:59:03,970
make use of the different andean s so if

00:59:02,410 --> 00:59:06,100
you're sending from a little endian to a

00:59:03,970 --> 00:59:09,550
big endian system it can automatically

00:59:06,100 --> 00:59:11,200
transform those and vice versa so it'll

00:59:09,550 --> 00:59:14,230
be really interesting to see the ability

00:59:11,200 --> 00:59:16,690
to pull the hard drives out of a linux

00:59:14,230 --> 00:59:18,340
server and put them in a FreeBSD arm 64

00:59:16,690 --> 00:59:21,040
server and import the pool and the file

00:59:18,340 --> 00:59:31,120
system still works any further question

00:59:21,040 --> 00:59:34,180
you have a question so the question was

00:59:31,120 --> 00:59:36,310
if it's safe to run on Linux the actual

00:59:34,180 --> 00:59:38,800
core is that if s code is shared between

00:59:36,310 --> 00:59:40,210
all the operating systems is the only

00:59:38,800 --> 00:59:41,770
things that are specific to Linux is

00:59:40,210 --> 00:59:43,900
what's called the Solaris porting layer

00:59:41,770 --> 00:59:45,700
which is you know a bit of code that

00:59:43,900 --> 00:59:48,130
adapts the code that was originally for

00:59:45,700 --> 00:59:49,780
Solaris and makes it work in Linux

00:59:48,130 --> 00:59:50,890
because you know the Linux kernel has

00:59:49,780 --> 00:59:53,590
different primitives for a couple of

00:59:50,890 --> 00:59:55,600
things so the core is NFS features are

00:59:53,590 --> 00:59:56,400
very stable the integration with Linux

00:59:55,600 --> 00:59:59,200
is

00:59:56,400 --> 01:00:00,880
apparently production-ready now Lawrence

00:59:59,200 --> 01:00:02,800
Livermore labs in the US which is a big

01:00:00,880 --> 01:00:05,290
government research lab has been using

01:00:02,800 --> 01:00:06,760
it for more than five years and it's

01:00:05,290 --> 01:00:08,680
been fine for them

01:00:06,760 --> 01:00:12,040
I personally not used it so I don't know

01:00:08,680 --> 01:00:14,380
but I've been using the ZFS of freebsd

01:00:12,040 --> 01:00:18,640
in production since 2011 and had no

01:00:14,380 --> 01:00:21,520
problems any final questions oh okay and

01:00:18,640 --> 01:00:23,850
thanks then I got thanks oh thanks for

01:00:21,520 --> 01:00:23,850

YouTube URL: https://www.youtube.com/watch?v=6Bo4vYgmVhk


