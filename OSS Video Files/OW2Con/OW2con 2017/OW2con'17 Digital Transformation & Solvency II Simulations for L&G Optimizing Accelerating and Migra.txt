Title: OW2con'17 Digital Transformation & Solvency II Simulations for L&G Optimizing Accelerating and Migra
Publication date: 2017-06-28
Playlist: OW2con 2017
Description: 
	Legal & General is engaged into a very innovative Digital Transformation, which goes from the services offered to its customers to the management of its IT Applications and Infrastructure. 
As part of this strategic evolution, this talk presents how L&G together with ActiveEon Software and Services was able to replace 2 schedulers (Tibco DataSynapse and IBM AlgoBatch) with ProActive Workflows & Scheduling, and migrate the Solvency application to the Azure Cloud. A key aspect has been the ability to pipeline CPU-intensive tasks with I/O intensive ones. This alone allowed for 10% overall savings in runtime and grid resources, and for the high priority risk reports to be made available to customers 3x faster than the previous solution (16 hours down to 5 hours). 

Denis Caromel, CEO & Founder - ActiveEon
Guido Imperiale, Lead Integration Engineer - Legal & General, UK
Marco Castigliego, Agile Team Leader - ActiveEon
Paraita Wohler, Senior Software Engineer - ActiveEon
Captions: 
	00:00:01,960 --> 00:00:07,440
[Music]

00:00:05,240 --> 00:00:10,110
now there are a lot of people are

00:00:07,440 --> 00:00:13,940
talking about the digital transformation

00:00:10,110 --> 00:00:16,590
and actually today we will show you a

00:00:13,940 --> 00:00:19,460
concrete keys case of digital

00:00:16,590 --> 00:00:22,289
transformation where a very large

00:00:19,460 --> 00:00:26,910
billion-dollar company legal in general

00:00:22,289 --> 00:00:30,990
in UK has been using us to migrate a

00:00:26,910 --> 00:00:34,860
very critical application a risk

00:00:30,990 --> 00:00:39,149
evaluation in finance on to the cloud

00:00:34,860 --> 00:00:41,480
and actually on to the Azure cloud so we

00:00:39,149 --> 00:00:44,550
will see at the end that actually we get

00:00:41,480 --> 00:00:47,160
incredible acceleration in the execution

00:00:44,550 --> 00:00:51,239
of the application migrating from

00:00:47,160 --> 00:00:54,899
on-premises to the Azure cloud and also

00:00:51,239 --> 00:00:58,170
saving on the resources so I will give a

00:00:54,899 --> 00:01:02,699
brief introduction and then we will

00:00:58,170 --> 00:01:06,090
continue with concrete information about

00:01:02,699 --> 00:01:08,869
the use case so actively on is proposing

00:01:06,090 --> 00:01:12,360
a solution that actually goes from

00:01:08,869 --> 00:01:16,259
building automation workflow this is it

00:01:12,360 --> 00:01:17,880
here at the top a second part is

00:01:16,259 --> 00:01:20,189
scheduling in orchestration and the

00:01:17,880 --> 00:01:22,770
third part is a sauce management and

00:01:20,189 --> 00:01:25,020
monitoring so the workflow themself

00:01:22,770 --> 00:01:26,939
allows you to to build parallel and

00:01:25,020 --> 00:01:29,159
distributed execution so you can

00:01:26,939 --> 00:01:31,710
actually specify that you want to

00:01:29,159 --> 00:01:34,470
execute this task ten thousand times and

00:01:31,710 --> 00:01:38,100
ten thousand different distributed

00:01:34,470 --> 00:01:40,820
resources the second part is a milky

00:01:38,100 --> 00:01:44,130
user multi-application multi-tenant

00:01:40,820 --> 00:01:50,509
scheduler an Orchestrator that will

00:01:44,130 --> 00:01:53,700
apply a different priorities to the to

00:01:50,509 --> 00:01:56,159
the cooperate scheduling and

00:01:53,700 --> 00:01:59,700
orchestration and usually most of the

00:01:56,159 --> 00:02:02,369
schedulers will stop here and relied on

00:01:59,700 --> 00:02:04,520
external provisioning of resources with

00:02:02,369 --> 00:02:08,610
a given solution we actually as a

00:02:04,520 --> 00:02:11,670
building resource manager which allows

00:02:08,610 --> 00:02:14,220
you to provision the provision manage

00:02:11,670 --> 00:02:16,560
elasticity and I will we'll see

00:02:14,220 --> 00:02:20,940
the concrete case here we will be able

00:02:16,560 --> 00:02:23,610
to actually fine allocate tasks to

00:02:20,940 --> 00:02:27,060
associate based on memory consumption or

00:02:23,610 --> 00:02:29,370
IO or CPU assumption so why is it

00:02:27,060 --> 00:02:32,100
important and especially in a cloud

00:02:29,370 --> 00:02:33,150
migration framework I mean when you

00:02:32,100 --> 00:02:35,760
execute on Prime's

00:02:33,150 --> 00:02:38,430
you have your own resources when you

00:02:35,760 --> 00:02:41,160
move to the cloud you don't have your

00:02:38,430 --> 00:02:43,230
resources you pay per euro so it's

00:02:41,160 --> 00:02:46,650
getting very important that you actually

00:02:43,230 --> 00:02:48,990
optimize the usage of your resources and

00:02:46,650 --> 00:02:53,010
this is what we are doing in this

00:02:48,990 --> 00:02:55,920
diagram so we are actually compacting

00:02:53,010 --> 00:03:00,060
the execution making it more efficient

00:02:55,920 --> 00:03:04,530
and faster to get a faster result and to

00:03:00,060 --> 00:03:08,250
do more with less resources in a smaller

00:03:04,530 --> 00:03:12,180
time so we will move to the concrete

00:03:08,250 --> 00:03:16,580
case of solvency portfolio whisker

00:03:12,180 --> 00:03:20,209
calculation and I will agree to explain

00:03:16,580 --> 00:03:25,730
what he has achieved in this migration

00:03:20,209 --> 00:03:31,620
thank you so at legal in general we have

00:03:25,730 --> 00:03:34,860
the need to calculate the value of all

00:03:31,620 --> 00:03:39,000
of our financial instruments think about

00:03:34,860 --> 00:03:42,720
bonds securities but also mortgages and

00:03:39,000 --> 00:03:45,510
bank retirement funds etc under stress

00:03:42,720 --> 00:03:47,400
what does it mean it means how much

00:03:45,510 --> 00:03:50,220
money we're going to lose and the

00:03:47,400 --> 00:03:52,950
hypothetical market conditions so we

00:03:50,220 --> 00:03:55,739
need to change market conditions to a

00:03:52,950 --> 00:03:58,830
pathetical ones and recalculate the

00:03:55,739 --> 00:04:01,730
value of all of our portfolios and these

00:03:58,830 --> 00:04:05,580
need to happen two million times and

00:04:01,730 --> 00:04:11,030
it's a massive capital computation that

00:04:05,580 --> 00:04:15,690
so far takes belief the latest timing is

00:04:11,030 --> 00:04:20,840
3000 hours of CPU so it is imperative to

00:04:15,690 --> 00:04:23,070
come to paralyze it using computing grid

00:04:20,840 --> 00:04:28,600
so

00:04:23,070 --> 00:04:31,570
one of the biggest problems we have is

00:04:28,600 --> 00:04:34,870
that we have some tasks that are CP

00:04:31,570 --> 00:04:39,460
heavily CPU intensive and it used the

00:04:34,870 --> 00:04:42,100
CPU 100% for again 3,000 plus hours on

00:04:39,460 --> 00:04:43,750
the other side we have running in

00:04:42,100 --> 00:04:46,540
parallel with that tasks that are

00:04:43,750 --> 00:04:50,800
heavily i/o intensive that need to do

00:04:46,540 --> 00:04:55,750
about 2 terabytes worth of i/o within 8

00:04:50,800 --> 00:04:59,080
hour pay period so what a solution we

00:04:55,750 --> 00:05:04,120
came up together with proactive is to

00:04:59,080 --> 00:05:08,200
run the same some CPU intensive tasks in

00:05:04,120 --> 00:05:10,720
parallel with IO tasks on the same host

00:05:08,200 --> 00:05:14,380
so that every single host is slightly

00:05:10,720 --> 00:05:16,300
overloaded but in the end you end up a

00:05:14,380 --> 00:05:18,669
much higher throughput that you don't

00:05:16,300 --> 00:05:21,700
waste your time so you have one task

00:05:18,669 --> 00:05:24,190
that is stuck on IO 4 minutes and

00:05:21,700 --> 00:05:27,580
another task is consuming the CPU

00:05:24,190 --> 00:05:30,090
resources of the same host and that way

00:05:27,580 --> 00:05:32,620
in the end the overall throughput is

00:05:30,090 --> 00:05:34,900
maximized and we are not wasting any

00:05:32,620 --> 00:05:39,340
time we are not wasting any hardware

00:05:34,900 --> 00:05:42,669
resources we pay for a host per minute

00:05:39,340 --> 00:05:45,729
so whether a host is busy on CPU we pay

00:05:42,669 --> 00:05:48,010
X if a host is busy on Io we also pay

00:05:45,729 --> 00:05:50,770
the same amount if the host is busy on

00:05:48,010 --> 00:05:53,140
both we pay the same amount so we're

00:05:50,770 --> 00:05:56,080
effectively halving the amount of money

00:05:53,140 --> 00:06:02,160
we spend and the time it takes to do the

00:05:56,080 --> 00:06:05,860
same things this is a graph of our batch

00:06:02,160 --> 00:06:08,740
it is again it used to be in the

00:06:05,860 --> 00:06:12,520
beginning at 18 hours batch and it is

00:06:08,740 --> 00:06:17,650
running in proactive and every box you

00:06:12,520 --> 00:06:21,400
see is a task we have 3000 something

00:06:17,650 --> 00:06:25,240
tasks that are breaking down the problem

00:06:21,400 --> 00:06:27,820
into elementary units and we just submit

00:06:25,240 --> 00:06:30,640
them all at once to proactive and then

00:06:27,820 --> 00:06:33,039
proactive handles scheduling those tags

00:06:30,640 --> 00:06:36,770
along the grid here you have an example

00:06:33,039 --> 00:06:40,880
with 31

00:06:36,770 --> 00:06:43,220
host on the grid that was the our

00:06:40,880 --> 00:06:45,530
previous grid that was on-premises host

00:06:43,220 --> 00:06:49,880
they were permanently up owned by us

00:06:45,530 --> 00:06:54,380
incredibly expensive and you can see a

00:06:49,880 --> 00:06:56,810
red in here the pipelining of the CPU

00:06:54,380 --> 00:07:00,919
intensive tasks we share are colored in

00:06:56,810 --> 00:07:03,470
the purple blue and green with the i/o

00:07:00,919 --> 00:07:05,960
intensive tasks which are colored in

00:07:03,470 --> 00:07:10,479
yellow that are running at the same time

00:07:05,960 --> 00:07:13,639
on the same hosts and another big

00:07:10,479 --> 00:07:17,900
achievement we got with proactive is

00:07:13,639 --> 00:07:21,229
that the begin that first heard about of

00:07:17,900 --> 00:07:26,449
the tasks produces the most important

00:07:21,229 --> 00:07:28,910
results so we devised a way to run all

00:07:26,449 --> 00:07:32,630
day in port Morris most important tasks

00:07:28,910 --> 00:07:37,580
at the beginning and push out to our

00:07:32,630 --> 00:07:40,310
final users our consumers the most

00:07:37,580 --> 00:07:42,710
important reports ahead of the height of

00:07:40,310 --> 00:07:45,860
the end of the batch and since the whole

00:07:42,710 --> 00:07:48,860
batch takes 18 hours but the most

00:07:45,860 --> 00:07:52,599
important reports are out and ready to

00:07:48,860 --> 00:07:56,030
be consumed under just 5 hours because

00:07:52,599 --> 00:07:58,789
we really don't want to wait for the end

00:07:56,030 --> 00:08:03,830
of everything to start reading out the

00:07:58,789 --> 00:08:07,940
results this is the budget today so this

00:08:03,830 --> 00:08:12,020
was about one year ago this is the batch

00:08:07,940 --> 00:08:14,900
today we have migrated to Azure we are

00:08:12,020 --> 00:08:18,680
now running a grid that scales up and

00:08:14,900 --> 00:08:21,440
down on the fly on the left side of this

00:08:18,680 --> 00:08:26,270
graph you see how the batch starts

00:08:21,440 --> 00:08:29,389
timidly on only one host and then the

00:08:26,270 --> 00:08:32,750
batch schedules a thousand plus tasks on

00:08:29,389 --> 00:08:37,419
proactive and the grid automatically

00:08:32,750 --> 00:08:39,829
goes up to life all of those 64

00:08:37,419 --> 00:08:42,860
computation hosts were completely shut

00:08:39,829 --> 00:08:45,860
down we're not paying for them the

00:08:42,860 --> 00:08:47,810
moment we need them they go up 15

00:08:45,860 --> 00:08:50,360
minutes later or so we're truth we're

00:08:47,810 --> 00:08:53,649
still trimming the performance

00:08:50,360 --> 00:08:56,209
you see that Proactiv looks up the note

00:08:53,649 --> 00:08:59,720
registers them and starts running tasks

00:08:56,209 --> 00:09:02,360
in them and at the end of the batch the

00:08:59,720 --> 00:09:05,450
note goes that go down again so here we

00:09:02,360 --> 00:09:08,209
have a five hours batch we paid exactly

00:09:05,450 --> 00:09:10,490
for those five hours not a bank not a

00:09:08,209 --> 00:09:13,519
penny more there is another interesting

00:09:10,490 --> 00:09:16,010
bit in this graph that you can see you

00:09:13,519 --> 00:09:19,880
see that white area towards the bottom

00:09:16,010 --> 00:09:21,890
right now that is a host that died in

00:09:19,880 --> 00:09:23,930
the middle of the batch don't know what

00:09:21,890 --> 00:09:26,360
hardware failure something can carry no

00:09:23,930 --> 00:09:29,350
panic don't care the most important bit

00:09:26,360 --> 00:09:33,170
of weather is that proactive noticed

00:09:29,350 --> 00:09:35,750
routed around it and just moved the

00:09:33,170 --> 00:09:38,360
tasks that were designed for that host

00:09:35,750 --> 00:09:41,269
somewhere else and the batch proceeded

00:09:38,360 --> 00:09:51,649
completion without a glitch just very

00:09:41,269 --> 00:09:54,610
slightly slower and now again we use

00:09:51,649 --> 00:09:57,920
start we started on at 18 hours batch

00:09:54,610 --> 00:10:00,649
now thanks to proactive and thanks to

00:09:57,920 --> 00:10:03,019
additional optimizations we did along

00:10:00,649 --> 00:10:06,170
the way and thanks to zero it's now down

00:10:03,019 --> 00:10:08,690
to five hours and by the end of the year

00:10:06,170 --> 00:10:12,170
my hope and expectation is to go

00:10:08,690 --> 00:10:15,560
definitely below four hours and possibly

00:10:12,170 --> 00:10:18,640
less by adding more grid hosts more

00:10:15,560 --> 00:10:18,640

YouTube URL: https://www.youtube.com/watch?v=hLuXOZDmwLQ


