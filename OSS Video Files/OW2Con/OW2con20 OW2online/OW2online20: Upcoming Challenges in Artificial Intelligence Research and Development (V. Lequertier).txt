Title: OW2online20: Upcoming Challenges in Artificial Intelligence Research and Development (V. Lequertier)
Publication date: 2020-06-22
Playlist: OW2con20 OW2online
Description: 
	Artificial Intelligence is now smarter than ever, showing human-like abilities at complex tasks such as images classification or natural language processing. But despite its recent advances, it's still not a silver bullet. This talk will present a few challenges in the research and development of artificial intelligence that slow down its progress and adoption. In particular, problems around fairness, the training of models and how to share them will be introduced as well as possible Free Software solutions.
Captions: 
	00:00:01,300 --> 00:00:04,399
[Music]

00:00:10,219 --> 00:00:16,430
hello my name is Vincent hello Katya I'm

00:00:13,559 --> 00:00:19,470
a PhD student at the University Hospital

00:00:16,430 --> 00:00:22,800
my thesis is about artificial

00:00:19,470 --> 00:00:24,630
intelligence and healthcare and I'm also

00:00:22,800 --> 00:00:27,029
a free software foundation Europe

00:00:24,630 --> 00:00:29,369
volunteer today I'd like to talk about

00:00:27,029 --> 00:00:31,529
the upcoming challenges in efficient

00:00:29,369 --> 00:00:34,140
intelligence research and development

00:00:31,529 --> 00:00:39,000
and creating fairness parameters reuse

00:00:34,140 --> 00:00:42,059
and model training so for the first part

00:00:39,000 --> 00:00:45,239
of my talk about fairness I'd like to

00:00:42,059 --> 00:00:47,610
introduce some vocabulary the true

00:00:45,239 --> 00:00:50,370
positives and true negatives are worthy

00:00:47,610 --> 00:00:52,710
predictions they are correct meaning

00:00:50,370 --> 00:00:55,350
that you predicted the true energy

00:00:52,710 --> 00:00:58,949
observe the value while it was true or

00:00:55,350 --> 00:01:01,920
your predicted for false and the observe

00:00:58,949 --> 00:01:04,680
the value was false false positive and

00:01:01,920 --> 00:01:06,990
false negatives are where the wente

00:01:04,680 --> 00:01:09,270
predictions were incorrect so your

00:01:06,990 --> 00:01:13,640
predicted false and the observed the

00:01:09,270 --> 00:01:13,640
value was true also any other way around

00:01:14,330 --> 00:01:19,590
to explain the concept of fairness in

00:01:17,369 --> 00:01:20,280
algorithms I'd like to give you a few

00:01:19,590 --> 00:01:23,490
examples

00:01:20,280 --> 00:01:27,360
the compass' recidivism scoring

00:01:23,490 --> 00:01:29,939
algorithm tears if creator criminals are

00:01:27,360 --> 00:01:34,560
going to waste a debate a crime or not

00:01:29,939 --> 00:01:36,509
in some United States courts so problem

00:01:34,560 --> 00:01:39,329
with this algorithm is that it had

00:01:36,509 --> 00:01:42,390
different performance depending on the

00:01:39,329 --> 00:01:44,880
skin color the algorithm often

00:01:42,390 --> 00:01:48,240
mistakenly predicted the high risk of

00:01:44,880 --> 00:01:51,149
recidivism for black people and a low

00:01:48,240 --> 00:01:53,399
risk for white people you can see this

00:01:51,149 --> 00:01:56,820
in the false positive rate and the false

00:01:53,399 --> 00:02:01,950
negative rate see false positive rate

00:01:56,820 --> 00:02:05,670
were almost 50% for black people and the

00:02:01,950 --> 00:02:08,479
fourth negative rate was almost 50% for

00:02:05,670 --> 00:02:08,479
white people

00:02:10,640 --> 00:02:16,230
another example this time in the

00:02:13,079 --> 00:02:18,239
healthcare so this is a graph

00:02:16,230 --> 00:02:21,510
summarizing the result of an algorithm

00:02:18,239 --> 00:02:25,170
predicting the risk score for patients

00:02:21,510 --> 00:02:27,750
so the x-axis tells the risk score

00:02:25,170 --> 00:02:30,689
predicted by the algorithm and the

00:02:27,750 --> 00:02:35,040
y-axis predicts in a sari is in the

00:02:30,689 --> 00:02:37,079
number of conditions of the patients as

00:02:35,040 --> 00:02:39,900
you can see black people had to be

00:02:37,079 --> 00:02:43,530
secured to at the same predicted risk

00:02:39,900 --> 00:02:47,310
score as white people so this algorithm

00:02:43,530 --> 00:02:49,560
was achieving different results

00:02:47,310 --> 00:02:55,019
according to the skin color which is

00:02:49,560 --> 00:02:57,780
unfair obviously so an algorithm can be

00:02:55,019 --> 00:03:01,230
unfair because it can be there can be

00:02:57,780 --> 00:03:03,900
bias in the input data itself or it can

00:03:01,230 --> 00:03:08,340
be it can be trained with the wrong

00:03:03,900 --> 00:03:11,639
evolution metric or all the algorithm

00:03:08,340 --> 00:03:13,650
can be trained with the right evaluation

00:03:11,639 --> 00:03:16,409
metric but it forgets about everything

00:03:13,650 --> 00:03:20,459
else so it optimize for this metric but

00:03:16,409 --> 00:03:22,079
not for fairness for example you can

00:03:20,459 --> 00:03:27,780
have a bad prediction models that

00:03:22,079 --> 00:03:30,419
introduced unfairness here I'd like to

00:03:27,780 --> 00:03:33,090
introduce you to a fair loss function a

00:03:30,419 --> 00:03:35,449
loss function is used during the

00:03:33,090 --> 00:03:39,180
training of an algorithm to tell us how

00:03:35,449 --> 00:03:41,669
accurate it is that is how well it is

00:03:39,180 --> 00:03:43,799
performing so according to this metric

00:03:41,669 --> 00:03:48,030
you reduced the parameter of the model

00:03:43,799 --> 00:03:50,939
to improve its accuracy to create a loss

00:03:48,030 --> 00:03:56,939
for the loss function that is fair I add

00:03:50,939 --> 00:04:01,019
to this regular dose value so this is

00:03:56,939 --> 00:04:04,079
value comes from a function this

00:04:01,019 --> 00:04:06,780
function tells if a result is fair or

00:04:04,079 --> 00:04:10,650
not from the predictions of the

00:04:06,780 --> 00:04:14,069
algorithm so what you do is you sum the

00:04:10,650 --> 00:04:17,070
result of this function for all possible

00:04:14,069 --> 00:04:18,750
value of a protected attribute the

00:04:17,070 --> 00:04:22,510
protected attribute is something

00:04:18,750 --> 00:04:26,830
sensitive like the genders

00:04:22,510 --> 00:04:28,570
the skin color and so on so you sum up

00:04:26,830 --> 00:04:30,850
the result of this function for all

00:04:28,570 --> 00:04:33,700
possible value of a protected attribute

00:04:30,850 --> 00:04:37,840
you divide by the minimum and you try to

00:04:33,700 --> 00:04:39,760
minimize this value as well as well as

00:04:37,840 --> 00:04:42,610
the loss meaning that you both optimize

00:04:39,760 --> 00:04:47,920
for the accuracy of the model and for

00:04:42,610 --> 00:04:49,900
its furnace at the same time so the

00:04:47,920 --> 00:04:53,280
second part of my talk is about

00:04:49,900 --> 00:04:56,110
parameters reuse so what are parameters

00:04:53,280 --> 00:04:56,710
parameters in the context of the

00:04:56,110 --> 00:05:00,490
planning's

00:04:56,710 --> 00:05:03,490
is a small number of subtle that are

00:05:00,490 --> 00:05:06,070
added or multiplied together and they

00:05:03,490 --> 00:05:09,790
are organized into layers and they lead

00:05:06,070 --> 00:05:12,520
to a prediction in real-world examples

00:05:09,790 --> 00:05:16,990
there are millions or billions of those

00:05:12,520 --> 00:05:19,660
parameters using parameters mean reusing

00:05:16,990 --> 00:05:22,930
some layers from already trained model

00:05:19,660 --> 00:05:25,330
as part of your model training meaning

00:05:22,930 --> 00:05:28,330
just when you train your model to give

00:05:25,330 --> 00:05:31,300
accurate predictions and you will reuse

00:05:28,330 --> 00:05:36,280
parameters from already successful and

00:05:31,300 --> 00:05:39,570
already trained model so I'll give you

00:05:36,280 --> 00:05:39,570
some time to read this comic

00:05:45,490 --> 00:05:51,740
so training a mother can be very

00:05:48,289 --> 00:05:55,520
time-consuming it can take hours or days

00:05:51,740 --> 00:05:57,830
to train a large model by reusing some

00:05:55,520 --> 00:06:00,500
already trained model and some pair

00:05:57,830 --> 00:06:03,470
materials you avoid doing some

00:06:00,500 --> 00:06:10,190
computation which which saves you a lot

00:06:03,470 --> 00:06:13,360
of time using parameters also saves a

00:06:10,190 --> 00:06:16,580
lot of energy this table summarizes the

00:06:13,360 --> 00:06:18,410
study about the energy impact of natural

00:06:16,580 --> 00:06:22,190
language natural language processing

00:06:18,410 --> 00:06:23,210
models the result are in the Kaito white

00:06:22,190 --> 00:06:26,870
per hour

00:06:23,210 --> 00:06:30,169
multiplied by the PUA which is the

00:06:26,870 --> 00:06:33,410
impact the energy impact of the data

00:06:30,169 --> 00:06:34,990
center itself like things like cooling

00:06:33,410 --> 00:06:38,870
so machines and so on

00:06:34,990 --> 00:06:41,599
MZ was so in a carbon dioxide and

00:06:38,870 --> 00:06:45,110
you can see exact that the training

00:06:41,599 --> 00:06:47,539
those models takes a lot of energy so by

00:06:45,110 --> 00:06:50,449
don't have any to train them yourself

00:06:47,539 --> 00:06:52,729
and by reusing already trained model in

00:06:50,449 --> 00:06:59,270
natural language processing to avoid a

00:06:52,729 --> 00:07:02,330
lot of energy cost parameters reuse is

00:06:59,270 --> 00:07:05,389
also good for knowledge sharing by

00:07:02,330 --> 00:07:09,470
sharing models layers and parameter

00:07:05,389 --> 00:07:12,680
parameters with each other we we can

00:07:09,470 --> 00:07:16,460
liberate knowledge from multiple models

00:07:12,680 --> 00:07:18,680
as part of one model so we share the

00:07:16,460 --> 00:07:20,990
knowledge accumulated by different

00:07:18,680 --> 00:07:27,050
models as part of a bigger one to

00:07:20,990 --> 00:07:29,659
achieve maybe Whittle goals so what is

00:07:27,050 --> 00:07:33,500
needed is to make it easy to reuse

00:07:29,659 --> 00:07:35,900
parameters of models we want to have

00:07:33,500 --> 00:07:38,900
some licensing and some platforms where

00:07:35,900 --> 00:07:45,259
we can easily share different models so

00:07:38,900 --> 00:07:47,419
that everyone can reuse them and also we

00:07:45,259 --> 00:07:49,090
need to find ways to reuse parameters in

00:07:47,419 --> 00:07:52,539
model mains of AI

00:07:49,090 --> 00:07:55,669
some part of artificial intelligence

00:07:52,539 --> 00:07:57,220
currently make it but don't make it easy

00:07:55,669 --> 00:07:59,890
to reuse what

00:07:57,220 --> 00:08:02,950
parameters for example there is no way

00:07:59,890 --> 00:08:05,800
frankly in reinforcement learning to

00:08:02,950 --> 00:08:11,110
reuse some already trained model as part

00:08:05,800 --> 00:08:12,700
of other models the third part of my

00:08:11,110 --> 00:08:15,310
talk is about model training

00:08:12,700 --> 00:08:19,380
optimization so model training

00:08:15,310 --> 00:08:23,980
optimization means optimizing the way

00:08:19,380 --> 00:08:27,310
the model is going to be trained this is

00:08:23,980 --> 00:08:30,460
done by a process called EPR parameters

00:08:27,310 --> 00:08:32,950
tuning so what is the parameter tuning

00:08:30,460 --> 00:08:36,520
to explain this I'm going to make you an

00:08:32,950 --> 00:08:38,200
analogy with cake baking so when you

00:08:36,520 --> 00:08:41,110
bake your cake you have control over

00:08:38,200 --> 00:08:43,780
several parameters you can control the

00:08:41,110 --> 00:08:46,890
temperature of the oven the baking time

00:08:43,780 --> 00:08:49,870
and the cooling time also cake and

00:08:46,890 --> 00:08:52,450
modifying those parameters impact as a

00:08:49,870 --> 00:08:57,520
taste and thus the quality of the cake

00:08:52,450 --> 00:09:00,610
so this is the same with models you have

00:08:57,520 --> 00:09:03,160
a bunch of parameters and adjusting

00:09:00,610 --> 00:09:06,160
these parameters controlled the accuracy

00:09:03,160 --> 00:09:10,660
that is how well the model is going to

00:09:06,160 --> 00:09:12,310
perform with several parameters for

00:09:10,660 --> 00:09:15,100
example here I have three different

00:09:12,310 --> 00:09:18,160
orbitals candle takes three possible

00:09:15,100 --> 00:09:22,030
values c takes 10 possible values and

00:09:18,160 --> 00:09:24,970
gamma takes 10 possible values so now if

00:09:22,030 --> 00:09:27,790
you want to try all combination of those

00:09:24,970 --> 00:09:30,240
parameters to find the best ones you

00:09:27,790 --> 00:09:35,050
have three hundreds candidates to try

00:09:30,240 --> 00:09:37,839
that is 300 model to train and if you

00:09:35,050 --> 00:09:41,380
want to use 10-fold cross-validation you

00:09:37,839 --> 00:09:43,270
have three thousands models to train to

00:09:41,380 --> 00:09:46,150
find the best combination of three

00:09:43,270 --> 00:09:51,250
parameters so this is ready

00:09:46,150 --> 00:09:53,650
time-consuming and expensive but there

00:09:51,250 --> 00:09:56,950
are several solutions you can use

00:09:53,650 --> 00:10:00,520
something called already stopping so if

00:09:56,950 --> 00:10:03,430
I go back to the example of the cake if

00:10:00,520 --> 00:10:05,589
if the cake is already burned you know

00:10:03,430 --> 00:10:07,839
that there is no point in trying to

00:10:05,589 --> 00:10:09,520
increase the temperature or frozen you

00:10:07,839 --> 00:10:10,540
know that increasing the temperature

00:10:09,520 --> 00:10:13,600
will lead

00:10:10,540 --> 00:10:16,660
two more a blonde cake

00:10:13,600 --> 00:10:19,690
so it's pointless so this is the same

00:10:16,660 --> 00:10:23,650
with models you know that if the

00:10:19,690 --> 00:10:27,400
accuracy is not great trying to increase

00:10:23,650 --> 00:10:30,580
the value of the parameter is not going

00:10:27,400 --> 00:10:34,780
to help so you can stop and don't try to

00:10:30,580 --> 00:10:37,060
increase further this parameter you can

00:10:34,780 --> 00:10:39,610
also try to converge towards the best

00:10:37,060 --> 00:10:43,690
solution that is converge to one the

00:10:39,610 --> 00:10:46,510
best combination of epo parameters what

00:10:43,690 --> 00:10:49,180
you do is you monitor the accuracy of

00:10:46,510 --> 00:10:52,840
the model over several tries that is

00:10:49,180 --> 00:10:56,650
over several candidates and you try to

00:10:52,840 --> 00:11:00,670
find the most promising combination of

00:10:56,650 --> 00:11:04,630
parameters and you derive new candidates

00:11:00,670 --> 00:11:09,550
based based on the most promising result

00:11:04,630 --> 00:11:12,040
so far so it's it converge towards the

00:11:09,550 --> 00:11:14,830
best solution what you can do as all is

00:11:12,040 --> 00:11:17,050
to make the model training faster so

00:11:14,830 --> 00:11:21,400
maybe you don't need all your data or

00:11:17,050 --> 00:11:25,150
all your complexity complexity of your

00:11:21,400 --> 00:11:28,510
training loop to produce the parameters

00:11:25,150 --> 00:11:31,990
so maybe you can reduce the number of

00:11:28,510 --> 00:11:35,860
epochs required to train the model or

00:11:31,990 --> 00:11:38,950
you can maybe reduce the size of the

00:11:35,860 --> 00:11:42,220
data set used to Train C model in the

00:11:38,950 --> 00:11:44,080
context of equal parameters tuning that

00:11:42,220 --> 00:11:48,420
will make the model training faster and

00:11:44,080 --> 00:11:53,770
therefore you can try more candidates

00:11:48,420 --> 00:11:56,320
more combination of API parameters so

00:11:53,770 --> 00:11:59,490
thank you for listening and please ask

00:11:56,320 --> 00:11:59,490

YouTube URL: https://www.youtube.com/watch?v=sFM551B2alI


