Title: OW2con'15 Keynote Yann Mombrun: Multimedia document processing using the WebLab platform
Publication date: 2015-11-24
Playlist: OW2con 2015
Description: 
	AXES (http://axes-project.eu) was a project co-funded by the European Unionâ€™s Seventh Programme for Research, Technological Development and Demonstration. The goal of AXES was to develop tools that provide various types of users with new engaging ways to interact with audiovisual libraries, helping them to discover, browse, search and enrich video archives.
The OW2 WebLab (http://weblab-project.org) integration platform for multimedia processing has been used to aggregate the services developed by 9 partners in order to build the three demonstrators targeting various user groups: the media professionals, the journalists and researchers as well as the home users.
As a result of the AXES project, OW2 "Open AXES", is born. It provides a complete solution that gathers innovative audiovisual content analysis technologies (shot and keyframes detection, image classification, speech transcription, large scale indexing, similarity search, etc.) as well as an ergonomic interface to retrieve and navigate in video archives.
Captions: 
	00:00:05,870 --> 00:00:13,110
so I would like to to present web lab

00:00:10,200 --> 00:00:16,320
project and especially open access which

00:00:13,110 --> 00:00:18,990
is a web lab sub-project so first of all

00:00:16,320 --> 00:00:20,970
who am I so I'm yang maha from Erebus

00:00:18,990 --> 00:00:23,789
defense & space formerly known as

00:00:20,970 --> 00:00:26,880
Cassidian I've seen it in some slides

00:00:23,789 --> 00:00:29,279
this morning so cassidian has been

00:00:26,880 --> 00:00:32,910
merged with a stream which was a space

00:00:29,279 --> 00:00:34,710
subsidiary of EDS and now there are we

00:00:32,910 --> 00:00:38,280
are one of the subsidiary of the abuse

00:00:34,710 --> 00:00:42,210
groups so x EDS i mean a team of 10

00:00:38,280 --> 00:00:45,180
engineers and PhD students and now

00:00:42,210 --> 00:00:48,780
located in a long cool we were before in

00:00:45,180 --> 00:00:52,199
valuing emoji and we are working on our

00:00:48,780 --> 00:00:55,620
NT and R&D projects for unstructured

00:00:52,199 --> 00:00:59,460
document analysis and data fusion so

00:00:55,620 --> 00:01:01,640
what's unstructured document analysis so

00:00:59,460 --> 00:01:04,710
we start from asteroid unstructured data

00:01:01,640 --> 00:01:07,140
weather coming from the internet from

00:01:04,710 --> 00:01:11,189
social networks or from internal

00:01:07,140 --> 00:01:14,220
databases and we try to analyze this in

00:01:11,189 --> 00:01:18,090
order to produce structured an

00:01:14,220 --> 00:01:21,420
actionable knowledge or intelligence so

00:01:18,090 --> 00:01:26,310
to do that we need a lot of tools that

00:01:21,420 --> 00:01:28,890
have to be organized together to work

00:01:26,310 --> 00:01:30,540
into complex processing chains so

00:01:28,890 --> 00:01:32,939
starting from that I gatherings or

00:01:30,540 --> 00:01:35,520
accessing the data on the Internet and

00:01:32,939 --> 00:01:38,759
networks on local network social

00:01:35,520 --> 00:01:43,409
networks the normalization so extracting

00:01:38,759 --> 00:01:46,950
a text from document and converting the

00:01:43,409 --> 00:01:51,149
videos and the audio to have in to codex

00:01:46,950 --> 00:01:53,399
that are handled by the next services so

00:01:51,149 --> 00:01:55,950
doing some automatic translation if

00:01:53,399 --> 00:01:58,259
needed extract information with your

00:01:55,950 --> 00:02:00,990
phone texts like named entity

00:01:58,259 --> 00:02:03,780
recognition or object detection inside

00:02:00,990 --> 00:02:08,459
images speaker identification inside

00:02:03,780 --> 00:02:11,150
audio whatever and merge fuse all that

00:02:08,459 --> 00:02:13,770
information that is extracted from the

00:02:11,150 --> 00:02:16,940
documents and add

00:02:13,770 --> 00:02:22,430
index the data into structured databases

00:02:16,940 --> 00:02:25,140
to enable search whether text search

00:02:22,430 --> 00:02:30,240
geographical search and whatever is

00:02:25,140 --> 00:02:32,730
possible so it means we need to to have

00:02:30,240 --> 00:02:34,920
an integration platform before because

00:02:32,730 --> 00:02:37,830
of the complexity of this chain and the

00:02:34,920 --> 00:02:40,830
viability of the data because there are

00:02:37,830 --> 00:02:44,760
a lot of formats lot of media so do

00:02:40,830 --> 00:02:46,290
video and dark text and also a lot of

00:02:44,760 --> 00:02:48,810
language that have to be handled and

00:02:46,290 --> 00:02:51,000
some components that we are using are

00:02:48,810 --> 00:02:53,970
for instance only working on english

00:02:51,000 --> 00:02:56,730
while another one is working and french

00:02:53,970 --> 00:02:59,130
and german or whatever so we need to

00:02:56,730 --> 00:03:01,380
combine that component so that's why we

00:02:59,130 --> 00:03:05,490
have designed to wear blonde which is

00:03:01,380 --> 00:03:07,680
noble UW to project which is our

00:03:05,490 --> 00:03:10,140
integration platform use inside herbs

00:03:07,680 --> 00:03:12,420
difference in space it allows

00:03:10,140 --> 00:03:17,070
integration and the selection of

00:03:12,420 --> 00:03:18,840
software's and combine them because each

00:03:17,070 --> 00:03:22,320
of the software that you can find on the

00:03:18,840 --> 00:03:24,750
market or on open source does not alone

00:03:22,320 --> 00:03:27,990
the whole processing chain which is

00:03:24,750 --> 00:03:30,810
needed it does not make the data

00:03:27,990 --> 00:03:33,750
acquisition translation information

00:03:30,810 --> 00:03:36,600
extraction and visualization at the end

00:03:33,750 --> 00:03:39,480
you often need to use and to pick

00:03:36,600 --> 00:03:42,780
specific tools for each kind of the of

00:03:39,480 --> 00:03:44,459
your needs so we need to enable the

00:03:42,780 --> 00:03:48,600
entire operation of the selected tools

00:03:44,459 --> 00:03:52,530
and so we are relying on open sourced

00:03:48,600 --> 00:03:54,330
and of standards so far the service

00:03:52,530 --> 00:03:56,209
oriented architecture web services and

00:03:54,330 --> 00:03:59,000
semantic web for the knowledge and

00:03:56,209 --> 00:04:03,180
enabling the composition of services

00:03:59,000 --> 00:04:05,370
inside the chain so where rom is both an

00:04:03,180 --> 00:04:07,140
integration platform and in fact a set

00:04:05,370 --> 00:04:09,420
of media mining services for document

00:04:07,140 --> 00:04:11,700
processing that we are reusing in all

00:04:09,420 --> 00:04:13,910
our world at projects some of them are

00:04:11,700 --> 00:04:17,340
open source and providing on the w2

00:04:13,910 --> 00:04:20,130
whether on the web lab project or on the

00:04:17,340 --> 00:04:24,510
open access project and some other stay

00:04:20,130 --> 00:04:26,400
close to us at a bus so we're Labs is

00:04:24,510 --> 00:04:27,430
also range of projecting set of mega

00:04:26,400 --> 00:04:31,199
mining tools we start

00:04:27,430 --> 00:04:36,880
in 2004 France our studies then we work

00:04:31,199 --> 00:04:39,490
based on co-funding and and cooperating

00:04:36,880 --> 00:04:41,800
project using from a naso french

00:04:39,490 --> 00:04:46,020
national research and the european

00:04:41,800 --> 00:04:49,509
commission on web content so it was the

00:04:46,020 --> 00:04:53,199
nano project which aims at providing an

00:04:49,509 --> 00:04:55,600
infrastructure for document analysis so

00:04:53,199 --> 00:04:58,380
it was basically the same ID of web lamb

00:04:55,600 --> 00:05:04,840
so during this project we worked with a

00:04:58,380 --> 00:05:08,940
c.c a and together we two years later

00:05:04,840 --> 00:05:12,190
provided release the web lab on a w-2

00:05:08,940 --> 00:05:15,550
integrated w too thanks to other project

00:05:12,190 --> 00:05:17,590
like viet roseau time c and the ones

00:05:15,550 --> 00:05:20,110
that we i will focus next which is

00:05:17,590 --> 00:05:23,050
access which aims at extracting

00:05:20,110 --> 00:05:26,949
information inside audio-visual that

00:05:23,050 --> 00:05:29,979
audio visual databases and i'm now

00:05:26,949 --> 00:05:32,800
working on the sea project which is kind

00:05:29,979 --> 00:05:37,030
of follow-up of axes but she's working

00:05:32,800 --> 00:05:41,440
only on audio for so sip is a speaker

00:05:37,030 --> 00:05:46,120
identification integrated project so

00:05:41,440 --> 00:05:49,570
access project finished in 2014 so it's

00:05:46,120 --> 00:05:52,120
very still a website so administrative

00:05:49,570 --> 00:05:54,010
coordination was done by LC Nene's the

00:05:52,120 --> 00:05:56,680
technical partners well the University

00:05:54,010 --> 00:06:00,789
of Louisville for image classification

00:05:56,680 --> 00:06:03,820
image a notification object extraction

00:06:00,789 --> 00:06:05,169
place detection inside image and there

00:06:03,820 --> 00:06:09,520
were the technical coordinator of the

00:06:05,169 --> 00:06:13,300
project for imagine al-azizia was also

00:06:09,520 --> 00:06:17,349
oxford university in really in Grenoble

00:06:13,300 --> 00:06:22,750
and technical or for audio analysis it

00:06:17,349 --> 00:06:27,280
was a fun offer the GUI was done by DC

00:06:22,750 --> 00:06:31,659
you text search by 20 in 20 University

00:06:27,280 --> 00:06:34,330
in Poland and we were doing with Airbus

00:06:31,659 --> 00:06:35,870
the integration platform and feeling

00:06:34,330 --> 00:06:37,850
some gaps

00:06:35,870 --> 00:06:41,180
inside the whole processing chain where

00:06:37,850 --> 00:06:43,640
it was when it was needed our content

00:06:41,180 --> 00:06:46,910
provider an end user where the BBC

00:06:43,640 --> 00:06:50,840
research baden guardian which is the

00:06:46,910 --> 00:06:53,570
archives of netherlands so like ena

00:06:50,840 --> 00:06:57,290
Infante Deutsche Welle which is a

00:06:53,570 --> 00:07:00,260
broadcaster German broadcaster and so

00:06:57,290 --> 00:07:04,250
these three end users was also content

00:07:00,260 --> 00:07:08,120
providers and Erasmus University well

00:07:04,250 --> 00:07:10,970
the kind of end user so it's a people

00:07:08,120 --> 00:07:17,120
from the humanities which were using

00:07:10,970 --> 00:07:19,430
databases to produce articles so what's

00:07:17,120 --> 00:07:23,570
the aim of what was the aim of axes it

00:07:19,430 --> 00:07:25,640
was we were planning to develop tools to

00:07:23,570 --> 00:07:27,980
provide new ways to interact with the

00:07:25,640 --> 00:07:30,260
database and not only search inside but

00:07:27,980 --> 00:07:33,500
also be able to browse to explore the

00:07:30,260 --> 00:07:38,660
database and to experience it in order

00:07:33,500 --> 00:07:40,430
to ease the knowledge easy the

00:07:38,660 --> 00:07:45,290
production of knowledge based on the

00:07:40,430 --> 00:07:47,570
databases for plenty of users so various

00:07:45,290 --> 00:07:52,400
kind of user media professional so Dutch

00:07:47,570 --> 00:07:55,970
avello we had journalists we who aims at

00:07:52,400 --> 00:08:00,470
producing article reports based on the

00:07:55,970 --> 00:08:02,990
database of previously previous reports

00:08:00,470 --> 00:08:06,680
that I've been known researcher for from

00:08:02,990 --> 00:08:09,050
Erasmus home user so anyone wants to use

00:08:06,680 --> 00:08:15,380
a system and intelligence officer so it

00:08:09,050 --> 00:08:19,820
was for us and to do that our technical

00:08:15,380 --> 00:08:23,210
partners so mainly absurd and for

00:08:19,820 --> 00:08:25,310
computer vision and Kerry also kare 11

00:08:23,210 --> 00:08:27,430
we are using a lot of techniques of

00:08:25,310 --> 00:08:30,860
computer vision speech recognition

00:08:27,430 --> 00:08:34,850
energy analysis from Fran over search

00:08:30,860 --> 00:08:39,260
and navigation from this UN 20 the idea

00:08:34,850 --> 00:08:41,990
was to enable the multi the inter

00:08:39,260 --> 00:08:45,230
extraction and recognition of people

00:08:41,990 --> 00:08:46,850
places events and object category based

00:08:45,230 --> 00:08:50,000
on multi modal analysis

00:08:46,850 --> 00:08:53,000
so I mean that in the system we're

00:08:50,000 --> 00:08:56,899
extracting and trying to identify people

00:08:53,000 --> 00:09:01,069
based on the photo but also based on the

00:08:56,899 --> 00:09:03,829
speech and then a system does a fusion

00:09:01,069 --> 00:09:06,649
based on the assumption that it was that

00:09:03,829 --> 00:09:10,459
the same people were speaking and who

00:09:06,649 --> 00:09:14,240
was on the screen and the same thing for

00:09:10,459 --> 00:09:17,149
places and sing like that so what the

00:09:14,240 --> 00:09:19,940
final architecture of the access project

00:09:17,149 --> 00:09:22,959
so it starts from video content the

00:09:19,940 --> 00:09:25,790
video content which were provided by

00:09:22,959 --> 00:09:28,490
content provider we're located in

00:09:25,790 --> 00:09:31,459
structured and very particular databases

00:09:28,490 --> 00:09:33,019
so we had to develop some component to

00:09:31,459 --> 00:09:36,050
be able to retrieve the data so whether

00:09:33,019 --> 00:09:37,790
it was in fact very simple on a hard

00:09:36,050 --> 00:09:42,050
drive but also it might be inside

00:09:37,790 --> 00:09:45,230
specific databases then we had to do a

00:09:42,050 --> 00:09:48,829
lot of processing as I said so to do

00:09:45,230 --> 00:09:52,250
that we focused on the web lab for the

00:09:48,829 --> 00:09:57,829
orchestration and for the interaction

00:09:52,250 --> 00:10:00,139
between all services then the data which

00:09:57,829 --> 00:10:03,649
has been expand or molest and extracted

00:10:00,139 --> 00:10:07,579
and a part of analyze was indexed inside

00:10:03,649 --> 00:10:11,839
specific image databases and text

00:10:07,579 --> 00:10:14,480
databases 20 was working on the linking

00:10:11,839 --> 00:10:16,880
so the inking was not only the search

00:10:14,480 --> 00:10:19,430
members of the recommendation of similar

00:10:16,880 --> 00:10:22,279
content and scene like that based both

00:10:19,430 --> 00:10:26,899
on what you are browsing and on the

00:10:22,279 --> 00:10:29,810
content that that you are looking since

00:10:26,899 --> 00:10:32,600
we are many kind of user we have

00:10:29,810 --> 00:10:36,470
developed various systems with different

00:10:32,600 --> 00:10:39,589
UI and because the need of the user were

00:10:36,470 --> 00:10:41,870
not the same for instance the axis pro

00:10:39,589 --> 00:10:45,490
system which was the first one develop

00:10:41,870 --> 00:10:48,860
during the project was focusing on the

00:10:45,490 --> 00:10:52,220
archivist from National Institute of

00:10:48,860 --> 00:10:55,910
sound and vision and of BBC whose main

00:10:52,220 --> 00:10:59,750
work is in fact to watch the video store

00:10:55,910 --> 00:11:03,560
it in a database and annotate it to tell

00:10:59,750 --> 00:11:07,220
was inside so that later on other people

00:11:03,560 --> 00:11:09,530
can browse and find that video so for

00:11:07,220 --> 00:11:16,190
the moment it was done manually I mean

00:11:09,530 --> 00:11:20,510
when a TV show is is presenting on BBC

00:11:16,190 --> 00:11:23,870
and the transcript are put inside but

00:11:20,510 --> 00:11:26,450
people also run writing down who was

00:11:23,870 --> 00:11:28,880
inside rua the actors what was their

00:11:26,450 --> 00:11:31,790
rule but some of the thing like the

00:11:28,880 --> 00:11:34,310
people unification might be done almost

00:11:31,790 --> 00:11:37,340
automatically at least for helping the

00:11:34,310 --> 00:11:41,600
people so at the end of axis pro you I

00:11:37,340 --> 00:11:44,450
we had a system that pre that was pre

00:11:41,600 --> 00:11:48,050
computing the list of people inside the

00:11:44,450 --> 00:11:50,420
video so that the annotator might save

00:11:48,050 --> 00:11:53,540
some time instead of having to write

00:11:50,420 --> 00:11:57,170
everything he sees something was already

00:11:53,540 --> 00:12:00,830
written so in the ad probably to remove

00:11:57,170 --> 00:12:04,430
some errors he still has to add some

00:12:00,830 --> 00:12:07,640
data but some part of the information

00:12:04,430 --> 00:12:11,390
was already filled then we have

00:12:07,640 --> 00:12:13,760
developed the axis you I home you I so

00:12:11,390 --> 00:12:16,190
for the home user so the idea was

00:12:13,760 --> 00:12:19,520
referencing much more like YouTube where

00:12:16,190 --> 00:12:22,550
people are spending time and you watch a

00:12:19,520 --> 00:12:24,920
video and you will have recommendation

00:12:22,550 --> 00:12:29,270
of senior videos so that you can spend a

00:12:24,920 --> 00:12:34,310
lot of time on the system and the

00:12:29,270 --> 00:12:38,020
research UI which was focusing on

00:12:34,310 --> 00:12:43,940
journalists we want to produce reports

00:12:38,020 --> 00:12:46,490
and the open access project that i will

00:12:43,940 --> 00:12:50,020
present later is reusing the axis

00:12:46,490 --> 00:12:55,850
research UI which was the one which is

00:12:50,020 --> 00:12:58,460
the most easy to understand so what was

00:12:55,850 --> 00:13:04,010
the open access project so open access

00:12:58,460 --> 00:13:06,980
project is the free and or i will

00:13:04,010 --> 00:13:11,420
explain why or after open source version

00:13:06,980 --> 00:13:13,080
of axes which is a node w2 well done sir

00:13:11,420 --> 00:13:14,640
project it was

00:13:13,080 --> 00:13:18,120
proposed to European Commission and

00:13:14,640 --> 00:13:20,520
required by end user join the project

00:13:18,120 --> 00:13:24,420
because they wanted to be able to make

00:13:20,520 --> 00:13:26,760
some demonstration and maybe to buy some

00:13:24,420 --> 00:13:29,760
software later and they want it to be

00:13:26,760 --> 00:13:32,460
able to compare we were just before

00:13:29,760 --> 00:13:35,220
talking about benchmark but they wanted

00:13:32,460 --> 00:13:37,850
to compare the from an end user point of

00:13:35,220 --> 00:13:43,050
view the results of the system I mean

00:13:37,850 --> 00:13:46,050
comparing if one name on gtx tractor is

00:13:43,050 --> 00:13:48,300
better than another if one image

00:13:46,050 --> 00:13:50,880
recognition system is better than

00:13:48,300 --> 00:13:52,830
another so they wanted to be able to to

00:13:50,880 --> 00:13:55,260
plug the system to integrate another

00:13:52,830 --> 00:13:57,630
component inside and to see if the

00:13:55,260 --> 00:14:02,550
results are better or not but only from

00:13:57,630 --> 00:14:04,770
a user perspective and the idea was not

00:14:02,550 --> 00:14:08,340
to have a baseline for following project

00:14:04,770 --> 00:14:11,150
proposal so what we have done is that we

00:14:08,340 --> 00:14:15,450
have been using partners code who wanted

00:14:11,150 --> 00:14:18,570
we have replaced some of the components

00:14:15,450 --> 00:14:21,840
by similar component when it was not

00:14:18,570 --> 00:14:24,270
possible to use the code of the partners

00:14:21,840 --> 00:14:27,210
and when I say it's not fully open

00:14:24,270 --> 00:14:32,400
source yet is because two component fri

00:14:27,210 --> 00:14:34,020
/ back by the partners wanted to have

00:14:32,400 --> 00:14:37,790
their component available in the

00:14:34,020 --> 00:14:42,000
demonstration but we're not able to

00:14:37,790 --> 00:14:43,590
release their code so its binary system

00:14:42,000 --> 00:14:45,870
so it's a freeware for two of the

00:14:43,590 --> 00:14:48,060
components but the problem is for rest

00:14:45,870 --> 00:14:50,280
for the moment is that one of the

00:14:48,060 --> 00:14:53,370
components from technical is crucial for

00:14:50,280 --> 00:14:57,300
the system so we are working on

00:14:53,370 --> 00:15:02,490
replacing it in the meanwhile now that

00:14:57,300 --> 00:15:05,310
the project is finished and for a bus it

00:15:02,490 --> 00:15:09,150
was a good idea to have open access

00:15:05,310 --> 00:15:10,470
system because it's a complementary is

00:15:09,150 --> 00:15:12,450
complementary with the web lab

00:15:10,470 --> 00:15:16,470
demonstration because this one it will

00:15:12,450 --> 00:15:19,980
be focusing on multimedia processing so

00:15:16,470 --> 00:15:21,840
what's inside open access video

00:15:19,980 --> 00:15:24,750
normalization so you put your video

00:15:21,840 --> 00:15:25,430
inside it's normalizing the video to

00:15:24,750 --> 00:15:28,760
cone

00:15:25,430 --> 00:15:31,550
 to format which is endl by the next

00:15:28,760 --> 00:15:33,860
components then there is shot boundary

00:15:31,550 --> 00:15:37,399
detection provided by technical or and

00:15:33,860 --> 00:15:41,480
keyframe extraction and this is needed

00:15:37,399 --> 00:15:44,450
to prevent from analyzing all the images

00:15:41,480 --> 00:15:48,440
of a given video and to selecting only

00:15:44,450 --> 00:15:50,899
the key frames of a video then we have

00:15:48,440 --> 00:15:55,100
image concept extraction provided by K

00:15:50,899 --> 00:15:58,730
11 so it the aim was to annotate each

00:15:55,100 --> 00:16:01,640
keyframe with one of the concepts that I

00:15:58,730 --> 00:16:05,320
was wrong they have learned the 1000

00:16:01,640 --> 00:16:07,970
concept over the image net network and

00:16:05,320 --> 00:16:12,529
then there is a spoken-- women meta data

00:16:07,970 --> 00:16:16,790
search speech to text so previously it

00:16:12,529 --> 00:16:19,160
was done by fan over yeah yes but it was

00:16:16,790 --> 00:16:20,959
not possible to use a software in this

00:16:19,160 --> 00:16:24,010
kind of them also we have replaced it by

00:16:20,959 --> 00:16:27,770
CMU's fangs which is an open source and

00:16:24,010 --> 00:16:30,560
with the CMU models for English and Leo

00:16:27,770 --> 00:16:35,779
humor so it's a little more University

00:16:30,560 --> 00:16:39,620
for french models Simon similar image

00:16:35,779 --> 00:16:42,500
search so here again it was at the

00:16:39,620 --> 00:16:44,150
beginning provided by in really and it

00:16:42,500 --> 00:16:47,959
was not possible because they are

00:16:44,150 --> 00:16:50,839
selling the software basically so we

00:16:47,959 --> 00:16:54,279
replaced it by plastic which is another

00:16:50,839 --> 00:16:58,400
open source so on the fly search

00:16:54,279 --> 00:17:01,450
provided by a university of oxford on

00:16:58,400 --> 00:17:05,510
sofa search hood was one of the main

00:17:01,450 --> 00:17:09,890
results of the axis project in terms of

00:17:05,510 --> 00:17:14,179
research the idea of the on the fly

00:17:09,890 --> 00:17:17,480
search is that you cannot always pretty

00:17:14,179 --> 00:17:20,569
fine the kind of objects the kind of

00:17:17,480 --> 00:17:23,420
information that you want to extract and

00:17:20,569 --> 00:17:29,270
that you want to annotate inside the

00:17:23,420 --> 00:17:32,179
video for instance the k 11 system is

00:17:29,270 --> 00:17:35,300
relying on the 1000 classes that are

00:17:32,179 --> 00:17:36,410
provided by imagenet so there is 1000

00:17:35,300 --> 00:17:38,600
kind of

00:17:36,410 --> 00:17:42,560
information that is annotated so it can

00:17:38,600 --> 00:17:45,710
be there is a boat inside the video

00:17:42,560 --> 00:17:49,550
there is a person there is a hammer

00:17:45,710 --> 00:17:52,790
there is a the tiger whatever so there

00:17:49,550 --> 00:17:55,610
are 1000 classes but you always find a

00:17:52,790 --> 00:17:58,190
use case where another class was needed

00:17:55,610 --> 00:18:02,360
so the idea of the on the fly search if

00:17:58,190 --> 00:18:04,930
that in fact it Google's the the word

00:18:02,360 --> 00:18:08,270
that you are searching it download the

00:18:04,930 --> 00:18:10,700
images that are responding to google it

00:18:08,270 --> 00:18:13,910
learns a model from it and annotate your

00:18:10,700 --> 00:18:19,490
database based on that alarms on the

00:18:13,910 --> 00:18:22,030
flying model and then so favorite lies

00:18:19,490 --> 00:18:26,080
must view was brought in by tcu and

00:18:22,030 --> 00:18:28,880
recommendation by UT and the system is

00:18:26,080 --> 00:18:33,320
compliant compatible with human too and

00:18:28,880 --> 00:18:35,510
min so it's debian-based basically what

00:18:33,320 --> 00:18:38,120
was the driving motivator from ephesus

00:18:35,510 --> 00:18:43,760
perspective doing this kind of

00:18:38,120 --> 00:18:47,990
collaboration this kind of tool there is

00:18:43,760 --> 00:18:50,830
a lot of services I mean intelligence

00:18:47,990 --> 00:18:52,970
services that are very interesting in

00:18:50,830 --> 00:18:55,520
analyzing automatically what she's

00:18:52,970 --> 00:18:59,240
inside the video provided on YouTube for

00:18:55,520 --> 00:19:03,560
instance so and we are setting our stuff

00:18:59,240 --> 00:19:05,990
to intelligence services basically but

00:19:03,560 --> 00:19:08,300
also we are working on competitive

00:19:05,990 --> 00:19:12,050
intelligence and that's also something

00:19:08,300 --> 00:19:14,360
which is very interesting to be able to

00:19:12,050 --> 00:19:16,880
automatically I know when your

00:19:14,360 --> 00:19:19,370
competitors are providing some videos on

00:19:16,880 --> 00:19:21,620
YouTube and some demo of the system and

00:19:19,370 --> 00:19:23,540
to analyze it automatically and we are

00:19:21,620 --> 00:19:29,390
also working on that complementation

00:19:23,540 --> 00:19:31,790
spilled that's a tid we can see also

00:19:29,390 --> 00:19:34,190
more and more embedded technologies for

00:19:31,790 --> 00:19:40,030
image recognition to work on that

00:19:34,190 --> 00:19:44,840
respect to for embed the recognition of

00:19:40,030 --> 00:19:47,090
thanks to me in fact inside a access

00:19:44,840 --> 00:19:48,970
project what when it was only written

00:19:47,090 --> 00:19:51,100
here imaginating and

00:19:48,970 --> 00:19:54,070
is that if I interactive I since live

00:19:51,100 --> 00:19:56,200
and was working video OCR so extracting

00:19:54,070 --> 00:20:02,409
the subtitles and any kind of text which

00:19:56,200 --> 00:20:08,010
was written like an demonstration text

00:20:02,409 --> 00:20:08,010
like a slow down moto and sing I sad and

00:20:08,460 --> 00:20:16,720
we were also working I mean how partners

00:20:12,940 --> 00:20:20,200
were working on the extraction of a lot

00:20:16,720 --> 00:20:22,900
of things with am working here so there

00:20:20,200 --> 00:20:26,830
is a database of places provided by

00:20:22,900 --> 00:20:29,470
Oxford which annotate and the image and

00:20:26,830 --> 00:20:32,110
try to locate not only based on the

00:20:29,470 --> 00:20:34,960
metadata the image with based on what is

00:20:32,110 --> 00:20:37,480
seen so it was able to recognize why not

00:20:34,960 --> 00:20:42,220
place like a photo errands in like that

00:20:37,480 --> 00:20:50,080
but also it was trying to there was some

00:20:42,220 --> 00:20:53,470
some some effort on some typical city

00:20:50,080 --> 00:20:55,780
like I mean it's a country it's it's a

00:20:53,470 --> 00:20:58,179
city from the south of France it's

00:20:55,780 --> 00:21:01,330
something from south of Italy for

00:20:58,179 --> 00:21:04,480
instance it's really really hard because

00:21:01,330 --> 00:21:07,409
a lot of similarity between the various

00:21:04,480 --> 00:21:09,970
kind of that it was one of the work also

00:21:07,409 --> 00:21:12,240
thank you very much and we can apply

00:21:09,970 --> 00:21:12,240

YouTube URL: https://www.youtube.com/watch?v=Cfmkxmhr-nA


