Title: OW2con'15 Keynote Bruno Dillenseger : Benchmarking Orange Forge with CLIF
Publication date: 2015-11-24
Playlist: OW2con 2015
Description: 
	At the middle of this year, Orange switched its internal software forge (so-called Orange Forge) to a completely new hardware infrastructure and a new version of the Tuleap forge software. With more than 1000 connected users daily, among 16000 registered users worldwide, working on more than 5000 active projects, this switch was definitely a critical operation.
Could this new brand new Orange Forge platform cope with such a traffic? If so, what about users' quality of experience? This is what we tried to know using OW2's load testing project CLIF. Feedback says it was a good idea to do so.
Captions: 
	00:00:06,109 --> 00:00:13,740
so I'm starting this afternoon talking

00:00:09,389 --> 00:00:17,850
about matching benchmarking and well for

00:00:13,740 --> 00:00:21,710
a consortium like Oh w-2 benchmarking is

00:00:17,850 --> 00:00:23,580
software forge or what we call nowadays

00:00:21,710 --> 00:00:28,580
application lifecycle management

00:00:23,580 --> 00:00:32,669
platform is I think of of interest

00:00:28,580 --> 00:00:36,930
especially because the orange fourth is

00:00:32,669 --> 00:00:42,469
based on dilip software and 0 W to today

00:00:36,930 --> 00:00:42,469
is now experimenting this this software

00:00:44,059 --> 00:00:51,930
this work has been carried out with a

00:00:46,590 --> 00:00:55,010
two co-workers Christian commute okay so

00:00:51,930 --> 00:00:57,239
this is a basically an experience of

00:00:55,010 --> 00:01:05,760
practically using leaf to benchmark

00:00:57,239 --> 00:01:09,750
software for so what is orange for

00:01:05,760 --> 00:01:13,229
orange Forge is the software for for the

00:01:09,750 --> 00:01:16,439
range labs which means rng but not only

00:01:13,229 --> 00:01:19,100
are on the at Orange it's also open to

00:01:16,439 --> 00:01:24,049
the orange group and also to external

00:01:19,100 --> 00:01:24,049
partners and it's a worldwide platform

00:01:24,409 --> 00:01:35,659
we have more than 1,000 users different

00:01:29,369 --> 00:01:39,240
users connected daily more than 16,000

00:01:35,659 --> 00:01:43,220
registered users and more than 5,000

00:01:39,240 --> 00:01:48,030
projects active projects or important

00:01:43,220 --> 00:01:52,799
and the orange forge trackers are also

00:01:48,030 --> 00:01:56,750
widely used for a number of support

00:01:52,799 --> 00:02:00,960
teams so for example if you if a project

00:01:56,750 --> 00:02:03,689
as well requires a number of virtual

00:02:00,960 --> 00:02:06,659
machines to build its platform is using

00:02:03,689 --> 00:02:08,759
the tracker to put a ticket and to ask

00:02:06,659 --> 00:02:12,099
for resources

00:02:08,759 --> 00:02:14,170
but of course the the support of the

00:02:12,099 --> 00:02:19,780
forge itself is based on the fourth

00:02:14,170 --> 00:02:22,439
truckers okay it has any forge it

00:02:19,780 --> 00:02:26,980
integrates a great variety of services

00:02:22,439 --> 00:02:29,370
CVS svn gate wiki strikers documentation

00:02:26,980 --> 00:02:34,569
and package management download

00:02:29,370 --> 00:02:37,720
management a lot number of agile tools

00:02:34,569 --> 00:02:41,909
agility tools meaningless continuous

00:02:37,720 --> 00:02:44,769
integration and so on it's a fort and

00:02:41,909 --> 00:02:47,709
while this year we had at the middle of

00:02:44,769 --> 00:02:51,069
this year we had to move to a completely

00:02:47,709 --> 00:02:55,150
new infrastructure with new servers new

00:02:51,069 --> 00:02:58,299
filers new network and also we did an

00:02:55,150 --> 00:03:02,799
upgrade of the tulip software on of the

00:02:58,299 --> 00:03:06,099
Linux operating system so of course the

00:03:02,799 --> 00:03:09,450
critical questions were will this new

00:03:06,099 --> 00:03:12,189
platform stand the incoming traffic and

00:03:09,450 --> 00:03:15,910
what will be the quality of experience

00:03:12,189 --> 00:03:18,489
so the necessity was of course to do

00:03:15,910 --> 00:03:21,699
some benchmarking so the idea of

00:03:18,489 --> 00:03:24,159
benchmarking is that you have an

00:03:21,699 --> 00:03:27,699
arbitrary system under test made of

00:03:24,159 --> 00:03:29,919
number of servers or whatever and you

00:03:27,699 --> 00:03:33,970
are going to use some load injectors

00:03:29,919 --> 00:03:37,030
which are just machines on the network

00:03:33,970 --> 00:03:39,879
that generate some traffic according to

00:03:37,030 --> 00:03:45,549
a Givens in a given scenario using some

00:03:39,879 --> 00:03:47,530
specific protocols and while this load

00:03:45,549 --> 00:03:51,519
injectors are going to generate requests

00:03:47,530 --> 00:03:53,549
and submit this request to the system

00:03:51,519 --> 00:03:57,159
under test and they are going to measure

00:03:53,549 --> 00:04:00,129
a performance metric like response time

00:03:57,159 --> 00:04:02,199
or throughput things like this when you

00:04:00,129 --> 00:04:06,970
define a benchmark you specify the

00:04:02,199 --> 00:04:08,919
system under test and its interface you

00:04:06,970 --> 00:04:13,389
define incoming traffic which will be a

00:04:08,919 --> 00:04:17,349
mix of some typical kinds of requests

00:04:13,389 --> 00:04:19,630
under the system interface and of course

00:04:17,349 --> 00:04:22,240
also specify a metric of interest

00:04:19,630 --> 00:04:25,569
because the goal of benchmark is too

00:04:22,240 --> 00:04:28,000
prayer the performance of different

00:04:25,569 --> 00:04:32,139
alternatives so each time you change

00:04:28,000 --> 00:04:36,750
something you will get this metric as a

00:04:32,139 --> 00:04:36,750
result and you will be able to compare

00:04:36,930 --> 00:04:42,819
to do this we use the cliff because

00:04:39,669 --> 00:04:46,120
while orange is leading a created this

00:04:42,819 --> 00:04:50,650
project more than 12 years ago in object

00:04:46,120 --> 00:04:54,009
web and then our w-2 and we are still

00:04:50,650 --> 00:04:56,740
leading this project the peculiarities

00:04:54,009 --> 00:04:59,110
of cliff this these are the reason why

00:04:56,740 --> 00:05:01,569
we created this project because there

00:04:59,110 --> 00:05:04,479
are some others comparable but not with

00:05:01,569 --> 00:05:07,479
these features the first thing is that

00:05:04,479 --> 00:05:10,800
is generic and extensible so we can use

00:05:07,479 --> 00:05:14,729
it for a variety of IP based protocols

00:05:10,800 --> 00:05:19,330
voice over IP database mobile networks

00:05:14,729 --> 00:05:21,520
arbitrary protocols it's flexible it

00:05:19,330 --> 00:05:26,009
probably it's provided through a number

00:05:21,520 --> 00:05:32,530
of environments like eclipse-based

00:05:26,009 --> 00:05:34,840
environment pure java software you have

00:05:32,530 --> 00:05:37,930
also several command line tools better

00:05:34,840 --> 00:05:40,990
than aunt on shell scripts or 11 and

00:05:37,930 --> 00:05:45,780
there is also a plugin for Jenkins that

00:05:40,990 --> 00:05:48,460
would drive and to generate test report

00:05:45,780 --> 00:05:50,349
we say that its advanced Keefe is

00:05:48,460 --> 00:05:53,199
advanced because we can manage millions

00:05:50,349 --> 00:05:57,099
of virtual users the question is that

00:05:53,199 --> 00:06:01,120
just to know if these virtual users use

00:05:57,099 --> 00:06:04,419
a lot of resources on that of course bet

00:06:01,120 --> 00:06:10,330
there are no no real limit for example

00:06:04,419 --> 00:06:14,830
we use it for the set-top boxes on all

00:06:10,330 --> 00:06:17,560
the the domestic gateways to the light

00:06:14,830 --> 00:06:20,849
box when we want to emulate the traffic

00:06:17,560 --> 00:06:24,669
of these devices well there we have

00:06:20,849 --> 00:06:27,699
hundreds of thousand or millions of such

00:06:24,669 --> 00:06:30,760
devices but they do little traffic each

00:06:27,699 --> 00:06:34,890
but we we have no trouble to emulate

00:06:30,760 --> 00:06:38,230
this kind of a benchmark

00:06:34,890 --> 00:06:41,550
cliff has been used up to more than

00:06:38,230 --> 00:06:44,010
1,000 load injectors in parallel and

00:06:41,550 --> 00:06:46,090
when our number of miscellaneous

00:06:44,010 --> 00:06:50,410
facilities that you don't necessarily

00:06:46,090 --> 00:06:53,260
find in other projects and it's now we

00:06:50,410 --> 00:06:55,990
consider it at as major because we had a

00:06:53,260 --> 00:07:00,190
lot of feedback internally and

00:06:55,990 --> 00:07:04,780
externally thanks to to cliff being in

00:07:00,190 --> 00:07:09,550
open source and we all saw this plugin

00:07:04,780 --> 00:07:13,570
for Jenkins which can be viewed first as

00:07:09,550 --> 00:07:16,300
a web interface to cliff and also it can

00:07:13,570 --> 00:07:19,570
be used of course to do some performance

00:07:16,300 --> 00:07:21,970
testing in continuous integration and

00:07:19,570 --> 00:07:25,210
what is very convenient in our use case

00:07:21,970 --> 00:07:27,910
here is that we will be able very easily

00:07:25,210 --> 00:07:31,390
to run tests because it's just the web

00:07:27,910 --> 00:07:33,070
console web interface and also because

00:07:31,390 --> 00:07:37,090
it generates automatically the

00:07:33,070 --> 00:07:39,550
performance reports for each test and

00:07:37,090 --> 00:07:42,340
also you have the trend performance the

00:07:39,550 --> 00:07:49,570
performance trends so sorry from one

00:07:42,340 --> 00:07:55,030
test to another and so here is a schema

00:07:49,570 --> 00:07:57,850
of the orange orange for platform and we

00:07:55,030 --> 00:08:01,030
when we started this benchmarking

00:07:57,850 --> 00:08:05,190
campaign we had three platforms the the

00:08:01,030 --> 00:08:07,960
current prediction the quality insurance

00:08:05,190 --> 00:08:11,620
assurance platform which is just for

00:08:07,960 --> 00:08:15,210
test and also we have the new projection

00:08:11,620 --> 00:08:19,830
the future production which is the one

00:08:15,210 --> 00:08:23,350
that is currently running today and

00:08:19,830 --> 00:08:27,760
understand network we have the three

00:08:23,350 --> 00:08:30,100
load injection machines okay and we have

00:08:27,760 --> 00:08:35,380
also which is used for the benchmarking

00:08:30,100 --> 00:08:37,960
companion we have also another load

00:08:35,380 --> 00:08:41,050
injector with which will generate a thin

00:08:37,960 --> 00:08:44,200
load injection and which is not directly

00:08:41,050 --> 00:08:45,089
on the production network is here it is

00:08:44,200 --> 00:08:50,149
a

00:08:45,089 --> 00:08:55,439
in the orange lab the projection network

00:08:50,149 --> 00:08:58,319
okay so when we started to think about

00:08:55,439 --> 00:09:02,129
benchmarking orange fault of course we

00:08:58,319 --> 00:09:05,550
thought about the best practices but

00:09:02,129 --> 00:09:08,069
then we were confronted to reality the

00:09:05,550 --> 00:09:10,290
best practices in benchmarking is that

00:09:08,069 --> 00:09:15,720
first you have to generate you have to

00:09:10,290 --> 00:09:17,939
write a realistic scenarios and for

00:09:15,720 --> 00:09:22,290
example you can start from the logs of

00:09:17,939 --> 00:09:27,360
the platform of the service to to get

00:09:22,290 --> 00:09:30,680
the right request and the two to define

00:09:27,360 --> 00:09:33,300
the scenario but the reality is that

00:09:30,680 --> 00:09:38,129
there is a mix of a lot of different

00:09:33,300 --> 00:09:42,480
protocols most of them are based on HTTP

00:09:38,129 --> 00:09:46,860
so all the logs mixes all this HTTP

00:09:42,480 --> 00:09:49,740
requests the logs are too low level so

00:09:46,860 --> 00:09:53,910
it's very complex and the usage also is

00:09:49,740 --> 00:09:57,449
very variable it depends on the hour in

00:09:53,910 --> 00:09:59,339
the day the day in the week or in the

00:09:57,449 --> 00:10:02,490
during that weekend during the holidays

00:09:59,339 --> 00:10:05,029
or not holidays and so on so it's really

00:10:02,490 --> 00:10:11,670
really hard to figure out what is the

00:10:05,029 --> 00:10:15,059
the average traffic and what are the the

00:10:11,670 --> 00:10:20,040
real protocols that are used the tools

00:10:15,059 --> 00:10:22,790
and okay so the other so a very hot

00:10:20,040 --> 00:10:26,129
point here about defining the scenarios

00:10:22,790 --> 00:10:29,100
the other point in best practice is that

00:10:26,129 --> 00:10:31,620
you will run then you the benchmark on

00:10:29,100 --> 00:10:34,499
both the new platform on the exact copy

00:10:31,620 --> 00:10:37,829
of the production platform but without

00:10:34,499 --> 00:10:40,110
the production traffic okay so there

00:10:37,829 --> 00:10:43,410
should be no no users and you generate

00:10:40,110 --> 00:10:44,939
the same traffic on both but the problem

00:10:43,410 --> 00:10:48,660
here is that our quality assurance

00:10:44,939 --> 00:10:52,529
platform is not equal to the production

00:10:48,660 --> 00:10:56,069
platform it has less resources it's less

00:10:52,529 --> 00:10:58,510
powerful so we cannot do that we cannot

00:10:56,069 --> 00:11:05,320
generate the same traffic on this but

00:10:58,510 --> 00:11:09,730
because it won't be relevant okay so

00:11:05,320 --> 00:11:14,680
second problem we don't have a quality a

00:11:09,730 --> 00:11:17,170
nicer production platform and and then

00:11:14,680 --> 00:11:19,270
the third point in best practices that

00:11:17,170 --> 00:11:21,220
you change only one element at a time

00:11:19,270 --> 00:11:23,350
and then you can really compare the

00:11:21,220 --> 00:11:25,690
influence the influence of this

00:11:23,350 --> 00:11:28,240
parameter of this element and this

00:11:25,690 --> 00:11:32,070
really help troubleshooting and knowing

00:11:28,240 --> 00:11:35,110
what to do to to increase performance

00:11:32,070 --> 00:11:37,680
but well here it's bad luck but we have

00:11:35,110 --> 00:11:40,600
all your hardware or you software

00:11:37,680 --> 00:11:43,900
another operating system you file around

00:11:40,600 --> 00:11:49,060
so on so there are lot of changes at the

00:11:43,900 --> 00:11:52,720
time and so well we cannot do the best

00:11:49,060 --> 00:11:55,870
practice so we had to adopt a pragmatic

00:11:52,720 --> 00:12:00,250
approach first we choose a number of

00:11:55,870 --> 00:12:03,610
services and we define users in our laws

00:12:00,250 --> 00:12:07,030
related to these services so we define a

00:12:03,610 --> 00:12:10,720
nice VN check out and commit scenario a

00:12:07,030 --> 00:12:15,970
git push and fetch scenario a scenario

00:12:10,720 --> 00:12:19,330
also with who is going with browsing the

00:12:15,970 --> 00:12:24,520
web portal and then which is using some

00:12:19,330 --> 00:12:27,340
agility tools and also having a look at

00:12:24,520 --> 00:12:31,830
this vent free the g3 some trackers on

00:12:27,340 --> 00:12:34,210
forums and so on fortunately the api's

00:12:31,830 --> 00:12:37,630
between the previous version of tulip

00:12:34,210 --> 00:12:39,640
and current version did not change so we

00:12:37,630 --> 00:12:43,060
can really define the same scenario on

00:12:39,640 --> 00:12:47,410
run and the old version and the old

00:12:43,060 --> 00:12:49,930
projection and on the new version well

00:12:47,410 --> 00:12:52,870
secondly where we have posed to create

00:12:49,930 --> 00:12:54,880
data sets which means we have to

00:12:52,870 --> 00:12:58,060
provision the number of projects of

00:12:54,880 --> 00:13:00,400
users of repositories and database

00:12:58,060 --> 00:13:04,150
content so that we can run the test we

00:13:00,400 --> 00:13:06,460
started from existing projects the thing

00:13:04,150 --> 00:13:09,250
here is also that there is a lot of

00:13:06,460 --> 00:13:11,170
variability from one project to another

00:13:09,250 --> 00:13:15,310
as some

00:13:11,170 --> 00:13:19,180
we'll have a lot of data probably with a

00:13:15,310 --> 00:13:22,410
very small number of big files very big

00:13:19,180 --> 00:13:25,000
files and sometimes it's the contrary so

00:13:22,410 --> 00:13:28,990
here again it's very hard to figure out

00:13:25,000 --> 00:13:32,170
what is the representative data set so

00:13:28,990 --> 00:13:35,470
while we try to do a number of things

00:13:32,170 --> 00:13:39,300
and the different scenarios with big

00:13:35,470 --> 00:13:43,990
repositories more repositories and so on

00:13:39,300 --> 00:13:45,459
and that we forest service we will run

00:13:43,990 --> 00:13:48,579
the test and we compare the performance

00:13:45,459 --> 00:13:51,190
of current production with the project

00:13:48,579 --> 00:13:56,100
the performance of the new platform but

00:13:51,190 --> 00:14:00,190
of course well the the old platform is

00:13:56,100 --> 00:14:03,279
this is submitted to the real user

00:14:00,190 --> 00:14:06,070
traffic while the new platform well is

00:14:03,279 --> 00:14:08,829
either empty there is no traffic or we

00:14:06,070 --> 00:14:11,190
have to generate ourselves some

00:14:08,829 --> 00:14:16,570
background traffic using these scenarios

00:14:11,190 --> 00:14:21,630
and finally I think what we took has a

00:14:16,570 --> 00:14:25,839
reference to see that to check if our

00:14:21,630 --> 00:14:30,519
our workload is comparable to the real

00:14:25,839 --> 00:14:35,380
work load is the network throughput it

00:14:30,519 --> 00:14:37,480
was the easiest way while what was

00:14:35,380 --> 00:14:42,970
interesting in using cliff for this

00:14:37,480 --> 00:14:45,940
benchmark well first we use the provided

00:14:42,970 --> 00:14:49,149
HTTP injector for a web-based tools but

00:14:45,940 --> 00:14:51,600
I would say that most almost all

00:14:49,149 --> 00:14:56,430
benchmarking tools have an HTTP

00:14:51,600 --> 00:14:59,860
generator traffic generator but what is

00:14:56,430 --> 00:15:01,899
what is really valuable with refills is

00:14:59,860 --> 00:15:06,160
that we could develop our own svn

00:15:01,899 --> 00:15:09,430
injector and get injector plugins to

00:15:06,160 --> 00:15:11,440
generate traffic for svn and get we use

00:15:09,430 --> 00:15:13,990
the kief console itself because it's

00:15:11,440 --> 00:15:16,779
based on eclipse so you have already

00:15:13,990 --> 00:15:20,440
when you install the main cliff console

00:15:16,779 --> 00:15:22,950
and the which is the main cliff GUI you

00:15:20,440 --> 00:15:24,170
have finally you have

00:15:22,950 --> 00:15:28,230
development a Java development

00:15:24,170 --> 00:15:32,160
environment and what we did is that we

00:15:28,230 --> 00:15:35,580
took some existing Java clients Java

00:15:32,160 --> 00:15:38,970
stacks forage svn and get and we just

00:15:35,580 --> 00:15:42,720
wrapped them in torquay firm your cliff

00:15:38,970 --> 00:15:45,060
injector okay so what was really

00:15:42,720 --> 00:15:47,490
interesting with this svn and get

00:15:45,060 --> 00:15:50,820
injector is that we don't have to

00:15:47,490 --> 00:15:53,810
provide data set in this case because we

00:15:50,820 --> 00:15:57,150
when we do some commits we generate

00:15:53,810 --> 00:16:00,330
files and file contents on the fly in a

00:15:57,150 --> 00:16:03,570
random manner so we don't have to have a

00:16:00,330 --> 00:16:06,990
pre-existing repository and in the other

00:16:03,570 --> 00:16:09,750
way when we do some check out or some

00:16:06,990 --> 00:16:13,410
fetch actually we just discard the data

00:16:09,750 --> 00:16:15,750
so so the load injector is not

00:16:13,410 --> 00:16:21,030
overloaded with that that's a real an

00:16:15,750 --> 00:16:24,270
advantage okay what we did also is that

00:16:21,030 --> 00:16:27,660
we use the cliff probes which are

00:16:24,270 --> 00:16:32,510
provided with cliff so on the service

00:16:27,660 --> 00:16:36,480
side we could examine check the usage of

00:16:32,510 --> 00:16:38,340
CPU of memory and so on also of the

00:16:36,480 --> 00:16:42,450
network bandwidth this was very

00:16:38,340 --> 00:16:45,060
important and we did the same at the

00:16:42,450 --> 00:16:46,890
load injection site because we wanted to

00:16:45,060 --> 00:16:50,070
check that the load injectors were not

00:16:46,890 --> 00:16:53,730
themselves saturating you have to check

00:16:50,070 --> 00:16:56,460
that load injector is very healthy the

00:16:53,730 --> 00:17:00,120
wise the scenario won't be the one you

00:16:56,460 --> 00:17:06,810
have defined and measures will be will

00:17:00,120 --> 00:17:08,820
be biased okay and so what was

00:17:06,810 --> 00:17:13,290
interesting also is that with tiff you

00:17:08,820 --> 00:17:15,720
separate the very concept of test plan

00:17:13,290 --> 00:17:18,540
and with the concept of scenario and

00:17:15,720 --> 00:17:21,089
within a test test plan you can mix a

00:17:18,540 --> 00:17:24,329
number of scenarios and deploy them of

00:17:21,089 --> 00:17:26,400
one injector or or another so it was

00:17:24,329 --> 00:17:28,860
very convenient to be able to use

00:17:26,400 --> 00:17:29,980
scenarios in a number of ways without

00:17:28,860 --> 00:17:33,790
having to

00:17:29,980 --> 00:17:36,220
to merge them and also it's very easy to

00:17:33,790 --> 00:17:39,460
take a scenario to change the number of

00:17:36,220 --> 00:17:43,570
virtual users depending and the workload

00:17:39,460 --> 00:17:47,020
you want to generate okay so now let's

00:17:43,570 --> 00:17:49,900
get let's go to the benchmarking

00:17:47,020 --> 00:17:53,460
campaign itself so the first step was to

00:17:49,900 --> 00:17:58,540
get a reference benchmark starting from

00:17:53,460 --> 00:18:00,820
current production platform so we there

00:17:58,540 --> 00:18:04,120
are the the orange for users are there

00:18:00,820 --> 00:18:07,480
they generate naturally some requests

00:18:04,120 --> 00:18:10,049
and the background load workload on the

00:18:07,480 --> 00:18:13,120
production platform and then we did some

00:18:10,049 --> 00:18:15,100
where we run our scenarios between the

00:18:13,120 --> 00:18:18,940
unit traffic which means with just a

00:18:15,100 --> 00:18:21,220
single virtual user and so this way and

00:18:18,940 --> 00:18:24,280
one service at a time so we can have a

00:18:21,220 --> 00:18:28,120
reference performance in terms of

00:18:24,280 --> 00:18:33,040
response time mainly for the production

00:18:28,120 --> 00:18:37,240
platform with with current with the real

00:18:33,040 --> 00:18:39,690
traffic then the step two is that we are

00:18:37,240 --> 00:18:42,250
going to compare this reference

00:18:39,690 --> 00:18:46,660
performance with the new production

00:18:42,250 --> 00:18:48,820
platform but with just the unit traffic

00:18:46,660 --> 00:18:51,040
and the new production platform which

00:18:48,820 --> 00:18:54,790
means there is no other background

00:18:51,040 --> 00:18:57,580
workload and of course what we expect is

00:18:54,790 --> 00:18:59,080
that we must have that the new

00:18:57,580 --> 00:19:02,890
production platform must be always

00:18:59,080 --> 00:19:05,799
faster and finally it was not the case

00:19:02,890 --> 00:19:09,880
so we had to work we found a lot of

00:19:05,799 --> 00:19:11,890
defects Indians in the setup and so it

00:19:09,880 --> 00:19:17,080
was this step where it's very important

00:19:11,890 --> 00:19:20,620
we had to fix a number of things the

00:19:17,080 --> 00:19:22,870
third step is that we compared the new

00:19:20,620 --> 00:19:25,780
production the performance of the new

00:19:22,870 --> 00:19:28,000
production but this time with heavy

00:19:25,780 --> 00:19:30,520
traffic so this time we took our

00:19:28,000 --> 00:19:34,299
scenarios but we deploy them on three

00:19:30,520 --> 00:19:36,700
load injectors and we may we increase

00:19:34,299 --> 00:19:39,850
the number of virtual users so we had

00:19:36,700 --> 00:19:42,909
three load injectors mix of all services

00:19:39,850 --> 00:19:47,619
web svn get and so on

00:19:42,909 --> 00:19:50,950
with multiple sessions we had 100 active

00:19:47,619 --> 00:19:52,539
virtual users simultaneously and we

00:19:50,950 --> 00:19:54,279
check that the network bandwidth was

00:19:52,539 --> 00:19:58,269
close to that of the real traffic which

00:19:54,279 --> 00:20:02,049
means it was about 200 or 300 make a bit

00:19:58,269 --> 00:20:04,960
per second and then we compare once

00:20:02,049 --> 00:20:08,649
again with the reference performance and

00:20:04,960 --> 00:20:11,259
here again we found a number of problems

00:20:08,649 --> 00:20:12,970
that we're aquatics and whether of

00:20:11,259 --> 00:20:14,739
course the idea is that the new

00:20:12,970 --> 00:20:20,590
production platform should be globally

00:20:14,739 --> 00:20:24,879
faster faster here is an example of what

00:20:20,590 --> 00:20:27,999
we we got the results we got this is

00:20:24,879 --> 00:20:31,419
jada Jenkins agree web application with

00:20:27,999 --> 00:20:33,519
the cliff plugin and automatically the

00:20:31,419 --> 00:20:36,849
test test reports are generated at the

00:20:33,519 --> 00:20:40,059
end of a of a test and here what we see

00:20:36,849 --> 00:20:44,529
is the trend the cliff performance trend

00:20:40,059 --> 00:20:47,409
so that means that we have eight here

00:20:44,529 --> 00:20:51,989
for example we have hate test runs and

00:20:47,409 --> 00:20:57,639
we can compare the average response time

00:20:51,989 --> 00:21:00,070
in blue the boxes in blue in red it's

00:20:57,639 --> 00:21:02,200
the maximum response time it in the

00:21:00,070 --> 00:21:05,080
point in deep blue with just the line is

00:21:02,200 --> 00:21:08,919
the minimum response time and you have

00:21:05,080 --> 00:21:12,249
the plus or minus the stunt standard

00:21:08,919 --> 00:21:16,419
deviation which is the mustache around

00:21:12,249 --> 00:21:19,659
the the average so we could easily see

00:21:16,419 --> 00:21:23,019
here when we change a parameter either

00:21:19,659 --> 00:21:26,129
if the performance was better of words

00:21:23,019 --> 00:21:28,389
we see for example that the two last are

00:21:26,129 --> 00:21:30,580
really better than the previous ones

00:21:28,389 --> 00:21:35,979
because we changed something really

00:21:30,580 --> 00:21:37,629
important so for each kind of request of

00:21:35,979 --> 00:21:40,389
an hour scenarios here you have the

00:21:37,629 --> 00:21:43,989
login you have the view of the agile

00:21:40,389 --> 00:21:47,499
dive dashboard while I cut the remaining

00:21:43,989 --> 00:21:49,239
but you for each kind of request you see

00:21:47,499 --> 00:21:50,739
the response time and the evolution of

00:21:49,239 --> 00:21:55,119
response time when you change the

00:21:50,739 --> 00:21:56,830
parameter okay in step 4 then we tuned

00:21:55,119 --> 00:22:01,210
the new production platform

00:21:56,830 --> 00:22:03,820
so here we say okay performance is fair

00:22:01,210 --> 00:22:07,059
compared to the old production platform

00:22:03,820 --> 00:22:08,890
but probably we can do better because we

00:22:07,059 --> 00:22:12,429
have a new hardware which is more

00:22:08,890 --> 00:22:16,210
powerful we have more resources okay so

00:22:12,429 --> 00:22:20,010
we still run our heavy traffic scenario

00:22:16,210 --> 00:22:23,950
with 100 ventral users on all services

00:22:20,010 --> 00:22:27,460
but now we put some probes and we can

00:22:23,950 --> 00:22:32,380
see if the CPU is okay or not if the

00:22:27,460 --> 00:22:35,200
memory is full or not and so on and we

00:22:32,380 --> 00:22:42,010
compare of course we try to change the

00:22:35,200 --> 00:22:45,789
resources added cpu or or change the run

00:22:42,010 --> 00:22:55,419
configuration to see if we can increase

00:22:45,789 --> 00:22:59,200
performance okay so I ok so during step

00:22:55,419 --> 00:23:00,880
214 and 3 as i said we found a number of

00:22:59,200 --> 00:23:04,090
problems the main problem is that the

00:23:00,880 --> 00:23:07,980
PHP cash was not working because it was

00:23:04,090 --> 00:23:11,019
very stupid problem about a package

00:23:07,980 --> 00:23:14,950
which was renamed in the in dublin i

00:23:11,019 --> 00:23:17,710
think it was debian new version with

00:23:14,950 --> 00:23:22,500
another name and so the PHP cash was not

00:23:17,710 --> 00:23:27,010
working and we found it this reddit

00:23:22,500 --> 00:23:30,340
sorry and finally we found that thanks

00:23:27,010 --> 00:23:33,279
to the tulips embedded administration

00:23:30,340 --> 00:23:35,230
tools we found that if this cache was

00:23:33,279 --> 00:23:39,220
empty and what was not working so we

00:23:35,230 --> 00:23:42,220
could fix it while most of the problems

00:23:39,220 --> 00:23:45,250
were really rated tu caches and we saw

00:23:42,220 --> 00:23:48,820
that because the first test run was very

00:23:45,250 --> 00:23:51,639
bad in performance and if we we do

00:23:48,820 --> 00:23:53,860
another test it was much better so it

00:23:51,639 --> 00:23:57,700
shows that at the first front caches

00:23:53,860 --> 00:24:02,740
whir whir networking and then it was

00:23:57,700 --> 00:24:04,809
working we had all sewed up some

00:24:02,740 --> 00:24:07,760
problems with the load injection itself

00:24:04,809 --> 00:24:10,700
and this is very classical

00:24:07,760 --> 00:24:12,170
a point here you really have to check

00:24:10,700 --> 00:24:14,000
that your load injection system is

00:24:12,170 --> 00:24:18,970
performing all right and the problem

00:24:14,000 --> 00:24:25,130
here is that the network infrastructure

00:24:18,970 --> 00:24:28,580
at a certain point was in on a switch we

00:24:25,130 --> 00:24:31,760
just fast it on its switch with 100

00:24:28,580 --> 00:24:35,240
megabit per second and we saw that we go

00:24:31,760 --> 00:24:38,570
far beyond this limit and so our traffic

00:24:35,240 --> 00:24:40,910
was limited to 100 megabit per second

00:24:38,570 --> 00:24:42,380
and first of all you don't know if it's

00:24:40,910 --> 00:24:45,200
a problem with your system under test

00:24:42,380 --> 00:24:48,380
you say well it's saturating or what

00:24:45,200 --> 00:24:51,800
what's happening now you have first to

00:24:48,380 --> 00:24:54,230
think about a problem in your wrote

00:24:51,800 --> 00:24:55,970
injection system and so we could fix it

00:24:54,230 --> 00:25:00,680
we change the network equipment of

00:24:55,970 --> 00:25:04,370
course to go to 200 or 300 megabits per

00:25:00,680 --> 00:25:06,580
second so we found a number of things

00:25:04,370 --> 00:25:11,300
about the version of the linux kernel's

00:25:06,580 --> 00:25:14,630
using an FS overload local disk also

00:25:11,300 --> 00:25:17,990
moving the temporary directory to in

00:25:14,630 --> 00:25:20,000
memory to memory was was also an

00:25:17,990 --> 00:25:23,240
interesting thing all these are a lot of

00:25:20,000 --> 00:25:26,180
details but you can you can have a gain

00:25:23,240 --> 00:25:29,510
of 10 percent or twenty percent or more

00:25:26,180 --> 00:25:34,040
with such details with each of these

00:25:29,510 --> 00:25:37,010
details and step 4 which was the tuning

00:25:34,040 --> 00:25:40,040
of the platform with the probes was very

00:25:37,010 --> 00:25:43,130
very interesting because we saw that the

00:25:40,040 --> 00:25:46,460
memory the ram was oversized we did not

00:25:43,130 --> 00:25:48,980
need some so much RAM so this is the

00:25:46,460 --> 00:25:53,570
reason why we took part of this one and

00:25:48,980 --> 00:25:56,360
we put it the we affected it to the

00:25:53,570 --> 00:25:58,570
temporary directory and also we found

00:25:56,360 --> 00:26:02,350
that the CPU was really really

00:25:58,570 --> 00:26:07,100
overloaded and so we increased the CPU

00:26:02,350 --> 00:26:13,550
so this are the average performance gain

00:26:07,100 --> 00:26:17,290
speed up and we got with step 4 so we by

00:26:13,550 --> 00:26:20,850
tuning and just by adding a number of

00:26:17,290 --> 00:26:24,450
processors and by moving

00:26:20,850 --> 00:26:26,840
the temporary director is a two run it's

00:26:24,450 --> 00:26:31,080
an average of thirty percent

00:26:26,840 --> 00:26:33,060
third-person speed up here we have four

00:26:31,080 --> 00:26:35,760
it's for each kind of request so you

00:26:33,060 --> 00:26:42,210
have the svn commit the agile backlog

00:26:35,760 --> 00:26:45,720
edge i guard wall and so on okay finally

00:26:42,210 --> 00:26:48,480
step 5 is that now we have defined all

00:26:45,720 --> 00:26:51,180
these scenarios to to see the response

00:26:48,480 --> 00:26:53,490
time we can use it to monitor the

00:26:51,180 --> 00:26:55,650
quality of experience of the fall so

00:26:53,490 --> 00:26:58,160
this is a reason why with yes we have

00:26:55,650 --> 00:27:00,960
this load injector which is outside the

00:26:58,160 --> 00:27:04,500
infrastructure which is on the same

00:27:00,960 --> 00:27:06,540
network as we used every day when we

00:27:04,500 --> 00:27:09,720
connect to a ranch for and so we can

00:27:06,540 --> 00:27:13,140
monitor the response time and the

00:27:09,720 --> 00:27:18,750
quality of experience and we can show it

00:27:13,140 --> 00:27:22,710
we use here the periodic test run which

00:27:18,750 --> 00:27:24,590
is enabled by by Jenkins we have you see

00:27:22,710 --> 00:27:27,480
that we have these strands of

00:27:24,590 --> 00:27:29,100
performance and we can set some

00:27:27,480 --> 00:27:32,190
notifications to the platform

00:27:29,100 --> 00:27:37,170
administrators when something is going

00:27:32,190 --> 00:27:41,370
wrong or too slow okay so now it's time

00:27:37,170 --> 00:27:44,280
for conclusion so first of all we were

00:27:41,370 --> 00:27:46,740
confronted to the deep complexity of

00:27:44,280 --> 00:27:48,990
benchmarking it's true that i'm leading

00:27:46,740 --> 00:27:52,020
the cliff project for more than 10 years

00:27:48,990 --> 00:27:56,160
but a moral java developer than a bench

00:27:52,020 --> 00:27:58,770
parker run so i discovered or rediscover

00:27:56,160 --> 00:28:01,020
the number of things so the key thing is

00:27:58,770 --> 00:28:03,600
to identify and to be able to define

00:28:01,020 --> 00:28:07,080
realistic scenarios and data sets and

00:28:03,600 --> 00:28:10,650
this is very complex I've been reported

00:28:07,080 --> 00:28:15,300
that in in many benchmarking campaigns

00:28:10,650 --> 00:28:19,440
it's the more costly step is defining

00:28:15,300 --> 00:28:21,570
scenarios so we had to simplify we have

00:28:19,440 --> 00:28:24,450
to cope with it and to be pragmatic we

00:28:21,570 --> 00:28:28,260
did not have also a lot of resources and

00:28:24,450 --> 00:28:31,879
a lot of time we had to go quick so

00:28:28,260 --> 00:28:34,089
beware very important beware of the

00:28:31,879 --> 00:28:37,940
defaults at the load injection side

00:28:34,089 --> 00:28:40,849
before before saying that your system

00:28:37,940 --> 00:28:44,149
under test is not performing okay check

00:28:40,849 --> 00:28:47,269
first your rod injection system and used

00:28:44,149 --> 00:28:49,729
complementary Mon territory monitoring

00:28:47,269 --> 00:28:52,759
tools to troubleshoot when you see that

00:28:49,729 --> 00:28:56,449
performance are not all right and it may

00:28:52,759 --> 00:28:58,159
be a network issue or something else not

00:28:56,449 --> 00:29:00,949
necessary your system under test and

00:28:58,159 --> 00:29:03,169
when it's your system under test also by

00:29:00,949 --> 00:29:07,989
using monitoring tools you're going to

00:29:03,169 --> 00:29:12,829
identify what are the resources that are

00:29:07,989 --> 00:29:15,079
that our fault we experienced it the I

00:29:12,829 --> 00:29:17,329
efficiency of cliff and Jenkins we did

00:29:15,079 --> 00:29:19,639
really a lot of tests brands changing

00:29:17,329 --> 00:29:22,029
one parameter and running a new test and

00:29:19,639 --> 00:29:24,709
getting automatically the new

00:29:22,029 --> 00:29:27,679
performance report so most of the time

00:29:24,709 --> 00:29:31,099
here we did very short a few minutes

00:29:27,679 --> 00:29:33,649
short test just a few minutes so we

00:29:31,099 --> 00:29:35,839
could have a very quick feedback and say

00:29:33,649 --> 00:29:41,059
okay this change was good or is it was

00:29:35,839 --> 00:29:43,219
not good of course it was a very very

00:29:41,059 --> 00:29:46,789
easy with cliff to mix all these

00:29:43,219 --> 00:29:49,699
different protocols it's very easy with

00:29:46,789 --> 00:29:52,669
cliff to do that and to reuse scenarios

00:29:49,699 --> 00:29:56,599
in different test plans with different

00:29:52,669 --> 00:29:58,819
objectives okay and then it was

00:29:56,599 --> 00:30:01,089
definitely a good idea to benchmark we

00:29:58,819 --> 00:30:04,039
have no doubt today that without this

00:30:01,089 --> 00:30:06,979
benchmarking campaign and the tuning we

00:30:04,039 --> 00:30:11,629
made this is a service would have

00:30:06,979 --> 00:30:14,209
crushed it's clear so we avoided big

00:30:11,629 --> 00:30:16,339
troubles and now while we can benefit

00:30:14,209 --> 00:30:19,729
from the work we did here with the

00:30:16,339 --> 00:30:24,789
scenarios 22 now to monitor the quality

00:30:19,729 --> 00:30:24,789

YouTube URL: https://www.youtube.com/watch?v=5uh1Ff8tcXE


