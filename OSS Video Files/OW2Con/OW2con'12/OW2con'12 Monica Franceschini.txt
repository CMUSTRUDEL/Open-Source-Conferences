Title: OW2con'12 Monica Franceschini
Publication date: 2012-12-10
Playlist: OW2con'12
Description: 
	SpagoBI and Big Data: next Open Source Information Management suite - Monica Franceschini, Engineering
AbstractÂ : Organizations adopt Business Intelligence tools to analyze tons of data: nonetheless, several business leaders do not dispose of the information they actually need. This happens because the information management scenario is evolving. Various new contents are adding to structured information, supported by already known processes, tools and practices, including information coming from social computing. They will be managed by disparate processes, fragmented tools, new practices. This information will combine with various contents of enterprise systems: documents, transactional data, databases and data warehouses, images, audio, texts, videos. This huge amount of contents is named "big data", even though it is not just related to a big amount of data. It refers to the capability of managing data that are growing along three dimensions - volume, velocity and variety - respecting the simplicity of the user interface. The speech describes SpagoBI approach to the "big data" scenario and presents SpagoBI suite roadmap, which is two-fold. It aims to address existing emerging analytical areas and domains, providing the suite with new capabilities - including big data and open data support, in-memory analysis, real time and mobile BI - and following a research path towards the realization of a new generation of SpagoBI suite.
Captions: 
	00:00:00,000 --> 00:00:04,410
good morning everybody my name is Monica

00:00:02,250 --> 00:00:06,450
francescani and Paco bi solution

00:00:04,410 --> 00:00:08,849
architecture for this power bi

00:00:06,450 --> 00:00:12,630
competency center of the engineering

00:00:08,849 --> 00:00:17,250
group and I'm going to introduce you to

00:00:12,630 --> 00:00:20,670
first of all to Spago bi as a new manage

00:00:17,250 --> 00:00:23,430
information management suite for Spock

00:00:20,670 --> 00:00:26,400
OPI towards big data so first of all we

00:00:23,430 --> 00:00:30,060
take a look at Spago bi nowadays Spock

00:00:26,400 --> 00:00:32,130
OPI is in a hundred percent open-source

00:00:30,060 --> 00:00:34,320
a solution that will stay open source

00:00:32,130 --> 00:00:38,430
forever with no vendor lock-in and

00:00:34,320 --> 00:00:41,430
software looking then it is an in a

00:00:38,430 --> 00:00:44,670
comprehensive suite that covers all the

00:00:41,430 --> 00:00:48,480
BI areas through different engines and

00:00:44,670 --> 00:00:52,160
many kinds of documents it offers it

00:00:48,480 --> 00:00:57,030
provides unique solutions for mobile BI

00:00:52,160 --> 00:01:02,329
adopt reporting real-time behind and

00:00:57,030 --> 00:01:06,930
location intelligence and we also supply

00:01:02,329 --> 00:01:12,750
lots of support services such as pace as

00:01:06,930 --> 00:01:14,850
you go support services so Jim before me

00:01:12,750 --> 00:01:19,979
from Holdren works already told you

00:01:14,850 --> 00:01:25,080
about the 3v or that well defined and

00:01:19,979 --> 00:01:28,979
the concept of big data we just add some

00:01:25,080 --> 00:01:31,170
more v which is the value of e because

00:01:28,979 --> 00:01:36,900
from our point of view is important to

00:01:31,170 --> 00:01:42,420
extract value from the informations so

00:01:36,900 --> 00:01:45,450
let's focus on four important concept of

00:01:42,420 --> 00:01:49,530
Spock OPI we focus on Spock OPI server

00:01:45,450 --> 00:01:52,409
which manages data source as the concept

00:01:49,530 --> 00:01:55,229
of the repositories the data storage is

00:01:52,409 --> 00:01:59,250
which typically are for instance that

00:01:55,229 --> 00:02:02,939
our houses and relational databases over

00:01:59,250 --> 00:02:06,360
which we make our analysis over the data

00:02:02,939 --> 00:02:09,580
where data sources we create data sets

00:02:06,360 --> 00:02:12,340
data sets is the concept and

00:02:09,580 --> 00:02:14,830
of the query that interrogate these data

00:02:12,340 --> 00:02:18,430
source to provide the results needed for

00:02:14,830 --> 00:02:20,890
the documents each data set can be used

00:02:18,430 --> 00:02:22,930
by different tangents because we provide

00:02:20,890 --> 00:02:25,900
as I said before a different kind of

00:02:22,930 --> 00:02:28,600
engines and each engine can use the same

00:02:25,900 --> 00:02:34,020
data set so the same query to produce

00:02:28,600 --> 00:02:37,750
different kind of analytics so nowadays

00:02:34,020 --> 00:02:40,570
we can approach we can use the atl

00:02:37,750 --> 00:02:44,260
approach through talent which offers us

00:02:40,570 --> 00:02:47,740
the capability to attract data from Big

00:02:44,260 --> 00:02:51,400
Data storages such as a dupe and others

00:02:47,740 --> 00:02:56,560
through its connectors and then it's

00:02:51,400 --> 00:03:00,970
able to define the way to well extract

00:02:56,560 --> 00:03:03,760
data and manage the informations to put

00:03:00,970 --> 00:03:05,850
them into to load them into a typical

00:03:03,760 --> 00:03:10,060
data warehouse that we use nowadays

00:03:05,850 --> 00:03:13,840
where's Paco bi that Spock OPI uses as a

00:03:10,060 --> 00:03:16,060
data source and with the data sets

00:03:13,840 --> 00:03:19,690
Aquarius this data source which is a

00:03:16,060 --> 00:03:22,840
typical data source and add some more

00:03:19,690 --> 00:03:25,269
information true for instance they

00:03:22,840 --> 00:03:27,610
behavioral model which enables the

00:03:25,269 --> 00:03:29,709
documents to have to behave in a

00:03:27,610 --> 00:03:33,489
different way depending on the user who

00:03:29,709 --> 00:03:35,709
is executing the document and also some

00:03:33,489 --> 00:03:38,739
cross services for documents

00:03:35,709 --> 00:03:41,170
interactions for instance then of course

00:03:38,739 --> 00:03:46,360
spec OPI produces the documents

00:03:41,170 --> 00:03:49,300
depending on the engine chosen so now we

00:03:46,360 --> 00:03:53,160
think about enlarging and reaching the

00:03:49,300 --> 00:03:55,900
concept of data source we think about

00:03:53,160 --> 00:03:59,170
analytical DBMS for instance such as

00:03:55,900 --> 00:04:02,950
torrid eternities info bright and so on

00:03:59,170 --> 00:04:05,830
vector wise also which are of course

00:04:02,950 --> 00:04:09,280
sequel databases for distributed

00:04:05,830 --> 00:04:11,980
platforms but we can also think about no

00:04:09,280 --> 00:04:15,660
sequel databases such as MongoDB and

00:04:11,980 --> 00:04:17,880
Cassandra to store unstructured data and

00:04:15,660 --> 00:04:21,690
we want to use them as that

00:04:17,880 --> 00:04:25,080
sources and then all the typical system

00:04:21,690 --> 00:04:28,770
so for instance I will think about javi

00:04:25,080 --> 00:04:33,090
first of all and HBase often works and

00:04:28,770 --> 00:04:36,480
so on and so the the data set becomes

00:04:33,090 --> 00:04:40,380
just a plugin for Spock OPI towards this

00:04:36,480 --> 00:04:43,770
kind of new data storages and then Spock

00:04:40,380 --> 00:04:47,280
OPI is able to execute the document as

00:04:43,770 --> 00:04:49,740
usual so applying its addiction on

00:04:47,280 --> 00:04:51,390
information such as the behavioral model

00:04:49,740 --> 00:04:56,550
and across every season as I said before

00:04:51,390 --> 00:04:59,280
to produce different kind of analysis so

00:04:56,550 --> 00:05:02,310
the first scenario that we we approach

00:04:59,280 --> 00:05:06,090
is the scenario of the not real time

00:05:02,310 --> 00:05:08,160
analytics that's why because we because

00:05:06,090 --> 00:05:12,360
we think about batch processing data

00:05:08,160 --> 00:05:14,640
coming from for instance as i said

00:05:12,360 --> 00:05:19,020
before batch processing logging and so

00:05:14,640 --> 00:05:23,030
on which are stored in repositories for

00:05:19,020 --> 00:05:25,710
big data such as a dupe and all the

00:05:23,030 --> 00:05:29,330
project embedded in a dupes for instance

00:05:25,710 --> 00:05:32,490
we think about each base or high V and

00:05:29,330 --> 00:05:36,360
so the concept of data source get

00:05:32,490 --> 00:05:39,510
enlarged through this concept of adding

00:05:36,360 --> 00:05:44,130
HBase and high with at the first step

00:05:39,510 --> 00:05:46,800
and the data set will be executed with

00:05:44,130 --> 00:05:50,910
the high v ql language query language

00:05:46,800 --> 00:05:53,730
that will provide as the informations

00:05:50,910 --> 00:05:56,000
from these data tools data sources but

00:05:53,730 --> 00:05:59,790
we can also could also think about

00:05:56,000 --> 00:06:03,330
directly connected into HD HDFS

00:05:59,790 --> 00:06:07,280
filesystem and use pig to interrogate

00:06:03,330 --> 00:06:09,810
this this data source of course the

00:06:07,280 --> 00:06:13,170
documents will want to be a real time

00:06:09,810 --> 00:06:17,340
because of the high latency taken to

00:06:13,170 --> 00:06:20,010
produce the results so we go to the

00:06:17,340 --> 00:06:23,880
second scenario which involves the

00:06:20,010 --> 00:06:25,480
complex event processing engine so we

00:06:23,880 --> 00:06:29,410
think about

00:06:25,480 --> 00:06:32,170
data a flies past so that offline data

00:06:29,410 --> 00:06:34,770
continuously running and the use case is

00:06:32,170 --> 00:06:38,560
typically for instance the Twitter or

00:06:34,770 --> 00:06:43,330
the mobile devices data provided from

00:06:38,560 --> 00:06:46,780
these engines and we think about a data

00:06:43,330 --> 00:06:51,220
source for instance for instance storm

00:06:46,780 --> 00:06:54,130
which is able to collect data and behave

00:06:51,220 --> 00:06:57,010
like a data stores and the kind of data

00:06:54,130 --> 00:07:00,310
set we think of is for instance Esper

00:06:57,010 --> 00:07:03,150
which is the which offers an a query

00:07:00,310 --> 00:07:06,670
language which is the EPL query language

00:07:03,150 --> 00:07:10,270
well the this produces a real-time

00:07:06,670 --> 00:07:12,780
document which is developed through the

00:07:10,270 --> 00:07:15,700
console document which we already

00:07:12,780 --> 00:07:19,480
developed for another project which was

00:07:15,700 --> 00:07:23,620
in an eclipse project called a bomb

00:07:19,480 --> 00:07:29,050
which was made for monitoring Virginia's

00:07:23,620 --> 00:07:32,860
data and services so this is the second

00:07:29,050 --> 00:07:35,890
scenario and the third scenario is much

00:07:32,860 --> 00:07:38,500
more kind of research scenario because

00:07:35,890 --> 00:07:43,360
we think about linked data because

00:07:38,500 --> 00:07:46,510
relationship matters and moreover in the

00:07:43,360 --> 00:07:50,440
case of B data of many many data it's

00:07:46,510 --> 00:07:52,840
important to and unstructured data it's

00:07:50,440 --> 00:07:56,890
important to give annotations for this

00:07:52,840 --> 00:08:00,190
data to understand them better and so we

00:07:56,890 --> 00:08:03,700
think about the concept of annotation

00:08:00,190 --> 00:08:07,000
and semantic over this data of course

00:08:03,700 --> 00:08:09,180
this is not only available for big data

00:08:07,000 --> 00:08:12,280
it could also be a smaller data or

00:08:09,180 --> 00:08:15,130
normal amount of data because we are

00:08:12,280 --> 00:08:18,180
talking about link and data so the data

00:08:15,130 --> 00:08:22,330
source in this case could be a database

00:08:18,180 --> 00:08:25,270
capable of collecting RDF files RDF

00:08:22,330 --> 00:08:28,300
informations such as for instance big

00:08:25,270 --> 00:08:31,600
data are or instance we could think

00:08:28,300 --> 00:08:34,660
about storing vs suffice these

00:08:31,600 --> 00:08:36,219
informations on HBase and then with the

00:08:34,660 --> 00:08:41,289
proper parody

00:08:36,219 --> 00:08:45,360
keep the results with pig and then also

00:08:41,289 --> 00:08:48,189
add a sparql which which is the w3c

00:08:45,360 --> 00:08:51,670
format standard format for the query

00:08:48,189 --> 00:08:57,970
over the annotation already f of files

00:08:51,670 --> 00:09:01,569
and so this is a much more complex

00:08:57,970 --> 00:09:03,519
scenario which involves real time or not

00:09:01,569 --> 00:09:06,879
real time documents depending of course

00:09:03,519 --> 00:09:09,790
on where the data is stored because it

00:09:06,879 --> 00:09:13,089
could also happen okay well I almost

00:09:09,790 --> 00:09:15,959
finished it could also happen that this

00:09:13,089 --> 00:09:18,970
data and already annotated come from

00:09:15,959 --> 00:09:22,110
streams of bytes so for instance the

00:09:18,970 --> 00:09:27,100
data source could also be storm in our

00:09:22,110 --> 00:09:30,810
concept and so this is the third and

00:09:27,100 --> 00:09:34,240
last scenario this is the comprehensive

00:09:30,810 --> 00:09:36,250
view of all the scenarios we are go we

00:09:34,240 --> 00:09:39,939
are approaching already we already

00:09:36,250 --> 00:09:42,699
implementing them and so I finished a

00:09:39,939 --> 00:09:44,740
little bit faster so there is time to

00:09:42,699 --> 00:09:53,470
questions and I'm glad to answer if you

00:09:44,740 --> 00:10:03,519
have one thank you thank you you have

00:09:53,470 --> 00:10:07,600
any questions for Monica Monica you

00:10:03,519 --> 00:10:12,819
talked about innovation from the

00:10:07,600 --> 00:10:15,819
technology point of view and the so you

00:10:12,819 --> 00:10:20,800
underlined the value of using open

00:10:15,819 --> 00:10:24,910
source solutions to be innovative can

00:10:20,800 --> 00:10:28,680
you also give some examples of use case

00:10:24,910 --> 00:10:32,620
scenarios where these concepts may be

00:10:28,680 --> 00:10:39,250
adopted and possibly use a scenario

00:10:32,620 --> 00:10:43,180
which are ready to come maybe on the

00:10:39,250 --> 00:10:45,970
open data or in the telecommunication

00:10:43,180 --> 00:10:49,089
and yeah of course well the second

00:10:45,970 --> 00:10:53,920
scenario is so so the streaming

00:10:49,089 --> 00:10:58,920
the bite streaming is a more is more a

00:10:53,920 --> 00:11:04,120
dock for telecom telecommunication

00:10:58,920 --> 00:11:09,129
problems or wants to catch the data when

00:11:04,120 --> 00:11:14,649
it flies so the the meaning is when it

00:11:09,129 --> 00:11:16,769
is put when you analyze the data so it's

00:11:14,649 --> 00:11:20,620
important to catch them in that moment

00:11:16,769 --> 00:11:25,209
specifically otherwise well of course we

00:11:20,620 --> 00:11:27,850
have other kind of use cases for

00:11:25,209 --> 00:11:30,100
instance the semantic use case is very

00:11:27,850 --> 00:11:32,529
popular and the research at the

00:11:30,100 --> 00:11:37,290
University there are many studies about

00:11:32,529 --> 00:11:40,959
it and it would be a really innovative

00:11:37,290 --> 00:11:45,009
approach for big data to produce also

00:11:40,959 --> 00:11:47,620
more complex query over the big data so

00:11:45,009 --> 00:11:50,189
adding information there there are many

00:11:47,620 --> 00:11:53,110
providers who wants to have information

00:11:50,189 --> 00:11:55,779
typically in the open data scenario and

00:11:53,110 --> 00:11:58,720
in Italy we have some examples and then

00:11:55,779 --> 00:12:02,589
it's important for us to use these

00:11:58,720 --> 00:12:05,319
informations so to try to analyze data

00:12:02,589 --> 00:12:09,129
in a better way with more significance

00:12:05,319 --> 00:12:12,329
and so this is the use case I think you

00:12:09,129 --> 00:12:12,329

YouTube URL: https://www.youtube.com/watch?v=m14uTgE6Y5A


