Title: Scaling TB's of data with Apache Spark & Scala DSL at Production- Chetan Khatri-FOSSASIA 2018
Publication date: 2018-03-24
Playlist: FOSSASIA Summit 2018
Description: 
	Speaker: Chetan Khatri
Info: https://2018.fossasia.org/event/speakers.html#chetankumar-khatri3230

Apache Spark is one of the top big-data processing platforms and has driven the adoption of Scala in many industry and academic settings. As entire Apache Spark framework has been written in scala as a base it’s real pleasure to understand beauty of functional Scala DSL with Spark.

This talk is intent to present :

Primary data structures (RDD DataSet Dataframe) usage in universal large scale data processing with Hbase (Data lake) Hive (Analytical Engine).

Case study: He will go through importance of physical data split up techniques such as coalesce Partition Repartition and other important spark internals in Scaling TB’s of data / ~17 billions records

Also the talk gives understanding about crucial part and very interesting way of understanding parallel &amp concurrent distributed data processing – tuning memory cache Disk I/O Leaking memory Internal shuffle spark executor spark driver etc.

Room: Training room 2-1
Track: Database
Date: Saturday, 24th March, 2018

Event Page: http://2018.fossasia.org
Follow FOSSASIA on Twitter: https://twitter.com/fossasia/
Like FOSSASIA on Facebook: https://www.facebook.com/fossasia/ 

Produced by Engineers.SG
Captions: 
	00:00:07,130 --> 00:00:14,549
hi everyone Japan country from India

00:00:12,090 --> 00:00:16,529
vanilla I'm speaking on scaling the

00:00:14,549 --> 00:00:23,579
piece of data with apache spark and

00:00:16,529 --> 00:00:27,259
Scala TSL high production so how many of

00:00:23,579 --> 00:00:34,350
for you knows about a spark

00:00:27,259 --> 00:00:36,270
Scala so Who am I I am lead of data

00:00:34,350 --> 00:00:39,480
science big data and technology

00:00:36,270 --> 00:00:41,870
evangelist at Exxon labs India and I'm

00:00:39,480 --> 00:00:44,820
committed of Apache spark HBase and

00:00:41,870 --> 00:00:49,050
Alexia language I co-authored the

00:00:44,820 --> 00:00:52,740
curricula in coach university for IOT

00:00:49,050 --> 00:00:59,790
Big Data ml in EA and previously I

00:00:52,740 --> 00:01:01,410
worked with XLR and Nizar games so today

00:00:59,790 --> 00:01:04,650
I will speak about about the spark and

00:01:01,410 --> 00:01:05,150
Scala will talk about rdd's and Twitter

00:01:04,650 --> 00:01:08,970
frames

00:01:05,150 --> 00:01:11,760
thetacd api's and spark operation data

00:01:08,970 --> 00:01:14,760
platform components and the engineering

00:01:11,760 --> 00:01:17,880
data processing platform with the case

00:01:14,760 --> 00:01:19,650
study will rethink about phosphate

00:01:17,880 --> 00:01:24,570
architecture what are the component and

00:01:19,650 --> 00:01:26,280
how you paint in this path jobs and we

00:01:24,570 --> 00:01:30,479
talk about very small and concurrency

00:01:26,280 --> 00:01:32,700
with spark for those who don't know what

00:01:30,479 --> 00:01:34,920
is parked so spark is a faster

00:01:32,700 --> 00:01:38,729
general-purpose clustering computing

00:01:34,920 --> 00:01:41,760
system and unified engine for processing

00:01:38,729 --> 00:01:45,630
the data it provides highly on API for

00:01:41,760 --> 00:01:49,170
Scala Java Python and it suppose the

00:01:45,630 --> 00:01:51,479
general execution graph it has the

00:01:49,170 --> 00:01:54,600
different component and framework for

00:01:51,479 --> 00:01:58,470
different data processing you can use

00:01:54,600 --> 00:02:01,049
the structured data for spark SQL

00:01:58,470 --> 00:02:04,290
machine learning for MLA and you are

00:02:01,049 --> 00:02:07,340
crossing the offlane they have streaming

00:02:04,290 --> 00:02:09,780
spark swimming a structure streaming

00:02:07,340 --> 00:02:11,580
those who don't know what is Scala

00:02:09,780 --> 00:02:14,489
so Scala is a functional programming

00:02:11,580 --> 00:02:16,620
language it is also support the

00:02:14,489 --> 00:02:18,100
functional paradigm plus is suppose the

00:02:16,620 --> 00:02:21,070
object oriented

00:02:18,100 --> 00:02:24,970
a strongly type and type inference and

00:02:21,070 --> 00:02:29,020
it supports higher-order functions it

00:02:24,970 --> 00:02:34,660
has the power of the lazy computational

00:02:29,020 --> 00:02:38,350
framework data structure in hepatis Park

00:02:34,660 --> 00:02:43,990
you will have RDD data frame and data

00:02:38,350 --> 00:02:47,110
set from spark 2 dot X and 2.3 so ugly

00:02:43,990 --> 00:02:52,390
is the EPS trick data structure hepatis

00:02:47,110 --> 00:02:54,310
Park which if you have the you know site

00:02:52,390 --> 00:02:55,420
and when you distribute to the entire

00:02:54,310 --> 00:02:58,480
Hadoop cluster

00:02:55,420 --> 00:03:00,040
it will parties on and it will be

00:02:58,480 --> 00:03:03,760
distributed and suffer through different

00:03:00,040 --> 00:03:06,130
executors and the course so when I talk

00:03:03,760 --> 00:03:09,280
about executors it is the different node

00:03:06,130 --> 00:03:12,130
and when I talk about guava the West

00:03:09,280 --> 00:03:15,910
path context start and the driver beach

00:03:12,130 --> 00:03:19,600
which is the master node and worker as

00:03:15,910 --> 00:03:21,700
we exhibit or nodes so our daily will

00:03:19,600 --> 00:03:26,080
get split ale and distributed across the

00:03:21,700 --> 00:03:30,519
different nodes and you will have a

00:03:26,080 --> 00:03:35,260
distributed file system like NTFS or SC

00:03:30,519 --> 00:03:38,470
bucket hardness characteristics is a

00:03:35,260 --> 00:03:42,100
immutable and resilient when you talk

00:03:38,470 --> 00:03:44,170
about resilience means when inspark

00:03:42,100 --> 00:03:46,810
supposed to thing operations one is

00:03:44,170 --> 00:03:51,430
trust my son other is exon so when you

00:03:46,810 --> 00:03:53,430
have the oddity and one oddity you apply

00:03:51,430 --> 00:03:56,019
the transformation you again get another

00:03:53,430 --> 00:03:58,239
oddity again you apply transformation

00:03:56,019 --> 00:04:00,940
assume that something went wrong and

00:03:58,239 --> 00:04:03,700
your transmission got failed you have

00:04:00,940 --> 00:04:06,700
ability to recreate the oddity that you

00:04:03,700 --> 00:04:08,530
had earlier and they always build RDP

00:04:06,700 --> 00:04:14,650
will be given table without changing and

00:04:08,530 --> 00:04:17,560
providing any first my son oddity is a

00:04:14,650 --> 00:04:21,360
compile type safe and strongly type

00:04:17,560 --> 00:04:23,620
inference so in oddity you will have the

00:04:21,360 --> 00:04:26,440
type inference so when you create the

00:04:23,620 --> 00:04:29,729
function function will take argument as

00:04:26,440 --> 00:04:32,189
a function all the value so when one

00:04:29,729 --> 00:04:35,039
function is taking the argument as an

00:04:32,189 --> 00:04:37,770
integer and if you pass the string it

00:04:35,039 --> 00:04:39,930
will get on the spot so what happens is

00:04:37,770 --> 00:04:42,960
on time and compile time errors if you

00:04:39,930 --> 00:04:44,580
have like 17 years later you create the

00:04:42,960 --> 00:04:46,620
sponsor and then you are exhibiting

00:04:44,580 --> 00:04:48,719
spouse job distributed popular thing as

00:04:46,620 --> 00:04:51,360
you know debugging in a distributed

00:04:48,719 --> 00:04:53,550
computing is a cumbersome okay if we

00:04:51,360 --> 00:04:55,650
come to your workload the extend us and

00:04:53,550 --> 00:04:57,960
after penis if you get to know this at

00:04:55,650 --> 00:05:01,620
the time is you and that's why your

00:04:57,960 --> 00:05:03,330
sponge of God fail so ideally provides

00:05:01,620 --> 00:05:09,899
the type safety and strong type

00:05:03,330 --> 00:05:13,259
inference lazy evaluation so here lazy

00:05:09,899 --> 00:05:17,039
doesn't mean the way human being is lazy

00:05:13,259 --> 00:05:21,180
but it's not so forth but it provides

00:05:17,039 --> 00:05:23,069
the so when you when you played the

00:05:21,180 --> 00:05:27,569
oddity and provide the transformation

00:05:23,069 --> 00:05:29,909
like map or filter or flood map it

00:05:27,569 --> 00:05:34,229
peeled the lineage I mean that the

00:05:29,909 --> 00:05:38,550
Pacific graph in in the system and until

00:05:34,229 --> 00:05:42,810
you apply the Exxon like reduce by key

00:05:38,550 --> 00:05:46,020
or so or tab before that it will not

00:05:42,810 --> 00:05:47,580
execute entire graph so you apply the

00:05:46,020 --> 00:05:49,620
every time transformation if you have

00:05:47,580 --> 00:05:52,620
great day directed acyclic graph and

00:05:49,620 --> 00:05:58,830
when you apply the exa from that time it

00:05:52,620 --> 00:06:00,389
will execute your entire graph so spark

00:05:58,830 --> 00:06:06,360
has a two type of operation one is

00:06:00,389 --> 00:06:08,520
transformation other is exon see this is

00:06:06,360 --> 00:06:10,800
all about spark operations and a

00:06:08,520 --> 00:06:14,789
transformation and the exons is support

00:06:10,800 --> 00:06:18,089
these transformations like a filter site

00:06:14,789 --> 00:06:21,360
map map artisans and mathematical like

00:06:18,089 --> 00:06:25,099
sample speak of the random set theory

00:06:21,360 --> 00:06:28,349
like Union intersection subject and

00:06:25,099 --> 00:06:30,419
listing Cardassian ship data structuring

00:06:28,349 --> 00:06:33,449
with how the Paulson and koalas will

00:06:30,419 --> 00:06:37,620
talk about fine detail excellency

00:06:33,449 --> 00:06:40,219
suppose all the county news collect and

00:06:37,620 --> 00:06:40,219
save as file

00:06:41,219 --> 00:06:47,199
so we'll talk about when to use our

00:06:43,659 --> 00:06:51,849
Denis when you when you don't feel about

00:06:47,199 --> 00:06:54,279
lots of lambda function then DSL that

00:06:51,849 --> 00:06:56,050
doesn't mean you don't care that which

00:06:54,279 --> 00:06:59,110
lambda function you need to apply on

00:06:56,050 --> 00:07:02,289
this data set or you don't care about

00:06:59,110 --> 00:07:05,020
controller data set you don't feel the

00:07:02,289 --> 00:07:07,300
flexibility on the data set at the time

00:07:05,020 --> 00:07:08,889
you can use the RTD or you don't care

00:07:07,300 --> 00:07:10,779
about the schema of structure of the

00:07:08,889 --> 00:07:12,969
data so when you have a data set you

00:07:10,779 --> 00:07:14,589
will have the structure of the data or

00:07:12,969 --> 00:07:17,020
you don't care about optimizes and

00:07:14,589 --> 00:07:19,389
performance so if you think is that I

00:07:17,020 --> 00:07:22,779
really can create it's a very slow on

00:07:19,389 --> 00:07:25,629
non jvm language like python ah-ah

00:07:22,779 --> 00:07:28,120
so what happens about this spa has been

00:07:25,629 --> 00:07:30,580
created with these Scala and when you

00:07:28,120 --> 00:07:35,169
suppose the different aps with spa

00:07:30,580 --> 00:07:37,930
Python so when you perform any action on

00:07:35,169 --> 00:07:40,089
transformation on spark it will pick

00:07:37,930 --> 00:07:42,669
color and typical and sent back to the

00:07:40,089 --> 00:07:45,969
JVM connections and then it will get

00:07:42,669 --> 00:07:48,189
same operation again so when you don't

00:07:45,969 --> 00:07:51,789
care about slow performance you use the

00:07:48,189 --> 00:07:54,669
oddity or don't care about in it one in

00:07:51,789 --> 00:08:00,099
efficiencies that does mean maybe you

00:07:54,669 --> 00:08:02,439
think unwittingly unknowingly that you

00:08:00,099 --> 00:08:07,419
don't want to apply the transformation

00:08:02,439 --> 00:08:11,589
for example you can see this basic photo

00:08:07,419 --> 00:08:15,999
code here if you can see the third line

00:08:11,589 --> 00:08:19,240
reduced by key and the filter so ideally

00:08:15,999 --> 00:08:22,029
this wrong here because first you apply

00:08:19,240 --> 00:08:24,849
this filter as low as you get data to

00:08:22,029 --> 00:08:27,789
this path you spiky will do the massive

00:08:24,849 --> 00:08:30,699
suffering on it a cluster and that will

00:08:27,789 --> 00:08:34,389
provide efficiencies and not optimized

00:08:30,699 --> 00:08:36,339
yes son so actually you should give us

00:08:34,389 --> 00:08:38,589
the line number four and then line

00:08:36,339 --> 00:08:41,199
number three so what will happen you

00:08:38,589 --> 00:08:43,990
applied the filter first and then you

00:08:41,199 --> 00:08:45,760
apply the your exon so what will happen

00:08:43,990 --> 00:08:48,010
you you are reading the lace data on

00:08:45,760 --> 00:08:49,490
your cluster and then you are applying

00:08:48,010 --> 00:08:53,660
the

00:08:49,490 --> 00:08:56,320
excellent on top of that so then we talk

00:08:53,660 --> 00:09:00,320
about structured in spa so what's

00:08:56,320 --> 00:09:03,950
structured ApS pathless provide so spot

00:09:00,320 --> 00:09:08,930
provides you the data frame lettuce ATP

00:09:03,950 --> 00:09:12,560
ice so while we use the data set because

00:09:08,930 --> 00:09:16,160
data set is a strongly typing ability to

00:09:12,560 --> 00:09:19,460
use powerful Lemlich function so when I

00:09:16,160 --> 00:09:22,070
talk about powerful limited function

00:09:19,460 --> 00:09:25,130
that does mean anonymous function that

00:09:22,070 --> 00:09:30,290
text argument as a function returns you

00:09:25,130 --> 00:09:32,360
this function okay and spot SQL

00:09:30,290 --> 00:09:34,670
optimized with execution engine like

00:09:32,360 --> 00:09:36,650
catalyst and tungsten so what had

00:09:34,670 --> 00:09:41,089
happened that when you perform so spark

00:09:36,650 --> 00:09:43,460
SQL and spark data frame is a more equal

00:09:41,089 --> 00:09:47,210
in terms of faster performance the

00:09:43,460 --> 00:09:50,540
performance pianistic wa but the thing

00:09:47,210 --> 00:09:53,870
is like that when you provide the SQL as

00:09:50,540 --> 00:09:57,050
a string to the spot node SQL it will

00:09:53,870 --> 00:09:59,120
execute but if you have the like type

00:09:57,050 --> 00:10:03,110
product even if they select you house

00:09:59,120 --> 00:10:05,120
SWE any team and they temp this type O

00:10:03,110 --> 00:10:07,910
button this would be execute and it

00:10:05,120 --> 00:10:10,100
provides the error one time so you will

00:10:07,910 --> 00:10:13,070
not be able to catch that error as a

00:10:10,100 --> 00:10:15,560
compile time so it is very cumbersome

00:10:13,070 --> 00:10:18,410
when you run the workload of like 10 G

00:10:15,560 --> 00:10:20,990
20 V and you execute this and went home

00:10:18,410 --> 00:10:22,670
and when you come back you see your

00:10:20,990 --> 00:10:26,810
failed because of this typo

00:10:22,670 --> 00:10:30,760
so data set is a strongly telling it

00:10:26,810 --> 00:10:33,770
will throw it in the air on compile time

00:10:30,760 --> 00:10:36,650
the dissent can be constructed from JVM

00:10:33,770 --> 00:10:38,150
object and it used the functional

00:10:36,650 --> 00:10:40,820
transformation like map filter and

00:10:38,150 --> 00:10:43,430
thread map or data frame is a data set

00:10:40,820 --> 00:10:46,610
organized into lambda columns so data

00:10:43,430 --> 00:10:49,040
set in a frame is the areas of data set

00:10:46,610 --> 00:10:52,120
of row as always the JVM object that has

00:10:49,040 --> 00:10:52,120
the columns

00:10:52,440 --> 00:10:59,770
so we'll compare that when you when you

00:10:57,280 --> 00:11:03,400
have SQL sparks equal when you have the

00:10:59,770 --> 00:11:06,820
data frame in data sets and how it how

00:11:03,400 --> 00:11:09,610
it compared each other so so when use

00:11:06,820 --> 00:11:10,060
the sparks equal compile-time syntax

00:11:09,610 --> 00:11:12,040
error

00:11:10,060 --> 00:11:14,470
I mean syntax error will be done then

00:11:12,040 --> 00:11:17,260
also analysis error will be done time

00:11:14,470 --> 00:11:19,420
later frame provides the syntax of the

00:11:17,260 --> 00:11:21,210
compile time but analysis error or a

00:11:19,420 --> 00:11:24,700
long time if I'm going to talk about

00:11:21,210 --> 00:11:27,670
syntax error so and the analysis error

00:11:24,700 --> 00:11:30,160
you are acquiring some table which

00:11:27,670 --> 00:11:32,410
abolished does not exist you are trying

00:11:30,160 --> 00:11:35,680
to query the view which is not that in a

00:11:32,410 --> 00:11:37,120
spark context or you are trying to erase

00:11:35,680 --> 00:11:39,010
some of the column so applying some of

00:11:37,120 --> 00:11:42,730
the transformation on top of the some of

00:11:39,010 --> 00:11:46,030
the columns which is not there so data

00:11:42,730 --> 00:11:47,350
sets your syntax error and your analysis

00:11:46,030 --> 00:11:49,480
error will be compiled down

00:11:47,350 --> 00:11:51,970
so think if you're coding in a spark job

00:11:49,480 --> 00:11:54,490
if it provides a compile time in the

00:11:51,970 --> 00:11:56,830
mass index and combine em you you know

00:11:54,490 --> 00:12:00,730
what's happening right you don't have to

00:11:56,830 --> 00:12:02,530
wait and execute the job so analysis

00:12:00,730 --> 00:12:04,210
error our code we for jumble and so on

00:12:02,530 --> 00:12:07,290
the cluster so it saves the massive

00:12:04,210 --> 00:12:07,290
amount of time for you

00:12:08,920 --> 00:12:15,250
so when they talk about data frame and

00:12:11,839 --> 00:12:20,449
intercept from 2016 in spark to dot o

00:12:15,250 --> 00:12:22,850
you will have you will have to think

00:12:20,449 --> 00:12:25,009
only type and type api's if you use a

00:12:22,850 --> 00:12:27,529
data frame is the alias of the data set

00:12:25,009 --> 00:12:34,250
of row we Sun type if you use the typed

00:12:27,529 --> 00:12:36,949
API it's a data set of the any genre so

00:12:34,250 --> 00:12:39,680
think in this example we'll have a data

00:12:36,949 --> 00:12:43,690
frame API code so this is same as we

00:12:39,680 --> 00:12:46,699
used with the oddity so we have a fast

00:12:43,690 --> 00:12:49,370
data frame which is lit up

00:12:46,699 --> 00:12:52,790
I mean RDD is the unstructured and if

00:12:49,370 --> 00:12:55,610
you want to make it structured you will

00:12:52,790 --> 00:12:59,209
to convert our daily to data frame so

00:12:55,610 --> 00:13:01,660
here you say past RDD dot 2df I use a

00:12:59,209 --> 00:13:04,670
project sprint and number of stories and

00:13:01,660 --> 00:13:06,920
then you can apply any transformation

00:13:04,670 --> 00:13:09,649
like filter you say project is equal to

00:13:06,920 --> 00:13:12,319
finance and then you say group by an

00:13:09,649 --> 00:13:16,759
aggregation of this sum and call the

00:13:12,319 --> 00:13:19,730
function count and you say limit 100 and

00:13:16,759 --> 00:13:23,180
so on 100 so what would happen here this

00:13:19,730 --> 00:13:25,660
will provide you the I mean strongly

00:13:23,180 --> 00:13:28,069
typed thus you will have the here you

00:13:25,660 --> 00:13:29,930
will have ability to call same thing

00:13:28,069 --> 00:13:32,889
that you do with the sparks equal in

00:13:29,930 --> 00:13:32,889
this

00:13:34,170 --> 00:13:41,080
this dam is same for my son we are

00:13:37,000 --> 00:13:43,630
trying to first create the view we will

00:13:41,080 --> 00:13:45,340
have the virtual cable inside this park

00:13:43,630 --> 00:13:49,060
and then you can apply these park

00:13:45,340 --> 00:13:52,090
transformation and so if you execute

00:13:49,060 --> 00:13:54,910
this code what would happen if you have

00:13:52,090 --> 00:13:57,430
a typo as this query select like right

00:13:54,910 --> 00:13:59,800
it will give you a runtime error so you

00:13:57,430 --> 00:14:02,080
can save your amount of time and those

00:13:59,800 --> 00:14:03,550
who are walking this park they know you

00:14:02,080 --> 00:14:05,770
need to walk sometime or night because

00:14:03,550 --> 00:14:08,260
of your work load is working this

00:14:05,770 --> 00:14:11,170
problem you need to stay like most spark

00:14:08,260 --> 00:14:13,210
jobs and go to be yarn and check the or

00:14:11,170 --> 00:14:15,940
knows what's happening usually I see it

00:14:13,210 --> 00:14:17,740
you say the plan of exon then you

00:14:15,940 --> 00:14:20,530
provide all thousand code executed it

00:14:17,740 --> 00:14:25,240
says out of memory sometime I mean you

00:14:20,530 --> 00:14:27,580
understand the pain you so why we need

00:14:25,240 --> 00:14:29,890
structure the API that need a frame SQ

00:14:27,580 --> 00:14:36,010
linearity the same is you can apply with

00:14:29,890 --> 00:14:38,410
data frame and SQL and RDD so spark has

00:14:36,010 --> 00:14:42,880
a abstract syntax tree when you provide

00:14:38,410 --> 00:14:44,770
the SQL data frame and it have say it so

00:14:42,880 --> 00:14:47,770
first whatever you use you use the data

00:14:44,770 --> 00:14:51,370
set or data frame or sparks equal it

00:14:47,770 --> 00:14:55,720
will generate first and this all plan of

00:14:51,370 --> 00:14:58,420
XM and then it will create the logical

00:14:55,720 --> 00:15:00,400
plant where they talk about logical

00:14:58,420 --> 00:15:02,530
plant it will check the column names and

00:15:00,400 --> 00:15:05,580
the table them with this exists or not

00:15:02,530 --> 00:15:08,020
and then it will create the optimized

00:15:05,580 --> 00:15:10,330
logical plant then it will create the

00:15:08,020 --> 00:15:12,160
multiple physical plant with the

00:15:10,330 --> 00:15:13,930
post-motor so cost more than they give

00:15:12,160 --> 00:15:15,490
you statistical understanding that if

00:15:13,930 --> 00:15:18,100
you apply if you can secure this a

00:15:15,490 --> 00:15:20,380
cyclic graph how much time it will take

00:15:18,100 --> 00:15:22,630
and then it will choose the best post

00:15:20,380 --> 00:15:25,210
model and select the physical plant and

00:15:22,630 --> 00:15:27,130
paint the oddity now again you have come

00:15:25,210 --> 00:15:27,820
back to our ready so this oddities is

00:15:27,130 --> 00:15:29,620
not the same

00:15:27,820 --> 00:15:33,850
already this is highly efficient high

00:15:29,620 --> 00:15:36,010
level api's of the oddity and so and it

00:15:33,850 --> 00:15:39,280
is not going with a bus park and you

00:15:36,010 --> 00:15:41,260
still there and this oddity is the high

00:15:39,280 --> 00:15:45,110
efficiency and optimized code you

00:15:41,260 --> 00:15:48,680
understand as it is the java beans

00:15:45,110 --> 00:15:52,580
and because Parkinson JVM and it

00:15:48,680 --> 00:15:54,500
strongly type are today's so if it's

00:15:52,580 --> 00:15:56,800
between that oddity that you've seen at

00:15:54,500 --> 00:15:59,540
the last and which we talked earlier

00:15:56,800 --> 00:16:05,870
earlier is the low-level API at this

00:15:59,540 --> 00:16:11,060
high drive API so it has a API in us

00:16:05,870 --> 00:16:14,540
upon a spark to load X C this basic

00:16:11,060 --> 00:16:17,570
example we are trying to read the JSON

00:16:14,540 --> 00:16:20,660
file sparked or read or JSON and then we

00:16:17,570 --> 00:16:22,340
say convert data to domain objects we

00:16:20,660 --> 00:16:25,070
are trying to make position structure

00:16:22,340 --> 00:16:27,170
with the case class okay class those who

00:16:25,070 --> 00:16:29,540
don't know in Scala is like pooja in

00:16:27,170 --> 00:16:31,430
Java which we make the structure of

00:16:29,540 --> 00:16:34,490
Jason with the structure of the case

00:16:31,430 --> 00:16:36,410
class so you can do mastering and unmask

00:16:34,490 --> 00:16:38,840
sling with the case plus two Jason and

00:16:36,410 --> 00:16:42,170
Jason to case plus and you provide here

00:16:38,840 --> 00:16:45,020
employee name string and integer so now

00:16:42,170 --> 00:16:48,830
we are saying employee leader site data

00:16:45,020 --> 00:16:51,560
set of employee equal to employees to

00:16:48,830 --> 00:16:54,140
the frame as employed it will try to map

00:16:51,560 --> 00:16:56,810
your case class with structure of Jason

00:16:54,140 --> 00:16:59,870
so now when you see the line number for

00:16:56,810 --> 00:17:02,180
filter let us say it employs dataset not

00:16:59,870 --> 00:17:05,690
free stock P is equal to P naught else

00:17:02,180 --> 00:17:08,180
so you can read as a like Java object P

00:17:05,690 --> 00:17:11,510
dot edge and when you say greater than 3

00:17:08,180 --> 00:17:13,760
if you if you will say integer right now

00:17:11,510 --> 00:17:15,620
it is at 3 if you apply the any string

00:17:13,760 --> 00:17:17,510
it will case to compile them you don't

00:17:15,620 --> 00:17:19,790
have to wait at the runtime and execute

00:17:17,510 --> 00:17:22,190
the job if using the IntelliJ or like

00:17:19,790 --> 00:17:23,860
that it means so there on the compile

00:17:22,190 --> 00:17:27,080
time this is wrong right

00:17:23,860 --> 00:17:30,440
so create a state API saves your time

00:17:27,080 --> 00:17:33,170
some people I mean obviously spark you

00:17:30,440 --> 00:17:35,690
know cause think some time why can't I

00:17:33,170 --> 00:17:38,210
just use the SQL string as I create will

00:17:35,690 --> 00:17:40,160
be a poly spandex is the same thing but

00:17:38,210 --> 00:17:42,760
it takes a lot of time you can save the

00:17:40,160 --> 00:17:45,380
time but you can use the strongly type

00:17:42,760 --> 00:17:48,230
data set API which is under the hood

00:17:45,380 --> 00:17:50,180
this color optimization you can see all

00:17:48,230 --> 00:17:52,970
the time and you know what's happening

00:17:50,180 --> 00:17:57,310
right when you use the oddity you have

00:17:52,970 --> 00:17:59,330
to say how to do but here you say what

00:17:57,310 --> 00:18:01,400
you have the freedom you have the

00:17:59,330 --> 00:18:07,520
control on the code and you have the

00:18:01,400 --> 00:18:09,800
flexibility to chain the data and you

00:18:07,520 --> 00:18:13,670
can apply any function on top of the

00:18:09,800 --> 00:18:17,330
higher high order so this is the one

00:18:13,670 --> 00:18:20,960
example we are trying here so here you

00:18:17,330 --> 00:18:23,540
have the employee you are trying to join

00:18:20,960 --> 00:18:26,390
one table understand events employee

00:18:23,540 --> 00:18:30,470
table is the one a DBMS table and events

00:18:26,390 --> 00:18:33,230
file is demand pacified so the first

00:18:30,470 --> 00:18:36,320
thing we are trying to join on ID and

00:18:33,230 --> 00:18:38,060
then we have filtering even state on

00:18:36,320 --> 00:18:40,160
some dip

00:18:38,060 --> 00:18:42,170
so what happening here that events file

00:18:40,160 --> 00:18:44,330
is a packet file and pathway is the

00:18:42,170 --> 00:18:47,390
under the hood optimized for a spark

00:18:44,330 --> 00:18:49,640
based on polymer structure and employ

00:18:47,390 --> 00:18:51,980
table we are trying to eat from the our

00:18:49,640 --> 00:18:54,590
DBMS and then you join and then your

00:18:51,980 --> 00:18:57,560
plan offender but think if you will

00:18:54,590 --> 00:19:00,260
first figure out the scale when you scan

00:18:57,560 --> 00:19:03,380
the table and then you filter out and

00:19:00,260 --> 00:19:05,570
you scan the events and then you will

00:19:03,380 --> 00:19:07,220
join you can save the time but physical

00:19:05,570 --> 00:19:10,880
plan with purified push down and

00:19:07,220 --> 00:19:13,670
concluding it's always good to first

00:19:10,880 --> 00:19:15,800
filter out and then enjoy it out

00:19:13,670 --> 00:19:18,740
I mean sometime when you initiate the

00:19:15,800 --> 00:19:20,900
code is part of some time you fill the

00:19:18,740 --> 00:19:23,930
code is not optimized if mo is optimized

00:19:20,900 --> 00:19:27,590
but you need to align the way our

00:19:23,930 --> 00:19:31,010
transformation X Sun that that does help

00:19:27,590 --> 00:19:34,910
you to in a physical plant I optimize it

00:19:31,010 --> 00:19:36,500
so when you first figure out any ID be

00:19:34,910 --> 00:19:40,040
messed about the employees you are

00:19:36,500 --> 00:19:41,480
really taking less data on the spark

00:19:40,040 --> 00:19:43,520
cluster and then you are applying

00:19:41,480 --> 00:19:46,330
transformation so it can save a lot of

00:19:43,520 --> 00:19:46,330
time from you

00:19:52,400 --> 00:19:59,720
data frames are faster than Adi did it

00:19:55,380 --> 00:20:02,760
because you see any one of the chart

00:19:59,720 --> 00:20:04,530
idly Scala and Henry Python is expensive

00:20:02,760 --> 00:20:06,270
because as I said you need to Deepika

00:20:04,530 --> 00:20:10,140
and pick up before you string to JVM

00:20:06,270 --> 00:20:12,690
objects and others are in the frame are

00:20:10,140 --> 00:20:14,130
equal with the with a new spark SQL or

00:20:12,690 --> 00:20:16,370
you still need a frame is a second

00:20:14,130 --> 00:20:16,370
earlier

00:20:20,280 --> 00:20:27,400
so one more benefit with little site API

00:20:23,530 --> 00:20:31,270
it is optimized for casing so actually

00:20:27,400 --> 00:20:34,090
the engine called tungsten tungsten use

00:20:31,270 --> 00:20:37,120
it is to generate the code which I sold

00:20:34,090 --> 00:20:38,950
earlier the high level optimized oddly

00:20:37,120 --> 00:20:42,010
tungsten is the engine which will spread

00:20:38,950 --> 00:20:44,140
that and tungsten also help to case data

00:20:42,010 --> 00:20:47,410
so as much as you can in case the data

00:20:44,140 --> 00:20:49,540
okay so let us say it takes less maybe

00:20:47,410 --> 00:20:51,250
to clear some more data which is good

00:20:49,540 --> 00:20:53,230
for you if you have the less memory and

00:20:51,250 --> 00:20:55,240
want you to case more data and apply the

00:20:53,230 --> 00:21:01,720
transformation and the external rome

00:20:55,240 --> 00:21:06,100
update data sets of faster because the

00:21:01,720 --> 00:21:08,710
way if you see the Z ID f-- Engine for

00:21:06,100 --> 00:21:12,160
Java Java bauxite you provide the follow

00:21:08,710 --> 00:21:14,980
then you have the X on and it takes care

00:21:12,160 --> 00:21:17,050
that with the abstraction on the API

00:21:14,980 --> 00:21:19,540
that how which order you need to execute

00:21:17,050 --> 00:21:23,230
so actually DSS I inspired from that

00:21:19,540 --> 00:21:25,920
only in a JVM that so it takes care

00:21:23,230 --> 00:21:28,390
about the citizen and encoding/decoding

00:21:25,920 --> 00:21:31,810
instead of you use the Carlos - Taizo or

00:21:28,390 --> 00:21:38,680
Java SE legs are so encoder is the is a

00:21:31,810 --> 00:21:40,390
faster way to use data API is before we

00:21:38,680 --> 00:21:42,840
proceed further one clear study any

00:21:40,390 --> 00:21:42,840
questions

00:21:44,450 --> 00:21:55,320
so I work with 17 TB of data Plus around

00:21:50,720 --> 00:21:58,950
6.5 Millions 27 for DES and 17 billion

00:21:55,320 --> 00:22:01,740
connections historical data so what we

00:21:58,950 --> 00:22:04,049
face the problem and how we fix some of

00:22:01,740 --> 00:22:06,299
the issues that I am talking before that

00:22:04,049 --> 00:22:08,480
if any questions I will be happy to

00:22:06,299 --> 00:22:08,480
answer

00:22:09,920 --> 00:22:13,069
[Music]

00:22:19,769 --> 00:22:32,409
hold on to pmc member of spa feei very

00:22:26,820 --> 00:22:34,389
promising and you know scala and the

00:22:32,409 --> 00:22:35,919
other side api is compiled MC you

00:22:34,389 --> 00:22:37,720
understand when you run this power job

00:22:35,919 --> 00:22:55,090
it failed after some time and you feel

00:22:37,720 --> 00:22:57,970
somehow operations home for panther and

00:22:55,090 --> 00:23:00,070
checking it is kind of a given in your

00:22:57,970 --> 00:23:03,549
code and then you do the join operation

00:23:00,070 --> 00:23:05,820
you have both strings so yeah we see all

00:23:03,549 --> 00:23:05,820
the time

00:23:06,400 --> 00:23:11,800
the classes we have to go back to

00:23:09,310 --> 00:23:13,990
strings for for selling corporations or

00:23:11,800 --> 00:23:16,420
other things so compile time safety is

00:23:13,990 --> 00:23:19,210
gone there people don't get a this is

00:23:16,420 --> 00:23:25,870
one of the things that maybe five pound

00:23:19,210 --> 00:23:29,170
graph to show it yeah so if you the

00:23:25,870 --> 00:23:30,970
question is so we have to take the model

00:23:29,170 --> 00:23:33,190
they have case clusters with their

00:23:30,970 --> 00:23:37,180
functions in it with their methods but

00:23:33,190 --> 00:23:42,190
when we want to call the functions or

00:23:37,180 --> 00:23:45,270
methods we almost always go back to maps

00:23:42,190 --> 00:23:45,270
on data

00:23:45,640 --> 00:23:52,230
probably study cannot optimize report

00:23:49,600 --> 00:23:55,360
that they cannot get the full benefit oh

00:23:52,230 --> 00:23:56,830
yeah I mean strokes are using these

00:23:55,360 --> 00:24:01,720
paths to North Korean products on

00:23:56,830 --> 00:24:05,200
horseback to door to door to the spot to

00:24:01,720 --> 00:24:08,650
load three so I seal the high-level

00:24:05,200 --> 00:24:11,860
api's in a person ready so the benefit

00:24:08,650 --> 00:24:15,580
sometimes you get the UDF and then you

00:24:11,860 --> 00:24:18,070
have that function point of the ID to

00:24:15,580 --> 00:24:21,040
the frame but if you the higher-order

00:24:18,070 --> 00:24:25,150
function it's the core site now you you

00:24:21,040 --> 00:24:26,470
provide the map and those and you create

00:24:25,150 --> 00:24:30,100
the function and pass the function to

00:24:26,470 --> 00:24:33,220
map and then get written that until you

00:24:30,100 --> 00:24:36,850
use the UDF I think that is screen or

00:24:33,220 --> 00:24:38,080
the in 2.2 but if you quote the scalar

00:24:36,850 --> 00:24:41,200
function and then you pass color

00:24:38,080 --> 00:24:46,530
function to the map that supports that

00:24:41,200 --> 00:24:46,530
we teach yeah

00:24:47,669 --> 00:24:54,669
he's writing out our own encoders makes

00:24:52,539 --> 00:24:59,230
it faster than serialization should be

00:24:54,669 --> 00:25:01,870
Co no I'm saying the data sector wise to

00:24:59,230 --> 00:25:03,880
encode that 2.3 so you don't have to use

00:25:01,870 --> 00:25:05,679
the I mean something you can say the

00:25:03,880 --> 00:25:07,659
kind of riser is equal to through

00:25:05,679 --> 00:25:08,350
inspark submit when you when you say

00:25:07,659 --> 00:25:11,440
hyperparameters

00:25:08,350 --> 00:25:13,330
but it has by default so I'm saying that

00:25:11,440 --> 00:25:15,880
you don't provide the type of parameter

00:25:13,330 --> 00:25:16,690
if you were very familiar with spark 1.6

00:25:15,880 --> 00:25:19,149
and if you are coming with that

00:25:16,690 --> 00:25:22,149
background we think is good because when

00:25:19,149 --> 00:25:23,740
you see see it's much more art than

00:25:22,149 --> 00:25:26,470
science when you walk this path right

00:25:23,740 --> 00:25:27,909
you know that this hyper parameter is

00:25:26,470 --> 00:25:30,519
there is so many hyperparameters

00:25:27,909 --> 00:25:32,380
sometimes I have take a look one source

00:25:30,519 --> 00:25:34,090
code and see what some a different value

00:25:32,380 --> 00:25:36,039
of the type of parameter and then check

00:25:34,090 --> 00:25:39,090
that how it's optimized or not optimized

00:25:36,039 --> 00:25:39,090
in that way

00:25:41,450 --> 00:25:47,010
so before later I mean the yesterday

00:25:44,970 --> 00:25:49,650
when you create the architecture of the

00:25:47,010 --> 00:25:51,810
Big Data of faceted architecture so see

00:25:49,650 --> 00:25:53,850
I'm talking about fast do that as soon

00:25:51,810 --> 00:25:55,710
as you get your get processed data it

00:25:53,850 --> 00:25:59,120
may be the value for your customer it

00:25:55,710 --> 00:26:01,890
make the value for your customer because

00:25:59,120 --> 00:26:03,480
so you have three component mainly used

00:26:01,890 --> 00:26:06,270
in a like you have a little house and

00:26:03,480 --> 00:26:09,020
streaming message bus little EQ use for

00:26:06,270 --> 00:26:12,630
low-cost and massive scale let us say

00:26:09,020 --> 00:26:15,120
HBase in Jason is very faster faster

00:26:12,630 --> 00:26:17,400
Word message scale you've been scared

00:26:15,120 --> 00:26:20,130
let us talk about s3 bucket in Jason

00:26:17,400 --> 00:26:23,030
Sweeney right and you can scale

00:26:20,130 --> 00:26:26,540
massively data warehouse like high

00:26:23,030 --> 00:26:31,410
faster queries and tronics and

00:26:26,540 --> 00:26:33,990
reliability so you can scale your KPI

00:26:31,410 --> 00:26:37,020
Square is on high which is scale

00:26:33,990 --> 00:26:40,020
horizontally on different nodes and

00:26:37,020 --> 00:26:43,250
custom streaming messaging bus so low

00:26:40,020 --> 00:26:46,560
latency API is like Kafka and Genesis

00:26:43,250 --> 00:26:49,020
and still you want to optimize that you

00:26:46,560 --> 00:26:53,610
can use the akka it's a very high level

00:26:49,020 --> 00:26:57,030
API so we use a cup on that and the

00:26:53,610 --> 00:26:59,700
because aquabox on actors actor is the

00:26:57,030 --> 00:27:03,020
very lightweight more detector model

00:26:59,700 --> 00:27:03,020
instead of going to that church

00:27:06,410 --> 00:27:14,640
so when it become complexity so you have

00:27:09,720 --> 00:27:16,230
a application here and so when you can

00:27:14,640 --> 00:27:21,270
when you in this data with the streaming

00:27:16,230 --> 00:27:23,340
verses like in SSN Kafka so that and in

00:27:21,270 --> 00:27:25,170
the Sun it goes to the do direct assume

00:27:23,340 --> 00:27:27,630
that you can move any component to

00:27:25,170 --> 00:27:30,390
anywhere in data and people move I mean

00:27:27,630 --> 00:27:34,680
sometimes we will move data from later

00:27:30,390 --> 00:27:36,720
neck to get a warehouse and then you

00:27:34,680 --> 00:27:39,180
unique immigration process data to the

00:27:36,720 --> 00:27:42,480
your application auto object so it's

00:27:39,180 --> 00:27:44,210
complex lower ETL process unless you

00:27:42,480 --> 00:27:48,540
planned is some of the principles of

00:27:44,210 --> 00:27:50,490
optimization like if you have so when

00:27:48,540 --> 00:27:53,940
you create the cross / topic and it is

00:27:50,490 --> 00:27:56,760
to be rich miss and then if you have

00:27:53,940 --> 00:27:58,680
managers if you have the department

00:27:56,760 --> 00:28:01,320
those who want to query the data now

00:27:58,680 --> 00:28:06,390
joining one table of HBase and one table

00:28:01,320 --> 00:28:09,510
of 5 is not performance reacting because

00:28:06,390 --> 00:28:14,430
what will happen you cannot join because

00:28:09,510 --> 00:28:17,730
HBase is good to link data in a form of

00:28:14,430 --> 00:28:21,660
columns you cannot you can delete a form

00:28:17,730 --> 00:28:24,570
of SQL by using the tools like drill but

00:28:21,660 --> 00:28:27,870
dwell is not that scalable it's not that

00:28:24,570 --> 00:28:29,790
optimized to apply I mean even we try to

00:28:27,870 --> 00:28:33,030
join one table of grid and one table of

00:28:29,790 --> 00:28:35,850
high it takes a lot of time so it's not

00:28:33,030 --> 00:28:39,330
only taking a lot of time but you are

00:28:35,850 --> 00:28:42,360
killing the infrastructure because for

00:28:39,330 --> 00:28:45,210
those pal jobs is not enough on jobs on

00:28:42,360 --> 00:28:50,220
hold for inter cluster so you need to

00:28:45,210 --> 00:28:51,000
think about not toiling over a location

00:28:50,220 --> 00:28:54,380
of the sources

00:28:51,000 --> 00:28:54,380
when you turn your job

00:28:57,190 --> 00:29:02,809
so I'll talk about they can't witness

00:28:59,809 --> 00:29:04,490
one of the yesterday so it's it's kind

00:29:02,809 --> 00:29:09,080
of changing the game with changing the

00:29:04,490 --> 00:29:11,120
data platform so what business want in

00:29:09,080 --> 00:29:13,789
return they want voice doing returning

00:29:11,120 --> 00:29:15,200
what is really telling and when where

00:29:13,789 --> 00:29:16,610
and why and how is happening right

00:29:15,200 --> 00:29:19,009
that's what we in this one we just don't

00:29:16,610 --> 00:29:21,379
care use this part we use whatever you

00:29:19,009 --> 00:29:23,809
use inside that so challenges was the

00:29:21,379 --> 00:29:26,269
weekly data reference and daily interest

00:29:23,809 --> 00:29:28,820
rate spa job execution

00:29:26,269 --> 00:29:34,580
failures with unutilized spark jobs

00:29:28,820 --> 00:29:38,629
right and scalability of missus data 4.6

00:29:34,580 --> 00:29:41,539
million events for big plus you will

00:29:38,629 --> 00:29:43,759
have posting historical data around well

00:29:41,539 --> 00:29:47,240
throughout 30 TB of data and 17 million

00:29:43,759 --> 00:29:49,999
connections plus linear in sequential

00:29:47,240 --> 00:29:53,240
execution job mode with broken pipe of

00:29:49,999 --> 00:29:54,499
letters and joining 17 billion chronic

00:29:53,240 --> 00:29:56,029
smells records with the skewed data

00:29:54,499 --> 00:29:59,240
nature so what will happen skewed data

00:29:56,029 --> 00:30:02,809
nature means data is the end up in

00:29:59,240 --> 00:30:05,480
theory so it's not physically fit at

00:30:02,809 --> 00:30:08,330
some point of time and you would have to

00:30:05,480 --> 00:30:13,549
determine duplication with out like an

00:30:08,330 --> 00:30:17,809
item kind of one of the kpi's the

00:30:13,549 --> 00:30:19,159
solution we propose right was like 5x

00:30:17,809 --> 00:30:22,070
performance or improvement by the

00:30:19,159 --> 00:30:24,440
engineer in Titanic - can't even Indian

00:30:22,070 --> 00:30:26,240
pipelines propose to highly penetrant

00:30:24,440 --> 00:30:27,919
plastic non-blocking a synchronous

00:30:26,240 --> 00:30:31,850
architecture to say our customers 22

00:30:27,919 --> 00:30:37,129
asses on time from 20 years to caters

00:30:31,850 --> 00:30:38,149
for 6.6 4 / 6 min remains and for

00:30:37,129 --> 00:30:39,679
historical know what the next

00:30:38,149 --> 00:30:41,450
performance you get a because you're

00:30:39,679 --> 00:30:43,220
using the under the whole document isn't

00:30:41,450 --> 00:30:46,190
whatever is we speak all your same thing

00:30:43,220 --> 00:30:48,619
I'm talking about here and that helps in

00:30:46,190 --> 00:30:51,230
MDM like mastery feminism and you see

00:30:48,619 --> 00:30:55,389
duplicate items duplicate like ours to

00:30:51,230 --> 00:30:55,389
placate outlets and for that

00:30:56,820 --> 00:31:03,490
so you are you listed how it happened

00:30:59,799 --> 00:31:07,740
lights I thought some of the so you can

00:31:03,490 --> 00:31:12,519
see here right how it was so much of the

00:31:07,740 --> 00:31:13,149
customers will have the legacy data de

00:31:12,519 --> 00:31:17,429
minimis

00:31:13,149 --> 00:31:20,230
so you notice me 27 Kafka ingesting data

00:31:17,429 --> 00:31:23,769
storage base intellect and you will have

00:31:20,230 --> 00:31:27,009
a process data warehouse at high and

00:31:23,769 --> 00:31:29,049
then this is kind of earlier what will

00:31:27,009 --> 00:31:33,129
happening earlier with the police park

00:31:29,049 --> 00:31:35,200
and will spark signal and GPS here you

00:31:33,129 --> 00:31:36,970
might wonder why we have posted here

00:31:35,200 --> 00:31:38,950
because how is not made for micro

00:31:36,970 --> 00:31:40,419
services like you kind of screen micro

00:31:38,950 --> 00:31:43,360
services you can expose the micro

00:31:40,419 --> 00:31:46,750
services because I will take the one of

00:31:43,360 --> 00:31:51,370
the heat to the Gloucester the next time

00:31:46,750 --> 00:31:53,620
and I mean post we also support the way

00:31:51,370 --> 00:31:56,259
in JSON you can muscle inside that and

00:31:53,620 --> 00:31:58,929
you can expose the AP is that talks to

00:31:56,259 --> 00:32:04,450
me you are angular and QW is the

00:31:58,929 --> 00:32:06,759
reporting tool for this board so this is

00:32:04,450 --> 00:32:08,139
the main I think the problem you have to

00:32:06,759 --> 00:32:11,950
anything they did a first-rate

00:32:08,139 --> 00:32:13,899
architecture well you have to use many

00:32:11,950 --> 00:32:16,659
more to scale use the data like when you

00:32:13,899 --> 00:32:19,179
want to have a performance and the

00:32:16,659 --> 00:32:21,669
remedy of they did our house and no that

00:32:19,179 --> 00:32:24,730
is a streaming if you mix out this three

00:32:21,669 --> 00:32:27,519
component with better combination with

00:32:24,730 --> 00:32:30,009
the performance optimization it helps to

00:32:27,519 --> 00:32:32,409
create fast data architecture let me

00:32:30,009 --> 00:32:35,980
give you one example what was happening

00:32:32,409 --> 00:32:39,250
with you I mean we were executing our

00:32:35,980 --> 00:32:41,080
workloads on sequential mode you execute

00:32:39,250 --> 00:32:44,049
one spot when things some it is

00:32:41,080 --> 00:32:46,840
wait until it get completed again second

00:32:44,049 --> 00:32:50,470
again second again so you if you

00:32:46,840 --> 00:32:52,000
understand when data model according to

00:32:50,470 --> 00:32:54,730
KPI said which is more than you know

00:32:52,000 --> 00:32:57,820
what you mean Fox and then you know with

00:32:54,730 --> 00:32:59,879
that you need to join again like outlet

00:32:57,820 --> 00:33:02,309
and item when structure and connections

00:32:59,879 --> 00:33:04,919
so obviously this takes more than right

00:33:02,309 --> 00:33:06,760
so what we need we create this kind of

00:33:04,919 --> 00:33:09,990
graph you can

00:33:06,760 --> 00:33:13,299
we use the BMC control-m you can use the

00:33:09,990 --> 00:33:16,480
airflow or kind of different open source

00:33:13,299 --> 00:33:18,250
tools so you know those items outlays

00:33:16,480 --> 00:33:20,679
organization and files that master

00:33:18,250 --> 00:33:22,150
tables so you start better than the

00:33:20,679 --> 00:33:24,340
executing so this will you there's your

00:33:22,150 --> 00:33:26,770
cluster if you have the good memory and

00:33:24,340 --> 00:33:31,120
post exhibitors you can interesting

00:33:26,770 --> 00:33:33,610
cluster plus so sometimes people I mean

00:33:31,120 --> 00:33:35,919
something was no mistake by providing

00:33:33,610 --> 00:33:39,460
more executors and core and think the

00:33:35,919 --> 00:33:42,400
workload will will be fast so as I said

00:33:39,460 --> 00:33:44,590
are they are then science so when you

00:33:42,400 --> 00:33:48,070
said number of tasks is equal to number

00:33:44,590 --> 00:33:50,320
of executors into number of core so each

00:33:48,070 --> 00:33:54,160
so what will happen if you have eight

00:33:50,320 --> 00:34:00,520
executors with eight eight for each so

00:33:54,160 --> 00:34:01,570
you can execute 64 tasks parallely so

00:34:00,520 --> 00:34:05,350
one more thing

00:34:01,570 --> 00:34:08,230
the hypergravity tuning so you have to

00:34:05,350 --> 00:34:10,119
enable the external software service on

00:34:08,230 --> 00:34:13,659
the yarn if you are using the source

00:34:10,119 --> 00:34:18,399
manager and that will help you to change

00:34:13,659 --> 00:34:20,530
the runtime executors so what is plus

00:34:18,399 --> 00:34:22,929
takes got for you right in your business

00:34:20,530 --> 00:34:24,399
cluster is for high having me if you

00:34:22,929 --> 00:34:26,590
have the product managers they use the

00:34:24,399 --> 00:34:30,580
SQL queries like then also hands-on

00:34:26,590 --> 00:34:34,240
hydrograph strong and so when you unable

00:34:30,580 --> 00:34:36,280
the external suffer service if five jobs

00:34:34,240 --> 00:34:39,129
are running with time and you will see

00:34:36,280 --> 00:34:41,800
on your fifth spawn job and those are

00:34:39,129 --> 00:34:44,649
taking more cause and executors

00:34:41,800 --> 00:34:47,500
so if the order of this pal job is the

00:34:44,649 --> 00:34:49,690
priority it will reduce the executors

00:34:47,500 --> 00:34:50,889
and provide this stop otherwise if you

00:34:49,690 --> 00:34:54,280
learn your job

00:34:50,889 --> 00:34:56,919
having five TB of data with twenty

00:34:54,280 --> 00:34:59,080
executors and twenty course and you

00:34:56,919 --> 00:35:02,740
provide executed memory into our memory

00:34:59,080 --> 00:35:03,760
so do Iowa is the master node which

00:35:02,740 --> 00:35:07,020
which

00:35:03,760 --> 00:35:10,480
allocate the spark context and execution

00:35:07,020 --> 00:35:13,330
so anytime you are your job is good a

00:35:10,480 --> 00:35:13,740
location to the cluster now other jokes

00:35:13,330 --> 00:35:15,869
can

00:35:13,740 --> 00:35:19,910
start because you are holding the memory

00:35:15,869 --> 00:35:23,820
and infrastructure resources so for that

00:35:19,910 --> 00:35:25,890
over allocation of the resources for

00:35:23,820 --> 00:35:28,080
small job so what people do is they said

00:35:25,890 --> 00:35:30,540
idea small jobs but you provide more the

00:35:28,080 --> 00:35:34,890
location of the memory that make hold to

00:35:30,540 --> 00:35:37,589
the jobs and locate the a cluster so job

00:35:34,890 --> 00:35:39,510
so what you can do those path jobs were

00:35:37,589 --> 00:35:41,730
taking more resources but they were less

00:35:39,510 --> 00:35:44,010
disco volume we didn't see when and have

00:35:41,730 --> 00:35:46,280
a parameter exhibitors cause memory on

00:35:44,010 --> 00:35:48,720
exhibit on driver has been reduced to a

00:35:46,280 --> 00:35:50,940
pending jobs to execute and not to block

00:35:48,720 --> 00:35:53,849
in a cluster so your job is not like a

00:35:50,940 --> 00:35:56,520
slowly but utilize the entire staff with

00:35:53,849 --> 00:36:00,080
it with the approach that may not hold

00:35:56,520 --> 00:36:00,080
the spa jobs

00:36:01,830 --> 00:36:10,660
so this is the one of the example you

00:36:04,720 --> 00:36:13,750
can see here you provide the master as a

00:36:10,660 --> 00:36:17,200
young here never mode is a cluster never

00:36:13,750 --> 00:36:19,300
memory system exhibited 12 and if I use

00:36:17,200 --> 00:36:21,370
the spark norsu for not service work of

00:36:19,300 --> 00:36:23,080
everyone - and you provide spark no

00:36:21,370 --> 00:36:27,550
dynamical equation and I went through

00:36:23,080 --> 00:36:32,350
and this will help you to allocate

00:36:27,550 --> 00:36:34,500
resources exhibit a memory you have 30

00:36:32,350 --> 00:36:37,750
game exhibit of course is 10 for example

00:36:34,500 --> 00:36:40,090
so number of tasks electrons in parallel

00:36:37,750 --> 00:36:43,050
e is equal to number of executors into

00:36:40,090 --> 00:36:43,050
course

00:36:46,109 --> 00:36:51,309
so one thing is you can apply to

00:36:49,180 --> 00:36:53,769
applause one Apple dislike reduce memory

00:36:51,309 --> 00:36:55,869
and cause and increase the executors so

00:36:53,769 --> 00:36:58,329
this will this can allow us to better

00:36:55,869 --> 00:37:01,660
utilize our resources on the cluster

00:36:58,329 --> 00:37:03,700
without locking others out if you reduce

00:37:01,660 --> 00:37:04,960
the executors and use the same memory

00:37:03,700 --> 00:37:07,630
that you had if it has the prayer

00:37:04,960 --> 00:37:11,279
problem this will run slower for the

00:37:07,630 --> 00:37:15,279
Tron but will allow others to use and I

00:37:11,279 --> 00:37:17,619
mean the cluster box in parallel so the

00:37:15,279 --> 00:37:19,869
problem happened like five in entire

00:37:17,619 --> 00:37:21,670
team five people are working right if I

00:37:19,869 --> 00:37:24,400
on my job other people will come there

00:37:21,670 --> 00:37:26,529
like because there is no space remaining

00:37:24,400 --> 00:37:30,279
ones bugs of this guy is taking entire

00:37:26,529 --> 00:37:31,510
memory so first you enter the number I

00:37:30,279 --> 00:37:34,269
mean you cannot use same type of

00:37:31,510 --> 00:37:35,890
parameter for all jobs you have to

00:37:34,269 --> 00:37:39,760
understand the number of Records in your

00:37:35,890 --> 00:37:41,619
size and with this disk intensive or the

00:37:39,760 --> 00:37:45,190
memory intensive memory intensive if

00:37:41,619 --> 00:37:47,950
using the pasta store case in the casing

00:37:45,190 --> 00:37:49,740
mechanism in a spa job based on that you

00:37:47,950 --> 00:37:52,599
provide that so this is one of the

00:37:49,740 --> 00:37:55,059
example yeah I mean we understood

00:37:52,599 --> 00:37:56,859
whether the count number of and you can

00:37:55,059 --> 00:38:00,180
utilize and change the exam what you

00:37:56,859 --> 00:38:00,180
need to change and everything

00:38:00,839 --> 00:38:07,660
one more thing this Park provides the

00:38:03,490 --> 00:38:09,099
controlling flexibility in open source I

00:38:07,660 --> 00:38:11,259
mean this park is open source tool and

00:38:09,099 --> 00:38:14,799
as you know it provides the flexibility

00:38:11,259 --> 00:38:17,440
in the sense you can split the physical

00:38:14,799 --> 00:38:19,450
data in partisans and you can say that

00:38:17,440 --> 00:38:20,289
how many number of files you want in

00:38:19,450 --> 00:38:25,059
that position

00:38:20,289 --> 00:38:26,769
I for example spark MapReduce

00:38:25,059 --> 00:38:28,839
transformations are very small small

00:38:26,769 --> 00:38:30,730
files on larger data set at some

00:38:28,839 --> 00:38:34,420
exchange which lead to increase this IO

00:38:30,730 --> 00:38:36,970
memory IO file IO and they when did I

00:38:34,420 --> 00:38:40,029
write it takes reading I mean reading

00:38:36,970 --> 00:38:42,400
files also think of time so also

00:38:40,029 --> 00:38:44,049
downstream high queries and spa jobs get

00:38:42,400 --> 00:38:46,329
impact on the performance and sometimes

00:38:44,049 --> 00:38:48,819
it fails this quarter existing this kind

00:38:46,329 --> 00:38:52,960
of else those people can see container

00:38:48,819 --> 00:38:55,750
lost accepts on etc so what happens then

00:38:52,960 --> 00:38:57,940
you already know the your dimensions in

00:38:55,750 --> 00:39:00,280
fact you know in your down

00:38:57,940 --> 00:39:02,650
api's how you need to do slice and dice

00:39:00,280 --> 00:39:07,030
so according to that you paid the

00:39:02,650 --> 00:39:09,039
partisans and instead person to Paris on

00:39:07,030 --> 00:39:11,410
and then you can get the you can post

00:39:09,039 --> 00:39:12,819
get up to that but again when you post

00:39:11,410 --> 00:39:15,910
data it will give the smaller smaller

00:39:12,819 --> 00:39:19,569
size which impact the performance so you

00:39:15,910 --> 00:39:20,920
can use the depends the partitioning

00:39:19,569 --> 00:39:22,930
after that you can apply any

00:39:20,920 --> 00:39:24,819
partitioning and colors so when you talk

00:39:22,930 --> 00:39:27,220
about repositioning it allows you to

00:39:24,819 --> 00:39:32,950
redistribution of data equally to all

00:39:27,220 --> 00:39:36,700
partisans and reduce the number of files

00:39:32,950 --> 00:39:41,500
and also you can use the co let's call

00:39:36,700 --> 00:39:46,869
as will not do suffering okay so it is

00:39:41,500 --> 00:39:48,369
faster then the partitioning one more

00:39:46,869 --> 00:39:51,670
thing if you study partitioning you need

00:39:48,369 --> 00:39:53,559
to bump up your driver memory because

00:39:51,670 --> 00:39:54,880
when you say distribution with your

00:39:53,559 --> 00:39:58,210
software from there all the way

00:39:54,880 --> 00:39:59,950
broadcast also works and Qualis will not

00:39:58,210 --> 00:40:03,579
do stuff all but it will reuse to your

00:39:59,950 --> 00:40:10,510
number of files that you store on you

00:40:03,579 --> 00:40:12,220
know in a partisan one more thing that

00:40:10,510 --> 00:40:14,890
this is also one of the good point to

00:40:12,220 --> 00:40:16,750
understand don't use streaming

00:40:14,890 --> 00:40:19,900
everywhere

00:40:16,750 --> 00:40:22,210
it's good to use a frequent baseboard if

00:40:19,900 --> 00:40:25,420
you have the if you think your business

00:40:22,210 --> 00:40:29,339
how it is and whatever you are doing

00:40:25,420 --> 00:40:32,380
if you okay with kind of 2-minute of SLA

00:40:29,339 --> 00:40:36,069
it's okay to use that you know frequent

00:40:32,380 --> 00:40:37,660
baseball because streaming will will

00:40:36,069 --> 00:40:39,839
have problems sometimes with partisan

00:40:37,660 --> 00:40:45,270
hardware faders GC spike in traffic

00:40:39,839 --> 00:40:50,140
because it it still use the your your

00:40:45,270 --> 00:40:52,329
JVM GC and everything but data says also

00:40:50,140 --> 00:40:56,230
have the feet I mean the good thing as I

00:40:52,329 --> 00:40:59,680
told her they all to encoder decoder it

00:40:56,230 --> 00:41:03,789
use the effect memory okay so your hip

00:40:59,680 --> 00:41:06,640
memories of it so that is also so king

00:41:03,789 --> 00:41:08,680
of the basement job immediately scale to

00:41:06,640 --> 00:41:10,250
night size it needs to be it does it

00:41:08,680 --> 00:41:13,130
works and goes away you know

00:41:10,250 --> 00:41:19,250
I mean you don't allocate permanently

00:41:13,130 --> 00:41:21,560
some of the cluster resources so this is

00:41:19,250 --> 00:41:25,430
the one example historical data

00:41:21,560 --> 00:41:27,320
processing so things like this are they

00:41:25,430 --> 00:41:31,550
all tables in hi

00:41:27,320 --> 00:41:33,770
and partisans so you want to spend your

00:41:31,550 --> 00:41:35,620
device coming with the POS data when you

00:41:33,770 --> 00:41:38,240
do triceps right so those are the files

00:41:35,620 --> 00:41:41,630
those five new policies with data and

00:41:38,240 --> 00:41:43,640
then you dump into all Devils so I will

00:41:41,630 --> 00:41:46,040
click my file item by file file errors

00:41:43,640 --> 00:41:48,530
in twice and my file every file is a

00:41:46,040 --> 00:41:50,750
Python file id okay inside that you will

00:41:48,530 --> 00:41:53,510
have the data so we need 20 by file ID

00:41:50,750 --> 00:41:56,330
jump stream it will read only the koalas

00:41:53,510 --> 00:42:01,070
are understood it is not reading the

00:41:56,330 --> 00:42:04,250
entire table for you now when we try to

00:42:01,070 --> 00:42:05,440
join it together if you see the first

00:42:04,250 --> 00:42:08,330
thing

00:42:05,440 --> 00:42:10,850
those who walk with spark familiar with

00:42:08,330 --> 00:42:13,010
this type of discotheques state exhibit

00:42:10,850 --> 00:42:16,310
a lot stranger contain a filter by yarn

00:42:13,010 --> 00:42:18,560
on existing memory on a website this is

00:42:16,310 --> 00:42:22,580
the failure and we want it to dump all

00:42:18,560 --> 00:42:25,760
the data except okay so we only want

00:42:22,580 --> 00:42:29,960
twice and state to cut off here then

00:42:25,760 --> 00:42:32,510
that's pain so after doing some of they

00:42:29,960 --> 00:42:35,630
are ending that we got was feeling

00:42:32,510 --> 00:42:38,840
temporary new here and inside or later

00:42:35,630 --> 00:42:40,850
to high internal table so the days to

00:42:38,840 --> 00:42:45,080
think external table internals ever time

00:42:40,850 --> 00:42:47,360
so external table when you delegate so

00:42:45,080 --> 00:42:49,640
interpretable candidate it will direct

00:42:47,360 --> 00:42:52,550
the table structure made of straw and

00:42:49,640 --> 00:42:54,200
the data so when you make the external

00:42:52,550 --> 00:42:58,640
table it will delete the whole structure

00:42:54,200 --> 00:43:01,880
or the data so we dumped out to the

00:42:58,640 --> 00:43:04,010
internal table here - table and pocket

00:43:01,880 --> 00:43:06,470
format format without non constant data

00:43:04,010 --> 00:43:10,430
so you really everything join it and

00:43:06,470 --> 00:43:15,110
down down to the hive so when when you

00:43:10,430 --> 00:43:16,880
want it to double it up to their sift we

00:43:15,110 --> 00:43:19,670
do aníbal the yarn across

00:43:16,880 --> 00:43:21,619
service and I will dynamically social

00:43:19,670 --> 00:43:25,640
okay sir and don't have a permit us to

00:43:21,619 --> 00:43:27,140
utilize cluster my business here okay so

00:43:25,640 --> 00:43:29,029
whatever you applied with this

00:43:27,140 --> 00:43:32,750
transmission apply here just to get data

00:43:29,029 --> 00:43:36,380
to the one thing and then move bulk load

00:43:32,750 --> 00:43:42,289
to the shift or somewhere else

00:43:36,380 --> 00:43:44,420
okay so you can see here 1 TB of output

00:43:42,289 --> 00:43:46,609
data suffer what's happening here and

00:43:44,420 --> 00:43:48,710
you can see all exhibited I utilize on

00:43:46,609 --> 00:43:53,750
the cluster sometime when you open these

00:43:48,710 --> 00:43:56,660
park UI you can see some here so exist

00:43:53,750 --> 00:44:04,759
expiration most which park provides the

00:43:56,660 --> 00:44:08,380
is I told earlier dynamic Luigi so that

00:44:04,759 --> 00:44:08,380
we have been integrated again

00:44:11,559 --> 00:44:18,200
so how is a nebula using I mean how it

00:44:15,890 --> 00:44:20,859
was possible because of the open source

00:44:18,200 --> 00:44:23,930
part and the scholar pre-medicine

00:44:20,859 --> 00:44:28,280
and everything is an open source light

00:44:23,930 --> 00:44:30,230
if you feel other tools like informatica

00:44:28,280 --> 00:44:31,970
powers and ETL goes where you don't have

00:44:30,230 --> 00:44:33,380
flexibility and control you don't know

00:44:31,970 --> 00:44:35,390
how under the whole dope you messin

00:44:33,380 --> 00:44:37,339
happening how'd it is physically

00:44:35,390 --> 00:44:39,589
speaking where what you need to apply

00:44:37,339 --> 00:44:42,050
and you wanna spend your business logic

00:44:39,589 --> 00:44:43,609
and transformation twenties according to

00:44:42,050 --> 00:44:46,339
that you create the new transformation

00:44:43,609 --> 00:44:51,230
and Pattinson and you can control the

00:44:46,339 --> 00:44:51,880
number of files on that yeah so that's

00:44:51,230 --> 00:44:55,270
it

00:44:51,880 --> 00:44:55,270
any question

00:44:57,369 --> 00:45:00,369
hi

00:45:03,430 --> 00:45:07,240
as on premises

00:45:11,000 --> 00:45:17,390
aw cmf yeah if somebody responds up to

00:45:15,109 --> 00:45:20,030
Emma and it will take care but mostly

00:45:17,390 --> 00:45:22,790
the issues like that when you see Emma

00:45:20,030 --> 00:45:25,369
what happens because you need to do in

00:45:22,790 --> 00:45:27,950
and how to apply some of the R&D that is

00:45:25,369 --> 00:45:30,740
 or not will cost you is at the

00:45:27,950 --> 00:45:34,250
end of winning right that the time you

00:45:30,740 --> 00:45:36,650
are okay resources in that so all the

00:45:34,250 --> 00:45:39,950
mice especially is like you don't have

00:45:36,650 --> 00:45:41,839
to pay anything earlier if you are

00:45:39,950 --> 00:45:43,670
jobless for eight hours it failed you

00:45:41,839 --> 00:45:46,660
again need to tune it right in

00:45:43,670 --> 00:45:46,660
distributed nature so

00:45:52,569 --> 00:45:55,569
in

00:45:58,190 --> 00:46:01,190
transportation

00:46:04,820 --> 00:46:12,810
Emma so you can cut code from the spark

00:46:10,380 --> 00:46:15,290
job so Emma is just taking the sponge of

00:46:12,810 --> 00:46:18,750
job right so your job fight when wind

00:46:15,290 --> 00:46:22,530
man good job you pass through the EMR

00:46:18,750 --> 00:46:26,090
that will be taken care under the hood

00:46:22,530 --> 00:46:26,090
I assume the database also use the

00:46:26,450 --> 00:46:32,190
opposite spark so that that has that you

00:46:29,970 --> 00:46:33,750
can ask whatever I say in the Emma

00:46:32,190 --> 00:46:36,930
there's nothing that you can't do with

00:46:33,750 --> 00:46:41,310
EMR Google has the data proc on the

00:46:36,930 --> 00:46:44,150
cloud which is the parameter service

00:46:41,310 --> 00:46:44,150
from Amazon and

00:46:46,670 --> 00:46:49,670
anyone

00:46:52,270 --> 00:47:04,960
okay thank you you can tweet me if you

00:47:01,430 --> 00:47:04,960

YouTube URL: https://www.youtube.com/watch?v=G5GrFJCSRtE


