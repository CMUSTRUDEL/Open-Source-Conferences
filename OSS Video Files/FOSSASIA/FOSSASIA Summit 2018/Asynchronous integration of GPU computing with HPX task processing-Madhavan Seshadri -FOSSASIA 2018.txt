Title: Asynchronous integration of GPU computing with HPX task processing-Madhavan Seshadri -FOSSASIA 2018
Publication date: 2018-03-24
Playlist: FOSSASIA Summit 2018
Description: 
	Speaker: Madhavan Seshadri, Software Developer Ste||ar Group
Info: https://2018.fossasia.org/event/speakers.html#madhavan-seshadri3190

Developing massively parallel systems is restricted by the complex tasks which need to be managed by the programmer. GPU computing provides the opportunity to parallelize data parallel algorithms while CPU can run the sequential code. With increasing algorithmic development, some new algorithms require iterations of parallel computation on the GPUs (computation scale larger than GPU memory) while some require multiple different data parallel algorithms to run simultaneously, which are notorious to be managed by the programmer.

HPX is an open-source, general purpose C++ library for developing parallel and distributed applications with a broad community usage. This talk aims to discuss the development of HPX Compute language (HPXCL) API for the integration of GPU computation with asynchronous many task execution library HPX. Asynchronous functions are provided for kernel launch, kernel execution and data transfer with the capability to hide the communication latency through computation. To give an example, computation on multiple CPU nodes, GPU nodes can all occur in parallel and can be synchronized when the results are required by the user. This system unleashes the potential to take computation to the exa-scale level.

This development is currently spearheaded by the Stellar Group Community which is a consortium of global researchers. The presenter has been a contributor to this community since his Google Summer of Code participation in 2017.

Github Link:Â https://github.com/STEllAR-GROUP/hpxcl

Track: Kernel & Platform | Room: Training room 4-3 l Date: Friday, 23rd March, 2018

Event Page: http://2018.fossasia.org
Follow FOSSASIA on Twitter: https://twitter.com/fossasia/
Like FOSSASIA on Facebook: https://www.facebook.com/fossasia/

Produced by Engineers.SG
Captions: 
	00:00:00,030 --> 00:00:05,270
Thank You Jerry so very pleasant

00:00:03,300 --> 00:00:09,929
afternoon to one and all present here so

00:00:05,270 --> 00:00:11,429
I mean we all know that with the

00:00:09,929 --> 00:00:13,139
increasing amount of data that we

00:00:11,429 --> 00:00:16,139
collect we want to process the data as

00:00:13,139 --> 00:00:17,340
quickly as possible so this is one step

00:00:16,139 --> 00:00:21,210
towards bad direction

00:00:17,340 --> 00:00:23,189
so today my talk is going to be about HP

00:00:21,210 --> 00:00:26,390
X here which is a synchronous

00:00:23,189 --> 00:00:30,750
integration of GPU computing with HP X

00:00:26,390 --> 00:00:34,170
so Who am I I am Madhu and Seshadri I

00:00:30,750 --> 00:00:38,219
work at visa Singapore

00:00:34,170 --> 00:00:39,840
I graduated from NTU last year and I've

00:00:38,219 --> 00:00:42,120
been working for the stellar group under

00:00:39,840 --> 00:00:45,570
which the HP X and HPCL development has

00:00:42,120 --> 00:00:48,050
been going on since since my google

00:00:45,570 --> 00:00:51,180
Summer of Code participation in 2017 so

00:00:48,050 --> 00:00:52,890
I also have contributions in the vehicle

00:00:51,180 --> 00:00:56,629
routeing domain I've published a few

00:00:52,890 --> 00:00:59,579
papers and a few journals in conferences

00:00:56,629 --> 00:01:01,489
so what is the main goal our main goal

00:00:59,579 --> 00:01:04,559
is towards massively parallel

00:01:01,489 --> 00:01:07,650
parallelism like we want to make sure

00:01:04,559 --> 00:01:11,490
that we can execute as many tasks as as

00:01:07,650 --> 00:01:13,740
in an as parallel manner as possible so

00:01:11,490 --> 00:01:16,710
but as such we are inhibited by the

00:01:13,740 --> 00:01:18,150
complexity of new languages and various

00:01:16,710 --> 00:01:19,890
programming techniques that come with

00:01:18,150 --> 00:01:23,310
different sets of devices as well as the

00:01:19,890 --> 00:01:25,950
versions of the devices so most of the

00:01:23,310 --> 00:01:29,729
compute power today is split across GPU

00:01:25,950 --> 00:01:32,310
devices CPU devices as well as so the

00:01:29,729 --> 00:01:34,590
and each type of GPU devices have their

00:01:32,310 --> 00:01:40,170
own coding coding language for example

00:01:34,590 --> 00:01:42,150
you have good open CL and whatnot so

00:01:40,170 --> 00:01:43,860
another main challenge that we are

00:01:42,150 --> 00:01:46,079
facing right now is the architectural

00:01:43,860 --> 00:01:48,030
integration of heterogenic devices under

00:01:46,079 --> 00:01:50,700
one single framework of once and single

00:01:48,030 --> 00:01:53,009
umbrellas is difficult and it's

00:01:50,700 --> 00:01:57,250
oftentimes its inefficient and what the

00:01:53,009 --> 00:02:00,130
native implementation provides us so we

00:01:57,250 --> 00:02:03,150
as such we need to integrate diverse

00:02:00,130 --> 00:02:06,430
programs under one single umbrella and

00:02:03,150 --> 00:02:08,619
we want to maximize the utilization of

00:02:06,430 --> 00:02:14,800
the available processes as much as

00:02:08,619 --> 00:02:17,070
possible so before I go into depth about

00:02:14,800 --> 00:02:20,230
the design changes that came along with

00:02:17,070 --> 00:02:21,700
integrating GPU devices with HP X I

00:02:20,230 --> 00:02:24,490
would like to give a bit of introduction

00:02:21,700 --> 00:02:27,700
about what HP XS and what it can do so

00:02:24,490 --> 00:02:29,170
HP axis and is a runtime light library

00:02:27,700 --> 00:02:31,630
which of those finely grained

00:02:29,170 --> 00:02:36,760
parallelism for CPUs and see on people

00:02:31,630 --> 00:02:40,200
so as such the integration of GPUs into

00:02:36,760 --> 00:02:42,820
HP X right now is a bit experimental and

00:02:40,200 --> 00:02:46,510
while you're designing a solution for

00:02:42,820 --> 00:02:49,600
this we want to minimize the application

00:02:46,510 --> 00:02:53,200
level latency that a new API level layer

00:02:49,600 --> 00:02:57,220
API layer would incur and we also need

00:02:53,200 --> 00:02:59,470
to minimize the CPU to GPU you know

00:02:57,220 --> 00:03:02,709
inter processor communication time as

00:02:59,470 --> 00:03:06,299
well as the vice versa so we also want

00:03:02,709 --> 00:03:10,110
to minimize IO stalls and data transfers

00:03:06,299 --> 00:03:12,310
we want to overlap computation with

00:03:10,110 --> 00:03:15,880
communication as much as possible then

00:03:12,310 --> 00:03:18,160
when it's not hindering we also would

00:03:15,880 --> 00:03:19,959
like to provide a common synchronization

00:03:18,160 --> 00:03:21,880
mechanism what if you have some tasks

00:03:19,959 --> 00:03:24,489
running on the CPU some tasks running on

00:03:21,880 --> 00:03:26,709
the GPUs and other processors but what

00:03:24,489 --> 00:03:28,660
if you want to synchronize all of them

00:03:26,709 --> 00:03:30,310
using a common synchronization mechanism

00:03:28,660 --> 00:03:35,280
we want to we want to be able to provide

00:03:30,310 --> 00:03:35,280
that using this so

00:03:36,330 --> 00:03:43,780
HP also what is HP XE l HP XE l is just

00:03:40,270 --> 00:03:46,420
the integration of GPU into GPU

00:03:43,780 --> 00:03:49,000
processing into HP X framework so right

00:03:46,420 --> 00:03:52,150
now the focus is on CUDA there is also a

00:03:49,000 --> 00:03:54,940
version which implement which you know

00:03:52,150 --> 00:03:58,000
brings in OpenCL implementation into HP

00:03:54,940 --> 00:03:59,980
X so it provides a uniform

00:03:58,000 --> 00:04:02,350
synchronization mechanism through the

00:03:59,980 --> 00:04:09,190
use of futures more about this and the

00:04:02,350 --> 00:04:12,640
later slides and we've reduce the API

00:04:09,190 --> 00:04:16,150
level or the common layer level latency

00:04:12,640 --> 00:04:19,359
which is incurred we've benchmarked it

00:04:16,150 --> 00:04:21,160
to against certain algorithms like dense

00:04:19,359 --> 00:04:22,480
matrix multiplications pass matrix

00:04:21,160 --> 00:04:25,990
vector product which is highly data

00:04:22,480 --> 00:04:29,730
parallel so here's how the presentation

00:04:25,990 --> 00:04:32,410
is going to be now I I will focus on

00:04:29,730 --> 00:04:35,470
giving a brief overview about the HP X

00:04:32,410 --> 00:04:37,120
components as well as how the new

00:04:35,470 --> 00:04:39,570
components which we are bringing in

00:04:37,120 --> 00:04:42,040
integrates with HP X after that I'll

00:04:39,570 --> 00:04:47,440
discuss a bit about synchronization of

00:04:42,040 --> 00:04:50,050
tasks measurements as well as the last

00:04:47,440 --> 00:04:52,060
would be conclusion so if you look at

00:04:50,050 --> 00:04:55,180
this architecture diagram and just focus

00:04:52,060 --> 00:04:57,490
on the right-hand side a bit so you see

00:04:55,180 --> 00:04:59,560
that there are multiple localities which

00:04:57,490 --> 00:05:01,360
are separated by a Gaussian parcel I

00:04:59,560 --> 00:05:03,040
will discuss more about each of these

00:05:01,360 --> 00:05:05,110
components in detail over the next few

00:05:03,040 --> 00:05:07,720
slides but this is just to give an

00:05:05,110 --> 00:05:09,960
overview of how things things work with

00:05:07,720 --> 00:05:13,389
HP X

00:05:09,960 --> 00:05:15,610
so first things first what is Argos

00:05:13,389 --> 00:05:17,800
address it's just an active global

00:05:15,610 --> 00:05:22,030
address space which made which maintains

00:05:17,800 --> 00:05:26,080
the list of active global objects in

00:05:22,030 --> 00:05:28,509
memory it uses 128 bit virtual

00:05:26,080 --> 00:05:30,310
addressing space which spans across all

00:05:28,509 --> 00:05:33,490
the localities now if you see the

00:05:30,310 --> 00:05:36,610
diagram the shaded region the shaded

00:05:33,490 --> 00:05:39,190
region is the objects which are visible

00:05:36,610 --> 00:05:41,080
globally which means objects from one

00:05:39,190 --> 00:05:43,539
locality are visible across other

00:05:41,080 --> 00:05:45,639
localities and can be just used with the

00:05:43,539 --> 00:05:47,590
same set of references the unshaded

00:05:45,639 --> 00:05:49,900
region is the local objects which are

00:05:47,590 --> 00:05:54,039
pertaining to that locality and are not

00:05:49,900 --> 00:05:55,870
shared across the localities so the

00:05:54,039 --> 00:05:58,330
address of this addresses of these

00:05:55,870 --> 00:06:00,909
objects are unique and system-wide that

00:05:58,330 --> 00:06:04,810
means you just use the same references

00:06:00,909 --> 00:06:07,569
across different localities to obtain

00:06:04,810 --> 00:06:10,169
the devices in the domain for this

00:06:07,569 --> 00:06:13,240
integration of GPO with HP X we use

00:06:10,169 --> 00:06:14,919
Argos for just storing a system-wide

00:06:13,240 --> 00:06:18,430
level list of devices which are

00:06:14,919 --> 00:06:20,020
available across the localities so then

00:06:18,430 --> 00:06:22,180
the next thing I would like to discuss

00:06:20,020 --> 00:06:24,520
this parcel now we've discussed about an

00:06:22,180 --> 00:06:27,419
address space which maintains the list

00:06:24,520 --> 00:06:29,139
of objects in memory but what about

00:06:27,419 --> 00:06:32,590
intercommunication between these

00:06:29,139 --> 00:06:34,570
localities so that in HP X that happens

00:06:32,590 --> 00:06:36,430
through a component called partial the

00:06:34,570 --> 00:06:39,250
underlying implementation of partial can

00:06:36,430 --> 00:06:42,520
be changed from openmpi to like openmp

00:06:39,250 --> 00:06:44,919
openmpi or just plain ol MPI so

00:06:42,520 --> 00:06:47,770
internode communication in HP X happens

00:06:44,919 --> 00:06:50,440
through parcels the implementation of

00:06:47,770 --> 00:06:52,270
this level of this layer is one-sided

00:06:50,440 --> 00:06:55,449
and no pooling is required from any

00:06:52,270 --> 00:06:58,449
locality so as to minimize the wastage

00:06:55,449 --> 00:07:00,300
of CPU cycles so the next thing I would

00:06:58,449 --> 00:07:02,949
like to discuss is the thread manager

00:07:00,300 --> 00:07:05,319
thread manager we use it for scheduling

00:07:02,949 --> 00:07:06,969
jobs sync like making sure the jobs are

00:07:05,319 --> 00:07:12,819
getting executed in the localities we

00:07:06,969 --> 00:07:15,130
want them to get executed in etc so yeah

00:07:12,819 --> 00:07:16,479
as I just said the management scheduling

00:07:15,130 --> 00:07:18,900
of the jobs is managed by the thread

00:07:16,479 --> 00:07:18,900
manager

00:07:18,969 --> 00:07:24,849
actions are something which come from

00:07:22,179 --> 00:07:27,699
accumulators in hpx so basically this

00:07:24,849 --> 00:07:29,619
the actions provide a mechanism in which

00:07:27,699 --> 00:07:32,050
you can transport a function from one

00:07:29,619 --> 00:07:34,990
locality to another along with a set of

00:07:32,050 --> 00:07:36,219
parameters as arguments execute the

00:07:34,990 --> 00:07:37,990
result in a completely different

00:07:36,219 --> 00:07:40,719
locality and then bring the result back

00:07:37,990 --> 00:07:45,399
so it it provides remote invocation

00:07:40,719 --> 00:07:47,849
mechanism for for transporting fung so

00:07:45,399 --> 00:07:50,379
there are some inbuilt settings which

00:07:47,849 --> 00:07:53,679
hpx provides so as to optimize the

00:07:50,379 --> 00:07:56,349
utilization of the hardware so you must

00:07:53,679 --> 00:08:01,029
have cache by now that locality is it's

00:07:56,349 --> 00:08:02,709
just set of nodes in which jobs like

00:08:01,029 --> 00:08:06,099
threads and jobs can you can just run

00:08:02,709 --> 00:08:08,800
and are being executed so from this

00:08:06,099 --> 00:08:11,619
diagram you can see that different GPU

00:08:08,800 --> 00:08:14,199
dough-like GPGPU devices can be attached

00:08:11,619 --> 00:08:17,229
to any one locality we want to be able

00:08:14,199 --> 00:08:19,599
to think of this as a single unified

00:08:17,229 --> 00:08:21,699
computing resource and then be able to

00:08:19,599 --> 00:08:25,329
just get your jobs to them and be able

00:08:21,699 --> 00:08:27,219
to synchronize whenever we want so what

00:08:25,329 --> 00:08:28,749
are the new classes which have which we

00:08:27,219 --> 00:08:31,749
are bringing in through the integration

00:08:28,749 --> 00:08:33,969
of GPU so we're bringing in three main

00:08:31,749 --> 00:08:36,009
classes device buffer and program

00:08:33,969 --> 00:08:39,969
there's more on this over the next few

00:08:36,009 --> 00:08:42,490
slides so device is just like one it's a

00:08:39,969 --> 00:08:43,870
one-to-one device this device class and

00:08:42,490 --> 00:08:45,819
the objects which are created from it

00:08:43,870 --> 00:08:48,220
it's just a one-to-one mapping of the

00:08:45,819 --> 00:08:50,490
actual physical device so your GPUs

00:08:48,220 --> 00:08:53,170
could be spanning across different

00:08:50,490 --> 00:08:56,040
different localities which are different

00:08:53,170 --> 00:08:58,990
nodes and then using your agus you can

00:08:56,040 --> 00:09:02,620
have the references of all the devices

00:08:58,990 --> 00:09:05,079
available in your memory right now so

00:09:02,620 --> 00:09:08,920
it's used for creating buffered and

00:09:05,079 --> 00:09:10,870
program objects so yeah objects across

00:09:08,920 --> 00:09:15,430
locality can be accessed from one single

00:09:10,870 --> 00:09:17,949
one single place so device also comes up

00:09:15,430 --> 00:09:19,660
with its it's an a it's an abstraction

00:09:17,949 --> 00:09:23,170
from the native implementation it comes

00:09:19,660 --> 00:09:25,660
with facility to build kernels runtime

00:09:23,170 --> 00:09:27,560
as well as to execute them and we need

00:09:25,660 --> 00:09:29,370
it

00:09:27,560 --> 00:09:33,600
yeah

00:09:29,370 --> 00:09:37,250
so that next to discuss about is buffers

00:09:33,600 --> 00:09:39,690
so Bob you can think of buffers as just

00:09:37,250 --> 00:09:44,220
you know data pipe

00:09:39,690 --> 00:09:46,950
so basically buffers Paris to the

00:09:44,220 --> 00:09:49,020
localities which require the the usage

00:09:46,950 --> 00:09:52,230
of those data so let's say I'm a threat

00:09:49,020 --> 00:09:54,210
I come across a particular particular

00:09:52,230 --> 00:09:56,220
reference of an object which I don't

00:09:54,210 --> 00:09:57,480
possess right now so what I'll do is

00:09:56,220 --> 00:10:02,670
I'll go to a gas

00:09:57,480 --> 00:10:04,290
I'll get the I I'll ask the guys to send

00:10:02,670 --> 00:10:07,080
me that copy of the data that is

00:10:04,290 --> 00:10:10,980
happening through parcels so as buffers

00:10:07,080 --> 00:10:14,130
so in case of buffers via we use this to

00:10:10,980 --> 00:10:16,110
transport the that pallet II we

00:10:14,130 --> 00:10:17,990
transport it to the GPU to by device

00:10:16,110 --> 00:10:20,760
attached to that locality

00:10:17,990 --> 00:10:22,080
so it's just one buffer data argument

00:10:20,760 --> 00:10:24,540
which needs to be passed to the kernel

00:10:22,080 --> 00:10:25,830
can be provided here so the next thing I

00:10:24,540 --> 00:10:28,830
would like to discuss is is about

00:10:25,830 --> 00:10:32,640
program program is just it's it's a

00:10:28,830 --> 00:10:34,680
class which gives you functions to to do

00:10:32,640 --> 00:10:36,209
runtime compilation of any piece of code

00:10:34,680 --> 00:10:38,550
which you want currently it takes two

00:10:36,209 --> 00:10:40,890
different types of inputs you can load

00:10:38,550 --> 00:10:43,709
kernels from files and you can load

00:10:40,890 --> 00:10:46,740
kernels from strings so basically if you

00:10:43,709 --> 00:10:48,240
just see you can obtain like you can

00:10:46,740 --> 00:10:50,160
call the device and you can create a

00:10:48,240 --> 00:10:51,839
program with that particular device and

00:10:50,160 --> 00:10:56,459
whenever you want to run that program

00:10:51,839 --> 00:10:58,020
you can specify the parameters of grade

00:10:56,459 --> 00:11:00,839
and block which you would normally

00:10:58,020 --> 00:11:02,660
provide to you know a CUDA

00:11:00,839 --> 00:11:07,230
implementation as well as the list of

00:11:02,660 --> 00:11:08,339
parameters as an argument vector so

00:11:07,230 --> 00:11:09,779
we've discussed about different

00:11:08,339 --> 00:11:11,400
components which the framework is

00:11:09,779 --> 00:11:14,520
providing the next thing I would like to

00:11:11,400 --> 00:11:17,070
discuss about is how do you synchronize

00:11:14,520 --> 00:11:19,110
jobs which are running in CPUs and GPUs

00:11:17,070 --> 00:11:21,480
and have just one synchronizing

00:11:19,110 --> 00:11:23,610
mechanism so as I mentioned earlier we

00:11:21,480 --> 00:11:26,130
are using futures for it so every time

00:11:23,610 --> 00:11:29,790
you call any asynchronous function what

00:11:26,130 --> 00:11:32,459
it does is it returns your future hpx

00:11:29,790 --> 00:11:34,440
provides two inbuilt functions weight

00:11:32,459 --> 00:11:36,150
and weight all weight takes in a single

00:11:34,440 --> 00:11:38,400
future as an argument and just it's a

00:11:36,150 --> 00:11:39,810
blocking call way it all takes in a

00:11:38,400 --> 00:11:41,730
vector of future

00:11:39,810 --> 00:11:48,839
and then it waits for all the all the

00:11:41,730 --> 00:11:50,999
futures to return their value so using

00:11:48,839 --> 00:11:53,579
all these mechanisms it if you have an

00:11:50,999 --> 00:11:57,149
asynchronous execution tree of tasks you

00:11:53,579 --> 00:12:00,449
can execute tasks you can just start the

00:11:57,149 --> 00:12:03,199
tasks and get the result as as and when

00:12:00,449 --> 00:12:05,759
you require them just before the tip the

00:12:03,199 --> 00:12:08,910
the next task in the pipeline is is

00:12:05,759 --> 00:12:12,689
required so how it all fits into place

00:12:08,910 --> 00:12:14,459
so basically as I have mentioned earlier

00:12:12,689 --> 00:12:17,220
you have a get all devices function in

00:12:14,459 --> 00:12:18,930
HP XEL which gives you the list of

00:12:17,220 --> 00:12:22,559
devices which are connected across the

00:12:18,930 --> 00:12:25,470
nodes to obtain a particular device from

00:12:22,559 --> 00:12:27,870
you can just use the vector and get any

00:12:25,470 --> 00:12:30,660
device from that vector using the vector

00:12:27,870 --> 00:12:33,360
you can create a program object program

00:12:30,660 --> 00:12:34,980
object lets you create a program from a

00:12:33,360 --> 00:12:39,839
file or a string as i've mentioned

00:12:34,980 --> 00:12:41,639
before which returns you a future you

00:12:39,839 --> 00:12:44,610
can then initiate buffers for transfer

00:12:41,639 --> 00:12:46,199
to your kernel so you can just create

00:12:44,610 --> 00:12:47,790
the buffer and then you can use the in

00:12:46,199 --> 00:12:51,899
queue write function to write something

00:12:47,790 --> 00:12:54,180
to the buffer so just before let's say

00:12:51,899 --> 00:12:56,100
in this in this small small snippet of

00:12:54,180 --> 00:12:58,079
code just before you want to execute the

00:12:56,100 --> 00:13:00,509
program you just wait for all the

00:12:58,079 --> 00:13:05,309
buffers to finish transferring data to

00:13:00,509 --> 00:13:07,350
that particular GPU so this this

00:13:05,309 --> 00:13:12,449
particular links provide link provides

00:13:07,350 --> 00:13:15,660
you a complete sample which triple

00:13:12,449 --> 00:13:18,629
devices attached to your to to the nodes

00:13:15,660 --> 00:13:21,089
to be able to split the computation into

00:13:18,629 --> 00:13:23,819
multiple smaller pieces execute them

00:13:21,089 --> 00:13:25,500
across different GPUs and then bring the

00:13:23,819 --> 00:13:26,620
result back and stitch into one single

00:13:25,500 --> 00:13:31,270
image

00:13:26,620 --> 00:13:34,200
this just it's not clear but it's just a

00:13:31,270 --> 00:13:39,780
black and white Mandel brought him a

00:13:34,200 --> 00:13:42,730
twitch use now if you carefully see the

00:13:39,780 --> 00:13:45,490
execution time and the plot which which

00:13:42,730 --> 00:13:48,940
is here there is not much scalability on

00:13:45,490 --> 00:13:51,070
increasing from one node to like one GPO

00:13:48,940 --> 00:13:52,750
device to multiple GPO device this is

00:13:51,070 --> 00:13:55,690
actually because of the fact that we are

00:13:52,750 --> 00:13:58,030
for measuring the time we are including

00:13:55,690 --> 00:13:59,830
the data transfer time like it's

00:13:58,030 --> 00:14:03,040
basically end to end time so data

00:13:59,830 --> 00:14:04,960
transfer execution and then bring the

00:14:03,040 --> 00:14:06,310
result back like all of the timing is

00:14:04,960 --> 00:14:09,220
measured in this plot so there's not

00:14:06,310 --> 00:14:12,070
much advantage in you know going from

00:14:09,220 --> 00:14:14,530
single to two to three GPUs but if we

00:14:12,070 --> 00:14:17,470
were just to measure the GPU execution

00:14:14,530 --> 00:14:23,260
time that would be highly scalable in

00:14:17,470 --> 00:14:27,250
this case so previously I show I showed

00:14:23,260 --> 00:14:29,740
an example of how multiple devices can

00:14:27,250 --> 00:14:32,610
can be like multiple devices can take

00:14:29,740 --> 00:14:34,870
the jobs and execute them and parallel

00:14:32,610 --> 00:14:37,150
in this section I would just like to

00:14:34,870 --> 00:14:41,020
discuss about the overhead which this

00:14:37,150 --> 00:14:44,290
layer is bringing in so as I said before

00:14:41,020 --> 00:14:48,100
we we tested against several different

00:14:44,290 --> 00:14:50,140
algorithms like against let's say sparse

00:14:48,100 --> 00:14:52,600
matrix vector math expector product

00:14:50,140 --> 00:14:55,600
dense matrix multiplication we we see

00:14:52,600 --> 00:14:57,910
this has all been implemented on only

00:14:55,600 --> 00:14:59,830
one device we see that there is not much

00:14:57,910 --> 00:15:03,910
difference between the native

00:14:59,830 --> 00:15:07,240
implementation and that of our our layer

00:15:03,910 --> 00:15:10,090
which with like the overhead incurred by

00:15:07,240 --> 00:15:13,060
earlier but our layer provides the

00:15:10,090 --> 00:15:15,700
capability to execute jobs on multiple

00:15:13,060 --> 00:15:17,350
different GPUs so the same thing with

00:15:15,700 --> 00:15:19,360
that of the partitions benchmark as well

00:15:17,350 --> 00:15:23,140
there's not much latency which is

00:15:19,360 --> 00:15:27,010
incurred so so what we discussed today

00:15:23,140 --> 00:15:28,510
was I just introduced about HP X as well

00:15:27,010 --> 00:15:31,420
as the integration of GPU processing

00:15:28,510 --> 00:15:34,600
along with the CPUs

00:15:31,420 --> 00:15:36,949
provided so this framework provides the

00:15:34,600 --> 00:15:39,529
opportunity to execute jobs on multiple

00:15:36,949 --> 00:15:41,120
GPUs in parallel along with the CPUs and

00:15:39,529 --> 00:15:47,870
synchronize them as and when you require

00:15:41,120 --> 00:15:55,149
it provide minimal overhead while adding

00:15:47,870 --> 00:15:58,579
a new common layer and then be sure I

00:15:55,149 --> 00:16:02,259
abroad example and implemented it using

00:15:58,579 --> 00:16:02,259
three GPUs thank you

00:16:03,560 --> 00:16:07,090
[Applause]

00:16:09,670 --> 00:16:13,770
any questions yes

00:16:14,139 --> 00:16:17,769
we do have we have a version for OpenCL

00:16:16,570 --> 00:16:20,949
and we are currently testing it right

00:16:17,769 --> 00:16:22,899
now so it is a common layer which you

00:16:20,949 --> 00:16:24,639
can call you know over like you can

00:16:22,899 --> 00:16:27,699
which you can supply like OpenCL kernel

00:16:24,639 --> 00:16:29,709
such CUDA kernels and be able to work

00:16:27,699 --> 00:16:34,630
with them in the same manner you don't

00:16:29,709 --> 00:16:38,070
need to deal with the differences or you

00:16:34,630 --> 00:16:38,070
know the language level differences

00:16:39,450 --> 00:16:42,080
yes

00:16:43,230 --> 00:16:46,160
he does very

00:16:46,490 --> 00:16:49,660
like your

00:16:52,770 --> 00:16:56,920
okay

00:16:54,680 --> 00:17:03,100
yes

00:16:56,920 --> 00:17:06,189
decide that so basically so basically

00:17:03,100 --> 00:17:08,919
what what you would need to do is like

00:17:06,189 --> 00:17:10,900
as I said developer the onus is still on

00:17:08,919 --> 00:17:13,510
you to identify the parts of the program

00:17:10,900 --> 00:17:16,089
which can be data parallel and then be

00:17:13,510 --> 00:17:18,910
able to just provide those data parallel

00:17:16,089 --> 00:17:21,790
parts to the GPUs for execution the

00:17:18,910 --> 00:17:28,530
parts which still need to be in sequence

00:17:21,790 --> 00:17:28,530
you can use HP x hundred and CP yes

00:17:37,240 --> 00:17:43,139
or dissertation

00:17:40,210 --> 00:17:43,139
separately

00:17:51,740 --> 00:18:02,460
[Music]

00:17:54,140 --> 00:18:07,059
okay can you run multiple skier on the

00:18:02,460 --> 00:18:09,919
same Colonel of multiple posts with 70

00:18:07,059 --> 00:18:12,290
yes so that was the that was the that

00:18:09,919 --> 00:18:14,299
was a whole point about different

00:18:12,290 --> 00:18:16,940
localities so each locality you can

00:18:14,299 --> 00:18:19,280
think of them as a separate host and

00:18:16,940 --> 00:18:21,679
then this is HBase is a layer on top of

00:18:19,280 --> 00:18:25,540
your host so that abstracts the host

00:18:21,679 --> 00:18:25,540

YouTube URL: https://www.youtube.com/watch?v=aRz19P_gbMI


