Title: Using AI Responsibly - Raymond Chan & Jeremy Osborn
Publication date: 2020-03-20
Playlist: FOSSASIA Summit 2020 - Artificial Intelligence
Description: 
	FOSSASIA Summit 2020 - Artificial Intelligence
Captions: 
	00:00:02,030 --> 00:00:08,849
- a 20/20 it's great to be here albeit

00:00:05,640 --> 00:00:10,889
remote so hope you're all well I hope

00:00:08,849 --> 00:00:14,849
everybody's keeping well we'd like to

00:00:10,889 --> 00:00:17,369
start this talk by taking a picture if

00:00:14,849 --> 00:00:19,619
you don't want to be this picture then

00:00:17,369 --> 00:00:23,189
please hide your face hold from

00:00:19,619 --> 00:00:30,900
something in front of your face okay

00:00:23,189 --> 00:00:32,070
thank you very much osage 2025 it's

00:00:30,900 --> 00:00:35,070
great to be back

00:00:32,070 --> 00:00:37,110
proud to see everybody here again as

00:00:35,070 --> 00:00:39,899
we've done in past talks we'd like to

00:00:37,110 --> 00:00:41,760
take a picture if you don't want

00:00:39,899 --> 00:00:44,850
yourself in this picture and please on

00:00:41,760 --> 00:00:50,489
something in front of your face okay

00:00:44,850 --> 00:00:55,140
let's go thank you very much so what a

00:00:50,489 --> 00:00:57,960
year it's been so we've had the 28th

00:00:55,140 --> 00:01:00,719
amendment in America the Trump amendment

00:00:57,960 --> 00:01:03,210
he's in office until the party gets

00:01:00,719 --> 00:01:05,760
sufficient votes to overtake that will

00:01:03,210 --> 00:01:07,920
trigger an election but this has to be a

00:01:05,760 --> 00:01:09,900
physical nationwide poll that overtakes

00:01:07,920 --> 00:01:11,939
sentiment analysis across all social

00:01:09,900 --> 00:01:14,310
media news outlets and whatever the

00:01:11,939 --> 00:01:16,170
government want to do it has to overtake

00:01:14,310 --> 00:01:21,299
the AI before any elections triggered

00:01:16,170 --> 00:01:24,600
otherwise it still the president toget

00:01:21,299 --> 00:01:26,850
25 predicted by a young and tracked

00:01:24,600 --> 00:01:29,040
through wearable tech was stopped in his

00:01:26,850 --> 00:01:34,049
tracks after only a thousand cases and

00:01:29,040 --> 00:01:36,540
no deaths Singapore continues to act as

00:01:34,049 --> 00:01:38,939
a bridge between areas

00:01:36,540 --> 00:01:41,100
it's acting is an area where you can get

00:01:38,939 --> 00:01:43,710
access to that balkanized wet soaking it

00:01:41,100 --> 00:01:48,600
logins to the Chinese web the Russian

00:01:43,710 --> 00:01:49,979
web EU where the American web and one of

00:01:48,600 --> 00:01:52,530
the interesting things about the

00:01:49,979 --> 00:01:54,960
American web is that it's got very

00:01:52,530 --> 00:01:57,479
aggressive AI in see comments you log on

00:01:54,960 --> 00:02:00,030
with bank details and a night predicts

00:01:57,479 --> 00:02:02,909
what what it is you want what it is you

00:02:00,030 --> 00:02:06,180
need sends it to you and deduct payments

00:02:02,909 --> 00:02:09,450
before you do anything and you then got

00:02:06,180 --> 00:02:13,569
to start to fight to get refunds through

00:02:09,450 --> 00:02:16,840
online ala courts we

00:02:13,569 --> 00:02:20,920
that drone swarms at exits of transport

00:02:16,840 --> 00:02:22,870
hubs using facial recognition technology

00:02:20,920 --> 00:02:26,909
these are tracking people with

00:02:22,870 --> 00:02:29,889
temperatures or other known suspects etc

00:02:26,909 --> 00:02:33,400
not sure what term they use but is

00:02:29,889 --> 00:02:34,900
starting to track people across lots of

00:02:33,400 --> 00:02:39,459
town was quite interesting seeing the

00:02:34,900 --> 00:02:42,359
swarms fly around them so who are we we

00:02:39,459 --> 00:02:44,590
are data kind a global nonprofit that

00:02:42,359 --> 00:02:46,659
harnesses the power of data science and

00:02:44,590 --> 00:02:51,129
AI in the service of humanity is a

00:02:46,659 --> 00:02:53,949
straight line they do pro bono data work

00:02:51,129 --> 00:02:56,620
through meetups and all sorts of other

00:02:53,949 --> 00:03:00,099
events that they organize for charities

00:02:56,620 --> 00:03:02,199
and nonprofits it starts to get the

00:03:00,099 --> 00:03:06,129
charities and nonprofits interested in

00:03:02,199 --> 00:03:08,949
being data-driven and it starts to

00:03:06,129 --> 00:03:11,260
introduce people who are data scientists

00:03:08,949 --> 00:03:15,370
and interesting data and data-driven

00:03:11,260 --> 00:03:19,239
things in introduced to the NGOs and the

00:03:15,370 --> 00:03:23,849
charities there are six chapters around

00:03:19,239 --> 00:03:27,759
the world two in America one in the UK

00:03:23,849 --> 00:03:34,530
one in India and one here in in

00:03:27,759 --> 00:03:37,419
Singapore so the results from the

00:03:34,530 --> 00:03:42,009
pictures that we've been taking so we've

00:03:37,419 --> 00:03:45,819
been analyzing the data through glean d

00:03:42,009 --> 00:03:49,150
biased data and we've been looking at

00:03:45,819 --> 00:03:50,650
the audience through and all sorts of

00:03:49,150 --> 00:03:53,069
other things through social media and

00:03:50,650 --> 00:03:57,400
various other things that we can get

00:03:53,069 --> 00:04:00,250
data allowed to have the data off and we

00:03:57,400 --> 00:04:02,859
are as a slice as we've been using d

00:04:00,250 --> 00:04:05,169
biased homomorphic encryption it's

00:04:02,859 --> 00:04:07,530
securely stored it's all auditable and

00:04:05,169 --> 00:04:10,209
only a few people have access to the

00:04:07,530 --> 00:04:13,269
unencrypted data this goal was to

00:04:10,209 --> 00:04:17,709
maximize ticket sales so maximize a

00:04:13,269 --> 00:04:19,440
revenue that phosphate got and so we can

00:04:17,709 --> 00:04:22,389
then play that back into the community

00:04:19,440 --> 00:04:26,800
giving away free tickets really good

00:04:22,389 --> 00:04:27,460
tickets bursaries and prizes the models

00:04:26,800 --> 00:04:29,919
were developed

00:04:27,460 --> 00:04:33,610
some were assessed by a panel against

00:04:29,919 --> 00:04:36,970
the Singapore framework for AI and the

00:04:33,610 --> 00:04:39,460
data sharing agreement we're going to

00:04:36,970 --> 00:04:41,910
present some charts on that because

00:04:39,460 --> 00:04:44,560
we've been having some issues with that

00:04:41,910 --> 00:04:46,630
and now we've got to change things

00:04:44,560 --> 00:04:49,720
especially now as this indigo framework

00:04:46,630 --> 00:04:51,340
is regulation we've got to be end you'll

00:04:49,720 --> 00:04:54,370
have a look at maybe re-engining

00:04:51,340 --> 00:04:57,220
engineering some of those things so

00:04:54,370 --> 00:04:59,440
here's some results divided the

00:04:57,220 --> 00:05:02,080
community up into these groups into five

00:04:59,440 --> 00:05:05,080
groups we normalized the attendance to

00:05:02,080 --> 00:05:07,090
the 2019 attendance that we've been

00:05:05,080 --> 00:05:09,580
tracking that we've had to go back

00:05:07,090 --> 00:05:11,320
through the data and track all that what

00:05:09,580 --> 00:05:14,919
we've been seeing is if some groups are

00:05:11,320 --> 00:05:17,199
being impacted and others have gone have

00:05:14,919 --> 00:05:20,680
been in the numbers and now we've got to

00:05:17,199 --> 00:05:22,900
go through all this data and say see why

00:05:20,680 --> 00:05:26,800
that was happening we may be discovering

00:05:22,900 --> 00:05:29,110
proxies and now the AI framework is

00:05:26,800 --> 00:05:30,940
regulation we need to start to fix this

00:05:29,110 --> 00:05:33,669
and we engineer some of some of the work

00:05:30,940 --> 00:05:35,650
that's being done had we been looking

00:05:33,669 --> 00:05:38,139
for proxies earlier thing maybe we could

00:05:35,650 --> 00:05:41,260
have avoided some of this bias and have

00:05:38,139 --> 00:05:43,900
fairer access but we can't go back in

00:05:41,260 --> 00:05:49,169
correcting now but what we can do is

00:05:43,900 --> 00:05:49,169
review the strategy going forward so

00:05:49,380 --> 00:05:55,479
thanks it was a she 2020 so I'd like to

00:05:53,620 --> 00:05:57,039
take a picture but maybe we shouldn't

00:05:55,479 --> 00:05:58,889
because we don't know what kind of

00:05:57,039 --> 00:06:04,449
things we're going to start to introduce

00:05:58,889 --> 00:06:07,330
into this so let's do a bit of a revisit

00:06:04,449 --> 00:06:10,440
early in 2019 the PDP a published a

00:06:07,330 --> 00:06:13,360
framework and they invited comments

00:06:10,440 --> 00:06:15,310
looking at this especially the

00:06:13,360 --> 00:06:18,909
contributors we thought there wasn't

00:06:15,310 --> 00:06:22,330
enough evolved involvement or protection

00:06:18,909 --> 00:06:24,550
of civil society so data kind people

00:06:22,330 --> 00:06:27,699
from effective altruism and interested

00:06:24,550 --> 00:06:29,039
individuals got together and wrote a

00:06:27,699 --> 00:06:32,199
response paragraph by paragraph

00:06:29,039 --> 00:06:33,759
suggesting improvement we didn't where

00:06:32,199 --> 00:06:35,349
the substance of the framework we

00:06:33,759 --> 00:06:38,310
thought it had add dimensions of work

00:06:35,349 --> 00:06:40,270
towards protections awareness and for

00:06:38,310 --> 00:06:41,230
protections and awareness and for the

00:06:40,270 --> 00:06:43,680
PDP a

00:06:41,230 --> 00:06:46,150
improvements that potentially stop

00:06:43,680 --> 00:06:48,910
litigation this was in the form of we

00:06:46,150 --> 00:06:52,870
called ourselves and yeah what did we

00:06:48,910 --> 00:06:56,230
call ourselves a working group nonprofit

00:06:52,870 --> 00:06:59,220
working group or AI some of the stuff

00:06:56,230 --> 00:07:05,500
got into the version 2 of the paper and

00:06:59,220 --> 00:07:09,100
we were credited with that but before we

00:07:05,500 --> 00:07:11,620
get to they are responsible AI I think a

00:07:09,100 --> 00:07:14,590
lot of this starts with with data and

00:07:11,620 --> 00:07:17,890
this is just the latest graphic in

00:07:14,590 --> 00:07:23,740
indicates in some of the major areas how

00:07:17,890 --> 00:07:25,630
much data is is going to going away from

00:07:23,740 --> 00:07:27,070
from all our interactions because we've

00:07:25,630 --> 00:07:29,170
moved increasingly from the physical to

00:07:27,070 --> 00:07:31,480
the digital when we generating vast

00:07:29,170 --> 00:07:34,000
amounts of data with every action that

00:07:31,480 --> 00:07:36,100
we take it pours onto servers with

00:07:34,000 --> 00:07:38,140
greater volume and greater speeds than

00:07:36,100 --> 00:07:41,980
ever before and we give up this data

00:07:38,140 --> 00:07:43,330
most literally is this data at all store

00:07:41,980 --> 00:07:45,250
the firehose whatever you want to call

00:07:43,330 --> 00:07:48,580
it that feeds the machines that generate

00:07:45,250 --> 00:07:51,430
revenue with new ever more accurate

00:07:48,580 --> 00:07:53,980
recommendation engines in and of itself

00:07:51,430 --> 00:07:55,870
this may not be really a problem more of

00:07:53,980 --> 00:07:58,750
an irritation of Thompson's interruption

00:07:55,870 --> 00:08:01,960
to Panthers and to some mixing slightly

00:07:58,750 --> 00:08:03,370
creepy when you're on another computer

00:08:01,960 --> 00:08:06,790
it seems to know what you're looking at

00:08:03,370 --> 00:08:09,970
on a different computer but everything

00:08:06,790 --> 00:08:12,400
is data these days to my mind one of the

00:08:09,970 --> 00:08:14,680
emerging battleground sees voice more we

00:08:12,400 --> 00:08:17,500
speak two things and more we record more

00:08:14,680 --> 00:08:20,590
data we are giving up to make for

00:08:17,500 --> 00:08:22,780
accurate engines that's a good thing

00:08:20,590 --> 00:08:24,880
isn't it we want the models to become

00:08:22,780 --> 00:08:26,710
more more accurate isn't it even more

00:08:24,880 --> 00:08:30,070
irritating when we recommend with hair

00:08:26,710 --> 00:08:32,979
products when we're bald and various

00:08:30,070 --> 00:08:34,419
mistakes like that but it seems to me

00:08:32,979 --> 00:08:35,770
that we live in this age of constant

00:08:34,419 --> 00:08:37,870
cognitive dissonance

00:08:35,770 --> 00:08:40,510
we know we're giving away data that may

00:08:37,870 --> 00:08:42,640
be a bad idea it's invasive but there's

00:08:40,510 --> 00:08:45,580
so much cool stuff out there and so much

00:08:42,640 --> 00:08:49,000
of it for free but it is how this data

00:08:45,580 --> 00:08:51,040
was taken Vince doing it and what he's

00:08:49,000 --> 00:08:52,960
done without data that can be part of

00:08:51,040 --> 00:08:54,940
the problem Shoshana Zubov

00:08:52,960 --> 00:08:56,790
you know book surveillance capital is

00:08:54,940 --> 00:08:59,530
talks about data being expropriated

00:08:56,790 --> 00:09:01,660
since there is no late apparent law

00:08:59,530 --> 00:09:03,550
against it in many cases data is taken

00:09:01,660 --> 00:09:07,420
without permission worked into product

00:09:03,550 --> 00:09:10,870
and sold back to us they seem to work on

00:09:07,420 --> 00:09:12,420
the but they they seem to work on the

00:09:10,870 --> 00:09:15,670
idea that it's better to ask forgiveness

00:09:12,420 --> 00:09:17,140
for permission and so if you look at

00:09:15,670 --> 00:09:18,940
products like Street View that was

00:09:17,140 --> 00:09:21,250
developed in this way we will drove

00:09:18,940 --> 00:09:23,140
around to lots of pictures since public

00:09:21,250 --> 00:09:26,140
spaces are free and it's been brilliant

00:09:23,140 --> 00:09:27,880
is being booms and many people but we

00:09:26,140 --> 00:09:30,550
are being habituated when you're giving

00:09:27,880 --> 00:09:32,980
away a lot and then paying to get it

00:09:30,550 --> 00:09:35,530
back if we object this it feels like a

00:09:32,980 --> 00:09:37,870
losing battle things like the right to

00:09:35,530 --> 00:09:40,300
be forgotten potentially damaging data

00:09:37,870 --> 00:09:42,480
removed who's being rolled back across

00:09:40,300 --> 00:09:45,940
the world and this can cause problems

00:09:42,480 --> 00:09:47,410
the case of made of soft ironing in Iran

00:09:45,940 --> 00:09:47,620
shows what can happen when mistakes are

00:09:47,410 --> 00:09:50,530
made

00:09:47,620 --> 00:09:52,060
the name was similar to one associated

00:09:50,530 --> 00:09:55,930
with shooting during the election in

00:09:52,060 --> 00:10:00,160
2009 a link was made after searching for

00:09:55,930 --> 00:10:02,380
a similar name and her her face an alias

00:10:00,160 --> 00:10:05,070
face was then posted as the face of the

00:10:02,380 --> 00:10:07,810
martyr the authorities approached her

00:10:05,070 --> 00:10:10,810
asking her to debunk the killing his

00:10:07,810 --> 00:10:12,460
fake new shoes for clearing person she

00:10:10,810 --> 00:10:13,840
wouldn't do that and what she was then

00:10:12,460 --> 00:10:15,730
told that she could be charged with

00:10:13,840 --> 00:10:19,630
treason threatened with imprisonment

00:10:15,730 --> 00:10:22,450
even death and eventually she became a

00:10:19,630 --> 00:10:26,410
refugee and made people think this is

00:10:22,450 --> 00:10:29,230
just the beginning a lot of this will

00:10:26,410 --> 00:10:32,950
get worse and worse as IOT and wearables

00:10:29,230 --> 00:10:35,790
become more ubiquitous gathering data

00:10:32,950 --> 00:10:38,830
about how we react to things and

00:10:35,790 --> 00:10:41,040
mistakes will be made you could say that

00:10:38,830 --> 00:10:43,420
this has been done for a long time but

00:10:41,040 --> 00:10:45,580
these days with the models using AI

00:10:43,420 --> 00:10:49,480
systems they are getting better at doing

00:10:45,580 --> 00:10:50,710
it at scale and that's presumably that

00:10:49,480 --> 00:10:52,450
will go for the mistakes and other

00:10:50,710 --> 00:10:54,460
mistakes as well things like to find

00:10:52,450 --> 00:10:56,620
Frank's personality model is being used

00:10:54,460 --> 00:10:59,770
against what we post on social media for

00:10:56,620 --> 00:11:02,710
quite a while it isn't it's what we post

00:10:59,770 --> 00:11:05,020
but we post the language used length of

00:11:02,710 --> 00:11:06,750
words and other parameters and this is

00:11:05,020 --> 00:11:09,330
met with some success in predicting

00:11:06,750 --> 00:11:10,890
which demographic deeper in now imagine

00:11:09,330 --> 00:11:12,870
that when so much more emotion is

00:11:10,890 --> 00:11:15,270
transmitted with our voice our manner

00:11:12,870 --> 00:11:17,340
deportment a video stream

00:11:15,270 --> 00:11:19,890
I got it with directing measurable

00:11:17,340 --> 00:11:22,440
parameters from wearables and we are

00:11:19,890 --> 00:11:27,090
setting ourselves up to the manipulated

00:11:22,440 --> 00:11:28,230
big time that's a but then you have a

00:11:27,090 --> 00:11:29,790
look at things and how much more

00:11:28,230 --> 00:11:31,680
information is gathered from watching

00:11:29,790 --> 00:11:33,710
someone react to a video clip with micro

00:11:31,680 --> 00:11:38,160
emotions surprise anger confusion

00:11:33,710 --> 00:11:40,440
sympathetic both sort of event surveys I

00:11:38,160 --> 00:11:42,570
liked it couple that with heartbeat

00:11:40,440 --> 00:11:44,310
blood pressure breathing rates arousal

00:11:42,570 --> 00:11:46,740
anxiety states you know something that

00:11:44,310 --> 00:11:47,610
says I can be manipulative far more

00:11:46,740 --> 00:11:50,580
successfully

00:11:47,610 --> 00:11:57,180
I took my applying Micra stimulating

00:11:50,580 --> 00:12:01,050
stimuli images text the sound to hurt in

00:11:57,180 --> 00:12:02,790
mind not always sure maybe it's not on

00:12:01,050 --> 00:12:05,220
issuing something positive that's really

00:12:02,790 --> 00:12:07,590
cool that we didn't know we wanted but

00:12:05,220 --> 00:12:09,660
when democracy is at stake and money

00:12:07,590 --> 00:12:12,270
buys more airtime a social media time

00:12:09,660 --> 00:12:14,940
than the next guy then inequalities in

00:12:12,270 --> 00:12:16,680
the world are amplified it might be okay

00:12:14,940 --> 00:12:19,140
if we're not towards a healthier

00:12:16,680 --> 00:12:21,750
lifestyle like bad sugary food and drink

00:12:19,140 --> 00:12:24,870
so our health can be nudged towards

00:12:21,750 --> 00:12:27,060
better things but if I'm sold remedies

00:12:24,870 --> 00:12:29,750
to cure my bad day behavior or insurance

00:12:27,060 --> 00:12:32,430
premiums go up then we can create

00:12:29,750 --> 00:12:33,870
socio-economic inequalities and there

00:12:32,430 --> 00:12:36,089
are people who can't afford to get

00:12:33,870 --> 00:12:38,070
better and might be trapped in a policy

00:12:36,089 --> 00:12:40,890
that determines the food that I get and

00:12:38,070 --> 00:12:42,660
that can track people further as per a

00:12:40,890 --> 00:12:44,640
diet can lead to poor academic

00:12:42,660 --> 00:12:48,810
achievement and no way to get out of

00:12:44,640 --> 00:12:50,339
this trap and on social media experience

00:12:48,810 --> 00:12:53,070
have been done on millions of people and

00:12:50,339 --> 00:12:55,920
published in academic journals now show

00:12:53,070 --> 00:12:57,870
mark shifting voting behavior with a B

00:12:55,920 --> 00:13:00,360
type testing in social media strains

00:12:57,870 --> 00:13:02,339
these experiments do not have to follow

00:13:00,360 --> 00:13:05,010
the same ethical standards that academia

00:13:02,339 --> 00:13:06,750
does in the US I believe the common rule

00:13:05,010 --> 00:13:09,990
introduced after many manipulative

00:13:06,750 --> 00:13:11,550
experiments in the 1960s but academics

00:13:09,990 --> 00:13:14,160
and now knocking on the doors of some

00:13:11,550 --> 00:13:15,750
companies to do this kind of research do

00:13:14,160 --> 00:13:17,610
they live with a cognitive dissidence

00:13:15,750 --> 00:13:20,550
and pretend they're doing good or are

00:13:17,610 --> 00:13:23,400
the unethical researchers next question

00:13:20,550 --> 00:13:27,420
these kind of things with respect to our

00:13:23,400 --> 00:13:30,030
privacy giving our dates away could be a

00:13:27,420 --> 00:13:32,160
violation of privacy we feel what does

00:13:30,030 --> 00:13:35,970
it feel like our privacy violence's

00:13:32,160 --> 00:13:38,040
there's research that talks about that

00:13:35,970 --> 00:13:40,710
we behave as if our personal physical

00:13:38,040 --> 00:13:41,490
space is being violated and how do we

00:13:40,710 --> 00:13:43,440
deal with this

00:13:41,490 --> 00:13:45,270
there aren't cultural norms is Italy

00:13:43,440 --> 00:13:47,670
that we can learn and we start to a

00:13:45,270 --> 00:13:50,880
deity and respect in different cultural

00:13:47,670 --> 00:13:53,940
areas and we need to do that online as

00:13:50,880 --> 00:13:56,760
well somehow but after that we need

00:13:53,940 --> 00:13:59,100
control of our data who has it who takes

00:13:56,760 --> 00:14:00,930
in and what they do with it in the voice

00:13:59,100 --> 00:14:02,460
battleground companies have been taken

00:14:00,930 --> 00:14:05,430
to task about snooping on our daily

00:14:02,460 --> 00:14:09,240
interactions televisions digital

00:14:05,430 --> 00:14:11,760
assistants toys even and some of this

00:14:09,240 --> 00:14:14,490
data is being transcribed by humans that

00:14:11,760 --> 00:14:16,860
people are reading this and for those

00:14:14,490 --> 00:14:18,870
who say they well you've got nothing to

00:14:16,860 --> 00:14:21,000
Harleys do you really want your private

00:14:18,870 --> 00:14:23,580
conversations listening to transcribed

00:14:21,000 --> 00:14:25,290
and potentially sold on how do you react

00:14:23,580 --> 00:14:26,970
if somebody's following me around all

00:14:25,290 --> 00:14:30,870
them and making notes on everything that

00:14:26,970 --> 00:14:33,450
you do nothing react very well so I do

00:14:30,870 --> 00:14:35,820
it physically would you have all your

00:14:33,450 --> 00:14:38,400
phone records bank records location

00:14:35,820 --> 00:14:40,790
records publish publicly not the same as

00:14:38,400 --> 00:14:44,070
a PPI that's often talked about in in

00:14:40,790 --> 00:14:46,200
privacy terms but we all have something

00:14:44,070 --> 00:14:49,230
to hide we all have something we want to

00:14:46,200 --> 00:14:51,420
keep private my kingdom is a mapping out

00:14:49,230 --> 00:14:53,160
our living spaces and patents on smart

00:14:51,420 --> 00:14:55,710
beds recording and analyzing sleep

00:14:53,160 --> 00:14:58,140
patterns remotely children's toys have

00:14:55,710 --> 00:14:59,760
been co-opted and worse than that not

00:14:58,140 --> 00:15:01,290
only are they recording and passing on

00:14:59,760 --> 00:15:02,640
your child's conversations with Ettore

00:15:01,290 --> 00:15:05,760
but they're also asking personal

00:15:02,640 --> 00:15:07,470
questions or they were I think you've

00:15:05,760 --> 00:15:09,330
been banned in certain areas and

00:15:07,470 --> 00:15:11,730
questions about where they live details

00:15:09,330 --> 00:15:13,980
of other family members this can't be

00:15:11,730 --> 00:15:15,780
right and strange conditions are put

00:15:13,980 --> 00:15:19,050
into T's and C's that accompany these

00:15:15,780 --> 00:15:22,020
these devices no upgrades may not work

00:15:19,050 --> 00:15:25,320
as expected unless we can take your data

00:15:22,020 --> 00:15:28,500
I'm not sure the liberty some of these

00:15:25,320 --> 00:15:29,880
things make sense to so once that

00:15:28,500 --> 00:15:32,820
privacy is violated

00:15:29,880 --> 00:15:34,920
once our data is expropriated do we have

00:15:32,820 --> 00:15:37,410
any say are we then

00:15:34,920 --> 00:15:40,259
manipulated by increasingly accurate AI

00:15:37,410 --> 00:15:43,920
that nudges us towards a herb blender is

00:15:40,259 --> 00:15:46,199
that the kind of society we want some of

00:15:43,920 --> 00:15:48,869
this has tried to be reflected in some

00:15:46,199 --> 00:15:51,209
of the frameworks that have been

00:15:48,869 --> 00:15:58,379
published around the world and my

00:15:51,209 --> 00:15:59,970
colleague Raymond well thank you Jeremy

00:15:58,379 --> 00:16:02,429
for this excellent introduction to the

00:15:59,970 --> 00:16:04,049
AI governance framework for my part of

00:16:02,429 --> 00:16:05,850
the talk I will focus on the big picture

00:16:04,049 --> 00:16:07,739
trends the AI governance proposed by

00:16:05,850 --> 00:16:09,929
governments nonprofits and countries all

00:16:07,739 --> 00:16:13,799
available to mitigate the negative

00:16:09,929 --> 00:16:15,989
impacts on people and society we use the

00:16:13,799 --> 00:16:19,619
paper the global landscape of AI ethics

00:16:15,989 --> 00:16:24,329
guidelines that was published in July

00:16:19,619 --> 00:16:26,819
2019 and contains 84 published AI

00:16:24,329 --> 00:16:28,699
governance frameworks these frameworks

00:16:26,819 --> 00:16:31,790
cover the following broad themes

00:16:28,699 --> 00:16:35,519
transparency justice and fairness

00:16:31,790 --> 00:16:39,749
non-maleficence responsibilities privacy

00:16:35,519 --> 00:16:43,429
freedom and autonomy Trust but never but

00:16:39,749 --> 00:16:47,189
now benevolence sustainability dignity

00:16:43,429 --> 00:16:49,439
solidarity I will cover only the first

00:16:47,189 --> 00:16:51,569
seven in this talk as the last four are

00:16:49,439 --> 00:16:53,850
less common for each of the principles

00:16:51,569 --> 00:16:56,040
Lyster I provide an example where the

00:16:53,850 --> 00:16:57,299
principal has either a positive or a

00:16:56,040 --> 00:17:00,889
negative outcome

00:16:57,299 --> 00:17:02,999
spoiler alert they are mostly negative

00:17:00,889 --> 00:17:05,159
framework development has mostly

00:17:02,999 --> 00:17:06,600
occurred in Europe and North America but

00:17:05,159 --> 00:17:08,839
a small number of frameworks have been

00:17:06,600 --> 00:17:12,360
developed in Asia including Singapore

00:17:08,839 --> 00:17:14,309
since the paper has been accepted to

00:17:12,360 --> 00:17:19,230
more important frameworks as also have

00:17:14,309 --> 00:17:23,459
also emerged from China the first team

00:17:19,230 --> 00:17:26,819
I'm gonna cover is concurrency this is

00:17:23,459 --> 00:17:28,049
also part of explain ability interpreted

00:17:26,819 --> 00:17:31,110
interpretability

00:17:28,049 --> 00:17:33,480
and disclosure this generally boils down

00:17:31,110 --> 00:17:35,340
to using machine learning or statistical

00:17:33,480 --> 00:17:36,929
models that can explain the reason

00:17:35,340 --> 00:17:39,750
behind the decisions that were made a

00:17:36,929 --> 00:17:41,639
positive example of this our credit

00:17:39,750 --> 00:17:43,950
scores in the u.s. that are used to

00:17:41,639 --> 00:17:46,860
determine eligibility for loans and

00:17:43,950 --> 00:17:48,750
credit cards everyone has a right to

00:17:46,860 --> 00:17:50,460
receive a credit report that explains

00:17:48,750 --> 00:17:53,220
all the information that goes into the

00:17:50,460 --> 00:17:56,490
skull like previous mortgage utility

00:17:53,220 --> 00:17:58,890
payments total debt and other things

00:17:56,490 --> 00:18:00,780
like that this transparency reduces the

00:17:58,890 --> 00:18:02,490
likelihood of miscommunication and gives

00:18:00,780 --> 00:18:07,080
customers an idea on how to improve

00:18:02,490 --> 00:18:09,690
their scores the next theme is justice

00:18:07,080 --> 00:18:12,200
and fairness it also covers include

00:18:09,690 --> 00:18:15,420
inclusion diversity and accessibility

00:18:12,200 --> 00:18:17,760
this is going to be a negative example

00:18:15,420 --> 00:18:20,040
some years back a large e-commerce

00:18:17,760 --> 00:18:23,610
company built a tool to automate

00:18:20,040 --> 00:18:25,740
recruiting decisions give the system 100

00:18:23,610 --> 00:18:28,680
resumes and it would give you the top 5

00:18:25,740 --> 00:18:31,230
candidates unfortunately the algorithm

00:18:28,680 --> 00:18:33,300
was very biased against women it turned

00:18:31,230 --> 00:18:35,490
out that the root cause of this was that

00:18:33,300 --> 00:18:37,740
men was over well over-represented among

00:18:35,490 --> 00:18:40,650
highest in the historical data set that

00:18:37,740 --> 00:18:42,240
was used to train the model fortunately

00:18:40,650 --> 00:18:45,020
the system was decommissioned after a

00:18:42,240 --> 00:18:47,370
year once the issue was realized as

00:18:45,020 --> 00:18:49,110
Jerry mentioned previously this would be

00:18:47,370 --> 00:18:55,710
very bad for your company if you did

00:18:49,110 --> 00:18:58,730
this the next theme is non-maleficence

00:18:55,710 --> 00:19:02,520
which covers safety prevention and

00:18:58,730 --> 00:19:04,440
security like the Hippocratic oath taken

00:19:02,520 --> 00:19:06,290
by medical doctors it is the principle

00:19:04,440 --> 00:19:08,640
of first do no harm

00:19:06,290 --> 00:19:10,470
just this year dozens of women in

00:19:08,640 --> 00:19:12,360
Singapore had the imager stolen from

00:19:10,470 --> 00:19:15,330
social media sites and doctor using the

00:19:12,360 --> 00:19:17,400
tea new app the app allows the user to

00:19:15,330 --> 00:19:19,290
replace porn star faces in photographic

00:19:17,400 --> 00:19:21,960
images with faces of targets at women

00:19:19,290 --> 00:19:23,340
such doctored images were subsequently

00:19:21,960 --> 00:19:25,950
uploaded to sex forums

00:19:23,340 --> 00:19:27,810
note that creating folding or possessing

00:19:25,950 --> 00:19:30,450
these kinds of images is a form of

00:19:27,810 --> 00:19:32,040
sexual harassment and is covered in the

00:19:30,450 --> 00:19:34,800
recently passed protection from

00:19:32,040 --> 00:19:36,660
harassment act following the outcry the

00:19:34,800 --> 00:19:38,640
app was removed from the App Store but

00:19:36,660 --> 00:19:40,520
the harm to this woman has already been

00:19:38,640 --> 00:19:43,440
done

00:19:40,520 --> 00:19:46,530
the female responsibility also covers

00:19:43,440 --> 00:19:51,240
accountability liability and acting with

00:19:46,530 --> 00:19:53,820
integrity in 2019 650k

00:19:51,240 --> 00:19:56,550
leukemia had to flee Myanmar of

00:19:53,820 --> 00:19:58,580
Bangladesh falling prosecution a lot of

00:19:56,550 --> 00:20:02,190
that violence was fueled by hate speech

00:19:58,580 --> 00:20:02,610
spread on social media in Myanmar social

00:20:02,190 --> 00:20:04,620
media

00:20:02,610 --> 00:20:07,380
is in fact synonymous with the Internet

00:20:04,620 --> 00:20:09,269
due to the high penetration rate what

00:20:07,380 --> 00:20:10,830
level responsibility there's a social

00:20:09,269 --> 00:20:12,690
media platform there or hate speech

00:20:10,830 --> 00:20:15,120
propagate that through its content

00:20:12,690 --> 00:20:17,510
recommendation system in many legal

00:20:15,120 --> 00:20:20,190
jurisdictions the answer is quite a bit

00:20:17,510 --> 00:20:22,769
especially in the last couple of years

00:20:20,190 --> 00:20:26,970
when several countries pass anti fake

00:20:22,769 --> 00:20:29,490
news loss today credit the social media

00:20:26,970 --> 00:20:31,289
platform in question has significantly

00:20:29,490 --> 00:20:33,750
increased the number of Burmese speaking

00:20:31,289 --> 00:20:35,970
content moderators to deal with such

00:20:33,750 --> 00:20:38,250
issues but still this may not be enough

00:20:35,970 --> 00:20:40,980
especially considering our current kovat

00:20:38,250 --> 00:20:42,990
19 pandemic situation and all the fake

00:20:40,980 --> 00:20:45,169
news and cures being passed around on

00:20:42,990 --> 00:20:47,370
the Internet

00:20:45,169 --> 00:20:50,090
privacy is also important party I

00:20:47,370 --> 00:20:53,669
governance as well as data governance

00:20:50,090 --> 00:20:55,710
once again in 2019 we saw the year that

00:20:53,669 --> 00:20:57,750
news broke about Cambridge analytical

00:20:55,710 --> 00:20:59,779
how is the data from millions of users

00:20:57,750 --> 00:21:02,370
to feed their political ad campaigns

00:20:59,779 --> 00:21:03,840
this was very much against the terms and

00:21:02,370 --> 00:21:05,909
conditions of service for the social

00:21:03,840 --> 00:21:09,269
media platform when they took their data

00:21:05,909 --> 00:21:11,850
from not only did this part of political

00:21:09,269 --> 00:21:13,350
firestorm but Cambridge Anika analytical

00:21:11,850 --> 00:21:15,870
was eventually shuttered in a hail of

00:21:13,350 --> 00:21:20,159
contribution controversy for abusing the

00:21:15,870 --> 00:21:22,409
use of this personal data finally we

00:21:20,159 --> 00:21:24,710
cover freedom and autonomy which

00:21:22,409 --> 00:21:27,840
includes things like consent

00:21:24,710 --> 00:21:29,880
self-determination and empowerment make

00:21:27,840 --> 00:21:32,130
sure your customer customers know what

00:21:29,880 --> 00:21:34,080
they are getting into and have the

00:21:32,130 --> 00:21:37,250
freedom to decide what's in their best

00:21:34,080 --> 00:21:40,559
interest a positive example would be

00:21:37,250 --> 00:21:42,840
making your Terms of Service easily

00:21:40,559 --> 00:21:45,269
understandable the picture contains the

00:21:42,840 --> 00:21:47,700
Terms of Service of a number of very

00:21:45,269 --> 00:21:50,340
well-known online platforms as you can

00:21:47,700 --> 00:21:54,510
see they are mind-numbingly long don't

00:21:50,340 --> 00:21:56,370
do this ask for consent as and when you

00:21:54,510 --> 00:21:58,649
need to instead of getting it upfront

00:21:56,370 --> 00:22:00,120
for everything possible as generally

00:21:58,649 --> 00:22:01,860
mentioned previously don't let

00:22:00,120 --> 00:22:05,010
organizations push you around to

00:22:01,860 --> 00:22:06,750
maximize profits in summary whichever

00:22:05,010 --> 00:22:09,299
part of all you are in there are

00:22:06,750 --> 00:22:11,039
frameworks coming into force many of

00:22:09,299 --> 00:22:14,250
them will become laws and regulations in

00:22:11,039 --> 00:22:15,920
the near term the faster we start

00:22:14,250 --> 00:22:17,750
considering them in a

00:22:15,920 --> 00:22:20,390
yeah architectures the more likely that

00:22:17,750 --> 00:22:23,360
we don't run afoul when they eventually

00:22:20,390 --> 00:22:24,740
become law of the land thank you very

00:22:23,360 --> 00:22:31,220
much and I'll hand it over back to

00:22:24,740 --> 00:22:34,210
Jeremy to close out okay thank you very

00:22:31,220 --> 00:22:37,270
much Raymond some really good stuff

00:22:34,210 --> 00:22:41,060
hopefully soon people lot to think about

00:22:37,270 --> 00:22:43,130
and I think that people here can help

00:22:41,060 --> 00:22:46,820
this can be a little bit of a call to

00:22:43,130 --> 00:22:48,860
action reopen source software is a

00:22:46,820 --> 00:22:51,290
responsible movement out of which I

00:22:48,860 --> 00:22:53,590
think will come protections and ways of

00:22:51,290 --> 00:22:56,720
working to help mitigate bias prejudice

00:22:53,590 --> 00:22:59,360
wrong outcomes as individuals you can

00:22:56,720 --> 00:23:02,060
question if the outcomes and motivations

00:22:59,360 --> 00:23:04,190
in applying AI align with the values of

00:23:02,060 --> 00:23:07,430
the organization does the organization

00:23:04,190 --> 00:23:08,900
have any values building monitors and

00:23:07,430 --> 00:23:11,980
checks at regular interviews in the

00:23:08,900 --> 00:23:15,110
pipeline including overtime maybe

00:23:11,980 --> 00:23:17,930
instrument certain areas so the outcomes

00:23:15,110 --> 00:23:20,060
are within expected boundaries I know

00:23:17,930 --> 00:23:22,520
that's not always possible deep neural

00:23:20,060 --> 00:23:24,800
nets and things like that but building

00:23:22,520 --> 00:23:27,110
alerts when you're finding outliers so

00:23:24,800 --> 00:23:29,450
that an investigation can be done

00:23:27,110 --> 00:23:31,730
perhaps these investigations can be done

00:23:29,450 --> 00:23:35,210
with external and internal ethical

00:23:31,730 --> 00:23:37,850
review boards involving legal and social

00:23:35,210 --> 00:23:41,180
scholars and other areas not just

00:23:37,850 --> 00:23:44,750
technical people maybe we can set up

00:23:41,180 --> 00:23:46,960
cities and juries where AI where a is

00:23:44,750 --> 00:23:49,760
going to be implemented by public bodies

00:23:46,960 --> 00:23:51,770
to gauge the temperature of people upon

00:23:49,760 --> 00:23:54,500
which these manipulations will be

00:23:51,770 --> 00:23:57,980
happening do your work well do what you

00:23:54,500 --> 00:24:00,580
do best and do your work responsibly be

00:23:57,980 --> 00:24:03,470
brave and be counted

00:24:00,580 --> 00:24:04,300
thank you very much so we're from data

00:24:03,470 --> 00:24:08,870
kind

00:24:04,300 --> 00:24:10,940
we look data but we need to be careful

00:24:08,870 --> 00:24:14,620
with it okay thank you very much

00:24:10,940 --> 00:24:14,620

YouTube URL: https://www.youtube.com/watch?v=Qdy09_19zDw


