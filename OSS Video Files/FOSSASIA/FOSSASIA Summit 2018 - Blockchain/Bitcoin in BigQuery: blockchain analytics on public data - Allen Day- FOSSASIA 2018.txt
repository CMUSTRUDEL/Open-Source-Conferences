Title: Bitcoin in BigQuery: blockchain analytics on public data - Allen Day- FOSSASIA 2018
Publication date: 2018-03-31
Playlist: FOSSASIA Summit 2018 - Blockchain
Description: 
	Speaker: Allen Day, Science Advocate Google
Info: https://2018.fossasia.org/event/speakers.html#allen-day3357

Cryptocurrencies have captured the imagination of technologists, financiers, and economists. Perhaps even more intriguing are the long-term, diverse applications of the blockchain. By increasing transparency of cryptocurrency systems, the contained data becomes more accessible and useful.The Bitcoin blockchain data are now available for exploration with BigQuery. All historical data are in the big query-public-data:bitcoin_blockchain dataset, which updates every 10 minutes. We hope that by making the data more transparent, users of the data can gain a deeper understanding of how cryptocurrency systems function and how they might best be used for the benefit of society.

Room: Lecture Theatre
Track: Blochain
Date: Friday, 23rd March, 14:00 - 14:40

Event Page: http://2018.fossasia.org
Follow FOSSASIA on Twitter: https://twitter.com/fossasia/
Like FOSSASIA on Facebook: https://www.facebook.com/fossasia/

Produced by Engineers.SG
Captions: 
	00:00:00,030 --> 00:00:03,560
and I've got a lot of slides

00:00:09,700 --> 00:00:14,770
all right good afternoon everyone

00:00:12,019 --> 00:00:18,320
we've got Alan here from Google right

00:00:14,770 --> 00:00:20,599
he's gonna talk to us about all the

00:00:18,320 --> 00:00:23,600
blockchain information and how to query

00:00:20,599 --> 00:00:25,340
it in bigquery you got any friends they

00:00:23,600 --> 00:00:26,300
sold their Bitcoin you want to know how

00:00:25,340 --> 00:00:29,750
much money they're made

00:00:26,300 --> 00:00:32,419
ask him how to do it yeah we have all

00:00:29,750 --> 00:00:34,489
the blogging Vida Wow all right here we

00:00:32,419 --> 00:00:35,780
go so yeah my name is Alan Day you can

00:00:34,489 --> 00:00:38,870
follow me on Twitter that's my username

00:00:35,780 --> 00:00:40,940
up there I'm to be talking about a data

00:00:38,870 --> 00:00:43,609
set that we released last month

00:00:40,940 --> 00:00:46,219
co-author and I for bringing the Bitcoin

00:00:43,609 --> 00:00:48,519
data into into bigquery what does that

00:00:46,219 --> 00:00:53,749
mean what are the implications all right

00:00:48,519 --> 00:00:58,069
make sure I can drive my deck where's my

00:00:53,749 --> 00:01:01,729
mouse okay

00:00:58,069 --> 00:01:04,360
yeah the agenda let's just go let's just

00:01:01,729 --> 00:01:06,530
go okay so first of all let's start here

00:01:04,360 --> 00:01:08,990
take photos if you want to visit this

00:01:06,530 --> 00:01:10,430
URL shortened links there's a blog post

00:01:08,990 --> 00:01:12,740
that describes what I'm gonna be talking

00:01:10,430 --> 00:01:14,690
about in the first half of my talk and

00:01:12,740 --> 00:01:16,310
there's a data set as well this is so

00:01:14,690 --> 00:01:21,890
that you can get to the the bigquery

00:01:16,310 --> 00:01:26,780
public data set of the Bitcoin data just

00:01:21,890 --> 00:01:30,380
to give you some sense of what's what's

00:01:26,780 --> 00:01:32,840
in here here's a query looking at the

00:01:30,380 --> 00:01:34,520
number of addresses that are receiving

00:01:32,840 --> 00:01:36,920
money per day so it's a measure of the

00:01:34,520 --> 00:01:38,480
number of transactions as you can see

00:01:36,920 --> 00:01:40,640
it's it's quite simple to do the query

00:01:38,480 --> 00:01:42,410
and here on the the right side I'm

00:01:40,640 --> 00:01:44,810
showing a visualization using data

00:01:42,410 --> 00:01:47,090
studio so this is a connector that can

00:01:44,810 --> 00:01:51,140
read for my sequel or post press or

00:01:47,090 --> 00:01:52,550
bigquery or CSV files cool sheets etc we

00:01:51,140 --> 00:01:54,350
use this to do the visualizations in the

00:01:52,550 --> 00:01:57,880
blog post you can do iframe embeds you

00:01:54,350 --> 00:02:02,030
can also embed it inside of slide shares

00:01:57,880 --> 00:02:04,250
so just to give you some sense of why

00:02:02,030 --> 00:02:07,070
why there's an importance to get the

00:02:04,250 --> 00:02:09,289
data from the Bitcoin database into

00:02:07,070 --> 00:02:11,000
something like bigquery first we need to

00:02:09,289 --> 00:02:11,830
step back and talk about different types

00:02:11,000 --> 00:02:14,290
of

00:02:11,830 --> 00:02:16,870
can I turn this down at all I've got

00:02:14,290 --> 00:02:21,120
some really bad feedback and the a/v

00:02:16,870 --> 00:02:22,960
room turned me down thank you

00:02:21,120 --> 00:02:24,280
you didn't understand that there's

00:02:22,960 --> 00:02:27,400
basically two different types of

00:02:24,280 --> 00:02:29,830
workloads for databases one is for

00:02:27,400 --> 00:02:32,670
online transaction processing so LTP

00:02:29,830 --> 00:02:35,530
type systems are basically very good at

00:02:32,670 --> 00:02:37,720
doing high transaction volumes they can

00:02:35,530 --> 00:02:39,610
ingest data very quickly it's highly

00:02:37,720 --> 00:02:41,140
normalized and it's designed for a

00:02:39,610 --> 00:02:42,640
specific type of operations because they

00:02:41,140 --> 00:02:44,190
want to ingest data very quickly so they

00:02:42,640 --> 00:02:46,720
do one thing they do it really well

00:02:44,190 --> 00:02:50,470
OLAP type systems online analytic

00:02:46,720 --> 00:02:52,210
processing Rd normalized so that they

00:02:50,470 --> 00:02:54,040
can accommodate many different types of

00:02:52,210 --> 00:02:56,260
queries not trying to anticipate what

00:02:54,040 --> 00:02:58,810
the workload looks like and typically

00:02:56,260 --> 00:03:01,270
the data Rd normalized or duplicated in

00:02:58,810 --> 00:03:02,830
in various ways to support slicing the

00:03:01,270 --> 00:03:04,360
data across multiple dimensions right

00:03:02,830 --> 00:03:07,060
because usually a row you can consider

00:03:04,360 --> 00:03:09,940
the field as being dimensions on the on

00:03:07,060 --> 00:03:11,290
the the ID of the row and there's

00:03:09,940 --> 00:03:15,490
various ways and we want to aggregate

00:03:11,290 --> 00:03:17,080
summarized join sub select according to

00:03:15,490 --> 00:03:20,620
those fields and so that requires having

00:03:17,080 --> 00:03:24,010
some indexes or caches of the data to

00:03:20,620 --> 00:03:25,270
help you do that efficiently yes these

00:03:24,010 --> 00:03:29,170
kind of systems are designed for

00:03:25,270 --> 00:03:31,600
flexible operations if we add another

00:03:29,170 --> 00:03:33,190
constraint on to an OLTP system that

00:03:31,600 --> 00:03:36,430
it's append only that basically

00:03:33,190 --> 00:03:38,830
describes the Bitcoin or blockchain type

00:03:36,430 --> 00:03:40,480
systems with the exception that they're

00:03:38,830 --> 00:03:45,250
not particularly fast in the case of

00:03:40,480 --> 00:03:47,440
Bitcoin whereas we can consider a

00:03:45,250 --> 00:03:49,720
bigquery to be at least Google's product

00:03:47,440 --> 00:03:52,390
for doing an OLAP type system so if you

00:03:49,720 --> 00:03:55,680
want to have both of these capabilities

00:03:52,390 --> 00:03:58,660
of both having good transactional

00:03:55,680 --> 00:04:02,140
appending a transaction handling as well

00:03:58,660 --> 00:04:03,310
as good analytics processing you need to

00:04:02,140 --> 00:04:05,200
have two different systems to support

00:04:03,310 --> 00:04:06,820
both types of workloads and so that's

00:04:05,200 --> 00:04:09,340
what's the motivation for preparing the

00:04:06,820 --> 00:04:10,480
data set I'm talking about today just to

00:04:09,340 --> 00:04:12,700
give you some example of what is

00:04:10,480 --> 00:04:14,410
bigquery it's basically a no operations

00:04:12,700 --> 00:04:17,919
data warehousing solution from Google

00:04:14,410 --> 00:04:20,020
cloud we can host terabytes many there's

00:04:17,919 --> 00:04:22,590
largest data set is in the petabyte

00:04:20,020 --> 00:04:24,700
range of data stored in bigquery and

00:04:22,590 --> 00:04:25,600
it's quite easy to use because you can

00:04:24,700 --> 00:04:27,130
use a standard sequin

00:04:25,600 --> 00:04:28,510
face whereas a Bitcoin there's there's

00:04:27,130 --> 00:04:31,120
really no way to query the data in that

00:04:28,510 --> 00:04:36,250
manner it's too optimized and normalized

00:04:31,120 --> 00:04:38,320
and in a binary format bigquery runs on

00:04:36,250 --> 00:04:40,030
on Google's data centers we consider

00:04:38,320 --> 00:04:41,170
data centers as being a computer just a

00:04:40,030 --> 00:04:44,860
bit of background as to how this

00:04:41,170 --> 00:04:47,040
bigquery system works we don't actually

00:04:44,860 --> 00:04:49,000
don't do any indexing so the matter of

00:04:47,040 --> 00:04:50,470
speeding thing ups that we're doing is

00:04:49,000 --> 00:04:52,900
we're replicating the data across many

00:04:50,470 --> 00:04:54,400
different IO devices the spindles in the

00:04:52,900 --> 00:04:56,470
data center and so when you're doing a

00:04:54,400 --> 00:04:58,660
query we're just partitioning up the

00:04:56,470 --> 00:05:00,070
scan and scanning across all these

00:04:58,660 --> 00:05:02,470
partitions of the data and then

00:05:00,070 --> 00:05:03,940
aggregating into some compute nodes so

00:05:02,470 --> 00:05:05,410
we have the system called board this is

00:05:03,940 --> 00:05:06,880
the basis for another project you may be

00:05:05,410 --> 00:05:09,460
aware of called kubernetes which is an

00:05:06,880 --> 00:05:12,330
open source project for handling virtual

00:05:09,460 --> 00:05:14,440
machines and this is the the type of

00:05:12,330 --> 00:05:15,640
compute container that's that's reading

00:05:14,440 --> 00:05:17,380
the data from the spindles I was just

00:05:15,640 --> 00:05:18,780
showing you and of course that it has to

00:05:17,380 --> 00:05:20,860
move across the network so we have a

00:05:18,780 --> 00:05:22,720
proprietary network at Google - called

00:05:20,860 --> 00:05:25,240
Jupiter we build all our own network

00:05:22,720 --> 00:05:27,040
switches and so on not not all the Shelf

00:05:25,240 --> 00:05:29,310
commodities stuff and so we bundle all

00:05:27,040 --> 00:05:31,300
these three different components into

00:05:29,310 --> 00:05:33,610
into the system for bigquery I was

00:05:31,300 --> 00:05:36,640
talking about data would be ingested in

00:05:33,610 --> 00:05:39,970
dropped into storage somewhere and then

00:05:36,640 --> 00:05:41,980
moved over to compute possibly writing

00:05:39,970 --> 00:05:43,390
intermediate results into storage back

00:05:41,980 --> 00:05:45,760
and forth ultimately giving the result

00:05:43,390 --> 00:05:47,110
out to the sequel consumer depicted here

00:05:45,760 --> 00:05:49,110
on the far right side of the graph so

00:05:47,110 --> 00:05:51,250
that's basically how these systems work

00:05:49,110 --> 00:05:52,750
now as part of bigquery we have

00:05:51,250 --> 00:05:55,030
something called the public data sets

00:05:52,750 --> 00:05:57,100
program and I'll tell you a little bit

00:05:55,030 --> 00:05:59,110
about this and so the bigquery sorry the

00:05:57,100 --> 00:06:01,000
Bitcoin data set is part of this public

00:05:59,110 --> 00:06:05,830
data sets program it's effectively open

00:06:01,000 --> 00:06:08,320
data as of October we had 71 different

00:06:05,830 --> 00:06:11,380
public data sets that were were in this

00:06:08,320 --> 00:06:13,930
in this program the tables are world

00:06:11,380 --> 00:06:15,400
readable and we have a free access to

00:06:13,930 --> 00:06:16,900
your for bigquery so if you want to look

00:06:15,400 --> 00:06:18,730
at these you can query I believe it's up

00:06:16,900 --> 00:06:20,320
to 500 gigabytes of data per month and

00:06:18,730 --> 00:06:24,070
not have to pay anything you just have

00:06:20,320 --> 00:06:25,600
to have a Google Cloud account in

00:06:24,070 --> 00:06:28,300
aggregate there's something like more

00:06:25,600 --> 00:06:30,430
than 700 tables many of which are live

00:06:28,300 --> 00:06:31,810
updated on a regular basis the bitcoin

00:06:30,430 --> 00:06:35,440
table is updated every 10 minutes

00:06:31,810 --> 00:06:37,510
because that's how often a block is

00:06:35,440 --> 00:06:38,650
produced so it's it's as real time as it

00:06:37,510 --> 00:06:41,350
gets

00:06:38,650 --> 00:06:43,300
we've got in excess of four billion

00:06:41,350 --> 00:06:45,580
records in this in this data set or this

00:06:43,300 --> 00:06:47,500
aggregate of data sets and the total

00:06:45,580 --> 00:06:49,560
amount of query data is is in excess of

00:06:47,500 --> 00:06:51,790
30 petabytes and this is all all

00:06:49,560 --> 00:06:55,169
exponentially growing numbers from the

00:06:51,790 --> 00:06:57,850
existence the inception of the program

00:06:55,169 --> 00:06:59,980
okay so let's talk about how one might

00:06:57,850 --> 00:07:02,410
go about doing this kind of OLAP type of

00:06:59,980 --> 00:07:04,930
analysis on the Bitcoin data set here's

00:07:02,410 --> 00:07:06,310
here's the the system architecture so we

00:07:04,930 --> 00:07:08,020
can consider Bitcoin as being this

00:07:06,310 --> 00:07:10,360
peer-to-peer network that's maintaining

00:07:08,020 --> 00:07:13,330
the distributed ledger and what we did

00:07:10,360 --> 00:07:15,490
is we we built a custom Bitcoin client

00:07:13,330 --> 00:07:17,290
using Bitcoin J and if you want to go

00:07:15,490 --> 00:07:19,979
ahead and grab the source code for how

00:07:17,290 --> 00:07:22,120
we did this you can you can get it here

00:07:19,979 --> 00:07:23,350
nothing super complex but if you wanted

00:07:22,120 --> 00:07:25,750
to know how the client works you could

00:07:23,350 --> 00:07:28,960
you could use that the data are coming

00:07:25,750 --> 00:07:30,940
in in real time from the peers we're not

00:07:28,960 --> 00:07:32,620
mining we're just syncing the data and

00:07:30,940 --> 00:07:34,150
if somebody requests data from our node

00:07:32,620 --> 00:07:35,320
we'll push it out so we are operating a

00:07:34,150 --> 00:07:38,080
full node we're not trying to mine

00:07:35,320 --> 00:07:39,490
bitcoins and then every ten minutes when

00:07:38,080 --> 00:07:41,800
a new block is added on to the end of

00:07:39,490 --> 00:07:44,229
the network we take that block and we

00:07:41,800 --> 00:07:46,479
sync it over to bigquery and then that's

00:07:44,229 --> 00:07:48,310
made visual visible via this data studio

00:07:46,479 --> 00:07:51,700
tool I was showing you earlier for doing

00:07:48,310 --> 00:07:54,610
basic time series type type analyses or

00:07:51,700 --> 00:07:58,300
other analyses and then also bigquery

00:07:54,610 --> 00:08:00,250
has the connector to the Kaggle Catalan

00:07:58,300 --> 00:08:02,770
community how many of you guys are data

00:08:00,250 --> 00:08:06,430
scientists in the room okay we've got a

00:08:02,770 --> 00:08:08,350
few so this is the largest online data

00:08:06,430 --> 00:08:10,570
science community acquired by Google

00:08:08,350 --> 00:08:12,789
late last year and so there's a

00:08:10,570 --> 00:08:15,400
connector to bring bigquery data sets

00:08:12,789 --> 00:08:18,340
into Kaggle via a connector so that you

00:08:15,400 --> 00:08:20,620
can query it from our or query it from

00:08:18,340 --> 00:08:23,770
Jupiter notebook to bring the data into

00:08:20,620 --> 00:08:25,030
a scientific notebook type environment

00:08:23,770 --> 00:08:27,220
to do further analyses and there's a

00:08:25,030 --> 00:08:29,350
bunch of users now who are collaborating

00:08:27,220 --> 00:08:31,539
on query in this data analyzing it

00:08:29,350 --> 00:08:33,909
exploring it together and you can go

00:08:31,539 --> 00:08:37,779
check that out on Kaggle here's another

00:08:33,909 --> 00:08:39,550
example of some data that we can join

00:08:37,779 --> 00:08:45,279
the the Bitcoin data set again so this

00:08:39,550 --> 00:08:47,260
is the mining difficulty shown in orange

00:08:45,279 --> 00:08:50,110
so this is a measure of it's sort of an

00:08:47,260 --> 00:08:51,680
indirect measure of the price of a

00:08:50,110 --> 00:08:54,259
Bitcoin and

00:08:51,680 --> 00:08:58,999
and in blue you can see that we have

00:08:54,259 --> 00:09:03,050
some popularity metric for the keyword

00:08:58,999 --> 00:09:04,819
Bitcoin shown in blue notice this this

00:09:03,050 --> 00:09:07,129
figure is a bit outdated

00:09:04,819 --> 00:09:09,350
but you might draw some conclusions as

00:09:07,129 --> 00:09:11,749
to how popularity may be an indicator of

00:09:09,350 --> 00:09:15,589
difficulty or or other attributes of the

00:09:11,749 --> 00:09:17,420
of the network some other interesting

00:09:15,589 --> 00:09:19,999
kind of analysis we can do is moving

00:09:17,420 --> 00:09:22,610
beyond doing time based aggregations or

00:09:19,999 --> 00:09:25,910
scanning type operations is to do

00:09:22,610 --> 00:09:28,399
graph analysis type of analyses because

00:09:25,910 --> 00:09:29,809
the fundamental type of data is a

00:09:28,399 --> 00:09:32,449
transactional ledger so it's it's

00:09:29,809 --> 00:09:35,749
stating that some amount of bitcoins is

00:09:32,449 --> 00:09:40,850
moving from account a to account B and

00:09:35,749 --> 00:09:43,040
here you can see I'm depicting the first

00:09:40,850 --> 00:09:45,139
documented transaction for exchanging

00:09:43,040 --> 00:09:49,129
bitcoins for a physical good so in this

00:09:45,139 --> 00:09:51,889
case the red node indicated here is the

00:09:49,129 --> 00:09:54,350
purchaser of two Domino's pizzas for the

00:09:51,889 --> 00:09:59,300
price of 10,000 bitcoins that was about

00:09:54,350 --> 00:10:00,439
$40 at the time so he was the purchaser

00:09:59,300 --> 00:10:01,730
was actually advised not to make the

00:10:00,439 --> 00:10:02,929
purchase because he was overpaying but

00:10:01,730 --> 00:10:04,550
he wanted to prove the point that the

00:10:02,929 --> 00:10:06,769
coins could be used for for real

00:10:04,550 --> 00:10:09,470
transactions now upstream all these

00:10:06,769 --> 00:10:11,529
blues blue nodes I'm showing you in the

00:10:09,470 --> 00:10:14,360
graph actually represent other other

00:10:11,529 --> 00:10:17,240
wallets in the Bitcoin transaction space

00:10:14,360 --> 00:10:20,449
and the arrow directed edges indicate

00:10:17,240 --> 00:10:22,279
money flow of bitcoins from them to to

00:10:20,449 --> 00:10:25,129
this red node so I did a recursive query

00:10:22,279 --> 00:10:26,509
up I believe three three levels from

00:10:25,129 --> 00:10:28,600
from that node so you can see there's

00:10:26,509 --> 00:10:32,929
some some interesting structure in here

00:10:28,600 --> 00:10:39,410
here on the on the on the right is a

00:10:32,929 --> 00:10:41,749
different sub Network in mid 2017 there

00:10:39,410 --> 00:10:43,490
was a ransomware attack called wanna cry

00:10:41,749 --> 00:10:45,589
it was basically a malware that

00:10:43,490 --> 00:10:48,619
encrypted many computers on the network

00:10:45,589 --> 00:10:50,959
and demanded payment of a ransom in

00:10:48,619 --> 00:10:52,160
bitcoins to one of three addresses they

00:10:50,959 --> 00:10:53,990
could only have three addresses because

00:10:52,160 --> 00:10:56,240
the the payload and the malware had to

00:10:53,990 --> 00:10:57,230
be small and then if you if you made the

00:10:56,240 --> 00:10:58,279
payment they would send you the

00:10:57,230 --> 00:10:59,480
encryption key to then Rick could be

00:10:58,279 --> 00:11:01,970
able to recover the data off of your

00:10:59,480 --> 00:11:03,980
hard drive you can see here that I'm

00:11:01,970 --> 00:11:06,150
looking downstream of each of these

00:11:03,980 --> 00:11:08,820
three

00:11:06,150 --> 00:11:10,290
now where payment addresses into the

00:11:08,820 --> 00:11:12,030
network and I was curious to see if the

00:11:10,290 --> 00:11:14,480
money is sort of flowing toward one

00:11:12,030 --> 00:11:16,800
place or it's flowing across the network

00:11:14,480 --> 00:11:18,030
I'm not really sure exactly what the

00:11:16,800 --> 00:11:21,360
structure is here I didn't go further

00:11:18,030 --> 00:11:22,920
into this analysis but one might imagine

00:11:21,360 --> 00:11:23,730
that there there's some patterns that

00:11:22,920 --> 00:11:25,290
could be discovered

00:11:23,730 --> 00:11:28,400
oh the edges by the way are indicating

00:11:25,290 --> 00:11:31,110
the weight or the proportional to the

00:11:28,400 --> 00:11:32,880
the logarithm of the volume of the

00:11:31,110 --> 00:11:40,200
transaction and my computer just turned

00:11:32,880 --> 00:11:41,790
off I'll be right back yeah so that's

00:11:40,200 --> 00:11:43,920
some type of analysis you can do on this

00:11:41,790 --> 00:11:46,260
data set now these were done in bigquery

00:11:43,920 --> 00:11:47,430
which meant that the analysis was

00:11:46,260 --> 00:11:48,840
actually a bit difficult because this

00:11:47,430 --> 00:11:51,090
kind of linear scanning is not set up

00:11:48,840 --> 00:11:55,140
for for graph traversal or graph

00:11:51,090 --> 00:11:59,400
traversal or recursive type of queries

00:11:55,140 --> 00:12:00,780
that don't follow something like a

00:11:59,400 --> 00:12:02,790
linear scan which is what bigquery is

00:12:00,780 --> 00:12:04,170
optimized for and so to get deeper into

00:12:02,790 --> 00:12:05,520
this kind of analysis that's actually

00:12:04,170 --> 00:12:06,900
appropriate to move the data to a graph

00:12:05,520 --> 00:12:10,980
database and I'll talk more about that

00:12:06,900 --> 00:12:12,570
later yes another visualization of the

00:12:10,980 --> 00:12:14,450
same thing this is downstream the pizza

00:12:12,570 --> 00:12:17,580
transaction you can see that this

00:12:14,450 --> 00:12:18,780
there's a bunch of capital notebooks

00:12:17,580 --> 00:12:20,160
like I was mentioning earlier some

00:12:18,780 --> 00:12:23,010
people are writing these things it's

00:12:20,160 --> 00:12:25,020
quite active and yet feel free to join

00:12:23,010 --> 00:12:27,990
in clone somebody's notebook comment on

00:12:25,020 --> 00:12:29,910
it requests analyses the the calculators

00:12:27,990 --> 00:12:32,400
are a pretty vibrant active community

00:12:29,910 --> 00:12:36,180
and this is the Bitcoin dataset has been

00:12:32,400 --> 00:12:37,800
in the top five for the last month and a

00:12:36,180 --> 00:12:40,830
half for which it's been available it

00:12:37,800 --> 00:12:43,860
was briefly at number one okay so that

00:12:40,830 --> 00:12:45,510
that's a Bitcoin a dataset overview now

00:12:43,860 --> 00:12:47,490
I'm beginning to work on the etherium

00:12:45,510 --> 00:12:51,630
dataset so the etherion dataset this is

00:12:47,490 --> 00:12:57,060
the number two most popular largest by

00:12:51,630 --> 00:12:59,340
market cap cryptocurrency today and it

00:12:57,060 --> 00:13:01,110
it has some similar attributes to

00:12:59,340 --> 00:13:03,330
Bitcoin but it has a bunch of additional

00:13:01,110 --> 00:13:06,030
features in particular something called

00:13:03,330 --> 00:13:08,340
the smart contract which is sort of like

00:13:06,030 --> 00:13:10,710
program it basically acts as a as a

00:13:08,340 --> 00:13:14,160
global distributed computer not just

00:13:10,710 --> 00:13:16,380
allowing transfers of value via this

00:13:14,160 --> 00:13:18,450
distributed ledger but also keeping a

00:13:16,380 --> 00:13:20,520
record of

00:13:18,450 --> 00:13:22,860
computing operations that can take place

00:13:20,520 --> 00:13:27,300
on the blockchain so it's not only value

00:13:22,860 --> 00:13:28,710
it's a computer okay yes

00:13:27,300 --> 00:13:30,600
it's it's different you know in another

00:13:28,710 --> 00:13:32,370
way as well so I just told you it has a

00:13:30,600 --> 00:13:34,350
turing-complete smart contract language

00:13:32,370 --> 00:13:37,020
so you can write arbitrarily complex

00:13:34,350 --> 00:13:39,570
programs to execute against the etherium

00:13:37,020 --> 00:13:41,460
virtual machine the block team is also

00:13:39,570 --> 00:13:44,220
quite a bit shorter so rather than 10

00:13:41,460 --> 00:13:46,260
minutes on on Bitcoin a new block is

00:13:44,220 --> 00:13:48,000
produced every 15 seconds so this

00:13:46,260 --> 00:13:49,620
produces a bit more of a challenge to

00:13:48,000 --> 00:13:50,910
keep up with the data and keep it

00:13:49,620 --> 00:13:52,500
streaming into bigquery

00:13:50,910 --> 00:13:56,040
but we're able to do this it's not a

00:13:52,500 --> 00:13:58,890
problem yes and so a lot of these icos

00:13:56,040 --> 00:14:00,570
and other things you hear about related

00:13:58,890 --> 00:14:02,280
to crypto currencies most of that

00:14:00,570 --> 00:14:07,500
activity is actually happening on the

00:14:02,280 --> 00:14:09,240
the etherium blockchain today so

00:14:07,500 --> 00:14:12,060
technical update of where I am in the

00:14:09,240 --> 00:14:13,560
status of bringing this project in is I

00:14:12,060 --> 00:14:15,240
have the I have the peer-to-peer node

00:14:13,560 --> 00:14:17,550
running I have a full sync of the

00:14:15,240 --> 00:14:20,280
etherium blockchain in Google cloud I've

00:14:17,550 --> 00:14:23,490
loaded all of it into bigquery and it's

00:14:20,280 --> 00:14:25,470
updating every few seconds it's also

00:14:23,490 --> 00:14:27,930
available via Kaggle if you wanted to

00:14:25,470 --> 00:14:29,430
look at it today you can contact me

00:14:27,930 --> 00:14:31,800
afterward I have my contact details on

00:14:29,430 --> 00:14:32,730
my final slide I can add your user name

00:14:31,800 --> 00:14:34,320
to the project it's currently

00:14:32,730 --> 00:14:36,480
invite-only right now so I'm looking to

00:14:34,320 --> 00:14:38,880
when I release a blog to have some

00:14:36,480 --> 00:14:41,280
interesting analyses ready to go just to

00:14:38,880 --> 00:14:42,150
make a lot more impact for the for the

00:14:41,280 --> 00:14:45,960
launch to look at some interesting

00:14:42,150 --> 00:14:47,640
things we can do with the data yes and

00:14:45,960 --> 00:14:50,340
there's also some auxiliary like

00:14:47,640 --> 00:14:51,840
pre-processing I've done that makes this

00:14:50,340 --> 00:14:57,390
dataset a lot more interesting so let's

00:14:51,840 --> 00:14:58,950
get into that I created a view of the

00:14:57,390 --> 00:15:01,320
data into something that's a token

00:14:58,950 --> 00:15:06,390
transfer so this is something like an 80

00:15:01,320 --> 00:15:08,220
line query that extracts out something

00:15:06,390 --> 00:15:09,630
called an e rc 20 token so the majority

00:15:08,220 --> 00:15:12,510
of the cryptocurrencies are actually

00:15:09,630 --> 00:15:14,910
based on this template called er C 20

00:15:12,510 --> 00:15:16,650
it's an it's a you can consider it like

00:15:14,910 --> 00:15:19,470
a software class or an object that's

00:15:16,650 --> 00:15:20,880
inherited from and so because of that if

00:15:19,470 --> 00:15:23,490
you wanted to look for activities

00:15:20,880 --> 00:15:24,750
related to e rc 20 tokens you can see

00:15:23,490 --> 00:15:26,580
all of them and they all follow the same

00:15:24,750 --> 00:15:28,920
kind of pattern and that's how I was

00:15:26,580 --> 00:15:30,630
able to produce this view so here I'm

00:15:28,920 --> 00:15:31,860
looking at one particular token called

00:15:30,630 --> 00:15:33,720
0x this is a

00:15:31,860 --> 00:15:35,310
tributed exchange protocol you can go

00:15:33,720 --> 00:15:37,950
check it out I'm gonna you're not gonna

00:15:35,310 --> 00:15:41,040
get further into that and by doing this

00:15:37,950 --> 00:15:43,589
kind of query of exporting the source

00:15:41,040 --> 00:15:45,839
node the target node and then some

00:15:43,589 --> 00:15:47,279
measure of the value moving between

00:15:45,839 --> 00:15:50,040
source and target so we're looking at

00:15:47,279 --> 00:15:52,589
the total token transfer moving within

00:15:50,040 --> 00:15:55,140
the network we can export that data into

00:15:52,589 --> 00:15:56,850
a nice open source data visualization

00:15:55,140 --> 00:16:01,140
and a piece of software called get fee

00:15:56,850 --> 00:16:03,060
and start to build some data

00:16:01,140 --> 00:16:04,200
visualizations of the transaction

00:16:03,060 --> 00:16:04,709
network data that looks something like

00:16:04,200 --> 00:16:06,660
this

00:16:04,709 --> 00:16:07,800
now I'm not a data visualization expert

00:16:06,660 --> 00:16:09,120
maybe this is not the best way to

00:16:07,800 --> 00:16:12,000
represent it but I thought it looked

00:16:09,120 --> 00:16:14,850
pretty interesting this is all the 0x

00:16:12,000 --> 00:16:16,829
transactions of more than 1,000 tokens

00:16:14,850 --> 00:16:19,380
moving within the network so it's a few

00:16:16,829 --> 00:16:21,570
thousand US dollars of greater and what

00:16:19,380 --> 00:16:25,380
I did is I colored the nodes according

00:16:21,570 --> 00:16:27,450
to a partitioning of the graph so I can

00:16:25,380 --> 00:16:29,880
basically say things that are red our

00:16:27,450 --> 00:16:31,320
nodes generally interact with one

00:16:29,880 --> 00:16:32,940
another more than they do with nodes

00:16:31,320 --> 00:16:34,529
that are of another color and likewise

00:16:32,940 --> 00:16:37,079
for blue and likewise for green and so

00:16:34,529 --> 00:16:38,810
on so there's different partitions

00:16:37,079 --> 00:16:42,680
within the graph that tend to interact

00:16:38,810 --> 00:16:48,240
intra partition and not inter-party ssin

00:16:42,680 --> 00:16:49,769
and the lines between these nodes so the

00:16:48,240 --> 00:16:51,959
nodes represent an address and the lines

00:16:49,769 --> 00:16:53,820
between them represent a transfer of

00:16:51,959 --> 00:16:55,980
value between them now this is this is

00:16:53,820 --> 00:16:57,930
across all time so I've not done any

00:16:55,980 --> 00:16:59,190
kind of time windowing but you could

00:16:57,930 --> 00:17:00,690
also do analysis of this in a

00:16:59,190 --> 00:17:02,279
longitudinal manner if you wanted to do

00:17:00,690 --> 00:17:05,010
some kind of time windowed based

00:17:02,279 --> 00:17:06,480
analysis to look for various types of

00:17:05,010 --> 00:17:09,740
aberrations or anomalies where you see

00:17:06,480 --> 00:17:11,819
unusual spikes of activity for example

00:17:09,740 --> 00:17:15,240
now interesting to note here to produce

00:17:11,819 --> 00:17:16,860
that that table I actually needed to

00:17:15,240 --> 00:17:18,750
identify this thing called the transfer

00:17:16,860 --> 00:17:20,730
function from a RC 20 so all the ERC

00:17:18,750 --> 00:17:23,490
twenties tokens implement this function

00:17:20,730 --> 00:17:27,540
it has a four byte method signature 0 XA

00:17:23,490 --> 00:17:31,460
9 0 5 9 CB B so there's four billion

00:17:27,540 --> 00:17:34,380
possible addresses in the etherium

00:17:31,460 --> 00:17:36,059
method signature space and by doing this

00:17:34,380 --> 00:17:38,400
I was able to find all of the all of the

00:17:36,059 --> 00:17:39,690
transactions oh here's a here's a zoomed

00:17:38,400 --> 00:17:40,890
in picture of the red node this is

00:17:39,690 --> 00:17:43,080
actually one of the cryptocurrency

00:17:40,890 --> 00:17:45,090
exchanges this is by Nance and so you

00:17:43,080 --> 00:17:45,600
can see there's some users who are doing

00:17:45,090 --> 00:17:47,700
like

00:17:45,600 --> 00:17:49,889
high-volume transactions of by Nance and

00:17:47,700 --> 00:17:51,389
then lower volume some of them are

00:17:49,889 --> 00:17:52,769
actually connected to other exchanges as

00:17:51,389 --> 00:17:56,159
well so you can start to type these

00:17:52,769 --> 00:17:59,279
these nodes and and infer attributes

00:17:56,159 --> 00:18:03,149
about them based on based on their

00:17:59,279 --> 00:18:07,110
position in the network but yeah back to

00:18:03,149 --> 00:18:10,440
this method signature thing the the RC

00:18:07,110 --> 00:18:13,889
20 contract is is open source and so we

00:18:10,440 --> 00:18:15,509
can compile it and we can identify what

00:18:13,889 --> 00:18:16,860
method signature it would have if you

00:18:15,509 --> 00:18:18,720
had the source code so we know it

00:18:16,860 --> 00:18:20,190
happens to be called transfer this is

00:18:18,720 --> 00:18:22,289
its byte signature when it's compiled

00:18:20,190 --> 00:18:24,330
and it takes two arguments so there's

00:18:22,289 --> 00:18:26,850
the calling address is calling to

00:18:24,330 --> 00:18:28,830
another address to transfer the value to

00:18:26,850 --> 00:18:31,529
that address and there's an unsigned int

00:18:28,830 --> 00:18:32,970
of 256 bits of how many things need to

00:18:31,529 --> 00:18:35,070
be transferred how many units of the

00:18:32,970 --> 00:18:37,470
token and there's a balance that's

00:18:35,070 --> 00:18:40,320
maintained in the smart contract it's

00:18:37,470 --> 00:18:43,440
updated as a result of this method call

00:18:40,320 --> 00:18:45,990
so within the open source space there's

00:18:43,440 --> 00:18:48,240
well there's a database called four byte

00:18:45,990 --> 00:18:50,580
for byte like the number four for byte

00:18:48,240 --> 00:18:53,190
directory that indexes these things and

00:18:50,580 --> 00:18:55,200
for byte directory has six thousand nine

00:18:53,190 --> 00:18:57,389
hundred forty two methods for which

00:18:55,200 --> 00:18:58,649
there are sources available that can be

00:18:57,389 --> 00:19:00,809
compiled to determine the method

00:18:58,649 --> 00:19:02,580
signature so we can start to type the

00:19:00,809 --> 00:19:06,899
transactions by looking at the source

00:19:02,580 --> 00:19:10,289
code for the smart contracts and here's

00:19:06,899 --> 00:19:11,909
sort of how that works so for byte

00:19:10,289 --> 00:19:13,289
directory takes these and this is an

00:19:11,909 --> 00:19:15,960
open data set by the way if you wanted

00:19:13,289 --> 00:19:17,759
to download these methods signatures the

00:19:15,960 --> 00:19:20,159
the guy behind it Piper Merriam he's

00:19:17,759 --> 00:19:21,720
he's taken the solidity source code

00:19:20,159 --> 00:19:24,480
that's the language is used to write

00:19:21,720 --> 00:19:26,879
smart contracts compiled it and then I

00:19:24,480 --> 00:19:28,379
pulled the data into bigquery by by

00:19:26,879 --> 00:19:29,340
mirroring it in as a part of the public

00:19:28,379 --> 00:19:30,860
data set so this is one of these

00:19:29,340 --> 00:19:35,460
auxiliary data sets that's available

00:19:30,860 --> 00:19:38,549
with the etherium data and then what I

00:19:35,460 --> 00:19:41,279
did is I actually looked at the get

00:19:38,549 --> 00:19:43,139
github bigquery public data set so we

00:19:41,279 --> 00:19:46,080
have all of the github history in

00:19:43,139 --> 00:19:48,240
bigquery it's like 60 terabytes of data

00:19:46,080 --> 00:19:49,980
and the next talk after mine from Felipe

00:19:48,240 --> 00:19:52,169
Hoffa we'll talk about some interesting

00:19:49,980 --> 00:19:54,090
analysis purely on the github data set

00:19:52,169 --> 00:19:57,000
in isolation but here I'm talking about

00:19:54,090 --> 00:19:59,429
it in the context of aetherium so what I

00:19:57,000 --> 00:20:00,150
can do is I can query this data set find

00:19:59,429 --> 00:20:03,030
all the solidity

00:20:00,150 --> 00:20:04,470
source files okay and then Piper gave me

00:20:03,030 --> 00:20:08,280
a web hook where I can create a new

00:20:04,470 --> 00:20:10,230
github repo copy all the existing sole

00:20:08,280 --> 00:20:13,980
source files into my new git repo and

00:20:10,230 --> 00:20:15,750
when I commit them his his database is

00:20:13,980 --> 00:20:17,400
aware that I made a commit he can pull

00:20:15,750 --> 00:20:18,750
in the new files compile them update the

00:20:17,400 --> 00:20:21,480
database and then the signatures get

00:20:18,750 --> 00:20:23,670
updated in bigquery automatically so

00:20:21,480 --> 00:20:25,620
this is effectively serverless updated

00:20:23,670 --> 00:20:28,050
the database just by scraping or

00:20:25,620 --> 00:20:31,830
crawling crawling the the the

00:20:28,050 --> 00:20:34,620
synchronized github data set so so given

00:20:31,830 --> 00:20:36,630
that and I also have this OLAP OLTP

00:20:34,620 --> 00:20:38,750
bridge coming from the etherium network

00:20:36,630 --> 00:20:41,250
into this theory in bigquery data set

00:20:38,750 --> 00:20:42,720
there's a possibility here to combine

00:20:41,250 --> 00:20:44,340
these things and one of the one of the

00:20:42,720 --> 00:20:46,050
great things about open data and link to

00:20:44,340 --> 00:20:47,550
data is that we can take these two

00:20:46,050 --> 00:20:50,130
existing data sets and there's not

00:20:47,550 --> 00:20:51,450
obviously some benefit to doing that but

00:20:50,130 --> 00:20:53,310
we're actually able to give birth to a

00:20:51,450 --> 00:20:55,470
third data set that is the etherium

00:20:53,310 --> 00:20:58,800
virtual machine stacktrace analytics

00:20:55,470 --> 00:21:00,540
data set that comes out from both github

00:20:58,800 --> 00:21:01,050
and and bringing in in aetherium into

00:21:00,540 --> 00:21:02,520
bigquery

00:21:01,050 --> 00:21:05,280
so this is the this is the real value of

00:21:02,520 --> 00:21:08,100
having the the public data sets is that

00:21:05,280 --> 00:21:10,470
we can accelerate the ability to to

00:21:08,100 --> 00:21:12,660
innovate by joining link by linking the

00:21:10,470 --> 00:21:14,430
data all right so what do those

00:21:12,660 --> 00:21:17,850
stacktrace analytics look like I just

00:21:14,430 --> 00:21:19,650
started doing this you can see here what

00:21:17,850 --> 00:21:21,330
I'm showing is not only transfer events

00:21:19,650 --> 00:21:22,740
so transfer is the really big one which

00:21:21,330 --> 00:21:25,650
is what I was talking about earlier this

00:21:22,740 --> 00:21:29,280
is token transfers I think I'm showing

00:21:25,650 --> 00:21:34,440
like the top 50 here the the second one

00:21:29,280 --> 00:21:36,750
by like largest volume by day so this is

00:21:34,440 --> 00:21:38,700
this is the etherium for the last two

00:21:36,750 --> 00:21:41,210
years and this is the number of those

00:21:38,700 --> 00:21:44,430
particular types of method calls Oh

00:21:41,210 --> 00:21:46,560
aggregated per unit time so you can see

00:21:44,430 --> 00:21:48,450
it's really dominated by by transfers of

00:21:46,560 --> 00:21:51,780
tokens anybody want to speculate as to

00:21:48,450 --> 00:21:58,460
what this this a pink pink method is

00:21:51,780 --> 00:22:01,230
from I'm sure one of you can guess sorry

00:21:58,460 --> 00:22:02,910
yes crypto Kitty's exactly so this is

00:22:01,230 --> 00:22:05,250
actually a breeding event to produce a

00:22:02,910 --> 00:22:08,700
new kitten from two parent crypto

00:22:05,250 --> 00:22:09,990
kiddies yeah it was really a flash in

00:22:08,700 --> 00:22:11,280
the pan though it only lasted about a

00:22:09,990 --> 00:22:12,600
week I don't have the zoomed in version

00:22:11,280 --> 00:22:14,010
of this graphic but yeah it's pretty

00:22:12,600 --> 00:22:15,390
much dead from now

00:22:14,010 --> 00:22:17,100
there's something interesting happening

00:22:15,390 --> 00:22:19,200
like a month ago when I made this chart

00:22:17,100 --> 00:22:20,910
in yellow I didn't get deeper into it

00:22:19,200 --> 00:22:22,680
but if any of you again want to look at

00:22:20,910 --> 00:22:25,140
this I can give you early access to the

00:22:22,680 --> 00:22:27,750
data set do a study and tell me like

00:22:25,140 --> 00:22:29,520
okay what are the derivatives of changes

00:22:27,750 --> 00:22:31,470
of applications and and what's the

00:22:29,520 --> 00:22:36,930
what's the latest trend on the etherium

00:22:31,470 --> 00:22:39,360
network all the data are available okay

00:22:36,930 --> 00:22:41,340
so so back to back to this method

00:22:39,360 --> 00:22:43,320
analysis I told you there's a method

00:22:41,340 --> 00:22:45,930
signature space of 4 billion possible

00:22:43,320 --> 00:22:47,190
methods we know about six hundred six

00:22:45,930 --> 00:22:49,860
thousand seven hundred ninety two of

00:22:47,190 --> 00:22:52,080
them what the names of the types are but

00:22:49,860 --> 00:22:54,570
for the rest of them we we don't know

00:22:52,080 --> 00:22:56,310
and there's a lot here's what a typical

00:22:54,570 --> 00:23:00,900
method call looks like it's just some

00:22:56,310 --> 00:23:02,940
byte byte array okay we can break it up

00:23:00,900 --> 00:23:04,740
because we know for example that the

00:23:02,940 --> 00:23:06,060
method signatures four bytes right and

00:23:04,740 --> 00:23:11,720
then we know that each additional

00:23:06,060 --> 00:23:14,490
argument is always 256 bits or 32 bytes

00:23:11,720 --> 00:23:16,860
so we can start to get some information

00:23:14,490 --> 00:23:19,680
about this like I can tell you you know

00:23:16,860 --> 00:23:22,980
argument to looks like it might be an

00:23:19,680 --> 00:23:24,930
address you could possibly measure the

00:23:22,980 --> 00:23:26,760
amount of entropy in the bits here like

00:23:24,930 --> 00:23:28,320
how unpredictable or how compressible is

00:23:26,760 --> 00:23:31,080
this is this which gives you some sense

00:23:28,320 --> 00:23:31,860
of how address like is it right

00:23:31,080 --> 00:23:33,630
addresses should be unpredictable

00:23:31,860 --> 00:23:36,110
private keys should certainly be very

00:23:33,630 --> 00:23:36,110
unpredictable

00:23:36,920 --> 00:23:41,100
you know this third value it's all zeros

00:23:39,180 --> 00:23:42,900
what does that mean it could be a

00:23:41,100 --> 00:23:45,270
boolean probably it's a boolean might be

00:23:42,900 --> 00:23:48,170
an OL it's highly unlikely to be an

00:23:45,270 --> 00:23:50,430
address right that's too easy to guess

00:23:48,170 --> 00:23:52,020
and then this fourth one you know who

00:23:50,430 --> 00:23:53,940
knows what that is because it has four

00:23:52,020 --> 00:23:58,190
zeros at the end is that significant for

00:23:53,940 --> 00:24:01,440
any reason yeah so any anybody want to

00:23:58,190 --> 00:24:03,540
speculate as to how you might might take

00:24:01,440 --> 00:24:12,570
these data and get some more information

00:24:03,540 --> 00:24:14,660
out of there you could do that that'll

00:24:12,570 --> 00:24:16,680
cost you some some money to do it though

00:24:14,660 --> 00:24:21,360
yeah not a bad idea

00:24:16,680 --> 00:24:22,390
yeah sure probe it yeah probe it anybody

00:24:21,360 --> 00:24:25,600
else

00:24:22,390 --> 00:24:27,490
I mean I guess if you if you if you

00:24:25,600 --> 00:24:29,050
forked the main network onto your test

00:24:27,490 --> 00:24:30,160
environment you could effectively give

00:24:29,050 --> 00:24:31,300
yourself free money and do as much of

00:24:30,160 --> 00:24:32,380
that as you want yeah that's not a bad

00:24:31,300 --> 00:24:33,730
idea

00:24:32,380 --> 00:24:35,230
that's more like moving into a

00:24:33,730 --> 00:24:39,670
penetration testing kind of angle to

00:24:35,230 --> 00:24:42,190
find vulnerabilities and weaknesses on

00:24:39,670 --> 00:24:44,260
the network it's very interesting I

00:24:42,190 --> 00:24:46,330
wanna think more about that well you

00:24:44,260 --> 00:24:48,400
could actually treat this as a deep

00:24:46,330 --> 00:24:51,490
learning problem right so we could start

00:24:48,400 --> 00:24:54,700
to classify these data by looking for

00:24:51,490 --> 00:24:56,440
things like you know typically transfers

00:24:54,700 --> 00:25:00,430
of tokens we'll have some kind of round

00:24:56,440 --> 00:25:01,930
number so if we observe some byte string

00:25:00,430 --> 00:25:04,690
that represents a round number in a

00:25:01,930 --> 00:25:05,980
floating-point space maybe it's a maybe

00:25:04,690 --> 00:25:09,490
at the transfer event and you could

00:25:05,980 --> 00:25:12,220
think about other such types of analysis

00:25:09,490 --> 00:25:14,620
you could do to further characterize the

00:25:12,220 --> 00:25:16,810
methods so doing this sort of thing lets

00:25:14,620 --> 00:25:18,400
you get deeper into understanding what

00:25:16,810 --> 00:25:20,380
types of activity people are doing and

00:25:18,400 --> 00:25:21,880
then by looking at top-level methods and

00:25:20,380 --> 00:25:24,070
then the stack traces underneath them

00:25:21,880 --> 00:25:25,960
starts to give you some sense of what

00:25:24,070 --> 00:25:28,420
types of activities are being done on

00:25:25,960 --> 00:25:30,460
chain and given that every method call

00:25:28,420 --> 00:25:32,110
actually costs a little bit of gas or

00:25:30,460 --> 00:25:34,090
costs a little bit of aetherium to run

00:25:32,110 --> 00:25:36,340
the method you can then begin to

00:25:34,090 --> 00:25:38,830
attribute to any method call what is the

00:25:36,340 --> 00:25:40,600
real fundamental economic value that

00:25:38,830 --> 00:25:42,700
allows you to exchange ethers or dollars

00:25:40,600 --> 00:25:44,080
for for computing on the virtual machine

00:25:42,700 --> 00:25:46,150
so there's some some interesting

00:25:44,080 --> 00:25:47,700
fundamental analysis that can be done on

00:25:46,150 --> 00:25:50,530
this type of data but we need more

00:25:47,700 --> 00:25:54,850
elucidation of actually what these

00:25:50,530 --> 00:25:56,850
methods are I was showing you some

00:25:54,850 --> 00:25:59,410
method call analysis here's a website

00:25:56,850 --> 00:26:01,530
that's starting to index these things

00:25:59,410 --> 00:26:04,570
called distributed applications or adaps

00:26:01,530 --> 00:26:06,130
doing something in only on the on the

00:26:04,570 --> 00:26:09,040
the transpose dimension so we have

00:26:06,130 --> 00:26:12,900
methods on smart contracts and this is

00:26:09,040 --> 00:26:15,580
an index of smart contract interaction

00:26:12,900 --> 00:26:19,090
across the network measuring interesting

00:26:15,580 --> 00:26:20,920
things like daily active users or how

00:26:19,090 --> 00:26:23,290
much aetherium actually flowed through

00:26:20,920 --> 00:26:25,570
the contract and if we start to look at

00:26:23,290 --> 00:26:29,650
interactions with contracts we can begin

00:26:25,570 --> 00:26:31,420
to cluster games for example should have

00:26:29,650 --> 00:26:33,550
similar behavior in general we could

00:26:31,420 --> 00:26:35,080
start to find new games just through the

00:26:33,550 --> 00:26:36,130
method of interaction so this is a way

00:26:35,080 --> 00:26:39,250
of beginning to

00:26:36,130 --> 00:26:42,820
build something like it looks quite a

00:26:39,250 --> 00:26:45,430
bit like like a web portal like Yahoo

00:26:42,820 --> 00:26:47,500
way back in the day start to build

00:26:45,430 --> 00:26:51,160
something like an index or a search

00:26:47,500 --> 00:26:54,970
index on top of on top of the etherium

00:26:51,160 --> 00:26:58,960
chain all right so so where I am today

00:26:54,970 --> 00:27:01,360
is that graphic I was showing you before

00:26:58,960 --> 00:27:04,060
of the the transaction network with the

00:27:01,360 --> 00:27:08,320
colored partitions it's actually a lot

00:27:04,060 --> 00:27:09,940
easier to build on a graph database and

00:27:08,320 --> 00:27:11,290
so there's a nice combination of

00:27:09,940 --> 00:27:13,750
something called BigTable which is

00:27:11,290 --> 00:27:15,900
another Google cloud offering plus Janus

00:27:13,750 --> 00:27:20,020
graph which is an open-source project

00:27:15,900 --> 00:27:23,290
that can allow one to store graph data

00:27:20,020 --> 00:27:25,360
in something that's easier to do this

00:27:23,290 --> 00:27:27,250
kind of recursive traversal query than

00:27:25,360 --> 00:27:29,680
it is to do it in a scanning type of

00:27:27,250 --> 00:27:31,180
system like like bigquery so I've loaded

00:27:29,680 --> 00:27:35,170
some of the data into this into this

00:27:31,180 --> 00:27:37,750
database and the next step is to start

00:27:35,170 --> 00:27:40,120
to do these type of analysis to pre pre

00:27:37,750 --> 00:27:42,130
calculate things like what is the

00:27:40,120 --> 00:27:44,920
PageRank of a node or how central and

00:27:42,130 --> 00:27:46,840
important is a particular node within

00:27:44,920 --> 00:27:48,520
the network possibly within the scope of

00:27:46,840 --> 00:27:50,770
the transaction or within a time bounded

00:27:48,520 --> 00:27:53,490
scope to further characterize these

00:27:50,770 --> 00:27:55,090
nodes and edges to begin to make

00:27:53,490 --> 00:27:57,070
statements about what their

00:27:55,090 --> 00:27:58,450
characteristics are and then once you

00:27:57,070 --> 00:28:02,050
calculate these things you can put the

00:27:58,450 --> 00:28:03,850
data back into bigquery for filtering in

00:28:02,050 --> 00:28:06,220
an OLAP kind of scenario so we've got

00:28:03,850 --> 00:28:07,780
the oil TPE we've got an OLAP linear

00:28:06,220 --> 00:28:11,050
scan type database and then now we have

00:28:07,780 --> 00:28:12,640
a graph database for doing some like

00:28:11,050 --> 00:28:15,180
complex analysis to bring things back

00:28:12,640 --> 00:28:16,900
into the OLAP space for building reports

00:28:15,180 --> 00:28:19,480
so this is happening now

00:28:16,900 --> 00:28:21,130
if you're a CAG ler if you are in

00:28:19,480 --> 00:28:22,840
particular if you're a data scientist

00:28:21,130 --> 00:28:25,840
with Network science experience or

00:28:22,840 --> 00:28:27,040
you're a topologist please contact me I

00:28:25,840 --> 00:28:28,300
would love to talk to you about other

00:28:27,040 --> 00:28:33,730
interesting things we can do with these

00:28:28,300 --> 00:28:35,440
data to bring more clarity and

00:28:33,730 --> 00:28:39,190
transparency as to what's happening on

00:28:35,440 --> 00:28:41,470
the network also I'm looking for for

00:28:39,190 --> 00:28:42,880
more different data sources right so

00:28:41,470 --> 00:28:44,350
like this adapt rater database is

00:28:42,880 --> 00:28:47,350
curating a list of apps that's really

00:28:44,350 --> 00:28:51,390
great there are some other data sources

00:28:47,350 --> 00:28:51,390
that are available like in

00:28:52,450 --> 00:28:58,240
various types of document document sets

00:28:55,930 --> 00:29:00,640
that have been leaked to the media

00:28:58,240 --> 00:29:02,880
where you can get some information about

00:29:00,640 --> 00:29:05,560
what addresses might be associated with

00:29:02,880 --> 00:29:06,970
what named players so that could be an

00:29:05,560 --> 00:29:08,260
exchange it could be an individual could

00:29:06,970 --> 00:29:09,910
be an organization any number of things

00:29:08,260 --> 00:29:12,040
I mean help on this too so if you have

00:29:09,910 --> 00:29:13,410
this type of data or like to do scraping

00:29:12,040 --> 00:29:15,430
and crawling and want to work on

00:29:13,410 --> 00:29:17,200
aetherium or blockchain kind of stuff

00:29:15,430 --> 00:29:18,280
those kind of data can be joined in the

00:29:17,200 --> 00:29:19,840
same kind of way I was just showing you

00:29:18,280 --> 00:29:20,920
the method method signature data can be

00:29:19,840 --> 00:29:24,280
joined so that would also be really

00:29:20,920 --> 00:29:25,720
helpful yeah and just I was planning on

00:29:24,280 --> 00:29:27,490
releasing this actually like about a

00:29:25,720 --> 00:29:29,830
month ago but some other deadlines were

00:29:27,490 --> 00:29:32,200
hitting so it's now scheduled for q2 but

00:29:29,830 --> 00:29:34,680
I will be blocking this and letting the

00:29:32,200 --> 00:29:37,390
data out very very shortly

00:29:34,680 --> 00:29:39,370
that's all I got I do have quite a bit

00:29:37,390 --> 00:29:42,250
of time for questions like almost ten

00:29:39,370 --> 00:29:45,340
minutes so if you want to interact with

00:29:42,250 --> 00:29:48,600
me online please I'm pretty predictable

00:29:45,340 --> 00:29:51,010
and easy to find usually it's my name

00:29:48,600 --> 00:29:53,350
yeah finally let's let's talk about this

00:29:51,010 --> 00:29:55,480
and yeah we've got ten minutes to talk

00:29:53,350 --> 00:29:57,660
about it now freeform style so I'll take

00:29:55,480 --> 00:30:04,999
questions Thanks

00:29:57,660 --> 00:30:04,999
[Applause]

00:30:08,310 --> 00:30:13,810
okay we have a mic the questions up

00:30:11,260 --> 00:30:35,000
there up in the back

00:30:13,810 --> 00:30:38,480
okay shout it out what longest chain

00:30:35,000 --> 00:30:41,330
right yeah so in this case because there

00:30:38,480 --> 00:30:42,860
is a collision that happens I don't know

00:30:41,330 --> 00:30:45,200
if a collision has ever happened how

00:30:42,860 --> 00:30:46,910
will your system handle that because now

00:30:45,200 --> 00:30:48,740
you have two of the longest changed that

00:30:46,910 --> 00:30:50,420
to distribute it out and then whoever

00:30:48,740 --> 00:30:52,790
figures out the next longest chain that

00:30:50,420 --> 00:30:53,750
becomes the longest okay so how do you

00:30:52,790 --> 00:30:56,750
handle that right now

00:30:53,750 --> 00:30:58,640
yeah so each block that's loaded to the

00:30:56,750 --> 00:31:00,770
database has a it's a link to list

00:30:58,640 --> 00:31:03,110
effectively so it a block references its

00:31:00,770 --> 00:31:04,220
previous block and so when you have two

00:31:03,110 --> 00:31:05,630
blocks that are produced at the same

00:31:04,220 --> 00:31:08,810
time in the network's trying to fight

00:31:05,630 --> 00:31:10,070
out like which is the longest chain we

00:31:08,810 --> 00:31:11,360
don't take a position on that we just

00:31:10,070 --> 00:31:12,770
start both of them in the database and

00:31:11,360 --> 00:31:14,870
then eventually one of them appears to

00:31:12,770 --> 00:31:17,030
be a dead branch where it has only a few

00:31:14,870 --> 00:31:19,640
blocks kind of hanging off what becomes

00:31:17,030 --> 00:31:22,010
the main chain yeah so we capture all of

00:31:19,640 --> 00:31:24,350
that yeah those those branches and

00:31:22,010 --> 00:31:26,560
etherium are called uncles yeah uncle

00:31:24,350 --> 00:31:26,560
blocks

00:31:29,240 --> 00:31:32,240
questions

00:31:37,050 --> 00:31:47,260
lots are those queryable as well yeah

00:31:40,060 --> 00:31:48,520
yeah yeah you can see them sure it's all

00:31:47,260 --> 00:31:50,110
well I mean it's just a sequel query

00:31:48,520 --> 00:31:51,370
right so the data are all structured and

00:31:50,110 --> 00:32:09,059
ready to go for you so you can do

00:31:51,370 --> 00:32:14,929
whatever you want with that don't be shy

00:32:09,059 --> 00:32:14,929
ask me stop here we go

00:32:18,860 --> 00:32:26,030
also any plans to start capturing

00:32:23,160 --> 00:32:28,860
stellar or some of the other

00:32:26,030 --> 00:32:30,440
alternatives yeah there's actually some

00:32:28,860 --> 00:32:33,090
so I mentioned you know a lot of these

00:32:30,440 --> 00:32:35,250
icos and cryptocurrency projects are are

00:32:33,090 --> 00:32:36,480
on a theory 'm there's not really

00:32:35,250 --> 00:32:37,560
blocked ins for them yet but there are

00:32:36,480 --> 00:32:42,180
some other ones for which their chains

00:32:37,560 --> 00:32:44,160
speller ripple - few others yeah there's

00:32:42,180 --> 00:32:46,740
interesting analysis can be done on all

00:32:44,160 --> 00:32:48,060
of those I'm I would say I'm a bit

00:32:46,740 --> 00:32:49,620
biased toward looking at things that

00:32:48,060 --> 00:32:52,590
have something more than just a

00:32:49,620 --> 00:32:54,390
distributed ledger and something more

00:32:52,590 --> 00:32:56,520
like a virtual machine there's different

00:32:54,390 --> 00:32:58,830
I guess more interesting types of

00:32:56,520 --> 00:33:02,010
insights that could be had by looking at

00:32:58,830 --> 00:33:03,900
those datasets but I've I've not yet

00:33:02,010 --> 00:33:06,150
been preparing any of them if you have

00:33:03,900 --> 00:33:08,280
some specific suggestions about what

00:33:06,150 --> 00:33:08,970
might be interesting and why I would I

00:33:08,280 --> 00:33:12,930
would love to have that conversation

00:33:08,970 --> 00:33:14,880
either afterward or or or on Twitter we

00:33:12,930 --> 00:33:17,060
can start a start a thread yeah so good

00:33:14,880 --> 00:33:17,060
idea

00:33:23,090 --> 00:33:27,330
here we go

00:33:24,570 --> 00:33:31,110
I have question about the real-time

00:33:27,330 --> 00:33:33,420
transaction yeah so can it support a

00:33:31,110 --> 00:33:35,970
real-time transaction I mean in the

00:33:33,420 --> 00:33:37,710
memory member pool ah no I'm not

00:33:35,970 --> 00:33:40,980
supporting them pool so so the question

00:33:37,710 --> 00:33:43,080
is related to when when a would appear

00:33:40,980 --> 00:33:45,000
what wants to submit a transaction to

00:33:43,080 --> 00:33:47,450
the network it's not immediately added

00:33:45,000 --> 00:33:50,430
to the block and there's actually this

00:33:47,450 --> 00:33:51,930
queue of transactions that could be

00:33:50,430 --> 00:33:53,970
added to the next block and usually what

00:33:51,930 --> 00:33:55,650
the miner will do is grab the most

00:33:53,970 --> 00:33:57,420
valuable ones for somebody who bid to

00:33:55,650 --> 00:33:59,730
add their transaction next because it's

00:33:57,420 --> 00:34:02,430
a high party transaction that's what

00:33:59,730 --> 00:34:05,070
you're getting at we are not currently

00:34:02,430 --> 00:34:07,470
monitoring the mempool so we are

00:34:05,070 --> 00:34:09,570
something like I guess it's it depends

00:34:07,470 --> 00:34:11,790
on how long a transactions transaction

00:34:09,570 --> 00:34:14,850
can sit in the mempool but it's usually

00:34:11,790 --> 00:34:16,649
not more than a few minutes like five

00:34:14,850 --> 00:34:17,940
blocks ahead so something like 75

00:34:16,649 --> 00:34:20,880
seconds

00:34:17,940 --> 00:34:22,230
so we're 75 seconds behind real-time but

00:34:20,880 --> 00:34:23,700
realize those transactions that are

00:34:22,230 --> 00:34:25,679
proposed in the men pool can actually be

00:34:23,700 --> 00:34:27,960
cancelled out before they're executed or

00:34:25,679 --> 00:34:28,409
they could even fail so indexing the

00:34:27,960 --> 00:34:30,629
things in the

00:34:28,409 --> 00:34:32,280
Poole I'm not really sure how much how

00:34:30,629 --> 00:34:33,329
much value there is there although if

00:34:32,280 --> 00:34:34,950
you're doing trading it could be an

00:34:33,329 --> 00:34:36,599
interesting like leading indicator of

00:34:34,950 --> 00:34:38,280
what's about to happen

00:34:36,599 --> 00:34:49,119
yeah I'm not currently tracking that

00:34:38,280 --> 00:34:52,419
it's a great question okay

00:34:49,119 --> 00:35:00,519
Oh maybe I don't know it kind of trickle

00:34:52,419 --> 00:35:02,829
in hi this looks all really beautiful

00:35:00,519 --> 00:35:06,279
but is there a particular reason of why

00:35:02,829 --> 00:35:07,749
you are doing this why I'm doing this I

00:35:06,279 --> 00:35:10,349
thought it was just really interesting

00:35:07,749 --> 00:35:13,119
like you hear a lot of so-so okay so

00:35:10,349 --> 00:35:14,859
take a step back I'm a developer

00:35:13,119 --> 00:35:17,140
advocate I'm in developer relations I

00:35:14,859 --> 00:35:19,329
get basically marked on my performance

00:35:17,140 --> 00:35:21,700
of bringing attention to Google cloud

00:35:19,329 --> 00:35:24,880
there is a lot of hype around blockchain

00:35:21,700 --> 00:35:27,970
and a lot of what's being disliked a lot

00:35:24,880 --> 00:35:29,680
of the narrative in the media there's no

00:35:27,970 --> 00:35:32,369
substance behind it it's like somebody

00:35:29,680 --> 00:35:35,499
on some news network spouting nonsense

00:35:32,369 --> 00:35:37,119
coming from who knows we're not actually

00:35:35,499 --> 00:35:38,980
referencing any data right and if you

00:35:37,119 --> 00:35:42,119
look at a lot of websites like that do

00:35:38,980 --> 00:35:44,680
have a chart it's some low res gift that

00:35:42,119 --> 00:35:47,200
came out of probably a consultancy and

00:35:44,680 --> 00:35:49,839
was resized repeatedly nobody has access

00:35:47,200 --> 00:35:53,109
to the real data right so giving people

00:35:49,839 --> 00:35:56,410
more access to data produces like a more

00:35:53,109 --> 00:35:58,539
sophisticated concrete conversation in

00:35:56,410 --> 00:36:00,579
the public discourse space about what's

00:35:58,539 --> 00:36:02,259
really going on so now we don't have to

00:36:00,579 --> 00:36:03,880
depend on these consultants anymore

00:36:02,259 --> 00:36:06,339
who are operating these databases and

00:36:03,880 --> 00:36:07,809
selling access to build reports and you

00:36:06,339 --> 00:36:09,489
can't really influence what report

00:36:07,809 --> 00:36:11,859
they're gonna build for you unless you

00:36:09,489 --> 00:36:14,049
pay them right we just give the data

00:36:11,859 --> 00:36:15,989
away for free please go build the report

00:36:14,049 --> 00:36:20,170
you want and talk about that

00:36:15,989 --> 00:36:21,789
yeah it's democratizing the data googles

00:36:20,170 --> 00:36:23,950
mission right making the data publicly

00:36:21,789 --> 00:36:26,049
available accessible and useful

00:36:23,950 --> 00:36:29,499
so yeah completely in line with

00:36:26,049 --> 00:36:34,380
corporate mission whoa Google is awesome

00:36:29,499 --> 00:36:34,380
yeah right we love open data at Google

00:36:47,510 --> 00:36:52,380
so how big is the data set on bigquery

00:36:50,550 --> 00:36:53,880
cuz you said you have 500 gigs free to

00:36:52,380 --> 00:36:55,470
use I'm wondering how many queries kind

00:36:53,880 --> 00:36:57,000
of yeah so if you did select star and

00:36:55,470 --> 00:37:01,530
all the columns you would burn it up so

00:36:57,000 --> 00:37:03,839
bigquery sorry Bitcoin the the primary

00:37:01,530 --> 00:37:05,760
data is 200 gigs and we do a little bit

00:37:03,839 --> 00:37:08,010
denormalization to make it easy to query

00:37:05,760 --> 00:37:09,150
in bigquery and so it's 500 gigs so if

00:37:08,010 --> 00:37:10,349
you did select start all the fields

00:37:09,150 --> 00:37:12,930
you'd burn it up but in reality you're

00:37:10,349 --> 00:37:13,319
not doing that if your iam is a lot

00:37:12,930 --> 00:37:16,800
smaller

00:37:13,319 --> 00:37:18,810
if theorem is actually 70 gigabytes for

00:37:16,800 --> 00:37:22,650
the top rate the full node and it's 200

00:37:18,810 --> 00:37:24,000
gigabytes in bigquery yeah they're not

00:37:22,650 --> 00:37:26,190
huge data sets they're just very

00:37:24,000 --> 00:37:28,440
difficult to access even it's public

00:37:26,190 --> 00:37:36,150
open data already uncontrolled by anyone

00:37:28,440 --> 00:37:38,430
just really hard to get to yeah sure all

00:37:36,150 --> 00:37:44,450
right one minute 36 seconds I'm calling

00:37:38,430 --> 00:37:44,450

YouTube URL: https://www.youtube.com/watch?v=_IX6bhE6Qu4


