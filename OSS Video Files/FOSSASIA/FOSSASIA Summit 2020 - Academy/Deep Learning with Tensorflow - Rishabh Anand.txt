Title: Deep Learning with Tensorflow - Rishabh Anand
Publication date: 2020-04-01
Playlist: FOSSASIA Summit 2020 - Academy
Description: 
	Get started with TensorFlow, the largest growing open-source Machine Learning library, to manipulate data and build neural networks without hassle.

FOSSASIA Summit 2020 - Academy

Speaker: Rishabh Anand, A*STAR
Captions: 
	00:00:00,030 --> 00:00:04,290
right so I'm an active open-source

00:00:02,280 --> 00:00:05,960
contributor on github training AI

00:00:04,290 --> 00:00:10,410
projects like tensorflow

00:00:05,960 --> 00:00:13,559
Kara's some products by hugging face IBM

00:00:10,410 --> 00:00:16,920
the the sought I'm a writer

00:00:13,559 --> 00:00:20,699
on AI and machine learning data science

00:00:16,920 --> 00:00:23,310
on medium.com and I've been working in

00:00:20,699 --> 00:00:26,699
the machine learning and AI field for

00:00:23,310 --> 00:00:30,150
the past five years at research

00:00:26,699 --> 00:00:34,620
institutions local startups doing my own

00:00:30,150 --> 00:00:37,140
side projects so this is I guess what

00:00:34,620 --> 00:00:41,700
I've been doing for the past past while

00:00:37,140 --> 00:00:44,610
and I picked up tensorflow ever since

00:00:41,700 --> 00:00:47,250
its beta when its beta was still in

00:00:44,610 --> 00:00:50,820
development I started learning

00:00:47,250 --> 00:00:52,620
tensorflow and using it since then so

00:00:50,820 --> 00:00:55,920
this is just a rundown of what we're

00:00:52,620 --> 00:00:58,379
gonna do today I'll start off with some

00:00:55,920 --> 00:00:59,640
prerequisites you know what exactly is

00:00:58,379 --> 00:01:01,920
needed

00:00:59,640 --> 00:01:04,619
I guess understand at a high level what

00:01:01,920 --> 00:01:07,350
I'll be talking about today I'll give a

00:01:04,619 --> 00:01:10,080
quick refresher to machine learning in

00:01:07,350 --> 00:01:15,060
general in terms of like the terminology

00:01:10,080 --> 00:01:19,130
I'll be using some of the some of the

00:01:15,060 --> 00:01:22,290
concepts that this talk will cover and

00:01:19,130 --> 00:01:25,860
from then on once we're comfortable with

00:01:22,290 --> 00:01:28,740
that we'll move on into PR tensorflow

00:01:25,860 --> 00:01:29,490
what it is who is using it what can you

00:01:28,740 --> 00:01:31,650
do with it

00:01:29,490 --> 00:01:35,250
what tensorflow provides for the average

00:01:31,650 --> 00:01:37,619
developer / researcher and after that

00:01:35,250 --> 00:01:41,430
we'll be doing a bit of live coding so

00:01:37,619 --> 00:01:44,399
if you have your laptop's at hand you

00:01:41,430 --> 00:01:50,100
can you can follow along we'll be

00:01:44,399 --> 00:01:51,299
covering two I guess exercises that kind

00:01:50,100 --> 00:01:51,990
of get you up and running with

00:01:51,299 --> 00:01:55,100
tensorflow

00:01:51,990 --> 00:01:59,189
the API the syntax used intensive flow

00:01:55,100 --> 00:02:01,020
and how you can build data pipelines all

00:01:59,189 --> 00:02:03,930
the way from I guess loading and

00:02:01,020 --> 00:02:05,610
pre-processing your data all the way to

00:02:03,930 --> 00:02:08,819
building your models training and

00:02:05,610 --> 00:02:14,360
testing it and checking its performance

00:02:08,819 --> 00:02:14,360
metrics and yeah let's get started

00:02:15,400 --> 00:02:21,320
so at a high level to understand

00:02:19,250 --> 00:02:22,610
tensorflow you first need to understand

00:02:21,320 --> 00:02:24,200
what machine learning is and to

00:02:22,610 --> 00:02:26,840
understand what machine learning is you

00:02:24,200 --> 00:02:30,200
need to have a basic math background you

00:02:26,840 --> 00:02:33,050
know linear algebra calculus so even to

00:02:30,200 --> 00:02:37,400
understand this talk you need to have

00:02:33,050 --> 00:02:41,600
some bit of background in in these two

00:02:37,400 --> 00:02:44,440
topics and then basic understanding of

00:02:41,600 --> 00:02:46,880
machine learning concepts neurons layers

00:02:44,440 --> 00:02:52,280
activation neural networks in general

00:02:46,880 --> 00:02:58,280
how they work you probably also may need

00:02:52,280 --> 00:03:01,760
to know like a fair bit on what a neural

00:02:58,280 --> 00:03:04,790
network is and what makes it work or the

00:03:01,760 --> 00:03:06,830
underlying architecture or the mechanism

00:03:04,790 --> 00:03:09,320
inside the neural network that makes it

00:03:06,830 --> 00:03:13,640
do makes it do what it's supposed to do

00:03:09,320 --> 00:03:16,000
and of course to even understand the

00:03:13,640 --> 00:03:20,000
tensorflow syntax of the thames flow api

00:03:16,000 --> 00:03:21,380
you need to be proficient in Python in

00:03:20,000 --> 00:03:25,030
terms of you know creating functions

00:03:21,380 --> 00:03:27,980
working with classes working with

00:03:25,030 --> 00:03:31,130
different variables and you know using

00:03:27,980 --> 00:03:36,680
packages especially for a numerical

00:03:31,130 --> 00:03:38,900
computing and visualization so now I'll

00:03:36,680 --> 00:03:41,180
give a quick recap on you know machine

00:03:38,900 --> 00:03:42,980
learning in general so I think we can

00:03:41,180 --> 00:03:48,050
spend a bit of time touching that before

00:03:42,980 --> 00:03:49,850
we deep dive into tensorflow so machine

00:03:48,050 --> 00:03:52,490
learning it's a subset of artificial

00:03:49,850 --> 00:03:55,310
intelligence it's where you give data to

00:03:52,490 --> 00:04:00,590
a computing system it finds patterns in

00:03:55,310 --> 00:04:05,330
that data and it's able to predict or

00:04:00,590 --> 00:04:08,900
off of unseen data so if I have a stock

00:04:05,330 --> 00:04:11,660
market model I just train it or train

00:04:08,900 --> 00:04:14,420
that model on historical stock data

00:04:11,660 --> 00:04:17,150
stock price data and you know given a

00:04:14,420 --> 00:04:19,250
few variables I can get it predict

00:04:17,150 --> 00:04:22,270
tomorrow's die a stock price data with a

00:04:19,250 --> 00:04:26,870
certain X amount of confidence or

00:04:22,270 --> 00:04:30,290
accuracy so machine learning

00:04:26,870 --> 00:04:32,810
the two major categories supervised and

00:04:30,290 --> 00:04:35,750
unsupervised supervised means that your

00:04:32,810 --> 00:04:38,870
training data set it consists of both

00:04:35,750 --> 00:04:40,550
the feature space and the labels so the

00:04:38,870 --> 00:04:42,949
features are something that described

00:04:40,550 --> 00:04:46,400
the object for example if it's an apple

00:04:42,949 --> 00:04:49,669
well you know we can look at its color

00:04:46,400 --> 00:04:52,460
its redness in terms of RGB values they

00:04:49,669 --> 00:04:55,010
can be a feature it's weight it could be

00:04:52,460 --> 00:04:57,080
a feature its diameter anything that

00:04:55,010 --> 00:04:59,720
uniquely describes that object is

00:04:57,080 --> 00:05:02,360
considered a feature and the label which

00:04:59,720 --> 00:05:04,850
means like yes it's an apple

00:05:02,360 --> 00:05:07,699
yep that's the label so this could also

00:05:04,850 --> 00:05:10,190
work for other objects like an orange

00:05:07,699 --> 00:05:13,400
same thing you'd want to take its color

00:05:10,190 --> 00:05:15,320
its mass its diameter with the class

00:05:13,400 --> 00:05:17,449
label of orange so that's what

00:05:15,320 --> 00:05:20,630
supervised learning is it's where you

00:05:17,449 --> 00:05:22,550
give all all the data to the machine

00:05:20,630 --> 00:05:26,300
learning algorithm to find patterns in

00:05:22,550 --> 00:05:27,800
it semi-supervised back then it was

00:05:26,300 --> 00:05:30,710
called unsupervised learning

00:05:27,800 --> 00:05:34,070
so now semi-supervised learning is you

00:05:30,710 --> 00:05:37,820
just give the algorithm just the

00:05:34,070 --> 00:05:39,590
features so the model or the algorithm

00:05:37,820 --> 00:05:42,710
finds its own patterns in the data

00:05:39,590 --> 00:05:45,110
either through clustering or some

00:05:42,710 --> 00:05:47,990
mechanism that it enables it to

00:05:45,110 --> 00:05:50,930
understand what what's going on in the

00:05:47,990 --> 00:05:53,150
data its distribution and things to

00:05:50,930 --> 00:05:57,289
describe that distribution it figures

00:05:53,150 --> 00:05:59,210
all of it out by itself and given data

00:05:57,289 --> 00:06:02,300
given new data that it hasn't seen

00:05:59,210 --> 00:06:04,639
before you can use this algorithm to

00:06:02,300 --> 00:06:07,220
predict off of it seeing which cluster

00:06:04,639 --> 00:06:12,560
it most probably belongs to so that

00:06:07,220 --> 00:06:14,810
semi-supervised so neurons

00:06:12,560 --> 00:06:18,229
you know neurons it's a terminology it's

00:06:14,810 --> 00:06:19,729
I guess inspired by the human brain

00:06:18,229 --> 00:06:24,229
itself the new neurons in the human

00:06:19,729 --> 00:06:26,570
brain so when you have a neuron it takes

00:06:24,229 --> 00:06:28,310
in information from one side and it

00:06:26,570 --> 00:06:31,870
spits out something after doing

00:06:28,310 --> 00:06:35,810
something to its input similarly a

00:06:31,870 --> 00:06:38,210
neuron in machine learning terms it's a

00:06:35,810 --> 00:06:39,590
mathematical representation of what the

00:06:38,210 --> 00:06:44,300
biologic

00:06:39,590 --> 00:06:46,460
like biological neuron does so it it

00:06:44,300 --> 00:06:48,710
computes the equation of a line so you

00:06:46,460 --> 00:06:50,720
know in high school we learned that y

00:06:48,710 --> 00:06:53,000
equals MX plus C is the equation of a

00:06:50,720 --> 00:06:56,180
line M being the gradient and C being

00:06:53,000 --> 00:06:59,980
the y-intercept in machine learning the

00:06:56,180 --> 00:07:03,620
gradient M is called the weight W and

00:06:59,980 --> 00:07:06,110
y-intercept C is called the bias B and X

00:07:03,620 --> 00:07:10,670
as you can see in the last point it is

00:07:06,110 --> 00:07:13,970
the input so what a neuron does is it

00:07:10,670 --> 00:07:17,450
takes the input X it multiplies it or it

00:07:13,970 --> 00:07:20,120
does a dot product with with its

00:07:17,450 --> 00:07:22,430
respective weight and adds a bias to it

00:07:20,120 --> 00:07:24,920
and then it adds this thing called the

00:07:22,430 --> 00:07:27,410
non-linearity to it which is the

00:07:24,920 --> 00:07:29,720
activation function so when you have

00:07:27,410 --> 00:07:32,720
well a line which is linear in nature

00:07:29,720 --> 00:07:35,330
and you have complex data you want to

00:07:32,720 --> 00:07:39,290
learn some nonlinear mapping between x

00:07:35,330 --> 00:07:41,210
and y X is the input Y is the output so

00:07:39,290 --> 00:07:43,570
you want some nonlinear representation

00:07:41,210 --> 00:07:45,710
mapping the two so when you add

00:07:43,570 --> 00:07:50,720
non-linearity as an activation function

00:07:45,710 --> 00:07:52,640
it enables better learning it allows the

00:07:50,720 --> 00:07:52,970
model to find better patterns in the

00:07:52,640 --> 00:07:56,900
data

00:07:52,970 --> 00:07:58,850
and usually nonlinear models they're

00:07:56,900 --> 00:08:00,440
more accurate and they're much more

00:07:58,850 --> 00:08:07,820
versatile when you apply them to real

00:08:00,440 --> 00:08:08,900
life data alright so if you've I guess

00:08:07,820 --> 00:08:12,170
if you're familiar with machine learning

00:08:08,900 --> 00:08:13,940
in general you would have seen some of

00:08:12,170 --> 00:08:16,370
these layers you know you see the fully

00:08:13,940 --> 00:08:19,420
connected layer which is just a stack of

00:08:16,370 --> 00:08:22,790
neurons one on top of each other

00:08:19,420 --> 00:08:25,850
convolutional layers they take in an

00:08:22,790 --> 00:08:28,010
input image and they come and they

00:08:25,850 --> 00:08:30,500
perform a convolution function on it you

00:08:28,010 --> 00:08:33,140
know if you have an image you can think

00:08:30,500 --> 00:08:36,320
of the the convolutional kernel as this

00:08:33,140 --> 00:08:38,720
flashlight I'm you know scanning the

00:08:36,320 --> 00:08:43,760
image parts of the image one by one row

00:08:38,720 --> 00:08:47,630
by row and based on that I have to

00:08:43,760 --> 00:08:51,350
control thing I have I guess a matrix

00:08:47,630 --> 00:08:53,839
that it I can figure

00:08:51,350 --> 00:08:55,910
the features from that matrix and infer

00:08:53,839 --> 00:08:58,070
what that object could be or what that

00:08:55,910 --> 00:09:02,170
image could be after performing a series

00:08:58,070 --> 00:09:04,759
of convolutional functions on it

00:09:02,170 --> 00:09:06,649
maximum pooling it's usually a comput

00:09:04,759 --> 00:09:09,620
usually accompanies the convolutional

00:09:06,649 --> 00:09:12,470
layers max pooling it lowers the

00:09:09,620 --> 00:09:18,170
dimensionality makes it easier for the

00:09:12,470 --> 00:09:20,720
model to learn so it so when you have a

00:09:18,170 --> 00:09:23,269
convolutional layer apply to some input

00:09:20,720 --> 00:09:27,980
data and you have a max pool layer after

00:09:23,269 --> 00:09:30,230
that it lowers the number of parameters

00:09:27,980 --> 00:09:33,079
in the network and makes it much more

00:09:30,230 --> 00:09:34,790
lighter so parameters here refers to the

00:09:33,079 --> 00:09:37,810
weights and biases in the individual

00:09:34,790 --> 00:09:39,949
neurons so if you apply max pooling

00:09:37,810 --> 00:09:41,870
immediately after a convolution it

00:09:39,949 --> 00:09:44,269
lowers the dimensionality thus bringing

00:09:41,870 --> 00:09:46,190
down the parameter count making the

00:09:44,269 --> 00:09:48,920
model faster if you were to put it into

00:09:46,190 --> 00:09:50,389
production so flattened it's it's a

00:09:48,920 --> 00:09:52,190
simple concept if you have an n

00:09:50,389 --> 00:09:55,730
dimensional matrix for example I have a

00:09:52,190 --> 00:09:58,459
matrix 2 by 3 matrix and if I were to

00:09:55,730 --> 00:10:01,610
apply a flattened layer on it it would

00:09:58,459 --> 00:10:04,160
give me a 6 by 1 vector because 2 times

00:10:01,610 --> 00:10:07,459
3 is 6 and it would just give me a

00:10:04,160 --> 00:10:10,850
single column or a single row after you

00:10:07,459 --> 00:10:15,010
flatten it so you can think of the

00:10:10,850 --> 00:10:19,970
flattened layer as you know I bring in I

00:10:15,010 --> 00:10:21,560
bring in bread put it in and when it

00:10:19,970 --> 00:10:23,060
goes through the flattened layer you

00:10:21,560 --> 00:10:25,399
just basically get like a disk at the

00:10:23,060 --> 00:10:30,649
end so you can imagine it like that so

00:10:25,399 --> 00:10:32,029
it returns a 1 by M or M by 1 vector so

00:10:30,649 --> 00:10:33,500
when you take these different layers in

00:10:32,029 --> 00:10:35,630
the previous slide and you put them

00:10:33,500 --> 00:10:37,310
together in different combinations you

00:10:35,630 --> 00:10:41,180
get this thing called the architecture

00:10:37,310 --> 00:10:44,260
the neural network architecture so the

00:10:41,180 --> 00:10:47,180
architecture individual layers they have

00:10:44,260 --> 00:10:49,550
parameters the weights and biases in the

00:10:47,180 --> 00:10:51,230
individual neurons so an architecture is

00:10:49,550 --> 00:10:53,269
just a huge collection of all of them

00:10:51,230 --> 00:10:56,000
together so an architect in an

00:10:53,269 --> 00:10:58,459
architecture or a model you take in

00:10:56,000 --> 00:11:01,670
inputs X and you map them through that

00:10:58,459 --> 00:11:05,240
entire neural network to the output Y so

00:11:01,670 --> 00:11:09,170
when you through the process of training

00:11:05,240 --> 00:11:12,140
when you fine-tune the values of W width

00:11:09,170 --> 00:11:15,050
W and biases B in the network when you

00:11:12,140 --> 00:11:17,810
train the parameters the network is

00:11:15,050 --> 00:11:20,649
becoming I guess more adept at creating

00:11:17,810 --> 00:11:23,510
that representation between x and y so

00:11:20,649 --> 00:11:24,860
when you have after fully training the

00:11:23,510 --> 00:11:28,070
network when you have that final

00:11:24,860 --> 00:11:31,279
configuration of parameters W and B in

00:11:28,070 --> 00:11:34,580
all the layers all the neurons you can

00:11:31,279 --> 00:11:36,110
call it a frozen model and this is the

00:11:34,580 --> 00:11:38,930
final configuration you can put that

00:11:36,110 --> 00:11:44,120
final model into production and use it

00:11:38,930 --> 00:11:46,360
to predict unseen data so whatever I

00:11:44,120 --> 00:11:49,430
mentioned neurons layers architecture

00:11:46,360 --> 00:11:52,430
this slide kind of summarizes everything

00:11:49,430 --> 00:11:54,050
brings it all together so all you know

00:11:52,430 --> 00:11:55,850
the yellow dots green dots and the

00:11:54,050 --> 00:11:59,000
purple dots they're all the individual

00:11:55,850 --> 00:12:02,270
neurons and here it's just fully

00:11:59,000 --> 00:12:04,070
connected Lea all the four layers

00:12:02,270 --> 00:12:06,589
they're fully connected which means

00:12:04,070 --> 00:12:09,800
they're just a stack of neurons it takes

00:12:06,589 --> 00:12:14,089
in inputs you know X 1 2 3 4 or how many

00:12:09,800 --> 00:12:17,060
ever inputs you have and it Maps it the

00:12:14,089 --> 00:12:20,529
output Y so for example if I were to

00:12:17,060 --> 00:12:23,899
take an image supposing the layer 1 l1

00:12:20,529 --> 00:12:25,970
takes in an image and the network is

00:12:23,899 --> 00:12:28,610
supposed to predict which class it

00:12:25,970 --> 00:12:32,540
belongs to out of 4 you know is it Y 1 2

00:12:28,610 --> 00:12:35,149
3 & 4 you would build such a network

00:12:32,540 --> 00:12:37,060
that takes in that image it does

00:12:35,149 --> 00:12:40,160
something in the middle and it spits out

00:12:37,060 --> 00:12:41,930
the classified value or pertaining to

00:12:40,160 --> 00:12:44,990
that image so if I give it a picture of

00:12:41,930 --> 00:12:48,680
a car it would tell me ok out of for

00:12:44,990 --> 00:12:51,760
example apple orange car and bus it

00:12:48,680 --> 00:12:54,950
would fire that specific neuron saying

00:12:51,760 --> 00:12:56,270
yes this is a bus sorry this yes this is

00:12:54,950 --> 00:12:59,060
a car so yeah

00:12:56,270 --> 00:13:04,540
X to Y mapping that's ultimately what a

00:12:59,060 --> 00:13:09,709
neural network is so tensorflow is a

00:13:04,540 --> 00:13:11,600
library built by Google ai it if I'm not

00:13:09,709 --> 00:13:15,140
wrong it really it's beta version

00:13:11,600 --> 00:13:17,000
released somewhere back in 2015 and ever

00:13:15,140 --> 00:13:18,420
since then it's one of the largest

00:13:17,000 --> 00:13:22,200
growing machine learning for

00:13:18,420 --> 00:13:25,980
works on the planet today so you use

00:13:22,200 --> 00:13:27,600
tensorflow in any one of these languages

00:13:25,980 --> 00:13:29,700
you're comfortable with it has lots of

00:13:27,600 --> 00:13:31,740
support so you can use it in any one of

00:13:29,700 --> 00:13:35,070
these languages to build neural networks

00:13:31,740 --> 00:13:38,519
and you can train these neural networks

00:13:35,070 --> 00:13:40,589
in their specific languages tensorflow

00:13:38,519 --> 00:13:43,889
is even compatible with different kinds

00:13:40,589 --> 00:13:45,199
of hardware like you know GPUs GPUs and

00:13:43,889 --> 00:13:48,300
I'll cover that later

00:13:45,199 --> 00:13:50,430
so tensorflow it when you break it down

00:13:48,300 --> 00:13:52,079
you get tenser and flow tenser you can

00:13:50,430 --> 00:13:54,990
think of it as an N dimensional array

00:13:52,079 --> 00:13:57,740
and flow just means the movement or the

00:13:54,990 --> 00:14:01,110
flow of these n dimensional arrays

00:13:57,740 --> 00:14:03,510
through the neural network to get that

00:14:01,110 --> 00:14:07,949
final mapping or representation when you

00:14:03,510 --> 00:14:11,490
train it so attentive law it's widely

00:14:07,949 --> 00:14:14,250
used it's used by you know major

00:14:11,490 --> 00:14:17,880
corporations it's used by you know all

00:14:14,250 --> 00:14:20,610
of these even the major players in most

00:14:17,880 --> 00:14:24,380
industries they use tensor flow for

00:14:20,610 --> 00:14:27,870
different projects different products

00:14:24,380 --> 00:14:31,199
research institutions even a star were

00:14:27,870 --> 00:14:34,589
heavy on tensor flow for performing some

00:14:31,199 --> 00:14:36,540
you know simulations when you're quickly

00:14:34,589 --> 00:14:39,110
crunching some numbers tensor floors or

00:14:36,540 --> 00:14:41,190
go to a framework and you know

00:14:39,110 --> 00:14:44,160
practitioners hobbyists even now

00:14:41,190 --> 00:14:46,829
research students in you know even high

00:14:44,160 --> 00:14:49,079
school university educators they're all

00:14:46,829 --> 00:14:51,420
using tensor flow for their classes and

00:14:49,079 --> 00:14:53,430
now you know hopefully by the end of

00:14:51,420 --> 00:14:58,320
this even you know you're able to use

00:14:53,430 --> 00:15:03,750
tensor flow to a fair extent so tensor

00:14:58,320 --> 00:15:07,050
flow while it has its theoretical usages

00:15:03,750 --> 00:15:09,060
you can also use tensor flow for all

00:15:07,050 --> 00:15:12,269
these sorts of things all these sorts of

00:15:09,060 --> 00:15:15,000
things so you have Twitter Facebook you

00:15:12,269 --> 00:15:16,769
know we know Facebook uses PI touch the

00:15:15,000 --> 00:15:18,720
different thing so you have Twitter you

00:15:16,769 --> 00:15:20,910
have all these companies like Airbnb

00:15:18,720 --> 00:15:22,980
they're recommending products to use

00:15:20,910 --> 00:15:25,490
Singapore Airlines they're recommend

00:15:22,980 --> 00:15:28,069
flights to you so they're all building

00:15:25,490 --> 00:15:31,970
recommendation systems using tensorflow

00:15:28,069 --> 00:15:36,350
even social media like

00:15:31,970 --> 00:15:38,240
LinkedIn they are heavy on tensorflow

00:15:36,350 --> 00:15:40,339
when bringing people together or

00:15:38,240 --> 00:15:43,339
connecting them or recommending who you

00:15:40,339 --> 00:15:47,089
should connect with next fraud and fault

00:15:43,339 --> 00:15:49,040
detection when when you have a camera

00:15:47,089 --> 00:15:52,189
and you're focusing on something and you

00:15:49,040 --> 00:15:54,230
detect a crack in for example a piece of

00:15:52,189 --> 00:15:57,139
metal you know you can use tensorflow

00:15:54,230 --> 00:15:59,600
for that now Twitter is even using it

00:15:57,139 --> 00:16:01,060
has a new fake news detection algorithm

00:15:59,600 --> 00:16:03,860
so whenever you come across a piece of

00:16:01,060 --> 00:16:07,490
external news it'll actually tell you

00:16:03,860 --> 00:16:09,500
this may be misleading or this news in

00:16:07,490 --> 00:16:12,800
like this news piece may have been

00:16:09,500 --> 00:16:14,480
manipulated so please verify so open

00:16:12,800 --> 00:16:16,329
source software we have companies like

00:16:14,480 --> 00:16:17,509
hugging face all of them using

00:16:16,329 --> 00:16:19,879
tensorflow

00:16:17,509 --> 00:16:21,500
to build their open-source projects

00:16:19,879 --> 00:16:23,360
giving it and you know putting it into

00:16:21,500 --> 00:16:26,620
the hands of developers around the world

00:16:23,360 --> 00:16:30,649
and you know again research applications

00:16:26,620 --> 00:16:32,540
companies like open AI deep mind

00:16:30,649 --> 00:16:34,850
google's deepmind they're using it for

00:16:32,540 --> 00:16:36,290
self-driving car research computer

00:16:34,850 --> 00:16:41,269
vision and natural language processing

00:16:36,290 --> 00:16:44,600
research so i guess just a humor all of

00:16:41,269 --> 00:16:46,339
you a south korean father his son was a

00:16:44,600 --> 00:16:47,949
software engineer he used tenth the

00:16:46,339 --> 00:16:50,990
fluid to create a cucumber

00:16:47,949 --> 00:16:53,360
classification model so in like the in

00:16:50,990 --> 00:16:55,670
like the farm whenever cucumbers from

00:16:53,360 --> 00:16:58,819
the farm they were bought into into the

00:16:55,670 --> 00:17:01,040
warehouse the classifier would predict

00:16:58,819 --> 00:17:03,170
what class of cucumbers it would belong

00:17:01,040 --> 00:17:04,850
to and you know there would be like the

00:17:03,170 --> 00:17:08,120
sweeper thing that kind of sweeps it

00:17:04,850 --> 00:17:10,669
into its specific bucket so yeah even

00:17:08,120 --> 00:17:12,850
you know farms Agrotech you know

00:17:10,669 --> 00:17:15,589
tensorflow is big in that so yeah

00:17:12,850 --> 00:17:17,809
tensorflow its usage is it's really just

00:17:15,589 --> 00:17:21,140
constrained by your imagination what you

00:17:17,809 --> 00:17:24,709
want to use it for so tensorflow

00:17:21,140 --> 00:17:27,530
while you know it it has its merits we

00:17:24,709 --> 00:17:30,020
you know why specifically the companies

00:17:27,530 --> 00:17:32,480
use tensorflow why does a single

00:17:30,020 --> 00:17:37,159
developer use tensorflow tensorflow is

00:17:32,480 --> 00:17:42,470
kind of like this huge ecosystem of of

00:17:37,159 --> 00:17:45,049
many different products that enable you

00:17:42,470 --> 00:17:45,730
to do different things so on one hand it

00:17:45,049 --> 00:17:49,059
lets you

00:17:45,730 --> 00:17:51,760
simple data pipelines it lets you ingest

00:17:49,059 --> 00:17:54,220
data load pre-process all of them

00:17:51,760 --> 00:17:57,059
it lets you build and reuse neural

00:17:54,220 --> 00:17:59,260
network models it lets you take those

00:17:57,059 --> 00:18:01,240
train models when you freeze their

00:17:59,260 --> 00:18:04,510
weights and you kind of put them into

00:18:01,240 --> 00:18:07,270
production you can use tensorflow for

00:18:04,510 --> 00:18:10,419
that and you can even visualize your

00:18:07,270 --> 00:18:12,010
training process using some dashboard

00:18:10,419 --> 00:18:14,350
using a dashboard that tensorflow

00:18:12,010 --> 00:18:16,270
provides it's fully open source and you

00:18:14,350 --> 00:18:18,940
can do all of this without paying a

00:18:16,270 --> 00:18:20,830
single dollar and ten supply even allows

00:18:18,940 --> 00:18:25,030
you to train on specialized hardware

00:18:20,830 --> 00:18:27,460
that Google built themselves so the

00:18:25,030 --> 00:18:30,370
entire ecosystem and the infrastructure

00:18:27,460 --> 00:18:32,500
is provided by the Google tensorflow

00:18:30,370 --> 00:18:37,240
team so that you can do anything you

00:18:32,500 --> 00:18:38,650
want with it so if you're - if you're

00:18:37,240 --> 00:18:40,720
here for the talk are you tuning in

00:18:38,650 --> 00:18:42,820
online I'm guessing that you are here

00:18:40,720 --> 00:18:44,890
for one of these purposes you want to

00:18:42,820 --> 00:18:47,410
learn 10244 projects you want to apply

00:18:44,890 --> 00:18:49,840
it at work or you would you just want to

00:18:47,410 --> 00:18:53,290
learn something new and exciting that's

00:18:49,840 --> 00:18:55,900
highly in demand now it is any or you

00:18:53,290 --> 00:18:58,000
just you know you're a student educator

00:18:55,900 --> 00:19:03,040
wanting to pick up this for your school

00:18:58,000 --> 00:19:06,040
projects so by the end of this I want to

00:19:03,040 --> 00:19:07,780
show you that this workshop will be will

00:19:06,040 --> 00:19:10,110
be covering a bit of basic tenets of law

00:19:07,780 --> 00:19:14,070
and that should get you up and running

00:19:10,110 --> 00:19:17,140
for whatever you want to do beyond this

00:19:14,070 --> 00:19:20,140
all right so the tensorflow

00:19:17,140 --> 00:19:22,870
API it's really simple to learn for me

00:19:20,140 --> 00:19:25,690
it took a few weeks to get started with

00:19:22,870 --> 00:19:27,309
it there are some highly complicated

00:19:25,690 --> 00:19:29,770
machine learning frameworks out there

00:19:27,309 --> 00:19:32,590
that require even more time and

00:19:29,770 --> 00:19:34,750
expertise to get used to the syntax but

00:19:32,590 --> 00:19:36,700
tensorflow it's really pythonic so if

00:19:34,750 --> 00:19:39,250
you're familiar with Python you can

00:19:36,700 --> 00:19:40,720
easily get started with tensorflow and

00:19:39,250 --> 00:19:43,540
you know you can hit the ground running

00:19:40,720 --> 00:19:46,320
it's as simple as that and neural

00:19:43,540 --> 00:19:49,600
networks all that math and that

00:19:46,320 --> 00:19:52,390
complexity underlying complexity tends

00:19:49,600 --> 00:19:55,240
to flow abstracts all of that away such

00:19:52,390 --> 00:19:57,070
that you can build your entire data

00:19:55,240 --> 00:19:59,399
pipeline you know loading your data set

00:19:57,070 --> 00:20:01,710
training your model checking performance

00:19:59,399 --> 00:20:03,779
tricks and you know deploying that well

00:20:01,710 --> 00:20:05,519
within you know 20 to 30 lines of code

00:20:03,779 --> 00:20:07,049
and you know when you compare that

00:20:05,519 --> 00:20:09,210
they're like the number of lines of code

00:20:07,049 --> 00:20:12,570
that organizations or corporations write

00:20:09,210 --> 00:20:14,820
20 to 30 lines is nothing so tensorflow

00:20:12,570 --> 00:20:16,529
allows you to do all that really easily

00:20:14,820 --> 00:20:18,629
and tensorflow

00:20:16,529 --> 00:20:21,149
it's you know you can it's fully

00:20:18,629 --> 00:20:23,159
customizable while they also provide

00:20:21,149 --> 00:20:25,679
while they provide some basic functions

00:20:23,159 --> 00:20:28,259
for I guess beginners to use you can

00:20:25,679 --> 00:20:30,119
also customize it for your own personal

00:20:28,259 --> 00:20:33,539
use case you can create custom layers

00:20:30,119 --> 00:20:35,429
you can create custom input pipelines

00:20:33,539 --> 00:20:38,159
you can do all of that for both

00:20:35,429 --> 00:20:41,969
beginners and advanced users with

00:20:38,159 --> 00:20:44,669
tensorflow so what tensorflow does is

00:20:41,969 --> 00:20:46,200
when you're creating that when you're

00:20:44,669 --> 00:20:48,089
creating that neural network when you're

00:20:46,200 --> 00:20:51,629
you know specifying each layer one by

00:20:48,089 --> 00:20:54,179
one what it's doing in I guess the

00:20:51,629 --> 00:20:55,409
background at a low level is it's

00:20:54,179 --> 00:20:57,929
creating this thing called the

00:20:55,409 --> 00:21:02,519
computation graph the computation graph

00:20:57,929 --> 00:21:05,460
is simply an organized view of all the

00:21:02,519 --> 00:21:09,479
processes occurring in your entire

00:21:05,460 --> 00:21:13,429
pipeline so the computation graph it

00:21:09,479 --> 00:21:16,169
consists of nodes and edges each node

00:21:13,429 --> 00:21:19,529
well it pertains to a specific operation

00:21:16,169 --> 00:21:23,070
or data value that is being given to it

00:21:19,529 --> 00:21:25,499
thanks and an edge is basically that

00:21:23,070 --> 00:21:31,379
flow of data or that operation being

00:21:25,499 --> 00:21:33,229
performed on that variable yeah so for

00:21:31,379 --> 00:21:36,779
example you know say we have this

00:21:33,229 --> 00:21:39,809
multivariate function FX y equals y plus

00:21:36,779 --> 00:21:42,509
2 plus X square Y so if we were to break

00:21:39,809 --> 00:21:45,499
this down or we give this to something

00:21:42,509 --> 00:21:48,929
like tensorflow it construct a

00:21:45,499 --> 00:21:51,450
computation graph with this equation so

00:21:48,929 --> 00:21:54,779
to help you original eyes this let's

00:21:51,450 --> 00:21:56,279
imagine that seem FX y function and we

00:21:54,779 --> 00:22:00,529
want to build the computation graph for

00:21:56,279 --> 00:22:05,219
it so we take Y and the constant 2 and

00:22:00,529 --> 00:22:09,179
we take X and we get X square by

00:22:05,219 --> 00:22:11,609
multiplying X with itself the the yellow

00:22:09,179 --> 00:22:13,059
circle that you see there it represents

00:22:11,609 --> 00:22:14,799
the multiplication operation

00:22:13,059 --> 00:22:16,570
and the squares that you see it

00:22:14,799 --> 00:22:18,519
represents the data value that's going

00:22:16,570 --> 00:22:21,669
in either a variable or constant like

00:22:18,519 --> 00:22:25,090
like 2 so now that we have all the basic

00:22:21,669 --> 00:22:27,519
building blocks the variables X square Y

00:22:25,090 --> 00:22:33,129
and 2 we want to all bring them together

00:22:27,519 --> 00:22:37,029
to form that FX Y equation so now when

00:22:33,129 --> 00:22:41,049
you have X square Y and 2 you can

00:22:37,029 --> 00:22:43,120
multiply X Square + y to get X square Y

00:22:41,049 --> 00:22:46,929
at the same time you can take the same

00:22:43,120 --> 00:22:50,139
value of y and add it to 2 to get y plus

00:22:46,929 --> 00:22:53,499
2 and finally the last operation in this

00:22:50,139 --> 00:22:56,559
computation graph to top it all off it

00:22:53,499 --> 00:22:59,799
adds both these things together the y +

00:22:56,559 --> 00:23:01,929
2 the y + 2 part of it and the X X

00:22:59,799 --> 00:23:05,529
square Y part of it it adds both of them

00:23:01,929 --> 00:23:09,220
together to form that final equation F X

00:23:05,529 --> 00:23:11,289
Y or than expression so at a high level

00:23:09,220 --> 00:23:12,879
this is water computation graph is we

00:23:11,289 --> 00:23:15,610
have a really complicated I guess

00:23:12,879 --> 00:23:18,519
equation we break it up into into its

00:23:15,610 --> 00:23:22,210
individual steps to simplify the problem

00:23:18,519 --> 00:23:24,070
while also ensuring that we finally get

00:23:22,210 --> 00:23:27,249
to that equation at the end or that

00:23:24,070 --> 00:23:31,590
process we finish it in a very organized

00:23:27,249 --> 00:23:34,149
method so tensorflow

00:23:31,590 --> 00:23:40,049
back when it was in its boiler

00:23:34,149 --> 00:23:40,049
development as an engineer his name is

00:23:40,679 --> 00:23:47,169
from socially yeah my bad forgot yeah

00:23:44,529 --> 00:23:49,210
from socially fellowmen Twitter you

00:23:47,169 --> 00:23:50,860
should do he's really active and he

00:23:49,210 --> 00:23:53,619
gives lots of good advice when it comes

00:23:50,860 --> 00:23:57,429
to machine learning so he created this

00:23:53,619 --> 00:23:59,320
thing called chaos chaos is so if you

00:23:57,429 --> 00:24:01,779
think high-tensile floor is at this

00:23:59,320 --> 00:24:05,019
level of abstraction at you know taking

00:24:01,779 --> 00:24:08,110
away all the ugly math underneath chaos

00:24:05,019 --> 00:24:11,169
was in like a layer above that so it was

00:24:08,110 --> 00:24:13,360
even more it was even more I guess

00:24:11,169 --> 00:24:17,429
easier to use chaos than it was to use

00:24:13,360 --> 00:24:21,100
tensor flow back then so

00:24:17,429 --> 00:24:22,720
so intensive labayda when you work I

00:24:21,100 --> 00:24:23,290
guess initialize or create a neural

00:24:22,720 --> 00:24:25,540
network

00:24:23,290 --> 00:24:28,030
you had to also initialize the

00:24:25,540 --> 00:24:30,910
individual widths and biases or the

00:24:28,030 --> 00:24:34,809
parameters manually so you actually had

00:24:30,910 --> 00:24:37,960
to write W equals B equals for all the

00:24:34,809 --> 00:24:40,450
layers that you had in your network but

00:24:37,960 --> 00:24:43,210
Kara and naturally this took you know

00:24:40,450 --> 00:24:44,500
50-plus lines of code and for anyone who

00:24:43,210 --> 00:24:46,420
didn't had that kind of time or for

00:24:44,500 --> 00:24:49,330
beginners trying to I guess quickly

00:24:46,420 --> 00:24:52,330
prototype this was I guess it's still

00:24:49,330 --> 00:24:55,150
too complicated for them so to remove

00:24:52,330 --> 00:24:58,780
all that to push all that aside first of

00:24:55,150 --> 00:25:01,059
all he created chaos so you could do

00:24:58,780 --> 00:25:04,000
this same thing you no longer had to

00:25:01,059 --> 00:25:05,950
initialize your width biases parameters

00:25:04,000 --> 00:25:08,260
hyper parameters you no longer had to

00:25:05,950 --> 00:25:10,450
initialize any of that all you had to do

00:25:08,260 --> 00:25:12,460
was call a specific function and we

00:25:10,450 --> 00:25:14,559
would do all of that and your entire

00:25:12,460 --> 00:25:17,050
pipeline whatever took you that 50 lines

00:25:14,559 --> 00:25:19,360
of code intensive low you could do the

00:25:17,050 --> 00:25:23,350
same thing into less than 20 lines of

00:25:19,360 --> 00:25:26,200
code using chaos so since then you know

00:25:23,350 --> 00:25:28,420
as as the caresse community as the chaos

00:25:26,200 --> 00:25:30,700
project and the tensorflow project as

00:25:28,420 --> 00:25:34,390
they've grown you know the couple of

00:25:30,700 --> 00:25:37,990
years chaos is now part of tensive law

00:25:34,390 --> 00:25:42,270
so using the TF or tensorflow dot chaos

00:25:37,990 --> 00:25:44,350
package you can get access to those

00:25:42,270 --> 00:25:47,890
functions available in chaos that

00:25:44,350 --> 00:25:51,040
abstract away everything so now almost

00:25:47,890 --> 00:25:53,230
all the models that you see on github

00:25:51,040 --> 00:25:56,920
open source all the tutorials you use

00:25:53,230 --> 00:26:01,450
even tensorflow version 2.0 it's fully

00:25:56,920 --> 00:26:03,040
based on the chaos spec meaning you no

00:26:01,450 --> 00:26:06,309
longer have to do these low-level

00:26:03,040 --> 00:26:09,429
operations to get your job done you can

00:26:06,309 --> 00:26:15,940
use the high-level TF chaos library and

00:26:09,429 --> 00:26:18,730
do the same thing so tensorflow chaos it

00:26:15,940 --> 00:26:22,470
has this thing called the LEAs api so

00:26:18,730 --> 00:26:24,820
what the LEAs api does is it provides a

00:26:22,470 --> 00:26:27,040
bunch of different functions or

00:26:24,820 --> 00:26:29,290
different classes that represent all

00:26:27,040 --> 00:26:30,520
these different layers so earlier in the

00:26:29,290 --> 00:26:32,920
slides we mentioned

00:26:30,520 --> 00:26:35,590
you know the fully connected layer the

00:26:32,920 --> 00:26:38,350
convolutional max pooling layer so to

00:26:35,590 --> 00:26:39,820
actually implement that in Ron numpy if

00:26:38,350 --> 00:26:41,770
you heard of that the numerical

00:26:39,820 --> 00:26:44,350
computing library in Python if we were

00:26:41,770 --> 00:26:48,220
to use P la numpy to write all of that

00:26:44,350 --> 00:26:50,740
it would be a mess but in TF chaos dot

00:26:48,220 --> 00:26:52,809
layers all you have to call a fully

00:26:50,740 --> 00:26:55,030
connected layer you just need to write

00:26:52,809 --> 00:26:57,610
the word dense so dense is just another

00:26:55,030 --> 00:27:00,550
name for fully connected so you all you

00:26:57,610 --> 00:27:02,650
need to do is type in dense and pass in

00:27:00,550 --> 00:27:02,830
a bit of parameters and you're good to

00:27:02,650 --> 00:27:04,360
go

00:27:02,830 --> 00:27:06,280
you no longer have to initialize the

00:27:04,360 --> 00:27:08,530
weights the parameters or the hyper

00:27:06,280 --> 00:27:10,600
parameters or if you wanted to

00:27:08,530 --> 00:27:13,360
initialize a convolutional here all you

00:27:10,600 --> 00:27:17,170
have to do is type in conf 2d and it

00:27:13,360 --> 00:27:19,150
gets the job done so the LEAs api so all

00:27:17,170 --> 00:27:20,679
these there are different layers if you

00:27:19,150 --> 00:27:22,870
were to bring them together to create

00:27:20,679 --> 00:27:24,610
the neural network architecture you have

00:27:22,870 --> 00:27:28,540
to use this thing called a sequential

00:27:24,610 --> 00:27:30,880
API so all of this is part of TF chaos

00:27:28,540 --> 00:27:33,610
so sequential you can think of it as

00:27:30,880 --> 00:27:36,340
like a bucket and all your layers or I

00:27:33,610 --> 00:27:38,230
guess the individual objects that you

00:27:36,340 --> 00:27:41,260
put in your bucket in an organized way

00:27:38,230 --> 00:27:43,809
so now that I have my container I put in

00:27:41,260 --> 00:27:46,420
you know all my layer objects one on top

00:27:43,809 --> 00:27:48,490
of each other I stack them up to create

00:27:46,420 --> 00:27:50,020
my neural network and within you know

00:27:48,490 --> 00:27:52,000
less than seven lines of code if you are

00:27:50,020 --> 00:27:53,950
creating a simple neural network you

00:27:52,000 --> 00:27:55,360
have an architecture that you can train

00:27:53,950 --> 00:27:58,059
or do whatever you want with it

00:27:55,360 --> 00:28:00,340
and each of these layers data

00:27:58,059 --> 00:28:02,920
automatically flows between them so you

00:28:00,340 --> 00:28:05,260
no longer have to specify connections

00:28:02,920 --> 00:28:07,240
between the layers like you had to do in

00:28:05,260 --> 00:28:11,470
you know the original version of

00:28:07,240 --> 00:28:15,309
tensorflow so tensorflow

00:28:11,470 --> 00:28:18,070
ever since its debut in early 2015 all

00:28:15,309 --> 00:28:22,050
the way till now the ecosystem has grown

00:28:18,070 --> 00:28:25,270
so much it has a lot of I guess

00:28:22,050 --> 00:28:27,910
subsystems or sub modules that the the

00:28:25,270 --> 00:28:31,360
ecosystem is provided to make all your

00:28:27,910 --> 00:28:35,250
machine learning needs much easier to

00:28:31,360 --> 00:28:35,250
acquire or achieve

00:28:35,770 --> 00:28:40,270
so now now that we've done like a quick

00:28:38,230 --> 00:28:41,710
recap of the past I guess you have four

00:28:40,270 --> 00:28:43,750
to five years of tensorflow

00:28:41,710 --> 00:28:46,620
and what and the contribution it has

00:28:43,750 --> 00:28:49,000
made to the machine learning community I

00:28:46,620 --> 00:28:52,540
guess it's time to finally do a bit of

00:28:49,000 --> 00:28:54,700
live coding so if you have your laptop's

00:28:52,540 --> 00:28:58,900
please ensure that it's connected to the

00:28:54,700 --> 00:29:04,180
Wi-Fi if not like are you connected to

00:28:58,900 --> 00:29:07,000
the Wi-Fi Wi-Fi okay yeah so yeah we'll

00:29:04,180 --> 00:29:10,630
be doing a bit of live coding so who's

00:29:07,000 --> 00:29:12,700
head of Google collab raise your hand if

00:29:10,630 --> 00:29:16,240
you've heard of collab okay yeah yeah

00:29:12,700 --> 00:29:18,490
so collab those of you who haven't heard

00:29:16,240 --> 00:29:20,830
of it it's an online notebook it's

00:29:18,490 --> 00:29:23,110
similar to Jupiter but it's hosted on

00:29:20,830 --> 00:29:25,060
the web so in the case that you want to

00:29:23,110 --> 00:29:27,790
quickly experiment with things you can

00:29:25,060 --> 00:29:29,920
quickly hop on to a Google collab

00:29:27,790 --> 00:29:34,240
notebook and immediately get started

00:29:29,920 --> 00:29:36,850
with with coding so the advantages of

00:29:34,240 --> 00:29:41,410
you know using collab compat you know

00:29:36,850 --> 00:29:45,220
having an IDE or having you know your

00:29:41,410 --> 00:29:49,270
specialized editor or set up environment

00:29:45,220 --> 00:29:51,460
is that collab it has it you know it it

00:29:49,270 --> 00:29:52,600
comes prepackaged with all the major

00:29:51,460 --> 00:29:54,460
data science and machine learning

00:29:52,600 --> 00:29:57,640
packages you know it comes with

00:29:54,460 --> 00:30:00,460
tensorflow tensor board numpy scipy

00:29:57,640 --> 00:30:02,080
matplotlib it has support for all of

00:30:00,460 --> 00:30:04,030
them so you no longer have to install

00:30:02,080 --> 00:30:06,400
all of them because there are some

00:30:04,030 --> 00:30:07,630
issues that people have faced that you

00:30:06,400 --> 00:30:09,640
know they're trying to install a package

00:30:07,630 --> 00:30:12,160
for doing machine learning projects and

00:30:09,640 --> 00:30:14,110
it just fails the installation fails so

00:30:12,160 --> 00:30:18,580
I guess so to do away with all of that

00:30:14,110 --> 00:30:19,330
you get this containerized notebook in

00:30:18,580 --> 00:30:21,820
your hand

00:30:19,330 --> 00:30:25,060
that you can do the same thing with so

00:30:21,820 --> 00:30:27,130
collab and additional benefit is if you

00:30:25,060 --> 00:30:28,930
do not want to spend you know thousands

00:30:27,130 --> 00:30:33,490
of dollars on a custom machine learning

00:30:28,930 --> 00:30:36,610
Rick you can use the hosted virtual

00:30:33,490 --> 00:30:38,980
machines given by collab you have you

00:30:36,610 --> 00:30:43,990
know you have your basic CPUs you have

00:30:38,980 --> 00:30:47,680
the the you have the Tesla GPUs and you

00:30:43,990 --> 00:30:50,260
even have access to the

00:30:47,680 --> 00:30:52,240
the Google brain teams a custom hardware

00:30:50,260 --> 00:30:54,730
called the tensor processing unit or the

00:30:52,240 --> 00:30:59,400
TPU you can get or access to all of that

00:30:54,730 --> 00:31:02,020
free of cost for 12 hours using collab

00:30:59,400 --> 00:31:04,660
but if you want I guess

00:31:02,020 --> 00:31:06,970
opt in for better virtual machines or

00:31:04,660 --> 00:31:09,760
you want more training time you know up

00:31:06,970 --> 00:31:13,240
to like a day 24 hours you can sign up

00:31:09,760 --> 00:31:17,260
for their pro version it's $10 per month

00:31:13,240 --> 00:31:20,560
and you have even more support for your

00:31:17,260 --> 00:31:22,960
machine learning projects and creating a

00:31:20,560 --> 00:31:27,010
google collab notebook is as simple as

00:31:22,960 --> 00:31:30,550
creating a Google Doc so for that we

00:31:27,010 --> 00:31:33,460
will be visiting Google Drive so if all

00:31:30,550 --> 00:31:37,450
of you can visit drive.google.com on

00:31:33,460 --> 00:31:42,970
your browsers right now hold on let me

00:31:37,450 --> 00:31:45,670
zoom in yeah and yeah so once you're in

00:31:42,970 --> 00:31:47,530
Google Drive you can log in with your

00:31:45,670 --> 00:31:51,550
Gmail account or your Google account and

00:31:47,530 --> 00:31:54,610
you can click the new button and when

00:31:51,550 --> 00:31:58,360
you see more you can in the drop down

00:31:54,610 --> 00:32:01,450
menu you can select Google collab and

00:31:58,360 --> 00:32:04,300
when you click it it redirects you to

00:32:01,450 --> 00:32:06,730
another page which is a fresh notebook

00:32:04,300 --> 00:32:09,250
for you so yeah it's as simple as

00:32:06,730 --> 00:32:14,770
creating a Google Doc ah is there does

00:32:09,250 --> 00:32:16,480
anyone not see Google collab in there in

00:32:14,770 --> 00:32:22,870
there in that drop-down menu everyone's

00:32:16,480 --> 00:32:28,420
on collab okay okay yeah okay yeah so

00:32:22,870 --> 00:32:37,900
yeah ignore the yeah oh uh so can you

00:32:28,420 --> 00:32:40,150
access collab have you access : okay sir

00:32:37,900 --> 00:32:42,430
have you accessed okay good so you see

00:32:40,150 --> 00:32:45,010
something like this right okay yeah so

00:32:42,430 --> 00:32:48,790
I'll give you like a quick run through

00:32:45,010 --> 00:32:50,860
of collab if oh yeah so this is like a

00:32:48,790 --> 00:32:53,770
like a custom thing in a while while I'm

00:32:50,860 --> 00:32:56,200
at it you know I can I can show you you

00:32:53,770 --> 00:33:00,320
know just for I guess the banter you can

00:32:56,200 --> 00:33:02,610
go to you can go to tools settings

00:33:00,320 --> 00:33:04,830
miscellaneous and you can select one of

00:33:02,610 --> 00:33:08,550
these funky little animations that's

00:33:04,830 --> 00:33:09,900
gonna start appearing I will disable

00:33:08,550 --> 00:33:13,640
this option because it's pretty

00:33:09,900 --> 00:33:15,990
distracting but you know feel free to

00:33:13,640 --> 00:33:20,250
enable that alright so this is like a

00:33:15,990 --> 00:33:23,160
quick run-through for collab so this I

00:33:20,250 --> 00:33:26,100
guess black rectangle you see it's

00:33:23,160 --> 00:33:27,810
called a cell but before we even start

00:33:26,100 --> 00:33:30,240
coding there's this button called

00:33:27,810 --> 00:33:34,110
connect yeah that's this button called

00:33:30,240 --> 00:33:35,580
connect can you try connecting yeah so

00:33:34,110 --> 00:33:39,330
when you click connect it's going to say

00:33:35,580 --> 00:33:42,770
it's you know allocating the VM and

00:33:39,330 --> 00:33:46,260
after that it's going to show some basic

00:33:42,770 --> 00:33:48,060
basic metrics like RAM usage yeah it

00:33:46,260 --> 00:33:51,350
should say connected and it should show

00:33:48,060 --> 00:33:57,900
you yeah something like that

00:33:51,350 --> 00:34:00,240
is everyone connected I mean good yeah

00:33:57,900 --> 00:34:04,230
so that means that our notebook right

00:34:00,240 --> 00:34:08,820
now it's now it's now sitting on top of

00:34:04,230 --> 00:34:11,160
that VM we can finally start coding so

00:34:08,820 --> 00:34:13,650
yeah so this black rectangle that you

00:34:11,160 --> 00:34:16,530
see here with display button it's called

00:34:13,650 --> 00:34:19,110
a cell and whatever you type in this

00:34:16,530 --> 00:34:20,910
cell you know you can type to come up

00:34:19,110 --> 00:34:24,900
here it's it's a Python notebook so you

00:34:20,910 --> 00:34:27,450
can only code in Python here so if I can

00:34:24,900 --> 00:34:30,300
type in something like you know x equals

00:34:27,450 --> 00:34:34,110
five simple Python and if I do y equals

00:34:30,300 --> 00:34:36,330
five and I do X plus y and if I were to

00:34:34,110 --> 00:34:39,900
click this play button it's going to run

00:34:36,330 --> 00:34:42,210
the Python specifically in that cell and

00:34:39,900 --> 00:34:45,929
if you want you can create more cells

00:34:42,210 --> 00:34:50,970
and you can you can do even more with

00:34:45,929 --> 00:34:54,270
that okay so notice how in my second

00:34:50,970 --> 00:34:56,850
cell I do not have or I haven't

00:34:54,270 --> 00:34:59,340
initialized the variable X I only did it

00:34:56,850 --> 00:35:01,260
in the top cell though I guess the best

00:34:59,340 --> 00:35:04,200
thing about notebooks is that for every

00:35:01,260 --> 00:35:06,180
cell you create it has a memory log of

00:35:04,200 --> 00:35:08,430
all the past variables you declared or

00:35:06,180 --> 00:35:11,850
all the past actions that you've done in

00:35:08,430 --> 00:35:14,100
the in the cells above so you don't need

00:35:11,850 --> 00:35:16,920
to keep reinitializing variable

00:35:14,100 --> 00:35:21,180
so over here I only initialize the

00:35:16,920 --> 00:35:23,550
variable Z and when I add X plus Z it

00:35:21,180 --> 00:35:26,640
takes the value of x from the previous

00:35:23,550 --> 00:35:31,710
cell and adds it to the value of Z that

00:35:26,640 --> 00:35:33,870
I initialized here so there's this

00:35:31,710 --> 00:35:37,400
button called code so when you click

00:35:33,870 --> 00:35:40,800
code oh yeah but also if you hover over

00:35:37,400 --> 00:35:42,930
the bottom of like a pre-existing cell

00:35:40,800 --> 00:35:45,840
you get this thing called code in text

00:35:42,930 --> 00:35:47,940
you can click code and you know if you

00:35:45,840 --> 00:35:50,190
want to create notebooks that you want

00:35:47,940 --> 00:35:52,440
to publicly release for I guess your own

00:35:50,190 --> 00:35:54,420
notes or you know you want to create

00:35:52,440 --> 00:35:57,360
like a tutorial using a colab notebook

00:35:54,420 --> 00:35:59,130
you can click the text button and this

00:35:57,360 --> 00:36:01,980
it's you know it's just it's just a

00:35:59,130 --> 00:36:04,170
bunch of you know basic text and you

00:36:01,980 --> 00:36:06,480
know it appears over here and for

00:36:04,170 --> 00:36:07,800
example this text I could say this is a

00:36:06,480 --> 00:36:10,350
good description of what I'll be doing

00:36:07,800 --> 00:36:12,530
in the in the cell below and yeah

00:36:10,350 --> 00:36:14,880
comments but at the same time you can

00:36:12,530 --> 00:36:16,290
you can write some good descriptions of

00:36:14,880 --> 00:36:18,420
what you're gonna do in the cell or like

00:36:16,290 --> 00:36:22,050
the entire project so yeah this is

00:36:18,420 --> 00:36:24,990
collab in a nutshell if you are using

00:36:22,050 --> 00:36:26,730
Mac OS and you want a shortcut you don't

00:36:24,990 --> 00:36:28,860
want to keep pressing this Run button or

00:36:26,730 --> 00:36:31,260
the split button you can do command

00:36:28,860 --> 00:36:33,510
enter and it'll run the cell if we're on

00:36:31,260 --> 00:36:35,340
Windows it's ctrl enter or shift enter

00:36:33,510 --> 00:36:42,570
you can try one of them it should run

00:36:35,340 --> 00:36:43,920
the specific cell yeah so it should also

00:36:42,570 --> 00:36:45,720
be this it depends on the keyboard you

00:36:43,920 --> 00:36:47,220
have if you have a command key then it's

00:36:45,720 --> 00:36:49,140
command enter if you have a control key

00:36:47,220 --> 00:36:54,680
its control Enter or shift enter you can

00:36:49,140 --> 00:36:54,680
try one of them is running

00:37:03,380 --> 00:37:10,260
all right so now that we have created so

00:37:08,579 --> 00:37:13,230
yeah to delete a cell you can hover over

00:37:10,260 --> 00:37:16,980
the cell there's this trash icon you can

00:37:13,230 --> 00:37:21,089
click it'll delete the cell all right

00:37:16,980 --> 00:37:25,950
shift enter shift enter okay alright so

00:37:21,089 --> 00:37:27,690
now that we are here oh yeah

00:37:25,950 --> 00:37:32,010
so we've done like a quick tour of

00:37:27,690 --> 00:37:34,760
collab so now now we're actually going

00:37:32,010 --> 00:37:37,559
to be using tensorflow from scratch so

00:37:34,760 --> 00:37:38,069
before we begin building models with

00:37:37,559 --> 00:37:41,010
tensorflow

00:37:38,069 --> 00:37:43,410
we first need a data set to work with so

00:37:41,010 --> 00:37:50,309
tens of load data sets or when you write

00:37:43,410 --> 00:37:52,349
it in code DFDS it it comes with a whole

00:37:50,309 --> 00:37:54,960
bunch of data set you know over 150 data

00:37:52,349 --> 00:37:59,809
sets that you know it's in different

00:37:54,960 --> 00:38:02,579
categories like text text audio video

00:37:59,809 --> 00:38:06,059
summarization tasks you can have access

00:38:02,579 --> 00:38:12,390
to all of that using PFDs or tens of

00:38:06,059 --> 00:38:17,339
load data sets so I guess when we go

00:38:12,390 --> 00:38:18,599
back to the to the notebook we can

00:38:17,339 --> 00:38:20,700
import tensorflow

00:38:18,599 --> 00:38:23,280
of course but before that we can import

00:38:20,700 --> 00:38:33,800
the numerical computing library numpy

00:38:23,280 --> 00:38:37,400
and then we can import tensorflow as TF

00:38:33,800 --> 00:38:40,340
so rather as TF or as NP it's just a

00:38:37,400 --> 00:38:42,650
shorthand notation in Python so you

00:38:40,340 --> 00:38:45,050
don't need to like type in tensorflow

00:38:42,650 --> 00:38:46,520
you know every time you type every time

00:38:45,050 --> 00:38:48,920
you want to use something from it you

00:38:46,520 --> 00:38:52,640
can say TF dot and it'll mean the same

00:38:48,920 --> 00:38:55,130
thing so yeah so we have import

00:38:52,640 --> 00:38:58,280
tensorflow STF and now finally let's

00:38:55,130 --> 00:39:05,350
import let me just do increase this okay

00:38:58,280 --> 00:39:09,500
we can import tensor flow data sets as

00:39:05,350 --> 00:39:11,750
TF D s so in Col Abbott how also has

00:39:09,500 --> 00:39:12,980
this autocomplete feature as you would

00:39:11,750 --> 00:39:14,660
have seen you know when I'm typing

00:39:12,980 --> 00:39:17,390
there's a drop-down menu of all the

00:39:14,660 --> 00:39:20,060
different options that the collab

00:39:17,390 --> 00:39:23,240
environment provides so when you're as

00:39:20,060 --> 00:39:30,260
you're typing you can tab complete that

00:39:23,240 --> 00:39:39,590
specific line okay so now we can run

00:39:30,260 --> 00:39:43,730
this cell actually before this are the

00:39:39,590 --> 00:39:51,350
current version of collab it's let me

00:39:43,730 --> 00:39:54,080
check the tensorflow version so yeah

00:39:51,350 --> 00:39:59,000
you type PF diversion and you run that

00:39:54,080 --> 00:40:01,940
specific cell if you see 1.15 that's not

00:39:59,000 --> 00:40:04,370
the version we're using can you can you

00:40:01,940 --> 00:40:07,250
try typing TF dot version C or version

00:40:04,370 --> 00:40:10,460
your notebook is on because if it's all

00:40:07,250 --> 00:40:13,250
on 1.15 we'll have to convert it to the

00:40:10,460 --> 00:40:15,380
2 dot X because that's the the most

00:40:13,250 --> 00:40:22,940
recent configuration or release of

00:40:15,380 --> 00:40:26,230
Python the stable release do you see

00:40:22,940 --> 00:40:26,230
something like 1.15

00:40:37,800 --> 00:40:45,060
so we're not going to be using 1.15 1.15

00:40:42,150 --> 00:40:50,390
is the I guess the older version the

00:40:45,060 --> 00:40:54,810
most recent version is 2.2 one yeah so

00:40:50,390 --> 00:41:00,860
to upgrade tensorflow in collab we'll

00:40:54,810 --> 00:41:04,890
have to you know re we have to rerun the

00:41:00,860 --> 00:41:08,250
we need to restart the VM by you know by

00:41:04,890 --> 00:41:10,710
invoking the the 2 dot X package so if

00:41:08,250 --> 00:41:14,580
you want to use tensorflow to point a to

00:41:10,710 --> 00:41:17,610
point X you need to type the percentage

00:41:14,580 --> 00:41:25,490
sign tensorflow underscore version space

00:41:17,610 --> 00:41:28,620
2 dot X oh yeah when you when you hover

00:41:25,490 --> 00:41:31,500
above so if I have a cell below and I

00:41:28,620 --> 00:41:32,970
hover on top there'll be a code button

00:41:31,500 --> 00:41:41,300
that comes up if you click it'll create

00:41:32,970 --> 00:41:44,310
a cell above or you can create or

00:41:41,300 --> 00:41:46,890
alternatively you can create another

00:41:44,310 --> 00:41:49,500
code cell here and you can see this

00:41:46,890 --> 00:41:52,710
arrow button on the top and it lets you

00:41:49,500 --> 00:41:55,350
move your cell up and down like the

00:41:52,710 --> 00:42:05,820
arrows here like I can move the cell up

00:41:55,350 --> 00:42:12,270
and down yeah any issue so far or the

00:42:05,820 --> 00:42:14,220
arrows don't work or the up one it's

00:42:12,270 --> 00:42:18,930
only greyed out if it's like the top of

00:42:14,220 --> 00:42:19,910
the notebook is everything okay on this

00:42:18,930 --> 00:42:23,790
side

00:42:19,910 --> 00:42:37,860
yeah percentage tensorflow underscore

00:42:23,790 --> 00:42:39,390
version space 2x yeah if you want to use

00:42:37,860 --> 00:42:41,880
the most recent version of tense flow

00:42:39,390 --> 00:42:44,370
because the collab notebooks up until

00:42:41,880 --> 00:42:46,830
now they haven't been updated to dot X

00:42:44,370 --> 00:42:49,830
all of them they're still running 1.15

00:42:46,830 --> 00:42:52,080
so if you really want to use like the

00:42:49,830 --> 00:42:57,900
latest version you need to specify it on

00:42:52,080 --> 00:42:59,250
your own at the top so now that we've

00:42:57,900 --> 00:43:00,840
kind of found out that we're using the

00:42:59,250 --> 00:43:02,790
wrong version of tense flow we need to

00:43:00,840 --> 00:43:04,980
restart this washing machine so if you

00:43:02,790 --> 00:43:12,630
go to is this runtime option at the top

00:43:04,980 --> 00:43:13,520
and you click restart and run all you

00:43:12,630 --> 00:43:16,670
know

00:43:13,520 --> 00:43:18,590
you this pop up when you click yes

00:43:16,670 --> 00:43:23,510
you know it's reconnecting reallocating

00:43:18,590 --> 00:43:26,870
space it's booting up that VM and when

00:43:23,510 --> 00:43:30,050
it starts running yeah finally you know

00:43:26,870 --> 00:43:36,440
it says that 2 dot X selected yeah

00:43:30,050 --> 00:43:38,960
everyone good ok on this side yeah all

00:43:36,440 --> 00:43:42,380
right so now that we are using 2 dot X

00:43:38,960 --> 00:43:47,540
we have imported all of this we can go

00:43:42,380 --> 00:43:50,090
back to the slides all right so now that

00:43:47,540 --> 00:43:51,650
we have imported numpy the numerical

00:43:50,090 --> 00:43:53,810
computing library tensorflow

00:43:51,650 --> 00:43:56,150
which is what we're gonna use for the

00:43:53,810 --> 00:43:58,640
majority of you know the neural network

00:43:56,150 --> 00:44:01,730
construction and the pipeline and you

00:43:58,640 --> 00:44:03,530
know T FD s tensor flow data sets the

00:44:01,730 --> 00:44:05,780
next thing we'd want to move on in terms

00:44:03,530 --> 00:44:08,780
of the the concepts is tensor flow

00:44:05,780 --> 00:44:12,230
models so to build a model or to build a

00:44:08,780 --> 00:44:14,720
neural network as I mentioned earlier

00:44:12,230 --> 00:44:16,340
you need to use the sequential API so

00:44:14,720 --> 00:44:19,100
sequential you can think of it as a

00:44:16,340 --> 00:44:21,260
container and one by one you put your

00:44:19,100 --> 00:44:23,240
layers one on top of each other to

00:44:21,260 --> 00:44:28,280
create the entire model or neural

00:44:23,240 --> 00:44:31,180
network so when you know when Kaos was

00:44:28,280 --> 00:44:35,960
you know was finally under tensor flow

00:44:31,180 --> 00:44:39,170
the models sub the models sub module

00:44:35,960 --> 00:44:42,200
work it came with you know automatic

00:44:39,170 --> 00:44:44,150
support for you know training testing it

00:44:42,200 --> 00:44:46,880
came with support for your loss

00:44:44,150 --> 00:44:48,700
functions optimizers so you no longer

00:44:46,880 --> 00:44:53,750
have to write all of that from scratch

00:44:48,700 --> 00:44:58,130
so using your tensor flow datasets DFDS

00:44:53,750 --> 00:45:00,260
datasets you can train them you can

00:44:58,130 --> 00:45:06,470
train the tensile model on these data

00:45:00,260 --> 00:45:10,070
sets so when you have a model you it's

00:45:06,470 --> 00:45:13,040
basically it consists of layers so TFK

00:45:10,070 --> 00:45:14,930
layers the layers API module it comes

00:45:13,040 --> 00:45:18,140
with all these you know some of these

00:45:14,930 --> 00:45:21,080
common layers you know input for you

00:45:18,140 --> 00:45:24,620
know taking in an input taking in that x

00:45:21,080 --> 00:45:27,170
value dense which is basically the

00:45:24,620 --> 00:45:30,470
synonym for a fully connected layer

00:45:27,170 --> 00:45:32,750
activation which you either have an

00:45:30,470 --> 00:45:35,510
option to type in the activation

00:45:32,750 --> 00:45:37,130
function in the tense block or you can

00:45:35,510 --> 00:45:39,890
specify a new layer called the

00:45:37,130 --> 00:45:42,470
activation layer and type in that

00:45:39,890 --> 00:45:43,250
specific function there we'll see more

00:45:42,470 --> 00:45:45,980
of that later

00:45:43,250 --> 00:45:50,270
so come to D it's your two deconvolution

00:45:45,980 --> 00:45:54,740
Leah max pool 2 D is your maximum

00:45:50,270 --> 00:45:56,900
pooling in two-dimensional space flatten

00:45:54,740 --> 00:45:59,810
is that thing that take takes in an

00:45:56,900 --> 00:46:03,800
n-dimensional matrix and squashes it

00:45:59,810 --> 00:46:07,130
into a vector and reshape is especially

00:46:03,800 --> 00:46:11,119
a so when you're when you're import

00:46:07,130 --> 00:46:13,850
aches in an N dimensional matrix your

00:46:11,119 --> 00:46:15,800
reshape Leah well it it's pretty

00:46:13,850 --> 00:46:18,800
self-explanatory it reshapes that

00:46:15,800 --> 00:46:23,330
incoming vector into a target shape so

00:46:18,800 --> 00:46:26,840
if I have 28 by 28 image going in but I

00:46:23,330 --> 00:46:29,960
only want us a seminary so 28 by 28 is

00:46:26,840 --> 00:46:32,869
784 and when you multiply them so I have

00:46:29,960 --> 00:46:35,900
a 28 by 28 pixel image going into my

00:46:32,869 --> 00:46:37,790
pipeline right using the input layer so

00:46:35,900 --> 00:46:41,570
immediately when you put a reshape layer

00:46:37,790 --> 00:46:44,270
under that and you type in 1784 it'll

00:46:41,570 --> 00:46:46,130
take that 28 by 28 and squash it so in a

00:46:44,270 --> 00:46:49,100
way you can think of your reshape layer

00:46:46,130 --> 00:46:51,320
as kind of like a sub as a subcategory

00:46:49,100 --> 00:46:54,310
of flatten because even though flatten

00:46:51,320 --> 00:46:57,109
takes in something and changes its shape

00:46:54,310 --> 00:46:58,670
so yeah when you're building some basic

00:46:57,109 --> 00:47:02,590
neural networks these are the common

00:46:58,670 --> 00:47:05,330
layers that are used there are much more

00:47:02,590 --> 00:47:07,760
so instead of Maxwell you can use your

00:47:05,330 --> 00:47:10,730
global average pooling layers

00:47:07,760 --> 00:47:14,119
you also have 1d convolutions 3d

00:47:10,730 --> 00:47:16,190
convolutions and you have a bunch of

00:47:14,119 --> 00:47:17,930
other layers like your attention layer

00:47:16,190 --> 00:47:23,720
but we're not going to be covering that

00:47:17,930 --> 00:47:26,119
keep this tutorial simple so as I guess

00:47:23,720 --> 00:47:28,580
to get our get ourselves comfortable

00:47:26,119 --> 00:47:32,060
with the tensor flow syntax the PFDs

00:47:28,580 --> 00:47:35,840
api will be training on this data set

00:47:32,060 --> 00:47:40,490
called amnesty who sort of amnesty so M

00:47:35,840 --> 00:47:40,940
NIST is this 28 by 28 image hand-drawn

00:47:40,490 --> 00:47:43,520
digits

00:47:40,940 --> 00:47:45,319
the data set so it has hand-drawn digits

00:47:43,520 --> 00:47:47,569
from you know from zero all the way to

00:47:45,319 --> 00:47:50,930
nine so you can see an example of both

00:47:47,569 --> 00:47:53,780
of them they're all 28 28 by 28 pixels

00:47:50,930 --> 00:47:56,780
that's you know in it and that's a zero

00:47:53,780 --> 00:47:59,089
so since they are in Europe they're

00:47:56,780 --> 00:48:01,730
grayscale images you can flip the color

00:47:59,089 --> 00:48:03,859
map so you can either present it as you

00:48:01,730 --> 00:48:05,660
know the digit is black and like the

00:48:03,859 --> 00:48:06,380
background is white or the other way

00:48:05,660 --> 00:48:09,170
around

00:48:06,380 --> 00:48:11,000
in the case of zero so this is what

00:48:09,170 --> 00:48:13,480
we're gonna do now we're gonna take in

00:48:11,000 --> 00:48:17,030
this entire Emnes data set we're gonna

00:48:13,480 --> 00:48:19,670
build a really simple neural network to

00:48:17,030 --> 00:48:26,930
train on this data set and classify and

00:48:19,670 --> 00:48:29,630
hand-drawn digits all right okay so now

00:48:26,930 --> 00:48:34,130
that we have that I'm just gonna take a

00:48:29,630 --> 00:48:35,990
seat here okay so now that we imported

00:48:34,130 --> 00:48:38,720
the numerical computing library

00:48:35,990 --> 00:48:41,030
tensorflow and tens of load data sets we

00:48:38,720 --> 00:48:44,750
finally want to go ahead and load our

00:48:41,030 --> 00:48:49,960
data set so I'm just going to call this

00:48:44,750 --> 00:48:54,109
data set train or die's train as tfd s

00:48:49,960 --> 00:48:57,740
dot load and I just need to type in

00:48:54,109 --> 00:48:59,960
eminence over here so earlier I

00:48:57,740 --> 00:49:01,940
mentioned that tensorflow data sets has

00:48:59,960 --> 00:49:04,130
a I guess a repository or a collection

00:49:01,940 --> 00:49:06,530
of like various data sets that are at

00:49:04,130 --> 00:49:08,569
your disposal all you need to do is type

00:49:06,530 --> 00:49:10,339
in the name the specific name of the

00:49:08,569 --> 00:49:14,300
data set and you have it in your hands

00:49:10,339 --> 00:49:17,359
so TF d s dot load it takes in the

00:49:14,300 --> 00:49:20,660
entire data set and it I guess dumps it

00:49:17,359 --> 00:49:22,730
into the DS train variable but here

00:49:20,660 --> 00:49:25,220
since we're only creating the training

00:49:22,730 --> 00:49:30,589
set we can specify the split that we

00:49:25,220 --> 00:49:32,810
want so here the split refers to which

00:49:30,589 --> 00:49:34,640
part of the data set we want do we want

00:49:32,810 --> 00:49:37,430
the training set or do we want the

00:49:34,640 --> 00:49:41,510
testing set so here we can specify the

00:49:37,430 --> 00:49:50,290
split to be trained so this just gives

00:49:41,510 --> 00:49:52,180
so amnesty the Emnes data set yeah

00:49:50,290 --> 00:49:55,000
so the eminence data set it consists of

00:49:52,180 --> 00:50:00,310
60,000 training images and 10,000

00:49:55,000 --> 00:50:04,180
testing images so we only want that

00:50:00,310 --> 00:50:07,990
first 60,000 images in the data set so

00:50:04,180 --> 00:50:10,390
that's why we specify the Train split so

00:50:07,990 --> 00:50:12,970
I also mentioned that supervised

00:50:10,390 --> 00:50:15,010
learning is a branch of machine learning

00:50:12,970 --> 00:50:17,440
or category of machine learning where

00:50:15,010 --> 00:50:20,710
you give the model both the features and

00:50:17,440 --> 00:50:22,750
the labels so as super lized is

00:50:20,710 --> 00:50:25,810
basically a boolean you just need to

00:50:22,750 --> 00:50:27,430
give you need to say true and it'll give

00:50:25,810 --> 00:50:30,250
you both the features which is basically

00:50:27,430 --> 00:50:32,050
the 28 by 28 pixels and it also give you

00:50:30,250 --> 00:50:34,300
the class label you know does it you

00:50:32,050 --> 00:50:37,810
know zero to nine which category it

00:50:34,300 --> 00:50:40,000
belongs to so now that we have this we

00:50:37,810 --> 00:50:42,640
can finally run this cell

00:50:40,000 --> 00:50:44,740
let me just minimize this okay yeah so

00:50:42,640 --> 00:50:47,860
what is going to do is is it's it'll

00:50:44,740 --> 00:50:49,900
download the entire data set from Google

00:50:47,860 --> 00:50:53,920
Cloud Storage so there's a publicly

00:50:49,900 --> 00:50:56,050
available or accessible URL so it pulls

00:50:53,920 --> 00:51:00,760
that they pulls the entire data set or

00:50:56,050 --> 00:51:02,620
the training data set from that URL so

00:51:00,760 --> 00:51:05,890
now now that we have the training data

00:51:02,620 --> 00:51:07,900
set in our hands let's visualize you

00:51:05,890 --> 00:51:12,430
know what the individual instances look

00:51:07,900 --> 00:51:15,370
like so we can say you know let's

00:51:12,430 --> 00:51:19,090
iterate through I guess four examples

00:51:15,370 --> 00:51:22,260
and let's visualize them but before we

00:51:19,090 --> 00:51:26,710
iterate we want to import matplotlib

00:51:22,260 --> 00:51:31,240
which is the visualization library so in

00:51:26,710 --> 00:51:35,140
my auto complete menu matplotlib pipe

00:51:31,240 --> 00:51:38,050
lot as PLT when you it'll auto complete

00:51:35,140 --> 00:51:41,220
this so PLT is the visualization package

00:51:38,050 --> 00:51:44,080
it lets you draw graphs histograms

00:51:41,220 --> 00:51:49,600
figures sketches anything images

00:51:44,080 --> 00:51:53,530
anything so now that we have PLT we can

00:51:49,600 --> 00:51:56,670
finally iterate through the training

00:51:53,530 --> 00:51:56,670
data set des train

00:52:00,160 --> 00:52:11,180
so DS train is so the class that belongs

00:52:06,410 --> 00:52:11,749
to is the data set builder class in the

00:52:11,180 --> 00:52:14,150
tensorflow

00:52:11,749 --> 00:52:15,650
ecosystem or the tensor flow library so

00:52:14,150 --> 00:52:19,369
it comes with this specific function

00:52:15,650 --> 00:52:22,489
called take so from my entire 60,000

00:52:19,369 --> 00:52:26,029
image data set I want to take a part of

00:52:22,489 --> 00:52:28,009
it and you need to specify n which is

00:52:26,029 --> 00:52:30,140
how many you want to take so C out of

00:52:28,009 --> 00:52:32,869
six 60,000 I want to take the first

00:52:30,140 --> 00:52:37,880
three so I type in DS

00:52:32,869 --> 00:52:39,739
train take three right so now we're

00:52:37,880 --> 00:52:46,749
going to iterate through the first three

00:52:39,739 --> 00:52:51,920
training instances so image dot label

00:52:46,749 --> 00:52:54,559
sorry image common label it's going to

00:52:51,920 --> 00:52:58,819
be stored in this example variable over

00:52:54,559 --> 00:53:02,719
here yeah in this example variable over

00:52:58,819 --> 00:53:06,529
here so example example is just an array

00:53:02,719 --> 00:53:10,930
with two elements example at the first

00:53:06,529 --> 00:53:14,390
at the zero at index is the image and

00:53:10,930 --> 00:53:16,430
example in the first index is your label

00:53:14,390 --> 00:53:19,999
you know as in like you know 0 to 9

00:53:16,430 --> 00:53:24,469
which one it belongs to so tensorflow it

00:53:19,999 --> 00:53:28,279
has its own custom data type called a

00:53:24,469 --> 00:53:31,660
tensor and you can't access tensors on

00:53:28,279 --> 00:53:35,690
its own you need to take its numpy

00:53:31,660 --> 00:53:38,509
representation for both of them so at

00:53:35,690 --> 00:53:41,660
the end of example at the zero index we

00:53:38,509 --> 00:53:44,539
can type in dot numpy for both of them

00:53:41,660 --> 00:53:47,029
and this gives us the numpy version or

00:53:44,539 --> 00:53:50,779
the number representation of the image

00:53:47,029 --> 00:53:54,170
and the label and matplotlib or PLT it's

00:53:50,779 --> 00:53:56,809
only able to visualize numpy array is

00:53:54,170 --> 00:54:00,309
not tensors so that's why that's another

00:53:56,809 --> 00:54:00,309
reason why we want to convert it

00:54:03,549 --> 00:54:08,809
so now that we have loaded a single

00:54:06,470 --> 00:54:14,980
image and label from that example

00:54:08,809 --> 00:54:14,980
instance we can finally visualize it

00:54:16,240 --> 00:54:27,349
yeah so we can say

00:54:19,130 --> 00:54:31,789
PLT dot m show image and we can even add

00:54:27,349 --> 00:54:35,210
a title to each visualization as the

00:54:31,789 --> 00:54:38,289
label so what you'll see here pretty

00:54:35,210 --> 00:54:40,760
quickly is that when you create a plot

00:54:38,289 --> 00:54:42,859
whatever it represents you can add a

00:54:40,760 --> 00:54:46,400
title to that you can even label the

00:54:42,859 --> 00:54:49,400
axes the x and y axis so now that we

00:54:46,400 --> 00:54:51,980
have this let me zoom out for a bit you

00:54:49,400 --> 00:54:57,260
when you run this cell it'll give you an

00:54:51,980 --> 00:55:00,789
error which is 28 by 28 comma 1 is an

00:54:57,260 --> 00:55:03,380
invalid shape so PLT it only takes in

00:55:00,789 --> 00:55:06,289
two-dimensional images and since M NIST

00:55:03,380 --> 00:55:09,859
is a grayscale image it should only have

00:55:06,289 --> 00:55:13,579
three channels so I'm sure all of you

00:55:09,859 --> 00:55:16,130
are familiar with the RGB concept so

00:55:13,579 --> 00:55:18,470
grayscale images they only have two

00:55:16,130 --> 00:55:21,109
dimensions while RGB images colored

00:55:18,470 --> 00:55:24,650
images they have three dimensions one

00:55:21,109 --> 00:55:27,859
for each RGB Channel which is why this

00:55:24,650 --> 00:55:32,059
image will have to use the numpy

00:55:27,859 --> 00:55:37,099
operator dot reshape so let me zoom back

00:55:32,059 --> 00:55:40,400
in so when we say plc dot M show image

00:55:37,099 --> 00:55:49,630
we want to add the dot reshape such that

00:55:40,400 --> 00:55:51,289
we want to reshape it 28 by 28 everyone

00:55:49,630 --> 00:55:54,250
up to speed

00:55:51,289 --> 00:55:54,250
okay

00:56:00,840 --> 00:56:07,330
or you just need to click the click the

00:56:03,310 --> 00:56:11,140
cell you are yeah so the when you're

00:56:07,330 --> 00:56:14,100
hosted on the the CPU runtime it uses

00:56:11,140 --> 00:56:16,930
your local CPU so if your device is slow

00:56:14,100 --> 00:56:22,140
naturally collab may also take a while

00:56:16,930 --> 00:56:22,140
to run each cell or perform operations

00:56:31,620 --> 00:56:35,680
we want

00:56:33,430 --> 00:56:37,930
okay so originally the the number

00:56:35,680 --> 00:56:43,930
representation from that tensor that we

00:56:37,930 --> 00:56:46,330
get it's 28 by 28 by one but PLT only

00:56:43,930 --> 00:56:49,090
takes in two-dimensional images this one

00:56:46,330 --> 00:56:50,950
is three dimensional it has one two and

00:56:49,090 --> 00:56:53,500
three dimensions but over here we just

00:56:50,950 --> 00:56:56,500
want the first two dimensions and since

00:56:53,500 --> 00:56:58,720
that last one is just one where you can

00:56:56,500 --> 00:57:08,620
completely cut it off without affecting

00:56:58,720 --> 00:57:09,329
anything 28 by 28 yeah so yeah when you

00:57:08,620 --> 00:57:12,359
hover

00:57:09,329 --> 00:57:15,029
the reshape the word reshape it says

00:57:12,359 --> 00:57:17,880
that it's a built-in method in from

00:57:15,029 --> 00:57:20,190
numpy so all these tensors when you get

00:57:17,880 --> 00:57:22,880
there numpy representation anything you

00:57:20,190 --> 00:57:26,819
can do with numpy you can do it on them

00:57:22,880 --> 00:57:29,209
so yeah when you run this cell yeah it

00:57:26,819 --> 00:57:33,599
finally gives you this

00:57:29,209 --> 00:57:37,829
does everyone see digits yeah

00:57:33,599 --> 00:57:42,150
so four is the first training instance

00:57:37,829 --> 00:57:45,479
in in the amnesty data set so when you

00:57:42,150 --> 00:57:49,380
zoom into this we can see you know the

00:57:45,479 --> 00:57:52,410
title over here we gave the we wanted to

00:57:49,380 --> 00:57:55,440
also add the title for each plot over

00:57:52,410 --> 00:57:58,079
here you can see that for each image it

00:57:55,440 --> 00:58:05,249
has its corresponding label which is the

00:57:58,079 --> 00:58:07,739
title yeah okay so now that we have our

00:58:05,249 --> 00:58:11,039
training data set we want to we want to

00:58:07,739 --> 00:58:13,229
pre-process it such that we can finally

00:58:11,039 --> 00:58:18,509
feed it into the network that will be

00:58:13,229 --> 00:58:20,180
building soon so for that DFDS tens of

00:58:18,509 --> 00:58:22,890
load data sets comes with many

00:58:20,180 --> 00:58:26,039
pre-processing built-in pre-processing

00:58:22,890 --> 00:58:29,249
methods and functions that all you need

00:58:26,039 --> 00:58:31,769
to do is call the function and i'll call

00:58:29,249 --> 00:58:34,680
that method and it'll perform that step

00:58:31,769 --> 00:58:38,069
or that operation on your data set so

00:58:34,680 --> 00:58:42,180
now what we're going to do is or this

00:58:38,069 --> 00:58:44,819
raw des train data set that we have we

00:58:42,180 --> 00:58:48,569
want to batch it so that we can train

00:58:44,819 --> 00:58:51,119
the model in batches and we also want to

00:58:48,569 --> 00:58:53,579
fit the fit the end we want we want to

00:58:51,119 --> 00:58:55,789
shuffle the data set so that you know

00:58:53,579 --> 00:58:58,829
the data set isn't really predictable

00:58:55,789 --> 00:59:01,380
and also we want to fit the entire thing

00:58:58,829 --> 00:59:03,989
into the memory of our virtual machine

00:59:01,380 --> 00:59:08,789
our hosted virtual machine so we're

00:59:03,989 --> 00:59:11,690
gonna do all that in this cell now so

00:59:08,789 --> 00:59:11,690
des train

00:59:13,950 --> 00:59:18,570
for that we first need to normalize the

00:59:16,230 --> 00:59:21,300
data set so normalization it basically

00:59:18,570 --> 00:59:23,339
means when your data set all the

00:59:21,300 --> 00:59:25,349
individual features are in this case all

00:59:23,339 --> 00:59:29,150
the different pixels they're going to be

00:59:25,349 --> 00:59:33,690
in the 0 to 255 range because in the RGB

00:59:29,150 --> 00:59:35,579
channels it ranges from 0 to 255 to 15

00:59:33,690 --> 00:59:38,760
the highest in intensity for that

00:59:35,579 --> 00:59:40,619
channel 0 being the minimum so here we

00:59:38,760 --> 00:59:45,359
want to normalize it into a range

00:59:40,619 --> 00:59:47,190
between 0 and 1 because if you have in

00:59:45,359 --> 00:59:49,290
machine learning or when you're building

00:59:47,190 --> 00:59:53,010
or training models when you have too

00:59:49,290 --> 00:59:54,750
large a number there's a chance that the

00:59:53,010 --> 00:59:57,270
data set will overflow because the

00:59:54,750 --> 00:59:59,040
numbers are becoming too huge so the

00:59:57,270 --> 01:00:03,480
more that you work with decimal numbers

00:59:59,040 --> 01:00:05,970
really small numbers the better so

01:00:03,480 --> 01:00:09,420
normalize it takes in you know the

01:00:05,970 --> 01:00:12,839
images or you know the the pixel the

01:00:09,420 --> 01:00:18,000
pixel values and it also takes in the

01:00:12,839 --> 01:00:23,790
labels so what we want to do is first we

01:00:18,000 --> 01:00:27,480
want to return back the images but we

01:00:23,790 --> 01:00:30,720
want to divide each pixel by 255 but 255

01:00:27,480 --> 01:00:33,450
it's an integer value but the images

01:00:30,720 --> 01:00:36,960
they are in an other data type

01:00:33,450 --> 01:00:39,839
representation so they are in the you

01:00:36,960 --> 01:00:42,960
int 8 data type representation we want

01:00:39,839 --> 01:00:47,790
to convert them into float 32 so to do

01:00:42,960 --> 01:00:49,770
that we can say t f dot cast so when you

01:00:47,790 --> 01:00:51,599
have an integer and you want its string

01:00:49,770 --> 01:00:54,780
representation you kind of cast it

01:00:51,599 --> 01:00:57,060
writing STR brackets and that integer

01:00:54,780 --> 01:00:59,670
inside it'll give you the same but in

01:00:57,060 --> 01:01:03,569
string format similarly we want to cast

01:00:59,670 --> 01:01:06,780
the you int 8 representation and we want

01:01:03,569 --> 01:01:10,349
to change all of that into float 32 so

01:01:06,780 --> 01:01:13,470
we can say we want to cast the all the

01:01:10,349 --> 01:01:17,970
images that given to this function into

01:01:13,470 --> 01:01:21,589
the TF dot float 32 data type

01:01:17,970 --> 01:01:24,540
representation and now that we have

01:01:21,589 --> 01:01:25,380
converted it into a format that we can

01:01:24,540 --> 01:01:27,700
work with

01:01:25,380 --> 01:01:31,630
we can finally divide it by

01:01:27,700 --> 01:01:35,320
five five so when you take a large

01:01:31,630 --> 01:01:38,560
number or a number between 0 and 255 and

01:01:35,320 --> 01:01:44,980
you divide it by 255 you squash it into

01:01:38,560 --> 01:01:48,700
the range 0 to 1 so here we have casted

01:01:44,980 --> 01:01:53,230
the images to float 32 and we can now

01:01:48,700 --> 01:01:57,339
divide divided by 255 and we can also

01:01:53,230 --> 01:01:59,890
return the the labels and just run the

01:01:57,339 --> 01:02:01,990
run the cell so notice here that when

01:01:59,890 --> 01:02:04,720
you type when you write in a function in

01:02:01,990 --> 01:02:06,369
a colab cell it doesn't run the function

01:02:04,720 --> 01:02:08,940
because you never called it it just

01:02:06,369 --> 01:02:08,940
exists

01:02:09,119 --> 01:02:20,770
so now let's pre-process the ds3 and

01:02:12,430 --> 01:02:24,460
data set so D s train equals the

01:02:20,770 --> 01:02:27,010
Asturian dot map so what dot map does is

01:02:24,460 --> 01:02:29,609
it takes in a mapping function and it

01:02:27,010 --> 01:02:32,410
applies that function to every single

01:02:29,609 --> 01:02:35,500
instance that exists within the data set

01:02:32,410 --> 01:02:37,450
so for all the images we want to apply

01:02:35,500 --> 01:02:40,960
this normalize function that we wrote

01:02:37,450 --> 01:02:44,589
above and we want to apply it to all the

01:02:40,960 --> 01:02:47,260
images and at the same time we want to

01:02:44,589 --> 01:02:50,950
do it in such a way that it's all inside

01:02:47,260 --> 01:02:52,599
the memory of your of your machine so we

01:02:50,950 --> 01:02:54,790
want to type in this thing called the

01:02:52,599 --> 01:02:58,089
this parameter called the num parallel

01:02:54,790 --> 01:03:01,750
calls as you're typing it should it

01:02:58,089 --> 01:03:04,390
should autocomplete yeah and you should

01:03:01,750 --> 01:03:08,140
get this num underscore parallel

01:03:04,390 --> 01:03:11,849
underscore calls so we have to type in

01:03:08,140 --> 01:03:19,810
this thing called TF dot data dot

01:03:11,849 --> 01:03:21,520
experimental dot Auto Tune so this

01:03:19,810 --> 01:03:23,710
entire thing it seems like a mouthful

01:03:21,520 --> 01:03:26,950
but all it's doing is it's taking that

01:03:23,710 --> 01:03:28,510
entire huge data set and it's fitting it

01:03:26,950 --> 01:03:31,060
into your machine so that you don't get

01:03:28,510 --> 01:03:33,160
any out of memory errors so all it's

01:03:31,060 --> 01:03:35,680
doing is its cashing it in your machine

01:03:33,160 --> 01:03:38,579
so we don't run into any errors

01:03:35,680 --> 01:03:38,579
downstream

01:03:39,670 --> 01:03:48,180
so now that we have normalized the data

01:03:42,310 --> 01:03:48,180
set we now want to batch up the data set

01:03:48,600 --> 01:03:57,820
so we can say da strain equals D a

01:03:52,540 --> 01:04:00,160
strain dot batch so all the all the

01:03:57,820 --> 01:04:03,150
60,000 images that are in the data set

01:04:00,160 --> 01:04:09,370
now it's going to break it's gonna

01:04:03,150 --> 01:04:12,160
convert it into batches of 32 so it

01:04:09,370 --> 01:04:15,160
converts it into a blocks of 32 images

01:04:12,160 --> 01:04:17,110
so there have been many machine learning

01:04:15,160 --> 01:04:20,020
papers that have come out a while back

01:04:17,110 --> 01:04:22,450
on why batching your data set helps the

01:04:20,020 --> 01:04:25,540
neural network trained better it's I

01:04:22,450 --> 01:04:28,060
guess you could call it like a good

01:04:25,540 --> 01:04:30,370
practice to do so whenever you are

01:04:28,060 --> 01:04:32,800
instead of training on the entire data

01:04:30,370 --> 01:04:36,130
set you train on batches of it so that

01:04:32,800 --> 01:04:38,110
the model can generalize well so now

01:04:36,130 --> 01:04:40,570
that we have batched up the data set we

01:04:38,110 --> 01:04:43,540
now want to shuffle it so as to make it

01:04:40,570 --> 01:04:46,810
you know a bit more random something

01:04:43,540 --> 01:04:48,820
that the network can't really it's not

01:04:46,810 --> 01:04:51,550
predictable predictable basically so you

01:04:48,820 --> 01:04:55,600
keep giving it random instances to train

01:04:51,550 --> 01:04:58,600
on so for that we want to type in des

01:04:55,600 --> 01:05:02,380
train dot shuffle and inside this

01:04:58,600 --> 01:05:05,620
shuffle function inside this shuffle

01:05:02,380 --> 01:05:09,010
function we want to type in the size of

01:05:05,620 --> 01:05:11,470
the of the data set so we type in 60,000

01:05:09,010 --> 01:05:13,450
here because we have 60,000 images in

01:05:11,470 --> 01:05:16,330
the data set but if you're using this on

01:05:13,450 --> 01:05:21,030
a data set with ten 10,000 images you

01:05:16,330 --> 01:05:24,190
want to type in 10,000 instead of 60,000

01:05:21,030 --> 01:05:27,010
so now what this shuffle function does

01:05:24,190 --> 01:05:31,120
is a randomly samples from your data set

01:05:27,010 --> 01:05:35,590
and keeps giving random training

01:05:31,120 --> 01:05:37,660
instances for the model so now again we

01:05:35,590 --> 01:05:41,200
want to do this in a very memory

01:05:37,660 --> 01:05:45,960
efficient manner so we type in da store

01:05:41,200 --> 01:05:49,120
train dot prefetch so prefetch is

01:05:45,960 --> 01:05:52,849
another tensorflow data set method that

01:05:49,120 --> 01:05:55,970
when you apply it to your data set it

01:05:52,849 --> 01:05:57,979
into memory really efficiently and we

01:05:55,970 --> 01:06:00,499
want to use the same strategy that we

01:05:57,979 --> 01:06:02,180
used up here so we here we use the

01:06:00,499 --> 01:06:04,249
auto-tune so it does everything

01:06:02,180 --> 01:06:05,359
automatically we don't need to do we

01:06:04,249 --> 01:06:08,989
don't need to put in any more effort

01:06:05,359 --> 01:06:13,519
into loading and processing the data set

01:06:08,989 --> 01:06:17,680
in memory so in the prefetch method we

01:06:13,519 --> 01:06:23,539
again we can type in TF data dot

01:06:17,680 --> 01:06:25,549
experimental dot auto-tune again your

01:06:23,539 --> 01:06:27,799
autocomplete if it's enabled on your

01:06:25,549 --> 01:06:30,039
collab notebook it should autocomplete

01:06:27,799 --> 01:06:30,039
this

01:06:34,120 --> 01:06:39,700
so has everyone reached until this

01:06:36,940 --> 01:06:45,280
prefetch point there's give me a thumbs

01:06:39,700 --> 01:06:48,940
up when you're finished typing yeah we

01:06:45,280 --> 01:06:51,430
can run this code and we have you know

01:06:48,940 --> 01:06:54,340
we've completed processing or training

01:06:51,430 --> 01:06:57,100
data set and now we have to do the same

01:06:54,340 --> 01:07:00,970
thing for our testing data set and to do

01:06:57,100 --> 01:07:06,310
that we can create a new cell we can say

01:07:00,970 --> 01:07:09,130
add es test by the way if you ever run

01:07:06,310 --> 01:07:11,520
into any errors just just you know raise

01:07:09,130 --> 01:07:14,260
your hand or you just call out because

01:07:11,520 --> 01:07:17,740
intensive flow many errors are usually

01:07:14,260 --> 01:07:19,930
caused by internal virtual machine

01:07:17,740 --> 01:07:20,620
errors like it's not your fault it just

01:07:19,930 --> 01:07:22,780
happens

01:07:20,620 --> 01:07:24,940
so yeah if you run the cell again it

01:07:22,780 --> 01:07:26,650
should clear up unless it's like a typo

01:07:24,940 --> 01:07:29,200
or something which means yeah you need

01:07:26,650 --> 01:07:32,110
to like cross where verify with the code

01:07:29,200 --> 01:07:34,450
on the screen so yeah now that we have

01:07:32,110 --> 01:07:36,010
finished pre processing our training

01:07:34,450 --> 01:07:37,600
data set we have to do the same thing

01:07:36,010 --> 01:07:42,660
for our testing data set and it's a

01:07:37,600 --> 01:07:46,270
similar process PFDs the FDS dot load

01:07:42,660 --> 01:07:49,480
again we want the amnesty data set but

01:07:46,270 --> 01:07:53,560
this time for the split we want the

01:07:49,480 --> 01:07:55,840
testing data set we want the testing

01:07:53,560 --> 01:08:01,200
data set we know we need the test data

01:07:55,840 --> 01:08:03,790
set again we need it as supervised as

01:08:01,200 --> 01:08:05,410
true so it's gonna give us everything

01:08:03,790 --> 01:08:12,040
it's gonna give us both the features and

01:08:05,410 --> 01:08:14,140
the labels so now that we have loaded in

01:08:12,040 --> 01:08:16,630
our testing data set you know we can

01:08:14,140 --> 01:08:19,270
just go ahead and run this cell it

01:08:16,630 --> 01:08:21,160
shouldn't give any errors and now we can

01:08:19,270 --> 01:08:25,270
finally start pre processing our testing

01:08:21,160 --> 01:08:28,960
set so it's a similar process actually

01:08:25,270 --> 01:08:31,990
we can take this in this cell copy

01:08:28,960 --> 01:08:36,820
everything and paste it down here and

01:08:31,990 --> 01:08:40,440
instead of DS train we had we had to

01:08:36,820 --> 01:08:40,440
change it to DS test everywhere

01:08:45,960 --> 01:08:52,359
and we can

01:08:48,909 --> 01:08:55,210
so this DS test dot shuffle we can

01:08:52,359 --> 01:08:56,980
completely remove that line because we

01:08:55,210 --> 01:08:59,289
don't really need to shuffle our testing

01:08:56,980 --> 01:09:01,119
data set because it's already it already

01:08:59,289 --> 01:09:03,969
contains instances the model has not

01:09:01,119 --> 01:09:07,389
seen before but yes we do want to batch

01:09:03,969 --> 01:09:09,339
it because it's working in batches it's

01:09:07,389 --> 01:09:18,039
much faster than processing the entire

01:09:09,339 --> 01:09:19,659
data set at a shot again you see over

01:09:18,039 --> 01:09:22,210
here that we're using the map function

01:09:19,659 --> 01:09:25,179
or the map method provided by tensorflow

01:09:22,210 --> 01:09:30,039
data sets and we applied the normalized

01:09:25,179 --> 01:09:31,659
function on all the instances when you

01:09:30,039 --> 01:09:40,269
run this cell I shouldn't give you any

01:09:31,659 --> 01:09:44,579
errors just give me a thumbs up when

01:09:40,269 --> 01:09:44,579
you've finished the prefetch function

01:09:46,799 --> 01:09:52,539
okay so for like the online viewers the

01:09:50,710 --> 01:09:55,360
question is why aren't we shuffling the

01:09:52,539 --> 01:09:57,760
testing data set we're not shuffling it

01:09:55,360 --> 01:09:59,500
we but you first let me explain why we

01:09:57,760 --> 01:10:01,809
want to shuffle our training data set we

01:09:59,500 --> 01:10:03,519
shuffle it because we want to be fully

01:10:01,809 --> 01:10:06,369
random and we want to keep giving it

01:10:03,519 --> 01:10:09,190
random instances when training so that

01:10:06,369 --> 01:10:11,590
it doesn't I guess find patterns in the

01:10:09,190 --> 01:10:13,179
data set itself saying that you know I'm

01:10:11,590 --> 01:10:16,300
gonna get this instance next for

01:10:13,179 --> 01:10:18,909
training so which is why we shuffle it

01:10:16,300 --> 01:10:22,659
so similar way so if I have a deck of

01:10:18,909 --> 01:10:24,940
cards and I want to or when I'm playing

01:10:22,659 --> 01:10:27,070
a game of cards I shuffle the deck first

01:10:24,940 --> 01:10:30,159
before you know giving it out each of

01:10:27,070 --> 01:10:32,199
the players so that that way there's a

01:10:30,159 --> 01:10:34,360
fully random chance of getting that card

01:10:32,199 --> 01:10:36,340
instead you know there's no chance of I

01:10:34,360 --> 01:10:39,550
guess cheating or in this case the model

01:10:36,340 --> 01:10:42,789
cheating or learning something that it's

01:10:39,550 --> 01:10:44,139
not supposed to learn so yeah but for

01:10:42,789 --> 01:10:46,659
testing we don't really need to shuffle

01:10:44,139 --> 01:10:49,239
it because the testing data set it

01:10:46,659 --> 01:10:50,920
contains the training instances the

01:10:49,239 --> 01:10:52,960
images that the model has never seen

01:10:50,920 --> 01:10:56,460
before so yeah that's why we don't need

01:10:52,960 --> 01:10:58,940
to shuffle it it's already random enough

01:10:56,460 --> 01:11:01,219
yeah remove you can

01:10:58,940 --> 01:11:03,170
move shuffle from the testing data set

01:11:01,219 --> 01:11:08,540
but you want to keep it for the training

01:11:03,170 --> 01:11:09,860
data set keep the training fair yeah

01:11:08,540 --> 01:11:16,400
just run the code it shouldn't give you

01:11:09,860 --> 01:11:20,239
any errors okay good okay so now that we

01:11:16,400 --> 01:11:22,610
have loaded in and pre-processed our

01:11:20,239 --> 01:11:25,190
training and testing data sets we can

01:11:22,610 --> 01:11:31,040
finally start building the model right

01:11:25,190 --> 01:11:33,949
so for this the TF Kaos dot leas library

01:11:31,040 --> 01:11:36,410
it abstracts away all the math and the

01:11:33,949 --> 01:11:37,900
complete on the complicated theory that

01:11:36,410 --> 01:11:40,370
you need to know when you're creating

01:11:37,900 --> 01:11:43,400
neural networks you no longer have to

01:11:40,370 --> 01:11:45,730
initialize your individual neurons you

01:11:43,400 --> 01:11:48,560
don't need to instantiate your

01:11:45,730 --> 01:11:50,870
parameters or hyper parameters all of it

01:11:48,560 --> 01:11:54,140
is done in the background so now let's

01:11:50,870 --> 01:11:58,010
go ahead and import some more import our

01:11:54,140 --> 01:12:01,489
model or the sequential API and let's go

01:11:58,010 --> 01:12:04,010
ahead and download so I'm sorry call

01:12:01,489 --> 01:12:06,080
some of the layers that I mentioned so

01:12:04,010 --> 01:12:08,960
in this specific amnesty example we'll

01:12:06,080 --> 01:12:12,560
be using the input layer we'll be using

01:12:08,960 --> 01:12:15,140
the reshape layer and the dense layer

01:12:12,560 --> 01:12:18,469
and the activation layer so these four

01:12:15,140 --> 01:12:20,560
we can quickly build a neural network in

01:12:18,469 --> 01:12:26,800
you know less than ten lines of code so

01:12:20,560 --> 01:12:36,320
to use it we have to say from tensorflow

01:12:26,800 --> 01:12:39,590
dot care us dot models import sequential

01:12:36,320 --> 01:12:41,960
yeah it should order complete yeah so

01:12:39,590 --> 01:12:43,760
sequential again just to recap

01:12:41,960 --> 01:12:46,130
sequential you can think of it as a

01:12:43,760 --> 01:12:47,870
cardboard box and everything that we're

01:12:46,130 --> 01:12:50,060
putting inside is basically I guess a

01:12:47,870 --> 01:12:53,360
food packet so yeah it's a container

01:12:50,060 --> 01:12:55,580
that contains the like the individual

01:12:53,360 --> 01:12:57,140
layers connected together in the

01:12:55,580 --> 01:12:59,180
background so now that we have

01:12:57,140 --> 01:13:02,900
sequential we want to go ahead and

01:12:59,180 --> 01:13:07,610
import our Lea's so for that from tencel

01:13:02,900 --> 01:13:10,400
flow don't care us dot Lea's so this is

01:13:07,610 --> 01:13:12,360
the Lea's API that was you know that was

01:13:10,400 --> 01:13:16,600
shown in the slides earlier

01:13:12,360 --> 01:13:21,730
so from here we want to import the input

01:13:16,600 --> 01:13:23,950
layer we want to import reshape we want

01:13:21,730 --> 01:13:26,200
to import dense which is basically

01:13:23,950 --> 01:13:32,590
another name for a fully connected layer

01:13:26,200 --> 01:13:35,260
and we want to import the activation leo

01:13:32,590 --> 01:13:36,940
so here earlier I mentioned that you

01:13:35,260 --> 01:13:39,490
don't really need the activation layer

01:13:36,940 --> 01:13:41,620
you there's a parameter in your dense

01:13:39,490 --> 01:13:43,680
layer called activation that you can

01:13:41,620 --> 01:13:46,210
just type in but for the sake of

01:13:43,680 --> 01:13:49,060
visualizing the computation graph that

01:13:46,210 --> 01:13:51,400
we're building I wanted to add on the

01:13:49,060 --> 01:13:53,350
activation function for I guess ease of

01:13:51,400 --> 01:13:56,860
understanding as to how tensorflow

01:13:53,350 --> 01:13:58,990
goes about building that graph so let's

01:13:56,860 --> 01:14:05,370
run this session again shouldn't give

01:13:58,990 --> 01:14:10,780
any errors so we can say a model equals

01:14:05,370 --> 01:14:13,780
sequential so here we're initializing

01:14:10,780 --> 01:14:17,410
our container our box that we're going

01:14:13,780 --> 01:14:20,800
to dump all the layers into so yeah

01:14:17,410 --> 01:14:24,280
model equals sequential add an

01:14:20,800 --> 01:14:27,070
individual layer into this box or

01:14:24,280 --> 01:14:30,280
sequential container all we have to do

01:14:27,070 --> 01:14:33,310
is type model dot add it's as simple as

01:14:30,280 --> 01:14:35,350
that you can just need to go on you know

01:14:33,310 --> 01:14:39,130
adding different layers one after the

01:14:35,350 --> 01:14:42,700
other so first we're going to add in my

01:14:39,130 --> 01:14:45,400
input layer to take in well the

01:14:42,700 --> 01:14:47,910
different the X values or the pixel

01:14:45,400 --> 01:14:53,380
values that I'm going to be feeding it

01:14:47,910 --> 01:14:57,100
so of course input it needs your target

01:14:53,380 --> 01:15:01,420
shape and over here we noticed like we

01:14:57,100 --> 01:15:03,790
got an error up above here saying that

01:15:01,420 --> 01:15:07,540
the image that we received it's 28 by 28

01:15:03,790 --> 01:15:11,860
by 1 so similarly here we want to type

01:15:07,540 --> 01:15:15,520
in the dimensions or the dimensions or

01:15:11,860 --> 01:15:19,450
size of one single image so M missed

01:15:15,520 --> 01:15:21,820
images it consists of 28 by 28 images so

01:15:19,450 --> 01:15:26,100
yeah it's going to take in that matrix

01:15:21,820 --> 01:15:31,350
over here next we'll be using

01:15:26,100 --> 01:15:34,560
the reshape Lia to take in this 28 by 28

01:15:31,350 --> 01:15:44,370
by one matrix and we're gonna squash

01:15:34,560 --> 01:15:47,070
everything down into 784 yeah so all it

01:15:44,370 --> 01:15:49,650
does is it takes that square and it just

01:15:47,070 --> 01:15:52,200
gives you a single row when you reshape

01:15:49,650 --> 01:15:56,430
it into seventy eighty four so this coma

01:15:52,200 --> 01:15:59,540
I added because so if you can see here I

01:15:56,430 --> 01:16:03,780
had this extra comma with nothing here

01:15:59,540 --> 01:16:08,160
yeah because when we work with this

01:16:03,780 --> 01:16:11,220
model it's kind of a way that we write

01:16:08,160 --> 01:16:13,230
it in numpy to create vectors otherwise

01:16:11,220 --> 01:16:15,180
it's just gonna create another list we

01:16:13,230 --> 01:16:18,570
don't want a list we want an umpire

01:16:15,180 --> 01:16:20,670
array so if you add this comma it's so

01:16:18,570 --> 01:16:23,060
much easier to work with numpy arrays

01:16:20,670 --> 01:16:25,890
without having to work with you know

01:16:23,060 --> 01:16:29,520
really complicated dimensionalities and

01:16:25,890 --> 01:16:32,030
whatnot so next after we do that we can

01:16:29,520 --> 01:16:35,910
finally go ahead and start building our

01:16:32,030 --> 01:16:39,150
neural network or our feed-forward

01:16:35,910 --> 01:16:40,950
neural network so we type in dents which

01:16:39,150 --> 01:16:43,860
is just another name for fully connected

01:16:40,950 --> 01:16:46,890
and as I mentioned the fully connected

01:16:43,860 --> 01:16:50,070
layer is just a stack of neurons so over

01:16:46,890 --> 01:16:52,020
here as the first variable we want to

01:16:50,070 --> 01:16:55,020
type in the number of neurons in this

01:16:52,020 --> 01:17:00,450
layer so yeah we can go ahead and I can

01:16:55,020 --> 01:17:03,240
type something like 256 so in usually

01:17:00,450 --> 01:17:09,560
the best practice is working with the

01:17:03,240 --> 01:17:22,730
like neurons is initializing it based on

01:17:09,560 --> 01:17:22,730
multiples of 16 so 16 32 64 128 256 512

01:17:33,290 --> 01:17:36,350
[Music]

01:17:39,240 --> 01:17:45,590
of neurons in alia it usually gives the

01:17:41,940 --> 01:17:50,520
the best generalization over data set

01:17:45,590 --> 01:17:52,620
all right so next after we add our dense

01:17:50,520 --> 01:17:58,020
layer of fully connected layer you want

01:17:52,620 --> 01:18:00,750
to add our activation Lea and activation

01:17:58,020 --> 01:18:04,700
over here we can use something like the

01:18:00,750 --> 01:18:07,800
value or rectified linear unit

01:18:04,700 --> 01:18:09,090
activation function as I mentioned

01:18:07,800 --> 01:18:12,810
earlier I'm not really getting into the

01:18:09,090 --> 01:18:15,690
math or like the individual concepts you

01:18:12,810 --> 01:18:17,490
can change this activation function to

01:18:15,690 --> 01:18:19,860
be anything it could be the sigmoid

01:18:17,490 --> 01:18:22,410
activation function it could be soft max

01:18:19,860 --> 01:18:24,510
but usually soft max is reserved for the

01:18:22,410 --> 01:18:25,800
final output layer because it gives a

01:18:24,510 --> 01:18:28,350
probability distribution

01:18:25,800 --> 01:18:34,260
pertaining to which class something

01:18:28,350 --> 01:18:37,980
would most likely belong to so again we

01:18:34,260 --> 01:18:42,240
want to add and other we want to add

01:18:37,980 --> 01:18:47,340
another fully connected layer so we type

01:18:42,240 --> 01:18:49,520
in dense again so over here when you're

01:18:47,340 --> 01:18:52,230
going for new networks you want to have

01:18:49,520 --> 01:18:55,560
kind of like a decreasing or tapering

01:18:52,230 --> 01:18:57,600
number of units all converging to like

01:18:55,560 --> 01:19:00,510
the number of classes that you have so

01:18:57,600 --> 01:19:02,880
over here in this specific M list case

01:19:00,510 --> 01:19:06,240
we have ten output classes corresponding

01:19:02,880 --> 01:19:08,790
to zero till nine so because of that we

01:19:06,240 --> 01:19:11,070
want our final layer to have ten neurons

01:19:08,790 --> 01:19:14,880
so you know each one corresponding to

01:19:11,070 --> 01:19:18,200
zero to nine so 256 you know next up we

01:19:14,880 --> 01:19:22,650
have 128 we want to taper it down and

01:19:18,200 --> 01:19:26,640
similarly we want to have a rail u

01:19:22,650 --> 01:19:35,730
activation function on it and actually

01:19:26,640 --> 01:19:36,139
we can you can just copy we can just

01:19:35,730 --> 01:19:39,079
copy

01:19:36,139 --> 01:19:41,719
this this block and just paste it down

01:19:39,079 --> 01:19:45,979
so let me just add a bit of spacers so

01:19:41,719 --> 01:19:47,719
you can see yeah so we have this is a

01:19:45,979 --> 01:19:49,760
three layer neural network

01:19:47,719 --> 01:19:52,749
it consists of three fully connected

01:19:49,760 --> 01:19:55,729
Lea's all of them having their own

01:19:52,749 --> 01:19:58,309
activation function so here we can

01:19:55,729 --> 01:20:06,110
finally put something like ten and yes

01:19:58,309 --> 01:20:09,050
sorry oh yeah it's ten now so depending

01:20:06,110 --> 01:20:10,999
on what depth you want your network to

01:20:09,050 --> 01:20:12,739
have you know we can so in this case

01:20:10,999 --> 01:20:15,919
this is a three layer neural network

01:20:12,739 --> 01:20:17,929
suppose I want a fine layer neural

01:20:15,919 --> 01:20:19,519
network all I need to do is add in two

01:20:17,929 --> 01:20:21,229
more dense layers or two more fully

01:20:19,519 --> 01:20:24,349
connected layers each with their

01:20:21,229 --> 01:20:27,439
activation functions but when it comes

01:20:24,349 --> 01:20:31,340
to the output layer we want to have the

01:20:27,439 --> 01:20:33,679
softmax activation function because it

01:20:31,340 --> 01:20:36,760
gives us a final probability

01:20:33,679 --> 01:20:42,199
distribution pertaining to which class

01:20:36,760 --> 01:20:47,179
an instance belongs to so in are all out

01:20:42,199 --> 01:20:50,209
of all these ten neurons it's gonna so

01:20:47,179 --> 01:20:53,959
the index that it fires the most at or

01:20:50,209 --> 01:20:56,360
it activates at it corresponds to the

01:20:53,959 --> 01:21:00,469
class it belongs to so for example if

01:20:56,360 --> 01:21:04,099
the third neuron fires then that means

01:21:00,469 --> 01:21:06,590
that it belongs to you know the number

01:21:04,099 --> 01:21:08,719
three or that that digit that

01:21:06,590 --> 01:21:10,820
handwritten digit it corresponds to

01:21:08,719 --> 01:21:13,130
number three but for example if the

01:21:10,820 --> 01:21:15,380
eighth neuron has the highest

01:21:13,130 --> 01:21:17,329
probability in this distribution it

01:21:15,380 --> 01:21:21,639
means that the number in the image is

01:21:17,329 --> 01:21:26,649
probably an eight so yeah we have this

01:21:21,639 --> 01:21:30,110
so now we want to do this thing called

01:21:26,649 --> 01:21:33,800
compiling a model so compiling a model

01:21:30,110 --> 01:21:36,530
is you know is this chaos term that you

01:21:33,800 --> 01:21:39,229
use where you provide the loss function

01:21:36,530 --> 01:21:41,749
and the optimizer that you're gonna use

01:21:39,229 --> 01:21:44,079
to train the network so over here for

01:21:41,749 --> 01:21:47,659
the loss we're gonna be using a sparse

01:21:44,079 --> 01:21:49,130
categorical cross entropy so all it does

01:21:47,659 --> 01:21:51,890
is it takes all

01:21:49,130 --> 01:21:56,450
my predictions it compares it to all the

01:21:51,890 --> 01:21:58,790
actual labels and it computes a loss

01:21:56,450 --> 01:22:01,040
based on that but additionally you can

01:21:58,790 --> 01:22:04,700
also use the mean squared error loss or

01:22:01,040 --> 01:22:06,740
you know add the mean absolute error or

01:22:04,700 --> 01:22:09,320
any activation function you can think of

01:22:06,740 --> 01:22:11,570
so in the is the official tensorflow

01:22:09,320 --> 01:22:13,330
documentation there's an entire list of

01:22:11,570 --> 01:22:16,070
all the loss functions that you can use

01:22:13,330 --> 01:22:19,850
so yeah you can search that up based on

01:22:16,070 --> 01:22:23,840
a few if you want to explore so over

01:22:19,850 --> 01:22:27,920
here we want the spa's categorical cross

01:22:23,840 --> 01:22:30,920
entropy so this could be anything I'm

01:22:27,920 --> 01:22:35,360
just choosing a sparse categorical cross

01:22:30,920 --> 01:22:37,580
entropy as my loss function but feel

01:22:35,360 --> 01:22:39,350
free to choose anything but based on

01:22:37,580 --> 01:22:41,330
your loss function it also affects the

01:22:39,350 --> 01:22:43,430
training different loss functions are

01:22:41,330 --> 01:22:48,320
known to have different I guess training

01:22:43,430 --> 01:22:52,400
results so for optimizer we want to

01:22:48,320 --> 01:22:55,850
choose the atom optimizer so atom stands

01:22:52,400 --> 01:22:58,580
for adaptive momentum I don't want to

01:22:55,850 --> 01:23:01,870
get down to the math but it's one of the

01:22:58,580 --> 01:23:06,170
best optimizers out there today

01:23:01,870 --> 01:23:09,290
so metrics is this thing that all neural

01:23:06,170 --> 01:23:10,910
networks kind of have so when you have

01:23:09,290 --> 01:23:13,490
so these metrics are basically

01:23:10,910 --> 01:23:16,670
performance metrics so over here we want

01:23:13,490 --> 01:23:18,680
to maintain the accuracy or you want to

01:23:16,670 --> 01:23:21,080
check the accuracy of the network as

01:23:18,680 --> 01:23:24,460
it's training so later you'll see when

01:23:21,080 --> 01:23:26,480
we are actually training on the data set

01:23:24,460 --> 01:23:29,170
yeah when we're actually training on the

01:23:26,480 --> 01:23:32,120
data set the accuracy is also printed

01:23:29,170 --> 01:23:34,550
side-by-side so you you know if all goes

01:23:32,120 --> 01:23:38,630
well you should see an increasing

01:23:34,550 --> 01:23:40,610
accuracy so just give me a thumbs up

01:23:38,630 --> 01:23:42,880
after you finish this compiled step on

01:23:40,610 --> 01:23:46,190
this side sorry

01:23:42,880 --> 01:23:49,640
yeah just compile so model dot compiled

01:23:46,190 --> 01:23:52,310
and you provide the loss optimizer and

01:23:49,640 --> 01:23:56,060
metrics so all of these are you need to

01:23:52,310 --> 01:23:58,550
specify them because they're all the

01:23:56,060 --> 01:24:00,260
hyper parameters like even the choice of

01:23:58,550 --> 01:24:02,570
loss function optimizer they're hyper

01:24:00,260 --> 01:24:06,170
parameters that you choose beforehand

01:24:02,570 --> 01:24:09,020
so yeah and the metrics yeah you need to

01:24:06,170 --> 01:24:14,120
have at least one metric other than the

01:24:09,020 --> 01:24:16,280
loss to track during training so I just

01:24:14,120 --> 01:24:19,150
click this cell shouldn't give you any

01:24:16,280 --> 01:24:21,830
errors yeah

01:24:19,150 --> 01:24:24,800
so now what I want to introduce you know

01:24:21,830 --> 01:24:29,330
before we start training is this thing

01:24:24,800 --> 01:24:31,940
called a callback so a callback is

01:24:29,330 --> 01:24:34,790
basically this thing that logs my

01:24:31,940 --> 01:24:37,370
training my training routine

01:24:34,790 --> 01:24:40,340
so all the data associated with training

01:24:37,370 --> 01:24:43,100
like the loss values my accuracy values

01:24:40,340 --> 01:24:45,770
over my you know my training sequence is

01:24:43,100 --> 01:24:49,100
gonna log all that and kind of store it

01:24:45,770 --> 01:24:53,690
in like a form of history and a callback

01:24:49,100 --> 01:24:56,090
what it does is you can visualize the

01:24:53,690 --> 01:24:59,290
callbacks in this thing called tensor

01:24:56,090 --> 01:25:02,120
board so tensorflow it comes with this

01:24:59,290 --> 01:25:04,460
this visualization called dashboard as

01:25:02,120 --> 01:25:07,820
usual ization dashboard dashboard called

01:25:04,460 --> 01:25:11,210
tensile blood as you can see you know on

01:25:07,820 --> 01:25:12,860
the right side so all these graphs that

01:25:11,210 --> 01:25:16,310
you see they're all the different

01:25:12,860 --> 01:25:19,790
metrics that I'm trying to track so by

01:25:16,310 --> 01:25:21,920
default you track the accuracy and the

01:25:19,790 --> 01:25:25,370
loss but you can add in your own custom

01:25:21,920 --> 01:25:28,160
stuff so yeah tensile board it allows

01:25:25,370 --> 01:25:30,350
you to visualize your performance

01:25:28,160 --> 01:25:33,950
metrics during training during testing

01:25:30,350 --> 01:25:37,130
and it gives you the graphs to kind of

01:25:33,950 --> 01:25:39,320
see you know where I'm going wrong what

01:25:37,130 --> 01:25:41,920
else I need to do to I guess

01:25:39,320 --> 01:25:45,050
bring my model back on the right track

01:25:41,920 --> 01:25:48,530
so to use a callback or to use the

01:25:45,050 --> 01:25:52,480
tensor board called axe specifically we

01:25:48,530 --> 01:25:59,140
can import the tensile board call back

01:25:52,480 --> 01:26:02,440
like so tensile load Karis dot callbacks

01:25:59,140 --> 01:26:02,440
import Oh

01:26:06,379 --> 01:26:14,539
yeah it's tensorflow yeah yeah yeah

01:26:11,030 --> 01:26:25,039
so yeah tensorflow dot Karis callbacks

01:26:14,539 --> 01:26:26,899
import the tensor board callback so

01:26:25,039 --> 01:26:32,659
tensile boy you can think of it as

01:26:26,899 --> 01:26:34,729
suppose I'm running my 2.4 2.4 km and my

01:26:32,659 --> 01:26:37,519
friend is you know the guy timing me at

01:26:34,729 --> 01:26:40,849
the end of every lap tensile board is

01:26:37,519 --> 01:26:43,609
that guy so it basically logs everything

01:26:40,849 --> 01:26:45,439
so when I'm running around the law when

01:26:43,609 --> 01:26:47,599
I'm running around the track whenever I

01:26:45,439 --> 01:26:49,729
finish one lap he's gonna scream out the

01:26:47,599 --> 01:26:51,979
timing like okay you took in a two

01:26:49,729 --> 01:26:54,379
minutes to complete one lap and he keeps

01:26:51,979 --> 01:26:56,149
you know screaming out the lap timings

01:26:54,379 --> 01:26:57,949
the individual lap timings whenever I

01:26:56,149 --> 01:27:00,499
pass like that finish line again so

01:26:57,949 --> 01:27:02,359
that's what tensile board does as my

01:27:00,499 --> 01:27:05,300
model is training you know every epoch

01:27:02,359 --> 01:27:07,219
when it's training it logs that I guess

01:27:05,300 --> 01:27:09,979
timing but in this case the loss in

01:27:07,219 --> 01:27:13,879
accuracy it logs all of that information

01:27:09,979 --> 01:27:17,209
and it displays that so it's kind of I

01:27:13,879 --> 01:27:21,499
guess it's synonymous to the the timer

01:27:17,209 --> 01:27:22,459
person when you're running so yeah we

01:27:21,499 --> 01:27:24,979
can

01:27:22,459 --> 01:27:26,989
yeah we can run this cell shouldn't give

01:27:24,979 --> 01:27:30,519
you any errors

01:27:26,989 --> 01:27:33,649
so yeah let's create our first callback

01:27:30,519 --> 01:27:35,199
so TB or you know the tensor board

01:27:33,649 --> 01:27:41,899
called back you can call this anything

01:27:35,199 --> 01:27:44,809
TB equals 10 sir board so it's asking

01:27:41,899 --> 01:27:46,459
for this thing called the log gear which

01:27:44,809 --> 01:27:50,419
basically stands for my logging

01:27:46,459 --> 01:27:52,189
directory so this is basically it's a

01:27:50,419 --> 01:27:55,219
directory where I want to store all my

01:27:52,189 --> 01:27:59,889
log files so let's just say I want to

01:27:55,219 --> 01:27:59,889
store it in this folder called my logs

01:28:00,550 --> 01:28:09,289
yeah let you store it in my logs and

01:28:04,359 --> 01:28:11,659
when we run this again no errors so now

01:28:09,289 --> 01:28:14,539
that we have a callback we can finally

01:28:11,659 --> 01:28:15,559
fit the model on the data set or in

01:28:14,539 --> 01:28:18,379
other terms we can finally start

01:28:15,559 --> 01:28:19,720
training so the trainer train a model on

01:28:18,379 --> 01:28:25,390
a data set we

01:28:19,720 --> 01:28:28,690
do we have to write a model dot fit and

01:28:25,390 --> 01:28:32,170
remember the DS train variable from olya

01:28:28,690 --> 01:28:37,210
we just need to specify our training

01:28:32,170 --> 01:28:41,080
data set yeah we have to specify the

01:28:37,210 --> 01:28:43,090
training data set we have to specify the

01:28:41,080 --> 01:28:45,820
number of epochs that is gonna train on

01:28:43,090 --> 01:28:48,910
so for I guess we're running low on time

01:28:45,820 --> 01:28:49,990
so I'm just gonna specify well I guess

01:28:48,910 --> 01:28:51,640
two epochs

01:28:49,990 --> 01:28:54,340
it's not gonna give me much of an a

01:28:51,640 --> 01:28:57,880
performance boost but just to visualize

01:28:54,340 --> 01:29:00,340
what usually happens when training after

01:28:57,880 --> 01:29:03,370
that I'm gonna say validation data it's

01:29:00,340 --> 01:29:06,600
gonna autocomplete equals D s underscore

01:29:03,370 --> 01:29:11,290
test so DS test from earlier it's my

01:29:06,600 --> 01:29:14,080
it's my testing data set and finally I

01:29:11,290 --> 01:29:16,870
want my timer I want to tie I want that

01:29:14,080 --> 01:29:20,260
time a person to be there so callbacks

01:29:16,870 --> 01:29:22,720
equals an array of all my comm callbacks

01:29:20,260 --> 01:29:25,000
so I can have as many callbacks as I

01:29:22,720 --> 01:29:27,730
want and Kaos has support for you know

01:29:25,000 --> 01:29:29,950
over ten different callbacks but here

01:29:27,730 --> 01:29:32,980
we're only using one so we just need to

01:29:29,950 --> 01:29:35,110
specify an array for callbacks and just

01:29:32,980 --> 01:29:39,270
you know type in TB or you know your

01:29:35,110 --> 01:29:42,250
called back here into it yeah

01:29:39,270 --> 01:29:44,710
tensor board you specify your logging

01:29:42,250 --> 01:29:48,160
directory I just call this my logs you

01:29:44,710 --> 01:29:51,520
can call it anything and you just need

01:29:48,160 --> 01:29:53,680
to fit the model so again you can just

01:29:51,520 --> 01:29:57,850
go ahead and click this if you run into

01:29:53,680 --> 01:30:03,340
an error it's probably because of some

01:29:57,850 --> 01:30:08,470
dimensionality issue so to fix this we

01:30:03,340 --> 01:30:11,290
ought to go back all the way here yeah

01:30:08,470 --> 01:30:16,870
so we had to go back all the way here

01:30:11,290 --> 01:30:20,080
and reload all the cells from this DS

01:30:16,870 --> 01:30:23,050
Trianon words so this one again it's

01:30:20,080 --> 01:30:25,210
more of a like a tensorflow error it's

01:30:23,050 --> 01:30:28,690
not an error on our part all you need to

01:30:25,210 --> 01:30:32,740
do is reload reload the data set

01:30:28,690 --> 01:30:33,989
actually over here in your DS train you

01:30:32,740 --> 01:30:37,199
can actually say

01:30:33,989 --> 01:30:39,750
download equals true so that actually

01:30:37,199 --> 01:30:42,389
fully caches the entire data set in

01:30:39,750 --> 01:30:48,989
memory so we don't get those weird

01:30:42,389 --> 01:30:51,090
errors at the bottom or your model

01:30:48,989 --> 01:30:55,829
training is taking a long time have you

01:30:51,090 --> 01:31:00,630
start have you clicked that fit cell are

01:30:55,829 --> 01:31:03,320
you still here so I'm I'm up all the way

01:31:00,630 --> 01:31:03,320
back where we started

01:31:03,440 --> 01:31:13,710
do you get this error or do you see you

01:31:09,090 --> 01:31:16,920
know something like oh so you see

01:31:13,710 --> 01:31:23,239
something or the play button so it's

01:31:16,920 --> 01:31:26,219
yeah that or this play button over here

01:31:23,239 --> 01:31:34,230
okay that again it depends on I guess

01:31:26,219 --> 01:31:37,349
that the speed of your machine yeah yeah

01:31:34,230 --> 01:31:40,139
but okay so we're using the CPU runtime

01:31:37,349 --> 01:31:42,389
the CPU it uses your local machine but

01:31:40,139 --> 01:31:44,790
if you go for your GPU runtime which

01:31:42,389 --> 01:31:48,389
we'll be doing in the next next segment

01:31:44,790 --> 01:31:50,310
the GPU runtime it uses the hosted VM so

01:31:48,389 --> 01:31:54,239
yeah it connects your machine to a Tesla

01:31:50,310 --> 01:31:55,500
k-8 GPU again since they're giving it

01:31:54,239 --> 01:31:59,239
out for free you know that you can't

01:31:55,500 --> 01:32:01,710
really complain much Tesla we okay

01:31:59,239 --> 01:32:04,020
they're not that fancy but you know we

01:32:01,710 --> 01:32:04,760
had to make do you know we want to train

01:32:04,020 --> 01:32:09,119
free of cost

01:32:04,760 --> 01:32:11,880
so from here for all the way from ds3

01:32:09,119 --> 01:32:15,800
and where we first loaded it we may need

01:32:11,880 --> 01:32:15,800
to run all these cells once again

01:32:25,820 --> 01:32:32,280
yep so again when you just rerun all the

01:32:30,330 --> 01:32:35,280
cells if you run into a dimensionality

01:32:32,280 --> 01:32:38,280
error especially when you are about just

01:32:35,280 --> 01:32:40,640
about to Train just reload all the cells

01:32:38,280 --> 01:32:43,700
restart your VM and run all your cells

01:32:40,640 --> 01:32:49,020
it's you know a tenth of our error oh

01:32:43,700 --> 01:32:53,670
you you but do you see anything below

01:32:49,020 --> 01:32:55,890
that yeah again it duh it's based on

01:32:53,670 --> 01:32:56,790
your machines performance your local

01:32:55,890 --> 01:33:03,720
machines performance

01:32:56,790 --> 01:33:09,150
oh so the app on this side did you see

01:33:03,720 --> 01:33:11,850
the the training happening okay yeah so

01:33:09,150 --> 01:33:14,310
in some cases you may see a message some

01:33:11,850 --> 01:33:16,770
cases you might not but do you get

01:33:14,310 --> 01:33:20,790
something that looks like this and so

01:33:16,770 --> 01:33:23,280
you see something moving oh yeah great

01:33:20,790 --> 01:33:25,610
so technically you can even train up to

01:33:23,280 --> 01:33:33,750
12 Phi e box

01:33:25,610 --> 01:33:36,510
you know just yeah so over here in your

01:33:33,750 --> 01:33:38,700
DS train variable over here you can set

01:33:36,510 --> 01:33:40,140
download equals true so it's gonna pull

01:33:38,700 --> 01:33:42,720
everything that's there in that

01:33:40,140 --> 01:33:44,550
repository down to my machine so that I

01:33:42,720 --> 01:33:47,970
don't need to keep pulling it whenever I

01:33:44,550 --> 01:33:50,460
need refer to it so everything is now

01:33:47,970 --> 01:33:52,830
finally on my machine I don't need to

01:33:50,460 --> 01:33:55,700
rely on anything else and it's much

01:33:52,830 --> 01:33:58,500
faster when you download the data set

01:33:55,700 --> 01:33:59,850
so yeah India's trained just download

01:33:58,500 --> 01:34:01,800
equals true you don't need to do the

01:33:59,850 --> 01:34:03,360
same for testing yeah you don't need to

01:34:01,800 --> 01:34:05,490
do that for testing because the testing

01:34:03,360 --> 01:34:08,100
data set is only like 10 K images so

01:34:05,490 --> 01:34:13,520
it's not that much effort but 60 K yeah

01:34:08,100 --> 01:34:16,970
it's it's a huge number all right so

01:34:13,520 --> 01:34:19,470
yeah you can exactly yeah so notebooks

01:34:16,970 --> 01:34:21,930
you'll actually save all your things

01:34:19,470 --> 01:34:23,160
it's it's like creating a Google Doc all

01:34:21,930 --> 01:34:24,960
right so when you type in something and

01:34:23,160 --> 01:34:27,630
when I close that browser when I go back

01:34:24,960 --> 01:34:29,190
to my drive it's there similarly when

01:34:27,630 --> 01:34:32,010
you close a colab notebook you can

01:34:29,190 --> 01:34:33,780
access it in your drive yeah and it'll

01:34:32,010 --> 01:34:35,280
actually like save all your output so

01:34:33,780 --> 01:34:37,410
you don't need to run everything again

01:34:35,280 --> 01:34:38,369
because there are some I guess training

01:34:37,410 --> 01:34:40,469
jobs that take

01:34:38,369 --> 01:34:41,939
twelve hours right I don't want to keep

01:34:40,469 --> 01:34:43,679
running that every day just because I

01:34:41,939 --> 01:34:44,909
closed my browser at the same time I

01:34:43,679 --> 01:34:47,309
don't want to keep my browser in my

01:34:44,909 --> 01:34:48,809
Chrome or like my browser or the tab in

01:34:47,309 --> 01:34:51,749
my browser because you know it looks

01:34:48,809 --> 01:34:54,479
messy so I can close it come back it'll

01:34:51,749 --> 01:34:55,800
retain all my all that information so I

01:34:54,479 --> 01:35:01,399
guess that's another benefit of using

01:34:55,800 --> 01:35:01,399
collab all right so you know just just

01:35:01,699 --> 01:35:09,479
yeah yeah so you can do command or

01:35:05,669 --> 01:35:12,409
control s yeah and I it'll save it

01:35:09,479 --> 01:35:14,610
similar to what you do with Google Docs

01:35:12,409 --> 01:35:17,070
it auto saves your similarly even

01:35:14,610 --> 01:35:25,379
collabs autosave you know the cool lab

01:35:17,070 --> 01:35:28,709
notebooks all of them autosave yes even

01:35:25,379 --> 01:35:33,030
Docs it auto saves actually all the you

01:35:28,709 --> 01:35:34,800
know sheets talks all opera or Google

01:35:33,030 --> 01:35:38,459
slides all of them are autosave by

01:35:34,800 --> 01:35:42,719
default but I guess for brevity's sake

01:35:38,459 --> 01:35:44,849
you can keep yeah yeah but if you want

01:35:42,719 --> 01:35:49,369
to manually save you can do command S or

01:35:44,849 --> 01:35:56,039
control s and like force save again yeah

01:35:49,369 --> 01:35:58,019
yeah so I guess I changed this to two or

01:35:56,039 --> 01:36:01,559
five so we're just training for fy'y box

01:35:58,019 --> 01:36:06,979
instead of two so yeah we just need to

01:36:01,559 --> 01:36:06,979
wait for this training loop to end yeah

01:36:10,089 --> 01:36:19,239
oh yeah so the the callbacks in general

01:36:16,030 --> 01:36:21,579
not all of them are updated to 2.0 and

01:36:19,239 --> 01:36:24,309
not all of them are you know when you

01:36:21,579 --> 01:36:26,800
put them on a VM they can run at the

01:36:24,309 --> 01:36:28,659
same with the same performance so that's

01:36:26,800 --> 01:36:31,629
why some of them may be too slow for

01:36:28,659 --> 01:36:35,409
your CPU clock it'll give you the error

01:36:31,629 --> 01:36:36,999
but you don't really need to worry so so

01:36:35,409 --> 01:36:39,449
even like to the online viewers if you

01:36:36,999 --> 01:36:44,280
have any errors if you're running this

01:36:39,449 --> 01:36:47,559
right now you can ignore any warnings

01:36:44,280 --> 01:36:49,419
because most of the most of the warnings

01:36:47,559 --> 01:36:52,300
they're just because of some internal

01:36:49,419 --> 01:36:54,609
error with tencel board or tensorflow so

01:36:52,300 --> 01:36:55,780
you don't really need to care you can

01:36:54,609 --> 01:36:57,699
just ignore all of them and just

01:36:55,780 --> 01:36:59,879
continue coding yeah they won't affect

01:36:57,699 --> 01:36:59,879
anything

01:37:02,969 --> 01:37:11,679
it'll just train you yeah so you are

01:37:09,699 --> 01:37:15,219
when you wherever you are when you close

01:37:11,679 --> 01:37:19,149
when you close a tab that contains a

01:37:15,219 --> 01:37:20,469
code book or if the notebook is inactive

01:37:19,149 --> 01:37:22,389
for a really long time like I haven't

01:37:20,469 --> 01:37:24,969
done anything to it for like two hours

01:37:22,389 --> 01:37:27,149
right it'll automatically disconnect

01:37:24,969 --> 01:37:30,010
remember when we started we click this

01:37:27,149 --> 01:37:32,379
connect button that was here it a liqu

01:37:30,010 --> 01:37:35,619
AIT's the VM and it connects everything

01:37:32,379 --> 01:37:37,659
and then you can start coding yeah so if

01:37:35,619 --> 01:37:39,609
you don't use it if it's idle for a long

01:37:37,659 --> 01:37:41,679
time it'll disconnect all you need to do

01:37:39,609 --> 01:37:46,119
is click that connect button again and

01:37:41,679 --> 01:37:48,729
you can run all your cells so actually

01:37:46,119 --> 01:37:50,679
if you're if you're lazy to run you know

01:37:48,729 --> 01:37:53,379
click keep clicking the play button if

01:37:50,679 --> 01:37:56,079
you click a runtime in your menu you can

01:37:53,379 --> 01:37:58,599
just click run all I'll just you know do

01:37:56,079 --> 01:38:00,219
a full sweep so you don't need to keep

01:37:58,599 --> 01:38:02,319
clicking the play button so you can do

01:38:00,219 --> 01:38:05,289
whatever you want but in the background

01:38:02,319 --> 01:38:09,969
the code is running alright so now that

01:38:05,289 --> 01:38:13,510
we have trained now that we have trained

01:38:09,969 --> 01:38:16,659
our model for Firefox if you can see

01:38:13,510 --> 01:38:20,229
over here the accuracy you can see that

01:38:16,659 --> 01:38:23,880
on our testing set the DES test we're

01:38:20,229 --> 01:38:26,820
reaching a 97 in 97.5 accurate

01:38:23,880 --> 01:38:29,130
see which is kind of huge right so that

01:38:26,820 --> 01:38:33,720
means out of all the 10,000 images it

01:38:29,130 --> 01:38:37,050
got 97% of them correct that's already

01:38:33,720 --> 01:38:40,560
about nine point seven nine point seven

01:38:37,050 --> 01:38:43,200
Kay yeah so yeah so this is your testing

01:38:40,560 --> 01:38:45,600
accuracy or validation accuracy and this

01:38:43,200 --> 01:38:49,950
is your training accuracy we got a 99

01:38:45,600 --> 01:38:51,710
here and 97 here you can always for most

01:38:49,950 --> 01:38:54,060
cases you can always expect your

01:38:51,710 --> 01:38:55,830
validation or testing accuracy to be

01:38:54,060 --> 01:38:58,560
slightly lower than your training

01:38:55,830 --> 01:39:00,630
accuracy because again these testing

01:38:58,560 --> 01:39:03,510
instances we've never really fully seen

01:39:00,630 --> 01:39:04,770
them or the model hasn't seen them but

01:39:03,510 --> 01:39:06,630
it's seen everything in the training

01:39:04,770 --> 01:39:11,310
data set which is like it accounts for

01:39:06,630 --> 01:39:13,290
the discrepancy all right so now that we

01:39:11,310 --> 01:39:15,030
finished training for fy'y box and we

01:39:13,290 --> 01:39:18,300
have these you know random numbers over

01:39:15,030 --> 01:39:21,210
here we can finally visualize this

01:39:18,300 --> 01:39:23,250
entire thing intensive board so to

01:39:21,210 --> 01:39:26,870
invoke tensile board and if you want to

01:39:23,250 --> 01:39:31,980
visualize it in collab you can type in

01:39:26,870 --> 01:39:35,310
the % Lord underscore ext which is

01:39:31,980 --> 01:39:46,160
basically Lord extension and you can

01:39:35,310 --> 01:39:51,390
type in tensile board over here yeah and

01:39:46,160 --> 01:39:55,110
you can run this cell yeah shouldn't

01:39:51,390 --> 01:39:58,320
give any errors so yeah % Lord

01:39:55,110 --> 01:40:00,210
underscore ext space tensile board so

01:39:58,320 --> 01:40:03,240
that gives you control over using

01:40:00,210 --> 01:40:06,300
tensile board so now that we have I

01:40:03,240 --> 01:40:08,040
guess imported tensile board we can

01:40:06,300 --> 01:40:10,320
finally visualize our training

01:40:08,040 --> 01:40:15,390
performance all those metrics intensive

01:40:10,320 --> 01:40:15,690
board so again exclamation sign tensile

01:40:15,390 --> 01:40:19,350
board

01:40:15,690 --> 01:40:24,210
it should autocomplete now and for this

01:40:19,350 --> 01:40:25,680
it wants a logging directory so it's

01:40:24,210 --> 01:40:28,500
going to search for that logging

01:40:25,680 --> 01:40:33,120
directory on my machine and it'll dig

01:40:28,500 --> 01:40:35,880
out all my log files from there so over

01:40:33,120 --> 01:40:37,680
here when in my tensile board callback I

01:40:35,880 --> 01:40:40,740
said that I want my log in there

01:40:37,680 --> 01:40:44,190
treat to be my logs so over here I just

01:40:40,740 --> 01:40:46,620
need to type in my underscore logs again

01:40:44,190 --> 01:40:48,750
so what tensile bonds gonna do is it's

01:40:46,620 --> 01:40:52,980
actually gonna go into that folder and

01:40:48,750 --> 01:40:58,140
just get all my training logs so now if

01:40:52,980 --> 01:41:12,840
I can zoom out and so else in launching

01:40:58,140 --> 01:41:15,570
tensor board and that cell hold on yeah

01:41:12,840 --> 01:41:19,710
it should be I think she yeah

01:41:15,570 --> 01:41:22,970
it's so over here it's just one director

01:41:19,710 --> 01:41:25,410
word LOD dir there's no - or underscore

01:41:22,970 --> 01:41:33,240
alright so you should see something like

01:41:25,410 --> 01:41:35,300
this and this is tensor board so does

01:41:33,240 --> 01:41:37,860
everyone see this something like this

01:41:35,300 --> 01:41:44,160
yeah I just just give it some time to

01:41:37,860 --> 01:41:46,970
run ok can I know what error that is

01:41:44,160 --> 01:41:46,970
what's it saying oh

01:41:49,490 --> 01:41:54,890
can you try loading that extension again

01:41:58,280 --> 01:42:03,120
on this side

01:42:00,330 --> 01:42:05,420
do you have tensor board extensible

01:42:03,120 --> 01:42:05,420
running

01:42:14,350 --> 01:42:19,640
so I'm guessing you finished your

01:42:16,880 --> 01:42:26,510
training loop successfully did you get

01:42:19,640 --> 01:42:28,550
any errors for training yeah so I think

01:42:26,510 --> 01:42:30,560
to speed things up you can set the

01:42:28,550 --> 01:42:33,800
epochs to 2 and just quickly run that

01:42:30,560 --> 01:42:41,630
sell the model dot fit sell do you see

01:42:33,800 --> 01:42:45,320
this oh so just a recap you need to type

01:42:41,630 --> 01:42:47,540
in percent load underscore ext tensile

01:42:45,320 --> 01:42:50,240
board and when you run that it's gonna

01:42:47,540 --> 01:42:54,530
invoke tensor board such that you can

01:42:50,240 --> 01:42:58,100
display it in your collab notebook and

01:42:54,530 --> 01:43:02,390
in your next cell you can type % tensile

01:42:58,100 --> 01:43:05,350
board and as a command line parameter

01:43:02,390 --> 01:43:08,810
you can give it log dear L og dir and

01:43:05,350 --> 01:43:11,210
specify the logging directory that you

01:43:08,810 --> 01:43:15,830
typed in over here with your 10 support

01:43:11,210 --> 01:43:18,230
call back and when you run that cell it

01:43:15,830 --> 01:43:22,360
should say launching tensor board and it

01:43:18,230 --> 01:43:22,360
should it should show everything

01:43:36,659 --> 01:43:44,260
so do you see something like this okay

01:43:42,040 --> 01:43:49,170
yeah so tense about it it takes a while

01:43:44,260 --> 01:43:52,060
to load because it needs to build the UI

01:43:49,170 --> 01:43:56,139
but I guess we're running low on time

01:43:52,060 --> 01:43:58,000
here it's already 5:00 so I'll give you

01:43:56,139 --> 01:43:59,440
like a quick recap or like a quick

01:43:58,000 --> 01:44:01,570
run-through of tensile board on my

01:43:59,440 --> 01:44:06,100
screen up here so tensile board as you

01:44:01,570 --> 01:44:09,159
can see we it logs my training accuracy

01:44:06,100 --> 01:44:12,100
and my loss values so over here you can

01:44:09,159 --> 01:44:15,820
see a gradual increase in accuracy you

01:44:12,100 --> 01:44:19,239
know we're hitting up to you know 99 99

01:44:15,820 --> 01:44:22,270
percent training accuracy and testing

01:44:19,239 --> 01:44:24,130
accuracy is about you know 97 and at the

01:44:22,270 --> 01:44:25,900
same time when you see our lost values

01:44:24,130 --> 01:44:27,969
when you're training a good thing that

01:44:25,900 --> 01:44:30,429
or like something that should really

01:44:27,969 --> 01:44:32,320
keep an eye out for is if you're lost

01:44:30,429 --> 01:44:33,880
values are decreasing if you're lost

01:44:32,320 --> 01:44:35,920
values are decreasing it means that your

01:44:33,880 --> 01:44:38,800
model is actually training because the

01:44:35,920 --> 01:44:41,560
number of testing instances that it's

01:44:38,800 --> 01:44:44,409
getting correct that's also increasing

01:44:41,560 --> 01:44:47,710
so I'm getting more things correct which

01:44:44,409 --> 01:44:50,020
is why the loss which kind of measures

01:44:47,710 --> 01:44:52,540
how far apart my predictions are from

01:44:50,020 --> 01:44:54,760
the true labels so yeah it should be

01:44:52,540 --> 01:44:57,880
decreasing and over here we can see that

01:44:54,760 --> 01:45:02,429
the graphic church it shows some gradual

01:44:57,880 --> 01:45:06,420
decay and the loss and yeah that's good

01:45:02,429 --> 01:45:06,420
so over here into yeah

01:45:16,980 --> 01:45:25,320
oh yeah you clear your logs I should

01:45:20,460 --> 01:45:27,210
have met so actually no you don't you

01:45:25,320 --> 01:45:30,000
don't need to clear the logs because it

01:45:27,210 --> 01:45:32,090
automatically it overrides whatever's

01:45:30,000 --> 01:45:34,530
there yeah

01:45:32,090 --> 01:45:36,390
because as you're running these cells

01:45:34,530 --> 01:45:38,850
right it's going to overwrite all the

01:45:36,390 --> 01:45:41,610
pre-existing variables or whatever it

01:45:38,850 --> 01:45:50,910
had in its memory so yeah you don't need

01:45:41,610 --> 01:45:52,590
to worry about that okay sorry oh yeah

01:45:50,910 --> 01:45:54,330
that that usually happens when you're

01:45:52,590 --> 01:45:56,070
running tends to flow on like something

01:45:54,330 --> 01:46:02,610
like Jupiter something on a local

01:45:56,070 --> 01:46:04,620
machine not not hosted runtime because

01:46:02,610 --> 01:46:08,550
I've gotten that six-year local host

01:46:04,620 --> 01:46:12,990
with the pot six zero zero six that

01:46:08,550 --> 01:46:15,930
usually happens when you're using tensor

01:46:12,990 --> 01:46:18,120
board when you're not using tensor board

01:46:15,930 --> 01:46:19,440
the way you're supposed to in collab can

01:46:18,120 --> 01:46:21,900
you check again whether you have written

01:46:19,440 --> 01:46:24,330
both of these done first you need to

01:46:21,900 --> 01:46:26,870
load the extension and then you need to

01:46:24,330 --> 01:46:26,870
call it

01:46:29,570 --> 01:46:34,030
if you type in tensor board on its own

01:46:31,310 --> 01:46:38,630
is going to assume that you are using

01:46:34,030 --> 01:46:41,480
tensor board or using localhost not

01:46:38,630 --> 01:46:44,239
something that's hosted online which is

01:46:41,480 --> 01:46:47,179
why when you're using a local host

01:46:44,239 --> 01:46:49,190
runtime you can click another local host

01:46:47,179 --> 01:46:51,889
it'll direct you to another port URL

01:46:49,190 --> 01:47:01,489
which is why you can see ten support in

01:46:51,889 --> 01:47:02,989
the way you saw it but oh yeah you can

01:47:01,489 --> 01:47:06,980
you can look yeah I think that should be

01:47:02,989 --> 01:47:10,520
fine and then after that you can type in

01:47:06,980 --> 01:47:14,139
percent tensor board and specify the

01:47:10,520 --> 01:47:14,139
logging directory like so

01:47:34,949 --> 01:47:42,619
reloading the extension if it doesn't if

01:47:38,610 --> 01:47:42,619
it doesn't show up in the first try I

01:47:58,699 --> 01:48:04,769
think for now is it possible to refer to

01:48:02,070 --> 01:48:10,260
the slides on the screen I think I can

01:48:04,769 --> 01:48:12,300
quickly brush through yeah yes so so

01:48:10,260 --> 01:48:13,440
tensor board this is what it is it's a

01:48:12,300 --> 01:48:15,510
user interface kind of like a

01:48:13,440 --> 01:48:18,030
visualization dashboard that you can use

01:48:15,510 --> 01:48:20,219
to visualize your training and testing

01:48:18,030 --> 01:48:22,679
metrics performance metrics it logs

01:48:20,219 --> 01:48:25,249
everything and contains metadata on your

01:48:22,679 --> 01:48:27,989
training performance so at the same time

01:48:25,249 --> 01:48:30,030
earlier I mentioned that tensorflow what

01:48:27,989 --> 01:48:33,030
it does is it creates a computation

01:48:30,030 --> 01:48:36,829
graph so to visualize this computation

01:48:33,030 --> 01:48:40,170
graph you need to go to the graphs

01:48:36,829 --> 01:48:43,260
section in the header and you'll be

01:48:40,170 --> 01:48:46,559
presented with a bunch of boxes with

01:48:43,260 --> 01:48:48,389
arrows connecting you know with arrows

01:48:46,559 --> 01:48:51,179
connecting each other so this is called

01:48:48,389 --> 01:48:56,280
a computation graph this corresponds to

01:48:51,179 --> 01:48:58,229
the model that we wrote above here so

01:48:56,280 --> 01:49:00,840
the model that I wrote here with you

01:48:58,229 --> 01:49:02,519
know three dense layers it's the exact

01:49:00,840 --> 01:49:04,769
same thing that's represented here in

01:49:02,519 --> 01:49:07,860
the form of a computation graph so it

01:49:04,769 --> 01:49:11,550
takes in my input like so it feeds it

01:49:07,860 --> 01:49:14,789
through all the layers in my in my

01:49:11,550 --> 01:49:16,499
network it finds the lost value and at

01:49:14,789 --> 01:49:18,780
the same time it's computing the metrics

01:49:16,499 --> 01:49:23,039
so if you were to double click on one of

01:49:18,780 --> 01:49:27,199
these computation graph nodes you can

01:49:23,039 --> 01:49:30,420
actually it'll it'll expand and it'll

01:49:27,199 --> 01:49:32,340
give you a bit more information so Matt

01:49:30,420 --> 01:49:35,880
mull over here stands for matrix

01:49:32,340 --> 01:49:40,050
multiplication so only I also mentioned

01:49:35,880 --> 01:49:42,749
that earlier it computes the W dot X

01:49:40,050 --> 01:49:45,989
plus B so this is exactly what is

01:49:42,749 --> 01:49:47,210
happening you multiply the x value so in

01:49:45,989 --> 01:49:49,400
this map mo

01:49:47,210 --> 01:49:51,140
node it's taking in the inputs

01:49:49,400 --> 01:49:53,570
multiplying it with the respective

01:49:51,140 --> 01:49:58,460
weight and it's adding the bias in this

01:49:53,570 --> 01:50:01,520
bias add node so this happens for all

01:49:58,460 --> 01:50:07,730
the nodes in your computation graph and

01:50:01,520 --> 01:50:09,970
that's exactly how yeah that's how it's

01:50:07,730 --> 01:50:13,250
visualized over here

01:50:09,970 --> 01:50:15,590
of course whip tensor flow attends a

01:50:13,250 --> 01:50:17,480
board you can do much more depending on

01:50:15,590 --> 01:50:19,700
the kinds of metrics that you use you

01:50:17,480 --> 01:50:26,240
know custom metrics you'll have a lot

01:50:19,700 --> 01:50:29,020
more graphs to show yeah something like

01:50:26,240 --> 01:50:31,580
this you have a lot of graphs over here

01:50:29,020 --> 01:50:33,620
it all depends on what kind of metrics

01:50:31,580 --> 01:50:37,970
you give and what kind of callbacks

01:50:33,620 --> 01:50:39,800
you're using it also kind of specifies

01:50:37,970 --> 01:50:42,800
what exactly you're trying to track

01:50:39,800 --> 01:50:46,300
during your training so now that we have

01:50:42,800 --> 01:50:51,500
kind of trained of a simple three layer

01:50:46,300 --> 01:50:53,690
new network on a mist I guess we can go

01:50:51,500 --> 01:50:58,580
ahead and do something a bit more

01:50:53,690 --> 01:51:02,260
advanced so a while back are there any

01:50:58,580 --> 01:51:02,260
issues still with the tensor board

01:51:18,790 --> 01:51:32,260
hold on really check

01:51:59,390 --> 01:52:04,040
let me just plug in my charger

01:53:01,580 --> 01:53:09,560
excuse me yeah on this side were you

01:53:04,350 --> 01:53:09,560
able to get tensor it works right okay

01:53:10,760 --> 01:53:18,239
okay great yeah so I guess just to bring

01:53:15,090 --> 01:53:20,040
everyone up to speed so yeah Skylar's in

01:53:18,239 --> 01:53:24,060
the sky last section you can see all the

01:53:20,040 --> 01:53:26,940
graphs all the graphs that that we try

01:53:24,060 --> 01:53:31,260
to log so it has the lost graph and my

01:53:26,940 --> 01:53:35,250
accuracy graph so do you see it okay

01:53:31,260 --> 01:53:38,400
yeah so also when you go to this graphs

01:53:35,250 --> 01:53:39,780
header you should see this

01:53:38,400 --> 01:53:41,850
you should see this computation graph

01:53:39,780 --> 01:53:43,860
right there all nodes and they have

01:53:41,850 --> 01:53:45,780
arrows connecting to it or edges

01:53:43,860 --> 01:53:47,489
connecting to it this is our computation

01:53:45,780 --> 01:53:50,100
graph this exactly corresponds to the

01:53:47,489 --> 01:53:52,590
model that we wrote you know in the

01:53:50,100 --> 01:53:54,420
cells above yeah and you can you can

01:53:52,590 --> 01:53:55,949
double click one of them and it'll

01:53:54,420 --> 01:53:59,070
expand to show you what's happening

01:53:55,949 --> 01:54:02,160
inside it so over here we see that the

01:53:59,070 --> 01:54:04,020
matrix multiplication W times X like the

01:54:02,160 --> 01:54:07,350
you're multiplying the incoming input

01:54:04,020 --> 01:54:13,620
with the weight and this you know we're

01:54:07,350 --> 01:54:17,760
also adding the bias yeah and you can

01:54:13,620 --> 01:54:22,620
double-click to close it so yeah this is

01:54:17,760 --> 01:54:24,780
I guess like a quick overview with you

01:54:22,620 --> 01:54:27,110
know M NIST and tensile blow tensor

01:54:24,780 --> 01:54:27,110
board

01:54:34,950 --> 01:54:39,760
yeah tensile board it's been there ever

01:54:37,630 --> 01:54:41,350
since version one came out so it's

01:54:39,760 --> 01:54:44,950
compatible with everything it just that

01:54:41,350 --> 01:54:46,570
for this specific example I guess to

01:54:44,950 --> 01:54:49,270
keep up with the most recent

01:54:46,570 --> 01:54:52,980
developments intensive flow to point X

01:54:49,270 --> 01:54:52,980
is the is the stable release

01:55:15,290 --> 01:55:21,350
like the online of your the question was

01:55:17,720 --> 01:55:24,980
why why has Google changed a lot of the

01:55:21,350 --> 01:55:28,310
API aspects when you when we moved from

01:55:24,980 --> 01:55:31,640
one point version 1.1 X all the way to

01:55:28,310 --> 01:55:34,700
version 2 I guess the answer for that is

01:55:31,640 --> 01:55:36,740
as Kerris became the standard for

01:55:34,700 --> 01:55:38,360
creating models or basically building

01:55:36,740 --> 01:55:39,710
data pipelines machine learning

01:55:38,360 --> 01:55:43,790
pipelines

01:55:39,710 --> 01:55:46,790
it was way easier to use and more

01:55:43,790 --> 01:55:50,480
convenient so tensorflow they had to do

01:55:46,790 --> 01:55:53,240
a full 180 degree flip when changing

01:55:50,480 --> 01:55:55,130
their all the low-level operations and

01:55:53,240 --> 01:55:57,110
converting it into like a high-level

01:55:55,130 --> 01:55:59,660
abstraction like Kerris that's why

01:55:57,110 --> 01:56:03,200
they've deprecated most of the

01:55:59,660 --> 01:56:05,390
underlying I guess ugly code and they've

01:56:03,200 --> 01:56:08,560
made it much more convenient it's all

01:56:05,390 --> 01:56:10,910
the sub modules that exist in version 2

01:56:08,560 --> 01:56:13,850
it's really convenient to access them

01:56:10,910 --> 01:56:15,440
and you don't need you know you don't

01:56:13,850 --> 01:56:17,420
need to bang your head against the wall

01:56:15,440 --> 01:56:25,490
when trying to make a work it's pretty

01:56:17,420 --> 01:56:28,580
simple yeah yeah they had to rewrite a

01:56:25,490 --> 01:56:32,000
lot of code to bring it up to the the

01:56:28,580 --> 01:56:33,920
current version yeah but I guess it's

01:56:32,000 --> 01:56:36,680
for like the benefit of the community

01:56:33,920 --> 01:56:38,600
because you know you want many people to

01:56:36,680 --> 01:56:42,410
be able to use it both advanced users

01:56:38,600 --> 01:56:44,150
and beginners so they're kind of cater

01:56:42,410 --> 01:56:46,700
to both of them you want to give them

01:56:44,150 --> 01:56:49,100
what they want so for advanced users you

01:56:46,700 --> 01:56:50,390
want a lot of customized ability for

01:56:49,100 --> 01:56:52,280
beginners if you give them the bare

01:56:50,390 --> 01:56:54,980
minimum they'll be able to work with it

01:56:52,280 --> 01:56:56,740
so yeah that's why they had to break

01:56:54,980 --> 01:57:00,380
down quite a bit

01:56:56,740 --> 01:57:03,190
construct that the TF dot Kara's the

01:57:00,380 --> 01:57:03,190
complete library

01:57:20,260 --> 01:57:24,290
it's

01:57:21,800 --> 01:57:28,010
five-thirty I guess like we can speed it

01:57:24,290 --> 01:57:31,780
up I have a few demo notebooks that I

01:57:28,010 --> 01:57:35,480
can so I have like links to all my

01:57:31,780 --> 01:57:39,800
notebooks that I'll give soon like you

01:57:35,480 --> 01:57:43,040
can check it after the talk box yeah so

01:57:39,800 --> 01:57:47,410
I did have an other talk lined up for

01:57:43,040 --> 01:57:50,810
like a workshop hands-on thing for today

01:57:47,410 --> 01:57:55,720
so I thought if we have time we could

01:57:50,810 --> 01:57:59,840
run or run through this kovat 19 X ray

01:57:55,720 --> 01:58:03,230
x-ray classification task you know so

01:57:59,840 --> 01:58:05,930
what it is is it's this data set

01:58:03,230 --> 01:58:09,860
consisting of you know 146 x-ray scans

01:58:05,930 --> 01:58:13,040
so you know from all the past cases and

01:58:09,860 --> 01:58:14,840
like in the past two months you know one

01:58:13,040 --> 01:58:17,750
of the professors at the University of

01:58:14,840 --> 01:58:20,600
Montreal he compiled all of these x-rays

01:58:17,750 --> 01:58:24,920
with their you know the diagnosis with

01:58:20,600 --> 01:58:28,100
the you know diagnosis and he made it

01:58:24,920 --> 01:58:30,590
open source on github so I'll share the

01:58:28,100 --> 01:58:32,980
notebook where I kind of create a tensor

01:58:30,590 --> 01:58:36,290
for pipeline that takes in all these

01:58:32,980 --> 01:58:38,300
x-ray images and trains a convolutional

01:58:36,290 --> 01:58:47,810
neural network so you can see it at your

01:58:38,300 --> 01:58:51,800
own time yeah actually yeah you can you

01:58:47,810 --> 01:58:53,600
can see it over here you can access so

01:58:51,800 --> 01:58:57,860
even to the online viewers because of

01:58:53,600 --> 01:59:00,010
time constraints and I guess to prepare

01:58:57,860 --> 01:59:04,820
for like is there a next talk after this

01:59:00,010 --> 01:59:07,040
yeah so I guess for for time for lack of

01:59:04,820 --> 01:59:10,970
time I think you can speed up and like

01:59:07,040 --> 01:59:13,070
close close the session for today so

01:59:10,970 --> 01:59:15,890
these are the two notebooks that I wrote

01:59:13,070 --> 01:59:20,390
a while back to prepare for this talk so

01:59:15,890 --> 01:59:24,440
this is like the complete version of you

01:59:20,390 --> 01:59:26,900
know all the code it has no errors so

01:59:24,440 --> 01:59:29,630
you can download the notebook into your

01:59:26,900 --> 01:59:32,330
own Google Drive and you can connect it

01:59:29,630 --> 01:59:34,370
and you can run all of them and for evil

01:59:32,330 --> 01:59:35,540
for one of them actually the amnesty

01:59:34,370 --> 01:59:38,360
tutorial

01:59:35,540 --> 01:59:40,760
I have hooked it up to the the hosted

01:59:38,360 --> 01:59:42,980
GPU so you don't need to run it on your

01:59:40,760 --> 01:59:46,340
local machine anymore so if you go to

01:59:42,980 --> 01:59:49,220
the amnesty version and run all of them

01:59:46,340 --> 01:59:53,560
it runs on the GPU and the same thing

01:59:49,220 --> 01:59:56,420
for the covert 19 training data set task

01:59:53,560 --> 01:59:59,150
when you run this notebook all of its on

01:59:56,420 --> 02:00:01,880
the GPU so no matter which device you're

01:59:59,150 --> 02:00:05,000
running your code on you should get some

02:00:01,880 --> 02:00:07,310
really fast performance upgrades so yeah

02:00:05,000 --> 02:00:10,520
so it's device agnostic shouldn't really

02:00:07,310 --> 02:00:12,140
be a problem all right so I guess now

02:00:10,520 --> 02:00:14,570
that we're kind of finished more at the

02:00:12,140 --> 02:00:19,640
coding segment I think I can wrap it up

02:00:14,570 --> 02:00:22,000
here so in this tutorial what I've

02:00:19,640 --> 02:00:25,760
covered is the basics of tensorflow

02:00:22,000 --> 02:00:30,350
a bit of history why was created what it

02:00:25,760 --> 02:00:33,440
provides I've gone through the model the

02:00:30,350 --> 02:00:35,360
models API the layers API tensor board

02:00:33,440 --> 02:00:41,090
to visualize all your performance or

02:00:35,360 --> 02:00:43,100
training training metrics so Karis

02:00:41,090 --> 02:00:45,560
back-end it was supposed to be covered

02:00:43,100 --> 02:00:47,390
in the second talk but what it does is

02:00:45,560 --> 02:00:51,770
it provides you with some low-level

02:00:47,390 --> 02:00:56,000
operations like you know the mean sum

02:00:51,770 --> 02:00:57,920
square absolute dot product R max it

02:00:56,000 --> 02:00:59,780
provides you all these tools so that you

02:00:57,920 --> 02:01:02,900
can write your own loss function you can

02:00:59,780 --> 02:01:06,470
write your own optimizer you can write

02:01:02,900 --> 02:01:08,330
your own metrics to track so yeah you

02:01:06,470 --> 02:01:10,880
could do this all with the chaos backend

02:01:08,330 --> 02:01:14,630
and in the second notebook for Covell 19

02:01:10,880 --> 02:01:18,220
notebook it has a segment for custom

02:01:14,630 --> 02:01:18,220
metrics and custom loss functions

02:01:21,679 --> 02:01:28,280
and yep so well with tensorflow what you

02:01:25,489 --> 02:01:29,749
can do is you can do you know data

02:01:28,280 --> 02:01:31,789
collection you can do data

02:01:29,749 --> 02:01:34,099
pre-processing you can load data sets

02:01:31,789 --> 02:01:35,229
your own data sets or the ones provided

02:01:34,099 --> 02:01:38,780
by tensorflow

02:01:35,229 --> 02:01:40,969
you can create build you can build and

02:01:38,780 --> 02:01:44,420
train test models and you can even

02:01:40,969 --> 02:01:46,670
visualize their performance in tensor

02:01:44,420 --> 02:01:48,880
board and you know once you're happy

02:01:46,670 --> 02:01:52,219
with its performance you can actually

02:01:48,880 --> 02:01:54,920
you know push it to production you can

02:01:52,219 --> 02:01:56,840
put it up on specialized Hardware using

02:01:54,920 --> 02:01:58,789
this thing called tensor flow serving

02:01:56,840 --> 02:02:03,170
which allows you to take the model the

02:01:58,789 --> 02:02:05,179
frozen weight and serve it in the cloud

02:02:03,170 --> 02:02:07,189
so all you need to do is you know give

02:02:05,179 --> 02:02:11,929
it an API call and you can get back to

02:02:07,189 --> 02:02:16,880
predictions so if you are in Singapore

02:02:11,929 --> 02:02:20,239
and you would want to explore more about

02:02:16,880 --> 02:02:22,309
tensorflow you can you can consider

02:02:20,239 --> 02:02:24,949
joining the tensor flow and deep

02:02:22,309 --> 02:02:27,920
learning Singapore meetup group so me

02:02:24,949 --> 02:02:29,599
when you go to meetup.com you can search

02:02:27,920 --> 02:02:32,030
up tensorflow and deep learning

02:02:29,599 --> 02:02:35,150
Singapore and you can become a member

02:02:32,030 --> 02:02:37,699
it's the largest tensorflow meetup group

02:02:35,150 --> 02:02:40,909
on the planet today it's growing really

02:02:37,699 --> 02:02:44,329
quickly so it holds talks every month

02:02:40,909 --> 02:02:46,070
so they're you know we have lightning

02:02:44,329 --> 02:02:47,539
talks you know those five-minute talks

02:02:46,070 --> 02:02:50,689
where someone comes and presents their

02:02:47,539 --> 02:02:52,880
projects or we have the hosts or someone

02:02:50,689 --> 02:02:54,619
from you know all these big corporations

02:02:52,880 --> 02:02:57,650
using machine learning they come over

02:02:54,619 --> 02:03:00,289
and they kind of give us a glimpse as to

02:02:57,650 --> 02:03:04,340
what that company is doing or their ml

02:03:00,289 --> 02:03:06,769
teams are doing and or another thing

02:03:04,340 --> 02:03:10,099
that you know the PFD L meetup does is

02:03:06,769 --> 02:03:14,300
it covers like cutting-edge research

02:03:10,099 --> 02:03:17,389
papers so all the fancy research that's

02:03:14,300 --> 02:03:21,050
taking place at Google brain at deep

02:03:17,389 --> 02:03:24,019
mind opening I they write up the code or

02:03:21,050 --> 02:03:26,239
they are take that paper and convert

02:03:24,019 --> 02:03:29,659
that into like tensorflow code and they

02:03:26,239 --> 02:03:33,469
kind of do like a live demo at the talk

02:03:29,659 --> 02:03:34,630
so if you're considering joining our

02:03:33,469 --> 02:03:37,900
there are lots of

02:03:34,630 --> 02:03:39,699
it's first you get the network at the

02:03:37,900 --> 02:03:40,750
same time you're learning really cool

02:03:39,699 --> 02:03:43,120
things about tensorflow

02:03:40,750 --> 02:03:45,040
if you're confident enough to like you

02:03:43,120 --> 02:03:47,320
know create your own site project and

02:03:45,040 --> 02:03:51,280
you want to demo it in front of you know

02:03:47,320 --> 02:03:54,449
200 odd people for a given session you

02:03:51,280 --> 02:03:57,760
can if you just become a member

02:03:54,449 --> 02:04:01,690
additionally if you want something a bit

02:03:57,760 --> 02:04:05,050
more low level or low key you can join

02:04:01,690 --> 02:04:07,960
the dev space Singapore at the Google

02:04:05,050 --> 02:04:11,320
APEC headquarters so dev spaces well

02:04:07,960 --> 02:04:13,389
space for all the community events

02:04:11,320 --> 02:04:16,150
related developers and programming in

02:04:13,389 --> 02:04:19,210
general so almost monthly they have

02:04:16,150 --> 02:04:22,179
different talks about a wide range of

02:04:19,210 --> 02:04:25,199
topics some machine learning some web

02:04:22,179 --> 02:04:27,610
development some I guess app development

02:04:25,199 --> 02:04:29,800
so yeah all their machine learning

02:04:27,610 --> 02:04:34,659
intensive flow talks you can you can try

02:04:29,800 --> 02:04:37,000
attending them other than that if you do

02:04:34,659 --> 02:04:39,159
have any doubts or you want reach out if

02:04:37,000 --> 02:04:41,830
you have any questions about the talk or

02:04:39,159 --> 02:04:43,239
machine learning AI in general you can

02:04:41,830 --> 02:04:45,100
try connecting with me

02:04:43,239 --> 02:04:49,170
on any one of these platforms I'm active

02:04:45,100 --> 02:04:53,230
on github LinkedIn Twitter and

02:04:49,170 --> 02:04:54,880
medium.com when I share the slides URL

02:04:53,230 --> 02:04:57,760
you can actually click one of these

02:04:54,880 --> 02:05:02,050
images and it'll take you to one of the

02:04:57,760 --> 02:05:03,460
profiles well yeah other than that well

02:05:02,050 --> 02:05:06,250
thank you for tuning in thank you

02:05:03,460 --> 02:05:13,090
whoever showed up and like to the online

02:05:06,250 --> 02:05:15,310
viewers yeah any questions and if you

02:05:13,090 --> 02:05:17,739
want to access the slides showed it up

02:05:15,310 --> 02:05:20,500
in the front you can access it at this

02:05:17,739 --> 02:05:23,380
URL so if you want to take a picture or

02:05:20,500 --> 02:05:25,239
you want to come back later or you want

02:05:23,380 --> 02:05:30,730
to review the content especially the the

02:05:25,239 --> 02:05:35,590
kovat 19 data set task you can come back

02:05:30,730 --> 02:05:37,449
to this on well slide a slideshow at

02:05:35,590 --> 02:05:42,010
this URL FA

02:05:37,449 --> 02:05:43,900
- TF - 20/20 yeah other than that No

02:05:42,010 --> 02:05:46,300
thank you for coming it's kind of like

02:05:43,900 --> 02:05:48,460
late into the evening especially on a

02:05:46,300 --> 02:05:50,800
Friday hope you enjoyed

02:05:48,460 --> 02:05:54,040
there are some parts that did seem kind

02:05:50,800 --> 02:05:55,630
of dull because well you need to have

02:05:54,040 --> 02:05:58,300
kind of like a deep intuition of what's

02:05:55,630 --> 02:05:59,890
going on underneath like to fully

02:05:58,300 --> 02:06:02,980
understand what tensorflow is doing

02:05:59,890 --> 02:06:05,320
surface level which is why some parts

02:06:02,980 --> 02:06:08,620
may seem like you know what am I saying

02:06:05,320 --> 02:06:10,120
so yeah but the more you kind of dwelve

02:06:08,620 --> 02:06:12,580
into it tends to flow the more you do

02:06:10,120 --> 02:06:15,280
side projects or the more you explore or

02:06:12,580 --> 02:06:16,960
read and read its documentation the more

02:06:15,280 --> 02:06:21,760
you will kind of understand what's going

02:06:16,960 --> 02:06:24,040
on under the hood which kind of

02:06:21,760 --> 02:06:27,150
simplifies the entire thing so yeah

02:06:24,040 --> 02:06:27,150

YouTube URL: https://www.youtube.com/watch?v=8WvFn_GCSqM


