Title: Koalas: Easy Transition from Pandas to Spark - Ben Sadeghi
Publication date: 2020-03-26
Playlist: FOSSASIA Summit 2020 - Python
Description: 
	Pandas, the de-facto standard DataFrame implementation in Python, is very popular among data scientists, but it does not scale well to big data. It was designed for small data sets that a single machine could handle. On the other hand, Apache Spark has emerged as the de-facto standard for big data workloads. Today many data scientists use Pandas for coursework, pet projects, and small data tasks, but when they work with very large data sets, they either have to migrate to PySpark to leverage Spark or downsample their data so that they can use pandas.

Now with Koalas, an open-source implementation of the Pandas API on Apache Spark, data scientists can make the transition from a single machine to a distributed environment without needing to learn a new framework. In this talk, we'll go through the basics of Koalas, along with demos.

FOSSASIA Summit 2020 - Python

Speaker: Ben Sadeghi, Partner Solutions Architect Databricks
Captions: 
	00:00:00,000 --> 00:00:07,830
we talk about koalas which is basically

00:00:04,230 --> 00:00:13,400
an open-source project to expose the

00:00:07,830 --> 00:00:16,560
pandas API to execute code on spy

00:00:13,400 --> 00:00:23,789
prosperous quartermaster so a lot of

00:00:16,560 --> 00:00:25,590
terms just came here about pandas so can

00:00:23,789 --> 00:00:29,189
we actually give us a brief introduction

00:00:25,590 --> 00:00:32,099
on pandas and some of the issues that

00:00:29,189 --> 00:00:34,800
could be caused by it

00:00:32,099 --> 00:00:36,960
it is single threaded it is memory bound

00:00:34,800 --> 00:00:40,649
but nevertheless it's a very very

00:00:36,960 --> 00:00:44,520
popular framework for slicing and dicing

00:00:40,649 --> 00:00:46,950
your data in Python so often data

00:00:44,520 --> 00:00:48,210
scientists they might spend 3/4 of the

00:00:46,950 --> 00:00:51,840
time in pandas

00:00:48,210 --> 00:00:53,910
and then the other quarter of the time

00:00:51,840 --> 00:00:55,800
in scikit-learn to the actual sugar

00:00:53,910 --> 00:00:57,840
right so both of them we're going to

00:00:55,800 --> 00:01:04,049
begin just slicing and dicing for data

00:00:57,840 --> 00:01:06,540
structure and now koalas is an open

00:01:04,049 --> 00:01:09,540
source project initially developed by

00:01:06,540 --> 00:01:15,210
data breaks resource in April of last

00:01:09,540 --> 00:01:17,689
year to basically take the same API the

00:01:15,210 --> 00:01:22,049
paddlers provides but basically have

00:01:17,689 --> 00:01:26,369
haven't had that be a wrapper for the

00:01:22,049 --> 00:01:29,270
Apache spark ideas and spark as you may

00:01:26,369 --> 00:01:34,560
well know is the is the big data

00:01:29,270 --> 00:01:38,750
distributed in memory engine that that

00:01:34,560 --> 00:01:38,750
became so popular it's our anniversary

00:01:39,110 --> 00:01:48,270
so we're the goals because is really to

00:01:42,560 --> 00:01:51,299
to make pandas big data ready again this

00:01:48,270 --> 00:01:54,149
is the issues being the pandas right now

00:01:51,299 --> 00:01:56,130
the single threaded and memory balance

00:01:54,149 --> 00:01:59,810
right it's really just meant to be on

00:01:56,130 --> 00:02:03,930
single machine single single processor

00:01:59,810 --> 00:02:07,979
while the live had spark which leverages

00:02:03,930 --> 00:02:10,170
a cluster of machines is fully

00:02:07,979 --> 00:02:12,390
distributed in terms of

00:02:10,170 --> 00:02:14,400
compute and the memory so when we

00:02:12,390 --> 00:02:17,670
actually really data set into spark it

00:02:14,400 --> 00:02:20,610
goes ahead and distributes partitions

00:02:17,670 --> 00:02:22,560
that across all these these workers and

00:02:20,610 --> 00:02:25,800
from then on all the computation is done

00:02:22,560 --> 00:02:30,300
in a parallel fashion and this comes out

00:02:25,800 --> 00:02:32,640
of the box so right as I mentioned and

00:02:30,300 --> 00:02:36,180
the cost package was of the source in

00:02:32,640 --> 00:02:38,640
April of 2019 now we're seeing I think

00:02:36,180 --> 00:02:42,480
this is the slice without dates around

00:02:38,640 --> 00:02:50,420
8,000 daily downloads for this for this

00:02:42,480 --> 00:02:53,910
package it's available via policy and

00:02:50,420 --> 00:02:56,900
yeah architectural II it's really just a

00:02:53,910 --> 00:03:00,080
lean API layer sitting on top of

00:02:56,900 --> 00:03:05,069
sparking self right it's just that the

00:03:00,080 --> 00:03:08,750
notation and a black specifications have

00:03:05,069 --> 00:03:15,420
been adopted from patency that's right

00:03:08,750 --> 00:03:17,030
so a few design principles and one is to

00:03:15,420 --> 00:03:20,220
be my funny

00:03:17,030 --> 00:03:24,120
spark itself for example uses canvas

00:03:20,220 --> 00:03:28,500
over these characters Tim pythonic is to

00:03:24,120 --> 00:03:31,049
be famous right so of course quality is

00:03:28,500 --> 00:03:34,590
going to come just in case function I

00:03:31,049 --> 00:03:38,190
have this notation everything is known

00:03:34,590 --> 00:03:42,959
by friendly you can import export in

00:03:38,190 --> 00:03:46,859
ounce is very much integrated with the

00:03:42,959 --> 00:03:54,090
koalas and the documentation is in the

00:03:46,859 --> 00:03:56,459
PI data staff okay so really we're

00:03:54,090 --> 00:03:59,370
trying to have call as functions of

00:03:56,459 --> 00:04:02,069
handles functions be identical so that

00:03:59,370 --> 00:04:06,450
the same naming naming conventions same

00:04:02,069 --> 00:04:09,690
functionality and basically anything

00:04:06,450 --> 00:04:12,870
that's found to be distributable pair

00:04:09,690 --> 00:04:14,130
lives of all within the pandas API is

00:04:12,870 --> 00:04:20,550
taking over and

00:04:14,130 --> 00:04:25,850
in the koalas project okay and if you do

00:04:20,550 --> 00:04:28,800
notes here guardrails beyond safety so

00:04:25,850 --> 00:04:31,650
Thomas is actually the quality meant to

00:04:28,800 --> 00:04:35,460
be just performance right so if there

00:04:31,650 --> 00:04:39,000
any function that he processes which

00:04:35,460 --> 00:04:43,710
cannot be distributed you cannot be

00:04:39,000 --> 00:04:46,760
executed at scale or left alone so you

00:04:43,710 --> 00:04:49,440
can now be you can be rest assured that

00:04:46,760 --> 00:04:52,500
anything you do koalas is going

00:04:49,440 --> 00:04:56,040
paralyzed right and cast a scale to

00:04:52,500 --> 00:04:59,730
petabytes I'll give an example of a one

00:04:56,040 --> 00:05:03,060
group that's that's using koalas that is

00:04:59,730 --> 00:05:08,430
the pandas API process around a petabyte

00:05:03,060 --> 00:05:10,380
evening a few exceptions things that

00:05:08,430 --> 00:05:13,610
aren't very Liza balandin can be

00:05:10,380 --> 00:05:16,590
dangerous are these these that these two

00:05:13,610 --> 00:05:19,260
castings really taking your balls that

00:05:16,590 --> 00:05:22,320
frame and passing it over to a panda's

00:05:19,260 --> 00:05:24,870
data frame or not high right now these

00:05:22,320 --> 00:05:27,980
two actually wind up pulling everything

00:05:24,870 --> 00:05:30,900
from your workers into your driver and

00:05:27,980 --> 00:05:34,560
as potential there to kill your track

00:05:30,900 --> 00:05:37,740
record you get another member so just as

00:05:34,560 --> 00:05:43,490
with data frames are collected for you

00:05:37,740 --> 00:05:43,490
it's quite gurus careful don't do it

00:05:46,220 --> 00:05:50,919
so I think a few differences between

00:05:51,039 --> 00:06:01,759
high spark and this is it is you know

00:05:57,530 --> 00:06:03,979
it's comes everything at the box type

00:06:01,759 --> 00:06:06,710
system is basically based on numpy and

00:06:03,979 --> 00:06:10,610
of course it's very pythonic pi spark

00:06:06,710 --> 00:06:16,610
itself so this is the spark api for

00:06:10,610 --> 00:06:19,069
actually the pythonic a part has kind of

00:06:16,610 --> 00:06:23,750
has some pretty different compositions i

00:06:19,069 --> 00:06:29,139
some abstract to top top it's type

00:06:23,750 --> 00:06:31,180
system is from NC sequel we are

00:06:29,139 --> 00:06:33,469
underneath it's actually running

00:06:31,180 --> 00:06:39,259
Scarlett data frames it guys

00:06:33,469 --> 00:06:41,569
it's calling those those are clinics few

00:06:39,259 --> 00:06:47,060
differences between a panda's data frame

00:06:41,569 --> 00:06:50,479
I despite that frame so one difference

00:06:47,060 --> 00:06:52,729
on the panda side data fits are somewhat

00:06:50,479 --> 00:06:56,120
mutable on a spark side country

00:06:52,729 --> 00:07:02,599
immutable and the rest actually is just

00:06:56,120 --> 00:07:06,199
a just a demonstration of writing or

00:07:02,599 --> 00:07:08,300
performing the same tasks one thing

00:07:06,199 --> 00:07:10,550
that's really that's really popular is

00:07:08,300 --> 00:07:13,460
this value accountants we see here grew

00:07:10,550 --> 00:07:16,460
by then count it's such a common task

00:07:13,460 --> 00:07:24,440
that has has this value counts function

00:07:16,460 --> 00:07:29,330
at a point in ice bar which is fine it's

00:07:24,440 --> 00:07:31,759
fine but the trouble is if you want to

00:07:29,330 --> 00:07:35,029
if you already have a pipeline or in

00:07:31,759 --> 00:07:37,849
written pandas and your data volume has

00:07:35,029 --> 00:07:40,789
grown to the point that pandas is no

00:07:37,849 --> 00:07:45,289
longer the appropriate solution you need

00:07:40,789 --> 00:07:47,960
more spark and prior to koalas you would

00:07:45,289 --> 00:07:51,520
have had to basically do rewrite you

00:07:47,960 --> 00:07:54,009
will have to take this little snippet

00:07:51,520 --> 00:07:56,199
expanded under that right I do this line

00:07:54,009 --> 00:08:00,610
by line so it's just a very

00:07:56,199 --> 00:08:03,370
time-consuming task right so give me an

00:08:00,610 --> 00:08:05,229
example another example here this is a

00:08:03,370 --> 00:08:07,509
very very simple one we're going to end

00:08:05,229 --> 00:08:10,240
up on the side on the left we're an

00:08:07,509 --> 00:08:13,690
import we're gonna read some data a CSV

00:08:10,240 --> 00:08:17,020
file right we can change its column

00:08:13,690 --> 00:08:19,990
names to X Y and set one and we can

00:08:17,020 --> 00:08:23,860
create a new column called X 2 which is

00:08:19,990 --> 00:08:28,930
a product of the X column by itself an

00:08:23,860 --> 00:08:32,320
expert on the PI smart side you have to

00:08:28,930 --> 00:08:33,250
do a smart beat add a few options then

00:08:32,320 --> 00:08:35,919
to CSV

00:08:33,250 --> 00:08:39,010
great you want to change things and it

00:08:35,919 --> 00:08:40,779
is to data frame XY said you want to add

00:08:39,010 --> 00:08:45,520
a new column you have to do this with

00:08:40,779 --> 00:08:53,130
all them things too okay now now we

00:08:45,520 --> 00:08:58,180
introduce koalas this is now the same

00:08:53,130 --> 00:08:59,410
thing that we just did in high spot so

00:08:58,180 --> 00:09:01,930
everything that you see here on the

00:08:59,410 --> 00:09:04,890
right actually gets mapped to the

00:09:01,930 --> 00:09:07,209
previous bike sparklines

00:09:04,890 --> 00:09:09,610
as you can see basically the only

00:09:07,209 --> 00:09:14,190
difference here is that important on a

00:09:09,610 --> 00:09:17,890
statement at the very top so in fact if

00:09:14,190 --> 00:09:20,470
the project continues to advance at the

00:09:17,890 --> 00:09:25,209
pace that it is you will soon be able to

00:09:20,470 --> 00:09:28,000
just take your pandas script at the very

00:09:25,209 --> 00:09:34,810
top instead of saying import pandas as

00:09:28,000 --> 00:09:36,820
PD you would say import koalas as PD so

00:09:34,810 --> 00:09:39,070
and then you don't have to change a PD

00:09:36,820 --> 00:09:40,630
that one that's at you you literally

00:09:39,070 --> 00:09:42,610
change the top line and suddenly you

00:09:40,630 --> 00:09:43,620
want me to cure workload from a

00:09:42,610 --> 00:09:47,790
single-threaded

00:09:43,620 --> 00:09:50,790
memory bound framework or fully digital

00:09:47,790 --> 00:09:50,790
okay

00:09:51,069 --> 00:09:59,419
all right demo time it's the fun part so

00:09:55,249 --> 00:10:00,439
I'm a spark cluster going live on Azure

00:09:59,419 --> 00:10:03,589
data bricks

00:10:00,439 --> 00:10:06,169
I made a here we go here's my little

00:10:03,589 --> 00:10:06,980
cluster it's actually very little as a

00:10:06,169 --> 00:10:13,399
single worker

00:10:06,980 --> 00:10:15,799
I have koalas and salt right I just

00:10:13,399 --> 00:10:23,239
actually died pulled in from me my story

00:10:15,799 --> 00:10:24,679
okay and let me jump to my notebook here

00:10:23,239 --> 00:10:27,199
we go so this is the data bricks

00:10:24,679 --> 00:10:29,809
notebook very similar to Jim Jupiter for

00:10:27,199 --> 00:10:30,879
those familiar okay so I'm going to put

00:10:29,809 --> 00:10:38,629
a few things

00:10:30,879 --> 00:10:42,559
pandas numpy and koalas itself okay good

00:10:38,629 --> 00:10:46,189
good let's let's do just let's make make

00:10:42,559 --> 00:10:50,929
a koala series go through a bunch of

00:10:46,189 --> 00:10:53,720
types and castings of one type of data

00:10:50,929 --> 00:10:57,169
frame to another so this is a series

00:10:53,720 --> 00:11:08,769
right so just as pandas have series

00:10:57,169 --> 00:11:11,119
koalas I see as well there's a second

00:11:08,769 --> 00:11:16,339
next of that series that sort of that

00:11:11,119 --> 00:11:18,619
the basic element soon after here we go

00:11:16,339 --> 00:11:20,389
that's our series now I'm going to do

00:11:18,619 --> 00:11:23,389
something different I'm going to create

00:11:20,389 --> 00:11:26,779
like koalas data frame but this is how

00:11:23,389 --> 00:11:30,470
you would do it in pandas as well and if

00:11:26,779 --> 00:11:34,959
you see what you do do that actually

00:11:30,470 --> 00:11:40,399
tell us so this is a transport job

00:11:34,959 --> 00:11:43,609
here's the data great now PD I'm gonna

00:11:40,399 --> 00:11:47,970
create I'm going to use pandas here to

00:11:43,609 --> 00:11:53,260
create the arranged by

00:11:47,970 --> 00:11:55,300
and save us later and I'm gonna also

00:11:53,260 --> 00:11:56,110
going to use pandas to create a panda's

00:11:55,300 --> 00:11:59,829
data frame

00:11:56,110 --> 00:12:07,480
so that's PDF okay that's panda's data

00:11:59,829 --> 00:12:11,649
frame now do a cast from so we're doing

00:12:07,480 --> 00:12:13,180
koalas from a pan from pandas this kind

00:12:11,649 --> 00:12:16,300
of data frame that we just just built

00:12:13,180 --> 00:12:17,769
and basically doing cast from how does

00:12:16,300 --> 00:12:23,380
that work frame to call us out of frame

00:12:17,769 --> 00:12:25,540
done if I wanted to display it it'll

00:12:23,380 --> 00:12:29,019
actually start a spark jump right so

00:12:25,540 --> 00:12:32,350
this part is the spark job the Panda

00:12:29,019 --> 00:12:39,339
side did not so this actually ran on the

00:12:32,350 --> 00:12:42,040
driver of the cluster this ran on the

00:12:39,339 --> 00:12:44,440
workers this one is fully distributed

00:12:42,040 --> 00:12:47,019
the first one was for singles very

00:12:44,440 --> 00:12:50,709
automatically but same things Amazon's

00:12:47,019 --> 00:12:53,709
right I could do something else too I

00:12:50,709 --> 00:12:57,130
can take the panda's data frame and cast

00:12:53,709 --> 00:12:58,720
it over to spark one yeah so you can

00:12:57,130 --> 00:13:01,089
actually we can go back and forth

00:12:58,720 --> 00:13:03,820
between spark and as well as their

00:13:01,089 --> 00:13:05,019
friends interchangeable ending on

00:13:03,820 --> 00:13:08,079
whichever how much you're trying to do

00:13:05,019 --> 00:13:10,630
okay now I'm going to spark that a

00:13:08,079 --> 00:13:12,310
friend to call as one again this can be

00:13:10,630 --> 00:13:17,699
full distributed you can see a spark job

00:13:12,310 --> 00:13:17,699
come up okay let's do some fun stuff

00:13:17,880 --> 00:13:25,750
types okay so we have that part that

00:13:22,930 --> 00:13:29,529
causes that frame when you head on it's

00:13:25,750 --> 00:13:32,290
now I'm going to run a bunch of standard

00:13:29,529 --> 00:13:34,300
pandas calls like hand you see them so

00:13:32,290 --> 00:13:39,130
those those familiar with pandas refine

00:13:34,300 --> 00:13:39,810
all these calls very very common use

00:13:39,130 --> 00:13:42,210
them everyday

00:13:39,810 --> 00:13:45,510
you know take a look at the index of

00:13:42,210 --> 00:13:49,529
this little dummy that data set that we

00:13:45,510 --> 00:13:52,500
put together columns and I'm going to

00:13:49,529 --> 00:13:55,890
cast it to an umpire this by the way is

00:13:52,500 --> 00:13:58,160
dangerous this is taking data from the

00:13:55,890 --> 00:14:01,350
cluster and bring it to the driver

00:13:58,160 --> 00:14:05,660
careful so I just want a few exceptions

00:14:01,350 --> 00:14:09,089
and exceptions to watch out for right

00:14:05,660 --> 00:14:11,310
so kindness has this function called

00:14:09,089 --> 00:14:13,980
describe it's basically a statistical

00:14:11,310 --> 00:14:16,440
summary of your data center call us

00:14:13,980 --> 00:14:19,890
those to accept that now it does it on a

00:14:16,440 --> 00:14:22,380
spark job right now we have our count me

00:14:19,890 --> 00:14:27,060
and Max standard deviations to qualify

00:14:22,380 --> 00:14:29,070
us right we can imagine - no this one

00:14:27,060 --> 00:14:34,140
liner on terabytes of data I gave you

00:14:29,070 --> 00:14:42,740
summaries transpose again these are all

00:14:34,140 --> 00:14:42,740
pandas calls good quicksort

00:14:46,330 --> 00:14:56,510
sorting by specific value I will sort by

00:14:50,420 --> 00:15:01,760
Intex 5 do the missing missing data so

00:14:56,510 --> 00:15:05,420
I'm gonna take my hand as data frame add

00:15:01,760 --> 00:15:09,110
a new column e to it plus a bunch of

00:15:05,420 --> 00:15:10,670
yeah so add some dates use the index of

00:15:09,110 --> 00:15:12,050
our dates that we generally say in the

00:15:10,670 --> 00:15:14,900
go

00:15:12,050 --> 00:15:17,300
add new column right now that column E

00:15:14,900 --> 00:15:20,960
is a bunch of Nance Knowles

00:15:17,300 --> 00:15:26,300
and for a couple of these entries we're

00:15:20,960 --> 00:15:34,670
gonna add the value of want to it if I

00:15:26,300 --> 00:15:38,120
catch this guy 1 okay there we go

00:15:34,670 --> 00:15:43,430
so here's a new panda's data frame has a

00:15:38,120 --> 00:15:47,390
bunch of dates of the index 5 : e 1 has

00:15:43,430 --> 00:15:52,070
now two missing values I'm going to cast

00:15:47,390 --> 00:15:54,140
that too of course a koalas data frame

00:15:52,070 --> 00:15:58,490
now the smart job still the same thing

00:15:54,140 --> 00:16:01,730
which is running on spark and then you

00:15:58,490 --> 00:16:04,940
still get to use all the standard Python

00:16:01,730 --> 00:16:10,910
the pandas tools for handling missing

00:16:04,940 --> 00:16:13,490
values like drop a name and so basically

00:16:10,910 --> 00:16:19,760
drop my name drops any road which has an

00:16:13,490 --> 00:16:22,700
MA in it for this case it's not filling

00:16:19,760 --> 00:16:26,000
missing data we take that the data frame

00:16:22,700 --> 00:16:33,410
and anytime season and n a.m. it's gonna

00:16:26,000 --> 00:16:35,900
stamp it with a 5/5 great statistical

00:16:33,410 --> 00:16:38,290
work you already saw saw some of this

00:16:35,900 --> 00:16:38,290
earlier

00:16:39,570 --> 00:16:50,100
that's our configurations so in the back

00:16:44,000 --> 00:16:53,550
you can leverage arrow for serializing

00:16:50,100 --> 00:16:55,920
and deserializing your I thought boss so

00:16:53,550 --> 00:16:59,279
what I'm going to do is actually do two

00:16:55,920 --> 00:17:03,660
things first I'm going to run a very

00:16:59,279 --> 00:17:10,740
simple job here with arrow enabled I see

00:17:03,660 --> 00:17:15,299
I'm gonna create a range of 300,000 so 0

00:17:10,740 --> 00:17:17,970
to 1000 is 1 we're not cast that to

00:17:15,299 --> 00:17:21,329
Kaiba's I'm gonna run the exact same I'm

00:17:17,970 --> 00:17:25,380
what time this all yeah and we write the

00:17:21,329 --> 00:17:30,230
same thing with arrow disabled I will

00:17:25,380 --> 00:17:32,910
see the fraternity okay there it is

00:17:30,230 --> 00:17:35,360
creating this is actually all that

00:17:32,910 --> 00:17:40,830
worked on as far as it took around

00:17:35,360 --> 00:17:45,030
roughly one third of a second here we go

00:17:40,830 --> 00:17:53,179
with arrow disabled I step back for a

00:17:45,030 --> 00:17:57,860
second and then we took off with you so

00:17:53,179 --> 00:18:03,080
one point one so maybe four times going

00:17:57,860 --> 00:18:03,080
enable arrow again

00:18:11,109 --> 00:18:21,379
okay groupings so the data of the common

00:18:17,119 --> 00:18:24,889
task within within pandas slicing dicing

00:18:21,379 --> 00:18:28,549
grouping right again let us create a

00:18:24,889 --> 00:18:32,149
koala stamp frame bunch of random

00:18:28,549 --> 00:18:38,090
numbers in there too alright so four

00:18:32,149 --> 00:18:41,720
columns here is when two parts B is 1 2

00:18:38,090 --> 00:18:45,080
3 is string.c in D are normally

00:18:41,720 --> 00:18:48,649
distributed random numbers okay screwed

00:18:45,080 --> 00:18:53,769
by a it's um yeah this again everything

00:18:48,649 --> 00:18:53,769
now this is all hand as API code right

00:18:55,059 --> 00:19:04,970
by two columns get there some good good

00:19:02,029 --> 00:19:07,220
good very common tasks within within

00:19:04,970 --> 00:19:12,279
pandas floating as well

00:19:07,220 --> 00:19:18,529
I'm gonna import google map but lip yeah

00:19:12,279 --> 00:19:20,690
thank you John Arthur and creating new

00:19:18,529 --> 00:19:24,440
datasets this was going to be a panda's

00:19:20,690 --> 00:19:29,720
data frame and four columns of random

00:19:24,440 --> 00:19:34,489
numbers thousand rows each okay I'm

00:19:29,720 --> 00:19:37,850
gonna cast that to a koalas that frame

00:19:34,489 --> 00:19:42,279
and actually print this out if you want

00:19:37,850 --> 00:19:42,279
just do that okay

00:19:46,950 --> 00:19:59,970
so just a bunch of normally distributed

00:19:49,800 --> 00:20:07,410
at numbers thousand rows great maybe it

00:19:59,970 --> 00:20:11,310
was a bad nevermind let's take the

00:20:07,410 --> 00:20:13,590
cumulative sum great and I want to prop

00:20:11,310 --> 00:20:17,610
plop plop fizz commit yourself using

00:20:13,590 --> 00:20:20,580
that partly there we go

00:20:17,610 --> 00:20:26,040
again everything here that we're doing

00:20:20,580 --> 00:20:27,570
is in the back any smart okay I think

00:20:26,040 --> 00:20:31,200
you ever sandwich stuff you know reading

00:20:27,570 --> 00:20:33,920
writing so I'm gonna take that cause

00:20:31,200 --> 00:20:39,720
that afraid that that we just generated

00:20:33,920 --> 00:20:45,900
oh we're doing we take the CSV file I

00:20:39,720 --> 00:20:48,690
have lying around walls that a friend to

00:20:45,900 --> 00:20:51,570
a CSV I'm going to take that and read it

00:20:48,690 --> 00:20:59,520
back so it's a write a CSV file and then

00:20:51,570 --> 00:21:04,610
read the CSV file back again other other

00:20:59,520 --> 00:21:04,610
data formats supported here are archaic

00:21:09,090 --> 00:21:21,670
Delta is also supported format and even

00:21:14,470 --> 00:21:25,210
horse so regardless of what your what

00:21:21,670 --> 00:21:28,180
format your data has been written in we

00:21:25,210 --> 00:21:37,990
now should do all your audio activity

00:21:28,180 --> 00:21:44,650
with ok and so if you like it's a great

00:21:37,990 --> 00:21:47,830
post on data explorer google it hours to

00:21:44,650 --> 00:21:49,870
minutes always well one that works

00:21:47,830 --> 00:21:54,600
customer data processing around a

00:21:49,870 --> 00:21:58,090
petabyte of data using the pandas API

00:21:54,600 --> 00:22:00,790
it's pretty wild and of course if you

00:21:58,090 --> 00:22:04,330
want to contribute please give up

00:22:00,790 --> 00:22:09,610
columns like Arabic slash Wallace is the

00:22:04,330 --> 00:22:13,750
repo thank you very much I'm Ben Cellini

00:22:09,610 --> 00:22:17,980
find me on LinkedIn to Twitter add in

00:22:13,750 --> 00:22:22,780
github with the pen CDE handle and I

00:22:17,980 --> 00:22:25,600
have all the slides and demos posted on

00:22:22,780 --> 00:22:27,850
github but again give up side pencil

00:22:25,600 --> 00:22:35,830
again you'll see a repo called Faust

00:22:27,850 --> 00:22:50,800
Asia 2020 thank you very much does I

00:22:35,830 --> 00:22:53,110
want to make question and that's that's

00:22:50,800 --> 00:22:55,120
also really cool yes yes so right now

00:22:53,110 --> 00:23:00,580
there might be a few methods that aren't

00:22:55,120 --> 00:23:03,120
having yet been mapped over so yeah but

00:23:00,580 --> 00:23:05,860
all the really popular ones common ones

00:23:03,120 --> 00:23:08,200
have in them so it's a good chance that

00:23:05,860 --> 00:23:11,310
you can just do change your import line

00:23:08,200 --> 00:23:11,310
and let the restaurant

00:23:11,720 --> 00:23:17,690
so this is a question about like us

00:23:17,810 --> 00:23:23,790
present first it's quite recently pandas

00:23:20,850 --> 00:23:27,090
actually won 2-1

00:23:23,790 --> 00:23:30,870
and there were actually a lot of changes

00:23:27,090 --> 00:23:34,920
to the API some countries that used to

00:23:30,870 --> 00:23:38,490
work in 0 to 5 stopped working away that

00:23:34,920 --> 00:23:42,990
people thought in life so how does the

00:23:38,490 --> 00:23:44,670
hola hollister chorus per day today we

00:23:42,990 --> 00:23:47,970
are we're sticking with the following

00:23:44,670 --> 00:24:20,160
one version 1.0 forward so we're not

00:23:47,970 --> 00:24:50,510
actually put two versions right so that

00:24:20,160 --> 00:24:50,510
was the go-ahead for us to happen yes

00:24:52,940 --> 00:24:59,820
yes so we can do something we're talking

00:24:57,090 --> 00:25:02,070
about instruments or something yeah you

00:24:59,820 --> 00:25:04,210
talked about these two polyester a

00:25:02,070 --> 00:25:08,649
friend

00:25:04,210 --> 00:25:13,019
so these only so you often do these just

00:25:08,649 --> 00:25:13,019
to just see where your dad looks like

00:25:13,200 --> 00:25:18,220
pandas world in an umpire world so we

00:25:15,999 --> 00:25:21,399
can do is before doing the called

00:25:18,220 --> 00:25:27,850
Pandavas or phenom by is downsampled

00:25:21,399 --> 00:25:29,889
again use a call for sample say 1% of

00:25:27,850 --> 00:25:35,799
all the data distributed across West

00:25:29,889 --> 00:25:38,249
Malaysia we know more visit our bank we

00:25:35,799 --> 00:25:42,190
don't want to run our direct enforce any

00:25:38,249 --> 00:25:44,830
down sampling by default rates that's

00:25:42,190 --> 00:25:47,379
that's and that's why these fees to

00:25:44,830 --> 00:25:51,149
really just leads to another in a

00:25:47,379 --> 00:25:55,779
dangerous as a thermal bomb collect

00:25:51,149 --> 00:25:57,909
dangerous with any of the sparkly but

00:25:55,779 --> 00:26:00,519
honestly I really find myself using

00:25:57,909 --> 00:26:31,720
these even even during darkness

00:26:00,519 --> 00:26:34,799
I just demo them to social challenges no

00:26:31,720 --> 00:26:34,799

YouTube URL: https://www.youtube.com/watch?v=n5kQTYJBQrQ


