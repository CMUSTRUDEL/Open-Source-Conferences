Title: Exploring Augmented Reality for Mobile by Srikar Mutnuri
Publication date: 2019-03-23
Playlist: FOSSASIA Summit 2019 - Mobile Technologies
Description: 
	15 March 2019 17:00, Training Room 2-2

The talk would focus on the basics of Augmented Reality and the various aspects which make it possible, in the mobile context. An exploration of some current problems being faced would be done, and links to Machine Learning (or AI in general) shall be drawn to potentially eliminate these issues.

We'll also be seeing how various libraries (ARCore/ARKit) are handling the key features of AR.

Outline of the talk:

Vision
Augmented Reality (AR)
Vision in AR
Current AR libraries and some problems
Combine with Artificial Intelligence
Conclusion
Captions: 
	00:00:00,000 --> 00:00:05,149
to be going evolve of material memories

00:00:10,020 --> 00:00:13,880
he has two meters on the experience

00:00:13,920 --> 00:00:16,990
[Music]

00:00:17,890 --> 00:00:26,050
i hi everyone thanks for coming here so

00:00:22,860 --> 00:00:28,210
so I'm seeker and I'm currently as

00:00:26,050 --> 00:00:31,630
working as an XR developer in T sales

00:00:28,210 --> 00:00:34,210
gen9 and I am pretty much a beginner in

00:00:31,630 --> 00:00:36,280
this field so and I wanted to just share

00:00:34,210 --> 00:00:40,420
whatever I found interesting in AR and

00:00:36,280 --> 00:00:42,010
you know talk about it and as you might

00:00:40,420 --> 00:00:44,650
have guessed from the title I'll be

00:00:42,010 --> 00:00:46,960
speaking about augmented reality and

00:00:44,650 --> 00:00:48,490
what goes on behind the scenes and

00:00:46,960 --> 00:00:50,830
augmented reality and highlight some

00:00:48,490 --> 00:00:53,290
possible issues and you know you try to

00:00:50,830 --> 00:00:58,060
at least show some solutions while I've

00:00:53,290 --> 00:00:59,950
not actually made any demo to showcase

00:00:58,060 --> 00:01:01,990
right now at least I'm hoping to do so

00:00:59,950 --> 00:01:04,540
in the future and come back and share it

00:01:01,990 --> 00:01:06,880
so it's just putting things out there so

00:01:04,540 --> 00:01:09,009
this is somewhat an outline of my talk

00:01:06,880 --> 00:01:10,390
so I'll be talking about some vision and

00:01:09,009 --> 00:01:13,270
then AR and then how all these things

00:01:10,390 --> 00:01:15,060
come in and then put in some ml AI

00:01:13,270 --> 00:01:18,429
etcetera and then give some conclusion

00:01:15,060 --> 00:01:21,909
so we'll first talk about vision so

00:01:18,429 --> 00:01:24,189
wisdom in this scenario is a basically

00:01:21,909 --> 00:01:27,539
computer vision which I think has been a

00:01:24,189 --> 00:01:31,179
very active field for quite some time so

00:01:27,539 --> 00:01:34,210
and in my view I think you can summarize

00:01:31,179 --> 00:01:36,399
the entire CV process into three

00:01:34,210 --> 00:01:37,960
these three steps so you acquire an

00:01:36,399 --> 00:01:40,060
image from a sensor and then you process

00:01:37,960 --> 00:01:43,359
it and then you analyze it to get some

00:01:40,060 --> 00:01:45,909
details out of it to see an example this

00:01:43,359 --> 00:01:48,369
is a example license plate detection so

00:01:45,909 --> 00:01:50,950
you are applying your history canny edge

00:01:48,369 --> 00:01:53,200
detection process to get that edge

00:01:50,950 --> 00:01:56,340
detected and then you drop the

00:01:53,200 --> 00:01:59,109
rectangular part of the image to get the

00:01:56,340 --> 00:02:00,729
necessary thing and then you try to

00:01:59,109 --> 00:02:03,609
detect your characters in this and then

00:02:00,729 --> 00:02:07,090
you analyze it to perform OCR and then

00:02:03,609 --> 00:02:09,880
do that stuff all right so I think CV

00:02:07,090 --> 00:02:12,700
forms a foundation of like whatever we

00:02:09,880 --> 00:02:15,459
do in physical reality so right now if I

00:02:12,700 --> 00:02:18,940
give a computer an image it's mostly

00:02:15,459 --> 00:02:21,299
with the real scenarios in mind so

00:02:18,940 --> 00:02:23,680
taking this one step further is

00:02:21,299 --> 00:02:25,750
augmented reality as you can see this is

00:02:23,680 --> 00:02:27,760
a spectrum which is where pretty famous

00:02:25,750 --> 00:02:30,819
it's been proposed by Malcolm

00:02:27,760 --> 00:02:33,640
so augmented reality is just a layer on

00:02:30,819 --> 00:02:35,250
top of physical reality and all the way

00:02:33,640 --> 00:02:38,319
to the other side is virtual reality

00:02:35,250 --> 00:02:40,690
where you take your user to a entirely

00:02:38,319 --> 00:02:45,640
new world and then you it will be fully

00:02:40,690 --> 00:02:48,069
virtual right so and now what is

00:02:45,640 --> 00:02:49,420
augmented reality if you do a quick

00:02:48,069 --> 00:02:50,980
Google search you will be getting this

00:02:49,420 --> 00:02:52,959
as a result I think this is one of the

00:02:50,980 --> 00:02:55,750
good definitions which I like so you're

00:02:52,959 --> 00:02:57,940
just adding a virtual content to the

00:02:55,750 --> 00:02:59,319
real world and then you are giving a

00:02:57,940 --> 00:03:01,569
composite view of it so you are

00:02:59,319 --> 00:03:02,920
augmenting the users reality in some

00:03:01,569 --> 00:03:05,590
sense all right

00:03:02,920 --> 00:03:08,709
so the defining characteristics of an

00:03:05,590 --> 00:03:10,540
ideal AR scenario will be these so you

00:03:08,709 --> 00:03:13,000
are blending the real with the imaginary

00:03:10,540 --> 00:03:15,310
so in this case I mean the famous famous

00:03:13,000 --> 00:03:17,349
game probably could be Pokemon go so

00:03:15,310 --> 00:03:19,239
you're seeing the pic character Pikachu

00:03:17,349 --> 00:03:21,609
and your real world somewhere and then

00:03:19,239 --> 00:03:23,319
you are trying to interact with it by

00:03:21,609 --> 00:03:24,910
this is the second one and the third one

00:03:23,319 --> 00:03:27,220
is that the character which you're

00:03:24,910 --> 00:03:30,280
positioning will always have to exhibit

00:03:27,220 --> 00:03:32,019
some sort of predictable behavior so if

00:03:30,280 --> 00:03:33,310
I am placing for example a chair in some

00:03:32,019 --> 00:03:35,349
corner of the room it doesn't mean that

00:03:33,310 --> 00:03:36,940
after a while the chair starts floating

00:03:35,349 --> 00:03:37,599
around that's not a predictable behavior

00:03:36,940 --> 00:03:41,470
to it

00:03:37,599 --> 00:03:43,510
so that should not happen ideally but

00:03:41,470 --> 00:03:46,150
because the real world is a lot of

00:03:43,510 --> 00:03:49,569
chaotic you have all these things

00:03:46,150 --> 00:03:51,790
changing at I don't under - bill levels

00:03:49,569 --> 00:03:55,060
so the user could decide to move the

00:03:51,790 --> 00:03:56,590
camera back or it could just just some

00:03:55,060 --> 00:03:59,260
blur could happen because lack of

00:03:56,590 --> 00:04:00,700
focusing or some image could come in or

00:03:59,260 --> 00:04:02,560
the lights could just turn off or

00:04:00,700 --> 00:04:05,560
something like that but all these things

00:04:02,560 --> 00:04:07,650
should be handled effectively by your

00:04:05,560 --> 00:04:13,959
application and this is where I think

00:04:07,650 --> 00:04:17,079
computer vision comes in and summarizing

00:04:13,959 --> 00:04:19,989
AR I think this is what these four steps

00:04:17,079 --> 00:04:23,440
could be basically some sort of an

00:04:19,989 --> 00:04:26,590
algorithm to say what AR does internally

00:04:23,440 --> 00:04:28,090
so while some condition is true which is

00:04:26,590 --> 00:04:30,280
which could be optional depending on

00:04:28,090 --> 00:04:32,199
your use case so I have to update the

00:04:30,280 --> 00:04:34,419
tracking data which is the position of

00:04:32,199 --> 00:04:36,460
the user the rotation of the device

00:04:34,419 --> 00:04:37,840
etcetera and then you update the

00:04:36,460 --> 00:04:39,580
environmental data so the lighting

00:04:37,840 --> 00:04:41,530
conditions the scale or something

00:04:39,580 --> 00:04:44,349
like that and then check if there has

00:04:41,530 --> 00:04:46,990
been any previous updates which has

00:04:44,349 --> 00:04:49,449
happened so from the previous data if

00:04:46,990 --> 00:04:51,460
there is any change happening so you

00:04:49,449 --> 00:04:53,710
have to understand okay so there is a

00:04:51,460 --> 00:04:55,210
change so I'll go to the fourth step and

00:04:53,710 --> 00:04:57,370
then I'll update the already placed

00:04:55,210 --> 00:05:00,699
virtual objects position based on

00:04:57,370 --> 00:05:03,669
whatever I got yeah

00:05:00,699 --> 00:05:05,500
so the first two steps are normally

00:05:03,669 --> 00:05:08,740
simultaneous they're called a slam

00:05:05,500 --> 00:05:10,960
Gautam's which most AR libraries use so

00:05:08,740 --> 00:05:15,990
so we'll be looking at them together for

00:05:10,960 --> 00:05:19,030
the rest of the talks so because our

00:05:15,990 --> 00:05:21,190
user and adornment is not predictable we

00:05:19,030 --> 00:05:23,949
need to constantly keep a track of the

00:05:21,190 --> 00:05:26,800
position of the user's device and at all

00:05:23,949 --> 00:05:28,479
times so this guy happens why a

00:05:26,800 --> 00:05:31,210
positional tracking and rotational

00:05:28,479 --> 00:05:34,569
tracking and it is known as pose

00:05:31,210 --> 00:05:37,419
estimation in any are at least right so

00:05:34,569 --> 00:05:39,759
I accept and I accept in some cases the

00:05:37,419 --> 00:05:41,889
the device and the virtual object you

00:05:39,759 --> 00:05:43,930
are placing needs to have some some form

00:05:41,889 --> 00:05:46,029
of a common coordinate system to speak

00:05:43,930 --> 00:05:50,500
with each other or at least some common

00:05:46,029 --> 00:05:53,169
language if not the thing all right and

00:05:50,500 --> 00:05:55,629
normally you use sensors to do this so I

00:05:53,169 --> 00:05:59,169
could use camera accelerometer gyroscope

00:05:55,629 --> 00:06:05,169
and etcetera to inform my decisions in

00:05:59,169 --> 00:06:07,479
this region and so before I begin my

00:06:05,169 --> 00:06:09,610
experience all I need is some difference

00:06:07,479 --> 00:06:11,139
points right so I need to have some

00:06:09,610 --> 00:06:13,419
difference to base all my experience

00:06:11,139 --> 00:06:15,370
about so I scan my room and then I know

00:06:13,419 --> 00:06:17,830
ok so there are some fixed points for me

00:06:15,370 --> 00:06:20,379
to identify this is a rotational thing

00:06:17,830 --> 00:06:22,180
and then I turn this side and so they

00:06:20,379 --> 00:06:23,889
should see some fixed point there to

00:06:22,180 --> 00:06:26,319
identify that there is there are some

00:06:23,889 --> 00:06:27,849
rotation happen so for this you use

00:06:26,319 --> 00:06:31,810
something called as key points which are

00:06:27,849 --> 00:06:34,870
quite some prom of distinctive images in

00:06:31,810 --> 00:06:37,389
an image so listing two points in an

00:06:34,870 --> 00:06:39,729
image I'm sorry so they help us to keep

00:06:37,389 --> 00:06:41,949
track of what things are constant etc

00:06:39,729 --> 00:06:44,319
and they're called as features in this

00:06:41,949 --> 00:06:45,819
environment you can also call them as

00:06:44,319 --> 00:06:49,690
stackable you store them in a database

00:06:45,819 --> 00:06:50,910
and then you understand and use it for

00:06:49,690 --> 00:06:53,490
further references

00:06:50,910 --> 00:06:55,500
and all the tripe features should have

00:06:53,490 --> 00:06:57,390
the following property so they should be

00:06:55,500 --> 00:07:00,540
reliable and they should be invariant to

00:06:57,390 --> 00:07:02,910
any sort of movements so reliable in the

00:07:00,540 --> 00:07:04,980
sense that I have suppose if I have a

00:07:02,910 --> 00:07:06,420
Peck as a feature point it is not

00:07:04,980 --> 00:07:10,860
reliable because it'll keep moving

00:07:06,420 --> 00:07:14,070
around and my experience will go so you

00:07:10,860 --> 00:07:16,020
sum all this happen you use some slam

00:07:14,070 --> 00:07:17,820
method as I mentioned earlier so you

00:07:16,020 --> 00:07:20,940
have sift and serve which are quite

00:07:17,820 --> 00:07:24,660
popular but comparatively speaking brisk

00:07:20,940 --> 00:07:26,520
I think is a bit faster so and it is a

00:07:24,660 --> 00:07:31,980
derivative of something called as a fast

00:07:26,520 --> 00:07:34,880
algorithm and for any algorithm you need

00:07:31,980 --> 00:07:37,230
two points to work so it should be

00:07:34,880 --> 00:07:39,090
detecting sufficient key points to

00:07:37,230 --> 00:07:42,180
understand you're an adornment and it

00:07:39,090 --> 00:07:44,550
should describe the key points and give

00:07:42,180 --> 00:07:46,740
them some form of unique fingerprints so

00:07:44,550 --> 00:07:48,180
it should be like I each of the key

00:07:46,740 --> 00:07:51,840
points even if they are in the billions

00:07:48,180 --> 00:07:53,910
of number so each of them should have a

00:07:51,840 --> 00:07:55,350
unique fingerprint to be for the app to

00:07:53,910 --> 00:07:58,020
be able to differentiate one from the

00:07:55,350 --> 00:08:00,240
other and these key points are often

00:07:58,020 --> 00:08:01,980
called a spatial anchor so most more

00:08:00,240 --> 00:08:04,440
often than not the developers when

00:08:01,980 --> 00:08:06,690
you're working on it we use it to define

00:08:04,440 --> 00:08:09,840
a position and object and keep it

00:08:06,690 --> 00:08:12,870
stationary or at least perform some

00:08:09,840 --> 00:08:14,310
motion and this should ideally happen

00:08:12,870 --> 00:08:17,760
each frame unless we want to happen

00:08:14,310 --> 00:08:21,330
otherwise so giving an example we have

00:08:17,760 --> 00:08:24,120
this is the fast algorithm so if you see

00:08:21,330 --> 00:08:26,550
here the central pixel P is taken in an

00:08:24,120 --> 00:08:29,090
image and then you take the surrounding

00:08:26,550 --> 00:08:30,480
16 pixels to find out the brightness

00:08:29,090 --> 00:08:33,539
comparison results

00:08:30,480 --> 00:08:35,370
so for brisk to call it key feature

00:08:33,539 --> 00:08:37,380
points at least nine pixels should be

00:08:35,370 --> 00:08:39,840
brighter or darker so in this case if

00:08:37,380 --> 00:08:42,150
you see all the top part of the circle

00:08:39,840 --> 00:08:44,070
is or all brighter so P could be a

00:08:42,150 --> 00:08:47,580
feature point and now once you detect

00:08:44,070 --> 00:08:50,010
this feature point for brisk at least it

00:08:47,580 --> 00:08:53,610
creates a binary string by encoding all

00:08:50,010 --> 00:08:55,920
these brightness comparison results and

00:08:53,610 --> 00:08:59,220
then giving it the value of a unique

00:08:55,920 --> 00:09:03,089
fingerprint so this is an example image

00:08:59,220 --> 00:09:04,830
so for this be able to apply brisk of it

00:09:03,089 --> 00:09:06,900
so here I just use some

00:09:04,830 --> 00:09:08,820
OpenCV which is readily available you

00:09:06,900 --> 00:09:10,530
have a brisk algorithm it does all the

00:09:08,820 --> 00:09:12,780
processing for you and all that but you

00:09:10,530 --> 00:09:16,230
don't use or might not be using OpenCV

00:09:12,780 --> 00:09:17,790
no mobiles even but yeah so all these

00:09:16,230 --> 00:09:20,250
are the key points I hope this is

00:09:17,790 --> 00:09:22,200
visible so if you see there are a lot of

00:09:20,250 --> 00:09:24,090
key points and especially these circles

00:09:22,200 --> 00:09:27,120
which you are looking at so all these

00:09:24,090 --> 00:09:30,870
are the key points and they might not

00:09:27,120 --> 00:09:35,070
actually be needed in in our real

00:09:30,870 --> 00:09:37,110
scenarios so now once you detect these

00:09:35,070 --> 00:09:39,990
key points you make a database out of it

00:09:37,110 --> 00:09:42,450
as I mentioned earlier and you ideally

00:09:39,990 --> 00:09:44,100
will be looking at all these images in

00:09:42,450 --> 00:09:45,870
multiple scales so as for example I

00:09:44,100 --> 00:09:47,640
could take the same image in ten

00:09:45,870 --> 00:09:50,040
different scales and then I could say

00:09:47,640 --> 00:09:51,900
that okay if my key points are not

00:09:50,040 --> 00:09:53,640
present in at least six of them I can

00:09:51,900 --> 00:09:55,800
discard them safely because they are not

00:09:53,640 --> 00:09:59,130
reliable to me so that way you could

00:09:55,800 --> 00:10:01,230
ideally you could reduce some key points

00:09:59,130 --> 00:10:03,030
and then you could use them later so you

00:10:01,230 --> 00:10:05,700
can say that more key points are better

00:10:03,030 --> 00:10:09,420
but they are very very much expensive to

00:10:05,700 --> 00:10:11,790
track and this is where error correction

00:10:09,420 --> 00:10:15,000
comes in again to reduce your number of

00:10:11,790 --> 00:10:17,280
key points so you could ideally say that

00:10:15,000 --> 00:10:19,830
I am removing some form of outliers so I

00:10:17,280 --> 00:10:21,660
can say by using some simple geometry

00:10:19,830 --> 00:10:24,150
method I could write an algorithm to

00:10:21,660 --> 00:10:25,440
connect two key points and then discard

00:10:24,150 --> 00:10:27,180
all the key points that are out of the

00:10:25,440 --> 00:10:29,250
top of it and keep them to the right but

00:10:27,180 --> 00:10:31,170
you could this this this thing could

00:10:29,250 --> 00:10:34,560
vary depending on the scenario you have

00:10:31,170 --> 00:10:36,840
it just some example which I came as I'm

00:10:34,560 --> 00:10:39,990
saying so you can use the the remaining

00:10:36,840 --> 00:10:42,450
key points to calculate the pose of the

00:10:39,990 --> 00:10:46,710
user and then define the whole

00:10:42,450 --> 00:10:49,170
experience again so so going back to our

00:10:46,710 --> 00:10:50,640
algorithm this one step gets added so

00:10:49,170 --> 00:10:52,710
performing error correction will be

00:10:50,640 --> 00:10:54,870
quite important in this say in this case

00:10:52,710 --> 00:10:56,580
because it will be like some form of a

00:10:54,870 --> 00:11:02,210
feedback loop which are getting to I

00:10:56,580 --> 00:11:05,910
don't know refine the movements etc okay

00:11:02,210 --> 00:11:08,100
so other other aspects of AR are

00:11:05,910 --> 00:11:10,080
basically lighting of the surrounding

00:11:08,100 --> 00:11:12,540
environment the user interaction so I

00:11:10,080 --> 00:11:14,280
should be able to probably hit this

00:11:12,540 --> 00:11:16,020
flick a ball or something and they

00:11:14,280 --> 00:11:18,670
should fly to the other corner of the

00:11:16,020 --> 00:11:20,590
room and then there could be points on

00:11:18,670 --> 00:11:22,900
so they could be feature points detected

00:11:20,590 --> 00:11:25,750
on some sort of a slanting surface they

00:11:22,900 --> 00:11:28,390
and another part is about hiding objects

00:11:25,750 --> 00:11:30,220
so I should be like I am behind the

00:11:28,390 --> 00:11:31,960
table the camera should be able to

00:11:30,220 --> 00:11:33,820
identify that I am behind the table and

00:11:31,960 --> 00:11:36,160
when something else comes in front of me

00:11:33,820 --> 00:11:41,800
I should be hidden behind behind that

00:11:36,160 --> 00:11:44,650
object so once you understand what these

00:11:41,800 --> 00:11:47,350
are you get to place the virtual object

00:11:44,650 --> 00:11:50,920
in the users in adornment and this is

00:11:47,350 --> 00:11:52,510
done by mesh using some meshes so all 3d

00:11:50,920 --> 00:11:55,810
all virtual objects by a space

00:11:52,510 --> 00:11:57,910
specifically are a combination or are

00:11:55,810 --> 00:12:00,280
made up of meshes so if you see the

00:11:57,910 --> 00:12:03,100
breakdown here so this is like I am they

00:12:00,280 --> 00:12:04,870
are just ultimately a bunch of points so

00:12:03,100 --> 00:12:06,610
you're taking a point cloud and then

00:12:04,870 --> 00:12:09,940
you're make generating a mesh out of it

00:12:06,610 --> 00:12:12,810
and now considering the data you have

00:12:09,940 --> 00:12:15,160
from the earlier steps you try to change

00:12:12,810 --> 00:12:17,620
certain parameters of that particular

00:12:15,160 --> 00:12:19,990
object so what you do is you just could

00:12:17,620 --> 00:12:22,510
very add some shadows to it or you could

00:12:19,990 --> 00:12:24,250
change the amount of lighting which is

00:12:22,510 --> 00:12:26,440
being reflected off it and then you

00:12:24,250 --> 00:12:28,090
could change it scale or dimensions or

00:12:26,440 --> 00:12:29,410
something or amount of object visible

00:12:28,090 --> 00:12:31,690
cetera

00:12:29,410 --> 00:12:33,280
the last part as I mentioned is called

00:12:31,690 --> 00:12:36,910
occlusion and it's especially difficult

00:12:33,280 --> 00:12:38,410
because you need a depth sensor data or

00:12:36,910 --> 00:12:42,400
something like that to understand what

00:12:38,410 --> 00:12:46,990
it is so how do I our libraries handle

00:12:42,400 --> 00:12:50,440
this so these are some of the libraries

00:12:46,990 --> 00:12:52,390
which I've been familiar with so I will

00:12:50,440 --> 00:12:56,770
give examples out of yakou because

00:12:52,390 --> 00:12:59,680
that's what I've been using mostly so on

00:12:56,770 --> 00:13:01,420
launching an AR app you for the app will

00:12:59,680 --> 00:13:03,730
first of all scan the surroundings and

00:13:01,420 --> 00:13:05,890
then as I mentioned there is already an

00:13:03,730 --> 00:13:10,150
existing database so it will try to

00:13:05,890 --> 00:13:12,970
match the the points it scans at that on

00:13:10,150 --> 00:13:14,770
launch with existing database and then

00:13:12,970 --> 00:13:17,170
it'll give the pre-downloaded

00:13:14,770 --> 00:13:18,880
key points and all that and then if

00:13:17,170 --> 00:13:21,490
nothing exists it will initialize a new

00:13:18,880 --> 00:13:23,410
map but in either case when the user

00:13:21,490 --> 00:13:24,070
starts so for example I have a map of

00:13:23,410 --> 00:13:26,500
this location

00:13:24,070 --> 00:13:28,810
and then I start moving further as I go

00:13:26,500 --> 00:13:31,209
further my map keeps getting bigger

00:13:28,810 --> 00:13:34,809
and this data ultimate

00:13:31,209 --> 00:13:37,119
will be used to create my AR experience

00:13:34,809 --> 00:13:39,040
but again another thing is that my

00:13:37,119 --> 00:13:43,389
bigger maps will mean more computations

00:13:39,040 --> 00:13:45,339
to manage because all these computations

00:13:43,389 --> 00:13:46,829
are like I have to triangulate the

00:13:45,339 --> 00:13:49,480
positions etcetera etcetera etcetera

00:13:46,829 --> 00:13:52,119
because and additionally this is more

00:13:49,480 --> 00:13:55,929
difficult because my phone has a limited

00:13:52,119 --> 00:13:58,959
number of resources which I could use so

00:13:55,929 --> 00:14:01,300
this is an example so taking this image

00:13:58,959 --> 00:14:03,429
so those at the top thing is the feature

00:14:01,300 --> 00:14:06,069
points that I detected and this could be

00:14:03,429 --> 00:14:08,980
some form of a map and this is the depth

00:14:06,069 --> 00:14:16,689
sensor data which most libraries are not

00:14:08,980 --> 00:14:18,910
currently doing and so some major issues

00:14:16,689 --> 00:14:21,220
so we've seen what half what are the

00:14:18,910 --> 00:14:23,079
basics and all that so some major issues

00:14:21,220 --> 00:14:25,119
I can say that it's what improper

00:14:23,079 --> 00:14:26,829
occlusion and performance drops the

00:14:25,119 --> 00:14:28,389
other two depth distortions an

00:14:26,829 --> 00:14:31,360
inaccurate tracking data are basically

00:14:28,389 --> 00:14:32,949
or some variant of occlusion handling so

00:14:31,360 --> 00:14:35,199
if you are able to handle occlusion well

00:14:32,949 --> 00:14:38,649
and then distortions I am talking could

00:14:35,199 --> 00:14:41,709
be probably corrected automatically so I

00:14:38,649 --> 00:14:43,629
could hard-coded all of these but that's

00:14:41,709 --> 00:14:47,679
a pretty difficult task especially in

00:14:43,629 --> 00:14:48,999
real scenarios and this is an example of

00:14:47,679 --> 00:14:50,740
occlusion as I've mentioned so the

00:14:48,999 --> 00:14:52,929
dragon should ideally be behind the

00:14:50,740 --> 00:14:54,790
chair but it's been front of it because

00:14:52,929 --> 00:15:00,249
it the camera doesn't know that there is

00:14:54,790 --> 00:15:02,079
a chair to identify so try to solving

00:15:00,249 --> 00:15:04,540
occlusion you have to get a depth

00:15:02,079 --> 00:15:07,269
cameras and then get the depth data and

00:15:04,540 --> 00:15:08,889
then aggregate this over frame over a

00:15:07,269 --> 00:15:10,420
bunch of frames to generate okay there

00:15:08,889 --> 00:15:12,610
is this continuous data so there could

00:15:10,420 --> 00:15:20,220
be a 3d object presenter and this is

00:15:12,610 --> 00:15:20,220
assuming that I have yeah

00:15:21,530 --> 00:15:36,200
it could vary depending on right now

00:15:34,100 --> 00:15:39,020
mobile devices ideally it will have a

00:15:36,200 --> 00:15:42,380
single camera so when you consider

00:15:39,020 --> 00:15:44,000
example tango phone so it kind it does

00:15:42,380 --> 00:15:46,190
this depth calculations pretty well but

00:15:44,000 --> 00:15:48,170
right now the thing is that you most

00:15:46,190 --> 00:15:54,590
phones do not have this depths the depth

00:15:48,170 --> 00:15:56,780
the depth that I'm not actually sure of

00:15:54,590 --> 00:15:58,850
that so I think in one of the examples

00:15:56,780 --> 00:16:01,580
I've seen they've used I think five

00:15:58,850 --> 00:16:03,500
cameras to get that stereo data and then

00:16:01,580 --> 00:16:06,830
manage things accordingly but I'm not

00:16:03,500 --> 00:16:08,480
actually sure of that okay so because

00:16:06,830 --> 00:16:11,480
most phones don't have the depth sensor

00:16:08,480 --> 00:16:14,240
data and you your task is becoming that

00:16:11,480 --> 00:16:18,560
you have to generate a 3d reconstruction

00:16:14,240 --> 00:16:20,900
from a 2d image right so assuming that

00:16:18,560 --> 00:16:23,930
you do have the depth sensor data you

00:16:20,900 --> 00:16:25,760
take this point cloud and add it to the

00:16:23,930 --> 00:16:29,300
point or which you've made from a layer

00:16:25,760 --> 00:16:31,100
added to the depth data from this thing

00:16:29,300 --> 00:16:33,230
and then combine it over again combine

00:16:31,100 --> 00:16:37,100
it over multiple frames to end up

00:16:33,230 --> 00:16:38,780
generating a mesh but this is still the

00:16:37,100 --> 00:16:41,089
hottest with more performance tops

00:16:38,780 --> 00:16:45,140
because there is only limited resource

00:16:41,089 --> 00:16:47,300
we have black we could use so one good

00:16:45,140 --> 00:16:50,180
solution which I think could be applied

00:16:47,300 --> 00:16:53,360
here is bringing in some form of machine

00:16:50,180 --> 00:16:55,070
learning or augmented reality but that's

00:16:53,360 --> 00:16:58,280
just one of the solutions and I think

00:16:55,070 --> 00:17:00,380
I'll continue with this so the goal here

00:16:58,280 --> 00:17:02,660
is simple so you have to detect a depth

00:17:00,380 --> 00:17:04,610
in a given image by using some method

00:17:02,660 --> 00:17:06,949
and then you use this data to generate a

00:17:04,610 --> 00:17:10,850
mesh out of it by combining it with the

00:17:06,949 --> 00:17:12,860
existing point cloud data so some ways

00:17:10,850 --> 00:17:14,839
to do this is that some some they are

00:17:12,860 --> 00:17:17,150
already using some neural networks along

00:17:14,839 --> 00:17:18,829
with some other tools to do this so this

00:17:17,150 --> 00:17:23,150
is an example from something called les

00:17:18,829 --> 00:17:25,280
elidio so they try to create a mesh in

00:17:23,150 --> 00:17:27,140
real time and then place the objects on

00:17:25,280 --> 00:17:30,290
top of it so when you see that the

00:17:27,140 --> 00:17:32,370
camera moves pal down you see that the

00:17:30,290 --> 00:17:35,640
balls there ahead

00:17:32,370 --> 00:17:37,230
so before we go into oversee what is

00:17:35,640 --> 00:17:39,029
happening here so probably we could take

00:17:37,230 --> 00:17:41,370
a small detour this is one example of

00:17:39,029 --> 00:17:45,299
portrayed modern pixel phones so if you

00:17:41,370 --> 00:17:47,669
see that there are things like this the

00:17:45,299 --> 00:17:49,710
depth here is changing because of the

00:17:47,669 --> 00:17:51,360
thing and I think this is this is the

00:17:49,710 --> 00:17:53,100
example where I mentioned they used five

00:17:51,360 --> 00:17:56,250
different phones to calculate the depth

00:17:53,100 --> 00:17:59,760
at least during the training phase okay

00:17:56,250 --> 00:18:02,159
the second part is that this is a face

00:17:59,760 --> 00:18:02,730
tracking which from a Arcos augmented

00:18:02,159 --> 00:18:05,429
faces

00:18:02,730 --> 00:18:08,490
it has newly been released so I mean I

00:18:05,429 --> 00:18:10,140
think few months ago or one month ago so

00:18:08,490 --> 00:18:13,529
here what is happening is that if you

00:18:10,140 --> 00:18:15,210
see that there is this green color mesh

00:18:13,529 --> 00:18:17,460
which is forming on phase which is being

00:18:15,210 --> 00:18:19,890
tracked as the user moves around and

00:18:17,460 --> 00:18:22,500
this is another example which I wanted

00:18:19,890 --> 00:18:25,020
to tell okay so the common denominator

00:18:22,500 --> 00:18:28,049
between these two is always instance of

00:18:25,020 --> 00:18:29,970
law so Google especially they use a

00:18:28,049 --> 00:18:35,279
tensor flow specifically tensor flow

00:18:29,970 --> 00:18:38,039
light to to create this experience so

00:18:35,279 --> 00:18:41,010
they made the processing faster so they

00:18:38,039 --> 00:18:44,010
introduced GPU support as well to give

00:18:41,010 --> 00:18:46,080
some sort of I don't know speed-up

00:18:44,010 --> 00:18:48,539
between these things and to put things

00:18:46,080 --> 00:18:50,760
in context for the Augmented faces

00:18:48,539 --> 00:18:52,529
example so the depth tracking and all

00:18:50,760 --> 00:18:55,830
that so that they took some model and

00:18:52,529 --> 00:18:57,870
then they deployed it on to the the

00:18:55,830 --> 00:19:01,409
phone and then they you could use the

00:18:57,870 --> 00:19:04,470
CPU to do that or a GPU so using a CPU

00:19:01,409 --> 00:19:06,750
you have number of so it takes around 30

00:19:04,470 --> 00:19:08,039
milliseconds per frame to infer some

00:19:06,750 --> 00:19:11,480
data out of it

00:19:08,039 --> 00:19:13,409
but in GPU you it is reducing quite

00:19:11,480 --> 00:19:14,039
drastically to around 10 or something

00:19:13,409 --> 00:19:16,470
like that

00:19:14,039 --> 00:19:18,779
so these are some other results so for

00:19:16,470 --> 00:19:20,909
the full mesh and the light mesh so a

00:19:18,779 --> 00:19:24,330
while I understand that this is just not

00:19:20,909 --> 00:19:27,390
the only solution so you could have a

00:19:24,330 --> 00:19:30,210
bunch of other solutions like they also

00:19:27,390 --> 00:19:33,419
had I think it's from colonel so they

00:19:30,210 --> 00:19:35,460
had a thing called mega depth so they

00:19:33,419 --> 00:19:39,929
were using some form of CNN's etcetera

00:19:35,460 --> 00:19:43,230
to understand what the depth information

00:19:39,929 --> 00:19:44,640
from image the second one was I think

00:19:43,230 --> 00:19:46,110
which which I found personally

00:19:44,640 --> 00:19:50,850
interesting was something called

00:19:46,110 --> 00:19:52,710
point net that was about removing the if

00:19:50,850 --> 00:19:54,510
you remember the point Lord had to be

00:19:52,710 --> 00:19:56,760
generated into 3d mesh and then it had

00:19:54,510 --> 00:19:58,890
to be occluded right so from point cloud

00:19:56,760 --> 00:20:00,720
directly to occlusion you from there

00:19:58,890 --> 00:20:03,029
trying to understand what the point load

00:20:00,720 --> 00:20:04,890
itself without having generate without

00:20:03,029 --> 00:20:08,130
having to generate a 3d model out of it

00:20:04,890 --> 00:20:10,500
okay so these are examples so we could

00:20:08,130 --> 00:20:12,480
probably develop more such things and

00:20:10,500 --> 00:20:16,320
then put it into tens of flow light and

00:20:12,480 --> 00:20:18,299
then work on top of that and enhance the

00:20:16,320 --> 00:20:23,070
overall air performance without doing

00:20:18,299 --> 00:20:24,720
any compromises so so far whatever we

00:20:23,070 --> 00:20:28,919
have seen I think could we'll probably

00:20:24,720 --> 00:20:32,460
be around 1% of what they are actually

00:20:28,919 --> 00:20:34,679
is it is still very much an emerging

00:20:32,460 --> 00:20:39,360
field and it has a very great potential

00:20:34,679 --> 00:20:41,159
for exploration so I think even from a

00:20:39,360 --> 00:20:43,649
business perspective you could see that

00:20:41,159 --> 00:20:46,350
augmented reality is around here it's

00:20:43,649 --> 00:20:48,480
it's moving towards the productivity

00:20:46,350 --> 00:20:50,580
area and it's taking around five to ten

00:20:48,480 --> 00:20:53,820
years to do this according to Gartner

00:20:50,580 --> 00:20:56,789
but I think because we are all in the

00:20:53,820 --> 00:21:00,690
open source world we could probably take

00:20:56,789 --> 00:21:02,909
up these conversions and then I don't

00:21:00,690 --> 00:21:04,710
know speed this things up to probably go

00:21:02,909 --> 00:21:07,169
there less than two years something

00:21:04,710 --> 00:21:09,809
because and I'm still a beginner as I

00:21:07,169 --> 00:21:12,059
said and I'll I'm probably hoping to

00:21:09,809 --> 00:21:13,190
come up in some future for summit and

00:21:12,059 --> 00:21:16,950
then give the results of whatever

00:21:13,190 --> 00:21:19,200
happened in this process so here are

00:21:16,950 --> 00:21:21,630
some great references which I found are

00:21:19,200 --> 00:21:24,539
interesting I'll leave this up in my

00:21:21,630 --> 00:21:31,159
github or somewhere so you could take a

00:21:24,539 --> 00:21:31,159
look at them so thank you any questions

00:21:37,630 --> 00:21:43,360
in that before eeeh I think is

00:21:39,670 --> 00:21:45,250
cross-platform so we basically use what

00:21:43,360 --> 00:21:48,460
in our team we used something called as

00:21:45,250 --> 00:21:51,100
unity engine so on top of in unity you

00:21:48,460 --> 00:21:52,480
can just install some I mean add these

00:21:51,100 --> 00:21:54,670
libraries and then take any build

00:21:52,480 --> 00:21:57,970
whatever you want book with before yeah

00:21:54,670 --> 00:22:00,550
I have done with a Android and then iOS

00:21:57,970 --> 00:22:02,620
both of them AR core is basically for

00:22:00,550 --> 00:22:05,140
Android but you could do have some

00:22:02,620 --> 00:22:09,940
extensions into iOS I am not so sure

00:22:05,140 --> 00:22:12,540
about a UH a orchids this thing does

00:22:09,940 --> 00:22:12,540
that answer you

00:22:21,540 --> 00:22:26,650
yeah a our foundation I think it's

00:22:23,950 --> 00:22:30,340
relatively new right so what he our code

00:22:26,650 --> 00:22:32,770
has been a bit there yeah and when you

00:22:30,340 --> 00:22:34,780
compare the features between a our core

00:22:32,770 --> 00:22:37,390
and a our foundation stung some things

00:22:34,780 --> 00:22:39,250
are still lacking so I think for the

00:22:37,390 --> 00:22:42,280
last time I have seen one thing is about

00:22:39,250 --> 00:22:44,170
image detection so it will detect an

00:22:42,280 --> 00:22:46,510
image in an environment and then it will

00:22:44,170 --> 00:22:49,450
put an object on top of it as it detects

00:22:46,510 --> 00:22:51,070
that image so that feature I don't think

00:22:49,450 --> 00:22:52,210
is still present in our foundation maybe

00:22:51,070 --> 00:22:55,270
they might have added it I'm not sure

00:22:52,210 --> 00:22:58,390
and that's one of the things and I find

00:22:55,270 --> 00:23:00,220
our core more appealing because Google

00:22:58,390 --> 00:23:02,290
is doing a lot of other research in

00:23:00,220 --> 00:23:04,810
terms of tens of flow etc and you can

00:23:02,290 --> 00:23:06,840
easily write some I don't know an

00:23:04,810 --> 00:23:10,090
Android app in native at least to

00:23:06,840 --> 00:23:11,980
combine this this a arc over tens of

00:23:10,090 --> 00:23:13,750
flow or something like that and then you

00:23:11,980 --> 00:23:16,020
know continue the experience or

00:23:13,750 --> 00:23:16,020
something

00:23:24,930 --> 00:23:31,400
yeah on device that's why tensorflow

00:23:27,030 --> 00:23:34,380
light comes in so this this has yeah

00:23:31,400 --> 00:23:36,840
yeah you can do that so tens of low

00:23:34,380 --> 00:23:39,030
light has optimized some I'm not sure

00:23:36,840 --> 00:23:40,050
how they do it internally I'm still yet

00:23:39,030 --> 00:23:42,390
to explore that part

00:23:40,050 --> 00:23:56,190
so yeah the tensor flow light will be of

00:23:42,390 --> 00:23:58,200
a great help in this case so normally I

00:23:56,190 --> 00:23:59,850
think they could be not sure of the

00:23:58,200 --> 00:24:02,340
games which are they're like the big

00:23:59,850 --> 00:24:04,800
ones so normally from what I've seen

00:24:02,340 --> 00:24:07,260
many companies are internally using it

00:24:04,800 --> 00:24:09,929
so in case of for example for who for a

00:24:07,260 --> 00:24:13,880
at least you could have some companies

00:24:09,929 --> 00:24:16,650
are doing it - I don't know guide the

00:24:13,880 --> 00:24:19,170
field agents and doing various tasks or

00:24:16,650 --> 00:24:20,880
something like that so it depends on the

00:24:19,170 --> 00:24:24,230
use case again but I'm not so reg of

00:24:20,880 --> 00:24:24,230
exact names which I can

00:24:33,940 --> 00:24:39,640
yeah so that's one of the good

00:24:36,290 --> 00:24:39,640
applications you could have yes

00:24:51,170 --> 00:24:56,920

YouTube URL: https://www.youtube.com/watch?v=dIs0MQ-sVyo


