Title: Machine Learning for CI by Andrea Frittoli
Publication date: 2019-03-27
Playlist: FOSSASIA Summit 2019 - Artificial Intelligence
Description: 
	15 March 2019 16:35, Event Hall 2-1

As more applications move to a DevOps model with CI/CD pipelines, the testing required for this development model to work inevitably generates lots of data. There are valuable insights hidden in this data that ML can help extract with minimal human intervention. Using open source tools like TensorFlow and Pandas we trained ML algorithms with real-life data from the OpenStack community's CI system. We built a Kubernetes application that sets up a prediction pipeline to automate the analysis of CI jobs in near real time. It uses the trained model to classify new inputs and predict insights like test results or hosting cloud provider. In this talk, we present our experience training different ML models with the large dataset from OpenStack's CI and how this can be leveraged for automated failure identification and analysis. We also discuss how these techniques can be used with any CI system.
Captions: 
	00:00:00,000 --> 00:00:04,080
okay our next presentation will be from

00:00:02,610 --> 00:00:06,569
Andrea frito-lay's

00:00:04,080 --> 00:00:09,780
he is a open source developer advocate

00:00:06,569 --> 00:00:12,269
at IBM and he will tell us about his

00:00:09,780 --> 00:00:16,199
experience using machine learning for a

00:00:12,269 --> 00:00:20,039
continuous integration piece for the

00:00:16,199 --> 00:00:23,580
introduction so welcome everyone so yeah

00:00:20,039 --> 00:00:25,439
my name is Andrea Vitaly so I present

00:00:23,580 --> 00:00:26,160
today work with done with a couple of

00:00:25,439 --> 00:00:31,849
colleagues

00:00:26,160 --> 00:00:31,849
Soumitra Aneesh and kira Wolfert and so

00:00:31,880 --> 00:00:38,309
we the idea is that we wanted to really

00:00:35,960 --> 00:00:41,040
learn more about machine learning and

00:00:38,309 --> 00:00:44,850
with open source there's a lot of open

00:00:41,040 --> 00:00:47,309
source tools for machine learning so

00:00:44,850 --> 00:00:49,739
there's a lot of documentation and a lot

00:00:47,309 --> 00:00:52,320
of books you can find and you have very

00:00:49,739 --> 00:00:55,890
powerful tools at your disposal so we

00:00:52,320 --> 00:00:57,750
thought okay let's let's make something

00:00:55,890 --> 00:00:59,910
that is interesting to us and let's

00:00:57,750 --> 00:01:02,640
apply machine learning to it we have we

00:00:59,910 --> 00:01:08,280
have been working with the open up

00:01:02,640 --> 00:01:12,350
instead community and there is a very

00:01:08,280 --> 00:01:15,119
large scale CSS GI system there so

00:01:12,350 --> 00:01:16,680
because the tools are kind of there the

00:01:15,119 --> 00:01:18,659
open-source tools but what is hard to

00:01:16,680 --> 00:01:20,640
find is an open source data set if you

00:01:18,659 --> 00:01:22,170
will and you really need a good data set

00:01:20,640 --> 00:01:24,900
if you want to do something meaningful

00:01:22,170 --> 00:01:28,740
and it's it's hard to get good quality

00:01:24,900 --> 00:01:31,380
data but because we have this system in

00:01:28,740 --> 00:01:35,100
open and OpenStack and all the logs

00:01:31,380 --> 00:01:40,229
everything is available publicly so we

00:01:35,100 --> 00:01:43,770
thought well let's look at this data

00:01:40,229 --> 00:01:48,509
that we get from CI so here we are

00:01:43,770 --> 00:01:52,380
showing in million of tests see it's one

00:01:48,509 --> 00:01:53,909
other fruit endless power to seven so

00:01:52,380 --> 00:01:56,100
it's the number of tests that run daily

00:01:53,909 --> 00:01:59,490
and this is a graph I got it update

00:01:56,100 --> 00:02:01,159
until the beginning of March and so we

00:01:59,490 --> 00:02:03,119
do continuous integration continuous

00:02:01,159 --> 00:02:05,390
continuously log data they are in

00:02:03,119 --> 00:02:07,920
OpenStack so we have lots of data and

00:02:05,390 --> 00:02:10,200
and one of the problems the community

00:02:07,920 --> 00:02:12,690
there has always been to have enough

00:02:10,200 --> 00:02:13,310
engineers to triage all the failures and

00:02:12,690 --> 00:02:16,640
understand

00:02:13,310 --> 00:02:19,010
everything that is going on there so we

00:02:16,640 --> 00:02:21,530
thought well we might use a y:i to try

00:02:19,010 --> 00:02:24,860
and solve this problem and help us as

00:02:21,530 --> 00:02:26,599
human scale better as you know in

00:02:24,860 --> 00:02:29,000
open-source community you may have a

00:02:26,599 --> 00:02:30,800
limited amount of Engineers and they may

00:02:29,000 --> 00:02:32,720
be limited amount of time that they can

00:02:30,800 --> 00:02:35,209
dedicate to the project so it's we

00:02:32,720 --> 00:02:37,970
thought okay maybe AI can help us here

00:02:35,209 --> 00:02:43,970
looking at this log files from from CI

00:02:37,970 --> 00:02:46,750
and understanding what is going on so

00:02:43,970 --> 00:02:49,190
what we do there for OpenStack is

00:02:46,750 --> 00:02:52,790
integration testing in a vehicle

00:02:49,190 --> 00:02:55,160
machines so every time a patch is

00:02:52,790 --> 00:02:56,630
submitted to OpenStack OpenStack if you

00:02:55,160 --> 00:03:00,410
don't know it's a cloud management

00:02:56,630 --> 00:03:02,360
system so it allows you to manages cloud

00:03:00,410 --> 00:03:06,709
and network we have 12 networks and

00:03:02,360 --> 00:03:09,260
storage and so forth so we have public

00:03:06,709 --> 00:03:11,720
cloud donors that donate resources to

00:03:09,260 --> 00:03:13,940
the OpenStack projects we can spin up

00:03:11,720 --> 00:03:16,549
VMs and every time you want to run tests

00:03:13,940 --> 00:03:18,620
so we spin up of yemm's we install the

00:03:16,549 --> 00:03:20,870
entire OpenStack on top of this we have

00:03:18,620 --> 00:03:22,640
tow machine and then we run end-to-end

00:03:20,870 --> 00:03:24,739
tests so which means we actually create

00:03:22,640 --> 00:03:27,709
virtual machine storage networks and so

00:03:24,739 --> 00:03:31,100
forth in the VM and all these generate

00:03:27,709 --> 00:03:33,049
loads on the virtual machine so and we

00:03:31,100 --> 00:03:36,100
gather then system logs we get our

00:03:33,049 --> 00:03:39,230
application logs and we also gather what

00:03:36,100 --> 00:03:41,230
the logs that are produced by a tool

00:03:39,230 --> 00:03:44,829
which is called dis that allows you to

00:03:41,230 --> 00:03:47,480
store in kind of CSV type format

00:03:44,829 --> 00:03:49,790
information about key statistics of your

00:03:47,480 --> 00:03:52,459
operating system like the CPU

00:03:49,790 --> 00:03:56,030
utilization the disk i/o the memory

00:03:52,459 --> 00:03:57,980
utilization the average load minute or

00:03:56,030 --> 00:04:01,220
five minutes and so forth so there are a

00:03:57,980 --> 00:04:04,910
large number of statistics and we get

00:04:01,220 --> 00:04:07,569
this data from for all the CI jobs than

00:04:04,910 --> 00:04:11,900
the million of tests that are run daily

00:04:07,569 --> 00:04:14,000
so we thought we could use the distant

00:04:11,900 --> 00:04:17,169
data because this is numeric numerical

00:04:14,000 --> 00:04:19,700
data as well so it made it will make

00:04:17,169 --> 00:04:21,769
hello yeah it will make life a little

00:04:19,700 --> 00:04:25,580
bit easier in terms of normalization and

00:04:21,769 --> 00:04:27,080
it's also interesting because did we get

00:04:25,580 --> 00:04:28,639
this data running tests on

00:04:27,080 --> 00:04:32,150
openstax but it's not necessarily

00:04:28,639 --> 00:04:36,169
OpenStack specific because it can the

00:04:32,150 --> 00:04:37,789
same kind of system machine learning

00:04:36,169 --> 00:04:39,979
model you could be applied to any

00:04:37,789 --> 00:04:47,229
distant data generated in any kind of

00:04:39,979 --> 00:04:50,060
use case so one other thing that we

00:04:47,229 --> 00:04:52,789
recognized in the beginning maybe a bit

00:04:50,060 --> 00:04:54,379
late is that yeah so you need data

00:04:52,789 --> 00:04:57,199
actually if you want to do something as

00:04:54,379 --> 00:04:59,479
I was saying in the beginning so even if

00:04:57,199 --> 00:05:00,830
you're not sure exactly what kind of

00:04:59,479 --> 00:05:02,419
machine learning model you're going to

00:05:00,830 --> 00:05:05,900
use and you're still considering things

00:05:02,419 --> 00:05:07,729
if you have a chance if you have data to

00:05:05,900 --> 00:05:09,289
start collecting it do it as soon as

00:05:07,729 --> 00:05:11,750
possible because the more data you will

00:05:09,289 --> 00:05:13,430
be able to together the better it will

00:05:11,750 --> 00:05:16,340
be the outcome for your machine learning

00:05:13,430 --> 00:05:19,460
work so what we have done because the

00:05:16,340 --> 00:05:21,409
the storage space from the CI system in

00:05:19,460 --> 00:05:23,539
APIs text side has limited space and

00:05:21,409 --> 00:05:25,969
they regularly clean up the logs there

00:05:23,539 --> 00:05:27,530
so we set up a system that regularly

00:05:25,969 --> 00:05:31,880
fetches the logs that are interesting to

00:05:27,530 --> 00:05:35,990
us and stores them in a as raw data in a

00:05:31,880 --> 00:05:39,169
free storage we use a function that is

00:05:35,990 --> 00:05:41,990
run as periodically so a couple of times

00:05:39,169 --> 00:05:45,699
a day I think and we just get the latest

00:05:41,990 --> 00:05:45,699
logs and we store them in s3

00:05:48,499 --> 00:06:02,459
so we built an application that let us

00:05:58,969 --> 00:06:05,729
create what we call the a dataset which

00:06:02,459 --> 00:06:08,569
is a normalized data set that is taken

00:06:05,729 --> 00:06:11,459
out of the raw data that we created so

00:06:08,569 --> 00:06:14,939
one of the first actually the first step

00:06:11,459 --> 00:06:17,490
before you you dive into writing Python

00:06:14,939 --> 00:06:19,529
code doing things is actually to explore

00:06:17,490 --> 00:06:20,869
your data and try to visualize it if

00:06:19,529 --> 00:06:23,519
possible if there are multiple

00:06:20,869 --> 00:06:26,399
dimensions you may try to cut it in some

00:06:23,519 --> 00:06:29,819
way or to you know aggregate it in some

00:06:26,399 --> 00:06:31,709
format so that you you can find where

00:06:29,819 --> 00:06:33,240
there is more information in your data

00:06:31,709 --> 00:06:36,439
sometimes you have a lot of redundancy

00:06:33,240 --> 00:06:38,369
like if you look at the system

00:06:36,439 --> 00:06:41,610
performance like the different

00:06:38,369 --> 00:06:45,539
indicators like in our case you may see

00:06:41,610 --> 00:06:47,699
that the graph for the CPU matches very

00:06:45,539 --> 00:06:50,009
much the graph of the average load for

00:06:47,699 --> 00:06:51,659
instance and then is well maybe using

00:06:50,009 --> 00:06:53,669
both of them it's redundant because it's

00:06:51,659 --> 00:06:55,829
the same kind of information and because

00:06:53,669 --> 00:06:57,539
the more information you have the more

00:06:55,829 --> 00:06:59,550
processing you have to do so it's better

00:06:57,539 --> 00:07:01,439
to try and you know get rid of the

00:06:59,550 --> 00:07:02,879
information that is not relevant so yeah

00:07:01,439 --> 00:07:04,680
there are a lot of data first and then

00:07:02,879 --> 00:07:08,009
but then focus on the one that is

00:07:04,680 --> 00:07:11,969
actually relevant for your model this is

00:07:08,009 --> 00:07:14,819
an example it's a Python tool we call it

00:07:11,969 --> 00:07:17,610
Cemal CI and machine learning CI machine

00:07:14,819 --> 00:07:21,589
learning that allows us to from the raw

00:07:17,610 --> 00:07:25,379
data set to select a number of features

00:07:21,589 --> 00:07:27,959
so that we can have a repeat we can have

00:07:25,379 --> 00:07:29,579
a we have a tool a way to from the raw

00:07:27,959 --> 00:07:33,269
data that we collected over time to

00:07:29,579 --> 00:07:36,629
extract the data set that we know as a

00:07:33,269 --> 00:07:38,189
specific format so we do two things we

00:07:36,629 --> 00:07:40,769
filter it so we filter in terms of

00:07:38,189 --> 00:07:44,039
features that we select we want to use

00:07:40,769 --> 00:07:47,069
the CPU or the memory and then we select

00:07:44,039 --> 00:07:50,189
the size of the data set and we do a

00:07:47,069 --> 00:07:53,490
normalization normalization because this

00:07:50,189 --> 00:07:55,979
is numeric already means mostly that we

00:07:53,490 --> 00:07:59,430
want to make sure that the numbers are

00:07:55,979 --> 00:08:00,930
between minus 1 and 1 because it makes a

00:07:59,430 --> 00:08:06,389
computation much

00:08:00,930 --> 00:08:08,520
less CPU intensive if you will so this

00:08:06,389 --> 00:08:10,919
is an example of how the distillate

00:08:08,520 --> 00:08:14,490
looks likes as I said is a kind of CSV

00:08:10,919 --> 00:08:16,380
so it's a big table and you have time

00:08:14,490 --> 00:08:18,960
stamps and for every time step you have

00:08:16,380 --> 00:08:21,300
a lot of columns so what we do here we

00:08:18,960 --> 00:08:24,690
do a selection of which columns that are

00:08:21,300 --> 00:08:27,389
interesting for us so this is the CPU

00:08:24,690 --> 00:08:30,690
utilization for user space and this is

00:08:27,389 --> 00:08:33,060
the average load over one minute system

00:08:30,690 --> 00:08:34,740
load and then we also do down sampling

00:08:33,060 --> 00:08:38,130
because we have this data for every

00:08:34,740 --> 00:08:39,899
second and we realize and I will get in

00:08:38,130 --> 00:08:41,969
more details about this later that you

00:08:39,899 --> 00:08:44,610
don't really actually need that kind of

00:08:41,969 --> 00:08:50,640
granularity so you may get good results

00:08:44,610 --> 00:08:53,040
with a lower granularity and then we do

00:08:50,640 --> 00:08:55,589
data normalization because we have a

00:08:53,040 --> 00:08:57,690
matrix as a result of the filtering with

00:08:55,589 --> 00:09:00,270
them before but this matrix is actually

00:08:57,690 --> 00:09:03,589
one sample for us because it's a result

00:09:00,270 --> 00:09:06,660
of one CI test so we want to do

00:09:03,589 --> 00:09:08,880
unrolling which means that this matrix

00:09:06,660 --> 00:09:12,120
actually becomes a vector a long vector

00:09:08,880 --> 00:09:16,020
of numbers and normalization so this is

00:09:12,120 --> 00:09:21,720
a sample of unrolled data so you see the

00:09:16,020 --> 00:09:23,660
CPU that was here this free value six

00:09:21,720 --> 00:09:25,980
one one 75126

00:09:23,660 --> 00:09:28,500
become actually three different column

00:09:25,980 --> 00:09:31,740
USR one u.s. or two us are free so we

00:09:28,500 --> 00:09:35,730
have this for every instant in time that

00:09:31,740 --> 00:09:37,380
we get we selected and these lines now

00:09:35,730 --> 00:09:39,360
they correspond to different samples so

00:09:37,380 --> 00:09:42,120
this is the CI job once you have two in

00:09:39,360 --> 00:09:44,100
CI job three hope this makes it a bit

00:09:42,120 --> 00:09:45,600
more clear and you see the numbers they

00:09:44,100 --> 00:09:48,420
have random values and after

00:09:45,600 --> 00:09:51,500
normalization they all belong between

00:09:48,420 --> 00:09:51,500
minus 1 and 1

00:09:52,640 --> 00:09:58,790
another thing that you you may want to

00:09:55,650 --> 00:10:02,130
take care when you build the data set is

00:09:58,790 --> 00:10:05,160
do not use everything for training it's

00:10:02,130 --> 00:10:06,540
important to train your model but it's

00:10:05,160 --> 00:10:10,589
also important to have an evaluation

00:10:06,540 --> 00:10:12,120
phase where you test how accurate is

00:10:10,589 --> 00:10:14,250
your model in predicting the things that

00:10:12,120 --> 00:10:14,940
you want to predict and you may also

00:10:14,250 --> 00:10:18,210
want to have

00:10:14,940 --> 00:10:20,460
small deaf data said that you can use

00:10:18,210 --> 00:10:25,770
for fine-tuning hyper parameters of your

00:10:20,460 --> 00:10:27,150
of your model again all this the data

00:10:25,770 --> 00:10:28,740
set oh sorry

00:10:27,150 --> 00:10:31,710
once something I wanted to mention is

00:10:28,740 --> 00:10:34,890
that the labels term there's labels is a

00:10:31,710 --> 00:10:36,810
bit overloaded as a word in there we

00:10:34,890 --> 00:10:40,410
just need need the name of the other

00:10:36,810 --> 00:10:44,730
features that we have like this us are

00:10:40,410 --> 00:10:48,510
one us are two there so and this is the

00:10:44,730 --> 00:10:50,460
data and the classes is actually the

00:10:48,510 --> 00:10:52,470
values are the things that we want to

00:10:50,460 --> 00:10:53,570
predict getting more details about that

00:10:52,470 --> 00:10:56,550
in a moment

00:10:53,570 --> 00:11:01,320
data sets are also stored in as free

00:10:56,550 --> 00:11:02,790
storage the next thing is that we wanted

00:11:01,320 --> 00:11:05,760
to do is to be able to define an

00:11:02,790 --> 00:11:07,560
experiment so again we have a tool

00:11:05,760 --> 00:11:10,740
written in Python that we created that

00:11:07,560 --> 00:11:13,140
allows us to define the hyper parameter

00:11:10,740 --> 00:11:15,330
of an experiment and serve them in a in

00:11:13,140 --> 00:11:18,450
a forma so that we can rerun experiments

00:11:15,330 --> 00:11:22,290
in a repeatable way so we have a data

00:11:18,450 --> 00:11:24,780
set that we can recreate and then we can

00:11:22,290 --> 00:11:28,020
run experiments so there are multiple

00:11:24,780 --> 00:11:30,360
hyper parameters we can select how what

00:11:28,020 --> 00:11:32,190
is the structure of the neural network

00:11:30,360 --> 00:11:34,440
for instance how many hidden layer some

00:11:32,190 --> 00:11:39,890
neuron per layer so many steps and so

00:11:34,440 --> 00:11:43,770
forth and then we have a wrapper command

00:11:39,890 --> 00:11:47,060
that allows us CIA CIA ml train model

00:11:43,770 --> 00:11:50,940
allows us to actually run the training

00:11:47,060 --> 00:11:52,880
so the reason we did this pleat this

00:11:50,940 --> 00:11:55,220
separation in different steps so

00:11:52,880 --> 00:11:57,690
normalization preparation of the data

00:11:55,220 --> 00:12:00,240
normalization and then training is that

00:11:57,690 --> 00:12:03,660
in in our experience here we use tends

00:12:00,240 --> 00:12:06,060
to flow as a model and tends to flow has

00:12:03,660 --> 00:12:08,040
tools and API that allows you to do data

00:12:06,060 --> 00:12:11,490
normalization but we wanted to do that

00:12:08,040 --> 00:12:12,660
directly with Python tools because when

00:12:11,490 --> 00:12:14,040
we started we didn't know for sure

00:12:12,660 --> 00:12:15,450
whether we wanted to stick with

00:12:14,040 --> 00:12:17,280
tensorflow maybe switch to other

00:12:15,450 --> 00:12:18,990
different frameworks so we wanted to be

00:12:17,280 --> 00:12:20,730
able to normalize our data and get it to

00:12:18,990 --> 00:12:22,680
us clean state and then we could use to

00:12:20,730 --> 00:12:25,910
feed to any kind of machine learning

00:12:22,680 --> 00:12:25,910
frame that we wanted to use

00:12:27,649 --> 00:12:33,779
so we are able to run the training on

00:12:31,200 --> 00:12:37,589
your laptop locally with this but we

00:12:33,779 --> 00:12:40,700
also integrated this with fiddle FF DL

00:12:37,589 --> 00:12:44,940
which is an open-source project by

00:12:40,700 --> 00:12:49,040
merely contribution by IBM and that

00:12:44,940 --> 00:12:51,870
basically allows you to take a model and

00:12:49,040 --> 00:13:00,660
do the training distribute the training

00:12:51,870 --> 00:13:02,160
on a kubernetes cluster so this is a bit

00:13:00,660 --> 00:13:04,680
more about the training infrastructure

00:13:02,160 --> 00:13:07,019
this is the picture is the architecture

00:13:04,680 --> 00:13:10,380
of fiddle itself I will not get into

00:13:07,019 --> 00:13:11,250
details of that we use the estimator API

00:13:10,380 --> 00:13:13,860
in tensorflow

00:13:11,250 --> 00:13:15,630
which is actually based on Kiera's which

00:13:13,860 --> 00:13:17,670
is another open source library which

00:13:15,630 --> 00:13:20,730
allows very high level of abstraction

00:13:17,670 --> 00:13:23,880
when you do it's very good if you want

00:13:20,730 --> 00:13:26,040
to start defining modest very simple so

00:13:23,880 --> 00:13:29,100
we created the Cemal wrapper which is

00:13:26,040 --> 00:13:31,470
Python script that involved the terms of

00:13:29,100 --> 00:13:33,089
flow API and do the training run the

00:13:31,470 --> 00:13:35,430
training we do the normalization the

00:13:33,089 --> 00:13:36,480
preparation of experiments so as I was

00:13:35,430 --> 00:13:38,579
saying that the machine learning

00:13:36,480 --> 00:13:41,730
framework is interchangeable and the

00:13:38,579 --> 00:13:43,500
training options is local or we we can

00:13:41,730 --> 00:13:45,570
also we have a helm car to deploy your

00:13:43,500 --> 00:13:51,540
application so then running container or

00:13:45,570 --> 00:13:55,680
we can use fiddle so for the prediction

00:13:51,540 --> 00:13:58,430
side the model that we have in mind that

00:13:55,680 --> 00:14:01,020
we implemented here is even driven so

00:13:58,430 --> 00:14:03,930
because we have CI jobs that are

00:14:01,020 --> 00:14:08,130
executed the event that are generated is

00:14:03,930 --> 00:14:09,720
when a CI job is completed then new data

00:14:08,130 --> 00:14:11,579
is generated and we want to run

00:14:09,720 --> 00:14:12,990
inference on that data so based on the

00:14:11,579 --> 00:14:15,959
model that we were trained so we don't

00:14:12,990 --> 00:14:17,490
have a request for a inference based on

00:14:15,959 --> 00:14:19,529
that model but we have an event that is

00:14:17,490 --> 00:14:21,959
triggered because of that we've written

00:14:19,529 --> 00:14:26,339
a since when a this application that

00:14:21,959 --> 00:14:28,800
basically includes an amputee t client

00:14:26,339 --> 00:14:31,290
OpenStack CI system generates and QT

00:14:28,800 --> 00:14:33,680
events when jobs are completed so we can

00:14:31,290 --> 00:14:36,810
listen to these events download the logs

00:14:33,680 --> 00:14:38,370
and then run inference and predict the

00:14:36,810 --> 00:14:40,320
parameters that we want to predict on

00:14:38,370 --> 00:14:43,840
death

00:14:40,320 --> 00:14:45,700
because the CI job from OpenStack is

00:14:43,840 --> 00:14:48,400
considered for us a trusted source of

00:14:45,700 --> 00:14:52,000
data we can use the new data that comes

00:14:48,400 --> 00:14:57,580
in as well to continue training the

00:14:52,000 --> 00:14:59,710
model with new data okay so this is all

00:14:57,580 --> 00:15:01,690
about the kind of infrastructure and how

00:14:59,710 --> 00:15:04,480
we structure the project so I will get a

00:15:01,690 --> 00:15:06,100
bit more now in details what kind of

00:15:04,480 --> 00:15:10,890
inference we've done but what kind of

00:15:06,100 --> 00:15:13,390
training so we've done two main type of

00:15:10,890 --> 00:15:15,790
experiments the first one is a binary

00:15:13,390 --> 00:15:17,920
classification basically we want to

00:15:15,790 --> 00:15:20,800
predict very simply if the tests are

00:15:17,920 --> 00:15:23,040
passed or failed so we've seen enough

00:15:20,800 --> 00:15:25,810
logs over the years and we thought well

00:15:23,040 --> 00:15:27,370
let's make a bet I think just looking at

00:15:25,810 --> 00:15:30,250
the system load we are able to predict

00:15:27,370 --> 00:15:32,500
whether a test was passed or failed so

00:15:30,250 --> 00:15:33,670
let's build a model and see if you are

00:15:32,500 --> 00:15:35,500
able to predict that

00:15:33,670 --> 00:15:37,240
so it was supervised training because we

00:15:35,500 --> 00:15:41,170
actually know whether the test is passed

00:15:37,240 --> 00:15:43,290
or fail we use the DNN classifier with

00:15:41,170 --> 00:15:46,990
two classes from tensorflow

00:15:43,290 --> 00:15:50,050
we we pick the specific CI job which is

00:15:46,990 --> 00:15:53,670
called tempest full in OpenStack there

00:15:50,050 --> 00:15:56,860
are two there are several but two main

00:15:53,670 --> 00:15:59,050
CI pipeline one is the test check

00:15:56,860 --> 00:16:00,700
pipeline which is executed when the code

00:15:59,050 --> 00:16:02,800
is initially submitted to the project

00:16:00,700 --> 00:16:05,200
before it's actually reviewed by

00:16:02,800 --> 00:16:07,150
developers so there is a lot of

00:16:05,200 --> 00:16:09,550
variability in there and we didn't want

00:16:07,150 --> 00:16:13,300
to use the data because we didn't want

00:16:09,550 --> 00:16:15,580
to try and predict failures that are due

00:16:13,300 --> 00:16:18,250
to a typo in the code or something like

00:16:15,580 --> 00:16:19,900
that instead we took data from the gate

00:16:18,250 --> 00:16:22,000
pipelines so the gate pipeline is

00:16:19,900 --> 00:16:24,010
executed on code that's been tested

00:16:22,000 --> 00:16:26,440
already by the automatic system has been

00:16:24,010 --> 00:16:28,540
reviewed by the contributors and has

00:16:26,440 --> 00:16:30,370
been approved so just before the code is

00:16:28,540 --> 00:16:32,950
merged the tests are executed again and

00:16:30,370 --> 00:16:34,810
this is the gate pipeline and it's clean

00:16:32,950 --> 00:16:37,750
data and the failures are related maybe

00:16:34,810 --> 00:16:39,910
to infrastructure issues and maybe some

00:16:37,750 --> 00:16:42,430
new version of a library is released and

00:16:39,910 --> 00:16:44,080
is pulled in and the test and it fails

00:16:42,430 --> 00:16:46,360
so maybe there is a race condition as

00:16:44,080 --> 00:16:50,170
that it was not picked up during the the

00:16:46,360 --> 00:16:53,040
check testing and so forth we have three

00:16:50,170 --> 00:16:57,400
thousand example and we split them in

00:16:53,040 --> 00:16:59,410
mm and red for training a 904 test in

00:16:57,400 --> 00:17:01,060
terms of hyper parameter we use kind of

00:16:59,410 --> 00:17:02,740
the standard settings the real

00:17:01,060 --> 00:17:04,510
activation function for the output layer

00:17:02,740 --> 00:17:08,350
sigmoid which worked very well with the

00:17:04,510 --> 00:17:10,570
binary classification default Optima

00:17:08,350 --> 00:17:12,579
optimizer again so the adaptive gradient

00:17:10,570 --> 00:17:15,190
descent it's a good one because

00:17:12,579 --> 00:17:17,110
basically it's adaptive so it doesn't

00:17:15,190 --> 00:17:18,459
really matter the learning rate the

00:17:17,110 --> 00:17:20,440
initial learning rate that you said

00:17:18,459 --> 00:17:25,120
because it will adapt itself and select

00:17:20,440 --> 00:17:27,520
a good value for that and so it's a

00:17:25,120 --> 00:17:30,640
neural network we have we started with

00:17:27,520 --> 00:17:34,870
five hidden layers and 100 units per

00:17:30,640 --> 00:17:39,730
layer and we run 500 epochs which means

00:17:34,870 --> 00:17:46,120
that we go 500 times over the our inputs

00:17:39,730 --> 00:17:49,240
we have 2100 samples for for training so

00:17:46,120 --> 00:17:50,980
we repeat that 500 times and then we use

00:17:49,240 --> 00:17:55,900
an input function it randomizes the

00:17:50,980 --> 00:18:01,650
order to make it more effective so then

00:17:55,900 --> 00:18:06,940
we run a different exam test so our main

00:18:01,650 --> 00:18:08,650
sorry key metric is accuracy that we

00:18:06,940 --> 00:18:10,960
looked at and we try different

00:18:08,650 --> 00:18:13,270
combination of features so for instance

00:18:10,960 --> 00:18:14,890
using just looking at the CPU just

00:18:13,270 --> 00:18:16,180
looking at the memory or the average

00:18:14,890 --> 00:18:18,240
load or combination of different

00:18:16,180 --> 00:18:22,320
switches features from the distant data

00:18:18,240 --> 00:18:27,580
and we saw that in terms of accuracy the

00:18:22,320 --> 00:18:31,270
combination of CPU and average system

00:18:27,580 --> 00:18:34,900
load was the most effective one so this

00:18:31,270 --> 00:18:38,290
looks a bit the other way around because

00:18:34,900 --> 00:18:42,190
accuracy the better the closer to 1 the

00:18:38,290 --> 00:18:45,520
better right so all this test that we

00:18:42,190 --> 00:18:47,020
did they had reasonable accuracy but

00:18:45,520 --> 00:18:49,420
they were close enough that if you would

00:18:47,020 --> 00:18:51,400
display them with bars that from between

00:18:49,420 --> 00:18:54,070
0 and 1 you wouldn't notice the

00:18:51,400 --> 00:18:56,170
difference enough so we actually

00:18:54,070 --> 00:18:57,640
displayed one minus the courtesy so that

00:18:56,170 --> 00:18:58,840
you can appreciate the difference in

00:18:57,640 --> 00:19:03,450
occurrence between the different

00:18:58,840 --> 00:19:05,430
experiments we also looked at the loss

00:19:03,450 --> 00:19:08,310
the loss was slightly worse than in this

00:19:05,430 --> 00:19:13,590
case but still acceptable and not very

00:19:08,310 --> 00:19:15,180
much different from the other one so and

00:19:13,590 --> 00:19:18,330
with this we achieved an accuracy of

00:19:15,180 --> 00:19:23,400
0.99 to is pretty good we think it's

00:19:18,330 --> 00:19:27,930
pretty reasonable on a 900 AD evaluation

00:19:23,400 --> 00:19:36,570
set it means about seven mistakes which

00:19:27,930 --> 00:19:42,270
is pretty good another thing that we

00:19:36,570 --> 00:19:45,990
wanted to see if it's how how well this

00:19:42,270 --> 00:19:48,480
training model could work with CI a

00:19:45,990 --> 00:19:50,400
different CI job so there are several CI

00:19:48,480 --> 00:19:53,250
jobs that are executed for OpenStack and

00:19:50,400 --> 00:19:55,620
for it we're an end-to-end test which is

00:19:53,250 --> 00:19:57,780
the tempest full job and then there is a

00:19:55,620 --> 00:20:00,000
Python 3 version of that which is very

00:19:57,780 --> 00:20:02,400
similar but slightly different because

00:20:00,000 --> 00:20:05,490
one of the component Swift doesn't

00:20:02,400 --> 00:20:07,170
support Python 3 so those tests are not

00:20:05,490 --> 00:20:12,560
executed that service is not trying

00:20:07,170 --> 00:20:12,560
running which affects the overall system

00:20:13,160 --> 00:20:17,180
matrix sorry

00:20:18,330 --> 00:20:24,780
so what we did pray in the model with

00:20:21,810 --> 00:20:27,210
the tempest full data set and we did

00:20:24,780 --> 00:20:31,380
around the evaluation or the tempest

00:20:27,210 --> 00:20:33,300
pool pie free so and the result was kind

00:20:31,380 --> 00:20:35,160
of okayed so the courtesy went down to

00:20:33,300 --> 00:20:39,860
from zero nine nine four two zero nine

00:20:35,160 --> 00:20:45,630
five three which is still reasonable but

00:20:39,860 --> 00:20:48,480
loss doubled and the precision against

00:20:45,630 --> 00:20:51,720
recall area it's much worse

00:20:48,480 --> 00:20:54,840
which means that it made mean that our

00:20:51,720 --> 00:20:56,610
data on the other CI job with me was too

00:20:54,840 --> 00:21:00,650
biased or maybe there was a little bit

00:20:56,610 --> 00:21:00,650
of overfitting that was happening there

00:21:00,980 --> 00:21:07,860
so to summarize for the binary

00:21:03,150 --> 00:21:12,000
classification we found that with ten

00:21:07,860 --> 00:21:14,730
seconds sampling is the best but one

00:21:12,000 --> 00:21:17,390
minute may be enough that we can get a

00:21:14,730 --> 00:21:22,170
very good accuracy and a very good

00:21:17,390 --> 00:21:24,540
precision versus recall curve the graph

00:21:22,170 --> 00:21:27,810
on the right and on the right side is

00:21:24,540 --> 00:21:29,940
the training loss while the training is

00:21:27,810 --> 00:21:35,460
done so it it goes down smoothly and

00:21:29,940 --> 00:21:37,590
it's pretty consistent so the second

00:21:35,460 --> 00:21:39,540
experiment is a multi class type of

00:21:37,590 --> 00:21:42,570
classification that we did as I was

00:21:39,540 --> 00:21:43,890
saying in the beginning the VMS are

00:21:42,570 --> 00:21:47,100
donated by different cloud providers

00:21:43,890 --> 00:21:49,410
around the world so one thing that we

00:21:47,100 --> 00:21:51,240
wanted to see if if you are able just

00:21:49,410 --> 00:21:54,860
looking at the system profiles to detect

00:21:51,240 --> 00:21:57,240
which cloud provider is hosting the test

00:21:54,860 --> 00:21:59,630
so the classes in this case are 10

00:21:57,240 --> 00:22:04,950
because there are 10 different that are

00:21:59,630 --> 00:22:07,020
sorry providing resources so we use a

00:22:04,950 --> 00:22:10,260
similar set up the data set is the same

00:22:07,020 --> 00:22:14,640
a same type of hyper parameters there

00:22:10,260 --> 00:22:18,480
and we ran again we used a resolution of

00:22:14,640 --> 00:22:21,840
1 minute the same features the loss

00:22:18,480 --> 00:22:25,080
converges but the curse we got was 0.6

00:22:21,840 --> 00:22:27,720
which is not really good not good enough

00:22:25,080 --> 00:22:29,820
definitely so we went back to doing

00:22:27,720 --> 00:22:31,770
different experiments again with try

00:22:29,820 --> 00:22:32,250
different combination and we try it ok

00:22:31,770 --> 00:22:34,919
let's

00:22:32,250 --> 00:22:37,650
let's put more data in it maybe we need

00:22:34,919 --> 00:22:40,080
more system features and we added like

00:22:37,650 --> 00:22:41,909
the memory we added we tried even with

00:22:40,080 --> 00:22:44,070
this guy oh but there was no significant

00:22:41,909 --> 00:22:48,960
improvement in the accuracy the top

00:22:44,070 --> 00:22:51,960
value there is 0.6 of the one - accuracy

00:22:48,960 --> 00:22:58,260
so it's not sorry the bottom one is 0.4

00:22:51,960 --> 00:23:00,240
so which again is the 0.6 accuracy so we

00:22:58,260 --> 00:23:03,150
said well maybe we are down sampling too

00:23:00,240 --> 00:23:06,480
much so we try changing resolution we

00:23:03,150 --> 00:23:08,730
tried 10 seconds 30 seconds maybe we

00:23:06,480 --> 00:23:11,429
tried even down simply more go into 1

00:23:08,730 --> 00:23:13,380
minute and 10 minutes but again the best

00:23:11,429 --> 00:23:16,289
we could get was not very good with

00:23:13,380 --> 00:23:17,820
slightly improvement here going down to

00:23:16,289 --> 00:23:21,059
10 seconds but it's not really

00:23:17,820 --> 00:23:22,980
significant we try changing the hyper

00:23:21,059 --> 00:23:26,039
parameters maybe we need the more

00:23:22,980 --> 00:23:28,169
complex or more simple a simpler network

00:23:26,039 --> 00:23:30,690
there so it turns out actually using

00:23:28,169 --> 00:23:35,480
three layers instead of five layers is

00:23:30,690 --> 00:23:38,159
lightly better we got 0 6 6 8 but still

00:23:35,480 --> 00:23:42,000
not not really useful we couldn't really

00:23:38,159 --> 00:23:43,890
do a good prediction there so what

00:23:42,000 --> 00:23:45,960
changed thing was actually going back to

00:23:43,890 --> 00:23:47,909
the data and this is one of the key

00:23:45,960 --> 00:23:50,760
outcome of the work that we have done

00:23:47,909 --> 00:23:52,289
and I could take away that is the data

00:23:50,760 --> 00:23:54,090
is really important so you should know

00:23:52,289 --> 00:23:56,130
your data and should look at your data

00:23:54,090 --> 00:24:00,030
understand what is going on as much as

00:23:56,130 --> 00:24:03,570
possible so as I was said I said before

00:24:00,030 --> 00:24:06,140
we had 10 classes but it turns out that

00:24:03,570 --> 00:24:09,179
some of these classes where actually

00:24:06,140 --> 00:24:11,250
could be converged because we had

00:24:09,179 --> 00:24:16,559
several regions from the same cloud

00:24:11,250 --> 00:24:18,120
provider so for example OVH at two

00:24:16,559 --> 00:24:20,360
different regions and another cloud

00:24:18,120 --> 00:24:23,820
providers at three different regions so

00:24:20,360 --> 00:24:25,440
we thought well maybe because there are

00:24:23,820 --> 00:24:27,570
the same cloud provider even if they're

00:24:25,440 --> 00:24:29,940
in different geographies they will use a

00:24:27,570 --> 00:24:31,500
similar type of setup they will use

00:24:29,940 --> 00:24:34,620
similar hardware so maybe we can

00:24:31,500 --> 00:24:39,270
collapse them into a single class this

00:24:34,620 --> 00:24:42,000
way reduce classes to 6 and we tried

00:24:39,270 --> 00:24:43,880
running our experiments again and what

00:24:42,000 --> 00:24:46,080
we saw is that the accuracy was

00:24:43,880 --> 00:24:49,590
increasing dramatically

00:24:46,080 --> 00:24:53,460
so we managed to achieve 0.9 as an

00:24:49,590 --> 00:24:55,830
accuracy so basically the takeaway from

00:24:53,460 --> 00:24:58,610
here here was that we were trying to

00:24:55,830 --> 00:25:01,169
separate things that were actually not

00:24:58,610 --> 00:25:03,860
not possible to separate because they're

00:25:01,169 --> 00:25:06,120
very similar so we had and so that was

00:25:03,860 --> 00:25:07,799
and that's why we couldn't get a good

00:25:06,120 --> 00:25:11,399
accuracy because it was kind of random

00:25:07,799 --> 00:25:13,860
for the model whether to detect that a

00:25:11,399 --> 00:25:18,149
certain test run in region a or region B

00:25:13,860 --> 00:25:19,919
for a certain term cloud provider so we

00:25:18,149 --> 00:25:21,929
did some extra tuning which are

00:25:19,919 --> 00:25:24,600
different than Network topologies and we

00:25:21,929 --> 00:25:29,940
managed to get to 0.95 in terms of

00:25:24,600 --> 00:25:35,610
accuracy with one-minute resolution and

00:25:29,940 --> 00:25:37,649
again using a CPU and system average we

00:25:35,610 --> 00:25:43,740
tried again and then doing this for the

00:25:37,649 --> 00:25:46,860
multi class applying to a different CI

00:25:43,740 --> 00:25:49,380
job so we use the same similar setup as

00:25:46,860 --> 00:25:52,740
before but this time it didn't work

00:25:49,380 --> 00:25:54,269
really that well so the best accuracy we

00:25:52,740 --> 00:25:56,309
could get doing this kind of training

00:25:54,269 --> 00:26:02,190
with one CI job and evaluation with a

00:25:56,309 --> 00:26:05,010
diferencia job it was around 0.77 so not

00:26:02,190 --> 00:26:12,149
not really good and also the the average

00:26:05,010 --> 00:26:16,049
loss and the loss increased very much so

00:26:12,149 --> 00:26:20,490
to summarize with the multi class

00:26:16,049 --> 00:26:23,279
example again we had this user CPU and

00:26:20,490 --> 00:26:25,049
the one minute load average was the best

00:26:23,279 --> 00:26:29,490
selection in terms of features from the

00:26:25,049 --> 00:26:31,740
decent data to use to predict this so

00:26:29,490 --> 00:26:34,889
it's an interesting take away that just

00:26:31,740 --> 00:26:39,179
looking at a CPU and a system average

00:26:34,889 --> 00:26:42,169
load every every minute on a system you

00:26:39,179 --> 00:26:44,700
can actually identify what is the

00:26:42,169 --> 00:26:47,639
underlying cloud provider that is

00:26:44,700 --> 00:26:49,679
providing the VM so this looks with it

00:26:47,639 --> 00:26:51,720
makes us think that this could be used

00:26:49,679 --> 00:26:54,450
maybe in future for doing things like

00:26:51,720 --> 00:26:57,480
creating specific benchmarks that could

00:26:54,450 --> 00:26:59,879
be then applied to different cloud

00:26:57,480 --> 00:27:07,529
providers and see you know recognize

00:26:59,879 --> 00:27:10,590
different type of fishes in there so to

00:27:07,529 --> 00:27:17,249
conclude I am not sure how we're doing

00:27:10,590 --> 00:27:22,950
the time yeah so to conclude as I was

00:27:17,249 --> 00:27:25,559
saying the key takeaway collect data you

00:27:22,950 --> 00:27:27,419
need a data set and even if there is a

00:27:25,559 --> 00:27:29,879
lot of open source software I think a

00:27:27,419 --> 00:27:33,749
big challenge will be open source data

00:27:29,879 --> 00:27:36,599
and a lot of data a lot of company a lot

00:27:33,749 --> 00:27:38,580
of closer the closer's data the good

00:27:36,599 --> 00:27:40,470
thing with large open source communities

00:27:38,580 --> 00:27:43,799
that they have this ability to produce a

00:27:40,470 --> 00:27:45,659
lot of data which is open source the

00:27:43,799 --> 00:27:50,009
data itself available for everyone and

00:27:45,659 --> 00:27:53,159
you can actually use that to run machine

00:27:50,009 --> 00:27:56,279
learning experiments the other thing is

00:27:53,159 --> 00:27:59,789
you need domain-specific knowledge on

00:27:56,279 --> 00:28:01,769
the data that you are using if you are a

00:27:59,789 --> 00:28:05,249
lawyer and you try to do a machine

00:28:01,769 --> 00:28:07,099
learning project for something related

00:28:05,249 --> 00:28:10,229
to accounting you probably won't work so

00:28:07,099 --> 00:28:14,609
in for in our case we worked for several

00:28:10,229 --> 00:28:17,309
years on tuning the QA and CI system and

00:28:14,609 --> 00:28:19,649
reading logs from for OpenStack so we

00:28:17,309 --> 00:28:24,539
knew the data so that was why it was a

00:28:19,649 --> 00:28:27,029
good fit I strongly recommend I mean in

00:28:24,539 --> 00:28:29,909
our case because it's just a simple

00:28:27,029 --> 00:28:32,279
neural network that we used it worked

00:28:29,909 --> 00:28:34,289
well just running on CPU so we did we

00:28:32,279 --> 00:28:37,999
tried running on GPU but we didn't see

00:28:34,289 --> 00:28:41,369
very much improvement but nonetheless

00:28:37,999 --> 00:28:43,979
you we recommend work with cloud tools

00:28:41,369 --> 00:28:47,489
run your training on the cloud there are

00:28:43,979 --> 00:28:49,649
several ways you can do that but you

00:28:47,489 --> 00:28:51,330
don't want to have a dependency like you

00:28:49,649 --> 00:28:53,580
start your training on your laptop and

00:28:51,330 --> 00:28:55,590
then battery runs out or you have to go

00:28:53,580 --> 00:28:57,659
somewhere you want to have this running

00:28:55,590 --> 00:29:00,090
in the cloud and yeah you want to have

00:28:57,659 --> 00:29:01,830
repeatable experiments and the ability

00:29:00,090 --> 00:29:03,720
to collect the data and and have it on

00:29:01,830 --> 00:29:05,940
the cloud especially because if it's

00:29:03,720 --> 00:29:07,619
just just you but maybe then some other

00:29:05,940 --> 00:29:09,239
colleagues or some friends joining the

00:29:07,619 --> 00:29:11,759
project and you want to share it and

00:29:09,239 --> 00:29:13,200
collaborate so if you have it if you

00:29:11,759 --> 00:29:19,500
have everything set up in the cloud

00:29:13,200 --> 00:29:21,299
makes things much much easier and also

00:29:19,500 --> 00:29:23,730
well we were able to confirm that the

00:29:21,299 --> 00:29:26,990
system load plays a role in the failures

00:29:23,730 --> 00:29:31,230
of course with the binary classification

00:29:26,990 --> 00:29:33,570
we actually got the information about

00:29:31,230 --> 00:29:36,840
that system is the tests are failed or

00:29:33,570 --> 00:29:40,559
there passed which we know already but

00:29:36,840 --> 00:29:42,659
you get a level of confidence as well

00:29:40,559 --> 00:29:47,549
with the prediction so in future we

00:29:42,659 --> 00:29:49,500
think we could use this confidence level

00:29:47,549 --> 00:29:51,750
that you can get about the failure of

00:29:49,500 --> 00:29:53,789
the test to build a system that allows

00:29:51,750 --> 00:29:57,659
you to see that a test even if it's

00:29:53,789 --> 00:29:59,190
reported as passed it's not identified

00:29:57,659 --> 00:30:01,799
as passed with high confidence by the

00:29:59,190 --> 00:30:06,960
system which might means which might

00:30:01,799 --> 00:30:10,230
able which might let you that detect

00:30:06,960 --> 00:30:12,539
early that see something is going wrong

00:30:10,230 --> 00:30:15,600
so situation real-life situation we had

00:30:12,539 --> 00:30:17,580
with the CI system is that page after

00:30:15,600 --> 00:30:21,779
page maybe some tests are added which

00:30:17,580 --> 00:30:23,429
address condition maybe the performance

00:30:21,779 --> 00:30:25,950
in one of the services degrade so it

00:30:23,429 --> 00:30:28,409
starts using more memory and necessity

00:30:25,950 --> 00:30:32,250
or simply there are more services so the

00:30:28,409 --> 00:30:34,309
CI jobs quality or did slightly degrades

00:30:32,250 --> 00:30:37,980
over time until it gets to a point where

00:30:34,309 --> 00:30:40,350
the failure rate for test should

00:30:37,980 --> 00:30:42,149
actually pass becomes too high so it's

00:30:40,350 --> 00:30:44,460
not good anymore for a CI system because

00:30:42,149 --> 00:30:48,000
it brings you a very bad developer

00:30:44,460 --> 00:30:52,380
experience so having the this kind of

00:30:48,000 --> 00:30:54,630
confidence evaluation whether the test

00:30:52,380 --> 00:30:57,690
is pass or fail maybe may be used to

00:30:54,630 --> 00:31:00,360
detect that something is getting worse

00:30:57,690 --> 00:31:02,880
in your test system right so that you

00:31:00,360 --> 00:31:09,750
can use these to predict that something

00:31:02,880 --> 00:31:12,000
is going wrong the wrong direction at a

00:31:09,750 --> 00:31:14,639
future work that we might do is to look

00:31:12,000 --> 00:31:16,590
at using similar techniques for other

00:31:14,639 --> 00:31:20,279
type of data like the logs from

00:31:16,590 --> 00:31:25,230
application for instance for other

00:31:20,279 --> 00:31:26,639
features we could look at predicting

00:31:25,230 --> 00:31:27,150
other things like the type of failure

00:31:26,639 --> 00:31:30,360
that have

00:31:27,150 --> 00:31:33,660
and the difficult bit with that is that

00:31:30,360 --> 00:31:35,970
you really net then need a human curated

00:31:33,660 --> 00:31:37,710
data set so basically if you have

00:31:35,970 --> 00:31:40,080
engineers that usually look at the logs

00:31:37,710 --> 00:31:43,410
and they say oh well this failed yes but

00:31:40,080 --> 00:31:46,650
it failed because I don't know flight

00:31:43,410 --> 00:31:51,480
provider X at an outage or because there

00:31:46,650 --> 00:31:52,980
was a network issue somewhere so you

00:31:51,480 --> 00:31:55,260
could use this information start

00:31:52,980 --> 00:31:56,880
collecting this information and label

00:31:55,260 --> 00:31:58,980
your experiment with this information

00:31:56,880 --> 00:32:01,830
and then you could use this to Train

00:31:58,980 --> 00:32:04,050
in a similar way model to detect the

00:32:01,830 --> 00:32:05,550
type of failure but this requires a lot

00:32:04,050 --> 00:32:09,650
of work because you need to connect to

00:32:05,550 --> 00:32:12,900
collect all these human input basically

00:32:09,650 --> 00:32:16,080
another thing that we thought about for

00:32:12,900 --> 00:32:19,740
future work is that you could try to

00:32:16,080 --> 00:32:21,840
define some metric that allows us trying

00:32:19,740 --> 00:32:24,600
to do clustering on the data clustering

00:32:21,840 --> 00:32:26,520
is interesting it's very difficult out

00:32:24,600 --> 00:32:28,920
but if you manage to find the right

00:32:26,520 --> 00:32:31,830
metric you may be able then to display

00:32:28,920 --> 00:32:34,880
your data in some n dimensional space or

00:32:31,830 --> 00:32:39,650
two or three dimensional space and see

00:32:34,880 --> 00:32:42,420
the specific test runs live in a certain

00:32:39,650 --> 00:32:44,940
in clustering certain areas of the space

00:32:42,420 --> 00:32:48,540
and that allows you to identify that

00:32:44,940 --> 00:32:50,670
this CI job or these data samples they

00:32:48,540 --> 00:32:52,200
have something in common and this allows

00:32:50,670 --> 00:32:53,760
you basically it's a way that you can

00:32:52,200 --> 00:32:58,050
use to automatically build classes

00:32:53,760 --> 00:33:02,790
without doing the human created data set

00:32:58,050 --> 00:33:05,900
before other things that we want to do

00:33:02,790 --> 00:33:09,660
explore more for the job Portability and

00:33:05,900 --> 00:33:11,580
where if we are able to move forward

00:33:09,660 --> 00:33:13,470
with getting interesting information

00:33:11,580 --> 00:33:20,190
maybe from the clustering we could use

00:33:13,470 --> 00:33:24,179
this data also in the real CI system so

00:33:20,190 --> 00:33:26,880
the talk is available on github and all

00:33:24,179 --> 00:33:28,350
the code that we wrote for Cemal project

00:33:26,880 --> 00:33:32,750
is also available on github

00:33:28,350 --> 00:33:32,750
and if you have any question

00:33:34,059 --> 00:33:41,289
sorry could you explain go through again

00:33:37,749 --> 00:33:44,320
why the that why the prediction of the

00:33:41,289 --> 00:33:47,440
pass and failure is useful from what I

00:33:44,320 --> 00:33:49,570
could tell the the input data for the

00:33:47,440 --> 00:33:56,529
four prediction is only the load levels

00:33:49,570 --> 00:34:03,639
right of the test being run right there

00:33:56,529 --> 00:34:07,059
miss something so as it is now it's

00:34:03,639 --> 00:34:09,520
information we already have yeah so in

00:34:07,059 --> 00:34:11,529
that sense it's not useful it's useful

00:34:09,520 --> 00:34:13,869
in a way in the sense that it it proves

00:34:11,529 --> 00:34:18,039
us that there is information that isn't

00:34:13,869 --> 00:34:20,039
bad that in system data from this load

00:34:18,039 --> 00:34:22,720
profiles we are able to infer something

00:34:20,039 --> 00:34:27,629
that is whether the test is pass or fail

00:34:22,720 --> 00:34:31,539
with a very good accuracy so this

00:34:27,629 --> 00:34:33,279
basically it was the the beginning in

00:34:31,539 --> 00:34:34,750
the beginning what prompted us to

00:34:33,279 --> 00:34:37,389
continue and do other kind of

00:34:34,750 --> 00:34:39,190
experiments and look at the multi-class

00:34:37,389 --> 00:34:42,909
classification for instance and try to

00:34:39,190 --> 00:34:44,409
extract more data from this load data

00:34:42,909 --> 00:34:50,200
that we have on the system that's

00:34:44,409 --> 00:34:52,119
basically it I mean the other thing that

00:34:50,200 --> 00:34:55,690
I mentioned a minutes is not something

00:34:52,119 --> 00:34:57,490
that we're really proud but because you

00:34:55,690 --> 00:34:59,500
can get an estimation of the confidence

00:34:57,490 --> 00:35:01,690
of the prediction you could build a

00:34:59,500 --> 00:35:04,390
system where you get you can say well

00:35:01,690 --> 00:35:06,160
the system predicts that this is passed

00:35:04,390 --> 00:35:08,740
off a with a certain level of confidence

00:35:06,160 --> 00:35:14,020
and then you can use this confidence

00:35:08,740 --> 00:35:16,059
level in your CI system to say well if

00:35:14,020 --> 00:35:18,039
I'm running the same test the same type

00:35:16,059 --> 00:35:19,630
of CI jobs and the the confidence

00:35:18,039 --> 00:35:22,329
degrades it means that something is

00:35:19,630 --> 00:35:24,490
changing in that in in the in the system

00:35:22,329 --> 00:35:27,220
we are testing okay and so you could use

00:35:24,490 --> 00:35:29,470
this information as a kind of warning to

00:35:27,220 --> 00:35:30,910
look in what is going what is going on

00:35:29,470 --> 00:35:32,500
it could be that for instance the memory

00:35:30,910 --> 00:35:34,839
utilization of your services increased

00:35:32,500 --> 00:35:38,470
and that changed a lot profile okay and

00:35:34,839 --> 00:35:39,910
that degrades the level of the

00:35:38,470 --> 00:35:44,339
prediction the confidence in the

00:35:39,910 --> 00:35:44,339

YouTube URL: https://www.youtube.com/watch?v=Ee242vZrj6w


