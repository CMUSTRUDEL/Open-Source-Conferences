Title: TensorFlow 2.0 is Coming! What's New? How do I Migrate? by Aurélien Géron
Publication date: 2019-03-27
Playlist: FOSSASIA Summit 2019 - Artificial Intelligence
Description: 
	16 March 2019 16:40, Event Hall 2-1

TensorFlow is Google's popular deep learning library. The upcoming 2.0 version will bring many important improvements, with a strong focus on usability. It will be much easier to learn and use than before (in particular thanks to the Keras API), while preserving TensorFlow's key advantages over competing libraries such as Facebook's PyTorch or Microsoft's Cognitive Toolkit (CNTK): namely its amazing portability, its support for high-end hardware acceleration (including TPUs), its rich API and its huge ecosystem. In this talk, you will learn what's new in TensorFlow 2 and how to handle the migration from TF 1.x.

Try out these Open Source Jupyter notebooks to learn TensorFlow 2.0: https://github.com/ageron/tf2_course
Captions: 
	00:00:00,000 --> 00:00:08,880
speaker is over Tianjin he's an AI

00:00:03,720 --> 00:00:11,429
expert at Kiwi soft which he found it is

00:00:08,880 --> 00:00:16,379
or thought of best-selling books at

00:00:11,429 --> 00:00:22,199
already but tensorflow and former Google

00:00:16,379 --> 00:00:24,990
video classification team alright okay

00:00:22,199 --> 00:00:27,750
just a show of hand who here uses tensor

00:00:24,990 --> 00:00:30,810
flow professionally or who hasn't yeah

00:00:27,750 --> 00:00:32,880
who has used that I mean alright okay so

00:00:30,810 --> 00:00:35,070
not everyone I'll try to just a high

00:00:32,880 --> 00:00:36,329
level then so thanks for the

00:00:35,070 --> 00:00:36,870
introduction and thanks for the

00:00:36,329 --> 00:00:39,360
invitation

00:00:36,870 --> 00:00:41,340
my name is Owen and Jeff oh you already

00:00:39,360 --> 00:00:43,950
mentioned that I worked as the lead of

00:00:41,340 --> 00:00:46,110
YouTube's a video classification team in

00:00:43,950 --> 00:00:47,820
Paris which means my job was to tag

00:00:46,110 --> 00:00:49,590
every video with you know what it's

00:00:47,820 --> 00:00:52,110
about which is mostly cats of course on

00:00:49,590 --> 00:00:53,309
YouTube and then I wrote this book hands

00:00:52,110 --> 00:00:54,350
on machine learning with psychic

00:00:53,309 --> 00:00:57,270
learning tensorflow

00:00:54,350 --> 00:00:59,699
and the second edition is coming up

00:00:57,270 --> 00:01:02,039
anyway today the talk is about

00:00:59,699 --> 00:01:04,710
tensorflow to which you might have heard

00:01:02,039 --> 00:01:08,010
has just not been released but the alpha

00:01:04,710 --> 00:01:10,740
version has just been released and i'll

00:01:08,010 --> 00:01:14,760
explain what's new and how you migrate

00:01:10,740 --> 00:01:16,439
from 1001 to tends to flow to ok so yeah

00:01:14,760 --> 00:01:19,590
it was just announced I think 10 days

00:01:16,439 --> 00:01:21,600
ago so at the tensorflow dev summit and

00:01:19,590 --> 00:01:24,479
they announced the alpha version with a

00:01:21,600 --> 00:01:27,560
lot of improvements and the final

00:01:24,479 --> 00:01:29,700
release candidate is scheduled for q2

00:01:27,560 --> 00:01:31,140
2019 whatever that means I guess it's

00:01:29,700 --> 00:01:36,329
probably going to be like end of June

00:01:31,140 --> 00:01:38,460
possibly but we'll see and why did they

00:01:36,329 --> 00:01:40,650
actually create censor flow - it's a

00:01:38,460 --> 00:01:43,530
major change so it's going to impact a

00:01:40,650 --> 00:01:44,909
lot of people and major changes well

00:01:43,530 --> 00:01:46,710
allow you to do a lot of cleanup and

00:01:44,909 --> 00:01:48,720
improvements even though they're

00:01:46,710 --> 00:01:51,000
breaking and the the the reason the

00:01:48,720 --> 00:01:52,619
motivation initially is this these are

00:01:51,000 --> 00:01:55,590
the number of citations and machine

00:01:52,619 --> 00:01:57,799
learning papers that's early 2018 so the

00:01:55,590 --> 00:02:00,060
graphs have changed I should update them

00:01:57,799 --> 00:02:02,490
but you can see that tensorflow

00:02:00,060 --> 00:02:06,689
is you know by far at the time the most

00:02:02,490 --> 00:02:08,819
cited library in scientific papers you

00:02:06,689 --> 00:02:10,950
know cafe Theano torch were all kind of

00:02:08,819 --> 00:02:12,510
stagnating or going down and then you

00:02:10,950 --> 00:02:13,920
have tensorflow and Cara's shooting up

00:02:12,510 --> 00:02:15,569
but you also have this pie

00:02:13,920 --> 00:02:17,340
Torche going up and a lot of the pi

00:02:15,569 --> 00:02:19,260
torch users you might have heard are

00:02:17,340 --> 00:02:20,910
kind of migrating from tensorflow one to

00:02:19,260 --> 00:02:23,099
pi torch and everybody says you know

00:02:20,910 --> 00:02:24,209
it's it's much simpler it's really it's

00:02:23,099 --> 00:02:26,880
really nice and that's true

00:02:24,209 --> 00:02:30,209
so the tensorflow basically had to react

00:02:26,880 --> 00:02:32,160
and and the first improvement is on

00:02:30,209 --> 00:02:34,440
simplicity to make it much easier to use

00:02:32,160 --> 00:02:36,660
so let's look at a little bit of pi

00:02:34,440 --> 00:02:39,090
torch code if you want to compute like 1

00:02:36,660 --> 00:02:42,330
plus 1/2 plus 1/4 and so on this is what

00:02:39,090 --> 00:02:44,400
it looks like it's very very natural you

00:02:42,330 --> 00:02:48,090
know python code you can just create

00:02:44,400 --> 00:02:50,819
your tensor X y equals to 1 then this

00:02:48,090 --> 00:02:54,000
iteration X equal X plus y and y equals

00:02:50,819 --> 00:02:56,430
y divided by 2 and run this 50 times and

00:02:54,000 --> 00:02:57,959
you converge to 2 fantastic it's very

00:02:56,430 --> 00:03:00,600
natural now let's look at the equivalent

00:02:57,959 --> 00:03:02,370
code in tensorflow 1 it would look like

00:03:00,600 --> 00:03:03,989
this you would have the first phase

00:03:02,370 --> 00:03:05,880
which is the construction phase where

00:03:03,989 --> 00:03:08,730
you actually build a computation graph

00:03:05,880 --> 00:03:10,709
no computation is actually executed here

00:03:08,730 --> 00:03:13,049
it's just building a computation graph

00:03:10,709 --> 00:03:14,100
and you know you create all the

00:03:13,049 --> 00:03:16,260
operations that you'll need to

00:03:14,100 --> 00:03:18,870
manipulates plus this thing that says

00:03:16,260 --> 00:03:20,940
you know I will initialize all the

00:03:18,870 --> 00:03:22,500
variables and this is where the actual

00:03:20,940 --> 00:03:24,690
computation start you need to create a

00:03:22,500 --> 00:03:26,880
session initialize all the variables and

00:03:24,690 --> 00:03:28,470
then run your iteration where you you

00:03:26,880 --> 00:03:29,910
know explicitly say I want to execute

00:03:28,470 --> 00:03:32,700
this part of the graph in this part of

00:03:29,910 --> 00:03:34,680
the graph so if you think of computation

00:03:32,700 --> 00:03:36,840
graphs or tensorflow graphs as kind of a

00:03:34,680 --> 00:03:38,820
language basically here you're writing a

00:03:36,840 --> 00:03:40,410
program in that language and here you're

00:03:38,820 --> 00:03:43,140
executing that program so it's kind of

00:03:40,410 --> 00:03:45,239
meta programming and it has a lot of

00:03:43,140 --> 00:03:47,579
advantages I'll get to it but clearly

00:03:45,239 --> 00:03:49,109
you know for most simple work you don't

00:03:47,579 --> 00:03:51,060
want to be coding like this it's it's

00:03:49,109 --> 00:03:53,640
it's a little bit too complicated and

00:03:51,060 --> 00:03:55,530
verbose the other thing is it can hide

00:03:53,640 --> 00:03:58,739
subtle bugs like for example if I decide

00:03:55,530 --> 00:03:59,069
to execute add up and divide up in one

00:03:58,739 --> 00:04:02,040
shot

00:03:59,069 --> 00:04:03,870
well tensorflow is not or tensorflow one

00:04:02,040 --> 00:04:06,030
is not going to see any dependencies

00:04:03,870 --> 00:04:07,500
between these two operations as far as

00:04:06,030 --> 00:04:09,720
its concerns those are completely

00:04:07,500 --> 00:04:11,700
separate and independent so it's happy

00:04:09,720 --> 00:04:13,859
to launch them in parallel and therefore

00:04:11,700 --> 00:04:16,079
the order of execution is not guaranteed

00:04:13,859 --> 00:04:17,639
and as you can see the result is no no

00:04:16,079 --> 00:04:18,810
longer - in fact if you execute this

00:04:17,639 --> 00:04:21,510
code multiple times you'll get different

00:04:18,810 --> 00:04:23,550
results every time and that's kind of

00:04:21,510 --> 00:04:24,930
hard to catch it's not super frequent

00:04:23,550 --> 00:04:26,039
but it does happen or

00:04:24,930 --> 00:04:28,889
did happen in since therefore one is

00:04:26,039 --> 00:04:30,150
fixed in two is I'll I'll explain and

00:04:28,889 --> 00:04:32,910
the other thing is it's a bit hard to

00:04:30,150 --> 00:04:35,669
debug say you have X and Y is a function

00:04:32,910 --> 00:04:38,820
of X and Z is a function of Y well if

00:04:35,669 --> 00:04:41,009
you execute Z and you get a value that's

00:04:38,820 --> 00:04:43,500
actually not a number well you're kind

00:04:41,009 --> 00:04:45,750
of stuck why do I get an and value here

00:04:43,500 --> 00:04:48,150
if you could debug you might end up

00:04:45,750 --> 00:04:50,250
saying oh wait I'm dividing by 0 over

00:04:48,150 --> 00:04:52,440
there that's the reason but as far as

00:04:50,250 --> 00:04:54,720
you know python is concerned this is

00:04:52,440 --> 00:04:56,639
just a function call that directly gets

00:04:54,720 --> 00:04:58,199
executed in c++ it doesn't see the

00:04:56,639 --> 00:05:00,090
details so in the stack trace all you

00:04:58,199 --> 00:05:01,620
get is boom there's an exception on this

00:05:00,090 --> 00:05:03,419
line I don't know any you know any more

00:05:01,620 --> 00:05:05,820
of that in tensor for one there actually

00:05:03,419 --> 00:05:07,259
is a graph debugger but not but not you

00:05:05,820 --> 00:05:10,169
know not many people use it and it's all

00:05:07,259 --> 00:05:12,900
bit tricky to use the other thing that's

00:05:10,169 --> 00:05:15,270
harder is to profile for the same reason

00:05:12,900 --> 00:05:19,289
like if I try to execute a time it on

00:05:15,270 --> 00:05:21,330
you know running Z here I'll get some

00:05:19,289 --> 00:05:23,699
number that's you know how fast it runs

00:05:21,330 --> 00:05:26,009
but I won't actually have the detail in

00:05:23,699 --> 00:05:27,690
terms of you know this operation of that

00:05:26,009 --> 00:05:30,599
operation that operation which one runs

00:05:27,690 --> 00:05:32,220
faster it's not clear for exactly the

00:05:30,599 --> 00:05:33,630
same reason and again there are tools

00:05:32,220 --> 00:05:36,240
that were developed that allow you to

00:05:33,630 --> 00:05:37,530
you know profile graphs but again not

00:05:36,240 --> 00:05:40,020
many people use them and it's not very

00:05:37,530 --> 00:05:42,810
natural so the solution came in

00:05:40,020 --> 00:05:44,550
tensorflow 1.4 so it's not at tensorflow

00:05:42,810 --> 00:05:47,190
to this actually the solution came way

00:05:44,550 --> 00:05:48,599
earlier it's called your execution when

00:05:47,190 --> 00:05:51,449
you start your tensorflow program you're

00:05:48,599 --> 00:05:53,729
just activated and you know magically

00:05:51,449 --> 00:05:55,800
for now it's not in graph mode anymore

00:05:53,729 --> 00:05:57,210
so any operation you execute actually

00:05:55,800 --> 00:05:59,909
gets run right away and you get the

00:05:57,210 --> 00:06:01,680
answer right away so as you can see it's

00:05:59,909 --> 00:06:04,620
almost the same code as in pi torch

00:06:01,680 --> 00:06:06,150
right there's this extra line but this

00:06:04,620 --> 00:06:08,099
line actually disappears because in

00:06:06,150 --> 00:06:10,800
tensorflow too eager mode is the default

00:06:08,099 --> 00:06:12,419
so when you look at a you know

00:06:10,800 --> 00:06:15,860
tensorflow to program or a PI torch

00:06:12,419 --> 00:06:18,720
program they can look very very similar

00:06:15,860 --> 00:06:20,820
this last thing here is because these

00:06:18,720 --> 00:06:22,979
are tensors if you want to see the value

00:06:20,820 --> 00:06:24,300
without having a long you know line here

00:06:22,979 --> 00:06:28,139
you can just call numpy it gives you the

00:06:24,300 --> 00:06:30,270
actual value all right so now that we've

00:06:28,139 --> 00:06:32,070
seen that basically eager execution is

00:06:30,270 --> 00:06:33,810
much much simpler to use you might

00:06:32,070 --> 00:06:36,120
wonder why did we use graph mode in the

00:06:33,810 --> 00:06:37,950
first place why would you want to use it

00:06:36,120 --> 00:06:40,560
and the first reason I've

00:06:37,950 --> 00:06:43,140
to earlier if you do B equals a plus 3

00:06:40,560 --> 00:06:45,150
and C equals 8 times 5 in graph mode

00:06:43,140 --> 00:06:47,100
this will lead to a graph that looks

00:06:45,150 --> 00:06:50,190
somewhat like this actually exactly like

00:06:47,100 --> 00:06:52,920
this and it's easy to see when you look

00:06:50,190 --> 00:06:54,750
at this graph that this addition

00:06:52,920 --> 00:06:56,550
operation this multiplication operation

00:06:54,750 --> 00:06:58,590
are completely independent and they can

00:06:56,550 --> 00:07:00,780
be run in parallel that's the first

00:06:58,590 --> 00:07:02,880
thing you gain from you know having a

00:07:00,780 --> 00:07:04,890
computation graph is that tensorflow can

00:07:02,880 --> 00:07:06,240
actually run automatically things in

00:07:04,890 --> 00:07:08,130
parallel without you having to

00:07:06,240 --> 00:07:11,310
manipulate any threats or anything it

00:07:08,130 --> 00:07:13,080
paralyzes everything for you so if you

00:07:11,310 --> 00:07:14,550
have multiple cores you know different

00:07:13,080 --> 00:07:17,220
operations will not run actually in

00:07:14,550 --> 00:07:18,840
different cores the second benefit is

00:07:17,220 --> 00:07:21,030
that if you actually express everything

00:07:18,840 --> 00:07:23,730
in a graph that graph can actually be

00:07:21,030 --> 00:07:27,510
run entirely on an accelerator like a

00:07:23,730 --> 00:07:29,790
GPU or a TPU and without all the

00:07:27,510 --> 00:07:32,760
back-and-forth with Python you know with

00:07:29,790 --> 00:07:34,410
the CPU I mean and this can speed things

00:07:32,760 --> 00:07:36,980
up a lot if you're if you know if you're

00:07:34,410 --> 00:07:39,630
manipulating operations of huge matrices

00:07:36,980 --> 00:07:42,480
then it might not make a big difference

00:07:39,630 --> 00:07:44,700
because the the overhead of going back

00:07:42,480 --> 00:07:46,860
to to python might not be that huge but

00:07:44,700 --> 00:07:48,900
if you have lots of lots and lots of

00:07:46,860 --> 00:07:50,820
smaller operations that need to run

00:07:48,900 --> 00:07:53,820
really fast this can make a big

00:07:50,820 --> 00:07:56,490
difference another benefit of graphs is

00:07:53,820 --> 00:07:58,490
that since you have this kind of

00:07:56,490 --> 00:08:00,480
representation of all your computations

00:07:58,490 --> 00:08:02,790
tensorflow can actually analyze the

00:08:00,480 --> 00:08:04,620
graph and possibly optimize it so of

00:08:02,790 --> 00:08:08,010
course like getting rid of nodes that

00:08:04,620 --> 00:08:10,800
you don't use or in the case of excel a

00:08:08,010 --> 00:08:13,860
it can do things like find pairs of

00:08:10,800 --> 00:08:15,870
operations that you know for which an

00:08:13,860 --> 00:08:18,060
actual optimized implementation exists

00:08:15,870 --> 00:08:19,410
and it will swap out the pair of

00:08:18,060 --> 00:08:21,990
operations and replace it with the

00:08:19,410 --> 00:08:23,880
optimized version okay so it can speed

00:08:21,990 --> 00:08:25,380
things up that way so that's a benefit

00:08:23,880 --> 00:08:29,160
of having something symbolic that you

00:08:25,380 --> 00:08:31,140
can play with and another benefit and I

00:08:29,160 --> 00:08:33,210
guess to me this is like one of the

00:08:31,140 --> 00:08:36,750
biggest advantages of tensorflow over

00:08:33,210 --> 00:08:39,240
the competition it's the portability you

00:08:36,750 --> 00:08:41,910
you take a tensor flow graph and you can

00:08:39,240 --> 00:08:43,410
just run it on a mobile device you can

00:08:41,910 --> 00:08:44,850
run it in a web browser

00:08:43,410 --> 00:08:46,590
I invite you if you haven't seen it

00:08:44,850 --> 00:08:49,410
already to go to tensorflow org slash

00:08:46,590 --> 00:08:51,779
j/s slash demos you'll see tensorflow

00:08:49,410 --> 00:08:53,790
jas directly running the browser here

00:08:51,779 --> 00:08:56,610
I played this little game and neural-net

00:08:53,790 --> 00:08:57,870
was actually trained in the browser but

00:08:56,610 --> 00:08:59,970
you don't have to train in the browser

00:08:57,870 --> 00:09:01,529
you can train it in Python export the

00:08:59,970 --> 00:09:03,810
graph and then run it in your browser so

00:09:01,529 --> 00:09:07,110
portability is also one key elements of

00:09:03,810 --> 00:09:08,399
having graphs and then you know of

00:09:07,110 --> 00:09:10,769
course you can run the same graph on

00:09:08,399 --> 00:09:12,959
your servers and and go to production

00:09:10,769 --> 00:09:16,019
and all and this will be the same graph

00:09:12,959 --> 00:09:18,839
running in all these environments okay

00:09:16,019 --> 00:09:20,519
so the good news is in tensorflow 2 you

00:09:18,839 --> 00:09:23,009
can actually have the best of both

00:09:20,519 --> 00:09:24,899
worlds by default everything is eager

00:09:23,009 --> 00:09:26,459
meaning you know every time I execute a

00:09:24,899 --> 00:09:28,199
tensorflow operation I get the result

00:09:26,459 --> 00:09:31,470
right away so it's easy to debug it's

00:09:28,199 --> 00:09:33,809
easy to profile it's natural to code but

00:09:31,470 --> 00:09:36,089
if I want to speed up this little

00:09:33,809 --> 00:09:38,790
function here I can do that very simply

00:09:36,089 --> 00:09:42,749
just add this decorator at TF dot

00:09:38,790 --> 00:09:44,850
function and that's about it everything

00:09:42,749 --> 00:09:47,370
will happen under the hood this function

00:09:44,850 --> 00:09:49,889
will now be a graph function if you will

00:09:47,370 --> 00:09:52,649
so the way it works is the first time

00:09:49,889 --> 00:09:55,769
you actually call this function with

00:09:52,649 --> 00:09:57,809
actual values like 2 & 3 well the

00:09:55,769 --> 00:10:00,449
function the Python function will be

00:09:57,809 --> 00:10:02,759
traced and by tracing I mean it's

00:10:00,449 --> 00:10:05,370
actually going to be executed but not

00:10:02,759 --> 00:10:07,379
with the values 2 & 3 instead it's going

00:10:05,370 --> 00:10:09,329
to be called with so called symbolic

00:10:07,379 --> 00:10:11,519
tensors which if you know tensorflow 1

00:10:09,329 --> 00:10:13,589
you can think of as placeholders okay so

00:10:11,519 --> 00:10:16,019
it's basically calling this function

00:10:13,589 --> 00:10:18,000
that you wrote with this thing here that

00:10:16,019 --> 00:10:20,250
says I'm a tensor I'm a scalar there's

00:10:18,000 --> 00:10:23,550
no shape like shape is empty and it's a

00:10:20,250 --> 00:10:25,259
float32 and and then the operations

00:10:23,550 --> 00:10:28,410
don't actually compute anything they

00:10:25,259 --> 00:10:30,899
build a graph right so this will be run

00:10:28,410 --> 00:10:32,550
in graph mode and once the function is

00:10:30,899 --> 00:10:35,009
traced well you have this nice little

00:10:32,550 --> 00:10:36,509
graph in memory that's cached in this

00:10:35,009 --> 00:10:38,429
tensor flow function so the next time

00:10:36,509 --> 00:10:40,110
that you call this same function with

00:10:38,429 --> 00:10:40,920
different you know scalar values

00:10:40,110 --> 00:10:42,750
different floats

00:10:40,920 --> 00:10:44,550
it'll reuse the same graph of course it

00:10:42,750 --> 00:10:46,589
won't create it every time and so you

00:10:44,550 --> 00:10:48,629
can get you know there a lot of speed up

00:10:46,589 --> 00:10:50,910
with this so the graph might look

00:10:48,629 --> 00:10:52,980
something like this with x and y being

00:10:50,910 --> 00:10:54,449
placeholders if you can actually still

00:10:52,980 --> 00:10:55,949
go and look at the graph that was

00:10:54,449 --> 00:10:58,949
generated and those would be actual

00:10:55,949 --> 00:11:00,420
placeholders in the graph okay but you

00:10:58,949 --> 00:11:01,980
don't need to handle them yourself you

00:11:00,420 --> 00:11:03,600
don't need to feed anything basically

00:11:01,980 --> 00:11:05,800
you don't need to manipulate graphs

00:11:03,600 --> 00:11:08,110
sessions placeholders

00:11:05,800 --> 00:11:11,800
these are handled automatically now so

00:11:08,110 --> 00:11:14,290
right you get the power of graphs with

00:11:11,800 --> 00:11:18,430
the ease of programming and all the

00:11:14,290 --> 00:11:20,529
benefits of eager execution okay so you

00:11:18,430 --> 00:11:22,390
can mix and match eager in graph mode so

00:11:20,529 --> 00:11:25,600
for example here this I'm coming back to

00:11:22,390 --> 00:11:27,130
the 1 plus 1/2 plus 1/4 thing and you

00:11:25,600 --> 00:11:28,630
can see that this is the the whole code

00:11:27,130 --> 00:11:29,980
and I've decided for some reason that I

00:11:28,630 --> 00:11:33,370
just want to speed up this little piece

00:11:29,980 --> 00:11:34,870
here and this this is the graph function

00:11:33,370 --> 00:11:37,750
or the tensorflow function as they're

00:11:34,870 --> 00:11:40,060
called but I have this external loop and

00:11:37,750 --> 00:11:42,760
it just calls this on and on and on and

00:11:40,060 --> 00:11:44,110
I get the right result by the way the

00:11:42,760 --> 00:11:46,660
problem that I mentioned earlier that

00:11:44,110 --> 00:11:48,550
the addition and division operation

00:11:46,660 --> 00:11:49,899
might run in parallel and you don't know

00:11:48,550 --> 00:11:51,910
which one will run first that has been

00:11:49,899 --> 00:11:54,730
fixed and tensorflow to anything that

00:11:51,910 --> 00:11:56,800
affects the state or a stateful object

00:11:54,730 --> 00:11:58,420
such as a variable will actually

00:11:56,800 --> 00:12:01,089
automatically be run in the right order

00:11:58,420 --> 00:12:02,410
by the graph ok so intense flow ensures

00:12:01,089 --> 00:12:04,060
that this will be run first and this

00:12:02,410 --> 00:12:07,510
will be run second it won't try to

00:12:04,060 --> 00:12:10,089
paralyze it and this is fixed ok now

00:12:07,510 --> 00:12:13,260
also notice that I didn't need to create

00:12:10,089 --> 00:12:15,640
like a global variables initializer I

00:12:13,260 --> 00:12:17,110
just create a variable and it's

00:12:15,640 --> 00:12:19,120
completely natural it's like

00:12:17,110 --> 00:12:21,760
object-oriented this is where the state

00:12:19,120 --> 00:12:23,529
is it's nowhere else there's no like

00:12:21,760 --> 00:12:25,959
default graph or default session or

00:12:23,529 --> 00:12:28,620
anything everything is linked to a

00:12:25,959 --> 00:12:30,399
Python object and so it's pretty natural

00:12:28,620 --> 00:12:31,540
don't need to handle the control

00:12:30,399 --> 00:12:34,329
dependencies I mentioned that already

00:12:31,540 --> 00:12:35,950
now look at this loop here I have this

00:12:34,329 --> 00:12:38,350
for loop that's outside of a tensorflow

00:12:35,950 --> 00:12:39,399
function so it's just running on and on

00:12:38,350 --> 00:12:41,560
and on and calling my tensorflow

00:12:39,399 --> 00:12:43,480
function but i actually actually put it

00:12:41,560 --> 00:12:46,899
inside another tensor function if I

00:12:43,480 --> 00:12:49,480
wanted to you can do that and this loop

00:12:46,899 --> 00:12:52,270
here will actually run in the graph it's

00:12:49,480 --> 00:12:54,880
not going to run during tracing time at

00:12:52,270 --> 00:12:57,699
tracing time because of some magic

00:12:54,880 --> 00:13:00,160
called autograph what tensorflow does is

00:12:57,699 --> 00:13:02,890
it captures the source code of this

00:13:00,160 --> 00:13:05,529
function here analyzes it and notices

00:13:02,890 --> 00:13:07,630
hey I'm actually calling TF dot range

00:13:05,529 --> 00:13:11,140
and not range ok and since I'm calling

00:13:07,630 --> 00:13:13,089
TF dot range here this means that this

00:13:11,140 --> 00:13:13,600
should actually be ported to the graph

00:13:13,089 --> 00:13:16,449
itself

00:13:13,600 --> 00:13:18,730
so during tracing it'll just skip this

00:13:16,449 --> 00:13:19,240
and this loop will be executed when you

00:13:18,730 --> 00:13:21,519
when you

00:13:19,240 --> 00:13:23,619
execute the graph itself right so it

00:13:21,519 --> 00:13:26,290
makes it very easy to write like dynamic

00:13:23,619 --> 00:13:27,100
models like our own ends and things like

00:13:26,290 --> 00:13:30,249
that

00:13:27,100 --> 00:13:32,290
using this kind of code

00:13:30,249 --> 00:13:34,899
I could actually remove the TF function

00:13:32,290 --> 00:13:36,610
declaration up there because here I'm

00:13:34,899 --> 00:13:38,860
calling this function run loop

00:13:36,610 --> 00:13:41,709
it's a TF function and it depends on run

00:13:38,860 --> 00:13:43,240
step so it's actually recursive when

00:13:41,709 --> 00:13:45,550
this function gets called and it tries

00:13:43,240 --> 00:13:46,809
to turn this into a graph it's going to

00:13:45,550 --> 00:13:48,220
see wait it depends on this function

00:13:46,809 --> 00:13:49,809
it's going to turn this into a graph and

00:13:48,220 --> 00:13:51,759
so on so it's recursive you don't need

00:13:49,809 --> 00:13:53,350
the sprinkle TF function everywhere you

00:13:51,759 --> 00:13:56,769
can basically put it at your entry

00:13:53,350 --> 00:14:10,119
points like your main functions

00:13:56,769 --> 00:14:12,579
everything good so far right yes right

00:14:10,119 --> 00:14:14,589
good question nice oh so if you have

00:14:12,579 --> 00:14:17,759
like if you actually want to port your

00:14:14,589 --> 00:14:20,319
code and an employ install it in like a

00:14:17,759 --> 00:14:21,730
ported to to a web browser and so on it

00:14:20,319 --> 00:14:24,660
needs to be exported to a format called

00:14:21,730 --> 00:14:27,549
save model and of course this only

00:14:24,660 --> 00:14:29,350
supports tensorflow operations right it

00:14:27,549 --> 00:14:32,170
doesn't support like are very arbitrary

00:14:29,350 --> 00:14:33,699
Python code if it did I mean you'd need

00:14:32,170 --> 00:14:36,009
to have like a Python environment in the

00:14:33,699 --> 00:14:38,230
mobile device or so on so you need to

00:14:36,009 --> 00:14:39,790
basically in order for it to be portable

00:14:38,230 --> 00:14:41,769
you need to export it to a saved model

00:14:39,790 --> 00:14:43,720
and this only accepts tensorflow

00:14:41,769 --> 00:14:45,819
function so you would have to basically

00:14:43,720 --> 00:14:47,290
convert everything to TF functions in

00:14:45,819 --> 00:14:48,730
order to be able to export it but

00:14:47,290 --> 00:14:50,619
usually what here I'm showing like the

00:14:48,730 --> 00:14:55,029
low-level API usually what you'll use

00:14:50,619 --> 00:14:57,399
this 10 is TF Charis Charis is the you

00:14:55,029 --> 00:14:59,170
know you all know Kerris but I think

00:14:57,399 --> 00:15:01,360
people think of Karass as a library

00:14:59,170 --> 00:15:03,249
because it started out that way but now

00:15:01,360 --> 00:15:05,350
it's more like an API and there are

00:15:03,249 --> 00:15:08,379
several implementations of the Charis

00:15:05,350 --> 00:15:10,179
api one of which is the original and

00:15:08,379 --> 00:15:12,220
reference implementation also called

00:15:10,179 --> 00:15:15,970
Chara's I call it care esteem so there's

00:15:12,220 --> 00:15:18,249
no confusion and and this implementation

00:15:15,970 --> 00:15:20,889
is actually less used than the the

00:15:18,249 --> 00:15:23,529
Charis implementation inside tensorflow

00:15:20,889 --> 00:15:25,839
which is called TF harris and usually

00:15:23,529 --> 00:15:28,360
you would use TF Chara's for most of

00:15:25,839 --> 00:15:30,610
your work like it's so flexible now that

00:15:28,360 --> 00:15:31,809
you don't really very often need to go

00:15:30,610 --> 00:15:32,520
to that level unless you're really

00:15:31,809 --> 00:15:34,950
customizing

00:15:32,520 --> 00:15:36,870
things and at the Karis level it

00:15:34,950 --> 00:15:39,210
compiles everything for you so you need

00:15:36,870 --> 00:15:40,770
to make sure if you just use standard

00:15:39,210 --> 00:15:42,330
layers everything will be okay if you

00:15:40,770 --> 00:15:43,020
write your own custom layers they need

00:15:42,330 --> 00:15:44,280
to use only

00:15:43,020 --> 00:15:45,690
you know tensor flow functions if you

00:15:44,280 --> 00:15:48,230
want to be for it to be portable does

00:15:45,690 --> 00:15:48,230
that answer your question

00:15:50,910 --> 00:15:55,290
the eager motors basically in a

00:15:52,980 --> 00:15:56,880
sequential fashion right exactly so when

00:15:55,290 --> 00:16:01,460
we are moving to graph mode just

00:15:56,880 --> 00:16:05,130
changing the RTA function will it still

00:16:01,460 --> 00:16:09,570
be guitar we have to do one more level

00:16:05,130 --> 00:16:11,220
of debugging to make sure all my see the

00:16:09,570 --> 00:16:15,450
executions are happening in order and

00:16:11,220 --> 00:16:19,110
all those things right so usually for I

00:16:15,450 --> 00:16:21,060
run a bunch of tests right and usually

00:16:19,110 --> 00:16:23,010
you always always get the same result

00:16:21,060 --> 00:16:24,810
but in this particular example for

00:16:23,010 --> 00:16:27,030
example since I'm running a loop with TF

00:16:24,810 --> 00:16:29,340
dot range this will actually complain in

00:16:27,030 --> 00:16:31,380
eager mode so what you would do is one

00:16:29,340 --> 00:16:33,240
your testing or debug your programming

00:16:31,380 --> 00:16:36,030
you would run you would have this as

00:16:33,240 --> 00:16:37,470
range right and then once you're happy

00:16:36,030 --> 00:16:38,940
and it passes all your tests and you

00:16:37,470 --> 00:16:40,860
that TF dot range and that's where it

00:16:38,940 --> 00:16:42,480
would be ported to the graph right so I

00:16:40,860 --> 00:16:44,130
I can't say that it's a hundred percent

00:16:42,480 --> 00:16:45,810
automatic and you just guaranteed to

00:16:44,130 --> 00:16:48,060
give you the same result but usually

00:16:45,810 --> 00:16:49,650
you'd be programming in completely

00:16:48,060 --> 00:16:53,220
mode and then at the end trying to

00:16:49,650 --> 00:16:54,900
transition to TF functions and normally

00:16:53,220 --> 00:16:57,090
a lot of the constructs are just taking

00:16:54,900 --> 00:16:58,320
into account just at TF and you're good

00:16:57,090 --> 00:17:00,450
like if you have print statements

00:16:58,320 --> 00:17:02,400
replaced them with TF dot print and work

00:17:00,450 --> 00:17:05,940
as well if you have assert you replace

00:17:02,400 --> 00:17:08,070
them with TF to a desert okay so yeah so

00:17:05,940 --> 00:17:10,920
this will actually put the loop itself

00:17:08,070 --> 00:17:13,110
in the graph if you have range of n then

00:17:10,920 --> 00:17:14,850
it would actually run when tracing the

00:17:13,110 --> 00:17:16,980
function and so you'd end up with a

00:17:14,850 --> 00:17:19,260
graph that actually calls the the

00:17:16,980 --> 00:17:20,790
function 50 times for example right so

00:17:19,260 --> 00:17:21,930
make sure you call TF range if you

00:17:20,790 --> 00:17:25,470
actually want the loop to be in the

00:17:21,930 --> 00:17:28,650
graph alright so the control flow that

00:17:25,470 --> 00:17:30,840
autograph automatically handles is

00:17:28,650 --> 00:17:32,460
things like if statements this also ends

00:17:30,840 --> 00:17:34,550
up in the graph all right so it's

00:17:32,460 --> 00:17:37,020
captured and and ends up in the graph

00:17:34,550 --> 00:17:39,780
return statements are handled gracefully

00:17:37,020 --> 00:17:41,520
in the graph as well you have loops with

00:17:39,780 --> 00:17:43,440
for loops you also have you know break

00:17:41,520 --> 00:17:45,630
and while and TF dot print and TF data

00:17:43,440 --> 00:17:46,170
stewards autograph captures a lot of the

00:17:45,630 --> 00:17:48,870
very

00:17:46,170 --> 00:17:51,600
the most common constructs for control

00:17:48,870 --> 00:17:53,760
flow and if you just use TF operations

00:17:51,600 --> 00:17:55,860
plus these simple control flows if you

00:17:53,760 --> 00:17:57,900
just call functions that you wrote

00:17:55,860 --> 00:18:00,090
yourself you're good to go if you call

00:17:57,900 --> 00:18:02,520
like an external library or even the

00:18:00,090 --> 00:18:04,350
Python standard library then it probably

00:18:02,520 --> 00:18:07,920
won't work because it's called not

00:18:04,350 --> 00:18:11,310
calling tensorflow operations right so

00:18:07,920 --> 00:18:14,100
so that's it yeah the so the simplicity

00:18:11,310 --> 00:18:16,620
of you know eager mode and graph mode is

00:18:14,100 --> 00:18:18,930
is it that makes a huge difference with

00:18:16,620 --> 00:18:20,430
tensorflow too if you ran away from

00:18:18,930 --> 00:18:22,260
tensorflow one because of the complexity

00:18:20,430 --> 00:18:25,320
try it again with tensor flow - and I'm

00:18:22,260 --> 00:18:27,180
sure you'll love it as I do the the

00:18:25,320 --> 00:18:30,480
second improvement is that it's much

00:18:27,180 --> 00:18:32,370
more pythonic like one of the problems

00:18:30,480 --> 00:18:34,260
with tensor flow one it was that a lot

00:18:32,370 --> 00:18:35,760
of things were kind of name based using

00:18:34,260 --> 00:18:37,530
global scopes and things like that and

00:18:35,760 --> 00:18:39,480
this has been improved tremendously so

00:18:37,530 --> 00:18:41,520
one example is you know sharing weights

00:18:39,480 --> 00:18:43,410
across layers typically in the siamese

00:18:41,520 --> 00:18:44,670
networks you'd have some architecture

00:18:43,410 --> 00:18:46,830
like this where these two layers are

00:18:44,670 --> 00:18:48,870
assuring weights and in sensor for one

00:18:46,830 --> 00:18:50,850
you'd implement this somehow like this

00:18:48,870 --> 00:18:53,040
where you'd have these two layers here

00:18:50,850 --> 00:18:55,260
and if you look closely they have the

00:18:53,040 --> 00:18:57,390
same name and that's how you would share

00:18:55,260 --> 00:18:59,580
weights between layers it was named

00:18:57,390 --> 00:19:00,660
based that's kind of riddle it's easy to

00:18:59,580 --> 00:19:02,400
break this if you have multiple

00:19:00,660 --> 00:19:04,680
libraries creating different pieces of

00:19:02,400 --> 00:19:06,030
the graph and by a chance they actually

00:19:04,680 --> 00:19:08,070
happen to use the same name you're out

00:19:06,030 --> 00:19:09,750
of luck you could use name scope that

00:19:08,070 --> 00:19:11,610
was a whole mess in the south notice the

00:19:09,750 --> 00:19:14,130
second one says reuse equals true that's

00:19:11,610 --> 00:19:16,800
how you would share it weights in 1001

00:19:14,130 --> 00:19:18,300
now in sense flow to its yeah so you'd

00:19:16,800 --> 00:19:19,800
use you know variable scopes and get

00:19:18,300 --> 00:19:21,420
variable and a lot of constructs

00:19:19,800 --> 00:19:24,200
actually came from the fact that was

00:19:21,420 --> 00:19:27,780
this logic of being based with on names

00:19:24,200 --> 00:19:29,940
so instead in tensor flow - what you do

00:19:27,780 --> 00:19:31,650
is you actually creates Karis layers

00:19:29,940 --> 00:19:35,640
like this if you use Cara's before this

00:19:31,650 --> 00:19:37,260
is very natural Karis code and except we

00:19:35,640 --> 00:19:39,450
imported from tensor flow because we're

00:19:37,260 --> 00:19:41,580
using TF chaos this is not you know the

00:19:39,450 --> 00:19:44,160
the Pippins stole Kara's thing this is

00:19:41,580 --> 00:19:45,750
the Charis that comes with tension flow

00:19:44,160 --> 00:19:46,860
right you don't need to install anything

00:19:45,750 --> 00:19:49,560
else

00:19:46,860 --> 00:19:51,240
well I create one layer here and then I

00:19:49,560 --> 00:19:53,010
call it twice with two different inputs

00:19:51,240 --> 00:19:55,230
and that's how I share weights

00:19:53,010 --> 00:19:59,190
it's very object-oriented and natural

00:19:55,230 --> 00:20:02,049
okay everybody get this

00:19:59,190 --> 00:20:04,000
so that's one thing another thing is

00:20:02,049 --> 00:20:05,710
we're in tensorflow one there was some

00:20:04,000 --> 00:20:07,299
global state that was used like he

00:20:05,710 --> 00:20:10,120
created an optimizer and then you say I

00:20:07,299 --> 00:20:12,070
want to minimize this loss well you want

00:20:10,120 --> 00:20:13,840
to minimize the loss by tweaking some

00:20:12,070 --> 00:20:15,549
variables but I'm not telling you which

00:20:13,840 --> 00:20:18,520
variables to tweak here so how does it

00:20:15,549 --> 00:20:19,929
know well it knew because actually it

00:20:18,520 --> 00:20:22,179
would called you know internally it

00:20:19,929 --> 00:20:25,059
would call TF trainable variables it's

00:20:22,179 --> 00:20:26,440
as if you had called it like this and TF

00:20:25,059 --> 00:20:28,419
trainable variables would actually look

00:20:26,440 --> 00:20:31,390
up the default graph and in there will

00:20:28,419 --> 00:20:33,580
look up a collection of variables and so

00:20:31,390 --> 00:20:35,169
it assumes that people had inserted

00:20:33,580 --> 00:20:38,710
stuff in there it's like using global

00:20:35,169 --> 00:20:40,000
scope it's not very clean instead you

00:20:38,710 --> 00:20:42,460
know it's like global to the graph it's

00:20:40,000 --> 00:20:45,610
the same kind of problem in the way this

00:20:42,460 --> 00:20:48,760
was thought out using global and global

00:20:45,610 --> 00:20:50,110
scope and names so yeah it assumes one

00:20:48,760 --> 00:20:51,850
model per graph it's only available

00:20:50,110 --> 00:20:53,679
actually in graph mode so you don't yeah

00:20:51,850 --> 00:20:55,900
collections basically in tensor flow too

00:20:53,679 --> 00:20:58,480
are dead they're removed variable scopes

00:20:55,900 --> 00:21:00,640
don't exist anymore all this like mess

00:20:58,480 --> 00:21:02,710
with names and so on that's in global

00:21:00,640 --> 00:21:04,270
scope that's gone so instead what you do

00:21:02,710 --> 00:21:06,880
is you create a Karass layer like this

00:21:04,270 --> 00:21:08,950
model like this and each layer actually

00:21:06,880 --> 00:21:11,110
handles its own weights right it's

00:21:08,950 --> 00:21:13,150
there's no like other object where we're

00:21:11,110 --> 00:21:13,630
sending the variables to no it's like

00:21:13,150 --> 00:21:15,940
this

00:21:13,630 --> 00:21:18,340
component where which owns the variables

00:21:15,940 --> 00:21:20,350
itself and since this model contains

00:21:18,340 --> 00:21:22,539
several layers if you ask the model

00:21:20,350 --> 00:21:24,970
itself what are the weights of the model

00:21:22,539 --> 00:21:27,130
it recursively ask each layer and it

00:21:24,970 --> 00:21:29,710
gives you the whole list of weights for

00:21:27,130 --> 00:21:31,480
the whole model and it's kind of clean

00:21:29,710 --> 00:21:34,659
right it's just in one place and you

00:21:31,480 --> 00:21:36,730
don't need this global thing and so now

00:21:34,659 --> 00:21:39,039
when you when you call the optimizer as

00:21:36,730 --> 00:21:40,720
minimize method well you can pass very

00:21:39,039 --> 00:21:43,750
easily all the variables that you want

00:21:40,720 --> 00:21:46,419
to to minimize so this changes our

00:21:43,750 --> 00:21:49,240
variable scope is removed get variable

00:21:46,419 --> 00:21:51,580
is removed you know minimize now

00:21:49,240 --> 00:21:52,960
requires you to pass in some variables

00:21:51,580 --> 00:21:55,380
but it's actually easy if you've wrapped

00:21:52,960 --> 00:21:57,820
them in layers and caris layers

00:21:55,380 --> 00:22:00,520
yeah and that leads me to the next point

00:21:57,820 --> 00:22:01,990
it's everything has been cleaned up a

00:22:00,520 --> 00:22:03,610
lot another complaint that people had

00:22:01,990 --> 00:22:05,350
about tensor flow one is that it is

00:22:03,610 --> 00:22:08,950
starting to be very cluttered with a lot

00:22:05,350 --> 00:22:10,950
of stuff duplicated api's and a lot of

00:22:08,950 --> 00:22:13,920
you know unused things

00:22:10,950 --> 00:22:16,470
in TF contrib so like one example of

00:22:13,920 --> 00:22:16,920
duplication is TF players and TF Kara's

00:22:16,470 --> 00:22:20,340
that layers

00:22:16,920 --> 00:22:22,500
well that's gone now everything is is

00:22:20,340 --> 00:22:24,060
high level and it's in the fish you know

00:22:22,500 --> 00:22:26,400
the official high level API for

00:22:24,060 --> 00:22:28,350
tensorflow two is chaos so everything's

00:22:26,400 --> 00:22:31,050
in there and there's no duplication here

00:22:28,350 --> 00:22:33,540
TF contrib this is like a snapshot of

00:22:31,050 --> 00:22:35,730
where TF dot-com trip contains in 1001

00:22:33,540 --> 00:22:38,310
and in there you have a lot of things

00:22:35,730 --> 00:22:40,260
that haven't just never been used like

00:22:38,310 --> 00:22:42,000
if you search on github you won't

00:22:40,260 --> 00:22:44,010
actually find anybody using them or you

00:22:42,000 --> 00:22:44,880
know almost nothing others are just not

00:22:44,010 --> 00:22:46,800
maintained anymore

00:22:44,880 --> 00:22:48,780
some have actually already ported being

00:22:46,800 --> 00:22:50,580
ported to core API but they're still

00:22:48,780 --> 00:22:52,830
there so it's was becoming a big mess

00:22:50,580 --> 00:22:54,900
and in maintenance nightmares so they

00:22:52,830 --> 00:22:58,770
just decided to completely remove it

00:22:54,900 --> 00:22:59,700
there's this there will be no TF cotton

00:22:58,770 --> 00:23:02,100
contrib anymore

00:22:59,700 --> 00:23:04,890
where anybody can just push some code in

00:23:02,100 --> 00:23:06,750
that just won't happen and the important

00:23:04,890 --> 00:23:09,360
parts of those have already been moved

00:23:06,750 --> 00:23:10,680
to tensorflow two core API and the other

00:23:09,360 --> 00:23:13,380
ones that were less important or less

00:23:10,680 --> 00:23:15,990
used some of them have been moved to the

00:23:13,380 --> 00:23:18,900
add-ons we have the one of GD DS here

00:23:15,990 --> 00:23:21,870
Jason who who handles that or one of the

00:23:18,900 --> 00:23:24,090
things that that's around this and oh a

00:23:21,870 --> 00:23:27,270
special interest group is what I just

00:23:24,090 --> 00:23:28,800
had sick and yeah and so this get

00:23:27,270 --> 00:23:30,210
removed and some of them were just not

00:23:28,800 --> 00:23:33,420
used and so they're there they

00:23:30,210 --> 00:23:35,070
disappeared alright the API has been

00:23:33,420 --> 00:23:37,440
cleaned up as well like there were lots

00:23:35,070 --> 00:23:41,160
of a lot a lot of things just at the

00:23:37,440 --> 00:23:43,470
root names name scope and and so now a

00:23:41,160 --> 00:23:46,110
lot of packages have been created so

00:23:43,470 --> 00:23:47,760
instead of you know TF daata sir's a you

00:23:46,110 --> 00:23:49,710
well TF those are actually the wrong

00:23:47,760 --> 00:23:51,840
example but as a lot of things have been

00:23:49,710 --> 00:23:54,120
moved into sub packages so it cleans

00:23:51,840 --> 00:23:55,650
everything up and there are some aliases

00:23:54,120 --> 00:23:59,370
for the most commonly used functions

00:23:55,650 --> 00:24:01,170
like TF x for exponential the actual

00:23:59,370 --> 00:24:03,090
function is TF math dot exponential

00:24:01,170 --> 00:24:05,490
that's so used that they kept the alias

00:24:03,090 --> 00:24:07,950
in TX but overall it's really very

00:24:05,490 --> 00:24:09,810
cleaned up and a migration tool has been

00:24:07,950 --> 00:24:11,850
provided that's where you know you yeah

00:24:09,810 --> 00:24:13,140
you're pretty happy at this point you're

00:24:11,850 --> 00:24:15,540
like oh and this tomb has changed well

00:24:13,140 --> 00:24:17,130
there's a migration tool and so my last

00:24:15,540 --> 00:24:19,680
point will be about migration I'm almost

00:24:17,130 --> 00:24:21,780
out of time my recommendation would be

00:24:19,680 --> 00:24:23,640
if you have a tensor flow one code basic

00:24:21,780 --> 00:24:24,480
that exists today try to migrate it

00:24:23,640 --> 00:24:26,910
first

00:24:24,480 --> 00:24:28,710
to care us as much as you can a lot of

00:24:26,910 --> 00:24:30,750
the functionalities that are available

00:24:28,710 --> 00:24:33,180
intensive pro 2 with Karis just work the

00:24:30,750 --> 00:24:34,890
same way in 1001 so if you can migrate

00:24:33,180 --> 00:24:36,720
as much as you can to Karis the

00:24:34,890 --> 00:24:38,340
transition will be much easier the

00:24:36,720 --> 00:24:41,130
second thing is anywhere you're using

00:24:38,340 --> 00:24:42,300
ETF contrib I'll try to migrate to

00:24:41,130 --> 00:24:45,240
something else because you have

00:24:42,300 --> 00:24:46,860
configured in tensorflow to alright no

00:24:45,240 --> 00:24:52,200
migration possible if you're using TF

00:24:46,860 --> 00:24:54,900
contribs the upgrade tool you just give

00:24:52,200 --> 00:24:57,300
the in the directory of your source code

00:24:54,900 --> 00:24:59,700
and the directory of the output and it

00:24:57,300 --> 00:25:01,440
just converts it so mostly it will

00:24:59,700 --> 00:25:04,320
actually just rename things and put them

00:25:01,440 --> 00:25:06,720
in TF coop at v1 they have this

00:25:04,320 --> 00:25:09,240
compatibility module where basically all

00:25:06,720 --> 00:25:11,880
of tensorflow one is still supported

00:25:09,240 --> 00:25:13,440
except for TF contrib inside this

00:25:11,880 --> 00:25:15,900
package of course it doesn't give you

00:25:13,440 --> 00:25:17,520
nice and canonical code but it allows

00:25:15,900 --> 00:25:21,090
you to have production code still work

00:25:17,520 --> 00:25:23,070
intensive player 2 using this now some

00:25:21,090 --> 00:25:26,160
some things like TF classes that log

00:25:23,070 --> 00:25:28,830
loss actually has a perfectly good

00:25:26,160 --> 00:25:30,480
function in TF Kerris but it's not

00:25:28,830 --> 00:25:32,370
migrated to the Charis version because

00:25:30,480 --> 00:25:34,230
it doesn't exactly do the same thing

00:25:32,370 --> 00:25:36,000
like there's subtle differences so

00:25:34,230 --> 00:25:38,190
they're being very conservative just

00:25:36,000 --> 00:25:40,380
pointing you to the compat v1 function

00:25:38,190 --> 00:25:41,820
so in some cases you might have to you

00:25:40,380 --> 00:25:43,440
know check that you're not impacted by

00:25:41,820 --> 00:25:45,180
those little differences and then

00:25:43,440 --> 00:25:47,760
migrate to their appropriate function

00:25:45,180 --> 00:25:49,740
and you know random uniformed now is

00:25:47,760 --> 00:25:51,450
underscore uniform is a random dot

00:25:49,740 --> 00:25:53,550
uniform like it does some of these

00:25:51,450 --> 00:25:55,110
things very nicely for you it also takes

00:25:53,550 --> 00:25:56,550
care of all the function arguments if

00:25:55,110 --> 00:25:59,220
the order has changed or the names have

00:25:56,550 --> 00:26:00,600
changed so yeah it does this there's a

00:25:59,220 --> 00:26:06,270
nice tool you want you can try which is

00:26:00,600 --> 00:26:08,280
called oops a tf2 up ml which you just

00:26:06,270 --> 00:26:12,000
give it like a Jupiter notebook or

00:26:08,280 --> 00:26:14,280
online if you have a github repo Li like

00:26:12,000 --> 00:26:16,140
I do with some notebooks you just change

00:26:14,280 --> 00:26:19,800
the URL instead of github com you

00:26:16,140 --> 00:26:21,060
replace that with CF - ml the same the

00:26:19,800 --> 00:26:22,890
rest of URL is the same it'll

00:26:21,060 --> 00:26:24,690
automatically convert it you can

00:26:22,890 --> 00:26:25,680
download the upgraded file and you'll

00:26:24,690 --> 00:26:28,920
see you know all the different

00:26:25,680 --> 00:26:30,810
everything and lastly you know it's I'm

00:26:28,920 --> 00:26:33,090
just out of time now lastly it's

00:26:30,810 --> 00:26:35,160
actually tensorflow was already open

00:26:33,090 --> 00:26:37,310
source but I feel that the mentality of

00:26:35,160 --> 00:26:40,790
the tensorflow

00:26:37,310 --> 00:26:44,240
has opened up with tensorflow too and

00:26:40,790 --> 00:26:47,120
like for example there's this community

00:26:44,240 --> 00:26:49,130
project where anyone can submit RFC's a

00:26:47,120 --> 00:26:51,380
request for comments and and proposals

00:26:49,130 --> 00:26:54,230
for what tensorflow too should be this

00:26:51,380 --> 00:26:56,180
is run for since last August and worked

00:26:54,230 --> 00:26:59,420
pretty well there are lots of proposals

00:26:56,180 --> 00:27:00,860
like like these and people contributed

00:26:59,420 --> 00:27:03,620
this way so I think in the spirit of

00:27:00,860 --> 00:27:04,880
this conference the the it was already

00:27:03,620 --> 00:27:07,360
open source but I think it's even more

00:27:04,880 --> 00:27:09,950
open now you can you know go to these

00:27:07,360 --> 00:27:13,250
discussion groups or developer groups if

00:27:09,950 --> 00:27:14,900
you want more details so yeah I think to

00:27:13,250 --> 00:27:16,820
conclude it's cleaner more

00:27:14,900 --> 00:27:19,280
object-oriented pythonic you get this

00:27:16,820 --> 00:27:22,190
you know the ease of use of eager

00:27:19,280 --> 00:27:24,560
execution the power of graph mode which

00:27:22,190 --> 00:27:26,570
gives you know speed and portability you

00:27:24,560 --> 00:27:28,880
have like super rich API is to handle a

00:27:26,570 --> 00:27:31,910
you know I didn't talk much about Karis

00:27:28,880 --> 00:27:34,400
but you know beautiful API that you can

00:27:31,910 --> 00:27:35,720
use a data API for efficient loading and

00:27:34,400 --> 00:27:37,160
everything this was already in

00:27:35,720 --> 00:27:39,410
tensorflow one so that's why I didn't

00:27:37,160 --> 00:27:42,560
cover it now the great API huge

00:27:39,410 --> 00:27:45,320
community like a most used framework out

00:27:42,560 --> 00:27:47,600
there and machine learning documentation

00:27:45,320 --> 00:27:48,710
has improved a lot and many projects

00:27:47,600 --> 00:27:50,570
have been built on top of it

00:27:48,710 --> 00:27:53,060
you also get like TP use on the cloud

00:27:50,570 --> 00:27:54,620
which right now is this is the only

00:27:53,060 --> 00:27:56,330
library that can actually benefit from

00:27:54,620 --> 00:27:59,300
TP use part which is actually coming

00:27:56,330 --> 00:28:00,350
soon on this and it's even more open so

00:27:59,300 --> 00:28:02,900
if you want to play with this I have

00:28:00,350 --> 00:28:05,930
some Jupiter notebooks that are open

00:28:02,900 --> 00:28:07,610
source on my github account /a Geron

00:28:05,930 --> 00:28:09,200
there's a whole tf2 course with

00:28:07,610 --> 00:28:11,060
exercises if you want to go to this and

00:28:09,200 --> 00:28:12,380
I'm writing the second edition of my

00:28:11,060 --> 00:28:14,180
book this was the notebooks for the

00:28:12,380 --> 00:28:16,250
first edition and the second edition is

00:28:14,180 --> 00:28:19,160
uh Australia is here so I'm like a

00:28:16,250 --> 00:28:21,290
chapter 13 or so and it goes into all

00:28:19,160 --> 00:28:23,570
the details you can try that out and

00:28:21,290 --> 00:28:25,600
with that I think that's it thank you

00:28:23,570 --> 00:28:29,110
very much if you have any questions

00:28:25,600 --> 00:28:38,630
[Applause]

00:28:29,110 --> 00:28:41,000
yeah sorry I let me repeat that again so

00:28:38,630 --> 00:28:43,330
what any changes with respects to the

00:28:41,000 --> 00:28:45,920
input of data and the output of data

00:28:43,330 --> 00:28:48,650
specifically with respects to turning

00:28:45,920 --> 00:28:51,200
that into a on line prediction the on

00:28:48,650 --> 00:28:53,510
line prediction couple yes question so

00:28:51,200 --> 00:28:55,700
the one of the difficulty people had

00:28:53,510 --> 00:28:57,230
with tensorflow one is okay sure I can I

00:28:55,700 --> 00:28:59,450
can train a model locally but how do I

00:28:57,230 --> 00:29:01,400
deploy it to production and in different

00:28:59,450 --> 00:29:04,360
environments how do I train it on

00:29:01,400 --> 00:29:07,130
multiple machines and so on and how do I

00:29:04,360 --> 00:29:10,130
you know prepare the data and everything

00:29:07,130 --> 00:29:13,610
so this is one of the announcements that

00:29:10,130 --> 00:29:15,740
came with with tensor flow to attend to

00:29:13,610 --> 00:29:17,840
flow to dev summit was there's this

00:29:15,740 --> 00:29:21,050
platform called T FX sensor flow

00:29:17,840 --> 00:29:22,910
extended which includes several projects

00:29:21,050 --> 00:29:25,580
they're not included in tensor flow but

00:29:22,910 --> 00:29:27,830
you can download them on this side and

00:29:25,580 --> 00:29:29,090
they all help to productionize tensor

00:29:27,830 --> 00:29:32,240
flow models so you have for example

00:29:29,090 --> 00:29:34,460
tensorflow transform of TF transform

00:29:32,240 --> 00:29:37,220
which allows you to write pre-processing

00:29:34,460 --> 00:29:39,350
code once and you can run this code and

00:29:37,220 --> 00:29:41,570
it's written in Python you can run it in

00:29:39,350 --> 00:29:43,760
batch mode over all your your training

00:29:41,570 --> 00:29:46,160
data and it'll just convert it using

00:29:43,760 --> 00:29:47,780
Apache beam and to you know your

00:29:46,160 --> 00:29:49,880
preprocessdataset and then you can train

00:29:47,780 --> 00:29:52,250
your model on that and then it also

00:29:49,880 --> 00:29:55,580
generates a tensor flow graph that you

00:29:52,250 --> 00:29:57,890
can put in your model once it's trained

00:29:55,580 --> 00:30:00,020
you just plug it in and put that in

00:29:57,890 --> 00:30:02,060
production and so it will be able to

00:30:00,020 --> 00:30:04,790
also pre process using the same function

00:30:02,060 --> 00:30:08,540
they'll pre process incoming data on the

00:30:04,790 --> 00:30:10,970
fly as well so it includes tf-x includes

00:30:08,540 --> 00:30:12,860
also TF serving which is like a very

00:30:10,970 --> 00:30:15,230
powerful server that's what they use

00:30:12,860 --> 00:30:17,270
internally at Google to serve multiple

00:30:15,230 --> 00:30:18,800
models it handles multiple versions if

00:30:17,270 --> 00:30:20,540
you switch from one version to the other

00:30:18,800 --> 00:30:22,040
it works fine it'll actually supports

00:30:20,540 --> 00:30:24,040
having multiple models at the same time

00:30:22,040 --> 00:30:26,720
if you're doing experiments for example

00:30:24,040 --> 00:30:28,940
so fantastic platform there's also

00:30:26,720 --> 00:30:31,400
things like model validation another

00:30:28,940 --> 00:30:33,830
module tensorflow validation and I'm

00:30:31,400 --> 00:30:36,230
missing a few but to check out T effects

00:30:33,830 --> 00:30:38,780
and this is like the way to

00:30:36,230 --> 00:30:40,040
productionize tensorflow models and they

00:30:38,780 --> 00:30:41,570
just released

00:30:40,040 --> 00:30:43,610
already released some components last

00:30:41,570 --> 00:30:50,180
year but now you have the whole the

00:30:43,610 --> 00:30:52,940
whole framework yeah still related down

00:30:50,180 --> 00:30:55,700
the deployment Armando are there plans

00:30:52,940 --> 00:30:58,340
or is there an initiative already for

00:30:55,700 --> 00:31:01,280
like tensorflow to support onyx or is it

00:30:58,340 --> 00:31:04,370
like the FX is the only way to go right

00:31:01,280 --> 00:31:06,710
now I don't think there's I'm sure

00:31:04,370 --> 00:31:10,550
there's no onyx I don't think it's even

00:31:06,710 --> 00:31:11,060
in the roadmap or so that's a good

00:31:10,550 --> 00:31:14,240
question

00:31:11,060 --> 00:31:16,700
like why why they're not using own X one

00:31:14,240 --> 00:31:19,820
of the feedback I got is that onyx is

00:31:16,700 --> 00:31:21,590
kind of a talking point right now on the

00:31:19,820 --> 00:31:23,990
the argument is who actually in this

00:31:21,590 --> 00:31:26,990
room uses all next deploy yeah it's not

00:31:23,990 --> 00:31:28,880
that use is the point and so investing

00:31:26,990 --> 00:31:30,380
some efforts on that doesn't seem

00:31:28,880 --> 00:31:33,890
productive for the tensorflow team

00:31:30,380 --> 00:31:35,630
that's I'm speaking for them so so take

00:31:33,890 --> 00:31:39,220
you know I'm not entirely sure that's

00:31:35,630 --> 00:31:39,220
the logic but that's what my feeling is

00:31:39,700 --> 00:31:46,490
any other questions but in terms of

00:31:44,540 --> 00:31:47,900
portability if you think of onyx as a

00:31:46,490 --> 00:31:49,870
way to save your model in a kind of

00:31:47,900 --> 00:31:52,460
portable way across multiple platforms

00:31:49,870 --> 00:31:53,960
tensorflow actually includes chaos right

00:31:52,460 --> 00:31:55,760
and you can if you want to have

00:31:53,960 --> 00:31:58,130
portability across frameworks you can

00:31:55,760 --> 00:32:00,410
save your your Kerris model using Karis

00:31:58,130 --> 00:32:01,910
format right and once you do that you

00:32:00,410 --> 00:32:03,830
can actually take that model and run it

00:32:01,910 --> 00:32:06,500
on any chaos implementation and this

00:32:03,830 --> 00:32:09,830
also includes like CNT K this includes

00:32:06,500 --> 00:32:11,720
you know other piano and other libraries

00:32:09,830 --> 00:32:14,330
it doesn't include PI torch which

00:32:11,720 --> 00:32:16,250
doesn't have a Keres implementation so

00:32:14,330 --> 00:32:18,320
in terms of portability even at the

00:32:16,250 --> 00:32:21,140
model level it's arguable you know

00:32:18,320 --> 00:32:29,840
arguably the the portability thanks to

00:32:21,140 --> 00:32:32,090
Karis is higher but yeah I know but you

00:32:29,840 --> 00:32:33,020
can Kerris allows you to save a train

00:32:32,090 --> 00:32:36,620
model with all the weights and

00:32:33,020 --> 00:32:38,930
everything yeah so yeah try to try it

00:32:36,620 --> 00:32:41,480
out now obviously I'd be happy if they

00:32:38,930 --> 00:32:45,470
had onyx support right then but right

00:32:41,480 --> 00:32:48,940
now I don't think it's a plan well thank

00:32:45,470 --> 00:32:48,940

YouTube URL: https://www.youtube.com/watch?v=8bQ7OOZLY_E


