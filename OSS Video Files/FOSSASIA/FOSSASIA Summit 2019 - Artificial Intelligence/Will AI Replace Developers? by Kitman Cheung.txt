Title: Will AI Replace Developers? by Kitman Cheung
Publication date: 2019-03-27
Playlist: FOSSASIA Summit 2019 - Artificial Intelligence
Description: 
	15 March 2019 14:00, Event Hall 2-1

Will AI Replace Developers?
Captions: 
	00:00:00,000 --> 00:00:05,210
good afternoon and thank you for being

00:00:01,650 --> 00:00:07,740
here I instead of Lorentz reading a

00:00:05,210 --> 00:00:09,690
meaningless bio I thought I'd just tell

00:00:07,740 --> 00:00:11,910
you a little something about myself when

00:00:09,690 --> 00:00:14,940
when I get started I started my career

00:00:11,910 --> 00:00:17,400
as a developer in IBM when I graduated

00:00:14,940 --> 00:00:19,980
from university at the time I was

00:00:17,400 --> 00:00:22,260
studying engineering and then software

00:00:19,980 --> 00:00:24,439
seems to be the place to go so I ended

00:00:22,260 --> 00:00:26,640
up getting a job as a software developer

00:00:24,439 --> 00:00:30,390
back in 1999

00:00:26,640 --> 00:00:32,489
so I'm old and like you guys tired most

00:00:30,390 --> 00:00:34,320
of the time but and throughout my my

00:00:32,489 --> 00:00:36,750
journey at IBM I have had an opportunity

00:00:34,320 --> 00:00:39,600
to work in many different areas with a

00:00:36,750 --> 00:00:42,719
lot of different teams 16 months all I

00:00:39,600 --> 00:00:44,460
did was travel around work with

00:00:42,719 --> 00:00:47,070
open-source developers introduce

00:00:44,460 --> 00:00:49,020
technology code with them may

00:00:47,070 --> 00:00:51,390
participate in a couple project myself

00:00:49,020 --> 00:00:53,730
and that was a lot of fun and it's fun

00:00:51,390 --> 00:00:56,579
to be back here I don't get to do as

00:00:53,730 --> 00:00:59,370
many community community-based event as

00:00:56,579 --> 00:01:01,190
I I would like these days so I thought

00:00:59,370 --> 00:01:03,480
we'd do something a little lighter after

00:01:01,190 --> 00:01:05,189
two session of coding I know you guys

00:01:03,480 --> 00:01:08,299
had a Cuban Eddy session this morning

00:01:05,189 --> 00:01:11,330
some of some of you and then I say I

00:01:08,299 --> 00:01:14,250
neural-net synthesis type session

00:01:11,330 --> 00:01:16,560
earlier just different from 11 to around

00:01:14,250 --> 00:01:18,090
1 o'clock I think so I thought we'd do

00:01:16,560 --> 00:01:21,330
something a little later and asked an

00:01:18,090 --> 00:01:24,360
interesting question there's been a lot

00:01:21,330 --> 00:01:26,189
of talk about AI robotic robotic

00:01:24,360 --> 00:01:28,229
automation replacing human being in

00:01:26,189 --> 00:01:30,960
their jobs so the question that I'm

00:01:28,229 --> 00:01:35,130
putting on the board is we owe a I

00:01:30,960 --> 00:01:37,920
replace developer normally at this point

00:01:35,130 --> 00:01:40,380
I probably bring up a photo of like the

00:01:37,920 --> 00:01:42,509
Terminator and Skynet and say you know

00:01:40,380 --> 00:01:45,530
they're coming we've got to defend

00:01:42,509 --> 00:01:48,689
ourselves but I thought I would do this

00:01:45,530 --> 00:01:53,479
u.s. patent number nine into eight zero

00:01:48,689 --> 00:02:00,259
one five seven anyone know what this is

00:01:53,479 --> 00:02:03,350
any guess no that's not be a pen

00:02:00,259 --> 00:02:07,640
case your developer pretty close pretty

00:02:03,350 --> 00:02:12,910
close so this is a patent filed by

00:02:07,640 --> 00:02:16,129
Amazon so what you're seeing here is a

00:02:12,910 --> 00:02:20,569
device designed by Amazon to keep the

00:02:16,129 --> 00:02:25,519
human worker safe in a AI and robotics

00:02:20,569 --> 00:02:27,530
and able environment okay now they were

00:02:25,519 --> 00:02:29,360
very quick in the news article to say

00:02:27,530 --> 00:02:32,750
that we never implemented it we didn't

00:02:29,360 --> 00:02:34,549
put a human being inside a cage now for

00:02:32,750 --> 00:02:37,340
fun I was downstairs looking at this

00:02:34,549 --> 00:02:39,110
thinking man this is interesting this is

00:02:37,340 --> 00:02:40,910
a brief downstairs if you if you

00:02:39,110 --> 00:02:44,090
recognize it this is the little claw

00:02:40,910 --> 00:02:46,489
machine in the claw machine you've got

00:02:44,090 --> 00:02:50,540
the merchandise sitting inside a machine

00:02:46,489 --> 00:02:52,760
the robot sitting inside the machine the

00:02:50,540 --> 00:02:54,920
joystick on the outside human being gets

00:02:52,760 --> 00:02:58,190
what they want and then we walk freely

00:02:54,920 --> 00:03:01,610
around everywhere contrasting with this

00:02:58,190 --> 00:03:04,130
the merchandise is outside the cage the

00:03:01,610 --> 00:03:07,220
claw and the robots are outside the cage

00:03:04,130 --> 00:03:09,739
that joystick is on the inside and by

00:03:07,220 --> 00:03:11,989
the way based on this based on what I

00:03:09,739 --> 00:03:14,060
read I don't think the human u operator

00:03:11,989 --> 00:03:15,920
get to drive this it's sitting on a

00:03:14,060 --> 00:03:17,930
robotic platforms that talk to the other

00:03:15,920 --> 00:03:20,780
robot so they don't run into each other

00:03:17,930 --> 00:03:22,250
so when you want to go somewhere you

00:03:20,780 --> 00:03:24,500
give the instruction I want to go there

00:03:22,250 --> 00:03:25,819
but it's not under your control so I'm

00:03:24,500 --> 00:03:29,060
bringing this up because I think it's

00:03:25,819 --> 00:03:32,660
kind of interesting to see how pervasive

00:03:29,060 --> 00:03:38,630
we expecting automation and AI to be in

00:03:32,660 --> 00:03:40,660
a very near future so technology's

00:03:38,630 --> 00:03:46,780
always taking jobs away from human being

00:03:40,660 --> 00:03:51,440
it's nothing new farming right 1871 I

00:03:46,780 --> 00:03:54,769
think over nine hundred thousand of the

00:03:51,440 --> 00:03:58,209
the working population in England and

00:03:54,769 --> 00:04:01,040
Wales are agricultural worker farmers

00:03:58,209 --> 00:04:03,829
2014 that numbers are around forty

00:04:01,040 --> 00:04:06,650
thousand ninety five percent aquaculture

00:04:03,829 --> 00:04:08,540
job gone because of automation automated

00:04:06,650 --> 00:04:12,920
mission and irrigation farming

00:04:08,540 --> 00:04:13,910
machineries US Postal Service cut 25

00:04:12,920 --> 00:04:15,560
percent of the workforce

00:04:13,910 --> 00:04:20,260
in the last ten years because the

00:04:15,560 --> 00:04:20,260
e-billing and mobile connectivity's

00:04:20,410 --> 00:04:24,950
robotics jobs manufacturing jobs out

00:04:23,480 --> 00:04:27,620
there today

00:04:24,950 --> 00:04:28,910
I think McKinsey in just last year I

00:04:27,620 --> 00:04:32,210
believe of the year before release a

00:04:28,910 --> 00:04:35,240
report that says about eight eight

00:04:32,210 --> 00:04:39,380
hundred millions job worldwide will be

00:04:35,240 --> 00:04:42,320
gone in the next ten years or so in fact

00:04:39,380 --> 00:04:44,210
I think Price Price Waterhouse Cooper

00:04:42,320 --> 00:04:46,280
released something similar thirty eight

00:04:44,210 --> 00:04:49,820
percent of all jobs in us that exist

00:04:46,280 --> 00:04:53,210
today will be gone by 2030 because of

00:04:49,820 --> 00:04:56,540
Technology Truckers this one is it's the

00:04:53,210 --> 00:04:58,610
most dire driverless car right everybody

00:04:56,540 --> 00:05:03,140
know that that's coming ai automation

00:04:58,610 --> 00:05:06,710
integrated into one they figure about

00:05:03,140 --> 00:05:09,080
thirty 3.1 million out of the 3.8

00:05:06,710 --> 00:05:11,600
million jobs in the u.s. today will be

00:05:09,080 --> 00:05:12,560
gone in the immediate future in the

00:05:11,600 --> 00:05:18,250
foreseeable future

00:05:12,560 --> 00:05:22,940
that's eighty five percent free workers

00:05:18,250 --> 00:05:28,910
pale workers used to be if you go back

00:05:22,940 --> 00:05:31,460
as little as maybe fifty years ago you

00:05:28,910 --> 00:05:33,140
have about 1.3 million 1.4 million

00:05:31,460 --> 00:05:35,450
people in the u.s. working in a railroad

00:05:33,140 --> 00:05:37,130
to actually ship things around today

00:05:35,450 --> 00:05:39,110
with all the automation and all the

00:05:37,130 --> 00:05:41,000
signal system that's in place there's

00:05:39,110 --> 00:05:44,030
only about one hundred eighty-seven

00:05:41,000 --> 00:05:47,360
thousand of those employees left they

00:05:44,030 --> 00:05:49,700
used to ship about 600 655 billions my a

00:05:47,360 --> 00:05:51,800
ton miles so how far do you ship how

00:05:49,700 --> 00:05:56,960
many times a year now they're shipping

00:05:51,800 --> 00:05:59,540
1.85 Julian trip full capacity with 85%

00:05:56,960 --> 00:06:03,070
workforce cut this one this one's

00:05:59,540 --> 00:06:07,070
interesting you know what these guys are

00:06:03,070 --> 00:06:08,300
their computers did you know that

00:06:07,070 --> 00:06:10,220
there's a profession called the

00:06:08,300 --> 00:06:14,750
computers exact I think some of you are

00:06:10,220 --> 00:06:22,310
nodding saying yeah so NASA the the that

00:06:14,750 --> 00:06:24,500
show the NASA show I that's right so the

00:06:22,310 --> 00:06:26,330
show is about human being there's a ha

00:06:24,500 --> 00:06:27,759
that are hired by NASA to compute

00:06:26,330 --> 00:06:30,259
trajectory

00:06:27,759 --> 00:06:32,509
they're all gone a hundred percent of

00:06:30,259 --> 00:06:36,529
that job is gone so computers has

00:06:32,509 --> 00:06:39,110
replaced computers so thanks this

00:06:36,529 --> 00:06:43,400
question right Stephen Hawking's things

00:06:39,110 --> 00:06:45,680
that that's going to happen and over

00:06:43,400 --> 00:06:48,229
here on the other side it's a study

00:06:45,680 --> 00:06:50,689
conducted by the US Department of Energy

00:06:48,229 --> 00:06:53,029
at the Oakridge lab three researcher

00:06:50,689 --> 00:06:55,729
they're actually conduct an academic

00:06:53,029 --> 00:06:58,849
study and found that they figure by the

00:06:55,729 --> 00:07:01,430
year 2040 code generated by artificial

00:06:58,849 --> 00:07:04,749
intelligence will be good enough to

00:07:01,430 --> 00:07:07,009
replace code written by human

00:07:04,749 --> 00:07:14,449
everybody's looking at me why are you

00:07:07,009 --> 00:07:17,719
here I hate you but I'll get to some of

00:07:14,449 --> 00:07:21,199
that you know so let's take a look at it

00:07:17,719 --> 00:07:23,180
we're programming languages are coming

00:07:21,199 --> 00:07:24,740
from what programming actually kind of

00:07:23,180 --> 00:07:27,439
began now try that an abridged version

00:07:24,740 --> 00:07:29,180
or history of programming in it and you

00:07:27,439 --> 00:07:31,310
know if I miss out any key points please

00:07:29,180 --> 00:07:33,259
don't be offended I don't mean to leave

00:07:31,310 --> 00:07:34,639
out any major technology I put this

00:07:33,259 --> 00:07:40,490
together at 2:00 in the morning last

00:07:34,639 --> 00:07:44,479
last night so the Year 1800 sure card is

00:07:40,490 --> 00:07:47,319
a guy that was in weaving fabric he

00:07:44,479 --> 00:07:49,699
created a loom that takes a punch card

00:07:47,319 --> 00:07:51,909
that will allow you to program it to

00:07:49,699 --> 00:07:55,550
weave a particular pattern on a cloth

00:07:51,909 --> 00:07:57,409
first programmable machine out there if

00:07:55,550 --> 00:08:02,300
you would the first computing device

00:07:57,409 --> 00:08:04,879
invented by this man Ada Lovelace first

00:08:02,300 --> 00:08:09,259
real programmer my first person to write

00:08:04,879 --> 00:08:12,620
very first general-purpose algorithm it

00:08:09,259 --> 00:08:15,169
was a thought idea it was never compiled

00:08:12,620 --> 00:08:18,110
and actually executed but she was wily a

00:08:15,169 --> 00:08:19,639
mathematician wily accepted as the

00:08:18,110 --> 00:08:23,479
world's first computer programmers

00:08:19,639 --> 00:08:26,870
moving forward Alan Turing 1936

00:08:23,479 --> 00:08:28,539
you know Turing machine the first first

00:08:26,870 --> 00:08:32,449
if you were one of the very first

00:08:28,539 --> 00:08:36,079
general-purpose computing device John

00:08:32,449 --> 00:08:37,849
Backus a few years later Fortran first

00:08:36,079 --> 00:08:40,279
high-level language written to replace

00:08:37,849 --> 00:08:40,610
punch card moving away from talking

00:08:40,279 --> 00:08:42,800
machine

00:08:40,610 --> 00:08:46,370
code but speaking more human-like

00:08:42,800 --> 00:08:49,820
language just a couple years later a few

00:08:46,370 --> 00:08:51,950
years later Grace Hopper cobalt my

00:08:49,820 --> 00:08:55,610
cobalt those of you who knows very

00:08:51,950 --> 00:08:57,050
verbose businesslike programming

00:08:55,610 --> 00:09:00,230
language that's in the mainframe that's

00:08:57,050 --> 00:09:02,360
still widely used today if you can

00:09:00,230 --> 00:09:03,709
believe so there's a little break in the

00:09:02,360 --> 00:09:05,149
time line I'm jumping forward a little

00:09:03,709 --> 00:09:06,579
bit because a whole bunch of other

00:09:05,149 --> 00:09:13,160
things started happening after that

00:09:06,579 --> 00:09:16,730
basic got invented C C++ Punic s-- with

00:09:13,160 --> 00:09:19,640
the one that I like 1994 Rasmus Lloyd

00:09:16,730 --> 00:09:22,370
off PHP I don't know if anyone didn't

00:09:19,640 --> 00:09:24,050
PHP coding before because I had I had

00:09:22,370 --> 00:09:26,149
the pleasure were actually meeting rest

00:09:24,050 --> 00:09:28,519
residents a few times when I was doing

00:09:26,149 --> 00:09:30,170
the the open source timeline and I had

00:09:28,519 --> 00:09:32,810
had many beers with him really really

00:09:30,170 --> 00:09:35,450
smart guy so I like that data point in

00:09:32,810 --> 00:09:37,519
particular but coming all the way to

00:09:35,450 --> 00:09:39,769
some of the C C++ work and and then

00:09:37,519 --> 00:09:44,959
moving forward to tensorflow in nineteen

00:09:39,769 --> 00:09:47,360
so 2015 curious in 2015 cafe in 2017

00:09:44,959 --> 00:09:49,279
right these are all the languages and as

00:09:47,360 --> 00:09:51,290
you can see new languages keep popping

00:09:49,279 --> 00:09:54,620
up put any questions why do we have them

00:09:51,290 --> 00:09:57,740
in the first place well I think they're

00:09:54,620 --> 00:10:00,470
a few obvious reason right one computer

00:09:57,740 --> 00:10:03,920
are not really smart enough to

00:10:00,470 --> 00:10:06,890
understand human language the ambiguity

00:10:03,920 --> 00:10:09,019
the nuances in it is too too complex so

00:10:06,890 --> 00:10:12,350
what do we have to do we have to invent

00:10:09,019 --> 00:10:15,050
a simplified dialogue so that we can

00:10:12,350 --> 00:10:18,410
have agree basically using small words

00:10:15,050 --> 00:10:20,420
and short sentences so that computer can

00:10:18,410 --> 00:10:22,490
understand the instruction that we give

00:10:20,420 --> 00:10:24,529
it so it can do the job that we want it

00:10:22,490 --> 00:10:26,510
to do

00:10:24,529 --> 00:10:28,670
second aspect why do we have so many of

00:10:26,510 --> 00:10:32,149
them well the other aspect of it is that

00:10:28,670 --> 00:10:34,250
this changing need in terms of what we

00:10:32,149 --> 00:10:35,660
want the computer to do so each of the

00:10:34,250 --> 00:10:38,149
one of the language and needs to set a

00:10:35,660 --> 00:10:40,519
framework was created to solve a very

00:10:38,149 --> 00:10:43,610
specific sets of problem again with

00:10:40,519 --> 00:10:45,170
small words and short sentences so that

00:10:43,610 --> 00:10:48,320
the computer we can understand and how

00:10:45,170 --> 00:10:51,230
to actually process that it's invented

00:10:48,320 --> 00:10:54,559
by necessity and developer is there to

00:10:51,230 --> 00:10:58,689
actually help regular business

00:10:54,559 --> 00:11:00,849
conveyed their requirement to a machine

00:10:58,689 --> 00:11:04,039
but then this happened

00:11:00,849 --> 00:11:07,339
this was last year so this is what makes

00:11:04,039 --> 00:11:08,899
it even more interesting now Google

00:11:07,339 --> 00:11:11,299
duplex I'm sure all of you have heard

00:11:08,899 --> 00:11:16,509
about that one last year around me

00:11:11,299 --> 00:11:19,939
timeframe see you Pichai mr. pitch a

00:11:16,509 --> 00:11:23,449
Google CEO did a demonstration in May in

00:11:19,939 --> 00:11:26,119
Google i/o conference where in in the

00:11:23,449 --> 00:11:29,389
for in the forum he showcased Google

00:11:26,119 --> 00:11:32,839
duplex talking to a human being who runs

00:11:29,389 --> 00:11:38,389
a shop actually to recording was play

00:11:32,839 --> 00:11:39,529
first one the Google I went and booked

00:11:38,389 --> 00:11:41,929
an appointment for a haircut or

00:11:39,529 --> 00:11:45,799
hairstyle I can't remember exactly it

00:11:41,929 --> 00:11:49,099
was for a lady second one was a booking

00:11:45,799 --> 00:11:50,649
for dinner reservation right if you

00:11:49,099 --> 00:11:53,419
think back you know I mentioned touring

00:11:50,649 --> 00:11:55,549
in many ways everyone looked at that and

00:11:53,419 --> 00:11:57,739
the whole audience just went you know

00:11:55,549 --> 00:12:00,169
they all went crazy they all clap they

00:11:57,739 --> 00:12:02,089
thought hey we got the Turing test done

00:12:00,169 --> 00:12:05,689
the imitating the imitation game is

00:12:02,089 --> 00:12:08,389
there so so that's one major piece it

00:12:05,689 --> 00:12:10,279
was seamless he was able to actually

00:12:08,389 --> 00:12:13,009
understand the nuances of the languages

00:12:10,279 --> 00:12:15,009
and actually complete the task it was

00:12:13,009 --> 00:12:17,209
given a task it went ahead and did that

00:12:15,009 --> 00:12:20,119
maybe a little lesser-known the other

00:12:17,209 --> 00:12:21,409
project and from IBM I do read all our

00:12:20,119 --> 00:12:24,199
own press stuff

00:12:21,409 --> 00:12:27,429
IBM debater projected beta actually was

00:12:24,199 --> 00:12:29,720
released a month after in June last year

00:12:27,429 --> 00:12:33,259
for those of you may not be familiar

00:12:29,720 --> 00:12:35,859
it's a project as an offshoot from right

00:12:33,259 --> 00:12:38,059
after we won jeopardy against Ken Jenny

00:12:35,859 --> 00:12:40,309
right we play jeopardy against Ken

00:12:38,059 --> 00:12:41,869
Jennings and then after that immediately

00:12:40,309 --> 00:12:48,439
after that this project was kicked off

00:12:41,869 --> 00:12:50,869
now what it did it's a simple I would

00:12:48,439 --> 00:12:53,269
still say it's a narrow AI its job is to

00:12:50,869 --> 00:12:55,629
go and engage in a debate with a human

00:12:53,269 --> 00:12:59,659
being and actually try to win a debate

00:12:55,629 --> 00:13:01,429
so they during the live event they did

00:12:59,659 --> 00:13:03,259
to debate and there's a standard

00:13:01,429 --> 00:13:04,849
international debate format not I know

00:13:03,259 --> 00:13:07,489
what that is but apparently it goes

00:13:04,849 --> 00:13:08,430
something like the AI comes up makes an

00:13:07,489 --> 00:13:10,410
argument

00:13:08,430 --> 00:13:12,779
human being comes up make a

00:13:10,410 --> 00:13:15,180
counter-argument you know so to

00:13:12,779 --> 00:13:16,770
perspective being presented the key part

00:13:15,180 --> 00:13:20,610
here though is da I supposed to be

00:13:16,770 --> 00:13:23,970
listening the whole time and they a I

00:13:20,610 --> 00:13:27,089
has to form a rebuttal and then a

00:13:23,970 --> 00:13:29,279
summation now during this process the

00:13:27,089 --> 00:13:31,880
audience are the judge the audience were

00:13:29,279 --> 00:13:36,120
basically the evaluation criteria is

00:13:31,880 --> 00:13:37,589
which of the two competitor presented

00:13:36,120 --> 00:13:39,660
more compelling reason for them to

00:13:37,589 --> 00:13:42,360
change their mind it's not that they

00:13:39,660 --> 00:13:44,850
agree with one or the other we did is

00:13:42,360 --> 00:13:48,150
about how far they shift from the

00:13:44,850 --> 00:13:49,920
previous decision if they were on one

00:13:48,150 --> 00:13:51,960
side of the coin they move further the

00:13:49,920 --> 00:13:53,310
other way then the winner is even though

00:13:51,960 --> 00:13:55,200
they may not have changed their mind

00:13:53,310 --> 00:13:57,930
the winner is still decided that that

00:13:55,200 --> 00:14:00,050
shifted their opinion the most and the

00:13:57,930 --> 00:14:03,570
result was we won one and we lost one

00:14:00,050 --> 00:14:06,060
unfortunately about a month ago we lost

00:14:03,570 --> 00:14:09,060
again except this time was against the

00:14:06,060 --> 00:14:11,910
war champion debater at our event a live

00:14:09,060 --> 00:14:16,339
cast web cast at how the event in in San

00:14:11,910 --> 00:14:19,260
Francisco so we did lose but we we did

00:14:16,339 --> 00:14:24,180
compete against the best in in the world

00:14:19,260 --> 00:14:26,310
and the other aspect about that is one

00:14:24,180 --> 00:14:29,279
of the feedback was da I was able to

00:14:26,310 --> 00:14:31,709
really use fat to create an argument

00:14:29,279 --> 00:14:32,940
what's missing it's the empathy aspect

00:14:31,709 --> 00:14:34,140
wasn't connecting with the audience

00:14:32,940 --> 00:14:37,170
those are some of the feedback that we

00:14:34,140 --> 00:14:41,490
got so why all this right this is about

00:14:37,170 --> 00:14:45,959
natural language programming algorithm

00:14:41,490 --> 00:14:49,110
directly if you would if you even if you

00:14:45,959 --> 00:14:51,600
look at the debater example the opponent

00:14:49,110 --> 00:14:54,480
the human beings argument is in fact

00:14:51,600 --> 00:14:57,480
input to a program that needs to

00:14:54,480 --> 00:15:00,420
generate a particular rebuttal right so

00:14:57,480 --> 00:15:02,130
it's an outcome that is being asked for

00:15:00,420 --> 00:15:03,750
at a very high level basically what I'm

00:15:02,130 --> 00:15:06,570
asking the program to do is argue

00:15:03,750 --> 00:15:08,839
against me listen to what I have to say

00:15:06,570 --> 00:15:12,690
and argue against what I'm saying right

00:15:08,839 --> 00:15:14,339
and then there's this by you something

00:15:12,690 --> 00:15:16,620
that's in Rice University anyone

00:15:14,339 --> 00:15:20,130
familiar with this particular piece of

00:15:16,620 --> 00:15:22,490
research this is where the beginning

00:15:20,130 --> 00:15:22,490
have

00:15:23,290 --> 00:15:28,360
this is AI d'etre codes is a deep

00:15:27,110 --> 00:15:32,360
learning project at Rice University

00:15:28,360 --> 00:15:36,320
where they uses neural neural network

00:15:32,360 --> 00:15:38,990
sketch learning capabilities where they

00:15:36,320 --> 00:15:40,670
look at code pattern of thousands and

00:15:38,990 --> 00:15:43,100
thousands and tens of thousands of Java

00:15:40,670 --> 00:15:44,930
file and functions and then based on a

00:15:43,100 --> 00:15:47,510
pattern they start using deep learning

00:15:44,930 --> 00:15:49,820
to recognize what are the input what are

00:15:47,510 --> 00:15:53,030
the output what are the design pattern

00:15:49,820 --> 00:15:55,610
that's recurring and then what it can do

00:15:53,030 --> 00:15:57,650
now is we can actually allow a human

00:15:55,610 --> 00:15:59,660
programmer to give a high-level

00:15:57,650 --> 00:16:02,690
description of what you need to get done

00:15:59,660 --> 00:16:04,370
and it will go create a sketch and then

00:16:02,690 --> 00:16:06,440
generate the code necessary to complete

00:16:04,370 --> 00:16:08,810
that task it will present two or three

00:16:06,440 --> 00:16:11,540
options back to the user apparently and

00:16:08,810 --> 00:16:15,140
let the developer decide which one to

00:16:11,540 --> 00:16:20,180
use that's kind of the onset of where of

00:16:15,140 --> 00:16:23,120
is going a little bit scary but are we

00:16:20,180 --> 00:16:27,410
done for i I hope I don't think so

00:16:23,120 --> 00:16:30,050
here's a study that's conducted other

00:16:27,410 --> 00:16:33,340
University of Oxford likelihood of jobs

00:16:30,050 --> 00:16:36,680
that will be replaced by a computer and

00:16:33,340 --> 00:16:39,470
the blue stuff is asked the dark blue

00:16:36,680 --> 00:16:41,990
stuff that that's us and the lower end

00:16:39,470 --> 00:16:47,090
of the spectrum the less likely to hire

00:16:41,990 --> 00:16:48,980
in more likely ok turns out what we do

00:16:47,090 --> 00:16:50,630
is not that simple what you guys do

00:16:48,980 --> 00:16:51,650
nowadays I say we I haven't done any

00:16:50,630 --> 00:16:53,090
coding for ten years

00:16:51,650 --> 00:16:55,730
I apologize that sound like such a

00:16:53,090 --> 00:16:58,880
Celica a fraud sitting here saying we

00:16:55,730 --> 00:17:01,880
haven't done any coding in ages but what

00:16:58,880 --> 00:17:03,800
you guys do are difficult takes

00:17:01,880 --> 00:17:06,350
creativity stakes a lot higher level

00:17:03,800 --> 00:17:08,360
thought process as well so if you look

00:17:06,350 --> 00:17:11,090
at the the job being replace the blue

00:17:08,360 --> 00:17:12,920
line is virtually a single pixel hunt on

00:17:11,090 --> 00:17:15,470
this graph versus some of the other

00:17:12,920 --> 00:17:19,340
stuff if your sales rep which I happen

00:17:15,470 --> 00:17:20,990
to be that red guy look at me so I

00:17:19,340 --> 00:17:22,700
should move back to coding that's why I

00:17:20,990 --> 00:17:25,400
think this is a good time for me to for

00:17:22,700 --> 00:17:28,670
another career change just yet so turns

00:17:25,400 --> 00:17:34,419
out it's not that easy 2040 is a long

00:17:28,670 --> 00:17:37,669
way off still second new jobs are coming

00:17:34,419 --> 00:17:40,190
here's an interesting fact in fact 90%

00:17:37,669 --> 00:17:42,440
of all jobs basically in human history

00:17:40,190 --> 00:17:45,440
has been taken over by technology

00:17:42,440 --> 00:17:50,770
already the fact that AI will start

00:17:45,440 --> 00:17:52,909
writing code it will happen if

00:17:50,770 --> 00:17:56,450
everything else has happened in the past

00:17:52,909 --> 00:17:59,200
technology has taken jobs over in for as

00:17:56,450 --> 00:18:01,340
long as you know for the last 140 years

00:17:59,200 --> 00:18:03,309
it will happen again

00:18:01,340 --> 00:18:06,789
but here's the part that's interesting

00:18:03,309 --> 00:18:10,130
technology also create and use new jobs

00:18:06,789 --> 00:18:13,010
and as human

00:18:10,130 --> 00:18:16,760
we've always shifted our skills and our

00:18:13,010 --> 00:18:20,539
day-to-day practice to continue if you

00:18:16,760 --> 00:18:22,520
think about it 90% of all job in the

00:18:20,539 --> 00:18:24,890
past has already been taken and yet we

00:18:22,520 --> 00:18:27,380
all sitting here working out collective

00:18:24,890 --> 00:18:30,380
butts off to make sure that the global

00:18:27,380 --> 00:18:31,880
economy is running so the fact that some

00:18:30,380 --> 00:18:34,419
of the jobs that we know today is

00:18:31,880 --> 00:18:37,700
shifting away it's no big deal

00:18:34,419 --> 00:18:38,929
technology always create new jobs now

00:18:37,700 --> 00:18:43,039
how many of you would consider yourself

00:18:38,929 --> 00:18:45,500
a web developer or mobile web

00:18:43,039 --> 00:18:50,120
application developer with web design or

00:18:45,500 --> 00:18:53,299
anybody one ok imagine for a second time

00:18:50,120 --> 00:18:56,210
travel you guys fly all the way back to

00:18:53,299 --> 00:18:58,640
1991 and somebody come and ask you what

00:18:56,210 --> 00:19:01,220
do you do for a living you don't want a

00:18:58,640 --> 00:19:03,320
web designer I could move our

00:19:01,220 --> 00:19:06,830
applications they'll look at me and go

00:19:03,320 --> 00:19:08,779
well what kind of web do you eve because

00:19:06,830 --> 00:19:13,070
the world wide web didn't exist until

00:19:08,779 --> 00:19:15,590
1992 so the whole classification of web

00:19:13,070 --> 00:19:17,299
designer web application graphics

00:19:15,590 --> 00:19:23,360
designer that does all that work none of

00:19:17,299 --> 00:19:26,600
those job existed in 1991 in fact Dell

00:19:23,360 --> 00:19:29,750
did a recent study of their own and said

00:19:26,600 --> 00:19:32,840
they concluded that 85% of all jobs that

00:19:29,750 --> 00:19:36,080
will exist in 2030 hasn't even been

00:19:32,840 --> 00:19:37,549
invented yet so that's the that's the

00:19:36,080 --> 00:19:39,440
part that I think we can just take a

00:19:37,549 --> 00:19:45,380
step back and breathe because the fact

00:19:39,440 --> 00:19:46,070
that you guys are here means this my my

00:19:45,380 --> 00:19:47,650
Latins

00:19:46,070 --> 00:19:51,450
terrible but a core

00:19:47,650 --> 00:19:53,770
Amparo means steal I am learning

00:19:51,450 --> 00:19:57,910
Michelangelo said that when he was 87

00:19:53,770 --> 00:19:59,410
years old right so you guys are here

00:19:57,910 --> 00:20:01,030
because you guys are interested in new

00:19:59,410 --> 00:20:03,340
technology always sharpen your skills

00:20:01,030 --> 00:20:04,720
and transforming so I think one advice

00:20:03,340 --> 00:20:07,600
don't ever stop

00:20:04,720 --> 00:20:10,660
and the fact that we are in the lifelong

00:20:07,600 --> 00:20:12,790
learning center or Institute it's just

00:20:10,660 --> 00:20:14,620
so fitting that that that's what we here

00:20:12,790 --> 00:20:18,240
for all right so keep evolving because

00:20:14,620 --> 00:20:20,710
you have to anyways it's nothing new

00:20:18,240 --> 00:20:22,480
second it's going to be about a

00:20:20,710 --> 00:20:24,460
partnership between human being and

00:20:22,480 --> 00:20:28,240
machines humans are good at certain

00:20:24,460 --> 00:20:29,770
things common sense probably a silly

00:20:28,240 --> 00:20:31,570
word to use because common sense usually

00:20:29,770 --> 00:20:33,580
are not that common once we outside this

00:20:31,570 --> 00:20:35,860
room I find calm you know most

00:20:33,580 --> 00:20:37,120
programmers are pretty good at common

00:20:35,860 --> 00:20:40,050
sense but most of the other guys out

00:20:37,120 --> 00:20:42,900
there not so sure common sense

00:20:40,050 --> 00:20:45,520
compassion dreaming extraction and

00:20:42,900 --> 00:20:47,679
generalization those are two elements

00:20:45,520 --> 00:20:50,770
that is critical if you think about what

00:20:47,679 --> 00:20:53,710
we do what a machine good at learning

00:20:50,770 --> 00:20:55,720
new languages the truth is very new

00:20:53,710 --> 00:20:59,290
future the computer will speak you know

00:20:55,720 --> 00:21:01,660
38 languages seamlessly and we will

00:20:59,290 --> 00:21:04,150
struggle to speak one for me anyway I

00:21:01,660 --> 00:21:07,809
struggle to speak one and my French is

00:21:04,150 --> 00:21:11,500
it's a it's non-existent in my my wife

00:21:07,809 --> 00:21:12,760
makes fun of my Chinese so natural

00:21:11,500 --> 00:21:14,800
language is a big one pattern

00:21:12,760 --> 00:21:16,080
recognition unlimited capabilities in

00:21:14,800 --> 00:21:18,850
terms of recognizing patterns

00:21:16,080 --> 00:21:20,050
remembering things so there's a little

00:21:18,850 --> 00:21:21,520
bit of a marriage that's gonna happen

00:21:20,050 --> 00:21:23,740
here and I think we all heard this

00:21:21,520 --> 00:21:26,350
before right so how do you actually

00:21:23,740 --> 00:21:28,450
match the two things together so that we

00:21:26,350 --> 00:21:31,840
continue to be relevant in this new AI

00:21:28,450 --> 00:21:33,130
based economy it's going to be about

00:21:31,840 --> 00:21:37,270
this partnership so I'll move that

00:21:33,130 --> 00:21:38,920
forward here's a bit of a study that I

00:21:37,270 --> 00:21:42,190
found in KD Nuggets last night around

00:21:38,920 --> 00:21:44,620
3:00 a.m. I'm reading away going and

00:21:42,190 --> 00:21:47,920
there's a little bio that that's there

00:21:44,620 --> 00:21:50,860
that you can find out the articles are

00:21:47,920 --> 00:21:52,870
interesting your AI skills worth less

00:21:50,860 --> 00:21:54,429
than you think so again one of these

00:21:52,870 --> 00:21:56,080
challenging peyto that's that's how I

00:21:54,429 --> 00:21:58,630
came across it because I was preparing

00:21:56,080 --> 00:22:01,270
for this talk one of the things that

00:21:58,630 --> 00:22:03,280
that was found out these days

00:22:01,270 --> 00:22:05,920
the author actually tested two models

00:22:03,280 --> 00:22:07,900
and the author actually worked in Google

00:22:05,920 --> 00:22:09,250
was part of the team that tinkered

00:22:07,900 --> 00:22:13,030
around with with tensorflow

00:22:09,250 --> 00:22:14,740
and so on and he was playing around two

00:22:13,030 --> 00:22:16,630
models one day he 50 deemed to be better

00:22:14,740 --> 00:22:18,640
when they deemed to be worse he started

00:22:16,630 --> 00:22:22,110
training his model and that's the

00:22:18,640 --> 00:22:24,730
accuracy performance between the two and

00:22:22,110 --> 00:22:28,150
one of the things that he came came to

00:22:24,730 --> 00:22:29,640
realize is that deep skills encoding the

00:22:28,150 --> 00:22:33,090
fact that you building a better model

00:22:29,640 --> 00:22:35,710
it's much less relevant than good data

00:22:33,090 --> 00:22:38,620
so the fact that code is being generated

00:22:35,710 --> 00:22:40,210
it's not a big problem you need good

00:22:38,620 --> 00:22:43,210
data to create great machine learning

00:22:40,210 --> 00:22:46,720
models in fact he's noticed that if you

00:22:43,210 --> 00:22:52,020
train the bad model with like 30,000 or

00:22:46,720 --> 00:22:54,700
something like 30 or 40,000 data point

00:22:52,020 --> 00:22:58,060
versus a good model with 30,000 data

00:22:54,700 --> 00:22:59,620
point the one the bad model you train

00:22:58,060 --> 00:23:03,670
with more data it's going to perform

00:22:59,620 --> 00:23:07,030
better so one thing that you can do in

00:23:03,670 --> 00:23:09,970
this space I think is the the piece that

00:23:07,030 --> 00:23:14,080
I mentioned about before on abstraction

00:23:09,970 --> 00:23:15,940
and generalization we as human curate

00:23:14,080 --> 00:23:18,910
data set for machine learning to happen

00:23:15,940 --> 00:23:21,160
that curation is not going to change you

00:23:18,910 --> 00:23:23,320
know at least I am not seeing algorithm

00:23:21,160 --> 00:23:25,630
that says the curation is going to be

00:23:23,320 --> 00:23:27,040
automated in a way that in a meaningful

00:23:25,630 --> 00:23:28,390
way there's some work being done in that

00:23:27,040 --> 00:23:30,430
space but I think we're going to

00:23:28,390 --> 00:23:32,110
continue to play a big role we will be

00:23:30,430 --> 00:23:34,060
actually the teacher of these AI

00:23:32,110 --> 00:23:36,700
capabilities and will be making that

00:23:34,060 --> 00:23:39,160
generalization judgment call to help AI

00:23:36,700 --> 00:23:42,430
because you need good data to create

00:23:39,160 --> 00:23:45,340
good AI and some of that decision is

00:23:42,430 --> 00:23:47,650
really not so easily programmed right

00:23:45,340 --> 00:23:49,360
because the truth is there is no context

00:23:47,650 --> 00:23:50,830
when deep learning happens there's a

00:23:49,360 --> 00:23:52,690
bunch of ones and zeroes and pattern

00:23:50,830 --> 00:23:54,610
recognitions if you gives it the wrong

00:23:52,690 --> 00:23:58,870
sets of ones and zeroes it's gonna come

00:23:54,610 --> 00:24:01,240
up the wrong pattern I think the new

00:23:58,870 --> 00:24:02,920
role of the developers becomes one where

00:24:01,240 --> 00:24:05,710
you are the collaborator the

00:24:02,920 --> 00:24:09,340
orchestrator the conductor and the

00:24:05,710 --> 00:24:12,700
supervisors more and more so you can

00:24:09,340 --> 00:24:14,919
start to trust generate a capability so

00:24:12,700 --> 00:24:17,739
we ship different products right and

00:24:14,919 --> 00:24:19,179
is one of the things that we ship IBM

00:24:17,739 --> 00:24:21,869
cloud private for data and there's a few

00:24:19,179 --> 00:24:25,779
things in here that's interesting one

00:24:21,869 --> 00:24:30,129
it's open we use open source technology

00:24:25,779 --> 00:24:31,480
a ICAI 360 one thing that that I knew

00:24:30,129 --> 00:24:33,519
talked about earlier something that we

00:24:31,480 --> 00:24:36,489
open source and we offer for AI fairness

00:24:33,519 --> 00:24:38,289
so we are very much looking at open

00:24:36,489 --> 00:24:40,200
source capabilities we do very much

00:24:38,289 --> 00:24:42,850
spend a lot of time working with

00:24:40,200 --> 00:24:45,879
different framework late tends to flow

00:24:42,850 --> 00:24:48,220
cafe PI towards all that will work

00:24:45,879 --> 00:24:50,440
within this with this this this

00:24:48,220 --> 00:24:52,259
framework but the idea is really to

00:24:50,440 --> 00:24:56,519
start creating trust and transparency

00:24:52,259 --> 00:24:59,049
again something that that's a key focus

00:24:56,519 --> 00:25:01,960
part of what we need to do I think and

00:24:59,049 --> 00:25:05,639
this is part of the things that a lot of

00:25:01,960 --> 00:25:08,379
a world leader you Elon Musk and Stephen

00:25:05,639 --> 00:25:14,549
Hawking's and various people have spoken

00:25:08,379 --> 00:25:19,299
out to say that AI is dangerous okay I

00:25:14,549 --> 00:25:21,429
think that's a sure yeah I can't be

00:25:19,299 --> 00:25:23,830
dangerous anything when misuse can be

00:25:21,429 --> 00:25:26,559
dangerous but I think that's much more

00:25:23,830 --> 00:25:27,690
is more and more critical to monitor to

00:25:26,559 --> 00:25:30,039
make sure that you don't you're not

00:25:27,690 --> 00:25:32,049
unnecessarily biasing your decision to

00:25:30,039 --> 00:25:34,059
make sure that transparency is there and

00:25:32,049 --> 00:25:37,539
that's why we invested so much in

00:25:34,059 --> 00:25:39,789
helping developer that create codes to

00:25:37,539 --> 00:25:41,200
be able to monitor and to be able to

00:25:39,789 --> 00:25:43,659
make sure that there's trust compliance

00:25:41,200 --> 00:25:45,249
and and that sort of capabilities build

00:25:43,659 --> 00:25:50,080
in I think I'm running short on time

00:25:45,249 --> 00:25:52,029
already so another set so to monitoring

00:25:50,080 --> 00:25:55,239
and then collaborating with teammates

00:25:52,029 --> 00:25:57,759
creating AI that you can trust ethically

00:25:55,239 --> 00:25:59,499
I if you would free of biases or

00:25:57,759 --> 00:26:02,169
unnecessary by Z I say that because

00:25:59,499 --> 00:26:06,489
sometimes it's not a bad thing to have

00:26:02,169 --> 00:26:07,899
bias in your model well okay it is a bad

00:26:06,489 --> 00:26:09,669
thing to have bias in your model how

00:26:07,899 --> 00:26:14,289
many data scientist is in the room okay

00:26:09,669 --> 00:26:16,619
so biases as is so the AI temper bias or

00:26:14,289 --> 00:26:18,609
from it's really about underfitting your

00:26:16,619 --> 00:26:20,559
your date so you don't have enough data

00:26:18,609 --> 00:26:22,539
set to to create a good model as opposed

00:26:20,559 --> 00:26:23,980
to overfitting but that bias is not

00:26:22,539 --> 00:26:26,649
really a good thing so you try to make

00:26:23,980 --> 00:26:28,240
shoes bias free right but explain

00:26:26,649 --> 00:26:32,340
ability and so on

00:26:28,240 --> 00:26:35,110
I think the how many more slider I got

00:26:32,340 --> 00:26:36,700
okay I'll just do this and then I'll

00:26:35,110 --> 00:26:40,779
jump to the conclusion bit I think as a

00:26:36,700 --> 00:26:43,119
interest of time what are the advice I

00:26:40,779 --> 00:26:44,590
found that keeps happening in all the

00:26:43,119 --> 00:26:46,840
readings that I was doing to prepare for

00:26:44,590 --> 00:26:49,860
this talk it's to start looking at how

00:26:46,840 --> 00:26:54,039
to use AI as a lever for better outcomes

00:26:49,860 --> 00:26:55,480
not necessarily sell AI but to use AI to

00:26:54,039 --> 00:26:59,080
help sell whatever it is that you're

00:26:55,480 --> 00:27:02,950
selling that's another key aspect that

00:26:59,080 --> 00:27:05,889
we should be looking at this only it's a

00:27:02,950 --> 00:27:08,799
very very small market to create new AI

00:27:05,889 --> 00:27:10,960
algorithm it's a very much bigger market

00:27:08,799 --> 00:27:14,619
to apply those AI in two different areas

00:27:10,960 --> 00:27:17,710
to actually help make money so if you if

00:27:14,619 --> 00:27:20,649
your plan is to build a better image

00:27:17,710 --> 00:27:22,480
recognition algorithm that boat has

00:27:20,649 --> 00:27:25,059
sails that boat has sailed long ago

00:27:22,480 --> 00:27:28,090
right no point doing that but if your

00:27:25,059 --> 00:27:30,369
goal is to use existing capabilities to

00:27:28,090 --> 00:27:33,519
enhance something or use that visual

00:27:30,369 --> 00:27:37,019
recognition capabilities tweak it

00:27:33,519 --> 00:27:39,789
retrain it to recognize a radiology

00:27:37,019 --> 00:27:42,039
images that's maybe something different

00:27:39,789 --> 00:27:44,619
right so use it to enhance what you're

00:27:42,039 --> 00:27:48,539
doing today rather than sell the actual

00:27:44,619 --> 00:27:50,980
technology itself and part of that is

00:27:48,539 --> 00:27:54,009
sometimes to actually leverage tools

00:27:50,980 --> 00:27:56,860
that you have available to you that can

00:27:54,009 --> 00:27:59,409
help in terms of making it quicker to go

00:27:56,860 --> 00:28:01,899
to market so I'm saying since you're not

00:27:59,409 --> 00:28:05,019
interested inventing image recognition

00:28:01,899 --> 00:28:06,970
don't write it from scratch right use

00:28:05,019 --> 00:28:08,320
the tools that's available to you and I

00:28:06,970 --> 00:28:11,679
think Anoop showed you earlier today

00:28:08,320 --> 00:28:15,249
that new nets is a new capabilities that

00:28:11,679 --> 00:28:19,179
we're providing where for images and

00:28:15,249 --> 00:28:20,830
speech you can provide a data set and it

00:28:19,179 --> 00:28:23,350
will actually do the model generation

00:28:20,830 --> 00:28:25,629
and all the hyper parameterization

00:28:23,350 --> 00:28:28,809
tuning for you so that you can have a

00:28:25,629 --> 00:28:30,159
very accurate deep learning model

00:28:28,809 --> 00:28:32,470
without you actually writing any code

00:28:30,159 --> 00:28:34,749
you can then take it and tweak it even

00:28:32,470 --> 00:28:38,049
further that's up to you but you use the

00:28:34,749 --> 00:28:39,759
tools use it as a lever you know don't

00:28:38,049 --> 00:28:41,980
make it a research project right so

00:28:39,759 --> 00:28:44,290
start thinking that way

00:28:41,980 --> 00:28:46,150
I guess my advice also is because

00:28:44,290 --> 00:28:47,890
because we keep an eye on some of the

00:28:46,150 --> 00:28:49,929
stuff that's developing right Blake

00:28:47,890 --> 00:28:51,520
tensorflow came out the tensorflow was

00:28:49,929 --> 00:28:52,360
really unusable so then they wrote this

00:28:51,520 --> 00:28:53,799
thing talker

00:28:52,360 --> 00:28:55,840
curious that actually helps it become

00:28:53,799 --> 00:28:58,299
usable so if you're not watching an

00:28:55,840 --> 00:29:03,030
adopt and out thing and it uses writing

00:28:58,299 --> 00:29:06,010
pencil flow you're gonna fall behind so

00:29:03,030 --> 00:29:07,510
to s you stop write some of the other

00:29:06,010 --> 00:29:10,090
stuff that AI can do

00:29:07,510 --> 00:29:12,460
I learned something new the other day

00:29:10,090 --> 00:29:14,799
these this just X thing IBM X modified

00:29:12,460 --> 00:29:16,650
that means collab mister collaboration

00:29:14,799 --> 00:29:18,970
it's a cool thing to do these days

00:29:16,650 --> 00:29:21,340
fashion brand does this with like you

00:29:18,970 --> 00:29:23,980
know supreme t-shirts with like Louis

00:29:21,340 --> 00:29:27,070
Vuitton purses or something so IBM had a

00:29:23,980 --> 00:29:30,640
collaboration Spotify to write to help

00:29:27,070 --> 00:29:32,500
write music so you think AI is you know

00:29:30,640 --> 00:29:34,480
it's writing program well yeah is

00:29:32,500 --> 00:29:35,559
actually writing music so in this case

00:29:34,480 --> 00:29:37,630
this is something that you can actually

00:29:35,559 --> 00:29:41,140
go and have a listen when you have a

00:29:37,630 --> 00:29:45,040
moment we actually use AI to help a

00:29:41,140 --> 00:29:47,710
music producer compose a song by

00:29:45,040 --> 00:29:50,890
studying musical composition and then

00:29:47,710 --> 00:29:54,460
matching that with sentiment analysis on

00:29:50,890 --> 00:29:57,010
social social data set we're the the AI

00:29:54,460 --> 00:29:59,470
is basically able to suggest what type

00:29:57,010 --> 00:30:02,110
of composition evoke what kind of

00:29:59,470 --> 00:30:04,900
emotion in the listener they did the

00:30:02,110 --> 00:30:08,260
same thing with to 20,000 songs and the

00:30:04,900 --> 00:30:10,090
Billboard top 100 for lyrics so what

00:30:08,260 --> 00:30:12,460
kind of words in what combination

00:30:10,090 --> 00:30:14,080
creates what kind of emotion then the

00:30:12,460 --> 00:30:15,000
music producer Alex the kid I don't know

00:30:14,080 --> 00:30:18,520
who that is

00:30:15,000 --> 00:30:20,559
just wrote just basically ask similar to

00:30:18,520 --> 00:30:23,200
a programmer that I mentioned before in

00:30:20,559 --> 00:30:25,450
when by you sending in a high level

00:30:23,200 --> 00:30:30,340
requirement he sends in an emotion to

00:30:25,450 --> 00:30:32,590
try to evoke and actually come that the

00:30:30,340 --> 00:30:35,650
AI has to come back with sample

00:30:32,590 --> 00:30:38,650
composition sample words to be used as a

00:30:35,650 --> 00:30:42,040
collaboration between AI and human being

00:30:38,650 --> 00:30:44,740
now pretty easy to kind of take that

00:30:42,040 --> 00:30:46,929
extension go well how's that what does

00:30:44,740 --> 00:30:50,320
that mean to evolving job or you love

00:30:46,929 --> 00:30:54,170
job market how's that changing the way

00:30:50,320 --> 00:30:58,160
the way we work maybe

00:30:54,170 --> 00:31:01,100
year 2040 comes along maybe you no

00:30:58,160 --> 00:31:03,350
longer have music producer or DJs maybe

00:31:01,100 --> 00:31:04,460
the new job title of music producer

00:31:03,350 --> 00:31:08,090
involved you

00:31:04,460 --> 00:31:11,390
creating a genetic algorithm that

00:31:08,090 --> 00:31:13,970
generate music and maybe you create two

00:31:11,390 --> 00:31:15,920
or three or four of these AI BOTS and

00:31:13,970 --> 00:31:18,680
you get them to work together to

00:31:15,920 --> 00:31:20,240
generate music 24/7 maybe that's what

00:31:18,680 --> 00:31:23,930
make that's what it means to be a

00:31:20,240 --> 00:31:25,460
musician twenty five years from now so

00:31:23,930 --> 00:31:27,260
maybe that's a whole new thing that

00:31:25,460 --> 00:31:30,140
hasn't happened yet I mean the whole

00:31:27,260 --> 00:31:33,700
concept of DJ's and me and DJs has been

00:31:30,140 --> 00:31:37,730
s musician David Guetta all these guys

00:31:33,700 --> 00:31:38,990
bring back 40 years 50 45 50 years a DJ

00:31:37,730 --> 00:31:42,250
just some guy sitting on a radio

00:31:38,990 --> 00:31:44,690
spinning records DJ today is a superstar

00:31:42,250 --> 00:31:47,450
it's nothing that says that that a

00:31:44,690 --> 00:31:49,520
programmer were no longer programming

00:31:47,450 --> 00:31:51,650
basic algorithm wouldn't be sitting out

00:31:49,520 --> 00:31:54,020
there creating AI and neural network

00:31:51,650 --> 00:31:57,110
algorithm that generate music and you

00:31:54,020 --> 00:31:59,330
may be the next music superstar who

00:31:57,110 --> 00:32:01,280
knows right so with that I'll stop

00:31:59,330 --> 00:32:02,630
because I'm already over time Lauren's

00:32:01,280 --> 00:32:03,170
looking at me saying get the heck off of

00:32:02,630 --> 00:32:05,780
the stage

00:32:03,170 --> 00:32:07,670
I hope this what too boring for you and

00:32:05,780 --> 00:32:14,440
then lighten up your afternoon a little

00:32:07,670 --> 00:32:16,940
bit thank you so much very much Kidman

00:32:14,440 --> 00:32:22,460
we have time for a question

00:32:16,940 --> 00:32:25,280
maybe thank you very interesting I guess

00:32:22,460 --> 00:32:28,370
my thinking around this is around this

00:32:25,280 --> 00:32:31,130
narrow AI versus general AI right I mean

00:32:28,370 --> 00:32:34,040
will will a I replace programmers is

00:32:31,130 --> 00:32:36,170
really a question of is it open is it

00:32:34,040 --> 00:32:37,940
general AI general intelligence or

00:32:36,170 --> 00:32:40,760
narrow intelligence cuz narrow AI is

00:32:37,940 --> 00:32:42,980
essentially an optimizer I mean right so

00:32:40,760 --> 00:32:45,860
I think it's doing narrow yeah use case

00:32:42,980 --> 00:32:47,630
right right yeah so so I guess the

00:32:45,860 --> 00:32:49,100
question to whether where the AI will

00:32:47,630 --> 00:32:51,610
replace programmers is it's more

00:32:49,100 --> 00:32:54,620
creativity and you know intention

00:32:51,610 --> 00:32:56,270
exactly well I think it's back you you

00:32:54,620 --> 00:32:58,280
right on I think it's the fact that it's

00:32:56,270 --> 00:33:02,060
a collaboration that that yields a more

00:32:58,280 --> 00:33:03,230
powerful team yeah today it's no

00:33:02,060 --> 00:33:05,930
different than let's say today you use

00:33:03,230 --> 00:33:07,830
an IDE to improve your to improve your

00:33:05,930 --> 00:33:09,990
productivity tomorrow

00:33:07,830 --> 00:33:11,700
that idea of AI elements in it that

00:33:09,990 --> 00:33:13,940
allows you to be the person that

00:33:11,700 --> 00:33:17,010
orchestrated the compose that that

00:33:13,940 --> 00:33:18,720
direct what happened as opposed to the

00:33:17,010 --> 00:33:23,760
person who write the line align the

00:33:18,720 --> 00:33:25,230
individual lines of code so short answer

00:33:23,760 --> 00:33:27,809
I think it's an arrowy either the helps

00:33:25,230 --> 00:33:29,880
instead of replace so that what is IBM

00:33:27,809 --> 00:33:32,549
doing in the in the realm of general AI

00:33:29,880 --> 00:33:35,130
I mean how do I mean maybe is beyond

00:33:32,549 --> 00:33:39,779
your scope but how do you then think

00:33:35,130 --> 00:33:41,820
about AI becoming general AI that is

00:33:39,779 --> 00:33:44,220
more capable like you know essentially

00:33:41,820 --> 00:33:47,460
enhancing the capabilities of AI to a

00:33:44,220 --> 00:33:49,340
quantum level so brought up quantum

00:33:47,460 --> 00:33:51,899
that's an interesting one as well

00:33:49,340 --> 00:33:53,610
because we do we are actually working on

00:33:51,899 --> 00:33:55,649
some quantum computing capabilities and

00:33:53,610 --> 00:34:00,210
who knows what they were what that will

00:33:55,649 --> 00:34:03,149
bring IBM research has been a driving

00:34:00,210 --> 00:34:04,740
force if you would behind much of the AI

00:34:03,149 --> 00:34:07,500
technology as well as various

00:34:04,740 --> 00:34:10,080
advancement in terms of things that we

00:34:07,500 --> 00:34:14,190
that I have seen coming to market our

00:34:10,080 --> 00:34:18,210
focus is still me in narrow AI I think

00:34:14,190 --> 00:34:21,480
from the generator spective what I can

00:34:18,210 --> 00:34:26,280
see is more research around necessary

00:34:21,480 --> 00:34:30,450
hardware to advance that forward to make

00:34:26,280 --> 00:34:34,020
it a viable use case today I my personal

00:34:30,450 --> 00:34:37,260
belief is that today generalize not

00:34:34,020 --> 00:34:39,000
really hasn't been quite all that

00:34:37,260 --> 00:34:42,119
successful in actually in creating

00:34:39,000 --> 00:34:44,550
computer to be a general AI as part of

00:34:42,119 --> 00:34:47,089
it is compute related so that the

00:34:44,550 --> 00:34:50,490
resource I have seen are things like

00:34:47,089 --> 00:34:53,429
neurosynaptic chipset for example that

00:34:50,490 --> 00:34:55,649
increases the the compute density to a

00:34:53,429 --> 00:34:57,869
point where maybe we can start tackling

00:34:55,649 --> 00:34:59,970
that problem set so that there's a whole

00:34:57,869 --> 00:35:02,970
set of research under dark power

00:34:59,970 --> 00:35:05,900
initiative right now to create I think

00:35:02,970 --> 00:35:09,080
they laid the test that latest one has a

00:35:05,900 --> 00:35:12,089
4096 cores on a single chip

00:35:09,080 --> 00:35:13,890
running on like 1/10 the power right the

00:35:12,089 --> 00:35:17,940
idea is to create something that mimics

00:35:13,890 --> 00:35:21,090
the human synapse to allow high density

00:35:17,940 --> 00:35:23,360
compute and that compute may one day

00:35:21,090 --> 00:35:28,260
create generally I I mean that they

00:35:23,360 --> 00:35:31,290
probably come sooner than we think in

00:35:28,260 --> 00:35:33,120
fact I talked about Stephen Hawking's

00:35:31,290 --> 00:35:35,820
way too much because he's kind of passed

00:35:33,120 --> 00:35:39,080
away last year and but Stephen Hawking's

00:35:35,820 --> 00:35:42,000
actually one way school is that and at a

00:35:39,080 --> 00:35:43,530
at a certain level he believes that

00:35:42,000 --> 00:35:46,830
there's really no fundamental difference

00:35:43,530 --> 00:35:51,950
between a biological versus a mechanic's

00:35:46,830 --> 00:35:55,380
computer in terms of ability to mimic

00:35:51,950 --> 00:35:57,840
human in human thought process if you

00:35:55,380 --> 00:35:59,880
would so so I think it will come I don't

00:35:57,840 --> 00:36:00,780
think it's there yet but the research

00:35:59,880 --> 00:36:02,280
that we're doing right now I think

00:36:00,780 --> 00:36:05,490
primarily around hardware that I can see

00:36:02,280 --> 00:36:07,500
but then you know I don't know what they

00:36:05,490 --> 00:36:12,840
do all the time they close the door and

00:36:07,500 --> 00:36:16,520
don't let me in thank you so much thank

00:36:12,840 --> 00:36:16,520

YouTube URL: https://www.youtube.com/watch?v=txxFOofqV1s


