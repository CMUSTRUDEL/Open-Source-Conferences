Title: Develop AI at the network edge by Mashrin Srivastava
Publication date: 2019-04-01
Playlist: FOSSASIA Summit 2019 - Lightning Talks
Description: 
	17 March 2019 10:30, Lecture Theatre

Today, in most cases, IoT and AI work together in the cloud. Data from IoT devices is transmitted back to a central hub in the cloud where it is analyzed and stored and actionable insights are sent back to the device. In this talk, I will discuss some real-life use-cases of performing AI on edge and its importance; followed by the challenges of the presently used approaches in terms of latency, network connectivity, privacy, cost, etc and its possible solutions.
Captions: 
	00:00:00,120 --> 00:00:03,360
so basically what I am talking about

00:00:01,469 --> 00:00:06,109
here is it will give me a minute please

00:00:03,360 --> 00:00:10,130
yeah and just set it to and you can just

00:00:06,109 --> 00:00:12,509
so guess what I'm talking about here is

00:00:10,130 --> 00:00:15,170
why you need artificial intelligence to

00:00:12,509 --> 00:00:17,970
start with so you know the scarcity of

00:00:15,170 --> 00:00:21,480
talent and human resources it's like we

00:00:17,970 --> 00:00:23,220
can start from flight two so if I take a

00:00:21,480 --> 00:00:25,590
typical use case in you know case of

00:00:23,220 --> 00:00:27,720
India oh one you know hundred thousand

00:00:25,590 --> 00:00:30,539
schools have just one teacher there is a

00:00:27,720 --> 00:00:33,510
very poor you know patient to dr. ratio

00:00:30,539 --> 00:00:37,200
so you know the human talent is cars and

00:00:33,510 --> 00:00:39,090
AI can actually help you know help it

00:00:37,200 --> 00:00:41,520
reach the next six billion people who

00:00:39,090 --> 00:00:42,950
are not you know having access to it

00:00:41,520 --> 00:00:45,690
next slide please

00:00:42,950 --> 00:00:47,850
so across the sector you know AI can

00:00:45,690 --> 00:00:50,610
help Skaar help scale the scarce human

00:00:47,850 --> 00:00:53,820
resources next slide please so in a

00:00:50,610 --> 00:00:55,379
typical AI development cycle right so we

00:00:53,820 --> 00:00:57,539
do realize now that you know it is

00:00:55,379 --> 00:01:00,329
important that we use artificial

00:00:57,539 --> 00:01:02,670
intelligence in a very you know good

00:01:00,329 --> 00:01:04,650
manner wherever required for giving

00:01:02,670 --> 00:01:06,479
access to many more people but in

00:01:04,650 --> 00:01:07,950
typical AI development life cycle we

00:01:06,479 --> 00:01:09,659
have this four stages it starts with

00:01:07,950 --> 00:01:11,670
that data acquisition writing an

00:01:09,659 --> 00:01:13,670
architect's file training it to get your

00:01:11,670 --> 00:01:16,470
weights and biases and then deploying it

00:01:13,670 --> 00:01:18,750
so this is the deployment phase which I

00:01:16,470 --> 00:01:20,070
am talking about here and one thing we

00:01:18,750 --> 00:01:21,659
need to realize is that you know your

00:01:20,070 --> 00:01:24,270
training environment where you train and

00:01:21,659 --> 00:01:26,310
actually build your model with your GPUs

00:01:24,270 --> 00:01:27,780
it's not the same as your deployment

00:01:26,310 --> 00:01:29,610
environment your end-users will not have

00:01:27,780 --> 00:01:30,720
the same GPUs there so it's very

00:01:29,610 --> 00:01:32,400
important to treat both of them

00:01:30,720 --> 00:01:33,990
differently and the deployment phase is

00:01:32,400 --> 00:01:36,229
what I am going to talk about today next

00:01:33,990 --> 00:01:36,229
slide please

00:01:36,740 --> 00:01:42,869
so think of scenario that you know the

00:01:39,299 --> 00:01:44,880
human body works as a sensor and think

00:01:42,869 --> 00:01:46,890
of that you are falling and do to make

00:01:44,880 --> 00:01:48,659
that information reach your brain or the

00:01:46,890 --> 00:01:50,070
central system and then you give that

00:01:48,659 --> 00:01:51,420
and get those and sensor information

00:01:50,070 --> 00:01:53,189
into the brain and then it gives back

00:01:51,420 --> 00:01:54,990
there's a lot of latency it's if we keep

00:01:53,189 --> 00:01:56,490
working in the way that we've used today

00:01:54,990 --> 00:01:59,210
the cloud way there's a lot of the

00:01:56,490 --> 00:02:01,979
network latency next slide please so

00:01:59,210 --> 00:02:04,610
okay so what is edge AI so what I'm

00:02:01,979 --> 00:02:07,590
talking about here is what for all those

00:02:04,610 --> 00:02:09,000
for the AI work we started with GPUs and

00:02:07,590 --> 00:02:10,590
then we realized that you know GPUs are

00:02:09,000 --> 00:02:12,300
expensive there's a lot of application

00:02:10,590 --> 00:02:12,970
for deployment where we don't actually

00:02:12,300 --> 00:02:14,620
need a cheap

00:02:12,970 --> 00:02:16,570
you but when we added in the end

00:02:14,620 --> 00:02:17,950
products it's very difficult to position

00:02:16,570 --> 00:02:20,770
in the market because that one component

00:02:17,950 --> 00:02:24,610
is extremely expensive so people move to

00:02:20,770 --> 00:02:25,840
different cloud you know solutions

00:02:24,610 --> 00:02:28,750
because it's pay as you use

00:02:25,840 --> 00:02:30,100
subscription-based and then what

00:02:28,750 --> 00:02:32,230
happened is now it comes with its own

00:02:30,100 --> 00:02:33,580
set of problems and limitation that that

00:02:32,230 --> 00:02:35,280
is you know privacy security because

00:02:33,580 --> 00:02:37,690
it's going over the network this network

00:02:35,280 --> 00:02:38,350
latency there's bandwidth issues next

00:02:37,690 --> 00:02:41,680
slide please

00:02:38,350 --> 00:02:43,990
so so yeaj a quicker response you have

00:02:41,680 --> 00:02:46,000
everything central on the nodes so like

00:02:43,990 --> 00:02:47,200
latency is less you have better security

00:02:46,000 --> 00:02:50,110
and privacy everything in the network

00:02:47,200 --> 00:02:51,459
you know you have lesser band bandwidth

00:02:50,110 --> 00:02:53,440
requirement because you have less data

00:02:51,459 --> 00:02:55,270
sync you don't send just everything only

00:02:53,440 --> 00:02:57,730
important stuff is in the cloud or the

00:02:55,270 --> 00:03:00,070
central node less communication cost

00:02:57,730 --> 00:03:01,959
because of that reason reduce dependency

00:03:00,070 --> 00:03:03,670
on the network and reduce power

00:03:01,959 --> 00:03:06,610
consumption next slide please

00:03:03,670 --> 00:03:08,020
so why should you care so you know right

00:03:06,610 --> 00:03:10,630
now from six percent and twenty

00:03:08,020 --> 00:03:14,020
seventeen it the the number of devices

00:03:10,630 --> 00:03:15,670
with a ji functionality will reach seven

00:03:14,020 --> 00:03:16,810
you know will increase seven to forty

00:03:15,670 --> 00:03:20,200
three percent by a twenty twenty-three

00:03:16,810 --> 00:03:21,850
next slide please so the next slide

00:03:20,200 --> 00:03:23,440
please so I will be taking you know

00:03:21,850 --> 00:03:27,340
telling you about two two things which

00:03:23,440 --> 00:03:28,630
can help in deploying your applications

00:03:27,340 --> 00:03:30,489
on the network edge so first thing here

00:03:28,630 --> 00:03:34,630
is the noodle compute stick so this is a

00:03:30,489 --> 00:03:36,489
tiny very low form factor device what

00:03:34,630 --> 00:03:38,380
happens here is you go train your model

00:03:36,489 --> 00:03:40,540
anywhere like training environment or

00:03:38,380 --> 00:03:43,239
not restricted you good you know use a

00:03:40,540 --> 00:03:45,220
discrete GPU have a cloud instance and

00:03:43,239 --> 00:03:46,570
train your model now you have unused

00:03:45,220 --> 00:03:48,239
whatever framework you want to a lot of

00:03:46,570 --> 00:03:50,680
people now have a problem with you know

00:03:48,239 --> 00:03:52,000
being comfortable with different

00:03:50,680 --> 00:03:54,250
framework if you are organization

00:03:52,000 --> 00:03:56,500
someone will be comfortable in you know

00:03:54,250 --> 00:03:59,760
why not someone else in cafes someone

00:03:56,500 --> 00:04:02,170
tensorflow maybe so what happens now is

00:03:59,760 --> 00:04:04,299
so again one thing I'll be showing you

00:04:02,170 --> 00:04:06,280
in the next slide will be the open vino

00:04:04,299 --> 00:04:08,260
so what it does is you give your model

00:04:06,280 --> 00:04:09,970
in any of these frameworks at the end of

00:04:08,260 --> 00:04:11,709
it is our intermediate representation

00:04:09,970 --> 00:04:14,170
that is the XML and Bill so I have a

00:04:11,709 --> 00:04:16,030
model architecture from that whatever

00:04:14,170 --> 00:04:18,250
model you want and I have your weights

00:04:16,030 --> 00:04:20,440
and biases as a binary blob and then

00:04:18,250 --> 00:04:21,880
once I have the XML and the binary blob

00:04:20,440 --> 00:04:24,099
it becomes framework independent then

00:04:21,880 --> 00:04:26,889
and there now again I can just deploy it

00:04:24,099 --> 00:04:30,069
to any framework that I want be it a CPU

00:04:26,889 --> 00:04:33,550
a GPU immediate which this is a mirrored

00:04:30,069 --> 00:04:35,710
by the way or what FPGA is so it is you

00:04:33,550 --> 00:04:37,449
know becoming both framework and

00:04:35,710 --> 00:04:39,370
platform independent for you and your

00:04:37,449 --> 00:04:41,740
developers in a complete manner for this

00:04:39,370 --> 00:04:44,319
device this is a noodle compute stick so

00:04:41,740 --> 00:04:46,750
it is just $99 and what happens is you

00:04:44,319 --> 00:04:48,520
know you just offload your model here

00:04:46,750 --> 00:04:49,900
and all the processing happens here so

00:04:48,520 --> 00:04:52,000
even with the add-on Celeron or

00:04:49,900 --> 00:04:54,250
Raspberry Pi for that matter you can

00:04:52,000 --> 00:04:55,509
just infer your deep learning models so

00:04:54,250 --> 00:04:57,370
just imagine this you know something

00:04:55,509 --> 00:05:00,189
like a Celeron processor and a Raspberry

00:04:57,370 --> 00:05:01,270
Pi is now able to infer your deep

00:05:00,189 --> 00:05:03,250
learning model and you don't actually

00:05:01,270 --> 00:05:06,069
need to go for those expensive GPU

00:05:03,250 --> 00:05:07,659
solution as well as you know have all

00:05:06,069 --> 00:05:10,029
the limitations that you have with the

00:05:07,659 --> 00:05:12,669
cloud so there is to end this there is

00:05:10,029 --> 00:05:13,990
no one right platform but you know be

00:05:12,669 --> 00:05:16,150
intelligent and smart enough for

00:05:13,990 --> 00:05:18,009
choosing the right platforms for each

00:05:16,150 --> 00:05:19,990
component of a product so you just don't

00:05:18,009 --> 00:05:21,879
need to just have a GPU and continue it

00:05:19,990 --> 00:05:23,529
or a cloud and continue with it so

00:05:21,879 --> 00:05:25,150
there's not one right platform just

00:05:23,529 --> 00:05:28,090
thing that you know at each phase which

00:05:25,150 --> 00:05:28,719
one is required and make use of it so

00:05:28,090 --> 00:05:30,969
next slide please

00:05:28,719 --> 00:05:32,830
this is the picture for the open V know

00:05:30,969 --> 00:05:35,620
this is open source tool and free to use

00:05:32,830 --> 00:05:39,639
so you have your trained model in any of

00:05:35,620 --> 00:05:41,560
these any of these frameworks will have

00:05:39,639 --> 00:05:44,110
a model optimizer will convert it to IR

00:05:41,560 --> 00:05:45,969
so you have those XML and pin again a

00:05:44,110 --> 00:05:48,069
lot of layers that is there in training

00:05:45,969 --> 00:05:49,930
is not actually used it influencing so

00:05:48,069 --> 00:05:52,900
we automatically take care of it in a

00:05:49,930 --> 00:05:54,339
black box and you know those layers are

00:05:52,900 --> 00:05:57,219
remove and optimized so you have an

00:05:54,339 --> 00:05:58,960
optimized graphs now which is again to

00:05:57,219 --> 00:06:00,699
repeat both framework and platform

00:05:58,960 --> 00:06:02,919
independent and then using the API you

00:06:00,699 --> 00:06:04,479
can just put it on any of the platforms

00:06:02,919 --> 00:06:05,760
thank you so much this is mushroom from

00:06:04,479 --> 00:06:08,620
rental thank you

00:06:05,760 --> 00:06:08,620

YouTube URL: https://www.youtube.com/watch?v=kUafMs6CRPk


