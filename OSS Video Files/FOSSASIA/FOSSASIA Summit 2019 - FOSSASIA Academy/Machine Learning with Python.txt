Title: Machine Learning with Python
Publication date: 2019-04-01
Playlist: FOSSASIA Summit 2019 - FOSSASIA Academy
Description: 
	15 March 2019 19:00, Event Hall 2-1

Participants will learn about basic machine learning algorithms such as linear regression and build a single layer perceptron to improve the performance of the linear regression model.
Captions: 
	00:00:00,060 --> 00:00:05,400
my name is Russia you can call me rich

00:00:02,010 --> 00:00:07,710
I'm from national junior college and I

00:00:05,400 --> 00:00:09,480
represent building blocks which is an

00:00:07,710 --> 00:00:11,070
organization in Singapore led by

00:00:09,480 --> 00:00:14,670
students created by students for

00:00:11,070 --> 00:00:18,060
students to enrich to promote enrichment

00:00:14,670 --> 00:00:20,160
of computer science education and we

00:00:18,060 --> 00:00:24,029
want to bring more people on board into

00:00:20,160 --> 00:00:26,849
the computer science journey so before

00:00:24,029 --> 00:00:30,000
we begin let me introduce myself

00:00:26,849 --> 00:00:31,800
so I am a final year student at National

00:00:30,000 --> 00:00:34,620
Junior College yes unfortunately I'm

00:00:31,800 --> 00:00:36,840
taking my a-levels this year I am

00:00:34,620 --> 00:00:38,480
certified in machine learning and deep

00:00:36,840 --> 00:00:42,180
neural networks from Stanford University

00:00:38,480 --> 00:00:44,309
I worked in an around Singapore at a

00:00:42,180 --> 00:00:47,030
couple of places I was an X machine

00:00:44,309 --> 00:00:50,550
learning research at E star and I

00:00:47,030 --> 00:00:52,860
co-founded an AI start-up last year with

00:00:50,550 --> 00:00:55,140
a friend of mine and now I'm heading the

00:00:52,860 --> 00:00:58,440
AI lab at this startup called unscramble

00:00:55,140 --> 00:01:00,570
AI I write articles on machine learning

00:00:58,440 --> 00:01:04,110
and generally on tech and reviews on

00:01:00,570 --> 00:01:07,110
medium and I open source stuff on github

00:01:04,110 --> 00:01:09,180
I joined building blocks this year and

00:01:07,110 --> 00:01:11,130
so far the journey has been great so if

00:01:09,180 --> 00:01:15,390
you're a student please do take note

00:01:11,130 --> 00:01:17,340
that building blocks is hiring so you

00:01:15,390 --> 00:01:23,340
can join us if you're looking for some

00:01:17,340 --> 00:01:25,799
fun okay so for today's workshop I'm

00:01:23,340 --> 00:01:27,840
assuming that you know a bit of Python

00:01:25,799 --> 00:01:31,920
how to work with files how to manipulate

00:01:27,840 --> 00:01:33,720
data in a basic way and a bit of high

00:01:31,920 --> 00:01:36,990
school math you know working with

00:01:33,720 --> 00:01:39,570
algebra vectors graphs and a bit of

00:01:36,990 --> 00:01:41,729
calculus that is differentiation but

00:01:39,570 --> 00:01:45,390
don't worry the math isn't that hectic

00:01:41,729 --> 00:01:49,829
or crazy and if you have any confusions

00:01:45,390 --> 00:01:52,049
or doubts just shoot a question so

00:01:49,829 --> 00:01:54,869
machine learning is a subfield of

00:01:52,049 --> 00:01:57,270
artificial intelligence where we enable

00:01:54,869 --> 00:02:00,270
computers to learn without explicitly

00:01:57,270 --> 00:02:02,460
giving them rules so given some training

00:02:00,270 --> 00:02:05,850
data the machine learning algorithm is

00:02:02,460 --> 00:02:07,649
able to find patterns in the data and is

00:02:05,850 --> 00:02:10,670
able to give predictions when released

00:02:07,649 --> 00:02:13,280
into the world so for example Google

00:02:10,670 --> 00:02:15,260
Translate it works

00:02:13,280 --> 00:02:18,140
machine learning where it was trained on

00:02:15,260 --> 00:02:21,650
huge bodies of text in two different

00:02:18,140 --> 00:02:24,500
languages and it basically now has been

00:02:21,650 --> 00:02:26,990
released or online you know you quick

00:02:24,500 --> 00:02:28,310
google search and you'll open up Google

00:02:26,990 --> 00:02:30,350
Translate and you can type in anything

00:02:28,310 --> 00:02:32,420
and translate it for you that is one

00:02:30,350 --> 00:02:35,900
form of machine learning so machine

00:02:32,420 --> 00:02:40,489
learning before machine learning came

00:02:35,900 --> 00:02:42,800
into play back then in the growing years

00:02:40,489 --> 00:02:48,800
of tech ever since the internet boom in

00:02:42,800 --> 00:02:50,810
the early 2000s we didn't have data that

00:02:48,800 --> 00:02:53,720
much data and at the same time we didn't

00:02:50,810 --> 00:02:56,150
have enough computing power or lots of

00:02:53,720 --> 00:02:58,970
power to run really huge algorithms

00:02:56,150 --> 00:03:01,190
right so we resorted to traditional rule

00:02:58,970 --> 00:03:04,100
based algorithms so what do I mean by

00:03:01,190 --> 00:03:05,930
rule-based say you want to create an

00:03:04,100 --> 00:03:08,959
algorithm that's able to classify two

00:03:05,930 --> 00:03:10,370
different types of flowers right so you

00:03:08,959 --> 00:03:13,010
have there are two different types of

00:03:10,370 --> 00:03:14,230
iris flowers iris setosa and iris

00:03:13,010 --> 00:03:17,120
virginica

00:03:14,230 --> 00:03:20,450
so say you made this algorithm that I

00:03:17,120 --> 00:03:22,010
give you an image of of a flower and

00:03:20,450 --> 00:03:23,959
you're able to tell me whether it

00:03:22,010 --> 00:03:27,650
belongs to one of these classes or

00:03:23,959 --> 00:03:30,790
labels right so what a traditional rule

00:03:27,650 --> 00:03:34,130
based algorithm programmer would do is

00:03:30,790 --> 00:03:36,980
he'd write rule after rule after rule

00:03:34,130 --> 00:03:38,570
and at one point of time he gets sick of

00:03:36,980 --> 00:03:41,750
it because how many rules can you write

00:03:38,570 --> 00:03:45,320
but then it's worse if I give you an

00:03:41,750 --> 00:03:47,570
anomaly if I give you a picture that was

00:03:45,320 --> 00:03:50,150
supposed to be one of the flowers but

00:03:47,570 --> 00:03:52,370
your rule-based algorithm give a wrong

00:03:50,150 --> 00:03:55,250
prediction saying it's something else we

00:03:52,370 --> 00:03:57,290
have a huge problem on our hands if say

00:03:55,250 --> 00:04:02,480
in the case it's been released online or

00:03:57,290 --> 00:04:05,329
released to the public for use so say I

00:04:02,480 --> 00:04:07,790
give it a picture of iris virginica it'd

00:04:05,329 --> 00:04:10,010
be a huge problem if your rule-based

00:04:07,790 --> 00:04:12,019
algorithm predicted it as iris setosa

00:04:10,010 --> 00:04:15,440
just because it looks the same or almost

00:04:12,019 --> 00:04:16,010
looks the same as iris virginica iris

00:04:15,440 --> 00:04:21,229
setosa

00:04:16,010 --> 00:04:24,229
given the image that is provided this is

00:04:21,229 --> 00:04:25,940
exactly why we have relied on the

00:04:24,229 --> 00:04:27,110
support of machine learning ever since

00:04:25,940 --> 00:04:30,080
we've been able to

00:04:27,110 --> 00:04:32,810
lots of training data and lots of

00:04:30,080 --> 00:04:36,550
compute power ever since 2013 with

00:04:32,810 --> 00:04:39,319
google deepmind all these huge companies

00:04:36,550 --> 00:04:42,050
embarking on all these AI experiments

00:04:39,319 --> 00:04:46,129
and adventures so that life becomes so

00:04:42,050 --> 00:04:47,780
much more convenient for all of us so in

00:04:46,129 --> 00:04:50,479
current times when it comes to training

00:04:47,780 --> 00:04:51,889
data or data as a whole you can see that

00:04:50,479 --> 00:04:53,539
it's really complex you know it's not

00:04:51,889 --> 00:04:56,990
really perfect sometimes it needs a bit

00:04:53,539 --> 00:05:00,110
of cleaning dusting it's not readily

00:04:56,990 --> 00:05:02,599
available for use in machine learning so

00:05:00,110 --> 00:05:05,629
we need to clean it and at times data

00:05:02,599 --> 00:05:08,030
can be of low quality sometimes they can

00:05:05,629 --> 00:05:09,650
be from surveys or they can be from

00:05:08,030 --> 00:05:11,030
these questionnaires but at times

00:05:09,650 --> 00:05:13,639
they're incomplete and you need to

00:05:11,030 --> 00:05:15,949
bridge the gaps fill in the gaps to and

00:05:13,639 --> 00:05:19,490
show that your training data is good for

00:05:15,949 --> 00:05:20,629
use so as the rule based example that I

00:05:19,490 --> 00:05:22,310
mentioned earlier

00:05:20,629 --> 00:05:24,289
you can't keep writing rules that

00:05:22,310 --> 00:05:26,210
doesn't comes a point in time where you

00:05:24,289 --> 00:05:28,159
can't think of new rules and your

00:05:26,210 --> 00:05:29,629
algorithm keeps failing because well

00:05:28,159 --> 00:05:33,349
it's predicting the wrong thing because

00:05:29,629 --> 00:05:38,479
two things look the same which is why

00:05:33,349 --> 00:05:40,009
again we need machine learning so ever

00:05:38,479 --> 00:05:42,520
since machine learning came into play

00:05:40,009 --> 00:05:45,770
we've had a huge boost in performance or

00:05:42,520 --> 00:05:48,589
in algorithms and whenever you need

00:05:45,770 --> 00:05:50,389
accurate predictions you'd go to machine

00:05:48,589 --> 00:05:52,789
learning some form of statistical

00:05:50,389 --> 00:05:56,210
machine learning or neural networks if

00:05:52,789 --> 00:05:58,039
you heard of them right and the beauty

00:05:56,210 --> 00:05:59,690
of machine learning or neural networks

00:05:58,039 --> 00:06:02,060
is that it's able to find even more

00:05:59,690 --> 00:06:04,310
complex patterns in data so that

00:06:02,060 --> 00:06:06,860
compared to the rule-based counterpart

00:06:04,310 --> 00:06:10,940
it's so much more versatile in being

00:06:06,860 --> 00:06:13,370
used in certain certain news cases so

00:06:10,940 --> 00:06:15,560
machine learning has penetrated almost

00:06:13,370 --> 00:06:17,960
every industry known to man it's being

00:06:15,560 --> 00:06:20,029
used in banking finance social media

00:06:17,960 --> 00:06:22,789
recommendation algorithms on Instagram

00:06:20,029 --> 00:06:24,800
and YouTube and even your assistants

00:06:22,789 --> 00:06:26,750
like Google assistant and Syria it all

00:06:24,800 --> 00:06:28,909
uses some form of machine learning which

00:06:26,750 --> 00:06:33,949
powers it to make your life much more

00:06:28,909 --> 00:06:36,709
convenient and funny thing it's also

00:06:33,949 --> 00:06:38,419
being used in cucumber farming so that

00:06:36,709 --> 00:06:39,950
was this one person who used a machine

00:06:38,419 --> 00:06:42,890
learning algorithm

00:06:39,950 --> 00:06:45,470
his cucumber farm in South Korea and he

00:06:42,890 --> 00:06:47,390
helped his dad by actually segregating

00:06:45,470 --> 00:06:49,100
the two types of cucumbers or the

00:06:47,390 --> 00:06:50,540
different types of cucumbers in two

00:06:49,100 --> 00:06:53,450
different baskets using machine learning

00:06:50,540 --> 00:06:56,060
he scanned the cucumber and so what kind

00:06:53,450 --> 00:06:59,060
or type it is and well he put it into

00:06:56,060 --> 00:07:01,640
different buckets so I think you can you

00:06:59,060 --> 00:07:03,950
have some picture or some clarity as to

00:07:01,640 --> 00:07:07,580
why machine learning is so useful not

00:07:03,950 --> 00:07:10,130
only in tech but also in other fields of

00:07:07,580 --> 00:07:11,990
life such as well agriculture we see a

00:07:10,130 --> 00:07:14,890
huge boom in Agrotech and all these

00:07:11,990 --> 00:07:17,120
startups coming up and using AI and

00:07:14,890 --> 00:07:20,900
transforming the lives of many people

00:07:17,120 --> 00:07:24,170
who who perform traditional manual labor

00:07:20,900 --> 00:07:28,640
and by leveraging the immense power of

00:07:24,170 --> 00:07:31,070
machine learning so at the end of this

00:07:28,640 --> 00:07:32,810
workshop I think one takeaway would be

00:07:31,070 --> 00:07:35,060
asking yourself how well you bow you

00:07:32,810 --> 00:07:37,430
would be if you knew machine learning

00:07:35,060 --> 00:07:39,530
right you'd have so much power in your

00:07:37,430 --> 00:07:42,380
hands I like to call machine learning

00:07:39,530 --> 00:07:44,360
engineers wizards because well they have

00:07:42,380 --> 00:07:48,530
the key to a better future a more

00:07:44,360 --> 00:07:51,560
convenient future for all of us so for

00:07:48,530 --> 00:07:53,090
this workshop it's roughly two hours but

00:07:51,560 --> 00:07:53,360
I'll keep it short I know it's late into

00:07:53,090 --> 00:07:56,630
the night

00:07:53,360 --> 00:07:56,960
so thank you for coming here at this

00:07:56,630 --> 00:07:59,780
hour

00:07:56,960 --> 00:08:01,910
so we'll be I'll be running through this

00:07:59,780 --> 00:08:04,520
basic form of machine learning algorithm

00:08:01,910 --> 00:08:06,410
called linear regression and then we'll

00:08:04,520 --> 00:08:08,890
be more will be doing some theory I'll

00:08:06,410 --> 00:08:11,810
be showing you a bit of math warning

00:08:08,890 --> 00:08:13,430
that yes math is involved but don't

00:08:11,810 --> 00:08:17,540
worry if you have a question just shoot

00:08:13,430 --> 00:08:20,120
and then if everyone is ok and we can

00:08:17,540 --> 00:08:22,430
touch base I'll be moving on to this

00:08:20,120 --> 00:08:27,110
basic neural network architecture called

00:08:22,430 --> 00:08:29,150
the single layer perceptron and what we

00:08:27,110 --> 00:08:31,010
will be doing is applying all the

00:08:29,150 --> 00:08:33,200
knowledge that will be gaining in this

00:08:31,010 --> 00:08:36,410
workshop I'll be doing like a live

00:08:33,200 --> 00:08:38,560
coding session with all of you and we'll

00:08:36,410 --> 00:08:41,120
be testing it on real-life data sets so

00:08:38,560 --> 00:08:43,250
yeah that's a that's the spark a bit of

00:08:41,120 --> 00:08:47,660
interest you know where it's applied and

00:08:43,250 --> 00:08:51,020
whatnot ok linear regression linear

00:08:47,660 --> 00:08:53,670
regression is a linear model so we know

00:08:51,020 --> 00:08:57,029
that the relationship between very

00:08:53,670 --> 00:08:59,940
x and y is defined by a best-fit line

00:08:57,029 --> 00:09:03,180
right and the best-fit line it has a

00:08:59,940 --> 00:09:05,100
relationship of FX equals MX plus C or y

00:09:03,180 --> 00:09:08,670
equals MX plus C as some of us have

00:09:05,100 --> 00:09:10,680
learned in high school right so M is the

00:09:08,670 --> 00:09:12,750
gradient and C's the intercept and

00:09:10,680 --> 00:09:15,269
together with the x and y variables

00:09:12,750 --> 00:09:21,630
they're able to create the best fit line

00:09:15,269 --> 00:09:24,089
on the graph so to imagine so before we

00:09:21,630 --> 00:09:25,740
delve into the math and all the concepts

00:09:24,089 --> 00:09:28,440
I'd like you to visualize what linear

00:09:25,740 --> 00:09:30,829
regression really does so say you're

00:09:28,440 --> 00:09:34,260
playing football I give you a blindfold

00:09:30,829 --> 00:09:36,390
you're blindfolded and then I I spin you

00:09:34,260 --> 00:09:38,279
and you have no sense of direction or

00:09:36,390 --> 00:09:41,100
orientation you have no idea where the

00:09:38,279 --> 00:09:43,140
goal is and say I tell you ok

00:09:41,100 --> 00:09:45,000
immediately take a kick at the goal I

00:09:43,140 --> 00:09:48,000
don't care if you know where the goal is

00:09:45,000 --> 00:09:48,990
or not just take a kick and you take the

00:09:48,000 --> 00:09:51,720
kick and Wow

00:09:48,990 --> 00:09:53,490
I mean obviously the ball flies

00:09:51,720 --> 00:09:55,440
somewhere else because you have no idea

00:09:53,490 --> 00:09:58,110
where the goal is and you have no idea

00:09:55,440 --> 00:10:00,870
where you are in relation to that goal

00:09:58,110 --> 00:10:04,589
post and then I ask you to take off the

00:10:00,870 --> 00:10:09,149
blindfold and see how off your kick is

00:10:04,589 --> 00:10:12,660
or how how badly you kicked in a random

00:10:09,149 --> 00:10:14,640
direction and I tell you to take a kick

00:10:12,660 --> 00:10:17,850
again where the blindfold and take a

00:10:14,640 --> 00:10:20,100
kick again and you can see that now you

00:10:17,850 --> 00:10:21,959
kind of have some idea of where the goal

00:10:20,100 --> 00:10:24,660
is because when you open the blindfold

00:10:21,959 --> 00:10:28,920
you kind of saw where you were you saw

00:10:24,660 --> 00:10:30,630
where the goalpost was and now you kind

00:10:28,920 --> 00:10:33,660
of have a rough idea of where exactly

00:10:30,630 --> 00:10:36,000
the kick or which direction you're

00:10:33,660 --> 00:10:38,279
supposed to kick in and you repeat this

00:10:36,000 --> 00:10:42,720
process multiple times and you can see

00:10:38,279 --> 00:10:45,899
that the ball or the goal you miss less

00:10:42,720 --> 00:10:47,850
frequently and the distance between your

00:10:45,899 --> 00:10:51,510
kick and the actual goalpost keeps

00:10:47,850 --> 00:10:53,070
decreasing as you keep improving or you

00:10:51,510 --> 00:10:55,410
get a better understanding of where you

00:10:53,070 --> 00:10:59,720
are in relation to the goalpost so

00:10:55,410 --> 00:11:03,449
similarly linear regression is a way of

00:10:59,720 --> 00:11:05,660
taking random variables the gradient and

00:11:03,449 --> 00:11:07,290
the y-intercept or the vertical

00:11:05,660 --> 00:11:10,380
intercept

00:11:07,290 --> 00:11:13,070
draws the line which is analogous to a

00:11:10,380 --> 00:11:17,449
football player taking that random kick

00:11:13,070 --> 00:11:19,680
it sees how off that line is and

00:11:17,449 --> 00:11:21,360
analogous to how when you take off the

00:11:19,680 --> 00:11:23,459
blindfold you see where you are where

00:11:21,360 --> 00:11:27,540
the ball went and where the goalposts

00:11:23,459 --> 00:11:28,230
actually is so and you correct the

00:11:27,540 --> 00:11:30,690
variables

00:11:28,230 --> 00:11:33,449
mn see the gradient and the y-intercept

00:11:30,690 --> 00:11:36,089
slowly just how the football player

00:11:33,449 --> 00:11:39,300
corrects his stance his positioning and

00:11:36,089 --> 00:11:42,620
his kicking speed and angle and again

00:11:39,300 --> 00:11:45,209
this process is continuous so long as

00:11:42,620 --> 00:11:47,550
until the person is able to get the ball

00:11:45,209 --> 00:11:49,230
into the goal or in this case we

00:11:47,550 --> 00:11:52,139
achieved the best fit line or something

00:11:49,230 --> 00:11:54,480
that's close to it so this is linear

00:11:52,139 --> 00:11:57,420
regression in action so the green line

00:11:54,480 --> 00:11:59,759
is the best fit line or the thing we're

00:11:57,420 --> 00:12:01,920
trying to emulate and the red line we're

00:11:59,759 --> 00:12:04,380
constantly updating so you can see the

00:12:01,920 --> 00:12:06,209
red line is actually moving up right so

00:12:04,380 --> 00:12:08,100
and we can see that the gradient or the

00:12:06,209 --> 00:12:10,620
slope of the line and the y-intercept

00:12:08,100 --> 00:12:13,560
it's changing with every time it moves

00:12:10,620 --> 00:12:15,180
up or down which was that the linear

00:12:13,560 --> 00:12:17,850
regression algorithm is trying to

00:12:15,180 --> 00:12:20,459
emulate or gonna come as close as

00:12:17,850 --> 00:12:25,860
possible to the best fit line and today

00:12:20,459 --> 00:12:27,720
we're gonna do exactly this so earlier I

00:12:25,860 --> 00:12:31,620
mentioned that the relationship between

00:12:27,720 --> 00:12:34,680
x and y or X and FX which is basically Y

00:12:31,620 --> 00:12:37,769
is y equals MX plus C where m is the

00:12:34,680 --> 00:12:40,170
gradient and C is the y-intercept we

00:12:37,769 --> 00:12:42,630
call this the hypothesis function in

00:12:40,170 --> 00:12:44,519
machine learning so the hypothesis

00:12:42,630 --> 00:12:47,639
function is basically a guideline or

00:12:44,519 --> 00:12:50,939
this this rule that we need to follow to

00:12:47,639 --> 00:12:52,829
get the best fit line so if you were to

00:12:50,939 --> 00:12:54,899
convert this the simple Python you can

00:12:52,829 --> 00:12:57,540
you can say if you can define a function

00:12:54,899 --> 00:13:01,079
hypothesis function you can pass in the

00:12:57,540 --> 00:13:03,870
variables the y-intercept see the

00:13:01,079 --> 00:13:07,920
gradient m and the x value and return MX

00:13:03,870 --> 00:13:11,370
plus C so while we plotted the line

00:13:07,920 --> 00:13:12,930
right so we had the values of m XC and y

00:13:11,370 --> 00:13:16,170
and were able to get a decent

00:13:12,930 --> 00:13:18,269
relationship y equals MX plus C but as I

00:13:16,170 --> 00:13:20,850
said the football player took the wrong

00:13:18,269 --> 00:13:22,889
kick at the start he has no idea where

00:13:20,850 --> 00:13:25,740
the goalpost is where the ball is and

00:13:22,889 --> 00:13:28,050
where he is so similarly the values that

00:13:25,740 --> 00:13:30,449
are initialized for the gradient and the

00:13:28,050 --> 00:13:33,420
y-intercept a completely random at start

00:13:30,449 --> 00:13:35,819
and slowly through the process of linear

00:13:33,420 --> 00:13:39,029
regression we optimize these values of

00:13:35,819 --> 00:13:41,089
the gradient and the y-intercept so when

00:13:39,029 --> 00:13:43,709
we've predicted or this line

00:13:41,089 --> 00:13:47,220
relationship of y equals MX plus C using

00:13:43,709 --> 00:13:49,920
these variables m and C we want to know

00:13:47,220 --> 00:13:52,110
how off the kick is right we want to

00:13:49,920 --> 00:13:54,420
know how off the line is to the best fit

00:13:52,110 --> 00:13:58,740
line or the line that it's supposed to

00:13:54,420 --> 00:14:00,810
be so to find the error we use this

00:13:58,740 --> 00:14:02,279
thing called the loss function or the

00:14:00,810 --> 00:14:05,190
error function or the cost function

00:14:02,279 --> 00:14:06,930
there are many names to it but in

00:14:05,190 --> 00:14:09,389
machine learning we usually go for the

00:14:06,930 --> 00:14:12,360
loss function the loss function is a

00:14:09,389 --> 00:14:17,550
mathematical formula which takes in your

00:14:12,360 --> 00:14:19,560
variables and it compares it with the

00:14:17,550 --> 00:14:23,399
real values that you're supposed to be

00:14:19,560 --> 00:14:25,380
in the future and gives you the

00:14:23,399 --> 00:14:29,370
difference between how what you are

00:14:25,380 --> 00:14:33,870
currently at and and the real values so

00:14:29,370 --> 00:14:37,110
yes this is the loss function it may

00:14:33,870 --> 00:14:40,410
look scary but all it does is it takes

00:14:37,110 --> 00:14:43,500
the difference of f x and y which is

00:14:40,410 --> 00:14:46,170
basically FX is our line and y is the

00:14:43,500 --> 00:14:49,829
real value we take the difference and we

00:14:46,170 --> 00:14:52,920
square them and this this thing over

00:14:49,829 --> 00:14:54,930
here this it's we call this Sigma

00:14:52,920 --> 00:14:58,110
notation some of you may have learnt it

00:14:54,930 --> 00:15:00,630
in high school so what the Sigma

00:14:58,110 --> 00:15:03,689
notation does is it basically takes the

00:15:00,630 --> 00:15:07,170
sum of all these differences for all the

00:15:03,689 --> 00:15:08,880
examples in our data set or the training

00:15:07,170 --> 00:15:11,850
data set their work feeding the model

00:15:08,880 --> 00:15:13,680
and it just takes the average so as to

00:15:11,850 --> 00:15:17,430
make the difference or the error more

00:15:13,680 --> 00:15:20,100
consistent across all the examples again

00:15:17,430 --> 00:15:22,860
if we were to compute this or change

00:15:20,100 --> 00:15:25,410
this in simple Python we'd define a loss

00:15:22,860 --> 00:15:28,529
function where we take in the gradient

00:15:25,410 --> 00:15:31,170
the y-intercept x and y we initialize

00:15:28,529 --> 00:15:34,690
this variable called total loss which is

00:15:31,170 --> 00:15:37,300
basically across all the examples the

00:15:34,690 --> 00:15:40,210
is the loss that represents this round

00:15:37,300 --> 00:15:43,480
so when I say around I remember how the

00:15:40,210 --> 00:15:45,880
football player he takes the kick and he

00:15:43,480 --> 00:15:49,840
checks well like where the ball is and

00:15:45,880 --> 00:15:52,150
where he is in relation to the goal here

00:15:49,840 --> 00:15:54,400
I'll be calling it around but in machine

00:15:52,150 --> 00:15:57,100
learning we call it an epoch or an

00:15:54,400 --> 00:15:59,080
iteration right an iteration is

00:15:57,100 --> 00:16:01,720
basically one step that's closer to a

00:15:59,080 --> 00:16:05,110
final objective of optimizing that line

00:16:01,720 --> 00:16:07,540
to the best fit line again so we convert

00:16:05,110 --> 00:16:09,910
we get the hypothesis function which is

00:16:07,540 --> 00:16:13,000
basically y equals MX plus C or FX

00:16:09,910 --> 00:16:15,790
equals MX plus C we find the difference

00:16:13,000 --> 00:16:20,560
between our line and the actual real

00:16:15,790 --> 00:16:23,260
value we square them and we take the sum

00:16:20,560 --> 00:16:25,810
across all training examples and we take

00:16:23,260 --> 00:16:28,660
the average and we append it or we

00:16:25,810 --> 00:16:30,580
enumerate this total loss variable which

00:16:28,660 --> 00:16:34,360
is over here and we return the total

00:16:30,580 --> 00:16:37,000
loss so what exactly does the loss

00:16:34,360 --> 00:16:39,340
function do it takes our line as

00:16:37,000 --> 00:16:40,900
mentioned earlier compares it with the

00:16:39,340 --> 00:16:43,990
real value of what the line should be

00:16:40,900 --> 00:16:46,540
and it gives you that margin that error

00:16:43,990 --> 00:16:48,790
margin of how off you are in your

00:16:46,540 --> 00:16:53,620
prediction of the line or the values for

00:16:48,790 --> 00:16:56,770
the gradient and y-intercept so that

00:16:53,620 --> 00:16:57,730
step that was in the brackets FX minus y

00:16:56,770 --> 00:17:00,820
whole square

00:16:57,730 --> 00:17:04,030
it's basically represents the line the

00:17:00,820 --> 00:17:06,579
blue line is the is the is the line that

00:17:04,030 --> 00:17:09,310
we currently plotted using our random

00:17:06,579 --> 00:17:11,860
values of M and C and the pink colored

00:17:09,310 --> 00:17:14,770
dots are the actual real-life values and

00:17:11,860 --> 00:17:18,189
the red lines you see is basically the

00:17:14,770 --> 00:17:21,189
difference of FX minus y so if we take

00:17:18,189 --> 00:17:23,620
this Y position and the Y position that

00:17:21,189 --> 00:17:25,510
we predicted this red line shows the

00:17:23,620 --> 00:17:28,930
difference which is basically denoted by

00:17:25,510 --> 00:17:34,360
that inner statement in that formula

00:17:28,930 --> 00:17:38,020
that I showed you but then if we take a

00:17:34,360 --> 00:17:39,970
random kick and we correct ourselves we

00:17:38,020 --> 00:17:42,250
need to know by how much we should

00:17:39,970 --> 00:17:45,040
correct ourselves the loss function only

00:17:42,250 --> 00:17:47,470
shows our error but the actual process

00:17:45,040 --> 00:17:48,580
of taking that error and optimizing the

00:17:47,470 --> 00:17:52,690
gradient and wine

00:17:48,580 --> 00:17:54,460
depth using that error margin is what we

00:17:52,690 --> 00:17:57,539
should perform next so that we are one

00:17:54,460 --> 00:18:00,039
step closer to a final objective of

00:17:57,539 --> 00:18:03,820
getting close to the best fit line or

00:18:00,039 --> 00:18:05,500
something close to that so in high

00:18:03,820 --> 00:18:08,409
school some of you may boast of you have

00:18:05,500 --> 00:18:13,419
may have learnt about differentiation or

00:18:08,409 --> 00:18:17,080
derivatives and whatnot and so here when

00:18:13,419 --> 00:18:19,630
we perform differentiation on that

00:18:17,080 --> 00:18:21,940
formula I just showed you it actually

00:18:19,630 --> 00:18:24,909
gives us that change in the error

00:18:21,940 --> 00:18:27,010
between what we were at in the previous

00:18:24,909 --> 00:18:28,539
round and what we're supposed to be in

00:18:27,010 --> 00:18:31,450
the next round

00:18:28,539 --> 00:18:34,480
it may sound abstract to you machine

00:18:31,450 --> 00:18:36,370
learning is is a bunch of abstract

00:18:34,480 --> 00:18:37,570
concepts but hopefully through this

00:18:36,370 --> 00:18:39,970
workshop you have a clear understanding

00:18:37,570 --> 00:18:44,260
of what really happens in these kind of

00:18:39,970 --> 00:18:47,440
algorithms so if we calculate this

00:18:44,260 --> 00:18:49,659
derivative off that formula that loss

00:18:47,440 --> 00:18:53,710
function or the loss formula that we

00:18:49,659 --> 00:18:56,440
showed earlier we come we equate this to

00:18:53,710 --> 00:18:59,799
0 that means the change is close to zero

00:18:56,440 --> 00:19:01,630
and when something is close is zero the

00:18:59,799 --> 00:19:03,600
variables that are dependent on it which

00:19:01,630 --> 00:19:06,610
is basically the gradient and

00:19:03,600 --> 00:19:11,919
y-intercept is either at its maximum or

00:19:06,610 --> 00:19:14,409
minimum right so here we need a value of

00:19:11,919 --> 00:19:16,960
that loss value or the loss function

00:19:14,409 --> 00:19:19,720
where it gives the least possible error

00:19:16,960 --> 00:19:23,260
or the minimum error which is if you

00:19:19,720 --> 00:19:26,860
were able to plot a curve of the of the

00:19:23,260 --> 00:19:29,289
rounds and loss and the loss values

00:19:26,860 --> 00:19:32,500
you'd get a u-shape a convex function

00:19:29,289 --> 00:19:34,899
where that bottom or that plateau at the

00:19:32,500 --> 00:19:37,990
bottom of the u-shaped curve is where

00:19:34,899 --> 00:19:40,240
you want to reach and we we gradually go

00:19:37,990 --> 00:19:45,000
down that slope until the point where

00:19:40,240 --> 00:19:47,740
that gradient value is zero or minimum

00:19:45,000 --> 00:19:49,980
and we call this a machine learning or

00:19:47,740 --> 00:19:52,330
linear regression or statistical

00:19:49,980 --> 00:19:55,419
algorithms we call this gradient descent

00:19:52,330 --> 00:19:57,639
so I mentioned earlier that it forms a

00:19:55,419 --> 00:20:00,580
u-shaped curve and we're trying to get

00:19:57,639 --> 00:20:02,470
to the bottom of that curve and we're

00:20:00,580 --> 00:20:05,590
basically jumping gradient values

00:20:02,470 --> 00:20:10,240
of the loss function where the loss is

00:20:05,590 --> 00:20:11,830
the minimum so there's one type of

00:20:10,240 --> 00:20:14,650
differentiation called

00:20:11,830 --> 00:20:17,710
partial differentiation it may be out of

00:20:14,650 --> 00:20:19,809
the high school syllabus but if you've

00:20:17,710 --> 00:20:21,820
taken a math or statistics course in

00:20:19,809 --> 00:20:25,510
university you may have come across

00:20:21,820 --> 00:20:27,280
partial differentiation but you don't

00:20:25,510 --> 00:20:32,110
need to know how exactly it works for

00:20:27,280 --> 00:20:34,419
this workshop but please note that it's

00:20:32,110 --> 00:20:38,409
what we use to get the derivatives of

00:20:34,419 --> 00:20:40,510
the loss function so we take the

00:20:38,409 --> 00:20:44,140
derivatives of the loss function with

00:20:40,510 --> 00:20:48,220
respect to the gradient value m and the

00:20:44,140 --> 00:20:50,590
current y-intercept C and these are the

00:20:48,220 --> 00:20:52,390
formulas that we get after performing

00:20:50,590 --> 00:20:54,840
partial differentiation we just

00:20:52,390 --> 00:20:59,830
basically get the average of the sum of

00:20:54,840 --> 00:21:03,640
of our line - the real line or the

00:20:59,830 --> 00:21:06,880
best-fit line and and that's the and

00:21:03,640 --> 00:21:10,150
that's the derivative of the y-intercept

00:21:06,880 --> 00:21:12,429
with respect to the loss function and we

00:21:10,150 --> 00:21:15,700
if we were to do the same for the

00:21:12,429 --> 00:21:19,059
gradient it would give us the same of FX

00:21:15,700 --> 00:21:21,039
minus y and if you actually do the math

00:21:19,059 --> 00:21:24,039
you'll calculate that we just need to

00:21:21,039 --> 00:21:25,960
multiply the x value at the end again if

00:21:24,039 --> 00:21:28,450
we were to convert this into simple

00:21:25,960 --> 00:21:32,260
Python we don't define a function called

00:21:28,450 --> 00:21:34,360
get derivatives where we do where we set

00:21:32,260 --> 00:21:37,150
the values of the derivatives in with

00:21:34,360 --> 00:21:40,360
respect the y-intercept C and gradient M

00:21:37,150 --> 00:21:44,169
and we get the hypothesis function and

00:21:40,360 --> 00:21:47,559
as shown by the formula here these two

00:21:44,169 --> 00:21:49,929
variables update this value of DC and DM

00:21:47,559 --> 00:21:52,750
which is basically the derivatives with

00:21:49,929 --> 00:21:57,700
respect to C and M and we average them

00:21:52,750 --> 00:22:01,120
and we return them so now that we have

00:21:57,700 --> 00:22:03,159
the change the change in value for the

00:22:01,120 --> 00:22:05,860
gradient and the y-intercept we can

00:22:03,159 --> 00:22:09,210
finally optimize or update the values

00:22:05,860 --> 00:22:13,510
for the gradient and the y-intercept so

00:22:09,210 --> 00:22:16,149
we we take the current values of the

00:22:13,510 --> 00:22:16,930
gradient and the y-intercept and we

00:22:16,149 --> 00:22:19,930
subtract

00:22:16,930 --> 00:22:21,730
that change in error from it and we

00:22:19,930 --> 00:22:23,530
multiply that change in error with

00:22:21,730 --> 00:22:25,300
something called the learning rate in

00:22:23,530 --> 00:22:28,930
machine learning is denoted by this

00:22:25,300 --> 00:22:31,000
variable called alpha or ETA but here

00:22:28,930 --> 00:22:33,990
for simplicity sake since most of us are

00:22:31,000 --> 00:22:37,540
familiar with alpha we'll use alpha so

00:22:33,990 --> 00:22:40,210
what alpha does is it decides the extent

00:22:37,540 --> 00:22:44,350
to which or extent by which the values

00:22:40,210 --> 00:22:47,350
are updated so we can use this formula

00:22:44,350 --> 00:22:50,080
where the new values of the gradient and

00:22:47,350 --> 00:22:54,790
y-intercept are the current values of M

00:22:50,080 --> 00:22:58,630
and C subtracted and we subtract the the

00:22:54,790 --> 00:23:00,430
change D JDM and the DJ DC which is

00:22:58,630 --> 00:23:03,550
basically the derivatives of m and C

00:23:00,430 --> 00:23:05,650
with respect to the loss and we multiply

00:23:03,550 --> 00:23:09,160
it to the alpha learning rate which

00:23:05,650 --> 00:23:10,930
basically shows us the proportion of the

00:23:09,160 --> 00:23:15,580
current values that's supposed to be

00:23:10,930 --> 00:23:17,380
overwritten in the next round and if we

00:23:15,580 --> 00:23:20,260
were to convert this again into simple

00:23:17,380 --> 00:23:22,270
Python we'd get the derivatives using

00:23:20,260 --> 00:23:24,880
that function we wrote earlier get

00:23:22,270 --> 00:23:28,810
derivatives where we get the values of

00:23:24,880 --> 00:23:30,820
DC and DM we update them we update the

00:23:28,810 --> 00:23:34,690
next round of values with the current

00:23:30,820 --> 00:23:36,760
values subtract we subtract the the

00:23:34,690 --> 00:23:40,120
change from the current values and we

00:23:36,760 --> 00:23:44,410
return the the better values of

00:23:40,120 --> 00:23:47,860
optimized values of C and M so if we

00:23:44,410 --> 00:23:51,100
would do a progress check I understand

00:23:47,860 --> 00:23:53,350
that most of you may be still confused

00:23:51,100 --> 00:23:55,840
with the math but as we write the code

00:23:53,350 --> 00:24:00,670
it may be clearer as to what exactly

00:23:55,840 --> 00:24:03,460
we're doing and we'll be writing these

00:24:00,670 --> 00:24:05,260
helper functions along along the way in

00:24:03,460 --> 00:24:08,470
the live coding session that we'll be

00:24:05,260 --> 00:24:13,930
doing now and yeah let's perform linear

00:24:08,470 --> 00:24:15,880
regression now so we using these

00:24:13,930 --> 00:24:18,670
functions that we've learned in this

00:24:15,880 --> 00:24:21,940
first part of the workshop we initially

00:24:18,670 --> 00:24:24,250
around create get random values for the

00:24:21,940 --> 00:24:26,590
gradient and y-intercept we plot the

00:24:24,250 --> 00:24:28,330
line find the error as in we find how

00:24:26,590 --> 00:24:30,759
off the line is compared to the real

00:24:28,330 --> 00:24:33,940
values we find the grady

00:24:30,759 --> 00:24:36,669
or the change in the lost function where

00:24:33,940 --> 00:24:40,199
the error is the minimum and we updated

00:24:36,669 --> 00:24:42,969
with we update the parameters the

00:24:40,199 --> 00:24:45,940
gradient and y-intercept M and C with

00:24:42,969 --> 00:24:48,399
that change and finally we can redraw

00:24:45,940 --> 00:24:49,869
the line and of course the line is going

00:24:48,399 --> 00:24:53,769
to be much closer to the best fit line

00:24:49,869 --> 00:24:56,799
and finally if we were to sum all of

00:24:53,769 --> 00:24:59,440
that up into the final linear regression

00:24:56,799 --> 00:25:00,999
function we take in the values of x and

00:24:59,440 --> 00:25:04,359
y they were trying to find a linear

00:25:00,999 --> 00:25:07,629
relationship to we randomly initialize

00:25:04,359 --> 00:25:10,389
the values of CNM as shown by the first

00:25:07,629 --> 00:25:12,789
two lines of the function here alpha we

00:25:10,389 --> 00:25:14,979
set it to a really low value not too low

00:25:12,789 --> 00:25:17,919
that it makes the algorithm too slow in

00:25:14,979 --> 00:25:21,009
learning the optimal values of M and C

00:25:17,919 --> 00:25:23,079
but not too large enough that it takes

00:25:21,009 --> 00:25:25,269
even longer time because it just keeps

00:25:23,079 --> 00:25:27,940
bouncing back and forth the optimal

00:25:25,269 --> 00:25:30,549
values and the number of rounds or

00:25:27,940 --> 00:25:33,039
iterations will set it to thousand so we

00:25:30,549 --> 00:25:35,769
hope that by thousand rounds were able

00:25:33,039 --> 00:25:37,839
to get values that are close to the

00:25:35,769 --> 00:25:41,349
perfect values or the optimal values of

00:25:37,839 --> 00:25:45,009
M and C so using the update parameters

00:25:41,349 --> 00:25:49,509
function that we wrote earlier we give

00:25:45,009 --> 00:25:52,659
in the learning rate X Y C and M and by

00:25:49,509 --> 00:25:54,759
the end of these thousand steps it'll

00:25:52,659 --> 00:25:57,009
keep repeating and keep optimizing these

00:25:54,759 --> 00:25:59,409
values of the gradient and y-intercept

00:25:57,009 --> 00:26:01,569
such that by the end of the algorithm we

00:25:59,409 --> 00:26:03,219
can plot a line that's basically the

00:26:01,569 --> 00:26:06,999
best fit line or something that's close

00:26:03,219 --> 00:26:11,739
to it okay now we'll be doing the live

00:26:06,999 --> 00:26:14,499
coding session okay so do all of you or

00:26:11,739 --> 00:26:17,440
most of you have internet connection if

00:26:14,499 --> 00:26:21,810
you don't please raise your hands yeah

00:26:17,440 --> 00:26:23,700
yeah okay

00:26:21,810 --> 00:26:25,980
we'll start in two minutes please do

00:26:23,700 --> 00:26:36,270
configure the Wi-Fi because we'll be

00:26:25,980 --> 00:26:38,880
using something online but so far does

00:26:36,270 --> 00:26:42,680
anyone have any questions or any doubts

00:26:38,880 --> 00:26:42,680
as to what's happening here

00:27:08,720 --> 00:27:16,190
does anyone still not have internet

00:27:10,910 --> 00:27:21,230
connection okay so I'm assuming that all

00:27:16,190 --> 00:27:25,130
of us are connected so yeah now it's ko

00:27:21,230 --> 00:27:27,110
its coding time so I'm assuming that all

00:27:25,130 --> 00:27:29,600
of you have a Google Drive account or a

00:27:27,110 --> 00:27:31,430
gmail account so please go to this

00:27:29,600 --> 00:27:33,350
website drive.google.com

00:27:31,430 --> 00:27:48,490
basically access your Google Drive

00:27:33,350 --> 00:27:48,490
please anyone not on the drive site yet

00:27:48,970 --> 00:27:59,420
okay everyone's in right okay so when

00:27:57,200 --> 00:28:02,240
you're in your Google Drive click the

00:27:59,420 --> 00:28:04,450
new button and you'll get this drop down

00:28:02,240 --> 00:28:04,450
menu

00:28:13,690 --> 00:28:21,760
okay yeah you will see this drop-down

00:28:18,130 --> 00:28:23,620
menu and at the bottom of the drop down

00:28:21,760 --> 00:28:26,530
menu you'll see this option called more

00:28:23,620 --> 00:28:29,200
and if you hover on more you'll get this

00:28:26,530 --> 00:28:31,350
other second drop-down menu where you'll

00:28:29,200 --> 00:28:38,260
see this thing that says collaboratory

00:28:31,350 --> 00:28:41,560
it has this Co icon or logo yeah in some

00:28:38,260 --> 00:28:44,380
cases you may not have the collaboratory

00:28:41,560 --> 00:28:47,410
app but you can download it I think in

00:28:44,380 --> 00:28:49,360
Google Drive when you go to the drop

00:28:47,410 --> 00:28:52,590
down menu you can actually download apps

00:28:49,360 --> 00:28:55,510
that you can put on to Google Drive so

00:28:52,590 --> 00:29:00,430
it'll open a pop-up window and you can

00:28:55,510 --> 00:29:04,450
search collaboratory or collab it's

00:29:00,430 --> 00:29:06,220
spelled CL co L a B I think if you type

00:29:04,450 --> 00:29:15,510
the first five letters you can probably

00:29:06,220 --> 00:29:15,510
get the anyone who didn't find it yet

00:29:30,789 --> 00:29:40,740
this side is is everyone clear is

00:29:33,039 --> 00:29:40,740
everyone okay oh yeah sure

00:29:41,520 --> 00:29:46,150
again if you haven't connected to the

00:29:43,780 --> 00:29:52,120
Wi-Fi sofa please do because we'll be

00:29:46,150 --> 00:29:53,950
coding on an online platform at the end

00:29:52,120 --> 00:29:56,409
of the workshop I'll be open sourcing

00:29:53,950 --> 00:29:59,350
all of this on github on Twitter

00:29:56,409 --> 00:30:01,659
LinkedIn and Facebook I'll give you the

00:29:59,350 --> 00:30:03,850
contact details and my account so that

00:30:01,659 --> 00:30:07,780
you can check out these slides as well

00:30:03,850 --> 00:30:21,190
as the collab notebook for future

00:30:07,780 --> 00:30:23,350
reference it'd be better if you can do

00:30:21,190 --> 00:30:31,860
this on a laptop because we will

00:30:23,350 --> 00:30:31,860
actually be writing some code okay then

00:30:44,540 --> 00:30:46,960
me a second

00:31:15,509 --> 00:31:24,840
okay does everyone see this on the

00:31:18,490 --> 00:31:36,220
screen right anyone still unclear okay

00:31:24,840 --> 00:31:57,399
just give me a second hold on yeah it's

00:31:36,220 --> 00:31:59,879
this one anyone not clear please do

00:31:57,399 --> 00:31:59,879
raise your hand

00:32:09,330 --> 00:32:39,670
so if you click anyone else okay then

00:32:32,470 --> 00:32:44,860
ah that's good okay let me just increase

00:32:39,670 --> 00:32:49,840
the fonts can everyone see this can the

00:32:44,860 --> 00:32:53,920
back row see this or how about this the

00:32:49,840 --> 00:32:57,000
back we can see this right okay okay

00:32:53,920 --> 00:32:57,000
yeah cool

00:33:12,060 --> 00:33:17,170
so what you've currently just opened is

00:33:15,700 --> 00:33:21,070
this thing called collaboratory

00:33:17,170 --> 00:33:24,250
notebooks it's this online coding

00:33:21,070 --> 00:33:26,800
environment that is specifically meant

00:33:24,250 --> 00:33:29,920
for machine learning anyone can access

00:33:26,800 --> 00:33:32,140
it and it's free and best of all if

00:33:29,920 --> 00:33:35,410
you're doing some really cool projects

00:33:32,140 --> 00:33:37,360
that may be intensive in terms of

00:33:35,410 --> 00:33:40,420
computational power they can actually

00:33:37,360 --> 00:33:44,290
give you a provision of free cloud GPU

00:33:40,420 --> 00:33:47,980
for 12 hours so yeah that's pretty cool

00:33:44,290 --> 00:33:51,760
so what we'll be doing is a notebook is

00:33:47,980 --> 00:33:53,710
basically a cell based editor where you

00:33:51,760 --> 00:33:55,900
can actually type code in cells and you

00:33:53,710 --> 00:33:59,590
can run individual cells it's great for

00:33:55,900 --> 00:34:07,720
visualizing things and for interactive

00:33:59,590 --> 00:34:10,540
presentations okay so so first off we'll

00:34:07,720 --> 00:34:17,620
start off by importing all the libraries

00:34:10,540 --> 00:34:23,290
that we need so okay please make sure

00:34:17,620 --> 00:34:24,460
that you are connected to the server so

00:34:23,290 --> 00:34:26,470
you just need to click the connect

00:34:24,460 --> 00:34:28,570
button if you're not connected and if

00:34:26,470 --> 00:34:31,980
you are connected it'll show connected

00:34:28,570 --> 00:34:35,200
and it'll show you some memory usage

00:34:31,980 --> 00:34:37,169
everyone good so far okay everyone's

00:34:35,200 --> 00:34:42,340
connected okay

00:34:37,169 --> 00:34:46,060
so for this specific example we'll be

00:34:42,340 --> 00:34:47,890
using a few Python libraries or packages

00:34:46,060 --> 00:34:50,590
that come pre-installed when you're

00:34:47,890 --> 00:34:52,750
using collab right and that's the cool

00:34:50,590 --> 00:34:54,580
part because machine learning you need

00:34:52,750 --> 00:34:57,610
to have all these weird sorts of

00:34:54,580 --> 00:35:00,010
libraries and modules but what collab

00:34:57,610 --> 00:35:01,840
does is it has it all pre-installed so

00:35:00,010 --> 00:35:05,620
you just need to call it and you can use

00:35:01,840 --> 00:35:08,280
it so we'll be using this library called

00:35:05,620 --> 00:35:11,890
numpy has anyone heard of numpy before

00:35:08,280 --> 00:35:14,140
ok great so numpy for those of you who

00:35:11,890 --> 00:35:16,170
don't know it's this library or module

00:35:14,140 --> 00:35:18,390
that enables us to do

00:35:16,170 --> 00:35:21,870
numerical computing so anything to do

00:35:18,390 --> 00:35:24,560
with vectors differentiation anything to

00:35:21,870 --> 00:35:29,400
do with matrices or matrix operations

00:35:24,560 --> 00:35:31,500
numpy is your go to library next we'll

00:35:29,400 --> 00:35:40,350
be importing this thing called

00:35:31,500 --> 00:35:44,760
matplotlib anyone who's heard of

00:35:40,350 --> 00:35:49,500
matplotlib okay so what matplotlib does

00:35:44,760 --> 00:35:52,080
or we use this pipeline sub module from

00:35:49,500 --> 00:35:56,280
the matplotlib library and this enables

00:35:52,080 --> 00:35:58,830
us to draw cross so this enables enables

00:35:56,280 --> 00:36:01,800
us to give really cool presentations and

00:35:58,830 --> 00:36:04,710
visualizations by drawing colorful

00:36:01,800 --> 00:36:06,990
charts either in 2d or 3d and you can

00:36:04,710 --> 00:36:17,160
even show images on this which is pretty

00:36:06,990 --> 00:36:19,320
cool okay so to run to run a cell that's

00:36:17,160 --> 00:36:22,170
display button over here and when you

00:36:19,320 --> 00:36:23,610
click play it'll actually compile the

00:36:22,170 --> 00:36:26,130
code that you've written in that

00:36:23,610 --> 00:36:29,010
specific cell for which you wrote where

00:36:26,130 --> 00:36:32,880
you click play for so now we can see

00:36:29,010 --> 00:36:34,650
that the code has run and does anyone

00:36:32,880 --> 00:36:38,090
have any errors so far everyone's cool

00:36:34,650 --> 00:36:41,280
right okay sorry

00:36:38,090 --> 00:36:43,650
okay okay great okay so that's this

00:36:41,280 --> 00:36:46,650
button called code and when you click it

00:36:43,650 --> 00:36:48,360
it'll give you a new cell so that's

00:36:46,650 --> 00:36:50,850
another interesting fact about collab

00:36:48,360 --> 00:36:54,750
you can just keep adding cells and run

00:36:50,850 --> 00:36:56,760
them individually and it actually uses

00:36:54,750 --> 00:36:59,430
the variables that you declared in

00:36:56,760 --> 00:37:01,260
previous cells above so you don't need

00:36:59,430 --> 00:37:03,810
to run the entire thing again and again

00:37:01,260 --> 00:37:05,430
you can just run cell after cell because

00:37:03,810 --> 00:37:10,890
it's all dependent on what you've

00:37:05,430 --> 00:37:14,910
already ran above so when when we start

00:37:10,890 --> 00:37:18,300
off with a linear regression we what we

00:37:14,910 --> 00:37:20,640
need data right so for now we'll be

00:37:18,300 --> 00:37:24,030
creating some dummy data before we apply

00:37:20,640 --> 00:37:25,650
our algorithm to something do it like a

00:37:24,030 --> 00:37:30,620
real-life data set like the iris

00:37:25,650 --> 00:37:30,620
classification data set so

00:37:31,850 --> 00:37:41,760
yeah yeah so so we need the X&Y and to

00:37:37,260 --> 00:37:47,490
create them numpy comes with this sub

00:37:41,760 --> 00:37:53,550
module that allows you to create well

00:37:47,490 --> 00:37:56,100
random digits hold on so it can either

00:37:53,550 --> 00:37:59,370
create random digits or it can actually

00:37:56,100 --> 00:38:01,950
create variables for you or randomly

00:37:59,370 --> 00:38:04,500
initialized data values for you that you

00:38:01,950 --> 00:38:20,390
can actually plot on a graph so if you

00:38:04,500 --> 00:38:21,510
type this thing called x equals okay

00:38:20,390 --> 00:38:26,130
okay

00:38:21,510 --> 00:38:28,110
yeah so what this linspace sub module

00:38:26,130 --> 00:38:30,390
does is it basically creates a linear

00:38:28,110 --> 00:38:35,220
space and a linear space is basically

00:38:30,390 --> 00:38:35,700
continuous values from say one to a

00:38:35,220 --> 00:38:38,640
hundred

00:38:35,700 --> 00:38:41,940
so what we're what we're trying to

00:38:38,640 --> 00:38:46,430
declare here is we want values from one

00:38:41,940 --> 00:38:51,330
to hundred and we can do the same for y

00:38:46,430 --> 00:38:54,690
so usually in machine learning we denote

00:38:51,330 --> 00:38:57,720
X and y X with the capital R with the

00:38:54,690 --> 00:39:03,810
capital X and y with a small X it's just

00:38:57,720 --> 00:39:07,530
for notation simplicity okay and again

00:39:03,810 --> 00:39:10,020
if we were to run this yeah it ran so

00:39:07,530 --> 00:39:16,670
now if I were to actually print these

00:39:10,020 --> 00:39:22,340
values of right

00:39:16,670 --> 00:39:25,490
click Play and yeah so basically all it

00:39:22,340 --> 00:39:29,720
gave me were just random values from one

00:39:25,490 --> 00:39:32,000
two hundred or in the range 1 to 100 so

00:39:29,720 --> 00:39:33,620
this allows you for quick prototyping if

00:39:32,000 --> 00:39:36,080
you just want to test out your algorithm

00:39:33,620 --> 00:39:37,790
its performance on a random data set you

00:39:36,080 --> 00:39:42,670
can actually create a random data set

00:39:37,790 --> 00:39:54,680
with just this linear space variable

00:39:42,670 --> 00:39:59,570
okay so okay we ran this okay and for a

00:39:54,680 --> 00:40:19,670
line we need random values for M and C

00:39:59,570 --> 00:40:24,410
right so so if we would actually print

00:40:19,670 --> 00:40:25,820
the values of m and C so at the

00:40:24,410 --> 00:40:28,370
beginning I told you that linear

00:40:25,820 --> 00:40:30,470
regression assumes that the values of

00:40:28,370 --> 00:40:33,260
the gradient m and the y-intercept are

00:40:30,470 --> 00:40:36,800
completely random it is gonna result in

00:40:33,260 --> 00:40:40,190
a horrible best fit line so if you run

00:40:36,800 --> 00:40:45,650
this yeah we just get two random values

00:40:40,190 --> 00:40:51,380
of M and C and next since we have the M

00:40:45,650 --> 00:40:54,010
X MX C and why well let's plot the line

00:40:51,380 --> 00:41:05,150
so let's write this helper function

00:40:54,010 --> 00:41:10,810
called plot line so it takes in M C and

00:41:05,150 --> 00:41:10,810
well x and y

00:41:11,070 --> 00:41:18,720
so for reference I've already written

00:41:13,440 --> 00:41:20,670
this function to save time so yet this

00:41:18,720 --> 00:41:30,450
is the function hold on let me just copy

00:41:20,670 --> 00:41:34,350
this okay yeah so we're using the PI

00:41:30,450 --> 00:41:40,530
plots graphing capabilities so given the

00:41:34,350 --> 00:41:43,590
values of given the values of the

00:41:40,530 --> 00:41:48,240
gradient y-intercept X and y we can

00:41:43,590 --> 00:41:51,600
actually plot these values and let's

00:41:48,240 --> 00:41:54,710
call the plot line function for our

00:41:51,600 --> 00:41:54,710
random values

00:42:05,630 --> 00:42:12,519
okay so what this will do we're calling

00:42:10,190 --> 00:42:16,099
the plot line function below over here

00:42:12,519 --> 00:42:18,680
yeah over here and what it does is

00:42:16,099 --> 00:42:21,380
basically takes our values and converts

00:42:18,680 --> 00:42:25,539
it to a line so if we were to run this

00:42:21,380 --> 00:42:25,539
function hold on

00:43:24,720 --> 00:43:28,460
so

00:43:26,420 --> 00:43:30,170
function we're just experimenting with

00:43:28,460 --> 00:43:32,559
different values to show how off the

00:43:30,170 --> 00:43:36,500
line is so as you can see from here

00:43:32,559 --> 00:43:45,079
points are all represented by these blue

00:43:36,500 --> 00:43:47,210
dots and sorry yeah hold on so if you

00:43:45,079 --> 00:43:50,990
were to run that cell we can see that

00:43:47,210 --> 00:43:54,700
the the line we plotted is pretty off

00:43:50,990 --> 00:43:54,700
from what we're supposed to get

00:44:20,120 --> 00:44:25,550
hold on let me just scroll up so you can

00:44:23,130 --> 00:44:25,550
get the code

00:44:44,670 --> 00:44:49,120
so over here we're just taking these

00:44:47,200 --> 00:44:53,080
values that we gave the function and

00:44:49,120 --> 00:44:54,850
with plotting this on the graph with

00:44:53,080 --> 00:44:57,790
scattering we're creating a scatter plot

00:44:54,850 --> 00:45:01,510
for the points that we created above the

00:44:57,790 --> 00:45:03,610
X&Y points that we created above and we

00:45:01,510 --> 00:45:11,290
are drawing the line and we're labeling

00:45:03,610 --> 00:45:15,250
it the linear regression line does

00:45:11,290 --> 00:45:17,020
everyone have the code now if you

00:45:15,250 --> 00:45:32,560
haven't finished copying it please do

00:45:17,020 --> 00:45:34,360
raise your hands okay yeah so the

00:45:32,560 --> 00:45:36,790
thousand is basically what it does is it

00:45:34,360 --> 00:45:38,260
creates a larger range for the graph to

00:45:36,790 --> 00:45:41,280
be shown so we have a rough idea of how

00:45:38,260 --> 00:45:41,280
off the line is

00:46:08,990 --> 00:46:14,150
is anyone still copying the code yeah

00:46:47,109 --> 00:46:49,170
you

00:47:00,600 --> 00:47:05,790
time you randomly initialize MNC X&Y

00:47:03,810 --> 00:47:07,170
it'll keep giving you a different graph

00:47:05,790 --> 00:47:10,710
or a different line for all of them

00:47:07,170 --> 00:47:12,690
which shows that linear regression

00:47:10,710 --> 00:47:15,150
starts off with random variables and

00:47:12,690 --> 00:47:18,680
optimizes them as we gradually move

00:47:15,150 --> 00:47:18,680
along the rounds

00:47:39,900 --> 00:47:42,680
yes

00:48:27,400 --> 00:48:34,339
is anyone still running the code okay so

00:48:32,180 --> 00:48:38,960
I'm assuming that everyone's written the

00:48:34,339 --> 00:48:40,490
code and ran the cell who doesn't see

00:48:38,960 --> 00:48:43,130
the graph yet everyone should have a

00:48:40,490 --> 00:48:44,750
graph by running the code right and we

00:48:43,130 --> 00:48:47,450
can see that this red line over here

00:48:44,750 --> 00:48:49,880
it's really off from what it's supposed

00:48:47,450 --> 00:48:52,940
to be it's supposed to be something

00:48:49,880 --> 00:48:55,369
close to these blue dots which are

00:48:52,940 --> 00:48:58,880
basically represent it which basically

00:48:55,369 --> 00:49:01,819
represent your points so these are the

00:48:58,880 --> 00:49:03,740
current values for MNC and through the

00:49:01,819 --> 00:49:06,470
process of linear regression we'll try

00:49:03,740 --> 00:49:09,140
to get these such that it represents the

00:49:06,470 --> 00:49:11,510
best fit line for these set of points

00:49:09,140 --> 00:49:16,220
over here okay

00:49:11,510 --> 00:49:19,190
so the next process is we at the start I

00:49:16,220 --> 00:49:21,890
mentioned that we follow the hypothesis

00:49:19,190 --> 00:49:24,380
function which is basically a set of

00:49:21,890 --> 00:49:27,530
rule it's a rule or a guideline that we

00:49:24,380 --> 00:49:46,280
follow such that we optimize values for

00:49:27,530 --> 00:49:50,660
foil so here we're defining a function

00:49:46,280 --> 00:50:03,309
called the hypothesis function yeah and

00:49:50,660 --> 00:50:03,309
we basically return the MX plus C yeah

00:50:31,030 --> 00:50:38,810
okay has everyone good to go okay so

00:50:35,450 --> 00:50:41,690
next we want to create this function

00:50:38,810 --> 00:50:44,990
that represents this math formula here

00:50:41,690 --> 00:50:48,740
which is our loss function so the loss

00:50:44,990 --> 00:50:50,630
function again a refresher the loss

00:50:48,740 --> 00:50:53,000
function what it does is it takes our

00:50:50,630 --> 00:50:55,850
line and it compares it with the current

00:50:53,000 --> 00:51:03,950
line and sees how off our predictions

00:50:55,850 --> 00:51:06,500
are with what it's supposed to be so and

00:51:03,950 --> 00:51:08,630
that function on the slide we initialize

00:51:06,500 --> 00:51:12,260
this value called total loss to be zero

00:51:08,630 --> 00:51:16,730
and through the process we are going to

00:51:12,260 --> 00:51:19,460
iterate through we're going to iterate

00:51:16,730 --> 00:51:22,070
and we're going to find the average loss

00:51:19,460 --> 00:51:38,720
for that one round that we're running

00:51:22,070 --> 00:51:43,940
this fall so for so what this line

00:51:38,720 --> 00:51:47,150
is in the math formula we saw that it's

00:51:43,940 --> 00:51:49,910
represented by X I and y i right

00:51:47,150 --> 00:51:53,020
so X I and why I represent a single

00:51:49,910 --> 00:51:55,760
training example in our whole data set

00:51:53,020 --> 00:51:58,850
so say I have a data set comprising of

00:51:55,760 --> 00:52:00,140
cat and dog images right so one image

00:51:58,850 --> 00:52:03,110
you know it could be a cat or dog

00:52:00,140 --> 00:52:06,680
whatever it is that represents a single

00:52:03,110 --> 00:52:08,240
training example in our data set and in

00:52:06,680 --> 00:52:12,020
machine learning we call this a training

00:52:08,240 --> 00:52:17,180
instance so and it's represented by X I

00:52:12,020 --> 00:52:19,670
and Y I and similarly we're taking we're

00:52:17,180 --> 00:52:22,070
iterating through the inter entirety of

00:52:19,670 --> 00:52:24,710
the X and y values our data set and

00:52:22,070 --> 00:52:31,180
we're looking at it example by example

00:52:24,710 --> 00:52:34,190
or instance by instance and we use a

00:52:31,180 --> 00:52:42,790
hypothesis function that we wrote

00:52:34,190 --> 00:52:48,610
earlier we pass in the values of M C and

00:52:42,790 --> 00:52:51,850
X so the next part of the formula is

00:52:48,610 --> 00:52:55,010
getting getting the difference between

00:52:51,850 --> 00:52:59,120
getting the difference between our line

00:52:55,010 --> 00:53:00,860
and the real values so I'm sorry this is

00:52:59,120 --> 00:53:02,870
supposed to be X I because we're

00:53:00,860 --> 00:53:05,630
calculating the hypothesis function for

00:53:02,870 --> 00:53:08,120
that one specific example so next we

00:53:05,630 --> 00:53:11,360
will we will create a variable called

00:53:08,120 --> 00:53:13,520
diff or difference so what this does is

00:53:11,360 --> 00:53:16,550
it basically takes the difference

00:53:13,520 --> 00:53:21,560
between our line - what it's supposed to

00:53:16,550 --> 00:53:25,100
be next in the formula we have the

00:53:21,560 --> 00:53:27,500
squaring off this difference and what

00:53:25,100 --> 00:53:33,320
we'll do is we'll call this the square

00:53:27,500 --> 00:53:34,820
is equal to num PI dot square so earlier

00:53:33,320 --> 00:53:37,160
I mentioned that number is used for

00:53:34,820 --> 00:53:39,290
numerical computing one cool thing about

00:53:37,160 --> 00:53:42,020
numpy is that it provides all these

00:53:39,290 --> 00:53:45,560
functions math functions like absolute

00:53:42,020 --> 00:53:49,070
square root square power such that we

00:53:45,560 --> 00:53:51,020
can you we can take numbers and we can

00:53:49,070 --> 00:53:52,460
perform these kind of arithmetic

00:53:51,020 --> 00:53:56,870
operations on them

00:53:52,460 --> 00:54:02,720
really easily and we pass in the value

00:53:56,870 --> 00:54:07,880
of the difference next after we have the

00:54:02,720 --> 00:54:09,770
square we want to take the sum off we

00:54:07,880 --> 00:54:12,830
want to take the sum of that difference

00:54:09,770 --> 00:54:15,950
right so in the slides we have this

00:54:12,830 --> 00:54:19,720
Sigma notation over here that over these

00:54:15,950 --> 00:54:24,770
instances we take the sum of all of them

00:54:19,720 --> 00:54:26,540
so we will call this the summation I'm

00:54:24,770 --> 00:54:30,560
not calling this sum because sum is a

00:54:26,540 --> 00:54:34,370
reserved keyword in Python and we have

00:54:30,560 --> 00:54:39,920
the numpy function of NP dot some of the

00:54:34,370 --> 00:54:45,110
square has everyone written the code for

00:54:39,920 --> 00:54:50,030
this so far if I'm going too fast please

00:54:45,110 --> 00:54:56,180
you tell me I can slow down and now we

00:54:50,030 --> 00:55:00,470
have the average which is basically 1/2

00:54:56,180 --> 00:55:03,200
times M m over here is the length of X

00:55:00,470 --> 00:55:05,450
or basically the number of examples in

00:55:03,200 --> 00:55:06,980
our training set so if the length of X

00:55:05,450 --> 00:55:10,730
is hundred that means we have hundred

00:55:06,980 --> 00:55:16,000
instances in our training set and we

00:55:10,730 --> 00:55:16,000
multiply this again with a summation I

00:55:18,220 --> 00:55:24,010
mean more than average we can actually

00:55:20,810 --> 00:55:25,940
call this the error itself or the loss

00:55:24,010 --> 00:55:29,060
because at this point of time we've

00:55:25,940 --> 00:55:32,510
successfully taken this math formula and

00:55:29,060 --> 00:55:34,730
converted it into simple four lines of

00:55:32,510 --> 00:55:37,970
code so all these four lines of code

00:55:34,730 --> 00:55:41,750
actually represent our error for this

00:55:37,970 --> 00:55:45,110
one specific training example and now we

00:55:41,750 --> 00:55:53,560
can finally add this or append this to

00:55:45,110 --> 00:55:59,220
the total loss variable and then

00:55:53,560 --> 00:55:59,220
we can finally return our total loss

00:56:20,670 --> 00:56:30,210
are we good to go okay so we run this

00:56:25,589 --> 00:56:32,910
cell and we create another cell so up

00:56:30,210 --> 00:56:35,369
next we actually have to find the

00:56:32,910 --> 00:56:37,980
partial derivative of the loss function

00:56:35,369 --> 00:56:39,539
so that we perform this process called

00:56:37,980 --> 00:56:42,480
gradient descent as I had mentioned

00:56:39,539 --> 00:56:45,420
earlier so we've taken we've written

00:56:42,480 --> 00:56:50,730
this loss function and now we perform

00:56:45,420 --> 00:56:52,920
this so again we find the derivatives

00:56:50,730 --> 00:56:55,380
with respect to the gradient and the

00:56:52,920 --> 00:56:59,640
y-intercept and we can convert this

00:56:55,380 --> 00:57:04,849
again into simple Python with this so we

00:56:59,640 --> 00:57:04,849
define a function called get derivatives

00:57:06,049 --> 00:57:11,910
we pass in the values of the gradient

00:57:09,059 --> 00:57:18,260
the y-intercept our X values and Y

00:57:11,910 --> 00:57:21,510
values and then we initialize our

00:57:18,260 --> 00:57:26,430
derivative with respect C and our

00:57:21,510 --> 00:57:29,880
derivative respect em zero and just like

00:57:26,430 --> 00:57:32,579
the loss function over here we did not M

00:57:29,880 --> 00:57:37,700
as the number of training instances in

00:57:32,579 --> 00:57:41,160
our data set yep

00:57:37,700 --> 00:57:45,779
again we want to take this change and we

00:57:41,160 --> 00:57:47,490
want to change all the values so again

00:57:45,779 --> 00:57:50,220
we want to be consistent across our

00:57:47,490 --> 00:57:53,160
whole data set which is why again we

00:57:50,220 --> 00:57:59,099
will be iterating through all the

00:57:53,160 --> 00:58:01,619
instances in our data set so the zip

00:57:59,099 --> 00:58:04,910
function is a reserved keyword in Python

00:58:01,619 --> 00:58:08,069
which basically combines two arrays so

00:58:04,910 --> 00:58:10,799
it actually puts them column by column

00:58:08,069 --> 00:58:13,710
together such that it becomes one big

00:58:10,799 --> 00:58:15,569
Omega array comprising of two small

00:58:13,710 --> 00:58:19,519
areas and you can iterate through each

00:58:15,569 --> 00:58:24,140
training instance or each set of values

00:58:19,519 --> 00:58:24,140
rapidly in this zipped array

00:58:27,030 --> 00:58:33,240
so again we get the hypothesis function

00:58:29,400 --> 00:58:42,780
we use it to get our f of X which is

00:58:33,240 --> 00:58:44,550
basically our line so the hypothesis

00:58:42,780 --> 00:58:46,920
function as we have written here it

00:58:44,550 --> 00:58:49,620
takes in the values of our gradient the

00:58:46,920 --> 00:58:53,430
y-intercept and the X of I which is

00:58:49,620 --> 00:59:00,180
basically the the index of the training

00:58:53,430 --> 00:59:04,170
instance and if we take reference to the

00:59:00,180 --> 00:59:09,420
formulas over here we take the sum of

00:59:04,170 --> 00:59:11,760
all the partial derivatives and we take

00:59:09,420 --> 00:59:14,010
the average of them so again if you were

00:59:11,760 --> 00:59:18,030
to convert this into simple Python we

00:59:14,010 --> 00:59:23,820
can say DC the derivative with respect

00:59:18,030 --> 00:59:33,830
to C it is FX minus y I as denoted by

00:59:23,820 --> 00:59:37,770
that formula and D DM is FX minus y I

00:59:33,830 --> 00:59:39,300
multiplied to X I so if you were to

00:59:37,770 --> 00:59:40,680
actually do the math the partial

00:59:39,300 --> 00:59:43,530
differentiation for the loss function

00:59:40,680 --> 00:59:46,850
with respect to both the variables C and

00:59:43,530 --> 00:59:49,410
M you'd get this formula over here and

00:59:46,850 --> 00:59:57,140
we're representing the these two

00:59:49,410 --> 00:59:59,250
equations in Python so the next step is

00:59:57,140 --> 01:00:01,230
since we're doing this for one training

00:59:59,250 --> 01:00:05,160
example we don't need to take the sum

01:00:01,230 --> 01:00:08,820
for this so what we need to do next is

01:00:05,160 --> 01:00:12,270
we need to take the average of it across

01:00:08,820 --> 01:00:15,830
all training examples so we can do this

01:00:12,270 --> 01:00:21,210
with a simple slash equals 2 which

01:00:15,830 --> 01:00:25,950
divides which divides the value all the

01:00:21,210 --> 01:00:27,780
values present in the DC and DM array by

01:00:25,950 --> 01:00:34,430
M which is the number of training

01:00:27,780 --> 01:00:37,790
iterations and then we return them and

01:00:34,430 --> 01:00:37,790
we run it

01:00:49,769 --> 01:00:54,489
in the mean time let me go get my laptop

01:00:52,150 --> 01:00:56,229
charger if you have any questions or

01:00:54,489 --> 01:00:59,039
doubts with the output you're getting

01:00:56,229 --> 01:00:59,039
please do ask

01:01:42,840 --> 01:01:53,200
hold on the mister excuse me for a

01:01:50,500 --> 01:01:55,060
moment I'll be scrolling up to the plot

01:01:53,200 --> 01:01:59,160
function because some of you may have

01:01:55,060 --> 01:01:59,160
not caught the exact functions

01:02:14,600 --> 01:02:23,040
okay so in a sense some of you I have

01:02:19,950 --> 01:02:25,610
arrived late so I'll just do a recap so

01:02:23,040 --> 01:02:29,610
that even those who have been with us

01:02:25,610 --> 01:02:35,190
you can get a rough idea of where the

01:02:29,610 --> 01:02:37,650
code is going and our progress so we

01:02:35,190 --> 01:02:40,110
start off with importing the libraries

01:02:37,650 --> 01:02:45,450
numpy and matplotlib numpy for numerical

01:02:40,110 --> 01:02:49,080
computation and matplotlib for the the

01:02:45,450 --> 01:02:54,390
graph drawing and the curves we have the

01:02:49,080 --> 01:02:59,070
x and y data set which is random values

01:02:54,390 --> 01:03:01,680
in the range of 1 200 so it's

01:02:59,070 --> 01:03:05,790
represented by capital X and lowercase Y

01:03:01,680 --> 01:03:08,520
and next we randomly initialize the

01:03:05,790 --> 01:03:11,940
values of the gradient and the

01:03:08,520 --> 01:03:15,560
y-intercept by using the numpy random

01:03:11,940 --> 01:03:18,600
Rand N which basically gives two values

01:03:15,560 --> 01:03:24,870
which are randomly initialized based on

01:03:18,600 --> 01:03:27,660
your computer's specification next up we

01:03:24,870 --> 01:03:29,910
have the plot line function which takes

01:03:27,660 --> 01:03:36,320
in our values of the gradient a

01:03:29,910 --> 01:03:40,550
y-intercept X and Y and we plot a line

01:03:36,320 --> 01:03:40,550
so if I were to run all this

01:03:47,280 --> 01:03:51,360
yeah so we can see these blue dots are

01:03:50,010 --> 01:03:53,370
the points that we're trying to create

01:03:51,360 --> 01:03:56,640
the best line for best fit line for and

01:03:53,370 --> 01:03:58,530
this red line over here it is our line

01:03:56,640 --> 01:04:00,780
or our estimate using these random

01:03:58,530 --> 01:04:03,240
values we can see that it's really off

01:04:00,780 --> 01:04:05,460
are showing that linear regression

01:04:03,240 --> 01:04:07,770
starts off with a random line randomly

01:04:05,460 --> 01:04:10,020
drawn and how we're going to optimize it

01:04:07,770 --> 01:04:12,030
in such a way that by the end of this

01:04:10,020 --> 01:04:13,890
linear regression function it'll create

01:04:12,030 --> 01:04:17,550
something that's close to the best fit

01:04:13,890 --> 01:04:19,380
line for these blue points okay next we

01:04:17,550 --> 01:04:21,270
have the hypothesis function which is

01:04:19,380 --> 01:04:23,040
basically our guideline or rule that

01:04:21,270 --> 01:04:24,840
we're trying to follow it creates a

01:04:23,040 --> 01:04:26,100
linear relationship or it assumes a

01:04:24,840 --> 01:04:32,700
linear relationship between the

01:04:26,100 --> 01:04:35,310
variables x and y we run this so after

01:04:32,700 --> 01:04:37,680
we get after we plot the line we want to

01:04:35,310 --> 01:04:38,760
see how off the prediction is so we have

01:04:37,680 --> 01:04:41,430
this thing called the loss function

01:04:38,760 --> 01:04:44,310
which basically takes the difference

01:04:41,430 --> 01:04:46,320
between our line and the line or the

01:04:44,310 --> 01:04:49,110
value that it's supposed to be our

01:04:46,320 --> 01:04:54,740
prediction and what the real value of Y

01:04:49,110 --> 01:04:57,710
should be and it uses the loss formula

01:04:54,740 --> 01:05:03,440
or the loss function formula of this and

01:04:57,710 --> 01:05:06,060
if we change this to simple Python we

01:05:03,440 --> 01:05:09,450
calculate the hypothesis function or our

01:05:06,060 --> 01:05:12,870
prediction we get the difference of FX

01:05:09,450 --> 01:05:15,780
minus y I which is basically our line -

01:05:12,870 --> 01:05:18,750
what it's supposed to be we square the

01:05:15,780 --> 01:05:21,900
difference using the pre-built numpy dot

01:05:18,750 --> 01:05:24,630
square function and we sum all these

01:05:21,900 --> 01:05:27,720
errors together and we take the average

01:05:24,630 --> 01:05:30,570
of this error and then finally we add it

01:05:27,720 --> 01:05:32,960
to the total loss variable and we return

01:05:30,570 --> 01:05:32,960
this loss

01:05:36,490 --> 01:05:41,020
and this get derivatives function what

01:05:38,860 --> 01:05:42,460
we're doing is we're translating this

01:05:41,020 --> 01:05:44,590
change in error

01:05:42,460 --> 01:05:46,540
where the error value is them is the

01:05:44,590 --> 01:05:49,000
minimum so when you take the derivative

01:05:46,540 --> 01:05:51,670
of something and equate it to zero the

01:05:49,000 --> 01:05:53,860
variables that are dependent off on it

01:05:51,670 --> 01:05:56,710
such as the gradient and y-intercept are

01:05:53,860 --> 01:06:00,580
the most optimal values when the error

01:05:56,710 --> 01:06:02,440
is zero so we change this to simple

01:06:00,580 --> 01:06:06,220
Python and we get this function where we

01:06:02,440 --> 01:06:10,750
calculate the prediction or FX and using

01:06:06,220 --> 01:06:14,170
this formula of FX minus y I and FX

01:06:10,750 --> 01:06:16,270
minus y I times X I if you do the math

01:06:14,170 --> 01:06:21,070
you can actually get these formulas but

01:06:16,270 --> 01:06:24,310
for times sake we'll just end it at this

01:06:21,070 --> 01:06:26,710
and then da we calculate the derivatives

01:06:24,310 --> 01:06:31,900
with respect to C and M take the average

01:06:26,710 --> 01:06:33,910
and we return them okay so that's a

01:06:31,900 --> 01:06:39,040
quick recap of what we've written so far

01:06:33,910 --> 01:06:41,770
because some of you entered late so this

01:06:39,040 --> 01:06:44,260
is a general recap so after we get the

01:06:41,770 --> 01:06:48,490
derivatives the next step is well to

01:06:44,260 --> 01:06:52,750
update the values of M and C so we move

01:06:48,490 --> 01:06:57,730
on and we use our update formula of M

01:06:52,750 --> 01:07:00,760
equals M minus alpha alpha multiplied to

01:06:57,730 --> 01:07:04,000
the respective derivatives or with

01:07:00,760 --> 01:07:09,850
respect to C and M and we when we

01:07:04,000 --> 01:07:11,440
convert this to Python we so if we

01:07:09,850 --> 01:07:19,240
convert this to Python we can define a

01:07:11,440 --> 01:07:25,270
function called update parameters which

01:07:19,240 --> 01:07:28,030
basically takes in the values of M C X Y

01:07:25,270 --> 01:07:29,770
and earlier I mentioned that we use this

01:07:28,030 --> 01:07:33,550
thing called the learning rate the

01:07:29,770 --> 01:07:36,850
learning rate it shows us or it tells us

01:07:33,550 --> 01:07:39,430
how quickly the the algorithm is going

01:07:36,850 --> 01:07:40,869
to converge to the optimal values so

01:07:39,430 --> 01:07:43,150
when I say converge it basically means

01:07:40,869 --> 01:07:45,430
through rounds of iteration or through

01:07:43,150 --> 01:07:47,530
the rounds of drawing the line seeing

01:07:45,430 --> 01:07:48,170
how off we are and correcting that error

01:07:47,530 --> 01:07:50,930
margin

01:07:48,170 --> 01:07:53,150
we can actually get the Alpha value a

01:07:50,930 --> 01:07:54,470
value actually dictates how fast we're

01:07:53,150 --> 01:07:57,380
going to get there or how slow we're

01:07:54,470 --> 01:07:59,570
going to get there so sometimes choosing

01:07:57,380 --> 01:08:02,150
the right or the perfect or the optimal

01:07:59,570 --> 01:08:04,580
alpha value is really important when

01:08:02,150 --> 01:08:07,130
doing linear regression because that

01:08:04,580 --> 01:08:09,890
because the speed of learning it well

01:08:07,130 --> 01:08:12,260
depends on Alpha so if we were to take

01:08:09,890 --> 01:08:15,320
reference to this formula again if we

01:08:12,260 --> 01:08:19,760
convert this to code using this get

01:08:15,320 --> 01:08:23,900
derivatives for a function above we

01:08:19,760 --> 01:08:29,570
return the values of DC and DM and we

01:08:23,900 --> 01:08:34,060
can call this function and we pass in

01:08:29,570 --> 01:08:40,880
the values of MC x and y a gradient

01:08:34,060 --> 01:08:43,070
y-intercept X&Y points and then we can

01:08:40,880 --> 01:08:46,160
finally use the update rule or the

01:08:43,070 --> 01:08:51,080
update policy or the update equation to

01:08:46,160 --> 01:08:54,190
get the the next round of values for our

01:08:51,080 --> 01:09:01,180
for the gradient and the y-intercept so

01:08:54,190 --> 01:09:08,270
alpha times D M and C equals to C minus

01:09:01,180 --> 01:09:12,460
alpha times D C then after this we

01:09:08,270 --> 01:09:14,630
basically return the values of m and C

01:09:12,460 --> 01:09:17,480
so through rounds of linear regression

01:09:14,630 --> 01:09:19,850
or through iterations or epochs we can

01:09:17,480 --> 01:09:21,650
see that the values of M and C get

01:09:19,850 --> 01:09:24,200
closer and closer to the values that

01:09:21,650 --> 01:09:26,540
they're supposed to be as in they become

01:09:24,200 --> 01:09:33,440
really close they're giving us the best

01:09:26,540 --> 01:09:35,960
fit line so after the update parameter

01:09:33,440 --> 01:09:38,840
function has been written well the next

01:09:35,960 --> 01:09:42,350
step is to perform linear regression so

01:09:38,840 --> 01:09:44,720
all these functions above the hypothesis

01:09:42,350 --> 01:09:46,580
function loss function get derivatives

01:09:44,720 --> 01:09:48,890
function and the update parameters

01:09:46,580 --> 01:09:50,980
function all of these were basically the

01:09:48,890 --> 01:09:53,890
helper functions that's gonna aid us in

01:09:50,980 --> 01:09:56,890
finally writing our linear regression

01:09:53,890 --> 01:09:56,890
algorithm

01:10:04,130 --> 01:10:10,620
so again we give it the values of MC

01:10:07,700 --> 01:10:13,290
sorry so linear regression since it's

01:10:10,620 --> 01:10:16,470
the final function we just need a pass

01:10:13,290 --> 01:10:19,920
in X&Y because the values of M and C

01:10:16,470 --> 01:10:23,250
they are created internally in this

01:10:19,920 --> 01:10:26,430
function so they take reference with the

01:10:23,250 --> 01:10:30,570
code we can see that mm see the first

01:10:26,430 --> 01:10:33,600
two lines yeah the first two lines we

01:10:30,570 --> 01:10:36,240
create random values for them in this

01:10:33,600 --> 01:10:39,030
case we're multiplying 0.01 to them so

01:10:36,240 --> 01:10:41,310
as to ensure that the values remain

01:10:39,030 --> 01:10:43,740
small and easy to calculate with because

01:10:41,310 --> 01:10:46,560
when you have two large values or really

01:10:43,740 --> 01:10:50,010
big values your computer is going to

01:10:46,560 --> 01:10:53,040
overflow because it can't compute with

01:10:50,010 --> 01:10:55,380
that large of a number so again we

01:10:53,040 --> 01:11:01,230
create random variables using the

01:10:55,380 --> 01:11:05,690
pre-built MMP random dot r and n so this

01:11:01,230 --> 01:11:10,580
gives us two random values for the

01:11:05,690 --> 01:11:10,580
y-intercept and the gradient

01:11:24,969 --> 01:11:33,619
okay so now that we've initialized the

01:11:29,119 --> 01:11:36,739
random values of M and C next step is to

01:11:33,619 --> 01:11:38,750
create is to initialize the alpha or the

01:11:36,739 --> 01:11:40,850
learning rate which basically dictates

01:11:38,750 --> 01:11:42,770
or shows how fast the algorithm is going

01:11:40,850 --> 01:11:45,290
to be in learning the relationship

01:11:42,770 --> 01:11:48,409
between x and y so here we're going to

01:11:45,290 --> 01:11:50,060
set it something really low like 0.05 so

01:11:48,409 --> 01:11:52,580
usually alpha values are set to

01:11:50,060 --> 01:11:54,980
something small so that

01:11:52,580 --> 01:11:57,409
they can converge faster rather than

01:11:54,980 --> 01:11:59,780
having a really large value where it

01:11:57,409 --> 01:12:03,130
keeps bouncing back and forth between

01:11:59,780 --> 01:12:06,860
the optimal value so supposing that the

01:12:03,130 --> 01:12:11,090
optimal value is 2 so it'll keep jumping

01:12:06,860 --> 01:12:12,619
from 1 to 3 multiple times or 0 to 4

01:12:11,090 --> 01:12:15,020
multiple times until it finally

01:12:12,619 --> 01:12:19,040
converges to 2 but if we have something

01:12:15,020 --> 01:12:21,949
like 0.05 we can probably jump from 1.5

01:12:19,040 --> 01:12:24,830
to 2.5 and Sun and then finally converge

01:12:21,949 --> 01:12:29,060
it to so learning is much smoother it's

01:12:24,830 --> 01:12:31,400
faster and more efficient so up next is

01:12:29,060 --> 01:12:34,070
the number of rounds and the number of

01:12:31,400 --> 01:12:36,050
rounds basically denotes the number of

01:12:34,070 --> 01:12:37,880
iterations or epochs that this thing's

01:12:36,050 --> 01:12:40,130
gonna run for or the algorithm is gonna

01:12:37,880 --> 01:12:42,679
run for so by the end of thousand rounds

01:12:40,130 --> 01:12:45,650
we hope that our algorithm is able to

01:12:42,679 --> 01:12:48,230
get a decent value for M NC and can

01:12:45,650 --> 01:12:49,580
optimize it such that it's able to give

01:12:48,230 --> 01:12:54,530
us something that's close to the best

01:12:49,580 --> 01:12:57,080
fit line so again we iterate through

01:12:54,530 --> 01:13:02,270
these different values through these

01:12:57,080 --> 01:13:04,100
numbers or through these epochs and we

01:13:02,270 --> 01:13:05,900
hope that by thousand rounds we can get

01:13:04,100 --> 01:13:08,260
em NC that's going to give us the best

01:13:05,900 --> 01:13:08,260
fit line

01:13:13,290 --> 01:13:18,640
so CNM we can get it using this update

01:13:17,320 --> 01:13:21,550
parameters function which basically

01:13:18,640 --> 01:13:32,020
gives us the values of M and C and

01:13:21,550 --> 01:13:34,450
returns it so for this update parameters

01:13:32,020 --> 01:13:40,330
we give it the gradient the y-intercept

01:13:34,450 --> 01:13:42,610
X Y and our learning rate alpha such

01:13:40,330 --> 01:13:45,250
that it's able to get calculate the next

01:13:42,610 --> 01:13:47,800
value of M that's closer to the optimal

01:13:45,250 --> 01:13:51,850
values by using that formula that was

01:13:47,800 --> 01:13:55,870
shown earlier and then after that we can

01:13:51,850 --> 01:13:58,300
basically return M and C so by the end

01:13:55,870 --> 01:14:00,760
of this training loop we call this think

01:13:58,300 --> 01:14:03,550
the training loop for the training job

01:14:00,760 --> 01:14:05,710
or the training cycle we hope that the

01:14:03,550 --> 01:14:07,450
values of M and C have converged the

01:14:05,710 --> 01:14:10,530
optimal values of what they're supposed

01:14:07,450 --> 01:14:13,360
to be and then we can return m and C

01:14:10,530 --> 01:14:16,090
remember this plot line function that

01:14:13,360 --> 01:14:19,030
we'd written earlier what we can do is

01:14:16,090 --> 01:14:22,239
we can call this plot line function in

01:14:19,030 --> 01:14:24,910
our linear regression algorithm just

01:14:22,239 --> 01:14:28,300
after we've initialized these random

01:14:24,910 --> 01:14:30,790
values for M and C just to give us a

01:14:28,300 --> 01:14:35,350
rough idea of how off the line is when

01:14:30,790 --> 01:14:37,690
we first start off then what we can do

01:14:35,350 --> 01:14:41,020
is after this training job or training

01:14:37,690 --> 01:14:44,020
cycle we can actually plot it again so

01:14:41,020 --> 01:14:52,989
this shows us if the line has improved

01:14:44,020 --> 01:14:56,470
at all okay yeah and for simplicity sake

01:14:52,989 --> 01:14:58,660
or for greater visualization or detail

01:14:56,470 --> 01:15:02,680
we can actually print out these initial

01:14:58,660 --> 01:15:06,280
values of M and C and we can print the

01:15:02,680 --> 01:15:13,469
final values of M and C after this

01:15:06,280 --> 01:15:15,360
entire process has ended so yeah so this

01:15:13,469 --> 01:15:18,420
the code for the final linear regression

01:15:15,360 --> 01:15:23,659
algorithm so what it does is it takes

01:15:18,420 --> 01:15:26,999
the values of x and y our data set and

01:15:23,659 --> 01:15:29,489
assigns it random values of m and c and

01:15:26,999 --> 01:15:31,860
by the end of the training cycle we

01:15:29,489 --> 01:15:32,489
optimize these values to get something

01:15:31,860 --> 01:15:59,280
that's better

01:15:32,489 --> 01:16:01,710
at giving us the best fit line a silly

01:15:59,280 --> 01:16:03,479
mistake on my part are in the update

01:16:01,710 --> 01:16:08,309
parameters function in the linear

01:16:03,479 --> 01:16:11,219
regression algorithm formula above we

01:16:08,309 --> 01:16:13,619
returned m and c it's supposed to be m

01:16:11,219 --> 01:16:22,559
and C here as well we don't want to

01:16:13,619 --> 01:16:25,079
confuse the values of both of them so

01:16:22,559 --> 01:16:27,239
then after writing all of this all the

01:16:25,079 --> 01:16:31,619
helper functions we've converted math

01:16:27,239 --> 01:16:39,599
into code we can finally call linear

01:16:31,619 --> 01:16:46,110
regression of x and y that we

01:16:39,599 --> 01:16:48,809
initialized over here in this cell so

01:16:46,110 --> 01:16:51,150
now it is the moment of truth all these

01:16:48,809 --> 01:16:53,340
functions that you're written above all

01:16:51,150 --> 01:16:55,260
these contribute to the final linear

01:16:53,340 --> 01:16:57,510
regression formula or the linear

01:16:55,260 --> 01:17:01,650
regression algorithm to optimize this

01:16:57,510 --> 01:17:05,639
line so let's run this hopefully the MNC

01:17:01,650 --> 01:17:08,519
values are able to get optimized so we

01:17:05,639 --> 01:17:11,999
can see a horribly drawn line over here

01:17:08,519 --> 01:17:15,510
with values of these values of M and C

01:17:11,999 --> 01:17:17,269
and with well we see that the values of

01:17:15,510 --> 01:17:20,820
M and C have improved in some way

01:17:17,269 --> 01:17:22,559
through thousand iterations we can see

01:17:20,820 --> 01:17:25,809
that this line is not the best fit line

01:17:22,559 --> 01:17:28,059
clearly because it doesn't really

01:17:25,809 --> 01:17:31,869
is it is not a really parallel to these

01:17:28,059 --> 01:17:34,449
blue points here it shows us that the

01:17:31,869 --> 01:17:35,320
either the number of rounds of iteration

01:17:34,449 --> 01:17:37,780
is not enough

01:17:35,320 --> 01:17:41,230
so either we need to pump it up I don't

01:17:37,780 --> 01:17:44,619
know 10,000 100,000 or that the values

01:17:41,230 --> 01:17:47,559
of MNC are really small so we can play

01:17:44,619 --> 01:17:49,989
around with these values of MNC such

01:17:47,559 --> 01:17:52,659
that they give us values that we can

01:17:49,989 --> 01:17:56,199
optimize further within these thousand

01:17:52,659 --> 01:18:00,340
rounds so here we get something that's

01:17:56,199 --> 01:18:03,159
almost close to the best fit line so

01:18:00,340 --> 01:18:06,820
what this does what so when we initially

01:18:03,159 --> 01:18:09,250
randomized these variables for MNC we

01:18:06,820 --> 01:18:11,650
get a really horrible line and by the

01:18:09,250 --> 01:18:12,940
end of thousand iterations or thousand

01:18:11,650 --> 01:18:16,949
rounds we can get something that's

01:18:12,940 --> 01:18:21,010
closer to the points that we plotted and

01:18:16,949 --> 01:18:22,809
we can even see that the values of the

01:18:21,010 --> 01:18:25,420
MNC have changed from what they

01:18:22,809 --> 01:18:26,860
previously were which shows us that the

01:18:25,420 --> 01:18:29,829
linear regression algorithm is doing

01:18:26,860 --> 01:18:32,199
something to optimize these values so

01:18:29,829 --> 01:18:33,820
for some of you it may work and for some

01:18:32,199 --> 01:18:35,289
of you it may not probably because of

01:18:33,820 --> 01:18:37,389
the internal state of your machine

01:18:35,289 --> 01:18:42,099
because when we initialize random

01:18:37,389 --> 01:18:44,440
variables on a computer it it's unique

01:18:42,099 --> 01:18:46,059
for each computer so your line may be

01:18:44,440 --> 01:18:49,300
different from the person sitting next

01:18:46,059 --> 01:18:50,849
to you which is why some of the values

01:18:49,300 --> 01:18:54,429
that were randomly assigned may not

01:18:50,849 --> 01:18:57,159
assign here may not be fully reflective

01:18:54,429 --> 01:18:59,320
of the true capabilities of the linear

01:18:57,159 --> 01:19:01,960
regression algorithm because in the

01:18:59,320 --> 01:19:05,860
slide that I showed you this was a

01:19:01,960 --> 01:19:08,739
visualization of linear regression where

01:19:05,860 --> 01:19:10,210
we take in the x and y variables and we

01:19:08,739 --> 01:19:13,239
can see that it's supposed to be doing

01:19:10,210 --> 01:19:15,820
this so this is the true representative

01:19:13,239 --> 01:19:19,929
of the linear regression function such

01:19:15,820 --> 01:19:21,760
that it optimizes MNC there give us

01:19:19,929 --> 01:19:26,920
something that's close to the best fit

01:19:21,760 --> 01:19:30,219
line okay

01:19:26,920 --> 01:19:33,610
due to time constraints I will be moving

01:19:30,219 --> 01:19:36,510
a bit faster hopefully it's not too much

01:19:33,610 --> 01:19:36,510
of a trouble

01:19:37,230 --> 01:19:46,650
so now that we've we've performed linear

01:19:43,830 --> 01:19:49,170
regression and some of us almost of us

01:19:46,650 --> 01:19:51,030
have the bare-bones concept of linear

01:19:49,170 --> 01:19:53,610
regression and how it works we will be

01:19:51,030 --> 01:19:56,790
moving on to this thing called the

01:19:53,610 --> 01:19:58,710
single layer perceptron so it comes from

01:19:56,790 --> 01:20:01,770
the root word percept which basically

01:19:58,710 --> 01:20:02,940
means think or has the ability to think

01:20:01,770 --> 01:20:05,580
so

01:20:02,940 --> 01:20:08,370
single layer because it does I'll be

01:20:05,580 --> 01:20:11,430
showing you later on it consists of a

01:20:08,370 --> 01:20:12,870
single nonlinear function in it which is

01:20:11,430 --> 01:20:17,160
able to give us a better representation

01:20:12,870 --> 01:20:19,020
of the data set and linear regression in

01:20:17,160 --> 01:20:21,870
this form in the most basic form it can

01:20:19,020 --> 01:20:24,450
only take in one value of x for the next

01:20:21,870 --> 01:20:27,210
value of y or for a corresponding value

01:20:24,450 --> 01:20:29,130
of x and y but in single layer

01:20:27,210 --> 01:20:32,220
perceptrons it can take in multiple

01:20:29,130 --> 01:20:34,230
values of X which we call features right

01:20:32,220 --> 01:20:35,910
so for a flower we have multiple

01:20:34,230 --> 01:20:38,640
features you know it's the length of its

01:20:35,910 --> 01:20:42,989
stalk the length of length of its petal

01:20:38,640 --> 01:20:45,750
the width of its petal and we can use

01:20:42,989 --> 01:20:48,360
all these attributes or features or

01:20:45,750 --> 01:20:53,390
characteristics of the flower to predict

01:20:48,360 --> 01:21:02,040
what type of flower it is but before I

01:20:53,390 --> 01:21:06,530
begin it might be a problem because my

01:21:02,040 --> 01:21:06,530
charger can't fit in okay

01:21:09,850 --> 01:21:14,560
okay so linear regression it assumed a

01:21:13,210 --> 01:21:17,140
linear relationship between the

01:21:14,560 --> 01:21:19,930
variables x and y so what single-layer

01:21:17,140 --> 01:21:22,750
perceptrons do is it assumes a nonlinear

01:21:19,930 --> 01:21:24,940
relationship between x and y so linear

01:21:22,750 --> 01:21:27,340
relationships are restricted in the

01:21:24,940 --> 01:21:29,230
kinds of mappings they can do between

01:21:27,340 --> 01:21:31,540
those two variables because well we're

01:21:29,230 --> 01:21:33,760
only confined to a line but a curve can

01:21:31,540 --> 01:21:36,130
have any kind of shape and form which

01:21:33,760 --> 01:21:40,150
makes it really flexible and giving us

01:21:36,130 --> 01:21:42,730
an ideal or more than ideal mapping or

01:21:40,150 --> 01:21:45,370
representation or relationship between x

01:21:42,730 --> 01:21:47,200
and y so as mentioned in the slides we

01:21:45,370 --> 01:21:50,290
can get the best fit curve for the data

01:21:47,200 --> 01:21:53,650
using a nonlinear method such as a

01:21:50,290 --> 01:21:58,510
single-layer perceptron but before we

01:21:53,650 --> 01:22:00,910
begin the inspiration for a single-layer

01:21:58,510 --> 01:22:04,780
perceptron came from the biological

01:22:00,910 --> 01:22:07,240
neuron in the human brain so what it

01:22:04,780 --> 01:22:09,790
does is it gets information from the

01:22:07,240 --> 01:22:12,760
neurons around it it processes that

01:22:09,790 --> 01:22:15,940
information and it sends it outwards to

01:22:12,760 --> 01:22:18,700
other neurons nearby so it gets an

01:22:15,940 --> 01:22:22,570
information from other neurons processes

01:22:18,700 --> 01:22:24,550
it and sends it out the other end so the

01:22:22,570 --> 01:22:26,650
computational version or the

01:22:24,550 --> 01:22:28,870
mathematical model that we that

01:22:26,650 --> 01:22:32,410
scientists or researchers try to give to

01:22:28,870 --> 01:22:35,050
this biological neuron was the single

01:22:32,410 --> 01:22:37,330
layer perceptron neuron so when I say

01:22:35,050 --> 01:22:39,580
single layer it means that there's only

01:22:37,330 --> 01:22:41,890
one function or a single function that

01:22:39,580 --> 01:22:44,820
were that were performing between x and

01:22:41,890 --> 01:22:48,220
y to get the relationship between them

01:22:44,820 --> 01:22:52,090
so as I said it can take in multiple

01:22:48,220 --> 01:22:54,640
inputs so x1 all the way to xn so every

01:22:52,090 --> 01:22:56,530
to consider flowers such as the iris

01:22:54,640 --> 01:22:58,510
flowers we can have something like the

01:22:56,530 --> 01:23:01,000
width of the stalk the width and length

01:22:58,510 --> 01:23:02,950
of the petal and all of these can be

01:23:01,000 --> 01:23:05,140
individual numbers or attributes or

01:23:02,950 --> 01:23:07,660
features that we can pass into the

01:23:05,140 --> 01:23:09,100
single layer perceptron and we can get

01:23:07,660 --> 01:23:12,250
the final prediction to see whether

01:23:09,100 --> 01:23:16,210
either its iris setosa or iris virginica

01:23:12,250 --> 01:23:19,300
in this in this workshop we'll be

01:23:16,210 --> 01:23:22,350
looking at single layer perceptron where

01:23:19,300 --> 01:23:24,060
it gives us multiple predictions

01:23:22,350 --> 01:23:28,280
and the prediction with the highest

01:23:24,060 --> 01:23:31,380
probability of occurring is the one that

01:23:28,280 --> 01:23:35,310
inputs most likely denote so if I give

01:23:31,380 --> 01:23:36,330
in multiple features as my X values it

01:23:35,310 --> 01:23:39,420
would give me the different

01:23:36,330 --> 01:23:41,430
probabilities of y1 y2 and y3 which are

01:23:39,420 --> 01:23:44,310
basically predictions and the one with

01:23:41,430 --> 01:23:47,130
the highest value among y1 y2 and y3 is

01:23:44,310 --> 01:23:50,280
well basically it shows us what it is so

01:23:47,130 --> 01:23:52,950
if one type of flower was y1 another

01:23:50,280 --> 01:23:55,020
type y2 and y3 was another type of

01:23:52,950 --> 01:23:57,420
flower say that at the end of this

01:23:55,020 --> 01:24:00,540
entire training loop and cycle we get

01:23:57,420 --> 01:24:04,200
that Y 2 is the value with the highest

01:24:00,540 --> 01:24:06,930
amongst y1 2 and 3 we can say that the

01:24:04,200 --> 01:24:14,370
flower that we gave it it belongs to the

01:24:06,930 --> 01:24:16,920
second type of flower so what it does is

01:24:14,370 --> 01:24:20,670
it takes in these attributes or features

01:24:16,920 --> 01:24:22,830
it performs the summation function which

01:24:20,670 --> 01:24:25,410
is the same as we did in linear

01:24:22,830 --> 01:24:26,970
regression and it performs this thing

01:24:25,410 --> 01:24:30,360
called the activation function which

01:24:26,970 --> 01:24:33,360
I'll be showing you later on and it

01:24:30,360 --> 01:24:36,330
gives multiple outputs y 1 2 y 3 or

01:24:33,360 --> 01:24:38,940
depending on your use case and the one

01:24:36,330 --> 01:24:42,540
with the highest value or the highest

01:24:38,940 --> 01:24:46,380
probability is the one that the input

01:24:42,540 --> 01:24:49,710
belongs to or corresponds to the fun

01:24:46,380 --> 01:24:51,270
thing about single-layer perceptrons is

01:24:49,710 --> 01:24:53,660
that we only need the gradient to

01:24:51,270 --> 01:24:56,070
optimize we can completely ignore the

01:24:53,660 --> 01:24:59,220
y-intercept which makes this more

01:24:56,070 --> 01:25:01,340
efficient compared to linear regression

01:24:59,220 --> 01:25:03,750
because in linear regression where only

01:25:01,340 --> 01:25:05,460
we're trying to optimize two values but

01:25:03,750 --> 01:25:09,120
in single air perceptrons we are

01:25:05,460 --> 01:25:11,610
optimizing just one gradient value so in

01:25:09,120 --> 01:25:14,370
single air perceptrons will be working

01:25:11,610 --> 01:25:16,080
with vectors and matrices so at the

01:25:14,370 --> 01:25:18,360
start in this slide I showed you that it

01:25:16,080 --> 01:25:21,030
can take in multiple inputs right so

01:25:18,360 --> 01:25:23,700
these multiple inputs can be represented

01:25:21,030 --> 01:25:26,700
as a single vector so a vector is a

01:25:23,700 --> 01:25:29,010
single dimensional matrix so it's an N

01:25:26,700 --> 01:25:30,900
times one dimensional matrix you can

01:25:29,010 --> 01:25:32,730
assume it to be a column of values one

01:25:30,900 --> 01:25:34,770
after another which denote different

01:25:32,730 --> 01:25:35,440
things like the petal width or the petal

01:25:34,770 --> 01:25:39,350
length

01:25:35,440 --> 01:25:42,530
so we perform the summation function on

01:25:39,350 --> 01:25:46,490
the different attributes we multiply it

01:25:42,530 --> 01:25:49,520
with the corresponding values of M we

01:25:46,490 --> 01:25:50,870
don't need C because as I said that's

01:25:49,520 --> 01:25:53,270
the key that makes single-layer

01:25:50,870 --> 01:25:56,570
perceptrons more efficient and we

01:25:53,270 --> 01:25:58,730
perform the activation function on it so

01:25:56,570 --> 01:26:00,950
what exactly is an activation function

01:25:58,730 --> 01:26:04,130
the activation function is something

01:26:00,950 --> 01:26:06,740
that introduces non-linearity so

01:26:04,130 --> 01:26:08,150
initially I said that the initial

01:26:06,740 --> 01:26:10,640
relationship that the single-layer

01:26:08,150 --> 01:26:14,480
perceptron assumes is y equals MX right

01:26:10,640 --> 01:26:16,280
but that's linear in some way what the

01:26:14,480 --> 01:26:19,340
activation function does is it converts

01:26:16,280 --> 01:26:23,660
that y equals MX into something

01:26:19,340 --> 01:26:25,820
nonlinear so it takes the linear

01:26:23,660 --> 01:26:28,400
relationship apply is a random function

01:26:25,820 --> 01:26:30,320
or the activation function on it such

01:26:28,400 --> 01:26:32,870
that the model is able to find even more

01:26:30,320 --> 01:26:35,800
patterns in the data which is basically

01:26:32,870 --> 01:26:39,650
the task of machine learning so in this

01:26:35,800 --> 01:26:42,560
workshop we'll be using this activation

01:26:39,650 --> 01:26:44,450
function called sigmoid so the sigmoid

01:26:42,560 --> 01:26:47,060
function it whenever it takes in an

01:26:44,450 --> 01:26:50,480
input it performs this computation or

01:26:47,060 --> 01:26:53,090
this calculation on the X input such

01:26:50,480 --> 01:26:56,660
that it's able to squash any number any

01:26:53,090 --> 01:26:58,370
size into a number between 0 & 1 so if

01:26:56,660 --> 01:27:00,680
you have a calculator with you now you

01:26:58,370 --> 01:27:02,960
can just key in this formula and for X

01:27:00,680 --> 01:27:06,320
you can put in any value it'll give you

01:27:02,960 --> 01:27:08,420
something between 0 & 1 and in

01:27:06,320 --> 01:27:10,910
probability theory we can assume

01:27:08,420 --> 01:27:13,610
probability of an event occurring as

01:27:10,910 --> 01:27:15,170
something between 0 & 1 right so if

01:27:13,610 --> 01:27:16,280
there's something that's close to 1 we

01:27:15,170 --> 01:27:19,010
can say that there's a high probability

01:27:16,280 --> 01:27:20,390
or high chance of it occurring and if

01:27:19,010 --> 01:27:21,650
it's something close to 0 we can say

01:27:20,390 --> 01:27:25,700
that it's probably not going to happen

01:27:21,650 --> 01:27:27,320
so again finding the errors in

01:27:25,700 --> 01:27:29,030
prediction the fun fact about

01:27:27,320 --> 01:27:32,000
single-layer perceptrons is that it

01:27:29,030 --> 01:27:34,280
follows the same loss function as the

01:27:32,000 --> 01:27:37,000
linear regression which is basically

01:27:34,280 --> 01:27:41,300
this math formula that was shown earlier

01:27:37,000 --> 01:27:43,280
so what happens next is we take these

01:27:41,300 --> 01:27:45,350
the same values of the partial

01:27:43,280 --> 01:27:46,130
derivatives but in single-layer

01:27:45,350 --> 01:27:47,870
perceptrons

01:27:46,130 --> 01:27:49,040
we perform this thing called back

01:27:47,870 --> 01:27:52,370
propagation

01:27:49,040 --> 01:27:55,400
which is a step of taking these values

01:27:52,370 --> 01:27:59,120
of MNC taking the derivatives and

01:27:55,400 --> 01:28:02,510
passing them back up the chain over here

01:27:59,120 --> 01:28:04,700
so when we give a prediction we take in

01:28:02,510 --> 01:28:07,040
the values and we bring it out from left

01:28:04,700 --> 01:28:09,890
to right but in black propagation it

01:28:07,040 --> 01:28:12,860
calculates the derivatives of the value

01:28:09,890 --> 01:28:16,610
of m and it sends it back through using

01:28:12,860 --> 01:28:19,370
differentiation and this way it's you

01:28:16,610 --> 01:28:22,190
can think of it as a blame game so the

01:28:19,370 --> 01:28:24,320
final prediction blames the activation

01:28:22,190 --> 01:28:26,720
function for being wrong the activation

01:28:24,320 --> 01:28:29,690
function blames the y equals MX formula

01:28:26,720 --> 01:28:32,330
for being wrong and then through this

01:28:29,690 --> 01:28:35,510
whole blame game or process we can find

01:28:32,330 --> 01:28:37,850
who exactly or which step is giving the

01:28:35,510 --> 01:28:40,670
wrong value and we can optimize the

01:28:37,850 --> 01:28:44,800
value at that specific position which is

01:28:40,670 --> 01:28:49,070
what back propagation does but for again

01:28:44,800 --> 01:28:53,360
time issues will not be covering back

01:28:49,070 --> 01:28:57,440
propagation and we'll just do a hold on

01:28:53,360 --> 01:28:59,540
I think my battery died can I get some

01:28:57,440 --> 01:29:04,610
help over here the charge is not fitting

01:28:59,540 --> 01:29:17,300
into the black point here is there an

01:29:04,610 --> 01:29:19,640
adapter so single-layer perceptrons and

01:29:17,300 --> 01:29:22,460
linear regression models they're really

01:29:19,640 --> 01:29:24,650
similar in some aspects but in other

01:29:22,460 --> 01:29:26,810
aspects they may be slightly different

01:29:24,650 --> 01:29:28,940
which shows the difference in

01:29:26,810 --> 01:29:30,410
performance if you were to use a neural

01:29:28,940 --> 01:29:34,160
network such as a single layer

01:29:30,410 --> 01:29:36,410
perceptron and you use basic traditional

01:29:34,160 --> 01:29:38,840
algorithms such as linear regression we

01:29:36,410 --> 01:29:41,510
can see lots of differences in its

01:29:38,840 --> 01:29:46,480
performance accuracy precision and

01:29:41,510 --> 01:29:52,420
everything that gives it its final form

01:29:46,480 --> 01:29:52,420
hold on okay great

01:29:52,979 --> 01:29:59,070
so if we were to do a final study on

01:29:56,360 --> 01:30:03,150
single ear perceptrons and linear

01:29:59,070 --> 01:30:07,079
regression yeah we can see that both of

01:30:03,150 --> 01:30:10,010
them assume a partial or semi true

01:30:07,079 --> 01:30:13,590
relationship of y equals MX or linear

01:30:10,010 --> 01:30:15,420
relationship both of them compute errors

01:30:13,590 --> 01:30:18,059
of the change in errors using the loss

01:30:15,420 --> 01:30:20,429
function both of you both of them use

01:30:18,059 --> 01:30:21,719
some form of gradient descent in single

01:30:20,429 --> 01:30:24,209
layer perceptrons we use back

01:30:21,719 --> 01:30:26,639
propagation but in linear regression you

01:30:24,209 --> 01:30:29,010
use gradient descent but invariably they

01:30:26,639 --> 01:30:31,829
both do the same thing of passing the

01:30:29,010 --> 01:30:35,670
error from the prediction to the

01:30:31,829 --> 01:30:37,639
functions that gave it the value and the

01:30:35,670 --> 01:30:40,590
differences is that linear regression

01:30:37,639 --> 01:30:42,269
works with just single values but in

01:30:40,590 --> 01:30:44,789
this case in single layer perceptrons

01:30:42,269 --> 01:30:47,159
you can pass in multiple inputs making

01:30:44,789 --> 01:30:49,920
them more versatile for many use cases

01:30:47,159 --> 01:30:52,349
and single layer perceptrons apply

01:30:49,920 --> 01:30:54,749
activation functions which introduces

01:30:52,349 --> 01:30:57,630
non-linearity to the model and since

01:30:54,749 --> 01:30:59,039
since curves have more flexibility we

01:30:57,630 --> 01:31:01,349
can see that they are able to find

01:30:59,039 --> 01:31:03,530
better patterns in the data and can give

01:31:01,349 --> 01:31:06,690
more accurate predictions

01:31:03,530 --> 01:31:10,469
so now let's code a single layer

01:31:06,690 --> 01:31:14,369
perceptron so in the linear regression

01:31:10,469 --> 01:31:16,199
we saw how math heavy it was and most

01:31:14,369 --> 01:31:18,059
researchers they don't have that kind of

01:31:16,199 --> 01:31:20,699
time to write the individual helper

01:31:18,059 --> 01:31:24,170
functions and those convert those

01:31:20,699 --> 01:31:27,749
formulas to code which is why developers

01:31:24,170 --> 01:31:30,749
in many companies such as Google and

01:31:27,749 --> 01:31:34,260
Facebook they created these open-source

01:31:30,749 --> 01:31:37,139
machine learning libraries so what they

01:31:34,260 --> 01:31:37,769
do is all these formulas and helper

01:31:37,139 --> 01:31:39,900
functions

01:31:37,769 --> 01:31:41,909
it just squashes it so that your entire

01:31:39,900 --> 01:31:44,749
machine learning code can be converted

01:31:41,909 --> 01:31:48,059
to just few lines of machine learning

01:31:44,749 --> 01:31:50,070
machine learning scripts so now what

01:31:48,059 --> 01:31:52,139
we'll be doing is we'll be using a

01:31:50,070 --> 01:31:54,900
machine one such machine learning a

01:31:52,139 --> 01:31:57,239
library where we're going to build say a

01:31:54,900 --> 01:32:00,150
single layer perceptron will be

01:31:57,239 --> 01:32:03,239
pre-processing the data before hand so

01:32:00,150 --> 01:32:05,159
what the problem with machine learning

01:32:03,239 --> 01:32:06,270
is that a machine learning engineer or

01:32:05,159 --> 01:32:11,060
technician usually

01:32:06,270 --> 01:32:14,010
he spends about 80% of that time

01:32:11,060 --> 01:32:16,590
wrangling with the data spends lots of

01:32:14,010 --> 01:32:18,960
his time you know cleaning dusting and

01:32:16,590 --> 01:32:21,060
you know cleansing it such that he can

01:32:18,960 --> 01:32:23,760
spend 20 percent of his time feeding it

01:32:21,060 --> 01:32:26,240
into the model and now will actually be

01:32:23,760 --> 01:32:28,530
using one such machine learning library

01:32:26,240 --> 01:32:29,700
some of you may think it's scikit-learn

01:32:28,530 --> 01:32:32,520
if you've heard of it

01:32:29,700 --> 01:32:34,710
but no one will be using tensorflow so

01:32:32,520 --> 01:32:37,950
tensorflow is this open source machine

01:32:34,710 --> 01:32:41,940
learning framework it was released about

01:32:37,950 --> 01:32:44,160
three four years back and version 2.0

01:32:41,940 --> 01:32:46,260
just released last week at the

01:32:44,160 --> 01:32:50,370
tensorflow dev summit which makes this

01:32:46,260 --> 01:32:53,070
even more exciting because tensorflow is

01:32:50,370 --> 01:32:54,600
up and coming it'd be great if you get a

01:32:53,070 --> 01:32:57,090
hands-on experience with using it

01:32:54,600 --> 01:32:59,520
because nowadays most machine learning

01:32:57,090 --> 01:33:00,030
projects and research projects are using

01:32:59,520 --> 01:33:03,110
tensorflow

01:33:00,030 --> 01:33:05,490
or some form of tensorflow or

01:33:03,110 --> 01:33:08,100
abstractions over tensorflow for these

01:33:05,490 --> 01:33:11,340
projects making it allowing for quick

01:33:08,100 --> 01:33:12,900
prototyping experimentation you can just

01:33:11,340 --> 01:33:15,630
write a bunch of code few lines of code

01:33:12,900 --> 01:33:17,670
you can have a entire suite of machine

01:33:15,630 --> 01:33:19,560
learning tools at your disposal if you

01:33:17,670 --> 01:33:21,750
want to read more about tensorflow and

01:33:19,560 --> 01:33:23,790
its applications in the real world you

01:33:21,750 --> 01:33:27,210
can visit the website tensorflow dog and

01:33:23,790 --> 01:33:30,780
you can get hands-on code labs such as

01:33:27,210 --> 01:33:33,210
the ones that i've shown you and you can

01:33:30,780 --> 01:33:35,310
get a hands-on examples of different

01:33:33,210 --> 01:33:38,600
real-life use cases datasets that you

01:33:35,310 --> 01:33:42,300
can play with yeah all of its available

01:33:38,600 --> 01:33:44,400
tensorflow dog so now we'll be using

01:33:42,300 --> 01:33:48,660
tensorflow to predict the flower type

01:33:44,400 --> 01:33:51,330
okay so flower type as I said it can

01:33:48,660 --> 01:33:53,400
consist in this workshop will be taking

01:33:51,330 --> 01:33:56,880
two kinds of iris flowers setosa and

01:33:53,400 --> 01:33:59,490
virginica we'll be using these features

01:33:56,880 --> 01:34:02,940
the sepal length with petal length and

01:33:59,490 --> 01:34:05,700
width and we'll be taking these four

01:34:02,940 --> 01:34:08,760
attributes and we'll be predicting the

01:34:05,700 --> 01:34:11,010
type given a new data set or given a new

01:34:08,760 --> 01:34:13,710
training example that the model has

01:34:11,010 --> 01:34:16,020
never seen so in the case that we take a

01:34:13,710 --> 01:34:16,740
model today and we were going to release

01:34:16,020 --> 01:34:18,600
it online

01:34:16,740 --> 01:34:20,099
so that anyone can give it different

01:34:18,600 --> 01:34:22,320
values of length and

01:34:20,099 --> 01:34:24,389
variables we can actually predict what

01:34:22,320 --> 01:34:28,500
kind of flour it's gonna be or what type

01:34:24,389 --> 01:34:31,560
of flour is gonna be so for the data set

01:34:28,500 --> 01:34:43,020
can you all please visit this tiny URL

01:34:31,560 --> 01:34:51,389
link so capital F capital A iris SLP all

01:34:43,020 --> 01:34:52,320
caps so what this should do is hold on

01:34:51,389 --> 01:34:54,770
let me just connect to the internet

01:34:52,320 --> 01:34:54,770
again

01:35:05,820 --> 01:35:10,320
okay so you should be seeing something

01:35:08,340 --> 01:35:13,440
like this does everyone have something

01:35:10,320 --> 01:35:15,660
like this yeah so this is the data set

01:35:13,440 --> 01:35:17,880
we're gonna use but you don't need to

01:35:15,660 --> 01:35:20,220
download it will just be automatically

01:35:17,880 --> 01:35:22,050
downloading it within collab so one

01:35:20,220 --> 01:35:25,040
thing that's cool about collab is it

01:35:22,050 --> 01:35:28,770
allows you to perform command line

01:35:25,040 --> 01:35:30,630
functions inside one of the cells if you

01:35:28,770 --> 01:35:34,560
put an exclamation mark in front of it

01:35:30,630 --> 01:35:37,020
you can basically do command line

01:35:34,560 --> 01:35:39,150
functions on it so you can either use

01:35:37,020 --> 01:35:41,730
the same collab that we used for the

01:35:39,150 --> 01:35:43,710
linear regression example but for just

01:35:41,730 --> 01:35:47,390
to keep it clean I'll create a new one

01:35:43,710 --> 01:35:53,840
just for the single layer perceptron

01:35:47,390 --> 01:36:03,540
okay so okay so let me just close this

01:35:53,840 --> 01:36:06,570
okay so again if you do not know how to

01:36:03,540 --> 01:36:08,820
access a collab notebook you can go to

01:36:06,570 --> 01:36:12,030
your Google Drive login using your gmail

01:36:08,820 --> 01:36:14,460
account really simple you can click the

01:36:12,030 --> 01:36:17,580
new button in the drop down menu you'll

01:36:14,460 --> 01:36:20,280
see this thing called more and if you

01:36:17,580 --> 01:36:22,440
were to click on more again you can get

01:36:20,280 --> 01:36:24,960
this other drop-down link and you'll see

01:36:22,440 --> 01:36:29,880
this thing called collaboratory with the

01:36:24,960 --> 01:36:33,270
icon logo of CEO everyone's able to

01:36:29,880 --> 01:36:38,160
access a collab notebook right okay

01:36:33,270 --> 01:36:40,440
great okay so I have my collab notebook

01:36:38,160 --> 01:36:53,990
up and running and let me just connect

01:36:40,440 --> 01:36:57,390
it okay so in this page that you see

01:36:53,990 --> 01:36:59,190
just copy this link over here so the

01:36:57,390 --> 01:37:01,650
link over there that leads to this page

01:36:59,190 --> 01:37:03,180
just copy it we you don't need to

01:37:01,650 --> 01:37:06,180
actually download this onto your

01:37:03,180 --> 01:37:08,670
physical machine or local machine so

01:37:06,180 --> 01:37:13,380
what you can do is you can do

01:37:08,670 --> 01:37:15,330
exclamation mark W get so what W get

01:37:13,380 --> 01:37:17,160
does is it's a function that allows you

01:37:15,330 --> 01:37:19,210
to download content off the internet

01:37:17,160 --> 01:37:23,080
without actually having to download it

01:37:19,210 --> 01:37:26,710
cailli then with that URL you just paste

01:37:23,080 --> 01:37:29,830
it so that long link just paste it with

01:37:26,710 --> 01:37:35,320
W get exclamation W get in front of it

01:37:29,830 --> 01:37:38,230
and if you were to run this you can

01:37:35,320 --> 01:37:41,530
actually download that entire thing yep

01:37:38,230 --> 01:37:46,810
it's downloaded and it's called the iris

01:37:41,530 --> 01:37:49,390
SLP dot CSV additionally if you are not

01:37:46,810 --> 01:37:53,140
really familiar with command line

01:37:49,390 --> 01:37:56,440
functions you could you can import this

01:37:53,140 --> 01:38:00,790
library called pandas so what pandas

01:37:56,440 --> 01:38:03,070
allows you to do is you can work with

01:38:00,790 --> 01:38:05,850
data sets you can make you can form

01:38:03,070 --> 01:38:08,050
tables with them allows for really easy

01:38:05,850 --> 01:38:13,450
visualization and representation of your

01:38:08,050 --> 01:38:18,219
data so so pandas comes with the

01:38:13,450 --> 01:38:21,340
built-in PD dot read CSV function okay

01:38:18,219 --> 01:38:23,620
so a read CSV and you can actually dump

01:38:21,340 --> 01:38:28,140
in this link over here and it'll

01:38:23,620 --> 01:38:31,150
actually read that online URL link and

01:38:28,140 --> 01:38:35,560
what we'll be doing next is converting

01:38:31,150 --> 01:38:38,739
this data set into a data frame so what

01:38:35,560 --> 01:38:40,989
a data frame is is you can assume it to

01:38:38,739 --> 01:38:43,360
be an Excel spreadsheet so we're

01:38:40,989 --> 01:38:45,550
converting that raw content that you saw

01:38:43,360 --> 01:38:48,340
these number these values and their

01:38:45,550 --> 01:38:51,190
flower types will actually be converting

01:38:48,340 --> 01:38:53,050
it it does something that looks and

01:38:51,190 --> 01:38:54,640
works like an Excel spreadsheet which

01:38:53,050 --> 01:38:56,080
makes it so much easier because I'm

01:38:54,640 --> 01:38:58,300
assuming that most of us have worked

01:38:56,080 --> 01:39:03,910
with Excel before and we know that we

01:38:58,300 --> 01:39:06,719
can easily manipulate data from that so

01:39:03,910 --> 01:39:11,170
you can actually print the first few

01:39:06,719 --> 01:39:14,230
your training examples on from that data

01:39:11,170 --> 01:39:16,590
set by typing data set head so let's see

01:39:14,230 --> 01:39:16,590
if this works

01:39:17,730 --> 01:39:23,340
okay turns out that the download link

01:39:20,800 --> 01:39:23,340
doesn't work

01:39:23,420 --> 01:39:28,460
in the case that you're not using a

01:39:26,160 --> 01:39:31,560
collab notebook you can use the pandas

01:39:28,460 --> 01:39:33,630
alternative to doing that but since the

01:39:31,560 --> 01:39:35,940
since collab comes with this method that

01:39:33,630 --> 01:39:43,790
automatically downloads it will just use

01:39:35,940 --> 01:39:51,630
this okay so again we'll import pandas

01:39:43,790 --> 01:39:53,340
as PD and we'll call this data set and

01:39:51,630 --> 01:39:56,880
over here in your download link you

01:39:53,340 --> 01:40:00,150
should see something called iris SLP dot

01:39:56,880 --> 01:40:03,210
CSV CSV stands for comma separated

01:40:00,150 --> 01:40:05,280
values so as you can see all these

01:40:03,210 --> 01:40:08,190
values here they are separated by commas

01:40:05,280 --> 01:40:10,410
and you can after each comma you can

01:40:08,190 --> 01:40:10,770
assume it to be something like an excel

01:40:10,410 --> 01:40:13,530
sheet

01:40:10,770 --> 01:40:20,040
so a comma is basically one cell in the

01:40:13,530 --> 01:40:25,080
excel sheet so we can say P D or pandas

01:40:20,040 --> 01:40:28,580
don't read CSV iris s LP dot CSV so

01:40:25,080 --> 01:40:32,490
since collab is already downloaded this

01:40:28,580 --> 01:40:38,370
link for us we can actually use it off

01:40:32,490 --> 01:40:41,910
the bat yep so now we have successfully

01:40:38,370 --> 01:40:44,700
loaded our data set into collab right so

01:40:41,910 --> 01:40:46,110
again as I mentioned earlier machine

01:40:44,700 --> 01:40:47,520
learning technicians and machine

01:40:46,110 --> 01:40:50,070
learning engineers those who work with

01:40:47,520 --> 01:40:52,620
ml on a daily basis most of that time

01:40:50,070 --> 01:40:55,020
goes into doing stuff like this opening

01:40:52,620 --> 01:40:57,570
or loading data sets seeing if there are

01:40:55,020 --> 01:41:00,870
missing values patching up errors in the

01:40:57,570 --> 01:41:02,640
data sets if something's wrong and only

01:41:00,870 --> 01:41:06,390
a small bit of time is actually spent

01:41:02,640 --> 01:41:12,090
creating the model training the model so

01:41:06,390 --> 01:41:14,190
yeah now that we have successfully now

01:41:12,090 --> 01:41:19,140
that we've successfully created our data

01:41:14,190 --> 01:41:20,760
set run this yeah so as we can see it

01:41:19,140 --> 01:41:22,200
looks like something off an excel sheet

01:41:20,760 --> 01:41:24,690
you know where each of these are the

01:41:22,200 --> 01:41:26,310
individual columns and cells and we can

01:41:24,690 --> 01:41:28,110
see that we're using the simple and

01:41:26,310 --> 01:41:31,430
simple with Battlement and petal width

01:41:28,110 --> 01:41:35,880
to predict the final type of the flower

01:41:31,430 --> 01:41:39,340
so let's create another self

01:41:35,880 --> 01:41:41,920
so what we'll be doing now is since we

01:41:39,340 --> 01:41:44,290
have the data set we need to convert it

01:41:41,920 --> 01:41:51,880
into something that the code that the

01:41:44,290 --> 01:41:54,130
algorithm can read right so so the

01:41:51,880 --> 01:41:56,800
pandas dataframe comes with this

01:41:54,130 --> 01:41:59,350
built-in function called data set values

01:41:56,800 --> 01:42:04,090
we just basically converts all of this

01:41:59,350 --> 01:42:06,460
into an array that we can use so if we

01:42:04,090 --> 01:42:09,730
were to create an other thing called

01:42:06,460 --> 01:42:11,770
features so when I say features it

01:42:09,730 --> 01:42:13,630
basically refers to these values such as

01:42:11,770 --> 01:42:16,900
sepal length sepal width petal length

01:42:13,630 --> 01:42:19,060
and petal width and they are the

01:42:16,900 --> 01:42:31,210
attributes that describe the flower type

01:42:19,060 --> 01:42:34,830
so so over here we're saying that we

01:42:31,210 --> 01:42:38,290
want all the columns which is denoted by

01:42:34,830 --> 01:42:40,239
this colon over here comma States that

01:42:38,290 --> 01:42:42,489
we're moving on to the next axis which

01:42:40,239 --> 01:42:45,190
is basically the column axis and when we

01:42:42,489 --> 01:42:47,260
say colon 4 we're trying to say that we

01:42:45,190 --> 01:42:50,080
want everything until the fourth column

01:42:47,260 --> 01:42:52,060
or before the 4th column so in well in

01:42:50,080 --> 01:42:56,170
Python programming indexing starts from

01:42:52,060 --> 01:43:01,120
0 right so moving from 0 1 2 3 4 right

01:42:56,170 --> 01:43:02,469
so we have 0 we have 0 1 2 3 4 and in

01:43:01,120 --> 01:43:05,560
this function we're just saying we want

01:43:02,469 --> 01:43:08,190
everything before 4 and then we can

01:43:05,560 --> 01:43:14,260
create another variable called labels

01:43:08,190 --> 01:43:16,150
which is basically minus 1 so what minus

01:43:14,260 --> 01:43:18,580
1 does is it basically just gets that

01:43:16,150 --> 01:43:21,040
last column if you have a really long

01:43:18,580 --> 01:43:23,260
set of columns or a large number of

01:43:21,040 --> 01:43:25,360
columns if you do minus 1 it just

01:43:23,260 --> 01:43:27,070
basically gets that last column and the

01:43:25,360 --> 01:43:29,710
colon basically says yeah we want the

01:43:27,070 --> 01:43:34,420
all the rows or all these hundred

01:43:29,710 --> 01:43:38,730
training samples or examples and now if

01:43:34,420 --> 01:43:38,730
we were to print features

01:43:40,980 --> 01:43:49,050
if we were to print the first example in

01:43:44,850 --> 01:43:51,630
our features data set yep it corresponds

01:43:49,050 --> 01:43:53,130
do with the values over here five point

01:43:51,630 --> 01:43:56,760
one three point five one point four and

01:43:53,130 --> 01:44:03,840
zero point two and we could do the same

01:43:56,760 --> 01:44:05,880
for so we could do the same for the

01:44:03,840 --> 01:44:11,580
labels and it'll generate it'll just

01:44:05,880 --> 01:44:13,560
return iris setosa so now that now that

01:44:11,580 --> 01:44:15,750
we have the features we can finally

01:44:13,560 --> 01:44:17,580
import numpy because we're finally

01:44:15,750 --> 01:44:19,800
getting into the exciting part we're

01:44:17,580 --> 01:44:22,530
gonna create we're gonna transform the

01:44:19,800 --> 01:44:25,550
data such that it's accessible or usable

01:44:22,530 --> 01:44:30,180
by our machine learning algorithm so

01:44:25,550 --> 01:44:34,050
features again we can call it NP dot

01:44:30,180 --> 01:44:36,810
array so it takes NP dot array on umpire

01:44:34,050 --> 01:44:38,970
a takes in a regular array and creates

01:44:36,810 --> 01:44:41,370
the numpy version of it which basically

01:44:38,970 --> 01:44:43,710
means that it's good for computation we

01:44:41,370 --> 01:44:47,180
can do anything with it and let me just

01:44:43,710 --> 01:44:50,010
feed in the features variable but since

01:44:47,180 --> 01:44:53,190
labels are text values our iris

01:44:50,010 --> 01:44:56,250
virginica or iris setosa we need to do a

01:44:53,190 --> 01:45:08,660
bit more pre-processing so we can

01:44:56,250 --> 01:45:10,099
actually iterate through the labels and

01:45:08,660 --> 01:45:12,980
we can say

01:45:10,099 --> 01:45:19,599
if the labels if the ight index in the

01:45:12,980 --> 01:45:19,599
labels is equal to iris setosa

01:45:21,550 --> 01:45:27,829
then so now what we're gonna do is we're

01:45:25,639 --> 01:45:31,699
gonna above this we can create this

01:45:27,829 --> 01:45:35,210
empty array called Y which is basically

01:45:31,699 --> 01:45:39,489
our x and y variables this is y so if it

01:45:35,210 --> 01:45:44,510
sees iris setosa we can append the array

01:45:39,489 --> 01:45:46,400
1 0 so this 1 0 notation we call this

01:45:44,510 --> 01:45:46,880
thing one hot encoding in machine

01:45:46,400 --> 01:45:50,150
learning

01:45:46,880 --> 01:45:51,980
so 1 hot means that if we have multiple

01:45:50,150 --> 01:45:54,469
classes or multiple types that were

01:45:51,980 --> 01:45:58,219
predicting the whenever it's a 1 it

01:45:54,469 --> 01:46:00,710
shows that it belongs to that type so if

01:45:58,219 --> 01:46:02,750
it sees something like iris setosa we're

01:46:00,710 --> 01:46:04,310
just saying out of the two out of the

01:46:02,750 --> 01:46:07,730
two categories of flowers that can

01:46:04,310 --> 01:46:10,070
belong to when it sees this these kind

01:46:07,730 --> 01:46:13,460
of values for the petal and sepals and

01:46:10,070 --> 01:46:15,889
sin widths we are assigning it to be the

01:46:13,460 --> 01:46:17,810
iris setosa so 1 & 0

01:46:15,889 --> 01:46:19,699
they're just binary values so we're just

01:46:17,810 --> 01:46:22,940
saying that one represents iris setosa

01:46:19,699 --> 01:46:26,389
if it's there in the first if it's the

01:46:22,940 --> 01:46:29,650
first number and then if it's the second

01:46:26,389 --> 01:46:44,500
number we create another if statement if

01:46:29,650 --> 01:46:48,079
labels I is equal to Iris then we append

01:46:44,500 --> 01:46:49,790
0 1 which basically is saying which is

01:46:48,079 --> 01:46:52,310
basically saying now if you see

01:46:49,790 --> 01:46:54,980
something that a label that corresponds

01:46:52,310 --> 01:46:57,800
to iris virginica or the second type in

01:46:54,980 --> 01:47:00,469
our training example we want Y to be

01:46:57,800 --> 01:47:04,849
appended with 0 1 this time showing that

01:47:00,469 --> 01:47:07,849
this value holds the features for iris

01:47:04,849 --> 01:47:10,070
virginica and not iris setosa and then

01:47:07,849 --> 01:47:15,290
now we can finally create the numpy

01:47:10,070 --> 01:47:16,610
array for y and for feature since we're

01:47:15,290 --> 01:47:18,349
already pretty much done with the

01:47:16,610 --> 01:47:20,329
features with pre-processed it we can

01:47:18,349 --> 01:47:23,020
actually call it x because now we're

01:47:20,329 --> 01:47:26,810
finally ready to pass it on to our

01:47:23,020 --> 01:47:31,360
machine learning algorithm and we run it

01:47:26,810 --> 01:47:31,360
hold on my bed

01:47:59,260 --> 01:48:06,020
runs okay so now we can actually go on

01:48:02,929 --> 01:48:08,300
to building our model so colab since it

01:48:06,020 --> 01:48:10,790
was created by Google and tensorflow

01:48:08,300 --> 01:48:13,010
it's also created by Google naturally

01:48:10,790 --> 01:48:16,280
we'd expect collab to have some support

01:48:13,010 --> 01:48:20,179
for using tensorflow so we can easily

01:48:16,280 --> 01:48:22,010
say import tensorflow as TF again if you

01:48:20,179 --> 01:48:24,020
want more details in documentation for

01:48:22,010 --> 01:48:26,060
tensorflow you can visit tensorflow dog

01:48:24,020 --> 01:48:28,460
and you can see the cool kinds of

01:48:26,060 --> 01:48:31,449
playgrounds and data sets they provide

01:48:28,460 --> 01:48:34,850
for quick experimentation so we are

01:48:31,449 --> 01:48:37,100
importing tensorflow and intensive flow

01:48:34,850 --> 01:48:40,640
we have this sub module called Kara's

01:48:37,100 --> 01:48:41,960
which comprises of other functions or

01:48:40,640 --> 01:48:44,449
smaller functions that's going to help

01:48:41,960 --> 01:48:49,330
us build a model with even fewer lines

01:48:44,449 --> 01:48:54,670
of code so we can say from tensorflow

01:48:49,330 --> 01:49:01,190
don't more from dot Kara's dot models

01:48:54,670 --> 01:49:04,520
import sequential so sequential is a

01:49:01,190 --> 01:49:06,590
class or an object that basically holds

01:49:04,520 --> 01:49:09,320
the different layers for a new network

01:49:06,590 --> 01:49:12,020
so neural networks they consist of

01:49:09,320 --> 01:49:14,330
multiple layers and information passes

01:49:12,020 --> 01:49:16,040
through each of the layers and performs

01:49:14,330 --> 01:49:18,230
different computations depending on what

01:49:16,040 --> 01:49:19,880
kind of layer it is and sequential is

01:49:18,230 --> 01:49:22,280
just basically a container for all of

01:49:19,880 --> 01:49:23,780
these layers so what we'll be doing is

01:49:22,280 --> 01:49:26,330
we'll be using this thing called the

01:49:23,780 --> 01:49:29,780
dense layer which basically represents

01:49:26,330 --> 01:49:32,510
one neuron or one set of neurons or one

01:49:29,780 --> 01:49:45,280
layer and we call from cancer floor

01:49:32,510 --> 01:49:47,230
chaos layers import dense dense yeah

01:49:45,280 --> 01:49:48,190
so so far doesn't

01:49:47,230 --> 01:49:50,560
how many questions with the

01:49:48,190 --> 01:49:52,420
pre-processing steps because now we're

01:49:50,560 --> 01:49:54,820
actually gonna go we're gonna dive in to

01:49:52,420 --> 01:50:00,180
creating tensor flow models and passing

01:49:54,820 --> 01:50:02,620
in data through it next since we have

01:50:00,180 --> 01:50:07,270
called the modules we need four tensor

01:50:02,620 --> 01:50:12,880
flow we call them and yeah we click a

01:50:07,270 --> 01:50:15,730
new cell model equals sequential again I

01:50:12,880 --> 01:50:18,160
said that sick draw is an object which

01:50:15,730 --> 01:50:20,320
basically represents a container for the

01:50:18,160 --> 01:50:23,200
layers and since a single layer

01:50:20,320 --> 01:50:26,980
perceptron is consists of a single layer

01:50:23,200 --> 01:50:30,130
you'll just be doing model dot ad so ad

01:50:26,980 --> 01:50:32,980
is a function that basically takes that

01:50:30,130 --> 01:50:36,880
layer and dumps it into our sequential

01:50:32,980 --> 01:50:41,290
container dense is an other object which

01:50:36,880 --> 01:50:44,739
basically takes in the the input and

01:50:41,290 --> 01:50:46,989
gives the output so earlier I said that

01:50:44,739 --> 01:50:49,030
it assumes a y equals MX relationship

01:50:46,989 --> 01:50:49,900
and then applies the activation function

01:50:49,030 --> 01:50:52,360
on it right

01:50:49,900 --> 01:50:55,270
so in this dense function we'll be doing

01:50:52,360 --> 01:50:56,470
exactly that so over here there are two

01:50:55,270 --> 01:50:58,780
kinds of flowers that we're trying to

01:50:56,470 --> 01:51:01,090
predict either iris virginica iris

01:50:58,780 --> 01:51:04,420
setosa so we're gonna put the number of

01:51:01,090 --> 01:51:08,590
new with number of output gates as two

01:51:04,420 --> 01:51:10,720
or the output units as two so depending

01:51:08,590 --> 01:51:14,710
on the probability value for either one

01:51:10,720 --> 01:51:18,760
of those two units we'll know which kind

01:51:14,710 --> 01:51:22,239
of flower it belongs to next we have the

01:51:18,760 --> 01:51:25,540
input shape so what input shape is is

01:51:22,239 --> 01:51:28,690
it's the dimensions of the training data

01:51:25,540 --> 01:51:31,180
that we're putting into it so since here

01:51:28,690 --> 01:51:34,090
we're using only four attributes we'll

01:51:31,180 --> 01:51:36,340
just say four comma because four

01:51:34,090 --> 01:51:40,620
represents the petal length battle with

01:51:36,340 --> 01:51:43,900
sepal length and sepal width and then

01:51:40,620 --> 01:51:45,190
activation so earlier in the slides i

01:51:43,900 --> 01:51:48,790
mentioned that we're going to be using

01:51:45,190 --> 01:51:51,370
the sigmoid activation function which

01:51:48,790 --> 01:51:53,920
basically takes really large values and

01:51:51,370 --> 01:51:55,960
squashes it to a value between zero and

01:51:53,920 --> 01:51:58,360
one such that it gives us a probability

01:51:55,960 --> 01:52:00,500
value of what the inputs could

01:51:58,360 --> 01:52:05,120
correspond to

01:52:00,500 --> 01:52:06,860
next up we have the compliation step so

01:52:05,120 --> 01:52:09,650
intensive flow you need to compile the

01:52:06,860 --> 01:52:12,530
model basically we need to give it the

01:52:09,650 --> 01:52:16,550
loss function so the loss you can just

01:52:12,530 --> 01:52:18,830
type loss equals mean square error so

01:52:16,550 --> 01:52:21,320
this formula I showed you here for the

01:52:18,830 --> 01:52:24,260
loss function it's called the mean

01:52:21,320 --> 01:52:26,390
square error formula or the mean square

01:52:24,260 --> 01:52:28,610
error loss function there are all there

01:52:26,390 --> 01:52:30,560
are different kinds of loss functions

01:52:28,610 --> 01:52:32,030
that you can find documentation for it

01:52:30,560 --> 01:52:34,900
on the Thames flow website you have

01:52:32,030 --> 01:52:37,280
things like hinge loss absolute loss

01:52:34,900 --> 01:52:39,560
categorical cross entropy but all these

01:52:37,280 --> 01:52:43,790
are the terms we won't be discussing it

01:52:39,560 --> 01:52:47,390
today so mean square error is our

01:52:43,790 --> 01:52:53,000
preferred choice of loss function epochs

01:52:47,390 --> 01:52:55,580
is sorry optimizer so the optimizer is

01:52:53,000 --> 01:52:57,410
out of the scope of this workshop but

01:52:55,580 --> 01:53:00,140
what the optimizer does is it basically

01:52:57,410 --> 01:53:02,600
makes the running of this algorithm much

01:53:00,140 --> 01:53:05,150
faster depending on what kind of system

01:53:02,600 --> 01:53:08,600
I'm using so here I'll just be using

01:53:05,150 --> 01:53:10,640
Adam Adam stands for adaptive momentum

01:53:08,600 --> 01:53:14,420
which is another machine learning term

01:53:10,640 --> 01:53:16,430
but we won't be getting into that you

01:53:14,420 --> 01:53:19,190
can ignore the optimized a bit but add

01:53:16,430 --> 01:53:21,230
it in your code and then the for the

01:53:19,190 --> 01:53:24,620
metrics what metrics are is basically

01:53:21,230 --> 01:53:26,210
what we're looking at for the model so

01:53:24,620 --> 01:53:29,960
of course we're gonna be looking at the

01:53:26,210 --> 01:53:31,820
mean square error so through gradient

01:53:29,960 --> 01:53:33,680
descent and back propagation you should

01:53:31,820 --> 01:53:36,230
observe that the lost values keep

01:53:33,680 --> 01:53:38,090
decreasing over time so if I took the

01:53:36,230 --> 01:53:41,660
lost values from our original linear

01:53:38,090 --> 01:53:43,130
regression code and I actually I was it

01:53:41,660 --> 01:53:44,630
I was actually supposed to print them

01:53:43,130 --> 01:53:46,790
out you can see that the values are

01:53:44,630 --> 01:53:48,140
decreasing which shows that the line is

01:53:46,790 --> 01:53:50,690
becoming closer to the best-fit line

01:53:48,140 --> 01:53:54,380
because it's giving a smaller error and

01:53:50,690 --> 01:53:56,330
that's shown by the decreasing values so

01:53:54,380 --> 01:53:58,460
for the metrics we want to observe the

01:53:56,330 --> 01:54:01,790
mean square error or that difference and

01:53:58,460 --> 01:54:05,690
we also want to measure its accuracy for

01:54:01,790 --> 01:54:07,280
the model so

01:54:05,690 --> 01:54:08,989
now that we're done compiling it you can

01:54:07,280 --> 01:54:10,790
actually intensify though you can

01:54:08,989 --> 01:54:12,530
actually get a summary of your model it

01:54:10,790 --> 01:54:17,630
actually prints out this really detailed

01:54:12,530 --> 01:54:19,340
table of your model yeah so as we can

01:54:17,630 --> 01:54:21,680
see we passed so this is like the

01:54:19,340 --> 01:54:24,170
container you can assume it to be a box

01:54:21,680 --> 01:54:26,989
and then the different layers are

01:54:24,170 --> 01:54:27,949
basically things that we put in into

01:54:26,989 --> 01:54:30,199
that container

01:54:27,949 --> 01:54:34,430
so over here we passed in one dense

01:54:30,199 --> 01:54:35,960
layer or one single layer for the single

01:54:34,430 --> 01:54:38,570
layer perceptron which is shown over

01:54:35,960 --> 01:54:42,350
here and we can see that we're giving

01:54:38,570 --> 01:54:44,840
Cooper we have two output units one for

01:54:42,350 --> 01:54:46,940
iris setosa and one for iris virginica

01:54:44,840 --> 01:54:49,930
so the one with the higher value at the

01:54:46,940 --> 01:54:56,989
end shows what flower type it belongs to

01:54:49,930 --> 01:55:00,199
next up we can finally we can actually

01:54:56,989 --> 01:55:02,810
train this algorithm now that we have

01:55:00,199 --> 01:55:07,390
the x and y variables in our hands we

01:55:02,810 --> 01:55:10,310
have the model we can say model dot fit

01:55:07,390 --> 01:55:12,410
so I'm storing this mauve this training

01:55:10,310 --> 01:55:17,000
cycle in this variable called history or

01:55:12,410 --> 01:55:18,440
training history the training history is

01:55:17,000 --> 01:55:20,660
going to contain our metrics and the

01:55:18,440 --> 01:55:24,080
values for the mean square error or the

01:55:20,660 --> 01:55:29,000
difference as well as accuracy and I

01:55:24,080 --> 01:55:30,380
pass in X Y and just as in linear

01:55:29,000 --> 01:55:33,350
regression we pass in the number of

01:55:30,380 --> 01:55:35,239
rounds so again the football player when

01:55:33,350 --> 01:55:38,210
he's kicking the ball he takes multiple

01:55:35,239 --> 01:55:40,610
rounds to finally score the goal so

01:55:38,210 --> 01:55:43,760
similarly we have multiple rounds of

01:55:40,610 --> 01:55:45,170
optimizing the values so here we're

01:55:43,760 --> 01:55:47,390
gonna just take something like 50

01:55:45,170 --> 01:55:50,210
because since it's a neural network and

01:55:47,390 --> 01:55:51,830
it assumes a nonlinear relationship the

01:55:50,210 --> 01:55:53,750
good thing is we don't need to train it

01:55:51,830 --> 01:55:55,370
for something like thousand rounds we

01:55:53,750 --> 01:55:59,000
can train it for something as less than

01:55:55,370 --> 01:56:02,540
as close to 10 or 50 depending on how

01:55:59,000 --> 01:56:08,000
much data you have and finally if you

01:56:02,540 --> 01:56:09,590
were to run this you can actually so

01:56:08,000 --> 01:56:12,260
tensorflow gives you this really nice

01:56:09,590 --> 01:56:15,140
visualization of your training cycle so

01:56:12,260 --> 01:56:17,600
for all these hundred examples and for

01:56:15,140 --> 01:56:19,520
these epochs it completes all of them

01:56:17,600 --> 01:56:21,710
and it actually gives you your metric

01:56:19,520 --> 01:56:23,600
so over that we said we wanted to see

01:56:21,710 --> 01:56:25,580
its mean square error or the average

01:56:23,600 --> 01:56:29,030
mean square error and the accuracy for

01:56:25,580 --> 01:56:30,860
the training cycle for each round and if

01:56:29,030 --> 01:56:32,990
you actually compare you can see that on

01:56:30,860 --> 01:56:35,720
average these values keep decreasing and

01:56:32,990 --> 01:56:38,210
this shows that a model is actually

01:56:35,720 --> 01:56:40,370
learning because the lower the lost

01:56:38,210 --> 01:56:42,500
value it shows that the models learn

01:56:40,370 --> 01:56:44,630
better because the predictions are less

01:56:42,500 --> 01:56:46,880
off or less different from what it's

01:56:44,630 --> 01:56:49,160
supposed to be and we can see that the

01:56:46,880 --> 01:56:53,330
accuracy is also improving which is a

01:56:49,160 --> 01:56:58,460
good thing and while we actually

01:56:53,330 --> 01:57:00,890
achieved a 98% accuracy on this so when

01:56:58,460 --> 01:57:04,910
you so this shows that the training

01:57:00,890 --> 01:57:07,010
cycle was really good and that the model

01:57:04,910 --> 01:57:08,810
has performed really well so if you

01:57:07,010 --> 01:57:12,260
actually take this model and you were

01:57:08,810 --> 01:57:14,750
able to put this into production and you

01:57:12,260 --> 01:57:17,570
can actually send it so that billions of

01:57:14,750 --> 01:57:19,280
people can use it across the world it

01:57:17,570 --> 01:57:22,430
would be really accurate in its

01:57:19,280 --> 01:57:25,940
predictions which is a good thing next

01:57:22,430 --> 01:57:29,420
we want to see whether the loss values

01:57:25,940 --> 01:57:33,830
are decreasing so so the training

01:57:29,420 --> 01:57:35,660
history variable we can convert this

01:57:33,830 --> 01:57:38,570
training history object into a

01:57:35,660 --> 01:57:41,620
dictionary by simply typing in double

01:57:38,570 --> 01:57:43,790
underscore D ICT a double underscore and

01:57:41,620 --> 01:57:48,230
that basically gives us a dictionary

01:57:43,790 --> 01:57:50,500
form of our training history and then we

01:57:48,230 --> 01:57:55,580
can see this dictionary over here and

01:57:50,500 --> 01:57:57,380
again if we access the history key over

01:57:55,580 --> 01:57:59,030
here so I'm assuming that most of you

01:57:57,380 --> 01:58:01,640
know how to work with dictionaries in

01:57:59,030 --> 01:58:04,730
Python so dictionaries consist of a key

01:58:01,640 --> 01:58:07,250
value pair where each value corresponds

01:58:04,730 --> 01:58:10,160
to a specific key so similarly in this

01:58:07,250 --> 01:58:13,010
object over here in this dictionary

01:58:10,160 --> 01:58:16,820
object we can see that it has different

01:58:13,010 --> 01:58:19,960
things like do validation metrics perms

01:58:16,820 --> 01:58:23,180
which is basically your parameters and

01:58:19,960 --> 01:58:26,360
we'll be accessing the history key with

01:58:23,180 --> 01:58:30,380
it and under that will access the loss

01:58:26,360 --> 01:58:31,970
and if we print it you can actually see

01:58:30,380 --> 01:58:33,460
that the values are decreasing over time

01:58:31,970 --> 01:58:38,620
which show that the mall

01:58:33,460 --> 01:58:41,680
is training really well and if you were

01:58:38,620 --> 01:58:43,570
to do some further analysis on this you

01:58:41,680 --> 01:58:48,760
can actually put this into a variable

01:58:43,570 --> 01:58:54,280
called losses and four epochs or rounds

01:58:48,760 --> 01:58:56,710
you can actually use numpy does this

01:58:54,280 --> 01:59:00,670
function called a range which just

01:58:56,710 --> 01:59:02,800
basically gives you one two three four

01:59:00,670 --> 01:59:04,660
all the way to that number you specify

01:59:02,800 --> 01:59:07,360
so over here we have been training it

01:59:04,660 --> 01:59:15,670
for 50 bucks so we'll just put this as

01:59:07,360 --> 01:59:19,180
50 we'll run this will import matplotlib

01:59:15,670 --> 01:59:21,070
again for some graphing to see if the

01:59:19,180 --> 01:59:23,880
loss is really decreasing over time and

01:59:21,070 --> 01:59:23,880
to see performance

01:59:30,239 --> 01:59:37,090
and if you were to plot the losses

01:59:33,780 --> 01:59:39,300
against sorry the epochs against the

01:59:37,090 --> 01:59:39,300
loss

01:59:49,800 --> 01:59:54,930
you'd actually get this and we can see

01:59:52,500 --> 01:59:56,730
that the loss function all keeps

01:59:54,930 --> 01:59:59,040
decreasing throughout our training cycle

01:59:56,730 --> 02:00:01,110
we don't see any sudden peaks that show

01:59:59,040 --> 02:00:02,940
that the model suddenly messed up in

02:00:01,110 --> 02:00:05,250
that training cycle which was that a

02:00:02,940 --> 02:00:08,460
model is either ready for production or

02:00:05,250 --> 02:00:11,820
is really good but the problems with

02:00:08,460 --> 02:00:15,000
machine learning is that sometimes when

02:00:11,820 --> 02:00:17,400
we have too few of training instances

02:00:15,000 --> 02:00:19,140
here we only had 100 and it's actually

02:00:17,400 --> 02:00:21,900
surprising that we got the accuracy of

02:00:19,140 --> 02:00:23,460
close to 98 because when I tried it

02:00:21,900 --> 02:00:26,880
earlier I got something that was less

02:00:23,460 --> 02:00:28,170
than 50% accurate so we can see that the

02:00:26,880 --> 02:00:31,350
number of training examples you have

02:00:28,170 --> 02:00:33,540
show how well the model is gonna be when

02:00:31,350 --> 02:00:35,490
fully trained because the more number of

02:00:33,540 --> 02:00:38,100
examples you have the better the

02:00:35,490 --> 02:00:39,750
performance which is analogous to doing

02:00:38,100 --> 02:00:41,190
math problem for example you're really

02:00:39,750 --> 02:00:43,320
bad at doing one type of math problems

02:00:41,190 --> 02:00:45,450
the more you solve those kind of

02:00:43,320 --> 02:00:47,490
problems the better you get so that when

02:00:45,450 --> 02:00:48,720
you do the exam you can actually score

02:00:47,490 --> 02:00:50,940
really high marks on those kind of

02:00:48,720 --> 02:00:55,290
problems which shows that you trained

02:00:50,940 --> 02:00:58,440
well on that kind of problem so now that

02:00:55,290 --> 02:01:01,950
we've finished building our single layer

02:00:58,440 --> 02:01:05,310
perceptron that is it for the coding

02:01:01,950 --> 02:01:07,110
part so now it's time for some

02:01:05,310 --> 02:01:09,420
concluding words now that we've finished

02:01:07,110 --> 02:01:11,790
building our linear regression and

02:01:09,420 --> 02:01:14,220
single layer perceptron we can actually

02:01:11,790 --> 02:01:17,400
see that for real-life cases in

02:01:14,220 --> 02:01:19,110
different use cases across different

02:01:17,400 --> 02:01:22,170
industries machine learning is really

02:01:19,110 --> 02:01:23,490
helpful but the thing is currently

02:01:22,170 --> 02:01:26,550
machine learning is at a state where

02:01:23,490 --> 02:01:28,350
it's still inaccessible to many which

02:01:26,550 --> 02:01:30,780
makes me really happy that you know

02:01:28,350 --> 02:01:33,360
youths like from junior college all the

02:01:30,780 --> 02:01:35,640
way to professionals are coming for

02:01:33,360 --> 02:01:39,720
these kind of talks right so shout out

02:01:35,640 --> 02:01:42,570
to all of you because you're you're

02:01:39,720 --> 02:01:45,210
progressing and putting your best foot

02:01:42,570 --> 02:01:47,640
forward so as to learn machine learning

02:01:45,210 --> 02:01:50,160
you can apply it to any data set you

02:01:47,640 --> 02:01:52,290
have any real life problem and with the

02:01:50,160 --> 02:01:55,110
power you now possessed build machine

02:01:52,290 --> 02:01:57,870
learning algorithms you can solve major

02:01:55,110 --> 02:02:00,290
problems that the world is facing you

02:01:57,870 --> 02:02:01,560
can optimize it for anything you want

02:02:00,290 --> 02:02:03,150
but the

02:02:01,560 --> 02:02:04,680
with machine learning is that you know

02:02:03,150 --> 02:02:06,930
as we can see from the linear regression

02:02:04,680 --> 02:02:09,120
example there's lots of math involved

02:02:06,930 --> 02:02:11,790
lots of concepts that are really

02:02:09,120 --> 02:02:14,220
abstract for some to understand which is

02:02:11,790 --> 02:02:19,710
why it takes lots of time and effort to

02:02:14,220 --> 02:02:22,980
learn but this workshop is while an

02:02:19,710 --> 02:02:25,290
attempt to democratize AI education so a

02:02:22,980 --> 02:02:26,940
professor at Stanford University

02:02:25,290 --> 02:02:29,160
professor Andrew young he was from

02:02:26,940 --> 02:02:30,840
raffles institution in Singapore he went

02:02:29,160 --> 02:02:34,320
to Stanford and is now a professor there

02:02:30,840 --> 02:02:37,680
he his big objective of big aim is to

02:02:34,320 --> 02:02:40,020
democratize AI put AI or machine

02:02:37,680 --> 02:02:42,450
learning into the hands of all different

02:02:40,020 --> 02:02:45,210
age groups so that anyone is capable of

02:02:42,450 --> 02:02:48,210
solving world world scale issues and

02:02:45,210 --> 02:02:50,940
problems and this workshop is one of

02:02:48,210 --> 02:02:52,620
many steps taken by governments and

02:02:50,940 --> 02:02:56,970
organizations around the world

02:02:52,620 --> 02:02:58,440
such that anyone can ask be motivated or

02:02:56,970 --> 02:03:00,810
interested in learning machine learning

02:02:58,440 --> 02:03:03,060
and computer science in general because

02:03:00,810 --> 02:03:07,320
you guys are the Wizards of tomorrow and

02:03:03,060 --> 02:03:09,140
the solutions you have they are limited

02:03:07,320 --> 02:03:12,570
by your imagination

02:03:09,140 --> 02:03:15,660
and with that you can thank you for

02:03:12,570 --> 02:03:17,910
attending this workshop if you have any

02:03:15,660 --> 02:03:20,280
problems or any issues or you just want

02:03:17,910 --> 02:03:22,740
to chat about anything related to tech

02:03:20,280 --> 02:03:27,740
machine learning you can you can contact

02:03:22,740 --> 02:03:33,720
me on either github LinkedIn medium or

02:03:27,740 --> 02:03:41,100
Twitter so yeah and then if you're a

02:03:33,720 --> 02:03:43,860
student who takes computing at school I

02:03:41,100 --> 02:03:47,610
mean if you take computing in secondary

02:03:43,860 --> 02:03:49,170
school or for your a-levels and you are

02:03:47,610 --> 02:03:50,640
keen on writing the comb a computer

02:03:49,170 --> 02:03:52,140
science wave or you're in generally

02:03:50,640 --> 02:03:54,420
interested in teaching others how tech

02:03:52,140 --> 02:03:58,110
works and how computer science is able

02:03:54,420 --> 02:04:01,760
to change industries building blocks as

02:03:58,110 --> 02:04:04,590
I mentioned earlier it's while hiring so

02:04:01,760 --> 02:04:06,300
please do contact us if you're

02:04:04,590 --> 02:04:08,100
interested in joining giving these kind

02:04:06,300 --> 02:04:10,260
of talks because not only is it an

02:04:08,100 --> 02:04:12,110
investment in your portfolio you're

02:04:10,260 --> 02:04:14,389
enabling others to learn and in

02:04:12,110 --> 02:04:16,670
moving their lives as well and with that

02:04:14,389 --> 02:04:22,670
yeah thank you all for attending this

02:04:16,670 --> 02:04:24,590
workshop thank you any questions single

02:04:22,670 --> 02:04:26,510
layer perceptron related or linear

02:04:24,590 --> 02:04:32,659
regression related anything in general

02:04:26,510 --> 02:04:34,429
about machine learning hold on let me

02:04:32,659 --> 02:04:44,780
just get the mic to you so that everyone

02:04:34,429 --> 02:04:50,830
can listen I was thinking about chaos

02:04:44,780 --> 02:05:01,400
theory and is there any ways to actually

02:04:50,830 --> 02:05:03,110
go along with that chaos theory yeah so

02:05:01,400 --> 02:05:05,150
the question was okay so there's this

02:05:03,110 --> 02:05:07,000
thing called KL divergence which is

02:05:05,150 --> 02:05:10,760
really complicated machine learning

02:05:07,000 --> 02:05:12,050
terminology KL divergence is one kind of

02:05:10,760 --> 02:05:15,260
problem that really hasn't been

02:05:12,050 --> 02:05:17,570
optimized yet I mean to all of you who

02:05:15,260 --> 02:05:19,580
kind of have a knowledge of machine

02:05:17,570 --> 02:05:21,409
learning you may know that it's an

02:05:19,580 --> 02:05:25,040
unsolved problem is to try to get there

02:05:21,409 --> 02:05:26,780
but as of now the state of computing KL

02:05:25,040 --> 02:05:29,630
divergence hasn't been solved but that's

02:05:26,780 --> 02:05:31,520
a really high-level question you do not

02:05:29,630 --> 02:05:36,710
need to worry about that if you have any

02:05:31,520 --> 02:05:38,330
other questions anyone all right then I

02:05:36,710 --> 02:05:38,960
think we can wrap it up for tonight so

02:05:38,330 --> 02:05:41,119
Friday night

02:05:38,960 --> 02:05:45,560
been a really long day for most of you

02:05:41,119 --> 02:05:47,780
so yeah thank you all for coming I'll

02:05:45,560 --> 02:05:49,940
just be like standing over here so maybe

02:05:47,780 --> 02:05:52,270
too shy to ask you you can like talk to

02:05:49,940 --> 02:05:52,270

YouTube URL: https://www.youtube.com/watch?v=7H7mvU1JLOU


