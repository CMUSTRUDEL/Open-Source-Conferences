Title: AnalyticDB for MySQL - Ang Wei Shan
Publication date: 2020-04-02
Playlist: FOSSASIA Summit 2020 - Database
Description: 
	AnalyticDB for MySQL is a high-performance data warehousing service from Alibaba Cloud. AnalyticDB for MySQL uses a distributed computing architecture that enables it to use the elastic scaling capability of the cloud to compute tens of billions of data records in real time.

In this talk, I will be sharing about AnalyticDB for MySQL, the underlying architecture and how we achieve it.

FOSSASIA Summit 2020 - Database

Speaker: Ang Wei Shan, Database Architect Alibaba Cloud
Captions: 
	00:00:00,000 --> 00:00:11,090
oh this is the first time I'm presenting

00:00:05,940 --> 00:00:13,889
and there's only two people yeah that's

00:00:11,090 --> 00:00:15,809
and so my name is Li Shan I work in

00:00:13,889 --> 00:00:19,199
Alibaba cloud I'm a database

00:00:15,809 --> 00:00:21,480
architecture so the cool thing about a

00:00:19,199 --> 00:00:24,060
Libra what database group is that our

00:00:21,480 --> 00:00:26,609
unit is actually cut across the entire

00:00:24,060 --> 00:00:29,220
Alibaba Group so we don't just support a

00:00:26,609 --> 00:00:30,510
librivox all we support top out came out

00:00:29,220 --> 00:00:32,910
all the different business units we

00:00:30,510 --> 00:00:34,950
didn't Alibaba so we have like the s

00:00:32,910 --> 00:00:36,480
Ari's in our team we have like the

00:00:34,950 --> 00:00:38,160
developers the product managers

00:00:36,480 --> 00:00:40,170
everybody is in the same group cut

00:00:38,160 --> 00:00:44,100
across so this is actually quite unique

00:00:40,170 --> 00:00:46,230
within a live over so five things to

00:00:44,100 --> 00:00:48,239
talk about today introduction database

00:00:46,230 --> 00:00:50,129
overview the first two will be a little

00:00:48,239 --> 00:00:52,949
bit of marketing and then number three

00:00:50,129 --> 00:00:55,739
number four would be technical dip dive

00:00:52,949 --> 00:00:58,379
I don't have the demo today because I

00:00:55,739 --> 00:00:59,820
just burst my benchmark limits yesterday

00:00:58,379 --> 00:01:01,800
night uh-huh

00:00:59,820 --> 00:01:07,470
so I still have some numbers to show

00:01:01,800 --> 00:01:12,479
though should I stand up I shouldn't

00:01:07,470 --> 00:01:15,240
stand out right yeah so the rocket I

00:01:12,479 --> 00:01:18,060
used to be a solution engineer in Oracle

00:01:15,240 --> 00:01:23,909
then before that I was sorry in global

00:01:18,060 --> 00:01:28,049
sign the Alibaba cloud has actually a

00:01:23,909 --> 00:01:30,600
wide variety of databases there is the

00:01:28,049 --> 00:01:34,710
oil TV databases the the the OLAP

00:01:30,600 --> 00:01:37,140
databases no sequel databases one cool

00:01:34,710 --> 00:01:39,780
thing to say is that doing double eleven

00:01:37,140 --> 00:01:41,939
last year actually the entire workload

00:01:39,780 --> 00:01:44,970
was supported on Alibaba cloud we were

00:01:41,939 --> 00:01:49,880
able to process about 500,000 orders per

00:01:44,970 --> 00:01:52,229
second that was quite impressive we have

00:01:49,880 --> 00:01:54,329
different types of databases the

00:01:52,229 --> 00:01:57,630
relational databases the no sequel

00:01:54,329 --> 00:01:59,549
analytic and certain tools so we have

00:01:57,630 --> 00:02:03,240
like for example

00:01:59,549 --> 00:02:05,219
polar DB that is for OLTP workload it's

00:02:03,240 --> 00:02:07,469
a cloud native database we have

00:02:05,219 --> 00:02:09,539
Cassandra we have so we have a

00:02:07,469 --> 00:02:12,480
partnership with MongoDB so

00:02:09,539 --> 00:02:14,489
and currently in MongoDB 4.2 we are the

00:02:12,480 --> 00:02:16,140
only one in the world that has it the

00:02:14,489 --> 00:02:18,599
other core vendors that doesn't have it

00:02:16,140 --> 00:02:19,799
and then we have click house kill house

00:02:18,599 --> 00:02:23,040
is pretty cool is written by the

00:02:19,799 --> 00:02:28,700
Russians it's a super fast OLAP database

00:02:23,040 --> 00:02:33,599
highly compatible with my sequel decided

00:02:28,700 --> 00:02:35,849
all the entire range of services that we

00:02:33,599 --> 00:02:38,670
have we have full our DB we have the

00:02:35,849 --> 00:02:40,680
standard RDS you're all over an analytic

00:02:38,670 --> 00:02:44,370
DVD you can see how all this fits into

00:02:40,680 --> 00:02:46,560
it one cool thing I'll mention is desk

00:02:44,370 --> 00:02:48,780
database autonomous service so this is

00:02:46,560 --> 00:02:51,599
like a service that can monitor your

00:02:48,780 --> 00:02:53,909
databases for example if it when our

00:02:51,599 --> 00:02:57,180
resources you can do auto scaling for

00:02:53,909 --> 00:03:00,120
you for if it can do smart index

00:02:57,180 --> 00:03:02,189
recommendations you can also do things

00:03:00,120 --> 00:03:03,689
like it basically the idea is to tell

00:03:02,189 --> 00:03:07,859
you that database is gonna fail before

00:03:03,689 --> 00:03:09,870
you even know that it so of course we

00:03:07,859 --> 00:03:14,099
make a lot of contribution to the my

00:03:09,870 --> 00:03:16,680
sequel community as well there's a lot

00:03:14,099 --> 00:03:18,840
of enhancement so we have our own

00:03:16,680 --> 00:03:22,879
version of a li sequel so there is a

00:03:18,840 --> 00:03:25,979
combination of all the different

00:03:22,879 --> 00:03:28,669
upstream contribution and the

00:03:25,979 --> 00:03:31,079
contribution from my picanha and others

00:03:28,669 --> 00:03:34,290
made into a me contribute back to the

00:03:31,079 --> 00:03:35,669
upstream we have a service called a

00:03:34,290 --> 00:03:39,449
distributed relational database service

00:03:35,669 --> 00:03:43,109
so this is like a database proxy that

00:03:39,449 --> 00:03:45,209
allows you to do sharding natively so

00:03:43,109 --> 00:03:47,519
you just configure the proxy will be ill

00:03:45,209 --> 00:03:49,319
will know where are all the different

00:03:47,519 --> 00:03:51,120
shots and the data sort so it just rots

00:03:49,319 --> 00:03:56,159
- it automatically so we were often

00:03:51,120 --> 00:04:00,900
source this this year as well white

00:03:56,159 --> 00:04:03,620
papers last last few years we have

00:04:00,900 --> 00:04:06,090
started to release a lot of white papers

00:04:03,620 --> 00:04:08,759
talking about a design philosophy that

00:04:06,090 --> 00:04:12,000
we did why we did how we did it one

00:04:08,759 --> 00:04:15,949
other thing that is really cool is X

00:04:12,000 --> 00:04:18,650
engine so we can think of X engine as

00:04:15,949 --> 00:04:21,779
replacement for I know DB in my sequel

00:04:18,650 --> 00:04:22,850
the prison why we design X engine is

00:04:21,779 --> 00:04:25,490
because for

00:04:22,850 --> 00:04:27,110
Camas look looks supported by Taobao we

00:04:25,490 --> 00:04:30,230
have a very unique workload because

00:04:27,110 --> 00:04:32,090
sometimes hot data cache hot data close

00:04:30,230 --> 00:04:33,680
I recorded I might become Hadid I really

00:04:32,090 --> 00:04:35,570
quickly due to promotions and all that

00:04:33,680 --> 00:04:36,740
and we realized that I know DV couldn't

00:04:35,570 --> 00:04:38,480
couldn't handle the kind of workload

00:04:36,740 --> 00:04:42,500
they're switching from cold to hot

00:04:38,480 --> 00:04:44,420
Samad us instantaneously so we had to

00:04:42,500 --> 00:04:46,700
come out of our own ex engine storage

00:04:44,420 --> 00:04:48,830
engine to do it the other one that I'll

00:04:46,700 --> 00:04:51,170
be talking about is analytic DB today

00:04:48,830 --> 00:04:53,570
analytic TV is our own cloud native

00:04:51,170 --> 00:04:54,710
design OLAP database so today I'll be

00:04:53,570 --> 00:05:02,150
talking about it we release a white

00:04:54,710 --> 00:05:03,680
paper on it as well that's all for the

00:05:02,150 --> 00:05:07,400
marketing now is gonna go to the deep

00:05:03,680 --> 00:05:09,140
dive technical stuff so and I'll take DB

00:05:07,400 --> 00:05:14,450
for my sequel has a quite a long history

00:05:09,140 --> 00:05:17,630
you started off back in 2011 it was it

00:05:14,450 --> 00:05:21,050
was for it was designed for

00:05:17,630 --> 00:05:22,610
it was called Garuda in 2011 and after

00:05:21,050 --> 00:05:25,880
that we realized that is not good enough

00:05:22,610 --> 00:05:28,460
then we release a new version 2014

00:05:25,880 --> 00:05:32,560
called 80s and it's on Alibaba cloud

00:05:28,460 --> 00:05:38,110
after that we release 2016 and today is

00:05:32,560 --> 00:05:44,300
is an Ltd be 3.0 its water %

00:05:38,110 --> 00:05:46,520
compatible with sequel 2003 standards we

00:05:44,300 --> 00:05:49,930
support it we use it during double 11 to

00:05:46,520 --> 00:05:53,600
support the real-time analytics for for

00:05:49,930 --> 00:05:59,450
internal business units there are some

00:05:53,600 --> 00:06:02,960
of the features of analytic DB one of

00:05:59,450 --> 00:06:04,880
the cooler thing is you realize is in

00:06:02,960 --> 00:06:07,160
traditional relational databases most of

00:06:04,880 --> 00:06:09,890
the most data historian rows format and

00:06:07,160 --> 00:06:12,650
it's very slow when you do things like

00:06:09,890 --> 00:06:16,010
some more aggregation that is why

00:06:12,650 --> 00:06:19,550
columnar theory started things like SAV

00:06:16,010 --> 00:06:22,730
HANA they start using opposing in-memory

00:06:19,550 --> 00:06:24,650
columnar saw so the problem is those

00:06:22,730 --> 00:06:27,230
kind of databases are weak in OLTP

00:06:24,650 --> 00:06:30,440
workload so we actually have a design

00:06:27,230 --> 00:06:33,260
that is row column hybrid that allows

00:06:30,440 --> 00:06:34,849
you to do quick point lookups and also

00:06:33,260 --> 00:06:37,249
allows you to do

00:06:34,849 --> 00:06:41,419
paalam the salma aggregation really

00:06:37,249 --> 00:06:45,709
quickly with very little overhead column

00:06:41,419 --> 00:06:48,949
joins rule inquiries so we uses direct

00:06:45,709 --> 00:06:51,499
basic cyclic graph as well so think of

00:06:48,949 --> 00:06:54,919
it as like a MapReduce kind of concept

00:06:51,499 --> 00:06:56,360
it's a stage by stage pipelining so in

00:06:54,919 --> 00:06:59,119
my readers you have like a lot of maps

00:06:56,360 --> 00:07:01,459
and after that you reduce it so for da

00:06:59,119 --> 00:07:03,409
GE execution engine is the same we split

00:07:01,459 --> 00:07:04,939
the workload into different nodes then

00:07:03,409 --> 00:07:06,529
after that we run in stage by station

00:07:04,939 --> 00:07:10,159
and finally we consolidate a resource

00:07:06,529 --> 00:07:15,939
Pakenham master node so I start to go

00:07:10,159 --> 00:07:22,809
into the nitty-gritty details there's a

00:07:15,939 --> 00:07:26,809
architecture diagram of analytic DB the

00:07:22,809 --> 00:07:31,399
thing that you want to see is in the

00:07:26,809 --> 00:07:33,740
storage layer there will be a single

00:07:31,399 --> 00:07:36,169
leader and two followers GM's is

00:07:33,740 --> 00:07:40,999
transaction must transaction management

00:07:36,169 --> 00:07:42,499
kg is the direct basic acyclic graph wim

00:07:40,999 --> 00:07:45,800
is the wiggler management so you have a

00:07:42,499 --> 00:07:48,740
leader followers on the storage portion

00:07:45,800 --> 00:07:50,479
you have two workers workers for the for

00:07:48,740 --> 00:07:54,649
workers with leaders and followers as

00:07:50,479 --> 00:07:59,059
well and all these are all sing through

00:07:54,649 --> 00:08:03,169
the rough consensus on the resource

00:07:59,059 --> 00:08:05,719
portion on the passions in the next few

00:08:03,169 --> 00:08:08,329
months we will have will be able to do

00:08:05,719 --> 00:08:11,719
kind of like a hybrid partitioning table

00:08:08,329 --> 00:08:14,659
so you can have all hot data on nvme SSD

00:08:11,719 --> 00:08:17,089
and your cold data on OSS so this

00:08:14,659 --> 00:08:19,879
reduces store the storage cost but quite

00:08:17,089 --> 00:08:22,099
a bit considering when you have more

00:08:19,879 --> 00:08:27,499
than 20 terabytes hundred terabytes of

00:08:22,099 --> 00:08:30,110
data oh the other thing I want to

00:08:27,499 --> 00:08:32,449
mention is optimizer so we support rule

00:08:30,110 --> 00:08:33,339
base and cost-based optimizer you might

00:08:32,449 --> 00:08:37,880
be wondering

00:08:33,339 --> 00:08:40,130
rubezh is like the designer was twenty

00:08:37,880 --> 00:08:42,189
years ago but why to use it I'll explain

00:08:40,130 --> 00:08:42,189
later

00:08:43,050 --> 00:08:49,529
storage engine architecture so this

00:08:45,089 --> 00:08:52,050
storage engine worry ourselves to things

00:08:49,529 --> 00:08:56,459
on a mention here predicate query Lane

00:08:52,050 --> 00:08:58,589
lately oh sorry this allows you to do

00:08:56,459 --> 00:09:01,800
predicate filtering at a storage layer

00:08:58,589 --> 00:09:03,300
site so instead of say if you have a

00:09:01,800 --> 00:09:04,620
where clause instead of sending the

00:09:03,300 --> 00:09:07,290
where Clause to the storage and getting

00:09:04,620 --> 00:09:09,240
all the data back is actually able to do

00:09:07,290 --> 00:09:12,269
the predicate push down at a storage

00:09:09,240 --> 00:09:15,060
layer so if you do a select where name

00:09:12,269 --> 00:09:16,980
it goes to a shine the data that is

00:09:15,060 --> 00:09:19,740
returned from the storage back to the

00:09:16,980 --> 00:09:21,120
compute node is only where conservation

00:09:19,740 --> 00:09:24,300
and then you do the joint and all that

00:09:21,120 --> 00:09:27,540
so as you can imagine this allows you to

00:09:24,300 --> 00:09:29,670
reduce significant significantly the

00:09:27,540 --> 00:09:32,910
amount of data sent between the storage

00:09:29,670 --> 00:09:36,149
and the computer itself yeah this is

00:09:32,910 --> 00:09:40,350
this is really really cool transaction

00:09:36,149 --> 00:09:44,550
manager so we have two phase commit MVCC

00:09:40,350 --> 00:09:47,600
for snapshots isolation we have

00:09:44,550 --> 00:09:50,579
consensus our manager RAF manager that

00:09:47,600 --> 00:09:52,649
apply the consistency across different

00:09:50,579 --> 00:09:55,890
notes if you need consistency if you

00:09:52,649 --> 00:09:58,560
need distributed transactions caches we

00:09:55,890 --> 00:10:05,430
cache the blocks index partition primary

00:09:58,560 --> 00:10:07,709
key indexes and testing here the gray

00:10:05,430 --> 00:10:09,810
color thing is actually cool in a sense

00:10:07,709 --> 00:10:12,360
where the storage portion is pluggable

00:10:09,810 --> 00:10:14,070
so you can point it to object storage

00:10:12,360 --> 00:10:18,959
you can point into HDFS you can point it

00:10:14,070 --> 00:10:26,610
to whatever file system you like if you

00:10:18,959 --> 00:10:28,260
want to so they are modular the next I

00:10:26,610 --> 00:10:30,779
want to talk about some of the

00:10:28,260 --> 00:10:33,140
optimization and engineering stuff that

00:10:30,779 --> 00:10:33,140
we did

00:10:35,330 --> 00:10:40,010
in your traditional OLAP databases

00:10:37,750 --> 00:10:44,030
teradata and all those they actually

00:10:40,010 --> 00:10:45,650
have the read request and the write

00:10:44,030 --> 00:10:47,960
request all serve within the note so you

00:10:45,650 --> 00:10:49,850
start up a target our service and then

00:10:47,960 --> 00:10:52,010
all the note or all the request is being

00:10:49,850 --> 00:10:53,930
sent to it so this means that when

00:10:52,010 --> 00:10:56,930
you're doing for example bout loading or

00:10:53,930 --> 00:10:59,300
real-time streaming and night or

00:10:56,930 --> 00:11:02,300
whatever you affect your read your

00:10:59,300 --> 00:11:04,010
reporting a lot basically we will have

00:11:02,300 --> 00:11:05,540
to do the ball loading at night and into

00:11:04,010 --> 00:11:07,310
the reporting in the morning this is

00:11:05,540 --> 00:11:09,110
like really old-school traditional stuff

00:11:07,310 --> 00:11:13,220
right so what we do here is we actually

00:11:09,110 --> 00:11:14,930
split out the read notes and a right now

00:11:13,220 --> 00:11:17,570
so the ripoff and the right path is

00:11:14,930 --> 00:11:19,370
actually processed separately so they

00:11:17,570 --> 00:11:23,330
never they never have to compete for

00:11:19,370 --> 00:11:26,960
resources and the data are all returning

00:11:23,330 --> 00:11:29,570
to locks commit locks and imagine you

00:11:26,960 --> 00:11:32,030
can think of it like Cassandra SST

00:11:29,570 --> 00:11:34,250
tables they are all returned to that

00:11:32,030 --> 00:11:35,990
says the mem table and to the SSD table

00:11:34,250 --> 00:11:38,480
and we didn't as T table they do a

00:11:35,990 --> 00:11:41,000
compaction so this is the concept is

00:11:38,480 --> 00:11:42,890
similar but what we do for a compaction

00:11:41,000 --> 00:11:49,460
is we use parallel might reduce jobs to

00:11:42,890 --> 00:11:51,860
do it does really really quickly in the

00:11:49,460 --> 00:11:54,430
diagram on the bottom right you can see

00:11:51,860 --> 00:11:58,790
that when a client do a select query

00:11:54,430 --> 00:12:00,740
hits one sorry do a regular Korean

00:11:58,790 --> 00:12:02,930
erases to the right note and if you want

00:12:00,740 --> 00:12:04,970
to if it goes to a read note to do this

00:12:02,930 --> 00:12:07,370
select so he wants to read what is just

00:12:04,970 --> 00:12:09,770
right so there is a way to do it you can

00:12:07,370 --> 00:12:11,750
choose whether you want to pull the Reno

00:12:09,770 --> 00:12:15,200
will pull the data from the right note

00:12:11,750 --> 00:12:17,870
if you need like guaranteed session

00:12:15,200 --> 00:12:19,910
consistency or if you don't really care

00:12:17,870 --> 00:12:21,050
you can do a bounded stillness read and

00:12:19,910 --> 00:12:22,640
then just really from the distributed

00:12:21,050 --> 00:12:25,190
file system below so it's really up to

00:12:22,640 --> 00:12:27,740
you so some job some some queries they

00:12:25,190 --> 00:12:28,940
require real-time consistency so they

00:12:27,740 --> 00:12:30,500
have to read it from the right now but

00:12:28,940 --> 00:12:34,130
someday don't care they just do it from

00:12:30,500 --> 00:12:36,200
the from the file system itself and you

00:12:34,130 --> 00:12:37,730
can add those read notes like the

00:12:36,200 --> 00:12:39,940
partition will just auto balance by

00:12:37,730 --> 00:12:39,940
itself

00:12:43,130 --> 00:12:50,970
one of the thing that a lot of data

00:12:48,660 --> 00:12:53,640
warehouse designer did get a lot here it

00:12:50,970 --> 00:12:56,340
is where to put the indexes that it

00:12:53,640 --> 00:12:58,500
because indexes are not shipped for

00:12:56,340 --> 00:13:00,060
writes so whenever you put like indexes

00:12:58,500 --> 00:13:01,260
it affects all the writes so do

00:13:00,060 --> 00:13:02,790
everything like oh I have to put it on a

00:13:01,260 --> 00:13:04,560
primary key column I have to put it on

00:13:02,790 --> 00:13:06,690
this column that column and it's

00:13:04,560 --> 00:13:08,880
problematic right every time a new all

00:13:06,690 --> 00:13:10,980
our queries turns up and then it will

00:13:08,880 --> 00:13:14,000
just blows out database because there's

00:13:10,980 --> 00:13:17,040
no index and you do a full table scan

00:13:14,000 --> 00:13:19,110
one thing with it is indexes on every

00:13:17,040 --> 00:13:21,180
single column so we don't care we index

00:13:19,110 --> 00:13:23,670
every column so whatever the cou you hit

00:13:21,180 --> 00:13:25,680
at and now to give you a DB it doesn't

00:13:23,670 --> 00:13:28,020
matter so you can just throw anything at

00:13:25,680 --> 00:13:29,640
it so you don't have to like thing of

00:13:28,020 --> 00:13:30,810
the indexers indexes and then you run a

00:13:29,640 --> 00:13:33,780
query you just throw everything at it

00:13:30,810 --> 00:13:36,900
it's fine and we have so for example on

00:13:33,780 --> 00:13:42,150
the left you have a select query that

00:13:36,900 --> 00:13:46,200
uses the name the sex the city and some

00:13:42,150 --> 00:13:47,790
JSON data type so we we have inverted

00:13:46,200 --> 00:13:50,160
index by main index and all this

00:13:47,790 --> 00:13:54,500
filtering can be done at index layer

00:13:50,160 --> 00:13:57,360
itself so if you are doing slightly more

00:13:54,500 --> 00:14:00,000
selective query you don't even need to

00:13:57,360 --> 00:14:01,650
hit your tail up Falls at all everything

00:14:00,000 --> 00:14:04,020
can be done at index level the filtering

00:14:01,650 --> 00:14:06,420
is all done in it the filtering is all

00:14:04,020 --> 00:14:10,560
done an index level this is really

00:14:06,420 --> 00:14:17,070
really cool I don't think a lot of other

00:14:10,560 --> 00:14:19,769
databases to this Josiah manage

00:14:17,070 --> 00:14:25,470
key engineering feature was the hybrid

00:14:19,769 --> 00:14:28,339
row and column storage so we have the

00:14:25,470 --> 00:14:32,279
detail file and a metadata detail foul

00:14:28,339 --> 00:14:33,899
thing over so this detail metadata file

00:14:32,279 --> 00:14:35,339
start a memory

00:14:33,899 --> 00:14:37,319
there's always cache in memory or Ram

00:14:35,339 --> 00:14:39,899
and install things like statistics

00:14:37,319 --> 00:14:41,759
dictionary did he mean seismic size

00:14:39,899 --> 00:14:46,250
distinct on and all these will help to

00:14:41,759 --> 00:14:48,269
do column pruning during the query time

00:14:46,250 --> 00:14:49,649
so based on all these statistic

00:14:48,269 --> 00:14:52,440
information didn't know what should call

00:14:49,649 --> 00:14:54,389
them to prune and so in our design

00:14:52,440 --> 00:14:55,949
itself the data in each table partition

00:14:54,389 --> 00:14:58,319
is maintained in a single file called a

00:14:55,949 --> 00:15:00,769
detail file and they're all separated in

00:14:58,319 --> 00:15:05,100
the multiple row groups we fix eyes

00:15:00,769 --> 00:15:07,920
within a roll group itself the values on

00:15:05,100 --> 00:15:10,560
the same columns are start together in

00:15:07,920 --> 00:15:13,019
data blocks next to each other so even

00:15:10,560 --> 00:15:14,579
if the columns when you need to worry

00:15:13,019 --> 00:15:16,319
all the columns they are star and

00:15:14,579 --> 00:15:18,990
different data blocks but they all

00:15:16,319 --> 00:15:20,579
besides each other so a discipline

00:15:18,990 --> 00:15:22,519
itself can pick up all the data by

00:15:20,579 --> 00:15:24,810
itself so you don't need random access

00:15:22,519 --> 00:15:28,529
it's not a random access I will pattern

00:15:24,810 --> 00:15:31,100
anymore it's it's a it's it's not random

00:15:28,529 --> 00:15:35,040
anymore so this is like quite fast

00:15:31,100 --> 00:15:38,699
sequential I will pattern and you have

00:15:35,040 --> 00:15:41,100
caught if you have complex data type so

00:15:38,699 --> 00:15:42,360
on the left you have all the data block

00:15:41,100 --> 00:15:44,190
for the column right order heater

00:15:42,360 --> 00:15:47,579
they're all fixed size and if you have

00:15:44,190 --> 00:15:49,470
things like JSON or vectors sometimes

00:15:47,579 --> 00:15:53,130
they are unpredictable in size it could

00:15:49,470 --> 00:15:55,380
be one Mac 10 Mac 16 man so if you try

00:15:53,130 --> 00:15:57,269
to accommodate them in the same file

00:15:55,380 --> 00:15:59,190
itself you're gonna have problems

00:15:57,269 --> 00:16:02,279
because sometimes when you read a block

00:15:59,190 --> 00:16:04,199
its it for kak and so now it's like 16

00:16:02,279 --> 00:16:09,000
max so it doesn't make sense right so we

00:16:04,199 --> 00:16:12,990
have can't design very very similar to

00:16:09,000 --> 00:16:16,680
Postgres Co stable so if you have JSON

00:16:12,990 --> 00:16:18,959
or complex data type what the main file

00:16:16,680 --> 00:16:23,970
actually stores is just a pointer to a

00:16:18,959 --> 00:16:25,410
dish to an additional file so

00:16:23,970 --> 00:16:27,449
starring it all in the sim that I've

00:16:25,410 --> 00:16:29,519
enough out there blocks it's just a

00:16:27,449 --> 00:16:38,399
pointer to an external vol so this is

00:16:29,519 --> 00:16:41,879
really quick right some people might

00:16:38,399 --> 00:16:43,970
think if you index every column when you

00:16:41,879 --> 00:16:47,189
do the writes is gonna be very very slow

00:16:43,970 --> 00:16:49,920
so how do we make sure that the indexes

00:16:47,189 --> 00:16:53,750
doesn't conflict with the right

00:16:49,920 --> 00:16:57,240
performance one of the design thing

00:16:53,750 --> 00:16:59,399
design breakthrough that we did was we

00:16:57,240 --> 00:17:06,059
have a concept of the baseline data and

00:16:59,399 --> 00:17:08,280
the incremental data so we have a tape

00:17:06,059 --> 00:17:10,970
we have a main main main index or min

00:17:08,280 --> 00:17:15,209
therefore and all rights rise to it

00:17:10,970 --> 00:17:19,189
maybe I just go through this so there is

00:17:15,209 --> 00:17:19,189
incremental data and baseline data

00:17:21,820 --> 00:17:29,790
is when we will try to incremental data

00:17:24,400 --> 00:17:29,790
emerges back to the mean main data file

00:17:30,180 --> 00:17:40,210
how it is this first on incremental data

00:17:35,590 --> 00:17:42,690
site we make immutable at the same time

00:17:40,210 --> 00:17:45,580
in an atomic session we create a second

00:17:42,690 --> 00:17:52,120
increment of the earth all that accepts

00:17:45,580 --> 00:17:53,860
all incoming requests for this the Fed

00:17:52,120 --> 00:17:56,800
merging of the baseline data and

00:17:53,860 --> 00:18:00,460
incremental still data is completed all

00:17:56,800 --> 00:18:02,830
true is surf incoming requests and once

00:18:00,460 --> 00:18:07,210
the merges the merch is completed the

00:18:02,830 --> 00:18:10,710
old arrives remove and is being served

00:18:07,210 --> 00:18:10,710
by the baseline data no incremental data

00:18:12,870 --> 00:18:21,430
everyone's optimizer so

00:18:19,190 --> 00:18:25,790
example and Oracle they used to support

00:18:21,430 --> 00:18:27,650
our Bo rubies optimizer until seven IRA

00:18:25,790 --> 00:18:29,600
I and then they start using crossways

00:18:27,650 --> 00:18:31,820
because they think that class base is

00:18:29,600 --> 00:18:34,250
much more powerful much more advanced

00:18:31,820 --> 00:18:37,750
which is true Andrew base has no place

00:18:34,250 --> 00:18:37,750
in the world more but we think otherwise

00:18:37,960 --> 00:18:47,000
in a LTP sorry you know all that kind of

00:18:42,530 --> 00:18:49,970
database right the Sun sometimes you

00:18:47,000 --> 00:18:51,890
don't really need to do you didn't need

00:18:49,970 --> 00:18:53,870
to do planning for example if you want

00:18:51,890 --> 00:18:55,220
to do point lookup you know that you're

00:18:53,870 --> 00:18:58,190
gonna use the index you didn't need to

00:18:55,220 --> 00:18:59,930
plan at all so perhaps this better to

00:18:58,190 --> 00:19:02,120
actually just put in the rule itself and

00:18:59,930 --> 00:19:03,590
say that of a point lookup so this kind

00:19:02,120 --> 00:19:07,280
of queries don't do even do planning

00:19:03,590 --> 00:19:10,790
just do this way so in in in in our

00:19:07,280 --> 00:19:13,760
scenario because we have so much users

00:19:10,790 --> 00:19:15,440
crewing our ATV all the time it doesn't

00:19:13,760 --> 00:19:16,880
make sense to do that two ways that kind

00:19:15,440 --> 00:19:19,790
of CPU cycles so that kind of useless

00:19:16,880 --> 00:19:21,350
queries like like point lookup so we

00:19:19,790 --> 00:19:24,500
just tell the rules and say every time

00:19:21,350 --> 00:19:26,480
we see this kind of request with indexes

00:19:24,500 --> 00:19:28,250
on it just do a point for point lookout

00:19:26,480 --> 00:19:33,860
for you just just use this index

00:19:28,250 --> 00:19:37,160
straightaway and approve join order

00:19:33,860 --> 00:19:39,560
optimization so one time will DIF we

00:19:37,160 --> 00:19:42,230
will collect all the rows and then if

00:19:39,560 --> 00:19:44,480
based on the predicate based on the rows

00:19:42,230 --> 00:19:45,740
returned at runtime will decide wherever

00:19:44,480 --> 00:19:47,780
you want to use this table will join

00:19:45,740 --> 00:19:49,970
this table or a a to join be a be to

00:19:47,780 --> 00:19:54,500
join a so this is quite standard

00:19:49,970 --> 00:19:56,960
optimization parallel queries my secret

00:19:54,500 --> 00:20:01,040
it has it I think yeah so we have this

00:19:56,960 --> 00:20:03,050
as well partitioning tuning so if you

00:20:01,040 --> 00:20:05,630
Korea where cross and aware causes in a

00:20:03,050 --> 00:20:07,490
for example query by date and this date

00:20:05,630 --> 00:20:09,110
is in two partition so we just read

00:20:07,490 --> 00:20:11,180
these two partitions and the rest is

00:20:09,110 --> 00:20:16,100
bringing no so this is very standard

00:20:11,180 --> 00:20:21,740
optimization as well one really cool

00:20:16,100 --> 00:20:23,990
thing is for mr. basis optimizer our our

00:20:21,740 --> 00:20:26,420
execution engine is all storage aware

00:20:23,990 --> 00:20:28,190
because it's all cognitive right so we

00:20:26,420 --> 00:20:32,090
all know we actually put in some

00:20:28,190 --> 00:20:34,820
smartness and storage layers of so

00:20:32,090 --> 00:20:37,250
seit early on the predicate push down

00:20:34,820 --> 00:20:40,370
join push down index and joins based on

00:20:37,250 --> 00:20:42,080
at the storage site so that the

00:20:40,370 --> 00:20:43,910
optimizer doesn't even need to do all

00:20:42,080 --> 00:20:46,190
this when he sends the query to the

00:20:43,910 --> 00:20:49,820
storage site the stars would know and do

00:20:46,190 --> 00:20:53,030
the filtering and push myself so this is

00:20:49,820 --> 00:20:54,380
I think there's only one come there's

00:20:53,030 --> 00:20:56,540
only one day the reason we're all doing

00:20:54,380 --> 00:20:59,030
this right now I think it's exadata

00:20:56,540 --> 00:21:02,660
Oracle Exadata they have a it's like a

00:20:59,030 --> 00:21:05,380
single wreck machine that the storage is

00:21:02,660 --> 00:21:07,940
like really expensive and it's got

00:21:05,380 --> 00:21:14,030
smartness Burien likes my engine buildin

00:21:07,940 --> 00:21:20,180
that is very expensive then I'll talk a

00:21:14,030 --> 00:21:23,360
little bit about the execution engine we

00:21:20,180 --> 00:21:26,360
have reuse code Chen very similar to

00:21:23,360 --> 00:21:30,200
LLVM so it's compiled runtime as I am

00:21:26,360 --> 00:21:35,480
the CPU cache friendly one thing I wanna

00:21:30,200 --> 00:21:38,900
touch on is the DHD engine daj engine

00:21:35,480 --> 00:21:42,170
was used first in MapReduce and after

00:21:38,900 --> 00:21:44,960
that by spa spa rdd's so basically it's

00:21:42,170 --> 00:21:47,720
a stage by stage pipelining but a

00:21:44,960 --> 00:21:50,000
problem with met reducers whenever you

00:21:47,720 --> 00:21:51,740
do the multiple Maps right initial hit

00:21:50,000 --> 00:21:53,570
status you need to start the Buriti down

00:21:51,740 --> 00:21:54,710
at this and after that at a reduced site

00:21:53,570 --> 00:21:56,480
you need to fetch you from the disk and

00:21:54,710 --> 00:22:01,250
then do the processing again so you can

00:21:56,480 --> 00:22:03,200
have multiple stages in spark oh sorry

00:22:01,250 --> 00:22:05,420
in my reduce and if one is filled then

00:22:03,200 --> 00:22:07,400
you need to do a retry all right you

00:22:05,420 --> 00:22:09,680
need to put in a retry logic so in spa

00:22:07,400 --> 00:22:11,210
RTD is the same but Spock is more

00:22:09,680 --> 00:22:14,540
powerful because you can have unlimited

00:22:11,210 --> 00:22:16,490
amount of stages and then we can do like

00:22:14,540 --> 00:22:18,230
whatever you want and all the way to the

00:22:16,490 --> 00:22:19,910
end and then you do the calculation

00:22:18,230 --> 00:22:21,320
itself and all this can be done in

00:22:19,910 --> 00:22:25,820
memory that's why Spock is really quick

00:22:21,320 --> 00:22:28,280
so never do spa all different

00:22:25,820 --> 00:22:30,860
implementations of TS engine and we uses

00:22:28,280 --> 00:22:34,010
Gaugin and Gina so the reason why I use

00:22:30,860 --> 00:22:35,810
it is because in essence and after

00:22:34,010 --> 00:22:38,270
giving for my sequel is actually MPP

00:22:35,810 --> 00:22:40,100
year based multiple notes the data is

00:22:38,270 --> 00:22:43,150
all fragments all over the place so you

00:22:40,100 --> 00:22:45,380
need some way to so generate a

00:22:43,150 --> 00:22:48,490
optimizers how you generated execution

00:22:45,380 --> 00:22:51,200
and this one is discipline is to be

00:22:48,490 --> 00:22:54,800
carve out and distributed to all the

00:22:51,200 --> 00:22:56,150
different worker nodes and so you were

00:22:54,800 --> 00:22:59,690
neither engineer do all the stage by

00:22:56,150 --> 00:23:01,760
stage calculation and then the results

00:22:59,690 --> 00:23:09,820
returned back to the coordinator itself

00:23:01,760 --> 00:23:09,820
a benchmarks

00:23:10,490 --> 00:23:15,090
so this is one of the benchmarks in our

00:23:13,020 --> 00:23:17,310
white paper all this can be found on

00:23:15,090 --> 00:23:20,310
github how we do the tests and all this

00:23:17,310 --> 00:23:25,740
so this can be reproduced on your own if

00:23:20,310 --> 00:23:28,890
you have dumps on it so when you on Ali

00:23:25,740 --> 00:23:30,390
walks off and so when you buy a ADB

00:23:28,890 --> 00:23:33,420
instance you don't get it instance you

00:23:30,390 --> 00:23:34,860
get like clusters or instance this we

00:23:33,420 --> 00:23:36,240
call it no groups you can have like two

00:23:34,860 --> 00:23:39,090
no groups for no group six no groups

00:23:36,240 --> 00:23:41,940
when you buy two no groups you get read

00:23:39,090 --> 00:23:51,570
to no groups one will be the new groups

00:23:41,940 --> 00:23:53,490
and one will be the right no groups so

00:23:51,570 --> 00:23:54,930
we ran for the white paper so we're

00:23:53,490 --> 00:23:56,940
entering into Korea a full scan

00:23:54,930 --> 00:23:59,910
multi-table join in point loca so these

00:23:56,940 --> 00:24:04,860
are very common queries that data

00:23:59,910 --> 00:24:08,340
warehouse were uses it's hard to see

00:24:04,860 --> 00:24:10,970
from there but okay basically trying to

00:24:08,340 --> 00:24:15,110
say that a TV performance is awesome

00:24:10,970 --> 00:24:20,030
look at the white paper and check it out

00:24:15,110 --> 00:24:20,030
this was a recent benchmark that we did

00:24:20,150 --> 00:24:29,280
the it's quite hard to tell so those

00:24:25,440 --> 00:24:32,100
four things know some of the query run

00:24:29,280 --> 00:24:34,470
time we actually cut off at 250 seconds

00:24:32,100 --> 00:24:38,790
because in one of the query I think it's

00:24:34,470 --> 00:24:40,920
query to know who won my sequel actually

00:24:38,790 --> 00:24:43,500
took more than 10,000 seconds to run the

00:24:40,920 --> 00:24:46,740
query so we had to cut off at to 250

00:24:43,500 --> 00:24:48,780
seconds we test this we tested this

00:24:46,740 --> 00:24:52,440
against my sequel crystal spa and in

00:24:48,780 --> 00:24:54,480
power we had to remove clarify at 9:18

00:24:52,440 --> 00:24:58,500
because it hit women

00:24:54,480 --> 00:25:01,770
presto itself you can see that most of

00:24:58,500 --> 00:25:04,260
the comparison itself it's actually a

00:25:01,770 --> 00:25:10,080
lot faster I'm talking Allen magnitudes

00:25:04,260 --> 00:25:12,870
faster of course this is not 100% for

00:25:10,080 --> 00:25:16,380
presses and for for comparison against

00:25:12,870 --> 00:25:18,690
my sequel is not very fair the reason is

00:25:16,380 --> 00:25:21,030
because when you buy a TV instance you

00:25:18,690 --> 00:25:24,060
get like two clusters where

00:25:21,030 --> 00:25:28,610
in the my security so we actually create

00:25:24,060 --> 00:25:31,410
a tick box that we added up all the

00:25:28,610 --> 00:25:33,590
instant CPA and put in a single box and

00:25:31,410 --> 00:25:36,510
then do the comparison so it's not like

00:25:33,590 --> 00:25:39,420
100 is not a full Apple to Apple but for

00:25:36,510 --> 00:25:41,310
spark presto empires Apple to Apple so

00:25:39,420 --> 00:25:43,380
we when I really do a benchmark we

00:25:41,310 --> 00:25:44,880
always listed on and get up and the

00:25:43,380 --> 00:25:49,880
script so we can reproduce it if you

00:25:44,880 --> 00:25:53,520
like yeah so I was trying to do bench

00:25:49,880 --> 00:25:56,250
demo yesterday but I finish up my work

00:25:53,520 --> 00:25:59,700
loud internal test account so I

00:25:56,250 --> 00:26:01,890
basically screw up my test account by

00:25:59,700 --> 00:26:08,460
using up all the credits so I don't have

00:26:01,890 --> 00:26:11,190
a demo today but if you like let me know

00:26:08,460 --> 00:26:14,640
I can like give me address I can get you

00:26:11,190 --> 00:26:26,660
a test account and test it all yeah

00:26:14,640 --> 00:26:26,660

YouTube URL: https://www.youtube.com/watch?v=QV0hafKgyEk


