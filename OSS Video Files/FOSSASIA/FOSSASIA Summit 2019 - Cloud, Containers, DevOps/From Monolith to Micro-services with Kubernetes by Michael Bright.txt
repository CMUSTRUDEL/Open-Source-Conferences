Title: From Monolith to Micro-services with Kubernetes by Michael Bright
Publication date: 2019-03-31
Playlist: FOSSASIA Summit 2019 - Cloud, Containers, DevOps
Description: 
	16 March 2019 10:30, Lecture Theatre

Tutorial session demonstrating the evolution of a software monolith to micro service implementation running on Kubernetes.

We'll look at various design patterns (Strangler, API Gateway, Ingress, Service Mesh, Hybrid app) & operational practicalities of rolling out service upgrades to a Machine Learning app.

Attendees will learn about real Kubernetes use cases supported by *visual* demonstrations showing what's going on at each step to explain the concepts.
Captions: 
	00:00:00,350 --> 00:00:06,540
I'd say it's been a strange day so far

00:00:03,480 --> 00:00:10,190
myself I'm so I'm a freelance consultant

00:00:06,540 --> 00:00:13,860
and trainer on cloud native solutions

00:00:10,190 --> 00:00:20,210
I'm a Brit living in France for about 27

00:00:13,860 --> 00:00:24,330
years so soon be brexit refugee no doubt

00:00:20,210 --> 00:00:30,420
and I run a local docker meetup group

00:00:24,330 --> 00:00:34,170
and Python and this is Grenoble in the

00:00:30,420 --> 00:00:39,600
French Alps right live okay

00:00:34,170 --> 00:00:43,649
so first of all so why why would you go

00:00:39,600 --> 00:00:46,010
from monolith to microcircuits but first

00:00:43,649 --> 00:00:48,300
a bit of history

00:00:46,010 --> 00:00:51,120
so it's interesting if you look back the

00:00:48,300 --> 00:00:55,670
last 20 25 years it's been quite

00:00:51,120 --> 00:00:58,559
phenomenal the way computing has evolved

00:00:55,670 --> 00:01:02,670
25 years ago you know we were still

00:00:58,559 --> 00:01:04,500
running their net for servers and I

00:01:02,670 --> 00:01:06,630
guess you figure if you're running

00:01:04,500 --> 00:01:09,180
virtual machines it was because you were

00:01:06,630 --> 00:01:11,549
running IBM mainframes and don't think

00:01:09,180 --> 00:01:14,880
there were many instances of virtual

00:01:11,549 --> 00:01:18,900
machines at that time then a VMware came

00:01:14,880 --> 00:01:21,570
along so then you know virtual machines

00:01:18,900 --> 00:01:25,530
became new thing running on hypervisors

00:01:21,570 --> 00:01:30,060
which led on to to cloud with a choice

00:01:25,530 --> 00:01:33,780
of hypervisor Velen available so Clara

00:01:30,060 --> 00:01:36,180
would search for elastic systems then

00:01:33,780 --> 00:01:39,540
containers came along which existed

00:01:36,180 --> 00:01:43,110
before docker but docker made containers

00:01:39,540 --> 00:01:46,979
really usable and that has really

00:01:43,110 --> 00:01:51,119
changed the system again and the new fad

00:01:46,979 --> 00:01:53,340
I would say I believe in it it's real

00:01:51,119 --> 00:01:56,130
the new thing I would say a new kid on

00:01:53,340 --> 00:01:58,880
the block is service so we're going to

00:01:56,130 --> 00:02:01,200
talk about micro services which is a bit

00:01:58,880 --> 00:02:02,939
actually you know you could do micro

00:02:01,200 --> 00:02:06,520
service on virtual machines or

00:02:02,939 --> 00:02:09,280
containers or syphilis and

00:02:06,520 --> 00:02:12,069
one initial point I would say well the

00:02:09,280 --> 00:02:14,380
future will be hybrid we will use all of

00:02:12,069 --> 00:02:15,730
these technologies depending on

00:02:14,380 --> 00:02:22,300
different constraints

00:02:15,730 --> 00:02:24,250
important to us okay but why still would

00:02:22,300 --> 00:02:26,890
you want to move away from our wonderful

00:02:24,250 --> 00:02:30,250
monoliths have been serving us so well

00:02:26,890 --> 00:02:33,819
well these monoliths more or less

00:02:30,250 --> 00:02:36,010
they're you know big bricks of software

00:02:33,819 --> 00:02:39,819
that are developed in some sort of water

00:02:36,010 --> 00:02:42,160
fit waterfall development mode where the

00:02:39,819 --> 00:02:44,769
new release is like six twelve months

00:02:42,160 --> 00:02:46,630
out it's only at that point and the

00:02:44,769 --> 00:02:47,049
official release comes out in the

00:02:46,630 --> 00:02:49,810
meantime

00:02:47,049 --> 00:02:52,600
if you need to patch for example it's a

00:02:49,810 --> 00:02:57,549
bit of complicated process but for the

00:02:52,600 --> 00:02:59,489
vendor and for the person deploying and

00:02:57,549 --> 00:03:06,130
generally just don't have very much

00:02:59,489 --> 00:03:07,989
agility with modes even when it's an

00:03:06,130 --> 00:03:12,670
interior architecture that you still

00:03:07,989 --> 00:03:14,950
have fairly large pieces of software

00:03:12,670 --> 00:03:17,950
it's difficult to deal with and these

00:03:14,950 --> 00:03:21,459
are particularly ill suited to large

00:03:17,950 --> 00:03:25,390
enterprise scale or web scale and

00:03:21,459 --> 00:03:27,100
obviously with sort of applications we

00:03:25,390 --> 00:03:29,709
have on the web today from likes of

00:03:27,100 --> 00:03:32,650
Google and Facebook basically the gaffer

00:03:29,709 --> 00:03:35,140
we're running at enormous scale and so

00:03:32,650 --> 00:03:37,530
microservice really makes sense on that

00:03:35,140 --> 00:03:37,530
scale

00:03:38,960 --> 00:03:44,990
so as I said I mean these are you know

00:03:41,860 --> 00:03:49,390
big pieces software with components are

00:03:44,990 --> 00:03:52,100
tightly coupled generally the components

00:03:49,390 --> 00:03:54,770
should linked together with libraries

00:03:52,100 --> 00:03:55,520
and you can't separate out components

00:03:54,770 --> 00:03:57,170
very easily

00:03:55,520 --> 00:03:59,060
of course those libraries could be

00:03:57,170 --> 00:03:59,900
reused but there's still a hole to the

00:03:59,060 --> 00:04:04,930
build process

00:03:59,900 --> 00:04:07,910
and packaging associated to that and so

00:04:04,930 --> 00:04:12,770
generally it is difficult to move

00:04:07,910 --> 00:04:14,720
difficult to evolve to innovate micro

00:04:12,770 --> 00:04:18,410
services on the other hand we're really

00:04:14,720 --> 00:04:20,540
talking about splitting up an overall

00:04:18,410 --> 00:04:24,050
application into components where each

00:04:20,540 --> 00:04:27,080
component does one function and it does

00:04:24,050 --> 00:04:30,020
it well okay and that has several

00:04:27,080 --> 00:04:32,660
implications that means that when you're

00:04:30,020 --> 00:04:34,580
working at web scale for example you

00:04:32,660 --> 00:04:39,770
might have some parts of the application

00:04:34,580 --> 00:04:41,420
maybe a database which you know actually

00:04:39,770 --> 00:04:43,400
might be just one instance but other

00:04:41,420 --> 00:04:46,940
parts which may be a front-end web

00:04:43,400 --> 00:04:48,740
server which you want to scale based on

00:04:46,940 --> 00:04:52,610
the amount of traffic coming into the

00:04:48,740 --> 00:04:55,880
system one of the advantages of cloud of

00:04:52,610 --> 00:04:59,720
course here why why do you use cloud at

00:04:55,880 --> 00:05:03,140
all you know what's the big interest one

00:04:59,720 --> 00:05:05,150
interest is agility ability to fire up

00:05:03,140 --> 00:05:09,440
the applications on the fly but

00:05:05,150 --> 00:05:11,720
especially to to scale out based on the

00:05:09,440 --> 00:05:15,470
traffic coming for the network and that

00:05:11,720 --> 00:05:19,910
in particular allow us to mutual eyes

00:05:15,470 --> 00:05:22,910
the resources in the cloud so you want

00:05:19,910 --> 00:05:24,650
to be able to scale up some components

00:05:22,910 --> 00:05:27,620
when there is need for them and scale

00:05:24,650 --> 00:05:30,870
down when there isn't need to make

00:05:27,620 --> 00:05:34,860
efficient use of cloud or datacenter

00:05:30,870 --> 00:05:36,840
sources so microservices architecture

00:05:34,860 --> 00:05:39,500
components that they are likely coupled

00:05:36,840 --> 00:05:42,600
what that means is they're no longer

00:05:39,500 --> 00:05:44,930
linked together as a binary though

00:05:42,600 --> 00:05:47,820
they're interconnected over the network

00:05:44,930 --> 00:05:50,750
they can be scaled independently and

00:05:47,820 --> 00:05:53,910
then you can be deployed or upgraded

00:05:50,750 --> 00:05:59,130
independently if you do it right of

00:05:53,910 --> 00:06:01,140
course it isn't free I'm being able to

00:05:59,130 --> 00:06:04,560
deploy and operate independently it's

00:06:01,140 --> 00:06:08,880
really important to be able to quickly

00:06:04,560 --> 00:06:11,100
evolve your service and even you can

00:06:08,880 --> 00:06:13,230
even replace implementations of some

00:06:11,100 --> 00:06:17,790
components because the components I'm

00:06:13,230 --> 00:06:19,290
saying do one thing one thing well you

00:06:17,790 --> 00:06:21,770
might decide you might have an

00:06:19,290 --> 00:06:24,810
implementation of let's say a web server

00:06:21,770 --> 00:06:27,660
it suited you well do that in Python in

00:06:24,810 --> 00:06:30,860
the beginning now you want to go with go

00:06:27,660 --> 00:06:34,110
or maybe even the C++ ultimate speed

00:06:30,860 --> 00:06:36,200
under the microsecond meet that sort of

00:06:34,110 --> 00:06:42,270
a change

00:06:36,200 --> 00:06:44,820
okay so Vantage's say you do the

00:06:42,270 --> 00:06:47,100
component of micro service does one

00:06:44,820 --> 00:06:50,310
thing well and this means you can have

00:06:47,100 --> 00:06:53,550
small focused project teams working on

00:06:50,310 --> 00:06:56,490
these things who really are focused to

00:06:53,550 --> 00:06:58,650
just guarantee the functionality of this

00:06:56,490 --> 00:07:01,470
component they might be working on

00:06:58,650 --> 00:07:07,400
several components but each one they

00:07:01,470 --> 00:07:11,310
have some focus on it's easy to scale

00:07:07,400 --> 00:07:13,290
these components to deploy them to test

00:07:11,310 --> 00:07:15,760
them at least in the sense of unit

00:07:13,290 --> 00:07:19,810
testing and so easy

00:07:15,760 --> 00:07:22,030
to evolve them and also because they're

00:07:19,810 --> 00:07:24,130
separate components where it's just

00:07:22,030 --> 00:07:28,720
network communications easy to compose

00:07:24,130 --> 00:07:30,580
new services from micro services and so

00:07:28,720 --> 00:07:32,560
each component as well you can choose to

00:07:30,580 --> 00:07:34,870
re-implement in like best of class

00:07:32,560 --> 00:07:36,970
technology change the implementation

00:07:34,870 --> 00:07:39,850
language or maybe change the

00:07:36,970 --> 00:07:43,240
implementation of a database go from to

00:07:39,850 --> 00:07:45,480
SQL to some know SQL or graph

00:07:43,240 --> 00:07:49,270
functionality whatever

00:07:45,480 --> 00:07:52,450
so are there a panacea no of course

00:07:49,270 --> 00:07:55,810
there's some cost of course there's

00:07:52,450 --> 00:07:57,400
great complexity technically you have

00:07:55,810 --> 00:08:00,850
more components and you have to manage

00:07:57,400 --> 00:08:05,020
more things and you need to orchestrate

00:08:00,850 --> 00:08:07,450
these are micro services you also have

00:08:05,020 --> 00:08:10,770
greater complexity in your organization

00:08:07,450 --> 00:08:15,460
because you have more teams to manage

00:08:10,770 --> 00:08:17,410
and then operationally in terms of

00:08:15,460 --> 00:08:20,950
monitoring debugging and end-to-end

00:08:17,410 --> 00:08:24,910
testing so of your overall service these

00:08:20,950 --> 00:08:27,490
tasks are more difficult Network

00:08:24,910 --> 00:08:29,170
communication error is critical so you

00:08:27,490 --> 00:08:32,440
really need to manage that in terms of

00:08:29,170 --> 00:08:35,140
latency and detecting where problems are

00:08:32,440 --> 00:08:37,380
maybe doing what we call circuit

00:08:35,140 --> 00:08:41,140
breaking when a problem is detected to

00:08:37,380 --> 00:08:45,250
isolate problems one particular area so

00:08:41,140 --> 00:08:49,000
there's a sort of a mindset of design to

00:08:45,250 --> 00:08:52,460
fail and but to keep working when there

00:08:49,000 --> 00:08:55,700
are failures in your system

00:08:52,460 --> 00:08:58,310
and then be aware micro-services

00:08:55,700 --> 00:09:02,090
I useless if you don't adopt best

00:08:58,310 --> 00:09:03,620
practices so it's really essential if

00:09:02,090 --> 00:09:05,960
you're doing micro service you really

00:09:03,620 --> 00:09:10,010
want some DevOps processes you and

00:09:05,960 --> 00:09:11,480
behavior and test-driven development you

00:09:10,010 --> 00:09:14,570
need to be very rigorous about

00:09:11,480 --> 00:09:17,210
implementation implementing the API is

00:09:14,570 --> 00:09:21,170
around these components you have to

00:09:17,210 --> 00:09:24,140
guarantee your interfaces and you need

00:09:21,170 --> 00:09:28,040
to provide stable api's with some degree

00:09:24,140 --> 00:09:29,840
of that good compatibility so providing

00:09:28,040 --> 00:09:33,440
you stay with those specs some best

00:09:29,840 --> 00:09:35,020
practices then micro services are a good

00:09:33,440 --> 00:09:38,120
solution

00:09:35,020 --> 00:09:42,620
so I mentioned one thing is that we need

00:09:38,120 --> 00:09:49,430
to orchestrate so basically now is the

00:09:42,620 --> 00:09:52,570
system scale your application scales

00:09:49,430 --> 00:09:55,220
across maybe a dense data center of

00:09:52,570 --> 00:10:00,770
hundreds of nodes maybe even thousands

00:09:55,220 --> 00:10:02,180
or maybe with a cloud provider as there

00:10:00,770 --> 00:10:04,490
are more and more instances of each

00:10:02,180 --> 00:10:07,250
component it becomes impossible for an

00:10:04,490 --> 00:10:09,850
operator to manage that and you just

00:10:07,250 --> 00:10:12,920
wouldn't want them to do that so you

00:10:09,850 --> 00:10:15,680
want to do thing you want to be able to

00:10:12,920 --> 00:10:19,430
place your components on different nodes

00:10:15,680 --> 00:10:21,380
to ensure availability if a node goes

00:10:19,430 --> 00:10:24,730
down for example that your overall

00:10:21,380 --> 00:10:27,320
service still continues to work maybe

00:10:24,730 --> 00:10:29,780
respect some affinity non affinity

00:10:27,320 --> 00:10:33,580
constraints as well to guarantee the

00:10:29,780 --> 00:10:36,080
availability of the overall applications

00:10:33,580 --> 00:10:39,190
and you may want to take advantage of

00:10:36,080 --> 00:10:39,190
specialized hardware

00:10:39,750 --> 00:10:45,270
maybe monitor which containers are not

00:10:43,140 --> 00:10:49,650
functioning and whether they're started

00:10:45,270 --> 00:10:51,570
and ready to go and also yeah how are

00:10:49,650 --> 00:10:52,920
you going to update your applications

00:10:51,570 --> 00:10:56,130
again this is somewhere by an

00:10:52,920 --> 00:11:02,220
Orchestrator like kubernetes could come

00:10:56,130 --> 00:11:04,980
in okay so we need orchestration and I'm

00:11:02,220 --> 00:11:08,430
going to assume that we we part with the

00:11:04,980 --> 00:11:10,500
capabilities I'm going to advanced bit

00:11:08,430 --> 00:11:12,420
slow here so I'm not going to present

00:11:10,500 --> 00:11:14,120
you kubernetes but if you're in this

00:11:12,420 --> 00:11:17,730
talk I guess you're aware that basically

00:11:14,120 --> 00:11:21,180
kubernetes is a a cluster management

00:11:17,730 --> 00:11:24,200
system coming from Google initially

00:11:21,180 --> 00:11:27,570
based on their experience in-house

00:11:24,200 --> 00:11:29,520
container orchestration the basic

00:11:27,570 --> 00:11:32,190
architecture has a set of master nodes

00:11:29,520 --> 00:11:34,170
is basically the control plane and a set

00:11:32,190 --> 00:11:36,839
of worker nodes where your actual

00:11:34,170 --> 00:11:38,970
containers will be running I say

00:11:36,839 --> 00:11:44,660
containers but actually be running pods

00:11:38,970 --> 00:11:48,110
Kuban it is come to that no moment okay

00:11:44,660 --> 00:11:50,650
pals skip over most of this want to

00:11:48,110 --> 00:11:53,420
spend time in it but just to say that

00:11:50,650 --> 00:11:56,090
kubernetes is a declarative system you

00:11:53,420 --> 00:11:58,730
say for example you want to run an

00:11:56,090 --> 00:12:02,210
engine X web server you're going to say

00:11:58,730 --> 00:12:04,610
to cube that is okay these are my

00:12:02,210 --> 00:12:07,700
constraints I want I'd say three

00:12:04,610 --> 00:12:11,300
replicas three instances of this and

00:12:07,700 --> 00:12:13,370
it's kubernetes that then bossy what's

00:12:11,300 --> 00:12:17,330
the utilization of the different nodes

00:12:13,370 --> 00:12:19,960
and it will decide where to place those

00:12:17,330 --> 00:12:19,960
instances

00:12:23,180 --> 00:12:29,990
okay I'm gonna skip just two parts just

00:12:26,570 --> 00:12:33,560
to say so a pod is a kubernetes concept

00:12:29,990 --> 00:12:36,350
which represents one or more containers

00:12:33,560 --> 00:12:38,450
running together the idea is that you

00:12:36,350 --> 00:12:42,550
have one main container which provided

00:12:38,450 --> 00:12:45,050
your frankish tality let's see webserver

00:12:42,550 --> 00:12:47,120
staying on that one and any extra

00:12:45,050 --> 00:12:50,440
containers we didn't call them a sidecar

00:12:47,120 --> 00:12:52,970
which are basically providing ancillary

00:12:50,440 --> 00:12:55,550
functionality helping that main

00:12:52,970 --> 00:13:02,570
functionality so see an example of that

00:12:55,550 --> 00:13:07,160
with with this do okay so as I saying

00:13:02,570 --> 00:13:12,820
I had intention doing a lot of demos but

00:13:07,160 --> 00:13:16,010
it's been I will show just nevertheless

00:13:12,820 --> 00:13:21,170
this is a visualization of in fact I'm

00:13:16,010 --> 00:13:23,570
running doc of a desktop this was the

00:13:21,170 --> 00:13:27,500
environment of the ganache over my cube

00:13:23,570 --> 00:13:29,980
Annette is closed from digitalocean it

00:13:27,500 --> 00:13:32,810
went away yesterday so I recreated it

00:13:29,980 --> 00:13:35,960
today I still had connectivity problems

00:13:32,810 --> 00:13:38,510
but okay just to show I was hoping to

00:13:35,960 --> 00:13:46,270
demonstrate across a three node system

00:13:38,510 --> 00:13:46,270
one master and two workers but I will do

00:13:47,010 --> 00:13:53,399
the same demonstration based on docker

00:13:50,800 --> 00:13:53,399
for desktop

00:13:59,710 --> 00:14:04,420
just check

00:14:01,700 --> 00:14:04,420
connected okay

00:14:06,840 --> 00:14:09,410
so

00:14:09,810 --> 00:14:19,790
we enlarge this probably can't see much

00:14:14,940 --> 00:14:23,330
it's that readable okay so I'm going to

00:14:19,790 --> 00:14:23,330
just show how

00:14:29,020 --> 00:14:41,380
I'm going to basically deploy a Redis

00:14:33,120 --> 00:14:46,209
key-value store recommend basically you

00:14:41,380 --> 00:14:49,330
dude cube CTL create or apply of the

00:14:46,209 --> 00:14:54,660
llamo definition file for creating

00:14:49,330 --> 00:14:58,209
deployment and what we're seeing here

00:14:54,660 --> 00:15:01,600
this block is a set of kubernetes

00:14:58,209 --> 00:15:05,649
objects that have the same run dagger

00:15:01,600 --> 00:15:09,100
app tank and what we're seeing is a

00:15:05,649 --> 00:15:14,080
deployment I'm saying they deploy me one

00:15:09,100 --> 00:15:19,779
instance of Redis okay that's a book you

00:15:14,080 --> 00:15:22,720
see it twice and we we have a replica

00:15:19,779 --> 00:15:24,760
set I am at work should not seeing sorry

00:15:22,720 --> 00:15:27,149
and that's the third one is the pod it

00:15:24,760 --> 00:15:27,149
will become clear

00:15:29,430 --> 00:15:36,670
I'm gonna run a flask application which

00:15:33,370 --> 00:15:38,320
would normally be a front-end okay it

00:15:36,670 --> 00:15:41,200
was just right there as the container

00:15:38,320 --> 00:15:43,149
started up if it was having to take time

00:15:41,200 --> 00:15:45,690
pulling images it would have stayed read

00:15:43,149 --> 00:15:45,690
a lot longer

00:15:47,080 --> 00:15:49,769
okay

00:15:50,470 --> 00:15:57,840
and I can do things like so I've just I

00:15:54,630 --> 00:15:57,840
should show you

00:16:00,760 --> 00:16:06,820
so the yellow file for creating those

00:16:03,990 --> 00:16:10,839
basically you specifying them the number

00:16:06,820 --> 00:16:13,690
of replicas of that so the initial

00:16:10,839 --> 00:16:16,029
version I did replicas one obviously and

00:16:13,690 --> 00:16:19,260
now I'm going to rerun essentially the

00:16:16,029 --> 00:16:19,260
same llamó

00:16:24,550 --> 00:16:27,779
the replicas for

00:16:28,890 --> 00:16:36,260
or you can see how it quickly creates

00:16:31,730 --> 00:16:39,600
several containers several pots rather

00:16:36,260 --> 00:16:40,770
if I'd had my other environment working

00:16:39,600 --> 00:16:42,990
then these would obviously be

00:16:40,770 --> 00:16:45,800
distributed across the the to worker

00:16:42,990 --> 00:16:45,800
nodes that I had

00:16:50,980 --> 00:16:55,600
okay I'm gonna move quite quickly so

00:16:53,770 --> 00:16:58,380
I've got some slides just representing

00:16:55,600 --> 00:17:04,079
that deployment that I just did but

00:16:58,380 --> 00:17:09,190
those okay so I tried to organize

00:17:04,079 --> 00:17:11,620
presentation by in two main parts really

00:17:09,190 --> 00:17:14,199
deployment strategies and architecture

00:17:11,620 --> 00:17:17,770
design patterns those might not be the

00:17:14,199 --> 00:17:20,260
best names for these categories but you

00:17:17,770 --> 00:17:23,020
know naming is the hardest thing in

00:17:20,260 --> 00:17:25,000
computer science so laughter let me get

00:17:23,020 --> 00:17:26,530
away with that one so first of all first

00:17:25,000 --> 00:17:31,600
of all what I mean by deployment

00:17:26,530 --> 00:17:35,920
strategies so basically yeah how are we

00:17:31,600 --> 00:17:38,910
going to roll out our services to see

00:17:35,920 --> 00:17:42,190
how a data center or our Clyde our cloud

00:17:38,910 --> 00:17:44,800
and then how we're going to upgrade them

00:17:42,190 --> 00:17:48,670
and this sort of thing as services

00:17:44,800 --> 00:17:51,090
evolve and so there's a group of things

00:17:48,670 --> 00:17:56,800
I'm putting under this banner

00:17:51,090 --> 00:17:58,870
so there are deployment strategies force

00:17:56,800 --> 00:18:00,730
of evolving a service upgrading a

00:17:58,870 --> 00:18:05,890
service or a look at some of those

00:18:00,730 --> 00:18:08,340
examples some of these most of them can

00:18:05,890 --> 00:18:13,120
be implemented by cuban artists alone

00:18:08,340 --> 00:18:15,460
the more advanced ones like doing like

00:18:13,120 --> 00:18:16,000
canary roll out some a be testing

00:18:15,460 --> 00:18:18,100
rollouts

00:18:16,000 --> 00:18:21,150
would require something like an api

00:18:18,100 --> 00:18:21,150
gateway in front

00:18:21,650 --> 00:18:29,580
so I'm going to look at service upgrade

00:18:24,540 --> 00:18:36,510
strategies health checks and what we

00:18:29,580 --> 00:18:38,790
call the strangler pattern okay so first

00:18:36,510 --> 00:18:42,150
of all for service upgrade strategies so

00:18:38,790 --> 00:18:45,300
really saying for example the the flask

00:18:42,150 --> 00:18:49,230
of Redis that I just deployed how can I

00:18:45,300 --> 00:18:52,200
go about upgrading let's say the flask

00:18:49,230 --> 00:18:56,100
application in a way that it makes sense

00:18:52,200 --> 00:18:59,460
to the overall application so for

00:18:56,100 --> 00:19:02,790
example the first way of doing this is a

00:18:59,460 --> 00:19:04,110
thing called recreate which is fine in

00:19:02,790 --> 00:19:08,130
development but not very good in

00:19:04,110 --> 00:19:09,960
production because you stopped the old

00:19:08,130 --> 00:19:13,110
version of our application completely

00:19:09,960 --> 00:19:15,390
and then you deploy the new one so

00:19:13,110 --> 00:19:17,640
obviously that's not exactly a very

00:19:15,390 --> 00:19:20,010
production friendly nevertheless it is a

00:19:17,640 --> 00:19:24,240
deployment strategy which is fine for a

00:19:20,010 --> 00:19:27,930
developer on his laptop the one which is

00:19:24,240 --> 00:19:33,750
used by default in kubernetes is ramped

00:19:27,930 --> 00:19:37,470
which basically kubernetes will deploy a

00:19:33,750 --> 00:19:40,230
new version of an instance once that's

00:19:37,470 --> 00:19:43,860
running it will stop an instance so if

00:19:40,230 --> 00:19:48,330
you have let's say as I had here for

00:19:43,860 --> 00:19:49,830
flask instances it will upgrade one when

00:19:48,330 --> 00:19:53,010
it's ready it will stop one of the

00:19:49,830 --> 00:19:53,930
existing ones and so on and so during

00:19:53,010 --> 00:19:56,970
the upgrade

00:19:53,930 --> 00:20:01,190
we'll see we actually have a mix of all

00:19:56,970 --> 00:20:06,180
the new tend to say blue and green

00:20:01,190 --> 00:20:09,360
versions of the service so that's that's

00:20:06,180 --> 00:20:13,050
one way of doing it advantage of that is

00:20:09,360 --> 00:20:15,450
that you require but you have a service

00:20:13,050 --> 00:20:18,690
running all the time it's not like the

00:20:15,450 --> 00:20:21,210
first one where you stop everything but

00:20:18,690 --> 00:20:22,940
you get into a mixed situation so you

00:20:21,210 --> 00:20:25,039
could have requests coming in

00:20:22,940 --> 00:20:27,820
treated by the old or the new version

00:20:25,039 --> 00:20:30,940
and you might even have some

00:20:27,820 --> 00:20:35,590
incompatibilities of API to manage

00:20:30,940 --> 00:20:35,590
that's something you have to design out

00:20:35,769 --> 00:20:44,029
ok so I will show that one in a moment

00:20:39,489 --> 00:20:46,309
another way of doing it is blue green so

00:20:44,029 --> 00:20:49,129
blue green if you have the the old

00:20:46,309 --> 00:20:50,989
version running you start out the same

00:20:49,129 --> 00:20:53,599
number of instances of the new version

00:20:50,989 --> 00:20:57,200
and once they are all ready you then

00:20:53,599 --> 00:20:59,809
switch to the new version so that's

00:20:57,200 --> 00:21:02,119
great some slightly ideal except that of

00:20:59,809 --> 00:21:04,279
course it means you you need double the

00:21:02,119 --> 00:21:08,559
resources during the the switch over

00:21:04,279 --> 00:21:08,559
time that might not be a problem

00:21:11,380 --> 00:21:18,070
canary is basically saying okay I'm

00:21:15,880 --> 00:21:21,660
rolling out this new version service I'm

00:21:18,070 --> 00:21:24,910
not completely sure yet maybe I want to

00:21:21,660 --> 00:21:27,610
just set out for one percent of the

00:21:24,910 --> 00:21:31,990
users something like this see if the

00:21:27,610 --> 00:21:34,630
phone rings and roll back so you know

00:21:31,990 --> 00:21:38,020
try it and see

00:21:34,630 --> 00:21:42,220
and for that sort of functionality you

00:21:38,020 --> 00:21:44,710
generally need to implement that in an

00:21:42,220 --> 00:21:47,560
API gateway or service mesh which we

00:21:44,710 --> 00:21:50,110
will come to and the last one is EB

00:21:47,560 --> 00:21:53,710
testing which are very similar to canary

00:21:50,110 --> 00:21:57,430
testing but with the difference that you

00:21:53,710 --> 00:22:00,820
selectively determine which version the

00:21:57,430 --> 00:22:02,890
service you need so if you have a

00:22:00,820 --> 00:22:06,220
service in production and you actually

00:22:02,890 --> 00:22:08,860
want to upgrade it just for a certain

00:22:06,220 --> 00:22:10,360
class of users for example it's just

00:22:08,860 --> 00:22:12,340
with them you want to test then you

00:22:10,360 --> 00:22:15,640
could do that and that as well would

00:22:12,340 --> 00:22:19,570
require an API API gate to a your

00:22:15,640 --> 00:22:23,890
service mesh to to implement that so

00:22:19,570 --> 00:22:27,930
those the choices and I will show an

00:22:23,890 --> 00:22:33,870
example of ramped which is the standard

00:22:27,930 --> 00:22:33,870
sorry the default way of doing things

00:22:37,650 --> 00:22:43,200
okay

00:22:40,140 --> 00:22:46,440
so basically I have another llamo file

00:22:43,200 --> 00:22:50,929
had already created its should actually

00:22:46,440 --> 00:22:50,929
show it but if I do

00:23:00,780 --> 00:23:04,830
my channel is a bit strange I don't have

00:23:02,610 --> 00:23:07,380
cooking copied a strip okay if I do a

00:23:04,830 --> 00:23:09,270
diff with the one I just used to get to

00:23:07,380 --> 00:23:11,760
four replicas

00:23:09,270 --> 00:23:14,550
we'll just see as I'm using a different

00:23:11,760 --> 00:23:17,280
image in that file a person I'm going to

00:23:14,550 --> 00:23:20,550
pass from v1 to v2 so I'm going to

00:23:17,280 --> 00:23:23,520
perform that update and we should see as

00:23:20,550 --> 00:23:28,050
I talk we'll be a moment we're okay on

00:23:23,520 --> 00:23:31,230
the right is the for flask pots all in

00:23:28,050 --> 00:23:34,770
version one I will see that we have more

00:23:31,230 --> 00:23:39,780
than that more than four pots during the

00:23:34,770 --> 00:23:45,300
upgrade there's go fairly quickly okay

00:23:39,780 --> 00:23:51,290
so we see you pods they read it until

00:23:45,300 --> 00:23:54,060
they're up and running okay it's all

00:23:51,290 --> 00:23:55,980
okay it's a shame of a burka my

00:23:54,060 --> 00:24:00,470
visualization because we would have seen

00:23:55,980 --> 00:24:00,470
the replica set or would have seen is

00:24:00,530 --> 00:24:04,770
there's a deployment object which is

00:24:02,880 --> 00:24:08,130
flask app and then we would have a

00:24:04,770 --> 00:24:10,830
replica set which represents the version

00:24:08,130 --> 00:24:13,140
1 of this flask app and it is the one

00:24:10,830 --> 00:24:17,370
that creates the pods so what would

00:24:13,140 --> 00:24:19,140
actually have seen is still the same

00:24:17,370 --> 00:24:21,780
deployment but would have had two

00:24:19,140 --> 00:24:24,330
replicas sets the old one now with zero

00:24:21,780 --> 00:24:28,080
instances and the new one with four

00:24:24,330 --> 00:24:30,720
instances and then these the new flask

00:24:28,080 --> 00:24:35,990
at pod of instances which you can see a

00:24:30,720 --> 00:24:35,990
vision - okay so just to

00:24:39,010 --> 00:24:42,090
let's be careful where I click

00:24:42,240 --> 00:24:47,940
I was just to give you an idea show you

00:24:44,700 --> 00:24:50,040
something real around when these

00:24:47,940 --> 00:24:57,150
strategies okay so that's the demo I

00:24:50,040 --> 00:25:01,500
just showed you okay another thing to do

00:24:57,150 --> 00:25:04,830
then is to validate so containers yeah

00:25:01,500 --> 00:25:06,630
it's fine just deploying them but we

00:25:04,830 --> 00:25:09,810
need to be sure that they are healthy

00:25:06,630 --> 00:25:13,410
before we actually put them online so

00:25:09,810 --> 00:25:15,690
you know Cuba Nettie's will not want to

00:25:13,410 --> 00:25:18,300
route traffic to containers or pots that

00:25:15,690 --> 00:25:20,550
are known not to be functioning and so

00:25:18,300 --> 00:25:24,360
how do you do that you do that through

00:25:20,550 --> 00:25:30,440
health checks and there are two types of

00:25:24,360 --> 00:25:30,440
health checks liveness and readiness so

00:25:31,070 --> 00:25:38,760
liveness basically is detecting that the

00:25:36,210 --> 00:25:41,820
Pardons containers are themselves just

00:25:38,760 --> 00:25:44,310
up and running with readiness is

00:25:41,820 --> 00:25:49,320
detecting they are actually capable of

00:25:44,310 --> 00:25:53,280
serving traffic okay and there are three

00:25:49,320 --> 00:25:56,670
types of probe that you can do so either

00:25:53,280 --> 00:26:00,600
a combined command to execute check

00:25:56,670 --> 00:26:02,850
something about the container or an HTTP

00:26:00,600 --> 00:26:07,200
request to tries typically you would

00:26:02,850 --> 00:26:08,970
just have a like the standard URL but

00:26:07,200 --> 00:26:13,890
with a slash health or something like

00:26:08,970 --> 00:26:15,360
that to test or a TCP request to try and

00:26:13,890 --> 00:26:19,920
it's cuba Nettie's that's going to

00:26:15,360 --> 00:26:23,720
basically poll your pod to see if that

00:26:19,920 --> 00:26:23,720
tells check passes or not

00:26:24,290 --> 00:26:29,930
likewise there are so readiness

00:26:26,510 --> 00:26:34,630
readiness probes so the first one

00:26:29,930 --> 00:26:38,210
although there are things like an HP

00:26:34,630 --> 00:26:41,210
HTTP request to try this might be just a

00:26:38,210 --> 00:26:43,790
test to verify that the party is there

00:26:41,210 --> 00:26:46,970
that it's up okay but not necessarily

00:26:43,790 --> 00:26:49,340
testing your application yet okay with

00:26:46,970 --> 00:26:50,390
the readiness probes the idea is it

00:26:49,340 --> 00:26:54,710
actually going to be testing the

00:26:50,390 --> 00:26:57,170
application so it's a second stage of

00:26:54,710 --> 00:26:59,750
readiness and these the same probe types

00:26:57,170 --> 00:27:02,690
as for the eliteness probes same type

00:26:59,750 --> 00:27:04,390
but maybe some different URL shall be

00:27:02,690 --> 00:27:06,890
tested

00:27:04,390 --> 00:27:09,380
okay so the definition of alive lists

00:27:06,890 --> 00:27:14,240
bro the five Ness probe is something

00:27:09,380 --> 00:27:21,410
like this so in this case it's something

00:27:14,240 --> 00:27:23,690
that you exec on the container and you

00:27:21,410 --> 00:27:27,160
can specify the initial delay when to

00:27:23,690 --> 00:27:29,600
start testing and during what period and

00:27:27,160 --> 00:27:31,280
kubernetes could determine the toilet

00:27:29,600 --> 00:27:33,800
after a certain time this doesn't

00:27:31,280 --> 00:27:36,440
succeed then we should just destroy that

00:27:33,800 --> 00:27:42,410
container and that pod and create

00:27:36,440 --> 00:27:44,420
another one and britainís probes are

00:27:42,410 --> 00:27:47,630
exactly the same it's literally just the

00:27:44,420 --> 00:27:50,440
keyword lightness probe is replaced with

00:27:47,630 --> 00:27:50,440
the readiness probe

00:27:51,730 --> 00:27:57,130
okay so I just wanted to say okay that's

00:27:54,580 --> 00:28:03,360
an important thing about the way

00:27:57,130 --> 00:28:03,360
kubernetes operates in deploying pods

00:28:05,049 --> 00:28:10,750
the other thing is so how do you go

00:28:08,290 --> 00:28:13,270
actually from a monolith that such the

00:28:10,750 --> 00:28:15,730
subject the talk so you know how would

00:28:13,270 --> 00:28:17,770
you do that it's not easy I've said it's

00:28:15,730 --> 00:28:20,770
a monolith you know how the heck you

00:28:17,770 --> 00:28:24,280
know move to micro-services you know you

00:28:20,770 --> 00:28:26,020
can no re-implement everything six

00:28:24,280 --> 00:28:29,470
months down the line yes we've got the

00:28:26,020 --> 00:28:31,600
new service exactly same functionality

00:28:29,470 --> 00:28:34,090
the old service but you know you've lost

00:28:31,600 --> 00:28:36,669
six months so you probably want to have

00:28:34,090 --> 00:28:42,100
some more sort of incremental way of

00:28:36,669 --> 00:28:43,900
moving to micro services and basically

00:28:42,100 --> 00:28:46,630
the Strangler pattern provides a

00:28:43,900 --> 00:28:52,960
possible solution for this the idea is

00:28:46,630 --> 00:28:54,640
that you would progressively take out or

00:28:52,960 --> 00:28:56,559
disable parts of your monolith

00:28:54,640 --> 00:28:59,650
application and reimplemented

00:28:56,559 --> 00:29:04,450
in micro services and so you would be in

00:28:59,650 --> 00:29:06,580
a gradual transition until you have lots

00:29:04,450 --> 00:29:09,190
of micro services and maybe each is one

00:29:06,580 --> 00:29:12,070
small part of the monolith which is left

00:29:09,190 --> 00:29:15,730
functional which is being strangled by

00:29:12,070 --> 00:29:18,460
the micro services that's what the

00:29:15,730 --> 00:29:23,070
strangler pattern is it's basically a

00:29:18,460 --> 00:29:23,070
migration deployment strategy

00:29:23,970 --> 00:29:30,330
okay this is something I took of the

00:29:27,180 --> 00:29:34,190
azure documentation site just showing

00:29:30,330 --> 00:29:37,140
the migration from left to right from

00:29:34,190 --> 00:29:39,510
lots of legacy in a bit of the modern

00:29:37,140 --> 00:29:43,620
micro service implementation through to

00:29:39,510 --> 00:29:46,920
I think just the modern implementation

00:29:43,620 --> 00:29:53,070
where you finally you've strangled the

00:29:46,920 --> 00:29:56,520
original model okay the other thing I

00:29:53,070 --> 00:29:59,700
want to talk about is like I grouped as

00:29:56,520 --> 00:30:02,730
architecture design patterns okay so

00:29:59,700 --> 00:30:05,040
really so I'm not talking about like

00:30:02,730 --> 00:30:06,810
standard component design patterns you

00:30:05,040 --> 00:30:08,580
know Java design patterns or whatever

00:30:06,810 --> 00:30:13,010
you might have within your component

00:30:08,580 --> 00:30:15,000
sense another another subject entirely

00:30:13,010 --> 00:30:17,880
micro-service themselves is worth

00:30:15,000 --> 00:30:22,500
mentioning micro-service is design

00:30:17,880 --> 00:30:25,110
button nor sidecar pattern which we

00:30:22,500 --> 00:30:29,600
mentioned exists that you can have

00:30:25,110 --> 00:30:29,600
second third containers within a pod

00:30:30,120 --> 00:30:35,040
now we're concerned with basically

00:30:32,400 --> 00:30:39,360
exposing services and ingress

00:30:35,040 --> 00:30:42,120
functionality so providing access to

00:30:39,360 --> 00:30:47,750
services within the Kuban assist cluster

00:30:42,120 --> 00:30:51,390
itself I'll come on to that and then

00:30:47,750 --> 00:30:53,760
externally around your cluster the use

00:30:51,390 --> 00:30:58,550
of things like an API gateway service

00:30:53,760 --> 00:31:03,890
mesh and even more advanced api gateways

00:30:58,550 --> 00:31:03,890
based on call the API gateway pattern

00:31:04,200 --> 00:31:11,710
and I would say this this part is real

00:31:08,469 --> 00:31:16,090
warm that that last bit of API gateway

00:31:11,710 --> 00:31:18,759
in service mesh there are several API

00:31:16,090 --> 00:31:22,239
gateways really well established on the

00:31:18,759 --> 00:31:24,489
other engine xh8 proxy and so on but now

00:31:22,239 --> 00:31:28,779
we have service meshes coming from sto

00:31:24,489 --> 00:31:31,629
linker D and so on and there's quite a

00:31:28,779 --> 00:31:33,669
battle going on between the gateways

00:31:31,629 --> 00:31:39,909
themselves between the service meshes

00:31:33,669 --> 00:31:43,359
and between the two of them okay so that

00:31:39,909 --> 00:31:47,169
first part looking at how we access our

00:31:43,359 --> 00:31:51,969
services so you know we've deployed them

00:31:47,169 --> 00:31:53,859
as a set of pods but so far we don't

00:31:51,969 --> 00:31:57,219
actually have a way of addressing them

00:31:53,859 --> 00:31:59,919
each pod has its own IP address and

00:31:57,219 --> 00:32:02,499
Maeby's exposing some ports but though

00:31:59,919 --> 00:32:06,749
those our specific addresses to a

00:32:02,499 --> 00:32:10,749
particular pod and the very nature of

00:32:06,749 --> 00:32:13,509
kubernetes these contain the platform's

00:32:10,749 --> 00:32:15,159
is that pods of containers are coming

00:32:13,509 --> 00:32:17,739
and going there they're dying or they're

00:32:15,159 --> 00:32:21,749
being scaled up scaled down so you

00:32:17,739 --> 00:32:26,169
certainly can't depend upon the pod IP

00:32:21,749 --> 00:32:30,039
remaining valid you could attach to a

00:32:26,169 --> 00:32:33,489
host something like call no deport or

00:32:30,039 --> 00:32:36,399
host port but then you're exposing your

00:32:33,489 --> 00:32:38,469
infrastructure and you don't actually

00:32:36,399 --> 00:32:41,739
the same thing if you were addressing

00:32:38,469 --> 00:32:44,499
directly the the pod as well you should

00:32:41,739 --> 00:32:46,690
actually be on an isolated Network and

00:32:44,499 --> 00:32:49,779
that shouldn't be something you should

00:32:46,690 --> 00:32:54,909
be extra into the exterior so what we

00:32:49,779 --> 00:32:57,700
need is basically service endpoints a

00:32:54,909 --> 00:33:01,119
well known address so we can use to

00:32:57,700 --> 00:33:02,720
address our application and then the

00:33:01,119 --> 00:33:05,720
platform take care

00:33:02,720 --> 00:33:09,020
routing that through to the pots that

00:33:05,720 --> 00:33:11,929
implement that service okay

00:33:09,020 --> 00:33:16,400
I'm sorry I've said most of what is on

00:33:11,929 --> 00:33:21,490
here okay

00:33:16,400 --> 00:33:25,669
so basic principle is we have this

00:33:21,490 --> 00:33:29,809
service of some some sort which is going

00:33:25,669 --> 00:33:31,909
to access and load balance across the

00:33:29,809 --> 00:33:35,929
various spots available in our system

00:33:31,909 --> 00:33:38,960
and our end user application he should

00:33:35,929 --> 00:33:42,950
just need to know the service IP in port

00:33:38,960 --> 00:33:46,280
or or hostname okay this is where you'd

00:33:42,950 --> 00:33:49,909
have reverse proxies that map to a nice

00:33:46,280 --> 00:33:53,960
application name through to some

00:33:49,909 --> 00:33:56,990
internal address on your system so a

00:33:53,960 --> 00:34:01,130
various ways of doing this a note port

00:33:56,990 --> 00:34:02,330
and host port almost the same thing I

00:34:01,130 --> 00:34:04,880
don't remember the difference but

00:34:02,330 --> 00:34:09,589
essentially it's providing an access

00:34:04,880 --> 00:34:13,099
direct to the actual cluster nodes in

00:34:09,589 --> 00:34:16,419
your system this cluster IP which is an

00:34:13,099 --> 00:34:20,270
internal address which is addressable by

00:34:16,419 --> 00:34:24,159
pods within your cluster just purely

00:34:20,270 --> 00:34:28,520
internal and there's a load balancer

00:34:24,159 --> 00:34:30,800
typically load balancer functionality

00:34:28,520 --> 00:34:33,530
requires natural implementation a load

00:34:30,800 --> 00:34:38,240
balancer so when you create a service

00:34:33,530 --> 00:34:41,450
arm I don't know yes

00:34:38,240 --> 00:34:45,440
or if you create a load balancer service

00:34:41,450 --> 00:34:46,700
it will actually behind the scenes that

00:34:45,440 --> 00:34:50,860
will be integrated into their

00:34:46,700 --> 00:34:56,090
infrastructure if you run a mini-cooper

00:34:50,860 --> 00:34:58,790
kubernetes in the new vm there is that

00:34:56,090 --> 00:35:01,970
functionality already implemented if

00:34:58,790 --> 00:35:03,400
you're doing yourself on a VM you've

00:35:01,970 --> 00:35:05,900
created then you would actually actually

00:35:03,400 --> 00:35:11,690
have to add something like an engine

00:35:05,900 --> 00:35:14,900
eeks to do that load balancing ok

00:35:11,690 --> 00:35:16,910
so note ports basically you user who's

00:35:14,900 --> 00:35:19,610
wanting to access our application will

00:35:16,910 --> 00:35:24,260
attack this well-known input and that

00:35:19,610 --> 00:35:26,680
will be mapped onto if I cheat sorry in

00:35:24,260 --> 00:35:30,380
this case you will actually address

00:35:26,680 --> 00:35:33,220
directly the IP and port of one of these

00:35:30,380 --> 00:35:36,170
nodes okay

00:35:33,220 --> 00:35:39,460
that works but you know you're exposing

00:35:36,170 --> 00:35:39,460
part of your infrastructure

00:35:41,300 --> 00:35:45,980
load balancer on the other hand you

00:35:43,310 --> 00:35:48,040
really have an independent endpoint

00:35:45,980 --> 00:35:50,410
address and it will take care of

00:35:48,040 --> 00:35:57,430
balancing across the nodes of course

00:35:50,410 --> 00:36:02,980
addressing to the specific pods and

00:35:57,430 --> 00:36:06,530
ingress controller so that ingress is

00:36:02,980 --> 00:36:10,400
possible controlling access into your

00:36:06,530 --> 00:36:13,460
cluster and an ingress controller would

00:36:10,400 --> 00:36:15,830
actually be an external element like an

00:36:13,460 --> 00:36:18,560
engine eeks or a checkbox is something

00:36:15,830 --> 00:36:20,930
which will implement the ingress rules

00:36:18,560 --> 00:36:24,140
that you specify about how to access

00:36:20,930 --> 00:36:26,800
your services I don't want to do a demo

00:36:24,140 --> 00:36:31,420
on that there are slides just on

00:36:26,800 --> 00:36:31,420
creation of a load balancer service

00:36:32,410 --> 00:36:37,869
actually what I will do because I had

00:36:35,089 --> 00:36:37,869
prepared that

00:36:38,819 --> 00:36:42,289
it's like an ad

00:36:46,370 --> 00:36:52,100
if I were going to use this application

00:36:48,200 --> 00:36:56,480
I would actually create a service on my

00:36:52,100 --> 00:36:58,520
Redis application here and in my

00:36:56,480 --> 00:37:00,980
architecture this would be basically

00:36:58,520 --> 00:37:06,160
used internally so that my flask

00:37:00,980 --> 00:37:10,280
front-end can attack the Redis database

00:37:06,160 --> 00:37:13,550
and if I wanted to access my flask

00:37:10,280 --> 00:37:20,290
application for outside again I would

00:37:13,550 --> 00:37:20,290
create a service application

00:37:22,190 --> 00:37:26,930
interesting I know see the the replica

00:37:24,680 --> 00:37:32,809
set there as a deaath awesome tupaea

00:37:26,930 --> 00:37:35,720
appearing because of a bug okay I don't

00:37:32,809 --> 00:37:38,180
want to spend long on this I really want

00:37:35,720 --> 00:37:42,650
to get to I'm running after time and I

00:37:38,180 --> 00:37:46,960
want to move on to API gateways and

00:37:42,650 --> 00:37:49,970
service meshes so API gateways

00:37:46,960 --> 00:37:52,039
essentially is an external software

00:37:49,970 --> 00:37:55,190
which will be doing typically things

00:37:52,039 --> 00:37:59,119
like access control load balancing maybe

00:37:55,190 --> 00:38:02,299
reverse proxy functionality and allowing

00:37:59,119 --> 00:38:07,339
to access services within your

00:38:02,299 --> 00:38:10,160
kubernetes cluster without seeing the

00:38:07,339 --> 00:38:13,039
internal structure okay and there are

00:38:10,160 --> 00:38:19,700
things that this can be doing things

00:38:13,039 --> 00:38:22,849
like TLS encryption are essentially

00:38:19,700 --> 00:38:24,710
doing so offload from your cluster you

00:38:22,849 --> 00:38:26,960
don't have to implement these things in

00:38:24,710 --> 00:38:31,750
the micro services but you wrote them as

00:38:26,960 --> 00:38:31,750
the entry into your your cluster

00:38:33,400 --> 00:38:39,040
and actually on the diagram another

00:38:37,450 --> 00:38:40,780
functionality which is important is that

00:38:39,040 --> 00:38:42,910
they might be doing protocol conversion

00:38:40,780 --> 00:38:48,520
as well you might be just wanting to

00:38:42,910 --> 00:38:52,050
expose an HTTP API but internally that's

00:38:48,520 --> 00:38:57,040
going to map onto to rest to RPC

00:38:52,050 --> 00:38:59,830
whatever and there are many examples of

00:38:57,040 --> 00:39:02,700
api gateways which exists I've just

00:38:59,830 --> 00:39:06,730
named a few there nginx aichi proxy

00:39:02,700 --> 00:39:09,910
guess traffic should be in there and

00:39:06,730 --> 00:39:13,090
there's a newer generation based on

00:39:09,910 --> 00:39:16,660
envoi which are being created

00:39:13,090 --> 00:39:18,850
oh c'mon to that moment

00:39:16,660 --> 00:39:21,100
okay I'm personally going quickly so I

00:39:18,850 --> 00:39:24,820
want to get on to the next contender is

00:39:21,100 --> 00:39:27,730
service mesh so one problem with micro

00:39:24,820 --> 00:39:29,680
service is well first of all over the

00:39:27,730 --> 00:39:33,840
network communication aspect is much

00:39:29,680 --> 00:39:38,110
more critical so there are things like

00:39:33,840 --> 00:39:39,460
detection of errors or maybe circuit

00:39:38,110 --> 00:39:43,530
breaking or these sorts of

00:39:39,460 --> 00:39:46,930
functionalities or maybe just logging or

00:39:43,530 --> 00:39:49,750
encryption a lot of functionalities

00:39:46,930 --> 00:39:53,110
which could be common to all your

00:39:49,750 --> 00:39:56,170
functions so you don't want to have to

00:39:53,110 --> 00:40:00,550
implement them in the each micro service

00:39:56,170 --> 00:40:03,040
or basically the micromo nodes and the

00:40:00,550 --> 00:40:06,070
size of those the container image is

00:40:03,040 --> 00:40:08,440
going to be larger so they approach with

00:40:06,070 --> 00:40:12,400
service meshes say okay well we'll

00:40:08,440 --> 00:40:14,370
create a cluster wide capability which

00:40:12,400 --> 00:40:18,970
provided these provides these services

00:40:14,370 --> 00:40:21,600
over the network of course and so this

00:40:18,970 --> 00:40:23,940
reduces a lot of the

00:40:21,600 --> 00:40:29,190
functionality you would need to do

00:40:23,940 --> 00:40:29,190
within the the microservices themselves

00:40:31,960 --> 00:40:37,930
oh yes on one point is you know

00:40:36,070 --> 00:40:39,310
initially with microservices people who

00:40:37,930 --> 00:40:43,720
tend to implement these sorts of things

00:40:39,310 --> 00:40:46,150
but then in libraries but given the one

00:40:43,720 --> 00:40:48,190
advantage or maybe a risk with

00:40:46,150 --> 00:40:52,420
micro-services is that you can the

00:40:48,190 --> 00:40:55,960
polyglot you might do one component in

00:40:52,420 --> 00:41:00,220
Python one and go and in roast etc but

00:40:55,960 --> 00:41:03,130
of course if you're building with common

00:41:00,220 --> 00:41:05,050
libraries nice to have you might be

00:41:03,130 --> 00:41:08,830
having problems then the polyglot

00:41:05,050 --> 00:41:10,270
environment so that's you know it's

00:41:08,830 --> 00:41:13,360
something you don't have with service

00:41:10,270 --> 00:41:14,980
mesh basically it's provided by set of

00:41:13,360 --> 00:41:18,010
containers they're external for your

00:41:14,980 --> 00:41:19,750
actual micro services and the soft load

00:41:18,010 --> 00:41:23,340
functionality is provided through

00:41:19,750 --> 00:41:23,340
sidecar containers

00:41:26,580 --> 00:41:34,400
okay so there are other examples I say

00:41:30,120 --> 00:41:37,140
the two main service meshes around today

00:41:34,400 --> 00:41:43,620
certainly have a lot of traction link Rd

00:41:37,140 --> 00:41:48,020
and sto it was looking for a while like

00:41:43,620 --> 00:41:51,510
SEO is just gaining so much mind chef

00:41:48,020 --> 00:41:52,950
basically supported by part of it came

00:41:51,510 --> 00:41:57,450
from lift already

00:41:52,950 --> 00:42:00,750
I think they created envoi proxy Spotify

00:41:57,450 --> 00:42:03,300
IBM Google and others so you know

00:42:00,750 --> 00:42:05,460
there's a lot of traction behind it but

00:42:03,300 --> 00:42:07,500
there's still a link Rd which has been

00:42:05,460 --> 00:42:10,850
around for longer and I read just a

00:42:07,500 --> 00:42:14,850
couple of days ago they've secured some

00:42:10,850 --> 00:42:17,250
venture capital funding so that's good I

00:42:14,850 --> 00:42:19,620
think you know we're open source we do

00:42:17,250 --> 00:42:23,090
you need more than a solution feature

00:42:19,620 --> 00:42:23,090
these functionalities

00:42:25,560 --> 00:42:34,450
okay so very quickly with an sto

00:42:31,630 --> 00:42:37,030
implementation what you would have is an

00:42:34,450 --> 00:42:40,270
invoice EU is going to run in each of

00:42:37,030 --> 00:42:43,810
your pots I think I won't have the time

00:42:40,270 --> 00:42:45,790
to show a demo or that unfortunately but

00:42:43,810 --> 00:42:48,550
basically when you install this tio you

00:42:45,790 --> 00:42:51,160
can then enable a namespace with in

00:42:48,550 --> 00:42:56,349
kubernetes that any pots that will be

00:42:51,160 --> 00:42:58,390
created will be created in sto mode so

00:42:56,349 --> 00:43:00,760
if I were to recreate my flask

00:42:58,390 --> 00:43:04,660
application I actually did it just

00:43:00,760 --> 00:43:07,450
earlier as a test in an sto enabled

00:43:04,660 --> 00:43:09,609
namespace then we'd find that the

00:43:07,450 --> 00:43:12,070
podrick crates actually has two

00:43:09,609 --> 00:43:14,940
containers automatically there is the

00:43:12,070 --> 00:43:19,570
invoice proxy which is inserted into our

00:43:14,940 --> 00:43:22,420
micro service pardon so in this way

00:43:19,570 --> 00:43:24,880
basically all network communication from

00:43:22,420 --> 00:43:28,990
our micro service is going to pacify

00:43:24,880 --> 00:43:31,740
this in boy proxy and so we've offloaded

00:43:28,990 --> 00:43:35,430
things like encryption functionality

00:43:31,740 --> 00:43:35,430
into the service mesh

00:43:38,560 --> 00:43:48,300
okay so let me just say service mesh

00:43:42,370 --> 00:43:48,300
that there's a lot fraction around that

00:43:48,330 --> 00:43:55,800
service mesh offenders are saying today

00:43:50,530 --> 00:43:59,460
well we still need PPI gateways today

00:43:55,800 --> 00:44:01,600
but maybe not tomorrow so you know

00:43:59,460 --> 00:44:04,900
service mesh is really about this

00:44:01,600 --> 00:44:09,160
east-west traffic between services

00:44:04,900 --> 00:44:11,770
within within your cluster is the API

00:44:09,160 --> 00:44:15,370
gateway is really up top north-south

00:44:11,770 --> 00:44:19,330
traffic into your cluster but sto for

00:44:15,370 --> 00:44:22,000
example think about six months ago they

00:44:19,330 --> 00:44:25,060
added first implementation of Gateway

00:44:22,000 --> 00:44:27,000
functionality within sto so they're

00:44:25,060 --> 00:44:30,040
starting to north-south traffic

00:44:27,000 --> 00:44:35,350
nevertheless there really is there

00:44:30,040 --> 00:44:37,560
really is room for both try to move on

00:44:35,350 --> 00:44:40,330
very quickly

00:44:37,560 --> 00:44:42,940
so there's another sort of API gateway

00:44:40,330 --> 00:44:45,520
which is coming out based on what we

00:44:42,940 --> 00:44:48,550
call the API gateway pattern they

00:44:45,520 --> 00:44:50,530
actually this guy Chris Richardson who

00:44:48,550 --> 00:44:52,390
termed the coin and it's basically

00:44:50,530 --> 00:44:55,210
talking about an API gateway

00:44:52,390 --> 00:44:59,860
functionality where the Gateway

00:44:55,210 --> 00:45:03,550
understands the API is that are passing

00:44:59,860 --> 00:45:05,740
through it and it also maybe understands

00:45:03,550 --> 00:45:09,490
the api's of the infrastructure on which

00:45:05,740 --> 00:45:13,120
it is running and so an example of this

00:45:09,490 --> 00:45:15,370
is a project core glue there's an open

00:45:13,120 --> 00:45:19,630
source project from a company solo dot

00:45:15,370 --> 00:45:24,130
IO and there they are demonstrating how

00:45:19,630 --> 00:45:25,630
they can do hybrid applications where

00:45:24,130 --> 00:45:28,900
because of their understanding of

00:45:25,630 --> 00:45:31,120
different protocols and the API is that

00:45:28,900 --> 00:45:34,660
are being used and the infrastructure

00:45:31,120 --> 00:45:39,430
API they can actually allow so a more

00:45:34,660 --> 00:45:42,570
flexible way of federating across

00:45:39,430 --> 00:45:44,359
existing legacy Samanas Apps

00:45:42,570 --> 00:45:47,380
microservices and then

00:45:44,359 --> 00:45:49,819
like the next step in a way serverless

00:45:47,380 --> 00:45:54,559
service functions could be considered

00:45:49,819 --> 00:45:57,650
the Nano services so I really wanted to

00:45:54,559 --> 00:45:59,119
just I've run out of time but I really

00:45:57,650 --> 00:46:02,210
wanted to get to that and say that

00:45:59,119 --> 00:46:04,309
that's as interesting next step micro

00:46:02,210 --> 00:46:07,759
service evolution so I encourage you to

00:46:04,309 --> 00:46:10,309
have a look I what salute at IO are

00:46:07,759 --> 00:46:13,579
doing they're a small start-up in around

00:46:10,309 --> 00:46:17,890
about 18 months they have several tools

00:46:13,579 --> 00:46:21,920
for helping to migrate to micro services

00:46:17,890 --> 00:46:26,630
okay I basically said what's on this

00:46:21,920 --> 00:46:29,890
slide to summarize I mean micro services

00:46:26,630 --> 00:46:32,180
they offer new deployment possibilities

00:46:29,890 --> 00:46:37,759
really increase in agility the ability

00:46:32,180 --> 00:46:40,420
to deploy to scale and upgrade more

00:46:37,759 --> 00:46:41,680
easily they can also facilitate

00:46:40,420 --> 00:46:45,249
best-in-class

00:46:41,680 --> 00:46:46,430
implementations of components

00:46:45,249 --> 00:46:50,150
nevertheless

00:46:46,430 --> 00:46:53,869
that's useless if you don't adopt best

00:46:50,150 --> 00:46:56,390
practices and and that includes not just

00:46:53,869 --> 00:46:58,579
the design but in terms of your

00:46:56,390 --> 00:47:02,539
organization way these teams work

00:46:58,579 --> 00:47:04,730
together you can do incremental rollout

00:47:02,539 --> 00:47:07,339
with like the Strangler pattern for

00:47:04,730 --> 00:47:11,420
example of course you can do greenfield

00:47:07,339 --> 00:47:13,849
deployment if you're really lucky there

00:47:11,420 --> 00:47:15,980
are as well hybrid approaches like I've

00:47:13,849 --> 00:47:20,749
just shown you during this project

00:47:15,980 --> 00:47:22,789
called glue and then there are it's

00:47:20,749 --> 00:47:26,960
interesting to offload a functionality

00:47:22,789 --> 00:47:29,509
via API data and/or service mesh I would

00:47:26,960 --> 00:47:32,690
maintain that I think for foreseeable

00:47:29,509 --> 00:47:34,900
future we will see both of these being

00:47:32,690 --> 00:47:34,900
used

00:47:34,980 --> 00:47:48,640
okay thank you speak quick and sorry for

00:47:38,470 --> 00:47:51,670
lack of demo any questions okay we are

00:47:48,640 --> 00:47:53,770
literally running out of time so if you

00:47:51,670 --> 00:47:56,230
guys have any question Michael will be

00:47:53,770 --> 00:48:02,950
here today winning hand tomorrow all day

00:47:56,230 --> 00:48:05,050
so please can deck him so please put

00:48:02,950 --> 00:48:07,210
your hand for Michael for a great and

00:48:05,050 --> 00:48:11,610
presentation

00:48:07,210 --> 00:48:11,610

YouTube URL: https://www.youtube.com/watch?v=zavjh89thLU


