Title: Exploit your GPU power with PyCUDA (and friends)
Publication date: 2015-04-19
Playlist: EuroPython 2011
Description: 
	[EuroPython 2011] Stefano Brilli - 22 June 2011 in "Track Spaghetti"
Captions: 
	00:00:07,000 --> 00:00:13,219
Thanks hi everyone I'm happy that so

00:00:10,219 --> 00:00:15,709
many of you have came here to follow my

00:00:13,219 --> 00:00:18,859
talk today I'm going to show you how to

00:00:15,709 --> 00:00:22,280
exploit your GPU power with Baku de and

00:00:18,859 --> 00:00:24,800
friends someone asked me if the entrace

00:00:22,280 --> 00:00:27,170
part means opencl well we are not

00:00:24,800 --> 00:00:29,539
looking at opencl but we are looking at

00:00:27,170 --> 00:00:32,570
a couple of libraries that lets you do

00:00:29,539 --> 00:00:36,410
GPU computing at an ayah leveled my

00:00:32,570 --> 00:00:38,810
poodle we will compare opencl in a

00:00:36,410 --> 00:00:45,260
couple of occasion with cuda and seeing

00:00:38,810 --> 00:00:48,050
the main difference well during the

00:00:45,260 --> 00:00:51,079
preparation of this talk i found a lot

00:00:48,050 --> 00:00:57,200
of documentation a lot of material and i

00:00:51,079 --> 00:01:00,860
get read them in the to this reference

00:00:57,200 --> 00:01:04,399
website i also wrote some code that i

00:01:00,860 --> 00:01:08,600
can show you today but you can go on the

00:01:04,399 --> 00:01:11,900
website and download it and well let's

00:01:08,600 --> 00:01:15,440
see what are the topics for today for

00:01:11,900 --> 00:01:18,650
today i'm going to talk about what is a

00:01:15,440 --> 00:01:22,520
GPU and what is GPU computing a little

00:01:18,650 --> 00:01:27,260
introduction then i will talk about cuda

00:01:22,520 --> 00:01:31,159
and how it works and then i will look

00:01:27,260 --> 00:01:34,700
about by cuda how you can access to the

00:01:31,159 --> 00:01:38,840
cuda and VLC api's from python we will

00:01:34,700 --> 00:01:41,560
see a short example of how i coulda

00:01:38,840 --> 00:01:44,540
works and what are the main features and

00:01:41,560 --> 00:01:48,170
we will talk a little about the tasks

00:01:44,540 --> 00:01:50,240
that around pass on GPU and finally i

00:01:48,170 --> 00:01:52,040
will present you the libraries that's it

00:01:50,240 --> 00:01:55,939
before for doing GPU computing at a

00:01:52,040 --> 00:02:01,490
higher level let's start with the first

00:01:55,939 --> 00:02:04,210
topic and what is a GPU well GPU is the

00:02:01,490 --> 00:02:07,939
crowning for graphic processing unit and

00:02:04,210 --> 00:02:10,759
they were firstly designed to help the

00:02:07,939 --> 00:02:13,040
CPU in doing graphic rendering

00:02:10,759 --> 00:02:15,950
operations they are found in all modern

00:02:13,040 --> 00:02:18,590
home almost all modern devices and

00:02:15,950 --> 00:02:19,610
personal computer due to the high

00:02:18,590 --> 00:02:22,610
parallel natural

00:02:19,610 --> 00:02:25,550
of the graphic rendering operations they

00:02:22,610 --> 00:02:27,860
became more and more multi-core

00:02:25,550 --> 00:02:30,440
processor they increase the number of

00:02:27,860 --> 00:02:36,530
their course to do this operation past

00:02:30,440 --> 00:02:38,990
and if you look at the GPU you can find

00:02:36,530 --> 00:02:42,560
some difference with the with the

00:02:38,990 --> 00:02:46,100
respect to the CPU they meet behind me

00:02:42,560 --> 00:02:52,340
show how best to processing unit differs

00:02:46,100 --> 00:02:54,709
by themselves the CPU try to achieve the

00:02:52,340 --> 00:02:57,950
best performance rely on a big and fast

00:02:54,709 --> 00:03:01,310
Kashian systems why and in a complex

00:02:57,950 --> 00:03:03,770
flow control while the GPU try to

00:03:01,310 --> 00:03:07,820
achieve the best arithmetic approval to

00:03:03,770 --> 00:03:11,500
relying on a grid number of arithmetical

00:03:07,820 --> 00:03:15,050
logical units and try to keep simple the

00:03:11,500 --> 00:03:19,160
control system and the memory cache and

00:03:15,050 --> 00:03:21,320
the caching system so when the GPUs

00:03:19,160 --> 00:03:24,500
overcame their limits of doing just

00:03:21,320 --> 00:03:27,080
graphic rendering operations this

00:03:24,500 --> 00:03:32,209
leading to the born of the GPU computing

00:03:27,080 --> 00:03:37,549
to the birther the GPU computing in GPU

00:03:32,209 --> 00:03:43,540
computing you the GPU work as a

00:03:37,549 --> 00:03:47,180
coprocessor for the cpu and in your

00:03:43,540 --> 00:03:50,150
application you can move the tasks that

00:03:47,180 --> 00:03:55,310
can be a securing parallel to the GPU

00:03:50,150 --> 00:03:57,110
and achieve the and exploit the haim

00:03:55,310 --> 00:04:00,080
with the core architecture a tribute to

00:03:57,110 --> 00:04:02,299
achieve some speed ups however this

00:04:00,080 --> 00:04:05,600
speed ups are little and limited by the

00:04:02,299 --> 00:04:08,890
handle slow so GPU computing as more

00:04:05,600 --> 00:04:12,230
sense if you said when your application

00:04:08,890 --> 00:04:14,450
do a lot of parallel computation the

00:04:12,230 --> 00:04:17,019
main time consuming the task of your

00:04:14,450 --> 00:04:20,150
application is a parallel acceptable one

00:04:17,019 --> 00:04:22,640
also in GPU computing you have to

00:04:20,150 --> 00:04:24,979
consider some overheads over ads that

00:04:22,640 --> 00:04:27,410
are given by the memory transfer and the

00:04:24,979 --> 00:04:30,440
communication between the CPU and GPU

00:04:27,410 --> 00:04:31,200
you have transferred the input data to

00:04:30,440 --> 00:04:33,420
the GPU

00:04:31,200 --> 00:04:35,480
and then do the computation the GPU and

00:04:33,420 --> 00:04:42,210
then finally transferred back to the CPU

00:04:35,480 --> 00:04:46,640
the results of your computation so how

00:04:42,210 --> 00:04:51,330
CUDA is related in this car out Buddha

00:04:46,640 --> 00:04:54,480
is related to this context CUDA is an

00:04:51,330 --> 00:05:01,530
nvidia technology the lets you do GPU

00:04:54,480 --> 00:05:03,390
computing it was related since 2006 and

00:05:01,530 --> 00:05:06,120
now it's widely diffused in

00:05:03,390 --> 00:05:09,060
high-performance computing environments

00:05:06,120 --> 00:05:12,540
and also in some and user software on

00:05:09,060 --> 00:05:15,210
the nvidia dhadkan site you can find a

00:05:12,540 --> 00:05:17,460
showcase that lists all the libraries

00:05:15,210 --> 00:05:19,770
and the soft many of the libraries and

00:05:17,460 --> 00:05:24,600
many of the software that relies on cuda

00:05:19,770 --> 00:05:26,640
to achieve high speed apps however CUDA

00:05:24,600 --> 00:05:28,710
is not the only technology to do GPU

00:05:26,640 --> 00:05:32,480
computing the other most famous

00:05:28,710 --> 00:05:35,400
alternative is offensive opencl is

00:05:32,480 --> 00:05:39,240
library that lets you write application

00:05:35,400 --> 00:05:49,320
that scale on parallel application the

00:05:39,240 --> 00:05:52,770
scale on the jet ski and they work on a

00:05:49,320 --> 00:05:55,350
notorious atherogenesis number of

00:05:52,770 --> 00:05:59,880
architecture so it's like you write once

00:05:55,350 --> 00:06:02,550
run everywhere opencl is very similar to

00:05:59,880 --> 00:06:05,550
cuda if you we talked about the

00:06:02,550 --> 00:06:08,370
programming model there are they are

00:06:05,550 --> 00:06:11,900
very similar there are also automatic

00:06:08,370 --> 00:06:15,240
tools that can you that you can use

00:06:11,900 --> 00:06:18,990
translate the cuda programs to the

00:06:15,240 --> 00:06:24,210
opencl programs and vice versa I link at

00:06:18,990 --> 00:06:27,410
the one to Don my reference site and so

00:06:24,210 --> 00:06:30,990
the first question that you may ask is

00:06:27,410 --> 00:06:33,810
why using one instead of another why

00:06:30,990 --> 00:06:37,200
using a fancy Allah instead akuto could

00:06:33,810 --> 00:06:39,960
instead opencl a sense that if we talk

00:06:37,200 --> 00:06:43,990
about performance they are almost the

00:06:39,960 --> 00:06:48,100
same they

00:06:43,990 --> 00:06:51,250
maintainer of the pie khuda Empire

00:06:48,100 --> 00:06:54,970
opencl bindings as compiled the list

00:06:51,250 --> 00:06:58,660
when we missed the pro and cons of each

00:06:54,970 --> 00:07:01,270
technology it finally says that there

00:06:58,660 --> 00:07:05,500
are no strong reason to use one instead

00:07:01,270 --> 00:07:08,530
of another in my opinion there are still

00:07:05,500 --> 00:07:11,010
some reason to use CUDA in a production

00:07:08,530 --> 00:07:14,800
environment there are a couple of reason

00:07:11,010 --> 00:07:18,670
the first reason is the maturity of CUDA

00:07:14,800 --> 00:07:21,490
with respect opencl as i said before q

00:07:18,670 --> 00:07:23,080
de is being related since 2006 and now

00:07:21,490 --> 00:07:25,570
is why i use it in high performance

00:07:23,080 --> 00:07:28,360
computing and is tested on a wide range

00:07:25,570 --> 00:07:31,750
of software and opencl lack sticks

00:07:28,360 --> 00:07:34,540
mature-looking and the second reason is

00:07:31,750 --> 00:07:36,490
suffered because for the q de

00:07:34,540 --> 00:07:41,050
technologie the support is provided by

00:07:36,490 --> 00:07:44,620
the nvidia while for opencl every other

00:07:41,050 --> 00:07:47,020
producer must provide his son its own

00:07:44,620 --> 00:07:49,960
support to the open sea open sea el

00:07:47,020 --> 00:07:53,710
technology and in many cases i found

00:07:49,960 --> 00:07:57,300
that this support is a updated or maybe

00:07:53,710 --> 00:08:00,760
is the driver that support opengl are

00:07:57,300 --> 00:08:02,920
still in beta release or does not work

00:08:00,760 --> 00:08:05,830
very well so this is why for a

00:08:02,920 --> 00:08:18,010
production environment I feel looking at

00:08:05,830 --> 00:08:21,430
could afford doing GPU computing okay so

00:08:18,010 --> 00:08:23,530
this I think another difference opens

00:08:21,430 --> 00:08:25,630
you're running on the radeon cards as

00:08:23,530 --> 00:08:29,590
far as i know the radeon cards are

00:08:25,630 --> 00:08:32,350
faster with integer math and nvidia's

00:08:29,590 --> 00:08:33,340
gtx cards are faster at floating point

00:08:32,350 --> 00:08:35,830
particularly double-precision

00:08:33,340 --> 00:08:39,220
floating-point and i think have better i

00:08:35,830 --> 00:08:40,480
triple e accuracy as well so there there

00:08:39,220 --> 00:08:42,280
may be a difference there if you're

00:08:40,480 --> 00:08:44,350
choosing a card as to whether you go

00:08:42,280 --> 00:08:46,750
with cooter and nvidia and opencl and

00:08:44,350 --> 00:08:51,940
ati scar this is about surrounding an

00:08:46,750 --> 00:08:53,830
open CL program okay there are a work

00:08:51,940 --> 00:08:56,950
that I linked it on my reference site

00:08:53,830 --> 00:09:00,520
about the comparison from

00:08:56,950 --> 00:09:04,080
and opencl the program that ran on the

00:09:00,520 --> 00:09:07,390
Nvidia and donor T graphic cards and

00:09:04,080 --> 00:09:10,780
they show that the other graphic cards

00:09:07,390 --> 00:09:15,250
yes ground faster on some kind of tasks

00:09:10,780 --> 00:09:17,890
but many of the programs that run faster

00:09:15,250 --> 00:09:20,710
than the nvidia ones are right and not

00:09:17,890 --> 00:09:29,320
in opencl but in a day intermediate

00:09:20,710 --> 00:09:31,510
language and the opencl program they are

00:09:29,320 --> 00:09:33,520
compiled to an intermediate language and

00:09:31,510 --> 00:09:35,020
then it's acute it by the GPU you can

00:09:33,520 --> 00:09:38,110
upload and they are automatically

00:09:35,020 --> 00:09:40,660
translated and executed on the GPU the

00:09:38,110 --> 00:09:43,390
same thing is for the nvidia cards the

00:09:40,660 --> 00:09:48,310
video graphic cards when you compile to

00:09:43,390 --> 00:09:50,830
Deputy X format and then most of the

00:09:48,310 --> 00:09:53,880
performers are again at if you write in

00:09:50,830 --> 00:10:01,150
or you modify this intermediate language

00:09:53,880 --> 00:10:04,030
okay so let's see the next slide oh well

00:10:01,150 --> 00:10:08,230
I want to show to you how CUDA

00:10:04,030 --> 00:10:10,180
programming model works and well Austin

00:10:08,230 --> 00:10:11,680
device at the two key words that they

00:10:10,180 --> 00:10:14,800
could a programming model used to

00:10:11,680 --> 00:10:16,540
identify the CPU related resource and

00:10:14,800 --> 00:10:21,510
the device rate the resource likes

00:10:16,540 --> 00:10:25,090
threads and memory CUDA lets you define

00:10:21,510 --> 00:10:26,920
functions that will be executed on the

00:10:25,090 --> 00:10:29,410
device instead of the aust at the

00:10:26,920 --> 00:10:35,100
runtime and this function are colored

00:10:29,410 --> 00:10:39,520
kernel and you can decide how many tres

00:10:35,100 --> 00:10:42,070
will be executing these functions so you

00:10:39,520 --> 00:10:44,770
can launch a function and device and

00:10:42,070 --> 00:10:47,200
then executing make it executed by a

00:10:44,770 --> 00:10:52,470
certain number of threads each of these

00:10:47,200 --> 00:11:00,010
threads as an ID an ID to identify the

00:10:52,470 --> 00:11:03,490
threads inside the nid this heads are

00:11:00,010 --> 00:11:05,830
then divided in blocks each thread as an

00:11:03,490 --> 00:11:08,530
ID and each block as an ID and all the

00:11:05,830 --> 00:11:09,470
blocks format the execution grid so we

00:11:08,530 --> 00:11:12,440
have

00:11:09,470 --> 00:11:17,060
a lot of threads that are executing the

00:11:12,440 --> 00:11:20,450
same kernel function and you can fetch

00:11:17,060 --> 00:11:25,900
they d by the inside of the your kernel

00:11:20,450 --> 00:11:29,200
to know how I which thread you are and

00:11:25,900 --> 00:11:31,790
to fetch the memory and to choose what

00:11:29,200 --> 00:11:35,990
what are the competition's that you have

00:11:31,790 --> 00:11:38,090
to do inside the your thread during

00:11:35,990 --> 00:11:41,870
their life threads can access to

00:11:38,090 --> 00:11:43,760
different memory spaces the first memory

00:11:41,870 --> 00:11:48,010
space that the thread can access to is

00:11:43,760 --> 00:11:51,200
the local memory space it's a space that

00:11:48,010 --> 00:11:55,660
isn't accessible by any other threads in

00:11:51,200 --> 00:12:02,360
the execution grid or in the same block

00:11:55,660 --> 00:12:04,460
second memory space is a tertiary April

00:12:02,360 --> 00:12:07,760
despair block shared memory space and

00:12:04,460 --> 00:12:10,190
it's a space that can be used to share

00:12:07,760 --> 00:12:12,080
the data the results of the computations

00:12:10,190 --> 00:12:14,810
between the threads of the same blocks

00:12:12,080 --> 00:12:18,290
you can do communication inside this

00:12:14,810 --> 00:12:21,380
space the last space that the tribe can

00:12:18,290 --> 00:12:23,690
access is the global memory space each

00:12:21,380 --> 00:12:26,030
strategy of the grid can access this

00:12:23,690 --> 00:12:28,640
global memory and if this global

00:12:26,030 --> 00:12:30,980
memories is the only memory space that

00:12:28,640 --> 00:12:34,490
can be accessed at all so bad so they us

00:12:30,980 --> 00:12:36,830
put the data in the global memory then

00:12:34,490 --> 00:12:39,370
each thread can fetch the data from the

00:12:36,830 --> 00:12:42,110
global memory do that computation using

00:12:39,370 --> 00:12:44,240
maybe the local memory or the shared

00:12:42,110 --> 00:12:46,670
memory and then put the results in the

00:12:44,240 --> 00:12:50,450
global memory again and the Earth's can

00:12:46,670 --> 00:12:52,910
fetch the results back to the CPU so

00:12:50,450 --> 00:12:57,800
this is how could a programming model

00:12:52,910 --> 00:13:00,020
works well since we are at in Europe

00:12:57,800 --> 00:13:04,400
icon we want to see how to access so the

00:13:00,020 --> 00:13:08,150
DS from python and one answer is by

00:13:04,400 --> 00:13:12,950
khuda hai khuda is a library the lets

00:13:08,150 --> 00:13:15,790
you access to the nvidia see api's let

00:13:12,950 --> 00:13:21,170
you access to the nvidia cuda technology

00:13:15,790 --> 00:13:23,330
in a very easy manner you have a lot of

00:13:21,170 --> 00:13:27,450
fish would

00:13:23,330 --> 00:13:29,490
make you easy to write the progress to

00:13:27,450 --> 00:13:33,810
make you easy to write programs that

00:13:29,490 --> 00:13:36,020
exploit the cuter technology it work

00:13:33,810 --> 00:13:39,570
like Python library should work

00:13:36,020 --> 00:13:42,810
providing you some visual like automatic

00:13:39,570 --> 00:13:45,930
object cleanup and I drove checking and

00:13:42,810 --> 00:13:48,840
a trap and translating into exception so

00:13:45,930 --> 00:13:51,980
you can just do a simply try catch to

00:13:48,840 --> 00:13:54,960
under some error and also it has

00:13:51,980 --> 00:13:59,100
official that is a killer feature of pi

00:13:54,960 --> 00:14:02,520
cuda and is meta programming with the

00:13:59,100 --> 00:14:06,290
nvidia see api's you can do meta

00:14:02,520 --> 00:14:09,330
programming a while on pakoda you can

00:14:06,290 --> 00:14:11,820
write you can generate your code at the

00:14:09,330 --> 00:14:15,810
runtime and then compile and applaud to

00:14:11,820 --> 00:14:20,370
the GPU exploiting these this fissure

00:14:15,810 --> 00:14:25,910
and that it's something that you can do

00:14:20,370 --> 00:14:25,910
with then videos see api's opencl can

00:14:26,570 --> 00:14:36,180
well to see up i put the words i

00:14:31,190 --> 00:14:38,940
prepared a very simple example and it's

00:14:36,180 --> 00:14:41,910
about the sum of two vectors it's a very

00:14:38,940 --> 00:14:44,910
easy it's very simple task and the

00:14:41,910 --> 00:14:48,540
classical way to account to resolve this

00:14:44,910 --> 00:14:50,550
task is to looping all over the elements

00:14:48,540 --> 00:14:53,580
of these two vectors and win the Sun and

00:14:50,550 --> 00:14:55,950
then put the results in an R vector it's

00:14:53,580 --> 00:14:58,680
easy on the GPU we use a different

00:14:55,950 --> 00:15:03,690
approach to solving this task and we

00:14:58,680 --> 00:15:08,880
have that each thread the H thread do

00:15:03,690 --> 00:15:11,580
just one Sam and we launch entry to

00:15:08,880 --> 00:15:15,450
cover all the elements of the array so

00:15:11,580 --> 00:15:18,180
when the colonel execution terminate we

00:15:15,450 --> 00:15:22,560
have all the results in our destination

00:15:18,180 --> 00:15:27,270
vector if you try to write down a

00:15:22,560 --> 00:15:29,850
program that do this simple task it may

00:15:27,270 --> 00:15:34,650
takes you some more line of what you are

00:15:29,850 --> 00:15:35,910
expecting and this is different try to

00:15:34,650 --> 00:15:39,900
read this code

00:15:35,910 --> 00:15:45,420
zooming all the older part of this code

00:15:39,900 --> 00:15:47,970
and we start from the bottom it's a you

00:15:45,420 --> 00:15:51,510
there are about the fifty percent of

00:15:47,970 --> 00:15:54,540
code less in the left part where where

00:15:51,510 --> 00:15:56,910
there is the nvidia capi program and on

00:15:54,540 --> 00:16:00,330
the rack part we have the PI cuda

00:15:56,910 --> 00:16:03,300
program there are fifty percent less

00:16:00,330 --> 00:16:07,470
cool but we are zooming the part

00:16:03,300 --> 00:16:11,360
starting from the top the first part is

00:16:07,470 --> 00:16:14,880
both in bot program is dedicated to the

00:16:11,360 --> 00:16:17,690
to include at the imports while on the

00:16:14,880 --> 00:16:22,890
nvidia capi program we just need to

00:16:17,690 --> 00:16:27,120
include the hooded oth hitter to right

00:16:22,890 --> 00:16:29,430
our hood up our cooler programs on the

00:16:27,120 --> 00:16:31,890
pie food in the Fukuda programs we have

00:16:29,430 --> 00:16:34,320
to import some module the first mode

00:16:31,890 --> 00:16:37,440
that we import is the PI CUDA dot out in

00:16:34,320 --> 00:16:40,170
it that performs all the operation that

00:16:37,440 --> 00:16:44,490
are needed to execute some code on the

00:16:40,170 --> 00:16:47,820
on your kulla device then we import our

00:16:44,490 --> 00:16:49,770
PI cooler driver that contains a lot of

00:16:47,820 --> 00:16:52,970
function that are needed to write our

00:16:49,770 --> 00:16:57,420
program we had seen them yet in this

00:16:52,970 --> 00:17:00,330
presentation then we import a source

00:16:57,420 --> 00:17:03,570
model class that is the class that will

00:17:00,330 --> 00:17:07,110
handle our code we are seeing the next

00:17:03,570 --> 00:17:10,320
slide how is class work and finally we

00:17:07,110 --> 00:17:13,530
import numpy I forget to say before that

00:17:10,320 --> 00:17:16,110
pi CUDA is tightly integrated with amp i

00:17:13,530 --> 00:17:20,010
we use the umpire to manage our erase

00:17:16,110 --> 00:17:22,800
our memory to data as to be copied to

00:17:20,010 --> 00:17:27,270
the GPU on which we do competition so we

00:17:22,800 --> 00:17:30,000
import up I to manage our two vectors in

00:17:27,270 --> 00:17:32,610
the next slide we are seeing the our

00:17:30,000 --> 00:17:36,180
cannon this is the function that will be

00:17:32,610 --> 00:17:39,180
executed on the device and do and is

00:17:36,180 --> 00:17:43,020
extremely simple we do what I've seen

00:17:39,180 --> 00:17:45,600
seen before it does the take the

00:17:43,020 --> 00:17:48,090
threaded in which we are and then some

00:17:45,600 --> 00:17:50,750
the corresponding true elements

00:17:48,090 --> 00:17:54,299
the vector and put the result in a

00:17:50,750 --> 00:17:57,120
resultant vector one beautiful thing of

00:17:54,299 --> 00:18:00,390
Pi CUDA is that this function is exactly

00:17:57,120 --> 00:18:04,140
the same on the nvidia capi program and

00:18:00,390 --> 00:18:06,840
on the pipe udah program we just need to

00:18:04,140 --> 00:18:08,580
wrap the dysfunction with the source

00:18:06,840 --> 00:18:11,549
model class and this class we

00:18:08,580 --> 00:18:14,549
automatically chrome compile and upload

00:18:11,549 --> 00:18:16,080
to the GPU at the runtime so you can

00:18:14,549 --> 00:18:19,049
understand how to do meta programming

00:18:16,080 --> 00:18:22,080
you can basically upload to the GPU

00:18:19,049 --> 00:18:24,480
compile and upload any kind of string

00:18:22,080 --> 00:18:29,630
any kind of program that you want to

00:18:24,480 --> 00:18:32,490
generate in the next slide we are doing

00:18:29,630 --> 00:18:34,830
we are I brought a check I wrote

00:18:32,490 --> 00:18:38,700
function this is why every function in

00:18:34,830 --> 00:18:40,890
the end vse api's returns an error code

00:18:38,700 --> 00:18:43,890
these are our code is obviously to be

00:18:40,890 --> 00:18:47,120
check if to make the things work right

00:18:43,890 --> 00:18:51,020
and I write a function that to this

00:18:47,120 --> 00:18:53,750
haiku that does not need this picture

00:18:51,020 --> 00:18:56,309
does not ease the error checking because

00:18:53,750 --> 00:18:59,309
automatically can catch and translate

00:18:56,309 --> 00:19:05,370
the hair roots in into attraction so we

00:18:59,309 --> 00:19:10,320
can just handle it with a try-catch in

00:19:05,370 --> 00:19:14,370
this slide we are just declaring our

00:19:10,320 --> 00:19:17,220
arrays we do a simple Malik on the

00:19:14,370 --> 00:19:20,580
nvidia capi program and we use the numpy

00:19:17,220 --> 00:19:25,399
for the pie cuda program it's quite

00:19:20,580 --> 00:19:29,070
simple and in the next slide we are

00:19:25,399 --> 00:19:32,820
coping in the video capi program we are

00:19:29,070 --> 00:19:36,990
copying the contents of our vectors to

00:19:32,820 --> 00:19:41,940
the GPU using the cuda ama lak function

00:19:36,990 --> 00:19:44,880
this is not needed on my huda you don't

00:19:41,940 --> 00:19:47,700
need to exploit lee especially move the

00:19:44,880 --> 00:19:49,440
memory from the earth to the device

00:19:47,700 --> 00:19:53,600
because we are seeing the next slide I

00:19:49,440 --> 00:19:53,600
hope I could I can do this automatically

00:19:54,169 --> 00:19:59,850
we are at the colonel lunch so each of

00:19:58,260 --> 00:20:03,150
the programs can

00:19:59,850 --> 00:20:07,380
figure configure that Colonel specifying

00:20:03,150 --> 00:20:11,270
how how many threads where blocks as to

00:20:07,380 --> 00:20:15,210
belong said 250 seats and then

00:20:11,270 --> 00:20:21,630
specifying also how many blocks you want

00:20:15,210 --> 00:20:24,059
to be launched you want lunch and after

00:20:21,630 --> 00:20:26,160
they can execution on the nvidia capi

00:20:24,059 --> 00:20:28,380
program we copy back to the austere

00:20:26,160 --> 00:20:30,809
results while hi khuda as i said before

00:20:28,380 --> 00:20:33,720
can handle this automatically it can

00:20:30,809 --> 00:20:36,740
undred this by a specifically using the

00:20:33,720 --> 00:20:40,860
driver function driver out to driver in

00:20:36,740 --> 00:20:43,740
which a user trapper your vector this

00:20:40,860 --> 00:20:46,950
function tells which are their race that

00:20:43,740 --> 00:20:49,470
has to be copied the GPU before the

00:20:46,950 --> 00:20:51,419
colonel lounge and which are their race

00:20:49,470 --> 00:20:54,500
that has to be copied back to the cpu

00:20:51,419 --> 00:21:00,960
after the candidates finish that is a

00:20:54,500 --> 00:21:03,929
execution after that we have 0 we check

00:21:00,960 --> 00:21:08,130
our results too we do the same operation

00:21:03,929 --> 00:21:12,480
the cpu to see if the things are going

00:21:08,130 --> 00:21:16,010
to ripe and we check adapted results on

00:21:12,480 --> 00:21:20,760
the cpu and the GPU are matching and

00:21:16,010 --> 00:21:23,100
finally we freeze artists and on them

00:21:20,760 --> 00:21:25,710
via capi programmer we have to do the

00:21:23,100 --> 00:21:27,179
free and the free operation while on

00:21:25,710 --> 00:21:29,159
pakoda you don't need to do this

00:21:27,179 --> 00:21:33,299
operation because the object are

00:21:29,159 --> 00:21:36,780
automatically collected automatically

00:21:33,299 --> 00:21:41,340
collected and this is a simple example

00:21:36,780 --> 00:21:44,220
of how pi could have work one

00:21:41,340 --> 00:21:48,750
interesting thing is that if you try to

00:21:44,220 --> 00:21:51,150
run this example on your GPU you won't

00:21:48,750 --> 00:21:55,260
get any speed ups you want to get maybe

00:21:51,150 --> 00:21:58,350
a little speed up because some English

00:21:55,260 --> 00:22:02,340
rube actors is a two simple task for a

00:21:58,350 --> 00:22:06,990
GPU it's more like moving memory that

00:22:02,340 --> 00:22:09,000
doing the Sun because you have to move

00:22:06,990 --> 00:22:12,059
your data from the austere to the divide

00:22:09,000 --> 00:22:12,940
the device and then extracted fetch both

00:22:12,059 --> 00:22:15,160
the

00:22:12,940 --> 00:22:18,460
elements of the array and then yes

00:22:15,160 --> 00:22:19,720
finally to sam and then yes to write to

00:22:18,460 --> 00:22:21,310
the global memory and then they all

00:22:19,720 --> 00:22:23,710
started to fetch from the global memory

00:22:21,310 --> 00:22:25,840
the results back it's a lot of memory

00:22:23,710 --> 00:22:30,400
transfer poor job doing just one Sam so

00:22:25,840 --> 00:22:33,030
this is why it's not you got not be data

00:22:30,400 --> 00:22:37,150
from these from this operation

00:22:33,030 --> 00:22:40,480
unfortunately in most real-world pieces

00:22:37,150 --> 00:22:44,020
you have to face some more complex some

00:22:40,480 --> 00:22:49,810
more complex problems about doing

00:22:44,020 --> 00:22:55,840
justice am of shoe vector this problem

00:22:49,810 --> 00:23:00,760
that I see now is about that it medical

00:22:55,840 --> 00:23:05,950
intensity it medical intensity is when

00:23:00,760 --> 00:23:09,130
you have a few at medical operation and

00:23:05,950 --> 00:23:11,650
a lot of memory transfers it's not good

00:23:09,130 --> 00:23:13,780
for GPU because you are limited by the

00:23:11,650 --> 00:23:17,080
bandwidth of the membrane and you can

00:23:13,780 --> 00:23:24,910
exploit your GPU power YouTube you Eric

00:23:17,080 --> 00:23:29,740
metical throughput well I practical

00:23:24,910 --> 00:23:33,790
intensity I was it before in real-world

00:23:29,740 --> 00:23:38,920
tasks you have to write some or compass

00:23:33,790 --> 00:23:43,180
program usually and it's it may be easy

00:23:38,920 --> 00:23:45,760
to write a kernel that dude all these

00:23:43,180 --> 00:23:50,050
your computation your task the Soviet

00:23:45,760 --> 00:23:53,110
ask that Sabri task and do the right

00:23:50,050 --> 00:23:56,290
things but it may be harder to write a

00:23:53,110 --> 00:24:02,080
kernel that run fast that give you some

00:23:56,290 --> 00:24:06,960
speed ups this is why writing a book

00:24:02,080 --> 00:24:12,130
good colonel is a matter of that cone

00:24:06,960 --> 00:24:14,740
that group many topics the first thing

00:24:12,130 --> 00:24:17,260
that you have to know to write a book

00:24:14,740 --> 00:24:20,920
kernel is the CUDA programming model and

00:24:17,260 --> 00:24:25,610
I just do a little brief but in the

00:24:20,920 --> 00:24:29,000
tales I skippered many details and

00:24:25,610 --> 00:24:30,860
I get to know how work liquid a premium

00:24:29,000 --> 00:24:32,780
water you have to know how works the

00:24:30,860 --> 00:24:35,960
underlying architecture and you have to

00:24:32,780 --> 00:24:41,710
know how the CUDA programming model is

00:24:35,960 --> 00:24:44,420
mapped on the architecture this is our

00:24:41,710 --> 00:24:47,030
basic things you have to know to do a

00:24:44,420 --> 00:24:51,799
good work of the optimization to write a

00:24:47,030 --> 00:24:54,260
good corner so you can press if you have

00:24:51,799 --> 00:24:56,660
a task or so you can start writing the

00:24:54,260 --> 00:24:58,669
first Garner you have in mind and if you

00:24:56,660 --> 00:25:02,630
know these all these things that exceed

00:24:58,669 --> 00:25:07,640
you can just modify your code

00:25:02,630 --> 00:25:14,540
step-by-step to achieve some speed ups

00:25:07,640 --> 00:25:19,160
and a good work of this kind is done by

00:25:14,540 --> 00:25:26,000
Mark Harris marker is a guy that works

00:25:19,160 --> 00:25:28,190
at an nvidia and it's a it made a my

00:25:26,000 --> 00:25:32,330
opinion one of the most beautiful work

00:25:28,190 --> 00:25:35,809
of who the optimization it sure you can

00:25:32,330 --> 00:25:37,850
take a reduction reduction operation you

00:25:35,809 --> 00:25:41,809
make you know what is with our

00:25:37,850 --> 00:25:44,240
reductions and it starts from a simple

00:25:41,809 --> 00:25:47,960
colonel from the first canna he has mine

00:25:44,240 --> 00:25:53,600
and then he starts then he step-by-step

00:25:47,960 --> 00:25:55,750
modified the code to adapt the it to the

00:25:53,600 --> 00:25:59,540
underlying architecture it show what you

00:25:55,750 --> 00:26:03,860
modify and how and why we modify the

00:25:59,540 --> 00:26:07,070
things and if finally get the 30 time

00:26:03,860 --> 00:26:10,429
speed ups from the resin with respect to

00:26:07,070 --> 00:26:12,679
the original colonel that he wrote and I

00:26:10,429 --> 00:26:16,010
think that this work on my website and

00:26:12,679 --> 00:26:18,470
if you are planning to start using who

00:26:16,010 --> 00:26:21,980
does start leaning Buddha i suggest you

00:26:18,470 --> 00:26:25,250
to look at this example because it's a

00:26:21,980 --> 00:26:34,460
very clarifying example about how you

00:26:25,250 --> 00:26:36,130
can write good colonel well in the hand

00:26:34,460 --> 00:26:40,360
before

00:26:36,130 --> 00:26:43,120
I wanted to show you a couple of

00:26:40,360 --> 00:26:47,350
libraries better let you do GPU

00:26:43,120 --> 00:26:51,880
computing more easily than knowing all

00:26:47,350 --> 00:26:54,490
this stuff that I listed the first

00:26:51,880 --> 00:26:57,040
library that I want to show you is also

00:26:54,490 --> 00:27:00,190
the most exciting in my opinion and is

00:26:57,040 --> 00:27:04,300
called cooperate cooperate is a library

00:27:00,190 --> 00:27:08,380
that lets you do GPU computing using

00:27:04,300 --> 00:27:11,410
just simply simple Python signups you

00:27:08,380 --> 00:27:17,830
don't have to know other language and

00:27:11,410 --> 00:27:19,480
let's see an example okay to do to do in

00:27:17,830 --> 00:27:24,160
GPU computing with copper add is a

00:27:19,480 --> 00:27:26,980
simple as you have to just import the

00:27:24,160 --> 00:27:31,780
names of the copyright module and then

00:27:26,980 --> 00:27:34,690
we can defy define our function a i

00:27:31,780 --> 00:27:39,280
defined the simply x that's why function

00:27:34,690 --> 00:27:41,890
and i define this function using a

00:27:39,280 --> 00:27:44,230
standard list comprehension syntax using

00:27:41,890 --> 00:27:46,360
standard zip operator that is an

00:27:44,230 --> 00:27:49,690
operator that is right over right and by

00:27:46,360 --> 00:27:52,510
evening let i imported cop rat model

00:27:49,690 --> 00:27:55,870
from the popular model and then i can

00:27:52,510 --> 00:27:59,560
define this function and i can declare

00:27:55,870 --> 00:28:05,080
vectors as a standard packing list and

00:27:59,560 --> 00:28:08,200
then I call the ax plus y function this

00:28:05,080 --> 00:28:11,430
function will be automatically executed

00:28:08,200 --> 00:28:15,220
on the GPU instead of the Austin this is

00:28:11,430 --> 00:28:17,740
this magic is done just by the decorator

00:28:15,220 --> 00:28:20,380
ATS you that we see are above the

00:28:17,740 --> 00:28:23,320
function and this decree told the

00:28:20,380 --> 00:28:26,830
greater tells Copperhead that he added

00:28:23,320 --> 00:28:29,590
function has to be translated to a CUDA

00:28:26,830 --> 00:28:33,220
kernel and then it compile and upload to

00:28:29,590 --> 00:28:37,120
GPU and then it's acute on the device

00:28:33,220 --> 00:28:40,990
without that you know you don't need to

00:28:37,120 --> 00:28:44,950
know what is behind this but let's see

00:28:40,990 --> 00:28:48,660
some details about copperhead well as i

00:28:44,950 --> 00:28:48,660
said before corporate can

00:28:48,850 --> 00:28:56,840
works with just the Python syntax a

00:28:53,240 --> 00:29:00,530
subset of the Python syntax you can use

00:28:56,840 --> 00:29:03,020
not own the function of button obviously

00:29:00,530 --> 00:29:05,150
but you can use lambda you can use map

00:29:03,020 --> 00:29:07,600
you can use zip you can use a lot of

00:29:05,150 --> 00:29:11,330
function that the cooperate as

00:29:07,600 --> 00:29:13,760
overwritten and to write it when you

00:29:11,330 --> 00:29:17,210
define your function that will be

00:29:13,760 --> 00:29:21,500
executed on the GPU it actually has some

00:29:17,210 --> 00:29:23,660
limitations you can work on a bias kind

00:29:21,500 --> 00:29:25,700
of array you can work on list you can

00:29:23,660 --> 00:29:28,280
work a list of lists you can list of

00:29:25,700 --> 00:29:33,770
tuples but you can work on array of

00:29:28,280 --> 00:29:35,720
clays classes and meka Crassus also when

00:29:33,770 --> 00:29:38,210
you are defined your function you have

00:29:35,720 --> 00:29:44,900
to adopt a strong type in policy so your

00:29:38,210 --> 00:29:48,620
function can return values like 10 to

00:29:44,900 --> 00:29:51,860
force but not you can but not the one or

00:29:48,620 --> 00:29:53,930
foods you can do this this strong typing

00:29:51,860 --> 00:29:56,510
policy is needed to achieve the best

00:29:53,930 --> 00:30:00,140
performance when you run a corporate

00:29:56,510 --> 00:30:04,040
program it's tightly integrated with no

00:30:00,140 --> 00:30:07,490
pie ok so we can rewrite the previous

00:30:04,040 --> 00:30:11,270
example just changing the dis lines

00:30:07,490 --> 00:30:13,550
using a numpy and it works you don't

00:30:11,270 --> 00:30:15,560
need the two what if you want to use

00:30:13,550 --> 00:30:18,440
copyright you don't need to modify or

00:30:15,560 --> 00:30:23,360
rewrite your programs to make it works

00:30:18,440 --> 00:30:25,760
copyright and I totes offer an easy

00:30:23,360 --> 00:30:29,120
switching between CPU and GPU with the

00:30:25,760 --> 00:30:32,180
constructor with with the poster quit

00:30:29,120 --> 00:30:34,760
you can say where your function will be

00:30:32,180 --> 00:30:38,210
executed since we are using standard

00:30:34,760 --> 00:30:41,480
patents I attacks syntax we can execute

00:30:38,210 --> 00:30:46,550
the function but on the GPU or the CPU

00:30:41,480 --> 00:30:51,470
and if work it works the bottle bottles

00:30:46,550 --> 00:30:53,270
that device and you just have to use the

00:30:51,470 --> 00:31:01,180
construct with to specify where the

00:30:53,270 --> 00:31:01,180
places the situation is it's a president

00:31:01,310 --> 00:31:07,140
so one of the problems that I faced

00:31:04,560 --> 00:31:08,700
using cooter in the past is not knowing

00:31:07,140 --> 00:31:12,360
when to put things on to the CUDA card

00:31:08,700 --> 00:31:15,480
or when to keep them on the CPU can you

00:31:12,360 --> 00:31:17,820
repeat the croissants so I have an array

00:31:15,480 --> 00:31:21,330
i want to multiply it yes do i do it on

00:31:17,820 --> 00:31:23,760
the cpu the three-year-old GPU or the

00:31:21,330 --> 00:31:26,130
six-month-old big GPU on my other

00:31:23,760 --> 00:31:29,340
computer i have to think about the task

00:31:26,130 --> 00:31:33,080
and the same here do i execute on the

00:31:29,340 --> 00:31:36,500
cpu or the GPU which is faster in

00:31:33,080 --> 00:31:42,720
copperhead are they talking about

00:31:36,500 --> 00:31:48,660
deciding for you actually Capri Edwards

00:31:42,720 --> 00:31:51,870
that no I prolly know because Capri odd

00:31:48,660 --> 00:31:55,560
is a developing approach project there

00:31:51,870 --> 00:31:59,190
is a very good guy Bryan cons are that

00:31:55,560 --> 00:32:04,080
is the maintainer of this project II he

00:31:59,190 --> 00:32:05,910
publishes the I'll incurred on my

00:32:04,080 --> 00:32:10,530
reference site some the condition that

00:32:05,910 --> 00:32:12,060
he wrote about cooperate and well he say

00:32:10,530 --> 00:32:17,910
that there is the blocking project that

00:32:12,060 --> 00:32:21,660
there are some rather it may is

00:32:17,910 --> 00:32:26,570
meaningful it's me he is mainly focused

00:32:21,660 --> 00:32:32,150
on another kind of task is trying to

00:32:26,570 --> 00:32:32,150
bring data parallelism to python but

00:32:32,870 --> 00:32:40,350
trying to implement the Karaka prada in

00:32:36,870 --> 00:32:42,720
other languages instead of using cuda

00:32:40,350 --> 00:32:46,050
because corporate i would say is the

00:32:42,720 --> 00:32:50,370
detector from CUDA is a not sweetie

00:32:46,050 --> 00:32:53,540
bound cuda and no is not thinking to

00:32:50,370 --> 00:32:56,280
this kind of optimization and also you

00:32:53,540 --> 00:33:01,980
when you use the construct with you

00:32:56,280 --> 00:33:05,880
cannot the you are that you have that

00:33:01,980 --> 00:33:09,240
these two vectors are also storage on

00:33:05,880 --> 00:33:11,910
the author or device memory because if

00:33:09,240 --> 00:33:12,990
you is using your functional a device

00:33:11,910 --> 00:33:15,090
memory you

00:33:12,990 --> 00:33:17,460
also the vectors are stored on the best

00:33:15,090 --> 00:33:24,210
memory and you cannot mix this you

00:33:17,460 --> 00:33:27,090
cannot do with the places here in your

00:33:24,210 --> 00:33:30,660
XP and then you do a wheat-based which

00:33:27,090 --> 00:33:32,940
pew with the same vectors maybe in the

00:33:30,660 --> 00:33:35,520
future they will support this kind of

00:33:32,940 --> 00:33:43,100
digitization Andrew profiling to see

00:33:35,520 --> 00:33:46,380
what are the better best choice and well

00:33:43,100 --> 00:33:50,730
next slide I want list some cons of this

00:33:46,380 --> 00:33:52,710
library well the first occurrence is

00:33:50,730 --> 00:33:57,000
about the lack of an extensive

00:33:52,710 --> 00:34:00,300
documentation on the main page of the

00:33:57,000 --> 00:34:04,140
project you can find some documentation

00:34:00,300 --> 00:34:10,909
about copperhead but there is not too

00:34:04,140 --> 00:34:17,639
much to it and developing projects Osten

00:34:10,909 --> 00:34:24,330
lakhs from extensive documentation as i

00:34:17,639 --> 00:34:26,639
said before 2m copperhead is a project

00:34:24,330 --> 00:34:29,040
time to support Orion platform test the

00:34:26,639 --> 00:34:31,230
den kuda because copyright is a project

00:34:29,040 --> 00:34:35,820
that we want to bring gangs to brings

00:34:31,230 --> 00:34:38,340
that apparel is meant to python and it's

00:34:35,820 --> 00:34:43,830
not strictly related to who that may be

00:34:38,340 --> 00:34:50,369
the next back and we'll be open CL maybe

00:34:43,830 --> 00:34:56,550
my guess is and as i used this library

00:34:50,369 --> 00:34:58,770
for a little time and i wrote some

00:34:56,550 --> 00:35:00,390
example the result so an example data

00:34:58,770 --> 00:35:04,109
right and i put on my reference site

00:35:00,390 --> 00:35:06,869
about image fading simply example and in

00:35:04,109 --> 00:35:08,970
my opinion and also the outer opinion is

00:35:06,869 --> 00:35:12,420
that this library is not really for a

00:35:08,970 --> 00:35:14,430
production environment it as i said

00:35:12,420 --> 00:35:20,280
before as some locks of documentation

00:35:14,430 --> 00:35:23,990
and it needs some some adjustment this

00:35:20,280 --> 00:35:23,990
is why cooperated

00:35:24,170 --> 00:35:33,500
as a strong typing policy so he has a

00:35:30,160 --> 00:35:35,809
strong type inference system that it

00:35:33,500 --> 00:35:38,270
doesn't understand always what you are

00:35:35,809 --> 00:35:39,589
trying to do if it undone doesn't

00:35:38,270 --> 00:35:41,839
understand what we're trying to do

00:35:39,589 --> 00:35:44,750
usually it means some light something

00:35:41,839 --> 00:35:48,319
like some time inference the exception

00:35:44,750 --> 00:35:51,650
or this kind of these sort of things and

00:35:48,319 --> 00:35:56,210
you have to modify your code to achieve

00:35:51,650 --> 00:35:58,099
the to run your function so there are

00:35:56,210 --> 00:36:01,460
seen some adjustment that has to be made

00:35:58,099 --> 00:36:05,750
to this library the second library that

00:36:01,460 --> 00:36:10,040
I want to shoot you is piano piano is a

00:36:05,750 --> 00:36:12,710
library to let you define mathematical

00:36:10,040 --> 00:36:18,819
mathematical expression and it compiles

00:36:12,710 --> 00:36:21,650
them to shift some speed up its offer

00:36:18,819 --> 00:36:26,270
transparent GPU computing but let's see

00:36:21,650 --> 00:36:33,470
how this library work before seeing the

00:36:26,270 --> 00:36:36,440
details I copied the example that I did

00:36:33,470 --> 00:36:43,220
before for Copperhead and it's the same

00:36:36,440 --> 00:36:45,609
as a x py but since piano is a mad

00:36:43,220 --> 00:36:49,640
special compiler we have to declare

00:36:45,609 --> 00:36:51,710
which are the arguments that the

00:36:49,640 --> 00:36:55,700
function take as input so we declare our

00:36:51,710 --> 00:36:58,099
a that is a scholar and then x and y

00:36:55,700 --> 00:37:02,270
that are the two vectors and then we

00:36:58,099 --> 00:37:08,770
define our operation and this this line

00:37:02,270 --> 00:37:12,410
is the expi y equal function this line

00:37:08,770 --> 00:37:14,450
just compile your expression and the

00:37:12,410 --> 00:37:18,740
Optima is a u.s. passion for executing

00:37:14,450 --> 00:37:21,260
and you can after define your back door

00:37:18,740 --> 00:37:23,450
with the standard pattern list as we

00:37:21,260 --> 00:37:26,270
seen before and finally you can call

00:37:23,450 --> 00:37:28,309
your function if you look at this code

00:37:26,270 --> 00:37:31,400
that you can find any reference to GPU

00:37:28,309 --> 00:37:34,400
programming because as I said at the

00:37:31,400 --> 00:37:37,010
on/off ray offers you a transport GPU

00:37:34,400 --> 00:37:40,280
computing so how you

00:37:37,010 --> 00:37:43,220
do you run this code on your Jeep you

00:37:40,280 --> 00:37:46,160
just simply setting an environment

00:37:43,220 --> 00:37:48,490
variable this you can set the Theon

00:37:46,160 --> 00:37:54,320
effects variable of your environment

00:37:48,490 --> 00:37:57,020
setting device equal to and and your

00:37:54,320 --> 00:37:59,720
program will be automatically execute on

00:37:57,020 --> 00:38:03,760
your GPU the function can we compile a

00:37:59,720 --> 00:38:08,510
code that will be executed on the GPU

00:38:03,760 --> 00:38:12,890
it's some other tests of this library

00:38:08,510 --> 00:38:15,770
are that is integrated with numpy so we

00:38:12,890 --> 00:38:20,530
as we seen before we can substitute the

00:38:15,770 --> 00:38:20,530
light day lists with a numpy array and

00:38:21,070 --> 00:38:28,310
things which differs from copper add is

00:38:25,520 --> 00:38:31,010
that is it has an exhaustive

00:38:28,310 --> 00:38:33,500
documentation on the homepage project

00:38:31,010 --> 00:38:37,210
there are a lot of example simple

00:38:33,500 --> 00:38:39,890
example I'm more complex example and

00:38:37,210 --> 00:38:43,670
it's really for the production

00:38:39,890 --> 00:38:45,620
environment it has a bsd license it has

00:38:43,670 --> 00:38:50,530
an active community of developers and

00:38:45,620 --> 00:38:56,570
users it has a good bug tracking system

00:38:50,530 --> 00:38:58,820
some cons of this library are that too

00:38:56,570 --> 00:39:03,460
right there is no full performance

00:38:58,820 --> 00:39:06,110
portability since if you want to write a

00:39:03,460 --> 00:39:10,690
function a program that the run past

00:39:06,110 --> 00:39:13,880
bought on GPU and CPU you have to use

00:39:10,690 --> 00:39:16,820
some special function to say which are

00:39:13,880 --> 00:39:21,470
the variable that has to be copied to

00:39:16,820 --> 00:39:25,640
the GPU and the variables that can be

00:39:21,470 --> 00:39:28,970
left on the GPU otherwise you are in

00:39:25,640 --> 00:39:32,000
containing some performance decreasing

00:39:28,970 --> 00:39:34,660
given by the massive memory transfers

00:39:32,000 --> 00:39:34,660
that piano

00:39:35,180 --> 00:39:42,319
and also you have that not all the

00:39:40,220 --> 00:39:46,970
function the library run faster on the

00:39:42,319 --> 00:39:51,250
GPU this is why Fiano is was mainly

00:39:46,970 --> 00:39:54,710
designed to write some support vector

00:39:51,250 --> 00:39:59,300
machine learning algorithms so it's very

00:39:54,710 --> 00:40:01,880
good at executed running linear algebra

00:39:59,300 --> 00:40:06,490
programs linear algebra operation and

00:40:01,880 --> 00:40:11,300
they all run fast but for them for this

00:40:06,490 --> 00:40:14,660
presentation I write a example example

00:40:11,300 --> 00:40:17,020
about the convolution and I was

00:40:14,660 --> 00:40:21,410
surprised that this convolution was a

00:40:17,020 --> 00:40:24,250
faster on a cpu than the GPU maybe I was

00:40:21,410 --> 00:40:29,660
wrong in writing something because but

00:40:24,250 --> 00:40:32,540
this is my first impression well I'm

00:40:29,660 --> 00:40:34,670
almost finished I want to just do a

00:40:32,540 --> 00:40:38,210
little summary of what we have seen

00:40:34,670 --> 00:40:41,690
today we have seen what is a GPU and

00:40:38,210 --> 00:40:46,819
what is GP computer parity how GPU

00:40:41,690 --> 00:40:49,670
computing would you question and we have

00:40:46,819 --> 00:40:52,730
seen what is cuda and how CUDA

00:40:49,670 --> 00:40:55,790
programming model works a little brief

00:40:52,730 --> 00:41:00,290
the details are obviously a bible online

00:40:55,790 --> 00:41:05,109
i suggest you to read the cuda the

00:41:00,290 --> 00:41:08,299
nvidia chip programming guide it's very

00:41:05,109 --> 00:41:11,780
very good introductory required guide if

00:41:08,299 --> 00:41:14,119
you are interesting these topics and we

00:41:11,780 --> 00:41:17,059
have seen what is pi cuda and a little

00:41:14,119 --> 00:41:19,609
example of how I could have work so what

00:41:17,059 --> 00:41:25,250
are the advantages in using pi cuda

00:41:19,609 --> 00:41:27,619
instead of Korra and we have just do a

00:41:25,250 --> 00:41:31,430
little beer for what are the things that

00:41:27,619 --> 00:41:34,130
are important to write a good colonel

00:41:31,430 --> 00:41:36,650
fast colonel and finally we have seen

00:41:34,130 --> 00:41:38,809
two libraries that lets you do easy GPU

00:41:36,650 --> 00:41:44,480
computing without knowing all the

00:41:38,809 --> 00:41:47,330
details of this of the GPU computing of

00:41:44,480 --> 00:42:06,830
the GPU computing

00:41:47,330 --> 00:42:10,760
oh so that's all the back yes yes just

00:42:06,830 --> 00:42:13,790
wondering is there is any research or

00:42:10,760 --> 00:42:18,050
someone on the web where there is some

00:42:13,790 --> 00:42:20,030
metrics about me can you repeat that

00:42:18,050 --> 00:42:24,170
yeah yeah yeah I'm just wondering

00:42:20,030 --> 00:42:27,530
basically the the choice a developer has

00:42:24,170 --> 00:42:31,190
to do it when thinking about CUDA is

00:42:27,530 --> 00:42:34,400
about the overhead and if there are time

00:42:31,190 --> 00:42:39,580
research or somewhere where it's say

00:42:34,400 --> 00:42:43,610
that under I don't know 100 operation 1

00:42:39,580 --> 00:42:47,630
100 operation like I don't know 100 plus

00:42:43,610 --> 00:42:50,060
1 under x with the scoring basically

00:42:47,630 --> 00:42:52,100
that you can go to pick CUDA or you

00:42:50,060 --> 00:42:54,980
don't if they're somewhere this kind of

00:42:52,100 --> 00:42:58,280
data because thats that's we can all

00:42:54,980 --> 00:43:01,250
offers experiment in it refers in all

00:42:58,280 --> 00:43:03,380
corner but looks like the data that's

00:43:01,250 --> 00:43:06,680
meeting it that yes aye khuda as

00:43:03,380 --> 00:43:10,610
official like this that it's not like

00:43:06,680 --> 00:43:13,300
that we test all the debris when the

00:43:10,610 --> 00:43:19,160
operation goes faster but it has the

00:43:13,300 --> 00:43:21,920
camera launch configuration it's a very

00:43:19,160 --> 00:43:25,580
important thing when you do GQ

00:43:21,920 --> 00:43:31,060
programming to rightly configure your

00:43:25,580 --> 00:43:35,060
Colonel the the ratio between the

00:43:31,060 --> 00:43:39,380
threads per blocks and blocks can change

00:43:35,060 --> 00:43:44,570
a lot your speed up your performance so

00:43:39,380 --> 00:43:47,900
by CUDA as some function then like you

00:43:44,570 --> 00:43:50,870
remember the source model class well as

00:43:47,900 --> 00:43:54,560
some classes like this that can you

00:43:50,870 --> 00:43:56,400
specify just the kernel code without any

00:43:54,560 --> 00:44:01,630
refer meant today

00:43:56,400 --> 00:44:05,640
tret index to pick you up patch in the

00:44:01,630 --> 00:44:10,000
source model class 2 and it

00:44:05,640 --> 00:44:14,200
automatically choose the configuration

00:44:10,000 --> 00:44:17,760
of your cagna and it's a good thing

00:44:14,200 --> 00:44:20,380
because otherwise you have to test on

00:44:17,760 --> 00:44:21,970
every time and when you are in

00:44:20,380 --> 00:44:27,040
production environment maybe you don't

00:44:21,970 --> 00:44:29,350
know which which your your program will

00:44:27,040 --> 00:44:31,060
be run maybe you are running you are

00:44:29,350 --> 00:44:33,250
writing a program that use coulda but

00:44:31,060 --> 00:44:37,300
you're not you don't know if you use it

00:44:33,250 --> 00:44:46,590
are you running a last generation fear

00:44:37,300 --> 00:44:50,440
me good advice or another one oh I

00:44:46,590 --> 00:44:53,620
didn't I wanted to show you the Fisher

00:44:50,440 --> 00:44:55,450
but it would thereby redic knowledge of

00:44:53,620 --> 00:44:58,810
how they put a programming model works

00:44:55,450 --> 00:45:03,990
and also out the device which is the

00:44:58,810 --> 00:45:07,090
device model of the CUDA architecture

00:45:03,990 --> 00:45:12,720
however you can send me an email you can

00:45:07,090 --> 00:45:12,720
answer to your question if if you want I

00:45:12,810 --> 00:45:20,170
so I can see how CUDA could help me to

00:45:16,170 --> 00:45:22,590
do some liner computing on vectors but

00:45:20,170 --> 00:45:25,870
that the library also provide any

00:45:22,590 --> 00:45:28,750
high-level functions to maybe work on

00:45:25,870 --> 00:45:31,270
let's say geographical data or maybe

00:45:28,750 --> 00:45:34,600
bidding indexes or I don't know whatever

00:45:31,270 --> 00:45:39,220
there are some high-level function but

00:45:34,600 --> 00:45:42,880
they are especially about linear algebra

00:45:39,220 --> 00:45:48,910
i I didn't found anything about the

00:45:42,880 --> 00:45:52,600
geographical library for CUDA but if

00:45:48,910 --> 00:45:56,160
your problems are related what kind of

00:45:52,600 --> 00:46:00,610
problem there are in geographical

00:45:56,160 --> 00:46:03,690
problems but if you use linear algebra

00:46:00,610 --> 00:46:06,340
you use something that is more

00:46:03,690 --> 00:46:09,360
mathematical you can find a lot of

00:46:06,340 --> 00:46:16,420
library the piano is one good

00:46:09,360 --> 00:46:19,720
there are also some nvidia wrote also

00:46:16,420 --> 00:46:25,360
some linear algebra functions they call

00:46:19,720 --> 00:46:29,050
them I do I forget the name like if we

00:46:25,360 --> 00:46:31,930
can check alive maybe up and you can use

00:46:29,050 --> 00:46:35,400
this function to make shopping your

00:46:31,930 --> 00:46:39,100
linear algebra problems if you have a

00:46:35,400 --> 00:46:42,670
complex task away a custom task you

00:46:39,100 --> 00:46:46,030
usually it's better if you write your

00:46:42,670 --> 00:46:50,050
own kinda you can write for complex task

00:46:46,030 --> 00:46:52,750
I work I use it kuda in my work for a

00:46:50,050 --> 00:46:54,910
couple of task and also i have always to

00:46:52,750 --> 00:46:57,760
write my own kernel this is why it's

00:46:54,910 --> 00:47:00,040
very important to know to know how they

00:46:57,760 --> 00:47:06,550
architectural work our they could have

00:47:00,040 --> 00:47:10,990
my base map it I have actually to the

00:47:06,550 --> 00:47:15,280
first one is maybe i'm missing somewhat

00:47:10,990 --> 00:47:17,620
but isn't it the most basic precondition

00:47:15,280 --> 00:47:19,720
for doing this stuff that the problem

00:47:17,620 --> 00:47:22,810
you are trying to solve can be naturally

00:47:19,720 --> 00:47:30,520
decomposed into smaller sub tasks yes so

00:47:22,810 --> 00:47:32,860
this is the well you asking me I don't

00:47:30,520 --> 00:47:36,010
understand the question exactly the

00:47:32,860 --> 00:47:39,930
question is you have presented some

00:47:36,010 --> 00:47:43,770
problems simple problems all of them are

00:47:39,930 --> 00:47:47,050
list comprehensions nap expressions

00:47:43,770 --> 00:47:52,330
stuff which can be naturally decomposed

00:47:47,050 --> 00:47:54,520
into smaller steps yes if you if you

00:47:52,330 --> 00:47:57,820
have a pile of code which is a

00:47:54,520 --> 00:48:01,060
sequential in nature highly

00:47:57,820 --> 00:48:05,710
interdependent what are you doing then

00:48:01,060 --> 00:48:09,070
well there are the a good question it's

00:48:05,710 --> 00:48:14,140
a more technical question well if you

00:48:09,070 --> 00:48:18,460
were a lot of see Alex people code that

00:48:14,140 --> 00:48:20,970
you can paralyze you can try to change

00:48:18,460 --> 00:48:22,210
your program maybe if you are doing

00:48:20,970 --> 00:48:28,240
maybe

00:48:22,210 --> 00:48:32,410
you can use cruda to just solve many of

00:48:28,240 --> 00:48:36,609
these problems in parallel just one

00:48:32,410 --> 00:48:38,349
thread so can solve one of your program

00:48:36,609 --> 00:48:39,670
or maybe one block and so the one of

00:48:38,349 --> 00:48:45,040
your product and they're from the aust

00:48:39,670 --> 00:48:50,200
you can solve a bunch a group of the dis

00:48:45,040 --> 00:48:52,950
problems well it's a it's I problem

00:48:50,200 --> 00:48:55,750
deafened entities finding a solution

00:48:52,950 --> 00:48:58,000
Google algorithmic problem i think or

00:48:55,750 --> 00:49:00,369
yes yes it's more a kind of their

00:48:58,000 --> 00:49:05,230
finding the right algorithm and the

00:49:00,369 --> 00:49:08,380
right structure to to to model your

00:49:05,230 --> 00:49:09,970
problem and then you can get good at

00:49:08,380 --> 00:49:13,930
programming is not a simple so simple

00:49:09,970 --> 00:49:16,930
task many people say yeah well you use

00:49:13,930 --> 00:49:19,930
cuda you get 50 / 50 times speed ups

00:49:16,930 --> 00:49:24,700
it's not so easy it's easy and a lot of

00:49:19,930 --> 00:49:27,820
task linear algebra and hi paralee

00:49:24,700 --> 00:49:30,910
invisible task but writing a custom

00:49:27,820 --> 00:49:34,170
algorithm is something more complex all

00:49:30,910 --> 00:49:38,560
right second one is which is your main

00:49:34,170 --> 00:49:43,599
platform for doing this stuff and how is

00:49:38,560 --> 00:49:46,810
the Linux experience oh well I is reach

00:49:43,599 --> 00:49:51,280
out to a Mac just some month ago so high

00:49:46,810 --> 00:49:54,940
before use it always leaves and Linux

00:49:51,280 --> 00:49:58,650
and well it works I tried the pie cuda

00:49:54,940 --> 00:50:02,290
and cuda on linux platform on mac

00:49:58,650 --> 00:50:07,290
platform and I don't doubt try windows I

00:50:02,290 --> 00:50:10,300
ate windows and it works well it works

00:50:07,290 --> 00:50:15,220
quite well I found just a little

00:50:10,300 --> 00:50:19,480
difficult to instil cooperate because it

00:50:15,220 --> 00:50:21,730
has some dependencies it's a developing

00:50:19,480 --> 00:50:24,010
project so you have to download your

00:50:21,730 --> 00:50:26,170
dependencies there is not you have to

00:50:24,010 --> 00:50:29,800
check out the source code from the

00:50:26,170 --> 00:50:32,740
repositories and compile you have to do

00:50:29,800 --> 00:50:36,200
this kind of stuff not too much

00:50:32,740 --> 00:50:39,509
difficult but is not the image

00:50:36,200 --> 00:50:41,700
just stalling by CUDA if you have flown

00:50:39,509 --> 00:50:44,160
to its top ayuda you just have to type

00:50:41,700 --> 00:50:46,349
is installed by cuda and you got back

00:50:44,160 --> 00:50:51,470
udah it's very simple after you have

00:50:46,349 --> 00:50:53,940
this tile installer that your BD SDK

00:50:51,470 --> 00:50:58,049
toolkit that you can download for free

00:50:53,940 --> 00:50:59,039
from the video so just an announced

00:50:58,049 --> 00:51:00,900
first of all I'm running the

00:50:59,039 --> 00:51:03,150
high-performance python tutorial this

00:51:00,900 --> 00:51:04,890
afternoon and in that we're going to be

00:51:03,150 --> 00:51:07,319
looking a lot of compiler stuff on the

00:51:04,890 --> 00:51:09,210
cpu and also having look at pi cuda at

00:51:07,319 --> 00:51:11,099
the end there's going to be an awful lot

00:51:09,210 --> 00:51:12,839
of questions that come out of that for

00:51:11,099 --> 00:51:15,720
our session so I've arranged a birds of

00:51:12,839 --> 00:51:18,150
a feather meeting tomorrow morning 1032

00:51:15,720 --> 00:51:20,670
11am in the open space outside the big

00:51:18,150 --> 00:51:22,859
tent so if you're interested in high

00:51:20,670 --> 00:51:24,480
performance computing of any sort or

00:51:22,859 --> 00:51:26,279
artificial intelligence please come

00:51:24,480 --> 00:51:28,799
along 1030 and the coffee break to the

00:51:26,279 --> 00:51:30,299
open space and I've got a practical

00:51:28,799 --> 00:51:33,390
answer a practical bit of experiments

00:51:30,299 --> 00:51:36,359
for the chap who asked about which array

00:51:33,390 --> 00:51:39,000
size is make the the best performance on

00:51:36,359 --> 00:51:40,529
cuda the short answer is don't think

00:51:39,000 --> 00:51:42,990
there is a clear answer it all depends

00:51:40,529 --> 00:51:45,839
on which hardware you're running be its

00:51:42,990 --> 00:51:49,250
CPU and GPU I've used it over two years

00:51:45,839 --> 00:51:51,690
I've gone down to writing C in cuda and

00:51:49,250 --> 00:51:54,180
you can get great performance benefits

00:51:51,690 --> 00:51:57,500
I've got 160 times speedup versus an

00:51:54,180 --> 00:51:59,700
open MP optimized CPU quad-core solution

00:51:57,500 --> 00:52:03,539
but that took about six months with a

00:51:59,700 --> 00:52:05,490
work now that's the downside the upside

00:52:03,539 --> 00:52:08,369
is that Nvidia are saying that we can

00:52:05,490 --> 00:52:10,529
buy a thousand cause on a card for 400

00:52:08,369 --> 00:52:12,539
pounds at the moment running on a 750

00:52:10,529 --> 00:52:15,150
watt power supply what they want within

00:52:12,539 --> 00:52:17,009
the next four years is 20,000 cause so

00:52:15,150 --> 00:52:19,829
that's ahead of Moore's law so if you

00:52:17,009 --> 00:52:22,319
start to use khuda aur opencl any of the

00:52:19,829 --> 00:52:23,730
massively parallel techniques now you're

00:52:22,319 --> 00:52:24,930
going to end up being ahead of the curve

00:52:23,730 --> 00:52:27,750
a couple of years down the line when

00:52:24,930 --> 00:52:30,869
many more problems will just run faster

00:52:27,750 --> 00:52:33,509
naturally really easily on a GPU or a

00:52:30,869 --> 00:52:35,099
couple of GPUs or cluster of GPUs so if

00:52:33,509 --> 00:52:36,480
you're at all interested in this area

00:52:35,099 --> 00:52:38,309
for the next couple of years it's

00:52:36,480 --> 00:52:40,920
definitely worth getting a car even a

00:52:38,309 --> 00:52:42,509
slow cheap card and just experimenting

00:52:40,920 --> 00:52:43,500
so that you're aware of the language and

00:52:42,509 --> 00:52:45,660
the different way you have to think

00:52:43,500 --> 00:52:47,190
about the problem and then you just have

00:52:45,660 --> 00:52:48,320
to start doing performance benchmarks

00:52:47,190 --> 00:52:54,460
and see when it really works out

00:52:48,320 --> 00:52:54,460
for you any more questions

00:53:02,240 --> 00:53:09,530
or you shall simple example with two

00:53:06,680 --> 00:53:12,860
vectors which are I mean one dimensional

00:53:09,530 --> 00:53:15,290
very CS but what about going to higher

00:53:12,860 --> 00:53:19,670
dimensions of the matrix or three

00:53:15,290 --> 00:53:22,400
dimensional array so is everything much

00:53:19,670 --> 00:53:24,980
more complicated you can easily use this

00:53:22,400 --> 00:53:28,610
well it's not much more complicated

00:53:24,980 --> 00:53:31,540
there are as I said before there are a

00:53:28,610 --> 00:53:37,280
lot of libraries that do this this

00:53:31,540 --> 00:53:41,570
simple linear algebra tasks in a very

00:53:37,280 --> 00:53:44,270
efficient way so if you want to do some

00:53:41,570 --> 00:53:45,980
some linear algebra operation you don't

00:53:44,270 --> 00:53:49,970
write your own car that you just use

00:53:45,980 --> 00:53:52,550
some libraries switching to other

00:53:49,970 --> 00:53:56,060
dimension is not is really not a problem

00:53:52,550 --> 00:53:59,119
since I I don't present the details of

00:53:56,060 --> 00:54:02,750
the CUDA programming model but every

00:53:59,119 --> 00:54:07,430
thread can have an ID that can be up to

00:54:02,750 --> 00:54:10,730
the three-dimensional int array of

00:54:07,430 --> 00:54:14,720
integers so you can get you can shape

00:54:10,730 --> 00:54:17,930
your treads to get to out to scale on

00:54:14,720 --> 00:54:21,230
more dimension also the blocks can have

00:54:17,930 --> 00:54:28,040
in a two-dimensional array so you can

00:54:21,230 --> 00:54:32,450
easily do things like this on yes there

00:54:28,040 --> 00:54:34,369
is no not too difficult so in fact on

00:54:32,450 --> 00:54:38,090
Marduk got this idea there's a library

00:54:34,369 --> 00:54:40,040
of kuta kernels that maybe nvidia supply

00:54:38,090 --> 00:54:43,190
is that possible to access through the

00:54:40,040 --> 00:54:46,010
pie cuda interface oh well I don't

00:54:43,190 --> 00:54:48,260
understand the question so if kind of

00:54:46,010 --> 00:54:50,240
common tasks like FFT or something that

00:54:48,260 --> 00:54:52,640
you really want to engineer your kernel

00:54:50,240 --> 00:54:55,540
for I've got this kind of feeling that

00:54:52,640 --> 00:54:59,060
Nvidia provide a library of them o

00:54:55,540 --> 00:55:03,290
mighty know can I help you um tell you

00:54:59,060 --> 00:55:06,740
get that yeah that's good sorry it's not

00:55:03,290 --> 00:55:09,140
your problem is my fault at that

00:55:06,740 --> 00:55:11,510
community borders for my delivery a cozy

00:55:09,140 --> 00:55:15,350
cozy yes there are a lot of libraries

00:55:11,510 --> 00:55:18,110
for doing this kind of signal processing

00:55:15,350 --> 00:55:20,990
there are nvidia provides his own

00:55:18,110 --> 00:55:25,340
library about doing a fast Fourier

00:55:20,990 --> 00:55:28,160
transformation and also I'd like for

00:55:25,340 --> 00:55:32,570
this talk I wanted to implement the haar

00:55:28,160 --> 00:55:35,180
wavelet function you know yes they are

00:55:32,570 --> 00:55:39,260
with compression algorithm for an image

00:55:35,180 --> 00:55:42,680
but I don't have enough time to do this

00:55:39,260 --> 00:55:44,510
but it's not too difficult to use to

00:55:42,680 --> 00:55:48,410
implement something like this and there

00:55:44,510 --> 00:55:51,890
are also some libraries I suggest you to

00:55:48,410 --> 00:55:54,020
look on the going to see on the nvidia

00:55:51,890 --> 00:55:56,240
site what are the libraries that are

00:55:54,020 --> 00:56:00,470
officially supported by the empire that

00:55:56,240 --> 00:56:02,860
the nvidia and you will find you surely

00:56:00,470 --> 00:56:05,390
something about senior processing and

00:56:02,860 --> 00:56:12,340
something more mathematical something

00:56:05,390 --> 00:56:16,870
more formal in this way ah well there is

00:56:12,340 --> 00:56:21,560
some libraries ab bindings for Python

00:56:16,870 --> 00:56:25,040
someone i don't know i don't i didn't

00:56:21,560 --> 00:56:28,130
try the library for doing for your

00:56:25,040 --> 00:56:35,530
transformation but i think that maybe

00:56:28,130 --> 00:56:39,110
there are bindings for debris well maybe

00:56:35,530 --> 00:56:44,030
Justin all these gpus are changing

00:56:39,110 --> 00:56:47,660
because they are the modern GPUs such as

00:56:44,030 --> 00:56:51,080
the videos Fermi or la cocina announced

00:56:47,660 --> 00:56:57,200
at the AMD GPU are much more similar to

00:56:51,080 --> 00:57:01,100
CPU they already can do class and

00:56:57,200 --> 00:57:05,710
calling methods calling functions with

00:57:01,100 --> 00:57:09,800
recursion and so on the lady is a GPU is

00:57:05,710 --> 00:57:12,710
able to do quite much more complex

00:57:09,800 --> 00:57:15,500
things so you are not limited to the

00:57:12,710 --> 00:57:17,800
linear algebra right now with the modern

00:57:15,500 --> 00:57:19,600
GPUs and a

00:57:17,800 --> 00:57:22,720
it's for the background on the on the

00:57:19,600 --> 00:57:25,480
future and the only thing that we need

00:57:22,720 --> 00:57:29,460
to care about the GPU computing is about

00:57:25,480 --> 00:57:33,570
the round trip between the CPU and GPU

00:57:29,460 --> 00:57:37,890
goes then PCI Express is a good

00:57:33,570 --> 00:57:42,130
point-to-point connection but it's slow

00:57:37,890 --> 00:57:45,250
it's so that something can change when

00:57:42,130 --> 00:57:47,980
we have we have a cpu and GPU on the

00:57:45,250 --> 00:57:50,230
same die where if we they are in this on

00:57:47,980 --> 00:57:54,730
the same diet such as the AMD fusion

00:57:50,230 --> 00:57:58,990
project we don't have as low balls in

00:57:54,730 --> 00:58:02,440
the middle so the CPU can send data on

00:57:58,990 --> 00:58:06,040
the GPU just which is on the each side

00:58:02,440 --> 00:58:12,100
so it will be changing a lot the GPU

00:58:06,040 --> 00:58:15,400
computing in the next year's well one

00:58:12,100 --> 00:58:18,040
last thing i forgot to say that i write

00:58:15,400 --> 00:58:20,170
some code that i put on the reference

00:58:18,040 --> 00:58:24,250
site and this is i want to show you as

00:58:20,170 --> 00:58:27,430
an example how you can simply translate

00:58:24,250 --> 00:58:33,640
a program from the nvidia capi to buy

00:58:27,430 --> 00:58:37,240
cuda this is an example that that the

00:58:33,640 --> 00:58:41,080
nvidia provide yes that and then Vidya

00:58:37,240 --> 00:58:44,170
provide with his SDK I just translated

00:58:41,080 --> 00:58:46,660
this example 2 pi cuda to do the same

00:58:44,170 --> 00:58:48,910
exactly thing since the colonel is

00:58:46,660 --> 00:58:52,660
always the same and this is a suburb

00:58:48,910 --> 00:58:55,240
filter operation this is the the the

00:58:52,660 --> 00:59:00,520
conclusion that i tried with a no and it

00:58:55,240 --> 00:59:02,260
was low and it can do where the you can

00:59:00,520 --> 00:59:04,240
see the difference but to you either

00:59:02,260 --> 00:59:06,310
inside this program there are three

00:59:04,240 --> 00:59:11,860
kernels that do the same operation using

00:59:06,310 --> 00:59:14,800
some different feature of the cuda the

00:59:11,860 --> 00:59:18,610
cuda premium model and I didn't talk

00:59:14,800 --> 00:59:21,220
about texture about OpenGL integration

00:59:18,610 --> 00:59:23,440
into operation there are a lot of fish

00:59:21,220 --> 00:59:26,770
that I can't that I would like to

00:59:23,440 --> 00:59:28,360
present to you but there is not time but

00:59:26,770 --> 00:59:29,900
you can find is the script the script

00:59:28,360 --> 00:59:34,039
this

00:59:29,900 --> 00:59:36,859
is quite complete for the CUDA Fisher

00:59:34,039 --> 00:59:38,720
that you can use so you can just go on

00:59:36,859 --> 00:59:47,770
the reference site and download it if

00:59:38,720 --> 00:59:53,809
you like okay any more questions is

00:59:47,770 --> 00:59:56,539
probably the last question so I saw in

00:59:53,809 --> 00:59:58,609
the coder that you have u f2 and you can

00:59:56,539 --> 01:00:00,859
specify the number of blocks and of

00:59:58,609 --> 01:00:03,589
threads yes and there is it bound to

01:00:00,859 --> 01:00:05,539
your architecture that you have so if

01:00:03,589 --> 01:00:08,930
someone else use it on another chipset

01:00:05,539 --> 01:00:12,079
does it affect before I for this example

01:00:08,930 --> 01:00:15,549
I use it a quite low number so it will

01:00:12,079 --> 01:00:19,579
run on every architecture but yes it's a

01:00:15,549 --> 01:00:22,339
hardware dependent you have some cars

01:00:19,579 --> 01:00:27,140
that supports em more not a greater

01:00:22,339 --> 01:00:28,910
number of threads per blocks and you

01:00:27,140 --> 01:00:33,920
have architecture that supports month

01:00:28,910 --> 01:00:36,589
that can run more blocks than others but

01:00:33,920 --> 01:00:41,349
however for this simple example it will

01:00:36,589 --> 01:00:44,630
I tried it on my mac and on a Linux

01:00:41,349 --> 01:00:48,529
personal computer and it works quite

01:00:44,630 --> 01:00:51,920
well I prob I tried it on some also on

01:00:48,529 --> 01:01:00,410
some different GPUs at work we have my

01:00:51,920 --> 01:01:04,430
job we have gtx 260 for doing some some

01:01:00,410 --> 01:01:08,690
price and on my Mac there is a mobile

01:01:04,430 --> 01:01:10,700
version of the nvidia GPU and on my

01:01:08,690 --> 01:01:12,680
other work station i got an interest

01:01:10,700 --> 01:01:15,829
they integrated the chipset of the

01:01:12,680 --> 01:01:21,319
nvidia an old one editor answer on every

01:01:15,829 --> 01:01:23,569
platform that I tried thanks their work

01:01:21,319 --> 01:01:25,580
any more questions surely that was a

01:01:23,569 --> 01:01:27,640
lost him

01:01:25,580 --> 01:01:27,640

YouTube URL: https://www.youtube.com/watch?v=M5ckXyiiu7g


