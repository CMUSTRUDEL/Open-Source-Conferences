Title: Yury Selivanov - High Performance Networking in Python
Publication date: 2016-07-29
Playlist: EuroPython 2016
Description: 
	Yury Selivanov - High Performance Networking in Python
[EuroPython 2016]
[19 July 2016]
[Bilbao, Euskadi, Spain]
(https://ep2016.europython.eu//conference/talks/high-performance-networking-in-python)

The talk will cover new async/await syntax in Python, asyncio library
and ecosystem around it, and ways to use them for creating high
performance servers. It will explain how to build custom event loops
for asyncio, with an example of using the libuv library with Cython to
achieve 2-3x performance boost over vanilla asyncio.

-----

The talk will start with an overview of async/await syntax introduced
with PEP 492 in Python 3.5. We'll go through asynchronous context
managers and iteration protocols it introduces. I'll briefly explain
how the feature is implemented in CPython core.

Then we'll explore asyncio design. I'll briefly cover event loop,
policies, transports, protocols and streams abstractions. I'll explain
that event loops are pluggable, which really makes asyncio a universal
framework.

We'll cover libuv - a high performance networking library that drives
NodeJS. I'll highlight where it's similar to asyncio and how it's
different.

In the final part of the talk I'll explain how to make an asyncio
compatible event loop on top of libuv. I'll showcase Cython, which is
an amazing tool for tasks like this.

Finally, I'll share some ideas on how we can further improve the
performance of asyncio and networking in Python, and what are the
challenges that we will face.


**Objectives:**

1. Deeper understanding of async/await in Python and why it's important.
2. Deeper understanding of asyncio architecture and protocols.
3. How to improve asyncio performance by implementing custom event loops.
4. Show that it's easy to integrate existing complex & low level
libraries with Cython.
5. Some perspective on how Python may evolve wrt networking.
Captions: 
	00:00:00,000 --> 00:00:04,620
Toronto Canada my name is Yuri Stella

00:00:01,650 --> 00:00:10,050
I'm co-founder of magic stack check out

00:00:04,620 --> 00:00:13,200
our website it's magic dot IO I'm an

00:00:10,050 --> 00:00:15,540
avid Python users since 2008 I think the

00:00:13,200 --> 00:00:17,369
first Python version I still I started

00:00:15,540 --> 00:00:20,070
to use actually was platen - but then in

00:00:17,369 --> 00:00:22,050
the months I switched to Python 3 I used

00:00:20,070 --> 00:00:25,380
it since alpha 2 or something I never

00:00:22,050 --> 00:00:29,340
looked back so use Python 3 I'm C Python

00:00:25,380 --> 00:00:31,980
core developer since 2013 but I believe

00:00:29,340 --> 00:00:36,329
I actually started to do things even

00:00:31,980 --> 00:00:39,059
before that you might know me from web 3

00:00:36,329 --> 00:00:42,030
6 2 which I co-authored with Larry

00:00:39,059 --> 00:00:46,890
Hastings and Brett cannon its inspect

00:00:42,030 --> 00:00:50,750
signature API then I have then I've

00:00:46,890 --> 00:00:54,329
created web 4 9 - that's async await

00:00:50,750 --> 00:00:56,730
that we have in Python 3.5 and I'm also

00:00:54,329 --> 00:00:58,140
helping wither and Victor spinner to

00:00:56,730 --> 00:01:00,000
maintain a scene hello

00:00:58,140 --> 00:01:03,870
I also created evil OOP more on that

00:01:00,000 --> 00:01:06,299
later structure the Scopes I actually

00:01:03,870 --> 00:01:08,939
wanted to talk you to tell you so much

00:01:06,299 --> 00:01:12,000
about how to write high-performance code

00:01:08,939 --> 00:01:14,010
in Python and in a sink with I think I

00:01:12,000 --> 00:01:16,890
own in particular but unfortunately I

00:01:14,010 --> 00:01:22,369
had to cut with my my slides like like I

00:01:16,890 --> 00:01:26,479
don't know 50% of my slides had to go so

00:01:22,369 --> 00:01:27,780
we'll briefly start with an overview of

00:01:26,479 --> 00:01:30,270
async/await

00:01:27,780 --> 00:01:33,450
then will quickly cover async i/o and

00:01:30,270 --> 00:01:35,250
you loop then will answer or try to

00:01:33,450 --> 00:01:36,509
answer a question how you should write

00:01:35,250 --> 00:01:39,240
your protocols how I should implement

00:01:36,509 --> 00:01:41,490
them using sockets or protocols or maybe

00:01:39,240 --> 00:01:44,280
you should use streams then I'll present

00:01:41,490 --> 00:01:45,990
you with something new it's a new

00:01:44,280 --> 00:01:48,360
high-performance driver that I

00:01:45,990 --> 00:01:51,270
open-source like two hours ago

00:01:48,360 --> 00:01:53,670
and then we'll recap I have to say that

00:01:51,270 --> 00:01:56,610
there will be no funny cat slides just

00:01:53,670 --> 00:01:59,939
because performance is hard so only sad

00:01:56,610 --> 00:02:03,689
and depressed cards cats from now so

00:01:59,939 --> 00:02:06,990
let's start there should be just one

00:02:03,689 --> 00:02:08,670
obvious way to do it right so we have

00:02:06,990 --> 00:02:12,690
five different ways to the collisions in

00:02:08,670 --> 00:02:13,350
Python first one is to do callbacks and

00:02:12,690 --> 00:02:15,300
deferred

00:02:13,350 --> 00:02:17,160
we stood actually started in the

00:02:15,300 --> 00:02:20,910
originated of this approach one of the

00:02:17,160 --> 00:02:22,770
first major framers at least that used

00:02:20,910 --> 00:02:25,710
that and kind of validated that it is

00:02:22,770 --> 00:02:28,080
possible then we have stackless python

00:02:25,710 --> 00:02:30,240
and green LEDs and I'm pretty sure

00:02:28,080 --> 00:02:33,120
everybody heard about it went LED and

00:02:30,240 --> 00:02:37,920
driven that's a good example of framers

00:02:33,120 --> 00:02:39,660
that use them in short they the programs

00:02:37,920 --> 00:02:43,650
in revamp look like look like normal

00:02:39,660 --> 00:02:46,440
programs in they kind of look like you

00:02:43,650 --> 00:02:48,840
are using threads but instead it's it's

00:02:46,440 --> 00:02:50,460
just one program one thread and every

00:02:48,840 --> 00:02:52,680
point of your code can actually suspend

00:02:50,460 --> 00:02:55,410
and then resume it's a lot of dark magic

00:02:52,680 --> 00:02:58,260
and as guida said it will never be

00:02:55,410 --> 00:03:02,310
merged in in c python so those guys are

00:02:58,260 --> 00:03:08,880
kind of on their own then we have yield

00:03:02,310 --> 00:03:11,010
and it was possible to use generators as

00:03:08,880 --> 00:03:17,070
core teams in python since I believe

00:03:11,010 --> 00:03:19,050
python 2.5 or something and twisted has

00:03:17,070 --> 00:03:21,630
a decorator called enlighten callbacks

00:03:19,050 --> 00:03:24,780
so that we can kinda implement modern

00:03:21,630 --> 00:03:27,000
looking code using coroutines in twist

00:03:24,780 --> 00:03:29,970
and it could do could do this for years

00:03:27,000 --> 00:03:33,750
then in Python 3.3 you from was

00:03:29,970 --> 00:03:36,660
introduced and async async IO benefits

00:03:33,750 --> 00:03:38,790
from it that's that's how most of the I

00:03:36,660 --> 00:03:42,330
think io code is written using utrom and

00:03:38,790 --> 00:03:45,660
then in Python 3 point 5 we have async

00:03:42,330 --> 00:03:47,430
await that's that's the new way and why

00:03:45,660 --> 00:03:49,590
do I think that async await is the

00:03:47,430 --> 00:03:52,110
answer well first of all it's dedicated

00:03:49,590 --> 00:03:55,800
syntax for coroutines it's concise and

00:03:52,110 --> 00:03:58,260
readable it's it's easy to actually once

00:03:55,800 --> 00:03:59,490
over a large chunk of code and see

00:03:58,260 --> 00:04:03,470
what's what's actually going on you will

00:03:59,490 --> 00:04:05,970
never confuse co-routines and generators

00:04:03,470 --> 00:04:07,530
there is now a new built-in type

00:04:05,970 --> 00:04:09,600
recovery since it's actually the first

00:04:07,530 --> 00:04:11,250
time in Python history that we have a

00:04:09,600 --> 00:04:14,970
new dedicated built-in type just

00:04:11,250 --> 00:04:17,790
accordions we also have new concepts

00:04:14,970 --> 00:04:20,250
18:4 and acing with and I believe this

00:04:17,790 --> 00:04:22,890
is something rather unique to buy them

00:04:20,250 --> 00:04:24,690
when we added a sink and a weight a lot

00:04:22,890 --> 00:04:26,150
of people actually told us well we copy

00:04:24,690 --> 00:04:28,100
this from c-sharp

00:04:26,150 --> 00:04:30,229
yes we copied it from the c-sharp but we

00:04:28,100 --> 00:04:32,630
also introduced new things and I believe

00:04:30,229 --> 00:04:35,630
this kind a thing for and a thing with

00:04:32,630 --> 00:04:37,550
are kind of unique like I haven't seen

00:04:35,630 --> 00:04:41,720
any other imperative language that has

00:04:37,550 --> 00:04:43,940
this this construct eating kuwait is

00:04:41,720 --> 00:04:46,190
also generic concept a lot of people

00:04:43,940 --> 00:04:49,070
think that async/await '''l can only

00:04:46,190 --> 00:04:52,220
work with async io that's not true

00:04:49,070 --> 00:04:54,350
actually I think I uses async/await but

00:04:52,220 --> 00:04:55,900
you can build entirely new framework and

00:04:54,350 --> 00:04:58,160
and use them on your own

00:04:55,900 --> 00:05:01,010
that's for instance what David Beasley

00:04:58,160 --> 00:05:02,360
did with his framework called curio he

00:05:01,010 --> 00:05:04,430
uses async/await and completely

00:05:02,360 --> 00:05:08,389
different way from how async/await is

00:05:04,430 --> 00:05:12,530
used in a sink i/o and also async/await

00:05:08,389 --> 00:05:15,710
are it's fast if you write something

00:05:12,530 --> 00:05:19,070
like a Fibonacci calculator you will see

00:05:15,710 --> 00:05:21,349
that it will run just twice slower and

00:05:19,070 --> 00:05:23,419
that is fine actually because even in

00:05:21,349 --> 00:05:25,820
big a cinco programs you don't have as

00:05:23,419 --> 00:05:28,039
much async/await calls as you have

00:05:25,820 --> 00:05:30,650
normal function calls it's you cannot

00:05:28,039 --> 00:05:31,970
even compare it's like 100 times more so

00:05:30,650 --> 00:05:33,590
use async/await

00:05:31,970 --> 00:05:35,539
as much as possible it won't hurt the

00:05:33,590 --> 00:05:40,940
performance you won't you won't see any

00:05:35,539 --> 00:05:43,750
any drawbacks so core genes are subtype

00:05:40,940 --> 00:05:47,990
of of generators but not a classical

00:05:43,750 --> 00:05:51,919
pythonic sense in in Python they share

00:05:47,990 --> 00:05:56,060
the same C struct layout they share like

00:05:51,919 --> 00:05:58,729
99% of the implementation but coercion

00:05:56,060 --> 00:06:02,080
is not a an instance of a generator

00:05:58,729 --> 00:06:05,330
actually and you can you can see this

00:06:02,080 --> 00:06:07,760
the sharing of the machinery if you if

00:06:05,330 --> 00:06:10,010
you for example disassemble accordion

00:06:07,760 --> 00:06:14,659
you will see that it still uses you from

00:06:10,010 --> 00:06:18,349
opcode then we have types core team

00:06:14,659 --> 00:06:20,870
originally we we introduced it to make

00:06:18,349 --> 00:06:23,479
old-style yield from co-routines from

00:06:20,870 --> 00:06:28,070
async io compatible with nuke routines

00:06:23,479 --> 00:06:30,320
that uses async/await syntax because you

00:06:28,070 --> 00:06:33,349
cannot just await on things you you can

00:06:30,320 --> 00:06:35,120
only await on available objects so you

00:06:33,349 --> 00:06:37,370
cannot avoid on number one and you

00:06:35,120 --> 00:06:40,070
cannot avoid on a generator but if you

00:06:37,370 --> 00:06:42,710
wrap the generator in with that

00:06:40,070 --> 00:06:45,110
curtain decorator you can avoid on it

00:06:42,710 --> 00:06:47,840
actually and again David easily use this

00:06:45,110 --> 00:06:49,700
kind of creatively interior if you are

00:06:47,840 --> 00:06:51,440
interested in async/await I definitely

00:06:49,700 --> 00:06:53,510
recommend you to take a look at how

00:06:51,440 --> 00:06:54,830
async IO is implemented and how euro is

00:06:53,510 --> 00:06:58,430
implemented just to compare it to

00:06:54,830 --> 00:07:01,250
different approaches and then we have a

00:06:58,430 --> 00:07:04,250
bunch of protocols or async iterators

00:07:01,250 --> 00:07:06,980
and async context managers let's move on

00:07:04,250 --> 00:07:11,210
let's talk about a single dbv siphon and

00:07:06,980 --> 00:07:13,790
you've a loop so async IO is developed

00:07:11,210 --> 00:07:18,410
by Guido himself originally I think a

00:07:13,790 --> 00:07:20,330
lot of it is inspired by twisted and

00:07:18,410 --> 00:07:22,280
it's actually good because twists that

00:07:20,330 --> 00:07:24,530
existed for I don't know 20 years

00:07:22,280 --> 00:07:25,970
something and they validated that the

00:07:24,530 --> 00:07:28,940
dis concept of a synchronous programming

00:07:25,970 --> 00:07:31,400
Python actually works so I think we

00:07:28,940 --> 00:07:33,950
copied quite a lot from twisted and

00:07:31,400 --> 00:07:35,900
twisted actually plans to to use async

00:07:33,950 --> 00:07:37,960
IO at some point when they fully might

00:07:35,900 --> 00:07:42,200
migrate to Python 3 they will just use

00:07:37,960 --> 00:07:44,630
async IO event loop a lot of people call

00:07:42,200 --> 00:07:47,450
async iframe work well it's not a

00:07:44,630 --> 00:07:50,570
framework I would I would call it a tool

00:07:47,450 --> 00:07:52,160
box actually it doesn't implement HTTP

00:07:50,570 --> 00:07:55,040
for instance or any other high level

00:07:52,160 --> 00:07:57,290
protocols it just provides the machinery

00:07:55,040 --> 00:07:59,120
and api's for you to develop this this

00:07:57,290 --> 00:08:03,820
kind of stuff you want HTTP you probably

00:07:59,120 --> 00:08:07,370
would use a HTTP for that if you want

00:08:03,820 --> 00:08:09,290
memcache driver you go and google it and

00:08:07,370 --> 00:08:12,470
that's also part of standard library

00:08:09,290 --> 00:08:16,090
which is both good and bad why is it bad

00:08:12,470 --> 00:08:20,180
Python has slow release cadence we see

00:08:16,090 --> 00:08:23,060
new Python a major releases every year

00:08:20,180 --> 00:08:27,440
and a half and bug-fix releases usually

00:08:23,060 --> 00:08:29,840
are half a year apart and I would say

00:08:27,440 --> 00:08:31,730
that for a think i/o sometimes it's not

00:08:29,840 --> 00:08:33,800
enough sometimes we discover bars and we

00:08:31,730 --> 00:08:37,400
want to fix them as soon as possible but

00:08:33,800 --> 00:08:40,729
we have to stick with the Python release

00:08:37,400 --> 00:08:43,160
cycle but it's also good because because

00:08:40,729 --> 00:08:44,990
you kinda know that async i/o will will

00:08:43,160 --> 00:08:47,420
stay with us forever I hope it will be

00:08:44,990 --> 00:08:50,120
supported by someone always because it's

00:08:47,420 --> 00:08:53,460
a part of the standard library and also

00:08:50,120 --> 00:08:54,839
Python has a huge network of

00:08:53,460 --> 00:08:57,270
our billboards with different

00:08:54,839 --> 00:08:58,920
architectures and different operating

00:08:57,270 --> 00:09:01,350
systems and it's quite important

00:08:58,920 --> 00:09:05,520
actually to test something as convoluted

00:09:01,350 --> 00:09:09,270
and as hard as IO on different platforms

00:09:05,520 --> 00:09:11,370
so it's good a sync iris is quite stable

00:09:09,270 --> 00:09:15,029
right now and will be even more stable

00:09:11,370 --> 00:09:19,589
pretty soon so what's inside I think i/o

00:09:15,029 --> 00:09:22,350
so we have standardized and pluggable

00:09:19,589 --> 00:09:24,510
event loop actually async i/o from the

00:09:22,350 --> 00:09:26,760
beginning was it was envisioned in a way

00:09:24,510 --> 00:09:28,910
that you can swap the event loop

00:09:26,760 --> 00:09:32,520
implementation with something different

00:09:28,910 --> 00:09:37,440
it has it defines protocols and

00:09:32,520 --> 00:09:41,399
transports that's one way to actually

00:09:37,440 --> 00:09:43,410
marry callback style programming and and

00:09:41,399 --> 00:09:46,170
async/await is to actually develop

00:09:43,410 --> 00:09:49,440
protocols using low-level primitives

00:09:46,170 --> 00:09:52,709
such as protocols it also has factories

00:09:49,440 --> 00:09:54,750
for servers and connections and streams

00:09:52,709 --> 00:09:57,900
and this is also quite important because

00:09:54,750 --> 00:10:01,260
if you implement a server let's say

00:09:57,900 --> 00:10:02,610
using using blocking sockets you

00:10:01,260 --> 00:10:04,050
implement at once and then you start

00:10:02,610 --> 00:10:05,910
implement a second time you will see

00:10:04,050 --> 00:10:07,560
that you have lots and lots of

00:10:05,910 --> 00:10:08,790
boilerplate code that kind of looks

00:10:07,560 --> 00:10:10,860
looks the same every time

00:10:08,790 --> 00:10:13,020
so async i/o takes care of that and

00:10:10,860 --> 00:10:14,790
factors out all of this implementation

00:10:13,020 --> 00:10:17,670
and convenient helpers for creating

00:10:14,790 --> 00:10:20,420
servers and creating connections it also

00:10:17,670 --> 00:10:23,130
defines features and tasks tasks are

00:10:20,420 --> 00:10:25,500
tasks tasks is something that actually

00:10:23,130 --> 00:10:27,480
runs the core team that pushes the value

00:10:25,500 --> 00:10:30,779
into core teams that suspends them and

00:10:27,480 --> 00:10:32,970
resumes them in a framework independent

00:10:30,779 --> 00:10:35,940
way it's called court in rapper Colson

00:10:32,970 --> 00:10:40,490
Runner actually and futures features

00:10:35,940 --> 00:10:45,060
allow you to to interface with callbacks

00:10:40,490 --> 00:10:45,839
that's that's how you actually introduce

00:10:45,060 --> 00:10:48,720
async/await

00:10:45,839 --> 00:10:53,670
into something that uses callbacks it

00:10:48,720 --> 00:10:55,649
also has interfaces for creating and

00:10:53,670 --> 00:10:58,050
communicating with sub processes as

00:10:55,649 --> 00:11:01,440
synchronously it has queues and by the

00:10:58,050 --> 00:11:02,820
way queue is is a very useful class you

00:11:01,440 --> 00:11:05,970
should you should definitely use it it's

00:11:02,820 --> 00:11:06,720
it's exceptionally hard to create an

00:11:05,970 --> 00:11:10,290
asynchronous queue

00:11:06,720 --> 00:11:12,120
that supports constellation stuff all

00:11:10,290 --> 00:11:14,550
the stuff like that without without box

00:11:12,120 --> 00:11:17,959
like we still fixed a lot of Hugh box in

00:11:14,550 --> 00:11:20,129
3.5 the to

00:11:17,959 --> 00:11:22,829
cubes are useful for things like

00:11:20,129 --> 00:11:24,689
connection pools for instance just

00:11:22,829 --> 00:11:27,540
definitely check it out and we also have

00:11:24,689 --> 00:11:29,279
locks events semaphores

00:11:27,540 --> 00:11:31,500
everything like that everything that

00:11:29,279 --> 00:11:35,370
everybody nobody knows how to use

00:11:31,500 --> 00:11:36,870
actually and as Lucas Wong said on his

00:11:35,370 --> 00:11:38,490
talkin Pike on the US couple of months

00:11:36,870 --> 00:11:42,720
ago if you if you have that locks you

00:11:38,490 --> 00:11:44,790
can still have them in a sing heyo so

00:11:42,720 --> 00:11:47,819
event loop is the foundation it's the

00:11:44,790 --> 00:11:52,189
engine that actually writes that

00:11:47,819 --> 00:11:55,110
actually executes a single code it also

00:11:52,189 --> 00:11:56,879
it also provides factories for testing

00:11:55,110 --> 00:11:58,550
futures it's also net I almost

00:11:56,879 --> 00:12:00,990
plex.earth at the engine that actually

00:11:58,550 --> 00:12:04,800
reads the data and pushes the data to

00:12:00,990 --> 00:12:07,170
the wire it provides api's for a lot

00:12:04,800 --> 00:12:09,959
low-level api s-- for scheduling code

00:12:07,170 --> 00:12:12,000
base for scheduling timed events for

00:12:09,959 --> 00:12:15,180
working with sub processes and handling

00:12:12,000 --> 00:12:18,540
unix signals and the best part about it

00:12:15,180 --> 00:12:21,959
is that you can replace it so that's

00:12:18,540 --> 00:12:25,550
what we kind of did with you loop you to

00:12:21,959 --> 00:12:27,990
loop is 99.9% compatible with 18k off

00:12:25,550 --> 00:12:32,490
I'm not aware of any incompatibilities

00:12:27,990 --> 00:12:34,290
but maybe there are some as far as I

00:12:32,490 --> 00:12:36,059
know you can drop in you've a loop in

00:12:34,290 --> 00:12:39,209
pretty much any program and it will just

00:12:36,059 --> 00:12:42,720
work it's written in sytem and by the

00:12:39,209 --> 00:12:46,199
way this icon is just amazing it's it's

00:12:42,720 --> 00:12:48,500
unfortunate that it's not as widespread

00:12:46,199 --> 00:12:50,550
and I think it's kind of

00:12:48,500 --> 00:12:52,230
underappreciated what you can do inside

00:12:50,550 --> 00:12:54,689
on essentially it's a superset of

00:12:52,230 --> 00:12:57,149
whitening language you can use you can't

00:12:54,689 --> 00:13:00,059
strictly type it and it will compile to

00:12:57,149 --> 00:13:02,009
see and you will have C speed you can

00:13:00,059 --> 00:13:04,620
easily achieve it with the syntax closer

00:13:02,009 --> 00:13:06,470
to Python so definitely check out Python

00:13:04,620 --> 00:13:09,120
and try to site on and try to use it

00:13:06,470 --> 00:13:11,399
you've abused asleep you levy is

00:13:09,120 --> 00:13:14,339
something that keeps not just running

00:13:11,399 --> 00:13:18,929
actually no JSU's asleep will be read as

00:13:14,339 --> 00:13:19,620
its event loop and it's actually a good

00:13:18,929 --> 00:13:22,560
thing because

00:13:19,620 --> 00:13:25,830
no dress is super widespread and it's

00:13:22,560 --> 00:13:28,920
it's it's very very well tested so levy

00:13:25,830 --> 00:13:32,790
is stable and it's fast it also provides

00:13:28,920 --> 00:13:36,180
fast tasks and future's so even your

00:13:32,790 --> 00:13:39,600
async/await code runs faster on you the

00:13:36,180 --> 00:13:43,410
loop by about 30% and it's also thanks

00:13:39,600 --> 00:13:46,230
to libuv and a few hacks it's super fun

00:13:43,410 --> 00:13:49,200
it has super fast IO so how hot is your

00:13:46,230 --> 00:13:51,510
loop well compared to a sink io it's two

00:13:49,200 --> 00:13:54,510
to four times faster on simple

00:13:51,510 --> 00:13:57,810
benchmarks like echo server again nobody

00:13:54,510 --> 00:13:59,700
nobody probably deploys echo servers in

00:13:57,810 --> 00:14:02,970
in real life so as soon as you add more

00:13:59,700 --> 00:14:07,080
and more Python code of course it will

00:14:02,970 --> 00:14:09,540
become slower but again in even in real

00:14:07,080 --> 00:14:11,220
applications I've seen reports that a

00:14:09,540 --> 00:14:14,250
singer that you've a loop runs called

00:14:11,220 --> 00:14:16,290
about 30% faster and also the latency

00:14:14,250 --> 00:14:20,790
distribution is much better with you

00:14:16,290 --> 00:14:23,640
loop so it's faster than a singer what

00:14:20,790 --> 00:14:26,790
about other platforms and and framers

00:14:23,640 --> 00:14:30,150
for instance the same X Server written

00:14:26,790 --> 00:14:32,040
in Python that uses and which is 2 times

00:14:30,150 --> 00:14:34,500
faster than no GS and that's kind of

00:14:32,040 --> 00:14:37,140
interesting because no GS is itself

00:14:34,500 --> 00:14:39,510
written mostly in v8 that's the

00:14:37,140 --> 00:14:41,459
JavaScript implementation it uses sleepy

00:14:39,510 --> 00:14:44,070
V which is written in C and there is a

00:14:41,459 --> 00:14:46,709
thin layer of JavaScript on top of it so

00:14:44,070 --> 00:14:49,440
still even that uses the same the BV is

00:14:46,709 --> 00:14:53,130
two times faster for the same homes the

00:14:49,440 --> 00:14:59,160
same amount of code it is as fast as go

00:14:53,130 --> 00:15:00,900
run with go max Marx talks said to one

00:14:59,160 --> 00:15:06,690
that essentially means go cannot

00:15:00,900 --> 00:15:08,180
parallel wise on multiple CPU load but

00:15:06,690 --> 00:15:11,040
still it's it's quite an impressive

00:15:08,180 --> 00:15:13,529
impressive result because it's like a

00:15:11,040 --> 00:15:16,730
fully compiled language and itself it's

00:15:13,529 --> 00:15:20,250
also has I think a bit more efficient

00:15:16,730 --> 00:15:21,779
implementation of Io than than Lavigne

00:15:20,250 --> 00:15:25,430
just because Libby is trying to be

00:15:21,779 --> 00:15:28,350
generic it supports Windows it supports

00:15:25,430 --> 00:15:31,950
supports UNIX going supports it too but

00:15:28,350 --> 00:15:35,310
in slightly different way anyways

00:15:31,950 --> 00:15:38,100
and of course it's much faster than

00:15:35,310 --> 00:15:40,589
twisted on tornado just because it's it

00:15:38,100 --> 00:15:43,279
uses a lot a lot of it isn't see like

00:15:40,589 --> 00:15:46,050
most of you de loop isn't see so

00:15:43,279 --> 00:15:47,970
initially my idea or for this talk was

00:15:46,050 --> 00:15:50,459
to end with this slide just use you loop

00:15:47,970 --> 00:15:53,190
thank you for your time questions but

00:15:50,459 --> 00:15:56,100
unfortunately unfortunately it's not

00:15:53,190 --> 00:15:59,610
very easy so part three let's talk about

00:15:56,100 --> 00:16:01,260
sockets streams and protocols that's

00:15:59,610 --> 00:16:03,269
basically one obvious way to do it

00:16:01,260 --> 00:16:06,209
episode two so what should you choose

00:16:03,269 --> 00:16:08,610
should you use co-routines like sucks

00:16:06,209 --> 00:16:10,529
and all suck receive suck connect or

00:16:08,610 --> 00:16:14,910
should we use high level streaming API

00:16:10,529 --> 00:16:19,350
or maybe you should use lower low level

00:16:14,910 --> 00:16:21,329
protocols and transfers here is an echo

00:16:19,350 --> 00:16:23,190
server implemented with loop sock

00:16:21,329 --> 00:16:25,740
methods and if you look at it closely

00:16:23,190 --> 00:16:29,130
you will see that if you can't drop a

00:16:25,740 --> 00:16:30,990
sink and a wait here keywords it looks

00:16:29,130 --> 00:16:37,110
like like a normal working code that

00:16:30,990 --> 00:16:39,540
that uses the socket module so it is

00:16:37,110 --> 00:16:41,430
kinda convenient when you have lots and

00:16:39,540 --> 00:16:45,769
lots of code and old-style called

00:16:41,430 --> 00:16:49,470
blocking code you can kinda easily

00:16:45,769 --> 00:16:51,660
convert it to a sink and a weight here

00:16:49,470 --> 00:16:55,740
are streams here is the streams version

00:16:51,660 --> 00:16:57,420
of the echo server it's quite it's quite

00:16:55,740 --> 00:16:59,790
high level as you see you you don't you

00:16:57,420 --> 00:17:03,089
don't work with sockets anymore you have

00:16:59,790 --> 00:17:05,400
reader and writer and here is a

00:17:03,089 --> 00:17:07,709
low-level implementation of echo server

00:17:05,400 --> 00:17:11,189
using protocols so essentially protocol

00:17:07,709 --> 00:17:13,650
is something that the event would just

00:17:11,189 --> 00:17:16,260
pushes the data in and protocol has a

00:17:13,650 --> 00:17:19,860
transport to push the data back to the

00:17:16,260 --> 00:17:21,929
client to the server so the key method

00:17:19,860 --> 00:17:25,620
here is data received that that's like

00:17:21,929 --> 00:17:27,390
the main method it the event who pushes

00:17:25,620 --> 00:17:29,970
the data to the data received and

00:17:27,390 --> 00:17:33,299
protocol can process the data and then

00:17:29,970 --> 00:17:35,490
called transfer dot write to actually

00:17:33,299 --> 00:17:39,260
send that the process data or send the

00:17:35,490 --> 00:17:39,260
response back to the core

00:17:39,350 --> 00:17:44,280
for servers it's quite a simple

00:17:42,330 --> 00:17:44,850
implementation but you can imagine it

00:17:44,280 --> 00:17:49,590
gets pretty

00:17:44,850 --> 00:17:56,190
Carrie or for more complex protocols so

00:17:49,590 --> 00:18:00,840
downsides when you use low-level loop

00:17:56,190 --> 00:18:03,750
dot suck methods loop cannot buffer for

00:18:00,840 --> 00:18:07,139
you so you are responsible to to

00:18:03,750 --> 00:18:09,179
implement the buffering on top and you

00:18:07,139 --> 00:18:11,190
also have no flow control which without

00:18:09,179 --> 00:18:12,809
buffers doesn't make any sense you you

00:18:11,190 --> 00:18:14,820
don't install control but when you start

00:18:12,809 --> 00:18:16,710
implement the buffers you won't have it

00:18:14,820 --> 00:18:20,490
and it's quite a tricky thing to

00:18:16,710 --> 00:18:21,990
implement correctly and another thing

00:18:20,490 --> 00:18:24,779
where you should why you shouldn't use

00:18:21,990 --> 00:18:28,080
it is just because the event loop has no

00:18:24,779 --> 00:18:31,529
idea what are you doing right now let's

00:18:28,080 --> 00:18:35,039
say you are reading some data ok event

00:18:31,529 --> 00:18:39,230
loop will add your file descriptor to a

00:18:35,039 --> 00:18:46,379
selector which can be equal or KQ on

00:18:39,230 --> 00:18:48,509
unix and essentially wait or for an

00:18:46,379 --> 00:18:50,639
event and when when it receives this

00:18:48,509 --> 00:18:52,350
event it will try to read the data to

00:18:50,639 --> 00:18:54,149
the data back to you but it will also

00:18:52,350 --> 00:18:57,259
remove the file descriptor from the

00:18:54,149 --> 00:18:59,129
selector that's an extra system call

00:18:57,259 --> 00:19:02,789
because it doesn't know will you

00:18:59,129 --> 00:19:04,830
continue reading the data or will you do

00:19:02,789 --> 00:19:06,870
your write the data now or will you just

00:19:04,830 --> 00:19:09,960
stop or will you close it close the

00:19:06,870 --> 00:19:12,389
connection so it cannot predict what was

00:19:09,960 --> 00:19:14,399
going on when use streams and streams

00:19:12,389 --> 00:19:17,610
are by the way you are are using

00:19:14,399 --> 00:19:20,309
protocols event hook just knows because

00:19:17,610 --> 00:19:22,289
you you have an intent just keep sending

00:19:20,309 --> 00:19:24,840
the data to my data received or to my

00:19:22,289 --> 00:19:27,419
stream and when I don't need this data I

00:19:24,840 --> 00:19:29,850
will pause the connection myself so even

00:19:27,419 --> 00:19:33,720
hope can actually optimize for that and

00:19:29,850 --> 00:19:35,039
flow control is kind important I like

00:19:33,720 --> 00:19:37,110
this picture because it illustrates that

00:19:35,039 --> 00:19:38,909
sometimes you can you kind of have to

00:19:37,110 --> 00:19:43,049
push back on something slow or something

00:19:38,909 --> 00:19:45,320
that you don't want to use right now so

00:19:43,049 --> 00:19:48,419
we CPI you should use you should use

00:19:45,320 --> 00:19:50,460
loop dot sock methods when when you are

00:19:48,419 --> 00:19:54,809
quickly prototyping something or when

00:19:50,460 --> 00:19:56,909
you are forging some existing code but I

00:19:54,809 --> 00:19:58,380
would highly recommend you to actually

00:19:56,909 --> 00:20:01,559
stick to strings

00:19:58,380 --> 00:20:03,120
even for reporting code just rewrite it

00:20:01,559 --> 00:20:05,370
in strings because streams are much

00:20:03,120 --> 00:20:07,740
easier to use you can just say give me

00:20:05,370 --> 00:20:11,130
exactly this amount of data or you can

00:20:07,740 --> 00:20:14,100
tell streams read until you see slash n

00:20:11,130 --> 00:20:18,179
or something like that and it will do it

00:20:14,100 --> 00:20:20,190
it also implements a buffer reason write

00:20:18,179 --> 00:20:22,110
buffers quite quite efficiently and you

00:20:20,190 --> 00:20:26,400
can use async/await to program the

00:20:22,110 --> 00:20:29,610
entire protocols with streams and use

00:20:26,400 --> 00:20:31,830
protocols and transports or performance

00:20:29,610 --> 00:20:33,840
actually if you want exceptional

00:20:31,830 --> 00:20:36,840
performance you have to go you have to

00:20:33,840 --> 00:20:38,789
go a low level so for this talk let's

00:20:36,840 --> 00:20:40,919
talk let's focus on protocols and

00:20:38,789 --> 00:20:44,280
transports and again again it's kinda

00:20:40,919 --> 00:20:46,710
important for your application code you

00:20:44,280 --> 00:20:48,659
should always use async/await never even

00:20:46,710 --> 00:20:51,090
touch never think about transport

00:20:48,659 --> 00:20:54,000
transfers and protocols this stuff is

00:20:51,090 --> 00:20:56,690
just for drivers drivers for PostgreSQL

00:20:54,000 --> 00:20:59,640
for memcache for regice for any kind of

00:20:56,690 --> 00:21:02,400
this kind of code high level code should

00:20:59,640 --> 00:21:05,669
never think about protocols always use

00:21:02,400 --> 00:21:08,880
async/await it will be enough so let's

00:21:05,669 --> 00:21:09,750
focus on protocols so as we mentioned

00:21:08,880 --> 00:21:12,539
before

00:21:09,750 --> 00:21:15,809
loop pushes data to protocols protocol

00:21:12,539 --> 00:21:18,179
send data back using transfers and

00:21:15,809 --> 00:21:21,750
protocols can implement specialized read

00:21:18,179 --> 00:21:25,799
and write buffers they can also do flow

00:21:21,750 --> 00:21:29,549
control they can hint the event loop or

00:21:25,799 --> 00:21:33,030
through the transport both resume and

00:21:29,549 --> 00:21:35,220
post read methods and you have full

00:21:33,030 --> 00:21:38,340
control over how I always performed you

00:21:35,220 --> 00:21:41,730
you call transfer that's right you can

00:21:38,340 --> 00:21:44,220
pause or resume data consumption so you

00:21:41,730 --> 00:21:48,230
have you have you have to control the

00:21:44,220 --> 00:21:51,510
i/o so how to use protocol transfers

00:21:48,230 --> 00:21:53,280
there are basically two strategies the

00:21:51,510 --> 00:21:55,679
first one is you implement your own

00:21:53,280 --> 00:21:58,200
abstractions your own buffering

00:21:55,679 --> 00:22:01,020
buffering and your own stream

00:21:58,200 --> 00:22:03,360
abstractions and a good example of that

00:22:01,020 --> 00:22:06,120
is a HTTP that's that's what they do

00:22:03,360 --> 00:22:07,919
they have buffers and then streams

00:22:06,120 --> 00:22:10,260
specifically designed to handle and

00:22:07,919 --> 00:22:11,820
parse HTTP protocol and then they they

00:22:10,260 --> 00:22:14,010
just use async/await

00:22:11,820 --> 00:22:17,070
it's a fine approach it will be slower

00:22:14,010 --> 00:22:19,890
than using then using callbacks and

00:22:17,070 --> 00:22:21,840
accelerating everything in C but but

00:22:19,890 --> 00:22:25,410
it's it's quite it's quite good still

00:22:21,840 --> 00:22:27,870
quite good so the second strategy is to

00:22:25,410 --> 00:22:31,980
actually implement the the whole

00:22:27,870 --> 00:22:35,970
protocol parsing in in callbacks and

00:22:31,980 --> 00:22:38,360
then create create a say that allows you

00:22:35,970 --> 00:22:42,570
to use a seam in a way and the main

00:22:38,360 --> 00:22:44,790
reason why this why this is a better

00:22:42,570 --> 00:22:46,980
might be a better strategy and why this

00:22:44,790 --> 00:22:50,160
can offer better performance is because

00:22:46,980 --> 00:22:51,990
you can just drop by them completely you

00:22:50,160 --> 00:22:56,580
can you can go low-level you can use

00:22:51,990 --> 00:22:58,620
seitan you can use C so part for async

00:22:56,580 --> 00:23:00,740
the dream this is something that I just

00:22:58,620 --> 00:23:04,500
opened sourced a couple of hours ago

00:23:00,740 --> 00:23:06,420
this is right now the fastest PostgreSQL

00:23:04,500 --> 00:23:08,280
driver for a single and for python

00:23:06,420 --> 00:23:11,310
actually it's two times faster than

00:23:08,280 --> 00:23:13,410
psycho PG it completely really

00:23:11,310 --> 00:23:15,240
implements the the protocol from ground

00:23:13,410 --> 00:23:18,330
up it doesn't usually pick you that's

00:23:15,240 --> 00:23:19,200
the the factors library for working with

00:23:18,330 --> 00:23:21,680
PostgreSQL

00:23:19,200 --> 00:23:25,830
so we just implemented it completely

00:23:21,680 --> 00:23:27,330
from scratch it uses Postgres binary

00:23:25,830 --> 00:23:29,490
data formant and by the way when you are

00:23:27,330 --> 00:23:32,100
implementing protocol and you have a

00:23:29,490 --> 00:23:34,650
choice use text or binary always choose

00:23:32,100 --> 00:23:36,510
binary it's easier to read binary this

00:23:34,650 --> 00:23:38,250
is usually just less data because the

00:23:36,510 --> 00:23:40,440
encoding is more efficient and you can

00:23:38,250 --> 00:23:42,600
process this much faster you can because

00:23:40,440 --> 00:23:44,880
well how binary formats works usually

00:23:42,600 --> 00:23:48,360
you have a length field that tells you

00:23:44,880 --> 00:23:50,190
how how much data follows this frame and

00:23:48,360 --> 00:23:52,050
then you have another one sitting with

00:23:50,190 --> 00:23:55,320
frames much faster you can decode types

00:23:52,050 --> 00:23:58,710
much faster so always choose binary and

00:23:55,320 --> 00:24:01,650
also not all Postgres types can can be

00:23:58,710 --> 00:24:06,060
encoded and text and actually decoded

00:24:01,650 --> 00:24:08,010
from text so small also composite types

00:24:06,060 --> 00:24:09,540
for instance if you have a recursive

00:24:08,010 --> 00:24:14,280
composite type it's just not possible to

00:24:09,540 --> 00:24:15,750
deposit in in take a Pidgey so what so

00:24:14,280 --> 00:24:18,630
what we did for async we do we actually

00:24:15,750 --> 00:24:20,790
forgot about the BPI completely there is

00:24:18,630 --> 00:24:22,400
no db-api for async/await but for

00:24:20,790 --> 00:24:24,200
instance what a you could read us

00:24:22,400 --> 00:24:28,160
they kinda sprinkle a sink and a weight

00:24:24,200 --> 00:24:30,530
on top of on top of existing db-api so

00:24:28,160 --> 00:24:34,250
our idea was let's let's build a driver

00:24:30,530 --> 00:24:37,610
that just is tailored for both Grace and

00:24:34,250 --> 00:24:42,110
uses Potter's features and we also

00:24:37,610 --> 00:24:48,680
support all built-in Padres types there

00:24:42,110 --> 00:24:51,560
is basically so Postgres lost prepared

00:24:48,680 --> 00:24:53,270
statements because it and it doesn't

00:24:51,560 --> 00:24:54,920
need to actually parse the same query

00:24:53,270 --> 00:24:56,780
over and over again when you prepare a

00:24:54,920 --> 00:24:59,420
statement it has basically some

00:24:56,780 --> 00:25:01,190
structure on the server with a plan with

00:24:59,420 --> 00:25:03,170
the parsed query and progress already

00:25:01,190 --> 00:25:05,510
know knows how to accept your arguments

00:25:03,170 --> 00:25:07,310
and do this kind of stuff so we use

00:25:05,510 --> 00:25:10,040
prepared statements every time even when

00:25:07,310 --> 00:25:12,170
you don't explicitly create them we have

00:25:10,040 --> 00:25:13,640
an overview cache of of prepared

00:25:12,170 --> 00:25:16,400
statements and we do that transparently

00:25:13,640 --> 00:25:19,010
for you we also dynamically build

00:25:16,400 --> 00:25:22,730
pipelines were efficiently encoding and

00:25:19,010 --> 00:25:25,040
decoding decoding data so the pipeline

00:25:22,730 --> 00:25:28,430
is essentially an array of pointers to

00:25:25,040 --> 00:25:30,710
see functions that can process process

00:25:28,430 --> 00:25:35,690
the stream like with enormous speed so

00:25:30,710 --> 00:25:38,480
and it actually shows this chart

00:25:35,690 --> 00:25:42,830
compares different Postgres drivers or

00:25:38,480 --> 00:25:45,220
different languages the fastest one is a

00:25:42,830 --> 00:25:48,710
synch period manages to push almost

00:25:45,220 --> 00:25:52,850
900,000 queries

00:25:48,710 --> 00:25:55,850
to the server the second one is a OPG

00:25:52,850 --> 00:25:57,980
that's another driver that uses slip PQ

00:25:55,850 --> 00:25:59,630
which is also is written in C but

00:25:57,980 --> 00:26:02,000
unfortunately psychopathy doesn't

00:25:59,630 --> 00:26:04,460
doesn't provide an efficient async

00:26:02,000 --> 00:26:07,850
interface so so it's slower and also a

00:26:04,460 --> 00:26:10,700
sink with a LPG and psycho Petri they

00:26:07,850 --> 00:26:14,390
use text Pro text baiting coding so it

00:26:10,700 --> 00:26:16,789
always will be slower then you will see

00:26:14,390 --> 00:26:20,120
to go implementations and then you will

00:26:16,789 --> 00:26:21,799
see nodejs drivers which are just ten

00:26:20,120 --> 00:26:24,770
times slower

00:26:21,799 --> 00:26:27,409
the funny part about this one is that no

00:26:24,770 --> 00:26:29,690
JSP G's is actually pure JavaScript

00:26:27,409 --> 00:26:32,630
implementation of the driver and pidgin

00:26:29,690 --> 00:26:34,640
native is using lip PQ so somehow a lot

00:26:32,630 --> 00:26:37,940
of JavaScript is faster than I don't see

00:26:34,640 --> 00:26:39,559
I have no idea how the funny thing about

00:26:37,940 --> 00:26:41,419
this performance is that there is

00:26:39,559 --> 00:26:43,429
another library it's not a part of this

00:26:41,419 --> 00:26:45,710
chart because it's kind of slow

00:26:43,429 --> 00:26:47,750
it's called pi PostgreSQL nobody knows

00:26:45,710 --> 00:26:51,500
about it we use this for several years

00:26:47,750 --> 00:26:53,029
and then we just created a sync with you

00:26:51,500 --> 00:26:56,659
anyways it's a pure Python

00:26:53,029 --> 00:26:59,720
implementation and it's as fast as pure

00:26:56,659 --> 00:27:01,370
JavaScript implementation so everybody

00:26:59,720 --> 00:27:02,750
is saying that Python is slower than

00:27:01,370 --> 00:27:06,080
JavaScript you shouldn't you shouldn't

00:27:02,750 --> 00:27:07,549
you shouldn't use it but we kind of saw

00:27:06,080 --> 00:27:10,460
that it's possible to write a pure

00:27:07,549 --> 00:27:16,490
Python code as fast as node.js code so

00:27:10,460 --> 00:27:18,590
maybe Python isn't that slow so itching

00:27:16,490 --> 00:27:21,140
PGR architecture it's basically

00:27:18,590 --> 00:27:22,669
implemented in the meat of it is

00:27:21,140 --> 00:27:24,950
implemented in core protocol core

00:27:22,669 --> 00:27:27,169
protocol is something written in syphon

00:27:24,950 --> 00:27:30,020
it uses callbacks to process the the

00:27:27,169 --> 00:27:32,830
protocol then we have a protocol class

00:27:30,020 --> 00:27:35,720
that just wraps core protocol and

00:27:32,830 --> 00:27:38,059
inserts some future objects into it so

00:27:35,720 --> 00:27:41,360
that you can use async away and the rest

00:27:38,059 --> 00:27:43,760
of async Petry is just pure Python

00:27:41,360 --> 00:27:46,940
implementation that just implements the

00:27:43,760 --> 00:27:49,549
high level the high level API so how

00:27:46,940 --> 00:27:50,990
would you parse parse progress protocol

00:27:49,549 --> 00:27:53,299
naive approach would be just to use

00:27:50,990 --> 00:27:55,970
Python bites and memory views but

00:27:53,299 --> 00:27:57,860
unfortunately doing so will cause it

00:27:55,970 --> 00:27:59,210
will cause a lot of Python objects to be

00:27:57,860 --> 00:28:01,070
creative and you will you

00:27:59,210 --> 00:28:04,669
we'll actually see how how long you

00:28:01,070 --> 00:28:06,799
spend memory on memory allocation so the

00:28:04,669 --> 00:28:10,820
solution is to use site on and go or go

00:28:06,799 --> 00:28:15,200
to the see see see types and just don't

00:28:10,820 --> 00:28:17,750
even touch by Python bites and and

00:28:15,200 --> 00:28:19,669
memory reviews so this is a preview of

00:28:17,750 --> 00:28:20,299
read buffer it's it's it's a bit bigger

00:28:19,669 --> 00:28:23,000
than this

00:28:20,299 --> 00:28:24,950
it's it's API but you can see the first

00:28:23,000 --> 00:28:27,470
method is the most important feed beta

00:28:24,950 --> 00:28:29,929
that's what a protocol data received

00:28:27,470 --> 00:28:31,700
actually calls protocol data received

00:28:29,929 --> 00:28:33,529
has just two lines in it the first one

00:28:31,700 --> 00:28:35,299
pushes the data to read buffer and the

00:28:33,529 --> 00:28:37,370
second one calls a function that just

00:28:35,299 --> 00:28:40,279
reads from the buffer and this buffer is

00:28:37,370 --> 00:28:41,450
kinda tailored for progress protocol it

00:28:40,279 --> 00:28:45,980
has a low-level

00:28:41,450 --> 00:28:49,100
read in theory to in 16 and the second

00:28:45,980 --> 00:28:52,070
most important call here is try read

00:28:49,100 --> 00:28:57,559
bytes try read by it either returns you

00:28:52,070 --> 00:28:59,539
a low-level see they arrive or it

00:28:57,559 --> 00:29:01,039
returns a you null pointer and if it

00:28:59,539 --> 00:29:03,890
returns and now pointer then you

00:29:01,039 --> 00:29:06,740
actually call read which returns you

00:29:03,890 --> 00:29:08,840
quite an object which is much slower but

00:29:06,740 --> 00:29:11,600
most of the time 99% of the time try

00:29:08,840 --> 00:29:15,730
read white succeeds and we can we can

00:29:11,600 --> 00:29:18,380
avoid creating any Python objects so

00:29:15,730 --> 00:29:20,149
again high level or high level logic of

00:29:18,380 --> 00:29:24,799
acing a jury is built on pure Python

00:29:20,149 --> 00:29:26,809
that is how you can actually use it you

00:29:24,799 --> 00:29:30,020
can see it's a pretty high level a high

00:29:26,809 --> 00:29:32,059
level API prepare statement we enter a

00:29:30,020 --> 00:29:35,570
transaction with a sink with and we

00:29:32,059 --> 00:29:37,250
iterate over scrollable cursor part five

00:29:35,570 --> 00:29:41,270
let's recap so don't be afraid of

00:29:37,250 --> 00:29:43,190
protocols use them to implement really

00:29:41,270 --> 00:29:45,230
really high performance drivers and use

00:29:43,190 --> 00:29:48,200
Python for you siphon for a low-level

00:29:45,230 --> 00:29:50,570
code it's really much easier to code

00:29:48,200 --> 00:29:52,669
inside them than and see you can quickly

00:29:50,570 --> 00:29:54,950
refactoring code completely change

00:29:52,669 --> 00:29:56,990
everything and it will just work

00:29:54,950 --> 00:29:58,490
async/await should always be used in

00:29:56,990 --> 00:30:00,679
your application call don't think about

00:29:58,490 --> 00:30:03,590
protocols and transport use use only

00:30:00,679 --> 00:30:04,159
high level code and again once you have

00:30:03,590 --> 00:30:06,049
passed

00:30:04,159 --> 00:30:07,760
database drivers memcache driver stuff

00:30:06,049 --> 00:30:08,650
like that and you use your loop you will

00:30:07,760 --> 00:30:11,460
see you

00:30:08,650 --> 00:30:13,690
your application being much much faster

00:30:11,460 --> 00:30:16,330
lootcrate future was introduced in

00:30:13,690 --> 00:30:19,420
Python 3 point 5 point 2 actually that's

00:30:16,330 --> 00:30:22,270
a new feature with this if you use if

00:30:19,420 --> 00:30:23,980
you use you create future you loop can

00:30:22,270 --> 00:30:25,450
actually inject fast future

00:30:23,980 --> 00:30:27,160
implementation into your code because

00:30:25,450 --> 00:30:31,080
you will loop implements its own version

00:30:27,160 --> 00:30:35,980
of the future and it's about 30% faster

00:30:31,080 --> 00:30:38,170
then then I think i/o future always use

00:30:35,980 --> 00:30:40,480
binary protocols never never even try to

00:30:38,170 --> 00:30:42,160
parse the export of course it doesn't

00:30:40,480 --> 00:30:46,690
make any sense if you if you can do

00:30:42,160 --> 00:30:49,900
binary go binary always profile your

00:30:46,690 --> 00:30:52,720
code is actually funny because when I

00:30:49,900 --> 00:30:55,690
when 18kg actually started to work I

00:30:52,720 --> 00:30:58,060
benchmark that against a OPG it was 2

00:30:55,690 --> 00:31:01,150
times slower and I didn't understand why

00:30:58,060 --> 00:31:03,250
because it should be faster like there

00:31:01,150 --> 00:31:04,630
is no way it can be slower so I spent

00:31:03,250 --> 00:31:06,490
about 30 hours so that will sleep

00:31:04,630 --> 00:31:11,050
optimizing asynchrony and made it two

00:31:06,490 --> 00:31:13,390
times faster four times faster so the

00:31:11,050 --> 00:31:16,870
important lesson from this is that if

00:31:13,390 --> 00:31:19,510
that first run showed that async a jury

00:31:16,870 --> 00:31:21,850
was like 30% faster than a Epogen maybe

00:31:19,510 --> 00:31:25,590
I wouldn't spend so much time trying to

00:31:21,850 --> 00:31:29,740
optimize it so always profile always

00:31:25,590 --> 00:31:31,990
analyze and I'm try to push it forward

00:31:29,740 --> 00:31:33,670
and by the way second code can be

00:31:31,990 --> 00:31:35,350
profile but will grind and you can

00:31:33,670 --> 00:31:38,250
visualize results and cake is grind it's

00:31:35,350 --> 00:31:41,640
a very useful tool check it out and

00:31:38,250 --> 00:31:44,740
satin has a useful flag it's called - a

00:31:41,640 --> 00:31:48,070
it generates HTML representation of your

00:31:44,740 --> 00:31:50,500
source file and each line is highlighted

00:31:48,070 --> 00:31:52,540
it's either blank or it's a shade of

00:31:50,500 --> 00:31:55,900
yellow and the most yellow lines use

00:31:52,540 --> 00:31:58,720
more Python C API Python C API and it is

00:31:55,900 --> 00:32:01,090
slow so basically you have a quick way

00:31:58,720 --> 00:32:03,760
of analyzing your second code

00:32:01,090 --> 00:32:06,190
it's its speed so definitely check out

00:32:03,760 --> 00:32:08,830
that also always try to the zero copy

00:32:06,190 --> 00:32:11,980
try to avoid working with bytes memory

00:32:08,830 --> 00:32:14,380
views all this kind of stuff go level

00:32:11,980 --> 00:32:17,520
with seitan and and don't copy Python

00:32:14,380 --> 00:32:17,520
objects never

00:32:18,490 --> 00:32:25,360
and one of the last advices actually is

00:32:21,970 --> 00:32:28,570
to implement an efficient buffer to

00:32:25,360 --> 00:32:31,210
write data for instance for acing PG

00:32:28,570 --> 00:32:33,280
what we do for writing messages we have

00:32:31,210 --> 00:32:35,650
a write buffer that just pre allocates a

00:32:33,280 --> 00:32:39,820
portion of memory and then we compose

00:32:35,650 --> 00:32:43,780
messages in with high level API and we

00:32:39,820 --> 00:32:45,370
we don't touch this that memory at all

00:32:43,780 --> 00:32:48,070
and when the message is ready which is

00:32:45,370 --> 00:32:49,360
send it so we have high level API of

00:32:48,070 --> 00:32:51,549
creating the message but we don't

00:32:49,360 --> 00:32:56,080
allocate any memory while we are doing

00:32:51,549 --> 00:32:58,870
so so when you have this this control

00:32:56,080 --> 00:33:01,480
you should definitely say set TCP no

00:32:58,870 --> 00:33:04,809
delay like we probably will set it by

00:33:01,480 --> 00:33:06,520
default in async i/o in Python 3.6 right

00:33:04,809 --> 00:33:08,830
now it's not that you should do it

00:33:06,520 --> 00:33:14,290
because it will speed up transfer the

00:33:08,830 --> 00:33:16,630
right method basically we with this flag

00:33:14,290 --> 00:33:19,690
set from the socket socket does doesn't

00:33:16,630 --> 00:33:22,000
wait until it receives TCP arc message

00:33:19,690 --> 00:33:24,910
it just sends the data as soon as you as

00:33:22,000 --> 00:33:27,760
you do it but if you have if you don't

00:33:24,910 --> 00:33:30,190
have control over how how frequently you

00:33:27,760 --> 00:33:32,410
are calling a transfer but right you can

00:33:30,190 --> 00:33:34,870
basically use TCP quark what what you do

00:33:32,410 --> 00:33:36,910
you port the channel then you do

00:33:34,870 --> 00:33:39,040
multiple rights to it then you uncork it

00:33:36,910 --> 00:33:41,500
and it just sends all your all of your

00:33:39,040 --> 00:33:45,070
data in as few TCP packets as as

00:33:41,500 --> 00:33:47,230
possible and the last slide is is

00:33:45,070 --> 00:33:49,630
timeouts always implement timeouts as

00:33:47,230 --> 00:33:54,370
part of the API don't ask you users to

00:33:49,630 --> 00:33:56,580
use async async i/o wait for because

00:33:54,370 --> 00:34:00,190
wait for is slow it wraps the court in

00:33:56,580 --> 00:34:02,890
into a task and that that comes with a

00:34:00,190 --> 00:34:05,740
huge penalty your code will become 30%

00:34:02,890 --> 00:34:08,590
faster slower if you use wait for so

00:34:05,740 --> 00:34:10,629
design timeouts as part of your API at

00:34:08,590 --> 00:34:15,460
the lower level implement timeouts with

00:34:10,629 --> 00:34:20,159
a loop dot call later method and it will

00:34:15,460 --> 00:34:20,159
just work that's it thank you

00:34:28,490 --> 00:34:34,910
yes so I think we have cost time for

00:34:31,919 --> 00:34:34,910
maybe one or two questions

00:34:36,800 --> 00:34:46,560
hi thank you for the presentation I want

00:34:43,410 --> 00:34:50,910
to ask you about using a sink I you and

00:34:46,560 --> 00:34:53,820
you be your even loop not for high

00:34:50,910 --> 00:34:58,650
performance but for high concurrency do

00:34:53,820 --> 00:35:01,650
you have any any would you use it for

00:34:58,650 --> 00:35:05,970
high concurrency harnesses in a scenario

00:35:01,650 --> 00:35:09,060
with hundreds of thousands of concurrent

00:35:05,970 --> 00:35:10,380
connections but yes you will hope is

00:35:09,060 --> 00:35:17,000
even better for that because it uses

00:35:10,380 --> 00:35:19,700
less memory than ETA oh I know he'll you

00:35:17,000 --> 00:35:22,080
give a loop develop is much better for

00:35:19,700 --> 00:35:24,540
highly concurrent applications of

00:35:22,080 --> 00:35:26,310
handles like hundreds of thousands of

00:35:24,540 --> 00:35:30,540
connections simply because it uses less

00:35:26,310 --> 00:35:33,870
memory it's it's again it's faster we

00:35:30,540 --> 00:35:35,610
test that you've a loop with 100,000

00:35:33,870 --> 00:35:41,880
connections from the candles it's pretty

00:35:35,610 --> 00:35:45,710
pretty pretty ok thank you unfortunately

00:35:41,880 --> 00:35:45,710

YouTube URL: https://www.youtube.com/watch?v=pi49aiLBas8


