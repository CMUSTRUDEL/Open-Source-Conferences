Title: Christian Trebing - Get in control of your workflows with Airflow
Publication date: 2016-07-30
Playlist: EuroPython 2016
Description: 
	Christian Trebing - Get in control of your workflows with Airflow
[EuroPython 2016]
[20 July 2016]
[Bilbao, Euskadi, Spain]
(https://ep2016.europython.eu//conference/talks/get-in-control-of-your-workflows-with-airflow)

Airflow (https://github.com/airbnb/airflow) is an open source Python
package from Airbnb to control your workflows.

This talk will explain the concepts behind Airflow, demonstrating how
to define your own workflows in Python code and how to extend the
functionality with new task operators and UI blueprints by developing
your own plugins. You'll also get to hear about our experiences at
Blue Yonder,  using this tool in real-world scenarios.

-----

Whenever you work with data, sooner or later you stumble across the
definition of your workflows. At what point should you process your
customer's data? What subsequent steps are necessary? And what went
wrong with your data processing last Saturday night?

At Blue Yonder we use Airflow (https://github.com/airbnb/airflow), an
open source Python package from Airbnb to solve these problems. It can
be extended with new functionality by developing plugins in Python,
without the need to fork the repo. With Airflow, we define workflows
as directed acyclic graphs and get a shiny UI for free. Airflow comes
with some task operators which can be used out of the box to complete
certain tasks. For more specific cases, tasks can be developed by the
end user. Best of all: even the configuration is done completely in
Python!

This talk will explain the concepts behind Airflow, demonstrating how
to define your own workflows in Python code and how to extend the
functionality with new task operators and UI blueprints. You'll also
get to hear about our experiences using this tool in real-world
scenarios.
Captions: 
	00:00:00,079 --> 00:00:04,830
hi welcome in this last session the

00:00:02,730 --> 00:00:06,750
PyCharm room for today our first speaker

00:00:04,830 --> 00:00:07,919
is Christian thriving he's going to talk

00:00:06,750 --> 00:00:17,369
about getting in control of your

00:00:07,919 --> 00:00:19,500
workflows with airflow so hi welcome to

00:00:17,369 --> 00:00:22,230
my talk yeah getting control of your

00:00:19,500 --> 00:00:23,939
workflows with airflow and constant

00:00:22,230 --> 00:00:26,939
living and I'm working as a software

00:00:23,939 --> 00:00:29,010
developer at blue yonder so we have the

00:00:26,939 --> 00:00:31,679
boots also here if you're interested

00:00:29,010 --> 00:00:36,570
later on just drop by and ask any

00:00:31,679 --> 00:00:39,420
questions so imagine the following

00:00:36,570 --> 00:00:42,030
scenario which I know personally from my

00:00:39,420 --> 00:00:44,940
daily life you are at a data driven

00:00:42,030 --> 00:00:47,309
company each night you get data from

00:00:44,940 --> 00:00:49,850
your customers and this data wants to be

00:00:47,309 --> 00:00:53,219
processed that's how you make money

00:00:49,850 --> 00:00:54,809
processing happens in separate steps so

00:00:53,219 --> 00:00:56,610
for example you have to take care that

00:00:54,809 --> 00:00:58,710
the status well it's you have to book

00:00:56,610 --> 00:01:00,149
that data you apply some machine

00:00:58,710 --> 00:01:02,010
learning steps you have to take

00:01:00,149 --> 00:01:04,379
decisions based on the results of the

00:01:02,010 --> 00:01:06,390
machine learning and if errors happen

00:01:04,379 --> 00:01:08,820
then you need to get an overview of what

00:01:06,390 --> 00:01:11,430
happened why did it happen when did it

00:01:08,820 --> 00:01:14,549
happen especially since most of this

00:01:11,430 --> 00:01:16,950
stuff is running at night and you need

00:01:14,549 --> 00:01:20,009
to see it next morning what possibly

00:01:16,950 --> 00:01:22,590
went wrong and as you already might have

00:01:20,009 --> 00:01:24,530
guessed we have tight time schedule so

00:01:22,590 --> 00:01:28,470
time does matter and processing time

00:01:24,530 --> 00:01:32,189
what options do you have to work in to

00:01:28,470 --> 00:01:34,500
realize such a scenario the first thing

00:01:32,189 --> 00:01:36,960
that comes to mind of most people of us

00:01:34,500 --> 00:01:38,570
is doing it with corn and we also had

00:01:36,960 --> 00:01:41,759
many projects where we started with that

00:01:38,570 --> 00:01:44,159
it's a great way to start it works out

00:01:41,759 --> 00:01:46,530
of the box but you only have time

00:01:44,159 --> 00:01:49,110
triggers you cannot say this grunt job

00:01:46,530 --> 00:01:52,130
depends on that cron job please start

00:01:49,110 --> 00:01:55,710
afterwards you just say at some time

00:01:52,130 --> 00:01:58,290
start so for example at 22 o'clock hook

00:01:55,710 --> 00:02:01,909
your data at midnight to the predict run

00:01:58,290 --> 00:02:04,619
and at 2 o'clock to the decide run

00:02:01,909 --> 00:02:06,119
besides that the error handling also has

00:02:04,619 --> 00:02:07,649
heart you always have to search for the

00:02:06,119 --> 00:02:11,430
correct log files when something went

00:02:07,649 --> 00:02:13,950
wrong so now as I said we have a tight

00:02:11,430 --> 00:02:16,530
time schedule and you would like to get

00:02:13,950 --> 00:02:18,659
finished earlier so as we see each of

00:02:16,530 --> 00:02:21,140
these steps roughly runs around one and

00:02:18,659 --> 00:02:24,900
a half hours so why not compressing that

00:02:21,140 --> 00:02:28,140
so we could do better we could here

00:02:24,900 --> 00:02:30,239
start to predict edge at twenty a half

00:02:28,140 --> 00:02:32,670
before midnight and at one o'clock start

00:02:30,239 --> 00:02:34,500
to decide works most of the time but

00:02:32,670 --> 00:02:36,810
sometimes your database is slow

00:02:34,500 --> 00:02:39,299
sometimes you have other issues and then

00:02:36,810 --> 00:02:41,040
one run takes lehenga longer here the

00:02:39,299 --> 00:02:43,079
book data run maybe takes ten minutes

00:02:41,040 --> 00:02:44,879
longer the data is not there the predict

00:02:43,079 --> 00:02:47,010
run fades the decision around Fayed's

00:02:44,879 --> 00:02:48,930
you completely run fails which is very

00:02:47,010 --> 00:02:51,090
bad when you discover the next morning

00:02:48,930 --> 00:02:53,489
because your customer cannot get the

00:02:51,090 --> 00:02:55,590
data he wants so that's an issue with

00:02:53,489 --> 00:02:57,950
gone because of that when we use the

00:02:55,590 --> 00:03:00,840
gone mechanism we always had buffers and

00:02:57,950 --> 00:03:05,639
yeah if the schedule was not too tight

00:03:00,840 --> 00:03:08,340
that worked fine but what about the next

00:03:05,639 --> 00:03:10,590
step our customer sends more data the

00:03:08,340 --> 00:03:11,609
processing time gets longer and we need

00:03:10,590 --> 00:03:14,190
to find better solutions

00:03:11,609 --> 00:03:16,590
why not writing our own tool it's so

00:03:14,190 --> 00:03:18,419
simple we just have to check that the

00:03:16,590 --> 00:03:20,639
first round stopped and that the next

00:03:18,419 --> 00:03:22,799
round will start that cannot be that

00:03:20,639 --> 00:03:25,680
hard and the start is very easy in

00:03:22,799 --> 00:03:27,120
multiple projects we did that and it

00:03:25,680 --> 00:03:31,019
worked for the first steps but

00:03:27,120 --> 00:03:32,970
afterwards you see the limit soon you

00:03:31,019 --> 00:03:35,010
have maybe concurrency that you have

00:03:32,970 --> 00:03:38,879
multiple tasks running at once you need

00:03:35,010 --> 00:03:42,150
to know why what tasks failed you might

00:03:38,879 --> 00:03:43,829
not only wanting to do a timely triggers

00:03:42,150 --> 00:03:46,739
but also trigger manual things

00:03:43,829 --> 00:03:50,160
afterwards you might have want a new I

00:03:46,739 --> 00:03:52,680
or an external endpoint at that point

00:03:50,160 --> 00:03:54,750
you have to take a decision either you

00:03:52,680 --> 00:03:56,400
can accept the limits that's fine all

00:03:54,750 --> 00:03:58,230
your own work for implement a

00:03:56,400 --> 00:04:01,379
implementation gets much more complex

00:03:58,230 --> 00:04:06,659
than you thought initially so you are

00:04:01,379 --> 00:04:08,970
stuck we were in that situation also we

00:04:06,659 --> 00:04:10,530
wanted to harmonize all these workflow

00:04:08,970 --> 00:04:13,769
tools we had in our different projects

00:04:10,530 --> 00:04:15,780
and we had to look at several workflow

00:04:13,769 --> 00:04:18,599
at several open source implementations

00:04:15,780 --> 00:04:21,539
there are many interesting things with

00:04:18,599 --> 00:04:23,669
many different properties so for example

00:04:21,539 --> 00:04:26,280
we also had to look at what if I Luigi

00:04:23,669 --> 00:04:26,770
but this was more an HDFS phase two

00:04:26,280 --> 00:04:29,379
which was

00:04:26,770 --> 00:04:31,389
not in our technology stack and also

00:04:29,379 --> 00:04:33,310
several other tools in the end we

00:04:31,389 --> 00:04:35,620
decided for air flow which is an open

00:04:33,310 --> 00:04:39,550
source project initiated by Airbnb

00:04:35,620 --> 00:04:41,500
therefore the name air flow what did we

00:04:39,550 --> 00:04:43,569
decide for that well the tool itself

00:04:41,500 --> 00:04:46,960
it's written in Python we know that if

00:04:43,569 --> 00:04:48,639
you like it and one thing that was

00:04:46,960 --> 00:04:50,620
really cool as the do workflows are

00:04:48,639 --> 00:04:52,930
defined in Python code so they are not

00:04:50,620 --> 00:04:55,030
sitting in some JSON files not sitting

00:04:52,930 --> 00:04:58,659
in some database rows but really each

00:04:55,030 --> 00:05:00,099
workflow is a Python code you can enter

00:04:58,659 --> 00:05:03,069
it in your version control system you

00:05:00,099 --> 00:05:07,080
get all versioning and that's really a

00:05:03,069 --> 00:05:10,270
very good way of managing that it has

00:05:07,080 --> 00:05:13,479
most of the features I said you will run

00:05:10,270 --> 00:05:15,580
within the limitations so you can have a

00:05:13,479 --> 00:05:18,130
look at the present and the past runs it

00:05:15,580 --> 00:05:20,409
has login features it's great that it's

00:05:18,130 --> 00:05:22,509
extensible so you can write your own

00:05:20,409 --> 00:05:24,789
extensions and Python code and plug it

00:05:22,509 --> 00:05:26,680
in without having to modify the open

00:05:24,789 --> 00:05:29,800
source code but it detects these plugins

00:05:26,680 --> 00:05:31,599
are tell more on that later it is an

00:05:29,800 --> 00:05:33,639
under active development so at the

00:05:31,599 --> 00:05:36,069
moment it's an Apache Incubator project

00:05:33,639 --> 00:05:37,750
and people are erecting on the pull

00:05:36,069 --> 00:05:38,979
request so there's lots of traffic in

00:05:37,750 --> 00:05:41,620
there and you can see that it gets

00:05:38,979 --> 00:05:44,080
further it has a nice UI which I will

00:05:41,620 --> 00:05:45,430
show you you can define your own rest

00:05:44,080 --> 00:05:47,500
interface and it's relatively

00:05:45,430 --> 00:05:49,630
lightweight you have two processes on a

00:05:47,500 --> 00:05:53,020
server and you need a database to store

00:05:49,630 --> 00:05:55,180
all that information how does the

00:05:53,020 --> 00:05:59,529
workflow look like this is the Python

00:05:55,180 --> 00:06:02,370
code I talked about mainly it is dynamic

00:05:59,529 --> 00:06:06,580
as you click no not a directed acyclic

00:06:02,370 --> 00:06:08,440
graph each watch each workflow and you

00:06:06,580 --> 00:06:10,000
instantiate it you give some parameters

00:06:08,440 --> 00:06:11,919
like bonus the first run

00:06:10,000 --> 00:06:14,800
what scheduling do you have you can give

00:06:11,919 --> 00:06:17,919
that in constant acts or and other in

00:06:14,800 --> 00:06:20,710
time Delta awesome and you can define

00:06:17,919 --> 00:06:23,050
your workflow steps as operators so here

00:06:20,710 --> 00:06:25,000
I tell more about operators later but

00:06:23,050 --> 00:06:27,430
we're three snaps we are doing we are

00:06:25,000 --> 00:06:30,729
booking the data we are predicting and

00:06:27,430 --> 00:06:33,190
we are we take the decision and the

00:06:30,729 --> 00:06:35,740
connection between the steps you do y at

00:06:33,190 --> 00:06:37,330
the set upstream so you say before the

00:06:35,740 --> 00:06:39,930
predict happens the pipette I needs to

00:06:37,330 --> 00:06:42,570
happen and before the decide happens the

00:06:39,930 --> 00:06:45,720
Hritik needs to happen so this should be

00:06:42,570 --> 00:06:49,320
the craft doesn't work yeah okay let's

00:06:45,720 --> 00:06:52,320
go to the next complex stuff so maybe

00:06:49,320 --> 00:06:55,050
you want to have a fan in fan out we

00:06:52,320 --> 00:06:57,509
have more data and the brick predict run

00:06:55,050 --> 00:07:00,270
takes longer we want to paralyze that

00:06:57,509 --> 00:07:02,550
and maybe we say we do some prediction

00:07:00,270 --> 00:07:05,940
for German customers and some prediction

00:07:02,550 --> 00:07:08,970
for the UK locations so by that I can

00:07:05,940 --> 00:07:10,860
say predict Germany predict UK both

00:07:08,970 --> 00:07:13,020
depend on the booking of the day time

00:07:10,860 --> 00:07:16,169
and the decision depends on both of them

00:07:13,020 --> 00:07:18,570
so it's very nicely to describe and it

00:07:16,169 --> 00:07:21,770
will give you that craft directly but

00:07:18,570 --> 00:07:23,639
that you can build arbitrary complex

00:07:21,770 --> 00:07:26,310
workflows you also have the possibility

00:07:23,639 --> 00:07:28,680
for decisions and for switches but at

00:07:26,310 --> 00:07:31,199
least for us we did not need them up to

00:07:28,680 --> 00:07:33,270
now so most of our workflows are quite

00:07:31,199 --> 00:07:38,190
linear just with a few processes in

00:07:33,270 --> 00:07:41,759
there so how does the nice UI look like

00:07:38,190 --> 00:07:44,820
I already promised you you have here an

00:07:41,759 --> 00:07:46,860
overview where you see what workflows do

00:07:44,820 --> 00:07:49,470
you have what is the schedule of them

00:07:46,860 --> 00:07:52,590
and also what are the statuses most

00:07:49,470 --> 00:07:54,780
recently so it's a little bit small to

00:07:52,590 --> 00:07:56,699
read but you have here saying which

00:07:54,780 --> 00:07:58,530
tasks how many tasks have run correctly

00:07:56,699 --> 00:08:01,169
how many tasks are running currently

00:07:58,530 --> 00:08:04,880
also you can see what are erroneous and

00:08:01,169 --> 00:08:08,940
water currently up for retry

00:08:04,880 --> 00:08:11,250
you can run each view each you can have

00:08:08,940 --> 00:08:14,159
a look at each deck run explicitly so

00:08:11,250 --> 00:08:16,650
you see here the sequence this is

00:08:14,159 --> 00:08:19,169
color-coded all so that you can see when

00:08:16,650 --> 00:08:20,970
which witch that was successful which is

00:08:19,169 --> 00:08:23,340
currently running erroneous and so forth

00:08:20,970 --> 00:08:26,370
so this is a run that did not start so

00:08:23,340 --> 00:08:30,539
this is just a scheduled but starting

00:08:26,370 --> 00:08:34,349
was not an actual now the three view

00:08:30,539 --> 00:08:38,700
shows you an overview of all the runs so

00:08:34,349 --> 00:08:41,190
here you see each each column is a run

00:08:38,700 --> 00:08:43,500
day so you see for each day here these

00:08:41,190 --> 00:08:46,500
three days went correctly all green and

00:08:43,500 --> 00:08:48,480
the last run currently had an issue here

00:08:46,500 --> 00:08:50,010
with in the second step it's yellow this

00:08:48,480 --> 00:08:52,200
means it's up for retry

00:08:50,010 --> 00:08:53,670
so you get a nice overview on how did it

00:08:52,200 --> 00:08:56,340
behave in the past and

00:08:53,670 --> 00:08:59,400
currently also which helps and that is a

00:08:56,340 --> 00:09:01,460
runtime view which for which you can see

00:08:59,400 --> 00:09:05,570
for example performance degradation

00:09:01,460 --> 00:09:09,330
where we see here we have three runs and

00:09:05,570 --> 00:09:11,190
these colors are all different tasks so

00:09:09,330 --> 00:09:13,860
let's say this is the booking data step

00:09:11,190 --> 00:09:15,540
the blue one this is the prediction step

00:09:13,860 --> 00:09:18,540
and this is the decision step and you

00:09:15,540 --> 00:09:21,390
see one behave the same and the other

00:09:18,540 --> 00:09:23,430
two changed over time so very useful for

00:09:21,390 --> 00:09:27,120
seeing which of the steps much of techne

00:09:23,430 --> 00:09:31,200
longer you can see each run also as a

00:09:27,120 --> 00:09:36,050
Gantt chart to see one was each step

00:09:31,200 --> 00:09:38,940
happening and you have a lock view which

00:09:36,050 --> 00:09:40,950
which really is useful where you can

00:09:38,940 --> 00:09:43,260
output things like unfortunately it's a

00:09:40,950 --> 00:09:45,570
little bit smaller here it says the

00:09:43,260 --> 00:09:47,670
decision task has started a job in the

00:09:45,570 --> 00:09:51,660
backend system and the job ID is 17 and

00:09:47,670 --> 00:09:53,460
the setter and the next the next

00:09:51,660 --> 00:09:56,370
iteration it asked what is the status

00:09:53,460 --> 00:09:57,780
now and then we see it is finished but

00:09:56,370 --> 00:10:02,100
that you can see how each task was

00:09:57,780 --> 00:10:04,560
processed now what are the building

00:10:02,100 --> 00:10:06,900
blocks of your workflows these are

00:10:04,560 --> 00:10:09,150
operators and there are already many

00:10:06,900 --> 00:10:11,850
operators delivered in air flow as an

00:10:09,150 --> 00:10:14,430
example you can operate you can start

00:10:11,850 --> 00:10:17,340
things on the bash you can start things

00:10:14,430 --> 00:10:19,770
with HTTP requests you can execute

00:10:17,340 --> 00:10:21,180
statements on databases you can write

00:10:19,770 --> 00:10:24,600
directly Python code which is executed

00:10:21,180 --> 00:10:27,450
or you can send mails and this is just a

00:10:24,600 --> 00:10:29,940
few examples there are more in the in a

00:10:27,450 --> 00:10:32,720
flow delivered there not only this

00:10:29,940 --> 00:10:35,160
operators but also sensors sensors are

00:10:32,720 --> 00:10:37,920
our steps and your workflow that wait

00:10:35,160 --> 00:10:40,800
for things so on HTTP sensor could for

00:10:37,920 --> 00:10:42,330
example always query and URL and ask

00:10:40,800 --> 00:10:43,830
whether it is finished or what is the

00:10:42,330 --> 00:10:46,380
status on that and based on that it will

00:10:43,830 --> 00:10:49,050
wait I will proceed in the workflow in

00:10:46,380 --> 00:10:51,750
the same way an HDFS sends out could

00:10:49,050 --> 00:10:53,910
check for files on the file system and

00:10:51,750 --> 00:10:55,490
then SQL sensor could check for values

00:10:53,910 --> 00:10:58,260
in the database

00:10:55,490 --> 00:11:00,750
many things already you can do with

00:10:58,260 --> 00:11:03,570
these operators but there might be

00:11:00,750 --> 00:11:06,000
situations when you need more for

00:11:03,570 --> 00:11:08,670
example for us we had an awesome

00:11:06,000 --> 00:11:11,490
processing and our back-end systems so

00:11:08,670 --> 00:11:13,470
we had here our a flow system we had our

00:11:11,490 --> 00:11:15,600
back-end system for example the machine

00:11:13,470 --> 00:11:17,730
learning system for the predictions we

00:11:15,600 --> 00:11:20,550
wanted to start a job so we trigger and

00:11:17,730 --> 00:11:24,180
we trigger an HTTP request there we get

00:11:20,550 --> 00:11:27,420
back a job ID and then we let it run for

00:11:24,180 --> 00:11:29,400
five minutes half an hour or so and we

00:11:27,420 --> 00:11:31,590
constantly asked whether it is finished

00:11:29,400 --> 00:11:33,900
or not and when it is finished we can

00:11:31,590 --> 00:11:36,450
start the next trouble this would be

00:11:33,900 --> 00:11:39,150
possible to do already with standard

00:11:36,450 --> 00:11:41,400
methods of air flow so we could use the

00:11:39,150 --> 00:11:44,070
simple HTTP operator to start it and the

00:11:41,400 --> 00:11:47,370
sensor as I described to wait until it

00:11:44,070 --> 00:11:50,040
is finished this works but it has the

00:11:47,370 --> 00:11:52,950
disadvantage that you don't see directly

00:11:50,040 --> 00:11:55,140
how long did it take so you remember the

00:11:52,950 --> 00:11:57,030
last view with the runtimes I would like

00:11:55,140 --> 00:12:00,360
to see how long did my decision take and

00:11:57,030 --> 00:12:02,580
therefore I wanted this step decide as a

00:12:00,360 --> 00:12:03,750
certain length the length of this is the

00:12:02,580 --> 00:12:07,290
length that it took on the backend

00:12:03,750 --> 00:12:11,100
system so this is possible we can do

00:12:07,290 --> 00:12:13,890
this with a new operator I won't explain

00:12:11,100 --> 00:12:17,220
each line in detail also you can find

00:12:13,890 --> 00:12:19,350
afterwards this as a complete air flow

00:12:17,220 --> 00:12:21,270
example plug-in on the github people

00:12:19,350 --> 00:12:24,360
which you can see afterwards I can check

00:12:21,270 --> 00:12:27,120
for each line so we have an HTTP

00:12:24,360 --> 00:12:29,100
connection defined we have some some

00:12:27,120 --> 00:12:31,290
endpoints beside that we can trigger

00:12:29,100 --> 00:12:33,030
that and it delivers us a job ID and we

00:12:31,290 --> 00:12:34,589
have a job status we can ask when we

00:12:33,030 --> 00:12:38,940
have to drop idea what is the status of

00:12:34,589 --> 00:12:41,940
that so within the execution we run the

00:12:38,940 --> 00:12:44,490
post on the decide to get back the job

00:12:41,940 --> 00:12:48,390
ID then we wait for the job with the job

00:12:44,490 --> 00:12:51,420
ID and once the status is finished we

00:12:48,390 --> 00:12:53,339
are done and then wouldn't the air flow

00:12:51,420 --> 00:12:59,400
database we know how long did this

00:12:53,339 --> 00:13:02,220
decision step take now how do you get

00:12:59,400 --> 00:13:04,110
these operators into your system as I

00:13:02,220 --> 00:13:07,200
said we don't want to modify the coach

00:13:04,110 --> 00:13:10,770
the air flow code directly but we can do

00:13:07,200 --> 00:13:12,810
this in a Python package we can we can

00:13:10,770 --> 00:13:14,580
say we have this plugin that has some

00:13:12,810 --> 00:13:17,820
own operators that are some flask

00:13:14,580 --> 00:13:18,620
blueprints and lay that in our file

00:13:17,820 --> 00:13:20,540
system and

00:13:18,620 --> 00:13:22,490
the airflow configuration we just can

00:13:20,540 --> 00:13:24,470
say you're blockin is here and your

00:13:22,490 --> 00:13:26,090
workflow definitions are there and on

00:13:24,470 --> 00:13:31,430
the start of airflow it will detect them

00:13:26,090 --> 00:13:34,010
automatically also that plugin is

00:13:31,430 --> 00:13:36,260
defined in python as you can see here we

00:13:34,010 --> 00:13:39,170
have two FL packing manager and it just

00:13:36,260 --> 00:13:41,270
say inherit from the airflow block and

00:13:39,170 --> 00:13:43,550
we have our Europe - plugin this has

00:13:41,270 --> 00:13:47,180
three operators I need and also a

00:13:43,550 --> 00:13:50,710
blueprint what does it about that

00:13:47,180 --> 00:13:53,210
blueprint why do you need that we had

00:13:50,710 --> 00:13:56,180
the requirement we wanted to have an

00:13:53,210 --> 00:13:57,980
endpoint to talk and a rest style with

00:13:56,180 --> 00:14:00,620
our airflow system so that we can also

00:13:57,980 --> 00:14:03,590
program attic Elysees I want manually to

00:14:00,620 --> 00:14:06,320
start in a trigger I want to know is a

00:14:03,590 --> 00:14:09,200
diagram finished or not this

00:14:06,320 --> 00:14:11,330
functionality was not there in an

00:14:09,200 --> 00:14:13,450
airflow but you can write it as a class

00:14:11,330 --> 00:14:16,100
blueprint you can define that endpoints

00:14:13,450 --> 00:14:18,950
and it is detected automatically and

00:14:16,100 --> 00:14:22,970
edit within the web server also this you

00:14:18,950 --> 00:14:25,430
will see in the example repo how would

00:14:22,970 --> 00:14:27,260
such a rest endpoint look like we have

00:14:25,430 --> 00:14:29,930
here the airflow server running on port

00:14:27,260 --> 00:14:32,750
8080 we have defined this endpoint

00:14:29,930 --> 00:14:34,490
trigger and we say we give the name of

00:14:32,750 --> 00:14:36,560
the workflow which is daily processing

00:14:34,490 --> 00:14:39,380
and we get back the name of the workflow

00:14:36,560 --> 00:14:42,140
and the run ID which we can use

00:14:39,380 --> 00:14:46,190
afterwards to ask for the status so this

00:14:42,140 --> 00:14:49,430
what's fine now what happens inside of

00:14:46,190 --> 00:14:52,280
airflow it works with two processes at

00:14:49,430 --> 00:14:54,290
at least two processes I should say we

00:14:52,280 --> 00:14:57,200
have a scheduling process that takes

00:14:54,290 --> 00:14:59,180
care when each trouble should run and we

00:14:57,200 --> 00:15:01,520
have a web server that gives the UI and

00:14:59,180 --> 00:15:04,490
all the other blueprints also you need

00:15:01,520 --> 00:15:07,040
database several databases are supported

00:15:04,490 --> 00:15:10,490
we are using at our company the

00:15:07,040 --> 00:15:12,170
phosphorus and SQLite SQLite currently

00:15:10,490 --> 00:15:14,540
has a restriction that you cannot run

00:15:12,170 --> 00:15:16,400
parallel tasks on them but we are using

00:15:14,540 --> 00:15:18,620
the SQLite more for the development

00:15:16,400 --> 00:15:20,720
testing stuff so this is fine and for

00:15:18,620 --> 00:15:23,960
production you can use the Postgres and

00:15:20,720 --> 00:15:26,750
there you don't have that limitation you

00:15:23,960 --> 00:15:29,870
can also look how do you want your tasks

00:15:26,750 --> 00:15:32,240
to be executed we are using most of the

00:15:29,870 --> 00:15:34,220
time just HTTP requests we are saying we

00:15:32,240 --> 00:15:35,750
trigger tasks in the backend system we

00:15:34,220 --> 00:15:37,310
are waiting until it is finished so the

00:15:35,750 --> 00:15:41,090
air flow system itself there's no high

00:15:37,310 --> 00:15:43,010
workload on that so we are happy that

00:15:41,090 --> 00:15:44,930
this runs within one schedule within the

00:15:43,010 --> 00:15:46,370
scheduler process directly or one we

00:15:44,930 --> 00:15:49,310
want to have multiple tasks in parallel

00:15:46,370 --> 00:15:51,290
we work with sub processes but it's also

00:15:49,310 --> 00:15:53,450
possible if you trigger the stuff wire

00:15:51,290 --> 00:15:55,340
bash scripts or similar things that you

00:15:53,450 --> 00:15:57,890
want to more power behind the executor

00:15:55,340 --> 00:15:59,870
notes itself and to do that you can use

00:15:57,890 --> 00:16:01,910
celery which is a framework with

00:15:59,870 --> 00:16:03,740
multiple worker nodes and you can use

00:16:01,910 --> 00:16:08,870
that there's already a connection from

00:16:03,740 --> 00:16:10,190
air flow to celery how we use it - most

00:16:08,870 --> 00:16:11,750
of the things that already mentioned in

00:16:10,190 --> 00:16:14,330
the meantime we use the automatic

00:16:11,750 --> 00:16:16,610
schedules and we have manual triggers we

00:16:14,330 --> 00:16:18,770
use one air flow instance per system we

00:16:16,610 --> 00:16:20,420
manage so we also had how do we that

00:16:18,770 --> 00:16:22,520
connection do we have one central

00:16:20,420 --> 00:16:24,260
company air for instance or one air flow

00:16:22,520 --> 00:16:27,380
instance per system and for us it was

00:16:24,260 --> 00:16:28,520
easier to do it that way databases reuse

00:16:27,380 --> 00:16:31,820
puskÃ¡s in SQLite

00:16:28,520 --> 00:16:34,400
execute us a lightweight and also we are

00:16:31,820 --> 00:16:37,190
contributing to air flow this is really

00:16:34,400 --> 00:16:38,540
good but that works fine this external

00:16:37,190 --> 00:16:41,030
triggers that you can trigger them

00:16:38,540 --> 00:16:43,910
manually they were not there one year

00:16:41,030 --> 00:16:45,620
ago and we needed them definitely before

00:16:43,910 --> 00:16:48,830
using air flow so we wrote a full

00:16:45,620 --> 00:16:50,540
request that was also worked with and

00:16:48,830 --> 00:16:53,360
now we thank these two pull requests

00:16:50,540 --> 00:16:56,450
this is an air flow and we are also have

00:16:53,360 --> 00:16:58,070
some necessary functionality for the

00:16:56,450 --> 00:16:59,300
blackened detections that we also open

00:16:58,070 --> 00:17:00,800
pull requests there and there's an

00:16:59,300 --> 00:17:06,020
active communication with their

00:17:00,800 --> 00:17:08,900
community with all these good things

00:17:06,020 --> 00:17:10,880
about air flow there are at least a few

00:17:08,900 --> 00:17:13,490
challenges I want to make you aware of

00:17:10,880 --> 00:17:14,930
because these were things we struggled a

00:17:13,490 --> 00:17:17,540
little bit and also with the project

00:17:14,930 --> 00:17:20,330
teams are using air flow at our site

00:17:17,540 --> 00:17:22,670
this is has to do with how is scheduling

00:17:20,330 --> 00:17:26,500
handled and how is the start time

00:17:22,670 --> 00:17:28,790
interpret interpreted so scheduling

00:17:26,500 --> 00:17:31,640
there are two dates that are important

00:17:28,790 --> 00:17:34,370
for that it's to start date this means

00:17:31,640 --> 00:17:36,020
when did the processing of this task of

00:17:34,370 --> 00:17:38,750
this workflow start on the server so

00:17:36,020 --> 00:17:41,150
that's quite easy it's the time of the

00:17:38,750 --> 00:17:41,850
server but there's also an execution

00:17:41,150 --> 00:17:43,919
date that

00:17:41,850 --> 00:17:46,380
prominently shown on the eye and that

00:17:43,919 --> 00:17:48,360
sometimes shows strange values these

00:17:46,380 --> 00:17:50,010
values are consistent and they're

00:17:48,360 --> 00:17:53,700
explainable but they are not always

00:17:50,010 --> 00:17:55,890
obvious the reason as the history from

00:17:53,700 --> 00:17:58,470
airflow so this was used in each

00:17:55,890 --> 00:18:03,419
scenario so this extract transform load

00:17:58,470 --> 00:18:06,990
and this means that for each they wanted

00:18:03,419 --> 00:18:09,210
to process daily data which was accurate

00:18:06,990 --> 00:18:12,870
which was coming in the whole day long

00:18:09,210 --> 00:18:14,940
so let's say on 19th of July the whole

00:18:12,870 --> 00:18:17,039
day data came in and then you wanted to

00:18:14,940 --> 00:18:19,110
process that data for the 19th of July

00:18:17,039 --> 00:18:22,140
and when can you process the data you

00:18:19,110 --> 00:18:25,020
can process it only after the 19th which

00:18:22,140 --> 00:18:27,150
is the 20th so let's say today so today

00:18:25,020 --> 00:18:29,610
this task of data processing runs and

00:18:27,150 --> 00:18:33,179
what is the execution date it's the 19th

00:18:29,610 --> 00:18:35,340
so it's always one iteration back this

00:18:33,179 --> 00:18:36,960
is because they said originally well

00:18:35,340 --> 00:18:38,789
this is more a description this is the

00:18:36,960 --> 00:18:42,059
data from the 19th therefore it's the

00:18:38,789 --> 00:18:43,679
execution date that's fine when you know

00:18:42,059 --> 00:18:45,210
that it does not scare you that you

00:18:43,679 --> 00:18:47,610
think the system is doing wild things

00:18:45,210 --> 00:18:49,200
but that's consistent but yeah you have

00:18:47,610 --> 00:18:51,289
to get used to that we have some

00:18:49,200 --> 00:18:54,240
workflow starting in a weekly schedule

00:18:51,289 --> 00:18:56,730
which means when I trigger that now it

00:18:54,240 --> 00:18:59,280
gives me the start date of Monday the

00:18:56,730 --> 00:19:04,559
week before also that it's consistent

00:18:59,280 --> 00:19:06,960
but you need to get used to that then we

00:19:04,559 --> 00:19:09,659
have to start date you might remember

00:19:06,960 --> 00:19:12,929
for 10 minutes ago that we get a start

00:19:09,659 --> 00:19:15,720
date for each workflow and if the

00:19:12,929 --> 00:19:19,799
workflow is scheduled automatically and

00:19:15,720 --> 00:19:23,419
you start the server it will know that

00:19:19,799 --> 00:19:27,720
it has to fill up tasks so when we say

00:19:23,419 --> 00:19:29,610
start edge is today 20th of July now we

00:19:27,720 --> 00:19:31,830
start the server at 23 of July the

00:19:29,610 --> 00:19:34,020
scheduler at the started we have given

00:19:31,830 --> 00:19:36,240
the 17th of July then it will detect

00:19:34,020 --> 00:19:38,280
that there are some runs missing and it

00:19:36,240 --> 00:19:41,250
will fill these runs automatically so it

00:19:38,280 --> 00:19:43,470
will first trigger the run for this with

00:19:41,250 --> 00:19:45,570
execution date 17th then execution date

00:19:43,470 --> 00:19:47,820
18th and there's a regular run then the

00:19:45,570 --> 00:19:50,640
19th will be processed at the current at

00:19:47,820 --> 00:19:52,210
the correct point if of time you need to

00:19:50,640 --> 00:19:54,550
check whether this is applica

00:19:52,210 --> 00:19:56,290
for you so when you have these things I

00:19:54,550 --> 00:19:58,600
would need really to process this data

00:19:56,290 --> 00:20:00,190
that's fine when you have more other

00:19:58,600 --> 00:20:02,140
thing I want to trigger something in the

00:20:00,190 --> 00:20:03,910
backend and I need to trigger it just

00:20:02,140 --> 00:20:05,590
once because this back-end shop will

00:20:03,910 --> 00:20:08,260
take care of all cleaning up that stuff

00:20:05,590 --> 00:20:10,240
then this is a little bit strange and

00:20:08,260 --> 00:20:12,670
can lead to issues when you trigger it

00:20:10,240 --> 00:20:14,590
too much but you can work around with

00:20:12,670 --> 00:20:17,470
that when you give the correct start

00:20:14,590 --> 00:20:19,300
time already so you can determine that

00:20:17,470 --> 00:20:21,010
in code you can determine that and

00:20:19,300 --> 00:20:22,960
variable there are several options I

00:20:21,010 --> 00:20:24,970
won't discuss them in detail but this is

00:20:22,960 --> 00:20:26,860
the thing you should have you should

00:20:24,970 --> 00:20:29,170
have in mind when you do that when you

00:20:26,860 --> 00:20:31,120
wonder why does the special happen it's

00:20:29,170 --> 00:20:33,250
possible to handle that but you need to

00:20:31,120 --> 00:20:34,900
know the concept behind that if you have

00:20:33,250 --> 00:20:39,550
some further questions maybe we can

00:20:34,900 --> 00:20:42,100
discuss afterwards okay and that's it

00:20:39,550 --> 00:20:44,050
also from my presentation I'll give you

00:20:42,100 --> 00:20:46,630
here that's the incubator project for

00:20:44,050 --> 00:20:49,690
airflow it has a nice documentation

00:20:46,630 --> 00:20:51,940
which is here also very useful is the

00:20:49,690 --> 00:20:53,800
common pitfalls page and the air flow

00:20:51,940 --> 00:20:55,390
Vicki they're also the stuff with the

00:20:53,800 --> 00:20:58,900
execution date is explained in more

00:20:55,390 --> 00:21:01,000
detail and the plugin which I have shown

00:20:58,900 --> 00:21:03,520
you parts of you can find here at our

00:21:01,000 --> 00:21:06,460
blue yonder repo you can download that

00:21:03,520 --> 00:21:09,070
you have the steps in the readme on how

00:21:06,460 --> 00:21:13,860
to use that in your air for instance so

00:21:09,070 --> 00:21:13,860
that's it from my side any questions

00:21:31,140 --> 00:21:39,270
yes hello thanks for the presentation

00:21:33,890 --> 00:21:41,940
you showed us the GUI is it possible to

00:21:39,270 --> 00:21:44,640
to manage test dependencies in this GUI

00:21:41,940 --> 00:21:49,500
or is it just to display things that you

00:21:44,640 --> 00:21:52,200
that you wrote in the code definitions

00:21:49,500 --> 00:21:54,450
itself you do that in code so you can

00:21:52,200 --> 00:21:57,720
view that code from the GUI but you have

00:21:54,450 --> 00:22:00,630
to change it in your code editor okay

00:21:57,720 --> 00:22:05,010
because well you know in a way from

00:22:00,630 --> 00:22:10,590
we've got homemade the schedule and well

00:22:05,010 --> 00:22:15,440
with I think 100,000 tasks inside and is

00:22:10,590 --> 00:22:20,790
it scalable air flow do you use this

00:22:15,440 --> 00:22:23,250
amount of tests in in your system we

00:22:20,790 --> 00:22:26,070
know we don't have that high data volume

00:22:23,250 --> 00:22:29,130
in our system so for us it's more that

00:22:26,070 --> 00:22:31,980
we have persistence these these nightly

00:22:29,130 --> 00:22:34,140
runs that have several tasks but not a

00:22:31,980 --> 00:22:36,120
thousand some millions of that I've seen

00:22:34,140 --> 00:22:39,030
in the documentation page from Airbnb

00:22:36,120 --> 00:22:41,490
these stacks seem to be much bigger so

00:22:39,030 --> 00:22:43,140
it would be worth to ask them what is

00:22:41,490 --> 00:22:45,710
the limit for that but we did not reach

00:22:43,140 --> 00:22:45,710
it up to now

00:22:51,470 --> 00:22:57,450
hi I would like to ask about the

00:22:54,540 --> 00:22:59,940
execution date and run date okay is it

00:22:57,450 --> 00:23:04,200
possible to configure it because we have

00:22:59,940 --> 00:23:05,640
like similar example when you collect

00:23:04,200 --> 00:23:08,670
data for last month and you want to

00:23:05,640 --> 00:23:10,200
running for example 15 days later or in

00:23:08,670 --> 00:23:13,020
the opposite direction you want to like

00:23:10,200 --> 00:23:16,230
collect data for next month and run it

00:23:13,020 --> 00:23:18,300
50 days before that like is it consumed

00:23:16,230 --> 00:23:21,000
if you can configure it like the delay

00:23:18,300 --> 00:23:24,420
or maybe even if you can postpone it if

00:23:21,000 --> 00:23:27,390
you see ok I will run tomorrow but if I

00:23:24,420 --> 00:23:30,110
have no data tomorrow I will retry the

00:23:27,390 --> 00:23:30,110
day after tomorrow

00:23:30,230 --> 00:23:35,550
well at first the logic is not

00:23:32,550 --> 00:23:38,970
configurable so this is in the

00:23:35,550 --> 00:23:41,790
scheduling code itself regarding the

00:23:38,970 --> 00:23:45,450
stuff running two weeks after our two

00:23:41,790 --> 00:23:48,210
weeks from now I would say I have no

00:23:45,450 --> 00:23:50,850
quick answer to that maybe we could

00:23:48,210 --> 00:23:53,190
discuss it afterwards I think you can do

00:23:50,850 --> 00:23:54,630
many things with the scheduling so

00:23:53,190 --> 00:23:57,120
because the scheduling just helps you

00:23:54,630 --> 00:23:59,070
when does it run you also have to posit

00:23:57,120 --> 00:24:01,770
the possibility to schedule a run each

00:23:59,070 --> 00:24:03,960
day and as a first task of the run

00:24:01,770 --> 00:24:07,170
decide on whether you really want to run

00:24:03,960 --> 00:24:09,330
or not so this might be the first

00:24:07,170 --> 00:24:11,250
iteration when you say you implement the

00:24:09,330 --> 00:24:13,350
more complex logic than in your first

00:24:11,250 --> 00:24:16,620
task but maybe there's also other things

00:24:13,350 --> 00:24:17,750
ok thank you let's have one more

00:24:16,620 --> 00:24:21,030
question

00:24:17,750 --> 00:24:23,430
did you evaluate other tools when you

00:24:21,030 --> 00:24:26,730
decided about airflow and why did you

00:24:23,430 --> 00:24:30,030
decide for airflow if for example we had

00:24:26,730 --> 00:24:33,540
to look at Luigi but that was based on

00:24:30,030 --> 00:24:36,240
an HDFS stack which we did not run so

00:24:33,540 --> 00:24:38,340
therefore this was too heavy weight just

00:24:36,240 --> 00:24:40,410
to have an workflow system to set up

00:24:38,340 --> 00:24:43,860
this we also had to look at several

00:24:40,410 --> 00:24:46,800
OpenStack implementations but their main

00:24:43,860 --> 00:24:48,660
focus was on doing heavy work lifting

00:24:46,800 --> 00:24:50,340
with execution processes and how these

00:24:48,660 --> 00:24:53,010
are distributed and since we had very

00:24:50,340 --> 00:24:57,610
lightweight processes but needed more UI

00:24:53,010 --> 00:24:59,650
features and more more these

00:24:57,610 --> 00:25:02,170
more possibilities to define own

00:24:59,650 --> 00:25:04,960
operators this also just was not the

00:25:02,170 --> 00:25:07,240
main focus when you see this - nothing's

00:25:04,960 --> 00:25:10,200
great but the main focus is a different

00:25:07,240 --> 00:25:12,970
thing then yeah it's good to have

00:25:10,200 --> 00:25:14,560
because in theory also Jenkins does a

00:25:12,970 --> 00:25:17,590
lot of things are there with some

00:25:14,560 --> 00:25:20,530
plugins so if you already have Jenkins

00:25:17,590 --> 00:25:23,260
you have to convince your team to use

00:25:20,530 --> 00:25:25,780
something else so what could be one

00:25:23,260 --> 00:25:26,560
thing that you can do that otherwise you

00:25:25,780 --> 00:25:28,990
cannot do

00:25:26,560 --> 00:25:31,960
I mean Jenkins is great we also use

00:25:28,990 --> 00:25:34,390
Jenkins for our integration testing for

00:25:31,960 --> 00:25:42,970
scheduling our unit tests but not for

00:25:34,390 --> 00:25:46,500
not for the daily productive runs let's

00:25:42,970 --> 00:25:46,500

YouTube URL: https://www.youtube.com/watch?v=i3AL3OQGJ6A


