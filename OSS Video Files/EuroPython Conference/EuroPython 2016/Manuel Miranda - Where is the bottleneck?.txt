Title: Manuel Miranda - Where is the bottleneck?
Publication date: 2016-07-28
Playlist: EuroPython 2016
Description: 
	Manuel Miranda - Where is the bottleneck?
[EuroPython 2016]
[18 July 2016]
[Bilbao, Euskadi, Spain]
(https://ep2016.europython.eu//conference/talks/where-is-the-bottleneck)

We all know Python strength does not rely on its performance and speed
when running programs. This plus the flexibility of it, can lead to
build real slow and bad quality software.

In this talk you will discover a set of useful tools for diagnosing
where the bottleneck is in your programs along with trips for quickly
realizing which is the most needed resource.

-----

Have you ever felt like your software is eating your resources and you
have no clue why? Have you reviewed all the lines, debugged and
printed everything but you still don't know what's wrong?

In this talk I will conduct a fast intro of a basic set of tools you
can use to diagnose your software's performance and then we will go
through a simple piece of code to show how those tools work and what
you can expect from them

This set of tools will include basic ones given by the OS itself like
`htop`, `lsof`, `ps` and more advanced ones that let you plot the
memory usage for given functions like `memory_profiler`, check CPU
usage and the call graph between functions like `cprofile` and
`kcachegrind` and others.

By the end of the talk, you should have an idea of which are the most
typical causes that can make your program slow and you will have a
list of tools to search for and identify the source of the problems.
Captions: 
	00:00:00,000 --> 00:00:04,700
Manuel Miranda over-engineering

00:00:01,770 --> 00:00:07,170
skyscanner I've been working there since

00:00:04,700 --> 00:00:09,050
november from the last year and

00:00:07,170 --> 00:00:12,480
previously i was working in research

00:00:09,050 --> 00:00:14,460
which was about the algorithm ization

00:00:12,480 --> 00:00:17,789
and simulations and stuff so that's

00:00:14,460 --> 00:00:20,760
basically from all my experience comes

00:00:17,789 --> 00:00:22,500
from I hope you find the talk

00:00:20,760 --> 00:00:25,140
interesting so then I don't have to see

00:00:22,500 --> 00:00:28,849
people with their phones like throwing

00:00:25,140 --> 00:00:33,059
pokeballs to me so let's start with it

00:00:28,849 --> 00:00:34,950
so first of all like what I put the

00:00:33,059 --> 00:00:38,610
basic level in the top because this

00:00:34,950 --> 00:00:41,370
first tag and I wanted to try and I'll

00:00:38,610 --> 00:00:43,020
go first with the strategy what I

00:00:41,370 --> 00:00:46,800
usually check before trying to optimize

00:00:43,020 --> 00:00:49,039
a new program or not key points I've

00:00:46,800 --> 00:00:52,410
learned through all this experience and

00:00:49,039 --> 00:00:54,750
then i'll just show some tools i like to

00:00:52,410 --> 00:00:57,510
use and for me they are really useful i

00:00:54,750 --> 00:01:00,989
will go from 0 bracelet operating system

00:00:57,510 --> 00:01:03,989
tools which i'll skip like very fast

00:01:00,989 --> 00:01:06,840
then i'll show some resource resources

00:01:03,989 --> 00:01:09,000
tools like memory cpu and stuff and then

00:01:06,840 --> 00:01:12,180
some more fancy that i call them

00:01:09,000 --> 00:01:18,119
advanced because they do more stuff than

00:01:12,180 --> 00:01:20,610
just checking one resource so don't

00:01:18,119 --> 00:01:24,060
worry this is the slide with the most

00:01:20,610 --> 00:01:28,170
text in the presentation so i'll promise

00:01:24,060 --> 00:01:31,140
it will be just this one so the main key

00:01:28,170 --> 00:01:35,939
points I like to check when i start with

00:01:31,140 --> 00:01:39,479
a new program is that those so the first

00:01:35,939 --> 00:01:42,119
one is focus so when i start checking a

00:01:39,479 --> 00:01:45,659
new program i like to think what my head

00:01:42,119 --> 00:01:47,759
is there so i like to think what exactly

00:01:45,659 --> 00:01:50,159
what I want to achieve like which is the

00:01:47,759 --> 00:01:52,680
degree of acceptance like I want to

00:01:50,159 --> 00:01:55,200
achieve by optimizing this new program

00:01:52,680 --> 00:01:57,810
because sometimes like you are so happy

00:01:55,200 --> 00:01:59,610
about optimizing your promo yeah I'm

00:01:57,810 --> 00:02:01,409
gonna raise the speed up I'm gonna make

00:01:59,610 --> 00:02:04,200
it faster than blah blah blah and you

00:02:01,409 --> 00:02:06,960
end up being so deep in sorry for the

00:02:04,200 --> 00:02:08,489
word like you don't know like you

00:02:06,960 --> 00:02:11,580
start Monday optimizing the program and

00:02:08,489 --> 00:02:14,129
then you you stand up and your teammates

00:02:11,580 --> 00:02:16,230
as you like good weekend man oh I

00:02:14,129 --> 00:02:19,739
didn't do anything and just still

00:02:16,230 --> 00:02:23,790
struggling with it so next key point is

00:02:19,739 --> 00:02:27,180
cost like is the optimization we are

00:02:23,790 --> 00:02:29,459
doing to to do worth it like the company

00:02:27,180 --> 00:02:32,640
is paying you money or you are like

00:02:29,459 --> 00:02:36,420
wasting your time and for example if

00:02:32,640 --> 00:02:38,310
it's a speed-up of just winning some

00:02:36,420 --> 00:02:40,590
minutes for its execution and the

00:02:38,310 --> 00:02:43,410
execution takes some hours and maybe

00:02:40,590 --> 00:02:47,400
doesn't worth it so if the cost and the

00:02:43,410 --> 00:02:50,940
vocals are a bit like related then the

00:02:47,400 --> 00:02:53,360
third one is code knowledge if some of

00:02:50,940 --> 00:02:56,129
you have been through legacy code

00:02:53,360 --> 00:03:00,599
sometimes you see like this bunch of

00:02:56,129 --> 00:03:02,640
code that it's like I mean why why this

00:03:00,599 --> 00:03:05,970
guy is programming in Python and doing

00:03:02,640 --> 00:03:08,610
this big 1000 line files when you have

00:03:05,970 --> 00:03:12,480
just like built-in functions like all or

00:03:08,610 --> 00:03:15,000
while more these comprehension and this

00:03:12,480 --> 00:03:18,239
kind of stuff so before starting

00:03:15,000 --> 00:03:20,400
starting optimizing the code you should

00:03:18,239 --> 00:03:23,640
if it's not your coat you should ask

00:03:20,400 --> 00:03:25,470
like why is that way I mean because in

00:03:23,640 --> 00:03:28,739
legacy code believe me you change one

00:03:25,470 --> 00:03:30,430
line one print and bad stuff is starting

00:03:28,739 --> 00:03:37,989
to happen

00:03:30,430 --> 00:03:40,870
then also context awareness so context

00:03:37,989 --> 00:03:42,609
awareness this is related more like do

00:03:40,870 --> 00:03:44,260
you control all the environment the

00:03:42,609 --> 00:03:46,989
things you are trying to optimize is

00:03:44,260 --> 00:03:51,510
just cold or you depend on random stuff

00:03:46,989 --> 00:03:56,530
going our recording network issues and

00:03:51,510 --> 00:04:00,549
can we so random stamp like network

00:03:56,530 --> 00:04:03,129
issues or my SQL queries that sometimes

00:04:00,549 --> 00:04:06,430
take random time so before trying to

00:04:03,129 --> 00:04:10,959
optimize with this noise you should like

00:04:06,430 --> 00:04:12,220
be isolated from this stuff optimizing

00:04:10,959 --> 00:04:14,620
these queries or network performance

00:04:12,220 --> 00:04:17,979
stuff it's rated with another scope you

00:04:14,620 --> 00:04:22,180
should go into that part and the last

00:04:17,979 --> 00:04:24,940
one is local context so sometimes you

00:04:22,180 --> 00:04:28,360
just start and you say i'm going to

00:04:24,940 --> 00:04:31,690
optimize that i'm doing a git clone and

00:04:28,360 --> 00:04:34,030
I'm start coding and everything and you

00:04:31,690 --> 00:04:36,159
spend like two days coding and then you

00:04:34,030 --> 00:04:39,789
move that to production you obtain it

00:04:36,159 --> 00:04:41,500
like half like half time for executing

00:04:39,789 --> 00:04:44,590
your code you move it to production and

00:04:41,500 --> 00:04:46,599
it takes you more time why you know you

00:04:44,590 --> 00:04:49,930
don't know it just maybe because little

00:04:46,599 --> 00:04:51,880
machine stuff the resources are

00:04:49,930 --> 00:04:55,900
different operating system kernel

00:04:51,880 --> 00:04:57,760
version whatever so before starting set

00:04:55,900 --> 00:05:01,659
up a nice environment try to reproduce

00:04:57,760 --> 00:05:04,090
it as much as you can and if it's not

00:05:01,659 --> 00:05:05,770
possible don't just wait two days for

00:05:04,090 --> 00:05:08,789
moving your co2 production do it

00:05:05,770 --> 00:05:11,240
iteratively so you can have feedback

00:05:08,789 --> 00:05:15,740
soon enough

00:05:11,240 --> 00:05:19,520
so that's after three years of working

00:05:15,740 --> 00:05:21,740
with this kind of stuff I'm still not

00:05:19,520 --> 00:05:23,330
applying all of them sometimes but I

00:05:21,740 --> 00:05:25,460
think they are really important points

00:05:23,330 --> 00:05:28,300
that can save you a lot of time if it's

00:05:25,460 --> 00:05:32,300
like a big job you have to do about that

00:05:28,300 --> 00:05:36,560
so here you can check my skills in

00:05:32,300 --> 00:05:40,910
design so usually I try to approach this

00:05:36,560 --> 00:05:43,430
from the outside part to the inner part

00:05:40,910 --> 00:05:46,970
so usually when I start with a new coat

00:05:43,430 --> 00:05:49,280
I just try to let's check how much how

00:05:46,970 --> 00:05:50,990
long does it take to execute once we

00:05:49,280 --> 00:05:54,169
know how long we also want to know the

00:05:50,990 --> 00:05:56,180
resources it consumes like memory CPU or

00:05:54,169 --> 00:05:59,210
this kind of stuff and then we you have

00:05:56,180 --> 00:06:01,039
these two things the good thing like the

00:05:59,210 --> 00:06:04,400
thing you have to do then is understand

00:06:01,039 --> 00:06:06,470
why why is taking that time and if it's

00:06:04,400 --> 00:06:08,330
taking this time isn't normal to consume

00:06:06,470 --> 00:06:10,099
all these resources and to do that you

00:06:08,330 --> 00:06:12,530
have to have the code knowledge which

00:06:10,099 --> 00:06:13,729
are before like because you know you

00:06:12,530 --> 00:06:16,909
have to know what the program is doing

00:06:13,729 --> 00:06:19,669
and this way you will know it's

00:06:16,909 --> 00:06:24,770
reasonable or not to be consuming all

00:06:19,669 --> 00:06:29,509
these so once you understand the code

00:06:24,770 --> 00:06:32,539
you can just start writing or like going

00:06:29,509 --> 00:06:34,520
in more inside and this kind of stuff

00:06:32,539 --> 00:06:37,070
one thing I want to command is that when

00:06:34,520 --> 00:06:40,370
you take one code you don't know usually

00:06:37,070 --> 00:06:41,990
this measuring time measuring resources

00:06:40,370 --> 00:06:45,229
and all stuff you apply it to the whole

00:06:41,990 --> 00:06:47,960
code but if it's your code you usually

00:06:45,229 --> 00:06:49,550
know which like which is the problematic

00:06:47,960 --> 00:06:52,009
part of the code so you just go to

00:06:49,550 --> 00:06:55,009
monitor that part of course once you

00:06:52,009 --> 00:06:57,139
apply the code part you just have to

00:06:55,009 --> 00:06:58,400
execute against we you don't you know

00:06:57,139 --> 00:07:02,180
you don't mess up with the whole

00:06:58,400 --> 00:07:06,580
execution but that's basically the flow

00:07:02,180 --> 00:07:10,900
I usually use oh I lost my this

00:07:06,580 --> 00:07:15,069
so let's start with very basic tools I

00:07:10,900 --> 00:07:16,960
know you'll know them but for me the

00:07:15,069 --> 00:07:19,210
most interesting ones i mean the first

00:07:16,960 --> 00:07:23,020
thing i do maybe the first five minutes

00:07:19,210 --> 00:07:25,090
is using time and age stop for checking

00:07:23,020 --> 00:07:27,909
how it goes the thing I liked usually

00:07:25,090 --> 00:07:31,180
for time is to check if my program is

00:07:27,909 --> 00:07:35,199
really resource bonded or I above it

00:07:31,180 --> 00:07:39,389
because I don't know if how many of you

00:07:35,199 --> 00:07:42,400
don't know the time too okay so

00:07:39,389 --> 00:07:46,030
basically the output it gives you is

00:07:42,400 --> 00:07:48,550
that the execution time in CPU and the

00:07:46,030 --> 00:07:51,909
question time of the user so if you have

00:07:48,550 --> 00:07:53,919
many block blocking stuff like network

00:07:51,909 --> 00:07:56,289
queries or my SQL queries it will tell

00:07:53,919 --> 00:07:59,729
you like which is the difference of it

00:07:56,289 --> 00:08:03,099
so it can give you an idea of their all

00:07:59,729 --> 00:08:07,629
the things the program is doing then

00:08:03,099 --> 00:08:09,400
this H stop is basically while I started

00:08:07,629 --> 00:08:13,210
using because I started using well they

00:08:09,400 --> 00:08:14,800
made me started using Mac and I don't

00:08:13,210 --> 00:08:18,639
know if you have checked the output of

00:08:14,800 --> 00:08:20,940
top but oh wait I think I have to move

00:08:18,639 --> 00:08:20,940
there

00:08:21,110 --> 00:08:28,110
presenter view I don't know how to get

00:08:23,430 --> 00:08:30,900
out of it so if you check the output of

00:08:28,110 --> 00:08:33,000
top for Mac it's like I mean if someone

00:08:30,900 --> 00:08:34,980
can read something about that I'll give

00:08:33,000 --> 00:08:37,350
you a rather a rare Pokemon because it's

00:08:34,980 --> 00:08:39,540
just like Messi I don't know it just I

00:08:37,350 --> 00:08:42,180
mean lineups it's pretty much worse so

00:08:39,540 --> 00:08:45,030
if you go with eight stop and think it's

00:08:42,180 --> 00:08:46,860
I mean at least it orders by cpu it

00:08:45,030 --> 00:08:49,230
shows you the four different processors

00:08:46,860 --> 00:08:52,980
memory and blah blah blah but as I said

00:08:49,230 --> 00:08:58,640
this is really basic and you feel ready

00:08:52,980 --> 00:09:06,710
in all that so let's go to some more oh

00:08:58,640 --> 00:09:09,710
wait please more interesting stuff ah oh

00:09:06,710 --> 00:09:09,710
really

00:09:11,290 --> 00:09:19,839
oh that one so first one is memory

00:09:17,259 --> 00:09:22,810
profiler this one is quite interesting

00:09:19,839 --> 00:09:24,820
because it allows you to check the whole

00:09:22,810 --> 00:09:27,089
flow of the program like the memory

00:09:24,820 --> 00:09:31,449
consumption for the whole program

00:09:27,089 --> 00:09:34,779
function oriented by lines and all this

00:09:31,449 --> 00:09:38,160
so just to bother is another feature

00:09:34,779 --> 00:09:41,050
interesting feature that it can you

00:09:38,160 --> 00:09:43,389
trigger at the bugger once you've

00:09:41,050 --> 00:09:46,389
reached a maximum capacity for memory so

00:09:43,389 --> 00:09:48,399
you can tell when executing like if the

00:09:46,389 --> 00:09:52,329
memory i'm using is more than one

00:09:48,399 --> 00:09:54,430
gigabyte or 100 megabytes just drop me

00:09:52,329 --> 00:09:56,019
into the bowler console so then you can

00:09:54,430 --> 00:09:58,899
check the status of the program the

00:09:56,019 --> 00:10:03,279
objects you have initialized and all

00:09:58,899 --> 00:10:06,699
these so this is some examples of the

00:10:03,279 --> 00:10:10,209
output um let's start with that one for

00:10:06,699 --> 00:10:13,180
example here if you can see from the

00:10:10,209 --> 00:10:15,910
back I don't know it shows you like for

00:10:13,180 --> 00:10:18,069
example we have mean that it took like

00:10:15,910 --> 00:10:19,839
17 seconds and then we have first

00:10:18,069 --> 00:10:22,810
quarterly function and second costly

00:10:19,839 --> 00:10:25,149
function and it just marks you where its

00:10:22,810 --> 00:10:27,670
function starts so it gives you an idea

00:10:25,149 --> 00:10:29,260
of the whole program which is the

00:10:27,670 --> 00:10:32,920
function or the functions you should

00:10:29,260 --> 00:10:34,690
focus on like to really improve the

00:10:32,920 --> 00:10:37,510
memory consumption which sometimes can

00:10:34,690 --> 00:10:40,540
be a problem so as another example here

00:10:37,510 --> 00:10:43,540
we have the terminal output for just

00:10:40,540 --> 00:10:47,260
checking the memory consumption per line

00:10:43,540 --> 00:10:50,110
which is kind of interesting to this one

00:10:47,260 --> 00:10:53,170
is really slow it takes like eight times

00:10:50,110 --> 00:11:00,590
more to execute but I mean for one time

00:10:53,170 --> 00:11:04,310
is pretty enough so let me show you the

00:11:00,590 --> 00:11:06,490
oh my god does anyone know how to move

00:11:04,310 --> 00:11:12,010
the terminal to the presenters

00:11:06,490 --> 00:11:12,010
presentation part easily

00:11:15,570 --> 00:11:22,530
no not just because I don't want to be

00:11:18,900 --> 00:11:24,750
out and just Alta but well so the

00:11:22,530 --> 00:11:28,440
program i use for that profit can you

00:11:24,750 --> 00:11:31,290
see from the back i don't think so so

00:11:28,440 --> 00:11:34,020
their program i used as a test is that

00:11:31,290 --> 00:11:37,080
one so I mean you have already seen the

00:11:34,020 --> 00:11:39,240
graph but when I was first trying that I

00:11:37,080 --> 00:11:41,640
thought that is prime would take much

00:11:39,240 --> 00:11:44,070
more memory than a trial division

00:11:41,640 --> 00:11:47,010
because of the big number and stuff but

00:11:44,070 --> 00:11:50,070
it resulted the other way so with just

00:11:47,010 --> 00:11:52,710
this basic tool you can just struck use

00:11:50,070 --> 00:11:54,120
a full information and but something

00:11:52,710 --> 00:12:01,740
something interesting to have in your

00:11:54,120 --> 00:12:05,160
tool set so the next one is for the

00:12:01,740 --> 00:12:08,040
other resource which is called line

00:12:05,160 --> 00:12:10,830
profiler you know people in Python

00:12:08,040 --> 00:12:15,180
usually we get really cool names for the

00:12:10,830 --> 00:12:18,000
tools we are really original so this one

00:12:15,180 --> 00:12:19,950
is an advanced version of see profile

00:12:18,000 --> 00:12:21,630
see profile is the built-in profiling

00:12:19,950 --> 00:12:24,180
tool for Python I'm not presenting it

00:12:21,630 --> 00:12:27,060
that because it's well the one used to

00:12:24,180 --> 00:12:28,980
always but basically it's useful to know

00:12:27,060 --> 00:12:32,250
the CPU consumption time of your

00:12:28,980 --> 00:12:35,910
problems it shows you per line per

00:12:32,250 --> 00:12:38,280
function the average percentage of

00:12:35,910 --> 00:12:41,220
consumption of time consumption of your

00:12:38,280 --> 00:12:43,880
lines your functions and everything it's

00:12:41,220 --> 00:12:46,620
compatible with see profile output and

00:12:43,880 --> 00:12:49,470
it's that something interesting I found

00:12:46,620 --> 00:12:52,200
out that I mean sometimes you start

00:12:49,470 --> 00:12:54,630
profiling and those profiling tools like

00:12:52,200 --> 00:12:56,490
multiply the time of execution and then

00:12:54,630 --> 00:12:59,460
you are profiling and you get pissed off

00:12:56,490 --> 00:13:01,800
and said it and then you take

00:12:59,460 --> 00:13:03,540
control C and usually with some kind of

00:13:01,800 --> 00:13:05,970
tools you lose all the progress because

00:13:03,540 --> 00:13:08,160
the report is generated at the end with

00:13:05,970 --> 00:13:10,329
that one just generates what it it

00:13:08,160 --> 00:13:13,509
displays you what it

00:13:10,329 --> 00:13:18,549
it calculated from from that time so

00:13:13,509 --> 00:13:21,339
it's pretty cool so that's the example

00:13:18,549 --> 00:13:23,649
of the output for that one I mean it's

00:13:21,339 --> 00:13:27,579
the same code and for example here you

00:13:23,649 --> 00:13:30,730
can see just a the execution time like

00:13:27,579 --> 00:13:35,439
that percentage time the number of heat

00:13:30,730 --> 00:13:39,309
for its line the time in total and the

00:13:35,439 --> 00:13:41,829
time for heat so for example here we can

00:13:39,309 --> 00:13:46,059
see which is a problematic function to

00:13:41,829 --> 00:13:49,179
like this one is one great of magnitude

00:13:46,059 --> 00:13:52,269
above the first costly function which if

00:13:49,179 --> 00:13:54,129
we wanted to to go to like check the

00:13:52,269 --> 00:13:55,779
condoms like which one we want to

00:13:54,129 --> 00:14:00,970
optimize we will go definitely for that

00:13:55,779 --> 00:14:04,149
one so as pretty cool it also takes some

00:14:00,970 --> 00:14:07,660
time to execute with a full code and I

00:14:04,149 --> 00:14:12,699
mean if you have code that takes like

00:14:07,660 --> 00:14:16,149
hours to execute using these tools are

00:14:12,699 --> 00:14:20,350
kind of what it's too long to use them

00:14:16,149 --> 00:14:22,899
and it's too long because usually you

00:14:20,350 --> 00:14:24,249
have to monitor the whole code and if

00:14:22,899 --> 00:14:28,059
you have done this kind of stuff

00:14:24,249 --> 00:14:30,999
sometimes it's a bit more difficult

00:14:28,059 --> 00:14:34,029
because you have to modify your source

00:14:30,999 --> 00:14:36,459
code I don't know well the profile

00:14:34,029 --> 00:14:38,199
decorator it was there so if you want to

00:14:36,459 --> 00:14:40,569
monitor the functions you have to use

00:14:38,199 --> 00:14:42,399
that decorator then you have to change

00:14:40,569 --> 00:14:44,499
your source code and then you execute

00:14:42,399 --> 00:14:46,629
you realize that that wasn't the

00:14:44,499 --> 00:14:48,429
function you want to profile then you go

00:14:46,629 --> 00:14:52,239
to another function you have to execute

00:14:48,429 --> 00:14:57,369
again so it's kind of a messy stuff so

00:14:52,239 --> 00:15:01,029
that's where our super ipython comes to

00:14:57,369 --> 00:15:02,949
help so I Python line profiler and

00:15:01,029 --> 00:15:07,059
memory profiler are supported of us

00:15:02,949 --> 00:15:09,170
plugins for ipython and that's really

00:15:07,059 --> 00:15:11,750
cool because it allows you to

00:15:09,170 --> 00:15:13,779
profile any function in your code or

00:15:11,750 --> 00:15:16,100
source code of any other library

00:15:13,779 --> 00:15:19,850
interactively so you don't have to

00:15:16,100 --> 00:15:23,870
execute the full program so just let me

00:15:19,850 --> 00:15:27,529
show you one of the example output but

00:15:23,870 --> 00:15:30,079
now we will play a bit with it so we are

00:15:27,529 --> 00:15:32,510
if you check at the top of the screen we

00:15:30,079 --> 00:15:35,029
are just using the load extension lipo

00:15:32,510 --> 00:15:37,010
filer then i'm importing my program and

00:15:35,029 --> 00:15:40,100
then i'm using the common the magic

00:15:37,010 --> 00:15:43,550
common LP run with the function i want

00:15:40,100 --> 00:15:45,410
to profile which is the minus F and then

00:15:43,550 --> 00:15:48,829
the program I'm calling which is run

00:15:45,410 --> 00:15:53,510
marks well Randall mouth dot second

00:15:48,829 --> 00:15:55,160
costly function yeah so here we are

00:15:53,510 --> 00:16:00,230
having the report for the line profiler

00:15:55,160 --> 00:16:02,120
which it's a level one level deeper than

00:16:00,230 --> 00:16:05,000
we had previously just monitoring for

00:16:02,120 --> 00:16:07,519
the outside of function so here we can

00:16:05,000 --> 00:16:10,339
see that why is the second cuddly

00:16:07,519 --> 00:16:12,829
function more costly it's because it's

00:16:10,339 --> 00:16:14,660
indeed calling this editor Stannis

00:16:12,829 --> 00:16:16,220
method which I don't know what it does i

00:16:14,660 --> 00:16:18,760
think it's something with prime numbers

00:16:16,220 --> 00:16:22,130
but that's the one that it's actually

00:16:18,760 --> 00:16:25,699
taking lots of time so here we have also

00:16:22,130 --> 00:16:28,790
the heats the time the per hit time and

00:16:25,699 --> 00:16:33,110
the amount of time it's consuming which

00:16:28,790 --> 00:16:38,360
is what we want to actually work with so

00:16:33,110 --> 00:16:50,220
just to show you that I'm not lying

00:16:38,360 --> 00:16:56,929
okay so 00 so if we go to I know it's

00:16:50,220 --> 00:17:00,119
see what that one I passion and then I'm

00:16:56,929 --> 00:17:07,409
you put that on top then I'm losing the

00:17:00,119 --> 00:17:12,569
load next line profiler then from my

00:17:07,409 --> 00:17:16,380
program i'm importing a second costly

00:17:12,569 --> 00:17:18,419
function and then from algorithms now we

00:17:16,380 --> 00:17:20,549
are going to introspect the editors

00:17:18,419 --> 00:17:22,829
tennis function without touching our

00:17:20,549 --> 00:17:25,559
source code so you can say how easy it

00:17:22,829 --> 00:17:27,720
is to check like how much time it's line

00:17:25,559 --> 00:17:32,159
of the rattlesnake function is spending

00:17:27,720 --> 00:17:35,130
so from algorithms that math dot see if

00:17:32,159 --> 00:17:36,510
well I knew that already but sometimes

00:17:35,130 --> 00:17:39,029
you will have to check in the source

00:17:36,510 --> 00:17:41,789
code where each function is but you will

00:17:39,029 --> 00:17:46,500
know that so we are importing this

00:17:41,789 --> 00:17:50,309
function and now we are running a LP run

00:17:46,500 --> 00:17:53,820
we want to profile the editors tennis

00:17:50,309 --> 00:18:00,419
function and we are gonna run again the

00:17:53,820 --> 00:18:02,070
second costly function so now we are

00:18:00,419 --> 00:18:04,260
running the second currently function

00:18:02,070 --> 00:18:08,429
monitoring the time that editors tennis

00:18:04,260 --> 00:18:10,770
is taking so what if the second now it's

00:18:08,429 --> 00:18:13,230
taking only like 20 seconds of stuff so

00:18:10,770 --> 00:18:16,289
what if the function took like two hours

00:18:13,230 --> 00:18:19,020
really we have to evaluate like spend

00:18:16,289 --> 00:18:21,419
two hours so just to check the cost of

00:18:19,020 --> 00:18:24,750
editors 10s function that's another cool

00:18:21,419 --> 00:18:26,880
thing we have to we can do just well

00:18:24,750 --> 00:18:30,299
console see you know the repository for

00:18:26,880 --> 00:18:33,090
now so we can just call the editor

00:18:30,299 --> 00:18:36,059
Stannis function by itself by calling it

00:18:33,090 --> 00:18:37,070
with random arcs and it will show us

00:18:36,059 --> 00:18:40,820
there

00:18:37,070 --> 00:18:42,200
report as fast as it calculates just one

00:18:40,820 --> 00:18:44,929
iteration because in the original

00:18:42,200 --> 00:18:46,519
programming was calling it like 800

00:18:44,929 --> 00:18:48,919
thousand times which is not necessary to

00:18:46,519 --> 00:18:51,409
have a basic profiling of the function

00:18:48,919 --> 00:18:54,740
so we can see again here which are the

00:18:51,409 --> 00:18:56,419
functions that are like what functions

00:18:54,740 --> 00:18:59,450
well operations that are the most

00:18:56,419 --> 00:19:01,639
time-consuming so if that was a real

00:18:59,450 --> 00:19:04,610
problem we want to optimize we would

00:19:01,639 --> 00:19:07,789
check like those ones are really loops

00:19:04,610 --> 00:19:09,679
we what we need are these operations the

00:19:07,789 --> 00:19:12,019
most optimal ones and stuff we are not

00:19:09,679 --> 00:19:14,360
we are not going to go into this detail

00:19:12,019 --> 00:19:18,559
but now at least we have spotted where

00:19:14,360 --> 00:19:21,440
we want to do all our stuff and I mean

00:19:18,559 --> 00:19:27,470
for me I python is just pretty handy for

00:19:21,440 --> 00:19:32,870
doing this thing interactively so back

00:19:27,470 --> 00:19:35,419
to the presentation um I mean if some of

00:19:32,870 --> 00:19:38,450
you have worked with that already I mean

00:19:35,419 --> 00:19:41,120
come on like if you search for profiling

00:19:38,450 --> 00:19:42,980
in Python blind profiler and weaponry

00:19:41,120 --> 00:19:47,240
profiler and these kind of tools are the

00:19:42,980 --> 00:19:48,889
first ones you obtain so this second

00:19:47,240 --> 00:19:52,429
part of the presentation which I called

00:19:48,889 --> 00:19:55,519
I call the advanced ones are more cool

00:19:52,429 --> 00:19:57,409
or fancy tools that displays visual

00:19:55,519 --> 00:20:01,220
graph so you can play more interactively

00:19:57,409 --> 00:20:05,210
with them which I mean that's cool

00:20:01,220 --> 00:20:09,440
leagues at least it looks cool so the

00:20:05,210 --> 00:20:11,450
first one I like well I kind of love and

00:20:09,440 --> 00:20:14,840
hate at the same time you will see why

00:20:11,450 --> 00:20:16,879
but is this one called plot it doesn't

00:20:14,840 --> 00:20:21,009
have much companies and it's not that

00:20:16,879 --> 00:20:23,539
much maintained but it does its work so

00:20:21,009 --> 00:20:26,870
the fetch the features it has is that

00:20:23,539 --> 00:20:30,049
it's a really low overhead profiler so

00:20:26,870 --> 00:20:31,820
why is that because it uses the ashtrays

00:20:30,049 --> 00:20:34,419
and ultra is function from the airport

00:20:31,820 --> 00:20:37,940
operating system which basically it just

00:20:34,419 --> 00:20:40,129
reached the stack of the program being

00:20:37,940 --> 00:20:41,960
executed so it doesn't interact with

00:20:40,129 --> 00:20:42,860
your program like we've seen with memory

00:20:41,960 --> 00:20:45,200
profile instead

00:20:42,860 --> 00:20:49,850
which just put stuff in the middle of

00:20:45,200 --> 00:20:52,370
your poem so it does this stack analysis

00:20:49,850 --> 00:20:54,290
and it basically displays a call graph

00:20:52,370 --> 00:20:56,720
or for your functions how they are

00:20:54,290 --> 00:20:59,870
called between them and the time spent

00:20:56,720 --> 00:21:01,549
with them and it also displays flame

00:20:59,870 --> 00:21:03,830
graph which I don't know if you know

00:21:01,549 --> 00:21:06,380
what it is but it's something pretty

00:21:03,830 --> 00:21:08,510
fancy and for example in netflix they

00:21:06,380 --> 00:21:12,380
are using it I don't know if you follow

00:21:08,510 --> 00:21:14,870
their blood but it's pretty useful so

00:21:12,380 --> 00:21:17,480
there has a server running with toronado

00:21:14,870 --> 00:21:21,470
and with lesson set up you can just

00:21:17,480 --> 00:21:25,669
feedback this we were so you can upload

00:21:21,470 --> 00:21:33,169
this visualization in real real time so

00:21:25,669 --> 00:21:35,330
to show you these things for example

00:21:33,169 --> 00:21:38,510
here we have the profile bill that's why

00:21:35,330 --> 00:21:40,730
I hit the program because I mean in the

00:21:38,510 --> 00:21:43,610
call graph it's a bit messy and then if

00:21:40,730 --> 00:21:46,460
you want like to just move the stuff you

00:21:43,610 --> 00:21:48,440
have to play a bit and do like well

00:21:46,460 --> 00:21:51,320
movie the vid so I can see it better so

00:21:48,440 --> 00:21:52,700
it's kind of shitty but here we can see

00:21:51,320 --> 00:21:54,860
that they're at the Stennis function is

00:21:52,700 --> 00:21:57,200
indeed they won consuming most time and

00:21:54,860 --> 00:21:59,179
the main is with the one calling second

00:21:57,200 --> 00:22:03,020
call a function at first cuddly function

00:21:59,179 --> 00:22:05,419
so we we can see the flow of it and then

00:22:03,020 --> 00:22:07,910
we also see the size of that the width

00:22:05,419 --> 00:22:09,679
of the arrows is the time spent also

00:22:07,910 --> 00:22:12,650
calling these functions so it's pretty

00:22:09,679 --> 00:22:15,260
useful and also with the flame graph um

00:22:12,650 --> 00:22:17,059
I don't know I know you don't see

00:22:15,260 --> 00:22:20,030
anything here but let me explain it did

00:22:17,059 --> 00:22:22,160
you so it's the same like here at the

00:22:20,030 --> 00:22:24,679
beach we have the main function the main

00:22:22,160 --> 00:22:26,360
function calls second costly function

00:22:24,679 --> 00:22:29,360
and first currently function so you have

00:22:26,360 --> 00:22:32,690
to go from bottom to top and the width

00:22:29,360 --> 00:22:35,540
of the block in the tells you how long

00:22:32,690 --> 00:22:37,460
did it take to execute this function so

00:22:35,540 --> 00:22:41,090
here at the bottom it tells you that

00:22:37,460 --> 00:22:41,760
mean to ninety nine percent this edit

00:22:41,090 --> 00:22:44,580
Austin

00:22:41,760 --> 00:22:46,620
call to eighty-nine percent so that's

00:22:44,580 --> 00:22:48,450
pretty cool and let me show you why it's

00:22:46,620 --> 00:22:51,330
called flame graph because with a real

00:22:48,450 --> 00:22:54,210
problem it looks like that this is lots

00:22:51,330 --> 00:22:57,840
of stuff but you know have to iterate a

00:22:54,210 --> 00:23:08,010
bit and learn how to isolate stuff there

00:22:57,840 --> 00:23:10,050
so next one it's called oops pi formats

00:23:08,010 --> 00:23:12,600
I'm running out of time so I'm going to

00:23:10,050 --> 00:23:16,230
skip their code I had prepared but you

00:23:12,600 --> 00:23:18,750
can reach me out then later so by four

00:23:16,230 --> 00:23:22,050
months it doesn't have fancy graphs but

00:23:18,750 --> 00:23:24,930
it's pretty cool because I bet you have

00:23:22,050 --> 00:23:29,190
done a lot this typical from time import

00:23:24,930 --> 00:23:32,010
time start time time dot time then we do

00:23:29,190 --> 00:23:34,980
some stuff then end time time time and

00:23:32,010 --> 00:23:37,530
then we apply the subtraction and then

00:23:34,980 --> 00:23:40,950
we log it and stuff so basically

00:23:37,530 --> 00:23:43,230
performance is set of tools that with

00:23:40,950 --> 00:23:45,510
context managers and this kind of stuff

00:23:43,230 --> 00:23:48,300
it gives you like how many times a

00:23:45,510 --> 00:23:51,540
function has been called how long it

00:23:48,300 --> 00:23:53,100
took a function to process how many this

00:23:51,540 --> 00:23:55,200
one is really interesting that measure

00:23:53,100 --> 00:23:57,330
rate of events over time so it tells you

00:23:55,200 --> 00:23:59,010
how many times your function was caught

00:23:57,330 --> 00:24:01,320
during the last second during the last

00:23:59,010 --> 00:24:03,240
minute during the 15 minutes so this is

00:24:01,320 --> 00:24:06,300
really cool for generating metrics for

00:24:03,240 --> 00:24:09,830
an API or for this kind of stuff so it's

00:24:06,300 --> 00:24:12,480
not exactly an interactive or

00:24:09,830 --> 00:24:14,850
optimization tool like for checking

00:24:12,480 --> 00:24:16,440
their profile but it's more like for

00:24:14,850 --> 00:24:19,860
generating matrix so you can just

00:24:16,440 --> 00:24:21,630
monitor how your application is going

00:24:19,860 --> 00:24:23,220
something you have to taking into

00:24:21,630 --> 00:24:25,770
account is that these timers which are

00:24:23,220 --> 00:24:28,530
the ones measuring rate of events are

00:24:25,770 --> 00:24:30,630
shed variables so you have to be aware

00:24:28,530 --> 00:24:33,090
that if you are using threads and this

00:24:30,630 --> 00:24:36,840
kind of stuff it uses logs internally so

00:24:33,090 --> 00:24:39,690
we can just end up being a mess if you

00:24:36,840 --> 00:24:41,299
don't use properly so that the code I

00:24:39,690 --> 00:24:43,879
had prepared but

00:24:41,299 --> 00:24:45,200
I'll upload the slides and because i

00:24:43,879 --> 00:24:47,989
want to present this last one which is

00:24:45,200 --> 00:24:50,809
the one that basically so this last one

00:24:47,989 --> 00:24:55,220
is called k category and this one was

00:24:50,809 --> 00:24:59,299
originally for providing c and c++ in

00:24:55,220 --> 00:25:01,690
the research job I had I was doing C++

00:24:59,299 --> 00:25:04,299
stuff and that's the one I was using for

00:25:01,690 --> 00:25:09,230
optimizing all these pointers and

00:25:04,299 --> 00:25:13,159
operations and things it's a really old

00:25:09,230 --> 00:25:16,549
tool I think it started at 2002 or some

00:25:13,159 --> 00:25:18,409
stuff so it just uses well it displays

00:25:16,549 --> 00:25:21,379
you the call graph it displays the

00:25:18,409 --> 00:25:25,909
execution time it displays block bill of

00:25:21,379 --> 00:25:29,149
the F stand for the functions also the

00:25:25,909 --> 00:25:32,539
time cost for line which is not true in

00:25:29,149 --> 00:25:35,419
Python but it was in C++ I was I haven't

00:25:32,539 --> 00:25:38,049
been able to make it work properly in

00:25:35,419 --> 00:25:42,200
Python but I think I want to do it and

00:25:38,049 --> 00:25:45,980
it also displays assembly code which of

00:25:42,200 --> 00:25:48,559
course I use every day and it reads from

00:25:45,980 --> 00:25:52,129
see profile output using this tool which

00:25:48,559 --> 00:25:54,369
is by proof to call three so just to

00:25:52,129 --> 00:25:54,369
check

00:25:54,370 --> 00:26:01,110
the output this is the call graph we

00:25:56,770 --> 00:26:03,450
have from the same program so just to

00:26:01,110 --> 00:26:10,360
instead of showing you an image let's

00:26:03,450 --> 00:26:20,430
open the program so you can see how it

00:26:10,360 --> 00:26:23,050
goes so I have this profile already

00:26:20,430 --> 00:26:26,800
generated but basically what you have to

00:26:23,050 --> 00:26:31,150
do is call your Python program with the

00:26:26,800 --> 00:26:33,730
see profile module which will output the

00:26:31,150 --> 00:26:37,600
profile information and then you have to

00:26:33,730 --> 00:26:40,750
convert it to call green format so cold

00:26:37,600 --> 00:26:42,610
Lincoln well k catch a green can

00:26:40,750 --> 00:26:45,520
understand it it's called with kill

00:26:42,610 --> 00:26:48,030
because in mac k it doesn't work you

00:26:45,520 --> 00:26:54,429
have to do they use the q continuing

00:26:48,030 --> 00:26:58,990
yeah it's just like well so here we have

00:26:54,429 --> 00:27:01,059
these so basically here we have the

00:26:58,990 --> 00:27:03,460
order functions that have been called in

00:27:01,059 --> 00:27:05,620
our program which i want to check the

00:27:03,460 --> 00:27:08,380
ones that we basically control we are

00:27:05,620 --> 00:27:10,570
not going to optimize the math s square

00:27:08,380 --> 00:27:14,230
root of python or we are not going to

00:27:10,570 --> 00:27:15,910
but or the length or that so we wanted

00:27:14,230 --> 00:27:17,679
we just want to check the main at a

00:27:15,910 --> 00:27:20,710
thirstiness and the ones we actually

00:27:17,679 --> 00:27:24,130
programmed so in that view we have them

00:27:20,710 --> 00:27:27,250
ordered by like the inclusive time that

00:27:24,130 --> 00:27:29,559
it takes so the main has taken one

00:27:27,250 --> 00:27:32,200
hundred percent of them obviously and

00:27:29,559 --> 00:27:34,300
then we have like from the most time

00:27:32,200 --> 00:27:37,929
consuming to the last at least ten

00:27:34,300 --> 00:27:39,700
consumed so if we order by self we will

00:27:37,929 --> 00:27:43,230
see again that additive stance is the

00:27:39,700 --> 00:27:49,460
one that consumed more time so

00:27:43,230 --> 00:27:52,520
we can go for example here and here

00:27:49,460 --> 00:27:57,660
thank you

00:27:52,520 --> 00:28:01,530
we want to select main okay so in the

00:27:57,660 --> 00:28:03,750
first one it's a fast busy just rather

00:28:01,530 --> 00:28:07,110
than ordering here and all the stuff in

00:28:03,750 --> 00:28:11,340
the top right part we have a blog view

00:28:07,110 --> 00:28:12,930
which just lets us easily check which

00:28:11,340 --> 00:28:14,550
are the front which is the function or

00:28:12,930 --> 00:28:16,680
the functions that are taking the most

00:28:14,550 --> 00:28:18,750
time to execute which is like I mean

00:28:16,680 --> 00:28:23,400
just what by one load you can check okay

00:28:18,750 --> 00:28:26,390
here is the problem and in the bottom

00:28:23,400 --> 00:28:30,120
right part we have the call graph of our

00:28:26,390 --> 00:28:32,940
program flow so we called main at first

00:28:30,120 --> 00:28:35,820
then main called first quarterly fashion

00:28:32,940 --> 00:28:38,090
and sync on costly function and then

00:28:35,820 --> 00:28:40,620
first call the function called is prime

00:28:38,090 --> 00:28:42,630
second costly function called trial

00:28:40,620 --> 00:28:46,500
division and both of them called

00:28:42,630 --> 00:28:49,170
Eratosthenes and again like in the blob

00:28:46,500 --> 00:28:52,350
tool we have the size of the arrow and

00:28:49,170 --> 00:28:54,360
this thing here that tells you how much

00:28:52,350 --> 00:28:57,870
time was spent calling that function

00:28:54,360 --> 00:29:02,370
which is also the same here and then

00:28:57,870 --> 00:29:04,020
that 1x is how many times we call that

00:29:02,370 --> 00:29:08,070
function

00:29:04,020 --> 00:29:13,420
so that's a really interesting tool

00:29:08,070 --> 00:29:16,380
another thing i really like it about

00:29:13,420 --> 00:29:20,620
that when I was probably in six places

00:29:16,380 --> 00:29:23,800
that it's now obviously doesn't work but

00:29:20,620 --> 00:29:26,260
it was showing you a report of like

00:29:23,800 --> 00:29:29,080
we've seen in line profiler so for any

00:29:26,260 --> 00:29:32,650
function of your code that you have

00:29:29,080 --> 00:29:37,090
called it will it would display a cost

00:29:32,650 --> 00:29:39,940
what cpu time cipher next to the line so

00:29:37,090 --> 00:29:42,790
you know every line up might has taken

00:29:39,940 --> 00:29:46,240
to execute it I'm still working on it so

00:29:42,790 --> 00:29:54,340
I'll a loop datafile make it work

00:29:46,240 --> 00:29:56,650
correctly so just to finish how are we

00:29:54,340 --> 00:29:59,680
about time are those five minutes with

00:29:56,650 --> 00:30:04,060
questions or without questions now then

00:29:59,680 --> 00:30:09,520
I'll cool then let me show you like the

00:30:04,060 --> 00:30:11,500
code I skip from the performance one

00:30:09,520 --> 00:30:18,040
which is I think it's interesting to

00:30:11,500 --> 00:30:21,690
have to hear so now we are going to go

00:30:18,040 --> 00:30:21,690
to the terminal looks better

00:30:24,669 --> 00:30:30,810
so

00:30:27,170 --> 00:30:34,050
this is a goat that I've that is using

00:30:30,810 --> 00:30:36,210
performance so basically i'm importing

00:30:34,050 --> 00:30:38,520
timer which is at all given by

00:30:36,210 --> 00:30:41,160
performance so you can do with timer

00:30:38,520 --> 00:30:43,830
test dot type that's a shared by level i

00:30:41,160 --> 00:30:45,690
was talking about so in any other part

00:30:43,830 --> 00:30:49,830
of your code you can just access test

00:30:45,690 --> 00:30:52,050
timer and sprint the mean get max get

00:30:49,830 --> 00:30:54,960
bar which basically the it's printing

00:30:52,050 --> 00:30:57,300
the meantime it's taken to execute that

00:30:54,960 --> 00:31:00,210
part of code the variance of the

00:30:57,300 --> 00:31:03,330
executions mean rate which is the number

00:31:00,210 --> 00:31:06,030
of executions for one second quite 1

00:31:03,330 --> 00:31:08,340
minute rate for one minute and this kind

00:31:06,030 --> 00:31:10,890
of stuff it's pretty handy because I

00:31:08,340 --> 00:31:14,460
mean it's imagine you had a goat and you

00:31:10,890 --> 00:31:17,400
want to keep it like you know it's being

00:31:14,460 --> 00:31:19,560
executed around with 10 seconds or so so

00:31:17,400 --> 00:31:22,050
you can you just use this tool to

00:31:19,560 --> 00:31:25,290
trigger an alarm or lock and tell you oh

00:31:22,050 --> 00:31:27,480
god this function took like 20 seconds

00:31:25,290 --> 00:31:29,670
now something's going wrong so you can

00:31:27,480 --> 00:31:35,600
use that for your performance tests or

00:31:29,670 --> 00:31:41,300
this kind of stuff so for a test also

00:31:35,600 --> 00:31:43,950
let me show you this basically it's

00:31:41,300 --> 00:31:45,720
telling me that there is a lot slow

00:31:43,950 --> 00:31:49,500
execution because I don't know if you

00:31:45,720 --> 00:31:53,220
saw the threshold cipher I had here that

00:31:49,500 --> 00:31:54,870
I have threshold zero dot 21 then if the

00:31:53,220 --> 00:31:56,640
gain mean is above the threshold it's

00:31:54,870 --> 00:31:58,500
telling me slow execution something

00:31:56,640 --> 00:32:01,650
wrong happened and why is that because

00:31:58,500 --> 00:32:06,210
I'm doing asleep Randall uniform zero

00:32:01,650 --> 00:32:08,250
dot 103 which not exactly but the time

00:32:06,210 --> 00:32:10,470
the time it should take should be zero

00:32:08,250 --> 00:32:12,899
dot to but because of internal stuff it

00:32:10,470 --> 00:32:20,809
takes a bit more so that's

00:32:12,899 --> 00:32:20,809
handy to have so now yeah for finishing

00:32:23,950 --> 00:32:34,040
presentation um I know I've presented a

00:32:28,970 --> 00:32:37,790
bunch of tools um some of them want feat

00:32:34,040 --> 00:32:40,060
on your tool set some of them will so

00:32:37,790 --> 00:32:42,740
one thing it's really important before

00:32:40,060 --> 00:32:44,900
starting with this kind of stuff it's

00:32:42,740 --> 00:32:47,750
just building your tool set like what

00:32:44,900 --> 00:32:49,670
exactly is it like do i need this kind

00:32:47,750 --> 00:32:51,830
of tool or do i need something more

00:32:49,670 --> 00:32:55,250
advanced for the framework i'm using

00:32:51,830 --> 00:32:58,010
like Django debug tool bar or if I'm

00:32:55,250 --> 00:32:59,750
using gln greenlight profiler because

00:32:58,010 --> 00:33:02,870
see profile doesn't work well with them

00:32:59,750 --> 00:33:06,200
so just try to do some kind of research

00:33:02,870 --> 00:33:11,630
before using any tool I find in Stack

00:33:06,200 --> 00:33:17,450
Overflow or Google results so that's

00:33:11,630 --> 00:33:19,370
pretty much all I've hope you found it

00:33:17,450 --> 00:33:22,700
interesting or you have at least you

00:33:19,370 --> 00:33:26,120
learned something new so if you have any

00:33:22,700 --> 00:33:28,400
questions or if we don't have for time

00:33:26,120 --> 00:33:31,460
enough for questions you can just reach

00:33:28,400 --> 00:33:33,980
me outside come to talk about these or

00:33:31,460 --> 00:33:38,530
any other random stuff I'll be happy to

00:33:33,980 --> 00:33:38,530
talk with you so questions

00:33:58,610 --> 00:34:05,660
well first of all thank you very nice

00:34:02,860 --> 00:34:09,260
just a simple question which one of

00:34:05,660 --> 00:34:12,350
these do you use the most so as I said

00:34:09,260 --> 00:34:14,990
that my favorite one is K catch a green

00:34:12,350 --> 00:34:16,879
because it just I mean the execution is

00:34:14,990 --> 00:34:18,679
really slow but just buy one execution

00:34:16,879 --> 00:34:21,649
you have the full view of what's going

00:34:18,679 --> 00:34:23,840
on on your program thing is that since i

00:34:21,649 --> 00:34:26,090
don't have the CBO time after that

00:34:23,840 --> 00:34:28,550
execution usually I end up using line

00:34:26,090 --> 00:34:31,159
profiler for that interact interactively

00:34:28,550 --> 00:34:33,560
with ipython because once I have the big

00:34:31,159 --> 00:34:36,440
picture then I'll ok that's a function

00:34:33,560 --> 00:34:38,359
that it's pissing me off I'm going just

00:34:36,440 --> 00:34:40,790
to call to the ipython i call it

00:34:38,359 --> 00:34:44,090
manually so i know which lines exactly

00:34:40,790 --> 00:34:45,859
inside that function of the problematic

00:34:44,090 --> 00:34:48,889
ones so that's basically the two of them

00:34:45,859 --> 00:34:50,480
but the other ones i like also to use

00:34:48,889 --> 00:34:53,179
them because sometimes by using

00:34:50,480 --> 00:34:55,550
different tools I mean by just using one

00:34:53,179 --> 00:34:58,010
so sometimes features you have a narrow

00:34:55,550 --> 00:35:00,260
view of what's going on so by having

00:34:58,010 --> 00:35:03,109
different tool sometimes with this flame

00:35:00,260 --> 00:35:07,040
graph or this blob stuff or any other

00:35:03,109 --> 00:35:10,280
memory for example um you have more big

00:35:07,040 --> 00:35:13,220
picture of what's going on because with

00:35:10,280 --> 00:35:16,160
just one tool you don't see all of it

00:35:13,220 --> 00:35:18,350
and sadly there is no tool that does

00:35:16,160 --> 00:35:21,100
everything so you have to play a bit

00:35:18,350 --> 00:35:21,100
with it

00:35:22,170 --> 00:35:29,490
okay thank you we have more than one we

00:35:25,990 --> 00:35:29,490
have some minutes for questions

00:35:30,610 --> 00:35:36,490
a very interesting talk I'm you do you

00:35:34,330 --> 00:35:39,010
have any advice on measuring the

00:35:36,490 --> 00:35:41,920
performance of an ongoing process like a

00:35:39,010 --> 00:35:45,930
long running process or something like

00:35:41,920 --> 00:35:47,860
that so you mean something like API or

00:35:45,930 --> 00:35:51,100
yes something that is running in the

00:35:47,860 --> 00:35:53,530
background dreaming yes so I mean there

00:35:51,100 --> 00:35:55,410
are two kind of things here for for me

00:35:53,530 --> 00:35:58,060
long running processes can be

00:35:55,410 --> 00:36:01,420
simulations using in research that can

00:35:58,060 --> 00:36:07,330
take like one week or I don't know a lot

00:36:01,420 --> 00:36:09,760
of that so for those kind logging is a

00:36:07,330 --> 00:36:11,410
really important one like logging for

00:36:09,760 --> 00:36:13,510
example this by four months 1 i've

00:36:11,410 --> 00:36:16,000
showed you it's really interesting

00:36:13,510 --> 00:36:18,760
because then you can just have feedback

00:36:16,000 --> 00:36:21,850
of different metrics inside your program

00:36:18,760 --> 00:36:23,680
like how many calls how many times have

00:36:21,850 --> 00:36:26,470
to call this function you know is the

00:36:23,680 --> 00:36:28,660
big one is that number of calls they

00:36:26,470 --> 00:36:32,560
expect the one you were expecting or not

00:36:28,660 --> 00:36:35,170
so that's for one side and for the other

00:36:32,560 --> 00:36:38,140
one for example APA calls and this kind

00:36:35,170 --> 00:36:40,060
of stuff as I said by performance is

00:36:38,140 --> 00:36:42,670
already also an interesting one because

00:36:40,060 --> 00:36:45,490
it can tell you like how many times this

00:36:42,670 --> 00:36:48,490
endpoint has been called or this kind of

00:36:45,490 --> 00:36:52,180
stuff and the blob 1 i'm not using it in

00:36:48,490 --> 00:36:55,420
life services and in production services

00:36:52,180 --> 00:36:58,330
but I know people is like they are using

00:36:55,420 --> 00:37:00,040
it just to know how staff is building

00:36:58,330 --> 00:37:02,800
like this call graph is building like

00:37:00,040 --> 00:37:05,020
during the execution we have to set up

00:37:02,800 --> 00:37:07,390
because the comments i'm showing in the

00:37:05,020 --> 00:37:09,940
presentation are for the whole process

00:37:07,390 --> 00:37:11,860
so you start the monitor and then it

00:37:09,940 --> 00:37:14,800
ends and then it creates a file so you

00:37:11,860 --> 00:37:17,230
can open it but it has contacts managers

00:37:14,800 --> 00:37:20,470
and this kind of stuff so you can open

00:37:17,230 --> 00:37:22,540
the file resistir the activity close the

00:37:20,470 --> 00:37:25,330
file and then call another function do

00:37:22,540 --> 00:37:27,870
this so this file gets dynamically Apple

00:37:25,330 --> 00:37:30,520
evolved so then you can check the

00:37:27,870 --> 00:37:31,980
visualization from time to time so to

00:37:30,520 --> 00:37:34,230
see how it goes

00:37:31,980 --> 00:37:36,359
so but for me basically the most

00:37:34,230 --> 00:37:39,530
interesting part is the login for the

00:37:36,359 --> 00:37:39,530
long ones blogging

00:37:42,599 --> 00:37:47,660
oh I don't know if it was you or

00:37:49,880 --> 00:37:55,470
hi interesting talks thank you and the

00:37:52,830 --> 00:37:57,810
line or the memory profile I do they

00:37:55,470 --> 00:37:59,880
also work with arc parts or with command

00:37:57,810 --> 00:38:03,270
line parameters so with a command line

00:37:59,880 --> 00:38:06,210
parameters yes when if by my command my

00:38:03,270 --> 00:38:09,780
my my script is working on command line

00:38:06,210 --> 00:38:12,810
and expects parameters and they do they

00:38:09,780 --> 00:38:14,550
mix up or I saw that you called it with

00:38:12,810 --> 00:38:18,360
an minus a because you mean that because

00:38:14,550 --> 00:38:21,780
of the am proof thing so you can call it

00:38:18,360 --> 00:38:24,690
that way then it will just monitor all

00:38:21,780 --> 00:38:26,970
the decorated functions which take less

00:38:24,690 --> 00:38:30,560
time but you can also call it with Tamil

00:38:26,970 --> 00:38:33,660
Python so you do Python mine minus M

00:38:30,560 --> 00:38:35,760
memory profiler and then your program

00:38:33,660 --> 00:38:37,530
with all the arguments you are using for

00:38:35,760 --> 00:38:40,380
your script and then it will it will

00:38:37,530 --> 00:38:46,500
work anyway so there is two two ways of

00:38:40,380 --> 00:38:49,310
calling it so yeah okay thank you very

00:38:46,500 --> 00:38:49,310

YouTube URL: https://www.youtube.com/watch?v=qKdxOdT4c1U


