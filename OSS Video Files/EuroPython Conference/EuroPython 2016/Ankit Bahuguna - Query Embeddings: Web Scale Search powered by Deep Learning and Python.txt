Title: Ankit Bahuguna - Query Embeddings: Web Scale Search powered by Deep Learning and Python
Publication date: 2016-08-01
Playlist: EuroPython 2016
Description: 
	Ankit Bahuguna - Query Embeddings: Web Scale Search powered by Deep Learning and Python
[EuroPython 2016]
[18 July 2016]
[Bilbao, Euskadi, Spain]
(https://ep2016.europython.eu//conference/talks/query-embeddings)

Query Embeddings is an unsupervised deep learning based system, built
using Python and open source libraries (Annoy, keyvi etc.) which
recognizes similarity between queries and their vector
representations, for a web scale search engine integrated within Cliqz
browser [https://cliqz.com/en]. It improves recall for previously
unseen queries and is one of the many key components of our search
stack. The framework be utilized by other low latency systems
involving vector representations.

-----

A web search engine allows a user to type few words of query and it
presents list of potential relevant results within fraction of a
second. Traditionally, keywords in the user query were fuzzy-matched
in realtime with the keywords within different pages of the index and
they didn't really focus on understanding meaning of query. Recently,
Deep Learning + NLP techniques try to _represent sentences or
documents as fixed dimensional vectors in high dimensional space_.
These special vectors inherit semantics of the document.

Query embeddings is an unsupervised deep learning based system, built
using Python, Word2Vec, Annoy and Keyvi (https://github.com/cliqz-
oss/keyvi) which recognizes similarity between queries and their
vectors  for a web scale search engine within Cliqz browser.
(https://cliqz.com/en)

![][1]

The goal is to describe how query embeddings contribute to our
existing python search stack at scale and latency issues prevailing in
real time search system.  Also is a preview of separate vector index
for queries, utilized by retrieval system at runtime via ANNs to get
closest queries to user query, which is one of the many key components
of our search stack.

![][2]

Prerequisites: Basic experience in NLP, ML, Deep Learning, Web search
and Vector Algebra. Libraries:  Annoy.

[1]: https://sites.google.com/site/netankit/1.png
[2]: https://sites.google.com/site/netankit/3.png
Captions: 
	00:00:00,000 --> 00:00:04,140
lost so our next speaker is ankiets

00:00:02,370 --> 00:00:06,390
Bahuguna and he'll be talking about

00:00:04,140 --> 00:00:10,069
query embeddings web scale search

00:00:06,390 --> 00:00:10,069
powered by deep learning and Python

00:00:13,820 --> 00:00:20,699
thanks a lot I will be talking about

00:00:17,699 --> 00:00:23,310
Clarion bearings which is our system

00:00:20,699 --> 00:00:26,340
which we have developed at clicks which

00:00:23,310 --> 00:00:29,849
does use deep learning and the system is

00:00:26,340 --> 00:00:32,040
entirely built invitin a bit about

00:00:29,849 --> 00:00:34,260
myself I'm a software engineer and

00:00:32,040 --> 00:00:35,880
research at clicks I have a background

00:00:34,260 --> 00:00:39,210
in computer science and natural and it's

00:00:35,880 --> 00:00:40,770
recessing and deep learning we are

00:00:39,210 --> 00:00:43,410
building a web search engine which is

00:00:40,770 --> 00:00:46,079
part of a web browser and also the

00:00:43,410 --> 00:00:48,600
browser works on mobile to the areas

00:00:46,079 --> 00:00:51,660
that interest me our NLP information

00:00:48,600 --> 00:00:55,770
retrieval deep learning and I also am

00:00:51,660 --> 00:00:58,710
Mozilla representative since 2012 so

00:00:55,770 --> 00:01:01,590
about clicks we are based in Munich it's

00:00:58,710 --> 00:01:03,539
majority owned by Herbert Buda media we

00:01:01,590 --> 00:01:05,939
are international team of 90 experts

00:01:03,539 --> 00:01:08,760
from 28 different countries and we

00:01:05,939 --> 00:01:11,159
combined the power of data search and

00:01:08,760 --> 00:01:14,340
browsers so that we are redefining the

00:01:11,159 --> 00:01:16,409
browsing experience website as clicks

00:01:14,340 --> 00:01:18,840
calm and you can actually like check out

00:01:16,409 --> 00:01:21,060
our browsers so here I'm talking about

00:01:18,840 --> 00:01:23,400
search so I'll start with it so search

00:01:21,060 --> 00:01:24,930
up clicks looks something like this so

00:01:23,400 --> 00:01:27,630
when you open your web browser what you

00:01:24,930 --> 00:01:29,790
usually do is you go for a link or you

00:01:27,630 --> 00:01:32,549
go for a search term what clicks

00:01:29,790 --> 00:01:34,470
experience gives you is a web browser

00:01:32,549 --> 00:01:36,390
with the nitrous file which is

00:01:34,470 --> 00:01:38,820
intelligent enough to directly give you

00:01:36,390 --> 00:01:39,960
the site based on what your couriers so

00:01:38,820 --> 00:01:42,990
say if you're searching for something

00:01:39,960 --> 00:01:44,399
like Python wiki it you will get a

00:01:42,990 --> 00:01:46,560
Python wiki link if you want to search

00:01:44,399 --> 00:01:48,479
for like weather in Bilbao you'll get

00:01:46,560 --> 00:01:50,490
like the weather snippet and

00:01:48,479 --> 00:01:52,740
interestingly I found out that on Monday

00:01:50,490 --> 00:01:55,500
that's today it's 41 degrees so take

00:01:52,740 --> 00:01:57,299
care and of course if you want to search

00:01:55,500 --> 00:01:59,960
for news you'll get like real-time news

00:01:57,299 --> 00:02:03,360
so it's a combination of a lot of data

00:01:59,960 --> 00:02:05,310
built into a browser with like the

00:02:03,360 --> 00:02:09,030
technology of search behind it so it's

00:02:05,310 --> 00:02:10,709
all three things combined so a bit

00:02:09,030 --> 00:02:13,470
historically about how traditional

00:02:10,709 --> 00:02:16,470
search works so traditionally

00:02:13,470 --> 00:02:19,110
so search is a very long studied problem

00:02:16,470 --> 00:02:22,050
and by search I mean information

00:02:19,110 --> 00:02:24,120
retrieval of the web search and what

00:02:22,050 --> 00:02:26,070
they used to come up with was create a

00:02:24,120 --> 00:02:28,620
vector model of your documents and your

00:02:26,070 --> 00:02:32,040
query and then to a match at real time

00:02:28,620 --> 00:02:34,110
and the aim of the whole process was to

00:02:32,040 --> 00:02:36,600
come up with like the best URLs are the

00:02:34,110 --> 00:02:38,550
best documents for the user query over

00:02:36,600 --> 00:02:39,420
the time what we found out like search

00:02:38,550 --> 00:02:41,880
engines of world

00:02:39,420 --> 00:02:44,160
there was the web became rich the depth

00:02:41,880 --> 00:02:48,200
2.0 there was a lot of media which came

00:02:44,160 --> 00:02:52,020
in and people expected more from the web

00:02:48,200 --> 00:02:54,720
to come up with a third-story search our

00:02:52,020 --> 00:02:56,750
clicks is based on to match use the

00:02:54,720 --> 00:03:00,240
query where the query in our index and

00:02:56,750 --> 00:03:03,210
our index is based on query logs so if

00:03:00,240 --> 00:03:07,230
you type Facebook or FB it has to go to

00:03:03,210 --> 00:03:08,910
facebook.com given such an index you can

00:03:07,230 --> 00:03:12,300
actually construct a much more

00:03:08,910 --> 00:03:14,400
meaningful search result experience for

00:03:12,300 --> 00:03:16,080
the user because it's enriched by how

00:03:14,400 --> 00:03:19,440
many times people actually query and

00:03:16,080 --> 00:03:21,840
lead to the same page so what we aim is

00:03:19,440 --> 00:03:24,030
we construct alternative queries given a

00:03:21,840 --> 00:03:25,800
user query so if we find it directly

00:03:24,030 --> 00:03:28,140
it's great but if something which is

00:03:25,800 --> 00:03:30,600
which is different or we have not seen

00:03:28,140 --> 00:03:32,670
before we will try to construct them at

00:03:30,600 --> 00:03:35,700
runtime and try to search for those

00:03:32,670 --> 00:03:37,830
results in our index and broadly our

00:03:35,700 --> 00:03:40,680
index looks something like this so you

00:03:37,830 --> 00:03:43,019
have a query and it has your IDs which

00:03:40,680 --> 00:03:46,830
means you are ID is linked to some hash

00:03:43,019 --> 00:03:48,959
in value and that URL is the actual URL

00:03:46,830 --> 00:03:50,790
that people go to given the query and

00:03:48,959 --> 00:03:52,830
there are these frequency counts and

00:03:50,790 --> 00:03:54,930
everything which actually allows us to

00:03:52,830 --> 00:03:56,940
make a prediction ok this page is the

00:03:54,930 --> 00:04:00,239
rightful page that the user actually

00:03:56,940 --> 00:04:02,100
intended to give an overview of the

00:04:00,239 --> 00:04:05,640
search problem itself in a bit more

00:04:02,100 --> 00:04:08,580
depth the search problem can actually be

00:04:05,640 --> 00:04:10,769
seen as a two-step process first one is

00:04:08,580 --> 00:04:12,630
recall the second one is ranking so

00:04:10,769 --> 00:04:15,269
given your index of like billions of

00:04:12,630 --> 00:04:17,760
pages what you try to aim at is like get

00:04:15,269 --> 00:04:19,890
the most best set of candidate pages

00:04:17,760 --> 00:04:22,229
that you can say okay given a user query

00:04:19,890 --> 00:04:25,200
that should correspond to them so say I

00:04:22,229 --> 00:04:26,750
say I want to get the 10,000 pages a

00:04:25,200 --> 00:04:29,240
10,000 URLs

00:04:26,750 --> 00:04:31,520
billions of pages which best fit the

00:04:29,240 --> 00:04:33,380
query and then the problem comes up is

00:04:31,520 --> 00:04:36,620
the ranking problem the ranking problem

00:04:33,380 --> 00:04:39,440
means given these 10,000 pages what we

00:04:36,620 --> 00:04:42,200
want is give me the top ten hundred

00:04:39,440 --> 00:04:44,900
three results as you might know that

00:04:42,200 --> 00:04:46,220
given any search engine result page the

00:04:44,900 --> 00:04:48,260
second page is a dead page

00:04:46,220 --> 00:04:49,970
so everybody concerns about the first

00:04:48,260 --> 00:04:51,860
page so it's very important to have the

00:04:49,970 --> 00:04:54,050
top five or top three results as the

00:04:51,860 --> 00:04:56,690
best result for your query and that's

00:04:54,050 --> 00:04:59,360
all we care about Alex Alex what we want

00:04:56,690 --> 00:05:01,280
is like given a user query we try to

00:04:59,360 --> 00:05:06,290
come up with three best results from our

00:05:01,280 --> 00:05:09,950
two billion pages in the index so where

00:05:06,290 --> 00:05:13,910
does deep learning come up so what we

00:05:09,950 --> 00:05:16,010
aim at clicks is like we are trying a

00:05:13,910 --> 00:05:17,660
traditional method of search using fuzzy

00:05:16,010 --> 00:05:20,000
matching the words in the query to a

00:05:17,660 --> 00:05:21,530
document but then we are also use butyl

00:05:20,000 --> 00:05:23,900
eyes in something which is a bit deeper

00:05:21,530 --> 00:05:25,460
and a bit different which is using

00:05:23,900 --> 00:05:28,760
something called semantic vectors or

00:05:25,460 --> 00:05:30,740
distributed representation of words what

00:05:28,760 --> 00:05:33,140
we actually try to do is really present

00:05:30,740 --> 00:05:36,460
our queries as witness so a vector is

00:05:33,140 --> 00:05:41,270
like a fixed dimensional floating point

00:05:36,460 --> 00:05:44,390
list of numbers and what we try to do is

00:05:41,270 --> 00:05:46,460
given a query and given a vector that

00:05:44,390 --> 00:05:50,240
vector should semantically understand

00:05:46,460 --> 00:05:52,220
the meaning of the query this particular

00:05:50,240 --> 00:05:53,930
thing is called distributed

00:05:52,220 --> 00:05:56,090
representation where the words which are

00:05:53,930 --> 00:05:58,700
here in the same context share semantic

00:05:56,090 --> 00:06:00,919
meaning and the meaning of the query is

00:05:58,700 --> 00:06:02,960
defined by this vector these query

00:06:00,919 --> 00:06:05,660
vectors are learned in an unsupervised

00:06:02,960 --> 00:06:07,460
yet supervised manner where we focus on

00:06:05,660 --> 00:06:09,919
the context of the words in the

00:06:07,460 --> 00:06:12,470
sentences or the queries and learn the

00:06:09,919 --> 00:06:13,790
same and the area that we actually study

00:06:12,470 --> 00:06:16,430
this thing is called neuron parabolas

00:06:13,790 --> 00:06:18,860
take language model similarity between

00:06:16,430 --> 00:06:20,990
these queries is measured as a cosine

00:06:18,860 --> 00:06:23,060
distance between two vectors so if two

00:06:20,990 --> 00:06:26,090
vectors are close together in the vector

00:06:23,060 --> 00:06:29,030
space so they are more similar and hence

00:06:26,090 --> 00:06:31,280
what we do is that we try to get the

00:06:29,030 --> 00:06:33,650
closest queries based on how which are

00:06:31,280 --> 00:06:35,990
the closest vectors in space are to the

00:06:33,650 --> 00:06:38,270
user query vector and this gives us a

00:06:35,990 --> 00:06:40,400
recall set or the first set that we are

00:06:38,270 --> 00:06:43,610
actually fetched from our index we

00:06:40,400 --> 00:06:47,240
most accurately correspond to our user

00:06:43,610 --> 00:06:49,280
query so a simple example of to

00:06:47,240 --> 00:06:52,400
illustrate this there's like say a user

00:06:49,280 --> 00:06:55,190
types a simple query like Sims game PC

00:06:52,400 --> 00:06:57,830
download which is a game what our system

00:06:55,190 --> 00:06:59,660
actually gives us is sort of a list of

00:06:57,830 --> 00:07:01,699
these queries along with the cosine

00:06:59,660 --> 00:07:06,050
distance to the query vector that user

00:07:01,699 --> 00:07:08,690
typed so given the gray Sims game PC

00:07:06,050 --> 00:07:10,910
download we get like sort of a sorted

00:07:08,690 --> 00:07:14,150
list where the first one is like the

00:07:10,910 --> 00:07:18,020
most closest to two Sims game PC

00:07:14,150 --> 00:07:19,370
download bear in mind like it's it's a

00:07:18,020 --> 00:07:21,979
bit different to understand because

00:07:19,370 --> 00:07:23,810
you're not doing a word toward much but

00:07:21,979 --> 00:07:26,419
a vector 2 vector match so the vector

00:07:23,810 --> 00:07:28,490
for the query saves game PC to Sims game

00:07:26,419 --> 00:07:30,590
PC download is much closer to to

00:07:28,490 --> 00:07:31,849
download game PC Sims now this is coming

00:07:30,590 --> 00:07:34,880
from our search back end which is

00:07:31,849 --> 00:07:36,349
bag-of-words because we want to optimize

00:07:34,880 --> 00:07:38,960
the space as well

00:07:36,349 --> 00:07:41,840
so eventually the vector comes out to be

00:07:38,960 --> 00:07:43,639
the same and the values on the right are

00:07:41,840 --> 00:07:45,380
the cosine distances so as we move down

00:07:43,639 --> 00:07:48,320
the cosine distance increases and we'll

00:07:45,380 --> 00:07:50,300
see like you'll start getting some a bit

00:07:48,320 --> 00:07:52,639
far off results so we are usually

00:07:50,300 --> 00:07:56,360
concerned about like top 50 closest

00:07:52,639 --> 00:07:59,389
queries that come through this system so

00:07:56,360 --> 00:08:01,340
a bit more about how this learning

00:07:59,389 --> 00:08:03,830
process works and what what we actually

00:08:01,340 --> 00:08:06,050
utilizing in production is we use

00:08:03,830 --> 00:08:07,849
something called an unsupervised

00:08:06,050 --> 00:08:09,979
learning technique to learn these word

00:08:07,849 --> 00:08:12,590
representation so effectively what we

00:08:09,979 --> 00:08:14,570
want to learn is like given the

00:08:12,590 --> 00:08:16,550
continuous representation of the world

00:08:14,570 --> 00:08:20,270
you would like the distance of like two

00:08:16,550 --> 00:08:22,849
words CW and - CW - to reflect a

00:08:20,270 --> 00:08:24,409
meaningful similarity so for example if

00:08:22,849 --> 00:08:27,080
there's a vector like king and you

00:08:24,409 --> 00:08:29,180
subtract that like a vector of man and

00:08:27,080 --> 00:08:30,590
then you add a vector of woman you'd

00:08:29,180 --> 00:08:33,890
probably get a vector which is close to

00:08:30,590 --> 00:08:37,580
vector of Queen and the algorithm that

00:08:33,890 --> 00:08:39,169
defines this is were to act and we learn

00:08:37,580 --> 00:08:42,529
this representation and the

00:08:39,169 --> 00:08:44,839
corresponding vectors so a bit more

00:08:42,529 --> 00:08:47,660
about word to back it was actually given

00:08:44,839 --> 00:08:49,400
by Michelob in 2013 where he had two

00:08:47,660 --> 00:08:51,380
different models where continuous

00:08:49,400 --> 00:08:53,630
barbers representation and continued

00:08:51,380 --> 00:08:56,209
escaped remodel the

00:08:53,630 --> 00:08:57,560
we focus on again distributed

00:08:56,209 --> 00:09:00,019
representations that are learned by

00:08:57,560 --> 00:09:01,759
neural networks both models are trained

00:09:00,019 --> 00:09:05,630
using stochastic gradient descent and

00:09:01,759 --> 00:09:08,779
back propagation a bit more a visual

00:09:05,630 --> 00:09:11,060
indication of how this works is like in

00:09:08,779 --> 00:09:13,639
a civil or continuous bag of words model

00:09:11,060 --> 00:09:16,370
on the Left we have like a context words

00:09:13,639 --> 00:09:18,699
of five words say and we want to try to

00:09:16,370 --> 00:09:23,120
predict the center word so given like

00:09:18,699 --> 00:09:25,699
the cat sat on mat the word sat has to

00:09:23,120 --> 00:09:28,490
be projected given the other context

00:09:25,699 --> 00:09:30,649
words and the Skip ground model does the

00:09:28,490 --> 00:09:32,810
exact reverse so given the center word

00:09:30,649 --> 00:09:35,240
in the sentence or a context window you

00:09:32,810 --> 00:09:37,069
try to predict the surrounding words so

00:09:35,240 --> 00:09:39,920
given these two models you can actually

00:09:37,069 --> 00:09:41,839
like define these vectors for each word

00:09:39,920 --> 00:09:44,600
that you see as a lookup table and you

00:09:41,839 --> 00:09:48,649
can learn them using stochastic gradient

00:09:44,600 --> 00:09:51,759
descent I'll probably skip this because

00:09:48,649 --> 00:09:55,670
this has a lot of math in it but still

00:09:51,759 --> 00:09:57,259
so what we try to optimize is a noodle

00:09:55,670 --> 00:10:00,589
Prophecy language model tries to

00:09:57,259 --> 00:10:02,420
optimize given how many times you'll see

00:10:00,589 --> 00:10:05,480
a particular word given the context and

00:10:02,420 --> 00:10:08,180
given how many times you see a word not

00:10:05,480 --> 00:10:10,100
in its context so a best language model

00:10:08,180 --> 00:10:11,600
will actually say okay given a certain

00:10:10,100 --> 00:10:13,430
sequence of words you'll see the next

00:10:11,600 --> 00:10:15,380
word and given a certain sequence of

00:10:13,430 --> 00:10:17,060
word you will not see a certain word and

00:10:15,380 --> 00:10:21,769
that's where the model actually learns

00:10:17,060 --> 00:10:23,089
and this is one other example of how our

00:10:21,769 --> 00:10:25,370
traditional language model actually

00:10:23,089 --> 00:10:27,380
works so for example this the cat sits

00:10:25,370 --> 00:10:29,389
on the mat you try to predict what is

00:10:27,380 --> 00:10:32,240
the probability of Matt coming after the

00:10:29,389 --> 00:10:36,079
sequence in a certain vocabulary

00:10:32,240 --> 00:10:39,350
dictionary that you have but the only

00:10:36,079 --> 00:10:41,420
cash here that we have to worry about is

00:10:39,350 --> 00:10:43,760
like your vocabulary could be very very

00:10:41,420 --> 00:10:45,500
huge so what you might look at is like

00:10:43,760 --> 00:10:48,230
you want to try to predict a probability

00:10:45,500 --> 00:10:50,120
of a word say you have seven to ten

00:10:48,230 --> 00:10:51,649
million words in your vocabulary you

00:10:50,120 --> 00:10:55,220
want to predict for the ability of your

00:10:51,649 --> 00:10:57,680
one single word across all of them so to

00:10:55,220 --> 00:11:00,230
avoid this scheme what we use something

00:10:57,680 --> 00:11:02,209
called noise contrast of estimation we

00:11:00,230 --> 00:11:05,449
actually don't use the entire vocabulary

00:11:02,209 --> 00:11:07,370
to test our word against what we do is

00:11:05,449 --> 00:11:10,490
like we say okay we pick a

00:11:07,370 --> 00:11:12,890
five noisy words are ten noisy words so

00:11:10,490 --> 00:11:15,730
for this particular sequence a cat sits

00:11:12,890 --> 00:11:19,130
on the mat you're pretty much sure that

00:11:15,730 --> 00:11:21,680
the mat is the right word but so can be

00:11:19,130 --> 00:11:24,529
other words but then say the cat sits on

00:11:21,680 --> 00:11:27,350
the hair or something like that so these

00:11:24,529 --> 00:11:29,120
words will not be the exact sequence

00:11:27,350 --> 00:11:31,370
that you will find in day-to-day life

00:11:29,120 --> 00:11:34,010
and you can pick those words at random

00:11:31,370 --> 00:11:36,110
from a uniform distribution and get

00:11:34,010 --> 00:11:36,860
these noisy words as your training

00:11:36,110 --> 00:11:38,480
examples

00:11:36,860 --> 00:11:41,060
so what effective your model learns

00:11:38,480 --> 00:11:44,089
right now given the sequence what is the

00:11:41,060 --> 00:11:45,920
right word to get next as an X word and

00:11:44,089 --> 00:11:47,510
given the sequence which are not the

00:11:45,920 --> 00:11:49,250
right words so if the system

00:11:47,510 --> 00:11:51,650
differentiates is over and over again

00:11:49,250 --> 00:11:53,360
with millions of examples and you train

00:11:51,650 --> 00:11:55,279
this over certain iterations

00:11:53,360 --> 00:11:57,560
you'll probably get a model which is

00:11:55,279 --> 00:11:59,630
able to differentiate the position of

00:11:57,560 --> 00:12:01,430
the right words with the position of the

00:11:59,630 --> 00:12:07,730
bad words separated what will the clear

00:12:01,430 --> 00:12:10,250
distance so let's see like how this will

00:12:07,730 --> 00:12:12,410
work with an example itself so for

00:12:10,250 --> 00:12:14,510
example there is a document like the

00:12:12,410 --> 00:12:17,680
quick brown fox jumped over the lazy dog

00:12:14,510 --> 00:12:21,410
and we have a context window size of one

00:12:17,680 --> 00:12:23,570
we say okay given like the first three

00:12:21,410 --> 00:12:25,610
words let quick brown fox I have the

00:12:23,570 --> 00:12:28,790
center word quick and the surrounding

00:12:25,610 --> 00:12:31,160
words as the and brown so I want to get

00:12:28,790 --> 00:12:33,140
in a cantina slava Ford in my model what

00:12:31,160 --> 00:12:35,750
I want is like can you predict quick

00:12:33,140 --> 00:12:38,630
based on the and brown so it's just like

00:12:35,750 --> 00:12:40,160
a very simple example but at production

00:12:38,630 --> 00:12:42,140
we found like skip drum does much better

00:12:40,160 --> 00:12:45,260
so effectively what we try to find out

00:12:42,140 --> 00:12:47,900
is like we try to predict the context

00:12:45,260 --> 00:12:50,630
words from a target word so we predict

00:12:47,900 --> 00:12:52,730
the and brown from quick so given quick

00:12:50,630 --> 00:12:54,950
predict what is the probability of the

00:12:52,730 --> 00:12:57,260
predict what is the probability of brown

00:12:54,950 --> 00:12:58,850
and the objective function is defined

00:12:57,260 --> 00:13:01,160
over the entire data set so whatever

00:12:58,850 --> 00:13:04,100
data set we have our data set is built

00:13:01,160 --> 00:13:05,990
on a lot of Wikipedia radar a lot of

00:13:04,100 --> 00:13:08,000
query data title descriptions that we

00:13:05,990 --> 00:13:10,700
have and all other textual data that we

00:13:08,000 --> 00:13:12,230
have to actually learn how the queries

00:13:10,700 --> 00:13:14,480
are formed or how sentences are formed

00:13:12,230 --> 00:13:18,320
or what is the sequence of these words

00:13:14,480 --> 00:13:20,150
and we use as Judy for this say a train

00:13:18,320 --> 00:13:23,000
time T we have like a certain case

00:13:20,150 --> 00:13:26,120
they have quick anda and our goal is to

00:13:23,000 --> 00:13:29,390
predict the from quick so we select like

00:13:26,120 --> 00:13:31,730
some noise examples say like same number

00:13:29,390 --> 00:13:35,480
nose is like 1 and we say sheep sheep

00:13:31,730 --> 00:13:37,400
should not be like part of this so next

00:13:35,480 --> 00:13:40,190
we compute a loss for this pair of

00:13:37,400 --> 00:13:42,410
observers and noise examples and we get

00:13:40,190 --> 00:13:45,410
this objective function so what we try

00:13:42,410 --> 00:13:48,800
to do is given this which is like a lot

00:13:45,410 --> 00:13:50,990
of the value of the score so given the

00:13:48,800 --> 00:13:53,150
probability which is the correct send

00:13:50,990 --> 00:13:55,550
collect piece of sentence a correct

00:13:53,150 --> 00:13:57,860
piece of context though and Q should be

00:13:55,550 --> 00:14:01,040
given a score of 1 and given like a

00:13:57,860 --> 00:14:03,800
quick and sheep this code should be 0 so

00:14:01,040 --> 00:14:06,380
if you update the value of theta because

00:14:03,800 --> 00:14:08,360
that depends on it we can maximize this

00:14:06,380 --> 00:14:11,630
objective function as like a logistic

00:14:08,360 --> 00:14:13,490
like log likelihood and we can actually

00:14:11,630 --> 00:14:14,750
do a gradient descent on top of it

00:14:13,490 --> 00:14:16,370
so we perform an update on the

00:14:14,750 --> 00:14:18,020
embeddings and we repeat this process

00:14:16,370 --> 00:14:20,150
over and over again for different

00:14:18,020 --> 00:14:22,520
examples over the entire corpus and we

00:14:20,150 --> 00:14:25,340
come up with like a lookup table for

00:14:22,520 --> 00:14:27,590
verts and a vectors so we can define the

00:14:25,340 --> 00:14:29,750
dimensionality of a vector as I said in

00:14:27,590 --> 00:14:31,700
my slide that we use hundred as

00:14:29,750 --> 00:14:34,670
dimensions to represent that word and

00:14:31,700 --> 00:14:36,920
does pretty well for us so how do these

00:14:34,670 --> 00:14:38,570
word and bearings actually look like of

00:14:36,920 --> 00:14:40,490
what we have actually don't there is

00:14:38,570 --> 00:14:43,220
something like this so if you if you see

00:14:40,490 --> 00:14:45,080
like the word vectors or like you

00:14:43,220 --> 00:14:47,300
project these vectors in space what you

00:14:45,080 --> 00:14:50,450
find is like the vector for man and

00:14:47,300 --> 00:14:53,540
women is roughly equidistant from like

00:14:50,450 --> 00:14:55,550
king and queen and you will find this

00:14:53,540 --> 00:14:58,250
not just variation in gender but also

00:14:55,550 --> 00:15:00,650
variation like work dense like walking

00:14:58,250 --> 00:15:03,650
and walked and swimming and swam because

00:15:00,650 --> 00:15:05,780
you might have sentences where like the

00:15:03,650 --> 00:15:08,990
guy or the person is walking and a

00:15:05,780 --> 00:15:13,070
person is running he walks or he runs

00:15:08,990 --> 00:15:14,690
would occur in the same context and this

00:15:13,070 --> 00:15:16,940
is what the model actually caps really

00:15:14,690 --> 00:15:18,920
nicely and not just that we actually

00:15:16,940 --> 00:15:20,540
also have like some other informational

00:15:18,920 --> 00:15:24,080
features like countries and capitals

00:15:20,540 --> 00:15:25,850
like Spain and Madrid or Italy and Rome

00:15:24,080 --> 00:15:28,610
Germany and Berlin so these are like

00:15:25,850 --> 00:15:31,460
country capture relationships this is

00:15:28,610 --> 00:15:34,320
like a projection on a 2d scale using

00:15:31,460 --> 00:15:37,200
t-sne where you actually can see

00:15:34,320 --> 00:15:38,580
I mean it's the shot but you can

00:15:37,200 --> 00:15:42,030
actually see like some characters here

00:15:38,580 --> 00:15:45,240
at the bottom and here on the top you'll

00:15:42,030 --> 00:15:46,800
have like measured board some here like

00:15:45,240 --> 00:15:48,840
more less some more objective

00:15:46,800 --> 00:15:50,430
identifiers and this is like a

00:15:48,840 --> 00:15:53,850
projection that you can see if you see

00:15:50,430 --> 00:15:56,130
they're more semantically meaning words

00:15:53,850 --> 00:15:57,480
are actually closer in vector space and

00:15:56,130 --> 00:15:59,400
this is a very important property

00:15:57,480 --> 00:16:01,950
because if you can try to leverage this

00:15:59,400 --> 00:16:02,760
and construct like sentence or document

00:16:01,950 --> 00:16:04,830
representation

00:16:02,760 --> 00:16:06,750
you'll probably get like a similar

00:16:04,830 --> 00:16:09,750
documents in space as well and that is

00:16:06,750 --> 00:16:11,130
what query embeddings addresses so the

00:16:09,750 --> 00:16:13,020
way we generate a play vector using

00:16:11,130 --> 00:16:15,330
these word vectors is like for the same

00:16:13,020 --> 00:16:17,910
query since game PC download we have a

00:16:15,330 --> 00:16:20,190
vector for each of these words what we

00:16:17,910 --> 00:16:23,010
do is like we just don't use these were

00:16:20,190 --> 00:16:26,550
directors as they get the term relevance

00:16:23,010 --> 00:16:28,470
and term relevance for us is a bit sort

00:16:26,550 --> 00:16:30,600
of a custom classes that we come up with

00:16:28,470 --> 00:16:33,240
but actually what you see is like you'll

00:16:30,600 --> 00:16:35,940
get a score for each term in the in the

00:16:33,240 --> 00:16:38,220
query so this tells us like Sims is the

00:16:35,940 --> 00:16:40,020
most important relevant word in the

00:16:38,220 --> 00:16:43,410
query because it's the name or the name

00:16:40,020 --> 00:16:47,070
identifier and next week what we do is

00:16:43,410 --> 00:16:49,380
we use this term relevance and also a

00:16:47,070 --> 00:16:53,220
vector to calculate like a pair a vector

00:16:49,380 --> 00:16:55,440
average of these of these vectors so

00:16:53,220 --> 00:16:57,840
what a weighted average actually means

00:16:55,440 --> 00:16:59,550
is like say given two vectors of two

00:16:57,840 --> 00:17:01,470
different words and their weights or

00:16:59,550 --> 00:17:03,330
their term relevance you do an umpire

00:17:01,470 --> 00:17:05,070
average and you'll get like an average

00:17:03,330 --> 00:17:07,920
representation of those words and

00:17:05,070 --> 00:17:09,440
effectively what we actually say our

00:17:07,920 --> 00:17:11,820
query vector is is this average

00:17:09,440 --> 00:17:13,320
representation so given our vector and

00:17:11,820 --> 00:17:15,420
the term relevance we get like this

00:17:13,320 --> 00:17:17,580
average representation and this

00:17:15,420 --> 00:17:19,830
represents our query vectors effectively

00:17:17,580 --> 00:17:22,110
at the end same scan PC download is

00:17:19,830 --> 00:17:24,510
nothing but this hundred dimensional

00:17:22,110 --> 00:17:28,980
vector and that is what we use as our

00:17:24,510 --> 00:17:30,630
query vector a bit about term relevance

00:17:28,980 --> 00:17:33,720
so we have two different modes of term

00:17:30,630 --> 00:17:35,490
relevance usually it is the frequency of

00:17:33,720 --> 00:17:38,850
the words that you find but it's not

00:17:35,490 --> 00:17:40,920
very good for scale also likes you use

00:17:38,850 --> 00:17:44,100
something like tf-idf or these are

00:17:40,920 --> 00:17:44,360
representations but what we have used is

00:17:44,100 --> 00:17:46,790
some

00:17:44,360 --> 00:17:49,100
about TF 5 TF is like ha given the

00:17:46,790 --> 00:17:50,780
number of queries linked to a page how

00:17:49,100 --> 00:17:53,150
many times that term has occurred in

00:17:50,780 --> 00:17:55,490
those top five queries and that's a much

00:17:53,150 --> 00:17:58,150
better indication to us given the data

00:17:55,490 --> 00:18:01,820
or the data that we have that we can

00:17:58,150 --> 00:18:04,130
roughly say that yeah given the word

00:18:01,820 --> 00:18:05,540
statistics give me this number and give

00:18:04,130 --> 00:18:06,650
me the document frequency I'll get

00:18:05,540 --> 00:18:08,929
something like an absolute term

00:18:06,650 --> 00:18:11,570
relevance and the relative one is

00:18:08,929 --> 00:18:12,980
actually sort of a normalization on the

00:18:11,570 --> 00:18:15,320
all the pages that we have in our index

00:18:12,980 --> 00:18:17,179
what we found out is like if you

00:18:15,320 --> 00:18:19,460
normalize your scores across all the

00:18:17,179 --> 00:18:21,230
pages of your index the vectors are

00:18:19,460 --> 00:18:23,390
slightly better you get slightly better

00:18:21,230 --> 00:18:26,360
results and these all are data dependent

00:18:23,390 --> 00:18:28,760
we compute them on the fly each time we

00:18:26,360 --> 00:18:30,290
refresh our index and for example this

00:18:28,760 --> 00:18:32,000
looks something like this for each word

00:18:30,290 --> 00:18:35,570
you'll have like features like frequency

00:18:32,000 --> 00:18:37,280
dr. in frequency you have uqf and all

00:18:35,570 --> 00:18:41,299
the other stuff and similarly for all

00:18:37,280 --> 00:18:44,330
the other was as well so we what we

00:18:41,299 --> 00:18:45,740
actually create is like a query vector

00:18:44,330 --> 00:18:48,470
index now it's a given a traditional

00:18:45,740 --> 00:18:50,210
index which has all the documents we

00:18:48,470 --> 00:18:53,440
have all the queries and their vectors

00:18:50,210 --> 00:18:56,720
and we try to do a query vector lookup

00:18:53,440 --> 00:18:58,190
so we cannot do this for all the queries

00:18:56,720 --> 00:19:00,919
because there are just too many queries

00:18:58,190 --> 00:19:02,419
so what we found out is like given all

00:19:00,919 --> 00:19:04,330
the pages are indexed we can actually

00:19:02,419 --> 00:19:07,940
just pick the top five queries which

00:19:04,330 --> 00:19:10,700
effectively represent the page and we

00:19:07,940 --> 00:19:12,020
call them as top queries and from the

00:19:10,700 --> 00:19:14,809
page models we can actually get this

00:19:12,020 --> 00:19:17,450
data so roughly we come up with like 465

00:19:14,809 --> 00:19:19,880
million queries which represent all the

00:19:17,450 --> 00:19:22,160
pages on our index and we try to learn a

00:19:19,880 --> 00:19:24,980
query vectors for each one of them and

00:19:22,160 --> 00:19:27,400
if you just like dump the whole system

00:19:24,980 --> 00:19:30,140
on disk it's like around 700 gigs and

00:19:27,400 --> 00:19:32,720
what we actually have the problem now is

00:19:30,140 --> 00:19:34,880
like how do we get similar queries from

00:19:32,720 --> 00:19:38,450
these 465 million queries so given a

00:19:34,880 --> 00:19:43,250
user query find me the closest 50

00:19:38,450 --> 00:19:45,230
queries from this 465 million queries so

00:19:43,250 --> 00:19:47,570
how do we find closest queries should we

00:19:45,230 --> 00:19:49,070
use brute force it's too slow it's too

00:19:47,570 --> 00:19:51,530
too slow we cannot use hashing

00:19:49,070 --> 00:19:53,330
techniques that effectively because it's

00:19:51,530 --> 00:19:55,220
not very accurate for vectors because

00:19:53,330 --> 00:19:56,320
these vectors are semantic even a small

00:19:55,220 --> 00:19:58,659
loss in precision

00:19:56,320 --> 00:20:00,399
could lead to like haywire results so

00:19:58,659 --> 00:20:02,139
what a solution actually required was

00:20:00,399 --> 00:20:04,809
the application of a cosine similarity

00:20:02,139 --> 00:20:06,880
metric somehow he could should have to

00:20:04,809 --> 00:20:09,100
scale for like four hundred sixty five

00:20:06,880 --> 00:20:11,889
million queries and take ten

00:20:09,100 --> 00:20:13,600
milliseconds or less so the way we came

00:20:11,889 --> 00:20:15,730
up did the answer was something called

00:20:13,600 --> 00:20:18,039
approximate nearest neighbor vector

00:20:15,730 --> 00:20:22,330
model and they were actually pretty

00:20:18,039 --> 00:20:24,970
helpful for us so what the model that we

00:20:22,330 --> 00:20:27,210
use is called annoy it is a C++ and

00:20:24,970 --> 00:20:29,740
Python wrapper like that exists for this

00:20:27,210 --> 00:20:31,750
to build the approximate nearest

00:20:29,740 --> 00:20:34,120
neighbor models for all the vectors of

00:20:31,750 --> 00:20:36,519
queries that we have and always actually

00:20:34,120 --> 00:20:39,909
use in production and Spotify and now it

00:20:36,519 --> 00:20:43,059
clicks as well we can train all on the

00:20:39,909 --> 00:20:45,490
465 million documents at once but it's

00:20:43,059 --> 00:20:48,129
too slow because it is sort of memory

00:20:45,490 --> 00:20:50,649
intensive so what we do is like we don't

00:20:48,129 --> 00:20:53,470
train them all of them together we have

00:20:50,649 --> 00:20:55,570
a cluster where we actually host these

00:20:53,470 --> 00:20:57,759
models along with our search index so we

00:20:55,570 --> 00:21:00,039
train them as ten models it's like 46

00:20:57,759 --> 00:21:01,840
million queries each and we train it on

00:21:00,039 --> 00:21:03,909
ten trees well these trees actually mean

00:21:01,840 --> 00:21:06,309
I'll explain next and the size of the

00:21:03,909 --> 00:21:08,620
model is right around 27 gigs per

00:21:06,309 --> 00:21:10,600
Porsche 27 weeks per shot that what you

00:21:08,620 --> 00:21:13,360
get after training which is like around

00:21:10,600 --> 00:21:15,429
270 gigs if you if you just scale it to

00:21:13,360 --> 00:21:17,350
ten models and everything is stored in

00:21:15,429 --> 00:21:19,629
drum because for us the most important

00:21:17,350 --> 00:21:21,029
thing is latency given a search you want

00:21:19,629 --> 00:21:23,649
the results to happen pretty quickly

00:21:21,029 --> 00:21:27,039
later I show a demo of how this thing

00:21:23,649 --> 00:21:28,629
actually is used in production and then

00:21:27,039 --> 00:21:30,129
what at runtime what you try to do is

00:21:28,629 --> 00:21:32,470
like you query all these ten shots

00:21:30,129 --> 00:21:34,210
simultaneously and then sort them based

00:21:32,470 --> 00:21:36,009
on what cosine distance is that you get

00:21:34,210 --> 00:21:37,960
so your different parts of your shots

00:21:36,009 --> 00:21:39,970
might have different closest queries so

00:21:37,960 --> 00:21:42,070
eventually what you'd want is like you

00:21:39,970 --> 00:21:43,990
want is like the best representation of

00:21:42,070 --> 00:21:46,600
those queries which are closely matching

00:21:43,990 --> 00:21:49,450
the user query and where we actually

00:21:46,600 --> 00:21:51,399
found a nice cutoff was like 750 555 is

00:21:49,450 --> 00:21:52,659
a heuristic illunga as how many nearest

00:21:51,399 --> 00:21:55,539
queries would be very good for the

00:21:52,659 --> 00:21:58,419
system that doesn't really like decrease

00:21:55,539 --> 00:21:59,919
our recall or anything or mittens for

00:21:58,419 --> 00:22:04,240
that matter because this has a huge date

00:21:59,919 --> 00:22:06,370
in C costs as well so by by first I want

00:22:04,240 --> 00:22:10,380
to actually explain like how we actually

00:22:06,370 --> 00:22:12,570
use annoy and how an eye actually works

00:22:10,380 --> 00:22:13,830
it's it's one of my stream works that

00:22:12,570 --> 00:22:16,590
you can actually use if you are using

00:22:13,830 --> 00:22:18,480
vector calculus or like using like

00:22:16,590 --> 00:22:22,260
something vector based approaches for

00:22:18,480 --> 00:22:24,360
your recalls or ranking and we try to

00:22:22,260 --> 00:22:26,040
find out the nearest point to any query

00:22:24,360 --> 00:22:28,800
point and like a sub in sublinear time

00:22:26,040 --> 00:22:30,660
so what you try to find out is like you

00:22:28,800 --> 00:22:33,060
cannot do it one by one so it's not all

00:22:30,660 --> 00:22:35,190
n what you want to do is like try to do

00:22:33,060 --> 00:22:37,200
it in subunit I can you get it get those

00:22:35,190 --> 00:22:39,150
closest queries and like log of n time

00:22:37,200 --> 00:22:41,310
and the best case data structure for

00:22:39,150 --> 00:22:43,080
that is a tree so given like all your

00:22:41,310 --> 00:22:44,670
query vectors are represented by like

00:22:43,080 --> 00:22:47,220
each point represents a single query

00:22:44,670 --> 00:22:50,820
what you try to find out is like say

00:22:47,220 --> 00:22:53,220
given a certain point which is the

00:22:50,820 --> 00:22:55,290
nearest point or like a user vector

00:22:53,220 --> 00:22:57,420
which is like a user query that is some

00:22:55,290 --> 00:22:59,460
random point on the state space find

00:22:57,420 --> 00:23:01,890
with the nearest ones so to train that

00:22:59,460 --> 00:23:04,020
model first to build that tree what you

00:23:01,890 --> 00:23:07,560
do is like you just split this this type

00:23:04,020 --> 00:23:09,510
of space recursively so use split take

00:23:07,560 --> 00:23:11,010
two points at random and split the space

00:23:09,510 --> 00:23:13,560
you do it again

00:23:11,010 --> 00:23:16,740
and then you get something like a tree

00:23:13,560 --> 00:23:17,970
so you have like a certain segmentation

00:23:16,740 --> 00:23:20,010
or certain number of points in the

00:23:17,970 --> 00:23:22,470
cluster which are like in different

00:23:20,010 --> 00:23:25,590
parts of the tree you keep splitting and

00:23:22,470 --> 00:23:27,810
you end up with a huge binary tree the

00:23:25,590 --> 00:23:29,430
nice point about this binary tree is

00:23:27,810 --> 00:23:32,100
like the points that are close to each

00:23:29,430 --> 00:23:34,020
other in space are more likely to be

00:23:32,100 --> 00:23:36,630
close to each other in the tree itself

00:23:34,020 --> 00:23:38,790
so if you are trying to navigate through

00:23:36,630 --> 00:23:41,490
a node and you try to come up with like

00:23:38,790 --> 00:23:44,280
sunshine nodes that holds track or that

00:23:41,490 --> 00:23:46,320
whole branch would be composed of all

00:23:44,280 --> 00:23:49,710
the similar nodes in the vector space

00:23:46,320 --> 00:23:52,170
and this is a very important feature so

00:23:49,710 --> 00:23:54,720
how do we search for a point and the in

00:23:52,170 --> 00:23:58,710
the tree and these splits that we have

00:23:54,720 --> 00:24:01,020
built so say that X the red X is our

00:23:58,710 --> 00:24:02,010
like user query vector and we try to

00:24:01,020 --> 00:24:04,290
find out at which are the nearest

00:24:02,010 --> 00:24:06,930
vectors to this particular vector and

00:24:04,290 --> 00:24:08,640
give me the queries related to it so

00:24:06,930 --> 00:24:10,830
what you do is like you end up with like

00:24:08,640 --> 00:24:13,130
when you search for a point and you just

00:24:10,830 --> 00:24:16,260
jot down the path from the binary tree

00:24:13,130 --> 00:24:18,360
and you will get like these okay seven

00:24:16,260 --> 00:24:20,310
neighbors that you get and you used like

00:24:18,360 --> 00:24:23,340
a cosine metrics of how close it is if

00:24:20,310 --> 00:24:25,710
it's very close to like between zero and

00:24:23,340 --> 00:24:27,480
five it's much much more closer if it's

00:24:25,710 --> 00:24:30,000
more than one Lucas cause ANCA takes

00:24:27,480 --> 00:24:31,380
values between -2 and 2 so then you've

00:24:30,000 --> 00:24:34,950
actually decided okay how close your

00:24:31,380 --> 00:24:36,720
vector is but here what the problem is

00:24:34,950 --> 00:24:38,640
like you only feel like 7 neighbors

00:24:36,720 --> 00:24:40,680
coming what if we want more neighbors

00:24:38,640 --> 00:24:43,020
what if we want more than 7 closes

00:24:40,680 --> 00:24:45,090
queries so what we'd use is something

00:24:43,020 --> 00:24:46,380
called we don't just you navigate to one

00:24:45,090 --> 00:24:47,820
branch of the tree you can actually

00:24:46,380 --> 00:24:51,060
navigate to the second branch of the

00:24:47,820 --> 00:24:52,890
tree and this is maintain and sort of a

00:24:51,060 --> 00:24:54,720
priority queue and we can actually

00:24:52,890 --> 00:24:56,790
travel for the both the parts of the

00:24:54,720 --> 00:24:59,610
tree and get like these closest vectors

00:24:56,790 --> 00:25:01,290
and so you don't not only like look at

00:24:59,610 --> 00:25:03,270
the right with a light blue part but

00:25:01,290 --> 00:25:04,830
also like a slightly dark of you pod so

00:25:03,270 --> 00:25:06,450
you see both the sides of the tree

00:25:04,830 --> 00:25:10,170
because that's where the split up is and

00:25:06,450 --> 00:25:12,390
you can find okay both of these sort of

00:25:10,170 --> 00:25:16,410
areas and hyperspace are like closer to

00:25:12,390 --> 00:25:19,230
the user vector but sometimes you'll

00:25:16,410 --> 00:25:21,270
find like because we did it randomly

00:25:19,230 --> 00:25:23,280
what happens is like you can actually

00:25:21,270 --> 00:25:24,810
miss out on some nice zones because you

00:25:23,280 --> 00:25:27,450
just split across two different points

00:25:24,810 --> 00:25:29,700
so what you do is like to minimize this

00:25:27,450 --> 00:25:31,620
you train a forest of trees and it

00:25:29,700 --> 00:25:34,170
actually looks something like this

00:25:31,620 --> 00:25:38,460
so you not only like train on like a

00:25:34,170 --> 00:25:41,040
certain sequence of uh splits but you

00:25:38,460 --> 00:25:42,960
randomized those across say ten trees so

00:25:41,040 --> 00:25:45,090
effectively your model learns these ten

00:25:42,960 --> 00:25:47,400
configurations at once and searches for

00:25:45,090 --> 00:25:48,600
them in real time in value and this

00:25:47,400 --> 00:25:49,920
gives you like a pretty good

00:25:48,600 --> 00:25:51,390
representation because when you sort

00:25:49,920 --> 00:25:53,490
them and get like good query

00:25:51,390 --> 00:25:56,430
representations you'll get like some

00:25:53,490 --> 00:26:01,500
good similarity between queries so

00:25:56,430 --> 00:26:03,240
between a forest of trees so one bad or

00:26:01,500 --> 00:26:05,970
like a missing feature and annoy or like

00:26:03,240 --> 00:26:08,370
maybe it's a feature not a bug is like

00:26:05,970 --> 00:26:09,900
it doesn't let you store string values

00:26:08,370 --> 00:26:12,360
but it actually allows you to store

00:26:09,900 --> 00:26:15,390
indexes so you can actually store like

00:26:12,360 --> 00:26:17,820
for aquariums game PC download given

00:26:15,390 --> 00:26:20,070
this like a unique and like say like 501

00:26:17,820 --> 00:26:21,780
and that one will be stored with the

00:26:20,070 --> 00:26:23,250
vector and that model will be learned so

00:26:21,780 --> 00:26:25,230
when your credit annoy you'll get like

00:26:23,250 --> 00:26:28,230
an index back of all the indexes which

00:26:25,230 --> 00:26:30,000
are close to it so what we what we have

00:26:28,230 --> 00:26:31,890
our clicks is like we have developed a

00:26:30,000 --> 00:26:34,020
system called key v4 which is like a key

00:26:31,890 --> 00:26:35,850
value index which is also responsible

00:26:34,020 --> 00:26:39,000
for our entire search index

00:26:35,850 --> 00:26:41,880
we found it as much better than Redis or

00:26:39,000 --> 00:26:44,580
anything to compare it in terms of reeds

00:26:41,880 --> 00:26:46,710
and maintainability we developed it

00:26:44,580 --> 00:26:50,160
in-house it's written in C++ with python

00:26:46,710 --> 00:26:52,200
wrappers again and it actually stores

00:26:50,160 --> 00:26:55,800
your index to query representation so

00:26:52,200 --> 00:26:58,500
what you effectively see is given a user

00:26:55,800 --> 00:27:00,570
query you'll get a query vector you

00:26:58,500 --> 00:27:03,090
search within the annoying models the

00:27:00,570 --> 00:27:04,920
the closest query vectors you'll get

00:27:03,090 --> 00:27:07,380
indexes for these then you query the

00:27:04,920 --> 00:27:10,290
Kiwi index we'll get all the queries and

00:27:07,380 --> 00:27:12,150
effectively you can fetch the pages for

00:27:10,290 --> 00:27:14,640
all the queries that have closest to the

00:27:12,150 --> 00:27:17,760
user query and this is how we improve a

00:27:14,640 --> 00:27:20,310
recall and the results are pretty

00:27:17,760 --> 00:27:22,980
amazing in the sense that we get much

00:27:20,310 --> 00:27:25,500
richer set of a candidate pages after

00:27:22,980 --> 00:27:27,660
the first fetching step with like a

00:27:25,500 --> 00:27:30,570
higher possibility of expected pages

00:27:27,660 --> 00:27:31,530
among them and the reason it is going

00:27:30,570 --> 00:27:34,560
this way is because

00:27:31,530 --> 00:27:36,780
navien going beyond synonyms are doing a

00:27:34,560 --> 00:27:39,150
simple passing match but actually using

00:27:36,780 --> 00:27:42,540
how vectors are learned semantically it

00:27:39,150 --> 00:27:44,250
screws up sometimes but most of the time

00:27:42,540 --> 00:27:46,200
we'll find like there is a definite

00:27:44,250 --> 00:27:47,910
improvement because you'll you always

00:27:46,200 --> 00:27:50,190
try to learn those words which are near

00:27:47,910 --> 00:27:52,740
to the context and that's a very

00:27:50,190 --> 00:27:54,360
important feature and queries are now

00:27:52,740 --> 00:27:56,160
matched in real-time using a cosign

00:27:54,360 --> 00:27:58,230
vector similarity between query query

00:27:56,160 --> 00:27:59,610
vectors plus using the classical

00:27:58,230 --> 00:28:01,980
information retrieval techniques that we

00:27:59,610 --> 00:28:03,870
use at clicks and overall there's a

00:28:01,980 --> 00:28:05,970
recall improvement from previous release

00:28:03,870 --> 00:28:07,920
that we had was around five to seven

00:28:05,970 --> 00:28:09,630
percent so it's the improvement that we

00:28:07,920 --> 00:28:11,760
find on internal desks that how much you

00:28:09,630 --> 00:28:13,770
are improving on this and translated

00:28:11,760 --> 00:28:16,590
improvement in the final top three

00:28:13,770 --> 00:28:19,620
results is around one percent so that

00:28:16,590 --> 00:28:22,140
gives us a clear identification of where

00:28:19,620 --> 00:28:24,480
these vectors are actually useful or not

00:28:22,140 --> 00:28:25,830
and the system actually triggers only

00:28:24,480 --> 00:28:28,470
for those queries which you have never

00:28:25,830 --> 00:28:30,960
seen before so that's also like a very

00:28:28,470 --> 00:28:33,900
very important point here because for

00:28:30,960 --> 00:28:35,820
the scene queries like FB or Google you

00:28:33,900 --> 00:28:38,340
actually land it to a certain page you

00:28:35,820 --> 00:28:40,200
are definitely sure about it but for

00:28:38,340 --> 00:28:41,850
queries which are not seen before which

00:28:40,200 --> 00:28:43,860
are new to us which are not in our index

00:28:41,850 --> 00:28:45,480
you have to go beyond with the

00:28:43,860 --> 00:28:47,060
traditional techniques and this one-take

00:28:45,480 --> 00:28:49,470
need actually helps a lot

00:28:47,060 --> 00:28:52,320
so before I conclude

00:28:49,470 --> 00:28:54,029
I actually wanted to show like what the

00:28:52,320 --> 00:28:56,460
browser actually looks like so this is

00:28:54,029 --> 00:28:59,610
like a clicks browser and this is the

00:28:56,460 --> 00:29:02,039
search page and we actually have this

00:28:59,610 --> 00:29:05,190
snippet which comes up the idea of this

00:29:02,039 --> 00:29:07,350
verse to reduce that whole step of

00:29:05,190 --> 00:29:09,710
search engine result page and you can

00:29:07,350 --> 00:29:13,799
actually get like directly to our page

00:29:09,710 --> 00:29:16,769
so the libraries are like Spotify and on

00:29:13,799 --> 00:29:19,350
which is again available on github kiwi

00:29:16,769 --> 00:29:20,909
which is clicks OSS and oh get out that

00:29:19,350 --> 00:29:22,799
you will actually find it's pretty

00:29:20,909 --> 00:29:25,980
useful it's pretty active project as

00:29:22,799 --> 00:29:28,289
well both Tyvek can be trained using

00:29:25,980 --> 00:29:30,389
Jentezen if you want to do a prototype

00:29:28,289 --> 00:29:33,750
but i would recommend to use the

00:29:30,389 --> 00:29:35,340
original c code because it's it's a bit

00:29:33,750 --> 00:29:37,590
more optimized and we found like there

00:29:35,340 --> 00:29:39,629
are certain variations in like the

00:29:37,590 --> 00:29:42,179
models that are developed because of the

00:29:39,629 --> 00:29:44,129
commentary that we see there are other

00:29:42,179 --> 00:29:46,470
clicks OS projects that you can actually

00:29:44,129 --> 00:29:49,769
contribute to if you want to find the

00:29:46,470 --> 00:29:55,259
slides it it is actually on speaker deck

00:29:49,769 --> 00:29:58,169
it's qe Python bit dot ly /qe Python so

00:29:55,259 --> 00:30:00,629
before I conclude I just say this thing

00:29:58,169 --> 00:30:03,269
that we are still like working on this

00:30:00,629 --> 00:30:04,590
system we have like the first version of

00:30:03,269 --> 00:30:06,120
this thing ready but we are trying to

00:30:04,590 --> 00:30:07,529
look up at other approaches the deep

00:30:06,120 --> 00:30:09,809
learning like using something called

00:30:07,529 --> 00:30:11,580
long and short-term memory networks the

00:30:09,809 --> 00:30:13,710
only downside of that approach is like

00:30:11,580 --> 00:30:15,960
most of these user queries are like

00:30:13,710 --> 00:30:18,330
keyword based and you don't usually find

00:30:15,960 --> 00:30:20,610
people actually typing okay what is the

00:30:18,330 --> 00:30:22,259
height of Statue of Liberty those are

00:30:20,610 --> 00:30:24,090
things you'll probably hire say such of

00:30:22,259 --> 00:30:26,220
Liberty height and that sort of

00:30:24,090 --> 00:30:27,330
linguistic relationships maybe well

00:30:26,220 --> 00:30:29,730
captured by Alice tiems

00:30:27,330 --> 00:30:31,409
they are more complicated but this

00:30:29,730 --> 00:30:33,240
system is like simple enough to still

00:30:31,409 --> 00:30:35,340
give you pretty good results so we are

00:30:33,240 --> 00:30:38,970
trying to use this new metric that we

00:30:35,340 --> 00:30:40,980
have into ranking we are trying to use

00:30:38,970 --> 00:30:42,539
this go a to paid similarity using

00:30:40,980 --> 00:30:45,389
document vectors where again we are

00:30:42,539 --> 00:30:47,250
using like sort of a differentiated lsdm

00:30:45,389 --> 00:30:49,379
mul or like a paragraph two vectors

00:30:47,250 --> 00:30:50,820
model and we are trying to also improve

00:30:49,379 --> 00:30:53,429
our search system for there are some

00:30:50,820 --> 00:30:55,679
pages which are never queried before so

00:30:53,429 --> 00:30:58,529
we have a lot of lists of these pages we

00:30:55,679 --> 00:31:00,299
try to find out what what could be the

00:30:58,529 --> 00:31:01,980
best way to represent those pages so

00:31:00,299 --> 00:31:02,460
either using vectors or traditional

00:31:01,980 --> 00:31:05,220
Engram

00:31:02,460 --> 00:31:07,260
approach or something like this last but

00:31:05,220 --> 00:31:10,020
not the least I'll say thank you and

00:31:07,260 --> 00:31:13,440
I'll finish with this coat which was

00:31:10,020 --> 00:31:15,660
given by John Rupert fit in 1957 where

00:31:13,440 --> 00:31:18,510
he said you shall know a word by the

00:31:15,660 --> 00:31:20,340
company it keeps and Niccolo actually

00:31:18,510 --> 00:31:23,190
developed a model using the same

00:31:20,340 --> 00:31:26,010
contextual approach of words and it

00:31:23,190 --> 00:31:28,490
actually held us give good results so

00:31:26,010 --> 00:31:28,490
thank you

00:31:36,570 --> 00:31:41,490
any questions yeah

00:31:56,790 --> 00:32:02,760
so one of the reasons we had was like we

00:32:00,690 --> 00:32:04,800
wanted like a unified visa we tried a

00:32:02,760 --> 00:32:06,810
lot of these key value stores ourselves

00:32:04,800 --> 00:32:09,450
we tried Redis we try like a traditional

00:32:06,810 --> 00:32:11,190
database with Joey lastic search but

00:32:09,450 --> 00:32:12,990
what we found is like our needs are a

00:32:11,190 --> 00:32:14,880
bit different in the sense that we

00:32:12,990 --> 00:32:17,640
sometimes have a vector index where we

00:32:14,880 --> 00:32:19,890
need like our values should be a list of

00:32:17,640 --> 00:32:21,750
vectors sometimes it is just strings

00:32:19,890 --> 00:32:23,760
sometimes they are repeated strings

00:32:21,750 --> 00:32:25,440
where like you have the same JSON data

00:32:23,760 --> 00:32:27,090
structure again and again so we can

00:32:25,440 --> 00:32:28,440
actually optimize it more if you can

00:32:27,090 --> 00:32:32,100
write those parts of the code ourselves

00:32:28,440 --> 00:32:33,690
we started by doing that so I mean kili

00:32:32,100 --> 00:32:35,430
is a much bigger project here and I'm

00:32:33,690 --> 00:32:37,200
not really the expert in it but what I

00:32:35,430 --> 00:32:39,690
can say is like it has a lot of features

00:32:37,200 --> 00:32:42,660
in like you can actually like compress

00:32:39,690 --> 00:32:44,220
your keys you can do a message that sort

00:32:42,660 --> 00:32:46,110
of compression using cell there but

00:32:44,220 --> 00:32:48,690
snappy and that gives you like a much

00:32:46,110 --> 00:32:50,460
schoo hesed vector it's faster to index

00:32:48,690 --> 00:32:52,170
it's faster to reads and it's scalable

00:32:50,460 --> 00:32:54,600
in terms that we don't actually have to

00:32:52,170 --> 00:32:57,000
put this in memory we can actually still

00:32:54,600 --> 00:32:58,560
have it on disk and do a memory map so

00:32:57,000 --> 00:33:01,200
you can still have like a lots of data

00:32:58,560 --> 00:33:02,940
that can train the what we actually

00:33:01,200 --> 00:33:05,040
wanted in our use case was we wanted

00:33:02,940 --> 00:33:07,200
reads to be optimized because we don't

00:33:05,040 --> 00:33:09,300
have rights at all we can compile the

00:33:07,200 --> 00:33:11,490
index at once and then what we want at

00:33:09,300 --> 00:33:13,230
runtime is like user query and give data

00:33:11,490 --> 00:33:15,950
from the index for that key we works

00:33:13,230 --> 00:33:15,950
pretty nicely for us

00:33:27,489 --> 00:33:32,149
you were already talking about having no

00:33:29,929 --> 00:33:35,330
rights on the database I was wondering

00:33:32,149 --> 00:33:39,769
how you handle having new new data new

00:33:35,330 --> 00:33:48,080
queries new data to train your

00:33:39,769 --> 00:33:51,679
embeddings or embed it I will say near

00:33:48,080 --> 00:33:54,859
nearest neighbor index because from what

00:33:51,679 --> 00:33:58,279
and over there are still no no very no

00:33:54,859 --> 00:34:00,379
nothing very very rare are no

00:33:58,279 --> 00:34:03,489
implementations of nearest neighbors

00:34:00,379 --> 00:34:06,529
that can you know just update the index

00:34:03,489 --> 00:34:08,450
yeah so it's true

00:34:06,529 --> 00:34:10,879
so what we do is like we have a release

00:34:08,450 --> 00:34:13,669
cycle where we compile each annoy index

00:34:10,879 --> 00:34:15,530
every month and we have we also like get

00:34:13,669 --> 00:34:17,839
new queries a new query vectors for this

00:34:15,530 --> 00:34:19,730
so it's not like a one time system but

00:34:17,839 --> 00:34:22,309
it's to like say immediately if tomorrow

00:34:19,730 --> 00:34:24,349
I want to include like a set of results

00:34:22,309 --> 00:34:27,290
which are like new queries for tomorrow

00:34:24,349 --> 00:34:28,970
I cannot do that but to address the same

00:34:27,290 --> 00:34:30,530
issue we have news so the newest

00:34:28,970 --> 00:34:32,690
vertical actually handles us so for the

00:34:30,530 --> 00:34:34,700
most recent part of like say anything

00:34:32,690 --> 00:34:38,240
that is trending right now you'll have

00:34:34,700 --> 00:34:40,579
like in the news section so given like

00:34:38,240 --> 00:34:42,589
the concepts you usually find like say

00:34:40,579 --> 00:34:44,780
pokemons go was already available on

00:34:42,589 --> 00:34:46,639
Wikipedia before its release so you

00:34:44,780 --> 00:34:48,829
actually have these concepts which are

00:34:46,639 --> 00:34:51,230
already learned from Wikipedia data and

00:34:48,829 --> 00:34:52,730
that's what we use so you can always

00:34:51,230 --> 00:34:56,329
learn the concept for the new words like

00:34:52,730 --> 00:34:58,099
some some X Y D G X Y Z Gen X word which

00:34:56,329 --> 00:35:00,140
comes Gen Y word that comes up like

00:34:58,099 --> 00:35:02,180
tomorrow you probably not have it but

00:35:00,140 --> 00:35:09,490
it's it's a very hard problem anyway

00:35:02,180 --> 00:35:09,490
yeah anyone else

00:35:11,390 --> 00:35:15,589

YouTube URL: https://www.youtube.com/watch?v=Hcwovymu21o


