Title: Stefan Behnel - Fast Async Code with Cython and AsyncIO
Publication date: 2016-07-28
Playlist: EuroPython 2016
Description: 
	Stefan Behnel - Fast Async Code with Cython and AsyncIO
[EuroPython 2016]
[18 July 2016]
[Bilbao, Euskadi, Spain]
(https://ep2016.europython.eu//conference/talks/fast-async-code-with-cython-and-asyncio)

Learn how to use the new async/await language feature to write
asynchronous code in Python and [Cython][1]. See how to benefit from
the excellent low-level features that Cython provides to speed up or
parallelise your code, interface natively with external C/C++ code,
and achieve better responsiveness and lower latency also in mostly I/O
bound applications.

[1]: http://cython.org/

-----

Python has recently seen a fresh development boost around asynchronous
applications, triggered by the addition of the asyncio library and the
new async/await language features in Python 3.5, but coming from a
world of well established tools like [Twisted][2] and [Tornado][3].
The [Cython][1] compiler, which compiles Python code to C, has
accompanied and influenced this development. It provides full language
support for async/await under all Python versions starting from 2.6,
as well as native interoperability with existing Python code and the
new Python coroutines in Python 3.5.

Benchmarks show that, while fully compatible, Cython compiled
coroutines perform about 2-3x better than the same code executed in
Python, but they additionally allow to interface natively with
external C/C++ code, release the GIL, do parallel computation, and
much more. All of this extends the applicable zone for asynchronous
applications dramatically and can lead to better responsiveness and
lower latency also for mostly I/O bound applications.

This joined talk by an async I/O expert and one of the Cython core
developers explains how to write code with async/await in Python and
Cython, and shows how to benefit from the excellent low-level features
that Cython provides on top of Python.

[1]: http://cython.org/
[2]: https://twistedmatrix.com/
[3]: http://www.tornadoweb.org/
Captions: 
	00:00:00,290 --> 00:00:18,930
we'll be giving us this talk and there

00:00:04,319 --> 00:00:20,970
will be time for questions we are

00:00:18,930 --> 00:00:23,340
delivering this talk together with

00:00:20,970 --> 00:00:25,199
Stefan so we just thought that instead

00:00:23,340 --> 00:00:27,269
of having two standard talks that we

00:00:25,199 --> 00:00:28,710
give why don't do something spatial this

00:00:27,269 --> 00:00:31,289
year why don't bring you some fresh

00:00:28,710 --> 00:00:34,140
material so we tried our best to give

00:00:31,289 --> 00:00:35,610
you some new use cases new examples of

00:00:34,140 --> 00:00:39,090
the tools that you might already heard

00:00:35,610 --> 00:00:44,329
of many times so we are really happy to

00:00:39,090 --> 00:00:53,660
present you today how can we use to make

00:00:44,329 --> 00:01:28,140
software so let's start begin with so

00:00:53,660 --> 00:01:30,540
why don't you introduce yourself so I'm

00:01:28,140 --> 00:01:32,250
also organizing PyCon ze so I'm using

00:01:30,540 --> 00:01:34,110
this as another chance to remind

00:01:32,250 --> 00:01:36,479
everyone that yes it does exist this

00:01:34,110 --> 00:01:39,240
year it's in October so go on my condo

00:01:36,479 --> 00:01:42,750
de and see and come we are so happy to

00:01:39,240 --> 00:01:45,210
have everyone there and let me give a

00:01:42,750 --> 00:01:48,509
credit to the cool company where we're

00:01:45,210 --> 00:01:50,070
working on right now so scooby is an

00:01:48,509 --> 00:01:51,960
e-book flat right boot subscription

00:01:50,070 --> 00:01:54,930
service that operates for quite a while

00:01:51,960 --> 00:01:56,670
it has a coolest deal in Germany and

00:01:54,930 --> 00:02:00,119
it's available not only in Germany but

00:01:56,670 --> 00:02:04,020
worldwide we have a lot of books we have

00:02:00,119 --> 00:02:05,610
a decent price of $9.99 per month and

00:02:04,020 --> 00:02:09,020
people love it because they put

00:02:05,610 --> 00:02:11,489
five-star ratings to it that's of course

00:02:09,020 --> 00:02:13,680
advertising but what is even more

00:02:11,489 --> 00:02:15,569
relevance today is that the stuff

00:02:13,680 --> 00:02:18,390
we're showing you today is used in

00:02:15,569 --> 00:02:21,329
Tsukuba at least partially so we are

00:02:18,390 --> 00:02:24,810
python-based backends we are developing

00:02:21,329 --> 00:02:27,000
it from day to day on daily basis and

00:02:24,810 --> 00:02:31,519
the stuff that we are presenting you is

00:02:27,000 --> 00:02:33,209
tested in production so yes let's start

00:02:31,519 --> 00:02:35,519
about this talk

00:02:33,209 --> 00:02:37,470
how will it be structure so we start

00:02:35,519 --> 00:02:39,629
with the introduction to async i/o just

00:02:37,470 --> 00:02:42,060
to have everyone on the same track what

00:02:39,629 --> 00:02:45,540
what is it about and what will be their

00:02:42,060 --> 00:02:48,810
examples about today then we will go to

00:02:45,540 --> 00:02:52,129
site on topic and have a brief examples

00:02:48,810 --> 00:02:54,450
of what is it about and how it does work

00:02:52,129 --> 00:02:55,980
then we will show you how to use that

00:02:54,450 --> 00:02:58,829
together and how you can benefit from

00:02:55,980 --> 00:03:02,310
them from that and we'll show you some

00:02:58,829 --> 00:03:03,780
practical examples easy once more more

00:03:02,310 --> 00:03:05,730
complicated ones and then of course

00:03:03,780 --> 00:03:06,709
we'll have cool questions from you I

00:03:05,730 --> 00:03:12,150
hope so

00:03:06,709 --> 00:03:16,019
good so first async i/o hope you guys

00:03:12,150 --> 00:03:18,000
used a sink i/o I think that yeah we do

00:03:16,019 --> 00:03:21,470
not need to stop too much on that

00:03:18,000 --> 00:03:24,299
because it's such a buzzy topic you know

00:03:21,470 --> 00:03:27,599
it's represented very great on this

00:03:24,299 --> 00:03:29,699
conference so I think we just have to go

00:03:27,599 --> 00:03:31,829
very briefly about this everyone is on

00:03:29,699 --> 00:03:34,590
the same fact so a single oh it's

00:03:31,829 --> 00:03:37,819
finally finally a default tool in Python

00:03:34,590 --> 00:03:40,799
starting from 3.4 but back 40 to 3.3

00:03:37,819 --> 00:03:44,639
that we can use for the asynchronous

00:03:40,799 --> 00:03:46,949
Network communication it gives us tools

00:03:44,639 --> 00:03:48,599
that have been already available in

00:03:46,949 --> 00:03:51,840
different libraries it was there forever

00:03:48,599 --> 00:03:53,549
and twisted it was there in tornado and

00:03:51,840 --> 00:03:55,049
then there was a thing core as well but

00:03:53,549 --> 00:03:58,669
they think there is sort of a common

00:03:55,049 --> 00:03:58,669
tool to do this sort of work

00:04:05,770 --> 00:04:11,170
to show you some graphics

00:04:26,639 --> 00:04:33,389
the background just a very very brief

00:04:30,689 --> 00:04:36,960
recap why do we need this tool today why

00:04:33,389 --> 00:04:39,000
I was thinking about it so let's take a

00:04:36,960 --> 00:04:42,750
simple case of a synchronous processing

00:04:39,000 --> 00:04:44,939
of some requests so request on the Left

00:04:42,750 --> 00:04:47,039
processing so our back-end is in the

00:04:44,939 --> 00:04:48,870
middle responses are on the right this

00:04:47,039 --> 00:04:51,090
is a time scale so time is going down

00:04:48,870 --> 00:04:52,860
this is a traditional stuff that

00:04:51,090 --> 00:04:54,539
everyone was taught in the universities

00:04:52,860 --> 00:04:57,210
so we get the request we're processing

00:04:54,539 --> 00:04:59,759
it we throw the response out then we get

00:04:57,210 --> 00:05:01,979
the next request we process it we throw

00:04:59,759 --> 00:05:05,669
the response out synchronous standard

00:05:01,979 --> 00:05:07,349
and simple more realistic cases though

00:05:05,669 --> 00:05:10,229
that we have more than one request

00:05:07,349 --> 00:05:12,449
coming during the processing of the of

00:05:10,229 --> 00:05:15,479
the first one and they just have to wait

00:05:12,449 --> 00:05:18,060
we receive three requests but we can do

00:05:15,479 --> 00:05:21,449
just one task at a time so we do it with

00:05:18,060 --> 00:05:25,740
row responses as we are completing our

00:05:21,449 --> 00:05:28,590
tasks time is going the same thing then

00:05:25,740 --> 00:05:30,900
the deal about this thing is that most

00:05:28,590 --> 00:05:33,629
of the time at least and the i/o bound

00:05:30,900 --> 00:05:35,610
applications like most of the websites

00:05:33,629 --> 00:05:38,610
most of the web services most of the

00:05:35,610 --> 00:05:40,979
database applications are just adding

00:05:38,610 --> 00:05:43,199
waiting for i/o so this time is

00:05:40,979 --> 00:05:45,000
something that our CPUs a-wasting and we

00:05:43,199 --> 00:05:46,800
could save that if we could do other

00:05:45,000 --> 00:05:49,560
stuff in the meanwhile while we are

00:05:46,800 --> 00:05:52,169
waiting on the blocking resource so I

00:05:49,560 --> 00:05:54,360
mark here this blocks as waiting let's

00:05:52,169 --> 00:05:57,990
say it's database some other external

00:05:54,360 --> 00:06:00,029
API whatever and the a synchronous

00:05:57,990 --> 00:06:03,839
execution model lets us to save this

00:06:00,029 --> 00:06:05,729
time by switching between tasks so we

00:06:03,839 --> 00:06:08,699
have same three tasks tasks one task two

00:06:05,729 --> 00:06:11,159
and task three but in this scenario we

00:06:08,699 --> 00:06:14,339
work on task one then we do blocking

00:06:11,159 --> 00:06:16,409
request to the database and we can work

00:06:14,339 --> 00:06:18,779
on the other things in the meanwhile so

00:06:16,409 --> 00:06:20,849
we jump to the task 2 right away when

00:06:18,779 --> 00:06:22,740
task 2 is done it's a short one then we

00:06:20,849 --> 00:06:24,839
go back to task 1 because we got a reply

00:06:22,740 --> 00:06:26,969
from the database and we work on that

00:06:24,839 --> 00:06:30,750
and so on so we jump between tasks

00:06:26,969 --> 00:06:33,599
that's how we can cut the idling and

00:06:30,750 --> 00:06:35,699
waiting on the i/o and that's how we

00:06:33,599 --> 00:06:38,190
basically most of the internet related

00:06:35,699 --> 00:06:42,540
applications can save on the

00:06:38,190 --> 00:06:44,370
execution time of some tasks so what is

00:06:42,540 --> 00:06:47,160
the deal about they sing for you with

00:06:44,370 --> 00:06:49,860
this is first of all it's an event loop

00:06:47,160 --> 00:06:52,830
known as reactor from twisted like 12

00:06:49,860 --> 00:06:54,480
years ago was already there so again

00:06:52,830 --> 00:06:56,880
it's nothing new it's just a standard

00:06:54,480 --> 00:07:01,800
way to do that what I aloof does is

00:06:56,880 --> 00:07:03,780
essentially it's it's a tool that is

00:07:01,800 --> 00:07:06,090
managing the network events and it knows

00:07:03,780 --> 00:07:07,920
which event is related to which code in

00:07:06,090 --> 00:07:10,260
our application so it will be

00:07:07,920 --> 00:07:12,330
remembering when data available on

00:07:10,260 --> 00:07:14,370
particular socket which piece of code

00:07:12,330 --> 00:07:17,250
should it call and then when the piece

00:07:14,370 --> 00:07:19,500
of codes gives the control back to the

00:07:17,250 --> 00:07:21,210
AIA loop then it decides also which

00:07:19,500 --> 00:07:23,130
piece of code should be executed next so

00:07:21,210 --> 00:07:24,600
it does is jumping between blocks of

00:07:23,130 --> 00:07:27,690
code and it does the callbacks

00:07:24,600 --> 00:07:30,570
then another thing it finally gives us a

00:07:27,690 --> 00:07:32,160
common future class it's same interface

00:07:30,570 --> 00:07:33,810
basically almost same interfaces

00:07:32,160 --> 00:07:36,390
concurrent futures that we had before

00:07:33,810 --> 00:07:38,550
it's also pretty similar to turn eight

00:07:36,390 --> 00:07:40,140
the futures that we had and there are

00:07:38,550 --> 00:07:42,330
libraries that can convert one future

00:07:40,140 --> 00:07:46,110
into another so you can mix the

00:07:42,330 --> 00:07:48,540
frameworks and what is the future just a

00:07:46,110 --> 00:07:50,760
quick reminder it's so it's called

00:07:48,540 --> 00:07:54,390
deferred and twisted a placeholder for

00:07:50,760 --> 00:07:56,100
some results of probably network

00:07:54,390 --> 00:07:58,680
operation that is not available yet but

00:07:56,100 --> 00:08:00,870
will be available soon we're using it as

00:07:58,680 --> 00:08:03,270
a link to the future result of some

00:08:00,870 --> 00:08:06,570
operation to not block and wait but to

00:08:03,270 --> 00:08:07,380
do other things meanwhile and finally a

00:08:06,570 --> 00:08:09,990
coroutines

00:08:07,380 --> 00:08:15,360
again it's quite similar to her routines

00:08:09,990 --> 00:08:17,910
that we had in tornado but not only the

00:08:15,360 --> 00:08:21,450
core routine mechanic mechanics itself

00:08:17,910 --> 00:08:24,870
is updated but also the declaration of

00:08:21,450 --> 00:08:27,630
coroutine is updated starting from 3 to

00:08:24,870 --> 00:08:31,560
5 so we can use async/await

00:08:27,630 --> 00:08:34,169
loops async/await syntax instead of

00:08:31,560 --> 00:08:36,750
youth from syntax that is sort of

00:08:34,169 --> 00:08:39,780
fancier way to do that and site and

00:08:36,750 --> 00:08:42,660
supports it will have deeper review of

00:08:39,780 --> 00:08:45,360
that how can we use that together soon

00:08:42,660 --> 00:08:47,570
let me just quickly finish with a single

00:08:45,360 --> 00:08:51,560
introduction first

00:08:47,570 --> 00:08:54,380
yes so coroutines as I said is based on

00:08:51,560 --> 00:08:56,899
the generators can be declared with the

00:08:54,380 --> 00:09:01,940
utrom syntax or with async DEP syntax

00:08:56,899 --> 00:09:04,009
it's a function or a generator that can

00:09:01,940 --> 00:09:06,699
be suspended and give the control to

00:09:04,009 --> 00:09:09,589
other cover teams in the meanwhile a

00:09:06,699 --> 00:09:12,139
very simple example that I show you just

00:09:09,589 --> 00:09:14,720
to get started is we use a new pan

00:09:12,139 --> 00:09:16,490
tasting DEP syntax we do some processing

00:09:14,720 --> 00:09:20,990
then we want to do the blocking

00:09:16,490 --> 00:09:23,269
operation so we use async HTTP clients

00:09:20,990 --> 00:09:25,040
to fetch something for us fetching is a

00:09:23,269 --> 00:09:27,589
blocking operation so I think it should

00:09:25,040 --> 00:09:29,660
have a client will return us a future

00:09:27,589 --> 00:09:32,180
not the real result so we don't wait

00:09:29,660 --> 00:09:36,019
then we use await syntax or yield from

00:09:32,180 --> 00:09:37,759
syntax to point the I loop that ok this

00:09:36,019 --> 00:09:39,850
is something that we don't want to wait

00:09:37,759 --> 00:09:44,120
give control to other coroutines and

00:09:39,850 --> 00:09:46,310
just resume this core routine at this

00:09:44,120 --> 00:09:49,040
place when the result is ready on a

00:09:46,310 --> 00:09:50,779
socket and then when you have it we do

00:09:49,040 --> 00:09:53,620
more processing and return the result

00:09:50,779 --> 00:09:53,620
easy

00:10:09,290 --> 00:10:15,290
so finally we are getting closer to the

00:10:12,560 --> 00:10:19,430
the thing that we will be showing you

00:10:15,290 --> 00:10:23,870
today about how can we go even further

00:10:19,430 --> 00:10:25,550
in optimizing our simple tasks so on

00:10:23,870 --> 00:10:27,860
this graph what we are showing you is

00:10:25,550 --> 00:10:31,670
some front end servers that we have

00:10:27,860 --> 00:10:33,890
front-end let's say it's just some piece

00:10:31,670 --> 00:10:39,200
of the software that accepts network

00:10:33,890 --> 00:10:42,470
connections and it gives the tasks to

00:10:39,200 --> 00:10:44,180
back-end part that is doing some

00:10:42,470 --> 00:10:47,390
background processing for us

00:10:44,180 --> 00:10:50,120
I think IO is running here on the front

00:10:47,390 --> 00:10:52,280
server it's getting requests number one

00:10:50,120 --> 00:10:55,520
it does some processing and then okay we

00:10:52,280 --> 00:10:57,980
need some data from the backend so we do

00:10:55,520 --> 00:11:00,920
request to back-end back and does some

00:10:57,980 --> 00:11:04,880
processing meanwhile we have requests -

00:11:00,920 --> 00:11:09,380
that is going to the front-end server it

00:11:04,880 --> 00:11:11,120
is started it is executed and then I

00:11:09,380 --> 00:11:13,310
think here gives control again to the

00:11:11,120 --> 00:11:15,950
request number one because this result

00:11:13,310 --> 00:11:18,620
is already available here what we see

00:11:15,950 --> 00:11:21,290
here is that even though request one has

00:11:18,620 --> 00:11:24,110
begun before request - it will only get

00:11:21,290 --> 00:11:26,540
response afterwards so it was waiting

00:11:24,110 --> 00:11:28,790
longer than it could be waiting because

00:11:26,540 --> 00:11:31,220
the result from the back end that we

00:11:28,790 --> 00:11:34,010
need is already available here at this

00:11:31,220 --> 00:11:36,110
point but because there was request -

00:11:34,010 --> 00:11:37,730
coming in in the meanwhile and we have

00:11:36,110 --> 00:11:39,080
just one thread in which we execute

00:11:37,730 --> 00:11:40,790
everything we had to delay the

00:11:39,080 --> 00:11:48,020
processing of the request one even

00:11:40,790 --> 00:11:51,800
further it can get even worse if we have

00:11:48,020 --> 00:11:54,320
a request three so I think your decides

00:11:51,800 --> 00:11:58,610
which task should be done next and if at

00:11:54,320 --> 00:12:00,440
this point it decides ok we have the

00:11:58,610 --> 00:12:02,330
request number three and we have data

00:12:00,440 --> 00:12:04,220
available from the backend here which

00:12:02,330 --> 00:12:06,110
one should be the next it could decide

00:12:04,220 --> 00:12:09,100
ok let's work on the request three and

00:12:06,110 --> 00:12:11,840
then request one is delayed even further

00:12:09,100 --> 00:12:15,260
so even though coming in first it will

00:12:11,840 --> 00:12:17,150
be served last and recent some

00:12:15,260 --> 00:12:19,370
applications can be quite critical

00:12:17,150 --> 00:12:21,709
because we are not fair in the way we

00:12:19,370 --> 00:12:24,619
are dealing client requests

00:12:21,709 --> 00:12:26,930
and the most straightforward thing

00:12:24,619 --> 00:12:30,110
obviously what we can do is to make this

00:12:26,930 --> 00:12:31,850
time shorter so the shorter are one of

00:12:30,110 --> 00:12:36,050
the times that we need for processing

00:12:31,850 --> 00:12:39,379
some data the faster we can give

00:12:36,050 --> 00:12:41,569
responses and also the more flexible we

00:12:39,379 --> 00:12:44,899
are in switching between the tasks that

00:12:41,569 --> 00:12:47,660
we are currently executing so at this

00:12:44,899 --> 00:12:51,529
case by shorting the processing time on

00:12:47,660 --> 00:12:53,149
the async aside twice we see that even

00:12:51,529 --> 00:12:55,220
even though the backend could take the

00:12:53,149 --> 00:12:58,100
same time to give those data that we

00:12:55,220 --> 00:13:01,069
need even though we still are way better

00:12:58,100 --> 00:13:03,230
in serving the front end request because

00:13:01,069 --> 00:13:05,869
we are more efficient in switching

00:13:03,230 --> 00:13:08,929
between them and so our responses will

00:13:05,869 --> 00:13:14,029
be available faster and as you probably

00:13:08,929 --> 00:13:17,209
already guessed how can we reduce the

00:13:14,029 --> 00:13:18,230
latency how can we how can we process I

00:13:17,209 --> 00:13:24,230
think that your tasks more efficiently

00:13:18,230 --> 00:13:26,689
is only only if we reduce the task the

00:13:24,230 --> 00:13:29,439
time it takes to process every piece of

00:13:26,689 --> 00:13:32,179
the code that is there on a single side

00:13:29,439 --> 00:13:33,769
it's not database because database is

00:13:32,179 --> 00:13:36,470
probably the blocking response it will

00:13:33,769 --> 00:13:39,679
be processing some other place it is

00:13:36,470 --> 00:13:44,689
some usually Python code that is taking

00:13:39,679 --> 00:13:47,929
time to be executed and here's the sytem

00:13:44,689 --> 00:13:49,490
comes in now what Stefan give you

00:13:47,929 --> 00:13:53,679
introduction about seitan and then we

00:13:49,490 --> 00:13:53,679
talk about how can we optimize async

00:13:58,329 --> 00:15:52,809
then it's for example and whenever you

00:15:13,149 --> 00:15:54,610
want to do optimization and so first

00:15:52,809 --> 00:16:02,129
thing you would do is you would say a

00:15:54,610 --> 00:16:02,129
lot exciton and

00:16:04,730 --> 00:16:10,220
to get em compiled code into your iPad

00:16:07,699 --> 00:16:12,769
on a book what am i using here so I'm

00:16:10,220 --> 00:16:15,260
using Python the 3.5 and the latest

00:16:12,769 --> 00:16:22,389
iPhone release which I released a couple

00:16:15,260 --> 00:16:25,699
of days ago just for conference so and

00:16:22,389 --> 00:16:32,180
what you do is you can take an iPad

00:16:25,699 --> 00:16:35,300
himself and say thank you what I'm doing

00:16:32,180 --> 00:16:36,980
after here is and this is just plain

00:16:35,300 --> 00:16:38,839
Python and it could just use a plain

00:16:36,980 --> 00:16:42,649
Python code and say please compile for

00:16:38,839 --> 00:16:45,139
me but you're already using the finished

00:16:42,649 --> 00:16:47,540
items index for the set for optimization

00:16:45,139 --> 00:16:55,490
for using see data types in Python code

00:16:47,540 --> 00:16:58,010
a little function at 1 which is you know

00:16:55,490 --> 00:17:00,800
place world you add one to it I do the

00:16:58,010 --> 00:17:07,280
same with two values some add two values

00:17:00,800 --> 00:17:09,980
x and y and here it's already you can

00:17:07,280 --> 00:17:14,120
see it allows you to allows me to

00:17:09,980 --> 00:17:19,040
declare see data types you see that

00:17:14,120 --> 00:17:21,429
types for my variables and then what

00:17:19,040 --> 00:17:25,309
Titan will do is it's going to optimize

00:17:21,429 --> 00:17:27,350
the code for me okay so it's actually

00:17:25,309 --> 00:17:28,220
see integer so I can use native see

00:17:27,350 --> 00:17:30,020
operations on it

00:17:28,220 --> 00:17:33,740
and that's as fast as you process it

00:17:30,020 --> 00:17:37,960
goes as opposed to as fast as time can

00:17:33,740 --> 00:17:40,580
get with object which is way faster okay

00:17:37,960 --> 00:17:43,760
so this is how they look like I can

00:17:40,580 --> 00:17:47,919
declare argument types like in declare

00:17:43,760 --> 00:17:47,919
variables with this see def statement

00:17:50,770 --> 00:17:59,540
and then I'm using it in my function

00:17:52,940 --> 00:18:02,960
here adding it and from a are C integers

00:17:59,540 --> 00:18:08,470
the add operation will be run by a

00:18:02,960 --> 00:18:08,470
simple operation

00:18:15,380 --> 00:18:18,440
the nice thing is you don't have to care

00:18:16,970 --> 00:18:20,750
about these things right you just

00:18:18,440 --> 00:18:23,360
declare variables and sight on what

00:18:20,750 --> 00:18:28,280
generate efficiency code for you which

00:18:23,360 --> 00:18:32,780
usually does the right thing that's a

00:18:28,280 --> 00:18:35,780
way for for helping you understand

00:18:32,780 --> 00:18:40,700
what's right on mix of your code I'm not

00:18:35,780 --> 00:18:43,970
just saying is inside minus a which

00:18:40,700 --> 00:18:46,400
means annotate take my code and you

00:18:43,970 --> 00:18:51,320
think about it and this is what ii gives

00:18:46,400 --> 00:18:52,820
me outputs a little and tells me okay

00:18:51,320 --> 00:18:54,770
this is what i've seen in your code and

00:18:52,820 --> 00:18:57,290
when i click on it this is what i'm

00:18:54,770 --> 00:19:02,210
doing here so they're colorful asians

00:18:57,290 --> 00:19:05,770
and MC axis object variable so the the

00:19:02,210 --> 00:19:09,830
operation it does here is actually a

00:19:05,770 --> 00:19:13,490
Python object operation and down here as

00:19:09,830 --> 00:19:14,870
i said it's taken two variables two plus

00:19:13,490 --> 00:19:16,430
sign in years and at the direct

00:19:14,870 --> 00:19:20,420
co-operation so you can click through

00:19:16,430 --> 00:19:22,550
the code C word gives you and then take

00:19:20,420 --> 00:19:24,890
that as a hint where you have to touch

00:19:22,550 --> 00:19:27,020
your code right after optimize it will

00:19:24,890 --> 00:19:29,660
change about it okay

00:19:27,020 --> 00:19:32,090
so execute unit some all works as

00:19:29,660 --> 00:19:33,560
reflected I'm just getting the the

00:19:32,090 --> 00:19:35,240
functions at the word gift when you get

00:19:33,560 --> 00:19:37,780
them in Python I can just call them as

00:19:35,240 --> 00:19:45,070
they are and they get the right result

00:19:37,780 --> 00:19:45,070
okay functions defined Python functions

00:19:45,340 --> 00:19:51,100
more function types because when I'm

00:19:48,350 --> 00:20:00,560
interacting with C codes often necessary

00:19:51,100 --> 00:20:03,980
to define C functions directly in front

00:20:00,560 --> 00:20:07,250
of functions F and then we'd get with a

00:20:03,980 --> 00:20:09,710
plain C function static C function which

00:20:07,250 --> 00:20:15,880
can also pass around into C code as a

00:20:09,710 --> 00:20:15,880
call that for example which is and so on

00:20:26,990 --> 00:20:39,590
okay tell more things so you can do this

00:20:36,860 --> 00:20:42,419
you can just write clean Python code

00:20:39,590 --> 00:20:45,150
instead of writing second module and you

00:20:42,419 --> 00:20:47,580
can say I take this function compile it

00:20:45,150 --> 00:20:52,799
for me and then the combination will

00:20:47,580 --> 00:20:54,809
actually occur import time it's going to

00:20:52,799 --> 00:20:56,250
take the function analyze the code for

00:20:54,809 --> 00:20:59,600
you compile it for you and replace the

00:20:56,250 --> 00:21:02,490
function bar you compile function just

00:20:59,600 --> 00:21:04,620
it just decorated you can use that it's

00:21:02,490 --> 00:21:06,840
kind of like JIT compilation in the

00:21:04,620 --> 00:21:12,330
sense that it's the compelling steadily

00:21:06,840 --> 00:21:13,919
what is important nice feature okay how

00:21:12,330 --> 00:21:17,970
do you interact with C code so far I've

00:21:13,919 --> 00:21:24,740
only seen that you can use C data types

00:21:17,970 --> 00:21:29,340
variables in your code here's an example

00:21:24,740 --> 00:21:31,919
for using external C code what I'm doing

00:21:29,340 --> 00:21:32,940
here is I'm taking the math functions

00:21:31,919 --> 00:21:38,010
from Lipsy

00:21:32,940 --> 00:21:41,720
and I'm using the sine function for

00:21:38,010 --> 00:21:44,370
example I'm using the declaration for pi

00:21:41,720 --> 00:21:46,470
divided by the two taking the sine of it

00:21:44,370 --> 00:21:49,559
and printing it and this is often how

00:21:46,470 --> 00:21:54,450
typing code actually looks like so

00:21:49,559 --> 00:21:56,820
that's some - code some objects being

00:21:54,450 --> 00:22:00,090
there and you can really mix them freely

00:21:56,820 --> 00:22:02,039
as if it's just you know use this use

00:22:00,090 --> 00:22:10,950
that it's all there for you at the

00:22:02,039 --> 00:22:14,210
q-tips this is how memory allocation

00:22:10,950 --> 00:22:18,200
next example you can use milk and three

00:22:14,210 --> 00:22:23,340
in inside on there just use the usual

00:22:18,200 --> 00:22:26,130
the usual lipstick functions the Newseum

00:22:23,340 --> 00:22:28,620
often people prefer to use patent memory

00:22:26,130 --> 00:22:30,000
allocation instead because then you know

00:22:28,620 --> 00:22:32,809
the Python runtime knows about it

00:22:30,000 --> 00:22:32,809
understand

00:22:38,860 --> 00:22:44,360
yeah it's it's just plain C functions

00:22:42,740 --> 00:22:46,250
you can use them from insight encode the

00:22:44,360 --> 00:22:48,110
nice thing about it is that you can also

00:22:46,250 --> 00:22:49,429
pass C functions around you can

00:22:48,110 --> 00:22:52,040
obviously pass them around in this new

00:22:49,429 --> 00:22:54,770
code that you can also auto read them

00:22:52,040 --> 00:22:56,600
mm into Python and this is what I'm

00:22:54,770 --> 00:22:58,940
doing here I'm taking the math sine

00:22:56,600 --> 00:23:00,679
function for example I'm just assigning

00:22:58,940 --> 00:23:04,960
it through Python variable and then

00:23:00,679 --> 00:23:07,970
Sipan grows okay that's the C function

00:23:04,960 --> 00:23:08,990
object I have to wrap it so grabs it for

00:23:07,970 --> 00:23:11,600
me

00:23:08,990 --> 00:23:11,960
and then makes it part of my my module

00:23:11,600 --> 00:23:17,480
API

00:23:11,960 --> 00:23:20,720
okay so that means call it directly from

00:23:17,480 --> 00:23:23,720
from my notebook so I can take that

00:23:20,720 --> 00:23:26,480
function here and call directly into the

00:23:23,720 --> 00:23:28,400
Lib C sine function through an object

00:23:26,480 --> 00:23:32,179
wrapper that type of generated for me

00:23:28,400 --> 00:23:38,360
but you know environment really nice

00:23:32,179 --> 00:23:46,220
feature okay more involved example do

00:23:38,360 --> 00:23:48,799
integration runtime and what I'm doing

00:23:46,220 --> 00:24:00,590
here is I'm taking a piece of code or

00:23:48,799 --> 00:24:03,290
code and my second code I'm and then I'm

00:24:00,590 --> 00:24:06,440
instantiating runtime compiling

00:24:03,290 --> 00:24:12,559
Detroiter get calling the code

00:24:06,440 --> 00:24:17,600
converting arguments then cleaning

00:24:12,559 --> 00:24:19,130
everything up so this is exception base

00:24:17,600 --> 00:24:23,530
whenever something goes wrong I just

00:24:19,130 --> 00:24:23,530
raise a patent exception even though I'm

00:24:26,320 --> 00:24:31,610
even creating the around 10 goes wrong I

00:24:29,900 --> 00:24:42,860
can just raise the memory all right

00:24:31,610 --> 00:24:44,870
because functions I can call I can call

00:24:42,860 --> 00:24:46,100
them directly from a siphon code it

00:24:44,870 --> 00:24:49,809
looks a lot like Python

00:24:46,100 --> 00:24:49,809
except they're what I'm calling C code

00:24:50,609 --> 00:25:11,669
I'm okay

00:25:48,239 --> 00:25:55,269
second view series you can do slice

00:25:52,539 --> 00:26:15,970
assignments you can copy what value you

00:25:55,269 --> 00:26:19,509
can loop over them just now syphilis has

00:26:15,970 --> 00:26:23,019
a reputation of being kind of difficult

00:26:19,509 --> 00:26:24,730
to use language it's not from Saipan so

00:26:23,019 --> 00:26:30,190
if you from Saipan it's issues very

00:26:24,730 --> 00:26:30,700
beautiful because you know when it's

00:26:30,190 --> 00:26:41,919
done the right way

00:26:30,700 --> 00:26:44,830
so I recommend rather than which you can

00:26:41,919 --> 00:26:47,590
do but I wouldn't recommend that and

00:26:44,830 --> 00:26:51,190
this is an example how you use a CSS

00:26:47,590 --> 00:26:53,700
vector from standard library so I'm

00:26:51,190 --> 00:26:56,350
getting color values in here stubble

00:26:53,700 --> 00:27:00,570
assigning you to the suspect or just

00:26:56,350 --> 00:27:00,570
copy over and then I can do

00:27:02,179 --> 00:27:09,409
they would expect I can iterate over the

00:27:04,610 --> 00:27:11,779
vector do interests pass the vector into

00:27:09,409 --> 00:27:15,139
some external C++ function and pass it

00:27:11,779 --> 00:27:18,799
back and the passing bag vector into

00:27:15,139 --> 00:27:21,230
into Python basically what it does is it

00:27:18,799 --> 00:27:22,610
just copies it into you know the obvious

00:27:21,230 --> 00:27:23,360
piping representation in which in this

00:27:22,610 --> 00:27:25,610
case exists

00:27:23,360 --> 00:27:27,470
okay so automatically copies it over so

00:27:25,610 --> 00:27:30,860
that you can pass it back over the

00:27:27,470 --> 00:27:33,490
expected result this is how you use it

00:27:30,860 --> 00:27:36,490
has an in arguments Kevin recognized

00:27:33,490 --> 00:27:36,490
okay

00:27:46,659 --> 00:27:53,059
okay finally finally we have 15 minutes

00:27:51,230 --> 00:27:56,620
more to show you how it actually works

00:27:53,059 --> 00:28:03,230
together let's get right into the code

00:27:56,620 --> 00:28:05,600
so first we just have a little little

00:28:03,230 --> 00:28:09,380
helper function that will run something

00:28:05,600 --> 00:28:12,440
for us with async IO we get the instance

00:28:09,380 --> 00:28:16,390
of the I loop we run a core routine with

00:28:12,440 --> 00:28:20,299
that I love we return the result method

00:28:16,390 --> 00:28:21,980
so to show you that coroutines that will

00:28:20,299 --> 00:28:23,990
be made with seitan are totally

00:28:21,980 --> 00:28:28,159
compatible with one that you would do in

00:28:23,990 --> 00:28:30,230
native normal Python let show me you let

00:28:28,159 --> 00:28:34,720
me show you the following example so we

00:28:30,230 --> 00:28:34,720
use the fancy acing the F syntax now and

00:28:35,200 --> 00:28:41,179
this is just a very very basic function

00:28:38,299 --> 00:28:44,299
that will add one to something whatever

00:28:41,179 --> 00:28:46,490
you feed it in so you give the future in

00:28:44,299 --> 00:28:48,909
it will await for that future it will

00:28:46,490 --> 00:28:51,620
have results then it will say that it

00:28:48,909 --> 00:28:53,510
will print that result and just for us

00:28:51,620 --> 00:28:55,789
we've a remark that this was done with

00:28:53,510 --> 00:28:59,419
seitan and then it will return this

00:28:55,789 --> 00:29:01,370
result was one then I think that one

00:28:59,419 --> 00:29:03,380
will just return as one obviously but we

00:29:01,370 --> 00:29:07,309
want it to be according that's why we

00:29:03,380 --> 00:29:12,440
define it like this showing you that it

00:29:07,309 --> 00:29:16,909
runs we generate one then we add one and

00:29:12,440 --> 00:29:18,740
then we had one more this is there is

00:29:16,909 --> 00:29:23,320
out that we see we printed one two and

00:29:18,740 --> 00:29:27,649
the output at the end is 3 this is cycle

00:29:23,320 --> 00:29:30,080
the same thing but now it's done in pure

00:29:27,649 --> 00:29:33,950
Python the only difference is that I say

00:29:30,080 --> 00:29:36,940
here python that so that you see it's

00:29:33,950 --> 00:29:36,940
actually run with

00:29:39,090 --> 00:29:52,120
then you see that here in this example

00:29:47,910 --> 00:29:55,210
what i do is i generate number one then

00:29:52,120 --> 00:29:57,190
i use python function at one and then I

00:29:55,210 --> 00:29:59,200
use the siphon function function add one

00:29:57,190 --> 00:30:01,480
and then I run it all on the isle OOP

00:29:59,200 --> 00:30:03,640
and from prints we also see that the

00:30:01,480 --> 00:30:05,860
first one was executed with just Python

00:30:03,640 --> 00:30:08,770
second one with seitan which we could

00:30:05,860 --> 00:30:11,050
even not notice so it's totally easy to

00:30:08,770 --> 00:30:12,760
integrate it's easy to mix you can write

00:30:11,050 --> 00:30:14,590
sidon then you can write in Python then

00:30:12,760 --> 00:30:18,880
you can call it one from another and it

00:30:14,590 --> 00:30:22,240
still works next example I'm just a

00:30:18,880 --> 00:30:31,630
simple ping-pong game so we will make a

00:30:22,240 --> 00:30:37,840
function in Python we will call them the

00:30:31,630 --> 00:30:40,990
seitan version so we use we do the first

00:30:37,840 --> 00:30:43,360
definition with seitan its decrement by

00:30:40,990 --> 00:30:46,680
one function we will have a map of

00:30:43,360 --> 00:30:51,340
coroutines it's just some simple

00:30:46,680 --> 00:30:54,310
dictionaries that will say that first we

00:30:51,340 --> 00:30:56,200
will have the site inversion then we

00:30:54,310 --> 00:31:02,140
have a Python version it's actually a

00:30:56,200 --> 00:31:04,780
typo we pass this mapping in this

00:31:02,140 --> 00:31:07,000
condition is just not to print too much

00:31:04,780 --> 00:31:10,720
so to switch printing on and off based

00:31:07,000 --> 00:31:13,180
on the show variable we print Punk if

00:31:10,720 --> 00:31:21,100
it's a site on coroutine and we print

00:31:13,180 --> 00:31:24,910
pink pink if it's a python the same

00:31:21,100 --> 00:31:27,310
thing but now it prints pink because

00:31:24,910 --> 00:31:29,080
it's the python version the code is

00:31:27,310 --> 00:31:31,840
simple we see we just pick the right

00:31:29,080 --> 00:31:44,590
call routine based on the current value

00:31:31,840 --> 00:31:48,460
and its site and python site and fight

00:31:44,590 --> 00:31:51,000
and fight and fight them but what were

00:31:48,460 --> 00:31:53,370
we did this talk at all is too

00:31:51,000 --> 00:31:55,200
see if it actually makes sense to mix

00:31:53,370 --> 00:31:59,840
them if it gives us any advantage in the

00:31:55,200 --> 00:32:03,600
speed so now we will finally time it we

00:31:59,840 --> 00:32:06,990
recall the same thing first it's just a

00:32:03,600 --> 00:32:12,230
Python version we see that the result is

00:32:06,990 --> 00:32:16,049
3 4 3 ms then we call the site inversion

00:32:12,230 --> 00:32:19,500
it's 180 3 ms which is like double

00:32:16,049 --> 00:32:20,370
faster with no effort at all let me just

00:32:19,500 --> 00:32:25,490
remind you what is the difference

00:32:20,370 --> 00:32:32,220
between two two functions cycle Python

00:32:25,490 --> 00:32:34,919
difference is just this and we'd make it

00:32:32,220 --> 00:32:38,870
double as fast and then just to have it

00:32:34,919 --> 00:32:41,730
the third example is we mix it so we use

00:32:38,870 --> 00:32:45,990
for odd numbers sighting for even

00:32:41,730 --> 00:32:49,770
numbers Python we time it and the result

00:32:45,990 --> 00:32:51,690
is somewhere in the middle so it's it's

00:32:49,770 --> 00:32:58,740
actually not yeah it's actually it's

00:32:51,690 --> 00:33:01,080
actually worse yeah you also see that

00:32:58,740 --> 00:33:01,470
it's like so now it's sort of in the

00:33:01,080 --> 00:33:04,559
middle

00:33:01,470 --> 00:33:07,740
now now it's better yes okay

00:33:04,559 --> 00:33:11,179
now I have to run them all as the rice

00:33:07,740 --> 00:33:11,179
you can think that we made it up

00:33:14,670 --> 00:33:21,900
ya know it's fast it's fast it's health

00:33:19,080 --> 00:33:24,480
yeah yes yes so you see it's still

00:33:21,900 --> 00:33:26,490
double faster it makes sense if it all

00:33:24,480 --> 00:33:28,350
you need is just to put percent percent

00:33:26,490 --> 00:33:32,070
cycle and you get a double as fast then

00:33:28,350 --> 00:33:35,330
it's worth it I guess this vast example

00:33:32,070 --> 00:33:35,330
I'm not sure if we have time for that

00:33:40,700 --> 00:33:47,700
simple so it's just kind of a children

00:33:44,010 --> 00:33:49,830
game teaches division and you say number

00:33:47,700 --> 00:33:51,900
and if it's visible by three your safe

00:33:49,830 --> 00:33:55,020
is if it's visible about five is a buzz

00:33:51,900 --> 00:33:57,870
and if it's visible but but then you say

00:33:55,020 --> 00:34:10,110
if it was okay so that's what we

00:33:57,870 --> 00:34:11,880
implement in here okay so what we get in

00:34:10,110 --> 00:34:13,800
is a stream of numbers it's actually

00:34:11,880 --> 00:34:17,790
binary encoded numbers so four bytes per

00:34:13,800 --> 00:34:20,340
integer as a network stream and so I'm

00:34:17,790 --> 00:34:29,940
just unpacking those going through the

00:34:20,340 --> 00:34:32,550
numbers and okay so a bit of overhead

00:34:29,940 --> 00:34:34,140
because I'm putting stuff here there's a

00:34:32,550 --> 00:34:36,600
Python implementation of the whole thing

00:34:34,140 --> 00:34:38,159
and what you can see down here is I'm

00:34:36,600 --> 00:34:40,290
running through the values then using

00:34:38,159 --> 00:34:44,210
the array object for making everything

00:34:40,290 --> 00:34:44,210
run through the values here

00:34:50,390 --> 00:35:11,310
and then callback which which you know

00:35:01,050 --> 00:35:16,920
just a turn and to you I think I think a

00:35:11,310 --> 00:35:17,250
weight isn't data processing you just

00:35:16,920 --> 00:35:22,500
follow

00:35:17,250 --> 00:35:25,130
okay really nice so I'm taking this code

00:35:22,500 --> 00:35:25,130
verbally

00:35:26,010 --> 00:35:30,720
- cell that was Python code just taking

00:35:28,590 --> 00:35:39,450
it there renaming it sure we have two

00:35:30,720 --> 00:35:41,850
functions compiled and then I'm setting

00:35:39,450 --> 00:35:46,500
up my data stream here is just kind of a

00:35:41,850 --> 00:35:53,220
fake data stream that chunks into the

00:35:46,500 --> 00:35:54,060
stream you know loops processing running

00:35:53,220 --> 00:35:55,860
that

00:35:54,060 --> 00:35:57,240
yeah I'm building my data streams this

00:35:55,860 --> 00:36:06,920
what I'm expecting for the first 18

00:35:57,240 --> 00:36:06,920
values 1 2 3 and so on and that works

00:36:07,160 --> 00:36:16,860
works in both versions and performance

00:36:14,100 --> 00:36:18,690
comparison between the two and the

00:36:16,860 --> 00:36:20,850
second function from the Python function

00:36:18,690 --> 00:36:35,490
and you can see what the performance

00:36:20,850 --> 00:36:39,330
difference is seconds twice as long and

00:36:35,490 --> 00:36:41,700
then depending on chunk size the hands

00:36:39,330 --> 00:36:43,410
differ but it's usually something like

00:36:41,700 --> 00:36:47,550
twice as far as just by compiling it

00:36:43,410 --> 00:36:49,740
okay then the next thing I did was I

00:36:47,550 --> 00:36:53,130
rewrote the whole thing in a more you

00:36:49,740 --> 00:36:55,740
know CU shui I'm iterating and I'm

00:36:53,130 --> 00:37:00,090
running through just the data buffer at

00:36:55,740 --> 00:37:02,610
a you know C character level here doing

00:37:00,090 --> 00:37:05,700
the same thing in a more serious way by

00:37:02,610 --> 00:37:06,120
just casting you know 4 bytes to an

00:37:05,700 --> 00:37:27,600
integer

00:37:06,120 --> 00:37:32,160
we're just 40 percent faster here okay

00:37:27,600 --> 00:37:34,520
so what again overall is close to three

00:37:32,160 --> 00:37:34,520
times faster

00:37:40,820 --> 00:37:48,350
and then we talk yep I think that it's a

00:37:45,540 --> 00:37:48,350
good time for questions

00:37:59,250 --> 00:38:22,450
no don't run can you hear me oh yeah I

00:38:19,530 --> 00:38:28,300
was just wondering if you're using this

00:38:22,450 --> 00:38:30,070
at work and what you do if you're doing

00:38:28,300 --> 00:38:32,160
this technique at work and what you're

00:38:30,070 --> 00:38:35,800
actually doing with it

00:38:32,160 --> 00:38:38,500
you mean using site on together with

00:38:35,800 --> 00:38:41,619
async IO exactly how we are using a

00:38:38,500 --> 00:38:42,220
single there we did not compile I think

00:38:41,619 --> 00:38:46,300
yet

00:38:42,220 --> 00:38:50,320
Titan no pieces there but we played with

00:38:46,300 --> 00:38:52,180
it to make this talk and it works that

00:38:50,320 --> 00:38:54,160
was more the idea of the talk right - to

00:38:52,180 --> 00:38:57,210
convey that you know speed is important

00:38:54,160 --> 00:39:00,210
to win emeritus you can use for it

00:38:57,210 --> 00:39:00,210
Thanks

00:39:00,610 --> 00:39:08,170
yeah question am i right in thinking

00:39:04,210 --> 00:39:17,880
that this also worked with Python 2.7 I

00:39:08,170 --> 00:39:17,880
mean if you site on yeah the Edit in

00:39:19,110 --> 00:39:24,670
Python code but inside of site on that

00:39:21,850 --> 00:39:32,610
keyword yeah that's the major selling

00:39:24,670 --> 00:39:35,790
point for me so just one more question

00:39:32,610 --> 00:39:38,830
can you tell me about number integration

00:39:35,790 --> 00:39:41,820
can I use this to cast an umpire race

00:39:38,830 --> 00:39:46,660
how how well does that work with site on

00:39:41,820 --> 00:39:52,030
the question was site on extremely well

00:39:46,660 --> 00:39:54,640
like everyone uses it in scientific

00:39:52,030 --> 00:39:57,360
Python and we have special syntax

00:39:54,640 --> 00:40:00,040
insight on that allows you to say

00:39:57,360 --> 00:40:01,750
you know data buffer this whole looks

00:40:00,040 --> 00:40:04,900
like and then you can just iterate over

00:40:01,750 --> 00:40:08,140
it run through it as fast as the

00:40:04,900 --> 00:40:15,310
processor can and it's one of the main

00:40:08,140 --> 00:40:17,740
use cases Python yes right RDS

00:40:15,310 --> 00:40:19,930
I think our features insight on already

00:40:17,740 --> 00:40:22,810
read it for production or is it like

00:40:19,930 --> 00:40:26,080
experimental features like in beta or

00:40:22,810 --> 00:40:27,520
something so I think I think wait

00:40:26,080 --> 00:40:29,830
support insight on was developed at the

00:40:27,520 --> 00:40:32,590
same time as the Python support for a

00:40:29,830 --> 00:40:35,830
sink in the weight that were color

00:40:32,590 --> 00:40:38,680
changes recently which we followed by at

00:40:35,830 --> 00:40:41,740
the same level so they work equally well

00:40:38,680 --> 00:40:43,960
and the fun thing is they actually

00:40:41,740 --> 00:40:46,150
influence each other so and while the

00:40:43,960 --> 00:40:48,220
Python support was developed we

00:40:46,150 --> 00:40:50,950
developed outside and support and so we

00:40:48,220 --> 00:40:53,350
had an impact on how they did it and

00:40:50,950 --> 00:40:58,110
they changed how we did it and so that

00:40:53,350 --> 00:41:01,870
both improved by by working together I

00:40:58,110 --> 00:41:05,260
have a little final note but if you have

00:41:01,870 --> 00:41:07,750
any interest in getting more into detail

00:41:05,260 --> 00:41:09,580
of site on or off async is separately

00:41:07,750 --> 00:41:11,440
you are very welcome to the trainings

00:41:09,580 --> 00:41:12,100
that are tomorrow and the day after

00:41:11,440 --> 00:41:14,290
tomorrow

00:41:12,100 --> 00:41:24,850
that will cover this two topics really

00:41:14,290 --> 00:41:27,610
in detail Thanks

00:41:24,850 --> 00:41:30,580
thanks for a talk I was just wondering

00:41:27,610 --> 00:41:36,090
if if siphon could take advantage of the

00:41:30,580 --> 00:41:41,770
Python 3 type hints in order to compile

00:41:36,090 --> 00:41:44,170
actually so I discussed with you a

00:41:41,770 --> 00:41:45,310
couple times and the intention of the

00:41:44,170 --> 00:41:48,610
Python type-ins

00:41:45,310 --> 00:41:50,260
is not really something that's meant for

00:41:48,610 --> 00:41:53,680
steady compilation and it doesn't really

00:41:50,260 --> 00:41:57,520
help us so it's it's meant for type

00:41:53,680 --> 00:41:59,320
checking for annotating api is for

00:41:57,520 --> 00:42:01,540
making the machine understandable

00:41:59,320 --> 00:42:04,230
basically but it does not help in

00:42:01,540 --> 00:42:04,230

YouTube URL: https://www.youtube.com/watch?v=-hk6nEQEMLs


