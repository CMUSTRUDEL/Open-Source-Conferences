Title: Building your first app with python and MongoDB
Publication date: 2015-04-19
Playlist: EuroPython 2012
Description: 
	[EuroPython 2012] Ross Lawley - 5 JULY 2012 in "Track Spaghetti"
Captions: 
	00:02:24,150 --> 00:02:26,209
Oh

00:02:33,890 --> 00:02:40,010
oh let's starting our next speaker is

00:02:36,980 --> 00:02:44,150
Ross lastly lolly

00:02:40,010 --> 00:02:46,580
works with MongoDB and you will be

00:02:44,150 --> 00:02:49,250
speaking attention and you'll be

00:02:46,580 --> 00:02:58,280
speaking about writing apps with Python

00:02:49,250 --> 00:03:02,030
and MongoDB so thanks this one thank you

00:02:58,280 --> 00:03:04,670
very much so yes as I say hello my name

00:03:02,030 --> 00:03:08,060
is Ross Lally I work for 10 gen 10 dens

00:03:04,670 --> 00:03:11,000
the company behind MongoDB so we own the

00:03:08,060 --> 00:03:14,570
intellectual property and we basically

00:03:11,000 --> 00:03:16,459
dried the roadmap for MongoDB and we

00:03:14,570 --> 00:03:19,760
make our money through consulting and

00:03:16,459 --> 00:03:21,200
doing support and things I work on the

00:03:19,760 --> 00:03:22,850
on the drivers side so I don't actually

00:03:21,200 --> 00:03:25,670
work on building the database I'll work

00:03:22,850 --> 00:03:30,320
on the connectivity between the database

00:03:25,670 --> 00:03:30,920
and MongoDB through pymongo I'm based in

00:03:30,320 --> 00:03:33,530
London

00:03:30,920 --> 00:03:37,070
and we have other offices guys they work

00:03:33,530 --> 00:03:42,200
with Python in New York and in Palo Alto

00:03:37,070 --> 00:03:44,840
I also maintain a odm object data mapper

00:03:42,200 --> 00:03:47,750
called engine which is similar to

00:03:44,840 --> 00:03:52,820
kind of how Django's ORM works but works

00:03:47,750 --> 00:03:54,110
with works with MongoDB and if anyone

00:03:52,820 --> 00:03:56,450
likes that would like to follow me on

00:03:54,110 --> 00:03:58,370
Twitter it's Rossi zero and I have some

00:03:56,450 --> 00:04:00,470
stuff on github as well some code

00:03:58,370 --> 00:04:05,000
examples and things and we'll talk more

00:04:00,470 --> 00:04:07,850
about them later they this talk is going

00:04:05,000 --> 00:04:09,200
to be kind of split up into two parts by

00:04:07,850 --> 00:04:11,239
the end of the talk I hope to give you

00:04:09,200 --> 00:04:14,540
enough knowledge about what MongoDB is

00:04:11,239 --> 00:04:15,769
how you can use in Python and also give

00:04:14,540 --> 00:04:17,989
you some information of what you

00:04:15,769 --> 00:04:20,870
shouldn't do with it and all some of the

00:04:17,989 --> 00:04:23,600
bad practices or anti patterns that you

00:04:20,870 --> 00:04:24,770
can do with it to give you or to ensure

00:04:23,600 --> 00:04:26,660
that you have a good time with it and

00:04:24,770 --> 00:04:31,370
you have the best relationship with

00:04:26,660 --> 00:04:33,590
MongoDB as possible so the very first

00:04:31,370 --> 00:04:35,210
aspect of MongoDB is understanding where

00:04:33,590 --> 00:04:37,340
it came from will really help you

00:04:35,210 --> 00:04:39,410
understand why it is built like it is

00:04:37,340 --> 00:04:40,130
built and how you can use it to its best

00:04:39,410 --> 00:04:44,479
really

00:04:40,130 --> 00:04:47,360
so MongoDB was built by two guys or the

00:04:44,479 --> 00:04:50,629
company ten gems built by Dwight May

00:04:47,360 --> 00:04:52,370
and Eliot Horowitz who previously worked

00:04:50,629 --> 00:04:55,039
together at double-click Dwight was one

00:04:52,370 --> 00:04:56,599
of the founders there and a double-click

00:04:55,039 --> 00:04:58,759
that scale up and they had lots of

00:04:56,599 --> 00:05:01,280
problems scaling up to 30 serving 30

00:04:58,759 --> 00:05:03,530
billion hours a day and after

00:05:01,280 --> 00:05:06,229
double-click hooks got sold to Google

00:05:03,530 --> 00:05:08,300
they went on and did another venture

00:05:06,229 --> 00:05:10,189
called Trott wiki and there they had

00:05:08,300 --> 00:05:13,460
lots of problems again scaling out and

00:05:10,189 --> 00:05:15,800
and scaling up their databases and the

00:05:13,460 --> 00:05:17,949
one thing that they found was every time

00:05:15,800 --> 00:05:20,360
they did this when they scaled up

00:05:17,949 --> 00:05:22,520
relational databases they they went

00:05:20,360 --> 00:05:23,900
through lots of periods of hurt so to

00:05:22,520 --> 00:05:26,750
begin with when they started the project

00:05:23,900 --> 00:05:28,849
everything was super productive it's

00:05:26,750 --> 00:05:31,340
easy to scale and easy to be fast with a

00:05:28,849 --> 00:05:33,560
simple and small data set but as they

00:05:31,340 --> 00:05:35,870
grew and as their datasets grew and as

00:05:33,560 --> 00:05:38,029
the request the seconds grew and the

00:05:35,870 --> 00:05:39,349
volumes grew they found it to be able to

00:05:38,029 --> 00:05:42,110
cope with that data they have to start

00:05:39,349 --> 00:05:44,569
to denormalize it to be able to get the

00:05:42,110 --> 00:05:46,699
speed that they required to serve and as

00:05:44,569 --> 00:05:48,139
they carried on 40 normalizing that he

00:05:46,699 --> 00:05:51,409
went and did things like they got rid of

00:05:48,139 --> 00:05:54,710
joins totally they started adding custom

00:05:51,409 --> 00:05:58,129
sharding layers and uncustomed caching

00:05:54,710 --> 00:06:01,669
layers as well and managing and dealing

00:05:58,129 --> 00:06:03,229
with sharding at the time for them meant

00:06:01,669 --> 00:06:05,150
they had to write a lot of codes to be

00:06:03,229 --> 00:06:07,969
able to do that and they had some very

00:06:05,150 --> 00:06:09,529
specific needs for their caching layers

00:06:07,969 --> 00:06:11,900
so they have different types of objects

00:06:09,529 --> 00:06:15,020
stored they have some documents some

00:06:11,900 --> 00:06:16,520
images things like they say basically I

00:06:15,020 --> 00:06:19,580
think Elliott built four or five

00:06:16,520 --> 00:06:22,719
different databases at SHOT wiki before

00:06:19,580 --> 00:06:26,960
they came together and founded 10 gem

00:06:22,719 --> 00:06:28,189
though in 2007 they founded 10 June and

00:06:26,960 --> 00:06:30,020
initially they were going to do a

00:06:28,189 --> 00:06:32,779
platform as-a-service the kind of google

00:06:30,020 --> 00:06:35,000
appengine type thing and that's what

00:06:32,779 --> 00:06:38,000
they decided to go for and where they

00:06:35,000 --> 00:06:39,139
were going to go next when they were

00:06:38,000 --> 00:06:40,819
doing that they wanted to build their

00:06:39,139 --> 00:06:44,240
own custom datastore and this is where

00:06:40,819 --> 00:06:47,060
MongoDB came from so MongoDB was built

00:06:44,240 --> 00:06:49,250
to be able to as a document database to

00:06:47,060 --> 00:06:51,139
be able to do and the idea is it can

00:06:49,250 --> 00:06:54,529
horizontally scale out and you can add

00:06:51,139 --> 00:06:56,120
more and more nodes to it by a having

00:06:54,529 --> 00:06:58,009
documents is a different way to defining

00:06:56,120 --> 00:07:00,060
your data and the other side is they

00:06:58,009 --> 00:07:03,060
don't have things like joins which

00:07:00,060 --> 00:07:05,850
they're scaling earlier on at their

00:07:03,060 --> 00:07:07,830
previous experience just to mention

00:07:05,850 --> 00:07:09,930
MongoDB is literally only three years

00:07:07,830 --> 00:07:12,419
old so is it still relatively new

00:07:09,930 --> 00:07:16,200
the first release was in 2008 the first

00:07:12,419 --> 00:07:18,960
1.0 release was sorry 1.0 release in

00:07:16,200 --> 00:07:21,030
2009 august so it's literally coming up

00:07:18,960 --> 00:07:23,340
to its probably third year of being in a

00:07:21,030 --> 00:07:25,770
production system somewhere and a lot

00:07:23,340 --> 00:07:31,680
has changed in in the last three years

00:07:25,770 --> 00:07:33,120
so only only in august two years ago did

00:07:31,680 --> 00:07:35,790
things like sharding and replica sets

00:07:33,120 --> 00:07:37,500
come in and then later on things like

00:07:35,790 --> 00:07:40,020
journaling came in so it has really

00:07:37,500 --> 00:07:45,630
matured in that short time and has

00:07:40,020 --> 00:07:47,880
changed quite a lot though let's now go

00:07:45,630 --> 00:07:49,350
and focus on using MongoDB with Python

00:07:47,880 --> 00:07:50,910
and give you some examples of what

00:07:49,350 --> 00:07:51,780
document database is and what that

00:07:50,910 --> 00:07:56,430
really means

00:07:51,780 --> 00:07:58,190
so MongoDB the Python first of all what

00:07:56,430 --> 00:08:01,350
do we is the drivers what do we support

00:07:58,190 --> 00:08:04,169
well this is our build grid for in

00:08:01,350 --> 00:08:07,740
Jenkins that we have we support Python

00:08:04,169 --> 00:08:10,470
2.4 to two point seven by seven three

00:08:07,740 --> 00:08:12,600
point two and now three three I don't

00:08:10,470 --> 00:08:15,960
think it's in the grid yet Jason and pie

00:08:12,600 --> 00:08:18,210
pie so any pretty much all these flavors

00:08:15,960 --> 00:08:20,340
of Python if you use on whatever systems

00:08:18,210 --> 00:08:23,300
you'll be able to use the driver to be

00:08:20,340 --> 00:08:28,050
able to interact with pymongo and

00:08:23,300 --> 00:08:29,580
MongoDB so first of all what is a

00:08:28,050 --> 00:08:31,380
document database what that means it

00:08:29,580 --> 00:08:33,120
doesn't mean you store things like Excel

00:08:31,380 --> 00:08:35,370
spreadsheets or you store things like

00:08:33,120 --> 00:08:37,589
Word documents which can be a common

00:08:35,370 --> 00:08:39,630
misconception it just means you have a

00:08:37,589 --> 00:08:42,570
richer day's form of how you store your

00:08:39,630 --> 00:08:46,110
data so it's not flat like how you would

00:08:42,570 --> 00:08:47,490
store it in a able it can be dynamic the

00:08:46,110 --> 00:08:49,980
documents can look different to each

00:08:47,490 --> 00:08:51,660
other there's no predefined schema when

00:08:49,980 --> 00:08:54,240
you save them so here's an example of

00:08:51,660 --> 00:08:56,160
adjacent or document you might add in

00:08:54,240 --> 00:08:59,310
the shell and here's the exact same

00:08:56,160 --> 00:09:01,170
example in Python so you can see it maps

00:08:59,310 --> 00:09:03,779
really nicely to how you do things in

00:09:01,170 --> 00:09:05,250
Python it's a it's a dictionary you

00:09:03,779 --> 00:09:08,839
still have things like lists you can

00:09:05,250 --> 00:09:11,490
have lists inside dictionaries and stuff

00:09:08,839 --> 00:09:13,450
and the one of the key points is when we

00:09:11,490 --> 00:09:15,190
come to talk about scaling is ensuring

00:09:13,450 --> 00:09:17,830
that even though it's a scheme this

00:09:15,190 --> 00:09:19,720
database how you use and how you model

00:09:17,830 --> 00:09:22,150
your data is probably the most important

00:09:19,720 --> 00:09:25,510
aspect on how well you can scale no

00:09:22,150 --> 00:09:28,150
matter what one of the good things about

00:09:25,510 --> 00:09:30,190
MongoDB is is these documents allow you

00:09:28,150 --> 00:09:32,050
a lot of flexibility but it also can

00:09:30,190 --> 00:09:34,000
mean it gives you enough rope to hang

00:09:32,050 --> 00:09:36,010
yourself with and some levels of

00:09:34,000 --> 00:09:37,900
complexity complexity but hopefully

00:09:36,010 --> 00:09:42,190
we'll come on and discuss more Bayside's

00:09:37,900 --> 00:09:45,150
later on the very first thing to get

00:09:42,190 --> 00:09:47,500
started you need to install pi a

00:09:45,150 --> 00:09:49,660
lot at all these code samples will just

00:09:47,500 --> 00:09:51,130
go work in the shell I'll give a links

00:09:49,660 --> 00:09:52,780
to some of the workshops that I did

00:09:51,130 --> 00:09:54,750
yesterday so you'll be able to run

00:09:52,780 --> 00:09:57,370
through them at your own leisure as well

00:09:54,750 --> 00:09:59,410
so once you got pymongo installed all

00:09:57,370 --> 00:10:02,170
you have to do is then is go and make a

00:09:59,410 --> 00:10:05,680
connection to your database so if it's a

00:10:02,170 --> 00:10:07,300
local database it and by default PI

00:10:05,680 --> 00:10:09,460
 if you do a connection will expect

00:10:07,300 --> 00:10:11,350
it to be on localhost but if not you can

00:10:09,460 --> 00:10:13,090
just pass it through a string pass the

00:10:11,350 --> 00:10:16,540
port number through and you'll you'll be

00:10:13,090 --> 00:10:19,030
connected to the database or connected

00:10:16,540 --> 00:10:20,470
to D B the next part is we're

00:10:19,030 --> 00:10:22,450
doing here is where we're assigning DB

00:10:20,470 --> 00:10:24,370
equals a connection blocks tutorial

00:10:22,450 --> 00:10:27,730
that's where we're actually connecting

00:10:24,370 --> 00:10:29,650
to a database now in MongoDB this is

00:10:27,730 --> 00:10:32,170
quite unusual if you come from the

00:10:29,650 --> 00:10:34,180
sequel world there is no great database

00:10:32,170 --> 00:10:36,130
statements there are no create tables or

00:10:34,180 --> 00:10:37,930
create collections statements in MongoDB

00:10:36,130 --> 00:10:40,240
at all as soon as you start using

00:10:37,930 --> 00:10:42,220
something that essentially could come

00:10:40,240 --> 00:10:45,370
into existence so if you didn't have a

00:10:42,220 --> 00:10:48,130
tutorial database and you did DB equals

00:10:45,370 --> 00:10:49,960
connection dot tutorial which ever

00:10:48,130 --> 00:10:53,410
semantics you do as soon as you start

00:10:49,960 --> 00:10:56,890
inserting data into that database the

00:10:53,410 --> 00:10:58,390
data files will then exist so this is

00:10:56,890 --> 00:11:00,310
something to be wary of when you start

00:10:58,390 --> 00:11:03,730
going out into production to ensure that

00:11:00,310 --> 00:11:05,980
people do all use the same case for

00:11:03,730 --> 00:11:08,110
their date base names or collection

00:11:05,980 --> 00:11:10,810
names as well and to ensure that they

00:11:08,110 --> 00:11:14,500
that you don't have database objects you

00:11:10,810 --> 00:11:16,480
don't expect to exist so connecting is

00:11:14,500 --> 00:11:18,250
really easy so once you have a database

00:11:16,480 --> 00:11:19,330
object then you can start doing things

00:11:18,250 --> 00:11:21,610
like you can start looking into

00:11:19,330 --> 00:11:26,320
collections and start uploading and

00:11:21,610 --> 00:11:26,990
managing data so here's some examples of

00:11:26,320 --> 00:11:30,230
just adding a

00:11:26,990 --> 00:11:32,360
simple document up here we're doing is

00:11:30,230 --> 00:11:34,220
DB for the database and then the

00:11:32,360 --> 00:11:35,810
collection name so and here it's my

00:11:34,220 --> 00:11:37,640
collection and then I'm just doing a

00:11:35,810 --> 00:11:39,020
save and this is just a Python

00:11:37,640 --> 00:11:40,850
dictionary so it can be whatever you

00:11:39,020 --> 00:11:44,089
want it can be as complex or as simple

00:11:40,850 --> 00:11:47,470
as you need but just for brevity they're

00:11:44,089 --> 00:11:50,839
short here you can use the dot save a

00:11:47,470 --> 00:11:54,380
method to go and save or update a

00:11:50,839 --> 00:11:55,820
document if you have it it's kind of a

00:11:54,380 --> 00:11:59,000
gotcha to do this though because

00:11:55,820 --> 00:12:01,550
potentially it could do could add race

00:11:59,000 --> 00:12:03,380
conditions into your code so it's better

00:12:01,550 --> 00:12:05,180
to be explicit and if you want to insert

00:12:03,380 --> 00:12:06,920
something use the insert statement and

00:12:05,180 --> 00:12:09,020
if you want to update something is the

00:12:06,920 --> 00:12:11,180
update statement will come on to this in

00:12:09,020 --> 00:12:14,270
the moment but there we go so once you

00:12:11,180 --> 00:12:16,640
have data inside a MongoDB all you have

00:12:14,270 --> 00:12:18,320
to do is call the find command and then

00:12:16,640 --> 00:12:20,120
that will give you a cursor back and as

00:12:18,320 --> 00:12:21,800
you iterate over the cursor it will

00:12:20,120 --> 00:12:24,529
return the responses back so that's very

00:12:21,800 --> 00:12:27,620
similar to how any sort database cursors

00:12:24,529 --> 00:12:29,690
will work for you there is a helper just

00:12:27,620 --> 00:12:31,399
find that underscore one which will just

00:12:29,690 --> 00:12:33,860
instead of returning the cursor will

00:12:31,399 --> 00:12:36,290
return the very first match that it gets

00:12:33,860 --> 00:12:40,940
back so it's kind of very simple to use

00:12:36,290 --> 00:12:42,110
there some of the things that or you may

00:12:40,940 --> 00:12:45,290
have noticed here we have something

00:12:42,110 --> 00:12:47,899
called underscore ID and object ID and

00:12:45,290 --> 00:12:50,540
in my previous example we had date/time

00:12:47,899 --> 00:12:52,910
objects as well for dealing with dates

00:12:50,540 --> 00:12:56,690
and things none of these actually exist

00:12:52,910 --> 00:12:59,870
in Jason so the Jason spec is very

00:12:56,690 --> 00:13:02,630
simple has arrays nulls billions numbers

00:12:59,870 --> 00:13:05,089
and strings and maybe a couple more but

00:13:02,630 --> 00:13:07,670
to be able to add and deal with other

00:13:05,089 --> 00:13:09,680
types of data like binary data date time

00:13:07,670 --> 00:13:12,589
stamps and things are short integers

00:13:09,680 --> 00:13:15,649
long integers and floats and stuff we

00:13:12,589 --> 00:13:18,110
used Beeson which is a superset of jason

00:13:15,649 --> 00:13:21,079
and it's also how we store the data and

00:13:18,110 --> 00:13:22,760
on disk it has its own spec if you're

00:13:21,079 --> 00:13:24,740
interested in understanding more about

00:13:22,760 --> 00:13:27,399
how it stores the data you can go to the

00:13:24,740 --> 00:13:29,899
website and check it out if you want to

00:13:27,399 --> 00:13:32,510
but what it gives us essentially is a

00:13:29,899 --> 00:13:34,490
richer way to describe our data to for

00:13:32,510 --> 00:13:36,199
dates to be dates and for binary objects

00:13:34,490 --> 00:13:38,300
to be binary and not have to do some

00:13:36,199 --> 00:13:38,610
sort of manual mapping to decide or look

00:13:38,300 --> 00:13:41,220
in

00:13:38,610 --> 00:13:45,690
- and converts strings into date objects

00:13:41,220 --> 00:13:50,610
and such so once you have this data in

00:13:45,690 --> 00:13:52,829
ah how do you go and find it and this is

00:13:50,610 --> 00:13:54,000
where it's really flexible but it is

00:13:52,829 --> 00:13:56,040
very different from the sequel world

00:13:54,000 --> 00:13:58,290
there's no sequel or query language as

00:13:56,040 --> 00:14:00,720
such it's all declarative you provide a

00:13:58,290 --> 00:14:03,000
document example of how you want to

00:14:00,720 --> 00:14:05,160
query so in Python it's a dictionary and

00:14:03,000 --> 00:14:07,050
you just give it the examples to run so

00:14:05,160 --> 00:14:09,420
here we're doing a direct match for

00:14:07,050 --> 00:14:11,730
school and then later on we're doing a

00:14:09,420 --> 00:14:15,240
range match so we're doing score greater

00:14:11,730 --> 00:14:20,220
than or equals to 60 or less than or

00:14:15,240 --> 00:14:21,990
equals to 70 as MongoDB knows and

00:14:20,220 --> 00:14:23,820
understands what you what's inside your

00:14:21,990 --> 00:14:26,190
objects as well you can go and query at

00:14:23,820 --> 00:14:28,350
any level if you have nested like

00:14:26,190 --> 00:14:29,310
documents in an array for example you

00:14:28,350 --> 00:14:32,130
can use something called the dot

00:14:29,310 --> 00:14:33,750
operator so in the previous example we

00:14:32,130 --> 00:14:36,089
had posts and comments and comments as

00:14:33,750 --> 00:14:38,550
array you could just do post comments to

00:14:36,089 --> 00:14:42,269
author and you can match any hosts or

00:14:38,550 --> 00:14:43,620
documents that match there again sorting

00:14:42,269 --> 00:14:46,709
is very similar to how you would sort

00:14:43,620 --> 00:14:48,600
stuff in a relational world you can sort

00:14:46,709 --> 00:14:51,839
by a single field multiple fields that's

00:14:48,600 --> 00:14:53,490
fine and again you can paginate and you

00:14:51,839 --> 00:14:54,870
can use things like skips and limits to

00:14:53,490 --> 00:14:58,320
paginate through your result sets as

00:14:54,870 --> 00:15:00,690
well so it kind of wants to have almost

00:14:58,320 --> 00:15:02,550
all the features of the relational

00:15:00,690 --> 00:15:04,290
database in terms of being able to do

00:15:02,550 --> 00:15:09,560
adult queries find your data and

00:15:04,290 --> 00:15:09,560
discover your data yes sir

00:15:14,980 --> 00:15:20,980
you consider using keyword arguments for

00:15:17,079 --> 00:15:26,680
functions like find they lie and Django

00:15:20,980 --> 00:15:29,410
like dunder GT dunder LTE for lookups so

00:15:26,680 --> 00:15:31,630
in the core library no it will map more

00:15:29,410 --> 00:15:33,399
how to use it in currently in the

00:15:31,630 --> 00:15:35,709
JavaScript shell and keep it kind of

00:15:33,399 --> 00:15:38,320
synonymous between all the languages in

00:15:35,709 --> 00:15:41,380
my ends in the ORM it is a

00:15:38,320 --> 00:15:42,790
keywords like double underscore for less

00:15:41,380 --> 00:15:44,860
than and that kind of language as well

00:15:42,790 --> 00:15:46,449
and there are other mappers that do and

00:15:44,860 --> 00:15:49,300
abstract it out to that kind of level

00:15:46,449 --> 00:15:52,720
but by default it is you document by

00:15:49,300 --> 00:15:54,699
example for nearly every parts of the

00:15:52,720 --> 00:16:00,639
query language in fact for every part

00:15:54,699 --> 00:16:02,380
that we're sorry' so updating data so

00:16:00,639 --> 00:16:04,570
getting in today putting data into your

00:16:02,380 --> 00:16:06,790
application is relatively simple updates

00:16:04,570 --> 00:16:09,699
and data is very similar it again so you

00:16:06,790 --> 00:16:11,350
will pass a query in so what you want to

00:16:09,699 --> 00:16:13,470
update the match part and the second

00:16:11,350 --> 00:16:16,779
part is how you want to update it the

00:16:13,470 --> 00:16:18,399
first Scott here we have here is kind of

00:16:16,779 --> 00:16:21,360
unexpected to a lot of people so the

00:16:18,399 --> 00:16:24,790
very first example here we're updating a

00:16:21,360 --> 00:16:27,250
object or a document has underscore ID

00:16:24,790 --> 00:16:30,610
of one two three and we're updating it

00:16:27,250 --> 00:16:32,019
so score is 80 now people don't it does

00:16:30,610 --> 00:16:34,060
what it does there as it replaces the

00:16:32,019 --> 00:16:36,310
whole document and makes a document to

00:16:34,060 --> 00:16:38,980
be what that second part the document is

00:16:36,310 --> 00:16:42,040
score is 80 that generally is not what

00:16:38,980 --> 00:16:44,110
people want so to do this we have

00:16:42,040 --> 00:16:46,779
something called atomic operators which

00:16:44,110 --> 00:16:50,440
is a where we can do things like sets

00:16:46,779 --> 00:16:53,980
Sun sets pushes and pulls to deal with

00:16:50,440 --> 00:16:55,949
and set just parts of your data so it

00:16:53,980 --> 00:16:58,600
may be common you'll do something like

00:16:55,949 --> 00:17:00,069
go and update a whole document in the

00:16:58,600 --> 00:17:02,709
very first example and you go well

00:17:00,069 --> 00:17:04,780
where's my grade problem gone or where's

00:17:02,709 --> 00:17:06,069
my grade fields gone and it's because if

00:17:04,780 --> 00:17:08,140
you just use an update we pass a

00:17:06,069 --> 00:17:13,179
document it will replace everything with

00:17:08,140 --> 00:17:16,929
them and also behind the scenes save

00:17:13,179 --> 00:17:18,490
does this as well and so if you do save

00:17:16,929 --> 00:17:20,589
and you have an ID it will actually

00:17:18,490 --> 00:17:22,360
convert it to be the very same example

00:17:20,589 --> 00:17:25,480
as the top one which generally is not

00:17:22,360 --> 00:17:27,370
what people want so when we come to

00:17:25,480 --> 00:17:28,569
atomic updates what do you have you have

00:17:27,370 --> 00:17:31,539
things like sets justice

00:17:28,569 --> 00:17:32,950
that feels unset and set them because we

00:17:31,539 --> 00:17:34,299
have a raise as well you have to be able

00:17:32,950 --> 00:17:36,070
to deal with them you don't want to just

00:17:34,299 --> 00:17:37,570
set a whole array because you might get

00:17:36,070 --> 00:17:39,490
a race condition someone else might be

00:17:37,570 --> 00:17:40,929
trying to do at the same time if you try

00:17:39,490 --> 00:17:42,610
to save the whole array or two things

00:17:40,929 --> 00:17:44,649
did you would have a collision and one

00:17:42,610 --> 00:17:46,929
would win so you have some operators

00:17:44,649 --> 00:17:48,639
called dollar push to push on to the

00:17:46,929 --> 00:17:52,690
array and again dollar pull to remove

00:17:48,639 --> 00:17:54,490
from the array then there's some other

00:17:52,690 --> 00:17:56,740
flags that you can do for the examples

00:17:54,490 --> 00:17:58,720
here you can by default it'll only

00:17:56,740 --> 00:18:00,370
update the first document that matches

00:17:58,720 --> 00:18:02,200
you can do a multiple update by passing

00:18:00,370 --> 00:18:05,559
in the keyword arguments multi is true

00:18:02,200 --> 00:18:07,690
and you can also use an upset by passing

00:18:05,559 --> 00:18:10,269
in upset is true so if it doesn't match

00:18:07,690 --> 00:18:12,159
it will then go and create it as well so

00:18:10,269 --> 00:18:14,529
in some levels of flexibility there in

00:18:12,159 --> 00:18:20,710
terms of being able to query and update

00:18:14,529 --> 00:18:23,289
your data briefly indexing in MongoDB is

00:18:20,710 --> 00:18:25,419
very similar to indexing in in sequel

00:18:23,289 --> 00:18:28,360
all the indexes are stored as B trees

00:18:25,419 --> 00:18:31,120
you can do single field indexes compound

00:18:28,360 --> 00:18:34,750
indexes and there's also something

00:18:31,120 --> 00:18:36,639
called a geo index as well Gon Dixie's

00:18:34,750 --> 00:18:38,620
saw things on the TD plane they either

00:18:36,639 --> 00:18:40,269
have a flat world model when you go in

00:18:38,620 --> 00:18:43,149
query against them or you can have a

00:18:40,269 --> 00:18:45,370
simplified round world model so in terms

00:18:43,149 --> 00:18:48,370
of like doing geographical lookups in

00:18:45,370 --> 00:18:49,960
MongoDB it's only point based so you can

00:18:48,370 --> 00:18:52,149
do a point in polygon or point in

00:18:49,960 --> 00:18:54,789
boundary box it doesn't have all the

00:18:52,149 --> 00:18:56,590
features that you do in hasti is where

00:18:54,789 --> 00:18:58,210
you can deal with polygons and /

00:18:56,590 --> 00:19:01,149
sections and intersections of polygons

00:18:58,210 --> 00:19:03,549
as well so it's kind of a top level very

00:19:01,149 --> 00:19:06,309
simple and behind the scenes the Geo

00:19:03,549 --> 00:19:10,649
index is actually also a b-tree it

00:19:06,309 --> 00:19:13,960
basically does a geo hash of your data

00:19:10,649 --> 00:19:17,169
if you have multiple fields because

00:19:13,960 --> 00:19:19,120
dictionaries in MongoDB don't preserve

00:19:17,169 --> 00:19:21,070
order you have to use this array syntax

00:19:19,120 --> 00:19:23,799
that we have at the top there and then

00:19:21,070 --> 00:19:24,820
just pass in tuples with what field and

00:19:23,799 --> 00:19:31,419
then that's ascending or descending

00:19:24,820 --> 00:19:33,370
order be able to understand what's going

00:19:31,419 --> 00:19:35,110
on with your database and if you do need

00:19:33,370 --> 00:19:37,179
an index and stuff you can have a look

00:19:35,110 --> 00:19:39,549
at the query plan which uses that I

00:19:37,179 --> 00:19:40,419
explain after your query will they go

00:19:39,549 --> 00:19:42,250
and tell you and give you some

00:19:40,419 --> 00:19:45,850
statistics about what that queries

00:19:42,250 --> 00:19:48,910
doing and how it's using what curses

00:19:45,850 --> 00:19:50,590
it's using so in this example here we're

00:19:48,910 --> 00:19:51,820
using a basic cursor which means we're

00:19:50,590 --> 00:19:53,830
doing a full table scan

00:19:51,820 --> 00:19:57,370
there wasn't an index for this query for

00:19:53,830 --> 00:19:59,080
it to use and you get some statistics

00:19:57,370 --> 00:20:01,240
back like milliseconds how long it took

00:19:59,080 --> 00:20:03,790
this is relatively small how many

00:20:01,240 --> 00:20:06,610
objects it returned back how many how

00:20:03,790 --> 00:20:08,230
many indexes it scanned or how many

00:20:06,610 --> 00:20:10,330
objects on disk it had to go and read

00:20:08,230 --> 00:20:15,340
through so as you scale and as you get

00:20:10,330 --> 00:20:17,710
larger and larger documents and data

00:20:15,340 --> 00:20:19,600
sets then you can use this explained

00:20:17,710 --> 00:20:21,610
plan to find and try and debug some of

00:20:19,600 --> 00:20:23,410
your slow queries exactly like you would

00:20:21,610 --> 00:20:27,040
do in a sequel when you go and debug

00:20:23,410 --> 00:20:28,990
your sequel statement be able to

00:20:27,040 --> 00:20:30,550
understand what queries are slow for you

00:20:28,990 --> 00:20:33,460
there's also a profiler

00:20:30,550 --> 00:20:35,230
that you can turn on just you probably

00:20:33,460 --> 00:20:36,790
use the JavaScript cram to do this just

00:20:35,230 --> 00:20:40,240
in the shell to turn on for a period of

00:20:36,790 --> 00:20:42,880
time and by default is off you can set

00:20:40,240 --> 00:20:45,100
it to be log any queries that are under

00:20:42,880 --> 00:20:47,260
that take longer than 100 milliseconds

00:20:45,100 --> 00:20:48,850
or pass in your own threshold or you can

00:20:47,260 --> 00:20:50,620
have it on and flood and record every

00:20:48,850 --> 00:20:53,980
single query as well and do something

00:20:50,620 --> 00:20:55,030
that analytics on that generally I would

00:20:53,980 --> 00:20:57,130
only do that in a testing environment

00:20:55,030 --> 00:20:59,920
but it will give you information about

00:20:57,130 --> 00:21:01,240
about what queries are slow it stores

00:20:59,920 --> 00:21:02,950
them all in another collection so you

00:21:01,240 --> 00:21:05,080
can then go and query that collection or

00:21:02,950 --> 00:21:08,430
them by how long they took and start

00:21:05,080 --> 00:21:08,430
debugging some of your slow queries

00:21:08,610 --> 00:21:12,100
right I think the final part we're going

00:21:10,600 --> 00:21:16,510
to talk about in terms of functionality

00:21:12,100 --> 00:21:18,970
and using in Python is grid FS green FS

00:21:16,510 --> 00:21:22,530
is basically a convention built on top

00:21:18,970 --> 00:21:25,060
of MongoDB that all the drivers now are

00:21:22,530 --> 00:21:27,070
maintained and enforced which allows you

00:21:25,060 --> 00:21:30,580
to use the document database as a file

00:21:27,070 --> 00:21:32,080
store and the way it does this it has it

00:21:30,580 --> 00:21:34,060
basically creates two collections in the

00:21:32,080 --> 00:21:36,220
background one for storing metadata

00:21:34,060 --> 00:21:38,050
about the file the file name and you can

00:21:36,220 --> 00:21:41,230
store any other metadata you want the

00:21:38,050 --> 00:21:43,450
mime type for example and how big the

00:21:41,230 --> 00:21:44,830
file is or whatever you need and then

00:21:43,450 --> 00:21:46,810
there's another collection in the

00:21:44,830 --> 00:21:49,870
background where the file gets chunked

00:21:46,810 --> 00:21:51,670
up into small chunks and then you can

00:21:49,870 --> 00:21:53,890
just go and read back from them and the

00:21:51,670 --> 00:21:54,850
driver itself although they pass through

00:21:53,890 --> 00:21:57,400
each trunk

00:21:54,850 --> 00:21:59,800
and back and streaming them back so here

00:21:57,400 --> 00:22:01,720
we have an example it's exactly the same

00:21:59,800 --> 00:22:04,090
or should be the same interface is how

00:22:01,720 --> 00:22:06,520
you would save a file if you're just

00:22:04,090 --> 00:22:08,230
opening a file and reading to it in

00:22:06,520 --> 00:22:11,430
Python but you just have to make sure

00:22:08,230 --> 00:22:13,900
you connect to grid FS to begin with

00:22:11,430 --> 00:22:15,400
it's not the most performant of file

00:22:13,900 --> 00:22:17,530
systems obviously there are better

00:22:15,400 --> 00:22:20,740
network solutions out there but if you

00:22:17,530 --> 00:22:22,570
wanted a distributed orderable file

00:22:20,740 --> 00:22:23,230
system for small levels of files and

00:22:22,570 --> 00:22:25,180
it's fine

00:22:23,230 --> 00:22:26,650
reading back to them you can just read

00:22:25,180 --> 00:22:28,180
back in your application obviously

00:22:26,650 --> 00:22:29,950
you're going to have to go into your

00:22:28,180 --> 00:22:31,920
application and stream it back out that

00:22:29,950 --> 00:22:35,290
may not be the most performant for you

00:22:31,920 --> 00:22:37,750
but there are things like plug-ins for

00:22:35,290 --> 00:22:39,430
nginx so it will speak directly to grid

00:22:37,750 --> 00:22:42,730
FS which is orders of magnitude faster

00:22:39,430 --> 00:22:44,470
than doing it via Python or people also

00:22:42,730 --> 00:22:46,390
put them behind a varnish catch and

00:22:44,470 --> 00:22:49,210
cache them there as well it kind of

00:22:46,390 --> 00:22:50,710
depends on your needs it is maybe an

00:22:49,210 --> 00:22:54,420
itch problem for you but it may allow

00:22:50,710 --> 00:22:54,420
some flexibility for you as well

00:22:55,050 --> 00:23:00,540
right so before we go on and have a look

00:22:58,720 --> 00:23:03,340
at kind of some of the scaling

00:23:00,540 --> 00:23:06,520
attributes of MongoDB we're going to

00:23:03,340 --> 00:23:08,170
talk about object data mappers we're

00:23:06,520 --> 00:23:11,040
asking this date space so why might you

00:23:08,170 --> 00:23:14,320
have them why do you need them well

00:23:11,040 --> 00:23:16,390
generally the main reason is if you were

00:23:14,320 --> 00:23:18,160
working a large team you want some way

00:23:16,390 --> 00:23:19,870
of documenting your schemas anyway there

00:23:18,160 --> 00:23:22,000
may be a load of flexibility in there

00:23:19,870 --> 00:23:23,800
you may or may not have fields or have

00:23:22,000 --> 00:23:25,660
needs to have fields depending on you

00:23:23,800 --> 00:23:27,970
and depending on what you're trying to

00:23:25,660 --> 00:23:29,560
model but at the end of day it's not

00:23:27,970 --> 00:23:31,150
chaos you're not throwing everything in

00:23:29,560 --> 00:23:32,980
there people will need to know what they

00:23:31,150 --> 00:23:35,050
should look for otherwise you could

00:23:32,980 --> 00:23:36,520
attempt to potentially lose data so this

00:23:35,050 --> 00:23:40,480
is one of the reasons why people may

00:23:36,520 --> 00:23:42,400
choose a no DM as well they do things

00:23:40,480 --> 00:23:44,530
like data validation so if you have a

00:23:42,400 --> 00:23:46,270
date field it'll ensure that you pass in

00:23:44,530 --> 00:23:48,340
date times phones and stuff like that as

00:23:46,270 --> 00:23:50,860
well so it can be used for that side of

00:23:48,340 --> 00:23:52,630
things and they enforce schema and

00:23:50,860 --> 00:23:54,610
potentially you can dry up your code you

00:23:52,630 --> 00:23:56,200
may be able to generate forms from them

00:23:54,610 --> 00:24:00,130
because you've defined what you expect

00:23:56,200 --> 00:24:01,540
them to be say so there are a number of

00:24:00,130 --> 00:24:03,310
reasons why you should use them they're

00:24:01,540 --> 00:24:05,680
never going to be as performant as pi

00:24:03,310 --> 00:24:07,059
 I maintain one called engine

00:24:05,680 --> 00:24:08,649
it's not performing as prime

00:24:07,059 --> 00:24:10,570
Luongo its orders of magnitude slower

00:24:08,649 --> 00:24:13,029
but it may allow you to iterate and

00:24:10,570 --> 00:24:16,690
procite quicker so it's kind of up to

00:24:13,029 --> 00:24:18,549
you which one's V if you want to use one

00:24:16,690 --> 00:24:20,590
and as I mentioned there are lots of

00:24:18,549 --> 00:24:22,090
options these are the these are the ones

00:24:20,590 --> 00:24:25,389
that are mentioned on the pymongo

00:24:22,090 --> 00:24:26,590
website I know there are a few more so

00:24:25,389 --> 00:24:29,169
if anyone else knows the one that's

00:24:26,590 --> 00:24:30,490
maintained send the pull requests or

00:24:29,169 --> 00:24:33,580
open a ticket and I would like to add it

00:24:30,490 --> 00:24:35,379
to the list as well but and there are

00:24:33,580 --> 00:24:36,820
different flavors so if you like sequel

00:24:35,379 --> 00:24:38,860
alchemy and you're used to doing that

00:24:36,820 --> 00:24:41,080
means alchemy because it's kind of

00:24:38,860 --> 00:24:42,429
inspired by the same API if you want a

00:24:41,080 --> 00:24:45,279
lightweight one there are lighter weight

00:24:42,429 --> 00:24:48,309
ones like humongous and mini as

00:24:45,279 --> 00:24:52,029
well and some allow you to use the

00:24:48,309 --> 00:24:54,249
native syntax for querying like ming and

00:24:52,029 --> 00:24:56,289
then others actually produce their own

00:24:54,249 --> 00:24:59,769
query language like engine that

00:24:56,289 --> 00:25:01,690
uses like the Django ORM style as well

00:24:59,769 --> 00:25:05,789
so I can't really I'm not qualified to

00:25:01,690 --> 00:25:10,960
say which one's best but I like mine so

00:25:05,789 --> 00:25:13,179
the best way to learn more about MongoDB

00:25:10,960 --> 00:25:15,159
and Python or MongoDB in general is

00:25:13,179 --> 00:25:17,320
actually to go out download it and play

00:25:15,159 --> 00:25:19,690
with it so I mean you can any system

00:25:17,320 --> 00:25:21,879
that you run if you're on Windows Mac or

00:25:19,690 --> 00:25:23,559
Linux you'll be able to download it and

00:25:21,879 --> 00:25:25,269
download the database and put on your

00:25:23,559 --> 00:25:28,539
machine you should be able to connect to

00:25:25,269 --> 00:25:31,480
it so there is a tutorial in the PI

00:25:28,539 --> 00:25:33,970
 Docs and there's also the Europe -

00:25:31,480 --> 00:25:37,149
workshops which are also available on

00:25:33,970 --> 00:25:38,559
the files e.p the internal network

00:25:37,149 --> 00:25:39,610
internet here as well

00:25:38,559 --> 00:25:43,419
if you want to have a look there's some

00:25:39,610 --> 00:25:47,679
PDFs there that you can run through so I

00:25:43,419 --> 00:25:49,240
would advise to learn more about it just

00:25:47,679 --> 00:25:51,279
going to go and play with it it'd be

00:25:49,240 --> 00:25:53,169
very drive me to go and try and talk

00:25:51,279 --> 00:25:54,639
about building a whole application as an

00:25:53,169 --> 00:25:57,369
example because it would be a nice

00:25:54,639 --> 00:25:59,230
problem and solving it so hopefully I've

00:25:57,369 --> 00:26:03,610
given you some idea of kind of how you

00:25:59,230 --> 00:26:06,279
can query it also there are a few

00:26:03,610 --> 00:26:09,519
tutorials as well with Python one using

00:26:06,279 --> 00:26:11,409
Django nan rail and trying to you create

00:26:09,519 --> 00:26:13,179
a tumble log that I've written there

00:26:11,409 --> 00:26:14,230
that's in the main documentation and the

00:26:13,179 --> 00:26:16,990
tutorials as well

00:26:14,230 --> 00:26:19,840
anyone wants to supply one I'd gladly

00:26:16,990 --> 00:26:22,240
take patches and allow them to be in and

00:26:19,840 --> 00:26:24,520
turn into MongoDB docks I've written one

00:26:22,240 --> 00:26:26,230
on using flask for example and

00:26:24,520 --> 00:26:28,360
engine as well and it will walk you

00:26:26,230 --> 00:26:31,720
through building a full tumble log and

00:26:28,360 --> 00:26:33,730
get that up and running so to find them

00:26:31,720 --> 00:26:35,470
just go to doc stock MongoDB org and

00:26:33,730 --> 00:26:40,840
then have a look for tutorial he'll be

00:26:35,470 --> 00:26:42,880
there so the next aspects are talking

00:26:40,840 --> 00:26:44,169
about MongoDB are and to help you

00:26:42,880 --> 00:26:46,870
understand when you come to build your

00:26:44,169 --> 00:26:49,620
own apps is to talk about how the

00:26:46,870 --> 00:26:53,020
durability and how scaling works with it

00:26:49,620 --> 00:26:55,630
now not everyone and there's no reason

00:26:53,020 --> 00:26:57,880
why having a single database is bad or

00:26:55,630 --> 00:26:59,500
wrong I used to have one for just my

00:26:57,880 --> 00:27:01,510
paragraphs and different things as well

00:26:59,500 --> 00:27:02,740
and as long as I had it backed up and

00:27:01,510 --> 00:27:04,860
it's happy to the recovery

00:27:02,740 --> 00:27:07,240
everything was fine within my company

00:27:04,860 --> 00:27:09,100
but if you do have other requirements

00:27:07,240 --> 00:27:12,279
and other needs for your companies or

00:27:09,100 --> 00:27:14,890
whatever for your projects then the

00:27:12,279 --> 00:27:19,380
first port of call is replication which

00:27:14,890 --> 00:27:21,210
provides high level of durability they

00:27:19,380 --> 00:27:23,980
understand now replication works

00:27:21,210 --> 00:27:25,720
fundamentally MongoDB with replication

00:27:23,980 --> 00:27:27,130
is a single master system so there's

00:27:25,720 --> 00:27:29,799
only one place you will ever write to

00:27:27,130 --> 00:27:33,789
and then you'll have multiple slaves or

00:27:29,799 --> 00:27:35,649
claim the data off and what one of the

00:27:33,789 --> 00:27:38,289
nice features about replication is it

00:27:35,649 --> 00:27:40,840
handles or should handle automatic

00:27:38,289 --> 00:27:42,429
failover and automatic recovery as long

00:27:40,840 --> 00:27:44,470
as there is enough data for it to be

00:27:42,429 --> 00:27:46,179
able to catch up if a node goes down for

00:27:44,470 --> 00:27:47,799
a period of time as long as there's

00:27:46,179 --> 00:27:49,929
enough data on the other primary node

00:27:47,799 --> 00:27:52,299
for it to be able to catch up it will do

00:27:49,929 --> 00:27:57,010
and will come back into the replicas set

00:27:52,299 --> 00:27:58,419
automatically we'll talk about some of

00:27:57,010 --> 00:28:01,179
the advanced features in it in a moment

00:27:58,419 --> 00:28:04,990
but if you have three nodes for example

00:28:01,179 --> 00:28:08,500
as we do here we'll talk about the

00:28:04,990 --> 00:28:10,809
failures or fail oh this of it instantly

00:28:08,500 --> 00:28:13,090
you have to have tinos for replicas sets

00:28:10,809 --> 00:28:15,100
two or more notes but if you have two

00:28:13,090 --> 00:28:18,059
nodes you can never if one goes down

00:28:15,100 --> 00:28:20,500
you'll only ever going to read only so

00:28:18,059 --> 00:28:22,870
that's something to be wary of you have

00:28:20,500 --> 00:28:25,539
to have a majority of nodes visible so

00:28:22,870 --> 00:28:27,730
above 50% to be able to win an election

00:28:25,539 --> 00:28:29,789
to be able to have a primary if you

00:28:27,730 --> 00:28:33,220
don't have above 50% you'll be read-only

00:28:29,789 --> 00:28:36,070
so what happens when you set up you

00:28:33,220 --> 00:28:38,200
in the flag to the D you'll say

00:28:36,070 --> 00:28:39,970
use replication it would be the repple

00:28:38,200 --> 00:28:41,740
set flag you'll give a name and you'll

00:28:39,970 --> 00:28:44,200
add some config to say where your nodes

00:28:41,740 --> 00:28:46,360
are and what will happen when you bring

00:28:44,200 --> 00:28:49,030
them up it will initiate and call an

00:28:46,360 --> 00:28:50,800
election and the primary will be elected

00:28:49,030 --> 00:28:52,600
and it's a consensus election the

00:28:50,800 --> 00:28:54,910
primary is voted on who has the most

00:28:52,600 --> 00:28:56,590
up-to-date data and then some level of

00:28:54,910 --> 00:28:59,920
prioritization as well you can assign to

00:28:56,590 --> 00:29:01,810
your nodes and the main premise is if a

00:28:59,920 --> 00:29:03,100
node goes down for whatever reason you

00:29:01,810 --> 00:29:04,870
might taking it down it might have a

00:29:03,100 --> 00:29:07,360
failure it might be rebooting you might

00:29:04,870 --> 00:29:10,810
be upgrading you might be whatever if a

00:29:07,360 --> 00:29:13,150
node goes down then there is above 50%

00:29:10,810 --> 00:29:15,220
left up and a running that can see each

00:29:13,150 --> 00:29:18,700
other then between them they'll

00:29:15,220 --> 00:29:20,620
negotiate a new primary node and it will

00:29:18,700 --> 00:29:24,490
be based on who is the most up-to-date

00:29:20,620 --> 00:29:29,680
as I say and then while they're doing

00:29:24,490 --> 00:29:31,390
this negotiation they are in secondary

00:29:29,680 --> 00:29:34,330
so your application is in a read-only

00:29:31,390 --> 00:29:36,280
state you can't accept rights when you

00:29:34,330 --> 00:29:38,490
have a primary the secondary will start

00:29:36,280 --> 00:29:41,260
copying the data over again and

00:29:38,490 --> 00:29:43,510
hopefully your other node will come back

00:29:41,260 --> 00:29:46,450
up you rebooted you've upgraded the OS

00:29:43,510 --> 00:29:48,940
whatever and if there is enough data in

00:29:46,450 --> 00:29:51,370
the primary still using its operations

00:29:48,940 --> 00:29:53,320
log so if the secondary that went down

00:29:51,370 --> 00:29:55,120
is within the window of opportunities to

00:29:53,320 --> 00:29:58,090
be to be able to catch up it will

00:29:55,120 --> 00:29:58,930
automatically catch up and it will do

00:29:58,090 --> 00:30:01,930
this and go into something called

00:29:58,930 --> 00:30:03,670
recovery mode if it can't catch up you

00:30:01,930 --> 00:30:05,290
will have to manually bring it back up

00:30:03,670 --> 00:30:08,800
it's not magic you'll have to manually

00:30:05,290 --> 00:30:10,480
either get a backup or blow away the

00:30:08,800 --> 00:30:12,670
data and allow it to stream back over

00:30:10,480 --> 00:30:15,550
them in the primary so but in the most

00:30:12,670 --> 00:30:17,500
run during normal operations it should

00:30:15,550 --> 00:30:20,320
be able to recover naturally and

00:30:17,500 --> 00:30:21,700
automatically and there you go and then

00:30:20,320 --> 00:30:25,120
you have your your replica set

00:30:21,700 --> 00:30:27,070
re-established so there isn't it's not

00:30:25,120 --> 00:30:28,780
magic it's just that's how it does it

00:30:27,070 --> 00:30:30,520
tails what operations are happening on

00:30:28,780 --> 00:30:32,260
the primary so what updates happening on

00:30:30,520 --> 00:30:35,980
the master and then applies them to

00:30:32,260 --> 00:30:37,420
themselves there are some advanced

00:30:35,980 --> 00:30:39,370
features that you can do this so you can

00:30:37,420 --> 00:30:41,410
take this quite far and there is more

00:30:39,370 --> 00:30:43,600
control over this in MongoDB than

00:30:41,410 --> 00:30:46,030
probably anywhere else so you can do

00:30:43,600 --> 00:30:46,960
things like you can add extra durability

00:30:46,030 --> 00:30:50,320
when you do

00:30:46,960 --> 00:30:52,240
right so when you do an update MongoDB

00:30:50,320 --> 00:30:53,860
by default it does something called a

00:30:52,240 --> 00:30:56,020
fire-and-forget right which is a

00:30:53,860 --> 00:30:57,820
synchronous it doesn't even know if the

00:30:56,020 --> 00:31:00,280
database got that update or that insert

00:30:57,820 --> 00:31:03,640
which is probably not what you want and

00:31:00,280 --> 00:31:06,700
isn't the best of ugly isn't the best of

00:31:03,640 --> 00:31:07,990
defaults but it is what is so most

00:31:06,700 --> 00:31:09,730
people will do something like they'll

00:31:07,990 --> 00:31:13,120
say I want it to be applied to memory

00:31:09,730 --> 00:31:14,770
and this scales all the way up to I want

00:31:13,120 --> 00:31:16,630
to ensure that it hits two of my

00:31:14,770 --> 00:31:19,360
replicas second sets so if I

00:31:16,630 --> 00:31:21,940
accidentally lost one of my nodes I want

00:31:19,360 --> 00:31:23,530
to ensure that the data exists on at

00:31:21,940 --> 00:31:25,870
least one of the other nodes so I don't

00:31:23,530 --> 00:31:27,790
lose that node even if there is a severe

00:31:25,870 --> 00:31:29,860
hardware failure you will have that

00:31:27,790 --> 00:31:32,230
level of durability obviously it's going

00:31:29,860 --> 00:31:34,060
to block while it waits for the replicas

00:31:32,230 --> 00:31:36,970
set the secondary to apply that

00:31:34,060 --> 00:31:40,450
operation so you can go from totally

00:31:36,970 --> 00:31:42,430
non-blocking to being and lots of

00:31:40,450 --> 00:31:44,860
blocking essentially or longer times of

00:31:42,430 --> 00:31:47,230
blocking you can do other things like

00:31:44,860 --> 00:31:48,430
prioritization you can say and I'll give

00:31:47,230 --> 00:31:51,250
an example in a moment you can put you

00:31:48,430 --> 00:31:53,350
can prefer to some nodes and to others

00:31:51,250 --> 00:31:54,850
and potentially you can scale out your

00:31:53,350 --> 00:31:58,090
reads you can read directly from the

00:31:54,850 --> 00:32:01,210
secondaries but the secondaries may not

00:31:58,090 --> 00:32:03,700
be upstate so here's an example of a set

00:32:01,210 --> 00:32:06,340
up that we might have this is a 5 node

00:32:03,700 --> 00:32:08,830
replica set we have one that has a

00:32:06,340 --> 00:32:10,810
priority of 0 in the USA in this example

00:32:08,830 --> 00:32:12,730
we have our primary data center that's

00:32:10,810 --> 00:32:14,830
in the EU so we give them a priority of

00:32:12,730 --> 00:32:16,900
10 so we're always going to prefer that

00:32:14,830 --> 00:32:18,610
because most of our users are there and

00:32:16,900 --> 00:32:19,810
then the USA one might actually just be

00:32:18,610 --> 00:32:22,300
another data center it doesn't really

00:32:19,810 --> 00:32:25,690
matter but that one has a lower priority

00:32:22,300 --> 00:32:28,360
so we could prefer if something happens

00:32:25,690 --> 00:32:31,240
to the EU node if that availability zone

00:32:28,360 --> 00:32:32,440
went down I would still want ok I still

00:32:31,240 --> 00:32:34,810
want to be able to cover it and be able

00:32:32,440 --> 00:32:36,640
to handle so then the p5 would be the

00:32:34,810 --> 00:32:39,070
option and you can still use that in the

00:32:36,640 --> 00:32:40,510
USA and here's an example of what a lot

00:32:39,070 --> 00:32:43,240
of people will do they'll have a node

00:32:40,510 --> 00:32:44,980
that sets priorities Eero which means it

00:32:43,240 --> 00:32:46,630
can never become the master because they

00:32:44,980 --> 00:32:48,610
may do secondary things on it they may

00:32:46,630 --> 00:32:50,700
use it for backups they may use it for

00:32:48,610 --> 00:32:53,350
reporting analytics that kind of thing

00:32:50,700 --> 00:32:55,570
there's a lot of documentation a lot of

00:32:53,350 --> 00:32:57,670
other presentations that go into a lot

00:32:55,570 --> 00:32:59,740
of depth about replication about

00:32:57,670 --> 00:33:00,120
different levels of durability and stuff

00:32:59,740 --> 00:33:01,950
so

00:33:00,120 --> 00:33:03,660
if you're interested in that you'll need

00:33:01,950 --> 00:33:05,100
do just read up about it

00:33:03,660 --> 00:33:08,690
it's it's all there and it's all

00:33:05,100 --> 00:33:11,040
available on the Tenjin Doc's website

00:33:08,690 --> 00:33:12,870
the next part is sharding I'm going to

00:33:11,040 --> 00:33:15,200
cover this in even less depth because

00:33:12,870 --> 00:33:18,720
this is kind of levels of scaling that

00:33:15,200 --> 00:33:21,600
if you need it it's kind of conceptually

00:33:18,720 --> 00:33:23,880
easy to do but it can be expensive a lot

00:33:21,600 --> 00:33:25,740
of people may do it prematurely because

00:33:23,880 --> 00:33:27,630
it's easy to set up and it's fun to have

00:33:25,740 --> 00:33:29,610
lots of boxes but if you don't have a

00:33:27,630 --> 00:33:31,650
level of scale that needs it and you

00:33:29,610 --> 00:33:36,780
just want durability then replication is

00:33:31,650 --> 00:33:39,120
probably your best bet so scaling out I

00:33:36,780 --> 00:33:41,100
only go in theory you can go from a

00:33:39,120 --> 00:33:42,990
single node which is fine which gives

00:33:41,100 --> 00:33:45,000
you that's bottleneck you can only go as

00:33:42,990 --> 00:33:46,470
big as that node and go then you can go

00:33:45,000 --> 00:33:47,850
to replica sets where you'd have one

00:33:46,470 --> 00:33:49,860
primary and potentially you could breed

00:33:47,850 --> 00:33:51,450
from the secondaries and then you can go

00:33:49,860 --> 00:33:54,990
through and you can basically put a

00:33:51,450 --> 00:33:56,790
layer of sharding on sharding is you

00:33:54,990 --> 00:33:58,200
shot a single collection and you say I

00:33:56,790 --> 00:34:01,350
want to split this collection up by a

00:33:58,200 --> 00:34:03,750
key there is some granular detail of

00:34:01,350 --> 00:34:05,520
what you need to do there but it's best

00:34:03,750 --> 00:34:07,020
to look into the documentation or if

00:34:05,520 --> 00:34:09,560
anyone's got questions on it come and

00:34:07,020 --> 00:34:09,560
ask me afterwards

00:34:09,710 --> 00:34:14,100
it handles automatic ranging and

00:34:12,240 --> 00:34:15,720
balancing should you need it you can

00:34:14,100 --> 00:34:17,250
turn this on or off depending on how

00:34:15,720 --> 00:34:19,830
much you want to be involved in the

00:34:17,250 --> 00:34:22,110
ministration your shot is set it's range

00:34:19,830 --> 00:34:24,600
based so you provide a key and it spits

00:34:22,110 --> 00:34:27,750
up your data into ranges and migrates

00:34:24,600 --> 00:34:30,510
that data and essentially because you

00:34:27,750 --> 00:34:33,090
have multiple primaries now that all

00:34:30,510 --> 00:34:34,740
deal with a subsection of your data you

00:34:33,090 --> 00:34:36,590
can scale up because each of them can

00:34:34,740 --> 00:34:41,130
deal with more load than a single note

00:34:36,590 --> 00:34:43,860
the premise is kind of simple here is an

00:34:41,130 --> 00:34:46,590
example of a very durable setup that

00:34:43,860 --> 00:34:48,420
some may have use for sharding so you

00:34:46,590 --> 00:34:50,670
need configuration servers these deal

00:34:48,420 --> 00:34:53,310
with information about where the ranges

00:34:50,670 --> 00:34:55,020
of data exist and each of the colors the

00:34:53,310 --> 00:34:56,550
green yellow and reds are different

00:34:55,020 --> 00:34:58,410
replica sets and what they've done here

00:34:56,550 --> 00:35:00,690
is they've stripe them across they'd

00:34:58,410 --> 00:35:03,180
given across availability zone so if an

00:35:00,690 --> 00:35:05,130
outage happens if Dublin got hit and

00:35:03,180 --> 00:35:06,840
you're on Amazon by lightning and you

00:35:05,130 --> 00:35:08,090
lost that you'll be able to recover from

00:35:06,840 --> 00:35:10,560
it or whatever

00:35:08,090 --> 00:35:11,940
the premise is and they've also played

00:35:10,560 --> 00:35:12,650
with priorities to strike their

00:35:11,940 --> 00:35:14,270
priorities

00:35:12,650 --> 00:35:16,490
so they have tens and difference of

00:35:14,270 --> 00:35:19,670
nodes they lost an availability Center

00:35:16,490 --> 00:35:22,490
only a subsection of their website or

00:35:19,670 --> 00:35:24,529
users would be affected for a limited

00:35:22,490 --> 00:35:26,510
period of time while it brings up an

00:35:24,529 --> 00:35:30,579
Alexa new master there's a lot you can

00:35:26,510 --> 00:35:32,900
do with it any questions come and ask me

00:35:30,579 --> 00:35:33,440
Alyssa this is basically lessons I've

00:35:32,900 --> 00:35:35,059
lost that one

00:35:33,440 --> 00:35:38,450
so web-scale I'm not going to let you

00:35:35,059 --> 00:35:40,089
take a photo of that but because my

00:35:38,450 --> 00:35:42,109
father's might be angry

00:35:40,089 --> 00:35:44,119
scaling I've talked about and we are

00:35:42,109 --> 00:35:45,829
kind of when we go and give these talks

00:35:44,119 --> 00:35:47,150
we talk about these things and say look

00:35:45,829 --> 00:35:51,529
replication there you can scale out

00:35:47,150 --> 00:35:54,049
reeds and you can scale up and you can

00:35:51,529 --> 00:35:56,270
deal with sharding it's simple and easy

00:35:54,049 --> 00:35:58,309
but the truth is there's no magic bullet

00:35:56,270 --> 00:36:02,029
and because you have things like JSON

00:35:58,309 --> 00:36:04,400
documents it kind of it may help you or

00:36:02,029 --> 00:36:06,650
it may be a hindrance it depends on what

00:36:04,400 --> 00:36:08,539
your application does so there's no

00:36:06,650 --> 00:36:10,690
miracle-gro you can't just expand them

00:36:08,539 --> 00:36:13,099
grows as this product does here

00:36:10,690 --> 00:36:14,809
essentially scaling is hard it's a

00:36:13,099 --> 00:36:17,270
challenge you have to understand what

00:36:14,809 --> 00:36:19,640
are the expectations of your system so

00:36:17,270 --> 00:36:21,619
if you're a reed heavy system then

00:36:19,640 --> 00:36:23,420
having a large complex document might be

00:36:21,619 --> 00:36:26,750
great for you if you're right having

00:36:23,420 --> 00:36:28,779
system than having easy to insert and

00:36:26,750 --> 00:36:32,720
update rights is a different proposition

00:36:28,779 --> 00:36:35,900
if you're have requirements to be able

00:36:32,720 --> 00:36:37,520
to return back in sub 50 milliseconds

00:36:35,900 --> 00:36:39,289
back from the database then you're going

00:36:37,520 --> 00:36:41,270
to have a very different style charted

00:36:39,289 --> 00:36:43,609
set up than you would do if it doesn't

00:36:41,270 --> 00:36:45,559
matter how long reporting takes but

00:36:43,609 --> 00:36:48,770
actually I just need small portions of

00:36:45,559 --> 00:36:50,569
my data in RAM so there's no magic

00:36:48,770 --> 00:36:54,289
solution to it it's kind of the main

00:36:50,569 --> 00:36:57,020
point as I alluded to earlier on don't

00:36:54,289 --> 00:36:58,970
be parameter setting up sharding with

00:36:57,020 --> 00:37:00,710
MongoDB as a doddle you can set up on

00:36:58,970 --> 00:37:02,569
your local machine it can take you 30

00:37:00,710 --> 00:37:05,510
minutes to have nine different servers

00:37:02,569 --> 00:37:08,539
running on your local machine and doing

00:37:05,510 --> 00:37:11,150
a fully sharded system for you just so

00:37:08,539 --> 00:37:13,789
you can play with it that's great but if

00:37:11,150 --> 00:37:15,349
you don't need that level of scale don't

00:37:13,789 --> 00:37:18,020
do it don't go in production just and

00:37:15,349 --> 00:37:19,880
buy servers and things I've seen it at

00:37:18,020 --> 00:37:22,099
many times but if you don't need it

00:37:19,880 --> 00:37:23,869
they'd be afraid to change and iterate

00:37:22,099 --> 00:37:26,900
through your requirements as you and

00:37:23,869 --> 00:37:31,050
your applications change

00:37:26,900 --> 00:37:33,330
you will lose all your data right so

00:37:31,050 --> 00:37:35,430
this misconception comes about or people

00:37:33,330 --> 00:37:37,640
not understanding their right concerns

00:37:35,430 --> 00:37:40,020
so by default their fire-and-forget

00:37:37,640 --> 00:37:42,060
which means when we have a look at it

00:37:40,020 --> 00:37:44,250
the driver will just say primary node

00:37:42,060 --> 00:37:49,140
right this data and will not wait for a

00:37:44,250 --> 00:37:50,190
response you say if you add a unique key

00:37:49,140 --> 00:37:52,830
collision you wouldn't get any

00:37:50,190 --> 00:37:54,720
information aback about that so unless

00:37:52,830 --> 00:37:57,060
you speak to the server and say did you

00:37:54,720 --> 00:38:00,260
get it or what's going on you're not

00:37:57,060 --> 00:38:03,420
going to know how durable that data is

00:38:00,260 --> 00:38:05,790
so as I mentioned too when you have

00:38:03,420 --> 00:38:06,450
replica sets this can scale out as much

00:38:05,790 --> 00:38:08,280
as you want

00:38:06,450 --> 00:38:09,869
you can do things like I want to make

00:38:08,280 --> 00:38:11,640
sure at least two of my nodes got the

00:38:09,869 --> 00:38:13,680
data and you will block for a longer

00:38:11,640 --> 00:38:16,440
period of time but for most applications

00:38:13,680 --> 00:38:18,750
the minimal right concern you want or

00:38:16,440 --> 00:38:20,430
need will be to ensure that it's written

00:38:18,750 --> 00:38:23,730
into the journal and the journals flush

00:38:20,430 --> 00:38:25,890
to disk so in if you're doing an update

00:38:23,730 --> 00:38:28,619
all you do have to do an insert it's

00:38:25,890 --> 00:38:30,390
just put journal equals true it's part

00:38:28,619 --> 00:38:32,040
of it or just write help a function that

00:38:30,390 --> 00:38:36,060
does it for you and you just wrap that

00:38:32,040 --> 00:38:37,589
but that is the most common part not

00:38:36,060 --> 00:38:39,570
necessarily all your rights to the

00:38:37,589 --> 00:38:41,280
database need to be on the same write

00:38:39,570 --> 00:38:43,770
concern as well so if you're just

00:38:41,280 --> 00:38:45,540
storing log data or tracking mouse

00:38:43,770 --> 00:38:46,980
clicks over a screen you may just say

00:38:45,540 --> 00:38:49,470
throw them in memory and do the first

00:38:46,980 --> 00:38:51,300
option if you're creating a new user who

00:38:49,470 --> 00:38:53,099
has an account you may want to do this

00:38:51,300 --> 00:38:55,470
option to ensure that that data is fully

00:38:53,099 --> 00:38:57,960
durable so you have a lot of flexibility

00:38:55,470 --> 00:39:00,060
there it's important to understand what

00:38:57,960 --> 00:39:02,250
your needs are for your system the

00:39:00,060 --> 00:39:04,440
higher your write concern the longer it

00:39:02,250 --> 00:39:08,119
will block that will impact your system

00:39:04,440 --> 00:39:12,089
more and the speed and performance of it

00:39:08,119 --> 00:39:15,770
anteye pounds write scheme list doesn't

00:39:12,089 --> 00:39:17,910
mean chaos so just because you can

00:39:15,770 --> 00:39:20,070
doesn't necessarily mean you should I

00:39:17,910 --> 00:39:22,470
have a slide for that as well so bad

00:39:20,070 --> 00:39:24,570
things are don't put everything into a

00:39:22,470 --> 00:39:26,310
single collection be explicit about what

00:39:24,570 --> 00:39:27,440
a collection means keep things that are

00:39:26,310 --> 00:39:29,869
tight together

00:39:27,440 --> 00:39:32,839
there's no necessarily one-size-fits-all

00:39:29,869 --> 00:39:35,490
it's probably not a good plan for you

00:39:32,839 --> 00:39:38,119
don't over index they under index these

00:39:35,490 --> 00:39:40,200
are very similar to the sequel world

00:39:38,119 --> 00:39:42,180
unbounded arrays course

00:39:40,200 --> 00:39:44,190
because if you're continually adding to

00:39:42,180 --> 00:39:45,780
array that may impact your performance

00:39:44,190 --> 00:39:47,250
it's great if you've got a small machine

00:39:45,780 --> 00:39:50,520
and small amount of data is going to

00:39:47,250 --> 00:39:52,140
perform fast but if you get bigger and

00:39:50,520 --> 00:39:54,630
bigger and you start adding the 10th

00:39:52,140 --> 00:39:56,609
million or the or even inside 100,000

00:39:54,630 --> 00:39:58,230
thing to an array every time you add to

00:39:56,609 --> 00:40:00,030
that race and scan through it and that

00:39:58,230 --> 00:40:02,579
was slowly impact your performance so

00:40:00,030 --> 00:40:04,440
that has a real bad codes now I'm not

00:40:02,579 --> 00:40:05,760
saying never use it but just prove it

00:40:04,440 --> 00:40:09,480
when you're with your tests if it's

00:40:05,760 --> 00:40:11,609
right for you similarly arrays that

00:40:09,480 --> 00:40:13,140
store all the data so I there's quite

00:40:11,609 --> 00:40:16,079
often you will see the post comment with

00:40:13,140 --> 00:40:19,619
the post example with an array of

00:40:16,079 --> 00:40:20,880
comments in there now that's fine for

00:40:19,619 --> 00:40:22,619
displaying it you can just do a single

00:40:20,880 --> 00:40:24,119
query whack that back out into the front

00:40:22,619 --> 00:40:26,579
end and that might be perfect for you

00:40:24,119 --> 00:40:28,890
extremely fast but if you wanted to find

00:40:26,579 --> 00:40:30,900
all the comments where someone has

00:40:28,890 --> 00:40:35,040
queried or do any and level of analytics

00:40:30,900 --> 00:40:37,079
on items in an array then there's a

00:40:35,040 --> 00:40:38,700
problem because when you do queries in

00:40:37,079 --> 00:40:40,740
MongoDB you do queries against the

00:40:38,700 --> 00:40:42,480
document you don't need it you say you

00:40:40,740 --> 00:40:44,730
can match against subparts in an array

00:40:42,480 --> 00:40:46,680
but if you did it did the match you will

00:40:44,730 --> 00:40:48,569
return the whole document round and all

00:40:46,680 --> 00:40:50,490
those items in that array and you'll

00:40:48,569 --> 00:40:52,349
have to manually filter them out so that

00:40:50,490 --> 00:40:53,730
potentially could be a code smell you

00:40:52,349 --> 00:40:55,619
may want to separate them out into

00:40:53,730 --> 00:40:56,250
another collection or do a combination

00:40:55,619 --> 00:40:58,079
of both

00:40:56,250 --> 00:41:00,119
you may denormalize and have the latest

00:40:58,079 --> 00:41:01,680
five comments in one place and then all

00:41:00,119 --> 00:41:05,099
the comments somewhere else so you can

00:41:01,680 --> 00:41:06,990
do your aggregations over it references

00:41:05,099 --> 00:41:09,780
everywhere okay so people treat a

00:41:06,990 --> 00:41:11,819
MongoDB they come to it especially if

00:41:09,780 --> 00:41:13,290
they use some of the ATM's engine

00:41:11,819 --> 00:41:14,190
including and treat it just like a

00:41:13,290 --> 00:41:15,510
relational database

00:41:14,190 --> 00:41:16,950
they put foreign keys essentially

00:41:15,510 --> 00:41:18,510
everywhere which costs lots of queries

00:41:16,950 --> 00:41:21,119
going back and forward to the database

00:41:18,510 --> 00:41:23,309
you can't do joins so to deal with the

00:41:21,119 --> 00:41:25,200
foreign key or links to some data you do

00:41:23,309 --> 00:41:27,089
two queries usually want to get the data

00:41:25,200 --> 00:41:29,730
and another to get all those references

00:41:27,089 --> 00:41:31,500
and back and then merge them together so

00:41:29,730 --> 00:41:32,910
if you're doing too much of that or if

00:41:31,500 --> 00:41:36,780
you need things like transactions

00:41:32,910 --> 00:41:38,250
whatever then those things allude to

00:41:36,780 --> 00:41:39,990
that sequel might be your better

00:41:38,250 --> 00:41:42,410
solution just because it's new and

00:41:39,990 --> 00:41:45,150
different doesn't mean it's the rightful

00:41:42,410 --> 00:41:47,040
masses of embedded trees we can give

00:41:45,150 --> 00:41:49,410
examples of good schema design of wire

00:41:47,040 --> 00:41:52,589
trees Goods but honestly if you need

00:41:49,410 --> 00:41:54,119
trees and you need massive trees and he

00:41:52,589 --> 00:41:58,470
needs query and find relationship

00:41:54,119 --> 00:42:00,089
between things use a tree database as to

00:41:58,470 --> 00:42:02,640
say just because you can doesn't mean

00:42:00,089 --> 00:42:05,880
you necessarily should do this so a lot

00:42:02,640 --> 00:42:07,529
of things are like that and essentially

00:42:05,880 --> 00:42:09,509
use the right tool for the job if you

00:42:07,529 --> 00:42:11,630
need transactions if you need to be able

00:42:09,509 --> 00:42:14,190
to roll back and do multi-phase commits

00:42:11,630 --> 00:42:15,930
MongoDB doesn't provide that people will

00:42:14,190 --> 00:42:17,970
write it in their code it will bloat

00:42:15,930 --> 00:42:19,920
their code out that should give you

00:42:17,970 --> 00:42:22,140
warning signs in your head because the

00:42:19,920 --> 00:42:23,940
idea of it should be useful it doesn't

00:42:22,140 --> 00:42:25,650
mean that you can't just all you have to

00:42:23,940 --> 00:42:27,839
just use one database or one database

00:42:25,650 --> 00:42:29,849
types we have people who do things like

00:42:27,839 --> 00:42:31,829
shops they'll deal with the checkouts

00:42:29,849 --> 00:42:33,450
and the transactions in sequel database

00:42:31,829 --> 00:42:35,519
and they'll deal with products in

00:42:33,450 --> 00:42:37,140
MongoDB because what products have

00:42:35,519 --> 00:42:38,519
different types of schemas and describe

00:42:37,140 --> 00:42:41,339
themselves differently depending on the

00:42:38,519 --> 00:42:42,180
types they are so please use the right

00:42:41,339 --> 00:42:44,670
tool for the right job

00:42:42,180 --> 00:42:46,710
don't just troll and say MongoDB broke

00:42:44,670 --> 00:42:49,410
everything for you because that can also

00:42:46,710 --> 00:42:51,890
happen too but please understand you

00:42:49,410 --> 00:42:56,099
don't just have to pick a single item

00:42:51,890 --> 00:42:58,230
best practices so all I'll say the best

00:42:56,099 --> 00:43:00,180
practice for anything with dealing with

00:42:58,230 --> 00:43:01,440
data is just to prove your use case of

00:43:00,180 --> 00:43:04,259
it test and test again

00:43:01,440 --> 00:43:06,539
so we provide lots of tools for dealing

00:43:04,259 --> 00:43:08,549
and scaling yourself up that's great and

00:43:06,539 --> 00:43:10,529
we provide ways to be easy management of

00:43:08,549 --> 00:43:13,380
it doesn't mean that your data models

00:43:10,529 --> 00:43:15,359
work and scale well the only way to do

00:43:13,380 --> 00:43:17,549
that is work out what your application

00:43:15,359 --> 00:43:19,650
does if it's read heavy write heavy

00:43:17,549 --> 00:43:21,329
those kind of things and then test it

00:43:19,650 --> 00:43:24,150
and then test it a little bit more break

00:43:21,329 --> 00:43:26,640
it and find out where the the points are

00:43:24,150 --> 00:43:28,859
in MongoDB at this database layer that

00:43:26,640 --> 00:43:31,170
the don't scale or inopportune for you

00:43:28,859 --> 00:43:35,160
and then try and prove that your schema

00:43:31,170 --> 00:43:40,589
your schema works and is useful for your

00:43:35,160 --> 00:43:43,920
you space that's kind of it so questions

00:43:40,589 --> 00:43:46,640
and the first two questions get booked

00:43:43,920 --> 00:43:46,640
say well done

00:43:52,290 --> 00:43:57,400
one of the slide actually you showed

00:43:54,640 --> 00:43:59,770
three nodes and if one of them is down

00:43:57,400 --> 00:44:03,670
then you're still able to select a

00:43:59,770 --> 00:44:06,070
primary node from that or a master node

00:44:03,670 --> 00:44:09,100
from that so is there an entity which

00:44:06,070 --> 00:44:11,950
sits and works as a proxy like my sequel

00:44:09,100 --> 00:44:13,360
proxy which lets okay so when we're

00:44:11,950 --> 00:44:16,570
talking about elections and hands that

00:44:13,360 --> 00:44:18,970
work basically the service over heart be

00:44:16,570 --> 00:44:22,240
each other they all keep a status of how

00:44:18,970 --> 00:44:25,030
they see the replica set and if a node

00:44:22,240 --> 00:44:26,290
fails and they fail for three times so

00:44:25,030 --> 00:44:29,560
three heart beats it fails they will

00:44:26,290 --> 00:44:31,600
call an election and it's a consensus

00:44:29,560 --> 00:44:33,880
election so they all go my data's

00:44:31,600 --> 00:44:37,720
upstate to this I priority such-and-such

00:44:33,880 --> 00:44:40,000
and then one will say yes I should be

00:44:37,720 --> 00:44:41,830
the master and this election process

00:44:40,000 --> 00:44:43,660
will take or potentially can take a

00:44:41,830 --> 00:44:45,220
number of seconds so you may be in a

00:44:43,660 --> 00:44:49,660
read-only say for a number of seconds

00:44:45,220 --> 00:44:51,700
but given that if you are long as you

00:44:49,660 --> 00:44:53,710
have a majority up and available so as

00:44:51,700 --> 00:44:56,380
long as there's over fifty percent then

00:44:53,710 --> 00:44:58,120
a primary will be elected but there

00:44:56,380 --> 00:44:58,870
isn't no other there's no other system

00:44:58,120 --> 00:45:01,810
that you have to run in the background

00:44:58,870 --> 00:45:03,430
to do it it's the MongoDB the database

00:45:01,810 --> 00:45:06,070
itself when you set it up with the

00:45:03,430 --> 00:45:11,650
repple set flag that's part of the code

00:45:06,070 --> 00:45:14,560
that's in there so how's the traffic

00:45:11,650 --> 00:45:16,090
routed well they're speaking to each

00:45:14,560 --> 00:45:17,710
other and heart beating and basically

00:45:16,090 --> 00:45:19,300
running stats on each other so there's

00:45:17,710 --> 00:45:21,430
be all the nodes have to be able to

00:45:19,300 --> 00:45:23,020
communicate and discover each other to

00:45:21,430 --> 00:45:24,940
be able to understand how the traffic

00:45:23,020 --> 00:45:26,950
from the drivers routed is when you're

00:45:24,940 --> 00:45:28,720
using replica sets you'll use a replica

00:45:26,950 --> 00:45:30,850
set connection and you'll provide a seed

00:45:28,720 --> 00:45:33,190
list and from that seed this the driver

00:45:30,850 --> 00:45:36,070
will speak to the replica set and get

00:45:33,190 --> 00:45:38,260
the full see list of the replica set so

00:45:36,070 --> 00:45:40,540
if you provided just one single node as

00:45:38,260 --> 00:45:42,910
long as it was up and available the

00:45:40,540 --> 00:45:45,970
driver would then get the full topology

00:45:42,910 --> 00:45:47,200
of the replica set they come see me

00:45:45,970 --> 00:45:48,520
afterwards because the lovely lady from

00:45:47,200 --> 00:45:52,660
O'Reilly gave me a couple of books to

00:45:48,520 --> 00:45:54,130
give away so you can move on I used to

00:45:52,660 --> 00:45:55,540
have this problem and they just check it

00:45:54,130 --> 00:45:57,820
again was lost driver letters driver

00:45:55,540 --> 00:45:59,470
nothing change actually when I put the

00:45:57,820 --> 00:46:02,730
high load from the Python to the manga

00:45:59,470 --> 00:46:05,010
more than say percent of the time

00:46:02,730 --> 00:46:08,100
a computer spendin the Pythian rocker

00:46:05,010 --> 00:46:11,310
and on this 30% in the money so it looks

00:46:08,100 --> 00:46:14,730
like my manga is quite an optimize it

00:46:11,310 --> 00:46:17,460
yes a we're talking about optimizing of

00:46:14,730 --> 00:46:19,590
pymongo driver there are things you can

00:46:17,460 --> 00:46:21,480
try and do so someone came up to me

00:46:19,590 --> 00:46:23,520
recently in London said I've doing

00:46:21,480 --> 00:46:26,580
something in C++ it's extremely fast and

00:46:23,520 --> 00:46:29,010
why is it 20 times slower in in pymongo

00:46:26,580 --> 00:46:30,720
and basically as soon as they started

00:46:29,010 --> 00:46:32,609
manipulating something like batch size

00:46:30,720 --> 00:46:34,740
to ensure how much results they get back

00:46:32,609 --> 00:46:39,270
from the server it did less traffic and

00:46:34,740 --> 00:46:40,590
was a lot quicker so I'll come taxi you

00:46:39,270 --> 00:46:42,869
will come grab me afterwards so I can

00:46:40,590 --> 00:46:45,420
get some more specifics but I know that

00:46:42,869 --> 00:46:47,730
batch size is one area that has been

00:46:45,420 --> 00:46:49,290
sped up we also not only do we have that

00:46:47,730 --> 00:46:52,740
grid that I showed earlier on in terms

00:46:49,290 --> 00:46:54,540
of just perform it in terms of which

00:46:52,740 --> 00:46:57,390
versions of Python we test against we

00:46:54,540 --> 00:46:58,619
are not now having performance testing

00:46:57,390 --> 00:47:02,480
to make sure we don't regress and we

00:46:58,619 --> 00:47:08,340
continually improve performance so I

00:47:02,480 --> 00:47:12,030
installed the PI DB and DB

00:47:08,340 --> 00:47:14,640
on my machine and all of a sudden my

00:47:12,030 --> 00:47:18,180
viral petition was eaten up because

00:47:14,640 --> 00:47:21,210
there was it took up some two gigabytes

00:47:18,180 --> 00:47:23,130
of memory so just empty so can you

00:47:21,210 --> 00:47:25,410
yeah say something about memory

00:47:23,130 --> 00:47:28,230
latencies yes well so memory wise

00:47:25,410 --> 00:47:30,060
MongoDB is hugely memory hungry all its

00:47:28,230 --> 00:47:32,670
data files are mapped into virtual

00:47:30,060 --> 00:47:34,380
memory to try and make it efficient the

00:47:32,670 --> 00:47:36,960
other thing it does is it pre allocate

00:47:34,380 --> 00:47:39,500
space so if you have a data set it

00:47:36,960 --> 00:47:42,000
starts off with small data files of like

00:47:39,500 --> 00:47:45,570
64 megabytes and doubles up to 2 gigs

00:47:42,000 --> 00:47:47,460
now if you have over that level of data

00:47:45,570 --> 00:47:49,950
you'll have a whole file that's 2 gigs

00:47:47,460 --> 00:47:51,540
of data absolutely empty now if you're

00:47:49,950 --> 00:47:53,190
in a test environment that's wasteful

00:47:51,540 --> 00:47:55,650
and there are things you can do you pass

00:47:53,190 --> 00:47:57,150
a flag in saying no pre allocation so

00:47:55,650 --> 00:47:59,190
you won't create these extra files until

00:47:57,150 --> 00:48:00,900
they're needed the idea is if this

00:47:59,190 --> 00:48:03,150
father that exists on disk and you'll

00:48:00,900 --> 00:48:04,530
got high right levels you'll be able to

00:48:03,150 --> 00:48:06,000
scale out those right levels because

00:48:04,530 --> 00:48:07,140
you'll be able to have a place to put

00:48:06,000 --> 00:48:09,930
them and then it will start pre

00:48:07,140 --> 00:48:11,490
allocating another farm there's another

00:48:09,930 --> 00:48:13,830
thing you can do is you can others you

00:48:11,490 --> 00:48:16,530
can also pass in a small files flag as

00:48:13,830 --> 00:48:18,270
well between decrease the size

00:48:16,530 --> 00:48:20,490
the files to begin with so there'll be

00:48:18,270 --> 00:48:22,110
smaller to begin with and scale up so

00:48:20,490 --> 00:48:24,810
there's there's multiples opportunities

00:48:22,110 --> 00:48:27,360
there that you can do but yes it is

00:48:24,810 --> 00:48:29,490
extremely memory hungry and if you have

00:48:27,360 --> 00:48:30,540
other things on your system contending

00:48:29,490 --> 00:48:34,650
for memory that will impact your

00:48:30,540 --> 00:48:36,510
performance as well um good question

00:48:34,650 --> 00:48:39,930
have you ever heard of a hybrid solution

00:48:36,510 --> 00:48:42,620
between no sequel and MongoDB loads so

00:48:39,930 --> 00:48:46,710
yeah a sequel solution and MongoDB like

00:48:42,620 --> 00:48:50,570
or is it sufficient to use MongoDB and

00:48:46,710 --> 00:48:52,710
in the GFS solution for binaries so

00:48:50,570 --> 00:48:54,600
people use yeah I would definitely have

00:48:52,710 --> 00:48:56,640
argue using a hybrid solution for

00:48:54,600 --> 00:48:58,260
anything I use the best tools for the

00:48:56,640 --> 00:48:59,670
job that means if you're using sequel

00:48:58,260 --> 00:49:03,360
for certain things for very structure

00:48:59,670 --> 00:49:04,980
schema data that's fine open sky is a

00:49:03,360 --> 00:49:07,980
shop thing I know that they use both

00:49:04,980 --> 00:49:09,420
they use they use the sequel database or

00:49:07,980 --> 00:49:10,830
dealing with some of their structure

00:49:09,420 --> 00:49:13,530
data and then some of the more

00:49:10,830 --> 00:49:16,890
unstructured data is in MongoDB as well

00:49:13,530 --> 00:49:17,940
so it kind of yeah use those tools that

00:49:16,890 --> 00:49:20,040
you need them if you need a graph

00:49:17,940 --> 00:49:21,300
database just because you need a graph

00:49:20,040 --> 00:49:24,090
database doesn't mean you have to use

00:49:21,300 --> 00:49:26,310
neo4j for everything and so yeah mix and

00:49:24,090 --> 00:49:29,630
match and make the technology work for

00:49:26,310 --> 00:49:32,850
you and so I had a question about the

00:49:29,630 --> 00:49:35,280
elections and the self-healing aspect

00:49:32,850 --> 00:49:37,950
yeah so in the scenario where you have

00:49:35,280 --> 00:49:39,890
the the five five nodes yeah and

00:49:37,950 --> 00:49:42,990
you've lost the priority

00:49:39,890 --> 00:49:45,480
look what typical absolute worst-case

00:49:42,990 --> 00:49:46,830
scenario you're stuck back on your the

00:49:45,480 --> 00:49:49,920
primary node is somehow managed to end

00:49:46,830 --> 00:49:51,390
up on your zero priority node okay wait

00:49:49,920 --> 00:49:54,690
happening it might be on the zero but

00:49:51,390 --> 00:49:56,310
because here i mean ii zero cat it's

00:49:54,690 --> 00:49:57,750
that yeah related question what would

00:49:56,310 --> 00:50:00,210
happen then if you haven't got anything

00:49:57,750 --> 00:50:02,400
but that zero node left so if you had no

00:50:00,210 --> 00:50:04,140
other nodes left it could become primary

00:50:02,400 --> 00:50:05,310
you are and if you have less than 50%

00:50:04,140 --> 00:50:07,710
anyway you're going to go into the

00:50:05,310 --> 00:50:09,330
secondary system so you're going to be

00:50:07,710 --> 00:50:11,820
read-only and you're going to have to

00:50:09,330 --> 00:50:13,440
manually intervene it's a it's kind of

00:50:11,820 --> 00:50:15,390
you can choose what to do then you can

00:50:13,440 --> 00:50:17,790
reshape your thing and say okay I will

00:50:15,390 --> 00:50:19,800
now make that a note that said my

00:50:17,790 --> 00:50:21,920
primary thing was dealing with so your

00:50:19,800 --> 00:50:23,610
let's say your priority tens of

00:50:21,920 --> 00:50:25,050
disappeared then you bring them back up

00:50:23,610 --> 00:50:27,060
again yeah do you does that

00:50:25,050 --> 00:50:30,329
automatically trigger a new election or

00:50:27,060 --> 00:50:32,549
um so when they come back

00:50:30,329 --> 00:50:34,799
the topology will change but you may

00:50:32,549 --> 00:50:35,640
have to manually go and intervene to

00:50:34,799 --> 00:50:36,779
ensure because they're going to

00:50:35,640 --> 00:50:38,640
potentially how long they're going to

00:50:36,779 --> 00:50:41,099
recover they're going to be behind so

00:50:38,640 --> 00:50:42,989
you may go in run a command in the shell

00:50:41,099 --> 00:50:45,269
to actually take the current primary

00:50:42,989 --> 00:50:47,609
down and force an election when you know

00:50:45,269 --> 00:50:48,569
everything's up and running so yeah

00:50:47,609 --> 00:50:53,429
you're going to have to manage that

00:50:48,569 --> 00:50:57,089
yourself essentially okay that's

00:50:53,429 --> 00:51:00,410
everything the other question no so

00:50:57,089 --> 00:51:00,410

YouTube URL: https://www.youtube.com/watch?v=FLIEJt6IymY


