Title: Łukasz Kąkol - Pythonic code vs  performance
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Pythonic code vs. performance
[EuroPython 2018 - Talk - 2018-07-25 - Moorfoot]
[Edinburgh, UK]

By Łukasz Kąkol

Idiomatic Python is beautiful. If you’re new to Python, this talk is for you because I’m going to reveal the charm of python in front of you. I’ll present how boilerplate code can be replaced with idiomatic python. If you’re experienced python developer, this talk is also for you because I’ll compare the performance of the idiomatic code, both from CPU and memory point of view. Some of these results may surprise you.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,100 --> 00:00:03,940
[Applause]

00:00:04,970 --> 00:00:08,760
Thanks

00:00:06,270 --> 00:00:11,190
hi my name is Josh Conkle and I will

00:00:08,760 --> 00:00:14,730
speak about performance optimization and

00:00:11,190 --> 00:00:17,960
profiling as an examples I will use a

00:00:14,730 --> 00:00:21,180
comparison of pythonic code and patterns

00:00:17,960 --> 00:00:30,510
inherited from other languages like C

00:00:21,180 --> 00:00:32,940
C++ Java yeah I will try okay let me

00:00:30,510 --> 00:00:36,420
introduce myself a bit I'm working as

00:00:32,940 --> 00:00:38,910
senior Python developer at STX next it's

00:00:36,420 --> 00:00:42,750
a Python software house located in

00:00:38,910 --> 00:00:47,850
Poland I hope also gave my talks at

00:00:42,750 --> 00:00:50,700
PyCon UK last year and biases which is a

00:00:47,850 --> 00:00:54,559
Python conference in San Sebastian in

00:00:50,700 --> 00:00:59,730
Spain and I also will share my knowledge

00:00:54,559 --> 00:01:03,780
during local meetups in in Poznan

00:00:59,730 --> 00:01:08,540
important but what's most relevant for

00:01:03,780 --> 00:01:12,900
the topic I'm really interested in a

00:01:08,540 --> 00:01:17,100
performance optimizations and I'm really

00:01:12,900 --> 00:01:19,979
fascinated in it let's check up the

00:01:17,100 --> 00:01:20,939
agenda and I will first introduce you

00:01:19,979 --> 00:01:23,820
shortly

00:01:20,939 --> 00:01:27,780
to the topic of of the performance

00:01:23,820 --> 00:01:31,020
aspects then we would have a live demo

00:01:27,780 --> 00:01:34,740
and then I will summarize the tie atop

00:01:31,020 --> 00:01:39,420
shortly and at the end we would have

00:01:34,740 --> 00:01:42,420
some time for questions but before we

00:01:39,420 --> 00:01:46,290
get started let me get to know you a

00:01:42,420 --> 00:01:52,920
little bit better who have ever used any

00:01:46,290 --> 00:01:56,520
profiling tool ok who is using that

00:01:52,920 --> 00:02:03,420
frequently like on daily basis in daily

00:01:56,520 --> 00:02:09,750
job ok not that much and who is just

00:02:03,420 --> 00:02:12,690
starting a journey in Python ok and who

00:02:09,750 --> 00:02:16,740
has used any other languages

00:02:12,690 --> 00:02:21,050
and then Python like C C++ Java okay

00:02:16,740 --> 00:02:25,730
that's much okay so let's get started

00:02:21,050 --> 00:02:30,570
what are the base aspects of performance

00:02:25,730 --> 00:02:34,380
first of all its CPU we all want our

00:02:30,570 --> 00:02:37,470
code to be as fast as possible that's

00:02:34,380 --> 00:02:41,630
what our client and our end user most

00:02:37,470 --> 00:02:44,730
cares about Google statistics says that

00:02:41,630 --> 00:02:47,580
then that more than the half of the

00:02:44,730 --> 00:02:49,700
users will abandon the website if it

00:02:47,580 --> 00:02:55,260
takes more than three seconds to load

00:02:49,700 --> 00:02:58,520
and moreover each single second calls a

00:02:55,260 --> 00:03:03,210
customer satisfaction to drop by about

00:02:58,520 --> 00:03:05,630
15% and conversion to drop by around

00:03:03,210 --> 00:03:10,020
five to 10%

00:03:05,630 --> 00:03:12,900
another thing is memory the truth is

00:03:10,020 --> 00:03:16,140
that we don't care about the memory

00:03:12,900 --> 00:03:18,390
until we run out of it and I must admit

00:03:16,140 --> 00:03:20,580
it's fair approach because why should we

00:03:18,390 --> 00:03:24,209
care about something that it's not

00:03:20,580 --> 00:03:28,110
affecting us but we should be still

00:03:24,209 --> 00:03:33,510
aware that any memory leak can affect

00:03:28,110 --> 00:03:35,760
our software for example in on daily

00:03:33,510 --> 00:03:38,670
basis in testing environments we don't

00:03:35,760 --> 00:03:41,880
use as much fixtures as we would have in

00:03:38,670 --> 00:03:46,430
production so the scale makes a really

00:03:41,880 --> 00:03:51,390
difference here last factor is

00:03:46,430 --> 00:03:55,620
input/output operations and for example

00:03:51,390 --> 00:03:58,680
database operations or file system this

00:03:55,620 --> 00:04:04,800
may massively influenced the first

00:03:58,680 --> 00:04:07,530
aspect so the so the time of logging and

00:04:04,800 --> 00:04:10,830
application or running but it also has

00:04:07,530 --> 00:04:15,109
another side effects there are specific

00:04:10,830 --> 00:04:17,910
limited limitations or simultaneously

00:04:15,109 --> 00:04:20,960
input/output operations for example

00:04:17,910 --> 00:04:22,730
database based transactions which may

00:04:20,960 --> 00:04:26,030
lock and

00:04:22,730 --> 00:04:29,600
hold each other file system has limits

00:04:26,030 --> 00:04:31,210
on number of open of simultaneously open

00:04:29,600 --> 00:04:34,190
files

00:04:31,210 --> 00:04:37,430
moreover some services like Google App

00:04:34,190 --> 00:04:41,870
Engine provides resources like database

00:04:37,430 --> 00:04:45,140
up to certain quota then each read and

00:04:41,870 --> 00:04:47,710
write to database over that limit a cost

00:04:45,140 --> 00:04:47,710
you real money

00:04:48,640 --> 00:04:53,410
what should we do to optimize our code

00:04:51,650 --> 00:04:56,510
in a good way

00:04:53,410 --> 00:05:01,460
first we have to plan and predict our

00:04:56,510 --> 00:05:05,060
approach implementation and the data

00:05:01,460 --> 00:05:07,520
structure which we will use and predict

00:05:05,060 --> 00:05:08,860
which which one will give us the best

00:05:07,520 --> 00:05:12,680
results

00:05:08,860 --> 00:05:15,140
then we should profile our code it would

00:05:12,680 --> 00:05:18,490
be good to provide few proofs of Scone

00:05:15,140 --> 00:05:22,160
of concepts of course if you if we can

00:05:18,490 --> 00:05:26,900
we should use some fake data just to

00:05:22,160 --> 00:05:27,890
indicate which one would be the best but

00:05:26,900 --> 00:05:31,400
that's not all

00:05:27,890 --> 00:05:37,910
once we ship our code we should monitor

00:05:31,400 --> 00:05:41,860
the performance in living ecosystem keep

00:05:37,910 --> 00:05:46,010
in mind that you will not be able to

00:05:41,860 --> 00:05:49,790
test each data set you can be almost

00:05:46,010 --> 00:05:53,330
sure that your end user will have some

00:05:49,790 --> 00:05:59,930
edge case you could not invent on your

00:05:53,330 --> 00:06:03,220
own your testing environment and note

00:05:59,930 --> 00:06:09,530
that this might be also related to the

00:06:03,220 --> 00:06:12,710
tech stack the user is using once you

00:06:09,530 --> 00:06:15,740
have production benchmarks collected you

00:06:12,710 --> 00:06:19,100
should identify bottlenecks and quick

00:06:15,740 --> 00:06:24,200
wins and it really depends on each

00:06:19,100 --> 00:06:28,640
specific case you should then take a

00:06:24,200 --> 00:06:33,050
look into optimizing your code again you

00:06:28,640 --> 00:06:35,770
should first look into quick wins and

00:06:33,050 --> 00:06:35,770
bottlenecks

00:06:36,999 --> 00:06:41,719
but should we optimize everything of

00:06:41,059 --> 00:06:43,719
course not

00:06:41,719 --> 00:06:46,779
there is no point in optimizing

00:06:43,719 --> 00:06:52,039
something that that will give you like

00:06:46,779 --> 00:07:00,559
5% again and it will cost you a few

00:06:52,039 --> 00:07:02,869
weeks or even months of work so about

00:07:00,559 --> 00:07:06,259
profiling tools there are a lot of them

00:07:02,869 --> 00:07:08,360
available in a pipe in I will just

00:07:06,259 --> 00:07:13,180
introduce a few of them which I will use

00:07:08,360 --> 00:07:19,189
during the demo first one is C profile

00:07:13,180 --> 00:07:22,129
it's to inspect the CPU usage as divided

00:07:19,189 --> 00:07:26,180
by functions that's part of standard

00:07:22,129 --> 00:07:30,499
library and it's most accurate of the

00:07:26,180 --> 00:07:34,459
available tools next one is memory

00:07:30,499 --> 00:07:39,889
profiler that's third party library it's

00:07:34,459 --> 00:07:44,080
available on PI P it's better than other

00:07:39,889 --> 00:07:47,479
options but it's still not perfect

00:07:44,080 --> 00:07:51,019
results may vary a bit which I will

00:07:47,479 --> 00:07:53,539
present you later during the demo and it

00:07:51,019 --> 00:07:58,579
also takes some time to profile the

00:07:53,539 --> 00:08:02,209
memory next one is sis its built-in

00:07:58,579 --> 00:08:05,419
library it provides a provides us with a

00:08:02,209 --> 00:08:09,829
low-level operating system API which we

00:08:05,419 --> 00:08:13,389
can use to to inspect for example CPU

00:08:09,829 --> 00:08:18,050
usage of the process or the memory and

00:08:13,389 --> 00:08:21,469
last tool is this it's also built in our

00:08:18,050 --> 00:08:25,849
library we can use it to disassemble the

00:08:21,469 --> 00:08:31,419
Python code and inspect it on lower

00:08:25,849 --> 00:08:31,419
level ok so now it's time for demo

00:08:44,070 --> 00:08:48,440
is that one size font size okay

00:09:07,740 --> 00:09:10,740
okay

00:09:11,300 --> 00:09:16,829
okay so let's start with the first

00:09:13,560 --> 00:09:21,300
example first I will show you the usage

00:09:16,829 --> 00:09:26,040
of the tools I I will I will use during

00:09:21,300 --> 00:09:29,100
the presentation so first one is the

00:09:26,040 --> 00:09:35,899
function which I will use prop profile

00:09:29,100 --> 00:09:39,600
the CPU usage so I'm creating the list

00:09:35,899 --> 00:09:42,240
I'm creating the second list and then

00:09:39,600 --> 00:09:48,029
I'm deleting the first list and

00:09:42,240 --> 00:09:51,600
returning the second list okay and how

00:09:48,029 --> 00:09:58,199
we'll do we profile it we and we should

00:09:51,600 --> 00:10:01,110
import the profile and then import the

00:09:58,199 --> 00:10:08,899
function which we will use and provide

00:10:01,110 --> 00:10:08,899
it as a string so it can get evaluated

00:10:18,750 --> 00:10:21,750
oops

00:10:27,520 --> 00:10:44,380
okay all right okay so we can see that

00:10:41,580 --> 00:10:47,140
six functions has been called during

00:10:44,380 --> 00:10:48,160
during the execution of the main

00:10:47,140 --> 00:10:54,610
function

00:10:48,160 --> 00:10:57,760
it took 0.02 seconds and we got number

00:10:54,610 --> 00:11:00,490
of course of each function here in the

00:10:57,760 --> 00:11:04,779
last column we have the file name line

00:11:00,490 --> 00:11:06,880
number and the function name yeah we got

00:11:04,779 --> 00:11:10,270
the number of of course of the of that

00:11:06,880 --> 00:11:14,140
function we have total time of all the

00:11:10,270 --> 00:11:17,800
calls and then we got time for one

00:11:14,140 --> 00:11:21,850
single call average time then we got the

00:11:17,800 --> 00:11:24,300
cumulative time which is from the start

00:11:21,850 --> 00:11:34,410
up to the end of this function

00:11:24,300 --> 00:11:36,940
yeah and cumulative time per call okay

00:11:34,410 --> 00:11:41,079
we can also pass some additional

00:11:36,940 --> 00:11:43,750
arguments to see profile but I will not

00:11:41,079 --> 00:11:50,880
do it for now I just want to show you

00:11:43,750 --> 00:11:54,550
the simple cases of of CPU profile okay

00:11:50,880 --> 00:11:59,649
another another tool which we can use

00:11:54,550 --> 00:12:03,520
for profiling CPU is time it but it's

00:11:59,649 --> 00:12:07,420
not so accurate as see profile mostly

00:12:03,520 --> 00:12:15,010
because of a garbage collector not being

00:12:07,420 --> 00:12:17,399
run in time it so let's run the second

00:12:15,010 --> 00:12:17,399
example

00:12:22,990 --> 00:12:35,420
okay we got one second one point 85

00:12:28,610 --> 00:12:40,820
seconds its then at the time of running

00:12:35,420 --> 00:12:43,250
the 100 retries on this example so what

00:12:40,820 --> 00:12:47,600
we provide here is the real function -

00:12:43,250 --> 00:12:50,200
to measure measure and the set up string

00:12:47,600 --> 00:12:55,130
which also will be evaluated before

00:12:50,200 --> 00:13:01,630
running the the real function - to

00:12:55,130 --> 00:13:11,660
profile a time it is also available with

00:13:01,630 --> 00:13:20,510
the - M time it then we should provide

00:13:11,660 --> 00:13:24,940
the setup string and then the string

00:13:20,510 --> 00:13:24,940
with the real function - profile

00:13:30,010 --> 00:13:39,890
this will gives us a bit more readable

00:13:35,770 --> 00:13:44,540
result so we got the information that we

00:13:39,890 --> 00:13:47,590
have run 100 loops best of three of the

00:13:44,540 --> 00:13:56,420
of these loops is eighteen point five

00:13:47,590 --> 00:14:02,950
milliseconds and we have also nice plug

00:13:56,420 --> 00:14:17,900
in 4/4 time it in ipython we should just

00:14:02,950 --> 00:14:24,220
import import the function and then we

00:14:17,900 --> 00:14:24,220
use % time it CPU profile

00:14:30,240 --> 00:14:38,300
and here we you will have even more

00:14:33,050 --> 00:14:40,860
user-friendly way of showing the results

00:14:38,300 --> 00:14:42,139
will also have the standard deviation

00:14:40,860 --> 00:14:45,529
[Music]

00:14:42,139 --> 00:14:51,540
yeah so we got average time here and

00:14:45,529 --> 00:14:55,199
then the standard deviation here it has

00:14:51,540 --> 00:15:00,079
been run in 100 loops and it has been

00:14:55,199 --> 00:15:05,699
turned seven times okay so that's about

00:15:00,079 --> 00:15:12,629
the performance CPU performance and now

00:15:05,699 --> 00:15:18,119
let's go to the memory profiling so on

00:15:12,629 --> 00:15:24,559
the function we need to profile we

00:15:18,119 --> 00:15:27,329
should call the decorator we get we got

00:15:24,559 --> 00:15:32,879
it we should import memory profiler

00:15:27,329 --> 00:15:37,410
first then use the profile decorator

00:15:32,879 --> 00:15:41,299
from this library and we have a few

00:15:37,410 --> 00:15:41,299
optional parameters like precision

00:15:57,399 --> 00:16:07,459
yeah that takes a bit longer it probes

00:16:02,449 --> 00:16:13,399
the memory usage of the process after

00:16:07,459 --> 00:16:17,389
each line so here we got the code we're

00:16:13,399 --> 00:16:22,279
profiling we got the starting memory

00:16:17,389 --> 00:16:27,199
usage and we got increment and the total

00:16:22,279 --> 00:16:33,649
memory usage here so here you can see

00:16:27,199 --> 00:16:36,579
that first list took about one point

00:16:33,649 --> 00:16:40,790
ninety five megabytes second one took

00:16:36,579 --> 00:16:45,439
1.87 megabytes and then when we release

00:16:40,790 --> 00:16:48,259
actually the least a we get back one

00:16:45,439 --> 00:16:56,059
megabyte of memory what that may mean

00:16:48,259 --> 00:17:01,509
that we get a garbage collector didn't

00:16:56,059 --> 00:17:05,870
run yet or there is one more thing some

00:17:01,509 --> 00:17:09,589
small integers are being cached by by

00:17:05,870 --> 00:17:11,949
Python so these are just a few of the

00:17:09,589 --> 00:17:18,139
reasons but there are definitely more

00:17:11,949 --> 00:17:22,189
and more over there is we should also

00:17:18,139 --> 00:17:28,299
keep in mind that it's just probing so

00:17:22,189 --> 00:17:28,299
it might not be 100% accurate accurate

00:17:28,840 --> 00:17:41,809
okay another example will be greatest

00:17:35,990 --> 00:17:52,730
common divisor so we will disassemble it

00:17:41,809 --> 00:17:56,360
and see what the result will be okay all

00:17:52,730 --> 00:18:02,779
these codes are listed in documentation

00:17:56,360 --> 00:18:07,220
of this library so we got here the we're

00:18:02,779 --> 00:18:09,789
setting up the loop we're loading the Y

00:18:07,220 --> 00:18:09,789
variable

00:18:10,060 --> 00:18:26,650
then if Y is false we will jump to line

00:18:16,620 --> 00:18:31,990
24 so over here okay

00:18:26,650 --> 00:18:35,950
in next line we're loading x and y we

00:18:31,990 --> 00:18:40,210
are calling binary module function then

00:18:35,950 --> 00:18:44,110
we store it in temp variable next line

00:18:40,210 --> 00:18:50,050
is assignment of Y to X so we're loading

00:18:44,110 --> 00:18:54,490
loading Y and storing it in X next line

00:18:50,050 --> 00:18:58,840
is assignment of temp to why we're

00:18:54,490 --> 00:19:00,040
loading temp and storing Y and we got

00:18:58,840 --> 00:19:04,390
the end of the loop

00:19:00,040 --> 00:19:07,750
so we're jumping back to the second

00:19:04,390 --> 00:19:15,670
second line over here and then we're

00:19:07,750 --> 00:19:19,150
loading X and returning it so that's the

00:19:15,670 --> 00:19:19,660
simple example of disassembled Python

00:19:19,150 --> 00:19:23,650
code

00:19:19,660 --> 00:19:31,240
we'll also use it later for more

00:19:23,650 --> 00:19:38,110
interesting examples okay another thing

00:19:31,240 --> 00:19:40,510
is what what another thing I want to

00:19:38,110 --> 00:19:45,670
talk about is the reliability of

00:19:40,510 --> 00:19:48,850
profiling so we'll call both CPU

00:19:45,670 --> 00:19:53,250
profiling and memory profiling three

00:19:48,850 --> 00:19:53,250
times and see the results

00:20:00,259 --> 00:20:09,419
ya know that we got three different

00:20:04,769 --> 00:20:13,909
times keep in mind that it might depend

00:20:09,419 --> 00:20:18,089
on manufacturers like another processes

00:20:13,909 --> 00:20:21,570
being run in the background I tried I

00:20:18,089 --> 00:20:30,209
tried to isolate that virtual machine as

00:20:21,570 --> 00:20:34,769
much as I could but but it's still

00:20:30,209 --> 00:20:37,769
differs between the course about the

00:20:34,769 --> 00:20:42,869
memory we also got different results

00:20:37,769 --> 00:20:48,709
here first the first reason behind it is

00:20:42,869 --> 00:20:51,629
as I said before python caching and it's

00:20:48,709 --> 00:20:55,019
mostly visible in difference between

00:20:51,629 --> 00:20:58,200
first call and the other tools so here

00:20:55,019 --> 00:21:00,959
we got different result and here we got

00:20:58,200 --> 00:21:04,639
the same result at the end the increment

00:21:00,959 --> 00:21:09,950
still differs but it's because of

00:21:04,639 --> 00:21:09,950
caching in Python

00:21:16,330 --> 00:21:34,150
okay another example is creating lists

00:21:30,640 --> 00:21:44,910
by list comprehension by appending the

00:21:34,150 --> 00:21:49,300
result and by extending the list so

00:21:44,910 --> 00:21:54,540
definitely the fastest is list

00:21:49,300 --> 00:21:57,790
comprehension then four times slower is

00:21:54,540 --> 00:22:02,950
creating lists by appending each

00:21:57,790 --> 00:22:06,700
subsequent item and a bit faster is

00:22:02,950 --> 00:22:09,700
extending the list so whether you would

00:22:06,700 --> 00:22:13,360
have the possibility to extend use it

00:22:09,700 --> 00:22:22,180
because it's a small gain but in the

00:22:13,360 --> 00:22:27,070
scale it can really differ next example

00:22:22,180 --> 00:22:31,710
is a conversion of data types tuples

00:22:27,070 --> 00:22:31,710
lists sets and dictionaries

00:22:36,350 --> 00:22:49,850
so the fastest is the list then we got

00:22:41,809 --> 00:22:55,370
set tuple is much slower than the list

00:22:49,850 --> 00:22:57,889
and set and dictionary is even more

00:22:55,370 --> 00:22:59,659
slower is the slowest of these

00:22:57,889 --> 00:23:03,740
collections but we should keep in mind

00:22:59,659 --> 00:23:07,519
that dictionaries are keys and values

00:23:03,740 --> 00:23:08,029
players so the construction of it takes

00:23:07,519 --> 00:23:10,960
time

00:23:08,029 --> 00:23:16,700
moreover there is a hashing function

00:23:10,960 --> 00:23:20,809
which is caching the keys and we can see

00:23:16,700 --> 00:23:23,960
also the size of of these collections so

00:23:20,809 --> 00:23:30,279
smallest one is is tuple then we got

00:23:23,960 --> 00:23:35,809
list set and dictionary set is much

00:23:30,279 --> 00:23:40,480
larger than the tuples and lists it's

00:23:35,809 --> 00:23:45,950
almost four times larger

00:23:40,480 --> 00:23:48,500
but the reason behind it is that it's

00:23:45,950 --> 00:23:53,529
just optimized dictionary in the

00:23:48,500 --> 00:23:57,740
implementation so it just have keys and

00:23:53,529 --> 00:24:01,039
dummy values and is using just just the

00:23:57,740 --> 00:24:04,990
the keys of the of the dictionary and

00:24:01,039 --> 00:24:04,990
that's how set is implemented

00:24:11,030 --> 00:24:25,640
okay next example is using the values of

00:24:18,950 --> 00:24:30,550
the two lists iterating by indexes zip

00:24:25,640 --> 00:24:30,550
function and using a dictionary

00:24:37,820 --> 00:24:47,269
so zip function is the fastest one then

00:24:43,009 --> 00:24:53,200
we got dictionary and then we got

00:24:47,269 --> 00:24:53,200
iterating using indexes of two lists

00:24:53,229 --> 00:25:04,149
so zip looks like the best option but

00:24:59,169 --> 00:25:13,340
it's really not so usable in most cases

00:25:04,149 --> 00:25:17,210
but dictionary is still fine next

00:25:13,340 --> 00:25:21,139
example is checking if element is

00:25:17,210 --> 00:25:26,210
contained within the list so first one

00:25:21,139 --> 00:25:29,210
is just in keyword second one is running

00:25:26,210 --> 00:25:34,119
through all the list and checking if the

00:25:29,210 --> 00:25:38,119
element is present and the last case is

00:25:34,119 --> 00:25:42,229
binary search over the list so we're

00:25:38,119 --> 00:25:46,129
splitting the list in two equal parts

00:25:42,229 --> 00:25:49,309
and comparing if the element is present

00:25:46,129 --> 00:25:53,119
in left or in the right part of the list

00:25:49,309 --> 00:25:56,509
then doing that recursively note that

00:25:53,119 --> 00:26:03,950
the list must be sorted before running

00:25:56,509 --> 00:26:08,690
that operation so I compare all these

00:26:03,950 --> 00:26:13,279
checks for two cases first one is

00:26:08,690 --> 00:26:16,869
positive case so we will find this

00:26:13,279 --> 00:26:19,820
element in the list and second one is

00:26:16,869 --> 00:26:22,059
that we will not find this element in

00:26:19,820 --> 00:26:22,059
the list

00:26:33,900 --> 00:26:42,720
okay so first one is in keyword so we

00:26:39,720 --> 00:26:46,470
can see that it runs two times slower if

00:26:42,720 --> 00:26:50,309
element is not present in the list so

00:26:46,470 --> 00:26:53,750
the implementation here is a bit

00:26:50,309 --> 00:26:56,910
optimized for loop over the elements

00:26:53,750 --> 00:26:59,870
then we can see the for loop over and

00:26:56,910 --> 00:27:04,860
over the elements implemented in Python

00:26:59,870 --> 00:27:08,160
it's in positive case that's one point

00:27:04,860 --> 00:27:14,840
six seconds and for loop is two point

00:27:08,160 --> 00:27:21,270
five seconds and it also keeps the the

00:27:14,840 --> 00:27:25,170
trend of a positive to negative ratio so

00:27:21,270 --> 00:27:28,080
it's still two times taller and last one

00:27:25,170 --> 00:27:32,690
binary binary search

00:27:28,080 --> 00:27:36,270
it is definitely the fastest one so in

00:27:32,690 --> 00:27:38,910
keyword is pythonic way to check if

00:27:36,270 --> 00:27:41,700
element is present but if we really care

00:27:38,910 --> 00:27:45,240
about performance and really need to run

00:27:41,700 --> 00:27:48,420
it fast binary search is the best option

00:27:45,240 --> 00:27:52,200
here even if the implementation is more

00:27:48,420 --> 00:27:58,530
complex but it's still not that scary

00:27:52,200 --> 00:28:00,540
right it's just less than 15 lines so if

00:27:58,530 --> 00:28:03,900
we really care about performance we

00:28:00,540 --> 00:28:12,270
should use more complex solutions but

00:28:03,900 --> 00:28:18,390
which are more efficient okay next one

00:28:12,270 --> 00:28:20,670
is swapping the variables first example

00:28:18,390 --> 00:28:24,630
shows the swapping using temporary

00:28:20,670 --> 00:28:32,150
variable and second one is swapping

00:28:24,630 --> 00:28:32,150
tuple so assigning tuple of Y X to X Y

00:28:41,140 --> 00:28:52,360
okay so swapping using temporary

00:28:47,570 --> 00:28:55,850
variable appeared to be faster now okay

00:28:52,360 --> 00:29:00,920
that's not what I expected that might be

00:28:55,850 --> 00:29:02,660
as some some process running in the

00:29:00,920 --> 00:29:07,790
background which is interrupting that

00:29:02,660 --> 00:29:11,870
yeah again but different is not so so we

00:29:07,790 --> 00:29:14,240
can now probably if I run it again it

00:29:11,870 --> 00:29:17,060
will finally give expected results now

00:29:14,240 --> 00:29:22,220
yeah yeah it's it's finally finally

00:29:17,060 --> 00:29:27,620
gives the smaller value for swapping

00:29:22,220 --> 00:29:32,720
tuples so let's see how it's how it

00:29:27,620 --> 00:29:35,390
looks in this assembled code we got here

00:29:32,720 --> 00:29:39,260
it's it's swapping tuples so we're

00:29:35,390 --> 00:29:42,470
loading x and y and x then rotating

00:29:39,260 --> 00:29:47,180
these two storing in storing X and

00:29:42,470 --> 00:29:50,600
storing Y and then returning no while

00:29:47,180 --> 00:29:55,630
here we got load fast or fast load fast

00:29:50,600 --> 00:29:55,630
so fast and again load fast or fast it's

00:29:55,810 --> 00:30:05,320
three times load and store instead of

00:29:59,950 --> 00:30:05,320
two store and load and store

00:30:10,620 --> 00:30:18,750
next example is efficiency of string

00:30:15,160 --> 00:30:22,750
construction first one is AB string

00:30:18,750 --> 00:30:27,300
introduced in Python 3.6 and next one is

00:30:22,750 --> 00:30:32,760
formatted string and the last one is

00:30:27,300 --> 00:30:32,760
percent percent formatted string

00:30:39,790 --> 00:30:49,060
so if strings is are definitely fastest

00:30:44,610 --> 00:30:53,550
then we see that percent formatted

00:30:49,060 --> 00:30:57,160
strings are in the middle and format is

00:30:53,550 --> 00:31:02,860
longest yeah it's nice to use but it's

00:30:57,160 --> 00:31:06,420
slow but now in Python 3.6 we got F

00:31:02,860 --> 00:31:06,420
strings so let's use it

00:31:08,490 --> 00:31:22,680
okay so we got nested loop here and what

00:31:16,090 --> 00:31:28,740
I will do is iterate over the smaller

00:31:22,680 --> 00:31:32,380
portion of data in outer loop then

00:31:28,740 --> 00:31:34,720
iterate over a larger amount of data in

00:31:32,380 --> 00:31:40,120
outer loop and then split it equally

00:31:34,720 --> 00:31:44,760
between outer and inner loop so in all

00:31:40,120 --> 00:31:55,930
these cases we got the same number of

00:31:44,760 --> 00:31:58,870
iterations okay so maybe let's guess

00:31:55,930 --> 00:32:00,910
which one will be the fastest so the

00:31:58,870 --> 00:32:04,270
first the first example who thinks that

00:32:00,910 --> 00:32:10,290
it will be the fastest okay

00:32:04,270 --> 00:32:17,190
second example okay and the last one

00:32:10,290 --> 00:32:20,520
okay not everyone plays a bad but okay

00:32:17,190 --> 00:32:20,520
let's run it

00:32:27,710 --> 00:32:38,000
okay so this one is the fastest one so

00:32:34,010 --> 00:32:41,900
second one so iterating over more items

00:32:38,000 --> 00:32:47,840
in outer loop which I will expect

00:32:41,900 --> 00:32:51,280
otherwise but it's up here it's not yeah

00:32:47,840 --> 00:32:58,220
and there is not much difference between

00:32:51,280 --> 00:33:01,070
the first and the last example let's see

00:32:58,220 --> 00:33:05,120
at the disassembled code because it may

00:33:01,070 --> 00:33:12,290
explain a bit more so here we're

00:33:05,120 --> 00:33:17,780
starting first outer loop here we got we

00:33:12,290 --> 00:33:21,320
start we're starting inner loop here

00:33:17,780 --> 00:33:27,010
we're finishing the inner loop and here

00:33:21,320 --> 00:33:32,740
we're finishing outer loop so it's

00:33:27,010 --> 00:33:35,870
really what we would expect is that

00:33:32,740 --> 00:33:41,960
iterations over inner loop will be

00:33:35,870 --> 00:33:46,360
faster but it appears that the iteration

00:33:41,960 --> 00:34:00,130
over outer loop is faster why is that

00:33:46,360 --> 00:34:03,500
because here we'll we're jumping to the

00:34:00,130 --> 00:34:10,490
inner loop and here we're jumping to

00:34:03,500 --> 00:34:15,020
outer loop so if we jump over here

00:34:10,490 --> 00:34:22,370
frequently it's a bit slower than

00:34:15,020 --> 00:34:24,140
jumping over here not sure if that's if

00:34:22,370 --> 00:34:28,490
if that explains it

00:34:24,140 --> 00:34:29,210
but yeah it's it's pretty pretty hard to

00:34:28,490 --> 00:34:35,450
explain

00:34:29,210 --> 00:34:40,460
on that disassembled code but the fact

00:34:35,450 --> 00:34:44,450
is that more

00:34:40,460 --> 00:34:50,599
oops in outer loop is more efficient

00:34:44,450 --> 00:34:55,450
than than in the inner loop okay next

00:34:50,599 --> 00:35:00,530
example using global variable versus

00:34:55,450 --> 00:35:03,170
using parameter so again who thinks that

00:35:00,530 --> 00:35:07,130
the global variable would be more

00:35:03,170 --> 00:35:10,660
efficient okay there who thinks that

00:35:07,130 --> 00:35:20,980
parameterized variable more efficient

00:35:10,660 --> 00:35:27,470
okay let's see we're frequently using

00:35:20,980 --> 00:35:30,560
the global variable but it's not so

00:35:27,470 --> 00:35:36,170
efficient as parameterised version but

00:35:30,560 --> 00:35:39,830
it we will see their reason over here in

00:35:36,170 --> 00:35:45,170
this assembled code so their only

00:35:39,830 --> 00:35:47,960
difference here is that in global a

00:35:45,170 --> 00:35:53,599
global variable is loaded with loaded

00:35:47,960 --> 00:35:56,869
global instead of load fast over here so

00:35:53,599 --> 00:36:01,010
that's the only difference and appears

00:35:56,869 --> 00:36:05,770
twice where we load this global variable

00:36:01,010 --> 00:36:05,770
or X parameter of the function

00:36:09,960 --> 00:36:17,640
okay next example is slots who is

00:36:15,280 --> 00:36:21,100
familiar with slots

00:36:17,640 --> 00:36:26,920
okay so I was playing it a bit deeper so

00:36:21,100 --> 00:36:31,390
slots are the listed parameter names

00:36:26,920 --> 00:36:34,690
that object will be restricted to so if

00:36:31,390 --> 00:36:38,410
we got slots X over here

00:36:34,690 --> 00:36:42,010
we cannot dynamically assign a self dot

00:36:38,410 --> 00:36:44,080
Y for example it will just throw the

00:36:42,010 --> 00:36:52,230
runtime error we will not be able to do

00:36:44,080 --> 00:36:56,020
that okay so but that sounds like an

00:36:52,230 --> 00:37:00,630
restriction for us but what does it mean

00:36:56,020 --> 00:37:06,310
from for from performance point of view

00:37:00,630 --> 00:37:09,700
it's definitely faster with slots and as

00:37:06,310 --> 00:37:13,960
we will see in the second it consumes a

00:37:09,700 --> 00:37:17,470
much less memory than just the regular

00:37:13,960 --> 00:37:22,620
object without slots so we can see that

00:37:17,470 --> 00:37:26,230
it's three times more memory here used

00:37:22,620 --> 00:37:29,590
there isn't behind it is that objects

00:37:26,230 --> 00:37:35,320
without slots will over allocate the

00:37:29,590 --> 00:37:38,680
memory so that we can we can see it over

00:37:35,320 --> 00:37:43,630
here it's three times more that may of

00:37:38,680 --> 00:37:49,060
course differ based on different

00:37:43,630 --> 00:37:52,180
different number of slots if we for

00:37:49,060 --> 00:37:56,140
example gives a live here like five five

00:37:52,180 --> 00:38:01,320
five variable names or ten it will

00:37:56,140 --> 00:38:04,470
differ okay so that would be all from

00:38:01,320 --> 00:38:10,440
demo part

00:38:04,470 --> 00:38:10,440
let's summarize it shortly

00:38:12,880 --> 00:38:21,800
okay so first predict what would be the

00:38:18,710 --> 00:38:24,170
best solution based on your experience

00:38:21,800 --> 00:38:29,510
the Bears based on the examples you just

00:38:24,170 --> 00:38:33,440
saw or just based on your hunch predict

00:38:29,510 --> 00:38:34,700
but to have preliminary data and don't

00:38:33,440 --> 00:38:41,900
trust it at all

00:38:34,700 --> 00:38:45,230
always profile your code predictions

00:38:41,900 --> 00:38:48,470
might be misleading when I was preparing

00:38:45,230 --> 00:38:53,240
examples for this demo and contr a lot

00:38:48,470 --> 00:38:57,200
of surprises we seen seen one I have to

00:38:53,240 --> 00:39:02,960
rerun this three times because data data

00:38:57,200 --> 00:39:07,230
was not perfect enough I would say so

00:39:02,960 --> 00:39:09,580
things do not do not behave as we expect

00:39:07,230 --> 00:39:12,730
[Music]

00:39:09,580 --> 00:39:17,210
yeah we should always profile our code

00:39:12,730 --> 00:39:23,150
even if you think it's fast enough or

00:39:17,210 --> 00:39:24,260
fast test just check it for your own for

00:39:23,150 --> 00:39:28,970
your own peace of mind

00:39:24,260 --> 00:39:32,210
right and last week a calculate return

00:39:28,970 --> 00:39:35,120
on investment so try to find the best

00:39:32,210 --> 00:39:38,390
ratio between possible profit and

00:39:35,120 --> 00:39:41,660
optimization cost there is no point in

00:39:38,390 --> 00:39:43,970
spending weeks on optimization which

00:39:41,660 --> 00:39:47,870
will give you in unnoticeable

00:39:43,970 --> 00:39:51,980
improvement hey thank you for your

00:39:47,870 --> 00:39:56,060
attention you can find slides on the

00:39:51,980 --> 00:40:00,890
left side in the link or QR code I also

00:39:56,060 --> 00:40:03,170
put it in on our Python website I have

00:40:00,890 --> 00:40:07,850
also pushed the code snippets to github

00:40:03,170 --> 00:40:10,790
links also available here its repository

00:40:07,850 --> 00:40:12,740
is private for now I will make it public

00:40:10,790 --> 00:40:16,190
right after the talk

00:40:12,740 --> 00:40:20,150
and I also appreciate any feedback about

00:40:16,190 --> 00:40:22,970
my talk so I can improve myself to be

00:40:20,150 --> 00:40:25,190
better talks there is

00:40:22,970 --> 00:40:26,420
the link on the right side to the

00:40:25,190 --> 00:40:28,609
feedback form

00:40:26,420 --> 00:40:30,410
it's simple anonymous form I would

00:40:28,609 --> 00:40:34,099
really appreciate if you spend a few

00:40:30,410 --> 00:40:35,210
seconds to share your opinion thank you

00:40:34,099 --> 00:40:40,239
very much

00:40:35,210 --> 00:40:40,239
[Applause]

00:40:42,680 --> 00:40:51,630
so we have time for a couple of

00:40:45,450 --> 00:40:53,489
questions I see their hands thank thank

00:40:51,630 --> 00:40:56,910
you for the talk I was wondering is

00:40:53,489 --> 00:40:59,339
there any easy way in see profiler to

00:40:56,910 --> 00:41:01,650
give kind of like a person stat so there

00:40:59,339 --> 00:41:04,499
was total time there was cumulative time

00:41:01,650 --> 00:41:08,190
is there anything where you just give a

00:41:04,499 --> 00:41:11,069
parameter so that one line or one

00:41:08,190 --> 00:41:13,289
allocation doctor let's say 80% of the

00:41:11,069 --> 00:41:16,319
total time something like that just not

00:41:13,289 --> 00:41:19,700
to calculate the whole thing yes so that

00:41:16,319 --> 00:41:23,400
might be not so visible in the examples

00:41:19,700 --> 00:41:28,170
but there is a total time of all the

00:41:23,400 --> 00:41:31,529
calls or total time of or time per call

00:41:28,170 --> 00:41:34,049
for the function you can also give a

00:41:31,529 --> 00:41:36,720
parameter to see profile to sort the

00:41:34,049 --> 00:41:39,749
results by specific column okay so we

00:41:36,720 --> 00:41:42,479
can sort for by total time or time for

00:41:39,749 --> 00:41:45,599
call and also on short question so in

00:41:42,479 --> 00:41:48,059
terms of that slot the your example

00:41:45,599 --> 00:41:50,400
didn't allocate anything to a particular

00:41:48,059 --> 00:41:54,119
slot so let's say if you allocate a

00:41:50,400 --> 00:41:56,549
hundred mix or like read a file of the

00:41:54,119 --> 00:41:58,589
same size with slot in without sort do

00:41:56,549 --> 00:42:02,999
you think will make a difference yeah

00:41:58,589 --> 00:42:04,890
definitely because it's it's always good

00:42:02,999 --> 00:42:07,349
to profile the specific case because

00:42:04,890 --> 00:42:12,059
that depends what you're putting in the

00:42:07,349 --> 00:42:14,910
in this variable but in general object

00:42:12,059 --> 00:42:19,650
without slot is over al-qaida so it will

00:42:14,910 --> 00:42:23,130
just allocate some some memory right

00:42:19,650 --> 00:42:28,759
after the object to have some space to

00:42:23,130 --> 00:42:28,759
some dynamic assignments thank you

00:42:34,020 --> 00:42:37,290
of course

00:42:40,750 --> 00:42:45,820
I got two questions actually the first

00:42:43,630 --> 00:42:46,570
one is are there any graphic tools for

00:42:45,820 --> 00:42:48,099
it

00:42:46,570 --> 00:42:51,099
for the profiling libraries you've shown

00:42:48,099 --> 00:42:53,470
yes there are graphic tools I didn't

00:42:51,099 --> 00:42:55,359
list them because I wanted to adjust the

00:42:53,470 --> 00:42:58,150
simplest tool which will be most

00:42:55,359 --> 00:43:01,990
accurate but there are many tools you

00:42:58,150 --> 00:43:06,010
can just google it and yeah there are a

00:43:01,990 --> 00:43:07,000
lot of them I checked like week or two

00:43:06,010 --> 00:43:09,490
weeks ago

00:43:07,000 --> 00:43:12,490
there were a lot of them right things

00:43:09,490 --> 00:43:14,080
and the second question is since the

00:43:12,490 --> 00:43:17,740
profiling if showing us is not really

00:43:14,080 --> 00:43:21,570
reliable or not 100% reliable should we

00:43:17,740 --> 00:43:23,680
rerun it multiple times to get a better

00:43:21,570 --> 00:43:26,740
understanding of how the how the

00:43:23,680 --> 00:43:30,160
application performs yeah yeah when I

00:43:26,740 --> 00:43:33,280
run the memory profiler a few times in

00:43:30,160 --> 00:43:36,040
the loop for example it gives it

00:43:33,280 --> 00:43:40,570
stabilizes with with the time so first

00:43:36,040 --> 00:43:43,450
run is like a bit inaccurate but then it

00:43:40,570 --> 00:43:46,480
stabilizes and gives at least the total

00:43:43,450 --> 00:43:49,060
memory usage on the stable level then

00:43:46,480 --> 00:43:52,000
increment also stable establish like we

00:43:49,060 --> 00:43:54,869
see we saw in the example with when I

00:43:52,000 --> 00:44:02,609
run three times the same function with

00:43:54,869 --> 00:44:05,410
memory profiler total total memory for

00:44:02,609 --> 00:44:11,380
process has stabilized in the second

00:44:05,410 --> 00:44:13,660
answer and field line but increment

00:44:11,380 --> 00:44:16,930
still did not stabilize but it will

00:44:13,660 --> 00:44:22,599
stabilize at at some point because it

00:44:16,930 --> 00:44:24,480
probes the memory by inserting the the

00:44:22,599 --> 00:44:28,089
probes in into the hold

00:44:24,480 --> 00:44:30,080
okay we have to stop here thank you very

00:44:28,089 --> 00:44:32,160
much for your talk

00:44:30,080 --> 00:44:33,300
[Applause]

00:44:32,160 --> 00:44:36,899
you

00:44:33,300 --> 00:44:36,899

YouTube URL: https://www.youtube.com/watch?v=1AqW9-E6VCM


