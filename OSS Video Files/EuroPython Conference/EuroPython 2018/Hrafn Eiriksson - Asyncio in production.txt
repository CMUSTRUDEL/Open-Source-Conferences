Title: Hrafn Eiriksson - Asyncio in production
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Asyncio in production
[EuroPython 2018 - Talk - 2018-07-25 - Lammermuir]
[Edinburgh, UK]

By Hrafn Eiriksson

Much has been written about asynchronous programming in Python, especially after the introduction of asyncio into the standard library of the language. We've all seen the benchmarks that tell us how asyncio-powered web servers massively outperform their non-asyncio counterparts for trivial routes and we've seen the articles that tell us how to make the move from flask to aiohttp and why we should. 

Despite all of this, the question remains: How is it to use asyncio in a production setting? What are the pain points of migrating a large application to use asyncio? How does the performance of this real-world application differ after the migration? Is it still just unicorns and fairy tales? The goal of this talk is to address these questions. 

If you are curious about taking the step from asyncio-dabbling to creating actual mission critical software systems in asyncio; this is the talk for you!



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,120 --> 00:00:08,220
[Applause]

00:00:02,750 --> 00:00:10,469
thank you so much and thanks for coming

00:00:08,220 --> 00:00:12,750
along it's very nice to see all of you

00:00:10,469 --> 00:00:15,870
as he said my name is Rodney Erickson

00:00:12,750 --> 00:00:16,470
and I like a lot of people around this

00:00:15,870 --> 00:00:18,930
conference

00:00:16,470 --> 00:00:21,359
it seems I'm quite interested in async

00:00:18,930 --> 00:00:22,380
i/o and that's what I'm going to be

00:00:21,359 --> 00:00:23,939
talking about today

00:00:22,380 --> 00:00:27,420
you've maybe been to some of the other

00:00:23,939 --> 00:00:28,439
async i/o talks this morning in my mind

00:00:27,420 --> 00:00:30,779
they were a little bit kind of

00:00:28,439 --> 00:00:32,850
theoretical but I'm here to try to

00:00:30,779 --> 00:00:35,880
represent the more practical side of

00:00:32,850 --> 00:00:38,219
things so yeah you know just trying to

00:00:35,880 --> 00:00:39,960
use this thing and make it work so the

00:00:38,219 --> 00:00:42,270
first thing that I want to do today is

00:00:39,960 --> 00:00:44,579
to you know get a feeling of how many

00:00:42,270 --> 00:00:47,340
people in the audience have have toyed

00:00:44,579 --> 00:00:50,760
around with async i/o in Python shows of

00:00:47,340 --> 00:00:52,520
hands we definitely are in a Python

00:00:50,760 --> 00:00:54,600
conference that's for sure

00:00:52,520 --> 00:00:57,120
more interesting question in my mind

00:00:54,600 --> 00:00:59,399
however is the following how many of you

00:00:57,120 --> 00:01:02,699
have created actual mission critical

00:00:59,399 --> 00:01:04,080
systems in Python using async i/o and

00:01:02,699 --> 00:01:09,000
cannot you know deployed them to

00:01:04,080 --> 00:01:12,180
production it's still quite a lot but

00:01:09,000 --> 00:01:14,310
yeah still very less than the number of

00:01:12,180 --> 00:01:16,710
people that have toyed around with async

00:01:14,310 --> 00:01:18,890
i/o and this seems to be the case

00:01:16,710 --> 00:01:21,330
everywhere where I ask this question

00:01:18,890 --> 00:01:23,790
most Python misters have played around

00:01:21,330 --> 00:01:25,439
with the thing but they it hasn't really

00:01:23,790 --> 00:01:28,759
made its way into their production

00:01:25,439 --> 00:01:32,130
systems for one reason or another and

00:01:28,759 --> 00:01:34,290
this is despite the numerous blog posts

00:01:32,130 --> 00:01:38,090
and articles and all of this stuff that

00:01:34,290 --> 00:01:40,170
details how massively async io libraries

00:01:38,090 --> 00:01:42,210
outperform their kind of synchronous

00:01:40,170 --> 00:01:45,990
counterparts but also like their

00:01:42,210 --> 00:01:48,390
asynchronous alternatives so why aren't

00:01:45,990 --> 00:01:50,670
everyone using async ion protection when

00:01:48,390 --> 00:01:53,759
it's so obviously all rainbows and

00:01:50,670 --> 00:01:55,770
unicorns and there are definitely a

00:01:53,759 --> 00:01:57,930
number of reasons for this I'm just

00:01:55,770 --> 00:01:58,920
quickly gonna go through like some of my

00:01:57,930 --> 00:02:01,469
personal favorites

00:01:58,920 --> 00:02:03,450
first of all asynchronous programming is

00:02:01,469 --> 00:02:06,000
it's very different it requires a very

00:02:03,450 --> 00:02:09,030
different mental model of how to do

00:02:06,000 --> 00:02:12,209
programming and takes some time to

00:02:09,030 --> 00:02:13,710
adjust to secondly the async IO feature

00:02:12,209 --> 00:02:17,550
in Python is

00:02:13,710 --> 00:02:19,440
relatively new to the language and by

00:02:17,550 --> 00:02:20,940
that I mean acing Aoi told me like

00:02:19,440 --> 00:02:24,900
tornado and all of that kind of stuff

00:02:20,940 --> 00:02:26,250
because that dates back to 2002 but and

00:02:24,900 --> 00:02:28,110
programmers a little bit kind of

00:02:26,250 --> 00:02:31,140
reluctant to new shiny things when it

00:02:28,110 --> 00:02:32,280
comes to their production systems it's

00:02:31,140 --> 00:02:34,410
kind of interesting because at the same

00:02:32,280 --> 00:02:37,920
time we are very much drawn to new shiny

00:02:34,410 --> 00:02:40,080
things so it's a very thin line so third

00:02:37,920 --> 00:02:43,560
to convert an already existing Python

00:02:40,080 --> 00:02:47,940
application to async i/o is non-trivial

00:02:43,560 --> 00:02:50,040
it takes upwards fourth due to the lack

00:02:47,940 --> 00:02:52,740
of native support in the language in the

00:02:50,040 --> 00:02:54,870
past to do asynchronous programming the

00:02:52,740 --> 00:02:57,420
community has come up with solutions to

00:02:54,870 --> 00:02:58,710
this come up with the libraries like

00:02:57,420 --> 00:03:01,560
event let tornado

00:02:58,710 --> 00:03:03,540
twisted all of the stuff and many of

00:03:01,560 --> 00:03:05,610
these libraries are widely used and they

00:03:03,540 --> 00:03:08,310
they just work and people cannot just

00:03:05,610 --> 00:03:10,590
you know stick with them so last

00:03:08,310 --> 00:03:12,330
probably most importantly asynchronous

00:03:10,590 --> 00:03:16,290
programming is not always the solution

00:03:12,330 --> 00:03:18,540
to your problems now it so happens that

00:03:16,290 --> 00:03:20,670
I do have some experience with migrating

00:03:18,540 --> 00:03:24,750
actual in production Python code bases

00:03:20,670 --> 00:03:27,720
to use async i/o and even though async

00:03:24,750 --> 00:03:29,430
IO is by no means perfect my path like

00:03:27,720 --> 00:03:31,950
my experience thus far has been quite

00:03:29,430 --> 00:03:35,340
positive and that's kind of why I'm here

00:03:31,950 --> 00:03:37,410
today a little bit to spread word so my

00:03:35,340 --> 00:03:40,170
talk today is split roughly into three

00:03:37,410 --> 00:03:42,090
parts the first part in the first part

00:03:40,170 --> 00:03:44,130
I'm gonna be discussing discussing what

00:03:42,090 --> 00:03:46,640
led me to commit a lot of time and

00:03:44,130 --> 00:03:49,050
effort you know in in to migrating

00:03:46,640 --> 00:03:53,640
software that was already working in

00:03:49,050 --> 00:03:54,660
production to use async i/o then I'm

00:03:53,640 --> 00:03:56,340
going to talk about some of the

00:03:54,660 --> 00:03:58,470
roadblocks that I hate during this time

00:03:56,340 --> 00:04:00,510
and you know some of the caveats that

00:03:58,470 --> 00:04:03,510
you might want to be aware of if you

00:04:00,510 --> 00:04:05,640
want to do the same thing last I want to

00:04:03,510 --> 00:04:08,370
talk about some of the benefits of this

00:04:05,640 --> 00:04:11,160
work and you know try to back this up

00:04:08,370 --> 00:04:12,840
with some empirical data I should

00:04:11,160 --> 00:04:15,420
probably also mention what this talk is

00:04:12,840 --> 00:04:17,520
not about just so we're on the same page

00:04:15,420 --> 00:04:19,830
it's actually not a lot of things but

00:04:17,520 --> 00:04:21,930
mainly it's not an introduction into

00:04:19,830 --> 00:04:23,970
async i/o and how it works

00:04:21,930 --> 00:04:25,860
so yeah you're gonna have to look that

00:04:23,970 --> 00:04:27,389
up somewhere else but even though you

00:04:25,860 --> 00:04:30,539
have absolutely no knowledge

00:04:27,389 --> 00:04:33,300
of it at all don't worry I'm hoping this

00:04:30,539 --> 00:04:39,289
talk might at least can trigger you to

00:04:33,300 --> 00:04:42,590
explore the subject further interesting

00:04:39,289 --> 00:04:49,110
so why would you want to bother with

00:04:42,590 --> 00:04:52,349
async I oh sorry I'm just not gonna test

00:04:49,110 --> 00:04:55,169
this thing so your software orgy works

00:04:52,349 --> 00:04:57,569
right to understand where I'm coming

00:04:55,169 --> 00:05:00,659
from personally we're gonna need to

00:04:57,569 --> 00:05:03,060
explore my backgrounds a little bit just

00:05:00,659 --> 00:05:05,729
a teeny tiny little bit so in my past I

00:05:03,060 --> 00:05:07,199
worked for a company that was obsessed

00:05:05,729 --> 00:05:11,039
with using nodejs

00:05:07,199 --> 00:05:12,300
for everything admittedly I was kind of

00:05:11,039 --> 00:05:15,090
one of the people in charge so I cannot

00:05:12,300 --> 00:05:18,150
really free myself from that but despite

00:05:15,090 --> 00:05:21,120
no DS's and JavaScript's multiple pain

00:05:18,150 --> 00:05:23,310
points these technologies still taught

00:05:21,120 --> 00:05:25,969
me how useful asynchronous programming

00:05:23,310 --> 00:05:28,560
can be and how efficient it can be

00:05:25,969 --> 00:05:31,620
especially when your applications are

00:05:28,560 --> 00:05:33,689
just mostly i/o pounds by the end of my

00:05:31,620 --> 00:05:36,120
tenure of this company I had gotten used

00:05:33,689 --> 00:05:37,830
to like writing everything even what

00:05:36,120 --> 00:05:41,400
would be very simple sequential scripts

00:05:37,830 --> 00:05:43,289
in Python in an asynchronous fashion so

00:05:41,400 --> 00:05:46,409
when I joined my current employer a

00:05:43,289 --> 00:05:48,509
company called markets I had I had kind

00:05:46,409 --> 00:05:51,539
of to get unused like used to a new

00:05:48,509 --> 00:05:54,360
reality markets is a company that relies

00:05:51,539 --> 00:05:56,759
heavily on a micro services micro

00:05:54,360 --> 00:05:58,740
services based architecture and most of

00:05:56,759 --> 00:06:01,860
these services are coincidentally

00:05:58,740 --> 00:06:03,839
written in Python so initially when I

00:06:01,860 --> 00:06:07,349
started looking into how these micro

00:06:03,839 --> 00:06:09,029
services were structured and and how

00:06:07,349 --> 00:06:13,560
they were coated all I could see was

00:06:09,029 --> 00:06:15,439
just plain old blocking Python codes to

00:06:13,560 --> 00:06:17,669
be honest I wasn't too impressed

00:06:15,439 --> 00:06:19,830
especially coming from my awesome

00:06:17,669 --> 00:06:21,750
asynchronous backgrounds so at this

00:06:19,830 --> 00:06:24,029
point in time I started thinking like

00:06:21,750 --> 00:06:25,949
how are they scaling these things

00:06:24,029 --> 00:06:27,539
because some of these micro services

00:06:25,949 --> 00:06:31,139
were taking on some serious serious

00:06:27,539 --> 00:06:32,909
traffic so after a bit of research I

00:06:31,139 --> 00:06:34,830
noticed that most of these services were

00:06:32,909 --> 00:06:38,430
scaled out in a fairly classical

00:06:34,830 --> 00:06:40,469
pythonic way its service was running in

00:06:38,430 --> 00:06:41,129
multiple instances application instances

00:06:40,469 --> 00:06:42,719
and

00:06:41,129 --> 00:06:45,449
it's instance was a Kuna corn server

00:06:42,719 --> 00:06:47,069
that had you know multiple workers so

00:06:45,449 --> 00:06:49,889
scaling out was a matter of adding new

00:06:47,069 --> 00:06:53,580
application instances or adding new

00:06:49,889 --> 00:06:57,179
workers or both so because our micro

00:06:53,580 --> 00:06:59,039
services are very IO bound it was kind

00:06:57,179 --> 00:07:01,199
of apparent to me that all of these

00:06:59,039 --> 00:07:04,679
workers on the end we're just going to

00:07:01,199 --> 00:07:07,219
be stuck waiting on IO the whole time

00:07:04,679 --> 00:07:09,749
I was just thinking like what a waste

00:07:07,219 --> 00:07:11,909
and after thinking about this for a

00:07:09,749 --> 00:07:14,189
while I noticed that a big part of the

00:07:11,909 --> 00:07:16,800
services depended on a packets that was

00:07:14,189 --> 00:07:19,709
somewhere deep down doing this thing

00:07:16,800 --> 00:07:22,679
here probably some of you know this

00:07:19,709 --> 00:07:25,219
thing David actually touched upon this

00:07:22,679 --> 00:07:28,529
in the keynote this morning but

00:07:25,219 --> 00:07:30,899
obviously my node.js acclimated mind did

00:07:28,529 --> 00:07:32,999
not realize immediately what this date

00:07:30,899 --> 00:07:35,729
but after some research I figured out

00:07:32,999 --> 00:07:38,249
that this actually passes the standard

00:07:35,729 --> 00:07:40,709
library so your previously kind of

00:07:38,249 --> 00:07:42,469
blocking code will magically be doing

00:07:40,709 --> 00:07:44,569
asynchronous things behind the scenes

00:07:42,469 --> 00:07:47,519
cool

00:07:44,569 --> 00:07:49,229
so before async IO came along this was

00:07:47,519 --> 00:07:51,769
apparently one of the kind of standard

00:07:49,229 --> 00:07:54,679
ways of achieving concurrency and Python

00:07:51,769 --> 00:07:57,479
in my mind this is problematic in

00:07:54,679 --> 00:07:59,339
multiple ways mostly because this is

00:07:57,479 --> 00:08:01,860
kind of hidden away from the developer

00:07:59,339 --> 00:08:04,860
it's quite magical and it's hard to

00:08:01,860 --> 00:08:07,019
reason about as well also like many

00:08:04,860 --> 00:08:09,329
developers on my team and in the company

00:08:07,019 --> 00:08:11,669
were not making use of email ads

00:08:09,329 --> 00:08:12,929
multiple concurrency primitives because

00:08:11,669 --> 00:08:16,439
they didn't even know this was happening

00:08:12,929 --> 00:08:18,839
somewhere in their kind of software so

00:08:16,439 --> 00:08:21,419
the moral of the story is that async IO

00:08:18,839 --> 00:08:24,479
itself and we can async slash await

00:08:21,419 --> 00:08:27,779
keywords make this very more explicit

00:08:24,479 --> 00:08:30,509
and someone told me that explicit is

00:08:27,779 --> 00:08:32,189
better than implicit I should probably

00:08:30,509 --> 00:08:35,339
know that I have absolutely nothing

00:08:32,189 --> 00:08:37,709
against Amandla in general quite the

00:08:35,339 --> 00:08:39,630
opposite I find it fascinating that they

00:08:37,709 --> 00:08:42,389
were able to create something that works

00:08:39,630 --> 00:08:44,790
so well that I didn't notice it for you

00:08:42,389 --> 00:08:46,350
know months that it was kind of hard

00:08:44,790 --> 00:08:48,870
like doing asynchronous stuff behind the

00:08:46,350 --> 00:08:50,399
scenes but now that we do have a sink IO

00:08:48,870 --> 00:08:53,399
I think we should kind of start using

00:08:50,399 --> 00:08:54,900
that so yeah to summarize a little bit

00:08:53,399 --> 00:08:56,910
there are multiple display

00:08:54,900 --> 00:09:00,900
in which we can achieve concurrency in

00:08:56,910 --> 00:09:03,990
Python but in my mind async IO is the

00:09:00,900 --> 00:09:07,050
obvious way to go because it's explicit

00:09:03,990 --> 00:09:10,140
and it's a part of the language it's

00:09:07,050 --> 00:09:16,590
also supposedly quite performant but we

00:09:10,140 --> 00:09:19,140
will get back to that later on so

00:09:16,590 --> 00:09:22,350
assuming that async IO is indeed the

00:09:19,140 --> 00:09:25,230
future and that we want to kind of you

00:09:22,350 --> 00:09:27,330
know make our new projects use it or

00:09:25,230 --> 00:09:30,300
migrate our existing projects to use it

00:09:27,330 --> 00:09:33,000
what issues are we going to run into and

00:09:30,300 --> 00:09:36,690
how simple is it I've already mentioned

00:09:33,000 --> 00:09:38,040
it's actually not super simple but it's

00:09:36,690 --> 00:09:41,550
definitely getting better it's getting

00:09:38,040 --> 00:09:43,260
very simpler than it was not that long

00:09:41,550 --> 00:09:45,030
ago when I was taking a look at a sync

00:09:43,260 --> 00:09:46,560
IO in Python for the first time I

00:09:45,030 --> 00:09:48,810
noticed that people were relying quite

00:09:46,560 --> 00:09:51,960
heavily on monkey-patching

00:09:48,810 --> 00:09:54,740
again there are dependencies just to

00:09:51,960 --> 00:09:57,630
make them work with their async IO code

00:09:54,740 --> 00:09:59,370
for anyone that's serious about their

00:09:57,630 --> 00:10:00,900
code base and like serious about the

00:09:59,370 --> 00:10:04,230
reliability of their system that's

00:10:00,900 --> 00:10:06,240
probably not a great idea but since then

00:10:04,230 --> 00:10:09,060
for the kind of past you know a couple

00:10:06,240 --> 00:10:11,340
of months maybe up to a year it seems

00:10:09,060 --> 00:10:14,310
like the async IO ecosystem in Python

00:10:11,340 --> 00:10:16,520
has grown and matured rapidly we now

00:10:14,310 --> 00:10:19,830
have loads of mature web frameworks

00:10:16,520 --> 00:10:23,010
loads of mature kind of database drivers

00:10:19,830 --> 00:10:26,430
and just a lot of libraries for a lot of

00:10:23,010 --> 00:10:29,610
stuff and yeah it's it's getting quite

00:10:26,430 --> 00:10:31,620
good so I kind of feel now like it's

00:10:29,610 --> 00:10:33,420
getting to the point where library

00:10:31,620 --> 00:10:35,760
authors should no longer be ignoring

00:10:33,420 --> 00:10:40,080
async i/o and instead kind of defaulting

00:10:35,760 --> 00:10:42,900
to it now because I'm a very practical

00:10:40,080 --> 00:10:44,970
person as I spoke about earlier I think

00:10:42,900 --> 00:10:49,110
the best way to describe how you should

00:10:44,970 --> 00:10:52,410
go about migrating a Python project to

00:10:49,110 --> 00:10:54,590
async iOS simply to describe the process

00:10:52,410 --> 00:10:57,090
that I went through recently when I

00:10:54,590 --> 00:11:01,140
moved one of our kind of in production

00:10:57,090 --> 00:11:03,210
micro services to use async i/o having

00:11:01,140 --> 00:11:06,090
done this before I knew that the first

00:11:03,210 --> 00:11:08,460
thing I'd need to do was to kind of map

00:11:06,090 --> 00:11:10,830
out the dependencies of the service

00:11:08,460 --> 00:11:13,130
and research if there would exist async

00:11:10,830 --> 00:11:17,280
IO compatible versions of the

00:11:13,130 --> 00:11:19,200
dependencies and also since this was a

00:11:17,280 --> 00:11:21,990
micro service that exposed like a

00:11:19,200 --> 00:11:23,820
RESTful API obviously I had to figure

00:11:21,990 --> 00:11:26,790
out which kind of bad framework to use

00:11:23,820 --> 00:11:29,220
always a classical problem and when

00:11:26,790 --> 00:11:30,990
researching these kind of async iov

00:11:29,220 --> 00:11:33,210
frameworks that we have now I was

00:11:30,990 --> 00:11:35,610
pleasantly surprised to see the kind of

00:11:33,210 --> 00:11:38,730
white variety of web frameworks that we

00:11:35,610 --> 00:11:41,430
have access to this slide just shows a

00:11:38,730 --> 00:11:43,020
few example of those and all of these

00:11:41,430 --> 00:11:45,360
are kind of specifically targeted

00:11:43,020 --> 00:11:47,220
towards async io some of them aim to be

00:11:45,360 --> 00:11:48,810
flasks like some of them aim to be fast

00:11:47,220 --> 00:11:52,590
some of them a aim to have good

00:11:48,810 --> 00:11:54,840
WebSocket support and some of them aim

00:11:52,590 --> 00:11:59,670
to be compatible not only with flasks

00:11:54,840 --> 00:12:01,860
public API but also its private API in

00:11:59,670 --> 00:12:03,630
such a way that you're like that you're

00:12:01,860 --> 00:12:07,140
basically supposed to be able to do a

00:12:03,630 --> 00:12:09,420
find and replace or flask to court and

00:12:07,140 --> 00:12:11,670
you just sprinkle of you can a sink

00:12:09,420 --> 00:12:16,950
await keywords around and it just should

00:12:11,670 --> 00:12:19,200
work I haven't tried it out but I know a

00:12:16,950 --> 00:12:22,530
colleague of mine Phil actually wrote

00:12:19,200 --> 00:12:25,340
this framework so yeah it probably works

00:12:22,530 --> 00:12:25,340
he's a good guy

00:12:25,440 --> 00:12:29,520
this can be obviously this can be very

00:12:27,570 --> 00:12:31,260
useful if you are migrating a large kind

00:12:29,520 --> 00:12:32,960
of flask application to async i/o and

00:12:31,260 --> 00:12:35,820
you just want to get up to speed quickly

00:12:32,960 --> 00:12:38,580
now I didn't care that much about api

00:12:35,820 --> 00:12:39,960
compatibility with flask so i decided to

00:12:38,580 --> 00:12:42,600
go with an old web framework which is

00:12:39,960 --> 00:12:46,170
called a io HTTP simply because it's

00:12:42,600 --> 00:12:47,790
known to be mature special tested and i

00:12:46,170 --> 00:12:51,180
don't want to take my chances with new

00:12:47,790 --> 00:12:54,180
shiny stuff like quartz but even though

00:12:51,180 --> 00:12:56,430
the AI OSTP api is a little bit

00:12:54,180 --> 00:12:59,460
different from flask the flask api we

00:12:56,430 --> 00:13:02,850
all know and love migrating to it was

00:12:59,460 --> 00:13:05,280
super simple and in fact I kind of felt

00:13:02,850 --> 00:13:07,890
a little bit kind of relieved to be rid

00:13:05,280 --> 00:13:10,800
of the global request object that I've

00:13:07,890 --> 00:13:12,840
always found a little bit odd that just

00:13:10,800 --> 00:13:14,880
me anyway

00:13:12,840 --> 00:13:16,890
the same can be said about the migration

00:13:14,880 --> 00:13:18,330
from psycho PG 2 which is the old

00:13:16,890 --> 00:13:20,970
database driver that we used for

00:13:18,330 --> 00:13:21,769
Postgres to async PG which is the new

00:13:20,970 --> 00:13:24,050
de-facto

00:13:21,769 --> 00:13:27,139
awesome Postgres database driver for a

00:13:24,050 --> 00:13:29,540
sink IO except for the desperate lack of

00:13:27,139 --> 00:13:32,989
named parameters if the developers are

00:13:29,540 --> 00:13:35,179
in the room I was a little bit worried

00:13:32,989 --> 00:13:36,410
that migrating sentry the error tracking

00:13:35,179 --> 00:13:39,319
software that we use would be

00:13:36,410 --> 00:13:40,999
problematic and after searching the

00:13:39,319 --> 00:13:43,009
internet for a little bit I found some

00:13:40,999 --> 00:13:45,470
packets on Keita that was supposed to be

00:13:43,009 --> 00:13:48,339
you know an integration between sentry

00:13:45,470 --> 00:13:51,829
and AI OSTP couldn't really make it work

00:13:48,339 --> 00:13:54,529
so I figured I just I just couldn't be

00:13:51,829 --> 00:13:56,119
simpler for me to do this myself so I

00:13:54,529 --> 00:13:58,730
had already worked with middlewares in a

00:13:56,119 --> 00:14:00,019
iOS CP and I knew I could easily

00:13:58,730 --> 00:14:02,179
implement something like this as

00:14:00,019 --> 00:14:05,809
middleware and literally five minutes

00:14:02,179 --> 00:14:07,879
later this what I have very typical

00:14:05,809 --> 00:14:10,249
Middleburg just runs the request Handler

00:14:07,879 --> 00:14:12,709
race and if the request has the race is

00:14:10,249 --> 00:14:14,959
on and Kotik uncaught exception my

00:14:12,709 --> 00:14:18,110
sentry middlebury will will capture the

00:14:14,959 --> 00:14:21,709
exception fire it off to sentry and then

00:14:18,110 --> 00:14:24,079
just race again super simple so

00:14:21,709 --> 00:14:26,839
obviously this also makes use of the

00:14:24,079 --> 00:14:29,869
fact that the Python century client

00:14:26,839 --> 00:14:34,179
library also has support for a a iOS TP

00:14:29,869 --> 00:14:36,709
as a transport so so far so good

00:14:34,179 --> 00:14:39,230
unfortunately my migration was not

00:14:36,709 --> 00:14:41,779
completely without issues one of the

00:14:39,230 --> 00:14:44,629
features of my micro service prior to my

00:14:41,779 --> 00:14:47,329
migration was that its API was built and

00:14:44,629 --> 00:14:49,579
defined in slacker and it had some kind

00:14:47,329 --> 00:14:50,929
of fancy swagger UI thing on top of it

00:14:49,579 --> 00:14:53,799
so you could kind of interact with the

00:14:50,929 --> 00:14:56,360
API it's really cool

00:14:53,799 --> 00:14:58,329
anyway this support came through a

00:14:56,360 --> 00:15:01,160
library which is called connection

00:14:58,329 --> 00:15:02,509
probably some of you know it and after

00:15:01,160 --> 00:15:06,740
research in connection for a little bit

00:15:02,509 --> 00:15:09,199
I it looked like it would support using

00:15:06,740 --> 00:15:11,179
a iOS DP as a back-end but then I kind

00:15:09,199 --> 00:15:14,119
of tried it out and it didn't really

00:15:11,179 --> 00:15:17,299
work and the documentation was kind of

00:15:14,119 --> 00:15:18,619
non-existent so what I probably should

00:15:17,299 --> 00:15:20,059
have done of this point in time was just

00:15:18,619 --> 00:15:22,819
to fix the connection library and the

00:15:20,059 --> 00:15:25,970
documentation but so my to-do list no

00:15:22,819 --> 00:15:29,419
berries yeah instead I just dropped the

00:15:25,970 --> 00:15:31,910
slacker thing so up until this point

00:15:29,419 --> 00:15:34,130
I've been talking about async i/o like

00:15:31,910 --> 00:15:37,880
it's the perfect piece of software

00:15:34,130 --> 00:15:40,639
and it's not yeah that's just it's

00:15:37,880 --> 00:15:42,440
absolutely not there are quite a few

00:15:40,639 --> 00:15:45,440
things you might want to be aware of

00:15:42,440 --> 00:15:47,060
before starting to use async IO and I'm

00:15:45,440 --> 00:15:49,790
just gonna mention some of the issues

00:15:47,060 --> 00:15:53,290
that I've personally experienced you

00:15:49,790 --> 00:15:56,480
read spoke about more issues earlier and

00:15:53,290 --> 00:15:58,670
yeah and I think someone is talking even

00:15:56,480 --> 00:16:02,480
more about issues tomorrow in another

00:15:58,670 --> 00:16:04,730
talk so plenty of that first while async

00:16:02,480 --> 00:16:06,980
IO is conceptually simple in Python

00:16:04,730 --> 00:16:10,910
there is a steep learning curve to

00:16:06,980 --> 00:16:12,920
understand the whole beast we're talking

00:16:10,910 --> 00:16:15,050
about event loops event loop policies

00:16:12,920 --> 00:16:17,660
available zko routines generators

00:16:15,050 --> 00:16:21,350
futures tasks at executors transports

00:16:17,660 --> 00:16:24,079
protocols etc etc etc so if you're

00:16:21,350 --> 00:16:27,350
migrating a large important code base to

00:16:24,079 --> 00:16:30,019
you know use async i/o you ideally want

00:16:27,350 --> 00:16:32,120
to understand most of these and kind of

00:16:30,019 --> 00:16:34,550
how they interplay so yeah this takes

00:16:32,120 --> 00:16:38,209
time another issue that many developers

00:16:34,550 --> 00:16:41,029
that try out async i/o mention is as

00:16:38,209 --> 00:16:42,889
soon as you start developing you like

00:16:41,029 --> 00:16:44,870
migrating your Python code base to use

00:16:42,889 --> 00:16:47,680
async i/o your code base is going to be

00:16:44,870 --> 00:16:51,470
littered with async and the weight

00:16:47,680 --> 00:16:53,569
everywhere to me like yeah I can't

00:16:51,470 --> 00:16:56,060
experience this but to me this is like

00:16:53,569 --> 00:16:59,480
this is not a huge deal this is just the

00:16:56,060 --> 00:17:02,660
cost of explicitness and I'm gonna

00:16:59,480 --> 00:17:04,339
welcome that an issue that definitely

00:17:02,660 --> 00:17:06,079
did hit me personally was the fact that

00:17:04,339 --> 00:17:08,329
sometimes the async i/o stacked would

00:17:06,079 --> 00:17:10,939
just you know make my debugging live a

00:17:08,329 --> 00:17:12,919
little bit too difficult by kind of

00:17:10,939 --> 00:17:16,220
swallowing my exceptions and I had like

00:17:12,919 --> 00:17:18,100
no idea what's going on but this is

00:17:16,220 --> 00:17:21,110
definitely getting better every day and

00:17:18,100 --> 00:17:22,459
this is something that URI the one of

00:17:21,110 --> 00:17:24,799
the Python core committers

00:17:22,459 --> 00:17:26,780
around async i/o spoke about earlier as

00:17:24,799 --> 00:17:29,210
well so I'm very happy to hear that

00:17:26,780 --> 00:17:32,270
now accidentally running synchronous

00:17:29,210 --> 00:17:34,490
code that waits for I IO inside of your

00:17:32,270 --> 00:17:38,270
asynchronous functions yet another

00:17:34,490 --> 00:17:40,850
source of issues here what I can tell

00:17:38,270 --> 00:17:42,500
you is like it's very simple to monitor

00:17:40,850 --> 00:17:44,720
the init loop it's basically just about

00:17:42,500 --> 00:17:47,420
figuring out which tasks on the image

00:17:44,720 --> 00:17:50,360
loop take longer than X milliseconds

00:17:47,420 --> 00:17:52,460
and a if some tasks are doing that this

00:17:50,360 --> 00:17:54,220
might be a sign that it's actually doing

00:17:52,460 --> 00:17:58,150
some IO or some kind of blocking i/o

00:17:54,220 --> 00:17:59,930
inside of the async functions so yeah as

00:17:58,150 --> 00:18:03,680
mentioned this is just some of the

00:17:59,930 --> 00:18:06,740
examples so to summarize if you plan on

00:18:03,680 --> 00:18:08,450
migrating large project through async IO

00:18:06,740 --> 00:18:10,490
or you know create the project from

00:18:08,450 --> 00:18:12,470
scratch the most important thing I can

00:18:10,490 --> 00:18:15,100
tell you is just you know do the

00:18:12,470 --> 00:18:18,200
research first map out your dependencies

00:18:15,100 --> 00:18:21,560
see if there exists async IO compatible

00:18:18,200 --> 00:18:23,870
versions of them and you know if they

00:18:21,560 --> 00:18:26,150
exist actually make sure that they work

00:18:23,870 --> 00:18:28,520
and you know check them out like that's

00:18:26,150 --> 00:18:30,700
very useful what's out for some of the

00:18:28,520 --> 00:18:33,890
cultures and yeah profits

00:18:30,700 --> 00:18:36,020
so you've migrated all of your projects

00:18:33,890 --> 00:18:40,640
to async IO was it worth it

00:18:36,020 --> 00:18:43,640
are you any better off let's see how it

00:18:40,640 --> 00:18:45,470
went for me I'm gonna start out by

00:18:43,640 --> 00:18:47,660
taking a look at a kind of p4 after

00:18:45,470 --> 00:18:50,000
evaluation of micro sir the micro

00:18:47,660 --> 00:18:51,920
service that I spoke about earlier I've

00:18:50,000 --> 00:18:55,280
already mentioned the initial version of

00:18:51,920 --> 00:18:58,850
it had you know flask to declare an API

00:18:55,280 --> 00:19:01,010
that's psycho PTT to to communicate the

00:18:58,850 --> 00:19:04,400
Postgres database a lot of other stuff

00:19:01,010 --> 00:19:06,230
that matters less also utilized even

00:19:04,400 --> 00:19:08,360
that to make everything asynchronous

00:19:06,230 --> 00:19:10,790
behind the scenes so in theory it should

00:19:08,360 --> 00:19:12,620
have been quite efficient we've also

00:19:10,790 --> 00:19:16,400
gone through how the after version of

00:19:12,620 --> 00:19:18,800
this micro service looked like a iOS DP

00:19:16,400 --> 00:19:21,140
as a web framework async PG as a post

00:19:18,800 --> 00:19:22,940
curse database driver and to run the

00:19:21,140 --> 00:19:25,070
event loop itself we used the Python

00:19:22,940 --> 00:19:28,610
implementation of the UV loop which is

00:19:25,070 --> 00:19:30,800
apparently very fast and this is can our

00:19:28,610 --> 00:19:34,790
experimental setup quite standard we

00:19:30,800 --> 00:19:37,310
used the word project or WR k to perform

00:19:34,790 --> 00:19:40,340
HTTP benchmarking of both versions of

00:19:37,310 --> 00:19:42,890
the service we ran its configuration of

00:19:40,340 --> 00:19:44,930
the benchmark for 30 seconds 10 times

00:19:42,890 --> 00:19:48,560
using a variable number of connections

00:19:44,930 --> 00:19:51,940
and noted the median 25th percentile and

00:19:48,560 --> 00:19:55,310
75th percentile in terms of throughput

00:19:51,940 --> 00:19:57,410
recurse per second for all of the 10

00:19:55,310 --> 00:19:59,240
runs we also had some delay between runs

00:19:57,410 --> 00:20:00,519
just to kind of allow the server and

00:19:59,240 --> 00:20:03,009
client to settle

00:20:00,519 --> 00:20:06,549
and on this slide you can see this

00:20:03,009 --> 00:20:08,019
throughput comparison of the micro

00:20:06,549 --> 00:20:09,729
service running and quite a basic

00:20:08,019 --> 00:20:11,979
configuration with only one Kunekune

00:20:09,729 --> 00:20:15,129
worker and just on my machine

00:20:11,979 --> 00:20:18,219
so just in development we then you use

00:20:15,129 --> 00:20:20,889
this work thing the benchmarking tool to

00:20:18,219 --> 00:20:22,719
test the throughput of both versions on

00:20:20,889 --> 00:20:24,580
a very simple just ping route that

00:20:22,719 --> 00:20:27,149
doesn't really do anything interesting

00:20:24,580 --> 00:20:30,070
it kind of just returns the string pong

00:20:27,149 --> 00:20:31,989
and we use this quite a lot in our micro

00:20:30,070 --> 00:20:34,989
services just to do health checking like

00:20:31,989 --> 00:20:37,089
are you alive and as you can see the

00:20:34,989 --> 00:20:38,559
results here are quite conclusive like

00:20:37,089 --> 00:20:40,959
the async I aversion of the micro

00:20:38,559 --> 00:20:42,729
service that is the yellow line is able

00:20:40,959 --> 00:20:45,820
to handle long number like on average

00:20:42,729 --> 00:20:47,320
1400 Rica's per second while the even

00:20:45,820 --> 00:20:50,229
flat version is able to have the summer

00:20:47,320 --> 00:20:52,239
around 650 so in this case the

00:20:50,229 --> 00:20:55,269
performance increase is somewhere on

00:20:52,239 --> 00:20:57,519
twofold here we can see another

00:20:55,269 --> 00:20:59,619
benchmark that we did which is on a

00:20:57,519 --> 00:21:01,450
route that actually toss things it does

00:20:59,619 --> 00:21:04,539
you know some interesting database

00:21:01,450 --> 00:21:07,149
accesses and then kind of returns the

00:21:04,539 --> 00:21:09,089
results of the the database access and

00:21:07,149 --> 00:21:11,889
some transformations and in this

00:21:09,089 --> 00:21:14,499
experiment it seems like the P before

00:21:11,889 --> 00:21:16,629
the events let version is able to handle

00:21:14,499 --> 00:21:19,539
server around 200 to 300 recurs two

00:21:16,629 --> 00:21:21,969
seconds while the async version the

00:21:19,539 --> 00:21:24,969
yellow line again seems to be able to

00:21:21,969 --> 00:21:26,649
handle some around 800 to 1000 now the

00:21:24,969 --> 00:21:31,299
most interesting one is probably this

00:21:26,649 --> 00:21:33,759
one this is an actual plans mark on the

00:21:31,299 --> 00:21:35,559
micro service both of them in production

00:21:33,759 --> 00:21:37,989
both before and after the migration and

00:21:35,559 --> 00:21:39,879
in this particular experiment the number

00:21:37,989 --> 00:21:42,159
of Kunekune workers were just set to 3

00:21:39,879 --> 00:21:44,289
and we only ran the benchmarks on one

00:21:42,159 --> 00:21:46,629
application instance usually we have

00:21:44,289 --> 00:21:48,700
more application instances in production

00:21:46,629 --> 00:21:50,649
but testing all of them in conjunction

00:21:48,700 --> 00:21:52,239
is kind of besides the point here

00:21:50,649 --> 00:21:54,279
because they are supposed to be able to

00:21:52,239 --> 00:21:58,379
scale linearly if the load balancer that

00:21:54,279 --> 00:22:01,179
you're using is ok so as you can see

00:21:58,379 --> 00:22:04,539
here the before version the events that

00:22:01,179 --> 00:22:07,599
version of the micro service performs ok

00:22:04,539 --> 00:22:09,989
like it's route handling 1 like 1,300

00:22:07,599 --> 00:22:13,040
Rica's per second quite consistently

00:22:09,989 --> 00:22:15,770
while the async IO version

00:22:13,040 --> 00:22:18,770
whew Singh Lee the green line in this

00:22:15,770 --> 00:22:20,990
graph is able to have around 7,000

00:22:18,770 --> 00:22:23,450
Reapers per second and this is more than

00:22:20,990 --> 00:22:25,310
a five-fold increase in throughput so

00:22:23,450 --> 00:22:29,960
this kind of means that we can just get

00:22:25,310 --> 00:22:31,390
rid of like 80% of the application

00:22:29,960 --> 00:22:34,700
instances that we have in production

00:22:31,390 --> 00:22:37,790
which is quite nice because we save a

00:22:34,700 --> 00:22:40,750
lot of money so just to conclude the

00:22:37,790 --> 00:22:44,270
whole thing is async I overt the efforts

00:22:40,750 --> 00:22:46,420
we finally have explicit native support

00:22:44,270 --> 00:22:48,590
in Python to do asynchronous programming

00:22:46,420 --> 00:22:51,350
that is kind of slowly but surely

00:22:48,590 --> 00:22:54,340
becoming the de facto standard in the

00:22:51,350 --> 00:22:58,970
community and kind of replacing other

00:22:54,340 --> 00:23:02,540
concurrency libraries and as we just saw

00:22:58,970 --> 00:23:05,480
the performance the performance for off

00:23:02,540 --> 00:23:08,210
I see a sink IO is quite good it's very

00:23:05,480 --> 00:23:11,270
positive results and this mirror what I

00:23:08,210 --> 00:23:12,710
have seen on the map so all in all yes I

00:23:11,270 --> 00:23:17,500
think it's definitely worth it go check

00:23:12,710 --> 00:23:17,500
it out if you have not thank you

00:23:23,840 --> 00:23:50,160
all right we have time for questions I

00:23:47,340 --> 00:23:52,710
have yes I was doing some like I was

00:23:50,160 --> 00:23:54,140
using some stuff in our code that was

00:23:52,710 --> 00:23:56,880
apparently doing some very interesting

00:23:54,140 --> 00:24:00,630
requests like using the recast libraries

00:23:56,880 --> 00:24:03,539
somewhere it kind of deep down I didn't

00:24:00,630 --> 00:24:05,850
really understand why I was doing it but

00:24:03,539 --> 00:24:08,970
yes I experienced this problem and it's

00:24:05,850 --> 00:24:18,840
completely terrible very hard to figure

00:24:08,970 --> 00:24:21,659
out in this case well like this is

00:24:18,840 --> 00:24:23,730
basically an issue that like it didn't

00:24:21,659 --> 00:24:26,070
take a long time for me to figure out by

00:24:23,730 --> 00:24:28,200
this boss because I saw that some

00:24:26,070 --> 00:24:31,409
requests were just taking too long and

00:24:28,200 --> 00:24:33,450
we were basically everything was being

00:24:31,409 --> 00:24:36,299
held up because of this one request

00:24:33,450 --> 00:24:38,220
where I was doing like this thing so it

00:24:36,299 --> 00:24:40,260
was pretty simple for me to do it

00:24:38,220 --> 00:24:43,260
I basically just found the code and I

00:24:40,260 --> 00:24:45,360
just literally threw it out it wasn't

00:24:43,260 --> 00:24:47,760
necessary so I replaced it

00:24:45,360 --> 00:24:49,679
so that was my solution like another

00:24:47,760 --> 00:24:52,080
solution would have been to actually

00:24:49,679 --> 00:24:54,360
start using the AI OSTP client instead

00:24:52,080 --> 00:24:57,090
of the recast library then everything

00:24:54,360 --> 00:25:17,429
probably like it would have been able to

00:24:57,090 --> 00:25:21,630
make this work waiting in the queue in

00:25:17,429 --> 00:25:24,480
the main right so running thousand

00:25:21,630 --> 00:25:26,700
processes each with like a nice async

00:25:24,480 --> 00:25:28,740
event loop or

00:25:26,700 --> 00:25:30,600
like we are waiting for the tasks yeah

00:25:28,740 --> 00:25:32,400
they're in the meantime you're just

00:25:30,600 --> 00:25:33,570
waiting in the loop on the queue and

00:25:32,400 --> 00:25:40,850
they're just the process of writing

00:25:33,570 --> 00:25:40,850
inside I'm not sure I understand but

00:25:42,110 --> 00:25:48,660
using processes for the using actual

00:25:46,230 --> 00:25:55,140
like operating system processes oh yeah

00:25:48,660 --> 00:25:57,300
I see I think there is probably a lot of

00:25:55,140 --> 00:25:59,160
overhead in that to be honest like a lot

00:25:57,300 --> 00:26:01,950
of overhead compared to what async IO

00:25:59,160 --> 00:26:04,170
gives us which is very lightweight kind

00:26:01,950 --> 00:26:06,960
of you know just like some kind of task

00:26:04,170 --> 00:26:08,340
queue it's running the queue if you you

00:26:06,960 --> 00:26:10,230
have to maintain all of these processes

00:26:08,340 --> 00:26:12,660
and the operating system has to switch

00:26:10,230 --> 00:26:14,160
between them correctly so I think there

00:26:12,660 --> 00:26:42,780
would probably be a lot of overhead in

00:26:14,160 --> 00:26:46,050
there I can no I don't I don't but like

00:26:42,780 --> 00:26:47,970
I feel like I need to like I have never

00:26:46,050 --> 00:26:51,300
done anything with transports or

00:26:47,970 --> 00:26:53,160
protocols but given that I'm writing an

00:26:51,300 --> 00:26:55,260
actual you know in production very

00:26:53,160 --> 00:26:57,720
important micro service I feel like I

00:26:55,260 --> 00:27:02,940
need to be on top of what this stuff is

00:26:57,720 --> 00:27:04,920
doing no I'm not saying that absolutely

00:27:02,940 --> 00:27:06,330
not I'm just I'm just saying I should

00:27:04,920 --> 00:27:08,640
probably know what these kind of basic

00:27:06,330 --> 00:27:11,190
building blocks that are behind async IO

00:27:08,640 --> 00:27:13,470
I should know what they do but no I

00:27:11,190 --> 00:27:14,970
haven't used them and Yuri actually

00:27:13,470 --> 00:27:17,520
talked like talked about this earlier

00:27:14,970 --> 00:27:19,440
like I'm not supposed to be able like

00:27:17,520 --> 00:27:22,440
I'm not supposed to need to know what

00:27:19,440 --> 00:27:34,350
these things are so maybe I should just

00:27:22,440 --> 00:27:37,280
take his word maybe but no but I still

00:27:34,350 --> 00:27:37,280
feel like I need to know it

00:27:45,920 --> 00:27:51,060
well yeah yeah so basically the API is

00:27:49,440 --> 00:27:52,770
no longer defined in swagger and we

00:27:51,060 --> 00:27:59,160
don't have the swagger UI for this

00:27:52,770 --> 00:27:59,840
particular microservice which is I have

00:27:59,160 --> 00:28:03,180
not

00:27:59,840 --> 00:28:05,630
so that exists oh that's very good to

00:28:03,180 --> 00:28:21,090
know that's very good to know thank you

00:28:05,630 --> 00:28:24,470
learning a lot today so how should I put

00:28:21,090 --> 00:28:26,970
it like the code that I'm working with

00:28:24,470 --> 00:28:30,480
the company that I'm working for

00:28:26,970 --> 00:28:32,280
it doesn't use our arms a lot and

00:28:30,480 --> 00:28:37,140
definitely the protests that I have been

00:28:32,280 --> 00:28:38,970
using they don't so if you are yes you

00:28:37,140 --> 00:28:40,380
probably that's like this is one of the

00:28:38,970 --> 00:28:42,960
research things that you would need to

00:28:40,380 --> 00:28:45,420
do if you want to migrate a project

00:28:42,960 --> 00:28:47,160
that's using on an ORM you're gonna need

00:28:45,420 --> 00:28:51,060
to figure out if there is some support

00:28:47,160 --> 00:28:53,820
for over amps somewhere else I I don't

00:28:51,060 --> 00:28:56,280
know the answer like I don't really use

00:28:53,820 --> 00:28:59,280
our arms we just use you know like

00:28:56,280 --> 00:29:11,790
sequel queries directly so yeah resource

00:28:59,280 --> 00:29:13,920
it some of the shortcomings of yeah like

00:29:11,790 --> 00:29:15,270
I probably run into some of these but

00:29:13,920 --> 00:29:17,130
like these are not things that I'm

00:29:15,270 --> 00:29:19,200
dealing with a lot these services that

00:29:17,130 --> 00:29:21,360
like for example this story's not

00:29:19,200 --> 00:29:23,250
complex it's not it doesn't have to deal

00:29:21,360 --> 00:29:27,690
with a lot of timeouts and stuff like

00:29:23,250 --> 00:29:30,110
that so I haven't really run into a lot

00:29:27,690 --> 00:29:30,110
of these No

00:29:32,930 --> 00:29:38,480
[Music]

00:29:35,480 --> 00:29:38,480

YouTube URL: https://www.youtube.com/watch?v=zM3cMTcmmk0


