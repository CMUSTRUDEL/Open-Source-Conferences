Title: Anmol Krishan Sachdeva - Understanding and Implementing Recurrent Neural Networks using Python
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Understanding and Implementing Recurrent Neural Networks using Python
[EuroPython 2018 - Talk - 2018-07-25 - PyCharm [PyData]]
[Edinburgh, UK]

By Anmol Krishan Sachdeva

Recurrent Neural Networks (RNNs) have become famous over time due to their property of retaining internal memory. These neural nets are widely used in recognizing patterns in sequences of data, like numerical timer series data, images, handwritten text, spoken words, genome sequences, and much more. Since these nets possess memory, there is a certain analogy that we can make to the human brain in order to learn how RNNs work. RNNs can be thought of as a network of neurons with feedback connections, unlike feedforward connections which exist in other types of Artificial Neural Networks.

The flow of talk will be as follows: - Self Introduction - Introduction to Deep Learning - Artificial Neural Networks (ANNs) - Diving DEEP into Recurrent Neural Networks (RNNs) - Comparing Feedforward Networks with Feedback Networks - Quick walkthrough: Implementing RNNs using Python (Keras) - Understanding Backpropagation Through Time (BPTT) and Vanishing Gradient Problem - Towards more sophisticated RNNs: Gated Recurrent Units (GRUs)/Long Short-Term Memory (LSTMs) - End of talk - Questions and Answers Session



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,000 --> 00:00:06,029
all right a very warm welcome and good

00:00:03,659 --> 00:00:07,649
morning all of you my name is amol

00:00:06,029 --> 00:00:09,510
Christian sudeva and the topic for today

00:00:07,649 --> 00:00:12,750
is understanding and implementing

00:00:09,510 --> 00:00:16,590
recurrent neural networks so before

00:00:12,750 --> 00:00:18,750
starting let's first see an example if

00:00:16,590 --> 00:00:21,060
you are reading a book and it has say

00:00:18,750 --> 00:00:22,680
five chapters we are on chapter one we

00:00:21,060 --> 00:00:24,840
have read that we are going to chapter

00:00:22,680 --> 00:00:27,090
two we are starting reading it but

00:00:24,840 --> 00:00:29,490
suddenly we forgot everything that was

00:00:27,090 --> 00:00:32,189
there in Chapter one so how will we be

00:00:29,490 --> 00:00:34,620
able to understand chapter two it means

00:00:32,189 --> 00:00:39,450
short-term memory plays a very crucial

00:00:34,620 --> 00:00:41,570
role on that motion I would like to

00:00:39,450 --> 00:00:46,320
start with the recurrent neural networks

00:00:41,570 --> 00:00:49,350
so Martin has already introduced me I'll

00:00:46,320 --> 00:00:51,719
be skipping this prerequisites are you

00:00:49,350 --> 00:00:53,879
should be aware of Python language you

00:00:51,719 --> 00:00:55,579
should have a decent knowledge of

00:00:53,879 --> 00:00:59,100
artificial neural networks and

00:00:55,579 --> 00:01:01,199
elementary linear algebra so recurrent

00:00:59,100 --> 00:01:02,910
neural networks can be thought of as the

00:01:01,199 --> 00:01:06,020
neural networks which persist

00:01:02,910 --> 00:01:09,560
information and we can think it off as

00:01:06,020 --> 00:01:13,290
sequential processes basically

00:01:09,560 --> 00:01:14,850
influencing decisions so in contrast to

00:01:13,290 --> 00:01:16,740
the traditional neural networks which

00:01:14,850 --> 00:01:19,080
don't persist information recurrent

00:01:16,740 --> 00:01:21,439
neural networks are having this

00:01:19,080 --> 00:01:23,939
advantage over those so we can think

00:01:21,439 --> 00:01:26,159
recurrent neural networks as the

00:01:23,939 --> 00:01:28,070
networks having loops to themselves I'll

00:01:26,159 --> 00:01:30,329
be explaining the architecture now and

00:01:28,070 --> 00:01:32,189
these are one of the most complex

00:01:30,329 --> 00:01:37,020
supervised deep learning algorithms we

00:01:32,189 --> 00:01:41,250
have today so consider a normal neural

00:01:37,020 --> 00:01:43,680
network and it has many layers X naught

00:01:41,250 --> 00:01:47,009
X 1 X 2 is basically the input that we

00:01:43,680 --> 00:01:49,229
are providing to the layer a is the

00:01:47,009 --> 00:01:52,200
hidden layer so the input from X naught

00:01:49,229 --> 00:01:53,610
goes to a and the input gets transferred

00:01:52,200 --> 00:01:56,009
from one hidden layer to another hidden

00:01:53,610 --> 00:01:58,439
layer simultaneously it gets transferred

00:01:56,009 --> 00:02:00,229
from the other hidden layer to the next

00:01:58,439 --> 00:02:03,030
hidden layer and we can think it off

00:02:00,229 --> 00:02:05,549
squashing this thing that you see on

00:02:03,030 --> 00:02:07,950
your right hand side into the thing that

00:02:05,549 --> 00:02:12,140
is known as the vanilla RN and that

00:02:07,950 --> 00:02:12,140
means it is having loop to itself

00:02:13,280 --> 00:02:19,490
rnns can be of many architectures the

00:02:17,510 --> 00:02:21,800
first one is one too many architecture

00:02:19,490 --> 00:02:26,120
in which you provide one input and you

00:02:21,800 --> 00:02:29,960
can get many outputs so you can think of

00:02:26,120 --> 00:02:32,930
say you provide with an image and it

00:02:29,960 --> 00:02:35,990
provides with certain caption so an

00:02:32,930 --> 00:02:38,240
image is mapped into caption of say five

00:02:35,990 --> 00:02:40,190
or six words so it is one image getting

00:02:38,240 --> 00:02:42,310
mapped into five or six words that is

00:02:40,190 --> 00:02:45,080
one-to-many transformation of RNN

00:02:42,310 --> 00:02:47,780
likewise we have many to one so many to

00:02:45,080 --> 00:02:51,590
one can be thought of as say we are

00:02:47,780 --> 00:02:54,320
having video frames and we are

00:02:51,590 --> 00:02:56,300
generating text out of the video so it

00:02:54,320 --> 00:03:00,440
can be mapped to many to many or many to

00:02:56,300 --> 00:03:04,100
one so we can get from a video at a

00:03:00,440 --> 00:03:08,660
sentence and similarly we can have many

00:03:04,100 --> 00:03:09,860
to many transformation also so recurrent

00:03:08,660 --> 00:03:12,310
neural networks have the following

00:03:09,860 --> 00:03:14,840
applications that is image captioning

00:03:12,310 --> 00:03:17,150
subtitle generation time series

00:03:14,840 --> 00:03:19,070
classification language modeling natural

00:03:17,150 --> 00:03:20,870
language processing even chatbot

00:03:19,070 --> 00:03:26,630
development is also based on RN lens

00:03:20,870 --> 00:03:29,900
nowadays so there is a major problem in

00:03:26,630 --> 00:03:33,830
the vanilla and in that is if the RN n

00:03:29,900 --> 00:03:36,980
is having so many layers so adjusting

00:03:33,830 --> 00:03:40,820
the weights at each hidden layer is a

00:03:36,980 --> 00:03:42,830
problem that means once the information

00:03:40,820 --> 00:03:45,800
from X naught is transferred to a it is

00:03:42,830 --> 00:03:47,720
multiplied by some weight matrix then

00:03:45,800 --> 00:03:49,459
transferred to the next layer then

00:03:47,720 --> 00:03:52,070
transferred to next then transferred to

00:03:49,459 --> 00:03:54,380
next and simultaneously the output gets

00:03:52,070 --> 00:03:56,209
generated on the top now we have

00:03:54,380 --> 00:03:58,730
something called as loss function that

00:03:56,209 --> 00:04:01,730
is the difference between the actual

00:03:58,730 --> 00:04:04,660
output and the predicted output so if

00:04:01,730 --> 00:04:07,940
the loss functions for say XT plus 1

00:04:04,660 --> 00:04:11,870
generates some error so there is some

00:04:07,940 --> 00:04:13,640
like error of say 0.15 percent and it's

00:04:11,870 --> 00:04:16,250
neat to be back propagated throughout

00:04:13,640 --> 00:04:18,650
the network and back propagating through

00:04:16,250 --> 00:04:23,660
a network that is so large is difficult

00:04:18,650 --> 00:04:25,260
because each time you can think of say W

00:04:23,660 --> 00:04:29,250
is getting multiplied at

00:04:25,260 --> 00:04:31,620
each live and multiplying W that is

00:04:29,250 --> 00:04:34,440
between 0 and 1 multiplying anything by

00:04:31,620 --> 00:04:36,900
0 and 1 between 0 and 1 say zero point

00:04:34,440 --> 00:04:39,410
two if we multiply something by zero

00:04:36,900 --> 00:04:42,660
point to multiple times then it will be

00:04:39,410 --> 00:04:45,750
tending to very small value so it will

00:04:42,660 --> 00:04:48,840
be like the gradient will be propagating

00:04:45,750 --> 00:04:51,600
through the whole chain but it will be

00:04:48,840 --> 00:04:53,730
taking much time to Train so it is not

00:04:51,600 --> 00:04:55,260
feasible solution so when in our and

00:04:53,730 --> 00:04:59,010
then suppose from the vanishing gradient

00:04:55,260 --> 00:05:00,720
problem likewise we have the exploding

00:04:59,010 --> 00:05:03,240
gradient problem so exploring gradient

00:05:00,720 --> 00:05:05,600
problem is when the value of W is

00:05:03,240 --> 00:05:10,980
greater than one so if you multiply

00:05:05,600 --> 00:05:13,290
simultaneously a number more by say a

00:05:10,980 --> 00:05:17,670
factor of more than one then it will

00:05:13,290 --> 00:05:21,780
tend to always go and multiply to many

00:05:17,670 --> 00:05:24,510
huge values now the vanishing gradient

00:05:21,780 --> 00:05:26,760
problem can be thought of as W is less

00:05:24,510 --> 00:05:28,890
than 1 and exploding gradient can be

00:05:26,760 --> 00:05:30,840
thought of as the blue is greater than 1

00:05:28,890 --> 00:05:32,430
to solve these vanishing gradient

00:05:30,840 --> 00:05:36,120
problem and the exploring gradient

00:05:32,430 --> 00:05:38,370
plopping problems we use certain

00:05:36,120 --> 00:05:41,060
techniques so for exploring gradient

00:05:38,370 --> 00:05:43,680
plop problem we have truncated black

00:05:41,060 --> 00:05:47,070
propagation so in truncated back

00:05:43,680 --> 00:05:49,380
propagation we divide the whole set into

00:05:47,070 --> 00:05:52,230
certain batches and in those certain

00:05:49,380 --> 00:05:55,280
batches we just back propagate within

00:05:52,230 --> 00:05:57,720
those certain batches sequentially in

00:05:55,280 --> 00:05:59,400
system of rewards and penalties it is

00:05:57,720 --> 00:06:02,910
like reinforcement learning so we

00:05:59,400 --> 00:06:04,770
provide rewards for say if the back

00:06:02,910 --> 00:06:07,110
propagation is doing well else we

00:06:04,770 --> 00:06:09,930
provide penalties and we have gradient

00:06:07,110 --> 00:06:11,610
clipping if if the gradient goes beyond

00:06:09,930 --> 00:06:13,890
some range then we just clip that

00:06:11,610 --> 00:06:15,660
gradient and the we don't propagate it

00:06:13,890 --> 00:06:17,370
through the network for managing

00:06:15,660 --> 00:06:19,790
gradient problem we have smart weight

00:06:17,370 --> 00:06:22,470
initialization that is like a guess work

00:06:19,790 --> 00:06:24,930
then we have Echo state networks and we

00:06:22,470 --> 00:06:27,270
have LS TM so I'll be talking today of

00:06:24,930 --> 00:06:31,260
LST M that is long short term memory and

00:06:27,270 --> 00:06:34,170
it is one of the most use variants of

00:06:31,260 --> 00:06:38,550
RNN so LS TM is one of the most used

00:06:34,170 --> 00:06:39,150
variant of RNN and the approach for lsdm

00:06:38,550 --> 00:06:42,030
is

00:06:39,150 --> 00:06:44,730
making the weight W equal to one so you

00:06:42,030 --> 00:06:46,080
can think of you are not having W less

00:06:44,730 --> 00:06:48,210
than one you are not having the blue

00:06:46,080 --> 00:06:52,460
greater than one so what else we can do

00:06:48,210 --> 00:06:52,460
we can just make W equal to one now

00:06:52,940 --> 00:06:59,870
coming to the architecture of the lsdm C

00:06:56,820 --> 00:07:03,690
is the new introduction in here HT is

00:06:59,870 --> 00:07:06,210
the state of the current input the

00:07:03,690 --> 00:07:10,200
hidden layer and C T can be thought of

00:07:06,210 --> 00:07:13,520
as a cell state so in here if we provide

00:07:10,200 --> 00:07:18,060
the input XT and the hidden layer and we

00:07:13,520 --> 00:07:21,090
transfer this thing to four states f IG

00:07:18,060 --> 00:07:24,300
o f is the you can say the final gate of

00:07:21,090 --> 00:07:26,220
or gate gate is the input gate G is

00:07:24,300 --> 00:07:29,820
another gate there's no name for it and

00:07:26,220 --> 00:07:32,550
O is the output gate we take we apply

00:07:29,820 --> 00:07:36,240
the function like the element wise

00:07:32,550 --> 00:07:39,060
multiplication and the addition using

00:07:36,240 --> 00:07:42,240
this formula so see T is basically F

00:07:39,060 --> 00:07:45,240
times that is forget gate times the CT

00:07:42,240 --> 00:07:47,400
minus one that is the previous state the

00:07:45,240 --> 00:07:51,300
the previous state of the cell that was

00:07:47,400 --> 00:07:54,780
there so it is basically forgetting some

00:07:51,300 --> 00:07:57,780
part of the memory so it is like you you

00:07:54,780 --> 00:07:59,520
were reading something and you actually

00:07:57,780 --> 00:08:02,130
forgot some part of it but you retain

00:07:59,520 --> 00:08:04,070
certain part of it so this is actually

00:08:02,130 --> 00:08:06,960
forgetting major part of it and

00:08:04,070 --> 00:08:08,940
retaining some part of it and that some

00:08:06,960 --> 00:08:13,860
part is used for training the rest of

00:08:08,940 --> 00:08:15,690
the network and this is actually applied

00:08:13,860 --> 00:08:19,440
through the tenets function and the

00:08:15,690 --> 00:08:25,020
output gate is multiplied and we get the

00:08:19,440 --> 00:08:29,940
output of the hidden layer so comparing

00:08:25,020 --> 00:08:31,860
the RNN and the LS TM it is we can say

00:08:29,940 --> 00:08:34,080
the LS TM has a sophisticated

00:08:31,860 --> 00:08:36,930
architecture in terms that it has a self

00:08:34,080 --> 00:08:39,390
State and using this cell state it is

00:08:36,930 --> 00:08:42,150
like a superhighway through which W is

00:08:39,390 --> 00:08:46,220
not getting changed so so we are

00:08:42,150 --> 00:08:48,900
actually changing the element-wise

00:08:46,220 --> 00:08:50,370
fraction that is f and we are not

00:08:48,900 --> 00:08:52,800
changing the weight matrix so

00:08:50,370 --> 00:08:54,929
multiplying anything by element

00:08:52,800 --> 00:08:56,910
or scalar multiplication is much simpler

00:08:54,929 --> 00:08:59,189
than the matrix multiplication which is

00:08:56,910 --> 00:09:01,139
involved in the weight matrix

00:08:59,189 --> 00:09:02,850
multiplication that is used in the

00:09:01,139 --> 00:09:04,860
vanilla RN n so that is why the

00:09:02,850 --> 00:09:10,470
vanishing gradient problem goes off in

00:09:04,860 --> 00:09:14,399
lsdm now we can start building the LST m

00:09:10,470 --> 00:09:18,869
and the code I'll be providing for that

00:09:14,399 --> 00:09:23,579
so you can just join in this you can

00:09:18,869 --> 00:09:25,949
just implement this as it as it is now I

00:09:23,579 --> 00:09:30,269
will just shifting to the implementation

00:09:25,949 --> 00:09:34,429
thing so the major tasks of implementing

00:09:30,269 --> 00:09:34,429
the RN n involves data pre-processing

00:09:48,720 --> 00:09:52,839
data pre-processing then building the

00:09:50,980 --> 00:09:53,980
recurrent neural model and meeting

00:09:52,839 --> 00:09:57,730
making the predictions and

00:09:53,980 --> 00:10:00,069
visualizations so before implementing we

00:09:57,730 --> 00:10:03,399
need certain libraries that is scarus

00:10:00,069 --> 00:10:04,149
library then we'd use scikit-learn we

00:10:03,399 --> 00:10:10,569
use tensorflow

00:10:04,149 --> 00:10:13,559
and yeah so first us is data

00:10:10,569 --> 00:10:16,899
pre-processing so we import the number

00:10:13,559 --> 00:10:18,819
library we import the matlock library we

00:10:16,899 --> 00:10:21,059
import the pandas library the numpy

00:10:18,819 --> 00:10:25,180
library is for visualizing the results

00:10:21,059 --> 00:10:27,189
it's for area manipulation math lot is

00:10:25,180 --> 00:10:31,120
for visualizing the results and pandas

00:10:27,189 --> 00:10:33,879
is for managing the datasets because in

00:10:31,120 --> 00:10:37,230
the Kira's library yeah

00:10:33,879 --> 00:10:37,230
zoom okay

00:10:50,350 --> 00:10:57,290
any genome okay so since the Kairos

00:10:55,100 --> 00:10:59,839
library doesn't support the data frames

00:10:57,290 --> 00:11:04,699
of pandas we need to use the number

00:10:59,839 --> 00:11:07,730
errors for Kira's library now before

00:11:04,699 --> 00:11:10,459
starting let me introduce you to the

00:11:07,730 --> 00:11:15,350
problem so today we will be predicting

00:11:10,459 --> 00:11:19,220
the stock price for the Google for first

00:11:15,350 --> 00:11:26,120
month of 2017 so these are the records

00:11:19,220 --> 00:11:29,120
that you can see these these are the

00:11:26,120 --> 00:11:32,060
records that you can see so these are 20

00:11:29,120 --> 00:11:35,149
records that is each month has 20

00:11:32,060 --> 00:11:37,459
financial days that means weekdays so

00:11:35,149 --> 00:11:42,759
this is the record that we have to

00:11:37,459 --> 00:11:42,759
predict on and the training data set is

00:11:44,769 --> 00:11:50,750
this so we will be predicting the open

00:11:48,110 --> 00:11:54,380
stock prices of Google for first month

00:11:50,750 --> 00:11:58,149
of 2017 and we will be training on the

00:11:54,380 --> 00:12:00,949
year 2012 to 2016 so five years of data

00:11:58,149 --> 00:12:04,339
now actually the prediction should look

00:12:00,949 --> 00:12:15,350
like this so if I take these two columns

00:12:04,339 --> 00:12:18,680
and just insert the plot for it so the

00:12:15,350 --> 00:12:23,269
result that we should predict should be

00:12:18,680 --> 00:12:26,420
like this so today our task is to mimic

00:12:23,269 --> 00:12:32,949
this behavior of the stock price in

00:12:26,420 --> 00:12:37,160
January 2017 now let's get started so

00:12:32,949 --> 00:12:39,620
the training data set is I'm calling it

00:12:37,160 --> 00:12:41,750
Google stock price train dot CSV I'm

00:12:39,620 --> 00:12:45,250
importing it as a data set the panda's

00:12:41,750 --> 00:12:50,990
data frame so I'm using PD dot read CSV

00:12:45,250 --> 00:12:53,120
then since this data frame contains many

00:12:50,990 --> 00:12:56,810
columns what we only require the open

00:12:53,120 --> 00:13:02,980
stock prices the open stock prices that

00:12:56,810 --> 00:13:06,170
is column 1 so I use the I lock function

00:13:02,980 --> 00:13:08,300
for selecting for the first parameter in

00:13:06,170 --> 00:13:10,760
a lock is the colon that means it will

00:13:08,300 --> 00:13:12,530
be selecting all the rules and the

00:13:10,760 --> 00:13:15,730
column and the second parameter is the

00:13:12,530 --> 00:13:18,890
column parameter that is one to two one

00:13:15,730 --> 00:13:21,200
points to the open and two basically

00:13:18,890 --> 00:13:24,380
points to the next column but since the

00:13:21,200 --> 00:13:26,210
second value is omitted so it is just

00:13:24,380 --> 00:13:28,940
considering the first column that is the

00:13:26,210 --> 00:13:31,130
open column and dot values is for

00:13:28,940 --> 00:13:32,990
converting it to the number arrays so we

00:13:31,130 --> 00:13:40,220
are just converting the data panda's

00:13:32,990 --> 00:13:42,800
data frame to the number array now we

00:13:40,220 --> 00:13:44,500
need to scale this data and we will be

00:13:42,800 --> 00:13:47,540
using the scikit-learn pre-processing

00:13:44,500 --> 00:13:51,200
library and from that we will be using

00:13:47,540 --> 00:13:53,450
the min/max scalar class so SC is the

00:13:51,200 --> 00:13:56,930
object of mimic scalar class and the

00:13:53,450 --> 00:14:01,310
feature range parameter here tells that

00:13:56,930 --> 00:14:04,040
you need to convert the values of the

00:14:01,310 --> 00:14:06,430
column between the range 0 to 1 so

00:14:04,040 --> 00:14:11,480
everything will be scaled between 0 to 1

00:14:06,430 --> 00:14:14,380
and after that we just fit and transform

00:14:11,480 --> 00:14:17,690
so fit and transform means fitting

00:14:14,380 --> 00:14:19,850
basically means normalizing the data so

00:14:17,690 --> 00:14:21,470
it actually takes the minimum value from

00:14:19,850 --> 00:14:25,010
that column and the maximum value from

00:14:21,470 --> 00:14:26,960
that column and it puts it the transform

00:14:25,010 --> 00:14:30,290
puts it into the normalization function

00:14:26,960 --> 00:14:32,420
the normalization function can be the

00:14:30,290 --> 00:14:34,730
standardized function or the normal

00:14:32,420 --> 00:14:36,220
normalization function so in

00:14:34,730 --> 00:14:40,310
standardized function we take the mean

00:14:36,220 --> 00:14:43,520
value and we subtract it from the actual

00:14:40,310 --> 00:14:46,610
value and then we divide it by mean

00:14:43,520 --> 00:14:48,590
value but in normalization function we

00:14:46,610 --> 00:14:51,770
take the mean value we subtract it from

00:14:48,590 --> 00:14:53,390
the main value and we divide it by the

00:14:51,770 --> 00:14:57,230
difference between the max value and the

00:14:53,390 --> 00:14:59,180
min value so it is a basic normalization

00:14:57,230 --> 00:15:02,990
thing that we are doing so fit and

00:14:59,180 --> 00:15:06,530
transform does this thing after that we

00:15:02,990 --> 00:15:10,370
need to create the training data the

00:15:06,530 --> 00:15:14,360
training list and predicted ground truth

00:15:10,370 --> 00:15:15,800
list so extreme is a list an empty list

00:15:14,360 --> 00:15:19,940
and Whiterun is an empty list

00:15:15,800 --> 00:15:24,740
that is for the training you can say set

00:15:19,940 --> 00:15:27,950
and the ground truth set so we have one

00:15:24,740 --> 00:15:31,070
two five eight records here in the

00:15:27,950 --> 00:15:33,110
predict in the stock prediction data so

00:15:31,070 --> 00:15:39,829
we have one two five eight records these

00:15:33,110 --> 00:15:42,380
are one two five eight records and we

00:15:39,829 --> 00:15:45,170
are taking the time step as sixty so

00:15:42,380 --> 00:15:47,060
what is time step basically time step is

00:15:45,170 --> 00:15:48,740
a very important concept here in the

00:15:47,060 --> 00:15:51,140
recurrent neural networks it is

00:15:48,740 --> 00:15:53,630
basically how many observations you will

00:15:51,140 --> 00:15:56,450
be taking into consideration for

00:15:53,630 --> 00:16:01,730
training the next value or you can say

00:15:56,450 --> 00:16:05,750
if you are focusing on just predicting

00:16:01,730 --> 00:16:07,370
the is value then what set of values you

00:16:05,750 --> 00:16:10,010
will be taking into consideration before

00:16:07,370 --> 00:16:12,680
that I eight value so it is basically I

00:16:10,010 --> 00:16:14,390
am starting from the sixtieth record it

00:16:12,680 --> 00:16:16,160
means it will be taking the data of

00:16:14,390 --> 00:16:20,000
three months training on that and

00:16:16,160 --> 00:16:22,459
predicting the sixty-first value so it

00:16:20,000 --> 00:16:25,310
is just an assumption you can take

00:16:22,459 --> 00:16:29,450
twenty days 18 days any anything but it

00:16:25,310 --> 00:16:31,459
is like during my testing I took sixty

00:16:29,450 --> 00:16:34,339
and it it produced great results so I'm

00:16:31,459 --> 00:16:37,279
using sixty as the time step value and

00:16:34,339 --> 00:16:40,730
one two five eight is the number of

00:16:37,279 --> 00:16:48,110
records so I append to the empty list

00:16:40,730 --> 00:16:51,050
the training scaled values from the I

00:16:48,110 --> 00:16:54,140
minus 60th value to the current value so

00:16:51,050 --> 00:16:57,380
that is basically appending it is making

00:16:54,140 --> 00:16:59,570
an array in which one record contains

00:16:57,380 --> 00:17:03,170
sixty previous values from the current

00:16:59,570 --> 00:17:06,260
value so it is like a sixth you can say

00:17:03,170 --> 00:17:10,910
n cross sixty matrix that is getting

00:17:06,260 --> 00:17:12,620
created now the white rain is basically

00:17:10,910 --> 00:17:16,069
the ground truth and the ground truth

00:17:12,620 --> 00:17:19,459
will be just having the current value so

00:17:16,069 --> 00:17:21,199
it is from i to i plus 1 and zero is

00:17:19,459 --> 00:17:23,329
basically the column that is the first

00:17:21,199 --> 00:17:25,130
column because we have already cleaned

00:17:23,329 --> 00:17:27,439
our data and we are just having one

00:17:25,130 --> 00:17:29,539
column for open stock prices so zero

00:17:27,439 --> 00:17:31,999
wants that thing

00:17:29,539 --> 00:17:34,129
now extreme comma well-trained equals NP

00:17:31,999 --> 00:17:36,739
dot area so it is converting to dumpy

00:17:34,129 --> 00:17:39,129
array so we are generating extreme

00:17:36,739 --> 00:17:42,649
number array and Whiterun numpy array

00:17:39,129 --> 00:17:45,979
then we need to reshape the extreme

00:17:42,649 --> 00:17:49,700
array so reshaping means that to work

00:17:45,979 --> 00:17:53,749
seamlessly with the you can say number

00:17:49,700 --> 00:17:58,100
arrays and RN and stuff we need to add

00:17:53,749 --> 00:18:02,779
an extra dimension to the this existing

00:17:58,100 --> 00:18:05,389
number array so basically this is this

00:18:02,779 --> 00:18:08,359
basically X train dot shape is the

00:18:05,389 --> 00:18:11,059
number of rows extreme dot shape one is

00:18:08,359 --> 00:18:15,440
number of columns and one is basically

00:18:11,059 --> 00:18:19,519
the indicator column that is the open

00:18:15,440 --> 00:18:22,729
stock column so we are just reshaping to

00:18:19,519 --> 00:18:24,320
build it a 3d array from a 2d array now

00:18:22,729 --> 00:18:26,929
the first step of data pre-processing

00:18:24,320 --> 00:18:29,869
has been done and we are on to the next

00:18:26,929 --> 00:18:36,710
step of building the RNN does anyone

00:18:29,869 --> 00:18:40,759
have any doubt in this yeah yeah so

00:18:36,710 --> 00:18:44,539
basically we need to train the data we

00:18:40,759 --> 00:18:50,570
need to say we are not having we are

00:18:44,539 --> 00:18:54,200
having values say 700 800 1000 to say 20

00:18:50,570 --> 00:18:56,450
20 to 5 in the stock prices and it is

00:18:54,200 --> 00:18:58,669
not ideal it will take a lot of time and

00:18:56,450 --> 00:19:00,470
it will not generate good results if the

00:18:58,669 --> 00:19:04,129
data is not scaled between some value

00:19:00,470 --> 00:19:04,429
say 0 or 1 so I have provided range 0 to

00:19:04,129 --> 00:19:07,840
00:19:04,429 --> 00:19:10,399
to make it more cleaner so it will be

00:19:07,840 --> 00:19:12,889
approximated to the nearest value so it

00:19:10,399 --> 00:19:19,840
is just cleaning the data and smoothing

00:19:12,889 --> 00:19:19,840
the data so scaling has that use yeah

00:19:24,620 --> 00:19:40,500
yeah yeah yeah all right so basically

00:19:38,390 --> 00:19:42,870
the question that you are asking is a

00:19:40,500 --> 00:19:45,510
very good question and we use something

00:19:42,870 --> 00:19:47,310
called grid search for this that is

00:19:45,510 --> 00:19:51,000
basically a part of hyper parameter

00:19:47,310 --> 00:19:53,820
tuning so these are basically pointing

00:19:51,000 --> 00:19:55,950
towards the hyperparameters stuff so 60

00:19:53,820 --> 00:19:58,380
is like a hyper parameter only which is

00:19:55,950 --> 00:20:01,920
making or which is amending how the

00:19:58,380 --> 00:20:06,750
predictions are being made so what we

00:20:01,920 --> 00:20:09,450
can do is after this is done like this

00:20:06,750 --> 00:20:12,570
is the first cook for you you all people

00:20:09,450 --> 00:20:15,300
but I tried it with certain parameters

00:20:12,570 --> 00:20:17,190
and in the hyper parameter tuning the

00:20:15,300 --> 00:20:20,070
grid search library provides you with

00:20:17,190 --> 00:20:23,190
you can say you can provide arrays of

00:20:20,070 --> 00:20:25,860
values say I have something called say

00:20:23,190 --> 00:20:30,210
time step so I provide time step equal

00:20:25,860 --> 00:20:33,750
to 30 comma 60 comma 300 suppose and I

00:20:30,210 --> 00:20:36,750
have another value say optimizer and I I

00:20:33,750 --> 00:20:39,630
take three optimizers suppose which are

00:20:36,750 --> 00:20:40,980
used for this type of regression this is

00:20:39,630 --> 00:20:43,890
a regression problem because we are

00:20:40,980 --> 00:20:47,100
dealing with the continuous values so we

00:20:43,890 --> 00:20:49,440
take say rmsprop optimizer that I will

00:20:47,100 --> 00:20:52,290
be telling you more about and we take

00:20:49,440 --> 00:20:55,470
Adam optimizer and we say an Adam

00:20:52,290 --> 00:20:57,750
optimizer so it will be doing a

00:20:55,470 --> 00:21:01,680
cross-product so it will be take this

00:20:57,750 --> 00:21:05,670
parameter 30 time step cross it with the

00:21:01,680 --> 00:21:08,760
first rmsprop and generate result then

00:21:05,670 --> 00:21:10,470
30 Adam generate result 30 and Adam

00:21:08,760 --> 00:21:13,860
generate result then it will be doing

00:21:10,470 --> 00:21:16,140
same thing with 60 rmsprop 60 Adam 60

00:21:13,860 --> 00:21:18,810
and Adam so it will be generating nine

00:21:16,140 --> 00:21:22,620
results and from those nine results you

00:21:18,810 --> 00:21:24,840
can actually get the matrix formed which

00:21:22,620 --> 00:21:29,880
will be showing which one is generating

00:21:24,840 --> 00:21:32,100
good accuracy and less loss so so using

00:21:29,880 --> 00:21:34,980
that grid search thing you can actually

00:21:32,100 --> 00:21:38,280
judge which hyper parameter to tea

00:21:34,980 --> 00:21:43,560
- what will you so that is part of grid

00:21:38,280 --> 00:21:47,130
search Thanks no I have not because it

00:21:43,560 --> 00:21:52,620
is like I don't have that much time for

00:21:47,130 --> 00:21:55,200
showing yeah so now building the RN and

00:21:52,620 --> 00:21:57,990
stuff so we are importing four classes

00:21:55,200 --> 00:22:00,170
sequential thence lsdm and drop of

00:21:57,990 --> 00:22:03,720
sequential class is for basically

00:22:00,170 --> 00:22:06,180
inputting the first you can say set of

00:22:03,720 --> 00:22:09,360
inputs to the neural network dense

00:22:06,180 --> 00:22:11,580
classes for the last layer of the neural

00:22:09,360 --> 00:22:14,400
network that's the output layer lsdm

00:22:11,580 --> 00:22:16,590
classes for making the hidden layers of

00:22:14,400 --> 00:22:17,850
the neural network and dropout is for

00:22:16,590 --> 00:22:19,470
something called as dropout

00:22:17,850 --> 00:22:22,830
regularization which I will be telling

00:22:19,470 --> 00:22:24,960
you more about further so we are making

00:22:22,830 --> 00:22:26,850
a regresar since it's not a

00:22:24,960 --> 00:22:28,380
classification problem it's we are

00:22:26,850 --> 00:22:30,870
dealing with the continuous data sets so

00:22:28,380 --> 00:22:32,310
it's a regression problem so we make an

00:22:30,870 --> 00:22:34,680
object of sequential class called

00:22:32,310 --> 00:22:36,990
regressor and we add the first layer to

00:22:34,680 --> 00:22:39,030
it so it's regressor dot add so

00:22:36,990 --> 00:22:41,600
regressive dot add will add layers to

00:22:39,030 --> 00:22:44,220
the neural network so first there is the

00:22:41,600 --> 00:22:48,330
lsdm layer the hidden layer that we are

00:22:44,220 --> 00:22:51,450
adding and units units equal to 50 means

00:22:48,330 --> 00:22:52,980
the number of neurons you can say that

00:22:51,450 --> 00:22:56,940
we are introducing to the network so

00:22:52,980 --> 00:22:59,370
this layer will have 50 neurons and you

00:22:56,940 --> 00:23:01,640
can take it any value so it was again

00:22:59,370 --> 00:23:04,380
the hypo parameter stuff so I took 50

00:23:01,640 --> 00:23:06,780
then returned sequence is equal the true

00:23:04,380 --> 00:23:08,460
means that the output of this layer will

00:23:06,780 --> 00:23:11,850
be getting forwarded to the next layer

00:23:08,460 --> 00:23:14,550
and input shape equals extra not shape

00:23:11,850 --> 00:23:16,770
one that points to the columns of the

00:23:14,550 --> 00:23:18,750
data set and one is for reshaping it to

00:23:16,770 --> 00:23:21,630
the third dimension so we are just using

00:23:18,750 --> 00:23:23,850
the the column and the third dimensional

00:23:21,630 --> 00:23:27,570
value for it we don't need to use the

00:23:23,850 --> 00:23:29,010
first row dimension value for it so this

00:23:27,570 --> 00:23:32,430
will be adding the first layer and

00:23:29,010 --> 00:23:35,280
adding the first layer may introduce

00:23:32,430 --> 00:23:38,640
overfitting see so to deal with the

00:23:35,280 --> 00:23:40,560
overfitting problem that is to say avoid

00:23:38,640 --> 00:23:42,600
the noise that is being created by the

00:23:40,560 --> 00:23:44,520
overfitting stuff we use the drop out

00:23:42,600 --> 00:23:47,400
regularization step so drop out actually

00:23:44,520 --> 00:23:48,260
means dropping out certain number of

00:23:47,400 --> 00:23:52,100
neurons

00:23:48,260 --> 00:23:55,970
from the lair so out of 50 I have chosen

00:23:52,100 --> 00:23:57,920
0.2 that's 20% to be dropped out so it

00:23:55,970 --> 00:24:00,140
means it will be just considering 40

00:23:57,920 --> 00:24:03,680
neurons for the next layer and out of

00:24:00,140 --> 00:24:06,320
those 50 neurons 10 neurons at random or

00:24:03,680 --> 00:24:09,050
say 20% of those 50 neurons will be

00:24:06,320 --> 00:24:14,470
dropped out at random so this actually

00:24:09,050 --> 00:24:14,470
avoids the overfitting problem yeah

00:24:20,139 --> 00:24:25,130
you know so basically it's a stag Delos

00:24:22,700 --> 00:24:28,580
TM so I'm using four hidden layers in

00:24:25,130 --> 00:24:42,820
this and for the four layers I'm adding

00:24:28,580 --> 00:24:47,000
four drop outs so yeah hmm yeah mm-hmm

00:24:42,820 --> 00:24:49,639
no no it's like during the a box a box

00:24:47,000 --> 00:24:51,769
is basically how many times the data the

00:24:49,639 --> 00:24:56,000
full data will be propagated forward and

00:24:51,769 --> 00:24:58,809
backward to lessen the loss the loss

00:24:56,000 --> 00:25:02,059
generated by you can say the gradient so

00:24:58,809 --> 00:25:04,549
it's actually for the number of a box

00:25:02,059 --> 00:25:07,269
the drop out will be done at random so

00:25:04,549 --> 00:25:11,720
dropping out means just not considering

00:25:07,269 --> 00:25:14,210
20% of those neurons randomly in one a

00:25:11,720 --> 00:25:17,389
POC and in the next a POC it will again

00:25:14,210 --> 00:25:19,309
be choosing random 20% and then B not

00:25:17,389 --> 00:25:21,889
considering so it's like a random stuff

00:25:19,309 --> 00:25:25,129
going on I've chosen hundred a box so

00:25:21,889 --> 00:25:27,620
it's fully propagating the data a

00:25:25,129 --> 00:25:30,820
forward and backward and it training

00:25:27,620 --> 00:25:33,379
it's done hundred times on this data so

00:25:30,820 --> 00:25:36,620
it's like for each a pocket is

00:25:33,379 --> 00:25:38,269
generating some random value some it's

00:25:36,620 --> 00:25:40,850
choosing some random neuron and it's

00:25:38,269 --> 00:25:42,379
dropping out ten random neurons so it

00:25:40,850 --> 00:25:48,769
actually avoids the overfitting problem

00:25:42,379 --> 00:25:51,980
in that yeah and then similarly I'm

00:25:48,769 --> 00:25:55,730
adding three more layers so this is

00:25:51,980 --> 00:25:58,970
regressor dot add LST m now you have

00:25:55,730 --> 00:26:00,259
added the first layer initially so you

00:25:58,970 --> 00:26:02,480
don't need to provide the input shape

00:26:00,259 --> 00:26:04,970
because LST m-class already knows that

00:26:02,480 --> 00:26:06,340
what is the input so from the second

00:26:04,970 --> 00:26:08,360
layer you don't need to provide this

00:26:06,340 --> 00:26:10,970
parameter input shape it will

00:26:08,360 --> 00:26:14,690
automatically take from the previous

00:26:10,970 --> 00:26:17,210
thing and this is the second layer then

00:26:14,690 --> 00:26:19,220
this is the third layer and the fourth

00:26:17,210 --> 00:26:22,580
layer and for clear we don't actually

00:26:19,220 --> 00:26:24,470
need the output return sequence because

00:26:22,580 --> 00:26:27,260
we need to pass it through the dense

00:26:24,470 --> 00:26:28,970
layer and not another lsdm layer so we

00:26:27,260 --> 00:26:32,460
will be omitting the return sequence

00:26:28,970 --> 00:26:34,710
equal to true parameter we can

00:26:32,460 --> 00:26:36,750
return sequence equal to false but the

00:26:34,710 --> 00:26:39,170
default value of return sequences false

00:26:36,750 --> 00:26:45,350
so we are not putting here the parameter

00:26:39,170 --> 00:26:48,990
and dense is the final output layer so

00:26:45,350 --> 00:26:50,820
basically it is the units equal to 1

00:26:48,990 --> 00:26:54,210
means it will be having just one neuron

00:26:50,820 --> 00:26:56,520
that's the resultant neuron so dense is

00:26:54,210 --> 00:27:00,660
actually corresponding to the result of

00:26:56,520 --> 00:27:02,730
the neural network now the building of

00:27:00,660 --> 00:27:05,760
RNN has been done but the compilation

00:27:02,730 --> 00:27:10,440
phase is still left compilation actually

00:27:05,760 --> 00:27:12,540
means compiling with a certain loss

00:27:10,440 --> 00:27:14,430
function and the OP and choosing the

00:27:12,540 --> 00:27:18,330
right optimizer so as I told you earlier

00:27:14,430 --> 00:27:21,960
rmsprop and Adam are the two optimizers

00:27:18,330 --> 00:27:25,770
that are used with RNN and adam is the

00:27:21,960 --> 00:27:28,590
one that usually works apart from say

00:27:25,770 --> 00:27:30,750
arenal so it works with CNN also so it

00:27:28,590 --> 00:27:33,500
is a much wider optimizer that you can

00:27:30,750 --> 00:27:36,360
use and it gives good results again

00:27:33,500 --> 00:27:38,760
hyperparameters tuning thing so I have

00:27:36,360 --> 00:27:41,400
chosen the optimizer as Adam and since

00:27:38,760 --> 00:27:43,520
it is the regression stuff so we will be

00:27:41,400 --> 00:27:47,430
using mean squared error loss function

00:27:43,520 --> 00:27:48,990
if it would have been say in the

00:27:47,430 --> 00:27:51,300
classification stuff then we would have

00:27:48,990 --> 00:27:55,020
used the cross entropy or binary cross

00:27:51,300 --> 00:27:58,350
entropy loss function then we have

00:27:55,020 --> 00:28:02,010
compiled this and then we are ready to

00:27:58,350 --> 00:28:05,160
fit it so fitting the RNN fall training

00:28:02,010 --> 00:28:08,310
set so we pass the fit function x train

00:28:05,160 --> 00:28:10,140
y train at box equal 200 bath sizes 32

00:28:08,310 --> 00:28:13,170
so it will be taking batch of 32 it will

00:28:10,140 --> 00:28:14,940
be dividing the data set into 32 say

00:28:13,170 --> 00:28:17,310
element batches and which will be

00:28:14,940 --> 00:28:20,370
working on those battles it's again

00:28:17,310 --> 00:28:22,140
hyper parameter tuning stuff so what I

00:28:20,370 --> 00:28:26,250
mean by hyper parameter tuning is that

00:28:22,140 --> 00:28:29,610
tuning the parameters of the classes in

00:28:26,250 --> 00:28:33,540
such a way that it provides good result

00:28:29,610 --> 00:28:35,760
or good predictions now the task 3 is

00:28:33,540 --> 00:28:38,070
the prediction task so we have done the

00:28:35,760 --> 00:28:40,440
RN n building so we have done the data

00:28:38,070 --> 00:28:43,890
pre-processing stuff now is the task to

00:28:40,440 --> 00:28:45,780
predict the results so actually the data

00:28:43,890 --> 00:28:48,060
set for

00:28:45,780 --> 00:28:54,540
was separate than the data set for

00:28:48,060 --> 00:28:57,930
testing so we just take the test data

00:28:54,540 --> 00:28:59,760
set and we merge that test data set with

00:28:57,930 --> 00:29:02,460
the actual prediction data set to make

00:28:59,760 --> 00:29:05,610
it a totem say a total data and data set

00:29:02,460 --> 00:29:09,480
which will contain every value so here

00:29:05,610 --> 00:29:11,580
we are just converting the read CSV we

00:29:09,480 --> 00:29:13,560
are creating a data frame of the test

00:29:11,580 --> 00:29:15,360
data set and then we are again

00:29:13,560 --> 00:29:18,480
converting it to the numpy array as we

00:29:15,360 --> 00:29:22,880
ended it with the price the prediction

00:29:18,480 --> 00:29:25,860
price stuff then we concatenate the open

00:29:22,880 --> 00:29:28,290
column of the prediction data set with

00:29:25,860 --> 00:29:31,050
the test column so it will be one two

00:29:28,290 --> 00:29:33,660
five eight plus twenty recalls that's

00:29:31,050 --> 00:29:36,810
one two seven eight records in total

00:29:33,660 --> 00:29:39,690
because during the time of testing we

00:29:36,810 --> 00:29:43,590
need to have we need to consider the

00:29:39,690 --> 00:29:45,420
previous sixty records and for

00:29:43,590 --> 00:29:46,560
considering those previous exterior

00:29:45,420 --> 00:29:49,710
calls we should have a data set

00:29:46,560 --> 00:29:52,890
incomplete so I'm just using this

00:29:49,710 --> 00:29:57,270
complete data set and xs0 is for

00:29:52,890 --> 00:30:01,470
vertical join input equals data set must

00:29:57,270 --> 00:30:05,850
so basically it is taking it is creating

00:30:01,470 --> 00:30:08,760
another you can say number array which

00:30:05,850 --> 00:30:13,080
is based on the most data set and it

00:30:08,760 --> 00:30:16,470
will be taking the values 60 values from

00:30:13,080 --> 00:30:18,210
the current state so counting from one

00:30:16,470 --> 00:30:20,340
to five record number one two five eight

00:30:18,210 --> 00:30:22,470
it will be taking into account the

00:30:20,340 --> 00:30:25,160
previous sixty values to predict the

00:30:22,470 --> 00:30:28,380
results of the record one two five eight

00:30:25,160 --> 00:30:31,350
now we are reshaping it again

00:30:28,380 --> 00:30:34,620
and since we have already fitted the

00:30:31,350 --> 00:30:39,270
data earlier we'll just be transforming

00:30:34,620 --> 00:30:41,160
it so we are just taking the input numpy

00:30:39,270 --> 00:30:44,100
array and we are transforming the input

00:30:41,160 --> 00:30:48,390
number array for making the predictions

00:30:44,100 --> 00:30:51,980
and we use another list called X test

00:30:48,390 --> 00:30:55,560
and we append to this list the values

00:30:51,980 --> 00:30:59,440
from the last 20 rows that we need to

00:30:55,560 --> 00:31:02,049
test and then we form the number air

00:30:59,440 --> 00:31:04,899
of this and we just shape it again

00:31:02,049 --> 00:31:07,480
reshape it again now the prediction can

00:31:04,899 --> 00:31:12,940
be done using the predict function of

00:31:07,480 --> 00:31:15,279
the regressor class the the sequence the

00:31:12,940 --> 00:31:19,090
class that we formed so regress it or

00:31:15,279 --> 00:31:22,000
predict basically makes prediction on

00:31:19,090 --> 00:31:24,850
the X test list that we just formed the

00:31:22,000 --> 00:31:28,110
number array and inverse transform is

00:31:24,850 --> 00:31:30,820
the function that is used to inverse the

00:31:28,110 --> 00:31:34,570
normalization stuff that we did so now

00:31:30,820 --> 00:31:38,200
we are just you can say we are inverting

00:31:34,570 --> 00:31:40,090
the scaling that we did we made the

00:31:38,200 --> 00:31:41,679
scaling from 0 to 1 we normalize the

00:31:40,090 --> 00:31:44,710
results from 0 to 1 we are just

00:31:41,679 --> 00:31:49,210
inversing the transform to actually

00:31:44,710 --> 00:31:50,860
convert them into the real value so it

00:31:49,210 --> 00:31:53,230
will be something like zero point

00:31:50,860 --> 00:31:55,600
something is converted into 770 that is

00:31:53,230 --> 00:32:01,870
say a stock price for one record and

00:31:55,600 --> 00:32:04,149
then we plot these results so since the

00:32:01,870 --> 00:32:08,980
time is short I have already run this

00:32:04,149 --> 00:32:13,960
thing and pycharm so it was say 100 a

00:32:08,980 --> 00:32:16,830
box and you can see that I'll just show

00:32:13,960 --> 00:32:16,830
you say

00:32:22,130 --> 00:32:27,020
you see that with increasing number of

00:32:24,470 --> 00:32:29,510
epochs the loss function is decreasing

00:32:27,020 --> 00:32:31,880
so the value of loss is decreasing so

00:32:29,510 --> 00:32:35,810
the loss must have started with say some

00:32:31,880 --> 00:32:38,690
value 1 3 or something point 0 0 3 4 and

00:32:35,810 --> 00:32:44,930
it's now at the end of 100 that box it's

00:32:38,690 --> 00:32:48,640
1 0 0 1 3 1 5 so you can see as I'm

00:32:44,930 --> 00:32:50,900
going up the loss function is increasing

00:32:48,640 --> 00:32:52,700
so the value of loss function is

00:32:50,900 --> 00:32:54,680
increasing it means that the training

00:32:52,700 --> 00:32:57,050
has been done correctly and we have

00:32:54,680 --> 00:32:59,420
achieved certain level of accuracy so

00:32:57,050 --> 00:33:03,680
that the loss function has minimized its

00:32:59,420 --> 00:33:05,540
value from say it's in 30s or 40s for

00:33:03,680 --> 00:33:12,710
let's say Oh point zero zero three zero

00:33:05,540 --> 00:33:15,550
two point 1 3 and we can see that the

00:33:12,710 --> 00:33:15,550
plot that we have

00:33:19,160 --> 00:33:25,140
it's following the trend of ups and

00:33:22,770 --> 00:33:27,230
downs of the market but it's not

00:33:25,140 --> 00:33:29,790
actually predicting the correct values

00:33:27,230 --> 00:33:32,670
though it is following the trend that's

00:33:29,790 --> 00:33:34,170
like if the values is value of the stock

00:33:32,670 --> 00:33:35,820
prices going up it is following that

00:33:34,170 --> 00:33:37,890
trend so blue is the predicted one and

00:33:35,820 --> 00:33:43,020
red is the actual one

00:33:37,890 --> 00:33:46,080
so using these parameters I was able to

00:33:43,020 --> 00:33:48,590
generate this result doing hyper

00:33:46,080 --> 00:33:53,240
parameter tuning with grid search will

00:33:48,590 --> 00:33:57,590
definitely improvise these results and

00:33:53,240 --> 00:34:00,930
it will actually match with some error

00:33:57,590 --> 00:34:03,630
with the actual redline so it is like

00:34:00,930 --> 00:34:06,450
just following the trend with the red

00:34:03,630 --> 00:34:08,340
thing so we can see that it is actually

00:34:06,450 --> 00:34:11,940
falling following the trend so we can

00:34:08,340 --> 00:34:15,600
see on x-axis at point two point five it

00:34:11,940 --> 00:34:17,520
is just going down then it's following

00:34:15,600 --> 00:34:20,670
the upward trend and then it's being

00:34:17,520 --> 00:34:22,830
stable at the end so it was this

00:34:20,670 --> 00:34:26,570
prediction that using these hyper

00:34:22,830 --> 00:34:26,570
parameters I was able to make

00:34:35,550 --> 00:34:38,149
yes

00:34:48,250 --> 00:35:01,660
so the source code for this I'll be

00:34:51,119 --> 00:35:10,630
updating my slides but it's on it's on

00:35:01,660 --> 00:35:12,400
github it's at this address I'll be just

00:35:10,630 --> 00:35:18,880
uploading my slides and it will get

00:35:12,400 --> 00:35:23,760
uploaded on the session page and after

00:35:18,880 --> 00:35:27,930
that so these are my acknowledgments

00:35:23,760 --> 00:35:31,599
professor martin christian then my

00:35:27,930 --> 00:35:36,250
supervisors Christopher EULA for

00:35:31,599 --> 00:35:42,060
creating good blocks and yeah if you

00:35:36,250 --> 00:35:42,060
have any questions then you can ask yeah

00:35:46,380 --> 00:35:54,280
thank you very much we have time for a

00:35:48,910 --> 00:35:56,369
couple of questions thank you I wanted

00:35:54,280 --> 00:36:01,630
to ask how did you tune your

00:35:56,369 --> 00:36:03,430
architecture why four layers okay when

00:36:01,630 --> 00:36:06,550
we deal with our n ins we cannot go

00:36:03,430 --> 00:36:08,890
beyond say we generally choose to three

00:36:06,550 --> 00:36:11,800
or four layers because it actually

00:36:08,890 --> 00:36:14,800
doesn't make any sense going beyond four

00:36:11,800 --> 00:36:18,700
layers as the results will not improve

00:36:14,800 --> 00:36:20,950
over choosing folio so for just showing

00:36:18,700 --> 00:36:23,410
the implementation stuff I used four

00:36:20,950 --> 00:36:27,069
layers and it was just making some good

00:36:23,410 --> 00:36:29,140
assumptions when compared with using

00:36:27,069 --> 00:36:31,329
three layers or two layers but generally

00:36:29,140 --> 00:36:34,540
in our n ends we just don't go beyond

00:36:31,329 --> 00:36:37,119
four layers it can be the case that if

00:36:34,540 --> 00:36:40,480
we are using our n ends with some other

00:36:37,119 --> 00:36:42,880
neural network say CNN for say image

00:36:40,480 --> 00:36:45,670
captioning tasks then it will be another

00:36:42,880 --> 00:36:48,069
scenario we'll be getting the results of

00:36:45,670 --> 00:36:50,530
the vector generated from the CNN and we

00:36:48,069 --> 00:36:53,500
will be transferring those CNN results

00:36:50,530 --> 00:36:57,940
into the first hidden layer that's the

00:36:53,500 --> 00:37:01,040
h0 thing and those results will be just

00:36:57,940 --> 00:37:03,080
making further you can say vectors

00:37:01,040 --> 00:37:05,030
forming vectors which will then be

00:37:03,080 --> 00:37:07,430
propagated throughout the chain so it

00:37:05,030 --> 00:37:09,380
will be having more impact on the RNN

00:37:07,430 --> 00:37:12,110
training and in that case we can just

00:37:09,380 --> 00:37:14,570
lower down from four layers to two

00:37:12,110 --> 00:37:16,910
layers because it is already having lots

00:37:14,570 --> 00:37:19,850
of information from the CNN stuff so it

00:37:16,910 --> 00:37:22,910
is basically based on that thing so if

00:37:19,850 --> 00:37:25,940
we consider a basic example that's the

00:37:22,910 --> 00:37:28,760
perfect roommate example that is very

00:37:25,940 --> 00:37:31,700
famous in field of RNN if you have a

00:37:28,760 --> 00:37:35,270
roommate who cooks food for you and he

00:37:31,700 --> 00:37:40,190
cooks say apple pie chicken and say

00:37:35,270 --> 00:37:44,630
anything x was it anything so on sunny

00:37:40,190 --> 00:37:47,830
day he cooks say apple pie and if it's

00:37:44,630 --> 00:37:50,300
another sunny day then he just goes off

00:37:47,830 --> 00:37:52,160
the duty and doesn't cook anything if

00:37:50,300 --> 00:37:56,480
it's rainy day he's at home and he cooks

00:37:52,160 --> 00:37:59,810
next dish so it cooks chicken for you so

00:37:56,480 --> 00:38:02,510
it is like the first input is whether

00:37:59,810 --> 00:38:04,370
that's sunny or any the second input is

00:38:02,510 --> 00:38:06,920
the dish that he made previously

00:38:04,370 --> 00:38:09,350
so these input provided to a state will

00:38:06,920 --> 00:38:12,860
predict the next output this like a

00:38:09,350 --> 00:38:15,530
vector product plus and the addition of

00:38:12,860 --> 00:38:19,690
some of the other things that goes in

00:38:15,530 --> 00:38:23,620
complex neural networks you can just

00:38:19,690 --> 00:38:23,620
make sense out of it yeah

00:38:24,740 --> 00:38:41,270
yeah I'll be making it public yeah okay

00:38:29,300 --> 00:38:42,920
thank you very much again no basically I

00:38:41,270 --> 00:38:53,180
applied the inverse transform function

00:38:42,920 --> 00:38:55,910
so yeah yeah yeah it will be inside this

00:38:53,180 --> 00:38:58,250
range but the results will get inverted

00:38:55,910 --> 00:39:00,290
so the current value so it will be

00:38:58,250 --> 00:39:02,690
inverse transform function so that will

00:39:00,290 --> 00:39:05,230
be adding something divided by something

00:39:02,690 --> 00:39:07,400
that is lesser so it will be just

00:39:05,230 --> 00:39:12,770
exploding the value from zero one two

00:39:07,400 --> 00:39:14,599
scale of say hundreds they yeah it's

00:39:12,770 --> 00:39:18,740
it's yeah that is why the prediction is

00:39:14,599 --> 00:39:20,270
not predicting the actual result it is

00:39:18,740 --> 00:39:22,280
having some variation with the results

00:39:20,270 --> 00:39:24,230
because you are using some smoothen

00:39:22,280 --> 00:39:27,130
value and using that smoothen value you

00:39:24,230 --> 00:39:31,330
are going to predict actual results so

00:39:27,130 --> 00:39:31,330

YouTube URL: https://www.youtube.com/watch?v=bSdBG7hToOg


