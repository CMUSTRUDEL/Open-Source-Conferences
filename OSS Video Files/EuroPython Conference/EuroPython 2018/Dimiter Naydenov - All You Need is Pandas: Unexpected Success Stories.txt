Title: Dimiter Naydenov - All You Need is Pandas: Unexpected Success Stories
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	All You Need is Pandas: Unexpected Success Stories
[EuroPython 2018 - Talk - 2018-07-25 - Fintry [PyData]]
[Edinburgh, UK]

By Dimiter Naydenov

Learning to use the awesome Pandas toolkit helped me immensely in lots of ways. Finding novel,
efficient solutions to complex day-to-day problems with Pandas not only saves time, but can be fun
and rewarding experience.

In this talk I'll present use cases I had to solve, but the "traditional" approach proved tough
and/or otherwise frustrating implement nicely. Since I was just starting to learn Pandas, decided to
try an alternative solution with it. What I learned changed the way I think about data processing
with Python, and it only got better since!

The use cases deals with extracting pen strokes from handwritten SVG samples, and recomposing them
into reusable letters and numbers. When you need to compare each stroke to all others, often more
than once, resulted in inefficient, slow, and hard to maintain code. Even a naive Pandas approach
with loops helped to reduce both the memory footprint, and improve the performance considerably!
Improving the implementation further, vectorizing inner loops, and taking advantage of multi-index
operations, I managed to get the same results, using less memory and a lot faster (by orders of
magnitude).



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:02,840 --> 00:00:06,350
thank you hello everyone thanks for

00:00:05,700 --> 00:00:09,450
coming

00:00:06,350 --> 00:00:14,920
my name is mitternight enough and this

00:00:09,450 --> 00:00:17,600
is my first ever Europe item talk so I'm

00:00:14,920 --> 00:00:21,570
[Applause]

00:00:17,600 --> 00:00:24,630
I'm quite passionate about pandas and I

00:00:21,570 --> 00:00:28,380
hope by the end of my talk you might

00:00:24,630 --> 00:00:32,630
want to try it as well so let me first

00:00:28,380 --> 00:00:35,850
tell you a few things about myself so I

00:00:32,630 --> 00:00:38,910
have been a software developer for over

00:00:35,850 --> 00:00:44,340
20 years now I started back in the day

00:00:38,910 --> 00:00:47,520
with basic and Pascal went to C C++ C

00:00:44,340 --> 00:00:49,739
sharp given pitch before ears and then I

00:00:47,520 --> 00:00:52,590
discovered Python through Django and

00:00:49,739 --> 00:00:53,100
Python became my favorite language by

00:00:52,590 --> 00:00:55,680
far

00:00:53,100 --> 00:00:58,230
since then I've used it for pretty much

00:00:55,680 --> 00:01:02,640
everything server-side software

00:00:58,230 --> 00:01:09,060
scripting web apps mobile apps and all

00:01:02,640 --> 00:01:12,869
sorts of other things so I was working

00:01:09,060 --> 00:01:16,850
for canonical for four years and I was

00:01:12,869 --> 00:01:21,479
working on a port from Python to go of a

00:01:16,850 --> 00:01:24,960
cloud deployment suite and after that I

00:01:21,479 --> 00:01:27,930
decided it's time to get on my own

00:01:24,960 --> 00:01:32,189
so I went full time into freelancing

00:01:27,930 --> 00:01:39,150
with Python again happily and founded my

00:01:32,189 --> 00:01:45,030
small own company developed it so what

00:01:39,150 --> 00:01:48,770
about pandas so seriously

00:01:45,030 --> 00:01:53,130
how many of you have used pandas before

00:01:48,770 --> 00:01:55,229
alright great so have you used it for

00:01:53,130 --> 00:02:02,189
anything else than scientific and

00:01:55,229 --> 00:02:04,610
statistical software okay so just a

00:02:02,189 --> 00:02:07,619
quick introduction for those of you who

00:02:04,610 --> 00:02:11,489
don't know about it so pandas is an open

00:02:07,619 --> 00:02:13,850
source Python library it was created in

00:02:11,489 --> 00:02:17,370
2008 by Wes McKinney

00:02:13,850 --> 00:02:21,750
it has high performance and easy-to-use

00:02:17,370 --> 00:02:23,700
date structures and a great API for data

00:02:21,750 --> 00:02:27,300
analysis built on the solid foundation

00:02:23,700 --> 00:02:33,890
of numpy and it's also very well

00:02:27,300 --> 00:02:38,070
documented well you know in a way so I

00:02:33,890 --> 00:02:42,450
first heard about pandas in Europe I

00:02:38,070 --> 00:02:44,130
turn 2012 I think and since then I kept

00:02:42,450 --> 00:02:45,870
hearing about it from all sorts of

00:02:44,130 --> 00:02:48,560
people all the time

00:02:45,870 --> 00:02:51,930
and I decided to look into it and see

00:02:48,560 --> 00:02:57,410
actually what is all about I'm not from

00:02:51,930 --> 00:02:59,900
a scientific or financial background so

00:02:57,410 --> 00:03:03,540
that was my first experience with it

00:02:59,900 --> 00:03:05,820
basically I liked about it with that

00:03:03,540 --> 00:03:08,580
it's easy to install it has very few

00:03:05,820 --> 00:03:12,680
requirements especially on Linux it's

00:03:08,580 --> 00:03:15,960
trivial but also on Windows and Mac OS

00:03:12,680 --> 00:03:19,470
it's as fast as numpy yet a lot more

00:03:15,960 --> 00:03:21,960
flexible and I personally don't really

00:03:19,470 --> 00:03:25,890
like numpy that much because I found it

00:03:21,960 --> 00:03:30,000
somewhat counterintuitive and awkward to

00:03:25,890 --> 00:03:32,790
use and this also reads and write

00:03:30,000 --> 00:03:36,780
formats in pretty much any format you

00:03:32,790 --> 00:03:40,830
might have to deal with especially CSV

00:03:36,780 --> 00:03:44,160
Excel and hdf5 to name just a few which

00:03:40,830 --> 00:03:48,150
was an obvious advantage for me and also

00:03:44,160 --> 00:03:51,890
since I am quite a visual thinker I like

00:03:48,150 --> 00:03:58,320
how easy it is to plot stuff with pandas

00:03:51,890 --> 00:04:01,890
with map lovely so I did try it but I

00:03:58,320 --> 00:04:06,420
found some works and pain points which

00:04:01,890 --> 00:04:10,140
kind of put me off and I want to share a

00:04:06,420 --> 00:04:12,180
few of them with you so it has a good

00:04:10,140 --> 00:04:14,670
documentation but at the time there were

00:04:12,180 --> 00:04:18,209
not a lot of tutorials and handled

00:04:14,670 --> 00:04:20,010
hands-on guides you know it was a bit

00:04:18,209 --> 00:04:22,169
intimidating to read all that

00:04:20,010 --> 00:04:23,360
documentation and know where to start

00:04:22,169 --> 00:04:27,390
from

00:04:23,360 --> 00:04:27,630
there are also confusingly many ways to

00:04:27,390 --> 00:04:30,270
do

00:04:27,630 --> 00:04:34,790
the same thing kind of unpaid tonic in a

00:04:30,270 --> 00:04:38,670
way at least then also there are lots

00:04:34,790 --> 00:04:41,460
lots of indexing like every sort of

00:04:38,670 --> 00:04:43,500
indexing operation which was it's also

00:04:41,460 --> 00:04:46,650
its power but I didn't understood it

00:04:43,500 --> 00:04:50,640
then and it was kind of seemed to me

00:04:46,650 --> 00:04:53,250
pointless especially the multi index and

00:04:50,640 --> 00:04:55,860
it has same defaults for most things it

00:04:53,250 --> 00:05:00,600
can handle lots of types of data

00:04:55,860 --> 00:05:04,020
intelligently however not as fast as you

00:05:00,600 --> 00:05:06,390
might like so you might want to actually

00:05:04,020 --> 00:05:09,390
be specific when you want to deal with

00:05:06,390 --> 00:05:13,590
specific types of data like that day

00:05:09,390 --> 00:05:18,060
time or floats or integers and do some

00:05:13,590 --> 00:05:21,750
conversions in between so let me tell

00:05:18,060 --> 00:05:26,910
you about the project of mine which I

00:05:21,750 --> 00:05:30,330
kind of found unexpectedly how good fit

00:05:26,910 --> 00:05:33,990
pandas for some of the tasks I had to

00:05:30,330 --> 00:05:38,070
deal with so the project is an SVG mail

00:05:33,990 --> 00:05:41,850
labels generator which means send

00:05:38,070 --> 00:05:45,540
personalized mail in senders labeled on

00:05:41,850 --> 00:05:49,350
the envelope in sender's handwriting and

00:05:45,540 --> 00:05:52,800
this is done by following a few

00:05:49,350 --> 00:05:55,850
requirements so one of one of them is

00:05:52,800 --> 00:06:01,050
acquire sample of the user's handwriting

00:05:55,850 --> 00:06:04,950
on a tablet and it's acquired in a

00:06:01,050 --> 00:06:08,190
vectorized SVG format then extract

00:06:04,950 --> 00:06:10,980
individual letter or symbol SVG files

00:06:08,190 --> 00:06:15,510
small ones from each of those sample

00:06:10,980 --> 00:06:20,910
pages per user then out of those compose

00:06:15,510 --> 00:06:24,840
arbitrary word SVG files and make them

00:06:20,910 --> 00:06:28,980
look as if they're written by hand and

00:06:24,840 --> 00:06:31,560
finally generate mail labels from those

00:06:28,980 --> 00:06:35,930
words sticking them together into

00:06:31,560 --> 00:06:40,500
multi-line multi word labels

00:06:35,930 --> 00:06:41,289
so first the acquisition of handwriting

00:06:40,500 --> 00:06:46,050
sample

00:06:41,289 --> 00:06:49,389
is done on a tablet stills with more pen

00:06:46,050 --> 00:06:54,099
at the user if swarm or more of those

00:06:49,389 --> 00:06:58,569
samples they're saved as SVG files and

00:06:54,099 --> 00:07:01,749
this is an example of one of those so

00:06:58,569 --> 00:07:05,279
basically it's standardized text that

00:07:01,749 --> 00:07:08,529
you every user decides what to write and

00:07:05,279 --> 00:07:13,149
it writes them it writes that sample on

00:07:08,529 --> 00:07:15,759
several different pages so to have this

00:07:13,149 --> 00:07:20,080
for comparison basically and each of

00:07:15,759 --> 00:07:22,330
those things are basically SVG the pen

00:07:20,080 --> 00:07:28,749
strokes are recorded individually in the

00:07:22,330 --> 00:07:31,930
SVG file as vectorized curves and this

00:07:28,749 --> 00:07:35,740
is for example how it looks like one of

00:07:31,930 --> 00:07:39,610
the outputs of that process which is

00:07:35,740 --> 00:07:46,120
mailing label then force one of the

00:07:39,610 --> 00:07:49,569
users so so zooming is kind of weird

00:07:46,120 --> 00:07:53,259
so this is the generalized process

00:07:49,569 --> 00:07:55,839
it's a multi-stage pipeline of sorts so

00:07:53,259 --> 00:08:02,080
it first starts with the parsing of the

00:07:55,839 --> 00:08:06,969
SVG sample page then enter pandas pandas

00:08:02,080 --> 00:08:09,969
is used to read those in present them in

00:08:06,969 --> 00:08:13,930
a tabular fashion in a data frame so

00:08:09,969 --> 00:08:16,059
they can be easily handled then there is

00:08:13,930 --> 00:08:18,999
a letter extraction process which ever

00:08:16,059 --> 00:08:22,149
uses pandas to extract individual

00:08:18,999 --> 00:08:26,259
strokes and combine them as they were on

00:08:22,149 --> 00:08:28,719
the page so that you can come from

00:08:26,259 --> 00:08:33,219
single individual strokes to actual

00:08:28,719 --> 00:08:35,969
letters and then reuse those then there

00:08:33,219 --> 00:08:39,789
is a classification step which is done

00:08:35,969 --> 00:08:43,839
manually and basically labels each of

00:08:39,789 --> 00:08:48,459
those extracted letters as ABC dollar

00:08:43,839 --> 00:08:50,560
sign and so on after we have this there

00:08:48,459 --> 00:08:53,769
is the word building stage where we

00:08:50,560 --> 00:08:56,649
select letter variants or

00:08:53,769 --> 00:08:58,540
specific words stick them together apply

00:08:56,649 --> 00:09:02,379
some alignment and so on

00:08:58,540 --> 00:09:06,249
and finally there is the labeling stage

00:09:02,379 --> 00:09:08,649
which is producing labels out of those

00:09:06,249 --> 00:09:09,660
words and aligning them ready for

00:09:08,649 --> 00:09:15,220
printing

00:09:09,660 --> 00:09:17,399
so let's look into the person first the

00:09:15,220 --> 00:09:21,790
problem is how to extract meaningful

00:09:17,399 --> 00:09:25,269
information from that XML SVG in Python

00:09:21,790 --> 00:09:28,809
and what I found is this excellent SVG

00:09:25,269 --> 00:09:29,529
pad tools library which has a lot to

00:09:28,809 --> 00:09:34,059
offer

00:09:29,529 --> 00:09:35,860
so it has pad base class and a few

00:09:34,059 --> 00:09:38,709
subclasses thereof

00:09:35,860 --> 00:09:43,749
like line cubic Bezier quadratic Bezier

00:09:38,709 --> 00:09:47,049
and a few other top-level utilities each

00:09:43,749 --> 00:09:49,889
of those classes have rich api's for Pat

00:09:47,049 --> 00:09:52,989
intersection calculating bounding boxes

00:09:49,889 --> 00:09:55,089
transformation scaling and all sorts of

00:09:52,989 --> 00:09:59,649
other things you can cut paths you can

00:09:55,089 --> 00:10:02,139
like translate them and so on and also

00:09:59,649 --> 00:10:06,490
it allows you to easily read and write

00:10:02,139 --> 00:10:09,939
lists of SVG paths into or from SVG

00:10:06,490 --> 00:10:15,040
files and also apply some like scaling

00:10:09,939 --> 00:10:17,470
and other things so and it just takes a

00:10:15,040 --> 00:10:21,759
single line so this is basically an

00:10:17,470 --> 00:10:25,660
example of how how it is how easy it is

00:10:21,759 --> 00:10:29,379
to get those paths from from a file and

00:10:25,660 --> 00:10:31,269
this SVG to paths

00:10:29,379 --> 00:10:33,759
takes a file name and a bunch of other

00:10:31,269 --> 00:10:35,619
optional arguments deciding how to

00:10:33,759 --> 00:10:38,939
convert and what to convert so it

00:10:35,619 --> 00:10:41,290
converts everything to those three

00:10:38,939 --> 00:10:45,129
primitives line cubic and quadratic

00:10:41,290 --> 00:10:47,350
Bezier it has those art circles and all

00:10:45,129 --> 00:10:50,679
other things you know it converts them

00:10:47,350 --> 00:10:52,749
all into those and returns the list of

00:10:50,679 --> 00:10:55,360
baptistin sees and a list of

00:10:52,749 --> 00:11:01,629
dictionaries which contain the extra XML

00:10:55,360 --> 00:11:06,009
attributes of each of the paths so once

00:11:01,629 --> 00:11:06,499
we have this this is the easiest and

00:11:06,009 --> 00:11:12,109
simple

00:11:06,499 --> 00:11:15,019
where I found so we use pandas dataframe

00:11:12,109 --> 00:11:17,209
dot from records a class method which

00:11:15,019 --> 00:11:20,749
takes an iterable or in this case a

00:11:17,209 --> 00:11:24,079
generator of dictionary like objects

00:11:20,749 --> 00:11:28,399
with the same structure and in this case

00:11:24,079 --> 00:11:30,919
what I cared about is the actual index

00:11:28,399 --> 00:11:34,369
of that path instance within the file

00:11:30,919 --> 00:11:36,199
and as well it's bounding box so the

00:11:34,369 --> 00:11:38,269
minimum and maximum horizontal and

00:11:36,199 --> 00:11:42,739
vertical coordinates that fully

00:11:38,269 --> 00:11:46,449
encompass that stroke and we get the

00:11:42,739 --> 00:11:50,989
structure that looks kind of like this

00:11:46,449 --> 00:11:53,569
then on to the letter extraction so the

00:11:50,989 --> 00:11:57,139
problem is quite computationally

00:11:53,569 --> 00:12:00,289
intensive if you dress it from an if you

00:11:57,139 --> 00:12:02,899
know algorithm so you need to compare

00:12:00,289 --> 00:12:05,419
each stroke with all nearby strokes

00:12:02,899 --> 00:12:08,959
which might have something to do with it

00:12:05,419 --> 00:12:11,299
and merge them together as letters and

00:12:08,959 --> 00:12:14,359
what I found is that using a data frame

00:12:11,299 --> 00:12:17,239
simple iteration and filtering abate

00:12:14,359 --> 00:12:20,299
over multiple passes you can do that

00:12:17,239 --> 00:12:25,519
that easily and in quite quite quickly

00:12:20,299 --> 00:12:29,349
as well so the multiple passes are done

00:12:25,519 --> 00:12:32,720
by basically taking the data frame and

00:12:29,349 --> 00:12:36,559
returning it modified along with two

00:12:32,720 --> 00:12:39,799
sets of indices one for merged paths and

00:12:36,559 --> 00:12:41,779
one for get and merge paths which you

00:12:39,799 --> 00:12:45,559
can see here using the date frame you

00:12:41,779 --> 00:12:48,079
can easily extract those and then the

00:12:45,559 --> 00:12:51,889
each of the steps which I am going to

00:12:48,079 --> 00:12:54,859
show one of them which is this merging

00:12:51,889 --> 00:12:57,949
the fully overlapping paths basically

00:12:54,859 --> 00:13:01,369
all of them look look like this so we

00:12:57,949 --> 00:13:04,449
eye traits over each over the data frame

00:13:01,369 --> 00:13:08,059
taking each part in sequence and then we

00:13:04,449 --> 00:13:10,579
filter the data frame for example in

00:13:08,059 --> 00:13:13,699
this case all the paths that fully

00:13:10,579 --> 00:13:16,609
overlap their bounding box fully overlap

00:13:13,699 --> 00:13:18,499
with these current paths we take this as

00:13:16,609 --> 00:13:19,630
candidates like a subset of the date

00:13:18,499 --> 00:13:23,260
frame

00:13:19,630 --> 00:13:25,000
then we ran a fairly complicated merge

00:13:23,260 --> 00:13:27,670
procedure which I won't show because

00:13:25,000 --> 00:13:30,130
it's like a page and a half but

00:13:27,670 --> 00:13:32,980
basically what it does its updates the

00:13:30,130 --> 00:13:35,650
data frame so that when you merge two

00:13:32,980 --> 00:13:39,190
parts they have the same bounding box so

00:13:35,650 --> 00:13:42,490
updates the xmin xmax and so on of both

00:13:39,190 --> 00:13:46,270
to match the combined bounding box of

00:13:42,490 --> 00:13:51,640
both and also dates those merged and

00:13:46,270 --> 00:13:56,940
unmerged sets and returns the date frame

00:13:51,640 --> 00:13:59,470
and after each of those steps we run

00:13:56,940 --> 00:14:02,650
update data frame step which calculates

00:13:59,470 --> 00:14:05,140
additional properties for each of the

00:14:02,650 --> 00:14:08,860
paths and since pandals allows this

00:14:05,140 --> 00:14:11,590
quite easily you can chain assignments

00:14:08,860 --> 00:14:14,560
like this you know like for example

00:14:11,590 --> 00:14:16,510
calculating the width or the height of

00:14:14,560 --> 00:14:18,660
the bounding box the half with half

00:14:16,510 --> 00:14:22,870
height which is used in some of the

00:14:18,660 --> 00:14:25,810
merge steps also the area with the width

00:14:22,870 --> 00:14:28,780
x height and the aspect we divided by

00:14:25,810 --> 00:14:31,930
height and finally we need to sort the

00:14:28,780 --> 00:14:34,630
values so that they come in kind of

00:14:31,930 --> 00:14:39,790
natural writing order top to bottom

00:14:34,630 --> 00:14:43,450
left right so then once we have this we

00:14:39,790 --> 00:14:46,990
have a bunch of smaller files let their

00:14:43,450 --> 00:14:48,910
files which we then need to classify and

00:14:46,990 --> 00:14:52,230
this is a deliberately manual block

00:14:48,910 --> 00:14:54,820
process as per the client requirements

00:14:52,230 --> 00:14:57,220
there is an external tool they used

00:14:54,820 --> 00:15:00,700
already for for this sort of thing there

00:14:57,220 --> 00:15:03,970
is no pandas unfortunately so it loads

00:15:00,700 --> 00:15:07,000
the merged and unclassified letters a

00:15:03,970 --> 00:15:10,150
letter s VG's shows them one by one to

00:15:07,000 --> 00:15:13,170
human allows the human to us to align

00:15:10,150 --> 00:15:16,630
them you know in the box of the letter

00:15:13,170 --> 00:15:18,520
or the background and also allows them

00:15:16,630 --> 00:15:20,320
to label them like this is a dollar sign

00:15:18,520 --> 00:15:25,390
this is an and capital a this is a

00:15:20,320 --> 00:15:27,970
lowercase L so on once we have this we

00:15:25,390 --> 00:15:31,150
have labeled as VG letter files letter

00:15:27,970 --> 00:15:33,610
variants and then we come

00:15:31,150 --> 00:15:36,700
the word building so this is an example

00:15:33,610 --> 00:15:38,710
of an intermediate output of the

00:15:36,700 --> 00:15:41,890
algorithm which is a debug version

00:15:38,710 --> 00:15:45,250
showing the letters they're bounding

00:15:41,890 --> 00:15:49,510
boxes in green and the running baseline

00:15:45,250 --> 00:15:52,720
of the word which is the line along

00:15:49,510 --> 00:15:54,160
which all the letters are aligned so it

00:15:52,720 --> 00:15:58,720
looks like they're written on the same

00:15:54,160 --> 00:16:02,890
line so it takes a single word as an

00:15:58,720 --> 00:16:04,920
input for example testing it does a

00:16:02,890 --> 00:16:07,990
selection process for for each letter

00:16:04,920 --> 00:16:10,870
either sequentially or randomly with the

00:16:07,990 --> 00:16:13,000
seeds it picks a labelled variant for

00:16:10,870 --> 00:16:16,240
that letter then the horizontal

00:16:13,000 --> 00:16:18,610
composition merging selected variants

00:16:16,240 --> 00:16:21,670
with variable kerning which is a

00:16:18,610 --> 00:16:25,450
typographical term for the spacing

00:16:21,670 --> 00:16:28,960
between the letters and then there is a

00:16:25,450 --> 00:16:31,839
vertical alignment step which according

00:16:28,960 --> 00:16:35,440
to the running baseline aligns certain

00:16:31,839 --> 00:16:39,430
letters like for example G Y and others

00:16:35,440 --> 00:16:42,300
to order they're either below the base

00:16:39,430 --> 00:16:45,100
line or above the baseline as needed and

00:16:42,300 --> 00:16:49,690
outputs a single SVG file for that work

00:16:45,100 --> 00:16:52,779
in the same size so the label link just

00:16:49,690 --> 00:16:55,420
to remind you how it looks basically it

00:16:52,779 --> 00:16:59,230
takes as an input an excel file with

00:16:55,420 --> 00:17:01,930
mail addresses no surprise here pandas

00:16:59,230 --> 00:17:05,130
works great with this so the structure

00:17:01,930 --> 00:17:10,000
is one row per label one column per line

00:17:05,130 --> 00:17:14,939
as as simple as parsing using pandas

00:17:10,000 --> 00:17:18,880
rate Excel and the generation stage

00:17:14,939 --> 00:17:23,040
builds words with variable spacing one

00:17:18,880 --> 00:17:26,800
for each column and the alignment is

00:17:23,040 --> 00:17:29,410
done with so-called variable leading so

00:17:26,800 --> 00:17:31,900
the leading is the vertical equivalent

00:17:29,410 --> 00:17:36,940
of kerning so the spacing between the

00:17:31,900 --> 00:17:39,400
lines and that's it basically so I think

00:17:36,940 --> 00:17:43,990
I should tell you what I learned from

00:17:39,400 --> 00:17:44,920
this process so pandas is great for any

00:17:43,990 --> 00:17:47,590
sort

00:17:44,920 --> 00:17:50,220
of table-based data processing that was

00:17:47,590 --> 00:17:53,620
kind of an unexpected discovery for me

00:17:50,220 --> 00:17:55,240
so it might be intimidating at first if

00:17:53,620 --> 00:17:58,300
you haven't used it there is what to

00:17:55,240 --> 00:18:01,270
rate but if you learn just a few things

00:17:58,300 --> 00:18:06,330
and start from there like filtering and

00:18:01,270 --> 00:18:09,880
iteration you can go a long way also

00:18:06,330 --> 00:18:12,400
take time to understand the indexing and

00:18:09,880 --> 00:18:15,510
the power of multi index because that

00:18:12,400 --> 00:18:19,350
gives you the power to deal with

00:18:15,510 --> 00:18:23,440
multi-dimensional that that in a very

00:18:19,350 --> 00:18:25,600
comprehensible way then also of course

00:18:23,440 --> 00:18:28,450
anytime you need to deal with CSV or

00:18:25,600 --> 00:18:32,050
Excel which is quite a pain otherwise

00:18:28,450 --> 00:18:34,480
with pandas it's trivial and fast it

00:18:32,050 --> 00:18:37,420
doesn't have to be you know financial

00:18:34,480 --> 00:18:39,760
date or anything and also dock the

00:18:37,420 --> 00:18:41,860
documentation is great there is a lot to

00:18:39,760 --> 00:18:46,120
read so it could be a bit confusing at

00:18:41,860 --> 00:18:49,030
first but I would suggest start with ten

00:18:46,120 --> 00:18:51,610
minutes to pandas which is one of the

00:18:49,030 --> 00:18:54,940
main sections of the documentation there

00:18:51,610 --> 00:18:59,350
are also a lot of tutorials now a lot of

00:18:54,940 --> 00:19:01,500
cookbooks you know hands-on guides and

00:18:59,350 --> 00:19:04,270
it grew a lot they were actually

00:19:01,500 --> 00:19:07,300
recently a documentation sprints force

00:19:04,270 --> 00:19:12,610
for pandas which expanded up even

00:19:07,300 --> 00:19:13,770
further so with that I have just one

00:19:12,610 --> 00:19:17,110
more thing to say

00:19:13,770 --> 00:19:20,080
please consider buying west McKinney's

00:19:17,110 --> 00:19:22,600
book by - for data analysis because it's

00:19:20,080 --> 00:19:26,770
great and it will help you a lot with

00:19:22,600 --> 00:19:29,730
your journey into pandas and I'll be

00:19:26,770 --> 00:19:29,730
happy to take any questions

00:19:35,510 --> 00:19:38,460
thanks very much

00:19:36,990 --> 00:19:43,800
are there any questions you've got also

00:19:38,460 --> 00:19:47,670
time sorry I may ask a silly question I

00:19:43,800 --> 00:19:50,430
know you said well we need to spend to

00:19:47,670 --> 00:19:53,130
have you meet any I mean you your

00:19:50,430 --> 00:19:54,240
practical user keys your practical life

00:19:53,130 --> 00:19:56,580
work

00:19:54,240 --> 00:19:59,820
have you made some limitation of pandas

00:19:56,580 --> 00:20:02,940
oh yeah well there are things works that

00:19:59,820 --> 00:20:05,040
you tend to learn to live with but you

00:20:02,940 --> 00:20:07,500
tend to overcome as well like for

00:20:05,040 --> 00:20:09,840
example dealing with any sort of

00:20:07,500 --> 00:20:12,809
numerical data that can have gaps in it

00:20:09,840 --> 00:20:15,630
or possibly strings or anything they

00:20:12,809 --> 00:20:18,179
turn up as Nance instead of you know

00:20:15,630 --> 00:20:20,790
something else so if you expect to get

00:20:18,179 --> 00:20:25,280
integers you might get floats instead

00:20:20,790 --> 00:20:29,670
but the type converter is one one thing

00:20:25,280 --> 00:20:32,190
another use case I would like to rise to

00:20:29,670 --> 00:20:36,090
our community to to pay attention to is

00:20:32,190 --> 00:20:39,120
from my work the other case the theta

00:20:36,090 --> 00:20:41,820
the theta input we caught is a change

00:20:39,120 --> 00:20:45,300
but nicely the Jason Jason file is it

00:20:41,820 --> 00:20:48,179
just Jason Jim and so on Jason nice data

00:20:45,300 --> 00:20:51,480
Jason stream so the pandas you know you

00:20:48,179 --> 00:20:54,870
use pandas rate Jason can only process

00:20:51,480 --> 00:20:57,420
one level yeah so you make it's a very I

00:20:54,870 --> 00:20:59,610
haven't used it personally for Jason I

00:20:57,420 --> 00:21:02,250
think Postgres is better for that if you

00:20:59,610 --> 00:21:06,420
can't afford it I mean if you can have

00:21:02,250 --> 00:21:09,600
it I was no I mean my solution is I have

00:21:06,420 --> 00:21:11,700
to write my personal library to process

00:21:09,600 --> 00:21:13,950
this one into a data frame but that's

00:21:11,700 --> 00:21:16,020
quite aesthetic so I was I was always

00:21:13,950 --> 00:21:18,630
thinking of you if the pandas can

00:21:16,020 --> 00:21:21,990
observe this feature basically he

00:21:18,630 --> 00:21:23,910
analysis the Jason's files because the

00:21:21,990 --> 00:21:26,850
output is always even though it's nicely

00:21:23,910 --> 00:21:29,610
the Jason the output will be pandas will

00:21:26,850 --> 00:21:31,800
be dependents data frame so so I was

00:21:29,610 --> 00:21:35,040
thinking if it pandas can absorb the

00:21:31,800 --> 00:21:37,920
feature piece okay and firstly step one

00:21:35,040 --> 00:21:39,929
analysis the the gzip well to identify

00:21:37,920 --> 00:21:42,090
the you know they're like keywords you

00:21:39,929 --> 00:21:44,370
capture with just the crunch and the get

00:21:42,090 --> 00:21:46,740
the did form oh I will be a pro in

00:21:44,370 --> 00:21:51,300
Romans fall and his butt palace is

00:21:46,740 --> 00:21:55,650
splendid ivory my question is the

00:21:51,300 --> 00:21:58,650
limitation of the pandas yeah so I'm

00:21:55,650 --> 00:22:00,360
sure you can go a long way using pandas

00:21:58,650 --> 00:22:00,990
for some of pipe some part of that

00:22:00,360 --> 00:22:02,580
process

00:22:00,990 --> 00:22:07,170
you know reading the Jason the nested

00:22:02,580 --> 00:22:09,809
Jason and for sure if you can convert it

00:22:07,170 --> 00:22:15,210
to something more tabular you'll get a

00:22:09,809 --> 00:22:23,540
lot more out of families there any other

00:22:15,210 --> 00:22:23,540
questions I hope you try it

00:22:24,080 --> 00:22:31,280

YouTube URL: https://www.youtube.com/watch?v=8YH9u29_3dQ


