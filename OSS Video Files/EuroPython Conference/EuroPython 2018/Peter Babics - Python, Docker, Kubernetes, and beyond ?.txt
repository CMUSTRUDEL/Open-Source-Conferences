Title: Peter Babics - Python, Docker, Kubernetes, and beyond ?
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Python, Docker, Kubernetes, and beyond ?
[EuroPython 2018 - Talk - 2018-07-25 - Moorfoot]
[Edinburgh, UK]

By Peter Babics

Have you ever tried to manage deployment of multiple python applications through various
linux distributions ? If so, you must have heard of Docker and maybe also Kubernetes.
Distributing python applications using docker is simple and allows to create static packages
containing everything required for them to run. Also it allows to freeze everything, packages, available
libraries, files on filesystem.
In my speech I would like to tell you about our brief journey, of moving our trading platform
from standalone application directly on host system, through deploying it in docker and latter
moving it to kubernetes. I will explain our struggles with implementing stable and fast CI using Gitlab CI and Docker,
image (package) storage and cleanup of old images and finally I will tell you how we are deploying
our platform to kubernetes, with nothing more than yaml-s and templating.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,120 --> 00:00:04,390
[Applause]

00:00:05,029 --> 00:00:11,250
so hi

00:00:07,880 --> 00:00:13,830
let me start this with a question so how

00:00:11,250 --> 00:00:17,840
many of you are know about docker or

00:00:13,830 --> 00:00:20,100
kubernetes raise your hands please and

00:00:17,840 --> 00:00:29,429
how many of you are using it in

00:00:20,100 --> 00:00:31,130
production make a Spock spoke so so let

00:00:29,429 --> 00:00:35,969
me first introduce Coltrane

00:00:31,130 --> 00:00:38,969
we are technological company we are

00:00:35,969 --> 00:00:43,469
trading stocks on different stock

00:00:38,969 --> 00:00:47,489
exchanges around the Europe we are

00:00:43,469 --> 00:00:50,219
placed in Pratt we have a relatively

00:00:47,489 --> 00:00:54,030
small team of developers there is 14 of

00:00:50,219 --> 00:00:56,640
us as I said we are developing trading

00:00:54,030 --> 00:01:01,949
platform which we are also using we are

00:00:56,640 --> 00:01:03,539
not selling it outside and we use mostly

00:01:01,949 --> 00:01:07,049
open source projects to run our

00:01:03,539 --> 00:01:09,360
infrastructure and even our most of our

00:01:07,049 --> 00:01:16,470
code uses of phone source project

00:01:09,360 --> 00:01:22,140
projects our trading platform is written

00:01:16,470 --> 00:01:23,659
in Python obviously it uses react.js for

00:01:22,140 --> 00:01:26,850
front-end

00:01:23,659 --> 00:01:29,900
in our applications even in our trading

00:01:26,850 --> 00:01:33,509
platforms we heavily use async i/o

00:01:29,900 --> 00:01:36,710
because there's a lot of data and we

00:01:33,509 --> 00:01:39,509
want to process it in parallel

00:01:36,710 --> 00:01:42,390
concurrently excuse me

00:01:39,509 --> 00:01:46,710
and we are storing data in Redis and

00:01:42,390 --> 00:01:49,680
timescale DB so also we are using

00:01:46,710 --> 00:01:53,100
third-party libraries integrating it

00:01:49,680 --> 00:01:56,189
with integrating them with Saturn those

00:01:53,100 --> 00:02:05,909
libraries are written in C C++ Java or

00:01:56,189 --> 00:02:08,580
even Scala so Satan right so there are

00:02:05,909 --> 00:02:11,430
lots there are a lot of data so we have

00:02:08,580 --> 00:02:14,099
to split our application into multiple

00:02:11,430 --> 00:02:17,739
processes and also

00:02:14,099 --> 00:02:22,840
applications around which help us like

00:02:17,739 --> 00:02:26,799
there's reporting some graph graphs and

00:02:22,840 --> 00:02:29,290
other tools we use also we are using

00:02:26,799 --> 00:02:31,569
messaging to integrate all these

00:02:29,290 --> 00:02:35,739
applications together and pass data from

00:02:31,569 --> 00:02:39,969
one to another when I still first

00:02:35,739 --> 00:02:43,150
started working for court Lane there was

00:02:39,969 --> 00:02:45,900
kind of a chaos all the applications who

00:02:43,150 --> 00:02:50,200
have deployed on physical servers

00:02:45,900 --> 00:02:53,409
managed by circus circus' D this process

00:02:50,200 --> 00:02:57,219
management system similar to supervisor

00:02:53,409 --> 00:02:59,919
D if you know packages were installed in

00:02:57,219 --> 00:03:02,769
the virtual end so each application has

00:02:59,919 --> 00:03:06,159
a separate virtual end with installed

00:03:02,769 --> 00:03:09,790
pre-installed packages probably the

00:03:06,159 --> 00:03:18,340
standard and riffing run and the single

00:03:09,790 --> 00:03:21,430
user rows of this also this parent that

00:03:18,340 --> 00:03:23,889
it was simple it was simple in the

00:03:21,430 --> 00:03:26,229
manner that when a new user came to the

00:03:23,889 --> 00:03:28,150
project all he had to do was clone

00:03:26,229 --> 00:03:30,400
project wait withdraw and install

00:03:28,150 --> 00:03:33,099
packages and he could run run the

00:03:30,400 --> 00:03:37,000
application this it was the same for the

00:03:33,099 --> 00:03:39,069
deployment there were some disadvantages

00:03:37,000 --> 00:03:43,870
like there was some package versioning

00:03:39,069 --> 00:03:46,810
hell which means that we had different

00:03:43,870 --> 00:03:52,269
packages and whenever we updated one of

00:03:46,810 --> 00:03:55,689
our own packages it may use some new ver

00:03:52,269 --> 00:03:59,620
third party libraries and we had to

00:03:55,689 --> 00:04:03,609
somehow may have migrated those changes

00:03:59,620 --> 00:04:05,259
into our other applications and there

00:04:03,609 --> 00:04:07,540
was no failure because when the server

00:04:05,259 --> 00:04:09,609
died everything on they died and the

00:04:07,540 --> 00:04:12,209
report was not ultimately migrated

00:04:09,609 --> 00:04:12,209
somewhere else

00:04:17,840 --> 00:04:25,020
what happened next was docker when I

00:04:22,049 --> 00:04:30,270
started we are already looking into the

00:04:25,020 --> 00:04:34,080
use of docker I said was yes it has a

00:04:30,270 --> 00:04:37,770
promising features like it was able to

00:04:34,080 --> 00:04:40,080
unify in our environment so that we

00:04:37,770 --> 00:04:43,740
could run the local development with the

00:04:40,080 --> 00:04:47,310
same kind of package of the image as it

00:04:43,740 --> 00:04:51,330
was in staging CI and the planning

00:04:47,310 --> 00:04:52,770
production the deployment was all also

00:04:51,330 --> 00:04:54,960
simple because all you had to do was

00:04:52,770 --> 00:04:57,539
build and build the image just run one

00:04:54,960 --> 00:05:02,009
command and you have probably running

00:04:57,539 --> 00:05:06,270
application migrations were also

00:05:02,009 --> 00:05:08,419
simplified as the image contained

00:05:06,270 --> 00:05:10,919
everything you don't you don't have to

00:05:08,419 --> 00:05:19,220
install anything else just pull the

00:05:10,919 --> 00:05:23,370
imogen you have it it's speed up our CI

00:05:19,220 --> 00:05:25,620
no matter that image when we build them

00:05:23,370 --> 00:05:27,360
it bill and the image is built it

00:05:25,620 --> 00:05:31,620
contains everything so we can use this

00:05:27,360 --> 00:05:37,560
image through altar to hold other stages

00:05:31,620 --> 00:05:40,620
of CI and we have atomic releases as the

00:05:37,560 --> 00:05:46,099
built in which has some tag and even

00:05:40,620 --> 00:05:49,289
some hash so we can we have image with

00:05:46,099 --> 00:05:56,699
we have image with the hash which is

00:05:49,289 --> 00:05:58,800
atomic and unique in our registry there

00:05:56,699 --> 00:06:01,620
are also there also there were some

00:05:58,800 --> 00:06:03,990
challenges we had to overcome first to

00:06:01,620 --> 00:06:08,039
introduce Docherty to our infrastructure

00:06:03,990 --> 00:06:11,610
and those were like how do we store the

00:06:08,039 --> 00:06:14,789
images for this case we decided to use

00:06:11,610 --> 00:06:18,000
github registry as we already had good

00:06:14,789 --> 00:06:24,270
luck instance in our infrastructure and

00:06:18,000 --> 00:06:26,779
it had this feature next thing was image

00:06:24,270 --> 00:06:26,779
caching because

00:06:27,240 --> 00:06:32,949
it's kind of sad but we have a pretty

00:06:30,370 --> 00:06:35,830
slow uplink to the internet like our

00:06:32,949 --> 00:06:37,600
internal network is fast it it is it has

00:06:35,830 --> 00:06:47,080
a gigabit but uplink

00:06:37,600 --> 00:06:50,320
it's around 20 megabits much so so as I

00:06:47,080 --> 00:06:55,060
said we are using it up and github has

00:06:50,320 --> 00:06:57,729
CI which has steps defined in the git

00:06:55,060 --> 00:07:00,940
repository so anybody who has access to

00:06:57,729 --> 00:07:05,889
get the git repository can update the

00:07:00,940 --> 00:07:10,710
pipeline definition and modified it to

00:07:05,889 --> 00:07:17,919
whatever he wants I wanted to have a

00:07:10,710 --> 00:07:20,260
built stage in C I like the simple CI

00:07:17,919 --> 00:07:25,270
build is this you just want docker built

00:07:20,260 --> 00:07:29,530
and it should build the image and repair

00:07:25,270 --> 00:07:31,990
everything but C user has access to CI

00:07:29,530 --> 00:07:36,340
definition he can update this and modify

00:07:31,990 --> 00:07:38,289
it maybe to something like this and by

00:07:36,340 --> 00:07:41,830
this he can effectively get access to

00:07:38,289 --> 00:07:45,729
the server on which the docker demon is

00:07:41,830 --> 00:07:48,250
running this means that you should have

00:07:45,729 --> 00:07:51,310
dedicated building environment which you

00:07:48,250 --> 00:07:54,810
can just take and throw out replace it

00:07:51,310 --> 00:07:58,300
with a new one clean so whenever

00:07:54,810 --> 00:08:02,050
somebody get access to this environment

00:07:58,300 --> 00:08:07,389
and does something harmful to it you can

00:08:02,050 --> 00:08:11,110
just clean it and go without any problem

00:08:07,389 --> 00:08:18,250
further next thing was CI pipeline

00:08:11,110 --> 00:08:21,630
design because you want to have fast CI

00:08:18,250 --> 00:08:25,150
you don't want to spend 20 minutes on

00:08:21,630 --> 00:08:29,800
building and then testing and then maybe

00:08:25,150 --> 00:08:33,430
integration test publishing whatever and

00:08:29,800 --> 00:08:36,690
the last thing was cleaning up cleaning

00:08:33,430 --> 00:08:39,419
of all the images

00:08:36,690 --> 00:08:44,089
the standard dock registry which you can

00:08:39,419 --> 00:08:47,250
download from the crab does not have

00:08:44,089 --> 00:08:52,920
automatic cleanup of all the images and

00:08:47,250 --> 00:08:57,060
because our images have around 5500

00:08:52,920 --> 00:09:00,389
megabytes and we have like hundreds of

00:08:57,060 --> 00:09:03,779
them we had we had to implement some

00:09:00,389 --> 00:09:07,079
kind of cleanup of these images we are

00:09:03,779 --> 00:09:09,569
not we are not running in abs in we are

00:09:07,079 --> 00:09:16,620
not only running in cloud so we have we

00:09:09,569 --> 00:09:19,889
don't have infinite storage so what

00:09:16,620 --> 00:09:22,319
doctor brought us was as I said unified

00:09:19,889 --> 00:09:24,709
a stable environment by means that we

00:09:22,319 --> 00:09:28,850
had the same image for local development

00:09:24,709 --> 00:09:34,550
ci staging and temp production and

00:09:28,850 --> 00:09:38,399
everything was baked in so developer

00:09:34,550 --> 00:09:41,009
when he buildin the image he could be

00:09:38,399 --> 00:09:45,149
sure that the packages which are in it

00:09:41,009 --> 00:09:46,740
and all the images I don't mean oh just

00:09:45,149 --> 00:09:49,829
those which are specific requirements

00:09:46,740 --> 00:09:54,439
but also the fourth party requirements

00:09:49,829 --> 00:09:59,250
and the full chain will have the

00:09:54,439 --> 00:10:05,850
specified version and this will be same

00:09:59,250 --> 00:10:08,160
in CI and staging and production because

00:10:05,850 --> 00:10:11,880
of the doctor nature when you build an

00:10:08,160 --> 00:10:13,740
image it has everything packed in so the

00:10:11,880 --> 00:10:17,040
basic of this is to create one image

00:10:13,740 --> 00:10:20,029
make everything in all the requirements

00:10:17,040 --> 00:10:23,870
or the development requirements location

00:10:20,029 --> 00:10:27,509
some environment specifics you need and

00:10:23,870 --> 00:10:30,779
you can run everything in this image as

00:10:27,509 --> 00:10:36,240
I will show you on the next slide it

00:10:30,779 --> 00:10:41,100
also brought us isolated environment by

00:10:36,240 --> 00:10:42,930
means that when the application runs it

00:10:41,100 --> 00:10:46,470
cannot access other applications running

00:10:42,930 --> 00:10:49,170
on the system you cannot take control

00:10:46,470 --> 00:10:51,250
over the other processes which are

00:10:49,170 --> 00:10:55,220
running there

00:10:51,250 --> 00:11:01,519
and that's probably just some kind of

00:10:55,220 --> 00:11:07,449
security feature so that's the CI and

00:11:01,519 --> 00:11:07,449
this is our spice this is our five

00:11:11,050 --> 00:11:16,030
so first we build them the image as I

00:11:14,380 --> 00:11:19,420
said it contains everything it contains

00:11:16,030 --> 00:11:25,120
all the packages application and some

00:11:19,420 --> 00:11:27,720
environment definitions then next we run

00:11:25,120 --> 00:11:30,460
tests go quality unit tests packaging

00:11:27,720 --> 00:11:34,420
those are run inside this image in

00:11:30,460 --> 00:11:37,090
parallel so each of those tests may be

00:11:34,420 --> 00:11:39,490
run like maybe two minutes and this

00:11:37,090 --> 00:11:42,520
speeds up entire process because in

00:11:39,490 --> 00:11:44,950
those stay in this that stage none of

00:11:42,520 --> 00:11:49,960
the jobs have to install the packages

00:11:44,950 --> 00:11:53,260
which was the bottleneck of our CI so

00:11:49,960 --> 00:11:54,550
next we optionally deploy next we

00:11:53,260 --> 00:11:58,060
optionally really is bleeding edge

00:11:54,550 --> 00:12:01,150
version and deploy to staging and then

00:11:58,060 --> 00:12:04,090
we run integration tests and publish the

00:12:01,150 --> 00:12:06,250
documentation note that leading edge

00:12:04,090 --> 00:12:09,460
release and state deployment to staging

00:12:06,250 --> 00:12:13,900
are optional so we can run integration

00:12:09,460 --> 00:12:19,600
tests immediately after the unit tests

00:12:13,900 --> 00:12:22,000
are complete so docker has also some

00:12:19,600 --> 00:12:25,570
kind of Locker has also disadvantages

00:12:22,000 --> 00:12:28,300
like there are known bugs and every day

00:12:25,570 --> 00:12:31,750
you can find a new one for example there

00:12:28,300 --> 00:12:35,470
are memory leaks there are some race

00:12:31,750 --> 00:12:36,580
conditions which lead to dead logs it

00:12:35,470 --> 00:12:40,420
has no failover

00:12:36,580 --> 00:12:44,590
if you don't use docker swarm what kind

00:12:40,420 --> 00:12:48,450
of state do custom right now is but also

00:12:44,590 --> 00:12:51,400
when the local diamond eyes you cannot

00:12:48,450 --> 00:12:53,230
manipulate the running containers your

00:12:51,400 --> 00:12:56,310
containers may still be running but you

00:12:53,230 --> 00:13:06,220
cannot stop them or restart them or

00:12:56,310 --> 00:13:08,380
create new ones there are few gotchas we

00:13:06,220 --> 00:13:11,440
found out when we started using docker

00:13:08,380 --> 00:13:17,610
and for example there's vit one pitfall

00:13:11,440 --> 00:13:19,820
who knows about the ID one pitfall yeah

00:13:17,610 --> 00:13:24,470
something you don't know

00:13:19,820 --> 00:13:32,000
nice so PID one pitfall is basically a

00:13:24,470 --> 00:13:34,460
problem or maybe a feature let's say so

00:13:32,000 --> 00:13:38,330
when when you run an application in

00:13:34,460 --> 00:13:41,510
docker it is started with PID 1 PID 1

00:13:38,330 --> 00:13:43,820
has special meaning in Linux because it

00:13:41,510 --> 00:13:55,060
is an init process which starts

00:13:43,820 --> 00:13:55,060
everything like sssh your UI whatever

00:13:56,200 --> 00:14:03,230
PID 1 doesn't inherit default default

00:14:00,350 --> 00:14:06,140
signal handler handlers which mean that

00:14:03,230 --> 00:14:09,440
you have to implement them by by

00:14:06,140 --> 00:14:11,200
yourself who knows about the signal

00:14:09,440 --> 00:14:17,570
handle of sting Linux

00:14:11,200 --> 00:14:22,610
ok so you have to implement those signal

00:14:17,570 --> 00:14:24,800
Heather handlers because you you usually

00:14:22,610 --> 00:14:28,940
want a graceful graceful shutdown of

00:14:24,800 --> 00:14:31,370
your application so when you run docker

00:14:28,940 --> 00:14:33,980
command the process is started with PID

00:14:31,370 --> 00:14:38,300
1 and when you run docker stop on it

00:14:33,980 --> 00:14:40,340
which should terminate the process the

00:14:38,300 --> 00:14:43,610
first what docker do is it sends

00:14:40,340 --> 00:14:45,650
six-term to it if the application

00:14:43,610 --> 00:14:49,070
doesn't shut down in 10 seconds after

00:14:45,650 --> 00:14:50,690
days it will send SiC kill effectively

00:14:49,070 --> 00:14:54,440
killing everything you may lose your

00:14:50,690 --> 00:14:57,350
state with this so it's good idea to

00:14:54,440 --> 00:15:00,710
implement the signal handlers this also

00:14:57,350 --> 00:15:04,970
applies for a process which does not run

00:15:00,710 --> 00:15:11,210
outside of which runs outside of Python

00:15:04,970 --> 00:15:14,900
outside of docker but also it has one

00:15:11,210 --> 00:15:17,630
other meaning that you have to like the

00:15:14,900 --> 00:15:19,520
process runs in docker you have to take

00:15:17,630 --> 00:15:22,160
care of the sub processes you run so if

00:15:19,520 --> 00:15:24,290
you are using sub process module and

00:15:22,160 --> 00:15:27,470
running other processes such I'll

00:15:24,290 --> 00:15:30,620
process this you have to terminate them

00:15:27,470 --> 00:15:34,690
and clean up after them because if you

00:15:30,620 --> 00:15:34,690
don't they will remain there

00:15:35,110 --> 00:15:40,700
doctor will probably somehow take care

00:15:38,210 --> 00:15:47,000
of them or cannot bill can't kill them

00:15:40,700 --> 00:15:48,890
at at the end so also we have to take in

00:15:47,000 --> 00:15:54,470
mind the user within and within the

00:15:48,890 --> 00:15:57,920
container because when somebody get

00:15:54,470 --> 00:16:00,470
access to your container and runs a

00:15:57,920 --> 00:16:06,890
shell in it let's say some kind of

00:16:00,470 --> 00:16:09,920
attacker the one can get the root access

00:16:06,890 --> 00:16:11,900
if you're on your applications your

00:16:09,920 --> 00:16:14,450
application not on the root which means

00:16:11,900 --> 00:16:16,370
that he can modify the file system

00:16:14,450 --> 00:16:18,800
within it even run out other

00:16:16,370 --> 00:16:21,290
applications and you should avoid this

00:16:18,800 --> 00:16:25,790
you should avoid this because you don't

00:16:21,290 --> 00:16:28,760
want it to modify the modified a

00:16:25,790 --> 00:16:33,250
integrant you know he can even run some

00:16:28,760 --> 00:16:33,250
kind of spam but you don't want that

00:16:38,650 --> 00:16:46,430
after we migrated to docker took around

00:16:42,530 --> 00:16:51,380
maybe a month we started looking where

00:16:46,430 --> 00:16:56,500
to move next and we found kubernetes it

00:16:51,380 --> 00:16:59,870
has some kind of maybe I didn't like

00:16:56,500 --> 00:17:06,830
doctor has a whale kubernetes has a

00:16:59,870 --> 00:17:08,980
wheel I don't know so what kubernetes is

00:17:06,830 --> 00:17:12,890
is basically a cluster orchestration

00:17:08,980 --> 00:17:15,250
this means that you have a bunch of

00:17:12,890 --> 00:17:18,110
servers you install kubernetes on them

00:17:15,250 --> 00:17:21,200
kubernetes somehow manages all of them

00:17:18,110 --> 00:17:22,910
and you just tell cuban st's to run the

00:17:21,200 --> 00:17:25,490
application somewhere in the cluster you

00:17:22,910 --> 00:17:28,670
don't care where you just want to have

00:17:25,490 --> 00:17:33,020
it up running and may be accessible on

00:17:28,670 --> 00:17:36,470
some kind of an address so what

00:17:33,020 --> 00:17:38,750
kubernetes was interesting in what your

00:17:36,470 --> 00:17:42,860
initial kubernetes was is interesting

00:17:38,750 --> 00:17:46,760
for us because it sold some kind of

00:17:42,860 --> 00:17:50,270
failover when a server failed so

00:17:46,760 --> 00:17:53,530
you burn with these dyes it migrates the

00:17:50,270 --> 00:17:56,120
workload from that server somewhere else

00:17:53,530 --> 00:18:00,100
so you don't have to care about it and

00:17:56,120 --> 00:18:05,570
you have you can sleep at 3 a.m. won't

00:18:00,100 --> 00:18:08,470
wake you up the configuration can be

00:18:05,570 --> 00:18:11,510
stored in a spaces these spaces are

00:18:08,470 --> 00:18:13,640
logical dividers you can have name space

00:18:11,510 --> 00:18:17,360
for production for staging given for

00:18:13,640 --> 00:18:20,930
different applications maybe namespace

00:18:17,360 --> 00:18:23,600
for moratorium vlogging even your

00:18:20,930 --> 00:18:26,120
applications and for each of these

00:18:23,600 --> 00:18:30,470
namespaces you can have stored a global

00:18:26,120 --> 00:18:33,650
configuration which can be then

00:18:30,470 --> 00:18:37,630
propagated from this namespace to the

00:18:33,650 --> 00:18:40,280
services running in in this namespace

00:18:37,630 --> 00:18:42,920
also it suffers some kind of basic

00:18:40,280 --> 00:18:47,840
service discovery you can access other

00:18:42,920 --> 00:18:50,030
services based on DNS like my service my

00:18:47,840 --> 00:18:55,070
namespace at least cluster look o is the

00:18:50,030 --> 00:18:58,790
standard address just have to my service

00:18:55,070 --> 00:19:01,850
and my namespace it has an ingress

00:18:58,790 --> 00:19:05,210
controller it's a person ingress

00:19:01,850 --> 00:19:08,690
controller ingress it's a way to expose

00:19:05,210 --> 00:19:11,030
the services applications running in

00:19:08,690 --> 00:19:16,790
kubernetes to outside world so the

00:19:11,030 --> 00:19:19,250
outside world can make requests and for

00:19:16,790 --> 00:19:23,780
example retrieve some kind of website

00:19:19,250 --> 00:19:26,540
which is running in kubernetes the other

00:19:23,780 --> 00:19:28,490
way around like the services with human

00:19:26,540 --> 00:19:31,820
communities can still access the outside

00:19:28,490 --> 00:19:33,560
world but for that outside world to

00:19:31,820 --> 00:19:39,550
access the community services you have

00:19:33,560 --> 00:19:42,170
to have an egress controller also

00:19:39,550 --> 00:19:44,240
kubernetes has one fancy feature and

00:19:42,170 --> 00:19:47,870
that's deployment history which means

00:19:44,240 --> 00:19:53,780
that when you deploy some kind of new

00:19:47,870 --> 00:19:57,320
service and it doesn't behave you want

00:19:53,780 --> 00:19:59,960
to revert it to previous versions so you

00:19:57,320 --> 00:20:02,460
can call cube ctlr rollout and do and

00:19:59,960 --> 00:20:06,030
this will

00:20:02,460 --> 00:20:09,640
deployed a previous version so it's

00:20:06,030 --> 00:20:14,200
really handsome ability when you want to

00:20:09,640 --> 00:20:16,270
simply revert revert something and you

00:20:14,200 --> 00:20:21,760
don't know which version version it was

00:20:16,270 --> 00:20:24,490
running before right now we are in the

00:20:21,760 --> 00:20:27,070
process of migration to kubernetes our

00:20:24,490 --> 00:20:32,980
main trading platform was already

00:20:27,070 --> 00:20:35,260
migrated week ago there are still other

00:20:32,980 --> 00:20:39,490
services which are running running still

00:20:35,260 --> 00:20:41,320
in docker on others other hosts but we

00:20:39,490 --> 00:20:45,160
are planning to migrate them in maybe

00:20:41,320 --> 00:20:47,470
two weeks and then we can join all the

00:20:45,160 --> 00:20:53,040
other servers into the kubernetes

00:20:47,470 --> 00:20:57,550
cluster we have in the environment yeah

00:20:53,040 --> 00:20:59,950
configured by namespace variables so we

00:20:57,550 --> 00:21:04,780
have production namespace in which we

00:20:59,950 --> 00:21:06,820
have configuration for some which

00:21:04,780 --> 00:21:10,480
specifies where you can find services

00:21:06,820 --> 00:21:14,470
like the messaging some kind of data

00:21:10,480 --> 00:21:20,700
storage access to databases and so on

00:21:14,470 --> 00:21:25,660
and we are deploying our services using

00:21:20,700 --> 00:21:28,390
playing ammos with ginger too like

00:21:25,660 --> 00:21:32,670
ginger two files containing gammas with

00:21:28,390 --> 00:21:37,750
four variables this allows us to have

00:21:32,670 --> 00:21:40,750
some conditionals in templates and have

00:21:37,750 --> 00:21:46,150
a single deployment file which is

00:21:40,750 --> 00:21:51,250
adaptable for different processes or

00:21:46,150 --> 00:21:53,110
maybe profiles configurations and so in

00:21:51,250 --> 00:21:59,410
this exam exam Polly can see that we

00:21:53,110 --> 00:22:02,950
have some profile of specified and if

00:21:59,410 --> 00:22:05,890
the profile is C Tolu the other there

00:22:02,950 --> 00:22:10,090
are other environment variables added to

00:22:05,890 --> 00:22:14,130
the big moment so no some non technical

00:22:10,090 --> 00:22:14,130
features about kubernetes are

00:22:14,460 --> 00:22:23,169
probes imagine that you have web

00:22:18,279 --> 00:22:25,960
application and deploy it to kubernetes

00:22:23,169 --> 00:22:29,760
you want to check that it is running and

00:22:25,960 --> 00:22:33,039
if it starts like maybe 30 seconds is

00:22:29,760 --> 00:22:35,620
you just don't want to care about it you

00:22:33,039 --> 00:22:38,559
just deploy it and want to see it

00:22:35,620 --> 00:22:42,880
running as fast as possible so probes

00:22:38,559 --> 00:22:47,049
what proves that do is that they check

00:22:42,880 --> 00:22:49,899
if application is running by accessing

00:22:47,049 --> 00:22:53,140
the some kind of port you specify or

00:22:49,899 --> 00:22:55,720
running some internal command everything

00:22:53,140 --> 00:22:58,690
the container this allows communities to

00:22:55,720 --> 00:23:00,730
check if the service is running and if

00:22:58,690 --> 00:23:06,850
it's healthy if it's not kubernetes will

00:23:00,730 --> 00:23:11,049
automatically restart it and then there

00:23:06,850 --> 00:23:15,010
are other strategies there are two major

00:23:11,049 --> 00:23:17,500
one one is rolling up update what this

00:23:15,010 --> 00:23:20,169
strategy does is that when you deploy a

00:23:17,500 --> 00:23:24,299
new version the old version is still

00:23:20,169 --> 00:23:26,440
running and new new version you know new

00:23:24,299 --> 00:23:30,190
deployment with new version is started

00:23:26,440 --> 00:23:34,059
and unless the deployment with new

00:23:30,190 --> 00:23:37,720
version is running available and stable

00:23:34,059 --> 00:23:39,549
the previous version is the previous the

00:23:37,720 --> 00:23:43,960
previous deployment with old version is

00:23:39,549 --> 00:23:46,419
still running so when the second version

00:23:43,960 --> 00:23:51,580
the new one is available the first one

00:23:46,419 --> 00:23:55,950
gets terminated it's shut down and all

00:23:51,580 --> 00:24:02,409
the traffic is forward forwarded to the

00:23:55,950 --> 00:24:06,279
new deployment another of these

00:24:02,409 --> 00:24:08,500
strategies recreate what it does is that

00:24:06,279 --> 00:24:12,159
it first Radames the previous version

00:24:08,500 --> 00:24:15,580
then starts the new and you can have

00:24:12,159 --> 00:24:18,669
some non zero to non zero the whole time

00:24:15,580 --> 00:24:22,110
this other strategy is good when you

00:24:18,669 --> 00:24:24,740
have some kind of resource which has a

00:24:22,110 --> 00:24:28,460
unique lock maybe

00:24:24,740 --> 00:24:30,860
you can have some data stored in files

00:24:28,460 --> 00:24:38,059
and you don't want to applications to

00:24:30,860 --> 00:24:40,670
access the file simultaneously so those

00:24:38,059 --> 00:24:45,020
were the fancy features or interesting

00:24:40,670 --> 00:24:49,150
features of kubernetes and as I

00:24:45,020 --> 00:24:53,720
mentioned dr. Haswell humanities has

00:24:49,150 --> 00:24:56,630
wheel so beyond that there are many

00:24:53,720 --> 00:24:59,059
wheels so we are looking into the

00:24:56,630 --> 00:25:03,190
kubernetes Federation next and that's

00:24:59,059 --> 00:25:10,850
beyond so thank you

00:25:03,190 --> 00:25:13,490
[Applause]

00:25:10,850 --> 00:25:19,940
and thank you Pizza please raise your

00:25:13,490 --> 00:25:22,460
hand if you have any questions are these

00:25:19,940 --> 00:25:26,000
ginger templates integrated somehow have

00:25:22,460 --> 00:25:31,070
kubernetes conflicts and secrets for

00:25:26,000 --> 00:25:34,220
deployment like you can send only pure

00:25:31,070 --> 00:25:37,789
Yama files to kubernetes and JSON files

00:25:34,220 --> 00:25:40,340
but you can you can't send the ginger to

00:25:37,789 --> 00:25:43,580
file so you first have to fill in the

00:25:40,340 --> 00:25:46,429
variables then you can send it and the

00:25:43,580 --> 00:25:49,370
way our deployment is designed that we

00:25:46,429 --> 00:25:51,649
take the ginger file and fill and fill

00:25:49,370 --> 00:25:54,340
in everything we have so we have also

00:25:51,649 --> 00:25:59,990
secrets and copic maps within deployment

00:25:54,340 --> 00:26:02,750
can I have one more which deployment

00:25:59,990 --> 00:26:05,779
strategy should I use if I have database

00:26:02,750 --> 00:26:08,899
migrations between versions I should use

00:26:05,779 --> 00:26:13,429
this recreate strategy or there is some

00:26:08,899 --> 00:26:21,889
others better solutions if you don't

00:26:13,429 --> 00:26:23,600
have any any way to fix like to take

00:26:21,889 --> 00:26:25,519
care of the difference between those two

00:26:23,600 --> 00:26:28,879
versions you have you should use the

00:26:25,519 --> 00:26:36,350
recreate but maybe there's a way I don't

00:26:28,879 --> 00:26:37,759
know how do you monitor all this that's

00:26:36,350 --> 00:26:40,850
an excellent question

00:26:37,759 --> 00:26:44,179
we are monitoring internal pots reusing

00:26:40,850 --> 00:26:47,679
hipster and we are monitoring entire

00:26:44,179 --> 00:26:47,679
cluster by Prometheus

00:26:53,460 --> 00:26:58,480
so previously you mentioned the fact

00:26:56,080 --> 00:27:01,770
that the problem of the pit one wooden

00:26:58,480 --> 00:27:03,820
Locker how that kubernetes helped you

00:27:01,770 --> 00:27:06,190
kubernetes doesn't solve that you have

00:27:03,820 --> 00:27:07,150
to solve this by yourself with within

00:27:06,190 --> 00:27:12,610
your application

00:27:07,150 --> 00:27:14,470
ok as Bendis salt and if so how we are

00:27:12,610 --> 00:27:17,559
solving this by registering signal

00:27:14,470 --> 00:27:20,410
handler which as I mentioned we are

00:27:17,559 --> 00:27:23,820
using async i/o so what do we basically

00:27:20,410 --> 00:27:28,020
do we just stop the loop which

00:27:23,820 --> 00:27:33,220
terminates all the all the running

00:27:28,020 --> 00:27:36,250
futures I think it's Confucius and then

00:27:33,220 --> 00:27:40,480
we just have a finally block where we

00:27:36,250 --> 00:27:43,150
just close all the handles state states

00:27:40,480 --> 00:27:52,000
save the state and clean up everything

00:27:43,150 --> 00:27:57,340
we need but also you can add a signal

00:27:52,000 --> 00:27:59,620
handler to loop a loop directly hi do

00:27:57,340 --> 00:28:02,140
you have some persistent that down a

00:27:59,620 --> 00:28:05,650
hard drive and how you do you manage its

00:28:02,140 --> 00:28:07,630
data between all the samples that you

00:28:05,650 --> 00:28:11,860
have we have it

00:28:07,630 --> 00:28:15,120
we had a persistence on file system we

00:28:11,860 --> 00:28:17,470
are right now migrating it to Redis but

00:28:15,120 --> 00:28:20,890
until now what we had to do when we

00:28:17,470 --> 00:28:22,510
migrated between hosts was that we had

00:28:20,890 --> 00:28:24,429
to shut down the service migrate the

00:28:22,510 --> 00:28:28,990
data and start the service again we have

00:28:24,429 --> 00:28:32,049
no shell storage we have time for last

00:28:28,990 --> 00:28:33,970
question at the beginning of the talk

00:28:32,049 --> 00:28:36,640
you said that you were running the

00:28:33,970 --> 00:28:39,070
problems with github runner how did you

00:28:36,640 --> 00:28:43,600
solve the problem with for a clean

00:28:39,070 --> 00:28:49,059
environment for each pipeline we have

00:28:43,600 --> 00:28:53,080
dedicated the like Dorian docker service

00:28:49,059 --> 00:28:58,480
running which which had was which hand

00:28:53,080 --> 00:29:01,980
was the pipeline but so we we have we

00:28:58,480 --> 00:29:04,670
have shared we have shared shared dint

00:29:01,980 --> 00:29:08,160
but

00:29:04,670 --> 00:29:09,980
when we are shutting down it sometimes

00:29:08,160 --> 00:29:12,840
so we clean it

00:29:09,980 --> 00:29:15,720
so basically when the attacker gets

00:29:12,840 --> 00:29:17,340
student it doesn't affect the entire

00:29:15,720 --> 00:29:19,920
house on which everything else is

00:29:17,340 --> 00:29:22,200
running so actually we have the same

00:29:19,920 --> 00:29:24,330
problem and the problem with Dean's is

00:29:22,200 --> 00:29:26,610
that you need to have a privileged

00:29:24,330 --> 00:29:29,160
container to run the tow car in docker

00:29:26,610 --> 00:29:34,980
and essentially you have a route if you

00:29:29,160 --> 00:29:42,270
if you do that so I wonder if you had

00:29:34,980 --> 00:29:45,930
another solution yeah maybe we didn't

00:29:42,270 --> 00:29:47,550
solve this the right way okay thank you

00:29:45,930 --> 00:29:49,680
yeah we are women we are planning to

00:29:47,550 --> 00:29:54,510
migrate it to different host to separate

00:29:49,680 --> 00:29:55,140
host so maybe these all soldiers okay

00:29:54,510 --> 00:29:57,860
fantastic

00:29:55,140 --> 00:30:03,819
thank you Peter let's thanks les

00:29:57,860 --> 00:30:03,819

YouTube URL: https://www.youtube.com/watch?v=KF6lU_fm_Bg


