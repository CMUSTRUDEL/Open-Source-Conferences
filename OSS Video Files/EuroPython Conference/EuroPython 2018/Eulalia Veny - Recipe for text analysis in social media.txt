Title: Eulalia Veny - Recipe for text analysis in social media
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Recipe for text analysis in social media:
[EuroPython 2018 - Talk - 2018-07-25 - PyCharm [PyData]]
[Edinburgh, UK]

By Eulalia Veny

The analysis of text data in social media is gaining more and more importance every day. The need for companies to know what people think and want is key to invest money in providing customers what they want. The first approach to text analysis was mainly statistical, but adding linguistic information has been proven to work well for improving the results.

One of the problems that you need to address when analyzing social media is time. People are constantly exchanging information, users write comments every day about what they think of a product, what they do or the places they visit. It is difficult to keep track of everything that happens. Moreover, information is sometimes expressed in short sentences, keywords, or isolated ideas, such as in Tweets. Language is usually unstructured because it is composed of isolated ideas, or without context.

I will talk about the problem of text analysis in social media. I will also explain briefly NaÃ¯ve Bayes classifiers, and how you can easily take advantage of them to analyse sentiment in social media, and I will use an example to show how linguistic information can help improve the results. I will also evaluate the pros and cons of supervised vs unsupervised learning.

Finally, I will introduce opinion lexicons, both dictionary based and corpus-based, and how lexicons can be used in semi-supervised learning and supervised learning. If I have time left, I will explain about other use cases of text analysis.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,000 --> 00:00:11,940
oh well hello before hello everybody

00:00:09,360 --> 00:00:15,599
before I start I want you to know that

00:00:11,940 --> 00:00:18,000
this is not a technical talk I'm a

00:00:15,599 --> 00:00:22,619
computational linguist so I prepare the

00:00:18,000 --> 00:00:25,230
data I pre-process the data for modeling

00:00:22,619 --> 00:00:28,470
later so this is a practical approach on

00:00:25,230 --> 00:00:31,140
how to how to prepare and how to

00:00:28,470 --> 00:00:35,100
pre-process data in order to be able to

00:00:31,140 --> 00:00:38,700
use it when modeling so just for you to

00:00:35,100 --> 00:00:43,469
know I'm a computational linguist I she

00:00:38,700 --> 00:00:46,379
said well you need first for this recipe

00:00:43,469 --> 00:00:49,350
you need to gather the corpus then you

00:00:46,379 --> 00:00:53,430
creep process it then you do the text

00:00:49,350 --> 00:00:55,530
modeling and then I'm going to tell you

00:00:53,430 --> 00:01:00,590
about the pros and cons of supervised

00:00:55,530 --> 00:01:03,120
learning so I'm going to talk about

00:01:00,590 --> 00:01:05,640
pre-processing in social media

00:01:03,120 --> 00:01:09,140
pre-processing texts so we need to know

00:01:05,640 --> 00:01:14,010
first what social media is and basically

00:01:09,140 --> 00:01:15,869
therefore four things that you need to

00:01:14,010 --> 00:01:19,500
take into account first it needs to be

00:01:15,869 --> 00:01:24,150
web-based app also it needs to be user

00:01:19,500 --> 00:01:26,460
generated content users must be able to

00:01:24,150 --> 00:01:30,299
create profiles and to be able to

00:01:26,460 --> 00:01:33,720
connect with other users and with this

00:01:30,299 --> 00:01:36,000
we have the development of social

00:01:33,720 --> 00:01:40,079
networks which are basically social

00:01:36,000 --> 00:01:44,430
media so what is social media with the

00:01:40,079 --> 00:01:49,110
definition we had before we can think of

00:01:44,430 --> 00:01:53,009
Twitter Facebook Instagram Pinterest

00:01:49,110 --> 00:01:55,409
maybe LinkedIn but also Amazon because

00:01:53,009 --> 00:01:59,630
in Amazon you have a lot of content

00:01:55,409 --> 00:02:04,219
reviews booking the same TripAdvisor and

00:01:59,630 --> 00:02:10,229
also Wikipedia if we think as a Content

00:02:04,219 --> 00:02:12,209
site where you can share ideas so the

00:02:10,229 --> 00:02:13,170
type of content we found in social media

00:02:12,209 --> 00:02:16,430
is

00:02:13,170 --> 00:02:21,540
as in Twitter for instance or any other

00:02:16,430 --> 00:02:24,870
web beta application also Instagram with

00:02:21,540 --> 00:02:27,230
images videos in YouTube for instance or

00:02:24,870 --> 00:02:27,230
a v-mail

00:02:27,569 --> 00:02:32,430
and then the step for text analysis as I

00:02:30,090 --> 00:02:36,180
said before I are those there and we

00:02:32,430 --> 00:02:41,610
start with gathering the corpus together

00:02:36,180 --> 00:02:45,000
the corpus sorry you can take corpora

00:02:41,610 --> 00:02:47,580
from online free sources or from web

00:02:45,000 --> 00:02:50,940
scrapping those are the two main basic

00:02:47,580 --> 00:02:53,670
places where you can find data so online

00:02:50,940 --> 00:02:57,090
free corpora you have the use disk opera

00:02:53,670 --> 00:03:04,860
there and for instance you have the

00:02:57,090 --> 00:03:10,290
first one so you can type input and ltk

00:03:04,860 --> 00:03:13,130
I know no because it's a video but you

00:03:10,290 --> 00:03:17,250
don't need I'm not writing any code you

00:03:13,130 --> 00:03:20,989
you write an ulti KDOT download and then

00:03:17,250 --> 00:03:24,060
you get these user graphic interface

00:03:20,989 --> 00:03:26,820
where you can download all the data okay

00:03:24,060 --> 00:03:27,390
it's just so that you know the the tools

00:03:26,820 --> 00:03:30,900
that you have

00:03:27,390 --> 00:03:32,670
I'm not coding but if you if you have a

00:03:30,900 --> 00:03:35,160
look at this you can download a lot of

00:03:32,670 --> 00:03:38,519
packages which are really useful when

00:03:35,160 --> 00:03:40,200
starting especially doing NLP based

00:03:38,519 --> 00:03:42,930
applications because they're really

00:03:40,200 --> 00:03:46,560
useful because they have labels they

00:03:42,930 --> 00:03:51,680
have a lot of resources that are really

00:03:46,560 --> 00:03:54,660
useful for starters also the big and

00:03:51,680 --> 00:03:57,510
Young University they have a really good

00:03:54,660 --> 00:04:00,570
corpora in English also in Spanish and

00:03:57,510 --> 00:04:04,530
Portuguese but any other languages are

00:04:00,570 --> 00:04:08,579
difficult to find the British national

00:04:04,530 --> 00:04:12,359
corpus you can access the corpora or you

00:04:08,579 --> 00:04:15,840
can download it and there's this guy

00:04:12,359 --> 00:04:19,380
called Martin Weiser and he also has a

00:04:15,840 --> 00:04:22,770
really good corpora of online English

00:04:19,380 --> 00:04:25,020
and he's got really good resources there

00:04:22,770 --> 00:04:26,210
too and then you can do the web

00:04:25,020 --> 00:04:28,039
scrapping

00:04:26,210 --> 00:04:30,830
of social media and information

00:04:28,039 --> 00:04:32,840
resources social media you know that is

00:04:30,830 --> 00:04:34,819
Facebook Twitter whatever and then the

00:04:32,840 --> 00:04:38,720
information resources in my case for

00:04:34,819 --> 00:04:41,930
instance I did a script that was able

00:04:38,720 --> 00:04:45,740
able to retrieve information from the

00:04:41,930 --> 00:04:47,990
Spanish academia web page and it was

00:04:45,740 --> 00:04:51,860
useful because I needed to check whether

00:04:47,990 --> 00:04:54,169
a list of words where do really exist or

00:04:51,860 --> 00:04:55,970
not in the Spanish academia and I could

00:04:54,169 --> 00:05:00,289
do it automatically so it was really

00:04:55,970 --> 00:05:04,610
helpful so the kind of text we find in

00:05:00,289 --> 00:05:07,039
social media are the ones that are a

00:05:04,610 --> 00:05:10,880
tricky ones for some reasons I'll tell

00:05:07,039 --> 00:05:13,669
you later but it determines the way

00:05:10,880 --> 00:05:17,180
we're going to analyze the data so we

00:05:13,669 --> 00:05:20,810
have posts we have tweets we have those

00:05:17,180 --> 00:05:24,500
tags and comments in the post I mean you

00:05:20,810 --> 00:05:26,330
see there's a lot of comments and in the

00:05:24,500 --> 00:05:28,159
tweets you can also have the hashtags

00:05:26,330 --> 00:05:30,620
all these information are really

00:05:28,159 --> 00:05:34,610
valuable information for text analysis

00:05:30,620 --> 00:05:36,500
because they will all those tags and

00:05:34,610 --> 00:05:38,690
comments and whatever are going to be

00:05:36,500 --> 00:05:40,460
really helpful when organizing and

00:05:38,690 --> 00:05:43,789
classifying texts and all these tasks

00:05:40,460 --> 00:05:45,650
will be willing to perform so now we now

00:05:43,789 --> 00:05:49,310
we have the corpus and we need to go to

00:05:45,650 --> 00:05:51,080
pre-processing when pre-processing you

00:05:49,310 --> 00:05:53,449
can do a lot of tasks but I'm going to

00:05:51,080 --> 00:05:56,210
explain those three because they are the

00:05:53,449 --> 00:06:01,219
main important one and also the most

00:05:56,210 --> 00:06:04,909
useful ones tokenization is separating a

00:06:01,219 --> 00:06:07,250
text into smaller units so you can

00:06:04,909 --> 00:06:09,259
separate into sentences or words or

00:06:07,250 --> 00:06:12,860
whatever unit you need and you might

00:06:09,259 --> 00:06:15,919
think this is very easy apparently but

00:06:12,860 --> 00:06:18,380
you might find some examples like those

00:06:15,919 --> 00:06:19,820
like city of Bombay you don't you have

00:06:18,380 --> 00:06:22,759
to decide whether you want to keep this

00:06:19,820 --> 00:06:26,270
as a whole unit or you want to separate

00:06:22,759 --> 00:06:28,430
each word as a separate unit so this is

00:06:26,270 --> 00:06:32,270
a lot of work you need to think about

00:06:28,430 --> 00:06:35,060
before you do that the processing also

00:06:32,270 --> 00:06:37,580
you will have problems such as X

00:06:35,060 --> 00:06:39,590
Malaysian Prime Minister you need to

00:06:37,580 --> 00:06:42,590
decide whether you want to keep those

00:06:39,590 --> 00:06:47,990
you need us one unit or separate ones so

00:06:42,590 --> 00:06:49,700
this is it also want and and there's you

00:06:47,990 --> 00:06:53,270
need to decide whether you want to keep

00:06:49,700 --> 00:06:56,150
the verb and negative form or you want

00:06:53,270 --> 00:06:58,820
or you want to separate them because

00:06:56,150 --> 00:07:03,170
especially the negatives are tricky when

00:06:58,820 --> 00:07:04,660
sentiment analysis text analysis so all

00:07:03,170 --> 00:07:07,220
of these you need to think about

00:07:04,660 --> 00:07:12,200
beforehand so that you can have the

00:07:07,220 --> 00:07:14,300
proper data you need also it depends on

00:07:12,200 --> 00:07:16,910
the language it's not only based on the

00:07:14,300 --> 00:07:19,700
text itself but the language you see

00:07:16,910 --> 00:07:21,950
Japanese and Chinese used to write

00:07:19,700 --> 00:07:26,050
everything together so they you cannot

00:07:21,950 --> 00:07:29,960
use spaces as word breakers and

00:07:26,050 --> 00:07:32,720
specially Japanese has also those four

00:07:29,960 --> 00:07:35,840
alphabets so it's not that easy to

00:07:32,720 --> 00:07:39,740
decide which one of the tokens are you

00:07:35,840 --> 00:07:43,310
going to separate as the word removal is

00:07:39,740 --> 00:07:46,610
pretty good also for removing words that

00:07:43,310 --> 00:07:51,950
are meaningless in the case of pronouns

00:07:46,610 --> 00:07:54,770
or determiners possessives yeah I know

00:07:51,950 --> 00:07:56,300
you can read those but it's basically a

00:07:54,770 --> 00:07:59,720
list of words you want to remove from

00:07:56,300 --> 00:08:03,050
your text because it will be noise in

00:07:59,720 --> 00:08:05,030
your algorithms in your models so it's

00:08:03,050 --> 00:08:08,450
very useful to have a list of words you

00:08:05,030 --> 00:08:12,140
don't need and finally a limitation and

00:08:08,450 --> 00:08:16,880
stemming are useful because you can

00:08:12,140 --> 00:08:20,270
group lemmas bay better than if you have

00:08:16,880 --> 00:08:22,490
all these inflectional endings that are

00:08:20,270 --> 00:08:24,470
also noise so for instance limit

00:08:22,490 --> 00:08:26,720
realization is to remove inflectional

00:08:24,470 --> 00:08:29,570
endings but coming back to the lemma

00:08:26,720 --> 00:08:31,760
the original lemma so in this case you

00:08:29,570 --> 00:08:36,500
will basically need a dictionary or a

00:08:31,760 --> 00:08:40,400
set of rules that help you to go back

00:08:36,500 --> 00:08:41,900
from the inflectional ending i mean the

00:08:40,400 --> 00:08:44,750
word with infraction aligning for

00:08:41,900 --> 00:08:47,120
instance in smiling to smile so you need

00:08:44,750 --> 00:08:50,620
all the set of rules or the dictionary

00:08:47,120 --> 00:08:53,200
to go back to the lemma which is smile

00:08:50,620 --> 00:08:56,620
stemming is easier because you

00:08:53,200 --> 00:08:59,740
you just chopped off word endings so you

00:08:56,620 --> 00:09:02,140
don't need to do you don't need the

00:08:59,740 --> 00:09:04,480
dictionary you can only top off the

00:09:02,140 --> 00:09:08,290
endings have a small list of endings and

00:09:04,480 --> 00:09:10,600
then you chop up and it's over so what

00:09:08,290 --> 00:09:13,240
are the problems we find in texts within

00:09:10,600 --> 00:09:15,340
social media basically the mess and the

00:09:13,240 --> 00:09:18,910
most important one is time sensitivity

00:09:15,340 --> 00:09:21,940
as you know Twitter Facebook and Amazon

00:09:18,910 --> 00:09:24,070
or Amazon reviews I mean and all of

00:09:21,940 --> 00:09:27,070
these sites have content which is

00:09:24,070 --> 00:09:29,530
dynamic it means that it's constantly

00:09:27,070 --> 00:09:31,780
changing people are constantly creating

00:09:29,530 --> 00:09:36,520
new content and it's really difficult to

00:09:31,780 --> 00:09:39,910
build a model because you cannot decide

00:09:36,520 --> 00:09:41,800
on a set of parameters and expect it to

00:09:39,910 --> 00:09:42,430
work because parameters are constantly

00:09:41,800 --> 00:09:45,820
changing

00:09:42,430 --> 00:09:48,970
so this is a the main problem we have in

00:09:45,820 --> 00:09:52,990
in social social media when analyzing

00:09:48,970 --> 00:09:55,240
the text also the short links if you try

00:09:52,990 --> 00:09:58,350
to analyze Twitter

00:09:55,240 --> 00:10:01,660
you'll see that you have for instance I

00:09:58,350 --> 00:10:03,490
don't know Superman and Clark Kent and

00:10:01,660 --> 00:10:05,500
you know that they are the same person

00:10:03,490 --> 00:10:07,690
but you might have a Twitter on someone

00:10:05,500 --> 00:10:11,530
on one name and a Twitter about the

00:10:07,690 --> 00:10:14,650
other name so when you come to cluster

00:10:11,530 --> 00:10:16,390
those words in into different groups

00:10:14,650 --> 00:10:18,010
you'll see that they might not be

00:10:16,390 --> 00:10:20,260
related in the same cluster because

00:10:18,010 --> 00:10:22,120
there's no contextual information that

00:10:20,260 --> 00:10:26,400
is telling you they are the same person

00:10:22,120 --> 00:10:29,680
so it's difficult the this is a problem

00:10:26,400 --> 00:10:31,690
for text analysis and this brings you to

00:10:29,680 --> 00:10:34,330
the semantic gap which is exactly what I

00:10:31,690 --> 00:10:37,150
just explained also the problem of

00:10:34,330 --> 00:10:39,640
unstructured data there's two Brian's

00:10:37,150 --> 00:10:41,290
first the varieties of tech of content

00:10:39,640 --> 00:10:43,300
quality you see there's people that

00:10:41,290 --> 00:10:45,180
write really good in a really polite

00:10:43,300 --> 00:10:49,060
manner but then there's people who write

00:10:45,180 --> 00:10:51,250
the way words come out of their mind

00:10:49,060 --> 00:10:53,970
and they don't try to make sense of the

00:10:51,250 --> 00:10:57,610
sentences so this is a problem and also

00:10:53,970 --> 00:11:00,910
acronyms abbreviations like you or two

00:10:57,610 --> 00:11:03,370
in this case or the aunt and the miss

00:11:00,910 --> 00:11:05,920
the misspellings like where is oh is a

00:11:03,370 --> 00:11:06,880
word but how do you distinguish where

00:11:05,920 --> 00:11:10,180
from we are

00:11:06,880 --> 00:11:13,930
so all of these problems you'll find in

00:11:10,180 --> 00:11:16,300
text analysis a lot and also you have

00:11:13,930 --> 00:11:18,699
abundant information there's tones of

00:11:16,300 --> 00:11:22,600
data and you need to cut somewhere in

00:11:18,699 --> 00:11:25,810
order to be able to process so

00:11:22,600 --> 00:11:29,620
applications in the real world you can

00:11:25,810 --> 00:11:33,880
use all those strategies for event

00:11:29,620 --> 00:11:38,410
detection for instance you know what is

00:11:33,880 --> 00:11:42,040
a new which is very famous or which is

00:11:38,410 --> 00:11:44,319
popular or to predict what kind of

00:11:42,040 --> 00:11:46,779
information is going to be fashion in

00:11:44,319 --> 00:11:49,240
the next week or whatever also you can

00:11:46,779 --> 00:11:52,660
take advantage of collaborative question

00:11:49,240 --> 00:11:54,759
and answering in sites like Stack

00:11:52,660 --> 00:11:58,600
Overflow for instance if you scrap the

00:11:54,759 --> 00:12:01,029
web you will be able to find really

00:11:58,600 --> 00:12:04,139
important information and a specific one

00:12:01,029 --> 00:12:08,860
rather than if you google your search

00:12:04,139 --> 00:12:10,509
also you can use Wikipedia to fill in

00:12:08,860 --> 00:12:13,959
the semantic gap I was talking about

00:12:10,509 --> 00:12:17,880
before if you use Wikipedia which is a

00:12:13,959 --> 00:12:21,189
trustful resource you might be able to

00:12:17,880 --> 00:12:24,399
to create somehow relations between

00:12:21,189 --> 00:12:26,649
those words that are apparently non

00:12:24,399 --> 00:12:29,949
related but with Wikipedia you find

00:12:26,649 --> 00:12:32,050
relations of the two it's also useful

00:12:29,949 --> 00:12:34,000
for sentiment analysis I will put an

00:12:32,050 --> 00:12:37,410
example later and also to identify

00:12:34,000 --> 00:12:41,050
influencers and see if they are a lot of

00:12:37,410 --> 00:12:43,449
mentions of that person or also for a

00:12:41,050 --> 00:12:45,579
wheat quality prediction like in Amazon

00:12:43,449 --> 00:12:48,810
you might want to know if a review is

00:12:45,579 --> 00:12:52,810
faithful or not and it's very useful

00:12:48,810 --> 00:12:57,069
this kind of tasks you can perform with

00:12:52,810 --> 00:13:02,050
NLP to decide whether to trust that user

00:12:57,069 --> 00:13:05,050
or not so now we're going to go through

00:13:02,050 --> 00:13:08,759
text modeling I'm gonna go through it

00:13:05,050 --> 00:13:11,410
very quick because I'm not an expert but

00:13:08,759 --> 00:13:15,519
with this pre-processing we did we

00:13:11,410 --> 00:13:18,350
should be able to come up with a proper

00:13:15,519 --> 00:13:21,280
way to I mean proper

00:13:18,350 --> 00:13:23,990
data set to be able to perform this so

00:13:21,280 --> 00:13:26,720
if you go to Amazon and you find those

00:13:23,990 --> 00:13:29,330
reviews you might want to separate well

00:13:26,720 --> 00:13:32,060
they already do but if you have your own

00:13:29,330 --> 00:13:37,190
page you might want to separate between

00:13:32,060 --> 00:13:41,090
positive and negative comments so to

00:13:37,190 --> 00:13:45,290
find positive and negative comments you

00:13:41,090 --> 00:13:49,100
need here

00:13:45,290 --> 00:13:52,880
you need a vocabulary of positive words

00:13:49,100 --> 00:13:56,660
negative words and neutral words okay so

00:13:52,880 --> 00:13:59,540
if we go back to these you'll first need

00:13:56,660 --> 00:14:02,300
to define the task you want maybe you

00:13:59,540 --> 00:14:03,650
want to group the words in clusters and

00:14:02,300 --> 00:14:06,830
you need to decide which kind of

00:14:03,650 --> 00:14:08,930
clusters you want so once we have this

00:14:06,830 --> 00:14:11,120
task defined we need to decide the

00:14:08,930 --> 00:14:12,860
strategy with strategy we're going to

00:14:11,120 --> 00:14:14,630
use for sentiment recognition

00:14:12,860 --> 00:14:16,570
there's mainly two ways they're

00:14:14,630 --> 00:14:20,300
supervised and unsupervised

00:14:16,570 --> 00:14:23,900
but unsupervised learning in this case

00:14:20,300 --> 00:14:25,790
it's really really difficult so the

00:14:23,900 --> 00:14:29,920
supervised learning you will need a

00:14:25,790 --> 00:14:33,050
labeled corpora which is time consuming

00:14:29,920 --> 00:14:34,790
predefined categories and you need to go

00:14:33,050 --> 00:14:36,830
all through the data before you decide

00:14:34,790 --> 00:14:39,890
on the categories and you can use

00:14:36,830 --> 00:14:42,590
sentiment lexicons for unsupervised

00:14:39,890 --> 00:14:47,420
learning you will use an unlabeled Cobra

00:14:42,590 --> 00:14:49,580
which is easy you can use k-means for

00:14:47,420 --> 00:14:52,070
category discovery which is also useful

00:14:49,580 --> 00:14:54,230
useful but you'll have the problem I

00:14:52,070 --> 00:14:59,030
said before with the time because it's

00:14:54,230 --> 00:15:03,470
constantly changing so you never know if

00:14:59,030 --> 00:15:05,480
your model will be properly good as I

00:15:03,470 --> 00:15:09,290
said before we need a list of vocabulary

00:15:05,480 --> 00:15:13,280
for positive negative and neutral and

00:15:09,290 --> 00:15:16,280
also you need a list of comments you

00:15:13,280 --> 00:15:20,120
want to analyze the result this is a

00:15:16,280 --> 00:15:23,570
very very basic algorithm for this night

00:15:20,120 --> 00:15:24,160
base but I mean you can use any one you

00:15:23,570 --> 00:15:28,190
want

00:15:24,160 --> 00:15:30,769
the the input will be mostly always the

00:15:28,190 --> 00:15:34,579
same so the result you'll get

00:15:30,769 --> 00:15:38,079
is generally a positive score and a

00:15:34,579 --> 00:15:41,449
negative score and then you can classify

00:15:38,079 --> 00:15:49,519
the text depending on on the result you

00:15:41,449 --> 00:15:52,069
get it is a very basic task in my

00:15:49,519 --> 00:15:55,279
opinion the best way is using sentiment

00:15:52,069 --> 00:15:57,699
lexicons because you have a word list of

00:15:55,279 --> 00:16:03,079
positive and a word list of negative

00:15:57,699 --> 00:16:07,689
words and it's a binary fashion

00:16:03,079 --> 00:16:12,829
so it's an easy text classification task

00:16:07,689 --> 00:16:15,799
the problem is well sorry these are two

00:16:12,829 --> 00:16:19,869
really good ones also you have work net

00:16:15,799 --> 00:16:23,239
for NL from NLP and you can download

00:16:19,869 --> 00:16:25,279
word net in in Python and it's very

00:16:23,239 --> 00:16:28,069
useful I'll add the link after if you

00:16:25,279 --> 00:16:30,549
want but those are really simple ones

00:16:28,069 --> 00:16:36,429
and they come with this binary fashion

00:16:30,549 --> 00:16:36,429
already classified so it's pretty useful

00:16:36,850 --> 00:16:41,269
another approach which I thought it was

00:16:39,679 --> 00:16:44,360
really interesting although it's not

00:16:41,269 --> 00:16:46,579
that it's not that new but it's a really

00:16:44,360 --> 00:16:51,310
interesting approach are polarity

00:16:46,579 --> 00:16:52,660
lexicons well those guys guys did is

00:16:51,310 --> 00:16:55,670
[Music]

00:16:52,660 --> 00:16:58,549
they've had a small list of positive

00:16:55,670 --> 00:17:02,600
adjectives and and they thought that

00:16:58,549 --> 00:17:05,559
every adjective connected with that

00:17:02,600 --> 00:17:12,760
adjective in the list by ant which is

00:17:05,559 --> 00:17:17,230
coordination will be necessarily synonym

00:17:12,760 --> 00:17:22,220
so they decided to scrap the web and

00:17:17,230 --> 00:17:24,470
find well more manually but they they

00:17:22,220 --> 00:17:29,110
found a lot of pairs of words connected

00:17:24,470 --> 00:17:32,750
by end and so all the words were added

00:17:29,110 --> 00:17:35,299
automatically to their already at least

00:17:32,750 --> 00:17:37,549
already built list of positive words

00:17:35,299 --> 00:17:39,919
they did the same with negative words

00:17:37,549 --> 00:17:43,330
with the current day with the word but

00:17:39,919 --> 00:17:49,179
or however so they could

00:17:43,330 --> 00:17:51,909
automatically have a bigger list of

00:17:49,179 --> 00:17:54,809
negative words so this is a really good

00:17:51,909 --> 00:17:58,510
semi-supervised approach for this task

00:17:54,809 --> 00:18:02,919
because you get to to have the best of

00:17:58,510 --> 00:18:06,070
both like the unsupervised is less

00:18:02,919 --> 00:18:08,649
time-consuming and it's easier this is

00:18:06,070 --> 00:18:11,620
an example from dan jurafsky stanford

00:18:08,649 --> 00:18:14,320
NLP course this is a really good book

00:18:11,620 --> 00:18:16,450
also they have a book that they are

00:18:14,320 --> 00:18:21,490
editing right now the third edition I

00:18:16,450 --> 00:18:23,710
guess is is going to be ready in a few

00:18:21,490 --> 00:18:27,549
months and this is a really good course

00:18:23,710 --> 00:18:28,870
also for a lot of NLP tasks and this is

00:18:27,549 --> 00:18:37,330
a really good approach

00:18:28,870 --> 00:18:40,389
so far I've I've seen so far so the

00:18:37,330 --> 00:18:44,740
results are basically depending on the

00:18:40,389 --> 00:18:50,019
task you want to perform as I said

00:18:44,740 --> 00:18:52,450
before having tasks in NLP depends a lot

00:18:50,019 --> 00:18:55,029
on the text you have I mean if you are

00:18:52,450 --> 00:18:57,039
analyzing Twitter you might need some

00:18:55,029 --> 00:18:59,350
strategies really different from the

00:18:57,039 --> 00:19:01,450
strategies you're going to use if you

00:18:59,350 --> 00:19:05,470
are analyzing I don't know text in a

00:19:01,450 --> 00:19:07,480
novel so this is a very important thing

00:19:05,470 --> 00:19:09,610
you have to bear in mind because if your

00:19:07,480 --> 00:19:12,909
task is different you're going to need a

00:19:09,610 --> 00:19:16,840
completely different strategy only also

00:19:12,909 --> 00:19:19,659
a very different algorithm so kaminsky

00:19:16,840 --> 00:19:25,889
means is a good one for clustering if

00:19:19,659 --> 00:19:29,169
you have a bag of words and you need to

00:19:25,889 --> 00:19:32,049
you need to group them in clusters but

00:19:29,169 --> 00:19:37,659
if you want to do I don't know

00:19:32,049 --> 00:19:40,480
for instance decide what in inner spell

00:19:37,659 --> 00:19:44,080
corrector if you want to know if a

00:19:40,480 --> 00:19:45,309
grammar is well written or not I mean in

00:19:44,080 --> 00:19:47,380
a sentence if you want to check the

00:19:45,309 --> 00:19:48,970
grammar you might need a completely

00:19:47,380 --> 00:19:54,370
different approach and you might want to

00:19:48,970 --> 00:19:56,220
use engrams or another strategy so

00:19:54,370 --> 00:19:59,310
[Music]

00:19:56,220 --> 00:20:01,830
using the lexicons for sentiment

00:19:59,310 --> 00:20:04,530
recognition for me is the best approach

00:20:01,830 --> 00:20:07,140
I as I said before and we have pranced

00:20:04,530 --> 00:20:10,710
like the topic discovery is a challenge

00:20:07,140 --> 00:20:13,020
as and if you already have a small set

00:20:10,710 --> 00:20:17,160
of words it's really helpful although

00:20:13,020 --> 00:20:20,940
you can increase the amount of words

00:20:17,160 --> 00:20:23,400
automatically somehow and also the

00:20:20,940 --> 00:20:25,410
performance is better almost always I

00:20:23,400 --> 00:20:26,310
mean all the cases I've seen always the

00:20:25,410 --> 00:20:32,220
performance is better

00:20:26,310 --> 00:20:36,260
on the contrary dynamic language is

00:20:32,220 --> 00:20:36,260
difficult so it's really time consuming

00:20:36,350 --> 00:20:48,060
building the lexicons and labeling the

00:20:39,060 --> 00:20:51,390
corpora so well it depends on how if you

00:20:48,060 --> 00:20:53,340
need to have a task finished in a small

00:20:51,390 --> 00:20:58,110
amount of time or you have more time you

00:20:53,340 --> 00:21:00,690
can use one strategy or another one so

00:20:58,110 --> 00:21:02,700
this is the bibliography I'm going to

00:21:00,690 --> 00:21:05,700
upload this lights in case you want to

00:21:02,700 --> 00:21:09,450
check the mining text data is a really

00:21:05,700 --> 00:21:13,320
really good book they have a lot of

00:21:09,450 --> 00:21:18,570
algorithms explained for each kind of

00:21:13,320 --> 00:21:19,050
task so it's very useful and just to

00:21:18,570 --> 00:21:21,630
finish

00:21:19,050 --> 00:21:23,670
I'm from Majorca iMac organizer of the

00:21:21,630 --> 00:21:24,720
pied area and I'm a computational

00:21:23,670 --> 00:21:27,300
linguist

00:21:24,720 --> 00:21:29,220
I couldn't came up in the whole web with

00:21:27,300 --> 00:21:32,490
the proper definition of what a

00:21:29,220 --> 00:21:34,320
computational way stick is so I found

00:21:32,490 --> 00:21:37,170
this light which i think is awesome and

00:21:34,320 --> 00:21:38,700
I decided to cop it here I also have the

00:21:37,170 --> 00:21:42,930
information there it's from a

00:21:38,700 --> 00:21:44,570
presentation and well thank you very

00:21:42,930 --> 00:21:45,480
much for watching

00:21:44,570 --> 00:21:48,549
[Applause]

00:21:45,480 --> 00:21:48,549
[Music]

00:21:51,549 --> 00:22:00,320
we have a few minutes for questions how

00:21:58,220 --> 00:22:01,820
do you determine your stop words is

00:22:00,320 --> 00:22:03,950
there a common set of words that tend to

00:22:01,820 --> 00:22:08,480
work or is it based on your well

00:22:03,950 --> 00:22:11,720
basically anything without lexical

00:22:08,480 --> 00:22:16,909
information would do like for instance

00:22:11,720 --> 00:22:19,610
if you have the word house you know that

00:22:16,909 --> 00:22:22,070
it might be necessary to have the

00:22:19,610 --> 00:22:25,570
information of house because you can

00:22:22,070 --> 00:22:28,340
find synonyms I don't know you can find

00:22:25,570 --> 00:22:31,669
words that are related such as I know

00:22:28,340 --> 00:22:35,960
the roof or whatever but if you have for

00:22:31,669 --> 00:22:38,720
instance the determiner it's it has no

00:22:35,960 --> 00:22:41,450
semantic meaning which means that you

00:22:38,720 --> 00:22:45,019
cannot find relative words

00:22:41,450 --> 00:22:47,269
I mean related words sorry so basically

00:22:45,019 --> 00:22:50,690
this is what you take into account when

00:22:47,269 --> 00:22:52,940
listing stop words also it depends on

00:22:50,690 --> 00:22:55,309
your task you might wanna I don't know

00:22:52,940 --> 00:22:57,980
decide that house in your case makes no

00:22:55,309 --> 00:23:01,879
sense in your backup words so you won't

00:22:57,980 --> 00:23:06,169
use it so there are lists of sub words

00:23:01,879 --> 00:23:08,570
which are already built and you can I

00:23:06,169 --> 00:23:11,870
know TK for instance the package has

00:23:08,570 --> 00:23:14,330
some and I know many packages have lists

00:23:11,870 --> 00:23:16,370
but basically this is the main principle

00:23:14,330 --> 00:23:23,029
of when deciding whether word it is

00:23:16,370 --> 00:23:25,250
establish the word or not know you

00:23:23,029 --> 00:23:29,590
tokenize first and then you remove

00:23:25,250 --> 00:23:29,590
whatever you don't like usually thanks

00:23:29,649 --> 00:23:36,500
for the talk I was wondering how you

00:23:33,200 --> 00:23:38,059
apply k-means to a bag of words like

00:23:36,500 --> 00:23:41,870
this there must be a step in between

00:23:38,059 --> 00:23:44,509
right well as I said before I don't do a

00:23:41,870 --> 00:23:49,070
lot of modeling myself but I usually

00:23:44,509 --> 00:23:52,419
prepare the the data so I know the the

00:23:49,070 --> 00:23:56,509
problem in that case with k-means were

00:23:52,419 --> 00:23:59,580
in inmate the class the clusters it were

00:23:56,509 --> 00:24:02,489
that resulted from the first

00:23:59,580 --> 00:24:04,860
tasks made no sense at all because as I

00:24:02,489 --> 00:24:06,570
said in the example you might you might

00:24:04,860 --> 00:24:09,239
have two words that are related because

00:24:06,570 --> 00:24:11,489
you know it but when it comes to

00:24:09,239 --> 00:24:17,600
clustering somehow the algorithm doesn't

00:24:11,489 --> 00:24:23,759
find similarities so I my job was to

00:24:17,600 --> 00:24:25,470
decide how to improve the data so that

00:24:23,759 --> 00:24:28,049
the algorithm was better I don't know

00:24:25,470 --> 00:24:31,859
exactly how they did implement the

00:24:28,049 --> 00:24:33,690
algorithm itself okay so I'm not so much

00:24:31,859 --> 00:24:36,269
talking about k-means or anything it's

00:24:33,690 --> 00:24:38,629
more like how you represent the data to

00:24:36,269 --> 00:24:41,999
an algorithm like k-means because you

00:24:38,629 --> 00:24:45,269
like usually you have a set of vectors

00:24:41,999 --> 00:24:47,279
and then you compare the vectors it just

00:24:45,269 --> 00:24:49,379
makes sense to you and it's it's

00:24:47,279 --> 00:24:52,049
difficult like you need to encode the

00:24:49,379 --> 00:24:56,519
word into something that is meaningful

00:24:52,049 --> 00:25:00,450
for a machine right yeah but yeah I

00:24:56,519 --> 00:25:02,190
understand but I don't code D algorithm

00:25:00,450 --> 00:25:04,649
so I don't know if they did a step in

00:25:02,190 --> 00:25:09,059
the in the middle I mean I know I have

00:25:04,649 --> 00:25:13,919
the I have a set of data a list of words

00:25:09,059 --> 00:25:16,919
in that case and I had the results and I

00:25:13,919 --> 00:25:19,919
saw that the clusters made no sense and

00:25:16,919 --> 00:25:23,039
I had to come up with a solution on how

00:25:19,919 --> 00:25:25,649
to improve those clusters so from a

00:25:23,039 --> 00:25:28,830
linguistic point of view I saw that

00:25:25,649 --> 00:25:33,239
words that should be related were not

00:25:28,830 --> 00:25:36,559
related at all so I started reading a

00:25:33,239 --> 00:25:39,509
couple of articles and I found out that

00:25:36,559 --> 00:25:42,629
the problem with k-means for instance is

00:25:39,509 --> 00:25:46,109
that you need somehow information that

00:25:42,629 --> 00:25:49,379
relates the words and this was missing

00:25:46,109 --> 00:25:53,879
so this is why I was saying that if you

00:25:49,379 --> 00:25:56,039
use another resource like Wikipedia you

00:25:53,879 --> 00:25:59,100
can somehow find relations between the

00:25:56,039 --> 00:26:03,230
words but I'm not coding the k-means

00:25:59,100 --> 00:26:06,980
algorithm in this case so thank you

00:26:03,230 --> 00:26:06,980
another question

00:26:15,600 --> 00:26:22,470
hi that's the talk so these days on

00:26:19,590 --> 00:26:25,710
social media especially if I'm trying to

00:26:22,470 --> 00:26:30,150
measure sentiment what I see is a lot of

00:26:25,710 --> 00:26:33,870
people respond or view with emojis or

00:26:30,150 --> 00:26:36,330
they would use sarcasm or they would

00:26:33,870 --> 00:26:37,950
just react with like a Jif of something

00:26:36,330 --> 00:26:41,100
that they're feeling so when you're

00:26:37,950 --> 00:26:42,480
preparing data to to measure sentiment

00:26:41,100 --> 00:26:47,220
for example do you take these things

00:26:42,480 --> 00:26:52,920
into account or not yeah of course it's

00:26:47,220 --> 00:26:55,470
it's really difficult it is I mean most

00:26:52,920 --> 00:26:58,650
of the times you want to remove all the

00:26:55,470 --> 00:27:01,650
noise you can so when it comes to

00:26:58,650 --> 00:27:04,980
sarcasm and you find that some words are

00:27:01,650 --> 00:27:08,160
really not helping and our noise you

00:27:04,980 --> 00:27:12,920
basically remove them all so you can try

00:27:08,160 --> 00:27:14,970
to do another analysis which is deeper

00:27:12,920 --> 00:27:18,750
linguistically I mean and you try to

00:27:14,970 --> 00:27:20,640
label everything and with this you

00:27:18,750 --> 00:27:25,290
improve a lot but it takes a lot of time

00:27:20,640 --> 00:27:26,940
it's time-consuming and and also you

00:27:25,290 --> 00:27:28,530
need a lot of people working in that you

00:27:26,940 --> 00:27:31,950
need a lot of people labeling the data

00:27:28,530 --> 00:27:34,950
and helping with the task like you have

00:27:31,950 --> 00:27:37,080
stop list of stop words there is no such

00:27:34,950 --> 00:27:39,860
resource out there where you can say

00:27:37,080 --> 00:27:46,260
okay this emoji reflects this emotion no

00:27:39,860 --> 00:27:50,550
no thank you well not that I know but so

00:27:46,260 --> 00:27:53,790
far I have found a list so you imagine

00:27:50,550 --> 00:27:57,780
one of the difficulty is to decide if

00:27:53,790 --> 00:28:00,900
you split the combination of words so in

00:27:57,780 --> 00:28:04,200
your practical practical like experience

00:28:00,900 --> 00:28:06,390
what's kind of like metrics you make you

00:28:04,200 --> 00:28:10,040
use to determine if you need to split or

00:28:06,390 --> 00:28:15,720
not you did manually or you use a rules

00:28:10,040 --> 00:28:18,990
like Oh what kind of rules well you can

00:28:15,720 --> 00:28:22,250
you can use I mean of course you can use

00:28:18,990 --> 00:28:25,620
algorithms and try to integrate

00:28:22,250 --> 00:28:27,390
automatically and I mean you can get

00:28:25,620 --> 00:28:29,070
good results but if you want to be

00:28:27,390 --> 00:28:32,370
really really

00:28:29,070 --> 00:28:35,990
specific and make sure that all the

00:28:32,370 --> 00:28:39,110
words are the way you want them to be

00:28:35,990 --> 00:28:44,070
you usually can build a dictionary or

00:28:39,110 --> 00:28:47,520
you can use how do you determine if you

00:28:44,070 --> 00:28:49,460
want to split or not this is depending

00:28:47,520 --> 00:28:56,100
on what you want to do I mean the task

00:28:49,460 --> 00:28:59,970
if you want to I don't know if you need

00:28:56,100 --> 00:29:04,380
the names of people you might wanna

00:28:59,970 --> 00:29:07,530
focus on that task specifically and then

00:29:04,380 --> 00:29:11,790
you perform that task better and then

00:29:07,530 --> 00:29:16,290
you find I mean I usually work with

00:29:11,790 --> 00:29:18,870
lists because the tasks I did was I mean

00:29:16,290 --> 00:29:23,550
you needed a specific words to be found

00:29:18,870 --> 00:29:27,090
and without no other interference or so

00:29:23,550 --> 00:29:30,720
but I mean I don't like doing it this

00:29:27,090 --> 00:29:33,750
way you know there's many ways you can

00:29:30,720 --> 00:29:39,180
do it better I mean I'm a linguist and

00:29:33,750 --> 00:29:45,360
my and my job is to solve the problems

00:29:39,180 --> 00:29:48,330
that the algorithms cannot solve so my

00:29:45,360 --> 00:29:51,450
job is not so good as I mean it's not

00:29:48,330 --> 00:29:53,460
that cool as writing code and everything

00:29:51,450 --> 00:29:56,010
works perfectly I need to find the box

00:29:53,460 --> 00:29:58,680
and resolve them which is the problem

00:29:56,010 --> 00:30:03,900
with the algorithms which are obviously

00:29:58,680 --> 00:30:07,140
much fun but I work with work with lists

00:30:03,900 --> 00:30:14,970
or with rules like regular expressions

00:30:07,140 --> 00:30:16,110
or whatever I can basically we are

00:30:14,970 --> 00:30:18,060
running out of time for questions

00:30:16,110 --> 00:30:21,660
already so let's think

00:30:18,060 --> 00:30:23,720
oh yeah once again forever thank you

00:30:21,660 --> 00:30:23,720

YouTube URL: https://www.youtube.com/watch?v=-S2sVhZ92PU


