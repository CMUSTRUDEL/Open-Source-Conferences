Title: Guillem Duran - Hacking Reinforcement Learning
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Hacking Reinforcement Learning
[EuroPython 2018 - Talk - 2018-07-25 - Fintry [PyData]]
[Edinburgh, UK]

By Guillem Duran


Creating huge datasets of top performing examples for Reinforcement Learning (RL) has always been tricky, but if we allow
ourselves to cheat a bit it can be done very easily. During this talk, I will present a new family of algorithms that allow to efficiently generate very high quality samples for any known RL environment.

This new generation of planning algorithms achieves a performance which is several orders of magnitude higher
than any other existing alternative, while offering linear time complexity and good scalability.


This talk will be a practical example
of how we can use new tools for hacking any reinforcement learning environment, and make it generate superhuman level games.

Hacking RL, as any other hacking process will be divided in four phases:

During information gathering, I will briefly explain what are the main ideas behind Reinforcement Learning.
I will also talk about how our theory (FractalAI) came to be, and what are the fundamental concepts behind it.


We will find an attack vector against the environment API, and explain how it can be exploited. I will explain
the fundamental concepts needed to build a new generation of exploits, that will allow us to have complete control over the data the environment produces.


This is the time to test the new exploits and to show a proof of concept. We will exploit the attack vector to gain access
to the environment. Using only a laptop I will show how it is possible to sample data which surpasses human performance way faster than real time.


Once we have gained control of the environment, we will measure how well the exploits work, and how well the techniques presented
can generalize to other types of environments.


I want the talk to be as simple and fast as possible, with a lot of graphical examples, videos, and a Jupyter notebook.
The Q&amp;A session is the time to apply some social engineering to get me to talk about the details that you find more interesting.
I have prepared additional material covering the most common questions and concerns, but feel free to ask whatever you want, I love challenging questions ;)


License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:00,060 --> 00:00:08,490
[Applause]

00:00:01,820 --> 00:00:16,440
hey morning let me look at this first

00:00:08,490 --> 00:00:19,910
okay well I mean run and this is hacking

00:00:16,440 --> 00:00:21,810
reinforcement learning before I start I

00:00:19,910 --> 00:00:23,039
would like to ask you a few questions

00:00:21,810 --> 00:00:24,720
first

00:00:23,039 --> 00:00:28,619
who here is already familiar with

00:00:24,720 --> 00:00:30,990
reinforcement learning a lot of people

00:00:28,619 --> 00:00:32,520
nice and who's expecting to learn

00:00:30,990 --> 00:00:37,620
something new about it today

00:00:32,520 --> 00:00:42,719
oh then okay I'm super sorry to

00:00:37,620 --> 00:00:44,100
disappoint you because I got super sad

00:00:42,719 --> 00:00:46,379
when I found out that there was no

00:00:44,100 --> 00:00:49,289
breakfast and no social event so I

00:00:46,379 --> 00:00:51,690
decided to go on a strike and instead of

00:00:49,289 --> 00:00:54,809
talking about reform learning I will

00:00:51,690 --> 00:00:56,879
just be telling story about hacking AI

00:00:54,809 --> 00:01:00,859
Corp and playing some video games

00:00:56,879 --> 00:01:03,660
so first thing first what's a guy called

00:01:00,859 --> 00:01:06,960
well every story

00:01:03,660 --> 00:01:10,140
needs a big corporation and AI Corp is

00:01:06,960 --> 00:01:13,890
huge and super cool people use its

00:01:10,140 --> 00:01:15,650
services to the room a is tough but in

00:01:13,890 --> 00:01:18,270
the end is some sort of token exchange

00:01:15,650 --> 00:01:21,240
where you can make pretty good money if

00:01:18,270 --> 00:01:23,939
you're a smart enough in order to do so

00:01:21,240 --> 00:01:26,520
you first have to access the environment

00:01:23,939 --> 00:01:29,820
or the end marketplace where you can

00:01:26,520 --> 00:01:31,380
exchange money for data once you have

00:01:29,820 --> 00:01:34,079
enough data you will be allowed to

00:01:31,380 --> 00:01:36,470
access the algorithm marketplace when

00:01:34,079 --> 00:01:39,270
you can trade money for research

00:01:36,470 --> 00:01:40,860
everybody loves research kiss once you

00:01:39,270 --> 00:01:42,079
have it if you know how to do a little

00:01:40,860 --> 00:01:44,939
magic with it

00:01:42,079 --> 00:01:47,729
you can't reform it into a lot of money

00:01:44,939 --> 00:01:49,710
so you only have to repeat this cycle

00:01:47,729 --> 00:01:51,530
over and over again and become a

00:01:49,710 --> 00:01:54,600
millionaire

00:01:51,530 --> 00:01:57,119
the thing is that if we found a way to

00:01:54,600 --> 00:01:59,759
bypass the end marketplace and get data

00:01:57,119 --> 00:02:01,409
for free then we will have money to

00:01:59,759 --> 00:02:05,219
trade more money to trade for research

00:02:01,409 --> 00:02:06,869
so we'll get rich quicker today I'm

00:02:05,219 --> 00:02:09,500
going to talk about the master plan to

00:02:06,869 --> 00:02:09,500
accomplish this

00:02:10,250 --> 00:02:14,150
as any Hakeem process first we need to

00:02:12,890 --> 00:02:17,330
gather some information about our

00:02:14,150 --> 00:02:18,950
targets so let's stop talking about a

00:02:17,330 --> 00:02:22,760
bits about reinforcement learning and

00:02:18,950 --> 00:02:24,980
planning most people use the environment

00:02:22,760 --> 00:02:26,840
to do reinforcement learning which is

00:02:24,980 --> 00:02:28,880
like doing machine learning when you

00:02:26,840 --> 00:02:31,760
don't have needed labels nor examples

00:02:28,880 --> 00:02:33,860
available the environment represent your

00:02:31,760 --> 00:02:36,020
data source and also the tasks to be

00:02:33,860 --> 00:02:39,080
solved for example legging can Atari

00:02:36,020 --> 00:02:40,790
game so first you need to generate and

00:02:39,080 --> 00:02:43,130
label data by something from the

00:02:40,790 --> 00:02:46,880
environment for example playing again

00:02:43,130 --> 00:02:49,010
taking random actions so it's time to

00:02:46,880 --> 00:02:51,920
take an action the environment will

00:02:49,010 --> 00:02:53,810
return several things first you'll get a

00:02:51,920 --> 00:02:57,140
new PI array which represents an

00:02:53,810 --> 00:03:00,590
observation a float which represents the

00:02:57,140 --> 00:03:02,480
reward the bigger the better a boolean

00:03:00,590 --> 00:03:05,209
indicating if the game has already

00:03:02,480 --> 00:03:09,140
finished and a dictionary containing

00:03:05,209 --> 00:03:11,660
additional information in training time

00:03:09,140 --> 00:03:15,080
you will use all this information to

00:03:11,660 --> 00:03:17,320
create a label for its example so you

00:03:15,080 --> 00:03:19,190
end up with a data set of played games

00:03:17,320 --> 00:03:23,989
all the reinforcement learning

00:03:19,190 --> 00:03:25,940
algorithms are about creating labels

00:03:23,989 --> 00:03:30,560
which are good enough to be learnt by

00:03:25,940 --> 00:03:33,200
the noodle network so you need to train

00:03:30,560 --> 00:03:34,970
a neural network with those examples so

00:03:33,200 --> 00:03:36,590
you get a model which is capable of

00:03:34,970 --> 00:03:39,140
getting highest court sebaceans

00:03:36,590 --> 00:03:42,739
when using only the observations in test

00:03:39,140 --> 00:03:44,750
time to play the games and there are

00:03:42,739 --> 00:03:46,850
also a handful of weird people who uses

00:03:44,750 --> 00:03:49,790
a a corpse services to do planning

00:03:46,850 --> 00:03:52,190
rather than during formal learning both

00:03:49,790 --> 00:03:56,510
approaches aim to find a way to fight to

00:03:52,190 --> 00:03:58,280
to find high score games but when doing

00:03:56,510 --> 00:04:00,799
planning the approach taken is a bit

00:03:58,280 --> 00:04:03,019
different in planning instead of

00:04:00,799 --> 00:04:05,209
training noodle Network you can do

00:04:03,019 --> 00:04:09,140
whatever you want to sample the

00:04:05,209 --> 00:04:11,360
environment and get a good score the aim

00:04:09,140 --> 00:04:13,610
of the aim of our hack is to use

00:04:11,360 --> 00:04:16,579
planning to generate super cool datasets

00:04:13,610 --> 00:04:19,760
so we can train better our our

00:04:16,579 --> 00:04:22,190
reinforcement learning models so let's

00:04:19,760 --> 00:04:25,640
get to talk about planning a bit

00:04:22,190 --> 00:04:27,080
a few years ago I met on Twitter circle

00:04:25,640 --> 00:04:28,970
none death which is a Spanish

00:04:27,080 --> 00:04:32,480
mathematician who's even crazier than

00:04:28,970 --> 00:04:35,060
again together we challenged our self to

00:04:32,480 --> 00:04:38,270
derive from first principles what we

00:04:35,060 --> 00:04:40,460
call fractal AI which is our own theory

00:04:38,270 --> 00:04:41,840
or artificial intelligence that turns

00:04:40,460 --> 00:04:44,180
out that can also be used to solve

00:04:41,840 --> 00:04:47,000
planning problems we love wheels

00:04:44,180 --> 00:04:48,440
challenges so we basically did it for

00:04:47,000 --> 00:04:50,680
the pleasure of finding things out and

00:04:48,440 --> 00:04:53,450
have some fun along the way

00:04:50,680 --> 00:04:55,310
at first we got really curious about the

00:04:53,450 --> 00:04:57,920
paper that defined intelligence as a

00:04:55,310 --> 00:04:59,510
thermodynamic process and this was known

00:04:57,920 --> 00:05:02,750
within the framework of non-equilibrium

00:04:59,510 --> 00:05:05,170
statistical mechanics but then deep

00:05:02,750 --> 00:05:07,970
learning wasn't a thing on the field so

00:05:05,170 --> 00:05:11,690
this guy alexander business gross worked

00:05:07,970 --> 00:05:13,400
out an equation for intelligence this

00:05:11,690 --> 00:05:15,020
equation may look a bit intimidating at

00:05:13,400 --> 00:05:17,990
first but it's actually easy to

00:05:15,020 --> 00:05:20,990
understand it says that if you want to

00:05:17,990 --> 00:05:23,900
look intelligent in any situation then

00:05:20,990 --> 00:05:26,120
your needs action will be the one that

00:05:23,900 --> 00:05:29,450
list to the highest number of good

00:05:26,120 --> 00:05:32,590
possible outcomes in the future and what

00:05:29,450 --> 00:05:37,220
else what are those possible outcomes

00:05:32,590 --> 00:05:39,680
well you only have to count all the

00:05:37,220 --> 00:05:41,960
paths that adds it up to an arbitrary

00:05:39,680 --> 00:05:45,919
point in the future and map them to a

00:05:41,960 --> 00:05:48,260
score this means that if we know the

00:05:45,919 --> 00:05:51,200
future this equation tells us how to

00:05:48,260 --> 00:05:53,240
choose the right action unfortunately

00:05:51,200 --> 00:05:56,390
knowing about the future is not that

00:05:53,240 --> 00:05:59,000
easy so the only thing we can do is

00:05:56,390 --> 00:06:01,130
sample a bunch of random walks and use

00:05:59,000 --> 00:06:03,530
them as if I were to cook the food cone

00:06:01,130 --> 00:06:06,290
which is like the space of all future

00:06:03,530 --> 00:06:09,890
possible actions let me show you an

00:06:06,290 --> 00:06:12,169
example this is how we have a cone can

00:06:09,890 --> 00:06:13,430
be approximated you can see that the

00:06:12,169 --> 00:06:16,700
path of the cone have two different

00:06:13,430 --> 00:06:20,030
colors blue fourth path that started by

00:06:16,700 --> 00:06:23,900
moving left and right eye and read four

00:06:20,030 --> 00:06:25,850
paths that start by moving right the

00:06:23,900 --> 00:06:28,520
action chosen on each step will follow

00:06:25,850 --> 00:06:32,750
the color distribution of the cone the

00:06:28,520 --> 00:06:35,450
cone if the cone is blue the cards will

00:06:32,750 --> 00:06:36,110
turn left a lot if it's red they will

00:06:35,450 --> 00:06:38,509
turn right

00:06:36,110 --> 00:06:41,120
and if the colors are balanced they will

00:06:38,509 --> 00:06:43,729
just be going straight this is because

00:06:41,120 --> 00:06:46,099
the environment here is continuous but

00:06:43,729 --> 00:06:49,060
if it were discrete we would choose the

00:06:46,099 --> 00:06:51,409
action which has the most popular color

00:06:49,060 --> 00:06:55,300
how we sample the cone is important

00:06:51,409 --> 00:06:58,280
because the bigger the cone the better

00:06:55,300 --> 00:07:00,860
the cone represents what the agencies at

00:06:58,280 --> 00:07:02,960
the given moment and in order to take an

00:07:00,860 --> 00:07:05,659
action it will only take into account

00:07:02,960 --> 00:07:08,780
the information that it sampled at its

00:07:05,659 --> 00:07:10,789
time step this means that according to

00:07:08,780 --> 00:07:13,159
the equations we only need to calculate

00:07:10,789 --> 00:07:15,289
a super-huge cone to have like a

00:07:13,159 --> 00:07:17,710
god-like agent capable of solving any

00:07:15,289 --> 00:07:20,960
problem sounds cool right

00:07:17,710 --> 00:07:23,900
so what is nobody using causal entropic

00:07:20,960 --> 00:07:27,440
forces well you can see in the Google

00:07:23,900 --> 00:07:30,110
trend plots how virtually nobody talks

00:07:27,440 --> 00:07:33,379
about it and the few people who does it

00:07:30,110 --> 00:07:35,270
just for criticizing it I mean it is so

00:07:33,379 --> 00:07:37,520
unpopular because sampling cones is

00:07:35,270 --> 00:07:39,860
super hard even if you make simplifying

00:07:37,520 --> 00:07:41,870
assumptions the complexity of the

00:07:39,860 --> 00:07:44,779
problem scales exponentially with this

00:07:41,870 --> 00:07:46,250
with the time horizon the equations

00:07:44,779 --> 00:07:49,129
presented can only be solved

00:07:46,250 --> 00:07:51,529
analytically in very few cases and Monte

00:07:49,129 --> 00:07:54,949
Carlo algorithms fail to approximate the

00:07:51,529 --> 00:07:57,020
cone when it gets bigger if we know how

00:07:54,949 --> 00:08:00,050
to solve the equations we will have like

00:07:57,020 --> 00:08:01,400
the ultimate planning algorithm but even

00:08:00,050 --> 00:08:03,529
though this approach may send

00:08:01,400 --> 00:08:05,330
intuitively it is impractical for

00:08:03,529 --> 00:08:09,589
solving problems that appear into

00:08:05,330 --> 00:08:11,569
informal learning papers it is

00:08:09,589 --> 00:08:15,319
impossible to calculate exactly become

00:08:11,569 --> 00:08:17,539
but is it possible to sample fast super

00:08:15,319 --> 00:08:20,569
small subsets of paths they can

00:08:17,539 --> 00:08:23,060
approximate well the full cone this is

00:08:20,569 --> 00:08:26,120
the question that we try to solve during

00:08:23,060 --> 00:08:28,009
the last five years we spend most of our

00:08:26,120 --> 00:08:30,349
treat free time trying to figure out a

00:08:28,009 --> 00:08:34,310
way to sample interesting sequences of

00:08:30,349 --> 00:08:36,229
actions we only had our own laptops so

00:08:34,310 --> 00:08:39,140
we need to do it under heavily

00:08:36,229 --> 00:08:41,690
constrained computational resources we

00:08:39,140 --> 00:08:43,669
were forced to find a new method in

00:08:41,690 --> 00:08:45,529
order to have two complete control over

00:08:43,669 --> 00:08:48,339
how we want to explore the space of all

00:08:45,529 --> 00:08:48,339
those possible trajectories

00:08:48,800 --> 00:08:54,829
after training come failing literally

00:08:51,619 --> 00:08:57,199
like 500 times we managed to create the

00:08:54,829 --> 00:08:58,309
sort of stochastic wavefunction that

00:08:57,199 --> 00:09:01,790
allow us to do exactly that

00:08:58,309 --> 00:09:04,249
in the end fractal AI is nothing but a

00:09:01,790 --> 00:09:06,100
set of rules about how to define swarms

00:09:04,249 --> 00:09:09,559
and move them around in any state space

00:09:06,100 --> 00:09:12,049
our Theory also offers tools to measure

00:09:09,559 --> 00:09:13,759
those swarms and compare them so they

00:09:12,049 --> 00:09:23,239
can be used to solve integrals and

00:09:13,759 --> 00:09:26,149
different kinds of equations so now we

00:09:23,239 --> 00:09:27,470
already know a bit the backgrounds that

00:09:26,149 --> 00:09:30,939
we needed it's time to find

00:09:27,470 --> 00:09:32,899
vulnerabilities and build exploits

00:09:30,939 --> 00:09:35,470
if you want to properly hack

00:09:32,899 --> 00:09:38,869
reinforcement learning we need to know

00:09:35,470 --> 00:09:41,959
how this workflow is implemented it

00:09:38,869 --> 00:09:45,079
turns out that all the modeling amounts

00:09:41,959 --> 00:09:47,569
to a bunch of tensor flow code and the

00:09:45,079 --> 00:09:49,309
environment follows the open AI Jamie

00:09:47,569 --> 00:09:52,309
interface which has become a de facto

00:09:49,309 --> 00:09:55,220
standard in the field so we want to

00:09:52,309 --> 00:09:57,860
preserve this public API in order for

00:09:55,220 --> 00:10:04,369
our tensor flow to not notice how we're

00:09:57,860 --> 00:10:06,619
hacking it so we're going to perform a

00:10:04,369 --> 00:10:09,110
good old man in the middle in order to

00:10:06,619 --> 00:10:11,209
hack it we gonna position ourselves as a

00:10:09,110 --> 00:10:15,559
proxy between the tensor flow code and

00:10:11,209 --> 00:10:17,990
the environment first we're gonna we're

00:10:15,559 --> 00:10:21,819
going to discard the action set by the

00:10:17,990 --> 00:10:24,860
tensor flow code and calculate our own

00:10:21,819 --> 00:10:26,480
how well first we need to read the

00:10:24,860 --> 00:10:31,009
internal state of the environment and

00:10:26,480 --> 00:10:33,819
then calculate our malicious action this

00:10:31,009 --> 00:10:37,309
action will be injected first inside the

00:10:33,819 --> 00:10:39,799
the environment so we get better games

00:10:37,309 --> 00:10:40,879
and then we also will inject it inside

00:10:39,799 --> 00:10:43,369
the info dictionary

00:10:40,879 --> 00:10:48,980
so the tensor flow code notice what we

00:10:43,369 --> 00:10:51,499
are doing in order to calculate those

00:10:48,980 --> 00:10:54,139
actions we will use two different zero

00:10:51,499 --> 00:10:57,529
days what we call fractal Monte Carlo

00:10:54,139 --> 00:10:59,629
and this one wave they will allow us to

00:10:57,529 --> 00:11:02,179
inject actions that lead to super human

00:10:59,629 --> 00:11:02,570
performance games while maintaining the

00:11:02,179 --> 00:11:08,990
original

00:11:02,570 --> 00:11:11,150
flower informal learning intact we will

00:11:08,990 --> 00:11:14,570
be using swarms Vicki because it's

00:11:11,150 --> 00:11:17,330
actually the fastest way we found how to

00:11:14,570 --> 00:11:19,600
calculate expectations on how good will

00:11:17,330 --> 00:11:22,760
be the paths that followed even action

00:11:19,600 --> 00:11:25,250
swarm will allow us to take advantage of

00:11:22,760 --> 00:11:27,230
both information contained in the state

00:11:25,250 --> 00:11:30,140
of space itself this is the observations

00:11:27,230 --> 00:11:31,880
and also in the reward function let me

00:11:30,140 --> 00:11:37,940
show you an example of how the swarm

00:11:31,880 --> 00:11:39,710
works you can see a swarm is nothing but

00:11:37,940 --> 00:11:42,560
a cloud of points that moves around

00:11:39,710 --> 00:11:46,250
randomly while also avoiding crashing

00:11:42,560 --> 00:11:48,530
into against the walls this plot of

00:11:46,250 --> 00:11:51,470
points in itself is nothing but a

00:11:48,530 --> 00:11:53,630
mathematical curiosity but if we care to

00:11:51,470 --> 00:11:57,080
record the path that each of these

00:11:53,630 --> 00:12:00,320
points also called walkers follow we

00:11:57,080 --> 00:12:02,210
will end up with a cone which is truly

00:12:00,320 --> 00:12:07,130
bigger than anything else you can

00:12:02,210 --> 00:12:09,620
calculate with other algorithms the only

00:12:07,130 --> 00:12:11,360
problem with the swarm is that they are

00:12:09,620 --> 00:12:13,100
like the closest thing to alien

00:12:11,360 --> 00:12:15,500
mathematics that we have seen so far

00:12:13,100 --> 00:12:17,270
because they are built on a different

00:12:15,500 --> 00:12:21,050
set of assumptions than traditional

00:12:17,270 --> 00:12:22,310
statistics and probability theory we

00:12:21,050 --> 00:12:24,590
think that it should be possible to

00:12:22,310 --> 00:12:27,410
prove that given enough computing time

00:12:24,590 --> 00:12:30,110
and computational resources they end up

00:12:27,410 --> 00:12:32,660
working but when you try to measure how

00:12:30,110 --> 00:12:35,030
well how well will they perform with

00:12:32,660 --> 00:12:37,340
respect to other algorithms you'll run

00:12:35,030 --> 00:12:38,840
into a lot of trouble that's why we

00:12:37,340 --> 00:12:41,030
prophesize that these family of

00:12:38,840 --> 00:12:42,860
algorithms lies beyond what can be

00:12:41,030 --> 00:12:46,490
proven with currently no mathematical

00:12:42,860 --> 00:12:49,340
tools so the only option we have is to

00:12:46,490 --> 00:12:51,110
try them out in real problems and see if

00:12:49,340 --> 00:12:53,600
they perform well enough to be used to

00:12:51,110 --> 00:12:55,250
solve them and also forget about

00:12:53,600 --> 00:13:00,470
rigorous proof case for now we don't

00:12:55,250 --> 00:13:03,470
have any alternative so let's present

00:13:00,470 --> 00:13:06,860
our zero days the first one is called

00:13:03,470 --> 00:13:08,660
the Swan Wave this algorithm basically

00:13:06,860 --> 00:13:11,180
consists in moving to swarm and

00:13:08,660 --> 00:13:14,480
recording the path it's followed so we

00:13:11,180 --> 00:13:16,100
end up with the cone unlike all other

00:13:14,480 --> 00:13:18,500
planning algorithms

00:13:16,100 --> 00:13:21,139
that built three of paths after taking

00:13:18,500 --> 00:13:21,769
its step in this case when using this

00:13:21,139 --> 00:13:23,870
one wave

00:13:21,769 --> 00:13:25,490
you only need to sample one cone in

00:13:23,870 --> 00:13:29,839
order to know the trajectory that the

00:13:25,490 --> 00:13:33,649
agent will follow this is how one wave

00:13:29,839 --> 00:13:36,670
looked like it built the path by keeping

00:13:33,649 --> 00:13:39,500
track of trajectories of the walkers and

00:13:36,670 --> 00:13:42,319
keeps expanding the strip until you

00:13:39,500 --> 00:13:44,089
reach the desired time horizon this is

00:13:42,319 --> 00:13:45,920
not exactly the same algorithm that you

00:13:44,089 --> 00:13:47,750
can find in the repository cuz this is

00:13:45,920 --> 00:13:50,449
an older version that turns out it

00:13:47,750 --> 00:13:53,480
didn't work as expected but theory is

00:13:50,449 --> 00:13:55,550
pretty much the same for example here in

00:13:53,480 --> 00:13:57,380
this in this case we were we managed to

00:13:55,550 --> 00:14:03,550
calculate a path in a continuous

00:13:57,380 --> 00:14:03,550
environment which was 1,500 steps long

00:14:04,569 --> 00:14:12,889
our other zero day is what we call the

00:14:08,449 --> 00:14:14,750
flattened Monte Carlo algorithm if we

00:14:12,889 --> 00:14:17,839
wanted to calculate path the other ways

00:14:14,750 --> 00:14:21,319
algorithms do we will just need to use

00:14:17,839 --> 00:14:24,319
this one this instead of using only one

00:14:21,319 --> 00:14:26,660
big tree to calculate the cone we

00:14:24,319 --> 00:14:30,069
calculate like a mini cone after taking

00:14:26,660 --> 00:14:33,290
each step it is true that doing this

00:14:30,069 --> 00:14:36,439
that using this algorithm requires more

00:14:33,290 --> 00:14:38,290
computational power but it really

00:14:36,439 --> 00:14:40,730
increases the performance of the agent

00:14:38,290 --> 00:14:43,069
in order to think about factor Monte

00:14:40,730 --> 00:14:45,920
Carlo is that it allows to approximate

00:14:43,069 --> 00:14:52,699
for every action a utility distribution

00:14:45,920 --> 00:14:54,709
of which one of them but let's see how

00:14:52,699 --> 00:14:56,120
it works in an environment which is more

00:14:54,709 --> 00:15:01,490
difficult than the one used to show how

00:14:56,120 --> 00:15:04,279
the Swan wave works in order to test

00:15:01,490 --> 00:15:06,139
this FMC algorithm self you built what

00:15:04,279 --> 00:15:08,389
we call like the hardcore lunar lander

00:15:06,139 --> 00:15:11,649
which is an environment where you can

00:15:08,389 --> 00:15:11,649
deploy several spaceships

00:15:13,980 --> 00:15:18,899
it's a spaceship has two continuous

00:15:16,230 --> 00:15:22,800
degrees of freedom which represent the

00:15:18,899 --> 00:15:26,160
true propellers that it has it also has

00:15:22,800 --> 00:15:27,690
a fuel bar and some health points and in

00:15:26,160 --> 00:15:30,510
order to make it more interesting we

00:15:27,690 --> 00:15:35,490
attach it to a rubber band with the hook

00:15:30,510 --> 00:15:38,310
at its end this pay ship will be

00:15:35,490 --> 00:15:40,860
rewarded if it uses the elastic hook to

00:15:38,310 --> 00:15:43,410
pick up a rock and it will be rewarded

00:15:40,860 --> 00:15:45,389
again if it's capable of dropping the

00:15:43,410 --> 00:15:48,089
rock inside the you know the inner green

00:15:45,389 --> 00:15:49,949
circle once it's off the rock

00:15:48,089 --> 00:15:52,380
it cannot be picked up again until it

00:15:49,949 --> 00:15:54,420
leaves the outer dashed circle of course

00:15:52,380 --> 00:15:59,790
the game will finish if the rocket

00:15:54,420 --> 00:16:01,649
crashes or runs out of well we have

00:15:59,790 --> 00:16:03,449
drowned the cone heel the same way we

00:16:01,649 --> 00:16:06,959
did with the cards but in this case

00:16:03,449 --> 00:16:08,820
things get a little Messier this grey

00:16:06,959 --> 00:16:11,279
lines here represent the fugitive

00:16:08,820 --> 00:16:14,430
trajectories that the spaceships the

00:16:11,279 --> 00:16:16,589
spaceship will follow while the colored

00:16:14,430 --> 00:16:19,170
lines represent the trajectory that the

00:16:16,589 --> 00:16:21,870
hook will follow if the hooks path

00:16:19,170 --> 00:16:24,060
changed color it will mean that at some

00:16:21,870 --> 00:16:30,240
point in the future the hook got either

00:16:24,060 --> 00:16:32,339
attached or detached from the rock it's

00:16:30,240 --> 00:16:34,980
actually easy to check to check that

00:16:32,339 --> 00:16:36,870
even though the agent is only storing a

00:16:34,980 --> 00:16:40,649
tiny fraction of the entire space of

00:16:36,870 --> 00:16:42,690
future of future paths it is capable of

00:16:40,649 --> 00:16:45,029
behaving reasonably well for example

00:16:42,690 --> 00:16:47,130
here you can see how the spaceship can

00:16:45,029 --> 00:16:49,410
take advantage of the elastic properties

00:16:47,130 --> 00:16:51,750
of the hook to catch the rock again and

00:16:49,410 --> 00:16:55,199
again and it's called several times in a

00:16:51,750 --> 00:16:58,140
row before losing it so it looks like

00:16:55,199 --> 00:17:00,779
our hypothesis of finding a small subset

00:16:58,140 --> 00:17:04,860
of useful paths still holds in these

00:17:00,779 --> 00:17:07,110
weird environments so let's see how to

00:17:04,860 --> 00:17:12,020
become Alice let's see how we can take

00:17:07,110 --> 00:17:15,480
advantage of it to build our exploits so

00:17:12,020 --> 00:17:20,809
now we have everything we know so let's

00:17:15,480 --> 00:17:20,809
split some stuff because it's demo time

00:17:24,880 --> 00:17:32,300
okay now if I manage to use this

00:17:29,690 --> 00:17:37,430
properly okay now it's time to play

00:17:32,300 --> 00:17:41,150
pac-man so while I speak I will just

00:17:37,430 --> 00:17:41,780
leave Aswang wave solving the pac-man

00:17:41,150 --> 00:17:47,240
game

00:17:41,780 --> 00:17:49,070
I hope yeah it's working perfect okay in

00:17:47,240 --> 00:17:50,380
order to run this one wave you have to

00:17:49,070 --> 00:17:53,870
take into account several parameters

00:17:50,380 --> 00:17:55,400
first the number of workers which is the

00:17:53,870 --> 00:17:57,740
sides of the swarm that you will be

00:17:55,400 --> 00:18:01,700
using the bigger the better

00:17:57,740 --> 00:18:04,610
this is like something which allow us to

00:18:01,700 --> 00:18:06,140
calculate a live demo I'm not sure

00:18:04,610 --> 00:18:08,300
because I didn't fix the random seats

00:18:06,140 --> 00:18:11,150
but we should get between 20,000 and

00:18:08,300 --> 00:18:13,520
30,000 points you can also have to set

00:18:11,150 --> 00:18:15,920
some hard limits on the maximum score

00:18:13,520 --> 00:18:19,550
that you want to sample and the maximum

00:18:15,920 --> 00:18:22,370
steps that you are allowed to take these

00:18:19,550 --> 00:18:24,920
numbers here represent how many times we

00:18:22,370 --> 00:18:28,040
want to repeat the same action after so

00:18:24,920 --> 00:18:29,870
as the deciding one most algorithms used

00:18:28,040 --> 00:18:32,690
to fit in the fits in number which they

00:18:29,870 --> 00:18:35,210
call like a skip frame but in this case

00:18:32,690 --> 00:18:36,620
we will just sample how many times

00:18:35,210 --> 00:18:42,290
you're gonna repeat that action from

00:18:36,620 --> 00:18:45,980
this distribution here then we only need

00:18:42,290 --> 00:18:47,540
to create an environment create choose

00:18:45,980 --> 00:18:54,260
the kind of distribution that we will

00:18:47,540 --> 00:18:56,930
use as a prior and run our swarm wave it

00:18:54,260 --> 00:19:00,620
should be about to finish Oh turns out

00:18:56,930 --> 00:19:03,590
that it performs fairly well okay it

00:19:00,620 --> 00:19:08,870
took 1 minute and 18 seconds and it is

00:19:03,590 --> 00:19:11,110
called 36 6000 points let's see how it

00:19:08,870 --> 00:19:11,110
played

00:19:29,159 --> 00:19:35,289
if you want to run the same demo every

00:19:31,809 --> 00:19:37,840
all the material it's in in the online

00:19:35,289 --> 00:19:39,610
repository so you can you only need to

00:19:37,840 --> 00:19:43,049
check the link which is in the abstract

00:19:39,610 --> 00:19:43,049
of the talk in order to play this

00:21:02,900 --> 00:21:15,590
oh yeah Priya some right what does mean

00:21:09,350 --> 00:21:19,280
scurrying oh wait there's more video

00:21:15,590 --> 00:21:22,910
games to come so what does means of

00:21:19,280 --> 00:21:26,030
getting 36,000 points in 1 minute and 18

00:21:22,910 --> 00:21:28,070
seconds okay I built a few fails which

00:21:26,030 --> 00:21:32,270
are about benchmarking the algorithm

00:21:28,070 --> 00:21:34,910
both against human performance and two

00:21:32,270 --> 00:21:37,340
papers that I have linked here but as

00:21:34,910 --> 00:21:41,990
this is a bit boring let's just plot

00:21:37,340 --> 00:21:44,420
song graphs first this color here we are

00:21:41,990 --> 00:21:47,540
comparing the score that this one wave

00:21:44,420 --> 00:21:49,580
achieved against different algorithms

00:21:47,540 --> 00:21:51,230
from other papers for example here you

00:21:49,580 --> 00:21:56,030
have the human performance which paths

00:21:51,230 --> 00:21:59,950
was like 40% of what we achieved use TC

00:21:56,030 --> 00:22:02,600
which is the one that alpha zero uses or

00:21:59,950 --> 00:22:04,400
also we have benchmarking it and the one

00:22:02,600 --> 00:22:06,890
which is the state of the art and these

00:22:04,400 --> 00:22:10,760
two are algorithms which run in a

00:22:06,890 --> 00:22:12,650
cluster in Amazon this score is not that

00:22:10,760 --> 00:22:15,950
impressive so why are we using this

00:22:12,650 --> 00:22:19,309
instead of other algorithms well because

00:22:15,950 --> 00:22:26,360
when you care to plot the efficiency of

00:22:19,309 --> 00:22:28,760
the algorithms we get this which is how

00:22:26,360 --> 00:22:30,830
many orders of magnitude faster Swan

00:22:28,760 --> 00:22:33,470
wave is with respect to other things

00:22:30,830 --> 00:22:35,600
that you can use for example you can see

00:22:33,470 --> 00:22:37,880
that it plays about five times faster

00:22:35,600 --> 00:22:40,580
than a human but when you compare it

00:22:37,880 --> 00:22:42,980
with other algorithms you get that it

00:22:40,580 --> 00:22:47,179
plays about 5,000 times faster than a

00:22:42,980 --> 00:22:49,280
cluster in Amazon or about thirty five

00:22:47,179 --> 00:22:51,110
thirty thousand times faster than the

00:22:49,280 --> 00:22:54,559
algorithm that offers that Alphaville

00:22:51,110 --> 00:22:57,340
uses so this is basically the only way

00:22:54,559 --> 00:23:02,300
you have to run a team of pac-man in

00:22:57,340 --> 00:23:06,290
life in the talk this was the Swan wave

00:23:02,300 --> 00:23:09,640
but what about fractal Montecarlo let's

00:23:06,290 --> 00:23:09,640
take how it works

00:23:11,180 --> 00:23:17,450
in this case I'm making the environment

00:23:15,020 --> 00:23:20,120
a bit different kiss and choosing not to

00:23:17,450 --> 00:23:22,340
clone the random seeds that we are using

00:23:20,120 --> 00:23:24,530
in this case so instead of being a

00:23:22,340 --> 00:23:27,590
deterministic environment what the

00:23:24,530 --> 00:23:30,410
agencies is no stochastic one so it's

00:23:27,590 --> 00:23:33,950
not capable to predict properly where

00:23:30,410 --> 00:23:36,290
the the goals will be in the future in

00:23:33,950 --> 00:23:39,920
this case we are taking about two

00:23:36,290 --> 00:23:44,990
thousand and five hundred steps samples

00:23:39,920 --> 00:23:49,309
per step at most and well it just tried

00:23:44,990 --> 00:23:51,050
to do the best it can in order to

00:23:49,309 --> 00:23:53,960
compare it with other algorithms to get

00:23:51,050 --> 00:23:56,420
an idea of why they work they usually

00:23:53,960 --> 00:24:01,250
use about one hundred and five thousand

00:23:56,420 --> 00:24:04,190
samples no yeah 100 one hundred and

00:24:01,250 --> 00:24:07,750
fifty thousand samples per step instead

00:24:04,190 --> 00:24:07,750
of two thousand and five hundred

00:24:53,370 --> 00:24:58,260
okay so this time it didn't manage to

00:24:56,310 --> 00:25:00,120
finish the first level you can expect if

00:24:58,260 --> 00:25:02,100
you tried it with these parameters to

00:25:00,120 --> 00:25:03,990
something guessed up to about 10,000

00:25:02,100 --> 00:25:06,750
points but given that the task it's so

00:25:03,990 --> 00:25:08,400
difficult we would need much more

00:25:06,750 --> 00:25:12,630
computational power to run it in real

00:25:08,400 --> 00:25:14,250
time this was just a small proof of

00:25:12,630 --> 00:25:16,290
concept of how we can solve the games

00:25:14,250 --> 00:25:18,570
but we are not actually performing the

00:25:16,290 --> 00:25:22,110
hack in order to do so we need to

00:25:18,570 --> 00:25:26,280
impersonate the interface of open AI

00:25:22,110 --> 00:25:27,720
team we can do that but in this case I'm

00:25:26,280 --> 00:25:33,510
showing you something a bit more

00:25:27,720 --> 00:25:36,000
interesting which is hacking instead of

00:25:33,510 --> 00:25:39,480
opening I team open a baselines package

00:25:36,000 --> 00:25:42,680
which is like the facto standard also

00:25:39,480 --> 00:25:45,510
for high quality implementations of

00:25:42,680 --> 00:25:48,260
informal learning algorithms where they

00:25:45,510 --> 00:25:52,080
instead of using only one environment

00:25:48,260 --> 00:25:53,550
they use several in parallel so instead

00:25:52,080 --> 00:25:57,720
of getting one observation you get a

00:25:53,550 --> 00:26:00,830
batch of them this time I'm using a pre

00:25:57,720 --> 00:26:02,880
calculated database of games that I

00:26:00,830 --> 00:26:07,230
calculated yesterday while visiting the

00:26:02,880 --> 00:26:10,860
castle about Qbert this basically allows

00:26:07,230 --> 00:26:13,380
you to load the games and playful

00:26:10,860 --> 00:26:16,470
quality samples without your algorithms

00:26:13,380 --> 00:26:18,890
and not notice in it you can see here

00:26:16,470 --> 00:26:22,080
that when I call the step I just

00:26:18,890 --> 00:26:24,330
inputting a new string and it's

00:26:22,080 --> 00:26:26,910
completely ignored and we only take the

00:26:24,330 --> 00:26:30,870
actions that our algorithm choose to

00:26:26,910 --> 00:26:33,900
take do you think about this is that in

00:26:30,870 --> 00:26:36,450
Qbert once you get one thousand hundred

00:26:33,900 --> 00:26:39,090
one hundred thousand points the counter

00:26:36,450 --> 00:26:41,930
restarts but I mean you can keep playing

00:26:39,090 --> 00:26:41,930
as long as you want

00:26:59,270 --> 00:27:05,340
when using this kind of environment you

00:27:03,150 --> 00:27:07,770
need to choose how long you want the

00:27:05,340 --> 00:27:10,620
chunks of the games calculated to be in

00:27:07,770 --> 00:27:13,410
this case we are either playing until

00:27:10,620 --> 00:27:16,950
you lose one life or see it you've

00:27:13,410 --> 00:27:19,460
restarted the counter either until you

00:27:16,950 --> 00:27:26,010
plays you lose more life or you get

00:27:19,460 --> 00:27:27,750
35,000 points other killers when we

00:27:26,010 --> 00:27:29,160
think about cubic that I don't really

00:27:27,750 --> 00:27:31,850
understand is that sometimes when you

00:27:29,160 --> 00:27:35,580
fall off the cliff you don't lose a life

00:27:31,850 --> 00:27:37,230
I'm not sure if this a book back or what

00:27:35,580 --> 00:27:43,740
but I mean this is clearly taking

00:27:37,230 --> 00:27:50,760
advantage of it okay enough for today

00:27:43,740 --> 00:27:51,809
this can clip playing forever so we

00:27:50,760 --> 00:27:54,510
managed to implement the proof of

00:27:51,809 --> 00:27:57,720
concept of the hack that was introduced

00:27:54,510 --> 00:28:00,300
few minutes before but instead of

00:27:57,720 --> 00:28:02,160
mimicking the environment interface you

00:28:00,300 --> 00:28:05,670
can also generate data several other

00:28:02,160 --> 00:28:08,940
ways for example here we have also

00:28:05,670 --> 00:28:10,740
deities for generating whole games so

00:28:08,940 --> 00:28:12,830
you can save them and use them later to

00:28:10,740 --> 00:28:17,250
do whatever you want with them or

00:28:12,830 --> 00:28:20,250
instead of whole games examples like

00:28:17,250 --> 00:28:24,059
Toby more of supervised learning fashion

00:28:20,250 --> 00:28:26,520
for example here you can create a

00:28:24,059 --> 00:28:28,410
generator which outputs the current

00:28:26,520 --> 00:28:31,920
observation the action that should be

00:28:28,410 --> 00:28:33,809
taken the reward that you will get how

00:28:31,920 --> 00:28:35,820
the Nets observation will look like and

00:28:33,809 --> 00:28:38,670
the boolean indicating if the game is

00:28:35,820 --> 00:28:40,980
over if instead of one single example

00:28:38,670 --> 00:28:43,860
you wanna sample a batch of them and

00:28:40,980 --> 00:28:48,090
feed it to Gators for example you can

00:28:43,860 --> 00:28:49,800
use this function here so this is pretty

00:28:48,090 --> 00:28:54,300
much the demo that I wanted to show you

00:28:49,800 --> 00:28:58,260
but we only played one game so what can

00:28:54,300 --> 00:29:01,700
we expect if we try it in all the

00:28:58,260 --> 00:29:01,700
different kinds of environments

00:29:03,410 --> 00:29:08,010
generating data with our hack is

00:29:05,820 --> 00:29:10,440
actually faster and cheaper than hiring

00:29:08,010 --> 00:29:14,250
a human to play any Atari game but

00:29:10,440 --> 00:29:17,640
pitiful and using the wave you can even

00:29:14,250 --> 00:29:19,230
beat the human record in 32 out of 50

00:29:17,640 --> 00:29:23,010
games which are the ones that are all

00:29:19,230 --> 00:29:26,030
always tried in planning papers these

00:29:23,010 --> 00:29:29,070
two games here bot sync and ice hockey

00:29:26,030 --> 00:29:31,710
do you get a reward of plus one if you

00:29:29,070 --> 00:29:34,530
manage to score a goal in hockey or you

00:29:31,710 --> 00:29:36,240
manage to hit your opponent in boxing if

00:29:34,530 --> 00:29:39,780
it's the other way around

00:29:36,240 --> 00:29:41,910
you will get -1 reward this means that

00:29:39,780 --> 00:29:44,460
even though the rewards are relatively

00:29:41,910 --> 00:29:52,350
sparse this kind of environments can

00:29:44,460 --> 00:29:55,470
easily be solved this is impossible to

00:29:52,350 --> 00:29:58,740
sample relatively to natively good

00:29:55,470 --> 00:30:00,810
performance on Montezuma revenge which

00:29:58,740 --> 00:30:03,870
is a game with a stream lease pass

00:30:00,810 --> 00:30:06,210
rewards in Montezuma after starting

00:30:03,870 --> 00:30:08,640
again you will only get rewarded once

00:30:06,210 --> 00:30:10,890
you're able to get to get the key which

00:30:08,640 --> 00:30:13,830
requires a sibling a sequence of precise

00:30:10,890 --> 00:30:17,160
movements where you won't be getting any

00:30:13,830 --> 00:30:19,260
reward at all meanwhile the wave can

00:30:17,160 --> 00:30:21,320
handle it because the swarm also uses

00:30:19,260 --> 00:30:23,730
information contained in the observation

00:30:21,320 --> 00:30:26,310
this means that when no reward is

00:30:23,730 --> 00:30:30,150
available the algorithm will just focus

00:30:26,310 --> 00:30:33,090
on exploring new new regions of the

00:30:30,150 --> 00:30:35,130
state space although it would be better

00:30:33,090 --> 00:30:37,680
to derive a specific algorithm for those

00:30:35,130 --> 00:30:42,350
kinds of problems it's nice to see that

00:30:37,680 --> 00:30:45,450
the song wife is also pretty robust but

00:30:42,350 --> 00:30:48,000
this one wave really shines when we

00:30:45,450 --> 00:30:50,000
don't have a sparse reward in that case

00:30:48,000 --> 00:30:53,490
Atari games are pretty easy to solve

00:30:50,000 --> 00:30:56,610
we just warm up only 15 Walker's you can

00:30:53,490 --> 00:31:01,080
solve asteroids and solving Backman

00:30:56,610 --> 00:31:03,630
takes by about 5,000 a funny thing about

00:31:01,080 --> 00:31:06,750
those environments is that most games

00:31:03,630 --> 00:31:09,000
have hard-coded maximum score once you

00:31:06,750 --> 00:31:11,520
reach it the score will either stop

00:31:09,000 --> 00:31:15,030
increasing or it will reset the counter

00:31:11,520 --> 00:31:17,570
to zero for example like in pac-man or

00:31:15,030 --> 00:31:17,570
Qbert

00:31:19,129 --> 00:31:24,659
but Atari games are not the only kind of

00:31:21,960 --> 00:31:25,889
problems that swirl which can solve you

00:31:24,659 --> 00:31:28,200
can also try more challenging

00:31:25,889 --> 00:31:30,830
environments like Sonic or other Sega

00:31:28,200 --> 00:31:33,989
games in the open area of retro library

00:31:30,830 --> 00:31:36,539
even with relatively small swarm for

00:31:33,989 --> 00:31:39,509
example this sonic give you can see here

00:31:36,539 --> 00:31:41,940
was calculated using swarm of 200

00:31:39,509 --> 00:31:45,239
walkers and in this laptop it took less

00:31:41,940 --> 00:31:47,340
than two minutes to calculate if you

00:31:45,239 --> 00:31:49,470
want to go to a more factory environment

00:31:47,340 --> 00:31:51,509
you can install the demand control

00:31:49,470 --> 00:31:54,119
library which allows you to try it on

00:31:51,509 --> 00:31:55,739
robots in this case we use the same

00:31:54,119 --> 00:31:58,649
walkers that we use to solve pac-man

00:31:55,739 --> 00:32:00,480
this is 5000 to get the robot to work

00:31:58,649 --> 00:32:03,720
pretty well even though it looks like

00:32:00,480 --> 00:32:05,850
it's a bit drunk but take into account

00:32:03,720 --> 00:32:07,559
that this is because we were sampling

00:32:05,850 --> 00:32:10,859
because it's dimensional vector

00:32:07,559 --> 00:32:17,279
completely at random so it could be

00:32:10,859 --> 00:32:20,700
worse well this is how the sign this one

00:32:17,279 --> 00:32:24,629
wave performed now let's take a look at

00:32:20,700 --> 00:32:26,369
fractal Montecarlo this is light we show

00:32:24,629 --> 00:32:28,440
the difference between using the method

00:32:26,369 --> 00:32:31,889
proposed in the original paper of causal

00:32:28,440 --> 00:32:34,889
entropic forces versus using a fractal

00:32:31,889 --> 00:32:37,279
Montecarlo when we are bound to use the

00:32:34,889 --> 00:32:40,109
same number of samples

00:32:37,279 --> 00:32:42,629
this means that usually to solve Atari

00:32:40,109 --> 00:32:44,700
games is completely an overkill so

00:32:42,629 --> 00:32:47,639
instead of using the traditional

00:32:44,700 --> 00:32:49,710
environments you can find in Python we

00:32:47,639 --> 00:32:52,609
will use the custom lunar lander to show

00:32:49,710 --> 00:32:52,609
how it performs

00:32:56,010 --> 00:33:00,690
well it turns out that this hardcore

00:32:58,110 --> 00:33:02,809
Lunar Lander is an environment which is

00:33:00,690 --> 00:33:05,190
very difficult to solve for humans

00:33:02,809 --> 00:33:08,100
flying the rocket alone is a very

00:33:05,190 --> 00:33:10,110
difficult task on its own but when you

00:33:08,100 --> 00:33:12,770
attach the hook and the rubberband it

00:33:10,110 --> 00:33:16,230
becomes nearly impossible to control

00:33:12,770 --> 00:33:19,290
this is due to the bouncing of the hood

00:33:16,230 --> 00:33:21,750
which creates a chaotic motion and it

00:33:19,290 --> 00:33:23,760
turns out that humans are not really

00:33:21,750 --> 00:33:28,620
good at predicting that kind of chaotic

00:33:23,760 --> 00:33:35,010
movements but well looks like the rocket

00:33:28,620 --> 00:33:37,290
managed to solve it pretty easily so why

00:33:35,010 --> 00:33:39,720
don't we make it harder and make the

00:33:37,290 --> 00:33:42,740
rocks much heavier so more than one

00:33:39,720 --> 00:33:45,450
rocket is needed to carry them around

00:33:42,740 --> 00:33:47,880
this increases both the degrees of

00:33:45,450 --> 00:33:50,370
freedom and the difficulty of the

00:33:47,880 --> 00:33:55,440
problem becoming much more challenging

00:33:50,370 --> 00:33:59,669
tasks all the examples here will

00:33:55,440 --> 00:34:03,059
calculate it did using 100,000 steps

00:33:59,669 --> 00:34:05,610
production even though that we only have

00:34:03,059 --> 00:34:08,310
one core available this was like the

00:34:05,610 --> 00:34:13,530
maximum the hardest problem that we

00:34:08,310 --> 00:34:16,520
could solve in this case creating few

00:34:13,530 --> 00:34:22,710
videos which were a few minutes long

00:34:16,520 --> 00:34:25,440
took several hours but we also tried

00:34:22,710 --> 00:34:29,040
increasing the number of agents in tasks

00:34:25,440 --> 00:34:31,200
with which were easier to sample to see

00:34:29,040 --> 00:34:34,790
how this algorithm scales with the

00:34:31,200 --> 00:34:34,790
degrees of freedom of the agents

00:34:40,649 --> 00:34:43,980
we are right

00:34:50,379 --> 00:34:56,089
besides moving rocks around for example

00:34:53,540 --> 00:34:58,310
you can make several agents collect food

00:34:56,089 --> 00:35:00,980
which is represented by these green

00:34:58,310 --> 00:35:03,800
bubbles and repeating bodies which are

00:35:00,980 --> 00:35:07,700
distributed across the environment in

00:35:03,800 --> 00:35:09,410
this case the agents are greedy this

00:35:07,700 --> 00:35:11,630
means that they are just they just care

00:35:09,410 --> 00:35:14,570
about collecting food and not crashing

00:35:11,630 --> 00:35:17,060
against each other but we can take this

00:35:14,570 --> 00:35:18,859
kind of tasks one step further and force

00:35:17,060 --> 00:35:24,079
the agents to maintain information while

00:35:18,859 --> 00:35:27,010
gathering food for example here the cars

00:35:24,079 --> 00:35:29,480
also wanna drop food inside the bots and

00:35:27,010 --> 00:35:32,079
the spaceships just want to keep their

00:35:29,480 --> 00:35:34,760
fuel level high eating when needed but

00:35:32,079 --> 00:35:36,829
if you also tell them that they like to

00:35:34,760 --> 00:35:39,020
be close to each other do you end up

00:35:36,829 --> 00:35:42,650
getting this kind of weird elastic

00:35:39,020 --> 00:35:44,450
motions these are the results of trying

00:35:42,650 --> 00:35:47,119
to minimize several cost functions at

00:35:44,450 --> 00:35:49,060
the same time and depending on how you

00:35:47,119 --> 00:35:51,619
define the personality of the agent

00:35:49,060 --> 00:35:53,420
which is how the weights of different

00:35:51,619 --> 00:35:55,310
objectives are balanced you can

00:35:53,420 --> 00:35:58,040
influence how rigid the formation will

00:35:55,310 --> 00:36:06,050
be and how often an agent will break it

00:35:58,040 --> 00:36:08,420
to gather food this is an example of how

00:36:06,050 --> 00:36:10,160
you can actually hack open open AI

00:36:08,420 --> 00:36:12,619
baselines I mean the only thing you need

00:36:10,160 --> 00:36:14,990
to do is comment the make Italian

00:36:12,619 --> 00:36:20,030
function that the library provides and

00:36:14,990 --> 00:36:22,099
use the one that we make and when

00:36:20,030 --> 00:36:24,859
sampling the environment just not the

00:36:22,099 --> 00:36:27,680
actions that you sampled and recover

00:36:24,859 --> 00:36:29,510
them from inside the info dictionary so

00:36:27,680 --> 00:36:31,339
pretty much with three lines of code you

00:36:29,510 --> 00:36:38,599
can hack any reinforcement learning

00:36:31,339 --> 00:36:40,160
algorithm so we finally managed to hack

00:36:38,599 --> 00:36:44,119
the environment and get data for free

00:36:40,160 --> 00:36:45,910
and actually only using the code

00:36:44,119 --> 00:36:49,160
presented presented in the talk

00:36:45,910 --> 00:36:50,720
approaching and enforce any inverse to

00:36:49,160 --> 00:36:54,770
informal learning problem should be

00:36:50,720 --> 00:36:57,170
feasible this means that we can approach

00:36:54,770 --> 00:37:01,240
reinforcement learning as a supervisor

00:36:57,170 --> 00:37:01,240
without any human intervention at all

00:37:01,970 --> 00:37:07,560
something like generating a dataset of 1

00:37:05,310 --> 00:37:09,300
million games of pac-man is far away

00:37:07,560 --> 00:37:13,230
cheaper than training an agent to play

00:37:09,300 --> 00:37:15,360
dota or even Starcraft 2 so once you

00:37:13,230 --> 00:37:18,660
have that data set you can just over fit

00:37:15,360 --> 00:37:21,360
a tip network and see how it performs I

00:37:18,660 --> 00:37:24,630
mean I bet that you can get pretty good

00:37:21,360 --> 00:37:26,160
performance if you do that if you're

00:37:24,630 --> 00:37:29,280
bored enough you could even train a

00:37:26,160 --> 00:37:31,410
model or use that ring model as a prior

00:37:29,280 --> 00:37:33,450
for generating another million games

00:37:31,410 --> 00:37:34,980
which would have even higher scores

00:37:33,450 --> 00:37:37,290
because you already learned before and

00:37:34,980 --> 00:37:40,740
repeat the cycle until you get the

00:37:37,290 --> 00:37:43,290
performance that you want but that's the

00:37:40,740 --> 00:37:44,880
topic for another story for now let's

00:37:43,290 --> 00:37:46,320
just have some fun trying out the new

00:37:44,880 --> 00:37:48,840
hacking techniques presented because

00:37:46,320 --> 00:37:50,820
this is how it finishes the first volume

00:37:48,840 --> 00:37:55,800
of my tales about hacking reinforcement

00:37:50,820 --> 00:37:58,680
learning before moving to the Q&A I've

00:37:55,800 --> 00:38:01,620
talked a bit about myself I am gay and

00:37:58,680 --> 00:38:04,440
Rambo you stay and I'm a PI data majorca

00:38:01,620 --> 00:38:08,340
organizer I studied telecommunications

00:38:04,440 --> 00:38:10,500
engineering few years ago and I learned

00:38:08,340 --> 00:38:13,380
to code in Python so I could hack so

00:38:10,500 --> 00:38:15,060
mega research as a hobby but I must

00:38:13,380 --> 00:38:17,100
confess that my dream is becoming a

00:38:15,060 --> 00:38:19,470
proper scientist and make a living out

00:38:17,100 --> 00:38:21,960
of my passion so if you happen to know

00:38:19,470 --> 00:38:23,400
some real AI researchers please tell

00:38:21,960 --> 00:38:28,800
them that it would be super happy to

00:38:23,400 --> 00:38:30,930
work with them so yeah this is it I hope

00:38:28,800 --> 00:38:33,120
you like it and feel free to take a look

00:38:30,930 --> 00:38:35,680
at the repository of the talk it was a

00:38:33,120 --> 00:38:44,950
pleasure to be here thank you very much

00:38:35,680 --> 00:38:47,450
[Applause]

00:38:44,950 --> 00:38:49,519
thank you we have five minutes for

00:38:47,450 --> 00:38:53,289
questions if you have a question can you

00:38:49,519 --> 00:38:53,289
quilt on this side of the room please

00:38:57,309 --> 00:39:02,569
I'm not passing the microphone can you

00:38:59,749 --> 00:39:05,229
just come here and ask the question my

00:39:02,569 --> 00:39:05,229
friend thank you

00:39:17,210 --> 00:39:21,170
reward what kind of reward how do you

00:39:19,579 --> 00:39:23,029
the reward I think it's the score that

00:39:21,170 --> 00:39:26,569
you can see in the screen so only the

00:39:23,029 --> 00:39:29,029
score yeah I mean in that case yes you

00:39:26,569 --> 00:39:32,569
get rewarded if you eat each one of the

00:39:29,029 --> 00:39:34,009
cherries like 50 buoyancy you eat one of

00:39:32,569 --> 00:39:36,289
the things that are in the cordon that

00:39:34,009 --> 00:39:37,970
allows you to eat the ghost and if you

00:39:36,289 --> 00:39:39,710
eat a ghost depending on how much of

00:39:37,970 --> 00:39:41,450
then you eat you get different rewards

00:39:39,710 --> 00:39:50,809
you eat the four of them you get like a

00:39:41,450 --> 00:40:01,039
super huge reward any similarities

00:39:50,809 --> 00:40:08,660
between is some kind of some kind of

00:40:01,039 --> 00:40:10,670
swamp articulate imitation yeah I mean

00:40:08,660 --> 00:40:14,390
they are pretty much the same it's like

00:40:10,670 --> 00:40:16,430
trying to sample distribution which

00:40:14,390 --> 00:40:18,349
matches the true distribution of the

00:40:16,430 --> 00:40:25,670
Euro state space I mean let me show you

00:40:18,349 --> 00:40:28,579
here basically you just to be randomly a

00:40:25,670 --> 00:40:30,349
bunch of workers but the thing is that

00:40:28,579 --> 00:40:31,999
the way they are distributed doesn't

00:40:30,349 --> 00:40:35,420
match the actual reward distribution of

00:40:31,999 --> 00:40:38,599
the function so what we do is move them

00:40:35,420 --> 00:40:40,339
around after each iteration so you

00:40:38,599 --> 00:40:41,960
pretty much end up with the distribution

00:40:40,339 --> 00:40:44,089
of the workers will match the

00:40:41,960 --> 00:40:44,450
distribution of the reward mean all the

00:40:44,089 --> 00:40:46,160
codes

00:40:44,450 --> 00:40:48,229
it's commented and presented on the

00:40:46,160 --> 00:40:51,109
github repository so you can take a look

00:40:48,229 --> 00:40:54,069
at it and get me in a coffee break so I

00:40:51,109 --> 00:40:54,069
can just play you in that

00:41:01,320 --> 00:41:07,900
nice talk Thanks thank you the the

00:41:05,829 --> 00:41:13,650
benchmarks you did against the deep

00:41:07,900 --> 00:41:15,940
learning attention did you know I mean

00:41:13,650 --> 00:41:19,900
actually I didn't do a proper benchmark

00:41:15,940 --> 00:41:23,530
course it is required to use and one

00:41:19,900 --> 00:41:29,440
thousand and five hundred steps per

00:41:23,530 --> 00:41:32,349
sample and it's like super expensive so

00:41:29,440 --> 00:41:35,230
I have not calculated a proper benchmark

00:41:32,349 --> 00:41:41,530
of it but I'm on it I mean I'm getting

00:41:35,230 --> 00:41:42,369
nauseous - yeah but I mean you want to

00:41:41,530 --> 00:41:45,880
see how it works

00:41:42,369 --> 00:41:48,609
I have benchmark it against two papers

00:41:45,880 --> 00:41:50,710
but in the repository is also a Excel

00:41:48,609 --> 00:41:52,660
spreadsheet but you can find the

00:41:50,710 --> 00:41:55,349
performance of all the algorithms that

00:41:52,660 --> 00:41:59,560
exist compared to what we are getting

00:41:55,349 --> 00:42:04,180
both learning algorithms and planning

00:41:59,560 --> 00:42:06,280
algorithms I mean I think the only fair

00:42:04,180 --> 00:42:08,920
way to compare this is to benchmark it

00:42:06,280 --> 00:42:10,660
against the human records cause the

00:42:08,920 --> 00:42:13,349
difference is like super huge in the

00:42:10,660 --> 00:42:13,349
scores that you get

00:42:21,310 --> 00:42:26,420
open AI team yeah but I mean it's

00:42:24,050 --> 00:42:27,050
completely outdated no one no one uses

00:42:26,420 --> 00:42:31,030
it anymore

00:42:27,050 --> 00:42:31,030
I think they actually removed that I

00:42:34,300 --> 00:42:39,530
don't know that I mean I used to you

00:42:36,530 --> 00:42:43,070
like the open IAE Jim benchmarks but

00:42:39,530 --> 00:42:45,020
nobody was using them so my eye below

00:42:43,070 --> 00:42:49,760
did a few scores over there a few years

00:42:45,020 --> 00:42:51,860
ago but I mean yes yeah we get you can

00:42:49,760 --> 00:42:53,720
find in this Etzel spreadsheet all the

00:42:51,860 --> 00:42:56,360
references from all the papers that we

00:42:53,720 --> 00:43:00,200
used to create the spreadsheet and we

00:42:56,360 --> 00:43:02,660
also yeah I had the twin games yeah

00:43:00,200 --> 00:43:11,150
that's where we got the the table for

00:43:02,660 --> 00:43:13,790
human records okay I want to thank you

00:43:11,150 --> 00:43:16,880
Gillan for the table was really nice to

00:43:13,790 --> 00:43:18,370
see the videos and thank you again thank

00:43:16,880 --> 00:43:26,010
you very much

00:43:18,370 --> 00:43:26,010

YouTube URL: https://www.youtube.com/watch?v=rZpZU8LHPco


