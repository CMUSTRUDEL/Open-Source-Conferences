Title: Peter Hoffmann - Using Pandas and Dask to work with large columnar datasets  in Apache Parquet
Publication date: 2018-08-22
Playlist: EuroPython 2018
Description: 
	Using Pandas and Dask to work with large columnar datasets  in Apache Parquet
[EuroPython 2018 - Talk - 2018-07-25 - Fintry [PyData]]
[Edinburgh, UK]

By Peter Hoffmann

Apache Parquet Data Format

Apache Parquet is a binary, efficient columnar data format. It uses various
techniques to store data in a CPU and I/O efficient way like row groups,
compression for pages in column chunks or dictionary encoding for columns.
Index hints and statistics to quickly skip over chunks of irrelevant data
enable efficient queries on large amount of data.

Apache Parquet with Pandas &amp; Dask

Apache Parquet files can be read into Pandas DataFrames with the two libraries
fastparquet and Apache Arrow. While Pandas is mostly used to work with data
that fits into memory, Apache Dask allows us to work with data larger then memory
and even larger than local disk space. Data can be split up into partitions
and stored in cloud object storage systems like Amazon S3 or Azure Storage.

Using Metadata from the partiton filenames, parquet column statistics and
dictonary filtering allows faster performance for selective queries without
reading all data. This talk will show how use partitioning, row group skipping 
and general data layout to speed up queries on large amount of data.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2018.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:03,560 --> 00:00:09,030
so thank you for the nice introduction

00:00:06,109 --> 00:00:11,429
yes I'm a data scientist or software

00:00:09,030 --> 00:00:13,980
engineer at blue yonder in my day-to-day

00:00:11,429 --> 00:00:16,619
work I work a lot with Python I work

00:00:13,980 --> 00:00:20,340
with sequel I'm responsible for the

00:00:16,619 --> 00:00:22,470
databases in our company you can find

00:00:20,340 --> 00:00:24,689
this the slides of the talk and some

00:00:22,470 --> 00:00:27,119
other stuff some software that we have

00:00:24,689 --> 00:00:29,550
open sourced in our github repository at

00:00:27,119 --> 00:00:32,430
Loyola on github

00:00:29,550 --> 00:00:34,530
when I'm not working I still like to

00:00:32,430 --> 00:00:36,210
fiddle around with Python and the Python

00:00:34,530 --> 00:00:38,579
community because I really like the

00:00:36,210 --> 00:00:43,190
community so I became one of the

00:00:38,579 --> 00:00:46,230
co-organizers of PyCon de 2017 and 2018

00:00:43,190 --> 00:00:49,770
also a co-founder and co-organizer of

00:00:46,230 --> 00:00:52,500
the PI data cosplay group we just

00:00:49,770 --> 00:00:54,780
started so we a small group at the

00:00:52,500 --> 00:00:59,760
moment but we really plan to become

00:00:54,780 --> 00:01:02,340
bigger so before I go into my talk let's

00:00:59,760 --> 00:01:04,890
pitch a little bit the conference will

00:01:02,340 --> 00:01:07,439
have the second time in a row the PI

00:01:04,890 --> 00:01:11,010
data in Karlsruhe that's in southern

00:01:07,439 --> 00:01:13,380
Germany near to the Black Forest it's

00:01:11,010 --> 00:01:16,590
always a combined event it's a PI data I

00:01:13,380 --> 00:01:23,460
was a PI other PyCon de with a PI data

00:01:16,590 --> 00:01:26,759
track last year we had about 450 people

00:01:23,460 --> 00:01:30,659
there we had three tracks which talks

00:01:26,759 --> 00:01:31,619
one track with tutorials we had a really

00:01:30,659 --> 00:01:33,659
nice venue

00:01:31,619 --> 00:01:37,079
with the Center for Arden media in

00:01:33,659 --> 00:01:39,659
Cosway so here are some impressions we

00:01:37,079 --> 00:01:41,729
had great keynotes we talked about the

00:01:39,659 --> 00:01:44,820
universe and how it's made of and what

00:01:41,729 --> 00:01:48,119
it was made from we had in parallel the

00:01:44,820 --> 00:01:50,159
grade at exhibition the open codes and

00:01:48,119 --> 00:01:52,560
we had another great keynote speaker

00:01:50,159 --> 00:01:53,040
which I was very proud of the founder of

00:01:52,560 --> 00:01:57,030
task

00:01:53,040 --> 00:01:59,040
Matthew Rocklin so probably if you think

00:01:57,030 --> 00:02:01,290
last year we had Matthew Rocklin and my

00:01:59,040 --> 00:02:03,750
talk is about tasks and pandas so

00:02:01,290 --> 00:02:06,600
probably who would be the nice keynote

00:02:03,750 --> 00:02:09,569
speaker for this year so that's brand

00:02:06,600 --> 00:02:11,610
news we have gotten West McKinney as a

00:02:09,569 --> 00:02:12,980
keynote speech keynote speaker so he's

00:02:11,610 --> 00:02:16,130
the founder of panda

00:02:12,980 --> 00:02:17,900
here's a PMC member at Apache arrow and

00:02:16,130 --> 00:02:20,690
 so that's what I'm talking about

00:02:17,900 --> 00:02:23,750
and yeah he is also with the Apache

00:02:20,690 --> 00:02:25,790
Software Foundation so I'm very proud on

00:02:23,750 --> 00:02:28,310
that we have got him as a keynote

00:02:25,790 --> 00:02:29,990
speaker because it touches lots of area

00:02:28,310 --> 00:02:32,600
that we do in the company and I am

00:02:29,990 --> 00:02:35,840
personally interested in we have already

00:02:32,600 --> 00:02:37,580
saw sold 150 tickets so if you are

00:02:35,840 --> 00:02:40,190
interested you really should hurry up

00:02:37,580 --> 00:02:44,780
because once we are getting a schedule

00:02:40,190 --> 00:02:46,580
out yeah the tickets will just go by

00:02:44,780 --> 00:02:49,489
with a blink of an eye

00:02:46,580 --> 00:02:53,530
and we expect to be sold out once we

00:02:49,489 --> 00:02:58,489
push the push detox out

00:02:53,530 --> 00:03:01,730
so let's come back to my talk today I

00:02:58,489 --> 00:03:04,700
will talk about columnar data in Apache

00:03:01,730 --> 00:03:05,780
 and how we use pandas and tasks to

00:03:04,700 --> 00:03:10,820
work with it

00:03:05,780 --> 00:03:14,810
API wise it's not that much in upon us

00:03:10,820 --> 00:03:18,320
in tasks to work with it but I will tell

00:03:14,810 --> 00:03:21,019
you a little bit more about the

00:03:18,320 --> 00:03:24,049
challenges and the motivations we had in

00:03:21,019 --> 00:03:27,380
our software architecture to that we

00:03:24,049 --> 00:03:30,170
need to change and then we came to the

00:03:27,380 --> 00:03:33,290
conclusion that Apache Apache is a nice

00:03:30,170 --> 00:03:35,150
storage technology for us and I'll show

00:03:33,290 --> 00:03:37,299
how we use partners in tasks to work

00:03:35,150 --> 00:03:42,019
with it and how it improved our

00:03:37,299 --> 00:03:44,269
architecture in our company so what do

00:03:42,019 --> 00:03:48,650
we do at the end of a machine learning

00:03:44,269 --> 00:03:51,230
company we work in the retail space so

00:03:48,650 --> 00:03:54,860
on business model is that the customer

00:03:51,230 --> 00:03:57,680
that's big retail change a chains they

00:03:54,860 --> 00:04:00,769
send us their customer data so mostly

00:03:57,680 --> 00:04:04,370
sales and stocks and product information

00:04:00,769 --> 00:04:07,579
and store information we put all this

00:04:04,370 --> 00:04:09,230
data in a big sequel database then we

00:04:07,579 --> 00:04:12,500
have all machine learning algorithms

00:04:09,230 --> 00:04:15,799
they we train it on the historical data

00:04:12,500 --> 00:04:17,840
and then we can make predictions for the

00:04:15,799 --> 00:04:21,530
future about future sales in different

00:04:17,840 --> 00:04:24,200
stores and then we use this probability

00:04:21,530 --> 00:04:27,009
density function to a build

00:04:24,200 --> 00:04:30,370
replenishment on top of it so we tell

00:04:27,009 --> 00:04:35,110
supply chains everyday you need to order

00:04:30,370 --> 00:04:38,020
this store this amount of products for

00:04:35,110 --> 00:04:39,909
the next day so that you are not going

00:04:38,020 --> 00:04:43,569
out of stock and you are not having too

00:04:39,909 --> 00:04:48,639
much waste that's all business model we

00:04:43,569 --> 00:04:51,190
work quiet with quite some data use

00:04:48,639 --> 00:04:55,870
mostly supervised regression for our

00:04:51,190 --> 00:04:58,479
models so we have we get lots of data

00:04:55,870 --> 00:05:00,580
from the customer but we have also some

00:04:58,479 --> 00:05:03,909
other additional data sources like

00:05:00,580 --> 00:05:07,000
events or weather data and based on this

00:05:03,909 --> 00:05:09,819
that we basically built a huge matrix

00:05:07,000 --> 00:05:12,310
with features for each product location

00:05:09,819 --> 00:05:15,400
and date combination and we use the

00:05:12,310 --> 00:05:18,520
historical values to train our data to

00:05:15,400 --> 00:05:20,800
train our models and then run on a daily

00:05:18,520 --> 00:05:25,530
basis or machine learning pipeline to

00:05:20,800 --> 00:05:29,289
each day generate new sales forecasts

00:05:25,530 --> 00:05:32,440
I'm not going any more into the machine

00:05:29,289 --> 00:05:36,159
learning today the only thing that is

00:05:32,440 --> 00:05:38,380
important so normally we have historical

00:05:36,159 --> 00:05:41,229
data from our clients about three to

00:05:38,380 --> 00:05:43,900
five years and that's up to a hundred

00:05:41,229 --> 00:05:48,819
billion records so that's quite some

00:05:43,900 --> 00:05:51,370
stuff let's look at maybe we should go

00:05:48,819 --> 00:05:54,639
back one or two years in time

00:05:51,370 --> 00:05:58,090
so how was our data flow at our company

00:05:54,639 --> 00:06:01,840
so we had this or we have this huge

00:05:58,090 --> 00:06:04,740
clusters of in-memory columnar

00:06:01,840 --> 00:06:07,900
distributed sequel a data basis its

00:06:04,740 --> 00:06:10,300
proprietary a software called accessible

00:06:07,900 --> 00:06:13,240
we normally won this before two eighth

00:06:10,300 --> 00:06:18,250
notes database clusters and it keeps all

00:06:13,240 --> 00:06:20,620
the data in memory so the customer sends

00:06:18,250 --> 00:06:24,340
us data every day we put it into the

00:06:20,620 --> 00:06:26,289
database then we have machine learning

00:06:24,340 --> 00:06:29,110
cluster based on Apache measles and

00:06:26,289 --> 00:06:31,810
Aurora we run an open source of like

00:06:29,110 --> 00:06:34,210
Jupiter tasks find us on top of it we

00:06:31,810 --> 00:06:36,219
pull the data out again from the excess

00:06:34,210 --> 00:06:37,520
oil from the database do the machine

00:06:36,219 --> 00:06:39,770
learning

00:06:37,520 --> 00:06:42,740
after this we insert it back in the

00:06:39,770 --> 00:06:45,860
database and then the customer pulls it

00:06:42,740 --> 00:06:48,949
from the database to give you some

00:06:45,860 --> 00:06:51,770
numbers if we calculate predictions for

00:06:48,949 --> 00:06:54,470
a triple killer is retailer that's about

00:06:51,770 --> 00:06:56,900
twenty thousand products about five

00:06:54,470 --> 00:07:00,379
hundred stores and we calculate 20 days

00:06:56,900 --> 00:07:03,319
of horizon so we calculate each day 20

00:07:00,379 --> 00:07:05,240
millions of decisions for each day for

00:07:03,319 --> 00:07:07,039
each customer and all this data has to

00:07:05,240 --> 00:07:10,120
go out of the exit out of the database

00:07:07,039 --> 00:07:13,610
and go go back into the database again

00:07:10,120 --> 00:07:16,099
so that's where we already see where we

00:07:13,610 --> 00:07:19,250
have a bottleneck on the one side we can

00:07:16,099 --> 00:07:21,440
really true your parallely size our

00:07:19,250 --> 00:07:23,180
machine learning algorithms based on

00:07:21,440 --> 00:07:25,669
clustering on product groups or

00:07:23,180 --> 00:07:29,479
locations so we really can scale this

00:07:25,669 --> 00:07:32,240
out to 500 workers with no problems but

00:07:29,479 --> 00:07:35,120
in front we have this database where we

00:07:32,240 --> 00:07:37,190
can at max pull out data with ten or

00:07:35,120 --> 00:07:41,090
fifteen connections and then the

00:07:37,190 --> 00:07:43,880
database is one full load and even worse

00:07:41,090 --> 00:07:45,949
we want to insert back the data into the

00:07:43,880 --> 00:07:49,219
database because a full table logs we

00:07:45,949 --> 00:07:52,430
only can insert with one connection at a

00:07:49,219 --> 00:07:54,319
time inserting Europe is about twenty

00:07:52,430 --> 00:07:57,409
five thousand rows a second if you

00:07:54,319 --> 00:07:59,240
calculated up we already use about two

00:07:57,409 --> 00:08:00,520
hours only to get the data back into the

00:07:59,240 --> 00:08:04,580
database

00:08:00,520 --> 00:08:06,620
if you working in the retail space and

00:08:04,580 --> 00:08:09,889
you look at the operations clock from a

00:08:06,620 --> 00:08:11,539
typical retailer two hours delay because

00:08:09,889 --> 00:08:14,029
we need to get all the data in the

00:08:11,539 --> 00:08:16,699
database that's a huge amount of time

00:08:14,029 --> 00:08:20,060
for them so we figured out okay we need

00:08:16,699 --> 00:08:23,000
to go bad we need to be better our

00:08:20,060 --> 00:08:25,219
conclusion was always Python is a really

00:08:23,000 --> 00:08:27,289
good companion for the data scientists

00:08:25,219 --> 00:08:31,610
they are really happy with it they like

00:08:27,289 --> 00:08:35,630
to build data data models all the stuff

00:08:31,610 --> 00:08:38,180
they really like it but it's has not

00:08:35,630 --> 00:08:44,089
been the best to move large data in and

00:08:38,180 --> 00:08:48,199
out so why do I care or what do we think

00:08:44,089 --> 00:08:50,850
about most of our data are huge time

00:08:48,199 --> 00:08:53,009
serious they

00:08:50,850 --> 00:08:55,800
ooh not a necessary need to be in a

00:08:53,009 --> 00:08:58,050
database so we would really like to have

00:08:55,800 --> 00:09:01,170
this time series in a distributed file

00:08:58,050 --> 00:09:04,410
system with a non-uniform schema we like

00:09:01,170 --> 00:09:06,000
to execute queries on top of it but most

00:09:04,410 --> 00:09:10,040
of the time you're only interested in a

00:09:06,000 --> 00:09:10,040
subset of the data are not all the data

00:09:11,690 --> 00:09:19,259
there are lots of technologies out there

00:09:15,240 --> 00:09:22,550
that work very nice with this kind of

00:09:19,259 --> 00:09:25,829
constraints but they are mostly in the

00:09:22,550 --> 00:09:28,410
Java ecosystem so they are not available

00:09:25,829 --> 00:09:35,660
in Python or like pesto or Impala or

00:09:28,410 --> 00:09:37,440
drill so that's amazing that they are

00:09:35,660 --> 00:09:39,690
available in Java

00:09:37,440 --> 00:09:42,240
but we needed something that's available

00:09:39,690 --> 00:09:47,430
in Python to really get our data in and

00:09:42,240 --> 00:09:49,889
out so the obvious solution for us was

00:09:47,430 --> 00:09:53,220
two years ago okay let's look into a

00:09:49,889 --> 00:09:56,699
pocket as a file format and look into a

00:09:53,220 --> 00:10:00,089
blob storage as a storage technology to

00:09:56,699 --> 00:10:04,019
get the data better into our machine

00:10:00,089 --> 00:10:07,680
learning pipelines two years ago there

00:10:04,019 --> 00:10:12,540
was not a really good Pocky library for

00:10:07,680 --> 00:10:14,819
Python or with some work from us and

00:10:12,540 --> 00:10:20,250
others this has changed so we're really

00:10:14,819 --> 00:10:21,899
happy with this before this was or this

00:10:20,250 --> 00:10:24,420
is a general problem in all all

00:10:21,899 --> 00:10:28,769
languages if you want to access data

00:10:24,420 --> 00:10:31,639
data or data formats you have often very

00:10:28,769 --> 00:10:34,259
good into our probability within you a

00:10:31,639 --> 00:10:37,110
ecosystem so in the Python world is

00:10:34,259 --> 00:10:40,649
mostly backed by an umpire but you have

00:10:37,110 --> 00:10:43,649
poor integration with other systems and

00:10:40,649 --> 00:10:46,050
theory a memory copy is very fast you

00:10:43,649 --> 00:10:48,630
have a little bit of D on serialization

00:10:46,050 --> 00:10:52,439
on top of it but it shouldn't be that

00:10:48,630 --> 00:10:55,949
big of an issue if you go back two years

00:10:52,439 --> 00:10:58,740
in time most of the time the solution

00:10:55,949 --> 00:11:02,579
for cross language data sharing was you

00:10:58,740 --> 00:11:06,180
need to go with CSV but CSV is not a

00:11:02,579 --> 00:11:12,329
file format it's just text and yeah I

00:11:06,180 --> 00:11:16,260
don't like it so the obvious solution is

00:11:12,329 --> 00:11:18,600
Pocky Pocky is a binary columnar data

00:11:16,260 --> 00:11:21,870
format give you a little bit of

00:11:18,600 --> 00:11:26,579
explanation what this means so typical

00:11:21,870 --> 00:11:29,309
records that we operate on always like

00:11:26,579 --> 00:11:32,760
on this day in this location we have

00:11:29,309 --> 00:11:34,380
sold this amount of products and of

00:11:32,760 --> 00:11:36,600
course in our real machine learning

00:11:34,380 --> 00:11:38,519
pipelines we have many more features but

00:11:36,600 --> 00:11:40,800
it's always a little bit the same

00:11:38,519 --> 00:11:43,620
looking at this so if you look at

00:11:40,800 --> 00:11:45,750
traditional database systems or the

00:11:43,620 --> 00:11:48,000
memory buffer how it's stored it's in a

00:11:45,750 --> 00:11:53,250
row based fashion so you write row after

00:11:48,000 --> 00:11:55,920
row into your memory whereas columnar

00:11:53,250 --> 00:11:59,850
storage takes a different approach so it

00:11:55,920 --> 00:12:02,040
writes all the cells from one column

00:11:59,850 --> 00:12:05,610
after it and then the next column and

00:12:02,040 --> 00:12:08,220
then the next column this might not be

00:12:05,610 --> 00:12:10,829
the optimal approach if you are only

00:12:08,220 --> 00:12:12,269
interested to work with one row because

00:12:10,829 --> 00:12:14,910
then the traditional memory buffer

00:12:12,269 --> 00:12:18,899
buffer is better you can select one row

00:12:14,910 --> 00:12:21,600
and just read the records that with this

00:12:18,899 --> 00:12:23,550
row but the columnar storage is much

00:12:21,600 --> 00:12:26,730
more efficient if you want to work on

00:12:23,550 --> 00:12:28,829
huge blocks of your data because you can

00:12:26,730 --> 00:12:33,240
fetch all the data it passes into two

00:12:28,829 --> 00:12:34,439
caches of your CPU and you can work with

00:12:33,240 --> 00:12:38,399
this blocks much better

00:12:34,439 --> 00:12:41,910
so pokey it's a columnar storage it was

00:12:38,399 --> 00:12:45,120
started in 2012 by Claudia on Twitter so

00:12:41,910 --> 00:12:46,949
it's in the Hadoop ecosystem or the

00:12:45,120 --> 00:12:50,189
Bourne and they had ecosystem the first

00:12:46,949 --> 00:12:54,089
implementations were in Java and the

00:12:50,189 --> 00:12:57,720
first 1.0 release was in 2013 it's now

00:12:54,089 --> 00:13:00,269
an apache top-level project and starting

00:12:57,720 --> 00:13:04,769
in fall 2016 and there was the first

00:13:00,269 --> 00:13:06,779
Pisan and c++ support at the moment in

00:13:04,769 --> 00:13:08,399
the Hadoop or in the Big Data Java

00:13:06,779 --> 00:13:12,839
ecosystem it's the state of the art

00:13:08,399 --> 00:13:15,630
default IO watch so why should you use

00:13:12,839 --> 00:13:16,350
parki as I said earlier to columnar

00:13:15,630 --> 00:13:19,890
format

00:13:16,350 --> 00:13:21,780
very good for vectorized operations it

00:13:19,890 --> 00:13:25,890
has very efficient encoding and

00:13:21,780 --> 00:13:27,810
compressions you can use protected push

00:13:25,890 --> 00:13:29,820
down to only read the data that you need

00:13:27,810 --> 00:13:31,830
I'll tell you later a little bit more

00:13:29,820 --> 00:13:33,810
how this works and it's an

00:13:31,830 --> 00:13:38,520
language-independent format so you have

00:13:33,810 --> 00:13:42,870
lips in Java Scala C++ and finally a big

00:13:38,520 --> 00:13:46,140
advantage over for example CSV it's that

00:13:42,870 --> 00:13:48,990
it saw splittable format so that you can

00:13:46,140 --> 00:13:51,600
only read parts of your file that you

00:13:48,990 --> 00:13:56,130
are interested in so for example in CSV

00:13:51,600 --> 00:13:58,080
you can't just jump to the row 100000

00:13:56,130 --> 00:14:00,270
and read a blog of it and you have

00:13:58,080 --> 00:14:01,940
always to seek to all the data to see

00:14:00,270 --> 00:14:05,400
where you are

00:14:01,940 --> 00:14:07,560
Punky's is much better you had have

00:14:05,400 --> 00:14:10,080
compression at a column level especially

00:14:07,560 --> 00:14:12,930
with the column format of your data this

00:14:10,080 --> 00:14:15,030
is very efficient and you have also rich

00:14:12,930 --> 00:14:18,570
metadata so you have a schema attached

00:14:15,030 --> 00:14:23,790
to your file and statistics which you

00:14:18,570 --> 00:14:26,400
can use to efficiently read your data at

00:14:23,790 --> 00:14:29,490
the moment there are two implementations

00:14:26,400 --> 00:14:32,280
that you can use in python to work with

00:14:29,490 --> 00:14:35,390
Sparky the one is the fast park a

00:14:32,280 --> 00:14:38,820
library it's more from from the task

00:14:35,390 --> 00:14:40,380
people they have implemented it and the

00:14:38,820 --> 00:14:43,410
other one is that the Apache Aero

00:14:40,380 --> 00:14:47,060
project that's mostly driven by West

00:14:43,410 --> 00:14:49,680
makini what is Apache arrow so it's a

00:14:47,060 --> 00:14:51,960
specification for an in-memory columnar

00:14:49,680 --> 00:14:54,620
data format so it Maps very good to the

00:14:51,960 --> 00:14:58,310
pocky on disk file format

00:14:54,620 --> 00:15:02,790
it's an language independent

00:14:58,310 --> 00:15:05,730
implementation of a memory format so the

00:15:02,790 --> 00:15:09,350
implementation is in C or C++ and you

00:15:05,730 --> 00:15:13,580
can expose the API through to other

00:15:09,350 --> 00:15:16,380
programming languages and so you the

00:15:13,580 --> 00:15:21,120
support for park' was brought to pandas

00:15:16,380 --> 00:15:23,730
in park' c++ without any additional code

00:15:21,120 --> 00:15:26,850
to read it and you use can use the

00:15:23,730 --> 00:15:30,920
apache arrow project with also other

00:15:26,850 --> 00:15:30,920
technologies like spark or drill

00:15:33,200 --> 00:15:40,710
so let's have a look at the parquet file

00:15:36,660 --> 00:15:44,550
structure as I said it's an on disk

00:15:40,710 --> 00:15:49,410
format each file splits up into row

00:15:44,550 --> 00:15:52,290
groups so row groups are about five

00:15:49,410 --> 00:15:55,920
megabyte to one gigabyte of data and

00:15:52,290 --> 00:15:58,040
it's a number from I would say ten

00:15:55,920 --> 00:16:02,460
thousand two hundred thousand rows

00:15:58,040 --> 00:16:06,510
within a row group you have column

00:16:02,460 --> 00:16:09,089
chunks so it's a columnar file format so

00:16:06,510 --> 00:16:11,700
for each column you write the data for

00:16:09,089 --> 00:16:14,820
each record one after another and then

00:16:11,700 --> 00:16:18,800
you have the page units within a column

00:16:14,820 --> 00:16:22,770
chunk that is used for compression and

00:16:18,800 --> 00:16:27,029
efficient storage and at the end of the

00:16:22,770 --> 00:16:29,460
file you have statistics this is on the

00:16:27,029 --> 00:16:32,040
media data of the file this is a very

00:16:29,460 --> 00:16:34,760
nice feature because with this way you

00:16:32,040 --> 00:16:38,339
can stream the data into a pack a file

00:16:34,760 --> 00:16:40,860
just keep records about the metadata and

00:16:38,339 --> 00:16:42,960
statistics and once you are done just by

00:16:40,860 --> 00:16:44,910
the statistics so you don't have to

00:16:42,960 --> 00:16:46,950
write the statistic at first

00:16:44,910 --> 00:16:49,260
or even worse keep all the data in

00:16:46,950 --> 00:16:53,730
memory before you write it but you can

00:16:49,260 --> 00:16:56,280
stream it out now if you want to work

00:16:53,730 --> 00:17:02,310
with pison and pack a file how does this

00:16:56,280 --> 00:17:04,740
look like so it's very easy you just

00:17:02,310 --> 00:17:08,790
have the pandas top level function read

00:17:04,740 --> 00:17:12,480
park' and you can either pass it a file

00:17:08,790 --> 00:17:17,610
handle or a file name and the first

00:17:12,480 --> 00:17:20,189
thing if you are interested in in I mean

00:17:17,610 --> 00:17:22,280
if it's efficient read of parkia files

00:17:20,189 --> 00:17:24,870
is that you should only specify this

00:17:22,280 --> 00:17:27,839
columns that you are interested to read

00:17:24,870 --> 00:17:30,510
in so if you have a pack a file with

00:17:27,839 --> 00:17:32,520
like I said earlier five columns and you

00:17:30,510 --> 00:17:36,210
are only interested in the day and the

00:17:32,520 --> 00:17:39,150
sale if you pass it in you will also

00:17:36,210 --> 00:17:41,160
only read the data from the disk from

00:17:39,150 --> 00:17:42,570
the columns that you have read that you

00:17:41,160 --> 00:17:45,570
have specified and

00:17:42,570 --> 00:17:47,700
Calem projection in column order this is

00:17:45,570 --> 00:17:49,830
also a fear feature that I contributed

00:17:47,700 --> 00:17:51,990
to pandas long that's been my first

00:17:49,830 --> 00:17:54,389
contribution to pandas I'm pretty proud

00:17:51,990 --> 00:17:57,919
of it but it was basically just passing

00:17:54,389 --> 00:18:00,539
it through the underlying our libraries

00:17:57,919 --> 00:18:02,610
yeah and as you can see if you pass the

00:18:00,539 --> 00:18:09,570
columns you will only read from the disk

00:18:02,610 --> 00:18:12,330
the columns that you requested what's

00:18:09,570 --> 00:18:14,190
next what you should care about if you

00:18:12,330 --> 00:18:16,889
want to work with efficient

00:18:14,190 --> 00:18:19,830
whisp our case predicate push down so it

00:18:16,889 --> 00:18:21,929
also you don't not only want to read the

00:18:19,830 --> 00:18:27,090
columns that you are interested in but

00:18:21,929 --> 00:18:29,460
also split date skip data that is not

00:18:27,090 --> 00:18:32,610
relevant for relevant for you this saves

00:18:29,460 --> 00:18:34,830
of course IO load as the data doesn't

00:18:32,610 --> 00:18:38,250
need to be transferred and this also

00:18:34,830 --> 00:18:41,279
saves CPU cycles because the data

00:18:38,250 --> 00:18:44,639
doesn't need to be decoded example here

00:18:41,279 --> 00:18:48,809
if you want to know which product are

00:18:44,639 --> 00:18:54,539
sold in the last you can only choose to

00:18:48,809 --> 00:18:58,950
select the columns product and dollar

00:18:54,539 --> 00:19:01,830
column and then you can filter out the

00:18:58,950 --> 00:19:04,950
euros and if you have the statistics on

00:19:01,830 --> 00:19:06,720
for the on file representation you can

00:19:04,950 --> 00:19:11,100
only read the cells that you are

00:19:06,720 --> 00:19:13,320
interested in predicate push down is at

00:19:11,100 --> 00:19:15,929
the moment only available in the first

00:19:13,320 --> 00:19:18,419
Park here a library in with the Python

00:19:15,929 --> 00:19:21,299
interface it's planned for pi arrow -

00:19:18,419 --> 00:19:24,360
it's already already there in the

00:19:21,299 --> 00:19:28,289
underlying park a C++ implementation but

00:19:24,360 --> 00:19:31,710
it's not yet exposed to the API in

00:19:28,289 --> 00:19:36,809
Python or to pandas so what you can do

00:19:31,710 --> 00:19:39,539
you can have a very easy filter syntax

00:19:36,809 --> 00:19:42,600
so you can specify the column and

00:19:39,539 --> 00:19:46,799
operation so equals big or smaller in

00:19:42,600 --> 00:19:51,690
not in and then a value and this is then

00:19:46,799 --> 00:19:54,880
used to skip the reading of certain

00:19:51,690 --> 00:19:59,890
parts in your file

00:19:54,880 --> 00:20:01,900
have a look for an example if you use so

00:19:59,890 --> 00:20:05,650
you can switch the engine between fast

00:20:01,900 --> 00:20:09,010
Pocky and pie arrow in pandas to read

00:20:05,650 --> 00:20:11,620
from the file so if you want to read the

00:20:09,010 --> 00:20:16,260
data and you are only interested in

00:20:11,620 --> 00:20:19,720
certain locations you can use this to

00:20:16,260 --> 00:20:21,610
skip reading of whole row groups and

00:20:19,720 --> 00:20:24,610
just read the row group you are

00:20:21,610 --> 00:20:27,730
interested in how is this done if you

00:20:24,610 --> 00:20:30,250
write the data through a file it

00:20:27,730 --> 00:20:34,150
also writes statistics and metadata

00:20:30,250 --> 00:20:37,810
about the row groups so for example here

00:20:34,150 --> 00:20:40,300
it writes for every column the min and

00:20:37,810 --> 00:20:44,470
the max values and if you read this

00:20:40,300 --> 00:20:46,690
metadata first then you can use this to

00:20:44,470 --> 00:20:51,100
skip the whole roll group if it doesn't

00:20:46,690 --> 00:20:54,460
match your predicate this works only

00:20:51,100 --> 00:20:58,360
very good if you work on search unsorted

00:20:54,460 --> 00:21:01,630
data because otherwise if you have

00:20:58,360 --> 00:21:04,720
randomly distributed columns more or

00:21:01,630 --> 00:21:06,190
less in each row group you will have the

00:21:04,720 --> 00:21:09,610
min/max values from your whole

00:21:06,190 --> 00:21:11,350
distribution of your data so if you want

00:21:09,610 --> 00:21:13,870
to use this feature you should make sure

00:21:11,350 --> 00:21:15,100
that if you write the data to disk that

00:21:13,870 --> 00:21:17,380
it's in a sorted order

00:21:15,100 --> 00:21:22,090
and then this will really speed up your

00:21:17,380 --> 00:21:30,970
you reach but otherwise you won't gain

00:21:22,090 --> 00:21:35,590
that much another trick that you can use

00:21:30,970 --> 00:21:38,710
on the one hand to minimize the data

00:21:35,590 --> 00:21:41,290
storage size on disk and also to speed

00:21:38,710 --> 00:21:44,590
up reading later is to use dictionary

00:21:41,290 --> 00:21:48,370
encoding this is a very good fit for

00:21:44,590 --> 00:21:52,330
pandas categoricals so if you have for

00:21:48,370 --> 00:21:54,730
example for your product the color of

00:21:52,330 --> 00:21:57,270
your product and it's always from the

00:21:54,730 --> 00:22:01,390
same category like blue red blue yellow

00:21:57,270 --> 00:22:03,880
you can use dictionary encoding and you

00:22:01,390 --> 00:22:06,880
can specify this and parquet will then

00:22:03,880 --> 00:22:08,650
store all the data use the dictionary

00:22:06,880 --> 00:22:11,770
and coding on disk

00:22:08,650 --> 00:22:15,790
you only store the name of your value

00:22:11,770 --> 00:22:18,309
once and then just store pointers to the

00:22:15,790 --> 00:22:21,070
value and this is much more efficient

00:22:18,309 --> 00:22:23,800
and on top of this you also could use

00:22:21,070 --> 00:22:27,130
run to run length encoding on the values

00:22:23,800 --> 00:22:32,830
so you this will also get smaller and

00:22:27,130 --> 00:22:35,440
then if you want to filter again based

00:22:32,830 --> 00:22:38,050
on on the color or here if the example

00:22:35,440 --> 00:22:40,780
was a sales type and then you can just

00:22:38,050 --> 00:22:45,220
check in the dictionary for this row

00:22:40,780 --> 00:22:47,380
group if your predicate matches this row

00:22:45,220 --> 00:22:50,679
group then reached read the data and

00:22:47,380 --> 00:22:52,990
otherwise not read it so basically the

00:22:50,679 --> 00:22:54,700
dictionary encoding you can work with it

00:22:52,990 --> 00:22:57,760
like a bloom filter but you don't have

00:22:54,700 --> 00:23:03,280
the false positives it works

00:22:57,760 --> 00:23:04,929
the performance penalty for adding

00:23:03,280 --> 00:23:07,390
dictionaries it's not that

00:23:04,929 --> 00:23:12,929
that high it's just about 1% of the

00:23:07,390 --> 00:23:16,120
reading reading time to to use it all

00:23:12,929 --> 00:23:18,400
the values in the column need to be

00:23:16,120 --> 00:23:20,710
dictionary encoded so for all row groups

00:23:18,400 --> 00:23:24,280
and all page chunks really must use the

00:23:20,710 --> 00:23:27,280
dictionary encoding and this is

00:23:24,280 --> 00:23:31,059
especially helpful if you have not

00:23:27,280 --> 00:23:33,280
sorted the column so you still can

00:23:31,059 --> 00:23:38,640
benefit from partial reading of your

00:23:33,280 --> 00:23:41,530
data so again if you look at the

00:23:38,640 --> 00:23:43,300
dictionary of each row group first and

00:23:41,530 --> 00:23:45,670
then you can decide ok to skip a whole

00:23:43,300 --> 00:23:47,640
row group and not read it from this and

00:23:45,670 --> 00:23:55,240
this will massively speed up your

00:23:47,640 --> 00:23:58,090
applications beside compression and

00:23:55,240 --> 00:24:00,780
encoding on a column level parky has

00:23:58,090 --> 00:24:03,940
compression on the whole plaque a file

00:24:00,780 --> 00:24:07,150
so you can shrink the data independent

00:24:03,940 --> 00:24:11,320
of its content it's more CPU intensive

00:24:07,150 --> 00:24:12,820
than the encoding but encoding and

00:24:11,320 --> 00:24:16,360
compression works better than

00:24:12,820 --> 00:24:19,570
compression alone and available are gzip

00:24:16,360 --> 00:24:21,470
snappy and proudly proudly is the newest

00:24:19,570 --> 00:24:24,650
one of the compression

00:24:21,470 --> 00:24:26,480
if you don't want to go deep and what

00:24:24,650 --> 00:24:26,900
what compression engine is the best for

00:24:26,480 --> 00:24:29,210
your data

00:24:26,900 --> 00:24:32,990
I'm just stick with stillness snappy and

00:24:29,210 --> 00:24:36,650
that's what we have seen here are some

00:24:32,990 --> 00:24:38,809
surge comparisons that we have done so

00:24:36,650 --> 00:24:43,220
it's I think it's based from the New

00:24:38,809 --> 00:24:45,350
York taxi data records the columns are I

00:24:43,220 --> 00:24:48,770
think a boo-boo at a time float and I

00:24:45,350 --> 00:24:53,150
int and we have tried to store it in

00:24:48,770 --> 00:24:56,390
different different formats and you see

00:24:53,150 --> 00:24:59,360
but parkie you are really you have a

00:24:56,390 --> 00:25:04,299
compression of factor 10 and you still

00:24:59,360 --> 00:25:04,299
read it much faster than for example CSV

00:25:04,929 --> 00:25:11,419
another trick that we very often use and

00:25:08,659 --> 00:25:14,390
this time we'll switch to task is data

00:25:11,419 --> 00:25:19,220
partitioning so until now we have always

00:25:14,390 --> 00:25:22,220
read from one file and just read parts

00:25:19,220 --> 00:25:24,860
of a file this is fine if you work on a

00:25:22,220 --> 00:25:28,100
lot local machine or Crysta data only

00:25:24,860 --> 00:25:30,080
from one consumer but as soon as you

00:25:28,100 --> 00:25:32,570
have a distributed system and work with

00:25:30,080 --> 00:25:36,350
multiple consumers and especially this

00:25:32,570 --> 00:25:40,549
multiple producers it makes sense to

00:25:36,350 --> 00:25:42,770
split the data into multiple files there

00:25:40,549 --> 00:25:45,919
is it's not a really a standard but it's

00:25:42,770 --> 00:25:48,860
based on beehive partitioning scheme so

00:25:45,919 --> 00:25:51,860
that you basically encode the

00:25:48,860 --> 00:25:54,620
partitioning scheme in your file name or

00:25:51,860 --> 00:25:58,340
in the past so for example so we have a

00:25:54,620 --> 00:26:00,380
directory date 2070 is your 1 0 1 and

00:25:58,340 --> 00:26:03,500
another directory within it location

00:26:00,380 --> 00:26:06,159
equals location 0 and then you have

00:26:03,500 --> 00:26:09,799
multiple parts of your pocket files and

00:26:06,159 --> 00:26:12,470
with dust you can create this kind of

00:26:09,799 --> 00:26:14,720
structure but also dust can be used to

00:26:12,470 --> 00:26:17,000
auto detect the structure and read the

00:26:14,720 --> 00:26:21,409
data back in so it just handled in to

00:26:17,000 --> 00:26:23,390
task directory and it also yeah

00:26:21,409 --> 00:26:26,390
it recognizes the structure of the

00:26:23,390 --> 00:26:30,860
directory and you can also use

00:26:26,390 --> 00:26:34,070
predicates to predicate push down and

00:26:30,860 --> 00:26:34,980
this in this way to only read the files

00:26:34,070 --> 00:26:41,100
where the

00:26:34,980 --> 00:26:43,309
barricade of your file path match if you

00:26:41,100 --> 00:26:45,600
are working in a distributed system

00:26:43,309 --> 00:26:48,480
sooner or later you have to move away

00:26:45,600 --> 00:26:51,360
from your local computer and let

00:26:48,480 --> 00:26:54,419
computation from storage the approach

00:26:51,360 --> 00:26:57,570
that we did was so we didn't want to

00:26:54,419 --> 00:27:00,270
host the HDFS file system on our own so

00:26:57,570 --> 00:27:02,970
we are running on Asia and then we

00:27:00,270 --> 00:27:08,549
decided to use the azure blob storage to

00:27:02,970 --> 00:27:11,760
store all our data the park' files you

00:27:08,549 --> 00:27:16,380
can pass into to to parque order by Eero

00:27:11,760 --> 00:27:18,870
either a file handle or a local file or

00:27:16,380 --> 00:27:22,230
something that behaves like a Python

00:27:18,870 --> 00:27:25,200
file or bi object so this is an example

00:27:22,230 --> 00:27:27,360
if you just get a stream or a file

00:27:25,200 --> 00:27:29,880
object from the blop blop service this

00:27:27,360 --> 00:27:32,840
way how do you access as a blob storage

00:27:29,880 --> 00:27:37,049
then pass it in to the park every table

00:27:32,840 --> 00:27:40,860
and then go to pandas and to get the

00:27:37,049 --> 00:27:44,429
data into pandas this works very well if

00:27:40,860 --> 00:27:46,830
you want to work with the whole data but

00:27:44,429 --> 00:27:49,440
you'll lose if you take the streaming

00:27:46,830 --> 00:27:52,169
approach you lose all the benefits that

00:27:49,440 --> 00:27:53,760
I explained before that we predict I'd

00:27:52,169 --> 00:27:57,140
push down and only to read the data

00:27:53,760 --> 00:28:01,799
you're interested in so we have also

00:27:57,140 --> 00:28:05,880
written an interface for the azure blob

00:28:01,799 --> 00:28:09,360
storage this implements the Python file

00:28:05,880 --> 00:28:11,160
interface so we have an i/o interface

00:28:09,360 --> 00:28:13,770
where you can tell it but I can see

00:28:11,160 --> 00:28:17,400
where you can read we have open sourced

00:28:13,770 --> 00:28:20,970
it at par as part of the simple kv

00:28:17,400 --> 00:28:23,040
library so simple kv as an abstraction

00:28:20,970 --> 00:28:27,270
layer over different storage

00:28:23,040 --> 00:28:30,510
technologies and this helps us to gain

00:28:27,270 --> 00:28:33,240
the benefit of predicate push down and

00:28:30,510 --> 00:28:35,570
also store the data on a remote

00:28:33,240 --> 00:28:35,570
filesystem

00:28:37,940 --> 00:28:44,820
another real life improvement that we

00:28:41,400 --> 00:28:46,250
have seen with apache arrow so as I told

00:28:44,820 --> 00:28:49,100
earlier

00:28:46,250 --> 00:28:50,750
I texture is that we need to get the

00:28:49,100 --> 00:28:54,410
data out of the database and then

00:28:50,750 --> 00:28:56,950
analyze it and find us the data in the

00:28:54,410 --> 00:28:59,750
database is stored in a columnar form

00:28:56,950 --> 00:29:01,220
until two years ago if we wanted to pull

00:28:59,750 --> 00:29:05,990
the data out

00:29:01,220 --> 00:29:09,350
we used PI ODBC as an layer to talk to

00:29:05,990 --> 00:29:11,900
the database but PI ODBC works in a

00:29:09,350 --> 00:29:14,870
robust form so all the data was at once

00:29:11,900 --> 00:29:17,810
feared for my column and data storage to

00:29:14,870 --> 00:29:21,020
the row wise and then when we passed it

00:29:17,810 --> 00:29:24,410
in to pandas it was again converted from

00:29:21,020 --> 00:29:27,590
the row wise to the column wise what we

00:29:24,410 --> 00:29:29,480
did and probably what many people do if

00:29:27,590 --> 00:29:32,360
they want to work with data in pandas

00:29:29,480 --> 00:29:34,130
yeah check the ugly real-life solution

00:29:32,360 --> 00:29:37,970
that was to export the data from the

00:29:34,130 --> 00:29:39,970
database as CSV bypass ODBC in total and

00:29:37,970 --> 00:29:43,070
then we did back into CSV

00:29:39,970 --> 00:29:45,260
yeah but CSV you all you lose all the

00:29:43,070 --> 00:29:49,880
type information and it's just a heck

00:29:45,260 --> 00:29:53,360
it's not a real thing how to do it so

00:29:49,880 --> 00:29:55,850
what was our solution one of my

00:29:53,360 --> 00:30:00,650
colleagues mechanic he implemented two

00:29:55,850 --> 00:30:03,530
ODBC that's an ODBC implementation for

00:30:00,650 --> 00:30:06,250
Python that can also use columnar data

00:30:03,530 --> 00:30:10,130
and also supports the Apache arrow

00:30:06,250 --> 00:30:12,260
standard so you can get the data from

00:30:10,130 --> 00:30:16,360
the database in a columnar fashion and

00:30:12,260 --> 00:30:19,430
then expose patches of PI arrow tables

00:30:16,360 --> 00:30:21,140
to the PI to the Python world and you

00:30:19,430 --> 00:30:24,200
could either then write it away as

00:30:21,140 --> 00:30:26,330
parkie again with nearly zero and memory

00:30:24,200 --> 00:30:29,150
transformations or you can get it into

00:30:26,330 --> 00:30:30,860
pandas and then work with this if you

00:30:29,150 --> 00:30:32,690
are more interested in about in this

00:30:30,860 --> 00:30:35,420
technology or two by GBC

00:30:32,690 --> 00:30:37,670
in general you can see the talk from

00:30:35,420 --> 00:30:39,260
micro punic last year at the PI corn or

00:30:37,670 --> 00:30:43,010
you can check out our tech block where

00:30:39,260 --> 00:30:43,340
we have some really deep dive how are

00:30:43,010 --> 00:30:50,990
you

00:30:43,340 --> 00:30:53,480
implemented this so where are we now

00:30:50,990 --> 00:30:55,610
so we moved away from the direct access

00:30:53,480 --> 00:30:58,700
to the database because it was the

00:30:55,610 --> 00:30:59,530
bottleneck so we now export the data

00:30:58,700 --> 00:31:02,350
from the

00:30:59,530 --> 00:31:05,470
database into the azure blob storage and

00:31:02,350 --> 00:31:07,720
then from there we run our machine

00:31:05,470 --> 00:31:10,030
learning models this is now the huge

00:31:07,720 --> 00:31:12,580
benefit that we really can scale out our

00:31:10,030 --> 00:31:15,070
machine learning models because the

00:31:12,580 --> 00:31:19,660
azure blob storage scales much much

00:31:15,070 --> 00:31:23,500
better than our database and the same

00:31:19,660 --> 00:31:25,650
thing with inserts on Azure blob storage

00:31:23,500 --> 00:31:29,050
we don't have to synchronize the insert

00:31:25,650 --> 00:31:34,720
every avid job can just insert into into

00:31:29,050 --> 00:31:37,420
one blob and then we can work on the

00:31:34,720 --> 00:31:40,480
data to get it back to the customer also

00:31:37,420 --> 00:31:43,180
a nice benefit is that we now use a file

00:31:40,480 --> 00:31:45,940
format which that is shared by different

00:31:43,180 --> 00:31:49,900
implementations that we can now also use

00:31:45,940 --> 00:31:52,750
hive pesto or twill to do analysis of

00:31:49,900 --> 00:31:56,260
the data and that we don't have to store

00:31:52,750 --> 00:31:57,990
this large time series in our in memory

00:31:56,260 --> 00:32:01,210
database which is much more expensive

00:31:57,990 --> 00:32:04,540
than the azure blob storage so there's a

00:32:01,210 --> 00:32:07,300
blob storage for terabytes it's not even

00:32:04,540 --> 00:32:11,320
yeah and then 10 or 15 euros so that's

00:32:07,300 --> 00:32:13,720
that's nothing and the query engines on

00:32:11,320 --> 00:32:15,820
the on the bottom right these are ones

00:32:13,720 --> 00:32:17,830
that you can fire up for the analysis

00:32:15,820 --> 00:32:19,360
and once you are done down you can fire

00:32:17,830 --> 00:32:24,070
them down again and you don't have to

00:32:19,360 --> 00:32:27,190
pay like the a database the whole day to

00:32:24,070 --> 00:32:30,280
keep it running so what we have learned

00:32:27,190 --> 00:32:33,790
is safe in one load in another's

00:32:30,280 --> 00:32:39,940
ecosystem this really helps us but

00:32:33,790 --> 00:32:41,830
always passes the intermediate data so

00:32:39,940 --> 00:32:44,350
if you are interested in this topic too

00:32:41,830 --> 00:32:48,220
and there are still lots of stuff to do

00:32:44,350 --> 00:32:50,530
in the Apache Apache project but also in

00:32:48,220 --> 00:32:53,050
the Aero project there are still lots of

00:32:50,530 --> 00:32:57,160
functions that are not yet exposed to

00:32:53,050 --> 00:33:01,060
Python or pandas or tasks so yeah some

00:32:57,160 --> 00:33:03,490
of my colleagues Apache members and they

00:33:01,060 --> 00:33:09,460
work on this project but there's still

00:33:03,490 --> 00:33:11,440
lots of things to do that's it um my

00:33:09,460 --> 00:33:12,660
talk using find us and asked to work

00:33:11,440 --> 00:33:19,460
with slalom

00:33:12,660 --> 00:33:22,170
[Applause]

00:33:19,460 --> 00:33:25,080
so if you have any questions feel free

00:33:22,170 --> 00:33:27,330
to ask them now or later or just to

00:33:25,080 --> 00:33:29,430
helping me at the conference if you have

00:33:27,330 --> 00:33:34,890
questions how can you quit on this side

00:33:29,430 --> 00:33:37,010
of the room please I'm really happy I

00:33:34,890 --> 00:33:40,020
sit on the left side of the room

00:33:37,010 --> 00:33:41,460
Thanks I remember two years ago you were

00:33:40,020 --> 00:33:43,680
outside Europe - but you were talking

00:33:41,460 --> 00:33:45,960
about spark yes my spark back then yeah

00:33:43,680 --> 00:33:47,610
now I see is that picture which stack

00:33:45,960 --> 00:33:51,810
that doesn't have high spark anymore not

00:33:47,610 --> 00:33:54,300
it's replaced with tasks yeah so just

00:33:51,810 --> 00:33:58,200
the question in that sense desk is ready

00:33:54,300 --> 00:34:02,310
for prime time and not I would not say

00:33:58,200 --> 00:34:05,010
not yet so just go back to PI spark also

00:34:02,310 --> 00:34:07,380
with RO PI spark has really improved all

00:34:05,010 --> 00:34:10,500
the Python access in inside spark has

00:34:07,380 --> 00:34:12,810
really improved also two years ago if

00:34:10,500 --> 00:34:15,570
you go from the JVM world to the Python

00:34:12,810 --> 00:34:17,760
world you have very expensive encoding

00:34:15,570 --> 00:34:19,770
and decoding everywhere view and it was

00:34:17,760 --> 00:34:23,270
really that slow that basically you

00:34:19,770 --> 00:34:26,640
can't use it at all I think you spark

00:34:23,270 --> 00:34:28,560
2.3 they have also introduced arrow for

00:34:26,640 --> 00:34:30,630
the conversation so that now you can

00:34:28,560 --> 00:34:33,960
work in PI spark in person

00:34:30,630 --> 00:34:37,050
we gotta get pandas results and it holds

00:34:33,960 --> 00:34:39,480
serialization is done in arrow so that's

00:34:37,050 --> 00:34:43,230
much faster now but God wasn't available

00:34:39,480 --> 00:34:45,419
so we went for the task direction we are

00:34:43,230 --> 00:34:46,860
still not using tasks everywhere in

00:34:45,419 --> 00:34:49,980
production so we still have an own

00:34:46,860 --> 00:34:52,200
schedule or and some own libraries but

00:34:49,980 --> 00:34:54,000
we plan to do it and we do it because

00:34:52,200 --> 00:34:55,650
it's much more lightweight and much more

00:34:54,000 --> 00:34:57,810
fits into our spec because we are

00:34:55,650 --> 00:34:59,940
packing company so that everything can

00:34:57,810 --> 00:35:01,890
still be Python yeah that's the reason

00:34:59,940 --> 00:35:05,280
for us the footprint of tasks is much

00:35:01,890 --> 00:35:07,210
much smaller than the path back there no

00:35:05,280 --> 00:35:09,400
thanks

00:35:07,210 --> 00:35:12,599
if you have that passion each come here

00:35:09,400 --> 00:35:15,520
we're not passing the mic from sorry

00:35:12,599 --> 00:35:18,309
sorry great top thank you

00:35:15,520 --> 00:35:18,970
I wanted to ask about the turbo DBC I

00:35:18,309 --> 00:35:21,609
think

00:35:18,970 --> 00:35:24,339
have you tried to use the support for

00:35:21,609 --> 00:35:27,849
rescue alchemy in pandas to actually get

00:35:24,339 --> 00:35:31,000
the data like that so we are too busy or

00:35:27,849 --> 00:35:34,990
to be' DBC also works nicely with SQL

00:35:31,000 --> 00:35:38,260
alchemy but you always so sequel Acme is

00:35:34,990 --> 00:35:40,450
other you can use it from Hondas but you

00:35:38,260 --> 00:35:42,369
always need the driver below to talk to

00:35:40,450 --> 00:35:45,789
the database and the eyes are used to

00:35:42,369 --> 00:35:49,329
buddy BC or PI ODBC and that's the thing

00:35:45,789 --> 00:35:51,670
that is fast or slow so if you want to

00:35:49,329 --> 00:35:54,869
do sequel alchemy in pandas that's just

00:35:51,670 --> 00:35:57,000
a layer layer above the other stuff

00:35:54,869 --> 00:35:59,289
thank you

00:35:57,000 --> 00:36:02,130
how would some questions from my

00:35:59,289 --> 00:36:05,170
practical euros the other case on

00:36:02,130 --> 00:36:07,119
there's one in our you know data

00:36:05,170 --> 00:36:10,150
pipeline flow there's a one step we need

00:36:07,119 --> 00:36:13,150
to read in the the fair which is in

00:36:10,150 --> 00:36:15,099
night state Jason Jason format and

00:36:13,150 --> 00:36:17,619
that's the juicer for me so the first

00:36:15,099 --> 00:36:20,650
time we try to use the pandas hunted you

00:36:17,619 --> 00:36:22,569
know with Jason reducer the Jason that

00:36:20,650 --> 00:36:26,710
function cannot handle the night state

00:36:22,569 --> 00:36:29,049
the problem so so the solution is quite

00:36:26,710 --> 00:36:32,319
an a basically I have to write and I

00:36:29,049 --> 00:36:37,029
like internal very static

00:36:32,319 --> 00:36:39,430
steak static library in myself to handle

00:36:37,029 --> 00:36:40,569
these short strings but you know all

00:36:39,430 --> 00:36:42,789
these sticker feelings have kind of a

00:36:40,569 --> 00:36:45,510
stable to make a transfer them to the

00:36:42,789 --> 00:36:48,640
pandas dataframe so I was thinking

00:36:45,510 --> 00:36:51,190
whether in future you want you you can

00:36:48,640 --> 00:36:53,980
or you like to think about you may be

00:36:51,190 --> 00:36:57,730
adding the feature like really in the

00:36:53,980 --> 00:36:59,470
nested Jason so maybe the solution could

00:36:57,730 --> 00:37:01,270
be first layer you have none like

00:36:59,470 --> 00:37:03,430
analysis the structure of the Jason to

00:37:01,270 --> 00:37:06,549
identify what is going on and then you

00:37:03,430 --> 00:37:09,730
use some like string cutting mechanism I

00:37:06,549 --> 00:37:12,970
mean it's from my it's my temporary

00:37:09,730 --> 00:37:15,809
solution for that so basically a park

00:37:12,970 --> 00:37:18,670
here um supports nested data structures

00:37:15,809 --> 00:37:20,950
necessary so in the park' files you can

00:37:18,670 --> 00:37:22,990
really store nested data structures

00:37:20,950 --> 00:37:25,119
like nice to Jason something like that

00:37:22,990 --> 00:37:27,400
yeah as a not Jason about nested data

00:37:25,119 --> 00:37:29,260
structures Pocky can handle this but I

00:37:27,400 --> 00:37:31,500
don't think this will be exposed to

00:37:29,260 --> 00:37:34,570
Python at you pandas pandas

00:37:31,500 --> 00:37:37,030
susceptibility to dimensional interface

00:37:34,570 --> 00:37:39,550
so it doesn't fit the blow of to genetic

00:37:37,030 --> 00:37:43,150
they just the reason is because the the

00:37:39,550 --> 00:37:46,180
master the Jason it will finally turn

00:37:43,150 --> 00:37:48,339
out to be a two-dimensional okay so is

00:37:46,180 --> 00:37:50,619
that's why I think it would be good to

00:37:48,339 --> 00:37:52,690
consume you know and the transport

00:37:50,619 --> 00:37:54,460
director data frame that's why I read a

00:37:52,690 --> 00:37:56,770
laboratory I mean it's an internal

00:37:54,460 --> 00:37:58,960
library to directly transfer this Jason

00:37:56,770 --> 00:38:02,770
let's take Jesus out toward it frame and

00:37:58,960 --> 00:38:07,020
pass to understand I think this is the

00:38:02,770 --> 00:38:07,020
way to go like you did it yeah

00:38:14,890 --> 00:38:23,210
do you have another question then about

00:38:18,950 --> 00:38:25,250
so let's go downstairs and so I went to

00:38:23,210 --> 00:38:25,980
thank you again our pleated for the nice

00:38:25,250 --> 00:38:31,309
presentation

00:38:25,980 --> 00:38:31,309

YouTube URL: https://www.youtube.com/watch?v=fcPzcooWrIY


