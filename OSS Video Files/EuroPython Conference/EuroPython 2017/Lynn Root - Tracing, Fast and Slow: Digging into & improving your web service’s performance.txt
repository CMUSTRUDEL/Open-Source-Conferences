Title: Lynn Root - Tracing, Fast and Slow: Digging into & improving your web service’s performance
Publication date: 2017-09-17
Playlist: EuroPython 2017
Description: 
	"Tracing, Fast and Slow: Digging into & improving your web service’s performance
[EuroPython 2017 - Talk - 2017-07-11 - Anfiteatro 1]
[Rimini, Italy]

Do you maintain a Rube Goldberg like service?
https://s-media-cache-ak0.pinimg.com/564x/92/27/a6/9227a66f6028bd19d418c4fb3a55b379.jpg
Perhaps it’s highly distributed? Or you recently walked onto a team with an unfamiliar codebase? Have you noticed your service responds slower than molasses? This talk will walk you through how to pinpoint bottlenecks, approaches and tools to make improvements, and make you seem like the hero! All in a day’s work.

The talk will describe various types of tracing a web service, including black &amp; white box tracing, tracing distributed systems, as well as various tools and external services available to measure performance. I’ll also present a few different rabbit holes to dive into when trying to improve your service’s performance.

License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2017.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:04,130 --> 00:00:08,480
hello good afternoon does anyone

00:00:06,500 --> 00:00:13,160
actually get the reference in my title

00:00:08,480 --> 00:00:15,230
like I feel like so it's a reference to

00:00:13,160 --> 00:00:17,930
a really awesome book Thinking Fast and

00:00:15,230 --> 00:00:21,380
Slow I highly recommend it no relation

00:00:17,930 --> 00:00:23,510
to this actual like taco so yes my name

00:00:21,380 --> 00:00:26,810
is Lin root I am a site reliability

00:00:23,510 --> 00:00:29,750
engineer at Spotify I also do a lot of

00:00:26,810 --> 00:00:33,219
open source evangelism internally and

00:00:29,750 --> 00:00:35,809
you might know me from PI ladies as well

00:00:33,219 --> 00:00:37,940
also unfortunately I'm going to take up

00:00:35,809 --> 00:00:40,129
like the whole time so if you have

00:00:37,940 --> 00:00:42,260
questions or want to chat you can come

00:00:40,129 --> 00:00:45,739
join me for a convenient coffee break

00:00:42,260 --> 00:00:47,870
right after this okay another quick

00:00:45,739 --> 00:00:50,179
question has anyone read the site

00:00:47,870 --> 00:00:53,780
reliability engineering book aka the

00:00:50,179 --> 00:00:55,850
Google SOE book I think I see a few

00:00:53,780 --> 00:00:58,549
hands all right well I highly recommend

00:00:55,850 --> 00:01:00,949
that book but the TLDR of like every

00:00:58,549 --> 00:01:04,330
chapter seems to be you've distributed

00:01:00,949 --> 00:01:06,650
tracing so with the prevalence of

00:01:04,330 --> 00:01:08,780
microservices where you may or may not

00:01:06,650 --> 00:01:10,939
own all the services that a request

00:01:08,780 --> 00:01:12,890
might flow through it's certainly

00:01:10,939 --> 00:01:15,619
imperative to understand where your code

00:01:12,890 --> 00:01:17,950
fits into the grand scheme of things and

00:01:15,619 --> 00:01:20,570
how everything operates with each other

00:01:17,950 --> 00:01:23,960
so there's three main needs to trace a

00:01:20,570 --> 00:01:26,780
system performance debugging capacity

00:01:23,960 --> 00:01:28,640
planning and problem diagnosis although

00:01:26,780 --> 00:01:31,759
it can help address many other issues as

00:01:28,640 --> 00:01:33,829
well so while this talk will have like a

00:01:31,759 --> 00:01:36,079
slight focus towards performance

00:01:33,829 --> 00:01:39,500
debugging these techniques can certainly

00:01:36,079 --> 00:01:41,570
be applicable to other needs so I have a

00:01:39,500 --> 00:01:43,579
bit of a jam-packed day to day and I'll

00:01:41,570 --> 00:01:45,500
start off with an overview of what

00:01:43,579 --> 00:01:47,929
tracing is and the problems we can try

00:01:45,500 --> 00:01:50,179
to diagnose with it and I'm also talk

00:01:47,929 --> 00:01:52,579
about some general types of tracing we

00:01:50,179 --> 00:01:54,200
can use and what key things to think

00:01:52,579 --> 00:01:58,280
about when scaling up to larger

00:01:54,200 --> 00:02:00,049
distributions and then the inspiration

00:01:58,280 --> 00:02:01,909
for this talk stem for me trying to

00:02:00,049 --> 00:02:05,119
improve the performance of one of my own

00:02:01,909 --> 00:02:08,179
team services which sort of implies we

00:02:05,119 --> 00:02:09,830
don't really trace that Spotify so I'll

00:02:08,179 --> 00:02:11,630
be running through some questions to ask

00:02:09,830 --> 00:02:14,710
and approaches to take when diagnosing

00:02:11,630 --> 00:02:17,270
and fixing your services bottlenecks and

00:02:14,710 --> 00:02:17,860
finally I'll wrap up with some precinct

00:02:17,270 --> 00:02:21,130
solutions

00:02:17,860 --> 00:02:22,390
for a profiling performance and as I

00:02:21,130 --> 00:02:24,220
mentioned before won't have time for

00:02:22,390 --> 00:02:28,030
questions so you can catch me right out

00:02:24,220 --> 00:02:31,330
there all right in the simplest terms a

00:02:28,030 --> 00:02:34,420
trace follows the complete workflow from

00:02:31,330 --> 00:02:36,640
the start of a transaction or request to

00:02:34,420 --> 00:02:39,250
its end including the components that it

00:02:36,640 --> 00:02:40,960
flows through so for a very simple web

00:02:39,250 --> 00:02:44,680
application it's pretty easy to

00:02:40,960 --> 00:02:47,200
understand the workflow of a request but

00:02:44,680 --> 00:02:49,060
then add some databases separate the

00:02:47,200 --> 00:02:51,010
front end from the back end maybe throw

00:02:49,060 --> 00:02:53,380
in some caching have an external API

00:02:51,010 --> 00:02:55,420
call all behind a load balancer then

00:02:53,380 --> 00:02:57,760
scale up to ten hundreds or thousands of

00:02:55,420 --> 00:03:01,150
times it gets kind of difficult to put

00:02:57,760 --> 00:03:02,620
together workflows of requests so

00:03:01,150 --> 00:03:05,980
historically we've been focused on

00:03:02,620 --> 00:03:08,080
machine centric metrics including system

00:03:05,980 --> 00:03:10,600
level metrics like CPU disk space and

00:03:08,080 --> 00:03:12,459
memory as well as Apps level metrics

00:03:10,600 --> 00:03:16,690
like requests per second response

00:03:12,459 --> 00:03:17,950
latency database rights etc following

00:03:16,690 --> 00:03:20,950
and understanding these metrics are

00:03:17,950 --> 00:03:22,750
quite important but there's no view into

00:03:20,950 --> 00:03:25,330
a services dependencies or its

00:03:22,750 --> 00:03:27,640
dependents and it's not it's also not

00:03:25,330 --> 00:03:30,489
possible to get a view of a complete

00:03:27,640 --> 00:03:32,049
flow of a request nor develop an

00:03:30,489 --> 00:03:35,920
understanding of how one service

00:03:32,049 --> 00:03:37,209
performs at scale so a workflow centric

00:03:35,920 --> 00:03:39,280
approach allows us to understand

00:03:37,209 --> 00:03:41,709
relationships of components within an

00:03:39,280 --> 00:03:43,660
entire system and then we can follow a

00:03:41,709 --> 00:03:45,549
request from beginning to end to

00:03:43,660 --> 00:03:47,830
understand bottlenecks and hone in on

00:03:45,549 --> 00:03:52,030
the anomalistic count and figure out

00:03:47,830 --> 00:03:53,769
where we need to add more resources so

00:03:52,030 --> 00:03:55,690
when looking at a very simplified system

00:03:53,769 --> 00:03:57,790
where we have a load balancer or

00:03:55,690 --> 00:03:59,769
front-end back-end database maybe an

00:03:57,790 --> 00:04:02,320
external dependency to a third party API

00:03:59,769 --> 00:04:04,299
and when we have redundant systems it

00:04:02,320 --> 00:04:08,230
gets particularly confusing to follow a

00:04:04,299 --> 00:04:11,650
request so how do we debug a program of

00:04:08,230 --> 00:04:13,900
a rare workflow how do we know which

00:04:11,650 --> 00:04:15,940
component of this system is the

00:04:13,900 --> 00:04:18,040
bottleneck which function call is taking

00:04:15,940 --> 00:04:19,930
the longest is there another app on my

00:04:18,040 --> 00:04:21,910
house causing distortion of machine

00:04:19,930 --> 00:04:23,650
centric metrics of performance metrics

00:04:21,910 --> 00:04:26,830
something like the noisy neighbors

00:04:23,650 --> 00:04:29,050
problem so as so many potential paths

00:04:26,830 --> 00:04:30,980
that a request can take with potential

00:04:29,050 --> 00:04:33,860
for issues at each and every

00:04:30,980 --> 00:04:35,750
node and edge this can be mind numbingly

00:04:33,860 --> 00:04:39,020
difficult and if we continue to be

00:04:35,750 --> 00:04:42,170
machine centric so and M tracing allow

00:04:39,020 --> 00:04:45,020
us to get a bigger picture to address

00:04:42,170 --> 00:04:46,820
these concerns and looking at the

00:04:45,020 --> 00:04:49,340
magnitudes of what we're up up writing

00:04:46,820 --> 00:04:53,840
at Spotify you can see that tracing if

00:04:49,340 --> 00:04:55,850
we did it would help us a lot so real

00:04:53,840 --> 00:04:58,580
quickly and there are a few reasons why

00:04:55,850 --> 00:05:01,130
we tracer system the one that inspired

00:04:58,580 --> 00:05:02,630
this talk is performance analysis this

00:05:01,130 --> 00:05:04,550
is trying to understand what happens at

00:05:02,630 --> 00:05:07,610
the fiftieth or semi fifth percentile

00:05:04,550 --> 00:05:09,650
the steady state problem and this will

00:05:07,610 --> 00:05:12,890
help us identify all agencies resource

00:05:09,650 --> 00:05:14,450
usages and other performance issues we

00:05:12,890 --> 00:05:16,580
were also able to understand questions

00:05:14,450 --> 00:05:18,560
like did this particular deploy of the

00:05:16,580 --> 00:05:22,550
service have an effect on latency of the

00:05:18,560 --> 00:05:25,310
overall whole system tracing can also

00:05:22,550 --> 00:05:29,480
clue us in on anomalistic request flows

00:05:25,310 --> 00:05:30,950
the 99.9% down the issues can still be

00:05:29,480 --> 00:05:32,930
related to performance or it can help

00:05:30,950 --> 00:05:38,150
identify problems with correctness like

00:05:32,930 --> 00:05:40,010
component failures or timeouts profiling

00:05:38,150 --> 00:05:41,180
is very similar to the first but here

00:05:40,010 --> 00:05:44,270
we're just interested in particular

00:05:41,180 --> 00:05:46,340
components or aspects of systems we

00:05:44,270 --> 00:05:50,270
don't necessarily care about the full

00:05:46,340 --> 00:05:52,400
workflow here a fourth one we can also

00:05:50,270 --> 00:05:55,400
answer questions of what a particular

00:05:52,400 --> 00:05:58,190
component depends on and what depends on

00:05:55,400 --> 00:06:02,660
it and particularly useful for a complex

00:05:58,190 --> 00:06:04,730
system complex systems so when with

00:06:02,660 --> 00:06:06,980
dependence dependence identified we can

00:06:04,730 --> 00:06:10,550
also attribute particularly expensive

00:06:06,980 --> 00:06:12,680
work like component a significant

00:06:10,550 --> 00:06:15,380
workload with dis right to component

00:06:12,680 --> 00:06:17,390
being so which can help be helpful when

00:06:15,380 --> 00:06:21,020
attribute and cost to team Z service

00:06:17,390 --> 00:06:22,670
owners or component owner owners and

00:06:21,020 --> 00:06:24,650
finally and we're able to create models

00:06:22,670 --> 00:06:27,770
of our entire systems that allow us to

00:06:24,650 --> 00:06:29,930
ask what is questions like what would

00:06:27,770 --> 00:06:36,290
happen to component a if we did a

00:06:29,930 --> 00:06:37,970
disaster recovery test on component B so

00:06:36,290 --> 00:06:40,370
there are very various approaches to

00:06:37,970 --> 00:06:43,220
tracing I'll only highlight three of

00:06:40,370 --> 00:06:44,430
them here at first is manual is also

00:06:43,220 --> 00:06:46,020
very simplistic

00:06:44,430 --> 00:06:48,530
where you are just generating your own

00:06:46,020 --> 00:06:50,460
trace IDs and adding them to your logs

00:06:48,530 --> 00:06:52,080
and there are very simple things that

00:06:50,460 --> 00:06:54,570
can be added to your web service here

00:06:52,080 --> 00:06:56,490
especially ones that do not have

00:06:54,570 --> 00:06:59,340
dependent or depending components that

00:06:56,490 --> 00:07:02,010
you don't have access to you won't give

00:06:59,340 --> 00:07:04,320
any pretty visualizations or help with

00:07:02,010 --> 00:07:06,900
centralized collection and beyond what

00:07:04,320 --> 00:07:10,050
we typically have with your logs but it

00:07:06,900 --> 00:07:12,990
still can provide insight so this is a

00:07:10,050 --> 00:07:15,360
flask example super simple using a

00:07:12,990 --> 00:07:18,390
decorator here you can simply add a UUID

00:07:15,360 --> 00:07:20,400
to each request received as a header

00:07:18,390 --> 00:07:22,560
then log at particular points of

00:07:20,400 --> 00:07:25,050
interest like at the beginning and end

00:07:22,560 --> 00:07:27,660
of a request and then any other in

00:07:25,050 --> 00:07:30,290
between components or function calls and

00:07:27,660 --> 00:07:32,400
where you want to propagate headers and

00:07:30,290 --> 00:07:34,380
this is exactly what I ended up doing

00:07:32,400 --> 00:07:34,980
for my service which may be wish for a

00:07:34,380 --> 00:07:37,920
better way

00:07:34,980 --> 00:07:41,970
hence this talk I must submit I do a lot

00:07:37,920 --> 00:07:44,280
of conference driven development so if

00:07:41,970 --> 00:07:45,750
your app is behind engine X fear that

00:07:44,280 --> 00:07:48,300
you're able to manipulate you can also

00:07:45,750 --> 00:07:50,550
turn on its ability to stamp each

00:07:48,300 --> 00:07:53,010
request with our extra quest ID header

00:07:50,550 --> 00:07:57,230
as we see here with the add header and

00:07:53,010 --> 00:08:00,480
proxy set enter you can also add a very

00:07:57,230 --> 00:08:05,580
simple like it simply adds a request ID

00:08:00,480 --> 00:08:07,530
to nginx with logs as well next up is

00:08:05,580 --> 00:08:09,570
black box tracing this is tracing with

00:08:07,530 --> 00:08:12,360
no implementation across the components

00:08:09,570 --> 00:08:14,340
it tries to infer the workflows and

00:08:12,360 --> 00:08:16,980
relationships by correlating variables

00:08:14,340 --> 00:08:20,100
and timing with an already defined log

00:08:16,980 --> 00:08:22,050
messages so from here a relationship

00:08:20,100 --> 00:08:24,500
relationship inference is done via

00:08:22,050 --> 00:08:27,090
statistical or regression analysis and

00:08:24,500 --> 00:08:29,100
this is easiest with a centralized

00:08:27,090 --> 00:08:31,500
logging and if there's somewhat of a

00:08:29,100 --> 00:08:34,290
standardized schema to log messages that

00:08:31,500 --> 00:08:36,150
contain like an ID or a timestamp it's

00:08:34,290 --> 00:08:38,910
particularly useful if instrumenting an

00:08:36,150 --> 00:08:40,560
entire system is too cumbersome or you

00:08:38,910 --> 00:08:43,170
can't otherwise and schmitt components

00:08:40,560 --> 00:08:46,380
that you don't own and as such is quite

00:08:43,170 --> 00:08:48,660
portable and there's very little to no

00:08:46,380 --> 00:08:50,640
overhead but it does require a lot of

00:08:48,660 --> 00:08:53,580
data points in order to incorrectly

00:08:50,640 --> 00:08:55,680
infer relationships and it also lacks

00:08:53,580 --> 00:08:57,360
accuracy with the absence of

00:08:55,680 --> 00:08:59,370
instrumenting components than cell

00:08:57,360 --> 00:09:01,500
as was the ability to attribute

00:08:59,370 --> 00:09:05,399
causality with asynchronous behavior and

00:09:01,500 --> 00:09:07,350
concurrency another approach to a black

00:09:05,399 --> 00:09:11,190
box tracing can be through network

00:09:07,350 --> 00:09:14,339
tapping using a slow or NS temp or IP a

00:09:11,190 --> 00:09:17,149
table packet data which I am sure the

00:09:14,339 --> 00:09:19,709
NSA is quite familiar with themselves

00:09:17,149 --> 00:09:21,829
and then the final type of tracing is

00:09:19,709 --> 00:09:24,149
through a metadata proximal propagation

00:09:21,829 --> 00:09:27,540
and this approach was made popular by

00:09:24,149 --> 00:09:28,800
Google's research paper on Dever and so

00:09:27,540 --> 00:09:30,779
components are instrumented at

00:09:28,800 --> 00:09:33,120
particular trace points to follow

00:09:30,779 --> 00:09:36,450
causality between functions components

00:09:33,120 --> 00:09:38,850
and systems or even with common articie

00:09:36,450 --> 00:09:41,690
libraries like G RPC and that will

00:09:38,850 --> 00:09:44,610
automatically add metadata to each call

00:09:41,690 --> 00:09:47,220
so metadata that is tracked includes a

00:09:44,610 --> 00:09:49,890
trace ID which represents one single

00:09:47,220 --> 00:09:51,959
trace or workflow and a stand ID for

00:09:49,890 --> 00:09:54,870
each and every point in a particular

00:09:51,959 --> 00:09:57,240
trace like request sent from client and

00:09:54,870 --> 00:10:00,420
request received by server server

00:09:57,240 --> 00:10:03,300
responds and then the spans start and

00:10:00,420 --> 00:10:05,880
end time so this approach works best

00:10:03,300 --> 00:10:08,880
when the system itself is designed with

00:10:05,880 --> 00:10:11,790
tracing in mind but not many people do

00:10:08,880 --> 00:10:13,949
that right so this avoids guesswork with

00:10:11,790 --> 00:10:16,260
the inferring causal relationships

00:10:13,949 --> 00:10:19,050
however I can add a bit of overhead to

00:10:16,260 --> 00:10:21,540
response time and Suri put so the use of

00:10:19,050 --> 00:10:25,040
sampling traces limits the burden here

00:10:21,540 --> 00:10:28,019
on the system and the data point storage

00:10:25,040 --> 00:10:30,360
sampling anywhere between 0.01 percent

00:10:28,019 --> 00:10:31,980
and 10 percent of requests is often

00:10:30,360 --> 00:10:36,630
planning to get an understanding of a

00:10:31,980 --> 00:10:38,970
system's performance so when starting to

00:10:36,630 --> 00:10:41,399
have many micro services and scaling out

00:10:38,970 --> 00:10:42,690
with many more resources there are a few

00:10:41,399 --> 00:10:44,550
points to keep in mind when

00:10:42,690 --> 00:10:47,870
instrumenting your system particularly

00:10:44,550 --> 00:10:50,310
with the metadata propagation approach

00:10:47,870 --> 00:10:52,230
so in terms of what to keep in mind and

00:10:50,310 --> 00:10:54,420
I'll go into the detail about each in a

00:10:52,230 --> 00:10:56,790
second we want to know what

00:10:54,420 --> 00:10:59,070
relationships detract essentially how to

00:10:56,790 --> 00:11:02,220
follow a trace and what is considered

00:10:59,070 --> 00:11:04,709
part of a workflow and how they are

00:11:02,220 --> 00:11:06,209
tracked constructing metadata to track

00:11:04,709 --> 00:11:08,339
causal relationships is particularly

00:11:06,209 --> 00:11:09,750
difficult there are a few approaches

00:11:08,339 --> 00:11:10,940
each with their own Forte's and

00:11:09,750 --> 00:11:13,770
drawbacks

00:11:10,940 --> 00:11:16,470
and then how to reduce overhead of

00:11:13,770 --> 00:11:18,210
tracking the approach one chooses and

00:11:16,470 --> 00:11:19,830
sampling is largely defined by what

00:11:18,210 --> 00:11:22,350
questions you're trying to answer with

00:11:19,830 --> 00:11:24,210
your tracing and there may be a clear

00:11:22,350 --> 00:11:28,230
answer but not without its own penalties

00:11:24,210 --> 00:11:30,210
and finally how to visualize the

00:11:28,230 --> 00:11:31,860
visualizations needed and will also be

00:11:30,210 --> 00:11:35,880
informed by what you're trying to answer

00:11:31,860 --> 00:11:38,880
with tracing alright so what to track

00:11:35,880 --> 00:11:41,010
when looking within a request we can

00:11:38,880 --> 00:11:43,290
take two points of view either the

00:11:41,010 --> 00:11:46,350
submitter point of view or the trigger

00:11:43,290 --> 00:11:49,020
point of view so the submitter point of

00:11:46,350 --> 00:11:51,600
view follows or just focuses on one

00:11:49,020 --> 00:11:53,640
complete request and doesn't take into

00:11:51,600 --> 00:11:57,210
account if part of that request is

00:11:53,640 --> 00:11:59,850
caused by another request or action so

00:11:57,210 --> 00:12:02,250
for instance the evicting cache here

00:11:59,850 --> 00:12:04,890
that was actually triggered by request -

00:12:02,250 --> 00:12:07,500
is it still attributed to request one

00:12:04,890 --> 00:12:11,790
since its data it comes from the first

00:12:07,500 --> 00:12:13,890
request the trigger point of view

00:12:11,790 --> 00:12:16,410
focuses on the trigger that initiates

00:12:13,890 --> 00:12:19,320
the action and we're in the same example

00:12:16,410 --> 00:12:21,660
a request to evict cache from request 1

00:12:19,320 --> 00:12:25,110
and therefore the eviction is included

00:12:21,660 --> 00:12:27,270
in request to trace so choosing which to

00:12:25,110 --> 00:12:29,670
follow depends on the answers that

00:12:27,270 --> 00:12:30,900
you're trying to find for instance it

00:12:29,670 --> 00:12:33,690
doesn't really matter which approach is

00:12:30,900 --> 00:12:35,760
chosen for performance profiling but

00:12:33,690 --> 00:12:38,520
following trigger causality will help

00:12:35,760 --> 00:12:43,890
detect anomalies by showing critical

00:12:38,520 --> 00:12:46,140
paths alright how to tracker essentially

00:12:43,890 --> 00:12:49,110
what is needed in your metadata this

00:12:46,140 --> 00:12:51,300
essentially boils down to it's very

00:12:49,110 --> 00:12:52,710
difficult to reliably track causal

00:12:51,300 --> 00:12:54,630
relationships within a distributed

00:12:52,710 --> 00:12:56,400
system now the sheer nature of a

00:12:54,630 --> 00:12:58,170
distributed system implies issues with

00:12:56,400 --> 00:13:01,020
ordering events and traces that happen

00:12:58,170 --> 00:13:03,210
across many hosts and there might not be

00:13:01,020 --> 00:13:05,340
a global synchronous clock available so

00:13:03,210 --> 00:13:07,710
care must be taken when deciding what

00:13:05,340 --> 00:13:12,060
goes into crafting the metadata that is

00:13:07,710 --> 00:13:15,570
threaded through an end-to-end trace so

00:13:12,060 --> 00:13:17,640
using a random ID like UUID or the X

00:13:15,570 --> 00:13:20,550
request ID header and we'll identify

00:13:17,640 --> 00:13:22,720
causal causal related activities but

00:13:20,550 --> 00:13:24,730
then tracing implementations

00:13:22,720 --> 00:13:28,629
must use some sort of external clock to

00:13:24,730 --> 00:13:30,939
collect traces and then in the absence

00:13:28,629 --> 00:13:33,759
of a global synchronized clock or to

00:13:30,939 --> 00:13:35,740
avoid issues like clock skew looking at

00:13:33,759 --> 00:13:37,870
networks send and receive messages can

00:13:35,740 --> 00:13:40,540
then be used to construct causal

00:13:37,870 --> 00:13:43,930
relationships because you can't exactly

00:13:40,540 --> 00:13:45,610
receive a message before it's sent and a

00:13:43,930 --> 00:13:48,579
lot of tracing implementations use this

00:13:45,610 --> 00:13:51,730
very simplistic approach however this

00:13:48,579 --> 00:13:53,829
approach lacks resiliency there's a

00:13:51,730 --> 00:13:56,709
potential for data loss from external

00:13:53,829 --> 00:14:00,149
systems or inability to add trace points

00:13:56,709 --> 00:14:02,470
to a components that's owned by others

00:14:00,149 --> 00:14:04,990
I'm tracing systems can also add a

00:14:02,470 --> 00:14:08,170
timestamp derived from a local logical

00:14:04,990 --> 00:14:10,750
plot to the workflow ID where this isn't

00:14:08,170 --> 00:14:12,579
exactly the local system timestamp but

00:14:10,750 --> 00:14:15,120
either a counter or sort of a randomized

00:14:12,579 --> 00:14:17,589
timestamp as paired with a trace message

00:14:15,120 --> 00:14:19,750
so with this approach we don't need at

00:14:17,589 --> 00:14:22,120
the tracing system to spend time on the

00:14:19,750 --> 00:14:24,430
ordering of traces they collect since

00:14:22,120 --> 00:14:26,319
it's explicit in the clock data but

00:14:24,430 --> 00:14:27,579
parallelization and concurrency can

00:14:26,319 --> 00:14:31,600
complicate understanding these

00:14:27,579 --> 00:14:34,509
relationships and then one can also add

00:14:31,600 --> 00:14:36,009
the previous trace points that have been

00:14:34,509 --> 00:14:37,959
already executed within the metadata

00:14:36,009 --> 00:14:40,209
itself to understand all the forks and

00:14:37,959 --> 00:14:41,889
joins and it also allows immediate

00:14:40,209 --> 00:14:44,139
availability of the tracing data itself

00:14:41,889 --> 00:14:46,839
as soon as a workflow ends so there's no

00:14:44,139 --> 00:14:49,300
need to spend time on collating or

00:14:46,839 --> 00:14:51,279
establishing the order of causal

00:14:49,300 --> 00:14:53,920
relationships but as you can imagine

00:14:51,279 --> 00:14:56,079
metadata will only grow in size as it

00:14:53,920 --> 00:15:00,000
follows the workflow adding to the

00:14:56,079 --> 00:15:02,199
payload so basically boils down to this

00:15:00,000 --> 00:15:04,180
if you really care about payload of

00:15:02,199 --> 00:15:07,209
requests than a simple unique ID is your

00:15:04,180 --> 00:15:09,879
go-to but at the expense of needing to

00:15:07,209 --> 00:15:12,100
infer relationships you can then add a

00:15:09,879 --> 00:15:14,790
time a timestamp of sorts to help

00:15:12,100 --> 00:15:17,110
establish explicit causal relationships

00:15:14,790 --> 00:15:19,540
but you're still susceptible to

00:15:17,110 --> 00:15:22,629
potential ordering issues of traces if

00:15:19,540 --> 00:15:24,459
data is lost now you may add the

00:15:22,629 --> 00:15:26,769
previously executed trace points to

00:15:24,459 --> 00:15:29,649
avoid data loss and understand the forks

00:15:26,769 --> 00:15:31,600
and joins of a trace while gaining

00:15:29,649 --> 00:15:33,220
immediate availability of trace data

00:15:31,600 --> 00:15:35,390
since causal relationships are already

00:15:33,220 --> 00:15:37,790
established but then you suffer

00:15:35,390 --> 00:15:40,130
in payload size and then there's also

00:15:37,790 --> 00:15:41,779
the fact that there are no open source

00:15:40,130 --> 00:15:44,560
tracing system that actually implements

00:15:41,779 --> 00:15:44,560
this last one

00:15:45,529 --> 00:15:49,970
so intent tracing will have an effect on

00:15:47,360 --> 00:15:53,000
runtime and storage overhead no matter

00:15:49,970 --> 00:15:55,910
what you choose for instance if Google

00:15:53,000 --> 00:15:58,399
were to trace all web searches despite

00:15:55,910 --> 00:16:01,760
its intelligent tracing implementation

00:15:58,399 --> 00:16:04,790
it would impose a 1.5% throughput

00:16:01,760 --> 00:16:07,579
penalty and add 16% to the response time

00:16:04,790 --> 00:16:09,740
I won't go into them very much detail

00:16:07,579 --> 00:16:14,060
but there are essentially three basic

00:16:09,740 --> 00:16:17,060
approaches to sampling first is head

00:16:14,060 --> 00:16:19,070
based which will make a random sampling

00:16:17,060 --> 00:16:20,750
decision at the start of a workflow and

00:16:19,070 --> 00:16:24,649
then we'll follow a follow it all the

00:16:20,750 --> 00:16:27,410
way through to completion the next one

00:16:24,649 --> 00:16:28,899
tael base which will make the sampling

00:16:27,410 --> 00:16:32,660
decision at the end of the workflow

00:16:28,899 --> 00:16:34,670
implying some caching going on here tale

00:16:32,660 --> 00:16:36,260
based sampling needs to be a little bit

00:16:34,670 --> 00:16:38,240
more intelligent but it's particularly

00:16:36,260 --> 00:16:42,110
useful for a tracing anomalistic

00:16:38,240 --> 00:16:44,209
behavior and finally unitary sampling

00:16:42,110 --> 00:16:45,800
where the sampling decision is made at

00:16:44,209 --> 00:16:47,990
the trace point itself and therefore

00:16:45,800 --> 00:16:51,920
prevents the construction of a full

00:16:47,990 --> 00:16:53,510
workflow so head base is the simplest

00:16:51,920 --> 00:16:55,820
and probably most ideal for a

00:16:53,510 --> 00:16:57,680
performance profiling and both head base

00:16:55,820 --> 00:17:00,800
and unit area are most often seen and

00:16:57,680 --> 00:17:03,019
current tracing implementations I'm not

00:17:00,800 --> 00:17:07,429
quite sure if there's a tracing system

00:17:03,019 --> 00:17:10,189
that actually implements tale based all

00:17:07,429 --> 00:17:11,780
right what visualizations you choose the

00:17:10,189 --> 00:17:14,540
tool to look at the sense upon what

00:17:11,780 --> 00:17:16,459
you're trying to figure out so I'm

00:17:14,540 --> 00:17:18,380
against charts are popular and

00:17:16,459 --> 00:17:21,319
definitely quite appealing but it only

00:17:18,380 --> 00:17:22,760
shows requests from a single trace and

00:17:21,319 --> 00:17:24,380
you can you definitely have seen this

00:17:22,760 --> 00:17:28,580
type before if you looked at the network

00:17:24,380 --> 00:17:30,770
tab of your browsers dev tools when

00:17:28,580 --> 00:17:33,770
trying to get a sense of where a systems

00:17:30,770 --> 00:17:37,220
bottlenecks are a request flow graph aka

00:17:33,770 --> 00:17:39,140
a directed acyclic graph will will show

00:17:37,220 --> 00:17:41,270
workflows as they are executed and

00:17:39,140 --> 00:17:44,179
unlike Gantt charts can a great

00:17:41,270 --> 00:17:47,630
information of multiple requests of the

00:17:44,179 --> 00:17:49,040
same workflow another useful

00:17:47,630 --> 00:17:51,290
representation is a

00:17:49,040 --> 00:17:53,680
in context Stream in order to visualize

00:17:51,290 --> 00:17:56,570
multiple requests of different workflows

00:17:53,680 --> 00:17:58,850
and this reveals both valid and invalid

00:17:56,570 --> 00:18:00,890
paths that a request can take and best

00:17:58,850 --> 00:18:05,030
for creating a general understanding of

00:18:00,890 --> 00:18:07,250
system behavior so what the takeaway

00:18:05,030 --> 00:18:10,580
here is there's a few things we need to

00:18:07,250 --> 00:18:11,870
consider when we trace a system you

00:18:10,580 --> 00:18:13,490
should have an understanding of what you

00:18:11,870 --> 00:18:16,580
want to do what questions you're trying

00:18:13,490 --> 00:18:17,690
to answer with tracing and then

00:18:16,580 --> 00:18:19,610
certainly there will be other

00:18:17,690 --> 00:18:21,620
realizations and questions that come

00:18:19,610 --> 00:18:24,440
from a trace system for example with

00:18:21,620 --> 00:18:26,420
staffer Google is able to audit systems

00:18:24,440 --> 00:18:28,370
for security I'm asserting that only

00:18:26,420 --> 00:18:30,770
authorized components are talking to

00:18:28,370 --> 00:18:32,870
sensitive services but not without

00:18:30,770 --> 00:18:35,060
understanding what you're trying to

00:18:32,870 --> 00:18:38,840
figure out you might end up approaching

00:18:35,060 --> 00:18:40,550
your instrumentation incorrectly the

00:18:38,840 --> 00:18:42,950
answer to this question will help

00:18:40,550 --> 00:18:45,770
identify the approach to causality

00:18:42,950 --> 00:18:49,310
whether from a trigger point of view or

00:18:45,770 --> 00:18:50,900
from submitter point of view another

00:18:49,310 --> 00:18:52,550
important question how much time do you

00:18:50,900 --> 00:18:54,920
want to put into instrumenting your

00:18:52,550 --> 00:18:57,740
system or can you even instrument all

00:18:54,920 --> 00:18:59,510
part this will inform the approach that

00:18:57,740 --> 00:19:01,910
you use to trace and be a black box or

00:18:59,510 --> 00:19:03,890
not if you can instrument like all the

00:19:01,910 --> 00:19:05,840
things or at least some of it it then

00:19:03,890 --> 00:19:07,370
becomes a question of what data you

00:19:05,840 --> 00:19:11,510
should propagate through an entire work

00:19:07,370 --> 00:19:13,310
through entire flow and finally how much

00:19:11,510 --> 00:19:15,920
of the flows do you want to understand

00:19:13,310 --> 00:19:17,840
you want to understand all the requests

00:19:15,920 --> 00:19:19,310
then you should be prepared to take a

00:19:17,840 --> 00:19:21,890
performance penalty on the service

00:19:19,310 --> 00:19:25,820
itself and then you can have fun storing

00:19:21,890 --> 00:19:28,700
all that data or is a percentage of the

00:19:25,820 --> 00:19:30,800
flows okay and then if so and then how

00:19:28,700 --> 00:19:32,680
do we approach sampling and that's in

00:19:30,800 --> 00:19:36,020
your answer of what we want to know

00:19:32,680 --> 00:19:38,200
questions so if I understand performance

00:19:36,020 --> 00:19:40,310
head based sampling is certainly fine

00:19:38,200 --> 00:19:42,110
you also need to think about whether or

00:19:40,310 --> 00:19:44,450
not you want to capture the full work

00:19:42,110 --> 00:19:46,670
flow of requests or only focused on a

00:19:44,450 --> 00:19:49,130
subset of the system and this will also

00:19:46,670 --> 00:19:53,570
inform your sampling approach via

00:19:49,130 --> 00:19:55,520
unitary or not and so in terms of

00:19:53,570 --> 00:19:57,759
performance and understanding where

00:19:55,520 --> 00:20:00,529
bottlenecks are

00:19:57,759 --> 00:20:02,659
you want to try and preserve the trigger

00:20:00,529 --> 00:20:04,279
causality rather than submitter as it

00:20:02,659 --> 00:20:07,879
shows like the critical path to that

00:20:04,279 --> 00:20:10,519
bottleneck head base sampling is fine as

00:20:07,879 --> 00:20:12,019
we don't need intelligence sampling and

00:20:10,519 --> 00:20:13,940
even with very low sample rates we can

00:20:12,019 --> 00:20:16,639
get a good idea of where our problem

00:20:13,940 --> 00:20:20,539
lies and since we essentially care about

00:20:16,639 --> 00:20:23,149
the 50th or 75th percentile and finally

00:20:20,539 --> 00:20:25,460
a request flow graph here is our deal

00:20:23,149 --> 00:20:27,769
since we don't care about a nominal sick

00:20:25,460 --> 00:20:29,779
behavior and we want some information of

00:20:27,769 --> 00:20:34,159
the big picture rather than looking into

00:20:29,779 --> 00:20:36,559
particular individual that workflows and

00:20:34,159 --> 00:20:39,019
so most often once you are tracing a

00:20:36,559 --> 00:20:42,919
system the problem will reveal itself as

00:20:39,019 --> 00:20:44,210
will the solution but not always so I do

00:20:42,919 --> 00:20:46,340
have a few questions to ask yourself

00:20:44,210 --> 00:20:50,480
when figuring out how to improve

00:20:46,340 --> 00:20:52,429
services performance first one is are

00:20:50,480 --> 00:20:55,279
you making multiple requests to the same

00:20:52,429 --> 00:20:57,409
service and round trips Network calls

00:20:55,279 --> 00:21:00,529
are expensive and perhaps there's a way

00:20:57,409 --> 00:21:04,340
to set up batch requests or accept batch

00:21:00,529 --> 00:21:06,529
requests on your end perhaps your

00:21:04,340 --> 00:21:10,399
service doesn't need to be synchronous

00:21:06,529 --> 00:21:12,200
or unnecessary unnecessarily blocks for

00:21:10,399 --> 00:21:14,570
example if you're some big social

00:21:12,200 --> 00:21:16,489
networking site can you grab a user's

00:21:14,570 --> 00:21:19,039
profile photo at the same time but you

00:21:16,489 --> 00:21:22,480
pull up their timeline while you try and

00:21:19,039 --> 00:21:25,129
grab their messages at the same time and

00:21:22,480 --> 00:21:28,549
is the same data being repeatedly

00:21:25,129 --> 00:21:30,889
requested but not cached or maybe you

00:21:28,549 --> 00:21:33,919
are caching too much or maybe not the

00:21:30,889 --> 00:21:35,169
right data is the expiration too high or

00:21:33,919 --> 00:21:39,619
too low

00:21:35,169 --> 00:21:39,999
and what about your site's assets could

00:21:39,619 --> 00:21:42,889
there be

00:21:39,999 --> 00:21:44,809
could they be better or ordered to

00:21:42,889 --> 00:21:46,489
improve loading time can you minimize

00:21:44,809 --> 00:21:49,549
the amount of inline scripts or maybe

00:21:46,489 --> 00:21:52,129
make your scripts async are there a lot

00:21:49,549 --> 00:21:55,789
of distinct domain lookups that add time

00:21:52,129 --> 00:21:57,889
to with the DNS responses and how about

00:21:55,789 --> 00:22:00,289
decreasing the number of actual files

00:21:57,889 --> 00:22:02,690
referenced or maybe minify and compress

00:22:00,289 --> 00:22:06,470
them there's a bunch of stuff that can

00:22:02,690 --> 00:22:08,359
be done with the front-end part and then

00:22:06,470 --> 00:22:09,900
finally perhaps you can use chunked

00:22:08,359 --> 00:22:13,170
encoding when returning large

00:22:09,900 --> 00:22:14,580
some data are you otherwise able to have

00:22:13,170 --> 00:22:16,590
your servers produce elements of

00:22:14,580 --> 00:22:18,600
response as they are needed rather than

00:22:16,590 --> 00:22:23,280
trying to produce all elements as fast

00:22:18,600 --> 00:22:26,520
as possible all right now probably the

00:22:23,280 --> 00:22:28,530
most interesting part so about a current

00:22:26,520 --> 00:22:30,150
tracing systems that are out there so

00:22:28,530 --> 00:22:31,890
there is an open standard for

00:22:30,150 --> 00:22:33,630
distributed tracing allowing developers

00:22:31,890 --> 00:22:35,640
to instrument their code without vendor

00:22:33,630 --> 00:22:39,630
lock-in and they do this by

00:22:35,640 --> 00:22:41,970
standardizing the trace span API one

00:22:39,630 --> 00:22:43,950
criticism I have of open tracing that

00:22:41,970 --> 00:22:46,350
they don't prescribe a way to implement

00:22:43,950 --> 00:22:48,860
more intelligent sampling other than a

00:22:46,350 --> 00:22:51,300
simple percentage and setting priority

00:22:48,860 --> 00:22:54,480
there's also a lack of standardization

00:22:51,300 --> 00:22:57,330
for how to track relationships whether

00:22:54,480 --> 00:23:00,480
submitter or trigger it's pretty much

00:22:57,330 --> 00:23:02,490
all submitter and it's mainly just the

00:23:00,480 --> 00:23:05,070
standardization for managing the span

00:23:02,490 --> 00:23:06,810
itself but mind you is a very young

00:23:05,070 --> 00:23:10,770
specification that's evolving and

00:23:06,810 --> 00:23:13,920
developing as we speak and there are a

00:23:10,770 --> 00:23:15,300
few self-hosted popular solutions that

00:23:13,920 --> 00:23:17,970
do support the open tracing

00:23:15,300 --> 00:23:20,670
specification probably the most widely

00:23:17,970 --> 00:23:23,460
used is the kin from twitter which has

00:23:20,670 --> 00:23:27,150
implementations in java go javascript

00:23:23,460 --> 00:23:29,580
ruby in scala the architecture setup is

00:23:27,150 --> 00:23:31,860
basically the instrumented app send data

00:23:29,580 --> 00:23:33,930
out-of-band to remote collector and that

00:23:31,860 --> 00:23:36,480
accepts a few different transport

00:23:33,930 --> 00:23:38,030
mechanisms including HTTP Kafka and

00:23:36,480 --> 00:23:40,980
scribe

00:23:38,030 --> 00:23:43,920
so with propagating data from a service

00:23:40,980 --> 00:23:48,870
all the current of Python libraries only

00:23:43,920 --> 00:23:51,390
support HTTP there's no RPC support and

00:23:48,870 --> 00:23:53,820
Zipkin does provide a nice Gantt chart

00:23:51,390 --> 00:23:56,910
or a wonderful part of individual traces

00:23:53,820 --> 00:23:59,220
you can see it can view a tree of

00:23:56,910 --> 00:24:01,890
dependencies but it's essentially only a

00:23:59,220 --> 00:24:05,300
tree with no information and like Layton

00:24:01,890 --> 00:24:07,590
sees or status codes or anything else

00:24:05,300 --> 00:24:09,570
using PI's opinion on which other

00:24:07,590 --> 00:24:11,730
libraries are based you can define a

00:24:09,570 --> 00:24:14,550
transport mechanism like I did here with

00:24:11,730 --> 00:24:17,310
HTTP transport which is can be just

00:24:14,550 --> 00:24:19,350
simply posting a request with the

00:24:17,310 --> 00:24:22,530
content of the trace you can otherwise

00:24:19,350 --> 00:24:23,779
make one for Kafka or describe but then

00:24:22,530 --> 00:24:25,580
otherwise it's just a

00:24:23,779 --> 00:24:31,340
context manager and being placed

00:24:25,580 --> 00:24:33,379
wherever you want to trace eager is

00:24:31,340 --> 00:24:35,389
another self-hosted system that supports

00:24:33,379 --> 00:24:38,960
open tracing specification and comes

00:24:35,389 --> 00:24:40,789
from uber rather than the application or

00:24:38,960 --> 00:24:44,149
client library reporting to a remote

00:24:40,789 --> 00:24:47,509
collector it reports to a local agent

00:24:44,149 --> 00:24:49,789
and via UDP who then sends out traces to

00:24:47,509 --> 00:24:52,669
a collector

00:24:49,789 --> 00:24:55,700
unlike Zipkin which supports Casca and

00:24:52,669 --> 00:24:59,839
elasticsearch in my sequel Yaeger only

00:24:55,700 --> 00:25:02,089
supports Cassandra for its storage the

00:24:59,839 --> 00:25:03,679
UI is a very similar to Zipkin oh is

00:25:02,089 --> 00:25:05,779
that a really pretty waterfall graphs

00:25:03,679 --> 00:25:07,429
and a defensive tree but again nothing

00:25:05,779 --> 00:25:09,739
to help aggregate that performance

00:25:07,429 --> 00:25:11,269
information we're interested in their

00:25:09,739 --> 00:25:12,979
documentation is also horribly lacking

00:25:11,269 --> 00:25:17,299
unfortunately but they do have a pretty

00:25:12,979 --> 00:25:19,369
decent tutorial to walk through their

00:25:17,299 --> 00:25:22,489
client library and for Python is a bit

00:25:19,369 --> 00:25:25,369
cringe worthy so this is a trimmed

00:25:22,489 --> 00:25:27,440
example from their Docs just meaning to

00:25:25,369 --> 00:25:29,570
give the gist here basically you can

00:25:27,440 --> 00:25:31,759
initialize a tracer set the open source

00:25:29,570 --> 00:25:33,979
or the open tracing Python library

00:25:31,759 --> 00:25:37,580
values and create a span a child spans

00:25:33,979 --> 00:25:40,279
contact managers but their usage at the

00:25:37,580 --> 00:25:44,210
end of time dots leap for yielding to IO

00:25:40,279 --> 00:25:46,099
IO loop it's a bit of a head-scratcher

00:25:44,210 --> 00:25:47,929
it's docks also make mention of

00:25:46,099 --> 00:25:50,559
supporting monkey-patching libraries

00:25:47,929 --> 00:25:54,759
like requests and Redis and euro Lib too

00:25:50,559 --> 00:25:57,739
so all I can say is use at your own risk

00:25:54,759 --> 00:26:00,049
after I present this at PyCon couple

00:25:57,739 --> 00:26:02,599
months ago like the day after they

00:26:00,049 --> 00:26:04,789
created an issue and basically made a

00:26:02,599 --> 00:26:09,019
comment in their code reasoning lines

00:26:04,789 --> 00:26:10,369
but I just still don't get why so there

00:26:09,019 --> 00:26:12,080
are a couple others I'm not familiar

00:26:10,369 --> 00:26:15,739
that's familiar with some including app

00:26:12,080 --> 00:26:18,200
- in light step and there are a few more

00:26:15,739 --> 00:26:21,769
that don't have Python client libraries

00:26:18,200 --> 00:26:23,330
yet in case you don't want to host your

00:26:21,769 --> 00:26:24,889
own system there are a few services out

00:26:23,330 --> 00:26:27,349
there to help

00:26:24,889 --> 00:26:29,499
there's stackdriver tracing google not

00:26:27,349 --> 00:26:33,169
to be confused with jack driver logging

00:26:29,499 --> 00:26:35,089
so unfortunately Google has no Python or

00:26:33,169 --> 00:26:35,640
G or PC client libraries instrument your

00:26:35,089 --> 00:26:37,860
Apple

00:26:35,640 --> 00:26:41,940
but they do have a rest and RPC

00:26:37,860 --> 00:26:44,370
interface if you feel so inclined but

00:26:41,940 --> 00:26:46,110
they do support as it can traces where

00:26:44,370 --> 00:26:48,420
you can set up a Google flavored to

00:26:46,110 --> 00:26:49,410
conserver either on their infrastructure

00:26:48,420 --> 00:26:51,450
or on yours

00:26:49,410 --> 00:26:52,650
and have it forward traces to

00:26:51,450 --> 00:26:54,810
stackdriver

00:26:52,650 --> 00:26:56,850
and they actually make it pretty easily

00:26:54,810 --> 00:26:58,290
pretty easy I was able to spin up a

00:26:56,850 --> 00:27:01,740
docker image and start doing traces

00:26:58,290 --> 00:27:03,540
within a couple minutes and knowingly if

00:27:01,740 --> 00:27:07,890
they have a storage limitation of 30

00:27:03,540 --> 00:27:10,410
days and same with their logging and my

00:27:07,890 --> 00:27:12,690
last criticism is their UI they have

00:27:10,410 --> 00:27:14,760
simple plots of response time over the

00:27:12,690 --> 00:27:16,770
past few hours and a list of all traces

00:27:14,760 --> 00:27:18,750
that are automatically provided in the

00:27:16,770 --> 00:27:20,970
UI but you have to like manually make

00:27:18,750 --> 00:27:22,230
analysis reports for each time period

00:27:20,970 --> 00:27:24,180
that you're interested in to get all

00:27:22,230 --> 00:27:25,410
that fancy distribution graphs they are

00:27:24,180 --> 00:27:29,190
not automatically generated

00:27:25,410 --> 00:27:31,290
unfortunately and then finally Amazon

00:27:29,190 --> 00:27:33,960
also has a tracing service available

00:27:31,290 --> 00:27:35,730
called x-ray I only set up their demo

00:27:33,960 --> 00:27:38,670
apps but and it looks like they do not

00:27:35,730 --> 00:27:42,750
explicitly support Python only no Java

00:27:38,670 --> 00:27:45,960
and.net paths but the Python SDK bottom

00:27:42,750 --> 00:27:48,320
has support for sending traces to a

00:27:45,960 --> 00:27:51,480
local daemon which then forwards to the

00:27:48,320 --> 00:27:53,880
x-ray service and what's nice about

00:27:51,480 --> 00:27:56,040
x-ray despite it being proprietary and

00:27:53,880 --> 00:27:58,200
not open tracing compliance and is

00:27:56,040 --> 00:28:00,660
you're able to configure sampling rates

00:27:58,200 --> 00:28:02,790
for different URL routes of your

00:28:00,660 --> 00:28:04,980
application based on either fixed

00:28:02,790 --> 00:28:07,920
requests per second or a percentage of

00:28:04,980 --> 00:28:12,060
requests however it's not impossible to

00:28:07,920 --> 00:28:14,040
configure these rules with bottom also

00:28:12,060 --> 00:28:16,650
are almost redeemable reserved

00:28:14,040 --> 00:28:18,720
visualizations so while there's a

00:28:16,650 --> 00:28:20,850
typical waterfall chart they also have a

00:28:18,720 --> 00:28:23,760
request flow graphic and where you can

00:28:20,850 --> 00:28:26,310
see average latency and captured traces

00:28:23,760 --> 00:28:30,030
per minute and requests broken down by

00:28:26,310 --> 00:28:32,220
response status so basically an AWS and

00:28:30,030 --> 00:28:34,590
x-ray seems pretty cool and probably the

00:28:32,220 --> 00:28:36,120
most useful out of all of these but

00:28:34,590 --> 00:28:40,140
it'll take some time instrumenting your

00:28:36,120 --> 00:28:42,030
app and introduces vendor lock-in and

00:28:40,140 --> 00:28:44,070
some arbol mentions that do app

00:28:42,030 --> 00:28:46,140
performance mentorships measurement I

00:28:44,070 --> 00:28:48,210
don't have personal experience with

00:28:46,140 --> 00:28:51,360
these but data dog in New Relic

00:28:48,210 --> 00:28:53,550
might be of interest to some of you all

00:28:51,360 --> 00:28:55,050
right a quick opinionated wrap-up got

00:28:53,550 --> 00:28:56,720
like a minute here if you run micro

00:28:55,050 --> 00:28:58,590
services you should be tracing them

00:28:56,720 --> 00:29:00,420
otherwise it's very difficult to

00:28:58,590 --> 00:29:02,580
understand an entire systems performance

00:29:00,420 --> 00:29:05,580
anomalistic behavior resource usage

00:29:02,580 --> 00:29:08,550
among other many aspects however good

00:29:05,580 --> 00:29:10,620
luck whether you choose a self-hosted

00:29:08,550 --> 00:29:12,350
solution or a provided service

00:29:10,620 --> 00:29:14,760
documentation is all-around lacking

00:29:12,350 --> 00:29:16,800
granted very young space very much

00:29:14,760 --> 00:29:19,950
growing as open tracing standard

00:29:16,800 --> 00:29:21,210
standard isn't developing and as I

00:29:19,950 --> 00:29:23,490
mentioned language support isn't a

00:29:21,210 --> 00:29:26,280
hundred percent even if it might not

00:29:23,490 --> 00:29:27,900
even be there and there's a lack of

00:29:26,280 --> 00:29:30,680
configuration for a relationship

00:29:27,900 --> 00:29:34,860
tracking or intelligent sampling and

00:29:30,680 --> 00:29:36,870
available visualizations but it is

00:29:34,860 --> 00:29:39,390
indeed an open spec and that can be

00:29:36,870 --> 00:29:41,520
influenced or you might feel so inclined

00:29:39,390 --> 00:29:45,330
to implement your own which I say good

00:29:41,520 --> 00:29:47,880
luck and then finally all this and some

00:29:45,330 --> 00:29:51,990
pretty graphs and stuff is up on my blog

00:29:47,880 --> 00:29:58,779
post appear you're interested thank you

00:29:51,990 --> 00:29:58,779

YouTube URL: https://www.youtube.com/watch?v=pprCOMmGnS0


