Title: Shailen Sobhee - Python Profiling with Intel® VTune™ Amplifier
Publication date: 2017-09-17
Playlist: EuroPython 2017
Description: 
	"Python Profiling with Intel® VTune™ Amplifier
[EuroPython 2017 - Talk - 2017-07-10 - PythonAnywhere Room]
[Rimini, Italy]

Python has grown in both significance and popularity in the last years, especially in the field of high performance computing and machine learning. When it comes to performance, there are numerous ways of profiling and measuring code performance—with each analysis tool having its own strengths and weaknesses. In this talk, we will introduce a rich GUI application (Intel® VTune™ Amplifier) which can be used to analyze the runtime performance of one’s Python application, and fully understand where the performance bottlenecks are in one’s code.  With this application, one may also analyze the call-stacks and get quick visual clues where one’s Python application is spending time or wasting CPU cycles.

License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2017.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:04,730 --> 00:00:11,240
I'm Shaolin I am a technical consulting

00:00:07,040 --> 00:00:13,429
engineer not for me okay

00:00:11,240 --> 00:00:15,559
at Intel in the developer products

00:00:13,429 --> 00:00:18,580
division team and we are based in Munich

00:00:15,559 --> 00:00:21,169
Germany and today's focus will be

00:00:18,580 --> 00:00:25,610
performance analysis of patent

00:00:21,169 --> 00:00:28,130
applications and yeah we have to say

00:00:25,610 --> 00:00:29,960
it's of no denial that the Biden is

00:00:28,130 --> 00:00:33,620
getting a lot of traction a lot of

00:00:29,960 --> 00:00:36,950
importance these days and if you look at

00:00:33,620 --> 00:00:41,360
what our friends are the good evil have

00:00:36,950 --> 00:00:43,640
published indeed Fyren has grown in

00:00:41,360 --> 00:00:44,810
popularity over the last years and in

00:00:43,640 --> 00:00:48,140
00:00:44,810 --> 00:00:50,870
Ayten remains the number one most used

00:00:48,140 --> 00:00:53,120
language and also what is more

00:00:50,870 --> 00:00:56,899
surprising is that python remains the

00:00:53,120 --> 00:01:01,880
number one programming language in

00:00:56,899 --> 00:01:05,600
hiring demand so it's a great skill to

00:01:01,880 --> 00:01:12,289
have in this decade to be able to be

00:01:05,600 --> 00:01:13,880
proficient in Python and when it comes

00:01:12,289 --> 00:01:17,139
to performance analysis there are

00:01:13,880 --> 00:01:19,899
certain fields that are kind of driving

00:01:17,139 --> 00:01:21,889
the technologies of the future and

00:01:19,899 --> 00:01:25,969
technologies that are kind of really

00:01:21,889 --> 00:01:28,459
important right now and these fields I

00:01:25,969 --> 00:01:33,229
would say would be mathematics and data

00:01:28,459 --> 00:01:34,549
science and to get my facts straight to

00:01:33,229 --> 00:01:37,579
get the numbers correct

00:01:34,549 --> 00:01:39,619
I want to stack overflow our favorite

00:01:37,579 --> 00:01:42,560
website where we have problems and Stack

00:01:39,619 --> 00:01:45,979
Overflow shows me that indeed button is

00:01:42,560 --> 00:01:49,340
the most used language in the field of

00:01:45,979 --> 00:01:51,740
mathematics and data science now you may

00:01:49,340 --> 00:01:53,599
think the math doesn't make sense

00:01:51,740 --> 00:01:56,299
if you add all the percentage it doesn't

00:01:53,599 --> 00:02:01,009
make to 100 well that's because out of

00:01:56,299 --> 00:02:03,560
those approximately 50k people who

00:02:01,009 --> 00:02:05,990
responded to the survey they chose

00:02:03,560 --> 00:02:11,140
several languages but most of them chose

00:02:05,990 --> 00:02:14,840
Biden over 50% of this so that's quite

00:02:11,140 --> 00:02:17,530
impressive so math and data science

00:02:14,840 --> 00:02:19,900
these fields

00:02:17,530 --> 00:02:22,959
actually drive high-performance

00:02:19,900 --> 00:02:25,030
computing or HPC and other fields like

00:02:22,959 --> 00:02:30,300
artificial intelligence machine learning

00:02:25,030 --> 00:02:33,850
or deep learning and enter realizes that

00:02:30,300 --> 00:02:36,160
these fields are going to define the

00:02:33,850 --> 00:02:40,120
future and so we have worked really hard

00:02:36,160 --> 00:02:42,730
to release a distribution of software

00:02:40,120 --> 00:02:45,100
which we call the Intel distribution for

00:02:42,730 --> 00:02:49,900
baton and it comes up out of the box

00:02:45,100 --> 00:02:51,940
with highly optimized sub libraries to

00:02:49,900 --> 00:02:55,620
allow you to develop high performance

00:02:51,940 --> 00:02:55,620
applications with baton

00:02:55,769 --> 00:03:01,540
we made it super easy to use

00:02:58,239 --> 00:03:07,150
super easy to install packages can be

00:03:01,540 --> 00:03:12,340
easily downloaded for anaconda or or

00:03:07,150 --> 00:03:14,500
even um providing the RPMs and our

00:03:12,340 --> 00:03:17,290
distribution of Python comes with highly

00:03:14,500 --> 00:03:21,180
optimized libraries like numpy sci-fi

00:03:17,290 --> 00:03:24,579
psychic learn which actually at the base

00:03:21,180 --> 00:03:27,430
leverages in that nkl which is short for

00:03:24,579 --> 00:03:30,310
math kernal library now in itself and

00:03:27,430 --> 00:03:34,120
gale if you've not heard about it a few

00:03:30,310 --> 00:03:37,390
words about it it's made in assembly

00:03:34,120 --> 00:03:39,579
it's super optimized mathematical

00:03:37,390 --> 00:03:42,519
routines have been designed to make the

00:03:39,579 --> 00:03:45,549
most out of the intel architecture how

00:03:42,519 --> 00:03:47,709
many calls you have make use of the

00:03:45,549 --> 00:03:50,350
latest instruction set architecture for

00:03:47,709 --> 00:03:52,870
instance AVX avx-512 whatever you have

00:03:50,350 --> 00:03:54,820
out of the box make the most of

00:03:52,870 --> 00:03:58,150
vectorization so you don't have to worry

00:03:54,820 --> 00:04:04,420
about this by using the intel

00:03:58,150 --> 00:04:07,840
distribution for python now performance

00:04:04,420 --> 00:04:09,820
is really important so how do we

00:04:07,840 --> 00:04:13,540
actually measure performance of a python

00:04:09,820 --> 00:04:17,560
application and the intel vtune

00:04:13,540 --> 00:04:18,280
amplifier vtrene amplifier is a code

00:04:17,560 --> 00:04:21,100
profiler

00:04:18,280 --> 00:04:23,470
it is a profiler that allows you to know

00:04:21,100 --> 00:04:26,169
where are the performance problems in

00:04:23,470 --> 00:04:28,870
your software it has been developed over

00:04:26,169 --> 00:04:30,639
many years over 15 years and it's still

00:04:28,870 --> 00:04:33,789
in development we're getting

00:04:30,639 --> 00:04:36,490
a lot of improvements Deb every day our

00:04:33,789 --> 00:04:38,439
engineers are working hard and over the

00:04:36,490 --> 00:04:41,949
last four years we have worked on

00:04:38,439 --> 00:04:44,349
profiling patent and what is great is

00:04:41,949 --> 00:04:46,900
that it comes with its low overhead

00:04:44,349 --> 00:04:49,810
sampling technology which is unrivaled

00:04:46,900 --> 00:04:52,590
no others profiler is able to get

00:04:49,810 --> 00:04:55,479
performance data as bad as good as

00:04:52,590 --> 00:04:57,159
individual amplifier so there are some

00:04:55,479 --> 00:04:59,590
techniques how we are able to get

00:04:57,159 --> 00:05:01,060
performance data with low overhead

00:04:59,590 --> 00:05:03,669
so basically when big brother is

00:05:01,060 --> 00:05:06,000
watching there is no big impact on the

00:05:03,669 --> 00:05:08,740
performance of your real application

00:05:06,000 --> 00:05:12,719
with Intel vision amplifier we are able

00:05:08,740 --> 00:05:12,719
to get precise

00:05:26,810 --> 00:05:33,569
we're able to get precise line level

00:05:29,729 --> 00:05:36,900
information some profilers allow you to

00:05:33,569 --> 00:05:39,180
do that but others you may use give you

00:05:36,900 --> 00:05:40,740
data at the function level so basically

00:05:39,180 --> 00:05:42,240
you have to kind of guess where the

00:05:40,740 --> 00:05:45,090
performance is we have a big function

00:05:42,240 --> 00:05:47,940
but with region you get right to the

00:05:45,090 --> 00:05:50,250
source line where there are bottlenecks

00:05:47,940 --> 00:05:51,810
now a bottleneck is basically like you

00:05:50,250 --> 00:05:54,030
know the bottle and the neck this is

00:05:51,810 --> 00:05:56,970
where the performance is kind of capped

00:05:54,030 --> 00:06:00,870
and our goal is to find those errors in

00:05:56,970 --> 00:06:03,509
your code and optimize on them in order

00:06:00,870 --> 00:06:06,300
to eventually increase the performance

00:06:03,509 --> 00:06:09,659
of your application and what is also

00:06:06,300 --> 00:06:12,750
great is that we can not only analyze

00:06:09,659 --> 00:06:16,949
the Python performance but also sight

00:06:12,750 --> 00:06:21,319
and language and if applicable any C

00:06:16,949 --> 00:06:23,400
code that your Python code is calling

00:06:21,319 --> 00:06:27,330
essentially you can analyze your whole

00:06:23,400 --> 00:06:31,380
system and get data about just not just

00:06:27,330 --> 00:06:33,990
the Python binary and and the bottom

00:06:31,380 --> 00:06:40,020
files been called but other modules that

00:06:33,990 --> 00:06:43,050
can be built in in C or C++ so in the

00:06:40,020 --> 00:06:45,150
coming ten to fifteen minutes so I'll be

00:06:43,050 --> 00:06:47,370
talking about why Python optimization is

00:06:45,150 --> 00:06:50,880
important how do we find those

00:06:47,370 --> 00:06:52,740
bottlenecks and a very short overview of

00:06:50,880 --> 00:06:56,969
the various profilers available on the

00:06:52,740 --> 00:06:59,759
market and then a very quick demo of how

00:06:56,969 --> 00:07:03,120
the GUI looks like what you see in the

00:06:59,759 --> 00:07:05,909
tool and if you worry about mixed mode

00:07:03,120 --> 00:07:06,840
profiling so why do we need byte an

00:07:05,909 --> 00:07:09,960
optimization

00:07:06,840 --> 00:07:12,630
well it's no denial Python is everywhere

00:07:09,960 --> 00:07:15,380
python is being used in a lot of

00:07:12,630 --> 00:07:18,539
application that today need a lot of

00:07:15,380 --> 00:07:23,669
performance so if you look at the web

00:07:18,539 --> 00:07:26,969
frameworks django turbo gears flask

00:07:23,669 --> 00:07:28,580
so all these require that stuff be done

00:07:26,969 --> 00:07:31,590
really really really fast

00:07:28,580 --> 00:07:34,529
there are build systems like s cons

00:07:31,590 --> 00:07:36,779
built BOTS don't know if you use it in

00:07:34,529 --> 00:07:39,060
your company but we use billboard for

00:07:36,779 --> 00:07:40,440
instance actually to build the package

00:07:39,060 --> 00:07:44,010
for intelligent amplifier

00:07:40,440 --> 00:07:46,020
and other tools across Intel scientific

00:07:44,010 --> 00:07:48,800
calculations there are tools like free

00:07:46,020 --> 00:07:53,160
card it's a 3d modeling software that

00:07:48,800 --> 00:07:55,410
that has large sections built in Python

00:07:53,160 --> 00:07:57,420
so these require a high performance

00:07:55,410 --> 00:08:01,140
there are also other tools if if you

00:07:57,420 --> 00:08:04,130
want if you know and Linux made out

00:08:01,140 --> 00:08:08,070
of button games there are games like

00:08:04,130 --> 00:08:10,650
civilization for that seems for these

00:08:08,070 --> 00:08:13,050
are Python based games obviously you

00:08:10,650 --> 00:08:16,260
want your game to to be efficient and

00:08:13,050 --> 00:08:19,350
run fast right so how do we measure the

00:08:16,260 --> 00:08:21,990
performance there are a couple of

00:08:19,350 --> 00:08:24,770
techniques there is code examination you

00:08:21,990 --> 00:08:28,260
can open the editor and check the code

00:08:24,770 --> 00:08:30,960
this can be very tedious if you don't

00:08:28,260 --> 00:08:32,430
own the code you have encoded it or if

00:08:30,960 --> 00:08:35,330
the code is super large how would you

00:08:32,430 --> 00:08:38,669
check everything but that's one way

00:08:35,330 --> 00:08:42,740
there's another way logging you

00:08:38,669 --> 00:08:45,870
basically enter pieces of code in your

00:08:42,740 --> 00:08:48,120
in your Python script and say ok print

00:08:45,870 --> 00:08:50,700
this time step here and then let me know

00:08:48,120 --> 00:08:53,790
at the end of my function how much

00:08:50,700 --> 00:08:58,170
standard function run this is also

00:08:53,790 --> 00:09:00,690
devious man your work then there is

00:08:58,170 --> 00:09:04,950
profiling profiling is basically the

00:09:00,690 --> 00:09:06,900
crowd how intelligent amplifier works in

00:09:04,950 --> 00:09:10,860
a sense what we are going to do is

00:09:06,900 --> 00:09:12,780
gather metrics from the system as the

00:09:10,860 --> 00:09:14,370
application is running and then at the

00:09:12,780 --> 00:09:16,620
end of the run we are going to analyze

00:09:14,370 --> 00:09:21,090
all those metrics and make sense out of

00:09:16,620 --> 00:09:23,970
all the data that we get and we are

00:09:21,090 --> 00:09:27,480
going to focus on CPU hotspot profiling

00:09:23,970 --> 00:09:29,400
and find places in your code where your

00:09:27,480 --> 00:09:31,950
code is spending a lot of time on the

00:09:29,400 --> 00:09:34,050
CPU or wasting a lot of time off you

00:09:31,950 --> 00:09:36,390
have a threaded application whether one

00:09:34,050 --> 00:09:39,350
thread is waiting on a lock and not

00:09:36,390 --> 00:09:41,730
doing anything or essentially stalling

00:09:39,350 --> 00:09:45,770
finding those issues and removing them

00:09:41,730 --> 00:09:45,770
is the way to go

00:09:48,149 --> 00:09:54,579
now profiling there are a couple of

00:09:51,790 --> 00:09:59,370
types of profiling there is even based

00:09:54,579 --> 00:10:03,399
profiling which is essentially

00:09:59,370 --> 00:10:06,820
collecting data when certain events

00:10:03,399 --> 00:10:08,950
happen for instance entering a function

00:10:06,820 --> 00:10:11,140
or exiting a function or loading a class

00:10:08,950 --> 00:10:12,880
and loading the class so things like

00:10:11,140 --> 00:10:16,300
that so these are those certain events

00:10:12,880 --> 00:10:17,950
we get performance data there is also

00:10:16,300 --> 00:10:21,310
instrumentation where the target

00:10:17,950 --> 00:10:24,550
application is modified and basically

00:10:21,310 --> 00:10:29,250
the application profiles itself and then

00:10:24,550 --> 00:10:32,680
there is something statistical profiling

00:10:29,250 --> 00:10:35,829
now this is how vision works vision is a

00:10:32,680 --> 00:10:39,149
statistical performance profiler there

00:10:35,829 --> 00:10:42,940
are some caveats to bear in mind

00:10:39,149 --> 00:10:43,899
obviously as a statistical method the

00:10:42,940 --> 00:10:45,250
larger the data

00:10:43,899 --> 00:10:47,170
the larger the time that your

00:10:45,250 --> 00:10:49,450
application is running the more accurate

00:10:47,170 --> 00:10:51,940
it is so this is why I have underlined

00:10:49,450 --> 00:10:54,279
approximate there but I've also put

00:10:51,940 --> 00:10:57,040
involved much more in much less

00:10:54,279 --> 00:10:59,079
intrusive so with this statistical

00:10:57,040 --> 00:11:01,029
method that we employ in order to

00:10:59,079 --> 00:11:04,930
measure performance of patent

00:11:01,029 --> 00:11:12,699
applications were able to to get low

00:11:04,930 --> 00:11:14,500
overhead performance profiles and the

00:11:12,699 --> 00:11:19,660
longer your application runs the better

00:11:14,500 --> 00:11:23,079
the results this is a short overview of

00:11:19,660 --> 00:11:25,870
the various profilers you may have seen

00:11:23,079 --> 00:11:28,180
or not there may be others but these are

00:11:25,870 --> 00:11:31,600
the most common ones Intel vision

00:11:28,180 --> 00:11:34,060
amplifier what is great is it with it is

00:11:31,600 --> 00:11:36,839
that it comes with a rich highly

00:11:34,060 --> 00:11:40,570
advanced highly customizable GUI viewer

00:11:36,839 --> 00:11:43,420
in order to see quickly and visually

00:11:40,570 --> 00:11:46,600
where all the problems works on Linux

00:11:43,420 --> 00:11:49,300
windows and what is also nice is this

00:11:46,600 --> 00:11:51,550
line level profiling not at the function

00:11:49,300 --> 00:11:54,790
level but right at the source line where

00:11:51,550 --> 00:11:58,319
your problems are and overhead very

00:11:54,790 --> 00:12:00,170
important patent interpreted world only

00:11:58,319 --> 00:12:02,030
1.1 X

00:12:00,170 --> 00:12:04,300
performance it and that's a really low

00:12:02,030 --> 00:12:07,760
number compared to other line profilers

00:12:04,300 --> 00:12:09,980
like line profile itself which has a 10x

00:12:07,760 --> 00:12:13,220
performance hit so basically when you

00:12:09,980 --> 00:12:16,850
use line profile it's unusable you get

00:12:13,220 --> 00:12:19,010
bogus theta-c profile gets you data the

00:12:16,850 --> 00:12:20,840
function level with a relatively low

00:12:19,010 --> 00:12:24,440
overhead but then again it's the

00:12:20,840 --> 00:12:27,080
granularity is very coarse and there are

00:12:24,440 --> 00:12:31,390
also other Python tools that come

00:12:27,080 --> 00:12:40,610
bundled in IDs I visual studio again

00:12:31,390 --> 00:12:43,240
function level 2x performance hit our

00:12:40,610 --> 00:12:45,980
tool works with basically every button

00:12:43,240 --> 00:12:48,080
distribution you may be using the even

00:12:45,980 --> 00:12:49,550
the Python distribution supplied by

00:12:48,080 --> 00:12:53,510
Ubuntu or whatever system you're using

00:12:49,550 --> 00:12:55,220
or our own obviously on the Intel

00:12:53,510 --> 00:13:01,210
distribution for Python which is built

00:12:55,220 --> 00:13:03,890
with ICC support for 2.7 X Python free

00:13:01,210 --> 00:13:05,600
and remote collection already set so you

00:13:03,890 --> 00:13:07,700
can be using a Windows machine and then

00:13:05,600 --> 00:13:09,530
you can remote profile a Linux machine

00:13:07,700 --> 00:13:12,250
where your Python code is running so

00:13:09,530 --> 00:13:12,250
that's really great

00:13:13,930 --> 00:13:18,650
you can attach to a running process if

00:13:16,670 --> 00:13:20,930
your Python code cannot be stopped you

00:13:18,650 --> 00:13:25,880
can just attach to the PID and get

00:13:20,930 --> 00:13:27,980
performance data and analyzing

00:13:25,880 --> 00:13:31,520
performance is actually really simple

00:13:27,980 --> 00:13:35,150
some three basic steps create the

00:13:31,520 --> 00:13:38,710
project in our tool configure the

00:13:35,150 --> 00:13:43,430
various settings run interpret

00:13:38,710 --> 00:13:45,400
essentially I did a small test just to

00:13:43,430 --> 00:13:49,370
show you how it works

00:13:45,400 --> 00:13:53,620
so I have actually a code in Python is

00:13:49,370 --> 00:13:53,620
doing something very very simple

00:14:00,470 --> 00:14:07,510
I will show you this piece of good I

00:14:03,460 --> 00:14:07,510
hope it's not too small

00:14:10,640 --> 00:14:16,070
can you see it is it good enough yeah

00:14:13,490 --> 00:14:17,930
okay I get the thumbs up so it's good so

00:14:16,070 --> 00:14:20,090
this code is very simple not a lot of

00:14:17,930 --> 00:14:22,550
lines of code that's it only one script

00:14:20,090 --> 00:14:24,710
but it does some computation some heavy

00:14:22,550 --> 00:14:28,190
computation so imagine seeing this in

00:14:24,710 --> 00:14:33,040
some high performance kernels what it

00:14:28,190 --> 00:14:33,040
does is there is a small main script

00:14:33,370 --> 00:14:41,240
there are two parts one is going to use

00:14:37,040 --> 00:14:45,110
multi processing and create the two two

00:14:41,240 --> 00:14:47,510
sub processes and then call multiply

00:14:45,110 --> 00:14:52,700
which is essentially going to multiply

00:14:47,510 --> 00:14:54,890
as it says two matrices a times B and

00:14:52,700 --> 00:14:58,660
store it in C so we are going to create

00:14:54,890 --> 00:15:04,210
two sub processes and do this highly

00:14:58,660 --> 00:15:10,910
while quite badly made free nested loop

00:15:04,210 --> 00:15:12,230
computation here so if you guys do this

00:15:10,910 --> 00:15:16,310
don't do it it's really bad

00:15:12,230 --> 00:15:18,140
implementation okay and and then there

00:15:16,310 --> 00:15:21,590
is another method which is out of the

00:15:18,140 --> 00:15:26,330
box using numpy so this is the blast

00:15:21,590 --> 00:15:28,070
multiply okay so basically in algebra

00:15:26,330 --> 00:15:30,380
and then we are going to run the code

00:15:28,070 --> 00:15:32,810
I've already run it in my Linux virtual

00:15:30,380 --> 00:15:34,820
machine I collected the results in order

00:15:32,810 --> 00:15:37,190
to save time and opened it in vision

00:15:34,820 --> 00:15:40,820
here on windows so this is how it looks

00:15:37,190 --> 00:15:51,980
like I have in my summary page and

00:15:40,820 --> 00:15:54,380
overview of an overview of my of the

00:15:51,980 --> 00:15:56,210
time that the application has run there

00:15:54,380 --> 00:15:59,390
is also the CPU time which is basically

00:15:56,210 --> 00:16:01,580
the time per CPU core here I see a

00:15:59,390 --> 00:16:03,560
hundred and thirteen which is which

00:16:01,580 --> 00:16:05,750
looks good because I have a dual-core

00:16:03,560 --> 00:16:07,430
system and it ups time my wall clock

00:16:05,750 --> 00:16:09,380
time was fifty seven times to

00:16:07,430 --> 00:16:11,620
approximately 100 so my code was

00:16:09,380 --> 00:16:13,720
actually quite parallelized

00:16:11,620 --> 00:16:17,879
and you can also see in the CPU usage

00:16:13,720 --> 00:16:24,279
histogram my CPU concurrency was 2 and

00:16:17,879 --> 00:16:28,959
that's great and some collection

00:16:24,279 --> 00:16:30,910
platform but although it opened to multi

00:16:28,959 --> 00:16:32,319
processes because of a two core systems

00:16:30,910 --> 00:16:35,129
that doesn't mean that it was great

00:16:32,319 --> 00:16:38,589
because you know we're free nested loop

00:16:35,129 --> 00:16:42,310
it's not so nice that's why I also in

00:16:38,589 --> 00:16:47,290
this script I'm also profiling the

00:16:42,310 --> 00:16:51,160
performance of this blast numpy good if

00:16:47,290 --> 00:16:53,199
I go in the bottom up or actually one

00:16:51,160 --> 00:16:55,000
more thing in the top hotspot it has

00:16:53,199 --> 00:16:58,660
already listed where you need to spend

00:16:55,000 --> 00:17:01,240
time to optimize your code so if I go

00:16:58,660 --> 00:17:03,279
into the bottom up this it has sorted

00:17:01,240 --> 00:17:09,390
all the various methods called in your

00:17:03,279 --> 00:17:14,829
Python script and you can see that

00:17:09,390 --> 00:17:17,500
multiply the aggregation of those two

00:17:14,829 --> 00:17:20,439
multiplies contributed to most of the

00:17:17,500 --> 00:17:23,740
time and because I've also collected the

00:17:20,439 --> 00:17:27,339
call stack I can go and drill down to

00:17:23,740 --> 00:17:30,870
how my method was called in my Python

00:17:27,339 --> 00:17:30,870
script I can double click on it and

00:17:31,350 --> 00:17:36,880
which it will open the source file and

00:17:34,659 --> 00:17:38,740
tag at the source line where most time

00:17:36,880 --> 00:17:41,140
was spent so this is were I didn't have

00:17:38,740 --> 00:17:42,340
double clicked on that line of that on

00:17:41,140 --> 00:17:47,850
that call stack line and it has

00:17:42,340 --> 00:17:54,820
automatically opened the source script I

00:17:47,850 --> 00:17:57,039
can move that line a bit here so we can

00:17:54,820 --> 00:18:01,720
see that most of the time was spent in

00:17:57,039 --> 00:18:04,740
doing this matrix multiplication 26

00:18:01,720 --> 00:18:04,740
percent of CPU usage

00:18:08,450 --> 00:18:13,670
and going back to the bottom up we can

00:18:10,670 --> 00:18:15,890
see the timeline how active was my CPU

00:18:13,670 --> 00:18:18,770
over the whole runtime of my application

00:18:15,890 --> 00:18:21,800
you can see that for the two

00:18:18,770 --> 00:18:24,170
multi-process that the package

00:18:21,800 --> 00:18:26,510
multi-process has created my CPU was

00:18:24,170 --> 00:18:29,020
active both processors were busy

00:18:26,510 --> 00:18:33,050
computing the matrix multiplication and

00:18:29,020 --> 00:18:33,980
then at the end of my stupid

00:18:33,050 --> 00:18:36,620
multiplication

00:18:33,980 --> 00:18:40,880
I had the Blas one and this can be seen

00:18:36,620 --> 00:18:44,530
at the very tiny end here I can do zoom

00:18:40,880 --> 00:18:44,530
in and filter in by selection

00:19:03,410 --> 00:19:09,650
this is the zoomed in timeline there is

00:19:06,919 --> 00:19:12,110
a very tiny little piece on that main

00:19:09,650 --> 00:19:15,650
thread which is Fredd ID three three

00:19:12,110 --> 00:19:23,320
four five and that was the blast version

00:19:15,650 --> 00:19:26,030
using numpy we can even zoom in further

00:19:23,320 --> 00:19:28,640
filter in zoom in and filter in so what

00:19:26,030 --> 00:19:30,559
this does is it will get that timeline

00:19:28,640 --> 00:19:32,150
I'm zooming in and then filter on the

00:19:30,559 --> 00:19:35,419
time I even tell me during that time

00:19:32,150 --> 00:19:38,390
line which methods were being called so

00:19:35,419 --> 00:19:44,360
even more control and more power and

00:19:38,390 --> 00:19:47,720
what you see so I can see that for this

00:19:44,360 --> 00:19:51,440
little part here for instance are a

00:19:47,720 --> 00:19:54,770
matrix product was called it is a shared

00:19:51,440 --> 00:20:00,440
object so an Empire Builder which this

00:19:54,770 --> 00:20:03,549
is a shared object and then the gold

00:20:00,440 --> 00:20:03,549
stocks phone and by

00:20:09,680 --> 00:20:30,650
so going back to my sides you are able

00:20:28,190 --> 00:20:34,340
to also run mixed mode analysis so

00:20:30,650 --> 00:20:37,970
basically get performance information

00:20:34,340 --> 00:20:40,520
about your Python code and also seitan

00:20:37,970 --> 00:20:45,980
or native code being called in your

00:20:40,520 --> 00:20:51,230
application bit C C++ and you get all

00:20:45,980 --> 00:20:53,510
these for instance here shared object so

00:20:51,230 --> 00:20:59,570
that's a native library and the other

00:20:53,510 --> 00:21:04,340
one is pi so um python script so the

00:20:59,570 --> 00:21:06,590
summary training your application

00:21:04,340 --> 00:21:09,560
obviously is a good thing everybody has

00:21:06,590 --> 00:21:13,400
to do it there are ways to do it which

00:21:09,560 --> 00:21:15,440
one is a tool for it um because I've

00:21:13,400 --> 00:21:17,830
been asked earlier by Tomas who is

00:21:15,440 --> 00:21:20,000
sitting in front maybe that's

00:21:17,830 --> 00:21:21,920
interesting for you in television

00:21:20,000 --> 00:21:23,470
amplify is a commercial product but

00:21:21,920 --> 00:21:26,510
there are ways also to get it for free

00:21:23,470 --> 00:21:29,000
it's for free in the beta program so if

00:21:26,510 --> 00:21:30,830
you sign up for the beta 2008 and 18

00:21:29,000 --> 00:21:33,620
that comes up with more advanced

00:21:30,830 --> 00:21:36,140
profiling capabilities for vision for

00:21:33,620 --> 00:21:37,970
instance getting detailed information

00:21:36,140 --> 00:21:41,240
about credit applications and also

00:21:37,970 --> 00:21:44,450
memory consumption it's available in the

00:21:41,240 --> 00:21:46,040
18 version beta it's for free for

00:21:44,450 --> 00:21:50,150
testing evaluation for a long period of

00:21:46,040 --> 00:21:54,190
time it's also for free for all people

00:21:50,150 --> 00:21:55,490
in academics students professors

00:21:54,190 --> 00:21:59,450
universities

00:21:55,490 --> 00:22:02,630
anybody from academia it's for free but

00:21:59,450 --> 00:22:05,420
only for companies that work on real

00:22:02,630 --> 00:22:07,760
projects and generate money you require

00:22:05,420 --> 00:22:09,380
license just a small word about it I'm

00:22:07,760 --> 00:22:11,780
an engineer I don't talk about business

00:22:09,380 --> 00:22:15,620
but I think that might be relevant for

00:22:11,780 --> 00:22:17,840
you you may get more information in to

00:22:15,620 --> 00:22:21,170
talk so conducted by my colleague David

00:22:17,840 --> 00:22:22,909
Levy there is infrastructure design

00:22:21,170 --> 00:22:25,249
patterns with patent on

00:22:22,909 --> 00:22:27,019
but what is more relevant to this talk

00:22:25,249 --> 00:22:29,179
would be probably the workshop and

00:22:27,019 --> 00:22:31,609
Thursday which is all about the hands-on

00:22:29,179 --> 00:22:34,669
on how to tune your application with our

00:22:31,609 --> 00:22:36,050
tools on this thank you very much for

00:22:34,669 --> 00:22:43,339
your attention

00:22:36,050 --> 00:22:43,339
[Applause]

00:22:51,639 --> 00:22:59,389
hi and thanks for your talk if I

00:22:55,940 --> 00:23:02,629
understand well you can annotate the

00:22:59,389 --> 00:23:05,779
source of Python and also see to see

00:23:02,629 --> 00:23:07,460
line by line the time of execution will

00:23:05,779 --> 00:23:11,059
it be possible also to annotate directly

00:23:07,460 --> 00:23:15,769
size and source and not the C++ or C

00:23:11,059 --> 00:23:18,049
source that this item generated what

00:23:15,769 --> 00:23:20,359
they mean by annotate house because

00:23:18,049 --> 00:23:22,220
there is instrumentation but tell me

00:23:20,359 --> 00:23:25,159
more about annotation in your case I

00:23:22,220 --> 00:23:27,099
mean just as we saw in in the diagram

00:23:25,159 --> 00:23:29,599
that you can see actually the source

00:23:27,099 --> 00:23:32,269
lines and the time that they took to

00:23:29,599 --> 00:23:36,019
execute the cumulative time this kind of

00:23:32,269 --> 00:23:39,289
profiling part if instead of showing the

00:23:36,019 --> 00:23:41,840
C source that was generated from the

00:23:39,289 --> 00:23:44,450
site and if we can see that directly the

00:23:41,840 --> 00:23:46,419
lines of sytem yeah actually you will

00:23:44,450 --> 00:23:49,419
start directly from the line of sight on

00:23:46,419 --> 00:23:49,419
okay

00:23:59,090 --> 00:24:05,730
yeah how does it work okay because the

00:24:04,020 --> 00:24:07,410
question was said without microphone so

00:24:05,730 --> 00:24:09,420
the application is already running it

00:24:07,410 --> 00:24:10,380
has a process ID how do you actually

00:24:09,420 --> 00:24:12,060
attach to it

00:24:10,380 --> 00:24:15,150
well there are mechanism so you already

00:24:12,060 --> 00:24:17,370
know the PID right if it's running also

00:24:15,150 --> 00:24:19,110
if it has a PID you can also know the

00:24:17,370 --> 00:24:22,290
name of the application and then in the

00:24:19,110 --> 00:24:25,260
GUI you can either provide the name of

00:24:22,290 --> 00:24:28,800
the process ID and which one will attach

00:24:25,260 --> 00:24:31,890
to it and one other question when you

00:24:28,800 --> 00:24:33,450
have C extension modules you also need

00:24:31,890 --> 00:24:35,640
that module to be compiled with the

00:24:33,450 --> 00:24:37,920
debug flag so that you can sample from

00:24:35,640 --> 00:24:39,960
it yeah and if you don't have access to

00:24:37,920 --> 00:24:42,750
that like if it's just the binary that's

00:24:39,960 --> 00:24:43,560
came in your distribution yeah that's a

00:24:42,750 --> 00:24:46,050
very good question

00:24:43,560 --> 00:24:49,410
um yeah in this case you will basically

00:24:46,050 --> 00:24:52,140
see thanks a dead beef which is

00:24:49,410 --> 00:24:55,250
basically a hex code for functions that

00:24:52,140 --> 00:25:00,780
you don't know the name our peyten

00:24:55,250 --> 00:25:03,120
binary provided by the distribution is

00:25:00,780 --> 00:25:06,180
built with ICC with a debug flag

00:25:03,120 --> 00:25:09,420
so essentially you can see deep down in

00:25:06,180 --> 00:25:09,900
Python itself the method names being

00:25:09,420 --> 00:25:11,700
used

00:25:09,900 --> 00:25:13,140
that's for Python for an external

00:25:11,700 --> 00:25:15,540
library obviously you would like to have

00:25:13,140 --> 00:25:18,840
- G to get the debug information for

00:25:15,540 --> 00:25:21,330
your code and your Python distribution

00:25:18,840 --> 00:25:23,070
comes with anaconda distribution or you

00:25:21,330 --> 00:25:25,680
have other Champy's this is just one of

00:25:23,070 --> 00:25:29,460
the way you can actually just do you

00:25:25,680 --> 00:25:33,960
just add the repository and then you can

00:25:29,460 --> 00:25:38,460
also do yum install but anaconda is a

00:25:33,960 --> 00:25:43,610
preferred way there is anaconda condom

00:25:38,460 --> 00:25:43,610
and some others thank you

00:25:47,160 --> 00:25:53,530
hello so you mentioned the vegan is a

00:25:50,920 --> 00:25:55,210
statistical tie profiler and we've seen

00:25:53,530 --> 00:25:57,930
some results of some of the code that

00:25:55,210 --> 00:26:00,190
you're running so that math matrix

00:25:57,930 --> 00:26:01,930
multiplication yeah I was wondering if

00:26:00,190 --> 00:26:04,360
the results that we've seen are actually

00:26:01,930 --> 00:26:06,700
the results of running the code maybe

00:26:04,360 --> 00:26:08,920
like a number of times 10,000 times and

00:26:06,700 --> 00:26:11,830
taking some critics things over those or

00:26:08,920 --> 00:26:13,870
was that just the one-off run and we

00:26:11,830 --> 00:26:15,850
just displayed the results of them all

00:26:13,870 --> 00:26:19,960
so that's an excellent question in this

00:26:15,850 --> 00:26:22,060
case it was run once so this is what you

00:26:19,960 --> 00:26:24,640
get right away but in order for yourself

00:26:22,060 --> 00:26:27,160
to confirm that the data that I got

00:26:24,640 --> 00:26:29,080
actually makes sense and it is true you

00:26:27,160 --> 00:26:30,880
run it yourself many times you can have

00:26:29,080 --> 00:26:35,170
actually another Python script that runs

00:26:30,880 --> 00:26:37,050
your script many times and also our tool

00:26:35,170 --> 00:26:39,190
between comes with a command-line

00:26:37,050 --> 00:26:42,220
interface as well so you can have this

00:26:39,190 --> 00:26:44,290
one line that does the profiling for use

00:26:42,220 --> 00:26:46,060
it's the results and everything so you

00:26:44,290 --> 00:26:48,190
can have your script and automate the

00:26:46,060 --> 00:26:51,760
running of your program many times and

00:26:48,190 --> 00:26:54,460
have vision wrapping your application in

00:26:51,760 --> 00:26:56,590
its command-line interface and this is

00:26:54,460 --> 00:27:00,010
how you can have your own build system

00:26:56,590 --> 00:27:02,860
or regression testing system and get the

00:27:00,010 --> 00:27:05,350
data and if that's the case is this how

00:27:02,860 --> 00:27:09,100
do you find the time is it quite slow to

00:27:05,350 --> 00:27:12,910
to run this kind of analysis like run

00:27:09,100 --> 00:27:14,890
multiple times or this one I didn't get

00:27:12,910 --> 00:27:16,960
there was any KO I was just wondering

00:27:14,890 --> 00:27:19,720
what time how much time do you have to

00:27:16,960 --> 00:27:21,790
spend to have a to say run your code

00:27:19,720 --> 00:27:26,160
10,000 times and draw statistics from it

00:27:21,790 --> 00:27:30,010
you have any type of matrix on that okay

00:27:26,160 --> 00:27:33,840
this depends on the resolution of your

00:27:30,010 --> 00:27:36,130
analysis so in my case I did the

00:27:33,840 --> 00:27:38,470
analysis with the resolution of 10

00:27:36,130 --> 00:27:41,560
milliseconds which is quite big actually

00:27:38,470 --> 00:27:43,930
so if you want more data more resolution

00:27:41,560 --> 00:27:46,210
you can lower this time and how many

00:27:43,930 --> 00:27:49,090
times objects it the lower the time

00:27:46,210 --> 00:27:51,460
duration to get the samples the larger

00:27:49,090 --> 00:27:53,620
the data the larger potentially the

00:27:51,460 --> 00:27:56,330
overhead and less occur it would be your

00:27:53,620 --> 00:28:02,280
result so it's playing around

00:27:56,330 --> 00:28:10,590
um in general anything longer than two

00:28:02,280 --> 00:28:13,440
three seconds is good enough hi to come

00:28:10,590 --> 00:28:15,180
to questions can you attach the profiler

00:28:13,440 --> 00:28:17,070
to a running process and does that

00:28:15,180 --> 00:28:19,410
process has to be built in the special

00:28:17,070 --> 00:28:20,910
way for that yeah I could jump rope

00:28:19,410 --> 00:28:23,160
funny in production kind of thing I

00:28:20,910 --> 00:28:25,950
think that question was asked already

00:28:23,160 --> 00:28:27,990
sorry I can so Dan says yes you again at

00:28:25,950 --> 00:28:29,580
the running process the second question

00:28:27,990 --> 00:28:32,160
was you had an early slide where you

00:28:29,580 --> 00:28:34,140
showed the presentation of the time

00:28:32,160 --> 00:28:36,720
taken in on a particular line of code

00:28:34,140 --> 00:28:39,090
that line of code had two calls it was

00:28:36,720 --> 00:28:41,310
like logging info brackets template

00:28:39,090 --> 00:28:43,560
format so there's two function calls in

00:28:41,310 --> 00:28:45,150
there can you decompose that in the in

00:28:43,560 --> 00:28:47,100
the browser to those two function calls

00:28:45,150 --> 00:28:48,810
and the process time that each one took

00:28:47,100 --> 00:28:52,140
because you're just showing the sum of

00:28:48,810 --> 00:28:54,660
the total for that line and when you

00:28:52,140 --> 00:28:57,570
have these multi the multi process so

00:28:54,660 --> 00:29:00,540
your question is you have created two

00:28:57,570 --> 00:29:02,820
multiprocessors no no making two

00:29:00,540 --> 00:29:05,070
function calls in one line two method

00:29:02,820 --> 00:29:08,400
calls in one line something like logging

00:29:05,070 --> 00:29:11,160
door info bracket template dot format so

00:29:08,400 --> 00:29:13,830
you're calling info and format yeah and

00:29:11,160 --> 00:29:16,470
can you decompose it in the browser well

00:29:13,830 --> 00:29:18,630
in this case it will aggregate the time

00:29:16,470 --> 00:29:20,850
and show you on that one source line the

00:29:18,630 --> 00:29:24,050
whole time folded but I think it's a bad

00:29:20,850 --> 00:29:28,610
practice to do this for code readability

00:29:24,050 --> 00:29:28,610
my opinions I don't know how you do it

00:29:29,600 --> 00:29:37,410
but wait hold on hold on I will add one

00:29:34,050 --> 00:29:39,240
more thing by the way in this case it

00:29:37,410 --> 00:29:41,610
you will see the source line because

00:29:39,240 --> 00:29:43,740
we're actually are associating time to a

00:29:41,610 --> 00:29:45,900
source line your source code but in the

00:29:43,740 --> 00:29:49,200
bottom of view you will see different

00:29:45,900 --> 00:29:50,970
functions to functions well but the

00:29:49,200 --> 00:29:52,860
thing is when you click on both

00:29:50,970 --> 00:29:54,960
functions we go to the same source line

00:29:52,860 --> 00:30:01,470
but you will know that time for each

00:29:54,960 --> 00:30:06,120
function I like here I would like to ask

00:30:01,470 --> 00:30:11,400
what interpreter do you use integer

00:30:06,120 --> 00:30:16,470
and if you have applied modifications to

00:30:11,400 --> 00:30:19,560
the interpreter to make it faster oh the

00:30:16,470 --> 00:30:21,780
acoustics are not so great um what what

00:30:19,560 --> 00:30:26,040
I got what wait what I got is that how

00:30:21,780 --> 00:30:28,380
is alignment done memory element a what

00:30:26,040 --> 00:30:30,240
interpreter do you use and have you

00:30:28,380 --> 00:30:31,490
maybe changed the interpreter to

00:30:30,240 --> 00:30:34,050
optimize it

00:30:31,490 --> 00:30:43,490
can anybody rephrase this for me please

00:30:34,050 --> 00:30:46,560
oh okay thank you Adam

00:30:43,490 --> 00:30:48,840
yeah well our interpreter has been made

00:30:46,560 --> 00:30:52,500
from scratch and compiled with ICC there

00:30:48,840 --> 00:30:54,600
were some changes um I don't know in

00:30:52,500 --> 00:30:57,240
detail what has changed but there were

00:30:54,600 --> 00:31:00,060
minor changes in the interpreter however

00:30:57,240 --> 00:31:02,250
all the libraries making use of heavy

00:31:00,060 --> 00:31:06,180
mathematics these have been redesigned

00:31:02,250 --> 00:31:07,560
completely making use of mkl so this is

00:31:06,180 --> 00:31:09,870
the benefit we're bringing with our

00:31:07,560 --> 00:31:12,060
Intel distribution of Python so that you

00:31:09,870 --> 00:31:14,040
guys when you do HPC based applications

00:31:12,060 --> 00:31:16,890
made in Python or machine learning deep

00:31:14,040 --> 00:31:21,870
learning or even using SDKs or

00:31:16,890 --> 00:31:23,520
frameworks like tensorflow cafe or even

00:31:21,870 --> 00:31:26,400
the autonomous driving SDK or the

00:31:23,520 --> 00:31:28,980
computer vision SDK from Intel that

00:31:26,400 --> 00:31:31,530
leverages the Python distribution you

00:31:28,980 --> 00:31:33,840
get the performance out of the box so

00:31:31,530 --> 00:31:38,760
you don't have to be like a math genius

00:31:33,840 --> 00:31:41,610
to code properly or a super software

00:31:38,760 --> 00:31:43,470
engineer with great skills and code

00:31:41,610 --> 00:31:46,460
optimization to create high performance

00:31:43,470 --> 00:31:53,460
it's done out of the box for you and

00:31:46,460 --> 00:31:55,710
welcome it may be already lunchtime just

00:31:53,460 --> 00:31:57,120
one thing if you have really interesting

00:31:55,710 --> 00:31:59,850
questions that you really want to get

00:31:57,120 --> 00:32:02,880
answered our workshop on it just on this

00:31:59,850 --> 00:32:05,250
topic could be very useful for you it's

00:32:02,880 --> 00:32:08,310
on Thursday check it out I have a

00:32:05,250 --> 00:32:10,530
question for class they're users because

00:32:08,310 --> 00:32:14,010
I see that they're on my machine we can

00:32:10,530 --> 00:32:17,040
connect to the play process but actually

00:32:14,010 --> 00:32:19,450
I have a cluster I can measure the

00:32:17,040 --> 00:32:22,299
performance of all the work

00:32:19,450 --> 00:32:24,700
machine or is it possible great question

00:32:22,299 --> 00:32:28,389
yes it is possible so you're probably

00:32:24,700 --> 00:32:32,500
using MPI right yeah yeah nothing I'm

00:32:28,389 --> 00:32:35,080
not using MPI but I'm using gesture okay

00:32:32,500 --> 00:32:37,000
let me take MPI as an example you have a

00:32:35,080 --> 00:32:39,309
cluster several nodes your Python code

00:32:37,000 --> 00:32:41,649
is being running on all you will have

00:32:39,309 --> 00:32:46,090
vtune amplifier the driver the sampling

00:32:41,649 --> 00:32:50,730
driver on all those guys and with MPI g2

00:32:46,090 --> 00:32:53,110
for instance you just pass MPI run g2

00:32:50,730 --> 00:32:55,240
amplifier X ECL which is the

00:32:53,110 --> 00:32:57,340
command-line interface tool and then

00:32:55,240 --> 00:32:57,580
your Python script and it will do the

00:32:57,340 --> 00:32:59,679
job

00:32:57,580 --> 00:33:02,440
out of for you and get you the results

00:32:59,679 --> 00:33:05,730
it's magic it's really nice any

00:33:02,440 --> 00:33:05,730
interesting things yeah

00:33:07,919 --> 00:33:15,819
okay thank you

00:33:09,710 --> 00:33:15,819

YouTube URL: https://www.youtube.com/watch?v=G31yUYwvGgc


