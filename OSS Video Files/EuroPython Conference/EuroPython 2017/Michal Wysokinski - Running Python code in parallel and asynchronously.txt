Title: Michal Wysokinski - Running Python code in parallel and asynchronously
Publication date: 2017-09-17
Playlist: EuroPython 2017
Description: 
	"Running Python code in parallel and asynchronously
[EuroPython 2017 - Talk - 2017-07-11 - Anfiteatro 2]
[Rimini, Italy]

My outline will be:
1) What does it mean to run code in parallel in Python? How does it differ from concurrency? Can they be applied at the some time?
2) GIL and why it complicates parallelism in Python (CPython), but only to some extent.
3)  Difference between a thread and a process from the OS point of view.
4) When parallelism in Python is useful and when to avoid it.
5) Description of how to achieve parallel execution in CPython and how to do it properly.
6) Possible traps when using parallel programming in Python.
7) What happens if the code runs both in parallel and asynchronously?
8) Is it really beneficial?
9) How such execution can be achieved?

As the outline shows I will focus on the parallel part as it is an important topic in our current time of multicore processors and multiprocessor systems.

The topic has been discussed a lot of times but mainly from the scientific point of view, where it's been used for speeding up calulcations time.  I will not go into these use cases (e.g. using MPI) but rather discuss it from web development point of view (e.g. multi worker applications).

License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2017.europython.eu/en/speaker-release-agreement/
Captions: 
	00:00:04,630 --> 00:00:10,709
[Applause]

00:00:07,019 --> 00:00:15,460
so hello everyone can you hear me well

00:00:10,709 --> 00:00:17,250
okay ah welcome to my talk I'm Macau and

00:00:15,460 --> 00:00:20,919
today I'll be I will be speaking about

00:00:17,250 --> 00:00:24,760
running your Python code in parallel for

00:00:20,919 --> 00:00:26,680
most part and asynchronous to be honest

00:00:24,760 --> 00:00:29,740
I've never spoken to such a big group of

00:00:26,680 --> 00:00:34,360
people so excuse me for being a little

00:00:29,740 --> 00:00:37,960
bit overwhelmed and let me advertise

00:00:34,360 --> 00:00:42,100
other talks a little bit so you can see

00:00:37,960 --> 00:00:46,360
that asynchronous and parallel topics

00:00:42,100 --> 00:00:49,300
are hot right now so even during this

00:00:46,360 --> 00:00:53,739
conference we have several talks about

00:00:49,300 --> 00:00:57,730
them mmm I want you to understand what

00:00:53,739 --> 00:00:59,440
my talk is about to do not mix it with

00:00:57,730 --> 00:01:03,820
some other talks I also encourage you to

00:00:59,440 --> 00:01:05,710
see hmm so there are some exchange

00:01:03,820 --> 00:01:08,020
constructs parallel talks there's also a

00:01:05,710 --> 00:01:11,140
talk about Python at CERN which I think

00:01:08,020 --> 00:01:14,289
would be interesting or I think it's a

00:01:11,140 --> 00:01:17,259
post-recession and this talks tries to

00:01:14,289 --> 00:01:20,649
be an overview of the topic so it's not

00:01:17,259 --> 00:01:24,819
an introduction that's why I labeled it

00:01:20,649 --> 00:01:29,470
as advanced and also you might feel that

00:01:24,819 --> 00:01:32,440
I skipped some parts and but I wanted to

00:01:29,470 --> 00:01:34,929
put tool to put it together as an

00:01:32,440 --> 00:01:38,560
overview so you can later research

00:01:34,929 --> 00:01:41,670
what's interesting for you and to not

00:01:38,560 --> 00:01:46,929
bug you with a lot of detail details

00:01:41,670 --> 00:01:50,259
ok so few words about me so I worked at

00:01:46,929 --> 00:01:54,580
the LHCb experiment at CERN looking for

00:01:50,259 --> 00:01:57,429
antimatter for some time and later I

00:01:54,580 --> 00:02:00,250
decided to pursue a PhD in computer

00:01:57,429 --> 00:02:03,479
science but then I've heard that if I

00:02:00,250 --> 00:02:07,360
drop out I will probably start a

00:02:03,479 --> 00:02:11,280
multi-billion dollar business but for

00:02:07,360 --> 00:02:11,280
some reason that hasn't happened yet

00:02:12,180 --> 00:02:19,690
yes and currently I work at Akamai and I

00:02:16,630 --> 00:02:21,610
know as a web developer we're fat

00:02:19,690 --> 00:02:27,370
obviously stands for frameworks and

00:02:21,610 --> 00:02:30,340
tools so what my job is at Akamai is to

00:02:27,370 --> 00:02:34,660
make sure that we use the best tools we

00:02:30,340 --> 00:02:37,930
can and how do you define the best tools

00:02:34,660 --> 00:02:40,270
so sometimes you hear that Facebook is

00:02:37,930 --> 00:02:44,710
using some tools or that Google is using

00:02:40,270 --> 00:02:49,480
some other but doing it that do we have

00:02:44,710 --> 00:02:51,580
the exact same scenario as them so my

00:02:49,480 --> 00:02:56,070
job is to create tools and to select

00:02:51,580 --> 00:02:56,070
tools which are the best fit for us and

00:02:56,100 --> 00:03:03,010
Akamai itself is a content delivery

00:03:00,330 --> 00:03:06,160
delivery network and cloud service

00:03:03,010 --> 00:03:09,610
provider so you're not very known in

00:03:06,160 --> 00:03:14,459
Europe for some reason but we have one

00:03:09,610 --> 00:03:19,000
of the largest or the largest network of

00:03:14,459 --> 00:03:22,530
computers talking with each other and we

00:03:19,000 --> 00:03:28,510
are also responsible from between 10 to

00:03:22,530 --> 00:03:31,540
25% of all web traffic we also have some

00:03:28,510 --> 00:03:34,330
security project it isn't security

00:03:31,540 --> 00:03:38,350
parent products lounge recently and we

00:03:34,330 --> 00:03:42,220
also have 16 offices in amia both sales

00:03:38,350 --> 00:03:44,830
and engineering okay there's a lot of

00:03:42,220 --> 00:03:47,140
mess when it comes to basic concepts in

00:03:44,830 --> 00:03:51,040
asynchronous and parallel programming so

00:03:47,140 --> 00:03:54,489
let me clarify some things first so when

00:03:51,040 --> 00:03:57,430
you have one pipeline and one worker one

00:03:54,489 --> 00:04:00,820
working on it you have a serial or

00:03:57,430 --> 00:04:03,820
sequential execution when you have also

00:04:00,820 --> 00:04:07,840
one pipeline but multiple workers and

00:04:03,820 --> 00:04:10,630
they do they work in the same time but

00:04:07,840 --> 00:04:13,420
not in parallel I would call it

00:04:10,630 --> 00:04:16,330
concurrent you may not agree with me and

00:04:13,420 --> 00:04:18,970
some people do not but let's assume at

00:04:16,330 --> 00:04:23,080
least for for this talk that it's great

00:04:18,970 --> 00:04:23,950
and also we have parallel execution so

00:04:23,080 --> 00:04:26,080
we have

00:04:23,950 --> 00:04:29,080
multiple pipelines we have multiple

00:04:26,080 --> 00:04:34,000
workers and they actually do their

00:04:29,080 --> 00:04:38,860
things in parallel and so the

00:04:34,000 --> 00:04:40,960
concurrency I usually when I think of

00:04:38,860 --> 00:04:45,070
concurrency I usually think about

00:04:40,960 --> 00:04:51,100
preemption how many of you know what

00:04:45,070 --> 00:04:52,690
preemption is okay and half of you so

00:04:51,100 --> 00:04:58,630
let me just say that preemption occurs

00:04:52,690 --> 00:05:00,850
if a thread has a CPU time and operating

00:04:58,630 --> 00:05:03,490
systems scheduler decides that there's

00:05:00,850 --> 00:05:07,960
some other thread that needs that time

00:05:03,490 --> 00:05:10,390
more so there the preemption occurs one

00:05:07,960 --> 00:05:13,540
thread is being stopped the other thread

00:05:10,390 --> 00:05:16,600
is being put into his place and they and

00:05:13,540 --> 00:05:18,910
then they switch roles until their job

00:05:16,600 --> 00:05:23,140
is complete so this is why you can

00:05:18,910 --> 00:05:26,110
sometimes see that things are concurrent

00:05:23,140 --> 00:05:28,510
because you achieve results in a certain

00:05:26,110 --> 00:05:33,700
amount of time but they are natural

00:05:28,510 --> 00:05:38,550
truly parallel okay so how would you

00:05:33,700 --> 00:05:41,380
call this I would call this a headache

00:05:38,550 --> 00:05:46,210
or you might call it parallel and

00:05:41,380 --> 00:05:48,450
asynchronous so another thing which I

00:05:46,210 --> 00:05:51,790
need to clarify is a difference between

00:05:48,450 --> 00:05:54,940
threads and processes because they are

00:05:51,790 --> 00:06:02,200
often mistaken or processes are wrongly

00:05:54,940 --> 00:06:05,130
called bigger threads so threads are the

00:06:02,200 --> 00:06:08,710
place where your code is executed and

00:06:05,130 --> 00:06:11,290
each processes each process has a thread

00:06:08,710 --> 00:06:14,770
and this thread can be scheduled for

00:06:11,290 --> 00:06:16,720
execution it can get CPU time and all

00:06:14,770 --> 00:06:19,240
threats share a virtual address space

00:06:16,720 --> 00:06:22,120
and system resources of the process and

00:06:19,240 --> 00:06:26,920
they do not share stacks local variables

00:06:22,120 --> 00:06:31,330
and also but they do share process heap

00:06:26,920 --> 00:06:34,900
and process is an execution environment

00:06:31,330 --> 00:06:37,210
for threads so it has its address space

00:06:34,900 --> 00:06:42,330
it has executable code

00:06:37,210 --> 00:06:47,160
it handle system objects so it it brings

00:06:42,330 --> 00:06:47,160
all what's necessary for threat to run

00:06:47,190 --> 00:06:54,849
so I wanted to clarify that because

00:06:50,970 --> 00:06:59,110
sometimes people don't know why Gil in

00:06:54,849 --> 00:07:02,080
Python complicates things so how it

00:06:59,110 --> 00:07:04,120
applies to Python so in Python we we

00:07:02,080 --> 00:07:07,180
have multi threading and multi

00:07:04,120 --> 00:07:09,250
processing and when we talk about multi

00:07:07,180 --> 00:07:11,680
threading we have one process so this

00:07:09,250 --> 00:07:15,570
one environment we have many threads

00:07:11,680 --> 00:07:18,370
only one interpreter and due to Gil

00:07:15,570 --> 00:07:21,190
there's a rule which says that in a

00:07:18,370 --> 00:07:24,910
Python process only one Python bytecode

00:07:21,190 --> 00:07:27,509
instruction is executing at once so if

00:07:24,910 --> 00:07:30,729
you have many threads you cannot execute

00:07:27,509 --> 00:07:37,509
many bytecode instructions from

00:07:30,729 --> 00:07:39,099
different threads at once but with IO

00:07:37,509 --> 00:07:41,919
it's really the different story because

00:07:39,099 --> 00:07:44,650
if you have IO and then it does not

00:07:41,919 --> 00:07:47,710
execute anybody code instructions so if

00:07:44,650 --> 00:07:49,780
you have threads and you do some IO in

00:07:47,710 --> 00:07:51,130
them you can actually see speed-up but

00:07:49,780 --> 00:07:55,360
that's because it's not going through

00:07:51,130 --> 00:07:57,039
Python interpreter and when we talk

00:07:55,360 --> 00:08:00,130
about multi processing we have many

00:07:57,039 --> 00:08:02,500
processes we have many threads at least

00:08:00,130 --> 00:08:06,039
one thread per process we have many

00:08:02,500 --> 00:08:08,500
interpreters and all threads have their

00:08:06,039 --> 00:08:13,479
own interpreter and that's why they can

00:08:08,500 --> 00:08:18,099
execute in parallel so do we have Alex

00:08:13,479 --> 00:08:21,729
Michael here ok not it's always

00:08:18,099 --> 00:08:24,370
dangerous tool of deciding someone

00:08:21,729 --> 00:08:26,560
sitting in front of you so during a chat

00:08:24,370 --> 00:08:28,210
with Raymond Raymond Hettinger he

00:08:26,560 --> 00:08:31,900
proposed the following classification

00:08:28,210 --> 00:08:33,969
which i think is simple but nice so if

00:08:31,900 --> 00:08:36,700
you have one core you usually want to

00:08:33,969 --> 00:08:40,990
run a single process with a single

00:08:36,700 --> 00:08:42,880
thread so for 2 to 16 cores because

00:08:40,990 --> 00:08:46,579
that's how many cores you can get in

00:08:42,880 --> 00:08:49,079
consumer PC pcs nowadays

00:08:46,579 --> 00:08:52,230
you can have multiple threads and

00:08:49,079 --> 00:08:54,360
multiple processes so why you should not

00:08:52,230 --> 00:08:56,670
use multiple threads on a read on a

00:08:54,360 --> 00:09:00,209
single core that's because even though

00:08:56,670 --> 00:09:03,720
that IO which might give you a speed-up

00:09:00,209 --> 00:09:07,589
when it's done in a thread it still

00:09:03,720 --> 00:09:11,129
needs some CPU time not a lot but it

00:09:07,589 --> 00:09:15,209
does so with only one core you want you

00:09:11,129 --> 00:09:18,480
should not achieve any speed-up and also

00:09:15,209 --> 00:09:21,899
when you have 16 plus course you usually

00:09:18,480 --> 00:09:26,129
have multiple CPUs so you enter the area

00:09:21,899 --> 00:09:28,709
of distributed computing and Aleks

00:09:26,129 --> 00:09:31,699
proposes that as the time goes by the

00:09:28,709 --> 00:09:36,329
second category becomes less relevant as

00:09:31,699 --> 00:09:41,879
we are in the era of big data and even

00:09:36,329 --> 00:09:44,459
one CPU with 16 on or 32 cores is not

00:09:41,879 --> 00:09:49,889
enough I would argue that for some cases

00:09:44,459 --> 00:09:52,370
like back int work services it is but

00:09:49,889 --> 00:09:57,209
you can you can hear more about that in

00:09:52,370 --> 00:10:03,180
Raymond Hettinger stock ok so you should

00:09:57,209 --> 00:10:07,230
have some knowledge about that now so

00:10:03,180 --> 00:10:10,829
when I as a back-end developer think of

00:10:07,230 --> 00:10:16,730
speed-up or performance boost which one

00:10:10,829 --> 00:10:16,730
I want to use parallel or asynchronous

00:10:19,889 --> 00:10:27,749
parallel because I want to execute many

00:10:24,929 --> 00:10:30,029
things at the same time and I want and

00:10:27,749 --> 00:10:35,959
if I want to gain responsiveness and

00:10:30,029 --> 00:10:35,959
lower latency I'm choosing asynchronous

00:10:36,109 --> 00:10:42,059
okay so when running things in parallel

00:10:39,449 --> 00:10:45,359
is useful well when you have big

00:10:42,059 --> 00:10:47,689
datasets or complex computations when

00:10:45,359 --> 00:10:51,209
you have problems with parallel nature

00:10:47,689 --> 00:10:54,869
so-called coarse-grained or when you

00:10:51,209 --> 00:10:56,970
have multi worker applications IO bound

00:10:54,869 --> 00:11:00,689
problems are not a good fit for being

00:10:56,970 --> 00:11:04,649
paralyzed as they require a lot of IO

00:11:00,689 --> 00:11:07,259
which is mostly serial sequential and

00:11:04,649 --> 00:11:11,059
also problems need to be complex enough

00:11:07,259 --> 00:11:13,970
so that parallel overhead caused by

00:11:11,059 --> 00:11:17,819
process maintenance communication

00:11:13,970 --> 00:11:21,980
scheduling synchronization is negligible

00:11:17,819 --> 00:11:29,129
to what's going on inside the process

00:11:21,980 --> 00:11:32,669
okay so who knows Amdahl's law some of

00:11:29,129 --> 00:11:35,100
you okay so Amdahl's law says how much

00:11:32,669 --> 00:11:39,540
speed up you can get when running in

00:11:35,100 --> 00:11:42,720
parallel and when you you need to know

00:11:39,540 --> 00:11:46,139
and how big part of your program needs

00:11:42,720 --> 00:11:49,499
to be sequential and if you know that

00:11:46,139 --> 00:11:51,989
you can approximate how much speed up

00:11:49,499 --> 00:11:55,889
you can get with a certain number of

00:11:51,989 --> 00:11:59,040
CPUs so let's say that we have a task

00:11:55,889 --> 00:12:01,709
that runs for 10 minutes but five

00:11:59,040 --> 00:12:05,489
minutes of the time is sequential work

00:12:01,709 --> 00:12:08,279
like loading data so you can see that if

00:12:05,489 --> 00:12:10,799
you have even an infinite number of CPUs

00:12:08,279 --> 00:12:14,720
we can only achieve a speed-up of two

00:12:10,799 --> 00:12:18,929
because that second part if it's really

00:12:14,720 --> 00:12:21,419
run in parallel then and that time goes

00:12:18,929 --> 00:12:25,199
to almost zero but you still have that

00:12:21,419 --> 00:12:27,480
5-minute time so it's so you really need

00:12:25,199 --> 00:12:28,870
to know your problem when you start

00:12:27,480 --> 00:12:32,650
working in

00:12:28,870 --> 00:12:35,110
parallel programming and just to give

00:12:32,650 --> 00:12:38,589
you an example of that some of you might

00:12:35,110 --> 00:12:40,810
say that it's a really trivial problem

00:12:38,589 --> 00:12:43,870
but in order to present you how that

00:12:40,810 --> 00:12:48,670
works I had to choose something like

00:12:43,870 --> 00:12:51,160
that and so here we have a small data

00:12:48,670 --> 00:12:54,370
set and a really simple operation we

00:12:51,160 --> 00:12:57,839
have an input vector of 1 million

00:12:54,370 --> 00:13:03,160
elements and we want to calculate

00:12:57,839 --> 00:13:07,480
outputs which are inputs plus 1 so we

00:13:03,160 --> 00:13:09,730
can run it sequentially and also we can

00:13:07,480 --> 00:13:13,810
run it in parallel in in different

00:13:09,730 --> 00:13:15,730
processes so how do you think how much

00:13:13,810 --> 00:13:23,620
the speed-up will be we are running on

00:13:15,730 --> 00:13:26,529
four cars two for none it will actually

00:13:23,620 --> 00:13:31,420
be slower because the problem is really

00:13:26,529 --> 00:13:37,540
simple and data set is small and it's

00:13:31,420 --> 00:13:43,200
not enough to to have any gain and in

00:13:37,540 --> 00:13:47,500
fact you actually lose something and and

00:13:43,200 --> 00:13:50,380
even for eight cores or more it gets

00:13:47,500 --> 00:13:55,839
even more complicated and you get even

00:13:50,380 --> 00:13:57,880
worse worse results okay so a common

00:13:55,839 --> 00:14:01,630
pattern in parallel programming is to

00:13:57,880 --> 00:14:05,770
put a problem difficult more difficult

00:14:01,630 --> 00:14:08,950
by running it in a for loop so here we

00:14:05,770 --> 00:14:13,990
have a problem that's 200 times more

00:14:08,950 --> 00:14:23,650
complicated and how much the speed-up

00:14:13,990 --> 00:14:26,410
will be now - for almost 4 yeah so the

00:14:23,650 --> 00:14:28,990
the speed-up comes from using processes

00:14:26,410 --> 00:14:32,200
which like I make like I mentioned

00:14:28,990 --> 00:14:36,900
earlier we need to have processes in

00:14:32,200 --> 00:14:39,339
Python to execute truly in parallel and

00:14:36,900 --> 00:14:41,290
arithmetic operations go through

00:14:39,339 --> 00:14:42,130
interpreter so so we need separate

00:14:41,290 --> 00:14:49,180
interpreters

00:14:42,130 --> 00:14:52,080
so here we have almost almost four okay

00:14:49,180 --> 00:14:55,360
so some some problems like that have

00:14:52,080 --> 00:15:02,490
parallel nature so here I was easily

00:14:55,360 --> 00:15:06,100
able to to divide my dataset into four

00:15:02,490 --> 00:15:10,950
subsets and I the most part of that

00:15:06,100 --> 00:15:13,900
program is running in the Impala so this

00:15:10,950 --> 00:15:17,290
this type of thing has a parallel nature

00:15:13,900 --> 00:15:19,060
so usually usually when we talk about

00:15:17,290 --> 00:15:22,540
parallel innate nature we talk about

00:15:19,060 --> 00:15:24,910
coarse-grained problems so if we have a

00:15:22,540 --> 00:15:27,160
loop of loops if you have multiple

00:15:24,910 --> 00:15:30,840
images to process if you have multiple

00:15:27,160 --> 00:15:34,480
datasets or a really big data set or

00:15:30,840 --> 00:15:37,080
maybe the data set is not big but the

00:15:34,480 --> 00:15:39,820
operations we want to run on it are long

00:15:37,080 --> 00:15:43,960
so those problems are called coarse

00:15:39,820 --> 00:15:47,470
grained and the and then there you can

00:15:43,960 --> 00:15:49,720
easily apply parallel programming but

00:15:47,470 --> 00:15:52,170
for fine grained problems there is a

00:15:49,720 --> 00:15:55,980
different story so when you have

00:15:52,170 --> 00:15:58,270
iteration of a single loop an image or a

00:15:55,980 --> 00:16:02,650
single small small dataset

00:15:58,270 --> 00:16:06,820
you should not paralyze that at least

00:16:02,650 --> 00:16:08,590
not with CPU because nowadays we can

00:16:06,820 --> 00:16:12,180
actually parallelize fine-grained

00:16:08,590 --> 00:16:15,700
problems with massively parallel

00:16:12,180 --> 00:16:18,220
architecture devices like GPUs because

00:16:15,700 --> 00:16:23,760
they have really a lot of processing

00:16:18,220 --> 00:16:23,760
units and the threads are really light

00:16:26,630 --> 00:16:33,519
okay so in parallel programming we have

00:16:29,199 --> 00:16:38,839
different memory architectures and the

00:16:33,519 --> 00:16:42,440
most known to our shared memory where

00:16:38,839 --> 00:16:45,139
each process has its own memory they

00:16:42,440 --> 00:16:48,079
were each process connects to a shared

00:16:45,139 --> 00:16:52,519
memory and it works on the same data set

00:16:48,079 --> 00:16:57,110
and we also have distributed memory aka

00:16:52,519 --> 00:17:00,589
message-passing so we need to pass data

00:16:57,110 --> 00:17:03,760
to processes and later get that back

00:17:00,589 --> 00:17:08,319
that's why it's called message passing

00:17:03,760 --> 00:17:08,319
so how to apply them in Python

00:17:13,319 --> 00:17:20,650
so for a short memory we have shared

00:17:16,780 --> 00:17:23,170
c-type objects and those are objects

00:17:20,650 --> 00:17:27,189
created inside memory and can be

00:17:23,170 --> 00:17:30,460
inherited by child processes so if we

00:17:27,189 --> 00:17:33,580
import multiple multi-processing value

00:17:30,460 --> 00:17:36,610
you can assign what type that value is

00:17:33,580 --> 00:17:40,710
you can assign it value in the beginning

00:17:36,610 --> 00:17:44,680
and we have also some other some other

00:17:40,710 --> 00:17:54,100
types label and primitives so let's see

00:17:44,680 --> 00:17:57,460
how they behave so I have two I have two

00:17:54,100 --> 00:18:00,880
programs so the difference is that one

00:17:57,460 --> 00:18:03,340
uses locking and the the other one is

00:18:00,880 --> 00:18:06,640
not the one on the Left does not use

00:18:03,340 --> 00:18:09,520
locking so we have shared memory so all

00:18:06,640 --> 00:18:14,500
processes have access to the same memory

00:18:09,520 --> 00:18:18,630
all they all can can read from it and

00:18:14,500 --> 00:18:23,350
write to it in the same time so what you

00:18:18,630 --> 00:18:26,230
if you do that and they will act in

00:18:23,350 --> 00:18:30,940
there actually will be some something

00:18:26,230 --> 00:18:33,550
called race conditions so sometimes two

00:18:30,940 --> 00:18:37,780
or more processes may read the same

00:18:33,550 --> 00:18:42,190
value so let's say that at index 2 I

00:18:37,780 --> 00:18:47,110
have value 2 and 4 processes with that

00:18:42,190 --> 00:18:51,400
and when they read that they add one to

00:18:47,110 --> 00:18:54,180
it so it's 3 and then they will four

00:18:51,400 --> 00:19:00,430
times write that 3 into the memory

00:18:54,180 --> 00:19:03,120
that's why you can achieve this so when

00:19:00,430 --> 00:19:05,800
you when you run the the left program

00:19:03,120 --> 00:19:10,150
you will get different values depending

00:19:05,800 --> 00:19:13,960
on what's going on in your system but

00:19:10,150 --> 00:19:16,330
the the answer will be wrong so for

00:19:13,960 --> 00:19:19,780
shared memory you always need to use

00:19:16,330 --> 00:19:23,440
some kind of synchronization and in this

00:19:19,780 --> 00:19:25,760
case I used locking so here we ensure

00:19:23,440 --> 00:19:29,860
that only one process can be

00:19:25,760 --> 00:19:32,560
it shared memory at the same time and

00:19:29,860 --> 00:19:36,850
only one can write to it

00:19:32,560 --> 00:19:40,250
so with that you get a good result but

00:19:36,850 --> 00:19:43,760
what's at the time you might say that

00:19:40,250 --> 00:19:46,570
the problem is too small or the data set

00:19:43,760 --> 00:19:49,760
is too small but that won't be the case

00:19:46,570 --> 00:19:53,390
the case here is when you use locking

00:19:49,760 --> 00:19:56,840
and you have multiple processes you in

00:19:53,390 --> 00:20:00,230
fact get sequential execution because

00:19:56,840 --> 00:20:03,080
only one process can take something from

00:20:00,230 --> 00:20:06,770
memory make calculations and write back

00:20:03,080 --> 00:20:10,940
to it so your codes will either be

00:20:06,770 --> 00:20:15,740
slower or run a more or less at the same

00:20:10,940 --> 00:20:18,920
time and believe me that here it's

00:20:15,740 --> 00:20:23,230
really easy to spot and see and usually

00:20:18,920 --> 00:20:26,810
we don't have that simple problems and

00:20:23,230 --> 00:20:30,710
actually you you can use something else

00:20:26,810 --> 00:20:35,900
these shared C type objects have their

00:20:30,710 --> 00:20:38,540
own locks so you can use them the the

00:20:35,900 --> 00:20:42,910
output will be the same but you will not

00:20:38,540 --> 00:20:46,430
create additional locks and you need to

00:20:42,910 --> 00:20:50,960
really keep the number of your locks as

00:20:46,430 --> 00:20:55,550
slow as possible to not to know what's

00:20:50,960 --> 00:20:58,910
going on okay so we also have managers

00:20:55,550 --> 00:21:01,760
in Python which are hybrid between the

00:20:58,910 --> 00:21:04,100
shared memory and message-passing so

00:21:01,760 --> 00:21:08,240
managers are proxies through which child

00:21:04,100 --> 00:21:11,240
processes can access data and when you

00:21:08,240 --> 00:21:15,590
create a manager it spawns new process

00:21:11,240 --> 00:21:19,370
which communicate through sockets so

00:21:15,590 --> 00:21:21,440
actually if you create if you type multi

00:21:19,370 --> 00:21:25,220
processing manager it will create a new

00:21:21,440 --> 00:21:28,010
process and you can later give that to

00:21:25,220 --> 00:21:31,390
two children of that process or you can

00:21:28,010 --> 00:21:35,110
even use it for remote access because

00:21:31,390 --> 00:21:35,110
choosing a circuit

00:21:36,100 --> 00:21:46,520
so for distributed memory the most

00:21:41,559 --> 00:21:52,549
commonly used tool is pull map how many

00:21:46,520 --> 00:21:55,159
of you have used pull map yeah some of

00:21:52,549 --> 00:21:56,990
you so it's really simple and nice so

00:21:55,159 --> 00:21:59,450
you just define how many processes you

00:21:56,990 --> 00:22:04,250
want a new map a certain function and a

00:21:59,450 --> 00:22:06,830
collection or its arguments and it just

00:22:04,250 --> 00:22:11,210
runs fine it's really high level and

00:22:06,830 --> 00:22:14,000
nice tool and you can simply achieve

00:22:11,210 --> 00:22:18,230
speed up so but you need to remember to

00:22:14,000 --> 00:22:24,049
always close or terminate your pool and

00:22:18,230 --> 00:22:26,120
later a tool to join it and if you if

00:22:24,049 --> 00:22:28,700
you're one of that kind that doesn't

00:22:26,120 --> 00:22:31,580
remember like me you can use it as a

00:22:28,700 --> 00:22:36,080
context manager and you'll and we also

00:22:31,580 --> 00:22:39,529
have something that looks like

00:22:36,080 --> 00:22:40,909
message-passing more so we also have

00:22:39,529 --> 00:22:45,169
pipes and queues

00:22:40,909 --> 00:22:48,409
so the basic difference is that pipe is

00:22:45,169 --> 00:22:51,260
only two ends it's really fast because

00:22:48,409 --> 00:22:55,820
it's usually using operating system

00:22:51,260 --> 00:23:00,230
pipes and cue I can have multiple

00:22:55,820 --> 00:23:02,299
producers and consumers but you need to

00:23:00,230 --> 00:23:06,409
have in mind that behind the scenes

00:23:02,299 --> 00:23:14,020
there are pipes connecting all elements

00:23:06,409 --> 00:23:16,340
of the network yes oh so pool has some

00:23:14,020 --> 00:23:19,070
overlooked features because people

00:23:16,340 --> 00:23:22,039
usually use it like like this so they

00:23:19,070 --> 00:23:26,120
create a pool with numbers of processes

00:23:22,039 --> 00:23:31,010
and then they just map map it some

00:23:26,120 --> 00:23:33,960
function to some some some some input so

00:23:31,010 --> 00:23:37,769
what you can define is for example

00:23:33,960 --> 00:23:40,649
max tasks tasks per child argument when

00:23:37,769 --> 00:23:43,200
sometimes your processes grow and

00:23:40,649 --> 00:23:46,200
consume more and more memory and you

00:23:43,200 --> 00:23:48,090
want to restart them once in a while so

00:23:46,200 --> 00:23:51,210
here you can define how many tasks

00:23:48,090 --> 00:23:52,429
should be executed per child until there

00:23:51,210 --> 00:23:56,580
is a new child

00:23:52,429 --> 00:23:59,309
you also can define a chunk size and I

00:23:56,580 --> 00:24:03,869
didn't know that until yesterday I think

00:23:59,309 --> 00:24:07,820
because nap usually Maps one execution

00:24:03,869 --> 00:24:15,360
to one element so if you have a map of

00:24:07,820 --> 00:24:19,139
four processes and let's say 12 12

00:24:15,360 --> 00:24:22,799
inputs the default chunk size is 1 so

00:24:19,139 --> 00:24:26,190
there will be 12 round trips between the

00:24:22,799 --> 00:24:28,350
worker and and the main process before

00:24:26,190 --> 00:24:32,070
the result will come back so you can

00:24:28,350 --> 00:24:36,149
optimize it tooth with that parameter so

00:24:32,070 --> 00:24:39,119
we can also define I'm up and I'm up

00:24:36,149 --> 00:24:43,860
unordered the difference is that I'm up

00:24:39,119 --> 00:24:48,539
still waits until all processes finish

00:24:43,860 --> 00:24:52,110
and because when you call map you will

00:24:48,539 --> 00:24:55,590
get a list with the results when you

00:24:52,110 --> 00:24:57,299
call a map you will get a generator but

00:24:55,590 --> 00:25:00,299
you will still need to wait until

00:24:57,299 --> 00:25:03,690
everything is finished and when you use

00:25:00,299 --> 00:25:06,600
a map unordered you get what comes what

00:25:03,690 --> 00:25:11,340
finishes first so that's that's useful

00:25:06,600 --> 00:25:14,399
and also there's an approach with at lie

00:25:11,340 --> 00:25:18,090
I think this is what's actually going on

00:25:14,399 --> 00:25:22,019
behind the scenes but it is discouraged

00:25:18,090 --> 00:25:25,049
to to use it because not maps are

00:25:22,019 --> 00:25:31,679
considered higher level and and better

00:25:25,049 --> 00:25:33,899
tools ok so we have different models for

00:25:31,679 --> 00:25:35,909
parallel programming we have also

00:25:33,899 --> 00:25:38,909
different models for distributed memory

00:25:35,909 --> 00:25:42,200
itself so we have something called

00:25:38,909 --> 00:25:45,100
worker based models so you can have a

00:25:42,200 --> 00:25:48,309
preferred model that

00:25:45,100 --> 00:25:51,490
coin uses so you might create your

00:25:48,309 --> 00:25:54,789
workers beforehand so you define that

00:25:51,490 --> 00:25:57,700
your application starts up with four

00:25:54,789 --> 00:26:01,150
processes or four threads that's pre

00:25:57,700 --> 00:26:03,730
fork you can have a worker model where

00:26:01,150 --> 00:26:07,419
you define during execution how many

00:26:03,730 --> 00:26:10,390
workers you need for example you

00:26:07,419 --> 00:26:15,760
optimize that to your data set and how

00:26:10,390 --> 00:26:17,770
it how its how well it divides and multi

00:26:15,760 --> 00:26:21,400
processing pool is an example of that

00:26:17,770 --> 00:26:24,130
and you also have a hybrid approach so

00:26:21,400 --> 00:26:26,980
you can define that number of workers

00:26:24,130 --> 00:26:29,289
beforehand and later you can scale them

00:26:26,980 --> 00:26:33,159
dynamically which is useful when you're

00:26:29,289 --> 00:26:39,100
working with something like a back-end

00:26:33,159 --> 00:26:41,140
server okay so when you when you want to

00:26:39,100 --> 00:26:44,770
create an application a multi working

00:26:41,140 --> 00:26:49,419
application and let's say that you want

00:26:44,770 --> 00:26:52,299
to respond to some requests when you

00:26:49,419 --> 00:26:56,610
have basically two approaches you can

00:26:52,299 --> 00:27:00,850
either use we use port and to use other

00:26:56,610 --> 00:27:02,980
flags for this ahead and I won't really

00:27:00,850 --> 00:27:06,400
go into details there's a really nice

00:27:02,980 --> 00:27:09,940
description on Stack Overflow about that

00:27:06,400 --> 00:27:13,090
and basically you can create as many

00:27:09,940 --> 00:27:16,870
processes or threads you want you can

00:27:13,090 --> 00:27:21,460
assign the same socket to them so all

00:27:16,870 --> 00:27:25,020
those workers can listen what's what's

00:27:21,460 --> 00:27:28,380
coming from the socket but you need to

00:27:25,020 --> 00:27:31,990
in in this scenario you have to ensure

00:27:28,380 --> 00:27:34,870
logs and synchronization because if

00:27:31,990 --> 00:27:39,010
you're going to read from all all those

00:27:34,870 --> 00:27:41,020
threads then you just get garbage so in

00:27:39,010 --> 00:27:44,830
twisted that's a really neat way of

00:27:41,020 --> 00:27:49,270
doing this so you create a socket let's

00:27:44,830 --> 00:27:51,920
say TCP socket you spawn child processes

00:27:49,270 --> 00:27:56,840
and you can later

00:27:51,920 --> 00:28:00,920
adopt a socket for the four child

00:27:56,840 --> 00:28:02,600
processes but then more and this is the

00:28:00,920 --> 00:28:04,790
approach which we should choose if you

00:28:02,600 --> 00:28:06,290
really want to tune your performance and

00:28:04,790 --> 00:28:11,420
if you really want to have access to

00:28:06,290 --> 00:28:13,370
some low level stuff and if you don't

00:28:11,420 --> 00:28:15,860
you can take a different approach which

00:28:13,370 --> 00:28:18,830
is most common where you have just a

00:28:15,860 --> 00:28:22,160
single thread reading from from the

00:28:18,830 --> 00:28:27,350
socket and this thread is responsible

00:28:22,160 --> 00:28:32,390
for i/o and later it delegates the work

00:28:27,350 --> 00:28:35,660
to to some other workers through AQ & AQ

00:28:32,390 --> 00:28:37,340
which I which I mentioned earlier so

00:28:35,660 --> 00:28:40,420
then you don't have any problems with

00:28:37,340 --> 00:28:45,670
synchronization and and stuff like that

00:28:40,420 --> 00:28:49,490
so up till now we've talked about

00:28:45,670 --> 00:28:54,920
so-called intra node communication so

00:28:49,490 --> 00:28:59,030
communication within a single single CPU

00:28:54,920 --> 00:29:03,080
or just one on server but you can also

00:28:59,030 --> 00:29:05,840
run your code or on multiple machines so

00:29:03,080 --> 00:29:09,770
there's there are many libraries you

00:29:05,840 --> 00:29:13,820
probably have heard about MPI which i

00:29:09,770 --> 00:29:17,120
think is the most commonly used library

00:29:13,820 --> 00:29:20,900
up to this day and when it comes to to

00:29:17,120 --> 00:29:26,650
scientists but there are some some other

00:29:20,900 --> 00:29:26,650
tools I personally like scooped

00:29:26,740 --> 00:29:34,190
maybe because it has a really nice

00:29:29,300 --> 00:29:36,850
slogan but it uses 0 mq sockets for

00:29:34,190 --> 00:29:40,330
communication it's really similar to

00:29:36,850 --> 00:29:44,720
multi-processing pool and it utilizes

00:29:40,330 --> 00:29:48,560
ssh connection for execution so you need

00:29:44,720 --> 00:29:51,860
to have SSH access to to the machines

00:29:48,560 --> 00:29:56,540
you want to run your your application on

00:29:51,860 --> 00:30:00,190
and later it connects to to them sense

00:29:56,540 --> 00:30:04,850
data and execute it so you can see that

00:30:00,190 --> 00:30:09,960
it's really really simple to use that

00:30:04,850 --> 00:30:16,070
okay so I've I've encountered some traps

00:30:09,960 --> 00:30:19,010
and some weird behavior over the years

00:30:16,070 --> 00:30:23,390
so I would like to share that with you

00:30:19,010 --> 00:30:30,120
so one possible trap is hyper threading

00:30:23,390 --> 00:30:35,100
so CPUs are often advertised as 16

00:30:30,120 --> 00:30:38,640
Hornet course 32 course 64 cards but how

00:30:35,100 --> 00:30:44,190
many physical course you get usually

00:30:38,640 --> 00:30:48,690
half of them so hyper threading works in

00:30:44,190 --> 00:30:51,530
this way that you have CPU pipeline and

00:30:48,690 --> 00:30:55,559
you have let's call them slots in them

00:30:51,530 --> 00:30:59,040
so if you have slots to run two things

00:30:55,559 --> 00:31:02,820
at the same time on one core then your

00:30:59,040 --> 00:31:07,400
two logical threads will run in in

00:31:02,820 --> 00:31:13,830
parallel but if you don't then they want

00:31:07,400 --> 00:31:18,059
so I had a problem I had a 12 core Intel

00:31:13,830 --> 00:31:21,570
Xeon machine and with 24 logical cores

00:31:18,059 --> 00:31:26,210
and when I ran my computation which was

00:31:21,570 --> 00:31:31,530
a really really complex computation and

00:31:26,210 --> 00:31:34,590
I'm sure that the result is not caused

00:31:31,530 --> 00:31:37,559
by communication or stuff like that I

00:31:34,590 --> 00:31:41,870
achieved these results so I've heard

00:31:37,559 --> 00:31:46,320
that Intel is launching a new tool for

00:31:41,870 --> 00:31:48,660
tuning and profiling Python so I think

00:31:46,320 --> 00:31:52,890
it might be interesting to to work with

00:31:48,660 --> 00:31:57,540
that also you don't always want to

00:31:52,890 --> 00:32:00,690
target 100% utilization because if you

00:31:57,540 --> 00:32:05,340
have four cores you have you prepared

00:32:00,690 --> 00:32:11,220
for workers and then you have 10% of

00:32:05,340 --> 00:32:13,080
each core not used in each CPU epoch so

00:32:11,220 --> 00:32:14,780
what you want to do is to just add

00:32:13,080 --> 00:32:18,170
workers to use that

00:32:14,780 --> 00:32:23,090
but you won't gain anything and actually

00:32:18,170 --> 00:32:26,450
you will lose the someone won't know why

00:32:23,090 --> 00:32:30,410
do we lose time here because we should

00:32:26,450 --> 00:32:36,260
utilize that additional spare 10% and it

00:32:30,410 --> 00:32:38,960
should be faster yes exactly so we are

00:32:36,260 --> 00:32:42,310
switching context so the all processes

00:32:38,960 --> 00:32:46,520
are fighting for the for resources and

00:32:42,310 --> 00:32:49,550
switching them and copying them for four

00:32:46,520 --> 00:32:52,370
different course it is a really

00:32:49,550 --> 00:32:59,720
expensive operation so don't always

00:32:52,370 --> 00:33:05,080
target that 100% also there's a funny

00:32:59,720 --> 00:33:10,550
thing in how pipes are implemented so

00:33:05,080 --> 00:33:14,270
pipes pipes cannot add these always

00:33:10,550 --> 00:33:17,090
pipes cannot send things both ways so if

00:33:14,270 --> 00:33:21,560
you define pipe duplex you'll actually

00:33:17,090 --> 00:33:24,010
get a socket and if sometimes you get a

00:33:21,560 --> 00:33:27,500
socket and sometimes you get a pipe and

00:33:24,010 --> 00:33:31,490
if you take into consideration that they

00:33:27,500 --> 00:33:35,180
have different buffers and defined in

00:33:31,490 --> 00:33:36,770
the kernel then you will be you you

00:33:35,180 --> 00:33:39,860
might encounter a situation where

00:33:36,770 --> 00:33:43,070
sometimes you will be able to send some

00:33:39,860 --> 00:33:46,100
something and sometimes not so that's

00:33:43,070 --> 00:33:49,970
interesting and also you have the usual

00:33:46,100 --> 00:33:53,870
topic which is deadlocks so when one

00:33:49,970 --> 00:33:56,330
process has some resource second process

00:33:53,870 --> 00:34:00,170
has its own resource and they wait for

00:33:56,330 --> 00:34:02,960
for each other but they do not free

00:34:00,170 --> 00:34:07,280
their resources then they will wait

00:34:02,960 --> 00:34:12,560
forever and do you know how to kill

00:34:07,280 --> 00:34:16,780
processes and threats in Python so there

00:34:12,560 --> 00:34:19,850
is a kill method so who used kill method

00:34:16,780 --> 00:34:22,280
okay so you couldn't use kill method

00:34:19,850 --> 00:34:26,360
because it does not exist because you

00:34:22,280 --> 00:34:27,740
cannot kill a threat it's by design

00:34:26,360 --> 00:34:31,100
because you might end up in this

00:34:27,740 --> 00:34:33,860
situation where your thread holds a

00:34:31,100 --> 00:34:35,780
resource when skilled other threads will

00:34:33,860 --> 00:34:38,900
never get it because it's never being

00:34:35,780 --> 00:34:43,600
freed so that's why you need to use some

00:34:38,900 --> 00:34:47,810
different mechanics and when we are at

00:34:43,600 --> 00:34:51,950
the threads there are some there's a

00:34:47,810 --> 00:34:54,410
common misconception with diamonds so if

00:34:51,950 --> 00:34:57,530
you have wild true or something similar

00:34:54,410 --> 00:35:01,310
in your thread it should be a diamond

00:34:57,530 --> 00:35:05,750
and diamonds should not be joined once

00:35:01,310 --> 00:35:08,060
you set up a thread as as diamond they

00:35:05,750 --> 00:35:10,790
should just run as long as the process

00:35:08,060 --> 00:35:15,070
is running and the only clean way of

00:35:10,790 --> 00:35:19,970
stopping them is up to you

00:35:15,070 --> 00:35:24,290
so also don't use global variables don't

00:35:19,970 --> 00:35:28,040
define stop equals false and then

00:35:24,290 --> 00:35:31,040
iterate it unless it it changes because

00:35:28,040 --> 00:35:33,320
you might never know what will happen

00:35:31,040 --> 00:35:36,740
and when those threads will be stopped

00:35:33,320 --> 00:35:39,200
so the common pattern is to use events

00:35:36,740 --> 00:35:43,490
and to just send an event in the main

00:35:39,200 --> 00:35:50,150
thread and to wait for that event in in

00:35:43,490 --> 00:35:53,630
worker threads okay so when it comes to

00:35:50,150 --> 00:35:57,890
parallel and asynchronous we finally

00:35:53,630 --> 00:35:59,870
reach the topic so we have basically two

00:35:57,890 --> 00:36:02,960
options a threaded option and the

00:35:59,870 --> 00:36:06,080
process option where we can defined

00:36:02,960 --> 00:36:09,760
exactly tours we can submit jobs to them

00:36:06,080 --> 00:36:12,950
and basically what you get is futures

00:36:09,760 --> 00:36:17,990
and also you can define them as context

00:36:12,950 --> 00:36:19,880
managers and you can run you can run

00:36:17,990 --> 00:36:22,910
that without starting at the i/o loop

00:36:19,880 --> 00:36:25,580
and just get futures and or you can use

00:36:22,910 --> 00:36:28,550
them with i/o loop then you need

00:36:25,580 --> 00:36:31,520
adapters and sometimes those adapters

00:36:28,550 --> 00:36:33,680
work and sometimes they don't so you

00:36:31,520 --> 00:36:36,440
need to be really careful and also

00:36:33,680 --> 00:36:39,140
keyword arguments are not allowed in

00:36:36,440 --> 00:36:41,210
executors so you might want to

00:36:39,140 --> 00:36:45,320
read the pep to know why if you really

00:36:41,210 --> 00:36:49,790
need your key keyword argument then use

00:36:45,320 --> 00:36:54,170
this partial also you can you can submit

00:36:49,790 --> 00:36:59,450
several jobs and wait for the mold for

00:36:54,170 --> 00:37:05,300
all of them to to finish so coming to to

00:36:59,450 --> 00:37:07,880
an end why would you want that so you

00:37:05,300 --> 00:37:12,380
might want that for long running tasks

00:37:07,880 --> 00:37:14,930
that might block your i/o loop so you

00:37:12,380 --> 00:37:17,540
might want that if you have some code

00:37:14,930 --> 00:37:19,930
which is incompatible with it with your

00:37:17,540 --> 00:37:24,080
ire loop and that will most certainly

00:37:19,930 --> 00:37:27,440
lock it like requests you cannot use

00:37:24,080 --> 00:37:31,720
requests with with any a loop that

00:37:27,440 --> 00:37:35,030
exists for now and also if you have some

00:37:31,720 --> 00:37:38,540
running blocking tasks that you want to

00:37:35,030 --> 00:37:44,500
run in parallel so what will you get

00:37:38,540 --> 00:37:48,160
when you use that headache because

00:37:44,500 --> 00:37:52,160
running things asynchronously is

00:37:48,160 --> 00:37:55,490
troublesome and when you introduce also

00:37:52,160 --> 00:37:58,460
running it in parallel it's also

00:37:55,490 --> 00:38:02,420
troublesome so you should really know

00:37:58,460 --> 00:38:05,690
that you need it okay so let me run for

00:38:02,420 --> 00:38:09,380
a moment just before I finish so you all

00:38:05,690 --> 00:38:11,780
know this gentleman sting teachers and

00:38:09,380 --> 00:38:13,790
he shall said that there should be one

00:38:11,780 --> 00:38:18,740
and preferably only one obvious way to

00:38:13,790 --> 00:38:21,440
do it so where have we gone wrong we

00:38:18,740 --> 00:38:26,120
have currently four commonly used a

00:38:21,440 --> 00:38:32,300
loops we can three types of asynchronous

00:38:26,120 --> 00:38:36,680
calls so if there are some decisive

00:38:32,300 --> 00:38:38,440
people in that crowd let's let's think

00:38:36,680 --> 00:38:40,450
how to fix that it

00:38:38,440 --> 00:38:42,849
Python 3 was created in order to clean

00:38:40,450 --> 00:38:43,390
up some mess which accumulated over the

00:38:42,849 --> 00:38:47,829
years

00:38:43,390 --> 00:38:49,950
and I feel that now we're creating such

00:38:47,829 --> 00:38:54,700
mess again

00:38:49,950 --> 00:38:56,710
ok so in summary python has a wide

00:38:54,700 --> 00:38:59,560
variety of options when it comes to

00:38:56,710 --> 00:39:02,500
panel and asynchronous programming

00:38:59,560 --> 00:39:04,390
inside you should really know your

00:39:02,500 --> 00:39:07,030
architecture when you use parallel

00:39:04,390 --> 00:39:10,119
programming you should always test your

00:39:07,030 --> 00:39:10,990
code before entering parallel concurrent

00:39:10,119 --> 00:39:14,670
world

00:39:10,990 --> 00:39:17,260
so first sequential than then parallel

00:39:14,670 --> 00:39:21,430
after you enter the concurrency world

00:39:17,260 --> 00:39:23,319
you should test it with fuzzing I didn't

00:39:21,430 --> 00:39:26,349
say anything about that but you can't

00:39:23,319 --> 00:39:29,470
really research that be aware of any

00:39:26,349 --> 00:39:32,920
incompatibilities between modules and I

00:39:29,470 --> 00:39:36,250
assure you that they do exist be sure

00:39:32,920 --> 00:39:40,240
when you be yeah be sure when you should

00:39:36,250 --> 00:39:42,720
expect available objects in asynchronous

00:39:40,240 --> 00:39:47,079
programming and handle them properly and

00:39:42,720 --> 00:39:50,710
also you know those tools are for us and

00:39:47,079 --> 00:39:53,319
they mostly work and you can create even

00:39:50,710 --> 00:39:56,170
production code with it if you test it

00:39:53,319 --> 00:39:59,230
well so don't be scared to seek out new

00:39:56,170 --> 00:40:00,270
options and to boldly go where no man

00:39:59,230 --> 00:40:02,750
has gone before

00:40:00,270 --> 00:40:11,869
thank

00:40:02,750 --> 00:40:11,869
[Applause]

00:40:12,760 --> 00:40:20,630
hopefully it will be too soft cake

00:40:16,960 --> 00:40:24,830
thanks Harlan for questions yeah yeah we

00:40:20,630 --> 00:40:31,460
have a trimmings for questions questions

00:40:24,830 --> 00:40:34,250
for me higher on a lot of any questions

00:40:31,460 --> 00:40:37,790
so actually I brought something nice

00:40:34,250 --> 00:40:46,580
from Poland for people who ask the best

00:40:37,790 --> 00:40:48,980
questions you mentioned about deadlock I

00:40:46,580 --> 00:40:52,040
am curious about if there's a that

00:40:48,980 --> 00:40:57,740
library also detect deadlocks in the

00:40:52,040 --> 00:41:00,830
program which can appear I know that

00:40:57,740 --> 00:41:04,180
such solutions exist but I but I don't

00:41:00,830 --> 00:41:08,740
remember the names but you can mostly

00:41:04,180 --> 00:41:13,630
get that with fuzzing or just testing

00:41:08,740 --> 00:41:13,630
some unexpected behaviors

00:41:20,160 --> 00:41:26,340
in your last slide

00:41:23,830 --> 00:41:28,900
you said you mentioned about

00:41:26,340 --> 00:41:30,850
incompatibilities between modules maybe

00:41:28,900 --> 00:41:33,460
you can tell something more about that

00:41:30,850 --> 00:41:37,090
but what was your experience what my

00:41:33,460 --> 00:41:39,940
dues were incompatible and what could

00:41:37,090 --> 00:41:42,820
you could you say that again and a bit

00:41:39,940 --> 00:41:45,070
louder so yeah right be aware of any

00:41:42,820 --> 00:42:02,940
incompatibilities between modules you

00:41:45,070 --> 00:42:07,830
use so what were you referring to the

00:42:02,940 --> 00:42:07,830
incompatible yes incompatibilities

00:42:12,900 --> 00:42:21,690
okay sorry for that um so like we

00:42:18,269 --> 00:42:23,819
mentioned we have different eye loops so

00:42:21,690 --> 00:42:27,450
let's say that you want to use tornado

00:42:23,819 --> 00:42:32,089
it has its own a loop but you want to

00:42:27,450 --> 00:42:36,599
also use some process executor which

00:42:32,089 --> 00:42:40,380
does not run really well with that

00:42:36,599 --> 00:42:45,630
without adapters so what you might to do

00:42:40,380 --> 00:42:48,930
is to first adapt your program your turn

00:42:45,630 --> 00:42:54,019
your tornado program to run on async I

00:42:48,930 --> 00:42:58,619
Oh and then ran that those executor for

00:42:54,019 --> 00:43:01,380
example curio does not work with any

00:42:58,619 --> 00:43:06,150
other i/o loops so you don't even get

00:43:01,380 --> 00:43:10,980
any so don't even get anything to

00:43:06,150 --> 00:43:12,390
connect to them and yeah and for example

00:43:10,980 --> 00:43:14,880
in tornado

00:43:12,390 --> 00:43:18,150
you should not yield from you should

00:43:14,880 --> 00:43:20,220
yield from a curtain so there there's

00:43:18,150 --> 00:43:24,029
also something incompatibility and

00:43:20,220 --> 00:43:26,020
that's that's what I meant okay thanks

00:43:24,029 --> 00:43:30,340
we have for representation

00:43:26,020 --> 00:43:30,340

YouTube URL: https://www.youtube.com/watch?v=ZKzCx4D5c3g


