Title: Eyal Trabelsi - Practical Optimisations for Pandas
Publication date: 2020-09-21
Playlist: EuroPython 2020
Description: 
	"Practical Optimisations for Pandas
EuroPython 2020 - Talk - 2020-07-23 - Parrot Data Science
Online

By Eyal Trabelsi

Writing performant pandas code is not an easy task, in this talk I will explain how to find the bottlenecks and how to write proper code with computational efficiency, and memory optimisation in mind.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2020.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:07,200 --> 00:00:12,799
hi everyone nice to meet you i'm joined

00:00:10,480 --> 00:00:15,440
uniform

00:00:12,799 --> 00:00:17,039
oh that's awesome how's the weather like

00:00:15,440 --> 00:00:21,039
in israel

00:00:17,039 --> 00:00:23,840
it's amazing oh that's pretty cool

00:00:21,039 --> 00:00:24,720
yeah all right so i'm gonna let you get

00:00:23,840 --> 00:00:27,840
started

00:00:24,720 --> 00:00:30,400
all the best thank you

00:00:27,840 --> 00:00:31,920
so hi everyone i hope you're enjoying

00:00:30,400 --> 00:00:34,160
all the amazing

00:00:31,920 --> 00:00:35,040
talks that already been taken today i'm

00:00:34,160 --> 00:00:37,760
very excited

00:00:35,040 --> 00:00:38,160
to speak in this amazing conference and

00:00:37,760 --> 00:00:40,800
i'm

00:00:38,160 --> 00:00:44,640
gonna speak to you about practical

00:00:40,800 --> 00:00:47,440
optimizations for your pandas code so

00:00:44,640 --> 00:00:48,719
i'm aldrabelsi i'm a software engineer

00:00:47,440 --> 00:00:51,680
at salesforce

00:00:48,719 --> 00:00:53,840
i have big passion for python data and

00:00:51,680 --> 00:00:56,000
performance optimization

00:00:53,840 --> 00:00:58,320
and you can find me sharing the

00:00:56,000 --> 00:01:00,660
knowledge at twitter and

00:00:58,320 --> 00:01:02,399
in media so let's begin

00:01:00,660 --> 00:01:05,439
[Music]

00:01:02,399 --> 00:01:08,799
uh let's take a brief overview

00:01:05,439 --> 00:01:11,520
of what expanders so pandas

00:01:08,799 --> 00:01:12,880
is a library for data manipulation and

00:01:11,520 --> 00:01:16,400
analysis

00:01:12,880 --> 00:01:20,080
it's loaded the data into

00:01:16,400 --> 00:01:23,600
ram and it's widely used and it has

00:01:20,080 --> 00:01:26,720
a vast ecosystem which make it

00:01:23,600 --> 00:01:29,840
a great tool for you to use in your

00:01:26,720 --> 00:01:32,560
data science projects

00:01:29,840 --> 00:01:34,000
but the next question is why should we

00:01:32,560 --> 00:01:37,600
even care about

00:01:34,000 --> 00:01:38,640
performance so obviously fast is better

00:01:37,600 --> 00:01:41,840
than slow

00:01:38,640 --> 00:01:45,759
we don't want to wait for our program to

00:01:41,840 --> 00:01:46,560
execute in addition memory efficiency is

00:01:45,759 --> 00:01:49,680
good

00:01:46,560 --> 00:01:52,799
we are terrified of either

00:01:49,680 --> 00:01:56,079
the out of memory error or our

00:01:52,799 --> 00:01:59,360
aws bills so

00:01:56,079 --> 00:02:00,159
by using more efficient machines or less

00:01:59,360 --> 00:02:03,520
machines

00:02:00,159 --> 00:02:04,159
we can save money and if we are even

00:02:03,520 --> 00:02:07,680
willing

00:02:04,159 --> 00:02:09,039
to spend that amount of money hardware

00:02:07,680 --> 00:02:11,520
will only take us

00:02:09,039 --> 00:02:11,520
so far

00:02:12,400 --> 00:02:18,560
so now i hope that i showed you why we

00:02:16,400 --> 00:02:22,080
should care about performance

00:02:18,560 --> 00:02:24,160
but the obvious answer is should we care

00:02:22,080 --> 00:02:28,160
about performance always

00:02:24,160 --> 00:02:30,400
or just uh in certain scenarios

00:02:28,160 --> 00:02:31,280
and i'm gonna speak when should we care

00:02:30,400 --> 00:02:36,000
about

00:02:31,280 --> 00:02:39,519
performance so first as developers and

00:02:36,000 --> 00:02:40,879
as a data scientist we should thrive to

00:02:39,519 --> 00:02:44,480
make our code

00:02:40,879 --> 00:02:45,360
cleaner and maintainable so performance

00:02:44,480 --> 00:02:50,560
optimization

00:02:45,360 --> 00:02:54,560
should come later on

00:02:50,560 --> 00:02:57,760
so uh it should come later on

00:02:54,560 --> 00:02:59,680
so obviously we should performance uh do

00:02:57,760 --> 00:03:02,879
performance optimization

00:02:59,680 --> 00:03:05,599
only if it's affect our users

00:03:02,879 --> 00:03:06,319
so when the program doesn't meet the

00:03:05,599 --> 00:03:09,360
requirements

00:03:06,319 --> 00:03:11,519
either that it doesn't just run fast

00:03:09,360 --> 00:03:14,319
enough or that it fails due to

00:03:11,519 --> 00:03:15,040
out of memory exception another good

00:03:14,319 --> 00:03:17,920
reason for

00:03:15,040 --> 00:03:19,599
performance optimization is when the

00:03:17,920 --> 00:03:23,280
development phase

00:03:19,599 --> 00:03:26,319
is hurt due to uh the fact that your uh

00:03:23,280 --> 00:03:29,200
your program run uh every once

00:03:26,319 --> 00:03:31,280
in a in in an hour instead of every 10

00:03:29,200 --> 00:03:34,640
minutes

00:03:31,280 --> 00:03:37,760
so you should really try to optimize

00:03:34,640 --> 00:03:38,799
only the parts of the code that are a

00:03:37,760 --> 00:03:41,519
troublesome

00:03:38,799 --> 00:03:42,319
it can be done by identifying our battle

00:03:41,519 --> 00:03:45,040
necks

00:03:42,319 --> 00:03:46,560
using profiling i'm gonna cover it

00:03:45,040 --> 00:03:49,840
briefly in this talk

00:03:46,560 --> 00:03:52,239
uh but obviously it's it's a big topic

00:03:49,840 --> 00:03:53,360
in addition when we refactor in the

00:03:52,239 --> 00:03:55,599
little code

00:03:53,360 --> 00:03:56,879
we want the code to be well tested

00:03:55,599 --> 00:04:00,400
because the code

00:03:56,879 --> 00:04:01,599
is useless if it's basically run fast

00:04:00,400 --> 00:04:05,280
and along

00:04:01,599 --> 00:04:05,280
and give us one results

00:04:05,360 --> 00:04:11,280
okay so this is a nice meme about uh

00:04:08,640 --> 00:04:12,239
the difference between a snail and

00:04:11,280 --> 00:04:14,720
python

00:04:12,239 --> 00:04:15,599
and uh enterprise employee doesn't find

00:04:14,720 --> 00:04:18,160
uh

00:04:15,599 --> 00:04:19,600
the difference but i'm here to tell you

00:04:18,160 --> 00:04:22,720
that you should have faith

00:04:19,600 --> 00:04:23,390
in python and in pandas and that you can

00:04:22,720 --> 00:04:24,479
gain uh

00:04:23,390 --> 00:04:27,440
[Music]

00:04:24,479 --> 00:04:29,199
a significant performance improvement if

00:04:27,440 --> 00:04:32,639
you follow

00:04:29,199 --> 00:04:36,320
what what i'm gonna show you so i did

00:04:32,639 --> 00:04:40,880
this analysis uh on a data set

00:04:36,320 --> 00:04:44,880
of invoices of mil invoices it's old

00:04:40,880 --> 00:04:45,680
uh 1 million lines it's a not a big data

00:04:44,880 --> 00:04:48,800
set but

00:04:45,680 --> 00:04:52,240
it allows me to show you the

00:04:48,800 --> 00:04:54,400
the issues and the tricks so let's begin

00:04:52,240 --> 00:04:55,520
and understand how we can optimize our

00:04:54,400 --> 00:04:58,400
code

00:04:55,520 --> 00:04:59,440
so i'm gonna cover uh the tips from the

00:04:58,400 --> 00:05:02,720
easiest

00:04:59,440 --> 00:05:03,199
and the the ones that are the best value

00:05:02,720 --> 00:05:06,639
for

00:05:03,199 --> 00:05:11,440
money either they allow the easiest or

00:05:06,639 --> 00:05:14,479
they give us the best boost

00:05:11,440 --> 00:05:17,759
so first we should only use what we need

00:05:14,479 --> 00:05:21,039
and by that i mean we should keep only

00:05:17,759 --> 00:05:22,320
the columns that we use so if i i'm not

00:05:21,039 --> 00:05:24,639
going to use

00:05:22,320 --> 00:05:26,639
the day column i can remove it and by

00:05:24,639 --> 00:05:29,919
that i'm saving a lot of

00:05:26,639 --> 00:05:33,600
memory which is consumed by

00:05:29,919 --> 00:05:34,240
the pandas also if there are rows that i

00:05:33,600 --> 00:05:37,520
don't need

00:05:34,240 --> 00:05:38,560
i should filter filter them first and by

00:05:37,520 --> 00:05:40,960
that i reduce

00:05:38,560 --> 00:05:42,560
both the memory footprint and the

00:05:40,960 --> 00:05:45,199
execution time

00:05:42,560 --> 00:05:47,440
we should be like this little fella and

00:05:45,199 --> 00:05:50,160
use only what we need

00:05:47,440 --> 00:05:51,120
the next thing is we should not reinvent

00:05:50,160 --> 00:05:54,160
the wheel

00:05:51,120 --> 00:05:57,039
we have vast ecosystems we have numpy

00:05:54,160 --> 00:05:58,479
sci-fi and many other libraries we

00:05:57,039 --> 00:06:01,440
should use these existing

00:05:58,479 --> 00:06:03,759
solutions because they have less bugs

00:06:01,440 --> 00:06:07,039
and they are highly optimized

00:06:03,759 --> 00:06:09,759
so instead of creating our own uh k

00:06:07,039 --> 00:06:12,560
n implementation we should probably use

00:06:09,759 --> 00:06:15,840
the scipy or psychic learn

00:06:12,560 --> 00:06:18,880
implementation so

00:06:15,840 --> 00:06:19,520
now we're gonna delve into some more

00:06:18,880 --> 00:06:22,720
details

00:06:19,520 --> 00:06:25,759
about optimizations so pandas

00:06:22,720 --> 00:06:26,720
really aids loops uh it's highly

00:06:25,759 --> 00:06:29,840
optimized

00:06:26,720 --> 00:06:32,960
for uh doing vectorized

00:06:29,840 --> 00:06:34,639
calculation which means calculating the

00:06:32,960 --> 00:06:38,319
entire a

00:06:34,639 --> 00:06:41,520
column in the same time so obviously

00:06:38,319 --> 00:06:44,560
the bad option is to simply

00:06:41,520 --> 00:06:48,160
iterate over our data frame and

00:06:44,560 --> 00:06:51,280
do some calculations so either by using

00:06:48,160 --> 00:06:55,120
intervals or a regular loop

00:06:51,280 --> 00:06:58,639
i can iterate and do some uh

00:06:55,120 --> 00:07:02,720
simple calculation on a row by row uh

00:06:58,639 --> 00:07:06,720
basis you see here i'm

00:07:02,720 --> 00:07:09,039
just doing subtraction uh

00:07:06,720 --> 00:07:09,759
and then price with the tip to get the

00:07:09,039 --> 00:07:13,360
original

00:07:09,759 --> 00:07:17,199
mail price and as you can see this takes

00:07:13,360 --> 00:07:20,319
35 minutes to execute it which is a lot

00:07:17,199 --> 00:07:21,280
i'm using uh the time it module in this

00:07:20,319 --> 00:07:23,680
uh

00:07:21,280 --> 00:07:24,800
in this talk in order to show you how

00:07:23,680 --> 00:07:27,919
long it does

00:07:24,800 --> 00:07:31,199
a method that take to be executed

00:07:27,919 --> 00:07:34,080
take so long i only

00:07:31,199 --> 00:07:34,080
run it once

00:07:34,800 --> 00:07:42,479
and we should look how we can

00:07:38,560 --> 00:07:45,680
optimize it better so instead of using

00:07:42,479 --> 00:07:46,479
the iteros i can simply use the apply

00:07:45,680 --> 00:07:49,680
method

00:07:46,479 --> 00:07:51,599
apply method get a function and i can do

00:07:49,680 --> 00:07:54,479
something that is very familiar

00:07:51,599 --> 00:07:55,840
for each row i am doing the same

00:07:54,479 --> 00:07:58,960
subtraction

00:07:55,840 --> 00:07:59,759
and returning the result so as you can

00:07:58,960 --> 00:08:03,520
see

00:07:59,759 --> 00:08:07,280
from 35 minutes we got to 22 seconds

00:08:03,520 --> 00:08:10,879
which uh is a major major change

00:08:07,280 --> 00:08:13,919
so just by doing that we got 100 times

00:08:10,879 --> 00:08:17,039
improvement in execution times so

00:08:13,919 --> 00:08:18,920
obviously intervals and regular loops

00:08:17,039 --> 00:08:20,639
are evil and we shouldn't use them

00:08:18,920 --> 00:08:23,840
[Music]

00:08:20,639 --> 00:08:26,879
but there is a better option we can use

00:08:23,840 --> 00:08:30,000
the vectorized methods of either

00:08:26,879 --> 00:08:31,199
a number pandas of sci-fi i will show

00:08:30,000 --> 00:08:34,159
you an example

00:08:31,199 --> 00:08:35,599
so here i'm doing the subtract on the

00:08:34,159 --> 00:08:39,519
entire

00:08:35,599 --> 00:08:42,479
column and although not just the

00:08:39,519 --> 00:08:43,200
look cleaner it will be much much much

00:08:42,479 --> 00:08:47,120
faster

00:08:43,200 --> 00:08:50,720
so instead of 22 seconds i'm getting

00:08:47,120 --> 00:08:53,600
two and a half milliseconds which is

00:08:50,720 --> 00:08:54,399
8 000 times improvement in execution

00:08:53,600 --> 00:08:57,040
time

00:08:54,399 --> 00:08:58,399
and i don't want to remind you how how

00:08:57,040 --> 00:09:01,279
long did it take

00:08:58,399 --> 00:09:02,320
when i used the iterate method instead

00:09:01,279 --> 00:09:05,440
so we should really

00:09:02,320 --> 00:09:09,839
use a vectorized method to

00:09:05,440 --> 00:09:09,839
optimize our code further

00:09:10,160 --> 00:09:13,519
the next optimization that i'm gonna

00:09:11,920 --> 00:09:17,360
speak to you about

00:09:13,519 --> 00:09:20,000
is picking dry types so

00:09:17,360 --> 00:09:23,279
i'm gonna try to convince you the

00:09:20,000 --> 00:09:26,880
motivation of it by simply creating

00:09:23,279 --> 00:09:32,480
a number numpy array which contains

00:09:26,880 --> 00:09:32,480
only the number one over and over again

00:09:32,560 --> 00:09:39,360
and i'm i'm gonna create that frame

00:09:36,160 --> 00:09:42,080
of this exactly same array

00:09:39,360 --> 00:09:43,519
with just different types with objects

00:09:42,080 --> 00:09:46,640
with floats with

00:09:43,519 --> 00:09:50,240
in 64 in 63 and so

00:09:46,640 --> 00:09:53,200
on and after i create it i will check

00:09:50,240 --> 00:09:55,040
the memory usage and as you can see the

00:09:53,200 --> 00:09:58,320
exactly same array

00:09:55,040 --> 00:10:01,920
can take you 80

00:09:58,320 --> 00:10:05,440
times more memory if you are using

00:10:01,920 --> 00:10:09,440
a boolean oval object

00:10:05,440 --> 00:10:12,480
so obviously we should try to improve it

00:10:09,440 --> 00:10:16,079
and let's see how our data look

00:10:12,480 --> 00:10:19,360
in my specific example so i can

00:10:16,079 --> 00:10:23,200
look at the data frame size by using the

00:10:19,360 --> 00:10:24,079
dot memory usage methods and as i can

00:10:23,200 --> 00:10:27,839
see

00:10:24,079 --> 00:10:32,240
we have 47 megabytes of

00:10:27,839 --> 00:10:34,800
data and if i want i can see how

00:10:32,240 --> 00:10:35,760
the memory is distributed across

00:10:34,800 --> 00:10:39,440
different

00:10:35,760 --> 00:10:42,880
columns if i want to optimize

00:10:39,440 --> 00:10:47,440
only one of those but i

00:10:42,880 --> 00:10:50,720
i advise to optimize all your types

00:10:47,440 --> 00:10:54,160
because it's easy so how can we do it

00:10:50,720 --> 00:10:56,079
so before before i will show you how we

00:10:54,160 --> 00:10:56,720
can do it i will show you the supported

00:10:56,079 --> 00:10:59,600
types in

00:10:56,720 --> 00:11:00,079
pandas so we have ants we have floats we

00:10:59,600 --> 00:11:03,760
have

00:11:00,079 --> 00:11:07,279
booleans we have objects we have a times

00:11:03,760 --> 00:11:11,440
and we have a few more types that

00:11:07,279 --> 00:11:14,320
no many people know about so we have

00:11:11,440 --> 00:11:15,519
the category type the category the

00:11:14,320 --> 00:11:18,959
category type

00:11:15,519 --> 00:11:22,320
is a gray type when the same value occur

00:11:18,959 --> 00:11:25,440
over and over and over again we have

00:11:22,320 --> 00:11:29,120
spouse types which are great

00:11:25,440 --> 00:11:32,640
when most of the array contain

00:11:29,120 --> 00:11:36,160
nones and we have nullabel inches

00:11:32,640 --> 00:11:39,200
integers and booleans so

00:11:36,160 --> 00:11:42,560
by default none is a thought so

00:11:39,200 --> 00:11:43,200
even if i have an array that is a

00:11:42,560 --> 00:11:46,240
boolean

00:11:43,200 --> 00:11:46,880
and i have only one none it will be

00:11:46,240 --> 00:11:50,000
casted

00:11:46,880 --> 00:11:52,399
as float so instead i can use the null

00:11:50,000 --> 00:11:53,120
integers and nullable booleans in order

00:11:52,399 --> 00:11:56,320
to

00:11:53,120 --> 00:11:59,120
avoid it so how can

00:11:56,320 --> 00:11:59,920
we optimize the types so the best

00:11:59,120 --> 00:12:03,040
solution

00:11:59,920 --> 00:12:04,959
is to use to load the data frame with

00:12:03,040 --> 00:12:06,800
the specific types that we want

00:12:04,959 --> 00:12:09,760
beforehand

00:12:06,800 --> 00:12:10,560
and if it's not enough we can use the as

00:12:09,760 --> 00:12:14,079
cast

00:12:10,560 --> 00:12:17,920
as type methods or two numeric

00:12:14,079 --> 00:12:20,240
and two time delta uh functions

00:12:17,920 --> 00:12:21,440
we have the downcast parameter which can

00:12:20,240 --> 00:12:24,800
help us understand

00:12:21,440 --> 00:12:28,800
if it's an n64 or an end

00:12:24,800 --> 00:12:32,560
32 and in our example

00:12:28,800 --> 00:12:36,000
i use the as type in order to emphasize

00:12:32,560 --> 00:12:38,000
the type change so i decided that tails

00:12:36,000 --> 00:12:40,480
adjustment should be a boolean

00:12:38,000 --> 00:12:41,279
and the middle price can be in 60

00:12:40,480 --> 00:12:44,800
instead of

00:12:41,279 --> 00:12:44,800
float 64.

00:12:44,959 --> 00:12:48,160
and let's see

00:12:49,200 --> 00:12:55,600
by just doing that so the memory usage

00:12:52,320 --> 00:12:57,200
was reduced to 3.7 megabytes and we can

00:12:55,600 --> 00:13:00,720
see how

00:12:57,200 --> 00:13:04,160
the memory is distributed which is

00:13:00,720 --> 00:13:08,079
12 times performance improvement

00:13:04,160 --> 00:13:08,800
in memory so we should really try to

00:13:08,079 --> 00:13:12,639
optimize

00:13:08,800 --> 00:13:15,760
and pick the most specific type

00:13:12,639 --> 00:13:18,399
of a

00:13:15,760 --> 00:13:18,399
data set

00:13:19,040 --> 00:13:25,200
in addition if we if we don't find

00:13:22,399 --> 00:13:26,399
a good enough type we can create our own

00:13:25,200 --> 00:13:30,320
custom types

00:13:26,399 --> 00:13:34,959
so pandas provide an extension array

00:13:30,320 --> 00:13:37,279
api to create such object

00:13:34,959 --> 00:13:38,399
but obviously it will take you a lot of

00:13:37,279 --> 00:13:40,390
time and

00:13:38,399 --> 00:13:41,680
i suggest to do it only

00:13:40,390 --> 00:13:45,440
[Music]

00:13:41,680 --> 00:13:48,720
if you are a very experienced

00:13:45,440 --> 00:13:49,760
pandas developer but there are open

00:13:48,720 --> 00:13:53,360
source types

00:13:49,760 --> 00:13:56,160
so there are types for a ip like

00:13:53,360 --> 00:13:57,040
objects which is in the cyberpunk

00:13:56,160 --> 00:14:00,399
library

00:13:57,040 --> 00:14:04,000
and there are special objects which

00:14:00,399 --> 00:14:08,320
are in the gale pandas

00:14:04,000 --> 00:14:08,320
so this is a nice meme about types

00:14:08,399 --> 00:14:12,320
about i'm gonna speak to you about some

00:14:11,360 --> 00:14:15,680
uh

00:14:12,320 --> 00:14:16,000
panda usage which functions uh you

00:14:15,680 --> 00:14:18,560
should

00:14:16,000 --> 00:14:19,040
choose over the other i'm not gonna

00:14:18,560 --> 00:14:22,000
cover

00:14:19,040 --> 00:14:22,959
all of them because it's a huge topic

00:14:22,000 --> 00:14:26,160
but

00:14:22,959 --> 00:14:30,000
we should just begin

00:14:26,160 --> 00:14:33,040
so we can process our

00:14:30,000 --> 00:14:36,240
data chunked by chunk and

00:14:33,040 --> 00:14:37,279
by that i mean we can put our data into

00:14:36,240 --> 00:14:41,120
smaller parts

00:14:37,279 --> 00:14:43,920
and then execute code into each chunk

00:14:41,120 --> 00:14:45,440
i think that we can work on large data

00:14:43,920 --> 00:14:49,440
sets that are much

00:14:45,440 --> 00:14:52,320
bigger than uh our model

00:14:49,440 --> 00:14:53,360
so it would look something and follow a

00:14:52,320 --> 00:14:56,800
huge file

00:14:53,360 --> 00:15:00,000
uh and we are gonna read the the file

00:14:56,800 --> 00:15:02,880
uh chunk by chunk and then we are

00:15:00,000 --> 00:15:05,199
gonna do some processing on it it's

00:15:02,880 --> 00:15:07,600
important to note that this

00:15:05,199 --> 00:15:09,360
is a great optimization to reduce the

00:15:07,600 --> 00:15:13,279
memory footprint

00:15:09,360 --> 00:15:16,480
uh but uh it's it's good only if

00:15:13,279 --> 00:15:19,199
uh there are no interactions between uh

00:15:16,480 --> 00:15:22,639
the different chunks

00:15:19,199 --> 00:15:23,519
another thing is uh we can optimize the

00:15:22,639 --> 00:15:26,560
way

00:15:23,519 --> 00:15:29,600
uh specific functions

00:15:26,560 --> 00:15:33,680
are being played by min and sum

00:15:29,600 --> 00:15:36,880
so the type as we say before can be our

00:15:33,680 --> 00:15:40,560
friend so if i'm going to calculate

00:15:36,880 --> 00:15:44,000
the mean of this specific column

00:15:40,560 --> 00:15:47,279
if it's a type object it will take me

00:15:44,000 --> 00:15:51,759
96 milliseconds to be executed and

00:15:47,279 --> 00:15:54,800
will take me only 4.2 if it's a float

00:15:51,759 --> 00:15:55,680
so we gain 20 times performance

00:15:54,800 --> 00:15:59,199
improvement

00:15:55,680 --> 00:15:59,199
just by knowing the type again

00:15:59,680 --> 00:16:05,839
data frame serialization so many times

00:16:03,040 --> 00:16:07,360
when we are loading and saving the data

00:16:05,839 --> 00:16:11,199
frame

00:16:07,360 --> 00:16:14,720
we are picking the the obvious

00:16:11,199 --> 00:16:17,680
format which is csv csv is great

00:16:14,720 --> 00:16:19,120
because it's readable and it's supported

00:16:17,680 --> 00:16:22,399
by many

00:16:19,120 --> 00:16:25,360
frameworks but it

00:16:22,399 --> 00:16:26,560
does not do the best in loading time and

00:16:25,360 --> 00:16:29,920
saving time

00:16:26,560 --> 00:16:32,880
so if the bottleneck in

00:16:29,920 --> 00:16:33,839
is by the loading time and saving time

00:16:32,880 --> 00:16:37,519
you should probably

00:16:33,839 --> 00:16:38,079
use a different format in addition uh

00:16:37,519 --> 00:16:41,360
the

00:16:38,079 --> 00:16:45,199
disk space will be affected but in our

00:16:41,360 --> 00:16:50,000
age it's a bit less crucial

00:16:45,199 --> 00:16:53,759
so as you can see here

00:16:50,000 --> 00:16:58,240
share the link of this

00:16:53,759 --> 00:17:01,279
dashboard so

00:16:58,240 --> 00:17:05,600
as you can see csv take longer to

00:17:01,279 --> 00:17:08,720
save and to load as a feather or pocket

00:17:05,600 --> 00:17:11,439
feather is somewhat a

00:17:08,720 --> 00:17:12,480
aero format and you can use a pyro as

00:17:11,439 --> 00:17:16,079
well

00:17:12,480 --> 00:17:19,839
and as you can see you'll even

00:17:16,079 --> 00:17:19,839
do better

00:17:20,000 --> 00:17:27,199
the next is uh we should really

00:17:23,839 --> 00:17:30,240
use this query and eval uses num

00:17:27,199 --> 00:17:31,440
num expression under uh the would normal

00:17:30,240 --> 00:17:33,679
expression

00:17:31,440 --> 00:17:35,039
is a library to evaluate numpy

00:17:33,679 --> 00:17:37,360
expressions

00:17:35,039 --> 00:17:39,039
they can improve our execution time i'm

00:17:37,360 --> 00:17:42,320
going to show you an example

00:17:39,039 --> 00:17:45,679
in the next slide and more importantly

00:17:42,320 --> 00:17:49,520
it will improve our memory footprint so

00:17:45,679 --> 00:17:52,320
when we are executing numpy function

00:17:49,520 --> 00:17:53,440
it does some uh intermediate calculation

00:17:52,320 --> 00:17:56,640
and by that in

00:17:53,440 --> 00:17:59,760
this footprint and this uh

00:17:56,640 --> 00:18:03,600
simple library knows how to

00:17:59,760 --> 00:18:05,840
avoid doing those uh intermediate steps

00:18:03,600 --> 00:18:07,840
but obviously not all the operations are

00:18:05,840 --> 00:18:10,880
supported because it's somewhat

00:18:07,840 --> 00:18:14,400
a new library so for

00:18:10,880 --> 00:18:16,160
mostly just for numerical calculation

00:18:14,400 --> 00:18:19,360
you should use it

00:18:16,160 --> 00:18:23,200
so in this example i'm gonna

00:18:19,360 --> 00:18:26,080
uh subselect the rows

00:18:23,200 --> 00:18:27,360
that are correlated to breakfast uh

00:18:26,080 --> 00:18:30,720
invoices

00:18:27,360 --> 00:18:33,679
as you can see by using the regular

00:18:30,720 --> 00:18:34,480
methods i'm getting 100 milliseconds as

00:18:33,679 --> 00:18:38,880
opposed to

00:18:34,480 --> 00:18:38,880
80 milliseconds using the query

00:18:39,200 --> 00:18:43,679
so we gain 20 percent performance

00:18:42,240 --> 00:18:46,799
improvement just by doing

00:18:43,679 --> 00:18:49,679
that but i want to warn you that

00:18:46,799 --> 00:18:50,320
this optimization is awesome only on

00:18:49,679 --> 00:18:53,120
somewhat

00:18:50,320 --> 00:18:54,480
big data sets on small data sets you

00:18:53,120 --> 00:18:58,720
will probably

00:18:54,480 --> 00:18:58,720
get the other way around

00:18:59,880 --> 00:19:06,000
concatenate so when we want to

00:19:02,960 --> 00:19:09,600
add roses to our data frames

00:19:06,000 --> 00:19:12,880
we will probably use one of these uh

00:19:09,600 --> 00:19:13,600
and append uh create a new data frame

00:19:12,880 --> 00:19:17,200
object

00:19:13,600 --> 00:19:20,559
objects when we are okay so

00:19:17,200 --> 00:19:24,080
if we have a for loop and we are adding

00:19:20,559 --> 00:19:27,520
a lot of objects to the data frame

00:19:24,080 --> 00:19:31,520
we should probably avoid using append

00:19:27,520 --> 00:19:32,570
and we should use append on a list

00:19:31,520 --> 00:19:36,010
and then use

00:19:32,570 --> 00:19:36,010
[Music]

00:19:37,120 --> 00:19:43,200
if uh the sorting is there a

00:19:40,240 --> 00:19:44,000
problematic part you can use some

00:19:43,200 --> 00:19:47,280
different

00:19:44,000 --> 00:19:48,400
uh sorting mechanism we have pandas

00:19:47,280 --> 00:19:51,840
numpy

00:19:48,400 --> 00:19:53,120
and pytorch and tensorflow basically i

00:19:51,840 --> 00:19:56,960
advise you

00:19:53,120 --> 00:20:00,080
to use the panda sorting we can use

00:19:56,960 --> 00:20:03,919
the kind the kind parameter

00:20:00,080 --> 00:20:06,159
to tell which algorithm to use

00:20:03,919 --> 00:20:07,120
the default is quicksort but we have

00:20:06,159 --> 00:20:10,480
meltshot

00:20:07,120 --> 00:20:14,320
and ipsort in addition if you are using

00:20:10,480 --> 00:20:17,520
a gpu machine you should probably use

00:20:14,320 --> 00:20:21,200
either tensorflow or by torch

00:20:17,520 --> 00:20:24,400
for the sorting

00:20:21,200 --> 00:20:28,159
group by our very

00:20:24,400 --> 00:20:30,480
cpu-heavy calculations

00:20:28,159 --> 00:20:33,600
and there are some easy techniques that

00:20:30,480 --> 00:20:36,480
you can use to reduce the execution time

00:20:33,600 --> 00:20:37,200
so the first one is as we said we should

00:20:36,480 --> 00:20:40,000
filter

00:20:37,200 --> 00:20:40,720
as early as we as we can and by doing

00:20:40,000 --> 00:20:44,320
that

00:20:40,720 --> 00:20:48,159
we are iterating on less a unless

00:20:44,320 --> 00:20:50,880
data in addition custom uh

00:20:48,159 --> 00:20:51,440
functions are very slow because there

00:20:50,880 --> 00:20:53,840
are

00:20:51,440 --> 00:20:54,799
some of the optimizations that are not

00:20:53,840 --> 00:20:58,000
done

00:20:54,799 --> 00:21:01,360
in the in the sea level of pandas

00:20:58,000 --> 00:21:02,240
and numpy so probably uh if it's

00:21:01,360 --> 00:21:05,360
possible

00:21:02,240 --> 00:21:06,320
you should extract out of the custom

00:21:05,360 --> 00:21:10,000
functions

00:21:06,320 --> 00:21:12,400
to either vectorize methods or other

00:21:10,000 --> 00:21:13,520
techniques and keep your custom

00:21:12,400 --> 00:21:17,840
functions

00:21:13,520 --> 00:21:17,840
as small as possible

00:21:18,640 --> 00:21:24,960
additional cpu

00:21:21,919 --> 00:21:25,919
bound operation is emerge we should

00:21:24,960 --> 00:21:29,520
again

00:21:25,919 --> 00:21:33,919
filter or aggregate early in order to

00:21:29,520 --> 00:21:33,919
reduce the size of that asset

00:21:33,960 --> 00:21:41,200
sometimes it will be

00:21:37,120 --> 00:21:44,880
more permanent to first

00:21:41,200 --> 00:21:48,159
a filter the data flame

00:21:44,880 --> 00:21:50,480
records by using where in

00:21:48,159 --> 00:21:51,679
technique instead of using the merge

00:21:50,480 --> 00:21:54,799
before

00:21:51,679 --> 00:21:58,480
this is best if uh you know that

00:21:54,799 --> 00:22:01,520
many records gonna disappear

00:21:58,480 --> 00:22:03,520
uh using an inner join and

00:22:01,520 --> 00:22:06,880
another technique that can help you is

00:22:03,520 --> 00:22:10,720
to join on the index

00:22:06,880 --> 00:22:14,400
so that's about uh how to use the

00:22:10,720 --> 00:22:15,120
bundles techniques now about compile

00:22:14,400 --> 00:22:16,960
code

00:22:15,120 --> 00:22:18,480
why should we even care about compiled

00:22:16,960 --> 00:22:22,320
code we are in the

00:22:18,480 --> 00:22:23,360
python so python by default has dynamic

00:22:22,320 --> 00:22:25,520
nature

00:22:23,360 --> 00:22:27,039
and we don't specify the types

00:22:25,520 --> 00:22:32,240
beforehand

00:22:27,039 --> 00:22:36,559
which make some compilation optimization

00:22:32,240 --> 00:22:39,679
missing for us so for some operations

00:22:36,559 --> 00:22:42,480
pure python can be slow

00:22:39,679 --> 00:22:43,120
i will show you an example i'm doing a

00:22:42,480 --> 00:22:46,400
full

00:22:43,120 --> 00:22:49,840
function which basically accumulate

00:22:46,400 --> 00:22:53,200
the number and until i get to n

00:22:49,840 --> 00:22:56,240
and leave it so this take

00:22:53,200 --> 00:22:59,840
80 seconds to be executed

00:22:56,240 --> 00:23:03,320
so for that we have site on a number

00:22:59,840 --> 00:23:06,720
which provide us compile

00:23:03,320 --> 00:23:10,640
optimizations for our python code

00:23:06,720 --> 00:23:14,559
and i'm gonna start with cyton

00:23:10,640 --> 00:23:17,760
so cyto can give us up to 50

00:23:14,559 --> 00:23:21,679
times speed up from pure python

00:23:17,760 --> 00:23:24,960
it has steep learning curve because

00:23:21,679 --> 00:23:28,080
it's adding uh it's between c and

00:23:24,960 --> 00:23:31,760
python and c is not the easiest to learn

00:23:28,080 --> 00:23:34,480
uh when we integrate sight on in

00:23:31,760 --> 00:23:35,200
into our project we need uh to compile

00:23:34,480 --> 00:23:38,240
the code

00:23:35,200 --> 00:23:41,760
the pi way x and then to add it

00:23:38,240 --> 00:23:44,400
uh with a setup in the setup ui which

00:23:41,760 --> 00:23:45,679
is a bit hard to integrate but the good

00:23:44,400 --> 00:23:48,880
thing is that

00:23:45,679 --> 00:23:52,320
the calculation of the compilation

00:23:48,880 --> 00:23:53,600
be a call beforehand and i will show an

00:23:52,320 --> 00:23:57,120
example for

00:23:53,600 --> 00:23:58,080
our methods so as you can see i'm just

00:23:57,120 --> 00:24:01,120
adding

00:23:58,080 --> 00:24:04,480
uh the type for more a

00:24:01,120 --> 00:24:04,880
complicated function it will be harder

00:24:04,480 --> 00:24:08,320
to

00:24:04,880 --> 00:24:09,440
understand and as you can see just by

00:24:08,320 --> 00:24:13,440
doing that

00:24:09,440 --> 00:24:15,919
we are getting to 360 milliseconds

00:24:13,440 --> 00:24:16,559
which is 49 times performance

00:24:15,919 --> 00:24:20,400
improvement

00:24:16,559 --> 00:24:23,360
over the pure python code

00:24:20,400 --> 00:24:24,159
we have a number in addition so number

00:24:23,360 --> 00:24:26,960
help us

00:24:24,159 --> 00:24:28,240
to compile the code using the llvm

00:24:26,960 --> 00:24:31,520
compiler

00:24:28,240 --> 00:24:34,400
so it can give us up to 20 times

00:24:31,520 --> 00:24:36,159
speed improvement although pure python

00:24:34,400 --> 00:24:36,960
it's really easy as as you will see

00:24:36,159 --> 00:24:40,159
later

00:24:36,960 --> 00:24:41,679
it's just adding a decorator to our

00:24:40,159 --> 00:24:44,240
function

00:24:41,679 --> 00:24:45,279
it's highly configurable if we want to

00:24:44,240 --> 00:24:48,400
make the function

00:24:45,279 --> 00:24:49,039
parallel we can simply add a parameter

00:24:48,400 --> 00:24:52,080
for it

00:24:49,039 --> 00:24:55,760
and it will uh do all the guild stuff

00:24:52,080 --> 00:24:56,080
for you and debugging can be easy for

00:24:55,760 --> 00:24:58,559
the

00:24:56,080 --> 00:24:59,360
python part but if you have issues in

00:24:58,559 --> 00:25:05,279
the

00:24:59,360 --> 00:25:05,279
compiled by code it won't be fun for you

00:25:05,360 --> 00:25:11,760
it it is a newer project than psyton

00:25:09,520 --> 00:25:13,039
and thus it supposed it should be

00:25:11,760 --> 00:25:15,840
support

00:25:13,039 --> 00:25:16,640
mostly numerical calculations and a

00:25:15,840 --> 00:25:19,840
little bit

00:25:16,640 --> 00:25:23,039
of of string like

00:25:19,840 --> 00:25:24,799
operations so in our example the only

00:25:23,039 --> 00:25:28,480
thing that i needed to do

00:25:24,799 --> 00:25:31,520
is to add this decorator that's saying

00:25:28,480 --> 00:25:34,880
that it's just in time compiler uh

00:25:31,520 --> 00:25:36,720
if i want it to uh to be parallel i

00:25:34,880 --> 00:25:40,159
simply add

00:25:36,720 --> 00:25:45,200
comma parallel equal to and that's it

00:25:40,159 --> 00:25:48,480
just by doing that i'm getting 440

00:25:45,200 --> 00:25:51,120
milliseconds which is a 43 times

00:25:48,480 --> 00:25:52,799
performance improvement for more

00:25:51,120 --> 00:25:56,960
complicated functions

00:25:52,799 --> 00:26:00,960
it might uh be faster than um

00:25:56,960 --> 00:26:02,400
than psyton and we can create uh

00:26:00,960 --> 00:26:06,240
vectorized methods

00:26:02,400 --> 00:26:08,480
using a number as well so

00:26:06,240 --> 00:26:11,679
if you ask me you should first try to

00:26:08,480 --> 00:26:14,960
use the existing vectorized methods

00:26:11,679 --> 00:26:18,159
of pandas sci-fi and number

00:26:14,960 --> 00:26:21,840
e and a pandas

00:26:18,159 --> 00:26:25,039
numpy and scipy and then number because

00:26:21,840 --> 00:26:28,159
it's easier and only if those doesn't

00:26:25,039 --> 00:26:30,760
help you you should use cyto

00:26:28,159 --> 00:26:32,000
we can benefit from general python

00:26:30,760 --> 00:26:35,760
optimizations

00:26:32,000 --> 00:26:36,720
as well as well in the in the python

00:26:35,760 --> 00:26:40,240
ecosystem

00:26:36,720 --> 00:26:43,360
so we have caching we can

00:26:40,240 --> 00:26:47,040
use caching to avoid unnecessary wall

00:26:43,360 --> 00:26:48,880
and computation we can even

00:26:47,040 --> 00:26:52,000
[Music]

00:26:48,880 --> 00:26:55,120
this will make our code run faster if we

00:26:52,000 --> 00:26:56,159
can we can use similar techniques to

00:26:55,120 --> 00:26:59,600
make

00:26:56,159 --> 00:27:00,799
our calculations on in an increment like

00:26:59,600 --> 00:27:04,000
manual

00:27:00,799 --> 00:27:07,440
in addition we can save

00:27:04,000 --> 00:27:10,240
memory footprint by using

00:27:07,440 --> 00:27:10,559
intermediate variables in a smarter way

00:27:10,240 --> 00:27:12,080
so

00:27:10,559 --> 00:27:13,919
when we are doing intermediate

00:27:12,080 --> 00:27:17,039
calculation

00:27:13,919 --> 00:27:20,480
we we will have the memory footprint

00:27:17,039 --> 00:27:22,880
of the two objects and

00:27:20,480 --> 00:27:23,760
just by you doing smarter valuable

00:27:22,880 --> 00:27:26,960
allocation

00:27:23,760 --> 00:27:30,159
we can save a bit of memory so

00:27:26,960 --> 00:27:34,399
an example for that i'm gonna use the

00:27:30,159 --> 00:27:37,600
mimic module and as you can see

00:27:34,399 --> 00:27:40,960
the big memory is eight

00:27:37,600 --> 00:27:45,279
gigabyte i'm allocating both the

00:27:40,960 --> 00:27:49,120
data variable and another variable here

00:27:45,279 --> 00:27:54,240
and if i would just override this data

00:27:49,120 --> 00:27:58,080
variable i will only get 7 gigabytes

00:27:54,240 --> 00:28:00,720
but again if it's not your battle neck

00:27:58,080 --> 00:28:01,520
i wouldn't play with it because we

00:28:00,720 --> 00:28:05,760
should

00:28:01,520 --> 00:28:05,760
first optimize for readability

00:28:06,080 --> 00:28:12,559
uh another thing that we can benefit is

00:28:09,200 --> 00:28:16,159
concurrency and parabolism so pandas

00:28:12,559 --> 00:28:17,360
run on a single process and as you can

00:28:16,159 --> 00:28:21,039
understand

00:28:17,360 --> 00:28:24,480
for a cpu bound a

00:28:21,039 --> 00:28:27,919
program we can benefit from

00:28:24,480 --> 00:28:30,799
parallelism and for io bound we can

00:28:27,919 --> 00:28:31,440
benefit from parallelism and concurrency

00:28:30,799 --> 00:28:34,799
the reason

00:28:31,440 --> 00:28:38,080
i put it this far in the

00:28:34,799 --> 00:28:40,799
in in my slide is because you will gain

00:28:38,080 --> 00:28:42,720
probably four times the improvement and

00:28:40,799 --> 00:28:45,919
it will make your code

00:28:42,720 --> 00:28:48,880
much much harder to maintain

00:28:45,919 --> 00:28:51,279
but if if it's the bottleneck and it

00:28:48,880 --> 00:28:53,919
affect your user you should probably

00:28:51,279 --> 00:28:55,600
use it and there are many more

00:28:53,919 --> 00:28:58,480
techniques for

00:28:55,600 --> 00:28:59,279
python optimizations there is an amazing

00:28:58,480 --> 00:29:02,640
book by

00:28:59,279 --> 00:29:04,880
mika and ian

00:29:02,640 --> 00:29:06,640
and if all of these does not suffice for

00:29:04,880 --> 00:29:09,760
you you'll probably

00:29:06,640 --> 00:29:10,720
should use a different library for data

00:29:09,760 --> 00:29:13,679
frames

00:29:10,720 --> 00:29:14,480
so you have a modein which is available

00:29:13,679 --> 00:29:17,919
for dusk

00:29:14,480 --> 00:29:21,679
and a way we have qdf which is

00:29:17,919 --> 00:29:24,640
pandas for a gpu

00:29:21,679 --> 00:29:25,840
for jpo applications and we have

00:29:24,640 --> 00:29:28,799
[Music]

00:29:25,840 --> 00:29:30,159
but obviously there is no free lunch

00:29:28,799 --> 00:29:33,360
every one of these

00:29:30,159 --> 00:29:34,399
uh and every every one of these

00:29:33,360 --> 00:29:37,919
framework has

00:29:34,399 --> 00:29:42,320
his own limitations and and

00:29:37,919 --> 00:29:45,360
issues for example qdf which is gpu

00:29:42,320 --> 00:29:47,600
can be slower on some uh on some

00:29:45,360 --> 00:29:51,279
operations like vlna

00:29:47,600 --> 00:29:51,840
and uh obviously installing it it's much

00:29:51,279 --> 00:29:55,120
harder

00:29:51,840 --> 00:29:58,399
and it's much less featureful so

00:29:55,120 --> 00:30:00,240
when you are picking your own

00:29:58,399 --> 00:30:01,679
framework you should really thrive to

00:30:00,240 --> 00:30:06,399
use

00:30:01,679 --> 00:30:09,919
the one that best fit your use case

00:30:06,399 --> 00:30:13,279
so let's go over uh the techniques that

00:30:09,919 --> 00:30:13,760
i uh listed we should only use what we

00:30:13,279 --> 00:30:16,640
need

00:30:13,760 --> 00:30:18,480
we shouldn't use uh columns or walls

00:30:16,640 --> 00:30:19,679
that we are not going to use in our data

00:30:18,480 --> 00:30:21,760
manipulation

00:30:19,679 --> 00:30:23,120
we shouldn't reinvent the will there are

00:30:21,760 --> 00:30:26,159
many smart people

00:30:23,120 --> 00:30:29,760
that already implemented uh

00:30:26,159 --> 00:30:33,360
most of the common algorithms

00:30:29,760 --> 00:30:36,399
uh we should avoid loops

00:30:33,360 --> 00:30:39,039
and use vectorized methods

00:30:36,399 --> 00:30:41,919
we should pick the right types for our

00:30:39,039 --> 00:30:45,120
pandas data frame to reduce the memory

00:30:41,919 --> 00:30:49,600
we should know which methods

00:30:45,120 --> 00:30:52,559
to use for our pandas code to reduce

00:30:49,600 --> 00:30:53,200
to reduce the execution time we can

00:30:52,559 --> 00:30:57,039
benefit

00:30:53,200 --> 00:30:59,679
from compiled code as well

00:30:57,039 --> 00:31:00,960
there are general python optimizations

00:30:59,679 --> 00:31:03,679
that we can do to

00:31:00,960 --> 00:31:04,240
achieve better performance i think that

00:31:03,679 --> 00:31:07,679
uh

00:31:04,240 --> 00:31:11,360
next the the next talk in this uh

00:31:07,679 --> 00:31:14,640
in this track uh speak about it

00:31:11,360 --> 00:31:15,360
and if nothing suffice for you maybe you

00:31:14,640 --> 00:31:18,480
should use

00:31:15,360 --> 00:31:21,760
a different framework so

00:31:18,480 --> 00:31:25,039
i've listed the additional resources

00:31:21,760 --> 00:31:28,240
if it's interest you uh there are

00:31:25,039 --> 00:31:32,559
workshops on number one and

00:31:28,240 --> 00:31:35,919
more and the best suggestion for me

00:31:32,559 --> 00:31:39,200
is to see the vectorization mindset

00:31:35,919 --> 00:31:42,320
uh talk which can make your life

00:31:39,200 --> 00:31:45,519
uh like tremendously

00:31:42,320 --> 00:31:48,799
better like if your code execute

00:31:45,519 --> 00:31:52,399
in uh ten thousands

00:31:48,799 --> 00:31:54,480
times less you can probably

00:31:52,399 --> 00:31:57,440
you shouldn't wait anymore for your code

00:31:54,480 --> 00:31:57,440
to be executed

00:31:57,840 --> 00:32:04,399
and that's it if you have any questions

00:32:01,600 --> 00:32:06,960
uh feel free to ask me now or later on

00:32:04,399 --> 00:32:10,640
uh on discord or in linkedin

00:32:06,960 --> 00:32:14,640
uh i i hope you enjoyed the talk uh

00:32:10,640 --> 00:32:17,679
that's it thank you hey i think we have

00:32:14,640 --> 00:32:19,440
time to take questions so would you want

00:32:17,679 --> 00:32:23,120
to take them now

00:32:19,440 --> 00:32:25,440
sure sure all right uh okay so

00:32:23,120 --> 00:32:26,480
the first question is from pascal it

00:32:25,440 --> 00:32:28,880
seems like map

00:32:26,480 --> 00:32:29,760
and apply function in pandas serve the

00:32:28,880 --> 00:32:32,399
same purpose

00:32:29,760 --> 00:32:34,000
maybe you can give a recommendation as

00:32:32,399 --> 00:32:38,000
to when to use one

00:32:34,000 --> 00:32:42,320
over the other so they are familiar

00:32:38,000 --> 00:32:45,600
the difference is uh that applies run

00:32:42,320 --> 00:32:49,279
on uh either the entire row

00:32:45,600 --> 00:32:51,039
or the entire column so a map i would

00:32:49,279 --> 00:32:55,200
use if i have a function

00:32:51,039 --> 00:32:58,640
that is executed only on one column

00:32:55,200 --> 00:33:02,640
and apply i will use if i have a

00:32:58,640 --> 00:33:05,519
function that is executed on two columns

00:33:02,640 --> 00:33:06,399
uh in addition it's important to note

00:33:05,519 --> 00:33:09,840
that

00:33:06,399 --> 00:33:12,880
when we are using uh and when we are

00:33:09,840 --> 00:33:16,320
using map and apply we should

00:33:12,880 --> 00:33:18,720
try not to use vectorized

00:33:16,320 --> 00:33:21,120
methods because they are optimized for

00:33:18,720 --> 00:33:25,360
the entire array

00:33:21,120 --> 00:33:27,840
instead of only specific value

00:33:25,360 --> 00:33:29,679
all right awesome so the next question

00:33:27,840 --> 00:33:33,519
is from francesco

00:33:29,679 --> 00:33:36,000
why not using just a sql database

00:33:33,519 --> 00:33:38,880
instead if performance is critical and

00:33:36,000 --> 00:33:41,919
the size is large

00:33:38,880 --> 00:33:45,279
so it really depends on the

00:33:41,919 --> 00:33:48,159
sql database basically sql

00:33:45,279 --> 00:33:49,760
databases has a lot of optimizations in

00:33:48,159 --> 00:33:53,200
it

00:33:49,760 --> 00:33:54,080
but but there are many issues with it as

00:33:53,200 --> 00:33:56,880
well

00:33:54,080 --> 00:33:57,519
by that that we are using pandas and

00:33:56,880 --> 00:34:00,720
it's in

00:33:57,519 --> 00:34:01,840
memory uh in in memory it's been

00:34:00,720 --> 00:34:04,720
calculated

00:34:01,840 --> 00:34:06,640
much much faster than you can do it with

00:34:04,720 --> 00:34:09,839
a disk just reading

00:34:06,640 --> 00:34:10,720
disk uh data will take you longer than

00:34:09,839 --> 00:34:14,000
calculating

00:34:10,720 --> 00:34:17,599
uh using vectorized methods so for

00:34:14,000 --> 00:34:20,240
huge uh huge data set probably pandas

00:34:17,599 --> 00:34:22,879
won't suffice for you unless you can do

00:34:20,240 --> 00:34:23,280
chunking and then you should really

00:34:22,879 --> 00:34:27,200
think

00:34:23,280 --> 00:34:29,839
if a sql database is best fit for you

00:34:27,200 --> 00:34:30,399
or you should go to use some frameworks

00:34:29,839 --> 00:34:35,359
like

00:34:30,399 --> 00:34:38,159
spark all right so the next question is

00:34:35,359 --> 00:34:38,720
what tools uh do you use to profile your

00:34:38,159 --> 00:34:40,560
code

00:34:38,720 --> 00:34:41,919
to find the bottlenecks this is from

00:34:40,560 --> 00:34:45,040
james

00:34:41,919 --> 00:34:48,320
okay so i didn't cover it uh in

00:34:45,040 --> 00:34:51,359
in much so i will try to give you

00:34:48,320 --> 00:34:54,480
a brief explanation

00:34:51,359 --> 00:34:58,000
so we can use a c profile

00:34:54,480 --> 00:35:02,000
to uh profile our cpu uh

00:34:58,000 --> 00:35:05,280
our cpu part of the code

00:35:02,000 --> 00:35:08,400
uh the bad part uh using it

00:35:05,280 --> 00:35:11,760
is that the profiling itself will make

00:35:08,400 --> 00:35:15,599
uh the code much slower like 100 times

00:35:11,760 --> 00:35:17,760
lower uh so for if the code is slow

00:35:15,599 --> 00:35:20,000
from the beginning we should use a

00:35:17,760 --> 00:35:23,440
static statistical

00:35:20,000 --> 00:35:27,520
uh profiler instead

00:35:23,440 --> 00:35:30,480
and in order to visualize the

00:35:27,520 --> 00:35:30,960
the output of the profiler itself i'm

00:35:30,480 --> 00:35:34,400
using

00:35:30,960 --> 00:35:37,839
a snakevis i will share a link

00:35:34,400 --> 00:35:41,760
later on and for memory i'm using

00:35:37,839 --> 00:35:45,040
the in the fill

00:35:41,760 --> 00:35:48,640
fill profiler or the mammoth

00:35:45,040 --> 00:35:48,640
cell magic that i just showed

00:35:50,000 --> 00:35:54,480
okay so the next question okay so this

00:35:53,200 --> 00:35:57,359
is not a question but

00:35:54,480 --> 00:35:59,359
simon says dusk is a good alternative

00:35:57,359 --> 00:36:03,680
for big data frames

00:35:59,359 --> 00:36:06,960
so and so yeah so i covered it in a bit

00:36:03,680 --> 00:36:10,640
so dusk is is amazing it's pretty mature

00:36:06,960 --> 00:36:14,800
uh and i briefly touched on it modin

00:36:10,640 --> 00:36:17,440
is a library that wraps the pandas api

00:36:14,800 --> 00:36:18,160
and basically there are engines inside

00:36:17,440 --> 00:36:20,640
it

00:36:18,160 --> 00:36:21,920
so one of those is dusk and the other

00:36:20,640 --> 00:36:24,880
one is ray

00:36:21,920 --> 00:36:26,079
uh so i have only good things to say

00:36:24,880 --> 00:36:28,800
about uh

00:36:26,079 --> 00:36:30,320
dusk but the good thing about molding

00:36:28,800 --> 00:36:34,160
and using it instead of

00:36:30,320 --> 00:36:37,440
uh a raw dusk is that if dusk

00:36:34,160 --> 00:36:40,640
doesn't support one of the

00:36:37,440 --> 00:36:46,000
one of the features of pandas uh

00:36:40,640 --> 00:36:49,599
it will default to uh the pandas the

00:36:46,000 --> 00:36:54,400
pandas operation so basically

00:36:49,599 --> 00:36:56,720
it should be just uh you can

00:36:54,400 --> 00:36:57,440
replace the framework in a much easier

00:36:56,720 --> 00:37:00,880
way

00:36:57,440 --> 00:37:00,880
than using the dusk

00:37:01,520 --> 00:37:06,960
okay and also peter asked if there is a

00:37:04,880 --> 00:37:08,880
downloadable version of your slides in a

00:37:06,960 --> 00:37:11,440
pdf format anyway

00:37:08,880 --> 00:37:12,160
i think it should be the talk right yeah

00:37:11,440 --> 00:37:15,200
yeah so

00:37:12,160 --> 00:37:18,960
i i think it's in html form but i

00:37:15,200 --> 00:37:22,320
can i can add a pdf format

00:37:18,960 --> 00:37:26,000
as well all right so

00:37:22,320 --> 00:37:26,560
daniel asks you what are the limitations

00:37:26,000 --> 00:37:29,839
of

00:37:26,560 --> 00:37:32,400
modern so

00:37:29,839 --> 00:37:33,040
uh there are many it's it's really

00:37:32,400 --> 00:37:36,480
depends

00:37:33,040 --> 00:37:39,760
on uh on two on two things

00:37:36,480 --> 00:37:43,040
uh which which uh

00:37:39,760 --> 00:37:46,160
which engine do you use

00:37:43,040 --> 00:37:47,359
uh whether dusk or array and then you

00:37:46,160 --> 00:37:50,000
have uh

00:37:47,359 --> 00:37:50,640
the limitation of that engine and in

00:37:50,000 --> 00:37:54,400
addition

00:37:50,640 --> 00:37:57,680
it's a pretty new uh library so

00:37:54,400 --> 00:37:58,079
in my personal use case uh sometimes

00:37:57,680 --> 00:38:01,200
it's

00:37:58,079 --> 00:38:03,599
just a fail for no reason

00:38:01,200 --> 00:38:04,560
but they are working on it to make it

00:38:03,599 --> 00:38:07,280
more stable

00:38:04,560 --> 00:38:07,920
and work better as you as you can

00:38:07,280 --> 00:38:11,040
imagine

00:38:07,920 --> 00:38:14,079
uh dusk and and a ray

00:38:11,040 --> 00:38:17,440
uh uses multiple processing

00:38:14,079 --> 00:38:20,880
techniques and if you have like 10 or

00:38:17,440 --> 00:38:24,079
100 machines you will get

00:38:20,880 --> 00:38:28,240
at most 10 times a performance

00:38:24,079 --> 00:38:30,880
improvement and as you can as you saw

00:38:28,240 --> 00:38:31,280
by using vectorized methods you can get

00:38:30,880 --> 00:38:34,000
to

00:38:31,280 --> 00:38:35,839
almost 1 million times performance

00:38:34,000 --> 00:38:38,320
improvements

00:38:35,839 --> 00:38:39,280
so obviously i would try to use those

00:38:38,320 --> 00:38:44,320
only if

00:38:39,280 --> 00:38:46,720
pandas doesn't fit my use case

00:38:44,320 --> 00:38:47,680
awesome so the next question is from

00:38:46,720 --> 00:38:51,040
krishna

00:38:47,680 --> 00:38:51,839
uh at a production level deployment

00:38:51,040 --> 00:38:54,800
which would be

00:38:51,839 --> 00:38:55,520
better pandas or pi spark does pandas

00:38:54,800 --> 00:38:57,760
have

00:38:55,520 --> 00:39:00,880
implementation for distributing it

00:38:57,760 --> 00:39:04,000
across multiple clusters

00:39:00,880 --> 00:39:07,839
so uh the

00:39:04,000 --> 00:39:12,000
the solution for distributing uh pandas

00:39:07,839 --> 00:39:15,599
across different systems is dusk

00:39:12,000 --> 00:39:18,480
and i think pandas and uh and the spark

00:39:15,599 --> 00:39:19,040
are a different use case so pandas is

00:39:18,480 --> 00:39:22,960
for

00:39:19,040 --> 00:39:26,000
medium data when you want really uh

00:39:22,960 --> 00:39:27,599
effective effective uh data

00:39:26,000 --> 00:39:30,079
transformation

00:39:27,599 --> 00:39:30,960
because you load all the data into

00:39:30,079 --> 00:39:33,760
memory

00:39:30,960 --> 00:39:34,800
and spark is great when you have really

00:39:33,760 --> 00:39:38,000
huge

00:39:34,800 --> 00:39:39,680
data sets so if your data is probably

00:39:38,000 --> 00:39:43,280
bigger than

00:39:39,680 --> 00:39:44,400
20 gigabytes or 50 gigabytes or 100

00:39:43,280 --> 00:39:46,960
gigabytes

00:39:44,400 --> 00:39:47,760
uh probably pandas will not suffice for

00:39:46,960 --> 00:39:49,839
you

00:39:47,760 --> 00:39:51,119
uh but then you will have different

00:39:49,839 --> 00:39:53,680
problems because uh

00:39:51,119 --> 00:39:54,880
most of the machine learning uh

00:39:53,680 --> 00:39:59,040
algorithms

00:39:54,880 --> 00:40:04,160
are supported in a single

00:39:59,040 --> 00:40:06,960
single machine and spark

00:40:04,160 --> 00:40:10,640
and spark machine learning is less

00:40:06,960 --> 00:40:13,680
featureful to say the least

00:40:10,640 --> 00:40:17,920
okay so the next question is from mark

00:40:13,680 --> 00:40:21,680
did pie pie enhance panda's numpy usage

00:40:17,920 --> 00:40:21,680
through vanilla python usage

00:40:22,880 --> 00:40:26,560
can you repeat the question okay uh did

00:40:25,680 --> 00:40:29,119
pie pie

00:40:26,560 --> 00:40:31,520
enhance panda's numpy usage through

00:40:29,119 --> 00:40:34,880
vanilla python usage

00:40:31,520 --> 00:40:37,440
so i didn't use a pipeline to say

00:40:34,880 --> 00:40:38,960
to be honest uh maybe maybe it does

00:40:37,440 --> 00:40:41,359
maybe it doesn't

00:40:38,960 --> 00:40:43,680
so i don't want to i don't know

00:40:41,359 --> 00:40:47,040
basically i don't know

00:40:43,680 --> 00:40:50,560
okay awesome so deepak asks

00:40:47,040 --> 00:40:51,760
sometimes we want to use sn for group i

00:40:50,560 --> 00:40:54,319
dot get group

00:40:51,760 --> 00:40:57,040
do you have any suggestions on which one

00:40:54,319 --> 00:41:00,400
to use which is better

00:40:57,040 --> 00:41:01,760
um can you write it in the queue i want

00:41:00,400 --> 00:41:04,800
to see the

00:41:01,760 --> 00:41:06,640
oh okay so uh if if the doubts are going

00:41:04,800 --> 00:41:09,599
to be more

00:41:06,640 --> 00:41:10,160
extensive or it requires any discussion

00:41:09,599 --> 00:41:13,200
you can

00:41:10,160 --> 00:41:14,000
reach out to the speaker on the breakout

00:41:13,200 --> 00:41:16,240
channel

00:41:14,000 --> 00:41:17,040
which is entirely just dedicated to

00:41:16,240 --> 00:41:20,240
talking about

00:41:17,040 --> 00:41:22,240
this presentation yeah we can take

00:41:20,240 --> 00:41:24,000
stuff there too all right so the next

00:41:22,240 --> 00:41:24,640
question i think this will be the last

00:41:24,000 --> 00:41:27,680
one

00:41:24,640 --> 00:41:30,800
any way to clear all objects at the

00:41:27,680 --> 00:41:34,079
end of the code like a clear cache

00:41:30,800 --> 00:41:34,800
uh so you can call the garbage

00:41:34,079 --> 00:41:38,240
collection

00:41:34,800 --> 00:41:42,240
of python yourself

00:41:38,240 --> 00:41:45,599
but i i specifically

00:41:42,240 --> 00:41:46,640
don't do it because it will make the

00:41:45,599 --> 00:41:49,680
code

00:41:46,640 --> 00:41:50,250
less maintainable and readable probably

00:41:49,680 --> 00:41:53,280
uh

00:41:50,250 --> 00:41:55,839
[Music]

00:41:53,280 --> 00:41:56,800
for you to understand which part of the

00:41:55,839 --> 00:41:59,760
code

00:41:56,800 --> 00:42:00,800
does take the most uh the most memorable

00:41:59,760 --> 00:42:04,400
footprint

00:42:00,800 --> 00:42:07,520
and just uh adjust the types or split it

00:42:04,400 --> 00:42:10,960
to chunks so i i i worked with

00:42:07,520 --> 00:42:14,079
a lot of memory bound

00:42:10,960 --> 00:42:17,599
programs and i never specifically had

00:42:14,079 --> 00:42:20,160
uh to invoke the garbage collection

00:42:17,599 --> 00:42:20,160
on my own

00:42:28,839 --> 00:42:31,839
okay

00:42:31,920 --> 00:42:37,839
do you hear me yeah yeah sorry uh what

00:42:35,119 --> 00:42:38,160
software did you use to make your slides

00:42:37,839 --> 00:42:41,280
it's

00:42:38,160 --> 00:42:45,359
right right uh so so i

00:42:41,280 --> 00:42:50,000
i've used the jupiter notebook

00:42:45,359 --> 00:42:53,040
jupiter notebook has a reveal js

00:42:50,000 --> 00:42:57,280
plugin which allows you to

00:42:53,040 --> 00:43:00,640
in a simple click to create the

00:42:57,280 --> 00:43:02,800
slides in addition there is other nice

00:43:00,640 --> 00:43:04,960
benefits to it the code can be

00:43:02,800 --> 00:43:09,040
executable as well if i wanted

00:43:04,960 --> 00:43:09,040
it to be interactive slide

00:43:09,119 --> 00:43:15,839
and turn uh you can transform the

00:43:13,680 --> 00:43:18,240
notebook to a medium post quite easily

00:43:15,839 --> 00:43:18,240
as well

00:43:21,119 --> 00:43:27,680
isn't that the package called rice

00:43:24,240 --> 00:43:31,280
so it's the package called

00:43:27,680 --> 00:43:34,880
rice and basically it's uses reveal js

00:43:31,280 --> 00:43:38,400
behind the scene um and and

00:43:34,880 --> 00:43:42,319
if you install a jupiter

00:43:38,400 --> 00:43:45,440
via anaconda you are getting it for free

00:43:42,319 --> 00:43:49,200
you can simply uh click the

00:43:45,440 --> 00:43:52,720
file and then you have uh export

00:43:49,200 --> 00:43:53,200
section to which uh to which a format

00:43:52,720 --> 00:43:56,319
you want

00:43:53,200 --> 00:43:59,440
the notebook to convert whether to

00:43:56,319 --> 00:44:03,760
a reveal.js slideshare to pdf

00:43:59,440 --> 00:44:06,480
or to regular python script

00:44:03,760 --> 00:44:08,800
all right could you unshare your screen

00:44:06,480 --> 00:44:13,680
for just a minute

00:44:08,800 --> 00:44:13,680
and yeah one second

00:44:20,839 --> 00:44:23,839
yep

00:44:24,730 --> 00:44:27,889
[Music]

00:44:30,319 --> 00:44:35,200
oh did you stop sharing your screen oh

00:44:32,880 --> 00:44:51,839
we can still see it

00:44:35,200 --> 00:44:51,839
did you want me to stop sharing yes okay

00:45:03,760 --> 00:45:07,200
oh yeah uh do you have any problems

00:45:05,520 --> 00:45:09,040
doing it it's fine

00:45:07,200 --> 00:45:12,000
i just wanted to play the applause

00:45:09,040 --> 00:45:12,000
soundtrack for you

00:45:12,400 --> 00:45:16,640
awesome okay so i think we have more

00:45:15,680 --> 00:45:19,599
questions but

00:45:16,640 --> 00:45:20,240
uh i think it's lunch break now so we

00:45:19,599 --> 00:45:21,839
can take

00:45:20,240 --> 00:45:23,760
the rest of the questions to the break

00:45:21,839 --> 00:45:27,040
room okay

00:45:23,760 --> 00:45:30,839
thank you all right see you

00:45:27,040 --> 00:45:33,839
see you all after lunch

00:45:30,839 --> 00:45:33,839

YouTube URL: https://www.youtube.com/watch?v=RXMiMpMfzXw


