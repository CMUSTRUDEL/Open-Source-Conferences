Title: Chin Hwee Ong - Speed Up Your Data Processing
Publication date: 2020-09-21
Playlist: EuroPython 2020
Description: 
	"Speed Up Your Data Processing
EuroPython 2020 - Talk - 2020-07-23 - Parrot Data Science
Online

By Chin Hwee Ong

In a data science project, one of the biggest bottlenecks (in terms of time) is the constant wait for the data processing code to finish executing. Slow code, as well as connectivity issues, affect every step of a typical data science workflow — be it for network I/O operations or computation-driven workloads. In this talk, I will be sharing about common bottlenecks in data processing within a typical data science workflow, and exploring the use of parallel and asynchronous programming using concurrent.futures module in Python to speed up your data processing pipelines so that you could focus more on getting value out of your data. Through real-life analogies, you will learn about:

ol
Sequential vs parallel processing,
Synchronous vs asynchronous execution,
Network I/O operations vs computation-driven workloads in a data science workflow,
When is parallelism and asynchronous programming a good idea,
How to implement parallel and asynchronous programming using concurrent.futures module to speed up your data processing pipelines
/ol

This talk assumes basic understanding of data pipelines and data science workflows. While the main target audience are data scientists and engineers building data pipelines, the talk is designed such that anyone with a basic understanding of the Python language would be able to understand the illustrated concepts and use cases.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2020.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:06,240 --> 00:00:12,080
our first speaker will be

00:00:08,960 --> 00:00:13,840
shin we will pronounce that correctly

00:00:12,080 --> 00:00:15,920
she will be talking about speeding up

00:00:13,840 --> 00:00:17,680
your data processing using parallel and

00:00:15,920 --> 00:00:20,000
asynchronous programming and data

00:00:17,680 --> 00:00:20,000
science

00:00:20,560 --> 00:00:25,840
jyn can you please can you please

00:00:24,000 --> 00:00:27,680
unmute and then start sharing your

00:00:25,840 --> 00:00:30,960
screen

00:00:27,680 --> 00:00:34,079
thank you hi yeah so

00:00:30,960 --> 00:00:37,040
yeah hello everyone i hope everyone had

00:00:34,079 --> 00:00:38,239
your lunch coffee dinner or whatever

00:00:37,040 --> 00:00:41,280
yeah anything

00:00:38,239 --> 00:00:43,040
basically y'all feel like so okay

00:00:41,280 --> 00:00:44,559
um today i'll be talking about how to

00:00:43,040 --> 00:00:45,120
speed up your data processing using

00:00:44,559 --> 00:00:47,520
parallel

00:00:45,120 --> 00:00:49,440
asynchronous programming and this is in

00:00:47,520 --> 00:00:52,640
the context of data science

00:00:49,440 --> 00:00:54,559
now a little bit about me i am greedy

00:00:52,640 --> 00:00:55,920
and i am a data engineer at st

00:00:54,559 --> 00:00:58,640
engineering so

00:00:55,920 --> 00:00:59,120
yeah i am in so i am part of a data

00:00:58,640 --> 00:01:00,879
science

00:00:59,120 --> 00:01:02,640
a relatively small data science team

00:01:00,879 --> 00:01:04,239
which works on like interesting data

00:01:02,640 --> 00:01:05,920
science problems in engineering

00:01:04,239 --> 00:01:08,000
um my background is in aerospace

00:01:05,920 --> 00:01:10,320
engineering and competition modeling

00:01:08,000 --> 00:01:12,000
and in my in the past in the course of

00:01:10,320 --> 00:01:15,600
my work i use pandas pretty

00:01:12,000 --> 00:01:17,680
much every day during working hours

00:01:15,600 --> 00:01:20,320
and that's why i contribute to the

00:01:17,680 --> 00:01:21,920
documentation for pandas 1.0 release so

00:01:20,320 --> 00:01:23,840
if you look at the documentation for

00:01:21,920 --> 00:01:25,360
pandas you might see something that i've

00:01:23,840 --> 00:01:28,479
contributed to

00:01:25,360 --> 00:01:31,040
and um and

00:01:28,479 --> 00:01:32,640
in my free time like before this whole

00:01:31,040 --> 00:01:36,159
epidemic

00:01:32,640 --> 00:01:38,240
i volunteer as a mentor at big data x

00:01:36,159 --> 00:01:39,680
a a community driven community a

00:01:38,240 --> 00:01:41,920
community which

00:01:39,680 --> 00:01:42,960
organizes like data engineering

00:01:41,920 --> 00:01:46,240
workshops

00:01:42,960 --> 00:01:47,439
for like people of all types of skills

00:01:46,240 --> 00:01:49,920
and backgrounds

00:01:47,439 --> 00:01:50,960
so that's a bit about me and that's why

00:01:49,920 --> 00:01:52,880
i have

00:01:50,960 --> 00:01:55,360
that's what i have skin in the game for

00:01:52,880 --> 00:01:58,640
this slide this presentation

00:01:55,360 --> 00:02:01,840
so now as i mentioned earlier i work

00:01:58,640 --> 00:02:02,240
in a reddit a small scale data science

00:02:01,840 --> 00:02:05,040
team

00:02:02,240 --> 00:02:07,360
so this is what a typical data science

00:02:05,040 --> 00:02:11,039
workflow looks like

00:02:07,360 --> 00:02:14,319
first we have to extract the raw data

00:02:11,039 --> 00:02:15,920
from the data source and

00:02:14,319 --> 00:02:17,680
so we will be getting it from the

00:02:15,920 --> 00:02:20,000
business like some client

00:02:17,680 --> 00:02:22,560
like some business problem it could be

00:02:20,000 --> 00:02:23,520
like some form of like csv or like some

00:02:22,560 --> 00:02:25,440
database

00:02:23,520 --> 00:02:28,480
or it could be an api so that's so

00:02:25,440 --> 00:02:30,319
that's where we extract the raw data

00:02:28,480 --> 00:02:33,120
second step is that we have to process

00:02:30,319 --> 00:02:35,680
the data so we massage the data

00:02:33,120 --> 00:02:36,720
we do the processing that is required to

00:02:35,680 --> 00:02:38,640
train our model

00:02:36,720 --> 00:02:40,160
which is a third step test that is

00:02:38,640 --> 00:02:43,440
whereby we fit

00:02:40,160 --> 00:02:44,800
the model we fit the data into the model

00:02:43,440 --> 00:02:47,040
and train the model

00:02:44,800 --> 00:02:49,200
and then lastly we will evaluate the

00:02:47,040 --> 00:02:51,599
model the performance of a model and

00:02:49,200 --> 00:02:53,440
if it works fine it looks great that's

00:02:51,599 --> 00:02:55,280
where we deployed the model into

00:02:53,440 --> 00:02:56,720
production so it looks pretty

00:02:55,280 --> 00:02:57,599
straightforward right it's nice that's

00:02:56,720 --> 00:03:00,959
very nice

00:02:57,599 --> 00:03:02,720
pipeline but

00:03:00,959 --> 00:03:04,560
when you're doing it like what are you

00:03:02,720 --> 00:03:06,560
doing with a real live data science

00:03:04,560 --> 00:03:08,080
project it doesn't really look like your

00:03:06,560 --> 00:03:10,480
keg or data set

00:03:08,080 --> 00:03:11,440
or like you're nicely cleaned up like

00:03:10,480 --> 00:03:13,599
the

00:03:11,440 --> 00:03:14,720
bootcamp problems because in the real

00:03:13,599 --> 00:03:17,519
world

00:03:14,720 --> 00:03:18,159
they made a major water deck in the data

00:03:17,519 --> 00:03:21,200
science project

00:03:18,159 --> 00:03:22,400
is the lack of data which if you have

00:03:21,200 --> 00:03:24,480
lack of data you don't

00:03:22,400 --> 00:03:26,720
need to think about how to process the

00:03:24,480 --> 00:03:29,519
data but usually it will be

00:03:26,720 --> 00:03:31,280
the form of poor quality data so if you

00:03:29,519 --> 00:03:32,480
have poor quality data that means that

00:03:31,280 --> 00:03:34,720
you need to

00:03:32,480 --> 00:03:37,040
put in more effort into your data

00:03:34,720 --> 00:03:39,760
processing

00:03:37,040 --> 00:03:41,440
so some examples would be that you have

00:03:39,760 --> 00:03:45,120
noisy images

00:03:41,440 --> 00:03:47,599
noisy text like missing values

00:03:45,120 --> 00:03:50,239
so all this all these problems will

00:03:47,599 --> 00:03:53,360
require like data processing

00:03:50,239 --> 00:03:56,959
and that goes to the very famous

00:03:53,360 --> 00:03:59,360
80 20 data science dilemma so what is

00:03:56,959 --> 00:04:02,879
the 80 20 data science dilemma

00:03:59,360 --> 00:04:05,120
so what it says that 80 of the time

00:04:02,879 --> 00:04:06,400
is actually spent trying to acquire the

00:04:05,120 --> 00:04:09,840
data

00:04:06,400 --> 00:04:13,200
and clean the data and only 20

00:04:09,840 --> 00:04:15,280
of time is actually spent developing the

00:04:13,200 --> 00:04:18,799
models

00:04:15,280 --> 00:04:21,519
so if you think that 80 20 it's

00:04:18,799 --> 00:04:22,720
seems pretty reasonable right um i don't

00:04:21,519 --> 00:04:26,840
have the buzz about boom

00:04:22,720 --> 00:04:29,680
because in reality it's closer to 90 10

00:04:26,840 --> 00:04:30,880
and this and it's going to the problem

00:04:29,680 --> 00:04:34,720
is going to get even

00:04:30,880 --> 00:04:34,720
worse when you have even more data

00:04:35,520 --> 00:04:38,960
so um in terms of excel in terms of what

00:04:38,240 --> 00:04:42,400
sort of data

00:04:38,960 --> 00:04:44,240
processing do we use and fight in data

00:04:42,400 --> 00:04:47,280
science because in data science

00:04:44,240 --> 00:04:50,400
python is the common language that

00:04:47,280 --> 00:04:53,040
most of most data scientists use

00:04:50,400 --> 00:04:55,120
and inevitably right we'll write in the

00:04:53,040 --> 00:04:58,160
problems whereby we will have to iterate

00:04:55,120 --> 00:05:02,000
an operation over

00:04:58,160 --> 00:05:03,919
like a list so

00:05:02,000 --> 00:05:06,560
an example would be that okay let's say

00:05:03,919 --> 00:05:10,080
i want to perform the square operation

00:05:06,560 --> 00:05:11,840
on a bunch of numbers so if we take so

00:05:10,080 --> 00:05:14,479
the first thing that we learn

00:05:11,840 --> 00:05:15,440
is that we use follow-ups in python and

00:05:14,479 --> 00:05:17,199
then

00:05:15,440 --> 00:05:20,160
how do we conduct that okay we

00:05:17,199 --> 00:05:23,199
initialize an empty list

00:05:20,160 --> 00:05:24,320
and then after and then we use the for

00:05:23,199 --> 00:05:28,880
loop

00:05:24,320 --> 00:05:32,160
and then we append the value to the list

00:05:28,880 --> 00:05:34,720
but as mentioned by our previous speaker

00:05:32,160 --> 00:05:36,000
for loops are actually a bad idea why is

00:05:34,720 --> 00:05:38,639
that so

00:05:36,000 --> 00:05:40,000
because four loops are actually running

00:05:38,639 --> 00:05:43,440
on the interpreter

00:05:40,000 --> 00:05:44,560
and it's not compiled and if you compare

00:05:43,440 --> 00:05:45,759
the performance of follow-ups and

00:05:44,560 --> 00:05:49,280
heighten you'll see

00:05:45,759 --> 00:05:52,000
it's terribly slow at least i think

00:05:49,280 --> 00:05:53,680
100 times slower which is quite

00:05:52,000 --> 00:05:56,880
disastrous

00:05:53,680 --> 00:05:59,440
so yeah so

00:05:56,880 --> 00:06:00,800
volumes are bad so why not list

00:05:59,440 --> 00:06:02,880
comprehensions

00:06:00,800 --> 00:06:04,840
now these comprehensions are slightly

00:06:02,880 --> 00:06:08,160
faster than for loops

00:06:04,840 --> 00:06:11,280
because because the list comprehensions

00:06:08,160 --> 00:06:13,199
are optimized for use

00:06:11,280 --> 00:06:15,840
for interpretation on the python

00:06:13,199 --> 00:06:18,000
integrator such that

00:06:15,840 --> 00:06:19,039
when the python interpreter sees the

00:06:18,000 --> 00:06:20,960
list operations

00:06:19,039 --> 00:06:22,639
it you'll be able to identify the

00:06:20,960 --> 00:06:23,520
repetitive patterns in the list

00:06:22,639 --> 00:06:26,080
compression

00:06:23,520 --> 00:06:28,800
and hence there is no need to call the

00:06:26,080 --> 00:06:31,680
append function at each iteration

00:06:28,800 --> 00:06:32,720
so this is in contrast to for loops in

00:06:31,680 --> 00:06:36,160
python whereby

00:06:32,720 --> 00:06:37,360
when it is for each iteration it will

00:06:36,160 --> 00:06:39,680
have to

00:06:37,360 --> 00:06:41,600
see that there is an append function and

00:06:39,680 --> 00:06:44,319
then it calls the append

00:06:41,600 --> 00:06:45,280
function from the list from the from the

00:06:44,319 --> 00:06:48,319
list

00:06:45,280 --> 00:06:49,759
so these comprehensions are slightly

00:06:48,319 --> 00:06:54,000
better than for loops

00:06:49,759 --> 00:06:57,360
but it may not be enough

00:06:54,000 --> 00:06:59,199
and now we go to pandas so i think the

00:06:57,360 --> 00:07:01,440
previous speaker has talked a lot about

00:06:59,199 --> 00:07:02,560
pandas and performance optimization

00:07:01,440 --> 00:07:05,440
because pandas

00:07:02,560 --> 00:07:07,039
has because pandas is designed and to be

00:07:05,440 --> 00:07:11,120
optimized for in-memory

00:07:07,039 --> 00:07:11,120
analytics using data frames

00:07:11,280 --> 00:07:16,240
so because of its like elegance and ease

00:07:14,240 --> 00:07:18,639
of use it is very popular

00:07:16,240 --> 00:07:20,319
among data scientists however when we

00:07:18,639 --> 00:07:22,160
look at large data sets

00:07:20,319 --> 00:07:23,360
that is where we run into performance

00:07:22,160 --> 00:07:25,759
and out of memory

00:07:23,360 --> 00:07:26,639
issues so what i mean by large data sets

00:07:25,759 --> 00:07:29,440
would be

00:07:26,639 --> 00:07:31,840
that data that is at least more than one

00:07:29,440 --> 00:07:31,840
gigabyte

00:07:32,400 --> 00:07:37,440
so if i if i run on like a sufficiently

00:07:36,080 --> 00:07:38,319
large data set that is like less than

00:07:37,440 --> 00:07:40,720
one gigabyte

00:07:38,319 --> 00:07:42,240
it's great finance is great but if

00:07:40,720 --> 00:07:43,280
you're looking at like hundreds of

00:07:42,240 --> 00:07:47,919
gigabytes of

00:07:43,280 --> 00:07:47,919
terabytes then that's not a good idea

00:07:48,319 --> 00:07:53,680
and it comes to the next problem why not

00:07:51,440 --> 00:07:54,800
just use a spark cluster because it's

00:07:53,680 --> 00:07:58,400
big data right

00:07:54,800 --> 00:07:59,120
big data like it's like like if my data

00:07:58,400 --> 00:08:00,560
is

00:07:59,120 --> 00:08:02,960
very big then just throw it into the

00:08:00,560 --> 00:08:06,960
spot cluster but

00:08:02,960 --> 00:08:10,240
um well there is always a price to pay

00:08:06,960 --> 00:08:12,000
for such tools because

00:08:10,240 --> 00:08:14,000
when you are when you suggest using a

00:08:12,000 --> 00:08:17,199
spark cluster

00:08:14,000 --> 00:08:18,879
there will be a communication overhead

00:08:17,199 --> 00:08:20,240
so what do i mean by communication

00:08:18,879 --> 00:08:22,720
overhead

00:08:20,240 --> 00:08:25,520
because in a spot cluster you're

00:08:22,720 --> 00:08:29,520
elaborating on distributed computing

00:08:25,520 --> 00:08:31,919
so in distributed computing you are

00:08:29,520 --> 00:08:33,159
called your net your computes are

00:08:31,919 --> 00:08:36,320
actually

00:08:33,159 --> 00:08:38,800
communicating between independent

00:08:36,320 --> 00:08:41,360
machines in a network

00:08:38,800 --> 00:08:42,640
so let me give you an example of how

00:08:41,360 --> 00:08:47,279
communication overhead

00:08:42,640 --> 00:08:50,480
looks like let's say uh i have a phone

00:08:47,279 --> 00:08:52,959
i have a phone and then i whatsapp you

00:08:50,480 --> 00:08:55,600
i want you so i'm in singapore right now

00:08:52,959 --> 00:08:58,720
i what sent you a message

00:08:55,600 --> 00:09:00,000
and then and then that message has to go

00:08:58,720 --> 00:09:02,000
through a network

00:09:00,000 --> 00:09:04,720
and then it goes and has to transmit to

00:09:02,000 --> 00:09:07,519
your network and get to your phone

00:09:04,720 --> 00:09:08,480
so this is what i call communication

00:09:07,519 --> 00:09:10,000
overheating

00:09:08,480 --> 00:09:13,040
whereby your phone is a machine and you

00:09:10,000 --> 00:09:15,920
have to go through the mobile network

00:09:13,040 --> 00:09:17,519
and secondly is the problem of like

00:09:15,920 --> 00:09:20,959
small big data

00:09:17,519 --> 00:09:24,240
or what's the definition of big data

00:09:20,959 --> 00:09:26,320
big data is not just about data that is

00:09:24,240 --> 00:09:30,959
too big to fit in memory

00:09:26,320 --> 00:09:33,760
it is also about how diverse

00:09:30,959 --> 00:09:35,440
how diverse the data set is so you have

00:09:33,760 --> 00:09:37,279
like five or five these

00:09:35,440 --> 00:09:39,120
one is the volume one is the volume

00:09:37,279 --> 00:09:41,360
another one is the variety

00:09:39,120 --> 00:09:43,440
so even if your data is too big to fade

00:09:41,360 --> 00:09:45,680
in memory it has large

00:09:43,440 --> 00:09:46,480
volume but it may not have a lot more

00:09:45,680 --> 00:09:50,080
variety

00:09:46,480 --> 00:09:53,120
and it may also not be large enough

00:09:50,080 --> 00:09:53,839
to justify using a spark cluster so if

00:09:53,120 --> 00:09:56,640
one okay

00:09:53,839 --> 00:09:58,080
so this particular term is like if you

00:09:56,640 --> 00:10:01,519
want to find out more about

00:09:58,080 --> 00:10:05,440
this particular term you can watch

00:10:01,519 --> 00:10:06,959
thomas talk on small big data 2020

00:10:05,440 --> 00:10:08,800
yeah so i will not elaborate so much

00:10:06,959 --> 00:10:12,800
into that

00:10:08,800 --> 00:10:14,320
um so now all right now i say that

00:10:12,800 --> 00:10:17,360
these compressors are not not good

00:10:14,320 --> 00:10:19,920
enough pandas is not good enough

00:10:17,360 --> 00:10:21,200
i don't have data that is big enough for

00:10:19,920 --> 00:10:23,600
a spark cluster

00:10:21,200 --> 00:10:24,480
so that's leaves me with parallel

00:10:23,600 --> 00:10:28,880
processing

00:10:24,480 --> 00:10:28,880
so what exactly is parallel processing

00:10:29,040 --> 00:10:36,560
um so of course i don't like to look

00:10:32,320 --> 00:10:41,360
at definitions so let's imagine

00:10:36,560 --> 00:10:41,360
that i work at a cafe which sells toast

00:10:41,440 --> 00:10:47,279
it's like okay so i'm from singapore and

00:10:44,800 --> 00:10:49,120
a traditional singaporean breakfast

00:10:47,279 --> 00:10:52,399
consists of

00:10:49,120 --> 00:10:55,279
coffee toast and egg

00:10:52,399 --> 00:10:56,959
today i should not talk about the egg

00:10:55,279 --> 00:11:00,000
but we will focus on the coffee

00:10:56,959 --> 00:11:03,440
and the toast

00:11:00,000 --> 00:11:05,200
task one i like to toast hundred slices

00:11:03,440 --> 00:11:08,079
of bread

00:11:05,200 --> 00:11:08,560
so some assumptions that i make is that

00:11:08,079 --> 00:11:11,680
one

00:11:08,560 --> 00:11:14,000
i am using a single slice toaster

00:11:11,680 --> 00:11:15,440
two each slice of toast takes two

00:11:14,000 --> 00:11:18,640
minutes to make

00:11:15,440 --> 00:11:21,120
and three this is a major assumption

00:11:18,640 --> 00:11:22,560
that there is no overhead time but in

00:11:21,120 --> 00:11:26,720
reality there will always

00:11:22,560 --> 00:11:26,720
be overheat time so keep that in mind

00:11:27,440 --> 00:11:33,920
um so what we are used to is

00:11:30,560 --> 00:11:36,720
sequential processing whereby we do

00:11:33,920 --> 00:11:39,040
things in sequence so if i have 100

00:11:36,720 --> 00:11:42,079
slices of bread

00:11:39,040 --> 00:11:44,000
i feed them one by one into the

00:11:42,079 --> 00:11:46,160
toaster which in this case is a

00:11:44,000 --> 00:11:49,040
processor

00:11:46,160 --> 00:11:49,360
and then after this whole process i will

00:11:49,040 --> 00:11:53,600
get

00:11:49,360 --> 00:11:53,600
100 slices of toast

00:11:54,240 --> 00:11:58,560
this whole execution time is going to

00:11:57,760 --> 00:12:02,000
take me

00:11:58,560 --> 00:12:03,760
200 minutes and

00:12:02,000 --> 00:12:06,079
yeah imagine that you have you've only

00:12:03,760 --> 00:12:06,959
gotten 100 slices of toast in 200

00:12:06,079 --> 00:12:10,720
minutes

00:12:06,959 --> 00:12:10,720
and you and imagine that you're in

00:12:12,240 --> 00:12:16,320
people are very impatient and then and

00:12:14,720 --> 00:12:17,680
then you have a lot of customers so

00:12:16,320 --> 00:12:20,240
you're not going to be able to serve

00:12:17,680 --> 00:12:22,480
your customers in time

00:12:20,240 --> 00:12:23,760
but if you think about parallel

00:12:22,480 --> 00:12:27,839
processing

00:12:23,760 --> 00:12:31,120
same thing we have 100 slices of bread

00:12:27,839 --> 00:12:33,680
we split them into four

00:12:31,120 --> 00:12:33,680
portions

00:12:34,800 --> 00:12:42,079
we fit them into four processors

00:12:39,040 --> 00:12:42,079
just like posters

00:12:42,560 --> 00:12:49,760
and then after that i get four batches

00:12:46,079 --> 00:12:50,480
of those so the task is actually

00:12:49,760 --> 00:12:54,800
executed

00:12:50,480 --> 00:12:54,800
into a poll of four

00:12:55,920 --> 00:13:02,959
okay it's executed in a pool of four

00:12:59,600 --> 00:13:05,839
toaster sub processors so each toasting

00:13:02,959 --> 00:13:07,279
sub process they run in parallel and

00:13:05,839 --> 00:13:09,200
independently from

00:13:07,279 --> 00:13:10,880
independently from each other which

00:13:09,200 --> 00:13:14,399
means that

00:13:10,880 --> 00:13:16,480
even if one toaster is out of order

00:13:14,399 --> 00:13:19,360
it's not going to affect how the other

00:13:16,480 --> 00:13:19,360
toasters are working

00:13:19,440 --> 00:13:22,959
and then after that i consolidate the

00:13:22,399 --> 00:13:26,000
batches

00:13:22,959 --> 00:13:28,959
of those into one whole stack of

00:13:26,000 --> 00:13:30,639
100 tools so what this means is that the

00:13:28,959 --> 00:13:34,000
output of each toasting process

00:13:30,639 --> 00:13:37,360
is consolidated and returned as

00:13:34,000 --> 00:13:40,399
an overall output and i don't really

00:13:37,360 --> 00:13:43,839
care about the order of my toast so

00:13:40,399 --> 00:13:43,839
it may not be in order

00:13:44,000 --> 00:13:48,320
and this whole process is going to take

00:13:46,399 --> 00:13:50,320
around 15 minutes

00:13:48,320 --> 00:13:51,760
and the speed up compared with

00:13:50,320 --> 00:13:54,639
sequential processing

00:13:51,760 --> 00:13:55,600
will be around four times so four

00:13:54,639 --> 00:13:58,240
thousands

00:13:55,600 --> 00:14:00,560
with a speed up of four types sounds

00:13:58,240 --> 00:14:00,560
great

00:14:01,680 --> 00:14:05,920
then next i will go to write what's the

00:14:04,480 --> 00:14:08,000
concept of

00:14:05,920 --> 00:14:08,959
asynchronous execution versus

00:14:08,000 --> 00:14:11,839
synchronous

00:14:08,959 --> 00:14:12,480
so what do i mean by asynchronous well

00:14:11,839 --> 00:14:16,560
let me give

00:14:12,480 --> 00:14:19,040
let me give you another example um

00:14:16,560 --> 00:14:21,519
but let's go back to the example of a

00:14:19,040 --> 00:14:23,600
traditional singaporean breakfast

00:14:21,519 --> 00:14:25,600
now we have we have the toast ready now

00:14:23,600 --> 00:14:28,240
we need to brew the coffee

00:14:25,600 --> 00:14:29,839
so same thing some assumptions here

00:14:28,240 --> 00:14:33,360
first thing i can do

00:14:29,839 --> 00:14:36,240
other stuff while making coffee so

00:14:33,360 --> 00:14:37,360
that means i just i just make the coffee

00:14:36,240 --> 00:14:39,839
and then

00:14:37,360 --> 00:14:40,639
make my toast or something second

00:14:39,839 --> 00:14:43,040
assumption

00:14:40,639 --> 00:14:44,160
one coffee maker to wake one cup of

00:14:43,040 --> 00:14:45,920
coffee

00:14:44,160 --> 00:14:48,320
because sometimes we have to do it

00:14:45,920 --> 00:14:49,760
manually that one person can make one

00:14:48,320 --> 00:14:52,240
part of coffee

00:14:49,760 --> 00:14:52,800
so the assumption is that each cup of

00:14:52,240 --> 00:14:56,480
coffee

00:14:52,800 --> 00:14:56,480
takes five minutes to make

00:14:56,560 --> 00:15:01,760
it and when we talk about synchronous

00:14:59,920 --> 00:15:04,639
execution what it means is that

00:15:01,760 --> 00:15:05,760
first i brew a cup of coffee on the

00:15:04,639 --> 00:15:10,839
coffee machine

00:15:05,760 --> 00:15:12,000
and i just stand there and wait for five

00:15:10,839 --> 00:15:15,040
minutes

00:15:12,000 --> 00:15:18,240
after my coffee is done then i

00:15:15,040 --> 00:15:21,519
toss my two slices of bread or single

00:15:18,240 --> 00:15:24,839
sliced toast after task two is complete

00:15:21,519 --> 00:15:27,360
so this is two types two it goes to four

00:15:24,839 --> 00:15:31,120
minutes

00:15:27,360 --> 00:15:33,680
and then like the total execution

00:15:31,120 --> 00:15:34,320
time will take up take nine minutes so

00:15:33,680 --> 00:15:37,920
which

00:15:34,320 --> 00:15:41,279
implies that if i want to make

00:15:37,920 --> 00:15:42,000
100 times of this i would take 900

00:15:41,279 --> 00:15:46,160
minutes

00:15:42,000 --> 00:15:49,600
to make 200 and 100 coffee

00:15:46,160 --> 00:15:53,040
so if i look at the cafe right

00:15:49,600 --> 00:15:55,600
that will be 100 sets for

00:15:53,040 --> 00:15:57,600
900 minutes and 900 minutes it's going

00:15:55,600 --> 00:15:59,360
to be like 15 hours

00:15:57,600 --> 00:16:01,839
i think by that time i will be out of

00:15:59,360 --> 00:16:01,839
business

00:16:02,560 --> 00:16:07,199
but if we look at the asynchronous way

00:16:04,880 --> 00:16:09,199
of execution is

00:16:07,199 --> 00:16:10,800
how i do it is that while i brew the

00:16:09,199 --> 00:16:12,959
coffee which i know

00:16:10,800 --> 00:16:14,480
that it's going to take five minutes i

00:16:12,959 --> 00:16:17,759
will make some post

00:16:14,480 --> 00:16:20,839
which takes two minutes each time

00:16:17,759 --> 00:16:22,240
and and if i do this process

00:16:20,839 --> 00:16:25,040
asynchronously

00:16:22,240 --> 00:16:25,519
i'm going to take five minutes for the

00:16:25,040 --> 00:16:29,680
same

00:16:25,519 --> 00:16:32,639
type of output so so effectively

00:16:29,680 --> 00:16:35,360
your execution time is being cut by

00:16:32,639 --> 00:16:35,360
almost half

00:16:35,680 --> 00:16:41,440
it looks good right like hey um if i buy

00:16:38,880 --> 00:16:43,120
like five if i buy four toasters i get

00:16:41,440 --> 00:16:45,440
four times speed up if i do

00:16:43,120 --> 00:16:46,720
asynchronous i can do more things at a

00:16:45,440 --> 00:16:48,959
time

00:16:46,720 --> 00:16:51,440
so let's go so this goes to the question

00:16:48,959 --> 00:16:52,880
of when is it a good idea to go for

00:16:51,440 --> 00:16:55,920
parallelism

00:16:52,880 --> 00:16:59,120
or to phrase it in another way is it a

00:16:55,920 --> 00:17:01,600
good idea to simply buy a 256

00:16:59,120 --> 00:17:04,559
chord processor and just parallelize all

00:17:01,600 --> 00:17:07,679
your chords

00:17:04,559 --> 00:17:10,000
well it's not

00:17:07,679 --> 00:17:12,799
that good idea if you consider some

00:17:10,000 --> 00:17:16,720
practical considerations

00:17:12,799 --> 00:17:19,600
one is your code already optimized

00:17:16,720 --> 00:17:20,079
because sometimes all you need to do is

00:17:19,600 --> 00:17:23,360
to

00:17:20,079 --> 00:17:25,839
rethink your approach for example

00:17:23,360 --> 00:17:26,720
if your code is slow because you are

00:17:25,839 --> 00:17:29,600
using

00:17:26,720 --> 00:17:30,799
for loops in your processing code you

00:17:29,600 --> 00:17:32,880
might want to consider

00:17:30,799 --> 00:17:35,760
converting your for loops into list

00:17:32,880 --> 00:17:39,120
comprehensions or map functions

00:17:35,760 --> 00:17:41,600
or array iterations

00:17:39,120 --> 00:17:42,960
secondly secondly we need to consider

00:17:41,600 --> 00:17:45,280
the problem architecture

00:17:42,960 --> 00:17:47,360
because the nature of the problem limits

00:17:45,280 --> 00:17:48,480
how successful the parallelization can

00:17:47,360 --> 00:17:52,720
be

00:17:48,480 --> 00:17:54,240
um so if so okay let's say we have uh

00:17:52,720 --> 00:17:56,080
some problems there are some

00:17:54,240 --> 00:17:58,080
computational problems which are

00:17:56,080 --> 00:18:00,320
embarrassingly parallel which means that

00:17:58,080 --> 00:18:02,960
it's very easy to parallelize everything

00:18:00,320 --> 00:18:04,720
but if your problem consists of

00:18:02,960 --> 00:18:06,559
processes which depend on each other's

00:18:04,720 --> 00:18:09,360
outputs

00:18:06,559 --> 00:18:11,039
with or intermediate results then it's

00:18:09,360 --> 00:18:14,480
not a good idea

00:18:11,039 --> 00:18:18,080
oh right um

00:18:14,480 --> 00:18:21,360
what okay data dependency means that

00:18:18,080 --> 00:18:22,160
like i have a function and then i have

00:18:21,360 --> 00:18:24,400
an input

00:18:22,160 --> 00:18:26,720
and then i have output and then our

00:18:24,400 --> 00:18:29,440
second function

00:18:26,720 --> 00:18:31,520
because the second function depends on

00:18:29,440 --> 00:18:34,559
the output of the first function

00:18:31,520 --> 00:18:37,120
so if it is if there's a subject

00:18:34,559 --> 00:18:38,240
if there's no dependency between the out

00:18:37,120 --> 00:18:41,679
between the

00:18:38,240 --> 00:18:42,160
between these presences then you might

00:18:41,679 --> 00:18:44,480
not

00:18:42,160 --> 00:18:45,280
want to paralyze that or it could be

00:18:44,480 --> 00:18:47,440
that like

00:18:45,280 --> 00:18:48,320
it will be that i have one task and

00:18:47,440 --> 00:18:51,120
another task

00:18:48,320 --> 00:18:53,200
and then one task is doing like coming

00:18:51,120 --> 00:18:54,960
out with some individual output

00:18:53,200 --> 00:18:56,720
and then the other person is going to

00:18:54,960 --> 00:18:59,840
take the integrated output

00:18:56,720 --> 00:19:00,480
then you can't really just parallelize

00:18:59,840 --> 00:19:04,000
your code

00:19:00,480 --> 00:19:06,640
that way and last but not least

00:19:04,000 --> 00:19:07,919
there is no free lunch in this world i

00:19:06,640 --> 00:19:08,240
repeat there's no free lunch in this

00:19:07,919 --> 00:19:10,720
world

00:19:08,240 --> 00:19:13,520
plus there will always be part of the

00:19:10,720 --> 00:19:17,280
work that cannot be parallelized

00:19:13,520 --> 00:19:17,840
so this is summed up in endless law

00:19:17,280 --> 00:19:21,360
which i

00:19:17,840 --> 00:19:23,360
will i will go through in more detail

00:19:21,360 --> 00:19:25,840
secondly there's also extra time

00:19:23,360 --> 00:19:27,840
required for coding and debugging

00:19:25,840 --> 00:19:29,360
parallel parallelized codes versus

00:19:27,840 --> 00:19:31,440
sequential code because i have to

00:19:29,360 --> 00:19:32,640
refactor my code i have to arrange it in

00:19:31,440 --> 00:19:33,760
a way whereby i can do the

00:19:32,640 --> 00:19:37,280
parallelization

00:19:33,760 --> 00:19:40,320
so this adds on to increase complexity

00:19:37,280 --> 00:19:42,480
increased capacity and on top of that

00:19:40,320 --> 00:19:44,320
there is also the problem of system

00:19:42,480 --> 00:19:45,200
overhead including communication

00:19:44,320 --> 00:19:49,919
overheat

00:19:45,200 --> 00:19:53,360
so if you thought so even though

00:19:49,919 --> 00:19:56,000
okay so okay yeah so

00:19:53,360 --> 00:19:57,600
okay sure okay so endless law states

00:19:56,000 --> 00:19:58,880
that the theoretical speed up is defined

00:19:57,600 --> 00:20:00,799
by the fraction of code that can be

00:19:58,880 --> 00:20:02,799
paralyzed so

00:20:00,799 --> 00:20:04,159
it looks all good but let's just look at

00:20:02,799 --> 00:20:06,559
what look at that outcome if there are

00:20:04,159 --> 00:20:09,120
no parallel parts you have no speed up

00:20:06,559 --> 00:20:10,080
if you have all parts parallel you have

00:20:09,120 --> 00:20:13,440
a lot you have

00:20:10,080 --> 00:20:15,360
if you have like infinite speed up

00:20:13,440 --> 00:20:16,400
but a steep speed up is limited by the

00:20:15,360 --> 00:20:18,240
fraction of the work that's not

00:20:16,400 --> 00:20:19,679
paralyzable

00:20:18,240 --> 00:20:21,520
because there will always be like the

00:20:19,679 --> 00:20:22,559
initialization whereby you can't

00:20:21,520 --> 00:20:25,200
paralyze the image

00:20:22,559 --> 00:20:26,640
initialization so this is going to limit

00:20:25,200 --> 00:20:28,960
how much you capitalize

00:20:26,640 --> 00:20:30,320
your workflow so it will not it so your

00:20:28,960 --> 00:20:34,400
it will not improve even

00:20:30,320 --> 00:20:37,280
if with infinite number of processes

00:20:34,400 --> 00:20:38,320
now let's go into the best way to watch

00:20:37,280 --> 00:20:40,400
the difference between multi-processing

00:20:38,320 --> 00:20:42,480
and multi trading so multi processing

00:20:40,400 --> 00:20:44,559
allows multiple processors at the same

00:20:42,480 --> 00:20:46,799
time using multiple processors

00:20:44,559 --> 00:20:48,000
multi trading means that the system

00:20:46,799 --> 00:20:49,440
excuse multiple threads

00:20:48,000 --> 00:20:51,039
of sub-processors at the same time

00:20:49,440 --> 00:20:52,159
within a single processor so the

00:20:51,039 --> 00:20:55,520
difference is between

00:20:52,159 --> 00:20:57,200
multiple processors and single processor

00:20:55,520 --> 00:20:59,200
and for multi-processing it is better

00:20:57,200 --> 00:21:02,640
for processing large values of data

00:20:59,200 --> 00:21:05,120
but from about multi-trading is best

00:21:02,640 --> 00:21:07,600
suited for io or blocking operations

00:21:05,120 --> 00:21:09,440
and i will talk more about that using

00:21:07,600 --> 00:21:11,039
some examples

00:21:09,440 --> 00:21:12,080
but before we talk about before we

00:21:11,039 --> 00:21:13,360
implement the code there are some

00:21:12,080 --> 00:21:15,440
considerations

00:21:13,360 --> 00:21:17,039
first one because data processing has to

00:21:15,440 --> 00:21:19,280
be more complete intensive

00:21:17,039 --> 00:21:20,799
so switching between threads becomes

00:21:19,280 --> 00:21:23,280
increasingly efficient

00:21:20,799 --> 00:21:24,080
on top of that there is also the global

00:21:23,280 --> 00:21:25,919
interpreter lock

00:21:24,080 --> 00:21:27,440
that does not allow parallel trigger

00:21:25,919 --> 00:21:30,480
execution

00:21:27,440 --> 00:21:32,720
so what do we do in this case

00:21:30,480 --> 00:21:34,880
how do we do how do we do a parallel

00:21:32,720 --> 00:21:38,159
synchronous in python without using any

00:21:34,880 --> 00:21:38,720
like third-party libraries it turns out

00:21:38,159 --> 00:21:40,880
that

00:21:38,720 --> 00:21:42,720
in python 3.2 there is already this

00:21:40,880 --> 00:21:44,559
module called current.futures

00:21:42,720 --> 00:21:46,240
which is a high level api for logic

00:21:44,559 --> 00:21:48,960
asynchronous parallel tasks

00:21:46,240 --> 00:21:50,480
and it's and it is an extraction layer

00:21:48,960 --> 00:21:52,480
over the multi-processing module

00:21:50,480 --> 00:21:54,240
and there are two modes of execution one

00:21:52,480 --> 00:21:55,039
is the trackpool executor for async

00:21:54,240 --> 00:21:56,480
multi trading

00:21:55,039 --> 00:21:59,039
second one is the process full

00:21:56,480 --> 00:22:00,720
executable for async multi-processing

00:21:59,039 --> 00:22:02,400
and if you look at the pricing standard

00:22:00,720 --> 00:22:05,440
library documentation it

00:22:02,400 --> 00:22:05,919
explains about how how the executors

00:22:05,440 --> 00:22:08,559
work

00:22:05,919 --> 00:22:09,679
by separating separating like those

00:22:08,559 --> 00:22:11,440
chunks

00:22:09,679 --> 00:22:13,280
separating the tasks into like

00:22:11,440 --> 00:22:14,320
separating stuff like the iterables into

00:22:13,280 --> 00:22:16,000
chunks

00:22:14,320 --> 00:22:18,640
so you can read one like in the

00:22:16,000 --> 00:22:18,640
documentation

00:22:19,200 --> 00:22:22,960
so if we look at like multi-processing

00:22:21,600 --> 00:22:25,840
and multi-trading right

00:22:22,960 --> 00:22:27,600
okay for the multi-processing executor

00:22:25,840 --> 00:22:28,960
it uses the multi-processing module and

00:22:27,600 --> 00:22:31,520
side steps the gil

00:22:28,960 --> 00:22:33,360
but for the threadful executor because

00:22:31,520 --> 00:22:35,360
it is still subject to the gil

00:22:33,360 --> 00:22:37,280
so it is not truly concurrent even

00:22:35,360 --> 00:22:39,679
though calculator futures is

00:22:37,280 --> 00:22:41,520
as the word concurrent so you need to

00:22:39,679 --> 00:22:43,679
consider that

00:22:41,520 --> 00:22:45,919
and there are two and then there's the

00:22:43,679 --> 00:22:49,280
submit operation the summary function

00:22:45,919 --> 00:22:50,640
which takes the function and the input

00:22:49,280 --> 00:22:52,880
arguments for the function and returns a

00:22:50,640 --> 00:22:55,039
futures object that represents the

00:22:52,880 --> 00:22:58,400
execution of the function

00:22:55,039 --> 00:22:59,840
and then map is similar executor.net is

00:22:58,400 --> 00:23:02,559
quite similar to the

00:22:59,840 --> 00:23:03,520
do in function map whereby you return

00:23:02,559 --> 00:23:04,960
the iterator that

00:23:03,520 --> 00:23:07,039
yields the result of the function being

00:23:04,960 --> 00:23:09,600
applied to every element of the list

00:23:07,039 --> 00:23:10,799
so okay so this is where i show you some

00:23:09,600 --> 00:23:13,600
examples of how

00:23:10,799 --> 00:23:14,400
we use calculate responding so first

00:23:13,600 --> 00:23:17,360
case

00:23:14,400 --> 00:23:19,600
it's about trying about getting data

00:23:17,360 --> 00:23:21,760
from an api

00:23:19,600 --> 00:23:23,200
oh uh i use it so i use the data.gov for

00:23:21,760 --> 00:23:26,480
sg real-time weather readings

00:23:23,200 --> 00:23:29,440
and the response is the json format so

00:23:26,480 --> 00:23:31,039
first i initialize the module and then i

00:23:29,440 --> 00:23:33,120
initialize the api requesters

00:23:31,039 --> 00:23:34,840
in this case in this example i use the

00:23:33,120 --> 00:23:36,480
trading module to monitor the track

00:23:34,840 --> 00:23:39,200
execution

00:23:36,480 --> 00:23:39,760
initialize the submission list and then

00:23:39,200 --> 00:23:41,679
i

00:23:39,760 --> 00:23:43,120
okay so first i try to use this

00:23:41,679 --> 00:23:46,080
comprehension

00:23:43,120 --> 00:23:47,360
and it takes me about 16 43 minutes to

00:23:46,080 --> 00:23:49,279
be able to process

00:23:47,360 --> 00:23:50,720
a certain amount a certain number of

00:23:49,279 --> 00:23:53,840
dates

00:23:50,720 --> 00:23:56,159
when i use trap proof executor it it

00:23:53,840 --> 00:23:56,880
the speed up is about 20.9 times

00:23:56,159 --> 00:24:00,000
compared

00:23:56,880 --> 00:24:02,159
with using these comprehensions

00:24:00,000 --> 00:24:04,400
so just as i mentioned that this

00:24:02,159 --> 00:24:06,799
calculation is the most optimized way

00:24:04,400 --> 00:24:07,520
of iterating without using parallel

00:24:06,799 --> 00:24:10,240
processing

00:24:07,520 --> 00:24:11,200
so this speed up is quite significant

00:24:10,240 --> 00:24:13,760
when you compare

00:24:11,200 --> 00:24:16,480
to just compare it to using these

00:24:13,760 --> 00:24:16,480
calculations

00:24:16,559 --> 00:24:19,760
and now the second case is whereby we

00:24:19,120 --> 00:24:22,480
are

00:24:19,760 --> 00:24:22,880
we are we're doing some image processing

00:24:22,480 --> 00:24:26,240
so

00:24:22,880 --> 00:24:28,559
um i will suggest x-ray images and

00:24:26,240 --> 00:24:29,760
and the reason why i need to do the data

00:24:28,559 --> 00:24:31,440
processing is because

00:24:29,760 --> 00:24:33,360
the images in the dataset are of

00:24:31,440 --> 00:24:34,159
different dimensions so i need to

00:24:33,360 --> 00:24:37,600
standardize

00:24:34,159 --> 00:24:40,880
size sizes so same thing

00:24:37,600 --> 00:24:44,159
i initialize the python modules

00:24:40,880 --> 00:24:44,720
i initialize the image resize process in

00:24:44,159 --> 00:24:48,080
this

00:24:44,720 --> 00:24:51,279
example i'm using os.get pid to monitor

00:24:48,080 --> 00:24:53,760
the process execution

00:24:51,279 --> 00:24:54,640
and i initialize the policy directory so

00:24:53,760 --> 00:24:58,840
in this data

00:24:54,640 --> 00:25:04,080
so in this example i am processing 141

00:24:58,840 --> 00:25:04,080
images if i use that function

00:25:04,240 --> 00:25:09,679
i get so it will take me about 29.48

00:25:08,000 --> 00:25:12,400
seconds

00:25:09,679 --> 00:25:13,440
if i use a list comprehension it is

00:25:12,400 --> 00:25:16,640
slightly better

00:25:13,440 --> 00:25:18,480
in that it is well slightly better but

00:25:16,640 --> 00:25:20,320
quite like not much difference is about

00:25:18,480 --> 00:25:24,240
29.71 seconds

00:25:20,320 --> 00:25:27,360
but if i use the process pull executor

00:25:24,240 --> 00:25:28,159
if i use the process full executor to

00:25:27,360 --> 00:25:31,600
get the

00:25:28,159 --> 00:25:36,880
to process my images using eight

00:25:31,600 --> 00:25:40,000
cores i get a speed up of about 4.3

00:25:36,880 --> 00:25:40,480
so effectively by process i take about

00:25:40,000 --> 00:25:42,480
seven

00:25:40,480 --> 00:25:44,720
seconds to process thousand four hundred

00:25:42,480 --> 00:25:46,799
images so that's also that's all

00:25:44,720 --> 00:25:48,880
that is the power of leveraging on

00:25:46,799 --> 00:25:52,000
processful executor

00:25:48,880 --> 00:25:52,640
for your prosperous processing and if

00:25:52,000 --> 00:25:55,039
you take a

00:25:52,640 --> 00:25:56,480
closer look at the code you can realize

00:25:55,039 --> 00:25:59,919
that the code is actually

00:25:56,480 --> 00:26:03,919
pretty simple to implement right

00:25:59,919 --> 00:26:06,720
yeah so some key takeaways that i'd like

00:26:03,919 --> 00:26:07,679
you to i would like you to remember from

00:26:06,720 --> 00:26:11,279
this i talk

00:26:07,679 --> 00:26:12,400
is that not all processes should be

00:26:11,279 --> 00:26:15,440
parallelized

00:26:12,400 --> 00:26:17,840
because parallel processes come with

00:26:15,440 --> 00:26:19,600
overheads there is no free lunch in this

00:26:17,840 --> 00:26:23,039
world

00:26:19,600 --> 00:26:23,039
because of mdel's law

00:26:23,120 --> 00:26:26,799
and you need to consider system overhead

00:26:25,120 --> 00:26:29,760
including communication overhead

00:26:26,799 --> 00:26:32,480
this is not just a problem of

00:26:29,760 --> 00:26:35,840
distributed computing this also exists

00:26:32,480 --> 00:26:37,760
in parallel parallelization

00:26:35,840 --> 00:26:40,080
even though the communication overhead

00:26:37,760 --> 00:26:42,400
is not that significant

00:26:40,080 --> 00:26:44,480
and last but not least if the cost of

00:26:42,400 --> 00:26:46,480
rewriting your code for parallelization

00:26:44,480 --> 00:26:47,760
outweighs the time savings from

00:26:46,480 --> 00:26:49,360
parallelizing your code

00:26:47,760 --> 00:26:51,039
this usually happens when your data set

00:26:49,360 --> 00:26:52,960
is not big enough

00:26:51,039 --> 00:26:54,400
please consider other ways of optimizing

00:26:52,960 --> 00:26:57,440
your quotes instead

00:26:54,400 --> 00:26:59,120
and yeah

00:26:57,440 --> 00:27:00,880
and if you can't understand everything

00:26:59,120 --> 00:27:03,039
else that i said just remember

00:27:00,880 --> 00:27:04,480
please do not use your for loops please

00:27:03,039 --> 00:27:06,720
either use

00:27:04,480 --> 00:27:07,520
list corporations or if that doesn't

00:27:06,720 --> 00:27:10,240
work

00:27:07,520 --> 00:27:12,640
use current.futures module for your

00:27:10,240 --> 00:27:15,010
parallelization

00:27:12,640 --> 00:27:16,480
yeah so there's some references and

00:27:15,010 --> 00:27:18,559
[Music]

00:27:16,480 --> 00:27:20,399
reach out to me at all this social

00:27:18,559 --> 00:27:21,360
social media platforms and you can check

00:27:20,399 --> 00:27:26,320
out my slides

00:27:21,360 --> 00:27:29,120
at this link at this yhar repo

00:27:26,320 --> 00:27:29,760
okay excellent thank you very much

00:27:29,120 --> 00:27:33,600
shinri

00:27:29,760 --> 00:27:34,320
that was very good so we don't have any

00:27:33,600 --> 00:27:36,240
questions

00:27:34,320 --> 00:27:37,970
other than the comment that the

00:27:36,240 --> 00:27:41,120
attendees love toast too

00:27:37,970 --> 00:27:42,799
[Laughter]

00:27:41,120 --> 00:27:44,880
actually good good good thing that we

00:27:42,799 --> 00:27:46,159
had lunch before so because otherwise we

00:27:44,880 --> 00:27:50,399
would have gotten really

00:27:46,159 --> 00:27:52,720
hungry so yeah thank you very much again

00:27:50,399 --> 00:27:55,200
and um oh there's one question there why

00:27:52,720 --> 00:27:58,000
do you not use third-party packages like

00:27:55,200 --> 00:28:02,399
multi-processing

00:27:58,000 --> 00:28:04,240
okay uh first so

00:28:02,399 --> 00:28:05,440
okay first i need to emphasize that

00:28:04,240 --> 00:28:08,720
multi-processing

00:28:05,440 --> 00:28:10,559
is not a third-party library in fact

00:28:08,720 --> 00:28:12,720
multi-processing is part of the python

00:28:10,559 --> 00:28:16,240
standard library

00:28:12,720 --> 00:28:16,720
and concurrent futures is an abstraction

00:28:16,240 --> 00:28:19,760
layer

00:28:16,720 --> 00:28:22,799
over the multi-processing module

00:28:19,760 --> 00:28:25,440
and python standard library

00:28:22,799 --> 00:28:27,600
and for this at this particular use case

00:28:25,440 --> 00:28:31,039
is whereby

00:28:27,600 --> 00:28:33,520
whereby i just want to process my data

00:28:31,039 --> 00:28:35,039
it's not about trying to implement my

00:28:33,520 --> 00:28:37,679
machine learning algorithms

00:28:35,039 --> 00:28:38,320
like scikit learn because it's because

00:28:37,679 --> 00:28:39,600
if you are trying

00:28:38,320 --> 00:28:42,399
to try to if you are trying to

00:28:39,600 --> 00:28:44,799
parallelize your machine learning

00:28:42,399 --> 00:28:46,159
training machine learning model training

00:28:44,799 --> 00:28:47,600
process

00:28:46,159 --> 00:28:49,520
all this stuff but all these third party

00:28:47,600 --> 00:28:52,640
libraries like scikit learn

00:28:49,520 --> 00:28:53,120
tensorflow pi thought they also they

00:28:52,640 --> 00:28:54,960
have

00:28:53,120 --> 00:28:56,480
the input they have their parallel

00:28:54,960 --> 00:28:59,520
implementation whereby

00:28:56,480 --> 00:29:03,200
you you whereby what you need to do is

00:28:59,520 --> 00:29:04,720
to set some settings on end jobs

00:29:03,200 --> 00:29:06,640
and like they will have they will have

00:29:04,720 --> 00:29:09,440
some parallel implementation

00:29:06,640 --> 00:29:09,760
but right place but then if i be the

00:29:09,440 --> 00:29:11,840
case

00:29:09,760 --> 00:29:13,919
by i just wanted to be able to do some

00:29:11,840 --> 00:29:15,120
processing that is not involving the

00:29:13,919 --> 00:29:16,480
model training process

00:29:15,120 --> 00:29:18,159
i want to i want to do the

00:29:16,480 --> 00:29:21,440
pre-processing that

00:29:18,159 --> 00:29:24,159
that goes before the monitoring

00:29:21,440 --> 00:29:26,640
or and let's say like in the case of

00:29:24,159 --> 00:29:26,640
images

00:29:28,720 --> 00:29:32,399
okay thank you thank you so thank you

00:29:31,360 --> 00:29:36,000
very much for the talk

00:29:32,399 --> 00:29:41,840
uh let me get your url plus

00:29:36,000 --> 00:29:41,840
like this

00:29:46,720 --> 00:29:48,799

YouTube URL: https://www.youtube.com/watch?v=PB7_5BQp1SU


