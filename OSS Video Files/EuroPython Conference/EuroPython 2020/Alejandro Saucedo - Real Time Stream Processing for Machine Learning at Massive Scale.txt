Title: Alejandro Saucedo - Real Time Stream Processing for Machine Learning at Massive Scale
Publication date: 2020-09-21
Playlist: EuroPython 2020
Description: 
	"Real Time Stream Processing for Machine Learning at Massive Scale
EuroPython 2020 - Talk - 2020-07-23 - Parrot Data Science
Online

By Alejandro Saucedo

This talk will provide a practical insight on how to build scalable data streaming machine learning pipelines to process large datasets in real time using Python and popular frameworks such as Kafka, SpaCy and Seldon.

We will be covering a case study performing automated content moderation on Reddit comments in real time. Our dataset will consist of 200k reddit comments from /r/science, 50,000 of which have been removed by moderators. We will be handling the stream data in a Kubernetes cluster, and the stream processing will be handled using the stream processing library Kafka. We will be running the end-to-end pipeline in Kubernetes with various components legeraging SKLearn, SpaCy and Seldon.

We will then dive into fundamental concepts on stream processing such as windows, watermarking and checkponting, and we will show how to use each of these frameworks to build complex data streaming pipelines that can perform real time processing at scale by building, deploying and monitoring a machine learning model which will process production incoming data..

Finally we will show best practices when using these frameworks, as well as a high level overview of tools that can be used for monitoring in-depth.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2020.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:05,839 --> 00:00:10,320
fantastic

00:00:07,279 --> 00:00:12,719
uh so um alejandro cecedo is

00:00:10,320 --> 00:00:14,080
um for for anyone who hasn't already

00:00:12,719 --> 00:00:17,359
read his bio he's the

00:00:14,080 --> 00:00:19,039
chief scientist at the institute for uh

00:00:17,359 --> 00:00:20,560
what is this ethical ai and machine

00:00:19,039 --> 00:00:22,480
learning okay

00:00:20,560 --> 00:00:24,080
uh at least the development of industry

00:00:22,480 --> 00:00:26,160
standards of machine learning bias

00:00:24,080 --> 00:00:27,279
adversarial attacks and uh differential

00:00:26,160 --> 00:00:31,199
privacy

00:00:27,279 --> 00:00:33,440
so this is bound to be um

00:00:31,199 --> 00:00:35,680
bound to be an awesome talk actually so

00:00:33,440 --> 00:00:38,880
um he'll be speaking on

00:00:35,680 --> 00:00:38,880
oh where am i here

00:00:39,040 --> 00:00:42,399
i said oh you gotta you actually got two

00:00:40,480 --> 00:00:43,440
talks going on at this conference that's

00:00:42,399 --> 00:00:45,440
awesome so this is on

00:00:43,440 --> 00:00:48,239
real-time stream processing for machine

00:00:45,440 --> 00:00:49,360
learning at a massive scale so i will

00:00:48,239 --> 00:00:52,960
turn

00:00:49,360 --> 00:00:56,320
uh this over to you alejandro and uh

00:00:52,960 --> 00:00:59,520
have fun awesome amazing

00:00:56,320 --> 00:01:01,600
thank you very much and uh yeah

00:00:59,520 --> 00:01:03,680
thank you very much for for the uh great

00:01:01,600 --> 00:01:04,239
introduction as uh you highlighted

00:01:03,680 --> 00:01:07,280
tomorrow

00:01:04,239 --> 00:01:08,960
uh i am uh also doing another talk i'm

00:01:07,280 --> 00:01:11,439
doing the keynote which

00:01:08,960 --> 00:01:12,479
focuses more on the topics that you

00:01:11,439 --> 00:01:15,840
mentioned around

00:01:12,479 --> 00:01:17,439
uh responsible development uh of machine

00:01:15,840 --> 00:01:18,320
learning and just generally software

00:01:17,439 --> 00:01:20,799
development

00:01:18,320 --> 00:01:23,360
but today we're going to be diving into

00:01:20,799 --> 00:01:26,479
real-time machine learning with python

00:01:23,360 --> 00:01:26,960
so quite exciting and practical uh deep

00:01:26,479 --> 00:01:30,079
dive

00:01:26,960 --> 00:01:30,799
into a very popular and uh challenging

00:01:30,079 --> 00:01:34,560
complex

00:01:30,799 --> 00:01:37,119
uh topic so a little bit about myself

00:01:34,560 --> 00:01:38,159
um uh my name is alejandro i'm the

00:01:37,119 --> 00:01:39,600
engineering director of cell and

00:01:38,159 --> 00:01:41,600
technologies and chief scientists at the

00:01:39,600 --> 00:01:45,040
institute for ethical ai i'm also

00:01:41,600 --> 00:01:47,280
a member at large at the acm and

00:01:45,040 --> 00:01:48,079
today we're going to be delving into the

00:01:47,280 --> 00:01:51,439
realm of

00:01:48,079 --> 00:01:53,680
real-time machine learning just to give

00:01:51,439 --> 00:01:56,880
you a little bit more about

00:01:53,680 --> 00:01:59,920
my current work so selden

00:01:56,880 --> 00:02:00,640
is an open source uh primarily company

00:01:59,920 --> 00:02:03,840
our

00:02:00,640 --> 00:02:06,799
main open source uh project has over

00:02:03,840 --> 00:02:08,399
2 million installations massive growth

00:02:06,799 --> 00:02:10,640
and user base

00:02:08,399 --> 00:02:11,680
please go check it out on github you

00:02:10,640 --> 00:02:13,920
know

00:02:11,680 --> 00:02:15,200
vanity metric but you know has all the

00:02:13,920 --> 00:02:18,640
github stars

00:02:15,200 --> 00:02:22,480
and uh working with quite a lot of uh

00:02:18,640 --> 00:02:24,239
large brands um basically on the

00:02:22,480 --> 00:02:25,840
mainly what is productionization of

00:02:24,239 --> 00:02:29,040
machine learning uh

00:02:25,840 --> 00:02:31,200
uh hence the topic today and

00:02:29,040 --> 00:02:33,200
with the institute uh a little bit about

00:02:31,200 --> 00:02:34,640
that it's a research center that focuses

00:02:33,200 --> 00:02:38,720
primarily on

00:02:34,640 --> 00:02:39,360
uh the research of uh topics as the ones

00:02:38,720 --> 00:02:40,720
mentioned

00:02:39,360 --> 00:02:42,720
primarily around the responsible

00:02:40,720 --> 00:02:45,040
development of the systems through

00:02:42,720 --> 00:02:46,640
contributions and standards and

00:02:45,040 --> 00:02:48,640
regulatory frameworks

00:02:46,640 --> 00:02:50,160
so we are actually part of the linux

00:02:48,640 --> 00:02:52,959
foundation

00:02:50,160 --> 00:02:55,280
which is quite an exciting piece for us

00:02:52,959 --> 00:02:58,080
primarily as that allows us to

00:02:55,280 --> 00:02:59,120
contribute to a lot of the open source

00:02:58,080 --> 00:03:01,920
initiatives

00:02:59,120 --> 00:03:04,080
from a very high level and also

00:03:01,920 --> 00:03:05,920
pragmatic perspective

00:03:04,080 --> 00:03:07,840
uh but today yeah we're gonna be diving

00:03:05,920 --> 00:03:09,120
into the conceptual introduction of uh

00:03:07,840 --> 00:03:12,000
stream processing

00:03:09,120 --> 00:03:12,720
we're gonna be delving into uh the

00:03:12,000 --> 00:03:14,879
concept of

00:03:12,720 --> 00:03:16,000
machine learning for real time and how

00:03:14,879 --> 00:03:18,480
that in itself is

00:03:16,000 --> 00:03:19,360
is potentially slightly different we're

00:03:18,480 --> 00:03:20,879
going to be

00:03:19,360 --> 00:03:22,640
diving into some of the trade-offs

00:03:20,879 --> 00:03:24,159
across the tools available

00:03:22,640 --> 00:03:26,159
and then we're going to be covering a

00:03:24,159 --> 00:03:27,920
hands-on use case

00:03:26,159 --> 00:03:29,440
uh that will allow you to get up and

00:03:27,920 --> 00:03:32,560
running um with

00:03:29,440 --> 00:03:34,720
with this topic so

00:03:32,560 --> 00:03:35,680
specifically in regards to uh what we're

00:03:34,720 --> 00:03:38,640
gonna be uh

00:03:35,680 --> 00:03:40,799
doing today is we're going to be uh

00:03:38,640 --> 00:03:42,560
fixing the front page of the internet

00:03:40,799 --> 00:03:44,319
right so what is the front page of the

00:03:42,560 --> 00:03:47,360
internet you say uh

00:03:44,319 --> 00:03:49,519
basically reddit uh we're going to be uh

00:03:47,360 --> 00:03:51,519
doing some real-time uh machine learning

00:03:49,519 --> 00:03:54,560
for reddit comments

00:03:51,519 --> 00:03:56,560
uh we basically have a uh playground

00:03:54,560 --> 00:03:58,879
where we have 200 000

00:03:56,560 --> 00:04:00,560
comments uh that we will be using to

00:03:58,879 --> 00:04:03,439
train a model

00:04:00,560 --> 00:04:04,720
and these comments come from our science

00:04:03,439 --> 00:04:07,920
and

00:04:04,720 --> 00:04:09,360
the premise of this is that this

00:04:07,920 --> 00:04:13,280
comments and this data set

00:04:09,360 --> 00:04:15,280
is the comment uh moderation and data

00:04:13,280 --> 00:04:16,880
set so in turn these are basically

00:04:15,280 --> 00:04:18,000
comments that have been removed by

00:04:16,880 --> 00:04:19,680
moderators

00:04:18,000 --> 00:04:22,320
so we're going to be training a model

00:04:19,680 --> 00:04:27,759
that you know cleans the internet out of

00:04:22,320 --> 00:04:29,520
uh you know i guess bad language etc

00:04:27,759 --> 00:04:32,080
course you know it's never as easy in

00:04:29,520 --> 00:04:34,240
such a wild wild

00:04:32,080 --> 00:04:36,080
west so let's take a trip to the past or

00:04:34,240 --> 00:04:38,960
more specifically a trip to the present

00:04:36,080 --> 00:04:39,680
and more specifically here you can see

00:04:38,960 --> 00:04:42,639
in this uh

00:04:39,680 --> 00:04:42,960
architectural diagram that this concept

00:04:42,639 --> 00:04:46,240
of

00:04:42,960 --> 00:04:48,639
etl is often referred to as

00:04:46,240 --> 00:04:49,680
extract transform load and what it

00:04:48,639 --> 00:04:53,040
basically means

00:04:49,680 --> 00:04:55,360
is in the old school when you have data

00:04:53,040 --> 00:04:57,120
in specific space you would basically

00:04:55,360 --> 00:04:58,479
want to extract that data

00:04:57,120 --> 00:05:00,400
do something with that data and put it

00:04:58,479 --> 00:05:01,440
back right this is the whole concept of

00:05:00,400 --> 00:05:04,560
etl

00:05:01,440 --> 00:05:06,800
uh now from that perspective etl is the

00:05:04,560 --> 00:05:07,840
sort of old-school way traditional way

00:05:06,800 --> 00:05:11,520
in which uh

00:05:07,840 --> 00:05:13,199
most uh sort of date engineering

00:05:11,520 --> 00:05:14,560
uh components have been have been

00:05:13,199 --> 00:05:16,720
tackled right your time

00:05:14,560 --> 00:05:17,680
is still something that is out of reach

00:05:16,720 --> 00:05:19,440
for a lot of

00:05:17,680 --> 00:05:21,600
organizations and they're still stuck in

00:05:19,440 --> 00:05:22,880
the batch processing as it's often

00:05:21,600 --> 00:05:25,440
referred to

00:05:22,880 --> 00:05:26,400
and specifically in terms of etl there's

00:05:25,440 --> 00:05:29,919
variations right

00:05:26,400 --> 00:05:32,960
you have etl you have elt you have el

00:05:29,919 --> 00:05:34,960
you have lt you have many many different

00:05:32,960 --> 00:05:36,880
variations of this

00:05:34,960 --> 00:05:38,000
approach right and and the problem with

00:05:36,880 --> 00:05:41,759
this is that there's

00:05:38,000 --> 00:05:43,919
not just one that uh that rules them all

00:05:41,759 --> 00:05:45,360
right there's actually a lot of

00:05:43,919 --> 00:05:47,919
specialized tools

00:05:45,360 --> 00:05:48,560
that are fit for purpose and some are

00:05:47,919 --> 00:05:50,960
you know

00:05:48,560 --> 00:05:52,720
a bit more useless than others but most

00:05:50,960 --> 00:05:53,600
of them are actually a large amount of

00:05:52,720 --> 00:05:56,479
them are actually

00:05:53,600 --> 00:05:58,560
very very specialized where they are

00:05:56,479 --> 00:05:59,520
very popular or the primary tool for a

00:05:58,560 --> 00:06:01,680
specific area

00:05:59,520 --> 00:06:03,919
so unfortunately when dealing with not

00:06:01,680 --> 00:06:07,039
just machine learning but just general

00:06:03,919 --> 00:06:09,199
data analytics um you have this

00:06:07,039 --> 00:06:10,639
whole host of different technologies

00:06:09,199 --> 00:06:12,560
that you have to interact with

00:06:10,639 --> 00:06:13,759
different set of uh patterns that you

00:06:12,560 --> 00:06:15,680
have to deal with

00:06:13,759 --> 00:06:18,479
now from from that perspective let's

00:06:15,680 --> 00:06:20,400
have a look at how some of those

00:06:18,479 --> 00:06:22,800
terminologies actually fit in regards to

00:06:20,400 --> 00:06:25,280
the tools with extract and load

00:06:22,800 --> 00:06:26,639
basically moving data around you have

00:06:25,280 --> 00:06:28,560
tools like nifi

00:06:26,639 --> 00:06:29,840
and flume which basically allow you to

00:06:28,560 --> 00:06:32,000
pipe some

00:06:29,840 --> 00:06:34,000
of the data from a database to another

00:06:32,000 --> 00:06:35,600
database perhaps in another format

00:06:34,000 --> 00:06:37,120
what you will see here is that those

00:06:35,600 --> 00:06:37,840
tools tend to also be a little bit

00:06:37,120 --> 00:06:40,960
ambiguous

00:06:37,840 --> 00:06:43,680
because some tend to like um

00:06:40,960 --> 00:06:45,280
fall into the category of others you

00:06:43,680 --> 00:06:47,440
know for example knife phy

00:06:45,280 --> 00:06:48,880
does actually do transformation even

00:06:47,440 --> 00:06:52,880
though it's its main

00:06:48,880 --> 00:06:55,039
um premise is is is transfer

00:06:52,880 --> 00:06:57,280
uh uh then you have in the etl

00:06:55,039 --> 00:06:59,280
components things like uzi or airflow

00:06:57,280 --> 00:07:01,520
which you most probably have heard of

00:06:59,280 --> 00:07:02,639
and then you have an old school etl

00:07:01,520 --> 00:07:05,360
where the human

00:07:02,639 --> 00:07:05,759
is the t well the human is the e the t

00:07:05,360 --> 00:07:08,560
and the

00:07:05,759 --> 00:07:10,000
l right so that in itself is also it

00:07:08,560 --> 00:07:12,000
comes to ptl and then you have others

00:07:10,000 --> 00:07:13,440
like elt right data warehousing

00:07:12,000 --> 00:07:15,120
components where you have like an

00:07:13,440 --> 00:07:16,720
extract and a load and then

00:07:15,120 --> 00:07:19,120
the actual processing happens in the

00:07:16,720 --> 00:07:22,000
data in the database itself

00:07:19,120 --> 00:07:24,319
now um there's now when we start talking

00:07:22,000 --> 00:07:25,919
about the future is when we start going

00:07:24,319 --> 00:07:27,680
in regards to okay

00:07:25,919 --> 00:07:30,160
today is all about yesterday was all

00:07:27,680 --> 00:07:31,680
about batch now we need to move into

00:07:30,160 --> 00:07:33,039
into the streaming world right and what

00:07:31,680 --> 00:07:36,160
what is the difference between

00:07:33,039 --> 00:07:37,520
um uh those two components right so we

00:07:36,160 --> 00:07:39,520
want to dive into what is there

00:07:37,520 --> 00:07:40,880
what does that even mean right and what

00:07:39,520 --> 00:07:42,720
that basically means is that

00:07:40,880 --> 00:07:44,160
ultimately when it comes to batch

00:07:42,720 --> 00:07:46,800
processing it's

00:07:44,160 --> 00:07:48,240
what the name uh suggests you actually

00:07:46,800 --> 00:07:50,479
take a chunk of data

00:07:48,240 --> 00:07:51,280
and you expect that chunk of data to be

00:07:50,479 --> 00:07:54,160
processed

00:07:51,280 --> 00:07:55,120
now in the real time you will expect at

00:07:54,160 --> 00:07:57,759
each data point

00:07:55,120 --> 00:07:58,800
as it comes to be processed and you can

00:07:57,759 --> 00:08:00,000
imagine that

00:07:58,800 --> 00:08:01,360
you know if you don't have the

00:08:00,000 --> 00:08:03,360
infrastructure that in itself would be

00:08:01,360 --> 00:08:05,280
much more costly if you have to like

00:08:03,360 --> 00:08:06,560
spin up get everything ready make sure

00:08:05,280 --> 00:08:09,120
that everything is there

00:08:06,560 --> 00:08:10,960
but fortunately there's been so many

00:08:09,120 --> 00:08:12,560
advancements in not only the

00:08:10,960 --> 00:08:14,400
data processing components but also in

00:08:12,560 --> 00:08:16,960
the infrastructure and general

00:08:14,400 --> 00:08:19,599
computing that has allowed for this real

00:08:16,960 --> 00:08:22,080
time to be much more of a possible thing

00:08:19,599 --> 00:08:22,800
but one of the things is that it's not

00:08:22,080 --> 00:08:26,560
it's

00:08:22,800 --> 00:08:28,800
more often than not uh uh batch

00:08:26,560 --> 00:08:30,400
and streaming right you never will find

00:08:28,800 --> 00:08:31,440
a situation where it's only bad you're

00:08:30,400 --> 00:08:33,599
only streaming

00:08:31,440 --> 00:08:35,599
there is a sort of combination of where

00:08:33,599 --> 00:08:36,320
both of these components actually fit

00:08:35,599 --> 00:08:38,399
together

00:08:36,320 --> 00:08:39,839
and from that perspective you would want

00:08:38,399 --> 00:08:42,159
to have uh

00:08:39,839 --> 00:08:43,519
uh or perhaps certain situations where

00:08:42,159 --> 00:08:45,839
you want to batch something

00:08:43,519 --> 00:08:47,600
and then ask something else maybe coming

00:08:45,839 --> 00:08:48,399
in you would want to still process it in

00:08:47,600 --> 00:08:49,920
real time

00:08:48,399 --> 00:08:51,360
or the other way around you may want to

00:08:49,920 --> 00:08:51,920
actually process and clean things as

00:08:51,360 --> 00:08:53,839
they come

00:08:51,920 --> 00:08:55,279
and then perform batches on the data

00:08:53,839 --> 00:08:57,200
that is already installed

00:08:55,279 --> 00:08:59,360
and the more interesting thing is that

00:08:57,200 --> 00:09:02,080
now there is a convergence

00:08:59,360 --> 00:09:02,560
in both worlds where the apis that used

00:09:02,080 --> 00:09:04,320
to be

00:09:02,560 --> 00:09:05,920
specifically used or the inter the

00:09:04,320 --> 00:09:06,399
programmatic interfaces that you used to

00:09:05,920 --> 00:09:08,959
use

00:09:06,399 --> 00:09:10,320
as libraries to deal with batch were

00:09:08,959 --> 00:09:11,200
very different to the ones that you

00:09:10,320 --> 00:09:12,560
would deal with

00:09:11,200 --> 00:09:14,240
streaming because they were different

00:09:12,560 --> 00:09:17,760
components but new

00:09:14,240 --> 00:09:21,120
um sort of work in the field

00:09:17,760 --> 00:09:22,240
has allowed and enabled for this higher

00:09:21,120 --> 00:09:23,680
abstraction

00:09:22,240 --> 00:09:25,760
that provides you with the ability to

00:09:23,680 --> 00:09:28,560
say well streaming in a way

00:09:25,760 --> 00:09:29,600
is just batch processing but in single

00:09:28,560 --> 00:09:31,360
data batches

00:09:29,600 --> 00:09:33,440
but then you can also have window based

00:09:31,360 --> 00:09:35,360
batches right and if you take a window

00:09:33,440 --> 00:09:36,800
that is the size of the entire stream

00:09:35,360 --> 00:09:39,600
that in itself is a batch where the

00:09:36,800 --> 00:09:41,360
entire stream is is a batch right so

00:09:39,600 --> 00:09:43,120
from that perspective the inter the

00:09:41,360 --> 00:09:43,839
programmatic interfaces the sdks the

00:09:43,120 --> 00:09:46,480
libraries

00:09:43,839 --> 00:09:48,880
are also adopting a sort of like unified

00:09:46,480 --> 00:09:52,160
interface to deal with both problems

00:09:48,880 --> 00:09:52,959
so now let's dive into some concepts of

00:09:52,160 --> 00:09:54,800
streaming

00:09:52,959 --> 00:09:56,320
the first one i already touch upon is

00:09:54,800 --> 00:09:57,760
windowing but then you also have

00:09:56,320 --> 00:09:59,519
different types of window

00:09:57,760 --> 00:10:01,120
you have the traditional sort of like

00:09:59,519 --> 00:10:02,160
tumbling windows which basically says

00:10:01,120 --> 00:10:05,279
okay i'm gonna take

00:10:02,160 --> 00:10:06,399
a chunk then that data that was seen in

00:10:05,279 --> 00:10:08,160
this period

00:10:06,399 --> 00:10:10,000
will be processed and then the next

00:10:08,160 --> 00:10:11,200
chunk but then you also have sliding

00:10:10,000 --> 00:10:13,519
windows which means that they're

00:10:11,200 --> 00:10:15,680
overlapping right you're processing data

00:10:13,519 --> 00:10:16,640
uh and again you know you could even say

00:10:15,680 --> 00:10:19,680
it's batches

00:10:16,640 --> 00:10:23,040
of data and in a sense some systems

00:10:19,680 --> 00:10:26,480
implemented with batch uh capabilities

00:10:23,040 --> 00:10:28,079
but just running it very uh often right

00:10:26,480 --> 00:10:29,760
so from that perspective is that

00:10:28,079 --> 00:10:31,360
abstraction again in play

00:10:29,760 --> 00:10:33,360
you have another concept called check

00:10:31,360 --> 00:10:34,640
pointing and as the name entails

00:10:33,360 --> 00:10:37,040
similar to how you would have it in a

00:10:34,640 --> 00:10:38,640
game what checkpointing does is keeping

00:10:37,040 --> 00:10:40,160
track of the stream progress

00:10:38,640 --> 00:10:42,720
and it's important if you have many

00:10:40,160 --> 00:10:44,880
consumers or many groups of consumers

00:10:42,720 --> 00:10:46,079
that are consistently being able to like

00:10:44,880 --> 00:10:48,320
read a specific

00:10:46,079 --> 00:10:49,680
stream of data if they suddenly crash

00:10:48,320 --> 00:10:50,320
you need to be able to know where did

00:10:49,680 --> 00:10:53,839
that

00:10:50,320 --> 00:10:55,200
uh last actually finish and because

00:10:53,839 --> 00:10:56,959
and the reason why this is important is

00:10:55,200 --> 00:10:58,240
because that starts introducing some of

00:10:56,959 --> 00:11:00,000
the

00:10:58,240 --> 00:11:02,079
terminologies such as you know

00:11:00,000 --> 00:11:05,440
processing at least once

00:11:02,079 --> 00:11:05,920
at most once or exactly once right do

00:11:05,440 --> 00:11:08,480
you

00:11:05,920 --> 00:11:10,480
and this depends on on how where is the

00:11:08,480 --> 00:11:11,279
biggest risk for your specific use case

00:11:10,480 --> 00:11:13,279
right so this

00:11:11,279 --> 00:11:14,720
a lot of things and concepts that right

00:11:13,279 --> 00:11:16,560
now we're kind of like

00:11:14,720 --> 00:11:18,399
powering through to just get a bit of

00:11:16,560 --> 00:11:19,360
insight on them being able to jump into

00:11:18,399 --> 00:11:21,279
a use case

00:11:19,360 --> 00:11:22,720
and there's a little bit more hard a

00:11:21,279 --> 00:11:24,000
little bit harder to grasp concept

00:11:22,720 --> 00:11:25,200
called watermarking

00:11:24,000 --> 00:11:27,040
right whenever you're talking about

00:11:25,200 --> 00:11:28,000
windows and you say i want to basically

00:11:27,040 --> 00:11:31,120
process data from

00:11:28,000 --> 00:11:32,959
this window to this window well then the

00:11:31,120 --> 00:11:35,200
question is are you talking about

00:11:32,959 --> 00:11:36,320
uh the time where the event was

00:11:35,200 --> 00:11:38,880
generated

00:11:36,320 --> 00:11:39,519
or the ev at the time in which the event

00:11:38,880 --> 00:11:41,120
came

00:11:39,519 --> 00:11:43,040
right and the reason why this is

00:11:41,120 --> 00:11:44,079
important is because if you actually say

00:11:43,040 --> 00:11:47,680
well i want to

00:11:44,079 --> 00:11:50,639
actually do uh windows of the time

00:11:47,680 --> 00:11:51,680
in which the event actually was

00:11:50,639 --> 00:11:53,839
generated

00:11:51,680 --> 00:11:54,800
that means that the event may come later

00:11:53,839 --> 00:11:56,959
right so you

00:11:54,800 --> 00:11:58,320
you may not take it into consideration

00:11:56,959 --> 00:11:59,920
in that window because

00:11:58,320 --> 00:12:01,600
even though it was supposed to it

00:11:59,920 --> 00:12:04,480
arrived later what

00:12:01,600 --> 00:12:06,399
watermarking does it basically says i

00:12:04,480 --> 00:12:08,639
want to make sure that i keep a buffer

00:12:06,399 --> 00:12:10,720
so that if some things that were

00:12:08,639 --> 00:12:13,680
supposed to arrive earlier

00:12:10,720 --> 00:12:14,160
actually come later down the line i want

00:12:13,680 --> 00:12:17,279
to

00:12:14,160 --> 00:12:19,200
take it and including included

00:12:17,279 --> 00:12:21,040
into that older batch right so that's

00:12:19,200 --> 00:12:23,120
basically what water marking is

00:12:21,040 --> 00:12:24,880
in a simple concept right so this is

00:12:23,120 --> 00:12:25,600
basically some of the high level concept

00:12:24,880 --> 00:12:28,000
of streaming

00:12:25,600 --> 00:12:29,120
you know it really encompasses quite a

00:12:28,000 --> 00:12:30,399
lot on those

00:12:29,120 --> 00:12:32,240
and you know once you get your head

00:12:30,399 --> 00:12:32,800
around it it starts becoming a little

00:12:32,240 --> 00:12:36,079
bit more

00:12:32,800 --> 00:12:37,600
um um intuitive of course they're still

00:12:36,079 --> 00:12:38,720
kind of like states and all of those

00:12:37,600 --> 00:12:40,399
things to deal with but

00:12:38,720 --> 00:12:42,079
that in itself is a good enough

00:12:40,399 --> 00:12:42,639
introduction to then dive into the next

00:12:42,079 --> 00:12:44,240
steps

00:12:42,639 --> 00:12:46,240
there are multiple different tools that

00:12:44,240 --> 00:12:48,160
actually tackle and provide you with the

00:12:46,240 --> 00:12:49,760
ability to do stream processing

00:12:48,160 --> 00:12:51,519
flink which provides you with multiple

00:12:49,760 --> 00:12:54,240
languages they also provide you with

00:12:51,519 --> 00:12:55,839
the ability to abstract some of those uh

00:12:54,240 --> 00:12:57,680
string processing components

00:12:55,839 --> 00:12:58,639
kafka streams which is part of kafka is

00:12:57,680 --> 00:12:59,440
the database that we're going to be

00:12:58,639 --> 00:13:01,600
using

00:12:59,440 --> 00:13:03,680
spark streaming forced which is the

00:13:01,600 --> 00:13:04,880
library the python library that again

00:13:03,680 --> 00:13:06,160
we're going to be leveraging

00:13:04,880 --> 00:13:07,839
to be able to do all of our stream

00:13:06,160 --> 00:13:08,880
processing and then other components

00:13:07,839 --> 00:13:10,399
like apache and

00:13:08,880 --> 00:13:12,160
seldom which we're going to be using for

00:13:10,399 --> 00:13:16,160
our machine learning so

00:13:12,160 --> 00:13:18,480
now we're leveraging today

00:13:16,160 --> 00:13:19,680
right so today we're going to be using

00:13:18,480 --> 00:13:20,959
this technology for the stream

00:13:19,680 --> 00:13:23,040
processing we're going to store

00:13:20,959 --> 00:13:24,000
all of the data that comes in on our

00:13:23,040 --> 00:13:26,399
kafka queue

00:13:24,000 --> 00:13:27,120
right and the stream processing will be

00:13:26,399 --> 00:13:29,920
done

00:13:27,120 --> 00:13:30,639
with first right the machine learning

00:13:29,920 --> 00:13:34,160
processing

00:13:30,639 --> 00:13:36,079
will be served with core and our machine

00:13:34,160 --> 00:13:37,920
learning itself will be done by

00:13:36,079 --> 00:13:40,240
psychic learn and space so we're going

00:13:37,920 --> 00:13:42,160
to delve into what that looks like

00:13:40,240 --> 00:13:43,600
so what does the traditional machine

00:13:42,160 --> 00:13:45,120
learning workflow look like well

00:13:43,600 --> 00:13:47,120
so in the traditional machine learning

00:13:45,120 --> 00:13:48,000
you take some data you transform that

00:13:47,120 --> 00:13:49,680
data

00:13:48,000 --> 00:13:52,079
you feed it into a model you train the

00:13:49,680 --> 00:13:54,240
model and then you rinse and repeat

00:13:52,079 --> 00:13:55,279
once you're happy with the accuracy you

00:13:54,240 --> 00:13:58,320
persist that model

00:13:55,279 --> 00:13:59,360
right you deploy that model and then new

00:13:58,320 --> 00:14:02,639
unseen data

00:13:59,360 --> 00:14:05,839
comes in and you get new predictions

00:14:02,639 --> 00:14:06,320
that basically come into us as it goes

00:14:05,839 --> 00:14:08,000
right

00:14:06,320 --> 00:14:09,680
so first you train a model with

00:14:08,000 --> 00:14:11,279
historical data

00:14:09,680 --> 00:14:13,279
and then once you're ready you persisted

00:14:11,279 --> 00:14:14,959
deploy it new data

00:14:13,279 --> 00:14:16,720
new predictions right so we don't see

00:14:14,959 --> 00:14:17,760
dates that's what we're going to be

00:14:16,720 --> 00:14:19,040
basically doing

00:14:17,760 --> 00:14:21,519
so first we're going to be training a

00:14:19,040 --> 00:14:22,240
model what do we have we have 200 000

00:14:21,519 --> 00:14:25,120
comments

00:14:22,240 --> 00:14:26,000
from angry redditers those comments come

00:14:25,120 --> 00:14:28,000
in text

00:14:26,000 --> 00:14:29,519
we're going to have to first convert

00:14:28,000 --> 00:14:30,480
those texts into something that the

00:14:29,519 --> 00:14:33,680
model can

00:14:30,480 --> 00:14:34,959
like read or learn from

00:14:33,680 --> 00:14:36,880
and then we're going to be taking a

00:14:34,959 --> 00:14:40,160
model that is going to

00:14:36,880 --> 00:14:41,040
actually uh um sort of we're going to

00:14:40,160 --> 00:14:43,360
train a model

00:14:41,040 --> 00:14:44,800
with that specific incoming data right

00:14:43,360 --> 00:14:46,399
what is the model going to try to do

00:14:44,800 --> 00:14:49,760
it's going to try to predict

00:14:46,399 --> 00:14:52,240
on whether that comment would have been

00:14:49,760 --> 00:14:53,760
uh moderated or not right that's

00:14:52,240 --> 00:14:55,920
basically what the model is doing

00:14:53,760 --> 00:14:56,800
so we basically first we cleaned the

00:14:55,920 --> 00:15:00,000
text

00:14:56,800 --> 00:15:01,680
we convert it into a um

00:15:00,000 --> 00:15:03,920
in this case tokens we're going to see

00:15:01,680 --> 00:15:07,040
what that is we pass it into

00:15:03,920 --> 00:15:08,800
it to be converted into a vector

00:15:07,040 --> 00:15:10,320
and then we pass it into our model which

00:15:08,800 --> 00:15:10,959
is in this case a logistic regression

00:15:10,320 --> 00:15:12,800
model

00:15:10,959 --> 00:15:14,079
but what does that even mean right in a

00:15:12,800 --> 00:15:17,360
more intuitive perspective

00:15:14,079 --> 00:15:19,519
let's take a um standard uh

00:15:17,360 --> 00:15:20,880
civil uh reddit comment right somebody

00:15:19,519 --> 00:15:23,760
that just wrote you are

00:15:20,880 --> 00:15:25,120
a dummy right so what happens first we

00:15:23,760 --> 00:15:26,480
clean it we pass it through the text

00:15:25,120 --> 00:15:28,480
screen so that becomes

00:15:26,480 --> 00:15:30,320
you are you are dummy right we're

00:15:28,480 --> 00:15:32,240
removing all of the stopwatch

00:15:30,320 --> 00:15:33,519
then we tokenize it what happens there

00:15:32,240 --> 00:15:36,079
well we convert

00:15:33,519 --> 00:15:37,360
it into tokens which in this case it

00:15:36,079 --> 00:15:41,360
abstracts some of those

00:15:37,360 --> 00:15:44,320
pronouns into pron then it basically um

00:15:41,360 --> 00:15:46,160
performs uh uh you know normalization on

00:15:44,320 --> 00:15:47,920
our data so that we can actually

00:15:46,160 --> 00:15:49,360
uh pass it in a much more standardized

00:15:47,920 --> 00:15:52,399
way right so uh

00:15:49,360 --> 00:15:54,480
limitization etc etc right

00:15:52,399 --> 00:15:56,639
then we can actually convert it into the

00:15:54,480 --> 00:15:58,560
vector through the vectorizer

00:15:56,639 --> 00:16:01,040
we then get what are referred to as one

00:15:58,560 --> 00:16:02,560
hot vectors of which token we have

00:16:01,040 --> 00:16:04,320
and then we pass it into our model that

00:16:02,560 --> 00:16:06,399
gives us a prediction a one

00:16:04,320 --> 00:16:07,920
or a zero a one is basically that it

00:16:06,399 --> 00:16:08,399
should have been moderated a zero is

00:16:07,920 --> 00:16:09,759
that

00:16:08,399 --> 00:16:11,920
it wouldn't have it's a very nice

00:16:09,759 --> 00:16:14,160
comment and as we would have assumed

00:16:11,920 --> 00:16:15,440
this one would have been moderated if

00:16:14,160 --> 00:16:16,560
you were to pass it through our model

00:16:15,440 --> 00:16:18,079
right and that's actually what happens

00:16:16,560 --> 00:16:21,120
if you pass it to them

00:16:18,079 --> 00:16:23,440
um now i'm not gonna dive into the very

00:16:21,120 --> 00:16:25,199
specifics of training the model itself

00:16:23,440 --> 00:16:27,360
but one thing that is interesting is we

00:16:25,199 --> 00:16:29,920
actually you actually can actually go

00:16:27,360 --> 00:16:32,079
to the repository and you can see some

00:16:29,920 --> 00:16:33,759
insights that were uncovered when

00:16:32,079 --> 00:16:35,440
going through this data set right you

00:16:33,759 --> 00:16:38,720
can see some of the

00:16:35,440 --> 00:16:41,600
breakdowns of the of the type of

00:16:38,720 --> 00:16:42,160
features that are throughout the data

00:16:41,600 --> 00:16:44,160
set

00:16:42,160 --> 00:16:46,959
you can also see some of the models that

00:16:44,160 --> 00:16:48,560
were compared logistic regression lda

00:16:46,959 --> 00:16:50,639
k nearest neighbors et cetera and how

00:16:48,560 --> 00:16:51,279
they performed so if you're curious

00:16:50,639 --> 00:16:54,560
please do

00:16:51,279 --> 00:16:55,440
go check it out now we have a trained

00:16:54,560 --> 00:16:57,279
model right so

00:16:55,440 --> 00:16:59,600
we're gonna see basically what i

00:16:57,279 --> 00:17:02,160
mentioned here is we have this repo

00:16:59,600 --> 00:17:02,959
in this repo you can find uh the

00:17:02,160 --> 00:17:05,839
contents

00:17:02,959 --> 00:17:08,000
of all of the data analysis if you're

00:17:05,839 --> 00:17:09,600
curious uh do check it out

00:17:08,000 --> 00:17:12,480
so right before i actually go into the

00:17:09,600 --> 00:17:16,319
next uh slide i just want to confirm

00:17:12,480 --> 00:17:16,319
can you uh hear me well now

00:17:16,720 --> 00:17:21,120
yeah we're good okay cool yeah i have

00:17:18,880 --> 00:17:22,480
the vpn

00:17:21,120 --> 00:17:24,160
yeah so again what we're going to be

00:17:22,480 --> 00:17:26,079
doing now is we're going to be taking

00:17:24,160 --> 00:17:27,439
that model that we already deployed

00:17:26,079 --> 00:17:28,720
that we already trained and we're going

00:17:27,439 --> 00:17:29,600
to deploy it right we're going to put it

00:17:28,720 --> 00:17:30,960
in production

00:17:29,600 --> 00:17:33,039
how are we going to do that well we're

00:17:30,960 --> 00:17:34,080
going to need the cop the next few

00:17:33,039 --> 00:17:35,039
components right

00:17:34,080 --> 00:17:36,799
we're going to need the stream

00:17:35,039 --> 00:17:37,120
processing components that are going to

00:17:36,799 --> 00:17:39,520
be

00:17:37,120 --> 00:17:40,480
moving data around we're going to need

00:17:39,520 --> 00:17:42,240
our queue

00:17:40,480 --> 00:17:44,000
that are basically going to be storing

00:17:42,240 --> 00:17:46,320
all of the streaming data

00:17:44,000 --> 00:17:48,320
in what are referred to as topics so

00:17:46,320 --> 00:17:50,240
each topic actually stores

00:17:48,320 --> 00:17:51,520
a set of messages right so that's

00:17:50,240 --> 00:17:52,320
basically our stream is going to be

00:17:51,520 --> 00:17:53,919
managed

00:17:52,320 --> 00:17:55,760
and then we're going to have this

00:17:53,919 --> 00:17:57,360
serving component which is going to be

00:17:55,760 --> 00:17:58,720
serving our model and we're going to see

00:17:57,360 --> 00:18:01,520
how each of those

00:17:58,720 --> 00:18:02,480
uh get generated right but first we're

00:18:01,520 --> 00:18:04,400
gonna see

00:18:02,480 --> 00:18:06,240
the flow what we're gonna have is a

00:18:04,400 --> 00:18:08,320
stream of data that is coming in

00:18:06,240 --> 00:18:09,280
reddit comments that basically our

00:18:08,320 --> 00:18:12,080
stream processor

00:18:09,280 --> 00:18:12,559
will be taking in and then pushing into

00:18:12,080 --> 00:18:14,240
the queue

00:18:12,559 --> 00:18:15,679
into the first topic called reddit

00:18:14,240 --> 00:18:19,200
stream right

00:18:15,679 --> 00:18:21,280
our then next uh stream processor

00:18:19,200 --> 00:18:22,960
is going to be listening to that next

00:18:21,280 --> 00:18:24,880
topic and is going to be

00:18:22,960 --> 00:18:26,080
as soon as one arrives we're going to

00:18:24,880 --> 00:18:26,880
take that message and we're going to

00:18:26,080 --> 00:18:30,240
push it

00:18:26,880 --> 00:18:32,559
into both uh first the prediction api

00:18:30,240 --> 00:18:34,160
to you know see whether that comment

00:18:32,559 --> 00:18:34,960
should be moderated or not based on the

00:18:34,160 --> 00:18:37,200
prediction

00:18:34,960 --> 00:18:38,320
and then on the response we're gonna be

00:18:37,200 --> 00:18:40,640
basically send it

00:18:38,320 --> 00:18:42,559
to our topic where all of the responses

00:18:40,640 --> 00:18:45,840
are stored

00:18:42,559 --> 00:18:47,280
and then if it's a you know should be

00:18:45,840 --> 00:18:49,360
moderated prediction

00:18:47,280 --> 00:18:50,480
we're gonna send it to the topic of

00:18:49,360 --> 00:18:52,000
alert right

00:18:50,480 --> 00:18:54,240
so basically what we're gonna be doing

00:18:52,000 --> 00:18:57,200
let's dive into each of the components

00:18:54,240 --> 00:18:58,400
so specifically first into the reddit

00:18:57,200 --> 00:19:00,720
source right so

00:18:58,400 --> 00:19:02,640
to generate uh reddit comments we're

00:19:00,720 --> 00:19:05,360
going to be leveraging first

00:19:02,640 --> 00:19:07,679
in first once you create the app

00:19:05,360 --> 00:19:09,919
component so if you've used uh

00:19:07,679 --> 00:19:10,960
flask in the past it basically is

00:19:09,919 --> 00:19:14,240
something as simple as

00:19:10,960 --> 00:19:17,280
app equals you know first initialization

00:19:14,240 --> 00:19:18,240
and the location of your queue that's

00:19:17,280 --> 00:19:20,720
basically it

00:19:18,240 --> 00:19:22,160
once you have the app you can define as

00:19:20,720 --> 00:19:25,200
a decorator

00:19:22,160 --> 00:19:26,400
exactly what are the functions that you

00:19:25,200 --> 00:19:28,240
want to be running

00:19:26,400 --> 00:19:30,080
right specifically this one is not going

00:19:28,240 --> 00:19:32,720
to be working on

00:19:30,080 --> 00:19:33,200
the queue it's just going to be running

00:19:32,720 --> 00:19:35,600
every

00:19:33,200 --> 00:19:36,640
0.1 seconds right so it's just going to

00:19:35,600 --> 00:19:38,160
be iterating

00:19:36,640 --> 00:19:40,240
what it's going to be doing is going to

00:19:38,160 --> 00:19:42,240
fetch a reddit comment

00:19:40,240 --> 00:19:44,640
it's going to create the data and it's

00:19:42,240 --> 00:19:45,440
going to then push it into the reddit

00:19:44,640 --> 00:19:48,880
stream

00:19:45,440 --> 00:19:50,240
topic right so app.topic reddit stream

00:19:48,880 --> 00:19:52,400
send right so that's basically going to

00:19:50,240 --> 00:19:53,200
send it to the reddit stream then it's

00:19:52,400 --> 00:19:56,160
going to

00:19:53,200 --> 00:19:56,559
uh go uh into developing the emo predict

00:19:56,160 --> 00:19:58,960
again

00:19:56,559 --> 00:19:59,840
app.agent we're going to create a topic

00:19:58,960 --> 00:20:01,360
reddit stream

00:19:59,840 --> 00:20:02,880
it's listening to that stream that we

00:20:01,360 --> 00:20:04,960
just created and

00:20:02,880 --> 00:20:06,559
all of the tokens are going to be coming

00:20:04,960 --> 00:20:07,360
in so the actual comment is going to be

00:20:06,559 --> 00:20:09,600
coming in

00:20:07,360 --> 00:20:10,799
we're going to then send it to selden

00:20:09,600 --> 00:20:11,760
that's basically the next thing i'm

00:20:10,799 --> 00:20:14,000
going to show

00:20:11,760 --> 00:20:16,000
then we're going to get that response

00:20:14,000 --> 00:20:18,000
whether it should be moderated or not

00:20:16,000 --> 00:20:19,520
and then we're going to be sending it to

00:20:18,000 --> 00:20:21,520
the prediction topic

00:20:19,520 --> 00:20:22,640
and if the probability is higher we're

00:20:21,520 --> 00:20:24,640
going to be sending it

00:20:22,640 --> 00:20:26,480
to the alert topic right so that's

00:20:24,640 --> 00:20:28,799
basically what this one does

00:20:26,480 --> 00:20:30,000
then we're going to be actually serving

00:20:28,799 --> 00:20:32,720
our model how do we actually

00:20:30,000 --> 00:20:34,559
serve our model that prediction function

00:20:32,720 --> 00:20:37,280
that prediction function that you saw

00:20:34,559 --> 00:20:38,240
here what that prediction function is is

00:20:37,280 --> 00:20:41,280
basically a

00:20:38,240 --> 00:20:42,559
rest client that uses the selling client

00:20:41,280 --> 00:20:44,400
to send a request right so this is

00:20:42,559 --> 00:20:45,600
basically a just sending a prediction

00:20:44,400 --> 00:20:47,600
request right so

00:20:45,600 --> 00:20:49,679
the prediction request is to that url

00:20:47,600 --> 00:20:52,240
which is inside of the cluster

00:20:49,679 --> 00:20:53,039
and their response basically is what is

00:20:52,240 --> 00:20:55,520
going to be

00:20:53,039 --> 00:20:56,400
uh forwarded with the with the

00:20:55,520 --> 00:20:58,159
predictions

00:20:56,400 --> 00:20:59,760
right and what does that look like with

00:20:58,159 --> 00:21:02,240
selden model serving

00:20:59,760 --> 00:21:02,799
that's basically taking that code that

00:21:02,240 --> 00:21:05,360
we just

00:21:02,799 --> 00:21:07,120
uh trained and converted into a fully

00:21:05,360 --> 00:21:09,120
fledged microservice right

00:21:07,120 --> 00:21:10,400
well that basically looks when you wrap

00:21:09,120 --> 00:21:13,520
models with selden

00:21:10,400 --> 00:21:15,440
you create a python class everything you

00:21:13,520 --> 00:21:17,039
put on the predict function

00:21:15,440 --> 00:21:18,880
is basically what is exposed in the

00:21:17,039 --> 00:21:21,200
restful api right so

00:21:18,880 --> 00:21:22,000
in this case we are actually passing the

00:21:21,200 --> 00:21:23,679
input

00:21:22,000 --> 00:21:25,440
to our model right so if you remember

00:21:23,679 --> 00:21:26,640
that model that we trained before

00:21:25,440 --> 00:21:28,480
it's basically just going through all of

00:21:26,640 --> 00:21:30,480
the steps and returning the predictions

00:21:28,480 --> 00:21:32,559
so anything that you send to the request

00:21:30,480 --> 00:21:34,640
api basically gets passed

00:21:32,559 --> 00:21:35,760
through all of those uh steps and then

00:21:34,640 --> 00:21:37,280
it gets returned

00:21:35,760 --> 00:21:39,280
right and that's basically what that

00:21:37,280 --> 00:21:42,080
component does right so you

00:21:39,280 --> 00:21:43,520
you basically send a request uh to here

00:21:42,080 --> 00:21:44,720
that basically gets passed through this

00:21:43,520 --> 00:21:47,200
predict function

00:21:44,720 --> 00:21:48,559
that basically goes into a transforms

00:21:47,200 --> 00:21:50,240
and then returns

00:21:48,559 --> 00:21:51,679
what our model predicted right so that's

00:21:50,240 --> 00:21:53,679
how how seldom

00:21:51,679 --> 00:21:55,360
uh wraps models right so that's how you

00:21:53,679 --> 00:21:57,120
can actually um do it

00:21:55,360 --> 00:21:58,720
now specifically in regards to that flow

00:21:57,120 --> 00:22:00,240
let's just kind of like recap of what

00:21:58,720 --> 00:22:02,640
happened there

00:22:00,240 --> 00:22:04,400
a new comment comes in right we created

00:22:02,640 --> 00:22:05,600
a stream processor that is just

00:22:04,400 --> 00:22:07,360
listening um

00:22:05,600 --> 00:22:09,039
that is just actually fetching comments

00:22:07,360 --> 00:22:10,159
and then pushing them into the reddit

00:22:09,039 --> 00:22:11,679
stream topic

00:22:10,159 --> 00:22:13,840
then we have another stream processor

00:22:11,679 --> 00:22:16,880
that fetches everything from

00:22:13,840 --> 00:22:17,679
the reddit streams and then sends it for

00:22:16,880 --> 00:22:20,240
prediction

00:22:17,679 --> 00:22:21,919
as a restful api and then all of the

00:22:20,240 --> 00:22:22,720
predictions get pushed to the prediction

00:22:21,919 --> 00:22:25,760
topic

00:22:22,720 --> 00:22:26,880
and all of the um um all of the

00:22:25,760 --> 00:22:29,200
predictions that are

00:22:26,880 --> 00:22:30,080
uh you know higher than one percent so

00:22:29,200 --> 00:22:32,080
higher than

00:22:30,080 --> 00:22:34,960
uh a percentage of probability or that

00:22:32,080 --> 00:22:36,960
they're predicted that they should be

00:22:34,960 --> 00:22:38,320
moderated then they get sent to the

00:22:36,960 --> 00:22:40,240
alert topic

00:22:38,320 --> 00:22:41,520
right so so that's basically that's

00:22:40,240 --> 00:22:43,679
basically it right and

00:22:41,520 --> 00:22:45,120
and ultimately the code require is this

00:22:43,679 --> 00:22:47,440
basically to wrap your model

00:22:45,120 --> 00:22:48,240
and containerize it so that's basically

00:22:47,440 --> 00:22:51,120
how you how you

00:22:48,240 --> 00:22:52,320
uh build your own sort of like server uh

00:22:51,120 --> 00:22:55,440
for the models

00:22:52,320 --> 00:22:59,360
you need basically the components that

00:22:55,440 --> 00:23:02,559
you know receives uh the raw uh comments

00:22:59,360 --> 00:23:03,840
and sends them to that uh machine

00:23:02,559 --> 00:23:06,720
learning service

00:23:03,840 --> 00:23:08,799
right and then you need just the thing

00:23:06,720 --> 00:23:09,600
that basically is continuously

00:23:08,799 --> 00:23:11,840
collecting

00:23:09,600 --> 00:23:13,679
reddit comments right and if you

00:23:11,840 --> 00:23:15,200
actually like take a look at the uh

00:23:13,679 --> 00:23:17,120
repository this probably would be much

00:23:15,200 --> 00:23:18,480
more intuitive uh because you can

00:23:17,120 --> 00:23:19,679
actually try it yourself right

00:23:18,480 --> 00:23:21,600
i think i think that's that's the main

00:23:19,679 --> 00:23:24,320
thing one thing to mention

00:23:21,600 --> 00:23:24,720
is that uh one of the things that we're

00:23:24,320 --> 00:23:27,360
doing

00:23:24,720 --> 00:23:29,679
uh with with selden is actually

00:23:27,360 --> 00:23:30,880
simplifying the way that uh seldom works

00:23:29,679 --> 00:23:34,000
right because right now

00:23:30,880 --> 00:23:34,400
you need a stream processor to listen to

00:23:34,000 --> 00:23:37,679
the

00:23:34,400 --> 00:23:40,080
input data send it as a request

00:23:37,679 --> 00:23:42,480
and then you know push that into the

00:23:40,080 --> 00:23:44,960
next topic right so that's basically how

00:23:42,480 --> 00:23:46,240
it was kind of like the main push right

00:23:44,960 --> 00:23:50,000
now we're actually uh

00:23:46,240 --> 00:23:52,400
releasing in um so open source repo

00:23:50,000 --> 00:23:53,440
the ability to not just expose a restful

00:23:52,400 --> 00:23:55,120
api

00:23:53,440 --> 00:23:57,279
but to be able to connect directly to

00:23:55,120 --> 00:23:58,080
kafka and this is quite interesting

00:23:57,279 --> 00:23:59,840
because

00:23:58,080 --> 00:24:01,600
this allows you to basically deploy your

00:23:59,840 --> 00:24:04,480
model and

00:24:01,600 --> 00:24:05,919
consume topics directly and then push to

00:24:04,480 --> 00:24:07,679
topics directly

00:24:05,919 --> 00:24:09,200
and um yeah i mean that's not something

00:24:07,679 --> 00:24:10,880
that we're currently working on

00:24:09,200 --> 00:24:12,720
uh for the community you know we would

00:24:10,880 --> 00:24:14,640
love to to hear any sort of feedbacks

00:24:12,720 --> 00:24:18,320
it's currently an open pr so

00:24:14,640 --> 00:24:21,600
uh do feel free to to come in and uh

00:24:18,320 --> 00:24:23,440
definitely add your thoughts

00:24:21,600 --> 00:24:25,039
or anything and you know we definitely

00:24:23,440 --> 00:24:27,760
always welcome um

00:24:25,039 --> 00:24:28,799
any sort of contribution whether it is

00:24:27,760 --> 00:24:32,240
just on the

00:24:28,799 --> 00:24:34,640
um thoughts or even if it is on uh

00:24:32,240 --> 00:24:37,520
suggestions or comments in there

00:24:34,640 --> 00:24:38,080
so with that i think we've managed to go

00:24:37,520 --> 00:24:40,159
through

00:24:38,080 --> 00:24:41,600
all of the the core components we

00:24:40,159 --> 00:24:43,440
managed to cover a conceptual

00:24:41,600 --> 00:24:45,760
introduction to stream processing

00:24:43,440 --> 00:24:46,480
uh we've managed to dive into the

00:24:45,760 --> 00:24:50,400
concept of

00:24:46,480 --> 00:24:52,960
real-time machine learning given that

00:24:50,400 --> 00:24:53,919
that nuance of having to train the model

00:24:52,960 --> 00:24:56,559
and then we've discussed

00:24:53,919 --> 00:24:58,559
some of the trade-offs across some of

00:24:56,559 --> 00:25:01,279
the different

00:24:58,559 --> 00:25:02,880
concepts and terminology and type of uh

00:25:01,279 --> 00:25:05,039
stream processing

00:25:02,880 --> 00:25:06,559
approaches together with a hands-on use

00:25:05,039 --> 00:25:10,640
case uh to tackle

00:25:06,559 --> 00:25:13,919
the challenge of um i guess angry people

00:25:10,640 --> 00:25:15,039
in the internet and with that said uh uh

00:25:13,919 --> 00:25:16,720
uh thank you very much

00:25:15,039 --> 00:25:18,799
and uh you know i'll be on the on the

00:25:16,720 --> 00:25:21,919
discord um uh we have the

00:25:18,799 --> 00:25:24,480
the the the talk which you can find uh

00:25:21,919 --> 00:25:25,919
i think if you search uh for streaming

00:25:24,480 --> 00:25:27,039
uh and then if you have any sort of

00:25:25,919 --> 00:25:30,320
questions

00:25:27,039 --> 00:25:32,720
or any ideas or extensions into this

00:25:30,320 --> 00:25:34,559
uh more than keem to actually delve into

00:25:32,720 --> 00:25:37,520
into some of that

00:25:34,559 --> 00:25:39,360
so with that i'll i'll pause that and

00:25:37,520 --> 00:25:42,799
i'll pass it over

00:25:39,360 --> 00:25:44,470
back to jason thank you very much

00:25:42,799 --> 00:25:49,109
alejandro

00:25:44,470 --> 00:25:49,109
[Applause]

00:25:50,559 --> 00:25:54,000
excellent talk so like he said uh please

00:25:53,279 --> 00:25:57,279
check out

00:25:54,000 --> 00:26:00,320
his room over there in discord um and

00:25:57,279 --> 00:26:00,720
uh you can uh just in discord just hit

00:26:00,320 --> 00:26:02,880
control

00:26:00,720 --> 00:26:04,240
k type in the name of the talk and and

00:26:02,880 --> 00:26:05,279
uh part of the name of the talk you'll

00:26:04,240 --> 00:26:07,600
find it and you can ask

00:26:05,279 --> 00:26:11,840
questions there so uh thank you very

00:26:07,600 --> 00:26:11,840

YouTube URL: https://www.youtube.com/watch?v=Kkt986ZhKxs


