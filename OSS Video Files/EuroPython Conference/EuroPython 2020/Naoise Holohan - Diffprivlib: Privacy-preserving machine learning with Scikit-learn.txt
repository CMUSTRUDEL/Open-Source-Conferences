Title: Naoise Holohan - Diffprivlib: Privacy-preserving machine learning with Scikit-learn
Publication date: 2020-09-21
Playlist: EuroPython 2020
Description: 
	"Diffprivlib: Privacy-preserving machine learning with Scikit-learn
EuroPython 2020 - Talk - 2020-07-23 - Parrot Data Science
Online

By Naoise Holohan

Data privacy is having an ever-increasing impact on the way data is stored, processed, accessed and utilised, as the legal and ethical effects of data protection regulations take effect around the globe. Differential privacy, considered by many to be the strongest privacy guarantee currently available, gives robust, provable guarantees on protecting privacy, and allows tasks to be completed on data with guarantees on the privacy of individuals in that data. This naturally extends to machine learning, where training datasets can contain sensitive personal information, that are vulnerable to privacy attacks on trained models.
By using differential privacy in the training process, a machine learning model can be trained to accurately represent the dataset at large, but without inadvertently revealing sensitive information about an individual. Diffprivlib is the first library of its kind to leverage the power of differential privacy with scikit-learn and numpy to give data scientists and researchers access to the tools to train accurate, portable models with robust, provable privacy guarantees built-in.
In this talk, we will introduce attendees to the idea of differential privacy, why it is necessary in today's world, and how diffprivlib can be seamlessly integrated within existing scripts to protect your trained models from privacy vulnerabilities. Attendees will be expected to have a basic understanding of sklearn (i.e., how to initialise, fit and predict a model). No knowledge of data privacy or differential privacy will be assumed or required.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2020.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:06,160 --> 00:00:10,240
right

00:00:06,799 --> 00:00:11,360
um thank you all for uh attending my

00:00:10,240 --> 00:00:14,320
talk this morning

00:00:11,360 --> 00:00:16,080
and so my name is nisha houlahan i'm a

00:00:14,320 --> 00:00:19,840
research scientist in ibm research

00:00:16,080 --> 00:00:22,880
in our dublin lab here in ireland and

00:00:19,840 --> 00:00:25,359
on the ai privacy and security team i

00:00:22,880 --> 00:00:27,439
work on differential privacy

00:00:25,359 --> 00:00:29,760
and what i'm going to talk to you about

00:00:27,439 --> 00:00:32,480
today is our work in the area of privacy

00:00:29,760 --> 00:00:34,160
preserving machine learning

00:00:32,480 --> 00:00:35,680
using the great power of psychic learn

00:00:34,160 --> 00:00:40,079
and python to

00:00:35,680 --> 00:00:43,440
help us on our way so to begin with

00:00:40,079 --> 00:00:45,440
the area of data privacy as we now know

00:00:43,440 --> 00:00:47,520
it can trace its origins back to about

00:00:45,440 --> 00:00:49,840
the 1960s

00:00:47,520 --> 00:00:52,079
and a lot of tools and algorithms have

00:00:49,840 --> 00:00:54,640
been developed since then that have

00:00:52,079 --> 00:00:56,399
um that are still in use to this day and

00:00:54,640 --> 00:00:57,680
are still very powerful for protecting

00:00:56,399 --> 00:01:00,800
data

00:00:57,680 --> 00:01:02,800
but the the ecosystem in which those

00:01:00,800 --> 00:01:04,320
techniques now reside have changed a lot

00:01:02,800 --> 00:01:06,880
in the last 20 years

00:01:04,320 --> 00:01:09,280
to the point where we now have a big

00:01:06,880 --> 00:01:11,280
risk of being able to

00:01:09,280 --> 00:01:13,360
link these anonymized datasets with

00:01:11,280 --> 00:01:16,560
external data sources

00:01:13,360 --> 00:01:17,040
and re-identifying individuals in this

00:01:16,560 --> 00:01:18,640
in our

00:01:17,040 --> 00:01:20,799
in these anonymized data sets or

00:01:18,640 --> 00:01:24,640
exposing other sensitive attributes that

00:01:20,799 --> 00:01:26,640
we wouldn't have wanted to originally

00:01:24,640 --> 00:01:28,240
there are many examples of these kind of

00:01:26,640 --> 00:01:30,560
attacks happening out in the wild and

00:01:28,240 --> 00:01:32,640
i've listed a couple of examples here

00:01:30,560 --> 00:01:34,240
so in the first example with netflix

00:01:32,640 --> 00:01:36,799
they publish a data set of

00:01:34,240 --> 00:01:38,479
anonymized movie ratings for their

00:01:36,799 --> 00:01:40,400
netflix prize competition

00:01:38,479 --> 00:01:41,840
they wanted researchers to improve their

00:01:40,400 --> 00:01:44,640
recommendation algorithm

00:01:41,840 --> 00:01:45,759
and they gave them some some data to use

00:01:44,640 --> 00:01:48,000
to help them

00:01:45,759 --> 00:01:50,000
um but researchers were able to link it

00:01:48,000 --> 00:01:51,600
with the publicly available internet

00:01:50,000 --> 00:01:55,040
movie database

00:01:51,600 --> 00:01:57,360
and um expose individuals in netflix

00:01:55,040 --> 00:01:58,880
in the netflix data set and expose the i

00:01:57,360 --> 00:02:00,000
guess the more sensitive ratings that

00:01:58,880 --> 00:02:01,439
they would have given

00:02:00,000 --> 00:02:05,759
to netflix that maybe they wouldn't have

00:02:01,439 --> 00:02:08,239
given to a public service like imdb

00:02:05,759 --> 00:02:10,879
in the aol case then they published a

00:02:08,239 --> 00:02:11,760
data set of anonymized internet search

00:02:10,879 --> 00:02:13,599
histories

00:02:11,760 --> 00:02:15,360
and reporters in the new york times are

00:02:13,599 --> 00:02:17,680
able to dig into the data and

00:02:15,360 --> 00:02:20,319
re-identify one individual in the data

00:02:17,680 --> 00:02:21,680
set and expose her entire search history

00:02:20,319 --> 00:02:23,920
which i'm sure for a lot of us would be

00:02:21,680 --> 00:02:26,080
quite a sensitive matter

00:02:23,920 --> 00:02:28,400
and finally then the new york city taxi

00:02:26,080 --> 00:02:30,800
and limousine commission published a

00:02:28,400 --> 00:02:32,160
data set of anonymized taxi trip records

00:02:30,800 --> 00:02:35,519
a few years ago

00:02:32,160 --> 00:02:36,319
and a blogger was able to attack the

00:02:35,519 --> 00:02:38,000
data set

00:02:36,319 --> 00:02:39,760
and link it with photographs of

00:02:38,000 --> 00:02:40,959
celebrities getting into taxis in new

00:02:39,760 --> 00:02:43,840
york city

00:02:40,959 --> 00:02:45,120
and re-identify though or link those

00:02:43,840 --> 00:02:47,120
records together and

00:02:45,120 --> 00:02:48,319
find out where the celebrities were

00:02:47,120 --> 00:02:50,319
traveling to and from

00:02:48,319 --> 00:02:51,840
and how much they tipped their driver i

00:02:50,319 --> 00:02:54,000
think in this case it's

00:02:51,840 --> 00:02:55,280
bradley cooper getting it into a taxi in

00:02:54,000 --> 00:02:57,680
manhattan

00:02:55,280 --> 00:02:59,360
um so all of these are examples of data

00:02:57,680 --> 00:03:00,800
sets that were anonymized using

00:02:59,360 --> 00:03:02,400
traditional methods

00:03:00,800 --> 00:03:04,640
published in the wild and then

00:03:02,400 --> 00:03:07,360
subsequently attacked

00:03:04,640 --> 00:03:10,720
but the um the privacy risks of data

00:03:07,360 --> 00:03:12,239
extend far beyond these kind of examples

00:03:10,720 --> 00:03:14,239
they extend to publishing simple

00:03:12,239 --> 00:03:17,120
statistics on databases

00:03:14,239 --> 00:03:18,480
um and being able to do to run database

00:03:17,120 --> 00:03:20,000
reconstruction attacks on those

00:03:18,480 --> 00:03:22,000
statistics to

00:03:20,000 --> 00:03:24,319
basically reconstruct the original data

00:03:22,000 --> 00:03:26,159
set and it also extends to machine

00:03:24,319 --> 00:03:27,040
learning models i guess more personal to

00:03:26,159 --> 00:03:29,200
this talk

00:03:27,040 --> 00:03:30,799
where you can reverse engineer the

00:03:29,200 --> 00:03:32,480
machine learning model to find out

00:03:30,799 --> 00:03:34,480
information about the original training

00:03:32,480 --> 00:03:38,480
data set

00:03:34,480 --> 00:03:39,519
so following on from these i guess the

00:03:38,480 --> 00:03:41,200
failings of

00:03:39,519 --> 00:03:43,040
traditional anonymization in the current

00:03:41,200 --> 00:03:46,319
world the idea of differential privacy

00:03:43,040 --> 00:03:48,799
was first conceived in 2006.

00:03:46,319 --> 00:03:50,239
and the key idea of differential privacy

00:03:48,799 --> 00:03:53,280
is that we introduce

00:03:50,239 --> 00:03:55,040
random noise to blur the data in such a

00:03:53,280 --> 00:03:57,280
way that we preserve the privacy of

00:03:55,040 --> 00:04:01,040
individuals in the data set

00:03:57,280 --> 00:04:03,439
but still allow population trends to be

00:04:01,040 --> 00:04:04,720
accurately observable

00:04:03,439 --> 00:04:06,560
and the unique selling point of

00:04:04,720 --> 00:04:07,840
differential privacy is that it is

00:04:06,560 --> 00:04:10,159
future proof

00:04:07,840 --> 00:04:12,560
and by future proof i mean that there's

00:04:10,159 --> 00:04:14,560
no data set that can be published

00:04:12,560 --> 00:04:16,400
tomorrow or in 10 years time that can

00:04:14,560 --> 00:04:17,120
undo the anonymization we've just

00:04:16,400 --> 00:04:18,720
applied

00:04:17,120 --> 00:04:20,400
there's no data set that we can publish

00:04:18,720 --> 00:04:21,440
to undo the the random noise that we've

00:04:20,400 --> 00:04:23,280
added

00:04:21,440 --> 00:04:25,280
and that makes differential privacy the

00:04:23,280 --> 00:04:26,080
strongest privacy guarantee that we have

00:04:25,280 --> 00:04:27,520
at the moment

00:04:26,080 --> 00:04:29,840
and that's why it's such a desirable

00:04:27,520 --> 00:04:32,080
area of research at the moment

00:04:29,840 --> 00:04:34,160
and differential privacy also introduced

00:04:32,080 --> 00:04:35,759
the idea of a privacy budget being

00:04:34,160 --> 00:04:37,440
attached to queries

00:04:35,759 --> 00:04:39,040
so we always talk about epsilon and

00:04:37,440 --> 00:04:41,360
differential privacy

00:04:39,040 --> 00:04:43,360
and this epsilon is linked to the amount

00:04:41,360 --> 00:04:45,199
of privacy leakage that we get when we

00:04:43,360 --> 00:04:46,880
ask a query

00:04:45,199 --> 00:04:48,560
typically when you have a data set you

00:04:46,880 --> 00:04:50,240
want to limit the amount of

00:04:48,560 --> 00:04:52,479
privacy budget that can be spent or the

00:04:50,240 --> 00:04:55,120
amount of privacy that can be leaked

00:04:52,479 --> 00:04:56,479
and that is encapsulated in this privacy

00:04:55,120 --> 00:04:57,680
budget

00:04:56,479 --> 00:04:59,759
and it's very easy when you're running

00:04:57,680 --> 00:05:01,199
queries to keep track of the privacy

00:04:59,759 --> 00:05:03,840
budget and add them up

00:05:01,199 --> 00:05:05,520
and to give a total privacy budget at

00:05:03,840 --> 00:05:06,800
the end of the privacy budget spend at

00:05:05,520 --> 00:05:10,720
the end of a

00:05:06,800 --> 00:05:12,080
series of queries so to look at a kind

00:05:10,720 --> 00:05:15,520
of a simple schematic

00:05:12,080 --> 00:05:17,199
of um of this if we have a sensitive

00:05:15,520 --> 00:05:18,800
data set from which we're looking to

00:05:17,199 --> 00:05:20,800
extract some kind of

00:05:18,800 --> 00:05:22,080
knowledge or information in a privacy

00:05:20,800 --> 00:05:23,759
preserving way

00:05:22,080 --> 00:05:25,520
um which we then want to be able to pass

00:05:23,759 --> 00:05:29,039
on to a data analyst

00:05:25,520 --> 00:05:31,440
um with without uh posing a threat to

00:05:29,039 --> 00:05:35,039
the individuals in the data set

00:05:31,440 --> 00:05:36,960
um we can extend this

00:05:35,039 --> 00:05:38,720
uh use case to a machine learning

00:05:36,960 --> 00:05:40,560
environment where we have a sensitive

00:05:38,720 --> 00:05:43,600
dataset being fed into a

00:05:40,560 --> 00:05:44,160
machine learning or ai algorithm and for

00:05:43,600 --> 00:05:45,600
privacy

00:05:44,160 --> 00:05:47,919
say we're going to inject differential

00:05:45,600 --> 00:05:49,840
privacy in some way into the

00:05:47,919 --> 00:05:52,160
training process and then pass the

00:05:49,840 --> 00:05:53,440
trained algorithm onto the data analyst

00:05:52,160 --> 00:05:55,840
and because of the guarantees of

00:05:53,440 --> 00:05:57,919
differential privacy

00:05:55,840 --> 00:05:59,759
we know that there are mathematical

00:05:57,919 --> 00:06:01,759
guarantees on the amount that the data

00:05:59,759 --> 00:06:03,360
analyst can infer about individuals in

00:06:01,759 --> 00:06:05,120
the sensitive data set

00:06:03,360 --> 00:06:07,680
and that gives us great comfort in being

00:06:05,120 --> 00:06:10,080
able to pass this information on

00:06:07,680 --> 00:06:12,880
to a to an external source who over over

00:06:10,080 --> 00:06:14,880
whom we don't have any control

00:06:12,880 --> 00:06:16,639
and at ibm research what we've done is

00:06:14,880 --> 00:06:18,240
we have built diff prove lib

00:06:16,639 --> 00:06:19,919
that does all the important stuff here

00:06:18,240 --> 00:06:21,680
in the middle it does the machine

00:06:19,919 --> 00:06:23,360
learning with differential privacy built

00:06:21,680 --> 00:06:24,880
in

00:06:23,360 --> 00:06:26,240
and that means that we can train machine

00:06:24,880 --> 00:06:28,160
learning models with differential

00:06:26,240 --> 00:06:30,880
privacy on sensitive data sets

00:06:28,160 --> 00:06:31,680
and then passing them on outside any uh

00:06:30,880 --> 00:06:36,800
sensitive on

00:06:31,680 --> 00:06:36,800
our secure enclave to external parties

00:06:37,199 --> 00:06:40,880
so our approach in building div

00:06:38,720 --> 00:06:42,319
privilege was to obviously to use python

00:06:40,880 --> 00:06:43,759
which is a very popular programming

00:06:42,319 --> 00:06:45,600
language for machine learning and data

00:06:43,759 --> 00:06:48,160
analytics

00:06:45,600 --> 00:06:48,800
we wanted to use i guess the de facto

00:06:48,160 --> 00:06:51,599
standards

00:06:48,800 --> 00:06:55,440
in data analytics and machine learning

00:06:51,599 --> 00:06:55,440
that are numpy and scikit-learn

00:06:55,599 --> 00:07:01,599
and and build upon them to add our

00:06:59,039 --> 00:07:03,759
differential privacy capabilities

00:07:01,599 --> 00:07:06,319
a core pillar of our work for diff

00:07:03,759 --> 00:07:08,160
privilege was to

00:07:06,319 --> 00:07:10,160
ensure an almost identical user

00:07:08,160 --> 00:07:11,360
experience to that of numpy and

00:07:10,160 --> 00:07:13,520
scikit-learn

00:07:11,360 --> 00:07:15,360
and that extended to having a lot of

00:07:13,520 --> 00:07:17,120
default parameter settings for the

00:07:15,360 --> 00:07:20,479
privacy aspects of

00:07:17,120 --> 00:07:22,800
of these functions to ensure that

00:07:20,479 --> 00:07:24,560
anybody who was familiar with numpy and

00:07:22,800 --> 00:07:26,319
scikit-learn would automatically be

00:07:24,560 --> 00:07:28,000
familiar with diff priv lib before they

00:07:26,319 --> 00:07:29,599
even started using it

00:07:28,000 --> 00:07:31,919
i think by and large we've we've

00:07:29,599 --> 00:07:35,599
achieved that goal um

00:07:31,919 --> 00:07:35,599
which you'll see shortly i'm sure

00:07:36,319 --> 00:07:40,479
so in a nutshell uh here we have some uh

00:07:38,880 --> 00:07:41,840
a quick code snippet which

00:07:40,479 --> 00:07:43,280
again if you're familiar with circuit

00:07:41,840 --> 00:07:44,240
learn this should be fairly familiar to

00:07:43,280 --> 00:07:46,800
you too

00:07:44,240 --> 00:07:47,919
um so in a nutshell diff privilab is a

00:07:46,800 --> 00:07:49,280
it's a

00:07:47,919 --> 00:07:51,360
library for doing machine learning with

00:07:49,280 --> 00:07:54,000
differential privacy built right in

00:07:51,360 --> 00:07:54,400
there's no expertise required the user

00:07:54,000 --> 00:07:55,680
doesn't

00:07:54,400 --> 00:07:57,599
need to know anything about differential

00:07:55,680 --> 00:07:58,400
privacy or even data privacy to get up

00:07:57,599 --> 00:07:59,840
and running

00:07:58,400 --> 00:08:01,520
again because of all the default

00:07:59,840 --> 00:08:03,360
parameter settings and the very similar

00:08:01,520 --> 00:08:05,919
user experience

00:08:03,360 --> 00:08:07,199
it's open source it's up on github it's

00:08:05,919 --> 00:08:08,479
free to use and to modify to your

00:08:07,199 --> 00:08:11,360
heart's content

00:08:08,479 --> 00:08:12,000
it's easy to install with pip and it's

00:08:11,360 --> 00:08:13,919
integrated again

00:08:12,000 --> 00:08:15,440
as i said with scikit-learn and numpy to

00:08:13,919 --> 00:08:17,360
get you up and running quickly

00:08:15,440 --> 00:08:19,680
and finally then it's easily integrated

00:08:17,360 --> 00:08:22,000
within existing scripts

00:08:19,680 --> 00:08:23,680
so typically if you have a a script

00:08:22,000 --> 00:08:25,520
that's running a bit of code for machine

00:08:23,680 --> 00:08:27,039
learning or data analytics

00:08:25,520 --> 00:08:29,039
and in one or two lines of code you

00:08:27,039 --> 00:08:30,879
should be able to replace the

00:08:29,039 --> 00:08:32,479
psychic learner the numpy functions with

00:08:30,879 --> 00:08:34,240
their div privilege equivalence

00:08:32,479 --> 00:08:36,560
and the script should run as it did

00:08:34,240 --> 00:08:40,880
before but with the added confidence now

00:08:36,560 --> 00:08:43,599
of its satisfying differential privacy

00:08:40,880 --> 00:08:45,440
so before i dig into some code um we

00:08:43,599 --> 00:08:48,640
just have a quick look at the four main

00:08:45,440 --> 00:08:52,000
modules of div lib the first of those

00:08:48,640 --> 00:08:52,640
is the mechanisms so at differentially

00:08:52,000 --> 00:08:55,360
private or

00:08:52,640 --> 00:08:56,800
differential privacy mechanism is the

00:08:55,360 --> 00:08:58,000
the basic building blocks of

00:08:56,800 --> 00:09:00,160
differential privacy

00:08:58,000 --> 00:09:03,600
they're the pieces of code that actually

00:09:00,160 --> 00:09:05,519
add the random noise to the data

00:09:03,600 --> 00:09:06,880
typically a user of diff privilege won't

00:09:05,519 --> 00:09:08,480
actually come into contact with any

00:09:06,880 --> 00:09:09,440
mechanisms because they're all used

00:09:08,480 --> 00:09:11,360
under the hood

00:09:09,440 --> 00:09:12,800
um in the tools and the models that we

00:09:11,360 --> 00:09:15,200
have to

00:09:12,800 --> 00:09:16,959
achieve differential privacy in essence

00:09:15,200 --> 00:09:17,680
a mechanism is just a probability

00:09:16,959 --> 00:09:19,680
distribution

00:09:17,680 --> 00:09:23,839
from which you are which we use to to

00:09:19,680 --> 00:09:23,839
add noise to the data

00:09:24,000 --> 00:09:28,399
the next module then is the models

00:09:26,160 --> 00:09:31,680
module which is the scikit-learn

00:09:28,399 --> 00:09:35,600
part of differivlib um we've

00:09:31,680 --> 00:09:37,040
a number of machine learning model from

00:09:35,600 --> 00:09:38,240
scikit-learn that we have implemented

00:09:37,040 --> 00:09:39,839
with differential privacy

00:09:38,240 --> 00:09:41,839
including things like logistic

00:09:39,839 --> 00:09:45,519
regression linear regression

00:09:41,839 --> 00:09:47,120
pca k means um

00:09:45,519 --> 00:09:48,640
and importantly each of the models that

00:09:47,120 --> 00:09:51,360
we have inherits the

00:09:48,640 --> 00:09:52,959
the learn equivalent as its parent class

00:09:51,360 --> 00:09:55,360
and that gives us access to a lot of

00:09:52,959 --> 00:09:57,760
second learn functionality for free

00:09:55,360 --> 00:09:58,640
um and that makes it uh much easier to

00:09:57,760 --> 00:10:01,279
use

00:09:58,640 --> 00:10:01,839
now we do have um additional warnings

00:10:01,279 --> 00:10:04,720
that we

00:10:01,839 --> 00:10:06,880
we push from diff privilege occasionally

00:10:04,720 --> 00:10:08,320
one of those including the privacy leak

00:10:06,880 --> 00:10:10,000
warning that you see here

00:10:08,320 --> 00:10:13,440
and i'll explain more about that when we

00:10:10,000 --> 00:10:13,440
move on to the notebooks shortly

00:10:14,480 --> 00:10:17,760
the next module we have then is the

00:10:16,480 --> 00:10:20,000
tools module

00:10:17,760 --> 00:10:21,600
which is the the numpy part of

00:10:20,000 --> 00:10:23,519
differivlib

00:10:21,600 --> 00:10:25,279
and this is a collection of kind of

00:10:23,519 --> 00:10:26,399
simple functions for data analytics

00:10:25,279 --> 00:10:28,480
tasks

00:10:26,399 --> 00:10:29,519
and that includes mean standard

00:10:28,480 --> 00:10:32,880
deviation

00:10:29,519 --> 00:10:34,560
sum count queries and importantly the

00:10:32,880 --> 00:10:35,760
histogram function as well which is a

00:10:34,560 --> 00:10:38,000
very important function

00:10:35,760 --> 00:10:40,240
for differential privacy it allows us to

00:10:38,000 --> 00:10:43,440
plot things like distributions and get

00:10:40,240 --> 00:10:44,839
counts of of data sets in um in an

00:10:43,440 --> 00:10:47,519
efficient manner from a privacy

00:10:44,839 --> 00:10:49,360
perspective

00:10:47,519 --> 00:10:51,120
and finally then we have the accountant

00:10:49,360 --> 00:10:52,560
module which is

00:10:51,120 --> 00:10:54,399
an account that keeps track of the

00:10:52,560 --> 00:10:57,360
privacy budget spend that i mentioned

00:10:54,399 --> 00:10:58,560
earlier so you can see in this snippet

00:10:57,360 --> 00:11:00,880
of code we have

00:10:58,560 --> 00:11:02,480
three queries run on the data set each

00:11:00,880 --> 00:11:04,240
with an epsilon of 0.1

00:11:02,480 --> 00:11:06,000
and then at the end we add that up to be

00:11:04,240 --> 00:11:06,880
a total of 0.3 which is what you'd

00:11:06,000 --> 00:11:09,200
expect

00:11:06,880 --> 00:11:10,399
but we also have capability for using

00:11:09,200 --> 00:11:13,680
advanced composition

00:11:10,399 --> 00:11:16,640
composition techniques that if we

00:11:13,680 --> 00:11:18,160
give a little bit of slack in the uh the

00:11:16,640 --> 00:11:19,839
guarantee that differential privacy

00:11:18,160 --> 00:11:21,839
provides we can get a big

00:11:19,839 --> 00:11:23,440
benefit in terms of the privacy budget

00:11:21,839 --> 00:11:26,839
that we spent

00:11:23,440 --> 00:11:28,079
so in this plot here we see that over 30

00:11:26,839 --> 00:11:29,839
queries

00:11:28,079 --> 00:11:32,880
using what we call naive composition

00:11:29,839 --> 00:11:35,200
without any slack in our guarantee that

00:11:32,880 --> 00:11:36,480
we're spending more than 0.4 of our

00:11:35,200 --> 00:11:38,079
privacy budget

00:11:36,480 --> 00:11:40,399
but having a little slack in that

00:11:38,079 --> 00:11:42,560
guarantee allows us to reduce the spend

00:11:40,399 --> 00:11:45,200
to just over 0.2

00:11:42,560 --> 00:11:47,120
so in essence this allows us to extract

00:11:45,200 --> 00:11:49,600
more knowledge for the same privacy

00:11:47,120 --> 00:11:49,600
guarantee

00:11:51,120 --> 00:11:55,760
so i'm going to move on to some

00:11:54,160 --> 00:11:58,399
notebook demos now quickly i think for

00:11:55,760 --> 00:12:01,279
the next 10 minutes or so

00:11:58,399 --> 00:12:02,800
so the first one of those is a quick 30

00:12:01,279 --> 00:12:06,959
second introduction to

00:12:02,800 --> 00:12:06,959
diff privilege um

00:12:08,240 --> 00:12:13,680
so to this is simply uh

00:12:12,000 --> 00:12:15,120
we're going to train a gaussian naive

00:12:13,680 --> 00:12:18,160
bayes classifier with

00:12:15,120 --> 00:12:22,800
uh differential privacy um

00:12:18,160 --> 00:12:26,160
we begin by importing the iris data set

00:12:22,800 --> 00:12:26,160
from using scikit-learn

00:12:27,120 --> 00:12:30,720
we're then going to import our gaussian

00:12:28,880 --> 00:12:31,120
naive bayes classifier from div prove

00:12:30,720 --> 00:12:34,240
lib

00:12:31,120 --> 00:12:35,360
and initialize it and train it using the

00:12:34,240 --> 00:12:37,680
fit method

00:12:35,360 --> 00:12:39,440
as i said before we have a as we saw

00:12:37,680 --> 00:12:40,160
before we have a privacy leak warning

00:12:39,440 --> 00:12:41,920
here

00:12:40,160 --> 00:12:44,320
and that's because we haven't specified

00:12:41,920 --> 00:12:47,519
the bounds hyper parameter

00:12:44,320 --> 00:12:50,079
and this balance hyperparameter

00:12:47,519 --> 00:12:50,959
ensures that the model is calibrated

00:12:50,079 --> 00:12:52,720
correctly

00:12:50,959 --> 00:12:54,399
and without specifying it what the model

00:12:52,720 --> 00:12:55,040
is going to do is actually going to read

00:12:54,399 --> 00:12:57,200
in that

00:12:55,040 --> 00:12:59,680
that that information from the data set

00:12:57,200 --> 00:13:01,360
which constitutes a privacy leak

00:12:59,680 --> 00:13:02,800
um above what we would expect from

00:13:01,360 --> 00:13:03,920
differential privacy so that's why we

00:13:02,800 --> 00:13:05,680
have a warning here

00:13:03,920 --> 00:13:08,560
and typically you wouldn't want this to

00:13:05,680 --> 00:13:11,360
appear in your scripts so we'll fix that

00:13:08,560 --> 00:13:13,760
at the end of this script as i also said

00:13:11,360 --> 00:13:14,800
before um each of our models inherits

00:13:13,760 --> 00:13:16,880
the cyclic learn

00:13:14,800 --> 00:13:20,480
class as its parent class and you can

00:13:16,880 --> 00:13:22,959
see that here

00:13:20,480 --> 00:13:23,680
we can now that the model is trained we

00:13:22,959 --> 00:13:25,440
can

00:13:23,680 --> 00:13:27,040
classify unseen examples so we're going

00:13:25,440 --> 00:13:29,920
to use the test data set for that

00:13:27,040 --> 00:13:30,720
and you can see using the predict method

00:13:29,920 --> 00:13:32,800
we have our

00:13:30,720 --> 00:13:34,240
classifications and we can then use the

00:13:32,800 --> 00:13:37,760
score function again

00:13:34,240 --> 00:13:40,160
um from scikit-learn to

00:13:37,760 --> 00:13:42,399
uh test the accuracy of the test data

00:13:40,160 --> 00:13:43,920
set and we're at about 77

00:13:42,399 --> 00:13:48,480
which is pretty good for the size of the

00:13:43,920 --> 00:13:50,000
data set

00:13:48,480 --> 00:13:52,160
so in this particular cell what we're

00:13:50,000 --> 00:13:55,760
going to do is we're going to run

00:13:52,160 --> 00:13:57,440
our classifier for a for various epsilon

00:13:55,760 --> 00:13:59,519
values

00:13:57,440 --> 00:14:01,040
and in order to suppress our privacy

00:13:59,519 --> 00:14:02,800
leak warning we're going to specify the

00:14:01,040 --> 00:14:05,600
bounds parameter

00:14:02,800 --> 00:14:08,240
upon initialization and what the bounds

00:14:05,600 --> 00:14:10,399
does in this case is it specifies the

00:14:08,240 --> 00:14:11,519
at the range in which the values in each

00:14:10,399 --> 00:14:13,600
column lie

00:14:11,519 --> 00:14:15,199
so that the model can correctly

00:14:13,600 --> 00:14:17,120
calibrate the noise that it's going to

00:14:15,199 --> 00:14:19,199
add obviously we're going to have to add

00:14:17,120 --> 00:14:20,959
a lot more noise to data that's spread

00:14:19,199 --> 00:14:23,839
over a wider range

00:14:20,959 --> 00:14:25,519
and this parameter fix our source that

00:14:23,839 --> 00:14:28,160
out in our model

00:14:25,519 --> 00:14:28,639
we then we're going to plot the accuracy

00:14:28,160 --> 00:14:30,959
of

00:14:28,639 --> 00:14:33,279
this bottle across various epsilons on a

00:14:30,959 --> 00:14:34,399
log scale from 10 to the minus 2 to 10

00:14:33,279 --> 00:14:35,839
to the 2.

00:14:34,399 --> 00:14:37,360
obviously because of the randomness

00:14:35,839 --> 00:14:38,880
involved there's always going to be some

00:14:37,360 --> 00:14:40,800
fluctuation in the values

00:14:38,880 --> 00:14:42,880
so these only give a single snapshot for

00:14:40,800 --> 00:14:46,079
each epsilon but it's still a good

00:14:42,880 --> 00:14:47,760
test to see how we're doing

00:14:46,079 --> 00:14:49,440
and then we're going to initialize the

00:14:47,760 --> 00:14:51,120
classifier with these parameters the

00:14:49,440 --> 00:14:53,680
bounds and the epsilon value

00:14:51,120 --> 00:14:56,079
and then fit it and test the accuracy of

00:14:53,680 --> 00:14:56,079
the model

00:14:56,399 --> 00:15:01,360
and as we can see here so for small

00:14:59,279 --> 00:15:02,079
epsilon which is again a small privacy

00:15:01,360 --> 00:15:05,120
budget

00:15:02,079 --> 00:15:08,000
we're getting quite uh poor accuracy and

00:15:05,120 --> 00:15:09,600
uh very jagged curve but as we increase

00:15:08,000 --> 00:15:12,079
our epsilon as we increase the privacy

00:15:09,600 --> 00:15:14,240
budget we're approaching uh 100 accuracy

00:15:12,079 --> 00:15:16,399
at the top that's exactly what we would

00:15:14,240 --> 00:15:19,120
expect

00:15:16,399 --> 00:15:19,760
so in the next notebook we're going to

00:15:19,120 --> 00:15:22,320
run through

00:15:19,760 --> 00:15:26,160
a differentially private machine

00:15:22,320 --> 00:15:27,519
learning pipeline

00:15:26,160 --> 00:15:29,920
this time we're going to use the adult

00:15:27,519 --> 00:15:31,279
data set from the uci machine learning

00:15:29,920 --> 00:15:34,320
repository

00:15:31,279 --> 00:15:38,480
uh i'm going to import it using numpy

00:15:34,320 --> 00:15:40,240
there we go and in order to construct a

00:15:38,480 --> 00:15:42,880
baseline we're going to

00:15:40,240 --> 00:15:45,199
train a pipeline using scikit-learn to

00:15:42,880 --> 00:15:47,279
begin with without any privacy

00:15:45,199 --> 00:15:49,360
and here we have the pipeline is going

00:15:47,279 --> 00:15:52,000
to be composed of a

00:15:49,360 --> 00:15:52,639
standard scalar which scales each column

00:15:52,000 --> 00:15:55,279
to have

00:15:52,639 --> 00:15:56,800
zero mean and unit variance we're then

00:15:55,279 --> 00:15:58,160
going to reduce the dimensionality of

00:15:56,800 --> 00:16:00,800
the data set to have

00:15:58,160 --> 00:16:01,440
only two columns and then we're going to

00:16:00,800 --> 00:16:03,199
feed that

00:16:01,440 --> 00:16:04,560
resulting data into a logistic

00:16:03,199 --> 00:16:06,399
regression

00:16:04,560 --> 00:16:09,120
um which is going to do our

00:16:06,399 --> 00:16:10,720
classification for us

00:16:09,120 --> 00:16:12,959
in this case the adult data set is a

00:16:10,720 --> 00:16:16,320
binary classification task

00:16:12,959 --> 00:16:20,000
um and because different

00:16:16,320 --> 00:16:22,560
is limited to using the lbfgs

00:16:20,000 --> 00:16:24,560
solver we're going to use that same

00:16:22,560 --> 00:16:27,279
solver for this non-private version so

00:16:24,560 --> 00:16:28,320
that we're comparing like with like

00:16:27,279 --> 00:16:30,560
so we're going to initialize the

00:16:28,320 --> 00:16:33,199
pipeline and then we're going to

00:16:30,560 --> 00:16:35,279
fit it and score it using our trading

00:16:33,199 --> 00:16:37,120
data set and test data sets

00:16:35,279 --> 00:16:40,800
and we see here that we have a a

00:16:37,120 --> 00:16:40,800
baseline accuracy of 80 percent

00:16:41,040 --> 00:16:44,320
so what we're now going to do is going

00:16:42,160 --> 00:16:46,000
to turn this particular non-private

00:16:44,320 --> 00:16:47,120
pipeline into a differentially private

00:16:46,000 --> 00:16:50,240
pipeline

00:16:47,120 --> 00:16:52,079
and parameterize it accordingly so if

00:16:50,240 --> 00:16:53,680
you go down here

00:16:52,079 --> 00:16:55,360
so we're going to import the models

00:16:53,680 --> 00:16:58,079
module from diff priv lib

00:16:55,360 --> 00:17:00,320
and we have a standard scalar here

00:16:58,079 --> 00:17:03,040
correctly parameterized with the bounds

00:17:00,320 --> 00:17:04,319
we have a pca which is principle

00:17:03,040 --> 00:17:06,000
components analysis reducing the

00:17:04,319 --> 00:17:07,919
dimension down to two

00:17:06,000 --> 00:17:09,280
and then we have logistic regression

00:17:07,919 --> 00:17:11,919
classifier

00:17:09,280 --> 00:17:13,919
again parameterized accordingly for each

00:17:11,919 --> 00:17:16,880
of these we're going to set the epsilon

00:17:13,919 --> 00:17:18,400
value to be one-third

00:17:16,880 --> 00:17:20,079
which means the excellent value for the

00:17:18,400 --> 00:17:21,839
entire pipeline when we add it up is

00:17:20,079 --> 00:17:23,919
going to be one

00:17:21,839 --> 00:17:26,160
so we're going to initialize that and

00:17:23,919 --> 00:17:27,600
we're going to fit and score it

00:17:26,160 --> 00:17:30,480
and as we can see here we have an

00:17:27,600 --> 00:17:32,480
epsilon of almost 81

00:17:30,480 --> 00:17:35,440
so that compares quite favorably to the

00:17:32,480 --> 00:17:36,400
non-private pipeline which was only 80.3

00:17:35,440 --> 00:17:39,440
percent

00:17:36,400 --> 00:17:40,799
it's not uncommon to see a higher

00:17:39,440 --> 00:17:43,039
accuracy from a different

00:17:40,799 --> 00:17:44,480
private model compared to a non-private

00:17:43,039 --> 00:17:44,960
model because of the noise that's been

00:17:44,480 --> 00:17:47,760
added

00:17:44,960 --> 00:17:49,679
it reduces overfitting um as one

00:17:47,760 --> 00:17:50,000
consequence which allows us to improve

00:17:49,679 --> 00:17:53,840
the

00:17:50,000 --> 00:17:53,840
accuracy as a byproduct

00:17:54,000 --> 00:17:59,360
and in this cell then is simply

00:17:57,440 --> 00:18:01,039
doing a similar task to the previous

00:17:59,360 --> 00:18:03,120
notebook where we're running our model

00:18:01,039 --> 00:18:04,559
over various epsilon values

00:18:03,120 --> 00:18:06,000
um i'm not going to run that because it

00:18:04,559 --> 00:18:08,240
takes a bit too much time but we can

00:18:06,000 --> 00:18:10,799
look at the results down here

00:18:08,240 --> 00:18:12,160
extracting them from pickle and you can

00:18:10,799 --> 00:18:14,400
see here we have

00:18:12,160 --> 00:18:16,160
again for small epsilon values for a

00:18:14,400 --> 00:18:19,600
small privacy budget we have

00:18:16,160 --> 00:18:22,559
very noisy results but things start um

00:18:19,600 --> 00:18:24,799
approaching our baseline accuracy at

00:18:22,559 --> 00:18:27,200
epsilon equals 0.1

00:18:24,799 --> 00:18:29,840
which is a very good result in this

00:18:27,200 --> 00:18:29,840
particular case

00:18:32,240 --> 00:18:35,679
and finally then the last notebook i

00:18:34,000 --> 00:18:38,160
want to share is a quick

00:18:35,679 --> 00:18:39,679
data exploration workflow with different

00:18:38,160 --> 00:18:44,720
lib

00:18:39,679 --> 00:18:44,720
i'm going to run all of this code

00:18:46,080 --> 00:18:49,600
so we begin by importing numpy div

00:18:48,960 --> 00:18:52,640
privilege

00:18:49,600 --> 00:18:53,840
and map.lib for all of our plotting

00:18:52,640 --> 00:18:56,240
in this case we're going to use our

00:18:53,840 --> 00:18:57,919
budget accountant this time to take uh

00:18:56,240 --> 00:18:59,600
keep track of our budget spent across

00:18:57,919 --> 00:19:01,840
the script and

00:18:59,600 --> 00:19:04,240
we're going to use an epsilon of 0.04

00:19:01,840 --> 00:19:06,720
for each of our queries

00:19:04,240 --> 00:19:08,559
in this case so we have initialized our

00:19:06,720 --> 00:19:12,400
budget accountant to have a

00:19:08,559 --> 00:19:12,400
an epsilon value of up to one

00:19:13,039 --> 00:19:17,600
so here we're going to use the cover

00:19:14,960 --> 00:19:19,120
type data set from scikit learn

00:19:17,600 --> 00:19:21,200
and we're going to do a little bit of

00:19:19,120 --> 00:19:24,160
pre-processing on the data set here

00:19:21,200 --> 00:19:25,760
and then we have our column names and

00:19:24,160 --> 00:19:27,120
the ranges of each column which you're

00:19:25,760 --> 00:19:29,919
going to use later on

00:19:27,120 --> 00:19:32,320
when we're specifying bounds for each uh

00:19:29,919 --> 00:19:32,320
query

00:19:33,120 --> 00:19:37,679
as i said before the histogram function

00:19:36,240 --> 00:19:38,000
is very important in different privacy

00:19:37,679 --> 00:19:40,080
and very

00:19:38,000 --> 00:19:41,679
efficient so we can use the histogram

00:19:40,080 --> 00:19:43,200
function in this case to

00:19:41,679 --> 00:19:45,360
see have a look at the distribution of

00:19:43,200 --> 00:19:47,679
the labels in this data set

00:19:45,360 --> 00:19:49,440
so we can see here that approximately 50

00:19:47,679 --> 00:19:52,559
percent of the examples in this data set

00:19:49,440 --> 00:19:58,000
are associated with label number two

00:19:52,559 --> 00:20:00,480
um and about 35 with label number one

00:19:58,000 --> 00:20:02,480
again all of these queries have a little

00:20:00,480 --> 00:20:04,000
bit of random noise because of the

00:20:02,480 --> 00:20:05,840
differential privacy guarantees that we

00:20:04,000 --> 00:20:08,640
have

00:20:05,840 --> 00:20:10,880
but for the size of the data set these

00:20:08,640 --> 00:20:13,679
results are quite accurate

00:20:10,880 --> 00:20:13,679
and reliable

00:20:14,880 --> 00:20:19,280
we can extend our analysis of the

00:20:17,919 --> 00:20:21,120
distributions of the data to the

00:20:19,280 --> 00:20:23,679
features of the data set as well so this

00:20:21,120 --> 00:20:26,559
is all of the columns of the data set

00:20:23,679 --> 00:20:29,840
um again using the histogram function

00:20:26,559 --> 00:20:29,840
here to access the data

00:20:30,159 --> 00:20:34,000
and these are the results that we get so

00:20:31,919 --> 00:20:35,760
we can see there is

00:20:34,000 --> 00:20:37,039
we can get a good visualization of the

00:20:35,760 --> 00:20:40,400
type of data that we're dealing with

00:20:37,039 --> 00:20:40,400
using these particular functions

00:20:41,039 --> 00:20:44,000
and now it's probably a good time to

00:20:42,240 --> 00:20:44,400
have a look at our accountant to see how

00:20:44,000 --> 00:20:46,960
much

00:20:44,400 --> 00:20:50,159
privacy budget we've spent so far so we

00:20:46,960 --> 00:20:53,360
can see here we have a total spend of uh

00:20:50,159 --> 00:20:55,200
0.52 and using the lend function then we

00:20:53,360 --> 00:20:58,400
see that we've executed 13 queries

00:20:55,200 --> 00:21:03,600
which correspond to the 12 queries here

00:20:58,400 --> 00:21:05,679
and the the one query previously

00:21:03,600 --> 00:21:07,360
so we can also use uh two-dimensional

00:21:05,679 --> 00:21:08,880
histograms to get even more insight from

00:21:07,360 --> 00:21:10,240
data

00:21:08,880 --> 00:21:12,400
and in this case we're plotting the

00:21:10,240 --> 00:21:15,600
distribution of each um

00:21:12,400 --> 00:21:19,039
of three features here as well as their

00:21:15,600 --> 00:21:19,039
distribution of labels

00:21:20,559 --> 00:21:23,919
and that's using the histogram 2d

00:21:22,240 --> 00:21:26,880
function again similar to the

00:21:23,919 --> 00:21:26,880
numpy equivalent

00:21:29,360 --> 00:21:35,679
we can extend that to uh plotting

00:21:33,520 --> 00:21:37,039
uh features against other features again

00:21:35,679 --> 00:21:40,000
using the 2

00:21:37,039 --> 00:21:41,200
dimensional histogram function here um

00:21:40,000 --> 00:21:43,360
in this case we're plotting

00:21:41,200 --> 00:21:44,640
the horizontal distance to hydrology on

00:21:43,360 --> 00:21:46,320
the x-axis

00:21:44,640 --> 00:21:49,679
and then using our colors we're using

00:21:46,320 --> 00:21:52,000
horizontal distance to roadways

00:21:49,679 --> 00:21:54,960
and another way then to compare features

00:21:52,000 --> 00:21:57,440
on two dimensions is to use color maps

00:21:54,960 --> 00:22:00,640
again which we execute using the

00:21:57,440 --> 00:22:00,640
two-dimensional histogram

00:22:01,440 --> 00:22:06,400
and finally then we have our more simple

00:22:04,640 --> 00:22:09,440
queries if we're looking to hone in on

00:22:06,400 --> 00:22:12,640
on specific features of the data

00:22:09,440 --> 00:22:15,679
using our mean variance count none and

00:22:12,640 --> 00:22:17,520
count non-zero as well and you can see

00:22:15,679 --> 00:22:19,200
then at the end we can examine our total

00:22:17,520 --> 00:22:20,640
privacy loss and if we have any other

00:22:19,200 --> 00:22:24,240
queries to execute we can

00:22:20,640 --> 00:22:27,840
use uh the remaining method to find out

00:22:24,240 --> 00:22:27,840
how much privacy budget we have to spend

00:22:29,360 --> 00:22:35,600
so uh before i finish uh here are some

00:22:34,159 --> 00:22:36,080
additional resources for you if you want

00:22:35,600 --> 00:22:38,640
to

00:22:36,080 --> 00:22:40,799
learn more so all of our code is

00:22:38,640 --> 00:22:41,360
available on our github repository on

00:22:40,799 --> 00:22:45,039
the

00:22:41,360 --> 00:22:46,559
um ibm domain and that includes

00:22:45,039 --> 00:22:49,360
all of the notebooks that i presented

00:22:46,559 --> 00:22:51,600
here today and other notebooks as well

00:22:49,360 --> 00:22:52,400
our documentation is hosted on read the

00:22:51,600 --> 00:22:53,840
docs

00:22:52,400 --> 00:22:55,760
and i guess most importantly if you want

00:22:53,840 --> 00:22:58,159
to get started with driftprivelid

00:22:55,760 --> 00:22:58,880
then uh one line command in your

00:22:58,159 --> 00:23:01,760
terminal pip

00:22:58,880 --> 00:23:04,080
install diff privilege and you'll be

00:23:01,760 --> 00:23:04,080
away

00:23:04,480 --> 00:23:08,320
so that's all i have for you today um

00:23:07,120 --> 00:23:10,320
hopefully we'll have some time for some

00:23:08,320 --> 00:23:12,960
questions if there's any going

00:23:10,320 --> 00:23:14,080
yeah we do we have four minutes for

00:23:12,960 --> 00:23:16,799
questions

00:23:14,080 --> 00:23:16,799
great okay

00:23:18,880 --> 00:23:25,120
uh i don't think we have any questions

00:23:22,640 --> 00:23:25,760
wow everything must have been perfectly

00:23:25,120 --> 00:23:29,039
clear

00:23:25,760 --> 00:23:31,600
oh we have a question ah great

00:23:29,039 --> 00:23:33,440
all right uh is it possible to set up

00:23:31,600 --> 00:23:35,360
differential privacy so that you could

00:23:33,440 --> 00:23:38,159
reverse it in the future if you needed

00:23:35,360 --> 00:23:41,200
to or is it a one-way process

00:23:38,159 --> 00:23:43,360
so typically what you do is you have the

00:23:41,200 --> 00:23:44,720
raw data set and a secure environment

00:23:43,360 --> 00:23:46,400
and you can publish differentially

00:23:44,720 --> 00:23:49,200
private statistics on that

00:23:46,400 --> 00:23:51,679
um there are other techniques so for

00:23:49,200 --> 00:23:53,520
example

00:23:51,679 --> 00:23:56,320
apple uses what we call local

00:23:53,520 --> 00:23:58,080
differential privacy which means

00:23:56,320 --> 00:24:00,559
the differential privacy is applied

00:23:58,080 --> 00:24:02,320
before the data even leaves your device

00:24:00,559 --> 00:24:04,159
and that means that the data controller

00:24:02,320 --> 00:24:07,679
only ever sees

00:24:04,159 --> 00:24:09,600
random no randomly or

00:24:07,679 --> 00:24:11,440
data that has been randomized and in

00:24:09,600 --> 00:24:13,679
that case you can't reverse engineers

00:24:11,440 --> 00:24:15,679
but typically if the data is valuable

00:24:13,679 --> 00:24:18,559
then you would keep a raw copy of it

00:24:15,679 --> 00:24:21,840
in a secure environment and only release

00:24:18,559 --> 00:24:24,320
queries on it using differential price

00:24:21,840 --> 00:24:25,840
okay that's awesome uh also uh just a

00:24:24,320 --> 00:24:27,200
little help for me could you and share

00:24:25,840 --> 00:24:30,640
your screen if awesome

00:24:27,200 --> 00:24:33,120
yes i can thank you

00:24:30,640 --> 00:24:34,880
all right so this is the next question

00:24:33,120 --> 00:24:38,000
what happens if the budget account

00:24:34,880 --> 00:24:39,919
runs out so the

00:24:38,000 --> 00:24:41,440
in an ideal scenario the idea is that

00:24:39,919 --> 00:24:42,320
you have a fixed privacy budget for a

00:24:41,440 --> 00:24:44,840
data set

00:24:42,320 --> 00:24:46,080
and once the budget runs out the data is

00:24:44,840 --> 00:24:50,400
destroyed

00:24:46,080 --> 00:24:51,279
um clearly that's not realistic in in

00:24:50,400 --> 00:24:53,279
today's world

00:24:51,279 --> 00:24:55,039
so a lot of the time you would give for

00:24:53,279 --> 00:24:57,039
example the idea is that you

00:24:55,039 --> 00:24:59,279
give a single data analyst a fixed

00:24:57,039 --> 00:25:01,600
privacy budget to spend on a data set

00:24:59,279 --> 00:25:04,080
and once that budget has been spent the

00:25:01,600 --> 00:25:05,840
access to that data set is revoked

00:25:04,080 --> 00:25:08,720
um so that would be the modern

00:25:05,840 --> 00:25:11,919
interpretation of that

00:25:08,720 --> 00:25:15,360
okay awesome so the next question you

00:25:11,919 --> 00:25:17,679
are inheriting from sk learns classes

00:25:15,360 --> 00:25:20,400
if they modify how do you guarantee that

00:25:17,679 --> 00:25:23,440
your code maintains compatibility

00:25:20,400 --> 00:25:26,880
and that requires updating of the code

00:25:23,440 --> 00:25:28,559
as simple as so typically a lot of the

00:25:26,880 --> 00:25:31,760
time the last two

00:25:28,559 --> 00:25:32,480
i think 22 and 23 we've had to push our

00:25:31,760 --> 00:25:34,640
patches

00:25:32,480 --> 00:25:35,840
um the following day for changes that

00:25:34,640 --> 00:25:37,440
cycle learn have made

00:25:35,840 --> 00:25:39,840
so it's just an ongoing process of

00:25:37,440 --> 00:25:43,919
keeping it up to date

00:25:39,840 --> 00:25:45,840
awesome what is the point of privacy

00:25:43,919 --> 00:25:46,960
differential could you be more specific

00:25:45,840 --> 00:25:50,240
and suggest

00:25:46,960 --> 00:25:52,720
a few using a scenario

00:25:50,240 --> 00:25:54,320
so the as i mentioned in the talk the

00:25:52,720 --> 00:25:56,159
differential privacy was conceived

00:25:54,320 --> 00:25:59,600
because of the failings of

00:25:56,159 --> 00:26:01,520
traditional anonymization methods and

00:25:59,600 --> 00:26:03,200
differential privacy isn't perfect but

00:26:01,520 --> 00:26:07,200
it works very well when you have

00:26:03,200 --> 00:26:08,840
a lot of data um a lot of sensitive data

00:26:07,200 --> 00:26:12,240
and when

00:26:08,840 --> 00:26:13,440
um sorry enough to lose my train of

00:26:12,240 --> 00:26:16,159
thought

00:26:13,440 --> 00:26:17,679
um so that there are very specific

00:26:16,159 --> 00:26:19,520
circumstances where differential privacy

00:26:17,679 --> 00:26:21,679
is very useful but we still need to use

00:26:19,520 --> 00:26:22,960
our traditional atomization methods to

00:26:21,679 --> 00:26:26,720
safeguard against

00:26:22,960 --> 00:26:27,120
um against data risk okay that's awesome

00:26:26,720 --> 00:26:29,919
i think

00:26:27,120 --> 00:26:30,480
right now we're out of time thank you

00:26:29,919 --> 00:26:32,480
for your

00:26:30,480 --> 00:26:33,919
talk you can actually reach out to the

00:26:32,480 --> 00:26:35,360
breakout room we actually have a few

00:26:33,919 --> 00:26:39,840
more questions you can

00:26:35,360 --> 00:26:39,840

YouTube URL: https://www.youtube.com/watch?v=LWneaO94esk


