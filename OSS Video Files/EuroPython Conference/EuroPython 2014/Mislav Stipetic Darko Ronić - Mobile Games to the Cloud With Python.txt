Title: Mislav Stipetic Darko Ronić - Mobile Games to the Cloud With Python
Publication date: 2015-04-18
Playlist: EuroPython 2014
Description: 
	Mislav Stipetic/Darko Ronić - Mobile Games to the Cloud With Python
[EuroPython 2014]
[22 July 2014]

When a mobile game development company decides to switch to a more cloud based development it is faced with obstacles different from those it’s used to on mobile devices. This talk explains how Python provided us with most of the infrastructure for this task and how a Python game backend was built as a result.

-----

#### The Talk

This talk has two goals. Showing the audience the lessons we learned during a project which moved a simple mobile game to a server backend is our first intention. In addition to that we want to describe how such a system works in a real life example, to show which problems and which requirements arise in its creation. When the audience leaves the talk they will know how a real-life mobile game uses Python for powering the backend servers.

 
#### The Problem

Most of the game development for mobile devices is focused on running the game on the device. The game designers and game developers play a primary role in creating the product. The server backend plays a supporting role providing a multiplayer or social experience to the users. Indeed, at Nanobit Ltd., things were also done that way. We had a small Python infrastructure built around Django which provided a small portion of multiplayer experience for the players. The majority of development was still focused on playing the game on the device. That way of thinking was put to test when we decided to center our future games around the multiplayer experience. Due to the fact that our infrastructure at the time was not enough for what we had in mind, we had to start from scratch. The decision was made to use Python as the center of our new infrastructure.

In order to achieve it, a server backend was needed that would allow the game to be played “in the cloud” with the device only being a terminal to the player. Most of the game logic would have to be processed in the cloud which meant that each player required a constant connection to the backend and with over 100.000 players in our previous games that presented a challenge. How to build an infrastructure which can support that? Since every user action had to be sent to the backend how to process thousands of them quick enough? Those problems were big and were just the start.


#### The Solution

The design of the backend lasted for a couple of months and produced a scalable infrastructure based on “workers” developed in Python, “web servers” that use Tornado and a custom message queue which connected the two. The storage part is a combination of Riak and Redis. Since the backend is scalable new workers and new web servers had to be deployed easily so an orchestration module was build using Fabric. The scalability and launching of new workers and web servers was achieved using Docker for creation and deployment of containers. Each container presents one module of the system (worker, web server, queue). The end result can now support all of our future games and only requires the game logic of each game to be added to the workers.


#### The Technologies

* Python for coding the game logic, web servers. More than 90% of the system was written in Python.
* Fabric
* SQLAlchemy
* Riak
* Redis
* ZeroMQ
* nginx
* Docker
* Websockets
* AWS


#### The Lessons Learned

* How to tune the backend to handle the increasing number of active players.
* How to tackle the problem of frequent connection dropping and reachability issues of poor mobile device Internet connection in Tornado with a little help of Redis.
* How to prevent users from trying to outsmart the system by denying illegal moves.
* How to enable game profile syncing and live updating.
* Improving the performance of workers by prioritizing data being stored to databases (Riak, SQL).
* New issues and lessons show up all the time so there will definitely be more of them by the time of the conference.


#### Basic Outline

1. Intro (5 min)
    1. Who are we?
    2. How was Python used in our previous games
    3. Why we decided to change it all
2. Requirements (6 min)
    1. What was the goal of creating the game backend
    2. Why was Python our first choice
3. Python backend (14 min)
    1. The architecture of the backend
    2. Which technologies did we use and how were they connected together
    3. How the backend handles the game logic
    4. Lessons learned
4. Questions & Answers (5 min)
Captions: 
	00:00:14,930 --> 00:00:19,520
hello everyone I'm darker and mysteries

00:00:17,930 --> 00:00:22,280
behind me he will be joining a bit later

00:00:19,520 --> 00:00:24,320
so we'll prepare from not a bit a

00:00:22,280 --> 00:00:26,180
creation company and we will be talking

00:00:24,320 --> 00:00:30,080
about mobile games to the club with

00:00:26,180 --> 00:00:34,340
Python or how we as a company made a

00:00:30,080 --> 00:00:37,129
transaction from a more more mobile from

00:00:34,340 --> 00:00:40,309
a mobile based gaming to a one that is

00:00:37,129 --> 00:00:42,199
cloud-based so i will divide this talk

00:00:40,309 --> 00:00:44,510
into three parts the first part will be

00:00:42,199 --> 00:00:48,590
my me then mr. will take over and then i

00:00:44,510 --> 00:00:52,520
will finish so let's get started so how

00:00:48,590 --> 00:00:56,660
is this talk going to be divided so we

00:00:52,520 --> 00:00:58,040
have four parts the intro and the

00:00:56,660 --> 00:00:59,420
requirements the Python Beckett but you

00:00:58,040 --> 00:01:01,070
will see that later but I want to tell

00:00:59,420 --> 00:01:03,220
you what our objectives are our first

00:01:01,070 --> 00:01:05,330
objectives is to show you how we

00:01:03,220 --> 00:01:07,369
connected various technologies using

00:01:05,330 --> 00:01:09,350
Python how we created a scalable

00:01:07,369 --> 00:01:11,300
architecture as a result of it and how

00:01:09,350 --> 00:01:12,770
we are using Python in our company on a

00:01:11,300 --> 00:01:15,110
number of places but not the second

00:01:12,770 --> 00:01:17,180
objective which I find more important is

00:01:15,110 --> 00:01:19,159
to show you the lessons we learned the

00:01:17,180 --> 00:01:21,500
challenges we face the difficulties we

00:01:19,159 --> 00:01:24,049
had and what the problems were when

00:01:21,500 --> 00:01:26,180
creating such a system that has a lot of

00:01:24,049 --> 00:01:28,460
customers that has a lot of players

00:01:26,180 --> 00:01:30,619
which are which are on smartphones and

00:01:28,460 --> 00:01:35,540
which problems arise with smartphone

00:01:30,619 --> 00:01:37,310
gaming and stuff like that so who are we

00:01:35,540 --> 00:01:38,960
I'm going to just talk you through this

00:01:37,310 --> 00:01:40,820
is going to be a short part about our

00:01:38,960 --> 00:01:42,820
company we are not a bit we're a

00:01:40,820 --> 00:01:48,170
creation game development company and we

00:01:42,820 --> 00:01:50,090
started in 2009 and up till until 2011

00:01:48,170 --> 00:01:52,310
we were all developing applications and

00:01:50,090 --> 00:01:55,220
then we started developing games because

00:01:52,310 --> 00:01:57,560
in that time the games were more

00:01:55,220 --> 00:02:00,680
profitable and more interesting part of

00:01:57,560 --> 00:02:04,460
the industry so after we switch the

00:02:00,680 --> 00:02:06,290
games we decided ok our games are on the

00:02:04,460 --> 00:02:08,899
smartphone but we need some kind of a

00:02:06,290 --> 00:02:11,420
back-end on on the servers which are

00:02:08,899 --> 00:02:14,299
going to connect the players of our

00:02:11,420 --> 00:02:16,910
games to get like a social part of the

00:02:14,299 --> 00:02:20,360
gaming so we started developing a

00:02:16,910 --> 00:02:23,150
back-end server in Python and after that

00:02:20,360 --> 00:02:25,609
we all we saw a steady growth we had all

00:02:23,150 --> 00:02:26,950
the games we developed connect to this

00:02:25,609 --> 00:02:29,560
bacon but

00:02:26,950 --> 00:02:32,680
sometime that back and beckoned became

00:02:29,560 --> 00:02:34,870
too slow and too it wasn't sufficient

00:02:32,680 --> 00:02:37,450
enough for our new requirements on in

00:02:34,870 --> 00:02:39,489
our new games so as you can see we are

00:02:37,450 --> 00:02:43,390
using Amazon and we have a lot of

00:02:39,489 --> 00:02:45,610
instances and we have a lot of requests

00:02:43,390 --> 00:02:49,209
daily and that's rising every day and

00:02:45,610 --> 00:02:50,620
every year so how did we use Python in

00:02:49,209 --> 00:02:52,360
the past like I said we needed some

00:02:50,620 --> 00:02:55,060
social social features so we started

00:02:52,360 --> 00:02:57,760
with our first iteration of server which

00:02:55,060 --> 00:03:01,330
was pretty easy we used simple HTTP

00:02:57,760 --> 00:03:03,430
server in Python and that that lasted

00:03:01,330 --> 00:03:06,370
for about a month and then we realize

00:03:03,430 --> 00:03:08,829
that that won't work we switch to a more

00:03:06,370 --> 00:03:11,860
robust and more proof solution which was

00:03:08,829 --> 00:03:13,840
Apache with Django and that was okay for

00:03:11,860 --> 00:03:15,519
our requirements back then our

00:03:13,840 --> 00:03:17,680
requirements were only to connect the

00:03:15,519 --> 00:03:20,739
players in a social way so they can

00:03:17,680 --> 00:03:23,470
stick and view each other's games they

00:03:20,739 --> 00:03:25,540
can send gifts to each other and stuff

00:03:23,470 --> 00:03:27,280
like that after that we added some more

00:03:25,540 --> 00:03:29,590
functionalities like updates in the

00:03:27,280 --> 00:03:32,440
game's push notification settings and

00:03:29,590 --> 00:03:34,090
stuff like that and we added a secondary

00:03:32,440 --> 00:03:36,760
server which was for our in-house

00:03:34,090 --> 00:03:39,519
analytic system and that was working

00:03:36,760 --> 00:03:41,980
pretty well but we only had one big

00:03:39,519 --> 00:03:44,650
issue we had only vertical scalability

00:03:41,980 --> 00:03:47,170
so every 12 months our server would

00:03:44,650 --> 00:03:48,970
become too small and would be able to

00:03:47,170 --> 00:03:51,880
handle the stress and the number of

00:03:48,970 --> 00:03:54,160
requests so we had to bump up the server

00:03:51,880 --> 00:03:55,989
and that lasted for three years every 12

00:03:54,160 --> 00:03:57,850
months a bigger server and we wanted to

00:03:55,989 --> 00:03:59,980
have horizontal scalability but we

00:03:57,850 --> 00:04:03,010
couldn't because we had the problem with

00:03:59,980 --> 00:04:05,170
save games which were on which were

00:04:03,010 --> 00:04:07,090
saved on the disk and this would not

00:04:05,170 --> 00:04:09,989
allow us to have horizontal scalability

00:04:07,090 --> 00:04:13,120
so a change was needed because of that

00:04:09,989 --> 00:04:14,889
so why did we make the change so we had

00:04:13,120 --> 00:04:16,450
a new game this new game and new

00:04:14,889 --> 00:04:18,310
requirements and these new requirements

00:04:16,450 --> 00:04:20,500
were okay let's have a better

00:04:18,310 --> 00:04:22,450
multiplayer let's have players who play

00:04:20,500 --> 00:04:25,180
against each other and when you have a

00:04:22,450 --> 00:04:28,330
party that's not possible because Apache

00:04:25,180 --> 00:04:31,000
is a pre fork worker based web server

00:04:28,330 --> 00:04:33,849
which doesn't allow you a lot of

00:04:31,000 --> 00:04:36,520
consistent persistent connections so we

00:04:33,849 --> 00:04:39,840
had to change that another thing we had

00:04:36,520 --> 00:04:42,660
my sequel my sequel wasn't

00:04:39,840 --> 00:04:45,030
good enough for stuff like that you

00:04:42,660 --> 00:04:47,220
couldn't we had problems with my sequel

00:04:45,030 --> 00:04:50,150
when we developed the analytic system

00:04:47,220 --> 00:04:54,360
because we had like 50 60 million

00:04:50,150 --> 00:04:55,889
million rows of data in one table and we

00:04:54,360 --> 00:04:57,630
had to join it that didn't work so we

00:04:55,889 --> 00:04:59,699
decided okay we're going to change that

00:04:57,630 --> 00:05:02,070
we're going to make a solution that is

00:04:59,699 --> 00:05:03,690
more scalable that could allow us not

00:05:02,070 --> 00:05:06,449
only to have the social part of the game

00:05:03,690 --> 00:05:08,220
but the entire multiplayer and one other

00:05:06,449 --> 00:05:10,199
thing we decided okay we got to change

00:05:08,220 --> 00:05:12,690
we're going to put the game logic of the

00:05:10,199 --> 00:05:14,520
game on the back end no longer will the

00:05:12,690 --> 00:05:17,010
game be played only on the smartphone

00:05:14,520 --> 00:05:19,440
it's going to be the plate on the on the

00:05:17,010 --> 00:05:21,479
back end from now on so a player would

00:05:19,440 --> 00:05:23,669
send a request to the server every time

00:05:21,479 --> 00:05:25,530
something happens in the game so that we

00:05:23,669 --> 00:05:27,630
can have save games on the server that

00:05:25,530 --> 00:05:29,880
we can have players that play on

00:05:27,630 --> 00:05:31,740
multiple devices and when you have saved

00:05:29,880 --> 00:05:33,479
games on devices like we had in the

00:05:31,740 --> 00:05:35,580
beginning you have problems with

00:05:33,479 --> 00:05:38,550
cheating because as soon as the player

00:05:35,580 --> 00:05:40,620
can reach its own his own save game he

00:05:38,550 --> 00:05:44,430
can cheat he can just change the number

00:05:40,620 --> 00:05:46,380
of like coins he has 24 billion he can

00:05:44,430 --> 00:05:49,440
change the price of some stuff in the

00:05:46,380 --> 00:05:51,479
game 2-2 billion and when he buys the

00:05:49,440 --> 00:05:53,700
stuff he gains two billion coins so that

00:05:51,479 --> 00:05:55,770
that was an issue so we decided we have

00:05:53,700 --> 00:05:57,300
to change that we have to we have to

00:05:55,770 --> 00:06:00,690
stop the players from cheating like that

00:05:57,300 --> 00:06:02,250
so what was the goal the goal was to

00:06:00,690 --> 00:06:04,229
have a system that was scale

00:06:02,250 --> 00:06:06,750
horizontally every part of the system

00:06:04,229 --> 00:06:09,930
that was primary primary part of the

00:06:06,750 --> 00:06:11,400
system could scale horizontally we had

00:06:09,930 --> 00:06:13,889
wanted to have an automatic deployment

00:06:11,400 --> 00:06:15,510
because up till now we had to upload the

00:06:13,889 --> 00:06:18,479
source code it's each time to the server

00:06:15,510 --> 00:06:20,340
and that was not feasible enough we had

00:06:18,479 --> 00:06:23,940
to upload it we had to restart the

00:06:20,340 --> 00:06:26,669
server paschi this time we decided to

00:06:23,940 --> 00:06:29,700
change something and we wanted to

00:06:26,669 --> 00:06:32,639
container eyes each part of our back-end

00:06:29,700 --> 00:06:35,160
so like if we have layers of Beckett we

00:06:32,639 --> 00:06:38,190
wanted to have them transferable easily

00:06:35,160 --> 00:06:42,810
transferable between instances between

00:06:38,190 --> 00:06:46,139
servers since I like i said the game the

00:06:42,810 --> 00:06:48,599
game logic had to be transferred to the

00:06:46,139 --> 00:06:51,150
backend that meant that each player had

00:06:48,599 --> 00:06:51,889
to have a persistent connection to the

00:06:51,150 --> 00:06:54,409
server

00:06:51,889 --> 00:06:56,360
and apache like i said wasn't good

00:06:54,409 --> 00:06:58,819
enough for that so we had to find a

00:06:56,360 --> 00:07:00,650
solution how to have like 10 20 30

00:06:58,819 --> 00:07:02,870
thousands of players which play

00:07:00,650 --> 00:07:05,569
simultaneously and each of these players

00:07:02,870 --> 00:07:09,680
have a personal connection to the server

00:07:05,569 --> 00:07:13,969
so another thing up till now until now

00:07:09,680 --> 00:07:16,430
the game would pull new data from the

00:07:13,969 --> 00:07:17,870
server it would send a request it would

00:07:16,430 --> 00:07:19,219
receive a response from the server we

00:07:17,870 --> 00:07:21,650
wanted to change it we wanted to have

00:07:19,219 --> 00:07:24,259
the server push new data to the to the

00:07:21,650 --> 00:07:29,389
games to the client each time new data

00:07:24,259 --> 00:07:32,449
would appear so we decided to use a sort

00:07:29,389 --> 00:07:35,599
of sort of events based architecture we

00:07:32,449 --> 00:07:39,099
had a number of requests that would be

00:07:35,599 --> 00:07:41,419
sent in a batch from the client if those

00:07:39,099 --> 00:07:43,069
requests would not go through they would

00:07:41,419 --> 00:07:47,259
be a scent again in like three or four

00:07:43,069 --> 00:07:51,740
seconds at that that allowed us to have

00:07:47,259 --> 00:07:54,050
to have do so the data would not be lost

00:07:51,740 --> 00:07:55,639
in transaction in turn in the transfer

00:07:54,050 --> 00:07:58,330
and stuff like that and the cheating it

00:07:55,639 --> 00:08:02,839
made cheating very difficult because we

00:07:58,330 --> 00:08:05,150
tried to to each request that would ally

00:08:02,839 --> 00:08:07,879
that would arrive on the back end would

00:08:05,150 --> 00:08:10,639
be checked to see if this request is

00:08:07,879 --> 00:08:13,159
aware is possible for instance if a

00:08:10,639 --> 00:08:15,110
player wanted to buy something at the

00:08:13,159 --> 00:08:18,319
game we would check if he if this player

00:08:15,110 --> 00:08:23,830
has enough coins has enough any not

00:08:18,319 --> 00:08:28,339
enough gems or any other any other

00:08:23,830 --> 00:08:29,990
resource in the game so why did we

00:08:28,339 --> 00:08:32,120
choose fighting we already had Python

00:08:29,990 --> 00:08:35,390
like I said we use Django we used a lot

00:08:32,120 --> 00:08:36,949
of things in Baton and that was pretty

00:08:35,390 --> 00:08:39,380
nice for us and we wanted to continue

00:08:36,949 --> 00:08:41,120
that way and like I said we needed a web

00:08:39,380 --> 00:08:43,130
server that would allow us to have a

00:08:41,120 --> 00:08:45,529
large number of persistent connections

00:08:43,130 --> 00:08:47,899
and tornado tornado fits perfectly here

00:08:45,529 --> 00:08:49,699
because it's made for that it's made for

00:08:47,899 --> 00:08:52,220
persistent connections from the clients

00:08:49,699 --> 00:08:54,050
and after a couple of thousand

00:08:52,220 --> 00:08:55,490
connections to one tornado instance we

00:08:54,050 --> 00:08:58,100
decided okay we can start another

00:08:55,490 --> 00:08:59,480
instance we can have a proxy in the

00:08:58,100 --> 00:09:01,730
front in front of that that would load

00:08:59,480 --> 00:09:04,040
balance between it we wanted automatic

00:09:01,730 --> 00:09:04,760
deployment fabric is also great for that

00:09:04,040 --> 00:09:07,340
we

00:09:04,760 --> 00:09:10,580
had fabric connect to our Amazon ec2

00:09:07,340 --> 00:09:12,920
instances check for each type of

00:09:10,580 --> 00:09:14,720
instance with tags and we used boat 0

00:09:12,920 --> 00:09:16,790
for that you can see both on the slide

00:09:14,720 --> 00:09:18,380
too and we had workers workers were part

00:09:16,790 --> 00:09:20,870
of the system that would that would

00:09:18,380 --> 00:09:25,160
process the requests that arrived from

00:09:20,870 --> 00:09:27,200
the smart phones from the games since we

00:09:25,160 --> 00:09:28,730
had a working Django server we wanted to

00:09:27,200 --> 00:09:31,700
have some sort of backward compatibility

00:09:28,730 --> 00:09:34,220
so we can reuse the database data we can

00:09:31,700 --> 00:09:36,260
reuse the code and we can reuse the

00:09:34,220 --> 00:09:38,780
stuff we already had that was required

00:09:36,260 --> 00:09:40,880
for the new game and we had a lot of

00:09:38,780 --> 00:09:43,010
database we had three databases and they

00:09:40,880 --> 00:09:44,510
all had great frameworks in Python so

00:09:43,010 --> 00:09:46,430
Python was the solution for that and we

00:09:44,510 --> 00:09:48,770
decide okay we can go with bitin from

00:09:46,430 --> 00:09:52,100
from did for this part on and continue

00:09:48,770 --> 00:09:53,210
the way it was used until then so I'm

00:09:52,100 --> 00:09:54,890
going to leave you to miss lope he's

00:09:53,210 --> 00:09:57,470
gonna walk you through the architecture

00:09:54,890 --> 00:09:59,810
end the parts of the system so I'll show

00:09:57,470 --> 00:10:02,750
dark architecture better on the diagram

00:09:59,810 --> 00:10:04,430
later but for now the diagram the

00:10:02,750 --> 00:10:09,830
architecture is divided into several

00:10:04,430 --> 00:10:12,260
layers a proxy routes the request to the

00:10:09,830 --> 00:10:14,270
web servers the web servers talk to the

00:10:12,260 --> 00:10:16,520
queue which the which talks to the world

00:10:14,270 --> 00:10:19,990
web to the workers and they talk to the

00:10:16,520 --> 00:10:22,930
day databases and send a reply back

00:10:19,990 --> 00:10:25,460
everything is horizontally scalable and

00:10:22,930 --> 00:10:29,440
every part that needs to be scaled and

00:10:25,460 --> 00:10:32,380
is a unit in this layer is dr. eyes so

00:10:29,440 --> 00:10:34,940
that makes deployment really easy so

00:10:32,380 --> 00:10:38,240
python is great here because not only

00:10:34,940 --> 00:10:40,280
did we develop the code that implements

00:10:38,240 --> 00:10:42,800
the system logic the game logic but also

00:10:40,280 --> 00:10:46,160
it worked great in binding all those

00:10:42,800 --> 00:10:48,530
services together with its libraries so

00:10:46,160 --> 00:10:52,610
everything is in a virtual private cloud

00:10:48,530 --> 00:10:55,040
so and only the proxy the engine X proxy

00:10:52,610 --> 00:10:57,290
is available to the outside you can see

00:10:55,040 --> 00:11:00,260
how the data travels through the system

00:10:57,290 --> 00:11:02,060
blue lines represent the inbound traffic

00:11:00,260 --> 00:11:05,240
and the yellow red lines represent the

00:11:02,060 --> 00:11:08,000
other traffic inbound traffic comes to

00:11:05,240 --> 00:11:10,600
the HTTP requests and the outbound

00:11:08,000 --> 00:11:16,100
traffic goes through the web socket as

00:11:10,600 --> 00:11:17,120
you can see we will use react amazon RDS

00:11:16,100 --> 00:11:23,050
and ready

00:11:17,120 --> 00:11:26,660
our back-end for the web web server part

00:11:23,050 --> 00:11:29,660
each kit consists of two parts the HTTP

00:11:26,660 --> 00:11:31,339
layer which receives requests and the

00:11:29,660 --> 00:11:34,490
web socket layer which pushes the

00:11:31,339 --> 00:11:36,890
responses back to D to the game the

00:11:34,490 --> 00:11:40,070
engine X uses IP caching so the

00:11:36,890 --> 00:11:44,690
persistent connections are possible and

00:11:40,070 --> 00:11:48,230
WebSockets WebSockets can survive the

00:11:44,690 --> 00:11:50,810
when the request comes he goes to the

00:11:48,230 --> 00:11:53,510
WebSocket to the web server and it's

00:11:50,810 --> 00:11:57,740
pushed to our in house built q and the

00:11:53,510 --> 00:12:00,320
responses are the responses come through

00:11:57,740 --> 00:12:04,060
reddy's and we use Redis as a qu

00:12:00,320 --> 00:12:07,700
simplest and the WebSockets block on the

00:12:04,060 --> 00:12:09,770
on the list until a request a response

00:12:07,700 --> 00:12:13,880
arise that gave us some problems but

00:12:09,770 --> 00:12:17,270
we'll talk about it later now the

00:12:13,880 --> 00:12:19,040
workers are single threaded Python

00:12:17,270 --> 00:12:21,770
programs they are localized and they are

00:12:19,040 --> 00:12:24,050
connected to our queue we had to use a

00:12:21,770 --> 00:12:26,959
current r cube because of some issues

00:12:24,050 --> 00:12:33,200
with react we'll talk about that later

00:12:26,959 --> 00:12:35,930
and they receive requests from our cue

00:12:33,200 --> 00:12:38,360
each message from client has a unique

00:12:35,930 --> 00:12:42,320
message type and when we develop new

00:12:38,360 --> 00:12:44,209
games we just have to process different

00:12:42,320 --> 00:12:47,000
events that are specific to that game

00:12:44,209 --> 00:12:51,680
and the rest of the system stays the

00:12:47,000 --> 00:12:53,990
same our queue is written on goal we had

00:12:51,680 --> 00:12:56,990
we had to write a queue because of an

00:12:53,990 --> 00:12:58,190
issue with react when because of its

00:12:56,990 --> 00:13:02,720
architecture it doesn't support

00:12:58,190 --> 00:13:05,060
transactions so when multiple multiple

00:13:02,720 --> 00:13:07,940
points all right to the same data point

00:13:05,060 --> 00:13:09,740
they can create what they call siblings

00:13:07,940 --> 00:13:12,050
which are pieces of data with different

00:13:09,740 --> 00:13:15,140
way clocks and you have to manage that

00:13:12,050 --> 00:13:19,220
that yourself we solve that issue by

00:13:15,140 --> 00:13:21,110
having a queue which routes requests for

00:13:19,220 --> 00:13:23,839
single users of single connections

00:13:21,110 --> 00:13:27,680
always do the same worker we use ring

00:13:23,839 --> 00:13:28,350
caching for that and so that make sure

00:13:27,680 --> 00:13:30,750
that

00:13:28,350 --> 00:13:33,540
every piece of data that is written to

00:13:30,750 --> 00:13:36,660
is written by the same worker and they

00:13:33,540 --> 00:13:41,760
will never write multiple times to the

00:13:36,660 --> 00:13:44,430
same data point the cue detects when

00:13:41,760 --> 00:13:46,470
workers go online and offline everything

00:13:44,430 --> 00:13:48,660
in your architecture is made that

00:13:46,470 --> 00:13:50,430
everything is detectable and messages

00:13:48,660 --> 00:13:54,030
will queue up until new work has come up

00:13:50,430 --> 00:13:57,920
or it will rehash and start using a

00:13:54,030 --> 00:14:00,360
different number of workers so

00:13:57,920 --> 00:14:02,180
technologies we used our engineers for

00:14:00,360 --> 00:14:06,210
the proxy tornado for the web server

00:14:02,180 --> 00:14:09,060
zeromq is used on several places so that

00:14:06,210 --> 00:14:11,400
we can turn off and on several

00:14:09,060 --> 00:14:14,130
components and the messages will go

00:14:11,400 --> 00:14:16,170
through as soon as it's turned on for

00:14:14,130 --> 00:14:20,670
the databases we use real credits and my

00:14:16,170 --> 00:14:22,740
sequel we was released for everything we

00:14:20,670 --> 00:14:25,800
use docker for all the components we

00:14:22,740 --> 00:14:29,310
just have to make get push deployment

00:14:25,800 --> 00:14:32,490
master and post receive hook will build

00:14:29,310 --> 00:14:35,430
the docker containers tell the fabric

00:14:32,490 --> 00:14:36,990
hotel with the instances which have to

00:14:35,430 --> 00:14:38,700
be restarted that a new image is

00:14:36,990 --> 00:14:40,800
available they will pull the image

00:14:38,700 --> 00:14:45,360
rester the docker and we will help a new

00:14:40,800 --> 00:14:51,090
code running and as well alchemy for

00:14:45,360 --> 00:14:54,060
everything SQL related so darker oh so

00:14:51,090 --> 00:14:55,770
as you can see we use Python to bind

00:14:54,060 --> 00:14:58,290
various technologies together that works

00:14:55,770 --> 00:14:59,880
great and as you can see we have a web

00:14:58,290 --> 00:15:02,280
server in Python we had workers in

00:14:59,880 --> 00:15:05,700
Python and we have them connecting

00:15:02,280 --> 00:15:07,170
various databases clients together but

00:15:05,700 --> 00:15:09,120
I'm going to talk you now to the lessons

00:15:07,170 --> 00:15:11,160
learned because when developing such a

00:15:09,120 --> 00:15:12,930
system you always face difficulties and

00:15:11,160 --> 00:15:15,750
we had a number of difficulties some

00:15:12,930 --> 00:15:17,870
more specific to the smartphone part of

00:15:15,750 --> 00:15:20,850
the industry so one of the first things

00:15:17,870 --> 00:15:23,340
mobile devices have mobile connections

00:15:20,850 --> 00:15:25,970
mobile connections can be slow when a

00:15:23,340 --> 00:15:28,560
person walks to the to the street

00:15:25,970 --> 00:15:30,600
sometimes it disconnects from one tower

00:15:28,560 --> 00:15:33,330
connects to another tower it connects to

00:15:30,600 --> 00:15:35,670
vivi it disconnects for the Wi-Fi so you

00:15:33,330 --> 00:15:38,520
have connections which drop and new

00:15:35,670 --> 00:15:40,560
connections which start and we had had

00:15:38,520 --> 00:15:41,970
that problem because we disappear

00:15:40,560 --> 00:15:44,579
doesn't help you when you

00:15:41,970 --> 00:15:46,529
try to discover connection drops so we

00:15:44,579 --> 00:15:48,540
had to fix that by monitoring our

00:15:46,529 --> 00:15:50,790
connections frequently we had to pink

00:15:48,540 --> 00:15:53,189
from the client to the server we had to

00:15:50,790 --> 00:15:55,980
pink from the service to the client why

00:15:53,189 --> 00:15:58,500
because each time a new connection would

00:15:55,980 --> 00:16:01,439
spot respawn it would steal some of the

00:15:58,500 --> 00:16:05,009
data which arrives to read and that was

00:16:01,439 --> 00:16:07,199
the problem so we used publish and

00:16:05,009 --> 00:16:10,709
subscribe and ready to fix that issue we

00:16:07,199 --> 00:16:13,800
had to to inter tornado communicate

00:16:10,709 --> 00:16:16,290
between tornado instances because we had

00:16:13,800 --> 00:16:18,839
multiple tornadoes and when a connection

00:16:16,290 --> 00:16:20,970
would drop no other tornado instance

00:16:18,839 --> 00:16:22,589
would know that it is dropped so that

00:16:20,970 --> 00:16:25,439
was a big issue it's still an issue and

00:16:22,589 --> 00:16:27,779
I think it is a problem with most with

00:16:25,439 --> 00:16:30,660
most Mart phone connections because they

00:16:27,779 --> 00:16:32,850
dropped frequently our old system also

00:16:30,660 --> 00:16:35,519
had this type of problems but Apache

00:16:32,850 --> 00:16:37,920
would fix it for us you had like when a

00:16:35,519 --> 00:16:40,110
player leaves the game it would leave a

00:16:37,920 --> 00:16:41,910
hanging connection which would drop only

00:16:40,110 --> 00:16:44,550
in three or four minutes but that wasn't

00:16:41,910 --> 00:16:46,860
the issue then now it is an issue so we

00:16:44,550 --> 00:16:49,199
had to fix it with multiple things with

00:16:46,860 --> 00:16:51,240
multiple on multiple places and that was

00:16:49,199 --> 00:16:53,730
the biggest issue another issue is what

00:16:51,240 --> 00:16:56,639
nyse liffe said react and race

00:16:53,730 --> 00:16:59,250
conditions in react so react does not

00:16:56,639 --> 00:17:00,870
allow you to help it allows you to have

00:16:59,250 --> 00:17:03,029
multiple workers right to the same data

00:17:00,870 --> 00:17:05,309
but when you do you have a condition

00:17:03,029 --> 00:17:06,659
which you have to fix react just tells

00:17:05,309 --> 00:17:08,640
you okay you have a condition you have

00:17:06,659 --> 00:17:10,949
two siblings now you have to decide

00:17:08,640 --> 00:17:13,319
which data is the correct one you have

00:17:10,949 --> 00:17:15,750
to merge the data yourself and that was

00:17:13,319 --> 00:17:18,390
an issue to us because if we if two

00:17:15,750 --> 00:17:20,220
workers write the data to react then the

00:17:18,390 --> 00:17:22,380
workers would have to know how to fix

00:17:20,220 --> 00:17:24,809
the issues that arise and since we have

00:17:22,380 --> 00:17:27,000
a lot of requests that come from from

00:17:24,809 --> 00:17:29,460
the games each request has it has its

00:17:27,000 --> 00:17:33,030
own request type we had to know how to

00:17:29,460 --> 00:17:35,100
fix each request each request type and

00:17:33,030 --> 00:17:36,900
that was a big problem so we decided

00:17:35,100 --> 00:17:38,460
okay we're going to just skip the

00:17:36,900 --> 00:17:40,049
problem we're going to pretend the

00:17:38,460 --> 00:17:42,360
problem isn't there we're going to fix

00:17:40,049 --> 00:17:44,669
it like Miss law said you gotta have one

00:17:42,360 --> 00:17:48,299
worker receive all the data from one

00:17:44,669 --> 00:17:51,539
client and this data would arrive in the

00:17:48,299 --> 00:17:55,240
same order and one worker would only

00:17:51,539 --> 00:17:57,550
right for his for heat for his

00:17:55,240 --> 00:18:00,400
and that would allow the client only to

00:17:57,550 --> 00:18:02,950
have one worker for him and that fixed

00:18:00,400 --> 00:18:04,720
the issue because there were no multiple

00:18:02,950 --> 00:18:08,500
workers writing to the same data point

00:18:04,720 --> 00:18:10,780
so another thing sequel data when we use

00:18:08,500 --> 00:18:13,929
sequel it blocks our single threaded

00:18:10,780 --> 00:18:16,570
nature of our workers and since workers

00:18:13,929 --> 00:18:19,030
have to process a lot of requests we had

00:18:16,570 --> 00:18:21,610
to make sure that it they run as fast as

00:18:19,030 --> 00:18:24,220
possible so we decided okay we're going

00:18:21,610 --> 00:18:27,070
to drop we got to push some data to

00:18:24,220 --> 00:18:29,380
additional threads which would write to

00:18:27,070 --> 00:18:31,870
the sequel only when there are no other

00:18:29,380 --> 00:18:34,090
data to process we decided that this

00:18:31,870 --> 00:18:36,220
data is low priority some of the sequel

00:18:34,090 --> 00:18:38,380
data is low priority and it can be

00:18:36,220 --> 00:18:40,660
written in two three or four seconds

00:18:38,380 --> 00:18:42,220
after the request was processed so the

00:18:40,660 --> 00:18:44,020
main thread will not block the main

00:18:42,220 --> 00:18:45,970
trade would be able to process as much

00:18:44,020 --> 00:18:48,820
as a quest as possible and that in

00:18:45,970 --> 00:18:51,820
general would increase the throughput of

00:18:48,820 --> 00:18:54,490
our system another thing tornado and

00:18:51,820 --> 00:18:56,800
ready they don't work together quite

00:18:54,490 --> 00:19:03,160
well because tornado does not allow you

00:18:56,800 --> 00:19:06,070
to block anything it has to go is it is

00:19:03,160 --> 00:19:08,020
synchronous but if you try to block in

00:19:06,070 --> 00:19:10,000
Redis you have to block on the web

00:19:08,020 --> 00:19:12,700
server side and if you block like on a

00:19:10,000 --> 00:19:14,500
block pop blocking pop or honor on us

00:19:12,700 --> 00:19:16,420
subscribe it would block the tornado

00:19:14,500 --> 00:19:19,870
itself completely so there were a couple

00:19:16,420 --> 00:19:21,730
of solutions but both solutions which

00:19:19,870 --> 00:19:23,860
right had some issues so we had to fix

00:19:21,730 --> 00:19:26,020
those issues we had issues when we try

00:19:23,860 --> 00:19:27,850
to disconnect from radius and stuff like

00:19:26,020 --> 00:19:30,550
that so we had to implement some new

00:19:27,850 --> 00:19:33,070
stuff on that part and that was a

00:19:30,550 --> 00:19:35,610
problem and another thing r react we

00:19:33,070 --> 00:19:39,580
have a react of five databases and we

00:19:35,610 --> 00:19:41,590
had one problem when one react instance

00:19:39,580 --> 00:19:42,850
crashed and we didn't know about it

00:19:41,590 --> 00:19:45,820
because we were in the middle of

00:19:42,850 --> 00:19:47,820
development and we have we had this one

00:19:45,820 --> 00:19:50,380
server crash for about one month and

00:19:47,820 --> 00:19:52,960
everything was working fine because we

00:19:50,380 --> 00:19:55,240
are healed itself magically we didn't

00:19:52,960 --> 00:19:57,370
know anything happened until one day we

00:19:55,240 --> 00:19:59,860
tried to connect to this instance that

00:19:57,370 --> 00:20:01,600
was down and we we realized that it

00:19:59,860 --> 00:20:03,610
wasn't responding to pings or anything

00:20:01,600 --> 00:20:06,190
but the data was secure data wasn't

00:20:03,610 --> 00:20:07,570
corrupted everything was working as it

00:20:06,190 --> 00:20:11,150
was supposed to work

00:20:07,570 --> 00:20:13,190
so what's in the future like like we

00:20:11,150 --> 00:20:15,320
said we'd authorised every container

00:20:13,190 --> 00:20:17,390
possible and docker is a great new

00:20:15,320 --> 00:20:19,520
technology and I think you have a great

00:20:17,390 --> 00:20:20,870
talk tomorrow by our colleague which

00:20:19,520 --> 00:20:23,630
will which will show you how to use

00:20:20,870 --> 00:20:26,060
docker in in a development environment I

00:20:23,630 --> 00:20:29,150
I will suggest you listen to that

00:20:26,060 --> 00:20:30,590
because it allows you some great stuff

00:20:29,150 --> 00:20:34,880
especially on the production like this

00:20:30,590 --> 00:20:37,910
you can transfer complete images of your

00:20:34,880 --> 00:20:39,560
pure code between service and doctor now

00:20:37,910 --> 00:20:43,370
has a native bytes and support so you

00:20:39,560 --> 00:20:45,200
can write docker docker containers from

00:20:43,370 --> 00:20:47,320
your Python code which you have for

00:20:45,200 --> 00:20:49,760
automatic deployment and stuff like that

00:20:47,320 --> 00:20:51,920
another thing we are working on is an

00:20:49,760 --> 00:20:54,650
automatic automatic testing facility

00:20:51,920 --> 00:20:59,000
which uses a lot of docker and it allows

00:20:54,650 --> 00:21:01,850
you to allow us to run a suit of tests

00:20:59,000 --> 00:21:04,640
on a new iteration of our web servers

00:21:01,850 --> 00:21:07,430
our dire back-end to see if each request

00:21:04,640 --> 00:21:10,550
that arrives would always yield the same

00:21:07,430 --> 00:21:11,810
response and since we have two

00:21:10,550 --> 00:21:14,780
connections currently we have one

00:21:11,810 --> 00:21:17,120
connection which is HTTP which has

00:21:14,780 --> 00:21:19,130
receiving requests and we have a

00:21:17,120 --> 00:21:21,290
WebSocket that is only for the responses

00:21:19,130 --> 00:21:23,000
to the game we want to unify this to one

00:21:21,290 --> 00:21:25,400
single connection be cut because it

00:21:23,000 --> 00:21:27,560
would be much easier than it is now this

00:21:25,400 --> 00:21:29,030
also brings us some issues but we are

00:21:27,560 --> 00:21:31,040
trying to fix it with one connection and

00:21:29,030 --> 00:21:32,600
one interesting thing when we developed

00:21:31,040 --> 00:21:35,150
our first game using this back-end it

00:21:32,600 --> 00:21:36,920
required five months to do it because we

00:21:35,150 --> 00:21:39,140
had to write the entire back and from

00:21:36,920 --> 00:21:40,910
scratch but the second game was finished

00:21:39,140 --> 00:21:42,620
in month and a half because we only had

00:21:40,910 --> 00:21:44,690
to transfer the game logic from the game

00:21:42,620 --> 00:21:46,580
to the to the back end we only already

00:21:44,690 --> 00:21:49,580
had the entire working back and the web

00:21:46,580 --> 00:21:51,800
service the cue the the workers and we

00:21:49,580 --> 00:21:53,810
only had to transfer how to process the

00:21:51,800 --> 00:21:57,050
requests that arrived to the server and

00:21:53,810 --> 00:21:58,850
that that would allow us to have our new

00:21:57,050 --> 00:22:01,580
games which would which would be

00:21:58,850 --> 00:22:04,100
developed oats also need a lot less time

00:22:01,580 --> 00:22:05,600
than the first game did so that's that's

00:22:04,100 --> 00:22:09,790
it you can ask the general some

00:22:05,600 --> 00:22:09,790
questions if you want thank you

00:22:33,270 --> 00:22:40,960
well we send it in adjacent form we yeah

00:22:38,080 --> 00:22:43,210
the colleague was asking how do we send

00:22:40,960 --> 00:22:46,030
the data from the from the client to the

00:22:43,210 --> 00:22:50,679
server's how this is how it is formatted

00:22:46,030 --> 00:22:53,140
so we use Jason to format the data we

00:22:50,679 --> 00:22:56,169
try to we wait for for like three

00:22:53,140 --> 00:22:58,090
seconds to see all the requests that are

00:22:56,169 --> 00:23:00,730
going to be sent to the server and then

00:22:58,090 --> 00:23:03,940
we send it in a batch if this batch goes

00:23:00,730 --> 00:23:08,880
through it is marked as completed and it

00:23:03,940 --> 00:23:11,919
isn't sent again if it isn't received if

00:23:08,880 --> 00:23:13,809
if a message is it received at it is

00:23:11,919 --> 00:23:16,270
processed it is sent again in a next

00:23:13,809 --> 00:23:18,340
page but the next batch always has the

00:23:16,270 --> 00:23:20,140
same order of messages like they were

00:23:18,340 --> 00:23:22,929
produced in the game so basically we

00:23:20,140 --> 00:23:25,419
have a JSON data which has requests

00:23:22,929 --> 00:23:28,150
which are requests that were generated

00:23:25,419 --> 00:23:30,159
in last 34 seconds that word process

00:23:28,150 --> 00:23:33,220
this is one type and the other type are

00:23:30,159 --> 00:23:36,370
messages that are critical high-priority

00:23:33,220 --> 00:23:38,530
they are sent instantaneously in as a

00:23:36,370 --> 00:23:41,380
one message those messages like our

00:23:38,530 --> 00:23:45,070
stuff like a player tries to attack

00:23:41,380 --> 00:23:46,990
another player a player tries to request

00:23:45,070 --> 00:23:50,080
the current leaderboard and stuff like

00:23:46,990 --> 00:23:52,620
that that so data that is needed as soon

00:23:50,080 --> 00:23:52,620
as possible

00:23:56,970 --> 00:24:01,809
well we didn't have it for the first

00:23:59,289 --> 00:24:04,179
game but we are doing it right now yeah

00:24:01,809 --> 00:24:07,240
we forgot so the colleague was asking

00:24:04,179 --> 00:24:10,149
was our method of testing and staging in

00:24:07,240 --> 00:24:11,889
our system so we didn't have it at the

00:24:10,149 --> 00:24:13,870
beginning but now we are doing this

00:24:11,889 --> 00:24:16,629
automatic testing which would allow us

00:24:13,870 --> 00:24:19,029
that as soon as we do a git push we

00:24:16,629 --> 00:24:21,879
would spin off a new back end in darker

00:24:19,029 --> 00:24:24,789
and it would had have unit tests for

00:24:21,879 --> 00:24:27,399
each for each request type that that is

00:24:24,789 --> 00:24:29,590
in our system and it would try to you to

00:24:27,399 --> 00:24:33,460
start some use cases in the game in it

00:24:29,590 --> 00:24:36,610
it then would would try to run those use

00:24:33,460 --> 00:24:39,429
cases and see if the responses in the

00:24:36,610 --> 00:24:48,490
database in the game RS they are

00:24:39,429 --> 00:24:50,679
supposed to be so okay I Prakash

00:24:48,490 --> 00:24:54,369
regarding apps of service so is that

00:24:50,679 --> 00:24:56,649
you're using um hue and others so what

00:24:54,369 --> 00:24:59,230
was the reason to use Redis full up sub

00:24:56,649 --> 00:25:02,649
service of zero Huber's was not having

00:24:59,230 --> 00:25:05,499
to well the question is why did we use

00:25:02,649 --> 00:25:07,960
we use both zeromq and read this and why

00:25:05,499 --> 00:25:09,490
do we decide to use Redis as a public

00:25:07,960 --> 00:25:12,009
published subscribe service and not

00:25:09,490 --> 00:25:14,590
zeromq so basically when we started

00:25:12,009 --> 00:25:17,470
using 0m view we try to use it in place

00:25:14,590 --> 00:25:20,289
of the custom in-house QB developed and

00:25:17,470 --> 00:25:22,480
we were realized that it didn't support

00:25:20,289 --> 00:25:24,850
what we needed and we already had read

00:25:22,480 --> 00:25:26,799
is doing this stuff and since red is

00:25:24,850 --> 00:25:28,600
very powerful it allows you to do

00:25:26,799 --> 00:25:30,610
blocking pops publish-subscribe key

00:25:28,600 --> 00:25:32,619
value storage everything you need we

00:25:30,610 --> 00:25:35,710
decided that as we are already using

00:25:32,619 --> 00:25:38,639
readies for this kind of stuff we can

00:25:35,710 --> 00:25:41,350
use it as a as a publish-subscribe so

00:25:38,639 --> 00:25:45,820
that's probably the main reason zeromq

00:25:41,350 --> 00:25:47,799
isn't as much it isn't so much used in

00:25:45,820 --> 00:25:54,869
our system as ready sis so basically

00:25:47,799 --> 00:25:57,639
there's a reason often five hot stuff I

00:25:54,869 --> 00:26:01,720
do you went something like oh and it's

00:25:57,639 --> 00:26:03,730
so so basically the question is why did

00:26:01,720 --> 00:26:06,340
we use to NATO and did we try to use

00:26:03,730 --> 00:26:08,500
other technologies such as web socket to

00:26:06,340 --> 00:26:10,119
you and stuff like that so when we

00:26:08,500 --> 00:26:12,309
started developing

00:26:10,119 --> 00:26:14,499
we decided we needed a web server that

00:26:12,309 --> 00:26:16,599
would support a lot larger large number

00:26:14,499 --> 00:26:20,079
of persistent connections and the choice

00:26:16,599 --> 00:26:22,389
we did some research with this some look

00:26:20,079 --> 00:26:24,009
at some documentation and subtropical

00:26:22,389 --> 00:26:26,469
eight and we decided that tornado in

00:26:24,009 --> 00:26:30,129
that point was the best solution because

00:26:26,469 --> 00:26:33,309
it it had a great support and it had

00:26:30,129 --> 00:26:35,439
connections with which tornado connects

00:26:33,309 --> 00:26:36,909
with the reddest connections with 0nq

00:26:35,439 --> 00:26:39,339
and that was what we needed at that time

00:26:36,909 --> 00:26:40,809
so basically that was the reason we did

00:26:39,339 --> 00:26:43,749
it but we are currently looking into

00:26:40,809 --> 00:26:45,249
finding an alternative because we are

00:26:43,749 --> 00:26:46,989
trying like I said we trying to unify

00:26:45,249 --> 00:26:49,539
the connections only have one connection

00:26:46,989 --> 00:26:51,429
and that would allow us to head to try

00:26:49,539 --> 00:26:53,949
new technologies and see if something

00:26:51,429 --> 00:27:00,959
else would fit into the shoes of the

00:26:53,949 --> 00:27:00,959
donut the tornado fits right now so yes

00:27:05,549 --> 00:27:24,039
yeah that's a great question I'm going

00:27:22,539 --> 00:27:26,949
to repeat it so the question is how do

00:27:24,039 --> 00:27:28,839
we deploy web servers because when you

00:27:26,949 --> 00:27:30,609
deploy web service and have if you have

00:27:28,839 --> 00:27:33,129
a persistent connection this persistent

00:27:30,609 --> 00:27:35,199
connection would drop and what the

00:27:33,129 --> 00:27:37,689
clients would then send request to a

00:27:35,199 --> 00:27:42,969
server that doesn't exist basically so

00:27:37,689 --> 00:27:45,429
we have the client is full bulletproof

00:27:42,969 --> 00:27:46,959
so to say to connection drops if a

00:27:45,429 --> 00:27:49,869
connection drops the client would know

00:27:46,959 --> 00:27:53,379
immediately because when the engine eeks

00:27:49,869 --> 00:27:56,109
and tornado drop it it drops the entire

00:27:53,379 --> 00:27:58,899
TCP connection and the client would then

00:27:56,109 --> 00:28:01,209
reconnect immediately to a new web

00:27:58,899 --> 00:28:06,099
server which is deployed automatically

00:28:01,209 --> 00:28:08,919
in that's like a second or two delay but

00:28:06,099 --> 00:28:11,139
since we don't have to have responses

00:28:08,919 --> 00:28:13,419
that are immediate we can veg the we can

00:28:11,139 --> 00:28:16,599
send the messages in batch in every four

00:28:13,419 --> 00:28:18,309
or five seconds that isn't a nation

00:28:16,599 --> 00:28:20,379
issue so we decided we can stop the

00:28:18,309 --> 00:28:22,479
docker containers and start them again

00:28:20,379 --> 00:28:23,860
in one second and the client would not

00:28:22,479 --> 00:28:27,090
know that anything happened

00:28:23,860 --> 00:28:27,090
it would just reconnect again

00:28:39,270 --> 00:28:43,780
okay the question is why did we use web

00:28:41,590 --> 00:28:47,559
sockets and only for the for the

00:28:43,780 --> 00:28:50,350
returning party mean yeah we wanted to

00:28:47,559 --> 00:28:52,480
have a persistent connection and one of

00:28:50,350 --> 00:28:55,480
the solutions was to have a low level

00:28:52,480 --> 00:28:57,790
TCP connection but we wanted to have a

00:28:55,480 --> 00:29:00,100
connection that is a more of a higher

00:28:57,790 --> 00:29:01,990
level sort of like HTTP because our

00:29:00,100 --> 00:29:06,010
requests that are arrived to the server

00:29:01,990 --> 00:29:07,900
are also HTTP connections so we decided

00:29:06,010 --> 00:29:10,090
to use web sockets because they are up

00:29:07,900 --> 00:29:12,910
an upgrade to our current architecture

00:29:10,090 --> 00:29:14,830
that we already use and they are an

00:29:12,910 --> 00:29:17,950
applique to do the connection we already

00:29:14,830 --> 00:29:19,900
use for receiving requests and the only

00:29:17,950 --> 00:29:22,000
we needed something that would that

00:29:19,900 --> 00:29:24,010
would have a stream like connection and

00:29:22,000 --> 00:29:27,450
web socket fitted the bill particular

00:29:24,010 --> 00:29:27,450

YouTube URL: https://www.youtube.com/watch?v=5DhDruQSIMk


