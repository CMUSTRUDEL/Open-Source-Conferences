Title: Heiko - The Shogun Machine Learning Toolbox
Publication date: 2015-04-18
Playlist: EuroPython 2014
Description: 
	Heiko - The Shogun Machine Learning Toolbox
[EuroPython 2014]
[24 July 2014]

We present the Shogun Machine Learning Toolbox, a framework for Machine Learning, which is the art of finding structure in data, with applications in object recognition, brain-computer interfaces, robotics,  stock-prices prediction, etc. We give a gentle introduction to ML and Shogun's Python interface, focussing on intuition and visualisation.

-----

We present the Shogun Machine Learning Toolbox, a unified framework for Machine Learning  algorithms. Machine Learning (ML) is the art of finding structure in data in an automated way and has given rise to a wide range of applications such as recommendation systems, object recognition, brain-computer interfaces, robotics, predicting stock prices, etc.

Our toolbox offers extensive bindings with other software and computing languages, Python being the major target. The library was initiated in 1999 and remained under heavy development henceforth. In addition to its mature core-framework, Shogun offers state-of-the-art techniques based on latest ML research. This is partly made possible by the 21 Google Summer of Code projects (5+8+8 since 2011) that our students successfully completed. Shogun's codebase has &gt;20k commits made by &gt;100 contributors representing &gt;500k lines of code. While its core is written in C++, a unique of technique for generating interfaces allows usage from a wide range of target languages -- under the same syntax. This includes in particular Python, but also Matlab/Octave, Java, C#, R, ruby, and more. We believe that users should be able to choose their favourite language rather than us dictating this choice. The same applies for supported OS (Linux, Mac, Win). Shogun is part of Debian Linux.

Features of Shogun include most classical ML methods such as classification, regression, dimensionality reduction, clustering, etc, most of them in different flavours. All implemented algorithms in Shogun work on a modular data representation, which allows to easily switch between different sorts of objects as for example strings or matrices. Common ML-tasks and data IO can be carried under a unified interface. This is also true for the various external open-source libraries that are embedded within Shogun.

Code examples are provided for all implemented algorithms. The main and most complete set of examples is in the Python language. In addition, in order to push usage of Shogun in education at universities, we recently started adding more illustrative IPython notebooks. A growing list of statically rendered versions are readily available from our [website](http://www.shogun-toolbox.org/page/documentation/notebook) and implement a cross-over of tutorial-style explanations, code, and visualization examples. We even took this up a notch and started building our own IPython-notebook server with Shogun installed in the cloud at (try cloud button in notebook view) . This allows users to try Shogun without installation via the IPython notebook web interface. All example notebooks can be loaded, interactively modified, and executed. In addition, using the Python Django framework, we built a collection of interactive web-demos where users can play around with basic ML algorithms, [demos](http://www.shogun-toolbox.org/page/documentation/demo)

In the proposed talk, we will give a gentle and general introduction to ML and the core functionality of Shogun, with a focus on its Python interface. This includes solving basic ML tasks such as classification and regression and some of the more recent features, such as last year's GSoC projects and their IPython notebook writeups. ML material will be presented with a focus on intuition and visualisation and no previous familiarity with ML methods is required.

## Key points in the talk

 * What are the goals in ML?
 * Example problems in ML (classification, regression, clustering)
 * Some basic algorithm ideas
 * Focus on Visualisation, not Maths

## Intended Audience

* All people dealing with data (data scientists, big-data hackers) who are looking for tools to deal with it
 * People with a general interest but no education in Machine Learning
 * People interested in the technology behind Shogun (swig, cloud notebook server, web-demos)
 * People from the ML community (scipy-stack)
 * ML scientists/Statisticians

## Code examples

 * [Classification](https://github.com/shogun-toolbox/shogun/blob/develop/examples/undocumented/python_modular/classifier_libsvm_modular.py)
 * [Clustering](https://github.com/shogun-toolbox/shogun/blob/develop/examples/undocumented/python_modular/graphical/em_2d_gmm.py)
 * [Source seperation](https://github.com/shogun-toolbox/shogun/blob/develop/examples/undocumented/python_modular/graphical/converter_jade_bss.py)
 * [IPython notebook examples](http://www.shogun-toolbox.org/page/documentation/notebook)

### Slide examples
See our Europython 2010 [slides](https://www.drop
Captions: 
	00:00:14,769 --> 00:00:20,830
hello and good morning to everyone our

00:00:17,920 --> 00:00:23,320
first speaker of the day will be Hagel

00:00:20,830 --> 00:00:26,020
Stratman and he will be speaking about

00:00:23,320 --> 00:00:35,739
the Shogun machine learning tool box hey

00:00:26,020 --> 00:00:37,840
go yeah hi good morning quite surprised

00:00:35,739 --> 00:00:40,239
is that full here given the early time

00:00:37,840 --> 00:00:42,660
of the day but that's good I guess so

00:00:40,239 --> 00:00:46,030
yes i said i'm michael schlapman and

00:00:42,660 --> 00:00:49,090
this is what I'll be talking about so

00:00:46,030 --> 00:00:50,650
since it's a quite short talk so I can't

00:00:49,090 --> 00:00:52,210
go really into details I just keep a

00:00:50,650 --> 00:00:54,460
high level overview over this project

00:00:52,210 --> 00:00:56,230
first tell you a bit about you know what

00:00:54,460 --> 00:00:58,510
it's about and the size and what to

00:00:56,230 --> 00:01:00,250
expect and then I'm gonna tell you a bit

00:00:58,510 --> 00:01:02,680
about what machine learning is about and

00:01:00,250 --> 00:01:03,940
then I'm going to talk about the you

00:01:02,680 --> 00:01:05,350
know the machine learning features that

00:01:03,940 --> 00:01:07,960
we have in sugarland i'm going to talk a

00:01:05,350 --> 00:01:10,719
bit about you know what you can do and

00:01:07,960 --> 00:01:12,369
since this is a quiet i feel you know

00:01:10,719 --> 00:01:14,170
Mickey place I can talk a bit about

00:01:12,369 --> 00:01:15,520
technical details of you know things

00:01:14,170 --> 00:01:18,009
that are going on under the hood and

00:01:15,520 --> 00:01:19,389
i'll close with some remarks on the nice

00:01:18,009 --> 00:01:23,229
community that evolves around the

00:01:19,389 --> 00:01:24,939
project recently so first um as we are

00:01:23,229 --> 00:01:26,590
an open source project kind of only

00:01:24,939 --> 00:01:27,999
everything that you know people talk

00:01:26,590 --> 00:01:29,319
about these things is quite a local

00:01:27,999 --> 00:01:33,159
perspective so here's some background

00:01:29,319 --> 00:01:34,569
about me so first got you know I before

00:01:33,159 --> 00:01:36,249
started doing these kind of things I got

00:01:34,569 --> 00:01:38,200
a bit distracted and pretended to be a

00:01:36,249 --> 00:01:39,520
musician and then finally started

00:01:38,200 --> 00:01:40,959
studying computer science and then

00:01:39,520 --> 00:01:44,020
machine learning in London and currently

00:01:40,959 --> 00:01:46,060
I do PhD UCL do neuroscience machine

00:01:44,020 --> 00:01:47,799
learning and for the people you know who

00:01:46,060 --> 00:01:50,319
know these words these are my research

00:01:47,799 --> 00:01:53,439
topics I mean particular interest in

00:01:50,319 --> 00:01:55,450
open source so I joined shoe goon to

00:01:53,439 --> 00:01:57,819
bring together my open source and my

00:01:55,450 --> 00:02:02,109
machine only interest in 2010 and I'm

00:01:57,819 --> 00:02:05,289
kind of guiding the project along um so

00:02:02,109 --> 00:02:07,389
what is some she learning about how many

00:02:05,289 --> 00:02:10,959
of you are kind of familiar with with

00:02:07,389 --> 00:02:13,890
this term or what it means ok so quite a

00:02:10,959 --> 00:02:16,150
few so this is a very very high level

00:02:13,890 --> 00:02:17,560
examples of applications what you can do

00:02:16,150 --> 00:02:21,250
and the things that I've come across so

00:02:17,560 --> 00:02:22,870
far so machine learning is a science and

00:02:21,250 --> 00:02:24,770
its really a science of patterns and

00:02:22,870 --> 00:02:26,990
information um

00:02:24,770 --> 00:02:28,640
well what does that mean it's it's kind

00:02:26,990 --> 00:02:30,110
of abstract thing and it involves lots

00:02:28,640 --> 00:02:32,870
of mathematics but what can you do with

00:02:30,110 --> 00:02:34,490
it it's quite useful for automating

00:02:32,870 --> 00:02:36,860
things for example for recognizing

00:02:34,490 --> 00:02:39,050
things and so one project I worked on

00:02:36,860 --> 00:02:40,820
last year was was about detecting frauds

00:02:39,050 --> 00:02:42,560
and our plane wings and there was a

00:02:40,820 --> 00:02:44,750
company what they did they injected

00:02:42,560 --> 00:02:46,520
ultrasonic sound waves into the airplane

00:02:44,750 --> 00:02:48,380
wing and the sound waves travel through

00:02:46,520 --> 00:02:50,420
the material they got reflected they

00:02:48,380 --> 00:02:52,130
come back you record them and then if

00:02:50,420 --> 00:02:54,170
there's like a little crack or something

00:02:52,130 --> 00:02:55,850
in the wing you can see this in the

00:02:54,170 --> 00:02:57,470
reflection and this is you want to do

00:02:55,850 --> 00:02:58,970
this in an automated way and it's kind

00:02:57,470 --> 00:03:02,510
of hard for humans to do this actually

00:02:58,970 --> 00:03:07,640
um so we developed some tools to do this

00:03:02,510 --> 00:03:09,260
automatically another one is so I like

00:03:07,640 --> 00:03:10,790
going to the Sun and then every year I

00:03:09,260 --> 00:03:13,250
go to the skin doctor and then they tell

00:03:10,790 --> 00:03:14,450
me off for getting sunburns and they

00:03:13,250 --> 00:03:16,850
always do this thing they always take a

00:03:14,450 --> 00:03:19,100
little photographs of my of my skin and

00:03:16,850 --> 00:03:21,140
then nice listening that scans them foo

00:03:19,100 --> 00:03:23,270
and what it actually does it looks for

00:03:21,140 --> 00:03:25,640
characteristic patterns in these

00:03:23,270 --> 00:03:27,380
photographs photographs that might

00:03:25,640 --> 00:03:30,110
indicate that you know there's something

00:03:27,380 --> 00:03:32,660
dangerous going on and another nice

00:03:30,110 --> 00:03:34,040
example is so two years ago was in three

00:03:32,660 --> 00:03:36,620
years ago I was in India and it was

00:03:34,040 --> 00:03:39,320
using my my credit card and then he got

00:03:36,620 --> 00:03:41,930
he got blocked immediately and I called

00:03:39,320 --> 00:03:44,390
the banks like sorry I need money and

00:03:41,930 --> 00:03:46,220
they're like ah we have some we thought

00:03:44,390 --> 00:03:47,780
this was some fraud so that computer

00:03:46,220 --> 00:03:51,260
system told them or he is likely to be

00:03:47,780 --> 00:03:53,090
some fraud and there are more examples

00:03:51,260 --> 00:03:54,709
for example you you might want to

00:03:53,090 --> 00:03:57,170
predict things not just recognize or

00:03:54,709 --> 00:04:00,910
detect things so another project I

00:03:57,170 --> 00:04:04,640
worked on was actually on predicting how

00:04:00,910 --> 00:04:06,200
HIV of a bunch of patients reacts to a

00:04:04,640 --> 00:04:08,720
certain treatment whether it's resistant

00:04:06,200 --> 00:04:11,030
or not so worried is we took the DNA of

00:04:08,720 --> 00:04:13,190
the of the HIV viruses of the individual

00:04:11,030 --> 00:04:15,110
patients and we put it into our pattern

00:04:13,190 --> 00:04:17,180
recognizing machine learning algorithm

00:04:15,110 --> 00:04:19,570
and then it all us don't give this

00:04:17,180 --> 00:04:23,630
patient this particular treatment and

00:04:19,570 --> 00:04:25,580
this is all learned from from from data

00:04:23,630 --> 00:04:27,260
so then more things I do a PhD in

00:04:25,580 --> 00:04:28,580
neuroscience we look a lot at you know

00:04:27,260 --> 00:04:30,350
brain scans and things like this but

00:04:28,580 --> 00:04:31,460
there's also commercial interests like

00:04:30,350 --> 00:04:33,020
you know these companies like Google

00:04:31,460 --> 00:04:36,420
Amazon Netflix i want to recommend you

00:04:33,020 --> 00:04:37,920
things that you might like um you

00:04:36,420 --> 00:04:39,600
some so some people are sometimes

00:04:37,920 --> 00:04:42,860
confused machine learning also is very

00:04:39,600 --> 00:04:45,360
related to computational statistics and

00:04:42,860 --> 00:04:47,100
so there's lots of exchange and for me

00:04:45,360 --> 00:04:48,540
it's really the same thing but you could

00:04:47,100 --> 00:04:50,190
maybe say machining once you automate

00:04:48,540 --> 00:04:52,590
things where statistic smart about

00:04:50,190 --> 00:04:54,720
understanding a certain process and you

00:04:52,590 --> 00:04:57,240
know all these passwords that you know

00:04:54,720 --> 00:04:59,610
around like big data deep learning is a

00:04:57,240 --> 00:05:01,710
nice one currently you can they're all

00:04:59,610 --> 00:05:04,050
kind of relate it and obviously you can

00:05:01,710 --> 00:05:07,890
use all this stuff to build robots okay

00:05:04,050 --> 00:05:09,720
so here's a bit about sugar this is our

00:05:07,890 --> 00:05:13,230
latest version we are an open source

00:05:09,720 --> 00:05:14,580
project we made public since 2004 so

00:05:13,230 --> 00:05:16,560
this means we're public for ten years

00:05:14,580 --> 00:05:18,120
now is quite old we currently eight core

00:05:16,560 --> 00:05:19,830
developers that kind of spend you know

00:05:18,120 --> 00:05:22,590
time every day developing and we got

00:05:19,830 --> 00:05:25,980
about 20 regular contributors so we be

00:05:22,590 --> 00:05:27,210
quite a big project actually given that

00:05:25,980 --> 00:05:29,280
we just you know coming from the

00:05:27,210 --> 00:05:31,290
community the original background is

00:05:29,280 --> 00:05:32,760
from academia so people at University

00:05:31,290 --> 00:05:34,980
have been developing this and i work at

00:05:32,760 --> 00:05:36,630
university so it's like Adam it but we

00:05:34,980 --> 00:05:39,420
are getting more into more applied and

00:05:36,630 --> 00:05:41,040
regions and what really boosted the

00:05:39,420 --> 00:05:42,600
project is four years ago we started

00:05:41,040 --> 00:05:44,730
doing the google Summer of Code and so

00:05:42,600 --> 00:05:48,060
far we've been doing 29 projects that's

00:05:44,730 --> 00:05:51,180
29 times 3 month full-time work so we

00:05:48,060 --> 00:05:52,890
got quite some impact with this and so

00:05:51,180 --> 00:05:54,780
I'd say this now and a couple of more

00:05:52,890 --> 00:05:57,000
times we'll do a workshop actually this

00:05:54,780 --> 00:05:59,670
weekend Sunday Monday which is free so

00:05:57,000 --> 00:06:01,800
feel free to drop in send I'll give you

00:05:59,670 --> 00:06:04,800
some details so he's a bit more about

00:06:01,800 --> 00:06:08,340
the size of the project I who's familiar

00:06:04,800 --> 00:06:10,290
with olo the website hollow few okay

00:06:08,340 --> 00:06:12,630
last time I gave this talk at University

00:06:10,290 --> 00:06:14,700
nobody know the website so it's kind of

00:06:12,630 --> 00:06:16,350
a they crawl get up to to get statistics

00:06:14,700 --> 00:06:18,300
so we got you know quite a few commit to

00:06:16,350 --> 00:06:19,590
get quite a few contributors I really

00:06:18,300 --> 00:06:21,060
like these comments here that they

00:06:19,590 --> 00:06:22,680
generate so we have a very low number of

00:06:21,060 --> 00:06:24,630
source code comments it's mostly written

00:06:22,680 --> 00:06:26,280
in C++ we have a well-established made

00:06:24,630 --> 00:06:29,610
sure code base of whatever this means

00:06:26,280 --> 00:06:32,550
and hundred sixty-two years of efforts

00:06:29,610 --> 00:06:34,440
so it's quite nice and yeah we got quite

00:06:32,550 --> 00:06:36,330
a few comments so here's the history num

00:06:34,440 --> 00:06:37,950
of code so we you know I could I could

00:06:36,330 --> 00:06:40,530
talk about exponential growth here and

00:06:37,950 --> 00:06:42,510
stuff but I won't we about to hit the

00:06:40,530 --> 00:06:44,040
million lines of code which is nice he

00:06:42,510 --> 00:06:45,810
is the number of commits per month so

00:06:44,040 --> 00:06:48,150
you know you see the summer kind of

00:06:45,810 --> 00:06:49,000
boost things and but even in winter we

00:06:48,150 --> 00:06:51,460
still have you known

00:06:49,000 --> 00:06:53,950
about 23 commits per day so it's quite

00:06:51,460 --> 00:06:56,110
an active project just to like set you

00:06:53,950 --> 00:06:59,800
up what we are talking about okay

00:06:56,110 --> 00:07:04,480
machine learning so you can't really see

00:06:59,800 --> 00:07:06,400
this one well okay so the most classic

00:07:04,480 --> 00:07:08,140
textbook machine learning is you know

00:07:06,400 --> 00:07:10,240
can be categorized into you know

00:07:08,140 --> 00:07:12,690
supervised learning and supervised

00:07:10,240 --> 00:07:16,240
learning and some other categories so

00:07:12,690 --> 00:07:17,890
all these textbook algorithms we have

00:07:16,240 --> 00:07:21,340
them implemented so this one is

00:07:17,890 --> 00:07:24,580
supervised learning and okay so I gotta

00:07:21,340 --> 00:07:26,380
hurry so this is learning from data that

00:07:24,580 --> 00:07:28,210
somebody labeled for you somebody gives

00:07:26,380 --> 00:07:29,740
you some information that he knows about

00:07:28,210 --> 00:07:32,680
the data and then you're trying to come

00:07:29,740 --> 00:07:34,840
up with this characterization of the

00:07:32,680 --> 00:07:37,450
data for some data that you haven't seen

00:07:34,840 --> 00:07:39,340
yet so you take you you know you take

00:07:37,450 --> 00:07:40,930
your your all your scans of your DNA

00:07:39,340 --> 00:07:42,640
that you had so far and that you know

00:07:40,930 --> 00:07:44,620
whether the treatment was effective the

00:07:42,640 --> 00:07:47,110
HIV treatment and then you try to

00:07:44,620 --> 00:07:48,700
predict this for a new patient so if you

00:07:47,110 --> 00:07:50,560
open textbook all these methods you'll

00:07:48,700 --> 00:07:51,970
pop across them so you know it's poor

00:07:50,560 --> 00:07:53,520
vector machines there was a password a

00:07:51,970 --> 00:07:55,510
couple of years ago orgasm processes

00:07:53,520 --> 00:07:56,919
logistic regression is something that

00:07:55,510 --> 00:07:58,360
you know big companies County are very

00:07:56,919 --> 00:08:00,430
interested in because you can paralyze

00:07:58,360 --> 00:08:05,080
it all these things are implemented

00:08:00,430 --> 00:08:07,240
within sugar and yeah I think I'll leave

00:08:05,080 --> 00:08:09,490
this or no so the other the other class

00:08:07,240 --> 00:08:11,260
of algorithms that we have quite a bit

00:08:09,490 --> 00:08:13,750
on is unsupervised learning and there

00:08:11,260 --> 00:08:15,520
it's a bit different you're just getting

00:08:13,750 --> 00:08:17,800
a bunch of data with no information to

00:08:15,520 --> 00:08:19,660
it and you you're trying to come up with

00:08:17,800 --> 00:08:22,900
a characterization of the process that

00:08:19,660 --> 00:08:24,910
generated the data which we write like

00:08:22,900 --> 00:08:26,200
this and again if you open textbook the

00:08:24,910 --> 00:08:27,790
all kinds of algorithms like you know

00:08:26,200 --> 00:08:29,169
clustering algorithms okay mean so you

00:08:27,790 --> 00:08:31,390
have a bunch of points and you want to

00:08:29,169 --> 00:08:32,710
find say you assume that three clusters

00:08:31,390 --> 00:08:34,750
what are the clusters where are there

00:08:32,710 --> 00:08:36,669
how can I know I can I characterize them

00:08:34,750 --> 00:08:38,229
can i use them for labeling things and

00:08:36,669 --> 00:08:40,870
we got quite a few late models if you

00:08:38,229 --> 00:08:42,849
know what that means which is basically

00:08:40,870 --> 00:08:46,120
trying to find a lower dimensional

00:08:42,849 --> 00:08:48,220
representation of your your information

00:08:46,120 --> 00:08:50,740
in order to describe it in a more

00:08:48,220 --> 00:08:54,280
efficient way for communication or to

00:08:50,740 --> 00:08:56,500
understand it um since I'm a bit in a

00:08:54,280 --> 00:08:58,150
rush oh go on so these are all textbook

00:08:56,500 --> 00:09:00,070
methods but we also got you know quite a

00:08:58,150 --> 00:09:00,700
few researchers implementing the work

00:09:00,070 --> 00:09:02,620
for

00:09:00,700 --> 00:09:04,270
tool box so that's for example what I do

00:09:02,620 --> 00:09:06,580
so the stuff is actually not available

00:09:04,270 --> 00:09:08,380
somewhere else and so these you know

00:09:06,580 --> 00:09:11,050
these things were kind of hot topics in

00:09:08,380 --> 00:09:14,200
in machine learning recently or still

00:09:11,050 --> 00:09:16,840
are like here this guy and they're all

00:09:14,200 --> 00:09:18,610
in there to get a feeling for what's in

00:09:16,840 --> 00:09:20,020
there have a look at our collection of I

00:09:18,610 --> 00:09:21,250
path and notebooks on our website

00:09:20,020 --> 00:09:22,900
they're quite nice they're kind of

00:09:21,250 --> 00:09:27,520
tutorials about the methods and what you

00:09:22,900 --> 00:09:28,870
can do with them skip this one here when

00:09:27,520 --> 00:09:30,190
you do machine learning in practice you

00:09:28,870 --> 00:09:31,930
have all sorts of problems like you want

00:09:30,190 --> 00:09:33,340
to import your data your nap reprocess

00:09:31,930 --> 00:09:35,500
it and all these things you can do this

00:09:33,340 --> 00:09:36,790
with the toolbox so you can you can have

00:09:35,500 --> 00:09:38,740
different types of different

00:09:36,790 --> 00:09:41,380
representations of data like you know

00:09:38,740 --> 00:09:43,600
dense matrices sparse data strings

00:09:41,380 --> 00:09:45,250
collections of documents data streams

00:09:43,600 --> 00:09:46,600
that's quite nice and that's kind of a

00:09:45,250 --> 00:09:48,040
unique feature of our toolbox that we

00:09:46,600 --> 00:09:49,930
can just handle all these strings on a

00:09:48,040 --> 00:09:51,880
unified framework they're different data

00:09:49,930 --> 00:09:53,860
types their pre processing tools their

00:09:51,880 --> 00:09:55,390
methods to evaluate your algorithms to

00:09:53,860 --> 00:09:58,210
tune the parameters and all this so it

00:09:55,390 --> 00:10:03,010
kind of caught or comes all included to

00:09:58,210 --> 00:10:04,590
make your life easier okay let's re what

00:10:03,010 --> 00:10:07,120
I'm going to say about machine learning

00:10:04,590 --> 00:10:08,920
so here are some technical features that

00:10:07,120 --> 00:10:11,680
might be interesting for you guys so

00:10:08,920 --> 00:10:13,660
we're written in C++ so might ask why

00:10:11,680 --> 00:10:17,140
are you analyzing conference well we

00:10:13,660 --> 00:10:18,730
provide we provide automatic interfaces

00:10:17,140 --> 00:10:22,450
to a bunch of languages i'm going to

00:10:18,730 --> 00:10:23,890
talk about this in a minute so but the

00:10:22,450 --> 00:10:25,660
reason why we're written in C++ is

00:10:23,890 --> 00:10:27,670
because we can then actually expose a

00:10:25,660 --> 00:10:30,520
framework to a lot of languages and

00:10:27,670 --> 00:10:32,830
since we are you know doing quite low

00:10:30,520 --> 00:10:34,360
level things it's you know we can do we

00:10:32,830 --> 00:10:35,890
can do efficient code we can handle the

00:10:34,360 --> 00:10:38,440
memory manually and do these kind of

00:10:35,890 --> 00:10:40,240
things we use quite a few you know

00:10:38,440 --> 00:10:41,680
cutting edge things for linear algebra

00:10:40,240 --> 00:10:43,540
and numerical computation like I ghen

00:10:41,680 --> 00:10:44,620
and recently we started using the NSEL

00:10:43,540 --> 00:10:48,100
and these kind of things to do

00:10:44,620 --> 00:10:49,840
computational GPUs if you if you want

00:10:48,100 --> 00:10:51,580
again I've get a grasp for for the

00:10:49,840 --> 00:10:55,060
interface have a look at our class list

00:10:51,580 --> 00:10:57,970
oxygen generator class list okay and now

00:10:55,060 --> 00:10:59,380
he'll see one of the nicest things so we

00:10:57,970 --> 00:11:01,360
don't believe that it's good to tell

00:10:59,380 --> 00:11:03,370
users what programming language to use

00:11:01,360 --> 00:11:06,160
but obviously we want to since we love

00:11:03,370 --> 00:11:07,720
all of python we use it a lot like I use

00:11:06,160 --> 00:11:09,790
it a lot from a research when I have an

00:11:07,720 --> 00:11:12,600
interface to this so you Swick does

00:11:09,790 --> 00:11:16,329
anybody need not know what's weakness

00:11:12,600 --> 00:11:19,180
okay that's a that's a magic tool and

00:11:16,329 --> 00:11:21,639
what we do is we write our C++ classes

00:11:19,180 --> 00:11:23,410
we define a bunch of type maps that

00:11:21,639 --> 00:11:26,199
converts you know see types to save

00:11:23,410 --> 00:11:27,610
Python types and then we have a list of

00:11:26,199 --> 00:11:29,949
classes that we want to expose then we

00:11:27,610 --> 00:11:32,980
press a button and at compile time the

00:11:29,949 --> 00:11:34,630
Swick thing generates interfaces to all

00:11:32,980 --> 00:11:36,279
these languages so like whenever

00:11:34,630 --> 00:11:38,079
implementing new algorithm and then I

00:11:36,279 --> 00:11:40,089
press a button then I can use it from

00:11:38,079 --> 00:11:43,149
Python i'm going to show you an example

00:11:40,089 --> 00:11:45,730
in a minute this is quite this is quite

00:11:43,149 --> 00:11:48,160
neat because we have interfaces to you

00:11:45,730 --> 00:11:49,690
know python octave matlab java our Ruby

00:11:48,160 --> 00:11:53,139
lure C sharp and it's all the same

00:11:49,690 --> 00:11:56,470
interface really with certain syntactic

00:11:53,139 --> 00:11:58,509
changes so in see this looks like this

00:11:56,470 --> 00:12:00,040
you know if you know C code you know you

00:11:58,509 --> 00:12:01,959
have your point on your type and your

00:12:00,040 --> 00:12:04,180
template and you find a new instance of

00:12:01,959 --> 00:12:06,190
a class and some other bunch of new

00:12:04,180 --> 00:12:10,389
instances and then you call methods on

00:12:06,190 --> 00:12:12,250
these classes five minutes thanks so now

00:12:10,389 --> 00:12:14,470
we go to python and this i do the same

00:12:12,250 --> 00:12:17,529
thing here but rather than plugging in a

00:12:14,470 --> 00:12:19,240
a 2d matrix at a pointer to a matrix are

00:12:17,529 --> 00:12:20,920
now plug in an umpire array but it's

00:12:19,240 --> 00:12:22,779
really the same interface yeah i define

00:12:20,920 --> 00:12:24,190
a bunch of instances and a bunch of

00:12:22,779 --> 00:12:26,410
classes and I call methods on these

00:12:24,190 --> 00:12:28,060
classes and then if I do a prediction

00:12:26,410 --> 00:12:30,279
yet china support vector machine I can

00:12:28,060 --> 00:12:32,649
get these here in Python you know the

00:12:30,279 --> 00:12:34,990
first index is one but if I go to octave

00:12:32,649 --> 00:12:36,790
and the first so it's zero but if I go

00:12:34,990 --> 00:12:38,800
to octave you know things don't really

00:12:36,790 --> 00:12:40,420
change and the first thing exists one

00:12:38,800 --> 00:12:42,160
but it's the same code that's running

00:12:40,420 --> 00:12:44,560
under the hood so it's quite neat I

00:12:42,160 --> 00:12:48,910
think so got to get java it's a bit more

00:12:44,560 --> 00:12:50,649
messy um okay then finally another thing

00:12:48,910 --> 00:12:53,019
that we love Python force ipython

00:12:50,649 --> 00:12:54,819
notebooks so I said we use this mainly

00:12:53,019 --> 00:12:57,160
we use this thing for our documentation

00:12:54,819 --> 00:12:58,930
quite a bit and what another thing we've

00:12:57,160 --> 00:13:00,610
set up is we've set up a web service we

00:12:58,930 --> 00:13:02,680
can try show them without installing it

00:13:00,610 --> 00:13:04,630
we're running an ipad notebook server in

00:13:02,680 --> 00:13:06,910
the cloud you can connect it with your

00:13:04,630 --> 00:13:09,069
with a github account and it can you can

00:13:06,910 --> 00:13:10,959
run our example notebooks after mitt is

00:13:09,069 --> 00:13:13,689
currently broken we broke it this week

00:13:10,959 --> 00:13:16,060
it's bad but they will fix it soon and

00:13:13,689 --> 00:13:18,069
we also got a bunch of interactive web

00:13:16,060 --> 00:13:20,050
demos like OCR recognition written in

00:13:18,069 --> 00:13:23,680
jungle and it's also quite nice under

00:13:20,050 --> 00:13:24,790
this link here okay I'm not going to

00:13:23,680 --> 00:13:27,460
talk about this

00:13:24,790 --> 00:13:29,080
we do the or testing use buildbot so we

00:13:27,460 --> 00:13:30,820
got quite a few builds that's that's

00:13:29,080 --> 00:13:35,710
also quite like this we got fedora a

00:13:30,820 --> 00:13:38,620
freebsd windows mac that offline in the

00:13:35,710 --> 00:13:40,180
screenshot but actually do work and last

00:13:38,620 --> 00:13:42,730
two minutes i'm going to talk a bit

00:13:40,180 --> 00:13:44,320
about our community so this is really

00:13:42,730 --> 00:13:45,610
the nicest thing about the project for

00:13:44,320 --> 00:13:48,100
me at least meeting all these people so

00:13:45,610 --> 00:13:49,780
we got quite a few quite a quite an

00:13:48,100 --> 00:13:51,430
active mailing list in the you know an

00:13:49,780 --> 00:13:55,450
active IC channel that turns out here

00:13:51,430 --> 00:13:58,300
this guy who's is the the the the the

00:13:55,450 --> 00:14:00,100
guy who introduced me Daniel he actually

00:13:58,300 --> 00:14:02,950
got back to us and I see we already knew

00:14:00,100 --> 00:14:04,180
each other so that's nice we got you

00:14:02,950 --> 00:14:06,250
know all sorts of people like this guy

00:14:04,180 --> 00:14:08,440
from Spain hear he's quite active and

00:14:06,250 --> 00:14:09,940
there's few people you know sitting in

00:14:08,440 --> 00:14:13,540
the know when Russia and just writing

00:14:09,940 --> 00:14:14,800
awesome code and I met few of them last

00:14:13,540 --> 00:14:15,910
year's kind of hard to talk to them

00:14:14,800 --> 00:14:18,490
because they don't speak English

00:14:15,910 --> 00:14:21,760
properly but yeah it's nice this is this

00:14:18,490 --> 00:14:25,360
guy here lambda wise living in Mumbai he

00:14:21,760 --> 00:14:27,070
works 26 hours a day and why it's really

00:14:25,360 --> 00:14:28,810
good stuff so it's nice it's nice

00:14:27,070 --> 00:14:30,670
talking to all these people have a have

00:14:28,810 --> 00:14:35,140
a look and I github page on our contact

00:14:30,670 --> 00:14:36,700
page and then some of code is as I

00:14:35,140 --> 00:14:38,800
mentioned before this really boosted I

00:14:36,700 --> 00:14:40,270
assume everyone here knows about this so

00:14:38,800 --> 00:14:42,880
we got we got currently we got eight

00:14:40,270 --> 00:14:45,520
project running and I'm mentoring three

00:14:42,880 --> 00:14:47,560
so it don't really sleep these days but

00:14:45,520 --> 00:14:49,180
it's quite cool so um if you're

00:14:47,560 --> 00:14:50,950
interested in machine learning either of

00:14:49,180 --> 00:14:54,640
mentoring a project or joining as a

00:14:50,950 --> 00:14:56,470
student get back to us so few future

00:14:54,640 --> 00:14:59,590
ideas we we just found it a non-profit

00:14:56,470 --> 00:15:01,060
Association to take we want to be able

00:14:59,590 --> 00:15:02,800
to take donations many open-source

00:15:01,060 --> 00:15:04,540
projects do these these days we can be

00:15:02,800 --> 00:15:06,790
transferring our license from GPL to PSD

00:15:04,540 --> 00:15:08,230
if people know what that means and we're

00:15:06,790 --> 00:15:09,580
kind of aiming for using sugar in

00:15:08,230 --> 00:15:11,710
educational purposes but also an

00:15:09,580 --> 00:15:13,780
industry and we also organize workshops

00:15:11,710 --> 00:15:16,810
so here's like we have a youtube footage

00:15:13,780 --> 00:15:18,910
over last year's workshop the next one

00:15:16,810 --> 00:15:20,530
is on Sunday and on Monday we got a

00:15:18,910 --> 00:15:22,270
hands-on session on sunday where you can

00:15:20,530 --> 00:15:24,490
learn how to use sugar from a practical

00:15:22,270 --> 00:15:26,800
perspective and like a bunch of talks

00:15:24,490 --> 00:15:28,750
bit more science stuff in the sea bass

00:15:26,800 --> 00:15:31,090
and the hands-on session is at

00:15:28,750 --> 00:15:32,410
researchgate check our website if you're

00:15:31,090 --> 00:15:34,360
interested it's free and you can just

00:15:32,410 --> 00:15:35,600
come along and grab a beer coffee with

00:15:34,360 --> 00:15:39,569
us

00:15:35,600 --> 00:15:41,160
so last slide as I said this is quite

00:15:39,569 --> 00:15:43,829
intense stuff so we always appreciate

00:15:41,160 --> 00:15:44,970
any kind of help so we can just use the

00:15:43,829 --> 00:15:46,439
tool box if you're interested in machine

00:15:44,970 --> 00:15:48,149
learning give us feedback you can fix

00:15:46,439 --> 00:15:50,430
bugs we got hundreds of bucks on github

00:15:48,149 --> 00:15:53,639
if you are like a super good C++

00:15:50,430 --> 00:15:55,319
software engineer you can help us with

00:15:53,639 --> 00:15:57,269
design problems that we have within the

00:15:55,319 --> 00:16:00,750
framework you can write Python examples

00:15:57,269 --> 00:16:02,370
and notebooks um this is this is

00:16:00,750 --> 00:16:04,050
actually quite cool there's a super fun

00:16:02,370 --> 00:16:06,240
writing these notebooks you can you know

00:16:04,050 --> 00:16:08,490
help help us with the documentation we

00:16:06,240 --> 00:16:10,110
need we have a website in jungle I don't

00:16:08,490 --> 00:16:12,269
know jungle I don't know how to use it

00:16:10,110 --> 00:16:13,709
we need people to help us if you have

00:16:12,269 --> 00:16:15,720
like the super next generation machine

00:16:13,709 --> 00:16:18,089
algorithm come and implement it just get

00:16:15,720 --> 00:16:21,980
back to us and yeah also come to

00:16:18,089 --> 00:16:21,980
workshop thank you

00:16:27,630 --> 00:16:39,390
yes please yeah so the question is

00:16:37,470 --> 00:16:41,280
what's the difference between sugar and

00:16:39,390 --> 00:16:46,920
other tool kits like scikit-learn are

00:16:41,280 --> 00:16:48,180
weaker orange yes so like taking psych

00:16:46,920 --> 00:16:50,340
alone which is the most similar one

00:16:48,180 --> 00:16:53,040
actually were quite similar project the

00:16:50,340 --> 00:16:55,440
thing is so if you want to use Sherwin

00:16:53,040 --> 00:16:57,720
you are not bound to python that's kind

00:16:55,440 --> 00:16:59,520
of a big big difference to the project

00:16:57,720 --> 00:17:01,290
and also since you have written in C++

00:16:59,520 --> 00:17:03,810
we can do things with a memory that the

00:17:01,290 --> 00:17:05,280
Python people have more trouble doing so

00:17:03,810 --> 00:17:07,589
we can now build like huge data

00:17:05,280 --> 00:17:09,000
structures of memory and we treat them

00:17:07,589 --> 00:17:10,589
really efficiently so we can have some

00:17:09,000 --> 00:17:12,689
really large scale examples you know

00:17:10,589 --> 00:17:15,990
with millions of examples that run on a

00:17:12,689 --> 00:17:17,699
single machine um but otherwise it's

00:17:15,990 --> 00:17:19,470
it's you know there's also quite a bit

00:17:17,699 --> 00:17:21,449
of overlap so we take a lot of

00:17:19,470 --> 00:17:22,770
inspiration from the cycle own website

00:17:21,449 --> 00:17:24,990
for example which i think is brilliant

00:17:22,770 --> 00:17:26,640
and the whole kind of way they document

00:17:24,990 --> 00:17:28,140
things and stuff like that so I know a

00:17:26,640 --> 00:17:29,760
few of the guys and I quite like the

00:17:28,140 --> 00:17:31,920
project also in there I think it's good

00:17:29,760 --> 00:17:42,510
to have like a you know bit of diversity

00:17:31,920 --> 00:17:44,730
in projects more questions yes so the

00:17:42,510 --> 00:17:46,830
question is whether we have used machine

00:17:44,730 --> 00:17:54,270
learning to improve children whoo that's

00:17:46,830 --> 00:17:56,310
a good one so so we see learning

00:17:54,270 --> 00:17:58,680
unfortunately can't write a you know

00:17:56,310 --> 00:18:01,170
memory bug free code for us but but it

00:17:58,680 --> 00:18:02,700
can do is we you know these these all

00:18:01,170 --> 00:18:06,090
uploads are quite nice so sometimes i do

00:18:02,700 --> 00:18:07,710
a bit of you know data mining on on our

00:18:06,090 --> 00:18:09,300
you know the number of classes and how

00:18:07,710 --> 00:18:10,800
they evolve and these kind of things but

00:18:09,300 --> 00:18:22,759
it's really more for visualization and

00:18:10,800 --> 00:18:24,199
for for marketing yes and

00:18:22,759 --> 00:18:27,529
years so the question is what I mean

00:18:24,199 --> 00:18:30,709
with large-scale um so I got a few

00:18:27,529 --> 00:18:33,589
escape this example here so these ones

00:18:30,709 --> 00:18:35,479
this one's here quite need so this was

00:18:33,589 --> 00:18:37,129
done on a laptop so there's an example

00:18:35,479 --> 00:18:39,769
from bioinformatics it's about

00:18:37,129 --> 00:18:42,139
supply-side recognition so splice site

00:18:39,769 --> 00:18:45,019
is something in your DNA where you you

00:18:42,139 --> 00:18:47,989
know where your genius your DNA is

00:18:45,019 --> 00:18:50,059
transcribed to RNA and then it's cut

00:18:47,989 --> 00:18:51,349
into pieces before it's translated

00:18:50,059 --> 00:18:53,869
protein you want to kind of predict

00:18:51,349 --> 00:18:56,029
where this happens and there's a data

00:18:53,869 --> 00:18:58,579
set here which we're talking about 15

00:18:56,029 --> 00:19:00,679
million examples and the dimension of

00:18:58,579 --> 00:19:04,369
the feature space the representation is

00:19:00,679 --> 00:19:06,979
200 million so that's quite big and this

00:19:04,369 --> 00:19:09,619
runs on a laptop in a couple of hours so

00:19:06,979 --> 00:19:11,690
and this works by the magic of you know

00:19:09,619 --> 00:19:13,369
defining data streams and kind of

00:19:11,690 --> 00:19:15,919
streaming files from the network and

00:19:13,369 --> 00:19:18,619
then putting them in the algorithms but

00:19:15,919 --> 00:19:20,899
sjögren is meant to run on a single

00:19:18,619 --> 00:19:32,509
computer so it's not a distributed

00:19:20,899 --> 00:19:34,450
toolbox okay more questions okay thanks

00:19:32,509 --> 00:19:36,510
guys

00:19:34,450 --> 00:19:36,510

YouTube URL: https://www.youtube.com/watch?v=I0Z4z_3ZwE0


