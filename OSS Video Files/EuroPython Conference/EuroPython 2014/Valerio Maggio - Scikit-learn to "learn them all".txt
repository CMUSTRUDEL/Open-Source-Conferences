Title: Valerio Maggio - Scikit-learn to "learn them all"
Publication date: 2015-04-18
Playlist: EuroPython 2014
Description: 
	Valerio Maggio - Scikit-learn to "learn them all"
[EuroPython 2014]
[24 July 2014]

Scikit-learn is a powerful library, providing implementations for 
many of the most popular machine learning algorithms. 
This talk will provide an overview of the "batteries" included in 
Scikit-learn, along with working code examples and internal insights, in order to 
get the best for our machine learning code.

-----

**Machine Learning** is about *using the right features, to build the right 
models, to achieve the right tasks* [[Flach, 2012]][0]
However, to come up with a definition of what actually means **right** for 
the problem at the hand, it is required to analyse 
huge amounts of data, and to evaluate the performance of different algorithms 
on these data.

However, deriving a working machine learning solution for a given problem 
is far from being a *waterfall* process. 
It is an iterative process where continuous refinements are required for the 
data to be used (i.e., the *right features*), and the algorithms to apply 
(i.e., the *right models*).

In this scenario, Python has been found very useful for practitioners and 
researchers: its high-level nature, in combination with available tools and 
libraries, allows to rapidly implement working machine learning code 
without *reinventing the wheel*.

[**Scikit-learn**](http://scikit-learn.org/stable/) is an actively 
developing Python library, built on top of the solid `numpy` and `scipy` 
packages.

Scikit-learn (`sklearn`) is an *all-in-one* software solution, providing 
implementations for several machine learning methods, along with datasets and 
(performance) evaluation algorithms.

These "batteries" included in the library, in combination with a nice and intuitive
software API, have made scikit-learn to become one of the most popular Python 
package to write machine learning code.

In this talk, a general overview of scikit-learn will be presented, along with 
brief explanations of the techniques provided out-of-the-box by the library.

These explanations will be supported by working code examples, and insights on 
algorithms' implementations aimed at providing hints on 
how to extend the library code.

Moreover, advantages and limitations of the `sklearn` package will be discussed 
according to other existing machine learning Python libraries
(e.g., [`shogun`](http://shogun-toolbox.org "Shogun Toolbox"), 
[`pyML`](http://pyml.sourceforge.net "PyML"), 
[`mlpy`](http://mlpy.sourceforge.net "MLPy")).

In conclusion, (examples of) applications of scikit-learn to big data and 
computational intensive tasks will be also presented.

The general outline of the talk is reported as follows (the order of the topics may vary):

*   Intro to Machine Learning
    *   Machine Learning in Python
    *   Intro to Scikit-Learn
*   Overview of Scikit-Learn
    *   Comparison with other existing ML Python libraries
*   Supervised Learning with `sklearn`
    *   Text Classification with SVM and Kernel Methods
*   Unsupervised Learning with `sklearn`
    *   Partitional and Model-based Clustering (i.e., k-means and Mixture Models)
*   Scaling up Machine Learning
    *   Parallel and Large Scale ML with `sklearn`

The talk is intended for an intermediate level audience (i.e., Advanced).
It requires basic math skills and a good knowledge of the Python language.

Good knowledge of the `numpy` and `scipy` packages is also a plus.

[0]: http://goo.gl/BnhoHa "Machine Learning: The Art and Science of Algorithms that Make Sense of Data, *Peter Flach, 2012*"
Captions: 
	00:00:14,870 --> 00:00:20,750
so the talk today is about so I could

00:00:17,990 --> 00:00:23,509
learn or in other words why i think i

00:00:20,750 --> 00:00:26,110
could learn is so cool first of all i

00:00:23,509 --> 00:00:28,880
would like to ask you three questions

00:00:26,110 --> 00:00:32,630
not what's your favorite color actually

00:00:28,880 --> 00:00:36,200
but if you already know what Marshall

00:00:32,630 --> 00:00:40,370
learning is how many of you oh great

00:00:36,200 --> 00:00:45,350
okay perfect the second one is have you

00:00:40,370 --> 00:00:47,420
ever used I could learn okay and the

00:00:45,350 --> 00:00:50,179
third one is how many of you also tend

00:00:47,420 --> 00:00:55,280
this the great training on psychic

00:00:50,179 --> 00:00:59,359
yesterday okay okay so just two brief

00:00:55,280 --> 00:01:03,469
questions okay so what actually much

00:00:59,359 --> 00:01:05,480
learning means Marshall learning there

00:01:03,469 --> 00:01:08,090
are many definitions about machine

00:01:05,480 --> 00:01:09,830
learning one of this is Marshall

00:01:08,090 --> 00:01:12,409
learning teaches machines how to carry

00:01:09,830 --> 00:01:14,240
out tasks by themselves okay it's very

00:01:12,409 --> 00:01:16,159
trivial their simple definition and

00:01:14,240 --> 00:01:18,590
there's that simple the complex they

00:01:16,159 --> 00:01:21,740
come with the details okay this is a

00:01:18,590 --> 00:01:25,400
very general definition but just to give

00:01:21,740 --> 00:01:27,590
you the intuition behind much learning

00:01:25,400 --> 00:01:30,710
at a glance much alone I guess about

00:01:27,590 --> 00:01:32,750
algorithms that are able to analyze to

00:01:30,710 --> 00:01:35,470
crunch the data and in particular to

00:01:32,750 --> 00:01:40,130
learn the data okay from the data

00:01:35,470 --> 00:01:43,130
they're basically exploits statistical

00:01:40,130 --> 00:01:46,570
approaches so that's why such as to also

00:01:43,130 --> 00:01:49,040
is very huge word in this cloud okay

00:01:46,570 --> 00:01:52,100
Marshall learning is almost related to

00:01:49,040 --> 00:01:55,190
data analysis techniques there are many

00:01:52,100 --> 00:01:57,620
buzzwords about Marshall learning you

00:01:55,190 --> 00:02:02,000
may have read about data analysis data

00:01:57,620 --> 00:02:05,180
mining data big data and data science

00:02:02,000 --> 00:02:07,460
okay data science actually is the story

00:02:05,180 --> 00:02:10,849
of the generalizable extraction of

00:02:07,460 --> 00:02:12,950
knowledge from data and marcia learning

00:02:10,849 --> 00:02:15,709
is related to data science according to

00:02:12,950 --> 00:02:18,050
do combine with this vernon diagram

00:02:15,709 --> 00:02:20,300
russia learning is in the middle okay

00:02:18,050 --> 00:02:22,760
and data science is a part of much

00:02:20,300 --> 00:02:24,769
learning because it exploits martial art

00:02:22,760 --> 00:02:28,760
machine learning is a fundamental part

00:02:24,769 --> 00:02:31,340
in the data science steps okay but what

00:02:28,760 --> 00:02:34,099
what is actually the relation of data

00:02:31,340 --> 00:02:37,760
mining and dating losses in general with

00:02:34,099 --> 00:02:41,420
much learning mantra learning is about

00:02:37,760 --> 00:02:45,079
to make predictions okay so instead of

00:02:41,420 --> 00:02:47,180
only analyze the data we have much

00:02:45,079 --> 00:02:50,959
learning is also able to generalize from

00:02:47,180 --> 00:02:54,049
this data okay so we have the idea is we

00:02:50,959 --> 00:02:57,079
have a bunch of data okay we may want to

00:02:54,049 --> 00:02:59,750
crunch these data to makes statistics

00:02:57,079 --> 00:03:02,209
analysis on this data but and that's it

00:02:59,750 --> 00:03:03,829
okay this is also called data mining for

00:03:02,209 --> 00:03:05,500
instance Marshall learning is a bit

00:03:03,829 --> 00:03:08,540
different because master learning

00:03:05,500 --> 00:03:11,109
performs this analysis but the the goal

00:03:08,540 --> 00:03:14,180
is a slightly different the goal is

00:03:11,109 --> 00:03:17,690
analyze this data and generalized try to

00:03:14,180 --> 00:03:21,409
find a to learn from this data a general

00:03:17,690 --> 00:03:25,970
model for future data for data that are

00:03:21,409 --> 00:03:29,120
already that are almost unseen at this

00:03:25,970 --> 00:03:32,090
time okay so the idea is the pattern

00:03:29,120 --> 00:03:35,629
exists in the data we cannot pin this

00:03:32,090 --> 00:03:38,299
pattern manually but we have data on it

00:03:35,629 --> 00:03:40,069
okay so we may learn from this data in

00:03:38,299 --> 00:03:42,709
other words this kind of learner is also

00:03:40,069 --> 00:03:44,690
known as learning by examples okay

00:03:42,709 --> 00:03:47,449
Marshall learning comes in two different

00:03:44,690 --> 00:03:49,819
settings there is the supervised

00:03:47,449 --> 00:03:53,419
settings this is the general pipeline of

00:03:49,819 --> 00:03:56,209
a marshal learning algorithm you have

00:03:53,419 --> 00:04:00,290
all the data on the upper left corner

00:03:56,209 --> 00:04:03,560
you translate you too late the data in a

00:04:00,290 --> 00:04:06,079
feature vectors there's almost a common

00:04:03,560 --> 00:04:08,239
step in / processing the data then you

00:04:06,079 --> 00:04:09,980
feed this feature vectors to your mash

00:04:08,239 --> 00:04:13,419
learning algorithm and the supervisor

00:04:09,980 --> 00:04:17,269
learning setting supports also the

00:04:13,419 --> 00:04:20,000
labels which is the set of expected

00:04:17,269 --> 00:04:23,780
results on this data and then we combine

00:04:20,000 --> 00:04:25,849
we generate this model from feature

00:04:23,780 --> 00:04:28,250
vectors and labels and we generalize

00:04:25,849 --> 00:04:31,970
that we get the model to predict for

00:04:28,250 --> 00:04:35,539
future data in the bottom left corner of

00:04:31,970 --> 00:04:36,860
the figure okay a classical example of

00:04:35,539 --> 00:04:38,870
supervisors learning is the

00:04:36,860 --> 00:04:42,050
classification you have two different

00:04:38,870 --> 00:04:42,470
groups of data in this case and you want

00:04:42,050 --> 00:04:45,980
to find

00:04:42,470 --> 00:04:49,370
and a general rule to separate these

00:04:45,980 --> 00:04:52,100
data this data okay so you you find in

00:04:49,370 --> 00:04:54,460
this case if a function that separates

00:04:52,100 --> 00:04:57,770
the data and for future data you will

00:04:54,460 --> 00:04:59,840
you will be able to to know which is the

00:04:57,770 --> 00:05:01,190
class in this case it's a binary

00:04:59,840 --> 00:05:05,000
classification so you have two classes

00:05:01,190 --> 00:05:08,690
and in the future when you got new data

00:05:05,000 --> 00:05:11,570
you will be able to predict which is the

00:05:08,690 --> 00:05:15,770
class associated to this data another

00:05:11,570 --> 00:05:17,600
example is the clustering in this case

00:05:15,770 --> 00:05:20,150
the setting is called unsupervised

00:05:17,600 --> 00:05:23,960
learning the pipeline processing is this

00:05:20,150 --> 00:05:26,360
one you have the same old processing but

00:05:23,960 --> 00:05:28,400
what you miss is the label part okay

00:05:26,360 --> 00:05:29,990
because that's why this is called

00:05:28,400 --> 00:05:31,850
unsupervised because you have no

00:05:29,990 --> 00:05:36,380
supervision on the data you have no

00:05:31,850 --> 00:05:38,690
label to predict okay and for as for the

00:05:36,380 --> 00:05:41,000
classroom the problem is get a bunch of

00:05:38,690 --> 00:05:43,550
data and try to cluster eyes in other

00:05:41,000 --> 00:05:45,740
words to separate the data into

00:05:43,550 --> 00:05:48,440
different groups okay so you have a

00:05:45,740 --> 00:05:52,540
bunch of data you want to identify the

00:05:48,440 --> 00:05:55,550
grips inside this data okay just a brief

00:05:52,540 --> 00:05:57,770
introduction so what about Python Python

00:05:55,550 --> 00:06:02,540
and data science are very related

00:05:57,770 --> 00:06:05,600
nowadays actually a python is getting

00:06:02,540 --> 00:06:07,910
more and more packages to for

00:06:05,600 --> 00:06:11,350
computational science according to this

00:06:07,910 --> 00:06:14,360
graph python is a cutting-edge

00:06:11,350 --> 00:06:16,900
technology for this kind of computation

00:06:14,360 --> 00:06:25,360
it's about all this it's in the upper

00:06:16,900 --> 00:06:29,390
right corner and actually it's replacing

00:06:25,360 --> 00:06:32,000
and substituting other technologies one

00:06:29,390 --> 00:06:34,729
of the advantages such as our or matlab

00:06:32,000 --> 00:06:38,560
person see one of the advantages of

00:06:34,729 --> 00:06:40,940
python is that Python provides a unique

00:06:38,560 --> 00:06:43,010
programming language across different

00:06:40,940 --> 00:06:45,410
application it has a very huge set of

00:06:43,010 --> 00:06:48,320
libraries to exploit and this is the

00:06:45,410 --> 00:06:50,630
case this is why the reason why python

00:06:48,320 --> 00:06:52,099
is the language of choice nowadays for

00:06:50,630 --> 00:06:55,110
data science almost language of choice

00:06:52,099 --> 00:07:00,270
and this is displacing our or math

00:06:55,110 --> 00:07:03,450
and by the way there will be also a PI

00:07:00,270 --> 00:07:06,660
data conference at the end of the week

00:07:03,450 --> 00:07:11,820
it will be started on friday so if you

00:07:06,660 --> 00:07:14,030
if you can please calm data science in

00:07:11,820 --> 00:07:17,370
Python actually matlab can be easily

00:07:14,030 --> 00:07:20,040
substituted by all these technologies

00:07:17,370 --> 00:07:22,830
such as a Python numpy sci-fi matplotlib

00:07:20,040 --> 00:07:25,590
for plotting but there are many other

00:07:22,830 --> 00:07:28,590
possibilities for especially for poor

00:07:25,590 --> 00:07:31,230
thing nowadays are it's could be easily

00:07:28,590 --> 00:07:34,710
substitute substituted with pandas it's

00:07:31,230 --> 00:07:39,900
right package and in the Python

00:07:34,710 --> 00:07:43,200
ecosystem we have also efficient Python

00:07:39,900 --> 00:07:45,090
interpreters that have been compiled for

00:07:43,200 --> 00:07:47,970
this kind of computation such as

00:07:45,090 --> 00:07:50,730
anaconda or n thought kanpai and we are

00:07:47,970 --> 00:07:53,850
also cited or projects like seitan

00:07:50,730 --> 00:07:56,250
seitan is a very great project to allow

00:07:53,850 --> 00:08:00,630
to boost the computation of your Python

00:07:56,250 --> 00:08:03,740
code okay the packages for Marshall

00:08:00,630 --> 00:08:08,010
learning python are manifold actually

00:08:03,740 --> 00:08:12,240
I'm trying to you to do to describe a

00:08:08,010 --> 00:08:16,350
bit all day a set of well-known packages

00:08:12,240 --> 00:08:18,750
for for Marshall learning code and I

00:08:16,350 --> 00:08:20,700
would like to do to make some

00:08:18,750 --> 00:08:24,690
consideration and why scikit-learn is a

00:08:20,700 --> 00:08:26,790
very great one ok we have Sparkle

00:08:24,690 --> 00:08:28,860
Marshall learning leap IML natural

00:08:26,790 --> 00:08:31,770
language toolkit and ltk sometimes

00:08:28,860 --> 00:08:33,660
called the sugar Marshall learning tool

00:08:31,770 --> 00:08:36,810
box this morning there's been talk about

00:08:33,660 --> 00:08:42,060
it cycle during of course pi brain and

00:08:36,810 --> 00:08:47,610
multi okay and there is a guy who set up

00:08:42,060 --> 00:08:51,780
a list at this on github where everybody

00:08:47,610 --> 00:08:54,750
can put is her a contribution to this

00:08:51,780 --> 00:08:57,120
list in order to distribute the the

00:08:54,750 --> 00:08:59,010
knowledge about available packages in

00:08:57,120 --> 00:09:04,470
different languages and python is very

00:08:59,010 --> 00:09:07,710
full of okay so we have spark Emily

00:09:04,470 --> 00:09:09,470
spark and live actually is implemented

00:09:07,710 --> 00:09:12,810
in Scala it's not

00:09:09,470 --> 00:09:16,200
python it's there is a rock being in

00:09:12,810 --> 00:09:17,790
Python which is called pi PI spark but

00:09:16,200 --> 00:09:20,180
actually the library for Marshall

00:09:17,790 --> 00:09:24,840
learning is a temporary early stage

00:09:20,180 --> 00:09:29,010
sugar is written in C++ and it offers a

00:09:24,840 --> 00:09:32,970
lot of interfaces one of this interface

00:09:29,010 --> 00:09:35,700
is in Python the other packages there

00:09:32,970 --> 00:09:39,480
are Python powered so we're trying to

00:09:35,700 --> 00:09:41,520
take to talk about this packages natural

00:09:39,480 --> 00:09:44,520
language faltan toolkit s4 it's

00:09:41,520 --> 00:09:47,550
implemented in pure python okay so no

00:09:44,520 --> 00:09:49,500
noon pie or sapphire loud but and the

00:09:47,550 --> 00:09:52,890
other packages are implemented in New

00:09:49,500 --> 00:09:54,420
pious up by so the code there is quite

00:09:52,890 --> 00:09:58,470
bored efficient for large-scale

00:09:54,420 --> 00:10:02,010
computations NLT k supports Python 2 and

00:09:58,470 --> 00:10:05,040
Python 3 or is also in a half a stage PI

00:10:02,010 --> 00:10:08,400
ml suppose Python to actually put Python

00:10:05,040 --> 00:10:11,310
tree is not so clear pipe rain supports

00:10:08,400 --> 00:10:14,040
only Python 2 and these are the two guys

00:10:11,310 --> 00:10:19,140
there supports both Python true and

00:10:14,040 --> 00:10:21,720
Python 3 okay what about the purpose of

00:10:19,140 --> 00:10:24,540
this packages NLT k is for a natural

00:10:21,720 --> 00:10:27,240
language processing ok and that's some

00:10:24,540 --> 00:10:30,510
algorithms for mash learning but

00:10:27,240 --> 00:10:33,780
actually it is not supposed to be used

00:10:30,510 --> 00:10:38,090
in complete much learning environment

00:10:33,780 --> 00:10:41,010
it's almost related to text analysis

00:10:38,090 --> 00:10:45,000
natural language processing channel PI

00:10:41,010 --> 00:10:48,720
ml is almost focuses on supervisor

00:10:45,000 --> 00:10:51,030
learning in particular to SVM technique

00:10:48,720 --> 00:10:54,000
which is support vector machine ok it

00:10:51,030 --> 00:10:56,850
doesn't many algorithm especially

00:10:54,000 --> 00:11:00,270
related to use some supervisors learning

00:10:56,850 --> 00:11:03,120
pi brain is for an inch neural network

00:11:00,270 --> 00:11:06,750
with which is another set of techniques

00:11:03,120 --> 00:11:09,420
in the martial learning ecosystem the

00:11:06,750 --> 00:11:12,480
other two guys there are somewhat

00:11:09,420 --> 00:11:15,330
general purpose ok so psychic FML

00:11:12,480 --> 00:11:17,640
Marshall learning PI R contains Olga

00:11:15,330 --> 00:11:20,040
rhythms for supervised and unsupervised

00:11:17,640 --> 00:11:21,780
learning and some others different

00:11:20,040 --> 00:11:22,350
slightly different settings for much

00:11:21,780 --> 00:11:27,570
learning

00:11:22,350 --> 00:11:31,290
okay so we're we remove we will not

00:11:27,570 --> 00:11:36,270
consider any more the PI ml and PI brain

00:11:31,290 --> 00:11:39,600
okay from here on so we ended up with

00:11:36,270 --> 00:11:42,510
these three libraries reading it in

00:11:39,600 --> 00:11:48,720
Python for our mush learning code so why

00:11:42,510 --> 00:11:51,900
to choose cycle learn Ben lorica it's he

00:11:48,720 --> 00:11:55,190
is a big data guy recommends I could

00:11:51,900 --> 00:11:58,020
learn for six reasons the first one is

00:11:55,190 --> 00:12:00,480
commitment to the documentation and

00:11:58,020 --> 00:12:04,380
usability second learn as a brilliant

00:12:00,480 --> 00:12:07,800
documentation and it's very very useful

00:12:04,380 --> 00:12:10,860
for newcomers and for people without any

00:12:07,800 --> 00:12:13,020
background about Marsha learning the

00:12:10,860 --> 00:12:14,790
second reason is moles all chosen and

00:12:13,020 --> 00:12:18,410
implemented by a dedicated team of

00:12:14,790 --> 00:12:21,090
expert and then the the set of models

00:12:18,410 --> 00:12:25,830
supported by the library covers most

00:12:21,090 --> 00:12:30,450
Marshall learning tasks okay python and

00:12:25,830 --> 00:12:34,200
PI data improves the support for data

00:12:30,450 --> 00:12:37,740
science data science tools the designs

00:12:34,200 --> 00:12:41,480
problems and actually I know if you know

00:12:37,740 --> 00:12:44,910
Kegel Kegel is a site where you may

00:12:41,480 --> 00:12:47,670
apply for competition for data science

00:12:44,910 --> 00:12:51,240
and psych it is one of the most used

00:12:47,670 --> 00:12:53,700
package for this kind of competition the

00:12:51,240 --> 00:12:55,200
fact the the another reason should be

00:12:53,700 --> 00:12:57,030
the focus second learn is a machine

00:12:55,200 --> 00:12:59,580
learning library and its goal is to

00:12:57,030 --> 00:13:01,500
provide a set of common algorithm to

00:12:59,580 --> 00:13:04,860
python users through a consistent

00:13:01,500 --> 00:13:07,020
interface these two features are two of

00:13:04,860 --> 00:13:10,410
the features that I like the most ok I

00:13:07,020 --> 00:13:15,450
will be more precise in few slides about

00:13:10,410 --> 00:13:17,190
this and finally but by no means so last

00:13:15,450 --> 00:13:20,070
button I no means least I can learn

00:13:17,190 --> 00:13:22,290
scales the most data problems okay so

00:13:20,070 --> 00:13:26,640
scalability is another feature that

00:13:22,290 --> 00:13:29,130
scikit-learn supports out of the box if

00:13:26,640 --> 00:13:32,820
you want to install second learn you

00:13:29,130 --> 00:13:35,699
have to pip very few comments you need

00:13:32,820 --> 00:13:37,859
to install new PI side pie

00:13:35,699 --> 00:13:40,049
matplotlib ipython actually is not

00:13:37,859 --> 00:13:42,359
needed it's just for convenience and

00:13:40,049 --> 00:13:44,730
then you install scikit-learn all the

00:13:42,359 --> 00:13:47,459
other packages moon pie inside pie in

00:13:44,730 --> 00:13:50,459
particular are required because cycle

00:13:47,459 --> 00:13:53,839
earth is based on noon pine supply okay

00:13:50,459 --> 00:13:56,429
but anyway if you want to install other

00:13:53,839 --> 00:13:59,459
version of the Python interpreter such

00:13:56,429 --> 00:14:03,359
as anaconda it's already provided out of

00:13:59,459 --> 00:14:08,759
the box the design philosophy of psychic

00:14:03,359 --> 00:14:11,939
it's one of the greatest feature of this

00:14:08,759 --> 00:14:14,040
package I guess in my opinion it

00:14:11,939 --> 00:14:16,350
includes all the batter is necessary for

00:14:14,040 --> 00:14:20,699
general purpose Marshall learning code

00:14:16,350 --> 00:14:23,069
it has as its it supports features for

00:14:20,699 --> 00:14:25,949
an functionalities for data and data

00:14:23,069 --> 00:14:28,259
sets feature selection extraction

00:14:25,949 --> 00:14:29,970
feature extraction algorithms Marshall

00:14:28,259 --> 00:14:32,309
learning algorithms in general in

00:14:29,970 --> 00:14:34,079
different settings so classification

00:14:32,309 --> 00:14:37,079
regression clustering and stuff like

00:14:34,079 --> 00:14:39,449
that and finally evaluation functions

00:14:37,079 --> 00:14:41,249
for cross-validation confusion matrix we

00:14:39,449 --> 00:14:43,949
will see some examples in the next

00:14:41,249 --> 00:14:47,039
slides the algorithm selection

00:14:43,949 --> 00:14:49,379
philosophy for this package is try to

00:14:47,039 --> 00:14:52,049
keep the core as light as possible and

00:14:49,379 --> 00:14:53,789
try to include only the well-known and

00:14:52,049 --> 00:14:57,149
largely used Marshall learning

00:14:53,789 --> 00:14:59,850
algorithms okay so the focus here is to

00:14:57,149 --> 00:15:02,549
be as much general purpose as possible

00:14:59,850 --> 00:15:06,959
okay so in order to include a broad

00:15:02,549 --> 00:15:11,160
audience of users at a glance this is a

00:15:06,959 --> 00:15:14,730
great sorry this is a great picture the

00:15:11,160 --> 00:15:17,609
big thing all the features are provided

00:15:14,730 --> 00:15:20,369
by scikit-learn and this figure here is

00:15:17,609 --> 00:15:23,600
has been gathered by the documentation

00:15:20,369 --> 00:15:26,819
this is a sort of map you may follow to

00:15:23,600 --> 00:15:28,049
that allows you to choose that the

00:15:26,819 --> 00:15:30,179
particular and Marshall learning

00:15:28,049 --> 00:15:32,429
techniques you want you to use in your

00:15:30,179 --> 00:15:34,859
martial learning code there are some

00:15:32,429 --> 00:15:37,850
clusters in this picture there is

00:15:34,859 --> 00:15:39,899
regression over there classification

00:15:37,850 --> 00:15:43,980
clustering and dimensionality reduction

00:15:39,899 --> 00:15:47,339
and you may follow this kind of a path

00:15:43,980 --> 00:15:49,320
over there too to decide which kind of

00:15:47,339 --> 00:15:53,940
which is the setting must suit

00:15:49,320 --> 00:15:58,920
for your problem okay the API of psyche

00:15:53,940 --> 00:16:01,980
is very intuitive and mostly consistent

00:15:58,920 --> 00:16:05,220
to every much learning technique there

00:16:01,980 --> 00:16:07,860
are four different objects there is the

00:16:05,220 --> 00:16:14,060
estimator the predictor transformer and

00:16:07,860 --> 00:16:18,240
model okay the these interfaces are

00:16:14,060 --> 00:16:19,860
implemented by most almost all the mushy

00:16:18,240 --> 00:16:21,900
learning algorithms included in the

00:16:19,860 --> 00:16:25,620
library for instance let's make an

00:16:21,900 --> 00:16:29,430
example the API for the estimator is the

00:16:25,620 --> 00:16:31,560
method fit okay the an estimator is an

00:16:29,430 --> 00:16:35,150
object that fits the model based on some

00:16:31,560 --> 00:16:38,000
training data and it's capable of

00:16:35,150 --> 00:16:41,370
inferring some properties on new data

00:16:38,000 --> 00:16:43,770
for example if we want you to create an

00:16:41,370 --> 00:16:46,680
algorithm which is called KNN or k

00:16:43,770 --> 00:16:48,660
neighbors classifiers we the KNN

00:16:46,680 --> 00:16:51,240
algorithm which is a classifier so it's

00:16:48,660 --> 00:16:54,690
it's for classification problems and

00:16:51,240 --> 00:16:59,370
then supervised learning it has the feed

00:16:54,690 --> 00:17:01,650
method but for all also sorry for also

00:16:59,370 --> 00:17:04,230
unsupervised learning algorithm such as

00:17:01,650 --> 00:17:07,530
k-means the k-means algorithm is an

00:17:04,230 --> 00:17:10,589
estimator as well and it implements the

00:17:07,530 --> 00:17:14,550
feet method to for feature selection is

00:17:10,589 --> 00:17:17,040
almost the same okay then the predictor

00:17:14,550 --> 00:17:20,750
the predictor provides the predict and

00:17:17,040 --> 00:17:24,589
the predicted probability method and

00:17:20,750 --> 00:17:28,560
finally the transformer is the transform

00:17:24,589 --> 00:17:30,390
is about the transform method that and

00:17:28,560 --> 00:17:32,400
sometimes there is also the feed

00:17:30,390 --> 00:17:34,200
transform method that applies the fit

00:17:32,400 --> 00:17:36,560
and then the transformation of the data

00:17:34,200 --> 00:17:38,460
the transformations use to to make

00:17:36,560 --> 00:17:43,170
transformation of the data in order to

00:17:38,460 --> 00:17:46,920
to to to make the data abled and in a

00:17:43,170 --> 00:17:50,460
uniform that is able to be processed by

00:17:46,920 --> 00:17:54,930
the algorithms finally the last one is

00:17:50,460 --> 00:17:57,270
the model the model as the general model

00:17:54,930 --> 00:17:59,550
you may create in your martial learning

00:17:57,270 --> 00:18:02,950
algorithm the model is for supervised

00:17:59,550 --> 00:18:07,420
then for unsupervised algorithms

00:18:02,950 --> 00:18:12,400
and another great feature of modular of

00:18:07,420 --> 00:18:16,230
Psych it as the pipeline's because psych

00:18:12,400 --> 00:18:20,110
it provides a great way to create

00:18:16,230 --> 00:18:22,060
pipeline processing so in this case you

00:18:20,110 --> 00:18:24,970
may create a pipeline of different

00:18:22,060 --> 00:18:28,900
processing steps okay just out of the

00:18:24,970 --> 00:18:31,360
box you may apply these select k bass

00:18:28,900 --> 00:18:33,190
which is feature selection step then

00:18:31,360 --> 00:18:36,250
after the feature selection you may

00:18:33,190 --> 00:18:38,770
apply your PC a PCAs feature is a an

00:18:36,250 --> 00:18:41,770
algorithm for dimensionality reduction

00:18:38,770 --> 00:18:43,900
and then you may apply logistic

00:18:41,770 --> 00:18:47,590
regression which is a classified

00:18:43,900 --> 00:18:51,520
classifier okay so you may associate the

00:18:47,590 --> 00:18:54,880
pipeline processing very very easily

00:18:51,520 --> 00:18:57,280
okay see and then you call the fit

00:18:54,880 --> 00:19:00,730
method on the pie plate and the feed

00:18:57,280 --> 00:19:02,800
method will and then the predict the

00:19:00,730 --> 00:19:06,160
only constraint here is that the last

00:19:02,800 --> 00:19:09,220
step of the pipeline should be a class

00:19:06,160 --> 00:19:13,570
that implements the predict method sold

00:19:09,220 --> 00:19:16,720
a predictor okay so far so good okay

00:19:13,570 --> 00:19:19,920
great so let's see some examples I

00:19:16,720 --> 00:19:25,000
killed in action we have it's very

00:19:19,920 --> 00:19:27,520
introductory example the first thing to

00:19:25,000 --> 00:19:30,730
consider is the data representation

00:19:27,520 --> 00:19:33,130
actually psych it is based on new pie at

00:19:30,730 --> 00:19:35,350
pie as you know so all the data are

00:19:33,130 --> 00:19:38,530
usually represented as matrices and

00:19:35,350 --> 00:19:41,920
vectors in general in much learning by

00:19:38,530 --> 00:19:44,650
definition we have the X matrix over

00:19:41,920 --> 00:19:47,530
there which is usually identified by the

00:19:44,650 --> 00:19:52,480
capital letter because it is a matrix as

00:19:47,530 --> 00:19:55,000
a matrix of n different rows and D

00:19:52,480 --> 00:19:57,400
different colors in this case I'm sorry

00:19:55,000 --> 00:19:59,650
in this case n is the number of samples

00:19:57,400 --> 00:20:03,090
we have in our data set and D is the

00:19:59,650 --> 00:20:06,160
number of features so the number of

00:20:03,090 --> 00:20:09,490
relevant information on the data we have

00:20:06,160 --> 00:20:13,420
okay so the data comes the training data

00:20:09,490 --> 00:20:15,700
com in this flavor and it under the hood

00:20:13,420 --> 00:20:16,400
it is implemented by syfy dot sparse

00:20:15,700 --> 00:20:20,900
matrices

00:20:16,400 --> 00:20:24,110
okay usually it is if i'm not mistaken

00:20:20,900 --> 00:20:27,680
should be csr implementation su cama

00:20:24,110 --> 00:20:31,600
sparse row a compressive sparse row okay

00:20:27,680 --> 00:20:36,400
and finally we have the labels because

00:20:31,600 --> 00:20:40,040
we know that the values for each of this

00:20:36,400 --> 00:20:41,630
data about the problem we have the

00:20:40,040 --> 00:20:44,000
problem we are going to consider is

00:20:41,630 --> 00:20:46,340
about the iris data set and we want to

00:20:44,000 --> 00:20:49,490
design an algorithm that is able to

00:20:46,340 --> 00:20:52,040
automatically recognize iris species

00:20:49,490 --> 00:20:55,340
okay so we have three different species

00:20:52,040 --> 00:20:59,240
of iris we have iris versicolor in on

00:20:55,340 --> 00:21:03,710
left iris blue jenika here and I wish

00:20:59,240 --> 00:21:06,830
the tosa year okay the features we're

00:21:03,710 --> 00:21:08,930
going to consider are for and are the

00:21:06,830 --> 00:21:10,610
length of the sample and with of

00:21:08,930 --> 00:21:13,310
disabled the language of the petal and

00:21:10,610 --> 00:21:16,670
the weed of the petal okay so every data

00:21:13,310 --> 00:21:19,280
and this data set comes as a vector and

00:21:16,670 --> 00:21:23,020
every sample sorry comes as a vector for

00:21:19,280 --> 00:21:28,040
different features okay this for here

00:21:23,020 --> 00:21:30,650
sike it a red has a great package to

00:21:28,040 --> 00:21:33,620
handle the data sets actually these

00:21:30,650 --> 00:21:37,610
particular desert is very well known in

00:21:33,620 --> 00:21:41,000
many fields and is already embedded in

00:21:37,610 --> 00:21:44,330
the psyche attorn library so you only

00:21:41,000 --> 00:21:47,750
need to import the data set package and

00:21:44,330 --> 00:21:50,240
called load iris and then you you call

00:21:47,750 --> 00:21:52,250
the function load iris and the iris

00:21:50,240 --> 00:21:55,250
object is a bunch object that contains

00:21:52,250 --> 00:21:57,770
different keys it has the toggle names

00:21:55,250 --> 00:22:00,220
the data the target a description of the

00:21:57,770 --> 00:22:03,230
data set and the future names okay

00:22:00,220 --> 00:22:05,450
description is the descriptive words

00:22:03,230 --> 00:22:07,610
description of data feature names are

00:22:05,450 --> 00:22:09,890
the four different features i already

00:22:07,610 --> 00:22:13,010
mentioned in the previous slides the

00:22:09,890 --> 00:22:15,020
target names are the targets we expected

00:22:13,010 --> 00:22:17,150
on this data set in particular certosa

00:22:15,020 --> 00:22:20,690
versicolor virginie the three different

00:22:17,150 --> 00:22:24,710
iris specious we want to predict then we

00:22:20,690 --> 00:22:27,560
have the data so we always thought data

00:22:24,710 --> 00:22:30,250
comes as a new pipe matrix dump int

00:22:27,560 --> 00:22:37,310
array the shape of this matrix is

00:22:30,250 --> 00:22:41,380
150 hundred a 150 rose x 44 which is

00:22:37,310 --> 00:22:45,260
four different colors columns and the

00:22:41,380 --> 00:22:47,510
targets are 150 because we have a value

00:22:45,260 --> 00:22:50,480
for the target value of target for each

00:22:47,510 --> 00:22:54,260
sample in the data set so n the number

00:22:50,480 --> 00:22:57,130
of samples in this case is 150 d the

00:22:54,260 --> 00:23:02,900
number of feature in this case is 4 and

00:22:57,130 --> 00:23:05,240
that's it the targets here is the result

00:23:02,900 --> 00:23:08,450
of the target okay so we have a value

00:23:05,240 --> 00:23:10,340
that ranges from 0 to 2 corresponding to

00:23:08,450 --> 00:23:13,670
the three different classes we want to

00:23:10,340 --> 00:23:16,790
predict we might try to apply a

00:23:13,670 --> 00:23:19,430
classification problem on this data we

00:23:16,790 --> 00:23:22,190
want to exploit the KNN algorithm the

00:23:19,430 --> 00:23:26,540
idea of the KNN classifiers is pretty

00:23:22,190 --> 00:23:29,960
simple in for example if we consider a k

00:23:26,540 --> 00:23:35,030
which is equal to 6 we're going to check

00:23:29,960 --> 00:23:38,840
the the classes as this is the new data

00:23:35,030 --> 00:23:41,120
we train our bottle with the training

00:23:38,840 --> 00:23:45,290
data and we want to predict the class of

00:23:41,120 --> 00:23:48,200
this new data on the the classes of the

00:23:45,290 --> 00:23:51,500
sexy dearest neighbors of this data okay

00:23:48,200 --> 00:23:56,120
in this case should be the virginica

00:23:51,500 --> 00:23:59,630
okay the dot the red dot okay very

00:23:56,120 --> 00:24:02,150
simple in circuit few lines of code we

00:23:59,630 --> 00:24:04,820
import the data set we call the K

00:24:02,150 --> 00:24:07,430
neighbor classifier algorithm in this

00:24:04,820 --> 00:24:10,010
case we select n neighbors equals to 1

00:24:07,430 --> 00:24:14,090
then we call the fit method and we train

00:24:10,010 --> 00:24:16,130
our model then if this is what we get

00:24:14,090 --> 00:24:18,200
actually if you want to plot the data

00:24:16,130 --> 00:24:21,440
these these are called the decision

00:24:18,200 --> 00:24:23,630
boundaries of the classifier and if you

00:24:21,440 --> 00:24:26,690
want to know for new data which is the

00:24:23,630 --> 00:24:30,710
kind which is a species of virus that

00:24:26,690 --> 00:24:34,460
has 3 centimeter x 5 centimeters sample

00:24:30,710 --> 00:24:38,840
and 4 times 2 centimeters petal width

00:24:34,460 --> 00:24:42,200
okay right let's check is dot target

00:24:38,840 --> 00:24:43,730
names of KN n dot predict because k NN

00:24:42,200 --> 00:24:47,140
is a classifier so

00:24:43,730 --> 00:24:50,300
it may fit the data and also predict

00:24:47,140 --> 00:24:54,200
after the training and it sells okay

00:24:50,300 --> 00:25:00,790
it's super jenika okay so that's good

00:24:54,200 --> 00:25:03,290
right then we may also try to instead of

00:25:00,790 --> 00:25:05,420
facing this problem as a classification

00:25:03,290 --> 00:25:08,000
you may also face this problem as a

00:25:05,420 --> 00:25:10,610
notary in an unsupervised setting so as

00:25:08,000 --> 00:25:12,500
a clustering problem in this case we are

00:25:10,610 --> 00:25:14,630
going to use the k-means algorithm the

00:25:12,500 --> 00:25:18,400
k-means algorithm is the idea is pretty

00:25:14,630 --> 00:25:21,530
simple the we want to recreate an a

00:25:18,400 --> 00:25:23,660
cluster of object and eat each object is

00:25:21,530 --> 00:25:28,240
equal distance to the center of this of

00:25:23,660 --> 00:25:31,820
this cluster okay and that's it and

00:25:28,240 --> 00:25:34,940
psych it it's very simple we have the

00:25:31,820 --> 00:25:37,220
k-means we specify the number of

00:25:34,940 --> 00:25:38,810
clusters we want to have in the k-means

00:25:37,220 --> 00:25:41,060
in this case we want three clusters

00:25:38,810 --> 00:25:45,320
because we're going to predict three

00:25:41,060 --> 00:25:47,210
different species for the iris and then

00:25:45,320 --> 00:25:49,310
this is the ground through so this is

00:25:47,210 --> 00:25:53,570
the value we expected this is what we

00:25:49,310 --> 00:25:57,260
got after calling the k-means as you may

00:25:53,570 --> 00:25:59,480
already notice the interface for the two

00:25:57,260 --> 00:26:01,370
algorithm is exactly the same even if

00:25:59,480 --> 00:26:02,900
the marshal learning settings are

00:26:01,370 --> 00:26:05,330
completely different in the former case

00:26:02,900 --> 00:26:07,850
it was supervised in this latter case is

00:26:05,330 --> 00:26:12,230
unsupervised okay so classification

00:26:07,850 --> 00:26:15,410
versus clustering finally we refuse

00:26:12,230 --> 00:26:19,370
lights to conclude another great battery

00:26:15,410 --> 00:26:22,120
included in Psych head and I'm I don't

00:26:19,370 --> 00:26:26,270
know how many other measure learning

00:26:22,120 --> 00:26:28,820
libraries in Python are so complete in

00:26:26,270 --> 00:26:31,790
terms of batteries is about the model

00:26:28,820 --> 00:26:34,220
evaluation algorithm Molly evaluation is

00:26:31,790 --> 00:26:37,010
necessary to know how do we know if our

00:26:34,220 --> 00:26:40,940
predictor or our prediction model is

00:26:37,010 --> 00:26:45,110
good so we apply model validation

00:26:40,940 --> 00:26:47,990
techniques we might simply try to verify

00:26:45,110 --> 00:26:50,750
that every prediction correspond to the

00:26:47,990 --> 00:26:53,660
actual to the actual target okay but

00:26:50,750 --> 00:26:57,750
this is meaningless because we're trying

00:26:53,660 --> 00:27:00,300
to verify if we train all the data

00:26:57,750 --> 00:27:03,380
on the training okay so this is this

00:27:00,300 --> 00:27:06,540
kind of evaluation is very poor because

00:27:03,380 --> 00:27:10,560
because it's based only on the train so

00:27:06,540 --> 00:27:14,370
we we're just checking if we are able to

00:27:10,560 --> 00:27:16,740
feed the data but we are not able to to

00:27:14,370 --> 00:27:19,380
test if the modal the final model is

00:27:16,740 --> 00:27:21,360
able to generalize okay because a key

00:27:19,380 --> 00:27:26,520
feature of this kind of technique is the

00:27:21,360 --> 00:27:29,250
generalization so no goat too much to

00:27:26,520 --> 00:27:30,990
the training data because it's if you

00:27:29,250 --> 00:27:34,830
will end up in a problem which is called

00:27:30,990 --> 00:27:39,390
overfitting but you need to generalize

00:27:34,830 --> 00:27:42,780
to be able to noise and to be able to

00:27:39,390 --> 00:27:45,630
predict even new data that are not

00:27:42,780 --> 00:27:49,890
actually identical to the training data

00:27:45,630 --> 00:27:51,210
okay one usually technique user

00:27:49,890 --> 00:27:54,240
technique in Marshall learning is the

00:27:51,210 --> 00:27:56,760
so-called confusion matrix okay mmm

00:27:54,240 --> 00:27:59,340
psych it provides or in the matrix

00:27:56,760 --> 00:28:02,010
package provides different kind of

00:27:59,340 --> 00:28:03,600
metrics to evaluate your performance in

00:28:02,010 --> 00:28:05,640
this case we're going to use the

00:28:03,600 --> 00:28:08,850
confusion matrix the confusion matrix is

00:28:05,640 --> 00:28:11,280
very simple is a matrix where it's the

00:28:08,850 --> 00:28:13,080
number 0 it has is square matrix where

00:28:11,280 --> 00:28:14,940
the rows and the columns correspond to

00:28:13,080 --> 00:28:17,610
the number of classes you want to

00:28:14,940 --> 00:28:20,700
predict okay and then the diagonal you

00:28:17,610 --> 00:28:22,200
have all the classes that you expect

00:28:20,700 --> 00:28:24,060
with respect to the classes that you

00:28:22,200 --> 00:28:26,220
predict okay so you have all the

00:28:24,060 --> 00:28:31,050
possible matchings if you have all the

00:28:26,220 --> 00:28:33,540
data there on the on the diagonal it

00:28:31,050 --> 00:28:37,830
sells that you predicted perfectly all

00:28:33,540 --> 00:28:42,510
the classes okay is that clear okay

00:28:37,830 --> 00:28:45,600
right thank you but I grew am very well

00:28:42,510 --> 00:28:47,940
known for you guys that already aware of

00:28:45,600 --> 00:28:50,130
martial learning is the cross-validation

00:28:47,940 --> 00:28:52,260
technique cross validation is motor

00:28:50,130 --> 00:28:54,570
validation techniques for assessing how

00:28:52,260 --> 00:28:56,870
the results of the statistical knows of

00:28:54,570 --> 00:29:00,030
the data is able to generalize to

00:28:56,870 --> 00:29:03,000
independent data set not only to the

00:29:00,030 --> 00:29:05,460
data set we used for training okay and

00:29:03,000 --> 00:29:07,920
psych it already provide all the

00:29:05,460 --> 00:29:10,490
features to handle this kind of stuff so

00:29:07,920 --> 00:29:10,490
suck it

00:29:11,900 --> 00:29:17,490
imposes us to write very few code just

00:29:15,480 --> 00:29:20,610
the few lines of code necessary to

00:29:17,490 --> 00:29:25,140
import the functions already provided in

00:29:20,610 --> 00:29:27,870
the library in other cases we were knee

00:29:25,140 --> 00:29:30,780
we were required to implement this kind

00:29:27,870 --> 00:29:34,440
of function over and over for every time

00:29:30,780 --> 00:29:39,480
in in our Python code okay so this is

00:29:34,440 --> 00:29:43,380
very very useful even for lazy

00:29:39,480 --> 00:29:45,780
programmers like me okay in this guy's

00:29:43,380 --> 00:29:47,850
we have we exploit the train test plate

00:29:45,780 --> 00:29:52,920
so we the idea of the cross validation

00:29:47,850 --> 00:29:55,679
here is the two splitting the data the

00:29:52,920 --> 00:29:58,710
training data into different sets the

00:29:55,679 --> 00:30:00,960
the training set and the test set so we

00:29:58,710 --> 00:30:04,290
fit on the training set and we predict

00:30:00,960 --> 00:30:07,170
on the test set okay so in this case we

00:30:04,290 --> 00:30:10,890
will see we see that there are some

00:30:07,170 --> 00:30:13,620
errors okay coming from this prediction

00:30:10,890 --> 00:30:18,240
okay there's a more obvious way to

00:30:13,620 --> 00:30:20,580
evaluate our prediction model okay so

00:30:18,240 --> 00:30:23,730
the last couple of things thank you the

00:30:20,580 --> 00:30:26,010
last couple of things is large scale out

00:30:23,730 --> 00:30:29,190
of the box okay another great battery

00:30:26,010 --> 00:30:32,040
included in Psych it is the support for

00:30:29,190 --> 00:30:35,160
large-scale computation in already out

00:30:32,040 --> 00:30:38,910
of the box you may combine scikit-learn

00:30:35,160 --> 00:30:42,360
code with every library you want to use

00:30:38,910 --> 00:30:44,760
for multiprocessing or parallel

00:30:42,360 --> 00:30:48,000
computation distributed computation but

00:30:44,760 --> 00:30:50,309
if you want to exploit the already

00:30:48,000 --> 00:30:53,730
provided features for this kind of stuff

00:30:50,309 --> 00:30:55,559
is some there are many techniques in the

00:30:53,730 --> 00:30:58,650
library that allows for a parameter

00:30:55,559 --> 00:31:01,650
which is called an underscore jobs if

00:30:58,650 --> 00:31:03,630
you set these parameters with a value

00:31:01,650 --> 00:31:09,210
different 2 1 which is the default value

00:31:03,630 --> 00:31:11,970
it performed performs the computation on

00:31:09,210 --> 00:31:14,700
the different CP who you have in your

00:31:11,970 --> 00:31:19,620
machine if you put the minus 1 value

00:31:14,700 --> 00:31:22,190
here this means that is gone it is going

00:31:19,620 --> 00:31:24,760
to exploit all the CPUs you have in your

00:31:22,190 --> 00:31:27,820
single machine okay

00:31:24,760 --> 00:31:29,530
and this is for different settings or

00:31:27,820 --> 00:31:31,660
four different kind of application in

00:31:29,530 --> 00:31:34,510
Marshall learning you may apply multiple

00:31:31,660 --> 00:31:38,920
processing for clustering the k-means

00:31:34,510 --> 00:31:40,900
examples we made few slides ago for

00:31:38,920 --> 00:31:42,990
cross validation for instance or for a

00:31:40,900 --> 00:31:45,850
grid search grid searches and other

00:31:42,990 --> 00:31:48,780
great features include that feature

00:31:45,850 --> 00:31:52,270
included in Psych it that is able to

00:31:48,780 --> 00:31:54,790
identify the best parameter for a

00:31:52,270 --> 00:31:56,800
prediction that for a predictor that

00:31:54,790 --> 00:31:59,710
maximizes the value for the

00:31:56,800 --> 00:32:03,550
cross-validation so we want to get the

00:31:59,710 --> 00:32:05,470
best parameters for our model that

00:32:03,550 --> 00:32:09,640
maximizes the cross-validation so that

00:32:05,470 --> 00:32:14,590
is able to generalize the best okay just

00:32:09,640 --> 00:32:18,220
to do to to to give the intuition okay

00:32:14,590 --> 00:32:20,890
this is a possible thanks to the jaw

00:32:18,220 --> 00:32:23,350
bleep library which is provided in the

00:32:20,890 --> 00:32:26,380
background okay so under the hood the

00:32:23,350 --> 00:32:29,440
new number jobs here correspond to a

00:32:26,380 --> 00:32:31,180
call to the job lip okay there the job

00:32:29,440 --> 00:32:34,180
lip is well documented as well so you

00:32:31,180 --> 00:32:37,090
might read the documentation for any

00:32:34,180 --> 00:32:39,930
additional details and last but by no

00:32:37,090 --> 00:32:44,230
means least psych admits any other

00:32:39,930 --> 00:32:46,990
libraries okay sorry psychic could be

00:32:44,230 --> 00:32:48,940
integrated with NLT k this is that is

00:32:46,990 --> 00:32:52,120
natural language took it and for psyche

00:32:48,940 --> 00:32:54,370
mhm just to make a couple of example in

00:32:52,120 --> 00:32:59,130
details psych admits natural language

00:32:54,370 --> 00:33:01,510
toolkit by design NLT k includes a

00:32:59,130 --> 00:33:03,880
additional module which is analytic aid

00:33:01,510 --> 00:33:07,180
of classified scikit-learn which is

00:33:03,880 --> 00:33:10,660
actually a wrapper in the NL TK library

00:33:07,180 --> 00:33:13,510
that allows to translate the API of

00:33:10,660 --> 00:33:16,960
psych it in the API used in l.a TK okay

00:33:13,510 --> 00:33:19,630
so if you have code on an l TK you want

00:33:16,960 --> 00:33:23,350
to apply a classifier exploiting the

00:33:19,630 --> 00:33:26,800
side kid library ok you may translate

00:33:23,350 --> 00:33:29,710
you may import the classifier from psych

00:33:26,800 --> 00:33:32,260
it and then you may use the psychic

00:33:29,710 --> 00:33:36,760
learn classifier class from the NLT k

00:33:32,260 --> 00:33:38,620
package over there and wrap the

00:33:36,760 --> 00:33:43,270
interface for this classifier

00:33:38,620 --> 00:33:45,490
to the one of cycads cash linear as we

00:33:43,270 --> 00:33:48,400
see that stands for support vector

00:33:45,490 --> 00:33:51,430
classifier okay and then you may also

00:33:48,400 --> 00:33:56,400
include this kind of stuff in a pipeline

00:33:51,430 --> 00:33:59,200
processing of psych it so in conclusion

00:33:56,400 --> 00:34:01,330
scikit-learn is not the only marshal

00:33:59,200 --> 00:34:03,880
learning library available in Python but

00:34:01,330 --> 00:34:06,010
it is powerful and in my opinion easy to

00:34:03,880 --> 00:34:09,010
use very efficient implementation

00:34:06,010 --> 00:34:11,590
provided it's based on new pipe I

00:34:09,010 --> 00:34:15,340
ensighten under the hood and it is

00:34:11,590 --> 00:34:17,980
highly integrated for example in an l TK

00:34:15,340 --> 00:34:21,700
or psyche ademas just to make an example

00:34:17,980 --> 00:34:24,520
so i really hope that you're looking

00:34:21,700 --> 00:34:32,409
forward to using it and thanks a lot for

00:34:24,520 --> 00:34:34,990
your kind attention thank you thank you

00:34:32,409 --> 00:34:37,240
for alario we have six minutes left for

00:34:34,990 --> 00:34:49,780
your questions please raise your hand

00:34:37,240 --> 00:34:51,580
and I'll come by with a microphone well

00:34:49,780 --> 00:34:53,110
thanks for the talk I've two short

00:34:51,580 --> 00:34:55,600
questions does if I could ruin and

00:34:53,110 --> 00:34:58,600
provide any online learning methods yes

00:34:55,600 --> 00:35:01,480
yeah yeah actually this is a point I I

00:34:58,600 --> 00:35:03,880
wasn't able to include in the slides the

00:35:01,480 --> 00:35:06,880
online learning is already provided and

00:35:03,880 --> 00:35:08,860
there are many classifiers or techniques

00:35:06,880 --> 00:35:13,000
that allows for a method which is called

00:35:08,860 --> 00:35:16,870
partial fit okay so you have did this

00:35:13,000 --> 00:35:19,270
method to provide the the modal a bunch

00:35:16,870 --> 00:35:22,270
of data one at a time ok so the

00:35:19,270 --> 00:35:24,520
interface has been extended by a partial

00:35:22,270 --> 00:35:27,750
fifth method so some techniques allow

00:35:24,520 --> 00:35:31,510
for online learning and another very

00:35:27,750 --> 00:35:33,790
great usage of this partial fit is in

00:35:31,510 --> 00:35:37,420
case of the so called out of core

00:35:33,790 --> 00:35:40,480
learning in that case the in the out of

00:35:37,420 --> 00:35:44,530
coal out of course our learning setting

00:35:40,480 --> 00:35:47,230
your your data are too too big to fit in

00:35:44,530 --> 00:35:50,170
the memory ok so you provide the data

00:35:47,230 --> 00:35:51,850
one bunch of the bunch of data one time

00:35:50,170 --> 00:35:54,160
because there

00:35:51,850 --> 00:35:58,060
too big to fit in the memory so you call

00:35:54,160 --> 00:36:00,190
the partial fit method to train in case

00:35:58,060 --> 00:36:03,760
of a classifier to fit your model a

00:36:00,190 --> 00:36:07,600
bunch at a bunch of the time okay thanks

00:36:03,760 --> 00:36:09,340
second so I got a quick question um is

00:36:07,600 --> 00:36:10,990
there any support for missing values or

00:36:09,340 --> 00:36:13,750
missing labels apart from just deleting

00:36:10,990 --> 00:36:15,730
them in case of online learning you're

00:36:13,750 --> 00:36:18,340
ready no just in general for any machine

00:36:15,730 --> 00:36:21,310
learning for missing labels missing

00:36:18,340 --> 00:36:22,540
labels are missing data what do you mean

00:36:21,310 --> 00:36:25,780
so like if you have a feature vector

00:36:22,540 --> 00:36:29,980
that just misses like a value at the

00:36:25,780 --> 00:36:35,670
third component actually I don't know

00:36:29,980 --> 00:36:42,160
okay I feel I don't know yeah thank you

00:36:35,670 --> 00:36:43,840
I'll just let I'm by so we have a very

00:36:42,160 --> 00:36:46,510
simple in pewter that's going to impute

00:36:43,840 --> 00:36:49,270
by a median or mean in the different

00:36:46,510 --> 00:36:50,680
directions so if you have very few

00:36:49,270 --> 00:36:53,980
missing data is going to work well if

00:36:50,680 --> 00:36:56,830
you have a lot then you might want to

00:36:53,980 --> 00:36:58,630
look at matrix completion methods which

00:36:56,830 --> 00:37:00,430
we do not have we had a google summer of

00:36:58,630 --> 00:37:02,530
code project on this last year they

00:37:00,430 --> 00:37:10,780
didn't finish we welcome contributions

00:37:02,530 --> 00:37:12,460
of course thank you hello hi um I have

00:37:10,780 --> 00:37:16,140
some experience actually with psychic

00:37:12,460 --> 00:37:21,100
before and I actually am ultimately a

00:37:16,140 --> 00:37:24,520
mathematician and I had no eight-year

00:37:21,100 --> 00:37:27,490
about all the stuff under the hood and I

00:37:24,520 --> 00:37:29,890
didn't want too deep to inside it to be

00:37:27,490 --> 00:37:34,450
too deep inside of the whole algorithm

00:37:29,890 --> 00:37:38,140
starts and mathematics and such and the

00:37:34,450 --> 00:37:41,140
biggest problem for me was to realize

00:37:38,140 --> 00:37:43,600
what do i do wrong so if you got some

00:37:41,140 --> 00:37:48,340
kind of big data set with fisher's

00:37:43,600 --> 00:37:51,610
labeled supervised learning how what

00:37:48,340 --> 00:37:55,570
would you advice to someone who doesn't

00:37:51,610 --> 00:37:59,650
know how to support inside what which

00:37:55,570 --> 00:38:03,370
steps or which and small smaller easy

00:37:59,650 --> 00:38:04,800
solutions should I consider to improve

00:38:03,370 --> 00:38:07,610
the results of the Pacific

00:38:04,800 --> 00:38:10,470
suffocation thanks yeah actually

00:38:07,610 --> 00:38:12,900
Marshall learning is about finding the

00:38:10,470 --> 00:38:16,830
right mobile with the right parameters

00:38:12,900 --> 00:38:20,250
okay so there are many steps you may

00:38:16,830 --> 00:38:22,830
want to apply in your training the

00:38:20,250 --> 00:38:25,470
different algorithms in general you

00:38:22,830 --> 00:38:29,100
apply data normalization steps so you

00:38:25,470 --> 00:38:33,180
might first of all the the first step I

00:38:29,100 --> 00:38:35,570
suggest is pre processing of the data ok

00:38:33,180 --> 00:38:38,190
so you analyze the data you make some

00:38:35,570 --> 00:38:40,020
statistical tests on the data some

00:38:38,190 --> 00:38:43,290
pre-processing some visualization of

00:38:40,020 --> 00:38:45,450
your data now that you know what kind of

00:38:43,290 --> 00:38:48,480
data you're dealing with ok so this is

00:38:45,450 --> 00:38:52,650
the first step the second one is try the

00:38:48,480 --> 00:38:57,420
the simplest model you you want to apply

00:38:52,650 --> 00:38:59,490
and then improve it once that time ok if

00:38:57,420 --> 00:39:06,780
you find the right model you want to use

00:38:59,490 --> 00:39:09,960
then you want to find your required to

00:39:06,780 --> 00:39:13,320
find the best settings for that model ok

00:39:09,960 --> 00:39:15,840
in that case you might end up using the

00:39:13,320 --> 00:39:19,730
grid search method for instance which is

00:39:15,840 --> 00:39:22,230
a method provided out of the box just to

00:39:19,730 --> 00:39:25,260
find the best combination of parameters

00:39:22,230 --> 00:39:29,340
that maximizes the values of the profs

00:39:25,260 --> 00:39:35,780
validation for instance and of course

00:39:29,340 --> 00:39:39,270
it's a training on the job right so ye

00:39:35,780 --> 00:39:41,340
you may find the right model for your

00:39:39,270 --> 00:39:44,280
productions or you might find the worst

00:39:41,340 --> 00:39:49,770
model and then you start over again and

00:39:44,280 --> 00:39:53,400
look for different models ok open that

00:39:49,770 --> 00:39:55,620
open this helps yes thanks again Valerio

00:39:53,400 --> 00:39:57,540
I think he is going he's going to be

00:39:55,620 --> 00:40:00,150
give a talk at pay data as well I think

00:39:57,540 --> 00:40:02,760
on Saturday isn't it yep on Saturday so

00:40:00,150 --> 00:40:06,360
if your attempt I data don't miss that

00:40:02,760 --> 00:40:07,530
talk as well and yeah thanks again thank

00:40:06,360 --> 00:40:09,590
you very much

00:40:07,530 --> 00:40:09,590

YouTube URL: https://www.youtube.com/watch?v=Zft1xvsi2bw


