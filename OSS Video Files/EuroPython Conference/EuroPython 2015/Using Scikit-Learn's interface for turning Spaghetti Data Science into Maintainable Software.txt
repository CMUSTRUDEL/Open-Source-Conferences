Title: Using Scikit-Learn's interface for turning Spaghetti Data Science into Maintainable Software
Publication date: 2015-08-08
Playlist: EuroPython 2015
Description: 
	Holger Peters - Using Scikit-Learn's interface for turning Spaghetti Data Science into Maintainable Software
[EuroPython 2015]
[21 July 2015]
[Bilbao, Euskadi, Spain]

Finding a good structure for number-crunching code can be a problem,
this especially applies to routines preceding the core algorithms:
transformations such as data processing and cleanup, as well as
feature construction.

With such code, the programmer faces the problem, that their code
easily  turns into a sequence of highly interdependent operations,
which are hard to separate. It can be challenging to test, maintain
and reuse such "Data Science Spaghetti code".

Scikit-Learn offers a simple yet powerful interface for data science
algorithms: the estimator and composite classes (called meta-
estimators). By example, I show how clever usage of meta-estimators
can encapsulate elaborate machine learning models into a maintainable
tree of objects that is both handy to use and simple to test.

Looking at examples, I will show how this approach simplifies model
development, testing and validation and how this brings together best
practices from software engineering as well as data science.

_Knowledge of Scikit-Learn is handy but not necessary to follow this talk._
Captions: 
	00:00:03,920 --> 00:00:08,610
thank you for the kind of direction

00:00:05,970 --> 00:00:12,059
hello I'm a hugger Peters from leander

00:00:08,610 --> 00:00:16,260
and I'm presenting to you today how to

00:00:12,059 --> 00:00:18,449
use psychic learns good work good

00:00:16,260 --> 00:00:22,080
interfaces for writing maintainable and

00:00:18,449 --> 00:00:25,920
testable machine learning code so this

00:00:22,080 --> 00:00:31,230
talk will not really focus on the best

00:00:25,920 --> 00:00:33,239
model development or the best algorithm

00:00:31,230 --> 00:00:34,680
it'll just show you a way how to

00:00:33,239 --> 00:00:38,250
structure your code in a way that you

00:00:34,680 --> 00:00:42,450
can test it and that you can use it in a

00:00:38,250 --> 00:00:44,160
reliable way in production for some of

00:00:42,450 --> 00:00:46,500
you who might not notice I could learn

00:00:44,160 --> 00:00:49,200
cycle learn is probably the most

00:00:46,500 --> 00:00:54,390
well-known machine learning package for

00:00:49,200 --> 00:00:56,640
Python and it's really it's really a

00:00:54,390 --> 00:01:00,320
great package it has all batteries

00:00:56,640 --> 00:01:05,129
included and this is its interface

00:01:00,320 --> 00:01:07,290
alright the problem in general that that

00:01:05,129 --> 00:01:11,159
I'm talking about is that of supervised

00:01:07,290 --> 00:01:17,040
machine learning in this talk and just

00:01:11,159 --> 00:01:20,970
imagine a problem we have on the left

00:01:17,040 --> 00:01:25,820
side here on the table we have a table

00:01:20,970 --> 00:01:30,659
with with data it's a season that's

00:01:25,820 --> 00:01:32,790
spring summer fall and winter we have a

00:01:30,659 --> 00:01:34,829
binary variable encoding whether we have

00:01:32,790 --> 00:01:37,860
a day that is a holiday or not each row

00:01:34,829 --> 00:01:40,110
is a data point and each column is what

00:01:37,860 --> 00:01:43,649
we call a feature on the right-hand side

00:01:40,110 --> 00:01:46,079
we have some some variable that will

00:01:43,649 --> 00:01:48,659
pull a target it is closely associated

00:01:46,079 --> 00:01:50,310
to the features and the target is a

00:01:48,659 --> 00:01:53,970
variable that we would like to predict

00:01:50,310 --> 00:01:58,020
from our features and so features are

00:01:53,970 --> 00:02:01,619
known data targets are is the data that

00:01:58,020 --> 00:02:05,430
we want to estimate from a given table

00:02:01,619 --> 00:02:09,490
on the left in order to do this

00:02:05,430 --> 00:02:11,650
in order to do this we actually have one

00:02:09,490 --> 00:02:13,990
data set where we have features and

00:02:11,650 --> 00:02:16,480
targets matching features and target

00:02:13,990 --> 00:02:18,790
data and we can use this to train a

00:02:16,480 --> 00:02:23,200
model and then have a model that

00:02:18,790 --> 00:02:28,540
predicts so the interface is as follows

00:02:23,200 --> 00:02:30,670
we have a class that represents a

00:02:28,540 --> 00:02:33,310
machine learning algorithm it has a

00:02:30,670 --> 00:02:36,940
method fit that gilts gets features

00:02:33,310 --> 00:02:39,550
named axe and a target array called why

00:02:36,940 --> 00:02:41,220
and that trains the model so the model

00:02:39,550 --> 00:02:45,640
learns about the correlations between

00:02:41,220 --> 00:02:48,850
features and targets and then we have a

00:02:45,640 --> 00:02:53,380
method predict that can be called upon

00:02:48,850 --> 00:02:56,740
the trained estimator and that gives us

00:02:53,380 --> 00:03:04,060
an estimate why for the given model and

00:02:56,740 --> 00:03:06,850
the given features X and this is the

00:03:04,060 --> 00:03:08,709
basic problem of machine learning there

00:03:06,850 --> 00:03:10,590
are algorithms to solve this and I'm not

00:03:08,709 --> 00:03:12,910
going to talk about these algorithms I

00:03:10,590 --> 00:03:16,930
rather would like to focus on how to

00:03:12,910 --> 00:03:19,120
prepare data the feature data X in this

00:03:16,930 --> 00:03:23,200
talk and how to make it in a way that is

00:03:19,120 --> 00:03:26,580
both testable reliable and readable to

00:03:23,200 --> 00:03:32,140
software developers and data scientists

00:03:26,580 --> 00:03:35,110
so if you I'm sure you want to see how

00:03:32,140 --> 00:03:39,640
this looks like in a short code snippet

00:03:35,110 --> 00:03:45,010
and this actually quite succinct so in

00:03:39,640 --> 00:03:47,380
this example here we generate some data

00:03:45,010 --> 00:03:56,709
sets extraneous tests and white rain

00:03:47,380 --> 00:04:00,459
whitest then we create a support vector

00:03:56,709 --> 00:04:01,930
aggressor that is our some algorithm

00:04:00,459 --> 00:04:04,690
that I take off the shelf and so I could

00:04:01,930 --> 00:04:07,600
learn we fit the training data and we

00:04:04,690 --> 00:04:11,920
predict on the test data set and in the

00:04:07,600 --> 00:04:15,660
end we can obtain a metric we can test

00:04:11,920 --> 00:04:16,750
how well is our prediction based on our

00:04:15,660 --> 00:04:24,640
input

00:04:16,750 --> 00:04:26,230
features X tests and so this is a train

00:04:24,640 --> 00:04:28,330
model and so I could learn it's very

00:04:26,230 --> 00:04:31,420
simple very easy and the big question

00:04:28,330 --> 00:04:34,270
now is how do we obtain on how can we

00:04:31,420 --> 00:04:36,100
best prepare input data for our

00:04:34,270 --> 00:04:38,110
estimator because that table that i

00:04:36,100 --> 00:04:41,500
showed you might come from an SQL

00:04:38,110 --> 00:04:43,780
database or from other inputs it

00:04:41,500 --> 00:04:47,100
sometimes has to be prepared for the

00:04:43,780 --> 00:04:49,870
model so we get a good prediction and

00:04:47,100 --> 00:04:56,340
you can think of this preparation in a

00:04:49,870 --> 00:04:56,340
way it's a bit like some some

00:04:56,430 --> 00:05:01,960
preparation as it's done in a factory so

00:04:59,830 --> 00:05:08,050
there are certain certain steps that are

00:05:01,960 --> 00:05:10,570
executed to prepare this data and you

00:05:08,050 --> 00:05:13,480
have to you have to cut pieces into the

00:05:10,570 --> 00:05:17,830
right shape so that the algorithm can

00:05:13,480 --> 00:05:22,180
work with them one typical preparation

00:05:17,830 --> 00:05:26,130
that we have for a lot of a lot of

00:05:22,180 --> 00:05:30,640
machine learning algorithms as that of a

00:05:26,130 --> 00:05:35,710
normal normally distributed scaling so

00:05:30,640 --> 00:05:39,600
what we imagine that your data has very

00:05:35,710 --> 00:05:42,310
high numbers and very low numbers and

00:05:39,600 --> 00:05:43,840
but your algorithm really would like to

00:05:42,310 --> 00:05:46,780
have the values that are nicely

00:05:43,840 --> 00:05:52,240
distributed around zero with the

00:05:46,780 --> 00:05:54,820
standard deviation of one and such a

00:05:52,240 --> 00:06:00,160
such a scaling can be easily phrased in

00:05:54,820 --> 00:06:03,660
Python code so X is an array and we just

00:06:00,160 --> 00:06:07,570
take the mean over all columns and

00:06:03,660 --> 00:06:10,270
subtract it from our array X so we

00:06:07,570 --> 00:06:13,180
subtract the mean of each column from

00:06:10,270 --> 00:06:14,380
each column and then we calculate based

00:06:13,180 --> 00:06:18,160
on this we calculate the standard

00:06:14,380 --> 00:06:21,580
deviation and divide by the standard

00:06:18,160 --> 00:06:25,690
deviation so now each column each column

00:06:21,580 --> 00:06:28,030
should be distributed around a mean of

00:06:25,690 --> 00:06:30,289
zero now with a standard deviation of

00:06:28,030 --> 00:06:34,009
approximately

00:06:30,289 --> 00:06:38,499
one and I've prepared a small sample for

00:06:34,009 --> 00:06:42,289
this so you can see above an input array

00:06:38,499 --> 00:06:44,990
X and it has two columns let's first

00:06:42,289 --> 00:06:49,009
focus on the rightmost column that would

00:06:44,990 --> 00:06:50,990
be a future variable with values 32 1831

00:06:49,009 --> 00:06:53,089
of course in reality we would have

00:06:50,990 --> 00:06:56,330
future race but for the example a very

00:06:53,089 --> 00:06:59,210
small one is sufficient and then we

00:06:56,330 --> 00:07:02,949
apply our scaling and in the in the end

00:06:59,210 --> 00:07:06,979
that column now has values that are

00:07:02,949 --> 00:07:11,539
based around 0 and are very close to

00:07:06,979 --> 00:07:14,599
zero and now I put in another problem

00:07:11,539 --> 00:07:16,580
that we have in data processing we have

00:07:14,599 --> 00:07:19,249
a whistling value so just imagine I

00:07:16,580 --> 00:07:21,199
showed you in the first slide I showed

00:07:19,249 --> 00:07:23,539
you an example where we have weather

00:07:21,199 --> 00:07:28,369
data just imagine that the thermometer

00:07:23,539 --> 00:07:31,279
that measured the temperature was broken

00:07:28,369 --> 00:07:34,990
on a day so you don't have a value here

00:07:31,279 --> 00:07:37,639
but you would like that your estimation

00:07:34,990 --> 00:07:41,449
you are still like an estimation for

00:07:37,639 --> 00:07:47,509
that day and in such cases we can we

00:07:41,449 --> 00:07:52,729
have ways how to fill this data with

00:07:47,509 --> 00:07:56,709
values and strategies so one strategy is

00:07:52,729 --> 00:08:01,009
just to replace this not a number value

00:07:56,709 --> 00:08:03,740
with the mean of this feature variable

00:08:01,009 --> 00:08:06,949
so you could take the mean of

00:08:03,740 --> 00:08:11,319
temperatures of historic data to replace

00:08:06,949 --> 00:08:11,319
such a missing temperature slot and

00:08:11,919 --> 00:08:16,669
because if you apply our algorithm with

00:08:15,589 --> 00:08:20,990
the mean and dividing by the standard

00:08:16,669 --> 00:08:24,949
deviation what you'll get is just a yeah

00:08:20,990 --> 00:08:26,809
in this example you'll get a a data

00:08:24,949 --> 00:08:30,409
error from our code because another

00:08:26,809 --> 00:08:36,289
number values will just break the means

00:08:30,409 --> 00:08:38,449
so I've prepared a bit code that is does

00:08:36,289 --> 00:08:41,120
a bit more than our code before so

00:08:38,449 --> 00:08:43,870
before we just subtracted the mean and

00:08:41,120 --> 00:08:47,950
divided by the standard deviation

00:08:43,870 --> 00:08:52,750
and now we would like to replace another

00:08:47,950 --> 00:08:57,029
number values by the mean and the reason

00:08:52,750 --> 00:08:59,950
our code failed before was that taking

00:08:57,029 --> 00:09:02,529
the mean of a column that contains that

00:08:59,950 --> 00:09:04,920
number values numerically just raises

00:09:02,529 --> 00:09:07,570
not a number so here I replaced our

00:09:04,920 --> 00:09:10,860
nampa mean function by the function and

00:09:07,570 --> 00:09:14,650
pine and mean which will yield even with

00:09:10,860 --> 00:09:19,710
none values in our array X it will yield

00:09:14,650 --> 00:09:22,510
a proper value for the mean then we can

00:09:19,710 --> 00:09:24,460
subtract again as we did before the mean

00:09:22,510 --> 00:09:29,290
and divide by the standard deviation and

00:09:24,460 --> 00:09:31,779
in the end will execute a function nampa

00:09:29,290 --> 00:09:35,529
Antuna to noon which will replace all

00:09:31,779 --> 00:09:38,740
not a number values by zero and in our

00:09:35,529 --> 00:09:42,130
rescaled data 0 is the mean of the data

00:09:38,740 --> 00:09:47,110
so we have replaced not a number values

00:09:42,130 --> 00:09:52,230
by the mean and so how does this new

00:09:47,110 --> 00:09:56,020
code transform our data and it actually

00:09:52,230 --> 00:09:58,600
seems to work pretty well the same data

00:09:56,020 --> 00:10:01,390
example with the new code as a resulting

00:09:58,600 --> 00:10:05,140
ray we're both columns are distributed

00:10:01,390 --> 00:10:10,959
around 0 with a small standard deviation

00:10:05,140 --> 00:10:14,500
and so this is an example of some data

00:10:10,959 --> 00:10:16,270
processing that you would apply maybe to

00:10:14,500 --> 00:10:25,870
your data before you feed it into the

00:10:16,270 --> 00:10:28,120
nesta mater and yeah this this small

00:10:25,870 --> 00:10:31,630
example actually has a few properties

00:10:28,120 --> 00:10:38,610
that are very interesting so I said that

00:10:31,630 --> 00:10:44,680
we if we go back to our example we

00:10:38,610 --> 00:10:46,600
actually transform our array X and take

00:10:44,680 --> 00:10:51,670
the standard deviations of all columns

00:10:46,600 --> 00:10:55,290
and the mean of all columns before we

00:10:51,670 --> 00:10:57,699
call estimator predict but what about

00:10:55,290 --> 00:11:00,579
the next call when we call

00:10:57,699 --> 00:11:03,100
tomato dot predict there's also an array

00:11:00,579 --> 00:11:05,939
acts that is feed fed into and we have

00:11:03,100 --> 00:11:09,179
to process data that goes into this

00:11:05,939 --> 00:11:11,319
predict accordingly as we have

00:11:09,179 --> 00:11:15,220
transformed the data that goes into fit

00:11:11,319 --> 00:11:17,679
why is this because our estimator is has

00:11:15,220 --> 00:11:19,749
learned about the shapes and

00:11:17,679 --> 00:11:24,910
correlations of the data that we gave it

00:11:19,749 --> 00:11:26,709
in fit so the data has to look has to

00:11:24,910 --> 00:11:29,919
have the same distributions the same

00:11:26,709 --> 00:11:35,649
shape as the data that it saw during fit

00:11:29,919 --> 00:11:37,809
and how can we do this how can we make

00:11:35,649 --> 00:11:39,879
sure that the data has been transformed

00:11:37,809 --> 00:11:41,439
in the same way and so I could learn has

00:11:39,879 --> 00:11:46,329
a concept for this and that's the

00:11:41,439 --> 00:11:51,669
transformer concept a transformer is an

00:11:46,329 --> 00:11:55,720
object that has this notion of a fit na

00:11:51,669 --> 00:11:57,720
transform step so we can fit data if

00:11:55,720 --> 00:12:01,629
hiddenness transform we can train it

00:11:57,720 --> 00:12:03,609
during with a method fit and we concerns

00:12:01,629 --> 00:12:05,889
form it with a method transform and

00:12:03,609 --> 00:12:07,539
there's shortcut define a secular and

00:12:05,889 --> 00:12:11,199
fit transform that is both at the same

00:12:07,539 --> 00:12:14,470
time what's important about this

00:12:11,199 --> 00:12:17,919
transform returns a modified version of

00:12:14,470 --> 00:12:21,399
our feature matrix x given a matrix X

00:12:17,919 --> 00:12:26,919
and during the fit it has to it can also

00:12:21,399 --> 00:12:31,569
see a Y and so now we can actually

00:12:26,919 --> 00:12:34,029
rephrase our code that did the scaling

00:12:31,569 --> 00:12:36,579
and not a number replacement in terms of

00:12:34,029 --> 00:12:39,279
such a transformer so I called this I

00:12:36,579 --> 00:12:41,679
wrote a little class it's called a lot

00:12:39,279 --> 00:12:43,539
number guessing scalar so because it

00:12:41,679 --> 00:12:46,239
gets us small replacement values for not

00:12:43,539 --> 00:12:50,319
numbers and it scales the data and I

00:12:46,239 --> 00:12:52,389
implemented a method fit that has desk

00:12:50,319 --> 00:12:55,809
the mean calculation as you can see and

00:12:52,389 --> 00:12:58,779
it saves the means and the standard

00:12:55,809 --> 00:13:02,679
deviations of the columns as attributes

00:12:58,779 --> 00:13:06,639
of the object self and then it has a

00:13:02,679 --> 00:13:08,980
method transform and transform does the

00:13:06,639 --> 00:13:10,690
actual transformation it subtracts the

00:13:08,980 --> 00:13:13,240
mean and it divides by the Senate

00:13:10,690 --> 00:13:17,050
aviation and it replaces not a number

00:13:13,240 --> 00:13:22,600
values by zeros which are zero is the

00:13:17,050 --> 00:13:27,010
mean of our transformed data and using

00:13:22,600 --> 00:13:29,320
this pattern we can fit our not number

00:13:27,010 --> 00:13:32,530
guesting transformer with our training

00:13:29,320 --> 00:13:34,680
data and then transform the data that we

00:13:32,530 --> 00:13:37,420
actually would like to use for predict

00:13:34,680 --> 00:13:42,190
we can translate transform it in very

00:13:37,420 --> 00:13:44,320
same way and another opportunity here is

00:13:42,190 --> 00:13:48,190
since we have a nicely defined interface

00:13:44,320 --> 00:13:52,630
and for this we can actually start

00:13:48,190 --> 00:13:56,910
testing it and I wrote a little test for

00:13:52,630 --> 00:14:00,730
our class I think you remember our

00:13:56,910 --> 00:14:03,550
example array and I create a knot number

00:14:00,730 --> 00:14:10,270
guessing scalar i invoke fit transform

00:14:03,550 --> 00:14:13,720
to obtain a transformed matrix and then

00:14:10,270 --> 00:14:17,320
i start testing assumptions that i have

00:14:13,720 --> 00:14:22,390
about the outcome of this transformation

00:14:17,320 --> 00:14:25,330
and now the issue that this test

00:14:22,390 --> 00:14:28,780
actually assess finds an issue our

00:14:25,330 --> 00:14:31,300
implementation was wrong because if i

00:14:28,780 --> 00:14:34,150
calculate the standard deviation for

00:14:31,300 --> 00:14:37,800
each column and I expect the standard

00:14:34,150 --> 00:14:37,800
deviation for each column to be one I

00:14:37,890 --> 00:14:45,160
realize they're the that the senator

00:14:41,920 --> 00:14:47,680
vation is not one and that has a very

00:14:45,160 --> 00:14:51,280
simple reason if we look back at the

00:14:47,680 --> 00:14:55,030
code i calculate the standard deviation

00:14:51,280 --> 00:15:00,100
of the input sample before i replace not

00:14:55,030 --> 00:15:08,650
a number values with zero what's the

00:15:00,100 --> 00:15:11,680
mean so in this example the standard

00:15:08,650 --> 00:15:15,610
deviation of the input sample is wider

00:15:11,680 --> 00:15:18,370
than the actual distribution of the data

00:15:15,610 --> 00:15:21,610
after replacing an odd number values

00:15:18,370 --> 00:15:25,000
with with the mean

00:15:21,610 --> 00:15:27,820
and because the mean is in the center so

00:15:25,000 --> 00:15:29,890
and we map not number values also to the

00:15:27,820 --> 00:15:35,589
center of the data and that makes the

00:15:29,890 --> 00:15:38,589
distribution kind of smaller and so in a

00:15:35,589 --> 00:15:42,279
way if we want to fix this code we have

00:15:38,589 --> 00:15:46,709
to we have to think about this transfer

00:15:42,279 --> 00:15:50,200
method and the solution is actually to

00:15:46,709 --> 00:15:51,970
make two transformation steps at first

00:15:50,200 --> 00:15:54,089
we want to have one transformation step

00:15:51,970 --> 00:15:57,220
that replaced snot number values with

00:15:54,089 --> 00:15:59,800
the mean and then we want to have a

00:15:57,220 --> 00:16:07,000
second transformation step that does the

00:15:59,800 --> 00:16:11,140
actual scaling of the data so we want

00:16:07,000 --> 00:16:13,570
two transformations and psychic learn

00:16:11,140 --> 00:16:15,670
has a nice weight how to do this it

00:16:13,570 --> 00:16:19,839
offers ways to compose several

00:16:15,670 --> 00:16:22,450
transformers several transformations in

00:16:19,839 --> 00:16:24,640
this case we use a building block and I

00:16:22,450 --> 00:16:26,769
apologize for the low contrast we use

00:16:24,640 --> 00:16:29,680
building blocks that are called

00:16:26,769 --> 00:16:35,519
pipelines of pipeline and a pipeline you

00:16:29,680 --> 00:16:41,079
say sequential if they change a chain of

00:16:35,519 --> 00:16:43,630
transformers and so during fit when we

00:16:41,079 --> 00:16:47,800
have when it when we are training and

00:16:43,630 --> 00:16:50,260
learning from a feature matrix X we use

00:16:47,800 --> 00:16:53,410
a first transformation translator one

00:16:50,260 --> 00:16:58,540
and invoke fear transform to obtain a

00:16:53,410 --> 00:17:03,190
transformed version of data and then we

00:16:58,540 --> 00:17:05,020
take our second transformer also apply

00:17:03,190 --> 00:17:07,959
fit transform with the result of the

00:17:05,020 --> 00:17:11,620
first transformation and finally we will

00:17:07,959 --> 00:17:13,870
obtain a transformed data set that was

00:17:11,620 --> 00:17:15,640
transformed by several steps it can have

00:17:13,870 --> 00:17:18,459
an arbitrary number of transformers in

00:17:15,640 --> 00:17:20,470
the predict when we have already learned

00:17:18,459 --> 00:17:21,610
the properties of the data like in our

00:17:20,470 --> 00:17:25,449
example the mean and the standard

00:17:21,610 --> 00:17:29,260
deviation we can just invoke transforms

00:17:25,449 --> 00:17:32,050
and get a transformed X in the end from

00:17:29,260 --> 00:17:33,970
building a power plant in cyclone we can

00:17:32,050 --> 00:17:35,340
build them pretty easily there is am a

00:17:33,970 --> 00:17:38,370
pipeline function

00:17:35,340 --> 00:17:44,220
and we pass it transformer objects and

00:17:38,370 --> 00:17:45,870
it will it returns a pipeline object and

00:17:44,220 --> 00:17:48,150
a pipeline object itself is a

00:17:45,870 --> 00:17:50,880
transformer that means that it has the

00:17:48,150 --> 00:17:53,850
fit and the transform method and we can

00:17:50,880 --> 00:17:57,000
just use it instead of our not a number

00:17:53,850 --> 00:18:01,970
guessing scalar that i just presented so

00:17:57,000 --> 00:18:04,830
we could go back and rewrite this class

00:18:01,970 --> 00:18:09,110
into two classes one doing the scaling

00:18:04,830 --> 00:18:13,260
and one doing the not number replacement

00:18:09,110 --> 00:18:15,480
or the question is maybe there's

00:18:13,260 --> 00:18:18,990
actually some someone has solved this

00:18:15,480 --> 00:18:20,970
for us already and indeed python has

00:18:18,990 --> 00:18:24,480
batteries included and scikit-learn has

00:18:20,970 --> 00:18:29,790
batteries included so we can actually

00:18:24,480 --> 00:18:33,690
also use two transformers from cyclins

00:18:29,790 --> 00:18:35,720
library one of these transformers is

00:18:33,690 --> 00:18:40,740
called the imputed because imputes

00:18:35,720 --> 00:18:43,260
missing values and so here not a number

00:18:40,740 --> 00:18:46,350
would be replaced by the mean and then

00:18:43,260 --> 00:18:48,450
we have the standard scalar that scales

00:18:46,350 --> 00:18:50,190
the data that is distributed in this

00:18:48,450 --> 00:18:54,030
example represented by the red

00:18:50,190 --> 00:19:00,350
distribution to 12 a data set that is

00:18:54,030 --> 00:19:06,650
distributed around zero and these two

00:19:00,350 --> 00:19:09,960
transformers can be joined by a pipeline

00:19:06,650 --> 00:19:11,820
so here you can see this we just put

00:19:09,960 --> 00:19:14,070
together the building blocks that we

00:19:11,820 --> 00:19:16,890
already have we saw make pipeline we use

00:19:14,070 --> 00:19:18,810
make pipeline here and pass it a in

00:19:16,890 --> 00:19:26,040
pewter instance and a sender scalar into

00:19:18,810 --> 00:19:30,000
instance and then if we fit transform a

00:19:26,040 --> 00:19:33,990
our example array we can actually make

00:19:30,000 --> 00:19:35,940
sure that our assumption holds true that

00:19:33,990 --> 00:19:38,310
we would like to have a standard

00:19:35,940 --> 00:19:43,040
deviation of one we could hear also

00:19:38,310 --> 00:19:47,010
check for the means and from other tests

00:19:43,040 --> 00:19:50,340
we have wrapped the data processing with

00:19:47,010 --> 00:19:51,960
I could learn transformers and we've

00:19:50,340 --> 00:19:55,940
done this in a way where we can

00:19:51,960 --> 00:19:59,090
individually test each building block so

00:19:55,940 --> 00:20:01,200
assume that these were not present in

00:19:59,090 --> 00:20:03,420
scikit-learn we could just write them

00:20:01,200 --> 00:20:08,430
ourselves and the test would be fairly

00:20:03,420 --> 00:20:10,230
easy and yeah I think this is the

00:20:08,430 --> 00:20:11,460
biggest gain that we can have from this

00:20:10,230 --> 00:20:12,720
so if you're leaving this talk and you

00:20:11,460 --> 00:20:17,220
want to take something away with it

00:20:12,720 --> 00:20:20,580
something away from it on if you want to

00:20:17,220 --> 00:20:22,650
write maintainable maintainable software

00:20:20,580 --> 00:20:25,620
if you want to avoid spaghetti code in

00:20:22,650 --> 00:20:29,090
your numeric code try to find ways how

00:20:25,620 --> 00:20:32,480
to separate different concerns different

00:20:29,090 --> 00:20:34,830
purposes in your code into independent

00:20:32,480 --> 00:20:36,540
composable units that you can then

00:20:34,830 --> 00:20:38,700
combine and you can test them

00:20:36,540 --> 00:20:42,110
individually you can combine them then

00:20:38,700 --> 00:20:47,450
you can make a test for the combined

00:20:42,110 --> 00:20:53,130
model and that's a really good way to

00:20:47,450 --> 00:20:54,840
structure your numeric algorithms so in

00:20:53,130 --> 00:20:59,700
the beginning I showed you an example of

00:20:54,840 --> 00:21:01,770
a machine learning problem where we just

00:20:59,700 --> 00:21:03,750
used a machine learning algorithm with

00:21:01,770 --> 00:21:08,160
an scikit-learn estimator that we fitted

00:21:03,750 --> 00:21:10,950
and predicted with now I extended this

00:21:08,160 --> 00:21:13,740
example with a pipeline that is the

00:21:10,950 --> 00:21:15,480
pre-processing make pipeline we use the

00:21:13,740 --> 00:21:18,120
in pewter we use the seller scalar and

00:21:15,480 --> 00:21:23,660
we can also add our estimator to this

00:21:18,120 --> 00:21:27,780
pipeline and now our object s does

00:21:23,660 --> 00:21:29,520
contain our whole algorithmic pipeline

00:21:27,780 --> 00:21:32,160
it does contain the pre processing of

00:21:29,520 --> 00:21:35,790
the data and it does contain the machine

00:21:32,160 --> 00:21:37,940
learning code and also it does contain

00:21:35,790 --> 00:21:41,130
all the fitted and estimated parameters

00:21:37,940 --> 00:21:44,100
coefficients that are present in our

00:21:41,130 --> 00:21:47,300
model so we could easily see realize

00:21:44,100 --> 00:21:51,630
this estimator object using pickle or

00:21:47,300 --> 00:21:54,300
another serialization library and a

00:21:51,630 --> 00:21:58,560
story to disk or send it across the

00:21:54,300 --> 00:22:00,450
world into a different network and then

00:21:58,560 --> 00:22:05,420
we could load it again

00:22:00,450 --> 00:22:11,130
alright and make predictions from it and

00:22:05,420 --> 00:22:13,500
just so to summarize what cycle learned

00:22:11,130 --> 00:22:17,280
and these interfaces can do for you and

00:22:13,500 --> 00:22:20,850
how I should use them we found that it's

00:22:17,280 --> 00:22:23,220
really beneficial to to use this these

00:22:20,850 --> 00:22:26,760
interfaces that scikit-learn provides

00:22:23,220 --> 00:22:30,930
for you if you want to write

00:22:26,760 --> 00:22:32,490
prepossessing code and you can use the

00:22:30,930 --> 00:22:34,890
fit transform interface for the

00:22:32,490 --> 00:22:38,100
Transformers use them write your own

00:22:34,890 --> 00:22:41,550
transformers if you don't find those

00:22:38,100 --> 00:22:45,200
that you need in a library if you write

00:22:41,550 --> 00:22:49,700
your own transformers try to separate

00:22:45,200 --> 00:22:52,920
concerns separate reps responsibilities

00:22:49,700 --> 00:22:55,230
estimating or scaling your data has

00:22:52,920 --> 00:22:57,000
nothing to do with correcting other

00:22:55,230 --> 00:22:59,150
number values so don't put them into the

00:22:57,000 --> 00:23:02,250
same transformer just right too and

00:22:59,150 --> 00:23:09,360
compose a new transformer out of the

00:23:02,250 --> 00:23:10,980
tools for for your model in the end if

00:23:09,360 --> 00:23:13,860
you keep your transformers and your

00:23:10,980 --> 00:23:18,990
class is small they are a lot easier to

00:23:13,860 --> 00:23:21,360
test and if tests fail you will find the

00:23:18,990 --> 00:23:26,660
issue a lot faster if they are simple

00:23:21,360 --> 00:23:30,360
and use the features like sterilization

00:23:26,660 --> 00:23:33,570
because you can actually quality control

00:23:30,360 --> 00:23:35,970
your estimators you can store them you

00:23:33,570 --> 00:23:39,900
can look at them again in the future

00:23:35,970 --> 00:23:41,490
it's really handy and in this short time

00:23:39,900 --> 00:23:43,950
I was not able to tell you everything

00:23:41,490 --> 00:23:46,400
about the compositional and testing

00:23:43,950 --> 00:23:50,760
things that you can do with scikit-learn

00:23:46,400 --> 00:23:53,430
so I just wanted to give you a an

00:23:50,760 --> 00:23:56,760
outlook on what else you could look at

00:23:53,430 --> 00:23:59,250
if you want to get into this topic there

00:23:56,760 --> 00:24:03,090
are tons of other transformers and other

00:23:59,250 --> 00:24:04,890
meteor transformers that compose in

00:24:03,090 --> 00:24:06,780
psychic learn that you can take a look

00:24:04,890 --> 00:24:10,020
at for example a feature union where you

00:24:06,780 --> 00:24:13,750
can combine different transformers for

00:24:10,020 --> 00:24:19,159
future generation and also

00:24:13,750 --> 00:24:21,679
estimators are composable inside loan so

00:24:19,159 --> 00:24:23,120
there's a cross-validation building

00:24:21,679 --> 00:24:26,230
block the grid search and so I could

00:24:23,120 --> 00:24:29,210
learn that actually takes estimators and

00:24:26,230 --> 00:24:30,529
extends their functionality so their

00:24:29,210 --> 00:24:36,320
predictions are cross-validated

00:24:30,529 --> 00:24:38,480
according to statistical methods so I'm

00:24:36,320 --> 00:24:40,669
at the end of my talk I thank you for

00:24:38,480 --> 00:24:45,830
your attention I'm happy to take

00:24:40,669 --> 00:24:47,240
questions if you like and if you also if

00:24:45,830 --> 00:25:01,990
you want to chat with me talk with me

00:24:47,240 --> 00:25:01,990
you can come up to me anytime hi

00:25:09,250 --> 00:25:15,310
hi could you please describe your

00:25:11,940 --> 00:25:17,560
testing environment to use it like a

00:25:15,310 --> 00:25:22,570
standard library like unit test sounds

00:25:17,560 --> 00:25:26,620
like that too um well basically we we

00:25:22,570 --> 00:25:28,180
use unit testing frameworks like unit

00:25:26,620 --> 00:25:32,050
tests or titles I personally prefer

00:25:28,180 --> 00:25:34,450
tightest as a test runner and we

00:25:32,050 --> 00:25:37,180
structure our tests or actually the

00:25:34,450 --> 00:25:39,570
tests like we were unit tests in other

00:25:37,180 --> 00:25:42,120
situations so in a most basic form

00:25:39,570 --> 00:25:44,460
testing numeric code is not

00:25:42,120 --> 00:25:47,830
fundamentally different than testing

00:25:44,460 --> 00:25:49,540
other code it's it's code it has to be

00:25:47,830 --> 00:25:51,730
tested you have to think of inputs and

00:25:49,540 --> 00:25:54,940
outputs and you have to structure your

00:25:51,730 --> 00:25:56,920
code in a way that you don't have to or

00:25:54,940 --> 00:26:00,630
that in most cases you don't have to do

00:25:56,920 --> 00:26:05,140
too much work to get a test running and

00:26:00,630 --> 00:26:13,030
so yeah we have some tools to generate

00:26:05,140 --> 00:26:14,680
data and to get more tests that are more

00:26:13,030 --> 00:26:20,620
going into the direction of integration

00:26:14,680 --> 00:26:24,670
tests but in general we just use the

00:26:20,620 --> 00:26:27,060
python tools that non data scientists

00:26:24,670 --> 00:26:27,060
also use

00:26:31,010 --> 00:26:33,850
other questions

00:26:40,080 --> 00:26:48,899
you for the dolt and there's data you

00:26:45,029 --> 00:26:53,370
apply also the transformations once you

00:26:48,899 --> 00:26:55,679
have all made all the training yes that

00:26:53,370 --> 00:26:59,190
is so if I understood a question

00:26:55,679 --> 00:27:01,529
correctly the question was if we also

00:26:59,190 --> 00:27:04,140
apply the transformations to the test

00:27:01,529 --> 00:27:06,659
data so you're talking about the data

00:27:04,140 --> 00:27:09,929
that I pass to predict right in the

00:27:06,659 --> 00:27:16,250
first example not the one that you used

00:27:09,929 --> 00:27:23,460
for the training the one that's yeah so

00:27:16,250 --> 00:27:28,769
I'm sorry here you're talking about yeah

00:27:23,460 --> 00:27:31,769
exactly yes we do this is this is the

00:27:28,769 --> 00:27:35,250
purpose of splitting in the transformer

00:27:31,769 --> 00:27:38,700
into those two methods I'll just pull up

00:27:35,250 --> 00:27:42,600
the slide again the whole purpose of

00:27:38,700 --> 00:27:45,690
splitting fit and transform here is that

00:27:42,600 --> 00:27:49,409
we can repeat repeat this transformation

00:27:45,690 --> 00:27:51,809
in transform without having to change

00:27:49,409 --> 00:27:55,380
values for them those estimated

00:27:51,809 --> 00:27:58,289
parameters mean and standard if we would

00:27:55,380 --> 00:28:05,100
execute the code in fit again then we

00:27:58,289 --> 00:28:09,919
would not get the same kind of data into

00:28:05,100 --> 00:28:09,919
our algorithm that the algorithm expects

00:28:15,590 --> 00:28:18,309
any other

00:28:29,850 --> 00:28:36,039
I'm how do you track your model

00:28:33,370 --> 00:28:38,769
performance over time so on and some of

00:28:36,039 --> 00:28:41,409
our applications we have like data going

00:28:38,769 --> 00:28:44,380
for four years and we have models that

00:28:41,409 --> 00:28:45,850
are built up and then for instance that

00:28:44,380 --> 00:28:47,980
model the assumptions underlying

00:28:45,850 --> 00:28:50,289
probabilities of the data so we're using

00:28:47,980 --> 00:28:52,389
mostly asian models the underlying

00:28:50,289 --> 00:28:55,299
probabilities are changing and we want

00:28:52,389 --> 00:28:57,399
to revalidate to see how on previous

00:28:55,299 --> 00:28:59,409
data sets or versions of data sets how

00:28:57,399 --> 00:29:01,720
the models that are over flattering

00:28:59,409 --> 00:29:03,940
around reading depending on on what we

00:29:01,720 --> 00:29:05,880
have so are you doing anything across

00:29:03,940 --> 00:29:09,580
versions of data sets to make sure that

00:29:05,880 --> 00:29:11,740
you know your assumptions aren't messing

00:29:09,580 --> 00:29:14,769
stuff or adding a new stuff that you

00:29:11,740 --> 00:29:17,529
have didn't have before okay so you're

00:29:14,769 --> 00:29:23,190
asking how we actually test the

00:29:17,529 --> 00:29:26,860
stability of our machine learning models

00:29:23,190 --> 00:29:36,940
well this is done with cross-validation

00:29:26,860 --> 00:29:41,529
methods and we yeah we have 44 sample

00:29:36,940 --> 00:29:44,679
data sets we have reference so reference

00:29:41,529 --> 00:29:48,279
scores and if the reference scores are

00:29:44,679 --> 00:29:51,580
going getting worse in the future than

00:29:48,279 --> 00:29:55,440
tests fail basically and then if that

00:29:51,580 --> 00:29:59,860
happens one has to look into into things

00:29:55,440 --> 00:30:02,860
why why things are getting worse there's

00:29:59,860 --> 00:30:05,940
not really a better way than using

00:30:02,860 --> 00:30:05,940
cross-validation methods

00:30:07,390 --> 00:30:13,159
yeah it's more of a monetary think so so

00:30:10,100 --> 00:30:16,520
this talk was more about actually

00:30:13,159 --> 00:30:20,090
testing testing the code whereas your

00:30:16,520 --> 00:30:22,820
question was rather about testing the

00:30:20,090 --> 00:30:33,850
quality of the model so I think these

00:30:22,820 --> 00:30:38,950
are two different concerns you say

00:30:33,850 --> 00:30:38,950
they're complementary yeah definitely

00:30:48,960 --> 00:30:54,580
so and I just curious when you do this

00:30:52,870 --> 00:30:57,100
what you're working i mean do you work

00:30:54,580 --> 00:30:59,740
in an ipad notebook God you do it a

00:30:57,100 --> 00:31:06,400
separate scripts or what you what you do

00:30:59,740 --> 00:31:09,220
for this yeah I'm personally not using

00:31:06,400 --> 00:31:12,250
ipython notebooks that much I just use a

00:31:09,220 --> 00:31:15,670
ride tests in test files and execute my

00:31:12,250 --> 00:31:18,580
test runner on them and then use

00:31:15,670 --> 00:31:24,030
continuous integration and all the

00:31:18,580 --> 00:31:29,050
tooling that is around unit testing the

00:31:24,030 --> 00:31:31,690
yeah I personally well I pathan notebook

00:31:29,050 --> 00:31:36,280
is no environment it's no I meant that

00:31:31,690 --> 00:31:38,920
is really great exploring things but

00:31:36,280 --> 00:31:43,210
it's not a environment for test-driven

00:31:38,920 --> 00:31:45,340
development and so there's no test run

00:31:43,210 --> 00:31:49,870
an iPad notebook and I personally think

00:31:45,340 --> 00:31:51,820
all the effort that I put into thinking

00:31:49,870 --> 00:31:54,310
about some test a certain that i could

00:31:51,820 --> 00:31:57,760
type into an ipad notebook if i put it

00:31:54,310 --> 00:31:59,980
into a unit test and check it into my

00:31:57,760 --> 00:32:01,960
repository it's done continuously over

00:31:59,980 --> 00:32:06,040
and over again so i really prefer this

00:32:01,960 --> 00:32:10,360
over extensive use of ipad notebooks i

00:32:06,040 --> 00:32:15,430
do use it if i want to quickly explore

00:32:10,360 --> 00:32:19,060
something and this is just an add-on

00:32:15,430 --> 00:32:20,680
zone no question your talk was about the

00:32:19,060 --> 00:32:24,010
testing stuff and this is really great

00:32:20,680 --> 00:32:26,590
with this modules let's say or small

00:32:24,010 --> 00:32:29,230
units but of course it's also important

00:32:26,590 --> 00:32:33,670
to have reusability then because then

00:32:29,230 --> 00:32:36,880
you can really change a model or apply

00:32:33,670 --> 00:32:39,630
to different problems reusing parts of

00:32:36,880 --> 00:32:39,630
your pipeline

00:32:43,840 --> 00:32:52,320
any other questions okay thank you thank

00:32:49,929 --> 00:32:52,320

YouTube URL: https://www.youtube.com/watch?v=LhtKkygNuNc


