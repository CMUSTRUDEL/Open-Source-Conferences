Title: Nazzaro - Image recognition and camera positioning with OpenCV. A tourist guide application.
Publication date: 2015-08-08
Playlist: EuroPython 2015
Description: 
	Francesco Nazzaro - Image recognition and camera positioning with OpenCV. A tourist guide application.
[EuroPython 2015]
[21 July 2015]
[Bilbao, Euskadi, Spain]

OpenCV Python bindings provide several ready to use tools for camera
calibration, image recognition and camera position estimation. This
talk will show how to recognize a picture, from a library of known
paintings, and compute the camera position with respect to the
recognized picture using OpenCV and numpy. This is applied to a
tourist guide application for Google Glass through the recognition of
the paintings exposed in the museum.
Captions: 
	00:00:01,820 --> 00:00:08,280
good morning my name is Francesca

00:00:05,009 --> 00:00:11,400
Nazzaro today I'm going to talk about

00:00:08,280 --> 00:00:15,210
image recognition and camera positioning

00:00:11,400 --> 00:00:19,380
with opencv a tourist guide application

00:00:15,210 --> 00:00:21,390
I work for be Avant solutions in Rome we

00:00:19,380 --> 00:00:24,050
developed software solutions for

00:00:21,390 --> 00:00:26,970
managing and publishing geospatial data

00:00:24,050 --> 00:00:32,070
using open source software based on

00:00:26,970 --> 00:00:35,460
linux and python image recognition is

00:00:32,070 --> 00:00:38,190
filled in evolution big companies invest

00:00:35,460 --> 00:00:52,710
a lot of resources in this research

00:00:38,190 --> 00:00:54,960
field they may sorry hello okay the main

00:00:52,710 --> 00:00:58,320
issues in image recognition are the

00:00:54,960 --> 00:01:01,140
following first of all we are we have to

00:00:58,320 --> 00:01:05,010
implement a human ability this is a very

00:01:01,140 --> 00:01:08,010
big challenge the images to compare can

00:01:05,010 --> 00:01:11,040
be distorted and oriented in different

00:01:08,010 --> 00:01:14,970
ways so we have to detect several

00:01:11,040 --> 00:01:19,500
features and the algorithm of detection

00:01:14,970 --> 00:01:22,979
will be scale invariant for our scope we

00:01:19,500 --> 00:01:26,040
can fin to use a ready-to-use image

00:01:22,979 --> 00:01:32,040
recognition tool like wall images let's

00:01:26,040 --> 00:01:34,710
see why we can not you use it this is a

00:01:32,040 --> 00:01:38,700
typical image taken with a smart phone

00:01:34,710 --> 00:01:40,890
or google glass and we had to recognize

00:01:38,700 --> 00:01:47,909
the picture in the center and not the

00:01:40,890 --> 00:01:52,070
arm of my boss or day back and in google

00:01:47,909 --> 00:01:55,920
images we can not define a library of

00:01:52,070 --> 00:01:58,920
images so it tries to match all the

00:01:55,920 --> 00:02:02,340
visuals in our image and not only the

00:01:58,920 --> 00:02:07,010
pictures visuals so he recognized room

00:02:02,340 --> 00:02:11,670
with a picture in the center or

00:02:07,010 --> 00:02:17,580
or other things so we have to find

00:02:11,670 --> 00:02:20,610
another strategies let's try to

00:02:17,580 --> 00:02:24,210
understand what could be the best

00:02:20,610 --> 00:02:26,820
candidate as a figured let's take a

00:02:24,210 --> 00:02:31,050
rectangle for example we can find three

00:02:26,820 --> 00:02:34,500
possible areas the journey is a flat

00:02:31,050 --> 00:02:37,620
surface if we move the area in any

00:02:34,500 --> 00:02:44,430
direction the contact of the square a

00:02:37,620 --> 00:02:47,370
will not change in aspect so we can we

00:02:44,430 --> 00:02:51,840
cannot locally date the deposition of a

00:02:47,370 --> 00:02:54,180
the zombie is an edge if we move beads

00:02:51,840 --> 00:02:58,290
on in the vertical direction the content

00:02:54,180 --> 00:03:02,070
of the square Bay will change but not

00:02:58,290 --> 00:03:04,440
not in the original direction so we can

00:03:02,070 --> 00:03:07,640
distinguish the position of the zombie

00:03:04,440 --> 00:03:11,250
all in the vertical direction the

00:03:07,640 --> 00:03:13,920
optimal case is the don't see that is a

00:03:11,250 --> 00:03:16,260
corner in fact we can distinguish the

00:03:13,920 --> 00:03:20,730
vertical and the horizontal position of

00:03:16,260 --> 00:03:23,070
this zone so the best candidate to be

00:03:20,730 --> 00:03:26,910
the officials are the colonel because

00:03:23,070 --> 00:03:29,400
they have an orientation so we need the

00:03:26,910 --> 00:03:32,400
a corner detection algorithm this

00:03:29,400 --> 00:03:35,370
algorithm should be should will be scale

00:03:32,400 --> 00:03:38,760
invariant the reason is shown in the fig

00:03:35,370 --> 00:03:41,640
in the figure an invariant algorithm

00:03:38,760 --> 00:03:45,150
will recognize the left line as a corner

00:03:41,640 --> 00:03:48,690
but it will organize the same line but

00:03:45,150 --> 00:03:51,209
bigger as two or three corners for the

00:03:48,690 --> 00:03:55,320
reason we need a scale invariant

00:03:51,209 --> 00:03:57,989
algorithm the solution is the scale

00:03:55,320 --> 00:04:01,940
invariant visual transform algorithm by

00:03:57,989 --> 00:04:04,470
David law here are presented the

00:04:01,940 --> 00:04:08,610
presented the basic steps of this

00:04:04,470 --> 00:04:11,370
algorithm first of all we first of all a

00:04:08,610 --> 00:04:14,400
difference of gaussians operator is

00:04:11,370 --> 00:04:18,329
applied to the image buying the startup

00:04:14,400 --> 00:04:19,200
standard deviation the result is a

00:04:18,329 --> 00:04:22,470
distribution

00:04:19,200 --> 00:04:25,800
and the extrema of the distribution are

00:04:22,470 --> 00:04:28,680
the scaling bio blobs different

00:04:25,800 --> 00:04:32,910
different of gaussians detector corners

00:04:28,680 --> 00:04:35,750
and edges an algorithm like Eris corner

00:04:32,910 --> 00:04:39,540
detector is used to live out the edges

00:04:35,750 --> 00:04:42,360
to obtain a rotational invariance an

00:04:39,540 --> 00:04:45,300
orientation is assigned to every key

00:04:42,360 --> 00:04:48,570
point then for each key point a

00:04:45,300 --> 00:04:51,900
descriptor vector is created key points

00:04:48,570 --> 00:04:53,880
are deficient recognized and the

00:04:51,900 --> 00:04:56,280
keyboard matching is performed through

00:04:53,880 --> 00:05:02,220
nearest neighbor algorithm between

00:04:56,280 --> 00:05:04,260
descriptors opencv as various image

00:05:02,220 --> 00:05:07,610
recognition algorithms already

00:05:04,260 --> 00:05:10,350
implemented and it has a python bindings

00:05:07,610 --> 00:05:12,870
let's see an example using sift

00:05:10,350 --> 00:05:17,790
algorithm scaling biofilter transform

00:05:12,870 --> 00:05:21,660
and I by do not become first of all we

00:05:17,790 --> 00:05:24,570
import image in black and white indeed

00:05:21,660 --> 00:05:34,500
image recognition algorithms are color

00:05:24,570 --> 00:05:37,620
colors independent we instantiate yet we

00:05:34,500 --> 00:05:40,350
instantiate the sift algorithm every

00:05:37,620 --> 00:05:43,380
major recognition algorithm in oven TV

00:05:40,350 --> 00:05:47,030
as a function called detect and compute

00:05:43,380 --> 00:05:50,370
that returns key points and descriptors

00:05:47,030 --> 00:05:52,320
this function may have four parameters

00:05:50,370 --> 00:05:56,160
for image recognition that can be

00:05:52,320 --> 00:05:58,890
optimized for the specific case opencv

00:05:56,160 --> 00:06:02,090
also as a function to represent key

00:05:58,890 --> 00:06:06,000
points and airy orientation on the image

00:06:02,090 --> 00:06:12,060
the shape of the color colored circles

00:06:06,000 --> 00:06:16,050
are the key points recognized and they

00:06:12,060 --> 00:06:20,040
have an orientation we perform the same

00:06:16,050 --> 00:06:22,290
projects on the image in the library we

00:06:20,040 --> 00:06:27,900
can immediate that we can have several

00:06:22,290 --> 00:06:30,020
images in our library we match

00:06:27,900 --> 00:06:32,420
descriptors with a nearest neighbor

00:06:30,020 --> 00:06:35,670
algorithm a code

00:06:32,420 --> 00:06:39,320
we start the good matches following loss

00:06:35,670 --> 00:06:41,670
ratio test described in the article

00:06:39,320 --> 00:06:44,940
starting from the number of the key

00:06:41,670 --> 00:06:47,850
points majid we can understand if the

00:06:44,940 --> 00:06:49,830
image has been recognized so we can set

00:06:47,850 --> 00:06:52,440
a threshold on the number of matches

00:06:49,830 --> 00:06:55,800
about which we can say that we have

00:06:52,440 --> 00:06:58,380
recognized the picture this threshold is

00:06:55,800 --> 00:07:01,830
closely related to the algorithm and the

00:06:58,380 --> 00:07:07,860
children and to the parameters used for

00:07:01,830 --> 00:07:10,980
the recognition in this slide I show an

00:07:07,860 --> 00:07:13,470
example of image recognized and not

00:07:10,980 --> 00:07:15,840
recognized we can see that the number of

00:07:13,470 --> 00:07:19,710
matches is in even significantly

00:07:15,840 --> 00:07:24,000
different for this example I use the

00:07:19,710 --> 00:07:27,990
shift a shift algorithm for our

00:07:24,000 --> 00:07:30,990
application we use we use the server

00:07:27,990 --> 00:07:35,010
algorithm that is an approximation of

00:07:30,990 --> 00:07:37,860
shift we use it because it is more

00:07:35,010 --> 00:07:41,120
computed computationally performant so

00:07:37,860 --> 00:07:44,010
for a real-time recognition is better

00:07:41,120 --> 00:07:46,230
but the number of he went found by this

00:07:44,010 --> 00:07:49,170
algorithm is significantly lower than

00:07:46,230 --> 00:07:53,610
the difference between the recognized

00:07:49,170 --> 00:07:57,810
and not recognize case the difference

00:07:53,610 --> 00:08:02,700
can be very small so we had to find a

00:07:57,810 --> 00:08:04,980
strategy to avoid false positives we

00:08:02,700 --> 00:08:09,930
developed an algorithm to compute the

00:08:04,980 --> 00:08:14,340
position of the observer so we will have

00:08:09,930 --> 00:08:16,020
emitted to exclude false positive let's

00:08:14,340 --> 00:08:22,200
try to compute the position of the

00:08:16,020 --> 00:08:24,240
server respect to the image we had to

00:08:22,200 --> 00:08:26,610
find the transformation that link the

00:08:24,240 --> 00:08:29,220
library major and the picture in the

00:08:26,610 --> 00:08:32,280
photo this transformation is called the

00:08:29,220 --> 00:08:37,220
oh ma graphy and Link different

00:08:32,280 --> 00:08:37,220
projective bro regular projective planes

00:08:37,250 --> 00:08:43,380
we are two cameras a and B looking at

00:08:41,040 --> 00:08:46,980
the same point B in a plane

00:08:43,380 --> 00:08:50,160
the projections of P in a and B are

00:08:46,980 --> 00:08:52,890
respectively PA and PB and we can

00:08:50,160 --> 00:08:59,460
express PA in function in factional of

00:08:52,890 --> 00:09:03,090
PB k and m m is the oh ma graphy and it

00:08:59,460 --> 00:09:06,930
it can be expressed through our that

00:09:03,090 --> 00:09:10,530
dissertation matrix and T that is AD

00:09:06,930 --> 00:09:14,160
resolution and the key decays matrices

00:09:10,530 --> 00:09:20,610
are the camera intrinsic parameters so

00:09:14,160 --> 00:09:24,390
we can compute them Callie this process

00:09:20,610 --> 00:09:28,080
is called the camera calibration and it

00:09:24,390 --> 00:09:30,720
is performed through just permitted we

00:09:28,080 --> 00:09:34,200
take pictures of a chessboard from

00:09:30,720 --> 00:09:36,090
different angles and we find the corners

00:09:34,200 --> 00:09:39,060
of the chess board with the era's

00:09:36,090 --> 00:09:42,330
algorithm for example and we find a

00:09:39,060 --> 00:09:44,970
distortion to obtain straight lines we

00:09:42,330 --> 00:09:48,510
can see that in this photo the lines of

00:09:44,970 --> 00:09:52,080
dishes were a slightly distorted and we

00:09:48,510 --> 00:09:55,200
have to correct this behavior this

00:09:52,080 --> 00:09:58,440
process is already implemented in oven

00:09:55,200 --> 00:10:01,460
civil on TV through the functions fine

00:09:58,440 --> 00:10:04,050
chessboard corners and colibri camera

00:10:01,460 --> 00:10:07,740
starting from these parameters we can

00:10:04,050 --> 00:10:10,170
apply a transformation to the image now

00:10:07,740 --> 00:10:15,690
we can see that the line of the

00:10:10,170 --> 00:10:19,050
chessboard are more straighter than now

00:10:15,690 --> 00:10:22,380
we can compute the position of the image

00:10:19,050 --> 00:10:25,650
of the picture in the major first of all

00:10:22,380 --> 00:10:28,880
we have to extract the magic key points

00:10:25,650 --> 00:10:31,850
for the image and for the library image

00:10:28,880 --> 00:10:35,130
the opencv function fine demography

00:10:31,850 --> 00:10:39,270
extract the Omega V transformation from

00:10:35,130 --> 00:10:42,030
to set of points and now we can create

00:10:39,270 --> 00:10:44,400
an array with the four corners of the

00:10:42,030 --> 00:10:48,300
picture and then transform the points

00:10:44,400 --> 00:10:51,150
with the on mog raphy matrix m this is

00:10:48,300 --> 00:10:54,540
done by the opencv functional

00:10:51,150 --> 00:10:56,400
perspective transform and we note that

00:10:54,540 --> 00:10:59,370
the computed tomography

00:10:56,400 --> 00:11:02,700
he is good because direct the red

00:10:59,370 --> 00:11:08,250
rectangle is over the picture in the in

00:11:02,700 --> 00:11:11,700
the image let's see that we can use this

00:11:08,250 --> 00:11:15,780
method to exclude the fourth positive in

00:11:11,700 --> 00:11:19,050
this case we have a lot of matches so if

00:11:15,780 --> 00:11:21,420
we induce to think that we have

00:11:19,050 --> 00:11:26,670
recognized the picture but the picture

00:11:21,420 --> 00:11:30,060
is not the same if we compute the

00:11:26,670 --> 00:11:33,360
position the picture positioning we can

00:11:30,060 --> 00:11:36,600
note that it is wrong in fact directly

00:11:33,360 --> 00:11:39,960
that the red rectangle does not fit with

00:11:36,600 --> 00:11:45,330
the real position of the picture so we

00:11:39,960 --> 00:11:50,700
we have a fourth positive and we we have

00:11:45,330 --> 00:11:55,920
found it because of the of the wrong

00:11:50,700 --> 00:12:00,510
position engl we tested image

00:11:55,920 --> 00:12:02,610
recognition also in 3d case and we can

00:12:00,510 --> 00:12:07,440
start with a picture of the Constantine

00:12:02,610 --> 00:12:11,100
arc taken from the right image

00:12:07,440 --> 00:12:14,010
recognition in in this case doesn't

00:12:11,100 --> 00:12:19,500
doesn't work because there are too many

00:12:14,010 --> 00:12:25,170
differences between the images so we

00:12:19,500 --> 00:12:27,470
have imagination works if in the library

00:12:25,170 --> 00:12:31,200
is included also an image of the arc

00:12:27,470 --> 00:12:34,200
from the right so we need at least three

00:12:31,200 --> 00:12:38,010
images for in the library from left

00:12:34,200 --> 00:12:40,380
right and center obviously we can apply

00:12:38,010 --> 00:12:44,040
this algorithm all in the case of front

00:12:40,380 --> 00:12:48,860
of objects very characteristic as arcs

00:12:44,040 --> 00:12:48,860
or churches therefore in touristy case

00:12:48,890 --> 00:12:56,340
we use the image recognition in a google

00:12:52,650 --> 00:12:58,890
glass application it is a tourist guide

00:12:56,340 --> 00:13:02,430
application that plays media contents

00:12:58,890 --> 00:13:05,990
based on localization for now it was

00:13:02,430 --> 00:13:09,250
tested in the archaeological area of the

00:13:05,990 --> 00:13:12,260
day palatino

00:13:09,250 --> 00:13:16,490
in this application match recognition is

00:13:12,260 --> 00:13:18,950
used for advanced location based on what

00:13:16,490 --> 00:13:25,010
you're watching and based on the plan

00:13:18,950 --> 00:13:28,370
amitri of the place and is is used to

00:13:25,010 --> 00:13:33,380
play advanced information the artwork

00:13:28,370 --> 00:13:48,200
you are watching thank you for your

00:13:33,380 --> 00:13:52,450
attention if you have any questions have

00:13:48,200 --> 00:13:52,450
you questions

00:14:00,710 --> 00:14:08,160
thank you I have a question do you do

00:14:04,560 --> 00:14:10,050
any other transformations to enhance the

00:14:08,160 --> 00:14:12,900
image quality before you do any

00:14:10,050 --> 00:14:14,790
processing like for example are you

00:14:12,900 --> 00:14:19,650
trying to detect if the image is blurry

00:14:14,790 --> 00:14:26,490
or there's like sparks from I know the

00:14:19,650 --> 00:14:29,280
street lights or something can you

00:14:26,490 --> 00:14:31,680
repeat again I don't need and eat let's

00:14:29,280 --> 00:14:35,070
say you take a you know a picture with

00:14:31,680 --> 00:14:38,340
your mobile phone and the camera isn't

00:14:35,070 --> 00:14:41,160
you know focused properly do you take

00:14:38,340 --> 00:14:43,560
the picture as it is or do you have

00:14:41,160 --> 00:14:45,750
other algorithms how to improve the

00:14:43,560 --> 00:14:51,200
picture so you can run their analysis

00:14:45,750 --> 00:14:55,320
now we we tested different cameras and

00:14:51,200 --> 00:14:59,310
every cameras as a calibration matrix

00:14:55,320 --> 00:15:09,360
that correct the distortion of the

00:14:59,310 --> 00:15:13,070
camera but the images must be focus the

00:15:09,360 --> 00:15:13,070
blurring is not allowed

00:15:24,740 --> 00:15:32,300
I in your example you had that red

00:15:29,480 --> 00:15:35,089
square and you could easily see that it

00:15:32,300 --> 00:15:42,529
was in some cases wrong but how does the

00:15:35,089 --> 00:15:50,180
computer see that it is wrong ok one

00:15:42,529 --> 00:15:52,220
moment with these computation we we can

00:15:50,180 --> 00:15:55,790
compute also the position of the

00:15:52,220 --> 00:16:00,080
observer and we have to be strapped

00:15:55,790 --> 00:16:06,800
rotation and translation from the amah

00:16:00,080 --> 00:16:12,940
graphy so opencv as also to useful

00:16:06,800 --> 00:16:15,980
functions one is a solve pnp that from

00:16:12,940 --> 00:16:20,740
antiques and these that are hammering

00:16:15,980 --> 00:16:24,320
Trinity parameters and the distorted

00:16:20,740 --> 00:16:28,730
image extract the rotation vectors and

00:16:24,320 --> 00:16:31,399
the translation vectors from these open

00:16:28,730 --> 00:16:36,829
syria's also implemented Rodriguez that

00:16:31,399 --> 00:16:39,880
is a an algorithm that extract from

00:16:36,829 --> 00:16:45,980
rotation vectors a rotation matrix and

00:16:39,880 --> 00:16:49,730
so we can compute the resolution in the

00:16:45,980 --> 00:16:55,149
system reference of the image of the a

00:16:49,730 --> 00:16:55,149
picture and then if the translation is

00:16:55,420 --> 00:17:05,660
is ok I Dec it's wrong so if the

00:17:02,000 --> 00:17:08,870
recession is back of the image for

00:17:05,660 --> 00:17:21,199
example we we can exclude that this off

00:17:08,870 --> 00:17:24,760
is 22 up or two down respect of the

00:17:21,199 --> 00:17:28,240
picture we can exclude these this match

00:17:24,760 --> 00:17:28,240
ok thank you

00:17:30,310 --> 00:17:33,880
more questions

00:17:38,540 --> 00:17:41,680
no question

00:17:52,380 --> 00:18:09,770
okay hi is this presentation available

00:17:54,780 --> 00:18:09,770
somewhere yes is a link ask see

00:18:14,580 --> 00:18:33,630
this without so you can find am I on my

00:18:31,000 --> 00:18:33,630
linking pages

00:18:37,900 --> 00:18:40,500
so

00:18:41,159 --> 00:18:47,159
one more question one more question and

00:18:44,429 --> 00:18:49,999
you say you did the library that is you

00:18:47,159 --> 00:18:54,029
only recognize a certain number of

00:18:49,999 --> 00:18:57,599
pictures right how large and is the

00:18:54,029 --> 00:19:01,859
library be for real time processing just

00:18:57,599 --> 00:19:08,690
10 pictures hundreds thousands we tested

00:19:01,859 --> 00:19:13,409
with 10 10 pictures in the library but

00:19:08,690 --> 00:19:20,479
we can we can improve this number but

00:19:13,409 --> 00:19:20,479
the time is the computational time is

00:19:21,470 --> 00:19:43,409
increase so we can paralyze the projects

00:19:26,009 --> 00:19:46,259
because our independent questions when

00:19:43,409 --> 00:19:48,239
you do image recognition I often see

00:19:46,259 --> 00:19:50,190
green lines going outside of the Red

00:19:48,239 --> 00:19:53,190
Square so basically you find the

00:19:50,190 --> 00:19:56,460
position of the image on your base image

00:19:53,190 --> 00:19:58,889
and why you use features extracted

00:19:56,460 --> 00:20:00,899
outside red area why don't you dream

00:19:58,889 --> 00:20:03,830
only two features that are inside the

00:20:00,899 --> 00:20:08,399
Red Square and when the feature are

00:20:03,830 --> 00:20:20,729
outside their picture why do you compare

00:20:08,399 --> 00:20:23,269
them because they okay II for example in

00:20:20,729 --> 00:20:23,269
this example

00:20:27,269 --> 00:20:36,490
okay so we can go to the arc example we

00:20:33,519 --> 00:20:39,159
can start anywhere and some features out

00:20:36,490 --> 00:20:42,730
like this you can see green green lines

00:20:39,159 --> 00:20:45,370
below the Red Square why you okay he he

00:20:42,730 --> 00:20:51,129
perform a feat obviously for the

00:20:45,370 --> 00:20:55,539
position that so the key points outside

00:20:51,129 --> 00:21:00,669
images so the wrong key points are are

00:20:55,539 --> 00:21:05,259
less than the exit key points so defeat

00:21:00,669 --> 00:21:15,809
exclude them probably with a chi-square

00:21:05,259 --> 00:21:15,809
test or more questions

00:21:16,880 --> 00:21:18,880
Oh

00:21:23,500 --> 00:21:27,910
I have a question about the demo that

00:21:26,110 --> 00:21:30,660
you did where do you do the processing

00:21:27,910 --> 00:21:35,830
on the glass or do you send it somewhere

00:21:30,660 --> 00:21:38,530
to next our own server to do it no I

00:21:35,830 --> 00:21:40,990
don't understand it excuse me so for in

00:21:38,530 --> 00:21:43,360
your application for the historical site

00:21:40,990 --> 00:21:46,180
you said that you display information

00:21:43,360 --> 00:21:48,520
depending on where you are in the side

00:21:46,180 --> 00:21:51,340
yeah and for that you need to figure out

00:21:48,520 --> 00:21:57,370
where you are so you're comparing to an

00:21:51,340 --> 00:22:02,920
image in your library II it its locality

00:21:57,370 --> 00:22:08,730
with the GPS and but the error for GPS

00:22:02,920 --> 00:22:13,360
is the order of meters and we we can

00:22:08,730 --> 00:22:16,030
find your localization through to

00:22:13,360 --> 00:22:19,150
extract what you are watching in the in

00:22:16,030 --> 00:22:24,930
the in the blaze so we can pre-select

00:22:19,150 --> 00:22:31,720
the the object that that you're watching

00:22:24,930 --> 00:22:34,300
the near the newest denier object for

00:22:31,720 --> 00:22:36,160
the GPS and you do this the processing

00:22:34,300 --> 00:22:39,820
that you have in your image right you

00:22:36,160 --> 00:22:43,720
compare what you see with the library is

00:22:39,820 --> 00:22:46,770
that yeah on the glass no its server

00:22:43,720 --> 00:22:46,770
side okay

00:22:50,770 --> 00:22:54,500
when you're converting to grayscale

00:22:53,330 --> 00:22:56,510
because obviously of potentially losing

00:22:54,500 --> 00:22:58,970
detailed information are you just doing

00:22:56,510 --> 00:23:01,940
a flat brace gullkin computation or are

00:22:58,970 --> 00:23:04,730
you doing Oh conversion are you doing

00:23:01,940 --> 00:23:08,180
any kind of optimization of the process

00:23:04,730 --> 00:23:11,390
of moving into grayscale we use the surf

00:23:08,180 --> 00:23:15,440
algorithmic because is is more

00:23:11,390 --> 00:23:17,090
computationally performant and I mean

00:23:15,440 --> 00:23:19,280
before before you are running this earth

00:23:17,090 --> 00:23:22,640
algorithm you're converting a color

00:23:19,280 --> 00:23:24,440
image to grayscale already set at least

00:23:22,640 --> 00:23:26,960
you suggest I thought that was what

00:23:24,440 --> 00:23:29,140
you're doing first are you doing just a

00:23:26,960 --> 00:23:32,440
convert is there any kind of special

00:23:29,140 --> 00:23:34,820
grace Kelling you're doing or is it just

00:23:32,440 --> 00:23:37,520
the conventional open see it does evan

00:23:34,820 --> 00:23:47,050
see we do any kind of a special but

00:23:37,520 --> 00:23:47,050
which are justice ok more questions

00:23:50,580 --> 00:23:53,750
no questions

00:23:56,039 --> 00:24:00,049

YouTube URL: https://www.youtube.com/watch?v=zGVl9S5Ylpk


