Title: Teodor Dima - Use Python to process 12mil events per minute and still keep it simple (Talk)
Publication date: 2015-08-08
Playlist: EuroPython 2015
Description: 
	Teodor Dima - Use Python to process 12mil events per minute and still keep it simple (Talk)
[EuroPython 2015]
[21 July 2015]
[Bilbao, Euskadi, Spain]

Creating a large-scale event processing system can be a daunting task.
Especially if you want it “stupid simple” and wrapped around each
client’s needs. We built a straightforward solution for this using
Python 3 and other open-source tools.

Main issues to solve for a system that needs to be both performant and scalable:

 - handling a throughput of 1 million events per minute in a 4 cores
AWS instance;

 - following the principle of least astonishment;

 - data aggregation and how Python's standard libraries and data
structures can help;

 - failsafe and profiling mechanisms that can be applied to any Linux
service in production;

 - addressing unexpected behaviors of Python’s Standard Library; like
reading from a file while it is written;

 - tackling a sudden spectacular cloud instance failure;

The alternative to this system would be to adopt existing technology
stacks that might be too general, add more complexity, bloat, costs
and which need extensive work to solve your specific problem.
Moreover, our approach resulted in over 85% drop on hardware
utilisation.

[Context: Production Software – II (where good coding reduces the client’s bill)][1]

[1]: https://eastvisionsystems.com/production-software-part-ii-good-coding-reduces-clients-bill/
Captions: 
	00:00:00,199 --> 00:00:05,670
so hi my name is Todd oedema I am a

00:00:04,049 --> 00:00:08,309
developer at this vision systems a

00:00:05,670 --> 00:00:10,080
company from Romania and we do a lot of

00:00:08,309 --> 00:00:12,599
things there but we mainly concern

00:00:10,080 --> 00:00:15,299
ourselves with at tech big data video

00:00:12,599 --> 00:00:16,830
technologies and real-time bidding for

00:00:15,299 --> 00:00:20,010
those that do not know what that means

00:00:16,830 --> 00:00:22,619
that really means this means that we

00:00:20,010 --> 00:00:24,630
must handle a lot of user events every

00:00:22,619 --> 00:00:28,170
second which must be continuously

00:00:24,630 --> 00:00:30,150
persisted into the database so what is

00:00:28,170 --> 00:00:31,880
an advanced simply puts an event is

00:00:30,150 --> 00:00:35,520
represented by a simple get request

00:00:31,880 --> 00:00:38,790
which is sent by a user from a browser

00:00:35,520 --> 00:00:41,700
which views advertising and that is sent

00:00:38,790 --> 00:00:43,920
to the event processing system that must

00:00:41,700 --> 00:00:46,350
analyze the data and then push it into

00:00:43,920 --> 00:00:49,050
the database the amount of data that can

00:00:46,350 --> 00:00:51,920
be generated by a single user is quite

00:00:49,050 --> 00:00:56,030
big a single user viewing a single ad

00:00:51,920 --> 00:01:00,120
can see concerned about at least about

00:00:56,030 --> 00:01:02,430
10 events of data these events are not

00:01:00,120 --> 00:01:05,489
sent all at the same time which means

00:01:02,430 --> 00:01:07,260
that the user must must keep a

00:01:05,489 --> 00:01:09,299
continuous a connection with the server

00:01:07,260 --> 00:01:15,240
and that means that you have a lot of

00:01:09,299 --> 00:01:17,820
users which send a lot of events from a

00:01:15,240 --> 00:01:20,700
lot of concurrent clients this problem

00:01:17,820 --> 00:01:24,360
has been seen long before this talk it

00:01:20,700 --> 00:01:26,909
is called see 10k and it basically means

00:01:24,360 --> 00:01:29,790
how can you handle 10,000 concurrent

00:01:26,909 --> 00:01:31,079
connections on the same machine during

00:01:29,790 --> 00:01:33,299
my time in the company we had our

00:01:31,079 --> 00:01:36,659
averages of about 12 million requests

00:01:33,299 --> 00:01:38,970
per minute that means about 200,000

00:01:36,659 --> 00:01:42,030
requests per second that must be handled

00:01:38,970 --> 00:01:44,130
by a single server cluster so the

00:01:42,030 --> 00:01:46,619
question is how can you handle such a

00:01:44,130 --> 00:01:48,659
traffic and moreover how can you handle

00:01:46,619 --> 00:01:52,020
such a traffic by using as little

00:01:48,659 --> 00:01:54,720
resources as possible so the nite

00:01:52,020 --> 00:01:57,119
solution will be to implement a system

00:01:54,720 --> 00:02:00,990
that responds to each request and since

00:01:57,119 --> 00:02:03,689
it's directly to the database this works

00:02:00,990 --> 00:02:07,530
for a small amount of data and is quite

00:02:03,689 --> 00:02:09,640
easy and fast to implement but it's

00:02:07,530 --> 00:02:14,140
ultimately unmaintained but

00:02:09,640 --> 00:02:17,319
and consumes a lot of resources another

00:02:14,140 --> 00:02:20,260
alternative solution would be to use the

00:02:17,319 --> 00:02:24,340
Apache trio cough cough storm or and zoo

00:02:20,260 --> 00:02:27,580
keeper or some alternatives that however

00:02:24,340 --> 00:02:31,240
if you want to configure and to tune the

00:02:27,580 --> 00:02:34,120
systems as good as you can into a

00:02:31,240 --> 00:02:38,290
coherent whole that takes time and it's

00:02:34,120 --> 00:02:41,050
often non pythonic although some some

00:02:38,290 --> 00:02:44,110
work has been put to make it a lot more

00:02:41,050 --> 00:02:49,060
pythonic by the ones at whether people

00:02:44,110 --> 00:02:51,070
add parsley so kudos to them initially

00:02:49,060 --> 00:02:54,280
when we had to implement such a system

00:02:51,070 --> 00:02:57,910
we had to ship it and that meant that we

00:02:54,280 --> 00:03:02,830
implemented a simple solution 91 which

00:02:57,910 --> 00:03:07,450
handle streams of events and then simply

00:03:02,830 --> 00:03:09,580
sent them to the database in order to

00:03:07,450 --> 00:03:11,050
check for data consist consistency

00:03:09,580 --> 00:03:13,030
though in order to be sure that we

00:03:11,050 --> 00:03:15,400
haven't dropped any event that reached

00:03:13,030 --> 00:03:17,769
the server but was not inserted into the

00:03:15,400 --> 00:03:20,799
database we checked with the access logs

00:03:17,769 --> 00:03:22,600
of the web server and we checked if data

00:03:20,799 --> 00:03:24,700
data corresponds with the data from the

00:03:22,600 --> 00:03:26,590
database because all the data that you

00:03:24,700 --> 00:03:29,260
need all the events are there in the

00:03:26,590 --> 00:03:31,900
access log so this led to a simple idea

00:03:29,260 --> 00:03:35,200
why not use the access log as a simple q

00:03:31,900 --> 00:03:37,000
for an event processing system the idea

00:03:35,200 --> 00:03:39,700
was that when the requests reached the

00:03:37,000 --> 00:03:42,430
machine they are received by our system

00:03:39,700 --> 00:03:44,500
by the ingenious web server the nginx

00:03:42,430 --> 00:03:47,890
web server solves the city and K problem

00:03:44,500 --> 00:03:50,650
so that would solve a lot of problems so

00:03:47,890 --> 00:03:53,560
between the access logs and the database

00:03:50,650 --> 00:03:56,769
there has to be another service which

00:03:53,560 --> 00:03:59,440
would take the data analyze it transform

00:03:56,769 --> 00:04:00,940
it and then push it into the database so

00:03:59,440 --> 00:04:04,180
we began to think about the

00:04:00,940 --> 00:04:06,280
implementations of such a project and if

00:04:04,180 --> 00:04:08,709
it would be resilient and feasible

00:04:06,280 --> 00:04:11,019
enough for us to do it after some

00:04:08,709 --> 00:04:12,910
prototypes and some new ideas we came up

00:04:11,019 --> 00:04:15,489
with a green structure and we can move

00:04:12,910 --> 00:04:20,380
this with a service that we called log

00:04:15,489 --> 00:04:22,240
bunker now this is a data flow diagram

00:04:20,380 --> 00:04:24,639
which shows a simplified schema

00:04:22,240 --> 00:04:27,509
data flow through a single virtual

00:04:24,639 --> 00:04:30,970
machine in the cloud as I said a single

00:04:27,509 --> 00:04:34,240
HTML request is sent by the user by a

00:04:30,970 --> 00:04:38,530
browser through a load balancer and then

00:04:34,240 --> 00:04:40,509
to the engine X web server in order in

00:04:38,530 --> 00:04:42,669
order to ensure that the data was easily

00:04:40,509 --> 00:04:46,180
swappable between the virtual machines

00:04:42,669 --> 00:04:50,949
we used the amazon web servers EBS

00:04:46,180 --> 00:04:53,770
elastic block a blob star service to

00:04:50,949 --> 00:04:56,470
sort the access log data then the access

00:04:53,770 --> 00:04:59,349
log was read by our service by log

00:04:56,470 --> 00:05:02,319
bunker and then it is absurd into the

00:04:59,349 --> 00:05:04,990
database inside the log bunker service

00:05:02,319 --> 00:05:09,550
there are actually three processes that

00:05:04,990 --> 00:05:11,110
work at the same time so two of these

00:05:09,550 --> 00:05:15,150
processes are the parser and the

00:05:11,110 --> 00:05:17,680
observer the parser reads the access log

00:05:15,150 --> 00:05:20,620
continuously just like a tale a unique

00:05:17,680 --> 00:05:22,780
style and then analyzes these events and

00:05:20,620 --> 00:05:25,030
cashes them into an internal cash

00:05:22,780 --> 00:05:26,949
structure this internal cash structure

00:05:25,030 --> 00:05:30,430
is using the Python standard types

00:05:26,949 --> 00:05:34,870
they're actually quite beautiful to use

00:05:30,430 --> 00:05:37,389
and very simple to use after it has

00:05:34,870 --> 00:05:40,810
cashed this data for a fixed period of

00:05:37,389 --> 00:05:42,789
seconds configurable of course it done

00:05:40,810 --> 00:05:44,860
takes this data and pushes it into the

00:05:42,789 --> 00:05:48,310
multi processing queue that makes the

00:05:44,860 --> 00:05:52,240
connection with the observer the episode

00:05:48,310 --> 00:05:54,699
are then pops the queue takes the data

00:05:52,240 --> 00:05:58,479
from that and then pushes it in the end

00:05:54,699 --> 00:06:00,639
of the database besides pushing it into

00:05:58,479 --> 00:06:03,699
the database the absurd actually logs

00:06:00,639 --> 00:06:07,930
into a special file into a we call it

00:06:03,699 --> 00:06:10,090
the bin log file every event that it has

00:06:07,930 --> 00:06:13,030
actually pushed into the database and

00:06:10,090 --> 00:06:14,830
with the offset of the access log so

00:06:13,030 --> 00:06:17,110
that in the event of a crash in the

00:06:14,830 --> 00:06:19,810
event that you actually want to reboot

00:06:17,110 --> 00:06:22,449
the system to restart it the service

00:06:19,810 --> 00:06:23,979
would then know which point was the last

00:06:22,449 --> 00:06:26,319
point that was introduced into the

00:06:23,979 --> 00:06:30,849
database and could restart from that

00:06:26,319 --> 00:06:34,750
point on the third and final process is

00:06:30,849 --> 00:06:35,520
the admin process this process checks

00:06:34,750 --> 00:06:38,129
period

00:06:35,520 --> 00:06:40,650
if the other two processes are active if

00:06:38,129 --> 00:06:43,889
they are not it shuts down the whole

00:06:40,650 --> 00:06:46,949
system now why did we do that why didn't

00:06:43,889 --> 00:06:50,039
we just reboot the whole service well

00:06:46,949 --> 00:06:52,259
there is a tiny probability that if you

00:06:50,039 --> 00:06:54,150
have something that can crash a process

00:06:52,259 --> 00:06:57,680
that can crash in the data processing

00:06:54,150 --> 00:07:00,240
service then you may have corrupted data

00:06:57,680 --> 00:07:03,629
even have data that does not reach the

00:07:00,240 --> 00:07:06,780
database but it is there somewhere in

00:07:03,629 --> 00:07:08,970
the system then that's bad but if you

00:07:06,780 --> 00:07:12,569
insert corrupt data into the database

00:07:08,970 --> 00:07:15,690
then that's really much worse so we try

00:07:12,569 --> 00:07:17,430
to avoid that at all costs another

00:07:15,690 --> 00:07:20,400
function of the administration process

00:07:17,430 --> 00:07:24,210
is is to have sync the data files the

00:07:20,400 --> 00:07:25,800
access file and the bin log file the app

00:07:24,210 --> 00:07:28,080
sync function is a function which

00:07:25,800 --> 00:07:31,710
synchronizes the virtual memory of a

00:07:28,080 --> 00:07:33,719
file with the actual disk content of

00:07:31,710 --> 00:07:36,240
that file which means that the file is

00:07:33,719 --> 00:07:38,849
persisted after you make the obscene

00:07:36,240 --> 00:07:41,120
call that is extremely important if you

00:07:38,849 --> 00:07:43,620
have if you want data persistence and

00:07:41,120 --> 00:07:45,210
the last important function is the

00:07:43,620 --> 00:07:48,599
service status data at a configurable

00:07:45,210 --> 00:07:51,990
port that was done with a single simple

00:07:48,599 --> 00:07:55,139
protocol it just accepts every single

00:07:51,990 --> 00:07:57,919
request that comes and then serves the

00:07:55,139 --> 00:08:01,069
JSON status data this status data is

00:07:57,919 --> 00:08:04,889
collected from the other two processes

00:08:01,069 --> 00:08:06,509
through some shared virtual memory that

00:08:04,889 --> 00:08:12,870
is controlled by a multi processing

00:08:06,509 --> 00:08:15,270
block a simple read write lock so one of

00:08:12,870 --> 00:08:18,870
the first issues that we thought about

00:08:15,270 --> 00:08:21,870
was how stable with the service be when

00:08:18,870 --> 00:08:24,139
it's the use the access log so turns out

00:08:21,870 --> 00:08:27,630
it's very easy and completely stable

00:08:24,139 --> 00:08:30,419
it's very easy to implement but there is

00:08:27,630 --> 00:08:32,760
one teensy tiny problem the offsets are

00:08:30,419 --> 00:08:35,820
hard to calculate efficiently and like I

00:08:32,760 --> 00:08:40,979
said we need those offsets to put them

00:08:35,820 --> 00:08:43,289
in the bin log file so so in order to do

00:08:40,979 --> 00:08:45,360
that using buffer to text files is

00:08:43,289 --> 00:08:47,070
almost useless because if you use buffer

00:08:45,360 --> 00:08:48,209
text files you cannot know the exact

00:08:47,070 --> 00:08:51,569
opposite of a single line

00:08:48,209 --> 00:08:53,730
text unbuffered to text files are really

00:08:51,569 --> 00:08:55,730
slow and could not help us so it's

00:08:53,730 --> 00:08:59,249
actually easier to open the file in

00:08:55,730 --> 00:09:01,139
binary white mode just read a number of

00:08:59,249 --> 00:09:03,149
parts add the number of bytes that are

00:09:01,139 --> 00:09:08,249
contained in a lie and there you go you

00:09:03,149 --> 00:09:09,749
have the offsets so between the parser

00:09:08,249 --> 00:09:12,809
and absorption process we have a

00:09:09,749 --> 00:09:15,179
multiprocessing Q the Q is using a pipe

00:09:12,809 --> 00:09:17,279
in the background a unix five widget and

00:09:15,179 --> 00:09:19,199
that pipe never corrupt the data we

00:09:17,279 --> 00:09:20,999
never had problems with it but there is

00:09:19,199 --> 00:09:23,819
a problem with the data transfer speed

00:09:20,999 --> 00:09:26,670
because when you actually insert some

00:09:23,819 --> 00:09:29,089
data into the queue it actually spawns a

00:09:26,670 --> 00:09:31,529
thread which then inserts the data

00:09:29,089 --> 00:09:35,009
gradually from the buffer is internal

00:09:31,529 --> 00:09:37,319
buffer into the pipe if this thread does

00:09:35,009 --> 00:09:39,059
not have the gill you are going going to

00:09:37,319 --> 00:09:41,129
have data in the buffer which is not

00:09:39,059 --> 00:09:42,990
actually inserted in the pipe so the

00:09:41,129 --> 00:09:45,689
connection between the parser and the

00:09:42,990 --> 00:09:48,149
observer is broken there are ways to

00:09:45,689 --> 00:09:52,649
minimize damages and I will talk about

00:09:48,149 --> 00:09:54,779
them in a later slide so how could a

00:09:52,649 --> 00:09:58,679
catastrophic crash the handle securely

00:09:54,779 --> 00:10:01,049
and efficiently where if that happens if

00:09:58,679 --> 00:10:03,689
you have a catastrophic crash then there

00:10:01,049 --> 00:10:05,639
are two files essential files which can

00:10:03,689 --> 00:10:07,980
be corrupted or incomplete access log in

00:10:05,639 --> 00:10:10,799
the bin log and you can manage that

00:10:07,980 --> 00:10:13,920
somewhat by just kept sinking them as

00:10:10,799 --> 00:10:16,410
often as you can so then if you result

00:10:13,920 --> 00:10:18,869
the machine or if you have a crash then

00:10:16,410 --> 00:10:20,879
you can recover the data you can just go

00:10:18,869 --> 00:10:25,649
on and then insert the data that you

00:10:20,879 --> 00:10:28,439
have there ok so this Python fast enough

00:10:25,649 --> 00:10:30,600
to ingest this data well yes actually

00:10:28,439 --> 00:10:32,579
with some performance optimizations C

00:10:30,600 --> 00:10:34,910
Python could get about twenty thousand

00:10:32,579 --> 00:10:38,040
requests per second on the same machine

00:10:34,910 --> 00:10:39,689
honesty forex arch amazon web service

00:10:38,040 --> 00:10:42,990
were chill ma xin that means that you

00:10:39,689 --> 00:10:45,689
have this processing power on a machine

00:10:42,990 --> 00:10:48,689
with four virtual cores and eight

00:10:45,689 --> 00:10:50,369
gigabytes of ram however after we

00:10:48,689 --> 00:10:51,869
inserted it into production we re

00:10:50,369 --> 00:10:58,139
implemented the data processing system

00:10:51,869 --> 00:11:01,740
in CIF oh that was quite easy to do it

00:10:58,139 --> 00:11:03,150
just took a week excluding texting and

00:11:01,740 --> 00:11:05,760
without any price you have an experience

00:11:03,150 --> 00:11:11,340
and it's doubled the performance so

00:11:05,760 --> 00:11:13,860
you're effing way to go there is another

00:11:11,340 --> 00:11:17,330
problem though like I said we called

00:11:13,860 --> 00:11:21,330
obscene on the two essential data files

00:11:17,330 --> 00:11:24,780
and we use the network file storage

00:11:21,330 --> 00:11:26,850
offered by AWS the f sync does affect

00:11:24,780 --> 00:11:29,760
the performance of the read from from

00:11:26,850 --> 00:11:33,150
the files and we had the problem at some

00:11:29,760 --> 00:11:36,030
points that periodically once every two

00:11:33,150 --> 00:11:38,070
days or so the network lacked in the

00:11:36,030 --> 00:11:42,360
amazon web service and that meant that

00:11:38,070 --> 00:11:45,690
the f sync call took from beneath 0.1

00:11:42,360 --> 00:11:48,300
seconds to almost 25 seconds which meant

00:11:45,690 --> 00:11:51,240
that the service but was blocked at that

00:11:48,300 --> 00:11:57,140
time but it was actually pretty rare and

00:11:51,240 --> 00:12:00,210
it did not affect the system too much so

00:11:57,140 --> 00:12:01,530
during one of the testing phases we have

00:12:00,210 --> 00:12:05,040
several strange behavior with the

00:12:01,530 --> 00:12:06,840
observer the parcel was reading

00:12:05,040 --> 00:12:10,970
continuously from Jax's log file it

00:12:06,840 --> 00:12:14,280
cached data happily the cash was full

00:12:10,970 --> 00:12:16,620
and the observer was not having the same

00:12:14,280 --> 00:12:19,320
efficiency as it would have under normal

00:12:16,620 --> 00:12:21,090
conditions so what what's happened there

00:12:19,320 --> 00:12:23,910
was at the parser process the parser

00:12:21,090 --> 00:12:26,160
thread was starving the feeder thread

00:12:23,910 --> 00:12:28,740
was starving the thread that would

00:12:26,160 --> 00:12:30,860
actually that had the purpose of taking

00:12:28,740 --> 00:12:34,950
the data and then inserted into the pipe

00:12:30,860 --> 00:12:40,320
the only way we could manage until now

00:12:34,950 --> 00:12:42,990
so to avoid this problem was to force

00:12:40,320 --> 00:12:47,070
the parser thread to sleep periodically

00:12:42,990 --> 00:12:53,250
it could not be fully fixed and it's

00:12:47,070 --> 00:12:55,560
just sort of problem right then so how

00:12:53,250 --> 00:12:58,860
can you maintain how easy is to maintain

00:12:55,560 --> 00:13:01,440
such a system a single machine that

00:12:58,860 --> 00:13:03,810
answers to this amount of requests like

00:13:01,440 --> 00:13:06,570
I said twenty twenty thousand requests

00:13:03,810 --> 00:13:09,060
per second contains only two essential

00:13:06,570 --> 00:13:10,890
processes the engineer service which

00:13:09,060 --> 00:13:14,360
takes the requests and just needs a

00:13:10,890 --> 00:13:16,519
configuration file and the Python demon

00:13:14,360 --> 00:13:19,220
banker which also needs a single

00:13:16,519 --> 00:13:22,100
configuration file those machines are

00:13:19,220 --> 00:13:23,779
small they only contain four cores and

00:13:22,100 --> 00:13:28,730
they are quite chief they are commodity

00:13:23,779 --> 00:13:30,980
hardware and they you can serve with a

00:13:28,730 --> 00:13:34,029
server cluster 12 million requests per

00:13:30,980 --> 00:13:36,470
minute by just using 15 virtual machines

00:13:34,029 --> 00:13:37,910
that means that there is no single point

00:13:36,470 --> 00:13:40,430
of failure when you have machines that

00:13:37,910 --> 00:13:43,730
are be small or you have enough

00:13:40,430 --> 00:13:47,510
processing power to obtain such a such

00:13:43,730 --> 00:13:49,519
an improvement and if you lose a machine

00:13:47,510 --> 00:13:53,180
then the system will not be affected as

00:13:49,519 --> 00:13:56,779
much as let's say if you lose a gigantic

00:13:53,180 --> 00:14:02,120
32 cores virtual machine with God knows

00:13:56,779 --> 00:14:04,100
how many gigabytes of RAM another thing

00:14:02,120 --> 00:14:05,930
to say about maintenance is that these

00:14:04,100 --> 00:14:08,540
machines are not throttle to a hundred

00:14:05,930 --> 00:14:10,519
percent these machines are throttle at

00:14:08,540 --> 00:14:14,000
the maximum of fifty percent in a normal

00:14:10,519 --> 00:14:17,269
situation and let's say seventy-five

00:14:14,000 --> 00:14:19,820
percent on Peaks this reduces the

00:14:17,269 --> 00:14:21,949
possibility of low availability and

00:14:19,820 --> 00:14:24,890
hardware failure because if you throttle

00:14:21,949 --> 00:14:29,029
a machine at one hundred percent then

00:14:24,890 --> 00:14:30,860
you will have some hardware failures and

00:14:29,029 --> 00:14:33,709
even if the pigs reach a hundred percent

00:14:30,860 --> 00:14:35,060
of system capacity then you have a queue

00:14:33,709 --> 00:14:38,570
you have an event queue that is

00:14:35,060 --> 00:14:41,660
represented by the access log file that

00:14:38,570 --> 00:14:44,300
access log file we just keep be appended

00:14:41,660 --> 00:14:45,740
to buy the engine Nix web server the

00:14:44,300 --> 00:14:48,620
engine X web server will happily

00:14:45,740 --> 00:14:52,310
continue to serve the requests and if

00:14:48,620 --> 00:14:56,000
the patent process does not does not

00:14:52,310 --> 00:15:00,019
cannot pace with the ingenious web

00:14:56,000 --> 00:15:02,480
server it doesn't really have to it will

00:15:00,019 --> 00:15:07,190
have a delay from from the point when

00:15:02,480 --> 00:15:09,380
the data reaches the virtual machine to

00:15:07,190 --> 00:15:11,449
when it is inserted into the database by

00:15:09,380 --> 00:15:13,640
about let's say a couple of minutes but

00:15:11,449 --> 00:15:16,279
it will it will not be critical you will

00:15:13,640 --> 00:15:19,579
never lose data you will just continue

00:15:16,279 --> 00:15:23,360
pushing it into the database when the

00:15:19,579 --> 00:15:25,660
peak is gone and the same thing applies

00:15:23,360 --> 00:15:27,889
to the database connection the database

00:15:25,660 --> 00:15:32,269
can actually lag quite

00:15:27,889 --> 00:15:33,889
much and if you have at some point in

00:15:32,269 --> 00:15:35,779
time some problems with the database

00:15:33,889 --> 00:15:38,689
let's say that you have a cluster of

00:15:35,779 --> 00:15:42,109
database servers and one of them or

00:15:38,689 --> 00:15:45,410
several of them crash then you will have

00:15:42,109 --> 00:15:48,410
less right performance on them and if

00:15:45,410 --> 00:15:51,739
the database connection lags then the

00:15:48,410 --> 00:15:54,199
observer will just lag in getting the

00:15:51,739 --> 00:15:57,259
data from the queue but what that means

00:15:54,199 --> 00:16:00,230
is that the data will actually add

00:15:57,259 --> 00:16:03,759
itself into the cache the cash will just

00:16:00,230 --> 00:16:06,649
add the data more and more and more and

00:16:03,759 --> 00:16:08,809
after the database is restored to its

00:16:06,649 --> 00:16:11,029
full power it will just continue to

00:16:08,809 --> 00:16:17,739
insert it into the database and it will

00:16:11,029 --> 00:16:20,559
just reach its hundred percent okay so

00:16:17,739 --> 00:16:24,279
that was it thank you for a retention

00:16:20,559 --> 00:16:24,279
there are questions please

00:16:38,940 --> 00:16:45,820
hi one question well actually have

00:16:43,930 --> 00:16:48,640
probably too but the first one is why

00:16:45,820 --> 00:16:51,970
between your posture and your upset are

00:16:48,640 --> 00:16:54,280
you don't use rabbitmq or any message

00:16:51,970 --> 00:16:57,400
queuing like I don't know zeromq even

00:16:54,280 --> 00:16:59,950
would solve your problem of Q that you

00:16:57,400 --> 00:17:03,280
have in the multiple sensing a bit in

00:16:59,950 --> 00:17:04,990
the power n dubz shorter yeah well

00:17:03,280 --> 00:17:07,510
firstly because it was easier to

00:17:04,990 --> 00:17:09,730
implement and because it did not affect

00:17:07,510 --> 00:17:13,420
the performance as much as we thought so

00:17:09,730 --> 00:17:16,060
even if even with that performance

00:17:13,420 --> 00:17:18,160
penalty he still reached 20,000 requests

00:17:16,060 --> 00:17:20,800
per second twenty thousand requests oh

00:17:18,160 --> 00:17:24,130
the limit of the engine X web server in

00:17:20,800 --> 00:17:27,610
an Amazon see Forex large instance is

00:17:24,130 --> 00:17:30,310
about 20 to 24 acres per second and we

00:17:27,610 --> 00:17:32,650
did not reach that limit and like I said

00:17:30,310 --> 00:17:35,560
we kept our servers to serve about

00:17:32,650 --> 00:17:37,510
12,000 3 k's per second so that we kept

00:17:35,560 --> 00:17:40,480
load the possibility of hardware failure

00:17:37,510 --> 00:17:41,590
so we could have implemented a message

00:17:40,480 --> 00:17:43,860
giving service we thought about

00:17:41,590 --> 00:17:46,960
implementing something with lettuce and

00:17:43,860 --> 00:17:49,000
we may implement that in the future we

00:17:46,960 --> 00:17:53,020
don't know but for now it's sitting

00:17:49,000 --> 00:17:55,450
what's quite a bit well just do and Q is

00:17:53,020 --> 00:17:58,920
a five lines and it's all your problem

00:17:55,450 --> 00:17:58,920
that's it ah okay

00:18:13,489 --> 00:18:19,379
thanks hey um I just want to ask about

00:18:16,109 --> 00:18:21,779
you talked about how even if your log

00:18:19,379 --> 00:18:22,769
bunk up process crashes or doesn't work

00:18:21,779 --> 00:18:24,749
you're not going to lose any logs

00:18:22,769 --> 00:18:26,969
because they're stored in the access log

00:18:24,749 --> 00:18:28,799
and it has a different engine X on each

00:18:26,969 --> 00:18:31,139
virtual machine you have to have a

00:18:28,799 --> 00:18:33,089
virtual machine crash will go down what

00:18:31,139 --> 00:18:34,559
happens to the log messages that are in

00:18:33,089 --> 00:18:35,820
the access log on that machine which

00:18:34,559 --> 00:18:38,369
have yet to be stored in the database

00:18:35,820 --> 00:18:40,229
they're gone they're not gone forever

00:18:38,369 --> 00:18:42,539
because they are kept in the access log

00:18:40,229 --> 00:18:46,529
and that access log will be kept on the

00:18:42,539 --> 00:18:49,049
ABS network partition so what that means

00:18:46,529 --> 00:18:50,729
is that temporarily while the virtual

00:18:49,049 --> 00:18:53,580
machine is down and you cannot have

00:18:50,729 --> 00:18:55,499
access to dvb-s network protection you

00:18:53,580 --> 00:19:00,719
will have some data that does not reach

00:18:55,499 --> 00:19:03,989
the database but if your support team is

00:19:00,719 --> 00:19:05,700
ready then it will reconnect with a TBS

00:19:03,989 --> 00:19:08,099
system to another log banker instance

00:19:05,700 --> 00:19:11,659
maybe order another level and just three

00:19:08,099 --> 00:19:11,659
insert it thanks

00:19:19,870 --> 00:19:26,590
I knew I had a second question why are

00:19:24,280 --> 00:19:29,980
you not using things like log stash or

00:19:26,590 --> 00:19:32,940
equivalent to do your passing logstash

00:19:29,980 --> 00:19:36,040
between what basically to pass your rugs

00:19:32,940 --> 00:19:37,720
what you do manually by you're

00:19:36,040 --> 00:19:39,970
implementing what those utilities do

00:19:37,720 --> 00:19:42,580
just pick up the information directly

00:19:39,970 --> 00:19:45,070
from the log why are dedicated utilities

00:19:42,580 --> 00:19:47,740
mean there are many already existing

00:19:45,070 --> 00:19:49,780
solutions to do that precise job i think

00:19:47,740 --> 00:19:52,059
i mean i'm not expert in knox dash but

00:19:49,780 --> 00:19:54,730
i've heard many things about logging and

00:19:52,059 --> 00:19:56,559
log parsing and whatnot and to me it

00:19:54,730 --> 00:19:57,850
seems that log stash or something

00:19:56,559 --> 00:20:00,550
equivalent would be a very good solution

00:19:57,850 --> 00:20:03,070
to extract information meaningful inside

00:20:00,550 --> 00:20:04,809
information from your logs because it's

00:20:03,070 --> 00:20:08,140
a structured information and probably

00:20:04,809 --> 00:20:10,630
you can inform logstash to read how to

00:20:08,140 --> 00:20:12,400
read text basically so that means that

00:20:10,630 --> 00:20:14,020
we'll have a crosser absorbers which

00:20:12,400 --> 00:20:17,350
have the engine X on each one of the

00:20:14,020 --> 00:20:19,510
machines and it's the same thing is just

00:20:17,350 --> 00:20:21,160
you you read your access look somehow

00:20:19,510 --> 00:20:22,570
and just instead of we're implementing

00:20:21,160 --> 00:20:24,640
something in bytes and you use it on

00:20:22,570 --> 00:20:26,140
other utility that is well maintained by

00:20:24,640 --> 00:20:27,610
professors but that means that it will

00:20:26,140 --> 00:20:29,950
be redirected to another crossref

00:20:27,610 --> 00:20:32,290
service which actually has lost a Shanda

00:20:29,950 --> 00:20:33,970
right well you can install I guess log

00:20:32,290 --> 00:20:38,590
stash on the medicine machines as where

00:20:33,970 --> 00:20:42,040
you run your log bunker yes that is sure

00:20:38,590 --> 00:20:45,540
however i am not i'm not really sure how

00:20:42,040 --> 00:20:48,340
is the performance on on those services

00:20:45,540 --> 00:20:50,050
we actually could implement this really

00:20:48,340 --> 00:20:51,550
quickly and had a really good

00:20:50,050 --> 00:20:56,200
performance and we thought that we could

00:20:51,550 --> 00:20:59,490
use this so i don't know ultimately i

00:20:56,200 --> 00:20:59,490
don't know how the performance would be

00:20:59,910 --> 00:21:03,000
thank you

00:21:03,610 --> 00:21:10,260
any other questions one more question

00:21:06,600 --> 00:21:10,260

YouTube URL: https://www.youtube.com/watch?v=tw1IaPc6Mxg


