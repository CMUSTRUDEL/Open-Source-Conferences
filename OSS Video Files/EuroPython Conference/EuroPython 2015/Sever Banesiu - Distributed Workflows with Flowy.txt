Title: Sever Banesiu - Distributed Workflows with Flowy
Publication date: 2015-08-08
Playlist: EuroPython 2015
Description: 
	Sever Banesiu - Distributed Workflows with Flowy
[EuroPython 2015]
[20 July 2015]
[Bilbao, Euskadi, Spain]

This presentation introduces Flowy, a library for building and running
distributed, asynchronous workflows built on top of different backends
(such as Amazonâ€™s SWF). Flowy deals away with the spaghetti code that
often crops up from orchestrating complex workflows. It is ideal for
applications that do multi-phased batch processing, media encoding,
long-running tasks, and/or background processing.

We'll start by discussing Flowy's unique execution model and see how
different execution topologies can be implemented on top of it. During
the talk we'll run and visualize workflows using a local backend.
We'll then take a look at what it takes to scale beyond a single
machine by using an external service like SWF.
Captions: 
	00:00:01,730 --> 00:00:07,950
hello everyone thank you for joining me

00:00:04,890 --> 00:00:12,590
my name is server and this presentation

00:00:07,950 --> 00:00:16,260
is about the library that I'm working on

00:00:12,590 --> 00:00:19,289
and which makes it easy to model and run

00:00:16,260 --> 00:00:21,539
distributed workflows there will be a

00:00:19,289 --> 00:00:24,539
Q&A section at the end hopefully if

00:00:21,539 --> 00:00:26,760
there is not enough time you can also

00:00:24,539 --> 00:00:32,130
stop me during the talk and ask me

00:00:26,760 --> 00:00:35,640
question if anything is unclear okay so

00:00:32,130 --> 00:00:38,250
let's get started we'll start by

00:00:35,640 --> 00:00:40,980
discussing a bit what the workflow is

00:00:38,250 --> 00:00:43,770
and then I will show you a quick demo

00:00:40,980 --> 00:00:46,260
and spend the next part of the

00:00:43,770 --> 00:00:53,489
presentation trying to explain what

00:00:46,260 --> 00:00:57,480
happened during them the demo so the

00:00:53,489 --> 00:01:00,899
term workflow is used in many different

00:00:57,480 --> 00:01:04,080
contexts but for our purpose a

00:01:00,899 --> 00:01:07,290
distributed workflow is some kind of

00:01:04,080 --> 00:01:10,670
complex process which is composed of a

00:01:07,290 --> 00:01:16,189
mix of independent and interdependent

00:01:10,670 --> 00:01:16,189
units of work that are called tasks

00:01:17,270 --> 00:01:25,790
usually workflows are modeled with daj s

00:01:22,020 --> 00:01:29,880
which stands for directed acyclic graphs

00:01:25,790 --> 00:01:33,390
thing dependency graphs between the

00:01:29,880 --> 00:01:37,799
tasks and they are model using some

00:01:33,390 --> 00:01:42,479
domain-specific language or with ad hoc

00:01:37,799 --> 00:01:44,640
code like when you have a job queue but

00:01:42,479 --> 00:01:47,280
what you really try to accomplish is to

00:01:44,640 --> 00:01:49,909
have an entire workflow and you use the

00:01:47,280 --> 00:01:53,220
job queue and the tasks in the job queue

00:01:49,909 --> 00:01:55,380
to do some work but also to schedule the

00:01:53,220 --> 00:02:01,020
next steps that should happen during the

00:01:55,380 --> 00:02:04,140
workflow neither of those provide a good

00:02:01,020 --> 00:02:09,929
solution and the reason for that is

00:02:04,140 --> 00:02:12,180
because degs are too rigid you cannot

00:02:09,929 --> 00:02:13,950
have dynamic staff happening there

00:02:12,180 --> 00:02:17,069
usually

00:02:13,950 --> 00:02:22,230
and the ad hoc approach where you have

00:02:17,069 --> 00:02:24,989
the job queues tends to create code that

00:02:22,230 --> 00:02:28,140
is hard to maintain because the entire

00:02:24,989 --> 00:02:31,940
workflow logic is spread across all the

00:02:28,140 --> 00:02:34,530
tasks that are part of the workflow

00:02:31,940 --> 00:02:38,930
another problem with the ad hoc approach

00:02:34,530 --> 00:02:42,060
is that usually it's very hard to

00:02:38,930 --> 00:02:45,690
synchronize tasks between them so if you

00:02:42,060 --> 00:02:49,080
want to have a task started only after

00:02:45,690 --> 00:02:54,540
other tasks are finished that's usually

00:02:49,080 --> 00:02:56,819
pretty hard to to do flow it takes a

00:02:54,540 --> 00:03:00,299
different approach for the work flow

00:02:56,819 --> 00:03:03,150
modeling problem and it uses a single

00:03:00,299 --> 00:03:07,099
threaded Python code and something that

00:03:03,150 --> 00:03:07,099
I call gradual concurrency inference

00:03:07,310 --> 00:03:13,200
here's the toy example of a video

00:03:09,870 --> 00:03:18,239
processing work flow at the top we have

00:03:13,200 --> 00:03:20,370
some input data and in our case is there

00:03:18,239 --> 00:03:23,519
are two URLs for a video and subtitle

00:03:20,370 --> 00:03:28,739
and then there is an entire workflow

00:03:23,519 --> 00:03:32,220
that will process this data and what it

00:03:28,739 --> 00:03:36,030
will do it will try to overlay the

00:03:32,220 --> 00:03:39,239
subtitle on the video and encode the

00:03:36,030 --> 00:03:41,940
video in some target formats it will

00:03:39,239 --> 00:03:44,190
also try to find some chapters some cut

00:03:41,940 --> 00:03:49,459
points in the videos again extract

00:03:44,190 --> 00:03:49,459
tunnels from from there and we'll try to

00:03:49,760 --> 00:03:58,739
analyze the subtitle and target some ads

00:03:54,150 --> 00:04:01,230
for for this video so the interesting

00:03:58,739 --> 00:04:07,889
thing here is something that you cannot

00:04:01,230 --> 00:04:10,799
easily do with daj s is the part where

00:04:07,889 --> 00:04:14,160
the tunnels are extracted this is a

00:04:10,799 --> 00:04:16,739
dynamic step and the number of tunnel

00:04:14,160 --> 00:04:20,400
extracts extraction tasks can different

00:04:16,739 --> 00:04:24,380
based on the video so this is where you

00:04:20,400 --> 00:04:24,380
need some some flexibility

00:04:25,490 --> 00:04:33,319
so next I would like to show you how

00:04:28,819 --> 00:04:36,680
this workflow is implemented in in flowy

00:04:33,319 --> 00:04:40,810
and then like I said earlier i'll try to

00:04:36,680 --> 00:04:44,349
explain what what really happened there

00:04:40,810 --> 00:04:44,349
so let's see

00:04:56,639 --> 00:05:09,300
alright so i start with the activities

00:05:04,490 --> 00:05:12,749
or rather the tasks and in this case i'm

00:05:09,300 --> 00:05:17,099
using some dammit tasks you can see all

00:05:12,749 --> 00:05:19,229
of them have some sleep timer in there

00:05:17,099 --> 00:05:21,389
just to simulate they are doing

00:05:19,229 --> 00:05:23,310
something and they are regular Python

00:05:21,389 --> 00:05:25,680
function that there's nothing special

00:05:23,310 --> 00:05:31,460
about them they just get some input data

00:05:25,680 --> 00:05:31,460
do some processing and output the result

00:05:31,819 --> 00:05:41,840
so this is similar with what you will

00:05:35,189 --> 00:05:41,840
get in salary or a regular job queue

00:05:48,289 --> 00:05:53,310
this is the the workflow code so it's

00:05:51,779 --> 00:05:56,789
the code that would implement the

00:05:53,310 --> 00:05:59,969
workflow that we saw earlier again it's

00:05:56,789 --> 00:06:04,560
a regular pattern code we are just

00:05:59,969 --> 00:06:07,589
calling the tasks but there is something

00:06:04,560 --> 00:06:11,189
funny about it because it has a closure

00:06:07,589 --> 00:06:14,189
and we are not importing the tasks the

00:06:11,189 --> 00:06:16,259
task functions themselves and there is a

00:06:14,189 --> 00:06:18,449
reason for this this is a kind of

00:06:16,259 --> 00:06:21,240
dependency injection and there is a

00:06:18,449 --> 00:06:25,339
reason for it and we'll see later why

00:06:21,240 --> 00:06:28,610
this would be useful other than that

00:06:25,339 --> 00:06:32,750
there are just function calls and

00:06:28,610 --> 00:06:32,750
regular Python code actually

00:06:44,360 --> 00:06:50,509
I'm going to demonstrate that this is

00:06:46,520 --> 00:06:53,509
not anything special by running this

00:06:50,509 --> 00:06:56,689
code so what I do here I import all the

00:06:53,509 --> 00:06:59,000
tasks and the workflow function I'm

00:06:56,689 --> 00:07:01,520
going to pass the tasks to the workflow

00:06:59,000 --> 00:07:04,370
closure and then call the closure with

00:07:01,520 --> 00:07:15,800
the input data and this will run the

00:07:04,370 --> 00:07:23,300
workflow code sequentially I'm also

00:07:15,800 --> 00:07:25,490
going to time this this execution so it

00:07:23,300 --> 00:07:29,750
will take a while because of the the

00:07:25,490 --> 00:07:33,129
timers that I have they're forcing the

00:07:29,750 --> 00:07:33,129
task to sleep

00:07:40,969 --> 00:07:44,379
and hopefully

00:07:53,430 --> 00:07:59,610
yeah that was happens ah sorry about

00:07:56,979 --> 00:07:59,610
that I'll try again

00:08:15,639 --> 00:08:18,419
alright

00:08:33,270 --> 00:08:38,640
I know what's going on but whatever ah

00:08:40,320 --> 00:08:45,090
yeah something something's wrong but so

00:08:45,660 --> 00:08:50,980
usually it should work with is just

00:08:49,330 --> 00:08:57,940
regular Python code so there is no

00:08:50,980 --> 00:09:01,060
reason for it not to work but yeah but

00:08:57,940 --> 00:09:03,100
the interesting part here are so running

00:09:01,060 --> 00:09:06,190
that code would take about 10 seconds

00:09:03,100 --> 00:09:08,620
because of all the timers and everything

00:09:06,190 --> 00:09:10,540
will happen in sequence so the

00:09:08,620 --> 00:09:13,990
interesting part is being able to run

00:09:10,540 --> 00:09:17,320
this as a workflow and have all that

00:09:13,990 --> 00:09:19,500
concurrency happening so I try to do

00:09:17,320 --> 00:09:19,500
that

00:09:33,540 --> 00:09:43,019
okay so it it went much faster about two

00:09:39,440 --> 00:09:48,000
seconds and the reason for that is

00:09:43,019 --> 00:09:51,630
because all tasks that could be they

00:09:48,000 --> 00:09:53,970
could be executed in parallel were

00:09:51,630 --> 00:09:57,389
executed at the same time as we can see

00:09:53,970 --> 00:10:00,990
in the diagram that was generated so the

00:09:57,389 --> 00:10:06,899
arrows there represent a dependency

00:10:00,990 --> 00:10:09,209
between the tasks and we can see a lot

00:10:06,899 --> 00:10:16,319
of them were being executed at the same

00:10:09,209 --> 00:10:21,769
time so I'm going to try to explain how

00:10:16,319 --> 00:10:25,860
that works and white it it went so fast

00:10:21,769 --> 00:10:31,560
versus the previous version which didn't

00:10:25,860 --> 00:10:34,620
work right so in order in order to

00:10:31,560 --> 00:10:38,130
understand what was happening during

00:10:34,620 --> 00:10:41,279
demo I have to talk about workflow

00:10:38,130 --> 00:10:44,550
engines first and we begin with a simple

00:10:41,279 --> 00:10:48,899
task queue where we have all the tasks

00:10:44,550 --> 00:10:51,449
that we want to be executed the

00:10:48,899 --> 00:10:54,540
workflows are pulling the tasks from the

00:10:51,449 --> 00:10:57,180
queue and are running them and as I said

00:10:54,540 --> 00:10:59,339
when you have an approach similar to

00:10:57,180 --> 00:11:02,579
this there must be some additional code

00:10:59,339 --> 00:11:07,920
in the task that will know to schedule

00:11:02,579 --> 00:11:10,889
other tasks when they are finished so

00:11:07,920 --> 00:11:12,810
they also generate other tasks beside

00:11:10,889 --> 00:11:18,000
the usual data processing that they are

00:11:12,810 --> 00:11:22,260
doing and this is not very good because

00:11:18,000 --> 00:11:25,199
the workflow logic will get spread and

00:11:22,260 --> 00:11:28,769
like I said it's also very hard to

00:11:25,199 --> 00:11:32,490
synchronize between different tasks so

00:11:28,769 --> 00:11:36,509
another idea would be to have the task

00:11:32,490 --> 00:11:40,139
generate a special type of task called

00:11:36,509 --> 00:11:43,199
the decision and what the decision does

00:11:40,139 --> 00:11:46,680
instead of doing some data processing it

00:11:43,199 --> 00:11:47,050
will all only scheduled other tasks in

00:11:46,680 --> 00:11:52,170
the

00:11:47,050 --> 00:11:52,170
you so it acts as a kind of orchestrator

00:11:52,410 --> 00:11:58,810
like we can see here the arrow for from

00:11:56,620 --> 00:12:01,750
the storage to the worker is reversed

00:11:58,810 --> 00:12:05,980
because the orchestrate the decision

00:12:01,750 --> 00:12:09,490
will read data from the data store in

00:12:05,980 --> 00:12:12,130
order to try to get a snapshot of the

00:12:09,490 --> 00:12:16,450
workflow history and the workflow state

00:12:12,130 --> 00:12:18,370
and based on that state and all the

00:12:16,450 --> 00:12:21,820
world all the tasks that were finished

00:12:18,370 --> 00:12:27,070
it will try to come up with other tests

00:12:21,820 --> 00:12:30,510
that must be executed next but this

00:12:27,070 --> 00:12:34,200
solution is also not very good because

00:12:30,510 --> 00:12:40,000
you could have concurrency problems so

00:12:34,200 --> 00:12:43,329
if you if to task finish one right after

00:12:40,000 --> 00:12:46,269
the other you can get to decision to

00:12:43,329 --> 00:12:49,420
decision scheduled and if those are

00:12:46,269 --> 00:12:52,089
executed in parallel by two workers they

00:12:49,420 --> 00:12:55,589
will generate duplicate tasks in the

00:12:52,089 --> 00:12:59,350
queue so this is not a perfect solution

00:12:55,589 --> 00:13:03,029
so in order to improve this even more we

00:12:59,350 --> 00:13:08,260
need to have the queues managed in a way

00:13:03,029 --> 00:13:12,250
that all the decisions for a particular

00:13:08,260 --> 00:13:16,720
workflow execution will happen in

00:13:12,250 --> 00:13:21,339
sequence and for this we introduce

00:13:16,720 --> 00:13:24,399
another layer that will ensure will

00:13:21,339 --> 00:13:27,480
ensure this another thing we would also

00:13:24,399 --> 00:13:32,230
want to add in some kind of time

00:13:27,480 --> 00:13:34,839
tracking system that will know how much

00:13:32,230 --> 00:13:39,850
time a worker has spent running some

00:13:34,839 --> 00:13:43,360
tasks so and it can it can declare the

00:13:39,850 --> 00:13:46,060
test as time out if a certain amount of

00:13:43,360 --> 00:13:52,510
time passes without the worker doing any

00:13:46,060 --> 00:13:55,779
progress so this is not something new

00:13:52,510 --> 00:13:59,130
this kind of workflow engine is

00:13:55,779 --> 00:14:00,460
implemented and provided by the Amazon

00:13:59,130 --> 00:14:03,640
swf

00:14:00,460 --> 00:14:05,350
service it's also available as an

00:14:03,640 --> 00:14:08,260
open-source alternative in the

00:14:05,350 --> 00:14:13,390
eucalyptus project with the same API

00:14:08,260 --> 00:14:16,780
that amazon has there is also read is

00:14:13,390 --> 00:14:19,840
based engine similar to these in the

00:14:16,780 --> 00:14:21,640
works that I know of and there's also

00:14:19,840 --> 00:14:24,930
the local back-end that you saw earlier

00:14:21,640 --> 00:14:29,740
in the demo and the local back-end will

00:14:24,930 --> 00:14:32,800
create all these engine and the workers

00:14:29,740 --> 00:14:36,280
in a single machine on a single machine

00:14:32,800 --> 00:14:39,250
and we'll run them only for the duration

00:14:36,280 --> 00:14:40,810
of the worker with for the duration of

00:14:39,250 --> 00:14:52,450
the workflow and then everything gets

00:14:40,810 --> 00:14:56,170
destroyed so hopefully by this time this

00:14:52,450 --> 00:14:59,610
was the code that the workflow cut code

00:14:56,170 --> 00:15:02,880
in the demo so hopefully at this time

00:14:59,610 --> 00:15:06,340
you kind of get an understanding that

00:15:02,880 --> 00:15:08,200
this code will run multiple times so

00:15:06,340 --> 00:15:11,890
every time a decision needs to be made

00:15:08,200 --> 00:15:16,450
for this workflow to have progress on it

00:15:11,890 --> 00:15:18,280
this code will be executed again so if I

00:15:16,450 --> 00:15:21,850
were to put a print statement there and

00:15:18,280 --> 00:15:28,900
run the workflow I will see a lot of

00:15:21,850 --> 00:15:31,560
print messages okay so I mentioned

00:15:28,900 --> 00:15:35,080
earlier about dependency injection and

00:15:31,560 --> 00:15:41,170
why that's needed and the reason for it

00:15:35,080 --> 00:15:44,980
is because flow we will will inject some

00:15:41,170 --> 00:15:50,610
proxies instead of the instead of the

00:15:44,980 --> 00:15:54,700
real task functions and the proxies are

00:15:50,610 --> 00:16:00,220
color balls and will act just as a task

00:15:54,700 --> 00:16:03,040
would but they are a bit special so when

00:16:00,220 --> 00:16:06,750
a proxy is called the call itself is non

00:16:03,040 --> 00:16:11,080
blocking so it will return very fast and

00:16:06,750 --> 00:16:13,209
the return the value of the proxy is a

00:16:11,080 --> 00:16:15,369
task result

00:16:13,209 --> 00:16:17,559
and the test result can have two three

00:16:15,369 --> 00:16:21,879
different types it can be a placeholder

00:16:17,559 --> 00:16:29,290
in the case that we don't have value for

00:16:21,879 --> 00:16:31,629
that task it can it can or maybe the

00:16:29,290 --> 00:16:35,050
task is currently running and we don't

00:16:31,629 --> 00:16:38,079
have a result for for it it can be a

00:16:35,050 --> 00:16:40,420
success if the task was completed

00:16:38,079 --> 00:16:43,119
successfully and we do have a value for

00:16:40,420 --> 00:16:48,519
it or it can be an error if for some

00:16:43,119 --> 00:16:52,029
reason that task failed the other thing

00:16:48,519 --> 00:16:55,089
a proxy called does it looks at the

00:16:52,029 --> 00:16:59,110
arguments and tries to find other task

00:16:55,089 --> 00:17:03,720
results that are part of the arguments

00:16:59,110 --> 00:17:08,220
if any of the argument is a placeholder

00:17:03,720 --> 00:17:12,339
then this means that the current

00:17:08,220 --> 00:17:16,179
activity or task cannot be scheduled yet

00:17:12,339 --> 00:17:20,039
because it has dependencies that are not

00:17:16,179 --> 00:17:22,750
yet satisfied so it will track the

00:17:20,039 --> 00:17:26,319
results of the previous proxy calls

00:17:22,750 --> 00:17:32,320
through the entire workflow like we can

00:17:26,319 --> 00:17:35,490
see here so in this case when the code

00:17:32,320 --> 00:17:40,539
is run for the first time in a workflow

00:17:35,490 --> 00:17:42,789
the embedded subtitle task will be

00:17:40,539 --> 00:17:44,440
scheduled and its result will be a

00:17:42,789 --> 00:17:49,360
placeholder because we don't have a

00:17:44,440 --> 00:17:54,279
value for it but the cause for the video

00:17:49,360 --> 00:17:56,770
encoding one schedule any activities

00:17:54,279 --> 00:17:59,380
because they will have placeholder as

00:17:56,770 --> 00:18:03,510
part of their arguments meaning that

00:17:59,380 --> 00:18:06,940
there are unsatisfied dependencies and

00:18:03,510 --> 00:18:10,450
in this case the results for the proxy

00:18:06,940 --> 00:18:15,510
calls for the D encode video task will

00:18:10,450 --> 00:18:18,429
also be placeholders so what this does

00:18:15,510 --> 00:18:22,000
it's actually building the daj

00:18:18,429 --> 00:18:25,960
dynamically at runtime by tracing all

00:18:22,000 --> 00:18:26,860
the results from the proxy calls to the

00:18:25,960 --> 00:18:32,380
arguments of

00:18:26,860 --> 00:18:36,880
proxy calls and finally workflow finish

00:18:32,380 --> 00:18:40,000
finish its execution when the result the

00:18:36,880 --> 00:18:42,790
returned value contains no placeholders

00:18:40,000 --> 00:18:45,750
meaning that all the activities or all

00:18:42,790 --> 00:18:50,460
the tasks tasks that were needed to

00:18:45,750 --> 00:18:50,460
compose the final result are finished

00:18:50,820 --> 00:18:58,570
and like you can see here this is true

00:18:54,610 --> 00:19:01,299
for even for data structures so we have

00:18:58,570 --> 00:19:03,850
here at Apple and the values are inside

00:19:01,299 --> 00:19:07,929
the topple and this will continue to

00:19:03,850 --> 00:19:10,570
work and the timeless there are in your

00:19:07,929 --> 00:19:12,250
list and those will also get picked up

00:19:10,570 --> 00:19:15,580
so you can use any kind of data

00:19:12,250 --> 00:19:18,850
structures for the return data as long

00:19:15,580 --> 00:19:23,110
as it can be Jason serialized that's

00:19:18,850 --> 00:19:25,240
what it's used for sterilization so

00:19:23,110 --> 00:19:29,730
there are a couple of important things

00:19:25,240 --> 00:19:32,590
to keep in mind when writing a workflow

00:19:29,730 --> 00:19:35,910
basically what you want is for all the

00:19:32,590 --> 00:19:40,390
decision executions to have the same

00:19:35,910 --> 00:19:43,150
execution path in your code for the same

00:19:40,390 --> 00:19:44,590
workflow instance right so for all the

00:19:43,150 --> 00:19:47,950
decisions that belong to the same

00:19:44,590 --> 00:19:50,860
workflow instance this usually means

00:19:47,950 --> 00:19:53,470
that you have to use pure functions in

00:19:50,860 --> 00:19:56,770
your workflow or if you want some kind

00:19:53,470 --> 00:19:58,870
of side effects either send those values

00:19:56,770 --> 00:20:02,169
through the input data to through to the

00:19:58,870 --> 00:20:09,059
workflow or have dedicated activities

00:20:02,169 --> 00:20:11,559
for them or dedicated tasks for them so

00:20:09,059 --> 00:20:14,350
the other thing you can do with the task

00:20:11,559 --> 00:20:17,230
result is to use it as a Python value

00:20:14,350 --> 00:20:19,090
like we see here I'm squaring two

00:20:17,230 --> 00:20:25,630
numbers and then I'm adding them

00:20:19,090 --> 00:20:27,880
together and when this happens if any of

00:20:25,630 --> 00:20:30,730
the value involved is a placeholder

00:20:27,880 --> 00:20:35,320
meaning that there is no result for it

00:20:30,730 --> 00:20:38,490
yet a special exception is raised that

00:20:35,320 --> 00:20:44,480
will interrupt the workflow the

00:20:38,490 --> 00:20:48,270
the execution of dysfunction so in

00:20:44,480 --> 00:20:52,290
effect this act as a barrier in your

00:20:48,270 --> 00:20:56,040
workflow and it won't get past until you

00:20:52,290 --> 00:20:59,429
have the values for for the results that

00:20:56,040 --> 00:21:04,320
are involved this also means that if you

00:20:59,429 --> 00:21:09,600
have code after this displays that can

00:21:04,320 --> 00:21:11,580
be a concurrent it won't be detected so

00:21:09,600 --> 00:21:15,929
you have to make sure that you access

00:21:11,580 --> 00:21:19,710
the values as late as possible to to

00:21:15,929 --> 00:21:23,460
have the greatest concurrency a similar

00:21:19,710 --> 00:21:26,240
thing happens in the original code of

00:21:23,460 --> 00:21:29,309
the example where we iterate over the

00:21:26,240 --> 00:21:33,390
chapters that are found in the video so

00:21:29,309 --> 00:21:36,120
here to this act as a barrier but being

00:21:33,390 --> 00:21:39,530
at the bottom it didn't affect the rest

00:21:36,120 --> 00:21:53,210
of the code so you may have not noticed

00:21:39,530 --> 00:21:55,860
it another example is when you have a

00:21:53,210 --> 00:21:58,710
situation like this one so here I'm

00:21:55,860 --> 00:22:03,260
squaring two numbers and then I may want

00:21:58,710 --> 00:22:06,990
to do some optional additional

00:22:03,260 --> 00:22:10,460
computation and it's not clear in what

00:22:06,990 --> 00:22:17,360
order the if conditions should be

00:22:10,460 --> 00:22:20,190
written because if in this case if the B

00:22:17,360 --> 00:22:22,800
computation so squaring of the B is the

00:22:20,190 --> 00:22:26,580
first one to finish because I have the

00:22:22,800 --> 00:22:29,850
conditional on the a value it will have

00:22:26,580 --> 00:22:32,370
to wait until the result for a is

00:22:29,850 --> 00:22:35,070
available available to progress further

00:22:32,370 --> 00:22:37,740
in the workflow and no matter how I try

00:22:35,070 --> 00:22:40,050
to write the code there will always be a

00:22:37,740 --> 00:22:42,390
case where the workflow cannot make

00:22:40,050 --> 00:22:44,670
progress until the other value is

00:22:42,390 --> 00:22:48,830
available and this is kind of a problem

00:22:44,670 --> 00:22:51,870
but it can be solved with something that

00:22:48,830 --> 00:22:57,190
it's called

00:22:51,870 --> 00:22:59,920
sub workflow so here I refactor the code

00:22:57,190 --> 00:23:03,250
and that did the processing for each

00:22:59,920 --> 00:23:07,000
number in part in a in a sub workflow

00:23:03,250 --> 00:23:10,000
and then in the main workflow i'm using

00:23:07,000 --> 00:23:13,620
the sub workflows as i would use a

00:23:10,000 --> 00:23:17,560
regular task and this way they can all

00:23:13,620 --> 00:23:20,050
happen in parallel and when both are

00:23:17,560 --> 00:23:25,140
finished i can sum them and return the

00:23:20,050 --> 00:23:25,140
result so workflows are a great way to

00:23:25,530 --> 00:23:34,000
to do more complex things that you

00:23:28,780 --> 00:23:37,950
couldn't without them and another thing

00:23:34,000 --> 00:23:40,420
to notice here in the main workflow I

00:23:37,950 --> 00:23:45,640
didn't have to do anything special to

00:23:40,420 --> 00:23:52,660
use the sub workflows they are just they

00:23:45,640 --> 00:23:58,780
are used just as regular tasks so for

00:23:52,660 --> 00:24:00,100
error handling you might expect the

00:23:58,780 --> 00:24:03,640
error handling to look something like

00:24:00,100 --> 00:24:07,300
this this is how a normal Python code

00:24:03,640 --> 00:24:11,320
would look like if you had some

00:24:07,300 --> 00:24:13,630
exceptions in in a function but this is

00:24:11,320 --> 00:24:17,200
not possible because as I said earlier

00:24:13,630 --> 00:24:21,360
the proxy coal is not blocking so you

00:24:17,200 --> 00:24:24,790
cannot get the exception at this point

00:24:21,360 --> 00:24:31,090
so actually this is the place where you

00:24:24,790 --> 00:24:35,920
have to to write your trial try except

00:24:31,090 --> 00:24:39,400
clause so the reason for this is because

00:24:35,920 --> 00:24:42,370
only at this point we can force the

00:24:39,400 --> 00:24:44,320
evaluation of the result and only at

00:24:42,370 --> 00:24:48,570
this point we know for sure if the

00:24:44,320 --> 00:24:52,570
computation words successful or not and

00:24:48,570 --> 00:24:55,750
this looks a bit strange and I don't

00:24:52,570 --> 00:24:57,790
like it too much there is a better way

00:24:55,750 --> 00:25:00,910
of doing it using the weight function

00:24:57,790 --> 00:25:04,780
and that comes in flowing and what these

00:25:00,910 --> 00:25:08,800
does it will try to evaluate

00:25:04,780 --> 00:25:15,640
ever to dereference the the proxy the

00:25:08,800 --> 00:25:18,190
test result and it's similar as doing an

00:25:15,640 --> 00:25:20,800
operation on it and the name is a

00:25:18,190 --> 00:25:24,160
reminder that this will act as a barrier

00:25:20,800 --> 00:25:27,610
so nothing will pass this point until

00:25:24,160 --> 00:25:31,270
and not not only that one past this

00:25:27,610 --> 00:25:34,510
point but won't be detected even if we

00:25:31,270 --> 00:25:43,110
could be executed in parallel until this

00:25:34,510 --> 00:25:43,110
value it is available but this is not

00:25:43,830 --> 00:25:50,890
always the case you are not you cannot

00:25:48,250 --> 00:25:52,510
maybe you don't want to use the value in

00:25:50,890 --> 00:25:55,480
the workflow itself you just want to

00:25:52,510 --> 00:26:00,310
pass the value from a task to another

00:25:55,480 --> 00:26:05,410
task and in this case how do you pick up

00:26:00,310 --> 00:26:08,620
errors so what would happen here if the

00:26:05,410 --> 00:26:11,800
result for B is an error when you're

00:26:08,620 --> 00:26:15,100
passing an error in the arguments of

00:26:11,800 --> 00:26:17,380
another proxy call the proxy call will

00:26:15,100 --> 00:26:21,930
also return an error so the errors

00:26:17,380 --> 00:26:26,620
propagate from one task to the other and

00:26:21,930 --> 00:26:29,370
if the result value that you try to

00:26:26,620 --> 00:26:32,800
return from the workflow contains error

00:26:29,370 --> 00:26:39,670
errors then the workflow itself will

00:26:32,800 --> 00:26:42,550
will fail so you cannot dodge errors you

00:26:39,670 --> 00:26:45,310
have to deal with them or you can ignore

00:26:42,550 --> 00:26:47,890
them by not making them part of the

00:26:45,310 --> 00:26:50,050
final result in which case you will get

00:26:47,890 --> 00:26:52,030
some warning message that you had some

00:26:50,050 --> 00:26:59,620
errors that were not picked up by your

00:26:52,030 --> 00:27:03,700
code or handled so the workflows can

00:26:59,620 --> 00:27:05,590
also scale by using some of the other

00:27:03,700 --> 00:27:13,800
backends that I mentioned earlier the

00:27:05,590 --> 00:27:13,800
amazon one hour eucalyptus and there are

00:27:14,310 --> 00:27:18,700
when you want to scale basically nothing

00:27:18,160 --> 00:27:20,410
changes

00:27:18,700 --> 00:27:22,840
the workflow so you would still use the

00:27:20,410 --> 00:27:24,610
code that you saw earlier there are some

00:27:22,840 --> 00:27:27,130
additional configurations that you have

00:27:24,610 --> 00:27:32,920
to do that happens outside of the codes

00:27:27,130 --> 00:27:34,570
or are not part of the code because when

00:27:32,920 --> 00:27:37,240
you scale and you want to run the world

00:27:34,570 --> 00:27:39,250
from multiple machines in a distributed

00:27:37,240 --> 00:27:42,850
system there can be all kinds of

00:27:39,250 --> 00:27:45,100
failures there is there are some xik

00:27:42,850 --> 00:27:51,580
execution time with the kids you can set

00:27:45,100 --> 00:27:55,480
and those will will will help you with

00:27:51,580 --> 00:27:58,320
fault tolerance there is another type of

00:27:55,480 --> 00:28:01,090
error that you can get when you scale

00:27:58,320 --> 00:28:03,460
which is a timeout error which is a

00:28:01,090 --> 00:28:07,600
subclass of the task error that we saw

00:28:03,460 --> 00:28:13,840
earlier so you can have special handling

00:28:07,600 --> 00:28:18,610
for timeouts there is automatically try

00:28:13,840 --> 00:28:21,700
mechanisms in place and you can for for

00:28:18,610 --> 00:28:25,270
the timeouts and you can configure them

00:28:21,700 --> 00:28:27,430
as you wish there is also the notion of

00:28:25,270 --> 00:28:30,280
the heartbeat and the heartbeats are

00:28:27,430 --> 00:28:33,610
some color balls that a task can call

00:28:30,280 --> 00:28:35,530
and what it does when when a heartbeat

00:28:33,610 --> 00:28:39,520
is called it will send a message to the

00:28:35,530 --> 00:28:43,750
backend telling the backend that the the

00:28:39,520 --> 00:28:46,000
current test is still doing progress but

00:28:43,750 --> 00:28:50,430
another thing that it does it will

00:28:46,000 --> 00:28:53,620
return a boolean value in the task and

00:28:50,430 --> 00:28:57,190
that boolean value can be used to know

00:28:53,620 --> 00:28:59,650
if the task timed out in which case you

00:28:57,190 --> 00:29:02,980
can abandon its execution because even

00:28:59,650 --> 00:29:05,470
if it finish the execution successfully

00:29:02,980 --> 00:29:11,940
its result will be rejected by the

00:29:05,470 --> 00:29:17,020
backend a another thing to keep in mind

00:29:11,940 --> 00:29:18,640
you should aim to have tasks written in

00:29:17,020 --> 00:29:22,390
such a way that they can run multiple

00:29:18,640 --> 00:29:27,460
times just because of the failures that

00:29:22,390 --> 00:29:30,550
can happen and there it tries the tasks

00:29:27,460 --> 00:29:33,190
or the activities i'm using the they

00:29:30,550 --> 00:29:35,080
mean mostly the same thing can be

00:29:33,190 --> 00:29:37,720
implemented in other languages so you

00:29:35,080 --> 00:29:40,930
can use flowy only for orchestration and

00:29:37,720 --> 00:29:45,850
workflow modeling so the engine and the

00:29:40,930 --> 00:29:49,930
logic to run the activities there are

00:29:45,850 --> 00:29:53,110
some restrictions on the the size of the

00:29:49,930 --> 00:30:00,270
data that can be passed as input or the

00:29:53,110 --> 00:30:02,710
result size each worker so when you are

00:30:00,270 --> 00:30:04,930
scaling and you run on multiple machines

00:30:02,710 --> 00:30:07,840
you would have workers that are running

00:30:04,930 --> 00:30:09,850
continuously not like we had for the

00:30:07,840 --> 00:30:11,740
local back end where they were running

00:30:09,850 --> 00:30:14,830
only for the duration of the workflow

00:30:11,740 --> 00:30:16,840
and those workers are single threaded

00:30:14,830 --> 00:30:19,360
single process so if you want more of

00:30:16,840 --> 00:30:21,580
them on single machine you have to use

00:30:19,360 --> 00:30:26,800
your own process manager and start them

00:30:21,580 --> 00:30:28,930
and make sure that they are alive and if

00:30:26,800 --> 00:30:31,930
the history gets too large so the

00:30:28,930 --> 00:30:33,490
decision must use the workflow history

00:30:31,930 --> 00:30:36,490
the word for execution history and the

00:30:33,490 --> 00:30:39,100
war prostate to make decisions and if

00:30:36,490 --> 00:30:43,320
the history gets too large and actually

00:30:39,100 --> 00:30:47,440
the history the data that is transferred

00:30:43,320 --> 00:30:50,140
by because of the history has an

00:30:47,440 --> 00:30:52,540
exponential growth you can reduce that

00:30:50,140 --> 00:30:57,370
by using sub workflows sub workflows

00:30:52,540 --> 00:31:03,430
will only appear as a single entity in

00:30:57,370 --> 00:31:09,280
the history so they they will you can

00:31:03,430 --> 00:31:12,880
get basically can get logarithmic data

00:31:09,280 --> 00:31:16,660
transfer by using sub workflows in a

00:31:12,880 --> 00:31:21,900
smart way and because of the fault

00:31:16,660 --> 00:31:25,720
tolerance built in you can scale down so

00:31:21,900 --> 00:31:27,730
you could like for example all the

00:31:25,720 --> 00:31:30,190
workers can die at some point in time

00:31:27,730 --> 00:31:33,640
and then after a while they would come

00:31:30,190 --> 00:31:36,460
back online and the workflow progress

00:31:33,640 --> 00:31:39,250
won't be lost you may still lose the

00:31:36,460 --> 00:31:41,650
progress on specific tasks but the

00:31:39,250 --> 00:31:42,759
workflow itself the work for progress

00:31:41,650 --> 00:31:46,029
one below then

00:31:42,759 --> 00:31:49,809
this is very useful for workflows that

00:31:46,029 --> 00:31:51,820
take a very long time to run I think the

00:31:49,809 --> 00:31:54,159
maximum duration for Amazon is like one

00:31:51,820 --> 00:31:56,259
year for a workflow so this can be very

00:31:54,159 --> 00:31:59,289
useful in some situations and you can

00:31:56,259 --> 00:32:01,059
also scale up very easily just start new

00:31:59,289 --> 00:32:05,619
machines and they will connect to the

00:32:01,059 --> 00:32:12,789
queues and start pulling tasks that need

00:32:05,619 --> 00:32:14,579
to be executed thank you that was all if

00:32:12,789 --> 00:32:17,819
you have questions like I think now to

00:32:14,579 --> 00:32:17,819
good time

00:32:25,980 --> 00:32:33,280
how does this compare to Hillary there

00:32:30,130 --> 00:32:35,770
is federica you can create tasks and it

00:32:33,280 --> 00:32:39,460
will automate them how can you compare

00:32:35,770 --> 00:32:42,450
it yeah so with salary so salary is a

00:32:39,460 --> 00:32:46,680
test distributed a skewed or job queue

00:32:42,450 --> 00:32:50,500
and did it's a bit different because

00:32:46,680 --> 00:32:54,250
here you have the orchestration of the

00:32:50,500 --> 00:32:56,290
tasks so if you have many tasks and you

00:32:54,250 --> 00:32:58,690
want them to operate in a certain way

00:32:56,290 --> 00:33:02,230
with some dependencies between them and

00:32:58,690 --> 00:33:04,690
to pass that data between them you can

00:33:02,230 --> 00:33:09,040
do that by writing single-threaded code

00:33:04,690 --> 00:33:11,530
and from that single threaded code the

00:33:09,040 --> 00:33:15,430
dependency graph will be inferred for

00:33:11,530 --> 00:33:17,710
you and it will make sure that the task

00:33:15,430 --> 00:33:21,940
are scheduled in the correct order and

00:33:17,710 --> 00:33:26,320
they get the data they need passed in so

00:33:21,940 --> 00:33:28,060
i would use salary for one of jobs some

00:33:26,320 --> 00:33:33,990
sending an email or something but not

00:33:28,060 --> 00:33:39,370
for hundreds of jobs that are somehow in

00:33:33,990 --> 00:33:43,750
interdependent yeah yeah it also has

00:33:39,370 --> 00:33:50,200
canvas which is more like daj where you

00:33:43,750 --> 00:33:53,290
define your workflow topology before not

00:33:50,200 --> 00:33:55,660
in such a dynamic way you can do with

00:33:53,290 --> 00:33:57,790
single threaded Python code where you

00:33:55,660 --> 00:34:02,740
can have conditions and for loops and

00:33:57,790 --> 00:34:05,920
and all there thank you what a

00:34:02,740 --> 00:34:11,380
synchronous library you use as a bottom

00:34:05,920 --> 00:34:15,700
of the flow be sorry so what does his

00:34:11,380 --> 00:34:19,990
evilness library I seem to be as in soom

00:34:15,700 --> 00:34:26,980
a big event I don't think I'm using any

00:34:19,990 --> 00:34:30,700
synchronous library for the local back

00:34:26,980 --> 00:34:32,710
end i'm using the the futures module to

00:34:30,700 --> 00:34:35,140
to implement the workers but there is no

00:34:32,710 --> 00:34:37,680
a synchronous library involved okay

00:34:35,140 --> 00:34:37,680
thanks

00:34:47,940 --> 00:34:53,679
yeah in the example workflow you showed

00:34:50,740 --> 00:34:56,320
one of the tasks returns the list so the

00:34:53,679 --> 00:34:57,430
list of chapter points that then gets

00:34:56,320 --> 00:35:00,580
fed into something that builds

00:34:57,430 --> 00:35:02,650
thumbnails for the chapters do you have

00:35:00,580 --> 00:35:04,090
to wait does that task essentially block

00:35:02,650 --> 00:35:07,480
until every single chapter has been

00:35:04,090 --> 00:35:09,460
found or would it be possible maybe with

00:35:07,480 --> 00:35:11,740
clay changes to support say a generator

00:35:09,460 --> 00:35:12,880
function so you could you could start

00:35:11,740 --> 00:35:14,950
building a thumbnail to the first

00:35:12,880 --> 00:35:17,890
chapter while the task is still finding

00:35:14,950 --> 00:35:21,250
the later chapters so here yeah so here

00:35:17,890 --> 00:35:25,740
it will block so any code under the

00:35:21,250 --> 00:35:28,869
thumbnails line won't be executed until

00:35:25,740 --> 00:35:32,220
we have the chapters and this is because

00:35:28,869 --> 00:35:36,520
the fine chapters returns a list and

00:35:32,220 --> 00:35:39,369
it's a single result and we cannot get

00:35:36,520 --> 00:35:42,000
partial results from the task so we have

00:35:39,369 --> 00:35:48,010
to wait until the entire result is

00:35:42,000 --> 00:35:49,990
available yeah so so anything below that

00:35:48,010 --> 00:35:52,750
will be blocked until the result is

00:35:49,990 --> 00:35:54,460
available and this isn't such a big

00:35:52,750 --> 00:35:57,520
problem usually because there are ways

00:35:54,460 --> 00:35:59,590
to write the code in and this doesn't

00:35:57,520 --> 00:36:02,200
become a problem or if it is a problem

00:35:59,590 --> 00:36:04,840
you can create the sub workflow so I

00:36:02,200 --> 00:36:07,000
could had a sub workflow that would do

00:36:04,840 --> 00:36:09,430
only the fine chapters and the tunnel

00:36:07,000 --> 00:36:11,950
generation and then call the sub

00:36:09,430 --> 00:36:17,200
workflow from here and have that running

00:36:11,950 --> 00:36:21,940
in parallel with the other with the

00:36:17,200 --> 00:36:24,820
other code so just to follow up them

00:36:21,940 --> 00:36:26,830
does that mean in this example add tags

00:36:24,820 --> 00:36:28,450
which you could start processing

00:36:26,830 --> 00:36:29,770
immediately won't be executed

00:36:28,450 --> 00:36:33,810
immediately because you're waiting for

00:36:29,770 --> 00:36:38,590
the video encoding to finish now so in

00:36:33,810 --> 00:36:42,820
this case in this example all the tasks

00:36:38,590 --> 00:36:44,800
that can be executed in parallel will be

00:36:42,820 --> 00:36:48,660
executed in parallel so the actual

00:36:44,800 --> 00:36:51,070
execution topology will basically look

00:36:48,660 --> 00:36:53,650
exactly like this one so

00:36:51,070 --> 00:36:55,990
this is how it will get executed that's

00:36:53,650 --> 00:36:58,480
why then the world the work flow

00:36:55,990 --> 00:37:06,400
duration was about two seconds instead

00:36:58,480 --> 00:37:08,910
of 11 or something the time for the last

00:37:06,400 --> 00:37:08,910
question

00:37:11,969 --> 00:37:16,779
kind of a repeat of the previous one yet

00:37:15,190 --> 00:37:18,369
made a good point about not the

00:37:16,779 --> 00:37:19,719
thumbnail line but the line above works

00:37:18,369 --> 00:37:22,569
finding the chapters and returning the

00:37:19,719 --> 00:37:23,589
list it won't return from fine chapters

00:37:22,569 --> 00:37:26,529
until it's found all three of the

00:37:23,589 --> 00:37:27,880
chapters but when if could you convert

00:37:26,529 --> 00:37:30,729
find chapters to be a generator or

00:37:27,880 --> 00:37:33,069
return it get it to return next chapter

00:37:30,729 --> 00:37:34,630
and then you can do the thumbnail for

00:37:33,069 --> 00:37:38,259
the first chapter while foreign chapters

00:37:34,630 --> 00:37:40,029
is to a party mr. so yeah you could you

00:37:38,259 --> 00:37:41,950
could have a task that will only find

00:37:40,029 --> 00:37:46,599
the first chapter and it turned that and

00:37:41,950 --> 00:37:49,059
then call the task again and it will

00:37:46,599 --> 00:37:51,670
resume from that point you can actually

00:37:49,059 --> 00:37:54,039
send the last chapter and find the next

00:37:51,670 --> 00:37:57,700
one and this way you can solve the

00:37:54,039 --> 00:37:59,640
problem if you want to so it really

00:37:57,700 --> 00:38:02,589
depends on how you write your code you

00:37:59,640 --> 00:38:05,019
just the only rule you have to remember

00:38:02,589 --> 00:38:07,599
is that when you try to access a value

00:38:05,019 --> 00:38:10,660
in the world flow it will block until

00:38:07,599 --> 00:38:12,969
the value is available that's basically

00:38:10,660 --> 00:38:15,039
the only thing you need to know anything

00:38:12,969 --> 00:38:19,359
below that point won't be detected and

00:38:15,039 --> 00:38:22,569
cannot be concurrent and that can be

00:38:19,359 --> 00:38:26,489
solved through sub workflows server

00:38:22,569 --> 00:38:26,489

YouTube URL: https://www.youtube.com/watch?v=-Jm_ZNPIqjY


