Title: This Code May Kill People: Factoring Ethics into Software Design, w  M. Bellotti - Heartifacts 2020
Publication date: 2020-08-28
Playlist: Heartifacts 2020
Description: 
	Heartifacts is a Code & Supply conference that encourages intimate discussions about mental health, community building, career management, and other topics software professionals need to talk about more.

Learn more about Heartifacts at https://heartifacts.codeandsupply.co and grab some Heartifacts logo merch at https://codeandsupply.co/x/hfmerch. Learn more about Code & Supply at https://www.codeandsupply.co.

------

Marianne Bellotti presenting.

Software engineers rarely think about the safety of their products, but as the world becomes more connected, involves more machine learning and deploys more sensors the potential for harm only increases. How should an engineer think about these challenges? What approaches can be added to the development lifecycle in order to factor in ethics into the process. How has software gone wrong in the past and how have the organizations in charge iterated to prevent misuse of their products?

[Content Warning: This talk discusses some aspects of software in the defense space]
Captions: 
	00:00:00,160 --> 00:00:04,799
up next we have marianne blotti she's a

00:00:02,320 --> 00:00:07,120
software engineer who's worked on safety

00:00:04,799 --> 00:00:09,200
safely sharing humanitarian data for the

00:00:07,120 --> 00:00:09,840
united nations and rain modernization

00:00:09,200 --> 00:00:11,519
teams

00:00:09,840 --> 00:00:13,120
as a consultant in new york before

00:00:11,519 --> 00:00:13,920
joining the united states digital

00:00:13,120 --> 00:00:15,920
service

00:00:13,920 --> 00:00:17,520
as a part of the usds she spent three

00:00:15,920 --> 00:00:18,000
and a half years working on some of the

00:00:17,520 --> 00:00:19,840
oldest

00:00:18,000 --> 00:00:21,920
and most complicated computer systems in

00:00:19,840 --> 00:00:22,960
the world she has since returned to the

00:00:21,920 --> 00:00:24,800
private sector

00:00:22,960 --> 00:00:26,880
working for off zero and rebellion

00:00:24,800 --> 00:00:27,519
defense specializing in system safety

00:00:26,880 --> 00:00:30,080
with

00:00:27,519 --> 00:00:31,519
technical debt heavy systems marianne is

00:00:30,080 --> 00:00:32,239
here to share her talk this code may

00:00:31,519 --> 00:00:35,920
kill people

00:00:32,239 --> 00:00:38,960
refactoring ethics into software design

00:00:35,920 --> 00:00:41,760
hi everybody my name is marianne bilotti

00:00:38,960 --> 00:00:43,520
um i work for a company called rebellion

00:00:41,760 --> 00:00:45,600
defense and i'm here to talk to you

00:00:43,520 --> 00:00:48,879
about factoring ethics

00:00:45,600 --> 00:00:51,280
into software design so

00:00:48,879 --> 00:00:54,000
to start off with i've been a software

00:00:51,280 --> 00:00:56,239
engineer for about 15 years

00:00:54,000 --> 00:00:58,640
i've taken to referring to myself as a

00:00:56,239 --> 00:01:01,199
relapsed anthropologist sings

00:00:58,640 --> 00:01:02,399
all of my formal education is actually

00:01:01,199 --> 00:01:05,519
in anthropology

00:01:02,399 --> 00:01:06,640
and organizational design and lately

00:01:05,519 --> 00:01:08,240
over the last

00:01:06,640 --> 00:01:09,680
five years of my career i've been

00:01:08,240 --> 00:01:10,159
bringing more and more of those skill

00:01:09,680 --> 00:01:13,200
sets

00:01:10,159 --> 00:01:15,920
to how i deal with engineering teams and

00:01:13,200 --> 00:01:19,119
computers rather than just strictly math

00:01:15,920 --> 00:01:20,960
science stem skills over

00:01:19,119 --> 00:01:23,439
my career i've done lots of government

00:01:20,960 --> 00:01:26,080
work including three and a half years at

00:01:23,439 --> 00:01:28,240
united states digital service

00:01:26,080 --> 00:01:31,200
lately i've been working core services

00:01:28,240 --> 00:01:32,960
at a company called rebellion defense

00:01:31,200 --> 00:01:34,400
now as the name rebellion defense

00:01:32,960 --> 00:01:36,159
probably implies

00:01:34,400 --> 00:01:37,840
i work for the defense and national

00:01:36,159 --> 00:01:40,560
security and industry

00:01:37,840 --> 00:01:41,840
and some of this well most of this talk

00:01:40,560 --> 00:01:44,240
doesn't actually deal

00:01:41,840 --> 00:01:45,840
specifically with the military it deals

00:01:44,240 --> 00:01:48,880
with the process of building

00:01:45,840 --> 00:01:49,840
software part of introducing those

00:01:48,880 --> 00:01:52,320
concepts

00:01:49,840 --> 00:01:53,680
include getting personal about my story

00:01:52,320 --> 00:01:55,920
which means that we're going to discuss

00:01:53,680 --> 00:01:57,200
the us military environment to a certain

00:01:55,920 --> 00:01:58,399
extent so i want to give you a bit of a

00:01:57,200 --> 00:02:00,960
content warning

00:01:58,399 --> 00:02:02,399
before we start off but i also want to

00:02:00,960 --> 00:02:04,399
clarify that that

00:02:02,399 --> 00:02:05,920
doesn't mean that we're going to be

00:02:04,399 --> 00:02:07,600
talking about

00:02:05,920 --> 00:02:09,360
war and graphic details there's not

00:02:07,600 --> 00:02:11,360
gonna be any graphic content

00:02:09,360 --> 00:02:13,920
and i'm going to avoid using military

00:02:11,360 --> 00:02:16,239
language as much as possible

00:02:13,920 --> 00:02:17,520
um generally when i talk about these

00:02:16,239 --> 00:02:18,800
issues when i start to have this

00:02:17,520 --> 00:02:20,640
conversation with people

00:02:18,800 --> 00:02:22,480
one of the things that they will say is

00:02:20,640 --> 00:02:23,599
like well if you care about ethical and

00:02:22,480 --> 00:02:24,800
safe software

00:02:23,599 --> 00:02:27,360
is it better not to work with the

00:02:24,800 --> 00:02:30,000
military at all like it seems like

00:02:27,360 --> 00:02:30,879
that's a that's a difficult place to

00:02:30,000 --> 00:02:32,959
really advance

00:02:30,879 --> 00:02:34,800
these these causes and what is

00:02:32,959 --> 00:02:37,840
interesting to me about that

00:02:34,800 --> 00:02:41,200
is that um it reflects a sort of general

00:02:37,840 --> 00:02:43,680
isolationist attitude in technology

00:02:41,200 --> 00:02:45,040
about the military space versus the

00:02:43,680 --> 00:02:48,400
civilian space

00:02:45,040 --> 00:02:50,239
that people very few people argue

00:02:48,400 --> 00:02:52,000
that we shouldn't have a military we

00:02:50,239 --> 00:02:53,040
shouldn't have a defense or a sector or

00:02:52,000 --> 00:02:54,480
we shouldn't

00:02:53,040 --> 00:02:55,680
have infrastructure for national

00:02:54,480 --> 00:02:56,879
security they just don't want to be

00:02:55,680 --> 00:02:58,000
involved in it and they don't want to

00:02:56,879 --> 00:02:59,599
touch it they want to stay in the

00:02:58,000 --> 00:03:01,680
civilian space and create

00:02:59,599 --> 00:03:02,800
a very clear separation between these

00:03:01,680 --> 00:03:05,040
two spaces

00:03:02,800 --> 00:03:06,560
the problem and the risk with doing that

00:03:05,040 --> 00:03:09,200
is that that creates

00:03:06,560 --> 00:03:09,599
bubbles in which very different cultures

00:03:09,200 --> 00:03:12,560
and

00:03:09,599 --> 00:03:13,840
potentially very different values exist

00:03:12,560 --> 00:03:15,599
within the bubbles

00:03:13,840 --> 00:03:17,599
and that's one of the problems with

00:03:15,599 --> 00:03:18,159
military language is that within the

00:03:17,599 --> 00:03:20,080
military

00:03:18,159 --> 00:03:21,760
environment a lot of the terms that

00:03:20,080 --> 00:03:23,280
you'll thrown around when describing

00:03:21,760 --> 00:03:25,280
technology projects

00:03:23,280 --> 00:03:27,280
mean completely different things then

00:03:25,280 --> 00:03:28,799
people external to those environments

00:03:27,280 --> 00:03:31,120
interpret them to me

00:03:28,799 --> 00:03:33,280
and this creates a lot of problems when

00:03:31,120 --> 00:03:33,920
you think about in america at least we

00:03:33,280 --> 00:03:36,720
want

00:03:33,920 --> 00:03:38,319
our military to be overseen by civilian

00:03:36,720 --> 00:03:39,440
forces and we want them to be grounded

00:03:38,319 --> 00:03:41,840
by rule of law

00:03:39,440 --> 00:03:43,440
those are our values we want them to

00:03:41,840 --> 00:03:44,959
reflect democratic norms

00:03:43,440 --> 00:03:47,200
and this becomes very very different

00:03:44,959 --> 00:03:49,040
difficult to do when there isn't

00:03:47,200 --> 00:03:50,480
a clear channel of communication and

00:03:49,040 --> 00:03:52,080
collaboration between

00:03:50,480 --> 00:03:54,080
the civilian side of things and the

00:03:52,080 --> 00:03:56,319
military side of things when we isolate

00:03:54,080 --> 00:03:57,680
ourselves from each other and so

00:03:56,319 --> 00:03:58,000
rebellion defense one of the things we

00:03:57,680 --> 00:03:59,760
say

00:03:58,000 --> 00:04:01,280
is that national security is everyone's

00:03:59,760 --> 00:04:02,319
responsibility and that we want to

00:04:01,280 --> 00:04:05,439
create a channel

00:04:02,319 --> 00:04:08,400
of collaboration between civilian side

00:04:05,439 --> 00:04:11,280
um and the military in order to ensure

00:04:08,400 --> 00:04:13,360
uh best outcomes for everybody

00:04:11,280 --> 00:04:14,879
so the thing that's important to know

00:04:13,360 --> 00:04:16,400
about my story

00:04:14,879 --> 00:04:18,479
is that i tried to do this work in the

00:04:16,400 --> 00:04:21,280
private sector and i couldn't really

00:04:18,479 --> 00:04:22,160
get much headway with it um because the

00:04:21,280 --> 00:04:24,320
private sector

00:04:22,160 --> 00:04:26,880
didn't take software safety seriously

00:04:24,320 --> 00:04:29,120
but the defense sector absolutely did

00:04:26,880 --> 00:04:30,720
and uh so like in the in the tech sector

00:04:29,120 --> 00:04:32,720
we have this attitude of like move fast

00:04:30,720 --> 00:04:35,919
and break things that is so

00:04:32,720 --> 00:04:38,000
fundamental to uh the culture

00:04:35,919 --> 00:04:39,440
of silicon valley and the wider tech

00:04:38,000 --> 00:04:41,919
community as a whole

00:04:39,440 --> 00:04:43,360
which sounds great and certainly when

00:04:41,919 --> 00:04:45,199
you think about it in terms of the agile

00:04:43,360 --> 00:04:46,560
agile software development it makes a

00:04:45,199 --> 00:04:48,240
lot of sense but

00:04:46,560 --> 00:04:49,680
when you start to consider that these

00:04:48,240 --> 00:04:51,600
are the things that are potentially

00:04:49,680 --> 00:04:54,160
getting broken by our software

00:04:51,600 --> 00:04:55,600
it has a wide variety it has a different

00:04:54,160 --> 00:04:57,520
context completely

00:04:55,600 --> 00:04:59,120
right like marginalized communities

00:04:57,520 --> 00:05:00,560
vulnerable people democratic

00:04:59,120 --> 00:05:03,440
institutions we can all

00:05:00,560 --> 00:05:05,199
think of software that has been built in

00:05:03,440 --> 00:05:07,600
the last five years or so

00:05:05,199 --> 00:05:09,360
that has done damage to these

00:05:07,600 --> 00:05:10,400
communities of people and these things

00:05:09,360 --> 00:05:12,240
that we hold dear

00:05:10,400 --> 00:05:14,080
and so moving fast and breaking things

00:05:12,240 --> 00:05:16,400
is a completely different meaning when

00:05:14,080 --> 00:05:19,360
you start thinking about in those sense

00:05:16,400 --> 00:05:20,400
um maybe a couple months ago i was

00:05:19,360 --> 00:05:23,039
watching

00:05:20,400 --> 00:05:24,160
the new york times serial the weekly

00:05:23,039 --> 00:05:27,199
that's on hulu

00:05:24,160 --> 00:05:29,280
and they were doing this episode on

00:05:27,199 --> 00:05:30,639
deep fakes in which they profiled this

00:05:29,280 --> 00:05:32,960
company desa

00:05:30,639 --> 00:05:34,479
and one of the engineers there um the

00:05:32,960 --> 00:05:34,880
journalist that was interviewing him was

00:05:34,479 --> 00:05:37,360
really

00:05:34,880 --> 00:05:38,080
pressing him on like the impact of

00:05:37,360 --> 00:05:39,919
actually being

00:05:38,080 --> 00:05:42,240
successful in what he was trying to do

00:05:39,919 --> 00:05:44,400
creating a perfect deep fake

00:05:42,240 --> 00:05:46,479
and he he said he had this quote and it

00:05:44,400 --> 00:05:48,160
just struck me right away and it sort of

00:05:46,479 --> 00:05:50,000
stuck with me since he said

00:05:48,160 --> 00:05:51,680
that's the thing about humans right we

00:05:50,000 --> 00:05:54,160
only react when people have

00:05:51,680 --> 00:05:55,440
suffered from something problems are

00:05:54,160 --> 00:05:57,039
gonna happen out of this

00:05:55,440 --> 00:05:58,560
and we're gonna build something and then

00:05:57,039 --> 00:06:01,759
we're gonna build something to fix it

00:05:58,560 --> 00:06:02,720
after people have suffered um and it

00:06:01,759 --> 00:06:04,560
struck me this

00:06:02,720 --> 00:06:07,039
this comment that he made because it was

00:06:04,560 --> 00:06:08,319
really indicative of the attitudes that

00:06:07,039 --> 00:06:10,560
i was experiencing

00:06:08,319 --> 00:06:12,400
technology at the time tech had this

00:06:10,560 --> 00:06:14,240
attitude of like well well let's make

00:06:12,400 --> 00:06:15,840
money now and we'll fix it

00:06:14,240 --> 00:06:17,680
when enough people suffer we'll worry

00:06:15,840 --> 00:06:19,680
about safety after we've made all the

00:06:17,680 --> 00:06:21,600
money

00:06:19,680 --> 00:06:23,440
and so one of the reasons why this is

00:06:21,600 --> 00:06:24,880
that a lot of private sector companies

00:06:23,440 --> 00:06:27,360
kind of see their products as

00:06:24,880 --> 00:06:28,560
toys and have real trouble imagining the

00:06:27,360 --> 00:06:30,880
consequences

00:06:28,560 --> 00:06:33,120
of what they're building um engineers

00:06:30,880 --> 00:06:34,560
also skew younger in the private sector

00:06:33,120 --> 00:06:36,560
and have limited experience

00:06:34,560 --> 00:06:38,160
therefore limited maturity and the

00:06:36,560 --> 00:06:41,039
defense industry

00:06:38,160 --> 00:06:41,440
human suffering is not hypothetical when

00:06:41,039 --> 00:06:43,039
i

00:06:41,440 --> 00:06:44,479
came into the defense industry what i

00:06:43,039 --> 00:06:45,840
realized is that i was having these

00:06:44,479 --> 00:06:48,800
conversations with people that

00:06:45,840 --> 00:06:51,039
actually made these decisions that had

00:06:48,800 --> 00:06:54,160
these kind of life-or-death consequences

00:06:51,039 --> 00:06:56,160
for both their people and

00:06:54,160 --> 00:06:58,160
other people civilians in other

00:06:56,160 --> 00:06:58,800
countries so on and so forth so for them

00:06:58,160 --> 00:07:01,120
it really

00:06:58,800 --> 00:07:02,319
this idea of like software should be

00:07:01,120 --> 00:07:04,560
safe and we should

00:07:02,319 --> 00:07:05,360
explore the consequences of how it's

00:07:04,560 --> 00:07:09,120
built

00:07:05,360 --> 00:07:10,880
on safety really resonated very deeply

00:07:09,120 --> 00:07:13,280
for them and they took it seriously in a

00:07:10,880 --> 00:07:17,440
way that i had not been exposed to

00:07:13,280 --> 00:07:19,120
before so um

00:07:17,440 --> 00:07:20,560
one of the reasons why this the the

00:07:19,120 --> 00:07:21,280
opportunities in this space are so

00:07:20,560 --> 00:07:24,479
compelling

00:07:21,280 --> 00:07:27,360
and so interesting to me is that

00:07:24,479 --> 00:07:27,759
um the business world frequently copies

00:07:27,360 --> 00:07:30,400
from

00:07:27,759 --> 00:07:30,960
leadership in the defense sector and the

00:07:30,400 --> 00:07:33,360
defense

00:07:30,960 --> 00:07:35,680
market itself is a huge huge market

00:07:33,360 --> 00:07:39,840
where like literally billions of dollars

00:07:35,680 --> 00:07:42,560
are are traded all the time and

00:07:39,840 --> 00:07:44,560
ultimately people copy what seems to be

00:07:42,560 --> 00:07:48,000
making other people a lot of money

00:07:44,560 --> 00:07:50,400
so the idea that i could develop these

00:07:48,000 --> 00:07:51,120
um concepts and approaches and this

00:07:50,400 --> 00:07:53,199
process in

00:07:51,120 --> 00:07:54,639
in a space where it was supported and

00:07:53,199 --> 00:07:57,840
encouraged and seen

00:07:54,639 --> 00:07:59,680
as a good thing and then if we were very

00:07:57,840 --> 00:08:01,520
successful in doing what we're doing

00:07:59,680 --> 00:08:03,280
that that might spread back into the

00:08:01,520 --> 00:08:04,160
private sector that people would copy

00:08:03,280 --> 00:08:06,960
these things

00:08:04,160 --> 00:08:07,599
was a really really attractive idea to

00:08:06,960 --> 00:08:09,199
me

00:08:07,599 --> 00:08:10,800
that we could potentially have an

00:08:09,199 --> 00:08:14,479
inflection point

00:08:10,800 --> 00:08:15,520
where um our methods are copied just far

00:08:14,479 --> 00:08:17,280
and wide

00:08:15,520 --> 00:08:19,120
outside of the defense industry in and

00:08:17,280 --> 00:08:21,039
of itself

00:08:19,120 --> 00:08:22,560
so the essential question that i'm

00:08:21,039 --> 00:08:24,479
interested in exploring

00:08:22,560 --> 00:08:26,639
as a software engineer is can we change

00:08:24,479 --> 00:08:30,160
the software development process

00:08:26,639 --> 00:08:32,719
to help make software itself safer

00:08:30,160 --> 00:08:34,159
and so let's go back to the isolationist

00:08:32,719 --> 00:08:35,839
approach

00:08:34,159 --> 00:08:37,919
generally when people come into the

00:08:35,839 --> 00:08:40,959
defense sector from

00:08:37,919 --> 00:08:43,279
the private sector and they want to

00:08:40,959 --> 00:08:45,040
kind of focus on more responsible

00:08:43,279 --> 00:08:45,920
software they kind of gravitate to this

00:08:45,040 --> 00:08:48,240
idea of well we'll

00:08:45,920 --> 00:08:50,240
only build things that have defensive

00:08:48,240 --> 00:08:51,760
applications no weapons we won't touch

00:08:50,240 --> 00:08:54,000
the weaponry at all

00:08:51,760 --> 00:08:55,200
and in in the beginning this seems quite

00:08:54,000 --> 00:08:57,200
sensible because

00:08:55,200 --> 00:08:58,959
uh the military well the government as a

00:08:57,200 --> 00:09:00,240
whole but the military specifically is

00:08:58,959 --> 00:09:02,560
dealing with the state that we

00:09:00,240 --> 00:09:04,320
like to refer to as technology poverty

00:09:02,560 --> 00:09:05,760
uh people who've never been exposed to

00:09:04,320 --> 00:09:07,680
defense tend to assume

00:09:05,760 --> 00:09:09,680
that what they see in movies and tv is

00:09:07,680 --> 00:09:10,959
an accurate reflection of what military

00:09:09,680 --> 00:09:12,959
technology is like

00:09:10,959 --> 00:09:14,399
and what has always struck me as ironic

00:09:12,959 --> 00:09:16,560
about that is that if you talk to

00:09:14,399 --> 00:09:19,040
software engineers about how hollywood

00:09:16,560 --> 00:09:19,920
depicts like their life and their

00:09:19,040 --> 00:09:22,000
careers

00:09:19,920 --> 00:09:24,080
they're just full of examples of like

00:09:22,000 --> 00:09:25,519
hilarious mistakes with technology and

00:09:24,080 --> 00:09:27,120
how technology works

00:09:25,519 --> 00:09:29,440
and yet we trust hollywood to get the

00:09:27,120 --> 00:09:31,360
military right well the reality of it is

00:09:29,440 --> 00:09:33,040
that the basic technology that you

00:09:31,360 --> 00:09:35,040
interact with in the private sector

00:09:33,040 --> 00:09:36,560
and even a mid-sized company is

00:09:35,040 --> 00:09:39,040
completely absent

00:09:36,560 --> 00:09:40,160
in the defense sector they don't have a

00:09:39,040 --> 00:09:41,920
lot of the tooling

00:09:40,160 --> 00:09:43,519
that we have in the private sector they

00:09:41,920 --> 00:09:44,480
don't have a lot of the capabilities

00:09:43,519 --> 00:09:47,360
that we have in the

00:09:44,480 --> 00:09:48,240
private sector they are working in some

00:09:47,360 --> 00:09:50,640
cases

00:09:48,240 --> 00:09:51,760
on spreadsheets in other cases on

00:09:50,640 --> 00:09:55,200
completely manual

00:09:51,760 --> 00:09:58,320
paper processes um

00:09:55,200 --> 00:10:00,560
but it might seem like there's a lot of

00:09:58,320 --> 00:10:01,760
low-hanging fruit in this like purely

00:10:00,560 --> 00:10:04,399
defensive

00:10:01,760 --> 00:10:05,600
area um but part of the problem with the

00:10:04,399 --> 00:10:09,200
offensive versus

00:10:05,600 --> 00:10:11,040
defensive dichotomy um is that

00:10:09,200 --> 00:10:12,959
really often the difference between

00:10:11,040 --> 00:10:14,000
offensive and defensive is only really

00:10:12,959 --> 00:10:16,720
determined by

00:10:14,000 --> 00:10:18,000
who's on what side of the technology i

00:10:16,720 --> 00:10:19,600
have a colleague who who

00:10:18,000 --> 00:10:21,440
put it really succinctly in a way i

00:10:19,600 --> 00:10:24,240
really i really liked

00:10:21,440 --> 00:10:26,320
he said the that they when he was in the

00:10:24,240 --> 00:10:29,279
service there was all sorts of chatter

00:10:26,320 --> 00:10:30,959
about um military aid and whether this

00:10:29,279 --> 00:10:32,800
missile system was offensive or this

00:10:30,959 --> 00:10:34,000
missile system was defensive and he said

00:10:32,800 --> 00:10:35,120
the only real difference between a

00:10:34,000 --> 00:10:36,959
missile system

00:10:35,120 --> 00:10:38,880
and offensive defensive is who we're

00:10:36,959 --> 00:10:40,480
pointing it out if it's pointing at us

00:10:38,880 --> 00:10:42,240
it's an offensive system if it's

00:10:40,480 --> 00:10:44,000
pointing at our enemies it's a defensive

00:10:42,240 --> 00:10:45,760
system that's the only difference and so

00:10:44,000 --> 00:10:48,079
that made the whole conversation

00:10:45,760 --> 00:10:50,399
extremely difficult for them to have it

00:10:48,079 --> 00:10:52,000
almost seemed pointless

00:10:50,399 --> 00:10:54,399
and so the other problem with this

00:10:52,000 --> 00:10:56,240
dichotomy of offensive defensive

00:10:54,399 --> 00:10:58,079
is that the reality of it is that any

00:10:56,240 --> 00:11:00,000
technology that makes the military more

00:10:58,079 --> 00:11:02,079
efficient can lead to more injury

00:11:00,000 --> 00:11:03,519
and unnecessary death if the fundamental

00:11:02,079 --> 00:11:06,000
decision-making process

00:11:03,519 --> 00:11:07,839
isn't itself safe so you don't

00:11:06,000 --> 00:11:10,160
necessarily get very far

00:11:07,839 --> 00:11:12,959
just kind of focusing on defensive

00:11:10,160 --> 00:11:15,040
technology only

00:11:12,959 --> 00:11:16,640
instead one of the ways that we decided

00:11:15,040 --> 00:11:18,160
that we wanted to think about this at

00:11:16,640 --> 00:11:20,079
rebellion was this idea of

00:11:18,160 --> 00:11:21,440
escalation versus de-escalation we

00:11:20,079 --> 00:11:22,240
didn't want to worry too much about

00:11:21,440 --> 00:11:23,920
whether we were

00:11:22,240 --> 00:11:25,440
we're touching the offensive side of the

00:11:23,920 --> 00:11:28,399
military we wanted

00:11:25,440 --> 00:11:30,399
to acknowledge that no one wants the

00:11:28,399 --> 00:11:31,760
military to act too early or

00:11:30,399 --> 00:11:35,279
unnecessarily

00:11:31,760 --> 00:11:37,760
that people want to everything to be

00:11:35,279 --> 00:11:39,200
appropriate and effective and right size

00:11:37,760 --> 00:11:40,720
and so what's important to understand

00:11:39,200 --> 00:11:42,560
about de-escalation

00:11:40,720 --> 00:11:44,560
is a de-escalation doesn't mean there

00:11:42,560 --> 00:11:46,640
isn't a conflict it doesn't mean that

00:11:44,560 --> 00:11:49,440
you avoid conflict or that you eliminate

00:11:46,640 --> 00:11:50,079
conflict but it means that you give the

00:11:49,440 --> 00:11:52,000
the group

00:11:50,079 --> 00:11:53,920
of people who need to make a decision

00:11:52,000 --> 00:11:55,120
more time and more information with

00:11:53,920 --> 00:11:56,880
which to check

00:11:55,120 --> 00:11:59,360
to make sure that the way they're

00:11:56,880 --> 00:12:01,279
responding is in fact the right way to

00:11:59,360 --> 00:12:04,720
respond

00:12:01,279 --> 00:12:07,279
so current military technology

00:12:04,720 --> 00:12:07,760
kind of leans towards escalating things

00:12:07,279 --> 00:12:10,079
in

00:12:07,760 --> 00:12:11,360
in some interesting ways and so there

00:12:10,079 --> 00:12:14,720
are three

00:12:11,360 --> 00:12:16,639
basic problems with um i would say all

00:12:14,720 --> 00:12:17,680
technology but technology in this space

00:12:16,639 --> 00:12:20,880
in particular

00:12:17,680 --> 00:12:22,240
uh first of all the the brand that the

00:12:20,880 --> 00:12:24,639
military uses

00:12:22,240 --> 00:12:26,639
uses colors and design elements that we

00:12:24,639 --> 00:12:28,399
know make people more aggressive

00:12:26,639 --> 00:12:29,760
make them more prone to jump to

00:12:28,399 --> 00:12:32,880
conclusions make them

00:12:29,760 --> 00:12:34,800
more prone to get excited um machine

00:12:32,880 --> 00:12:35,680
learning and ai companies frequently

00:12:34,800 --> 00:12:37,600
mislead

00:12:35,680 --> 00:12:39,519
about the accuracy of their products or

00:12:37,600 --> 00:12:40,880
how the conclusions are drawn from them

00:12:39,519 --> 00:12:42,720
and then the last point is that there

00:12:40,880 --> 00:12:44,480
are fundamental misunderstandings about

00:12:42,720 --> 00:12:45,120
automation itself so let's go through

00:12:44,480 --> 00:12:48,240
these

00:12:45,120 --> 00:12:50,480
pretty quickly so military brands

00:12:48,240 --> 00:12:52,399
generally speaking the software that's

00:12:50,480 --> 00:12:54,639
designed for the military

00:12:52,399 --> 00:12:56,399
will tend to lean on the colors towards

00:12:54,639 --> 00:12:56,880
the right side of the spectrum which are

00:12:56,399 --> 00:12:59,600
all

00:12:56,880 --> 00:13:00,240
warm active colors and less on the

00:12:59,600 --> 00:13:02,800
colors

00:13:00,240 --> 00:13:03,600
on sorry on the left side and less on

00:13:02,800 --> 00:13:06,079
the colors

00:13:03,600 --> 00:13:08,320
on the right side which are cool kind of

00:13:06,079 --> 00:13:10,160
more more calming colors

00:13:08,320 --> 00:13:12,240
and what's interesting about this is

00:13:10,160 --> 00:13:13,360
that if you go into other industries

00:13:12,240 --> 00:13:16,000
that have a really

00:13:13,360 --> 00:13:17,680
strong developed discipline around

00:13:16,000 --> 00:13:20,720
safety

00:13:17,680 --> 00:13:22,880
you'll see that they are very careful

00:13:20,720 --> 00:13:24,240
and strict about how you are

00:13:22,880 --> 00:13:26,480
interrupting people and

00:13:24,240 --> 00:13:28,720
how you are triggering those those kind

00:13:26,480 --> 00:13:31,920
of excitement pathways in the brain

00:13:28,720 --> 00:13:34,639
so for example this diagram is um

00:13:31,920 --> 00:13:35,760
from the aviation industry and it kind

00:13:34,639 --> 00:13:38,079
of lays out

00:13:35,760 --> 00:13:38,959
their policies around alerting as an

00:13:38,079 --> 00:13:41,120
industry

00:13:38,959 --> 00:13:42,880
so the conditions in which they will

00:13:41,120 --> 00:13:46,160
allow a strong inter

00:13:42,880 --> 00:13:46,480
interrupt in the uh the cockpit i mean

00:13:46,160 --> 00:13:48,000
that

00:13:46,480 --> 00:13:49,839
if the pilot doesn't do something

00:13:48,000 --> 00:13:52,000
immediately the plane is going to fall

00:13:49,839 --> 00:13:54,240
out of the sky and that is the standard

00:13:52,000 --> 00:13:56,000
and everything else has weaker and

00:13:54,240 --> 00:13:58,399
weaker and weaker interrupts and they're

00:13:56,000 --> 00:14:00,800
very specific in their guidelines around

00:13:58,399 --> 00:14:02,880
what it means in terms of colors you can

00:14:00,800 --> 00:14:04,800
use what it means in terms of like how

00:14:02,880 --> 00:14:07,839
that alert manifests

00:14:04,800 --> 00:14:08,880
um and in order to minimize both

00:14:07,839 --> 00:14:11,680
distraction but

00:14:08,880 --> 00:14:12,880
also ensure clear decision making from

00:14:11,680 --> 00:14:15,519
the pilots

00:14:12,880 --> 00:14:17,120
so you're gonna hear me talk about this

00:14:15,519 --> 00:14:20,079
idea of system safety

00:14:17,120 --> 00:14:22,240
or safety science and um it's an

00:14:20,079 --> 00:14:24,959
academic discipline that's grounded

00:14:22,240 --> 00:14:26,160
in our learnings our lessons learned

00:14:24,959 --> 00:14:29,440
from aviation

00:14:26,160 --> 00:14:31,600
nuclear energy medicine etc

00:14:29,440 --> 00:14:33,839
one of the fundamental concepts of it is

00:14:31,600 --> 00:14:36,000
this idea of empowering the operator

00:14:33,839 --> 00:14:38,000
that a person on the ground needs as

00:14:36,000 --> 00:14:40,240
much discretion as possible

00:14:38,000 --> 00:14:42,000
and when it comes to computer science we

00:14:40,240 --> 00:14:44,959
don't really have a sense

00:14:42,000 --> 00:14:46,240
of safety science we don't really have

00:14:44,959 --> 00:14:47,040
we haven't really integrated those

00:14:46,240 --> 00:14:50,560
concepts

00:14:47,040 --> 00:14:51,279
into uh our process in a real way just

00:14:50,560 --> 00:14:55,040
yet

00:14:51,279 --> 00:14:57,360
but for the purposes of um

00:14:55,040 --> 00:14:59,120
what i do in our conversation here it

00:14:57,360 --> 00:15:01,199
really leans heavily on this idea of

00:14:59,120 --> 00:15:03,199
human computer interaction like the

00:15:01,199 --> 00:15:05,519
computer doesn't replace anything it

00:15:03,199 --> 00:15:08,560
supports

00:15:05,519 --> 00:15:09,600
and so to the second point like ai black

00:15:08,560 --> 00:15:14,079
boxes

00:15:09,600 --> 00:15:15,199
um the the main problem with ai machine

00:15:14,079 --> 00:15:16,880
learning companies

00:15:15,199 --> 00:15:18,800
is that what incentivizes them to

00:15:16,880 --> 00:15:22,240
produce accurate

00:15:18,800 --> 00:15:24,320
aiml products um

00:15:22,240 --> 00:15:26,079
are forces that can be satisfied by just

00:15:24,320 --> 00:15:29,199
the perception of accuracy

00:15:26,079 --> 00:15:31,279
and so this is not exclusive to ai

00:15:29,199 --> 00:15:33,279
we see this the same sort of problem

00:15:31,279 --> 00:15:36,560
with diversity

00:15:33,279 --> 00:15:37,759
um with greenwashing organic and we also

00:15:36,560 --> 00:15:40,320
see it with safety

00:15:37,759 --> 00:15:41,519
where the perception of being safe the

00:15:40,320 --> 00:15:43,759
perception of being

00:15:41,519 --> 00:15:45,600
organic or environmentally friendly

00:15:43,759 --> 00:15:46,800
gives you the same benefit as actually

00:15:45,600 --> 00:15:48,880
being that way

00:15:46,800 --> 00:15:50,639
and so hey when ai machine learning

00:15:48,880 --> 00:15:51,360
common companies are not honest about

00:15:50,639 --> 00:15:53,759
like what

00:15:51,360 --> 00:15:55,759
exactly their products are doing how

00:15:53,759 --> 00:15:58,000
they reach the conclusion the confidence

00:15:55,759 --> 00:16:00,560
level of certain types of analysis

00:15:58,000 --> 00:16:02,399
it creates a situation where the user is

00:16:00,560 --> 00:16:04,160
going to make a lot of errors based on

00:16:02,399 --> 00:16:05,759
that analysis and that becomes very

00:16:04,160 --> 00:16:07,839
problematic

00:16:05,759 --> 00:16:08,880
and so the last thing is this idea of

00:16:07,839 --> 00:16:10,800
the fundamental

00:16:08,880 --> 00:16:12,240
misunderstandings that people have about

00:16:10,800 --> 00:16:14,880
automation um

00:16:12,240 --> 00:16:15,279
you you have likely heard people say

00:16:14,880 --> 00:16:17,519
this

00:16:15,279 --> 00:16:19,279
a lot and we'll continue to hear people

00:16:17,519 --> 00:16:20,320
say this a lot that automation will

00:16:19,279 --> 00:16:21,920
replace jobs

00:16:20,320 --> 00:16:23,680
it's going to remove jobs but the

00:16:21,920 --> 00:16:24,160
reality is that automation doesn't

00:16:23,680 --> 00:16:27,279
actually

00:16:24,160 --> 00:16:28,160
do that it redistributes things so

00:16:27,279 --> 00:16:31,199
automation

00:16:28,160 --> 00:16:31,839
redistributes human labor while it may

00:16:31,199 --> 00:16:35,199
eliminate

00:16:31,839 --> 00:16:37,360
a job by automating a process it's also

00:16:35,199 --> 00:16:39,519
creating jobs because you need people to

00:16:37,360 --> 00:16:41,279
write that automation and then maintain

00:16:39,519 --> 00:16:43,519
it over time

00:16:41,279 --> 00:16:45,040
the problem from the political point of

00:16:43,519 --> 00:16:47,199
view is that the people who lose their

00:16:45,040 --> 00:16:49,440
jobs to automation are generally not the

00:16:47,199 --> 00:16:51,680
same people who get the new jobs

00:16:49,440 --> 00:16:53,360
as a result of the automation but for

00:16:51,680 --> 00:16:55,120
the purpose of this conversation what we

00:16:53,360 --> 00:16:57,519
need to really understand is that it's

00:16:55,120 --> 00:16:58,320
this redistribution affects our ability

00:16:57,519 --> 00:17:02,399
to adapt

00:16:58,320 --> 00:17:04,400
and it changes what we do so

00:17:02,399 --> 00:17:05,600
automation redistributes human thought

00:17:04,400 --> 00:17:08,640
process and

00:17:05,600 --> 00:17:10,880
uh in in doing so it also

00:17:08,640 --> 00:17:12,799
redistributes where the decisions are

00:17:10,880 --> 00:17:14,319
being made within a process

00:17:12,799 --> 00:17:16,160
empowerment is a really important

00:17:14,319 --> 00:17:16,720
concept in this because it's like we

00:17:16,160 --> 00:17:18,720
need to

00:17:16,720 --> 00:17:19,760
give the human operator freedom to make

00:17:18,720 --> 00:17:21,760
decisions

00:17:19,760 --> 00:17:23,360
and in order to de-escalate we already

00:17:21,760 --> 00:17:25,760
said that we need to create more time

00:17:23,360 --> 00:17:26,880
and space for thought

00:17:25,760 --> 00:17:28,720
a lot of things something that you'll

00:17:26,880 --> 00:17:28,960
hear ethical ai people say all the time

00:17:28,720 --> 00:17:32,320
is

00:17:28,960 --> 00:17:33,120
keep human in the loop humans in the

00:17:32,320 --> 00:17:34,799
loop

00:17:33,120 --> 00:17:36,480
is the equivalent of empower the

00:17:34,799 --> 00:17:39,280
operator it basically means like

00:17:36,480 --> 00:17:41,919
computers should not be making decisions

00:17:39,280 --> 00:17:43,919
people should be making decisions but

00:17:41,919 --> 00:17:46,000
the problem with that is how do we know

00:17:43,919 --> 00:17:47,520
if a computer is making a decision or

00:17:46,000 --> 00:17:49,200
not

00:17:47,520 --> 00:17:51,360
well all right let's suppose we have an

00:17:49,200 --> 00:17:53,200
object detection algorithm

00:17:51,360 --> 00:17:55,200
and this object detection algorithm

00:17:53,200 --> 00:17:57,360
takes this picture of an adorable kitten

00:17:55,200 --> 00:17:58,080
in a wheelchair and identifies it as a

00:17:57,360 --> 00:18:00,320
cat

00:17:58,080 --> 00:18:01,360
is that a decision on the surface of it

00:18:00,320 --> 00:18:04,080
it doesn't seem like

00:18:01,360 --> 00:18:05,919
right just it's object detection it has

00:18:04,080 --> 00:18:08,640
detected an object and correctly

00:18:05,919 --> 00:18:10,480
labeled it but what if this object

00:18:08,640 --> 00:18:12,080
detection algorithm sees this picture of

00:18:10,480 --> 00:18:13,919
an adorable kitten in a wheelchair and

00:18:12,080 --> 00:18:14,240
gets confused by the wheelchair and goes

00:18:13,919 --> 00:18:15,840
no

00:18:14,240 --> 00:18:18,240
this is a pair of scissors it's not a

00:18:15,840 --> 00:18:20,320
cat is that a decision

00:18:18,240 --> 00:18:22,799
right well it really depends on the

00:18:20,320 --> 00:18:25,120
context right if our process

00:18:22,799 --> 00:18:27,360
is such that we are doing object

00:18:25,120 --> 00:18:30,080
detection on this in a way where a human

00:18:27,360 --> 00:18:30,720
can sort of fact check the algorithm and

00:18:30,080 --> 00:18:32,799
so that

00:18:30,720 --> 00:18:34,320
those mistakes are very easily

00:18:32,799 --> 00:18:36,240
identified

00:18:34,320 --> 00:18:38,080
then it's probably not a decision

00:18:36,240 --> 00:18:40,559
however if we're doing it

00:18:38,080 --> 00:18:41,600
over the course of millions of photos to

00:18:40,559 --> 00:18:42,960
the point that

00:18:41,600 --> 00:18:44,400
humans are never going to look at these

00:18:42,960 --> 00:18:45,760
things and then maybe these

00:18:44,400 --> 00:18:47,840
determinations

00:18:45,760 --> 00:18:49,360
actually have a real impact when they're

00:18:47,840 --> 00:18:51,600
wrong you

00:18:49,360 --> 00:18:53,360
would look at that as like the computer

00:18:51,600 --> 00:18:54,720
essentially making a decision for the

00:18:53,360 --> 00:18:58,080
human

00:18:54,720 --> 00:19:00,640
and so human supervision may not scale

00:18:58,080 --> 00:19:03,919
which what makes keeping human humans in

00:19:00,640 --> 00:19:06,080
the loop as a directive for ethical ai

00:19:03,919 --> 00:19:08,080
well intended but somewhat difficult to

00:19:06,080 --> 00:19:09,679
implement

00:19:08,080 --> 00:19:11,360
um so let's go back to this idea of

00:19:09,679 --> 00:19:12,799
examining the distribution of human

00:19:11,360 --> 00:19:14,320
thought

00:19:12,799 --> 00:19:16,480
some of you may be familiar with this

00:19:14,320 --> 00:19:18,160
concept if you have read the book

00:19:16,480 --> 00:19:21,200
thinking fast and slow

00:19:18,160 --> 00:19:22,559
i call it type 1 type 2 thinking in my

00:19:21,200 --> 00:19:24,320
writing in this talk

00:19:22,559 --> 00:19:25,840
it's also called system one system two

00:19:24,320 --> 00:19:28,320
thinking the the

00:19:25,840 --> 00:19:28,880
idea behind it is that we have different

00:19:28,320 --> 00:19:31,919
speeds

00:19:28,880 --> 00:19:32,480
and intensity of thinking um type one is

00:19:31,919 --> 00:19:34,640
very

00:19:32,480 --> 00:19:35,760
pattern matching based it's called fast

00:19:34,640 --> 00:19:39,039
thinking it's

00:19:35,760 --> 00:19:39,919
like versus like whereas type 2 is more

00:19:39,039 --> 00:19:42,320
calculation

00:19:39,919 --> 00:19:44,480
analysis slow thinking more resources

00:19:42,320 --> 00:19:46,720
resource intensive

00:19:44,480 --> 00:19:48,320
um and what we think happens the

00:19:46,720 --> 00:19:50,080
combination of these two thinking

00:19:48,320 --> 00:19:51,360
is that the human brain generally has

00:19:50,080 --> 00:19:53,280
this constant strain

00:19:51,360 --> 00:19:55,679
stream of type 1 thinking and then it

00:19:53,280 --> 00:19:56,880
uses type 2 thinking strategically and

00:19:55,679 --> 00:20:00,799
random intervals

00:19:56,880 --> 00:20:02,240
to error check the type 1 thinking

00:20:00,799 --> 00:20:04,799
um and what's interesting about this is

00:20:02,240 --> 00:20:06,880
that computers and humans actually have

00:20:04,799 --> 00:20:07,679
complementary relationships with type 1

00:20:06,880 --> 00:20:11,679
and type 2

00:20:07,679 --> 00:20:13,840
thinking so this cartoon is from xkcd

00:20:11,679 --> 00:20:15,360
and it sort of illustrates this concept

00:20:13,840 --> 00:20:16,720
quite nicely

00:20:15,360 --> 00:20:18,799
um something called the more of x

00:20:16,720 --> 00:20:21,200
paradox which is basically

00:20:18,799 --> 00:20:22,640
from the very early days of ai we have

00:20:21,200 --> 00:20:23,280
realized that things that were really

00:20:22,640 --> 00:20:25,280
easy for

00:20:23,280 --> 00:20:27,280
humans to do are actually really hard

00:20:25,280 --> 00:20:28,000
and resource intensive for computers to

00:20:27,280 --> 00:20:29,840
do but

00:20:28,000 --> 00:20:31,760
calculations that are really hard for

00:20:29,840 --> 00:20:33,200
humans to do are actually really easy

00:20:31,760 --> 00:20:35,679
for a computer to do

00:20:33,200 --> 00:20:37,919
so in this comic there are two tasks

00:20:35,679 --> 00:20:40,320
that we're trying to do we're trying to

00:20:37,919 --> 00:20:40,960
check whether a given picture is taken

00:20:40,320 --> 00:20:43,280
in a

00:20:40,960 --> 00:20:44,720
national park and then check whether

00:20:43,280 --> 00:20:47,280
it's a photo of a bird

00:20:44,720 --> 00:20:48,880
and the photo of the bird part is really

00:20:47,280 --> 00:20:50,880
hard for the computer to do

00:20:48,880 --> 00:20:53,039
and the geolocation part the national

00:20:50,880 --> 00:20:54,080
park part is really easy but that's the

00:20:53,039 --> 00:20:56,000
exact opposite

00:20:54,080 --> 00:20:58,080
of the humans relationship with these

00:20:56,000 --> 00:21:00,000
two tasks

00:20:58,080 --> 00:21:02,159
and so where a lot of our thinking

00:21:00,000 --> 00:21:05,679
around ai and machine learning

00:21:02,159 --> 00:21:07,200
kind of fails is that we assume

00:21:05,679 --> 00:21:08,720
that what we want to do to make a

00:21:07,200 --> 00:21:09,679
process better with this type of

00:21:08,720 --> 00:21:12,480
automation

00:21:09,679 --> 00:21:14,000
is to simply use the computer to replace

00:21:12,480 --> 00:21:15,840
all the type 1 thinking

00:21:14,000 --> 00:21:17,440
but the problem with that is that it

00:21:15,840 --> 00:21:18,080
kind of leaves the type 2 thinking

00:21:17,440 --> 00:21:19,760
homeless

00:21:18,080 --> 00:21:21,760
it means that it's got no place to plug

00:21:19,760 --> 00:21:25,360
in because the human is no longer doing

00:21:21,760 --> 00:21:27,280
the type 1 thinking the computer is

00:21:25,360 --> 00:21:28,720
and so a lot of people sort of default

00:21:27,280 --> 00:21:30,240
to this well that's okay

00:21:28,720 --> 00:21:32,480
because computers are more accurate than

00:21:30,240 --> 00:21:34,400
people so the computer doesn't need the

00:21:32,480 --> 00:21:36,880
type 2 error checking

00:21:34,400 --> 00:21:37,600
and yeah that's not that's not really

00:21:36,880 --> 00:21:40,320
true

00:21:37,600 --> 00:21:40,880
um computers are superior at pattern

00:21:40,320 --> 00:21:43,520
matching

00:21:40,880 --> 00:21:44,080
when you've given them a clearly defined

00:21:43,520 --> 00:21:47,200
known

00:21:44,080 --> 00:21:49,200
pattern but computers are really bad at

00:21:47,200 --> 00:21:50,640
identifying the patterns themselves

00:21:49,200 --> 00:21:52,400
um and the computer doesn't know the

00:21:50,640 --> 00:21:55,440
significance of the pattern

00:21:52,400 --> 00:21:57,520
and so as a result when we replace human

00:21:55,440 --> 00:21:58,000
type one thinking with computer type one

00:21:57,520 --> 00:22:00,080
thinking

00:21:58,000 --> 00:22:01,440
and removal the error checking the type

00:22:00,080 --> 00:22:03,919
two brings to the table

00:22:01,440 --> 00:22:05,120
we get biases we get overfitting and we

00:22:03,919 --> 00:22:07,520
have the inability

00:22:05,120 --> 00:22:08,320
to adapt to changing circumstances all

00:22:07,520 --> 00:22:11,200
of which

00:22:08,320 --> 00:22:12,559
are things that lead to unsafe outcomes

00:22:11,200 --> 00:22:15,679
um with

00:22:12,559 --> 00:22:16,400
software products and so we have to sort

00:22:15,679 --> 00:22:19,039
of think about

00:22:16,400 --> 00:22:20,880
how we reason about problems if we are

00:22:19,039 --> 00:22:22,559
thinking about problems in terms of

00:22:20,880 --> 00:22:23,840
replacement meaning that we keep the

00:22:22,559 --> 00:22:26,720
process the same way

00:22:23,840 --> 00:22:27,120
and the way it is and we just use the ai

00:22:26,720 --> 00:22:29,360
to do

00:22:27,120 --> 00:22:31,039
the human bits um in order to make the

00:22:29,360 --> 00:22:32,720
process faster

00:22:31,039 --> 00:22:34,240
we're going to remove our error checking

00:22:32,720 --> 00:22:35,440
and we also remove an element of a

00:22:34,240 --> 00:22:38,799
resilience because we

00:22:35,440 --> 00:22:40,799
can't adapt on the fly um

00:22:38,799 --> 00:22:42,400
and then because we remove those two

00:22:40,799 --> 00:22:46,159
things we become more vulnerable

00:22:42,400 --> 00:22:48,559
to cascading failures and bias and um

00:22:46,159 --> 00:22:50,720
it also reinforces existing structures

00:22:48,559 --> 00:22:51,440
even if they are no longer relevant

00:22:50,720 --> 00:22:55,440
which

00:22:51,440 --> 00:22:57,440
is broadly a bad thing so this idea of

00:22:55,440 --> 00:22:58,960
thinking and replacements is appealing

00:22:57,440 --> 00:23:00,799
because when when people talk about

00:22:58,960 --> 00:23:02,159
process they tend to think of process in

00:23:00,799 --> 00:23:02,799
the following manner they tend to think

00:23:02,159 --> 00:23:06,000
of process

00:23:02,799 --> 00:23:08,559
as being this nice perfectly curated

00:23:06,000 --> 00:23:10,400
orderly step by step thing where there's

00:23:08,559 --> 00:23:11,360
step one and then step two step three

00:23:10,400 --> 00:23:13,600
step four

00:23:11,360 --> 00:23:14,400
and if they they correctly identify that

00:23:13,600 --> 00:23:16,000
step two

00:23:14,400 --> 00:23:17,440
and step three can be handled by a

00:23:16,000 --> 00:23:20,000
computer then their

00:23:17,440 --> 00:23:21,600
their natural assumption about improving

00:23:20,000 --> 00:23:24,080
this system and solving this problem

00:23:21,600 --> 00:23:24,880
is to go from step one to some sort of

00:23:24,080 --> 00:23:27,520
computer

00:23:24,880 --> 00:23:28,880
generated step two step three to step

00:23:27,520 --> 00:23:30,080
four and that that's how they improve

00:23:28,880 --> 00:23:31,679
the process

00:23:30,080 --> 00:23:33,760
but the reality of processes is that

00:23:31,679 --> 00:23:35,600
they very rarely look that clean they

00:23:33,760 --> 00:23:36,320
tend to look a little bit more like this

00:23:35,600 --> 00:23:38,640
where you

00:23:36,320 --> 00:23:40,799
have a step one you have a step in

00:23:38,640 --> 00:23:43,600
between step one and step two

00:23:40,799 --> 00:23:45,120
that's undocumented but is necessary in

00:23:43,600 --> 00:23:48,000
order to get to step two

00:23:45,120 --> 00:23:48,799
um you have uh at the end of step three

00:23:48,000 --> 00:23:51,200
you have a

00:23:48,799 --> 00:23:52,640
condition that is sometimes relevant and

00:23:51,200 --> 00:23:54,799
sometimes not relevant

00:23:52,640 --> 00:23:56,480
that if it happens to come up you have

00:23:54,799 --> 00:23:58,480
an inconsistency where you can't even

00:23:56,480 --> 00:24:01,039
get to step four these are how processes

00:23:58,480 --> 00:24:03,919
look in reality

00:24:01,039 --> 00:24:05,919
uh and so with the what happens

00:24:03,919 --> 00:24:08,559
generally with technology is that it

00:24:05,919 --> 00:24:11,120
tends to grow to fit the shape

00:24:08,559 --> 00:24:12,000
of all sorts of things in its

00:24:11,120 --> 00:24:13,919
environment

00:24:12,000 --> 00:24:15,440
physical restrictions policy

00:24:13,919 --> 00:24:16,480
restrictions the structure of the

00:24:15,440 --> 00:24:19,039
organization

00:24:16,480 --> 00:24:20,159
technology can and does typically become

00:24:19,039 --> 00:24:23,360
root bound

00:24:20,159 --> 00:24:23,919
where we build on top of what was built

00:24:23,360 --> 00:24:26,000
before

00:24:23,919 --> 00:24:28,159
which is in itself built based on a

00:24:26,000 --> 00:24:30,240
bunch of assumptions and environmental

00:24:28,159 --> 00:24:32,400
variables that may have since changed

00:24:30,240 --> 00:24:33,679
and so as a result we get a system like

00:24:32,400 --> 00:24:35,440
this where we're going from we're

00:24:33,679 --> 00:24:37,279
parsing xml in order to put it in a

00:24:35,440 --> 00:24:39,440
database in order to have a middleware

00:24:37,279 --> 00:24:41,600
like feed into the api when probably we

00:24:39,440 --> 00:24:45,039
can have the api just serve the xml

00:24:41,600 --> 00:24:46,799
itself if we built it today and so

00:24:45,039 --> 00:24:48,720
i tend to push back really heavily from

00:24:46,799 --> 00:24:50,480
this attitude of let's take the existing

00:24:48,720 --> 00:24:52,240
process and just add computers and that

00:24:50,480 --> 00:24:54,640
will automatically make it better

00:24:52,240 --> 00:24:56,720
because very rarely is that in fact the

00:24:54,640 --> 00:25:00,159
case

00:24:56,720 --> 00:25:03,120
so there is a word within safety science

00:25:00,159 --> 00:25:03,919
for this concept of the variation in a

00:25:03,120 --> 00:25:06,240
real process

00:25:03,919 --> 00:25:07,360
it's called drift and drift is

00:25:06,240 --> 00:25:10,400
divergence

00:25:07,360 --> 00:25:11,279
from a specific formal process uh it can

00:25:10,400 --> 00:25:13,120
look like

00:25:11,279 --> 00:25:14,960
steps being removed it can look like

00:25:13,120 --> 00:25:17,440
stuff's being added

00:25:14,960 --> 00:25:18,640
in frequently or intermittently and the

00:25:17,440 --> 00:25:19,360
important thing to understand about

00:25:18,640 --> 00:25:22,559
drift is that

00:25:19,360 --> 00:25:23,440
all processes have some form of drift

00:25:22,559 --> 00:25:26,000
there is no

00:25:23,440 --> 00:25:27,360
process the 100 of the time looks

00:25:26,000 --> 00:25:29,360
perfectly like

00:25:27,360 --> 00:25:30,640
whatever the written down version of it

00:25:29,360 --> 00:25:32,480
is

00:25:30,640 --> 00:25:33,679
and so what causes drift well there are

00:25:32,480 --> 00:25:35,919
three main causes

00:25:33,679 --> 00:25:38,080
the first is that the process is blocked

00:25:35,919 --> 00:25:39,760
and it actually can't be done the way

00:25:38,080 --> 00:25:42,400
it's supposed to be done

00:25:39,760 --> 00:25:44,080
the second is that the process includes

00:25:42,400 --> 00:25:46,799
a level of variance

00:25:44,080 --> 00:25:48,880
that is not accurately reflected in

00:25:46,799 --> 00:25:50,159
whatever the formal written down version

00:25:48,880 --> 00:25:52,960
of the process is

00:25:50,159 --> 00:25:54,720
and then the third cost of cause of

00:25:52,960 --> 00:25:56,720
drift is that the environment has

00:25:54,720 --> 00:25:57,279
changed in a way that makes the process

00:25:56,720 --> 00:25:59,279
itself

00:25:57,279 --> 00:26:01,200
unsafe and so we have to adapt on the

00:25:59,279 --> 00:26:04,640
fly

00:26:01,200 --> 00:26:08,320
so in this example of this process these

00:26:04,640 --> 00:26:10,559
two orange areas

00:26:08,320 --> 00:26:12,960
are where our drift is where we have the

00:26:10,559 --> 00:26:14,640
adaptation that makes it possible for us

00:26:12,960 --> 00:26:16,640
to get to step one to step two

00:26:14,640 --> 00:26:18,240
that's a drift and when we have the

00:26:16,640 --> 00:26:22,840
inconsistency forming

00:26:18,240 --> 00:26:24,559
due to a a random factor that's also a

00:26:22,840 --> 00:26:27,039
draft and so

00:26:24,559 --> 00:26:28,880
when we think about problems and how we

00:26:27,039 --> 00:26:30,720
want to apply technology to them what we

00:26:28,880 --> 00:26:33,919
started to do is actually to think about

00:26:30,720 --> 00:26:36,240
the process and to start to try to map

00:26:33,919 --> 00:26:38,480
that drift what we would generally do in

00:26:36,240 --> 00:26:40,559
a normal software development cycle

00:26:38,480 --> 00:26:42,720
is to create a process map of what

00:26:40,559 --> 00:26:45,520
people were doing and this is just

00:26:42,720 --> 00:26:47,520
not distilling that process map into

00:26:45,520 --> 00:26:49,200
some kind of lowest common denominator

00:26:47,520 --> 00:26:50,080
but actually maintaining that level of

00:26:49,200 --> 00:26:52,480
variance

00:26:50,080 --> 00:26:53,600
it requires a lot of user research both

00:26:52,480 --> 00:26:55,840
in interviews

00:26:53,600 --> 00:26:56,720
and observations but once we get a sense

00:26:55,840 --> 00:26:58,720
of like where

00:26:56,720 --> 00:26:59,760
there is drift then we can start to ask

00:26:58,720 --> 00:27:02,000
ourselves like

00:26:59,760 --> 00:27:03,600
what's causing that drift and like what

00:27:02,000 --> 00:27:05,600
kind of technology can we bring to the

00:27:03,600 --> 00:27:07,440
table to resolve those problems

00:27:05,600 --> 00:27:09,600
and reduce or eliminate the need for

00:27:07,440 --> 00:27:10,080
drift if drift is being caused by

00:27:09,600 --> 00:27:11,919
something

00:27:10,080 --> 00:27:13,760
actually blocking the process from

00:27:11,919 --> 00:27:15,679
moving forward we can probably

00:27:13,760 --> 00:27:16,960
produce some sort of technical solution

00:27:15,679 --> 00:27:19,279
to unblock it

00:27:16,960 --> 00:27:20,399
but if a drift is being caused by too

00:27:19,279 --> 00:27:22,720
much variation

00:27:20,399 --> 00:27:24,720
that that's a place where we know we do

00:27:22,720 --> 00:27:26,960
not want to eliminate the human in the

00:27:24,720 --> 00:27:28,399
loop we do not want to remove the human

00:27:26,960 --> 00:27:30,480
operator from being

00:27:28,399 --> 00:27:32,559
involved in the decision making process

00:27:30,480 --> 00:27:34,159
there because there's more variation

00:27:32,559 --> 00:27:36,960
than we can actually express

00:27:34,159 --> 00:27:38,159
effectively to a computer and we also

00:27:36,960 --> 00:27:38,880
kind of want to think about it in terms

00:27:38,159 --> 00:27:42,159
of like

00:27:38,880 --> 00:27:44,000
how are we distributing and changing

00:27:42,159 --> 00:27:45,360
where the thinking is like where is the

00:27:44,000 --> 00:27:47,600
type 1 thinking

00:27:45,360 --> 00:27:49,279
and what causes the operator to use type

00:27:47,600 --> 00:27:52,480
1 type 2 thinking

00:27:49,279 --> 00:27:54,399
in this particular process

00:27:52,480 --> 00:27:56,320
um going through this process sort of

00:27:54,399 --> 00:27:58,320
allows us to generate a new list of

00:27:56,320 --> 00:27:59,840
problems so rather than thinking about

00:27:58,320 --> 00:28:02,000
like what is the existing process and

00:27:59,840 --> 00:28:03,600
how can we make it faster and better we

00:28:02,000 --> 00:28:06,240
start rethinking the origin

00:28:03,600 --> 00:28:06,960
the existing process reinventing the old

00:28:06,240 --> 00:28:09,600
process

00:28:06,960 --> 00:28:11,919
by designing specifically the computer

00:28:09,600 --> 00:28:13,919
the human computer interaction

00:28:11,919 --> 00:28:15,279
our goal is to combine the strengths of

00:28:13,919 --> 00:28:17,039
computers with the strengths

00:28:15,279 --> 00:28:18,559
of humans not to just assume that

00:28:17,039 --> 00:28:21,520
computers are better

00:28:18,559 --> 00:28:22,000
and like wholesale eliminate the humans

00:28:21,520 --> 00:28:24,080
um

00:28:22,000 --> 00:28:26,080
and one of the big questions that we

00:28:24,080 --> 00:28:26,559
kind of keep coming back to is like what

00:28:26,080 --> 00:28:28,799
is

00:28:26,559 --> 00:28:30,080
the intent of this system to sort of

00:28:28,799 --> 00:28:31,919
ignore the format

00:28:30,080 --> 00:28:33,360
of the system and just look at like what

00:28:31,919 --> 00:28:34,320
is the system actually trying to

00:28:33,360 --> 00:28:37,760
accomplish

00:28:34,320 --> 00:28:40,799
and how well does it accomplish it so

00:28:37,760 --> 00:28:42,720
in general um the goal of ai and machine

00:28:40,799 --> 00:28:44,559
learning should be to direct human

00:28:42,720 --> 00:28:47,600
thought to where it has the most value

00:28:44,559 --> 00:28:51,200
and not to eliminate it

00:28:47,600 --> 00:28:52,880
and so that kind of like concludes uh

00:28:51,200 --> 00:28:54,960
basically what i wanted to convey to you

00:28:52,880 --> 00:28:57,279
guys but i wanted to just say that like

00:28:54,960 --> 00:28:59,120
this is just the beginning of the work

00:28:57,279 --> 00:29:01,679
that we're doing in this space

00:28:59,120 --> 00:29:03,440
um we have been experimenting with a lot

00:29:01,679 --> 00:29:06,080
of these kind of concepts and we have

00:29:03,440 --> 00:29:09,200
regular conversations about like how

00:29:06,080 --> 00:29:11,360
we think we're doing on on our ethical

00:29:09,200 --> 00:29:13,200
positioning and how we can be better

00:29:11,360 --> 00:29:14,640
so if you'd like to continue to follow

00:29:13,200 --> 00:29:16,720
this conversation

00:29:14,640 --> 00:29:18,159
you can sort of follow my my thoughts

00:29:16,720 --> 00:29:21,200
about this on twitter

00:29:18,159 --> 00:29:24,000
and i post a lot of updates around

00:29:21,200 --> 00:29:25,440
uh what processes i'm working on and how

00:29:24,000 --> 00:29:28,960
i'm thinking about them to

00:29:25,440 --> 00:29:30,640
my medium both of which are under my

00:29:28,960 --> 00:29:32,159
account name belmar and if you'd like to

00:29:30,640 --> 00:29:33,240
reach out to me you can reach me at

00:29:32,159 --> 00:29:35,520
marianne

00:29:33,240 --> 00:29:36,240
rebelliondefense.com so thank you so

00:29:35,520 --> 00:29:37,840
much

00:29:36,240 --> 00:29:39,200
uh and i really look forward to the

00:29:37,840 --> 00:29:39,679
conversations we're gonna have in the

00:29:39,200 --> 00:29:42,640
chat

00:29:39,679 --> 00:29:43,200
thank you very much virtual marianne now

00:29:42,640 --> 00:29:45,039
we have

00:29:43,200 --> 00:29:48,000
actual marianne here one of the first

00:29:45,039 --> 00:29:50,240
questions i saw was how can we recognize

00:29:48,000 --> 00:29:52,799
when a system incorporates a human in an

00:29:50,240 --> 00:29:54,480
ethical way versus when humans remain

00:29:52,799 --> 00:29:55,840
in the loop but effectively it's still

00:29:54,480 --> 00:29:59,039
disempowering them

00:29:55,840 --> 00:29:59,840
uh i love this question because it ties

00:29:59,039 --> 00:30:03,039
into

00:29:59,840 --> 00:30:05,679
what i'm starting to work on now

00:30:03,039 --> 00:30:07,840
um there's an absolutely amazing paper

00:30:05,679 --> 00:30:10,559
called the ironies of automation that

00:30:07,840 --> 00:30:12,320
was published i think in 1983

00:30:10,559 --> 00:30:14,080
so what's really amazing about that

00:30:12,320 --> 00:30:16,240
particular paper for me is that if you

00:30:14,080 --> 00:30:18,399
read it with an sre mindset

00:30:16,240 --> 00:30:20,080
it will explain a lot of concepts that

00:30:18,399 --> 00:30:23,279
are very deeply familiar to us

00:30:20,080 --> 00:30:24,399
and yet it was published in 1983 so like

00:30:23,279 --> 00:30:26,080
there was this whole

00:30:24,399 --> 00:30:28,720
learning that was going on in

00:30:26,080 --> 00:30:30,159
manufacturing in the 80s that software

00:30:28,720 --> 00:30:30,799
completely missed and then had to

00:30:30,159 --> 00:30:33,200
relearn

00:30:30,799 --> 00:30:34,640
in its entirety but one of the things

00:30:33,200 --> 00:30:36,240
the author hits on in this paper that i

00:30:34,640 --> 00:30:39,279
think is really interesting

00:30:36,240 --> 00:30:41,080
is this concept of learning in

00:30:39,279 --> 00:30:42,320
systems and processes and that a

00:30:41,080 --> 00:30:45,200
repetitive

00:30:42,320 --> 00:30:46,240
action um no matter how tedious isn't

00:30:45,200 --> 00:30:48,480
necessarily

00:30:46,240 --> 00:30:50,000
useless even when the operator like

00:30:48,480 --> 00:30:52,799
really really dislikes doing

00:30:50,000 --> 00:30:53,840
it because it builds experience and

00:30:52,799 --> 00:30:57,200
expertise

00:30:53,840 --> 00:30:59,600
that allows them to actually be the

00:30:57,200 --> 00:31:00,960
the sanity check for the automation and

00:30:59,600 --> 00:31:04,080
then if you automate those

00:31:00,960 --> 00:31:06,080
processes does that learning go away and

00:31:04,080 --> 00:31:07,679
do you actually create human operators

00:31:06,080 --> 00:31:10,399
that are less acknowledgeable and less

00:31:07,679 --> 00:31:13,519
experienced and less capable of managing

00:31:10,399 --> 00:31:14,720
the machine and so that that's sort of

00:31:13,519 --> 00:31:16,159
like one of the things that i would

00:31:14,720 --> 00:31:16,720
think about if we're talking about is

00:31:16,159 --> 00:31:18,559
the human

00:31:16,720 --> 00:31:19,840
loop actually empowered or are they just

00:31:18,559 --> 00:31:22,320
like there to

00:31:19,840 --> 00:31:23,760
to um check a compliance box and like

00:31:22,320 --> 00:31:25,120
make everybody happy but couldn't

00:31:23,760 --> 00:31:26,240
actually overrule the system in the

00:31:25,120 --> 00:31:29,760
first place it's like

00:31:26,240 --> 00:31:31,760
how are they learning and developing

00:31:29,760 --> 00:31:34,559
deep expertise with which to

00:31:31,760 --> 00:31:35,440
challenge the determinations of the

00:31:34,559 --> 00:31:36,799
automation

00:31:35,440 --> 00:31:38,720
in that and if you can sort of figure

00:31:36,799 --> 00:31:40,799
out where the learning is in the process

00:31:38,720 --> 00:31:41,840
and if you want to automate that step

00:31:40,799 --> 00:31:43,919
where there's a lot of

00:31:41,840 --> 00:31:45,440
development going on for the human

00:31:43,919 --> 00:31:47,039
expert like where

00:31:45,440 --> 00:31:48,880
are they getting it after you've

00:31:47,039 --> 00:31:50,000
automated it and so thinking about those

00:31:48,880 --> 00:31:54,000
things like that i think

00:31:50,000 --> 00:31:55,600
are useful yeah i'm kind of curious do

00:31:54,000 --> 00:31:57,840
you think there's anything that the

00:31:55,600 --> 00:31:59,039
government space could learn from the

00:31:57,840 --> 00:32:02,159
private sector or

00:31:59,039 --> 00:32:04,880
vice versa especially in terms of ethics

00:32:02,159 --> 00:32:05,360
we have so much to learn from each other

00:32:04,880 --> 00:32:07,760
it's

00:32:05,360 --> 00:32:10,399
fascinating i think from the government

00:32:07,760 --> 00:32:13,519
learning from the private sector

00:32:10,399 --> 00:32:16,080
the government often traps itself into

00:32:13,519 --> 00:32:17,039
unsafe insecure situations because they

00:32:16,080 --> 00:32:20,240
are so

00:32:17,039 --> 00:32:23,039
intolerant of risk that they choose

00:32:20,240 --> 00:32:23,840
options especially with technology that

00:32:23,039 --> 00:32:26,320
are

00:32:23,840 --> 00:32:28,000
almost guaranteed to fail almost

00:32:26,320 --> 00:32:29,760
guaranteed to be less secure

00:32:28,000 --> 00:32:31,760
than what they could have done but are

00:32:29,760 --> 00:32:33,760
like more established that that

00:32:31,760 --> 00:32:34,799
legendary phrase no one ever gets fired

00:32:33,760 --> 00:32:37,279
for ibm

00:32:34,799 --> 00:32:38,799
like ibm is not the the source of it

00:32:37,279 --> 00:32:40,480
anymore but it still lives on in

00:32:38,799 --> 00:32:42,000
government there are definitely choices

00:32:40,480 --> 00:32:43,840
that people you watch people make

00:32:42,000 --> 00:32:45,200
and they know it's less secure than what

00:32:43,840 --> 00:32:46,480
they should be doing but it's what

00:32:45,200 --> 00:32:48,480
everyone else has been

00:32:46,480 --> 00:32:50,399
doing so there's like this security

00:32:48,480 --> 00:32:52,960
blanket and hiding behind it

00:32:50,399 --> 00:32:54,240
so like that's something like we

00:32:52,960 --> 00:32:57,279
continue to try to bring

00:32:54,240 --> 00:32:59,200
things like resilience engineering chaos

00:32:57,279 --> 00:33:03,039
engineering if you prefer the term

00:32:59,200 --> 00:33:05,600
into government reliability slos trying

00:33:03,039 --> 00:33:08,000
to kind of indoctrinate them into

00:33:05,600 --> 00:33:08,799
that way of thinking because it will

00:33:08,000 --> 00:33:10,799
ultimately

00:33:08,799 --> 00:33:12,480
lead to more stable and secure systems

00:33:10,799 --> 00:33:16,080
for them on the other side

00:33:12,480 --> 00:33:18,640
i think that um people in government

00:33:16,080 --> 00:33:20,080
think really deeply about impact and

00:33:18,640 --> 00:33:22,320
that's often

00:33:20,080 --> 00:33:23,279
missed because that's not the kind of

00:33:22,320 --> 00:33:24,880
brand

00:33:23,279 --> 00:33:26,720
that we have in our society about what

00:33:24,880 --> 00:33:27,919
government is particularly the federal

00:33:26,720 --> 00:33:29,840
government but people

00:33:27,919 --> 00:33:31,039
are like it is public service to them

00:33:29,840 --> 00:33:32,640
and they really do think

00:33:31,039 --> 00:33:34,480
very deeply about the impact of what

00:33:32,640 --> 00:33:35,840
they're doing um and so it's always

00:33:34,480 --> 00:33:38,080
weird for me going back

00:33:35,840 --> 00:33:40,080
to like more startup communities and

00:33:38,080 --> 00:33:41,919
listening to people talk about ipos

00:33:40,080 --> 00:33:43,440
and like their vesting schedule and

00:33:41,919 --> 00:33:44,799
stuff like that and then being like

00:33:43,440 --> 00:33:46,880
yeah is this really what we're spending

00:33:44,799 --> 00:33:48,480
our time on like i don't want to judge

00:33:46,880 --> 00:33:50,159
anybody but like i'm judging just a

00:33:48,480 --> 00:33:52,240
little bit you know it's kind of like

00:33:50,159 --> 00:33:53,360
i think that there needs to be a balance

00:33:52,240 --> 00:33:55,679
and that i think

00:33:53,360 --> 00:33:56,720
that perhaps uh both communities can

00:33:55,679 --> 00:34:00,080
learn to sort of

00:33:56,720 --> 00:34:02,240
balance that thing that that um exchange

00:34:00,080 --> 00:34:04,640
between the very pragmatic

00:34:02,240 --> 00:34:06,080
uh kind of capitalist side of life and

00:34:04,640 --> 00:34:09,359
the public service side

00:34:06,080 --> 00:34:21,839
of life i think we can both kind of even

00:34:09,359 --> 00:34:21,839
ourselves out on that front

00:34:39,200 --> 00:34:41,280

YouTube URL: https://www.youtube.com/watch?v=6obQb4O9OwM


