Title: Big Data Processing with Apache Spark
Publication date: 2018-03-28
Playlist: Scala
Description: 
	Spark is a fast and general cluster computing system for Big Data. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for stream processing.
Captions: 
	00:00:29,180 --> 00:00:35,750
[Music]

00:00:31,390 --> 00:00:38,750
okay so I already introduced myself

00:00:35,750 --> 00:00:41,930
again Pluralsight lifeand I actually you

00:00:38,750 --> 00:00:43,579
have a c-sharp background I find I

00:00:41,930 --> 00:00:46,519
actually have found that C sharp to

00:00:43,579 --> 00:00:49,040
Scala is a better jump in Java to Scala

00:00:46,519 --> 00:00:52,760
most people I have talked to have agreed

00:00:49,040 --> 00:00:54,050
with that the only thing is that job is

00:00:52,760 --> 00:00:56,780
starting to catch up so it'll be a

00:00:54,050 --> 00:01:02,120
little bit easier lambdas Java 10 now

00:00:56,780 --> 00:01:03,410
has limited type inference forget some

00:01:02,120 --> 00:01:07,149
of the other stuff that they have that

00:01:03,410 --> 00:01:12,009
they're getting there little by little

00:01:07,149 --> 00:01:12,009
alright so we're here for a party spark

00:01:14,250 --> 00:01:20,400
at my back there and the the HDMI port

00:01:18,270 --> 00:01:22,440
and he's like even without moving at all

00:01:20,400 --> 00:01:25,740
it would just go out so then I had to

00:01:22,440 --> 00:01:27,390
pull out my VGA adapter which I've

00:01:25,740 --> 00:01:29,490
actually I didn't learn because I didn't

00:01:27,390 --> 00:01:31,980
bring it but now I'm gonna start

00:01:29,490 --> 00:01:34,500
bringing a VGA cord so I have an adapter

00:01:31,980 --> 00:01:36,840
so I can do a VGA and try to be dynamic

00:01:34,500 --> 00:01:38,280
but they didn't have a V they had to

00:01:36,840 --> 00:01:40,590
actually call somebody to bring a VGA

00:01:38,280 --> 00:01:45,000
cord over which usually you at least

00:01:40,590 --> 00:01:49,350
have a team I am VG alright so we're

00:01:45,000 --> 00:01:51,540
back up so why not just use grep grep is

00:01:49,350 --> 00:01:55,800
great actually if you have a small data

00:01:51,540 --> 00:01:58,850
set I would say use grep it sits on one

00:01:55,800 --> 00:02:01,680
machine it's actually pretty darn fast

00:01:58,850 --> 00:02:07,320
but once you get to Big Data maybe not

00:02:01,680 --> 00:02:08,910
so much so why not Hadoop I do basically

00:02:07,320 --> 00:02:11,520
says okay we're gonna take all that crap

00:02:08,910 --> 00:02:14,250
stuff for the most part and run it

00:02:11,520 --> 00:02:15,780
across clusters I mean you could say

00:02:14,250 --> 00:02:17,400
what you want to say about it MapReduce

00:02:15,780 --> 00:02:21,690
this or that I mean it's it's you're

00:02:17,400 --> 00:02:23,880
running grep and giant clusters alright

00:02:21,690 --> 00:02:24,360
so now we're much faster so we're good

00:02:23,880 --> 00:02:29,280
right

00:02:24,360 --> 00:02:30,890
has anybody ever written Hadoop well

00:02:29,280 --> 00:02:33,269
that's good I mean most people haven't

00:02:30,890 --> 00:02:36,110
to be honest I haven't but I've seen it

00:02:33,269 --> 00:02:38,430
and it's not it's not pretty

00:02:36,110 --> 00:02:41,660
partially it was Java and so there's

00:02:38,430 --> 00:02:46,890
comparisons Java Scala Scala is a lot

00:02:41,660 --> 00:02:50,489
more terse so along comes spark spark

00:02:46,890 --> 00:02:52,049
does basically the same thing so here we

00:02:50,489 --> 00:02:53,549
had what like four different pictures

00:02:52,049 --> 00:02:55,080
that I put together and here we just

00:02:53,549 --> 00:02:58,350
have one um

00:02:55,080 --> 00:03:01,500
and it's even faster so you're doing it

00:02:58,350 --> 00:03:04,620
on less machines at a faster speed so

00:03:01,500 --> 00:03:08,610
that's why we want to use a spark so big

00:03:04,620 --> 00:03:09,910
data doesn't necessarily mean big code

00:03:08,610 --> 00:03:13,570
there's a reason

00:03:09,910 --> 00:03:18,100
so you can't read this I mean it's just

00:03:13,570 --> 00:03:21,490
this is this is not maintained so this

00:03:18,100 --> 00:03:27,430
is Hadoop basically so we don't want

00:03:21,490 --> 00:03:30,700
that what do we want you want smaller

00:03:27,430 --> 00:03:33,730
code so this is basically the same thing

00:03:30,700 --> 00:03:38,110
so this the core of the logic right here

00:03:33,730 --> 00:03:39,840
it's four lines of code done here here

00:03:38,110 --> 00:03:42,490
and here

00:03:39,840 --> 00:03:44,470
that's it's it's strewn all about

00:03:42,490 --> 00:03:46,420
there's different places it's just it's

00:03:44,470 --> 00:03:48,940
just a mess whereas here we say okay

00:03:46,420 --> 00:03:51,850
read in a text file flat map it's just

00:03:48,940 --> 00:03:53,770
kind of flatten everything count by

00:03:51,850 --> 00:03:57,390
value and then save it off so you've

00:03:53,770 --> 00:03:57,390
done all of that in four lines of code

00:03:57,510 --> 00:04:04,690
all right so further to some additional

00:04:00,610 --> 00:04:09,160
items here spark the the minimalistic

00:04:04,690 --> 00:04:11,940
code makes for better readability as

00:04:09,160 --> 00:04:14,290
well as it's it's much more expressive

00:04:11,940 --> 00:04:15,910
there's different things that you can

00:04:14,290 --> 00:04:24,760
say a tribute to that the small code

00:04:15,910 --> 00:04:27,040
itself the loss my chair I thought but

00:04:24,760 --> 00:04:28,690
yeah so the the the smaller code Lam

00:04:27,040 --> 00:04:31,390
does things like that all of the the

00:04:28,690 --> 00:04:33,669
joys of really Scala you get out of

00:04:31,390 --> 00:04:36,790
spark and they've done a good job of

00:04:33,669 --> 00:04:40,030
porting that into the other libraries

00:04:36,790 --> 00:04:41,950
that they have Java and Python it's fast

00:04:40,030 --> 00:04:44,530
there's actually a reason that it's

00:04:41,950 --> 00:04:48,220
magnitudes faster one they've put a lot

00:04:44,530 --> 00:04:51,460
of time and effort into that too it runs

00:04:48,220 --> 00:04:54,310
on memory for a part of it and we will

00:04:51,460 --> 00:04:57,580
get to that but that's that was really

00:04:54,310 --> 00:05:00,250
its biggest claim to fame there's a lot

00:04:57,580 --> 00:05:02,230
of things that that differentiated it

00:05:00,250 --> 00:05:04,570
but the fact that it could do things in

00:05:02,230 --> 00:05:06,700
memory and store things in memory really

00:05:04,570 --> 00:05:10,140
is the thing that made it leaps and

00:05:06,700 --> 00:05:10,140
bounds better than I do yeah

00:05:10,390 --> 00:05:23,080
but I thought summer was just the memory

00:05:14,080 --> 00:05:24,130
portion but the persistent layer so no

00:05:23,080 --> 00:05:26,680
so that's fine um

00:05:24,130 --> 00:05:29,050
so whenever I say in memory that's

00:05:26,680 --> 00:05:32,410
working with your data in memory

00:05:29,050 --> 00:05:34,630
you're still want to persist it to I

00:05:32,410 --> 00:05:38,950
mean they still have their background

00:05:34,630 --> 00:05:40,510
back-end to some extent in HDFS but

00:05:38,950 --> 00:05:44,440
that's not you don't have to actually

00:05:40,510 --> 00:05:51,070
you could do s3 just see like right

00:05:44,440 --> 00:05:52,570
regular disk store elastic anywhere or

00:05:51,070 --> 00:05:56,560
wherever you want and we'll actually get

00:05:52,570 --> 00:06:00,460
to that testability so one thing that

00:05:56,560 --> 00:06:03,160
was really really hard with Hadoop was

00:06:00,460 --> 00:06:05,680
testability because it was so your code

00:06:03,160 --> 00:06:07,930
was so intermeshed with everything else

00:06:05,680 --> 00:06:10,210
that you had to basically spin up a

00:06:07,930 --> 00:06:12,280
little mini Hadoop cluster just to test

00:06:10,210 --> 00:06:14,080
it whereas SPARC they've actually

00:06:12,280 --> 00:06:16,720
abstracted a lot of that out and you can

00:06:14,080 --> 00:06:19,720
actually write your code in two separate

00:06:16,720 --> 00:06:21,940
modules and things like that so it makes

00:06:19,720 --> 00:06:24,820
them much easier to test its interactive

00:06:21,940 --> 00:06:27,310
too so this again comes in part because

00:06:24,820 --> 00:06:30,610
of Scala

00:06:27,310 --> 00:06:33,880
so they've taken the scala shell and you

00:06:30,610 --> 00:06:35,200
can actually say SPARC - shell and I

00:06:33,880 --> 00:06:38,170
should have a little bit of time to show

00:06:35,200 --> 00:06:40,540
that at the end here but you run spark -

00:06:38,170 --> 00:06:42,820
shell and all of a sudden you can start

00:06:40,540 --> 00:06:45,640
testing out against local databases

00:06:42,820 --> 00:06:47,500
really you can even point to your

00:06:45,640 --> 00:06:49,720
cluster and test against real data if

00:06:47,500 --> 00:06:52,450
you want to or if you have like a test

00:06:49,720 --> 00:06:54,520
cluster - but you can start interacting

00:06:52,450 --> 00:06:57,880
it getting answers immediately see how

00:06:54,520 --> 00:07:00,310
code works and things like that fault

00:06:57,880 --> 00:07:03,040
tolerant so it's it's built with fault

00:07:00,310 --> 00:07:05,440
tolerance in mind and that's something

00:07:03,040 --> 00:07:07,330
we'll get to as we go through as well as

00:07:05,440 --> 00:07:09,490
it unifies Big Data so what does the

00:07:07,330 --> 00:07:13,360
unified Big Data mean there's what's

00:07:09,490 --> 00:07:14,700
called the Hadoop MapReduce explosion so

00:07:13,360 --> 00:07:18,340
if you're using Hadoop

00:07:14,700 --> 00:07:20,470
then okay now all of a sudden you want

00:07:18,340 --> 00:07:22,090
to kind of start streaming it a little

00:07:20,470 --> 00:07:23,279
bit so you're going to want to pull in

00:07:22,090 --> 00:07:25,589
some storm stuff

00:07:23,279 --> 00:07:28,109
all right so what if you want to pull in

00:07:25,589 --> 00:07:31,369
some a little bit of us the scale of

00:07:28,109 --> 00:07:34,279
world I do and there's scouting scouting

00:07:31,369 --> 00:07:39,089
the hive is kind of like a sequel

00:07:34,279 --> 00:07:41,429
back-end to for Hadoop my house for a

00:07:39,089 --> 00:07:44,489
machine learning drill so many things so

00:07:41,429 --> 00:07:45,869
again if you're using Hadoop but now all

00:07:44,489 --> 00:07:47,669
of a sudden you've exploded and now you

00:07:45,869 --> 00:07:51,299
need to learn all these other things if

00:07:47,669 --> 00:07:55,169
you want to really find use of it so

00:07:51,299 --> 00:07:57,449
spark brings that all together and you

00:07:55,169 --> 00:08:00,299
end up with what they called a unified

00:07:57,449 --> 00:08:03,169
platform for big data so that means

00:08:00,299 --> 00:08:07,919
everything's really built on spark core

00:08:03,169 --> 00:08:09,029
that's the the origins of it it's we'll

00:08:07,919 --> 00:08:11,399
get to it but it's they use things

00:08:09,029 --> 00:08:13,649
called rdd's and everything is built on

00:08:11,399 --> 00:08:17,489
top of that so what I mean is now they

00:08:13,649 --> 00:08:19,409
have sparks equal underneath it is rdd's

00:08:17,489 --> 00:08:22,289
and and kind of that batch layer and

00:08:19,409 --> 00:08:26,269
that resiliency that's there same thing

00:08:22,289 --> 00:08:28,829
spark streaming and a lib graphics and

00:08:26,269 --> 00:08:30,299
on top of that is really data frames and

00:08:28,829 --> 00:08:33,120
data sets that's where they're building

00:08:30,299 --> 00:08:35,250
to in today's world some of these are a

00:08:33,120 --> 00:08:38,000
little dated and we'll kind of get to

00:08:35,250 --> 00:08:40,800
that so like spark streaming has become

00:08:38,000 --> 00:08:43,050
it's now using data frames data set so

00:08:40,800 --> 00:08:44,970
be they have an older version of spark

00:08:43,050 --> 00:08:46,860
streaming but now they're also they have

00:08:44,970 --> 00:08:49,350
a better version of it

00:08:46,860 --> 00:08:55,860
graphics isn't really maintained anymore

00:08:49,350 --> 00:08:57,870
quite so whoa it's still there but as

00:08:55,860 --> 00:09:01,319
well as the cut the data bricks pops up

00:08:57,870 --> 00:09:04,800
data bricks is the company that so the

00:09:01,319 --> 00:09:09,870
founder the guy who created spark was

00:09:04,800 --> 00:09:11,189
Matassa haria and he comes out of why am

00:09:09,870 --> 00:09:14,819
i doing the link on the name out of

00:09:11,189 --> 00:09:17,490
caliber Klee so he came out of there and

00:09:14,819 --> 00:09:19,339
really a lot of the work he did was some

00:09:17,490 --> 00:09:23,220
out of what's called the amp lab

00:09:19,339 --> 00:09:26,370
algorithms machine and people lab so a

00:09:23,220 --> 00:09:29,220
lot of funding there and I think if I

00:09:26,370 --> 00:09:31,050
recall he actually built Mesa he worked

00:09:29,220 --> 00:09:33,379
on mesas here and started doing spark as

00:09:31,050 --> 00:09:35,870
a toy project and it really blew up and

00:09:33,379 --> 00:09:38,420
he saw a lot of potential

00:09:35,870 --> 00:09:41,240
so then data Burks comes along and adds

00:09:38,420 --> 00:09:44,120
that production stability so it's open

00:09:41,240 --> 00:09:49,340
source but data Brooks is now the

00:09:44,120 --> 00:09:52,310
probably over 50% of the contributors in

00:09:49,340 --> 00:09:54,980
that stability can't really read it here

00:09:52,310 --> 00:09:57,380
but smart they tell you SPARC 1/3 is

00:09:54,980 --> 00:09:58,970
binary compatible with SPARC 1x releases

00:09:57,380 --> 00:10:01,640
so they tried to keep binary

00:09:58,970 --> 00:10:04,220
compatibility across the major releases

00:10:01,640 --> 00:10:09,350
not even just like anything else this is

00:10:04,220 --> 00:10:11,180
actual binary compatibility they tell

00:10:09,350 --> 00:10:14,090
you the things that do change as well as

00:10:11,180 --> 00:10:17,020
they have their MIMO tests that actually

00:10:14,090 --> 00:10:21,050
tell you if somebody makes a code change

00:10:17,020 --> 00:10:25,460
then it Flags it and will say hey this

00:10:21,050 --> 00:10:27,290
is changing a public API so you can't do

00:10:25,460 --> 00:10:29,150
that and this version has to be it

00:10:27,290 --> 00:10:32,630
basically there has to be an override to

00:10:29,150 --> 00:10:34,700
that so that's pretty cool as well as in

00:10:32,630 --> 00:10:38,000
that stability world this has actually

00:10:34,700 --> 00:10:39,170
fallen a little bit as of late I think

00:10:38,000 --> 00:10:42,890
they're getting back to it a little bit

00:10:39,170 --> 00:10:45,500
more but they were following drop things

00:10:42,890 --> 00:10:48,470
and make sure you release on a regular

00:10:45,500 --> 00:10:50,570
timeframe so they would do 1 3 and that

00:10:48,470 --> 00:10:52,340
meant it was for one quarter one form in

00:10:50,570 --> 00:10:55,760
the next quarter one five next quarter

00:10:52,340 --> 00:10:57,140
once they went to the two series they

00:10:55,760 --> 00:10:59,030
had a lot of trouble getting into the

00:10:57,140 --> 00:11:01,190
two series getting the things that they

00:10:59,030 --> 00:11:03,080
wanted and I've faltered a little bit

00:11:01,190 --> 00:11:07,430
but now they're they seem to be back on

00:11:03,080 --> 00:11:08,720
a cadence just the regular slide here

00:11:07,430 --> 00:11:11,480
I'm gonna go through it real quick for

00:11:08,720 --> 00:11:13,730
time but who's using spark I mean this

00:11:11,480 --> 00:11:15,860
is even potentially outdated I could add

00:11:13,730 --> 00:11:20,180
so probably even bigger names onto here

00:11:15,860 --> 00:11:22,610
but you've got just tons of stuff we've

00:11:20,180 --> 00:11:24,320
already went over a quick Scylla is the

00:11:22,610 --> 00:11:26,690
main language because it was written in

00:11:24,320 --> 00:11:29,090
Scala and because of that you

00:11:26,690 --> 00:11:30,830
automatically get Java but one thing

00:11:29,090 --> 00:11:33,200
about Scott if it was written in Scala

00:11:30,830 --> 00:11:36,560
first a lot of times your Java API is

00:11:33,200 --> 00:11:39,800
halves that it's not perfect because I

00:11:36,560 --> 00:11:44,210
mean Scala to make Scala work that it

00:11:39,800 --> 00:11:47,270
has to write JVM code that Java isn't

00:11:44,210 --> 00:11:49,760
pretty in Java per se but they've

00:11:47,270 --> 00:11:51,500
actually spent time and they make sorta

00:11:49,760 --> 00:11:54,770
expose the public API is in a nice

00:11:51,500 --> 00:11:58,430
fashion for Java as well as Python and

00:11:54,770 --> 00:12:02,510
then they also added are the nice thing

00:11:58,430 --> 00:12:05,090
about R is that R by itself is kind of

00:12:02,510 --> 00:12:08,150
your own data science language but it's

00:12:05,090 --> 00:12:11,800
single node thing so SPARC allows you to

00:12:08,150 --> 00:12:14,060
write R and boost it out into real world

00:12:11,800 --> 00:12:17,180
however still if I underst

00:12:14,060 --> 00:12:20,450
I last I heard Scala and Python are

00:12:17,180 --> 00:12:22,700
their biggest and any surveys that I've

00:12:20,450 --> 00:12:25,660
seen most people use it either in Scala

00:12:22,700 --> 00:12:29,980
or Python Java growing a little bit but

00:12:25,660 --> 00:12:29,980
yeah those are the two that I've seen

00:12:30,940 --> 00:12:36,710
big data it kind of gets to something

00:12:34,370 --> 00:12:40,760
that I was saying before make sure that

00:12:36,710 --> 00:12:42,710
you actually have big data if you if you

00:12:40,760 --> 00:12:44,150
only have like maybe 100 gigabytes or

00:12:42,710 --> 00:12:45,770
something like that you probably could

00:12:44,150 --> 00:12:47,540
even nowadays and most machines you

00:12:45,770 --> 00:12:49,270
could probably get through that pretty

00:12:47,540 --> 00:12:52,880
quickly in some other something else

00:12:49,270 --> 00:12:55,670
there's some articles about Big Data or

00:12:52,880 --> 00:12:59,000
or or writing a grep for it or something

00:12:55,670 --> 00:13:00,410
like that so you want to make sure to

00:12:59,000 --> 00:13:01,880
use the right tools so just because I'm

00:13:00,410 --> 00:13:05,380
advocating SPARC doesn't this really

00:13:01,880 --> 00:13:07,640
mean it's the right thing for you so

00:13:05,380 --> 00:13:10,700
smart core like I said that's really

00:13:07,640 --> 00:13:15,410
where spark the the root of everything

00:13:10,700 --> 00:13:17,840
comes from so what you have is this is

00:13:15,410 --> 00:13:20,390
working if just the high level of how

00:13:17,840 --> 00:13:23,120
spark works so everything's run off of a

00:13:20,390 --> 00:13:24,680
driver here and if you want resiliency

00:13:23,120 --> 00:13:26,390
you can use things like zookeeper and

00:13:24,680 --> 00:13:27,980
and whatever that are based off your

00:13:26,390 --> 00:13:30,200
cluster but I'm not going to get at the

00:13:27,980 --> 00:13:34,310
high level resiliency at the driver

00:13:30,200 --> 00:13:37,700
level but you do get resiliency across

00:13:34,310 --> 00:13:40,930
your workers so you can say hey I have

00:13:37,700 --> 00:13:44,750
the driver is going to be the thing that

00:13:40,930 --> 00:13:47,060
manages who gets tasks and who who who

00:13:44,750 --> 00:13:49,760
has what data and whatever so it's gonna

00:13:47,060 --> 00:13:53,150
shoot for data locality so it's gonna

00:13:49,760 --> 00:13:55,400
say hey this worker was working on we

00:13:53,150 --> 00:13:57,770
know that things were partitioned so it

00:13:55,400 --> 00:13:59,510
works on things a through D so I'm gonna

00:13:57,770 --> 00:14:01,920
shove everything that's a through D over

00:13:59,510 --> 00:14:03,930
here so that way you're you're not

00:14:01,920 --> 00:14:05,700
you send it over here but the data that

00:14:03,930 --> 00:14:07,889
it needs is over here so it still has to

00:14:05,700 --> 00:14:12,360
make calls so it's going to try to keep

00:14:07,889 --> 00:14:14,579
it as local as possible so inside of

00:14:12,360 --> 00:14:17,700
there is called a spark context that's

00:14:14,579 --> 00:14:20,550
basically the thing that runs the driver

00:14:17,700 --> 00:14:23,579
nowadays actually I forgot to update

00:14:20,550 --> 00:14:25,560
this spark context is still there but

00:14:23,579 --> 00:14:26,850
they found that confusing as they got

00:14:25,560 --> 00:14:28,829
into things like sequel and things like

00:14:26,850 --> 00:14:31,620
that so they actually changed this to be

00:14:28,829 --> 00:14:33,360
sparks session ultimately it's still the

00:14:31,620 --> 00:14:36,660
same thing but it's just saying here's

00:14:33,360 --> 00:14:39,690
your session spark you can in fact get

00:14:36,660 --> 00:14:42,269
to the original spark context with spark

00:14:39,690 --> 00:14:51,449
session dot SC and it gives you a spark

00:14:42,269 --> 00:14:53,279
context the spark context is kind of the

00:14:51,449 --> 00:14:55,470
orchestrator of everything so it'll

00:14:53,279 --> 00:14:57,720
create the tasks it'll schedule the work

00:14:55,470 --> 00:14:59,790
across and again it will shoot for data

00:14:57,720 --> 00:15:02,699
locality as well as handle any kind of

00:14:59,790 --> 00:15:06,300
faults that may happen so it'll reshift

00:15:02,699 --> 00:15:09,300
work if node goes down and we'll see

00:15:06,300 --> 00:15:12,089
that the ultimate thing that happens

00:15:09,300 --> 00:15:16,019
here the the the core abstraction is

00:15:12,089 --> 00:15:18,660
called an RDD even if we whenever I

00:15:16,019 --> 00:15:21,510
again I said it all kind of went sequel

00:15:18,660 --> 00:15:25,170
this that ml live streaming graphics and

00:15:21,510 --> 00:15:27,560
now the data frames even still data

00:15:25,170 --> 00:15:31,680
frames is built on top of our giggy's

00:15:27,560 --> 00:15:33,540
but again these are things we do so an

00:15:31,680 --> 00:15:35,190
RDD is a collection of elements

00:15:33,540 --> 00:15:37,140
partitioned across the nodes of the

00:15:35,190 --> 00:15:39,959
cluster that can be operated on in

00:15:37,140 --> 00:15:43,190
parallel so mumbo-jumbo what does that

00:15:39,959 --> 00:15:47,130
mean means that it's a resilient

00:15:43,190 --> 00:15:53,430
distributed data set oh my stuff sauce

00:15:47,130 --> 00:15:55,319
hello that's my circle so a data set if

00:15:53,430 --> 00:15:57,480
so I mean it's pretty simple what's

00:15:55,319 --> 00:16:00,300
saying it's throwing your data it's

00:15:57,480 --> 00:16:05,699
distributed you know let's just get rid

00:16:00,300 --> 00:16:06,990
of those so it's distributed and so

00:16:05,699 --> 00:16:08,610
we've already gone over that we know

00:16:06,990 --> 00:16:10,769
that it's going to distribute the work

00:16:08,610 --> 00:16:12,900
from however many nodes as you have

00:16:10,769 --> 00:16:15,360
partitioned your cluster into and

00:16:12,900 --> 00:16:18,330
told the the spark worker to work on as

00:16:15,360 --> 00:16:21,990
well as its resilient so if a if

00:16:18,330 --> 00:16:24,660
something working on a particular task

00:16:21,990 --> 00:16:27,120
goes down then it's actually going to

00:16:24,660 --> 00:16:31,940
spin it back up but it actually works

00:16:27,120 --> 00:16:31,940
off of you have it in there no I don't

00:16:32,450 --> 00:16:38,400
so so this is just going with that it's

00:16:34,890 --> 00:16:41,280
lazy actually this does get to it so so

00:16:38,400 --> 00:16:43,650
it is a lazy thing and it's built off of

00:16:41,280 --> 00:16:46,320
what's called a dag directed acyclic

00:16:43,650 --> 00:16:49,260
graph and so the reason that that's

00:16:46,320 --> 00:16:51,780
useful is that and the reason that the

00:16:49,260 --> 00:16:54,870
laziness is useful is that it'll build

00:16:51,780 --> 00:16:57,180
out a you say you basically you say what

00:16:54,870 --> 00:16:59,400
you want it to do and then it'll

00:16:57,180 --> 00:17:00,780
optimize as much as it can especially

00:16:59,400 --> 00:17:03,420
once we get the data frames and data

00:17:00,780 --> 00:17:05,490
sets it gets really you can do tons of

00:17:03,420 --> 00:17:07,980
great things and without having to worry

00:17:05,490 --> 00:17:12,390
about performance the spark optimizer

00:17:07,980 --> 00:17:14,720
will handle it for you but as far as the

00:17:12,390 --> 00:17:19,410
resiliency goes if something goes down

00:17:14,720 --> 00:17:21,630
it's going to say okay I I know that I

00:17:19,410 --> 00:17:24,840
had a certain amount of data that was

00:17:21,630 --> 00:17:27,480
there and because of that graph I can

00:17:24,840 --> 00:17:29,070
start from a certain check point or

00:17:27,480 --> 00:17:30,990
something like that so you can if you

00:17:29,070 --> 00:17:32,400
set up in fact the spark streaming

00:17:30,990 --> 00:17:34,530
originally used checkpointing

00:17:32,400 --> 00:17:36,660
automatically behind the scenes where I

00:17:34,530 --> 00:17:39,060
would say okay instead of starting you

00:17:36,660 --> 00:17:41,490
say you said that you have to do do a

00:17:39,060 --> 00:17:45,210
map of reduce a map blah blah so we'll

00:17:41,490 --> 00:17:47,820
just call ABCD if it crashed on D but

00:17:45,210 --> 00:17:51,270
it's stored off at C so it can say hey I

00:17:47,820 --> 00:17:52,860
have the the graph at this point you can

00:17:51,270 --> 00:17:54,210
just take off from D so it saves that

00:17:52,860 --> 00:17:55,050
time and you don't have to redo the

00:17:54,210 --> 00:17:57,060
whole thing

00:17:55,050 --> 00:18:00,330
and even if it does have to go to put

00:17:57,060 --> 00:18:02,010
the original origination point it knows

00:18:00,330 --> 00:18:03,660
the data set it was working on and so it

00:18:02,010 --> 00:18:07,310
doesn't do the whole thing across the

00:18:03,660 --> 00:18:07,310
whole cluster that was at locality wise

00:18:10,100 --> 00:18:15,300
transformations so it can do things like

00:18:12,840 --> 00:18:16,650
map filter and things like that we're

00:18:15,300 --> 00:18:20,160
like I said I'm not going to get into

00:18:16,650 --> 00:18:21,840
too much of the code but then the other

00:18:20,160 --> 00:18:23,040
thing that they so they split it into

00:18:21,840 --> 00:18:26,190
things transformations which

00:18:23,040 --> 00:18:29,039
transformations are your lazy aspects

00:18:26,190 --> 00:18:32,840
Chin's are your whenever you're actually

00:18:29,039 --> 00:18:32,840
doing something so anytime that you say

00:18:32,899 --> 00:18:40,320
collect or save or counts or something

00:18:36,990 --> 00:18:42,059
like that it's actually going to act on

00:18:40,320 --> 00:18:45,059
that so all the way all along the way so

00:18:42,059 --> 00:18:47,340
if you run map filter voila it actually

00:18:45,059 --> 00:18:50,269
won't do anything it'll just be building

00:18:47,340 --> 00:18:52,679
that graph so then it knows what to do

00:18:50,269 --> 00:18:55,169
whenever you call collect

00:18:52,679 --> 00:18:56,730
so click back to the driver which that's

00:18:55,169 --> 00:18:58,590
one thing you don't necessarily want to

00:18:56,730 --> 00:19:00,779
call unless you've reduced your data set

00:18:58,590 --> 00:19:02,820
so if you have things split out across

00:19:00,779 --> 00:19:04,139
three clusters for a reason unless

00:19:02,820 --> 00:19:05,700
you've reduced it down to a manageable

00:19:04,139 --> 00:19:07,559
level if you call collect it all goes

00:19:05,700 --> 00:19:09,779
back to the driver and you could get a

00:19:07,559 --> 00:19:13,919
out of memory just because it's

00:19:09,779 --> 00:19:16,409
distributed for a reason so all these

00:19:13,919 --> 00:19:18,870
things require the data took to come

00:19:16,409 --> 00:19:21,899
back and do something and give an actual

00:19:18,870 --> 00:19:24,809
output if you're working with the data

00:19:21,899 --> 00:19:30,779
if you get back if the return type is of

00:19:24,809 --> 00:19:33,000
type RTD string or RTD tupple string int

00:19:30,779 --> 00:19:35,159
or whatever if it's wrapped in an RDD

00:19:33,000 --> 00:19:37,470
it's a transformation it's lazy because

00:19:35,159 --> 00:19:39,419
it's still in the RTD state if it gives

00:19:37,470 --> 00:19:40,649
you the data back so instead of an RTD

00:19:39,419 --> 00:19:42,210
of string you get the string a

00:19:40,649 --> 00:19:45,269
collection of strings or something like

00:19:42,210 --> 00:19:48,419
that and it's an action that's one way

00:19:45,269 --> 00:19:49,799
to tell so you've got your reduce all

00:19:48,419 --> 00:19:52,620
right so going back to our mechanics

00:19:49,799 --> 00:19:55,320
here now we talked about the workers so

00:19:52,620 --> 00:19:56,519
the workers have different executives

00:19:55,320 --> 00:19:58,649
that are going to do different things

00:19:56,519 --> 00:20:01,200
and you can even parallel eyes within

00:19:58,649 --> 00:20:03,029
there so your workers are kind of your

00:20:01,200 --> 00:20:07,950
machines and in here you have kind of

00:20:03,029 --> 00:20:09,059
your threads that are running so what

00:20:07,950 --> 00:20:11,669
it's gonna do is it's gonna have these

00:20:09,059 --> 00:20:15,659
tasks and it's gonna disseminate them

00:20:11,669 --> 00:20:18,090
out to the different workers and let's

00:20:15,659 --> 00:20:19,950
say that these finish and all of them

00:20:18,090 --> 00:20:23,850
finish and then all of a sudden they're

00:20:19,950 --> 00:20:25,019
gonna then then this one here crashes so

00:20:23,850 --> 00:20:27,110
what's going to happen instead of having

00:20:25,019 --> 00:20:31,049
to do the whole thing again this one

00:20:27,110 --> 00:20:33,659
restarts or maybe new one pops up or

00:20:31,049 --> 00:20:35,340
whatever now it's going to send the task

00:20:33,659 --> 00:20:37,500
down there and say hey this is where you

00:20:35,340 --> 00:20:39,399
were and it's going to be able to finish

00:20:37,500 --> 00:20:41,799
up and go right from them so all

00:20:39,399 --> 00:20:43,299
these remained in a good state you don't

00:20:41,799 --> 00:20:45,399
have to read you everything you just had

00:20:43,299 --> 00:20:48,960
to do that one specific thing get it

00:20:45,399 --> 00:20:51,129
back up to stay and then you're good

00:20:48,960 --> 00:20:53,559
alright so getting to your question

00:20:51,129 --> 00:20:55,359
about input or actually you were talking

00:20:53,559 --> 00:20:57,489
about but I think the other side and a

00:20:55,359 --> 00:20:59,409
helper so you as far as input there's

00:20:57,489 --> 00:21:01,479
tons of things I mean if you can think

00:20:59,409 --> 00:21:03,309
it there you'd probably support it but

00:21:01,479 --> 00:21:08,889
just to name a few you could read from

00:21:03,309 --> 00:21:12,700
the file system I do as three just a

00:21:08,889 --> 00:21:14,710
regular data JDBC database Cassandra as

00:21:12,700 --> 00:21:17,229
well as we'll get to memory so you can

00:21:14,710 --> 00:21:19,929
read off of memory in fact I haven't

00:21:17,229 --> 00:21:25,769
stayed as much up to date on it but they

00:21:19,929 --> 00:21:28,029
had a one of the things they had was a

00:21:25,769 --> 00:21:30,159
memory file system that they were

00:21:28,029 --> 00:21:34,059
working on but now I'm actually

00:21:30,159 --> 00:21:37,119
forgetting the name of it if it pops up

00:21:34,059 --> 00:21:38,889
our tachyon that's what is so tachyon

00:21:37,119 --> 00:21:40,809
it's kind of like basically saying hey

00:21:38,889 --> 00:21:42,729
we can store the memory is becoming

00:21:40,809 --> 00:21:45,789
cheaper and cheaper so we can actually

00:21:42,729 --> 00:21:48,249
store a ton of data and almost have it

00:21:45,789 --> 00:21:50,409
be persistent now I think they still

00:21:48,249 --> 00:21:53,679
have an underlying true persistence

00:21:50,409 --> 00:21:55,479
layer but they really boost it to a

00:21:53,679 --> 00:22:02,759
higher even higher level if you're using

00:21:55,479 --> 00:22:04,869
tachyon different structures park' Avro

00:22:02,759 --> 00:22:06,159
alright so I've been talking about how

00:22:04,869 --> 00:22:09,609
it can work off of memory so how does

00:22:06,159 --> 00:22:14,799
that work so I have my data it's coming

00:22:09,609 --> 00:22:16,899
in 1 2 3 4 5 or whatever then just

00:22:14,799 --> 00:22:22,330
pointing out that maybe it's a

00:22:16,899 --> 00:22:25,089
long-running task so then all of a

00:22:22,330 --> 00:22:28,749
sudden it's going to be 1 4 9 so it did

00:22:25,089 --> 00:22:32,499
its work but again it took all this time

00:22:28,749 --> 00:22:33,729
to actually run these each job and I

00:22:32,499 --> 00:22:37,389
mean if you're talking about truly big

00:22:33,729 --> 00:22:40,869
data it's going to take a while so now

00:22:37,389 --> 00:22:44,109
we do X plus 1 we do another thing and

00:22:40,869 --> 00:22:47,169
we end up with this new data set now

00:22:44,109 --> 00:22:49,149
what happens if that crashes we are

00:22:47,169 --> 00:22:50,710
gonna have to to some extent come back

00:22:49,149 --> 00:22:52,809
here unless you checkpoint it or

00:22:50,710 --> 00:22:54,940
something but we're gonna

00:22:52,809 --> 00:22:57,970
come back here and start over and this

00:22:54,940 --> 00:23:01,149
was a long task so instead we can say

00:22:57,970 --> 00:23:04,470
here we can call cash or persist on the

00:23:01,149 --> 00:23:07,419
RDD and so what will happen is

00:23:04,470 --> 00:23:10,659
asynchronously this will go down and

00:23:07,419 --> 00:23:11,980
write to memory or you can write it to

00:23:10,659 --> 00:23:16,139
this so there's different ways that you

00:23:11,980 --> 00:23:19,029
could do caching so a cache to disk is

00:23:16,139 --> 00:23:21,820
realistically more like a checkpoint

00:23:19,029 --> 00:23:23,799
because it's more persistent but you can

00:23:21,820 --> 00:23:26,440
you can actually do cache to memory with

00:23:23,799 --> 00:23:27,999
overflow to disk so it's all options

00:23:26,440 --> 00:23:29,559
that are available there's there's

00:23:27,999 --> 00:23:31,899
different ones as well as you can

00:23:29,559 --> 00:23:36,279
configure it to use things like backends

00:23:31,899 --> 00:23:40,049
like tachyon anymore not so now whenever

00:23:36,279 --> 00:23:42,580
it crashes actually I miss the

00:23:40,049 --> 00:23:45,340
transition so it writes it here now

00:23:42,580 --> 00:23:48,129
whenever it crashes they there should be

00:23:45,340 --> 00:23:54,549
an arrow going up so it comes up from

00:23:48,129 --> 00:23:55,899
here goes over to X plus one okay we can

00:23:54,549 --> 00:23:58,059
even change it now if we're working

00:23:55,899 --> 00:24:00,309
interactively we can say okay work off

00:23:58,059 --> 00:24:02,350
the same data set do X plus three and

00:24:00,309 --> 00:24:05,019
it's actually going to come from here go

00:24:02,350 --> 00:24:07,539
up and just it still skips this so you

00:24:05,019 --> 00:24:09,369
can actually if you there's something

00:24:07,539 --> 00:24:12,279
called the spark job server where you

00:24:09,369 --> 00:24:14,110
can pseudo share your jobs so if it's

00:24:12,279 --> 00:24:18,070
still in memory you can say hey go hit

00:24:14,110 --> 00:24:20,080
this HTTP endpoint and now you can run

00:24:18,070 --> 00:24:22,840
your own job and you don't they don't

00:24:20,080 --> 00:24:24,610
the people don't have to worry about the

00:24:22,840 --> 00:24:27,549
the length of time to do this thread dot

00:24:24,610 --> 00:24:33,100
sleep and whatnot so again running off

00:24:27,549 --> 00:24:35,529
of caches is really useful from a speed

00:24:33,100 --> 00:24:36,820
perspective quickly I go through kind of

00:24:35,529 --> 00:24:40,710
a different distribution and

00:24:36,820 --> 00:24:43,330
Instrumentation mechanisms everything is

00:24:40,710 --> 00:24:45,580
expected to be run through spark Submit

00:24:43,330 --> 00:24:49,090
so whenever I said there was a spark -

00:24:45,580 --> 00:24:53,649
shell that's really just spark submit

00:24:49,090 --> 00:24:55,149
wraps in a kind of a console so it still

00:24:53,649 --> 00:24:58,299
goes out and does the same thing it just

00:24:55,149 --> 00:24:59,860
makes it more interactive so spark

00:24:58,299 --> 00:25:01,659
submit says hey I have this jar that

00:24:59,860 --> 00:25:04,210
I've packaged I've written up on my code

00:25:01,659 --> 00:25:05,230
and I'm gonna say spark submit go run

00:25:04,210 --> 00:25:06,940
that and you're gonna pack

00:25:05,230 --> 00:25:08,950
past the jar as the parameter and

00:25:06,940 --> 00:25:12,580
whatnot and it's going to expect

00:25:08,950 --> 00:25:13,960
typically aspects of main I think you

00:25:12,580 --> 00:25:17,289
can even over you can pass in a

00:25:13,960 --> 00:25:19,090
variables so that you can say the main

00:25:17,289 --> 00:25:21,730
method is actually called foo or

00:25:19,090 --> 00:25:24,580
whatever but it's basically the entry

00:25:21,730 --> 00:25:27,190
point and then it's gonna go out and

00:25:24,580 --> 00:25:28,690
it's gonna say hey I need to run this

00:25:27,190 --> 00:25:32,559
somewhere and it's gonna go out to this

00:25:28,690 --> 00:25:36,370
one sir to the driver which will run

00:25:32,559 --> 00:25:38,110
your tasks then from there it's gonna do

00:25:36,370 --> 00:25:42,000
the building of the art you do the

00:25:38,110 --> 00:25:44,260
running of it send all the data out and

00:25:42,000 --> 00:25:48,789
act on those results whether that's

00:25:44,260 --> 00:25:51,039
collecting counting saving to s3 HDFS

00:25:48,789 --> 00:25:54,210
whatever my man it's gonna complete and

00:25:51,039 --> 00:25:59,169
basically be done so ultimately

00:25:54,210 --> 00:26:01,000
sparc submit at the core level is a

00:25:59,169 --> 00:26:03,549
batch type job but again we'll get to

00:26:01,000 --> 00:26:08,200
how we can do more of a real-time

00:26:03,549 --> 00:26:10,799
streaming I think the cluster managers

00:26:08,200 --> 00:26:10,799
that are available

00:26:14,250 --> 00:26:20,529
okay so there's sparks standalone

00:26:18,210 --> 00:26:23,260
however that's not maintained too well

00:26:20,529 --> 00:26:24,970
so actually from a server perspective

00:26:23,260 --> 00:26:28,230
most people are going to use something

00:26:24,970 --> 00:26:33,580
like yarn which is yet another resource

00:26:28,230 --> 00:26:37,299
negotiator negotiator and Apache maysa

00:26:33,580 --> 00:26:40,480
that's kind of the thing that's like

00:26:37,299 --> 00:26:43,240
Bend is helping with so we'll provide

00:26:40,480 --> 00:26:46,029
support around that for your management

00:26:43,240 --> 00:26:48,580
and things like that so there's actually

00:26:46,029 --> 00:26:50,500
a company called meso sphere that has

00:26:48,580 --> 00:26:52,240
something that makes me so it's even

00:26:50,500 --> 00:26:54,820
easier to use called DCOs

00:26:52,240 --> 00:26:57,250
and then the other thing that I didn't

00:26:54,820 --> 00:27:00,010
add to the slide came very recently

00:26:57,250 --> 00:27:01,539
sparked two-three was just released like

00:27:00,010 --> 00:27:03,970
less than a month ago or something like

00:27:01,539 --> 00:27:07,000
that and they have experimental support

00:27:03,970 --> 00:27:09,669
of running kubernetes natively

00:27:07,000 --> 00:27:12,039
I haven't dug into it too much at this

00:27:09,669 --> 00:27:14,409
point but I know everybody loves

00:27:12,039 --> 00:27:16,840
kubernetes nowadays so when they

00:27:14,409 --> 00:27:18,429
invested some time in there as far as

00:27:16,840 --> 00:27:23,499
the thing that's going to manage

00:27:18,429 --> 00:27:29,379
all of your resources I had stuff under

00:27:23,499 --> 00:27:32,649
there so I proved it for this all right

00:27:29,379 --> 00:27:34,299
so that's the core so underneath it all

00:27:32,649 --> 00:27:36,820
you have an RDD yeah that's that's

00:27:34,299 --> 00:27:39,249
basically what I'm saying there but now

00:27:36,820 --> 00:27:40,779
underneath on top of that there's

00:27:39,249 --> 00:27:44,499
abstraction so you can write your batch

00:27:40,779 --> 00:27:48,369
layer and rdd's and and that's nowadays

00:27:44,499 --> 00:27:50,649
an RDD is more akin to just writing C++

00:27:48,369 --> 00:27:53,169
now you're going to typically write

00:27:50,649 --> 00:27:57,460
things in sparks equal and data frames

00:27:53,169 --> 00:27:59,889
and data effects which also means it's

00:27:57,460 --> 00:28:01,539
called sparks equal because now you can

00:27:59,889 --> 00:28:04,840
actually do something say hey here's my

00:28:01,539 --> 00:28:06,190
data register it against the table and I

00:28:04,840 --> 00:28:07,450
think actually it's called register

00:28:06,190 --> 00:28:09,669
table now I think they got rid of the

00:28:07,450 --> 00:28:12,309
temp and then you can actually write

00:28:09,669 --> 00:28:13,450
sequel so if you know sequel I think

00:28:12,309 --> 00:28:17,649
they're up to

00:28:13,450 --> 00:28:20,619
sequel 2003 standards and you can

00:28:17,649 --> 00:28:23,649
actually write straight sequel and it'll

00:28:20,619 --> 00:28:28,869
parse it into down into data frame slash

00:28:23,649 --> 00:28:30,429
RDD logic so from their own blog as

00:28:28,869 --> 00:28:32,889
spark continues to grow we want to

00:28:30,429 --> 00:28:34,389
enable wider audiences beyond big data

00:28:32,889 --> 00:28:37,210
engineers to leverage the power of

00:28:34,389 --> 00:28:39,730
distributed processing so that's really

00:28:37,210 --> 00:28:41,559
saying is if you're writing an RDD

00:28:39,730 --> 00:28:43,419
you're still in MapReduce land we've

00:28:41,559 --> 00:28:44,529
they've simplified it and made it easier

00:28:43,419 --> 00:28:46,389
to understand but you still need to

00:28:44,529 --> 00:28:50,470
understand it a little bit more than

00:28:46,389 --> 00:28:52,240
maybe the typical data engineer whereas

00:28:50,470 --> 00:28:54,519
if you're writing sequel there's it

00:28:52,240 --> 00:28:57,730
blows up and now all of a sudden people

00:28:54,519 --> 00:29:00,820
who don't didn't know how to write Scala

00:28:57,730 --> 00:29:06,999
can write sequel and be able to leverage

00:29:00,820 --> 00:29:09,610
everything that spark offers so kind of

00:29:06,999 --> 00:29:13,210
where spark sequel came from a so the

00:29:09,610 --> 00:29:17,499
idea behind Hadoop they had hive patty

00:29:13,210 --> 00:29:20,289
drill and I believe Impala so these are

00:29:17,499 --> 00:29:23,799
all kind of the competitors to kind of a

00:29:20,289 --> 00:29:26,200
sequel ish Big Data thing spark sequel

00:29:23,799 --> 00:29:28,570
has as far as I see it blown them out of

00:29:26,200 --> 00:29:30,480
the water especially lately and the

00:29:28,570 --> 00:29:32,730
reason is because actually I

00:29:30,480 --> 00:29:36,330
don't have that next so we'll get to the

00:29:32,730 --> 00:29:39,330
reason in a second so high you can still

00:29:36,330 --> 00:29:41,160
point to hive and and it'll optimize so

00:29:39,330 --> 00:29:42,780
high you can write native hive and

00:29:41,160 --> 00:29:45,809
that's great but you can also write your

00:29:42,780 --> 00:29:48,600
spark sequel and point it to hive and

00:29:45,809 --> 00:29:51,450
it'll actually optimize the the queries

00:29:48,600 --> 00:29:54,090
against hive itself you can even write

00:29:51,450 --> 00:29:57,570
sparked equal to Jason Parque just about

00:29:54,090 --> 00:29:59,700
everything nowadays in fact Amazon

00:29:57,570 --> 00:30:02,250
redshift that redshift is what I was

00:29:59,700 --> 00:30:04,580
thinking about before and basically

00:30:02,250 --> 00:30:08,460
again everything that you can think of

00:30:04,580 --> 00:30:11,040
so the reason that sparks equal blows

00:30:08,460 --> 00:30:13,260
everybody out of the water is because of

00:30:11,040 --> 00:30:15,720
optimizations that it can make so all

00:30:13,260 --> 00:30:17,730
the things they've basically read all of

00:30:15,720 --> 00:30:21,840
these different database papers and

00:30:17,730 --> 00:30:23,610
learn and learn from what the database

00:30:21,840 --> 00:30:25,890
world has already been doing but now

00:30:23,610 --> 00:30:28,100
they've structured it into the big data

00:30:25,890 --> 00:30:31,559
world so predicates pushdowns

00:30:28,100 --> 00:30:35,070
if you write a filter so if you write

00:30:31,559 --> 00:30:38,970
something that's like data set map map

00:30:35,070 --> 00:30:41,520
filter but that filter the again it's

00:30:38,970 --> 00:30:44,520
all lazy so whenever it write whenever

00:30:41,520 --> 00:30:48,150
the the core goes to actually optimize

00:30:44,520 --> 00:30:51,240
this into the JVM code it's gonna sit

00:30:48,150 --> 00:30:53,400
there and say okay this that if that

00:30:51,240 --> 00:30:56,429
that filter that you wrote can go all

00:30:53,400 --> 00:30:58,320
the way down to the database layer and

00:30:56,429 --> 00:31:00,510
instead of pulling it out we can write

00:30:58,320 --> 00:31:03,690
that into like if you're if you're

00:31:00,510 --> 00:31:07,620
pointing to JDBC it'll say okay rather

00:31:03,690 --> 00:31:10,470
than saying select all from foo pulling

00:31:07,620 --> 00:31:13,169
it back in doing a map map filter you

00:31:10,470 --> 00:31:14,520
can it'll say hey that filter hasn't

00:31:13,169 --> 00:31:16,919
been touched yet we can actually push

00:31:14,520 --> 00:31:18,419
that all the way down into the there and

00:31:16,919 --> 00:31:21,929
now it's going to be select all from foo

00:31:18,419 --> 00:31:24,960
where whatever your filter is and so

00:31:21,929 --> 00:31:28,200
instead of getting a terabyte of data

00:31:24,960 --> 00:31:33,390
back maybe you get like a hundreds of

00:31:28,200 --> 00:31:36,419
gigabytes instead column pruning so if

00:31:33,390 --> 00:31:39,150
you actually call so like you basically

00:31:36,419 --> 00:31:42,030
say I want all my data and then at the

00:31:39,150 --> 00:31:42,930
end of it you through all your maps and

00:31:42,030 --> 00:31:44,220
filters and then

00:31:42,930 --> 00:31:49,020
even your projection your final

00:31:44,220 --> 00:31:51,600
projection you don't use like 20 of the

00:31:49,020 --> 00:31:53,520
columns it's gonna do that into the

00:31:51,600 --> 00:31:56,540
database layer it's going to say select

00:31:53,520 --> 00:31:58,860
the specific columns that were used and

00:31:56,540 --> 00:32:00,030
again that's just something that limits

00:31:58,860 --> 00:32:01,950
the data so whenever talking about

00:32:00,030 --> 00:32:04,380
terabytes or gigabytes hundreds of

00:32:01,950 --> 00:32:06,930
gigabytes of data if you're talking

00:32:04,380 --> 00:32:08,250
about limiting it by cutting the half of

00:32:06,930 --> 00:32:10,680
the columns out because you're not using

00:32:08,250 --> 00:32:12,180
them that's cutting your data down by

00:32:10,680 --> 00:32:13,620
half and that's all going over the

00:32:12,180 --> 00:32:17,490
network so that's kind of your slowest

00:32:13,620 --> 00:32:20,180
point at this point as well as it

00:32:17,490 --> 00:32:23,420
provides a uniform API and we'll get two

00:32:20,180 --> 00:32:23,420
data frames

00:32:50,550 --> 00:33:03,059
experts to optimize your queries huh I

00:32:54,350 --> 00:33:06,530
have used spark to I'm sure Lucas that

00:33:03,059 --> 00:33:07,710
concern sometimes these optimization

00:33:06,530 --> 00:33:16,380
engines

00:33:07,710 --> 00:33:19,679
hmm they tend to be oversold and yeah so

00:33:16,380 --> 00:33:24,320
so I yeah done I've spent my time my

00:33:19,679 --> 00:33:28,020
fare time in the sequel world and in

00:33:24,320 --> 00:33:30,410
general there so there's two paths so

00:33:28,020 --> 00:33:34,380
we'll see that but basically you can say

00:33:30,410 --> 00:33:36,540
spark is a spark sequel you can write

00:33:34,380 --> 00:33:38,490
sequel but you can also still write

00:33:36,540 --> 00:33:40,440
first of all if you really wanted to you

00:33:38,490 --> 00:33:42,120
can go down to the data frame later but

00:33:40,440 --> 00:33:43,679
you can also write things and I keep

00:33:42,120 --> 00:33:45,300
saying it but and we'll get to is data

00:33:43,679 --> 00:33:48,090
frames and data sets you can write snow

00:33:45,300 --> 00:33:49,950
write map filter things like that so you

00:33:48,090 --> 00:33:51,300
can still drop down and you can even go

00:33:49,950 --> 00:33:53,250
back and forth if you really wanted to

00:33:51,300 --> 00:33:54,570
to some extent you can say here's some

00:33:53,250 --> 00:33:57,210
sequel that I want to run against it

00:33:54,570 --> 00:34:03,360
it'll optimize it into it okay now I

00:33:57,210 --> 00:34:05,400
want to do XY and Z so if you I have

00:34:03,360 --> 00:34:08,700
seen some really like basically people

00:34:05,400 --> 00:34:11,010
who through doing support at like Bend

00:34:08,700 --> 00:34:13,679
I've seen people who have basically

00:34:11,010 --> 00:34:17,100
tried to port sequel into spark sequel

00:34:13,679 --> 00:34:19,080
and it's possible but it's not exactly

00:34:17,100 --> 00:34:21,510
you still should think about this stuff

00:34:19,080 --> 00:34:24,359
like if you're doing Union Union like

00:34:21,510 --> 00:34:26,129
I've seen some gnarly sequel code that

00:34:24,359 --> 00:34:29,820
people have tried to shove in to spark

00:34:26,129 --> 00:34:32,010
sequel and it'll work it's still not

00:34:29,820 --> 00:34:34,619
gonna be optimized to perfection because

00:34:32,010 --> 00:34:39,179
I mean you still have to think a little

00:34:34,619 --> 00:34:41,760
bit about your code and basically if

00:34:39,179 --> 00:34:44,580
you've got like hundreds of lines of

00:34:41,760 --> 00:34:47,639
sequel code probably doing something

00:34:44,580 --> 00:34:51,869
wrong if you ask me but that's the

00:34:47,639 --> 00:34:53,580
sequel world for a lot of people so I'm

00:34:51,869 --> 00:34:56,480
not trying to bash on anybody who may

00:34:53,580 --> 00:34:58,830
have that but it's things I've seen um

00:34:56,480 --> 00:35:00,000
but as far as cogeneration we'll

00:34:58,830 --> 00:35:02,040
actually get to some of the stuff that

00:35:00,000 --> 00:35:03,720
they've done it's phenomenal I think

00:35:02,040 --> 00:35:04,380
they've actually taken some of the stuff

00:35:03,720 --> 00:35:07,740
that they've done

00:35:04,380 --> 00:35:09,660
database world I think one of the things

00:35:07,740 --> 00:35:12,720
they've done which is called whole stage

00:35:09,660 --> 00:35:14,640
cogent is even better than what you

00:35:12,720 --> 00:35:18,150
would have would get with a database

00:35:14,640 --> 00:35:19,770
optimizations we should get to that but

00:35:18,150 --> 00:35:21,390
basically this is this is everything

00:35:19,770 --> 00:35:22,920
that goes in it's gonna say okay it's

00:35:21,390 --> 00:35:24,720
going to save a logical plan optimize

00:35:22,920 --> 00:35:27,510
logical plan and there's actually a UI

00:35:24,720 --> 00:35:30,930
where you can actually see the graph of

00:35:27,510 --> 00:35:32,760
of what does what at what level so you

00:35:30,930 --> 00:35:34,380
can you can see maybe where it's failing

00:35:32,760 --> 00:35:37,950
or whatever so there's there's actually

00:35:34,380 --> 00:35:41,220
built in monitoring low level Mon high

00:35:37,950 --> 00:35:45,990
level monitoring that you can get and

00:35:41,220 --> 00:35:47,490
dig into what's happening okay so like I

00:35:45,990 --> 00:35:50,850
said you can also go from sequel to

00:35:47,490 --> 00:35:54,510
brilli data frame to RDD to sequel you

00:35:50,850 --> 00:35:56,760
can go back and forth all you want so

00:35:54,510 --> 00:35:59,550
we've already seen this but again this

00:35:56,760 --> 00:36:03,210
is really where where it's at data

00:35:59,550 --> 00:36:04,680
frames data sets and we'll see what the

00:36:03,210 --> 00:36:06,990
difference is between game frames data

00:36:04,680 --> 00:36:09,300
sets in a second but this new API makes

00:36:06,990 --> 00:36:10,980
smart programs more concise and easier

00:36:09,300 --> 00:36:13,260
to understand and at the same time

00:36:10,980 --> 00:36:15,290
exposes more application semantics to

00:36:13,260 --> 00:36:18,210
the engine so whenever you write rdd's

00:36:15,290 --> 00:36:20,670
the engine will do a lot of cluster

00:36:18,210 --> 00:36:22,380
optimizations but it doesn't do things

00:36:20,670 --> 00:36:27,240
like predicate push down this and that

00:36:22,380 --> 00:36:28,710
because ultimately it's a black box mmm

00:36:27,240 --> 00:36:30,930
I'll save that because I believe I have

00:36:28,710 --> 00:36:33,150
a slide for it but a data frame is

00:36:30,930 --> 00:36:34,470
basically a table or two-dimensional

00:36:33,150 --> 00:36:36,840
array like structure in which each

00:36:34,470 --> 00:36:38,580
column contains measurements on one

00:36:36,840 --> 00:36:40,530
variable and each row contains one case

00:36:38,580 --> 00:36:45,480
so basically a data frame is a table of

00:36:40,530 --> 00:36:49,040
yes cool and that's what a data frame is

00:36:45,480 --> 00:36:51,000
a data set starts adding types to it and

00:36:49,040 --> 00:36:53,160
that's really the difference between

00:36:51,000 --> 00:36:54,870
data frames and data sets data frames

00:36:53,160 --> 00:36:58,860
are so one of the nice things that I

00:36:54,870 --> 00:37:01,200
actually didn't add with rdd's is you

00:36:58,860 --> 00:37:04,740
get the compiler unless you're writing

00:37:01,200 --> 00:37:06,510
in Python I guess but you get compiler

00:37:04,740 --> 00:37:08,670
errors so if the types don't match up

00:37:06,510 --> 00:37:10,170
then it's going to check in if you're

00:37:08,670 --> 00:37:12,900
using data frames that was the biggest

00:37:10,170 --> 00:37:15,720
PPAP peeve with people was you still

00:37:12,900 --> 00:37:18,390
didn't catch compiler like type checking

00:37:15,720 --> 00:37:20,339
type things data sets

00:37:18,390 --> 00:37:24,630
find some nice middle ground of both

00:37:20,339 --> 00:37:29,280
worlds so code ability so this is still

00:37:24,630 --> 00:37:31,380
much better than Hadoop but now four

00:37:29,280 --> 00:37:34,440
times skip going over exactly what it

00:37:31,380 --> 00:37:36,089
does in fact you can see what it does

00:37:34,440 --> 00:37:37,410
now because now we can say and this gets

00:37:36,089 --> 00:37:39,750
to can your question you can write that

00:37:37,410 --> 00:37:44,520
as a data frame where you say DF group

00:37:39,750 --> 00:37:45,930
by find the aggregate average age it's

00:37:44,520 --> 00:37:49,740
just the same you can write the sequel

00:37:45,930 --> 00:37:52,890
select the name the average age from DF

00:37:49,740 --> 00:37:55,230
table group by name so whatever your

00:37:52,890 --> 00:37:57,930
your flavor of preferences you can go

00:37:55,230 --> 00:37:59,280
with there in fact one thing that's

00:37:57,930 --> 00:38:00,839
really nice and really shows the

00:37:59,280 --> 00:38:02,910
optimizations is that I'm doing a group

00:38:00,839 --> 00:38:05,160
by here in general you're not going to

00:38:02,910 --> 00:38:06,930
do a group by and just kind of leave it

00:38:05,160 --> 00:38:10,050
at that

00:38:06,930 --> 00:38:11,970
in an RDD world because group bys are

00:38:10,050 --> 00:38:14,430
basically giant unions and things like

00:38:11,970 --> 00:38:17,040
that and it's really expensive but this

00:38:14,430 --> 00:38:19,500
a dataframe group by it actually knows

00:38:17,040 --> 00:38:21,540
that you're grouping by the name you're

00:38:19,500 --> 00:38:23,190
pulling out the age all those filters

00:38:21,540 --> 00:38:26,490
and predicates pushdowns all those

00:38:23,190 --> 00:38:28,109
optimizations that I talked about it can

00:38:26,490 --> 00:38:33,660
take advantage of and make that group ID

00:38:28,109 --> 00:38:35,640
not be nasty all right so the future

00:38:33,660 --> 00:38:37,530
spark performance with more efficient

00:38:35,640 --> 00:38:40,319
storage options advanced optimizers and

00:38:37,530 --> 00:38:41,730
direct operations on serialized data so

00:38:40,319 --> 00:38:44,160
this is another area that they've really

00:38:41,730 --> 00:38:46,849
optimized on is they actually have

00:38:44,160 --> 00:38:52,670
gotten down to writing at the core level

00:38:46,849 --> 00:38:56,640
they've written unsafe JVM so there's a

00:38:52,670 --> 00:38:59,849
Java dot unsafe I owe or whatever they

00:38:56,640 --> 00:39:03,440
actually have gone to a certain amount

00:38:59,849 --> 00:39:06,180
of byte management themselves where they

00:39:03,440 --> 00:39:09,240
store the data and like a really

00:39:06,180 --> 00:39:10,950
compressed format so that they can then

00:39:09,240 --> 00:39:13,470
even read it out without having to read

00:39:10,950 --> 00:39:14,760
the whole data back at points they know

00:39:13,470 --> 00:39:17,310
hey there here's a pointer to this

00:39:14,760 --> 00:39:20,670
specific point in the data so it

00:39:17,310 --> 00:39:24,170
optimizes deserializing serializing

00:39:20,670 --> 00:39:24,170
things like that and working against it

00:39:24,380 --> 00:39:28,589
alright so here yeah this is a slide

00:39:26,670 --> 00:39:31,250
that ever wanted to go with you so if

00:39:28,589 --> 00:39:33,760
you have an RDD you're gonna say

00:39:31,250 --> 00:39:37,339
takes a string and does an ends with

00:39:33,760 --> 00:39:39,349
option then ultimately it's a black box

00:39:37,339 --> 00:39:41,480
to the optimizer I mean it doesn't know

00:39:39,349 --> 00:39:43,220
that it does it's a type string it

00:39:41,480 --> 00:39:46,040
doesn't really know what you're doing it

00:39:43,220 --> 00:39:49,339
just knows that the output is going to

00:39:46,040 --> 00:39:51,680
be ends with is going to be a boolean so

00:39:49,339 --> 00:39:55,910
it's gonna say here's something and

00:39:51,680 --> 00:39:58,069
output is going to be a boolean so again

00:39:55,910 --> 00:39:59,990
all it really generates is an apply

00:39:58,069 --> 00:40:01,000
method taking in a string outputting a

00:39:59,990 --> 00:40:04,550
boolean

00:40:01,000 --> 00:40:06,349
whereas now if we say okay column X so

00:40:04,550 --> 00:40:07,970
we give it a little bit of information

00:40:06,349 --> 00:40:09,440
that no already knows about but it's

00:40:07,970 --> 00:40:13,160
saying hey this is specific to this

00:40:09,440 --> 00:40:14,990
column ends with post now it's going to

00:40:13,160 --> 00:40:17,060
actually write this thing that says ends

00:40:14,990 --> 00:40:18,980
with and it's going to even say okay

00:40:17,060 --> 00:40:22,310
this is a constant post so we're not

00:40:18,980 --> 00:40:23,780
gonna we can optimize there too so

00:40:22,310 --> 00:40:28,310
ultimately it becomes faster or smaller

00:40:23,780 --> 00:40:31,730
and smarter we'll come back a little bit

00:40:28,310 --> 00:40:36,770
to the great optimizations in sequel but

00:40:31,730 --> 00:40:40,280
I want to get to spark streaming so it's

00:40:36,770 --> 00:40:43,460
fast as well as it gets too pseudo

00:40:40,280 --> 00:40:47,800
exactly once because it's built on top

00:40:43,460 --> 00:40:47,800
of the fault tolerance that is our DD

00:40:47,920 --> 00:40:56,440
then it allows it to be it allows itself

00:40:52,160 --> 00:40:58,700
to be fault tolerant if you are doing

00:40:56,440 --> 00:41:01,339
the transfer mate that's right the

00:40:58,700 --> 00:41:03,500
transformations are exactly once because

00:41:01,339 --> 00:41:05,930
it's it's being managed and it's gonna

00:41:03,500 --> 00:41:08,839
have these these DAGs and rdd's that

00:41:05,930 --> 00:41:11,030
sort of things actions if you fail

00:41:08,839 --> 00:41:13,040
inside of an action you're it's still

00:41:11,030 --> 00:41:14,420
gonna run it it basically it's it failed

00:41:13,040 --> 00:41:16,550
it started to write something to the

00:41:14,420 --> 00:41:17,930
database it's gonna basically rewrite

00:41:16,550 --> 00:41:20,030
that same thing to the database so you

00:41:17,930 --> 00:41:22,160
still want to point to item potency at

00:41:20,030 --> 00:41:24,020
the action level but the transformations

00:41:22,160 --> 00:41:29,300
you're not gonna you're not going to get

00:41:24,020 --> 00:41:31,369
a data set back that has a and a as an

00:41:29,300 --> 00:41:33,140
example where a is your unique data set

00:41:31,369 --> 00:41:35,510
but it's it's gonna say hey it failed

00:41:33,140 --> 00:41:37,640
it's not gonna send back an a and then

00:41:35,510 --> 00:41:40,400
another a it's gonna send just that one

00:41:37,640 --> 00:41:44,480
back because of the again it's a it

00:41:40,400 --> 00:41:45,020
spark streaming is how many people here

00:41:44,480 --> 00:41:49,190
know

00:41:45,020 --> 00:41:51,980
have heard of mini-batch okay so sparks

00:41:49,190 --> 00:41:55,010
screaming the this is the the first lay

00:41:51,980 --> 00:41:57,080
level of spark streaming has something

00:41:55,010 --> 00:42:00,410
called a d-- stream and for a

00:41:57,080 --> 00:42:03,860
discretized stream and so it's not a

00:42:00,410 --> 00:42:06,130
true stream and it's basically because

00:42:03,860 --> 00:42:09,770
it's built on top of rdd's it's batched

00:42:06,130 --> 00:42:11,810
so it's basically saying take a bunch of

00:42:09,770 --> 00:42:13,820
stuff and then shove it down to be

00:42:11,810 --> 00:42:18,260
processed but it's doing it in mini

00:42:13,820 --> 00:42:23,660
batches they've taken that level of the

00:42:18,260 --> 00:42:26,630
original spark streaming to I think they

00:42:23,660 --> 00:42:28,869
could get maybe in the less than like

00:42:26,630 --> 00:42:33,350
around hundred milliseconds or so which

00:42:28,869 --> 00:42:36,110
realistically works for a lot of your

00:42:33,350 --> 00:42:37,220
real data sets it depends on your SLA s

00:42:36,110 --> 00:42:38,570
and things like that so if you're maybe

00:42:37,220 --> 00:42:39,650
working at a credit card company or

00:42:38,570 --> 00:42:43,550
something like that you want to do

00:42:39,650 --> 00:42:45,640
things a little bit faster but most most

00:42:43,550 --> 00:42:49,970
day to day work you can get away with

00:42:45,640 --> 00:42:51,650
mini batch but they actually have built

00:42:49,970 --> 00:42:54,200
it out and we'll get to that too to

00:42:51,650 --> 00:42:57,260
something called structured streaming so

00:42:54,200 --> 00:43:01,670
a streaming landscape before was storm

00:42:57,260 --> 00:43:05,119
there's kafka streams as well Samsa acha

00:43:01,670 --> 00:43:08,030
streams and the real competitor is

00:43:05,119 --> 00:43:13,340
something called flink flink

00:43:08,030 --> 00:43:17,900
I think I have a quote from them no I'm

00:43:13,340 --> 00:43:19,570
a duplicate myself here but flink they

00:43:17,900 --> 00:43:25,700
wrote an article that basically said

00:43:19,570 --> 00:43:27,770
batching is just a subset of streaming

00:43:25,700 --> 00:43:31,550
so basically you can think of it like

00:43:27,770 --> 00:43:34,070
hey we've thought of just like rdd's or

00:43:31,550 --> 00:43:37,490
spark made rdd's and everything with

00:43:34,070 --> 00:43:41,180
fault-tolerance built into it baby

00:43:37,490 --> 00:43:44,150
blink has built us an engine similar to

00:43:41,180 --> 00:43:47,119
spark where streaming is its native

00:43:44,150 --> 00:43:48,560
streaming now spark because of

00:43:47,119 --> 00:43:54,380
structured streaming which again I'll

00:43:48,560 --> 00:43:56,450
get to is something that spark really

00:43:54,380 --> 00:43:57,920
wet leaps and bounds above Madhu

00:43:56,450 --> 00:44:01,400
so it's

00:43:57,920 --> 00:44:04,670
gained tons of momentum flink in my

00:44:01,400 --> 00:44:08,060
opinion is never going to get fully past

00:44:04,670 --> 00:44:10,280
the early adopter phase strictly because

00:44:08,060 --> 00:44:12,440
spark has enough momentum and knows

00:44:10,280 --> 00:44:15,170
where its faults are that its invested

00:44:12,440 --> 00:44:19,820
time in there that flink doesn't so so

00:44:15,170 --> 00:44:23,360
again Hadoop spark so that's why people

00:44:19,820 --> 00:44:28,280
went to spark spark flank so it's not

00:44:23,360 --> 00:44:31,160
enough of a gap so and spark is gaining

00:44:28,280 --> 00:44:34,340
on that so realistically to relearn

00:44:31,160 --> 00:44:36,260
something as well as spark has we talked

00:44:34,340 --> 00:44:38,540
about and will briefly talk about it

00:44:36,260 --> 00:44:40,730
machine learning libraries that have

00:44:38,540 --> 00:44:42,860
things that are built-in so you don't

00:44:40,730 --> 00:44:44,900
need to learn a new thing you can just

00:44:42,860 --> 00:44:46,640
say hey I know my spark I can write

00:44:44,900 --> 00:44:48,230
machine learning why no but you still

00:44:46,640 --> 00:44:50,300
need to know machine learning but

00:44:48,230 --> 00:44:54,640
they've optimized some of it and I can

00:44:50,300 --> 00:44:57,610
write like my sequel my streaming my

00:44:54,640 --> 00:45:00,710
machine learning all has one basic layer

00:44:57,610 --> 00:45:02,630
where as Frank is just still in infancy

00:45:00,710 --> 00:45:05,060
on building out of machine learning and

00:45:02,630 --> 00:45:07,240
things like that so it's just my opinion

00:45:05,060 --> 00:45:10,160
but I don't think weeks yes I can with

00:45:07,240 --> 00:45:11,720
but as far as the original D streams go

00:45:10,160 --> 00:45:13,550
basically you have all these data

00:45:11,720 --> 00:45:16,490
sources that can kind of be streamed in

00:45:13,550 --> 00:45:18,050
they come in to spark streaming and then

00:45:16,490 --> 00:45:20,270
like I said it's mini batching so it's

00:45:18,050 --> 00:45:21,950
gonna say it argue gr D D and it's gonna

00:45:20,270 --> 00:45:26,270
shove it through it's gonna hit the core

00:45:21,950 --> 00:45:27,230
and then it's gonna save out just

00:45:26,270 --> 00:45:29,270
because I'm looking at the time I'm

00:45:27,230 --> 00:45:31,100
gonna speed up a little bit here but one

00:45:29,270 --> 00:45:34,760
of the libraries but like I keep

00:45:31,100 --> 00:45:36,680
mentioning is ml Lib they've built that

00:45:34,760 --> 00:45:39,710
on top so that was originally on top of

00:45:36,680 --> 00:45:45,530
rdd's they've now started to build it

00:45:39,710 --> 00:45:47,270
out on top of they've started to put it

00:45:45,530 --> 00:45:49,010
on front on top of data frames and data

00:45:47,270 --> 00:45:51,530
sets so it's even you get the

00:45:49,010 --> 00:45:53,690
optimizations there as well as from a

00:45:51,530 --> 00:45:55,070
machine learning side they can now they

00:45:53,690 --> 00:45:56,870
know more about your data they can even

00:45:55,070 --> 00:46:03,350
optimize on the machine learning side of

00:45:56,870 --> 00:46:04,670
algorithms let's see so things that are

00:46:03,350 --> 00:46:06,110
there they've got classification

00:46:04,670 --> 00:46:07,670
algorithms regression algorithms

00:46:06,110 --> 00:46:11,020
collaborative filtering clustering

00:46:07,670 --> 00:46:14,140
dimensional reduction feature extraction

00:46:11,020 --> 00:46:16,990
formation things that you can do with it

00:46:14,140 --> 00:46:22,660
are anti-spam fraud alerts for credit

00:46:16,990 --> 00:46:23,560
cards or whatever approval ratings your

00:46:22,660 --> 00:46:26,380
cluster graphs

00:46:23,560 --> 00:46:28,480
I think that's using k-means something

00:46:26,380 --> 00:46:32,410
like that as well as kind of getting

00:46:28,480 --> 00:46:34,060
just analytics out of things graphics

00:46:32,410 --> 00:46:35,950
I'm just gonna merely say that there is

00:46:34,060 --> 00:46:38,320
something all graphics there's they

00:46:35,950 --> 00:46:42,220
created something called graph frames

00:46:38,320 --> 00:46:44,740
which builds on data frames again but I

00:46:42,220 --> 00:46:51,370
haven't seen too much growth there so

00:46:44,740 --> 00:46:53,440
I'm just gonna kind of so now let's go

00:46:51,370 --> 00:46:56,110
to kind of the next generation of things

00:46:53,440 --> 00:46:57,550
so that was Allspark 1x and it's useful

00:46:56,110 --> 00:47:00,850
to kind of know where it's coming from

00:46:57,550 --> 00:47:02,350
and still it has a lot of relevance I've

00:47:00,850 --> 00:47:06,820
seen a lot of Stack Overflow questions

00:47:02,350 --> 00:47:09,880
that still are using rdd's right here

00:47:06,820 --> 00:47:12,900
let's get to what light Bend is focused

00:47:09,880 --> 00:47:16,390
on is big data is this big blobby thing

00:47:12,900 --> 00:47:18,460
but nowadays most people want fast data

00:47:16,390 --> 00:47:22,300
they want to see it basically they want

00:47:18,460 --> 00:47:25,570
to see streaming at scale so tons and

00:47:22,300 --> 00:47:27,970
tons of data now streaming so that's

00:47:25,570 --> 00:47:30,040
where the world's moving to so like Ben

00:47:27,970 --> 00:47:32,800
likes fast data so now we're talking

00:47:30,040 --> 00:47:37,990
about even more data so what is fast

00:47:32,800 --> 00:47:40,960
data basically we're gonna say hey big

00:47:37,990 --> 00:47:45,400
data was we're doing all this processing

00:47:40,960 --> 00:47:50,500
and then we go through and we get a

00:47:45,400 --> 00:47:51,850
results hey but big slow data even with

00:47:50,500 --> 00:47:53,050
spark it's still I mean you're talking

00:47:51,850 --> 00:47:55,120
about batch and you have to set up

00:47:53,050 --> 00:47:59,170
something that's gonna do in a process

00:47:55,120 --> 00:48:02,410
air or Rica kit off or whatever so

00:47:59,170 --> 00:48:04,210
what's better big fast data so again

00:48:02,410 --> 00:48:07,180
whenever we say fast data it's really

00:48:04,210 --> 00:48:09,730
big fast data is what what I'm referring

00:48:07,180 --> 00:48:11,560
to and so the difference here is you

00:48:09,730 --> 00:48:14,560
have to do your same thing you've got

00:48:11,560 --> 00:48:17,440
your cluster your incoming data and then

00:48:14,560 --> 00:48:20,170
basically you have okay got my answer

00:48:17,440 --> 00:48:22,480
got my answer got my answer it's going

00:48:20,170 --> 00:48:25,799
to kind of keep processing and nothing

00:48:22,480 --> 00:48:28,210
stops it's it's streaming through

00:48:25,799 --> 00:48:30,579
and so again that's kind of what I was

00:48:28,210 --> 00:48:37,119
saying that batch is really just a a

00:48:30,579 --> 00:48:39,519
subset of streaming just things that you

00:48:37,119 --> 00:48:43,089
can do with festive fast data take tough

00:48:39,519 --> 00:48:46,029
for you get that for time all right so

00:48:43,089 --> 00:48:47,559
structured streaming so i went over to

00:48:46,029 --> 00:48:49,059
what these streams are and these strings

00:48:47,559 --> 00:48:51,730
are still useful because I think

00:48:49,059 --> 00:48:53,589
structured streaming has gone leaps and

00:48:51,730 --> 00:48:58,150
bounds but it's still somewhat on its

00:48:53,589 --> 00:49:00,220
infancy but structured streaming is

00:48:58,150 --> 00:49:02,079
definitely the next level of things so

00:49:00,220 --> 00:49:04,960
simplest way to perform streaming

00:49:02,079 --> 00:49:06,609
analytics is not to have to stream or is

00:49:04,960 --> 00:49:07,029
not having to reason about streaming at

00:49:06,609 --> 00:49:10,269
all

00:49:07,029 --> 00:49:15,339
so how many people here know about

00:49:10,269 --> 00:49:17,769
streaming Windows and and dealing with

00:49:15,339 --> 00:49:18,460
state across your streams and things

00:49:17,769 --> 00:49:20,019
like that

00:49:18,460 --> 00:49:22,750
anybody have to deal with streaming

00:49:20,019 --> 00:49:26,890
stuff and state and and mute and all

00:49:22,750 --> 00:49:44,589
mutability or anything like that so it

00:49:26,890 --> 00:49:47,200
would be questioner yes so let me put it

00:49:44,589 --> 00:49:49,299
in a difference between streaming about

00:49:47,200 --> 00:49:52,750
is the better way to put it so batch is

00:49:49,299 --> 00:49:56,910
saying I have all this data and I'm

00:49:52,750 --> 00:50:00,609
going to take it from time to time B and

00:49:56,910 --> 00:50:02,680
shove it in process it and then later on

00:50:00,609 --> 00:50:04,359
I said okay what does that be I'm gonna

00:50:02,680 --> 00:50:06,400
do this and it's just manually kicked

00:50:04,359 --> 00:50:09,039
off and it's just a job that says here's

00:50:06,400 --> 00:50:10,680
here's all my data all at once go

00:50:09,039 --> 00:50:14,069
process through okay

00:50:10,680 --> 00:50:18,039
whereas streaming is basically saying

00:50:14,069 --> 00:50:20,109
here's a start stream start processing

00:50:18,039 --> 00:50:22,779
it'll start processing it and then B

00:50:20,109 --> 00:50:25,269
comes in and it just keep basically it's

00:50:22,779 --> 00:50:27,339
continuous processing versus batch

00:50:25,269 --> 00:50:29,710
processing now that's probably the best

00:50:27,339 --> 00:50:32,009
way to really think about it is bad

00:50:29,710 --> 00:50:34,180
processing is stuttery

00:50:32,009 --> 00:50:36,039
whenever I say streaming and things like

00:50:34,180 --> 00:50:38,380
that it's the same thing it's just

00:50:36,039 --> 00:50:41,350
saying you don't have to wait

00:50:38,380 --> 00:50:43,000
for it to come in so first of all maybe

00:50:41,350 --> 00:50:44,980
you have to you say hey I wait every

00:50:43,000 --> 00:50:46,840
hour I'm going to run a bad job here you

00:50:44,980 --> 00:50:48,850
you could like I said for these dreams

00:50:46,840 --> 00:50:51,670
you could get down to check every 100

00:50:48,850 --> 00:50:53,920
milliseconds so actually continuously

00:50:51,670 --> 00:50:55,630
stream that the program theoretically

00:50:53,920 --> 00:50:58,180
never stops it's going to keep going

00:50:55,630 --> 00:51:01,180
keep processing during all of your Big

00:50:58,180 --> 00:51:02,850
Data stuff and one of the hard things

00:51:01,180 --> 00:51:05,410
with streaming is machine learning

00:51:02,850 --> 00:51:08,440
that's one thing that's data bricks is

00:51:05,410 --> 00:51:11,440
and spark is taking on is saying hey we

00:51:08,440 --> 00:51:14,800
can actually kind of on-the-fly reef

00:51:11,440 --> 00:51:17,410
rejigger your your money your data model

00:51:14,800 --> 00:51:18,760
with this new data and then shove it

00:51:17,410 --> 00:51:23,340
back into the stream and now you have

00:51:18,760 --> 00:51:23,340
this updated algorithm to check against

00:51:30,860 --> 00:51:34,010
[Music]

00:51:34,050 --> 00:51:39,160
as far as I understand I will say

00:51:36,430 --> 00:51:42,100
machine learning is my weakest point so

00:51:39,160 --> 00:51:46,180
I can speak of it but not really that

00:51:42,100 --> 00:51:47,580
deep but I have seen some algorithms

00:51:46,180 --> 00:51:50,800
that they've written with streaming

00:51:47,580 --> 00:51:53,410
where they can get real-time prediction

00:51:50,800 --> 00:51:55,000
analysis across like and in fact they

00:51:53,410 --> 00:51:58,240
made a mistake at one of the SPARC

00:51:55,000 --> 00:52:01,630
summits they did a rather than grabbing

00:51:58,240 --> 00:52:03,880
a dataset for their their keynote talk

00:52:01,630 --> 00:52:07,510
and and pretending to do stuff and

00:52:03,880 --> 00:52:09,850
having real data but they hooked up to

00:52:07,510 --> 00:52:11,890
the Twitter stream and there were some

00:52:09,850 --> 00:52:13,270
profanity on that screen but it was

00:52:11,890 --> 00:52:15,670
because it was picking out things like

00:52:13,270 --> 00:52:17,440
that that realistically fit the

00:52:15,670 --> 00:52:20,860
algorithm and it was constantly adapting

00:52:17,440 --> 00:52:23,200
and things like that so if it can

00:52:20,860 --> 00:52:24,670
constantly adapt and and basically say

00:52:23,200 --> 00:52:27,160
hey here's the model what's stored off

00:52:24,670 --> 00:52:29,430
grab it story at gratis or it's that

00:52:27,160 --> 00:52:29,430
same thing

00:52:32,430 --> 00:52:43,230
people they doing is actually sample

00:52:34,650 --> 00:52:46,050
stream so you can sample the data and if

00:52:43,230 --> 00:52:48,480
you give it a million times a five

00:52:46,050 --> 00:52:51,960
percent error or a ten percent error and

00:52:48,480 --> 00:53:03,180
given that that major sample data less

00:52:51,960 --> 00:53:05,010
so there are sampling techniques because

00:53:03,180 --> 00:53:06,960
I was hoping to be about an hour but I

00:53:05,010 --> 00:53:10,140
probably wound up being about an hour

00:53:06,960 --> 00:53:14,820
and a half and then do you live in any

00:53:10,140 --> 00:53:19,620
additional questions so whatever we get

00:53:14,820 --> 00:53:21,930
to the the difference here so if I wrote

00:53:19,620 --> 00:53:25,400
a data frame data frame select the name

00:53:21,930 --> 00:53:29,640
age where age 21 the nice thing about

00:53:25,400 --> 00:53:32,250
the new age of streaming is now here we

00:53:29,640 --> 00:53:34,710
said okay so this is this is your core

00:53:32,250 --> 00:53:37,680
logic your primary logic and then here

00:53:34,710 --> 00:53:40,380
you're saying spark read it's in the

00:53:37,680 --> 00:53:42,360
format Jason and then load it and here

00:53:40,380 --> 00:53:44,870
you're saying output right format that

00:53:42,360 --> 00:53:48,690
you're trying to and saving you okay

00:53:44,870 --> 00:53:49,890
primary logic of your of streaming logic

00:53:48,690 --> 00:53:51,720
so the one thing that I was saying

00:53:49,890 --> 00:53:53,940
before is streaming has the idea of

00:53:51,720 --> 00:53:56,400
Windows and and windows of time that

00:53:53,940 --> 00:53:57,060
you're working on and whatever for the

00:53:56,400 --> 00:53:59,400
most part

00:53:57,060 --> 00:54:01,920
spark streaming or structured streaming

00:53:59,400 --> 00:54:03,540
has tried to get so that you'll have to

00:54:01,920 --> 00:54:05,790
worry about that so your core logic

00:54:03,540 --> 00:54:08,400
stays the same and if you now all you

00:54:05,790 --> 00:54:11,880
have to do is say read stream write the

00:54:08,400 --> 00:54:14,040
stream and start so you're keeping

00:54:11,880 --> 00:54:16,530
basically the same logic as your batch

00:54:14,040 --> 00:54:21,030
logic and you're changing like three

00:54:16,530 --> 00:54:24,780
lines of code so it's actually much

00:54:21,030 --> 00:54:26,610
easier that way so again this is just

00:54:24,780 --> 00:54:30,300
pointing out that it's really just

00:54:26,610 --> 00:54:34,230
continuous processing you can take kind

00:54:30,300 --> 00:54:36,030
of the subsets of data at a time so you

00:54:34,230 --> 00:54:37,530
get a streamlined API so I didn't really

00:54:36,030 --> 00:54:39,120
actually go over I thought that I had a

00:54:37,530 --> 00:54:40,650
I must have either gotten out of order

00:54:39,120 --> 00:54:44,310
or taking stuff out because like I said

00:54:40,650 --> 00:54:44,720
I don't want to go to low level but data

00:54:44,310 --> 00:54:47,180
frame

00:54:44,720 --> 00:54:53,240
are kind of the best of both worlds

00:54:47,180 --> 00:54:55,010
where you have your your they're

00:54:53,240 --> 00:54:57,020
actually datasets are the best with data

00:54:55,010 --> 00:54:58,700
frames which we're getting here but data

00:54:57,020 --> 00:55:01,579
frames allows you to do what I was

00:54:58,700 --> 00:55:03,470
saying is write sequel or write your map

00:55:01,579 --> 00:55:05,510
logic so it's still right allows you to

00:55:03,470 --> 00:55:08,270
write functional logic if you want to to

00:55:05,510 --> 00:55:10,280
drop down to the RDD level and then come

00:55:08,270 --> 00:55:15,890
back to the data frame and write things

00:55:10,280 --> 00:55:17,720
at a more sequel like fashion where

00:55:15,890 --> 00:55:21,740
you're telling the engine what you want

00:55:17,720 --> 00:55:24,470
not how to do it but then so a date of

00:55:21,740 --> 00:55:25,970
data frames were great but people

00:55:24,470 --> 00:55:28,880
complain about the type of this

00:55:25,970 --> 00:55:31,430
so from a functional aspect you didn't

00:55:28,880 --> 00:55:33,500
you you could say hey drop down from a

00:55:31,430 --> 00:55:37,339
row so that's what data frames store is

00:55:33,500 --> 00:55:40,130
a row and so row is just a here's an

00:55:37,339 --> 00:55:42,800
object that's it's your row in your

00:55:40,130 --> 00:55:44,240
table we don't we know kind of stuff

00:55:42,800 --> 00:55:47,119
about it but we don't we can't guarantee

00:55:44,240 --> 00:55:51,230
the types and this and that so that's

00:55:47,119 --> 00:55:53,089
what a data frame is whereas now that's

00:55:51,230 --> 00:56:01,819
just the new stuff so like I said sparks

00:55:53,089 --> 00:56:03,760
session a data set itself can be tight

00:56:01,819 --> 00:56:06,109
so you could say instead of being this a

00:56:03,760 --> 00:56:11,630
morphic kind of row thing you could say

00:56:06,109 --> 00:56:13,819
data set that is of type maybe a couple

00:56:11,630 --> 00:56:15,800
of three so it's a string and integer or

00:56:13,819 --> 00:56:17,510
a string or something like that so you

00:56:15,800 --> 00:56:19,520
can actually do that and work on your

00:56:17,510 --> 00:56:20,690
data now whatever you call map it's

00:56:19,520 --> 00:56:22,940
actually going to type check it and

00:56:20,690 --> 00:56:25,400
things like that and it's still they for

00:56:22,940 --> 00:56:29,990
a certain level they then still are able

00:56:25,400 --> 00:56:31,310
to get the logic out of it but you start

00:56:29,990 --> 00:56:33,500
to go if you start to go to the

00:56:31,310 --> 00:56:35,210
functional level of things you do start

00:56:33,500 --> 00:56:37,670
to lose go back into the black box

00:56:35,210 --> 00:56:39,829
territory a little bit because again

00:56:37,670 --> 00:56:47,270
you're you're writing raw code and it

00:56:39,829 --> 00:56:48,349
doesn't know what that is okay so one of

00:56:47,270 --> 00:56:51,020
the things that they have is called

00:56:48,349 --> 00:56:53,510
tungsten that's kind of the catalyst

00:56:51,020 --> 00:56:56,030
optimizer that's the thing that figures

00:56:53,510 --> 00:56:57,509
out what your code what you want your

00:56:56,030 --> 00:57:02,009
code to be doing and how to

00:56:57,509 --> 00:57:04,439
to my zit so inspark - they added a

00:57:02,009 --> 00:57:06,529
what's called home stage cogent actually

00:57:04,439 --> 00:57:09,359
as long as I didn't get rid of that -

00:57:06,529 --> 00:57:12,419
I'll show you how how that changes

00:57:09,359 --> 00:57:14,609
things I have a nice graphic for that

00:57:12,419 --> 00:57:17,489
but basically it it speeds up your

00:57:14,609 --> 00:57:19,499
processing because one thing that rdd's

00:57:17,489 --> 00:57:23,029
that they could do and data frames and

00:57:19,499 --> 00:57:26,309
all that is they could say hey I'm gonna

00:57:23,029 --> 00:57:28,709
run a map and a map and it's gonna do

00:57:26,309 --> 00:57:32,429
those and combine them into like kind of

00:57:28,709 --> 00:57:34,229
one it's earth it's going to pigtail

00:57:32,429 --> 00:57:36,139
them together so it's gonna say take the

00:57:34,229 --> 00:57:38,880
output and don't do all of it together

00:57:36,139 --> 00:57:41,279
take almost a stream of map so it's

00:57:38,880 --> 00:57:43,259
going to say map get the output shove it

00:57:41,279 --> 00:57:45,269
down to the next map until you get to an

00:57:43,259 --> 00:57:48,299
action or something that has to pull it

00:57:45,269 --> 00:57:51,239
together and it's going to do it kind of

00:57:48,299 --> 00:57:52,949
one at a time so that you get kind of

00:57:51,239 --> 00:57:56,039
that optimization you don't have to say

00:57:52,949 --> 00:57:57,809
do a bunch of stuff okay now do the next

00:57:56,039 --> 00:58:00,239
map now the next map it can just say

00:57:57,809 --> 00:58:03,089
flow through flow through flow through

00:58:00,239 --> 00:58:07,079
which reduces on virtual calls and

00:58:03,089 --> 00:58:09,179
things like that and again we'll get to

00:58:07,079 --> 00:58:10,889
what whole stage code gen is but the

00:58:09,179 --> 00:58:13,589
biggest thing you get there is even as

00:58:10,889 --> 00:58:16,109
fast as SPARC is they got five to ten

00:58:13,589 --> 00:58:17,429
speed ten times speed ups and basically

00:58:16,109 --> 00:58:20,839
you can think of spark as a compiler

00:58:17,429 --> 00:58:20,839
it's kind of almost getting as you go

00:58:21,979 --> 00:58:27,509
spark - they expanded sequel

00:58:25,429 --> 00:58:30,089
accumulators so if you need to

00:58:27,509 --> 00:58:32,819
accumulate data there's a specific API

00:58:30,089 --> 00:58:36,209
out there for it they simplify that in

00:58:32,819 --> 00:58:37,679
the newer versions data frame focused

00:58:36,209 --> 00:58:39,630
machine learning so like I said it used

00:58:37,679 --> 00:58:40,979
to be based on rdd's now they're machine

00:58:39,630 --> 00:58:43,019
learning libraries are based on data

00:58:40,979 --> 00:58:47,939
frames so you get a lot of benefits of

00:58:43,019 --> 00:58:50,249
all that Scalla 211 is the de-facto so

00:58:47,939 --> 00:58:53,899
it used to be Scala 210 in fact we're

00:58:50,249 --> 00:58:56,819
trying to get them to go up to Scala 212

00:58:53,899 --> 00:58:59,099
but there's some resistance there but

00:58:56,819 --> 00:59:00,569
again it gets to that they're trying to

00:58:59,099 --> 00:59:00,989
go with compatibility and things like

00:59:00,569 --> 00:59:03,869
that

00:59:00,989 --> 00:59:06,649
so if you up the versions then you have

00:59:03,869 --> 00:59:06,649
to change some stuff

00:59:07,140 --> 00:59:17,640
all right so sparks equal Y datasets so

00:59:13,089 --> 00:59:22,210
here we have the RDD skates type checked

00:59:17,640 --> 00:59:24,220
but it's a black box data frames great

00:59:22,210 --> 00:59:27,609
now we have this catalyst optimization

00:59:24,220 --> 00:59:29,799
but it's not tight checked so that's

00:59:27,609 --> 00:59:31,480
where like I said why we like data sets

00:59:29,799 --> 00:59:33,309
is because it's kind of giving you the

00:59:31,480 --> 00:59:38,410
best of both worlds it's giving you this

00:59:33,309 --> 00:59:40,210
engine and it's type checking a data set

00:59:38,410 --> 00:59:41,829
is a strongly typed immutable collection

00:59:40,210 --> 00:59:43,599
of objects that are mapped to a

00:59:41,829 --> 00:59:45,220
relational schema so basically it

00:59:43,599 --> 00:59:47,319
strongly typed but it's still this

00:59:45,220 --> 00:59:49,329
relational schema so you can whenever

00:59:47,319 --> 00:59:51,309
you throw in that relational schema it

00:59:49,329 --> 00:59:53,190
means that those certain things about it

00:59:51,309 --> 00:59:57,430
and you can optimize so again it's

00:59:53,190 --> 00:59:57,999
together so this is what I was saying

00:59:57,430 --> 01:00:00,670
before that

00:59:57,999 --> 01:00:02,769
okay so data set of my class so now you

01:00:00,670 --> 01:00:04,509
can actually do that but if you want to

01:00:02,769 --> 01:00:07,059
you could still do a data frame so

01:00:04,509 --> 01:00:08,680
anytime you see in the API data frame it

01:00:07,059 --> 01:00:15,999
means that it's really a data set of

01:00:08,680 --> 01:00:18,039
this amorphic row ok so a little bit

01:00:15,999 --> 01:00:20,650
more of why is basically if I write this

01:00:18,039 --> 01:00:23,980
code that says hey my schema is a

01:00:20,650 --> 01:00:26,200
boolean type I'm going to paralyze it

01:00:23,980 --> 01:00:28,779
and you see that the row it's a 0 true

01:00:26,200 --> 01:00:31,569
so it's got an integer a boolean a

01:00:28,779 --> 01:00:33,759
string so whenever you create the data

01:00:31,569 --> 01:00:35,079
frame you actually call collect and then

01:00:33,759 --> 01:00:38,739
at runtime you're going to get a class

01:00:35,079 --> 01:00:40,269
cast exception with data sets you change

01:00:38,739 --> 01:00:43,119
all you do is change it to a data set

01:00:40,269 --> 01:00:44,829
now it's going to give you a it's going

01:00:43,119 --> 01:00:47,259
to actually realize that these aren't

01:00:44,829 --> 01:00:51,279
this these aren't right and it's going

01:00:47,259 --> 01:00:54,220
to actually give you a compiler error so

01:00:51,279 --> 01:00:55,839
nice thing is combines functional with

01:00:54,220 --> 01:01:00,749
relational you get this nice middle

01:00:55,839 --> 01:01:02,440
ground ok as well as still faster

01:01:00,749 --> 01:01:03,999
compression things that we've already

01:01:02,440 --> 01:01:10,049
talked about coming back to spark

01:01:03,999 --> 01:01:12,670
streaming these streams so just soso

01:01:10,049 --> 01:01:14,619
from a flow perspective you may see this

01:01:12,670 --> 01:01:18,130
being a little disjointed I actually had

01:01:14,619 --> 01:01:19,390
a combined all of my to do my how to use

01:01:18,130 --> 01:01:21,039
of spark that

01:01:19,390 --> 01:01:22,329
changed over the time and trying to

01:01:21,039 --> 01:01:26,470
shove them together but as you can see

01:01:22,329 --> 01:01:27,819
some of them fell out of order but these

01:01:26,470 --> 01:01:30,549
streams we've already kind of gone over

01:01:27,819 --> 01:01:33,730
that this chunk of time is going to be

01:01:30,549 --> 01:01:35,619
in this RDD then is you're gonna have

01:01:33,730 --> 01:01:39,490
one second two second three second all

01:01:35,619 --> 01:01:41,440
that stuff and so along the way it

01:01:39,490 --> 01:01:43,779
basically just shoves things down into

01:01:41,440 --> 01:01:46,630
these functions so it's going to be

01:01:43,779 --> 01:01:50,500
running against parkour and along the

01:01:46,630 --> 01:01:53,019
way it's saving and printing so here

01:01:50,500 --> 01:01:54,400
it's going to save and at the same time

01:01:53,019 --> 01:01:56,859
while it's kind of saving it's still

01:01:54,400 --> 01:01:59,140
gathering this data so it's doing stuff

01:01:56,859 --> 01:02:00,460
down here but it's gathering now it's

01:01:59,140 --> 01:02:01,569
still gathering the data so that's

01:02:00,460 --> 01:02:04,690
really what we're talking about about

01:02:01,569 --> 01:02:07,329
these rdd's of things so it's it's

01:02:04,690 --> 01:02:10,029
storing it into this this block of data

01:02:07,329 --> 01:02:12,010
set that can be understood better and

01:02:10,029 --> 01:02:13,690
then all of a sudden this finishes now

01:02:12,010 --> 01:02:17,460
it can go through and finish the rest of

01:02:13,690 --> 01:02:22,450
it for thee it can grab the next batch

01:02:17,460 --> 01:02:24,130
stateful streaming basically actually

01:02:22,450 --> 01:02:27,490
for time I'm going to skip that and

01:02:24,130 --> 01:02:28,990
again if you feel necessary we can learn

01:02:27,490 --> 01:02:34,240
we can dig more into streaming and

01:02:28,990 --> 01:02:36,579
things like that I've kind of the

01:02:34,240 --> 01:02:38,700
training that I have set up in a few

01:02:36,579 --> 01:02:38,700
weeks

01:02:40,890 --> 01:02:44,319
wasn't certified have enough time so I

01:02:43,119 --> 01:02:47,230
wanted to at least have it there

01:02:44,319 --> 01:02:49,420
um checkpointing is an important thing

01:02:47,230 --> 01:02:51,670
to understand with these streams though

01:02:49,420 --> 01:02:53,650
and really rdd's you can throw it in

01:02:51,670 --> 01:02:55,900
yourself but basically you have this

01:02:53,650 --> 01:02:59,230
thing that's built this whole giant

01:02:55,900 --> 01:03:01,119
thing up to n plus one and all of a

01:02:59,230 --> 01:03:04,180
sudden it crashes you don't have to

01:03:01,119 --> 01:03:06,609
start your stream again and do 1 2 3 4

01:03:04,180 --> 01:03:10,049
etc so it's going to take all the time

01:03:06,609 --> 01:03:13,630
to rebuild your where your data was that

01:03:10,049 --> 01:03:15,640
where as spark streaming is going to say

01:03:13,630 --> 01:03:21,220
okay I have one it's going to check

01:03:15,640 --> 01:03:22,660
point it get go to all of that say

01:03:21,220 --> 01:03:24,730
synchronous or the check point is it

01:03:22,660 --> 01:03:27,269
check pointing is asynchronous and it's

01:03:24,730 --> 01:03:29,170
gonna build it out now whenever it fails

01:03:27,269 --> 01:03:31,059
it's going to have these check points

01:03:29,170 --> 01:03:32,259
that it's going to build out and instead

01:03:31,059 --> 01:03:34,660
it's going to start from here so it's

01:03:32,259 --> 01:03:35,829
similar to how it works off of caching

01:03:34,660 --> 01:03:38,410
and memory except this is more

01:03:35,829 --> 01:03:40,089
persistent and it's going to say hey

01:03:38,410 --> 01:03:47,970
start from here and start building up

01:03:40,089 --> 01:03:56,499
again so that was D strings oh yeah

01:03:47,970 --> 01:04:00,299
actually she either way we're actually

01:03:56,499 --> 01:04:02,829
gonna skip that for time okay so

01:04:00,299 --> 01:04:05,650
structured streaming is going to say hey

01:04:02,829 --> 01:04:09,880
we have this stream of data so we have

01:04:05,650 --> 01:04:11,710
the idea of data frames but now instead

01:04:09,880 --> 01:04:15,640
of thinking about it as this block of

01:04:11,710 --> 01:04:17,230
data it's going to be some data and some

01:04:15,640 --> 01:04:19,269
more data so it's going to keep coming

01:04:17,230 --> 01:04:22,200
in flowing in but it's the same logic

01:04:19,269 --> 01:04:22,200
that we had before

01:04:24,790 --> 01:04:30,890
so we've already seen this actually I

01:04:29,030 --> 01:04:36,730
could have just cut this out so read

01:04:30,890 --> 01:04:36,730
stream write stream instead and start

01:04:37,870 --> 01:04:44,240
okay so one thing that you get from

01:04:41,420 --> 01:04:49,570
structured streaming is you get the

01:04:44,240 --> 01:04:52,160
catalyst optimizer as well as you get

01:04:49,570 --> 01:04:57,020
it's a one-to-one graph so for the most

01:04:52,160 --> 01:05:01,280
part if you use a certain data sets so

01:04:57,020 --> 01:05:02,990
Kafka with with offsets and even writing

01:05:01,280 --> 01:05:08,090
out to Kafka I forget those certain

01:05:02,990 --> 01:05:11,450
things but if you use specific data sets

01:05:08,090 --> 01:05:12,350
that have guarantees then you can have

01:05:11,450 --> 01:05:15,440
exactly once

01:05:12,350 --> 01:05:17,510
so basically saying okay if you use

01:05:15,440 --> 01:05:22,550
Kafka it's gonna say hey it failed and

01:05:17,510 --> 01:05:25,190
then it's going to rewrite the data and

01:05:22,550 --> 01:05:26,870
it's gonna say hey you go to this offset

01:05:25,190 --> 01:05:29,090
in Kafka and it's gonna natively know

01:05:26,870 --> 01:05:30,740
okay where I'm reading from is from the

01:05:29,090 --> 01:05:32,900
right point I know that this is the last

01:05:30,740 --> 01:05:34,640
place that I have the data and even

01:05:32,900 --> 01:05:36,800
outputting it it's going to say okay

01:05:34,640 --> 01:05:38,630
here's the offset that it was going to I

01:05:36,800 --> 01:05:41,780
know that that hasn't been written I can

01:05:38,630 --> 01:05:44,750
rewrite it so they have to sum to a

01:05:41,780 --> 01:05:50,390
limited subset of things you can have

01:05:44,750 --> 01:05:56,630
exactly one streaming as well as they

01:05:50,390 --> 01:06:02,570
have real-time store as of spark 2.3

01:05:56,630 --> 01:06:07,370
they have real-time streaming so these

01:06:02,570 --> 01:06:09,170
strings we said was you could get about

01:06:07,370 --> 01:06:12,020
100 milliseconds because it's all babs

01:06:09,170 --> 01:06:14,180
together they actually have a way to

01:06:12,020 --> 01:06:18,610
turn on continue what they call

01:06:14,180 --> 01:06:18,610
continuous processing and data sets and

01:06:18,700 --> 01:06:23,630
it reconfigures the code so that

01:06:22,100 --> 01:06:26,540
actually you feeds it in and it's

01:06:23,630 --> 01:06:29,360
actually a real-time stream and I think

01:06:26,540 --> 01:06:31,520
they've gotten it down to like the net

01:06:29,360 --> 01:06:34,220
like ones of milliseconds or something

01:06:31,520 --> 01:06:36,170
like that so they they basically have

01:06:34,220 --> 01:06:38,240
said hey we with structured streaming

01:06:36,170 --> 01:06:41,450
you can write this equal like code

01:06:38,240 --> 01:06:44,300
and be processing it all the time as it

01:06:41,450 --> 01:06:46,190
comes in so the difference between as it

01:06:44,300 --> 01:06:48,050
comes in streaming and back streaming is

01:06:46,190 --> 01:06:49,690
batches saying hey we're not going to

01:06:48,050 --> 01:06:52,849
process it until we get to this point

01:06:49,690 --> 01:06:54,650
whereas real time continuous streaming

01:06:52,849 --> 01:06:56,240
is saying we got an object processing we

01:06:54,650 --> 01:06:59,089
got an object process we got an object

01:06:56,240 --> 01:07:02,780
processes and so they had to do some

01:06:59,089 --> 01:07:04,130
work to make sure that the exactly wants

01:07:02,780 --> 01:07:06,230
and things like that the guarantees that

01:07:04,130 --> 01:07:08,599
they had would remain high even in

01:07:06,230 --> 01:07:10,190
continuous but it's still there's still

01:07:08,599 --> 01:07:12,589
some unsupported operations within

01:07:10,190 --> 01:07:17,180
structured streaming because then you

01:07:12,589 --> 01:07:18,980
lose some of those guarantees so you can

01:07:17,180 --> 01:07:20,990
kind of see and I believe the compiler

01:07:18,980 --> 01:07:22,580
will yell at you for stuff like that but

01:07:20,990 --> 01:07:24,740
it's still somewhat in its experimental

01:07:22,580 --> 01:07:31,760
phase I think technically it's lists it

01:07:24,740 --> 01:07:36,290
is listed as experiment 1 2 3 still now

01:07:31,760 --> 01:07:39,140
in this part 2 series originally let's

01:07:36,290 --> 01:07:42,230
go here originally we said spark was

01:07:39,140 --> 01:07:44,480
built and memory like writing to get

01:07:42,230 --> 01:07:46,130
back and forth to the disk was the

01:07:44,480 --> 01:07:48,230
bottleneck so spark was great it said

01:07:46,130 --> 01:07:50,510
hey here we're gonna grow here the

01:07:48,230 --> 01:07:52,609
internet was a kind of a bottle in that

01:07:50,510 --> 01:07:54,230
too but now there's the internet is

01:07:52,609 --> 01:07:58,130
actually pretty darn fast especially

01:07:54,230 --> 01:07:59,690
within companies so now the processing

01:07:58,130 --> 01:08:03,589
is your slow point so you're running

01:07:59,690 --> 01:08:05,660
these complex algorithms and so they're

01:08:03,589 --> 01:08:08,570
their focus in spark those spark to

01:08:05,660 --> 01:08:10,640
series is how can we optimize your code

01:08:08,570 --> 01:08:14,869
even more than like data frames and

01:08:10,640 --> 01:08:16,609
things like that so what they've done as

01:08:14,869 --> 01:08:19,700
well as the focus is on continuous

01:08:16,609 --> 01:08:20,989
processing so what they've done is this

01:08:19,700 --> 01:08:25,420
is what I was talking about before

01:08:20,989 --> 01:08:28,339
is and it's really cool so the typical

01:08:25,420 --> 01:08:30,710
the typical process and even the

01:08:28,339 --> 01:08:37,609
database world is called a volcano model

01:08:30,710 --> 01:08:40,009
so what that means is okay yeah so you

01:08:37,609 --> 01:08:40,489
get your data I thought I heard the text

01:08:40,009 --> 01:08:42,650
at first

01:08:40,489 --> 01:08:44,569
and you're gonna transform it and one of

01:08:42,650 --> 01:08:46,160
so we saw that there was a ton of data

01:08:44,569 --> 01:08:47,569
down here selves one thing up and this

01:08:46,160 --> 01:08:48,920
is what I was talking about it's going

01:08:47,569 --> 01:08:50,929
to do that one thing and it's going to

01:08:48,920 --> 01:08:52,429
transform it's going to filter

01:08:50,929 --> 01:08:54,529
it's going to aggregate it and then all

01:08:52,429 --> 01:08:56,150
of a sudden it projects it out so it's

01:08:54,529 --> 01:08:58,819
doing that kind of one at a time type

01:08:56,150 --> 01:09:01,150
thing but it's called volcano because

01:08:58,819 --> 01:09:03,949
it's kind of spy smiling up and then

01:09:01,150 --> 01:09:05,359
finally spewing out so it's called the

01:09:03,949 --> 01:09:08,380
volcano motto you can actually look it

01:09:05,359 --> 01:09:13,069
up there's whole Papers written on it um

01:09:08,380 --> 01:09:16,370
but the idea with whole stagecoach n is

01:09:13,069 --> 01:09:22,250
now instead of really worrying about all

01:09:16,370 --> 01:09:26,739
that the idea was you have all these

01:09:22,250 --> 01:09:29,359
virtual calls that happened right and so

01:09:26,739 --> 01:09:30,710
it would run through a map virtual call

01:09:29,359 --> 01:09:31,969
this and that and this and that so you

01:09:30,710 --> 01:09:37,159
have the Trent

01:09:31,969 --> 01:09:39,620
so our trainer our transformation we had

01:09:37,159 --> 01:09:41,569
our filter we have the loop that's going

01:09:39,620 --> 01:09:42,759
through there and that was all virtual

01:09:41,569 --> 01:09:45,560
calls to different things

01:09:42,759 --> 01:09:47,690
whereas now whole stage koujun now that

01:09:45,560 --> 01:09:50,179
they can see your code better they

01:09:47,690 --> 01:09:53,239
actually said ok what would if you just

01:09:50,179 --> 01:09:55,100
went to a grad student and said write

01:09:53,239 --> 01:09:57,080
this right here's our business logic

01:09:55,100 --> 01:09:58,730
write something for it they'd write this

01:09:57,080 --> 01:09:59,810
for loop they wouldn't write here's a

01:09:58,730 --> 01:10:01,340
call to this here's a call to this

01:09:59,810 --> 01:10:03,679
here's a call to this they'd write this

01:10:01,340 --> 01:10:05,330
for loop probably and it turns out that

01:10:03,679 --> 01:10:09,140
because you get rid of all these virtual

01:10:05,330 --> 01:10:10,850
calls this is as I pointed out like five

01:10:09,140 --> 01:10:12,920
to ten in fact I think if you really

01:10:10,850 --> 01:10:14,210
want to jump the gun I think there were

01:10:12,920 --> 01:10:17,090
some spots where it was like a hundred

01:10:14,210 --> 01:10:19,370
times faster because you're avoiding all

01:10:17,090 --> 01:10:21,290
the overhead and you're just saying hey

01:10:19,370 --> 01:10:24,020
all of those things can be boiled down

01:10:21,290 --> 01:10:26,570
into this simple logic and so whenever I

01:10:24,020 --> 01:10:28,310
said spark as a compiler basically you

01:10:26,570 --> 01:10:30,620
say what you want it to do and it's

01:10:28,310 --> 01:10:32,870
going to write this instead so it's

01:10:30,620 --> 01:10:35,780
going to basically say hey this is much

01:10:32,870 --> 01:10:37,640
more optimized code I can get rid of all

01:10:35,780 --> 01:10:39,230
of those extra steps and just boil them

01:10:37,640 --> 01:10:43,640
into one thing that's why they're

01:10:39,230 --> 01:10:45,860
calling it whole stagecoach n so again

01:10:43,640 --> 01:10:48,159
talks about like the different aspects

01:10:45,860 --> 01:10:48,159
of it

01:10:49,520 --> 01:10:55,230
yeah so five two actually 100 times

01:10:52,140 --> 01:10:56,520
faster now this is a bit outdated but I

01:10:55,230 --> 01:11:00,510
kept it here just so I can talk about it

01:10:56,520 --> 01:11:02,760
real quick actually I already kind of

01:11:00,510 --> 01:11:05,100
tough touched on it but here's the

01:11:02,760 --> 01:11:07,140
important part is something called

01:11:05,100 --> 01:11:09,720
drizzle which basically gives them the

01:11:07,140 --> 01:11:14,010
continuous processing and it just came

01:11:09,720 --> 01:11:15,870
out last month but in spark - three

01:11:14,010 --> 01:11:19,080
continuous processing is something you

01:11:15,870 --> 01:11:20,610
can turn on it's very experimental I

01:11:19,080 --> 01:11:24,620
wouldn't necessarily do it in production

01:11:20,610 --> 01:11:26,220
but if you need that real time like

01:11:24,620 --> 01:11:28,320
milliseconds not even hundred

01:11:26,220 --> 01:11:30,690
milliseconds of response time within

01:11:28,320 --> 01:11:33,960
your processing then you can turn on

01:11:30,690 --> 01:11:37,820
continuous processing and their whole

01:11:33,960 --> 01:11:39,750
focus is on structured streaming so

01:11:37,820 --> 01:11:40,800
they're still working on machine

01:11:39,750 --> 01:11:42,690
learning they're still working on all

01:11:40,800 --> 01:11:45,360
that stuff but their focus is

01:11:42,690 --> 01:11:47,730
realistically hey Flint came out like I

01:11:45,360 --> 01:11:49,290
said said hey all of this batch stuff

01:11:47,730 --> 01:11:52,080
which it's really just a subset of

01:11:49,290 --> 01:11:55,380
streaming so spark came along and said

01:11:52,080 --> 01:11:57,360
hey we're gonna embrace that and write

01:11:55,380 --> 01:12:00,290
all of your code your data frame code

01:11:57,360 --> 01:12:02,250
that was batchi and that had all the the

01:12:00,290 --> 01:12:04,860
optimizations in it that we had built in

01:12:02,250 --> 01:12:06,480
now we created structured streaming so

01:12:04,860 --> 01:12:09,480
that all you have to do is change into

01:12:06,480 --> 01:12:11,670
start stream and a right read stream

01:12:09,480 --> 01:12:14,520
right stream and start instead of load

01:12:11,670 --> 01:12:17,700
and read and write so their focus is

01:12:14,520 --> 01:12:20,280
optimizing this and being able to work

01:12:17,700 --> 01:12:21,660
with as I mentioned windows of data and

01:12:20,280 --> 01:12:24,780
and some of the things that you have to

01:12:21,660 --> 01:12:26,250
do with structure or with streaming but

01:12:24,780 --> 01:12:27,570
we're gonna try to make it so that you

01:12:26,250 --> 01:12:30,360
don't have to think about it just like

01:12:27,570 --> 01:12:32,220
with rdd's and then two data frames and

01:12:30,360 --> 01:12:35,400
data sets they're trying to make it so

01:12:32,220 --> 01:12:37,200
that you don't have to think about those

01:12:35,400 --> 01:12:38,490
low levels anymore you just have to

01:12:37,200 --> 01:12:42,170
write your business logic and that's

01:12:38,490 --> 01:12:43,950
really what their focus is as well as

01:12:42,170 --> 01:12:47,250
there's gonna be a lot of stuff that

01:12:43,950 --> 01:12:48,630
comes out of Rhys lab so I mentioned

01:12:47,250 --> 01:12:50,900
before that spark came out of something

01:12:48,630 --> 01:12:53,130
called amp lab and so that was like a

01:12:50,900 --> 01:12:56,580
government-funded they got a lot of

01:12:53,130 --> 01:12:59,280
grant funding and out of UC Berkeley and

01:12:56,580 --> 01:13:02,350
so algorithms machine people whereas

01:12:59,280 --> 01:13:04,930
Rhys lab is the amp laughs ended

01:13:02,350 --> 01:13:07,200
hey there grant was done so they redid

01:13:04,930 --> 01:13:13,090
things and I created rise so that is

01:13:07,200 --> 01:13:15,940
real time no assets for security

01:13:13,090 --> 01:13:19,090
I don't remember rise lap as well

01:13:15,940 --> 01:13:20,770
because it's relatively newer but it

01:13:19,090 --> 01:13:25,630
gets to basically you're streaming

01:13:20,770 --> 01:13:27,190
things your security things again I

01:13:25,630 --> 01:13:29,590
forget what the I and II are but I'm

01:13:27,190 --> 01:13:34,390
pretty sure what r is for real time and

01:13:29,590 --> 01:13:35,860
S is for security and so that's that's

01:13:34,390 --> 01:13:37,780
really where a lot of the new things are

01:13:35,860 --> 01:13:39,670
going to be focused on in spark because

01:13:37,780 --> 01:13:45,100
they're going to feed that into spark

01:13:39,670 --> 01:13:46,930
essentially right so I kind of breezed

01:13:45,100 --> 01:13:51,970
through it at the end but again I wanted

01:13:46,930 --> 01:13:54,160
to leave a little bit of time I can take

01:13:51,970 --> 01:13:57,060
some questions first or kind of show you

01:13:54,160 --> 01:13:59,710
the the shell real quick if you want

01:13:57,060 --> 01:14:01,240
preferences anybody want to see the

01:13:59,710 --> 01:14:23,050
spark shell and see it in action a

01:14:01,240 --> 01:14:25,540
little bit so yes so so it is actually

01:14:23,050 --> 01:14:27,850
writing out so your logic and in fact

01:14:25,540 --> 01:14:28,780
the Z stream will yell at you so

01:14:27,850 --> 01:14:30,730
whenever we're talking about these

01:14:28,780 --> 01:14:33,880
streams I think they brought it into

01:14:30,730 --> 01:14:37,030
structured streaming too but if you so

01:14:33,880 --> 01:14:38,530
spark is lazy right so if I wrote code

01:14:37,030 --> 01:14:41,560
and I've seen Stack Overflow questions

01:14:38,530 --> 01:14:44,860
about this where if I write code that

01:14:41,560 --> 01:14:47,650
says map map map and then I tried to run

01:14:44,860 --> 01:14:49,600
that I've seen questions were the like

01:14:47,650 --> 01:14:52,510
where's my output it's it's it's

01:14:49,600 --> 01:14:55,300
actually a similar thing so you have to

01:14:52,510 --> 01:14:58,750
actually have a save option or or

01:14:55,300 --> 01:15:00,820
collect or some kind of action for it to

01:14:58,750 --> 01:15:02,320
actually do something so what it's going

01:15:00,820 --> 01:15:04,360
to do in streaming cases it's going to

01:15:02,320 --> 01:15:06,040
say map map map and again you have to

01:15:04,360 --> 01:15:08,920
think about it and especially the D

01:15:06,040 --> 01:15:10,630
stream and in the you have to think

01:15:08,920 --> 01:15:12,790
about as that batch and basically it's

01:15:10,630 --> 01:15:15,429
going to say run through the data that

01:15:12,790 --> 01:15:17,889
it has and maybe you

01:15:15,429 --> 01:15:19,989
have a print line or something and then

01:15:17,889 --> 01:15:21,550
print that out if it's going to be a

01:15:19,989 --> 01:15:23,469
save it's going to run through the data

01:15:21,550 --> 01:15:28,479
that it has and then it's going to save

01:15:23,469 --> 01:15:30,159
it to a database so the the item the the

01:15:28,479 --> 01:15:31,929
the exactly wants and things like that

01:15:30,159 --> 01:15:33,729
especially if you have windows of data

01:15:31,929 --> 01:15:35,499
that are constantly shifting and but

01:15:33,729 --> 01:15:38,949
they're still using some of the similar

01:15:35,499 --> 01:15:40,420
data that was already there you have to

01:15:38,949 --> 01:15:41,920
make sure that in your saves you still

01:15:40,420 --> 01:15:45,789
have to think about item potency and

01:15:41,920 --> 01:15:47,670
things like that because your every time

01:15:45,789 --> 01:15:51,280
that you call like save to the database

01:15:47,670 --> 01:15:53,709
then it's gonna it will finish so for

01:15:51,280 --> 01:15:55,869
that batch it'll finish it'll save to

01:15:53,709 --> 01:15:58,570
the database and then the next batch

01:15:55,869 --> 01:15:59,949
it's going to say run through save to

01:15:58,570 --> 01:16:02,380
the database and it will potentially

01:15:59,949 --> 01:16:05,860
overwrite o data because now it's old

01:16:02,380 --> 01:16:08,010
data too so if something changed for

01:16:05,860 --> 01:16:11,800
something then we want to write it out

01:16:08,010 --> 01:16:14,439
now if you don't have something if you

01:16:11,800 --> 01:16:16,449
just want a constant append then it can

01:16:14,439 --> 01:16:19,420
constantly be saving saving saving and

01:16:16,449 --> 01:16:22,360
shoving and just the same you could even

01:16:19,420 --> 01:16:24,760
have your stream almost be like this

01:16:22,360 --> 01:16:26,889
circular thing where you say hey I have

01:16:24,760 --> 01:16:29,079
my stream coming in and then I want to

01:16:26,889 --> 01:16:30,939
use stuff from the database so you can

01:16:29,079 --> 01:16:33,489
have logic in there that says read from

01:16:30,939 --> 01:16:35,530
the database and it's gonna read from

01:16:33,489 --> 01:16:37,900
the database and use that data which had

01:16:35,530 --> 01:16:39,219
been updated in the last round so that's

01:16:37,900 --> 01:16:40,920
kind of where that machine learning

01:16:39,219 --> 01:16:43,719
comes into play where it's saying hey

01:16:40,920 --> 01:16:46,360
the last batch saved it and now whenever

01:16:43,719 --> 01:16:49,030
I go to use it it's gonna say hey here's

01:16:46,360 --> 01:16:53,610
the new data that that I need to use to

01:16:49,030 --> 01:16:53,610
process against the the new stream

01:17:00,499 --> 01:17:07,869
with each with each mini badge it's

01:17:03,559 --> 01:17:07,869
saving it it's picking up we're kind of

01:17:12,610 --> 01:17:18,139
so the driver is actually a constant a

01:17:15,199 --> 01:17:21,979
long-lived thing in that case but the

01:17:18,139 --> 01:17:24,829
the tasks it's going to kick off the the

01:17:21,979 --> 01:17:26,719
core logic each time yeah so basically

01:17:24,829 --> 01:17:28,219
it's going to like from the D stream or

01:17:26,719 --> 01:17:30,260
really I mean whether you're talking to

01:17:28,219 --> 01:17:31,789
each string or or even structured

01:17:30,260 --> 01:17:33,409
streaming to some extent I haven't

01:17:31,789 --> 01:17:36,469
gotten into the how they've implemented

01:17:33,409 --> 01:17:37,789
continuous streaming but even the the

01:17:36,469 --> 01:17:41,239
structured streaming was still basically

01:17:37,789 --> 01:17:45,559
mini-batches where basically it's going

01:17:41,239 --> 01:17:48,139
to say hey go in and gather the data and

01:17:45,559 --> 01:17:51,289
then basically it's running a little

01:17:48,139 --> 01:17:53,420
mini program so all of that logic it's

01:17:51,289 --> 01:17:55,880
still running it off onto the spark

01:17:53,420 --> 01:17:57,559
workers the same as if you had run it

01:17:55,880 --> 01:18:02,110
through cron jobs or something that

01:17:57,559 --> 01:18:02,110
would be constantly running make sense

01:18:14,000 --> 01:18:19,750
[Music]

01:18:17,560 --> 01:18:44,290
let me I think for that actually would

01:18:19,750 --> 01:18:46,030
be a good point to if you are using

01:18:44,290 --> 01:18:48,760
something over and over again yeah you

01:18:46,030 --> 01:18:50,830
would want to cache it because then if

01:18:48,760 --> 01:18:53,530
it's if you're if you're not kind of

01:18:50,830 --> 01:18:55,480
constantly optimized retaining things

01:18:53,530 --> 01:19:14,380
then yeah you are going to want to cache

01:18:55,480 --> 01:19:16,869
it because why reprocess it by default

01:19:14,380 --> 01:19:20,290
it's to disk to disk with overflow I

01:19:16,869 --> 01:19:23,530
believe - I'm sorry I'm speaking before

01:19:20,290 --> 01:19:26,170
I think - memory by defaults to memory

01:19:23,530 --> 01:19:27,969
whenever you call cache but it takes

01:19:26,170 --> 01:19:30,639
parameters that you can override but by

01:19:27,969 --> 01:19:33,369
default if you just call cache the

01:19:30,639 --> 01:19:38,560
default parameter is to memory with disk

01:19:33,369 --> 01:19:40,340
overflow and if I can bump this up which

01:19:38,560 --> 01:19:41,990
I thought was just

01:19:40,340 --> 01:19:44,850
[Music]

01:19:41,990 --> 01:19:46,770
I've been spending more of my time in

01:19:44,850 --> 01:19:50,820
the Macworld than the windows world so I

01:19:46,770 --> 01:19:54,000
forget how to bump the size up so dunno

01:19:50,820 --> 01:20:05,840
if I right-click here that's what I did

01:19:54,000 --> 01:20:05,840
and it just froze on me I guess I should

01:20:13,190 --> 01:20:19,370
see that right right all right spark

01:20:25,390 --> 01:20:27,450
you

01:21:50,510 --> 01:21:57,110
actually it definitely isn't and support

01:21:53,970 --> 01:21:59,370
there's a lot of tickets and spark where

01:21:57,110 --> 01:22:00,450
there's problems around it and they

01:21:59,370 --> 01:22:03,300
actually don't care

01:22:00,450 --> 01:22:04,950
almost like there's just like like

01:22:03,300 --> 01:22:07,230
there's one that's been ongoing where

01:22:04,950 --> 01:22:07,950
it's just things just don't work right

01:22:07,230 --> 01:22:09,720
off the bat

01:22:07,950 --> 01:22:12,750
in fact it's Park will not work right

01:22:09,720 --> 01:22:14,520
off the bat because of spark well sparks

01:22:12,750 --> 01:22:17,730
sequel won't work off the bat because of

01:22:14,520 --> 01:22:21,720
how it stores temporary tables for the

01:22:17,730 --> 01:22:23,940
shell and you have to actually go out

01:22:21,720 --> 01:22:26,310
get this thing when you tails what that

01:22:23,940 --> 01:22:27,390
Hadoop uses and whatnot and if people

01:22:26,310 --> 01:22:28,760
don't know that then they get really

01:22:27,390 --> 01:22:34,710
confused

01:22:28,760 --> 01:22:36,290
alright so I believe that should do what

01:22:34,710 --> 01:22:42,660
I want

01:22:36,290 --> 01:22:53,010
map actually I can do exactly what I

01:22:42,660 --> 01:22:55,560
said x2 thread asleep would do it by a

01:22:53,010 --> 01:22:57,540
second actually that's a lot of stuff

01:22:55,560 --> 01:23:01,350
hold on a second that's not going to

01:22:57,540 --> 01:23:06,960
work at all I see to make RDD you say

01:23:01,350 --> 01:23:09,760
one to ten and then we'll say it's to

01:23:06,960 --> 01:23:13,830
dot map

01:23:09,760 --> 01:23:19,210
[Music]

01:23:13,830 --> 01:23:28,119
thread sleep all asleep for a second and

01:23:19,210 --> 01:23:32,320
then I'll do X plus 1 what it's a show

01:23:28,119 --> 01:23:36,070
it it automatically does if you look one

01:23:32,320 --> 01:23:37,630
line above justice that should be good

01:23:36,070 --> 01:23:40,750
so you'll see that this automatically

01:23:37,630 --> 01:23:45,460
returns it did nothing now if I do rez

01:23:40,750 --> 01:23:47,619
to dot cache oops that's not what I want

01:23:45,460 --> 01:23:51,130
that's I went to rise 3 dot cache

01:23:47,619 --> 01:23:55,480
because that's the neat little put so

01:23:51,130 --> 01:23:59,650
now if I do res 3 dot collect should

01:23:55,480 --> 01:24:01,540
take about 10 seconds ok so just it

01:23:59,650 --> 01:24:07,900
actually distributed it so it's a less

01:24:01,540 --> 01:24:11,199
time than that now if I do res 3 collect

01:24:07,900 --> 01:24:18,520
again immediate and I can even show that

01:24:11,199 --> 01:24:23,650
and I'll show the UI real quick is how I

01:24:18,520 --> 01:24:26,710
said that that comes built-in ok so here

01:24:23,650 --> 01:24:29,170
we can see that it took two seconds for

01:24:26,710 --> 01:24:32,199
the first one thirty second 36 seconds

01:24:29,170 --> 01:24:35,440
36 milliseconds for the second call and

01:24:32,199 --> 01:24:36,849
we conceived in that storage in fact it

01:24:35,440 --> 01:24:38,860
cached it twice because I accidentally

01:24:36,849 --> 01:24:42,040
called cache on the wrong thing but you

01:24:38,860 --> 01:24:45,099
can see that it's these things and I

01:24:42,040 --> 01:24:48,369
have 8 cores so cache partitions by

01:24:45,099 --> 01:24:52,630
default the SPARC shell runs across your

01:24:48,369 --> 01:24:55,989
maximum so it's floated across 8 it's

01:24:52,630 --> 01:24:57,670
fully cast because I mean it gets to if

01:24:55,989 --> 01:25:00,639
it can't catch at all then it'll just

01:24:57,670 --> 01:25:03,849
spill over so here it's saying it was

01:25:00,639 --> 01:25:05,469
able to fit it all in and then

01:25:03,849 --> 01:25:07,900
realistically we can come back to the

01:25:05,469 --> 01:25:10,270
jobs too and we can dig into it and we

01:25:07,900 --> 01:25:11,949
can say ok here's the graph that I was

01:25:10,270 --> 01:25:14,619
talking of the dag visualization where

01:25:11,949 --> 01:25:17,170
it's saying okay this is the hole this

01:25:14,619 --> 01:25:19,119
gets a little bit to the stages now

01:25:17,170 --> 01:25:19,989
we're using rdd's so you don't get whole

01:25:19,119 --> 01:25:22,150
stage cogent

01:25:19,989 --> 01:25:25,960
but in data frames stages come and use

01:25:22,150 --> 01:25:29,020
in handy so here we're saying make Rd

01:25:25,960 --> 01:25:30,640
and map and that's the stage and I think

01:25:29,020 --> 01:25:32,290
it still remains the same even if it's

01:25:30,640 --> 01:25:34,989
cashed yeah so it's still going to be

01:25:32,290 --> 01:25:36,370
look the same but it's ultimately going

01:25:34,989 --> 01:25:38,230
to say hey the data that you're

01:25:36,370 --> 01:25:42,370
collecting was already cast pull it out

01:25:38,230 --> 01:25:44,350
of the cache and whatever and if I

01:25:42,370 --> 01:25:49,270
recall it's the least frequently used

01:25:44,350 --> 01:25:53,110
the cache so that that's the efficient

01:25:49,270 --> 01:25:56,950
pattern so it will evict it out of cache

01:25:53,110 --> 01:25:58,420
if it runs out of memory and it will if

01:25:56,950 --> 01:26:08,530
you've used it more recently then you'll

01:25:58,420 --> 01:26:11,650
get that any other questions ok so like

01:26:08,530 --> 01:26:33,670
I said I wanted to just do it as a high

01:26:11,650 --> 01:26:38,560
level here so if you're talking

01:26:33,670 --> 01:26:42,670
exceptions then yeah it writes

01:26:38,560 --> 01:26:46,630
everything out - in fact like you can go

01:26:42,670 --> 01:26:48,430
to executor z-- and I can say even

01:26:46,630 --> 01:26:52,440
thread dump so you could do kind of a

01:26:48,430 --> 01:26:52,440
live thing and so see the thread dump

01:26:53,640 --> 01:26:58,450
but oh it's because I don't actually

01:26:56,230 --> 01:27:00,190
have if I had it set up to a cluster

01:26:58,450 --> 01:27:02,590
you'd be able to see like the standard

01:27:00,190 --> 01:27:05,380
error standard output and it would give

01:27:02,590 --> 01:27:07,480
you actual the errors or anything like

01:27:05,380 --> 01:27:10,480
that so an exception should bubble up in

01:27:07,480 --> 01:27:13,090
those that those exceptions there but I

01:27:10,480 --> 01:27:14,410
would say from looking at it from a data

01:27:13,090 --> 01:27:16,390
perspective that's a different thing

01:27:14,410 --> 01:27:18,070
than an exception if your data is not

01:27:16,390 --> 01:27:22,180
looking like you expect that's where

01:27:18,070 --> 01:27:27,120
okay in my opinion the SPARC shell comes

01:27:22,180 --> 01:27:27,120
in handy because you can actually say

01:27:27,820 --> 01:27:38,500
[Music]

01:27:29,940 --> 01:27:41,920
so you can pass in all these parameters

01:27:38,500 --> 01:27:44,740
and one of them is your master so the

01:27:41,920 --> 01:27:47,050
master URL by default is local user star

01:27:44,740 --> 01:27:50,320
which is use all your course but you can

01:27:47,050 --> 01:27:53,500
actually have it point to spark

01:27:50,320 --> 01:27:55,180
standalone maysa yarn and like I said

01:27:53,500 --> 01:27:57,610
they added kubernetes recently so you

01:27:55,180 --> 01:27:59,230
can absolutely say hey go point to my

01:27:57,610 --> 01:28:01,990
cluster my actual cluster and you could

01:27:59,230 --> 01:28:04,750
actually start checking out data and and

01:28:01,990 --> 01:28:12,280
you could run so you could sit there and

01:28:04,750 --> 01:28:13,450
say so you could sit there and say now

01:28:12,280 --> 01:28:15,910
now let's say that I'm actually

01:28:13,450 --> 01:28:17,590
connected to my cluster I could say like

01:28:15,910 --> 01:28:30,010
I have tables or something like that I

01:28:17,590 --> 01:28:36,520
can say spark dot create data I know I

01:28:30,010 --> 01:28:41,020
want to do load it's not great think

01:28:36,520 --> 01:28:42,710
it's load read

01:28:41,020 --> 01:28:46,310
[Music]

01:28:42,710 --> 01:28:49,590
and then from there I can read like

01:28:46,310 --> 01:28:53,010
maybe it's in CSV format I know I can

01:28:49,590 --> 01:28:55,400
say CSV point to the file path or

01:28:53,010 --> 01:28:58,320
whatever and then you can actually start

01:28:55,400 --> 01:29:00,420
munging your data or or you can you can

01:28:58,320 --> 01:29:03,330
actually even take it to another level

01:29:00,420 --> 01:29:05,430
you could say SPARC shell and pass in

01:29:03,330 --> 01:29:07,890
your jar that's your application logic

01:29:05,430 --> 01:29:11,100
and it'll you can and then what like

01:29:07,890 --> 01:29:13,650
actually invoke your classes and things

01:29:11,100 --> 01:29:15,870
like that so again that's where the

01:29:13,650 --> 01:29:25,170
interactive shell really can help you

01:29:15,870 --> 01:29:26,940
out yeah yeah but I mean the nice thing

01:29:25,170 --> 01:29:28,680
is I could print it out here I could I

01:29:26,940 --> 01:29:32,040
don't have to write it like I don't have

01:29:28,680 --> 01:29:34,080
to deploy or whatever I could now again

01:29:32,040 --> 01:29:36,989
different companies have different rules

01:29:34,080 --> 01:29:38,550
on where you can hit things but if

01:29:36,989 --> 01:29:42,030
you're if you're allowed to you could

01:29:38,550 --> 01:29:44,220
just access maybe a test cluster or

01:29:42,030 --> 01:29:53,340
something I don't know what yeah and

01:29:44,220 --> 01:29:57,090
then you can you can do so make Rd key

01:29:53,340 --> 01:30:00,900
one to ten and here I could do something

01:29:57,090 --> 01:30:09,360
like whereas zero dot instead of collect

01:30:00,900 --> 01:30:10,980
I could do for each I can do but

01:30:09,360 --> 01:30:13,060
actually cuz that's just that I could

01:30:10,980 --> 01:30:18,500
just say print

01:30:13,060 --> 01:30:19,940
and so there I can print it out and you

01:30:18,500 --> 01:30:27,730
can see it's not in over here because

01:30:19,940 --> 01:30:30,110
it's distributed any other questions so

01:30:27,730 --> 01:30:32,360
again this is what's supposed to be

01:30:30,110 --> 01:30:33,440
really really high-level I apologize for

01:30:32,360 --> 01:30:37,670
near the end some of the stuff getting

01:30:33,440 --> 01:30:39,159
mixed up in myself but if if you if you

01:30:37,670 --> 01:30:45,739
want to really get into it

01:30:39,159 --> 01:30:47,630
one I'll be having the session in about

01:30:45,739 --> 01:30:52,520
at the end of the month on Mondays so

01:30:47,630 --> 01:30:56,179
April 30th and May 7th I believe as well

01:30:52,520 --> 01:30:58,670
as you can see me for to give me your

01:30:56,179 --> 01:31:00,170
information or even ping me Twitter or

01:30:58,670 --> 01:31:02,330
whatever I mean I'm pretty easy to find

01:31:00,170 --> 01:31:04,070
because my name is Justin Pony it's I'm

01:31:02,330 --> 01:31:06,380
the only just in Pahoa out there you

01:31:04,070 --> 01:31:09,020
google me you get my Twitter my email

01:31:06,380 --> 01:31:12,679
Justin pehoe me at gmail I mean I'm not

01:31:09,020 --> 01:31:16,330
hiding ping me and you can ask for 30

01:31:12,679 --> 01:31:20,560
days chloral site trials and whatnot

01:31:16,330 --> 01:31:20,560
all right thanks

01:31:26,730 --> 01:31:28,790

YouTube URL: https://www.youtube.com/watch?v=WC5mKGxPPr8


