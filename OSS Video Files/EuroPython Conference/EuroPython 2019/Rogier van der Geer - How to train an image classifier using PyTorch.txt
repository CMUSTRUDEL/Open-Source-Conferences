Title: Rogier van der Geer - How to train an image classifier using PyTorch
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"How to train an image classifier using PyTorch
[EuroPython 2019 - Talk - 2019-07-10 - Singapore [PyData track]
[Basel, CH]

By Rogier van der Geer

Neural networks are everywhere nowadays. But while it seems everyone is using them, training your first neural network can be quite a hurdle to overcome.

In this talk I will take you by the hand, and following an example image classifier I trained, I will take you through the steps of making an image classifier in PyTorch. I will show you code snippets and explain the more intricate parts. Also, I will tell you about my experience, and about what mistakes to prevent. After this all you need to start training your first classifier is a data set!

Of course I will provide a link to the full codebase at the end. The talk will focus on the practical aspect of training a neural network, and will only touch the theoretical side very briefly. Some basic prior knowledge of neural networks is beneficial, but not required, to follow this talk.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:02,920 --> 00:00:10,750
thank you so we're definitely not going

00:00:07,779 --> 00:00:13,030
to do any particle physics today but

00:00:10,750 --> 00:00:16,090
today I'm going to give you help you do

00:00:13,030 --> 00:00:19,890
take a first few steps on training your

00:00:16,090 --> 00:00:23,619
first image classifier using PI torch

00:00:19,890 --> 00:00:25,570
just before we start can I get a a hand

00:00:23,619 --> 00:00:29,169
for everyone who has used by torch

00:00:25,570 --> 00:00:31,150
before only a few that's good because

00:00:29,169 --> 00:00:35,260
we're really going to start at the

00:00:31,150 --> 00:00:38,379
beginning and so for basically first I'm

00:00:35,260 --> 00:00:40,239
going to tell you first start at what is

00:00:38,379 --> 00:00:43,329
actually an image classifier that's not

00:00:40,239 --> 00:00:45,250
so difficult then we'll go by what's the

00:00:43,329 --> 00:00:48,399
neural network how do we actually build

00:00:45,250 --> 00:00:50,109
one in pi torch and then finally what

00:00:48,399 --> 00:00:53,969
can we do with them and in the spirit of

00:00:50,109 --> 00:00:56,829
this morning's this morning's keynote

00:00:53,969 --> 00:00:59,230
I'm going to show you how I played

00:00:56,829 --> 00:01:01,749
around with an image classifier and what

00:00:59,230 --> 00:01:04,720
cool things you can do with them

00:01:01,749 --> 00:01:06,700
ok so first let's have a look at what is

00:01:04,720 --> 00:01:09,189
a classifier so suppose we have a

00:01:06,700 --> 00:01:12,220
labeled training sets of points so we

00:01:09,189 --> 00:01:14,650
have two dimensions X 1 and X 2 and then

00:01:12,220 --> 00:01:17,380
we have a set of data points and they

00:01:14,650 --> 00:01:20,229
come in two classes either red or green

00:01:17,380 --> 00:01:21,880
Wow okay that's cool well in this case

00:01:20,229 --> 00:01:23,320
we can see a pattern so did the green

00:01:21,880 --> 00:01:24,850
ones are on the left side of the screen

00:01:23,320 --> 00:01:26,920
and the red ones are on the right side

00:01:24,850 --> 00:01:29,829
of the screen so we could say for

00:01:26,920 --> 00:01:31,960
example well let's draw a line somewhere

00:01:29,829 --> 00:01:34,360
around here and then say everything

00:01:31,960 --> 00:01:36,610
that's left of this line is green and

00:01:34,360 --> 00:01:38,680
everything that's right of this line at

00:01:36,610 --> 00:01:41,290
thread so now we have a model a very

00:01:38,680 --> 00:01:43,420
simple model it's just a line that can

00:01:41,290 --> 00:01:47,140
tell us whether a point belongs to one

00:01:43,420 --> 00:01:49,170
class or the other one so then when we

00:01:47,140 --> 00:01:51,700
have an unlabeled data set we have

00:01:49,170 --> 00:01:55,060
points like these in this case just

00:01:51,700 --> 00:01:57,820
white points then well what we can do is

00:01:55,060 --> 00:02:01,180
we can use this same line that we had

00:01:57,820 --> 00:02:03,670
before and use that to color pick all

00:02:01,180 --> 00:02:06,159
the points that we add into green and

00:02:03,670 --> 00:02:08,200
red well so far

00:02:06,159 --> 00:02:09,940
that's a classifier and I'm pretty sure

00:02:08,200 --> 00:02:12,819
and the most of you have seen something

00:02:09,940 --> 00:02:14,379
like this before now sometimes you want

00:02:12,819 --> 00:02:15,970
to do something that's a little bit more

00:02:14,379 --> 00:02:19,360
complicated

00:02:15,970 --> 00:02:21,010
using just a simple line and in this

00:02:19,360 --> 00:02:23,920
case we're going to use neural networks

00:02:21,010 --> 00:02:26,350
and neural networks probably most of you

00:02:23,920 --> 00:02:28,600
have heard about about them look

00:02:26,350 --> 00:02:31,480
something like this here you see a

00:02:28,600 --> 00:02:33,670
series of dots circles and they

00:02:31,480 --> 00:02:35,740
represent neurons and they come in

00:02:33,670 --> 00:02:37,690
layers so we have the input layer on the

00:02:35,740 --> 00:02:39,460
left a hidden layer there might be

00:02:37,690 --> 00:02:41,440
multiple in the middle and an output

00:02:39,460 --> 00:02:43,570
layer on the right and they are

00:02:41,440 --> 00:02:46,900
connected by these lines so how do these

00:02:43,570 --> 00:02:48,700
neurons actually work well neuron looks

00:02:46,900 --> 00:02:52,990
something like this you have a set of

00:02:48,700 --> 00:02:55,240
incoming signals on the left and all of

00:02:52,990 --> 00:02:57,430
these signals are then multiplied by a

00:02:55,240 --> 00:03:00,100
weight and these weights are the things

00:02:57,430 --> 00:03:01,720
that we use to train the neural network

00:03:00,100 --> 00:03:04,570
they are the things that we must learn

00:03:01,720 --> 00:03:07,180
when we actually build one neural

00:03:04,570 --> 00:03:09,340
network so what we do is we take those

00:03:07,180 --> 00:03:11,800
inputs we multiply them by the weights

00:03:09,340 --> 00:03:14,650
then sum them up and then in the end

00:03:11,800 --> 00:03:17,250
apply some kind of activation function

00:03:14,650 --> 00:03:21,340
of which the main property is that it's

00:03:17,250 --> 00:03:23,980
nonlinear which makes the neural network

00:03:21,340 --> 00:03:25,769
more capable of learning stuff typically

00:03:23,980 --> 00:03:29,470
what we use and we'll see that more

00:03:25,769 --> 00:03:33,130
today is we use the rectified linear

00:03:29,470 --> 00:03:36,340
unit shorthand is just value function

00:03:33,130 --> 00:03:38,920
which is y equals the maximum of zero

00:03:36,340 --> 00:03:42,280
and X so basically it's zero if X is

00:03:38,920 --> 00:03:44,140
negative and otherwise it's X ok well

00:03:42,280 --> 00:03:46,390
that's fairly soon so how do we actually

00:03:44,140 --> 00:03:49,269
use a neural network like this what do

00:03:46,390 --> 00:03:53,170
we do with it well in the case but we

00:03:49,269 --> 00:03:56,709
had before so we had the two deep these

00:03:53,170 --> 00:03:59,049
two deep points we had two dimensions x1

00:03:56,709 --> 00:04:01,110
and x2 and then we had these points that

00:03:59,049 --> 00:04:03,310
we wanted to classify in red or green

00:04:01,110 --> 00:04:05,290
well we could use the neural network

00:04:03,310 --> 00:04:08,320
like this we can say well we have two

00:04:05,290 --> 00:04:11,500
input nodes and one of them is x1 and

00:04:08,320 --> 00:04:13,600
the other one is x2 and then we use we

00:04:11,500 --> 00:04:15,060
propagate these signals through the

00:04:13,600 --> 00:04:18,880
network with these weights and these

00:04:15,060 --> 00:04:21,280
functions and then we think we can say

00:04:18,880 --> 00:04:23,140
well and we have two output nodes and

00:04:21,280 --> 00:04:26,229
then one of the output nodes is the

00:04:23,140 --> 00:04:28,790
probability that the point belongs to

00:04:26,229 --> 00:04:31,850
the green class and the other one the

00:04:28,790 --> 00:04:33,410
belongs to red class so now of course if

00:04:31,850 --> 00:04:35,690
you first make a neural network like

00:04:33,410 --> 00:04:37,460
this it's not going to do what you want

00:04:35,690 --> 00:04:39,950
because you have to set these weights to

00:04:37,460 --> 00:04:42,260
the correct values so what do you do is

00:04:39,950 --> 00:04:44,390
you take all those points that you have

00:04:42,260 --> 00:04:46,700
labeled before you pass them through

00:04:44,390 --> 00:04:49,820
network see whether it makes some error

00:04:46,700 --> 00:04:52,970
and then tune the weights in such a way

00:04:49,820 --> 00:04:54,770
that it will actually perform better so

00:04:52,970 --> 00:04:57,980
you have to do a lot of tuning before

00:04:54,770 --> 00:05:02,660
you get a neural network that actually

00:04:57,980 --> 00:05:05,090
does what you want but why we're talking

00:05:02,660 --> 00:05:08,330
here about image classification and not

00:05:05,090 --> 00:05:10,400
about 2d points so what we want to do is

00:05:08,330 --> 00:05:12,500
instead of just having two inputs we

00:05:10,400 --> 00:05:14,690
want to say when we have an image well

00:05:12,500 --> 00:05:15,950
in this case the left one here is a dog

00:05:14,690 --> 00:05:17,870
and the right one is a cat that's

00:05:15,950 --> 00:05:20,540
something that we can easily see but we

00:05:17,870 --> 00:05:23,750
would want a neural network to recognize

00:05:20,540 --> 00:05:25,460
something like that for us and for that

00:05:23,750 --> 00:05:28,250
you cannot use a simple neural network

00:05:25,460 --> 00:05:30,380
like the one I showed before but we're

00:05:28,250 --> 00:05:32,150
going to need something a little bit

00:05:30,380 --> 00:05:33,980
more complicated we're going to need

00:05:32,150 --> 00:05:37,340
something that is called

00:05:33,980 --> 00:05:39,650
a deep convolutional Network well that's

00:05:37,340 --> 00:05:42,290
quite a mouthful but in the end it's not

00:05:39,650 --> 00:05:44,900
so extremely complicated let's start

00:05:42,290 --> 00:05:46,580
with a deep part a deep neural network

00:05:44,900 --> 00:05:49,460
is just a neural network that has more

00:05:46,580 --> 00:05:51,650
than one hidden layer preferably in this

00:05:49,460 --> 00:05:55,730
case I think ten or so and could be

00:05:51,650 --> 00:05:58,130
twenty or thirty but just more than a

00:05:55,730 --> 00:06:00,530
few that's all that is there two deep

00:05:58,130 --> 00:06:02,440
neural networks and then the

00:06:00,530 --> 00:06:04,820
convolutional part what we need

00:06:02,440 --> 00:06:07,520
convolutions in order to interpret those

00:06:04,820 --> 00:06:09,770
images so on the left side here we see

00:06:07,520 --> 00:06:13,550
that are the input to a neural network

00:06:09,770 --> 00:06:17,450
like this is an image and what we do

00:06:13,550 --> 00:06:20,780
with these convolutions is we take a box

00:06:17,450 --> 00:06:24,560
and we say we aggregate all those pixels

00:06:20,780 --> 00:06:26,810
from that area of the image and then

00:06:24,560 --> 00:06:29,150
apply some function over it we could say

00:06:26,810 --> 00:06:30,860
we could try to see whether there's a

00:06:29,150 --> 00:06:33,230
big difference from the light right to

00:06:30,860 --> 00:06:34,420
the left or at the top and the bottom or

00:06:33,230 --> 00:06:36,980
the whether all the pixels have

00:06:34,420 --> 00:06:39,950
approximately the same value it's just

00:06:36,980 --> 00:06:41,830
some function that we use apply using

00:06:39,950 --> 00:06:45,340
those weights of a neuron

00:06:41,830 --> 00:06:48,159
and then what we do is we shift this box

00:06:45,340 --> 00:06:51,419
around across this image such that we

00:06:48,159 --> 00:06:54,460
get a matrix of the results of all this

00:06:51,419 --> 00:06:57,370
dysfunction across all the image and in

00:06:54,460 --> 00:07:00,460
this case you see we don't you just use

00:06:57,370 --> 00:07:01,960
one convolution we get in the end for

00:07:00,460 --> 00:07:03,849
future maps so we could use for

00:07:01,960 --> 00:07:07,840
convolutions of course you could use

00:07:03,849 --> 00:07:09,909
many more then typically what you do

00:07:07,840 --> 00:07:13,360
after using a convolution is you

00:07:09,909 --> 00:07:15,819
subsample so you save for all each out

00:07:13,360 --> 00:07:17,440
of two by two results we take the

00:07:15,819 --> 00:07:19,090
maximum result which is called max

00:07:17,440 --> 00:07:21,099
pooling or we could take the average

00:07:19,090 --> 00:07:24,069
result or something like that that's

00:07:21,099 --> 00:07:25,479
just we do that in order to reduce the

00:07:24,069 --> 00:07:27,400
size of our neural network because

00:07:25,479 --> 00:07:29,050
otherwise it might become too big and if

00:07:27,400 --> 00:07:31,389
we have too many weights to two it

00:07:29,050 --> 00:07:33,430
becomes too difficult to Train typically

00:07:31,389 --> 00:07:35,020
after that we do more convolutions so

00:07:33,430 --> 00:07:37,120
you have more future maps in this case

00:07:35,020 --> 00:07:39,550
we do ten more convolutions then more

00:07:37,120 --> 00:07:41,650
subsampling and in the end we make a

00:07:39,550 --> 00:07:44,500
fully connected layer and a fully

00:07:41,650 --> 00:07:46,360
connected layer is just one just like

00:07:44,500 --> 00:07:48,159
the one I showed you in the simple

00:07:46,360 --> 00:07:50,650
neural network before and then in the

00:07:48,159 --> 00:07:52,539
end we have an output in this case two

00:07:50,650 --> 00:07:54,190
outputs so we could one of them could

00:07:52,539 --> 00:07:56,620
represent the probability of the image

00:07:54,190 --> 00:08:02,919
being a robot and the other one of being

00:07:56,620 --> 00:08:04,900
a cat something like that okay a typical

00:08:02,919 --> 00:08:09,639
example of a convolutional neural

00:08:04,900 --> 00:08:12,250
network is vgg 16 fuji 16 is a big

00:08:09,639 --> 00:08:16,870
neural network and it consists of 16

00:08:12,250 --> 00:08:20,349
layers and has in total 144 million

00:08:16,870 --> 00:08:23,620
weights and what it does um I hope you

00:08:20,349 --> 00:08:25,810
recognize some of the the ingredients

00:08:23,620 --> 00:08:30,069
here and we start with an image and this

00:08:25,810 --> 00:08:32,020
image is 2 24 by 2 24 pixels and then 3

00:08:30,069 --> 00:08:34,930
layers one for each color red green and

00:08:32,020 --> 00:08:38,709
blue and then so what we do is we start

00:08:34,930 --> 00:08:41,469
at the left with two convolutions then

00:08:38,709 --> 00:08:44,019
some max pooling to subsample to more

00:08:41,469 --> 00:08:45,730
convolutions more subsampling and then

00:08:44,019 --> 00:08:49,149
more convolution subsampling more

00:08:45,730 --> 00:08:52,870
convolutions and then in the end we end

00:08:49,149 --> 00:08:54,260
up with the blue areas these are the

00:08:52,870 --> 00:08:56,240
fully connected layers

00:08:54,260 --> 00:08:58,520
and that we have in the end so as

00:08:56,240 --> 00:09:01,310
standard neural network at the end but

00:08:58,520 --> 00:09:03,530
then big with like 4000 nodes and in the

00:09:01,310 --> 00:09:07,850
end we have one layer with a thousand

00:09:03,530 --> 00:09:12,230
output nodes so why a thousand well in

00:09:07,850 --> 00:09:16,310
this case this is because this network

00:09:12,230 --> 00:09:18,140
was made to be trained on image net an

00:09:16,310 --> 00:09:20,210
image net is a collection of 14 million

00:09:18,140 --> 00:09:22,070
images that was annotated into a

00:09:20,210 --> 00:09:26,900
thousand classes of which for example

00:09:22,070 --> 00:09:31,190
cat and dog and so this this network was

00:09:26,900 --> 00:09:36,020
trained by with using a lot of computers

00:09:31,190 --> 00:09:41,420
to get like 90% accuracy on these 1000

00:09:36,020 --> 00:09:44,410
classes so mom cool this thing already

00:09:41,420 --> 00:09:47,060
exists but are we going to train our own

00:09:44,410 --> 00:09:50,150
image classifier well of course we are

00:09:47,060 --> 00:09:52,070
going to do that and to do that we use

00:09:50,150 --> 00:09:54,080
transfer learning so remember we had

00:09:52,070 --> 00:09:54,500
this network and this has already been

00:09:54,080 --> 00:09:58,430
trained

00:09:54,500 --> 00:09:59,990
it has 144 million weights that have all

00:09:58,430 --> 00:10:02,570
reasonable values such that it can

00:09:59,990 --> 00:10:04,550
accurately classify all those kinds of

00:10:02,570 --> 00:10:08,290
images now we're going to make use of

00:10:04,550 --> 00:10:11,420
that by taking off this last part of

00:10:08,290 --> 00:10:14,060
1000 classes just be moving that whole

00:10:11,420 --> 00:10:17,390
last layer and putting our own layer at

00:10:14,060 --> 00:10:19,910
the end not necessarily a simple layer

00:10:17,390 --> 00:10:22,120
like that but just a new classifier that

00:10:19,910 --> 00:10:25,460
we put on top so we remove the end and

00:10:22,120 --> 00:10:27,760
we add our own layer so this has the

00:10:25,460 --> 00:10:30,230
advantage that all the weights in the

00:10:27,760 --> 00:10:32,570
previous layers they already have some

00:10:30,230 --> 00:10:35,540
reasonable values this network already

00:10:32,570 --> 00:10:38,930
knows how to recognize sharp edges round

00:10:35,540 --> 00:10:40,370
edges strange patterns all those kind of

00:10:38,930 --> 00:10:42,530
things all the things that you typically

00:10:40,370 --> 00:10:44,780
see in a photo it already knows how to

00:10:42,530 --> 00:10:46,790
deal with those and then in the end it

00:10:44,780 --> 00:10:50,450
comes up with a set of features and we

00:10:46,790 --> 00:10:51,080
build a classifier on top of that isn't

00:10:50,450 --> 00:10:53,870
that cheating

00:10:51,080 --> 00:10:55,580
of course it's cheating but hey you

00:10:53,870 --> 00:10:57,950
never get anywhere in life without a

00:10:55,580 --> 00:10:59,870
little bit of cheating so what we want

00:10:57,950 --> 00:11:00,890
to do is we're going to make use of what

00:10:59,870 --> 00:11:01,839
people have done before

00:11:00,890 --> 00:11:03,970
and

00:11:01,839 --> 00:11:05,889
build our own classifier that doesn't

00:11:03,970 --> 00:11:08,649
classify stuff that they have trained it

00:11:05,889 --> 00:11:11,079
on because if you want to classify

00:11:08,649 --> 00:11:13,089
images into exactly those thousand

00:11:11,079 --> 00:11:15,399
classes of imagenet well of course you

00:11:13,089 --> 00:11:18,639
can use the pre trained version of vgg

00:11:15,399 --> 00:11:20,949
um if you don't well this is the way to

00:11:18,639 --> 00:11:25,810
go you just remove the last layer and

00:11:20,949 --> 00:11:28,240
well then you're all set so now you know

00:11:25,810 --> 00:11:31,120
exactly how to train your own image

00:11:28,240 --> 00:11:33,459
classifier right well let's have a look

00:11:31,120 --> 00:11:35,649
at the code but before that you might

00:11:33,459 --> 00:11:37,990
ask me the question why did he choose PI

00:11:35,649 --> 00:11:40,000
torch and not Kira's because you may

00:11:37,990 --> 00:11:41,920
have heard of Kira's as well what Kira's

00:11:40,000 --> 00:11:45,129
is also a library that allows you to do

00:11:41,920 --> 00:11:47,560
neural networks but you could say gars

00:11:45,129 --> 00:11:49,329
is their first PI torch is more flexible

00:11:47,560 --> 00:11:52,899
you could say Cara's it faster which

00:11:49,329 --> 00:11:54,790
month might sound very important but the

00:11:52,899 --> 00:11:56,949
main thing that I think is important is

00:11:54,790 --> 00:11:59,620
that PI torch lets you play with the

00:11:56,949 --> 00:12:01,420
internals basically that means that if

00:11:59,620 --> 00:12:04,000
you get to tweak the neural networks and

00:12:01,420 --> 00:12:06,100
not just import them and use them that

00:12:04,000 --> 00:12:08,529
you learn more from Python so that was

00:12:06,100 --> 00:12:14,529
the main reason that I chose for using

00:12:08,529 --> 00:12:17,470
using PI torch okay now let's have a

00:12:14,529 --> 00:12:20,920
look at it and here gets a little bit

00:12:17,470 --> 00:12:24,250
technical so bear with me first

00:12:20,920 --> 00:12:25,779
if we want to use Python we want to use

00:12:24,250 --> 00:12:28,949
a neural network we have to define a

00:12:25,779 --> 00:12:31,509
neural network and you do that by

00:12:28,949 --> 00:12:34,110
creating a class in this case we could

00:12:31,509 --> 00:12:37,029
create a class that's called net and

00:12:34,110 --> 00:12:40,720
this inherits from the neural network

00:12:37,029 --> 00:12:42,370
top module from torch and when we

00:12:40,720 --> 00:12:45,579
initialize this class we first

00:12:42,370 --> 00:12:47,740
initialize the superclass the module not

00:12:45,579 --> 00:12:50,350
so interesting and then we define the

00:12:47,740 --> 00:12:52,629
four layers of our class so first

00:12:50,350 --> 00:12:54,730
there's the convolutional layer which is

00:12:52,629 --> 00:12:57,430
a 2d convolutional layer with some

00:12:54,730 --> 00:13:00,579
parameters we'll go over those in a

00:12:57,430 --> 00:13:02,709
minute then a pool layer which is to the

00:13:00,579 --> 00:13:04,809
max pooling so this is a stop sampling

00:13:02,709 --> 00:13:07,660
where we take from it in this case a

00:13:04,809 --> 00:13:09,970
kernel size two so a two by two matrix

00:13:07,660 --> 00:13:11,980
we take the maximum value in order to

00:13:09,970 --> 00:13:14,439
reduce the size of our neural network a

00:13:11,980 --> 00:13:15,430
bit and then we have two fully connected

00:13:14,439 --> 00:13:22,120
layers like the

00:13:15,430 --> 00:13:25,990
layers that come after each other at the

00:13:22,120 --> 00:13:28,390
end and then the second fully connected

00:13:25,990 --> 00:13:32,050
layer ends with ten nodes so we have ten

00:13:28,390 --> 00:13:35,320
output node secondly we have to define

00:13:32,050 --> 00:13:37,660
the forward method and forward method it

00:13:35,320 --> 00:13:41,260
accepts a single argument that is called

00:13:37,660 --> 00:13:43,450
X that is the input and the input comes

00:13:41,260 --> 00:13:46,990
in batches and in this key case these

00:13:43,450 --> 00:13:50,050
are 32 by 32 pixel images in three

00:13:46,990 --> 00:13:52,810
channels now if we then apply this

00:13:50,050 --> 00:13:54,070
convolutional layer that's the first the

00:13:52,810 --> 00:13:57,310
second thing we do we apply the

00:13:54,070 --> 00:14:00,730
convolutional layer to X and that

00:13:57,310 --> 00:14:02,940
converts it to 18 channels so we get 18

00:14:00,730 --> 00:14:07,120
different types of convolutions and

00:14:02,940 --> 00:14:10,540
again 32 by 32 pixels then we applied

00:14:07,120 --> 00:14:12,640
apply this radio value function just to

00:14:10,540 --> 00:14:14,230
make it nonlinear that helps our neural

00:14:12,640 --> 00:14:17,170
network with learning more complicated

00:14:14,230 --> 00:14:20,080
stuff okay then we apply the pooling

00:14:17,170 --> 00:14:24,640
which reduces the picture size from 32

00:14:20,080 --> 00:14:26,830
by 32 to 16 by 16 pixels and then since

00:14:24,640 --> 00:14:28,980
we are done with the 2d stuff we have to

00:14:26,830 --> 00:14:32,380
reshape the whole vector to a single

00:14:28,980 --> 00:14:35,050
very long vector of size more than

00:14:32,380 --> 00:14:37,420
4-thousand then we are ready to apply

00:14:35,050 --> 00:14:40,420
first the first fully connected layer

00:14:37,420 --> 00:14:43,930
again the nonlinear function and then

00:14:40,420 --> 00:14:47,490
lastly the last fully connected layer

00:14:43,930 --> 00:14:50,230
after which our output has a size 10

00:14:47,490 --> 00:14:52,209
okay but hold on we weren't going to

00:14:50,230 --> 00:14:53,830
train our own raw network right where

00:14:52,209 --> 00:14:57,520
we're going to do transfer learning yes

00:14:53,830 --> 00:14:59,050
that's right so what we first have to do

00:14:57,520 --> 00:15:02,410
if we're going to do transfer learning

00:14:59,050 --> 00:15:04,690
well we have to import this pre-trained

00:15:02,410 --> 00:15:09,339
network so what we can do in this case

00:15:04,690 --> 00:15:11,649
I've chosen a squeezed net and I did

00:15:09,339 --> 00:15:13,120
that because while VGA is actually a

00:15:11,649 --> 00:15:17,350
little bit bigger than squeezing it

00:15:13,120 --> 00:15:20,320
takes a longer to run so I'd go for the

00:15:17,350 --> 00:15:21,970
easy option so let's have a look at

00:15:20,320 --> 00:15:24,160
squeeze net so squeezed net you can

00:15:21,970 --> 00:15:25,839
simply import it and then instantiate it

00:15:24,160 --> 00:15:28,120
say pre-trained is true and it will

00:15:25,839 --> 00:15:29,230
download the weights for you which is a

00:15:28,120 --> 00:15:31,630
big set of weights take

00:15:29,230 --> 00:15:33,520
a while but then you get a pre-trained

00:15:31,630 --> 00:15:37,990
Network and you're gonna it's ready to

00:15:33,520 --> 00:15:41,170
use all ready to use for you but we

00:15:37,990 --> 00:15:42,670
weren't going to use that the preteen

00:15:41,170 --> 00:15:44,440
had worked with a thousand classes so

00:15:42,670 --> 00:15:46,210
we're going to modify it let's have a

00:15:44,440 --> 00:15:49,690
look at the internals before we modify

00:15:46,210 --> 00:15:52,720
it so if we go have a look at the

00:15:49,690 --> 00:15:54,310
internals we see if we just print the

00:15:52,720 --> 00:15:57,460
network it will show us all the layers

00:15:54,310 --> 00:15:59,500
and we find that it consists of two

00:15:57,460 --> 00:16:01,060
parts first it's called the first part

00:15:59,500 --> 00:16:02,560
is called the features and it has a lot

00:16:01,060 --> 00:16:04,470
of layers and I couldn't fit them all on

00:16:02,560 --> 00:16:07,420
the slides I think it's 20 layers or so

00:16:04,470 --> 00:16:09,610
and then you have lots of convolutions

00:16:07,420 --> 00:16:13,150
and pooling and these readily functions

00:16:09,610 --> 00:16:15,010
all seek in a sequence after each other

00:16:13,150 --> 00:16:18,700
and then in the end there's the

00:16:15,010 --> 00:16:21,070
classifier part which consists of four

00:16:18,700 --> 00:16:23,100
pieces of which you already recognize

00:16:21,070 --> 00:16:25,750
three there's the two deconvolution

00:16:23,100 --> 00:16:28,900
there's the radio and then average

00:16:25,750 --> 00:16:31,810
pooling at the end the first part is

00:16:28,900 --> 00:16:34,510
drop out and drop out is a technique to

00:16:31,810 --> 00:16:38,280
help your neural network learn a little

00:16:34,510 --> 00:16:42,070
quicker by while you're training it

00:16:38,280 --> 00:16:44,470
dropping the input or the output of a

00:16:42,070 --> 00:16:48,090
half in this case with the probability

00:16:44,470 --> 00:16:51,580
of 50% so half of the neurons that makes

00:16:48,090 --> 00:16:54,700
it impossible for the network to rely on

00:16:51,580 --> 00:16:57,910
a single neuron or a small subset of

00:16:54,700 --> 00:17:00,910
neurons so it must make more connections

00:16:57,910 --> 00:17:04,300
to learn the same information which

00:17:00,910 --> 00:17:06,310
basically makes it more robust so what

00:17:04,300 --> 00:17:08,740
happens here in the classifier is we

00:17:06,310 --> 00:17:13,330
apply this drop out during training then

00:17:08,740 --> 00:17:15,550
we have this 2d convolution from 512 to

00:17:13,330 --> 00:17:19,240
a thousand and this is again where you

00:17:15,550 --> 00:17:21,040
see the 1000 classes of output in the

00:17:19,240 --> 00:17:25,140
end and then there's the value and the

00:17:21,040 --> 00:17:28,360
average pooling so in the end we have

00:17:25,140 --> 00:17:30,580
again a thousand outputs and one for

00:17:28,360 --> 00:17:33,640
each of the classes that it wants to be

00:17:30,580 --> 00:17:36,220
able to classify now if we are going to

00:17:33,640 --> 00:17:38,830
change this and make it our own

00:17:36,220 --> 00:17:42,160
classifier for our own classes well then

00:17:38,830 --> 00:17:43,040
all we need to do is well simply define

00:17:42,160 --> 00:17:47,090
the number of classes

00:17:43,040 --> 00:17:50,390
that we have for example for download

00:17:47,090 --> 00:17:53,000
the model set it up first it has a

00:17:50,390 --> 00:17:54,380
parameter that says in um classes so we

00:17:53,000 --> 00:17:56,300
can update it to four although

00:17:54,380 --> 00:17:59,540
internally it's it it's not even used

00:17:56,300 --> 00:18:00,950
but let's do it to be complete and then

00:17:59,540 --> 00:18:03,790
what we can do is we can take this

00:18:00,950 --> 00:18:08,270
classifier part remember that it's the

00:18:03,790 --> 00:18:09,920
with the 2d convolution layer was the

00:18:08,270 --> 00:18:12,290
one with index one and we can simply

00:18:09,920 --> 00:18:15,260
replace it with a new 2d convolution

00:18:12,290 --> 00:18:17,270
layer that goes from 512 just like the

00:18:15,260 --> 00:18:20,450
original but now to our number of

00:18:17,270 --> 00:18:22,640
classes not a thousand so that's all you

00:18:20,450 --> 00:18:25,550
need and now you have a new neural

00:18:22,640 --> 00:18:29,810
network that you can train in order to

00:18:25,550 --> 00:18:31,280
classify your classes okay now let's

00:18:29,810 --> 00:18:35,630
have a look at how you train a model

00:18:31,280 --> 00:18:38,120
like this that looks like this so we

00:18:35,630 --> 00:18:40,130
start with setting our model to the

00:18:38,120 --> 00:18:43,910
training mode that's important I'll get

00:18:40,130 --> 00:18:46,250
to why in a little bit then we need to

00:18:43,910 --> 00:18:48,770
define our create a criterion how do we

00:18:46,250 --> 00:18:51,290
score whether the model is good or bad

00:18:48,770 --> 00:18:53,750
in this case we use cross-entropy loss

00:18:51,290 --> 00:18:55,700
and we need to define an optimizer and

00:18:53,750 --> 00:18:58,850
the optimizer in this case is stochastic

00:18:55,700 --> 00:19:00,800
gradient descent we say okay these are

00:18:58,850 --> 00:19:04,310
the model parameters and then there are

00:19:00,800 --> 00:19:08,120
some arguments that we have a look at in

00:19:04,310 --> 00:19:10,400
at a later stage and then we loop

00:19:08,120 --> 00:19:12,710
through stuff that comes out of a loader

00:19:10,400 --> 00:19:14,540
object and again we'll look at the

00:19:12,710 --> 00:19:16,850
loader object later and these are the

00:19:14,540 --> 00:19:18,620
inputs so the images and the labels so

00:19:16,850 --> 00:19:21,920
the classes that you've labeled them to

00:19:18,620 --> 00:19:24,380
be and for each of those sets of images

00:19:21,920 --> 00:19:27,230
and labels because we do this in batches

00:19:24,380 --> 00:19:29,900
you always process multiple images at

00:19:27,230 --> 00:19:32,360
the same time for each of those sets we

00:19:29,900 --> 00:19:33,950
first reset the optimizer because we

00:19:32,360 --> 00:19:36,620
don't want to use any information from

00:19:33,950 --> 00:19:38,240
the last batch then we simply pass the

00:19:36,620 --> 00:19:40,670
images through a neural network and then

00:19:38,240 --> 00:19:42,590
we get some outputs we calculate how

00:19:40,670 --> 00:19:45,650
good the outputs are do the output

00:19:42,590 --> 00:19:50,360
correspond with the labels that we give

00:19:45,650 --> 00:19:52,730
it then we propagate these this loss

00:19:50,360 --> 00:19:55,010
backwards through the network so we

00:19:52,730 --> 00:19:55,789
calculate for each neuron how well did

00:19:55,010 --> 00:20:00,109
it do

00:19:55,789 --> 00:20:03,529
on scoring your your training images and

00:20:00,109 --> 00:20:06,529
then in the end when we know that we can

00:20:03,529 --> 00:20:08,450
optimize the weights and then every time

00:20:06,529 --> 00:20:10,309
we loop through all our training images

00:20:08,450 --> 00:20:12,440
we call this one a book and you're going

00:20:10,309 --> 00:20:14,149
to do this quite a few times when you

00:20:12,440 --> 00:20:17,869
want a classifier that works a little

00:20:14,149 --> 00:20:20,269
bit well okay and once you've done that

00:20:17,869 --> 00:20:22,369
say suppose you've you've trained 20 a

00:20:20,269 --> 00:20:25,970
box then of course you want to know how

00:20:22,369 --> 00:20:28,369
well does my model actually work well

00:20:25,970 --> 00:20:30,379
for that first we set the model to

00:20:28,369 --> 00:20:31,879
evaluation mode so what is this

00:20:30,379 --> 00:20:35,149
difference between the training and

00:20:31,879 --> 00:20:37,190
evaluation mode now well most

00:20:35,149 --> 00:20:39,889
importantly it disables the drop out of

00:20:37,190 --> 00:20:43,580
course if you're going to train that it

00:20:39,889 --> 00:20:45,440
might work well to let your model use

00:20:43,580 --> 00:20:47,269
only half of the information in some

00:20:45,440 --> 00:20:48,830
some of the stages but when you're

00:20:47,269 --> 00:20:50,509
evaluating when you're trying to

00:20:48,830 --> 00:20:52,460
actually classify an image you want to

00:20:50,509 --> 00:20:54,739
make sure that you use all possible

00:20:52,460 --> 00:20:56,749
information that you have and disable

00:20:54,739 --> 00:20:58,849
dropout so that's the most important

00:20:56,749 --> 00:21:02,779
reason why we always must call this eval

00:20:58,849 --> 00:21:06,349
and these train methods well then we can

00:21:02,779 --> 00:21:08,299
say with no Grad which prevents the PI

00:21:06,349 --> 00:21:10,340
torch to do internal calculation that

00:21:08,299 --> 00:21:12,919
internal calculations that you don't

00:21:10,340 --> 00:21:15,440
need and then again we loop through this

00:21:12,919 --> 00:21:19,820
loader we pass the inputs through the

00:21:15,440 --> 00:21:22,039
model to get the output we can get for

00:21:19,820 --> 00:21:24,139
the outputs these are vectors with the

00:21:22,039 --> 00:21:26,119
probability for each class and we can

00:21:24,139 --> 00:21:29,149
get the maximum of these which is then

00:21:26,119 --> 00:21:30,979
the class that it will classify the

00:21:29,149 --> 00:21:33,139
images being so we can get the

00:21:30,979 --> 00:21:35,419
predictions from that and we can sum the

00:21:33,139 --> 00:21:39,019
loss in order to get some idea of how

00:21:35,419 --> 00:21:40,879
well our model is performing so I

00:21:39,019 --> 00:21:42,830
promised you also to have a look a

00:21:40,879 --> 00:21:45,320
closer look at the loader so where does

00:21:42,830 --> 00:21:47,749
a data actually come from well what you

00:21:45,320 --> 00:21:50,749
need to do first is specify where are

00:21:47,749 --> 00:21:52,970
your images on disk and you do that by

00:21:50,749 --> 00:21:54,649
defining those image folders and you

00:21:52,970 --> 00:21:58,190
want to have a separate train and test

00:21:54,649 --> 00:22:00,489
set and so you you define two image

00:21:58,190 --> 00:22:02,960
folders one with the pass to the train

00:22:00,489 --> 00:22:05,389
pass through the train images and one

00:22:02,960 --> 00:22:07,790
with a path through the test images but

00:22:05,389 --> 00:22:10,760
to both of those you need to first also

00:22:07,790 --> 00:22:14,570
define a transform so what methods will

00:22:10,760 --> 00:22:17,060
be applied to the images when they are

00:22:14,570 --> 00:22:19,280
loaded and we define two different

00:22:17,060 --> 00:22:21,440
transforms one for the training images

00:22:19,280 --> 00:22:24,230
and one for the test images let's first

00:22:21,440 --> 00:22:26,360
have a look at the test images so what

00:22:24,230 --> 00:22:28,760
we do is we say we come compose the

00:22:26,360 --> 00:22:31,460
transform side compare it consists of

00:22:28,760 --> 00:22:36,110
multiple steps first we resize it to

00:22:31,460 --> 00:22:38,780
size 256 then we crop out the center the

00:22:36,110 --> 00:22:40,490
two two to three hundred and twenty-four

00:22:38,780 --> 00:22:42,830
pixels in the center and then we

00:22:40,490 --> 00:22:44,630
transform it to a tensor such that pi

00:22:42,830 --> 00:22:46,100
torch can work with it well that's

00:22:44,630 --> 00:22:47,660
really simple but for the training

00:22:46,100 --> 00:22:50,000
images we do well something that's a

00:22:47,660 --> 00:22:53,330
little bit different what we do is we

00:22:50,000 --> 00:22:55,910
take a randomly resized crop of the same

00:22:53,330 --> 00:22:57,290
size I'm from the image so we don't

00:22:55,910 --> 00:22:59,060
always look at the same part of the

00:22:57,290 --> 00:23:00,590
image but it could be a little bit more

00:22:59,060 --> 00:23:02,390
zoomed in or a little bit more sound

00:23:00,590 --> 00:23:05,060
zoomed out a little bit more to the ref

00:23:02,390 --> 00:23:07,880
left or the right this means that every

00:23:05,060 --> 00:23:10,220
time that we train in a park our model

00:23:07,880 --> 00:23:13,250
actually gets to see a different set of

00:23:10,220 --> 00:23:15,860
images well the source images were the

00:23:13,250 --> 00:23:18,560
same but the actual image it looks at is

00:23:15,860 --> 00:23:21,260
just a little bit shifted or zoomed so

00:23:18,560 --> 00:23:23,570
it gets to learn not from the individual

00:23:21,260 --> 00:23:25,970
pixels but from actually the information

00:23:23,570 --> 00:23:29,390
that's in the image that's really

00:23:25,970 --> 00:23:32,120
important now once we've defined those

00:23:29,390 --> 00:23:33,980
training test sets we can define the

00:23:32,120 --> 00:23:37,240
training tests loaders which are

00:23:33,980 --> 00:23:40,040
different simply a data loader where we

00:23:37,240 --> 00:23:42,290
provide the data set that we want to use

00:23:40,040 --> 00:23:43,970
we set the batch size that's the number

00:23:42,290 --> 00:23:46,070
of images that we process at the same

00:23:43,970 --> 00:23:48,620
time the number of workers is the number

00:23:46,070 --> 00:23:51,260
of processes that can process these

00:23:48,620 --> 00:23:52,760
images while loading them and we say we

00:23:51,260 --> 00:23:54,680
want to shuffle them that means that

00:23:52,760 --> 00:23:56,860
every time we train an app work or we

00:23:54,680 --> 00:23:59,060
evaluate we do this in a random order

00:23:56,860 --> 00:24:03,590
for training this is really important

00:23:59,060 --> 00:24:05,150
for testing it isn't okay so we're

00:24:03,590 --> 00:24:09,830
almost there but I skipped something

00:24:05,150 --> 00:24:12,200
that's fairly important remember that

00:24:09,830 --> 00:24:14,900
when we defined our optimizer which is

00:24:12,200 --> 00:24:17,750
to cast a gradient descent I said well

00:24:14,900 --> 00:24:19,610
there are these arguments at the end and

00:24:17,750 --> 00:24:20,269
the most important one is the first one

00:24:19,610 --> 00:24:22,429
the

00:24:20,269 --> 00:24:24,409
is the learning rate and this is the

00:24:22,429 --> 00:24:26,839
rate at which we changed the weights

00:24:24,409 --> 00:24:30,259
when we're training so we need to figure

00:24:26,839 --> 00:24:31,759
out what is actually a good value now

00:24:30,259 --> 00:24:36,379
suppose that we don't have only a single

00:24:31,759 --> 00:24:38,389
weight and we only I can only plot make

00:24:36,379 --> 00:24:40,789
a plot in a single dimension so suppose

00:24:38,389 --> 00:24:43,969
we have a single weight and we want to

00:24:40,789 --> 00:24:47,379
optimize this we want to find the place

00:24:43,969 --> 00:24:50,269
right there at the bottom of this graph

00:24:47,379 --> 00:24:52,969
now suppose that we start all the way at

00:24:50,269 --> 00:24:55,729
the right of this graph and we want to

00:24:52,969 --> 00:24:57,709
by taking little steps find the bottom

00:24:55,729 --> 00:24:59,299
of the graph then of course we want to

00:24:57,709 --> 00:25:01,940
make sure that we don't take for example

00:24:59,299 --> 00:25:03,950
steps that are too large if we make

00:25:01,940 --> 00:25:07,489
steps that are too large yeah you could

00:25:03,950 --> 00:25:09,589
step all the way across the valley to

00:25:07,489 --> 00:25:12,619
the opposite side and then if you're

00:25:09,589 --> 00:25:15,259
unlucky you might even go so far away

00:25:12,619 --> 00:25:17,929
that you in the end step out of the

00:25:15,259 --> 00:25:20,749
valley and even reduce the performance

00:25:17,929 --> 00:25:22,999
of your model on the other hand if your

00:25:20,749 --> 00:25:24,859
learning rate is too small then first of

00:25:22,999 --> 00:25:26,869
all it takes a very long time to get

00:25:24,859 --> 00:25:29,119
there but in this case you'll find this

00:25:26,869 --> 00:25:32,629
local optimum there and you won't find

00:25:29,119 --> 00:25:35,599
the global so balancing this learning

00:25:32,629 --> 00:25:37,489
rate is really important so how do we

00:25:35,599 --> 00:25:39,919
actually find the best learning rate for

00:25:37,489 --> 00:25:43,639
a problem well the best thing that you

00:25:39,919 --> 00:25:45,859
can do is just try them out basically

00:25:43,639 --> 00:25:48,259
well here we define a function we set

00:25:45,859 --> 00:25:51,259
the learning rate for the optimizer to a

00:25:48,259 --> 00:25:54,200
certain value and then for a certain

00:25:51,259 --> 00:25:56,029
range of values so this log space from

00:25:54,200 --> 00:25:57,709
some minimum learning rate to some

00:25:56,029 --> 00:25:59,509
maximum learning rate with a number of

00:25:57,709 --> 00:26:02,359
steps and for each of these learning

00:25:59,509 --> 00:26:04,219
rates we we set the optimizer to this

00:26:02,359 --> 00:26:07,669
learning rate and then we trained for a

00:26:04,219 --> 00:26:10,759
number of batches and then after that we

00:26:07,669 --> 00:26:12,200
evaluate for a number of patches so what

00:26:10,759 --> 00:26:14,959
you'll then find is that of course

00:26:12,200 --> 00:26:17,599
during the course of the doing this your

00:26:14,959 --> 00:26:20,809
model is going to first you're starting

00:26:17,599 --> 00:26:23,419
with a very low learning rate it's going

00:26:20,809 --> 00:26:25,339
to improve very very slowly and after a

00:26:23,419 --> 00:26:27,409
while this improvement is going to be

00:26:25,339 --> 00:26:29,570
quicker and quicker and quicker until

00:26:27,409 --> 00:26:32,710
your learning rate is so big that it

00:26:29,570 --> 00:26:35,509
will go all the way away from your

00:26:32,710 --> 00:26:37,700
local or global minimum where you're at

00:26:35,509 --> 00:26:42,499
and the performance will be great

00:26:37,700 --> 00:26:45,639
enormous ly so what it will look like if

00:26:42,499 --> 00:26:48,619
you do this it's something like this and

00:26:45,639 --> 00:26:51,619
so typically what you see is that first

00:26:48,619 --> 00:26:54,440
you have some some value of loss and as

00:26:51,619 --> 00:26:57,019
you increase the learning rate the your

00:26:54,440 --> 00:26:59,269
loss will go down until after at some

00:26:57,019 --> 00:27:01,669
point it will go up way all the way

00:26:59,269 --> 00:27:04,429
until your model doesn't do anything

00:27:01,669 --> 00:27:06,860
anymore so what we found here is that

00:27:04,429 --> 00:27:09,649
typically something well like 10 to the

00:27:06,860 --> 00:27:12,049
minus 3 is the optimal learning rate so

00:27:09,649 --> 00:27:14,450
that's what we set it to but of course

00:27:12,049 --> 00:27:16,100
the optimal value also depends on the

00:27:14,450 --> 00:27:17,269
state of your model if your model

00:27:16,100 --> 00:27:19,490
doesn't do anything yet

00:27:17,269 --> 00:27:21,950
well then probably a very high learning

00:27:19,490 --> 00:27:23,570
rate is good well if it's almost there

00:27:21,950 --> 00:27:26,149
you just want to squeeze out that last

00:27:23,570 --> 00:27:28,759
percent of accuracy then probably your

00:27:26,149 --> 00:27:33,049
very low learning rate is the right way

00:27:28,759 --> 00:27:35,659
to go so that for that we have the

00:27:33,049 --> 00:27:37,279
learning rate scheduler we can use for

00:27:35,659 --> 00:27:39,399
example the reduced learning rate on

00:27:37,279 --> 00:27:42,499
Plateau which is a scheduler that

00:27:39,399 --> 00:27:45,019
whenever the performance of your model

00:27:42,499 --> 00:27:47,509
during training is has reached a sort of

00:27:45,019 --> 00:27:50,450
photo its stable it reduces the learning

00:27:47,509 --> 00:27:52,730
rate and then tries again so after every

00:27:50,450 --> 00:27:54,499
app work we then have to call scheduler

00:27:52,730 --> 00:27:58,490
that step with the loss that we found

00:27:54,499 --> 00:28:00,159
and based on that it might reduce that

00:27:58,490 --> 00:28:02,960
learning rate

00:28:00,159 --> 00:28:04,970
that looks like something like this so

00:28:02,960 --> 00:28:07,129
if you while you're training your

00:28:04,970 --> 00:28:09,590
accuracy goes up at the beginning and

00:28:07,129 --> 00:28:12,289
then after a while it figures out okay

00:28:09,590 --> 00:28:13,970
maybe we're stable now let's reduce the

00:28:12,289 --> 00:28:17,419
learning rate and you'll see that it

00:28:13,970 --> 00:28:20,450
takes these steps after all the time

00:28:17,419 --> 00:28:25,070
until what at some point you decide that

00:28:20,450 --> 00:28:28,129
the accuracy is good enough ok we're all

00:28:25,070 --> 00:28:31,100
set let's have a look at actually some

00:28:28,129 --> 00:28:32,809
data that I've played with so of course

00:28:31,100 --> 00:28:34,940
if you want to train a model you need

00:28:32,809 --> 00:28:37,309
data so what I did is I took one of

00:28:34,940 --> 00:28:39,769
these a Raspberry Pi and I set it to

00:28:37,309 --> 00:28:43,249
work for a couple of months and I

00:28:39,769 --> 00:28:44,740
gathered the data set of photos taken in

00:28:43,249 --> 00:28:47,559
the world's largest cities

00:28:44,740 --> 00:28:51,190
I took 72 cities half a million images

00:28:47,559 --> 00:28:53,080
of 10,000 photographers all in all some

00:28:51,190 --> 00:28:55,690
30 gigabytes of data and I made sure

00:28:53,080 --> 00:28:58,179
that all of these are licensed for reuse

00:28:55,690 --> 00:29:00,880
such that I can show you show them to

00:28:58,179 --> 00:29:02,470
you right now so of course the first

00:29:00,880 --> 00:29:04,990
thing that you do when you get our data

00:29:02,470 --> 00:29:07,240
set is you have a look at the images

00:29:04,990 --> 00:29:09,220
themselves so first I live in Amsterdam

00:29:07,240 --> 00:29:11,860
so what I did is I had a fur I had a

00:29:09,220 --> 00:29:13,779
look at all or a subset of the images

00:29:11,860 --> 00:29:16,029
that were taken in Amsterdam so this is

00:29:13,779 --> 00:29:20,799
a night one and typically we don't have

00:29:16,029 --> 00:29:23,649
weather like this so often but well it's

00:29:20,799 --> 00:29:25,380
nice right something very typical also

00:29:23,649 --> 00:29:27,700
that you find in Amsterdam are bikes

00:29:25,380 --> 00:29:30,520
this is a very typical scene from

00:29:27,700 --> 00:29:34,299
Amsterdam for those of you who have been

00:29:30,520 --> 00:29:36,250
there I'm sure you'll recognize it okay

00:29:34,299 --> 00:29:38,169
well this looks good right let's have a

00:29:36,250 --> 00:29:41,679
look at another one so this is a really

00:29:38,169 --> 00:29:43,240
nice view but hold on this wasn't taken

00:29:41,679 --> 00:29:45,760
in Amsterdam we don't have any cliffs

00:29:43,240 --> 00:29:48,730
like to within 200 kilometers form arms

00:29:45,760 --> 00:29:50,649
again probably even more so what's going

00:29:48,730 --> 00:29:52,750
on let's have a look at the metadata

00:29:50,649 --> 00:29:53,409
well there's this tag that says

00:29:52,750 --> 00:29:55,360
Amsterdam

00:29:53,409 --> 00:29:57,760
so probably someone thought this we were

00:29:55,360 --> 00:29:59,500
taken in Amsterdam why it wasn't on the

00:29:57,760 --> 00:30:04,450
other hand it also has a tag that says

00:29:59,500 --> 00:30:07,510
Dublin interesting interest there quite

00:30:04,450 --> 00:30:08,890
a few tags actually on this image and it

00:30:07,510 --> 00:30:12,370
couldn't fit more than this this is less

00:30:08,890 --> 00:30:14,200
than 5% of the tags of this image and I

00:30:12,370 --> 00:30:16,659
certainly don't see a teddy bear Museum

00:30:14,200 --> 00:30:18,429
on this image or all kind of see these

00:30:16,659 --> 00:30:21,340
things so it turns out people don't

00:30:18,429 --> 00:30:24,490
always tag their images as they should

00:30:21,340 --> 00:30:26,950
be so I hadn't look at where all the

00:30:24,490 --> 00:30:29,730
images that were supposedly taken in

00:30:26,950 --> 00:30:33,190
Amsterdam were taken in the world well

00:30:29,730 --> 00:30:35,200
around here and actually the the bench

00:30:33,190 --> 00:30:37,990
that we were just looking at was taken

00:30:35,200 --> 00:30:40,179
right there at the edge in Korea some

00:30:37,990 --> 00:30:43,240
nice island in Korea well definitely not

00:30:40,179 --> 00:30:44,440
actually so what you can do then what

00:30:43,240 --> 00:30:46,480
you could do of course

00:30:44,440 --> 00:30:48,490
you only want the images from Amsterdam

00:30:46,480 --> 00:30:49,960
that were taken right there the red dots

00:30:48,490 --> 00:30:52,330
in the middle that's where Amsterdam

00:30:49,960 --> 00:30:54,909
actually is what you can do is you can

00:30:52,330 --> 00:30:57,100
take all the images take the median

00:30:54,909 --> 00:30:57,910
latitude and longitude and now the

00:30:57,100 --> 00:30:59,470
mathematic

00:30:57,910 --> 00:31:01,210
will cringe because of course these are

00:30:59,470 --> 00:31:03,400
circular values and you can take of

00:31:01,210 --> 00:31:05,800
latitude a median of the latitude or

00:31:03,400 --> 00:31:09,820
longitude but in the end you can just do

00:31:05,800 --> 00:31:11,320
it and it works then what you can do is

00:31:09,820 --> 00:31:13,300
you can remove all the images that were

00:31:11,320 --> 00:31:14,770
more than five kilometers away and you

00:31:13,300 --> 00:31:17,620
repeat this for all cities then we have

00:31:14,770 --> 00:31:20,680
a clean data set right okay let's do it

00:31:17,620 --> 00:31:22,450
okay then after that I had a look at all

00:31:20,680 --> 00:31:23,950
the other tags and thought of something

00:31:22,450 --> 00:31:25,600
cool that we could do these were the

00:31:23,950 --> 00:31:27,430
most common tags in this data set

00:31:25,600 --> 00:31:30,520
well of course if you're going to look

00:31:27,430 --> 00:31:34,680
for photos taken in cities then the most

00:31:30,520 --> 00:31:37,750
common Tigers City well but I think

00:31:34,680 --> 00:31:40,270
other than city maybe um this one

00:31:37,750 --> 00:31:42,430
skyline here is the most interesting one

00:31:40,270 --> 00:31:45,970
let's try to make it an image classifier

00:31:42,430 --> 00:31:47,740
that recognizes skylines of cities so

00:31:45,970 --> 00:31:52,210
what I did is I took the ten most common

00:31:47,740 --> 00:31:54,670
cities in my data set those were these I

00:31:52,210 --> 00:31:56,860
hear that the image counts I split these

00:31:54,670 --> 00:32:03,670
into the Train and a test set like this

00:31:56,860 --> 00:32:06,280
and then we train them up hold on we

00:32:03,670 --> 00:32:08,410
wait first we went and the way they

00:32:06,280 --> 00:32:11,590
actually is quite annoying because well

00:32:08,410 --> 00:32:15,280
training a model like this takes a while

00:32:11,590 --> 00:32:17,850
in this case I took a fast GPU I used my

00:32:15,280 --> 00:32:20,980
boss's credit card he doesn't know yet

00:32:17,850 --> 00:32:23,200
you'll be a little bit surprised and

00:32:20,980 --> 00:32:25,510
then I spent like 20 hours or so on

00:32:23,200 --> 00:32:28,270
training time and then I had a model

00:32:25,510 --> 00:32:32,010
so then well you feed in an image and

00:32:28,270 --> 00:32:32,010
who knows where this image was taken

00:32:33,330 --> 00:32:38,710
this is London and I've got it correct

00:32:35,710 --> 00:32:44,410
okay well that's nice so this one where

00:32:38,710 --> 00:32:47,430
is this this is Sydney and it learned

00:32:44,410 --> 00:32:53,050
that all right wow that's cool this one

00:32:47,430 --> 00:32:57,960
anyone this is Toronto I heard it right

00:32:53,050 --> 00:33:01,160
there okay cool this one is tough

00:32:57,960 --> 00:33:04,110
where is this

00:33:01,160 --> 00:33:06,780
this is La and actually the model go to

00:33:04,110 --> 00:33:11,250
the right so it's pretty impressed so

00:33:06,780 --> 00:33:14,850
this is clearly Chicago right and then

00:33:11,250 --> 00:33:15,870
here we have Philadelphia got it all

00:33:14,850 --> 00:33:19,530
right again

00:33:15,870 --> 00:33:22,500
this is Tokyo cool even this one it's

00:33:19,530 --> 00:33:24,480
not really so complicated it doesn't

00:33:22,500 --> 00:33:26,940
have too many buildings I would say I

00:33:24,480 --> 00:33:30,530
wouldn't know it but it got it all right

00:33:26,940 --> 00:33:35,400
it's Houston here we have Shanghai and

00:33:30,530 --> 00:33:38,730
this is clearly Chicago right wait what

00:33:35,400 --> 00:33:41,990
what just happened what turns out there

00:33:38,730 --> 00:33:45,240
was one photographer who labeled all his

00:33:41,990 --> 00:33:48,450
photos with the tag Chicago while all he

00:33:45,240 --> 00:33:52,620
did was take photos of sandals on

00:33:48,450 --> 00:33:55,110
pavement and my model well he got it all

00:33:52,620 --> 00:33:58,260
read it learned that a sandal on

00:33:55,110 --> 00:34:00,270
pavement it must be in Chicago and of

00:33:58,260 --> 00:34:02,190
course well then when you feed this test

00:34:00,270 --> 00:34:05,550
image to the model it gets all right

00:34:02,190 --> 00:34:09,720
this must be Chicago okay so we need to

00:34:05,550 --> 00:34:11,780
fix this let's come up with a plan okay

00:34:09,720 --> 00:34:14,310
first what we can do instead of

00:34:11,780 --> 00:34:16,500
splitting put the images randomly in

00:34:14,310 --> 00:34:19,110
Twain test set what we couldn't do is we

00:34:16,500 --> 00:34:21,960
can split them by photographer in this

00:34:19,110 --> 00:34:23,640
case at least the olders sandals will

00:34:21,960 --> 00:34:26,909
end up either in the train or in the

00:34:23,640 --> 00:34:29,990
test set so and we wait

00:34:26,909 --> 00:34:35,580
takes a while this is really annoying

00:34:29,990 --> 00:34:37,080
once in a while because it's late at

00:34:35,580 --> 00:34:39,510
night you want to do some hacking on

00:34:37,080 --> 00:34:42,179
your project and then you think of a

00:34:39,510 --> 00:34:43,770
solution you fix it you start training

00:34:42,179 --> 00:34:46,830
and then well you must wait till

00:34:43,770 --> 00:34:48,090
tomorrow to see the results anyway the

00:34:46,830 --> 00:34:49,530
results of this one were terrible

00:34:48,090 --> 00:34:51,600
because of course well if you put all

00:34:49,530 --> 00:34:53,520
those sandals in Train Set and it will

00:34:51,600 --> 00:34:54,030
get a very high train accuracy but the

00:34:53,520 --> 00:34:57,480
tests

00:34:54,030 --> 00:35:00,060
accuracy will be terrible in fact it

00:34:57,480 --> 00:35:02,130
will just be over trained on those in

00:35:00,060 --> 00:35:05,130
the end we just have too many miss

00:35:02,130 --> 00:35:08,520
tagged photos so I needed to come up

00:35:05,130 --> 00:35:10,800
with another plan and in this case what

00:35:08,520 --> 00:35:14,230
I did is I built another model and it is

00:35:10,800 --> 00:35:16,600
get what I did I took a

00:35:14,230 --> 00:35:19,870
classified only two classes and I said

00:35:16,600 --> 00:35:22,390
either it has a skyline or it is not a

00:35:19,870 --> 00:35:23,980
photo of a skyline and I trained on all

00:35:22,390 --> 00:35:27,040
data that I have so half a million

00:35:23,980 --> 00:35:29,290
images and I gave them the labels either

00:35:27,040 --> 00:35:31,120
it is a skyline when it has the skyline

00:35:29,290 --> 00:35:32,770
tag or it's not a skyline when it

00:35:31,120 --> 00:35:34,570
doesn't have the start line tag then I

00:35:32,770 --> 00:35:37,150
could make predictions for all data and

00:35:34,570 --> 00:35:39,610
make and then only use the data that

00:35:37,150 --> 00:35:42,880
were labeled with a positive prediction

00:35:39,610 --> 00:35:46,540
for skyline for my original model again

00:35:42,880 --> 00:35:49,630
then we have to wait takes a while get

00:35:46,540 --> 00:35:51,520
really annoying after a while but then

00:35:49,630 --> 00:35:55,960
in the end the results of this were

00:35:51,520 --> 00:35:58,750
pretty nice out of those with a tag that

00:35:55,960 --> 00:36:00,940
had skyline I had about 6,000 that were

00:35:58,750 --> 00:36:04,000
labeled by the model as actually having

00:36:00,940 --> 00:36:05,920
a skyline and about 1000 that were

00:36:04,000 --> 00:36:08,230
labeled as not having a skyline so I

00:36:05,920 --> 00:36:09,970
could just get rid of those and put it

00:36:08,230 --> 00:36:12,100
the other end I got a thousand images

00:36:09,970 --> 00:36:16,120
that did have a skyline according to my

00:36:12,100 --> 00:36:18,610
model but that didn't have the skyline

00:36:16,120 --> 00:36:21,790
tag so still I ended up with about the

00:36:18,610 --> 00:36:24,400
same number of images so I recreated

00:36:21,790 --> 00:36:27,550
this train test split that had to wait

00:36:24,400 --> 00:36:30,070
again as I tell you this gets really

00:36:27,550 --> 00:36:34,720
annoying and my boss will not be happy

00:36:30,070 --> 00:36:37,570
with me and in the end I got yet more

00:36:34,720 --> 00:36:40,680
results and as you can see in the end

00:36:37,570 --> 00:36:44,290
the accuracy was about about 70 percent

00:36:40,680 --> 00:36:46,450
after 200 airport training airports or

00:36:44,290 --> 00:36:49,450
so so that's I think 24 hours of

00:36:46,450 --> 00:36:53,740
training I think this was fairly

00:36:49,450 --> 00:36:55,660
reasonable ok well that's cool let's

00:36:53,740 --> 00:36:58,480
have a look at some of the actual

00:36:55,660 --> 00:37:00,340
results so this was it got it right

00:36:58,480 --> 00:37:02,170
Chicago of course that's something I

00:37:00,340 --> 00:37:05,020
would also recognize so that's cool

00:37:02,170 --> 00:37:08,800
it means it actually learns to recognize

00:37:05,020 --> 00:37:13,540
some of these cities also Los Angeles it

00:37:08,800 --> 00:37:17,680
got it right in this case it the model

00:37:13,540 --> 00:37:21,790
said New York City while in reality the

00:37:17,680 --> 00:37:24,190
label was Philadelphia but to be honest

00:37:21,790 --> 00:37:25,920
looking at this picture I probably would

00:37:24,190 --> 00:37:29,250
have gotten it wrong as well

00:37:25,920 --> 00:37:31,380
so sometimes it's not that bad here we

00:37:29,250 --> 00:37:33,390
have an example where the model says

00:37:31,380 --> 00:37:35,750
it's London probably because of the bad

00:37:33,390 --> 00:37:40,620
weather but it was actually taken in

00:37:35,750 --> 00:37:42,510
Toronto sometimes though you cannot

00:37:40,620 --> 00:37:45,120
explain the errors that the model makes

00:37:42,510 --> 00:37:47,700
because in this case well although the

00:37:45,120 --> 00:37:49,500
skyline is a bit difficult to see but

00:37:47,700 --> 00:37:51,990
you can see some high buildings in the

00:37:49,500 --> 00:37:55,860
end in the background but you can

00:37:51,990 --> 00:37:57,990
clearly see that this street definitely

00:37:55,860 --> 00:38:01,110
is not an American Street but it's

00:37:57,990 --> 00:38:02,940
something like in Asia so in this was

00:38:01,110 --> 00:38:05,120
actually taken in Shanghai and it got it

00:38:02,940 --> 00:38:05,120
wrong

00:38:05,280 --> 00:38:14,400
yeah so that was it but it before I and

00:38:11,060 --> 00:38:15,990
just some final remarks I'm training

00:38:14,400 --> 00:38:17,790
your own in an image classifier really

00:38:15,990 --> 00:38:20,040
isn't that difficult all you need to do

00:38:17,790 --> 00:38:21,810
is cheat a little and do transfer

00:38:20,040 --> 00:38:25,560
learning otherwise you won't be waiting

00:38:21,810 --> 00:38:28,380
for 24 hours but for months on end doing

00:38:25,560 --> 00:38:30,510
PI torch is fun Kiera's might be easier

00:38:28,380 --> 00:38:32,820
and faster but pythor is a lot of fun

00:38:30,510 --> 00:38:35,300
and in the end having clean data is way

00:38:32,820 --> 00:38:37,050
more important than having a good model

00:38:35,300 --> 00:38:40,860
thank you

00:38:37,050 --> 00:38:43,230
and after this if you want to have a

00:38:40,860 --> 00:38:45,480
look at my code you can have a look at

00:38:43,230 --> 00:38:48,990
this gitlab link you'll find an example

00:38:45,480 --> 00:38:51,960
or all the code that I used to create

00:38:48,990 --> 00:38:54,720
this image classifier and keep an eye on

00:38:51,960 --> 00:38:56,970
our blog blog dr/dt reason.com where I

00:38:54,720 --> 00:39:00,620
will make sort of transcript of this

00:38:56,970 --> 00:39:00,620
talk Thanks

00:39:06,160 --> 00:39:25,160
there are questions we have two mics so

00:39:09,170 --> 00:39:26,780
please line up hi you mentioned two very

00:39:25,160 --> 00:39:31,880
different kinds of hardware the

00:39:26,780 --> 00:39:34,910
Raspberry Pi and the GPU can you say a

00:39:31,880 --> 00:39:37,700
little bit more about whether this is

00:39:34,910 --> 00:39:39,770
really practical on a Raspberry Pi alone

00:39:37,700 --> 00:39:41,510
and if it's not then how does one

00:39:39,770 --> 00:39:46,540
actually take the next step to use the

00:39:41,510 --> 00:39:49,280
GPU as well yes of course thank you I

00:39:46,540 --> 00:39:51,740
did not train any of them these models

00:39:49,280 --> 00:39:54,530
on the Raspberry Pi I I merely used it

00:39:51,740 --> 00:39:59,109
to collect my data since that's just a

00:39:54,530 --> 00:40:02,480
bunch of web scraping you don't need any

00:39:59,109 --> 00:40:03,890
like big hardware for that if you're

00:40:02,480 --> 00:40:06,410
going to train a model

00:40:03,890 --> 00:40:09,319
I tried training it on my my MacBook

00:40:06,410 --> 00:40:12,319
that takes way too long I did have to

00:40:09,319 --> 00:40:14,359
get a machine with the GPU on the other

00:40:12,319 --> 00:40:16,190
hand if you have a very small training

00:40:14,359 --> 00:40:18,230
data set you might give it a try on your

00:40:16,190 --> 00:40:20,390
laptop it's still fun you might get as

00:40:18,230 --> 00:40:23,119
far as 10 or 20 a box and then get a

00:40:20,390 --> 00:40:24,770
reasonable accuracy it still is fun to

00:40:23,119 --> 00:40:28,040
play with if you want a little bit

00:40:24,770 --> 00:40:30,280
better I'm getting a GPU or I don't know

00:40:28,040 --> 00:40:34,119
getting a cloud machine with the GPU

00:40:30,280 --> 00:40:34,119
there's always the way to go

00:40:40,680 --> 00:40:47,410
in the one miss categorization where you

00:40:44,410 --> 00:40:50,760
had the Shanghai like very small section

00:40:47,410 --> 00:40:55,360
of skyline I include that in the Tessa

00:40:50,760 --> 00:40:56,980
so I didn't actually actually make the

00:40:55,360 --> 00:41:00,940
choice myself to include this in the

00:40:56,980 --> 00:41:02,620
test set I included all images that were

00:41:00,940 --> 00:41:05,230
classified by the previous model as

00:41:02,620 --> 00:41:07,630
being a skyline and so apparently it got

00:41:05,230 --> 00:41:09,490
to recognize the properties of what is a

00:41:07,630 --> 00:41:11,710
skyline and it recognized in the

00:41:09,490 --> 00:41:13,330
background this is something that looks

00:41:11,710 --> 00:41:14,470
a bit like a skyline well I guess my

00:41:13,330 --> 00:41:15,760
question is you mentioned that clean

00:41:14,470 --> 00:41:17,230
data is better than having a good model

00:41:15,760 --> 00:41:19,210
and so just to me doesn't look like

00:41:17,230 --> 00:41:21,250
clean data it might be like I'd expect a

00:41:19,210 --> 00:41:23,650
supergenius classifier to figure it out

00:41:21,250 --> 00:41:24,670
but I would have excluded this if I just

00:41:23,650 --> 00:41:26,740
saw your talk and didn't see this

00:41:24,670 --> 00:41:28,000
example so I'm curious if I'm just wrong

00:41:26,740 --> 00:41:31,600
and there's some value and stuff like

00:41:28,000 --> 00:41:33,850
this or well in this case I guess it's

00:41:31,600 --> 00:41:37,570
just laziness I didn't go through my

00:41:33,850 --> 00:41:39,310
entire test set before using it because

00:41:37,570 --> 00:41:41,560
we're going through thousands of images

00:41:39,310 --> 00:41:45,910
and manually labeling them as good or

00:41:41,560 --> 00:41:49,290
bad is not my idea of fun I think a

00:41:45,910 --> 00:41:49,290
proper data scientist must be lazy

00:41:51,360 --> 00:41:54,870
any more questions

00:41:57,310 --> 00:42:04,670
well again thanks

00:41:58,980 --> 00:42:04,670

YouTube URL: https://www.youtube.com/watch?v=eDRck258bnw


