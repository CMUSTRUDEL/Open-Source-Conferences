Title: Hubert Bryłkowski - Natural language processing with neural networks.
Publication date: 2019-09-23
Playlist: EuroPython 2019
Description: 
	"Natural language processing with neural networks.
[EuroPython 2019 - Talk - 2019-07-11 - Singapore [PyData track]
[Basel, CH]

By Hubert Bryłkowski

Getting started with a natural language processing and neural networks is easier nowadays thanks to the numerous talks and tutorials. The goal is to dive deeper for those who already know the basics, or want to expand their knowledge in a machine learning field. 
The talk will start with the common use cases that can be generalized to the specific problems in a NLP world. Then I will present an overview of possible features that we can use as input to our network, and show that even simple feature engineering can change our results.
Furthermore, I will compare different network architectures - starting with the fully connected networks, through convolution neural networks to recursive neural networks. I will not only considering the good parts, but also - what is usually overlooked - pitfalls of every solution. 
All of these  will be done considering number of parameters, which transfers into training and prediction costs and time. I will also share a number of “tricks” that enables getting the best results even out of the simple architectures, as these are usually the fastest and quite often hard to beat, at the same time being the easiest to interpret.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:05,340 --> 00:00:09,730
so hello everyone and thank you for

00:00:07,960 --> 00:00:11,830
attending this talk I know that the

00:00:09,730 --> 00:00:14,499
title may seem a little buzz worthy but

00:00:11,830 --> 00:00:17,890
I hope that I will present more down to

00:00:14,499 --> 00:00:23,679
the ground view so first few words about

00:00:17,890 --> 00:00:25,990
me I am back-end engineer in the machine

00:00:23,679 --> 00:00:29,650
learning team so besides of the problems

00:00:25,990 --> 00:00:32,050
with the regular engineer in the

00:00:29,650 --> 00:00:34,539
distributed environment I also have to

00:00:32,050 --> 00:00:36,190
face and solve the problems of machine

00:00:34,539 --> 00:00:39,340
learning think of in cooperation with

00:00:36,190 --> 00:00:41,649
our data science well if you want to

00:00:39,340 --> 00:00:44,170
talk with me I will be really happy to

00:00:41,649 --> 00:00:46,629
discuss the machine learning engineering

00:00:44,170 --> 00:00:48,160
stuff Python but also my hobbies like 3d

00:00:46,629 --> 00:00:52,390
printing and home brewing

00:00:48,160 --> 00:00:54,699
so what even is NLP as a natural

00:00:52,390 --> 00:00:57,340
language processing we may think of

00:00:54,699 --> 00:00:59,680
first processing of human voice and

00:00:57,340 --> 00:01:01,870
voice recognition but I will not stop

00:00:59,680 --> 00:01:04,629
here today but I will focus on the

00:01:01,870 --> 00:01:07,990
processing on the already text that we

00:01:04,629 --> 00:01:09,880
have textual data and why natural

00:01:07,990 --> 00:01:13,060
language processing is hard first

00:01:09,880 --> 00:01:15,159
language is ambiguous if we say that hey

00:01:13,060 --> 00:01:18,340
I had a sandwich with bacon it's hard to

00:01:15,159 --> 00:01:23,259
say whether we met Kevin Bacon for land

00:01:18,340 --> 00:01:25,810
or we had a sandwich of pork meat second

00:01:23,259 --> 00:01:28,329
texts are compositional characters

00:01:25,810 --> 00:01:30,189
composed words and then words composes

00:01:28,329 --> 00:01:33,479
sentences and finally paragraphs and

00:01:30,189 --> 00:01:36,640
whole books and here the problem is that

00:01:33,479 --> 00:01:37,240
even if we had the same letters that

00:01:36,640 --> 00:01:40,240
compose

00:01:37,240 --> 00:01:42,460
towards let's say burger and pizza they

00:01:40,240 --> 00:01:45,100
sure none of the characters but they

00:01:42,460 --> 00:01:47,560
still carry the meaning of junk food but

00:01:45,100 --> 00:01:50,890
we cannot conclude them only on based on

00:01:47,560 --> 00:01:55,540
their characters so that's a few reason

00:01:50,890 --> 00:01:58,329
why this is hard and if you want to

00:01:55,540 --> 00:01:59,909
learn more about the traditional NLP

00:01:58,329 --> 00:02:02,979
approaches and more

00:01:59,909 --> 00:02:04,630
overview of the opinion itself I highly

00:02:02,979 --> 00:02:07,299
recommend you to listening to the last

00:02:04,630 --> 00:02:09,369
year last year lecture about

00:02:07,299 --> 00:02:11,980
introduction to sentiment analysis with

00:02:09,369 --> 00:02:15,370
spacing the slides will be shared with

00:02:11,980 --> 00:02:17,050
you later so I will have a lot of links

00:02:15,370 --> 00:02:20,690
to the further reading

00:02:17,050 --> 00:02:22,930
then what do common problems we have in

00:02:20,690 --> 00:02:25,130
NLP because well if somebody already

00:02:22,930 --> 00:02:27,650
solved the issue we are having and we

00:02:25,130 --> 00:02:30,230
can generalize to that we can either use

00:02:27,650 --> 00:02:32,930
the ready solution or at least be

00:02:30,230 --> 00:02:34,580
inspired by it so the common problems

00:02:32,930 --> 00:02:37,370
are for example the current

00:02:34,580 --> 00:02:39,380
classification and this also includes

00:02:37,370 --> 00:02:41,690
the human sentiment whether for example

00:02:39,380 --> 00:02:43,370
they reviews on our business on the

00:02:41,690 --> 00:02:46,070
website are positive or negative because

00:02:43,370 --> 00:02:47,780
we if we have thousands or millions of

00:02:46,070 --> 00:02:51,950
them it would be hard to classify them

00:02:47,780 --> 00:02:53,990
by hand but also of our attribution who

00:02:51,950 --> 00:02:56,510
wrote the document and this is how -

00:02:53,990 --> 00:02:59,900
exciting because not only by the words

00:02:56,510 --> 00:03:03,170
used but also by the way they can

00:02:59,900 --> 00:03:05,300
construct the sentences and also like

00:03:03,170 --> 00:03:07,430
very practical use case whether the

00:03:05,300 --> 00:03:10,970
email we just received is a spam or not

00:03:07,430 --> 00:03:13,580
or whether is important or not another

00:03:10,970 --> 00:03:16,310
common issue is sake sequence - sequence

00:03:13,580 --> 00:03:19,120
and this includes but it's not limited

00:03:16,310 --> 00:03:21,290
to translation like Google Translate

00:03:19,120 --> 00:03:23,090
summarization of text like we have a

00:03:21,290 --> 00:03:25,850
whole article and we just want to create

00:03:23,090 --> 00:03:28,610
abstract of it just oh now if we would

00:03:25,850 --> 00:03:32,959
be interested in it and also respond

00:03:28,610 --> 00:03:34,940
generation really need feature in Gmail

00:03:32,959 --> 00:03:37,160
when we receive an email we just have

00:03:34,940 --> 00:03:41,000
one of few possible responses just to

00:03:37,160 --> 00:03:42,890
tap and respond to our Center another

00:03:41,000 --> 00:03:45,320
common problem is information extraction

00:03:42,890 --> 00:03:48,650
we have a sentence to multiple shares

00:03:45,320 --> 00:03:50,510
and during button Apple and we want to

00:03:48,650 --> 00:03:53,150
know the Apple refers to the company or

00:03:50,510 --> 00:03:55,280
to a fruit and this is riu SFIL in

00:03:53,150 --> 00:03:59,090
search engines or I would say in ads

00:03:55,280 --> 00:04:00,470
because when we have an ad we would want

00:03:59,090 --> 00:04:02,239
to know it would be relevant

00:04:00,470 --> 00:04:05,330
should it be relevant to the fruit or

00:04:02,239 --> 00:04:08,000
should it be relevant to iPhones so

00:04:05,330 --> 00:04:10,820
these are only a few the most common

00:04:08,000 --> 00:04:12,560
ones I would say but also you you will

00:04:10,820 --> 00:04:16,310
learn if you want that there are a lot

00:04:12,560 --> 00:04:20,450
of more than a lot of them why neural

00:04:16,310 --> 00:04:22,220
networks are good for an Opie text carry

00:04:20,450 --> 00:04:24,200
a lot of features and we can extract

00:04:22,220 --> 00:04:26,350
them by hand we can label them by hand

00:04:24,200 --> 00:04:29,370
we can in sentiment analysis for example

00:04:26,350 --> 00:04:31,560
find and hand pick the words that come

00:04:29,370 --> 00:04:34,229
the sentiment is either positive or

00:04:31,560 --> 00:04:36,960
negative whether the somebody--you

00:04:34,229 --> 00:04:41,280
mentions terrible probably they mean

00:04:36,960 --> 00:04:43,770
that our business is bad and the idea is

00:04:41,280 --> 00:04:46,680
that the neural networks will learn

00:04:43,770 --> 00:04:50,340
those features on their own and as the

00:04:46,680 --> 00:04:52,229
practice shows they usually do I will

00:04:50,340 --> 00:04:53,160
focus and show examples from this

00:04:52,229 --> 00:04:56,490
quote-unquote

00:04:53,160 --> 00:05:00,090
real life problem the IMDB sentiment

00:04:56,490 --> 00:05:04,289
analysis why I put real in quotes

00:05:00,090 --> 00:05:06,870
because in real life we do not have such

00:05:04,289 --> 00:05:10,830
beautiful data sets these are 25,000

00:05:06,870 --> 00:05:13,440
highly polar movie reviews and first our

00:05:10,830 --> 00:05:17,850
elevator will usually not be so polar

00:05:13,440 --> 00:05:21,360
and not so your will have usually a lot

00:05:17,850 --> 00:05:23,630
of more a lot of noise there so we'll

00:05:21,360 --> 00:05:26,160
have to live with that but as the

00:05:23,630 --> 00:05:28,919
exercise I think it's really good

00:05:26,160 --> 00:05:32,490
I will focus as the matrix as on

00:05:28,919 --> 00:05:37,349
accuracy later and also has the cost I

00:05:32,490 --> 00:05:39,720
will work on the training time my

00:05:37,349 --> 00:05:42,000
training time not like the number of

00:05:39,720 --> 00:05:44,039
parameters because right now when we are

00:05:42,000 --> 00:05:46,200
paying for servers on working on our

00:05:44,039 --> 00:05:49,229
computer that's the thing that we are

00:05:46,200 --> 00:05:51,270
concerned most about and when we have

00:05:49,229 --> 00:05:54,780
complex networks it won't translate

00:05:51,270 --> 00:05:57,930
directly into that but the downside is

00:05:54,780 --> 00:05:59,760
that in the future when we have better

00:05:57,930 --> 00:06:02,610
ways of parallelization or better

00:05:59,760 --> 00:06:04,440
algorithms we may find out that the

00:06:02,610 --> 00:06:06,870
networks that are really expensive to

00:06:04,440 --> 00:06:09,690
run right now will be cheaper in the

00:06:06,870 --> 00:06:12,840
future so like what everything it

00:06:09,690 --> 00:06:14,910
depends what do you choose our task

00:06:12,840 --> 00:06:16,260
definition we have a movie review we

00:06:14,910 --> 00:06:18,030
want to put that in our neural network

00:06:16,260 --> 00:06:18,870
and we want to decide whether it was

00:06:18,030 --> 00:06:24,080
good or bad

00:06:18,870 --> 00:06:24,080
so looks simple in but there's a catch

00:06:24,229 --> 00:06:31,050
on the review itself because we since

00:06:28,169 --> 00:06:33,479
the neural networks are basically matrix

00:06:31,050 --> 00:06:35,699
multiplications and additions and also

00:06:33,479 --> 00:06:39,210
activation functions we cannot throw

00:06:35,699 --> 00:06:42,720
text directly there we have to we need

00:06:39,210 --> 00:06:44,250
to have some numeral representation

00:06:42,720 --> 00:06:45,870
and to obtain that numerical

00:06:44,250 --> 00:06:49,440
representation we first need to have

00:06:45,870 --> 00:06:50,760
some features so this is example of the

00:06:49,440 --> 00:06:51,750
text like we can see a big

00:06:50,760 --> 00:06:53,850
disappointment

00:06:51,750 --> 00:06:56,730
incredibly bad very pretentious it's

00:06:53,850 --> 00:07:01,170
highly pure Perl Pollard like I

00:06:56,730 --> 00:07:03,060
mentioned but first to use this text as

00:07:01,170 --> 00:07:04,920
an input we need to do something with it

00:07:03,060 --> 00:07:08,490
we need to translate first in features

00:07:04,920 --> 00:07:10,500
and then into into this vector I will

00:07:08,490 --> 00:07:12,990
focus on this simple sentence what

00:07:10,500 --> 00:07:15,960
possible we can extract a quick brown

00:07:12,990 --> 00:07:18,420
fox it looks like we only have a few

00:07:15,960 --> 00:07:18,720
words but let's see what we can do with

00:07:18,420 --> 00:07:20,850
it

00:07:18,720 --> 00:07:22,350
first we can recognize that sentence and

00:07:20,850 --> 00:07:25,260
by tokenized

00:07:22,350 --> 00:07:29,280
I mean split it into chunks for the

00:07:25,260 --> 00:07:31,950
English most often they will be words so

00:07:29,280 --> 00:07:34,320
we have a tokens like app then quick and

00:07:31,950 --> 00:07:37,260
Brandon Fox and let's focus from the

00:07:34,320 --> 00:07:39,840
last one but if you work with another

00:07:37,260 --> 00:07:42,780
languages you may find that it's not so

00:07:39,840 --> 00:07:44,220
easy and especially for German because

00:07:42,780 --> 00:07:46,440
they glue words together

00:07:44,220 --> 00:07:48,810
I do not know German but I know that

00:07:46,440 --> 00:07:50,880
there's the case but you can use a

00:07:48,810 --> 00:07:54,840
library like Center speech from Google

00:07:50,880 --> 00:07:57,930
to try and live with that or try some

00:07:54,840 --> 00:08:00,660
other money so get back let's back get

00:07:57,930 --> 00:08:01,320
back to the word Fox what do we know

00:08:00,660 --> 00:08:07,050
right

00:08:01,320 --> 00:08:08,850
I only touch the cable okay let's focus

00:08:07,050 --> 00:08:11,820
about the word facts what do we know

00:08:08,850 --> 00:08:13,800
about if we use classical and all P

00:08:11,820 --> 00:08:17,520
models statistical ones we can extract

00:08:13,800 --> 00:08:19,560
information that it is a noun or we can

00:08:17,520 --> 00:08:23,070
label them that by hand but we can

00:08:19,560 --> 00:08:25,110
automate it also we if we use the word

00:08:23,070 --> 00:08:27,900
net database we can extract that the

00:08:25,110 --> 00:08:31,020
word Fox belongs to sin set since that

00:08:27,900 --> 00:08:33,089
is the wider meaning of the Canon can I

00:08:31,020 --> 00:08:34,890
guess not dog leash animal let's say

00:08:33,089 --> 00:08:36,719
simplify probably if there's the

00:08:34,890 --> 00:08:40,589
biologist they will be angry about that

00:08:36,719 --> 00:08:43,589
but okay what else when can we know

00:08:40,589 --> 00:08:46,950
about this word we can extract its stem

00:08:43,589 --> 00:08:49,770
stem is the core of the world and we can

00:08:46,950 --> 00:08:52,680
extract its lemma lemma is a basic form

00:08:49,770 --> 00:08:54,890
of the world and for that simple case

00:08:52,680 --> 00:08:58,610
they will both be Fox

00:08:54,890 --> 00:09:01,010
if we have an access to the whole corpus

00:08:58,610 --> 00:09:03,079
of the data we can also calculate the

00:09:01,010 --> 00:09:06,820
term frequency inverse document

00:09:03,079 --> 00:09:09,740
frequency it tells us basically how

00:09:06,820 --> 00:09:13,820
important this word is in this given

00:09:09,740 --> 00:09:16,190
sentence just to simplify and it can be

00:09:13,820 --> 00:09:18,649
another future so now for this specific

00:09:16,190 --> 00:09:22,190
token we can have one two three four

00:09:18,649 --> 00:09:25,339
five six possible features I will focus

00:09:22,190 --> 00:09:28,190
on the word itself now but remember that

00:09:25,339 --> 00:09:31,040
they are here and they can prove useful

00:09:28,190 --> 00:09:33,140
you can also create syntax parse trees

00:09:31,040 --> 00:09:38,060
from the classical NLP models and they

00:09:33,140 --> 00:09:40,820
can also boost your accuracy when you

00:09:38,060 --> 00:09:44,810
want to represent a word or a bunch of

00:09:40,820 --> 00:09:47,329
words in the way that now our new our

00:09:44,810 --> 00:09:49,880
neural network will understand we can

00:09:47,329 --> 00:09:53,510
use for example the bag of words is the

00:09:49,880 --> 00:09:56,540
simplest possible representation that I

00:09:53,510 --> 00:09:58,820
know first we construct a dictionary

00:09:56,540 --> 00:10:01,160
here it is constructed from the sentence

00:09:58,820 --> 00:10:04,610
a quick brown fox jumps over a lazy dog

00:10:01,160 --> 00:10:07,760
and for each word that occurs in our

00:10:04,610 --> 00:10:10,070
sentence Musa in one and four we eat

00:10:07,760 --> 00:10:13,760
what we do not have in our digital in

00:10:10,070 --> 00:10:16,010
our sandals we put zero and we also use

00:10:13,760 --> 00:10:22,850
only reserved one of the tokens for the

00:10:16,010 --> 00:10:25,519
unknown words and now since we already

00:10:22,850 --> 00:10:28,760
have a representation we can work with

00:10:25,519 --> 00:10:30,290
some network first the most basic

00:10:28,760 --> 00:10:33,769
architecture is fully connected neural

00:10:30,290 --> 00:10:37,279
network we have multiple inputs then we

00:10:33,769 --> 00:10:40,959
have hidden layer or not and then we

00:10:37,279 --> 00:10:43,070
pass our values through it the important

00:10:40,959 --> 00:10:45,140
part here is that everything is

00:10:43,070 --> 00:10:48,290
connected with everything so we will

00:10:45,140 --> 00:10:50,990
have a lot of operations and this is

00:10:48,290 --> 00:10:54,410
example network it's constructed by cars

00:10:50,990 --> 00:10:58,510
we have only one input one hidden layer

00:10:54,410 --> 00:11:02,029
and one output so this is very easy and

00:10:58,510 --> 00:11:03,829
after we train that network what will

00:11:02,029 --> 00:11:06,770
happen with the first layer because it

00:11:03,829 --> 00:11:07,830
is as big as our dictionary here it was

00:11:06,770 --> 00:11:12,480
00:11:07,830 --> 00:11:16,020
words on this IMDB dataset and each row

00:11:12,480 --> 00:11:18,630
will contain now really dense

00:11:16,020 --> 00:11:21,480
representation of a given word as it is

00:11:18,630 --> 00:11:24,480
or as it is very often called embedded

00:11:21,480 --> 00:11:27,810
so our first layer will construct

00:11:24,480 --> 00:11:30,690
embeddings for this four words in our

00:11:27,810 --> 00:11:32,910
dictionary and what we can do with that

00:11:30,690 --> 00:11:35,600
we can use it as a features in different

00:11:32,910 --> 00:11:39,390
models but we also can visualize it I

00:11:35,600 --> 00:11:42,240
reduced dimension i3 from 64 to 2 by

00:11:39,390 --> 00:11:45,900
Disney and we have this beautiful

00:11:42,240 --> 00:11:48,660
scatterplot but what information can we

00:11:45,900 --> 00:11:52,290
extract about it I as it was also stated

00:11:48,660 --> 00:11:54,780
in the first keynote we can look at the

00:11:52,290 --> 00:11:56,910
similarity here I looked at the

00:11:54,780 --> 00:12:00,210
similarity to the word ridiculous and

00:11:56,910 --> 00:12:04,500
the closest words to it are waist boring

00:12:00,210 --> 00:12:06,570
worst wars so our network without even

00:12:04,500 --> 00:12:09,000
knowing the meaning of the words learned

00:12:06,570 --> 00:12:13,140
that they basically mean that the review

00:12:09,000 --> 00:12:15,120
was bad on the other hand if we choose

00:12:13,140 --> 00:12:17,970
the word fantastic the nearest neighbors

00:12:15,120 --> 00:12:19,440
are excellent seven probably like no on

00:12:17,970 --> 00:12:23,960
the numeric scale from zero to ten

00:12:19,440 --> 00:12:26,370
simple eight amazing and so on and if we

00:12:23,960 --> 00:12:31,400
compare where these two I would say

00:12:26,370 --> 00:12:36,210
clusters are they will occur in totally

00:12:31,400 --> 00:12:37,920
opposite points in space on the one side

00:12:36,210 --> 00:12:40,440
we will have the representation for the

00:12:37,920 --> 00:12:42,960
positive sentiment on the other hand for

00:12:40,440 --> 00:12:45,240
the negative so this is nice those

00:12:42,960 --> 00:12:47,130
always to show to your company heavy

00:12:45,240 --> 00:12:49,910
because it looks awesome if use the

00:12:47,130 --> 00:12:52,710
tensor board for visualization you can

00:12:49,910 --> 00:12:55,730
do the testing already dare you do not

00:12:52,710 --> 00:12:58,410
have to do it by or by hand earlier and

00:12:55,730 --> 00:13:01,920
you can move around this interactive

00:12:58,410 --> 00:13:04,050
also 3d graph pros and cons are fully

00:13:01,920 --> 00:13:06,900
connected network with a bag of words as

00:13:04,050 --> 00:13:09,930
your own presentation it's simple so

00:13:06,900 --> 00:13:14,400
it's cheap and fast train one epic took

00:13:09,930 --> 00:13:16,950
about a second or two in collab so it's

00:13:14,400 --> 00:13:20,940
you can iterate on it fast and that's

00:13:16,950 --> 00:13:21,690
also a really good upside because then

00:13:20,940 --> 00:13:24,750
you can convert

00:13:21,690 --> 00:13:27,630
experiments really really quickly we

00:13:24,750 --> 00:13:30,300
always look at the whole text because at

00:13:27,630 --> 00:13:33,120
every word it's kinda interpretable

00:13:30,300 --> 00:13:36,360
because it's so simple that we cannot we

00:13:33,120 --> 00:13:39,690
usually can explain why the given result

00:13:36,360 --> 00:13:41,580
was chosen but the downside is that we

00:13:39,690 --> 00:13:44,790
can get close state-of-the-art

00:13:41,580 --> 00:13:49,410
might be my best result was about 89%

00:13:44,790 --> 00:13:51,480
current best result is 96% and we also

00:13:49,410 --> 00:13:54,120
do not carry the order of the words

00:13:51,480 --> 00:13:59,850
because it's just a bag just a set will

00:13:54,120 --> 00:14:02,910
lose that information so how can we fix

00:13:59,850 --> 00:14:05,340
the thing about the order of words let's

00:14:02,910 --> 00:14:07,320
consider this to reduce I love the

00:14:05,340 --> 00:14:09,090
cinema the movie but cinema was terrible

00:14:07,320 --> 00:14:11,130
I loved the cinema but the movie was

00:14:09,090 --> 00:14:15,030
terrible if we put them together in the

00:14:11,130 --> 00:14:16,680
bag this two sentences becomes the same

00:14:15,030 --> 00:14:20,640
the representation for these two will be

00:14:16,680 --> 00:14:22,110
exactly the same and if we had two

00:14:20,640 --> 00:14:24,090
sentences like this in our training set

00:14:22,110 --> 00:14:26,460
it will be only as half as bad because

00:14:24,090 --> 00:14:28,680
we'll produce I would say undefined

00:14:26,460 --> 00:14:32,310
results for these kind of sentences but

00:14:28,680 --> 00:14:35,460
if we had first one in training and then

00:14:32,310 --> 00:14:37,710
the second one in our production then we

00:14:35,460 --> 00:14:40,410
would conclude that well it is

00:14:37,710 --> 00:14:42,960
definitely a positive review so we have

00:14:40,410 --> 00:14:46,380
to watch for that one of the ways of

00:14:42,960 --> 00:14:48,150
anticipating that is to use a sequence

00:14:46,380 --> 00:14:50,160
of one whole vectors so instead of

00:14:48,150 --> 00:14:55,020
smashing them all into the one we just

00:14:50,160 --> 00:14:57,270
concatenate the sequence and one like

00:14:55,020 --> 00:14:59,790
what in practice if you plan to do it

00:14:57,270 --> 00:15:00,690
with input if you plan to do it that way

00:14:59,790 --> 00:15:03,840
use

00:15:00,690 --> 00:15:05,670
sparse matrices from scikit-learn for

00:15:03,840 --> 00:15:07,860
example because sparse representations

00:15:05,670 --> 00:15:12,150
will be much more made more efficient

00:15:07,860 --> 00:15:13,890
here mm-hm and if we have another

00:15:12,150 --> 00:15:16,920
sentence like a quick brown vixen or

00:15:13,890 --> 00:15:19,770
vixen is it's a female Fox it's not in

00:15:16,920 --> 00:15:22,980
our dictionary we will need to assign

00:15:19,770 --> 00:15:24,780
the unknown tag to it but since every

00:15:22,980 --> 00:15:28,590
single word will go into this unknown

00:15:24,780 --> 00:15:31,140
bag it may be not so good for our

00:15:28,590 --> 00:15:34,350
performance instead of doing that we can

00:15:31,140 --> 00:15:35,580
assign for example either the scene set

00:15:34,350 --> 00:15:38,460
for that word if

00:15:35,580 --> 00:15:40,740
is the dictionary we can try that but we

00:15:38,460 --> 00:15:43,200
can also try assigning the specific

00:15:40,740 --> 00:15:47,070
parts of speed that this given word

00:15:43,200 --> 00:15:50,220
represent and this actually improve my

00:15:47,070 --> 00:15:52,380
results even in especially when I had a

00:15:50,220 --> 00:15:55,920
very small dictionary of 1000 words it

00:15:52,380 --> 00:15:57,690
boosted I think from 86 to 89 percent so

00:15:55,920 --> 00:16:01,140
really good if you want to work with

00:15:57,690 --> 00:16:03,270
small dictionary and I also played a

00:16:01,140 --> 00:16:05,910
little and created model only on the

00:16:03,270 --> 00:16:10,100
part of speeches and I know it doesn't

00:16:05,910 --> 00:16:13,140
make sense but it actually had better

00:16:10,100 --> 00:16:15,740
results than 50 percent so it wasn't

00:16:13,140 --> 00:16:19,890
totally random it's had like 60 I think

00:16:15,740 --> 00:16:22,080
so I think the outcome is that people

00:16:19,890 --> 00:16:24,090
when I are angry and do not like

00:16:22,080 --> 00:16:25,590
something will write in different style

00:16:24,090 --> 00:16:29,730
than people that are happy with

00:16:25,590 --> 00:16:32,160
something and this is example network we

00:16:29,730 --> 00:16:35,850
have our input to the layers and output

00:16:32,160 --> 00:16:38,670
simple another way to approaching well

00:16:35,850 --> 00:16:41,550
since the part of speech is for unknown

00:16:38,670 --> 00:16:44,280
tags worked maybe we could assign them

00:16:41,550 --> 00:16:47,280
to every word maybe it will improve

00:16:44,280 --> 00:16:49,260
something and then we will have even

00:16:47,280 --> 00:16:52,410
bigger dictionary because we will also

00:16:49,260 --> 00:16:55,620
want to introduce include part of

00:16:52,410 --> 00:16:58,980
speeches there but for me it didn't help

00:16:55,620 --> 00:17:01,140
anything but remember that was only my

00:16:58,980 --> 00:17:07,530
case on this I Andy before you it it

00:17:01,140 --> 00:17:10,470
will make improve something another way

00:17:07,530 --> 00:17:12,330
of representing these features like we

00:17:10,470 --> 00:17:15,720
first we had this sparse representation

00:17:12,330 --> 00:17:18,600
of matrices and here we can have a dance

00:17:15,720 --> 00:17:23,220
representation so to each word we assign

00:17:18,600 --> 00:17:27,060
much more much smaller vector but not

00:17:23,220 --> 00:17:30,630
only one hot values but whole range from

00:17:27,060 --> 00:17:32,700
minus one to one joy and also zero it

00:17:30,630 --> 00:17:35,400
will have this vector will have length

00:17:32,700 --> 00:17:38,610
of one because then we have calculated

00:17:35,400 --> 00:17:42,660
this cousin a cosine similarity easier

00:17:38,610 --> 00:17:44,370
and better and here I also created with

00:17:42,660 --> 00:17:46,520
the embeddings for the words and then

00:17:44,370 --> 00:17:50,010
were the parts of speech another model

00:17:46,520 --> 00:17:53,100
why 5060 input

00:17:50,010 --> 00:17:56,220
we have 5,000 words and about 60

00:17:53,100 --> 00:17:59,400
possible part of speeches from the

00:17:56,220 --> 00:18:06,060
spacing and then by thousand because did

00:17:59,400 --> 00:18:07,710
it was how big my review can be also the

00:18:06,060 --> 00:18:13,260
problem is that I had to part those

00:18:07,710 --> 00:18:17,000
sequences to work with those network pad

00:18:13,260 --> 00:18:21,060
so either extended by this token pad or

00:18:17,000 --> 00:18:23,580
cut them if they were told so we will do

00:18:21,060 --> 00:18:26,760
some information pros and cons of fully

00:18:23,580 --> 00:18:28,980
connected networks with sequence it's

00:18:26,760 --> 00:18:31,860
still simple so cheap and fast learn

00:18:28,980 --> 00:18:34,800
still under two seconds order of words

00:18:31,860 --> 00:18:38,010
matter now they are still kinda

00:18:34,800 --> 00:18:41,310
interpretable but we can't get close the

00:18:38,010 --> 00:18:43,260
state-of-the-art 0.96 and words are

00:18:41,310 --> 00:18:45,660
given positioning matter more what did I

00:18:43,260 --> 00:18:48,210
mean by that because of the how neural

00:18:45,660 --> 00:18:50,520
network works if the word bad occurred

00:18:48,210 --> 00:18:53,280
here at the first position and then it

00:18:50,520 --> 00:18:55,200
occurred at the second or third it will

00:18:53,280 --> 00:18:57,450
be treated completely not completely but

00:18:55,200 --> 00:18:59,880
it will be treated differently and that

00:18:57,450 --> 00:19:01,770
also may be a problem and negations are

00:18:59,880 --> 00:19:04,110
effects that hard to cut not impossible

00:19:01,770 --> 00:19:07,260
at heart if you want to learn more about

00:19:04,110 --> 00:19:09,300
basic I think that Andrew ng deep

00:19:07,260 --> 00:19:13,380
learning course is the best way to start

00:19:09,300 --> 00:19:15,870
so you have a look if we have a review

00:19:13,380 --> 00:19:17,940
this movie was not good and we have a

00:19:15,870 --> 00:19:21,000
negation like I mentioned it's hard to

00:19:17,940 --> 00:19:23,190
cut we can use few things to anticipate

00:19:21,000 --> 00:19:25,350
that we can use the tool called war to

00:19:23,190 --> 00:19:29,820
phrase it's from work to vac repository

00:19:25,350 --> 00:19:31,920
and we can also use similar engine same

00:19:29,820 --> 00:19:33,810
but the hub site of the one from the

00:19:31,920 --> 00:19:38,360
word to vac it's written in C and it's

00:19:33,810 --> 00:19:41,820
blazing fast on the few million were

00:19:38,360 --> 00:19:45,120
sentences dataset it works within

00:19:41,820 --> 00:19:46,890
seconds in gains him I wasn't I wasn't

00:19:45,120 --> 00:19:49,620
patient enough to wait for the result

00:19:46,890 --> 00:19:52,500
and what to phrase orgasm will produce

00:19:49,620 --> 00:19:53,760
these sentences so now we have another

00:19:52,500 --> 00:19:55,170
word in our dictionary

00:19:53,760 --> 00:19:57,810
instead of having not in the good

00:19:55,170 --> 00:20:01,080
separately we will have them together it

00:19:57,810 --> 00:20:03,030
works by looking how often this words

00:20:01,080 --> 00:20:08,130
are cured together and

00:20:03,030 --> 00:20:10,140
often they are separated simply okay but

00:20:08,130 --> 00:20:11,910
the other way of anticipating that issue

00:20:10,140 --> 00:20:15,030
is for example to use convolutional

00:20:11,910 --> 00:20:19,140
neural networks CNN's I know that they

00:20:15,030 --> 00:20:21,810
are used usually on the images and today

00:20:19,140 --> 00:20:24,120
I learned that can be also used on audio

00:20:21,810 --> 00:20:27,120
transcriptions on run audio in general

00:20:24,120 --> 00:20:30,090
but we can also apply them to text let's

00:20:27,120 --> 00:20:30,810
think about this review this movie was

00:20:30,090 --> 00:20:34,260
not good

00:20:30,810 --> 00:20:36,810
what CNN's will do well will have this

00:20:34,260 --> 00:20:38,940
sliding window of one let's inner on

00:20:36,810 --> 00:20:41,640
that will first conclude the

00:20:38,940 --> 00:20:44,880
representation of the words inside that

00:20:41,640 --> 00:20:47,850
window so for this example I choose the

00:20:44,880 --> 00:20:50,190
window of two words will have this

00:20:47,850 --> 00:20:54,180
matrix that will will multiply our

00:20:50,190 --> 00:20:56,580
representation of the disc and movie and

00:20:54,180 --> 00:21:00,450
will create the representation of this

00:20:56,580 --> 00:21:02,340
movie together then for did movie was I

00:21:00,450 --> 00:21:04,290
will have representation for those two

00:21:02,340 --> 00:21:06,180
words and so on and so on we have

00:21:04,290 --> 00:21:09,060
everything and this part of the

00:21:06,180 --> 00:21:12,570
operation is called convolution and then

00:21:09,060 --> 00:21:15,780
we do the operation called pulling so we

00:21:12,570 --> 00:21:21,440
reduce dimensionality and pulling does

00:21:15,780 --> 00:21:25,020
not go in sliding window it goes into it

00:21:21,440 --> 00:21:27,630
goes over the multiple representations

00:21:25,020 --> 00:21:30,990
and reduce the emotionality I think

00:21:27,630 --> 00:21:33,240
that's better to show basically then we

00:21:30,990 --> 00:21:35,730
have another presentation of the

00:21:33,240 --> 00:21:39,480
Triplets of words and on top of that we

00:21:35,730 --> 00:21:42,840
will use standard fully connected

00:21:39,480 --> 00:21:46,110
Network you can use either max polling

00:21:42,840 --> 00:21:50,220
so in each dimension we will pick one

00:21:46,110 --> 00:21:52,050
maximum value or average pooling so in

00:21:50,220 --> 00:21:55,080
each dimension we will calculate the

00:21:52,050 --> 00:21:57,900
average of the value in vector I've

00:21:55,080 --> 00:22:00,960
heard that for text it's usually better

00:21:57,900 --> 00:22:03,180
to use max polling but I think it's

00:22:00,960 --> 00:22:04,170
always best to try both and find out

00:22:03,180 --> 00:22:07,230
what works for you

00:22:04,170 --> 00:22:10,770
it's as it's an just another hyper

00:22:07,230 --> 00:22:14,100
parameter for the convolution you have

00:22:10,770 --> 00:22:16,070
the window size how big it is and also

00:22:14,100 --> 00:22:18,680
the stride size so

00:22:16,070 --> 00:22:20,780
in wars you all done if here the stride

00:22:18,680 --> 00:22:25,910
size was one and the window size was

00:22:20,780 --> 00:22:28,880
also was two so we move our window size

00:22:25,910 --> 00:22:32,000
to every every one word if we had the

00:22:28,880 --> 00:22:33,850
stride of size two then we would only

00:22:32,000 --> 00:22:39,200
have our presentations for this movie

00:22:33,850 --> 00:22:39,680
then was not and it may work with bigger

00:22:39,200 --> 00:22:43,370
strikes

00:22:39,680 --> 00:22:44,870
especially when you have long text so if

00:22:43,370 --> 00:22:48,050
you want to work on whole paragraphs

00:22:44,870 --> 00:22:49,580
please consider that and that's the

00:22:48,050 --> 00:22:53,030
simple architecture and that's another

00:22:49,580 --> 00:22:55,430
architecture I came up with pros and

00:22:53,030 --> 00:22:58,880
cons of CNN's they paralyzed nicely and

00:22:55,430 --> 00:23:00,770
have fewer material parameters than the

00:22:58,880 --> 00:23:02,500
fully connected neural networks usually

00:23:00,770 --> 00:23:04,430
of course it depends how you build stuff

00:23:02,500 --> 00:23:07,250
order of words matter

00:23:04,430 --> 00:23:11,210
finally position of words also matter

00:23:07,250 --> 00:23:12,890
and if we want we can create a network

00:23:11,210 --> 00:23:15,770
that will look at the whole sentence

00:23:12,890 --> 00:23:18,980
it's not so easy in practice but in

00:23:15,770 --> 00:23:20,630
theory we can and if you want to learn

00:23:18,980 --> 00:23:22,460
more about the stuff another further

00:23:20,630 --> 00:23:24,860
reading understanding convolutional

00:23:22,460 --> 00:23:25,940
neural networks foreigner P I also

00:23:24,860 --> 00:23:30,340
recommend reading that

00:23:25,940 --> 00:23:30,340
so now recurrent neural networks

00:23:33,250 --> 00:23:39,190
remember how in CNN's we had a

00:23:35,810 --> 00:23:42,430
presentation constructed of the two

00:23:39,190 --> 00:23:48,910
words together now we'll have a

00:23:42,430 --> 00:23:51,980
representation for the word this then we

00:23:48,910 --> 00:23:54,380
move to the next word we create a

00:23:51,980 --> 00:23:56,360
representation for the word movie but we

00:23:54,380 --> 00:23:59,870
also take the previous word into the

00:23:56,360 --> 00:24:02,240
context then we create a representation

00:23:59,870 --> 00:24:05,990
for the world was but we also take into

00:24:02,240 --> 00:24:11,330
account the previous words and we always

00:24:05,990 --> 00:24:13,520
use the same matrix of weights and um

00:24:11,330 --> 00:24:16,390
when we get to the end we'll have a

00:24:13,520 --> 00:24:19,910
presentation of the whole sentence and

00:24:16,390 --> 00:24:22,130
this is really nice because in theory we

00:24:19,910 --> 00:24:24,890
cut the whole sentence we know what is

00:24:22,130 --> 00:24:28,670
there and we can work on that but the

00:24:24,890 --> 00:24:30,230
problem here is we usually

00:24:28,670 --> 00:24:32,030
we can have problems with vanishing

00:24:30,230 --> 00:24:34,850
credits and I will talk about that in a

00:24:32,030 --> 00:24:37,730
second and for you connected on top and

00:24:34,850 --> 00:24:39,650
now we can create a prediction we can

00:24:37,730 --> 00:24:45,020
also stack those layers like a girl

00:24:39,650 --> 00:24:46,520
Network and one we have a send a review

00:24:45,020 --> 00:24:49,010
like terrible I loved her previous

00:24:46,520 --> 00:24:51,800
movies the word terrible that indicates

00:24:49,010 --> 00:24:54,830
sentiment is at the beginning and went

00:24:51,800 --> 00:24:56,750
it will get to the end our resolution

00:24:54,830 --> 00:24:59,060
will have to go through very deep

00:24:56,750 --> 00:25:01,520
network and we all have to deal with

00:24:59,060 --> 00:25:03,830
problems like vanishing gradient and it

00:25:01,520 --> 00:25:05,560
will totally vanish and we lost the

00:25:03,830 --> 00:25:08,660
meaning of the word terrible or

00:25:05,560 --> 00:25:10,880
exploding because we are always

00:25:08,660 --> 00:25:13,600
multiplying by the same matrices of

00:25:10,880 --> 00:25:15,680
weights to anticipate that we can use

00:25:13,600 --> 00:25:17,420
bi-directional or enhance so first we'll

00:25:15,680 --> 00:25:20,000
go front to back then but in back to

00:25:17,420 --> 00:25:22,250
front merge in some way the results

00:25:20,000 --> 00:25:24,680
either concatenate some whatever and

00:25:22,250 --> 00:25:25,660
fully connected and work on top it

00:25:24,680 --> 00:25:28,010
should work

00:25:25,660 --> 00:25:30,680
so proms and cons can give better

00:25:28,010 --> 00:25:33,590
results look at the whole sentence but

00:25:30,680 --> 00:25:36,920
they are hard to train because as you

00:25:33,590 --> 00:25:41,480
may see the network will be as big as

00:25:36,920 --> 00:25:43,610
your sentence is or hollow review so we

00:25:41,480 --> 00:25:46,700
will have to deal with training very

00:25:43,610 --> 00:25:49,460
deep networks and this is really Islam

00:25:46,700 --> 00:25:51,830
if you want to learn more about our

00:25:49,460 --> 00:25:54,620
enhance I put a link to the Stanford

00:25:51,830 --> 00:25:56,480
lecture about it hmm

00:25:54,620 --> 00:25:59,210
so how to what is another way to

00:25:56,480 --> 00:26:02,900
anticipate that forgetting we can use

00:25:59,210 --> 00:26:05,360
Alice TMS or gr use and unfortunately I

00:26:02,900 --> 00:26:08,320
want to get into the detail here because

00:26:05,360 --> 00:26:11,690
well the architecture of the neurons is

00:26:08,320 --> 00:26:15,530
very complex this is all STM we passed

00:26:11,690 --> 00:26:17,870
the state and we all not only pass the

00:26:15,530 --> 00:26:21,050
representation but we also pass there at

00:26:17,870 --> 00:26:24,080
same state of the cell and these are di

00:26:21,050 --> 00:26:28,160
use gr use these are a little simpler

00:26:24,080 --> 00:26:31,100
but the important thing is we do not

00:26:28,160 --> 00:26:34,580
only carry then representation for what

00:26:31,100 --> 00:26:38,120
happened in the past in the vector but

00:26:34,580 --> 00:26:40,220
we also will carry a state and since

00:26:38,120 --> 00:26:42,230
it's more complex than we have more

00:26:40,220 --> 00:26:45,760
operation engage that remember of

00:26:42,230 --> 00:26:50,330
forget things we won't be necessarily

00:26:45,760 --> 00:26:52,910
forgetting stuff so since it's not

00:26:50,330 --> 00:26:55,280
always the simple matrix multiplication

00:26:52,910 --> 00:26:58,250
act like it was in our hands we will

00:26:55,280 --> 00:27:01,700
contain the info about the words at the

00:26:58,250 --> 00:27:04,340
beginning but again as you may see the

00:27:01,700 --> 00:27:07,040
design is pretty complex so when we are

00:27:04,340 --> 00:27:09,530
training network we have to do a lot of

00:27:07,040 --> 00:27:12,470
operations a lot of back propagation so

00:27:09,530 --> 00:27:15,470
it will take time but they can give best

00:27:12,470 --> 00:27:17,690
results and until the transformer came

00:27:15,470 --> 00:27:21,410
up they were most of the time state of

00:27:17,690 --> 00:27:23,210
the art and we can create even in Paris

00:27:21,410 --> 00:27:26,299
the architecture that will look at the

00:27:23,210 --> 00:27:30,650
whole sentences it's hard but it's

00:27:26,299 --> 00:27:33,260
possible and here another lecture from

00:27:30,650 --> 00:27:36,919
Stanford and link to a blog post about

00:27:33,260 --> 00:27:39,610
understanding alas TM networks now going

00:27:36,919 --> 00:27:42,860
back to the result of my experiments

00:27:39,610 --> 00:27:45,010
they weren't really successful so fully

00:27:42,860 --> 00:27:50,390
connected network with a bag of words

00:27:45,010 --> 00:27:54,440
achieved 0.89 accuracy why lsdm were

00:27:50,390 --> 00:28:00,320
really near it was 88% but the training

00:27:54,440 --> 00:28:02,720
time was like 60 times higher so it's

00:28:00,320 --> 00:28:05,900
not always worth in to to throw yourself

00:28:02,720 --> 00:28:08,390
into the most complex architecture at

00:28:05,900 --> 00:28:10,340
the beginning I think that it's always

00:28:08,390 --> 00:28:13,040
best to start with something simple and

00:28:10,340 --> 00:28:15,410
then iterate and compare with that

00:28:13,040 --> 00:28:17,900
because with the simpler architectures

00:28:15,410 --> 00:28:22,070
you will also gain the quick inference

00:28:17,900 --> 00:28:24,260
time which can be really useful and if

00:28:22,070 --> 00:28:26,090
you are i barely scratched the say first

00:28:24,260 --> 00:28:28,640
surface here and if you are interested

00:28:26,090 --> 00:28:31,790
in the machine learning in context of an

00:28:28,640 --> 00:28:34,520
LP I highly recommend that book it's I

00:28:31,790 --> 00:28:37,640
think not only the one to read but also

00:28:34,520 --> 00:28:40,880
the one if you want to work in that to

00:28:37,640 --> 00:28:43,790
have because I find myself often going

00:28:40,880 --> 00:28:46,190
back to reading specific parts because

00:28:43,790 --> 00:28:49,460
for example if you know how the size of

00:28:46,190 --> 00:28:52,429
the window in work to vac works with the

00:28:49,460 --> 00:28:55,850
produced embeddings you can find it

00:28:52,429 --> 00:28:57,570
there so that will be all

00:28:55,850 --> 00:29:05,329
and thank you

00:28:57,570 --> 00:29:05,329

YouTube URL: https://www.youtube.com/watch?v=F2LajzvSX18


