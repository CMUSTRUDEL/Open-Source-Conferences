Title: Teresa Ingram - Opt Out of Online Sexism – Open Source Activism
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"Opt Out of Online Sexism – Open Source Activism
[EuroPython 2019 - Talk - 2019-07-10 - PyCharm]
[Basel, CH]

By Teresa Ingram

""Although people of all genders can experience violence and abuse online, the abuse experienced by women is often sexist or misogynistic in nature, and online threats of violence against women are often sexualized and include specific references to women’s bodies. "" - Amnesty International. This abuse pushes women offline, affecting their social well-being, representation and economic potential. 

In this talk I will discuss how we plan to help resolve this with our browser extension, Opt Out. I will discuss the online global tragedy that is online sexual harassment, our idea and where we’re at with current implementation. I will also talk about what it’s like to build an open source activism project, one which aims to be lead by the community it’s trying to protect. 

We will cover current research and results from our own engagement with the community, where the idea came from and challenges we have faced and plan to face in the future. I will also dive into the intricate world of natural language processing (NLP) for online harassment and talk about balancing state-of-the-art data science with web development in an open source community, one being managed by someone relatively new to tech.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:02,760 --> 00:00:07,820
so good afternoon my name is Teresa

00:00:05,190 --> 00:00:10,980
Ingram and I am founder of the nonprofit

00:00:07,820 --> 00:00:12,719
opt-out that aims to build tools to help

00:00:10,980 --> 00:00:15,740
women and female identifying people

00:00:12,719 --> 00:00:18,119
re-engage with healthy online debate and

00:00:15,740 --> 00:00:20,279
just a warning before I begin this talk

00:00:18,119 --> 00:00:24,150
there will be some extreme language so

00:00:20,279 --> 00:00:27,449
just just FYI and so before I tell you

00:00:24,150 --> 00:00:32,550
about about opt-out I would like to tell

00:00:27,449 --> 00:00:34,980
you first about why we need to exist so

00:00:32,550 --> 00:00:38,910
Diane Abbott number two of the Labour

00:00:34,980 --> 00:00:40,620
Party in Britain during the 2017 general

00:00:38,910 --> 00:00:43,199
election amnesty as Amnesty

00:00:40,620 --> 00:00:45,899
International did a study on all of the

00:00:43,199 --> 00:00:48,600
tweets that all MPs received all those

00:00:45,899 --> 00:00:53,699
classified as hateful Diane Abbott

00:00:48,600 --> 00:00:56,760
received 45.1% of them Larry penny a

00:00:53,699 --> 00:00:59,839
journalist writer activist often writing

00:00:56,760 --> 00:01:03,019
about feminist issues by the age of 26

00:00:59,839 --> 00:01:06,720
she received her first real bomb threat

00:01:03,019 --> 00:01:08,730
and Johanna Schmidt Nielsen former

00:01:06,720 --> 00:01:12,090
leader of the Danish left-wing political

00:01:08,730 --> 00:01:14,940
party in listen alongside the usual

00:01:12,090 --> 00:01:16,200
barrage of threats against her body and

00:01:14,940 --> 00:01:19,980
harassment

00:01:16,200 --> 00:01:22,650
it was once reported by a troll that she

00:01:19,980 --> 00:01:24,000
was in fact dead now you might be

00:01:22,650 --> 00:01:27,420
sitting there thinking okay great

00:01:24,000 --> 00:01:30,270
Theresa but men and mail identifying

00:01:27,420 --> 00:01:31,290
people also suffer abuse online and you

00:01:30,270 --> 00:01:34,590
know these women that you're describing

00:01:31,290 --> 00:01:37,280
they are public personas so maybe they

00:01:34,590 --> 00:01:39,750
should be expecting something like this

00:01:37,280 --> 00:01:42,480
but really the stats speak for

00:01:39,750 --> 00:01:45,300
themselves it's not just public facing

00:01:42,480 --> 00:01:47,280
women that suffer this women are twice

00:01:45,300 --> 00:01:50,570
as likely to be sexually harassed online

00:01:47,280 --> 00:01:53,520
as men mainly affecting our young women

00:01:50,570 --> 00:01:56,760
90% of all victims of revenge porn are

00:01:53,520 --> 00:01:57,310
female and women are twice as likely to

00:01:56,760 --> 00:01:59,950
suffer

00:01:57,310 --> 00:02:03,610
first consequences as a result of this

00:01:59,950 --> 00:02:05,909
online abuse women and female

00:02:03,610 --> 00:02:09,099
identifying people disproportionately

00:02:05,909 --> 00:02:11,860
suffer online abuse particularly those

00:02:09,099 --> 00:02:15,550
who challenge the status quo online

00:02:11,860 --> 00:02:18,310
sexism is real and it's silencing the

00:02:15,550 --> 00:02:23,769
voices that society so desperately needs

00:02:18,310 --> 00:02:25,959
to hear so an opt-out we're aiming to

00:02:23,769 --> 00:02:28,720
put a stop to the silencing nature of

00:02:25,959 --> 00:02:30,640
online sexism we're building tools to

00:02:28,720 --> 00:02:33,280
help all female identifying people

00:02:30,640 --> 00:02:34,959
who've got something to say get back to

00:02:33,280 --> 00:02:37,569
the online spaces they've been chased

00:02:34,959 --> 00:02:40,660
out from we're doing this by not only

00:02:37,569 --> 00:02:46,120
building tools but by also building a

00:02:40,660 --> 00:02:48,370
movement by holding workshops that give

00:02:46,120 --> 00:02:49,510
female identifying people a chance to

00:02:48,370 --> 00:02:52,599
come together and share their

00:02:49,510 --> 00:02:54,849
experiences we're not all only building

00:02:52,599 --> 00:02:56,549
the important vital social

00:02:54,849 --> 00:02:59,019
infrastructure that these people need

00:02:56,549 --> 00:03:02,370
but also allowing them to come together

00:02:59,019 --> 00:03:05,260
and in doing so act in a form of protest

00:03:02,370 --> 00:03:07,060
by helping to form this community we are

00:03:05,260 --> 00:03:09,640
able to spot needed technical

00:03:07,060 --> 00:03:12,209
infrastructure and make sure that the

00:03:09,640 --> 00:03:15,130
tools that we build are fit for purpose

00:03:12,209 --> 00:03:21,040
ensuring that our tech is as community

00:03:15,130 --> 00:03:24,489
driven as possible in addition to our

00:03:21,040 --> 00:03:27,239
workshops we aim to build a website that

00:03:24,489 --> 00:03:29,980
supports women getting their voices back

00:03:27,239 --> 00:03:32,650
inspired by harass map an Egyptian based

00:03:29,980 --> 00:03:35,440
NGO that you can see here where an

00:03:32,650 --> 00:03:38,170
individual can submit reports of

00:03:35,440 --> 00:03:41,769
individual physical harassment which

00:03:38,170 --> 00:03:43,600
then gets displayed to an online map we

00:03:41,769 --> 00:03:46,120
will build our website to allow so many

00:03:43,600 --> 00:03:49,329
to anonymously submit their experiences

00:03:46,120 --> 00:03:52,290
this data will be stored studied and

00:03:49,329 --> 00:03:55,930
feed the models that our tools depend on

00:03:52,290 --> 00:03:58,030
our website will also transparently show

00:03:55,930 --> 00:04:01,000
details of what we're doing with the

00:03:58,030 --> 00:04:02,889
data and also the impact that our tools

00:04:01,000 --> 00:04:07,569
are having for women across the world

00:04:02,889 --> 00:04:09,340
hoping to fuel the movement our

00:04:07,569 --> 00:04:12,190
long-term goal is

00:04:09,340 --> 00:04:14,560
to have a virtual harassment which shows

00:04:12,190 --> 00:04:18,519
which communities on your chosen social

00:04:14,560 --> 00:04:20,970
media platform are sexist sexually

00:04:18,519 --> 00:04:23,710
aggressive or just downright nasty

00:04:20,970 --> 00:04:26,250
enabling women and female identifying

00:04:23,710 --> 00:04:30,900
people to navigate the murky waters of

00:04:26,250 --> 00:04:30,900
online society as best they can

00:04:30,990 --> 00:04:37,180
so the opt-out ethos the general data

00:04:35,110 --> 00:04:38,889
protection regulation or gdpr has

00:04:37,180 --> 00:04:40,419
changed our lives on social media

00:04:38,889 --> 00:04:43,270
platforms we have the right to be

00:04:40,419 --> 00:04:45,820
forgotten to dictate what has been

00:04:43,270 --> 00:04:48,430
recorded about us and to opt out if we

00:04:45,820 --> 00:04:51,130
wish but the abuse of women and female

00:04:48,430 --> 00:04:54,610
identifying people suffer online is not

00:04:51,130 --> 00:04:58,240
avoidable we see opt out as an extension

00:04:54,610 --> 00:05:01,450
of the gdpr that also protects the human

00:04:58,240 --> 00:05:05,850
rights of these people online allowing

00:05:01,450 --> 00:05:08,440
them to join in online debate once more

00:05:05,850 --> 00:05:10,750
so what tools are we talking about

00:05:08,440 --> 00:05:13,840
alongside the website and the workshops

00:05:10,750 --> 00:05:16,450
our main idea is a browser extension

00:05:13,840 --> 00:05:18,580
that filters out online sexism from an

00:05:16,450 --> 00:05:20,770
individual social media feed and it does

00:05:18,580 --> 00:05:23,440
so by a sentiment classification

00:05:20,770 --> 00:05:25,120
sentiment analyzer as you can see here

00:05:23,440 --> 00:05:27,099
apologies the video is not brilliant you

00:05:25,120 --> 00:05:27,639
can't see the button but you get the

00:05:27,099 --> 00:05:29,590
picture

00:05:27,639 --> 00:05:32,590
and so currently our tool works on

00:05:29,590 --> 00:05:34,539
twitter and we've got a very very simple

00:05:32,590 --> 00:05:38,260
neural net behind it that's trained on

00:05:34,539 --> 00:05:43,419
10,000 of your normal troll tweets and

00:05:38,260 --> 00:05:46,440
but nothing sexism specific our plan

00:05:43,419 --> 00:05:50,050
going forward is to retune this model

00:05:46,440 --> 00:05:54,370
with a sexism labelled data set from

00:05:50,050 --> 00:05:56,560
zero cuisine and his coworkers and once

00:05:54,370 --> 00:05:58,830
this is done and our website is up and

00:05:56,560 --> 00:06:01,450
running and the word has been spread

00:05:58,830 --> 00:06:05,919
hopefully we will start generating a

00:06:01,450 --> 00:06:08,740
larger sexism data set but we're going

00:06:05,919 --> 00:06:11,080
to need to annotate this data set and we

00:06:08,740 --> 00:06:12,270
are proposing to do so with the two

00:06:11,080 --> 00:06:16,740
round annotation

00:06:12,270 --> 00:06:18,870
scheme taking inspiration from Zurich

00:06:16,740 --> 00:06:21,810
once again and his coworkers we're going

00:06:18,870 --> 00:06:24,300
to first label based on the categories

00:06:21,810 --> 00:06:26,940
generalized directed explicit and

00:06:24,300 --> 00:06:28,380
implicit so here are some examples of

00:06:26,940 --> 00:06:31,320
what that actually looks like in terms

00:06:28,380 --> 00:06:34,440
of language generalized all students are

00:06:31,320 --> 00:06:36,420
lazy directed you are a lazy student

00:06:34,440 --> 00:06:39,270
which we may already have heard in our

00:06:36,420 --> 00:06:41,460
lives explicit the candidate did not

00:06:39,270 --> 00:06:43,820
write enough papers and implicit the

00:06:41,460 --> 00:06:47,790
candidate was not an innovative

00:06:43,820 --> 00:06:51,030
researcher but language is nuanced and

00:06:47,790 --> 00:06:55,530
complicated and it can be combinations

00:06:51,030 --> 00:06:58,370
of all of these and also sexist so for

00:06:55,530 --> 00:07:01,110
example the first comment there is both

00:06:58,370 --> 00:07:04,200
generalized using the bridezilla word

00:07:01,110 --> 00:07:06,840
and then also directed because it's

00:07:04,200 --> 00:07:10,020
directed at somebody and yeah similar

00:07:06,840 --> 00:07:11,490
sort of thing for the second it's

00:07:10,020 --> 00:07:13,260
important though even though this is

00:07:11,490 --> 00:07:17,550
going to be challenge that we identify

00:07:13,260 --> 00:07:20,820
what is explicitly a gret as explicitly

00:07:17,550 --> 00:07:22,980
sexist first because if we are to

00:07:20,820 --> 00:07:25,820
encourage respectful debate and avoid

00:07:22,980 --> 00:07:30,060
creating any unintended echo chambers or

00:07:25,820 --> 00:07:31,860
biases with our tool we need to get rid

00:07:30,060 --> 00:07:35,460
of the really obvious stuff first and

00:07:31,860 --> 00:07:38,370
then understand the implicit implied

00:07:35,460 --> 00:07:40,920
sexism later so once we've done this

00:07:38,370 --> 00:07:45,450
initial rounds of annotations we're then

00:07:40,920 --> 00:07:48,270
going to further classify the comments

00:07:45,450 --> 00:07:51,570
based on five different labels taken

00:07:48,270 --> 00:07:55,650
from Maria and savino's and misogyny

00:07:51,570 --> 00:08:00,150
labels what we have here underneath the

00:07:55,650 --> 00:08:03,330
different labels our tweet examples so

00:08:00,150 --> 00:08:07,170
discredit slurring over women with no

00:08:03,330 --> 00:08:09,270
larger other intention stereotyped and

00:08:07,170 --> 00:08:12,360
objectification to make women

00:08:09,270 --> 00:08:15,030
subordinate or description of a woman's

00:08:12,360 --> 00:08:15,390
physical appearance and /or comparisons

00:08:15,030 --> 00:08:18,720
to

00:08:15,390 --> 00:08:21,060
era standards and then dominance to

00:08:18,720 --> 00:08:22,890
preserve male control protect male

00:08:21,060 --> 00:08:28,500
interests and to exclude women from

00:08:22,890 --> 00:08:30,630
conversation sexual harassment and

00:08:28,500 --> 00:08:33,030
threats of violence to physically assert

00:08:30,630 --> 00:08:35,270
power over women or to intimidate and

00:08:33,030 --> 00:08:38,820
silence women through threats and

00:08:35,270 --> 00:08:40,710
derailing to justify abuse reject male

00:08:38,820 --> 00:08:43,670
responsibility and attempt to disrupt

00:08:40,710 --> 00:08:47,070
the conversation in order to refocus it

00:08:43,670 --> 00:08:50,070
with this two-level annotation scheme we

00:08:47,070 --> 00:08:52,530
will be able we hope we will be able to

00:08:50,070 --> 00:08:57,510
identify the different phases of online

00:08:52,530 --> 00:08:59,460
sexism so in addition to this data

00:08:57,510 --> 00:09:01,350
annotation and understanding we're going

00:08:59,460 --> 00:09:04,530
to deploy the what I call the three C's

00:09:01,350 --> 00:09:07,610
approach so content which is what I've

00:09:04,530 --> 00:09:11,010
already previously discussed context and

00:09:07,610 --> 00:09:12,270
conversation so content will be using

00:09:11,010 --> 00:09:15,180
the sentiment analyzer with the

00:09:12,270 --> 00:09:19,950
labelings that just talks about context

00:09:15,180 --> 00:09:22,560
so who is the abuser in relation to the

00:09:19,950 --> 00:09:25,830
target are they part of a bigger mob

00:09:22,560 --> 00:09:29,010
attack and this is important to know and

00:09:25,830 --> 00:09:30,660
then conversation has the sentiment of

00:09:29,010 --> 00:09:34,050
the conversation between the two taking

00:09:30,660 --> 00:09:35,700
a sustained nosedive this could be an

00:09:34,050 --> 00:09:37,500
indication of intimate partner violence

00:09:35,700 --> 00:09:41,370
and requires a very different solution

00:09:37,500 --> 00:09:43,920
to what we're offering with these labels

00:09:41,370 --> 00:09:45,750
and a better understanding of the

00:09:43,920 --> 00:09:48,330
behaviors and relationships of online

00:09:45,750 --> 00:09:50,910
sexism will be better informed to answer

00:09:48,330 --> 00:09:53,100
the age-old question of you know it when

00:09:50,910 --> 00:09:57,180
you see it which is characteristic of

00:09:53,100 --> 00:09:59,310
online sexism and so once this is all

00:09:57,180 --> 00:10:01,530
done we can start to build and test

00:09:59,310 --> 00:10:03,150
different models and really start to

00:10:01,530 --> 00:10:06,350
make a difference to women and female

00:10:03,150 --> 00:10:09,390
identifying people all over the globe

00:10:06,350 --> 00:10:11,460
but what's the coolest thing which I

00:10:09,390 --> 00:10:15,420
really really like about our tool is

00:10:11,460 --> 00:10:17,640
that we our consent focused meaning that

00:10:15,420 --> 00:10:22,260
we aim to block what an individual finds

00:10:17,640 --> 00:10:24,000
distressing and not what we think we're

00:10:22,260 --> 00:10:27,510
doing this by deploying a technique that

00:10:24,000 --> 00:10:29,220
I call big sister instead of Big Brother

00:10:27,510 --> 00:10:32,279
where there will be a local instance of

00:10:29,220 --> 00:10:34,019
the model in somebody's browser that

00:10:32,279 --> 00:10:36,990
they can supply feedback to with the

00:10:34,019 --> 00:10:39,029
simple click of a button the data stays

00:10:36,990 --> 00:10:42,990
locally and that people will be

00:10:39,029 --> 00:10:47,370
encouraged to share their labelings with

00:10:42,990 --> 00:10:49,920
opt-out via the website by focusing on

00:10:47,370 --> 00:10:51,930
individual consent and not a one model

00:10:49,920 --> 00:10:54,300
fits all approach we ensure that the

00:10:51,930 --> 00:10:57,750
diverse range of online interactions are

00:10:54,300 --> 00:11:02,570
not stifled but that productive and

00:10:57,750 --> 00:11:02,570
respectful interactions can flourish

00:11:06,260 --> 00:11:10,769
enable female enabling female

00:11:08,850 --> 00:11:13,230
identifying people to join healthy

00:11:10,769 --> 00:11:15,889
debate it's only possible if we also

00:11:13,230 --> 00:11:18,959
ensure that these people are safe online

00:11:15,889 --> 00:11:20,790
we plan to do this by utilizing the

00:11:18,959 --> 00:11:24,269
moderators that most social media

00:11:20,790 --> 00:11:26,459
platforms have effectively whenever our

00:11:24,269 --> 00:11:29,550
sentiment analyzers detect abuse the

00:11:26,459 --> 00:11:31,889
comments will go automatically to the

00:11:29,550 --> 00:11:34,949
moderators with a traffic light labeling

00:11:31,889 --> 00:11:36,810
scheme allowing them to prioritize more

00:11:34,949 --> 00:11:40,880
effectively what tweets or what comments

00:11:36,810 --> 00:11:43,529
sorry need attention immediately

00:11:40,880 --> 00:11:48,660
this ensures that the user safety is

00:11:43,529 --> 00:11:50,220
never compromised so once this is all

00:11:48,660 --> 00:11:52,560
said and done in a years down the line

00:11:50,220 --> 00:11:56,010
well we've got a great little NGO behind

00:11:52,560 --> 00:11:58,740
us and we're going to develop the

00:11:56,010 --> 00:11:59,910
browser extension we're going to be able

00:11:58,740 --> 00:12:02,510
to have we're going to have a

00:11:59,910 --> 00:12:06,510
functionality that allows people to just

00:12:02,510 --> 00:12:08,190
use a blacklist of account and so these

00:12:06,510 --> 00:12:11,610
people are automatically blocked from an

00:12:08,190 --> 00:12:14,339
individual's social media and these will

00:12:11,610 --> 00:12:16,910
be maintained and shared by what we call

00:12:14,339 --> 00:12:20,220
digit on T's which are groups of people

00:12:16,910 --> 00:12:22,529
that are seeded from the workshops that

00:12:20,220 --> 00:12:25,769
we're going to be holding and that also

00:12:22,529 --> 00:12:28,190
act like a support network for anybody

00:12:25,769 --> 00:12:30,980
who has suffered online

00:12:28,190 --> 00:12:32,750
ism we then have the automatic

00:12:30,980 --> 00:12:35,390
replacement of comments like I just

00:12:32,750 --> 00:12:37,340
described and then finally a sentiment

00:12:35,390 --> 00:12:39,950
dashboard that pops up before the page

00:12:37,340 --> 00:12:43,010
loads with a traffic light labeling

00:12:39,950 --> 00:12:44,990
scheme for each comment allowing the

00:12:43,010 --> 00:12:50,180
user to preemptively decide what they do

00:12:44,990 --> 00:12:53,030
and they don't want to see so we've got

00:12:50,180 --> 00:12:57,260
a lot to do as you can see and that

00:12:53,030 --> 00:12:59,840
we're planning to get working product by

00:12:57,260 --> 00:13:02,000
the end of August and they're not be so

00:12:59,840 --> 00:13:03,800
popular we'll get a huge data set

00:13:02,000 --> 00:13:05,210
straightaway and then we can start

00:13:03,800 --> 00:13:07,610
playing with that in September by the

00:13:05,210 --> 00:13:10,040
end of September and then what's really

00:13:07,610 --> 00:13:15,110
important is that we move across to

00:13:10,040 --> 00:13:18,140
different languages we're going to

00:13:15,110 --> 00:13:20,210
design the web app so all you'll need to

00:13:18,140 --> 00:13:22,490
do is change the data set and maybe some

00:13:20,210 --> 00:13:24,230
hyper parameters and you can change the

00:13:22,490 --> 00:13:28,250
language from English to Spanish to

00:13:24,230 --> 00:13:30,520
romanian to whatever you'd like and this

00:13:28,250 --> 00:13:33,680
will enable us to build the community

00:13:30,520 --> 00:13:38,110
that we want behind it because online

00:13:33,680 --> 00:13:41,180
sexism is not restricted just to English

00:13:38,110 --> 00:13:43,970
so with a topic like this I think it's

00:13:41,180 --> 00:13:48,740
really important for me to tell you all

00:13:43,970 --> 00:13:50,090
who's behind it and we're a bunch of

00:13:48,740 --> 00:13:50,750
volunteers at the moment apart from

00:13:50,090 --> 00:13:52,790
myself

00:13:50,750 --> 00:13:54,020
and while I'm working out of savings but

00:13:52,790 --> 00:13:57,140
most people are just working in their

00:13:54,020 --> 00:14:00,650
free time and we are a group of people

00:13:57,140 --> 00:14:02,780
from social scientists to data nerds but

00:14:00,650 --> 00:14:03,550
there is one characteristic that we all

00:14:02,780 --> 00:14:09,800
share

00:14:03,550 --> 00:14:11,600
we won't let hate win our vision we want

00:14:09,800 --> 00:14:14,240
to champion women back into the online

00:14:11,600 --> 00:14:16,280
worlds have been chased out from support

00:14:14,240 --> 00:14:18,830
them and their voices while still

00:14:16,280 --> 00:14:24,440
protecting them and holding perpetrators

00:14:18,830 --> 00:14:26,060
accountable we need to exist if you

00:14:24,440 --> 00:14:29,240
share our vision if you believe in the

00:14:26,060 --> 00:14:31,280
cause I ask you to join us even if it's

00:14:29,240 --> 00:14:34,130
just by talking to so many about the

00:14:31,280 --> 00:14:37,200
issue about what I've discussed today

00:14:34,130 --> 00:14:42,420
mentoring Kok contributions go onto the

00:14:37,200 --> 00:14:44,550
github stauros all this stuff I'm a

00:14:42,420 --> 00:14:46,980
relative novice I've got about a year

00:14:44,550 --> 00:14:49,140
and a half worth of software engineering

00:14:46,980 --> 00:14:51,720
experience but the community has rallied

00:14:49,140 --> 00:14:54,390
behind me an incredible amount and this

00:14:51,720 --> 00:14:56,010
ship is sailing so if you'd like to get

00:14:54,390 --> 00:15:02,900
on board just let me know

00:14:56,010 --> 00:15:02,900
online sexism has to stop let's opt-out

00:15:03,460 --> 00:15:12,549
[Applause]

00:15:06,750 --> 00:15:12,549
[Music]

00:15:15,860 --> 00:15:35,019
there's any questions you can use the

00:15:18,410 --> 00:15:37,880
mics thank you for not being a sexist

00:15:35,019 --> 00:15:42,620
but this fan is discriminating my high

00:15:37,880 --> 00:15:45,529
yeah yeah I think it's a really good

00:15:42,620 --> 00:15:48,649
idea I'm really impressed by what you're

00:15:45,529 --> 00:15:50,000
you and the team is doing and I noticed

00:15:48,649 --> 00:15:52,130
one thing that I think is a really

00:15:50,000 --> 00:15:54,800
really good idea which is like it's

00:15:52,130 --> 00:15:58,820
really customized to a certain user what

00:15:54,800 --> 00:16:01,100
he or she found that is offensive and

00:15:58,820 --> 00:16:03,560
then it's not one model fits all but

00:16:01,100 --> 00:16:05,870
they also raise a question in my mind

00:16:03,560 --> 00:16:08,990
like that maybe like Tanika challenges

00:16:05,870 --> 00:16:11,899
to kind of make it a customer model like

00:16:08,990 --> 00:16:14,060
it may require a lot of resources and so

00:16:11,899 --> 00:16:16,459
have your team thick figured out like

00:16:14,060 --> 00:16:18,079
what's the the approaches to to overcome

00:16:16,459 --> 00:16:20,240
this challenge and I would really like

00:16:18,079 --> 00:16:33,709
to know if not then maybe we could find

00:16:20,240 --> 00:16:35,690
a solution to do it know so please well

00:16:33,709 --> 00:16:38,899
first of all congratulations and your

00:16:35,690 --> 00:16:41,300
wonderful talk and therefore let's

00:16:38,899 --> 00:16:44,240
permission congratulations for the

00:16:41,300 --> 00:16:47,270
project itself thank you once sometimes

00:16:44,240 --> 00:16:49,700
it's too much easy to pretend that

00:16:47,270 --> 00:16:53,510
things doesn't exist or just happens to

00:16:49,700 --> 00:16:55,940
the others however I would like to ask

00:16:53,510 --> 00:17:01,329
you more about the technical

00:16:55,940 --> 00:17:03,649
infrastructure that you yeah a bit oh

00:17:01,329 --> 00:17:07,490
you would like me to discuss yes of

00:17:03,649 --> 00:17:11,929
course and so the browser extension is

00:17:07,490 --> 00:17:15,169
currently using chaos and tensorflow and

00:17:11,929 --> 00:17:18,169
that's obviously for the there not be

00:17:15,169 --> 00:17:22,240
stuff but it's a very very simple model

00:17:18,169 --> 00:17:25,209
it's not even using any RNN or

00:17:22,240 --> 00:17:28,990
or you know LT SMS it's very very simple

00:17:25,209 --> 00:17:31,390
um and the back end is just in a nice

00:17:28,990 --> 00:17:33,640
simple flash gap and we all make

00:17:31,390 --> 00:17:35,260
mistakes should have been Django but

00:17:33,640 --> 00:17:38,920
there we go

00:17:35,260 --> 00:17:40,360
it's fine um but they're this honest

00:17:38,920 --> 00:17:42,670
it's a very lightweight thing at the

00:17:40,360 --> 00:17:47,260
moment and what we're really focusing on

00:17:42,670 --> 00:17:49,210
is just trying to understand the science

00:17:47,260 --> 00:17:50,740
behind it first so we're putting a lot

00:17:49,210 --> 00:17:52,030
of effort into research and getting

00:17:50,740 --> 00:17:53,800
different datasets and playing around

00:17:52,030 --> 00:17:55,960
with them so the actual web

00:17:53,800 --> 00:17:57,610
infrastructure is a bit and thin on the

00:17:55,960 --> 00:17:59,020
ground but if there are any front-end

00:17:57,610 --> 00:18:01,270
developers that would like to join in

00:17:59,020 --> 00:18:03,250
please because I have no front-end

00:18:01,270 --> 00:18:05,230
experience so that would be really great

00:18:03,250 --> 00:18:10,830
and we don't have a front-end at the

00:18:05,230 --> 00:18:13,090
moment so any more questions feedback

00:18:10,830 --> 00:18:15,580
cool

00:18:13,090 --> 00:18:18,130
you mentioned switching datasets to

00:18:15,580 --> 00:18:19,690
switch languages why why do you need

00:18:18,130 --> 00:18:22,059
that way why can't you put everything

00:18:19,690 --> 00:18:23,830
into one dataset are there things that

00:18:22,059 --> 00:18:25,960
are sexist in one language and not in

00:18:23,830 --> 00:18:28,720
the other or or is it too much data I

00:18:25,960 --> 00:18:31,360
just presumed we'd need to do that we

00:18:28,720 --> 00:18:33,850
haven't okay yeah I just presumed we'd

00:18:31,360 --> 00:18:36,340
need to do that because because I think

00:18:33,850 --> 00:18:39,160
sexism is so different in different

00:18:36,340 --> 00:18:41,500
languages the advantage because it's

00:18:39,160 --> 00:18:43,540
different you can put all in same bottom

00:18:41,500 --> 00:18:45,850
it won't disturb it or that it's a very

00:18:43,540 --> 00:18:49,830
very good point I don't know I know they

00:18:45,850 --> 00:18:49,830
decide this but yeah yeah

00:18:58,180 --> 00:19:02,440
thank you for you talk nice I actually

00:19:00,790 --> 00:19:04,420
have a question about business models a

00:19:02,440 --> 00:19:06,040
non-technical question because I'm

00:19:04,420 --> 00:19:08,650
curious I guess all of those social

00:19:06,040 --> 00:19:10,630
medias now allowed to flag offensive

00:19:08,650 --> 00:19:14,470
content right you say you want to

00:19:10,630 --> 00:19:16,570
develop browser extension but do you

00:19:14,470 --> 00:19:20,800
know how effective this offensive

00:19:16,570 --> 00:19:21,520
flagging is and it takes some time I

00:19:20,800 --> 00:19:24,280
guess

00:19:21,520 --> 00:19:26,290
sorry I don't so you saying that and

00:19:24,280 --> 00:19:28,840
there's already something similar that

00:19:26,290 --> 00:19:31,030
their social media past another approach

00:19:28,840 --> 00:19:32,950
right so I can Facebook I guess you're

00:19:31,030 --> 00:19:35,050
targeting Twitter it allows to flag

00:19:32,950 --> 00:19:36,340
offensive content yeah but the

00:19:35,050 --> 00:19:38,890
individual still has to see it

00:19:36,340 --> 00:19:42,040
and so by us filtering it our huh you

00:19:38,890 --> 00:19:46,600
just yeah exactly

00:19:42,040 --> 00:19:50,620
and also Twitter Facebook I have heard

00:19:46,600 --> 00:19:51,940
incidences when somebody has reported

00:19:50,620 --> 00:19:54,760
something and they've they've turn

00:19:51,940 --> 00:19:57,310
around and said no you're wrong and or

00:19:54,760 --> 00:19:59,410
it's been very very long for them to do

00:19:57,310 --> 00:20:03,010
something and to take down the the

00:19:59,410 --> 00:20:05,740
comments so what this is trying to do is

00:20:03,010 --> 00:20:07,960
just because if you are for example a

00:20:05,740 --> 00:20:11,410
politician and you you know you

00:20:07,960 --> 00:20:13,290
socialize online but you you say

00:20:11,410 --> 00:20:17,530
something and then your you know your

00:20:13,290 --> 00:20:18,490
your feed is full of misogynistic or

00:20:17,530 --> 00:20:20,740
sexist

00:20:18,490 --> 00:20:22,390
you know abuse it dilutes what you're

00:20:20,740 --> 00:20:24,520
really trying to do which is just you

00:20:22,390 --> 00:20:27,760
know read the news or talk with friends

00:20:24,520 --> 00:20:30,550
and so with this way of filtering it the

00:20:27,760 --> 00:20:38,440
good stuff remains all right thank you

00:20:30,550 --> 00:20:41,020
thank you for your talk are there plans

00:20:38,440 --> 00:20:44,710
to collaborate with social media

00:20:41,020 --> 00:20:47,830
platforms that for example if a person

00:20:44,710 --> 00:20:52,600
has a lot of tweets or comments that get

00:20:47,830 --> 00:20:54,580
flexed by wide range of women that for

00:20:52,600 --> 00:20:56,830
example accounts get blocked or

00:20:54,580 --> 00:20:58,960
something like that that would be great

00:20:56,830 --> 00:21:01,090
and you know Twitter I mean this this is

00:20:58,960 --> 00:21:02,680
a problem that a lot of the social media

00:21:01,090 --> 00:21:04,810
platforms are being pressured to solve

00:21:02,680 --> 00:21:06,520
so it would be great if at some point we

00:21:04,810 --> 00:21:10,110
could be incorporated with the platforms

00:21:06,520 --> 00:21:10,110
but we've not received

00:21:10,330 --> 00:21:15,070
an incredible amount of support let's

00:21:12,880 --> 00:21:17,170
put it like that we have so at some

00:21:15,070 --> 00:21:24,250
point it would be great too but yeah

00:21:17,170 --> 00:21:26,170
we'll see thank you so I hope I have a

00:21:24,250 --> 00:21:28,120
question what kind of formats do you

00:21:26,170 --> 00:21:30,580
plan to support on the comments like in

00:21:28,120 --> 00:21:33,130
some social media platforms like Twitter

00:21:30,580 --> 00:21:34,660
I suppose that a lot of the problems

00:21:33,130 --> 00:21:37,420
that people will experience also coming

00:21:34,660 --> 00:21:38,950
like images or maybe audio or video your

00:21:37,420 --> 00:21:41,260
brand or do you have some plans to

00:21:38,950 --> 00:21:43,540
tackle those or or maybe in the pipeline

00:21:41,260 --> 00:21:46,390
at some point at some point yeah but I

00:21:43,540 --> 00:21:51,330
am that yeah at some point but for now

00:21:46,390 --> 00:21:51,330
just the yeah just tweet just text

00:21:59,190 --> 00:22:08,070
anything else going once going twice

00:22:11,999 --> 00:22:18,639
do you have many people using the

00:22:15,940 --> 00:22:21,249
extensions or like what's the right now

00:22:18,639 --> 00:22:26,950
or is it live was just like it's alive

00:22:21,249 --> 00:22:29,590
yet everything is this thing yeah

00:22:26,950 --> 00:22:31,629
exactly we've got a proof of concept and

00:22:29,590 --> 00:22:32,830
we're hoping to take it to and there's a

00:22:31,629 --> 00:22:34,769
funding body called the prototype

00:22:32,830 --> 00:22:37,600
funding Berlin in August

00:22:34,769 --> 00:22:38,999
Mison is also based in Berlin so and

00:22:37,600 --> 00:22:41,559
we're only going to bring this out on

00:22:38,999 --> 00:22:45,730
Firefox to begin with because chrome do

00:22:41,559 --> 00:22:46,929
have something similar but if you for

00:22:45,730 --> 00:22:48,850
example if you hit their API the

00:22:46,929 --> 00:22:51,340
perspective API with you are a feminist

00:22:48,850 --> 00:22:53,080
it comes back as toxic so they're

00:22:51,340 --> 00:22:55,929
one-size-fits-all approach it's just not

00:22:53,080 --> 00:22:57,190
fitting anybody so we're gonna stick

00:22:55,929 --> 00:23:02,440
with Mozilla maybe we get some funding

00:22:57,190 --> 00:23:03,820
they're living the dream but yeah so no

00:23:02,440 --> 00:23:06,700
it's not live yet but it will be soon

00:23:03,820 --> 00:23:17,049
I've just struggled left struggle with

00:23:06,700 --> 00:23:19,869
Amazon Web Services for a week so so you

00:23:17,049 --> 00:23:22,779
just mentioned you hope you make up

00:23:19,869 --> 00:23:25,269
funding from Mozilla and you also said

00:23:22,779 --> 00:23:27,610
you right now mostly operating it's out

00:23:25,269 --> 00:23:30,399
of focus basically yeah so do you have

00:23:27,610 --> 00:23:34,450
any other funding plans yes so the

00:23:30,399 --> 00:23:36,279
prototype fund which is which fits us

00:23:34,450 --> 00:23:39,059
perfectly so their categories are things

00:23:36,279 --> 00:23:44,019
like you know date like Internet health

00:23:39,059 --> 00:23:46,029
and we already have some contacts within

00:23:44,019 --> 00:23:49,240
the prototype phone so that's really the

00:23:46,029 --> 00:23:50,379
one that we're focusing on mmm yeah cool

00:23:49,240 --> 00:23:52,799
hope that works out

00:23:50,379 --> 00:23:52,799
thank you

00:24:01,800 --> 00:24:08,059

YouTube URL: https://www.youtube.com/watch?v=Ov0scTjDVuQ


