Title: Shailen Sobhee - Accelerate your Deep Learning Inferencing with the IntelÂ® DL Boost technology
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"Accelerate your Deep Learning Inferencing with the IntelÂ® DL Boost technology
[EuroPython 2019 - Talk - 2019-07-10 - Boston]
[Basel, CH]

By Shailen Sobhee

Learn about IntelÂ® Deep Learning Boost, also known as Vector Neural Network Instructions (VNNI), a new set of AVX-512 instructions, that are designed to deliver significantly more efficient Deep Learning (Inference) acceleration. Through this technology, I will show you how you can perform low-precision (INT8) inference much faster on hardware that support the VNNI instruction set (for example, the 2nd generation Intel Xeon Scalable processors, codenamed, Cascade Lake). In the live Jupyter notebook session, you can will be able to see the benefits of this new hardware technology. 

Note: This is an advanced talk. Knowledge about Deep Learning, Inferencing and basic awareness of hardware instruction sets would be desirable.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:01,310 --> 00:00:12,830
what about the rest of you guys AI so

00:00:08,450 --> 00:00:15,530
deep learning okay because my talk will

00:00:12,830 --> 00:00:19,130
be focused up around deep learning which

00:00:15,530 --> 00:00:23,109
is a very specific field of AI and I go

00:00:19,130 --> 00:00:25,279
even more specific on how I accelerate

00:00:23,109 --> 00:00:29,660
inferencing which is one of the

00:00:25,279 --> 00:00:33,140
important stages in deep learning it is

00:00:29,660 --> 00:00:36,530
a fairly advanced talk so I will turn it

00:00:33,140 --> 00:00:41,030
down just so that it's easier to digest

00:00:36,530 --> 00:00:43,160
so what I'm going to introduce to you

00:00:41,030 --> 00:00:45,680
actually this is the outline what is

00:00:43,160 --> 00:00:48,469
Intel deep learning boost it's a new

00:00:45,680 --> 00:00:50,960
fancy hardware technology I'm going to

00:00:48,469 --> 00:00:55,730
highlight what it is and why it is super

00:00:50,960 --> 00:00:57,910
useful and and explain what are the new

00:00:55,730 --> 00:01:00,500
vector registers that we included in our

00:00:57,910 --> 00:01:03,890
latest hardware that we recently

00:01:00,500 --> 00:01:09,729
released in April and then show you in a

00:01:03,890 --> 00:01:13,220
live example like why it is so nice

00:01:09,729 --> 00:01:17,560
alright so cool so what is Intel deep

00:01:13,220 --> 00:01:23,299
learning boost it is a set of new

00:01:17,560 --> 00:01:25,460
avx-512 registers in our latest hardware

00:01:23,299 --> 00:01:27,170
that we really recently released the

00:01:25,460 --> 00:01:29,540
code name is cascade Lakes it's the

00:01:27,170 --> 00:01:32,630
second generation of Xeon scalable

00:01:29,540 --> 00:01:34,700
processors these are really what we call

00:01:32,630 --> 00:01:36,560
big fat Xeon they are really powerful

00:01:34,700 --> 00:01:41,990
they're really good they're really fast

00:01:36,560 --> 00:01:44,960
and it's great for AI and we already

00:01:41,990 --> 00:01:47,750
proud that this new set of vector

00:01:44,960 --> 00:01:53,119
registers allow you to do inferencing

00:01:47,750 --> 00:01:55,100
really fast and those registers are

00:01:53,119 --> 00:01:58,820
called a vector neural network

00:01:55,100 --> 00:02:02,540
instructions or forethought VN ni + VN

00:01:58,820 --> 00:02:05,479
and I in itself can give you close to 2x

00:02:02,540 --> 00:02:07,340
boost on during the inferencing now what

00:02:05,479 --> 00:02:09,019
is inferencing inferencing is prediction

00:02:07,340 --> 00:02:11,810
so you have your deep neural network it

00:02:09,019 --> 00:02:13,790
has been trained on your data set the

00:02:11,810 --> 00:02:15,560
next step is make the neural network

00:02:13,790 --> 00:02:19,130
think and make decisions right

00:02:15,560 --> 00:02:22,459
so making the decisions that's what we

00:02:19,130 --> 00:02:25,400
call doing the predictions or technical

00:02:22,459 --> 00:02:29,120
term inferencing so we want to make the

00:02:25,400 --> 00:02:32,900
neural network decide faster make

00:02:29,120 --> 00:02:34,580
decisions faster in faster and this is

00:02:32,900 --> 00:02:36,800
what we're bringing up to you it

00:02:34,580 --> 00:02:45,950
hardware technology to boost this

00:02:36,800 --> 00:02:47,630
decision-making faster so oh yeah so

00:02:45,950 --> 00:02:51,290
that was it the summary of what I just

00:02:47,630 --> 00:02:57,410
said and let's go back to the deep

00:02:51,290 --> 00:03:00,140
learning foundations so why what is

00:02:57,410 --> 00:03:03,709
involved in in deep learning there's a

00:03:00,140 --> 00:03:07,000
lot of math involved so if you look at

00:03:03,709 --> 00:03:09,530
the typical convolutional neural network

00:03:07,000 --> 00:03:12,200
where you have a filter looping over

00:03:09,530 --> 00:03:14,540
your image there's a lot of math

00:03:12,200 --> 00:03:16,610
involved the cells being multiplied with

00:03:14,540 --> 00:03:19,370
others and then adding the numbers

00:03:16,610 --> 00:03:23,480
together so lots of multiplications lots

00:03:19,370 --> 00:03:27,190
of ads and this is heavy on a compute

00:03:23,480 --> 00:03:31,160
unit so since we make processes and

00:03:27,190 --> 00:03:32,720
processes what they do they compute we

00:03:31,160 --> 00:03:39,769
need to make sure that we can compute

00:03:32,720 --> 00:03:41,540
really fast in traditional like typical

00:03:39,769 --> 00:03:43,730
use cases especially in high-performance

00:03:41,540 --> 00:03:48,370
computing the data types are usually

00:03:43,730 --> 00:03:52,100
floating-point 32 so 32 bits numbers and

00:03:48,370 --> 00:03:55,310
we think that we could do things faster

00:03:52,100 --> 00:03:57,709
if we change the data type so I will

00:03:55,310 --> 00:04:00,290
come more along this in a second so

00:03:57,709 --> 00:04:01,549
recall back the convolution Network at

00:04:00,290 --> 00:04:05,570
the dimension these are really popular

00:04:01,549 --> 00:04:08,060
look at that this is if you look at the

00:04:05,570 --> 00:04:12,880
animated part this is a filter going on

00:04:08,060 --> 00:04:16,010
on an image right and as it goes around

00:04:12,880 --> 00:04:22,130
looping over that image lots of moles or

00:04:16,010 --> 00:04:25,250
multiplications lots of ads and we want

00:04:22,130 --> 00:04:27,260
to make this faster so why do we need

00:04:25,250 --> 00:04:31,290
Intel deep learning boost

00:04:27,260 --> 00:04:37,040
one of the key terms a key concept is

00:04:31,290 --> 00:04:41,310
quantization now what is quantization

00:04:37,040 --> 00:04:43,290
consider this number eight 96.1 nine to

00:04:41,310 --> 00:04:47,160
four if you represent this in floating

00:04:43,290 --> 00:04:50,220
point 32 so 32 bits you take a lot of

00:04:47,160 --> 00:04:53,310
space in memory a lot of space in your

00:04:50,220 --> 00:04:56,130
registers to represent that number but

00:04:53,310 --> 00:04:58,800
the bulk the most all the credit the

00:04:56,130 --> 00:05:01,940
information is actually in this 96 maybe

00:04:58,800 --> 00:05:05,220
you don't care about this dot 1 9 2 4

00:05:01,940 --> 00:05:07,980
it's insignificant compared to 96 right

00:05:05,220 --> 00:05:12,900
so if you will represent this number as

00:05:07,980 --> 00:05:14,790
an integer you lose less register to

00:05:12,900 --> 00:05:19,670
represent the number so only 8 bits to

00:05:14,790 --> 00:05:23,730
represent integer 96 and that's great so

00:05:19,670 --> 00:05:31,200
what benefits come out of this you use

00:05:23,730 --> 00:05:33,900
less power so less CPU energy to

00:05:31,200 --> 00:05:36,870
represent that number we also lower the

00:05:33,900 --> 00:05:39,780
memory bandwidth we also look at the

00:05:36,870 --> 00:05:41,940
storage so you recall earlier you have 4

00:05:39,780 --> 00:05:45,060
boxes to represent the 32 bit now we

00:05:41,940 --> 00:05:47,910
have only one so less storage for that

00:05:45,060 --> 00:05:50,190
and all of this at the benefit of higher

00:05:47,910 --> 00:05:54,180
performance so that's the key idea

00:05:50,190 --> 00:05:58,230
behind quantization here and as you

00:05:54,180 --> 00:06:00,780
recall you had this number 96 that 1 9 2

00:05:58,230 --> 00:06:02,760
4 so we are now reducing the precision

00:06:00,780 --> 00:06:04,380
of that number to just ninety-six so

00:06:02,760 --> 00:06:08,250
you're losing a little bit of accuracy

00:06:04,380 --> 00:06:12,930
so what is important is that we don't

00:06:08,250 --> 00:06:15,240
lose too much accuracy now a quick intro

00:06:12,930 --> 00:06:16,890
by this vector neural network

00:06:15,240 --> 00:06:19,770
instructions that we introduced that's

00:06:16,890 --> 00:06:22,760
hardware technology here so you recall

00:06:19,770 --> 00:06:26,490
that convolution right so there's this

00:06:22,760 --> 00:06:29,220
filter going over that image doing lots

00:06:26,490 --> 00:06:36,120
of multiplications lots of additions so

00:06:29,220 --> 00:06:36,950
we have vector registers in the hardware

00:06:36,120 --> 00:06:40,040
to do

00:06:36,950 --> 00:06:47,210
these multiplications and those

00:06:40,040 --> 00:06:50,330
additions faster so this is it so if you

00:06:47,210 --> 00:06:53,330
look on the first line over there that's

00:06:50,330 --> 00:06:55,790
the first generation of Xeon processors

00:06:53,330 --> 00:06:57,800
code-named skylake so if you would have

00:06:55,790 --> 00:07:02,680
to floating 32-bit floating-point

00:06:57,800 --> 00:07:07,340
numbers being added you use one

00:07:02,680 --> 00:07:09,620
instruction to do that so you multiply

00:07:07,340 --> 00:07:12,890
these two floating point numbers you get

00:07:09,620 --> 00:07:17,990
an output a 32-bit and if you would do

00:07:12,890 --> 00:07:21,680
this in lower precision so int 8 so you

00:07:17,990 --> 00:07:23,960
have two int eight numbers being

00:07:21,680 --> 00:07:28,520
multiplied and then if you have to add

00:07:23,960 --> 00:07:32,360
you actually use free instructions to do

00:07:28,520 --> 00:07:35,660
this free instructions to add to low

00:07:32,360 --> 00:07:38,930
precision integers and add them to get

00:07:35,660 --> 00:07:41,420
an output a 32-bit so we decided okay

00:07:38,930 --> 00:07:46,100
you're using free instructions you're

00:07:41,420 --> 00:07:48,230
spending CPU cycles a lot to do that can

00:07:46,100 --> 00:07:51,080
we do that better the answer is yes in

00:07:48,230 --> 00:07:54,520
our second generation cascade Lake

00:07:51,080 --> 00:07:59,210
processes we combine those free

00:07:54,520 --> 00:08:03,170
instructions into one and this is the

00:07:59,210 --> 00:08:05,630
result the same stuff that you do in

00:08:03,170 --> 00:08:08,390
free instructions you do that with just

00:08:05,630 --> 00:08:10,550
one so effectively

00:08:08,390 --> 00:08:13,970
less CPU cycles spent in doing this

00:08:10,550 --> 00:08:18,290
process of adding and multiplying in low

00:08:13,970 --> 00:08:19,820
precision in eight now you as a software

00:08:18,290 --> 00:08:21,740
developer maybe you don't care all this

00:08:19,820 --> 00:08:24,140
is hardware why do I need to know about

00:08:21,740 --> 00:08:26,030
this hardware is this software out that

00:08:24,140 --> 00:08:28,280
I can just do that out of the box easy

00:08:26,030 --> 00:08:30,140
for me the answer is yes we thought

00:08:28,280 --> 00:08:34,060
about you guys as a software developer

00:08:30,140 --> 00:08:38,830
and we contributed or prod to the market

00:08:34,060 --> 00:08:42,140
one product to do that for you and enter

00:08:38,830 --> 00:08:44,620
Intel distribution of open Vino so open

00:08:42,140 --> 00:08:48,410
Vino is a tool that can allow you to

00:08:44,620 --> 00:08:51,980
process into eight for you

00:08:48,410 --> 00:08:55,730
so a quick introduction to it so in open

00:08:51,980 --> 00:08:58,220
V know in a nutshell when you have done

00:08:55,730 --> 00:09:01,160
your training of your model

00:08:58,220 --> 00:09:03,170
right it is intensive flow or calf AMX

00:09:01,160 --> 00:09:04,850
net whatever whatever framework you have

00:09:03,170 --> 00:09:08,300
used there so you have cut you have

00:09:04,850 --> 00:09:10,930
obtained your trained model what open V

00:09:08,300 --> 00:09:13,940
no does it takes a trained model and

00:09:10,930 --> 00:09:16,670
sends it into the component called model

00:09:13,940 --> 00:09:20,990
optimizer to make that model more

00:09:16,670 --> 00:09:23,600
efficient and more CPU friendly let's

00:09:20,990 --> 00:09:25,220
spread this way and the result is this

00:09:23,600 --> 00:09:28,400
intermediate represented a

00:09:25,220 --> 00:09:31,010
representation marked as IR it's a

00:09:28,400 --> 00:09:33,380
combination of two files an XML file in

00:09:31,010 --> 00:09:36,580
a bin file that contains the weights of

00:09:33,380 --> 00:09:39,980
the neural network and traditionally a

00:09:36,580 --> 00:09:43,970
normal open we know that model will be

00:09:39,980 --> 00:09:46,670
in floating point 32 and then you do

00:09:43,970 --> 00:09:49,460
your inferencing on that terrain model

00:09:46,670 --> 00:09:53,360
that's the traditional way so first step

00:09:49,460 --> 00:09:54,260
get a train model optimize it with the

00:09:53,360 --> 00:09:56,290
model optimizer

00:09:54,260 --> 00:09:58,730
and then get that intermediate

00:09:56,290 --> 00:10:01,700
representation and then do inferencing

00:09:58,730 --> 00:10:05,300
with the inference engine that inference

00:10:01,700 --> 00:10:08,360
engine is the component of open vino

00:10:05,300 --> 00:10:10,460
that will allow you to do inference on

00:10:08,360 --> 00:10:14,060
any type of hardware you have beat the

00:10:10,460 --> 00:10:17,000
CPU beat an integrated GPU beat an FPGA

00:10:14,060 --> 00:10:18,620
maybe or even the mob video stick you

00:10:17,000 --> 00:10:20,780
know that little blue stick you may have

00:10:18,620 --> 00:10:23,240
seen that allows you to do inferencing

00:10:20,780 --> 00:10:25,070
on the edge for instance a drone that's

00:10:23,240 --> 00:10:27,530
flying or like a little robot going

00:10:25,070 --> 00:10:30,910
around you can do inferencing on that

00:10:27,530 --> 00:10:35,380
mobile device that's really great so

00:10:30,910 --> 00:10:38,960
this is how nice open window is now a

00:10:35,380 --> 00:10:41,540
step backward this talk was about low

00:10:38,960 --> 00:10:43,670
precision inferencing so how does open

00:10:41,540 --> 00:10:46,550
window fit in there for low precision

00:10:43,670 --> 00:10:50,260
inferencing so what oportunidad it takes

00:10:46,550 --> 00:10:54,130
a 32-bit representation of that

00:10:50,260 --> 00:10:56,630
intermediate representation and uses a

00:10:54,130 --> 00:10:58,370
component called calibration tool it

00:10:56,630 --> 00:10:59,770
will calibrate this 32-bit

00:10:58,370 --> 00:11:02,620
representation into a

00:10:59,770 --> 00:11:04,570
in eight model so then once you get this

00:11:02,620 --> 00:11:07,800
in eight intermediate representation

00:11:04,570 --> 00:11:16,270
then you do inferencing in low precision

00:11:07,800 --> 00:11:19,000
so that's the big picture of it and the

00:11:16,270 --> 00:11:22,330
part where you do all this calibration

00:11:19,000 --> 00:11:24,670
this is done once we call it kind of an

00:11:22,330 --> 00:11:26,230
offline stage you do it once once you

00:11:24,670 --> 00:11:29,470
have this intermediate presentation you

00:11:26,230 --> 00:11:31,690
can store it on your on your robot or

00:11:29,470 --> 00:11:36,790
your drone and then the online stage is

00:11:31,690 --> 00:11:38,560
this live process that that you do on

00:11:36,790 --> 00:11:40,120
the device so this is the online stage

00:11:38,560 --> 00:11:44,920
anyway enough said about that let me

00:11:40,120 --> 00:11:48,760
show you now live results of all the

00:11:44,920 --> 00:11:54,130
benefits of low precision inferencing so

00:11:48,760 --> 00:11:56,860
I will show you two cases in one case

00:11:54,130 --> 00:12:00,070
I'm going to do inferencing on the

00:11:56,860 --> 00:12:02,140
floating point 32 model and in the other

00:12:00,070 --> 00:12:05,770
case I'm going to show you inferencing

00:12:02,140 --> 00:12:09,700
of the same inferencing of like the same

00:12:05,770 --> 00:12:12,970
data set but in low precision integer 8

00:12:09,700 --> 00:12:19,780
so let's see the difference so I have

00:12:12,970 --> 00:12:28,260
here a an example which I will move to

00:12:19,780 --> 00:12:28,260
the screen my mouse went there

00:12:40,240 --> 00:12:42,839
cool

00:12:46,960 --> 00:12:53,290
so in this Jupiter notebook I'm going to

00:12:50,260 --> 00:12:55,960
show you inferencing in floating-point

00:12:53,290 --> 00:12:58,089
that little bit and see how fast we do

00:12:55,960 --> 00:13:00,520
that and inferencing in int 8 and how

00:12:58,089 --> 00:13:02,940
false we do that so let me maximize the

00:13:00,520 --> 00:13:07,240
screen view double-header

00:13:02,940 --> 00:13:10,420
cool so this is a very simple algorithm

00:13:07,240 --> 00:13:14,790
I'm just inferencing on cats and dogs

00:13:10,420 --> 00:13:17,350
it's an open data set and I'm using the

00:13:14,790 --> 00:13:20,740
Intel open window for doing inferencing

00:13:17,350 --> 00:13:24,580
so model optimizer will actually convert

00:13:20,740 --> 00:13:27,610
my ResNet model into this intermediate

00:13:24,580 --> 00:13:31,930
representation this is done great the

00:13:27,610 --> 00:13:38,890
next step is declare the network and so

00:13:31,930 --> 00:13:41,740
on great I do that ok it's done and next

00:13:38,890 --> 00:13:44,649
import matplotlib that will load and

00:13:41,740 --> 00:13:46,180
take care of plotting the results and so

00:13:44,649 --> 00:13:49,510
on and right now I'm processing the

00:13:46,180 --> 00:13:52,209
image now it's done and then you can see

00:13:49,510 --> 00:13:54,850
I'm proceeding with inferencing on my

00:13:52,209 --> 00:13:57,610
image data set so cards there are dogs

00:13:54,850 --> 00:13:59,740
really cute cats really nice friendly

00:13:57,610 --> 00:14:03,940
dogs in there but pay attention to the

00:13:59,740 --> 00:14:08,130
numbers what is my rate these are 32-bit

00:14:03,940 --> 00:14:12,010
representations of my of my network and

00:14:08,130 --> 00:14:19,240
I'm inferencing approximately 300 images

00:14:12,010 --> 00:14:21,570
per second maybe you'd be happy with 300

00:14:19,240 --> 00:14:24,400
images per second but can we do faster

00:14:21,570 --> 00:14:28,630
nan says yes by leveraging hardware

00:14:24,400 --> 00:14:31,570
technology for that and so the

00:14:28,630 --> 00:14:34,480
calibration tool if you recall was that

00:14:31,570 --> 00:14:37,150
pod that converts this 32-bit model into

00:14:34,480 --> 00:14:39,550
the in 8 model and I've already done

00:14:37,150 --> 00:14:41,740
that so in the interest of time I'll

00:14:39,550 --> 00:14:45,940
proceed with showing you how it is in

00:14:41,740 --> 00:14:49,779
the init model so I define my target

00:14:45,940 --> 00:14:53,260
device as you can see the CPU and the

00:14:49,779 --> 00:14:55,690
network in 8 call and I'm loading the

00:14:53,260 --> 00:14:57,260
plug-in the and I'm loading the model

00:14:55,690 --> 00:15:02,180
and so on and so forth

00:14:57,260 --> 00:15:05,450
now let's proceed with inferencing it's

00:15:02,180 --> 00:15:10,420
a same dataset same cat same dogs but as

00:15:05,450 --> 00:15:16,160
you can see I am inferencing faster

00:15:10,420 --> 00:15:24,410
approximately 600 close to 700 images

00:15:16,160 --> 00:15:27,260
per second and maybe we can plot this in

00:15:24,410 --> 00:15:32,350
a table to show the difference between

00:15:27,260 --> 00:15:35,630
the two here in front speed in 32-bit

00:15:32,350 --> 00:15:38,900
approximately 300 images and in int 8

00:15:35,630 --> 00:15:42,830
low precision close to 700 so what's the

00:15:38,900 --> 00:15:46,640
key message there leveraging low

00:15:42,830 --> 00:15:48,440
precision inferencing boosted by our

00:15:46,640 --> 00:15:51,500
vector neural network instruction and

00:15:48,440 --> 00:15:56,090
cascade Lake we almost got twice the

00:15:51,500 --> 00:16:00,020
performance on inferencing isn't that

00:15:56,090 --> 00:16:05,450
great so software technology low

00:16:00,020 --> 00:16:11,600
precision boosted by hardware twice

00:16:05,450 --> 00:16:20,180
faster I think that's nice so that's the

00:16:11,600 --> 00:16:23,030
key idea I wanted to share with you if

00:16:20,180 --> 00:16:25,460
you have the chance use cascade Lake

00:16:23,030 --> 00:16:27,800
cascade Lake is available on Amazon Web

00:16:25,460 --> 00:16:29,540
Services as of now it's right now the

00:16:27,800 --> 00:16:32,840
only cloud technology that offers

00:16:29,540 --> 00:16:37,730
cascade Lake with the Vienna instruction

00:16:32,840 --> 00:16:41,060
set and also if for the deep learning

00:16:37,730 --> 00:16:43,640
guys in there in the room take a look at

00:16:41,060 --> 00:16:46,520
low precision inferencing

00:16:43,640 --> 00:16:50,890
so from 32-bit the usual normal use case

00:16:46,520 --> 00:16:53,510
and consider in 8

00:16:50,890 --> 00:16:55,520
there are corner cases where in 8 won't

00:16:53,510 --> 00:16:58,070
work when since when you really care

00:16:55,520 --> 00:17:01,070
about precision say you're looking at

00:16:58,070 --> 00:17:03,620
cancer cells in an MRI image for

00:17:01,070 --> 00:17:06,020
instance we're very specific details are

00:17:03,620 --> 00:17:09,230
really really key and important

00:17:06,020 --> 00:17:12,850
then maybe not but in other cases like

00:17:09,230 --> 00:17:15,529
images of cats and dogs or maybe

00:17:12,850 --> 00:17:18,860
language processing sound and so on

00:17:15,529 --> 00:17:22,490
where precision may not be that critical

00:17:18,860 --> 00:17:33,289
then 88 is a great boost get stuff done

00:17:22,490 --> 00:17:36,559
faster that's it thank you okay we have

00:17:33,289 --> 00:17:40,390
some time for questions any question I

00:17:36,559 --> 00:17:43,970
would pass by with the microphone hi

00:17:40,390 --> 00:17:46,309
thank you first for the talk what's the

00:17:43,970 --> 00:17:49,070
trade-off in precision like if you

00:17:46,309 --> 00:17:53,000
compare the floating point 32 model and

00:17:49,070 --> 00:17:54,799
the 832 intake model because you show

00:17:53,000 --> 00:17:58,450
the increase in speed but not a decrease

00:17:54,799 --> 00:18:02,210
in precision right yeah there is a loss

00:17:58,450 --> 00:18:05,929
some degree of precision in there and it

00:18:02,210 --> 00:18:07,940
all depends on the nature of your of the

00:18:05,929 --> 00:18:13,789
task that you're doing so in my case I

00:18:07,940 --> 00:18:16,909
was classifying cats and dogs and I did

00:18:13,789 --> 00:18:21,230
lose some precision yes some but it

00:18:16,909 --> 00:18:23,299
didn't affect my results so how much you

00:18:21,230 --> 00:18:25,279
lose depends on your use case I can give

00:18:23,299 --> 00:18:27,440
you numbers because that's very specific

00:18:25,279 --> 00:18:28,909
on the nature okay you could have like

00:18:27,440 --> 00:18:33,950
some kind of a validation metric that

00:18:28,909 --> 00:18:36,980
you might okay yeah so in my case for

00:18:33,950 --> 00:18:40,250
the cats and dogs example my loss was

00:18:36,980 --> 00:18:43,070
not that to that great like not too big

00:18:40,250 --> 00:18:46,970
like five percent difference for me was

00:18:43,070 --> 00:18:53,000
okay totally okay depends on your use

00:18:46,970 --> 00:18:55,429
case Thanks so if I understood correctly

00:18:53,000 --> 00:18:57,799
in the demo you showed you compared

00:18:55,429 --> 00:19:00,230
using a floating-point without the

00:18:57,799 --> 00:19:02,779
special instructions to use int eight

00:19:00,230 --> 00:19:05,480
with special instructions have you try

00:19:02,779 --> 00:19:07,909
to benchmarking using imitate without

00:19:05,480 --> 00:19:11,659
the special instructions just to see if

00:19:07,909 --> 00:19:15,500
the lower memory footprint helps yeah so

00:19:11,659 --> 00:19:16,470
of course let me show you back this

00:19:15,500 --> 00:19:28,470
thing

00:19:16,470 --> 00:19:28,470
this one in gray oops

00:19:28,890 --> 00:19:35,110
okay one more okay in gray

00:19:32,169 --> 00:19:37,390
that's Skydeck the previous generation

00:19:35,110 --> 00:19:40,029
okay still the latest at step one not

00:19:37,390 --> 00:19:43,570
leaders still the great Xeon processor

00:19:40,029 --> 00:19:46,630
and the one that released this year is

00:19:43,570 --> 00:19:48,640
cascade Lake in yellow the difference

00:19:46,630 --> 00:19:51,480
between these two cascade Lake comes

00:19:48,640 --> 00:19:54,850
with vector mural instructions via Ninoy

00:19:51,480 --> 00:19:57,130
so the comparison that you're referring

00:19:54,850 --> 00:20:00,820
is comparing the last one cascade Lake

00:19:57,130 --> 00:20:03,190
versus sky lake yes and from there in my

00:20:00,820 --> 00:20:06,039
measurement results I got close to 2x

00:20:03,190 --> 00:20:12,070
performance boost so that was my real

00:20:06,039 --> 00:20:15,520
world result that I got on paper in

00:20:12,070 --> 00:20:17,529
theory true before X if you do the math

00:20:15,520 --> 00:20:19,779
you know like calculating how mean

00:20:17,529 --> 00:20:23,649
structions you use to do some mel and

00:20:19,779 --> 00:20:28,210
some ads and how one instruction will do

00:20:23,649 --> 00:20:30,640
it so it's on paper for X so on my

00:20:28,210 --> 00:20:33,820
machine when I did I got 2 X 2 that 9 X

00:20:30,640 --> 00:20:38,190
and yeah

00:20:33,820 --> 00:20:41,340
I think I also have it somewhere here

00:20:38,190 --> 00:20:41,340
speed up

00:20:51,390 --> 00:20:57,510
you see their difference okay 32 an 88

00:20:55,530 --> 00:21:02,670
speed-up was too 50.3 but that's

00:20:57,510 --> 00:21:05,610
comparing in Dayton FP 32 comparing the

00:21:02,670 --> 00:21:08,130
two in Dayton sky like an idiot on

00:21:05,610 --> 00:21:12,510
cascade Lake in another example it was

00:21:08,130 --> 00:21:15,570
also around works yeah and right now I'm

00:21:12,510 --> 00:21:20,760
comparing 32 bit on Cascade Lake and eat

00:21:15,570 --> 00:21:23,790
it on cascade Lake so if you would do 32

00:21:20,760 --> 00:21:24,270
bit on cascade like an ETA on cascade

00:21:23,790 --> 00:21:27,540
Lake

00:21:24,270 --> 00:21:34,049
so 32 bit on sky lake and into it on

00:21:27,540 --> 00:21:40,260
cascade leg then it's 4x - any other

00:21:34,049 --> 00:21:41,850
question you skipped a calibration part

00:21:40,260 --> 00:21:45,600
here I mean you said you turn it already

00:21:41,850 --> 00:21:48,360
yeah how much time does it that usually

00:21:45,600 --> 00:21:52,309
taken could you share and what it's

00:21:48,360 --> 00:21:55,890
based on or not yeah so the calibration

00:21:52,309 --> 00:21:58,169
takes some time to do that around 10

00:21:55,890 --> 00:22:02,100
minutes ish depending on how big your

00:21:58,169 --> 00:22:03,960
model is and because I don't have time

00:22:02,100 --> 00:22:09,090
10 minutes to do that that's why I did

00:22:03,960 --> 00:22:14,460
it offline but yeah for a model that was

00:22:09,090 --> 00:22:18,270
around 50 60 megabytes was like under 10

00:22:14,460 --> 00:22:21,419
minutes with which you do offline one

00:22:18,270 --> 00:22:25,169
time and then you're done yeah the goal

00:22:21,419 --> 00:22:27,780
is like on on the edge in real life like

00:22:25,169 --> 00:22:30,380
inferencing big quick decisions this has

00:22:27,780 --> 00:22:30,380
to be fast

00:22:33,250 --> 00:22:40,400
sorry Mike you're calibrating based on

00:22:38,090 --> 00:22:45,110
like your calibration is based on what

00:22:40,400 --> 00:22:47,960
the calibration is basically taking the

00:22:45,110 --> 00:22:51,070
32-bit representation of the weights and

00:22:47,960 --> 00:22:55,880
so on and representing them in in Tate

00:22:51,070 --> 00:22:58,280
and yeah so you know in a neural network

00:22:55,880 --> 00:23:00,190
of the nodes of the weights and so on so

00:22:58,280 --> 00:23:02,510
these numbers they are all in

00:23:00,190 --> 00:23:04,430
represented in 32-bit they take a lot of

00:23:02,510 --> 00:23:06,220
space to represent these numbers and

00:23:04,430 --> 00:23:08,480
going for all of that

00:23:06,220 --> 00:23:11,420
doing the calculations in between the

00:23:08,480 --> 00:23:13,760
nodes it takes a lot of CPU cycles to

00:23:11,420 --> 00:23:15,620
process all of these so that I guess to

00:23:13,760 --> 00:23:18,320
convert this whole thing well certain

00:23:15,620 --> 00:23:21,080
layers into in eighth actually not all

00:23:18,320 --> 00:23:27,020
of the layers are calibrated to indeed

00:23:21,080 --> 00:23:31,610
some of the layers yeah right we have

00:23:27,020 --> 00:23:34,490
one question here so the reduction from

00:23:31,610 --> 00:23:37,220
32 floating point to 8-bit is it like

00:23:34,490 --> 00:23:40,820
always possible or the algorithm that

00:23:37,220 --> 00:23:43,070
does it will is always succeed and if it

00:23:40,820 --> 00:23:45,560
does it is up to the person that created

00:23:43,070 --> 00:23:48,440
a model to like evaluate if it is like

00:23:45,560 --> 00:23:51,310
good enough yeah excellent question so a

00:23:48,440 --> 00:23:57,230
bit referred to the last question

00:23:51,310 --> 00:24:00,680
some not everything but some some layers

00:23:57,230 --> 00:24:04,610
depends on yeah open we know we'll

00:24:00,680 --> 00:24:07,210
figure out where they can or cannot and

00:24:04,610 --> 00:24:09,380
it will gives you a report like it has

00:24:07,210 --> 00:24:12,230
calibrated this layer it has done this

00:24:09,380 --> 00:24:16,010
successfully others it didn't but other

00:24:12,230 --> 00:24:18,530
at the end of the day when some parts

00:24:16,010 --> 00:24:20,540
have been converted to in date you

00:24:18,530 --> 00:24:23,990
should out of the box see some better

00:24:20,540 --> 00:24:26,470
performance compared to pure FP that

00:24:23,990 --> 00:24:31,100
it's so it's answer can be that I cannot

00:24:26,470 --> 00:24:33,170
convert this like it is like it could be

00:24:31,100 --> 00:24:35,830
when it's very complex open V no doesn't

00:24:33,170 --> 00:24:38,950
understand your neural network may be

00:24:35,830 --> 00:24:38,950
thank you

00:24:40,370 --> 00:24:46,980
any question five minutes

00:24:44,840 --> 00:24:49,260
I'm really happy there's so much

00:24:46,980 --> 00:24:52,230
interest in this it's that's great I

00:24:49,260 --> 00:24:56,520
didn't expect so many questions you guys

00:24:52,230 --> 00:24:58,650
are great so I have a question so you

00:24:56,520 --> 00:25:00,180
say that this technology is available on

00:24:58,650 --> 00:25:05,010
cascade Lake ziens

00:25:00,180 --> 00:25:09,120
yeah are they for sale right now can we

00:25:05,010 --> 00:25:11,460
get them right now and will this be a

00:25:09,120 --> 00:25:13,800
technology that will arrive us on a

00:25:11,460 --> 00:25:15,780
desktop or or is it going to be just on

00:25:13,800 --> 00:25:16,680
the CEOs on the data center type of

00:25:15,780 --> 00:25:19,980
thing okay

00:25:16,680 --> 00:25:22,230
so right now this vector neural network

00:25:19,980 --> 00:25:25,590
which is hardware technology is only

00:25:22,230 --> 00:25:27,720
available on cascade Lake and future

00:25:25,590 --> 00:25:32,760
exeunt processes of course when it will

00:25:27,720 --> 00:25:36,150
come to desktop I don't know but it's on

00:25:32,760 --> 00:25:37,830
cascade Lake now hardware we sell you

00:25:36,150 --> 00:25:40,080
mentioned sale right hard way is

00:25:37,830 --> 00:25:43,890
something we sell but open we know is

00:25:40,080 --> 00:25:49,530
open source technology you can read the

00:25:43,890 --> 00:25:52,290
source code online if you want and our

00:25:49,530 --> 00:25:57,270
AI software tools are free and open

00:25:52,290 --> 00:26:00,240
source we don't sell these any other

00:25:57,270 --> 00:26:03,990
question yes one question are you

00:26:00,240 --> 00:26:07,500
planning to have this on the mkl or are

00:26:03,990 --> 00:26:09,360
we going to like to add this operation

00:26:07,500 --> 00:26:11,520
on the mat canal library like if you

00:26:09,360 --> 00:26:13,710
want to use it annoying like standard

00:26:11,520 --> 00:26:18,540
Python like for the the next generation

00:26:13,710 --> 00:26:26,430
of hardware well uncle can treat it

00:26:18,540 --> 00:26:27,780
eight but like do we will you have like

00:26:26,430 --> 00:26:30,120
some kind of like a convolution

00:26:27,780 --> 00:26:32,820
operation directly implemented in mkl

00:26:30,120 --> 00:26:38,430
that's like optimized like using this

00:26:32,820 --> 00:26:42,360
set of instructions mm-hmm Michelle may

00:26:38,430 --> 00:26:46,150
want to add something on this one you're

00:26:42,360 --> 00:26:48,190
asking about MK oh there's also MK l DNN

00:26:46,150 --> 00:26:49,660
but hang in Piraeus word for Intel and

00:26:48,190 --> 00:26:50,710
actually for this question I think I

00:26:49,660 --> 00:26:53,320
know the answer

00:26:50,710 --> 00:26:55,450
so the mkl DNN is the extension of the

00:26:53,320 --> 00:26:57,400
mkl library for these deep neural

00:26:55,450 --> 00:26:59,530
network operations and it already has

00:26:57,400 --> 00:27:02,290
support for in tight operations so we're

00:26:59,530 --> 00:27:04,600
adding them as adding new algorithms as

00:27:02,290 --> 00:27:07,830
the architecture evolves and the Vienna

00:27:04,600 --> 00:27:07,830
nine instructions are already there

00:27:09,720 --> 00:27:15,280
so I guess these instructions can be

00:27:12,280 --> 00:27:17,640
used not only through your tool but for

00:27:15,280 --> 00:27:20,140
example a compiler could can generate

00:27:17,640 --> 00:27:26,620
assembly or the binary code that uses

00:27:20,140 --> 00:27:28,510
them yeah yes so so open Vino is one of

00:27:26,620 --> 00:27:31,810
the software solutions that we provide

00:27:28,510 --> 00:27:35,010
that's already taking advantage of in

00:27:31,810 --> 00:27:37,900
Tait and this process of converting

00:27:35,010 --> 00:27:40,540
models to int eight is actually kind of

00:27:37,900 --> 00:27:42,970
special because it's almost automatic

00:27:40,540 --> 00:27:47,340
it's almost like magic you don't need a

00:27:42,970 --> 00:27:50,050
data scientist to to tweak it manually

00:27:47,340 --> 00:27:52,900
so that part is only open we know but

00:27:50,050 --> 00:27:56,290
the mkl DNN instructions are available

00:27:52,900 --> 00:27:59,650
for anybody and there's other solutions

00:27:56,290 --> 00:28:02,440
that we're also working on at Intel for

00:27:59,650 --> 00:28:04,540
example a graph compiler called n graph

00:28:02,440 --> 00:28:07,260
it's also taking advantage of these

00:28:04,540 --> 00:28:11,620
instructions through the MPL DNN

00:28:07,260 --> 00:28:14,440
implementation so yes many front-ends

00:28:11,620 --> 00:28:17,260
can use this the direct optimization

00:28:14,440 --> 00:28:17,890
that Intel provides for libraries like

00:28:17,260 --> 00:28:20,320
tensorflow

00:28:17,890 --> 00:28:23,400
will also be using these instructions if

00:28:20,320 --> 00:28:32,200
you compile it with the Vienna and I

00:28:23,400 --> 00:28:37,060
Architecture extensions so yeah one last

00:28:32,200 --> 00:28:40,150
very quick question if not we can thank

00:28:37,060 --> 00:28:43,440
the speaker again the speakers thank you

00:28:40,150 --> 00:28:43,440

YouTube URL: https://www.youtube.com/watch?v=3MSWONJMOK0


