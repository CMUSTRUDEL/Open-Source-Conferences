Title: Christoph Heer - Is it me, or the GIL?
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"Is it me, or the GIL?
[EuroPython 2019 - Talk - 2019-07-10 - Shanghai]
[Basel, CH]

By Christoph Heer

Python's Global Interpreter Lock is a friend and rival at the same time. We, as developers, can focus on the design and implementation of applications without the hassle of memory management. On the other side, we complain about the GIL as the limiting factor of performance sensitive applications. Therefore, it is common to refactor parts of systems when the system doesn't perform or scale enough anymore. The refactoring often includes the switch of the used concurrency paradigms like replacing multithreading with multiprocessing or asyncio. Another option is moving logic of CPU-bound workload into C extensions or a full rewrite in a ""GIL-free"" language. But how do you know that the GIL is the actual performance bottleneck?

While scaling and developing performance sensitive components in Python, my colleagues and I often also assumed the GIL as cause of our performance problems because it is a common and simple answer for this usually complex and varied problems. Instead of starting a rewrite or major refactoring, we took a step back and tried to prove our assumption. With the result that analyzing the impact of the GIL contention on the overall performance is a very interesting problem without common practices or easy usable set of tools that support Python developers.  Within this talk, I will share and explain the methods and tools, which we use to analyze the relevance of the GIL on our application performance and how it helped us to stay focused on the actual problematic areas of our applications that required improvements to meet our performance goals.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:03,620 --> 00:00:11,629
context mom the background is that s was

00:00:09,980 --> 00:00:15,110
already introduced I'm actually doing

00:00:11,629 --> 00:00:17,810
quality insurance with an SP for one of

00:00:15,110 --> 00:00:20,869
SP s major products as a B Hana which is

00:00:17,810 --> 00:00:22,490
an in-memory database and it's basically

00:00:20,869 --> 00:00:24,380
powering the various enterprise

00:00:22,490 --> 00:00:26,929
applications you can find in the

00:00:24,380 --> 00:00:29,779
portfolio of SP what we are actually

00:00:26,929 --> 00:00:32,450
doing is we are testing each commit

00:00:29,779 --> 00:00:34,160
coming in to the source code and in our

00:00:32,450 --> 00:00:37,340
scale that means we are testing around

00:00:34,160 --> 00:00:40,310
800 commits every day therefore we are

00:00:37,340 --> 00:00:43,610
operating small infrastructure or

00:00:40,310 --> 00:00:47,240
physical Hardware with around 1600

00:00:43,610 --> 00:00:49,340
machines and as we are testing an

00:00:47,240 --> 00:00:51,500
in-memory database we need a huge amount

00:00:49,340 --> 00:00:55,100
of memory so overall we are currently

00:00:51,500 --> 00:01:00,050
using around 610 terabyte of memory

00:00:55,100 --> 00:01:02,390
across our landscape so the problem is

00:01:00,050 --> 00:01:04,699
if you have such an infrastructure if

00:01:02,390 --> 00:01:06,830
you have such load characteristics you

00:01:04,699 --> 00:01:09,979
need optimized services optimized tools

00:01:06,830 --> 00:01:11,780
to handle such incoming load and that's

00:01:09,979 --> 00:01:14,090
actually the main part what my

00:01:11,780 --> 00:01:16,280
colleagues and I are doing so we are

00:01:14,090 --> 00:01:18,789
developing such tools such services

00:01:16,280 --> 00:01:22,340
which are optimized for our workload and

00:01:18,789 --> 00:01:25,490
one of this tool is our own task

00:01:22,340 --> 00:01:28,249
execution framework which we put on top

00:01:25,490 --> 00:01:30,409
of Apache mesos Apache mesos is

00:01:28,249 --> 00:01:33,799
something similar to kubernetes but a

00:01:30,409 --> 00:01:37,789
bit more low-level so what it provides

00:01:33,799 --> 00:01:40,280
us is some kind of an interface to 3

00:01:37,789 --> 00:01:41,960
sources of machines so in the bottom you

00:01:40,280 --> 00:01:44,060
can see that we have multiple data

00:01:41,960 --> 00:01:46,670
centers with our own physical hardware

00:01:44,060 --> 00:01:50,060
but also various cloud providers with

00:01:46,670 --> 00:01:53,479
cloud instances every time instance has

00:01:50,060 --> 00:01:56,359
some resources available they will send

00:01:53,479 --> 00:01:58,700
them over petty methyls to our own task

00:01:56,359 --> 00:02:01,039
scheduler and our task scheduler then

00:01:58,700 --> 00:02:03,350
has to decide ok what kind of task

00:02:01,039 --> 00:02:05,850
should I now scheduled on disobey

00:02:03,350 --> 00:02:09,729
resources

00:02:05,850 --> 00:02:12,790
now the problem is if you add more and

00:02:09,729 --> 00:02:16,800
more machines this also gets a bit more

00:02:12,790 --> 00:02:19,959
complicated in especially in that case

00:02:16,800 --> 00:02:22,480
our task as we see is receiving more and

00:02:19,959 --> 00:02:24,820
more incoming offers with more and more

00:02:22,480 --> 00:02:27,550
events about changing states of tasks

00:02:24,820 --> 00:02:29,260
sometimes finish maybe so you have not

00:02:27,550 --> 00:02:31,720
the opportunity to announce a new task

00:02:29,260 --> 00:02:33,550
or a task fail so you have to reschedule

00:02:31,720 --> 00:02:36,160
it and you have to handle all this

00:02:33,550 --> 00:02:39,310
events the problem is at some point in

00:02:36,160 --> 00:02:41,380
time we hit a bottleneck that our

00:02:39,310 --> 00:02:43,870
scheduling system was not able actually

00:02:41,380 --> 00:02:45,970
to handle all the incoming events and

00:02:43,870 --> 00:02:46,150
use all the resources in an efficient

00:02:45,970 --> 00:02:48,790
way

00:02:46,150 --> 00:02:51,760
and now you can imagine with such amount

00:02:48,790 --> 00:02:55,420
of hardware you actually would like to

00:02:51,760 --> 00:02:57,250
use them as efficient as possible you

00:02:55,420 --> 00:02:58,840
can also see that around our task

00:02:57,250 --> 00:03:01,630
scheduler have various other services

00:02:58,840 --> 00:03:03,250
and databases so the task scheduler is

00:03:01,630 --> 00:03:05,320
basically just interacting with all

00:03:03,250 --> 00:03:07,780
these various services and at doing a

00:03:05,320 --> 00:03:10,080
lot of i/o operations therefore the

00:03:07,780 --> 00:03:13,750
initial design of the task scheduling

00:03:10,080 --> 00:03:16,450
the task scheduler is basically a big

00:03:13,750 --> 00:03:19,450
Python application with various threads

00:03:16,450 --> 00:03:22,269
who are handing the incoming data were

00:03:19,450 --> 00:03:26,079
processing the incoming data and which

00:03:22,269 --> 00:03:30,340
find the best way to schedule a certain

00:03:26,079 --> 00:03:32,290
task on a certain waiver resource off we

00:03:30,340 --> 00:03:34,920
also have other threads around in

00:03:32,290 --> 00:03:37,959
process itself which require for

00:03:34,920 --> 00:03:40,570
observability snake for example we have

00:03:37,959 --> 00:03:43,410
a fred who is responsible to transmit

00:03:40,570 --> 00:03:46,150
all accept to data to send free or

00:03:43,410 --> 00:03:48,670
disappear the tracing is in place for

00:03:46,150 --> 00:03:51,010
our applications so we have also Fred

00:03:48,670 --> 00:03:54,760
who's responsible to send that over to a

00:03:51,010 --> 00:03:57,579
giga instance now we have the problem

00:03:54,760 --> 00:03:59,890
that we have not the best performance

00:03:57,579 --> 00:04:02,739
animal research civilization goes down

00:03:59,890 --> 00:04:04,329
and another thing is about our opposite

00:04:02,739 --> 00:04:07,780
ability stacking that we can now

00:04:04,329 --> 00:04:09,670
actually inspect each fret and each part

00:04:07,780 --> 00:04:12,340
of the system and find out okay where's

00:04:09,670 --> 00:04:14,440
the bottleneck and the same method we

00:04:12,340 --> 00:04:17,500
also applied to find out okay why we

00:04:14,440 --> 00:04:19,510
cannot utilize all the vapor resources

00:04:17,500 --> 00:04:22,350
therefore we have to take a look inside

00:04:19,510 --> 00:04:26,170
of the resource of a handling friend and

00:04:22,350 --> 00:04:28,030
that's now semantic visualization of our

00:04:26,170 --> 00:04:30,400
disability tracing system which we are

00:04:28,030 --> 00:04:32,980
currently using and we can see the

00:04:30,400 --> 00:04:36,100
required time for certain operations so

00:04:32,980 --> 00:04:39,490
for example we see how long we need for

00:04:36,100 --> 00:04:41,290
selecting an offer for tasks or how long

00:04:39,490 --> 00:04:44,010
we need now to prepare actually the

00:04:41,290 --> 00:04:47,710
tasks so that it can be scheduled and

00:04:44,010 --> 00:04:50,440
the first strange thing is that for the

00:04:47,710 --> 00:04:52,990
same function we have actually different

00:04:50,440 --> 00:04:56,230
runtimes so for example for selecting

00:04:52,990 --> 00:04:59,950
offer we have a variant of various run

00:04:56,230 --> 00:05:02,230
times from 700 milliseconds down to 30

00:04:59,950 --> 00:05:05,050
milliseconds which is actually quite

00:05:02,230 --> 00:05:09,310
strange the next interesting thing is

00:05:05,050 --> 00:05:13,710
that we have also increased latency in

00:05:09,310 --> 00:05:17,770
in a way that it's extra not expected so

00:05:13,710 --> 00:05:20,440
what you see in in the highlighted boxes

00:05:17,770 --> 00:05:23,080
is the first thing which is the span

00:05:20,440 --> 00:05:25,990
which is captured on the schedule 11 so

00:05:23,080 --> 00:05:29,020
yeah capturing that we need around 200

00:05:25,990 --> 00:05:31,090
milliseconds for this API call the

00:05:29,020 --> 00:05:33,040
service which we are actually asking is

00:05:31,090 --> 00:05:35,200
also transmitting the data into the same

00:05:33,040 --> 00:05:37,330
system therefore we know that the

00:05:35,200 --> 00:05:40,440
service itself only took around 30

00:05:37,330 --> 00:05:44,500
milliseconds to process the API request

00:05:40,440 --> 00:05:46,720
so that's also a bit strange and we

00:05:44,500 --> 00:05:50,650
would also like to investigate it there

00:05:46,720 --> 00:05:52,960
a bit the next remaining thing which is

00:05:50,650 --> 00:05:56,500
also strange in this capture is there

00:05:52,960 --> 00:05:58,570
are also gaps between operations and if

00:05:56,500 --> 00:06:01,150
you take a look into the code that

00:05:58,570 --> 00:06:03,510
actually no gaps inside of the prepared

00:06:01,150 --> 00:06:06,250
tasks operation there are two

00:06:03,510 --> 00:06:08,800
separations this two API calls there's

00:06:06,250 --> 00:06:11,979
nothing in between why is there latency

00:06:08,800 --> 00:06:16,810
of multiple milliseconds in between and

00:06:11,979 --> 00:06:19,750
then we started to assume ok I mean we

00:06:16,810 --> 00:06:21,250
started we are using threads it must be

00:06:19,750 --> 00:06:22,620
the global interpreter lock we are

00:06:21,250 --> 00:06:25,840
hitting the global interpreter lock

00:06:22,620 --> 00:06:29,800
there's a contention and that's now our

00:06:25,840 --> 00:06:32,979
bottleneck but I mean that's no problem

00:06:29,800 --> 00:06:34,840
you just open a browser and perform some

00:06:32,979 --> 00:06:38,830
research and then you will find a lot of

00:06:34,840 --> 00:06:40,990
various ways how to mitigate the global

00:06:38,830 --> 00:06:42,970
interpreter lock contention so we could

00:06:40,990 --> 00:06:45,039
just start and replace all the modify

00:06:42,970 --> 00:06:49,060
ting thing with multi processing or a

00:06:45,039 --> 00:06:50,979
CIO staff or we could pinpoint certain

00:06:49,060 --> 00:06:53,710
see people found a certain function

00:06:50,979 --> 00:06:56,229
which CPU bound and Mike Ram to cipher

00:06:53,710 --> 00:07:00,310
which are actually releasing the

00:06:56,229 --> 00:07:03,159
killings and circumstances let's rewrite

00:07:00,310 --> 00:07:04,870
everything in a more faster language but

00:07:03,159 --> 00:07:06,460
if that would be now the solution I

00:07:04,870 --> 00:07:09,819
would probably not talk now about the

00:07:06,460 --> 00:07:12,879
global interpreter lock in general we

00:07:09,819 --> 00:07:15,669
have to say that such rewrites on major

00:07:12,879 --> 00:07:17,409
infections as super expensive I mean

00:07:15,669 --> 00:07:19,419
we're talking about a productive system

00:07:17,409 --> 00:07:21,879
who is powering a huge amount of

00:07:19,419 --> 00:07:24,340
workload and we would actually like to

00:07:21,879 --> 00:07:27,490
invest more time on new features make it

00:07:24,340 --> 00:07:29,530
even more efficient but now handling

00:07:27,490 --> 00:07:31,659
such performance problems is also

00:07:29,530 --> 00:07:34,449
important but we have to use the right

00:07:31,659 --> 00:07:39,009
things to actually solve the performance

00:07:34,449 --> 00:07:41,830
problems therefore we took one step back

00:07:39,009 --> 00:07:44,349
and decided ok let's first analyze the

00:07:41,830 --> 00:07:46,960
problem find out is it actually to give

00:07:44,349 --> 00:07:49,599
a consumption and if it done detected

00:07:46,960 --> 00:07:50,409
that we can actually go and maybe it is

00:07:49,599 --> 00:07:53,409
that even better

00:07:50,409 --> 00:07:56,979
what kind of mitigation is the best one

00:07:53,409 --> 00:07:59,860
for our application so let's take a look

00:07:56,979 --> 00:08:02,229
on the gear I mean it's probably easy

00:07:59,860 --> 00:08:04,449
you just import some module for example

00:08:02,229 --> 00:08:06,370
sis for the Python interpreter and then

00:08:04,449 --> 00:08:09,279
you ask the Python interpreter how it's

00:08:06,370 --> 00:08:09,759
going with together sadly that it's not

00:08:09,279 --> 00:08:11,740
true

00:08:09,759 --> 00:08:15,029
there is no simple function which you

00:08:11,740 --> 00:08:17,860
can use to get some guild statistics

00:08:15,029 --> 00:08:20,289
which means that we have to think about

00:08:17,860 --> 00:08:23,409
something else and then I thought a bit

00:08:20,289 --> 00:08:27,729
about ok what I would actually like is

00:08:23,409 --> 00:08:30,400
to know about a girl are various things

00:08:27,729 --> 00:08:34,149
the first thing I would like to know the

00:08:30,400 --> 00:08:36,130
basic fundamental metrics of a lock I

00:08:34,149 --> 00:08:38,320
mean at the end the global interpreter

00:08:36,130 --> 00:08:40,959
lock it's a lock so I would like to know

00:08:38,320 --> 00:08:43,580
how long does a threat actually wait

00:08:40,959 --> 00:08:47,149
fotolog and how long is

00:08:43,580 --> 00:08:49,550
that is actually holding that lock if I

00:08:47,149 --> 00:08:51,620
know that matrix and I mean we all know

00:08:49,550 --> 00:08:54,080
that if you have just some numbers they

00:08:51,620 --> 00:08:55,490
are often not very useful therefore I

00:08:54,080 --> 00:08:59,540
would also like to have some additional

00:08:55,490 --> 00:09:01,519
context okay which Fred it is which -

00:08:59,540 --> 00:09:03,890
function is actually suffering from that

00:09:01,519 --> 00:09:06,829
contention maybe is it may be possible

00:09:03,890 --> 00:09:08,630
to get trace ID or request ID in that

00:09:06,829 --> 00:09:12,050
part because then I can correlate that

00:09:08,630 --> 00:09:13,760
with other systems and I would actually

00:09:12,050 --> 00:09:16,250
like to use this in our productive

00:09:13,760 --> 00:09:18,500
environment because I cannot reproduce

00:09:16,250 --> 00:09:19,910
that in my local machine because my

00:09:18,500 --> 00:09:22,850
local machine is not connected to a

00:09:19,910 --> 00:09:26,990
cluster with multiple hundreds of

00:09:22,850 --> 00:09:30,200
machines so that's a problem and in best

00:09:26,990 --> 00:09:33,019
case it would be integrated with our

00:09:30,200 --> 00:09:34,490
existing observability stake so that my

00:09:33,019 --> 00:09:36,140
colleagues and I don't have to learn

00:09:34,490 --> 00:09:38,380
another tool how to use it how to

00:09:36,140 --> 00:09:42,170
interact with it

00:09:38,380 --> 00:09:44,420
so with that list I went through the

00:09:42,170 --> 00:09:47,180
internet and looked at what are the

00:09:44,420 --> 00:09:48,740
variables how can I analyze I mean I'm

00:09:47,180 --> 00:09:52,820
probably not the first person who is

00:09:48,740 --> 00:09:54,800
thinking about that problem so there are

00:09:52,820 --> 00:09:58,070
some relatable there's for example a

00:09:54,800 --> 00:10:05,000
super interesting talk by Dave Paisley

00:09:58,070 --> 00:10:06,740
from - 2010 sort of PyCon 2010 but it's

00:10:05,000 --> 00:10:09,010
actually quite up to date because it

00:10:06,740 --> 00:10:11,690
already talks about the nuclear

00:10:09,010 --> 00:10:15,770
implementation which we have - we talk

00:10:11,690 --> 00:10:19,339
to and resonance his talk he's

00:10:15,770 --> 00:10:22,040
explaining how he measured the guild

00:10:19,339 --> 00:10:24,260
contention and stored the data so that

00:10:22,040 --> 00:10:27,290
he actually was able to generate such

00:10:24,260 --> 00:10:29,240
nice craft the problem is the

00:10:27,290 --> 00:10:31,760
instrumentation and so on only works

00:10:29,240 --> 00:10:34,130
with Python one sakes and there are some

00:10:31,760 --> 00:10:35,899
other problems which make me not so

00:10:34,130 --> 00:10:37,790
useful in our productive environment

00:10:35,899 --> 00:10:39,949
main problem you have to shut down the

00:10:37,790 --> 00:10:42,070
interpreter at the end to dump all the

00:10:39,949 --> 00:10:46,279
data out and then you have to generate

00:10:42,070 --> 00:10:49,010
the visualization out of it I actually

00:10:46,279 --> 00:10:51,320
don't like to cut down our scheduler

00:10:49,010 --> 00:10:52,850
because I mean every minute it is not

00:10:51,320 --> 00:10:56,329
running we are basically losing

00:10:52,850 --> 00:10:56,870
resources the next thing which came into

00:10:56,329 --> 00:10:59,690
my mind

00:10:56,870 --> 00:11:01,880
as the Fred concurrency visualization of

00:10:59,690 --> 00:11:04,430
patron which is actually a very nice

00:11:01,880 --> 00:11:05,210
tool to visualize local tensions in your

00:11:04,430 --> 00:11:08,029
application

00:11:05,210 --> 00:11:11,360
the problem is patron doesn't take the

00:11:08,029 --> 00:11:14,630
girl into account in that example I just

00:11:11,360 --> 00:11:18,710
run application who is heavily guilt

00:11:14,630 --> 00:11:23,690
bound because it's basically always

00:11:18,710 --> 00:11:27,980
wasting time and in CPU cycles but you

00:11:23,690 --> 00:11:31,160
don't see anything there's a bit there's

00:11:27,980 --> 00:11:33,260
a new implementation called Gil load

00:11:31,160 --> 00:11:35,570
which is quite interesting because it's

00:11:33,260 --> 00:11:39,230
a profile which you can easily integrate

00:11:35,570 --> 00:11:41,779
in un existent - application and during

00:11:39,230 --> 00:11:44,300
the application runtime it will print

00:11:41,779 --> 00:11:48,190
out a load number it's comparable with

00:11:44,300 --> 00:11:50,930
the load of a Linux system for example

00:11:48,190 --> 00:11:52,610
the problem is at the end you have only

00:11:50,930 --> 00:11:55,700
in number you don't know okay what is

00:11:52,610 --> 00:11:59,089
now the root cause of that problem so if

00:11:55,700 --> 00:12:01,430
they do that on a application who is

00:11:59,089 --> 00:12:03,589
heavily bound to the girl and who ones

00:12:01,430 --> 00:12:05,270
always into the same contention I just

00:12:03,589 --> 00:12:07,820
get information yes you have a problem

00:12:05,270 --> 00:12:13,550
but no other information how to solve

00:12:07,820 --> 00:12:17,150
that a very promising approach is pies

00:12:13,550 --> 00:12:20,990
buyer which is a new profiler for Python

00:12:17,150 --> 00:12:22,310
written in rust which is very fast and

00:12:20,990 --> 00:12:25,070
you can easily attach it to a running

00:12:22,310 --> 00:12:27,980
application and then you can get this

00:12:25,070 --> 00:12:30,650
nice overview about what functions take

00:12:27,980 --> 00:12:34,010
what what amount of time and so on and

00:12:30,650 --> 00:12:37,010
it also includes to kill utilization

00:12:34,010 --> 00:12:39,920
which is quite nice but again you don't

00:12:37,010 --> 00:12:43,779
have a break down to find out okay well

00:12:39,920 --> 00:12:43,779
what is not a problem about it

00:12:44,589 --> 00:12:51,589
the truth is there is no magic your

00:12:47,870 --> 00:12:54,339
contention analytics tool and that means

00:12:51,589 --> 00:12:58,279
we probably have to do it by ourself

00:12:54,339 --> 00:13:00,769
okay that's great a tool which actually

00:12:58,279 --> 00:13:04,699
reveals the guild and hopefully it is

00:13:00,769 --> 00:13:09,249
able to give me all my wishes about such

00:13:04,699 --> 00:13:13,069
a tool for that we can use an existing

00:13:09,249 --> 00:13:16,999
framework and bases system tab which is

00:13:13,069 --> 00:13:20,029
available on Linux machines and it

00:13:16,999 --> 00:13:23,420
allows actually to analyze applications

00:13:20,029 --> 00:13:30,079
by attaching certain event handlers to

00:13:23,420 --> 00:13:33,050
applications so sorry it actually allows

00:13:30,079 --> 00:13:35,809
us to attach handlers to certain events

00:13:33,050 --> 00:13:38,180
which are emitted by applications or by

00:13:35,809 --> 00:13:40,220
the Linux kernel itself and then you can

00:13:38,180 --> 00:13:43,209
do certain calculations within this

00:13:40,220 --> 00:13:45,170
event handlers and print out to the

00:13:43,209 --> 00:13:48,170
measure x function

00:13:45,170 --> 00:13:51,110
the nice thing cfm56

00:13:48,170 --> 00:13:54,589
actually introduced support for system

00:13:51,110 --> 00:13:57,350
tab and DTrace and they are already some

00:13:54,589 --> 00:14:02,180
markers some event emitters who you can

00:13:57,350 --> 00:14:04,370
use to to analyze a patent application

00:14:02,180 --> 00:14:06,740
in the system tab so for example we have

00:14:04,370 --> 00:14:07,459
function entry and function return which

00:14:06,740 --> 00:14:09,079
will be involved

00:14:07,459 --> 00:14:11,600
every time the interpreter goes into a

00:14:09,079 --> 00:14:14,689
new Python function or returns from -

00:14:11,600 --> 00:14:16,579
function I can highly recommend the

00:14:14,689 --> 00:14:19,220
documentation about that because it's

00:14:16,579 --> 00:14:21,199
super verbose and helps really to

00:14:19,220 --> 00:14:23,809
understand the concepts of system tab

00:14:21,199 --> 00:14:25,399
and how you can use it and I also saw in

00:14:23,809 --> 00:14:27,500
the schedule of this conference there's

00:14:25,399 --> 00:14:32,120
another talk about low level profiling

00:14:27,500 --> 00:14:34,009
which will also cover system to problem

00:14:32,120 --> 00:14:36,290
about this approaches that most

00:14:34,009 --> 00:14:38,059
Freebirds Linux packages actually don't

00:14:36,290 --> 00:14:40,279
include the Python interpreter which is

00:14:38,059 --> 00:14:42,139
compiled with d'être support so the

00:14:40,279 --> 00:14:46,809
markets are not there you cannot use

00:14:42,139 --> 00:14:49,279
them also dinamarca for the Gil related

00:14:46,809 --> 00:14:52,639
areas where you would actually like to

00:14:49,279 --> 00:14:55,990
know when if read acquired something or

00:14:52,639 --> 00:14:59,589
when effort is actually dropping

00:14:55,990 --> 00:15:01,660
interpreter look but especially the last

00:14:59,589 --> 00:15:05,200
part was actually not so complicated to

00:15:01,660 --> 00:15:09,700
implement so this is just one part of

00:15:05,200 --> 00:15:12,730
the patch who introduced some markets

00:15:09,700 --> 00:15:14,950
regarding the guild so every time Fred

00:15:12,730 --> 00:15:17,920
will now drop the Gil it will emit the

00:15:14,950 --> 00:15:21,279
event about that and if every time Fred

00:15:17,920 --> 00:15:25,779
tries to claimed Gil it will also emit

00:15:21,279 --> 00:15:28,630
inventor start as you can see we can

00:15:25,779 --> 00:15:30,670
also add arbitrary attributes to this

00:15:28,630 --> 00:15:33,970
markers which will be done accessible

00:15:30,670 --> 00:15:36,279
insistent in that case I'm using the

00:15:33,970 --> 00:15:38,529
Freddy dance so that I have an

00:15:36,279 --> 00:15:40,570
understanding and I have an idea what

00:15:38,529 --> 00:15:43,600
Fred is now performing actually is

00:15:40,570 --> 00:15:45,730
actually performing this action now you

00:15:43,600 --> 00:15:47,589
may be asked why you don't use the Fred

00:15:45,730 --> 00:15:49,300
names I mean every application should

00:15:47,589 --> 00:15:51,790
have a nice Fred name so that you can

00:15:49,300 --> 00:15:54,970
actually recognize the frets the problem

00:15:51,790 --> 00:15:58,270
is at the moment there is sadly no C API

00:15:54,970 --> 00:16:01,690
to get to Fred names without actually

00:15:58,270 --> 00:16:04,000
holding the Gil which is complicated

00:16:01,690 --> 00:16:06,250
complicated if you would like to measure

00:16:04,000 --> 00:16:07,899
the time until you actually have to kill

00:16:06,250 --> 00:16:10,510
but you need to kill to get the Fred

00:16:07,899 --> 00:16:11,829
names complicated but I think that's

00:16:10,510 --> 00:16:15,040
also something we can solve at some

00:16:11,829 --> 00:16:17,890
point of time okay now we have this

00:16:15,040 --> 00:16:20,050
markers now our pattern interpreter is

00:16:17,890 --> 00:16:25,000
emitting this events at the point in

00:16:20,050 --> 00:16:28,209
time they require passed let's measure

00:16:25,000 --> 00:16:31,149
the time at that point we attaching

00:16:28,209 --> 00:16:34,600
probes so called event handlers to this

00:16:31,149 --> 00:16:36,970
events for example we're instructing

00:16:34,600 --> 00:16:38,890
system tab that it now should look

00:16:36,970 --> 00:16:41,140
inside of the shared library of the

00:16:38,890 --> 00:16:45,100
Python interpreter where the markers are

00:16:41,140 --> 00:16:47,740
located and for example if we now go in

00:16:45,100 --> 00:16:50,110
the claim pod so a Fred would like to

00:16:47,740 --> 00:16:53,350
acquire the care we will measure the

00:16:50,110 --> 00:16:56,800
time of today in nanoseconds and started

00:16:53,350 --> 00:16:59,560
in an hashmap of the center s key we are

00:16:56,800 --> 00:17:01,089
using the Fred identifier if you're now

00:16:59,560 --> 00:17:03,820
required to kill we can actually

00:17:01,089 --> 00:17:07,179
calculate how long does it take until

00:17:03,820 --> 00:17:09,670
the Fred acquired occur and also started

00:17:07,179 --> 00:17:13,150
in another data type of system tab

00:17:09,670 --> 00:17:15,490
which is an aggregate which allows us to

00:17:13,150 --> 00:17:18,580
actually get some statistics out of

00:17:15,490 --> 00:17:21,820
system type so for example distributions

00:17:18,580 --> 00:17:24,760
in the form of a histogram or an average

00:17:21,820 --> 00:17:27,400
and so on and the same thing we also do

00:17:24,760 --> 00:17:31,210
for the guild for where we can actually

00:17:27,400 --> 00:17:36,190
then now calculate how long we actually

00:17:31,210 --> 00:17:38,080
hold again now we are calculating these

00:17:36,190 --> 00:17:41,370
numbers what we actually would like is

00:17:38,080 --> 00:17:44,110
to print them out in some kind of report

00:17:41,370 --> 00:17:45,990
for that we can use the handlers which

00:17:44,110 --> 00:17:48,550
will be invoked at a startup and

00:17:45,990 --> 00:17:52,510
determination of the system tracing

00:17:48,550 --> 00:17:56,110
session so if the tracing session stops

00:17:52,510 --> 00:18:00,250
now we can print out some nice debugging

00:17:56,110 --> 00:18:02,920
informations terminating the tracing and

00:18:00,250 --> 00:18:05,110
print out a summary of overall the

00:18:02,920 --> 00:18:08,890
measured frets with the respective

00:18:05,110 --> 00:18:11,560
timings let's do some example in the

00:18:08,890 --> 00:18:16,270
first example I have a Python process

00:18:11,560 --> 00:18:18,820
with two i/o bound frets by dad I opened

00:18:16,270 --> 00:18:21,280
or how do I actually make an i/o bond

00:18:18,820 --> 00:18:23,140
fret is by simulating the i/o with

00:18:21,280 --> 00:18:25,600
hundreds sleep because it's basically

00:18:23,140 --> 00:18:29,500
the same behavior as for example a read

00:18:25,600 --> 00:18:32,620
request on a socket before the actually

00:18:29,500 --> 00:18:36,250
the fret goes into the sleep mode for a

00:18:32,620 --> 00:18:39,220
certain number of seconds it will

00:18:36,250 --> 00:18:42,220
actually release the game after the

00:18:39,220 --> 00:18:45,160
sleep is complete it will try to

00:18:42,220 --> 00:18:47,110
reacquire it again the same thing also

00:18:45,160 --> 00:18:49,750
happens on a socket you would like to

00:18:47,110 --> 00:18:51,070
read something from a socket you define

00:18:49,750 --> 00:18:53,260
the amount of bytes we would like to

00:18:51,070 --> 00:18:55,750
read as long as they're not enough bytes

00:18:53,260 --> 00:18:58,120
or actually no data at all available it

00:18:55,750 --> 00:19:00,870
will pluck and it will wait but before

00:18:58,120 --> 00:19:04,030
that Python will release little for you

00:19:00,870 --> 00:19:06,850
if we now measure that we can actually

00:19:04,030 --> 00:19:09,880
see that we have actually don't have any

00:19:06,850 --> 00:19:12,670
problem of death application you can see

00:19:09,880 --> 00:19:14,860
that the main thread only had to wait a

00:19:12,670 --> 00:19:16,450
bit more than one millisecond on the

00:19:14,860 --> 00:19:18,670
global interpreter lock which is quite

00:19:16,450 --> 00:19:20,770
nice especially if you know that this

00:19:18,670 --> 00:19:23,350
application runs for 15 seconds and

00:19:20,770 --> 00:19:25,760
their simulation mode

00:19:23,350 --> 00:19:28,760
if you are looking on on the whole time

00:19:25,760 --> 00:19:30,890
we also see that the main threat is the

00:19:28,760 --> 00:19:32,720
most prominent and Fred who holds

00:19:30,890 --> 00:19:34,880
together the most of the time which also

00:19:32,720 --> 00:19:37,070
makes sense it has to import the

00:19:34,880 --> 00:19:40,070
libraries to fretting library and so on

00:19:37,070 --> 00:19:44,120
so the most time will go probably foot

00:19:40,070 --> 00:19:46,580
in initialization dial frets itself a

00:19:44,120 --> 00:19:49,279
super lightweight so they don't have to

00:19:46,580 --> 00:19:52,460
wait that long on the girl and if they

00:19:49,279 --> 00:19:54,350
have to get then also they don't consume

00:19:52,460 --> 00:19:59,539
that much time with the Python

00:19:54,350 --> 00:20:02,480
interpreter overall we can say we hold

00:19:59,539 --> 00:20:05,659
the time only 0.2 percent of the full

00:20:02,480 --> 00:20:11,120
runtime on and we also had only a way

00:20:05,659 --> 00:20:11,990
time of less than 0.1 0.01 percent of

00:20:11,120 --> 00:20:15,289
the full run time

00:20:11,990 --> 00:20:17,510
so basically no kill contention at all

00:20:15,289 --> 00:20:23,330
and in that situations the kill is not a

00:20:17,510 --> 00:20:27,350
problem we can now change that easily if

00:20:23,330 --> 00:20:29,929
we go and introduce a cpu-bound fred how

00:20:27,350 --> 00:20:31,610
do we simulate a cpu bomb threat it's

00:20:29,929 --> 00:20:34,190
actually quite simple we just need an

00:20:31,610 --> 00:20:36,919
endless loop who is doing nothing so

00:20:34,190 --> 00:20:39,470
this while loop with a pass inside of

00:20:36,919 --> 00:20:41,630
the loop itself will do all the trick

00:20:39,470 --> 00:20:44,570
for us and we now have a new CPU bound

00:20:41,630 --> 00:20:47,179
front if we now take a look on the

00:20:44,570 --> 00:20:50,260
timings we can actually see a gear

00:20:47,179 --> 00:20:53,090
contention we see that the main thread

00:20:50,260 --> 00:20:56,270
has the same behavior as before same

00:20:53,090 --> 00:20:59,809
whole time but increased wait time okay

00:20:56,270 --> 00:21:02,270
that's already a problem problem the

00:20:59,809 --> 00:21:06,200
most important problem is that our Oh

00:21:02,270 --> 00:21:08,450
Fred now have a much higher wait time to

00:21:06,200 --> 00:21:09,649
get actually to kill so we are now

00:21:08,450 --> 00:21:12,230
waiting more than seven hundred

00:21:09,649 --> 00:21:18,740
milliseconds just forgetting the Gil and

00:21:12,230 --> 00:21:22,309
we actually are only waiting around 100

00:21:18,740 --> 00:21:26,240
milliseconds in our application the

00:21:22,309 --> 00:21:30,370
whole time is still the same and the CPU

00:21:26,240 --> 00:21:33,320
fret which is CPU bound is actually

00:21:30,370 --> 00:21:35,909
consuming all the available CPU time and

00:21:33,320 --> 00:21:39,749
therefore holding the kill nearly two

00:21:35,909 --> 00:21:43,589
RAAA so overall we now see that the girl

00:21:39,749 --> 00:21:47,339
was basically always active by any kind

00:21:43,589 --> 00:21:51,389
of by at least one fret and we also had

00:21:47,339 --> 00:21:54,179
some kind some wait time in there even

00:21:51,389 --> 00:21:56,940
more interesting is the latency for

00:21:54,179 --> 00:22:00,659
die-off rats if you are taking a look on

00:21:56,940 --> 00:22:02,909
the histogram after i/o frets then you

00:22:00,659 --> 00:22:05,549
will basically notice after some time

00:22:02,909 --> 00:22:10,109
that the latency is quite stable between

00:22:05,549 --> 00:22:13,319
4 & 8 milliseconds and that's quite

00:22:10,109 --> 00:22:16,619
interesting that it's so stable but it

00:22:13,319 --> 00:22:20,009
also shows us one main disadvantage of

00:22:16,619 --> 00:22:22,199
the skill contention the gill contention

00:22:20,009 --> 00:22:25,979
already effects the overall performance

00:22:22,199 --> 00:22:29,599
of the application by introducing 5

00:22:25,979 --> 00:22:34,319
milliseconds additional latency to any

00:22:29,599 --> 00:22:38,309
unlock of the global sorry any attempt

00:22:34,319 --> 00:22:42,569
to acquire to get after plotting my

00:22:38,309 --> 00:22:45,059
operation which is actually quite a

00:22:42,569 --> 00:22:47,159
problem in some cases because normally

00:22:45,059 --> 00:22:50,129
you run multiple i/o further array IO

00:22:47,159 --> 00:22:54,269
operations during for example an HP

00:22:50,129 --> 00:22:57,179
request why it's so stable because of

00:22:54,269 --> 00:22:59,789
the internal switch interval which is an

00:22:57,179 --> 00:23:01,499
implementation detail of the girl but

00:22:59,789 --> 00:23:05,579
it's actually quite interesting to see

00:23:01,499 --> 00:23:07,679
that in action every time a fret would

00:23:05,579 --> 00:23:09,719
like to acquire the girl it will check

00:23:07,679 --> 00:23:11,969
if someone is holding the gear if

00:23:09,719 --> 00:23:15,539
someone is holding the guild then it

00:23:11,969 --> 00:23:20,039
will go into an condition and will sleep

00:23:15,539 --> 00:23:22,499
up to 5 milliseconds if now the girl is

00:23:20,039 --> 00:23:24,929
still acquired it will send out a

00:23:22,499 --> 00:23:26,909
request to drop the girl so that the

00:23:24,929 --> 00:23:29,429
other week a deferred who's still

00:23:26,909 --> 00:23:32,190
holding the girl should please release

00:23:29,429 --> 00:23:36,779
the girl so that the new Fred can take

00:23:32,190 --> 00:23:40,589
over the club renovating that means this

00:23:36,779 --> 00:23:43,259
can add this already this additional

00:23:40,589 --> 00:23:46,049
latency if you only have Python

00:23:43,259 --> 00:23:48,389
instructions then you will basically

00:23:46,049 --> 00:23:49,660
always get this table latency of around

00:23:48,389 --> 00:23:52,809
5 milliseconds

00:23:49,660 --> 00:23:55,360
the problem is a Fred who's holding the

00:23:52,809 --> 00:23:58,539
Gil can also hold the kill even longer

00:23:55,360 --> 00:24:00,400
than this five milliseconds because some

00:23:58,539 --> 00:24:02,470
bad code operations take longer or

00:24:00,400 --> 00:24:04,059
you're calling out internet stone Steve

00:24:02,470 --> 00:24:06,309
functions it actually doesn't know

00:24:04,059 --> 00:24:09,970
anything about the kill and also don't

00:24:06,309 --> 00:24:13,240
release the girl okay

00:24:09,970 --> 00:24:15,130
now with that we have some tools search

00:24:13,240 --> 00:24:18,250
which we can use to analyze our

00:24:15,130 --> 00:24:20,980
application or productive application so

00:24:18,250 --> 00:24:23,500
here's the plan how we do that we deploy

00:24:20,980 --> 00:24:27,039
our new container before a custom

00:24:23,500 --> 00:24:31,480
cpython instrument a a custom seat -

00:24:27,039 --> 00:24:34,390
version including system on our on our

00:24:31,480 --> 00:24:36,220
cluster we go to this machine which is

00:24:34,390 --> 00:24:38,020
running the scheduler we attach to the

00:24:36,220 --> 00:24:43,299
process and we get some nice inserts

00:24:38,020 --> 00:24:46,210
about the kill contention in reality was

00:24:43,299 --> 00:24:48,039
a bit different first quite easy to

00:24:46,210 --> 00:24:51,039
deploy the container with the custom

00:24:48,039 --> 00:24:53,620
seat - motion topic dear at the at the

00:24:51,039 --> 00:24:55,299
end we had to install system tab under

00:24:53,620 --> 00:24:57,490
hosts because it's actually not that

00:24:55,299 --> 00:25:00,730
easy to get system tab running inside of

00:24:57,490 --> 00:25:03,250
container and that's a bit related to

00:25:00,730 --> 00:25:06,220
the architecture of system tab system

00:25:03,250 --> 00:25:09,039
tab is actually transforming your script

00:25:06,220 --> 00:25:11,320
which I just showed some slides ago in a

00:25:09,039 --> 00:25:15,730
real kernel extension and loads that at

00:25:11,320 --> 00:25:18,039
runtime and it then it is that kind of

00:25:15,730 --> 00:25:20,770
extra tension running to measure all the

00:25:18,039 --> 00:25:26,080
timing and in the end the data will will

00:25:20,770 --> 00:25:28,510
be printed out that's possible I also

00:25:26,080 --> 00:25:30,940
did that in our protective environment

00:25:28,510 --> 00:25:34,720
it was interesting but I don't recommend

00:25:30,940 --> 00:25:37,630
it especially if you're talking with an

00:25:34,720 --> 00:25:41,470
security guy he will probably not happy

00:25:37,630 --> 00:25:43,030
if you start yeah adding custom collar

00:25:41,470 --> 00:25:46,049
extensions in your productive

00:25:43,030 --> 00:25:49,179
environment and run some processes route

00:25:46,049 --> 00:25:52,160
your folio halation is basically car

00:25:49,179 --> 00:25:56,120
nevertheless I measured some nice

00:25:52,160 --> 00:25:58,549
results over two seconds observation of

00:25:56,120 --> 00:26:01,970
our process I found out that we actually

00:25:58,549 --> 00:26:04,070
hold the gear around 88 percent of the

00:26:01,970 --> 00:26:06,950
full runtime sorry of them of the

00:26:04,070 --> 00:26:10,700
measured time frame in the same time

00:26:06,950 --> 00:26:13,400
frame there were so many threats waiting

00:26:10,700 --> 00:26:15,830
for the peer that we had an overall wait

00:26:13,400 --> 00:26:20,049
time on the gear of nearly three hundred

00:26:15,830 --> 00:26:22,190
percent which actually proves this

00:26:20,049 --> 00:26:25,760
application really suffers from giro

00:26:22,190 --> 00:26:27,640
contention it's not great but it

00:26:25,760 --> 00:26:32,270
actually reviewed even more questions

00:26:27,640 --> 00:26:34,309
the problem is on the main question

00:26:32,270 --> 00:26:35,929
which first came up is okay other

00:26:34,309 --> 00:26:40,549
threats who are holding the key longer

00:26:35,929 --> 00:26:43,460
than five milliseconds or out of orders

00:26:40,549 --> 00:26:46,429
all frets actually give up the girl

00:26:43,460 --> 00:26:48,679
quite fast and we are hitting the limit

00:26:46,429 --> 00:26:51,919
of switching around various frets that

00:26:48,679 --> 00:26:53,770
would be one possibility but let's take

00:26:51,919 --> 00:26:57,860
a look if we actually see that you know

00:26:53,770 --> 00:26:59,030
infrastructure if we see that I would

00:26:57,860 --> 00:27:00,590
actually like to know okay which

00:26:59,030 --> 00:27:02,780
function is taking so much time and

00:27:00,590 --> 00:27:07,880
which function is not releasing together

00:27:02,780 --> 00:27:10,820
and also the one question which came up

00:27:07,880 --> 00:27:13,490
was is there may be some kind of

00:27:10,820 --> 00:27:16,130
clustering in these measurements so it

00:27:13,490 --> 00:27:18,080
is may be possible to identify some

00:27:16,130 --> 00:27:20,179
clusters for certain operations at a

00:27:18,080 --> 00:27:23,030
certain patterns in this measurement

00:27:20,179 --> 00:27:27,320
that are very intensive for the global

00:27:23,030 --> 00:27:30,679
interpreter lock contention overall the

00:27:27,320 --> 00:27:33,830
most part the biggest problem was that

00:27:30,679 --> 00:27:37,850
with 31 frets it was actually quite hard

00:27:33,830 --> 00:27:41,929
to read this text report and that's also

00:27:37,850 --> 00:27:43,669
a problem with so many frets that such

00:27:41,929 --> 00:27:45,110
tools are often not optimized for that

00:27:43,669 --> 00:27:47,830
amount of frets and I would not

00:27:45,110 --> 00:27:47,830
recommend it

00:27:47,979 --> 00:27:55,219
so one thing which came into my mind is

00:27:52,219 --> 00:27:58,190
that timelines are much easier to

00:27:55,219 --> 00:28:00,289
understand I mean if you take a look on

00:27:58,190 --> 00:28:02,509
this video tracing systems they're

00:28:00,289 --> 00:28:04,700
actually quite good in visualizing how

00:28:02,509 --> 00:28:07,190
long a certain operation takes it's much

00:28:04,700 --> 00:28:09,799
easier to recognize how long this

00:28:07,190 --> 00:28:13,309
operation takes based on the size of the

00:28:09,799 --> 00:28:15,139
span individual representation okay

00:28:13,309 --> 00:28:17,450
could we do that the same thing

00:28:15,139 --> 00:28:19,190
please also for the clear contention and

00:28:17,450 --> 00:28:22,429
maybe find out what's going on in our

00:28:19,190 --> 00:28:25,759
application therefore the idea is

00:28:22,429 --> 00:28:28,940
actually let's use still system tap

00:28:25,759 --> 00:28:32,359
because at least it proved that we can

00:28:28,940 --> 00:28:35,929
collect the data collect the data with

00:28:32,359 --> 00:28:38,869
system and print out a text file which

00:28:35,929 --> 00:28:41,539
returns can load into a to type your

00:28:38,869 --> 00:28:43,669
notebook and can do various analytics

00:28:41,539 --> 00:28:47,509
stuff on that can create some math

00:28:43,669 --> 00:28:49,460
charts some nice visualizations all the

00:28:47,509 --> 00:28:51,589
things we can do with our data science

00:28:49,460 --> 00:28:55,519
tools and I think that's one of the most

00:28:51,589 --> 00:28:58,039
nicest things about Python in nowadays

00:28:55,519 --> 00:29:00,889
that we all have this nice visualization

00:28:58,039 --> 00:29:05,239
libraries at hand to make data quite

00:29:00,889 --> 00:29:07,580
easily to recognize what I did is to

00:29:05,239 --> 00:29:11,899
visualize that with Pookie

00:29:07,580 --> 00:29:17,749
this is now the times a visualization of

00:29:11,899 --> 00:29:21,320
our 31 frets over two minutes what you

00:29:17,749 --> 00:29:24,859
can see is that in the areas where we

00:29:21,320 --> 00:29:28,879
have dark blue boxes there we have a Gil

00:29:24,859 --> 00:29:32,210
contention which is actually um we have

00:29:28,879 --> 00:29:34,879
a guild usage which shows that fret is

00:29:32,210 --> 00:29:37,460
holding the girl longer than 50

00:29:34,879 --> 00:29:41,889
milliseconds please remember it's

00:29:37,460 --> 00:29:45,710
wonderful five minutes milliseconds in

00:29:41,889 --> 00:29:47,659
the red boxes in the red areas we have

00:29:45,710 --> 00:29:50,930
no fret who are waiting for the peer

00:29:47,659 --> 00:29:53,450
longer than 50 milliseconds

00:29:50,930 --> 00:29:56,570
we are now thinking about optimizing web

00:29:53,450 --> 00:29:59,690
applications you normally try to aim as

00:29:56,570 --> 00:30:03,590
fast as possible and every web developer

00:29:59,690 --> 00:30:06,380
who says yeah 300 milliseconds I totally

00:30:03,590 --> 00:30:08,050
fine for that page I'm not sure about

00:30:06,380 --> 00:30:12,830
that

00:30:08,050 --> 00:30:15,710
okay let's zoom in a bit and we can take

00:30:12,830 --> 00:30:19,280
a closer look so this is an honor 20

00:30:15,710 --> 00:30:22,700
seconds 5 seconds now we came to the

00:30:19,280 --> 00:30:25,090
interesting part of one second and what

00:30:22,700 --> 00:30:29,120
we saw is that they're actually a

00:30:25,090 --> 00:30:31,940
clustering of big blue boxes so for

00:30:29,120 --> 00:30:35,210
example this part is taking more than

00:30:31,940 --> 00:30:38,150
500 milliseconds 500 milliseconds this

00:30:35,210 --> 00:30:40,850
red is doing all the work no artifact

00:30:38,150 --> 00:30:44,720
can do anything and the first question

00:30:40,850 --> 00:30:46,850
was became what is this red it's

00:30:44,720 --> 00:30:49,610
actually a threat who is collecting

00:30:46,850 --> 00:30:52,100
metrics of our scheduler is sending out

00:30:49,610 --> 00:30:54,370
his metrics into a central system so

00:30:52,100 --> 00:30:58,100
that we know how many tasks are running

00:30:54,370 --> 00:31:00,760
how our queues are currently and so on

00:30:58,100 --> 00:31:03,710
and after adding additional

00:31:00,760 --> 00:31:06,920
visualization I found out that this red

00:31:03,710 --> 00:31:11,050
is actually consuming 75% of the full

00:31:06,920 --> 00:31:14,570
PDF of the full kill whole time

00:31:11,050 --> 00:31:17,930
which is super expensive if this red is

00:31:14,570 --> 00:31:23,420
only taking some yeah metrics out of the

00:31:17,930 --> 00:31:24,470
out of the application ok so now we know

00:31:23,420 --> 00:31:26,960
what is a problem

00:31:24,470 --> 00:31:29,530
the nice thing is know as we know the

00:31:26,960 --> 00:31:33,470
problem you can actually fix the problem

00:31:29,530 --> 00:31:35,780
so what we did is we start to replacing

00:31:33,470 --> 00:31:38,090
the C extension which we were using

00:31:35,780 --> 00:31:42,050
inside of the spread to calculate all

00:31:38,090 --> 00:31:45,020
these various metrics we actually found

00:31:42,050 --> 00:31:47,210
out that the C extension never released

00:31:45,020 --> 00:31:50,300
a girl because it's actually internally

00:31:47,210 --> 00:31:53,360
using heavily to Python objects but if

00:31:50,300 --> 00:31:55,760
you read normally C extension at least

00:31:53,360 --> 00:31:58,700
that was the intention by reading that

00:31:55,760 --> 00:32:00,800
by by choosing that's the extension then

00:31:58,700 --> 00:32:02,720
you think normally and we probably

00:32:00,800 --> 00:32:05,590
release together quite quite early in

00:32:02,720 --> 00:32:10,190
the time it's not always wrong

00:32:05,590 --> 00:32:12,499
also the probably most simple fix was to

00:32:10,190 --> 00:32:16,309
just change the interval of our matrix

00:32:12,499 --> 00:32:20,710
collection so we just changed the

00:32:16,309 --> 00:32:23,570
interval from 10 seconds to 120 seconds

00:32:20,710 --> 00:32:25,489
so we are now collecting less often

00:32:23,570 --> 00:32:28,119
intervals but I mean if it can solve the

00:32:25,489 --> 00:32:29,989
Google contention not a big deal and

00:32:28,119 --> 00:32:36,070
that actually happened

00:32:29,989 --> 00:32:39,889
so before we saw a huge usage of time a

00:32:36,070 --> 00:32:43,850
usage of CPU time where the Gil was

00:32:39,889 --> 00:32:46,129
required for processing and also a huge

00:32:43,850 --> 00:32:49,100
amount of time where threads are waiting

00:32:46,129 --> 00:32:51,889
for the couple interpreter now after

00:32:49,100 --> 00:32:54,950
applying this simple fixes do simple

00:32:51,889 --> 00:33:00,320
changes it was actually possible to

00:32:54,950 --> 00:33:03,259
reduce the time by or basically how half

00:33:00,320 --> 00:33:08,419
the time so we only now hold it here for

00:33:03,259 --> 00:33:10,190
around 40 43 percent and we only wait 80

00:33:08,419 --> 00:33:12,669
percent on the girl

00:33:10,190 --> 00:33:15,649
still the wait time is not great but

00:33:12,669 --> 00:33:17,690
nevertheless with this simple fixes we

00:33:15,649 --> 00:33:21,049
were able actually to speed up our task

00:33:17,690 --> 00:33:23,539
execution scheduler so good that we can

00:33:21,049 --> 00:33:26,330
now actually use all the Babel resources

00:33:23,539 --> 00:33:28,399
and it basically saved us a huge amount

00:33:26,330 --> 00:33:30,379
of refactoring of our application of

00:33:28,399 --> 00:33:33,049
replacing Multi multi threading with

00:33:30,379 --> 00:33:35,149
multi processing on a single but we had

00:33:33,049 --> 00:33:37,070
basically to think about yeah it may be

00:33:35,149 --> 00:33:42,259
easy to rewrite a full application from

00:33:37,070 --> 00:33:45,139
scratch and I mean that is probably most

00:33:42,259 --> 00:33:47,929
important information out of ya

00:33:45,139 --> 00:33:50,720
performance related stuff please try

00:33:47,929 --> 00:33:53,299
first to find out if you have that

00:33:50,720 --> 00:33:56,210
problem if it makes sense to invest in

00:33:53,299 --> 00:33:59,029
that area and probably find even a

00:33:56,210 --> 00:34:02,600
better solution if you have more insert

00:33:59,029 --> 00:34:05,720
support especially now that will help us

00:34:02,600 --> 00:34:08,030
to find out ok what is now the next best

00:34:05,720 --> 00:34:09,409
evolutionary step for our application

00:34:08,030 --> 00:34:11,809
that's it now it makes sense to

00:34:09,409 --> 00:34:15,859
introduce multi processing or does it

00:34:11,809 --> 00:34:16,850
make sense for example to rewrite some

00:34:15,859 --> 00:34:18,740
function

00:34:16,850 --> 00:34:20,660
which a bit more CPU bound and other

00:34:18,740 --> 00:34:22,610
functions and moved them into the siphon

00:34:20,660 --> 00:34:29,900
for example where we can release the

00:34:22,610 --> 00:34:33,020
girl also this full topic prepared many

00:34:29,900 --> 00:34:34,130
mudwell additional ideas how we could

00:34:33,020 --> 00:34:38,840
actually improve

00:34:34,130 --> 00:34:41,390
- ecosystem in that area so the next

00:34:38,840 --> 00:34:44,750
things I would actually like to do but

00:34:41,390 --> 00:34:46,340
as you know times always a limit and I

00:34:44,750 --> 00:34:50,060
would actually like to bring this tool

00:34:46,340 --> 00:34:52,100
set in my reusable state so that it's

00:34:50,060 --> 00:34:56,570
easier to use for people who are not

00:34:52,100 --> 00:34:58,850
involved in developing that scripts but

00:34:56,570 --> 00:35:02,480
also I think it would be possible to

00:34:58,850 --> 00:35:04,430
extend C - in some areas to make it even

00:35:02,480 --> 00:35:09,340
easier to build such kind of tool sets

00:35:04,430 --> 00:35:12,080
so for example the C API for fret names

00:35:09,340 --> 00:35:15,910
one thing which we would be super

00:35:12,080 --> 00:35:19,070
helpful every time I had a reddit system

00:35:15,910 --> 00:35:21,430
tetrapod I had to map the thread

00:35:19,070 --> 00:35:24,020
identifier to human understand her name

00:35:21,430 --> 00:35:28,640
otherwise you have no chance with 31

00:35:24,020 --> 00:35:31,970
frets also one question I had in my mind

00:35:28,640 --> 00:35:35,150
is would it be possible to actually move

00:35:31,970 --> 00:35:38,720
this metrics collection directly in C -

00:35:35,150 --> 00:35:40,970
for like the profiler itself you don't

00:35:38,720 --> 00:35:43,790
have to be active all the time but maybe

00:35:40,970 --> 00:35:45,500
we can enable it on demand if we

00:35:43,790 --> 00:35:48,770
actually need it and collect the

00:35:45,500 --> 00:35:51,200
statistics and get them out if we have

00:35:48,770 --> 00:35:54,280
such an API it is probably also easier

00:35:51,200 --> 00:35:57,290
to integrate that in existing

00:35:54,280 --> 00:35:59,900
observability tools for example -

00:35:57,290 --> 00:36:01,730
superior tracing which shows yes this

00:35:59,900 --> 00:36:04,280
operation actually took seven hundred

00:36:01,730 --> 00:36:06,800
milliseconds but overall i only consumed

00:36:04,280 --> 00:36:09,020
30 million milliseconds of the Gil and

00:36:06,800 --> 00:36:11,270
at that point in time you would already

00:36:09,020 --> 00:36:14,930
know okay probably I have a problem for

00:36:11,270 --> 00:36:17,060
my Gil if you are interested in that

00:36:14,930 --> 00:36:18,500
topic and I hope there are some people

00:36:17,060 --> 00:36:21,530
interested because the room is quite

00:36:18,500 --> 00:36:23,870
full then please watch out for me I

00:36:21,530 --> 00:36:26,870
would actually like to talk about that I

00:36:23,870 --> 00:36:28,730
would like to hear some feedback things

00:36:26,870 --> 00:36:31,320
that could be improved or things which

00:36:28,730 --> 00:36:34,290
would be interesting for

00:36:31,320 --> 00:36:36,420
because with that feedback I think we

00:36:34,290 --> 00:36:39,150
can could actually start some some

00:36:36,420 --> 00:36:40,829
things in that area to make performance

00:36:39,150 --> 00:36:43,890
measurements in Python much better

00:36:40,829 --> 00:36:46,230
and I mean yes we have to gear we will

00:36:43,890 --> 00:36:49,020
probably never really remove the girl

00:36:46,230 --> 00:36:51,720
from cpython and because it is not so

00:36:49,020 --> 00:36:57,000
easy otherwise people would already do

00:36:51,720 --> 00:36:59,190
that but if we learn to live with it

00:36:57,000 --> 00:37:02,040
with the girl then we can also improve

00:36:59,190 --> 00:37:04,530
overall Python application performance

00:37:02,040 --> 00:37:08,880
and we don't have to rewrite them in I

00:37:04,530 --> 00:37:13,079
don't know C++ or C or assembler if you

00:37:08,880 --> 00:37:15,210
really like to do that okay then thank

00:37:13,079 --> 00:37:17,970
you very much and as we have five

00:37:15,210 --> 00:37:21,410
minutes I think we can do one or two

00:37:17,970 --> 00:37:21,410

YouTube URL: https://www.youtube.com/watch?v=HtbLNgXmLrw


