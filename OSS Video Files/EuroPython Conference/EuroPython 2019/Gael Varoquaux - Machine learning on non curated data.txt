Title: Gael Varoquaux - Machine learning on non curated data
Publication date: 2019-09-23
Playlist: EuroPython 2019
Description: 
	"Machine learning on non curated data
[EuroPython 2019 - Talk - 2019-07-11 - Singapore [PyData track]
[Basel, CH]

By Gael Varoquaux

According to industry surveys [1], the number one hassle of data scientists is cleaning the data to analyze it. Textbook statistical modeling is sufficient for noisy signals, but errors of a discrete nature break standard tools of machine learning. I will discuss how to easily run machine learning on data tables with two common dirty-data problems: missing values and non-normalized entries. On both problems, I will show how to run standard machine-learning tools such as scikit-learn in the presence of such errors. The talk will be didactic and will discuss simple software solutions. It will build on the latest improvements to scikit-learn for missing values and the DirtyCat package [2] for non normalized entries. I will also summarize theoretical analyses in recent machine learning publications. 

This talk targets data practitioners. Its goal are to help data scientists to be more efficient analysing data with such errors and understanding their impacts.

With missing values, I will use simple arguments and examples to outline how to obtain asymptotically good predictions [3]. Two components are key: imputation and adding an indicator of missingness. I will explain theoretical guidelines for these, and I will show how to implement these ideas in practice, with scikit-learn as a learner, or as a preprocesser.

For non-normalized categories, I will show that using their string representations to “vectorize” them, creating vectorial representations gives a simple but powerful solution that can be plugged in standard statistical analysis tools [4].

[1] Kaggle, the state of ML and data science 2017  https://www.kaggle.com/surveys/2017 
[2]  https://dirty-cat.github.io/stable/ 
[3] Josse Julie, Prost Nicolas, Scornet Erwan, and Varoquaux Gaël (2019). “On the consistency of supervised learning with missing values”. https://arxiv.org/abs/1902.06931 
[4] Cerda Patricio, Varoquaux Gaël, and Kégl Balázs. ""Similarity encoding for learning with dirty categorical variables."" Machine Learning 107.8-10 (2018): 1477 https://arxiv.org/abs/1806.00979



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:04,600 --> 00:00:10,969
thank you very much Zev for the nice

00:00:07,100 --> 00:00:12,889
introduction so today I will not talk

00:00:10,969 --> 00:00:15,499
about brain imaging this is a new area

00:00:12,889 --> 00:00:20,240
of research that we've started in the

00:00:15,499 --> 00:00:22,100
team on on dirty data and the reason

00:00:20,240 --> 00:00:24,590
we've started this is that as we all

00:00:22,100 --> 00:00:27,230
know data science is 80% of the time

00:00:24,590 --> 00:00:29,480
spending on preparing the data and 20%

00:00:27,230 --> 00:00:32,119
of the time spending complaining about

00:00:29,480 --> 00:00:35,720
the need to prepare the data so let's

00:00:32,119 --> 00:00:38,329
address those 20% of the time and really

00:00:35,720 --> 00:00:40,970
that the thing is once again with the

00:00:38,329 --> 00:00:43,520
modern machine learning tools such as a

00:00:40,970 --> 00:00:45,860
socket learn machine learning is easy

00:00:43,520 --> 00:00:48,260
and fun and we like to do it but the

00:00:45,860 --> 00:00:52,250
problem is really getting the data into

00:00:48,260 --> 00:00:54,020
the learner an industry survey is showed

00:00:52,250 --> 00:00:57,110
this this was an industry survey by

00:00:54,020 --> 00:01:01,130
goggle a few years ago and it asked you

00:00:57,110 --> 00:01:02,630
know what's the most blocking aspect of

00:01:01,130 --> 00:01:05,329
running a data science project in your

00:01:02,630 --> 00:01:07,340
organization dirty data came on top you

00:01:05,329 --> 00:01:11,570
know about things like hiring the right

00:01:07,340 --> 00:01:13,700
talent so dirty data you know seeing

00:01:11,570 --> 00:01:16,100
this we thought well let's let's let's

00:01:13,700 --> 00:01:18,260
tackle the problem and when we thought

00:01:16,100 --> 00:01:21,350
let's tackle the problem we didn't know

00:01:18,260 --> 00:01:24,369
what it meant I'm not sure we know these

00:01:21,350 --> 00:01:28,340
things I guess everybody has their own

00:01:24,369 --> 00:01:30,590
dirty data problem but but at least

00:01:28,340 --> 00:01:32,270
we've understood a few things and one

00:01:30,590 --> 00:01:34,430
thing that we've understood is that

00:01:32,270 --> 00:01:37,549
every machine learning research paper

00:01:34,430 --> 00:01:41,360
starts by let X a numerical matrix that

00:01:37,549 --> 00:01:42,710
lives in a matrix space and if we're

00:01:41,360 --> 00:01:44,180
gonna implement this it's going to be

00:01:42,710 --> 00:01:46,369
well you know give me your data as an

00:01:44,180 --> 00:01:48,170
umpire and we've always said you know

00:01:46,369 --> 00:01:53,570
sure you're gonna have to transform your

00:01:48,170 --> 00:01:54,920
data from a were the kind of data you

00:01:53,570 --> 00:01:58,909
have to the numpy all right but that's

00:01:54,920 --> 00:02:02,750
your job not us so yes in real life the

00:01:58,909 --> 00:02:04,369
data best case comes like this often

00:02:02,750 --> 00:02:09,200
there's a pandas dataframe so it's not

00:02:04,369 --> 00:02:13,280
exactly a numerical matrix and the first

00:02:09,200 --> 00:02:14,780
thing is that we can we will need to

00:02:13,280 --> 00:02:16,940
transform the different columns and

00:02:14,780 --> 00:02:18,950
different ways to cause this to

00:02:16,940 --> 00:02:22,010
a numerical array and I want to talk a

00:02:18,950 --> 00:02:23,840
bit about how to do this with wood

00:02:22,010 --> 00:02:26,410
scikit-learn because scikit-learn has

00:02:23,840 --> 00:02:29,240
gotten much more pleasant in the last

00:02:26,410 --> 00:02:33,110
few years to do this but then we're

00:02:29,240 --> 00:02:35,540
gonna hit a set of hard problems and one

00:02:33,110 --> 00:02:40,040
of them is the fact that one of these

00:02:35,540 --> 00:02:42,980
columns is not a well formatted

00:02:40,040 --> 00:02:44,450
categorical column and for machine

00:02:42,980 --> 00:02:47,060
learning it falls a bit between the

00:02:44,450 --> 00:02:49,130
cracks and another one is that we might

00:02:47,060 --> 00:02:53,330
have missing values and that also raises

00:02:49,130 --> 00:02:55,540
problems so the outline of my talk is

00:02:53,330 --> 00:02:58,340
going to be that I'll talk a bit about

00:02:55,540 --> 00:03:02,090
transforming columns with scikit-learn

00:02:58,340 --> 00:03:05,030
and here I just want to emphasize a bit

00:03:02,090 --> 00:03:07,700
things that are feasible with modern

00:03:05,030 --> 00:03:10,940
scikit-learn and that can make your life

00:03:07,700 --> 00:03:12,470
easier and this is just many less so I

00:03:10,940 --> 00:03:14,750
can learn then I'll talk about the

00:03:12,470 --> 00:03:18,590
problems of dirty category and this is

00:03:14,750 --> 00:03:20,210
more research he talked even though we

00:03:18,590 --> 00:03:22,570
do have software that you can use and

00:03:20,210 --> 00:03:25,250
then I'll talk about the problems of

00:03:22,570 --> 00:03:27,380
learning with missing values and this is

00:03:25,250 --> 00:03:33,290
more of a statistical talk but there

00:03:27,380 --> 00:03:35,570
will be take-home messages so :

00:03:33,290 --> 00:03:38,000
transforming the goal is to start with

00:03:35,570 --> 00:03:40,880
metadata frames and come out with a well

00:03:38,000 --> 00:03:43,510
formatted numpy array that can easily be

00:03:40,880 --> 00:03:45,560
plugged in statistics such as

00:03:43,510 --> 00:03:49,400
scikit-learn so it's a pre-processing

00:03:45,560 --> 00:03:52,970
problem so often the way we get our data

00:03:49,400 --> 00:03:55,030
is we read it from a CSV files so we

00:03:52,970 --> 00:03:58,220
could do this with pandas and we'll get

00:03:55,030 --> 00:04:00,170
a data frame that has different types in

00:03:58,220 --> 00:04:02,270
different columns and our goal is going

00:04:00,170 --> 00:04:06,650
to be to convert all those values to

00:04:02,270 --> 00:04:10,310
numerical values and let's look at

00:04:06,650 --> 00:04:11,750
gender so gender where so it's a

00:04:10,310 --> 00:04:13,489
categorical column and we're going to

00:04:11,750 --> 00:04:15,380
transform it to a numerical column and

00:04:13,489 --> 00:04:17,150
the standard way to do this is to use

00:04:15,380 --> 00:04:19,459
one hot encoding so in scikit-learn

00:04:17,150 --> 00:04:22,310
we'll use Clarinda pre-processing that

00:04:19,459 --> 00:04:24,230
one horan coder and then we'll call the

00:04:22,310 --> 00:04:27,500
fit transform method on the column

00:04:24,230 --> 00:04:29,479
gender and it's going to use two output

00:04:27,500 --> 00:04:29,760
indicator columns with 0 and ones that

00:04:29,479 --> 00:04:34,770
end

00:04:29,760 --> 00:04:36,750
the different genders okay now for dates

00:04:34,770 --> 00:04:39,570
we could use dependence date time

00:04:36,750 --> 00:04:42,630
support so pen des deals quite well

00:04:39,570 --> 00:04:45,240
natively with with this kind of strength

00:04:42,630 --> 00:04:47,880
as it knows how to convert them to the

00:04:45,240 --> 00:04:49,770
date time object and once we have the

00:04:47,880 --> 00:04:52,730
date time objects we can take its value

00:04:49,770 --> 00:04:56,310
in float and it's a value in I believe

00:04:52,730 --> 00:04:59,370
milliseconds to the epoch and so it's a

00:04:56,310 --> 00:05:01,290
numerical value that is reasonably well

00:04:59,370 --> 00:05:07,050
ordered and hopefully we can learn from

00:05:01,290 --> 00:05:10,020
it hopefully so something I'd like to

00:05:07,050 --> 00:05:11,220
stress is that in circuit one we like to

00:05:10,020 --> 00:05:14,430
work with things that we call

00:05:11,220 --> 00:05:19,200
transformers and if we look at the 100

00:05:14,430 --> 00:05:20,880
coder we can actually split the fitting

00:05:19,200 --> 00:05:22,980
of the one and a coder and the

00:05:20,880 --> 00:05:24,810
transforming the the idea is that during

00:05:22,980 --> 00:05:26,820
the fitting we're storing which

00:05:24,810 --> 00:05:29,160
categories are present in the data

00:05:26,820 --> 00:05:32,280
and during the transforming we're

00:05:29,160 --> 00:05:34,620
encoding this data accordingly so this

00:05:32,280 --> 00:05:37,800
separation between fit and transforming

00:05:34,620 --> 00:05:40,860
is quite important because it avoids

00:05:37,800 --> 00:05:42,420
data leakage between the train and the

00:05:40,860 --> 00:05:44,850
test set when we're evaluating the

00:05:42,420 --> 00:05:48,180
pipeline and we can also store the

00:05:44,850 --> 00:05:51,180
fitted transformer and apply it to new

00:05:48,180 --> 00:05:53,400
data at predict time for production for

00:05:51,180 --> 00:05:55,080
instance and it can be used with a bunch

00:05:53,400 --> 00:05:58,500
of tools in scikit-learn such as the

00:05:55,080 --> 00:06:04,560
pipeline or the crossbar that is used to

00:05:58,500 --> 00:06:06,870
do a cross validation so for dates it

00:06:04,560 --> 00:06:11,190
might be useful to shoehorn error Panda

00:06:06,870 --> 00:06:13,470
code into such a process and for this we

00:06:11,190 --> 00:06:15,330
can use the function transformer so we

00:06:13,470 --> 00:06:20,580
can define a small function that will

00:06:15,330 --> 00:06:22,260
take as input the pandas dataframe or or

00:06:20,580 --> 00:06:27,480
the pandas column we're interested in

00:06:22,260 --> 00:06:29,670
and then return as output a 2d array of

00:06:27,480 --> 00:06:31,770
numerical values and for this it's just

00:06:29,670 --> 00:06:34,530
you know taking the code that we were

00:06:31,770 --> 00:06:36,090
doing with pandas putting it in the

00:06:34,530 --> 00:06:40,140
proper function and making sure that

00:06:36,090 --> 00:06:43,020
we're returning a to the output and then

00:06:40,140 --> 00:06:44,910
once we have this we can use ask learned

00:06:43,020 --> 00:06:49,560
preprocessing that function transformer

00:06:44,910 --> 00:06:51,780
give it this function and tell it that

00:06:49,560 --> 00:06:53,250
we don't want validation because if we

00:06:51,780 --> 00:06:56,220
want validation it's going to try to

00:06:53,250 --> 00:06:58,860
check that the data is well formatted at

00:06:56,220 --> 00:07:01,650
the input and it's not so it will

00:06:58,860 --> 00:07:03,690
complain so function transformer can be

00:07:01,650 --> 00:07:05,879
a bit more clever you can tell it how to

00:07:03,690 --> 00:07:09,180
inverse transform it's a more more

00:07:05,879 --> 00:07:11,430
sophisticated tool and I don't I won't

00:07:09,180 --> 00:07:13,710
go into details what I just want to

00:07:11,430 --> 00:07:16,020
stress here is that it can be useful to

00:07:13,710 --> 00:07:18,000
look at the modern pre-processing

00:07:16,020 --> 00:07:22,139
documentation of scikit-learn because it

00:07:18,000 --> 00:07:25,560
has many useful tools for this purpose

00:07:22,139 --> 00:07:28,169
and once again pipelines are good now

00:07:25,560 --> 00:07:29,639
how do we put a panda's data frame in a

00:07:28,169 --> 00:07:31,699
pipeline and apply different

00:07:29,639 --> 00:07:35,099
transformers to the different columns

00:07:31,699 --> 00:07:37,110
for this we can use the column

00:07:35,099 --> 00:07:41,490
transformer object the column

00:07:37,110 --> 00:07:46,440
transformer object we'll take a list of

00:07:41,490 --> 00:07:48,120
pairs of Transformers aim selectors of

00:07:46,440 --> 00:07:50,729
columns and selectors of columns can be

00:07:48,120 --> 00:07:53,430
for instance column names okay so here

00:07:50,729 --> 00:07:57,150
with this code I'm telling that I want

00:07:53,430 --> 00:08:00,419
to apply a 100 coder to the gender and

00:07:57,150 --> 00:08:02,430
employee position title columns a my

00:08:00,419 --> 00:08:04,889
date transformer to the date first hired

00:08:02,430 --> 00:08:07,199
and now I can call the column

00:08:04,889 --> 00:08:10,259
transformer on a data frame it will do

00:08:07,199 --> 00:08:12,750
all the magic and comes out an empire

00:08:10,259 --> 00:08:18,360
right and so I can build complicated

00:08:12,750 --> 00:08:21,900
pipelines using this kind of patterns to

00:08:18,360 --> 00:08:25,169
get my raw data at least my raw data

00:08:21,900 --> 00:08:28,529
frame in and then you scikit-learn on

00:08:25,169 --> 00:08:33,570
this so this is useful for cross

00:08:28,529 --> 00:08:35,099
validation for instance and the benefit

00:08:33,570 --> 00:08:37,610
really is that we can use all the tools

00:08:35,099 --> 00:08:40,740
in scikit-learn for for model selection

00:08:37,610 --> 00:08:43,860
such as for instance we could pipeline

00:08:40,740 --> 00:08:45,930
this column transformer with a fast

00:08:43,860 --> 00:08:49,500
gradient boosting classifier that's new

00:08:45,930 --> 00:08:51,480
in in or twenty one and then just apply

00:08:49,500 --> 00:08:55,750
apply cross-validation on the raw data

00:08:51,480 --> 00:08:58,110
okay so if you're not using it

00:08:55,750 --> 00:09:03,060
you should probably be using it if you

00:08:58,110 --> 00:09:03,060
think it can be improved find an issue

00:09:03,839 --> 00:09:09,310
now if we do this on the example that

00:09:07,029 --> 00:09:11,470
I'm using we're gonna hit a problem and

00:09:09,310 --> 00:09:16,180
the problem is with the employee

00:09:11,470 --> 00:09:18,220
position title and really the reason is

00:09:16,180 --> 00:09:22,300
that there are many many different

00:09:18,220 --> 00:09:23,649
entries in this title for ten thousand

00:09:22,300 --> 00:09:26,709
rows there are four hundred unique

00:09:23,649 --> 00:09:29,649
entries so that will will lead to a

00:09:26,709 --> 00:09:31,000
bunch of different problems and some of

00:09:29,649 --> 00:09:32,589
them are numerical it's just gonna take

00:09:31,000 --> 00:09:34,839
while computation was just going to take

00:09:32,589 --> 00:09:37,329
a lot of time to run but some of them

00:09:34,839 --> 00:09:38,800
are statistical and the reason they're

00:09:37,329 --> 00:09:41,170
statistical is that we might have some

00:09:38,800 --> 00:09:44,519
rare categories there is only one

00:09:41,170 --> 00:09:46,660
instance of architect 3 in the data set

00:09:44,519 --> 00:09:48,790
we might have some overlapping

00:09:46,660 --> 00:09:51,040
categories we have different instances

00:09:48,790 --> 00:09:53,410
of police officers and the link between

00:09:51,040 --> 00:09:56,889
those instances is now obvious as we if

00:09:53,410 --> 00:09:59,139
we don't look at the screen the string

00:09:56,889 --> 00:10:02,519
content and if we just look consider

00:09:59,139 --> 00:10:05,110
these things as discrete categories and

00:10:02,519 --> 00:10:07,449
finally it's a detail but it's a real

00:10:05,110 --> 00:10:10,839
problem in practice we might have new

00:10:07,449 --> 00:10:12,790
categories in the test set so basically

00:10:10,839 --> 00:10:15,639
one hard encoder doesn't work well at

00:10:12,790 --> 00:10:20,139
all with this kind of data and sometimes

00:10:15,639 --> 00:10:23,009
we have this kind of data so the

00:10:20,139 --> 00:10:25,689
standard practice to do this is to use

00:10:23,009 --> 00:10:29,649
to resort to data curation cleaning your

00:10:25,689 --> 00:10:31,240
data it can be seen the the it's mostly

00:10:29,649 --> 00:10:33,670
techniques from database normalization

00:10:31,240 --> 00:10:35,110
and so one thing that we could do is we

00:10:33,670 --> 00:10:37,720
could do feature engineering and we

00:10:35,110 --> 00:10:39,370
could try to separate the position from

00:10:37,720 --> 00:10:41,800
the rank and maybe we could separate the

00:10:39,370 --> 00:10:45,250
position the rank and the department so

00:10:41,800 --> 00:10:47,230
this would require building rules that

00:10:45,250 --> 00:10:49,389
we might apply in pandas on strings to

00:10:47,230 --> 00:10:50,410
separate those things out the problem is

00:10:49,389 --> 00:10:52,389
it's going to take a little while to

00:10:50,410 --> 00:10:56,829
build those rules and and they usually

00:10:52,389 --> 00:10:59,379
have to be handcrafted another related

00:10:56,829 --> 00:11:01,839
problem for instance in a different

00:10:59,379 --> 00:11:03,730
database here we have a database or we

00:11:01,839 --> 00:11:05,889
have company names and we have the same

00:11:03,730 --> 00:11:09,390
company that's expressed under different

00:11:05,889 --> 00:11:12,760
names now that's a canonical problem

00:11:09,390 --> 00:11:13,660
database curation and it's known as

00:11:12,760 --> 00:11:16,660
deduplication

00:11:13,660 --> 00:11:19,810
or record linkage and the goal being to

00:11:16,660 --> 00:11:22,330
output a clean database basically to

00:11:19,810 --> 00:11:23,950
merge those different entries those

00:11:22,330 --> 00:11:27,730
different entities and represent them as

00:11:23,950 --> 00:11:29,500
the same entity now this is quite

00:11:27,730 --> 00:11:32,260
difficult to do in general without

00:11:29,500 --> 00:11:35,620
supervision you usually need an expert

00:11:32,260 --> 00:11:37,930
that shows a set of mergers to have an

00:11:35,620 --> 00:11:40,900
algorithm learn how to do the merges and

00:11:37,930 --> 00:11:45,430
one problem is that it can be suboptimal

00:11:40,900 --> 00:11:48,310
because here the data set here that the

00:11:45,430 --> 00:11:51,400
challenge is to detect froude with

00:11:48,310 --> 00:11:54,460
payments to doctors and it's a real

00:11:51,400 --> 00:11:58,120
question on whether we should merge the

00:11:54,460 --> 00:12:02,170
Pfizer Hong Kong branch with the Pfizer

00:11:58,120 --> 00:12:04,030
Korean range maybe they should be

00:12:02,170 --> 00:12:05,530
considered as the same entity and maybe

00:12:04,030 --> 00:12:10,420
not that really depends on the question

00:12:05,530 --> 00:12:11,890
at hand so the problem with this there

00:12:10,420 --> 00:12:15,640
was this view is that the goal is to

00:12:11,890 --> 00:12:19,990
output a clean database which is it may

00:12:15,640 --> 00:12:22,270
be that may be a question specific point

00:12:19,990 --> 00:12:25,020
of view what is a clean database and in

00:12:22,270 --> 00:12:28,750
general it's something super hard so

00:12:25,020 --> 00:12:31,210
really these things all these IDs are

00:12:28,750 --> 00:12:33,550
hard to make automatic and to make

00:12:31,210 --> 00:12:34,870
turnkey and I'd like to claim that there

00:12:33,550 --> 00:12:36,970
are much harder than supervised learning

00:12:34,870 --> 00:12:40,060
supervised learning so supervised

00:12:36,970 --> 00:12:41,950
machine learning is is a toolbox that

00:12:40,060 --> 00:12:45,010
works quite well as long as you have a

00:12:41,950 --> 00:12:46,780
supervision signal database cleaning is

00:12:45,010 --> 00:12:48,700
a hard problem and you will need a

00:12:46,780 --> 00:12:51,550
supervision signal but that supervision

00:12:48,700 --> 00:12:53,140
signal is basically a clean database so

00:12:51,550 --> 00:12:54,940
usually clean database cleaning you

00:12:53,140 --> 00:12:56,530
first have somebody between part of the

00:12:54,940 --> 00:12:57,580
database then you learn rule from this

00:12:56,530 --> 00:13:00,730
and then you clean the rest of the

00:12:57,580 --> 00:13:03,220
database so our goal here is not

00:13:00,730 --> 00:13:06,190
database cleaning it's working directly

00:13:03,220 --> 00:13:08,410
on the dirty data and doing good machine

00:13:06,190 --> 00:13:10,720
learning on the dirty data really the

00:13:08,410 --> 00:13:11,920
point being that the statistical

00:13:10,720 --> 00:13:14,080
questions so the supervised learning

00:13:11,920 --> 00:13:17,670
problem should inform the curation and

00:13:14,080 --> 00:13:17,670
ideally we should even curate

00:13:20,130 --> 00:13:27,310
so a first work we did with Patricia was

00:13:23,710 --> 00:13:28,990
serda I should stress that this part is

00:13:27,310 --> 00:13:33,130
really the work of Patricio Sirdar who's

00:13:28,990 --> 00:13:35,080
doing a PhD in in my group so the first

00:13:33,130 --> 00:13:37,240
thing that we did is that we took one

00:13:35,080 --> 00:13:39,160
heart encoding and we relaxed it and

00:13:37,240 --> 00:13:43,150
basically instead of having zeros and

00:13:39,160 --> 00:13:45,790
ones we added string distances between

00:13:43,150 --> 00:13:48,250
their representations of the of the

00:13:45,790 --> 00:13:50,140
categories and we encoded with string

00:13:48,250 --> 00:13:52,390
distances instead of zeros and one and

00:13:50,140 --> 00:13:54,430
that really tackles the problem of new

00:13:52,390 --> 00:13:56,080
categories in the test set because if

00:13:54,430 --> 00:13:58,210
there's a new category in the test set

00:13:56,080 --> 00:13:59,980
that's not represented in a train set I

00:13:58,210 --> 00:14:01,810
can just look at the string distances

00:13:59,980 --> 00:14:03,970
through the categories in the Train in

00:14:01,810 --> 00:14:05,980
the train sets and it also allows us to

00:14:03,970 --> 00:14:08,560
link categories if for instance I have

00:14:05,980 --> 00:14:10,420
typos in my columns which is something

00:14:08,560 --> 00:14:12,220
that does happen the typos are gonna

00:14:10,420 --> 00:14:13,660
give me very small string distances and

00:14:12,220 --> 00:14:16,050
those two columns are gonna look very

00:14:13,660 --> 00:14:16,050
similar

00:14:17,130 --> 00:14:21,130
so there are different strings

00:14:19,900 --> 00:14:23,800
similarities that we could be using

00:14:21,130 --> 00:14:25,990
maybe the most well-known one is the

00:14:23,800 --> 00:14:27,700
Levenstein distance the Levenstein

00:14:25,990 --> 00:14:30,970
distance is basically the number of

00:14:27,700 --> 00:14:33,730
edits that we need to do to one string

00:14:30,970 --> 00:14:36,070
to match the other and it's really a

00:14:33,730 --> 00:14:39,010
classic one there's the gyro will will

00:14:36,070 --> 00:14:41,170
clear distance it's the number of

00:14:39,010 --> 00:14:43,000
matching characters renormalized by the

00:14:41,170 --> 00:14:45,550
number of trend positions character

00:14:43,000 --> 00:14:48,130
transpositions it's well used in in the

00:14:45,550 --> 00:14:51,670
in the database community and there's

00:14:48,130 --> 00:14:55,210
what I call the Engram or Jaccard

00:14:51,670 --> 00:14:58,330
similarity if we define Engram as a

00:14:55,210 --> 00:14:59,830
group of n consecutive characters so for

00:14:58,330 --> 00:15:02,650
instance if I have London the first

00:14:59,830 --> 00:15:05,110
Engram will be ello n the second Engram

00:15:02,650 --> 00:15:07,000
will be Oh Andi the third anagram nd O

00:15:05,110 --> 00:15:09,280
so we're basically taking all those

00:15:07,000 --> 00:15:11,230
engrams here these are three grams we're

00:15:09,280 --> 00:15:12,990
taking all the three grams and then to

00:15:11,230 --> 00:15:16,210
compute the distance between two strings

00:15:12,990 --> 00:15:17,950
we're looking at the number of engrams

00:15:16,210 --> 00:15:20,320
in common between the two screens and

00:15:17,950 --> 00:15:22,540
the number of engrams divided by the

00:15:20,320 --> 00:15:25,480
number of engrams total okay so if the

00:15:22,540 --> 00:15:28,510
two strings are the same they have all

00:15:25,480 --> 00:15:30,010
the engrams in common so this is one so

00:15:28,510 --> 00:15:30,820
this is a similarity if they're

00:15:30,010 --> 00:15:32,600
completely different

00:15:30,820 --> 00:15:38,119
they have no Engram in

00:15:32,600 --> 00:15:42,649
so this is zero okay so these are three

00:15:38,119 --> 00:15:45,290
classic string similarities so because

00:15:42,649 --> 00:15:47,749
this is a Python conference we're giving

00:15:45,290 --> 00:15:49,610
you a Python implementation we have this

00:15:47,749 --> 00:15:51,999
software that we call dirty cat for

00:15:49,610 --> 00:15:57,879
dirty category and it allows me to put

00:15:51,999 --> 00:16:01,519
pictures of cats on my slides crucial

00:15:57,879 --> 00:16:04,879
it's available online VSD license and

00:16:01,519 --> 00:16:06,889
everything it's something in between a

00:16:04,879 --> 00:16:11,569
research quality software and production

00:16:06,889 --> 00:16:13,850
quality software I think it's reasonably

00:16:11,569 --> 00:16:15,189
good quality it's not as high quality as

00:16:13,850 --> 00:16:17,660
scikit-learn but it comes with

00:16:15,189 --> 00:16:19,850
documentation examples and everything

00:16:17,660 --> 00:16:24,800
you can you can look at it it also comes

00:16:19,850 --> 00:16:27,379
with example datasets and it provides

00:16:24,800 --> 00:16:30,769
similarity encoding so similarity

00:16:27,379 --> 00:16:31,670
encoder is just an encoder it well worth

00:16:30,769 --> 00:16:34,069
like scikit-learn

00:16:31,670 --> 00:16:36,170
you can instantiate it saying which

00:16:34,069 --> 00:16:39,709
similarity you want to use and then you

00:16:36,170 --> 00:16:41,029
can transform the column of the data

00:16:39,709 --> 00:16:42,920
frame or the data frame you're

00:16:41,029 --> 00:16:45,740
interested in transforming so it's a

00:16:42,920 --> 00:16:51,259
drop-in replacement for one hot encoding

00:16:45,740 --> 00:16:53,089
in scikit-learn now I don't show you how

00:16:51,259 --> 00:16:54,529
it performs on real data but before I

00:16:53,089 --> 00:16:57,949
show you how to performs in real data

00:16:54,529 --> 00:16:59,959
let me present another approach that is

00:16:57,949 --> 00:17:02,259
it's been around for quite a while

00:16:59,959 --> 00:17:06,049
that's called target encoder

00:17:02,259 --> 00:17:07,850
it's not known well enough the idea

00:17:06,049 --> 00:17:11,270
being that we're going to represent each

00:17:07,850 --> 00:17:13,220
category by the average target so for

00:17:11,270 --> 00:17:16,309
instance we're going to represent the

00:17:13,220 --> 00:17:19,250
police officer 3 by the average salary

00:17:16,309 --> 00:17:21,069
of the police officer 3 in our database

00:17:19,250 --> 00:17:23,779
if we're trying to predict the salary

00:17:21,069 --> 00:17:25,640
right so this gives us a 1d

00:17:23,779 --> 00:17:28,459
representation of all the all the

00:17:25,640 --> 00:17:30,890
categories I've shown in here so all our

00:17:28,459 --> 00:17:32,419
categories are embedded in one

00:17:30,890 --> 00:17:35,149
dimensional which is the average salary

00:17:32,419 --> 00:17:37,100
so we have the average salary and so you

00:17:35,149 --> 00:17:39,289
can see that in the database the person

00:17:37,100 --> 00:17:41,750
who makes the least amount of money is

00:17:39,289 --> 00:17:43,789
the crossing guard and the person who

00:17:41,750 --> 00:17:45,500
makes the most amount of money is d

00:17:43,789 --> 00:17:48,320
manager 3 manager

00:17:45,500 --> 00:17:50,179
actually so by the way this is maybe a

00:17:48,320 --> 00:17:52,010
bit surprising the ordering managers

00:17:50,179 --> 00:17:54,320
doesn't make sense right we have manager

00:17:52,010 --> 00:17:56,150
3 who makes less money that manager one

00:17:54,320 --> 00:18:00,070
who makes less money than manager - why

00:17:56,150 --> 00:18:02,840
is that because those are average

00:18:00,070 --> 00:18:05,360
salaries and we might have people with

00:18:02,840 --> 00:18:08,000
different level of experience or I don't

00:18:05,360 --> 00:18:09,799
know what and also is telling us that

00:18:08,000 --> 00:18:11,299
their signal is not a perfect signal

00:18:09,799 --> 00:18:15,080
it's a noisy signal there's this

00:18:11,299 --> 00:18:18,020
embedding but it's useful because it's

00:18:15,080 --> 00:18:21,650
embedding all the categories close by

00:18:18,020 --> 00:18:23,679
when they have the same link to Y so

00:18:21,650 --> 00:18:26,809
that's helping us build a simple

00:18:23,679 --> 00:18:30,740
decision function to do prediction from

00:18:26,809 --> 00:18:32,840
this representation ok now this is it

00:18:30,740 --> 00:18:34,190
comes with drawbacks the first one is it

00:18:32,840 --> 00:18:36,950
doesn't know how to deal with a new

00:18:34,190 --> 00:18:38,270
category if I tell you a category if I

00:18:36,950 --> 00:18:40,250
give you a category that I've never seen

00:18:38,270 --> 00:18:42,679
I don't know the average salary I can't

00:18:40,250 --> 00:18:44,390
represent it so I can I can represent it

00:18:42,679 --> 00:18:47,059
by the average salary of everybody but

00:18:44,390 --> 00:18:49,100
that's losing a bit of information and

00:18:47,059 --> 00:18:51,590
the other thing is it's absolutely not

00:18:49,100 --> 00:18:54,620
using the string structure of the

00:18:51,590 --> 00:18:56,630
category so typos for instance it will

00:18:54,620 --> 00:18:59,450
not find the links between typos unless

00:18:56,630 --> 00:19:02,809
it sees enough of those typos to see

00:18:59,450 --> 00:19:05,870
that they basically link to their

00:19:02,809 --> 00:19:07,340
targets in the same way so I'd like to

00:19:05,870 --> 00:19:09,140
say really it it's a complimentary

00:19:07,340 --> 00:19:13,280
approach to our approach it takes a

00:19:09,140 --> 00:19:17,120
different point of view and it's very

00:19:13,280 --> 00:19:19,460
interesting - it's also available in

00:19:17,120 --> 00:19:22,490
dirty cat because our goal and dirty cat

00:19:19,460 --> 00:19:24,620
is not to sell the methods that we

00:19:22,490 --> 00:19:27,650
developed but to help solve the problem

00:19:24,620 --> 00:19:29,600
which is their ticket occurrence and so

00:19:27,650 --> 00:19:32,090
its target encoder oops

00:19:29,600 --> 00:19:34,250
and I was editing this too late

00:19:32,090 --> 00:19:40,940
yesterday evening and target encoder

00:19:34,250 --> 00:19:46,659
does not take a similarity argument so

00:19:40,940 --> 00:19:49,909
Patricia Sirte did and numerical

00:19:46,659 --> 00:19:52,070
benchmarks on real life data set say

00:19:49,909 --> 00:19:56,990
here using seven real life data sets to

00:19:52,070 --> 00:19:59,000
compare the different approaches and we

00:19:56,990 --> 00:20:01,250
benchmark linear models

00:19:59,000 --> 00:20:03,679
and gradient boosted trees and what I'm

00:20:01,250 --> 00:20:05,809
showing you here is the average rank of

00:20:03,679 --> 00:20:08,780
the different methods across the

00:20:05,809 --> 00:20:11,200
different datasets so one would be that

00:20:08,780 --> 00:20:14,539
the method was always the best predictor

00:20:11,200 --> 00:20:16,760
across all the datasets and so what you

00:20:14,539 --> 00:20:19,429
can see and we so there's more in the

00:20:16,760 --> 00:20:20,780
paper we benchmark many other methods

00:20:19,429 --> 00:20:22,250
but I'm really giving the executive

00:20:20,780 --> 00:20:26,630
summary because many of the methods that

00:20:22,250 --> 00:20:29,350
we benchmarked we're not helpful and so

00:20:26,630 --> 00:20:33,350
what you can see is that target encoder

00:20:29,350 --> 00:20:36,280
helps so with gradient boosted trees

00:20:33,350 --> 00:20:39,230
that helps compared to what hot encoding

00:20:36,280 --> 00:20:40,669
one thing that that is not visible in

00:20:39,230 --> 00:20:44,059
those numbers is that gradient boosted

00:20:40,669 --> 00:20:46,130
trees do much better than linear models

00:20:44,059 --> 00:20:47,780
so I would advise you to focus on

00:20:46,130 --> 00:20:50,840
gradient boosted trees in practice

00:20:47,780 --> 00:20:54,289
they're much more useful for this kind

00:20:50,840 --> 00:20:57,890
of data set so target encoding helps a

00:20:54,289 --> 00:20:59,659
lot and then in the similarity in coding

00:20:57,890 --> 00:21:02,270
what we found is that the three gram

00:20:59,659 --> 00:21:04,880
distance the three gram similarity was

00:21:02,270 --> 00:21:09,559
really the most helpful and the others

00:21:04,880 --> 00:21:11,450
are not most not as helpful so our

00:21:09,559 --> 00:21:15,260
take-home message is really we can focus

00:21:11,450 --> 00:21:18,230
on similarity encoding with three gram

00:21:15,260 --> 00:21:20,720
distance and oh my it might be useful

00:21:18,230 --> 00:21:24,250
for instance to build a pipeline that

00:21:20,720 --> 00:21:26,510
stacks both a target encoding and

00:21:24,250 --> 00:21:29,240
similarity encoding because these two

00:21:26,510 --> 00:21:32,480
objects capture different information in

00:21:29,240 --> 00:21:35,080
the data and that's easy to do by the

00:21:32,480 --> 00:21:37,760
way with a column transformer you just

00:21:35,080 --> 00:21:41,049
select the column twice and and and and

00:21:37,760 --> 00:21:46,520
send it in in the two different encoders

00:21:41,049 --> 00:21:50,620
okay now in practice we're gonna hit a

00:21:46,520 --> 00:21:53,059
problem is that in many though not all

00:21:50,620 --> 00:21:55,760
databases the number of different

00:21:53,059 --> 00:21:59,030
categories grows with the amount of data

00:21:55,760 --> 00:22:02,360
here that's the that's the second work

00:21:59,030 --> 00:22:04,700
that we did with Patricio and we're now

00:22:02,360 --> 00:22:06,470
we've moved we've gathered more data set

00:22:04,700 --> 00:22:08,870
and now we've moved through 17 data sets

00:22:06,470 --> 00:22:11,340
it's actually hard to find datasets that

00:22:08,870 --> 00:22:13,470
are not curated and with

00:22:11,340 --> 00:22:15,810
open license people do not like to share

00:22:13,470 --> 00:22:17,820
their non curated dataset so please

00:22:15,810 --> 00:22:22,350
please do share your non character

00:22:17,820 --> 00:22:25,680
dataset that's the only way we can

00:22:22,350 --> 00:22:28,680
develop better methods so so what you're

00:22:25,680 --> 00:22:31,200
seeing here is that across many datasets

00:22:28,680 --> 00:22:33,210
as we increase the number of row or the

00:22:31,200 --> 00:22:36,480
number of different entries that we're

00:22:33,210 --> 00:22:40,710
seeing in a given column increases and

00:22:36,480 --> 00:22:45,060
increases sometimes very fast sometimes

00:22:40,710 --> 00:22:46,290
only slightly faster but it gives a

00:22:45,060 --> 00:22:52,440
problem because it means that you're

00:22:46,290 --> 00:22:54,840
forgetting to use the the similarity

00:22:52,440 --> 00:22:57,020
encoder we're gonna blow up that I

00:22:54,840 --> 00:23:00,660
mentioned and we're gonna end up running

00:22:57,020 --> 00:23:03,000
gradient boosting on things that have a

00:23:00,660 --> 00:23:05,550
hundred thousand features which is not

00:23:03,000 --> 00:23:10,080
only bad statistically but also will

00:23:05,550 --> 00:23:14,520
take a lot of time so really be oh and

00:23:10,080 --> 00:23:17,850
yeah so what there's this is this is

00:23:14,520 --> 00:23:20,370
related to problems in in for instance

00:23:17,850 --> 00:23:23,310
natural language processing whereas the

00:23:20,370 --> 00:23:24,780
corpus of the text gets bigger the

00:23:23,310 --> 00:23:26,970
number of different words that we see

00:23:24,780 --> 00:23:29,250
keeps increasing so in its it's quite

00:23:26,970 --> 00:23:35,430
related to classical natural language

00:23:29,250 --> 00:23:37,890
processing problems so we need to tackle

00:23:35,430 --> 00:23:39,720
this elsewhere we can't give this to you

00:23:37,890 --> 00:23:43,380
as a turnkey method that you can apply

00:23:39,720 --> 00:23:45,090
to large databases so birth similarity

00:23:43,380 --> 00:23:47,430
encoding and what are encoding our

00:23:45,090 --> 00:23:50,840
prototype methods what I mean by

00:23:47,430 --> 00:23:55,320
prototype method is that they compare

00:23:50,840 --> 00:23:57,750
the data to set of prototypes and by

00:23:55,320 --> 00:24:01,230
default it's the older prototypes on the

00:23:57,750 --> 00:24:03,540
on the training set the challenge now is

00:24:01,230 --> 00:24:07,620
to choose a small number of prototypes

00:24:03,540 --> 00:24:08,880
to be able to scale so we can take all

00:24:07,620 --> 00:24:11,280
the training set that's what we're

00:24:08,880 --> 00:24:15,990
taking my default it blows up we can

00:24:11,280 --> 00:24:18,150
take the most frequent but it's a

00:24:15,990 --> 00:24:20,700
strategic that's easy to game you can

00:24:18,150 --> 00:24:23,340
easily have a data set that breaks the

00:24:20,700 --> 00:24:25,260
strategy and one of the problems is that

00:24:23,340 --> 00:24:28,140
the most natural prototypes

00:24:25,260 --> 00:24:30,169
may not be in the training set for

00:24:28,140 --> 00:24:33,990
instance if my training set is made of

00:24:30,169 --> 00:24:36,210
big cat fat cat big dog fat dog

00:24:33,990 --> 00:24:39,419
I probably want to break this in bacon

00:24:36,210 --> 00:24:44,280
fat and cat and dog and none of these

00:24:39,419 --> 00:24:47,549
original entries actually have the

00:24:44,280 --> 00:24:51,570
rights terms okay so I need basically to

00:24:47,549 --> 00:24:53,280
break down my my categories so now I'll

00:24:51,570 --> 00:24:57,210
tell you how we estimate those

00:24:53,280 --> 00:25:00,059
prototypes and the thing that is going

00:24:57,210 --> 00:25:02,910
to save us is that while those those

00:25:00,059 --> 00:25:05,190
different strings grow they have common

00:25:02,910 --> 00:25:07,860
information here I'm showing you the

00:25:05,190 --> 00:25:09,690
growth in the number of three grams as I

00:25:07,860 --> 00:25:11,070
increase the number of strings and what

00:25:09,690 --> 00:25:12,540
you can see is that it's a smaller

00:25:11,070 --> 00:25:15,120
growth than the number of different

00:25:12,540 --> 00:25:17,549
strings and this makes sense because for

00:25:15,120 --> 00:25:21,150
instance if this dirtiness this

00:25:17,549 --> 00:25:24,809
diversity of the string is made from

00:25:21,150 --> 00:25:27,480
typos then typo is actually modified a

00:25:24,809 --> 00:25:29,669
small fraction in this string so yes I

00:25:27,480 --> 00:25:31,110
will I will have new three grams but

00:25:29,669 --> 00:25:35,460
most of the three grams will be in

00:25:31,110 --> 00:25:37,740
common in practice if I look at my data

00:25:35,460 --> 00:25:40,290
sets you can really clearly see this

00:25:37,740 --> 00:25:42,590
that the the sub strings are in common

00:25:40,290 --> 00:25:45,690
for instance in there's a drug named

00:25:42,590 --> 00:25:47,370
data set I can see that I have many

00:25:45,690 --> 00:25:48,780
different versions of alcohol but

00:25:47,370 --> 00:25:50,700
they're all versions of alcohol so

00:25:48,780 --> 00:25:55,080
there's alcohol in common everywhere in

00:25:50,700 --> 00:25:56,220
my employee salary problem I have sub

00:25:55,080 --> 00:26:00,020
strings that are really meaningful

00:25:56,220 --> 00:26:04,440
police is in common officer is in common

00:26:00,020 --> 00:26:06,540
technician senior so the challenge is

00:26:04,440 --> 00:26:10,250
going to grab this information and

00:26:06,540 --> 00:26:13,140
capture those meaningful sub strings and

00:26:10,250 --> 00:26:18,110
for this we're going to use techniques

00:26:13,140 --> 00:26:21,299
from topic modeling in a natural

00:26:18,110 --> 00:26:24,090
language processing and we're going to

00:26:21,299 --> 00:26:25,260
apply topic modeling on sub strings so

00:26:24,090 --> 00:26:27,570
what we're going to do is that we're

00:26:25,260 --> 00:26:30,540
going to represent all the older strings

00:26:27,570 --> 00:26:32,760
as their sub strings using an Engram

00:26:30,540 --> 00:26:34,380
representation and here I've shown a

00:26:32,760 --> 00:26:37,620
three gram representation but in

00:26:34,380 --> 00:26:38,919
practice we're doing a bit something

00:26:37,620 --> 00:26:40,690
slightly more sophisticated

00:26:38,919 --> 00:26:43,570
this we're taking the 2 grams the 3

00:26:40,690 --> 00:26:45,809
grams the 4 grams and also the worst

00:26:43,570 --> 00:26:50,619
that we've splits with a set of

00:26:45,809 --> 00:26:51,940
separator separating characters that we

00:26:50,619 --> 00:26:55,539
have default values but you can change

00:26:51,940 --> 00:26:59,769
them so then we build a big matrix that

00:26:55,539 --> 00:27:01,720
represents each entry by its substrings

00:26:59,769 --> 00:27:05,019
and then we apply matrix factorization

00:27:01,720 --> 00:27:09,460
on this really matrix factorization when

00:27:05,019 --> 00:27:12,549
it's doing here is to say I will I will

00:27:09,460 --> 00:27:14,259
separate this matrix in two matrices one

00:27:12,549 --> 00:27:16,359
matrix that is what I call the

00:27:14,259 --> 00:27:18,820
descriptions of the latent categories

00:27:16,359 --> 00:27:21,999
and it tells me what substrings are

00:27:18,820 --> 00:27:25,299
present in a latent category and another

00:27:21,999 --> 00:27:27,580
matrix which is what latent categories

00:27:25,299 --> 00:27:30,730
are present in a given entry okay so I'm

00:27:27,580 --> 00:27:33,279
really factorizing in the description of

00:27:30,730 --> 00:27:36,609
latent categories categories that I'm in

00:27:33,279 --> 00:27:39,820
fearing from the data or prototypes and

00:27:36,609 --> 00:27:43,749
how those are expressed in the data so

00:27:39,820 --> 00:27:46,509
to give you an example of the result

00:27:43,749 --> 00:27:48,759
this is and and so by the way we're

00:27:46,509 --> 00:27:51,279
using the activation we're using the

00:27:48,759 --> 00:27:53,950
activation matrix so the the one that

00:27:51,279 --> 00:27:55,960
expressed which categories are in which

00:27:53,950 --> 00:27:58,570
place in categories are elementary for

00:27:55,960 --> 00:28:00,249
using this to represent the data and so

00:27:58,570 --> 00:28:02,649
this is what I'm showing here these are

00:28:00,249 --> 00:28:06,129
those employee salary and the employee

00:28:02,649 --> 00:28:08,320
position titles and this is and I've run

00:28:06,129 --> 00:28:12,820
the model where the dimensionality of 8

00:28:08,320 --> 00:28:15,399
and this is the loadings that are showed

00:28:12,820 --> 00:28:19,210
so what you can if you squint your eyes

00:28:15,399 --> 00:28:22,139
what you can see here is that it has

00:28:19,210 --> 00:28:28,389
detected something like a technician

00:28:22,139 --> 00:28:31,059
like legal police in has detected those

00:28:28,389 --> 00:28:33,429
substrings okay and so one thing I'm not

00:28:31,059 --> 00:28:36,159
showing here that I should be is that

00:28:33,429 --> 00:28:39,460
we're using a heuristic to give a name

00:28:36,159 --> 00:28:41,259
to those columns and the name is really

00:28:39,460 --> 00:28:42,730
what's there what are the three words

00:28:41,259 --> 00:28:46,539
that are most represented in those

00:28:42,730 --> 00:28:48,039
columns so this is useful because it

00:28:46,539 --> 00:28:49,809
gives you it's giving you feature names

00:28:48,039 --> 00:28:52,600
were encoding this with feature names

00:28:49,809 --> 00:28:56,050
and so if we compare two similar

00:28:52,600 --> 00:29:00,280
encoder it's much more marked much more

00:28:56,050 --> 00:29:03,220
present much more interpretable and then

00:29:00,280 --> 00:29:04,630
we can do data science interpretable

00:29:03,220 --> 00:29:07,650
data science and for instance we can

00:29:04,630 --> 00:29:10,660
look at permutation importances of

00:29:07,650 --> 00:29:13,180
gradient boosted trees for instance with

00:29:10,660 --> 00:29:17,830
the string with the categories that were

00:29:13,180 --> 00:29:20,050
inferred from the data and this is what

00:29:17,830 --> 00:29:22,510
I get here so what I'm showing you here

00:29:20,050 --> 00:29:22,990
is that I've inferred from this messy

00:29:22,510 --> 00:29:26,320
data

00:29:22,990 --> 00:29:28,210
I've been spurred latent categories that

00:29:26,320 --> 00:29:30,790
makes sense and on which I can do an

00:29:28,210 --> 00:29:33,010
analysis and present it to you and then

00:29:30,790 --> 00:29:34,450
also by the word predicts well in the

00:29:33,010 --> 00:29:37,300
paper we're showing that it gives you a

00:29:34,450 --> 00:29:41,290
good prediction okay so you don't have

00:29:37,300 --> 00:29:43,060
to clean your data anymore so now I want

00:29:41,290 --> 00:29:44,890
to talk about one last thing which is

00:29:43,060 --> 00:29:47,200
learning with missing values so we've

00:29:44,890 --> 00:29:50,020
dealt with this non formatted

00:29:47,200 --> 00:29:52,210
categorical data and now we need to deal

00:29:50,020 --> 00:29:55,780
with the fact that some of our values

00:29:52,210 --> 00:29:57,820
are missing and so why doesn't the

00:29:55,780 --> 00:30:01,240
bloody machine learning toolkit work on

00:29:57,820 --> 00:30:03,610
this there is a fundamental reason is

00:30:01,240 --> 00:30:06,220
that machine learning models in general

00:30:03,610 --> 00:30:08,350
tend to need entries in a vector space

00:30:06,220 --> 00:30:11,610
or at least a metric space or at least

00:30:08,350 --> 00:30:14,290
an ordered space it's just easier for

00:30:11,610 --> 00:30:17,740
machine learning to draw analogies if it

00:30:14,290 --> 00:30:20,680
knows links between data and missing

00:30:17,740 --> 00:30:22,390
value is nowhere there so it's slightly

00:30:20,680 --> 00:30:25,200
more than an implementation problem

00:30:22,390 --> 00:30:28,990
there is a fundamental problem there

00:30:25,200 --> 00:30:33,000
there is a very very advanced and

00:30:28,990 --> 00:30:36,010
thorough literature on missing values in

00:30:33,000 --> 00:30:39,190
statistics and let me summarize it

00:30:36,010 --> 00:30:44,080
really quickly for you the canonical

00:30:39,190 --> 00:30:47,620
model is that we have a generating

00:30:44,080 --> 00:30:49,750
process for the complete data and being

00:30:47,620 --> 00:30:54,820
a random process that occludes the

00:30:49,750 --> 00:30:59,440
entries this is really the conceptual

00:30:54,820 --> 00:31:02,280
model on which the classic results stand

00:30:59,440 --> 00:31:04,509
upon and then there is a really classic

00:31:02,280 --> 00:31:08,019
situation which is known as me

00:31:04,509 --> 00:31:10,809
singing at random Mar that says him

00:31:08,019 --> 00:31:13,269
waving that for none observed values the

00:31:10,809 --> 00:31:16,929
probability of missingness does not

00:31:13,269 --> 00:31:18,789
depend on the non observed value this

00:31:16,929 --> 00:31:20,469
this might seem a bit mind-blowing if

00:31:18,789 --> 00:31:23,489
you look at the actual definition it's

00:31:20,469 --> 00:31:26,320
even more mind-blowing and people

00:31:23,489 --> 00:31:29,049
simplify it because it doesn't really

00:31:26,320 --> 00:31:31,089
make sense and it's true it doesn't

00:31:29,049 --> 00:31:33,519
really make sense the reason there is

00:31:31,089 --> 00:31:35,559
this definition is that it allows an

00:31:33,519 --> 00:31:38,829
unlikely hood framework to prove that

00:31:35,559 --> 00:31:43,329
and that was proven by Rubin forty years

00:31:38,829 --> 00:31:45,929
ago that maximizing the likelihood of

00:31:43,329 --> 00:31:48,339
the observed data while ignoring

00:31:45,929 --> 00:31:50,919
marginalizing in technical term the

00:31:48,339 --> 00:31:53,169
unobserved value will give the maximum

00:31:50,919 --> 00:31:54,729
likelihood of model a of the model of

00:31:53,169 --> 00:31:57,819
the complete Turner data generative

00:31:54,729 --> 00:31:59,409
process okay so it means that if you are

00:31:57,819 --> 00:32:00,819
modeling your data if you're doing

00:31:59,409 --> 00:32:02,529
classic statistics you're modeling your

00:32:00,819 --> 00:32:04,359
data with likelihood models that you

00:32:02,529 --> 00:32:07,479
believe and you believe you have an

00:32:04,359 --> 00:32:09,879
including process you can still do the

00:32:07,479 --> 00:32:12,779
you can still solve the problem when

00:32:09,879 --> 00:32:16,089
you're in a missing at random situation

00:32:12,779 --> 00:32:18,190
conversely if you're so and missing

00:32:16,089 --> 00:32:20,529
completely at random is a special case

00:32:18,190 --> 00:32:23,529
of this situation where the missingness

00:32:20,529 --> 00:32:26,529
is independent from the data and it's so

00:32:23,529 --> 00:32:27,969
easier so it's a special case of missing

00:32:26,529 --> 00:32:31,349
at random and it's easier to understand

00:32:27,969 --> 00:32:34,149
and the theorem still applies now

00:32:31,349 --> 00:32:35,769
conversely if you're in a missing not at

00:32:34,149 --> 00:32:37,629
random if you're not in this situation

00:32:35,769 --> 00:32:39,819
then missing this is not ignore ball if

00:32:37,629 --> 00:32:41,409
you try to maximize the likelihood while

00:32:39,819 --> 00:32:44,409
ignoring the missing data you will have

00:32:41,409 --> 00:32:46,719
problems in practice what does it look

00:32:44,409 --> 00:32:47,979
like I've shown you a complete data I'm

00:32:46,719 --> 00:32:49,509
sure it is showing you a missing

00:32:47,979 --> 00:32:50,289
completely at random so basically I've

00:32:49,509 --> 00:32:53,049
subsampled

00:32:50,289 --> 00:32:55,089
so here I'm deleting my missing values

00:32:53,049 --> 00:32:57,159
they're not on the data set so I've

00:32:55,089 --> 00:32:58,899
subsample it's a problem and I'm missing

00:32:57,159 --> 00:33:00,279
I'm showing you missing not at random

00:32:58,899 --> 00:33:01,869
and what you're seeing here is that we

00:33:00,279 --> 00:33:04,149
have some form of censoring process and

00:33:01,869 --> 00:33:07,599
part of the the data distribution is not

00:33:04,149 --> 00:33:11,469
well represented okay so this will give

00:33:07,599 --> 00:33:13,989
problem now I would like to say that

00:33:11,469 --> 00:33:16,989
this classic statistical point of view

00:33:13,989 --> 00:33:18,010
is not of interest to us here at least

00:33:16,989 --> 00:33:20,380
not completely

00:33:18,010 --> 00:33:23,200
of interest and we shouldn't take those

00:33:20,380 --> 00:33:25,270
results as fundamental results for

00:33:23,200 --> 00:33:28,270
machine learning there are two reasons

00:33:25,270 --> 00:33:30,760
one is there is not always an unobserved

00:33:28,270 --> 00:33:32,940
value for instance what the what is the

00:33:30,760 --> 00:33:36,310
age of this passive people are a single

00:33:32,940 --> 00:33:39,130
so even this assumption is broken in

00:33:36,310 --> 00:33:40,540
many many many data sets and the second

00:33:39,130 --> 00:33:42,420
one is that we're not trying to maximize

00:33:40,540 --> 00:33:45,580
likelihood so we're trying to predict

00:33:42,420 --> 00:33:48,880
now based on this we can just do machine

00:33:45,580 --> 00:33:50,320
learning but the bloody machine learning

00:33:48,880 --> 00:33:53,950
toolkit still doesn't work

00:33:50,320 --> 00:33:56,050
I've given you theory not practice okay

00:33:53,950 --> 00:33:59,620
practice I'll come back to this theory

00:33:56,050 --> 00:34:02,320
later practice we can impute and this

00:33:59,620 --> 00:34:03,310
goes back to the theory before impeding

00:34:02,320 --> 00:34:05,470
means we're gonna fill in the

00:34:03,310 --> 00:34:07,480
information we're gonna guess things for

00:34:05,470 --> 00:34:08,620
those values we haven't seen and once

00:34:07,480 --> 00:34:12,100
again there's a large statistical

00:34:08,620 --> 00:34:14,470
literature but it's focused on in sample

00:34:12,100 --> 00:34:17,050
testing doesn't tell you how to complete

00:34:14,470 --> 00:34:20,550
the test set and doesn't tell you what

00:34:17,050 --> 00:34:22,570
to do with the prediction so let me

00:34:20,550 --> 00:34:24,399
cover a bit the tools we have in

00:34:22,570 --> 00:34:26,530
scikit-learn there is mean imputation

00:34:24,399 --> 00:34:28,690
which is a special case of univariate

00:34:26,530 --> 00:34:30,790
imputation and we can for instance

00:34:28,690 --> 00:34:32,409
replace the missing values with the

00:34:30,790 --> 00:34:35,530
meaning of the feature so this is done

00:34:32,409 --> 00:34:37,510
with the simple impute er there is

00:34:35,530 --> 00:34:39,040
conditional imputation the idea being

00:34:37,510 --> 00:34:40,929
that you're modeling one feature as a

00:34:39,040 --> 00:34:42,960
function of the other and then you can

00:34:40,929 --> 00:34:46,360
learn predictive models across features

00:34:42,960 --> 00:34:48,970
and then you can predict missing values

00:34:46,360 --> 00:34:51,659
okay they're classic implementations in

00:34:48,970 --> 00:34:54,730
R and we now have an imputed

00:34:51,659 --> 00:34:56,500
implementation and a socket learning

00:34:54,730 --> 00:35:00,910
that can do this with linear models or a

00:34:56,500 --> 00:35:03,450
random force or other things the classic

00:35:00,910 --> 00:35:07,120
point of view tells you that missing

00:35:03,450 --> 00:35:10,090
mean imputation is a very very very bad

00:35:07,120 --> 00:35:11,860
thing because it will distort the the

00:35:10,090 --> 00:35:13,960
distribution so as you can see here I've

00:35:11,860 --> 00:35:15,400
imputed the missing data with the mean

00:35:13,960 --> 00:35:17,590
and you can see that we're collapsing

00:35:15,400 --> 00:35:19,830
the variance of the data along one

00:35:17,590 --> 00:35:22,270
direction so we shouldn't be doing that

00:35:19,830 --> 00:35:24,430
classic point of view and there are

00:35:22,270 --> 00:35:27,130
conditions that are known as Canoe

00:35:24,430 --> 00:35:28,750
congeniality conditions on an imputation

00:35:27,130 --> 00:35:31,600
that tell you that a good imputation

00:35:28,750 --> 00:35:34,830
match must must preserve the data profit

00:35:31,600 --> 00:35:37,540
tee's used by the later nail assistant

00:35:34,830 --> 00:35:40,570
now we've looked at supervised learning

00:35:37,540 --> 00:35:43,060
in this setting and we've shown we've

00:35:40,570 --> 00:35:45,160
proven that if the learner is powerful

00:35:43,060 --> 00:35:48,040
enough like a random forest or a

00:35:45,160 --> 00:35:50,080
gradient boosting tree imputing both the

00:35:48,040 --> 00:35:52,120
test and the train with the mean of the

00:35:50,080 --> 00:35:53,800
train is consistent in the sense that it

00:35:52,120 --> 00:35:57,400
converges to the best possible

00:35:53,800 --> 00:36:00,130
prediction and the reason is a we're not

00:35:57,400 --> 00:36:01,990
trying to maximize likelihoods be the

00:36:00,130 --> 00:36:04,510
learner will learn to recognize the

00:36:01,990 --> 00:36:06,640
imputed entries and will compensate for

00:36:04,510 --> 00:36:08,830
them so the learner basically learns

00:36:06,640 --> 00:36:11,290
those biases in the distribution and

00:36:08,830 --> 00:36:14,890
fixes them so we don't have to worry

00:36:11,290 --> 00:36:17,290
about the classical results in practice

00:36:14,890 --> 00:36:19,090
you can see it here I'm comparing mean

00:36:17,290 --> 00:36:20,680
imputation and intuitive imputation and

00:36:19,090 --> 00:36:23,110
what we can see is that if I have enough

00:36:20,680 --> 00:36:25,060
data they perform as well if I don't

00:36:23,110 --> 00:36:28,270
have an update sir then the iterative

00:36:25,060 --> 00:36:31,360
imputed there's better the notebooks are

00:36:28,270 --> 00:36:32,970
online and the slides are online the

00:36:31,360 --> 00:36:35,950
conclusion is when we have an app data

00:36:32,970 --> 00:36:38,560
iterative in pewter is not necessary

00:36:35,950 --> 00:36:40,090
mean imputation is enough but when we

00:36:38,560 --> 00:36:43,900
don't have enough data iterative in

00:36:40,090 --> 00:36:47,410
pewter helps now it may not be enough

00:36:43,900 --> 00:36:50,380
imputation may not be enough and here's

00:36:47,410 --> 00:36:51,850
a pathological example why what I'm

00:36:50,380 --> 00:36:53,260
trying to predict depends only on

00:36:51,850 --> 00:36:55,150
whether the data is missing or not

00:36:53,260 --> 00:36:57,010
suppose I'm trying to predict fraud and

00:36:55,150 --> 00:37:00,090
the only signal about fraud is that

00:36:57,010 --> 00:37:03,250
people have not fill in some information

00:37:00,090 --> 00:37:05,830
so this this will fall into missing not

00:37:03,250 --> 00:37:09,210
at random situations and in such a

00:37:05,830 --> 00:37:13,950
situation imputing makes the prediction

00:37:09,210 --> 00:37:17,710
impossible okay so if I impute I'm

00:37:13,950 --> 00:37:20,260
losing this information and I can't

00:37:17,710 --> 00:37:22,210
predict anymore so what's the solution

00:37:20,260 --> 00:37:25,120
the solution is to add a missingness

00:37:22,210 --> 00:37:27,640
indicator an extra column that tells me

00:37:25,120 --> 00:37:29,950
whether or not the data was present so I

00:37:27,640 --> 00:37:31,450
can impute but also exposed to the

00:37:29,950 --> 00:37:36,190
learner whether or not the data was

00:37:31,450 --> 00:37:37,720
present and if I do this so this is this

00:37:36,190 --> 00:37:40,450
is another simulation where we have

00:37:37,720 --> 00:37:43,300
specific censoring in the data and if I

00:37:40,450 --> 00:37:44,230
do this what you can see is that both

00:37:43,300 --> 00:37:47,680
the mean

00:37:44,230 --> 00:37:50,230
the iterative impute ur are consistent

00:37:47,680 --> 00:37:54,160
they converge with to the to the best

00:37:50,230 --> 00:37:56,320
prediction if there is the indicator but

00:37:54,160 --> 00:37:59,130
the iterative in pewter doesn't work

00:37:56,320 --> 00:38:02,800
well I told if there is not the

00:37:59,130 --> 00:38:05,079
indicator and also what we can see is

00:38:02,800 --> 00:38:07,540
that here so adding this this indicator

00:38:05,079 --> 00:38:08,849
this mask is absolutely crucial and the

00:38:07,540 --> 00:38:11,320
other thing that we can see is that

00:38:08,849 --> 00:38:14,339
iterative imputation in this situation

00:38:11,320 --> 00:38:17,950
is actually detrimental because it's

00:38:14,339 --> 00:38:21,820
making it harder for the learner to see

00:38:17,950 --> 00:38:23,410
this missing this pattern alright so

00:38:21,820 --> 00:38:25,810
basically we have two situations one

00:38:23,410 --> 00:38:28,570
where the missingness is not informative

00:38:25,810 --> 00:38:31,180
in which case due to the iterative and

00:38:28,570 --> 00:38:33,369
pewter is better one where missingness

00:38:31,180 --> 00:38:35,320
is informative in which case a iterative

00:38:33,369 --> 00:38:37,150
in pewter can harm because it makes it

00:38:35,320 --> 00:38:41,859
harder for the learner to learn this

00:38:37,150 --> 00:38:46,089
this informative missingness okay now to

00:38:41,859 --> 00:38:48,070
wrap up learning on dirty data first

00:38:46,089 --> 00:38:50,740
take-home message prepare the data by a

00:38:48,070 --> 00:38:52,900
column transform and that's easy second

00:38:50,740 --> 00:38:55,119
take a message use gradient boosting in

00:38:52,900 --> 00:38:55,990
my experience it really works well on

00:38:55,119 --> 00:38:58,180
this kind of data

00:38:55,990 --> 00:39:00,010
it's robust to all kind of weird entries

00:38:58,180 --> 00:39:03,460
in the data first thing you should try

00:39:00,010 --> 00:39:05,200
probably 30 categories so we're

00:39:03,460 --> 00:39:08,410
interested in statistical modeling a non

00:39:05,200 --> 00:39:11,020
curated categorical data please help us

00:39:08,410 --> 00:39:13,060
and give us your dirty data weather

00:39:11,020 --> 00:39:15,130
prediction task it helps us benchmark

00:39:13,060 --> 00:39:18,069
what we do it's very important and we

00:39:15,130 --> 00:39:21,880
have similarity in coding and more work

00:39:18,069 --> 00:39:23,829
that's coming up really soon supervised

00:39:21,880 --> 00:39:25,990
learning with missing data mean

00:39:23,829 --> 00:39:27,970
imputation with a missing indicator is

00:39:25,990 --> 00:39:30,160
actually a pretty good choice there are

00:39:27,970 --> 00:39:31,690
many more results in the paper and in

00:39:30,160 --> 00:39:34,089
general if you're interested in this

00:39:31,690 --> 00:39:37,089
area of research we have this research

00:39:34,089 --> 00:39:38,440
project that we call dirty data or there

00:39:37,089 --> 00:39:41,540
is ongoing research and there will be

00:39:38,440 --> 00:39:47,000
more thank you

00:39:41,540 --> 00:39:47,000
[Applause]

00:39:48,269 --> 00:39:53,799
Thank You Gayle we have five minutes for

00:39:51,249 --> 00:39:58,690
questions please come to the microphones

00:39:53,799 --> 00:40:02,589
in the aisles thanks a lot

00:39:58,690 --> 00:40:06,849
a little bit maybe not the best question

00:40:02,589 --> 00:40:10,150
for Europe python is there a version of

00:40:06,849 --> 00:40:13,690
dirty cat also for our and if not do you

00:40:10,150 --> 00:40:16,420
think it would be easy to part it dirty

00:40:13,690 --> 00:40:18,519
cat should be fairly easy to code the

00:40:16,420 --> 00:40:21,009
well dirty cat their ticket is several

00:40:18,519 --> 00:40:23,170
things and it will grow but both target

00:40:21,009 --> 00:40:24,969
encoding so target including the

00:40:23,170 --> 00:40:27,880
implementation one of our colleagues

00:40:24,969 --> 00:40:29,079
your s Vanessa that Borgia was also been

00:40:27,880 --> 00:40:30,609
as developer found that there was a

00:40:29,079 --> 00:40:32,229
better way to do target encoding so

00:40:30,609 --> 00:40:35,979
we're gonna fix this we're gonna improve

00:40:32,229 --> 00:40:37,239
target encoding but birth target

00:40:35,979 --> 00:40:39,249
encoding is similar to encoding are

00:40:37,239 --> 00:40:40,569
fairly easy to code could the Engram

00:40:39,249 --> 00:40:43,150
version for signature encoding don't

00:40:40,569 --> 00:40:45,880
bother about the other ones but yeah

00:40:43,150 --> 00:40:52,599
please do it go ahead there's one in

00:40:45,880 --> 00:40:54,900
inspark hi thank you for the talk it was

00:40:52,599 --> 00:40:57,969
very interesting I have two questions

00:40:54,900 --> 00:41:00,670
the first one is why three why the

00:40:57,969 --> 00:41:04,539
Engram number three is did you test two

00:41:00,670 --> 00:41:07,900
other numbers or is three-d-- golden

00:41:04,539 --> 00:41:09,400
standard and no.3 was more for the

00:41:07,900 --> 00:41:12,069
tactic reason in practice what we're

00:41:09,400 --> 00:41:15,009
using in these days is the two to three

00:41:12,069 --> 00:41:18,519
to four in the sub strings that are is

00:41:15,009 --> 00:41:20,859
separated by space specific characters

00:41:18,519 --> 00:41:24,880
such as spaces and this we did benchmark

00:41:20,859 --> 00:41:27,309
but we only have 17 data sets so our

00:41:24,880 --> 00:41:29,380
benchmarks are not fully trustworthy we

00:41:27,309 --> 00:41:32,440
need more benchmarks to know more data

00:41:29,380 --> 00:41:34,299
sets to do more benchmark and the second

00:41:32,440 --> 00:41:36,880
question is what if you have missing

00:41:34,299 --> 00:41:39,309
data in the dirty category what if you

00:41:36,880 --> 00:41:41,799
do not know if it's a police officer or

00:41:39,309 --> 00:41:43,269
a janitor or something good question

00:41:41,799 --> 00:41:48,219
yeah I forgot yeah I should have

00:41:43,269 --> 00:41:52,059
mentioned this so my missing data is

00:41:48,219 --> 00:41:54,579
more a problem for continuous values for

00:41:52,059 --> 00:41:57,299
categorical value I would advise in

00:41:54,579 --> 00:42:00,530
general to basically just add an

00:41:57,299 --> 00:42:03,320
indicator to represent

00:42:00,530 --> 00:42:05,480
the the missing value is a specific

00:42:03,320 --> 00:42:10,630
value in your encoding which could be

00:42:05,480 --> 00:42:10,630
zero zero zero zero by the way thank you

00:42:11,050 --> 00:42:17,840
we're interesting talk practical as well

00:42:13,850 --> 00:42:19,910
thanks do you have a plan to look into

00:42:17,840 --> 00:42:23,359
active learning at some point as well I

00:42:19,910 --> 00:42:25,910
mean I think on practice on real-world

00:42:23,359 --> 00:42:27,350
problems that might be interesting this

00:42:25,910 --> 00:42:29,240
is not our researcher than they're our

00:42:27,350 --> 00:42:32,300
research agenda is to put the human out

00:42:29,240 --> 00:42:33,770
of the loop but it is true that active

00:42:32,300 --> 00:42:36,590
learning for database curation is

00:42:33,770 --> 00:42:41,470
extremely useful and it probably

00:42:36,590 --> 00:42:41,470
complements what we're doing Thanks

00:42:42,609 --> 00:42:45,590
please give a round of applause to gaël

00:42:44,720 --> 00:42:53,590
thanks so much

00:42:45,590 --> 00:42:53,590

YouTube URL: https://www.youtube.com/watch?v=dw5u4nth6_M


