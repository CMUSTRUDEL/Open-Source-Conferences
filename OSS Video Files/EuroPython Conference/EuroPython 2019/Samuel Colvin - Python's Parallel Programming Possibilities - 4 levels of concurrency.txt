Title: Samuel Colvin - Python's Parallel Programming Possibilities - 4 levels of concurrency
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"Python's Parallel Programming Possibilities - 4 levels of concurrency
[EuroPython 2019 - Talk - 2019-07-10 - Boston]
[Basel, CH]

By Samuel Colvin

I'm going to talk about the 4 main levels of parallelism in modern Computing:


multiple (virtual) machines
multiple processes
multiple threads
multiple green threads, aka asyncio


Why you might use each of them, how to go about doing so with python and some of the pitfalls you might fall into along the way.

To do so, I'll give short examples in code of achieving each level:


leveraging multiple hosts using RQ, and also the possibility of RPC with HTTP
multiprocessing and threading using their respective modules from the python standard library
asyncio demonstrated with AIOHTTP


That sounds great, but there are ""gotchas"" you should know about before you get started, for example:


multiple machines can actually be multiple virtual machines on the same host
effectively communicating between processes is hard, how can we go about making it easier?
the limitations of threading and the GIL
runemin/emexecutor - do we ever really need to use multiprocessing or threading directly again
use of asyncio when dealing with both networking between hosts and between processes - you end up using two different kinds of concurrency at the same time. That can be confusing, but also awesome.


I'll finish of by showcasing a library I built, arq which is a job queueing and RPC library for python which uses asyncio and Redis.



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/

    "
Captions: 
	00:00:04,580 --> 00:00:09,180
so I guess I better start off by

00:00:06,720 --> 00:00:12,299
introducing myself and how I got to got

00:00:09,180 --> 00:00:13,799
to where I am so I built and run a sass

00:00:12,299 --> 00:00:16,020
platform called cheater Concha which is

00:00:13,799 --> 00:00:19,410
a monolithic django app running on

00:00:16,020 --> 00:00:22,289
Heroku using our cue for doing worker

00:00:19,410 --> 00:00:24,599
tasks we've now as we've grown build a

00:00:22,289 --> 00:00:27,240
bunch of micro surfaces outside that

00:00:24,599 --> 00:00:29,609
using a AO HTTP and a bunch of other

00:00:27,240 --> 00:00:31,050
libraries as part of that in that

00:00:29,609 --> 00:00:33,360
process over the last three or four

00:00:31,050 --> 00:00:35,940
years I've become a contributor to AO

00:00:33,360 --> 00:00:38,280
HTTP and to our queue and built a bunch

00:00:35,940 --> 00:00:41,129
of libraries of my own in particular AR

00:00:38,280 --> 00:00:42,809
q which is an async i/o successor to our

00:00:41,129 --> 00:00:44,789
queue or I will say successor and

00:00:42,809 --> 00:00:48,840
pedantic which isn't really relevant but

00:00:44,789 --> 00:00:51,090
is it's quite popular so I wanted to

00:00:48,840 --> 00:00:52,320
give this talk because I got a long way

00:00:51,090 --> 00:00:54,710
as a developer without really

00:00:52,320 --> 00:00:56,850
understanding the landscape of how

00:00:54,710 --> 00:00:59,609
parallel programming works within

00:00:56,850 --> 00:01:00,840
placing and also in general and so I

00:00:59,609 --> 00:01:03,420
wanted to kind of give a high-level

00:01:00,840 --> 00:01:04,920
introduction so I'm going to talk about

00:01:03,420 --> 00:01:06,390
the four levels of concurrency or the

00:01:04,920 --> 00:01:08,880
main four levels of concurrency that I

00:01:06,390 --> 00:01:10,260
see I'm going to demonstrate each of

00:01:08,880 --> 00:01:12,360
them with Python and I'm going to try

00:01:10,260 --> 00:01:13,920
and explain why you might use them on

00:01:12,360 --> 00:01:15,360
why you might not use them and I'm going

00:01:13,920 --> 00:01:17,190
to try and keep it mildly entertaining

00:01:15,360 --> 00:01:18,720
what I'm not going to do is try and

00:01:17,190 --> 00:01:21,600
prepare you for a computer science exam

00:01:18,720 --> 00:01:23,430
on distributed computing or we'd expect

00:01:21,600 --> 00:01:24,330
you or talk about the protocols this is

00:01:23,430 --> 00:01:26,610
going to be quite high level you're

00:01:24,330 --> 00:01:30,330
going to have to bear with me on that so

00:01:26,610 --> 00:01:31,950
why is parallel processing important I

00:01:30,330 --> 00:01:34,470
think this brass graph kind of

00:01:31,950 --> 00:01:37,800
demonstrates it this is the spread and

00:01:34,470 --> 00:01:40,020
average speed of CPUs over the last 25

00:01:37,800 --> 00:01:42,450
30 years what's interesting is that

00:01:40,020 --> 00:01:44,220
Python was conceived pretty much on

00:01:42,450 --> 00:01:48,150
there on the left-hand axis of this in

00:01:44,220 --> 00:01:51,510
about 1990 when most computers had one

00:01:48,150 --> 00:01:55,650
CPU and when CPU speeds were increasing

00:01:51,510 --> 00:01:58,110
really quickly in around 2005 that that

00:01:55,650 --> 00:02:01,260
effect plateaued and suddenly we started

00:01:58,110 --> 00:02:03,930
getting multiple CPUs in computers both

00:02:01,260 --> 00:02:05,220
in servers and in desktops I guess that

00:02:03,930 --> 00:02:06,390
was partly because people wanted it and

00:02:05,220 --> 00:02:09,020
it was really because the CPU

00:02:06,390 --> 00:02:12,209
manufacturers needs that we need to sell

00:02:09,020 --> 00:02:13,799
and so at that point Python had to had

00:02:12,209 --> 00:02:16,760
to adapt and how to implement multiple

00:02:13,799 --> 00:02:17,930
processing parallel processing but

00:02:16,760 --> 00:02:19,939
interesting thing is that it didn't

00:02:17,930 --> 00:02:21,590
start off with that they had to retrofit

00:02:19,939 --> 00:02:24,409
it later and you can still see a few of

00:02:21,590 --> 00:02:26,480
those bug bears now in the in the gale

00:02:24,409 --> 00:02:27,860
and stuff as I'll speak about later any

00:02:26,480 --> 00:02:29,480
other thing to mention is that the right

00:02:27,860 --> 00:02:31,849
hand side of this graph the kind of pick

00:02:29,480 --> 00:02:34,489
up recently may or may not be right I

00:02:31,849 --> 00:02:39,110
think this data they might have used who

00:02:34,489 --> 00:02:41,180
might have done benchmarks on hype

00:02:39,110 --> 00:02:42,409
high-end processors more recently than

00:02:41,180 --> 00:02:44,530
another one so that might be why there's

00:02:42,409 --> 00:02:47,450
that object at the end but I'm not sure

00:02:44,530 --> 00:02:50,060
so no talk would be complete without a

00:02:47,450 --> 00:02:52,609
math metaphor and so kind Tom who asked

00:02:50,060 --> 00:02:54,109
me has built this metaphor in Minecraft

00:02:52,609 --> 00:02:56,120
so the principle here is that we're

00:02:54,109 --> 00:02:58,340
thinking of a factory like a computer

00:02:56,120 --> 00:03:00,829
we're thinking of a process like a

00:02:58,340 --> 00:03:04,540
conveyor belt within a within a factory

00:03:00,829 --> 00:03:06,349
we're thinking of CPUs as a bit like

00:03:04,540 --> 00:03:08,239
individual people working on those

00:03:06,349 --> 00:03:10,879
production lines and then we're thinking

00:03:08,239 --> 00:03:15,099
of networking as the trucks coming to

00:03:10,879 --> 00:03:18,620
and leaving the leaving the factory so

00:03:15,099 --> 00:03:20,739
the highest level of parallel

00:03:18,620 --> 00:03:22,760
programming is multiple machines or

00:03:20,739 --> 00:03:24,949
computers or virtual machines or

00:03:22,760 --> 00:03:27,829
containers anything where the code sees

00:03:24,949 --> 00:03:30,190
itself as running on a specific computer

00:03:27,829 --> 00:03:31,910
and this is demonstrated here with

00:03:30,190 --> 00:03:33,530
factories as multiple different

00:03:31,910 --> 00:03:34,730
factories so instead of building one

00:03:33,530 --> 00:03:38,480
factory bigger you have multiple

00:03:34,730 --> 00:03:40,160
factories all running independently but

00:03:38,480 --> 00:03:44,030
perhaps networking between each other so

00:03:40,160 --> 00:03:45,230
in this case they are they're not

00:03:44,030 --> 00:03:46,579
networking between each other they're

00:03:45,230 --> 00:03:48,019
just doing their own thing you can

00:03:46,579 --> 00:03:50,139
imagine scenarios where we do that quite

00:03:48,019 --> 00:03:52,599
a lot so for example in front end

00:03:50,139 --> 00:03:55,010
servers on a web platform would

00:03:52,599 --> 00:03:56,269
generally talk to the database and talk

00:03:55,010 --> 00:03:57,590
to the client and use things like

00:03:56,269 --> 00:03:58,639
cookies for state but they wouldn't

00:03:57,590 --> 00:04:00,709
actually talk to each other they

00:03:58,639 --> 00:04:02,720
wouldn't know how many other machines

00:04:00,709 --> 00:04:04,190
were running around them but quite often

00:04:02,720 --> 00:04:08,840
they do have to communicate and that is

00:04:04,190 --> 00:04:10,940
where the communication comes in so to

00:04:08,840 --> 00:04:12,169
get to an example we're using our queue

00:04:10,940 --> 00:04:13,310
here I promise you this is the smallest

00:04:12,169 --> 00:04:14,979
text will go at any point in this

00:04:13,310 --> 00:04:19,310
presentation I hope you can all read it

00:04:14,979 --> 00:04:20,630
so our Q is queuing Lyautey library

00:04:19,310 --> 00:04:25,190
built with Redis as the name indicates

00:04:20,630 --> 00:04:27,110
in particular it uses Redis is lists to

00:04:25,190 --> 00:04:28,970
do the in curing so doing cure a job you

00:04:27,110 --> 00:04:31,190
push it into one end of the list and

00:04:28,970 --> 00:04:33,040
to execute the job you pop it out the

00:04:31,190 --> 00:04:35,210
other end of list and then execute it so

00:04:33,040 --> 00:04:37,660
the code we're going to use for most of

00:04:35,210 --> 00:04:40,820
our work of examples here is in the top

00:04:37,660 --> 00:04:43,010
top file here worker dot pi it's very

00:04:40,820 --> 00:04:46,070
simple it just downloads a web page in

00:04:43,010 --> 00:04:47,210
this case from Python Europe for one of

00:04:46,070 --> 00:04:49,730
the last few years

00:04:47,210 --> 00:04:51,320
takes the text and counts the number of

00:04:49,730 --> 00:04:53,590
words so splits it and then counts a

00:04:51,320 --> 00:04:55,070
number of elements in the resulting list

00:04:53,590 --> 00:04:57,290
very simple

00:04:55,070 --> 00:04:58,760
in reality you wouldn't need multiple

00:04:57,290 --> 00:05:01,220
machines or even multiple anything to do

00:04:58,760 --> 00:05:03,860
this but that's our example below you

00:05:01,220 --> 00:05:07,040
see the code we would use to incur those

00:05:03,860 --> 00:05:08,930
jobs using our queue so we take it

00:05:07,040 --> 00:05:10,760
completely vanilla Redis connection in

00:05:08,930 --> 00:05:14,180
this case I'm demonstrating it here on

00:05:10,760 --> 00:05:15,200
my laptop so I'm not actually running on

00:05:14,180 --> 00:05:16,610
multiple machines I'm running on

00:05:15,200 --> 00:05:18,590
multiple processes to demonstrate the

00:05:16,610 --> 00:05:21,230
principle but bear with me and then

00:05:18,590 --> 00:05:23,960
we're for each of the last four years in

00:05:21,230 --> 00:05:25,790
queueing a job where we where we run

00:05:23,960 --> 00:05:27,170
count words now the quick one of the

00:05:25,790 --> 00:05:29,840
interesting things to see here is that

00:05:27,170 --> 00:05:31,100
even in our our queue example which is

00:05:29,840 --> 00:05:33,680
running on the main machine not the

00:05:31,100 --> 00:05:36,740
worker we have to have access to the

00:05:33,680 --> 00:05:39,290
count words function so we can import it

00:05:36,740 --> 00:05:41,510
so we can include the job so if we look

00:05:39,290 --> 00:05:43,640
at running that below first thing we do

00:05:41,510 --> 00:05:45,800
on the right here in our two workers is

00:05:43,640 --> 00:05:49,100
we call our queue worker and that starts

00:05:45,800 --> 00:05:51,350
the worker which is doing a blocking pop

00:05:49,100 --> 00:05:55,430
from Redis waiting to execute the jobs

00:05:51,350 --> 00:05:58,010
to include those jobs we simply call our

00:05:55,430 --> 00:06:00,440
example here our key worker that bangs

00:05:58,010 --> 00:06:02,180
those four jobs into the queue and

00:06:00,440 --> 00:06:03,440
prints out the result which is what our

00:06:02,180 --> 00:06:05,960
queue gives us that we could use to get

00:06:03,440 --> 00:06:08,900
the actual result later and then you see

00:06:05,960 --> 00:06:10,760
those jobs being executed in in our

00:06:08,900 --> 00:06:12,260
queue here and if you can see closely

00:06:10,760 --> 00:06:14,030
enough you can see the years and how

00:06:12,260 --> 00:06:15,760
many words it's not very interesting but

00:06:14,030 --> 00:06:18,580
there we are

00:06:15,760 --> 00:06:20,210
so the advantages of multiple machines

00:06:18,580 --> 00:06:22,970
scalability is the big thing

00:06:20,210 --> 00:06:24,620
you can add machines very easily also

00:06:22,970 --> 00:06:26,690
adding machines as a linear cost if you

00:06:24,620 --> 00:06:29,600
have ten machines and you want an

00:06:26,690 --> 00:06:32,120
eleventh it gets 10% more expensive and

00:06:29,600 --> 00:06:34,100
lastly isolation which is demonstrated

00:06:32,120 --> 00:06:37,780
here with our factories if one of your

00:06:34,100 --> 00:06:37,780
factories were to blow up

00:06:38,980 --> 00:06:44,750
you can simply add another add another

00:06:42,260 --> 00:06:47,390
factory on the side or in the case of my

00:06:44,750 --> 00:06:50,620
craft pan to the left because adding a

00:06:47,390 --> 00:06:50,620
new factory in real time is too hard

00:06:52,120 --> 00:06:56,120
disadvantages and multiple machines well

00:06:54,170 --> 00:06:57,590
mainly complexity you have to serve up

00:06:56,120 --> 00:06:59,750
all your networking between your

00:06:57,590 --> 00:07:02,120
machines that's made very easy by

00:06:59,750 --> 00:07:04,100
platforms like Heroku and others but it

00:07:02,120 --> 00:07:05,930
still can be a problem particularly

00:07:04,100 --> 00:07:10,490
during development and so as you saw

00:07:05,930 --> 00:07:14,200
earlier quite often we use multiple

00:07:10,490 --> 00:07:14,200
processes to simulate multiple machines

00:07:14,260 --> 00:07:19,670
so to go onto multiple processes this is

00:07:17,080 --> 00:07:23,140
within a single factory running multiple

00:07:19,670 --> 00:07:24,890
production lines in in our analogy

00:07:23,140 --> 00:07:26,930
processes are an operating system

00:07:24,890 --> 00:07:28,970
concept and they're designed to keep

00:07:26,930 --> 00:07:30,820
different programs isolated from each

00:07:28,970 --> 00:07:33,170
other

00:07:30,820 --> 00:07:35,030
whilst what's running at the same time

00:07:33,170 --> 00:07:36,350
so they were developed for I guess

00:07:35,030 --> 00:07:37,460
originally for desktop applications

00:07:36,350 --> 00:07:39,920
where you were running two completely

00:07:37,460 --> 00:07:43,850
separate things but you can use them to

00:07:39,920 --> 00:07:45,530
run the same code in parallel so here's

00:07:43,850 --> 00:07:47,390
our example you see immediately it's

00:07:45,530 --> 00:07:48,830
quite a lot longer and the other main

00:07:47,390 --> 00:07:50,900
difference here is we're not using an

00:07:48,830 --> 00:07:53,330
external library for for the queueing

00:07:50,900 --> 00:07:56,090
we're using license standard multi

00:07:53,330 --> 00:07:58,850
processing library - standard process

00:07:56,090 --> 00:08:03,170
and joinable queue so we have exactly

00:07:58,850 --> 00:08:06,230
the same code here for adding up stuff

00:08:03,170 --> 00:08:08,120
for counting the words then we have our

00:08:06,230 --> 00:08:10,970
very rudimentary worker which is just a

00:08:08,120 --> 00:08:13,760
loop that runs taking jobs out of a

00:08:10,970 --> 00:08:15,920
queue and either executing them or if

00:08:13,760 --> 00:08:19,940
the the job is none that's our cue to

00:08:15,920 --> 00:08:21,590
quit and so - in queue those we have to

00:08:19,940 --> 00:08:23,180
create our processes the really

00:08:21,590 --> 00:08:26,750
interesting bit here is happening on

00:08:23,180 --> 00:08:29,150
line 20 what pythons doing in the

00:08:26,750 --> 00:08:32,210
background there is forking the main

00:08:29,150 --> 00:08:34,880
Python process to form multiple sub

00:08:32,210 --> 00:08:36,950
processes which have which at that point

00:08:34,880 --> 00:08:38,510
share memory but any further changes of

00:08:36,950 --> 00:08:39,920
memory would be copy-on-write so they

00:08:38,510 --> 00:08:41,900
would be changed so we now have

00:08:39,920 --> 00:08:43,610
completely separate processes and that

00:08:41,900 --> 00:08:45,650
new process is set off to run run our

00:08:43,610 --> 00:08:47,150
worker function we just saw with the

00:08:45,650 --> 00:08:50,300
argument in this case is just an ID to

00:08:47,150 --> 00:08:51,520
tell us what worker we're running in

00:08:50,300 --> 00:08:53,740
queueing our jobs is simply

00:08:51,520 --> 00:08:55,630
as simple as doing put on our on our cue

00:08:53,740 --> 00:08:58,750
object the Python this helpfully given

00:08:55,630 --> 00:09:00,430
us we can then wait for that cue to be

00:08:58,750 --> 00:09:03,400
empty for all of the jobs to be finished

00:09:00,430 --> 00:09:05,230
then we have to go about putting the nun

00:09:03,400 --> 00:09:06,940
job into each of those queues to stop

00:09:05,230 --> 00:09:09,700
them and then we wait for our processes

00:09:06,940 --> 00:09:13,450
to finish and you see there it printing

00:09:09,700 --> 00:09:18,760
out our our words as it did before

00:09:13,450 --> 00:09:20,250
again not interesting so the advantages

00:09:18,760 --> 00:09:23,140
of processes they're really easy to run

00:09:20,250 --> 00:09:24,580
no Networking required you get this off

00:09:23,140 --> 00:09:25,870
level guarantee that your multiple

00:09:24,580 --> 00:09:27,970
different processes are isolated they

00:09:25,870 --> 00:09:29,950
can't share memory after they've been

00:09:27,970 --> 00:09:31,660
forked and they're pretty fast to

00:09:29,950 --> 00:09:34,810
communicate either by doing networking

00:09:31,660 --> 00:09:36,670
within on a machine or multi process

00:09:34,810 --> 00:09:39,610
inter process communication all very

00:09:36,670 --> 00:09:41,590
quick compared to multiple machines I

00:09:39,610 --> 00:09:44,320
should say but the disadvantage of

00:09:41,590 --> 00:09:46,480
processes are quite significant you have

00:09:44,320 --> 00:09:48,130
those fixed limits to scaling if we go

00:09:46,480 --> 00:09:50,470
back to our factory analogy and we want

00:09:48,130 --> 00:09:52,990
to add another production line into our

00:09:50,470 --> 00:09:54,460
factory there's nowhere to put it we

00:09:52,990 --> 00:09:56,470
want to have 4 production lines we need

00:09:54,460 --> 00:09:58,840
to build a whole new factory I guess

00:09:56,470 --> 00:10:00,880
decommission our old factory and start

00:09:58,840 --> 00:10:02,230
running running our new factory we want

00:10:00,880 --> 00:10:03,610
to go back to having three then I guess

00:10:02,230 --> 00:10:05,680
we have to ignore our new full

00:10:03,610 --> 00:10:08,650
production line factory and go back to

00:10:05,680 --> 00:10:10,360
the three and secondly it gets really

00:10:08,650 --> 00:10:12,070
really hard to build a really really big

00:10:10,360 --> 00:10:14,020
factory so we can make it five times

00:10:12,070 --> 00:10:15,490
bigger or ten times bigger but it gets

00:10:14,020 --> 00:10:18,040
prohibitively expensive to have a

00:10:15,490 --> 00:10:21,190
thousand core machine so it's not linear

00:10:18,040 --> 00:10:23,950
to scale whereas you saw with multiple

00:10:21,190 --> 00:10:25,510
machines it was it was linear and again

00:10:23,950 --> 00:10:29,280
we don't have isolation if wonder if our

00:10:25,510 --> 00:10:29,280
machine breaks the whole thing's broken

00:10:29,760 --> 00:10:35,470
so next we get out of multiple threads

00:10:32,790 --> 00:10:37,600
so thread to a way of achieving

00:10:35,470 --> 00:10:40,510
concurrency with from within one process

00:10:37,600 --> 00:10:42,160
they come in kind of two variants kernel

00:10:40,510 --> 00:10:44,170
threads and user threads or green

00:10:42,160 --> 00:10:45,610
threads when we talk about threading in

00:10:44,170 --> 00:10:48,370
Python we're talking about kernel

00:10:45,610 --> 00:10:50,140
threads so it's important to remember

00:10:48,370 --> 00:10:53,830
that kernel threads are the only way

00:10:50,140 --> 00:10:57,010
from within a process to run a task on

00:10:53,830 --> 00:10:58,720
two different CPUs at the same time so

00:10:57,010 --> 00:11:00,220
we can do lots of things that look like

00:10:58,720 --> 00:11:02,440
parallel but unless we have kernel

00:11:00,220 --> 00:11:03,940
threads we we can't be running two

00:11:02,440 --> 00:11:04,650
things on two different CPUs at the same

00:11:03,940 --> 00:11:07,050
time

00:11:04,650 --> 00:11:08,340
and in our analogy here a production

00:11:07,050 --> 00:11:09,840
line has changed shape and you see we

00:11:08,340 --> 00:11:11,460
have three of these boxes that

00:11:09,840 --> 00:11:12,720
technically have phases and they're

00:11:11,460 --> 00:11:14,370
supposed to represent the workers so

00:11:12,720 --> 00:11:18,090
we're running multiple things on the

00:11:14,370 --> 00:11:20,160
same on the same process so are

00:11:18,090 --> 00:11:22,140
threating example looks suspiciously

00:11:20,160 --> 00:11:22,800
like our multi processing one that's not

00:11:22,140 --> 00:11:25,050
a coincidence

00:11:22,800 --> 00:11:26,850
pythons tried quite hard to keep the

00:11:25,050 --> 00:11:28,200
interface is the same between processing

00:11:26,850 --> 00:11:30,480
and threading so we have the same

00:11:28,200 --> 00:11:31,770
function as before we have exactly the

00:11:30,480 --> 00:11:33,720
same worker except we said quitting

00:11:31,770 --> 00:11:35,580
thread instead of quitting worker the

00:11:33,720 --> 00:11:38,970
difference is our imports that we're

00:11:35,580 --> 00:11:40,500
importing here from from cue and from

00:11:38,970 --> 00:11:42,930
threading to use those versions rather

00:11:40,500 --> 00:11:44,490
than the multi processing variants this

00:11:42,930 --> 00:11:47,220
is all basically the same except

00:11:44,490 --> 00:11:50,190
obviously line 21 where we create the

00:11:47,220 --> 00:11:51,750
thread we're creating a secondary sub

00:11:50,190 --> 00:11:53,160
thread within the main within the same

00:11:51,750 --> 00:11:56,730
Python process instead of creating

00:11:53,160 --> 00:11:58,680
multiple processes again we bang those

00:11:56,730 --> 00:12:00,830
and the years into the queue to run the

00:11:58,680 --> 00:12:02,970
run the workers wait for them to finish

00:12:00,830 --> 00:12:05,400
kill the threads and wait for them to

00:12:02,970 --> 00:12:09,480
have been killed and then we get the

00:12:05,400 --> 00:12:11,100
result again so the advantages of

00:12:09,480 --> 00:12:13,140
threads they're lighter even lighter

00:12:11,100 --> 00:12:15,240
than processes they're faster to create

00:12:13,140 --> 00:12:17,460
and faster switch between and they share

00:12:15,240 --> 00:12:20,100
memory which can be an advantage that

00:12:17,460 --> 00:12:21,660
can also be a big disadvantage and so

00:12:20,100 --> 00:12:23,340
disadvantages is kind of exactly the

00:12:21,660 --> 00:12:26,210
same thing they share memory so memory

00:12:23,340 --> 00:12:28,260
locking is horrid to use a go proverb

00:12:26,210 --> 00:12:30,270
communicate by sharing memory and said

00:12:28,260 --> 00:12:34,230
don't do not communicate by sharing

00:12:30,270 --> 00:12:35,820
memory share memory by communicating so

00:12:34,230 --> 00:12:37,680
we can't do that with pison threading

00:12:35,820 --> 00:12:39,450
Python provides some primitives for

00:12:37,680 --> 00:12:41,280
doing communication between threads that

00:12:39,450 --> 00:12:42,690
is secure but if you're not careful it

00:12:41,280 --> 00:12:45,120
can all go wrong and you won't get a

00:12:42,690 --> 00:12:47,130
nice warning it'll just burst into

00:12:45,120 --> 00:12:49,020
flames the second and bigger problem

00:12:47,130 --> 00:12:51,750
that is specific to python is the global

00:12:49,020 --> 00:12:53,670
interpreter lock so from the Python wiki

00:12:51,750 --> 00:12:54,810
the Galle protects Python objects

00:12:53,670 --> 00:12:57,780
preventing multiple threads for

00:12:54,810 --> 00:13:00,150
executing Python code more than one

00:12:57,780 --> 00:13:02,370
thread from executing code at once what

00:13:00,150 --> 00:13:04,020
so so the whole idea here was we would

00:13:02,370 --> 00:13:05,160
run stuff in parallel and now we've

00:13:04,020 --> 00:13:07,560
heard about this lock thing that

00:13:05,160 --> 00:13:09,540
prevents us from doing that at all let

00:13:07,560 --> 00:13:11,370
me let me try and demonstrate that with

00:13:09,540 --> 00:13:13,380
it with another example we've taken

00:13:11,370 --> 00:13:15,570
pretty much the same code but instead of

00:13:13,380 --> 00:13:17,250
doing a network request now we're doing

00:13:15,570 --> 00:13:18,840
something CPU bound so in this case

00:13:17,250 --> 00:13:21,300
counting a bunch of numbers and we're

00:13:18,840 --> 00:13:23,400
using standard Python some for doing

00:13:21,300 --> 00:13:25,020
that and we're going to do that in two

00:13:23,400 --> 00:13:26,490
ways one we're going to do that in a

00:13:25,020 --> 00:13:28,410
normal for loop just doing that task

00:13:26,490 --> 00:13:30,270
four times and in the other case we're

00:13:28,410 --> 00:13:31,560
going to go through all the pallava of

00:13:30,270 --> 00:13:34,530
creating our threads and running them in

00:13:31,560 --> 00:13:36,180
parallel what happens well it's not very

00:13:34,530 --> 00:13:38,040
exciting we actually get exactly the

00:13:36,180 --> 00:13:39,270
same time in fact it's even slightly

00:13:38,040 --> 00:13:40,860
quicker to do it without multi

00:13:39,270 --> 00:13:42,720
processing without multi-threading

00:13:40,860 --> 00:13:45,380
because we don't have to have the

00:13:42,720 --> 00:13:48,690
overload overhead of creating threads

00:13:45,380 --> 00:13:50,820
all is not lost you can do this same

00:13:48,690 --> 00:13:53,130
task with multiple threads and make it

00:13:50,820 --> 00:13:56,190
quicker here we're using numpy so num

00:13:53,130 --> 00:13:59,100
has some function is gonna do the

00:13:56,190 --> 00:14:01,080
summation in c c in turn can release the

00:13:59,100 --> 00:14:02,280
global interpreter lock and so here we

00:14:01,080 --> 00:14:04,500
can get the advantage of multiple

00:14:02,280 --> 00:14:07,200
threads so it's going to be quicker

00:14:04,500 --> 00:14:09,180
because it's done in c but also we see

00:14:07,200 --> 00:14:11,160
here that we nearly have the time by

00:14:09,180 --> 00:14:12,450
doing it in multiple threads so I guess

00:14:11,160 --> 00:14:13,920
they're not quite half is that we

00:14:12,450 --> 00:14:17,700
overhead of creating those those

00:14:13,920 --> 00:14:18,990
different threads and so anything where

00:14:17,700 --> 00:14:21,300
we can release the global interpreter

00:14:18,990 --> 00:14:25,530
lock using in because we're executing in

00:14:21,300 --> 00:14:27,390
C or where we're doing file IO tasks or

00:14:25,530 --> 00:14:30,690
we're doing networking threading can

00:14:27,390 --> 00:14:35,130
help but in pure Python process

00:14:30,690 --> 00:14:37,260
cpu-bound tasks doesn't really help so

00:14:35,130 --> 00:14:40,320
lastly we come to the fourth level of

00:14:37,260 --> 00:14:42,390
parallelism within well we didn't but

00:14:40,320 --> 00:14:45,150
also not necessarily unique to Python

00:14:42,390 --> 00:14:46,890
which is async i/o I think this is

00:14:45,150 --> 00:14:48,480
really cool I am obsessed by async I oh

00:14:46,890 --> 00:14:50,070
I think it's wonderful and I will try

00:14:48,480 --> 00:14:51,900
and persuade you that it's the way to go

00:14:50,070 --> 00:14:53,190
for lots and lots of things the idea

00:14:51,900 --> 00:14:55,790
here is we're doing cooperative

00:14:53,190 --> 00:14:58,800
scheduling so we have one kernel thread

00:14:55,790 --> 00:15:00,990
but within that we have some wonderful

00:14:58,800 --> 00:15:02,220
tools that allow us to seem like we're

00:15:00,990 --> 00:15:03,360
doing things at the same time when in

00:15:02,220 --> 00:15:06,780
the background we're actually only

00:15:03,360 --> 00:15:08,430
executing one bit of code at a time to

00:15:06,780 --> 00:15:10,500
do this we have an event loop that's

00:15:08,430 --> 00:15:12,660
effectively scheduling tasks in a way to

00:15:10,500 --> 00:15:15,540
keep something happening all the time so

00:15:12,660 --> 00:15:17,010
I promise you I won't carry on pushing

00:15:15,540 --> 00:15:19,589
the metaphor any any longer after this

00:15:17,010 --> 00:15:22,050
but without async i/o you see here in

00:15:19,589 --> 00:15:23,550
our top example when we're doing

00:15:22,050 --> 00:15:25,709
networking our thread has to stop

00:15:23,550 --> 00:15:27,360
because it is waiting for the networking

00:15:25,709 --> 00:15:29,080
to come back and give us a response

00:15:27,360 --> 00:15:32,980
that's thread and perhaps that hope

00:15:29,080 --> 00:15:34,600
to stop and wait for wait for the

00:15:32,980 --> 00:15:36,940
networking to finish before it can go on

00:15:34,600 --> 00:15:39,460
and do something out something else with

00:15:36,940 --> 00:15:43,960
async IO on the other hand our thread

00:15:39,460 --> 00:15:45,790
can carry on processing as as networking

00:15:43,960 --> 00:15:47,290
tasks are going on because our event

00:15:45,790 --> 00:15:51,730
loop is doing a clever job of scheduling

00:15:47,290 --> 00:15:53,290
tasks to fill in the gaps so in example

00:15:51,730 --> 00:15:54,790
first of all you immediately see it's

00:15:53,290 --> 00:15:56,410
already shorter than our examples before

00:15:54,790 --> 00:16:00,610
we don't have to do half as much fat and

00:15:56,410 --> 00:16:03,850
setup we do however have to call our Co

00:16:00,610 --> 00:16:06,760
routine are using in this case I think I

00:16:03,850 --> 00:16:09,040
owed run if it was Java JavaScript you

00:16:06,760 --> 00:16:10,450
could just set off your care routine and

00:16:09,040 --> 00:16:12,280
hope for the best or finish in the end

00:16:10,450 --> 00:16:14,500
and no one seems to mind in Python you

00:16:12,280 --> 00:16:15,940
have to either a way to Co routine or

00:16:14,500 --> 00:16:18,340
set it off like this if you haven't got

00:16:15,940 --> 00:16:22,270
an event loop running so our main hair

00:16:18,340 --> 00:16:24,760
routine is simply calling our count

00:16:22,270 --> 00:16:28,180
count words parity which I'll get to in

00:16:24,760 --> 00:16:30,880
a moment and putting the result of that

00:16:28,180 --> 00:16:33,280
which is the future into into a list and

00:16:30,880 --> 00:16:36,040
then lastly we use the special pair

00:16:33,280 --> 00:16:38,290
routine async Toyota gather to wait for

00:16:36,040 --> 00:16:41,070
the results of those those four pair

00:16:38,290 --> 00:16:44,530
routines and once they finished proceed

00:16:41,070 --> 00:16:46,540
so how does count words work and here we

00:16:44,530 --> 00:16:49,240
get to the big problem with a sing heyo

00:16:46,540 --> 00:16:50,560
we can no longer use requests we've had

00:16:49,240 --> 00:16:53,680
to rewrite this function internally in

00:16:50,560 --> 00:16:55,570
this case we're using a i/o HTTP we have

00:16:53,680 --> 00:16:59,080
to create our session explicitly in this

00:16:55,570 --> 00:17:02,260
case really requested it implicitly then

00:16:59,080 --> 00:17:03,880
we do our get request we get back this

00:17:02,260 --> 00:17:06,280
is a context manager and asynchronous

00:17:03,880 --> 00:17:08,770
context manager we get our response we

00:17:06,280 --> 00:17:10,930
can then await reading the text off the

00:17:08,770 --> 00:17:12,100
network for the response to that and

00:17:10,930 --> 00:17:15,610
finally we can do the same thing as

00:17:12,100 --> 00:17:18,820
before and count the number of number of

00:17:15,610 --> 00:17:22,000
words on our page and you see the result

00:17:18,820 --> 00:17:24,220
again so advantages of async i/o even

00:17:22,000 --> 00:17:26,500
lighter than processes and threads we

00:17:24,220 --> 00:17:28,650
can quite happily have say thousands of

00:17:26,500 --> 00:17:31,210
WebSockets connected to a single host

00:17:28,650 --> 00:17:33,460
processing all of them without enormous

00:17:31,210 --> 00:17:35,260
amounts of CPU or memory usage they're a

00:17:33,460 --> 00:17:36,700
lot easier to reason with because you

00:17:35,260 --> 00:17:39,010
are explicit about where you're going to

00:17:36,700 --> 00:17:42,230
go and do some networking where your

00:17:39,010 --> 00:17:44,809
current piece of code is going

00:17:42,230 --> 00:17:46,519
release and doing a wait and so other

00:17:44,809 --> 00:17:47,779
code might get executed when you're

00:17:46,519 --> 00:17:51,739
doing that looking when you're not doing

00:17:47,779 --> 00:17:53,090
networking and there's technically less

00:17:51,739 --> 00:17:54,409
risk of memory corruption because we're

00:17:53,090 --> 00:17:58,369
only ever running one bit of pattern the

00:17:54,409 --> 00:18:00,350
other time this advantage is well we

00:17:58,369 --> 00:18:02,359
don't get any speed-up of CPU at all by

00:18:00,350 --> 00:18:04,609
using vanilla icing heyo but the real

00:18:02,359 --> 00:18:06,980
problem is whole new way of thinking and

00:18:04,609 --> 00:18:09,950
in general you have to kind of rewrite

00:18:06,980 --> 00:18:11,389
applications it's it's possible in

00:18:09,950 --> 00:18:12,710
theory to adapt them but in general I

00:18:11,389 --> 00:18:14,539
think you basically have to abandon an

00:18:12,710 --> 00:18:16,460
existing project and start again if

00:18:14,539 --> 00:18:18,109
you're gonna start using async i/o all

00:18:16,460 --> 00:18:20,779
over the place might be able to get away

00:18:18,109 --> 00:18:23,139
with using in a few places but in

00:18:20,779 --> 00:18:26,359
general it's it's a whole new rewrite

00:18:23,139 --> 00:18:27,529
the point here is the whole brilliant

00:18:26,359 --> 00:18:29,419
thing about async i/o is that it's

00:18:27,529 --> 00:18:30,919
explicit but that means it can't be

00:18:29,419 --> 00:18:32,269
implicit you can't have some library

00:18:30,919 --> 00:18:35,210
that wraps around async oh there was

00:18:32,269 --> 00:18:37,100
someone asking it on Python Python ideas

00:18:35,210 --> 00:18:40,129
recently can't we make it implicit the

00:18:37,100 --> 00:18:41,929
whole point is it's not the point where

00:18:40,129 --> 00:18:43,039
it gets really tricky is where all of

00:18:41,929 --> 00:18:45,769
these four levels of parallelism

00:18:43,039 --> 00:18:48,590
interlink with each other so machines

00:18:45,769 --> 00:18:50,149
the RQ example I sort of showed you RQ

00:18:48,590 --> 00:18:53,299
actually does forking in the background

00:18:50,149 --> 00:18:55,159
to run its worker the multi processing

00:18:53,299 --> 00:18:56,749
joinable queue that I showed you is in

00:18:55,159 --> 00:18:59,119
fact using a thread in the background

00:18:56,749 --> 00:19:02,809
did to put things into the into the

00:18:59,119 --> 00:19:04,639
queue async i/o has thread pool executor

00:19:02,809 --> 00:19:05,409
a process port executor that I'll show

00:19:04,639 --> 00:19:07,580
you in a minute

00:19:05,409 --> 00:19:08,960
machines when they're communicating with

00:19:07,580 --> 00:19:10,580
each other because it's networking you

00:19:08,960 --> 00:19:13,220
then want to go into the async i/o world

00:19:10,580 --> 00:19:15,169
and AR queue and area HTTP that do that

00:19:13,220 --> 00:19:16,820
but all of these things interact and so

00:19:15,169 --> 00:19:21,549
it can get a bit confusing kind of where

00:19:16,820 --> 00:19:23,809
we are I want to talk about one of the

00:19:21,549 --> 00:19:25,489
uses of async ioad I don't think enough

00:19:23,809 --> 00:19:27,950
people are talking about which is in

00:19:25,489 --> 00:19:29,350
being a sane way of doing multi

00:19:27,950 --> 00:19:31,549
processing and multi-threading

00:19:29,350 --> 00:19:35,749
particularly multi-threading for file

00:19:31,549 --> 00:19:37,850
operations and multi processing for CPU

00:19:35,749 --> 00:19:39,440
bound tasks you get all of the

00:19:37,850 --> 00:19:41,239
performance improvements from threading

00:19:39,440 --> 00:19:42,859
or processes but from the comfort of a

00:19:41,239 --> 00:19:49,009
sink I am and it's much easier to reason

00:19:42,859 --> 00:19:51,529
with so have an example here we were

00:19:49,009 --> 00:19:54,139
using our same do caps as we had just

00:19:51,529 --> 00:19:55,130
now in numpy so we know that that is

00:19:54,139 --> 00:19:57,020
suitable

00:19:55,130 --> 00:20:01,220
a candidate for multi-threading because

00:19:57,020 --> 00:20:03,919
it releases the the kill but instead of

00:20:01,220 --> 00:20:06,830
just calling Arco routine we now have to

00:20:03,919 --> 00:20:10,070
create this thread pool executor that's

00:20:06,830 --> 00:20:12,740
creating a pool of threads in which to

00:20:10,070 --> 00:20:15,500
run our tasks and the clever bit is run

00:20:12,740 --> 00:20:17,380
an executor which returns a KO routine

00:20:15,500 --> 00:20:20,870
that waits for the tasks to be completed

00:20:17,380 --> 00:20:23,090
within the thread and there's process

00:20:20,870 --> 00:20:25,370
called executor which has exactly the

00:20:23,090 --> 00:20:26,809
same uses just a different name and is

00:20:25,370 --> 00:20:30,289
obviously creating multiple processes

00:20:26,809 --> 00:20:32,919
and doing it that way and so we create

00:20:30,289 --> 00:20:34,970
this list of co-routines and again

00:20:32,919 --> 00:20:38,260
gather them wait for them all to be

00:20:34,970 --> 00:20:40,130
completed and hey presto we get a time

00:20:38,260 --> 00:20:43,370
again we get the speed-up

00:20:40,130 --> 00:20:49,789
of multi-threading but from the comfort

00:20:43,370 --> 00:20:50,990
of a sink i/o so in summary I think I

00:20:49,789 --> 00:20:57,140
probably not taking up half enough time

00:20:50,990 --> 00:20:59,360
I don't ever clean we've talked about

00:20:57,140 --> 00:21:01,159
the four levels of concurrency which

00:20:59,360 --> 00:21:03,590
said that they're all possible with

00:21:01,159 --> 00:21:05,419
pison none of them are unique to Python

00:21:03,590 --> 00:21:07,610
but they're all possible async i/o I

00:21:05,419 --> 00:21:10,700
think today Python is probably leading

00:21:07,610 --> 00:21:12,409
the way in its ease at least even though

00:21:10,700 --> 00:21:13,820
it came to the party late I think it's

00:21:12,409 --> 00:21:15,559
kind of accepted as one of the best

00:21:13,820 --> 00:21:18,590
implementations of that I definitely

00:21:15,559 --> 00:21:20,179
think it's cleaner than than what's

00:21:18,590 --> 00:21:22,010
going in going on in other languages

00:21:20,179 --> 00:21:27,049
except for arguably JavaScript but that

00:21:22,010 --> 00:21:28,280
has its own own problems problems they

00:21:27,049 --> 00:21:29,750
all have their strengths and weaknesses

00:21:28,280 --> 00:21:31,309
and the key thing is to work out which

00:21:29,750 --> 00:21:33,860
one you want to use for a particular

00:21:31,309 --> 00:21:35,539
application and then they often interact

00:21:33,860 --> 00:21:37,909
with each other so they're not unique

00:21:35,539 --> 00:21:39,500
they don't get to stand in their high

00:21:37,909 --> 00:21:41,840
castle and be on their own they're all

00:21:39,500 --> 00:21:43,100
into interlink with each other but the

00:21:41,840 --> 00:21:45,460
real point I'm trying to get across

00:21:43,100 --> 00:21:48,470
today is that there's this landscape of

00:21:45,460 --> 00:21:50,840
different processes and process is a bad

00:21:48,470 --> 00:21:52,370
word to use of different tools out there

00:21:50,840 --> 00:21:53,390
and you need to have a bit of an

00:21:52,370 --> 00:21:55,700
understanding about what they're doing

00:21:53,390 --> 00:21:57,650
just taking the first working example

00:21:55,700 --> 00:21:59,750
off the top of the page putting it into

00:21:57,650 --> 00:22:01,760
an editor pressing pressing run and see

00:21:59,750 --> 00:22:03,260
what happens gets you a long way they've

00:22:01,760 --> 00:22:05,780
got me to a company that pays my salary

00:22:03,260 --> 00:22:07,970
but it's not always the best way and it

00:22:05,780 --> 00:22:08,760
becomes a problem when everything when

00:22:07,970 --> 00:22:09,840
everything goes wrong

00:22:08,760 --> 00:22:10,980
and you're trying to understand what's

00:22:09,840 --> 00:22:13,530
happening and you have no grounding

00:22:10,980 --> 00:22:14,820
because you've you've just taken the

00:22:13,530 --> 00:22:16,140
example that got it to work which is

00:22:14,820 --> 00:22:19,050
definitely what I did the first time

00:22:16,140 --> 00:22:19,800
around so thank you very much and I

00:22:19,050 --> 00:22:21,060
guess we've got lots of time for

00:22:19,800 --> 00:22:24,240
questions

00:22:21,060 --> 00:22:24,240
[Applause]

00:22:28,760 --> 00:22:32,100
having said questions if you've got a

00:22:30,720 --> 00:22:33,870
couple of minutes I'll do a little tiny

00:22:32,100 --> 00:22:36,330
bit of advertising some packages I've

00:22:33,870 --> 00:22:39,960
built since we've got a second so a RQ

00:22:36,330 --> 00:22:42,630
is a successor to RQ but it uses async

00:22:39,960 --> 00:22:44,700
i/o so its uses the async bindings for

00:22:42,630 --> 00:22:47,490
Redis and it allows you to in queue

00:22:44,700 --> 00:22:50,160
tasks from a i/o HTTP application or

00:22:47,490 --> 00:22:52,830
similar it also has some other useful

00:22:50,160 --> 00:22:54,570
features so it has this principle of

00:22:52,830 --> 00:22:56,220
every job has to be finished so it might

00:22:54,570 --> 00:22:58,200
be one run multiple times but it has to

00:22:56,220 --> 00:23:00,420
be executed doesn't actually use a list

00:22:58,200 --> 00:23:02,490
it uses a sorted set which means you can

00:23:00,420 --> 00:23:04,740
in queue tasks to be run at some point

00:23:02,490 --> 00:23:06,210
in the future and if they get stopped it

00:23:04,740 --> 00:23:08,220
automatically reruns them again when it

00:23:06,210 --> 00:23:09,120
comes back up dev tools I think is the

00:23:08,220 --> 00:23:11,070
most interesting thing I've ever written

00:23:09,120 --> 00:23:12,810
and no one seems to care at all so I'd

00:23:11,070 --> 00:23:14,310
love your feedback on it it's a it's

00:23:12,810 --> 00:23:15,780
basically a better print command that

00:23:14,310 --> 00:23:18,150
tells you the line where it happened and

00:23:15,780 --> 00:23:20,190
what you printed and prints it in a

00:23:18,150 --> 00:23:22,200
pretty way I use it all the time but I

00:23:20,190 --> 00:23:23,850
failed so far to persuade anyone Ounces

00:23:22,200 --> 00:23:27,390
interesting and pedantic is quite widely

00:23:23,850 --> 00:23:29,790
used as a type engine using Python data

00:23:27,390 --> 00:23:33,810
classic using Python type-ins thank you

00:23:29,790 --> 00:23:36,750
now questions lots of time for questions

00:23:33,810 --> 00:23:39,360
so we have also have two microphones

00:23:36,750 --> 00:23:43,710
over there you can probably see them but

00:23:39,360 --> 00:23:47,670
so please line up if you have questions

00:23:43,710 --> 00:23:51,860
line up behind the microphones and we'll

00:23:47,670 --> 00:23:54,090
be able to take a number of questions

00:23:51,860 --> 00:23:56,700
quite a number of questions I see we

00:23:54,090 --> 00:23:58,680
have one question so go for it maybe I

00:23:56,700 --> 00:24:02,880
didn't understand you well what you said

00:23:58,680 --> 00:24:09,150
there is no good tooling to do machine

00:24:02,880 --> 00:24:12,570
level parallelism but but as I

00:24:09,150 --> 00:24:16,380
understand salary is exactly the tool

00:24:12,570 --> 00:24:17,010
you can use to do your power to run your

00:24:16,380 --> 00:24:19,500
parallel

00:24:17,010 --> 00:24:24,150
workers either on a single machine or on

00:24:19,500 --> 00:24:25,710
a lot of machines so what I was saying

00:24:24,150 --> 00:24:27,540
is that there's no built into the

00:24:25,710 --> 00:24:30,570
standard library there's no way of doing

00:24:27,540 --> 00:24:33,240
the cross machine communication over

00:24:30,570 --> 00:24:34,470
HTTP or some other protocol there are

00:24:33,240 --> 00:24:36,240
some great libraries but they're not

00:24:34,470 --> 00:24:38,130
built into the standard library I think

00:24:36,240 --> 00:24:39,810
that's actually one of the most the

00:24:38,130 --> 00:24:42,000
reasons it's been so successful is that

00:24:39,810 --> 00:24:43,440
external libraries have to compete

00:24:42,000 --> 00:24:45,210
something really easy to use and on

00:24:43,440 --> 00:24:48,030
iterating quickly and taking advice

00:24:45,210 --> 00:24:49,980
whereas the standard library has to be

00:24:48,030 --> 00:24:52,080
slow-moving and has to be sure and has

00:24:49,980 --> 00:24:53,670
to not not respond to advice half as

00:24:52,080 --> 00:24:55,440
quickly so actually I think it's in some

00:24:53,670 --> 00:24:57,540
ways a good thing maybe multi-processing

00:24:55,440 --> 00:24:59,490
will be way easier if there was the

00:24:57,540 --> 00:25:01,260
equivalent requests one library everyone

00:24:59,490 --> 00:25:04,610
used that was designed to be super easy

00:25:01,260 --> 00:25:04,610
okay thanks

00:25:04,640 --> 00:25:10,170
hi so I have two questions regarding our

00:25:07,800 --> 00:25:12,240
a queue I'm already using our queue and

00:25:10,170 --> 00:25:14,160
I was going to ask first doesn't make

00:25:12,240 --> 00:25:16,320
sense for me to switch to our queue at

00:25:14,160 --> 00:25:20,520
drop-in replacement if I don't use any I

00:25:16,320 --> 00:25:22,710
think I oh that's first question I'm

00:25:20,520 --> 00:25:26,280
using our queue now can I just switch to

00:25:22,710 --> 00:25:28,110
a our queue for my completely

00:25:26,280 --> 00:25:40,980
synchronous code doesn't make any sense

00:25:28,110 --> 00:25:43,230
to switch to our queue for this you can

00:25:40,980 --> 00:25:44,580
do you might want to go and use thread

00:25:43,230 --> 00:25:46,710
pool executor or the process call

00:25:44,580 --> 00:25:49,200
executive from within a particular job

00:25:46,710 --> 00:25:51,330
to do that job in parallel but in

00:25:49,200 --> 00:25:54,150
general a our queue actually same as our

00:25:51,330 --> 00:25:56,940
queue is only running from process it's

00:25:54,150 --> 00:25:58,380
only running running one job well after

00:25:56,940 --> 00:25:59,820
you is running one job at a time per

00:25:58,380 --> 00:26:01,830
process and it thinks you run another

00:25:59,820 --> 00:26:03,780
Heroku worker or whatever it might be or

00:26:01,830 --> 00:26:05,880
or another another job and another

00:26:03,780 --> 00:26:08,179
terminal to run multiple workers in

00:26:05,880 --> 00:26:10,340
parallel a our queue will run

00:26:08,179 --> 00:26:12,320
up to 60 jobs at the same time using

00:26:10,340 --> 00:26:14,809
racing hair but obviously if your task

00:26:12,320 --> 00:26:16,309
is not networking or suitable facing

00:26:14,809 --> 00:26:17,779
hair that anyone is actually going to be

00:26:16,309 --> 00:26:19,940
running at any one time so actually

00:26:17,779 --> 00:26:22,700
those my second questions so if I still

00:26:19,940 --> 00:26:25,009
have just synchronous code it will still

00:26:22,700 --> 00:26:33,289
be running one at a time unless I fork

00:26:25,009 --> 00:26:47,990
multiple workers anyway right okay thank

00:26:33,289 --> 00:26:52,490
you very much never know any further

00:26:47,990 --> 00:26:56,690
question folks yeah oh yeah go for it I

00:26:52,490 --> 00:26:59,059
can give you the microphone is there any

00:26:56,690 --> 00:27:00,830
advantage to having a flattened list of

00:26:59,059 --> 00:27:03,320
co-routines that you want to run as

00:27:00,830 --> 00:27:06,139
opposed to calling a couple of different

00:27:03,320 --> 00:27:07,879
co-routines which themselves or does it

00:27:06,139 --> 00:27:31,340
matter as long as it's running on the

00:27:07,879 --> 00:27:35,119
same any further question otherwise I

00:27:31,340 --> 00:27:37,940
have questions I can ask you can you

00:27:35,119 --> 00:27:40,519
tell us a little bit about some sample

00:27:37,940 --> 00:27:43,789
use case real-world use case for say a

00:27:40,519 --> 00:27:46,100
RQ like you use it in your job if you

00:27:43,789 --> 00:27:53,659
can talk about that or how you have seen

00:27:46,100 --> 00:27:55,429
people use it we said I guess about a

00:27:53,659 --> 00:27:58,059
million emails a month not a great deal

00:27:55,429 --> 00:28:00,830
but a point it gets to quite high load

00:27:58,059 --> 00:28:02,390
we are currently tethered to mandrel

00:28:00,830 --> 00:28:04,490
although they

00:28:02,390 --> 00:28:06,620
I hate them because our 300 customers

00:28:04,490 --> 00:28:08,030
have all set up their DNS records to

00:28:06,620 --> 00:28:12,770
send the emails from mantels so moving

00:28:08,030 --> 00:28:15,110
over is gonna be hell about 5% of emails

00:28:12,770 --> 00:28:18,770
of transcend fremantle you get back 500

00:28:15,110 --> 00:28:21,800
- or 503 or just broken HTTP requests

00:28:18,770 --> 00:28:23,690
and so we're using help you both to go

00:28:21,800 --> 00:28:25,940
and send us emails quite fast but also

00:28:23,690 --> 00:28:28,310
to back off and retry those jobs when

00:28:25,940 --> 00:28:30,770
they when they inevitably fail quite a

00:28:28,310 --> 00:28:32,960
lot and so that's a our queue for

00:28:30,770 --> 00:28:35,030
instance have the facility to riku the

00:28:32,960 --> 00:28:38,680
failed jobs for you and let me try

00:28:35,030 --> 00:28:38,680
Lannisters we've got time let me try

00:28:50,120 --> 00:28:53,559
oh no I didn't know what to do

00:28:55,090 --> 00:29:00,309
so here's an example of a of you which

00:28:58,809 --> 00:29:03,429
is not especially different from what we

00:29:00,309 --> 00:29:05,889
what we were looking at earlier we have

00:29:03,429 --> 00:29:06,999
some tolling ready for setting up the

00:29:05,889 --> 00:29:09,629
things we're going to need when we're

00:29:06,999 --> 00:29:12,159
running jobs so you have a bit like in

00:29:09,629 --> 00:29:13,960
something like a i/o HTTP where you have

00:29:12,159 --> 00:29:16,960
startup pair routines for setting up say

00:29:13,960 --> 00:29:18,429
your database connection you can do the

00:29:16,960 --> 00:29:20,110
same thing here so you have startup and

00:29:18,429 --> 00:29:22,029
shutdown where we can add to this

00:29:20,110 --> 00:29:28,330
context which is the first argument to

00:29:22,029 --> 00:29:31,779
any to any any job we set up and any

00:29:28,330 --> 00:29:36,419
function we set up and I'm trying to

00:29:31,779 --> 00:29:36,419
remember here if I have an example of

00:29:36,899 --> 00:29:42,580
retry jobs so dessert basically there's

00:29:41,110 --> 00:29:45,190
an exception that you can raise which

00:29:42,580 --> 00:29:46,779
will retry the job and that is what is

00:29:45,190 --> 00:29:48,759
raised if you shut down the worker and

00:29:46,779 --> 00:29:52,299
the jobs haven't had time to finish so

00:29:48,759 --> 00:29:53,710
any jobs that get shut down that get

00:29:52,299 --> 00:29:56,139
stopped because the worker shuts down

00:29:53,710 --> 00:29:58,509
will automatically be renewed next time

00:29:56,139 --> 00:30:01,330
because the job is not removed from the

00:29:58,509 --> 00:30:03,669
sorted set until it's been finished so

00:30:01,330 --> 00:30:05,860
the problem with our queue is I saw I

00:30:03,669 --> 00:30:07,690
rewrote the Roku worker which basically

00:30:05,860 --> 00:30:09,879
deals with shutdown behavior in in

00:30:07,690 --> 00:30:12,369
Heroku because Heroku workers shutdown

00:30:09,879 --> 00:30:13,809
variably that was killing us generating

00:30:12,369 --> 00:30:16,899
invoices for example which is one of our

00:30:13,809 --> 00:30:19,240
slower jobs and so when I built or

00:30:16,899 --> 00:30:21,039
rebuilt a RQ I build this principle that

00:30:19,240 --> 00:30:23,590
your job might run twice but it will

00:30:21,039 --> 00:30:25,149
always run at least once so it's your

00:30:23,590 --> 00:30:27,549
job to take care of the fact it might

00:30:25,149 --> 00:30:31,330
happen more times but if it's just down

00:30:27,549 --> 00:30:33,330
it'll get pre acute okay hey in case

00:30:31,330 --> 00:30:42,159
your Java runs multiple times I guess

00:30:33,330 --> 00:30:43,629
you get only one result okay do

00:30:42,159 --> 00:30:45,249
something in Redis to say has this job

00:30:43,629 --> 00:30:46,340
already started that's that's your

00:30:45,249 --> 00:30:47,870
problem

00:30:46,340 --> 00:30:49,610
there's a kind of principle a boutique

00:30:47,870 --> 00:30:52,940
you can never have exactly once and so

00:30:49,610 --> 00:30:54,710
it prefers multiple times over anything

00:30:52,940 --> 00:30:56,990
taking example of sending your customer

00:30:54,710 --> 00:30:57,980
an email their invoice each month they

00:30:56,990 --> 00:30:59,480
would get a bit confused if they

00:30:57,980 --> 00:31:01,070
received it three times but that's still

00:30:59,480 --> 00:31:05,120
better than then not receiving it at all

00:31:01,070 --> 00:31:10,070
and that's normally the case any

00:31:05,120 --> 00:31:11,990
question anybody and okay so one last

00:31:10,070 --> 00:31:17,480
question and then I asked the next

00:31:11,990 --> 00:31:21,350
speaker to please come up slowly and set

00:31:17,480 --> 00:31:25,790
up there is no next speaker do you have

00:31:21,350 --> 00:31:28,190
any experience like releasing the Gil in

00:31:25,790 --> 00:31:30,980
like C extensions and stuff like that

00:31:28,190 --> 00:31:34,340
and you know writing those and if you

00:31:30,980 --> 00:31:38,950
can if so if you can speak about how you

00:31:34,340 --> 00:31:38,950
can do it if there are some tools easily

00:31:45,970 --> 00:31:51,110
cool to see the whole thing is now

00:31:48,590 --> 00:31:53,960
comparable which made it I think about

00:31:51,110 --> 00:31:55,580
50% faster lots of stuff but that is

00:31:53,960 --> 00:31:56,930
there's some tweaks you have to make to

00:31:55,580 --> 00:31:59,000
the Python but it's still valid it's

00:31:56,930 --> 00:32:01,010
normal license so in environments like

00:31:59,000 --> 00:32:02,510
Windows where we don't have those

00:32:01,010 --> 00:32:06,590
binaries available it still just works

00:32:02,510 --> 00:32:10,370
exactly the same so not directly no okay

00:32:06,590 --> 00:32:13,640
that's cool if there are no more

00:32:10,370 --> 00:32:15,350
questions we can thank the speaker thank

00:32:13,640 --> 00:32:18,240
you very much

00:32:15,350 --> 00:32:20,300
[Music]

00:32:18,240 --> 00:32:20,300

YouTube URL: https://www.youtube.com/watch?v=0RaotdCa_j0


