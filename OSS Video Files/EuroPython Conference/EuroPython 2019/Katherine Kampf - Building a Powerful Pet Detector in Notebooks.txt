Title: Katherine Kampf - Building a Powerful Pet Detector in Notebooks
Publication date: 2019-09-03
Playlist: EuroPython 2019
Description: 
	"Building a Powerful Pet Detector in Notebooks
[EuroPython 2019 - Talk - 2019-07-10 - MongoDB  [PyData track]
[Basel, CH]

By Katherine Kampf

Ever wondered what breed that dog or cat is? Let’s build a pet detector service to recognize them in pictures! In this talk, we will walk through the training, optimizing, and deploying of a deep learning model using Azure Notebooks. We will use transfer learning to retrain a MobileNet model using TensorFlow to recognize dog and cat breeds using the Oxford IIIT Pet Dataset. Next, we’ll optimize the model and tune our hyperparameters to improve the model accuracy. Finally, we will deploy the model as a web service in. Come to learn how you can quickly create accurate image recognition models with a few simple techniques!



License: This video is licensed under the CC BY-NC-SA 3.0 license: https://creativecommons.org/licenses/by-nc-sa/3.0/
Please see our speaker release agreement for details: https://ep2019.europython.eu/events/speaker-release-agreement/
Captions: 
	00:00:01,810 --> 00:00:05,950
so today as you mentioned we're going to

00:00:04,420 --> 00:00:08,920
be talking about how to build a pet

00:00:05,950 --> 00:00:11,169
detector in notebooks environment so

00:00:08,920 --> 00:00:12,820
just to give you a bit of a context

00:00:11,169 --> 00:00:14,919
we're gonna start off by doing a

00:00:12,820 --> 00:00:16,780
high-level overview of the deep learning

00:00:14,919 --> 00:00:18,730
mechanisms we'll be using and the

00:00:16,780 --> 00:00:20,620
machine learning workflow and then we'll

00:00:18,730 --> 00:00:23,710
dive deep into the spend most of the

00:00:20,620 --> 00:00:26,770
time in the demo notebook and talk about

00:00:23,710 --> 00:00:28,870
the actual code so just to give you a

00:00:26,770 --> 00:00:31,240
bit of context I'm Katherine Kampf I

00:00:28,870 --> 00:00:33,010
work for Microsoft I'm a program manager

00:00:31,240 --> 00:00:36,910
on a products called add your notebooks

00:00:33,010 --> 00:00:39,219
which is our free Azure hosted Jupiter

00:00:36,910 --> 00:00:41,859
notebook service so we'll be seeing it

00:00:39,219 --> 00:00:43,839
shortly and here's some of my contact

00:00:41,859 --> 00:00:46,300
information I'll be showing it later so

00:00:43,839 --> 00:00:48,819
don't worry too much about taking photos

00:00:46,300 --> 00:00:50,920
or anything just yet the most important

00:00:48,819 --> 00:00:54,879
thing to know about me is I really love

00:00:50,920 --> 00:00:56,890
dogs and even me knowing a bunch of dog

00:00:54,879 --> 00:00:59,289
breeds it's still sometimes difficult to

00:00:56,890 --> 00:00:59,859
tell them apart if we look at Alaskan

00:00:59,289 --> 00:01:01,870
Malamutes

00:00:59,859 --> 00:01:04,960
Union Huskies they can look super

00:01:01,870 --> 00:01:06,759
similar and especially when you're then

00:01:04,960 --> 00:01:09,159
trying to train a machine to understand

00:01:06,759 --> 00:01:11,740
this it can get difficult and you need a

00:01:09,159 --> 00:01:13,810
ton of data to get an algorithm that can

00:01:11,740 --> 00:01:16,479
successively distinguish between these

00:01:13,810 --> 00:01:18,009
different breeds so the way we're going

00:01:16,479 --> 00:01:20,979
to approach this today is using a

00:01:18,009 --> 00:01:22,479
technique called deep learning so this

00:01:20,979 --> 00:01:24,520
is often how deep learning is viewed

00:01:22,479 --> 00:01:26,710
where you have an input so that's the

00:01:24,520 --> 00:01:28,390
dog photograph and this sort of black

00:01:26,710 --> 00:01:30,100
box where we don't really know exactly

00:01:28,390 --> 00:01:33,340
what's happening but then we get these

00:01:30,100 --> 00:01:35,049
outputs of either a dog or cat or other

00:01:33,340 --> 00:01:37,930
or in this summer we're actually gonna

00:01:35,049 --> 00:01:40,170
go a bit farther to say if it's a dog or

00:01:37,930 --> 00:01:43,299
a cat which breed do we think it is and

00:01:40,170 --> 00:01:45,909
so this differs a bit from traditional

00:01:43,299 --> 00:01:48,700
machine learning where you'd often be

00:01:45,909 --> 00:01:51,369
doing manual feature extractions so this

00:01:48,700 --> 00:01:53,439
requires a bit more hands-on work to say

00:01:51,369 --> 00:01:55,719
you know try to discern which features

00:01:53,439 --> 00:01:58,780
of the data are most important whether

00:01:55,719 --> 00:02:01,570
it's size or color etc and it also

00:01:58,780 --> 00:02:03,369
requires domain expertise so this is

00:02:01,570 --> 00:02:06,579
easier to think about in the

00:02:03,369 --> 00:02:08,830
classification of topic space but if you

00:02:06,579 --> 00:02:11,769
try to apply the same thinking to a

00:02:08,830 --> 00:02:13,569
really specialized field it can get a

00:02:11,769 --> 00:02:15,310
bit more difficult to do this manual

00:02:13,569 --> 00:02:16,959
feature extraction

00:02:15,310 --> 00:02:19,030
and then you'll also need to run through

00:02:16,959 --> 00:02:21,160
different classification algorithms to

00:02:19,030 --> 00:02:23,010
try to distinguish which will work best

00:02:21,160 --> 00:02:25,569
and get you the highest accuracy and

00:02:23,010 --> 00:02:27,640
this differs again from deep learning

00:02:25,569 --> 00:02:30,580
and where we're gonna be doing a layered

00:02:27,640 --> 00:02:33,550
approach trying to understand different

00:02:30,580 --> 00:02:34,930
features pixel by pixel of our images

00:02:33,550 --> 00:02:37,030
and it's going to be the machine doing

00:02:34,930 --> 00:02:39,910
the heavy lifting of that work rather

00:02:37,030 --> 00:02:43,840
than manual data science work tuning the

00:02:39,910 --> 00:02:47,380
features so like I said deep learning

00:02:43,840 --> 00:02:49,030
will require a ton of data but once you

00:02:47,380 --> 00:02:51,610
have that data it becomes a lot easier

00:02:49,030 --> 00:02:54,120
for the machine to try to understand the

00:02:51,610 --> 00:02:56,739
data set and generate predictions

00:02:54,120 --> 00:02:58,120
specifically within deep learning today

00:02:56,739 --> 00:03:00,400
we're going to be using what's called a

00:02:58,120 --> 00:03:02,110
convolutional neural network so this is

00:03:00,400 --> 00:03:03,790
a really popular network to use

00:03:02,110 --> 00:03:07,420
specifically for image classification

00:03:03,790 --> 00:03:10,090
and that's because this works by

00:03:07,420 --> 00:03:13,140
preserving the RGB channels in the first

00:03:10,090 --> 00:03:16,890
layer so this will take each pixel value

00:03:13,140 --> 00:03:19,420
what value they are red green blue and

00:03:16,890 --> 00:03:22,239
preserve that in the first layer and

00:03:19,420 --> 00:03:24,150
then we'll use a technique and this is

00:03:22,239 --> 00:03:28,269
at a very high level but we'll use a

00:03:24,150 --> 00:03:30,850
technique to filter on the images

00:03:28,269 --> 00:03:33,430
themselves and try to extract which

00:03:30,850 --> 00:03:35,500
information is most important in the

00:03:33,430 --> 00:03:37,030
convolution layer and then next we'll

00:03:35,500 --> 00:03:39,850
move into pooling where we'll try to

00:03:37,030 --> 00:03:42,040
aggregate that data and reduce the

00:03:39,850 --> 00:03:43,600
amount of information because of course

00:03:42,040 --> 00:03:45,880
deep learning is super computationally

00:03:43,600 --> 00:03:47,920
expensive so if you can have a bit less

00:03:45,880 --> 00:03:50,290
of information going into the fully

00:03:47,920 --> 00:03:54,340
connected layer it can save you a lot in

00:03:50,290 --> 00:03:56,380
time and compute power and then in the

00:03:54,340 --> 00:03:59,260
end we'll get our prediction for whether

00:03:56,380 --> 00:04:01,359
or not the animals a dog cat or

00:03:59,260 --> 00:04:04,150
something else if the networks a bit

00:04:01,359 --> 00:04:05,769
confused so if we take a step back and

00:04:04,150 --> 00:04:06,940
look at the general machine learning

00:04:05,769 --> 00:04:09,190
workflow that we're gonna be walking

00:04:06,940 --> 00:04:11,260
through today you're often going to want

00:04:09,190 --> 00:04:13,359
to start off with data exploration and

00:04:11,260 --> 00:04:15,040
the data itself in a machine and webbing

00:04:13,359 --> 00:04:17,049
workflow this is going to be where all

00:04:15,040 --> 00:04:19,209
of the power for your deep learning that

00:04:17,049 --> 00:04:21,100
where it comes from and it's going to be

00:04:19,209 --> 00:04:25,439
the most important part of the process

00:04:21,100 --> 00:04:28,000
so this will involve finding a data set

00:04:25,439 --> 00:04:30,430
possibly transforming the data set into

00:04:28,000 --> 00:04:33,009
a particular format you need cleaning

00:04:30,430 --> 00:04:34,599
any data and running some visualizations

00:04:33,009 --> 00:04:37,270
trying to understand the basic

00:04:34,599 --> 00:04:38,860
attributes of the data and once you have

00:04:37,270 --> 00:04:40,990
a sense of that you can move in to

00:04:38,860 --> 00:04:43,270
training and with training this is where

00:04:40,990 --> 00:04:45,729
we'll actually be developing our

00:04:43,270 --> 00:04:47,770
algorithm so there's three main concepts

00:04:45,729 --> 00:04:51,909
that we like to think about with the

00:04:47,770 --> 00:04:54,460
training script compute and tuning so to

00:04:51,909 --> 00:04:55,300
start off with our training script this

00:04:54,460 --> 00:04:58,599
is where we'll try out different

00:04:55,300 --> 00:05:00,069
algorithmic approaches and of course you

00:04:58,599 --> 00:05:02,620
know your local box can be pretty

00:05:00,069 --> 00:05:04,240
powerful but compute wise depending on

00:05:02,620 --> 00:05:07,960
the size of your data you might need to

00:05:04,240 --> 00:05:10,629
scale out to a larger VM in the cloud or

00:05:07,960 --> 00:05:12,729
on Prem or a cluster computing

00:05:10,629 --> 00:05:15,039
environment and once you have that

00:05:12,729 --> 00:05:18,370
algorithm you might be fully satisfied

00:05:15,039 --> 00:05:20,259
with the accuracy you're seeing or you

00:05:18,370 --> 00:05:22,270
might want to do some tuning to try to

00:05:20,259 --> 00:05:24,879
refine that algorithm to see a bit

00:05:22,270 --> 00:05:26,289
higher accuracies and once you're happy

00:05:24,879 --> 00:05:28,210
with your model and then you'll move

00:05:26,289 --> 00:05:29,800
into the inference scene stage and this

00:05:28,210 --> 00:05:32,680
is where you're actually using your

00:05:29,800 --> 00:05:34,599
model in an application so this involves

00:05:32,680 --> 00:05:37,479
three different components where you'll

00:05:34,599 --> 00:05:40,150
have product ization so this can often

00:05:37,479 --> 00:05:41,830
mean refactoring your code so you might

00:05:40,150 --> 00:05:44,349
have started in a Jupiter notebook

00:05:41,830 --> 00:05:46,690
environment but you need to output a

00:05:44,349 --> 00:05:48,400
Python module so you'll have to do some

00:05:46,690 --> 00:05:50,400
work there to refactor it and we'll talk

00:05:48,400 --> 00:05:52,870
a bit about that later and then

00:05:50,400 --> 00:05:55,539
deploying your model to a web service so

00:05:52,870 --> 00:05:57,849
you can use in your applications or

00:05:55,539 --> 00:06:00,639
other folks across your company or

00:05:57,849 --> 00:06:02,080
organization can as well and once your

00:06:00,639 --> 00:06:04,750
model is deployed then you can write a

00:06:02,080 --> 00:06:06,729
test application to send a photo of a

00:06:04,750 --> 00:06:08,770
dog and get returned back with your

00:06:06,729 --> 00:06:10,210
breed prediction so that's the

00:06:08,770 --> 00:06:12,099
high-level overview of what we're gonna

00:06:10,210 --> 00:06:14,229
be walking through today we'll spend

00:06:12,099 --> 00:06:16,960
most of our time in these first two

00:06:14,229 --> 00:06:18,729
stages just for the sake of time but

00:06:16,960 --> 00:06:20,440
I'll point you to again a repository at

00:06:18,729 --> 00:06:23,800
the end of this that walks through the

00:06:20,440 --> 00:06:25,740
entire lifecycle so we're gonna start

00:06:23,800 --> 00:06:28,599
off talking about the data we're using

00:06:25,740 --> 00:06:31,150
so today we're using the oxford pet data

00:06:28,599 --> 00:06:34,089
set so this is a pretty common data set

00:06:31,150 --> 00:06:35,860
which contains 37 categories of

00:06:34,089 --> 00:06:37,270
different pet breeds so this is cats and

00:06:35,860 --> 00:06:41,319
dogs and around 2

00:06:37,270 --> 00:06:43,030
images per each breed and here's a link

00:06:41,319 --> 00:06:44,410
to it I definitely encourage you to if

00:06:43,030 --> 00:06:45,940
specially if you're getting started with

00:06:44,410 --> 00:06:48,400
machine learning or image recognition

00:06:45,940 --> 00:06:51,389
this is a really great and well labeled

00:06:48,400 --> 00:06:54,039
and well documented data set to use and

00:06:51,389 --> 00:06:57,669
then once we get into wanting to explore

00:06:54,039 --> 00:06:59,080
and understand our data there's a bunch

00:06:57,669 --> 00:07:01,389
of different great tools you can use

00:06:59,080 --> 00:07:03,190
depending on what types of data you're

00:07:01,389 --> 00:07:05,440
working with or the scale of your data

00:07:03,190 --> 00:07:07,990
and today we're gonna be using notebooks

00:07:05,440 --> 00:07:09,639
so if you're not familiar with Jupiter

00:07:07,990 --> 00:07:12,120
notebooks they essentially let you

00:07:09,639 --> 00:07:15,460
combine markdown texts images

00:07:12,120 --> 00:07:17,710
visualizations etc alongside executable

00:07:15,460 --> 00:07:19,479
code so it's super useful in data

00:07:17,710 --> 00:07:22,150
exploration and data science in general

00:07:19,479 --> 00:07:24,280
to tell a story around how you got to a

00:07:22,150 --> 00:07:26,470
specific graph or how you got to a

00:07:24,280 --> 00:07:29,139
specific model so you have the context

00:07:26,470 --> 00:07:31,240
either to present to others or look back

00:07:29,139 --> 00:07:33,729
on your own work and understand each

00:07:31,240 --> 00:07:36,220
step you went through and get strong

00:07:33,729 --> 00:07:37,630
visualizations of your data specifically

00:07:36,220 --> 00:07:39,280
today we're gonna be using what I

00:07:37,630 --> 00:07:42,550
mentioned earlier as your notebooks

00:07:39,280 --> 00:07:45,669
mainly for the free setup and scale out

00:07:42,550 --> 00:07:48,039
so my local box is fairly powerful but

00:07:45,669 --> 00:07:51,070
for something like this I'd rather use a

00:07:48,039 --> 00:07:53,680
really big beefy GPU machine and Azure

00:07:51,070 --> 00:07:56,770
so as your notebooks lets you connect

00:07:53,680 --> 00:07:58,840
from their free compute to a remote VM

00:07:56,770 --> 00:08:00,550
and Azure so super useful for scale out

00:07:58,840 --> 00:08:02,620
scenarios and so I can make it public

00:08:00,550 --> 00:08:05,409
for all of you to go and play with on

00:08:02,620 --> 00:08:07,330
your own as well so I'm going to switch

00:08:05,409 --> 00:08:11,889
over to the demo to show you exactly

00:08:07,330 --> 00:08:13,870
what this looks like so this is the

00:08:11,889 --> 00:08:17,610
github repository I'm going to link

00:08:13,870 --> 00:08:17,610
oh this isn't showing up

00:08:23,680 --> 00:08:28,150
all right let's see if we can fix this

00:08:37,910 --> 00:08:44,360
Oh No ah okay we're good all right so

00:08:42,500 --> 00:08:46,970
this is the github repository I'll link

00:08:44,360 --> 00:08:48,500
you to just to give you an overview if

00:08:46,970 --> 00:08:50,540
you scroll down here you'll see these

00:08:48,500 --> 00:08:52,490
little lunch badge if you click this

00:08:50,540 --> 00:08:54,620
it'll automatically clone it into your

00:08:52,490 --> 00:08:57,140
hazard notebooks account as a new

00:08:54,620 --> 00:08:58,940
project so super easy to get started and

00:08:57,140 --> 00:09:01,610
once it's cloned in which I've already

00:08:58,940 --> 00:09:02,840
done you'll get an overview of all the

00:09:01,610 --> 00:09:04,940
project files here

00:09:02,840 --> 00:09:07,220
and this is the compute picker I

00:09:04,940 --> 00:09:10,760
mentioned so there's free free compute

00:09:07,220 --> 00:09:14,960
offered as well as this is my camp VM

00:09:10,760 --> 00:09:18,170
that I'm connected to so it's a nc6 GPU

00:09:14,960 --> 00:09:21,950
machine and azure if we move into our

00:09:18,170 --> 00:09:26,450
notebook I'll just start off and run all

00:09:21,950 --> 00:09:29,150
of these so let's first try to

00:09:26,450 --> 00:09:30,980
understand what the data structure we're

00:09:29,150 --> 00:09:33,230
working with is so we know that we're

00:09:30,980 --> 00:09:35,570
using the Oxford pets data set but I'm

00:09:33,230 --> 00:09:37,340
just gonna do a quick LS to see the

00:09:35,570 --> 00:09:39,860
folder structure we're working with so

00:09:37,340 --> 00:09:42,500
it looks like we have 37 folders and

00:09:39,860 --> 00:09:44,150
presumably those contain the 200 images

00:09:42,500 --> 00:09:47,390
that we're going to be analyzing today

00:09:44,150 --> 00:09:49,040
and just to get a quick visual sense of

00:09:47,390 --> 00:09:51,500
these I created a little plotting

00:09:49,040 --> 00:09:53,000
function so we can see exactly the

00:09:51,500 --> 00:09:55,190
breeds we're gonna be working with and

00:09:53,000 --> 00:09:58,520
holler in the back if it's fonts not big

00:09:55,190 --> 00:10:00,500
enough I tried to make it big and I

00:09:58,520 --> 00:10:02,210
think it's very useful to always get

00:10:00,500 --> 00:10:04,580
this visual sense of your data for

00:10:02,210 --> 00:10:07,400
instance here we can see all these 37

00:10:04,580 --> 00:10:09,470
breeds and it'd be easy to once we have

00:10:07,400 --> 00:10:11,240
our model deployed submit a breed that's

00:10:09,470 --> 00:10:13,040
not covered by this data set and then we

00:10:11,240 --> 00:10:14,630
confused if we submitted a Golden

00:10:13,040 --> 00:10:18,610
Retriever which isn't reflected in this

00:10:14,630 --> 00:10:20,960
data and saw relatively low accuracy

00:10:18,610 --> 00:10:24,410
it's because it's not reflected in this

00:10:20,960 --> 00:10:27,530
original data set so as I mentioned

00:10:24,410 --> 00:10:30,380
we're working with 200 pet or 200 images

00:10:27,530 --> 00:10:31,910
per breed which may seem like a lot but

00:10:30,380 --> 00:10:34,580
for something like deep learning it's

00:10:31,910 --> 00:10:37,610
really not enough that's relatively

00:10:34,580 --> 00:10:40,190
pretty small data and we likely end up

00:10:37,610 --> 00:10:42,410
with an over fitted model that wouldn't

00:10:40,190 --> 00:10:44,870
scale out to the data we'd see in the

00:10:42,410 --> 00:10:46,310
wild so instead we're gonna be using

00:10:44,870 --> 00:10:47,390
instead of just training on that we're

00:10:46,310 --> 00:10:50,150
gonna be using a technique called

00:10:47,390 --> 00:10:51,410
transfer learning so with transfer

00:10:50,150 --> 00:10:52,970
learning we're gonna take

00:10:51,410 --> 00:10:54,439
a pre-trained model so this is the

00:10:52,970 --> 00:10:56,629
mobile net model that we're using today

00:10:54,439 --> 00:10:58,819
which has been trained on thousands of

00:10:56,629 --> 00:11:01,879
general images and then we'll do is

00:10:58,819 --> 00:11:05,209
retrain that last layer specifically to

00:11:01,879 --> 00:11:07,039
our 37 pet breeds so it'll use all the

00:11:05,209 --> 00:11:10,039
power of someone who trained this

00:11:07,039 --> 00:11:12,859
massive massive network but specifying

00:11:10,039 --> 00:11:15,889
it down to our data set so when I run

00:11:12,859 --> 00:11:22,489
this training job we can see here takes

00:11:15,889 --> 00:11:25,669
a bit and has a ton of output so we can

00:11:22,489 --> 00:11:29,749
see it took around 26 seconds and we saw

00:11:25,669 --> 00:11:31,999
an accuracy of almost eighty percent so

00:11:29,749 --> 00:11:34,849
this is from doing that initial transfer

00:11:31,999 --> 00:11:36,199
learning not tuning any of our hyper

00:11:34,849 --> 00:11:38,809
parameters which we'll get into later

00:11:36,199 --> 00:11:40,929
just using a flat learning rate we are

00:11:38,809 --> 00:11:43,609
still able to get to eighty percent in

00:11:40,929 --> 00:11:45,199
26 seconds which is pretty impressive if

00:11:43,609 --> 00:11:48,499
you look back this data set was first

00:11:45,199 --> 00:11:51,589
released in 2012 so seven years ago and

00:11:48,499 --> 00:11:53,929
even with you know a lot more compute

00:11:51,589 --> 00:11:55,819
power a lot more time data scientists

00:11:53,929 --> 00:11:57,409
were still only able to get to around

00:11:55,819 --> 00:11:59,389
fifty nine percent accuracy

00:11:57,409 --> 00:12:01,189
so it's pretty impressive how far we've

00:11:59,389 --> 00:12:05,329
been able to come in just a short amount

00:12:01,189 --> 00:12:08,059
of time so 79 80 percent is pretty great

00:12:05,329 --> 00:12:09,470
but I want to see if we can improve this

00:12:08,059 --> 00:12:10,970
all so now we're going to be working

00:12:09,470 --> 00:12:13,339
with what are called hyper parameters

00:12:10,970 --> 00:12:15,229
and these are attributes of your network

00:12:13,339 --> 00:12:17,539
you can determine beforehand so

00:12:15,229 --> 00:12:19,939
specifically we'll be looking at what's

00:12:17,539 --> 00:12:22,039
called learning rate and the learning

00:12:19,939 --> 00:12:25,339
rate is essentially how much you'll let

00:12:22,039 --> 00:12:27,470
the weight vary on a node from iteration

00:12:25,339 --> 00:12:30,229
to iteration so how quickly you're

00:12:27,470 --> 00:12:33,439
letting the network learn and oftentimes

00:12:30,229 --> 00:12:35,389
in data science you find yourself trying

00:12:33,439 --> 00:12:37,519
out a bunch of these values and just for

00:12:35,389 --> 00:12:39,409
looping through randomly cuz it's often

00:12:37,519 --> 00:12:42,949
difficult to determine which value might

00:12:39,409 --> 00:12:45,379
be the best for your specific network so

00:12:42,949 --> 00:12:47,659
instead of doing that by hand and taking

00:12:45,379 --> 00:12:49,249
hours to do it we're gonna use something

00:12:47,659 --> 00:12:51,319
called Azure machine learning service

00:12:49,249 --> 00:12:54,499
which lets you distribute this work

00:12:51,319 --> 00:12:56,749
across a cluster so I have a four node

00:12:54,499 --> 00:12:59,119
cluster in my Astra subscription and

00:12:56,749 --> 00:13:01,869
basically I'm going to send the training

00:12:59,119 --> 00:13:04,620
script to each of the worker nodes and

00:13:01,869 --> 00:13:05,880
it'll try out a bunch of so if we

00:13:04,620 --> 00:13:08,670
see here it'll try out a bunch of

00:13:05,880 --> 00:13:11,520
uniform random values for the learning

00:13:08,670 --> 00:13:14,340
rate and it'll tell me which gets the

00:13:11,520 --> 00:13:16,710
best accuracy so then I can treat that

00:13:14,340 --> 00:13:18,690
as my best model without having to do as

00:13:16,710 --> 00:13:20,520
much work and we're gonna do a couple of

00:13:18,690 --> 00:13:23,340
things to make this more efficient so

00:13:20,520 --> 00:13:25,280
I'll I need to update my calls but we're

00:13:23,340 --> 00:13:28,320
gonna use this early termination policy

00:13:25,280 --> 00:13:29,490
and basically this lets us if you see

00:13:28,320 --> 00:13:31,680
this point one five

00:13:29,490 --> 00:13:35,310
what's that saying is if we're seeing a

00:13:31,680 --> 00:13:37,140
run and the accuracy is less than or

00:13:35,310 --> 00:13:39,120
more than fifteen percent away from what

00:13:37,140 --> 00:13:41,340
our current best accuracy we've seen is

00:13:39,120 --> 00:13:43,530
then we'll just cut that run short and

00:13:41,340 --> 00:13:48,840
free up that compute resource to be used

00:13:43,530 --> 00:13:53,130
with a new value so as this runs we can

00:13:48,840 --> 00:13:54,930
see a bunch of output it's not loaded

00:13:53,130 --> 00:13:57,960
yet but sometimes it takes a bit

00:13:54,930 --> 00:14:01,470
depending on Wi-Fi but we can see all

00:13:57,960 --> 00:14:03,120
these jobs running so and a bit of

00:14:01,470 --> 00:14:06,240
information about our cluster so we have

00:14:03,120 --> 00:14:09,990
four nodes running and it'll run through

00:14:06,240 --> 00:14:13,260
each different job tell us how long it

00:14:09,990 --> 00:14:15,210
took what it's run ideas etc so I'll

00:14:13,260 --> 00:14:17,040
come back up in a bit so we can see the

00:14:15,210 --> 00:14:19,770
visualizations it'll start giving you

00:14:17,040 --> 00:14:21,960
but this even though it's a pretty

00:14:19,770 --> 00:14:24,510
efficient to distribute it across a

00:14:21,960 --> 00:14:27,180
cluster it'll still take around twenty

00:14:24,510 --> 00:14:28,620
five minutes so this is a thirty minute

00:14:27,180 --> 00:14:31,580
talk I don't really have time for that

00:14:28,620 --> 00:14:34,460
so oh yeah here you can see some of the

00:14:31,580 --> 00:14:40,230
validation accuracy starting to come in

00:14:34,460 --> 00:14:43,140
as well as different learning rates but

00:14:40,230 --> 00:14:44,790
I'll just skip ahead to a run we've

00:14:43,140 --> 00:14:47,460
already done in the past so this was a

00:14:44,790 --> 00:14:50,070
run I did yesterday and I feed it the

00:14:47,460 --> 00:14:52,170
specific run ID and now I can see a bit

00:14:50,070 --> 00:14:54,240
of the information see another graph of

00:14:52,170 --> 00:14:57,870
the validation accuracy as it went

00:14:54,240 --> 00:15:01,020
through training and see that the final

00:14:57,870 --> 00:15:03,210
accuracy was around 93% so with just an

00:15:01,020 --> 00:15:05,190
additional 25 minutes of training we

00:15:03,210 --> 00:15:08,490
were able to increase our accuracy by

00:15:05,190 --> 00:15:10,500
13% which is pretty exciting and 93% is

00:15:08,490 --> 00:15:12,060
a really great accuracy except

00:15:10,500 --> 00:15:14,850
especially for something like an image

00:15:12,060 --> 00:15:16,390
recognition task so now that I have that

00:15:14,850 --> 00:15:19,150
treated as my best

00:15:16,390 --> 00:15:22,540
run I'm gonna register it with the Azure

00:15:19,150 --> 00:15:24,640
machine learning service and this will

00:15:22,540 --> 00:15:27,160
basically let me use this model from

00:15:24,640 --> 00:15:29,140
anywhere or deploy it easily so if I

00:15:27,160 --> 00:15:32,230
wanted to access this mono from BS code

00:15:29,140 --> 00:15:34,630
etc or in a future notebook it's

00:15:32,230 --> 00:15:36,940
registered and available to me as well

00:15:34,630 --> 00:15:39,460
as anyone who's working inside my

00:15:36,940 --> 00:15:41,530
workspace so I know we just cover it a

00:15:39,460 --> 00:15:51,490
bit so I'm going to flip back to the

00:15:41,530 --> 00:15:53,500
slides to review some of the topics so

00:15:51,490 --> 00:15:55,180
again we just went through training and

00:15:53,500 --> 00:15:57,820
we are looking specifically at trying to

00:15:55,180 --> 00:15:59,140
do deep learning with small data and by

00:15:57,820 --> 00:16:01,540
nature deep learning is gonna require

00:15:59,140 --> 00:16:03,730
huge amounts of training data because

00:16:01,540 --> 00:16:05,950
it's doing that feature extraction and

00:16:03,730 --> 00:16:08,710
trying to figure out the best network

00:16:05,950 --> 00:16:10,390
structure all on its own and so you need

00:16:08,710 --> 00:16:13,840
as much data as possible to learn from

00:16:10,390 --> 00:16:15,640
for that so 200 images isn't going to be

00:16:13,840 --> 00:16:17,680
enough which is why we decided to use

00:16:15,640 --> 00:16:19,300
transfer learning and specifically

00:16:17,680 --> 00:16:23,800
transfer learning with the mobile net

00:16:19,300 --> 00:16:26,080
where we'll be taking the existing

00:16:23,800 --> 00:16:28,870
mobile net model and retraining the last

00:16:26,080 --> 00:16:31,900
layers specifically to our 37 pets and

00:16:28,870 --> 00:16:34,240
then since that was pretty good accuracy

00:16:31,900 --> 00:16:36,670
around 80% we decided to see if we could

00:16:34,240 --> 00:16:38,830
do any better by doing some hyper

00:16:36,670 --> 00:16:41,170
parameter tuning using Azure machine

00:16:38,830 --> 00:16:43,300
learning service so just to call out a

00:16:41,170 --> 00:16:44,470
couple more exciting things if you're

00:16:43,300 --> 00:16:46,960
just getting started with machine

00:16:44,470 --> 00:16:49,480
learning a MLS can be super useful it's

00:16:46,960 --> 00:16:51,490
got experiences where you can just do a

00:16:49,480 --> 00:16:52,930
drag and drop automated machine learning

00:16:51,490 --> 00:16:55,030
and it tries out a bunch of different

00:16:52,930 --> 00:16:57,370
classification algorithms for you or use

00:16:55,030 --> 00:17:00,490
it for hyper parameter tuning like we

00:16:57,370 --> 00:17:02,590
just saw an automated compute scale up

00:17:00,490 --> 00:17:04,990
scale down so I have a four node cluster

00:17:02,590 --> 00:17:07,120
in my subscription right now but I've

00:17:04,990 --> 00:17:09,010
set the min nodes to zero so whenever

00:17:07,120 --> 00:17:11,560
I'm not using it and not running my jobs

00:17:09,010 --> 00:17:14,650
it'll scale down and won't cost me money

00:17:11,560 --> 00:17:16,660
which is super great so once we have

00:17:14,650 --> 00:17:20,140
this model like I mentioned you might

00:17:16,660 --> 00:17:22,890
want to do some refactoring so I'm gonna

00:17:20,140 --> 00:17:25,410
move into vs code for that and this

00:17:22,890 --> 00:17:27,990
is the same demo notebook that we just

00:17:25,410 --> 00:17:30,480
had that ipy mb file but when I open it

00:17:27,990 --> 00:17:32,280
in BS code I see the JSON dump of

00:17:30,480 --> 00:17:35,400
whatever on notebook file looks like and

00:17:32,280 --> 00:17:39,780
I also see this option to import it so

00:17:35,400 --> 00:17:43,710
I'm gonna go ahead and click that and BS

00:17:39,780 --> 00:17:46,350
code will turn this into a dot py file

00:17:43,710 --> 00:17:47,640
with a bunch of cells so here I can see

00:17:46,350 --> 00:17:50,370
in the markdown has been turned into

00:17:47,640 --> 00:17:53,220
comments my Python code is still here

00:17:50,370 --> 00:17:55,680
and I can visually see that I have these

00:17:53,220 --> 00:17:57,990
little cells with this run cell option

00:17:55,680 --> 00:18:01,050
that'll bring up our Python interactive

00:17:57,990 --> 00:18:03,330
window and run a cell as you would see

00:18:01,050 --> 00:18:05,220
in Jupiter side-by-side and this

00:18:03,330 --> 00:18:08,370
essentially works as a Python console so

00:18:05,220 --> 00:18:10,230
you can type code in here etc so when

00:18:08,370 --> 00:18:12,540
you're into refactoring this can be

00:18:10,230 --> 00:18:14,370
super useful if we just highlight a

00:18:12,540 --> 00:18:16,230
snippet of code we'll have all the

00:18:14,370 --> 00:18:18,540
refactoring capabilities you're used to

00:18:16,230 --> 00:18:22,050
with an editor so you can see we can

00:18:18,540 --> 00:18:25,380
change all occurrences extract method

00:18:22,050 --> 00:18:27,510
etc all from what started as a Jupiter

00:18:25,380 --> 00:18:30,420
notebook so you can now refactor that

00:18:27,510 --> 00:18:32,520
into whatever form fits your workload

00:18:30,420 --> 00:18:35,580
best and I have an example in the github

00:18:32,520 --> 00:18:36,840
repository of what refactored Python

00:18:35,580 --> 00:18:39,360
module might look like

00:18:36,840 --> 00:18:41,160
and once we've refactored it to what

00:18:39,360 --> 00:18:44,150
we're happy with and deployed our model

00:18:41,160 --> 00:18:46,980
we'll want to go ahead and test it so

00:18:44,150 --> 00:18:50,550
this is a testing script I've written so

00:18:46,980 --> 00:18:52,410
we'll do the same action and run this

00:18:50,550 --> 00:18:53,520
cell to see what it looks like so

00:18:52,410 --> 00:18:54,150
basically what we're going to be doing

00:18:53,520 --> 00:18:56,100
here

00:18:54,150 --> 00:18:59,070
you can either we have some code to

00:18:56,100 --> 00:19:00,660
access a random pet to try it out or in

00:18:59,070 --> 00:19:02,280
this example I'm specifically trying

00:19:00,660 --> 00:19:04,650
with this little chihuahua I found on

00:19:02,280 --> 00:19:06,510
the internet and we can see here it

00:19:04,650 --> 00:19:09,540
loads the Chihuahua image as we would

00:19:06,510 --> 00:19:12,120
expect and something new that we just

00:19:09,540 --> 00:19:14,490
introduced is actually be able ability

00:19:12,120 --> 00:19:17,340
to debug sell buy sell so we just ran

00:19:14,490 --> 00:19:19,200
this first sell successfully and now I'm

00:19:17,340 --> 00:19:23,850
going to hit this debug sell and we can

00:19:19,200 --> 00:19:26,280
see I have this breakpoint here so once

00:19:23,850 --> 00:19:27,750
I hit debug sell it'll open the

00:19:26,280 --> 00:19:30,990
different tools I'm used to in my

00:19:27,750 --> 00:19:35,460
debugger I can step through etc or just

00:19:30,990 --> 00:19:36,090
continue on and once I continue on I can

00:19:35,460 --> 00:19:39,000
see

00:19:36,090 --> 00:19:42,540
that it gets too bright Chihuahua was a

00:19:39,000 --> 00:19:44,150
pretty accurate probability so this is

00:19:42,540 --> 00:19:47,250
super useful to be able to do this

00:19:44,150 --> 00:19:49,230
debugging bit by bit and we just

00:19:47,250 --> 00:19:50,760
introduced it this week so if you want

00:19:49,230 --> 00:19:54,240
to learn more about it come by our booth

00:19:50,760 --> 00:19:56,400
and we can talk more but this is gonna

00:19:54,240 --> 00:19:58,410
be super powerful when you're working in

00:19:56,400 --> 00:19:59,520
a data science space and trying to debug

00:19:58,410 --> 00:20:03,450
cell by cell

00:19:59,520 --> 00:20:04,830
and so now that we know our pet detector

00:20:03,450 --> 00:20:07,710
is working pretty well and we have a

00:20:04,830 --> 00:20:13,260
great probability we're good to go

00:20:07,710 --> 00:20:16,260
so just to rehash a bit about working

00:20:13,260 --> 00:20:18,660
with Python in Visual Studio code we

00:20:16,260 --> 00:20:21,020
just saw that we have debugging and

00:20:18,660 --> 00:20:25,320
refactoring capabilities there's also

00:20:21,020 --> 00:20:27,300
intellisense so auto completion as well

00:20:25,320 --> 00:20:29,250
as the ability to import-export Jupiter

00:20:27,300 --> 00:20:31,320
notebook so as you saw we were importing

00:20:29,250 --> 00:20:34,440
that Jupiter notebook it transformed it

00:20:31,320 --> 00:20:36,450
into a dot py file with different cells

00:20:34,440 --> 00:20:38,400
and you can continue to work and

00:20:36,450 --> 00:20:41,040
refactor that into a Python module or

00:20:38,400 --> 00:20:42,750
you can react sport it back as a jupiter

00:20:41,040 --> 00:20:46,710
notebook if say you want to present the

00:20:42,750 --> 00:20:48,740
information etc there's a variable

00:20:46,710 --> 00:20:52,140
explorer data viewer a bunch of

00:20:48,740 --> 00:20:53,730
full-fledged data science tools and if

00:20:52,140 --> 00:20:55,170
there's anything you don't see that you

00:20:53,730 --> 00:20:57,450
would love to see you please come talk

00:20:55,170 --> 00:20:59,970
to us we are heavily invested in making

00:20:57,450 --> 00:21:02,580
this a great experience so now that

00:20:59,970 --> 00:21:05,250
we've covered most of that workflow

00:21:02,580 --> 00:21:06,960
what's next so here's a link to the

00:21:05,250 --> 00:21:09,120
github repository where you can build

00:21:06,960 --> 00:21:11,430
your own pet detector as well as links

00:21:09,120 --> 00:21:13,830
to try out as your notebooks and the

00:21:11,430 --> 00:21:16,290
data science to lean in visual studio

00:21:13,830 --> 00:21:18,060
code and then I have some resources on

00:21:16,290 --> 00:21:20,280
the next slide as well but I'll let

00:21:18,060 --> 00:21:24,330
people take photos of this as they wish

00:21:20,280 --> 00:21:25,620
and I think we have a few minutes we'll

00:21:24,330 --> 00:21:27,900
have a few minutes left for questions

00:21:25,620 --> 00:21:30,860
and then I'll hang around outside in the

00:21:27,900 --> 00:21:30,860
hallway as well

00:21:35,280 --> 00:21:45,049
[Applause]

00:21:53,640 --> 00:21:59,830
or I'll just be outside the booth right

00:21:57,040 --> 00:22:00,670
thank you for the talk yeah I have a

00:21:59,830 --> 00:22:03,580
question

00:22:00,670 --> 00:22:06,790
usually in this kind of example so we

00:22:03,580 --> 00:22:10,560
talked about classification to the small

00:22:06,790 --> 00:22:17,530
amount of categories like brews of dogs

00:22:10,560 --> 00:22:22,150
what what I would do if what can I train

00:22:17,530 --> 00:22:25,900
something to classify let's say I know

00:22:22,150 --> 00:22:28,680
several thousand categories or maybe one

00:22:25,900 --> 00:22:31,150
hundred thousand okay I just want to

00:22:28,680 --> 00:22:37,690
create some classifier that tells me

00:22:31,150 --> 00:22:41,370
what what is on a picture yeah so how

00:22:37,690 --> 00:22:45,190
can I accomplish this task so shall we

00:22:41,370 --> 00:22:47,830
try a lot of classifiers for each

00:22:45,190 --> 00:22:51,480
separate category or maybe there are

00:22:47,830 --> 00:22:55,150
some approaches to do it just out of box

00:22:51,480 --> 00:22:57,250
yeah so it depends sort of on this how

00:22:55,150 --> 00:22:58,840
big the size of each category you have

00:22:57,250 --> 00:23:02,590
so if you have hundreds of thousands of

00:22:58,840 --> 00:23:04,540
categories and five images per each

00:23:02,590 --> 00:23:06,400
category then you'll have to try to use

00:23:04,540 --> 00:23:08,590
some pre-existing models and do transfer

00:23:06,400 --> 00:23:10,420
learning but if you're able to do if you

00:23:08,590 --> 00:23:11,620
have a hundred thousand categories and

00:23:10,420 --> 00:23:14,200
you have a hundred thousand images

00:23:11,620 --> 00:23:16,150
within those then you can employ a lot

00:23:14,200 --> 00:23:17,560
of different techniques whether you just

00:23:16,150 --> 00:23:21,100
want to do traditional people learning

00:23:17,560 --> 00:23:22,750
on your full data set or try out bit by

00:23:21,100 --> 00:23:24,700
bit depending on the topics you're

00:23:22,750 --> 00:23:26,320
working with in the image you could try

00:23:24,700 --> 00:23:29,290
different classification algorithms on

00:23:26,320 --> 00:23:33,790
each set as you mentioned okay and

00:23:29,290 --> 00:23:36,820
another question if there's some you

00:23:33,790 --> 00:23:40,770
know software's as a service in Azure

00:23:36,820 --> 00:23:44,950
that provides a classifier as service

00:23:40,770 --> 00:23:50,470
just for me not to write it by my own

00:23:44,950 --> 00:23:52,300
but just call some API yeah yeah so we

00:23:50,470 --> 00:23:54,580
have something that's called Azra

00:23:52,300 --> 00:23:58,440
cognitive services which basically our

00:23:54,580 --> 00:24:01,470
suite of exactly that API is for

00:23:58,440 --> 00:24:03,330
speech-to-text search image recognition

00:24:01,470 --> 00:24:05,870
etc so if

00:24:03,330 --> 00:24:10,670
search just as your cognitive services

00:24:05,870 --> 00:24:10,670
that's exactly what it does completely

00:24:12,560 --> 00:24:20,760
did in the demo you are using the image

00:24:17,970 --> 00:24:22,980
folder locally is that correctly

00:24:20,760 --> 00:24:25,860
understood in the kid hub repository you

00:24:22,980 --> 00:24:29,700
have images there and they are then

00:24:25,860 --> 00:24:31,080
uploaded to train your model if the

00:24:29,700 --> 00:24:35,040
images are not coming from somewhere

00:24:31,080 --> 00:24:37,350
else yes so these images I actually put

00:24:35,040 --> 00:24:40,170
into Azure storage are in the github

00:24:37,350 --> 00:24:42,060
repository as well if you want to so the

00:24:40,170 --> 00:24:43,500
note book refers to the actual storage

00:24:42,060 --> 00:24:48,330
so you need to upload it to the Azure

00:24:43,500 --> 00:24:50,640
storage and then yeah so I loaded them

00:24:48,330 --> 00:24:53,370
onto the specific VM I was working with

00:24:50,640 --> 00:24:54,960
but you could there's a limited amount

00:24:53,370 --> 00:24:56,730
of free storage in Azure notebooks as

00:24:54,960 --> 00:24:59,970
well so you could upload a subset of the

00:24:56,730 --> 00:25:02,940
data and work with it there or upload

00:24:59,970 --> 00:25:05,400
the full dataset to Asher and you could

00:25:02,940 --> 00:25:08,250
also use you also have a service for a

00:25:05,400 --> 00:25:10,500
service for uploading the images right

00:25:08,250 --> 00:25:12,240
on Asha or if I remember correctly yeah

00:25:10,500 --> 00:25:14,850
there's a couple services you can either

00:25:12,240 --> 00:25:16,740
do it from the azure portal or use the

00:25:14,850 --> 00:25:19,230
azure data explorer which will let you

00:25:16,740 --> 00:25:21,510
upload data to variety of stores

00:25:19,230 --> 00:25:23,700
depending on what your end stores and

00:25:21,510 --> 00:25:26,370
Azure if it's blob or Azure data Lake

00:25:23,700 --> 00:25:29,370
such as the possibility of marking up

00:25:26,370 --> 00:25:33,450
where in the image the object is that

00:25:29,370 --> 00:25:35,460
you are so this data set is nice because

00:25:33,450 --> 00:25:37,310
it actually it gives you the full image

00:25:35,460 --> 00:25:39,990
and then it also has a highlight around

00:25:37,310 --> 00:25:42,690
box highlight around the face of the pet

00:25:39,990 --> 00:25:45,330
which makes it a good sample data set

00:25:42,690 --> 00:25:47,480
for cases like this but do you mean in

00:25:45,330 --> 00:25:50,880
general or specific for this data set

00:25:47,480 --> 00:25:53,220
specific does that help that no that's

00:25:50,880 --> 00:25:56,070
good yeah it'll like box the face for

00:25:53,220 --> 00:25:57,960
you to make it a bit easier so you're if

00:25:56,070 --> 00:25:59,460
you're looking at a full dog or just a

00:25:57,960 --> 00:26:01,020
dogs but I just didn't see it in the

00:25:59,460 --> 00:26:03,710
github repository but it's somewhere

00:26:01,020 --> 00:26:07,680
yeah if you the link I provided to the

00:26:03,710 --> 00:26:09,930
like Oxford pets data set earlier in the

00:26:07,680 --> 00:26:14,540
slides if you read there it'll have the

00:26:09,930 --> 00:26:14,540
boxed images thank you yeah

00:26:20,270 --> 00:26:31,510
oh yeah this one or the one okay and

00:26:27,730 --> 00:26:34,910
I'll treat billing to the slides as well

00:26:31,510 --> 00:26:38,780
she's like them so if there are no more

00:26:34,910 --> 00:26:39,950
questions is there a question left there

00:26:38,780 --> 00:26:41,230
are more questions let's thank the

00:26:39,950 --> 00:26:46,819
speaker again

00:26:41,230 --> 00:26:46,819

YouTube URL: https://www.youtube.com/watch?v=YlIDlpspvVU


