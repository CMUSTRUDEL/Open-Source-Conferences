Title: NixOS at Tumblr by Graham Christensen (NixCon 2017)
Publication date: 2017-10-31
Playlist: NixCon 2017 (Munich)
Description: 
	Infrastructure automation testing is hard, but NixOS makes it a breeze. Using NixOS' testing framework, Tumblr developed a comprehensive integration test suite for Jetpants, Tumblr's database automation toolkit designed to manage billions of rows and hundreds of database machines. In this talk we will explore the challenges of testing Jetpants, mimicking complex replication and sharding topologies, and future applications of NixOS at Tumblr.
Captions: 
	00:00:00,030 --> 00:00:12,950
so I present to you gray hem thank you

00:00:08,060 --> 00:00:16,260
[Applause]

00:00:12,950 --> 00:00:20,310
hi you all know me as Graham I work for

00:00:16,260 --> 00:00:21,840
tumblr as my day job I work on the site

00:00:20,310 --> 00:00:25,730
reliability engineering team working

00:00:21,840 --> 00:00:27,689
specifically on online databases

00:00:25,730 --> 00:00:31,019
specifically for tumblr mostly that

00:00:27,689 --> 00:00:32,870
means MySQL and memcache and we're going

00:00:31,019 --> 00:00:36,719
to talk about how we use Nick Nix to

00:00:32,870 --> 00:00:38,280
make that reliable and I think a lot of

00:00:36,719 --> 00:00:42,239
this becomes interesting when we look at

00:00:38,280 --> 00:00:47,489
the scale of tumblr and the complexities

00:00:42,239 --> 00:00:50,039
of MySQL this is my tumblr somehow I

00:00:47,489 --> 00:00:56,489
managed to grab beta that tumblr com

00:00:50,039 --> 00:01:00,719
I would have expected it to be taken we

00:00:56,489 --> 00:01:03,930
have over 150 billion posts 370 million

00:01:00,719 --> 00:01:07,530
blogs on the network and we receive 30

00:01:03,930 --> 00:01:10,590
million posts a day that poses

00:01:07,530 --> 00:01:15,450
remarkable scaling challenges and earns

00:01:10,590 --> 00:01:20,189
us in the one of the top 25 sites in the

00:01:15,450 --> 00:01:22,400
United States according to Alexa we do

00:01:20,189 --> 00:01:24,920
this with hundreds of database systems

00:01:22,400 --> 00:01:28,920
obviously billions and billions of rows

00:01:24,920 --> 00:01:31,640
and we have just tons of data and when

00:01:28,920 --> 00:01:37,560
you do it with a tool called jet pants

00:01:31,640 --> 00:01:40,220
jet pants is a MySQL automation toolkit

00:01:37,560 --> 00:01:42,799
it helps us manipulate MySQL replication

00:01:40,220 --> 00:01:46,170
topologies and sharding schemes which

00:01:42,799 --> 00:01:47,700
I'll get into in a minute and it's

00:01:46,170 --> 00:01:52,649
really designed to our specific use

00:01:47,700 --> 00:01:54,299
cases in MySQL a very simple deployment

00:01:52,649 --> 00:01:56,130
involves a single server that handles

00:01:54,299 --> 00:01:59,430
all of the reads and all of the writes

00:01:56,130 --> 00:02:01,500
this has some benefits but major

00:01:59,430 --> 00:02:03,840
downsides involve a few you can only get

00:02:01,500 --> 00:02:06,360
a big enough server so you can only hold

00:02:03,840 --> 00:02:07,520
so much data and you can only serve so

00:02:06,360 --> 00:02:09,330
many reads

00:02:07,520 --> 00:02:13,039
additionally if this one server goes

00:02:09,330 --> 00:02:13,039
down everything goes down

00:02:13,800 --> 00:02:17,520
more complicated but more resilient is

00:02:15,870 --> 00:02:19,860
where you split the reads from the

00:02:17,520 --> 00:02:22,110
rights so right still go to the the

00:02:19,860 --> 00:02:24,500
master and then it replicates by sending

00:02:22,110 --> 00:02:27,570
right instructions down to each replica

00:02:24,500 --> 00:02:30,320
in this way you can scale is as many

00:02:27,570 --> 00:02:32,760
replicas as you need to serve as many

00:02:30,320 --> 00:02:34,830
reads as you need but again if you

00:02:32,760 --> 00:02:37,590
receive too many rights you can't scale

00:02:34,830 --> 00:02:41,190
the master other than by buying a bigger

00:02:37,590 --> 00:02:43,110
server also this can obviously only

00:02:41,190 --> 00:02:48,690
store as much data as can fit on a

00:02:43,110 --> 00:02:51,330
single system in the system if the

00:02:48,690 --> 00:02:53,280
master fails you can still serve reads

00:02:51,330 --> 00:02:56,610
any path doesn't require writing data

00:02:53,280 --> 00:02:59,690
can stay online this is big advantages

00:02:56,610 --> 00:03:02,280
if you architect your system correctly

00:02:59,690 --> 00:03:05,010
at tumblr we use what's called ranged

00:03:02,280 --> 00:03:09,380
based Chardon in order to split the

00:03:05,010 --> 00:03:13,190
rights among many different servers so

00:03:09,380 --> 00:03:16,519
we do it based on the primary ID so

00:03:13,190 --> 00:03:19,620
based on the record that is being stored

00:03:16,519 --> 00:03:22,860
this has a lot of benefits and as a

00:03:19,620 --> 00:03:25,950
sharding scheme is pretty pretty good if

00:03:22,860 --> 00:03:27,780
a single cluster or a single shard goes

00:03:25,950 --> 00:03:29,940
away all the other shards are online

00:03:27,780 --> 00:03:33,239
still happy still serving traffic still

00:03:29,940 --> 00:03:36,450
can accept the reads and writes and if a

00:03:33,239 --> 00:03:38,540
single master goes down for example it

00:03:36,450 --> 00:03:40,590
has a much smaller impact

00:03:38,540 --> 00:03:43,799
additionally there are some other

00:03:40,590 --> 00:03:47,780
shardene schemas that involve like

00:03:43,799 --> 00:03:49,980
modulus or like round-robin writing

00:03:47,780 --> 00:03:52,230
there was gonna be difficult to scale

00:03:49,980 --> 00:03:54,060
out after you set how many you have it

00:03:52,230 --> 00:03:55,440
can be very difficult to add more with

00:03:54,060 --> 00:04:00,060
range based shard and you simply add

00:03:55,440 --> 00:04:01,709
more at the end for an example if we

00:04:00,060 --> 00:04:03,299
have three database machines each

00:04:01,709 --> 00:04:05,970
accepting one through ten 11 through 20

00:04:03,299 --> 00:04:09,650
and 21 through 30 these are primary IDs

00:04:05,970 --> 00:04:12,540
a new post gets created with ID 14 and

00:04:09,650 --> 00:04:14,640
it goes into the 11 through 20 bucket

00:04:12,540 --> 00:04:18,840
the 11 through 10 and 21 through 30

00:04:14,640 --> 00:04:20,880
aren't touched at all and if we get more

00:04:18,840 --> 00:04:23,430
than 30 posts we can easily create a 31

00:04:20,880 --> 00:04:25,820
through 40 or a 31 through infinity for

00:04:23,430 --> 00:04:25,820
example

00:04:25,910 --> 00:04:32,300
in tumblr each of these is actually

00:04:29,260 --> 00:04:34,930
their own replication topologies so each

00:04:32,300 --> 00:04:38,440
one has a master in multiple replicas

00:04:34,930 --> 00:04:44,030
that way we have data resiliency and

00:04:38,440 --> 00:04:47,180
replication to survive failures those

00:04:44,030 --> 00:04:48,740
replication topologies have issues and

00:04:47,180 --> 00:04:51,500
and in my school especially it's a bit

00:04:48,740 --> 00:04:53,840
complicated to promote a replica to

00:04:51,500 --> 00:04:55,610
become a master if the replica master

00:04:53,840 --> 00:04:57,320
fails you have to take something which

00:04:55,610 --> 00:04:59,660
already has the data tell it it's a

00:04:57,320 --> 00:05:02,240
master and then convert everything else

00:04:59,660 --> 00:05:04,850
to read from that that can be a bit

00:05:02,240 --> 00:05:06,710
front a bit difficult to do if you mess

00:05:04,850 --> 00:05:09,500
up the parameters when you're setting up

00:05:06,710 --> 00:05:12,550
minus QL you can accidentally ruin your

00:05:09,500 --> 00:05:15,680
data set Jett pants handles this for us

00:05:12,550 --> 00:05:17,360
in this hypothetical cluster we have a

00:05:15,680 --> 00:05:20,000
master with receiving rights and two

00:05:17,360 --> 00:05:21,950
replicas receiving reads if the master

00:05:20,000 --> 00:05:25,160
dies no longer accepts rights but we can

00:05:21,950 --> 00:05:28,700
still serve reads we then take one of

00:05:25,160 --> 00:05:30,800
the replicas and set it as a master and

00:05:28,700 --> 00:05:34,730
take the other replicas and configure it

00:05:30,800 --> 00:05:35,990
to replicate from that new master we

00:05:34,730 --> 00:05:39,290
then destroyed the old infrastructure

00:05:35,990 --> 00:05:41,870
and clone from one system to the next to

00:05:39,290 --> 00:05:43,540
create a new replica and now we're back

00:05:41,870 --> 00:05:45,890
to where we started a healthy system

00:05:43,540 --> 00:05:51,250
with enough duplicates of the

00:05:45,890 --> 00:05:51,250
information now

00:05:57,479 --> 00:06:03,750
the the difficult thing about this

00:05:59,860 --> 00:06:07,479
system is it's very challenging to test

00:06:03,750 --> 00:06:11,080
and for a long time was entirely manual

00:06:07,479 --> 00:06:14,110
we would have actual database servers in

00:06:11,080 --> 00:06:16,479
our data center that we would clone and

00:06:14,110 --> 00:06:18,190
replicate and kill and test and and it

00:06:16,479 --> 00:06:19,600
took a long long time to do this these

00:06:18,190 --> 00:06:22,150
servers take quite a long time to

00:06:19,600 --> 00:06:27,090
configure because of their how unique

00:06:22,150 --> 00:06:31,810
they are and in order to test them you

00:06:27,090 --> 00:06:34,479
you need I have solved this with NICs

00:06:31,810 --> 00:06:36,870
and in order to do that I needed to

00:06:34,479 --> 00:06:39,750
setup Collins which is our asset tracker

00:06:36,870 --> 00:06:42,460
in order to track what Hardware we have

00:06:39,750 --> 00:06:44,199
we need real systems which we can SSH

00:06:42,460 --> 00:06:45,880
into we have to have a functioning

00:06:44,199 --> 00:06:47,560
Network so they can talk to each other

00:06:45,880 --> 00:06:50,110
and so they can replicate and they need

00:06:47,560 --> 00:06:52,330
to be actually in MySQL and as many of

00:06:50,110 --> 00:06:53,800
these as real as possible in order to

00:06:52,330 --> 00:06:55,870
replicate production so that we don't

00:06:53,800 --> 00:06:58,360
accidentally do something in production

00:06:55,870 --> 00:07:02,410
that breaks something

00:06:58,360 --> 00:07:04,479
in early in early 2017 there was a hack

00:07:02,410 --> 00:07:05,889
week at work and I thought well this

00:07:04,479 --> 00:07:09,159
would be a cool thing to try but it'll

00:07:05,889 --> 00:07:20,669
never certainly never work by the end of

00:07:09,159 --> 00:07:23,380
the week it worked and this was me and

00:07:20,669 --> 00:07:26,020
shortly after my co-workers and my CTO

00:07:23,380 --> 00:07:29,460
this hack week project earned me the CTO

00:07:26,020 --> 00:07:32,409
Choice Award which is was pretty fun

00:07:29,460 --> 00:07:34,270
this this testing framework was

00:07:32,409 --> 00:07:35,919
completely automatic you you said it you

00:07:34,270 --> 00:07:38,409
it runs it shuts down it tells you if it

00:07:35,919 --> 00:07:39,880
works or not it was expected to take six

00:07:38,409 --> 00:07:41,710
months to a year to implement this

00:07:39,880 --> 00:07:43,810
because of the complicated interactions

00:07:41,710 --> 00:07:46,000
and requirements it was expected to be

00:07:43,810 --> 00:07:49,690
dogged slow it was expected to use for

00:07:46,000 --> 00:07:54,130
vm's it was expected to use puppet each

00:07:49,690 --> 00:07:56,199
of those things are very slow it ended

00:07:54,130 --> 00:07:58,479
up as I noted it took a week to build

00:07:56,199 --> 00:08:02,080
the first prototype and it takes six

00:07:58,479 --> 00:08:04,629
minutes to run a complete test I've

00:08:02,080 --> 00:08:08,319
ended up making a DSL around

00:08:04,629 --> 00:08:10,419
the testing system here you can specify

00:08:08,319 --> 00:08:12,339
a number of spare DBS this is just spare

00:08:10,419 --> 00:08:14,139
hardware that's ready to be used number

00:08:12,339 --> 00:08:16,509
of replicas deeb use it starts out by

00:08:14,139 --> 00:08:17,679
creating a replication topology and here

00:08:16,509 --> 00:08:19,689
you can set the number of replicas to

00:08:17,679 --> 00:08:23,499
have and then you can specify a list of

00:08:19,689 --> 00:08:27,009
phases to use of test phases to run each

00:08:23,499 --> 00:08:30,159
of those are just bash wrappers starting

00:08:27,009 --> 00:08:31,839
spared D B's if you have six it'll just

00:08:30,159 --> 00:08:34,959
have six that are not associated into

00:08:31,839 --> 00:08:38,979
any cluster to start any replica DBS if

00:08:34,959 --> 00:08:46,899
you set two or three you will have three

00:08:38,979 --> 00:08:50,620
replicas on the one master and I used

00:08:46,899 --> 00:08:52,240
the monolith because just like in the

00:08:50,620 --> 00:08:55,029
movie this triggered a remarkable

00:08:52,240 --> 00:08:56,800
evolution in how we developed jet pants

00:08:55,029 --> 00:08:58,660
suddenly we were less afraid to go

00:08:56,800 --> 00:09:00,940
change that scary ssh code that could

00:08:58,660 --> 00:09:02,709
let you do nasty things of by mistake or

00:09:00,940 --> 00:09:05,500
you could put new lines in your sequel

00:09:02,709 --> 00:09:07,569
which is a pretty common thing to do and

00:09:05,500 --> 00:09:10,870
it's been it's been a really remarkable

00:09:07,569 --> 00:09:13,439
help I'd like to walk through an example

00:09:10,870 --> 00:09:16,630
test I think that would be interesting

00:09:13,439 --> 00:09:18,130
this is a shard master promotion so this

00:09:16,630 --> 00:09:20,829
is that case we talked about before

00:09:18,130 --> 00:09:22,149
where the master has died the replicas

00:09:20,829 --> 00:09:25,750
there will be two replicas in the

00:09:22,149 --> 00:09:27,519
cluster so when this test starts up

00:09:25,750 --> 00:09:29,500
before it runs any code this is what it

00:09:27,519 --> 00:09:33,699
guarantees will be there a single master

00:09:29,500 --> 00:09:36,910
and to read replicas in the test phase I

00:09:33,699 --> 00:09:38,589
have a this wrapper takes arbitrary Ruby

00:09:36,910 --> 00:09:41,259
code and runs it inside the jetpens

00:09:38,589 --> 00:09:43,750
environment this accepts a pool which is

00:09:41,259 --> 00:09:45,250
the entire shard or the shard range

00:09:43,750 --> 00:09:47,439
which is in this case post one through

00:09:45,250 --> 00:09:52,120
infinity it selects the master and shuts

00:09:47,439 --> 00:09:55,389
it down this is a to simulate a failed

00:09:52,120 --> 00:09:57,550
master at this point after that test

00:09:55,389 --> 00:09:59,230
phase runs the master is dead it can no

00:09:57,550 --> 00:10:02,589
longer accept writes reads are still

00:09:59,230 --> 00:10:05,500
okay I then run a jet pants promotion

00:10:02,589 --> 00:10:08,019
which is just a standard phase is just a

00:10:05,500 --> 00:10:11,380
bit of bash to run it calls jet pants

00:10:08,019 --> 00:10:14,769
promotion it calls passes demote the

00:10:11,380 --> 00:10:17,559
master always starts at 2.10 and the

00:10:14,769 --> 00:10:19,380
replicas are to 11 to 12 and so on for

00:10:17,559 --> 00:10:21,930
the the spares

00:10:19,380 --> 00:10:26,310
so this will take them at the old master

00:10:21,930 --> 00:10:28,560
and replace it with a replica we are now

00:10:26,310 --> 00:10:31,620
at this at this stage where we have the

00:10:28,560 --> 00:10:34,470
new master new master and the old

00:10:31,620 --> 00:10:36,480
replica is still down there

00:10:34,470 --> 00:10:39,180
and then we can assert certain things

00:10:36,480 --> 00:10:41,790
about about the state at the end so at

00:10:39,180 --> 00:10:44,040
this point we can ensure that the master

00:10:41,790 --> 00:10:46,709
is 2.11 as specified previously and

00:10:44,040 --> 00:10:49,949
actually a behavior of Jett pants is by

00:10:46,709 --> 00:10:54,540
default to configure the old master to

00:10:49,949 --> 00:10:57,449
become a replica of the new cluster so

00:10:54,540 --> 00:11:00,480
you ensure that the old master is now a

00:10:57,449 --> 00:11:04,949
replica and that involves some automatic

00:11:00,480 --> 00:11:08,010
logic to start by ice ql these run now

00:11:04,949 --> 00:11:11,040
on every pull request to Jett pants they

00:11:08,010 --> 00:11:14,250
are there's a bout a half dozen tests so

00:11:11,040 --> 00:11:15,899
it does take about 30 minutes but that's

00:11:14,250 --> 00:11:18,329
a lot faster than waiting for even a

00:11:15,899 --> 00:11:19,889
single manual test because of how it's

00:11:18,329 --> 00:11:23,459
slow it is to run those to the manual

00:11:19,889 --> 00:11:27,930
tests yeah it's been really remarkable

00:11:23,459 --> 00:11:29,459
and honestly using NYX and Jenkins has

00:11:27,930 --> 00:11:32,250
taken away almost all of the plan of

00:11:29,459 --> 00:11:34,350
Jenkins I think it took two maybe three

00:11:32,250 --> 00:11:36,920
commits to get our NYX build working

00:11:34,350 --> 00:11:42,630
inside a jetpack Jenkins which is

00:11:36,920 --> 00:11:44,760
astonishing really nice additionally we

00:11:42,630 --> 00:11:48,839
are using Nix to develop an internal

00:11:44,760 --> 00:11:52,380
database the database integrates with

00:11:48,839 --> 00:11:55,560
jet pants it's based on go and it has

00:11:52,380 --> 00:11:57,660
quite a few development tools going

00:11:55,560 --> 00:11:59,699
loves dev time tools for code generation

00:11:57,660 --> 00:12:03,360
and whatnot and it's a quite involved

00:11:59,699 --> 00:12:05,339
build process there are three developers

00:12:03,360 --> 00:12:06,720
on the team building this all three have

00:12:05,339 --> 00:12:08,730
embraced Nix and have quite liked the

00:12:06,720 --> 00:12:10,139
process it took setting up a dev

00:12:08,730 --> 00:12:12,870
environment from however long it would

00:12:10,139 --> 00:12:16,500
take to 10 minutes to download the

00:12:12,870 --> 00:12:17,970
dependencies as I said no time wasted on

00:12:16,500 --> 00:12:19,740
Jenkins if we make a change the build

00:12:17,970 --> 00:12:21,810
process we know it before we push the

00:12:19,740 --> 00:12:23,880
Jenkins we know if it works before we

00:12:21,810 --> 00:12:26,100
push it and using the NIC shell

00:12:23,880 --> 00:12:27,779
obviously lets us make sure we're

00:12:26,100 --> 00:12:29,970
following the same steps the build steps

00:12:27,779 --> 00:12:32,970
do

00:12:29,970 --> 00:12:35,970
the tests do integrate with Jenkins or

00:12:32,970 --> 00:12:37,529
jet pants so every time this internal

00:12:35,970 --> 00:12:41,339
database runs we run the full suite of

00:12:37,529 --> 00:12:43,409
jet pants tests the modularity that the

00:12:41,339 --> 00:12:46,829
module system affords us has made that

00:12:43,409 --> 00:12:48,959
incredibly easy the not to mention how

00:12:46,829 --> 00:12:52,349
incredible in the X OS test process is

00:12:48,959 --> 00:12:57,029
that's a remarkably innovative idea I

00:12:52,349 --> 00:12:58,499
believe this using Nix for this deploy

00:12:57,029 --> 00:12:59,399
process has given us wonderful

00:12:58,499 --> 00:13:02,789
traceability

00:12:59,399 --> 00:13:04,289
it's very trivial to know exactly what

00:13:02,789 --> 00:13:05,699
went into the build process and how we

00:13:04,289 --> 00:13:09,779
ended up with this broken build in

00:13:05,699 --> 00:13:12,749
production and has has made it much

00:13:09,779 --> 00:13:16,199
simpler to diagnose why something is not

00:13:12,749 --> 00:13:18,479
working unfortunately we are not using

00:13:16,199 --> 00:13:21,470
Nix who actually deploy this database it

00:13:18,479 --> 00:13:26,609
is go so it only depends on the linker

00:13:21,470 --> 00:13:28,769
the interpreter so we we do have to

00:13:26,609 --> 00:13:31,589
patch elf the interpreter out to refer

00:13:28,769 --> 00:13:33,509
to the standard a standard interpreter

00:13:31,589 --> 00:13:37,649
path and then we deploy the artifact

00:13:33,509 --> 00:13:39,179
with puppet in the future something that

00:13:37,649 --> 00:13:43,169
we've been working on is deploying

00:13:39,179 --> 00:13:44,970
memcache with a net boot we serve three

00:13:43,169 --> 00:13:49,409
million cache hits per second and that's

00:13:44,970 --> 00:13:51,779
cache hits not including misses we have

00:13:49,409 --> 00:13:53,819
many nodes I didn't even bother to count

00:13:51,779 --> 00:13:59,369
because it would take too long to find

00:13:53,819 --> 00:14:00,809
all the use cases and just so explain

00:13:59,369 --> 00:14:02,429
what memcache is real quick is it's a

00:14:00,809 --> 00:14:04,699
very very simple database it's a key

00:14:02,429 --> 00:14:07,099
value store you put data you get data

00:14:04,699 --> 00:14:09,779
and you can increment and decrement

00:14:07,099 --> 00:14:11,729
there is no persistence so if it reboots

00:14:09,779 --> 00:14:13,970
you've lost all your data if you stop

00:14:11,729 --> 00:14:16,259
start memcache you've lost all your data

00:14:13,970 --> 00:14:20,009
it's an incredible tool for making

00:14:16,259 --> 00:14:22,829
tumblr fast right now we deploy it onto

00:14:20,009 --> 00:14:24,989
this nodes with disks and we deploy it

00:14:22,829 --> 00:14:27,119
with puppet and sometimes those disks

00:14:24,989 --> 00:14:29,489
fail and that is very annoying when your

00:14:27,119 --> 00:14:30,689
service is memcache it doesn't matter if

00:14:29,489 --> 00:14:33,479
the disks have failed because we don't

00:14:30,689 --> 00:14:36,899
ever touch the disks the only thing that

00:14:33,479 --> 00:14:38,100
touches the disc is puppet and so when

00:14:36,899 --> 00:14:43,210
the disk fail we just have to shut it

00:14:38,100 --> 00:14:45,680
down anyway so some time ago I set up a

00:14:43,210 --> 00:14:48,080
pixee boot I had a pixie boots support

00:14:45,680 --> 00:14:52,460
so that our provisioning infrastructure

00:14:48,080 --> 00:14:55,070
can boot Nix OS and started doing a for

00:14:52,460 --> 00:14:57,650
another hack day a net booted memcache

00:14:55,070 --> 00:14:59,390
box I have that working the remaining

00:14:57,650 --> 00:15:04,790
process is integrating it with our

00:14:59,390 --> 00:15:09,230
monitoring tools it's quite likely we'll

00:15:04,790 --> 00:15:11,660
do a production pilot early 2018 we'll

00:15:09,230 --> 00:15:13,760
see that's we are hoping to get that

00:15:11,660 --> 00:15:16,610
scheduled we just had a recent incident

00:15:13,760 --> 00:15:18,680
which would make it quite nice to to be

00:15:16,610 --> 00:15:40,330
net booting these instances instead of

00:15:18,680 --> 00:15:40,330
running off the disk any questions okay

00:15:44,350 --> 00:15:51,110
Nix OS already has built-in support for

00:15:47,360 --> 00:15:54,020
pixie in fact the packet provisioning

00:15:51,110 --> 00:15:56,750
infrastructure uses that tooling

00:15:54,020 --> 00:15:58,910
extensively in that is open source the

00:15:56,750 --> 00:16:01,730
specific tooling that tumblr uses is

00:15:58,910 --> 00:16:03,440
probably not going to be too generic

00:16:01,730 --> 00:16:06,290
outside of what Nick's packages already

00:16:03,440 --> 00:16:07,790
supports I can't stress enough how

00:16:06,290 --> 00:16:12,020
wonderful the Nick's packages and this

00:16:07,790 --> 00:16:13,490
is in this regard you're when you import

00:16:12,020 --> 00:16:14,840
Nick's OS there's a system and then

00:16:13,490 --> 00:16:16,970
there's like a net boot a pixie

00:16:14,840 --> 00:16:19,750
attribute by default and you can just

00:16:16,970 --> 00:16:19,750
use that and be done

00:16:28,240 --> 00:16:34,070
sorry if you mentioned this already so

00:16:30,530 --> 00:16:38,330
your chat pans test today use the QE

00:16:34,070 --> 00:16:42,800
Mook testing infrastructure yep they let

00:16:38,330 --> 00:16:45,590
me just go back the make test that's the

00:16:42,800 --> 00:16:51,220
make test that's in X packages yeah it's

00:16:45,590 --> 00:17:07,520
the exact same Q testing that we use

00:16:51,220 --> 00:17:09,320
behind you did you yeah so as you know

00:17:07,520 --> 00:17:23,690
with our it came we test we often have

00:17:09,320 --> 00:17:25,520
these random failures so we did by this

00:17:23,690 --> 00:17:27,470
our solution is not very elegant we just

00:17:25,520 --> 00:17:32,270
run one test at a time

00:17:27,470 --> 00:17:33,650
and we only run a few simultaneous

00:17:32,270 --> 00:17:36,170
builds on the hosts at any given time

00:17:33,650 --> 00:17:37,820
we're not fully utilizing the capacity

00:17:36,170 --> 00:17:43,160
of the host so we don't typically run

00:17:37,820 --> 00:17:45,260
into those problems yeah I was hoping to

00:17:43,160 --> 00:17:48,160
find a nice solution as part of that but

00:17:45,260 --> 00:17:48,160
I have not

00:17:52,120 --> 00:18:03,770

YouTube URL: https://www.youtube.com/watch?v=6VH945-AaRY


