Title: Node.js Deep Debugging - Gireesh Punathil, IBM India
Publication date: 2021-05-28
Playlist: OpenJS World 2021
Description: 
	While there are established debugging techniques for well known problem types, we have come across rare and complex yet interesting production issues - stemming from pervasive build, deployment configurations and heterogeneous workload types that node.js is subjected to. In this session, we will illustrate case studies of few unique issues that we debugged, custom diagnostic tools that were used and lessons learned. The attendees will learn addressing deeper level production problems and self-diagnose anomalies.
Captions: 
	00:00:01,199 --> 00:00:04,880
hi

00:00:03,120 --> 00:00:07,520
welcome and thank you for joining this

00:00:04,880 --> 00:00:09,280
talk on node.js deeply bugging

00:00:07,520 --> 00:00:10,639
before we talk about what this session

00:00:09,280 --> 00:00:12,880
is all about

00:00:10,639 --> 00:00:14,000
what are the stated purpose and the

00:00:12,880 --> 00:00:16,480
expected outcome

00:00:14,000 --> 00:00:17,440
let me provide a brief intro myself in

00:00:16,480 --> 00:00:20,320
greece

00:00:17,440 --> 00:00:22,160
i work for ibm i'm an architect with a

00:00:20,320 --> 00:00:24,080
team called ibm runtimes

00:00:22,160 --> 00:00:26,000
that's responsible for developing and

00:00:24,080 --> 00:00:29,279
supporting language runtimes

00:00:26,000 --> 00:00:30,480
such as java and node.js i'm also a

00:00:29,279 --> 00:00:32,480
member of the

00:00:30,480 --> 00:00:35,840
node.js technical steering committee and

00:00:32,480 --> 00:00:35,840
the diagnostic working group

00:00:36,000 --> 00:00:40,320
what are the motivation behind today's

00:00:37,520 --> 00:00:42,719
talk i've been involved in

00:00:40,320 --> 00:00:43,680
debugging a number of end user reported

00:00:42,719 --> 00:00:46,079
issues

00:00:43,680 --> 00:00:47,840
in fact i am personally interested in

00:00:46,079 --> 00:00:50,480
troubleshooting

00:00:47,840 --> 00:00:51,280
complex issues that's coming out of the

00:00:50,480 --> 00:00:54,719
production

00:00:51,280 --> 00:00:58,239
deployments such as crash hang

00:00:54,719 --> 00:01:00,719
performance etc that essentially means

00:00:58,239 --> 00:01:01,440
i have spent a good amount of time

00:01:00,719 --> 00:01:04,159
debugging

00:01:01,440 --> 00:01:05,119
node.js and terminals and i felt it

00:01:04,159 --> 00:01:07,040
prudent

00:01:05,119 --> 00:01:09,119
to share some of these experiences some

00:01:07,040 --> 00:01:12,720
of the path i traveled

00:01:09,119 --> 00:01:15,280
with uh people who are using node.js and

00:01:12,720 --> 00:01:16,080
might come across same or similar

00:01:15,280 --> 00:01:19,360
situations

00:01:16,080 --> 00:01:22,560
as the issues that i debugged

00:01:19,360 --> 00:01:25,840
and instead of reinventing the wheel

00:01:22,560 --> 00:01:26,560
it would be pretty easy for them to

00:01:25,840 --> 00:01:28,720
reuse

00:01:26,560 --> 00:01:29,840
some of these experiences and

00:01:28,720 --> 00:01:33,200
[Music]

00:01:29,840 --> 00:01:36,000
lead the issues in hand towards

00:01:33,200 --> 00:01:36,560
queue care resolutions so that's the

00:01:36,000 --> 00:01:39,920
whole

00:01:36,560 --> 00:01:42,159
idea or the motivation behind this talk

00:01:39,920 --> 00:01:43,040
so today i have five issues or five

00:01:42,159 --> 00:01:45,439
topics

00:01:43,040 --> 00:01:47,360
in hand for each of the issues this is

00:01:45,439 --> 00:01:49,119
how i want to explain it

00:01:47,360 --> 00:01:51,280
i'll start with the execution

00:01:49,119 --> 00:01:54,560
environment where the issue is

00:01:51,280 --> 00:01:55,600
reported how the issue is manifested

00:01:54,560 --> 00:01:58,079
externally

00:01:55,600 --> 00:01:58,799
and then the whole path or the

00:01:58,079 --> 00:02:02,560
proceedings

00:01:58,799 --> 00:02:05,360
towards problem determination and

00:02:02,560 --> 00:02:06,880
what was the actual issue and how the

00:02:05,360 --> 00:02:10,000
issue got resolved

00:02:06,880 --> 00:02:12,319
and then the tools the methodologies and

00:02:10,000 --> 00:02:14,640
the best practices that i followed

00:02:12,319 --> 00:02:16,160
as well as what are the learnings and

00:02:14,640 --> 00:02:18,640
the insights

00:02:16,160 --> 00:02:21,040
that are reusable so that's the whole

00:02:18,640 --> 00:02:21,040
idea

00:02:21,280 --> 00:02:25,360
all right so here is our first topic the

00:02:24,400 --> 00:02:29,200
title says

00:02:25,360 --> 00:02:29,840
hi and steady state rss what is rss it

00:02:29,200 --> 00:02:33,360
stands for

00:02:29,840 --> 00:02:37,040
resident set size here size would mean

00:02:33,360 --> 00:02:39,680
uh the memory size as we know

00:02:37,040 --> 00:02:40,319
there are operating system primitives to

00:02:39,680 --> 00:02:42,800
allocate

00:02:40,319 --> 00:02:44,080
and reallocate memory to and from the

00:02:42,800 --> 00:02:48,000
process

00:02:44,080 --> 00:02:51,840
now at any given point in time

00:02:48,000 --> 00:02:54,720
the allocated or deallocated memory

00:02:51,840 --> 00:02:55,040
is not necessarily a true reflection of

00:02:54,720 --> 00:02:59,120
the

00:02:55,040 --> 00:03:02,080
actual memory accounted for the process

00:02:59,120 --> 00:03:03,040
this is because we rely on the virtual

00:03:02,080 --> 00:03:06,159
memory

00:03:03,040 --> 00:03:08,640
system and at any point in time

00:03:06,159 --> 00:03:09,599
in a sufficiently loaded multi-task

00:03:08,640 --> 00:03:11,360
system

00:03:09,599 --> 00:03:12,959
there could be other processes which are

00:03:11,360 --> 00:03:16,080
demanding memory

00:03:12,959 --> 00:03:17,040
and the memory which are not used by the

00:03:16,080 --> 00:03:19,360
process

00:03:17,040 --> 00:03:21,120
or memory that are not used by the

00:03:19,360 --> 00:03:21,920
process for a considerable amount of

00:03:21,120 --> 00:03:24,560
time

00:03:21,920 --> 00:03:25,040
is swapped out of the process and given

00:03:24,560 --> 00:03:27,760
to

00:03:25,040 --> 00:03:29,280
the other process that needs it and then

00:03:27,760 --> 00:03:32,400
at a later point in time

00:03:29,280 --> 00:03:34,480
when the current process requires it the

00:03:32,400 --> 00:03:37,680
memory is brought back in

00:03:34,480 --> 00:03:38,400
so this means the actual accounted

00:03:37,680 --> 00:03:41,519
memory

00:03:38,400 --> 00:03:44,720
is going to be equal or less than

00:03:41,519 --> 00:03:45,360
the allocated memory for the process in

00:03:44,720 --> 00:03:48,959
most of the

00:03:45,360 --> 00:03:52,239
practical scenarios so the rss

00:03:48,959 --> 00:03:55,280
the resident set size is expected to be

00:03:52,239 --> 00:03:56,879
or supposed to be the accounted memory

00:03:55,280 --> 00:03:59,439
for the process

00:03:56,879 --> 00:04:00,560
i stress on the word supposed to be

00:03:59,439 --> 00:04:04,560
because

00:04:00,560 --> 00:04:08,560
that's the whole essence of the

00:04:04,560 --> 00:04:11,200
issue at hand and we will see why so

00:04:08,560 --> 00:04:11,680
the issue was manifested in the form of

00:04:11,200 --> 00:04:14,720
a

00:04:11,680 --> 00:04:15,519
high and steady rss size in the node.js

00:04:14,720 --> 00:04:18,320
process

00:04:15,519 --> 00:04:19,280
and reported by many users with a

00:04:18,320 --> 00:04:22,000
similar symptom

00:04:19,280 --> 00:04:22,560
as they all are seeing very low heap

00:04:22,000 --> 00:04:26,000
usage

00:04:22,560 --> 00:04:29,040
but very high rss this is not a

00:04:26,000 --> 00:04:30,960
surprise because we know that node.js

00:04:29,040 --> 00:04:32,320
is not just using memory from the

00:04:30,960 --> 00:04:34,800
javascript heap

00:04:32,320 --> 00:04:35,360
it has also bearings on the native

00:04:34,800 --> 00:04:38,639
backend

00:04:35,360 --> 00:04:42,479
many objects have memory that is used

00:04:38,639 --> 00:04:45,040
in the native heap as well so

00:04:42,479 --> 00:04:46,400
low javascript heap but high native

00:04:45,040 --> 00:04:48,880
memory and high rss

00:04:46,400 --> 00:04:50,960
on the face of it is not a problem but

00:04:48,880 --> 00:04:53,759
because many users

00:04:50,960 --> 00:04:55,040
uh reported the same issue with a

00:04:53,759 --> 00:04:57,919
similar symptom

00:04:55,040 --> 00:04:58,320
we thought it's problem to investigate

00:04:57,919 --> 00:05:00,320
and

00:04:58,320 --> 00:05:02,320
started looking at it one of the common

00:05:00,320 --> 00:05:05,360
symptoms was the fact that

00:05:02,320 --> 00:05:06,000
they all had a history of huge memory

00:05:05,360 --> 00:05:08,240
allocation

00:05:06,000 --> 00:05:09,520
in the form of various objects getting

00:05:08,240 --> 00:05:11,919
created

00:05:09,520 --> 00:05:13,600
uh back to back and then later all the

00:05:11,919 --> 00:05:14,720
objects getting garbage collected but

00:05:13,600 --> 00:05:16,720
the rs is

00:05:14,720 --> 00:05:19,199
not proportionately coming down so that

00:05:16,720 --> 00:05:19,600
was a common symptom reported by most of

00:05:19,199 --> 00:05:22,720
these

00:05:19,600 --> 00:05:25,600
customers or end users now

00:05:22,720 --> 00:05:26,479
coming to the problem determination uh

00:05:25,600 --> 00:05:27,919
as we know

00:05:26,479 --> 00:05:29,919
the javascript heap is not a true

00:05:27,919 --> 00:05:31,360
reflection so we make use of the

00:05:29,919 --> 00:05:33,199
operating system tools

00:05:31,360 --> 00:05:35,759
to see what's the actual memory

00:05:33,199 --> 00:05:39,280
consumption of the process

00:05:35,759 --> 00:05:42,320
uh the latex tools such as stove pmap

00:05:39,280 --> 00:05:46,080
etc helps here and then we

00:05:42,320 --> 00:05:51,600
look at the consumption and it's correct

00:05:46,080 --> 00:05:53,840
we see high rss but then unfortunately

00:05:51,600 --> 00:05:54,960
we don't have a mechanism to see what is

00:05:53,840 --> 00:05:57,600
the breakup

00:05:54,960 --> 00:05:58,080
of the native memory consumption or what

00:05:57,600 --> 00:06:00,400
are the

00:05:58,080 --> 00:06:02,639
finest level of details about where this

00:06:00,400 --> 00:06:05,680
high rss is accounted for

00:06:02,639 --> 00:06:09,039
within the program or within the

00:06:05,680 --> 00:06:12,160
virtual machine so the only way

00:06:09,039 --> 00:06:14,720
to get to the details would be through

00:06:12,160 --> 00:06:17,360
uh running some simulation testing so we

00:06:14,720 --> 00:06:20,720
wrote custom programs

00:06:17,360 --> 00:06:23,520
to simulate the behavior but we were not

00:06:20,720 --> 00:06:25,039
getting the that's the point in time we

00:06:23,520 --> 00:06:29,600
got into some other hint

00:06:25,039 --> 00:06:32,960
around the behavior

00:06:29,600 --> 00:06:35,840
was not very consistent even with the

00:06:32,960 --> 00:06:37,120
same program that was run in different

00:06:35,840 --> 00:06:39,919
systems

00:06:37,120 --> 00:06:40,479
say for example different system with

00:06:39,919 --> 00:06:43,280
differing

00:06:40,479 --> 00:06:43,759
in specification or differing in terms

00:06:43,280 --> 00:06:46,800
of the

00:06:43,759 --> 00:06:49,520
load conditions so that was

00:06:46,800 --> 00:06:50,800
on the face of it as a challenge but it

00:06:49,520 --> 00:06:53,599
gave us a lead

00:06:50,800 --> 00:06:54,479
in terms of how do we proceed so one of

00:06:53,599 --> 00:06:57,840
the first step

00:06:54,479 --> 00:07:00,160
was to disambiguate or get an answer to

00:06:57,840 --> 00:07:00,880
why the same program is behaving

00:07:00,160 --> 00:07:04,000
differently

00:07:00,880 --> 00:07:06,800
in systems with differing spec

00:07:04,000 --> 00:07:07,440
or differing amount of physical memory

00:07:06,800 --> 00:07:10,080
or

00:07:07,440 --> 00:07:11,360
different set of load characteristics in

00:07:10,080 --> 00:07:15,120
the system

00:07:11,360 --> 00:07:18,240
so that actually gave us some

00:07:15,120 --> 00:07:22,080
opportunity to dig into the problem

00:07:18,240 --> 00:07:25,360
we see two patterns that is

00:07:22,080 --> 00:07:27,919
when the system is high end with

00:07:25,360 --> 00:07:29,440
ample amount of physical memory and the

00:07:27,919 --> 00:07:33,199
system is relatively

00:07:29,440 --> 00:07:36,319
idle then we saw that the rss

00:07:33,199 --> 00:07:39,599
once uh hiked up is never coming down

00:07:36,319 --> 00:07:41,039
whereas if the system is uh tight

00:07:39,599 --> 00:07:43,039
in terms of the physical memory with

00:07:41,039 --> 00:07:46,000
respect to the rolling process

00:07:43,039 --> 00:07:46,639
and if there are a lot of load in the

00:07:46,000 --> 00:07:49,759
system

00:07:46,639 --> 00:07:50,560
then we see that rss is immediately

00:07:49,759 --> 00:07:53,120
coming down

00:07:50,560 --> 00:07:55,759
which is in accordance with the

00:07:53,120 --> 00:07:58,960
expectation from the nodes process

00:07:55,759 --> 00:08:01,520
so uh reading through

00:07:58,960 --> 00:08:03,919
various uh documentations various

00:08:01,520 --> 00:08:06,720
manuals and specifications around

00:08:03,919 --> 00:08:07,120
virtual memory and resident set memory

00:08:06,720 --> 00:08:10,840
and

00:08:07,120 --> 00:08:15,039
with the help of a small c program which

00:08:10,840 --> 00:08:17,199
truly uh simulated the node.js scenario

00:08:15,039 --> 00:08:18,400
we could actually understand the root

00:08:17,199 --> 00:08:21,039
cause of the issue

00:08:18,400 --> 00:08:21,680
so in my small c test case what i was

00:08:21,039 --> 00:08:24,400
doing is

00:08:21,680 --> 00:08:25,599
allocate small chunks of memory in rapid

00:08:24,400 --> 00:08:29,440
succession

00:08:25,599 --> 00:08:29,840
but in large amount and then immediately

00:08:29,440 --> 00:08:33,279
free

00:08:29,840 --> 00:08:36,399
all those memory say for example uh 2 kb

00:08:33,279 --> 00:08:38,880
chunks of say 1000 such

00:08:36,399 --> 00:08:39,440
chunks allocate from the free pool and

00:08:38,880 --> 00:08:41,839
then free

00:08:39,440 --> 00:08:42,880
immediately then sleep for some time to

00:08:41,839 --> 00:08:46,080
see if the

00:08:42,880 --> 00:08:49,279
rss that shot up would come down or not

00:08:46,080 --> 00:08:52,640
so we could see that the rss

00:08:49,279 --> 00:08:55,839
stays on top forever unless you

00:08:52,640 --> 00:08:58,880
add more process to the system and bring

00:08:55,839 --> 00:08:59,839
in a demand in the memory the process is

00:08:58,880 --> 00:09:03,200
never going to

00:08:59,839 --> 00:09:04,080
relinquish the the amount of rss that

00:09:03,200 --> 00:09:07,920
was accounted

00:09:04,080 --> 00:09:09,200
in the system in the process that's

00:09:07,920 --> 00:09:11,920
really a surprise to me

00:09:09,200 --> 00:09:13,760
for at least myself and my fellow

00:09:11,920 --> 00:09:17,839
customers or the end users

00:09:13,760 --> 00:09:19,760
i don't know if that's a known art but

00:09:17,839 --> 00:09:20,959
it was really surprising the resident

00:09:19,760 --> 00:09:24,480
set size is

00:09:20,959 --> 00:09:25,920
not truly reflecting the resident memory

00:09:24,480 --> 00:09:28,480
of the process the process has

00:09:25,920 --> 00:09:30,320
long back freed up the memory but still

00:09:28,480 --> 00:09:32,320
unfortunately it's all accounted against

00:09:30,320 --> 00:09:34,560
the same process

00:09:32,320 --> 00:09:36,240
now what's the resolution we don't have

00:09:34,560 --> 00:09:38,720
a resolution yet identified

00:09:36,240 --> 00:09:40,240
we have an rfe opened against a libya v

00:09:38,720 --> 00:09:42,160
component

00:09:40,240 --> 00:09:43,519
uh unfortunately the challenge with

00:09:42,160 --> 00:09:45,839
respect to the

00:09:43,519 --> 00:09:46,800
implementation or the solution of this

00:09:45,839 --> 00:09:49,680
problem

00:09:46,800 --> 00:09:50,640
is the fact that uh we don't have a

00:09:49,680 --> 00:09:53,519
consistent

00:09:50,640 --> 00:09:55,279
or well-defined api that would capture

00:09:53,519 --> 00:09:58,880
the actual

00:09:55,279 --> 00:10:00,320
uh account or memory in a process

00:09:58,880 --> 00:10:01,920
the actual accounted memory in the

00:10:00,320 --> 00:10:05,120
process is something called

00:10:01,920 --> 00:10:07,360
working set size in linux uh

00:10:05,120 --> 00:10:08,480
unfortunately we don't have an api to

00:10:07,360 --> 00:10:11,760
get that value out of

00:10:08,480 --> 00:10:12,800
it and then secondly uh this is not

00:10:11,760 --> 00:10:15,360
something which we can

00:10:12,800 --> 00:10:17,120
implement cross-platform where uh

00:10:15,360 --> 00:10:18,399
node.js is supported in a number of

00:10:17,120 --> 00:10:20,959
other platforms where

00:10:18,399 --> 00:10:23,120
we may not be able to consistently get

00:10:20,959 --> 00:10:27,200
this value so for these reasons

00:10:23,120 --> 00:10:30,640
the rfe is

00:10:27,200 --> 00:10:33,040
still not completed so the key

00:10:30,640 --> 00:10:34,160
learning from this exercise is the fact

00:10:33,040 --> 00:10:37,519
that

00:10:34,160 --> 00:10:40,160
rss may not truly reflect

00:10:37,519 --> 00:10:40,880
the accounted memory or the active

00:10:40,160 --> 00:10:44,079
memory

00:10:40,880 --> 00:10:45,040
in your process it depends on the system

00:10:44,079 --> 00:10:48,480
characteristics

00:10:45,040 --> 00:10:49,760
also if it is largely ideal with a lot

00:10:48,480 --> 00:10:52,640
of ample memory

00:10:49,760 --> 00:10:54,880
never trust the rss that's my bottom

00:10:52,640 --> 00:10:54,880
line

00:10:56,240 --> 00:11:01,760
so the next one is exit race

00:10:59,519 --> 00:11:03,760
this is an issue that was reported in

00:11:01,760 --> 00:11:06,240
northeast version 11

00:11:03,760 --> 00:11:07,920
so what is exit race of course the name

00:11:06,240 --> 00:11:09,279
was given at a later point in time when

00:11:07,920 --> 00:11:11,120
we completely diagnosed

00:11:09,279 --> 00:11:12,880
the issue and the resolution was

00:11:11,120 --> 00:11:15,360
identified

00:11:12,880 --> 00:11:16,079
the issue was manifested as random

00:11:15,360 --> 00:11:18,959
crashes

00:11:16,079 --> 00:11:20,160
by all sort of programs with no specific

00:11:18,959 --> 00:11:23,200
patterns

00:11:20,160 --> 00:11:25,040
whatsoever that can be identified so we

00:11:23,200 --> 00:11:26,880
got a segmentation fault

00:11:25,040 --> 00:11:29,680
we got aboard we got illegal

00:11:26,880 --> 00:11:32,720
instructions to name a few

00:11:29,680 --> 00:11:35,279
the pd approach was to

00:11:32,720 --> 00:11:36,480
run the program many times under the

00:11:35,279 --> 00:11:40,720
problematic version

00:11:36,480 --> 00:11:42,560
and collect as many systems as possible

00:11:40,720 --> 00:11:44,000
associate the failing context from the

00:11:42,560 --> 00:11:47,200
terms and see

00:11:44,000 --> 00:11:49,600
if we can do some classification

00:11:47,200 --> 00:11:51,519
uh based on the failing context into a

00:11:49,600 --> 00:11:54,639
discrete set of

00:11:51,519 --> 00:11:57,920
patterns rather than indefinite set of

00:11:54,639 --> 00:11:58,320
random crashes so this approach helped

00:11:57,920 --> 00:12:01,279
in

00:11:58,320 --> 00:12:02,079
two aspects number one it helps to

00:12:01,279 --> 00:12:04,079
understand

00:12:02,079 --> 00:12:05,600
there are two or three or four discrete

00:12:04,079 --> 00:12:09,600
set of patterns

00:12:05,600 --> 00:12:12,639
and not for too many number two

00:12:09,600 --> 00:12:16,639
it revealed that in all the cases the

00:12:12,639 --> 00:12:20,639
main thread was in its exit path so that

00:12:16,639 --> 00:12:23,040
essentially means in all the programs

00:12:20,639 --> 00:12:24,160
it was about to exit or the main thread

00:12:23,040 --> 00:12:26,480
has initiated

00:12:24,160 --> 00:12:28,720
the cleanup activities so this was

00:12:26,480 --> 00:12:30,800
really a strong lead

00:12:28,720 --> 00:12:33,279
the other challenges that we faced were

00:12:30,800 --> 00:12:35,920
like when we did a git bisect

00:12:33,279 --> 00:12:37,440
uh it showed some comment and by looking

00:12:35,920 --> 00:12:39,040
at the changes that was

00:12:37,440 --> 00:12:40,720
belonging to the commit which was so

00:12:39,040 --> 00:12:42,959
evident that

00:12:40,720 --> 00:12:44,240
that commit has no relation with the

00:12:42,959 --> 00:12:46,560
issue because

00:12:44,240 --> 00:12:48,480
it was absolutely no way connected but

00:12:46,560 --> 00:12:53,839
then it was just adding to the

00:12:48,480 --> 00:12:53,839
timing window

00:12:54,800 --> 00:12:58,560
so the problem was that there is a set

00:12:57,519 --> 00:13:00,800
of resources

00:12:58,560 --> 00:13:01,760
or entities or assets whatever we want

00:13:00,800 --> 00:13:04,959
to call it

00:13:01,760 --> 00:13:07,680
that node creates or initializes

00:13:04,959 --> 00:13:08,800
at the time of the boat strap and those

00:13:07,680 --> 00:13:11,920
assets or

00:13:08,800 --> 00:13:12,720
entities or resources are destroyed or

00:13:11,920 --> 00:13:14,959
cleaned up

00:13:12,720 --> 00:13:16,800
in the reverse corridor at the time of

00:13:14,959 --> 00:13:20,560
the node exit

00:13:16,800 --> 00:13:23,760
so the order is absolutely important

00:13:20,560 --> 00:13:25,680
because there are interdependencies that

00:13:23,760 --> 00:13:27,760
could be defined between the resources

00:13:25,680 --> 00:13:29,040
and the core part that access these

00:13:27,760 --> 00:13:31,279
resources

00:13:29,040 --> 00:13:32,240
the issue pops up when there are

00:13:31,279 --> 00:13:33,920
multiple threads

00:13:32,240 --> 00:13:36,160
it could be the worker threads or it

00:13:33,920 --> 00:13:39,040
could be the internal helper threads

00:13:36,160 --> 00:13:40,000
that node uses to perform asynchronous

00:13:39,040 --> 00:13:41,519
operations

00:13:40,000 --> 00:13:43,920
so when there are multiple threads

00:13:41,519 --> 00:13:46,320
involved and if we don't do

00:13:43,920 --> 00:13:48,000
the cleanup in the reverse order

00:13:46,320 --> 00:13:51,120
properly or we don't harness

00:13:48,000 --> 00:13:52,880
the threats then these threads

00:13:51,120 --> 00:13:54,320
were accessing the resources which were

00:13:52,880 --> 00:13:57,519
already destroyed

00:13:54,320 --> 00:14:00,800
and that was causing the random crash

00:13:57,519 --> 00:14:02,079
and definitely based on the at that

00:14:00,800 --> 00:14:04,000
point where these threads were

00:14:02,079 --> 00:14:06,000
accessing the resource we were seeing

00:14:04,000 --> 00:14:07,839
random crashes

00:14:06,000 --> 00:14:10,079
so the resolution was to follow the

00:14:07,839 --> 00:14:13,120
order in which the

00:14:10,079 --> 00:14:16,880
resource was created to be followed

00:14:13,120 --> 00:14:19,040
at the time of the as well as

00:14:16,880 --> 00:14:20,560
to make sure that the helper threads

00:14:19,040 --> 00:14:21,600
which can potentially access these

00:14:20,560 --> 00:14:24,320
resources

00:14:21,600 --> 00:14:25,440
are um you know kept in a very

00:14:24,320 --> 00:14:26,959
consistent state

00:14:25,440 --> 00:14:28,880
at the time when we are doing the

00:14:26,959 --> 00:14:30,399
cleanup which means if

00:14:28,880 --> 00:14:32,079
one or more threads can access one of

00:14:30,399 --> 00:14:34,560
the resource we make sure

00:14:32,079 --> 00:14:36,720
the thread is brought to the state where

00:14:34,560 --> 00:14:39,279
it does not make for the threat to

00:14:36,720 --> 00:14:41,360
access that resource anymore that was

00:14:39,279 --> 00:14:43,600
the resolution

00:14:41,360 --> 00:14:45,360
one of the problem determination

00:14:43,600 --> 00:14:48,320
technique that we learned

00:14:45,360 --> 00:14:49,600
and applied in this scenario is called a

00:14:48,320 --> 00:14:52,639
timing injection

00:14:49,600 --> 00:14:55,839
so timing injection essentially involves

00:14:52,639 --> 00:14:58,160
adding sleep delays between activities

00:14:55,839 --> 00:14:58,880
where we suspect presence of multiple

00:14:58,160 --> 00:15:01,440
threads

00:14:58,880 --> 00:15:03,279
which can cause issues so basically

00:15:01,440 --> 00:15:04,480
widening the timing window in an

00:15:03,279 --> 00:15:09,120
artificial manner

00:15:04,480 --> 00:15:09,120
and leading to simulating race

00:15:10,839 --> 00:15:17,040
conditions all right

00:15:13,680 --> 00:15:18,399
data truncation with repiping again one

00:15:17,040 --> 00:15:20,720
of the complex issue

00:15:18,399 --> 00:15:21,600
that we have worked with this is

00:15:20,720 --> 00:15:23,920
something which is

00:15:21,600 --> 00:15:25,519
not even easy to explain to a normal

00:15:23,920 --> 00:15:28,240
programmer

00:15:25,519 --> 00:15:30,480
so what was happening consider to

00:15:28,240 --> 00:15:33,120
node.js style processors

00:15:30,480 --> 00:15:35,040
like any other process these process

00:15:33,120 --> 00:15:37,600
also have the standard streams

00:15:35,040 --> 00:15:39,360
such as the standard input output and

00:15:37,600 --> 00:15:41,519
the error stream

00:15:39,360 --> 00:15:42,560
a key difference with the child process

00:15:41,519 --> 00:15:45,920
of the node just

00:15:42,560 --> 00:15:50,000
is that these streams are essentially

00:15:45,920 --> 00:15:50,720
pipes long story short to our confusions

00:15:50,000 --> 00:15:53,199
around

00:15:50,720 --> 00:15:54,160
which process owns the console from

00:15:53,199 --> 00:15:56,639
where the process

00:15:54,160 --> 00:15:57,199
are spawned between the parent and child

00:15:56,639 --> 00:15:58,800
all the

00:15:57,199 --> 00:16:01,120
native stream handles or the child

00:15:58,800 --> 00:16:04,399
process are closed by default

00:16:01,120 --> 00:16:06,880
and then three pipes are created

00:16:04,399 --> 00:16:08,480
by the parent and the one end of the

00:16:06,880 --> 00:16:09,120
pipe is held by the parent while the

00:16:08,480 --> 00:16:12,560
other end

00:16:09,120 --> 00:16:14,880
is given to the child and the child uses

00:16:12,560 --> 00:16:16,720
it as if it's one of the standard

00:16:14,880 --> 00:16:17,839
streams so whenever the child writes

00:16:16,720 --> 00:16:21,040
into the stream

00:16:17,839 --> 00:16:22,959
it goes to the parent and whenever the

00:16:21,040 --> 00:16:25,839
parent writes into the input stream of

00:16:22,959 --> 00:16:28,320
the child it gets uh

00:16:25,839 --> 00:16:29,120
flow into the child process so that's

00:16:28,320 --> 00:16:31,120
why

00:16:29,120 --> 00:16:32,800
the child is able to use the standard

00:16:31,120 --> 00:16:35,279
streams effectively as if

00:16:32,800 --> 00:16:36,000
those are the standard input output and

00:16:35,279 --> 00:16:39,600
uh error

00:16:36,000 --> 00:16:40,320
streams while uh parent is able to find

00:16:39,600 --> 00:16:42,639
control

00:16:40,320 --> 00:16:43,680
those streams so that mechanism works

00:16:42,639 --> 00:16:46,320
pretty good

00:16:43,680 --> 00:16:48,639
now coming back to the context problem

00:16:46,320 --> 00:16:51,680
context what was happening is

00:16:48,639 --> 00:16:53,360
when a two child process were engaged in

00:16:51,680 --> 00:16:56,399
the communication

00:16:53,360 --> 00:16:58,880
by uh re-piping

00:16:56,399 --> 00:16:59,759
the standard streams for example one of

00:16:58,880 --> 00:17:02,240
the child process

00:16:59,759 --> 00:17:03,040
output stream was piped into the other

00:17:02,240 --> 00:17:05,760
child process

00:17:03,040 --> 00:17:07,199
input stream so that whenever the child

00:17:05,760 --> 00:17:09,280
process is writing

00:17:07,199 --> 00:17:11,120
the data should flow into the second

00:17:09,280 --> 00:17:12,880
child process so that was the

00:17:11,120 --> 00:17:14,240
understanding that was the design but

00:17:12,880 --> 00:17:16,000
unfortunately

00:17:14,240 --> 00:17:17,760
the data was getting lost in an

00:17:16,000 --> 00:17:19,839
arbitrarily manner in an arbitrary

00:17:17,760 --> 00:17:19,839
manner

00:17:20,400 --> 00:17:23,839
the challenges with respect to the

00:17:22,640 --> 00:17:26,799
problem declination

00:17:23,839 --> 00:17:27,439
was that the truncation pattern was not

00:17:26,799 --> 00:17:29,200
consistent

00:17:27,439 --> 00:17:31,600
every time we get different set of

00:17:29,200 --> 00:17:32,720
results sometimes the test passes and

00:17:31,600 --> 00:17:34,320
sometimes

00:17:32,720 --> 00:17:37,200
we don't get we don't see any data

00:17:34,320 --> 00:17:39,600
flowing into the second process etc

00:17:37,200 --> 00:17:40,799
the second challenge was that we did not

00:17:39,600 --> 00:17:43,039
have a proper tool

00:17:40,799 --> 00:17:43,919
we did not have a proper methodology

00:17:43,039 --> 00:17:49,039
also

00:17:43,919 --> 00:17:49,039
to debug this kind of a problem so

00:17:52,080 --> 00:17:56,080
one of the hypotheses that we made

00:17:54,000 --> 00:17:59,120
around this was

00:17:56,080 --> 00:18:00,160
the child endpoint is already a pipe we

00:17:59,120 --> 00:18:03,840
know about that

00:18:00,160 --> 00:18:06,400
and that essentially means there is um

00:18:03,840 --> 00:18:07,120
already a channel that is open between

00:18:06,400 --> 00:18:08,960
the

00:18:07,120 --> 00:18:10,720
one of the child process and the parent

00:18:08,960 --> 00:18:14,000
process now

00:18:10,720 --> 00:18:16,559
then the child end of the pipe is

00:18:14,000 --> 00:18:18,799
re-piped that essentially means the the

00:18:16,559 --> 00:18:19,760
file descriptor the underlying handle of

00:18:18,799 --> 00:18:22,880
the pipe

00:18:19,760 --> 00:18:25,679
is copied and

00:18:22,880 --> 00:18:26,880
redirected to the other end point of the

00:18:25,679 --> 00:18:28,720
second child

00:18:26,880 --> 00:18:30,240
what happens to the channel that is

00:18:28,720 --> 00:18:32,480
still open between

00:18:30,240 --> 00:18:33,600
the first child process and its parent

00:18:32,480 --> 00:18:36,480
process

00:18:33,600 --> 00:18:37,360
so arguably that is still open and the

00:18:36,480 --> 00:18:40,320
data could flow

00:18:37,360 --> 00:18:41,200
through that channel as well so now

00:18:40,320 --> 00:18:42,640
depending on

00:18:41,200 --> 00:18:45,360
which two pairs of the process are

00:18:42,640 --> 00:18:48,960
currently active which two process

00:18:45,360 --> 00:18:51,120
uh are having the cpu cycles to spare

00:18:48,960 --> 00:18:52,400
and which process is currently writing

00:18:51,120 --> 00:18:55,039
and which process is

00:18:52,400 --> 00:18:56,799
uh ready to read in terms of its code

00:18:55,039 --> 00:19:00,320
flow and control flow

00:18:56,799 --> 00:19:02,480
the data would actually go partly to the

00:19:00,320 --> 00:19:03,919
pattern process and partly to the second

00:19:02,480 --> 00:19:07,600
child process because both the

00:19:03,919 --> 00:19:09,919
both the pipes are open and it depends

00:19:07,600 --> 00:19:11,120
which pipe is being exercised which

00:19:09,919 --> 00:19:15,200
process is active

00:19:11,120 --> 00:19:15,200
etc that's really funny scenario

00:19:16,559 --> 00:19:19,919
with the help of some powerful tooling

00:19:18,640 --> 00:19:21,760
such as asteroids

00:19:19,919 --> 00:19:22,960
we were able to actually get to the

00:19:21,760 --> 00:19:25,760
bottom of it and

00:19:22,960 --> 00:19:26,240
prove this hypothesis we were able to

00:19:25,760 --> 00:19:29,440
see

00:19:26,240 --> 00:19:31,919
exactly what they suspected that

00:19:29,440 --> 00:19:33,919
data was partly going into one process

00:19:31,919 --> 00:19:36,960
and the rest of the data without any

00:19:33,919 --> 00:19:39,280
laws was flowing into the second process

00:19:36,960 --> 00:19:40,960
the solution was pretty straightforward

00:19:39,280 --> 00:19:43,120
when the pipes are re-piped

00:19:40,960 --> 00:19:44,080
we need to make sure that the existing

00:19:43,120 --> 00:19:46,400
pipe should be

00:19:44,080 --> 00:19:47,120
deactivated that means the remote

00:19:46,400 --> 00:19:53,360
endpoint

00:19:47,120 --> 00:19:55,440
of the existing pipe needs to be closed

00:19:53,360 --> 00:19:55,440
not

00:20:05,120 --> 00:20:11,520
that we available to build a theory

00:20:08,880 --> 00:20:12,400
that potentially explains the problem at

00:20:11,520 --> 00:20:15,840
hand

00:20:12,400 --> 00:20:18,720
and then write test cases and debug

00:20:15,840 --> 00:20:20,080
around the hypothesis to prove or

00:20:18,720 --> 00:20:22,960
disprove that hypothesis

00:20:20,080 --> 00:20:24,159
and then if it is not to be the case

00:20:22,960 --> 00:20:25,760
make a new theory

00:20:24,159 --> 00:20:27,919
and write a case and validate around

00:20:25,760 --> 00:20:31,919
that without that if you are

00:20:27,919 --> 00:20:34,159
just starting by writing test cases and

00:20:31,919 --> 00:20:35,200
traveling in the reverse direction i

00:20:34,159 --> 00:20:38,480
don't think it's

00:20:35,200 --> 00:20:41,679
going to give much use to the problem

00:20:38,480 --> 00:20:41,679
determination exercise

00:20:43,919 --> 00:20:48,559
okay shutting down node.js in flight

00:20:47,919 --> 00:20:51,120
well

00:20:48,559 --> 00:20:52,640
looks pretty straightforward looks not

00:20:51,120 --> 00:20:56,320
even an issue

00:20:52,640 --> 00:20:59,200
what is shutting down in flight

00:20:56,320 --> 00:21:00,720
terminate the javascript runtime and the

00:20:59,200 --> 00:21:03,679
platform that was started by the

00:21:00,720 --> 00:21:04,240
node.js process isn't it as simple as

00:21:03,679 --> 00:21:07,280
calling

00:21:04,240 --> 00:21:08,400
process.exit or killing the process with

00:21:07,280 --> 00:21:11,520
a signal such as

00:21:08,400 --> 00:21:14,400
control c the point is

00:21:11,520 --> 00:21:16,159
in this particular case we don't want to

00:21:14,400 --> 00:21:18,000
get out of the process

00:21:16,159 --> 00:21:19,919
but still want to terminate the node.js

00:21:18,000 --> 00:21:21,840
sequences

00:21:19,919 --> 00:21:23,600
but then what is left when node.js

00:21:21,840 --> 00:21:26,720
terminates

00:21:23,600 --> 00:21:29,200
that makes this case very special

00:21:26,720 --> 00:21:30,559
the use case here is that we make use of

00:21:29,200 --> 00:21:33,280
an embedder

00:21:30,559 --> 00:21:34,159
an embedder by definition is a process

00:21:33,280 --> 00:21:36,320
that embeds

00:21:34,159 --> 00:21:37,600
node.js with a container component

00:21:36,320 --> 00:21:40,400
relation

00:21:37,600 --> 00:21:41,039
not as a patent child or not as a server

00:21:40,400 --> 00:21:44,080
client

00:21:41,039 --> 00:21:46,720
relation but as an in-process component

00:21:44,080 --> 00:21:47,280
so this means an embedder has a custom

00:21:46,720 --> 00:21:50,080
launcher

00:21:47,280 --> 00:21:50,480
or an entry point function and the entry

00:21:50,080 --> 00:21:53,200
point

00:21:50,480 --> 00:21:53,760
is responsible for initializing a number

00:21:53,200 --> 00:21:57,360
of

00:21:53,760 --> 00:21:59,360
components of the embedded application

00:21:57,360 --> 00:22:00,559
and one of the component happens to be

00:21:59,360 --> 00:22:03,840
node.js

00:22:00,559 --> 00:22:07,440
now in this setup the lifecycle of the

00:22:03,840 --> 00:22:10,799
embedded process could be much larger

00:22:07,440 --> 00:22:13,600
much wider than the lifecycle of nodejs

00:22:10,799 --> 00:22:14,080
to give a simple example in response to

00:22:13,600 --> 00:22:16,559
certain

00:22:14,080 --> 00:22:17,840
external commands the embedder would

00:22:16,559 --> 00:22:20,799
want to shut down

00:22:17,840 --> 00:22:22,559
the running instance of the node while

00:22:20,799 --> 00:22:24,840
continuing to work with

00:22:22,559 --> 00:22:26,159
the rest of the components of the

00:22:24,840 --> 00:22:29,440
embedder there

00:22:26,159 --> 00:22:32,559
are many nodes inverters but the

00:22:29,440 --> 00:22:33,200
two things that i know of are one is

00:22:32,559 --> 00:22:36,799
electron

00:22:33,200 --> 00:22:38,400
and one is ibm iib iib stands for ibm

00:22:36,799 --> 00:22:41,360
integration bus

00:22:38,400 --> 00:22:43,760
that defines a framework for connecting

00:22:41,360 --> 00:22:46,480
heterogeneous applications

00:22:43,760 --> 00:22:46,880
so the problem is that node.js event

00:22:46,480 --> 00:22:49,840
loop

00:22:46,880 --> 00:22:51,360
is designed in such a manner that as

00:22:49,840 --> 00:22:52,480
long as there are active handles in the

00:22:51,360 --> 00:22:55,280
even loop

00:22:52,480 --> 00:22:56,080
the loop is not going to exit the only

00:22:55,280 --> 00:22:58,799
way to get

00:22:56,080 --> 00:22:59,360
out of the even loop is to send an exit

00:22:58,799 --> 00:23:01,440
signal

00:22:59,360 --> 00:23:02,559
but that's going to terminate not only

00:23:01,440 --> 00:23:09,280
the even loop

00:23:02,559 --> 00:23:13,120
but also the wider embedded process

00:23:09,280 --> 00:23:16,720
so that's where we need a mechanism to

00:23:13,120 --> 00:23:19,520
uh shut down the node.js instance

00:23:16,720 --> 00:23:20,000
specifically and in a custom manner

00:23:19,520 --> 00:23:23,120
where

00:23:20,000 --> 00:23:26,320
the even loop is quest and

00:23:23,120 --> 00:23:26,640
it silently and gracefully uh comes out

00:23:26,320 --> 00:23:28,559
of

00:23:26,640 --> 00:23:30,400
all the processing so that the embedder

00:23:28,559 --> 00:23:33,120
can continue

00:23:30,400 --> 00:23:33,840
so the way the stop api which was

00:23:33,120 --> 00:23:37,600
proposed

00:23:33,840 --> 00:23:39,679
uh work is like this it inserts an

00:23:37,600 --> 00:23:40,960
async handler into the even loop in the

00:23:39,679 --> 00:23:43,440
beginning and

00:23:40,960 --> 00:23:45,200
when the embedder wants to stop the

00:23:43,440 --> 00:23:50,000
node.js instance

00:23:45,200 --> 00:23:52,799
it um files in a sync event with the

00:23:50,000 --> 00:23:53,279
handle that we talked about earlier as

00:23:52,799 --> 00:23:56,720
the

00:23:53,279 --> 00:23:58,799
sync handle this triggers an async event

00:23:56,720 --> 00:24:00,000
in the even loop and subsequently the

00:23:58,799 --> 00:24:03,279
handler gets called

00:24:00,000 --> 00:24:04,320
which is um invoked in the main thread

00:24:03,279 --> 00:24:06,720
of course by

00:24:04,320 --> 00:24:08,080
stopping all other processing in the

00:24:06,720 --> 00:24:11,279
main thread

00:24:08,080 --> 00:24:13,039
and in the handler we close all the

00:24:11,279 --> 00:24:14,400
handles in the even loop and basically

00:24:13,039 --> 00:24:17,440
we close the even loop

00:24:14,400 --> 00:24:18,320
we come out of it we terminate all the

00:24:17,440 --> 00:24:21,440
worker threats

00:24:18,320 --> 00:24:25,120
and then start the cleanup activities

00:24:21,440 --> 00:24:27,360
in a graceful manner this eventually

00:24:25,120 --> 00:24:28,640
shuts down the node.js instance and

00:24:27,360 --> 00:24:32,720
returns to the caller

00:24:28,640 --> 00:24:35,120
which in this case is the embedder now

00:24:32,720 --> 00:24:36,080
my key take away from this particular

00:24:35,120 --> 00:24:38,480
activity

00:24:36,080 --> 00:24:39,440
is that when new architectural

00:24:38,480 --> 00:24:42,400
posabilities

00:24:39,440 --> 00:24:43,360
are attempted we learn a lot and in this

00:24:42,400 --> 00:24:45,279
case

00:24:43,360 --> 00:24:46,880
specifically around how the even loop

00:24:45,279 --> 00:24:48,799
works how the

00:24:46,880 --> 00:24:50,640
overall architecture of the even loop is

00:24:48,799 --> 00:24:51,360
designed and then how the threads

00:24:50,640 --> 00:24:54,720
coordinate

00:24:51,360 --> 00:24:54,720
around the even loop etcetera

00:24:56,159 --> 00:25:01,200
and the last one in this series is the

00:24:58,880 --> 00:25:04,640
truncated stdio data

00:25:01,200 --> 00:25:07,039
on process exit this was reported by

00:25:04,640 --> 00:25:08,640
many users with varying problem

00:25:07,039 --> 00:25:11,200
descriptions

00:25:08,640 --> 00:25:11,679
here the process is again a child

00:25:11,200 --> 00:25:14,240
process

00:25:11,679 --> 00:25:14,880
and they started out data are typically

00:25:14,240 --> 00:25:18,880
large

00:25:14,880 --> 00:25:20,880
like in kilobytes so that means

00:25:18,880 --> 00:25:22,640
if you're having a child process that

00:25:20,880 --> 00:25:25,520
writes kilobytes of data

00:25:22,640 --> 00:25:26,640
and exit immediately after the right the

00:25:25,520 --> 00:25:30,720
parent does not

00:25:26,640 --> 00:25:31,039
intercept the data in its completion so

00:25:30,720 --> 00:25:33,440
some

00:25:31,039 --> 00:25:35,039
amount of data gets truncated that's the

00:25:33,440 --> 00:25:37,760
issue

00:25:35,039 --> 00:25:39,279
now as we know from one of the previous

00:25:37,760 --> 00:25:40,799
topics

00:25:39,279 --> 00:25:42,640
that the parent and the child

00:25:40,799 --> 00:25:45,360
communicate through a pipe

00:25:42,640 --> 00:25:47,520
and the pipes are subjected to buffering

00:25:45,360 --> 00:25:50,320
and chunking

00:25:47,520 --> 00:25:51,039
and there is a max buffer parameter that

00:25:50,320 --> 00:25:53,360
defines

00:25:51,039 --> 00:25:55,279
how much data could be allowed through

00:25:53,360 --> 00:25:58,240
the channel in one shot

00:25:55,279 --> 00:25:59,279
a trivial suspect is the value of this

00:25:58,240 --> 00:26:01,760
max buffer

00:25:59,279 --> 00:26:03,600
we increased this but did not see any

00:26:01,760 --> 00:26:04,640
result any positive result that's coming

00:26:03,600 --> 00:26:07,760
out

00:26:04,640 --> 00:26:10,960
we tried testing in different platforms

00:26:07,760 --> 00:26:14,080
uh that basically exhibits a different

00:26:10,960 --> 00:26:14,480
pipe behavior but that was also not

00:26:14,080 --> 00:26:17,600
giving

00:26:14,480 --> 00:26:17,600
any clue as such

00:26:19,200 --> 00:26:23,279
now changing the data volume had a

00:26:22,159 --> 00:26:25,440
visible effect

00:26:23,279 --> 00:26:26,480
and more importantly each time we were

00:26:25,440 --> 00:26:29,360
getting different

00:26:26,480 --> 00:26:30,640
amounts of data so clearly there was

00:26:29,360 --> 00:26:33,120
some race condition

00:26:30,640 --> 00:26:33,840
but between which entities so the

00:26:33,120 --> 00:26:36,960
sequence

00:26:33,840 --> 00:26:39,440
that writes the data to the parent

00:26:36,960 --> 00:26:41,279
is one sequence and the sequence that

00:26:39,440 --> 00:26:43,679
wants to exit the process

00:26:41,279 --> 00:26:44,960
is the second product second sequence so

00:26:43,679 --> 00:26:46,400
clearly there is a raised condition

00:26:44,960 --> 00:26:49,440
between these two sequences

00:26:46,400 --> 00:26:50,159
and which one is able to progress

00:26:49,440 --> 00:26:52,640
further

00:26:50,159 --> 00:26:53,440
based on that the other one was lagging

00:26:52,640 --> 00:26:56,559
behind

00:26:53,440 --> 00:26:58,480
so based on cpu cycles based on

00:26:56,559 --> 00:26:59,840
operating system scheduling and based on

00:26:58,480 --> 00:27:03,039
various other

00:26:59,840 --> 00:27:06,000
um control flow related uh

00:27:03,039 --> 00:27:07,039
sequencing the data truncation was

00:27:06,000 --> 00:27:10,240
varying

00:27:07,039 --> 00:27:10,240
in a variety of manner

00:27:10,480 --> 00:27:14,559
but uh why wouldn't the process exit uh

00:27:13,120 --> 00:27:17,039
wait for the data right

00:27:14,559 --> 00:27:18,080
to complete that's where the crux of the

00:27:17,039 --> 00:27:21,279
problem is

00:27:18,080 --> 00:27:22,960
so the console.log kpi which was used to

00:27:21,279 --> 00:27:26,240
write the huge chunk of data

00:27:22,960 --> 00:27:28,880
is asynchronous by nature in node.js

00:27:26,240 --> 00:27:30,159
this means if you present a large amount

00:27:28,880 --> 00:27:34,399
of data to it

00:27:30,159 --> 00:27:34,399
only a chunk that is that volume

00:27:34,480 --> 00:27:38,640
which is you know bare minimum which the

00:27:37,679 --> 00:27:42,399
pipe can hold

00:27:38,640 --> 00:27:45,279
is written in the first place and then

00:27:42,399 --> 00:27:46,320
the rest of the data is scheduled for

00:27:45,279 --> 00:27:48,320
further writing

00:27:46,320 --> 00:27:51,440
when the pipe is drained completely and

00:27:48,320 --> 00:27:53,840
is ready to write again

00:27:51,440 --> 00:27:54,880
what if an exit is pending on the

00:27:53,840 --> 00:27:58,159
process

00:27:54,880 --> 00:28:01,600
is this data rewritten no

00:27:58,159 --> 00:28:02,320
the data is the data right activity is

00:28:01,600 --> 00:28:05,279
completely

00:28:02,320 --> 00:28:06,240
abandoned and a pending exit is

00:28:05,279 --> 00:28:10,000
processed first

00:28:06,240 --> 00:28:13,600
and here is this is actually the issue

00:28:10,000 --> 00:28:16,720
now uh what is the

00:28:13,600 --> 00:28:18,720
solution the solution would be not to

00:28:16,720 --> 00:28:20,960
use the

00:28:18,720 --> 00:28:22,240
console.log api which is asynchronous in

00:28:20,960 --> 00:28:24,480
nature

00:28:22,240 --> 00:28:25,600
uh so one of the non-workaround is to

00:28:24,480 --> 00:28:28,320
use process dot

00:28:25,600 --> 00:28:29,600
stdot write api as opposed to the

00:28:28,320 --> 00:28:32,880
control dot lip load

00:28:29,600 --> 00:28:35,760
api and process dot std dot

00:28:32,880 --> 00:28:36,960
s3 outdoor api is a blocking api so it

00:28:35,760 --> 00:28:40,000
holds the process

00:28:36,960 --> 00:28:43,440
on a blocking state until the whole data

00:28:40,000 --> 00:28:46,480
is written so that

00:28:43,440 --> 00:28:48,640
is essentially the known program but

00:28:46,480 --> 00:28:49,840
fundamentally for the console.log being

00:28:48,640 --> 00:28:52,880
asynchronous

00:28:49,840 --> 00:28:56,320
and at the exit point if there are data

00:28:52,880 --> 00:28:59,120
that's pending uh will get truncated

00:28:56,320 --> 00:29:00,159
does not have a comprehensive solution

00:28:59,120 --> 00:29:02,159
as of now

00:29:00,159 --> 00:29:03,919
and that means we do have a number of

00:29:02,159 --> 00:29:07,360
issues opened on that

00:29:03,919 --> 00:29:10,559
i don't think we we will have a

00:29:07,360 --> 00:29:11,440
design level proper fix for that in the

00:29:10,559 --> 00:29:15,039
near future

00:29:11,440 --> 00:29:17,120
but we do have a stable workaround

00:29:15,039 --> 00:29:18,399
so that's pretty much it is i hope you

00:29:17,120 --> 00:29:20,399
enjoyed this talk

00:29:18,399 --> 00:29:22,559
i want to conclude the talk by saying

00:29:20,399 --> 00:29:23,520
these are some of the examples of issues

00:29:22,559 --> 00:29:26,000
where we spend

00:29:23,520 --> 00:29:27,600
considerable amount of time debugging

00:29:26,000 --> 00:29:30,320
the knowledges and tunnels

00:29:27,600 --> 00:29:31,440
and gain vital insights around how

00:29:30,320 --> 00:29:33,840
things work

00:29:31,440 --> 00:29:35,520
plus usage of a variety of tools and

00:29:33,840 --> 00:29:37,360
techniques and methodologies

00:29:35,520 --> 00:29:38,960
i would like to strongly recommend folks

00:29:37,360 --> 00:29:40,240
who are interested in getting further

00:29:38,960 --> 00:29:42,960
details on this

00:29:40,240 --> 00:29:44,720
or how to use some of the tools or even

00:29:42,960 --> 00:29:45,600
to get started with contributing to

00:29:44,720 --> 00:29:48,000
node.js

00:29:45,600 --> 00:29:50,159
and start debugging you are welcome to

00:29:48,000 --> 00:29:50,880
write to me or any of my fellow project

00:29:50,159 --> 00:29:53,520
members

00:29:50,880 --> 00:29:54,080
we are more than happy to assist you

00:29:53,520 --> 00:29:55,679
there are

00:29:54,080 --> 00:29:57,279
enough issues in the backlog at the

00:29:55,679 --> 00:29:58,880
moment that are waiting to be

00:29:57,279 --> 00:30:00,480
hand-picked so

00:29:58,880 --> 00:30:01,919
you wouldn't be disappointed at all i'm

00:30:00,480 --> 00:30:03,600
sure on that once again

00:30:01,919 --> 00:30:06,720
thank you very much for listening to

00:30:03,600 --> 00:30:06,720

YouTube URL: https://www.youtube.com/watch?v=c_jemhuFeJc


