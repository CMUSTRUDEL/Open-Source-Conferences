Title: Observing Node.js: Using Metrics to Improve your Application Performance - Guilherme Hermeto
Publication date: 2021-05-28
Playlist: OpenJS World 2021
Description: 
	We all want to build apps that scale well, and at the same time, we need to keep the costs down. When you are scaling apps to hundreds of containers, you want to make sure you are making the best use of each instance and continually improving your app performance. Every second and in every request, your Node.js service can give you essential information on how well it performs, and you can use that data to build better applications. In this talk, you will learn which metrics you should have in place and how to use them to improve your code, creating a feedback loop that will help you continuously get the best of your Node.js application.
Captions: 
	00:00:00,719 --> 00:00:06,720
hello my name is guillermetto

00:00:03,439 --> 00:00:08,240
and the stock is observing node.js using

00:00:06,720 --> 00:00:09,840
metrics to improve your application

00:00:08,240 --> 00:00:11,040
performance

00:00:09,840 --> 00:00:13,280
there are many ways to improve

00:00:11,040 --> 00:00:16,000
application performance

00:00:13,280 --> 00:00:16,800
the one way we'll focus here today in

00:00:16,000 --> 00:00:19,920
this talk

00:00:16,800 --> 00:00:21,600
is how to use metrics to understand how

00:00:19,920 --> 00:00:22,880
your code impacts your application

00:00:21,600 --> 00:00:25,279
health

00:00:22,880 --> 00:00:27,199
and how can you create a feedback loop

00:00:25,279 --> 00:00:30,560
to evolve your coding style

00:00:27,199 --> 00:00:31,599
and create better applications before we

00:00:30,560 --> 00:00:34,880
start

00:00:31,599 --> 00:00:34,880
let me introduce myself

00:00:35,280 --> 00:00:40,160
i'm a senior platform engineer with the

00:00:37,360 --> 00:00:42,320
node.js platform team and netflix

00:00:40,160 --> 00:00:43,280
we run an internal serverless node.js

00:00:42,320 --> 00:00:46,079
platform

00:00:43,280 --> 00:00:49,600
that powers the device's user interfaces

00:00:46,079 --> 00:00:52,800
and auis for content production efforts

00:00:49,600 --> 00:00:54,879
for any platform changes we run canaris

00:00:52,800 --> 00:00:57,680
and watch the metrics to make sure the

00:00:54,879 --> 00:00:59,680
platform remains healthy

00:00:57,680 --> 00:01:00,719
and because of that i have a lot of

00:00:59,680 --> 00:01:02,640
experience

00:01:00,719 --> 00:01:04,159
using what i call metrics driven

00:01:02,640 --> 00:01:07,119
development

00:01:04,159 --> 00:01:07,520
to constantly evolve my code and not

00:01:07,119 --> 00:01:10,840
have

00:01:07,520 --> 00:01:12,720
a negative impact on the platform

00:01:10,840 --> 00:01:15,360
metrics

00:01:12,720 --> 00:01:16,640
it creates this feedback loop you write

00:01:15,360 --> 00:01:19,520
code

00:01:16,640 --> 00:01:20,880
you run a load or a canary you gather

00:01:19,520 --> 00:01:23,040
the metrics

00:01:20,880 --> 00:01:24,720
then you analyze and evaluate if you

00:01:23,040 --> 00:01:26,960
need start over

00:01:24,720 --> 00:01:28,799
and this should work for any language

00:01:26,960 --> 00:01:32,159
the metrics you watch will be different

00:01:28,799 --> 00:01:32,159
but the process is the same

00:01:32,799 --> 00:01:40,799
well it's easy for me to say use metrics

00:01:37,280 --> 00:01:43,680
but which ones there are so many of them

00:01:40,799 --> 00:01:44,560
and to be honest it's not immediately

00:01:43,680 --> 00:01:47,840
obvious

00:01:44,560 --> 00:01:48,320
how to use them and often you need to

00:01:47,840 --> 00:01:51,040
use

00:01:48,320 --> 00:01:52,000
a combination of two or more metrics to

00:01:51,040 --> 00:01:56,399
understand

00:01:52,000 --> 00:01:56,399
what you do with your application health

00:01:56,640 --> 00:02:00,960
let me start introducing you to your new

00:01:59,360 --> 00:02:04,079
best friend

00:02:00,960 --> 00:02:07,280
process cpu utilization

00:02:04,079 --> 00:02:08,080
it is the first indicator of application

00:02:07,280 --> 00:02:11,120
health

00:02:08,080 --> 00:02:14,000
because it's a sensitive metric

00:02:11,120 --> 00:02:16,640
high cpu utilization can be caused by

00:02:14,000 --> 00:02:20,080
many pathologies

00:02:16,640 --> 00:02:24,080
synchronous code blocking the event loop

00:02:20,080 --> 00:02:26,959
process restarts that's interesting

00:02:24,080 --> 00:02:28,959
when the process restarts it usually

00:02:26,959 --> 00:02:32,400
spikes the cpu

00:02:28,959 --> 00:02:35,440
so looking at a chart of your fleet

00:02:32,400 --> 00:02:37,760
and seeing high cpu it actually

00:02:35,440 --> 00:02:40,480
might mean that some of the insults

00:02:37,760 --> 00:02:42,400
might be restarting

00:02:40,480 --> 00:02:45,040
it could also be pressure on the garbage

00:02:42,400 --> 00:02:46,560
collector if scavenger is working a lot

00:02:45,040 --> 00:02:49,920
for some reason

00:02:46,560 --> 00:02:52,640
maybe a high promotion rate cpu usage

00:02:49,920 --> 00:02:52,640
will increase

00:02:52,959 --> 00:02:59,040
the event loop lag can indicate

00:02:56,879 --> 00:03:00,560
there is an unhealthy amount of

00:02:59,040 --> 00:03:03,840
synchronous code

00:03:00,560 --> 00:03:04,319
being executed it's not the easiest

00:03:03,840 --> 00:03:07,360
chart

00:03:04,319 --> 00:03:09,519
to analyze so that's why we

00:03:07,360 --> 00:03:11,200
use it in conjunction with the cpu

00:03:09,519 --> 00:03:14,480
utilization metric

00:03:11,200 --> 00:03:15,440
it makes easier to digest if these

00:03:14,480 --> 00:03:19,519
charts

00:03:15,440 --> 00:03:22,879
start moving up on the y-axis

00:03:19,519 --> 00:03:25,120
it's probably bad news it means that

00:03:22,879 --> 00:03:27,360
you take longer longer for the next

00:03:25,120 --> 00:03:29,519
event loop tick

00:03:27,360 --> 00:03:33,840
which also means that the request rate

00:03:29,519 --> 00:03:33,840
will drop

00:03:34,239 --> 00:03:40,720
heap utilization the good old heap

00:03:38,080 --> 00:03:43,760
if it's growing on the y-axis it means

00:03:40,720 --> 00:03:46,159
you probably have a memory leak

00:03:43,760 --> 00:03:48,400
it usually takes a while to manifest so

00:03:46,159 --> 00:03:50,239
it's not that easy to catch

00:03:48,400 --> 00:03:52,480
unless you're monitoring and alerting

00:03:50,239 --> 00:03:54,319
your metrics

00:03:52,480 --> 00:03:56,000
and we will understand the issue that

00:03:54,319 --> 00:03:58,840
caused the chart you see

00:03:56,000 --> 00:04:00,480
right now on your screen in a few

00:03:58,840 --> 00:04:04,080
minutes

00:04:00,480 --> 00:04:05,120
request rate this is all about how many

00:04:04,080 --> 00:04:08,560
requests

00:04:05,120 --> 00:04:11,040
your app can receive in a period of time

00:04:08,560 --> 00:04:13,599
well it's no surprise that we like to

00:04:11,040 --> 00:04:18,400
bucket it in one second intervals

00:04:13,599 --> 00:04:21,759
which give us requests per second or rts

00:04:18,400 --> 00:04:25,440
you can also add useful labels or tags

00:04:21,759 --> 00:04:26,800
like status codes or path it will allow

00:04:25,440 --> 00:04:29,120
us to filter

00:04:26,800 --> 00:04:30,320
or create aggregations by that given

00:04:29,120 --> 00:04:33,360
label

00:04:30,320 --> 00:04:34,000
so as an example we can see how many

00:04:33,360 --> 00:04:36,320
requests are

00:04:34,000 --> 00:04:37,440
done 200 how many requests are returned

00:04:36,320 --> 00:04:40,160
00:04:37,440 --> 00:04:40,880
and how many requests are returning any

00:04:40,160 --> 00:04:44,720
other such

00:04:40,880 --> 00:04:47,040
code this is a must

00:04:44,720 --> 00:04:48,800
have for any applications running

00:04:47,040 --> 00:04:50,400
production

00:04:48,800 --> 00:04:52,639
it's important to notice that this

00:04:50,400 --> 00:04:55,759
metric also affects

00:04:52,639 --> 00:04:56,800
cpu utilization an increase on this

00:04:55,759 --> 00:04:59,280
metric

00:04:56,800 --> 00:05:01,280
it will effectively means there is

00:04:59,280 --> 00:05:05,120
increase in cpu utilization

00:05:01,280 --> 00:05:08,240
and it it is expected more requests

00:05:05,120 --> 00:05:11,360
means that they're gonna need more cpu

00:05:08,240 --> 00:05:14,560
and because it's directly related

00:05:11,360 --> 00:05:17,199
it means that to get more requests

00:05:14,560 --> 00:05:19,039
you're gonna need more cpu which means

00:05:17,199 --> 00:05:21,680
that eventually you're gonna need more

00:05:19,039 --> 00:05:21,680
instances

00:05:22,639 --> 00:05:30,800
request latency well this is a good one

00:05:27,520 --> 00:05:31,919
so many people think immediately on this

00:05:30,800 --> 00:05:33,680
metric

00:05:31,919 --> 00:05:35,680
when we talk about application

00:05:33,680 --> 00:05:38,240
performance that's what they understand

00:05:35,680 --> 00:05:40,560
by application performance

00:05:38,240 --> 00:05:41,360
what this method is going to tell you is

00:05:40,560 --> 00:05:43,840
how long

00:05:41,360 --> 00:05:45,600
your requests are taking which

00:05:43,840 --> 00:05:47,600
eventually can translate how long your

00:05:45,600 --> 00:05:50,240
customers are waiting

00:05:47,600 --> 00:05:51,280
it is important that this metric is

00:05:50,240 --> 00:05:53,919
bucketed

00:05:51,280 --> 00:05:55,919
into percentiles because you want to

00:05:53,919 --> 00:05:57,120
know how long your average request is

00:05:55,919 --> 00:05:59,520
taking

00:05:57,120 --> 00:06:01,199
and how long your longer requests are

00:05:59,520 --> 00:06:02,880
taking

00:06:01,199 --> 00:06:06,080
right the ones on the nine nine

00:06:02,880 --> 00:06:09,840
percentile the 99.9 percentile

00:06:06,080 --> 00:06:09,840
you want to know the outlier

00:06:09,919 --> 00:06:13,199
in-flight requests that's an interesting

00:06:12,240 --> 00:06:15,360
one

00:06:13,199 --> 00:06:18,639
so this metric measures how many

00:06:15,360 --> 00:06:20,720
concurrent requests your app is handling

00:06:18,639 --> 00:06:22,160
not how many requests your app is

00:06:20,720 --> 00:06:25,039
receiving

00:06:22,160 --> 00:06:26,240
requests being handled are requests that

00:06:25,039 --> 00:06:28,639
have been received

00:06:26,240 --> 00:06:30,319
the queued from the task queue and are

00:06:28,639 --> 00:06:31,840
being processed by your application

00:06:30,319 --> 00:06:34,319
server

00:06:31,840 --> 00:06:35,120
it's also very useful metric to indicate

00:06:34,319 --> 00:06:37,440
when you have an

00:06:35,120 --> 00:06:38,319
handler or middleware leak because if

00:06:37,440 --> 00:06:40,479
you do

00:06:38,319 --> 00:06:42,960
it means your application might not be

00:06:40,479 --> 00:06:44,800
sending the responses to the user

00:06:42,960 --> 00:06:47,120
which means the request will eventually

00:06:44,800 --> 00:06:50,160
time out

00:06:47,120 --> 00:06:51,840
process restarts oh there is no

00:06:50,160 --> 00:06:53,599
controversy there

00:06:51,840 --> 00:06:55,039
if your application is crashing you need

00:06:53,599 --> 00:06:58,160
to know about it

00:06:55,039 --> 00:06:58,560
period and it's pretty easy to look at

00:06:58,160 --> 00:07:00,160
it

00:06:58,560 --> 00:07:01,759
and figure out you have a bug on your

00:07:00,160 --> 00:07:05,039
code

00:07:01,759 --> 00:07:07,520
but not always which bug

00:07:05,039 --> 00:07:09,759
so to debug the issues you have to

00:07:07,520 --> 00:07:12,720
resort to diagnostic tools like

00:07:09,759 --> 00:07:15,120
logging tracing and even analyzing core

00:07:12,720 --> 00:07:18,000
dumps

00:07:15,120 --> 00:07:18,479
you're probably thinking wow that's

00:07:18,000 --> 00:07:22,479
awesome

00:07:18,479 --> 00:07:24,800
i learned so much but now what

00:07:22,479 --> 00:07:27,120
now i'll show you a few examples based

00:07:24,800 --> 00:07:29,120
on real cases i saw in production

00:07:27,120 --> 00:07:30,479
and show you how can they affect your

00:07:29,120 --> 00:07:33,280
metrics

00:07:30,479 --> 00:07:34,000
to showcase that i created a simple

00:07:33,280 --> 00:07:36,880
application

00:07:34,000 --> 00:07:38,960
using only open source software which

00:07:36,880 --> 00:07:41,039
you can find on github

00:07:38,960 --> 00:07:43,199
that's the repo you see right there on

00:07:41,039 --> 00:07:45,199
your screen

00:07:43,199 --> 00:07:46,800
it has a script that to set up

00:07:45,199 --> 00:07:51,280
prometheus grafana

00:07:46,800 --> 00:07:54,720
and postgres using docker desktop

00:07:51,280 --> 00:07:56,479
okay so let's do this the first example

00:07:54,720 --> 00:07:58,479
will show you how to identify

00:07:56,479 --> 00:08:01,599
performance problems they are not

00:07:58,479 --> 00:08:03,919
caused by a bug per se but by

00:08:01,599 --> 00:08:05,360
coding style that's not suitable for

00:08:03,919 --> 00:08:07,120
node.js

00:08:05,360 --> 00:08:09,759
even though it might be fine for other

00:08:07,120 --> 00:08:12,639
languages it's not for node

00:08:09,759 --> 00:08:15,919
and it does not crashes the app it's

00:08:12,639 --> 00:08:15,919
just very inefficient

00:08:16,319 --> 00:08:22,000
this is our route handler file

00:08:19,680 --> 00:08:23,280
and in this file we have a percentile

00:08:22,000 --> 00:08:27,360
class

00:08:23,280 --> 00:08:30,720
that has a static builder

00:08:27,360 --> 00:08:34,159
and returns this builder class

00:08:30,720 --> 00:08:36,640
here this builder class

00:08:34,159 --> 00:08:39,120
it's a classic builder that you would

00:08:36,640 --> 00:08:42,320
see in a language like java that has

00:08:39,120 --> 00:08:47,440
chainable methods and

00:08:42,320 --> 00:08:50,720
a build method and eventually we return

00:08:47,440 --> 00:08:53,760
pre-configured percentile

00:08:50,720 --> 00:08:57,519
and then here below we have

00:08:53,760 --> 00:08:58,959
this route handler that you know uses

00:08:57,519 --> 00:09:03,519
the builder

00:08:58,959 --> 00:09:07,279
to build this percentile

00:09:03,519 --> 00:09:11,360
and eventually uses the percentile

00:09:07,279 --> 00:09:14,880
and then finally returns

00:09:11,360 --> 00:09:19,760
okay now if you look at the

00:09:14,880 --> 00:09:19,760
charts for the metrics

00:09:21,200 --> 00:09:29,440
okay when i start to load test this

00:09:25,200 --> 00:09:32,560
i started with a baseline

00:09:29,440 --> 00:09:36,720
being loaded with about 4k

00:09:32,560 --> 00:09:40,160
requests this baseline route

00:09:36,720 --> 00:09:43,200
is an echo that just returns the request

00:09:40,160 --> 00:09:46,560
headers back and

00:09:43,200 --> 00:09:50,560
um you can see that's pretty stable

00:09:46,560 --> 00:09:53,200
here in this location i

00:09:50,560 --> 00:09:54,640
start the call to that particular

00:09:53,200 --> 00:09:57,920
endpoint

00:09:54,640 --> 00:09:59,279
and you see that i added a little bit

00:09:57,920 --> 00:10:01,440
more than 10 percent

00:09:59,279 --> 00:10:03,680
uh when i did the load i added 10

00:10:01,440 --> 00:10:06,320
percent

00:10:03,680 --> 00:10:07,120
you can see a little bit more here but

00:10:06,320 --> 00:10:11,680
you're gonna see

00:10:07,120 --> 00:10:14,560
a 25 increasing cpu

00:10:11,680 --> 00:10:15,040
we don't know if that is reasonable or

00:10:14,560 --> 00:10:17,120
not

00:10:15,040 --> 00:10:18,560
because if you have a route that doesn't

00:10:17,120 --> 00:10:20,880
do anything

00:10:18,560 --> 00:10:23,440
and then you start loading a route that

00:10:20,880 --> 00:10:26,320
has business logic

00:10:23,440 --> 00:10:26,640
you're bounded to have more cpu right so

00:10:26,320 --> 00:10:29,680
you

00:10:26,640 --> 00:10:34,160
you really don't know if that

00:10:29,680 --> 00:10:37,519
is correct or not that reasonable or not

00:10:34,160 --> 00:10:37,519
at that particular point

00:10:37,920 --> 00:10:42,640
you can see here that latency is a

00:10:40,320 --> 00:10:47,440
stable

00:10:42,640 --> 00:10:51,279
all true so that's not a problem

00:10:47,440 --> 00:10:54,880
you see that you have more

00:10:51,279 --> 00:10:58,240
event loop lag but it is expected that

00:10:54,880 --> 00:10:58,720
you know if you increase uh dcp you use

00:10:58,240 --> 00:11:01,600
a

00:10:58,720 --> 00:11:05,040
synchronous code you are expected to

00:11:01,600 --> 00:11:08,560
have increase on event loop black

00:11:05,040 --> 00:11:11,839
here you also see that you can trace

00:11:08,560 --> 00:11:15,839
the heap pretty much horizontally

00:11:11,839 --> 00:11:15,839
it doesn't deviate too much

00:11:16,000 --> 00:11:20,560
same with the flight requests you have a

00:11:18,720 --> 00:11:23,600
little bump here but

00:11:20,560 --> 00:11:24,480
that's normal that's nothing if you look

00:11:23,600 --> 00:11:28,160
at the

00:11:24,480 --> 00:11:31,680
scavenger the miner dc you see that

00:11:28,160 --> 00:11:35,839
it kind of follows the

00:11:31,680 --> 00:11:39,279
request rate so this is expected

00:11:35,839 --> 00:11:42,720
okay um going

00:11:39,279 --> 00:11:46,240
here what i did was like okay

00:11:42,720 --> 00:11:46,800
so 10 percent good uh let me see what

00:11:46,240 --> 00:11:50,880
happens

00:11:46,800 --> 00:11:53,839
if i you know shift all the traffic

00:11:50,880 --> 00:11:54,959
from all the load that was happening on

00:11:53,839 --> 00:11:57,519
the baseline

00:11:54,959 --> 00:11:58,000
into that endpoint that's what i did

00:11:57,519 --> 00:12:01,279
here

00:11:58,000 --> 00:12:05,200
right there was a 60 percent

00:12:01,279 --> 00:12:08,560
um increase on the cpu

00:12:05,200 --> 00:12:11,600
at this point from the original

00:12:08,560 --> 00:12:15,839
baseline one

00:12:11,600 --> 00:12:18,880
and you see here nothing really changes

00:12:15,839 --> 00:12:23,279
anywhere okay

00:12:18,880 --> 00:12:23,279
so let's go back

00:12:23,600 --> 00:12:29,360
so here i made a small change it's the

00:12:27,519 --> 00:12:32,399
exact same code

00:12:29,360 --> 00:12:33,360
that the order right under the other

00:12:32,399 --> 00:12:36,320
file

00:12:33,360 --> 00:12:37,279
but in this case i'm defining the

00:12:36,320 --> 00:12:41,519
builder

00:12:37,279 --> 00:12:44,560
as a top level in this module

00:12:41,519 --> 00:12:45,440
and um you see here that i just returned

00:12:44,560 --> 00:12:48,240
the builder

00:12:45,440 --> 00:12:50,800
the builder references the person tile

00:12:48,240 --> 00:12:54,240
the percentile references the builder

00:12:50,800 --> 00:12:57,440
and that's okay we have this getter here

00:12:54,240 --> 00:13:00,240
and you see that is the exact same

00:12:57,440 --> 00:13:00,240
route handler

00:13:01,279 --> 00:13:09,279
at this point here i shifted the traffic

00:13:05,680 --> 00:13:12,880
to this new endpoint so at this point

00:13:09,279 --> 00:13:15,600
100 of the traffic start going

00:13:12,880 --> 00:13:16,720
to the new endpoint where the class is

00:13:15,600 --> 00:13:20,240
defined

00:13:16,720 --> 00:13:23,839
on the top level and um

00:13:20,240 --> 00:13:26,880
you see here that if you average

00:13:23,839 --> 00:13:29,040
this it's gonna be like a five

00:13:26,880 --> 00:13:30,000
percent increase four or five percent

00:13:29,040 --> 00:13:33,040
increase

00:13:30,000 --> 00:13:36,160
uh over the original one

00:13:33,040 --> 00:13:39,760
instead of a 60 increase

00:13:36,160 --> 00:13:43,279
it is a very meaningful change

00:13:39,760 --> 00:13:43,600
right and the performance it can show

00:13:43,279 --> 00:13:47,040
you

00:13:43,600 --> 00:13:49,680
that if you don't code in a way that's

00:13:47,040 --> 00:13:54,399
suitable for node.js

00:13:49,680 --> 00:13:54,399
you're going to have performancy issues

00:13:54,880 --> 00:14:02,320
in this case i have

00:13:58,000 --> 00:14:06,240
got a non-controversial hcp client

00:14:02,320 --> 00:14:09,920
fetching a large json

00:14:06,240 --> 00:14:13,519
from this github

00:14:09,920 --> 00:14:16,800
repo and it

00:14:13,519 --> 00:14:19,760
calls it every single time

00:14:16,800 --> 00:14:21,279
in the request path right let's go with

00:14:19,760 --> 00:14:24,320
it for now

00:14:21,279 --> 00:14:26,240
so if you look here

00:14:24,320 --> 00:14:27,600
this is again the same baseline that i

00:14:26,240 --> 00:14:30,560
had before i

00:14:27,600 --> 00:14:31,279
caused the echo route that just returns

00:14:30,560 --> 00:14:35,040
the red

00:14:31,279 --> 00:14:36,800
headers and here i start the call to

00:14:35,040 --> 00:14:39,839
this headline

00:14:36,800 --> 00:14:43,680
you can see that immediately

00:14:39,839 --> 00:14:46,880
the cpu starts to spike

00:14:43,680 --> 00:14:49,920
and it goes up to 150

00:14:46,880 --> 00:14:50,880
and that is what top was showing you can

00:14:49,920 --> 00:14:54,079
see that

00:14:50,880 --> 00:14:55,279
the event loop lag it grows the number

00:14:54,079 --> 00:14:58,560
of requests

00:14:55,279 --> 00:15:02,320
you see here it decreases

00:14:58,560 --> 00:15:06,079
and you see here it's a good example

00:15:02,320 --> 00:15:09,680
that shows that the 99.9 percentile

00:15:06,079 --> 00:15:12,560
starts pulling up and then it pulls the

00:15:09,680 --> 00:15:15,199
99 percentile and

00:15:12,560 --> 00:15:16,320
if the processing had crashed at this

00:15:15,199 --> 00:15:19,360
point here

00:15:16,320 --> 00:15:22,639
that's a crash that's actually

00:15:19,360 --> 00:15:25,199
where it became irresponsive but if

00:15:22,639 --> 00:15:25,760
that would have happened you would see

00:15:25,199 --> 00:15:28,959
that the

00:15:25,760 --> 00:15:31,839
95 percentile would grow up and then you

00:15:28,959 --> 00:15:34,240
would see the 75 percentile goal

00:15:31,839 --> 00:15:34,959
but in this case here it just became

00:15:34,240 --> 00:15:38,160
responsive

00:15:34,959 --> 00:15:42,240
and crashed so you see here

00:15:38,160 --> 00:15:45,199
that the hip usage is

00:15:42,240 --> 00:15:46,320
way higher than it was before and

00:15:45,199 --> 00:15:49,040
because

00:15:46,320 --> 00:15:52,320
requests are taking longer and longer

00:15:49,040 --> 00:15:55,600
than flight request is growing

00:15:52,320 --> 00:15:57,279
right you also see big bump on scavenger

00:15:55,600 --> 00:16:00,000
but that is

00:15:57,279 --> 00:16:00,880
like completely in line with the cpu

00:16:00,000 --> 00:16:04,800
usage

00:16:00,880 --> 00:16:07,839
right so that's to show you

00:16:04,800 --> 00:16:11,680
how cpu

00:16:07,839 --> 00:16:12,000
will cause problems if your instance is

00:16:11,680 --> 00:16:15,040
not

00:16:12,000 --> 00:16:17,839
responding it puts a lot of pressure on

00:16:15,040 --> 00:16:21,040
the other instances on the fleet

00:16:17,839 --> 00:16:24,880
and so on and so on and that becomes

00:16:21,040 --> 00:16:25,519
a snowball right so if you want to fix

00:16:24,880 --> 00:16:29,279
this

00:16:25,519 --> 00:16:32,160
well and this one is kind of obvious

00:16:29,279 --> 00:16:33,600
so if you want to fix that you don't

00:16:32,160 --> 00:16:36,240
want to call it

00:16:33,600 --> 00:16:37,519
every time in the request lifecycle the

00:16:36,240 --> 00:16:39,839
json is a static

00:16:37,519 --> 00:16:40,880
it's not going to change and let's

00:16:39,839 --> 00:16:43,199
suppose that

00:16:40,880 --> 00:16:44,560
it's changed it's so big that you

00:16:43,199 --> 00:16:45,680
probably want to have some type of

00:16:44,560 --> 00:16:48,880
caching here

00:16:45,680 --> 00:16:52,639
right so if you fetch one time

00:16:48,880 --> 00:16:55,440
the next maybe five a hundred thousand

00:16:52,639 --> 00:16:56,000
requests it comes from the cash then

00:16:55,440 --> 00:16:59,920
even

00:16:56,000 --> 00:17:02,320
if you're cashing for just a few seconds

00:16:59,920 --> 00:17:03,279
it's enough for you to reduce

00:17:02,320 --> 00:17:06,400
drastically

00:17:03,279 --> 00:17:06,400
the loads that you have

00:17:06,640 --> 00:17:10,240
to understand this example i would like

00:17:09,360 --> 00:17:12,959
you to

00:17:10,240 --> 00:17:14,480
understand this registry class which is

00:17:12,959 --> 00:17:17,520
a event emitter

00:17:14,480 --> 00:17:19,600
so it has the other methods from an

00:17:17,520 --> 00:17:22,240
event emitter it extends it

00:17:19,600 --> 00:17:23,760
this registry it's something that's

00:17:22,240 --> 00:17:25,600
going to batch it's like

00:17:23,760 --> 00:17:27,120
we can create an object that's going to

00:17:25,600 --> 00:17:30,240
match uh

00:17:27,120 --> 00:17:33,679
sending metrics to a remote server right

00:17:30,240 --> 00:17:36,160
it's an example it has a frequency

00:17:33,679 --> 00:17:37,600
and then when you start it it creates

00:17:36,160 --> 00:17:40,400
this interval

00:17:37,600 --> 00:17:41,760
that on that frequency it's going to

00:17:40,400 --> 00:17:44,240
publish

00:17:41,760 --> 00:17:45,520
those metrics so you can register the

00:17:44,240 --> 00:17:49,280
metrics

00:17:45,520 --> 00:17:52,240
and then to publish it the metrics

00:17:49,280 --> 00:17:53,200
an object that has a data getter and

00:17:52,240 --> 00:17:57,280
then

00:17:53,200 --> 00:17:59,280
new map and then we send it right

00:17:57,280 --> 00:18:00,880
it's important to understand one thing

00:17:59,280 --> 00:18:04,720
as a design decision

00:18:00,880 --> 00:18:08,080
on this class the registry decided to

00:18:04,720 --> 00:18:11,200
emit a data sent event here

00:18:08,080 --> 00:18:14,880
when it publishes okay that's

00:18:11,200 --> 00:18:18,400
it so here on this handle

00:18:14,880 --> 00:18:20,480
file we have a counter metric

00:18:18,400 --> 00:18:21,840
right the counter metric takes the

00:18:20,480 --> 00:18:24,400
registry

00:18:21,840 --> 00:18:26,000
and then it registers itself on the

00:18:24,400 --> 00:18:29,120
constructor

00:18:26,000 --> 00:18:32,799
and this is the important part

00:18:29,120 --> 00:18:36,000
it will add a listener to the event

00:18:32,799 --> 00:18:38,640
data send to prune its own data right

00:18:36,000 --> 00:18:39,600
and start fresh every time it sends to

00:18:38,640 --> 00:18:42,960
remote

00:18:39,600 --> 00:18:46,320
have no reason to keep accumulating more

00:18:42,960 --> 00:18:49,120
so here we have an example of

00:18:46,320 --> 00:18:50,480
metrics middleware and in this matrix

00:18:49,120 --> 00:18:53,679
middleware

00:18:50,480 --> 00:18:57,840
we get a path and with the path we

00:18:53,679 --> 00:18:57,840
will create a new counter metric

00:19:01,919 --> 00:19:05,200
okay so here i start with the same

00:19:04,400 --> 00:19:08,320
baseline

00:19:05,200 --> 00:19:12,320
the baseline that has the echo

00:19:08,320 --> 00:19:13,760
handler and it just returns the request

00:19:12,320 --> 00:19:16,880
headers

00:19:13,760 --> 00:19:20,320
then immediately here i start a

00:19:16,880 --> 00:19:21,600
10 increase you can see here that's a

00:19:20,320 --> 00:19:25,280
little bit more but

00:19:21,600 --> 00:19:25,520
it is actually what i told autocannon to

00:19:25,280 --> 00:19:27,520
do

00:19:25,520 --> 00:19:29,280
start a 10 percent increase on those

00:19:27,520 --> 00:19:31,760
routes

00:19:29,280 --> 00:19:33,200
and on that this particular events

00:19:31,760 --> 00:19:36,480
endpoint

00:19:33,200 --> 00:19:37,600
and you see that it's the cpu usage

00:19:36,480 --> 00:19:40,720
starts to grow

00:19:37,600 --> 00:19:43,679
it doesn't grow absolutely but

00:19:40,720 --> 00:19:45,200
yeah i just let it for a few hours

00:19:43,679 --> 00:19:48,160
running

00:19:45,200 --> 00:19:49,120
you can see that it grows but it's not

00:19:48,160 --> 00:19:52,240
too much

00:19:49,120 --> 00:19:55,120
the event look lag a little bit more but

00:19:52,240 --> 00:19:56,000
nothing noticeable you can see something

00:19:55,120 --> 00:19:59,919
weird though

00:19:56,000 --> 00:20:04,320
you see the 99.9 percentile here

00:19:59,919 --> 00:20:08,559
going up and here even when i stop it

00:20:04,320 --> 00:20:11,840
it kept going up right so

00:20:08,559 --> 00:20:14,240
let's try to get to the bottom of this

00:20:11,840 --> 00:20:15,760
and i think i found it right like it's

00:20:14,240 --> 00:20:19,200
definitely not

00:20:15,760 --> 00:20:22,240
the flight request because it seems like

00:20:19,200 --> 00:20:25,360
it's just a horizontal line here

00:20:22,240 --> 00:20:28,480
it doesn't deviate but

00:20:25,360 --> 00:20:32,080
if you look at here from uh

00:20:28,480 --> 00:20:34,880
this point is about 33 megabytes

00:20:32,080 --> 00:20:35,679
on the hip and on the top of this bike

00:20:34,880 --> 00:20:39,120
here

00:20:35,679 --> 00:20:42,480
it's about 1.35

00:20:39,120 --> 00:20:43,360
gigabytes this is a memory leak clearly

00:20:42,480 --> 00:20:46,080
happening

00:20:43,360 --> 00:20:47,039
right here and this is going to keep

00:20:46,080 --> 00:20:51,520
affecting

00:20:47,039 --> 00:20:51,520
my 99.9 percentile here

00:20:52,799 --> 00:20:58,640
so let's go back to the code and

00:20:56,080 --> 00:21:00,080
see one of the things that you could

00:20:58,640 --> 00:21:04,080
have done here

00:21:00,080 --> 00:21:07,200
is well instead of letting

00:21:04,080 --> 00:21:10,320
the metric itself listen to

00:21:07,200 --> 00:21:11,120
that event being emitted and pruning its

00:21:10,320 --> 00:21:13,200
own data

00:21:11,120 --> 00:21:15,039
and the registry here instead of

00:21:13,200 --> 00:21:18,480
emitting this event

00:21:15,039 --> 00:21:18,480
it will actually call the prune

00:21:19,520 --> 00:21:26,320
okay so in this last example

00:21:22,720 --> 00:21:30,000
let's look at leaky promises

00:21:26,320 --> 00:21:33,120
and how they affect our metrics

00:21:30,000 --> 00:21:36,480
so here i have this nice

00:21:33,120 --> 00:21:40,080
module that allows me to

00:21:36,480 --> 00:21:40,559
create and work that create a pull and

00:21:40,080 --> 00:21:44,000
work

00:21:40,559 --> 00:21:47,120
databases and so i create a bow i create

00:21:44,000 --> 00:21:50,559
a query and here

00:21:47,120 --> 00:21:53,760
i will do a connect inside my

00:21:50,559 --> 00:21:54,559
handle my route handler uh disconnects

00:21:53,760 --> 00:21:58,400
takes a

00:21:54,559 --> 00:22:01,440
sync function and here i will just

00:21:58,400 --> 00:22:02,720
do a query that's going to query all the

00:22:01,440 --> 00:22:05,280
u.s states from

00:22:02,720 --> 00:22:06,400
the possibilities database then i'll

00:22:05,280 --> 00:22:10,240
return it

00:22:06,400 --> 00:22:11,679
and i'll call next right so let's see

00:22:10,240 --> 00:22:15,039
how

00:22:11,679 --> 00:22:15,039
this works the metrics

00:22:16,559 --> 00:22:24,000
okay so here

00:22:20,320 --> 00:22:27,120
again we have the baseline calling the

00:22:24,000 --> 00:22:31,120
echo about 4k

00:22:27,120 --> 00:22:35,200
uh rps and here

00:22:31,120 --> 00:22:38,480
i start calling 10 increase

00:22:35,200 --> 00:22:39,200
to the promises db endpoint you see

00:22:38,480 --> 00:22:43,120
there's a

00:22:39,200 --> 00:22:46,400
huge spike on cpu um so

00:22:43,120 --> 00:22:48,880
this is actually not a concern because

00:22:46,400 --> 00:22:49,919
db access is something that's going to

00:22:48,880 --> 00:22:53,440
be slow

00:22:49,919 --> 00:22:56,799
most of the cases and the other route

00:22:53,440 --> 00:22:59,840
the echo route wasn't doing anything so

00:22:56,799 --> 00:23:03,039
yeah you're gonna see a lot of uh

00:22:59,840 --> 00:23:06,640
cpu utilization at this point it

00:23:03,039 --> 00:23:09,440
doesn't really it's not the real concern

00:23:06,640 --> 00:23:12,159
at this point ideally you want to reuse

00:23:09,440 --> 00:23:15,679
the connection pool

00:23:12,159 --> 00:23:18,159
uh the event look lag the heap

00:23:15,679 --> 00:23:21,679
it's all good uh you're gonna see that

00:23:18,159 --> 00:23:25,120
the 99.9 percentile keeps going up

00:23:21,679 --> 00:23:25,679
so you probably want to look at other

00:23:25,120 --> 00:23:28,240
metrics

00:23:25,679 --> 00:23:29,360
and then you finally see it when you

00:23:28,240 --> 00:23:32,799
start

00:23:29,360 --> 00:23:35,840
the in-flight request starts to leak

00:23:32,799 --> 00:23:38,960
and what does that mean that

00:23:35,840 --> 00:23:42,400
basically the promises are not

00:23:38,960 --> 00:23:45,039
resolving or they are

00:23:42,400 --> 00:23:47,120
rejecting at that point and when they're

00:23:45,039 --> 00:23:49,440
rejecting means that you're not sending

00:23:47,120 --> 00:23:52,799
the response right the rest not json

00:23:49,440 --> 00:23:56,159
if you're going back look here

00:23:52,799 --> 00:23:59,440
it doesn't get here it's just

00:23:56,159 --> 00:24:02,080
something happens i know that because

00:23:59,440 --> 00:24:03,279
that never goes to the end and never

00:24:02,080 --> 00:24:06,240
sends the response

00:24:03,279 --> 00:24:06,640
my flight to request starts to leak so

00:24:06,240 --> 00:24:08,720
this

00:24:06,640 --> 00:24:10,720
shows how in-flight requests are a good

00:24:08,720 --> 00:24:14,000
indicator when we have

00:24:10,720 --> 00:24:17,200
those kind of a leaky things happening

00:24:14,000 --> 00:24:21,039
at this point here i stopped the calls

00:24:17,200 --> 00:24:24,320
right you see going back to the whole

00:24:21,039 --> 00:24:27,440
echo endpoint right 100

00:24:24,320 --> 00:24:30,159
and then here i

00:24:27,440 --> 00:24:32,240
start to call the other route let me go

00:24:30,159 --> 00:24:35,600
and show you the code

00:24:32,240 --> 00:24:38,960
that one is the exact same code the only

00:24:35,600 --> 00:24:42,159
difference is that i move the next

00:24:38,960 --> 00:24:45,360
and i add a catch here

00:24:42,159 --> 00:24:49,039
so what happens like

00:24:45,360 --> 00:24:49,360
why it's because well we're not caching

00:24:49,039 --> 00:24:53,520
it

00:24:49,360 --> 00:24:54,720
and because i was using a promise and i

00:24:53,520 --> 00:24:58,080
was not

00:24:54,720 --> 00:25:00,480
judging it i was never finishing

00:24:58,080 --> 00:25:01,120
that request is never properly finishing

00:25:00,480 --> 00:25:04,720
it

00:25:01,120 --> 00:25:07,919
now it is so if you look here

00:25:04,720 --> 00:25:11,120
okay it's a huge usage even

00:25:07,919 --> 00:25:14,480
higher than before but that's okay

00:25:11,120 --> 00:25:16,000
right that is expected what happens here

00:25:14,480 --> 00:25:16,480
the difference that you're going to see

00:25:16,000 --> 00:25:20,000
here

00:25:16,480 --> 00:25:23,919
is that now you have stable

00:25:20,000 --> 00:25:25,840
in-flight requests right

00:25:23,919 --> 00:25:28,400
now you know a little more how to use

00:25:25,840 --> 00:25:28,960
metrics to improve your code use your

00:25:28,400 --> 00:25:31,440
judgment

00:25:28,960 --> 00:25:32,080
in every commit are the changes in the

00:25:31,440 --> 00:25:34,960
metrics

00:25:32,080 --> 00:25:37,520
in line with the code you are committing

00:25:34,960 --> 00:25:39,279
what other metrics are important

00:25:37,520 --> 00:25:40,799
there are metrics that i didn't have

00:25:39,279 --> 00:25:43,279
time to cover

00:25:40,799 --> 00:25:45,039
like a gc utilization by type and

00:25:43,279 --> 00:25:47,200
promotion rate

00:25:45,039 --> 00:25:48,240
a little research on that goes a long

00:25:47,200 --> 00:25:50,240
way

00:25:48,240 --> 00:25:53,440
metrics can tell you that your code has

00:25:50,240 --> 00:25:56,080
an issue but not always how to find it

00:25:53,440 --> 00:25:57,840
for that you need to use diagnostic

00:25:56,080 --> 00:26:01,440
tools like flame graphs

00:25:57,840 --> 00:26:04,000
hip profile deceptive tracing logging

00:26:01,440 --> 00:26:04,000
and others

00:26:04,559 --> 00:26:10,159
thank you so much and i hope this talk

00:26:06,640 --> 00:26:10,159

YouTube URL: https://www.youtube.com/watch?v=XokTvf9WlXQ


