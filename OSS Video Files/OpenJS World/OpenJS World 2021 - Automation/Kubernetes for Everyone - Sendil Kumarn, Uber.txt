Title: Kubernetes for Everyone - Sendil Kumarn, Uber
Publication date: 2021-05-31
Playlist: OpenJS World 2021 - Automation
Description: 
	The presentation will be structured as follows: What is the problem with running your services at scale?  The whole process of building, deploying, service discovery, load-balancing, routing, tracing, auth, graceful failures, rate limits, and moreâ€”is cumbersome and involves many moving parts. It is complex and nightmare for anyone running their services at scale. How does K8s help to run the services at scale?  Let us explore how the K8s container management system, can simplify many of the operational challenges and it includes how to deploy NodeJS services on K8s. What is K8s and how does it work? K8s is the de facto standard for running containerized applications. K8s groups containers that make up an application into logical units for easy management and discovery. We will deep dive into how K8s is architected and how makes it easy to run services at scale.
Captions: 
	00:00:00,080 --> 00:00:04,400
hello everyone welcome to openjs world

00:00:02,639 --> 00:00:06,080
good morning good afternoon and good

00:00:04,400 --> 00:00:06,879
evening from wherever you are joining us

00:00:06,080 --> 00:00:09,760
today

00:00:06,879 --> 00:00:10,880
i'm sending a full stack developer and

00:00:09,760 --> 00:00:14,480
you can reach me at

00:00:10,880 --> 00:00:14,480
sundance twitter handle

00:00:15,679 --> 00:00:20,000
i've written a couple of looks uh one on

00:00:18,080 --> 00:00:21,520
uh full stack development the other one

00:00:20,000 --> 00:00:23,439
is on rust and web assembly

00:00:21,520 --> 00:00:25,840
the rust and web assembly is the new one

00:00:23,439 --> 00:00:28,800
which is written in a cookbook from it

00:00:25,840 --> 00:00:30,400
that will help you in your rust and web

00:00:28,800 --> 00:00:31,760
assembly journey if you're

00:00:30,400 --> 00:00:33,600
learning rust and web assembly for the

00:00:31,760 --> 00:00:35,040
first time this group will help you to

00:00:33,600 --> 00:00:36,880
understand

00:00:35,040 --> 00:00:38,399
and you can find the links here below

00:00:36,880 --> 00:00:41,600
and i'll also publish

00:00:38,399 --> 00:00:43,520
the slides later to my talk

00:00:41,600 --> 00:00:45,039
great let's get started this will be an

00:00:43,520 --> 00:00:47,600
introduction to give another stock

00:00:45,039 --> 00:00:49,360
where we will see why we need kubernetes

00:00:47,600 --> 00:00:51,920
what problems does it solve

00:00:49,360 --> 00:00:52,960
where you can run it how it works

00:00:51,920 --> 00:00:56,320
underneath

00:00:52,960 --> 00:00:56,320
so let's get going

00:00:56,960 --> 00:01:00,000
we will start with a simple hello world

00:00:58,719 --> 00:01:02,960
application so it's a simple

00:01:00,000 --> 00:01:06,000
express js application uh which runs on

00:01:02,960 --> 00:01:08,000
port 3000 and exposes the slash endpoint

00:01:06,000 --> 00:01:09,200
on accessing it you'll get a string back

00:01:08,000 --> 00:01:13,600
saying hello

00:01:09,200 --> 00:01:16,159
it's very simple right

00:01:13,600 --> 00:01:17,920
once you start once the application is

00:01:16,159 --> 00:01:19,600
ready now we are thinking about how to

00:01:17,920 --> 00:01:20,720
move that into production so when you

00:01:19,600 --> 00:01:21,360
are thinking about moving into

00:01:20,720 --> 00:01:23,759
production

00:01:21,360 --> 00:01:26,080
then it's something like docker you can

00:01:23,759 --> 00:01:27,759
use to containerize the application and

00:01:26,080 --> 00:01:29,920
then move it to production it kind of

00:01:27,759 --> 00:01:32,400
gives you a lot of advantages right

00:01:29,920 --> 00:01:33,200
so in a docker you get a latest image of

00:01:32,400 --> 00:01:35,520
node

00:01:33,200 --> 00:01:36,880
you do all the required things for

00:01:35,520 --> 00:01:39,280
example npm install

00:01:36,880 --> 00:01:40,799
copying all the files and directories

00:01:39,280 --> 00:01:41,759
and then finally once you've done you're

00:01:40,799 --> 00:01:43,920
exposing a

00:01:41,759 --> 00:01:45,920
port in our case is 3000 and a

00:01:43,920 --> 00:01:48,079
kickstarter processor

00:01:45,920 --> 00:01:50,960
once you've started the process docker

00:01:48,079 --> 00:01:53,840
will run the process

00:01:50,960 --> 00:01:55,280
but it will not manage the process so

00:01:53,840 --> 00:01:58,079
once the process is

00:01:55,280 --> 00:01:58,880
dead because of some reason your nk

00:01:58,079 --> 00:02:00,719
application

00:01:58,880 --> 00:02:02,560
will not be accessible so docker will

00:02:00,719 --> 00:02:04,640
not control that on the other hand

00:02:02,560 --> 00:02:06,079
docker gives the portability and also it

00:02:04,640 --> 00:02:08,800
completely abstracts away

00:02:06,079 --> 00:02:10,399
operating system and our runtime trigger

00:02:08,800 --> 00:02:12,879
related stuff

00:02:10,399 --> 00:02:14,560
because you need not worry about it as

00:02:12,879 --> 00:02:16,480
long as you have docker installed it

00:02:14,560 --> 00:02:17,120
might be any machine or any operating

00:02:16,480 --> 00:02:18,720
system

00:02:17,120 --> 00:02:20,480
you can actually run your application

00:02:18,720 --> 00:02:23,520
inside as a process

00:02:20,480 --> 00:02:25,200
that's really awesome right and then

00:02:23,520 --> 00:02:26,720
once you go into production you

00:02:25,200 --> 00:02:27,520
obviously will have to scale your

00:02:26,720 --> 00:02:28,879
applications

00:02:27,520 --> 00:02:30,480
if there are a number of users

00:02:28,879 --> 00:02:32,160
increasing and once you're trying to

00:02:30,480 --> 00:02:34,160
scale the application

00:02:32,160 --> 00:02:35,760
at the obvious next step is putting a

00:02:34,160 --> 00:02:37,840
load balancer on top of it and then

00:02:35,760 --> 00:02:39,519
create a multiple containers the docker

00:02:37,840 --> 00:02:40,400
containers can be running in multiple

00:02:39,519 --> 00:02:43,040
parallely

00:02:40,400 --> 00:02:44,720
would a load balancer direct the traffic

00:02:43,040 --> 00:02:46,400
based on the resourcing or

00:02:44,720 --> 00:02:48,080
round robin or any method that you

00:02:46,400 --> 00:02:50,800
choose load balancers

00:02:48,080 --> 00:02:52,480
itself is a complex mechanism but there

00:02:50,800 --> 00:02:53,840
are a lot of tools that are available

00:02:52,480 --> 00:02:55,519
that makes it easier

00:02:53,840 --> 00:02:57,120
to build a load balancer outside of your

00:02:55,519 --> 00:02:58,159
system and then control everything

00:02:57,120 --> 00:03:00,080
together

00:02:58,159 --> 00:03:01,599
but over the course of project we found

00:03:00,080 --> 00:03:03,120
out there are two different things that

00:03:01,599 --> 00:03:04,560
we missed out and we create two

00:03:03,120 --> 00:03:08,080
different services for them

00:03:04,560 --> 00:03:10,319
in our case hello galaxy andromeda

00:03:08,080 --> 00:03:11,599
these two services exactly similar

00:03:10,319 --> 00:03:14,879
express.js function

00:03:11,599 --> 00:03:16,640
except for like some differences between

00:03:14,879 --> 00:03:18,400
galaxy and andromeda

00:03:16,640 --> 00:03:19,840
so once you have done this we could

00:03:18,400 --> 00:03:22,640
follow the same pattern

00:03:19,840 --> 00:03:23,840
create a docker image for that run it in

00:03:22,640 --> 00:03:26,640
a docker containers

00:03:23,840 --> 00:03:28,480
multiple containers scale it across and

00:03:26,640 --> 00:03:29,920
then put a load balancer on top

00:03:28,480 --> 00:03:32,400
through which all the connections will

00:03:29,920 --> 00:03:35,599
happen this is fairly straightforward

00:03:32,400 --> 00:03:36,319
but these services are not depending on

00:03:35,599 --> 00:03:39,760
each other

00:03:36,319 --> 00:03:41,920
they're completely segregated right

00:03:39,760 --> 00:03:43,200
but what if you want to connect these

00:03:41,920 --> 00:03:44,840
services together

00:03:43,200 --> 00:03:47,120
how do you how do you make the

00:03:44,840 --> 00:03:50,560
communication with them happen

00:03:47,120 --> 00:03:51,120
for that um it's a negative example if

00:03:50,560 --> 00:03:53,040
you take

00:03:51,120 --> 00:03:55,120
for example if hello world wants to talk

00:03:53,040 --> 00:03:58,000
with hello andromeda

00:03:55,120 --> 00:04:00,239
you have to understand the ip address or

00:03:58,000 --> 00:04:01,200
you have to know the ib address of hello

00:04:00,239 --> 00:04:02,879
enter menu

00:04:01,200 --> 00:04:04,879
you issue a request to that color

00:04:02,879 --> 00:04:06,959
andromeda and then enter meta will

00:04:04,879 --> 00:04:09,439
give a response you get that response

00:04:06,959 --> 00:04:13,599
and serve it back to your customers

00:04:09,439 --> 00:04:16,479
this is how it should work but

00:04:13,599 --> 00:04:18,720
doing that in a manual way is very

00:04:16,479 --> 00:04:20,000
inefficient and also takes a lot of time

00:04:18,720 --> 00:04:22,479
for us to do

00:04:20,000 --> 00:04:24,639
for example if for any reason we have

00:04:22,479 --> 00:04:27,440
changed the iphone 7

00:04:24,639 --> 00:04:29,360
up then you have to undeploy your hello

00:04:27,440 --> 00:04:31,680
world and then deploy it again

00:04:29,360 --> 00:04:32,960
with a new upgraded config so they are

00:04:31,680 --> 00:04:35,280
kind of interdependent

00:04:32,960 --> 00:04:36,400
with each other so we in the world of

00:04:35,280 --> 00:04:38,240
microservices

00:04:36,400 --> 00:04:39,759
or whenever you talk about microservices

00:04:38,240 --> 00:04:42,880
you want to isolate these

00:04:39,759 --> 00:04:44,400
services completely to get outside so

00:04:42,880 --> 00:04:46,479
you have to pull them apart

00:04:44,400 --> 00:04:47,520
make sure there is no interconnection

00:04:46,479 --> 00:04:49,520
between them and then

00:04:47,520 --> 00:04:51,440
deliver their masterpine services that's

00:04:49,520 --> 00:04:54,800
how you have to do it right

00:04:51,440 --> 00:04:59,120
so so in the world of microservices

00:04:54,800 --> 00:04:59,120
all the three services are completely

00:04:59,199 --> 00:05:05,120
independent and they are abstracted into

00:05:03,280 --> 00:05:07,039
a single instance right now so

00:05:05,120 --> 00:05:08,160
there can be hundreds of instances of

00:05:07,039 --> 00:05:10,320
hello galaxy

00:05:08,160 --> 00:05:11,440
running but you can abstract them as a

00:05:10,320 --> 00:05:14,960
single instance

00:05:11,440 --> 00:05:15,919
so that's how uh the microservices has

00:05:14,960 --> 00:05:18,560
to be architected

00:05:15,919 --> 00:05:19,919
in a better way right so the obvious

00:05:18,560 --> 00:05:23,039
question is how do

00:05:19,919 --> 00:05:24,560
how can we enable the communication

00:05:23,039 --> 00:05:26,560
between them

00:05:24,560 --> 00:05:28,160
so we can put in something like uh

00:05:26,560 --> 00:05:29,440
service to history so the role of

00:05:28,160 --> 00:05:32,160
service registry is

00:05:29,440 --> 00:05:34,000
very simple whenever a service boots up

00:05:32,160 --> 00:05:34,400
it goes to service industry and says

00:05:34,000 --> 00:05:37,680
like

00:05:34,400 --> 00:05:40,400
i'm booting up please uh it's

00:05:37,680 --> 00:05:41,600
have my ip address if anybody wants to

00:05:40,400 --> 00:05:44,000
connect with me

00:05:41,600 --> 00:05:46,000
just keep them beside your dress so they

00:05:44,000 --> 00:05:48,240
can connect with me straight away

00:05:46,000 --> 00:05:49,759
that's awesome right so the service

00:05:48,240 --> 00:05:50,479
registry will be a highly available

00:05:49,759 --> 00:05:53,039
service

00:05:50,479 --> 00:05:54,160
and if going there but it adds a bit of

00:05:53,039 --> 00:05:56,639
latency because

00:05:54,160 --> 00:05:58,080
every time when uh andromeda wants to

00:05:56,639 --> 00:06:00,400
connect with hello world

00:05:58,080 --> 00:06:01,360
it has to go to service registry get

00:06:00,400 --> 00:06:05,120
information

00:06:01,360 --> 00:06:06,960
and then go back that's a bit of

00:06:05,120 --> 00:06:09,280
work around but we can have a different

00:06:06,960 --> 00:06:11,280
workarounds for that for example

00:06:09,280 --> 00:06:12,479
have a service agents running on

00:06:11,280 --> 00:06:15,199
different services

00:06:12,479 --> 00:06:15,520
to make sure we are not always doing

00:06:15,199 --> 00:06:17,919
this

00:06:15,520 --> 00:06:18,639
two-way communication rather have an

00:06:17,919 --> 00:06:22,720
updated

00:06:18,639 --> 00:06:24,960
node always but you can solve them

00:06:22,720 --> 00:06:26,720
but the other problem that we are going

00:06:24,960 --> 00:06:29,600
to have here is like when services

00:06:26,720 --> 00:06:30,639
goes out of life like if something dies

00:06:29,600 --> 00:06:32,720
out of blue

00:06:30,639 --> 00:06:34,000
the service registry will not have any

00:06:32,720 --> 00:06:36,400
clue about it

00:06:34,000 --> 00:06:37,759
in order to have that we have to have

00:06:36,400 --> 00:06:38,880
another service which is called as

00:06:37,759 --> 00:06:40,960
health checker

00:06:38,880 --> 00:06:42,080
the responsibility of health checker is

00:06:40,960 --> 00:06:46,160
very simple

00:06:42,080 --> 00:06:47,440
it takes in all the information sorry it

00:06:46,160 --> 00:06:48,720
takes in the heartbeat from the

00:06:47,440 --> 00:06:50,960
different services

00:06:48,720 --> 00:06:52,160
and then says like whether services are

00:06:50,960 --> 00:06:54,400
live or not

00:06:52,160 --> 00:06:56,240
whenever a service is not sending a

00:06:54,400 --> 00:06:58,720
heartbeat or any information

00:06:56,240 --> 00:07:00,080
it just marks that service isn't not

00:06:58,720 --> 00:07:02,560
alive and removes that

00:07:00,080 --> 00:07:04,479
entry from service to history so no new

00:07:02,560 --> 00:07:07,599
services or new service requests

00:07:04,479 --> 00:07:09,840
will go to that particular endpoint

00:07:07,599 --> 00:07:12,160
that's great but pen service dice you

00:07:09,840 --> 00:07:13,599
obviously need to have a mechanism to

00:07:12,160 --> 00:07:16,880
restart them

00:07:13,599 --> 00:07:18,560
and also to replicate them and for these

00:07:16,880 --> 00:07:20,240
two functionalities

00:07:18,560 --> 00:07:22,479
you might need a couple of services

00:07:20,240 --> 00:07:24,240
which kind of looks sits and sees what

00:07:22,479 --> 00:07:24,720
is happening inside my microservices

00:07:24,240 --> 00:07:27,120
world

00:07:24,720 --> 00:07:29,039
and how can i change them and you need

00:07:27,120 --> 00:07:32,240
to have this mechanism

00:07:29,039 --> 00:07:34,400
and once you have done those things all

00:07:32,240 --> 00:07:36,319
the things are living as a containerized

00:07:34,400 --> 00:07:37,919
application so you need a docker image

00:07:36,319 --> 00:07:38,240
from where you can pull the image and

00:07:37,919 --> 00:07:40,560
then

00:07:38,240 --> 00:07:42,080
run your application so there has to be

00:07:40,560 --> 00:07:44,479
something like a docker

00:07:42,080 --> 00:07:46,000
repository or any medium that made up

00:07:44,479 --> 00:07:49,199
that could help you to

00:07:46,000 --> 00:07:51,039
download the images and then run them as

00:07:49,199 --> 00:07:52,800
containers

00:07:51,039 --> 00:07:54,720
and then you need to have authentication

00:07:52,800 --> 00:07:56,319
and authorization between the breakfast

00:07:54,720 --> 00:07:57,840
when you're communicating you have to

00:07:56,319 --> 00:07:59,520
authorize certain things and

00:07:57,840 --> 00:08:01,919
authenticate certain things

00:07:59,520 --> 00:08:03,440
based on the rules you have to have a

00:08:01,919 --> 00:08:05,520
mechanism in which like

00:08:03,440 --> 00:08:07,039
allowing retries whenever an area

00:08:05,520 --> 00:08:09,759
happens and also

00:08:07,039 --> 00:08:10,560
stopping resource hungry long polling of

00:08:09,759 --> 00:08:14,000
processes

00:08:10,560 --> 00:08:16,639
using something like timeout so

00:08:14,000 --> 00:08:18,560
overall you might need some kind of a

00:08:16,639 --> 00:08:19,599
setting that lives outside of your

00:08:18,560 --> 00:08:21,680
services

00:08:19,599 --> 00:08:22,800
and that could control how your services

00:08:21,680 --> 00:08:25,120
communicate

00:08:22,800 --> 00:08:26,240
talk with each other and respond to each

00:08:25,120 --> 00:08:27,919
other so that is

00:08:26,240 --> 00:08:29,680
the main intention of having these

00:08:27,919 --> 00:08:31,840
services right

00:08:29,680 --> 00:08:33,599
cool we can have a couple of service

00:08:31,840 --> 00:08:33,919
that kind of lives outside and make sure

00:08:33,599 --> 00:08:36,479
that

00:08:33,919 --> 00:08:37,440
everything is connected towards that and

00:08:36,479 --> 00:08:39,360
then finally

00:08:37,440 --> 00:08:41,519
we have to have a robust monitoring

00:08:39,360 --> 00:08:42,959
system and also some dashboards

00:08:41,519 --> 00:08:44,800
that is combined with the health

00:08:42,959 --> 00:08:47,200
checkers will actually help you

00:08:44,800 --> 00:08:48,959
help you a lot over here because they

00:08:47,200 --> 00:08:51,360
receive the heartbeats

00:08:48,959 --> 00:08:52,320
from these services so you can use those

00:08:51,360 --> 00:08:54,560
information

00:08:52,320 --> 00:08:57,360
and then create certain things out of it

00:08:54,560 --> 00:08:58,560
so create some graphs or visualizations

00:08:57,360 --> 00:09:00,640
out of it

00:08:58,560 --> 00:09:02,399
and then you need to have tracing for

00:09:00,640 --> 00:09:04,000
your services to identify

00:09:02,399 --> 00:09:06,480
where the problem actually is when

00:09:04,000 --> 00:09:09,200
something happens and also debug those

00:09:06,480 --> 00:09:11,519
problems find a way to

00:09:09,200 --> 00:09:13,440
pinpoint a bottleneck in your services

00:09:11,519 --> 00:09:15,040
and then try to fix that

00:09:13,440 --> 00:09:17,600
over a long run if you have more

00:09:15,040 --> 00:09:18,480
services like for example more than 10

00:09:17,600 --> 00:09:20,080
services

00:09:18,480 --> 00:09:22,000
and hundreds of thousands of instances

00:09:20,080 --> 00:09:23,839
of them running then tracing

00:09:22,000 --> 00:09:25,920
and debugging will actually help you a

00:09:23,839 --> 00:09:29,839
lot in rolling out the new features and

00:09:25,920 --> 00:09:29,839
understanding how your resources work

00:09:30,959 --> 00:09:35,360
so you remove all the three things that

00:09:33,040 --> 00:09:37,200
we basically started where our entire

00:09:35,360 --> 00:09:38,959
business logic will live if you remove

00:09:37,200 --> 00:09:40,480
all these three things

00:09:38,959 --> 00:09:42,320
all the other things that you're seeing

00:09:40,480 --> 00:09:44,480
in this picture except for the

00:09:42,320 --> 00:09:46,080
except for the grayed out ones are

00:09:44,480 --> 00:09:47,600
highly available service even your

00:09:46,080 --> 00:09:48,800
application has to be highly available

00:09:47,600 --> 00:09:50,480
when it's your code

00:09:48,800 --> 00:09:52,560
but all the other things are like

00:09:50,480 --> 00:09:54,399
generic boilerplate code which

00:09:52,560 --> 00:09:56,399
has to be highly available and you have

00:09:54,399 --> 00:09:58,080
to maintain and you're going to spend a

00:09:56,399 --> 00:09:59,839
lot of time in doing that

00:09:58,080 --> 00:10:01,279
and that's where cumulative actually

00:09:59,839 --> 00:10:03,120
comes into picture

00:10:01,279 --> 00:10:04,720
kubernetes is an open source project

00:10:03,120 --> 00:10:06,240
from google of course

00:10:04,720 --> 00:10:07,839
and it helps you to automate the

00:10:06,240 --> 00:10:09,360
deployment scaling

00:10:07,839 --> 00:10:11,440
and also management of different

00:10:09,360 --> 00:10:13,519
applications and all these applications

00:10:11,440 --> 00:10:15,519
has to be containerized it has to run in

00:10:13,519 --> 00:10:17,760
some sort of docker environment or some

00:10:15,519 --> 00:10:19,680
sort of container as can be involved

00:10:17,760 --> 00:10:23,200
but all the applications that kubernetes

00:10:19,680 --> 00:10:25,360
can handle or the containerized ones

00:10:23,200 --> 00:10:27,120
so let's get started by understanding

00:10:25,360 --> 00:10:28,880
the building blocks of cubicles in the

00:10:27,120 --> 00:10:31,839
given list world we call them as

00:10:28,880 --> 00:10:32,560
like the object model so the cumulative

00:10:31,839 --> 00:10:34,160
objects

00:10:32,560 --> 00:10:36,240
so let's go and see like one of the

00:10:34,160 --> 00:10:38,640
common objects that we will use more

00:10:36,240 --> 00:10:38,640
often

00:10:38,959 --> 00:10:43,760
in anything inside the equipment display

00:10:41,360 --> 00:10:46,000
we will just describe it in the form of

00:10:43,760 --> 00:10:47,519
animal valve so you provide an aml file

00:10:46,000 --> 00:10:50,160
to the kubernetes and this

00:10:47,519 --> 00:10:52,560
aml file will clearly specify the

00:10:50,160 --> 00:10:54,480
running state of the application so

00:10:52,560 --> 00:10:56,079
like for example if you have some ml

00:10:54,480 --> 00:10:57,519
files you can just go ahead and take a

00:10:56,079 --> 00:10:58,880
look at it and it will

00:10:57,519 --> 00:11:00,720
if you have some kubernetes

00:10:58,880 --> 00:11:02,000
configuration files which is written in

00:11:00,720 --> 00:11:03,680
yaml of course

00:11:02,000 --> 00:11:05,519
uh so you can just go ahead and take

00:11:03,680 --> 00:11:07,680
that file and see like

00:11:05,519 --> 00:11:09,839
what is currently running in the system

00:11:07,680 --> 00:11:12,720
how many replications are running

00:11:09,839 --> 00:11:13,200
how many deployments are there and what

00:11:12,720 --> 00:11:15,440
does

00:11:13,200 --> 00:11:17,360
uh what is the problem that we are

00:11:15,440 --> 00:11:18,800
having with the help of like some probes

00:11:17,360 --> 00:11:21,040
which we will see later

00:11:18,800 --> 00:11:23,040
but those are the things that you can

00:11:21,040 --> 00:11:24,880
infer from the yaml file itself

00:11:23,040 --> 00:11:27,360
it's very clear and crisp that you can

00:11:24,880 --> 00:11:27,360
understand

00:11:27,680 --> 00:11:31,760
so all the ml files will start with an

00:11:29,600 --> 00:11:32,240
api version irrespective of any object

00:11:31,760 --> 00:11:34,560
model

00:11:32,240 --> 00:11:36,880
the ml files will have an api version

00:11:34,560 --> 00:11:39,040
number at the beginning so here we have

00:11:36,880 --> 00:11:41,920
api version as version one

00:11:39,040 --> 00:11:42,720
and then for this particular model we

00:11:41,920 --> 00:11:44,560
have a spec

00:11:42,720 --> 00:11:46,320
inside the spec we actually define

00:11:44,560 --> 00:11:49,519
whatever the things that is required

00:11:46,320 --> 00:11:51,519
it's basically the custom our

00:11:49,519 --> 00:11:53,600
application related stuff will end up

00:11:51,519 --> 00:11:56,079
there so here inside the spec

00:11:53,600 --> 00:11:58,079
i put up seven containers inside the

00:11:56,079 --> 00:11:59,600
containers i have my hello world image

00:11:58,079 --> 00:12:01,440
which we filled previously

00:11:59,600 --> 00:12:03,440
so i'm just gonna use that image and

00:12:01,440 --> 00:12:06,639
then use a container port 80

00:12:03,440 --> 00:12:08,160
to expose these things okay that's

00:12:06,639 --> 00:12:09,600
pretty simple so you have this docker

00:12:08,160 --> 00:12:12,560
image inside

00:12:09,600 --> 00:12:13,440
and then you add in what kind of file it

00:12:12,560 --> 00:12:15,360
basically is

00:12:13,440 --> 00:12:16,480
so the time basically here determines

00:12:15,360 --> 00:12:18,720
the cumulus object

00:12:16,480 --> 00:12:20,800
here in this case you're defining it as

00:12:18,720 --> 00:12:24,079
a spot so what is a bond

00:12:20,800 --> 00:12:26,560
what is a container

00:12:24,079 --> 00:12:27,519
that contains more than one docker

00:12:26,560 --> 00:12:30,079
content

00:12:27,519 --> 00:12:32,160
you can have one or more than one docker

00:12:30,079 --> 00:12:34,800
container inside it so it's a grouping

00:12:32,160 --> 00:12:36,000
of containers that's where the name port

00:12:34,800 --> 00:12:37,680
comes from

00:12:36,000 --> 00:12:39,680
and kubernetes cluster knows how to

00:12:37,680 --> 00:12:41,440
manage the lifetime of these spots

00:12:39,680 --> 00:12:43,600
so what it does is like it goes in a

00:12:41,440 --> 00:12:45,440
three stage process in the beginning it

00:12:43,600 --> 00:12:48,240
starts the waiting process

00:12:45,440 --> 00:12:48,639
where it sits and pulls the image it

00:12:48,240 --> 00:12:50,959
does

00:12:48,639 --> 00:12:52,000
all the necessary mandatory work before

00:12:50,959 --> 00:12:54,560
running this images

00:12:52,000 --> 00:12:57,120
it does everything together and then

00:12:54,560 --> 00:13:00,079
assigns an ip address to that particular

00:12:57,120 --> 00:13:01,600
part and then it also has something

00:13:00,079 --> 00:13:03,360
called a straightness probe

00:13:01,600 --> 00:13:04,880
so the readiness probe is nothing but

00:13:03,360 --> 00:13:05,279
like it's a probe that you're gonna

00:13:04,880 --> 00:13:07,040
check

00:13:05,279 --> 00:13:08,320
every now and then to make sure that

00:13:07,040 --> 00:13:11,360
your application is ready

00:13:08,320 --> 00:13:12,800
for accepting incoming traffic and here

00:13:11,360 --> 00:13:13,680
you're saying execute a particular

00:13:12,800 --> 00:13:15,760
command

00:13:13,680 --> 00:13:17,680
and wait for first five seconds because

00:13:15,760 --> 00:13:19,200
my i have to connect with my database i

00:13:17,680 --> 00:13:21,279
have to apply certain secrets

00:13:19,200 --> 00:13:22,560
and all those things but after that for

00:13:21,279 --> 00:13:26,639
every five seconds

00:13:22,560 --> 00:13:29,440
make sure that my application is ready

00:13:26,639 --> 00:13:31,279
once it is ready it starts it moves the

00:13:29,440 --> 00:13:32,800
application into running state

00:13:31,279 --> 00:13:34,399
and in the running state you have

00:13:32,800 --> 00:13:36,720
something called a slightness probe

00:13:34,399 --> 00:13:37,760
and liars probe is very similar to

00:13:36,720 --> 00:13:39,360
readiness probe

00:13:37,760 --> 00:13:42,000
but on the other hand it checks whether

00:13:39,360 --> 00:13:45,120
the application is live and it can

00:13:42,000 --> 00:13:47,199
it can accept the new connection

00:13:45,120 --> 00:13:48,880
once everything is done your application

00:13:47,199 --> 00:13:51,519
goes into a terminated state

00:13:48,880 --> 00:13:52,560
where the application is existed exited

00:13:51,519 --> 00:13:55,440
and enclosed

00:13:52,560 --> 00:13:56,720
here cumulus actually uh waits if if

00:13:55,440 --> 00:13:59,519
exit happens

00:13:56,720 --> 00:14:00,240
naturally uh i mean like if the exit

00:13:59,519 --> 00:14:02,399
happens

00:14:00,240 --> 00:14:03,519
intentionally cumulus will just leave it

00:14:02,399 --> 00:14:06,000
out but if the

00:14:03,519 --> 00:14:07,279
exit is happening because of some errors

00:14:06,000 --> 00:14:10,240
or because of something else

00:14:07,279 --> 00:14:10,639
humans will try to reinstantiate the pod

00:14:10,240 --> 00:14:14,480
and then

00:14:10,639 --> 00:14:14,480
goes through the same life cycle again

00:14:14,720 --> 00:14:18,959
and also every kubernetes object that

00:14:17,760 --> 00:14:21,279
you're defining you can

00:14:18,959 --> 00:14:22,720
add a metadata in the metadata you can

00:14:21,279 --> 00:14:25,519
have key value first

00:14:22,720 --> 00:14:27,040
for example here i have a labels inside

00:14:25,519 --> 00:14:30,320
the labels i have app and

00:14:27,040 --> 00:14:33,199
logo so this kind of helps me to have

00:14:30,320 --> 00:14:34,880
one more level of grouping my components

00:14:33,199 --> 00:14:36,639
i can have different parts

00:14:34,880 --> 00:14:39,360
and then group them with different label

00:14:36,639 --> 00:14:41,920
like for example environment production

00:14:39,360 --> 00:14:42,959
and then apply sedan configurations only

00:14:41,920 --> 00:14:45,199
for that group

00:14:42,959 --> 00:14:46,880
so that's where metadata helps you out

00:14:45,199 --> 00:14:49,519
it gives another

00:14:46,880 --> 00:14:51,360
grouping mechanism for you to group

00:14:49,519 --> 00:14:54,240
things together and then deploy them and

00:14:51,360 --> 00:14:54,240
then have a value

00:14:55,519 --> 00:14:59,279
after the balls what do you basically

00:14:57,519 --> 00:15:00,959
see is like how many rep the car support

00:14:59,279 --> 00:15:02,800
can have and that's where replicas that

00:15:00,959 --> 00:15:04,959
comes in the picture so replica said

00:15:02,800 --> 00:15:06,800
it's very simpler you have an api

00:15:04,959 --> 00:15:07,440
version and then the kind as a replica

00:15:06,800 --> 00:15:09,519
set

00:15:07,440 --> 00:15:11,040
and then you put a replicas number of

00:15:09,519 --> 00:15:12,959
replicas that you want to do it

00:15:11,040 --> 00:15:14,959
and cuban this cluster actually manages

00:15:12,959 --> 00:15:15,920
the state make sure that if you're given

00:15:14,959 --> 00:15:19,040
three replicas

00:15:15,920 --> 00:15:21,360
three instances or three containers or

00:15:19,040 --> 00:15:22,000
three pods is basically running inside

00:15:21,360 --> 00:15:24,480
so that

00:15:22,000 --> 00:15:27,839
your application has enough bandwidth to

00:15:24,480 --> 00:15:27,839
accept all the traffic sales

00:15:29,680 --> 00:15:33,680
you can group this deployments and the

00:15:31,839 --> 00:15:36,079
pods oh sorry you can group this

00:15:33,680 --> 00:15:37,839
replica set and the bots together and

00:15:36,079 --> 00:15:39,440
you can call it as deployments

00:15:37,839 --> 00:15:40,959
so deployments actually provide a

00:15:39,440 --> 00:15:43,120
desired state of the application

00:15:40,959 --> 00:15:44,720
it also helps you to do seamless rollout

00:15:43,120 --> 00:15:46,720
and roll back

00:15:44,720 --> 00:15:48,000
together so there might be sending

00:15:46,720 --> 00:15:51,279
components for example

00:15:48,000 --> 00:15:52,000
your your database server and your

00:15:51,279 --> 00:15:54,079
application

00:15:52,000 --> 00:15:55,680
always goes in together so if one of

00:15:54,079 --> 00:15:58,160
them goes out of business you can

00:15:55,680 --> 00:15:58,959
actually delete the other one something

00:15:58,160 --> 00:16:00,480
like that right

00:15:58,959 --> 00:16:02,800
so you can group them together with

00:16:00,480 --> 00:16:04,959
deployments and say like okay so this

00:16:02,800 --> 00:16:06,399
my application and the database server i

00:16:04,959 --> 00:16:07,839
want to fire up the cars

00:16:06,399 --> 00:16:10,079
and then group them together and that's

00:16:07,839 --> 00:16:12,639
what deployment is all about

00:16:10,079 --> 00:16:13,199
kubernetes also provide one more level

00:16:12,639 --> 00:16:15,360
of

00:16:13,199 --> 00:16:17,199
no aggregation which is called as name

00:16:15,360 --> 00:16:18,480
space name space you can consider

00:16:17,199 --> 00:16:20,160
namespace as a folder

00:16:18,480 --> 00:16:21,839
inside which you put all your components

00:16:20,160 --> 00:16:23,680
inside so you have 10 teams

00:16:21,839 --> 00:16:25,680
in your company you can give 10

00:16:23,680 --> 00:16:28,959
different name spaces for these teams

00:16:25,680 --> 00:16:32,240
and then put a pods and deployments

00:16:28,959 --> 00:16:33,839
and and reflect assets whatever you want

00:16:32,240 --> 00:16:37,279
inside the game space

00:16:33,839 --> 00:16:39,839
and then run it together inside here

00:16:37,279 --> 00:16:41,279
so in a more pictorial way you have this

00:16:39,839 --> 00:16:43,600
cubicles cluster

00:16:41,279 --> 00:16:45,759
you can have a name space inside you can

00:16:43,600 --> 00:16:48,079
have multiple name spaces inside

00:16:45,759 --> 00:16:50,800
and then you can inside the name space

00:16:48,079 --> 00:16:52,880
you can have multiple deployments

00:16:50,800 --> 00:16:55,279
inside a deployment you have multiple

00:16:52,880 --> 00:16:56,880
ports inside the box you have to have

00:16:55,279 --> 00:17:00,399
multiple containers so that's how

00:16:56,880 --> 00:17:00,399
you build this is kind of architected

00:17:02,000 --> 00:17:06,000
but you have the sample file now you

00:17:03,920 --> 00:17:07,520
have to run this inside equivalence

00:17:06,000 --> 00:17:09,439
we know how to we know what are the

00:17:07,520 --> 00:17:12,480
basic elements inside ml file

00:17:09,439 --> 00:17:15,439
but how we can run this inside this

00:17:12,480 --> 00:17:17,280
and to see that let's take a division

00:17:15,439 --> 00:17:19,520
and go through its architecture of cuban

00:17:17,280 --> 00:17:21,839
it is how it is actually

00:17:19,520 --> 00:17:23,760
cuban is actually designed in a cluster

00:17:21,839 --> 00:17:26,559
format it's a highly available cluster

00:17:23,760 --> 00:17:27,600
of course it has a control plane that

00:17:26,559 --> 00:17:29,280
kind of controls

00:17:27,600 --> 00:17:32,160
everything that happens in the

00:17:29,280 --> 00:17:34,400
kubernetes by the humanities it is only

00:17:32,160 --> 00:17:35,919
for the equivalent that is i would say

00:17:34,400 --> 00:17:37,440
and then there is a worker node set up

00:17:35,919 --> 00:17:39,039
worker notes where actually your

00:17:37,440 --> 00:17:40,160
application ends up and this is where

00:17:39,039 --> 00:17:42,240
your application will live

00:17:40,160 --> 00:17:45,360
if you're running a container or a port

00:17:42,240 --> 00:17:47,760
it will live in one of your work records

00:17:45,360 --> 00:17:48,880
so basically it's called as brain of the

00:17:47,760 --> 00:17:51,600
equivalence cluster

00:17:48,880 --> 00:17:53,200
or brain of the humanities and then the

00:17:51,600 --> 00:17:54,799
worker nodes is actually where your

00:17:53,200 --> 00:17:56,000
application level and actual work

00:17:54,799 --> 00:17:58,240
happens

00:17:56,000 --> 00:17:59,440
so you have this yaml file you send this

00:17:58,240 --> 00:18:01,600
to control pane

00:17:59,440 --> 00:18:03,679
inside the control plane you have an api

00:18:01,600 --> 00:18:05,280
server that takes this file

00:18:03,679 --> 00:18:06,960
and then validates it whether it's

00:18:05,280 --> 00:18:09,440
perfect well it's correct

00:18:06,960 --> 00:18:11,280
yeah even before that you can send your

00:18:09,440 --> 00:18:13,840
business in three different ways

00:18:11,280 --> 00:18:15,679
one is via cli as you have seen here

00:18:13,840 --> 00:18:17,039
there's a cube ctrl which is a command

00:18:15,679 --> 00:18:20,000
line tool

00:18:17,039 --> 00:18:20,960
so you can use cube ctl create dash of

00:18:20,000 --> 00:18:22,720
actor gamble

00:18:20,960 --> 00:18:25,520
which actually pushes this information

00:18:22,720 --> 00:18:26,320
via cli you can also do it via rest api

00:18:25,520 --> 00:18:29,120
endpoints

00:18:26,320 --> 00:18:30,240
or you can also do with dashboard which

00:18:29,120 --> 00:18:32,960
gives a

00:18:30,240 --> 00:18:34,720
ui for creating these things so it goes

00:18:32,960 --> 00:18:36,000
to api server api server actually

00:18:34,720 --> 00:18:38,160
validates them

00:18:36,000 --> 00:18:39,120
once it is validated it gets the

00:18:38,160 --> 00:18:41,919
information

00:18:39,120 --> 00:18:42,880
from the etcd database so etc database

00:18:41,919 --> 00:18:45,520
is nothing but a

00:18:42,880 --> 00:18:47,679
kubernetes database uh which is a key

00:18:45,520 --> 00:18:50,799
value-based database

00:18:47,679 --> 00:18:52,480
and it stores the cluster information so

00:18:50,799 --> 00:18:55,200
api server will get the customer

00:18:52,480 --> 00:18:56,880
information from the edc data database

00:18:55,200 --> 00:18:58,880
it combines all the information that it

00:18:56,880 --> 00:19:01,280
has the current state

00:18:58,880 --> 00:19:03,200
which is from the adcb database and the

00:19:01,280 --> 00:19:05,600
new configuration that you have added

00:19:03,200 --> 00:19:07,520
it wraps them together plus the no

00:19:05,600 --> 00:19:09,280
details the no dealers here refers to

00:19:07,520 --> 00:19:12,080
the worker node which we will see

00:19:09,280 --> 00:19:13,919
so it passes all those things in and

00:19:12,080 --> 00:19:16,080
then pushes into shader

00:19:13,919 --> 00:19:18,240
shell is a queue it works on first and

00:19:16,080 --> 00:19:18,720
first out whenever something comes into

00:19:18,240 --> 00:19:21,760
it

00:19:18,720 --> 00:19:24,080
it goes ahead and tries to schedule it

00:19:21,760 --> 00:19:25,440
on the work and note before scheduling

00:19:24,080 --> 00:19:27,039
it it has to ensure

00:19:25,440 --> 00:19:29,039
that all the requirement that you've

00:19:27,039 --> 00:19:30,720
asked is actually available in one of

00:19:29,039 --> 00:19:32,720
the worker node for example you can go

00:19:30,720 --> 00:19:34,559
ahead and specify i want this particular

00:19:32,720 --> 00:19:38,000
type of volume for my

00:19:34,559 --> 00:19:40,160
humanoid pod so scheduler takes

00:19:38,000 --> 00:19:41,200
those into category and those into

00:19:40,160 --> 00:19:42,960
consideration

00:19:41,200 --> 00:19:44,559
and then finds a worker node that is

00:19:42,960 --> 00:19:46,640
best suited for the work

00:19:44,559 --> 00:19:48,240
and then calls the work and also it says

00:19:46,640 --> 00:19:50,640
i goes and says

00:19:48,240 --> 00:19:52,480
work or not hey can you start the work

00:19:50,640 --> 00:19:55,600
over here

00:19:52,480 --> 00:19:58,240
so once that happened

00:19:55,600 --> 00:20:00,559
inside the worker node the working load

00:19:58,240 --> 00:20:02,799
is actually made up of container runtime

00:20:00,559 --> 00:20:04,400
and the container runtime is nothing but

00:20:02,799 --> 00:20:06,480
something like docker runtime

00:20:04,400 --> 00:20:07,840
right so it is like if you're running

00:20:06,480 --> 00:20:09,919
your kubernetes cluster

00:20:07,840 --> 00:20:11,840
inside the top of your desktop this

00:20:09,919 --> 00:20:12,559
container runtime is just a docker

00:20:11,840 --> 00:20:15,280
runtime

00:20:12,559 --> 00:20:16,880
it's just kind of an operating system

00:20:15,280 --> 00:20:18,960
kind of you can assume that something

00:20:16,880 --> 00:20:21,440
like that it's kind of a node.js runtime

00:20:18,960 --> 00:20:23,120
or a runtime that is specific for the

00:20:21,440 --> 00:20:25,600
work

00:20:23,120 --> 00:20:27,120
so uh you pass in this few studio

00:20:25,600 --> 00:20:29,120
information to the control plane and the

00:20:27,120 --> 00:20:29,600
control plane will actually forward that

00:20:29,120 --> 00:20:32,240
to

00:20:29,600 --> 00:20:34,080
the vocal node but inside the worker

00:20:32,240 --> 00:20:34,559
node there is another thing called node

00:20:34,080 --> 00:20:37,280
agent

00:20:34,559 --> 00:20:39,039
or a cubelet this is basically as a

00:20:37,280 --> 00:20:39,919
connection between the control plane and

00:20:39,039 --> 00:20:42,720
broken

00:20:39,919 --> 00:20:44,720
so it sends information to the control

00:20:42,720 --> 00:20:46,240
plane as well as it gets the information

00:20:44,720 --> 00:20:48,559
from the control plane

00:20:46,240 --> 00:20:50,480
so once the scheduler says like this

00:20:48,559 --> 00:20:52,799
worker node has a capacity to run

00:20:50,480 --> 00:20:54,640
this requirement it passes that

00:20:52,799 --> 00:20:56,480
information to the node agent which is a

00:20:54,640 --> 00:20:58,080
cubelet and cubelet will proceed with

00:20:56,480 --> 00:21:01,200
the verb

00:20:58,080 --> 00:21:02,880
so going back to the first line once

00:21:01,200 --> 00:21:05,039
start work command is issued to the

00:21:02,880 --> 00:21:06,960
worker node the workload actually does

00:21:05,039 --> 00:21:08,640
the job and then it returns

00:21:06,960 --> 00:21:10,880
even before finishing the job like

00:21:08,640 --> 00:21:13,760
everything is fine i can start my work

00:21:10,880 --> 00:21:15,760
and then say next work has been started

00:21:13,760 --> 00:21:17,360
and that information is passed on to api

00:21:15,760 --> 00:21:18,480
server with an acknowledgement from the

00:21:17,360 --> 00:21:20,640
scheduler

00:21:18,480 --> 00:21:22,640
and once that happened api server go

00:21:20,640 --> 00:21:26,240
ahead and update the new cluster state

00:21:22,640 --> 00:21:26,240
inside the atc database

00:21:26,720 --> 00:21:30,159
apart from all these components control

00:21:29,039 --> 00:21:31,840
plane also had

00:21:30,159 --> 00:21:33,600
some crucial components just called as

00:21:31,840 --> 00:21:34,799
control manager which actually does a

00:21:33,600 --> 00:21:37,440
lot of work for us

00:21:34,799 --> 00:21:38,880
the control manager is one that runs

00:21:37,440 --> 00:21:41,520
continuous loops

00:21:38,880 --> 00:21:43,760
to ensure the current running state of

00:21:41,520 --> 00:21:44,480
the cubitus cluster is actually equal to

00:21:43,760 --> 00:21:46,640
the

00:21:44,480 --> 00:21:48,400
uh desired state that you have defined

00:21:46,640 --> 00:21:50,799
in your yaml files

00:21:48,400 --> 00:21:52,240
so it is a one that ensures if you have

00:21:50,799 --> 00:21:54,159
defined five replicas

00:21:52,240 --> 00:21:56,080
all the five are running that's the job

00:21:54,159 --> 00:21:58,400
of this control manager

00:21:56,080 --> 00:22:00,559
and then inside the worker node uh of

00:21:58,400 --> 00:22:02,240
course all the worker node has

00:22:00,559 --> 00:22:03,919
your application that is running your

00:22:02,240 --> 00:22:05,440
containers that are running

00:22:03,919 --> 00:22:07,520
and it has to ensure there is a

00:22:05,440 --> 00:22:09,200
mechanism for having a communication

00:22:07,520 --> 00:22:12,240
and that is achieved with the help of q

00:22:09,200 --> 00:22:14,240
proxy so q proxy helps to do the pawn to

00:22:12,240 --> 00:22:16,960
port communication part outside world

00:22:14,240 --> 00:22:19,840
communication and things like that

00:22:16,960 --> 00:22:21,039
and finally you have this add-ons which

00:22:19,840 --> 00:22:23,840
is another whole

00:22:21,039 --> 00:22:24,559
suit of tools that is helpful for you to

00:22:23,840 --> 00:22:26,880
do it

00:22:24,559 --> 00:22:28,480
and you can manage them you can pick and

00:22:26,880 --> 00:22:30,000
choose whatever you want like for

00:22:28,480 --> 00:22:30,720
example if you have your own custom

00:22:30,000 --> 00:22:34,400
login

00:22:30,720 --> 00:22:34,400
you can remove the logging from the

00:22:39,039 --> 00:22:42,720
like for equipmentnbc and dashboards and

00:22:41,360 --> 00:22:46,400
monitoring so you can

00:22:42,720 --> 00:22:47,039
use them together so this is how an

00:22:46,400 --> 00:22:49,200
entire

00:22:47,039 --> 00:22:51,039
application that interior cuban list

00:22:49,200 --> 00:22:51,840
looks like in a very very high level

00:22:51,039 --> 00:22:54,080
view right

00:22:51,840 --> 00:22:55,600
so that's these components with which

00:22:54,080 --> 00:22:58,159
they talk with each other

00:22:55,600 --> 00:23:01,440
and then take schedule and then create

00:22:58,159 --> 00:23:01,440
your application container

00:23:02,159 --> 00:23:06,080
inside this component the api server is

00:23:04,159 --> 00:23:07,600
actually you can

00:23:06,080 --> 00:23:09,200
have the system very highly available

00:23:07,600 --> 00:23:11,360
component and there can be multiple

00:23:09,200 --> 00:23:13,520
instances of api server running

00:23:11,360 --> 00:23:14,559
and it is basically to ensure like if

00:23:13,520 --> 00:23:16,720
something goes

00:23:14,559 --> 00:23:19,360
wrong or something goes over the picture

00:23:16,720 --> 00:23:20,880
the the other api servers can hit start

00:23:19,360 --> 00:23:23,200
and start doing the connectivity

00:23:20,880 --> 00:23:24,880
it's also very important to note api

00:23:23,200 --> 00:23:26,400
server is the only one that can connect

00:23:24,880 --> 00:23:29,039
to utc database on

00:23:26,400 --> 00:23:31,440
any database sorry any ecd database

00:23:29,039 --> 00:23:34,720
inside the given discluster

00:23:31,440 --> 00:23:37,760
to ensure that the state reading

00:23:34,720 --> 00:23:41,120
and also state writing direct

00:23:37,760 --> 00:23:44,559
policies or authority authorizations

00:23:41,120 --> 00:23:44,559
lives in the api server

00:23:44,640 --> 00:23:48,960
once that is there the etcd database may

00:23:47,039 --> 00:23:50,880
not live inside your computer's cluster

00:23:48,960 --> 00:23:52,320
itself you can pull that out and put it

00:23:50,880 --> 00:23:54,080
somewhere else

00:23:52,320 --> 00:23:56,559
it does not manage for you to level

00:23:54,080 --> 00:23:57,760
inside and etc database is an excellent

00:23:56,559 --> 00:23:59,840
resilient

00:23:57,760 --> 00:24:01,440
key value storage that you can use it

00:23:59,840 --> 00:24:03,200
for different applications

00:24:01,440 --> 00:24:04,960
the reason i'm pulling this outside of

00:24:03,200 --> 00:24:05,760
primitives cluster is basically very

00:24:04,960 --> 00:24:07,520
simple

00:24:05,760 --> 00:24:10,000
if it runs inside the cluster if the

00:24:07,520 --> 00:24:11,120
cluster is destroyed for any problem or

00:24:10,000 --> 00:24:13,840
something like that

00:24:11,120 --> 00:24:15,600
then your abcd database will also

00:24:13,840 --> 00:24:18,000
destroy with it

00:24:15,600 --> 00:24:18,640
you will not have any information about

00:24:18,000 --> 00:24:21,679
the cupid's

00:24:18,640 --> 00:24:23,919
cluster there so intcd database

00:24:21,679 --> 00:24:26,240
uh you can pull them out and then use

00:24:23,919 --> 00:24:28,320
them together later sorry you can move

00:24:26,240 --> 00:24:29,919
the cdcd database outside because when

00:24:28,320 --> 00:24:31,440
your cluster goes down

00:24:29,919 --> 00:24:33,360
all the information about the cluster

00:24:31,440 --> 00:24:35,039
also goes down with the adc database but

00:24:33,360 --> 00:24:36,880
if you have it running outside

00:24:35,039 --> 00:24:38,159
you can actually recreate the same cube

00:24:36,880 --> 00:24:40,080
in this instance

00:24:38,159 --> 00:24:42,720
uh even if it fails you can create a

00:24:40,080 --> 00:24:43,440
complete new cluster and use the ctcd

00:24:42,720 --> 00:24:46,960
database

00:24:43,440 --> 00:24:48,720
as a source that's how powerful

00:24:46,960 --> 00:24:51,440
kubernetes is

00:24:48,720 --> 00:24:51,840
and you can also have this you know a

00:24:51,440 --> 00:24:53,520
drill

00:24:51,840 --> 00:24:55,600
down version of quote on limit

00:24:53,520 --> 00:24:58,000
management so for example

00:24:55,600 --> 00:24:59,840
on the right hand side you can see uh

00:24:58,000 --> 00:25:02,799
you can breakfast for a

00:24:59,840 --> 00:25:03,520
set of memory and a cpu and also you can

00:25:02,799 --> 00:25:05,919
limit your

00:25:03,520 --> 00:25:08,320
memory and cpu to the maximum that

00:25:05,919 --> 00:25:11,360
you're going to use for your containers

00:25:08,320 --> 00:25:13,919
so you can have this level of you know

00:25:11,360 --> 00:25:14,480
quota and limitations at the container

00:25:13,919 --> 00:25:16,799
level

00:25:14,480 --> 00:25:17,679
at the pod level at the deployment at

00:25:16,799 --> 00:25:20,159
the name space

00:25:17,679 --> 00:25:20,960
at all the levels like you can have it

00:25:20,159 --> 00:25:24,080
drill down

00:25:20,960 --> 00:25:27,360
or at high level so you can have

00:25:24,080 --> 00:25:29,279
all those required steps for you so you

00:25:27,360 --> 00:25:32,000
have this cube ctrl file

00:25:29,279 --> 00:25:33,919
you want to apply this to the uh

00:25:32,000 --> 00:25:36,400
kubernetes you have done it

00:25:33,919 --> 00:25:37,360
so what does it solve once you put it

00:25:36,400 --> 00:25:40,000
inside

00:25:37,360 --> 00:25:42,240
based on whatever we've seen the pod

00:25:40,000 --> 00:25:45,039
replication controller replica

00:25:42,240 --> 00:25:46,159
sets and deployment kind of addresses

00:25:45,039 --> 00:25:49,200
all the problems

00:25:46,159 --> 00:25:51,120
with the tick mark over here so you can

00:25:49,200 --> 00:25:53,039
do this image pulling policy parts

00:25:51,120 --> 00:25:54,559
actually take care of life cycle

00:25:53,039 --> 00:25:56,080
if something goes down controller

00:25:54,559 --> 00:25:58,640
manager does the restart

00:25:56,080 --> 00:26:00,320
and replication part uh the lioness

00:25:58,640 --> 00:26:02,559
group and the readiness probe actually

00:26:00,320 --> 00:26:04,799
use it for use for health checkers

00:26:02,559 --> 00:26:06,159
and then it cubitus also provides

00:26:04,799 --> 00:26:08,480
add-ons for monitoring

00:26:06,159 --> 00:26:10,080
and graphing and also addition to that

00:26:08,480 --> 00:26:11,760
for tracing and debug

00:26:10,080 --> 00:26:13,600
of kubernetes provides a platform in

00:26:11,760 --> 00:26:15,360
which you can add more things like for

00:26:13,600 --> 00:26:16,880
example put in yager and then

00:26:15,360 --> 00:26:18,799
monitor all these services that are

00:26:16,880 --> 00:26:20,320
running so you can do all those weeks

00:26:18,799 --> 00:26:21,679
and match activities on top of it so

00:26:20,320 --> 00:26:24,720
cuban just kind of solves

00:26:21,679 --> 00:26:27,039
these problems and

00:26:24,720 --> 00:26:28,240
the most important thing uh in the world

00:26:27,039 --> 00:26:29,840
of microservices

00:26:28,240 --> 00:26:31,679
is how the communication between

00:26:29,840 --> 00:26:33,039
services happening and service registry

00:26:31,679 --> 00:26:35,039
is very essential

00:26:33,039 --> 00:26:36,799
so let's see how cuban risk actually

00:26:35,039 --> 00:26:40,000
solves it so kubernetes

00:26:36,799 --> 00:26:42,159
has an object model called the services

00:26:40,000 --> 00:26:45,039
that helps you to address all those

00:26:42,159 --> 00:26:48,720
connectivity or depart for the external

00:26:45,039 --> 00:26:51,039
and or to other cluster communications

00:26:48,720 --> 00:26:52,480
so you have a pod of course a port can

00:26:51,039 --> 00:26:54,400
have multiple

00:26:52,480 --> 00:26:55,520
containers inside that they're running

00:26:54,400 --> 00:26:59,520
inside

00:26:55,520 --> 00:27:01,120
and it can came it can have antibiotics

00:26:59,520 --> 00:27:02,480
actually enable spot support

00:27:01,120 --> 00:27:06,080
communication

00:27:02,480 --> 00:27:08,159
so you can have abstraction on top of

00:27:06,080 --> 00:27:10,799
this board and call it as a service

00:27:08,159 --> 00:27:12,240
so servers as you can see in the aml

00:27:10,799 --> 00:27:15,440
file it comes with a kind

00:27:12,240 --> 00:27:16,480
service you put in a metadata and labels

00:27:15,440 --> 00:27:18,880
for the servers

00:27:16,480 --> 00:27:20,880
and then you specify where the service

00:27:18,880 --> 00:27:21,919
has to obstruct or what the service has

00:27:20,880 --> 00:27:24,559
to understand

00:27:21,919 --> 00:27:25,200
see here we are abstracting on our

00:27:24,559 --> 00:27:27,679
labels

00:27:25,200 --> 00:27:29,600
or the selector app although well which

00:27:27,679 --> 00:27:32,399
were used in the first

00:27:29,600 --> 00:27:34,159
part an example and then it also

00:27:32,399 --> 00:27:35,760
specifies the protocol in port

00:27:34,159 --> 00:27:37,679
so if you have just applied that

00:27:35,760 --> 00:27:39,919
particular file which you have shown

00:27:37,679 --> 00:27:41,600
it creates a server subtraction in front

00:27:39,919 --> 00:27:44,000
of the port

00:27:41,600 --> 00:27:45,440
and then it assigns an ip to it but this

00:27:44,000 --> 00:27:47,360
ip can be used

00:27:45,440 --> 00:27:49,440
only within the cluster it cannot be

00:27:47,360 --> 00:27:51,360
used outside of the cluster

00:27:49,440 --> 00:27:52,559
so in order to access this from the

00:27:51,360 --> 00:27:55,600
outside you can

00:27:52,559 --> 00:27:58,080
go for something called as the type

00:27:55,600 --> 00:27:59,679
node port over here the type node port

00:27:58,080 --> 00:28:03,360
is basically a science

00:27:59,679 --> 00:28:06,000
dynamic node port to the ip address

00:28:03,360 --> 00:28:08,559
so for example here i put in my target

00:28:06,000 --> 00:28:09,120
port is like 32765 and this board is

00:28:08,559 --> 00:28:12,080
attached

00:28:09,120 --> 00:28:13,120
for my service so if i have to access

00:28:12,080 --> 00:28:16,399
the service i can

00:28:13,120 --> 00:28:18,000
call my worker node get its id and then

00:28:16,399 --> 00:28:21,120
go for the

00:28:18,000 --> 00:28:22,799
node port combine them together

00:28:21,120 --> 00:28:25,679
and then access it it will call my

00:28:22,799 --> 00:28:30,480
servers which will then turn on my port

00:28:25,679 --> 00:28:33,200
inside which my containers are running

00:28:30,480 --> 00:28:34,960
so uh the very next thing is basically

00:28:33,200 --> 00:28:36,559
the load balancer here the major

00:28:34,960 --> 00:28:37,440
difference is like the type is known

00:28:36,559 --> 00:28:38,720
balancer

00:28:37,440 --> 00:28:40,559
and then if you have to find node

00:28:38,720 --> 00:28:42,320
balancer you're going to give an ip

00:28:40,559 --> 00:28:44,240
so with role balancer you can actually

00:28:42,320 --> 00:28:46,559
connect to the external world

00:28:44,240 --> 00:28:48,240
and add it to that uh cumulus also

00:28:46,559 --> 00:28:50,080
provide the same request configuration

00:28:48,240 --> 00:28:51,600
where you can abstract one level more

00:28:50,080 --> 00:28:54,159
and create an ingress ip

00:28:51,600 --> 00:28:55,760
that takes care of handling all the

00:28:54,159 --> 00:28:58,159
breakfasts that are coming in

00:28:55,760 --> 00:28:59,039
and then pass this and pass this request

00:28:58,159 --> 00:29:01,520
to the service

00:28:59,039 --> 00:29:02,880
and then back into the port and then get

00:29:01,520 --> 00:29:04,960
the information back

00:29:02,880 --> 00:29:06,080
so you can use ingress for external

00:29:04,960 --> 00:29:09,279
access which is the

00:29:06,080 --> 00:29:11,520
common production ready access right so

00:29:09,279 --> 00:29:12,720
with services cuban risk actually helps

00:29:11,520 --> 00:29:15,679
us solving this

00:29:12,720 --> 00:29:16,240
huge puzzle of service registry and it

00:29:15,679 --> 00:29:18,320
kind of

00:29:16,240 --> 00:29:19,919
enables that you have a proxied

00:29:18,320 --> 00:29:20,799
environment through which you can access

00:29:19,919 --> 00:29:22,880
your pods

00:29:20,799 --> 00:29:24,799
using the proxy we can do a lot of

00:29:22,880 --> 00:29:25,520
things and kubernetes actually works as

00:29:24,799 --> 00:29:27,679
a

00:29:25,520 --> 00:29:29,279
service to industry i would not say

00:29:27,679 --> 00:29:31,279
humans is 100

00:29:29,279 --> 00:29:33,520
perfect service to industry but it kind

00:29:31,279 --> 00:29:35,760
of helps you to achieve the basic level

00:29:33,520 --> 00:29:37,600
of service registry

00:29:35,760 --> 00:29:39,120
once it is done the other components

00:29:37,600 --> 00:29:40,799
that you have seen which is like the

00:29:39,120 --> 00:29:42,799
retries timeout settings and

00:29:40,799 --> 00:29:44,559
authorization and authentication

00:29:42,799 --> 00:29:46,480
uh kubernetes does not provide a

00:29:44,559 --> 00:29:47,520
complete intimate solution but you can

00:29:46,480 --> 00:29:49,520
use something like

00:29:47,520 --> 00:29:50,720
istio or linkery they are called as

00:29:49,520 --> 00:29:52,640
service meshes

00:29:50,720 --> 00:29:56,960
that kind of helps you to apply these

00:29:52,640 --> 00:29:56,960
configurations outside of the envelope

00:29:58,159 --> 00:30:03,200
so a kubernetes kind of helps you to

00:30:01,600 --> 00:30:03,919
solve most of the things that we have

00:30:03,200 --> 00:30:06,159
seen

00:30:03,919 --> 00:30:08,480
in this list and for a few other things

00:30:06,159 --> 00:30:10,320
it provides a mechanism or provides a

00:30:08,480 --> 00:30:13,440
platform on top of which you can add

00:30:10,320 --> 00:30:16,159
more tools and then use that and then

00:30:13,440 --> 00:30:19,520
use that for making sure your

00:30:16,159 --> 00:30:21,760
application is running seamlessly

00:30:19,520 --> 00:30:23,520
so as we can as we have seen it is

00:30:21,760 --> 00:30:26,640
completely in these structures

00:30:23,520 --> 00:30:29,039
or decouples infrastructure so

00:30:26,640 --> 00:30:30,000
you can consider this analogy where like

00:30:29,039 --> 00:30:33,039
you have a

00:30:30,000 --> 00:30:35,120
big chunk of land or a server rack

00:30:33,039 --> 00:30:36,960
and then you give the server out to

00:30:35,120 --> 00:30:38,799
given it doesn't say like for my

00:30:36,960 --> 00:30:41,200
components for my

00:30:38,799 --> 00:30:42,000
uh services in it and kubernetes does

00:30:41,200 --> 00:30:43,360
that for you

00:30:42,000 --> 00:30:45,279
so you need not worry about like

00:30:43,360 --> 00:30:46,640
infrastructure how much memory to add

00:30:45,279 --> 00:30:48,240
what are the things that i have to do

00:30:46,640 --> 00:30:50,960
humanities actually does

00:30:48,240 --> 00:30:53,120
all those things for your box so it

00:30:50,960 --> 00:30:55,200
decouples the infrastructure completely

00:30:53,120 --> 00:30:56,559
it solves most of the micro services

00:30:55,200 --> 00:30:59,679
common problems

00:30:56,559 --> 00:31:00,000
and also also it provides a platform to

00:30:59,679 --> 00:31:03,039
ship

00:31:00,000 --> 00:31:04,960
awesome things like previously before

00:31:03,039 --> 00:31:07,039
containers it was difficult for us to

00:31:04,960 --> 00:31:09,039
stack away from certain things but give

00:31:07,039 --> 00:31:10,960
it this provides a whole new sort of

00:31:09,039 --> 00:31:14,080
abstraction with which you can

00:31:10,960 --> 00:31:16,159
ship awesome things to everybody

00:31:14,080 --> 00:31:17,200
with that if you have any questions

00:31:16,159 --> 00:31:19,279
about this pro

00:31:17,200 --> 00:31:20,559
about humanities or in general about

00:31:19,279 --> 00:31:22,159
full stack development

00:31:20,559 --> 00:31:24,240
please feel free to reach out to my

00:31:22,159 --> 00:31:25,600
twitter handle more than happy to help

00:31:24,240 --> 00:31:28,000
you out

00:31:25,600 --> 00:31:29,200
and that's all thanks for joining me

00:31:28,000 --> 00:31:31,200
again today

00:31:29,200 --> 00:31:32,720
have a great day and be safe in this

00:31:31,200 --> 00:31:36,080
weird times

00:31:32,720 --> 00:31:36,080

YouTube URL: https://www.youtube.com/watch?v=yU7GswtyCFE


