Title: TensorFlow.js - Bringing ML and Linear Algebra to Node.js - Ping Yu & Sandeep Gupta, Google
Publication date: 2020-05-11
Playlist: Node + JS Interactive 2019
Description: 
	TensorFlow.js - Bringing ML and Linear Algebra to Node.js - Ping Yu & Sandeep Gupta, Google
Speakers: Ping Yu, Sandeep Gupta
No Python required - this session will highlight unique opportunities by bringing ML and linear algebra to Node.js with TensorFlow.js. Nick will highlight how you can get started using pre-trained models, train your own models, and run TensorFlow.js in various Node.js environments (server, IoT).
Captions: 
	00:00:00,149 --> 00:00:05,490
okay I guess we can get started so

00:00:03,720 --> 00:00:07,080
thanks again thank you so much for

00:00:05,490 --> 00:00:08,880
joining us my name is Sanjeev Gupta

00:00:07,080 --> 00:00:11,250
I'm from the tensorflow team in Google

00:00:08,880 --> 00:00:14,099
and this talk is about machine learning

00:00:11,250 --> 00:00:17,029
and JavaScript I also have my colleague

00:00:14,099 --> 00:00:21,930
Bing here and he will go present with me

00:00:17,029 --> 00:00:30,630
so again thanks and welcome so you know

00:00:21,930 --> 00:00:33,090
we're seeing yeah we are seeing machine

00:00:30,630 --> 00:00:36,660
learning is beginning to have a really

00:00:33,090 --> 00:00:39,090
big impact in almost all fields of life

00:00:36,660 --> 00:00:41,760
around us right every day we see major

00:00:39,090 --> 00:00:43,230
news headlines we see new breakthroughs

00:00:41,760 --> 00:00:46,680
whether it's in transportation

00:00:43,230 --> 00:00:48,390
healthcare Environmental Sciences all

00:00:46,680 --> 00:00:50,520
kinds of applications even arts and

00:00:48,390 --> 00:00:52,440
creativity where problems that were

00:00:50,520 --> 00:00:54,870
really really difficult to solve just a

00:00:52,440 --> 00:00:57,390
few years back are now being solved in

00:00:54,870 --> 00:00:59,399
very impactful ways and very significant

00:00:57,390 --> 00:01:00,989
ways by these kinds of you know new

00:00:59,399 --> 00:01:05,729
computational approaches and new ways of

00:01:00,989 --> 00:01:07,580
using computer systems maybe just a

00:01:05,729 --> 00:01:10,049
quick show of hands how many of you are

00:01:07,580 --> 00:01:11,400
actively practicing machine learning or

00:01:10,049 --> 00:01:15,869
have some familiarity with machine

00:01:11,400 --> 00:01:18,540
learning okay so a relatively small

00:01:15,869 --> 00:01:20,130
number because many of you are somewhat

00:01:18,540 --> 00:01:21,360
new to the field let me just take a

00:01:20,130 --> 00:01:23,490
couple minutes and introduce some

00:01:21,360 --> 00:01:25,020
terminology here and and this will give

00:01:23,490 --> 00:01:27,540
you a flavor of why these kinds of

00:01:25,020 --> 00:01:29,549
methods are becoming so popular right so

00:01:27,540 --> 00:01:32,159
in most of classical programming right

00:01:29,549 --> 00:01:34,110
when we are trying to solve a problem or

00:01:32,159 --> 00:01:36,060
write a program for a computer to solve

00:01:34,110 --> 00:01:38,430
a problem the way we have usually done

00:01:36,060 --> 00:01:41,430
this is that we first come up with these

00:01:38,430 --> 00:01:43,920
rules and we try to write explicit code

00:01:41,430 --> 00:01:45,149
to codify those rules right so for

00:01:43,920 --> 00:01:48,090
example if you are trying to write a

00:01:45,149 --> 00:01:50,610
program that takes images and tries to

00:01:48,090 --> 00:01:53,250
detect whether it's an activity of let's

00:01:50,610 --> 00:01:54,960
say walking or running or bicycling you

00:01:53,250 --> 00:01:57,420
might come up with some features or

00:01:54,960 --> 00:01:59,070
rules to describe that one way to do

00:01:57,420 --> 00:02:01,320
that might be that let's measure the

00:01:59,070 --> 00:02:03,689
speed of the person if the speed is less

00:02:01,320 --> 00:02:05,850
than some number then we call this

00:02:03,689 --> 00:02:08,129
walking if the speed is more than that

00:02:05,850 --> 00:02:09,660
number maybe it is running right so you

00:02:08,129 --> 00:02:12,120
come up with these kinds of rules and

00:02:09,660 --> 00:02:13,230
you implement a computer program to to

00:02:12,120 --> 00:02:15,959
solve the problem that you

00:02:13,230 --> 00:02:18,450
to solve the issue with these approaches

00:02:15,959 --> 00:02:20,400
is that they very soon run into limits

00:02:18,450 --> 00:02:22,470
right you encounter situations where

00:02:20,400 --> 00:02:24,480
your new rules no longer work and even

00:02:22,470 --> 00:02:25,890
if your rules work well for the problem

00:02:24,480 --> 00:02:27,599
you are trying to solve they are

00:02:25,890 --> 00:02:29,220
generally not generalizable or

00:02:27,599 --> 00:02:30,540
extensible to a slightly different

00:02:29,220 --> 00:02:32,459
problem that you want to solve another

00:02:30,540 --> 00:02:36,269
time right so that's where classical

00:02:32,459 --> 00:02:38,790
programming runs into its limits like in

00:02:36,269 --> 00:02:41,430
machine learning it turns this whole

00:02:38,790 --> 00:02:43,110
concept sort of on its head it turns it

00:02:41,430 --> 00:02:46,530
upside down and the way you approach

00:02:43,110 --> 00:02:48,480
this is that what if I had some examples

00:02:46,530 --> 00:02:50,880
where I already know the answers and

00:02:48,480 --> 00:02:53,280
what if I can feed a lot of these

00:02:50,880 --> 00:02:55,500
examples and answers so we call this

00:02:53,280 --> 00:02:58,890
training data and answers on that

00:02:55,500 --> 00:03:02,250
training data into a computer program or

00:02:58,890 --> 00:03:04,380
a model and this model has the property

00:03:02,250 --> 00:03:07,170
that it can learn from these examples

00:03:04,380 --> 00:03:09,239
that it has fed so that it can come up

00:03:07,170 --> 00:03:10,950
with what these rules are right and

00:03:09,239 --> 00:03:12,510
these rules may be in a form that humans

00:03:10,950 --> 00:03:15,030
can understand or it may be in some

00:03:12,510 --> 00:03:18,600
abstract form that a computer program is

00:03:15,030 --> 00:03:21,209
choosing to describe that problem so

00:03:18,600 --> 00:03:23,010
this becomes a very generalizable way of

00:03:21,209 --> 00:03:24,840
solving a problem and so in practice the

00:03:23,010 --> 00:03:27,810
way you do that is you collect a lot of

00:03:24,840 --> 00:03:30,150
training data and you have that data

00:03:27,810 --> 00:03:32,370
labeled this as these are called human

00:03:30,150 --> 00:03:34,290
generated labels you feed it into a

00:03:32,370 --> 00:03:36,660
machine learning program or a model and

00:03:34,290 --> 00:03:39,030
then outcome these rules or a trained

00:03:36,660 --> 00:03:41,130
models so this is the training phase of

00:03:39,030 --> 00:03:43,709
the machine learning process and once

00:03:41,130 --> 00:03:46,319
you have trained a model now so this is

00:03:43,709 --> 00:03:48,810
some representation of the model now you

00:03:46,319 --> 00:03:51,030
can feed new data into this model and

00:03:48,810 --> 00:03:53,130
then that model is ready to give you new

00:03:51,030 --> 00:03:54,450
answers or new predictions on this model

00:03:53,130 --> 00:03:57,030
right so this is called the inference

00:03:54,450 --> 00:03:59,340
phase so this is sort of the most you

00:03:57,030 --> 00:04:01,530
know at a high level conceptually this

00:03:59,340 --> 00:04:04,170
is how a machine learning way of solving

00:04:01,530 --> 00:04:06,180
a problem will look like so just to look

00:04:04,170 --> 00:04:09,030
at this little bit visually let's say

00:04:06,180 --> 00:04:10,560
you are trying to classify images this

00:04:09,030 --> 00:04:14,310
is sort of what's happening there right

00:04:10,560 --> 00:04:16,590
so a model is a collection of layers or

00:04:14,310 --> 00:04:18,359
and all these layers are these are just

00:04:16,590 --> 00:04:21,060
computational blocks right each layer

00:04:18,359 --> 00:04:23,310
and each element in that layer is just

00:04:21,060 --> 00:04:25,530
doing a very simple math operation it's

00:04:23,310 --> 00:04:26,400
taking in some numbers it's multiplying

00:04:25,530 --> 00:04:28,110
those numbers with

00:04:26,400 --> 00:04:30,419
some other numbers and it's producing a

00:04:28,110 --> 00:04:32,759
new output and you do all this and you

00:04:30,419 --> 00:04:36,419
feed this forward and you wire this

00:04:32,759 --> 00:04:40,050
model so that when an image is fed in it

00:04:36,419 --> 00:04:42,000
produces an output that is close to the

00:04:40,050 --> 00:04:44,039
output that we expect so if you feed in

00:04:42,000 --> 00:04:46,229
an image of a cat we want all these

00:04:44,039 --> 00:04:48,180
things to flow in and the output of our

00:04:46,229 --> 00:04:51,539
model should be a number that we

00:04:48,180 --> 00:04:52,110
designate as indicating cat if it is not

00:04:51,539 --> 00:04:53,789
cat

00:04:52,110 --> 00:04:56,009
then we calculate an error or a

00:04:53,789 --> 00:04:58,800
difference and we propagate that error

00:04:56,009 --> 00:05:00,990
back through our model and we sort of

00:04:58,800 --> 00:05:02,699
tweak or adjust all the parameters of

00:05:00,990 --> 00:05:05,039
our model until we get the right answer

00:05:02,699 --> 00:05:06,900
right and we keep on doing this for lots

00:05:05,039 --> 00:05:08,789
and lots of examples and then we have a

00:05:06,900 --> 00:05:10,830
trained model for that particular task

00:05:08,789 --> 00:05:12,840
that we are trying to solve so this is

00:05:10,830 --> 00:05:14,760
sort of how machine learning approach of

00:05:12,840 --> 00:05:16,500
solving a problem works right now the

00:05:14,760 --> 00:05:19,110
reason why machine learning is really

00:05:16,500 --> 00:05:21,330
taking off now and it has become such a

00:05:19,110 --> 00:05:24,810
important part of problem solving today

00:05:21,330 --> 00:05:27,720
is for three main reasons first is that

00:05:24,810 --> 00:05:30,000
as we just saw it relies on availability

00:05:27,720 --> 00:05:32,430
of lot of data and not just quantity of

00:05:30,000 --> 00:05:36,030
data but good quality data data that's

00:05:32,430 --> 00:05:38,400
labeled and curated and and represents

00:05:36,030 --> 00:05:41,430
the full variety of the situation's you

00:05:38,400 --> 00:05:42,960
will encounter in real life so now the

00:05:41,430 --> 00:05:44,789
good news is there are lots and lots of

00:05:42,960 --> 00:05:46,889
these very large publicly available data

00:05:44,789 --> 00:05:48,900
sets which make it easy for any

00:05:46,889 --> 00:05:51,539
developer to get started and trained

00:05:48,900 --> 00:05:53,220
powerful machine learning models the

00:05:51,539 --> 00:05:56,870
second aspect is that these models can

00:05:53,220 --> 00:05:59,340
be computationally quite expensive

00:05:56,870 --> 00:06:01,440
although these are simple computations

00:05:59,340 --> 00:06:04,199
that they're running but they just run

00:06:01,440 --> 00:06:06,870
millions and millions of them and so you

00:06:04,199 --> 00:06:08,639
need you know very significant

00:06:06,870 --> 00:06:10,620
computation power to be able to run

00:06:08,639 --> 00:06:12,690
these models in a practically useful

00:06:10,620 --> 00:06:15,330
time frame and now there are these

00:06:12,690 --> 00:06:17,370
custom hardware's there are GPU advances

00:06:15,330 --> 00:06:18,870
and new types of accelerators that are

00:06:17,370 --> 00:06:20,669
coming up which have made it very

00:06:18,870 --> 00:06:22,699
practical to run machine learning models

00:06:20,669 --> 00:06:25,139
in a very very reasonable amount of time

00:06:22,699 --> 00:06:26,699
and then lastly the research in the

00:06:25,139 --> 00:06:28,680
field of ml has been growing and

00:06:26,699 --> 00:06:30,180
advancing at a very fast pace there are

00:06:28,680 --> 00:06:32,610
new publications that come out all the

00:06:30,180 --> 00:06:34,529
time and new ways of solving problems so

00:06:32,610 --> 00:06:37,700
basically all of these things kind of

00:06:34,529 --> 00:06:39,540
now give everyone the ability to do this

00:06:37,700 --> 00:06:42,720
for what

00:06:39,540 --> 00:06:44,370
a problem you are interested in and this

00:06:42,720 --> 00:06:46,110
is where frameworks like tensorflow come

00:06:44,370 --> 00:06:47,610
in right so so tensorflow

00:06:46,110 --> 00:06:49,590
is an open source library for doing

00:06:47,610 --> 00:06:52,740
machine learning and what that means is

00:06:49,590 --> 00:06:55,020
that lot of this mechanics and logistics

00:06:52,740 --> 00:06:58,410
of training a model of doing this kind

00:06:55,020 --> 00:07:00,000
of back propagation and and adjusting

00:06:58,410 --> 00:07:03,300
all the weights of running your

00:07:00,000 --> 00:07:04,920
experiments of creating a deployed model

00:07:03,300 --> 00:07:06,810
which can then be deployed and used in

00:07:04,920 --> 00:07:08,670
production all of these things are

00:07:06,810 --> 00:07:10,290
managed for you by a library like

00:07:08,670 --> 00:07:13,320
tensorflow so you don't have to reinvent

00:07:10,290 --> 00:07:15,180
all of this stuff it also has a bunch of

00:07:13,320 --> 00:07:18,350
pre trained models available that you

00:07:15,180 --> 00:07:21,300
can use off-the-shelf right but

00:07:18,350 --> 00:07:23,850
tensorflow was written with a Python

00:07:21,300 --> 00:07:25,290
front end right and this is where most

00:07:23,850 --> 00:07:28,320
of the machine learning tools that are

00:07:25,290 --> 00:07:30,930
out there today require one to learn

00:07:28,320 --> 00:07:32,820
Python and in fact python is termed as

00:07:30,930 --> 00:07:34,980
the language for data science right

00:07:32,820 --> 00:07:37,080
which is unfortunate because javascript

00:07:34,980 --> 00:07:39,150
is the most widely used programming

00:07:37,080 --> 00:07:41,630
language and really there haven't been

00:07:39,150 --> 00:07:44,430
too many accessible machine learning

00:07:41,630 --> 00:07:46,590
frameworks that JavaScript developers

00:07:44,430 --> 00:07:48,750
could use natively in JavaScript without

00:07:46,590 --> 00:07:50,640
having to have the burden of learning a

00:07:48,750 --> 00:07:53,760
new stack or a completely new

00:07:50,640 --> 00:07:55,830
programming paradigm so motivated by

00:07:53,760 --> 00:07:57,690
that we released this library called

00:07:55,830 --> 00:07:59,730
tensorflow javascript which is basically

00:07:57,690 --> 00:08:03,300
a version of tensorflow that is

00:07:59,730 --> 00:08:05,370
javascript native and so it lets you run

00:08:03,300 --> 00:08:07,170
machine learning models and even train

00:08:05,370 --> 00:08:09,930
machine learning models in the browser

00:08:07,170 --> 00:08:12,360
so you can run it in in web browsers

00:08:09,930 --> 00:08:14,880
through JavaScript you can run it

00:08:12,360 --> 00:08:16,470
server-side with node.js and as we will

00:08:14,880 --> 00:08:18,090
show you you can run it in variety of

00:08:16,470 --> 00:08:22,290
other platforms where JavaScript can be

00:08:18,090 --> 00:08:23,550
used this library is GPU accelerated and

00:08:22,290 --> 00:08:25,830
again we will talk a little bit more

00:08:23,550 --> 00:08:28,050
about performance later but we use WebGL

00:08:25,830 --> 00:08:29,970
acceleration in the browser so it is

00:08:28,050 --> 00:08:32,370
very very performant for the common

00:08:29,970 --> 00:08:34,919
types of machine learning models and it

00:08:32,370 --> 00:08:36,990
is completely open-source so it's it's

00:08:34,919 --> 00:08:39,840
open for the community to use buried on

00:08:36,990 --> 00:08:43,050
extend and contribute back into the into

00:08:39,840 --> 00:08:45,510
the library so when we release this our

00:08:43,050 --> 00:08:47,220
hope was that this gives web developers

00:08:45,510 --> 00:08:49,410
and JavaScript developers back-end

00:08:47,220 --> 00:08:51,360
developers an easier way to get started

00:08:49,410 --> 00:08:53,160
with ml and we have been very happy

00:08:51,360 --> 00:08:56,220
actually to see the

00:08:53,160 --> 00:09:01,110
and uptake this is an interesting sort

00:08:56,220 --> 00:09:03,000
of example this person pyramids he gives

00:09:01,110 --> 00:09:05,430
a lot of very influential talks in the

00:09:03,000 --> 00:09:08,220
JavaScript community and he gave a talk

00:09:05,430 --> 00:09:10,800
recently at nordic das and he sort of

00:09:08,220 --> 00:09:13,230
highlighted some of exactly these these

00:09:10,800 --> 00:09:15,300
motivational points that we had is that

00:09:13,230 --> 00:09:17,399
you know this whole excitement of ml

00:09:15,300 --> 00:09:20,250
like the JavaScript community is kind of

00:09:17,399 --> 00:09:22,680
missing out on and now it tends to flow

00:09:20,250 --> 00:09:24,689
Jas you know there's an easier way to

00:09:22,680 --> 00:09:26,850
get started and to be able to use

00:09:24,689 --> 00:09:29,339
machine learning and in fact he says

00:09:26,850 --> 00:09:30,779
down there that now you can bring the

00:09:29,339 --> 00:09:32,490
power of machine learning to your web

00:09:30,779 --> 00:09:35,490
application or JavaScript application

00:09:32,490 --> 00:09:37,860
with ten lines of code and and sort of

00:09:35,490 --> 00:09:39,930
we love we love this testimonial except

00:09:37,860 --> 00:09:42,600
that it's five lines of code it's not

00:09:39,930 --> 00:09:46,259
even ten lines of code I'll show you

00:09:42,600 --> 00:09:48,029
that so here for example this is how you

00:09:46,259 --> 00:09:49,649
know this is sort of the rough template

00:09:48,029 --> 00:09:52,290
of bringing in machine learning into

00:09:49,649 --> 00:09:53,910
your application there are first two

00:09:52,290 --> 00:09:56,189
lines of code which are basically

00:09:53,910 --> 00:09:58,170
importing our library and here we are

00:09:56,189 --> 00:10:01,079
showing the node example on top so you

00:09:58,170 --> 00:10:03,540
import the TF GS node package and the

00:10:01,079 --> 00:10:05,399
second line is importing one of our many

00:10:03,540 --> 00:10:08,040
pre trained models so this is the cocoa

00:10:05,399 --> 00:10:11,130
SSD model which is an object detection

00:10:08,040 --> 00:10:14,160
model that is trained so that when you

00:10:11,130 --> 00:10:16,110
give it images it will recognize a bunch

00:10:14,160 --> 00:10:18,149
of common objects present in that image

00:10:16,110 --> 00:10:20,759
and it will give you bounding boxes for

00:10:18,149 --> 00:10:23,279
where those objects are right so you can

00:10:20,759 --> 00:10:24,930
load the library on the model if you are

00:10:23,279 --> 00:10:27,209
running this in browser then the

00:10:24,930 --> 00:10:30,959
alternate is to just script source it

00:10:27,209 --> 00:10:33,120
from our hosted libraries and then you

00:10:30,959 --> 00:10:34,829
create an instance of the model which is

00:10:33,120 --> 00:10:36,870
right there that on that first line so

00:10:34,829 --> 00:10:40,199
basically I'm creating an instance of my

00:10:36,870 --> 00:10:44,459
cocoa SSD model and loading it and then

00:10:40,199 --> 00:10:46,920
I load an image and and decode a PNG

00:10:44,459 --> 00:10:49,649
image to convert it into a form that my

00:10:46,920 --> 00:10:51,779
machine learning model can ingest and

00:10:49,649 --> 00:10:53,670
that's it and then I call my model

00:10:51,779 --> 00:10:55,980
detect functions so you know model dot

00:10:53,670 --> 00:10:59,160
detect you pass it the image object and

00:10:55,980 --> 00:11:00,629
you get back your predictions so what

00:10:59,160 --> 00:11:03,029
you get back you see that image on the

00:11:00,629 --> 00:11:04,860
right it has identified a cup and a

00:11:03,029 --> 00:11:05,889
phone and a mouse and it puts bounding

00:11:04,860 --> 00:11:08,290
boxes on those object

00:11:05,889 --> 00:11:09,999
and you get a JSON object which tells

00:11:08,290 --> 00:11:11,499
you the names of those objects and it

00:11:09,999 --> 00:11:13,299
tells you the coordinates of where these

00:11:11,499 --> 00:11:16,569
are and it also gives you a probability

00:11:13,299 --> 00:11:20,019
of how confident it is of that

00:11:16,569 --> 00:11:22,389
prediction so you know super-powerful

00:11:20,019 --> 00:11:24,309
model five lines of code your web

00:11:22,389 --> 00:11:25,869
application can now be using an object

00:11:24,309 --> 00:11:28,359
detector right and you can do similar

00:11:25,869 --> 00:11:31,359
things with text and speech and lots of

00:11:28,359 --> 00:11:33,819
other types of models so why is this a

00:11:31,359 --> 00:11:36,100
good idea so on client side in the

00:11:33,819 --> 00:11:38,079
browser there are many many advantages

00:11:36,100 --> 00:11:41,079
of running machine learning in in

00:11:38,079 --> 00:11:43,419
client-side browser gives you a lot of

00:11:41,079 --> 00:11:46,029
interactivity right it's easy access to

00:11:43,419 --> 00:11:48,549
sensors like webcam and microphone etc

00:11:46,029 --> 00:11:50,529
so you can immediately take advantage of

00:11:48,549 --> 00:11:52,179
all this sensor data and put it into

00:11:50,529 --> 00:11:54,220
your machine learning model so in that

00:11:52,179 --> 00:11:55,629
object detection case for example the

00:11:54,220 --> 00:11:59,230
images could be coming from a webcam

00:11:55,629 --> 00:12:00,699
stream there is nothing to install so

00:11:59,230 --> 00:12:03,699
you know you can share with your users

00:12:00,699 --> 00:12:07,779
you just share a URL link and they have

00:12:03,699 --> 00:12:09,609
a web page which has the model in it and

00:12:07,779 --> 00:12:10,919
you know they download that and they're

00:12:09,609 --> 00:12:15,759
there and they are using that model

00:12:10,919 --> 00:12:17,290
directly from from that URL it has huge

00:12:15,759 --> 00:12:19,149
privacy implications because you are

00:12:17,290 --> 00:12:21,759
running these models locally client-side

00:12:19,149 --> 00:12:23,649
no data is going to the server right so

00:12:21,759 --> 00:12:27,879
for healthcare or any other privacy

00:12:23,649 --> 00:12:30,610
sensitive type of applications you this

00:12:27,879 --> 00:12:32,379
has enormous implications also it

00:12:30,610 --> 00:12:33,730
reduces server side costs because you

00:12:32,379 --> 00:12:44,040
don't have to stand up complicated

00:12:33,730 --> 00:12:44,040
architecture sorry I lost my

00:12:47,140 --> 00:12:58,090
project it oh really okay I can I think

00:12:56,030 --> 00:13:01,870
the connector was a little loose yeah

00:12:58,090 --> 00:13:01,870
okay let's hope this works

00:13:01,880 --> 00:13:05,540
and then lastly as I mentioned that

00:13:03,260 --> 00:13:08,210
because we use WebGL acceleration you

00:13:05,540 --> 00:13:11,900
get really really good performance on

00:13:08,210 --> 00:13:13,730
the server side you can you can run more

00:13:11,900 --> 00:13:15,620
powerful models that may not be

00:13:13,730 --> 00:13:18,140
practical to run client-side in the

00:13:15,620 --> 00:13:19,460
browser and you can basically take full

00:13:18,140 --> 00:13:22,190
advantage of whatever hardware you have

00:13:19,460 --> 00:13:25,040
and these could be you know multiple

00:13:22,190 --> 00:13:28,340
code machine or GPUs or even other

00:13:25,040 --> 00:13:30,500
custom hardware's there is a very large

00:13:28,340 --> 00:13:32,900
NPM package ecosystem so if you are

00:13:30,500 --> 00:13:34,940
using machine learning in node you can

00:13:32,900 --> 00:13:37,430
benefit from this whole ecosystem of

00:13:34,940 --> 00:13:39,440
node packages and you can bring machine

00:13:37,430 --> 00:13:41,270
learning directly into your node stack

00:13:39,440 --> 00:13:43,670
so you don't have to have like separate

00:13:41,270 --> 00:13:46,010
Python data science teams and a separate

00:13:43,670 --> 00:13:47,510
back and node team who sort of are not

00:13:46,010 --> 00:13:49,280
really talking to each other you can

00:13:47,510 --> 00:13:53,150
bring machine learning directly into

00:13:49,280 --> 00:13:55,220
node and have like a single stack and we

00:13:53,150 --> 00:13:57,620
because we bind to tensorflow see

00:13:55,220 --> 00:13:59,960
library when we run server side we get

00:13:57,620 --> 00:14:02,150
lot of performance benefits and sort of

00:13:59,960 --> 00:14:03,800
directly the ability to use any

00:14:02,150 --> 00:14:05,720
conventional traditional machine

00:14:03,800 --> 00:14:10,250
learning model that has been trained on

00:14:05,720 --> 00:14:13,340
the Python side so there are three main

00:14:10,250 --> 00:14:15,200
ways of using this library one is and

00:14:13,340 --> 00:14:17,120
that object detection example was was

00:14:15,200 --> 00:14:18,950
one example of this you can take a

00:14:17,120 --> 00:14:20,630
existing machine learning model whether

00:14:18,950 --> 00:14:22,820
it's a tensorflow JavaScript model or

00:14:20,630 --> 00:14:25,190
whether it's one of your Python models

00:14:22,820 --> 00:14:28,670
and you can bring it in and run it with

00:14:25,190 --> 00:14:31,310
tensorflow javascript second way to do

00:14:28,670 --> 00:14:32,930
this is to take a pre trained model but

00:14:31,310 --> 00:14:35,570
then often you have to customize it on

00:14:32,930 --> 00:14:38,420
your own data to solve a particular

00:14:35,570 --> 00:14:40,430
problem so we have easy ways of

00:14:38,420 --> 00:14:42,200
retraining a model and being able to

00:14:40,430 --> 00:14:44,210
modify it on a small amount of

00:14:42,200 --> 00:14:46,100
additional data so that you can retrain

00:14:44,210 --> 00:14:47,930
it and by the way another thing you can

00:14:46,100 --> 00:14:50,300
do is if any of you are familiar with

00:14:47,930 --> 00:14:52,130
the Google cloud auto ml service which

00:14:50,300 --> 00:14:54,380
is a really nice cloud-based way of

00:14:52,130 --> 00:14:56,600
bringing your own data to the cloud and

00:14:54,380 --> 00:14:58,800
getting a custom model trained for you

00:14:56,600 --> 00:15:00,240
with no ml expertise needed then

00:14:58,800 --> 00:15:02,279
that's also compatible with tensorflow

00:15:00,240 --> 00:15:04,410
JavaScript so you can train an auto ml

00:15:02,279 --> 00:15:06,149
model on the cloud and get a trained

00:15:04,410 --> 00:15:08,820
tensorflow JavaScript model that you can

00:15:06,149 --> 00:15:10,529
run and lastly for those of you who want

00:15:08,820 --> 00:15:13,230
to experiment and write new models from

00:15:10,529 --> 00:15:14,880
scratch there is a full programming API

00:15:13,230 --> 00:15:16,079
low-level programming API with

00:15:14,880 --> 00:15:18,420
JavaScript so you're writing JavaScript

00:15:16,079 --> 00:15:20,279
code and you can write new new models

00:15:18,420 --> 00:15:22,050
all right so the library could use it in

00:15:20,279 --> 00:15:24,690
any and it could be used in any of these

00:15:22,050 --> 00:15:26,430
ways because javascript is such a

00:15:24,690 --> 00:15:28,320
versatile language and it runs on many

00:15:26,430 --> 00:15:30,089
many platforms you can use tensorflow

00:15:28,320 --> 00:15:31,860
J's in all these places right so you can

00:15:30,089 --> 00:15:34,410
run it in browser you can run it on

00:15:31,860 --> 00:15:36,149
native mobile hybrid platforms such as

00:15:34,410 --> 00:15:38,160
react native we just recently announced

00:15:36,149 --> 00:15:40,709
integration with react native and you

00:15:38,160 --> 00:15:43,200
get first-class support with WebGL

00:15:40,709 --> 00:15:45,899
acceleration through react you can run

00:15:43,200 --> 00:15:47,700
it with node and then desktop there are

00:15:45,899 --> 00:15:50,370
examples of people building electron a

00:15:47,700 --> 00:15:53,430
place and using tensorflow JS through

00:15:50,370 --> 00:15:54,810
electron so so many ways of using this

00:15:53,430 --> 00:15:56,220
and we are continuously working on

00:15:54,810 --> 00:16:00,959
adding support for more and more

00:15:56,220 --> 00:16:03,270
platforms so as I said we prepackaged a

00:16:00,959 --> 00:16:04,890
bunch of ML models for common ml tasks

00:16:03,270 --> 00:16:07,079
and here are some examples there are

00:16:04,890 --> 00:16:09,029
models for doing image classification

00:16:07,079 --> 00:16:11,579
object detection there are models for

00:16:09,029 --> 00:16:13,589
recognizing human pose we had some of

00:16:11,579 --> 00:16:15,149
those demoed at our booth and you know

00:16:13,589 --> 00:16:17,430
you're welcome to stop by after this and

00:16:15,149 --> 00:16:19,410
see some more of these we do pose

00:16:17,430 --> 00:16:22,079
detection there's a very nice model for

00:16:19,410 --> 00:16:23,820
audio commands so if you speak words it

00:16:22,079 --> 00:16:25,980
can recognize the spoken words and you

00:16:23,820 --> 00:16:27,600
can use that to drive actions and then

00:16:25,980 --> 00:16:30,060
we are increasingly doing more and more

00:16:27,600 --> 00:16:33,450
around text so text has a variety of use

00:16:30,060 --> 00:16:35,040
cases like sentiment and toxicity and

00:16:33,450 --> 00:16:38,339
all of these models can be either just

00:16:35,040 --> 00:16:40,260
done used with a scripts source from our

00:16:38,339 --> 00:16:43,649
hosted scripts or you can npm install

00:16:40,260 --> 00:16:45,839
them so here are some more examples and

00:16:43,649 --> 00:16:48,360
using these models as building blocks

00:16:45,839 --> 00:16:50,160
you can build applications that solve

00:16:48,360 --> 00:16:52,649
these types of problems like

00:16:50,160 --> 00:16:54,839
accessibility or sentiment analysis

00:16:52,649 --> 00:16:56,519
conversational agents and a variety of

00:16:54,839 --> 00:16:58,170
different things so all of those

00:16:56,519 --> 00:17:00,959
examples you are seeing on the right are

00:16:58,170 --> 00:17:04,439
instances of models just running a

00:17:00,959 --> 00:17:05,880
client-side so let me just take a couple

00:17:04,439 --> 00:17:07,890
minutes and show you a very quick demo

00:17:05,880 --> 00:17:10,709
and just to show you how easy it is to

00:17:07,890 --> 00:17:12,689
retrain a model like this so this is a

00:17:10,709 --> 00:17:14,939
an application called teacher

00:17:12,689 --> 00:17:17,549
machine has anybody seen teachables

00:17:14,939 --> 00:17:19,139
machine so far so this is something that

00:17:17,549 --> 00:17:21,120
I would encourage you to try out on your

00:17:19,139 --> 00:17:23,970
own time as well after this but let's

00:17:21,120 --> 00:17:25,589
just you know see how this works so I'm

00:17:23,970 --> 00:17:27,659
going to skip this tutorial so this is

00:17:25,589 --> 00:17:29,129
the tea-table machine website and this

00:17:27,659 --> 00:17:30,899
will show you how you can take an

00:17:29,129 --> 00:17:34,320
existing machine learning model and

00:17:30,899 --> 00:17:36,929
retrain it in in a matter of seconds so

00:17:34,320 --> 00:17:40,590
I'm going to skip this tutorial and what

00:17:36,929 --> 00:17:43,649
you're seeing here is a simple image

00:17:40,590 --> 00:17:45,720
classifier and this web session has

00:17:43,649 --> 00:17:47,759
already loaded a powerful image

00:17:45,720 --> 00:17:49,860
classification model called mobile net

00:17:47,759 --> 00:17:52,440
and it's running in my browser session

00:17:49,860 --> 00:17:54,659
and we are going to modify this model to

00:17:52,440 --> 00:17:56,730
do very simple rock-paper-scissors

00:17:54,659 --> 00:18:00,350
classification okay so we're going to

00:17:56,730 --> 00:18:06,120
output the word rock for my first class

00:18:00,350 --> 00:18:09,029
paper for second and scissors for third

00:18:06,120 --> 00:18:11,220
okay and now we're going to record these

00:18:09,029 --> 00:18:13,139
training images so this green sample

00:18:11,220 --> 00:18:15,240
green class will be rock then this will

00:18:13,139 --> 00:18:17,399
be paper and this will be scissors and

00:18:15,240 --> 00:18:20,700
we'll just you know record some images

00:18:17,399 --> 00:18:21,960
from my webcam so let's record rock so

00:18:20,700 --> 00:18:27,000
I'll hold down this button and record

00:18:21,960 --> 00:18:36,029
some training images okay so this is

00:18:27,000 --> 00:18:40,620
rock rock now let's do paper rock rock

00:18:36,029 --> 00:18:41,850
and now let's do scissors paper okay so

00:18:40,620 --> 00:18:43,259
I'm just recording some training images

00:18:41,850 --> 00:18:45,629
right now so ignore it's predictions

00:18:43,259 --> 00:18:46,980
scissors and now we have trained and now

00:18:45,629 --> 00:18:49,049
it's ready so now I have a new version

00:18:46,980 --> 00:18:53,250
of models running in my browser so let's

00:18:49,049 --> 00:18:58,950
try it out rock paper rock paper

00:18:53,250 --> 00:19:01,889
scissors paper rock scissors so there

00:18:58,950 --> 00:19:05,929
you see how easy it is to train it with

00:19:01,889 --> 00:19:05,929
like you know scissors if TMA scissors

00:19:08,650 --> 00:19:12,610
and the nice thing about this is that

00:19:10,240 --> 00:19:15,760
you can very similarly train a speech

00:19:12,610 --> 00:19:17,170
model or a pose model and now with the

00:19:15,760 --> 00:19:18,550
new version of tea table machine it

00:19:17,170 --> 00:19:20,650
gives you the ability to once you have

00:19:18,550 --> 00:19:22,540
trained this model to save it and you

00:19:20,650 --> 00:19:24,970
can you know put it on on your shared

00:19:22,540 --> 00:19:26,860
Drive somewhere or you can download a 80

00:19:24,970 --> 00:19:30,970
fjs compatible model that you can run

00:19:26,860 --> 00:19:34,030
offline so very very approachable way of

00:19:30,970 --> 00:19:35,560
getting started with machine learning so

00:19:34,030 --> 00:19:37,240
at this point I want to turn it over to

00:19:35,560 --> 00:19:57,970
my colleague pink who will talk tell you

00:19:37,240 --> 00:20:11,310
more about the API thank you can turn

00:19:57,970 --> 00:20:15,910
your mic please and the core fine-grain

00:20:11,310 --> 00:20:18,850
to come show the new network internally

00:20:15,910 --> 00:20:21,130
and how you be able to find you know

00:20:18,850 --> 00:20:24,880
control where it's gaining and as future

00:20:21,130 --> 00:20:27,280
and as sended mentioned we support most

00:20:24,880 --> 00:20:30,550
concise execution of a server-side on

00:20:27,280 --> 00:20:34,800
the client-side we use WebGL also give

00:20:30,550 --> 00:20:39,040
you automatic GPU salvation and recently

00:20:34,800 --> 00:20:40,660
we just create a alpha 3ds for web

00:20:39,040 --> 00:20:43,960
assembly so give me your better

00:20:40,660 --> 00:20:49,570
acceleration on CPU side as well oh no

00:20:43,960 --> 00:20:52,810
side we use no GSD binding directly into

00:20:49,570 --> 00:20:56,410
tensorflow c library so you can utilize

00:20:52,810 --> 00:20:59,620
this enough to flow on inside your node

00:20:56,410 --> 00:21:02,020
running environment and also we support

00:20:59,620 --> 00:21:06,070
GPU support how many devices that have

00:21:02,020 --> 00:21:09,400
who doesn't work with like NVIDIA GPU

00:21:06,070 --> 00:21:12,850
cards you can use the flow of GPU

00:21:09,400 --> 00:21:13,850
library and also we have something

00:21:12,850 --> 00:21:17,600
called an STL

00:21:13,850 --> 00:21:24,290
if you what it does give you that if you

00:21:17,600 --> 00:21:36,380
have you can see a traditional section

00:21:24,290 --> 00:21:40,220
and also devices you is the WebGL it

00:21:36,380 --> 00:21:42,830
sounds weird well give you the WebGL API

00:21:40,220 --> 00:21:49,340
actually you could use other way if you

00:21:42,830 --> 00:21:51,110
want to so alright so now you we do give

00:21:49,340 --> 00:21:52,790
you a lot of models right but the fact

00:21:51,110 --> 00:21:54,650
is that you may have your own models

00:21:52,790 --> 00:21:56,750
like you have an ml department that

00:21:54,650 --> 00:22:00,080
build your own model all you have seen

00:21:56,750 --> 00:22:01,810
some you know nice model outside you

00:22:00,080 --> 00:22:04,400
want to bring into your JavaScript

00:22:01,810 --> 00:22:07,400
application you can do that so we give

00:22:04,400 --> 00:22:10,540
you a converter that can convert any

00:22:07,400 --> 00:22:13,970
tender flow models into a JavaScript

00:22:10,540 --> 00:22:15,440
friendly format and we do a lot of

00:22:13,970 --> 00:22:18,440
optimization for you so it can run

00:22:15,440 --> 00:22:24,110
faster on your browser or you know

00:22:18,440 --> 00:22:26,780
native mobile devices and we give you

00:22:24,110 --> 00:22:29,930
also the API for you to low download

00:22:26,780 --> 00:22:33,070
your model from like any static file

00:22:29,930 --> 00:22:36,740
serving services like s3 or Google Cloud

00:22:33,070 --> 00:22:39,170
Storage and you can inject that interact

00:22:36,740 --> 00:22:43,060
into your application and run the

00:22:39,170 --> 00:22:46,220
prediction as what you know show earlier

00:22:43,060 --> 00:22:52,360
so that's for the browser side but for

00:22:46,220 --> 00:22:55,210
server side we just announced a new API

00:22:52,360 --> 00:22:59,030
there's basically if you want to run

00:22:55,210 --> 00:23:01,340
Python model inside no now you don't

00:22:59,030 --> 00:23:05,450
need to convert you can directly run

00:23:01,340 --> 00:23:07,970
them using our C library and which means

00:23:05,450 --> 00:23:10,070
you have better app support our

00:23:07,970 --> 00:23:13,640
converter actually support about 200

00:23:10,070 --> 00:23:15,470
apps the core apps of tensorflow but

00:23:13,640 --> 00:23:18,200
actually tenable has about thousand ops

00:23:15,470 --> 00:23:21,050
so this will give you a thousand offs

00:23:18,200 --> 00:23:23,030
so you can run really powerful machine

00:23:21,050 --> 00:23:26,660
learning models inside to know right now

00:23:23,030 --> 00:23:27,549
and it support both 10 to flow 1.0

00:23:26,660 --> 00:23:30,039
version or two

00:23:27,549 --> 00:23:32,850
no version and it gives you better

00:23:30,039 --> 00:23:35,110
performance why is that because pie song

00:23:32,850 --> 00:23:37,840
10th fellow actually runs nice time pie

00:23:35,110 --> 00:23:40,269
song so it has a Python layer it give

00:23:37,840 --> 00:23:43,090
actually caused a lot of you know delay

00:23:40,269 --> 00:23:44,289
but you know VA is much faster than pie

00:23:43,090 --> 00:23:47,649
sauce so that's why we are slightly

00:23:44,289 --> 00:23:51,669
better than Python when you run you know

00:23:47,649 --> 00:23:55,239
10 flow model directly inside no Jess so

00:23:51,669 --> 00:23:57,580
here are some perform member for mobile

00:23:55,239 --> 00:23:59,409
net that's what you know sandy was

00:23:57,580 --> 00:24:02,679
showing there's an image recognition

00:23:59,409 --> 00:24:04,929
model 1000 G has give you about 20

00:24:02,679 --> 00:24:07,179
milliseconds inference time which means

00:24:04,929 --> 00:24:09,549
every recognition of the image takes

00:24:07,179 --> 00:24:12,220
about 20 misses which give you 50 frames

00:24:09,549 --> 00:24:14,350
per second if you're building a you know

00:24:12,220 --> 00:24:18,369
like real-time application that's plenty

00:24:14,350 --> 00:24:20,190
for you to to play with and TF lie if

00:24:18,369 --> 00:24:24,850
you don't know is a native

00:24:20,190 --> 00:24:27,129
implementation Google's another open

00:24:24,850 --> 00:24:29,580
source project it runs on the mobile

00:24:27,129 --> 00:24:34,090
device it is performance around like

00:24:29,580 --> 00:24:36,600
1914 on iPhone we do have some you know

00:24:34,090 --> 00:24:39,460
performance improvement room for

00:24:36,600 --> 00:24:43,239
improvements for Android phone but we

00:24:39,460 --> 00:24:46,509
really working on that so on server side

00:24:43,239 --> 00:24:48,759
you can see you know the the know

00:24:46,509 --> 00:24:52,450
performance is very close to the C power

00:24:48,759 --> 00:24:55,749
plus performance so the bow like if you

00:24:52,450 --> 00:25:00,730
get really cool GPU you get buggy a

00:24:55,749 --> 00:25:06,759
millisecond preference you guys ready

00:25:00,730 --> 00:25:08,980
for some code alright so now you can

00:25:06,759 --> 00:25:11,409
build you can load your model you can lo

00:25:08,980 --> 00:25:13,899
our pre trainer model what about you

00:25:11,409 --> 00:25:15,820
want to build your own model I want to

00:25:13,899 --> 00:25:20,679
show you how to do that you first we

00:25:15,820 --> 00:25:22,090
want to tackle the high-level API so you

00:25:20,679 --> 00:25:27,070
promised primitive way this is loading

00:25:22,090 --> 00:25:29,529
our NPM packages getting our so this is

00:25:27,070 --> 00:25:32,289
loading out no packages so baby we know

00:25:29,529 --> 00:25:33,999
magic we have the C binding into you

00:25:32,289 --> 00:25:35,559
know the tender flows C library we also

00:25:33,999 --> 00:25:40,509
package you the C library inside there's

00:25:35,559 --> 00:25:41,380
no NPM and also you can load the GPU

00:25:40,509 --> 00:25:46,630
version

00:25:41,380 --> 00:25:49,299
if you have GPO enabled a car let's go

00:25:46,630 --> 00:25:54,100
back to the you know image recognition

00:25:49,299 --> 00:25:57,389
model that shown earlier so you as sandy

00:25:54,100 --> 00:26:00,399
mentioned a typical neural network are

00:25:57,389 --> 00:26:02,259
kind of constructed layer by layer each

00:26:00,399 --> 00:26:04,509
layer extracts certain feature out of

00:26:02,259 --> 00:26:07,120
previous layer and passed on the result

00:26:04,509 --> 00:26:09,250
from to the next layer we'll show you

00:26:07,120 --> 00:26:12,009
how to do that the example will show you

00:26:09,250 --> 00:26:16,360
how to build a image recognition model

00:26:12,009 --> 00:26:18,580
using the layer API so it's kind of

00:26:16,360 --> 00:26:20,409
crazy writing a lot of code but I mean

00:26:18,580 --> 00:26:22,299
to be honest Lee is not a bad if you

00:26:20,409 --> 00:26:24,190
consider like using like your curry or

00:26:22,299 --> 00:26:24,779
anything it's not more complicated than

00:26:24,190 --> 00:26:28,929
that

00:26:24,779 --> 00:26:30,850
so here the first thing is you use a

00:26:28,929 --> 00:26:32,950
screen shell model what is sequential

00:26:30,850 --> 00:26:35,860
model is basically this like 90% of the

00:26:32,950 --> 00:26:38,289
model out there are sequential so what

00:26:35,860 --> 00:26:43,960
it means that you layer the the layers

00:26:38,289 --> 00:26:47,409
one by one so the next we add a couple

00:26:43,960 --> 00:26:50,049
layers those are very typical numerical

00:26:47,409 --> 00:26:52,600
layers the combo to D give you you know

00:26:50,049 --> 00:26:56,559
a feature extraction type of feature and

00:26:52,600 --> 00:26:58,929
the max pooling gives you a kind of zoom

00:26:56,559 --> 00:27:03,580
in kind of feature so baby you can look

00:26:58,929 --> 00:27:06,159
at detail of that image at the end we

00:27:03,580 --> 00:27:08,649
will flatten the image into a

00:27:06,159 --> 00:27:12,190
one-dimensional vector so you know the

00:27:08,649 --> 00:27:14,769
first output the classification for the

00:27:12,190 --> 00:27:16,750
class that we want so what this last

00:27:14,769 --> 00:27:19,840
layer will output is a probability for

00:27:16,750 --> 00:27:22,389
each class so that's about it I mean

00:27:19,840 --> 00:27:26,799
that's the model we we built and then we

00:27:22,389 --> 00:27:29,590
use compile messer to to basically set

00:27:26,799 --> 00:27:32,769
up how we would train this model so

00:27:29,590 --> 00:27:36,820
particularly we set up the last function

00:27:32,769 --> 00:27:39,279
to use categorical cross entropy with

00:27:36,820 --> 00:27:40,809
you know long name but the fact is that

00:27:39,279 --> 00:27:42,190
you don't care and you just you know

00:27:40,809 --> 00:27:45,129
copy that

00:27:42,190 --> 00:27:47,350
the optimizer is a gradient descent he's

00:27:45,129 --> 00:27:51,490
also you know easy to copy just three

00:27:47,350 --> 00:27:53,049
letters and then take that model now

00:27:51,490 --> 00:27:55,090
you're ready to train the train is a

00:27:53,049 --> 00:27:58,060
very simple just one Messer

00:27:55,090 --> 00:28:02,370
you get access you that's your input why

00:27:58,060 --> 00:28:04,570
is this your output of your data set and

00:28:02,370 --> 00:28:09,120
epoch is how long you gonna run this

00:28:04,570 --> 00:28:11,710
training session after training is done

00:28:09,120 --> 00:28:13,810
you can of course you can look at we

00:28:11,710 --> 00:28:15,310
have other message to show you what the

00:28:13,810 --> 00:28:17,530
detail of the training is what the

00:28:15,310 --> 00:28:20,020
accuracy is you know all the other good

00:28:17,530 --> 00:28:22,360
stuff I'm kind of didn't show here but

00:28:20,020 --> 00:28:26,200
after training is done you can save the

00:28:22,360 --> 00:28:28,120
model to a file that's for no but you

00:28:26,200 --> 00:28:31,230
can also save it to local storage in

00:28:28,120 --> 00:28:35,620
browser I'll strip it to some server

00:28:31,230 --> 00:28:39,430
through HV request so we provide a lot

00:28:35,620 --> 00:28:41,590
of those different varieties at the end

00:28:39,430 --> 00:28:43,630
the model is ready now you can plug it

00:28:41,590 --> 00:28:47,680
in into your application and start you

00:28:43,630 --> 00:28:49,840
know making your predictions so that's

00:28:47,680 --> 00:28:52,570
high level API or what is a low level

00:28:49,840 --> 00:28:55,990
API the lower view I actually drill down

00:28:52,570 --> 00:28:59,290
inside the layers each layer actually is

00:28:55,990 --> 00:29:02,140
many many smaller operations for example

00:28:59,290 --> 00:29:05,560
there matrix-multiply there's some kind

00:29:02,140 --> 00:29:06,940
of you know you know cost function or

00:29:05,560 --> 00:29:09,070
something like at the cosine or sine

00:29:06,940 --> 00:29:11,140
function or some kind of activation

00:29:09,070 --> 00:29:13,360
function you would use I want to show

00:29:11,140 --> 00:29:18,970
you how you can do that with 10 to the

00:29:13,360 --> 00:29:21,130
GS so let's say we have a set of data

00:29:18,970 --> 00:29:23,440
set we want to estimate you can see

00:29:21,130 --> 00:29:25,360
those dots those are you know kind of

00:29:23,440 --> 00:29:28,600
polynomial you know kind of function

00:29:25,360 --> 00:29:30,130
output but you know it's not really

00:29:28,600 --> 00:29:33,160
perfect right so you cannot really know

00:29:30,130 --> 00:29:35,140
what exactly the function is we want to

00:29:33,160 --> 00:29:39,490
estimate the parameter for that function

00:29:35,140 --> 00:29:42,430
so a B and C will be the goal for us for

00:29:39,490 --> 00:29:44,710
this model to estimate we're doing the

00:29:42,430 --> 00:29:49,390
same thing we're loading our library we

00:29:44,710 --> 00:29:51,670
create three variables with the initial

00:29:49,390 --> 00:29:54,550
value at zero point one what is the

00:29:51,670 --> 00:29:56,800
variable are the tensor that the model

00:29:54,550 --> 00:30:00,310
gonna update the training session was

00:29:56,800 --> 00:30:03,510
starting to tune to make sure the output

00:30:00,310 --> 00:30:07,540
is the same as what we gave it to them

00:30:03,510 --> 00:30:08,860
so here we use 10 to flowly need low

00:30:07,540 --> 00:30:12,250
level API like say

00:30:08,860 --> 00:30:15,700
at multiply where you know those kind of

00:30:12,250 --> 00:30:20,080
low-level API you could construct a tiny

00:30:15,700 --> 00:30:22,059
graph tiny neural network graph so this

00:30:20,080 --> 00:30:24,070
looks a little bit crazy right so just

00:30:22,059 --> 00:30:25,990
for a tiny little function and just so

00:30:24,070 --> 00:30:29,080
long so we have better way to do that so

00:30:25,990 --> 00:30:33,159
we can use chain function chair Masser

00:30:29,080 --> 00:30:37,149
to make it more concise to to express

00:30:33,159 --> 00:30:38,769
the same thing here we defined a lot

00:30:37,149 --> 00:30:42,399
function remember last time we just say

00:30:38,769 --> 00:30:45,010
oh it is a categorical entropy cross

00:30:42,399 --> 00:30:47,679
entropy here we have to define our own

00:30:45,010 --> 00:30:49,840
because we wanted more control right so

00:30:47,679 --> 00:30:51,279
here is a mean square error so what it

00:30:49,840 --> 00:30:55,659
does is that calculate the difference

00:30:51,279 --> 00:31:01,570
between your model output and you know

00:30:55,659 --> 00:31:04,840
your data set output and then we use the

00:31:01,570 --> 00:31:08,470
same as the SGD function the gradient

00:31:04,840 --> 00:31:13,769
descent function - as our optimizer in

00:31:08,470 --> 00:31:17,559
the end we manually run epochs time of

00:31:13,769 --> 00:31:19,480
minimization of the last function so at

00:31:17,559 --> 00:31:22,929
the end of the day is the same as what

00:31:19,480 --> 00:31:31,919
you just saw you know internally in the

00:31:22,929 --> 00:31:33,100
high-level API so that's about it so for

00:31:31,919 --> 00:31:36,190
tensorflow

00:31:33,100 --> 00:31:38,649
j s we also part of the tender flow

00:31:36,190 --> 00:31:41,950
ecosystem we not only give you

00:31:38,649 --> 00:31:45,120
javascript also give we also integrate

00:31:41,950 --> 00:31:47,019
it into tender flow world we have

00:31:45,120 --> 00:31:48,730
intensive more tender board in the

00:31:47,019 --> 00:31:50,559
visualization to give you a

00:31:48,730 --> 00:31:53,860
visualization of how the training

00:31:50,559 --> 00:31:56,799
happens like give you a lot of charts to

00:31:53,860 --> 00:31:59,830
show how the accuracy you know increase

00:31:56,799 --> 00:32:02,409
why is the training happens so those are

00:31:59,830 --> 00:32:05,429
the at the end of the line the last line

00:32:02,409 --> 00:32:08,799
is how you plug in into the tangible

00:32:05,429 --> 00:32:10,899
visualization so busy you can see in

00:32:08,799 --> 00:32:15,070
life when you have the 10 zaboor opens

00:32:10,899 --> 00:32:16,840
oh yeah here we go we do have a graph so

00:32:15,070 --> 00:32:20,309
that's what do you see when the training

00:32:16,840 --> 00:32:20,309
happened in a previous example

00:32:21,260 --> 00:32:26,179
alright now I'll hand it back to Sandeep

00:32:23,330 --> 00:32:28,340
to talk about our user and community

00:32:26,179 --> 00:32:30,230
Thanks Thank You ping I know we're

00:32:28,340 --> 00:32:31,760
running almost out of time I just take

00:32:30,230 --> 00:32:33,799
about three four minutes and give you

00:32:31,760 --> 00:32:36,010
show you a couple more examples and also

00:32:33,799 --> 00:32:40,070
point to some resources to get started

00:32:36,010 --> 00:32:41,480
so you know tensorflow jss is a growing

00:32:40,070 --> 00:32:46,610
community as I mentioned it's an open

00:32:41,480 --> 00:32:48,710
source project we very sort of happy to

00:32:46,610 --> 00:32:50,480
see the download statistics and also

00:32:48,710 --> 00:32:52,640
seeing more and more people join as

00:32:50,480 --> 00:32:54,410
contributors more than 200 people

00:32:52,640 --> 00:32:56,600
contributing code actively to tensorflow

00:32:54,410 --> 00:32:58,520
jeaious and invite and we invite you all

00:32:56,600 --> 00:33:00,320
to become part of this and also many

00:32:58,520 --> 00:33:02,210
developers are building really powerful

00:33:00,320 --> 00:33:03,710
extensions and libraries on top of

00:33:02,210 --> 00:33:05,750
tensorflow J's so there are some

00:33:03,710 --> 00:33:08,900
examples there which let you do some

00:33:05,750 --> 00:33:12,230
some specialized custom stuff on on top

00:33:08,900 --> 00:33:14,299
of the library I won't show you three

00:33:12,230 --> 00:33:15,890
examples this first one is from near

00:33:14,299 --> 00:33:17,450
Forum and your forum is here at this

00:33:15,890 --> 00:33:19,700
conference one of the one of the

00:33:17,450 --> 00:33:21,110
sponsors of the conference they have

00:33:19,700 --> 00:33:25,010
built this really nice application

00:33:21,110 --> 00:33:26,870
called called clinic jeaious which

00:33:25,010 --> 00:33:28,880
basically is a profiling tool so it

00:33:26,870 --> 00:33:30,470
plugs into your node workloads and it

00:33:28,880 --> 00:33:32,270
helps you profile and look for

00:33:30,470 --> 00:33:35,480
performance issues memory utilization

00:33:32,270 --> 00:33:38,179
CPU consumption things like that and in

00:33:35,480 --> 00:33:40,700
this tool they run a tensor flow JS

00:33:38,179 --> 00:33:43,250
model for denoising and understanding

00:33:40,700 --> 00:33:45,320
what the what this profile data means

00:33:43,250 --> 00:33:47,270
and you can check out a demo of this at

00:33:45,320 --> 00:33:50,929
the at the near forum booth here at this

00:33:47,270 --> 00:33:54,559
conference node-red is a open source

00:33:50,929 --> 00:33:58,490
library from IBM this is a flow based

00:33:54,559 --> 00:34:01,070
way of wiring together IOT solutions

00:33:58,490 --> 00:34:04,250
right so you have a drag-and-drop model

00:34:01,070 --> 00:34:06,380
and you can create IOT workflows and

00:34:04,250 --> 00:34:08,510
node-red offers integration with

00:34:06,380 --> 00:34:10,960
tensorflow JS so all the tensorflow JS

00:34:08,510 --> 00:34:14,060
capabilities are easy drag-and-drop

00:34:10,960 --> 00:34:17,929
modules that you can use to bring ml

00:34:14,060 --> 00:34:20,929
into your iot stack and very similarly

00:34:17,929 --> 00:34:23,690
low scient is another company that

00:34:20,929 --> 00:34:26,659
builds enterprise grade IOT services and

00:34:23,690 --> 00:34:29,090
solutions and los ant has been looking

00:34:26,659 --> 00:34:32,480
at ways of incorporating tensorflow JS

00:34:29,090 --> 00:34:35,030
based prediction for client-side edge

00:34:32,480 --> 00:34:36,740
edge prediction of machine learning

00:34:35,030 --> 00:34:39,140
and they wrote a really nice blog post

00:34:36,740 --> 00:34:40,100
to show how you could use this to build

00:34:39,140 --> 00:34:43,130
like a predictive maintenance

00:34:40,100 --> 00:34:44,540
application with sensor data right so a

00:34:43,130 --> 00:34:47,060
lot of these types of examples are

00:34:44,540 --> 00:34:52,220
beginning to show the power of like an

00:34:47,060 --> 00:34:55,130
easy ml workflow in node and IOT so just

00:34:52,220 --> 00:34:56,960
in closing I wanted to show this if you

00:34:55,130 --> 00:34:59,630
have machine learning needs if you

00:34:56,960 --> 00:35:02,330
envision as as JavaScript developers or

00:34:59,630 --> 00:35:04,460
as no developers if you have certain

00:35:02,330 --> 00:35:07,130
needs or requirements from the library

00:35:04,460 --> 00:35:09,590
we would love to get your inputs there's

00:35:07,130 --> 00:35:11,240
a there's a UX our study that we are

00:35:09,590 --> 00:35:14,420
doing so you know please feel free to

00:35:11,240 --> 00:35:18,710
join us and and give us your feedback we

00:35:14,420 --> 00:35:20,570
would love to hear from you and and here

00:35:18,710 --> 00:35:22,850
are some links that are very useful for

00:35:20,570 --> 00:35:25,190
getting started of course tensorflow dot

00:35:22,850 --> 00:35:27,800
slash slash Jas that's our main website

00:35:25,190 --> 00:35:31,520
it has all of the things examples

00:35:27,800 --> 00:35:33,980
documentation tutorials and the link to

00:35:31,520 --> 00:35:35,270
the github repositories up there where

00:35:33,980 --> 00:35:38,870
we have again lot of these examples

00:35:35,270 --> 00:35:40,850
built we have a mailing list in red

00:35:38,870 --> 00:35:42,410
there if you join the mailing list again

00:35:40,850 --> 00:35:44,630
that's an excellent way to interact with

00:35:42,410 --> 00:35:47,690
other developers and directly with us on

00:35:44,630 --> 00:35:49,610
the tens of loggia steam one thing I

00:35:47,690 --> 00:35:51,500
wanted to show you is that if you go to

00:35:49,610 --> 00:35:53,960
the Google code labs and we have these

00:35:51,500 --> 00:35:55,670
running at the Google booth outside if

00:35:53,960 --> 00:35:58,160
you go to the Google code labs and

00:35:55,670 --> 00:36:00,980
search for tensorflow j/s all of our

00:35:58,160 --> 00:36:02,810
examples are available as interactive

00:36:00,980 --> 00:36:04,850
notebooks so you can click through this

00:36:02,810 --> 00:36:07,100
and basically gets to be up and running

00:36:04,850 --> 00:36:08,750
and get started and explore all these

00:36:07,100 --> 00:36:12,860
different features we talked about here

00:36:08,750 --> 00:36:15,860
today lastly there is a new textbook

00:36:12,860 --> 00:36:18,020
that has come out which is really a very

00:36:15,860 --> 00:36:19,610
nice way to learn the basics of machine

00:36:18,020 --> 00:36:21,590
learning from a JavaScript programmer

00:36:19,610 --> 00:36:24,050
point of view all the examples in this

00:36:21,590 --> 00:36:25,880
book are written in JavaScript so that's

00:36:24,050 --> 00:36:28,130
something that that's worth checking out

00:36:25,880 --> 00:36:29,900
and also we have a overarching

00:36:28,130 --> 00:36:32,750
comprehensive tensorflow course on

00:36:29,900 --> 00:36:35,090
Coursera which has a T of JS module now

00:36:32,750 --> 00:36:37,430
just released this this week earlier

00:36:35,090 --> 00:36:40,160
this week so so plenty of resources to

00:36:37,430 --> 00:36:43,340
get started and looking forward to you

00:36:40,160 --> 00:36:45,620
know your your involvement in the in the

00:36:43,340 --> 00:36:46,680
community so again thank you so much for

00:36:45,620 --> 00:36:52,290
your attention

00:36:46,680 --> 00:36:52,290

YouTube URL: https://www.youtube.com/watch?v=4ESfxF38XDs


