Title: Validating Event Driven Architecture (EDA) with AsyncAPI - Waleed Ashraf, relayr GmbH
Publication date: 2020-05-11
Playlist: Node + JS Interactive 2019
Description: 
	Validating Event Driven Architecture (EDA) with AsyncAPI - Waleed Ashraf, relayr GmbH
Speakers: Waleed Ashraf
Validating Event Driven Architecture (EDA) with AsyncAPI.

https://www.asyncapi.com/ is an open-source initiative to provide a specification for EDA through AsyncAPI. It is based on the open-api initiative which also comes under the Linux Foundation.

At relayr Gmbh, we started using AsyncAPI for Kafka message validations. So, this talk is about what we learned with our experience and how you can easily and effectively integrate the specification in your system.
Captions: 
	00:00:00,030 --> 00:00:06,720
okay hi everyone my name is leadership I

00:00:03,959 --> 00:00:09,809
work as an odious developer at RIT lair

00:00:06,720 --> 00:00:12,290
in Berlin I'm also a member of no Deus

00:00:09,809 --> 00:00:14,759
Foundation and OpenGL foundation and

00:00:12,290 --> 00:00:17,640
contributed to some open source projects

00:00:14,759 --> 00:00:19,640
so today we are talking about validating

00:00:17,640 --> 00:00:23,240
even driven architect with a sink API

00:00:19,640 --> 00:00:27,539
how many of you already know I think API

00:00:23,240 --> 00:00:31,470
ok so yeah not a lot because it's

00:00:27,539 --> 00:00:33,329
relatively a new project but it's like

00:00:31,470 --> 00:00:37,530
very useful for anyone who is using

00:00:33,329 --> 00:00:39,239
Kafka RabbitMQ and qtt or web sockets so

00:00:37,530 --> 00:00:44,879
how many of you use any of these

00:00:39,239 --> 00:00:47,899
technologies Kafka yeah so to TLDR its

00:00:44,879 --> 00:00:51,660
swagger for Kafka so you'll get the idea

00:00:47,899 --> 00:00:55,260
or what it is so just keep my definition

00:00:51,660 --> 00:00:59,190
we'll talk about it in details and let's

00:00:55,260 --> 00:01:01,859
start so my name is Wally - eff and how

00:00:59,190 --> 00:01:09,330
my like colleagues pronounced it is like

00:01:01,859 --> 00:01:11,790
this well it and yeah so yeah that's the

00:01:09,330 --> 00:01:14,340
only joke I have in my slides so you can

00:01:11,790 --> 00:01:17,670
laugh as much as you want right now the

00:01:14,340 --> 00:01:20,130
rest is pretty boring ok so I work at

00:01:17,670 --> 00:01:22,830
relayer we are a high tea company based

00:01:20,130 --> 00:01:24,869
in Berlin of our clients we provide

00:01:22,830 --> 00:01:26,909
customized solution to clients we will

00:01:24,869 --> 00:01:28,530
receive data through impurity or cough

00:01:26,909 --> 00:01:31,079
cough through their to hardware and

00:01:28,530 --> 00:01:33,630
sensors we process them on our cloud

00:01:31,079 --> 00:01:37,229
then we showed them in a fine like fancy

00:01:33,630 --> 00:01:39,000
dashboard we send them alerts emails so

00:01:37,229 --> 00:01:43,579
just to get you an idea what kind of

00:01:39,000 --> 00:01:46,890
like industry we are working in our

00:01:43,579 --> 00:01:48,509
process is like we have like some

00:01:46,890 --> 00:01:51,119
sensors on machines then there's a

00:01:48,509 --> 00:01:52,680
gateway and then we ingest the data in

00:01:51,119 --> 00:01:55,799
our cloud we will do processing

00:01:52,680 --> 00:01:57,570
analytics storage and then we have some

00:01:55,799 --> 00:01:59,100
like dashboards and mobile apps where

00:01:57,570 --> 00:02:03,180
the client can see what's happening with

00:01:59,100 --> 00:02:05,130
their devices etc so that that puts us

00:02:03,180 --> 00:02:06,420
in a situation where we have a lot of

00:02:05,130 --> 00:02:08,069
would you have to deal a lot of

00:02:06,420 --> 00:02:10,730
asynchronous communication from the

00:02:08,069 --> 00:02:13,710
devices which are sending data to MQTT

00:02:10,730 --> 00:02:18,690
Kafka or maybe through even straight

00:02:13,710 --> 00:02:21,780
the protocols so a little world or open

00:02:18,690 --> 00:02:27,120
API and swagger is it swagger was

00:02:21,780 --> 00:02:30,090
initiated in 2010 it came the the schema

00:02:27,120 --> 00:02:33,630
definition came part of open API in 2015

00:02:30,090 --> 00:02:35,820
because it called so much value and used

00:02:33,630 --> 00:02:37,890
by everyone in the industry because that

00:02:35,820 --> 00:02:40,650
was the only way or one of the only way

00:02:37,890 --> 00:02:42,480
you can validate what's in your HTTP

00:02:40,650 --> 00:02:44,640
requests what you can send in the

00:02:42,480 --> 00:02:47,160
response or Hokie you can define the

00:02:44,640 --> 00:02:49,590
schema and the other part was you can

00:02:47,160 --> 00:02:55,140
get a nice fancy documentation for your

00:02:49,590 --> 00:02:58,110
all of HTTP communications and then we

00:02:55,140 --> 00:03:01,920
have async api the the project started

00:02:58,110 --> 00:03:05,670
around 2 years ago it's been used by

00:03:01,920 --> 00:03:07,410
slack SAP some sales force some

00:03:05,670 --> 00:03:11,940
companies but it's not very popular

00:03:07,410 --> 00:03:14,280
right now so the need for this is

00:03:11,940 --> 00:03:15,900
because even the messaging the icing

00:03:14,280 --> 00:03:19,310
communication we do between the micro

00:03:15,900 --> 00:03:21,840
services is it's a contract between two

00:03:19,310 --> 00:03:24,540
two services like one is the publisher

00:03:21,840 --> 00:03:26,760
and is the subscriber and they both need

00:03:24,540 --> 00:03:28,530
to know what kind of messages one is

00:03:26,760 --> 00:03:31,350
publishing and other is subscribing so

00:03:28,530 --> 00:03:33,540
they can validate it they can when they

00:03:31,350 --> 00:03:37,650
receive it they can check the the body

00:03:33,540 --> 00:03:39,960
and on the other side the producer

00:03:37,650 --> 00:03:45,930
should know what data should send or not

00:03:39,960 --> 00:03:47,850
to send the async API also gives you an

00:03:45,930 --> 00:03:49,890
ability to create documentation which is

00:03:47,850 --> 00:03:53,400
understandable by humans it's not like a

00:03:49,890 --> 00:03:55,950
JSON but it's actually a a nice

00:03:53,400 --> 00:03:57,600
recommendation just like swagger you can

00:03:55,950 --> 00:04:01,350
read about what channels or topics you

00:03:57,600 --> 00:04:04,590
have what payload do they have yeah

00:04:01,350 --> 00:04:07,800
there's also tooling around how to use

00:04:04,590 --> 00:04:10,080
the schema definition of your async API

00:04:07,800 --> 00:04:12,270
for testing and validation like you can

00:04:10,080 --> 00:04:13,980
test on runtime in your production

00:04:12,270 --> 00:04:16,470
environment if then you receive a cough

00:04:13,980 --> 00:04:18,120
course program as Kafka mqt message that

00:04:16,470 --> 00:04:20,370
is it to allayed according to your

00:04:18,120 --> 00:04:22,290
definition you have defined and yeah it

00:04:20,370 --> 00:04:24,840
gives you ability to to create nice

00:04:22,290 --> 00:04:26,920
documentation so let me just show you

00:04:24,840 --> 00:04:29,260
how it looks like

00:04:26,920 --> 00:04:33,100
you can go to the this website

00:04:29,260 --> 00:04:35,230
playground dot async API dot IO you see

00:04:33,100 --> 00:04:37,330
here I have defined I ml a schema in the

00:04:35,230 --> 00:04:41,170
ml file just like the swagger and here I

00:04:37,330 --> 00:04:43,960
have a nice view of it just like we see

00:04:41,170 --> 00:04:46,300
in the swagger files so it has few

00:04:43,960 --> 00:04:49,780
attributes like you can define under the

00:04:46,300 --> 00:04:52,960
channels you can define your your topics

00:04:49,780 --> 00:04:55,090
or your channels for for mqtt it's the

00:04:52,960 --> 00:04:57,280
type of forum Kafka it's the type of the

00:04:55,090 --> 00:05:00,220
message like I have defined user events

00:04:57,280 --> 00:05:02,140
and then message you can just tag with

00:05:00,220 --> 00:05:04,090
the ref just like in the swagger and you

00:05:02,140 --> 00:05:06,330
can define what properties it will have

00:05:04,090 --> 00:05:09,790
what all they required what is the type

00:05:06,330 --> 00:05:13,930
and here you can see a nice example

00:05:09,790 --> 00:05:16,300
where it also generates automatically

00:05:13,930 --> 00:05:20,680
generated example and you can see this

00:05:16,300 --> 00:05:24,420
this the schema itself what are the keys

00:05:20,680 --> 00:05:24,420
and what are the restrictions on them

00:05:26,400 --> 00:05:33,220
let's go back now you have seen an

00:05:29,410 --> 00:05:35,890
example so the the problem we face with

00:05:33,220 --> 00:05:38,830
even driven architect was that we had

00:05:35,890 --> 00:05:40,420
around 30 or 40 micro services written

00:05:38,830 --> 00:05:42,310
in node and all of them were

00:05:40,420 --> 00:05:46,180
communicating internally through Kafka

00:05:42,310 --> 00:05:49,360
we had more than 50 types of messages

00:05:46,180 --> 00:05:51,700
with different payload and not everyone

00:05:49,360 --> 00:05:53,890
knew what's inside so whenever you are

00:05:51,700 --> 00:05:56,440
debugging or creating something adding

00:05:53,890 --> 00:05:58,120
some key you had to ask someone a

00:05:56,440 --> 00:06:00,280
developer who has already worked on that

00:05:58,120 --> 00:06:01,720
type of message to see or go check the

00:06:00,280 --> 00:06:03,310
code by yourself and see what's inside

00:06:01,720 --> 00:06:04,870
if you want to change it if you want to

00:06:03,310 --> 00:06:07,810
add a new attribute if you want to

00:06:04,870 --> 00:06:10,090
remove it and it was a mess no one no

00:06:07,810 --> 00:06:12,100
one knew everyone and we also have some

00:06:10,090 --> 00:06:14,080
services in Scala obvious color people

00:06:12,100 --> 00:06:15,970
are working on them and we don't

00:06:14,080 --> 00:06:18,100
communicate the like the the different

00:06:15,970 --> 00:06:20,020
teams note the backend teams don't

00:06:18,100 --> 00:06:22,540
communicate much even the front-end team

00:06:20,020 --> 00:06:24,460
don't communicate much about what's in

00:06:22,540 --> 00:06:26,710
the payload and every time they have to

00:06:24,460 --> 00:06:29,350
check they have to look at at the code

00:06:26,710 --> 00:06:31,690
or we created our like a markdown files

00:06:29,350 --> 00:06:33,820
I studied me too to keep it up to date

00:06:31,690 --> 00:06:35,860
and then problem with with such

00:06:33,820 --> 00:06:38,200
documentation is it always gets outdated

00:06:35,860 --> 00:06:40,660
no one cares about it so you have

00:06:38,200 --> 00:06:43,630
different problems and

00:06:40,660 --> 00:06:46,300
that's I think happening in all the

00:06:43,630 --> 00:06:47,890
micro service driven architects where

00:06:46,300 --> 00:06:49,780
you have multiple services and they are

00:06:47,890 --> 00:06:52,660
communicating to each other through some

00:06:49,780 --> 00:06:55,360
of these protocols and then you don't

00:06:52,660 --> 00:06:56,800
you don't know how to have one source of

00:06:55,360 --> 00:07:02,290
truth for all the communication between

00:06:56,800 --> 00:07:02,830
the services so we had something like

00:07:02,290 --> 00:07:06,070
this

00:07:02,830 --> 00:07:08,680
we had client devices which were sending

00:07:06,070 --> 00:07:11,650
data through MQTT to our cloud and then

00:07:08,680 --> 00:07:13,210
we had a broker which converts the

00:07:11,650 --> 00:07:15,190
entity message to relative kafka

00:07:13,210 --> 00:07:19,110
messages and these are our services

00:07:15,190 --> 00:07:21,910
communicating internally through Kafka

00:07:19,110 --> 00:07:22,570
so yeah there was a lot of communication

00:07:21,910 --> 00:07:25,150
going on

00:07:22,570 --> 00:07:28,570
they still is and we wanted to have

00:07:25,150 --> 00:07:30,040
something to one for the documentation

00:07:28,570 --> 00:07:32,260
and the other for to allocate all the

00:07:30,040 --> 00:07:37,120
messages it has the required payload or

00:07:32,260 --> 00:07:39,070
not so in my company with their

00:07:37,120 --> 00:07:43,150
different teams like back-end DevOps

00:07:39,070 --> 00:07:45,040
front-end and all of them needs to know

00:07:43,150 --> 00:07:47,710
the content of the message especially

00:07:45,040 --> 00:07:49,180
the back in teams and we also reside in

00:07:47,710 --> 00:07:51,220
different offices some are in Munich

00:07:49,180 --> 00:07:55,090
Simon in Berlin and different people are

00:07:51,220 --> 00:07:58,380
working on different services there's no

00:07:55,090 --> 00:08:01,060
like one documentation and also we have

00:07:58,380 --> 00:08:03,610
MQTT protocol for the devices for the

00:08:01,060 --> 00:08:06,070
client so we need to tell them also what

00:08:03,610 --> 00:08:09,550
you what you should send to our cloud in

00:08:06,070 --> 00:08:11,020
a nice very documented way and also

00:08:09,550 --> 00:08:15,640
validate if they are sending it right

00:08:11,020 --> 00:08:17,680
way or the wrong way so previously we

00:08:15,640 --> 00:08:20,170
were doing something like this we had a

00:08:17,680 --> 00:08:22,000
readme file mark now and someone would

00:08:20,170 --> 00:08:24,220
know the codebase and know what's in the

00:08:22,000 --> 00:08:26,980
payload and for the testing we had

00:08:24,220 --> 00:08:29,950
hard-coded schema in a service in the

00:08:26,980 --> 00:08:32,080
services where we just write down all

00:08:29,950 --> 00:08:34,930
the properties and match them using the

00:08:32,080 --> 00:08:37,570
result library or something - but this

00:08:34,930 --> 00:08:39,940
gets outdated because and whenever you

00:08:37,570 --> 00:08:44,650
want to update you have to like write it

00:08:39,940 --> 00:08:46,660
down as a cold hard coded message with a

00:08:44,650 --> 00:08:48,670
sync API you created two documents for

00:08:46,660 --> 00:08:51,100
one for MQTT and one for Kafka messages

00:08:48,670 --> 00:08:52,870
i've defined all of our events just like

00:08:51,100 --> 00:08:54,230
I showed you for like device on/off

00:08:52,870 --> 00:08:55,640
temperature and what

00:08:54,230 --> 00:08:59,450
internal communications you've created

00:08:55,640 --> 00:09:02,180
user updated user deleted whatever so

00:08:59,450 --> 00:09:04,790
it's all our problem for for the

00:09:02,180 --> 00:09:09,880
documentation because it gives you like

00:09:04,790 --> 00:09:12,710
a nice view too and it also generates a

00:09:09,880 --> 00:09:14,180
output file which you can easily share

00:09:12,710 --> 00:09:17,420
let me just show you how it looks like

00:09:14,180 --> 00:09:26,960
in one of our service which is public so

00:09:17,420 --> 00:09:29,090
I just go to so this is a bit old UI for

00:09:26,960 --> 00:09:32,990
the async API but this is what you get

00:09:29,090 --> 00:09:33,950
if you want to generate a St ml document

00:09:32,990 --> 00:09:35,840
for out of it

00:09:33,950 --> 00:09:38,690
so you see these are the top topics on

00:09:35,840 --> 00:09:40,610
MQTT you can define and then we have

00:09:38,690 --> 00:09:43,460
defined all the of the payload and the

00:09:40,610 --> 00:09:44,720
messages inside it and the one of like

00:09:43,460 --> 00:09:46,340
this document we can share easily with

00:09:44,720 --> 00:09:48,530
our clients and they'll see what what's

00:09:46,340 --> 00:09:53,270
inside and we can also validate using

00:09:48,530 --> 00:09:54,740
this schema and inside our services so

00:09:53,270 --> 00:09:56,510
you can see such a very nice way it

00:09:54,740 --> 00:10:02,570
gives you all the messages the schemas

00:09:56,510 --> 00:10:04,700
you have used and the topics you have so

00:10:02,570 --> 00:10:06,500
now the come about how to validate and

00:10:04,700 --> 00:10:08,810
how to test it if the message is right

00:10:06,500 --> 00:10:13,280
or not or what property it is missing or

00:10:08,810 --> 00:10:16,550
or not so we wrote this library it's

00:10:13,280 --> 00:10:18,410
open source async API validator it's a

00:10:16,550 --> 00:10:20,690
key value base schema validation you

00:10:18,410 --> 00:10:22,270
give it the key of your message and you

00:10:20,690 --> 00:10:24,500
pass the payload and it will validate

00:10:22,270 --> 00:10:27,920
according to the schema definition you

00:10:24,500 --> 00:10:30,260
have defined it uses a GV module for

00:10:27,920 --> 00:10:32,420
jason comparison maybe you're all some

00:10:30,260 --> 00:10:35,630
of you already know it also browser

00:10:32,420 --> 00:10:39,650
compatible so i'll just show you how it

00:10:35,630 --> 00:10:42,160
works so the project is already on npm

00:10:39,650 --> 00:10:46,670
we use it widely in our services

00:10:42,160 --> 00:10:50,870
it's a validator you can go to the

00:10:46,670 --> 00:10:53,540
github or and just check it out so i'll

00:10:50,870 --> 00:10:56,180
just show you how it how it works so i

00:10:53,540 --> 00:10:59,960
have required the validator on top and

00:10:56,180 --> 00:11:03,980
then i'm i need to pass the schema which

00:10:59,960 --> 00:11:06,140
is which is this file the schema is same

00:11:03,980 --> 00:11:08,420
as i showed you in the example for the

00:11:06,140 --> 00:11:11,879
user event so i have defined the scheme

00:11:08,420 --> 00:11:13,769
we have these properties on a message

00:11:11,879 --> 00:11:19,110
and the message key is user-created

00:11:13,769 --> 00:11:21,959
let's say so I load the schema on this

00:11:19,110 --> 00:11:25,620
line and then I ask the validator to

00:11:21,959 --> 00:11:28,019
elevate this key of the message and the

00:11:25,620 --> 00:11:33,420
payload and then I log the result and

00:11:28,019 --> 00:11:37,620
let's see how how it works for this so

00:11:33,420 --> 00:11:40,740
it says data user name should match for

00:11:37,620 --> 00:11:46,980
my email and user Creator is the key so

00:11:40,740 --> 00:11:48,899
if I go and check the schema or I think

00:11:46,980 --> 00:11:51,029
it was the user name so the format is

00:11:48,899 --> 00:11:54,600
email because these types are defined as

00:11:51,029 --> 00:11:56,100
like normal JSON schema definition so

00:11:54,600 --> 00:11:58,470
you can just have it as the swagger file

00:11:56,100 --> 00:12:03,319
at the swagger definitions all of them

00:11:58,470 --> 00:12:05,639
like tie format or a patron so if I just

00:12:03,319 --> 00:12:09,389
just change it because I think it's

00:12:05,639 --> 00:12:15,589
missing an aunt and then just run it

00:12:09,389 --> 00:12:17,879
again okay so it's it's passing and

00:12:15,589 --> 00:12:20,519
let's see if there's any required

00:12:17,879 --> 00:12:22,139
property so this the ID is a required

00:12:20,519 --> 00:12:25,139
property in the message with which is

00:12:22,139 --> 00:12:27,959
mostly if you use like Kafka or some

00:12:25,139 --> 00:12:30,600
similar protocol so if you if you let's

00:12:27,959 --> 00:12:33,870
say if one of the message didn't send an

00:12:30,600 --> 00:12:36,899
ID and the rest of the payload was okay

00:12:33,870 --> 00:12:39,360
so yeah so it will tell you that yeah

00:12:36,899 --> 00:12:41,309
this property is required just like just

00:12:39,360 --> 00:12:43,470
like you have on on swagger definitions

00:12:41,309 --> 00:12:45,930
if you use it for validations

00:12:43,470 --> 00:12:49,170
so yeah this gave us a really good

00:12:45,930 --> 00:12:51,420
opportunity to an ability to validate

00:12:49,170 --> 00:12:53,790
our messages which are coming through

00:12:51,420 --> 00:12:55,589
the client-side because we were facing a

00:12:53,790 --> 00:12:58,170
lot of issues when we didn't know what's

00:12:55,589 --> 00:12:59,550
inside the message and how to debug or

00:12:58,170 --> 00:13:02,309
how to tell them what was the missing

00:12:59,550 --> 00:13:06,569
and also give them a nice documentation

00:13:02,309 --> 00:13:11,519
which they can use it so let's move

00:13:06,569 --> 00:13:14,879
forward so we started use it we started

00:13:11,519 --> 00:13:18,269
using it in production we made a private

00:13:14,879 --> 00:13:21,329
package of over a sink a PS schema so or

00:13:18,269 --> 00:13:23,790
like Kafka mqt schema we would push them

00:13:21,329 --> 00:13:27,629
into a private repo and on radhaji oh

00:13:23,790 --> 00:13:31,410
and we edited it as a dependency in the

00:13:27,629 --> 00:13:34,499
services and then used the validator to

00:13:31,410 --> 00:13:36,540
validate the messages so on we also

00:13:34,499 --> 00:13:37,649
doing it on run time and production for

00:13:36,540 --> 00:13:39,329
all the messages which are coming

00:13:37,649 --> 00:13:41,129
through the client side but for the

00:13:39,329 --> 00:13:42,869
messages internal communication like

00:13:41,129 --> 00:13:45,299
Kafka we only do it when running the

00:13:42,869 --> 00:13:47,279
unit test or when doing integration

00:13:45,299 --> 00:13:53,189
tests between the services not on on

00:13:47,279 --> 00:13:56,100
production live environment so the flow

00:13:53,189 --> 00:13:58,529
was like this so we have we consume the

00:13:56,100 --> 00:14:00,269
messages and we will forward it to the

00:13:58,529 --> 00:14:03,629
previously it was like this we forwarded

00:14:00,269 --> 00:14:05,009
to the relative service so with when we

00:14:03,629 --> 00:14:06,600
clear when we started consuming we

00:14:05,009 --> 00:14:10,379
started validating also venerating the

00:14:06,600 --> 00:14:14,670
message rule and if there's no error if

00:14:10,379 --> 00:14:17,160
the message schemas is is right if it's

00:14:14,670 --> 00:14:19,350
not valid we just log the errors but

00:14:17,160 --> 00:14:20,459
don't don't fail it or don't send an

00:14:19,350 --> 00:14:22,499
error back to the client

00:14:20,459 --> 00:14:25,439
just just log it and see it in the logs

00:14:22,499 --> 00:14:26,970
if there was something missing and if

00:14:25,439 --> 00:14:29,999
it's a valid message then just forward

00:14:26,970 --> 00:14:33,689
it to relative service or inside the

00:14:29,999 --> 00:14:35,279
cloud somewhere and even if there's an

00:14:33,689 --> 00:14:37,079
error it's not a valid message we'll

00:14:35,279 --> 00:14:38,759
still forward it to the up service the

00:14:37,079 --> 00:14:41,489
just so this was the flow when we

00:14:38,759 --> 00:14:43,439
started using the async API how to add

00:14:41,489 --> 00:14:45,899
how to get it inside the running

00:14:43,439 --> 00:14:49,499
production environment so we just

00:14:45,899 --> 00:14:51,809
validate log it and rest of the roost

00:14:49,499 --> 00:14:55,919
case remains the same however it was

00:14:51,809 --> 00:14:57,869
working before after like going through

00:14:55,919 --> 00:15:00,239
one or two weeks of process and seeing

00:14:57,869 --> 00:15:01,829
all the logs we saw that sometimes it

00:15:00,239 --> 00:15:03,540
was our schema which was not right and

00:15:01,829 --> 00:15:05,449
some time it was the payload the client

00:15:03,540 --> 00:15:07,439
was sending was not right so we we made

00:15:05,449 --> 00:15:09,749
tweaks in the in the schema definition

00:15:07,439 --> 00:15:12,089
and sometimes we communicated back with

00:15:09,749 --> 00:15:14,939
the client and asked them to change the

00:15:12,089 --> 00:15:17,669
payload and once that was done

00:15:14,939 --> 00:15:19,980
everything was fine we we started

00:15:17,669 --> 00:15:22,319
sending the error events to or

00:15:19,980 --> 00:15:24,480
forwarding we just like stopped it so

00:15:22,319 --> 00:15:26,399
when if the scheme of erudition says we

00:15:24,480 --> 00:15:29,009
throw and edit back to the client or

00:15:26,399 --> 00:15:31,949
don't don't let it go into our system so

00:15:29,009 --> 00:15:34,410
it was this move was pretty easy for us

00:15:31,949 --> 00:15:39,209
to to adopt the schema inside our

00:15:34,410 --> 00:15:43,949
reduction in my brain so the use cases

00:15:39,209 --> 00:15:46,259
for for us over was are pretty obvious

00:15:43,949 --> 00:15:48,259
it was the validating of all the

00:15:46,259 --> 00:15:51,980
communication we did for documentation

00:15:48,259 --> 00:15:55,199
we use it for the our system test and

00:15:51,980 --> 00:15:57,959
when now we also introduce this process

00:15:55,199 --> 00:16:00,449
of if you if a developer wants to extend

00:15:57,959 --> 00:16:03,180
a message remove a key from existing

00:16:00,449 --> 00:16:05,759
messages or want a different type of

00:16:03,180 --> 00:16:08,759
channel or topic you first open appear

00:16:05,759 --> 00:16:10,319
in the schema definition repo everyone's

00:16:08,759 --> 00:16:12,329
approves it the team approves it and

00:16:10,319 --> 00:16:15,000
then you start working on actually

00:16:12,329 --> 00:16:16,769
making the changes in the services so it

00:16:15,000 --> 00:16:18,569
gave us a really nice way how to

00:16:16,769 --> 00:16:21,149
implement or make changes in the

00:16:18,569 --> 00:16:23,579
existing messages otherwise it was

00:16:21,149 --> 00:16:25,560
previously someone would be working on

00:16:23,579 --> 00:16:27,480
the code itself and then reviewing and

00:16:25,560 --> 00:16:29,370
people would see oh ok this properties

00:16:27,480 --> 00:16:32,910
remove these properties added these are

00:16:29,370 --> 00:16:34,920
the checks on it so now we first get

00:16:32,910 --> 00:16:36,600
settled on the schema definition how the

00:16:34,920 --> 00:16:39,480
messages will look once that's approved

00:16:36,600 --> 00:16:42,329
you start working on the on the code

00:16:39,480 --> 00:16:45,269
itself so this saved us a lot of time in

00:16:42,329 --> 00:16:47,310
reviewing the PRS and actually

00:16:45,269 --> 00:16:49,319
developing new kind of messages it's

00:16:47,310 --> 00:16:51,089
just like if you do for the swagger

00:16:49,319 --> 00:16:53,069
first you change the swagger and

00:16:51,089 --> 00:16:54,540
introduce a new endpoint and everyone is

00:16:53,069 --> 00:16:58,680
ok with it then you start developing

00:16:54,540 --> 00:17:01,500
further on so for the external use case

00:16:58,680 --> 00:17:03,060
it helped us for a lot for the clients

00:17:01,500 --> 00:17:05,490
because previously we were managing our

00:17:03,060 --> 00:17:08,760
document by ourselves to share with them

00:17:05,490 --> 00:17:12,480
and now it also gives them a really nice

00:17:08,760 --> 00:17:15,150
error about what key is not a valid key

00:17:12,480 --> 00:17:17,549
and what type of properties it should

00:17:15,150 --> 00:17:19,949
have like it should be an email or it

00:17:17,549 --> 00:17:22,110
should have length less than 64 or

00:17:19,949 --> 00:17:27,679
something so it was easy for them also

00:17:22,110 --> 00:17:30,210
to fix if there's something is broken so

00:17:27,679 --> 00:17:31,770
when I started working when we started

00:17:30,210 --> 00:17:34,500
working I also got started contributing

00:17:31,770 --> 00:17:37,860
to a sync API the schema definition it's

00:17:34,500 --> 00:17:41,880
a very nodes getting very widely used in

00:17:37,860 --> 00:17:43,890
a lot of companies like slack and we

00:17:41,880 --> 00:17:46,260
just released version 2 of schema which

00:17:43,890 --> 00:17:48,300
which have a lot of different properties

00:17:46,260 --> 00:17:51,420
you can you can define custom

00:17:48,300 --> 00:17:53,450
bindings for protocols you know it's

00:17:51,420 --> 00:17:56,370
supposed different kind of schemas and

00:17:53,450 --> 00:17:59,730
you can do like channel based validation

00:17:56,370 --> 00:18:02,720
so we also have a slack channel for

00:17:59,730 --> 00:18:04,830
async API we also have bi-weekly

00:18:02,720 --> 00:18:08,280
meetings on YouTube you which you can

00:18:04,830 --> 00:18:12,240
anyone can join and yet open-source you

00:18:08,280 --> 00:18:13,920
can contribute in any kind of way and we

00:18:12,240 --> 00:18:16,110
are also working on a lot of tooling

00:18:13,920 --> 00:18:18,000
around the schema definition right now

00:18:16,110 --> 00:18:19,800
just like this like this validator we

00:18:18,000 --> 00:18:22,620
are working on generators which will

00:18:19,800 --> 00:18:25,050
generate the code from your defined

00:18:22,620 --> 00:18:26,700
schema and so if you define your schema

00:18:25,050 --> 00:18:29,640
for different messages you can generate

00:18:26,700 --> 00:18:31,400
in JavaScript or Java code in different

00:18:29,640 --> 00:18:33,900
languages through the schema definition

00:18:31,400 --> 00:18:37,320
just like you you have support for

00:18:33,900 --> 00:18:39,030
swagger which you can generate and so

00:18:37,320 --> 00:18:43,830
yeah there's a lot of opportunity for

00:18:39,030 --> 00:18:48,450
tooling around this right now so yeah

00:18:43,830 --> 00:18:52,010
that's it you have any question let me

00:18:48,450 --> 00:18:52,010
know so

00:18:52,820 --> 00:18:57,299
you can reach out to me on Twitter you

00:18:55,289 --> 00:18:59,039
can check these links if you have any

00:18:57,299 --> 00:19:01,609
questions and this is let me know if you

00:18:59,039 --> 00:19:01,609

YouTube URL: https://www.youtube.com/watch?v=_pEQlkA1g3U


