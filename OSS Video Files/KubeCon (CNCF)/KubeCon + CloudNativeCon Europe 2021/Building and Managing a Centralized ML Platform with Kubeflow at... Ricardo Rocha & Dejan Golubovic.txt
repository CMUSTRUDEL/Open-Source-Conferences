Title: Building and Managing a Centralized ML Platform with Kubeflow at... Ricardo Rocha & Dejan Golubovic
Publication date: 2021-05-09
Playlist: KubeCon + CloudNativeCon Europe 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Building and Managing a Centralized ML Platform with Kubeflow at CERN - Ricardo Rocha & Dejan Golubovic, CERN

CERN’s main mission is to expand human knowledge trying to understand the nature of the universe, and machine learning has been growing as a solution for challenges in different areas of development and operations. Areas where ML is being looked at include particle classification using graph neural networks during reconstruction, 3DGANs for much faster generation of simulation data, or reinforced learning for beam calibration. This session presents a recently introduced centralized service covering most use cases, handling data preparation, model training and serving. How it tries to improve resource usage (especially important when handling scarce resources such as accelerators) by offering different resource types (GPU, vGPU, TPU) for each use case. The session will also describe our journey with Kubeflow, the machine learning platform running on top of Kubernetes, and how we integrated on-premises resources and the different possibilities being looked at to extend to public clouds.
Captions: 
	00:00:00,960 --> 00:00:05,440
hello everyone welcome to um

00:00:03,360 --> 00:00:07,120
our talk on building and managing a

00:00:05,440 --> 00:00:08,960
centralized machine learning platform

00:00:07,120 --> 00:00:10,800
with cook flow at cern

00:00:08,960 --> 00:00:12,400
uh we'll be talking about some work

00:00:10,800 --> 00:00:17,920
we've been doing the last few months

00:00:12,400 --> 00:00:17,920
and a service that we open to our users

00:00:18,160 --> 00:00:22,320
hello my name is dan gulovus i am a

00:00:20,320 --> 00:00:25,039
computing engineer in ceramcloud

00:00:22,320 --> 00:00:25,920
team my focus is on machine learning

00:00:25,039 --> 00:00:27,920
infrastructure

00:00:25,920 --> 00:00:29,199
services with kubernetes and i will

00:00:27,920 --> 00:00:30,640
present this talk with my colleague

00:00:29,199 --> 00:00:33,120
ricardo

00:00:30,640 --> 00:00:34,960
my name is ricardo i'm a computer

00:00:33,120 --> 00:00:37,840
engineer also in the cern cloud team i

00:00:34,960 --> 00:00:40,000
focus mostly on containers networking

00:00:37,840 --> 00:00:41,040
and more recently gpu's accelerators and

00:00:40,000 --> 00:00:43,040
also machine learning

00:00:41,040 --> 00:00:44,800
and i'm also a member of the technical

00:00:43,040 --> 00:00:48,399
oversight committee of the cncf

00:00:44,800 --> 00:00:50,960
as an end-user representative

00:00:48,399 --> 00:00:52,559
so today we'll we'll give a talk about

00:00:50,960 --> 00:00:55,120
uh service at cern but

00:00:52,559 --> 00:00:56,640
just a very quick overview of what cern

00:00:55,120 --> 00:00:58,320
is about so cern is the european

00:00:56,640 --> 00:00:59,039
laboratory for particle physics the

00:00:58,320 --> 00:01:01,680
largest

00:00:59,039 --> 00:01:02,640
particle physics laboratory in the world

00:01:01,680 --> 00:01:04,479
and we build like

00:01:02,640 --> 00:01:06,320
large scientific machines that allow us

00:01:04,479 --> 00:01:08,000
to do fundamental research

00:01:06,320 --> 00:01:09,760
the largest we have is the large hadron

00:01:08,000 --> 00:01:12,880
collider you probably heard of it

00:01:09,760 --> 00:01:15,439
it's a 27 kilometer perimeter

00:01:12,880 --> 00:01:17,040
particle accelerator that is 100 meters

00:01:15,439 --> 00:01:18,640
in the ground and where we accelerate

00:01:17,040 --> 00:01:20,560
two beams of protons

00:01:18,640 --> 00:01:21,840
to very close to the speed of light and

00:01:20,560 --> 00:01:24,320
we make them collide at

00:01:21,840 --> 00:01:26,400
very specific points where we build

00:01:24,320 --> 00:01:27,920
large experiments and you see here cms

00:01:26,400 --> 00:01:30,400
lhcp atlas and

00:01:27,920 --> 00:01:32,159
alice to have an idea of the size you

00:01:30,400 --> 00:01:35,840
can see the geneva airport here

00:01:32,159 --> 00:01:37,119
on the picture this is a an image of the

00:01:35,840 --> 00:01:39,040
accelerator itself

00:01:37,119 --> 00:01:40,479
in the tunnel and you can see all the

00:01:39,040 --> 00:01:43,520
magnets that help us beam

00:01:40,479 --> 00:01:45,119
the uh bend the beam so that it

00:01:43,520 --> 00:01:48,320
circulates in the

00:01:45,119 --> 00:01:48,640
in the accelerator and this is a picture

00:01:48,320 --> 00:01:51,280
of

00:01:48,640 --> 00:01:53,680
um of one of the detectors the cms

00:01:51,280 --> 00:01:56,000
detector compact moon solenoid

00:01:53,680 --> 00:01:57,360
it's uh in a cavern 40 meters by 40

00:01:56,000 --> 00:01:59,360
meters also 100

00:01:57,360 --> 00:02:02,399
meters underground and this is where we

00:01:59,360 --> 00:02:04,640
make the proton beams collide

00:02:02,399 --> 00:02:06,880
this detector and the others as well act

00:02:04,640 --> 00:02:09,280
like gigantic cameras where we take

00:02:06,880 --> 00:02:10,160
something like 40 million uh pictures a

00:02:09,280 --> 00:02:12,239
second

00:02:10,160 --> 00:02:13,520
and the result of this is a large amount

00:02:12,239 --> 00:02:16,959
of data that we need to

00:02:13,520 --> 00:02:19,920
store and analyze um we we collect and

00:02:16,959 --> 00:02:20,959
and store more than 70 petabytes of data

00:02:19,920 --> 00:02:24,160
every year

00:02:20,959 --> 00:02:26,319
and this is after a lot of filtering uh

00:02:24,160 --> 00:02:27,920
one of one detector like this can

00:02:26,319 --> 00:02:31,519
generate something like one petabyte

00:02:27,920 --> 00:02:31,840
of data per second uh so that's why we

00:02:31,519 --> 00:02:34,080
are

00:02:31,840 --> 00:02:35,440
constantly looking to new technologies

00:02:34,080 --> 00:02:39,360
that can help us

00:02:35,440 --> 00:02:39,360
handle this amount of data

00:02:40,239 --> 00:02:44,959
so the main motivation for our service

00:02:42,959 --> 00:02:47,200
is the expanded usage of machine

00:02:44,959 --> 00:02:49,280
learning in high energy physics

00:02:47,200 --> 00:02:51,120
different groups at cern work on various

00:02:49,280 --> 00:02:51,599
machine learning projects in order to

00:02:51,120 --> 00:02:53,840
achieve

00:02:51,599 --> 00:02:55,200
scientific goals of the large hadron

00:02:53,840 --> 00:02:57,920
collider

00:02:55,200 --> 00:02:58,640
and we know that setting up and managing

00:02:57,920 --> 00:03:01,519
machine learning

00:02:58,640 --> 00:03:02,480
infrastructure is not an easy task and

00:03:01,519 --> 00:03:04,400
currently

00:03:02,480 --> 00:03:06,000
most groups at cern manage their own

00:03:04,400 --> 00:03:07,840
machine learning infrastructure so we

00:03:06,000 --> 00:03:10,000
have four main experiments which all

00:03:07,840 --> 00:03:12,560
branch to different groups and

00:03:10,000 --> 00:03:13,599
that means that a lot of people use

00:03:12,560 --> 00:03:16,400
their own

00:03:13,599 --> 00:03:18,640
machine learning infrastructure we want

00:03:16,400 --> 00:03:21,120
to offer a centralized place

00:03:18,640 --> 00:03:23,120
essentially service in order to reduce

00:03:21,120 --> 00:03:24,959
physicists efforts in infrastructure

00:03:23,120 --> 00:03:27,440
and to allow more time for scientific

00:03:24,959 --> 00:03:27,440
research

00:03:28,879 --> 00:03:32,799
one of the main applications of machine

00:03:31,120 --> 00:03:33,840
learning at cern is in particular

00:03:32,799 --> 00:03:35,680
reconstruction

00:03:33,840 --> 00:03:37,440
so during proton proton collisions

00:03:35,680 --> 00:03:39,920
short-lived particles are created in the

00:03:37,440 --> 00:03:41,360
detectors for example higgs boson which

00:03:39,920 --> 00:03:44,480
leaves

00:03:41,360 --> 00:03:46,959
10 to the minus 22 seconds

00:03:44,480 --> 00:03:47,680
and to capture the events of the

00:03:46,959 --> 00:03:50,239
short-lived

00:03:47,680 --> 00:03:51,840
particles we measure energy depositions

00:03:50,239 --> 00:03:54,000
in the detectors

00:03:51,840 --> 00:03:54,879
detectors can be considered as 3d

00:03:54,000 --> 00:03:57,280
cameras

00:03:54,879 --> 00:03:59,519
which leaves the opportunity to use

00:03:57,280 --> 00:04:01,920
convolutional neural networks

00:03:59,519 --> 00:04:02,799
besides convolutional neural networks we

00:04:01,920 --> 00:04:04,799
can use

00:04:02,799 --> 00:04:06,319
graph neural networks which are also

00:04:04,799 --> 00:04:09,680
very good at spatial

00:04:06,319 --> 00:04:13,040
representation so the example would be

00:04:09,680 --> 00:04:15,040
to take the output of the detector and

00:04:13,040 --> 00:04:16,959
let that be an input to a network and

00:04:15,040 --> 00:04:18,880
the output of the network would be

00:04:16,959 --> 00:04:21,280
the id of the particle whether it's a

00:04:18,880 --> 00:04:24,400
higgs boson or a muon or a pion

00:04:21,280 --> 00:04:26,720
for example and now lots of research is

00:04:24,400 --> 00:04:30,000
going towards graph neural networks

00:04:26,720 --> 00:04:31,120
uh another uh application is in detector

00:04:30,000 --> 00:04:33,199
simulations

00:04:31,120 --> 00:04:34,160
so large hadron collider is getting

00:04:33,199 --> 00:04:36,320
upgraded

00:04:34,160 --> 00:04:37,199
there will be more even more data in the

00:04:36,320 --> 00:04:39,280
future

00:04:37,199 --> 00:04:41,040
and more sophisticated and faster

00:04:39,280 --> 00:04:43,199
solutions are needed to support the

00:04:41,040 --> 00:04:46,400
upgrade from various perspectives

00:04:43,199 --> 00:04:48,560
one of them being simulations so uh

00:04:46,400 --> 00:04:50,320
simulations are performed to so that we

00:04:48,560 --> 00:04:53,040
can accurately

00:04:50,320 --> 00:04:55,199
uh estimate uh what is going to happen

00:04:53,040 --> 00:04:57,600
during the runs

00:04:55,199 --> 00:04:58,639
and the the traditional methods are

00:04:57,600 --> 00:05:02,240
monte carlo

00:04:58,639 --> 00:05:05,120
simulations but recently 3d

00:05:02,240 --> 00:05:05,759
uh guns have started to be more commonly

00:05:05,120 --> 00:05:08,000
used

00:05:05,759 --> 00:05:09,520
and they have proved to have a similar

00:05:08,000 --> 00:05:10,479
performance to state state-of-the-art

00:05:09,520 --> 00:05:13,120
monte carlo

00:05:10,479 --> 00:05:14,000
and they offer 20 000 times faster

00:05:13,120 --> 00:05:16,240
simulation

00:05:14,000 --> 00:05:17,600
and also with 3d gun data can be

00:05:16,240 --> 00:05:20,720
simulated on the fly

00:05:17,600 --> 00:05:22,840
which may reduce the need for storing

00:05:20,720 --> 00:05:25,840
the

00:05:22,840 --> 00:05:25,840
data

00:05:26,479 --> 00:05:30,639
so our goal is to set up a platform to

00:05:29,280 --> 00:05:32,560
support

00:05:30,639 --> 00:05:33,759
the end-to-end machine learning life

00:05:32,560 --> 00:05:35,520
cycles

00:05:33,759 --> 00:05:38,400
we want to be able to extract data from

00:05:35,520 --> 00:05:40,800
the detectors through spark or hdfs

00:05:38,400 --> 00:05:42,560
and operate on that data then we want

00:05:40,800 --> 00:05:43,440
fast iteration services such as

00:05:42,560 --> 00:05:46,720
notebooks

00:05:43,440 --> 00:05:49,440
because many users use notebooks daily

00:05:46,720 --> 00:05:50,479
at least the notebooks are a good

00:05:49,440 --> 00:05:53,039
starting point for

00:05:50,479 --> 00:05:55,440
every machine learning user then for

00:05:53,039 --> 00:05:57,039
more computationally extensive jobs we

00:05:55,440 --> 00:05:59,600
want to be able to perform

00:05:57,039 --> 00:06:00,800
distributed training with tensorflow or

00:05:59,600 --> 00:06:04,000
by torch

00:06:00,800 --> 00:06:04,639
and even we want to branch out to public

00:06:04,000 --> 00:06:08,000
cloud

00:06:04,639 --> 00:06:09,039
when resources are needed so then after

00:06:08,000 --> 00:06:11,039
the training

00:06:09,039 --> 00:06:12,960
we want to store our models and to be

00:06:11,039 --> 00:06:17,919
able to perform scalable

00:06:12,960 --> 00:06:17,919
serving for for the trained models

00:06:18,840 --> 00:06:24,000
cool

00:06:21,759 --> 00:06:25,759
so uh the platform that supports all of

00:06:24,000 --> 00:06:27,600
our goals is cube flow

00:06:25,759 --> 00:06:29,680
uh basically with kubeflow we are

00:06:27,600 --> 00:06:32,720
utilizing power of kubernetes

00:06:29,680 --> 00:06:33,120
to efficiently manage uh resources and

00:06:32,720 --> 00:06:36,319
we

00:06:33,120 --> 00:06:37,280
also offer users there all the desired

00:06:36,319 --> 00:06:39,600
features

00:06:37,280 --> 00:06:41,039
uh the infrastructure part of cube flow

00:06:39,600 --> 00:06:43,680
is managed by our cloud

00:06:41,039 --> 00:06:46,800
team and our users are physicists and

00:06:43,680 --> 00:06:48,720
scientists across the entire ser

00:06:46,800 --> 00:06:50,800
with kubeflow we can offer notebooks

00:06:48,720 --> 00:06:51,840
pipelines distributed training model

00:06:50,800 --> 00:06:53,840
serving

00:06:51,840 --> 00:06:55,919
and we can also offer bursting to public

00:06:53,840 --> 00:06:58,080
cloud when necessary

00:06:55,919 --> 00:07:00,400
and that means that basically all of our

00:06:58,080 --> 00:07:02,319
use cases are covered by kubeflow

00:07:00,400 --> 00:07:04,240
and ricardo will now discuss our setup

00:07:02,319 --> 00:07:07,120
and challenges in terms of

00:07:04,240 --> 00:07:07,520
setting up our cube flow instance yeah

00:07:07,120 --> 00:07:09,120
so

00:07:07,520 --> 00:07:11,520
i'll pick up on the nice description

00:07:09,120 --> 00:07:12,000
from there and before he does a cool

00:07:11,520 --> 00:07:14,720
demo

00:07:12,000 --> 00:07:16,400
i'll just uh talk about the layout of

00:07:14,720 --> 00:07:17,520
the infrastructure we are using so this

00:07:16,400 --> 00:07:20,080
is a very

00:07:17,520 --> 00:07:21,520
simplified overview of the clusters the

00:07:20,080 --> 00:07:23,919
layout of our clusters

00:07:21,520 --> 00:07:24,880
so we rely on as an entry point load

00:07:23,919 --> 00:07:27,599
balancer

00:07:24,880 --> 00:07:29,120
and this allows us to to simplify the

00:07:27,599 --> 00:07:30,240
deployment and for example to do

00:07:29,120 --> 00:07:32,720
upgrades by just

00:07:30,240 --> 00:07:34,720
uh adding entry points to the balancer

00:07:32,720 --> 00:07:36,800
new clusters on the back end

00:07:34,720 --> 00:07:38,000
and then there's a gateway that is our

00:07:36,800 --> 00:07:40,560
ingress gateway to

00:07:38,000 --> 00:07:44,560
to the services uh the main important

00:07:40,560 --> 00:07:47,919
bit here is that we have three types of

00:07:44,560 --> 00:07:50,560
nodes the first type is virtual gpus

00:07:47,919 --> 00:07:51,759
this is something that allows us to have

00:07:50,560 --> 00:07:54,240
a large amount of

00:07:51,759 --> 00:07:55,199
gpu resources although they are not as

00:07:54,240 --> 00:07:57,919
performance

00:07:55,199 --> 00:07:58,639
as having a full gpu but it allows us to

00:07:57,919 --> 00:08:00,639
have a much

00:07:58,639 --> 00:08:02,479
larger amount of resources for things

00:08:00,639 --> 00:08:06,000
like notebooks for example and we rely

00:08:02,479 --> 00:08:09,039
on t4s with time sharing in this case

00:08:06,000 --> 00:08:12,400
and then we have the pc pci bathroom

00:08:09,039 --> 00:08:15,120
note group type and here we it's mostly

00:08:12,400 --> 00:08:17,039
used for things like pipelines or or um

00:08:15,120 --> 00:08:18,160
distributed training hyperparameter

00:08:17,039 --> 00:08:20,160
optimization

00:08:18,160 --> 00:08:22,319
and also multiple serving where you you

00:08:20,160 --> 00:08:24,960
want to guarantee a certain latency for

00:08:22,319 --> 00:08:26,080
for the model serving uh we do not do

00:08:24,960 --> 00:08:27,840
today any kind of

00:08:26,080 --> 00:08:29,440
faster interconnect or anti-link or

00:08:27,840 --> 00:08:31,039
anything like this and the last bit we

00:08:29,440 --> 00:08:32,560
have here is cpu

00:08:31,039 --> 00:08:34,479
and in this case we have a much larger

00:08:32,560 --> 00:08:35,760
amount of resources it's not as

00:08:34,479 --> 00:08:37,120
interesting if you're doing deep

00:08:35,760 --> 00:08:39,200
learning but actually this platform

00:08:37,120 --> 00:08:41,279
ended up being used for other purposes

00:08:39,200 --> 00:08:42,719
as well where workflows and pipelines

00:08:41,279 --> 00:08:45,120
can be useful

00:08:42,719 --> 00:08:46,880
so you can see that we have something on

00:08:45,120 --> 00:08:51,279
the order of hundreds of

00:08:46,880 --> 00:08:53,440
virtual gpus or the tens of full

00:08:51,279 --> 00:08:55,040
gpus offered to the users a number of

00:08:53,440 --> 00:08:58,240
thousands of

00:08:55,040 --> 00:09:00,560
cpus just very quickly

00:08:58,240 --> 00:09:02,000
our deployment is based on uh kubernetes

00:09:00,560 --> 00:09:05,200
118 clusters today

00:09:02,000 --> 00:09:06,880
we use kubeflow 1 1 still and one

00:09:05,200 --> 00:09:08,800
difference from the standard 1 1

00:09:06,880 --> 00:09:11,839
deployment is that we upgraded this to

00:09:08,800 --> 00:09:13,519
one five and k native to zero fifteen

00:09:11,839 --> 00:09:15,200
all the clusters and the deployments are

00:09:13,519 --> 00:09:18,240
managed using uh

00:09:15,200 --> 00:09:20,560
githubs and we have a one repository

00:09:18,240 --> 00:09:22,320
where we define all the services and all

00:09:20,560 --> 00:09:24,480
the environments we support and it's all

00:09:22,320 --> 00:09:27,279
managed by argo cd

00:09:24,480 --> 00:09:29,600
there is one very good feature here

00:09:27,279 --> 00:09:32,320
which is uh argo cd allows us to

00:09:29,600 --> 00:09:34,080
use customize just for the flow

00:09:32,320 --> 00:09:37,040
deployment and then for the other

00:09:34,080 --> 00:09:38,160
components we rely on the operators for

00:09:37,040 --> 00:09:41,200
both the sto and

00:09:38,160 --> 00:09:42,800
nvidia gpu operator deployments and then

00:09:41,200 --> 00:09:45,040
for prometheus gay native

00:09:42,800 --> 00:09:48,720
cert manager we are relying on home

00:09:45,040 --> 00:09:48,720
charts upstream health charts

00:09:49,120 --> 00:09:52,880
we one of the key aspects is the

00:09:51,120 --> 00:09:54,480
integrations we do with the internal

00:09:52,880 --> 00:09:56,720
cern services so

00:09:54,480 --> 00:09:58,800
the first one is identity authorization

00:09:56,720 --> 00:09:59,360
and authentication and we link this to

00:09:58,800 --> 00:10:02,959
the cern

00:09:59,360 --> 00:10:06,399
sso we use uh which is based on

00:10:02,959 --> 00:10:07,519
key cloak so this allows us to have not

00:10:06,399 --> 00:10:09,600
only the

00:10:07,519 --> 00:10:11,600
tokens that identify the user but also

00:10:09,600 --> 00:10:13,360
the mapping of the users to the roles

00:10:11,600 --> 00:10:16,079
and the groups they belong to

00:10:13,360 --> 00:10:16,399
and what we do in our clusters is we we

00:10:16,079 --> 00:10:19,839
have

00:10:16,399 --> 00:10:21,680
dedicated namespaces per user where

00:10:19,839 --> 00:10:23,680
people have a default quota that is

00:10:21,680 --> 00:10:26,320
fixed and cannot be changed

00:10:23,680 --> 00:10:28,160
but also we have additional groups where

00:10:26,320 --> 00:10:29,920
people can belong to and this is defined

00:10:28,160 --> 00:10:30,720
in the cern identity if they belong to

00:10:29,920 --> 00:10:32,399
these groups

00:10:30,720 --> 00:10:34,320
and in those groups they can request

00:10:32,399 --> 00:10:35,760
additional quota like more gpus for

00:10:34,320 --> 00:10:39,200
example

00:10:35,760 --> 00:10:40,800
um and then the other very important

00:10:39,200 --> 00:10:41,760
part is the integration with our storage

00:10:40,800 --> 00:10:44,079
systems as we

00:10:41,760 --> 00:10:46,880
we mentioned like data is a key aspect

00:10:44,079 --> 00:10:48,880
of everything we do uh so we integrate

00:10:46,880 --> 00:10:50,560
with three the three main uh

00:10:48,880 --> 00:10:52,640
uh storage systems that are interesting

00:10:50,560 --> 00:10:54,800
in this case the first one is uh what we

00:10:52,640 --> 00:10:56,880
call cvmfs certain vmfs

00:10:54,800 --> 00:10:58,560
fs which is a real really only

00:10:56,880 --> 00:11:00,640
distributed hierarchical

00:10:58,560 --> 00:11:02,480
caching system for that is mostly used

00:11:00,640 --> 00:11:05,279
for software distribution

00:11:02,480 --> 00:11:05,839
the second one is an internal in-house

00:11:05,279 --> 00:11:08,399
developed

00:11:05,839 --> 00:11:09,680
system called eos that is uh holding all

00:11:08,399 --> 00:11:11,680
the physics data

00:11:09,680 --> 00:11:14,160
in this case the important part is that

00:11:11,680 --> 00:11:17,360
we need to we offer both kerberos and

00:11:14,160 --> 00:11:19,040
os2 based access os2 is very important

00:11:17,360 --> 00:11:20,560
for things like notebooks and anything

00:11:19,040 --> 00:11:23,279
that is uh

00:11:20,560 --> 00:11:25,600
like browser oriented and then the last

00:11:23,279 --> 00:11:27,440
one is hdfs

00:11:25,600 --> 00:11:29,200
they mentioned that in some cases people

00:11:27,440 --> 00:11:29,760
want to do the data preparation using

00:11:29,200 --> 00:11:32,640
spark

00:11:29,760 --> 00:11:35,440
and in this case we are accessing hdfs

00:11:32,640 --> 00:11:37,600
using kerberos credentials

00:11:35,440 --> 00:11:38,560
i will just summarize a couple of issues

00:11:37,600 --> 00:11:41,519
that we

00:11:38,560 --> 00:11:42,079
run well into while doing this the first

00:11:41,519 --> 00:11:45,519
one is

00:11:42,079 --> 00:11:47,519
that the couple releases were not always

00:11:45,519 --> 00:11:48,560
very consistent in terms of what's

00:11:47,519 --> 00:11:51,040
supporting what

00:11:48,560 --> 00:11:51,760
so one zero had for example multi-user

00:11:51,040 --> 00:11:55,360
support for

00:11:51,760 --> 00:11:58,000
for notebooks but not uh pipelines

00:11:55,360 --> 00:11:59,360
and one-on-one brought multi-user

00:11:58,000 --> 00:12:01,360
pipelines but actually some of the

00:11:59,360 --> 00:12:03,200
components were not talking to to this

00:12:01,360 --> 00:12:04,079
new api properly like kale from

00:12:03,200 --> 00:12:05,680
notebooks

00:12:04,079 --> 00:12:07,920
this meant that we spent quite a bit of

00:12:05,680 --> 00:12:09,519
time downstream

00:12:07,920 --> 00:12:11,279
fixing these bits when we did the

00:12:09,519 --> 00:12:11,920
upgrade and this is one of the reasons

00:12:11,279 --> 00:12:13,839
why

00:12:11,920 --> 00:12:15,120
we are still in one one and we are

00:12:13,839 --> 00:12:18,240
slowly uh

00:12:15,120 --> 00:12:19,920
updating to newer versions as well the

00:12:18,240 --> 00:12:20,480
second one is actually customized and

00:12:19,920 --> 00:12:23,200
the way

00:12:20,480 --> 00:12:24,000
uh kubflow is using customize is quite

00:12:23,200 --> 00:12:26,160
complex

00:12:24,000 --> 00:12:27,680
so we decided to spend some time

00:12:26,160 --> 00:12:29,839
simplifying things and

00:12:27,680 --> 00:12:31,839
removing some of the components out of

00:12:29,839 --> 00:12:33,680
it especially things like cert manager

00:12:31,839 --> 00:12:35,200
istio and k-native which are quite

00:12:33,680 --> 00:12:38,160
critical

00:12:35,200 --> 00:12:38,560
and in the end we deploy them in another

00:12:38,160 --> 00:12:39,839
way

00:12:38,560 --> 00:12:42,000
and we only deploy the workflow

00:12:39,839 --> 00:12:43,839
applications using customize

00:12:42,000 --> 00:12:45,360
and then the last one which is still an

00:12:43,839 --> 00:12:47,279
ongoing issue

00:12:45,360 --> 00:12:48,959
is how to manage additional packages

00:12:47,279 --> 00:12:50,560
that people might require for both the

00:12:48,959 --> 00:12:52,880
notebooks and then then for their

00:12:50,560 --> 00:12:55,279
pipelines as well

00:12:52,880 --> 00:12:57,440
and how can they install and add these

00:12:55,279 --> 00:12:58,560
packages easily to their containers so

00:12:57,440 --> 00:13:01,839
this is something i

00:12:58,560 --> 00:13:03,760
will mention more later

00:13:01,839 --> 00:13:05,040
the last bit i will mention before we

00:13:03,760 --> 00:13:07,279
jump to the demo is

00:13:05,040 --> 00:13:08,399
how we are doing uh bursting to the

00:13:07,279 --> 00:13:10,560
public clouds um

00:13:08,399 --> 00:13:12,320
this is very important for us because we

00:13:10,560 --> 00:13:14,240
can get access to a much larger amount

00:13:12,320 --> 00:13:16,480
of gpus and especially

00:13:14,240 --> 00:13:18,800
other types of accelerators like cpus or

00:13:16,480 --> 00:13:20,079
ipu's tpus are very interesting for our

00:13:18,800 --> 00:13:21,360
use cases

00:13:20,079 --> 00:13:23,360
also because they are very cost

00:13:21,360 --> 00:13:25,680
effective

00:13:23,360 --> 00:13:28,079
we tried this using different

00:13:25,680 --> 00:13:30,079
technologies over the last few years

00:13:28,079 --> 00:13:31,440
on the lower level we tried federation

00:13:30,079 --> 00:13:33,279
v1 and v2

00:13:31,440 --> 00:13:34,800
we also have deployments using the

00:13:33,279 --> 00:13:38,399
virtual couplet

00:13:34,800 --> 00:13:40,959
we are not still very um experts in

00:13:38,399 --> 00:13:43,279
istio but we are experimenting with it

00:13:40,959 --> 00:13:44,959
but actually for coop flow the

00:13:43,279 --> 00:13:47,120
most promising results and the way we

00:13:44,959 --> 00:13:49,120
are offering this is actually to

00:13:47,120 --> 00:13:51,760
directly expose the other clusters to

00:13:49,120 --> 00:13:53,600
the to the users via their jupiter

00:13:51,760 --> 00:13:54,560
environment so when you get your dripter

00:13:53,600 --> 00:13:56,399
environment

00:13:54,560 --> 00:13:58,560
your notebook environment you actually

00:13:56,399 --> 00:14:00,079
get the additional clusters configured

00:13:58,560 --> 00:14:01,760
and these clusters are configured with

00:14:00,079 --> 00:14:04,639
the same cern sso

00:14:01,760 --> 00:14:05,839
so we can do uh something like using the

00:14:04,639 --> 00:14:08,240
open policy agent

00:14:05,839 --> 00:14:10,240
to validate who is able to access which

00:14:08,240 --> 00:14:10,959
which clusters which groups can actually

00:14:10,240 --> 00:14:13,120
access

00:14:10,959 --> 00:14:14,560
each cluster and then we do the quota

00:14:13,120 --> 00:14:16,000
management the same way in those

00:14:14,560 --> 00:14:19,279
clusters as well

00:14:16,000 --> 00:14:21,279
this is working quite well this is a

00:14:19,279 --> 00:14:22,639
simple not so simple picture but it's

00:14:21,279 --> 00:14:25,120
kind of simple given

00:14:22,639 --> 00:14:27,040
what is behind but the key aspect is

00:14:25,120 --> 00:14:30,000
that this jupiter environment

00:14:27,040 --> 00:14:32,079
we have the cluster configuration and we

00:14:30,000 --> 00:14:34,880
are able to reuse the als

00:14:32,079 --> 00:14:36,079
token that the user already have has by

00:14:34,880 --> 00:14:39,360
logging into this

00:14:36,079 --> 00:14:41,120
system at cern and then but

00:14:39,360 --> 00:14:42,959
normally they would just submit to the

00:14:41,120 --> 00:14:45,279
same uh cluster where

00:14:42,959 --> 00:14:47,279
kubflow is running the the egyptian

00:14:45,279 --> 00:14:48,320
environment and they would submit a tf

00:14:47,279 --> 00:14:51,120
job that would use

00:14:48,320 --> 00:14:52,160
gpus on premises and then when once this

00:14:51,120 --> 00:14:55,360
training is done

00:14:52,160 --> 00:14:56,079
we we write to output artifacts to s3

00:14:55,360 --> 00:14:58,800
where it can

00:14:56,079 --> 00:15:00,959
be served from the s3 instance at cern

00:14:58,800 --> 00:15:01,680
by exposing additional clusters in the

00:15:00,959 --> 00:15:04,880
environment

00:15:01,680 --> 00:15:08,399
people can just source those clusters um

00:15:04,880 --> 00:15:09,040
and then submit the tf jobs to external

00:15:08,399 --> 00:15:10,800
clusters

00:15:09,040 --> 00:15:13,120
and this in this case it would be good

00:15:10,800 --> 00:15:15,920
the google cloud where we would have

00:15:13,120 --> 00:15:16,240
potentially thousands of gpus available

00:15:15,920 --> 00:15:19,040
or

00:15:16,240 --> 00:15:19,920
gpus in the end again the output

00:15:19,040 --> 00:15:22,480
artifacts

00:15:19,920 --> 00:15:24,320
are written to s3 and served in the same

00:15:22,480 --> 00:15:27,600
way as if they would have been trained

00:15:24,320 --> 00:15:31,440
internally so this is a quite promising

00:15:27,600 --> 00:15:31,440
and this is what we are offering today

00:15:32,240 --> 00:15:38,880
okay now we'll go back to our demo

00:15:35,440 --> 00:15:41,440
example so we remember 3d guns

00:15:38,880 --> 00:15:42,240
uh basically uh the the main issue with

00:15:41,440 --> 00:15:45,600
3d guns

00:15:42,240 --> 00:15:49,680
is uh extensive training time so

00:15:45,600 --> 00:15:52,800
uh for our model uh it would take two

00:15:49,680 --> 00:15:55,680
2.5 days to to properly train

00:15:52,800 --> 00:15:57,920
and if for example if we want to uh

00:15:55,680 --> 00:15:58,639
search hyper parameters or change the

00:15:57,920 --> 00:16:01,360
model

00:15:58,639 --> 00:16:02,720
that iteration could last for weeks or

00:16:01,360 --> 00:16:06,720
even months so this is

00:16:02,720 --> 00:16:09,279
uh with one gpu so to

00:16:06,720 --> 00:16:10,160
create a more scalable solution um a

00:16:09,279 --> 00:16:13,759
distributed

00:16:10,160 --> 00:16:16,000
model is created basically uh it's uh

00:16:13,759 --> 00:16:17,040
the model is trained using tensorflow

00:16:16,000 --> 00:16:20,720
strategies

00:16:17,040 --> 00:16:23,759
so uh for example we are using in our

00:16:20,720 --> 00:16:25,519
example a multi-worker mirror strategy

00:16:23,759 --> 00:16:27,360
with which uses different nodes with

00:16:25,519 --> 00:16:29,759
multiple gpus

00:16:27,360 --> 00:16:31,279
and also we have a script with uh

00:16:29,759 --> 00:16:34,480
accessing tpus for

00:16:31,279 --> 00:16:34,480
the distributed training

00:16:34,639 --> 00:16:42,079
uh so uh tf job and kubeflow

00:16:38,240 --> 00:16:43,279
helped us automate this distributed

00:16:42,079 --> 00:16:45,920
training process

00:16:43,279 --> 00:16:46,560
uh we are able to quickly iterate over

00:16:45,920 --> 00:16:48,800
different

00:16:46,560 --> 00:16:50,160
training configurations so basically we

00:16:48,800 --> 00:16:53,519
are encapsulating

00:16:50,160 --> 00:16:56,160
tensorflow distribution and we mean it's

00:16:53,519 --> 00:16:56,880
it's managing it across kubernetes spots

00:16:56,160 --> 00:17:00,959
uh

00:16:56,880 --> 00:17:02,880
so we are able with the job to

00:17:00,959 --> 00:17:04,480
run distributor training both locally

00:17:02,880 --> 00:17:05,520
and on a public cloud as ricardo was

00:17:04,480 --> 00:17:09,039
describing

00:17:05,520 --> 00:17:13,120
and uh yeah we at gcp we are

00:17:09,039 --> 00:17:15,679
using 128 preemptable uh

00:17:13,120 --> 00:17:17,439
machines for the distributed training

00:17:15,679 --> 00:17:20,959
and now we can

00:17:17,439 --> 00:17:20,959
move to the demo

00:17:21,120 --> 00:17:26,640
so let me share the screen

00:17:23,490 --> 00:17:26,640
[Music]

00:17:29,600 --> 00:17:34,400
so now we are going to show our demo

00:17:31,840 --> 00:17:36,960
here we can see at ml.cern.ch

00:17:34,400 --> 00:17:38,559
our service dashboard basically this is

00:17:36,960 --> 00:17:43,120
the cubesale dashboard

00:17:38,559 --> 00:17:47,840
and we can check all the

00:17:43,120 --> 00:17:47,840
kubeflow features

00:18:06,559 --> 00:18:10,480
so now we're going to show our demo uh

00:18:08,640 --> 00:18:11,360
we can see our dashboard here and we can

00:18:10,480 --> 00:18:12,880
see

00:18:11,360 --> 00:18:15,039
cubeflow features on the left we have

00:18:12,880 --> 00:18:18,240
pipelines notebook servers cutib

00:18:15,039 --> 00:18:20,480
and the other features so

00:18:18,240 --> 00:18:21,840
we'll go to our notebook which is

00:18:20,480 --> 00:18:25,120
basically where we

00:18:21,840 --> 00:18:25,120
have our demo prepared

00:18:27,520 --> 00:18:33,840
so uh here we have a couple of demos we

00:18:30,640 --> 00:18:37,200
are going to show uh the first one

00:18:33,840 --> 00:18:39,679
is um one from the

00:18:37,200 --> 00:18:41,600
from our examples repo basically we

00:18:39,679 --> 00:18:44,000
created a repository

00:18:41,600 --> 00:18:44,880
for onboarding our users for various

00:18:44,000 --> 00:18:48,720
cube flow

00:18:44,880 --> 00:18:51,840
features we are going to show kail

00:18:48,720 --> 00:18:53,039
this this example shows us how to

00:18:51,840 --> 00:18:55,840
convert

00:18:53,039 --> 00:18:56,880
a notebook to a pipeline without writing

00:18:55,840 --> 00:18:59,360
any additional

00:18:56,880 --> 00:19:00,320
python code so for that we're using kl

00:18:59,360 --> 00:19:02,400
deployment

00:19:00,320 --> 00:19:03,840
panel and basically the only thing we

00:19:02,400 --> 00:19:08,160
need to do is to annotate

00:19:03,840 --> 00:19:12,240
every every cell so that it converts

00:19:08,160 --> 00:19:15,039
properly to a pipeline component

00:19:12,240 --> 00:19:15,679
in addition to annotating we are

00:19:15,039 --> 00:19:18,000
creating

00:19:15,679 --> 00:19:20,160
connections between pipeline components

00:19:18,000 --> 00:19:22,640
and we can also add the gpu to any

00:19:20,160 --> 00:19:26,080
specific pipeline component

00:19:22,640 --> 00:19:28,240
so here we can in order to run

00:19:26,080 --> 00:19:29,440
we only need to click this button

00:19:28,240 --> 00:19:32,720
compile and run

00:19:29,440 --> 00:19:35,360
and we can see our pipeline uh running

00:19:32,720 --> 00:19:36,720
so while our pipeline is running we can

00:19:35,360 --> 00:19:39,919
check other

00:19:36,720 --> 00:19:43,520
uh features which we have in our service

00:19:39,919 --> 00:19:47,120
uh one of them is eos so

00:19:43,520 --> 00:19:48,000
eos is where most users where all users

00:19:47,120 --> 00:19:51,360
at cern

00:19:48,000 --> 00:19:54,960
have their personal directories

00:19:51,360 --> 00:19:58,880
and the mounting eos really allows

00:19:54,960 --> 00:20:02,400
us to be able to access

00:19:58,880 --> 00:20:05,760
broad data from

00:20:02,400 --> 00:20:08,240
multiple users basically every user can

00:20:05,760 --> 00:20:08,880
access their own personal folder here

00:20:08,240 --> 00:20:11,840
and also

00:20:08,880 --> 00:20:13,360
we can show up the usage of a gpu with

00:20:11,840 --> 00:20:16,960
nvidia smi

00:20:13,360 --> 00:20:20,559
so yeah we are starting with

00:20:16,960 --> 00:20:24,480
that the main example which we have

00:20:20,559 --> 00:20:24,960
is the 3d gun so here uh we have our 3d

00:20:24,480 --> 00:20:28,559
gun

00:20:24,960 --> 00:20:28,960
training so we have a different scripts

00:20:28,559 --> 00:20:31,760
here

00:20:28,960 --> 00:20:32,880
uh one of them is training a 3d gun with

00:20:31,760 --> 00:20:36,559
a cpu

00:20:32,880 --> 00:20:39,039
and then we train 3d gun with the gpu

00:20:36,559 --> 00:20:40,960
here and it's all distributed training

00:20:39,039 --> 00:20:43,200
though when it comes to gpus

00:20:40,960 --> 00:20:45,039
so basically here what we want to check

00:20:43,200 --> 00:20:49,760
is a

00:20:45,039 --> 00:20:52,320
strategy so we see that

00:20:49,760 --> 00:20:53,120
we see that we are using multi-worker

00:20:52,320 --> 00:20:56,960
mirrored

00:20:53,120 --> 00:21:00,159
strategy for gpu training and

00:20:56,960 --> 00:21:03,120
so as far as mentioning

00:21:00,159 --> 00:21:03,760
we are also uploading the train model to

00:21:03,120 --> 00:21:06,000
a bucket

00:21:03,760 --> 00:21:06,960
and we see that code here that after the

00:21:06,000 --> 00:21:09,919
model is trained

00:21:06,960 --> 00:21:11,919
we are uploading it to our cern bucket

00:21:09,919 --> 00:21:14,799
similarly for tpus

00:21:11,919 --> 00:21:17,840
only here we have a tpu strategy for

00:21:14,799 --> 00:21:17,840
distributed training

00:21:18,880 --> 00:21:22,159
uh we also in this repository have a

00:21:20,960 --> 00:21:25,679
docker file to

00:21:22,159 --> 00:21:27,440
build our image to to run our

00:21:25,679 --> 00:21:29,440
distributed training but we are not

00:21:27,440 --> 00:21:32,080
building it uh here

00:21:29,440 --> 00:21:32,480
we won't do that but we'll show our tf

00:21:32,080 --> 00:21:35,440
job

00:21:32,480 --> 00:21:36,559
yaml file so to submit a tf job

00:21:35,440 --> 00:21:39,200
basically we

00:21:36,559 --> 00:21:40,480
can define our number of replicas here

00:21:39,200 --> 00:21:44,320
number of gpus

00:21:40,480 --> 00:21:47,120
we are using here and then we also

00:21:44,320 --> 00:21:48,799
we select the image and we also can

00:21:47,120 --> 00:21:50,559
select if we want

00:21:48,799 --> 00:21:52,640
uh full training and the number of

00:21:50,559 --> 00:21:55,760
epochs and other customizable

00:21:52,640 --> 00:21:55,760
uh arguments

00:21:55,919 --> 00:22:03,600
so now we are going to submit our

00:21:59,760 --> 00:22:12,000
our 3d 3d gun

00:22:03,600 --> 00:22:15,360
tf job on a local cluster

00:22:12,000 --> 00:22:17,840
you have to do is to do cubectl

00:22:15,360 --> 00:22:17,840
apply

00:22:18,559 --> 00:22:25,440
3d gpu so yeah

00:22:22,080 --> 00:22:27,840
this one we are submitting to our local

00:22:25,440 --> 00:22:27,840
cluster

00:22:29,039 --> 00:22:32,080
and we can check our

00:22:32,320 --> 00:22:38,720
job and we see our team job

00:22:35,360 --> 00:22:40,400
running so now might be a good time to

00:22:38,720 --> 00:22:43,120
check if our pipeline

00:22:40,400 --> 00:22:44,159
has completed it has so we can see our

00:22:43,120 --> 00:22:47,039
logs

00:22:44,159 --> 00:22:47,919
and we can see that our pipeline has

00:22:47,039 --> 00:22:50,960
completely

00:22:47,919 --> 00:22:52,840
has completed training two models and we

00:22:50,960 --> 00:22:55,520
see which model was

00:22:52,840 --> 00:22:57,440
better and now we're running the

00:22:55,520 --> 00:22:59,120
distributed training of a 3d gun on a

00:22:57,440 --> 00:23:01,420
local cluster

00:22:59,120 --> 00:23:03,039
additionally we want to run a

00:23:01,420 --> 00:23:06,320
[Music]

00:23:03,039 --> 00:23:09,520
3d gun training on a google cluster

00:23:06,320 --> 00:23:12,559
so basically uh

00:23:09,520 --> 00:23:16,000
inside the cluster folder uh

00:23:12,559 --> 00:23:18,400
users would get uh um

00:23:16,000 --> 00:23:19,039
information about all available clusters

00:23:18,400 --> 00:23:20,960
uh

00:23:19,039 --> 00:23:23,200
in our service so here we only have a

00:23:20,960 --> 00:23:26,400
cern and the gcp cluster

00:23:23,200 --> 00:23:29,440
and all the users have to do

00:23:26,400 --> 00:23:30,799
to access the additional clusters is to

00:23:29,440 --> 00:23:34,880
source

00:23:30,799 --> 00:23:38,880
these files so gcp

00:23:34,880 --> 00:23:41,360
setup setup.sh

00:23:38,880 --> 00:23:43,360
and now they should be able to they

00:23:41,360 --> 00:23:50,240
should they are in

00:23:43,360 --> 00:23:52,000
the google cloud cluster

00:23:50,240 --> 00:23:53,520
so as we can see in the google cloud

00:23:52,000 --> 00:23:54,799
cluster there are no parts in my

00:23:53,520 --> 00:23:58,799
personal name space

00:23:54,799 --> 00:24:01,919
but in uh this local cluster we have our

00:23:58,799 --> 00:24:04,450
i have a couple of pods here running and

00:24:01,919 --> 00:24:06,559
some of them completed so um

00:24:04,450 --> 00:24:09,679
[Music]

00:24:06,559 --> 00:24:13,200
what we are going to submit here are

00:24:09,679 --> 00:24:16,240
a 3d gun example so we can go

00:24:13,200 --> 00:24:19,600
to um to our

00:24:16,240 --> 00:24:24,080
gcp uh our

00:24:19,600 --> 00:24:33,840
gcp aml file and we are going to

00:24:24,080 --> 00:24:33,840
submit that

00:24:34,880 --> 00:24:40,880
gcp so

00:24:37,919 --> 00:24:41,440
now we're submitting this tf job to our

00:24:40,880 --> 00:24:44,240
google

00:24:41,440 --> 00:24:44,240
google cluster

00:24:44,840 --> 00:24:52,559
meanwhile we can check

00:24:47,840 --> 00:24:55,120
if our training on our local cluster has

00:24:52,559 --> 00:24:55,120
completed

00:24:58,320 --> 00:25:04,799
flow and yes we can see that

00:25:01,360 --> 00:25:08,480
it has completed

00:25:04,799 --> 00:25:10,559
and to check the google cluster

00:25:08,480 --> 00:25:12,559
i'll be basically we want to have a

00:25:10,559 --> 00:25:15,679
watch and yes

00:25:12,559 --> 00:25:19,039
we can see here that our workers are

00:25:15,679 --> 00:25:22,279
uh deployed at nodes which have uh

00:25:19,039 --> 00:25:25,440
we v100s so in total we have

00:25:22,279 --> 00:25:29,279
128 nodes running and

00:25:25,440 --> 00:25:31,919
we have a 16 workers where each worker

00:25:29,279 --> 00:25:31,919
has 8

00:25:32,070 --> 00:25:36,559
[Music]

00:25:34,000 --> 00:25:37,279
so now our training job is running

00:25:36,559 --> 00:25:39,840
actually

00:25:37,279 --> 00:25:42,559
on a google uh cluster and this is what

00:25:39,840 --> 00:25:45,840
we see here

00:25:42,559 --> 00:25:48,240
so we can close this now

00:25:45,840 --> 00:25:49,039
and here we see that our local job has

00:25:48,240 --> 00:25:52,320
completed

00:25:49,039 --> 00:25:55,520
and now as ricardo was saying after

00:25:52,320 --> 00:25:58,880
the training we submit our model

00:25:55,520 --> 00:26:02,799
to a bucket so here we can see

00:25:58,880 --> 00:26:05,120
uh the the trained model

00:26:02,799 --> 00:26:06,960
stored on on our buckets so we have

00:26:05,120 --> 00:26:08,799
couple of files for each model and then

00:26:06,960 --> 00:26:09,679
this is all for for one model and for

00:26:08,799 --> 00:26:11,919
one ebook

00:26:09,679 --> 00:26:14,240
so we have our discriminator and

00:26:11,919 --> 00:26:16,799
generator for 3d gun stored

00:26:14,240 --> 00:26:17,520
in the bucket and we also have a saved

00:26:16,799 --> 00:26:21,360
model

00:26:17,520 --> 00:26:24,320
in the format so that it can be used for

00:26:21,360 --> 00:26:25,520
inference for serving and also we want

00:26:24,320 --> 00:26:28,960
to

00:26:25,520 --> 00:26:32,080
maybe want to check this these metrics

00:26:28,960 --> 00:26:34,400
basically this is how we can store a

00:26:32,080 --> 00:26:35,440
metrics how we store metrics about our

00:26:34,400 --> 00:26:39,840
model

00:26:35,440 --> 00:26:39,840
after each epoch

00:26:42,000 --> 00:26:48,559
so okay now we have covered the 3d gun

00:26:45,760 --> 00:26:50,320
the last thing i'd like to cover is the

00:26:48,559 --> 00:26:55,279
inference

00:26:50,320 --> 00:26:55,279
is the inference rd inference services

00:27:00,320 --> 00:27:02,880
services

00:27:03,600 --> 00:27:10,720
what we want to do here is to submit

00:27:07,360 --> 00:27:14,320
an inference service

00:27:10,720 --> 00:27:17,440
and to basically serve a model

00:27:14,320 --> 00:27:20,799
by only specifying where the model is

00:27:17,440 --> 00:27:23,360
located so you can see cube ctl

00:27:20,799 --> 00:27:23,360
apply

00:27:25,679 --> 00:27:29,919
and now we have created our uh inference

00:27:28,799 --> 00:27:32,960
service actually

00:27:29,919 --> 00:27:34,159
it was already it was already there but

00:27:32,960 --> 00:27:37,360
this is how we can

00:27:34,159 --> 00:27:42,480
we create it when we when we want

00:27:37,360 --> 00:27:45,360
and then to test our inference

00:27:42,480 --> 00:27:45,679
we can test it from here and we see that

00:27:45,360 --> 00:27:48,320
we

00:27:45,679 --> 00:27:50,799
are getting results and basically as we

00:27:48,320 --> 00:27:53,840
were discussing we're getting a 3d

00:27:50,799 --> 00:27:56,559
output for

00:27:53,840 --> 00:27:57,520
uh that represents the output of the

00:27:56,559 --> 00:28:00,799
detector

00:27:57,520 --> 00:28:04,320
so this is what happens when we do uh

00:28:00,799 --> 00:28:07,919
one inference but uh we want to do

00:28:04,320 --> 00:28:10,080
10 uh curl requests at the same time

00:28:07,919 --> 00:28:12,080
so that we can see what happens to the

00:28:10,080 --> 00:28:15,360
number of predictor pods

00:28:12,080 --> 00:28:17,039
as we can see they are

00:28:15,360 --> 00:28:20,480
the number of parts it is increasing

00:28:17,039 --> 00:28:20,960
it's auto scaling so that it can support

00:28:20,480 --> 00:28:23,600
uh

00:28:20,960 --> 00:28:26,640
[Music]

00:28:23,600 --> 00:28:29,279
client requests

00:28:26,640 --> 00:28:32,080
and basically uh yeah with this we have

00:28:29,279 --> 00:28:32,080
covered our demo

00:28:36,559 --> 00:28:40,399
so basically during this training we

00:28:38,399 --> 00:28:43,039
were able to

00:28:40,399 --> 00:28:45,279
reduce the execution time from one hour

00:28:43,039 --> 00:28:49,120
to 30 seconds for one e-book and for the

00:28:45,279 --> 00:28:49,120
full training we managed to get from

00:28:49,200 --> 00:28:55,679
60 hours to around 30 minutes so

00:28:53,039 --> 00:28:56,559
tf job really helped us speed up the

00:28:55,679 --> 00:28:58,480
development

00:28:56,559 --> 00:29:00,320
process and we see that we get almost

00:28:58,480 --> 00:29:03,520
linear improvement in our

00:29:00,320 --> 00:29:06,799
performance for our 3d gun model and our

00:29:03,520 --> 00:29:10,480
card will offer the closing remarks

00:29:06,799 --> 00:29:12,880
yeah so basically i hope this was a

00:29:10,480 --> 00:29:14,799
nice overview of the service we are

00:29:12,880 --> 00:29:16,480
offering and the potential that it has

00:29:14,799 --> 00:29:18,000
by offering uh like a consistent

00:29:16,480 --> 00:29:19,919
environment where people can

00:29:18,000 --> 00:29:22,559
do their development but also interact

00:29:19,919 --> 00:29:24,559
with the services

00:29:22,559 --> 00:29:26,080
we handle all the machine learning

00:29:24,559 --> 00:29:28,159
learning life cycle steps from

00:29:26,080 --> 00:29:31,200
preparation all the way to serving

00:29:28,159 --> 00:29:32,320
um we managed to centralize the

00:29:31,200 --> 00:29:34,880
resources

00:29:32,320 --> 00:29:36,399
that are pre-scarce such as accelerators

00:29:34,880 --> 00:29:38,399
in this case gpus

00:29:36,399 --> 00:29:40,720
and also we showed how we are doing

00:29:38,399 --> 00:29:42,960
currently the integration with external

00:29:40,720 --> 00:29:44,159
resources for gpu's dpus using public

00:29:42,960 --> 00:29:46,480
clouds

00:29:44,159 --> 00:29:48,000
there are steps we are still working on

00:29:46,480 --> 00:29:50,240
so one of the

00:29:48,000 --> 00:29:51,440
main ones is to onboard new use cases

00:29:50,240 --> 00:29:53,039
and from those there's a very

00:29:51,440 --> 00:29:54,559
interesting one for reinforcement

00:29:53,039 --> 00:29:55,679
learning uh for

00:29:54,559 --> 00:29:57,760
from the people doing the beam

00:29:55,679 --> 00:30:00,559
calibration where they want to

00:29:57,760 --> 00:30:02,240
um like keep the model live and and

00:30:00,559 --> 00:30:05,679
update it live

00:30:02,240 --> 00:30:08,080
while the beam is running and the second

00:30:05,679 --> 00:30:09,120
second thing i would like to mention is

00:30:08,080 --> 00:30:11,279
this

00:30:09,120 --> 00:30:13,520
needs to be for users to be able to

00:30:11,279 --> 00:30:14,720
curate their own environments and to add

00:30:13,520 --> 00:30:17,279
uh packages to

00:30:14,720 --> 00:30:18,720
to their own environments easily right

00:30:17,279 --> 00:30:20,320
now the only option is to install the

00:30:18,720 --> 00:30:21,600
packages on the notebook but that

00:30:20,320 --> 00:30:23,279
doesn't work well

00:30:21,600 --> 00:30:25,360
when you're doing a transition to

00:30:23,279 --> 00:30:26,320
pipelines for example or to distribute

00:30:25,360 --> 00:30:28,559
training

00:30:26,320 --> 00:30:29,840
so we have some experience using a tool

00:30:28,559 --> 00:30:31,919
called binder for

00:30:29,840 --> 00:30:33,279
jupyter notebooks and we are looking at

00:30:31,919 --> 00:30:36,640
integrating this with the

00:30:33,279 --> 00:30:38,720
kuflow jr web app as well and the last

00:30:36,640 --> 00:30:41,279
one is uh we are quite involved in the

00:30:38,720 --> 00:30:43,200
work ongoing work uh for flow

00:30:41,279 --> 00:30:43,840
improvements in metadata and artifact

00:30:43,200 --> 00:30:46,559
management

00:30:43,840 --> 00:30:48,159
so it's something that we'll also keep

00:30:46,559 --> 00:30:49,919
pushing for

00:30:48,159 --> 00:30:52,000
in the community so we would like to

00:30:49,919 --> 00:30:52,960
thank uh like everyone in the cupola

00:30:52,000 --> 00:30:54,960
community

00:30:52,960 --> 00:30:56,559
for for the great tooling that and of

00:30:54,960 --> 00:30:59,120
course all the kubernetes and cloud

00:30:56,559 --> 00:31:01,200
native tools that we rely on as well

00:30:59,120 --> 00:31:04,720
and we are happy to answer any questions

00:31:01,200 --> 00:31:04,720

YouTube URL: https://www.youtube.com/watch?v=HuWt1N8NFzU


