Title: Petabyte Scale Logging with Fluentd and Fluent Bit: A Use Case From... Hanzel Jesheen & Anurag Gupta
Publication date: 2021-05-09
Playlist: KubeCon + CloudNativeCon Europe 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Petabyte Scale Logging with Fluentd and Fluent Bit: A Use Case From Intuit - Hanzel Jesheen, Intuit & Anurag Gupta, Calyptia

In financial and software companies like Intuit, it’s mandatory to provide a reliable observability layer so internal end-users like developers from different line-of-businesses can access their application data. In the last few years, we faced several problems when scaling up logging. For all the new applications we deployed in our Kubernetes clusters, the data rate increased, creating decreased throughput, higher resource consumption, and unexpected backpressure. In this session, you will learn how we leveraged CNCF projects, Fluentd and Fluent Bit, to configure reliable buffering for network outages, flexible configurations for backpressure management, and tips to avoid common mistakes that will save you hours of troubleshooting. A secondary title for this talk would be: “How we achieved Petabyte scale logging with Fluentd and Fluent Bit”.
Captions: 
	00:00:01,439 --> 00:00:05,120
hi everyone welcome to petabyte scale

00:00:03,439 --> 00:00:07,359
logging with fluency influent bit

00:00:05,120 --> 00:00:08,240
a use case from intuit my name is

00:00:07,359 --> 00:00:10,240
onorock i'm

00:00:08,240 --> 00:00:12,160
part of the company calyptia and i help

00:00:10,240 --> 00:00:13,920
run product there

00:00:12,160 --> 00:00:15,759
so who are we so today it's going to be

00:00:13,920 --> 00:00:17,600
myself and hansel who's joining me from

00:00:15,759 --> 00:00:18,400
intuit who's on the cloud observability

00:00:17,600 --> 00:00:20,640
team there

00:00:18,400 --> 00:00:22,160
as a senior software engineer and for

00:00:20,640 --> 00:00:24,880
myself i'm part of the

00:00:22,160 --> 00:00:26,240
oss group that is a maintainer for the

00:00:24,880 --> 00:00:27,599
fluent bit project

00:00:26,240 --> 00:00:30,400
and i help drive product for both

00:00:27,599 --> 00:00:32,800
flipped and flinty

00:00:30,400 --> 00:00:34,239
so for those who are unfamiliar with

00:00:32,800 --> 00:00:37,120
what is fluenty and what

00:00:34,239 --> 00:00:39,040
what is fluent bit they are cloud native

00:00:37,120 --> 00:00:40,719
computing foundation projects

00:00:39,040 --> 00:00:42,879
they are part of the graduated projects

00:00:40,719 --> 00:00:43,520
alongside kubernetes prometheus and

00:00:42,879 --> 00:00:45,440
others

00:00:43,520 --> 00:00:47,680
they started 10 years ago and the

00:00:45,440 --> 00:00:50,960
primary problem they solve

00:00:47,680 --> 00:00:53,760
is how do you take data from point a

00:00:50,960 --> 00:00:54,640
and send it to point b and if we look at

00:00:53,760 --> 00:00:57,360
this problem

00:00:54,640 --> 00:00:57,760
over the last 10 years where you see

00:00:57,360 --> 00:01:00,000
that

00:00:57,760 --> 00:01:01,039
data sources continually evolve we have

00:01:00,000 --> 00:01:03,359
more things like

00:01:01,039 --> 00:01:05,080
mobile applications system logs

00:01:03,359 --> 00:01:09,040
kubernetes

00:01:05,080 --> 00:01:11,280
microservices containers app metrics

00:01:09,040 --> 00:01:13,040
and we have additional destinations that

00:01:11,280 --> 00:01:14,240
just keep compounding you might have

00:01:13,040 --> 00:01:16,960
things like

00:01:14,240 --> 00:01:19,040
uh amazon s3 you might have splunk

00:01:16,960 --> 00:01:22,640
elasticsearch loki

00:01:19,040 --> 00:01:24,799
azure gcp kafka

00:01:22,640 --> 00:01:26,000
and those destinations just continue to

00:01:24,799 --> 00:01:28,000
evolve and expand

00:01:26,000 --> 00:01:29,360
you might need to send data to multiple

00:01:28,000 --> 00:01:30,799
you might need to send data

00:01:29,360 --> 00:01:32,479
to all of them you might need to send

00:01:30,799 --> 00:01:34,799
data to a single one

00:01:32,479 --> 00:01:37,360
but in any case fluent d and fluent bid

00:01:34,799 --> 00:01:38,000
are really vendor neutral solutions to

00:01:37,360 --> 00:01:40,880
saying

00:01:38,000 --> 00:01:41,200
let's collect data once and send it to

00:01:40,880 --> 00:01:45,200
as

00:01:41,200 --> 00:01:47,680
many destinations as you require

00:01:45,200 --> 00:01:49,280
now when we look at challenges for

00:01:47,680 --> 00:01:51,840
logging at scale

00:01:49,280 --> 00:01:54,079
we have to look at this and perspective

00:01:51,840 --> 00:01:57,119
of a broad overview one is

00:01:54,079 --> 00:01:58,320
high scale can really equal very high

00:01:57,119 --> 00:02:00,479
costs

00:01:58,320 --> 00:02:02,479
you can have things like reliability and

00:02:00,479 --> 00:02:04,880
buffering uh how do you make sure that

00:02:02,479 --> 00:02:07,360
logs get sent where they're supposed to

00:02:04,880 --> 00:02:08,080
and you don't want to open up a request

00:02:07,360 --> 00:02:10,560
every single

00:02:08,080 --> 00:02:12,080
time that you have a new message think

00:02:10,560 --> 00:02:15,280
about that when you're sending

00:02:12,080 --> 00:02:18,160
10 000 to 100 000 messages per second

00:02:15,280 --> 00:02:19,040
networking always an issue here with

00:02:18,160 --> 00:02:21,520
things like

00:02:19,040 --> 00:02:23,120
ephemeral workloads and kubernetes or

00:02:21,520 --> 00:02:24,800
you're in the cloud

00:02:23,120 --> 00:02:26,239
you might have air gapped environments

00:02:24,800 --> 00:02:28,319
how do you make sure networking

00:02:26,239 --> 00:02:30,080
doesn't hurt you when you're doing

00:02:28,319 --> 00:02:31,840
logging at scale

00:02:30,080 --> 00:02:34,160
event throughput you might need to

00:02:31,840 --> 00:02:35,200
maximize how many messages per second

00:02:34,160 --> 00:02:37,280
you're sending

00:02:35,200 --> 00:02:38,400
you might have latency requirements you

00:02:37,280 --> 00:02:40,560
might have to

00:02:38,400 --> 00:02:42,879
get messages so that they your

00:02:40,560 --> 00:02:43,360
developers your operations folks can go

00:02:42,879 --> 00:02:45,440
and

00:02:43,360 --> 00:02:47,200
start to debug and diagnose all that

00:02:45,440 --> 00:02:50,080
information

00:02:47,200 --> 00:02:50,959
security so again in a large scale

00:02:50,080 --> 00:02:52,720
environment

00:02:50,959 --> 00:02:54,319
we might have sensitive information that

00:02:52,720 --> 00:02:56,239
might make its way through

00:02:54,319 --> 00:02:57,599
how do you look at securing the data

00:02:56,239 --> 00:02:59,599
transition

00:02:57,599 --> 00:03:01,200
and then last but not least when you're

00:02:59,599 --> 00:03:03,519
running this in production

00:03:01,200 --> 00:03:05,440
how do you look at operationality or

00:03:03,519 --> 00:03:08,480
ways to minimize

00:03:05,440 --> 00:03:09,760
the performance impacts uh at a larger

00:03:08,480 --> 00:03:12,080
scale

00:03:09,760 --> 00:03:13,840
so what we're we'll talk about in in

00:03:12,080 --> 00:03:15,200
this session is how into it solved many

00:03:13,840 --> 00:03:18,159
of these challenges

00:03:15,200 --> 00:03:19,920
uh but also how fluent d influent bit

00:03:18,159 --> 00:03:21,519
have solved these challenges

00:03:19,920 --> 00:03:23,200
over the course of the 10 years that

00:03:21,519 --> 00:03:25,440
that project's been running

00:03:23,200 --> 00:03:26,879
one is filtering parsing and compression

00:03:25,440 --> 00:03:29,440
so support for

00:03:26,879 --> 00:03:31,440
compressing workloads being able to take

00:03:29,440 --> 00:03:32,319
data and throw away stuff that isn't

00:03:31,440 --> 00:03:34,400
used

00:03:32,319 --> 00:03:35,599
parsing data so you can take log

00:03:34,400 --> 00:03:37,440
messages and

00:03:35,599 --> 00:03:39,760
make really good sense out of them so

00:03:37,440 --> 00:03:40,959
taking like for example an apache http

00:03:39,760 --> 00:03:43,360
server log

00:03:40,959 --> 00:03:45,440
and giving you the additional

00:03:43,360 --> 00:03:46,879
information like ips which you can then

00:03:45,440 --> 00:03:50,319
go enrich with geo

00:03:46,879 --> 00:03:54,000
information things like

00:03:50,319 --> 00:03:54,720
source ip the request type reliability

00:03:54,000 --> 00:03:56,799
and buffering

00:03:54,720 --> 00:03:59,040
so within fluency influence bit there

00:03:56,799 --> 00:04:01,599
are file system and memory buffers

00:03:59,040 --> 00:04:03,200
so if you need really fast performance

00:04:01,599 --> 00:04:05,680
and you want to send bulk messages you

00:04:03,200 --> 00:04:07,360
can use the memory buffer by default

00:04:05,680 --> 00:04:08,720
but both of these projects also support

00:04:07,360 --> 00:04:10,799
file system buffering

00:04:08,720 --> 00:04:12,319
which is great in case of things

00:04:10,799 --> 00:04:14,879
crashing

00:04:12,319 --> 00:04:15,840
for example you might have a server that

00:04:14,879 --> 00:04:17,519
goes down

00:04:15,840 --> 00:04:18,880
but because you're storing everything on

00:04:17,519 --> 00:04:20,320
the file system

00:04:18,880 --> 00:04:22,160
you're able to recover all of that

00:04:20,320 --> 00:04:23,919
information none of that data is lost

00:04:22,160 --> 00:04:25,440
they all follow in at least once

00:04:23,919 --> 00:04:27,520
mechanism

00:04:25,440 --> 00:04:29,040
which leads into networking so when

00:04:27,520 --> 00:04:30,800
networking goes down

00:04:29,040 --> 00:04:33,280
we don't lose data we have a

00:04:30,800 --> 00:04:35,759
configurable retry mechanism

00:04:33,280 --> 00:04:37,440
as well as back pressure handling so as

00:04:35,759 --> 00:04:41,520
the load increases

00:04:37,440 --> 00:04:44,880
from say a file or an application

00:04:41,520 --> 00:04:46,639
fluentbit is able to understand that

00:04:44,880 --> 00:04:48,479
maybe the application can't receive too

00:04:46,639 --> 00:04:50,880
many messages at the second

00:04:48,479 --> 00:04:52,080
and we'll recover try at a at a

00:04:50,880 --> 00:04:54,560
desirable time

00:04:52,080 --> 00:04:56,560
in the future and especially when you're

00:04:54,560 --> 00:04:58,479
logging at scale you might have

00:04:56,560 --> 00:05:00,160
thousands and thousands of endpoints or

00:04:58,479 --> 00:05:01,919
thousands and thousands of applications

00:05:00,160 --> 00:05:03,120
logging data

00:05:01,919 --> 00:05:05,680
how do you make sure that all these

00:05:03,120 --> 00:05:07,039
retries don't try exactly at the same

00:05:05,680 --> 00:05:09,360
time and unfortunately

00:05:07,039 --> 00:05:11,520
cause the denial of service these are

00:05:09,360 --> 00:05:13,520
places where we've built a lot of that

00:05:11,520 --> 00:05:15,600
functionality within the open source

00:05:13,520 --> 00:05:18,720
fluentine fluid bit

00:05:15,600 --> 00:05:21,680
event throughput another place where

00:05:18,720 --> 00:05:24,080
if you need additional performance and

00:05:21,680 --> 00:05:26,000
you have the resources to dedicate you

00:05:24,080 --> 00:05:27,680
can use aspects like multi-worker

00:05:26,000 --> 00:05:30,160
configuration in both 4d

00:05:27,680 --> 00:05:32,240
and fluent bit and and give you the

00:05:30,160 --> 00:05:34,560
ability to utilize

00:05:32,240 --> 00:05:36,479
a lot of the server resources that you

00:05:34,560 --> 00:05:39,120
have at your disposal

00:05:36,479 --> 00:05:41,360
with security there's two places to this

00:05:39,120 --> 00:05:43,759
if you're securing sensitive information

00:05:41,360 --> 00:05:46,479
you can use anonymization filters you

00:05:43,759 --> 00:05:48,960
can use parsing to remove those

00:05:46,479 --> 00:05:49,759
secure fields and then of course with

00:05:48,960 --> 00:05:52,000
tls

00:05:49,759 --> 00:05:53,199
both fluently influent bit support the

00:05:52,000 --> 00:05:56,639
ability for

00:05:53,199 --> 00:05:58,880
encryption and transit operationally

00:05:56,639 --> 00:05:59,840
we we can use things like different

00:05:58,880 --> 00:06:02,880
architectures

00:05:59,840 --> 00:06:05,840
so both products both projects have been

00:06:02,880 --> 00:06:07,280
designed with this ease of use and

00:06:05,840 --> 00:06:10,080
flexibility in mind

00:06:07,280 --> 00:06:10,639
you might want to minimize the resources

00:06:10,080 --> 00:06:13,280
spent

00:06:10,639 --> 00:06:15,520
say at your source within containers

00:06:13,280 --> 00:06:18,160
within your kubernetes nodes

00:06:15,520 --> 00:06:20,960
within your servers and do more of that

00:06:18,160 --> 00:06:23,199
processing at a centralized layer

00:06:20,960 --> 00:06:24,160
and we call that the forwarder aggregate

00:06:23,199 --> 00:06:26,400
aggregator called

00:06:24,160 --> 00:06:27,280
architecture so just some common

00:06:26,400 --> 00:06:29,199
architecture

00:06:27,280 --> 00:06:30,560
patterns right we have the forwarder

00:06:29,199 --> 00:06:33,039
only which

00:06:30,560 --> 00:06:34,960
allows you to take all of these sources

00:06:33,039 --> 00:06:37,280
and independent pieces

00:06:34,960 --> 00:06:38,160
and send data they each individually

00:06:37,280 --> 00:06:40,639
handle

00:06:38,160 --> 00:06:42,400
back pressure so you're not necessarily

00:06:40,639 --> 00:06:45,120
relying on a single point

00:06:42,400 --> 00:06:46,720
to grapple the data enrich it and then

00:06:45,120 --> 00:06:48,560
send it out

00:06:46,720 --> 00:06:50,880
the disadvantages is you have to do a

00:06:48,560 --> 00:06:52,960
little bit more configuration management

00:06:50,880 --> 00:06:54,960
um and it is sometimes hard to end more

00:06:52,960 --> 00:06:55,520
destinations which falls a little bit

00:06:54,960 --> 00:06:57,520
with

00:06:55,520 --> 00:06:58,639
the configuration right if i'm sending

00:06:57,520 --> 00:07:01,199
data to

00:06:58,639 --> 00:07:02,800
uh splunk elasticsearch and kafka at the

00:07:01,199 --> 00:07:04,960
same time

00:07:02,800 --> 00:07:06,319
i might have to go and configure that

00:07:04,960 --> 00:07:10,000
individually on a

00:07:06,319 --> 00:07:11,840
on a forwarder basis now a common

00:07:10,000 --> 00:07:15,120
architecture pattern we see with large

00:07:11,840 --> 00:07:17,120
scale is forwarders with aggregators

00:07:15,120 --> 00:07:19,199
so that is going to be less resource

00:07:17,120 --> 00:07:20,720
utilization on the edge devices

00:07:19,199 --> 00:07:22,240
all they're doing is collecting that

00:07:20,720 --> 00:07:23,199
data and forwarding it onto an

00:07:22,240 --> 00:07:25,520
aggregator

00:07:23,199 --> 00:07:27,840
it allows you to process and and scale

00:07:25,520 --> 00:07:29,680
the aggregator to your independently

00:07:27,840 --> 00:07:31,599
you can add more back ends with this

00:07:29,680 --> 00:07:33,280
with in a single tier

00:07:31,599 --> 00:07:35,360
and then of course the disadvantagers

00:07:33,280 --> 00:07:38,160
you do have to dedicate some of

00:07:35,360 --> 00:07:38,560
the aggregator instance now that's all

00:07:38,160 --> 00:07:41,360
for

00:07:38,560 --> 00:07:42,479
just a general overview of how fluenty

00:07:41,360 --> 00:07:44,879
and fluent bit uh

00:07:42,479 --> 00:07:47,120
our projects other employees within the

00:07:44,879 --> 00:07:49,120
cloud native computing foundation

00:07:47,120 --> 00:07:50,720
and now how you can look at some of the

00:07:49,120 --> 00:07:53,599
challenges that they solve for

00:07:50,720 --> 00:07:55,919
for at scale i'm really happy to pass it

00:07:53,599 --> 00:07:57,039
over to hansel who's going to be able to

00:07:55,919 --> 00:07:59,440
talk a bit about

00:07:57,039 --> 00:08:00,879
the use case specifics add intuit and

00:07:59,440 --> 00:08:03,039
give us

00:08:00,879 --> 00:08:04,319
an idea how petabyte scale can can be

00:08:03,039 --> 00:08:05,919
achieved in reality

00:08:04,319 --> 00:08:08,000
so with that let me hand it over to

00:08:05,919 --> 00:08:10,080
hazel

00:08:08,000 --> 00:08:11,680
thank you interact now we'll get into

00:08:10,080 --> 00:08:13,120
the use case i can into it into it

00:08:11,680 --> 00:08:13,919
provides financial solution for

00:08:13,120 --> 00:08:15,759
consumers

00:08:13,919 --> 00:08:17,120
small businesses and self-employed

00:08:15,759 --> 00:08:18,960
individuals

00:08:17,120 --> 00:08:21,680
major products from intuit include turbo

00:08:18,960 --> 00:08:24,240
tax quickbooks and mint

00:08:21,680 --> 00:08:26,319
at into it kubernetes is adopted widely

00:08:24,240 --> 00:08:28,319
across four business units

00:08:26,319 --> 00:08:30,800
it's used extensively at over 30

00:08:28,319 --> 00:08:32,320
segments with more than 5000 engineers

00:08:30,800 --> 00:08:34,080
onboarded to it

00:08:32,320 --> 00:08:37,440
we also have over 80 engineers

00:08:34,080 --> 00:08:39,519
developing our kubernetes platform

00:08:37,440 --> 00:08:41,120
we'll see how fluent d and fluent bet

00:08:39,519 --> 00:08:43,839
are used to handle logging

00:08:41,120 --> 00:08:44,560
from kubernetes cluster set into it the

00:08:43,839 --> 00:08:46,720
use case

00:08:44,560 --> 00:08:47,839
is to transport logs generated within

00:08:46,720 --> 00:08:50,000
the container

00:08:47,839 --> 00:08:52,839
which is returned to std out or std

00:08:50,000 --> 00:08:54,399
error stream to our persistent lock

00:08:52,839 --> 00:08:56,000
store

00:08:54,399 --> 00:08:57,440
to enable this we have a daemon set

00:08:56,000 --> 00:08:59,279
deployed in the cluster

00:08:57,440 --> 00:09:01,120
this ensures that a logging part is

00:08:59,279 --> 00:09:03,440
present in each and every node

00:09:01,120 --> 00:09:05,120
this part contains fluency of one bit

00:09:03,440 --> 00:09:05,519
which is then responsible for collecting

00:09:05,120 --> 00:09:07,360
the

00:09:05,519 --> 00:09:08,720
and forwarding the logs from that entire

00:09:07,360 --> 00:09:10,720
node

00:09:08,720 --> 00:09:12,320
the docker logging plugin fights logs

00:09:10,720 --> 00:09:13,600
into the files which are tailed and

00:09:12,320 --> 00:09:15,279
processed

00:09:13,600 --> 00:09:16,959
the log events must also be enriched

00:09:15,279 --> 00:09:18,560
with some metadata

00:09:16,959 --> 00:09:20,000
this would enable us to identify the

00:09:18,560 --> 00:09:22,000
cluster namespace

00:09:20,000 --> 00:09:23,120
power and container where each event

00:09:22,000 --> 00:09:24,560
originates from

00:09:23,120 --> 00:09:27,440
when running search queries at the log

00:09:24,560 --> 00:09:29,440
store and all this must be fast

00:09:27,440 --> 00:09:31,360
and efficient we want to be able to

00:09:29,440 --> 00:09:33,279
transport huge volumes of data

00:09:31,360 --> 00:09:35,200
with as low end-to-end latency as

00:09:33,279 --> 00:09:36,880
possible

00:09:35,200 --> 00:09:38,320
we have a centralized persistent log

00:09:36,880 --> 00:09:41,040
store where all

00:09:38,320 --> 00:09:43,440
customers can see and query log events

00:09:41,040 --> 00:09:44,320
we wanted a common path to transport all

00:09:43,440 --> 00:09:46,720
the logs

00:09:44,320 --> 00:09:48,560
and it's a pipeline this would simplify

00:09:46,720 --> 00:09:50,560
the connectivity between the vpcs

00:09:48,560 --> 00:09:53,760
containing the kubernetes clusters

00:09:50,560 --> 00:09:55,680
and the vpc containing the log store

00:09:53,760 --> 00:09:58,160
you only have to expose the log store to

00:09:55,680 --> 00:10:00,160
the pipeline instead of all the clusters

00:09:58,160 --> 00:10:02,000
this helps us avoid sending the log

00:10:00,160 --> 00:10:04,000
events over the internet

00:10:02,000 --> 00:10:05,760
sending data over the internet is very

00:10:04,000 --> 00:10:08,000
expensive and we can cut down the cost

00:10:05,760 --> 00:10:10,480
drastically by sending data through aws

00:10:08,000 --> 00:10:11,920
network only

00:10:10,480 --> 00:10:14,240
during any connectivity issues with the

00:10:11,920 --> 00:10:14,880
lock store once the buffers get filled

00:10:14,240 --> 00:10:17,360
up

00:10:14,880 --> 00:10:18,079
the logs are gone for good if you have

00:10:17,360 --> 00:10:20,079
an intermediate

00:10:18,079 --> 00:10:21,200
store for these logs this can be

00:10:20,079 --> 00:10:22,959
prevented

00:10:21,200 --> 00:10:24,560
this also provides an opportunity for

00:10:22,959 --> 00:10:27,040
other applications to read

00:10:24,560 --> 00:10:28,800
the raw log data this is especially

00:10:27,040 --> 00:10:31,440
useful for security compliance and

00:10:28,800 --> 00:10:33,120
analytics applications

00:10:31,440 --> 00:10:36,000
this is our name approach which is a

00:10:33,120 --> 00:10:37,760
string pipeline there is a demonstrator

00:10:36,000 --> 00:10:40,160
so each kubernetes node

00:10:37,760 --> 00:10:42,079
will have a logging port running in it

00:10:40,160 --> 00:10:44,720
this part will tell the log file from

00:10:42,079 --> 00:10:46,640
the host mounted as volume

00:10:44,720 --> 00:10:48,800
there is a fluent bit plugin that gets

00:10:46,640 --> 00:10:51,279
the community's metadata from its api

00:10:48,800 --> 00:10:53,440
and enriches the log events with that

00:10:51,279 --> 00:10:55,680
these enriched log events are then sent

00:10:53,440 --> 00:10:57,680
to a kinesis data stream

00:10:55,680 --> 00:10:58,800
the events are then routed to kinesis

00:10:57,680 --> 00:11:01,040
firehose

00:10:58,800 --> 00:11:02,959
now the kinesis fire hose has a native

00:11:01,040 --> 00:11:04,880
capability to write to the log store and

00:11:02,959 --> 00:11:07,680
that's how the english log data

00:11:04,880 --> 00:11:08,959
gets to the persistent store this

00:11:07,680 --> 00:11:11,760
provides a reliable

00:11:08,959 --> 00:11:14,000
scalable mechanism to transport the logs

00:11:11,760 --> 00:11:17,120
from hundreds of kubernetes cluster

00:11:14,000 --> 00:11:19,040
onto a centralized position log store

00:11:17,120 --> 00:11:20,959
we only need access to the lobster from

00:11:19,040 --> 00:11:23,120
fire hose and this data transfer

00:11:20,959 --> 00:11:25,279
happened within a releases network

00:11:23,120 --> 00:11:26,320
also the logs can be read from kinesis

00:11:25,279 --> 00:11:29,519
data stream

00:11:26,320 --> 00:11:31,120
while just using a consumer this

00:11:29,519 --> 00:11:32,079
pipeline was working well for us

00:11:31,120 --> 00:11:34,399
initially

00:11:32,079 --> 00:11:35,600
but as we started supporting more and

00:11:34,399 --> 00:11:38,640
larger cluster

00:11:35,600 --> 00:11:40,480
we started facing some challenges

00:11:38,640 --> 00:11:42,160
with this pipeline we had a hard limit

00:11:40,480 --> 00:11:43,680
on the number of events that can be

00:11:42,160 --> 00:11:46,720
transported every second

00:11:43,680 --> 00:11:47,440
in each pod with the solution that used

00:11:46,720 --> 00:11:49,760
fluency

00:11:47,440 --> 00:11:53,200
as a logging container we were capped at

00:11:49,760 --> 00:11:54,800
2500 events per second per node

00:11:53,200 --> 00:11:57,120
once we should serve it to fluent bit

00:11:54,800 --> 00:11:59,440
base logging container we were able to

00:11:57,120 --> 00:12:01,200
double that limit to 5000 events per

00:11:59,440 --> 00:12:03,200
second per node

00:12:01,200 --> 00:12:04,800
however this was still not enough

00:12:03,200 --> 00:12:06,399
especially for larger nodes which

00:12:04,800 --> 00:12:08,160
contain a lot of containers packed

00:12:06,399 --> 00:12:10,000
inside

00:12:08,160 --> 00:12:12,079
also as you saw there are two hops

00:12:10,000 --> 00:12:13,839
between the cluster and the log store

00:12:12,079 --> 00:12:15,120
each of these hops were introducing some

00:12:13,839 --> 00:12:18,000
latency

00:12:15,120 --> 00:12:20,000
which was quickly adding up we started

00:12:18,000 --> 00:12:22,000
seeing the median n2 and latency getting

00:12:20,000 --> 00:12:24,240
as high as 30 seconds

00:12:22,000 --> 00:12:25,680
this was not acceptable for our most

00:12:24,240 --> 00:12:27,200
time critical applications

00:12:25,680 --> 00:12:29,360
so we decided to reduce the number of

00:12:27,200 --> 00:12:31,440
hops and hence the time it takes for the

00:12:29,360 --> 00:12:33,440
transport

00:12:31,440 --> 00:12:34,560
to maintain this streaming pipeline we

00:12:33,440 --> 00:12:36,880
had to have

00:12:34,560 --> 00:12:37,680
some persistent resources running the

00:12:36,880 --> 00:12:39,760
lock load

00:12:37,680 --> 00:12:40,880
that needed to be transported was

00:12:39,760 --> 00:12:42,480
extremely elastic

00:12:40,880 --> 00:12:44,000
especially based on different times of

00:12:42,480 --> 00:12:45,920
the day

00:12:44,000 --> 00:12:47,279
however these resources even with the

00:12:45,920 --> 00:12:49,120
auto scaling enable

00:12:47,279 --> 00:12:51,120
were not able to scale efficiently to

00:12:49,120 --> 00:12:53,200
match that log load

00:12:51,120 --> 00:12:54,399
this was making the pipeline inefficient

00:12:53,200 --> 00:12:56,639
in terms of the cost

00:12:54,399 --> 00:12:58,720
and ended up being too expensive we

00:12:56,639 --> 00:13:01,200
wanted to cut down some costs as well

00:12:58,720 --> 00:13:02,079
so in simpler terms the target for the

00:13:01,200 --> 00:13:03,519
new pipeline

00:13:02,079 --> 00:13:05,360
were to increase the throughput

00:13:03,519 --> 00:13:06,880
dramatically without increasing the

00:13:05,360 --> 00:13:09,760
resource consumption by that

00:13:06,880 --> 00:13:11,600
port reduce the end-to-end latency of

00:13:09,760 --> 00:13:12,560
the log transport pipeline as much as

00:13:11,600 --> 00:13:14,160
possible

00:13:12,560 --> 00:13:15,600
and to minimize the cost needed to

00:13:14,160 --> 00:13:17,920
maintain it

00:13:15,600 --> 00:13:20,959
and this is where the s3 pipeline comes

00:13:17,920 --> 00:13:22,800
into play

00:13:20,959 --> 00:13:24,880
this is how the architecture of s3

00:13:22,800 --> 00:13:26,880
pipeline looks like

00:13:24,880 --> 00:13:28,320
the space enclosed by the blue dashed

00:13:26,880 --> 00:13:31,200
line represent

00:13:28,320 --> 00:13:33,440
a node running in the kubernetes cluster

00:13:31,200 --> 00:13:36,639
each node contains a daemon set power

00:13:33,440 --> 00:13:38,959
containing fluently the locks written by

00:13:36,639 --> 00:13:39,839
the container on its std out and std

00:13:38,959 --> 00:13:41,680
error streams

00:13:39,839 --> 00:13:43,360
will be written as files that are

00:13:41,680 --> 00:13:46,240
rotated in the node

00:13:43,360 --> 00:13:48,160
and it's all done by the docker daemon

00:13:46,240 --> 00:13:50,399
these files are exposed to the container

00:13:48,160 --> 00:13:51,519
by using a volume mounted directly onto

00:13:50,399 --> 00:13:53,600
the host

00:13:51,519 --> 00:13:56,480
the fluency container then tails these

00:13:53,600 --> 00:13:58,240
files and listens for any new log events

00:13:56,480 --> 00:13:59,760
and for any new log events fluently

00:13:58,240 --> 00:14:01,440
pushes it onto a buffer which is

00:13:59,760 --> 00:14:03,600
specific for each container

00:14:01,440 --> 00:14:04,800
at a certain fixed interval 10 seconds

00:14:03,600 --> 00:14:07,839
in this case

00:14:04,800 --> 00:14:10,480
the buffers are flush onto an s3 bucket

00:14:07,839 --> 00:14:12,320
object after compressing it with g6 so

00:14:10,480 --> 00:14:13,920
each s3 object would contain logs

00:14:12,320 --> 00:14:16,720
generated by the container for 10

00:14:13,920 --> 00:14:18,160
seconds compressed as a jzip file

00:14:16,720 --> 00:14:20,000
in the pipeline fluently is not

00:14:18,160 --> 00:14:21,040
responsible for enriching the log data

00:14:20,000 --> 00:14:23,199
with metadata

00:14:21,040 --> 00:14:25,040
but rather provides the logs as is onto

00:14:23,199 --> 00:14:26,560
the object

00:14:25,040 --> 00:14:28,399
now we have a separate deployment in the

00:14:26,560 --> 00:14:29,839
cluster which ensures that a single

00:14:28,399 --> 00:14:30,880
power will be running across this

00:14:29,839 --> 00:14:32,399
cluster

00:14:30,880 --> 00:14:34,639
this deployment communicates with

00:14:32,399 --> 00:14:37,360
kubernetes api and gets information

00:14:34,639 --> 00:14:38,880
about any new containers for any new

00:14:37,360 --> 00:14:40,880
containers created in the cluster

00:14:38,880 --> 00:14:42,720
it will fetch its placement and other

00:14:40,880 --> 00:14:43,760
metadata including its labels and

00:14:42,720 --> 00:14:45,600
annotations

00:14:43,760 --> 00:14:47,519
and pushes it on to a separate extra

00:14:45,600 --> 00:14:50,079
bucket

00:14:47,519 --> 00:14:51,120
within the log pipeline vpc we have a

00:14:50,079 --> 00:14:53,839
deployment running in

00:14:51,120 --> 00:14:56,079
aws far gate this deployment listens to

00:14:53,839 --> 00:14:57,839
any new objects in the log s3 packet

00:14:56,079 --> 00:14:59,440
for any new object in this bucket the

00:14:57,839 --> 00:15:00,959
path would indicate the container for

00:14:59,440 --> 00:15:02,639
which the logs are for

00:15:00,959 --> 00:15:04,240
the log pipeline deployment then fetch

00:15:02,639 --> 00:15:07,040
the corresponding metadata

00:15:04,240 --> 00:15:08,639
from the metadata bucket both these are

00:15:07,040 --> 00:15:10,800
then sent over together

00:15:08,639 --> 00:15:12,560
onto the log store present in a separate

00:15:10,800 --> 00:15:14,959
icon and bpc

00:15:12,560 --> 00:15:16,320
this communication happens over aws's

00:15:14,959 --> 00:15:18,320
transit gateway

00:15:16,320 --> 00:15:20,399
the log store is then able to apply the

00:15:18,320 --> 00:15:20,800
metadata to each and every event in the

00:15:20,399 --> 00:15:23,519
log

00:15:20,800 --> 00:15:25,440
s3 object the key takeaways in the

00:15:23,519 --> 00:15:26,880
architecture are that the log data is

00:15:25,440 --> 00:15:29,440
compressed from the cluster

00:15:26,880 --> 00:15:31,120
and only deflated at the log store and

00:15:29,440 --> 00:15:32,399
all the data transfer happens within the

00:15:31,120 --> 00:15:34,880
aws network

00:15:32,399 --> 00:15:35,440
and moreover crossover transfer have

00:15:34,880 --> 00:15:37,360
been

00:15:35,440 --> 00:15:41,839
via a transfer gateway created

00:15:37,360 --> 00:15:41,839
specifically for that purpose

00:15:41,920 --> 00:15:45,839
as you can see fluently is deployed in

00:15:44,639 --> 00:15:47,440
the cluster as a daemon set

00:15:45,839 --> 00:15:49,040
this ensures that there is a single

00:15:47,440 --> 00:15:50,959
power in every node

00:15:49,040 --> 00:15:52,480
and each pod is then responsible for

00:15:50,959 --> 00:15:54,880
transporting logs

00:15:52,480 --> 00:15:57,279
from all the containers in that node

00:15:54,880 --> 00:15:58,880
regardless of how big that node is

00:15:57,279 --> 00:16:00,399
from this we can realize that this part

00:15:58,880 --> 00:16:02,560
will become the bottleneck and the

00:16:00,399 --> 00:16:04,639
weakest link in the chain

00:16:02,560 --> 00:16:06,839
so the throughput will be set by the

00:16:04,639 --> 00:16:08,160
moon that can be transported by a single

00:16:06,839 --> 00:16:09,839
part

00:16:08,160 --> 00:16:11,519
the biggest gain in the throughput can

00:16:09,839 --> 00:16:14,399
be achieved by making demonstrate

00:16:11,519 --> 00:16:16,720
process transport more and more events

00:16:14,399 --> 00:16:17,519
one mechanism to achieve this is to do

00:16:16,720 --> 00:16:19,360
as little

00:16:17,519 --> 00:16:21,040
processing as possible at the daemon set

00:16:19,360 --> 00:16:23,360
part

00:16:21,040 --> 00:16:24,959
that is the key to note in the new

00:16:23,360 --> 00:16:26,639
pipeline design

00:16:24,959 --> 00:16:28,240
next we wanted to identify all the

00:16:26,639 --> 00:16:28,959
things that can be offloaded from the

00:16:28,240 --> 00:16:31,040
fluency

00:16:28,959 --> 00:16:32,639
and how we facilitate that function

00:16:31,040 --> 00:16:34,399
elsewhere

00:16:32,639 --> 00:16:36,000
after multiplication we were able to

00:16:34,399 --> 00:16:38,240
pick two cpu intensive

00:16:36,000 --> 00:16:39,759
processors that can be offloaded without

00:16:38,240 --> 00:16:40,800
any change in the events at the log

00:16:39,759 --> 00:16:42,160
store

00:16:40,800 --> 00:16:44,480
these were providing us with this

00:16:42,160 --> 00:16:46,000
massive genuine support

00:16:44,480 --> 00:16:48,160
so the first thing that we were able to

00:16:46,000 --> 00:16:49,920
offload was timestamp parsing

00:16:48,160 --> 00:16:51,440
timestamp has to be passed from each

00:16:49,920 --> 00:16:53,680
event for a couple of things

00:16:51,440 --> 00:16:55,759
to attribute a timestamp for each event

00:16:53,680 --> 00:16:57,440
and to identify the starting line

00:16:55,759 --> 00:16:58,959
of new event in case of multi-line

00:16:57,440 --> 00:17:01,519
events

00:16:58,959 --> 00:17:03,680
this was very cpu intensive but we were

00:17:01,519 --> 00:17:05,280
able to offload this onto our log store

00:17:03,680 --> 00:17:06,720
without any change in the end user

00:17:05,280 --> 00:17:08,799
experience

00:17:06,720 --> 00:17:10,640
the second thing we offloaded is the

00:17:08,799 --> 00:17:12,319
metadata enrichment

00:17:10,640 --> 00:17:14,160
in the previous pipeline each event was

00:17:12,319 --> 00:17:15,760
enriched with the required metadata by

00:17:14,160 --> 00:17:17,919
fluency itself

00:17:15,760 --> 00:17:20,559
this has now been offloaded and done by

00:17:17,919 --> 00:17:23,280
the metadata deployment

00:17:20,559 --> 00:17:24,319
external log pipeline deployment and the

00:17:23,280 --> 00:17:25,919
log store

00:17:24,319 --> 00:17:27,439
the metadata deployment communicates

00:17:25,919 --> 00:17:29,440
with the kubernetes api

00:17:27,439 --> 00:17:30,960
to get the required container metadata

00:17:29,440 --> 00:17:32,400
in an s3 object

00:17:30,960 --> 00:17:34,240
the external log pipeline deployment

00:17:32,400 --> 00:17:35,120
then collates this container specific

00:17:34,240 --> 00:17:37,520
metadata

00:17:35,120 --> 00:17:39,600
with each s3 log object which is also

00:17:37,520 --> 00:17:41,520
container specific

00:17:39,600 --> 00:17:43,120
both these are then sent together to the

00:17:41,520 --> 00:17:45,120
log store which is up which can then

00:17:43,120 --> 00:17:47,360
apply the metadata to all the live

00:17:45,120 --> 00:17:49,280
events in the file the additional

00:17:47,360 --> 00:17:50,799
advantage with this is that the metadata

00:17:49,280 --> 00:17:52,799
enrichment happens in batch

00:17:50,799 --> 00:17:55,440
across all the events in ns3 object at

00:17:52,799 --> 00:18:00,799
once this reduces the processing

00:17:55,440 --> 00:18:02,640
needed and the amount of data transfer

00:18:00,799 --> 00:18:04,400
when looking at the cost breakdown for

00:18:02,640 --> 00:18:07,039
the pipeline we found that the data

00:18:04,400 --> 00:18:10,160
transfer cost was the highest component

00:18:07,039 --> 00:18:11,840
higher than even compute and storage

00:18:10,160 --> 00:18:14,640
so if we can reduce the amount of data

00:18:11,840 --> 00:18:15,840
sent over and use a more cost effective

00:18:14,640 --> 00:18:18,320
way to transport it

00:18:15,840 --> 00:18:20,160
they can make huge savings in the cost

00:18:18,320 --> 00:18:22,559
one added benefit is that

00:18:20,160 --> 00:18:25,600
as we reduce the amount of data transfer

00:18:22,559 --> 00:18:27,440
the latency also goes down

00:18:25,600 --> 00:18:28,880
as mentioned before fluently compresses

00:18:27,440 --> 00:18:30,559
the data via gzip

00:18:28,880 --> 00:18:31,919
and the buffer when it's flushed to an

00:18:30,559 --> 00:18:34,480
s3 object

00:18:31,919 --> 00:18:36,240
in our normal load load we were getting

00:18:34,480 --> 00:18:39,520
around 10 times compression

00:18:36,240 --> 00:18:40,880
this gc file then be returned to log

00:18:39,520 --> 00:18:42,640
store asses

00:18:40,880 --> 00:18:44,400
what that means is that the log data

00:18:42,640 --> 00:18:45,360
stays compressed as soon as it leaves

00:18:44,400 --> 00:18:47,520
the clusters

00:18:45,360 --> 00:18:49,280
and until it reaches the long story or

00:18:47,520 --> 00:18:50,240
the log data is always compressed in

00:18:49,280 --> 00:18:51,919
transit

00:18:50,240 --> 00:18:54,400
just this one change cut down the data

00:18:51,919 --> 00:18:56,000
transferred to one tenth the volume to

00:18:54,400 --> 00:18:58,240
that of before

00:18:56,000 --> 00:18:59,280
also as we are applying the metadata in

00:18:58,240 --> 00:19:01,280
batch for an entire

00:18:59,280 --> 00:19:02,960
three object each object even need not

00:19:01,280 --> 00:19:04,960
have the metadata along with it

00:19:02,960 --> 00:19:06,880
this was also giving us large strings in

00:19:04,960 --> 00:19:08,840
terms of the amount of data transferred

00:19:06,880 --> 00:19:10,080
by cutting down the size of each row log

00:19:08,840 --> 00:19:11,600
event

00:19:10,080 --> 00:19:13,280
this is when we compare it to the

00:19:11,600 --> 00:19:15,360
previous string pipeline

00:19:13,280 --> 00:19:17,760
we also have set up an aws transit

00:19:15,360 --> 00:19:20,080
gateway between our log pipeline vpc

00:19:17,760 --> 00:19:22,000
and the log story pc and all the data

00:19:20,080 --> 00:19:24,240
transfer happens through this

00:19:22,000 --> 00:19:25,840
this was giving us around 60 in cost

00:19:24,240 --> 00:19:26,720
saving as opposed to sending the data

00:19:25,840 --> 00:19:29,760
over the internet

00:19:26,720 --> 00:19:32,080
using an app gateway

00:19:29,760 --> 00:19:34,240
to demonstrate how this new pipeline

00:19:32,080 --> 00:19:34,799
performs i have set up a test united

00:19:34,240 --> 00:19:37,280
cluster

00:19:34,799 --> 00:19:39,039
with three worker nodes each node

00:19:37,280 --> 00:19:40,960
contains the daemon set bar

00:19:39,039 --> 00:19:42,640
and a few other parts to generate some

00:19:40,960 --> 00:19:44,480
sample log events

00:19:42,640 --> 00:19:45,919
this cluster also contains the metadata

00:19:44,480 --> 00:19:47,520
deployment

00:19:45,919 --> 00:19:48,960
as you can see from our metrics

00:19:47,520 --> 00:19:51,120
dashboard that with

00:19:48,960 --> 00:19:52,000
three nodes it's able to transport 90

00:19:51,120 --> 00:19:55,360
000 events per

00:19:52,000 --> 00:19:57,440
second without any errors so that means

00:19:55,360 --> 00:19:58,960
that each node can support up to 30 000

00:19:57,440 --> 00:20:00,799
events per second

00:19:58,960 --> 00:20:03,120
we also see that the cpu of the parts

00:20:00,799 --> 00:20:04,000
stays stable and that it can constantly

00:20:03,120 --> 00:20:07,360
support the log

00:20:04,000 --> 00:20:09,440
over a period of time

00:20:07,360 --> 00:20:11,120
the lock generator in this each node

00:20:09,440 --> 00:20:12,159
collectively was generating 30 000

00:20:11,120 --> 00:20:15,120
events per second

00:20:12,159 --> 00:20:17,760
and we have three such nodes so the

00:20:15,120 --> 00:20:18,880
total log load was around 90 000 events

00:20:17,760 --> 00:20:21,200
per second

00:20:18,880 --> 00:20:23,600
and i ran this log test for an hour so

00:20:21,200 --> 00:20:25,600
that should give us 324 million events

00:20:23,600 --> 00:20:27,840
at our lock store that's exactly what we

00:20:25,600 --> 00:20:29,200
see in our log dashboard

00:20:27,840 --> 00:20:31,039
we can now see that the end to end

00:20:29,200 --> 00:20:33,120
latency which is a time difference

00:20:31,039 --> 00:20:36,080
between the container logging and event

00:20:33,120 --> 00:20:38,000
and the logs are indexing it we see that

00:20:36,080 --> 00:20:38,960
the median end-to-end latency is just

00:20:38,000 --> 00:20:41,679
around 9

00:20:38,960 --> 00:20:42,960
8 seconds and the 99th personnel is less

00:20:41,679 --> 00:20:44,880
than 14 seconds

00:20:42,960 --> 00:20:47,120
that's a massive drop from our streaming

00:20:44,880 --> 00:20:48,480
pipeline

00:20:47,120 --> 00:20:51,120
let me give a summary on the

00:20:48,480 --> 00:20:53,039
improvements of this s3 based pipeline

00:20:51,120 --> 00:20:54,799
over a streaming pipeline that we saw

00:20:53,039 --> 00:20:56,640
from the demo

00:20:54,799 --> 00:20:58,000
in the case of throughput we saw a

00:20:56,640 --> 00:20:59,440
massive six times

00:20:58,000 --> 00:21:02,000
jump in the number of events that can be

00:20:59,440 --> 00:21:05,039
transported from 5000 events per second

00:21:02,000 --> 00:21:06,000
per node to 30 000 events per second per

00:21:05,039 --> 00:21:08,320
node

00:21:06,000 --> 00:21:09,679
moreover we have tested transporting

00:21:08,320 --> 00:21:11,280
over one gigabyte

00:21:09,679 --> 00:21:13,200
per second from a single kubernetes

00:21:11,280 --> 00:21:15,039
cluster

00:21:13,200 --> 00:21:17,520
next up we see that the median engine

00:21:15,039 --> 00:21:19,039
latency drops to less than one-third

00:21:17,520 --> 00:21:21,120
from around 30 seconds in the streaming

00:21:19,039 --> 00:21:22,720
pipeline to just over eight seconds in

00:21:21,120 --> 00:21:24,960
the s3 based pipeline

00:21:22,720 --> 00:21:28,000
the difference grows considerably when

00:21:24,960 --> 00:21:31,200
considering the 99 first l which

00:21:28,000 --> 00:21:32,799
has a latency cut down by more than 75

00:21:31,200 --> 00:21:34,640
finally looking at the cost we see the

00:21:32,799 --> 00:21:36,720
cost saving of more than 92

00:21:34,640 --> 00:21:38,640
when compared to the streaming pipeline

00:21:36,720 --> 00:21:40,480
this is due to not having

00:21:38,640 --> 00:21:41,840
to keep persistent resources running

00:21:40,480 --> 00:21:43,919
compression of the log data

00:21:41,840 --> 00:21:45,440
and using transit gateway this was

00:21:43,919 --> 00:21:48,240
coming around to more than 50

00:21:45,440 --> 00:21:49,679
000 us dollars saved over every petabyte

00:21:48,240 --> 00:21:51,440
transfer

00:21:49,679 --> 00:21:53,760
this is a story of how we were able to

00:21:51,440 --> 00:21:54,400
create a petabyte scale logging pipeline

00:21:53,760 --> 00:21:56,480
at intuit

00:21:54,400 --> 00:21:58,480
using fluency and fluent bit thank you

00:21:56,480 --> 00:21:58,960
all so much for participating in this

00:21:58,480 --> 00:22:00,559
session

00:21:58,960 --> 00:22:02,720
and i would like to open up the forum

00:22:00,559 --> 00:22:06,559
for questions at this point

00:22:02,720 --> 00:22:06,559

YouTube URL: https://www.youtube.com/watch?v=OuKm2SiwnGM


