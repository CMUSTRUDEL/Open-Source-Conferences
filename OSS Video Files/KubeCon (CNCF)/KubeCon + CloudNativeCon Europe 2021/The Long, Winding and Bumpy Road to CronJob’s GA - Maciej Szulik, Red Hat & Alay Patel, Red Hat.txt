Title: The Long, Winding and Bumpy Road to CronJob’s GA - Maciej Szulik, Red Hat & Alay Patel, Red Hat
Publication date: 2021-05-09
Playlist: KubeCon + CloudNativeCon Europe 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

The Long, Winding and Bumpy Road to CronJob’s GA - Maciej Szulik, Red Hat & Alay Patel, Red Hat

The CronJob API just reached GA, and the new controller is solving all the performance and reliability problems of the past. Come join us to learn about the 6 year journey that got us here! We will talk about the people who sparked the discussions and delivered the initial implementation. We will cover all the major problems that users were faced to handle over the years. Finally, we will discuss the solutions and our gratitude to the users and developers standing behind this part of Kubernetes. Maciej (one of the co-authors of CronJob) and Alay (developer of the new controller) will try to squeeze as much as possible in 30 minutes: - From scheduled jobs, through jobs, to cron jobs - Evolution of the API - Known issues with the old controller implementation - Performance boost and improvements in the new controller - Possible future improvements
Captions: 
	00:00:00,240 --> 00:00:06,319
hi i am oleg madel

00:00:03,120 --> 00:00:09,679
and my name is mate and today

00:00:06,319 --> 00:00:13,519
i'm going to take you to a long winding

00:00:09,679 --> 00:00:16,000
and bumpy road to crown jobs ga

00:00:13,519 --> 00:00:16,720
the idea of cron jobs wasn't entirely

00:00:16,000 --> 00:00:20,320
new in the

00:00:16,720 --> 00:00:22,400
distributed world identical mechanism

00:00:20,320 --> 00:00:25,760
was already available in google's

00:00:22,400 --> 00:00:28,840
internal cluster management system borg

00:00:25,760 --> 00:00:30,000
which divided workloads in two basic

00:00:28,840 --> 00:00:34,079
categories

00:00:30,000 --> 00:00:35,520
long-running services and bad jobs

00:00:34,079 --> 00:00:40,079
for the remaining part of this

00:00:35,520 --> 00:00:40,079
presentation we'll focus on the latter

00:00:40,160 --> 00:00:44,800
it wasn't long after kubernetes was

00:00:42,160 --> 00:00:48,399
officially announced by google

00:00:44,800 --> 00:00:51,840
in mid 2014 that this functionality will

00:00:48,399 --> 00:00:54,480
be needed in this project as well

00:00:51,840 --> 00:00:56,640
my first exposure to this topic was in

00:00:54,480 --> 00:00:59,520
the spring of 2015

00:00:56,640 --> 00:01:01,199
when a bunch of us here at red hat along

00:00:59,520 --> 00:01:03,920
with our customers

00:01:01,199 --> 00:01:05,360
and small community that has gathered

00:01:03,920 --> 00:01:08,720
around kubernetes

00:01:05,360 --> 00:01:11,520
before its 1.0 release later that year

00:01:08,720 --> 00:01:11,840
we started tinkering with this problem

00:01:11,520 --> 00:01:14,880
and

00:01:11,840 --> 00:01:18,080
writing down our thoughts

00:01:14,880 --> 00:01:19,680
first in openshift repository in may

00:01:18,080 --> 00:01:23,280
june of that year

00:01:19,680 --> 00:01:25,840
eventually reaching consensus

00:01:23,280 --> 00:01:27,680
i've opened a proposal back in august

00:01:25,840 --> 00:01:29,680
2015.

00:01:27,680 --> 00:01:31,520
only after we moved our proposal to

00:01:29,680 --> 00:01:34,720
kubernetes repository

00:01:31,520 --> 00:01:35,920
more people including the authors of the

00:01:34,720 --> 00:01:38,720
borg paper

00:01:35,920 --> 00:01:40,000
started looking at it the proposal was

00:01:38,720 --> 00:01:43,119
sliced and diced

00:01:40,000 --> 00:01:44,799
into many directions eventually we've

00:01:43,119 --> 00:01:48,799
agreed to split the topic

00:01:44,799 --> 00:01:51,360
into two first the original proposal

00:01:48,799 --> 00:01:52,640
morphed from describing distributed cron

00:01:51,360 --> 00:01:55,439
functionality

00:01:52,640 --> 00:01:57,439
to a primitive which allows running a

00:01:55,439 --> 00:02:00,320
task to completion

00:01:57,439 --> 00:02:01,759
today this is simply known as a job

00:02:00,320 --> 00:02:05,280
resource

00:02:01,759 --> 00:02:08,319
this way our focus slightly shifted

00:02:05,280 --> 00:02:10,640
and we dived into implementing jobs

00:02:08,319 --> 00:02:13,360
which were part of 1.1 release of

00:02:10,640 --> 00:02:13,360
kubernetes

00:02:13,680 --> 00:02:18,239
it's important to mention the fact that

00:02:16,080 --> 00:02:21,760
back then kubernetes did not have

00:02:18,239 --> 00:02:22,080
api groups like it has today so adding a

00:02:21,760 --> 00:02:24,239
new

00:02:22,080 --> 00:02:25,360
resource was challenging to say the

00:02:24,239 --> 00:02:28,319
least

00:02:25,360 --> 00:02:30,239
on top of that jobs and cron jobs were

00:02:28,319 --> 00:02:34,879
very often trade blazers

00:02:30,239 --> 00:02:37,840
in the areas of api groups and versions

00:02:34,879 --> 00:02:39,200
finally after shipping jobs in january

00:02:37,840 --> 00:02:41,760
of 2016

00:02:39,200 --> 00:02:43,519
i was able to finally focus on the

00:02:41,760 --> 00:02:45,920
initial ask

00:02:43,519 --> 00:02:46,959
what i haven't mentioned before is that

00:02:45,920 --> 00:02:49,280
back then

00:02:46,959 --> 00:02:50,720
cron jobs were actually called schedule

00:02:49,280 --> 00:02:54,480
at jobs

00:02:50,720 --> 00:02:56,560
naming his heart isn't it honestly

00:02:54,480 --> 00:02:57,599
i can't remember the reasoning behind

00:02:56,560 --> 00:03:00,319
the name

00:02:57,599 --> 00:03:04,319
but i'm pretty sure it is still there in

00:03:00,319 --> 00:03:06,800
those proposals and discussions

00:03:04,319 --> 00:03:09,360
at that moment in time we've all had a

00:03:06,800 --> 00:03:12,080
pretty good understanding of the topic

00:03:09,360 --> 00:03:14,560
as well as the primitives and mechanism

00:03:12,080 --> 00:03:17,680
to build scheduled jobs

00:03:14,560 --> 00:03:20,560
so as soon as the proposal was merged we

00:03:17,680 --> 00:03:23,599
jumped into the implementation

00:03:20,560 --> 00:03:25,680
even though we initially targeted 1.3

00:03:23,599 --> 00:03:26,640
release we had to delay the

00:03:25,680 --> 00:03:30,159
functionality

00:03:26,640 --> 00:03:31,840
until 1.4 due to many challenges i've

00:03:30,159 --> 00:03:35,280
mentioned before

00:03:31,840 --> 00:03:39,040
this way in the late summer of 2016

00:03:35,280 --> 00:03:41,519
we had scheduled jobs in kubernetes

00:03:39,040 --> 00:03:44,000
there is one important cavite which i

00:03:41,519 --> 00:03:46,879
have which has a crucial impact

00:03:44,000 --> 00:03:47,760
to the rest of this presentation as you

00:03:46,879 --> 00:03:49,920
see

00:03:47,760 --> 00:03:51,599
the scheduled job controller was written

00:03:49,920 --> 00:03:54,239
in 2016.

00:03:51,599 --> 00:03:55,599
back then writing controllers was

00:03:54,239 --> 00:04:00,080
completely different

00:03:55,599 --> 00:04:01,519
from what it is today the controller was

00:04:00,080 --> 00:04:04,159
periodically pulling

00:04:01,519 --> 00:04:06,799
all scheduler jobs and triggering the

00:04:04,159 --> 00:04:09,280
ones that were supposed to run

00:04:06,799 --> 00:04:13,280
this way seemed the simplest and most

00:04:09,280 --> 00:04:16,639
obvious to solve the problem at the time

00:04:13,280 --> 00:04:18,560
after that initial sprint both with jobs

00:04:16,639 --> 00:04:20,639
and later with scheduled jobs

00:04:18,560 --> 00:04:23,040
we slowed down and focused on other

00:04:20,639 --> 00:04:25,120
features and fixing bugs

00:04:23,040 --> 00:04:26,479
the two major developments worth

00:04:25,120 --> 00:04:28,960
mentioning here

00:04:26,479 --> 00:04:29,759
were when we decided to rename the cron

00:04:28,960 --> 00:04:33,680
jobs

00:04:29,759 --> 00:04:36,240
which eventually landed in 2015

00:04:33,680 --> 00:04:37,120
the biggest challenge was that for a few

00:04:36,240 --> 00:04:39,600
releases

00:04:37,120 --> 00:04:40,960
we actually supported both the old

00:04:39,600 --> 00:04:44,080
scheduler jobs

00:04:40,960 --> 00:04:46,560
and a new cronjobs name which wasn't an

00:04:44,080 --> 00:04:49,759
easy thing to do

00:04:46,560 --> 00:04:52,160
eventually in 1.8 we've decided to

00:04:49,759 --> 00:04:54,880
promote cron jobs to beta

00:04:52,160 --> 00:04:56,240
without too much changes in the api or

00:04:54,880 --> 00:05:00,080
the controller

00:04:56,240 --> 00:05:00,080
that was back in 2017

00:05:00,240 --> 00:05:07,199
until 2020 hit us from many angles

00:05:04,720 --> 00:05:08,479
for us the biggest impact was the

00:05:07,199 --> 00:05:11,600
proposal to remove

00:05:08,479 --> 00:05:12,880
all the so-called perma beta apis from

00:05:11,600 --> 00:05:15,680
kubernetes

00:05:12,880 --> 00:05:16,639
in the near future the community

00:05:15,680 --> 00:05:18,639
gathered around

00:05:16,639 --> 00:05:20,240
special interest group devoted to

00:05:18,639 --> 00:05:23,440
running application

00:05:20,240 --> 00:05:25,520
or sigaps in short debated the problem

00:05:23,440 --> 00:05:28,000
for every api

00:05:25,520 --> 00:05:29,919
where we've been impacted since that

00:05:28,000 --> 00:05:33,600
only cron jobs had a similar

00:05:29,919 --> 00:05:36,560
issue of perma-bait state

00:05:33,600 --> 00:05:38,479
we've had many lengthy discussions which

00:05:36,560 --> 00:05:40,800
you can check in the sig apps meeting

00:05:38,479 --> 00:05:42,960
notes and recordings

00:05:40,800 --> 00:05:44,560
the outcome of which was that the best

00:05:42,960 --> 00:05:47,039
way to move forward

00:05:44,560 --> 00:05:49,120
is to reduce to a first introduce a

00:05:47,039 --> 00:05:51,039
completely new controller

00:05:49,120 --> 00:05:52,560
gradually switching the controllers

00:05:51,039 --> 00:05:55,520
through a few releases

00:05:52,560 --> 00:05:57,840
and only then promoting cron jobs api to

00:05:55,520 --> 00:06:00,240
general availability

00:05:57,840 --> 00:06:02,000
this would ensure that the existing

00:06:00,240 --> 00:06:05,360
production clusters wouldn't be

00:06:02,000 --> 00:06:07,680
immediately affected by these changes

00:06:05,360 --> 00:06:08,720
the main reason behind all this was

00:06:07,680 --> 00:06:11,440
coming from how

00:06:08,720 --> 00:06:13,360
modern controllers are written the

00:06:11,440 --> 00:06:15,440
previous controller was periodically

00:06:13,360 --> 00:06:18,240
polling api server

00:06:15,440 --> 00:06:23,120
modern controllers on the other hand are

00:06:18,240 --> 00:06:23,120
only notified about changes to resources

00:06:24,560 --> 00:06:28,240
the seemingly slight change has a

00:06:27,360 --> 00:06:31,280
significant

00:06:28,240 --> 00:06:33,120
impact both on the performance but also

00:06:31,280 --> 00:06:34,880
on the scheduling algorithm of the

00:06:33,120 --> 00:06:37,840
controller

00:06:34,880 --> 00:06:38,720
but more about this you will hear from

00:06:37,840 --> 00:06:40,960
ally

00:06:38,720 --> 00:06:44,160
who approached me last year expressing

00:06:40,960 --> 00:06:46,319
his interest to write that controller

00:06:44,160 --> 00:06:48,080
as much you mentioned most modern day

00:06:46,319 --> 00:06:50,560
controllers have evolved into a

00:06:48,080 --> 00:06:53,440
notification based system to synchronize

00:06:50,560 --> 00:06:55,840
state let's take a brief look at what

00:06:53,440 --> 00:06:58,319
the architecture of a stock controller

00:06:55,840 --> 00:06:59,440
in code kubernetes looks like this will

00:06:58,319 --> 00:07:01,440
help a lot in

00:06:59,440 --> 00:07:04,080
understanding how the new cron job

00:07:01,440 --> 00:07:06,880
controller is implemented

00:07:04,080 --> 00:07:07,599
as you can see in the picture there are

00:07:06,880 --> 00:07:10,000
two key

00:07:07,599 --> 00:07:10,720
elements to a cube controller that are

00:07:10,000 --> 00:07:14,160
connected

00:07:10,720 --> 00:07:17,039
by the queue on the left

00:07:14,160 --> 00:07:17,680
we have the shared informers giving us

00:07:17,039 --> 00:07:21,120
the add

00:07:17,680 --> 00:07:24,319
update delete notifications on the right

00:07:21,120 --> 00:07:26,639
we have the other element

00:07:24,319 --> 00:07:28,840
one which is responsible for performing

00:07:26,639 --> 00:07:31,599
the sync

00:07:28,840 --> 00:07:36,319
actions

00:07:31,599 --> 00:07:39,199
upon any event from the informer

00:07:36,319 --> 00:07:40,000
the resource event handlers allows us to

00:07:39,199 --> 00:07:43,039
push

00:07:40,000 --> 00:07:45,919
um keys or objects to the queue

00:07:43,039 --> 00:07:47,599
the handler on the right then pulls

00:07:45,919 --> 00:07:49,599
those objects from the queue and

00:07:47,599 --> 00:07:53,120
performs reconciliation

00:07:49,599 --> 00:07:56,000
the queue here makes sure that no two

00:07:53,120 --> 00:07:57,120
um handler workers are reconciling at

00:07:56,000 --> 00:08:00,080
the same time

00:07:57,120 --> 00:08:01,680
on the same key this allows us for

00:08:00,080 --> 00:08:06,400
safely scaling up

00:08:01,680 --> 00:08:06,400
the number of worker handler functions

00:08:07,120 --> 00:08:13,520
the informer also acts as a cache

00:08:10,560 --> 00:08:15,440
and allows the controller to list or get

00:08:13,520 --> 00:08:18,000
objects from it

00:08:15,440 --> 00:08:20,240
it is quite clear that if the new cron

00:08:18,000 --> 00:08:23,680
job controller is implemented on

00:08:20,240 --> 00:08:25,759
this architecture we would improve the

00:08:23,680 --> 00:08:27,440
performance by reducing the number of

00:08:25,759 --> 00:08:29,919
calls to api server

00:08:27,440 --> 00:08:32,719
also we will think we would improve on

00:08:29,919 --> 00:08:35,919
the scalability aspects

00:08:32,719 --> 00:08:38,959
now let's quickly dive into and look

00:08:35,919 --> 00:08:41,599
uh at some examples of the informers and

00:08:38,959 --> 00:08:45,519
event handlers

00:08:41,599 --> 00:08:48,800
so if you look at the code snippet

00:08:45,519 --> 00:08:49,760
this is the cron job controller using

00:08:48,800 --> 00:08:52,480
the

00:08:49,760 --> 00:08:53,440
jobs informer to register three event

00:08:52,480 --> 00:08:56,320
handlers

00:08:53,440 --> 00:08:56,560
typically a controller works on the set

00:08:56,320 --> 00:08:58,640
of

00:08:56,560 --> 00:09:00,959
objects for example the cron job

00:08:58,640 --> 00:09:01,680
controller cares about events from the

00:09:00,959 --> 00:09:05,120
jobs

00:09:01,680 --> 00:09:08,160
as well as crown jobs in order to

00:09:05,120 --> 00:09:10,800
cue the cron jobs

00:09:08,160 --> 00:09:13,200
effectively it uses both the crown jobs

00:09:10,800 --> 00:09:16,800
informer and the jobs informer

00:09:13,200 --> 00:09:20,880
the court snippet shows this um

00:09:16,800 --> 00:09:23,360
shows the ad job event handler

00:09:20,880 --> 00:09:24,240
further down the slide you can see that

00:09:23,360 --> 00:09:27,519
the edge of

00:09:24,240 --> 00:09:29,279
event handler takes in the um object

00:09:27,519 --> 00:09:33,040
that is coming in via

00:09:29,279 --> 00:09:33,760
the event um type casts it into the job

00:09:33,040 --> 00:09:36,800
object

00:09:33,760 --> 00:09:39,519
it then uses the job's

00:09:36,800 --> 00:09:40,320
owner reference to find the appropriate

00:09:39,519 --> 00:09:44,000
cron job

00:09:40,320 --> 00:09:48,560
for it it then enqueues the cron job

00:09:44,000 --> 00:09:51,120
onto the queue so to sum this up

00:09:48,560 --> 00:09:52,240
resource event handlers along resource

00:09:51,120 --> 00:09:55,680
event handlers are

00:09:52,240 --> 00:09:57,440
implemented in a way that converts the

00:09:55,680 --> 00:10:00,560
events from the informers

00:09:57,440 --> 00:10:05,040
into specific keys that are inserted

00:10:00,560 --> 00:10:07,600
onto the queue

00:10:05,040 --> 00:10:09,519
once we have the elements on the queue

00:10:07,600 --> 00:10:13,200
the wiring of the workers

00:10:09,519 --> 00:10:14,160
or handler um handlers that perform sync

00:10:13,200 --> 00:10:17,279
operations

00:10:14,160 --> 00:10:18,160
is actually quite simple the worker will

00:10:17,279 --> 00:10:21,440
then just

00:10:18,160 --> 00:10:24,399
um pull um or loop

00:10:21,440 --> 00:10:25,920
on a function called the process next i

00:10:24,399 --> 00:10:29,120
work item function

00:10:25,920 --> 00:10:33,040
um the very first thing in that function

00:10:29,120 --> 00:10:35,839
is the function is attempting to

00:10:33,040 --> 00:10:36,240
get the object from the queue this get

00:10:35,839 --> 00:10:38,800
call

00:10:36,240 --> 00:10:39,360
is a blocking call it will block until

00:10:38,800 --> 00:10:42,399
there is an

00:10:39,360 --> 00:10:44,880
element in the queue once the worker

00:10:42,399 --> 00:10:45,519
has the right key to process from the

00:10:44,880 --> 00:10:47,839
cube

00:10:45,519 --> 00:10:48,720
it will just pass that key to the sync

00:10:47,839 --> 00:10:50,880
function

00:10:48,720 --> 00:10:54,000
the sync function is where the business

00:10:50,880 --> 00:10:57,279
logic of the controller is

00:10:54,000 --> 00:10:59,519
once the sync function is done this call

00:10:57,279 --> 00:11:00,959
process to process next work item

00:10:59,519 --> 00:11:04,079
returns and then

00:11:00,959 --> 00:11:04,800
um the loop again blocks on the same

00:11:04,079 --> 00:11:08,399
call

00:11:04,800 --> 00:11:08,399
in the queues cat function

00:11:08,640 --> 00:11:12,160
looking at the slides more closely one

00:11:11,200 --> 00:11:15,279
may wonder

00:11:12,160 --> 00:11:17,040
what exactly is the req after variable

00:11:15,279 --> 00:11:17,920
that is being returned by the same

00:11:17,040 --> 00:11:20,399
function

00:11:17,920 --> 00:11:21,920
this is actually something unique about

00:11:20,399 --> 00:11:25,600
the new cron job controller

00:11:21,920 --> 00:11:28,160
and is a nice segue into

00:11:25,600 --> 00:11:30,480
um looking at the scheduling aspect of

00:11:28,160 --> 00:11:33,920
this new control

00:11:30,480 --> 00:11:35,519
along with the updates of the api server

00:11:33,920 --> 00:11:37,920
in order to implement this new

00:11:35,519 --> 00:11:39,839
controller we also need to handle the

00:11:37,920 --> 00:11:43,120
scheduling aspect

00:11:39,839 --> 00:11:46,000
for example

00:11:43,120 --> 00:11:46,480
my back um in this architecture that i

00:11:46,000 --> 00:11:49,440
just

00:11:46,480 --> 00:11:50,800
described couples of slides ago the only

00:11:49,440 --> 00:11:53,600
way

00:11:50,800 --> 00:11:54,399
a worker performing the sync operation

00:11:53,600 --> 00:11:57,600
can be

00:11:54,399 --> 00:12:00,880
invoked is if there is an element

00:11:57,600 --> 00:12:04,000
in the queue what if there is no add

00:12:00,880 --> 00:12:07,040
update or delete event around around the

00:12:04,000 --> 00:12:08,079
next scheduled time of a cronjob what

00:12:07,040 --> 00:12:10,560
will push

00:12:08,079 --> 00:12:12,560
the cron job element onto the queue

00:12:10,560 --> 00:12:14,560
around the scheduled time

00:12:12,560 --> 00:12:17,200
uh there is no guarantee in this

00:12:14,560 --> 00:12:19,680
architecture so we have to handle this

00:12:17,200 --> 00:12:21,760
case of scheduling

00:12:19,680 --> 00:12:23,200
then as you can see in the picture this

00:12:21,760 --> 00:12:25,600
is actually handled

00:12:23,200 --> 00:12:27,440
by by the workers themselves in the new

00:12:25,600 --> 00:12:31,120
controller's implementation

00:12:27,440 --> 00:12:32,240
um anytime the worker is done processing

00:12:31,120 --> 00:12:35,680
a cron job

00:12:32,240 --> 00:12:38,800
it will return the req

00:12:35,680 --> 00:12:39,200
after the time interval after which the

00:12:38,800 --> 00:12:42,079
next

00:12:39,200 --> 00:12:43,360
schedule for this cron job is supposed

00:12:42,079 --> 00:12:46,480
to be triggered

00:12:43,360 --> 00:12:50,160
um it uses something called

00:12:46,480 --> 00:12:52,160
a delaying interface the cue

00:12:50,160 --> 00:12:54,639
we use for this new controller

00:12:52,160 --> 00:12:58,000
implements this delaying interface

00:12:54,639 --> 00:13:00,720
this allows us for inserting elements

00:12:58,000 --> 00:13:02,480
into a queue into the queue after

00:13:00,720 --> 00:13:04,880
specified time interval

00:13:02,480 --> 00:13:06,000
so the time interval returned by the

00:13:04,880 --> 00:13:08,720
sync job

00:13:06,000 --> 00:13:09,839
sync cron job function will then be used

00:13:08,720 --> 00:13:13,040
to requeue

00:13:09,839 --> 00:13:13,760
the cron job object appropriately at the

00:13:13,040 --> 00:13:17,040
right time

00:13:13,760 --> 00:13:19,760
on the queue this is how we handle

00:13:17,040 --> 00:13:23,680
scheduling for the cron job

00:13:19,760 --> 00:13:26,480
now once the the enqueuing

00:13:23,680 --> 00:13:26,959
of elements onto the queue is taken care

00:13:26,480 --> 00:13:29,839
of

00:13:26,959 --> 00:13:30,240
the next part that comes into um into

00:13:29,839 --> 00:13:32,959
this

00:13:30,240 --> 00:13:34,000
new controller is how it implements the

00:13:32,959 --> 00:13:36,000
sync function

00:13:34,000 --> 00:13:37,519
as i said this is where the business

00:13:36,000 --> 00:13:41,519
logic of the controller

00:13:37,519 --> 00:13:44,800
is the very first thing it checks

00:13:41,519 --> 00:13:47,680
is if the cron job is suspended if

00:13:44,800 --> 00:13:48,000
it is indeed suspended this is a new op

00:13:47,680 --> 00:13:50,959
for

00:13:48,000 --> 00:13:52,720
the sync function and it simply returns

00:13:50,959 --> 00:13:55,360
if the cron job is not

00:13:52,720 --> 00:13:56,079
suspended it then looks at the most

00:13:55,360 --> 00:13:59,279
recent

00:13:56,079 --> 00:14:01,440
scheduled um time after the last

00:13:59,279 --> 00:14:04,320
scheduled time

00:14:01,440 --> 00:14:05,199
it then says if the current time is

00:14:04,320 --> 00:14:08,000
already

00:14:05,199 --> 00:14:10,000
missing um the deadline for this most

00:14:08,000 --> 00:14:13,120
recent schedule time

00:14:10,000 --> 00:14:14,320
if if it is missing if it has already

00:14:13,120 --> 00:14:18,000
missed the deadline

00:14:14,320 --> 00:14:21,600
the controller cannot do anything

00:14:18,000 --> 00:14:22,320
it simply calculates the time difference

00:14:21,600 --> 00:14:25,440
between

00:14:22,320 --> 00:14:28,560
next schedule time and the current time

00:14:25,440 --> 00:14:29,519
and returns this sdq after so that the

00:14:28,560 --> 00:14:32,560
cron job can be

00:14:29,519 --> 00:14:33,839
enqueued properly on the next scheduled

00:14:32,560 --> 00:14:37,680
time

00:14:33,839 --> 00:14:41,440
if it has not missed the deadline

00:14:37,680 --> 00:14:44,880
it will then go on to check the

00:14:41,440 --> 00:14:46,160
policy for this cron job the current

00:14:44,880 --> 00:14:49,600
policies can be

00:14:46,160 --> 00:14:50,399
allowed replace or forward depending on

00:14:49,600 --> 00:14:53,120
the policy

00:14:50,399 --> 00:14:53,600
it will take different actions these are

00:14:53,120 --> 00:14:56,959
simple

00:14:53,600 --> 00:14:59,839
switch case or if else conditions in the

00:14:56,959 --> 00:15:01,279
controller you can go check at the sync

00:14:59,839 --> 00:15:04,720
current job function

00:15:01,279 --> 00:15:07,680
to know the exact specifics of how

00:15:04,720 --> 00:15:07,680
it is implemented

00:15:09,120 --> 00:15:16,160
note one note is that

00:15:12,320 --> 00:15:19,120
upon suspension the same function does

00:15:16,160 --> 00:15:20,959
not bother to calculate the

00:15:19,120 --> 00:15:23,279
time difference and enqueue

00:15:20,959 --> 00:15:26,320
appropriately based on time

00:15:23,279 --> 00:15:28,360
the assumption here is when

00:15:26,320 --> 00:15:29,759
when the crown job is suspended

00:15:28,360 --> 00:15:31,759
unsuspending it

00:15:29,759 --> 00:15:33,199
means that we would have to update the

00:15:31,759 --> 00:15:36,000
crown job and the

00:15:33,199 --> 00:15:37,839
event will come in via api server

00:15:36,000 --> 00:15:38,560
through the informers and the resource

00:15:37,839 --> 00:15:40,880
handler

00:15:38,560 --> 00:15:43,519
resource event handlers we don't need to

00:15:40,880 --> 00:15:45,920
handle that case inside of the workers

00:15:43,519 --> 00:15:45,920
itself

00:15:46,240 --> 00:15:50,320
also note that the sync functions the

00:15:49,440 --> 00:15:53,120
sync function

00:15:50,320 --> 00:15:53,839
that i just described always performs

00:15:53,120 --> 00:15:57,199
the same

00:15:53,839 --> 00:15:58,639
um for the schedules that are older than

00:15:57,199 --> 00:16:01,839
the current time

00:15:58,639 --> 00:16:04,800
uh it re the it

00:16:01,839 --> 00:16:06,000
always tries to reuse req for the

00:16:04,800 --> 00:16:08,079
schedules that are

00:16:06,000 --> 00:16:09,199
newer than the current tank this

00:16:08,079 --> 00:16:13,920
distinction is

00:16:09,199 --> 00:16:13,920
important to understand the workflow

00:16:15,279 --> 00:16:20,399
now that the basic workflow of this new

00:16:18,560 --> 00:16:22,639
controller is established

00:16:20,399 --> 00:16:23,519
let's look at some of the corner cases

00:16:22,639 --> 00:16:26,560
that come

00:16:23,519 --> 00:16:30,079
in here one of the challenges

00:16:26,560 --> 00:16:32,800
is um how to handle updates

00:16:30,079 --> 00:16:34,079
um it's one of the harder things to work

00:16:32,800 --> 00:16:37,279
through and wrap your

00:16:34,079 --> 00:16:40,320
brain around first but once it is

00:16:37,279 --> 00:16:42,800
done um it is actually quite simple to

00:16:40,320 --> 00:16:42,800
understand

00:16:43,680 --> 00:16:51,839
the interesting part in the updates if

00:16:47,279 --> 00:16:51,839
what if the schedule is updated

00:16:52,000 --> 00:16:58,320
we already have the key for this

00:16:55,279 --> 00:17:00,079
updated cron job in the queue that is

00:16:58,320 --> 00:17:03,440
reflecting the order

00:17:00,079 --> 00:17:05,280
schedule what happens to that key um

00:17:03,440 --> 00:17:07,280
these are the kinds of questions that

00:17:05,280 --> 00:17:10,640
pop up um

00:17:07,280 --> 00:17:13,679
so anytime there is an

00:17:10,640 --> 00:17:16,079
update or change in the schedule we use

00:17:13,679 --> 00:17:16,880
the update cron job resource event

00:17:16,079 --> 00:17:20,000
handlers

00:17:16,880 --> 00:17:23,199
handler um to enqueue for the

00:17:20,000 --> 00:17:26,959
next scheduled time of the newer

00:17:23,199 --> 00:17:30,400
schedule uh this could mean

00:17:26,959 --> 00:17:33,840
two things uh one that the next

00:17:30,400 --> 00:17:37,679
scheduled time of the new schedule is

00:17:33,840 --> 00:17:41,440
um is earlier

00:17:37,679 --> 00:17:42,160
than the then the last process scheduled

00:17:41,440 --> 00:17:45,120
time

00:17:42,160 --> 00:17:45,520
or the other possibility is that the

00:17:45,120 --> 00:17:48,240
next

00:17:45,520 --> 00:17:48,799
scheduled time is pushed further back

00:17:48,240 --> 00:17:51,440
and is

00:17:48,799 --> 00:17:52,880
later than the last process scheduled

00:17:51,440 --> 00:17:55,440
time

00:17:52,880 --> 00:17:56,480
these are reflected as two possibilities

00:17:55,440 --> 00:18:00,160
in the picture

00:17:56,480 --> 00:18:03,840
the t1 minus delta t and the t1 plus

00:18:00,160 --> 00:18:06,240
delta t of course t1 being the last

00:18:03,840 --> 00:18:07,760
process scheduled time of the order

00:18:06,240 --> 00:18:11,520
schedule

00:18:07,760 --> 00:18:14,720
so these two possibilities um

00:18:11,520 --> 00:18:18,240
can occur in the for the

00:18:14,720 --> 00:18:20,720
the second possibility where

00:18:18,240 --> 00:18:24,160
the new schedule is pushed further back

00:18:20,720 --> 00:18:28,160
in time is actually easier to handle

00:18:24,160 --> 00:18:30,320
the workout will still be fired at t1

00:18:28,160 --> 00:18:32,880
it will determine that it has either

00:18:30,320 --> 00:18:35,280
missed the schedule or already has a job

00:18:32,880 --> 00:18:38,320
for the last schedule and do a new op

00:18:35,280 --> 00:18:42,000
it will then calculate the delta t

00:18:38,320 --> 00:18:44,400
and uh push the key back um

00:18:42,000 --> 00:18:45,120
into the schedule accordingly in this

00:18:44,400 --> 00:18:47,600
way we

00:18:45,120 --> 00:18:48,240
we make sure that we have t1 plus delta

00:18:47,600 --> 00:18:52,640
t

00:18:48,240 --> 00:18:55,600
um at t1 plus delta t we have a key

00:18:52,640 --> 00:18:57,679
on the on the queue to reflect this

00:18:55,600 --> 00:19:00,880
update

00:18:57,679 --> 00:19:03,600
if the next schedule is earlier than

00:19:00,880 --> 00:19:05,360
what the controller has processed the

00:19:03,600 --> 00:19:08,400
update cron job

00:19:05,360 --> 00:19:10,160
resource event handler will see this

00:19:08,400 --> 00:19:13,200
change and push the key

00:19:10,160 --> 00:19:14,320
for an earlier time this just means that

00:19:13,200 --> 00:19:17,440
the earlier

00:19:14,320 --> 00:19:20,640
uh earlier time key

00:19:17,440 --> 00:19:22,280
will will be fired first and then we

00:19:20,640 --> 00:19:26,000
have we will have

00:19:22,280 --> 00:19:26,559
a worker that is fired at t1 minus delta

00:19:26,000 --> 00:19:29,760
t

00:19:26,559 --> 00:19:33,760
so these are the two ways in which

00:19:29,760 --> 00:19:37,679
um we handle the update of schedule

00:19:33,760 --> 00:19:38,080
um it was one of the interesting things

00:19:37,679 --> 00:19:40,720
to

00:19:38,080 --> 00:19:42,960
implement when reworking on this

00:19:40,720 --> 00:19:46,480
architecture

00:19:42,960 --> 00:19:48,640
let's dig a little bit at on another set

00:19:46,480 --> 00:19:52,000
of challenges that were introduced

00:19:48,640 --> 00:19:55,039
due to cash one of the

00:19:52,000 --> 00:19:57,039
ways that we were we are able to

00:19:55,039 --> 00:20:00,799
increase the performance of the system

00:19:57,039 --> 00:20:03,200
is to um leverage the cash

00:20:00,799 --> 00:20:04,240
the classic problem of stale caches can

00:20:03,200 --> 00:20:07,200
manifest here

00:20:04,240 --> 00:20:08,240
as well the cron job informer and the

00:20:07,200 --> 00:20:10,559
job informer

00:20:08,240 --> 00:20:12,080
are the two caches that the new cron job

00:20:10,559 --> 00:20:14,960
controller uses

00:20:12,080 --> 00:20:15,679
and it can be easy to imagine that if

00:20:14,960 --> 00:20:18,880
one

00:20:15,679 --> 00:20:22,159
lags behind the other we could have

00:20:18,880 --> 00:20:24,960
uh it could cause some problems

00:20:22,159 --> 00:20:26,320
for example the crown job controller

00:20:24,960 --> 00:20:29,440
creates the job

00:20:26,320 --> 00:20:31,919
for this schedule and puts it

00:20:29,440 --> 00:20:32,799
in the crown job status in the active

00:20:31,919 --> 00:20:36,159
list

00:20:32,799 --> 00:20:39,760
this creates multiple updates um

00:20:36,159 --> 00:20:41,520
one update at the job informer and one

00:20:39,760 --> 00:20:44,640
at the ground dropping from

00:20:41,520 --> 00:20:47,280
as you can see in the um

00:20:44,640 --> 00:20:48,880
in the picture this updates directly go

00:20:47,280 --> 00:20:51,039
to the api server

00:20:48,880 --> 00:20:52,000
and then the informers would be able to

00:20:51,039 --> 00:20:55,120
see this

00:20:52,000 --> 00:20:57,360
depending on the exact scenario

00:20:55,120 --> 00:20:59,200
it could be possible that one informer

00:20:57,360 --> 00:21:01,520
sees it before

00:20:59,200 --> 00:21:03,840
and the other informer is still a little

00:21:01,520 --> 00:21:06,960
slow and does not see this update

00:21:03,840 --> 00:21:09,760
so the specific example illustrated here

00:21:06,960 --> 00:21:11,919
is that let's say cron job informer sees

00:21:09,760 --> 00:21:14,880
that update first

00:21:11,919 --> 00:21:17,120
its status will reflect an active job

00:21:14,880 --> 00:21:20,159
but the job informer is still slow

00:21:17,120 --> 00:21:20,640
it did not see the update yet so when

00:21:20,159 --> 00:21:23,840
the

00:21:20,640 --> 00:21:26,880
controller tries to go get use the

00:21:23,840 --> 00:21:27,520
lister and get the job object from the

00:21:26,880 --> 00:21:30,480
cache

00:21:27,520 --> 00:21:32,799
it will it will get a 404 not found

00:21:30,480 --> 00:21:32,799
error

00:21:32,960 --> 00:21:36,640
this could if this case is not handled

00:21:35,679 --> 00:21:38,559
appropriately

00:21:36,640 --> 00:21:40,799
this could mean that the controller

00:21:38,559 --> 00:21:43,600
would take improper decisions

00:21:40,799 --> 00:21:44,159
because of the lacking cash problem for

00:21:43,600 --> 00:21:47,039
example

00:21:44,159 --> 00:21:47,520
it could create a duplicate job for the

00:21:47,039 --> 00:21:51,440
same

00:21:47,520 --> 00:21:54,880
schedule the way we handle this

00:21:51,440 --> 00:21:56,080
um in the in the current implementation

00:21:54,880 --> 00:21:59,280
of the new controller

00:21:56,080 --> 00:22:01,840
is anytime there is a potential

00:21:59,280 --> 00:22:03,039
problem identified due to lagging caches

00:22:01,840 --> 00:22:06,640
and it could cause

00:22:03,039 --> 00:22:09,679
um it could cause behavioral

00:22:06,640 --> 00:22:10,960
problems we actually go to the api

00:22:09,679 --> 00:22:13,520
server and

00:22:10,960 --> 00:22:14,559
perform a live get call against the api

00:22:13,520 --> 00:22:18,320
server to get a

00:22:14,559 --> 00:22:21,440
fresh copy in the example earlier

00:22:18,320 --> 00:22:22,240
if the if the job is not found in in the

00:22:21,440 --> 00:22:24,400
informer

00:22:22,240 --> 00:22:26,000
we would make a live call against the

00:22:24,400 --> 00:22:28,400
api server to see

00:22:26,000 --> 00:22:29,120
if the job is actually missing or we are

00:22:28,400 --> 00:22:32,400
hitting this

00:22:29,120 --> 00:22:34,080
stale cache problem that way if if it is

00:22:32,400 --> 00:22:37,200
found in the api server

00:22:34,080 --> 00:22:42,320
we would process it accordingly and

00:22:37,200 --> 00:22:45,039
not cause it to create a duplicate

00:22:42,320 --> 00:22:45,600
there are two instances where this could

00:22:45,039 --> 00:22:49,760
happen

00:22:45,600 --> 00:22:49,760
and the current controller handles both

00:22:51,520 --> 00:22:55,440
the lagging caches are not the only

00:22:54,320 --> 00:22:59,039
problem that cash

00:22:55,440 --> 00:23:02,640
introduce the other problem is

00:22:59,039 --> 00:23:04,720
is actually quite easy to encounter um

00:23:02,640 --> 00:23:05,919
as i mentioned earlier the controller

00:23:04,720 --> 00:23:08,880
uses two

00:23:05,919 --> 00:23:11,039
informers which are caching objects now

00:23:08,880 --> 00:23:13,360
these informers can be shared

00:23:11,039 --> 00:23:14,880
across all the controllers um in the

00:23:13,360 --> 00:23:17,520
controller manager

00:23:14,880 --> 00:23:18,000
hence they are hence they are aptly

00:23:17,520 --> 00:23:21,440
named

00:23:18,000 --> 00:23:22,159
shared informers right so a caveat to

00:23:21,440 --> 00:23:25,520
this is

00:23:22,159 --> 00:23:28,799
the objects pulled from this cache

00:23:25,520 --> 00:23:30,720
should never be mutated in place it is

00:23:28,799 --> 00:23:32,240
it is actually one of the as i said it

00:23:30,720 --> 00:23:34,960
is actually one of the easiest

00:23:32,240 --> 00:23:37,679
mistakes to make while writing on a

00:23:34,960 --> 00:23:40,799
controller

00:23:37,679 --> 00:23:42,559
this has made it um into the list of

00:23:40,799 --> 00:23:45,279
community guidelines for

00:23:42,559 --> 00:23:47,200
writing the controller there is a link

00:23:45,279 --> 00:23:51,840
to that page in the slides

00:23:47,200 --> 00:23:51,840
you can go um read it in that

00:23:51,919 --> 00:23:55,919
the solution to this problem is actually

00:23:54,480 --> 00:24:00,400
very easy

00:23:55,919 --> 00:24:01,520
anytime we want to mutate the object

00:24:00,400 --> 00:24:04,720
from the cache

00:24:01,520 --> 00:24:07,440
just create a copy of the object

00:24:04,720 --> 00:24:09,279
when mutating that way that way other

00:24:07,440 --> 00:24:11,279
controllers will not see

00:24:09,279 --> 00:24:19,120
what you have mutated unless you

00:24:11,279 --> 00:24:22,320
explicitly submit it to the api server

00:24:19,120 --> 00:24:26,080
so what do we get after um

00:24:22,320 --> 00:24:29,279
all of this uh one should ask after the

00:24:26,080 --> 00:24:32,640
entire um re-architect and over

00:24:29,279 --> 00:24:35,520
um overcoming all the challenges um

00:24:32,640 --> 00:24:36,320
what are the advantages um we should

00:24:35,520 --> 00:24:39,279
expect

00:24:36,320 --> 00:24:39,679
so theoretically speaking we should get

00:24:39,279 --> 00:24:43,120
less

00:24:39,679 --> 00:24:46,000
calls to the api server this should make

00:24:43,120 --> 00:24:46,640
the cron job controller more performant

00:24:46,000 --> 00:24:50,080
because

00:24:46,640 --> 00:24:51,919
less time is being spent on waiting for

00:24:50,080 --> 00:24:54,640
those network calls

00:24:51,919 --> 00:24:55,760
this should also put less stress on the

00:24:54,640 --> 00:25:01,120
api server

00:24:55,760 --> 00:25:04,240
so the overall system is more performant

00:25:01,120 --> 00:25:07,679
it is also really easy to scale up the

00:25:04,240 --> 00:25:11,120
number of workers so assume

00:25:07,679 --> 00:25:14,000
the case where user knows that they have

00:25:11,120 --> 00:25:16,400
a lot of cron jobs in their cluster and

00:25:14,000 --> 00:25:18,240
the default of one worker is not

00:25:16,400 --> 00:25:20,480
sufficient for them

00:25:18,240 --> 00:25:21,520
the user can safely increase the number

00:25:20,480 --> 00:25:24,799
of workers

00:25:21,520 --> 00:25:29,039
and they could achieve this scaling

00:25:24,799 --> 00:25:32,240
up of the controller

00:25:29,039 --> 00:25:34,799
so how do we go about

00:25:32,240 --> 00:25:35,840
um testing this performance um

00:25:34,799 --> 00:25:38,080
improvements

00:25:35,840 --> 00:25:39,200
um in order to see the improvements we

00:25:38,080 --> 00:25:42,480
actually ran

00:25:39,200 --> 00:25:46,799
um some stress tests on it um

00:25:42,480 --> 00:25:50,799
we used a vm with 128 gigabyte of ram

00:25:46,799 --> 00:25:51,520
and 64 virtual cpus a really beefy

00:25:50,799 --> 00:25:54,320
machine

00:25:51,520 --> 00:25:55,600
um to create a single node cube cluster

00:25:54,320 --> 00:25:57,760
we wanted to be

00:25:55,600 --> 00:25:59,120
intelligent about what kinds of crown

00:25:57,760 --> 00:26:02,400
job we create

00:25:59,120 --> 00:26:06,960
so that we don't overload the um entire

00:26:02,400 --> 00:26:08,400
system so 20 cron jobs were created that

00:26:06,960 --> 00:26:11,440
were to be scheduled

00:26:08,400 --> 00:26:14,720
on every minute additionally

00:26:11,440 --> 00:26:15,440
to stress test the controller under

00:26:14,720 --> 00:26:19,200
duress

00:26:15,440 --> 00:26:22,320
we would create another 2

00:26:19,200 --> 00:26:25,760
100 cron jobs with the schedule of

00:26:22,320 --> 00:26:27,919
running at every 20 hours this would

00:26:25,760 --> 00:26:28,720
mean that the controller will still have

00:26:27,919 --> 00:26:31,120
to process

00:26:28,720 --> 00:26:34,640
all the cron jobs but it does not have

00:26:31,120 --> 00:26:37,279
to create the jobs or the parts for this

00:26:34,640 --> 00:26:38,159
this will limit the number of pods

00:26:37,279 --> 00:26:42,000
concurrently

00:26:38,159 --> 00:26:45,039
running in the system and potentially

00:26:42,000 --> 00:26:48,960
stop overloading other parts of the

00:26:45,039 --> 00:26:51,520
system like the cubelet or eps server

00:26:48,960 --> 00:26:53,600
we would additionally create batches of

00:26:51,520 --> 00:26:59,760
thousand cron jobs until the

00:26:53,600 --> 00:27:02,799
the total reaches to 5120 crown jobs

00:26:59,760 --> 00:27:05,520
with this sample workload we will change

00:27:02,799 --> 00:27:06,480
the controllers with the feature flag

00:27:05,520 --> 00:27:09,600
and compare

00:27:06,480 --> 00:27:12,320
both the old controllers performance and

00:27:09,600 --> 00:27:12,320
the new ones

00:27:13,279 --> 00:27:16,799
the old con the old controller would

00:27:15,600 --> 00:27:19,760
really start

00:27:16,799 --> 00:27:22,240
to show some performance problems when

00:27:19,760 --> 00:27:25,039
adding cron jobs in thousands

00:27:22,240 --> 00:27:27,440
for every thousand additional cron job

00:27:25,039 --> 00:27:29,679
added in the old controller

00:27:27,440 --> 00:27:31,919
we would see and it requires an

00:27:29,679 --> 00:27:34,640
additional two minute of delay

00:27:31,919 --> 00:27:37,200
in scheduling that those 20 jobs it's

00:27:34,640 --> 00:27:41,440
supposed to schedule every minute

00:27:37,200 --> 00:27:44,640
that is for 2120

00:27:41,440 --> 00:27:47,440
cron jobs instead of creating the

00:27:44,640 --> 00:27:48,720
the 20 jobs every minute it would take

00:27:47,440 --> 00:27:52,799
the controller

00:27:48,720 --> 00:27:56,000
about three minutes to create those jobs

00:27:52,799 --> 00:27:58,360
this is this is quite drastic when the

00:27:56,000 --> 00:28:01,200
workload was increased to

00:27:58,360 --> 00:28:04,960
5120 crore jobs about

00:28:01,200 --> 00:28:08,000
eight schedules were missed um

00:28:04,960 --> 00:28:11,440
before the next 20 jobs are created

00:28:08,000 --> 00:28:12,240
so it would take about nine minutes for

00:28:11,440 --> 00:28:15,840
the

00:28:12,240 --> 00:28:20,399
old crown job controller to um

00:28:15,840 --> 00:28:23,760
resync of the 20 crown jobs

00:28:20,399 --> 00:28:25,120
this linear increase be in time between

00:28:23,760 --> 00:28:28,559
processing consecutive

00:28:25,120 --> 00:28:31,440
job creation um when

00:28:28,559 --> 00:28:33,919
adding more and more cron jobs actually

00:28:31,440 --> 00:28:37,440
shows the scaling problems of the

00:28:33,919 --> 00:28:38,559
old controller comparing this with the

00:28:37,440 --> 00:28:41,840
new controller

00:28:38,559 --> 00:28:42,720
it did not really see any lag in job

00:28:41,840 --> 00:28:45,360
creation

00:28:42,720 --> 00:28:46,640
we actually went ahead and topped it up

00:28:45,360 --> 00:28:49,360
with additional

00:28:46,640 --> 00:28:51,760
thousand crown jobs making the total to

00:28:49,360 --> 00:28:54,880
600 6120

00:28:51,760 --> 00:28:59,039
cron jobs um and it still did not see

00:28:54,880 --> 00:29:02,720
any visible um delay in scheduling

00:28:59,039 --> 00:29:03,360
those 20 grunt jumps now obviously this

00:29:02,720 --> 00:29:06,799
is not

00:29:03,360 --> 00:29:10,240
a um a great real life model

00:29:06,799 --> 00:29:10,880
but it actually um the sample workload

00:29:10,240 --> 00:29:14,960
actually

00:29:10,880 --> 00:29:17,760
shows um how the new architecture

00:29:14,960 --> 00:29:19,840
is more performant under duress and

00:29:17,760 --> 00:29:22,559
solves some of the scaling problems of

00:29:19,840 --> 00:29:22,559
the old one

00:29:22,640 --> 00:29:26,799
along with the architectural change

00:29:24,640 --> 00:29:29,279
there are few smaller

00:29:26,799 --> 00:29:32,000
new additions going into the controller

00:29:29,279 --> 00:29:35,440
we now have a new histogram metrics

00:29:32,000 --> 00:29:39,200
um that users can go up look at

00:29:35,440 --> 00:29:42,320
this metric this matrix shows the skew

00:29:39,200 --> 00:29:44,559
between the job between when the job is

00:29:42,320 --> 00:29:48,159
supposed to be scheduled and when it is

00:29:44,559 --> 00:29:49,919
actually created we have more metrics

00:29:48,159 --> 00:29:52,880
planned for this controller in the

00:29:49,919 --> 00:29:56,240
future there are also minor

00:29:52,880 --> 00:29:56,799
optimizations one notable example is the

00:29:56,240 --> 00:30:00,080
compute

00:29:56,799 --> 00:30:03,279
next schedule it was observed that um

00:30:00,080 --> 00:30:04,080
the old controller will would um use a

00:30:03,279 --> 00:30:07,039
library

00:30:04,080 --> 00:30:07,600
function to collect all the scheduled

00:30:07,039 --> 00:30:09,919
time

00:30:07,600 --> 00:30:11,200
between the last schedule and the most

00:30:09,919 --> 00:30:14,399
recent schedule

00:30:11,200 --> 00:30:18,640
and store it in list

00:30:14,399 --> 00:30:20,159
um given that we only need the most

00:30:18,640 --> 00:30:23,679
decent schedule

00:30:20,159 --> 00:30:25,760
um and the granularity that the crown

00:30:23,679 --> 00:30:29,360
job supports is one minute

00:30:25,760 --> 00:30:32,640
it is actually very easy to replace this

00:30:29,360 --> 00:30:35,520
for loop and use uh

00:30:32,640 --> 00:30:36,640
math to calculate the most recent on

00:30:35,520 --> 00:30:39,279
schedule

00:30:36,640 --> 00:30:40,559
this optimization actually helped us in

00:30:39,279 --> 00:30:44,159
saving some

00:30:40,559 --> 00:30:46,320
memory requirements with this new

00:30:44,159 --> 00:30:47,520
controller we were able to drive the

00:30:46,320 --> 00:30:50,679
cron job api

00:30:47,520 --> 00:30:53,600
to general availability with kubernetes

00:30:50,679 --> 00:30:56,240
1.21

00:30:53,600 --> 00:30:56,960
lastly i would like to thank the seagaps

00:30:56,240 --> 00:31:00,080
community

00:30:56,960 --> 00:31:02,799
for allowing me the opportunity to work

00:31:00,080 --> 00:31:04,960
on this controller it was uh it was an

00:31:02,799 --> 00:31:07,760
amazing ride

00:31:04,960 --> 00:31:10,080
hopefully this also makes the user

00:31:07,760 --> 00:31:12,640
experience of cron jobs better

00:31:10,080 --> 00:31:14,240
um if you guys have any questions we can

00:31:12,640 --> 00:31:16,399
take them now

00:31:14,240 --> 00:31:17,279
also feel free to reach out to us

00:31:16,399 --> 00:31:19,840
offline

00:31:17,279 --> 00:31:19,840

YouTube URL: https://www.youtube.com/watch?v=o5h6s3A9bXY


