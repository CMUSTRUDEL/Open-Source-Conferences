Title: Scaling Apache Spark on Kube to Apple Scale - Amanda Moran & Holden Karau, Apple
Publication date: 2021-05-09
Playlist: KubeCon + CloudNativeCon Europe 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Scaling Apache Spark on Kube to Apple Scale - Amanda Moran & Holden Karau, Apple

Amanda and Holden will explore the customer workloads that easily ported to Apache Spark on Kubernetes, and which ones had more difficulty. The goal of this talk is to help the audience in their journey as either the operators of an Apache Spark-Kubernetes platform or as an end user. Considerations and best practices for end users of an Apache Spark on Kubernetes platform will be discussed. Additional advice for folks migrating from YARN with HDFS to Kubernetes will be included. This talk will include how to effectively deploy the new enhancements of Spark on Kube, like shuffle tracking and graceful decommissioning, as well as when not to use this.
Captions: 
	00:00:00,000 --> 00:00:03,199
hi there all welcome and thank you so

00:00:01,760 --> 00:00:05,200
much for joining us we're going to be

00:00:03,199 --> 00:00:06,480
talking about scaling apache spark on

00:00:05,200 --> 00:00:08,160
kubernetes

00:00:06,480 --> 00:00:10,080
we're amanda and holden and we're so

00:00:08,160 --> 00:00:13,519
delighted to be here at kubecon

00:00:10,080 --> 00:00:15,440
eu so first and foremost we're going to

00:00:13,519 --> 00:00:16,640
tell you a little bit about ourselves

00:00:15,440 --> 00:00:18,720
then we're going to talk about what is

00:00:16,640 --> 00:00:20,080
apache spark and why it matters to

00:00:18,720 --> 00:00:23,199
kubernetes

00:00:20,080 --> 00:00:25,039
what is yarn and mesos spark

00:00:23,199 --> 00:00:27,119
standalone mode so get ready for that

00:00:25,039 --> 00:00:28,400
one why you should put spark on

00:00:27,119 --> 00:00:29,760
kubernetes

00:00:28,400 --> 00:00:31,599
what worked well while we worked through

00:00:29,760 --> 00:00:32,880
this process and what didn't work as

00:00:31,599 --> 00:00:34,559
good

00:00:32,880 --> 00:00:36,640
best practices for doing this kind of

00:00:34,559 --> 00:00:38,640
transformation

00:00:36,640 --> 00:00:39,840
new features that have been implemented

00:00:38,640 --> 00:00:44,800
and upstreamed

00:00:39,840 --> 00:00:48,480
and areas for improvement

00:00:44,800 --> 00:00:51,520
so who are we so you'll see two

00:00:48,480 --> 00:00:53,120
two of us uh in this picture and this

00:00:51,520 --> 00:00:55,039
collection of pictures work for apple

00:00:53,120 --> 00:00:57,520
and that is me and holden

00:00:55,039 --> 00:00:58,399
and you also see our two pups timber and

00:00:57,520 --> 00:00:59,920
jack

00:00:58,399 --> 00:01:02,399
they support us throughout the day as we

00:00:59,920 --> 00:01:05,040
get our work done both of us have been

00:01:02,399 --> 00:01:06,159
and have worked for apple uh for over a

00:01:05,040 --> 00:01:09,280
year i think holden is

00:01:06,159 --> 00:01:12,799
it's getting close to two years and

00:01:09,280 --> 00:01:14,560
um both of us have been um

00:01:12,799 --> 00:01:16,320
a part of the apache spark community

00:01:14,560 --> 00:01:18,159
holden much more so she's an apache

00:01:16,320 --> 00:01:18,880
committer she's written books on apache

00:01:18,159 --> 00:01:20,400
spark

00:01:18,880 --> 00:01:22,320
she's been with it pretty much from the

00:01:20,400 --> 00:01:23,680
get-go i came to spark a little bit

00:01:22,320 --> 00:01:25,200
later but i've been involved with it for

00:01:23,680 --> 00:01:26,560
quite a few years i've done quite a few

00:01:25,200 --> 00:01:29,520
talks on spark and

00:01:26,560 --> 00:01:30,240
and it's a it's a technology that um i

00:01:29,520 --> 00:01:31,840
enjoy

00:01:30,240 --> 00:01:34,560
i like teaching other people about and

00:01:31,840 --> 00:01:34,560
using myself

00:01:35,040 --> 00:01:39,600
so all right so let's talk about spark

00:01:37,360 --> 00:01:43,439
and what are these things

00:01:39,600 --> 00:01:45,360
so what is spark so a lightning fast

00:01:43,439 --> 00:01:46,640
unified analytics engine is one of the

00:01:45,360 --> 00:01:48,880
taglines you'll hear

00:01:46,640 --> 00:01:50,560
quite frequently but apache spark is

00:01:48,880 --> 00:01:52,720
really used to do large-scale data

00:01:50,560 --> 00:01:54,399
processing on large data sets

00:01:52,720 --> 00:01:56,560
it's used by data scientists data

00:01:54,399 --> 00:01:58,320
engineers machine learning engineers

00:01:56,560 --> 00:02:00,159
basically anyone who is working with

00:01:58,320 --> 00:02:03,520
large data sets

00:02:00,159 --> 00:02:05,520
basically regardless of title so apache

00:02:03,520 --> 00:02:07,920
spark allows for batch jobs

00:02:05,520 --> 00:02:08,879
streaming jobs the ability to use spark

00:02:07,920 --> 00:02:12,319
sql

00:02:08,879 --> 00:02:14,319
r python scala or java

00:02:12,319 --> 00:02:15,840
you can do machine learning using spark

00:02:14,319 --> 00:02:17,840
and you can also do graph analytics with

00:02:15,840 --> 00:02:20,080
spark as well

00:02:17,840 --> 00:02:22,000
so some other taglines you may have

00:02:20,080 --> 00:02:24,160
heard about spark which is it's map

00:02:22,000 --> 00:02:25,280
hadoop mapreduce uh but with seven cups

00:02:24,160 --> 00:02:27,599
of coffee

00:02:25,280 --> 00:02:28,959
um of course we have a little disclaimer

00:02:27,599 --> 00:02:29,920
there you know check with your hardware

00:02:28,959 --> 00:02:31,440
vendor first

00:02:29,920 --> 00:02:33,200
but i mean there has been i mean there's

00:02:31,440 --> 00:02:33,920
been so many numbers thrown around the

00:02:33,200 --> 00:02:36,879
years

00:02:33,920 --> 00:02:38,800
uh but essentially you know spark jobs

00:02:36,879 --> 00:02:42,000
because of their ability to utilize

00:02:38,800 --> 00:02:42,800
uh memory and large uh and do that large

00:02:42,000 --> 00:02:45,360
processing

00:02:42,800 --> 00:02:46,800
um anywhere between uh 10 to 100 times

00:02:45,360 --> 00:02:48,720
faster than hadoop

00:02:46,800 --> 00:02:50,560
regardless of that's a big range but

00:02:48,720 --> 00:02:52,720
it's faster

00:02:50,560 --> 00:02:54,239
it is a spark is a good way for folks to

00:02:52,720 --> 00:02:54,800
learn functional programming which is

00:02:54,239 --> 00:02:56,800
true

00:02:54,800 --> 00:02:58,159
since scala is a functional functional

00:02:56,800 --> 00:03:00,879
language me myself

00:02:58,159 --> 00:03:02,239
i know this much about scala and i

00:03:00,879 --> 00:03:04,640
prefer to use python

00:03:02,239 --> 00:03:06,560
that's my uh language of choice when

00:03:04,640 --> 00:03:07,760
using spark so i did not take this

00:03:06,560 --> 00:03:09,040
opportunity to learn functional

00:03:07,760 --> 00:03:11,120
programming

00:03:09,040 --> 00:03:12,560
and it's a great it's a great way to use

00:03:11,120 --> 00:03:13,840
a lot of compute resources

00:03:12,560 --> 00:03:15,760
right because you're doing extremely

00:03:13,840 --> 00:03:18,080
large jobs across

00:03:15,760 --> 00:03:18,959
super large clusters using a ton of

00:03:18,080 --> 00:03:23,040
memory

00:03:18,959 --> 00:03:24,720
and cpu so let's talk about how spark

00:03:23,040 --> 00:03:28,080
works

00:03:24,720 --> 00:03:28,560
so um basically what spark does the tl

00:03:28,080 --> 00:03:30,560
dr

00:03:28,560 --> 00:03:32,000
is it spreads out compute across a

00:03:30,560 --> 00:03:34,159
cluster most specifically

00:03:32,000 --> 00:03:36,720
a spark cluster right there's two main

00:03:34,159 --> 00:03:38,480
components when you launch a spark job

00:03:36,720 --> 00:03:40,959
the driver which contains the spark

00:03:38,480 --> 00:03:42,879
contacts and which works to transform

00:03:40,959 --> 00:03:43,599
the user's code that i've written maybe

00:03:42,879 --> 00:03:46,000
in python

00:03:43,599 --> 00:03:48,159
right splits it up into tasks these

00:03:46,000 --> 00:03:50,159
bite-sized chunks that can be sent to be

00:03:48,159 --> 00:03:51,440
performed by the executors and the

00:03:50,159 --> 00:03:54,560
executors reside

00:03:51,440 --> 00:03:56,640
on each node so from there it's very

00:03:54,560 --> 00:03:57,920
easy to scale up and scale down

00:03:56,640 --> 00:04:00,159
depending on the workload because you

00:03:57,920 --> 00:04:01,680
can always add more nodes and more

00:04:00,159 --> 00:04:03,200
executors especially when you're working

00:04:01,680 --> 00:04:03,760
in a cloud native environment in the

00:04:03,200 --> 00:04:05,360
cloud

00:04:03,760 --> 00:04:07,920
it's really easy to add those extra

00:04:05,360 --> 00:04:10,480
resources

00:04:07,920 --> 00:04:11,760
so apache spark has abstracted away from

00:04:10,480 --> 00:04:13,599
the users

00:04:11,760 --> 00:04:15,120
any need to have to deal with

00:04:13,599 --> 00:04:16,400
orchestrating data processing

00:04:15,120 --> 00:04:18,400
parallelism

00:04:16,400 --> 00:04:20,079
or worrying about fault tolerance this

00:04:18,400 --> 00:04:23,520
is all taking care of them

00:04:20,079 --> 00:04:26,880
for them um so because

00:04:23,520 --> 00:04:28,639
of the architecture and knowing that

00:04:26,880 --> 00:04:30,479
spark would be spread across multiple

00:04:28,639 --> 00:04:32,000
nodes on very large clusters

00:04:30,479 --> 00:04:33,680
it was known that nodes would fail

00:04:32,000 --> 00:04:35,040
throughout this process and so the

00:04:33,680 --> 00:04:36,800
executors

00:04:35,040 --> 00:04:38,320
are able to handle those failures

00:04:36,800 --> 00:04:41,360
because they know that they will happen

00:04:38,320 --> 00:04:42,000
and they can just recompute results so

00:04:41,360 --> 00:04:44,080
um

00:04:42,000 --> 00:04:45,919
the executors are long-lived especially

00:04:44,080 --> 00:04:48,320
when compared to mapreduce

00:04:45,919 --> 00:04:50,400
and the executors store a mix of stored

00:04:48,320 --> 00:04:54,800
data in a mixture of memory and disk

00:04:50,400 --> 00:04:54,800
so it utilizes uh both of those to run

00:04:54,840 --> 00:04:58,880
faster

00:04:57,040 --> 00:05:00,639
with all this said right oh so now we

00:04:58,880 --> 00:05:03,440
know what spark is right

00:05:00,639 --> 00:05:05,199
um so spark can be run either in a

00:05:03,440 --> 00:05:08,240
standalone mode with just spark

00:05:05,199 --> 00:05:09,520
and a jvm on each machine or you can use

00:05:08,240 --> 00:05:11,039
a custom resource

00:05:09,520 --> 00:05:13,039
management system and that's what we're

00:05:11,039 --> 00:05:15,440
going to talk about next

00:05:13,039 --> 00:05:17,440
so what is yarn yarn is yet another

00:05:15,440 --> 00:05:20,560
resource negotiator

00:05:17,440 --> 00:05:21,199
so yarn was released in 2012 and was a

00:05:20,560 --> 00:05:23,440
rewrite

00:05:21,199 --> 00:05:25,039
of mapreduce the mapreduce engine from

00:05:23,440 --> 00:05:28,479
hadoop 1.0

00:05:25,039 --> 00:05:30,160
so mr2 which was the informal nickname

00:05:28,479 --> 00:05:32,080
uh even though it really means mapper it

00:05:30,160 --> 00:05:33,840
is 2.0 which is an application that is

00:05:32,080 --> 00:05:35,440
actually managed by yarn so it all gets

00:05:33,840 --> 00:05:38,479
a little bit confusing

00:05:35,440 --> 00:05:40,479
but it is the de facto standard for big

00:05:38,479 --> 00:05:42,880
data workloads

00:05:40,479 --> 00:05:44,720
so yarn supports a variety of process

00:05:42,880 --> 00:05:46,560
engines and applications

00:05:44,720 --> 00:05:48,080
um you can run hadoop and spark on the

00:05:46,560 --> 00:05:50,160
same cluster

00:05:48,080 --> 00:05:53,440
you can do isolation and dynamic

00:05:50,160 --> 00:05:56,479
allocation of resources

00:05:53,440 --> 00:05:59,440
so yarn actually keeps track of the

00:05:56,479 --> 00:06:00,400
available resources so memory cpu

00:05:59,440 --> 00:06:02,000
storage

00:06:00,400 --> 00:06:04,720
and includes multiple types of

00:06:02,000 --> 00:06:07,919
scheduling methods

00:06:04,720 --> 00:06:11,600
so it supports uh lightweight isolation

00:06:07,919 --> 00:06:14,160
and it allows to share local disk

00:06:11,600 --> 00:06:16,400
and um so with all this talk about you

00:06:14,160 --> 00:06:18,840
know resource allocation and managing

00:06:16,400 --> 00:06:20,160
resources what does this remind you of

00:06:18,840 --> 00:06:21,759
kubecon

00:06:20,160 --> 00:06:24,240
kind of reminds me of kubernetes doesn't

00:06:21,759 --> 00:06:27,440
it so what is mesos

00:06:24,240 --> 00:06:30,080
so mesos is a distributed system kernel

00:06:27,440 --> 00:06:31,919
so mesos is very similar to kubernetes

00:06:30,080 --> 00:06:33,919
but it's a bit more flexible

00:06:31,919 --> 00:06:35,680
not only can you manage containers but

00:06:33,919 --> 00:06:38,720
you can also manage applications

00:06:35,680 --> 00:06:40,560
that are not containerized so it does

00:06:38,720 --> 00:06:42,880
try to be more than just analytics

00:06:40,560 --> 00:06:44,960
like maybe yarn which is um being able

00:06:42,880 --> 00:06:46,560
to manage hadoop and spark

00:06:44,960 --> 00:06:48,080
uh there's a private company now that is

00:06:46,560 --> 00:06:49,759
dedicated to

00:06:48,080 --> 00:06:51,440
working on it and they are a bit more

00:06:49,759 --> 00:06:54,319
cube kubernetes focused

00:06:51,440 --> 00:06:55,520
than in the past and it also allows for

00:06:54,319 --> 00:06:59,840
local shared disk

00:06:55,520 --> 00:07:01,840
as well so let's go into standalone mode

00:06:59,840 --> 00:07:04,080
so are you tired of doing your actual

00:07:01,840 --> 00:07:06,000
job which is writing java or python and

00:07:04,080 --> 00:07:07,440
doing it data analytics

00:07:06,000 --> 00:07:10,080
and you want to spend more time writing

00:07:07,440 --> 00:07:12,240
shell scripts and managing your servers

00:07:10,080 --> 00:07:17,120
like they're your kids or your pets

00:07:12,240 --> 00:07:18,720
um then this is the mode for you so

00:07:17,120 --> 00:07:20,400
standalone spark essentially a way to

00:07:18,720 --> 00:07:21,039
create a spark cluster and manage that

00:07:20,400 --> 00:07:23,280
yourself

00:07:21,039 --> 00:07:24,880
so there's no support for dynamic

00:07:23,280 --> 00:07:25,599
resource allocation or resource

00:07:24,880 --> 00:07:28,160
management

00:07:25,599 --> 00:07:29,280
and scheduling this is the more painful

00:07:28,160 --> 00:07:31,199
option for sure

00:07:29,280 --> 00:07:32,560
but it is possible maybe you have a use

00:07:31,199 --> 00:07:34,720
case but this works better for you i

00:07:32,560 --> 00:07:37,120
would love to hear it actually

00:07:34,720 --> 00:07:38,800
so it does not support dynamic scaling

00:07:37,120 --> 00:07:40,000
so that's what you are for so maybe some

00:07:38,800 --> 00:07:43,039
awesome scripts that you wrote

00:07:40,000 --> 00:07:44,720
and uh pagerduty as well

00:07:43,039 --> 00:07:46,400
so let's get into why we should put

00:07:44,720 --> 00:07:47,039
spark on kubernetes so i think we've

00:07:46,400 --> 00:07:49,280
made it

00:07:47,039 --> 00:07:50,240
pretty clear that you need a dynamic

00:07:49,280 --> 00:07:52,879
resource

00:07:50,240 --> 00:07:54,479
uh cluster management system to help you

00:07:52,879 --> 00:07:56,639
manage your spark workloads

00:07:54,479 --> 00:07:58,080
um but what was wrong with yarn and

00:07:56,639 --> 00:08:02,240
mesos why i moved to kubernetes

00:07:58,080 --> 00:08:03,919
right so there's a layer of reasons

00:08:02,240 --> 00:08:05,599
right well first this is kubecon

00:08:03,919 --> 00:08:07,599
right so we want to focus on kubernetes

00:08:05,599 --> 00:08:09,440
but all jokes aside right

00:08:07,599 --> 00:08:12,240
so one true ring to rule them all we'll

00:08:09,440 --> 00:08:14,800
talk about that more in a second

00:08:12,240 --> 00:08:15,759
you can use your spare capacity for

00:08:14,800 --> 00:08:18,240
analytics

00:08:15,759 --> 00:08:19,599
and the cloud we're going to talk about

00:08:18,240 --> 00:08:21,520
python

00:08:19,599 --> 00:08:22,720
and what using kubernetes allows you to

00:08:21,520 --> 00:08:24,319
do with python

00:08:22,720 --> 00:08:26,400
we'll talk about security and what that

00:08:24,319 --> 00:08:27,840
gives you by using kubernetes

00:08:26,400 --> 00:08:30,560
and of course it's all about learning

00:08:27,840 --> 00:08:33,839
new skills right

00:08:30,560 --> 00:08:36,159
so just from our quick review of yarn

00:08:33,839 --> 00:08:37,599
and mesos it's easy to see the benefits

00:08:36,159 --> 00:08:38,560
of needing a container management

00:08:37,599 --> 00:08:40,560
platform

00:08:38,560 --> 00:08:41,760
when putting spark on kubernetes you can

00:08:40,560 --> 00:08:44,080
add spark and

00:08:41,760 --> 00:08:45,600
any other types of workloads so it

00:08:44,080 --> 00:08:48,560
doesn't have to just be

00:08:45,600 --> 00:08:50,480
spark or hadoop etc so you can better

00:08:48,560 --> 00:08:52,560
utilize your cluster's resources because

00:08:50,480 --> 00:08:54,720
you can have all types of workloads from

00:08:52,560 --> 00:08:57,839
across your organization on one

00:08:54,720 --> 00:08:57,839
kubernetes cluster

00:08:58,080 --> 00:09:01,360
so and then just to point this out right

00:09:00,560 --> 00:09:03,600
so you know

00:09:01,360 --> 00:09:04,560
many of us in our you know where we work

00:09:03,600 --> 00:09:05,839
um

00:09:04,560 --> 00:09:07,440
we may have just a little bit of

00:09:05,839 --> 00:09:09,200
everything doing a every type of

00:09:07,440 --> 00:09:11,200
technology that there is we have

00:09:09,200 --> 00:09:12,399
we have uh spread out across our our

00:09:11,200 --> 00:09:14,720
companies right

00:09:12,399 --> 00:09:16,000
so you could you know have three cluster

00:09:14,720 --> 00:09:18,480
management systems right

00:09:16,000 --> 00:09:19,040
mesos yarn standalone kubernetes that's

00:09:18,480 --> 00:09:22,160
actually four

00:09:19,040 --> 00:09:23,120
right but managing those all you know

00:09:22,160 --> 00:09:25,440
that that leads to

00:09:23,120 --> 00:09:26,560
a sad team sad engineers um because

00:09:25,440 --> 00:09:28,560
that's a lot of

00:09:26,560 --> 00:09:30,399
you know overhead for them to have to

00:09:28,560 --> 00:09:32,080
try to you know switch between platforms

00:09:30,399 --> 00:09:34,320
and troubleshooting et cetera

00:09:32,080 --> 00:09:35,680
um so of course the solution would just

00:09:34,320 --> 00:09:36,399
be turn or two or three of them off

00:09:35,680 --> 00:09:38,320
right

00:09:36,399 --> 00:09:40,000
um that'll just make all the workloads

00:09:38,320 --> 00:09:42,080
better right well yes and no you want to

00:09:40,000 --> 00:09:44,320
definitely try to converge on one

00:09:42,080 --> 00:09:45,519
and kubernetes seems like it's it's the

00:09:44,320 --> 00:09:48,800
direction that uh

00:09:45,519 --> 00:09:49,120
so many of us are taking and so it gives

00:09:48,800 --> 00:09:50,480
you

00:09:49,120 --> 00:09:52,320
all the benefits of kubernetes which

00:09:50,480 --> 00:09:53,839
we'll talk about here in a second and

00:09:52,320 --> 00:09:55,279
it just makes your life a little bit

00:09:53,839 --> 00:09:57,600
easier to just standardize on one

00:09:55,279 --> 00:09:59,839
platform

00:09:57,600 --> 00:10:00,800
so it also so like i said before you

00:09:59,839 --> 00:10:02,240
know that you can run

00:10:00,800 --> 00:10:04,079
different types of workloads within that

00:10:02,240 --> 00:10:05,600
one kubernetes cluster so it allows you

00:10:04,079 --> 00:10:08,720
to use that spare

00:10:05,600 --> 00:10:10,160
capacity so what's nice about kubernetes

00:10:08,720 --> 00:10:11,920
again is that multiple types of

00:10:10,160 --> 00:10:13,279
applications and services can be all

00:10:11,920 --> 00:10:15,279
running on the same cluster

00:10:13,279 --> 00:10:17,839
you don't need a dedicated spark cluster

00:10:15,279 --> 00:10:19,680
that would be managed by yarn or mesos

00:10:17,839 --> 00:10:21,920
to run your spark jobs

00:10:19,680 --> 00:10:21,920
um

00:10:22,800 --> 00:10:26,560
on that same cluster you can actually

00:10:25,120 --> 00:10:28,640
run them on the same

00:10:26,560 --> 00:10:31,200
so spark jobs that may be analyzing your

00:10:28,640 --> 00:10:32,640
services you can actually

00:10:31,200 --> 00:10:34,480
have the spark drops running on the same

00:10:32,640 --> 00:10:37,279
cluster as that

00:10:34,480 --> 00:10:38,000
um so like i kind of said they're kind

00:10:37,279 --> 00:10:40,560
of choppy

00:10:38,000 --> 00:10:42,160
the spark jobs you're doing uh can be

00:10:40,560 --> 00:10:43,279
analyzed any anything from the various

00:10:42,160 --> 00:10:44,000
services that are running on the

00:10:43,279 --> 00:10:45,360
platform

00:10:44,000 --> 00:10:47,120
for doing any other kind of data

00:10:45,360 --> 00:10:49,600
crunching right

00:10:47,120 --> 00:10:51,519
and so additionally right when we're

00:10:49,600 --> 00:10:52,720
talking about uh preemption and

00:10:51,519 --> 00:10:54,320
kubernetes

00:10:52,720 --> 00:10:56,399
so kubernetes allows the ability to

00:10:54,320 --> 00:10:58,480
preempt workloads uh based on priority

00:10:56,399 --> 00:11:00,000
classes which is really powerful

00:10:58,480 --> 00:11:02,000
so workloads that are less time

00:11:00,000 --> 00:11:02,880
sensitive than other jobs can be

00:11:02,000 --> 00:11:05,360
rescheduled

00:11:02,880 --> 00:11:06,399
when resource demand is lower and this

00:11:05,360 --> 00:11:09,760
is all just can be done

00:11:06,399 --> 00:11:11,519
you know fairly automatically

00:11:09,760 --> 00:11:13,600
the power of kubernetes allows for

00:11:11,519 --> 00:11:15,200
resources to be redistributed

00:11:13,600 --> 00:11:16,640
once a spark job is complete and the

00:11:15,200 --> 00:11:18,959
pods have been released

00:11:16,640 --> 00:11:22,720
so that that resources can be given to

00:11:18,959 --> 00:11:22,720
other spark jobs or to other services

00:11:25,040 --> 00:11:30,880
so um next year so talking about

00:11:28,399 --> 00:11:33,920
containers and python and security

00:11:30,880 --> 00:11:35,519
so the ability to easily move workloads

00:11:33,920 --> 00:11:38,560
from dev to prod

00:11:35,519 --> 00:11:41,279
to add new libraries um are you know

00:11:38,560 --> 00:11:42,399
that is because of things like isolation

00:11:41,279 --> 00:11:44,160
that cube has

00:11:42,399 --> 00:11:45,519
so data engineers and scientists are

00:11:44,160 --> 00:11:49,120
always trying to find

00:11:45,519 --> 00:11:50,560
new python packages to add um and ways

00:11:49,120 --> 00:11:52,160
you know new workloads that they want to

00:11:50,560 --> 00:11:53,920
be able to utilize the latest versions

00:11:52,160 --> 00:11:54,720
of these libraries and new versions of

00:11:53,920 --> 00:11:56,959
spark

00:11:54,720 --> 00:11:58,959
so with a single yarn mesos or

00:11:56,959 --> 00:12:02,160
standalone cube cluster

00:11:58,959 --> 00:12:04,000
um standalone spark cluster i should say

00:12:02,160 --> 00:12:05,920
your apps will be tied to using only

00:12:04,000 --> 00:12:08,160
using one version of spark

00:12:05,920 --> 00:12:09,200
or python that's in you know that's on

00:12:08,160 --> 00:12:11,120
that cluster

00:12:09,200 --> 00:12:12,800
with kubernetes and containerization of

00:12:11,120 --> 00:12:14,639
sparkler workloads

00:12:12,800 --> 00:12:18,000
one data scientist can be using the

00:12:14,639 --> 00:12:20,480
latest version of spark so 3.1.1

00:12:18,000 --> 00:12:21,360
with python version 3.0 while your data

00:12:20,480 --> 00:12:24,000
engineer

00:12:21,360 --> 00:12:25,920
can continue to use spark 2.4 why not

00:12:24,000 --> 00:12:27,600
with python version 2.5

00:12:25,920 --> 00:12:29,440
all along the same cluster because it's

00:12:27,600 --> 00:12:32,399
all containerized and

00:12:29,440 --> 00:12:34,160
isolated so anyone who has had to deal

00:12:32,399 --> 00:12:35,920
with python dependencies

00:12:34,160 --> 00:12:39,200
understands the importance of having the

00:12:35,920 --> 00:12:41,920
ability to abstract

00:12:39,200 --> 00:12:45,360
using containers so that their jobs can

00:12:41,920 --> 00:12:48,079
do and use exactly what they want to use

00:12:45,360 --> 00:12:49,040
also the ability to add increased

00:12:48,079 --> 00:12:51,200
isolation

00:12:49,040 --> 00:12:52,800
when dealing with data is very important

00:12:51,200 --> 00:12:54,560
and kubernetes allows for this

00:12:52,800 --> 00:12:57,120
where yarn or mesos just didn't to the

00:12:54,560 --> 00:12:57,120
same degree

00:12:57,440 --> 00:13:01,760
so also just to add when moving from

00:12:59,279 --> 00:13:03,040
yarn or mesos kubernetes has a lot of

00:13:01,760 --> 00:13:05,600
custom configurations

00:13:03,040 --> 00:13:06,079
that makes it a bit more flexible than

00:13:05,600 --> 00:13:08,480
yarn

00:13:06,079 --> 00:13:08,480
per se

00:13:09,120 --> 00:13:12,160
and last but not least learning a new

00:13:11,680 --> 00:13:14,079
skill

00:13:12,160 --> 00:13:15,600
right for holden and i personally

00:13:14,079 --> 00:13:17,279
adopting to kubernetes

00:13:15,600 --> 00:13:19,440
has allowed us to pick up one more tool

00:13:17,279 --> 00:13:20,639
for our tool belt uh and of course now

00:13:19,440 --> 00:13:21,839
we're so much more popular

00:13:20,639 --> 00:13:23,440
right because now not only do we know

00:13:21,839 --> 00:13:24,800
apache spark we also know all about

00:13:23,440 --> 00:13:27,360
kubernetes

00:13:24,800 --> 00:13:28,160
so with that said i will pass this off

00:13:27,360 --> 00:13:30,959
to holden

00:13:28,160 --> 00:13:32,639
to tell us more about what worked well

00:13:30,959 --> 00:13:34,800
uh what didn't work so well making the

00:13:32,639 --> 00:13:36,880
transformation from spark on yarn spark

00:13:34,800 --> 00:13:39,600
on mesos to spark on kubernetes

00:13:36,880 --> 00:13:40,720
thank you awesome and thanks for that

00:13:39,600 --> 00:13:43,519
introduction

00:13:40,720 --> 00:13:45,360
um so now i'm going to talk about sort

00:13:43,519 --> 00:13:47,600
of the second half of the presentation

00:13:45,360 --> 00:13:49,600
namely what worked well and where we had

00:13:47,600 --> 00:13:52,800
rooms for growth

00:13:49,600 --> 00:13:54,800
so small to medium-sized etl

00:13:52,800 --> 00:13:57,120
jobs worked really well migrating them

00:13:54,800 --> 00:13:58,800
to cube was relatively easy the only

00:13:57,120 --> 00:14:01,920
thing that we really had to do

00:13:58,800 --> 00:14:03,279
was increase the resource request to

00:14:01,920 --> 00:14:04,480
match the reality of what they were

00:14:03,279 --> 00:14:06,880
actually using

00:14:04,480 --> 00:14:08,480
because yarn and mesos weren't enforcing

00:14:06,880 --> 00:14:10,880
the resource

00:14:08,480 --> 00:14:13,040
limits quite as strictly as cube ended

00:14:10,880 --> 00:14:14,839
up enforcing them

00:14:13,040 --> 00:14:16,639
there were some challenges around

00:14:14,839 --> 00:14:19,120
integration with the different data

00:14:16,639 --> 00:14:21,120
sources and that mostly comes back to

00:14:19,120 --> 00:14:22,959
uh some networking configuration choices

00:14:21,120 --> 00:14:24,000
that were made

00:14:22,959 --> 00:14:26,480
and i think we could make some

00:14:24,000 --> 00:14:28,320
improvements there but that's sort of

00:14:26,480 --> 00:14:29,920
just something to keep in mind like make

00:14:28,320 --> 00:14:31,839
sure that you can easily access your

00:14:29,920 --> 00:14:35,680
data sources

00:14:31,839 --> 00:14:35,680
but it was a relatively easy fix

00:14:35,839 --> 00:14:39,839
large and long etl jobs were a bit more

00:14:38,399 --> 00:14:41,600
challenging

00:14:39,839 --> 00:14:43,519
long enough running jobs at low

00:14:41,600 --> 00:14:44,959
priorities tended to run into over

00:14:43,519 --> 00:14:47,440
commit issues

00:14:44,959 --> 00:14:49,360
they'd still succeed but often they take

00:14:47,440 --> 00:14:53,519
a lot longer than they would

00:14:49,360 --> 00:14:54,560
on mesos or yarn and the primary room

00:14:53,519 --> 00:14:57,279
for growth here

00:14:54,560 --> 00:14:58,000
is more efficient resource utilization

00:14:57,279 --> 00:15:01,920
and more

00:14:58,000 --> 00:15:03,680
effective handling of over commit issues

00:15:01,920 --> 00:15:06,480
multi-language jobs are where things

00:15:03,680 --> 00:15:09,120
started to get a little

00:15:06,480 --> 00:15:10,560
challenging um so one of the really

00:15:09,120 --> 00:15:11,120
great things is that dependency

00:15:10,560 --> 00:15:13,519
management

00:15:11,120 --> 00:15:14,880
improves substantially compared to yarn

00:15:13,519 --> 00:15:17,440
in the yarn world

00:15:14,880 --> 00:15:20,000
all the dependencies had to be managed

00:15:17,440 --> 00:15:22,959
by a systems administrator whereas

00:15:20,000 --> 00:15:23,519
with running on cube it could be very

00:15:22,959 --> 00:15:26,800
much more

00:15:23,519 --> 00:15:28,720
self-serve the initial migrations often

00:15:26,800 --> 00:15:31,279
ran into resource difficulty

00:15:28,720 --> 00:15:33,120
and unplanned exits caused large amounts

00:15:31,279 --> 00:15:35,839
of recomputation

00:15:33,120 --> 00:15:37,759
and this is you know the recomputation

00:15:35,839 --> 00:15:39,279
that is sort of expected

00:15:37,759 --> 00:15:41,199
but it was more than the amount of

00:15:39,279 --> 00:15:43,279
recomputation that we were seeing

00:15:41,199 --> 00:15:45,120
running these jobs in other cluster

00:15:43,279 --> 00:15:46,800
environments

00:15:45,120 --> 00:15:48,560
most of our opportunities for

00:15:46,800 --> 00:15:51,600
improvement are around

00:15:48,560 --> 00:15:53,360
memory allocation and specifically sort

00:15:51,600 --> 00:15:57,680
of how we share

00:15:53,360 --> 00:15:57,680
native and jvm memory

00:15:57,920 --> 00:16:02,480
complex ml jobs did not work well when

00:16:00,399 --> 00:16:06,800
we tried to migrate them

00:16:02,480 --> 00:16:08,800
primarily due to much more expensive

00:16:06,800 --> 00:16:10,240
recovery costs so in all these

00:16:08,800 --> 00:16:12,320
situations spark handles

00:16:10,240 --> 00:16:14,079
executor failure by recomputing data

00:16:12,320 --> 00:16:17,440
loss with the complex

00:16:14,079 --> 00:16:18,639
ml jobs the cost of recompeting that

00:16:17,440 --> 00:16:21,519
data is really

00:16:18,639 --> 00:16:22,959
really quite expensive the opportunities

00:16:21,519 --> 00:16:25,040
for improvement

00:16:22,959 --> 00:16:27,199
are around checkpointing um and so one

00:16:25,040 --> 00:16:28,880
of the things that we can do to sort of

00:16:27,199 --> 00:16:30,240
deal with this more expensive

00:16:28,880 --> 00:16:32,800
recomputation

00:16:30,240 --> 00:16:33,759
is um when we get to these really

00:16:32,800 --> 00:16:36,000
expensive points

00:16:33,759 --> 00:16:37,519
is checkpoint to persistent storage but

00:16:36,000 --> 00:16:39,920
we had difficulties connecting to

00:16:37,519 --> 00:16:41,680
persistent storage

00:16:39,920 --> 00:16:43,279
specifically the kinds with ttls that

00:16:41,680 --> 00:16:44,800
can do automatic cleanups

00:16:43,279 --> 00:16:48,160
and so this is one of the rooms for

00:16:44,800 --> 00:16:50,720
growth that we have in complex ml jobs

00:16:48,160 --> 00:16:51,600
streaming jobs had a lot of room for

00:16:50,720 --> 00:16:54,399
growth

00:16:51,600 --> 00:16:55,440
and just many areas of opportunity for

00:16:54,399 --> 00:16:58,160
investment

00:16:55,440 --> 00:16:59,040
um they just frankly don't work very

00:16:58,160 --> 00:17:00,160
well right now

00:16:59,040 --> 00:17:01,839
and a lot of that is around the

00:17:00,160 --> 00:17:02,240
connection to the different data sources

00:17:01,839 --> 00:17:03,759
um

00:17:02,240 --> 00:17:05,839
it turns out that our connection to the

00:17:03,759 --> 00:17:07,679
streaming data sources has even more

00:17:05,839 --> 00:17:09,520
room for growth than our connections to

00:17:07,679 --> 00:17:11,439
the batch data sources

00:17:09,520 --> 00:17:12,799
um also checkpointing is especially

00:17:11,439 --> 00:17:14,880
important in streaming

00:17:12,799 --> 00:17:17,039
and so the same problem that we had from

00:17:14,880 --> 00:17:19,839
ml is even more present with the

00:17:17,039 --> 00:17:19,839
streaming jobs

00:17:20,319 --> 00:17:24,079
so okay you know that's about our

00:17:22,720 --> 00:17:25,520
experience as moving to cube

00:17:24,079 --> 00:17:27,439
what are the best practices that we

00:17:25,520 --> 00:17:29,039
learned from this

00:17:27,439 --> 00:17:31,760
so one of the things that we learned is

00:17:29,039 --> 00:17:34,799
not to just cache everything

00:17:31,760 --> 00:17:36,320
caching was never free right but

00:17:34,799 --> 00:17:39,280
now it costs even more and that's

00:17:36,320 --> 00:17:42,000
because when we decommission an executor

00:17:39,280 --> 00:17:44,000
we now have to migrate the cache data

00:17:42,000 --> 00:17:45,360
and also now for caching stuff to disk

00:17:44,000 --> 00:17:48,559
disk is actually a

00:17:45,360 --> 00:17:49,679
metered resource so be careful like

00:17:48,559 --> 00:17:51,039
think about are you actually going to

00:17:49,679 --> 00:17:53,200
use the data twice

00:17:51,039 --> 00:17:54,400
if you're done with it tell spark you're

00:17:53,200 --> 00:17:55,919
done with it right

00:17:54,400 --> 00:17:57,679
especially for users in no quick

00:17:55,919 --> 00:17:59,679
environments the garbage collector isn't

00:17:57,679 --> 00:18:02,400
able

00:17:59,679 --> 00:18:03,919
to handle this as well so you need to

00:18:02,400 --> 00:18:06,320
explicitly tell spark that you're done

00:18:03,919 --> 00:18:07,840
with the data and it can get rid of it

00:18:06,320 --> 00:18:09,679
timbit of course is not done with the

00:18:07,840 --> 00:18:12,799
bone and he would prefer that we never

00:18:09,679 --> 00:18:12,799
get rid of the bone

00:18:13,280 --> 00:18:17,520
using disaggregated storage is super

00:18:16,320 --> 00:18:19,120
important um

00:18:17,520 --> 00:18:21,200
the big thing is there's no stay

00:18:19,120 --> 00:18:24,480
resident block manager anymore

00:18:21,200 --> 00:18:25,440
um and yeah okay data locality does

00:18:24,480 --> 00:18:27,679
matter

00:18:25,440 --> 00:18:29,600
but it's not enough to try and co-locate

00:18:27,679 --> 00:18:32,080
hdfs it's not worth it

00:18:29,600 --> 00:18:34,480
um another really interesting thing is

00:18:32,080 --> 00:18:36,320
that using cloud storage from on-prem

00:18:34,480 --> 00:18:37,919
doesn't have as much overhead as one

00:18:36,320 --> 00:18:39,440
might think if you structure your

00:18:37,919 --> 00:18:41,760
network correctly

00:18:39,440 --> 00:18:43,919
um and so initially in a lot of

00:18:41,760 --> 00:18:45,760
situations we'd assumed that we had to

00:18:43,919 --> 00:18:47,360
use hdfs but after we did some

00:18:45,760 --> 00:18:48,960
benchmarks it turns out that it was

00:18:47,360 --> 00:18:52,000
actually perfectly reasonable

00:18:48,960 --> 00:18:54,320
to use cloud storage from on-prem um so

00:18:52,000 --> 00:18:56,240
definitely like don't just assume that

00:18:54,320 --> 00:18:58,080
you need to co-locate hdfs

00:18:56,240 --> 00:18:59,760
take the time run the benchmarks and see

00:18:58,080 --> 00:19:01,840
if it's actually going to be worth it

00:18:59,760 --> 00:19:03,440
for you

00:19:01,840 --> 00:19:04,960
one of the other things is that you're

00:19:03,440 --> 00:19:06,320
going to need to increase your resource

00:19:04,960 --> 00:19:08,840
requests and we talked about this a

00:19:06,320 --> 00:19:11,840
little bit with the job migrations

00:19:08,840 --> 00:19:14,320
essentially yarn containers are

00:19:11,840 --> 00:19:14,880
very fuzzy definitions of containers and

00:19:14,320 --> 00:19:17,679
we have

00:19:14,880 --> 00:19:18,320
much stricter resource requirements in

00:19:17,679 --> 00:19:19,840
cube

00:19:18,320 --> 00:19:22,160
uh so you're going to need to allocate

00:19:19,840 --> 00:19:25,360
more memory a femoral disk starts

00:19:22,160 --> 00:19:28,720
mattering that wasn't tracked at all

00:19:25,360 --> 00:19:31,280
previously another one is don't set

00:19:28,720 --> 00:19:34,160
quota for the sake of quota

00:19:31,280 --> 00:19:34,960
default spark uses config maps we ended

00:19:34,160 --> 00:19:38,400
up doing

00:19:34,960 --> 00:19:42,799
a large rewrite of some code because

00:19:38,400 --> 00:19:44,720
of a config map quota and

00:19:42,799 --> 00:19:46,720
you know it turns out that config maps

00:19:44,720 --> 00:19:48,799
weren't as expensive as we had assumed

00:19:46,720 --> 00:19:50,960
so this is this is very important

00:19:48,799 --> 00:19:52,720
uh when you are starting to set quota

00:19:50,960 --> 00:19:55,679
definitely take the time

00:19:52,720 --> 00:19:56,320
to see if this is actually a constrained

00:19:55,679 --> 00:19:58,960
resource

00:19:56,320 --> 00:20:00,559
or if you know you don't actually need

00:19:58,960 --> 00:20:03,919
to have a quota here and it's perfectly

00:20:00,559 --> 00:20:03,919
fine to let things run wild

00:20:04,159 --> 00:20:07,520
so in addition to sort of the best

00:20:05,600 --> 00:20:09,280
stream practices for your jobs

00:20:07,520 --> 00:20:12,400
uh let's talk about what things we

00:20:09,280 --> 00:20:15,520
changed uh so we added a new mechanism

00:20:12,400 --> 00:20:16,799
for dynamic scaling and spark uh i'm

00:20:15,520 --> 00:20:18,320
really excited about this this is

00:20:16,799 --> 00:20:21,039
actually based on a design

00:20:18,320 --> 00:20:22,880
that i came up with five years ago uh

00:20:21,039 --> 00:20:24,799
and it just didn't make sense back then

00:20:22,880 --> 00:20:26,559
um but we'll talk more about it in a

00:20:24,799 --> 00:20:28,640
little bit if we remove the commit fig

00:20:26,559 --> 00:20:30,240
map requirement as we talked about uh we

00:20:28,640 --> 00:20:32,159
essentially added an alternative because

00:20:30,240 --> 00:20:34,960
of our quota system

00:20:32,159 --> 00:20:36,159
um persistent storage for fallback uh on

00:20:34,960 --> 00:20:37,679
out of disk events

00:20:36,159 --> 00:20:39,520
um and we were really hoping that was

00:20:37,679 --> 00:20:41,679
gonna trigger

00:20:39,520 --> 00:20:43,360
uh more frequently but it turns out that

00:20:41,679 --> 00:20:44,400
you know um because of how ephemeral

00:20:43,360 --> 00:20:45,919
this quarter works

00:20:44,400 --> 00:20:49,039
uh we don't actually get the out-of-disc

00:20:45,919 --> 00:20:50,640
events um in the same way so we we also

00:20:49,039 --> 00:20:51,679
added some additional hooks inside of

00:20:50,640 --> 00:20:54,960
there

00:20:51,679 --> 00:20:57,840
integrations into pvcs some templating

00:20:54,960 --> 00:21:01,280
um the we also gave users the ability to

00:20:57,840 --> 00:21:03,520
explicitly remove unneeded shuffle files

00:21:01,280 --> 00:21:04,320
and there were a bunch of sort of corner

00:21:03,520 --> 00:21:05,919
cases

00:21:04,320 --> 00:21:07,600
in sparks understanding of pod state

00:21:05,919 --> 00:21:09,120
that historically hadn't mattered

00:21:07,600 --> 00:21:12,159
but once we started to add dynamic

00:21:09,120 --> 00:21:14,080
scaling didn't matter

00:21:12,159 --> 00:21:15,600
um and so we added graceful

00:21:14,080 --> 00:21:17,760
decommissioning and we

00:21:15,600 --> 00:21:19,440
made this to support dynamic allocation

00:21:17,760 --> 00:21:21,360
on spark on cube

00:21:19,440 --> 00:21:23,520
and this is really important because

00:21:21,360 --> 00:21:27,280
historically spark on cube

00:21:23,520 --> 00:21:29,840
has not had a good dynamic allocation

00:21:27,280 --> 00:21:31,280
only recently a restricted dynamic

00:21:29,840 --> 00:21:33,520
allocation was added

00:21:31,280 --> 00:21:35,360
where if there was no data on an

00:21:33,520 --> 00:21:36,880
executor we could get rid of it

00:21:35,360 --> 00:21:38,720
but by adding graceful decommissioning

00:21:36,880 --> 00:21:40,000
if there is data on an executor

00:21:38,720 --> 00:21:42,799
we can still get rid of it we just

00:21:40,000 --> 00:21:44,559
migrate the data away first

00:21:42,799 --> 00:21:46,320
there are some alternatives proposed by

00:21:44,559 --> 00:21:48,480
other people in the community of adding

00:21:46,320 --> 00:21:50,000
a truly external shuffle service

00:21:48,480 --> 00:21:51,200
um and there's a few different ones and

00:21:50,000 --> 00:21:52,000
we're not sure which one is going to

00:21:51,200 --> 00:21:54,320
land

00:21:52,000 --> 00:21:55,280
but um you know it'll be interesting to

00:21:54,320 --> 00:21:56,559
see those things

00:21:55,280 --> 00:21:58,159
and i think even once one of those

00:21:56,559 --> 00:21:59,919
things land we'll probably still keep

00:21:58,159 --> 00:22:01,120
graceful decommissioning and we'll still

00:21:59,919 --> 00:22:02,640
leave it turned on

00:22:01,120 --> 00:22:04,400
because the truly external shuffle

00:22:02,640 --> 00:22:06,480
service only handles shuffle files

00:22:04,400 --> 00:22:08,240
and spark also has this concept of cache

00:22:06,480 --> 00:22:10,720
blocks and i think it makes sense to

00:22:08,240 --> 00:22:13,039
migrate cache blocks

00:22:10,720 --> 00:22:14,720
just while we're talking about this um

00:22:13,039 --> 00:22:16,080
one of the things that was a little

00:22:14,720 --> 00:22:18,559
counterintuitive

00:22:16,080 --> 00:22:19,679
uh with the configuration that we found

00:22:18,559 --> 00:22:21,200
was

00:22:19,679 --> 00:22:22,960
uh that we went from this sort of

00:22:21,200 --> 00:22:25,200
initial configuration of an executor

00:22:22,960 --> 00:22:27,360
idle time in a cache idle time of 120

00:22:25,200 --> 00:22:27,919
seconds and then we increased those idle

00:22:27,360 --> 00:22:29,840
times

00:22:27,919 --> 00:22:31,120
and we actually got better scale up and

00:22:29,840 --> 00:22:34,080
scale down

00:22:31,120 --> 00:22:36,960
with the higher idle times and this is

00:22:34,080 --> 00:22:38,960
because essentially

00:22:36,960 --> 00:22:40,000
spark doesn't do a great or really

00:22:38,960 --> 00:22:42,720
perfect job of

00:22:40,000 --> 00:22:43,919
keeping track of sort of if an executor

00:22:42,720 --> 00:22:45,919
is

00:22:43,919 --> 00:22:48,640
likely to have a job scheduled on it and

00:22:45,919 --> 00:22:50,240
so we would get into the situation

00:22:48,640 --> 00:22:51,760
where we would start to see executors

00:22:50,240 --> 00:22:53,200
coming up and going away essentially

00:22:51,760 --> 00:22:55,120
flapping very quickly

00:22:53,200 --> 00:22:56,640
when we tried to set tighter timeouts

00:22:55,120 --> 00:22:58,720
that we thought would actually

00:22:56,640 --> 00:23:00,320
cause better scale up and scale down

00:22:58,720 --> 00:23:01,600
experience but by relaxing these

00:23:00,320 --> 00:23:03,440
timeouts

00:23:01,600 --> 00:23:07,200
we actually got a much more reasonable

00:23:03,440 --> 00:23:07,200
scale up and scaled down experience

00:23:07,520 --> 00:23:10,640
um we added external shuffle storage

00:23:09,679 --> 00:23:12,159
this allows

00:23:10,640 --> 00:23:13,840
scaling beyond what executor or

00:23:12,159 --> 00:23:17,039
executive migrations

00:23:13,840 --> 00:23:18,960
support um there are

00:23:17,039 --> 00:23:20,080
alternative proposals for for doing

00:23:18,960 --> 00:23:23,520
essentially

00:23:20,080 --> 00:23:25,200
um more on top of this but this is

00:23:23,520 --> 00:23:27,120
really important because with executor

00:23:25,200 --> 00:23:28,799
to executor migrations

00:23:27,120 --> 00:23:30,400
we can only scale down to the point that

00:23:28,799 --> 00:23:31,520
we still have enough of mrl disk

00:23:30,400 --> 00:23:34,080
available

00:23:31,520 --> 00:23:34,720
for the data um and if you have a lot of

00:23:34,080 --> 00:23:36,159
data

00:23:34,720 --> 00:23:37,840
but like let's say your data scientist

00:23:36,159 --> 00:23:39,360
goes home at the end of the night really

00:23:37,840 --> 00:23:40,240
it probably makes sense to go ahead and

00:23:39,360 --> 00:23:42,720
put that data

00:23:40,240 --> 00:23:43,679
in some kind of external storage while

00:23:42,720 --> 00:23:45,520
the data scientist

00:23:43,679 --> 00:23:47,240
is you know taking a break from their

00:23:45,520 --> 00:23:50,799
job you know hanging out with their

00:23:47,240 --> 00:23:53,840
family um

00:23:50,799 --> 00:23:56,559
so what were some areas for improvement

00:23:53,840 --> 00:23:58,799
um so specifically in graceful

00:23:56,559 --> 00:24:00,320
decommissioning and dynamic allocation

00:23:58,799 --> 00:24:02,640
i would say our biggest area for

00:24:00,320 --> 00:24:05,039
improvement here is documentation

00:24:02,640 --> 00:24:05,760
it is possible to turn on but really

00:24:05,039 --> 00:24:08,080
right now

00:24:05,760 --> 00:24:08,960
um as as we saw from that configuration

00:24:08,080 --> 00:24:11,520
example

00:24:08,960 --> 00:24:13,440
it it sort of involves a lot of

00:24:11,520 --> 00:24:15,120
fine-tuning and playing with things

00:24:13,440 --> 00:24:16,960
so we haven't documented it because we

00:24:15,120 --> 00:24:18,000
don't know what the right settings are

00:24:16,960 --> 00:24:19,840
generally we know what the right

00:24:18,000 --> 00:24:21,360
settings are for our cluster but we

00:24:19,840 --> 00:24:22,240
haven't had enough other people sort of

00:24:21,360 --> 00:24:24,159
play with it

00:24:22,240 --> 00:24:26,159
so we don't have good recommendations

00:24:24,159 --> 00:24:27,440
here yet and so if you do want to play

00:24:26,159 --> 00:24:30,080
with graceful decommissioning and

00:24:27,440 --> 00:24:32,080
dynamic allocation on spark on cube

00:24:30,080 --> 00:24:34,400
i would really really appreciate your

00:24:32,080 --> 00:24:35,679
feedback on sort of what's working and

00:24:34,400 --> 00:24:37,120
what's not working

00:24:35,679 --> 00:24:40,559
and if you can contribute that to the

00:24:37,120 --> 00:24:42,799
documentation that would be amazing

00:24:40,559 --> 00:24:43,760
another thing is not all data is equal

00:24:42,799 --> 00:24:45,840
right

00:24:43,760 --> 00:24:48,000
and spark has some internal heuristics

00:24:45,840 --> 00:24:50,000
around what kind of data is more likely

00:24:48,000 --> 00:24:51,760
to be used or not they're not perfect

00:24:50,000 --> 00:24:54,400
um we could start by applying those

00:24:51,760 --> 00:24:56,000
heuristics to block migrations or we

00:24:54,400 --> 00:24:57,919
could try and come up with better

00:24:56,000 --> 00:25:00,960
heuristics for what kind of data

00:24:57,919 --> 00:25:02,480
is worth migrating

00:25:00,960 --> 00:25:04,720
another one is avoiding cascading

00:25:02,480 --> 00:25:08,960
failures

00:25:04,720 --> 00:25:10,720
this we we have some work on this

00:25:08,960 --> 00:25:12,720
and it essentially can come to the point

00:25:10,720 --> 00:25:14,640
where quota can trigger cascading

00:25:12,720 --> 00:25:16,480
failures as we do migrations

00:25:14,640 --> 00:25:19,279
and we force executors over their quota

00:25:16,480 --> 00:25:22,320
limits we have sort of a hacky solution

00:25:19,279 --> 00:25:23,360
uh longer term though i think this is

00:25:22,320 --> 00:25:26,240
this is an area

00:25:23,360 --> 00:25:27,279
for for better investigation uh lazy

00:25:26,240 --> 00:25:29,840
right back support

00:25:27,279 --> 00:25:31,840
i think is also really interesting and

00:25:29,840 --> 00:25:33,520
that's this idea that like yeah we still

00:25:31,840 --> 00:25:34,880
want to try and store data locally on

00:25:33,520 --> 00:25:37,520
the executor

00:25:34,880 --> 00:25:38,799
but um we can start writing it back to

00:25:37,520 --> 00:25:42,080
persistent storage

00:25:38,799 --> 00:25:44,080
as soon as it lands on the executor um

00:25:42,080 --> 00:25:45,600
i'm not sure if this is going to like

00:25:44,080 --> 00:25:48,480
give us good performance or not

00:25:45,600 --> 00:25:50,400
but i think it's an area that really has

00:25:48,480 --> 00:25:53,919
a lot of potential and i'd like to see

00:25:50,400 --> 00:25:53,919
some more folks investigating it

00:25:54,960 --> 00:25:59,440
so more generally spark on cube has a

00:25:57,279 --> 00:26:02,320
lot of areas for improvement

00:25:59,440 --> 00:26:03,919
uh documentation is still there is some

00:26:02,320 --> 00:26:04,640
documentation for spark on cube but i

00:26:03,919 --> 00:26:06,880
think this is an

00:26:04,640 --> 00:26:07,840
area where we can once again improve a

00:26:06,880 --> 00:26:10,320
lot

00:26:07,840 --> 00:26:11,679
another one is sort of q mechanisms and

00:26:10,320 --> 00:26:14,159
better understanding of the different

00:26:11,679 --> 00:26:17,120
kinds of jobs that spark is scheduling

00:26:14,159 --> 00:26:18,880
and integration with the cube scheduler

00:26:17,120 --> 00:26:19,919
so that we can actually get faster spin

00:26:18,880 --> 00:26:21,679
up

00:26:19,919 --> 00:26:23,679
another one is better communication for

00:26:21,679 --> 00:26:25,279
failure reasons

00:26:23,679 --> 00:26:27,760
this is something that i've been working

00:26:25,279 --> 00:26:30,080
on a little bit um

00:26:27,760 --> 00:26:31,440
because there's all kinds of reasons why

00:26:30,080 --> 00:26:36,960
things can fail

00:26:31,440 --> 00:26:38,720
and it can be difficult for a user to

00:26:36,960 --> 00:26:40,799
to get at that information it's not

00:26:38,720 --> 00:26:42,880
populated into the spark web ui

00:26:40,799 --> 00:26:45,279
and so i think finding ways to better

00:26:42,880 --> 00:26:47,039
communicate to users what's going on is

00:26:45,279 --> 00:26:50,240
really important

00:26:47,039 --> 00:26:52,400
uh dynamic preemption priorities this is

00:26:50,240 --> 00:26:54,720
this is really complicated in the spark

00:26:52,400 --> 00:26:57,440
world um and that's because

00:26:54,720 --> 00:26:58,480
we definitely have this concept of like

00:26:57,440 --> 00:27:02,000
pods that

00:26:58,480 --> 00:27:03,919
are maybe more important

00:27:02,000 --> 00:27:05,440
but the problem is which pods are more

00:27:03,919 --> 00:27:07,840
important it's going to change

00:27:05,440 --> 00:27:10,000
a lot while our jobs are running and

00:27:07,840 --> 00:27:12,400
even as we change resource profiles or

00:27:10,000 --> 00:27:15,520
even within the same resource profile

00:27:12,400 --> 00:27:17,200
as data ages in and out

00:27:15,520 --> 00:27:18,799
and so we don't have a really good way

00:27:17,200 --> 00:27:20,480
to communicate that to the cube

00:27:18,799 --> 00:27:23,520
scheduler right now

00:27:20,480 --> 00:27:25,279
um another one that is kind of maybe i

00:27:23,520 --> 00:27:26,960
don't know if this is a good idea but i

00:27:25,279 --> 00:27:29,200
think it's worth exploring because it's

00:27:26,960 --> 00:27:32,880
it's a sort of hack that we've used

00:27:29,200 --> 00:27:34,320
uh in spark on different systems

00:27:32,880 --> 00:27:35,679
and essentially it's where we use a

00:27:34,320 --> 00:27:37,600
shared local volume when we have

00:27:35,679 --> 00:27:38,720
multiple executors scheduled on the same

00:27:37,600 --> 00:27:41,360
node

00:27:38,720 --> 00:27:42,640
um and this allows us to sort of pass

00:27:41,360 --> 00:27:44,240
data back and forth

00:27:42,640 --> 00:27:45,760
without necessarily having to go through

00:27:44,240 --> 00:27:49,120
the jvm

00:27:45,760 --> 00:27:50,399
on both sides of it another one is

00:27:49,120 --> 00:27:51,760
handling of jobs with changing

00:27:50,399 --> 00:27:53,520
priorities or deadlines

00:27:51,760 --> 00:27:55,120
so we might have a job which is very low

00:27:53,520 --> 00:27:56,159
priority but it really does need to

00:27:55,120 --> 00:27:58,720
complete

00:27:56,159 --> 00:28:00,320
by the end of the month and so we don't

00:27:58,720 --> 00:28:01,039
have a good way to express that right

00:28:00,320 --> 00:28:04,399
now

00:28:01,039 --> 00:28:05,840
um in fact like a you know a user

00:28:04,399 --> 00:28:07,360
would actually probably have to cancel

00:28:05,840 --> 00:28:08,320
and reschedule their job at a higher

00:28:07,360 --> 00:28:10,320
priority

00:28:08,320 --> 00:28:13,039
if it was getting close to a deadline i

00:28:10,320 --> 00:28:16,159
think having a good way to express this

00:28:13,039 --> 00:28:18,320
um in cube or or in say like

00:28:16,159 --> 00:28:20,720
volcano or something could be very

00:28:18,320 --> 00:28:20,720
useful

00:28:21,200 --> 00:28:25,120
so in conclusion spark on cube

00:28:23,840 --> 00:28:27,360
migrations they're not

00:28:25,120 --> 00:28:28,640
something that you can just you know set

00:28:27,360 --> 00:28:31,600
and forget

00:28:28,640 --> 00:28:32,880
um and yes we we can prevent spark more

00:28:31,600 --> 00:28:35,840
than api servers

00:28:32,880 --> 00:28:37,279
it has less impact on on end users um

00:28:35,840 --> 00:28:39,360
but there's still a lot of work that we

00:28:37,279 --> 00:28:41,600
can be doing to make the spark on cube

00:28:39,360 --> 00:28:43,679
preemption experience better

00:28:41,600 --> 00:28:45,120
uh the increased isolation does come

00:28:43,679 --> 00:28:47,840
with some overhead

00:28:45,120 --> 00:28:50,159
i think it's well worth it for the

00:28:47,840 --> 00:28:52,000
benefits that we get the security

00:28:50,159 --> 00:28:54,480
improvements and the additional

00:28:52,000 --> 00:28:57,600
flexibility around

00:28:54,480 --> 00:28:59,520
python and native libraries

00:28:57,600 --> 00:29:01,279
and also we really like cake and dogs

00:28:59,520 --> 00:29:03,360
you know

00:29:01,279 --> 00:29:04,399
so thanks for coming we really

00:29:03,360 --> 00:29:07,120
appreciate it

00:29:04,399 --> 00:29:08,240
um i hope you are having a wonderful

00:29:07,120 --> 00:29:13,600
kubecon

00:29:08,240 --> 00:29:13,600

YouTube URL: https://www.youtube.com/watch?v=xX2z8ndp_zg


