Title: SIG-network: Updates and Future Directions - Bowei Du & Tim Hockin, Google
Publication date: 2021-05-09
Playlist: KubeCon + CloudNativeCon Europe 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

SIG-network: Updates and Future Directions - Bowei Du & Tim Hockin, Google

This session will be a deep-dive into recent changes in networking in Kubernetes. The talk will provide an overview of current and future projects, including APIs and interfaces (Service, Endpoint[Slice], DNS, Gateway, NetworkPolicy), infrastructure (kube-proxy, IPv4/6 dual-stack. Attendees to this session will come away with a good understanding of the areas covered by sig-network and what the future directions will be.
Captions: 
	00:00:00,160 --> 00:00:04,720
welcome to the sig network deep dive

00:00:03,040 --> 00:00:06,240
this has been a collaborative set of

00:00:04,720 --> 00:00:08,480
slides from the members of the

00:00:06,240 --> 00:00:12,160
kubernetes sig network community

00:00:08,480 --> 00:00:15,200
including myself franz j ricardo

00:00:12,160 --> 00:00:17,119
rob emmanuel

00:00:15,200 --> 00:00:18,640
so what is the area that is covered by

00:00:17,119 --> 00:00:22,000
sig network

00:00:18,640 --> 00:00:24,960
well it touches on all of the networking

00:00:22,000 --> 00:00:27,840
aspects of the kubernetes ecosystem

00:00:24,960 --> 00:00:29,519
this includes pod networking within in

00:00:27,840 --> 00:00:31,920
between nodes

00:00:29,519 --> 00:00:33,200
including interfaces such as cni and

00:00:31,920 --> 00:00:36,399
ipam

00:00:33,200 --> 00:00:39,120
cluster networking in and out

00:00:36,399 --> 00:00:40,160
of the cluster network service

00:00:39,120 --> 00:00:44,079
abstractions

00:00:40,160 --> 00:00:45,280
such as load balancing of l4 l7 and

00:00:44,079 --> 00:00:48,879
service discovery

00:00:45,280 --> 00:00:52,160
so using systems such as dns

00:00:48,879 --> 00:00:53,199
network policies i.e security and access

00:00:52,160 --> 00:00:55,280
control

00:00:53,199 --> 00:00:56,800
basically how you secure your pods and

00:00:55,280 --> 00:00:59,520
workloads

00:00:56,800 --> 00:01:00,719
and of course the apis associated with

00:00:59,520 --> 00:01:04,080
these functions

00:01:00,719 --> 00:01:06,720
and these apis include pod node

00:01:04,080 --> 00:01:09,760
endpoint endpoint slice service ingress

00:01:06,720 --> 00:01:11,360
gateway and network policy

00:01:09,760 --> 00:01:13,280
for those of you who are interested we

00:01:11,360 --> 00:01:16,159
have a well-attended zoom meeting

00:01:13,280 --> 00:01:17,040
every other thursday and a busy slack

00:01:16,159 --> 00:01:18,720
channel

00:01:17,040 --> 00:01:20,560
and don't worry we will put this

00:01:18,720 --> 00:01:23,280
information back up at the end

00:01:20,560 --> 00:01:24,880
of this presentation as we only have a

00:01:23,280 --> 00:01:26,240
35-minute slot

00:01:24,880 --> 00:01:29,119
and there are many excellent intro

00:01:26,240 --> 00:01:31,840
videos here are some previous intros and

00:01:29,119 --> 00:01:33,759
deep dives the links shown here

00:01:31,840 --> 00:01:35,439
we would like to take our 20 minutes to

00:01:33,759 --> 00:01:37,840
give an update about exciting things

00:01:35,439 --> 00:01:40,159
that have been happening in the sig

00:01:37,840 --> 00:01:42,479
things that have landed things that the

00:01:40,159 --> 00:01:45,680
sig is actively working on

00:01:42,479 --> 00:01:48,720
and future directions okay

00:01:45,680 --> 00:01:50,159
so what has been happening in the sig

00:01:48,720 --> 00:01:52,240
there are many smaller improvements

00:01:50,159 --> 00:01:54,000
features that people have requested

00:01:52,240 --> 00:01:55,360
and apis have gone ga that have been

00:01:54,000 --> 00:01:57,759
covered in detail before

00:01:55,360 --> 00:01:59,920
such as endpoint slice and also there

00:01:57,759 --> 00:02:02,799
are a couple of big items

00:01:59,920 --> 00:02:04,159
dual stack which is support for ipv4 in

00:02:02,799 --> 00:02:06,479
ipv6

00:02:04,159 --> 00:02:08,000
gateway api which is the next iteration

00:02:06,479 --> 00:02:11,440
of l4 l7 service

00:02:08,000 --> 00:02:14,800
apis rethinking how to support

00:02:11,440 --> 00:02:18,000
network topology in network routing

00:02:14,800 --> 00:02:20,879
and finally network policy working group

00:02:18,000 --> 00:02:21,280
which has already landed some features

00:02:20,879 --> 00:02:23,680
and

00:02:21,280 --> 00:02:24,560
are looking as well at a potential

00:02:23,680 --> 00:02:27,680
version two

00:02:24,560 --> 00:02:29,599
of the api let's talk about some of the

00:02:27,680 --> 00:02:31,280
improvements to kubernetes networking

00:02:29,599 --> 00:02:33,440
that have landed

00:02:31,280 --> 00:02:35,519
first we have loosened the restriction

00:02:33,440 --> 00:02:38,319
that a service type load balancer

00:02:35,519 --> 00:02:40,879
can only support a single protocol this

00:02:38,319 --> 00:02:43,200
is the multi-protocol services cap

00:02:40,879 --> 00:02:44,239
and the link is there in the slide

00:02:43,200 --> 00:02:47,040
basically

00:02:44,239 --> 00:02:47,440
before this improvement you could only

00:02:47,040 --> 00:02:50,560
have

00:02:47,440 --> 00:02:52,480
a single protocol service as part of

00:02:50,560 --> 00:02:54,800
your load balancer definition

00:02:52,480 --> 00:02:55,519
with the new changes you can actually

00:02:54,800 --> 00:02:58,800
have

00:02:55,519 --> 00:03:00,480
multiple protocols supported as part of

00:02:58,800 --> 00:03:02,560
your load balancer service

00:03:00,480 --> 00:03:04,080
for example in this case you see a mixed

00:03:02,560 --> 00:03:08,239
protocol service

00:03:04,080 --> 00:03:10,720
that actually uses both udp and tcp

00:03:08,239 --> 00:03:12,800
as part of the load balancer service it

00:03:10,720 --> 00:03:14,480
is the case that since this is new

00:03:12,800 --> 00:03:16,159
support actually depends on your

00:03:14,480 --> 00:03:19,040
particular cloud provider

00:03:16,159 --> 00:03:20,239
so please check the port status on your

00:03:19,040 --> 00:03:21,840
service object

00:03:20,239 --> 00:03:23,840
and if your cloud provider currently

00:03:21,840 --> 00:03:25,760
does not support

00:03:23,840 --> 00:03:28,080
having multiple protocols as a load

00:03:25,760 --> 00:03:31,760
balancer service you will see this error

00:03:28,080 --> 00:03:33,360
low bouncer mix protocol not supported

00:03:31,760 --> 00:03:35,040
the next improvement that has been added

00:03:33,360 --> 00:03:37,920
to kubernetes networking

00:03:35,040 --> 00:03:39,519
is a way to disable different types of

00:03:37,920 --> 00:03:44,080
lb ports

00:03:39,519 --> 00:03:46,239
we have had feedback from users that

00:03:44,080 --> 00:03:48,640
for certain cloud providers a low

00:03:46,239 --> 00:03:49,599
bouncer service actually does not need a

00:03:48,640 --> 00:03:52,799
node port

00:03:49,599 --> 00:03:55,360
in order to function and the

00:03:52,799 --> 00:03:56,720
need for node ports has actually caused

00:03:55,360 --> 00:03:59,200
issues where

00:03:56,720 --> 00:04:00,959
node ports have been allocated and have

00:03:59,200 --> 00:04:03,439
been completely used

00:04:00,959 --> 00:04:05,760
but actually they're not really part of

00:04:03,439 --> 00:04:08,319
the functional load balancer

00:04:05,760 --> 00:04:10,720
this restricts the number of load

00:04:08,319 --> 00:04:14,239
balancer services that a user can use

00:04:10,720 --> 00:04:17,440
with disable lb ports

00:04:14,239 --> 00:04:19,919
we can set a flag on

00:04:17,440 --> 00:04:21,519
allocate load balancer node ports and if

00:04:19,919 --> 00:04:24,080
this is set to false

00:04:21,519 --> 00:04:25,360
then creation of a load bouncer service

00:04:24,080 --> 00:04:28,560
actually does not

00:04:25,360 --> 00:04:30,160
also allocate a node port load balancer

00:04:28,560 --> 00:04:31,680
and pod lifecycle has also seen

00:04:30,160 --> 00:04:33,600
improvements in this past year

00:04:31,680 --> 00:04:35,759
it has always been the case that for a

00:04:33,600 --> 00:04:36,960
service with external traffic policy

00:04:35,759 --> 00:04:38,479
equals local

00:04:36,960 --> 00:04:40,400
when the pods in the service on a

00:04:38,479 --> 00:04:42,320
specific node have been terminated

00:04:40,400 --> 00:04:44,560
there may be a period in time when the

00:04:42,320 --> 00:04:45,360
external lb health check has not been

00:04:44,560 --> 00:04:47,040
updated

00:04:45,360 --> 00:04:48,880
which causes traffic to be black hole

00:04:47,040 --> 00:04:50,479
during this time interval the pod may

00:04:48,880 --> 00:04:52,800
still be gracefully shutting down

00:04:50,479 --> 00:04:55,040
but will not receive traffic we can see

00:04:52,800 --> 00:04:58,160
this illustrated in the diagram

00:04:55,040 --> 00:05:00,000
below where the pods are terminated the

00:04:58,160 --> 00:05:03,120
yellow lightning bolt but in that

00:05:00,000 --> 00:05:05,759
interim time between the termination

00:05:03,120 --> 00:05:06,800
and the health check there is potential

00:05:05,759 --> 00:05:08,960
for a black hole

00:05:06,800 --> 00:05:11,039
by tracking terminating endpoints in a

00:05:08,960 --> 00:05:12,880
separate state in an endpoint slice

00:05:11,039 --> 00:05:15,199
we are able to model pods in the

00:05:12,880 --> 00:05:17,120
graceful shutdown state

00:05:15,199 --> 00:05:19,280
while in this state the traffic steering

00:05:17,120 --> 00:05:19,840
behavior of external traffic policy

00:05:19,280 --> 00:05:22,400
local

00:05:19,840 --> 00:05:24,160
q proxy is somewhat changed first

00:05:22,400 --> 00:05:27,280
traffic will be sent to ready

00:05:24,160 --> 00:05:29,120
and terminating pods if no ready pods

00:05:27,280 --> 00:05:30,080
exist then the traffic will be sent to

00:05:29,120 --> 00:05:32,639
not ready

00:05:30,080 --> 00:05:33,680
and terminating pods in essence we try

00:05:32,639 --> 00:05:35,440
to deliver traffic

00:05:33,680 --> 00:05:37,440
as much as possible to endpoints that

00:05:35,440 --> 00:05:39,600
exist as opposed to black holing the

00:05:37,440 --> 00:05:41,280
traffic prematurely

00:05:39,600 --> 00:05:43,680
and now manuel will tell us about

00:05:41,280 --> 00:05:45,600
endpoint slice another big addition the

00:05:43,680 --> 00:05:48,400
sig network has been working on

00:05:45,600 --> 00:05:51,840
is graduating endpoint slices to general

00:05:48,400 --> 00:05:53,919
availability in kubernetes 121.

00:05:51,840 --> 00:05:56,080
slices were introduced as a beta feature

00:05:53,919 --> 00:05:57,919
to kubernetes and 117

00:05:56,080 --> 00:05:59,520
and they offer a more simple way to

00:05:57,919 --> 00:06:01,199
track network ends points within

00:05:59,520 --> 00:06:04,560
kubernetes at scale

00:06:01,199 --> 00:06:07,039
previously the endpoint object hold all

00:06:04,560 --> 00:06:08,639
endpoints of the pots correlating to the

00:06:07,039 --> 00:06:11,120
given surface object

00:06:08,639 --> 00:06:11,840
and therefore could grow quite big in

00:06:11,120 --> 00:06:14,720
case of

00:06:11,840 --> 00:06:15,680
lots of pots with endpoint slices those

00:06:14,720 --> 00:06:18,240
gigantic

00:06:15,680 --> 00:06:18,720
endpoint resources would be splitted

00:06:18,240 --> 00:06:21,360
into

00:06:18,720 --> 00:06:22,400
more granular endpoint slices where the

00:06:21,360 --> 00:06:25,600
endpoint slices

00:06:22,400 --> 00:06:29,039
only hold the subpart of addresses

00:06:25,600 --> 00:06:29,840
for the pots if noah pot changes its

00:06:29,039 --> 00:06:31,840
endpoint

00:06:29,840 --> 00:06:32,960
only this endpoint slices needs to

00:06:31,840 --> 00:06:35,600
update as well

00:06:32,960 --> 00:06:37,280
resulting in way less and way less

00:06:35,600 --> 00:06:38,000
consumption of for example memory and

00:06:37,280 --> 00:06:40,479
network

00:06:38,000 --> 00:06:41,919
big endpoint object previously caused

00:06:40,479 --> 00:06:45,199
issues in different

00:06:41,919 --> 00:06:46,240
in different kubernetes parts such as

00:06:45,199 --> 00:06:49,520
the api or

00:06:46,240 --> 00:06:51,680
etcd as it was just grown so big and

00:06:49,520 --> 00:06:53,440
therefore scaling was difficult

00:06:51,680 --> 00:06:55,520
with this new feature this becomes

00:06:53,440 --> 00:06:56,280
finally easy again endpoint slices

00:06:55,520 --> 00:07:00,560
support

00:06:56,280 --> 00:07:02,560
ipv4 ipv6 and fqdns

00:07:00,560 --> 00:07:04,800
finally there is an important security

00:07:02,560 --> 00:07:07,520
related update around the service

00:07:04,800 --> 00:07:09,120
external ips feature use of the service

00:07:07,520 --> 00:07:12,160
external ips feature

00:07:09,120 --> 00:07:13,840
allows users to specify an ip directly

00:07:12,160 --> 00:07:16,080
as a way to integrate with manually

00:07:13,840 --> 00:07:18,080
configured external load balancers

00:07:16,080 --> 00:07:20,240
unfortunately it is impossible for

00:07:18,080 --> 00:07:22,639
kubernetes to determine whether or not

00:07:20,240 --> 00:07:23,440
the ip specified actually belongs to the

00:07:22,639 --> 00:07:25,680
user

00:07:23,440 --> 00:07:26,720
or is an internet ip that they might be

00:07:25,680 --> 00:07:29,199
spoofing

00:07:26,720 --> 00:07:30,080
this can cause unwanted traffic capture

00:07:29,199 --> 00:07:33,280
in the cluster

00:07:30,080 --> 00:07:35,599
as external ips are a ga feature that

00:07:33,280 --> 00:07:37,440
is in use the recommendation from sig

00:07:35,599 --> 00:07:40,000
network is to disable this feature

00:07:37,440 --> 00:07:41,599
by default using an admission controller

00:07:40,000 --> 00:07:43,120
those who continue to use the feature

00:07:41,599 --> 00:07:45,440
should think about how they will defend

00:07:43,120 --> 00:07:47,919
against this attack vector

00:07:45,440 --> 00:07:50,720
let's now cover some of the big changes

00:07:47,919 --> 00:07:52,800
that have been going on in sig network

00:07:50,720 --> 00:07:55,120
first exciting news is that dual stack

00:07:52,800 --> 00:07:57,599
support is progressing to beta

00:07:55,120 --> 00:07:59,280
so what is dual stack support dual stack

00:07:57,599 --> 00:08:00,080
allows all kubernetes networking

00:07:59,280 --> 00:08:02,560
components

00:08:00,080 --> 00:08:05,759
that is pods services and nodes to have

00:08:02,560 --> 00:08:06,560
both ipv4 and ipv6 addresses at the same

00:08:05,759 --> 00:08:08,560
time

00:08:06,560 --> 00:08:10,879
also as part of the dual stack effort is

00:08:08,560 --> 00:08:12,400
the ability for clusters to migrate from

00:08:10,879 --> 00:08:15,520
single stack support

00:08:12,400 --> 00:08:16,080
to being dual stack enabled as you can

00:08:15,520 --> 00:08:18,160
imagine

00:08:16,080 --> 00:08:20,000
with all seamless migrations there needs

00:08:18,160 --> 00:08:21,599
to be careful design to make the smooth

00:08:20,000 --> 00:08:23,840
for the user

00:08:21,599 --> 00:08:26,160
we expect that services in a dual stack

00:08:23,840 --> 00:08:27,199
cluster will operate in single stack or

00:08:26,160 --> 00:08:29,280
both

00:08:27,199 --> 00:08:33,200
services from a migraine cluster will

00:08:29,280 --> 00:08:36,399
have previous semantics preserved

00:08:33,200 --> 00:08:39,279
details on the dual stack implementation

00:08:36,399 --> 00:08:40,159
it basically requires kubernetes 120 or

00:08:39,279 --> 00:08:42,080
higher

00:08:40,159 --> 00:08:45,120
and the implementation behavior has

00:08:42,080 --> 00:08:47,760
changed from previous alpha semantics

00:08:45,120 --> 00:08:49,040
you will need to enable dual stack using

00:08:47,760 --> 00:08:52,480
ipv6

00:08:49,040 --> 00:08:55,120
dual stack equals true feature gate ipv4

00:08:52,480 --> 00:08:58,320
ipv6 ranges will of course need to be

00:08:55,120 --> 00:09:00,000
specified at cluster creation time

00:08:58,320 --> 00:09:02,640
note that when converting from a single

00:09:00,000 --> 00:09:04,240
stack to a dual stack cluster

00:09:02,640 --> 00:09:06,080
all of the existing services will

00:09:04,240 --> 00:09:09,200
automatically inherit

00:09:06,080 --> 00:09:11,040
single stack with ipv4 this preserves

00:09:09,200 --> 00:09:14,399
the semantics of the current

00:09:11,040 --> 00:09:16,880
services to enable ipv6

00:09:14,399 --> 00:09:18,240
on an existing service you will need to

00:09:16,880 --> 00:09:21,360
recreate your service

00:09:18,240 --> 00:09:21,360
in dual stack mode

00:09:21,519 --> 00:09:24,560
this slide shows a summary of the

00:09:23,519 --> 00:09:27,680
changes you will see

00:09:24,560 --> 00:09:28,920
in the kubernetes api services will have

00:09:27,680 --> 00:09:31,519
a new field

00:09:28,920 --> 00:09:33,279
ipfamilypolicy this allows you to

00:09:31,519 --> 00:09:34,560
determine what kind of dual stack

00:09:33,279 --> 00:09:36,480
service you want

00:09:34,560 --> 00:09:37,760
independent of how the cluster is

00:09:36,480 --> 00:09:39,279
configured

00:09:37,760 --> 00:09:42,000
single stack means that you are

00:09:39,279 --> 00:09:44,080
explicitly requesting a particular stack

00:09:42,000 --> 00:09:46,560
and also you have to set ip families to

00:09:44,080 --> 00:09:48,640
choose which stack is to be used

00:09:46,560 --> 00:09:50,000
prefer means that you would like to be

00:09:48,640 --> 00:09:51,519
dual stack

00:09:50,000 --> 00:09:53,279
prefer means that you would like to be

00:09:51,519 --> 00:09:55,200
dual stack if it is available

00:09:53,279 --> 00:09:57,440
and the system will choose for you

00:09:55,200 --> 00:09:58,320
require means that dual stack must be

00:09:57,440 --> 00:09:59,839
enabled

00:09:58,320 --> 00:10:01,519
along with the policy is a list of

00:09:59,839 --> 00:10:03,680
preferences order does matter

00:10:01,519 --> 00:10:05,680
when looking at ip families the other

00:10:03,680 --> 00:10:07,920
resources such as pod and node

00:10:05,680 --> 00:10:11,120
have their ip related fields pluralized

00:10:07,920 --> 00:10:11,120
in a straightforward manner

00:10:11,600 --> 00:10:16,079
gateway api is another major effort in

00:10:14,160 --> 00:10:18,640
the sig

00:10:16,079 --> 00:10:21,200
the goal of the gateway api is to create

00:10:18,640 --> 00:10:24,240
a modern set of apis for deploying l4

00:10:21,200 --> 00:10:27,600
and l7 routing in kubernetes

00:10:24,240 --> 00:10:30,880
the design aims to be generic expressive

00:10:27,600 --> 00:10:32,480
extensible and role oriented

00:10:30,880 --> 00:10:34,320
we have seen the trend for kubernetes

00:10:32,480 --> 00:10:36,720
user personas to shift from

00:10:34,320 --> 00:10:38,800
empowered single developers towards a

00:10:36,720 --> 00:10:40,640
more role-oriented model where many

00:10:38,800 --> 00:10:41,360
different teams may collaborate together

00:10:40,640 --> 00:10:44,079
to deploy

00:10:41,360 --> 00:10:45,120
an application some common roles that we

00:10:44,079 --> 00:10:47,760
are thinking about

00:10:45,120 --> 00:10:49,680
include the infrastructure provider who

00:10:47,760 --> 00:10:51,040
manages the underlying infrastructure of

00:10:49,680 --> 00:10:52,880
the cluster

00:10:51,040 --> 00:10:54,640
cluster operators who deal with cluster

00:10:52,880 --> 00:10:56,560
global configuration

00:10:54,640 --> 00:11:00,000
and application developers who deploy

00:10:56,560 --> 00:11:02,880
their apps and workloads

00:11:00,000 --> 00:11:04,640
the gateway api has three main goals at

00:11:02,880 --> 00:11:07,279
the high level

00:11:04,640 --> 00:11:08,399
one better model personas and roles like

00:11:07,279 --> 00:11:11,920
we described

00:11:08,399 --> 00:11:12,839
previously support modern load balancing

00:11:11,920 --> 00:11:14,720
features with

00:11:12,839 --> 00:11:16,480
predictability maybe not perfect

00:11:14,720 --> 00:11:18,320
portability but at least predictable

00:11:16,480 --> 00:11:20,480
portability

00:11:18,320 --> 00:11:22,000
three create standard mechanisms for

00:11:20,480 --> 00:11:23,760
extension growth

00:11:22,000 --> 00:11:25,279
implementation and vendor-specific

00:11:23,760 --> 00:11:26,720
behaviors

00:11:25,279 --> 00:11:28,720
you will see that to accomplish these

00:11:26,720 --> 00:11:30,720
things we are focusing on

00:11:28,720 --> 00:11:32,720
creating a scalable resource model that

00:11:30,720 --> 00:11:34,399
can work well with our back

00:11:32,720 --> 00:11:36,160
creating a notion of levels of support

00:11:34,399 --> 00:11:38,079
and conformance akin to conformance

00:11:36,160 --> 00:11:39,760
profiles in kubernetes

00:11:38,079 --> 00:11:41,360
and creating a flexible resource model

00:11:39,760 --> 00:11:43,040
that allows for some degree

00:11:41,360 --> 00:11:45,440
of polymorphism when dealing with

00:11:43,040 --> 00:11:47,760
resource relationships

00:11:45,440 --> 00:11:48,640
this slide shows a sketch of how the api

00:11:47,760 --> 00:11:51,120
objects relate

00:11:48,640 --> 00:11:52,959
in the gateway api the infrastructure

00:11:51,120 --> 00:11:54,959
provider will deal with creating gateway

00:11:52,959 --> 00:11:56,399
class resources that determine what kind

00:11:54,959 --> 00:11:58,880
of gateways will be possible to

00:11:56,399 --> 00:12:01,040
provision in a given cluster

00:11:58,880 --> 00:12:02,000
the cluster operator and or application

00:12:01,040 --> 00:12:03,519
developer

00:12:02,000 --> 00:12:05,360
will create gateway objects that

00:12:03,519 --> 00:12:07,040
represent specific instances

00:12:05,360 --> 00:12:09,360
of load balancers and the ways that

00:12:07,040 --> 00:12:11,279
users can access these services

00:12:09,360 --> 00:12:12,480
finally application developers will

00:12:11,279 --> 00:12:13,680
write routes that model their

00:12:12,480 --> 00:12:15,839
applications

00:12:13,680 --> 00:12:18,480
note that routes are not limited to hdp

00:12:15,839 --> 00:12:20,079
and l7 but can also be l4

00:12:18,480 --> 00:12:21,519
in fact the type of the route object is

00:12:20,079 --> 00:12:24,000
protocol specific

00:12:21,519 --> 00:12:27,440
and this allows for a lot of room for

00:12:24,000 --> 00:12:29,279
extension to custom protocol types

00:12:27,440 --> 00:12:32,000
so we're really excited to announce that

00:12:29,279 --> 00:12:34,800
the v1 alpha 1 version has been cut

00:12:32,000 --> 00:12:37,279
and now there are at least six different

00:12:34,800 --> 00:12:38,720
implementations in the works

00:12:37,279 --> 00:12:41,120
please visit the website for more

00:12:38,720 --> 00:12:43,920
details v1 alpha 1

00:12:41,120 --> 00:12:44,880
includes features such as l4 and l7 load

00:12:43,920 --> 00:12:47,040
balancing

00:12:44,880 --> 00:12:49,360
advanced tls configuration beyond what

00:12:47,040 --> 00:12:51,040
is currently possible with ingress

00:12:49,360 --> 00:12:53,200
traffic splitting traffic mirroring

00:12:51,040 --> 00:12:56,800
header-based routing and modification

00:12:53,200 --> 00:12:56,800
and a lot more is in progress

00:12:57,440 --> 00:13:01,839
support for awareness of network

00:12:59,440 --> 00:13:04,800
topology is another area where the sig

00:13:01,839 --> 00:13:08,160
is focusing its efforts

00:13:04,800 --> 00:13:12,079
in kubernetes 121 we deprecated

00:13:08,160 --> 00:13:13,920
the old alpha topology implementation

00:13:12,079 --> 00:13:15,279
after a lot of discussion it turns out

00:13:13,920 --> 00:13:18,560
that the approach

00:13:15,279 --> 00:13:20,079
taken was likely too inflexible and much

00:13:18,560 --> 00:13:22,240
too prescriptive for the long-term

00:13:20,079 --> 00:13:24,639
evolution of kubernetes

00:13:22,240 --> 00:13:26,639
we have now instead moved towards a

00:13:24,639 --> 00:13:29,360
simpler approach

00:13:26,639 --> 00:13:31,279
first we handled the special case of a

00:13:29,360 --> 00:13:33,760
node local daemon set

00:13:31,279 --> 00:13:36,320
as a special case rather than bundling

00:13:33,760 --> 00:13:39,760
it with the generic topology notion

00:13:36,320 --> 00:13:43,120
and this is internal traffic policy

00:13:39,760 --> 00:13:43,600
second for topology aware network

00:13:43,120 --> 00:13:46,079
routing

00:13:43,600 --> 00:13:48,000
we use endpoint slice and a controller

00:13:46,079 --> 00:13:50,240
to manage the algorithm

00:13:48,000 --> 00:13:51,519
this makes the policy more decoupled in

00:13:50,240 --> 00:13:53,680
the system

00:13:51,519 --> 00:13:55,680
for the alpha we have a controller that

00:13:53,680 --> 00:13:56,480
allocates endpoints proportionally with

00:13:55,680 --> 00:13:59,920
hints

00:13:56,480 --> 00:14:02,079
that q proxy can consume each service

00:13:59,920 --> 00:14:04,120
can now opt into this algorithm using

00:14:02,079 --> 00:14:07,120
the annotation

00:14:04,120 --> 00:14:09,519
service.kubernetes.io topology aware

00:14:07,120 --> 00:14:09,519
hints

00:14:09,920 --> 00:14:15,440
this slide gives an example of what

00:14:11,839 --> 00:14:17,680
happens when using the topology hints

00:14:15,440 --> 00:14:18,560
given three zones with an equivalent

00:14:17,680 --> 00:14:21,760
amount of

00:14:18,560 --> 00:14:24,240
node capacity pod endpoints should be

00:14:21,760 --> 00:14:26,560
distributed evenly among them

00:14:24,240 --> 00:14:28,079
in this example zone a has one more

00:14:26,560 --> 00:14:30,000
endpoint than it needs

00:14:28,079 --> 00:14:32,560
and so that extra endpoint is then

00:14:30,000 --> 00:14:35,360
allocated to zone c

00:14:32,560 --> 00:14:37,040
the original behavior of q proxy without

00:14:35,360 --> 00:14:40,320
topology aware routing

00:14:37,040 --> 00:14:42,959
is to only use the hint for all zones

00:14:40,320 --> 00:14:44,480
which is described in this original

00:14:42,959 --> 00:14:46,160
section of the diagram

00:14:44,480 --> 00:14:47,920
whereas the smarter zone aware cube

00:14:46,160 --> 00:14:48,959
proxy will use the zonal hints and

00:14:47,920 --> 00:14:52,079
distribute

00:14:48,959 --> 00:14:54,399
traffic accordingly

00:14:52,079 --> 00:14:58,720
network policy is another area that the

00:14:54,399 --> 00:14:59,440
sig is working on of the most voted

00:14:58,720 --> 00:15:02,480
requests

00:14:59,440 --> 00:15:04,959
has been of a port range api similar to

00:15:02,480 --> 00:15:06,560
what many cni providers already have

00:15:04,959 --> 00:15:08,240
to this effect we've added this the

00:15:06,560 --> 00:15:10,240
kubernetes api now

00:15:08,240 --> 00:15:12,000
you can test it out by adding an end

00:15:10,240 --> 00:15:14,959
port field to your network policy

00:15:12,000 --> 00:15:17,199
definition also we have worked with the

00:15:14,959 --> 00:15:19,519
api machinery team to put out default

00:15:17,199 --> 00:15:21,279
labels on all namespaces

00:15:19,519 --> 00:15:23,680
this is a common thing that we have

00:15:21,279 --> 00:15:24,639
heard which is how to select a namespace

00:15:23,680 --> 00:15:28,079
when i don't

00:15:24,639 --> 00:15:30,000
have the rbac permissions to set labels

00:15:28,079 --> 00:15:33,040
thanks to our friends in api machinery

00:15:30,000 --> 00:15:35,040
for collaborating on this

00:15:33,040 --> 00:15:36,560
the sig has been collaborating on

00:15:35,040 --> 00:15:38,800
cluster network policy

00:15:36,560 --> 00:15:40,560
implementations this will allow

00:15:38,800 --> 00:15:42,399
administrators to help define

00:15:40,560 --> 00:15:43,680
defaults that apply across multiple

00:15:42,399 --> 00:15:46,560
namespaces

00:15:43,680 --> 00:15:48,399
this work is very early days but the

00:15:46,560 --> 00:15:49,759
kept is now available and we would love

00:15:48,399 --> 00:15:51,519
for anyone in the security or

00:15:49,759 --> 00:15:52,959
administrative space to leave some

00:15:51,519 --> 00:15:55,040
feedback

00:15:52,959 --> 00:15:57,199
also we've worked with sig testing to

00:15:55,040 --> 00:15:59,519
make sure that the network policy api is

00:15:57,199 --> 00:16:02,480
exercised on all relevant cnis that

00:15:59,519 --> 00:16:05,120
support network policies

00:16:02,480 --> 00:16:05,839
these are the early days for the cap to

00:16:05,120 --> 00:16:08,560
define

00:16:05,839 --> 00:16:10,399
cluster network policies once this kept

00:16:08,560 --> 00:16:12,959
merges different cni's

00:16:10,399 --> 00:16:15,120
with up to now bespoke network policy

00:16:12,959 --> 00:16:17,759
apis will be able to converge

00:16:15,120 --> 00:16:18,800
on a coming upstream cluster network

00:16:17,759 --> 00:16:21,920
policy covers

00:16:18,800 --> 00:16:24,800
the use cases of being able to enable

00:16:21,920 --> 00:16:27,360
cluster admins the ability to enforce

00:16:24,800 --> 00:16:28,320
secure by default policies on cluster

00:16:27,360 --> 00:16:30,000
tenants

00:16:28,320 --> 00:16:31,759
and hopefully the design is a happy

00:16:30,000 --> 00:16:34,160
medium between needed functionality

00:16:31,759 --> 00:16:37,519
without adding too much complexity

00:16:34,160 --> 00:16:39,759
and expressiveness the network policy

00:16:37,519 --> 00:16:42,720
group has also started discussing

00:16:39,759 --> 00:16:44,800
what would it take to create a v2 api

00:16:42,720 --> 00:16:47,199
which covers some issues such as

00:16:44,800 --> 00:16:50,079
no more defaults and then extensibility

00:16:47,199 --> 00:16:51,920
to other selectors

00:16:50,079 --> 00:16:54,959
one interesting thing that has been done

00:16:51,920 --> 00:16:57,120
in network policy is that of conformance

00:16:54,959 --> 00:16:58,079
we now have a suite of table tests for

00:16:57,120 --> 00:16:59,920
network policies

00:16:58,079 --> 00:17:01,440
that now can be used to test cni

00:16:59,920 --> 00:17:02,959
providers as well

00:17:01,440 --> 00:17:04,640
as well as learn about how different

00:17:02,959 --> 00:17:06,400
network policies affect pods in a

00:17:04,640 --> 00:17:08,000
cluster

00:17:06,400 --> 00:17:09,679
other work in the community is evolving

00:17:08,000 --> 00:17:12,319
to generate perhaps hundreds of

00:17:09,679 --> 00:17:14,000
different network policies automatically

00:17:12,319 --> 00:17:15,919
we have also begun testing network

00:17:14,000 --> 00:17:17,439
policies on windows nodes as well

00:17:15,919 --> 00:17:21,120
and have confirmed that our existing

00:17:17,439 --> 00:17:21,120
tests are windows compatible

00:17:21,760 --> 00:17:27,839
i would like to finally mention some hot

00:17:24,640 --> 00:17:29,440
off the presses efforts in the sig

00:17:27,839 --> 00:17:32,559
the sig has started to take a closer

00:17:29,440 --> 00:17:34,559
look at key proxy maintenance

00:17:32,559 --> 00:17:36,320
q proxy is a complicated component and

00:17:34,559 --> 00:17:37,200
has many different implementations and

00:17:36,320 --> 00:17:38,480
backends

00:17:37,200 --> 00:17:40,960
some of the principles that we are

00:17:38,480 --> 00:17:42,320
looking at is how do we make q proxy

00:17:40,960 --> 00:17:44,240
better for users

00:17:42,320 --> 00:17:46,400
which includes eliminating corner cases

00:17:44,240 --> 00:17:48,880
and bugs

00:17:46,400 --> 00:17:49,840
looking at different ways of factoring

00:17:48,880 --> 00:17:52,720
the component

00:17:49,840 --> 00:17:55,440
and having a new architecture which we

00:17:52,720 --> 00:17:57,120
call q proxy next-gen to speed q proxy

00:17:55,440 --> 00:17:58,640
improvements

00:17:57,120 --> 00:18:00,799
and finally as part of this new

00:17:58,640 --> 00:18:01,919
architecture make q proxy easier to

00:18:00,799 --> 00:18:04,559
maintain

00:18:01,919 --> 00:18:05,600
so have an organized maintenance of the

00:18:04,559 --> 00:18:07,679
component

00:18:05,600 --> 00:18:09,440
and also to look at the q proxy next

00:18:07,679 --> 00:18:14,000
generation architecture

00:18:09,440 --> 00:18:17,360
sort of as a natural roadmap

00:18:14,000 --> 00:18:20,480
that concludes our sig network deep dive

00:18:17,360 --> 00:18:22,320
update i hope you learned about what has

00:18:20,480 --> 00:18:24,000
been going on in the sig in the past

00:18:22,320 --> 00:18:26,720
year and have some

00:18:24,000 --> 00:18:28,640
understanding of where the sig is going

00:18:26,720 --> 00:18:32,960
in the future thank you

00:18:28,640 --> 00:18:32,960

YouTube URL: https://www.youtube.com/watch?v=Nn-qrp0TRnM


