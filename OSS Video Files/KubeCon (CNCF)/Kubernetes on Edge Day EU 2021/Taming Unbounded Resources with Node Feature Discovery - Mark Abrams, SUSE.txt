Title: Taming Unbounded Resources with Node Feature Discovery - Mark Abrams, SUSE
Publication date: 2021-05-05
Playlist: Kubernetes on Edge Day EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Taming Unbounded Resources with Node Feature Discovery - Mark Abrams, SUSE

Although Node Feature Discovery (NFD) can be used in any Kubernetes cluster, it's value really shines at the Edge. Kubernetes has 2 notable differences at the Far Edge versus in the data center. 1) The edge exploits and unbounded number of heterogenous resources 2) Enterprises are reporting cluster deployments defined in numbers from hundreds to 10s of thousands. These two characteristics will require management capabilities beyond what use in the data center today. This presentation and demo uses a small fleet of Kubernetes edge clusters and NFD to show how we can build solutions which expose the availability of any arbitrary resource desired. The capabilities proposed by NFD will provide much needed insights into the unbounded capabilities of the massive number of available resources at the Edge.
Captions: 
	00:00:00,160 --> 00:00:05,520
hi uh welcome to uh the kubernetes edge

00:00:03,600 --> 00:00:08,960
conference this is the

00:00:05,520 --> 00:00:10,800
session on taming unbounded resources

00:00:08,960 --> 00:00:13,280
with node feature discovery

00:00:10,800 --> 00:00:14,880
my name is mark abrams and i'm a field

00:00:13,280 --> 00:00:17,920
engineer and edge specialist

00:00:14,880 --> 00:00:20,800
at susa formerly with rancher

00:00:17,920 --> 00:00:22,000
so first of all i want to take a minute

00:00:20,800 --> 00:00:23,600
to say that

00:00:22,000 --> 00:00:25,359
edge means different things to different

00:00:23,600 --> 00:00:27,760
stakeholders

00:00:25,359 --> 00:00:28,400
for some the edge is the edge of the

00:00:27,760 --> 00:00:32,160
cloud

00:00:28,400 --> 00:00:35,360
and it's functions like the cloud but

00:00:32,160 --> 00:00:37,040
it's really uh it's closer to where the

00:00:35,360 --> 00:00:38,800
data is generated

00:00:37,040 --> 00:00:40,320
in this presentation i'm actually

00:00:38,800 --> 00:00:43,120
talking about uh

00:00:40,320 --> 00:00:45,600
compute at the edge of the network so

00:00:43,120 --> 00:00:47,200
kubernetes at the edge of the network

00:00:45,600 --> 00:00:49,440
not the edge of the cloud right these

00:00:47,200 --> 00:00:52,160
are the leaf nodes so

00:00:49,440 --> 00:00:53,600
the leaf clusters for example we'll see

00:00:52,160 --> 00:00:56,719
these edge use cases

00:00:53,600 --> 00:00:59,600
in factories in retail scenarios

00:00:56,719 --> 00:01:01,440
in energy in the automotive and

00:00:59,600 --> 00:01:03,680
aerospace industries

00:01:01,440 --> 00:01:06,640
we'll see it in smart cities and there's

00:01:03,680 --> 00:01:08,799
lots of other industries

00:01:06,640 --> 00:01:12,159
with that let's take a look at node

00:01:08,799 --> 00:01:15,119
feature discovery

00:01:12,159 --> 00:01:16,720
node feature discovery is really it's

00:01:15,119 --> 00:01:21,439
simply a fancy labeler

00:01:16,720 --> 00:01:24,240
you can use it to label kubernetes nodes

00:01:21,439 --> 00:01:25,360
so why do i need a labeler i think the

00:01:24,240 --> 00:01:28,240
number one benefit

00:01:25,360 --> 00:01:31,119
is for the ease of just mapping

00:01:28,240 --> 00:01:33,600
workloads to resources

00:01:31,119 --> 00:01:34,159
here is you know a five-year-old

00:01:33,600 --> 00:01:37,520
worksheet

00:01:34,159 --> 00:01:39,680
right a kindergarten worksheet

00:01:37,520 --> 00:01:43,600
finding the resources that's what this

00:01:39,680 --> 00:01:43,600
node feature discovery is doing for us

00:01:43,680 --> 00:01:48,560
in addition um you know i would say that

00:01:46,159 --> 00:01:50,640
with massive numbers of devices

00:01:48,560 --> 00:01:53,040
we must automate it's not even it's not

00:01:50,640 --> 00:01:55,119
an option we have to automate right

00:01:53,040 --> 00:01:56,399
and then of course the resource types

00:01:55,119 --> 00:01:58,320
that we're targeting don't

00:01:56,399 --> 00:01:59,680
fall neatly into the cpu and ram

00:01:58,320 --> 00:02:01,520
category

00:01:59,680 --> 00:02:04,560
and additionally we have the challenge

00:02:01,520 --> 00:02:07,840
that um the resource capabilities

00:02:04,560 --> 00:02:10,959
at the edge are unbounded

00:02:07,840 --> 00:02:14,000
do i mean by unbounded well because

00:02:10,959 --> 00:02:15,680
these unique resources are not they're

00:02:14,000 --> 00:02:17,440
not infinite right so

00:02:15,680 --> 00:02:18,879
i say unbounded because there's really

00:02:17,440 --> 00:02:22,319
no limit

00:02:18,879 --> 00:02:25,440
to what device might be attached

00:02:22,319 --> 00:02:27,040
to your notes with um with each

00:02:25,440 --> 00:02:29,280
individual use case there's

00:02:27,040 --> 00:02:30,480
like a finite set of resources but

00:02:29,280 --> 00:02:34,319
overall for the

00:02:30,480 --> 00:02:37,840
for the whole you know global edge

00:02:34,319 --> 00:02:40,400
the it's there it's unbounded so

00:02:37,840 --> 00:02:42,160
um anyways with your you know the fact

00:02:40,400 --> 00:02:45,680
that you have a finite set

00:02:42,160 --> 00:02:48,160
um in each use case is good because um

00:02:45,680 --> 00:02:50,000
now we can use labels to classify and

00:02:48,160 --> 00:02:52,480
give logical boundaries

00:02:50,000 --> 00:02:55,040
relevant to you know each individual

00:02:52,480 --> 00:02:55,040
enterprise

00:02:55,760 --> 00:02:59,519
without node feature discovery and you

00:02:58,159 --> 00:03:01,840
know massive number of

00:02:59,519 --> 00:03:03,760
numbers of clusters it could be like

00:03:01,840 --> 00:03:06,239
playing a game of go fish

00:03:03,760 --> 00:03:08,080
i don't know if you know this game but

00:03:06,239 --> 00:03:08,879
basically you take turns with your

00:03:08,080 --> 00:03:10,879
opponents

00:03:08,879 --> 00:03:13,280
asking if they have the resource you

00:03:10,879 --> 00:03:14,319
need i say to my opponent do you have

00:03:13,280 --> 00:03:15,920
any threes

00:03:14,319 --> 00:03:17,760
they don't have any in their hands so

00:03:15,920 --> 00:03:20,080
they say go fish

00:03:17,760 --> 00:03:21,040
it's a guessing game and eventually you

00:03:20,080 --> 00:03:23,840
can

00:03:21,040 --> 00:03:26,319
use cash and history to make more

00:03:23,840 --> 00:03:28,480
accurate guesses right so i have a sense

00:03:26,319 --> 00:03:31,280
of what my opponent has

00:03:28,480 --> 00:03:32,000
obviously in the real world we shouldn't

00:03:31,280 --> 00:03:35,360
guess

00:03:32,000 --> 00:03:37,200
which devices have the right resources

00:03:35,360 --> 00:03:41,599
so node feature discovery is going to

00:03:37,200 --> 00:03:41,599
take the guesswork out of it for us

00:03:41,840 --> 00:03:45,760
i'm going to demonstrate the power of

00:03:44,000 --> 00:03:46,400
node feature discovery but first i want

00:03:45,760 --> 00:03:50,959
to show

00:03:46,400 --> 00:03:53,439
my cluster architecture for the demo

00:03:50,959 --> 00:03:54,560
this is a touring pi version one board

00:03:53,439 --> 00:03:57,680
it has

00:03:54,560 --> 00:03:58,400
seven raspberry pi compute modules each

00:03:57,680 --> 00:04:01,920
res

00:03:58,400 --> 00:04:05,519
each module is a raspberry pi cm3

00:04:01,920 --> 00:04:09,200
plus with one gig of ram and four cpu

00:04:05,519 --> 00:04:11,120
for per device on device node seven

00:04:09,200 --> 00:04:13,840
i run the master and the remaining six

00:04:11,120 --> 00:04:16,079
devices are workers

00:04:13,840 --> 00:04:17,519
additionally this is not terribly

00:04:16,079 --> 00:04:20,880
relevant to the demo

00:04:17,519 --> 00:04:22,479
but i'm sure somebody's going to notice

00:04:20,880 --> 00:04:24,080
the node numbers and the host names

00:04:22,479 --> 00:04:26,240
don't match up with the

00:04:24,080 --> 00:04:27,199
device numbers in the diagram so here

00:04:26,240 --> 00:04:29,840
you can see that

00:04:27,199 --> 00:04:31,040
like the host name touring pi zero two

00:04:29,840 --> 00:04:34,960
is actually on

00:04:31,040 --> 00:04:34,960
uh device slot number three

00:04:35,280 --> 00:04:39,120
so in addition to having the seven

00:04:37,520 --> 00:04:41,919
modules um

00:04:39,120 --> 00:04:42,320
that's the compute right let's look at

00:04:41,919 --> 00:04:44,479
the

00:04:42,320 --> 00:04:45,919
attached devices and peripherals right

00:04:44,479 --> 00:04:47,919
and this is so these are the things that

00:04:45,919 --> 00:04:51,040
i attach to my edge nodes

00:04:47,919 --> 00:04:51,520
um here i have a bunch of usb ports um

00:04:51,040 --> 00:04:54,560
there are

00:04:51,520 --> 00:04:58,400
only eight there are eight ports

00:04:54,560 --> 00:05:01,280
um but they only map to four of

00:04:58,400 --> 00:05:04,000
the seven compute modules so not every

00:05:01,280 --> 00:05:07,280
compute module has access to usb

00:05:04,000 --> 00:05:09,680
and each pair of usb

00:05:07,280 --> 00:05:10,639
actually only maps to one of the compute

00:05:09,680 --> 00:05:14,320
modules

00:05:10,639 --> 00:05:16,880
and for example here is

00:05:14,320 --> 00:05:18,560
compute the compute module at device

00:05:16,880 --> 00:05:21,199
slot number two

00:05:18,560 --> 00:05:25,440
which has access to the set of usb

00:05:21,199 --> 00:05:28,000
connectors shown

00:05:25,440 --> 00:05:29,120
in addition there is an i2c hub on the

00:05:28,000 --> 00:05:31,199
touring pi

00:05:29,120 --> 00:05:32,320
now in this case every tour every

00:05:31,199 --> 00:05:35,600
compute module

00:05:32,320 --> 00:05:39,120
has access to this i2c hub

00:05:35,600 --> 00:05:41,360
and out of the box it's designed so that

00:05:39,120 --> 00:05:44,560
the hub exposes

00:05:41,360 --> 00:05:47,680
a real-time clock the ethernet

00:05:44,560 --> 00:05:49,039
hub for the devices and the power

00:05:47,680 --> 00:05:51,520
management for all

00:05:49,039 --> 00:05:52,800
seven of the devices of course you have

00:05:51,520 --> 00:05:56,000
to enable

00:05:52,800 --> 00:06:00,319
i2c in your operating system so you need

00:05:56,000 --> 00:06:00,319
to load the kernel modules to run i2c

00:06:01,039 --> 00:06:04,720
and then in addition each of these

00:06:03,440 --> 00:06:07,199
devices has

00:06:04,720 --> 00:06:08,960
40 pins of gpio that's pretty standard

00:06:07,199 --> 00:06:13,360
raspberry pi thing

00:06:08,960 --> 00:06:14,639
um but it's unique to each device so you

00:06:13,360 --> 00:06:17,919
can see here i've

00:06:14,639 --> 00:06:21,039
i've knocked out number five

00:06:17,919 --> 00:06:21,440
and the gpio pins for number five are in

00:06:21,039 --> 00:06:25,199
the

00:06:21,440 --> 00:06:28,000
red border so i'd like to show you now

00:06:25,199 --> 00:06:28,960
how we can use node feature discovery to

00:06:28,000 --> 00:06:32,880
access

00:06:28,960 --> 00:06:36,240
these three different resource types

00:06:32,880 --> 00:06:37,840
so let's go to the demo so

00:06:36,240 --> 00:06:39,360
the first thing i'm going to do i have

00:06:37,840 --> 00:06:42,800
kubernetes running on all the

00:06:39,360 --> 00:06:44,400
on the nodes as i said on the uh system

00:06:42,800 --> 00:06:46,400
and so the first thing i just show you

00:06:44,400 --> 00:06:49,039
the six nodes

00:06:46,400 --> 00:06:50,000
where the workers run and then i wrote

00:06:49,039 --> 00:06:52,880
this little uh

00:06:50,000 --> 00:06:54,960
bash script that will just um grab the

00:06:52,880 --> 00:06:57,280
labels using kubectl

00:06:54,960 --> 00:07:01,199
dumps the labels for each node into a

00:06:57,280 --> 00:07:02,400
directory so touring pi01.labels we'll

00:07:01,199 --> 00:07:05,680
have the labels from

00:07:02,400 --> 00:07:08,800
touring pi zero one i'm going to

00:07:05,680 --> 00:07:09,919
um show you the labels on touring pi

00:07:08,800 --> 00:07:12,720
zero one

00:07:09,919 --> 00:07:14,400
and you can see that the these are just

00:07:12,720 --> 00:07:16,000
standard labels i haven't installed node

00:07:14,400 --> 00:07:19,280
feature discovery yet

00:07:16,000 --> 00:07:22,880
so in a moment we'll do that so

00:07:19,280 --> 00:07:25,919
let's clear it and then let's install

00:07:22,880 --> 00:07:27,039
node feature discovery so i'll install

00:07:25,919 --> 00:07:30,400
it by installing

00:07:27,039 --> 00:07:31,120
a couple of config files these config

00:07:30,400 --> 00:07:35,599
files

00:07:31,120 --> 00:07:39,199
are there's a master

00:07:35,599 --> 00:07:40,560
and a worker i simply downloaded these i

00:07:39,199 --> 00:07:42,560
didn't modify these

00:07:40,560 --> 00:07:45,759
well i yeah i changed these to use an

00:07:42,560 --> 00:07:48,560
image container that i created for

00:07:45,759 --> 00:07:51,759
the arm system i couldn't get their arm

00:07:48,560 --> 00:07:53,680
containers working so i rolled my own

00:07:51,759 --> 00:07:55,599
but basically these are preset and ready

00:07:53,680 --> 00:07:56,560
to go so i've installed node feature

00:07:55,599 --> 00:07:59,440
discovery

00:07:56,560 --> 00:07:59,919
and now let's take a look at the labels

00:07:59,440 --> 00:08:04,080
on

00:07:59,919 --> 00:08:08,800
touring pi zero one again

00:08:04,080 --> 00:08:08,800
so here they are after node feature

00:08:10,840 --> 00:08:13,840
discovery

00:08:15,440 --> 00:08:21,440
great okay so you can see that um

00:08:18,560 --> 00:08:23,039
i i now have labels that have this

00:08:21,440 --> 00:08:25,520
domain prefix

00:08:23,039 --> 00:08:27,440
and then i've got stuff for cpu for

00:08:25,520 --> 00:08:29,759
kernel for storage for system

00:08:27,440 --> 00:08:30,560
this is all just defaults i didn't i

00:08:29,759 --> 00:08:32,959
didn't change

00:08:30,560 --> 00:08:34,479
anything here this is just what node

00:08:32,959 --> 00:08:37,519
feature discovery does

00:08:34,479 --> 00:08:40,320
out of the box and

00:08:37,519 --> 00:08:41,599
you can see there's even one for for usb

00:08:40,320 --> 00:08:45,440
uh this is actually

00:08:41,599 --> 00:08:49,680
the ethernet adapter for the devices

00:08:45,440 --> 00:08:52,240
um okay so we've got um

00:08:49,680 --> 00:08:54,000
we've we've now got uh node feature

00:08:52,240 --> 00:08:56,720
discovery installed we can see it's

00:08:54,000 --> 00:08:58,240
working and what i want to do is

00:08:56,720 --> 00:09:00,320
actually

00:08:58,240 --> 00:09:01,440
modify the config map so the node

00:09:00,320 --> 00:09:05,279
feature discovery

00:09:01,440 --> 00:09:09,120
is managed by a configuration file which

00:09:05,279 --> 00:09:13,200
tells nfd worker

00:09:09,120 --> 00:09:17,200
which types of resources

00:09:13,200 --> 00:09:20,399
to look for to find so i'll go into this

00:09:17,200 --> 00:09:21,760
config file and i will uncomment out the

00:09:20,399 --> 00:09:24,240
sources section

00:09:21,760 --> 00:09:25,920
and then i'll find the usb section i

00:09:24,240 --> 00:09:28,880
will uncomment out the usb

00:09:25,920 --> 00:09:29,920
section and open up the device class

00:09:28,880 --> 00:09:33,360
white list

00:09:29,920 --> 00:09:34,480
i will enter just one device class and

00:09:33,360 --> 00:09:37,680
that is class 0

00:09:34,480 --> 00:09:40,640
3. that is a human interface device

00:09:37,680 --> 00:09:41,279
i'll make it so that we can see the the

00:09:40,640 --> 00:09:43,360
class

00:09:41,279 --> 00:09:44,640
the vendor and the device on the label

00:09:43,360 --> 00:09:47,600
that gets applied

00:09:44,640 --> 00:09:48,959
and i'll save this and kubernetes is

00:09:47,600 --> 00:09:52,320
going to

00:09:48,959 --> 00:09:54,560
basically now label the nodes that have

00:09:52,320 --> 00:09:56,240
a human interface device now i have

00:09:54,560 --> 00:09:59,839
plugged in a keyboard

00:09:56,240 --> 00:10:02,480
um so a human interface device um

00:09:59,839 --> 00:10:03,600
that will show up on one of the usb

00:10:02,480 --> 00:10:07,360
ports

00:10:03,600 --> 00:10:10,800
i um just modified my script here

00:10:07,360 --> 00:10:12,959
well i'm i'm using the script but

00:10:10,800 --> 00:10:14,320
to get the labels but then i want to

00:10:12,959 --> 00:10:15,760
show you every node

00:10:14,320 --> 00:10:18,000
and i'm only going to we're just going

00:10:15,760 --> 00:10:19,440
to grep for usb we're not going to look

00:10:18,000 --> 00:10:20,560
at everything i'll just show you where

00:10:19,440 --> 00:10:23,920
the labels are

00:10:20,560 --> 00:10:27,760
for usb now so here you can see

00:10:23,920 --> 00:10:30,880
that usb on touring pi

00:10:27,760 --> 00:10:34,399
5 node 5

00:10:30,880 --> 00:10:37,440
is uh has this usb

00:10:34,399 --> 00:10:40,720
with a human interface device label

00:10:37,440 --> 00:10:46,240
so um now i'm gonna move

00:10:40,720 --> 00:10:48,640
the um the usb

00:10:46,240 --> 00:10:49,440
unplug it and plug it into a different

00:10:48,640 --> 00:10:51,760
port

00:10:49,440 --> 00:10:53,120
and so what's going to happen when i do

00:10:51,760 --> 00:10:55,600
that is that

00:10:53,120 --> 00:10:57,120
it's going to relabel it's going to

00:10:55,600 --> 00:10:58,240
remove the label from the one that no

00:10:57,120 --> 00:11:01,360
longer has it

00:10:58,240 --> 00:11:03,519
and then it adds it to the one that has

00:11:01,360 --> 00:11:06,079
it so you can see here it's moved

00:11:03,519 --> 00:11:07,360
originally it was on five and now it's

00:11:06,079 --> 00:11:09,200
on three

00:11:07,360 --> 00:11:10,720
um and that's just by unplugging and

00:11:09,200 --> 00:11:13,760
plugging so you know

00:11:10,720 --> 00:11:14,480
hot swap capability with node feature

00:11:13,760 --> 00:11:17,760
discovery

00:11:14,480 --> 00:11:20,480
figuring out which features

00:11:17,760 --> 00:11:21,680
exist which devices exist on uh which

00:11:20,480 --> 00:11:24,320
nodes

00:11:21,680 --> 00:11:25,440
uh great okay so let's let's so that's

00:11:24,320 --> 00:11:29,440
the usb

00:11:25,440 --> 00:11:31,839
let's take a look at i2c

00:11:29,440 --> 00:11:33,360
for i2c i'm going to use the kernel

00:11:31,839 --> 00:11:34,959
module so again i'm editing the

00:11:33,360 --> 00:11:37,200
configuration file

00:11:34,959 --> 00:11:39,279
i'm going to look for the custom section

00:11:37,200 --> 00:11:42,079
i will uncomment custom

00:11:39,279 --> 00:11:43,040
and then i'll add this block of text

00:11:42,079 --> 00:11:46,959
that

00:11:43,040 --> 00:11:48,399
or this this block that um i call it i2c

00:11:46,959 --> 00:11:49,839
power management that's a name i've

00:11:48,399 --> 00:11:53,440
chosen

00:11:49,839 --> 00:11:54,480
i'm going to match on loaded kernel

00:11:53,440 --> 00:11:56,240
modules

00:11:54,480 --> 00:11:58,240
and this is a feature of note feature

00:11:56,240 --> 00:12:01,279
discovery that allows me to select

00:11:58,240 --> 00:12:04,240
loaded kernel modules i2c dev

00:12:01,279 --> 00:12:04,880
and this other one if both of those

00:12:04,240 --> 00:12:08,320
exist

00:12:04,880 --> 00:12:10,959
it will label for i2c

00:12:08,320 --> 00:12:11,519
so we're going to get the labels again

00:12:10,959 --> 00:12:14,560
and

00:12:11,519 --> 00:12:17,440
we'll pick up which ones have

00:12:14,560 --> 00:12:18,240
i2c on them i'm going to take a look

00:12:17,440 --> 00:12:22,000
this time

00:12:18,240 --> 00:12:25,040
at you can see i'm grepping for

00:12:22,000 --> 00:12:27,120
both usb and i2c

00:12:25,040 --> 00:12:28,399
so we're going to look at all the nodes

00:12:27,120 --> 00:12:31,839
and just

00:12:28,399 --> 00:12:33,600
ignore everything except for i2c and usb

00:12:31,839 --> 00:12:36,000
and what we should see is that every

00:12:33,600 --> 00:12:39,600
node has i2c because

00:12:36,000 --> 00:12:40,800
as i said every node is connected to

00:12:39,600 --> 00:12:43,120
that bus

00:12:40,800 --> 00:12:44,079
um and so every node has those kernel

00:12:43,120 --> 00:12:46,079
modules

00:12:44,079 --> 00:12:48,720
and therefore the labels get applied

00:12:46,079 --> 00:12:49,839
everywhere plus we still have on node

00:12:48,720 --> 00:12:54,560
number three

00:12:49,839 --> 00:12:58,399
we've still got uh the that usb hub

00:12:54,560 --> 00:13:01,440
that usb human interface device great

00:12:58,399 --> 00:13:03,680
okay um last one so

00:13:01,440 --> 00:13:04,639
this one we're going to do gpio and for

00:13:03,680 --> 00:13:07,440
this i'm going to log

00:13:04,639 --> 00:13:10,639
in to the first node so to i'm going to

00:13:07,440 --> 00:13:13,200
log into touringpi001

00:13:10,639 --> 00:13:14,079
and the first thing i want to do is

00:13:13,200 --> 00:13:17,120
create

00:13:14,079 --> 00:13:17,600
a directory there is a directory that

00:13:17,120 --> 00:13:21,279
gets

00:13:17,600 --> 00:13:23,920
mapped to my container and that

00:13:21,279 --> 00:13:25,360
and and it's the node feature discovery

00:13:23,920 --> 00:13:28,959
worker container

00:13:25,360 --> 00:13:31,360
this directory hosts um

00:13:28,959 --> 00:13:32,160
any executable it could be any

00:13:31,360 --> 00:13:35,600
executable

00:13:32,160 --> 00:13:38,639
or a shell script and essentially

00:13:35,600 --> 00:13:39,440
the the output of whatever runs in that

00:13:38,639 --> 00:13:42,320
directory

00:13:39,440 --> 00:13:42,880
needs to be a list of labels so what i'm

00:13:42,320 --> 00:13:46,880
going to do

00:13:42,880 --> 00:13:50,560
is create just a short list of labels in

00:13:46,880 --> 00:13:50,560
a shell script

00:13:50,720 --> 00:13:55,360
those labels will then get applied to

00:13:53,440 --> 00:13:58,079
this node

00:13:55,360 --> 00:13:59,680
now i happen to know that each of these

00:13:58,079 --> 00:14:02,959
nodes has gpio

00:13:59,680 --> 00:14:06,320
but i'm only labeling touring pi01

00:14:02,959 --> 00:14:08,480
as having gpio and i'm saying that on

00:14:06,320 --> 00:14:10,959
pin number 40 it's an input pin

00:14:08,480 --> 00:14:13,040
and you can imagine that there might be

00:14:10,959 --> 00:14:17,040
other labels i want to apply

00:14:13,040 --> 00:14:20,639
for specific needs for each node

00:14:17,040 --> 00:14:23,279
so here's the file i will take this file

00:14:20,639 --> 00:14:24,639
move it into this directory that i

00:14:23,279 --> 00:14:27,600
created

00:14:24,639 --> 00:14:29,199
node feature discovery will notice the

00:14:27,600 --> 00:14:32,560
directory the change

00:14:29,199 --> 00:14:36,240
in the directory it will

00:14:32,560 --> 00:14:39,519
reload it its configuration file

00:14:36,240 --> 00:14:39,519
and it will

00:14:39,920 --> 00:14:47,360
begin to label the node that has

00:14:43,360 --> 00:14:49,839
gpio so i'm just changing this script

00:14:47,360 --> 00:14:52,000
to be executable we'll exit out of

00:14:49,839 --> 00:14:54,079
turing pi01

00:14:52,000 --> 00:14:56,079
node feature discovery is already doing

00:14:54,079 --> 00:14:59,120
its thing

00:14:56,079 --> 00:15:02,160
so now we'll get the labels again

00:14:59,120 --> 00:15:04,480
this time after i get the labels i'm

00:15:02,160 --> 00:15:07,600
going to

00:15:04,480 --> 00:15:09,760
take a look at the list of all nodes

00:15:07,600 --> 00:15:12,160
we'll look at all nodes we will

00:15:09,760 --> 00:15:15,440
exclusively look at

00:15:12,160 --> 00:15:19,600
usb i2c and

00:15:15,440 --> 00:15:20,000
also gpio whoops let's let's add gpio

00:15:19,600 --> 00:15:23,600
there

00:15:20,000 --> 00:15:27,440
as well so

00:15:23,600 --> 00:15:30,160
there we go usb i2c gpio

00:15:27,440 --> 00:15:30,560
and what you can see here is that i now

00:15:30,160 --> 00:15:34,240
have

00:15:30,560 --> 00:15:38,160
on touring pi01

00:15:34,240 --> 00:15:41,920
i have my labels for gpio

00:15:38,160 --> 00:15:46,079
i have my i2c label and then on

00:15:41,920 --> 00:15:48,320
node 3 i have my usb and my i2c label

00:15:46,079 --> 00:15:55,839
everything's got i2c

00:15:48,320 --> 00:15:55,839
and we're good to go

00:15:57,360 --> 00:16:02,639
so that's it for my demo

00:16:00,399 --> 00:16:04,480
very simply the node feature discovery

00:16:02,639 --> 00:16:07,120
worker is a daemon set

00:16:04,480 --> 00:16:09,199
and it scans the nodes for capabilities

00:16:07,120 --> 00:16:11,839
based on configuration

00:16:09,199 --> 00:16:13,519
it reports what it finds to the node

00:16:11,839 --> 00:16:15,839
feature discovery master

00:16:13,519 --> 00:16:16,720
the master communicates with kubernetes

00:16:15,839 --> 00:16:19,120
api

00:16:16,720 --> 00:16:20,000
and is is the component that actually

00:16:19,120 --> 00:16:23,839
does the

00:16:20,000 --> 00:16:23,839
labeling of the nodes

00:16:24,240 --> 00:16:28,240
from a human perspective the operations

00:16:27,839 --> 00:16:31,199
team

00:16:28,240 --> 00:16:32,880
of course needs to know their devices

00:16:31,199 --> 00:16:36,079
and the features that they

00:16:32,880 --> 00:16:39,279
want to expose they need to create a

00:16:36,079 --> 00:16:41,360
configuration to expose those resources

00:16:39,279 --> 00:16:44,160
node feature discovery does the rest as

00:16:41,360 --> 00:16:44,160
i just showed you

00:16:44,880 --> 00:16:49,199
how do we install it i installed it

00:16:46,800 --> 00:16:51,199
using kubernetes resource configs

00:16:49,199 --> 00:16:52,639
you could install it as a helm chart you

00:16:51,199 --> 00:16:56,240
can install it

00:16:52,639 --> 00:16:56,240
as an operator

00:16:56,880 --> 00:17:01,279
and when the underlying config map

00:16:58,800 --> 00:17:02,160
changes the worker reloads it the config

00:17:01,279 --> 00:17:04,959
map is

00:17:02,160 --> 00:17:06,480
that default config map with that was

00:17:04,959 --> 00:17:08,799
commented out

00:17:06,480 --> 00:17:10,160
that gets installed automatically with

00:17:08,799 --> 00:17:12,720
node feature discovery

00:17:10,160 --> 00:17:13,839
there's a healthy bit of documentation

00:17:12,720 --> 00:17:16,880
on

00:17:13,839 --> 00:17:18,079
how to use that configuration file and

00:17:16,880 --> 00:17:21,280
i'll share a link with you

00:17:18,079 --> 00:17:23,360
at the end on where you can go look at

00:17:21,280 --> 00:17:23,360
it

00:17:23,760 --> 00:17:28,240
this is a snippet of the config config

00:17:26,400 --> 00:17:30,640
map that i used

00:17:28,240 --> 00:17:31,360
on the right side you can see at the

00:17:30,640 --> 00:17:35,840
bottom

00:17:31,360 --> 00:17:38,400
my usb usb device class

00:17:35,840 --> 00:17:41,200
zero three right that was the human

00:17:38,400 --> 00:17:44,240
interface device

00:17:41,200 --> 00:17:46,480
this is pretty simple i run node feature

00:17:44,240 --> 00:17:50,240
discovery with my unique configuration

00:17:46,480 --> 00:17:52,799
and labeling happens i did find

00:17:50,240 --> 00:17:54,640
that there is a non-trivial issue for

00:17:52,799 --> 00:17:57,039
use at the edge

00:17:54,640 --> 00:17:57,679
at the edge we're managing not one

00:17:57,039 --> 00:18:00,480
single

00:17:57,679 --> 00:18:01,280
massive cluster of nodes like what i

00:18:00,480 --> 00:18:04,240
showed you

00:18:01,280 --> 00:18:05,120
that's that's actually that might be a

00:18:04,240 --> 00:18:07,520
cluster

00:18:05,120 --> 00:18:10,160
at the edge but we're really looking at

00:18:07,520 --> 00:18:12,799
a multitude of smaller clusters

00:18:10,160 --> 00:18:14,160
we're also often adding removing and

00:18:12,799 --> 00:18:17,600
changing the resources

00:18:14,160 --> 00:18:17,600
on these clusters over time

00:18:18,000 --> 00:18:24,240
so how can nfd be managed across

00:18:21,520 --> 00:18:26,559
hundreds or thousands of clusters

00:18:24,240 --> 00:18:28,000
to answer that question let's look at

00:18:26,559 --> 00:18:32,400
some architectures

00:18:28,000 --> 00:18:35,760
as we can to as we consider the problem

00:18:32,400 --> 00:18:37,200
so this is a typical uh

00:18:35,760 --> 00:18:38,880
data center and cloud deployment of

00:18:37,200 --> 00:18:41,679
kubernetes

00:18:38,880 --> 00:18:43,919
the it's pretty much app life cycle

00:18:41,679 --> 00:18:47,120
focused

00:18:43,919 --> 00:18:51,280
with one kubernetes cluster per

00:18:47,120 --> 00:18:53,440
development environment red lines

00:18:51,280 --> 00:18:54,880
in this image represent the cluster

00:18:53,440 --> 00:18:56,160
boundary so you can see i have four

00:18:54,880 --> 00:18:58,640
clusters

00:18:56,160 --> 00:19:00,320
and my little touring pi cluster

00:18:58,640 --> 00:19:03,440
represents something akin to

00:19:00,320 --> 00:19:06,240
just one of these environments so

00:19:03,440 --> 00:19:07,600
i showed how to manage the differences

00:19:06,240 --> 00:19:10,640
on six nodes

00:19:07,600 --> 00:19:13,600
with changes to a single

00:19:10,640 --> 00:19:14,640
node feature discovery configuration but

00:19:13,600 --> 00:19:17,840
in fact

00:19:14,640 --> 00:19:20,000
my edge looks more like this where i

00:19:17,840 --> 00:19:23,280
have lots of single node clusters

00:19:20,000 --> 00:19:25,760
some multi-node clusters each cluster

00:19:23,280 --> 00:19:27,200
is production at the edge it's all

00:19:25,760 --> 00:19:30,000
production

00:19:27,200 --> 00:19:31,760
um also in most case most of the edge

00:19:30,000 --> 00:19:33,200
scenarios that i've come across we're

00:19:31,760 --> 00:19:36,080
talking easily

00:19:33,200 --> 00:19:37,280
um 10 or 100 times the number of

00:19:36,080 --> 00:19:40,559
clusters shown here

00:19:37,280 --> 00:19:44,120
i have accounted i have 32 clusters here

00:19:40,559 --> 00:19:47,200
so should we play go fish with

00:19:44,120 --> 00:19:49,200
320 or 32 000

00:19:47,200 --> 00:19:50,799
clusters now i don't think we want to

00:19:49,200 --> 00:19:53,039
play go fish right

00:19:50,799 --> 00:19:54,160
we want to use node feature discovery

00:19:53,039 --> 00:19:56,640
how can we do it

00:19:54,160 --> 00:19:58,880
with this many clusters given how node

00:19:56,640 --> 00:20:02,320
feature discovery works

00:19:58,880 --> 00:20:06,480
i think that git ops is

00:20:02,320 --> 00:20:09,760
a solution for this problem

00:20:06,480 --> 00:20:10,159
one of of these three tools um i'm

00:20:09,760 --> 00:20:13,120
really

00:20:10,159 --> 00:20:14,000
only familiar with fleet however i know

00:20:13,120 --> 00:20:17,039
that argo

00:20:14,000 --> 00:20:20,400
cd and flux are also being used

00:20:17,039 --> 00:20:23,679
as git ops tooling there are others

00:20:20,400 --> 00:20:26,640
azure devops and and

00:20:23,679 --> 00:20:28,880
you know there's a bunch of options we

00:20:26,640 --> 00:20:32,000
need some outside tooling to manage

00:20:28,880 --> 00:20:34,240
large numbers of clusters in fleet it's

00:20:32,000 --> 00:20:37,360
possible to manage large numbers

00:20:34,240 --> 00:20:39,840
of clusters and target the

00:20:37,360 --> 00:20:41,600
workloads but also the custom

00:20:39,840 --> 00:20:42,640
configurations because these are really

00:20:41,600 --> 00:20:45,360
just

00:20:42,640 --> 00:20:47,360
kubernetes objects and so we can we can

00:20:45,360 --> 00:20:49,280
use fleet to actually manage those

00:20:47,360 --> 00:20:51,919
configurations

00:20:49,280 --> 00:20:54,559
fleet has been tested with a million

00:20:51,919 --> 00:20:57,200
clusters for example

00:20:54,559 --> 00:20:59,679
now despite the large scale even though

00:20:57,200 --> 00:21:02,400
it can handle that large scale

00:20:59,679 --> 00:21:04,799
fleet doesn't have some idea of an auto

00:21:02,400 --> 00:21:07,520
cluster labeler at this time so

00:21:04,799 --> 00:21:08,799
that's a feature that i will be

00:21:07,520 --> 00:21:12,880
requesting

00:21:08,799 --> 00:21:14,960
the concept of cluster is

00:21:12,880 --> 00:21:16,000
it's a custom resource definition in

00:21:14,960 --> 00:21:18,000
fleet

00:21:16,000 --> 00:21:19,679
because it's managing multiple clusters

00:21:18,000 --> 00:21:22,720
right but

00:21:19,679 --> 00:21:26,880
it's not a concept in kubernetes now

00:21:22,720 --> 00:21:29,280
fortunately there is a kubernetes

00:21:26,880 --> 00:21:30,240
special interest group called cluster

00:21:29,280 --> 00:21:32,000
api

00:21:30,240 --> 00:21:34,799
and i think that the solution they're

00:21:32,000 --> 00:21:38,320
working on there may hold the answer to

00:21:34,799 --> 00:21:41,360
how we can get our devops tools to

00:21:38,320 --> 00:21:44,799
have a standardized model

00:21:41,360 --> 00:21:46,880
for cluster labeling based on node

00:21:44,799 --> 00:21:49,919
labels

00:21:46,880 --> 00:21:49,919
down downstream

00:21:52,000 --> 00:21:55,360
this is what i think the gitops pipeline

00:21:53,760 --> 00:21:56,799
would look like so assuming we can get

00:21:55,360 --> 00:21:59,520
this working

00:21:56,799 --> 00:22:01,679
simply install node feature discovery

00:21:59,520 --> 00:22:03,520
into the pipeline itself

00:22:01,679 --> 00:22:05,679
make sure your config is set up either

00:22:03,520 --> 00:22:08,080
do that as part of the install

00:22:05,679 --> 00:22:09,760
or you know of course you can do it

00:22:08,080 --> 00:22:11,440
afterwards as well and you may end up

00:22:09,760 --> 00:22:14,080
doing it multiple times

00:22:11,440 --> 00:22:15,840
that is probably the aside from

00:22:14,080 --> 00:22:17,919
upgrading node feature discovery

00:22:15,840 --> 00:22:19,440
the config file is the changeable

00:22:17,919 --> 00:22:22,159
component

00:22:19,440 --> 00:22:24,320
i would then configure my tool in my

00:22:22,159 --> 00:22:25,520
case fleet to automatically update node

00:22:24,320 --> 00:22:28,720
labels

00:22:25,520 --> 00:22:32,000
and eventually

00:22:28,720 --> 00:22:33,760
i need to notify developers of what

00:22:32,000 --> 00:22:37,600
node labels are going to be available

00:22:33,760 --> 00:22:37,600
and what resources they map to

00:22:37,840 --> 00:22:41,840
rinse and repeat as resources change we

00:22:40,159 --> 00:22:44,400
repeat steps two through four

00:22:41,840 --> 00:22:44,400
as needed

00:22:46,400 --> 00:22:52,840
so that's a bit of hypothetical

00:22:49,919 --> 00:22:54,240
get ops i did want to also talk about

00:22:52,840 --> 00:22:56,559
security

00:22:54,240 --> 00:22:58,799
with node feature discovery it is

00:22:56,559 --> 00:22:59,360
currently possible to configure tls

00:22:58,799 --> 00:23:01,200
between

00:22:59,360 --> 00:23:03,120
the node feature discovery services

00:23:01,200 --> 00:23:05,280
between the master and the worker

00:23:03,120 --> 00:23:06,799
so that's out of the box that's a nice

00:23:05,280 --> 00:23:11,520
security enhancement

00:23:06,799 --> 00:23:11,520
that i have not yet enabled i

00:23:11,600 --> 00:23:18,320
as i was doing this i believe

00:23:14,799 --> 00:23:20,799
that allowing that local user specific

00:23:18,320 --> 00:23:23,840
capability where i

00:23:20,799 --> 00:23:25,600
put a script into a directory and let it

00:23:23,840 --> 00:23:27,840
execute that directory i believe there's

00:23:25,600 --> 00:23:29,600
some risk associated with that

00:23:27,840 --> 00:23:31,360
because it will execute anything in that

00:23:29,600 --> 00:23:36,159
directory now

00:23:31,360 --> 00:23:38,799
it only executes it in in the container

00:23:36,159 --> 00:23:40,480
however these containers are somewhat

00:23:38,799 --> 00:23:41,600
privileged because they're accessing

00:23:40,480 --> 00:23:44,080
resources

00:23:41,600 --> 00:23:44,880
they're at least need read access to the

00:23:44,080 --> 00:23:46,400
resources

00:23:44,880 --> 00:23:49,200
i'm sure there's a way to lock them down

00:23:46,400 --> 00:23:51,600
in kubernetes that would need to be

00:23:49,200 --> 00:23:54,000
managed in order for this to be more

00:23:51,600 --> 00:23:54,000
secure

00:23:54,720 --> 00:23:58,000
in addition it's possible probably to

00:23:57,200 --> 00:24:00,400
reduce the

00:23:58,000 --> 00:24:03,200
attack surface by limiting what features

00:24:00,400 --> 00:24:06,080
are usable in the configuration file

00:24:03,200 --> 00:24:07,279
in that config file there is a section

00:24:06,080 --> 00:24:10,400
called sources

00:24:07,279 --> 00:24:11,200
and we can do things like specify just

00:24:10,400 --> 00:24:14,240
usb

00:24:11,200 --> 00:24:17,360
just custom leave out local or you know

00:24:14,240 --> 00:24:19,279
only include local on the machines the

00:24:17,360 --> 00:24:21,520
the nodes that need it that sort of

00:24:19,279 --> 00:24:24,559
thing so that's a little bit about

00:24:21,520 --> 00:24:27,840
security this is

00:24:24,559 --> 00:24:30,480
a summary of what we just went through

00:24:27,840 --> 00:24:32,320
labels are applied to nodes in a cluster

00:24:30,480 --> 00:24:35,200
i think git ops will be useful

00:24:32,320 --> 00:24:35,919
in labeling large numbers of clusters

00:24:35,200 --> 00:24:40,080
features

00:24:35,919 --> 00:24:42,799
are unlocked using a configuration file

00:24:40,080 --> 00:24:43,360
node feature discovery has a limited

00:24:42,799 --> 00:24:46,320
number of

00:24:43,360 --> 00:24:46,960
out-of-the-box feature types which does

00:24:46,320 --> 00:24:50,159
include

00:24:46,960 --> 00:24:52,640
a kernel module discovery however

00:24:50,159 --> 00:24:53,200
when if if there are types that aren't

00:24:52,640 --> 00:24:56,320
matched

00:24:53,200 --> 00:24:56,640
you can use the local type when there's

00:24:56,320 --> 00:25:00,880
no

00:24:56,640 --> 00:25:04,000
other default available

00:25:00,880 --> 00:25:06,559
at some point it might make sense to

00:25:04,000 --> 00:25:07,279
use kubernetes device plug-ins instead

00:25:06,559 --> 00:25:10,240
of the

00:25:07,279 --> 00:25:11,679
local feature type that will depend on

00:25:10,240 --> 00:25:14,480
what's available

00:25:11,679 --> 00:25:16,640
um and what you know if if there is an

00:25:14,480 --> 00:25:18,480
existing device plug-in

00:25:16,640 --> 00:25:20,080
and it doesn't and it's not something

00:25:18,480 --> 00:25:21,200
that can be discovered with node feature

00:25:20,080 --> 00:25:23,200
discovery

00:25:21,200 --> 00:25:24,640
it may make sense to use the device

00:25:23,200 --> 00:25:27,840
plug-in instead

00:25:24,640 --> 00:25:30,000
that it has to be seen on a case-by-case

00:25:27,840 --> 00:25:32,320
basis

00:25:30,000 --> 00:25:33,039
some more summary items i didn't show

00:25:32,320 --> 00:25:36,159
this

00:25:33,039 --> 00:25:38,960
but there is a mount in the container

00:25:36,159 --> 00:25:41,360
for uniquely named configuration files

00:25:38,960 --> 00:25:42,080
instead of the default nfd worker conf

00:25:41,360 --> 00:25:43,840
so

00:25:42,080 --> 00:25:45,360
i was modifying the default

00:25:43,840 --> 00:25:48,080
configuration every time

00:25:45,360 --> 00:25:50,480
it is possible to take snippets of that

00:25:48,080 --> 00:25:53,760
put them in uniquely named files

00:25:50,480 --> 00:25:56,400
and provide a configuration through

00:25:53,760 --> 00:25:57,520
this other directory the nice thing

00:25:56,400 --> 00:26:00,720
about this i think

00:25:57,520 --> 00:26:03,760
is that it provides

00:26:00,720 --> 00:26:07,200
a capability that i think vendors

00:26:03,760 --> 00:26:07,919
should be promoting right if i have a

00:26:07,200 --> 00:26:10,640
device

00:26:07,919 --> 00:26:11,360
i should be enabling node feature

00:26:10,640 --> 00:26:14,480
discovery

00:26:11,360 --> 00:26:17,760
for those devices i know that

00:26:14,480 --> 00:26:18,400
currently nvidia for example well they

00:26:17,760 --> 00:26:20,400
don't

00:26:18,400 --> 00:26:21,679
i'm sure the config file is available

00:26:20,400 --> 00:26:24,159
but essentially

00:26:21,679 --> 00:26:25,039
their gpu operator use this node feature

00:26:24,159 --> 00:26:27,840
discovery

00:26:25,039 --> 00:26:28,640
to label nodes it would be as simple as

00:26:27,840 --> 00:26:31,200
finding that

00:26:28,640 --> 00:26:33,120
configuration that they use and you

00:26:31,200 --> 00:26:35,919
could label your own gpu nodes

00:26:33,120 --> 00:26:37,840
without the gpu operator there is some

00:26:35,919 --> 00:26:39,200
value to the gpu operator so i'm not

00:26:37,840 --> 00:26:42,559
saying don't use that

00:26:39,200 --> 00:26:44,320
but it's you know it's uh

00:26:42,559 --> 00:26:46,480
one of the one of the things that i

00:26:44,320 --> 00:26:49,200
think we should be looking at is getting

00:26:46,480 --> 00:26:52,320
vendors to support this

00:26:49,200 --> 00:26:55,360
also in the beginning i said nfd is just

00:26:52,320 --> 00:26:57,520
a labeler it's more than that it's a

00:26:55,360 --> 00:26:58,159
dynamic labeler it's a configurable

00:26:57,520 --> 00:27:01,039
label

00:26:58,159 --> 00:27:02,000
labeler nfd is very powerful and

00:27:01,039 --> 00:27:05,679
especially

00:27:02,000 --> 00:27:06,159
at the edge the last thing i want to get

00:27:05,679 --> 00:27:09,840
to

00:27:06,159 --> 00:27:13,120
is problems the pitfalls so

00:27:09,840 --> 00:27:15,760
i did discover that when

00:27:13,120 --> 00:27:16,640
a kernel module is loaded it doesn't

00:27:15,760 --> 00:27:19,760
mean

00:27:16,640 --> 00:27:21,520
that there's a device on the other end i

00:27:19,760 --> 00:27:24,720
have for example

00:27:21,520 --> 00:27:27,520
a node that has the i2c capability but

00:27:24,720 --> 00:27:31,200
when i physically unplug the device

00:27:27,520 --> 00:27:32,320
the the i2c slave that's on the other

00:27:31,200 --> 00:27:36,399
end

00:27:32,320 --> 00:27:39,919
there's it's not usable right so

00:27:36,399 --> 00:27:42,799
just loading the module alone isn't

00:27:39,919 --> 00:27:43,279
significant it's not enough to ensure

00:27:42,799 --> 00:27:46,320
that

00:27:43,279 --> 00:27:49,840
the feature exists there

00:27:46,320 --> 00:27:52,720
may be some

00:27:49,840 --> 00:27:53,679
symbiotic relationship between using the

00:27:52,720 --> 00:27:57,919
local feature

00:27:53,679 --> 00:28:00,960
to automate labeling and being able to

00:27:57,919 --> 00:28:03,200
use kubernetes to discover

00:28:00,960 --> 00:28:05,200
if the physical feature is actually

00:28:03,200 --> 00:28:05,760
there like some sort of a scout or a

00:28:05,200 --> 00:28:08,240
test

00:28:05,760 --> 00:28:09,039
of whether that physical item is there

00:28:08,240 --> 00:28:10,880
and if it is

00:28:09,039 --> 00:28:12,240
add another label through the local

00:28:10,880 --> 00:28:16,000
capability

00:28:12,240 --> 00:28:18,799
um or use health checks

00:28:16,000 --> 00:28:19,760
we have to look at how can we ensure

00:28:18,799 --> 00:28:22,559
that a device

00:28:19,760 --> 00:28:23,840
that can be plugged and unplugged is

00:28:22,559 --> 00:28:26,720
actually there

00:28:23,840 --> 00:28:27,360
just because the kernel or the device

00:28:26,720 --> 00:28:31,840
driver

00:28:27,360 --> 00:28:31,840
exists doesn't mean it's actually there

00:28:32,000 --> 00:28:35,120
what else so um

00:28:33,530 --> 00:28:38,000
[Music]

00:28:35,120 --> 00:28:39,919
removable devices so hot plugging works

00:28:38,000 --> 00:28:40,960
but it's not immediate in the demo i

00:28:39,919 --> 00:28:43,440
showed you

00:28:40,960 --> 00:28:44,399
i did it it looked like it was happening

00:28:43,440 --> 00:28:46,480
right away

00:28:44,399 --> 00:28:48,320
there was some time lag between when i

00:28:46,480 --> 00:28:50,480
unplugged it and replugged it

00:28:48,320 --> 00:28:52,480
a fair amount of time node feature

00:28:50,480 --> 00:28:55,120
discovery needs time

00:28:52,480 --> 00:28:55,840
to go through the kubernetes controller

00:28:55,120 --> 00:28:58,559
process

00:28:55,840 --> 00:28:59,840
of checking is the state what i think it

00:28:58,559 --> 00:29:03,120
should be no it's not

00:28:59,840 --> 00:29:05,200
change it once it changes the labels

00:29:03,120 --> 00:29:06,399
there's going to also be another process

00:29:05,200 --> 00:29:09,360
where kubernetes

00:29:06,399 --> 00:29:10,960
says oh this workload can't run on this

00:29:09,360 --> 00:29:13,279
node it doesn't match up

00:29:10,960 --> 00:29:14,320
i need to remove it and find a new a new

00:29:13,279 --> 00:29:15,760
resource

00:29:14,320 --> 00:29:17,440
that it can you know i need to

00:29:15,760 --> 00:29:19,520
reschedule it somewhere else

00:29:17,440 --> 00:29:22,320
so these are things to watch out for as

00:29:19,520 --> 00:29:24,240
you're using node feature discovery

00:29:22,320 --> 00:29:26,000
as i said i would share with you some

00:29:24,240 --> 00:29:28,720
links these first two links are

00:29:26,000 --> 00:29:29,360
the node feature discovery page there's

00:29:28,720 --> 00:29:31,200
there's a

00:29:29,360 --> 00:29:32,960
ton there i just called out these two

00:29:31,200 --> 00:29:34,720
links the get started

00:29:32,960 --> 00:29:36,240
and the features page which has all the

00:29:34,720 --> 00:29:37,760
list of features and how you configure

00:29:36,240 --> 00:29:41,360
that file

00:29:37,760 --> 00:29:42,399
i used the find class codes at usb.org

00:29:41,360 --> 00:29:45,600
to find

00:29:42,399 --> 00:29:47,520
my human interface device class and

00:29:45,600 --> 00:29:50,640
and some of the functionality that i

00:29:47,520 --> 00:29:51,760
used there i used k3s as my kubernetes

00:29:50,640 --> 00:29:53,360
distribution

00:29:51,760 --> 00:29:55,760
and you can find this entire

00:29:53,360 --> 00:29:56,480
presentation and the work that i've done

00:29:55,760 --> 00:30:01,440
here

00:29:56,480 --> 00:30:01,440
at maker slash nfd dash demo

00:30:02,559 --> 00:30:06,559
my name is mark abrams i contribute to

00:30:04,880 --> 00:30:09,840
open source and you can too

00:30:06,559 --> 00:30:09,840

YouTube URL: https://www.youtube.com/watch?v=Os8BgS2WbzE


