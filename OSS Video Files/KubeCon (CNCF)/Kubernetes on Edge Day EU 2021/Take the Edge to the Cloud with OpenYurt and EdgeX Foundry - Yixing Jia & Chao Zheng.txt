Title: Take the Edge to the Cloud with OpenYurt and EdgeX Foundry - Yixing Jia & Chao Zheng
Publication date: 2021-05-05
Playlist: Kubernetes on Edge Day EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Take the Edge to the Cloud with OpenYurt and EdgeX Foundry - Yixing Jia, VMware & Chao Zheng, Alibaba Cloud

According to the application-container market report, Edge applications can benefit a lot from using the application container model. Kubernetes(K8S) has been proven effective in managing containers on the cloud. However, the edge applications are usually fragmented, i.e., various protocols, hardware, etc., making it challenging to use K8S in the edge environment. As a plug-and-play Open Source IoT Edge platform, EdgeX Foundry(EdgeX) allows devices belonging to different manufacturers with different protocols to be connected. On the other hand, OpenYurt extends the native K8S to the edge using only plugins, which supports all upstream K8S features. In this talk, we will present our edge-cloud solution that integrates EdgeX into OpenYurt. Our solution leverages EdgeX’s ability to manage a rich collection of edge devices and uses OpenYurt to manage servers in the edge environment.
Captions: 
	00:00:00,000 --> 00:00:05,680
hello everyone i am charles software

00:00:03,280 --> 00:00:08,240
engineer at alibaba cloud

00:00:05,680 --> 00:00:09,920
today we are going to present our work

00:00:08,240 --> 00:00:13,280
take the edge to the cloud

00:00:09,920 --> 00:00:16,000
with open yard and the ajax foundry this

00:00:13,280 --> 00:00:16,560
is a cooperation project between alibaba

00:00:16,000 --> 00:00:21,039
cloud

00:00:16,560 --> 00:00:22,960
and the vmware this talk will be divided

00:00:21,039 --> 00:00:25,439
into four sections

00:00:22,960 --> 00:00:26,240
first i will introduce the four main

00:00:25,439 --> 00:00:28,720
challenges

00:00:26,240 --> 00:00:29,760
of applying kubernetes to edge computing

00:00:28,720 --> 00:00:32,079
environment

00:00:29,760 --> 00:00:34,000
and how open yard address these four

00:00:32,079 --> 00:00:36,239
challenges

00:00:34,000 --> 00:00:38,399
then eachine will provide the necessary

00:00:36,239 --> 00:00:41,120
backgrounds of ajax foundry

00:00:38,399 --> 00:00:42,640
and how we combine open yard with ajax

00:00:41,120 --> 00:00:44,960
foundry

00:00:42,640 --> 00:00:47,680
and finally we will wrap up the talk

00:00:44,960 --> 00:00:47,680
with a demo

00:00:48,559 --> 00:00:53,440
so why we need to use open yard and why

00:00:51,280 --> 00:00:56,000
we cannot use kubernetes in edge

00:00:53,440 --> 00:00:57,920
computing environment directly

00:00:56,000 --> 00:00:59,359
to answer this question we have to

00:00:57,920 --> 00:01:01,039
understand that

00:00:59,359 --> 00:01:02,480
applying kubernetes to the edge

00:01:01,039 --> 00:01:06,000
computing environment

00:01:02,480 --> 00:01:09,040
requiring us to overcome four challenges

00:01:06,000 --> 00:01:10,320
first how can we prevent the parts from

00:01:09,040 --> 00:01:12,159
being evicted

00:01:10,320 --> 00:01:15,680
if edge nodes are temporarily

00:01:12,159 --> 00:01:18,960
disconnected from the api server

00:01:15,680 --> 00:01:21,680
second how can api servers send requests

00:01:18,960 --> 00:01:22,000
to an edge node if they are located in

00:01:21,680 --> 00:01:25,200
two

00:01:22,000 --> 00:01:27,840
isolated networks

00:01:25,200 --> 00:01:28,400
third how can we deploy workload by

00:01:27,840 --> 00:01:30,960
region

00:01:28,400 --> 00:01:33,680
as system settings or device models in

00:01:30,960 --> 00:01:36,720
different regions may vary

00:01:33,680 --> 00:01:39,200
four how can we manage edge device

00:01:36,720 --> 00:01:41,520
with different protocols in a kubernetes

00:01:39,200 --> 00:01:43,680
native way

00:01:41,520 --> 00:01:45,439
we develop open yard to address these

00:01:43,680 --> 00:01:48,399
four challenges

00:01:45,439 --> 00:01:51,439
the design principle of open yard is

00:01:48,399 --> 00:01:53,840
extend the native kubernetes to the edge

00:01:51,439 --> 00:01:54,720
therefore we won't change the system

00:01:53,840 --> 00:01:58,640
architecture

00:01:54,720 --> 00:02:01,200
or any core components of the kubernetes

00:01:58,640 --> 00:02:03,280
in the other words we will tackle all

00:02:01,200 --> 00:02:05,439
the four challenges

00:02:03,280 --> 00:02:07,119
only using the native kubernetes

00:02:05,439 --> 00:02:09,679
workloads

00:02:07,119 --> 00:02:10,560
in addition we always keep the goal in

00:02:09,679 --> 00:02:12,480
mind that

00:02:10,560 --> 00:02:14,959
we have to support all upstream

00:02:12,480 --> 00:02:17,280
kubernetes features

00:02:14,959 --> 00:02:18,400
here is the system architecture of open

00:02:17,280 --> 00:02:21,040
yard

00:02:18,400 --> 00:02:22,000
as we can see to have an open yard

00:02:21,040 --> 00:02:24,400
cluster up and

00:02:22,000 --> 00:02:26,319
running we only need to deploy several

00:02:24,400 --> 00:02:28,640
controllers on cloud nodes

00:02:26,319 --> 00:02:29,840
and on each edge node we only need to

00:02:28,640 --> 00:02:33,200
deploy yard harp

00:02:29,840 --> 00:02:36,480
and the yard tunnel agent in addition

00:02:33,200 --> 00:02:38,400
we provide a command line tool yard ctl

00:02:36,480 --> 00:02:39,519
to help us to manage an open yard

00:02:38,400 --> 00:02:42,879
cluster

00:02:39,519 --> 00:02:44,000
by simply running iota convert all the

00:02:42,879 --> 00:02:47,599
above components

00:02:44,000 --> 00:02:49,519
will be automatically deployed next

00:02:47,599 --> 00:02:51,760
i will introduce each of these

00:02:49,519 --> 00:02:55,440
components and how they help us

00:02:51,760 --> 00:02:55,440
to tackle the four challenges

00:02:56,879 --> 00:03:01,040
the first problem we try to solve is how

00:02:59,519 --> 00:03:03,120
can we prevent parts

00:03:01,040 --> 00:03:04,879
from being evicted when a network

00:03:03,120 --> 00:03:08,000
connection between an edge node

00:03:04,879 --> 00:03:10,080
and an api server has been cut off

00:03:08,000 --> 00:03:12,239
in an edge computing environment the

00:03:10,080 --> 00:03:15,680
network connection between edge nodes

00:03:12,239 --> 00:03:18,159
and api server can be unstable however

00:03:15,680 --> 00:03:20,720
kubernetes require the api server to be

00:03:18,159 --> 00:03:23,120
tidally connected to worker nodes

00:03:20,720 --> 00:03:24,560
if a worker node is disconnected from

00:03:23,120 --> 00:03:26,799
the api server

00:03:24,560 --> 00:03:28,560
or the api server misses several

00:03:26,799 --> 00:03:31,440
heartbeats from the worker node

00:03:28,560 --> 00:03:32,560
due to poor network connection the node

00:03:31,440 --> 00:03:35,040
will be marked as

00:03:32,560 --> 00:03:36,959
unready and the parts running on the

00:03:35,040 --> 00:03:40,640
node will be erased from

00:03:36,959 --> 00:03:42,080
atcd later when a worker node reconnects

00:03:40,640 --> 00:03:44,560
to the api server

00:03:42,080 --> 00:03:45,680
the kubelet will notice that parts

00:03:44,560 --> 00:03:48,640
running on it

00:03:45,680 --> 00:03:50,879
are not in the etcd then it will

00:03:48,640 --> 00:03:52,400
physically evict the parts

00:03:50,879 --> 00:03:54,159
this is something we don't want it to

00:03:52,400 --> 00:03:57,120
happen

00:03:54,159 --> 00:03:58,400
to avoid this from happening we develop

00:03:57,120 --> 00:04:00,959
yacht hub

00:03:58,400 --> 00:04:02,400
yacht hub functions as a reverse proxy

00:04:00,959 --> 00:04:05,439
and local cache

00:04:02,400 --> 00:04:08,159
that is deployed on each edge node

00:04:05,439 --> 00:04:08,720
after it is up and running we will

00:04:08,159 --> 00:04:11,120
connect

00:04:08,720 --> 00:04:13,120
the kubelets to the yacht hub instead of

00:04:11,120 --> 00:04:14,319
connecting to the api server on the

00:04:13,120 --> 00:04:16,799
cloud

00:04:14,319 --> 00:04:17,600
during the runtime the yacht hub will

00:04:16,799 --> 00:04:19,359
keep checking

00:04:17,600 --> 00:04:21,359
the healthiness of the network

00:04:19,359 --> 00:04:24,560
connection between the edge node

00:04:21,359 --> 00:04:25,360
and an api server if the network between

00:04:24,560 --> 00:04:28,800
the edge node

00:04:25,360 --> 00:04:31,199
and api server is stable kubelet will

00:04:28,800 --> 00:04:33,680
talk to the api server directly

00:04:31,199 --> 00:04:36,320
and synchronize the pass state as it

00:04:33,680 --> 00:04:38,560
does in the native kubernetes

00:04:36,320 --> 00:04:40,840
in the meanwhile the yacht hub will

00:04:38,560 --> 00:04:42,560
cache post states in the local cache

00:04:40,840 --> 00:04:45,040
storage

00:04:42,560 --> 00:04:47,120
if the network between the edge node and

00:04:45,040 --> 00:04:49,440
api server is cut off

00:04:47,120 --> 00:04:51,680
the kubelet will use pass states in a

00:04:49,440 --> 00:04:54,160
local storage instead

00:04:51,680 --> 00:04:55,600
in addition we have a garbage collector

00:04:54,160 --> 00:04:58,080
running in github

00:04:55,600 --> 00:05:00,400
which periodically removes useless pod

00:04:58,080 --> 00:05:02,320
stage from the local cache

00:05:00,400 --> 00:05:03,680
at the same time we have the yacht

00:05:02,320 --> 00:05:06,080
controller manager

00:05:03,680 --> 00:05:06,800
running on the cloud to prohibit api

00:05:06,080 --> 00:05:11,840
server from

00:05:06,800 --> 00:05:11,840
expelling parts from unready edge nodes

00:05:12,000 --> 00:05:16,639
the second problem we wanted to resolve

00:05:14,320 --> 00:05:19,039
is how can we redirect the request from

00:05:16,639 --> 00:05:21,840
api server to edge nodes

00:05:19,039 --> 00:05:23,440
it is very common that users may want to

00:05:21,840 --> 00:05:26,720
fetch logs of pods

00:05:23,440 --> 00:05:29,199
or execute commands in parts

00:05:26,720 --> 00:05:32,080
this operation are normally conducted by

00:05:29,199 --> 00:05:35,039
using the kubictl command line tool

00:05:32,080 --> 00:05:36,320
which will send http request to the api

00:05:35,039 --> 00:05:38,800
server first

00:05:36,320 --> 00:05:41,520
and then api server will redirect the

00:05:38,800 --> 00:05:44,639
request to corresponding nodes

00:05:41,520 --> 00:05:47,520
however in a production environment api

00:05:44,639 --> 00:05:49,919
server is usually located on the cloud

00:05:47,520 --> 00:05:52,160
while edge nodes are located in user's

00:05:49,919 --> 00:05:54,960
private local networks

00:05:52,160 --> 00:05:56,639
therefore api server cannot talk to edge

00:05:54,960 --> 00:05:58,800
node directly

00:05:56,639 --> 00:06:00,240
we developed the yacht tunnel to tackle

00:05:58,800 --> 00:06:02,400
this problem

00:06:00,240 --> 00:06:03,600
yacht tunnel using a client server

00:06:02,400 --> 00:06:05,759
architecture

00:06:03,600 --> 00:06:07,120
on each cloud node we deploy a yacht

00:06:05,759 --> 00:06:09,600
tunnel server

00:06:07,120 --> 00:06:12,880
while on each edge node a yacht tunnel

00:06:09,600 --> 00:06:15,120
agent will be deployed

00:06:12,880 --> 00:06:17,280
even though the incoming traffic may be

00:06:15,120 --> 00:06:18,400
blocked outside of the private local

00:06:17,280 --> 00:06:20,800
network

00:06:18,400 --> 00:06:22,639
edge nodes can still send requests out

00:06:20,800 --> 00:06:25,039
of the local network

00:06:22,639 --> 00:06:26,400
therefore after yacht tunnel agent is

00:06:25,039 --> 00:06:28,960
set up

00:06:26,400 --> 00:06:31,280
it will initiate a reverse connection

00:06:28,960 --> 00:06:32,800
and register itself on the yacht tunnel

00:06:31,280 --> 00:06:35,759
server

00:06:32,800 --> 00:06:37,199
to redirect the request to natano server

00:06:35,759 --> 00:06:40,400
we inject the rules

00:06:37,199 --> 00:06:40,720
to the host ip table which will redirect

00:06:40,400 --> 00:06:43,680
all

00:06:40,720 --> 00:06:45,000
outgoing http requests with destination

00:06:43,680 --> 00:06:48,479
part

00:06:45,000 --> 00:06:51,520
10250 and 10255

00:06:48,479 --> 00:06:53,120
to the tunnel server yeah tunnel used

00:06:51,520 --> 00:06:56,240
the api server network

00:06:53,120 --> 00:06:58,080
proxy internally i will refer to api

00:06:56,240 --> 00:07:01,599
server network proxy as an

00:06:58,080 --> 00:07:04,880
amp in the rest of this talk amp does

00:07:01,599 --> 00:07:06,880
not work with old version of kubernetes

00:07:04,880 --> 00:07:08,400
that does not support the egress

00:07:06,880 --> 00:07:10,960
selector

00:07:08,400 --> 00:07:12,240
therefore we add an interceptor inside a

00:07:10,960 --> 00:07:15,599
tunnel server

00:07:12,240 --> 00:07:19,120
which will encapsulate http request in a

00:07:15,599 --> 00:07:22,240
format that is compatible with amp

00:07:19,120 --> 00:07:24,880
also since the amp used grpc

00:07:22,240 --> 00:07:26,960
protocol to transfer data between tunnel

00:07:24,880 --> 00:07:28,720
server and a tunnel agent

00:07:26,960 --> 00:07:30,000
we are able to further reduce the

00:07:28,720 --> 00:07:33,840
bandwidth between the cloud

00:07:30,000 --> 00:07:34,400
nodes and edge nodes after a http

00:07:33,840 --> 00:07:37,599
request

00:07:34,400 --> 00:07:40,400
is encapsulated the amp server

00:07:37,599 --> 00:07:40,720
will pick the corresponding amp agent

00:07:40,400 --> 00:07:44,960
and

00:07:40,720 --> 00:07:47,120
send a request to it

00:07:44,960 --> 00:07:48,080
the third challenge that prevents using

00:07:47,120 --> 00:07:50,080
kubernetes in

00:07:48,080 --> 00:07:52,160
edge computing environment is that

00:07:50,080 --> 00:07:54,400
kubernetes does not support deployed

00:07:52,160 --> 00:07:56,639
workloads by regions

00:07:54,400 --> 00:07:58,160
since an edge cluster can be located

00:07:56,639 --> 00:08:01,039
across multiple network

00:07:58,160 --> 00:08:02,960
or geographic regions an assistant or

00:08:01,039 --> 00:08:03,919
device setting in different regions may

00:08:02,960 --> 00:08:06,240
vary

00:08:03,919 --> 00:08:07,440
so users may want to deploy workloads by

00:08:06,240 --> 00:08:10,000
region

00:08:07,440 --> 00:08:11,599
for example the system architecture of

00:08:10,000 --> 00:08:15,160
edge nodes in a region 1

00:08:11,599 --> 00:08:16,800
maybe amd64 while in region 2 maybe

00:08:15,160 --> 00:08:19,280
arm64

00:08:16,800 --> 00:08:20,560
if users want to run same workload in

00:08:19,280 --> 00:08:22,720
both regions

00:08:20,560 --> 00:08:25,520
then they have to create two workloads

00:08:22,720 --> 00:08:28,639
with exactly the same configurations but

00:08:25,520 --> 00:08:30,960
only difference in container image name

00:08:28,639 --> 00:08:32,560
this can cause exponential increase in

00:08:30,960 --> 00:08:35,599
the maintenance efforts with the

00:08:32,560 --> 00:08:38,000
increasing number of workloads

00:08:35,599 --> 00:08:41,200
we developed the yacht app manager to

00:08:38,000 --> 00:08:43,680
add the maintenance burden on users

00:08:41,200 --> 00:08:44,240
specifically the yacht app manager

00:08:43,680 --> 00:08:47,600
contains

00:08:44,240 --> 00:08:51,279
two controllers the no pool controller

00:08:47,600 --> 00:08:53,680
and the united deployment controller

00:08:51,279 --> 00:08:55,120
the no pool controller can group nodes

00:08:53,680 --> 00:08:56,800
into pools

00:08:55,120 --> 00:08:58,959
can be nodes with same system

00:08:56,800 --> 00:09:00,480
architecture or nodes from the same

00:08:58,959 --> 00:09:03,680
network region

00:09:00,480 --> 00:09:07,200
and then we can manage nodes in a sample

00:09:03,680 --> 00:09:10,080
uniformly for example we can add labels

00:09:07,200 --> 00:09:11,440
annotations or tense to all nodes in a

00:09:10,080 --> 00:09:14,640
sample

00:09:11,440 --> 00:09:15,360
after we grouping nodes into pool united

00:09:14,640 --> 00:09:17,680
deployment

00:09:15,360 --> 00:09:19,440
controller can deploy the workloads

00:09:17,680 --> 00:09:21,440
based on a send template

00:09:19,440 --> 00:09:22,640
but different configurations across

00:09:21,440 --> 00:09:24,959
pools

00:09:22,640 --> 00:09:26,640
in addition we leverage the service

00:09:24,959 --> 00:09:29,360
topology feature to bound

00:09:26,640 --> 00:09:31,680
east western network traffic within a

00:09:29,360 --> 00:09:31,680
pool

00:09:32,959 --> 00:09:36,000
the last challenge is how can we support

00:09:35,440 --> 00:09:38,240
various

00:09:36,000 --> 00:09:39,600
kinds of edge device made by different

00:09:38,240 --> 00:09:42,720
manufacturers

00:09:39,600 --> 00:09:44,959
using different communication protocols

00:09:42,720 --> 00:09:47,279
existing solutions either need to change

00:09:44,959 --> 00:09:48,640
some key components of the kubernetes

00:09:47,279 --> 00:09:51,200
significantly

00:09:48,640 --> 00:09:54,080
or require users to implement the device

00:09:51,200 --> 00:09:56,880
adapter or driver from scratch

00:09:54,080 --> 00:10:00,160
which result in large development effort

00:09:56,880 --> 00:10:03,360
and the laws of some upstream features

00:10:00,160 --> 00:10:04,880
inspired by the unix philosophy do one

00:10:03,360 --> 00:10:07,600
thing and do it well

00:10:04,880 --> 00:10:09,519
we believe that kubernetes should focus

00:10:07,600 --> 00:10:11,760
on managing computing nodes

00:10:09,519 --> 00:10:15,040
while the device management should be

00:10:11,760 --> 00:10:17,360
down by a mature edge computing platform

00:10:15,040 --> 00:10:19,839
therefore we come up with the idea of

00:10:17,360 --> 00:10:21,519
integrating edge x foundry into open

00:10:19,839 --> 00:10:24,160
yard

00:10:21,519 --> 00:10:25,040
next eaching will give us some necessary

00:10:24,160 --> 00:10:27,839
backgrounds of

00:10:25,040 --> 00:10:28,480
ajax foundry and introduce how we

00:10:27,839 --> 00:10:32,959
combine

00:10:28,480 --> 00:10:35,040
rgx foundry with openyard

00:10:32,959 --> 00:10:36,000
okay thanks charles for the introduction

00:10:35,040 --> 00:10:39,279
hello everyone

00:10:36,000 --> 00:10:42,079
i'm ishin from vmware city office

00:10:39,279 --> 00:10:44,000
and also the maintainer of the open

00:10:42,079 --> 00:10:47,120
europe project

00:10:44,000 --> 00:10:48,480
so for those who are not familiar with

00:10:47,120 --> 00:10:50,880
edge foundry i'd

00:10:48,480 --> 00:10:52,000
i'd like to give a general introduction

00:10:50,880 --> 00:10:54,720
first

00:10:52,000 --> 00:10:56,959
then i will cover the open europe and

00:10:54,720 --> 00:10:59,040
ajax integration overview

00:10:56,959 --> 00:11:00,000
at last there will be a demo to

00:10:59,040 --> 00:11:03,680
illustrate how it

00:11:00,000 --> 00:11:06,160
works so what is adjacent foundry

00:11:03,680 --> 00:11:08,720
it is an open source when the neutron

00:11:06,160 --> 00:11:11,200
project and the linux foundation

00:11:08,720 --> 00:11:14,240
with apigee 2 license it is also

00:11:11,200 --> 00:11:17,600
microservice loosely coupled software

00:11:14,240 --> 00:11:20,079
framework for iot edge computing

00:11:17,600 --> 00:11:22,320
and at the same time it is hardware and

00:11:20,079 --> 00:11:24,560
os agnostic

00:11:22,320 --> 00:11:27,040
if we need to use one sentence to

00:11:24,560 --> 00:11:30,240
describe it

00:11:27,040 --> 00:11:32,079
uh ajax is a middleware that

00:11:30,240 --> 00:11:33,600
connects things with your i.t

00:11:32,079 --> 00:11:36,800
environment

00:11:33,600 --> 00:11:36,800
so clearly speaking

00:11:38,480 --> 00:11:44,399
the surface of edx constitute a

00:11:41,600 --> 00:11:45,360
dual transformation engine first is

00:11:44,399 --> 00:11:47,440
translating

00:11:45,360 --> 00:11:48,959
information coming from sensors and the

00:11:47,440 --> 00:11:50,959
devices

00:11:48,959 --> 00:11:52,079
vary hundreds of protocols and a

00:11:50,959 --> 00:11:55,200
thousand of

00:11:52,079 --> 00:11:57,519
from mice into hx and second

00:11:55,200 --> 00:11:58,399
it can deliver data to applications

00:11:57,519 --> 00:12:02,240
enterprise and

00:11:58,399 --> 00:12:04,079
cloud system over tcp based protocol

00:12:02,240 --> 00:12:06,959
information and the structure of

00:12:04,079 --> 00:12:06,959
customer choice

00:12:07,040 --> 00:12:11,040
now let's take a look at the edx

00:12:09,120 --> 00:12:14,240
architecture from south side

00:12:11,040 --> 00:12:14,800
to the north side it have it has four

00:12:14,240 --> 00:12:16,240
layers

00:12:14,800 --> 00:12:18,160
the device surface layer which is

00:12:16,240 --> 00:12:21,120
responsible to connect

00:12:18,160 --> 00:12:22,160
all kinds of sensor devices or

00:12:21,120 --> 00:12:24,320
activators

00:12:22,160 --> 00:12:26,639
over hundreds of ot protocol the

00:12:24,320 --> 00:12:27,839
co-service layer the supporting service

00:12:26,639 --> 00:12:30,880
layer and

00:12:27,839 --> 00:12:34,000
the application service layer let's take

00:12:30,880 --> 00:12:37,680
the data flow in edx system for example

00:12:34,000 --> 00:12:37,680
to illustrate how it works

00:12:37,760 --> 00:12:42,720
first sensor data is clacked by the

00:12:40,480 --> 00:12:44,800
device service from a scene

00:12:42,720 --> 00:12:47,360
and the data is passed to the code

00:12:44,800 --> 00:12:49,360
service for local persistence

00:12:47,360 --> 00:12:50,880
data is then passed to application

00:12:49,360 --> 00:12:53,440
service for

00:12:50,880 --> 00:12:54,240
transformation from my team featuring

00:12:53,440 --> 00:12:56,639
and they can

00:12:54,240 --> 00:12:58,000
zoom back into mouse to enterprise and

00:12:56,639 --> 00:13:02,079
cloud system

00:12:58,000 --> 00:13:02,480
when needed data is then available for

00:13:02,079 --> 00:13:05,440
ad

00:13:02,480 --> 00:13:06,320
analysis and can trigger device

00:13:05,440 --> 00:13:09,200
activation

00:13:06,320 --> 00:13:09,760
through command service basically there

00:13:09,200 --> 00:13:12,160
are two

00:13:09,760 --> 00:13:13,440
parts to control the added device one is

00:13:12,160 --> 00:13:16,240
through the cloud

00:13:13,440 --> 00:13:16,639
the other is some automation control and

00:13:16,240 --> 00:13:18,959
add

00:13:16,639 --> 00:13:22,160
through some predefined logic through

00:13:18,959 --> 00:13:24,639
the user defined application service or

00:13:22,160 --> 00:13:25,440
through engine or both since you can

00:13:24,639 --> 00:13:28,000
always

00:13:25,440 --> 00:13:29,920
combine them together like you can

00:13:28,000 --> 00:13:30,480
training models on the cloud and the

00:13:29,920 --> 00:13:33,920
creator

00:13:30,480 --> 00:13:36,560
and application service on the edge for

00:13:33,920 --> 00:13:36,560
influence

00:13:37,040 --> 00:13:41,600
now let's take a look at the edx

00:13:39,680 --> 00:13:44,000
deployment scenario

00:13:41,600 --> 00:13:44,720
in today's iot landscape it is

00:13:44,000 --> 00:13:47,920
imperative

00:13:44,720 --> 00:13:48,560
to leverage computer storage and network

00:13:47,920 --> 00:13:51,680
resource

00:13:48,560 --> 00:13:54,399
wherever they live so in the first

00:13:51,680 --> 00:13:54,959
scenario you can deploy the device

00:13:54,399 --> 00:13:57,680
service

00:13:54,959 --> 00:13:58,880
on the edge side and everything else in

00:13:57,680 --> 00:14:01,680
the cloud

00:13:58,880 --> 00:14:03,360
this is for latency in sensitive

00:14:01,680 --> 00:14:07,680
applications

00:14:03,360 --> 00:14:07,680
and the second scenario if the gateway

00:14:07,920 --> 00:14:14,959
has enough compute and storage resource

00:14:12,399 --> 00:14:16,399
you can deploy the full idx foundry

00:14:14,959 --> 00:14:20,000
service on the gateway and

00:14:16,399 --> 00:14:22,480
gain good response time in scenario 3

00:14:20,000 --> 00:14:24,320
if the gateway is not set good you can

00:14:22,480 --> 00:14:26,959
just deploy the device service

00:14:24,320 --> 00:14:28,720
and the core service there and move the

00:14:26,959 --> 00:14:32,000
application and the

00:14:28,720 --> 00:14:33,279
analytics to the server part the fourth

00:14:32,000 --> 00:14:36,160
scenario

00:14:33,279 --> 00:14:37,040
is quite similar with the first one

00:14:36,160 --> 00:14:40,079
except that it

00:14:37,040 --> 00:14:43,519
deploy ajax on the phone server

00:14:40,079 --> 00:14:46,560
to gain better latency response

00:14:43,519 --> 00:14:47,279
so we can see that the quantity and the

00:14:46,560 --> 00:14:50,560
function of

00:14:47,279 --> 00:14:52,720
macroservice deployed on given node

00:14:50,560 --> 00:14:56,240
depends on the use cases and the

00:14:52,720 --> 00:14:56,240
capability of the hardware

00:14:57,279 --> 00:15:02,240
now let's take a look at the open unit

00:14:59,600 --> 00:15:05,360
and the adjacent foundry integration

00:15:02,240 --> 00:15:08,000
on the cloud

00:15:05,360 --> 00:15:08,560
side we run the use control manager

00:15:08,000 --> 00:15:10,399
you're the

00:15:08,560 --> 00:15:13,519
app manager and the tunnel server

00:15:10,399 --> 00:15:16,800
manager you can join it on any public

00:15:13,519 --> 00:15:18,079
private or hyper cloud or on parameter

00:15:16,800 --> 00:15:20,880
center

00:15:18,079 --> 00:15:22,639
which according to your requirements and

00:15:20,880 --> 00:15:25,040
on the other side

00:15:22,639 --> 00:15:26,079
besides the default you have an internal

00:15:25,040 --> 00:15:28,800
agent

00:15:26,079 --> 00:15:30,959
we will deploy a new device controller

00:15:28,800 --> 00:15:34,880
which is responsible for sync data

00:15:30,959 --> 00:15:37,279
between the hx and the kubernetes

00:15:34,880 --> 00:15:39,680
so the xcode service will also be

00:15:37,279 --> 00:15:43,279
deployed on the edge side

00:15:39,680 --> 00:15:46,560
in some case that sensor's direct

00:15:43,279 --> 00:15:47,839
connect githui doesn't have enough

00:15:46,560 --> 00:15:50,880
resource we can just

00:15:47,839 --> 00:15:52,160
deploy the edx device service there the

00:15:50,880 --> 00:15:58,399
add node can

00:15:52,160 --> 00:15:58,399
be either unbased or x86 based

00:15:59,120 --> 00:16:03,759
now i'll use a demo to illustrate how it

00:16:01,680 --> 00:16:05,600
works

00:16:03,759 --> 00:16:07,759
let's take a look at the demo

00:16:05,600 --> 00:16:10,800
environment topology

00:16:07,759 --> 00:16:12,800
we have a mass node running on the cloud

00:16:10,800 --> 00:16:15,199
and we'll have two work nodes running in

00:16:12,800 --> 00:16:18,720
the private network

00:16:15,199 --> 00:16:21,360
the python is in my home office

00:16:18,720 --> 00:16:22,720
and the e-box is running in another

00:16:21,360 --> 00:16:25,600
location

00:16:22,720 --> 00:16:26,959
they together compose a unified

00:16:25,600 --> 00:16:30,399
coordinate cluster

00:16:26,959 --> 00:16:32,880
cross cloud and edge with different

00:16:30,399 --> 00:16:35,920
hardware architecture

00:16:32,880 --> 00:16:36,480
we use kubcontrol to get a node to check

00:16:35,920 --> 00:16:40,079
the node

00:16:36,480 --> 00:16:41,120
status in the cluster this is the pi

00:16:40,079 --> 00:16:43,279
volt node

00:16:41,120 --> 00:16:46,800
we connect the temperature and the hemi

00:16:43,279 --> 00:16:46,800
sensors through gpl pin

00:16:48,839 --> 00:16:55,360
17.

00:16:51,839 --> 00:16:56,240
this is an rgb led light connected to

00:16:55,360 --> 00:17:05,839
the pi

00:16:56,240 --> 00:17:05,839
through cpl pin 18 19 and 20.

00:17:06,880 --> 00:17:12,880
this is the second node which is x86

00:17:10,559 --> 00:17:12,880
based

00:17:15,039 --> 00:17:21,439
we next will deploy the ajax code

00:17:17,679 --> 00:17:21,439
service to the pi forward node

00:17:24,400 --> 00:17:33,280
we use kubercontrol getport to check the

00:17:27,039 --> 00:17:36,080
deployment creating status

00:17:33,280 --> 00:17:38,400
after few minutes we run the command

00:17:36,080 --> 00:17:38,400
again

00:17:42,400 --> 00:17:47,679
we can see that the idx service are up

00:17:45,120 --> 00:17:49,360
and running on the pi forward node

00:17:47,679 --> 00:17:52,320
the device controller will sink the

00:17:49,360 --> 00:17:54,799
device information to create

00:17:52,320 --> 00:18:00,400
we can use control get the device to

00:17:54,799 --> 00:18:02,400
list all the devices in the azx

00:18:00,400 --> 00:18:04,880
we can use group control get device

00:18:02,400 --> 00:18:07,200
profile to list all the device profile

00:18:04,880 --> 00:18:09,840
information

00:18:07,200 --> 00:18:10,559
also we can use google control device

00:18:09,840 --> 00:18:14,880
service

00:18:10,559 --> 00:18:17,120
to list the device service information

00:18:14,880 --> 00:18:18,000
like what we can do to the traditional

00:18:17,120 --> 00:18:20,480
kubernetes

00:18:18,000 --> 00:18:23,840
object we can change the device status

00:18:20,480 --> 00:18:23,840
by group control edit

00:18:24,400 --> 00:18:28,000
first we will run command group control

00:18:26,880 --> 00:18:30,720
i id

00:18:28,000 --> 00:18:30,720
device name

00:18:39,840 --> 00:18:43,600
we then set the device property light to

00:18:42,720 --> 00:18:47,280
color right

00:18:43,600 --> 00:18:56,160
as the desired value

00:18:47,280 --> 00:18:59,520
we save and exit

00:18:56,160 --> 00:19:02,080
you can see as x will turn the led light

00:18:59,520 --> 00:19:02,080
to right

00:19:06,160 --> 00:19:11,840
and the next we run control i t device

00:19:08,559 --> 00:19:11,840
name again

00:19:12,240 --> 00:19:17,760
this time we'll set the desired color to

00:19:15,360 --> 00:19:17,760
blue

00:19:19,919 --> 00:19:27,840
we save and exit we can see

00:19:23,039 --> 00:19:27,840
ajax turns the light to blue

00:19:28,080 --> 00:19:31,360
last we run group control id's device

00:19:30,320 --> 00:19:38,720
name again and

00:19:31,360 --> 00:19:42,240
cite the desired color to green

00:19:38,720 --> 00:19:45,280
then we save and exit we can see ajax

00:19:42,240 --> 00:19:47,360
will return the led light to green so

00:19:45,280 --> 00:19:50,080
with hx and open urls

00:19:47,360 --> 00:19:53,840
you can manage your id devices in your

00:19:50,080 --> 00:19:53,840
kubernetes native manner

00:19:53,919 --> 00:20:03,600
thanks for listening our session now

00:19:56,240 --> 00:20:03,600

YouTube URL: https://www.youtube.com/watch?v=rJHXSpQIjaY


