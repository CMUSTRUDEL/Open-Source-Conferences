Title: Lightning Talk: SNMP done quick - tuning JunOS for metrics extraction - Ben Kochie, GitLab
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Lightning Talk: SNMP done quick - tuning JunOS for metrics extraction - Ben Kochie, GitLab
Captions: 
	00:00:00,080 --> 00:00:04,400
welcome hi my name is ben cochi i'm one

00:00:02,879 --> 00:00:05,040
of the maintainers of the prometheus

00:00:04,400 --> 00:00:08,559
snmp

00:00:05,040 --> 00:00:11,840
exporter snmp is a networking protocol

00:00:08,559 --> 00:00:14,719
that's used to manage and

00:00:11,840 --> 00:00:16,480
gather data from network devices

00:00:14,719 --> 00:00:17,279
typically router switches that kind of

00:00:16,480 --> 00:00:19,920
thing

00:00:17,279 --> 00:00:21,279
it's very old but fortunately the data

00:00:19,920 --> 00:00:23,840
model that it uses maps

00:00:21,279 --> 00:00:25,119
very well into prometheus metrics the

00:00:23,840 --> 00:00:27,279
metric trees

00:00:25,119 --> 00:00:28,400
can be mapped into metrics and they're

00:00:27,279 --> 00:00:30,880
indexed in

00:00:28,400 --> 00:00:31,679
tables and the indexes can be mapped to

00:00:30,880 --> 00:00:35,200
labels

00:00:31,679 --> 00:00:36,960
this works out really well so i've got a

00:00:35,200 --> 00:00:39,200
couple of old juniper switches

00:00:36,960 --> 00:00:40,879
they're in a switch stack and there's

00:00:39,200 --> 00:00:41,760
quite a lot of ports and a lot of data

00:00:40,879 --> 00:00:46,000
together

00:00:41,760 --> 00:00:49,760
so let me start up a quick scrape

00:00:46,000 --> 00:00:52,000
well that's going it's taking a while

00:00:49,760 --> 00:00:53,600
let's take a look i've turned on snmp

00:00:52,000 --> 00:00:55,840
exporter debug logging

00:00:53,600 --> 00:00:58,320
and let's see how long it takes to

00:00:55,840 --> 00:01:00,399
gather this data

00:00:58,320 --> 00:01:03,199
well it's still going so while we're

00:01:00,399 --> 00:01:03,199
looking waiting

00:01:05,439 --> 00:01:09,680
let's take a look at the snmp

00:01:08,320 --> 00:01:10,880
configuration that i've added to my

00:01:09,680 --> 00:01:12,640
juniper switch

00:01:10,880 --> 00:01:14,240
so there's some stuff that i've left out

00:01:12,640 --> 00:01:15,759
but this is the interesting bit that

00:01:14,240 --> 00:01:17,200
helps improve performance

00:01:15,759 --> 00:01:18,960
uh the first thing i did to improve

00:01:17,200 --> 00:01:22,000
performance was i added a

00:01:18,960 --> 00:01:24,000
inter interface filter and this drops

00:01:22,000 --> 00:01:26,320
some of the data from the device that i

00:01:24,000 --> 00:01:27,280
don't actually need to gather from the

00:01:26,320 --> 00:01:28,960
device

00:01:27,280 --> 00:01:31,040
there's a number of sub-interfaces and

00:01:28,960 --> 00:01:33,119
it's a little bit cryptic but basically

00:01:31,040 --> 00:01:36,079
this drops the sub-interface data from

00:01:33,119 --> 00:01:37,759
the uh from the output of the switch

00:01:36,079 --> 00:01:41,280
and the second thing i've done is i've

00:01:37,759 --> 00:01:44,479
created a i've added the stats cache

00:01:41,280 --> 00:01:44,880
that caches the data for 29 seconds and

00:01:44,479 --> 00:01:47,040
this

00:01:44,880 --> 00:01:49,200
is designed to match with the scrape

00:01:47,040 --> 00:01:52,079
interval so if i hit the device

00:01:49,200 --> 00:01:52,799
twice from two different prometheus

00:01:52,079 --> 00:01:55,119
instances

00:01:52,799 --> 00:01:56,479
it'll produce cache data which is should

00:01:55,119 --> 00:01:58,320
be much faster

00:01:56,479 --> 00:01:59,520
than producing the pulling the raw data

00:01:58,320 --> 00:02:02,079
from the switch

00:01:59,520 --> 00:02:03,119
uh and but i i wanted to make sure that

00:02:02,079 --> 00:02:05,360
i didn't

00:02:03,119 --> 00:02:06,719
uh cache longer than one scrap interval

00:02:05,360 --> 00:02:09,119
so i've made it one second

00:02:06,719 --> 00:02:10,879
shorter than the actual scrape interval

00:02:09,119 --> 00:02:12,800
let's see how that squawk is doing okay

00:02:10,879 --> 00:02:15,520
so that walk completed and it took

00:02:12,800 --> 00:02:15,840
22 seconds well it's not bad but it's

00:02:15,520 --> 00:02:18,080
not

00:02:15,840 --> 00:02:20,000
great so let's see if we can figure out

00:02:18,080 --> 00:02:23,120
why and or how to improve this

00:02:20,000 --> 00:02:24,560
well so we've got two subtree walks here

00:02:23,120 --> 00:02:26,400
in the debug log

00:02:24,560 --> 00:02:27,599
uh one of them took 12 seconds one of

00:02:26,400 --> 00:02:30,400
them took took

00:02:27,599 --> 00:02:32,160
eight point uh 9.8 seconds well that

00:02:30,400 --> 00:02:35,120
pretty much matches up with

00:02:32,160 --> 00:02:36,800
the default uh iaf mib and so this is

00:02:35,120 --> 00:02:38,480
the walk configuration that i've

00:02:36,800 --> 00:02:41,200
i've asked the device to produce data

00:02:38,480 --> 00:02:42,480
for and so the interfaces table in the

00:02:41,200 --> 00:02:45,360
ifx table

00:02:42,480 --> 00:02:46,640
come from this ifmib and as you can see

00:02:45,360 --> 00:02:50,480
here

00:02:46,640 --> 00:02:51,040
these two tables the iftable and the ifx

00:02:50,480 --> 00:02:53,920
table

00:02:51,040 --> 00:02:55,200
contain a lot of subtrees and so the

00:02:53,920 --> 00:02:56,720
first thing we can do

00:02:55,200 --> 00:02:59,440
is well let's see what happens if we

00:02:56,720 --> 00:03:01,599
take and split that out so i've taken

00:02:59,440 --> 00:03:03,360
and i've built an expanded tree that

00:03:01,599 --> 00:03:06,640
takes and expands all of these

00:03:03,360 --> 00:03:09,360
subtrees and let's run that scrape

00:03:06,640 --> 00:03:11,760
so here's if expanded and let's see what

00:03:09,360 --> 00:03:14,000
happens if we try and load this

00:03:11,760 --> 00:03:15,040
and see and we'll wait for those logs to

00:03:14,000 --> 00:03:16,879
finish

00:03:15,040 --> 00:03:18,800
all right so that's going a little bit

00:03:16,879 --> 00:03:20,560
faster well sort of

00:03:18,800 --> 00:03:22,319
it's still taking somewhere in the order

00:03:20,560 --> 00:03:23,040
of five to six hundred milliseconds per

00:03:22,319 --> 00:03:25,519
subtree

00:03:23,040 --> 00:03:28,159
to gather all this data and so we

00:03:25,519 --> 00:03:31,760
haven't really improved the speed by

00:03:28,159 --> 00:03:34,159
making the scrapes more granular

00:03:31,760 --> 00:03:34,879
so it must be something about the scrape

00:03:34,159 --> 00:03:36,720
data that

00:03:34,879 --> 00:03:38,879
makes it take so long to produce those

00:03:36,720 --> 00:03:41,200
metrics so

00:03:38,879 --> 00:03:42,560
the next thing we can do is we can

00:03:41,200 --> 00:03:46,000
simply

00:03:42,560 --> 00:03:48,000
stop ingesting data we don't need so

00:03:46,000 --> 00:03:50,480
here's a generator config that i've

00:03:48,000 --> 00:03:52,720
created that only gathers exactly what i

00:03:50,480 --> 00:03:56,560
need from the device which is

00:03:52,720 --> 00:03:58,720
the high capacity counters for

00:03:56,560 --> 00:04:00,000
all the basics and then i've created a

00:03:58,720 --> 00:04:02,159
second config that gathers

00:04:00,000 --> 00:04:04,000
all the error counters and a couple of

00:04:02,159 --> 00:04:05,680
other things like admin status upper

00:04:04,000 --> 00:04:09,439
status and

00:04:05,680 --> 00:04:11,439
port speed and so once this is done

00:04:09,439 --> 00:04:12,799
producing data yeah so that still took

00:04:11,439 --> 00:04:15,040
24 seconds

00:04:12,799 --> 00:04:18,720
it definitely wasn't any faster so let's

00:04:15,040 --> 00:04:18,720
see what happens if i do the same thing

00:04:19,759 --> 00:04:26,000
uh and i only gather my my mini config

00:04:24,240 --> 00:04:27,840
well let's take a look let's wait for

00:04:26,000 --> 00:04:30,479
that

00:04:27,840 --> 00:04:30,479
walk to run

00:04:32,240 --> 00:04:35,440
and see how long that take that looks

00:04:34,160 --> 00:04:37,360
like it completed

00:04:35,440 --> 00:04:38,960
well that was much much faster i wonder

00:04:37,360 --> 00:04:40,800
why the

00:04:38,960 --> 00:04:42,000
the the system log seems to be a little

00:04:40,800 --> 00:04:45,440
bit lagged but

00:04:42,000 --> 00:04:48,320
um let's see if we can get that to

00:04:45,440 --> 00:04:48,320
produce more data

00:04:48,479 --> 00:04:52,639
there we go yeah so that that walk only

00:04:51,520 --> 00:04:56,160
took six seconds

00:04:52,639 --> 00:04:58,000
so the big trick to do if your

00:04:56,160 --> 00:05:00,320
gathering data is too slow is turn on

00:04:58,000 --> 00:05:02,320
snmp exporter to blog logging

00:05:00,320 --> 00:05:04,320
examine all of the sub trees to find out

00:05:02,320 --> 00:05:08,080
if any specific subtree is

00:05:04,320 --> 00:05:08,080
fast or slow and then

00:05:09,520 --> 00:05:12,880
reduce the amount of data that you're

00:05:10,800 --> 00:05:14,720
gathering

00:05:12,880 --> 00:05:16,639
thanks if you want to see these

00:05:14,720 --> 00:05:17,840
configurations i put them up on my

00:05:16,639 --> 00:05:21,520
github

00:05:17,840 --> 00:05:23,919
under my tools repo

00:05:21,520 --> 00:05:25,600
under the snmp exporter directory and

00:05:23,919 --> 00:05:27,440
there i have a lot of example configs

00:05:25,600 --> 00:05:30,000
here

00:05:27,440 --> 00:05:30,000

YouTube URL: https://www.youtube.com/watch?v=bmcdqK2LATc


