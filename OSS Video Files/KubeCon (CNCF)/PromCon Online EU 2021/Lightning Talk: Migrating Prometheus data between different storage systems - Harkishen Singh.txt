Title: Lightning Talk: Migrating Prometheus data between different storage systems - Harkishen Singh
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Lightning Talk: Migrating Prometheus data between different storage systems - Harkishen Singh, Timescale

"The Prometheus ecosystem includes many long-term data storage systems that allow users to store and analyze their Prometheus data. Each of these systems have different tradeoffs – and users sometimes want to switch between them as their needs evolve. But switching between these storage systems has not been possible because, until now, there has been no universal tool to migrate data between them.

Prom-migrator solves this problem. It is a universal, open-source Prometheus data migration tool that is community-driven and free to use. Prom-migrator migrates data from one remote-storage system to another remote-storage system, leveraging Prometheus’ remote storage API. This means that this tool can be used to migrate data to and from any storage system that supports the Prometheus remote_write/remote_read protocol (e.g. Promscale, Thanos, Cortex, M3DB, etc.).

Prom-migrator offers several key features:

Data migration from and to any storage system. This tool is designed to work with any remote storage system, so that users can migrate data across any system in a wide range of scenarios.

Informative outputs during runtime, allowing users to track progress. Prom-migrator keeps users informed about the migration progress, so that users can plan their time accordingly.

Ability to resume migration(s) in case of any unintended shutdowns. Keep a record of migration progress and, in the case failure or interruptions, automatically resume the migration where you left off.

Stateless working model. Easy deployment due to the fact that the migrator does not need to keep state. No need to worry about mounting volumes or attaching persistent storage.

In this talk, we will show how prom-migrator works, followed by a demo."
Captions: 
	00:00:00,080 --> 00:00:03,679
hey everyone in this presentation we

00:00:02,480 --> 00:00:05,120
will discuss about

00:00:03,679 --> 00:00:08,240
migrating vermeer's data between

00:00:05,120 --> 00:00:11,040
different storage systems

00:00:08,240 --> 00:00:12,080
before we start a brief introduction

00:00:11,040 --> 00:00:14,639
about me

00:00:12,080 --> 00:00:16,000
i am harkishan singh i am an active open

00:00:14,639 --> 00:00:17,199
source contributor to prometheus

00:00:16,000 --> 00:00:19,439
upstream

00:00:17,199 --> 00:00:20,560
i live in publisher india and i work as

00:00:19,439 --> 00:00:24,000
a software engineer at

00:00:20,560 --> 00:00:25,680
time scale i'm maintainer of fromscale

00:00:24,000 --> 00:00:28,480
which is a high performance from meteor

00:00:25,680 --> 00:00:30,240
data storage built on top of postgres

00:00:28,480 --> 00:00:32,560
that's about me let's begin with the

00:00:30,240 --> 00:00:32,560
topic

00:00:33,120 --> 00:00:37,360
our motivation behind developing a

00:00:34,800 --> 00:00:40,320
migration tool a prometheus was

00:00:37,360 --> 00:00:41,200
that at present there are 27 officially

00:00:40,320 --> 00:00:43,600
listed

00:00:41,200 --> 00:00:44,640
remote storage systems for prometheus

00:00:43,600 --> 00:00:46,160
however

00:00:44,640 --> 00:00:47,760
there are no good ways to migrate

00:00:46,160 --> 00:00:48,960
between them

00:00:47,760 --> 00:00:50,719
there can be a lot of reasons for

00:00:48,960 --> 00:00:51,440
migrating from ethiopia's data for

00:00:50,719 --> 00:00:55,440
example

00:00:51,440 --> 00:00:57,680
privacy high cardinality scalability etc

00:00:55,440 --> 00:00:58,559
the lack of proper migration tool leaves

00:00:57,680 --> 00:01:01,120
users with

00:00:58,559 --> 00:01:03,120
bad choices they are forced to throw

00:01:01,120 --> 00:01:05,840
away data in whole systems

00:01:03,120 --> 00:01:06,960
or run through systems parallely the old

00:01:05,840 --> 00:01:09,520
and the new one

00:01:06,960 --> 00:01:12,640
or do not go for any changes it is

00:01:09,520 --> 00:01:16,400
example of window login

00:01:12,640 --> 00:01:18,560
that's why we created prop migrator

00:01:16,400 --> 00:01:21,920
primary supports a wide range of meters

00:01:18,560 --> 00:01:23,360
compliant remote storage systems

00:01:21,920 --> 00:01:24,880
here is a compatibility table of

00:01:23,360 --> 00:01:26,080
different storage systems with from

00:01:24,880 --> 00:01:27,920
migrator

00:01:26,080 --> 00:01:30,479
limitations are of respective solid

00:01:27,920 --> 00:01:33,439
systems and not of from migrator

00:01:30,479 --> 00:01:34,560
please note that with backfill we mean

00:01:33,439 --> 00:01:36,159
pushing data

00:01:34,560 --> 00:01:38,400
to a solid system that already has

00:01:36,159 --> 00:01:42,240
existing data then

00:01:38,400 --> 00:01:42,240
then the data being actually pushed

00:01:42,560 --> 00:01:45,600
let's see how it works

00:01:47,200 --> 00:01:50,880
consider scenario of migration we want

00:01:49,200 --> 00:01:51,280
to pull data from the storage from the

00:01:50,880 --> 00:01:55,840
left

00:01:51,280 --> 00:01:58,640
to that on the right promoter pulls

00:01:55,840 --> 00:02:01,280
data in form of consecutive slabs each

00:01:58,640 --> 00:02:03,680
slab contains data in form of

00:02:01,280 --> 00:02:07,439
time range this time range increases

00:02:03,680 --> 00:02:10,479
with consecutive slabs by a minute

00:02:07,439 --> 00:02:12,160
as you can see after pulling a slab from

00:02:10,479 --> 00:02:14,319
my radar pushes it to storage

00:02:12,160 --> 00:02:16,720
on the right and at the same time push

00:02:14,319 --> 00:02:20,720
the next slab from strange from the left

00:02:16,720 --> 00:02:20,720
this is how it is my video in from

00:02:20,840 --> 00:02:25,680
migrator data migrations can be

00:02:23,840 --> 00:02:27,120
from a few megabytes to several

00:02:25,680 --> 00:02:29,920
petabytes

00:02:27,120 --> 00:02:31,440
prominent knows that migrations can be

00:02:29,920 --> 00:02:33,599
memory intensive

00:02:31,440 --> 00:02:34,959
for this reason promoter aims for a

00:02:33,599 --> 00:02:36,560
target memory to usage

00:02:34,959 --> 00:02:39,280
in such a way that there is a perfect

00:02:36,560 --> 00:02:41,840
balance between speed of data migration

00:02:39,280 --> 00:02:43,599
and the visualization of memory it

00:02:41,840 --> 00:02:44,640
follows an additive increase of time

00:02:43,599 --> 00:02:46,959
range

00:02:44,640 --> 00:02:48,080
when below the target memory region and

00:02:46,959 --> 00:02:50,319
multiplicity decrease

00:02:48,080 --> 00:02:51,680
when the memory usage exceeds the target

00:02:50,319 --> 00:02:53,840
region

00:02:51,680 --> 00:02:57,120
when within the aim target region the

00:02:53,840 --> 00:02:58,800
time range remains constant

00:02:57,120 --> 00:03:01,040
let's understand this better with the

00:02:58,800 --> 00:03:01,040
graph

00:03:02,400 --> 00:03:05,760
we start with the slabs from one minute

00:03:04,400 --> 00:03:09,680
and go with

00:03:05,760 --> 00:03:12,319
an increasing order of time time range

00:03:09,680 --> 00:03:13,920
and as we reach target region the time

00:03:12,319 --> 00:03:16,800
range is constant

00:03:13,920 --> 00:03:18,480
and if we exceed the target region of

00:03:16,800 --> 00:03:21,120
memory usage we do it

00:03:18,480 --> 00:03:22,879
multiplication decrease which is by half

00:03:21,120 --> 00:03:33,360
which is five minutes and then we again

00:03:22,879 --> 00:03:35,440
aim to be higher the target region

00:03:33,360 --> 00:03:38,879
pro migrator can gracefully restart

00:03:35,440 --> 00:03:38,879
after a failure or interruption

00:03:39,120 --> 00:03:42,480
levitate restart is achieved by pushing

00:03:41,920 --> 00:03:45,120
a max

00:03:42,480 --> 00:03:46,720
time of the last slab as a separate time

00:03:45,120 --> 00:03:49,920
series

00:03:46,720 --> 00:03:51,120
this max time is fetched when the major

00:03:49,920 --> 00:03:54,159
starts the next time

00:03:51,120 --> 00:03:57,680
which can be after a crash and it treats

00:03:54,159 --> 00:03:57,680
that as a starting point of current

00:03:58,840 --> 00:04:01,840
migration

00:04:00,239 --> 00:04:03,920
with this we achieve our goal of

00:04:01,840 --> 00:04:07,120
completely stateless working model

00:04:03,920 --> 00:04:10,080
that tracks progress interactively and

00:04:07,120 --> 00:04:10,720
can resume migration process increase in

00:04:10,080 --> 00:04:12,640
case of

00:04:10,720 --> 00:04:14,480
failure or interruption during the

00:04:12,640 --> 00:04:16,720
migration process

00:04:14,480 --> 00:04:17,919
this model has better control on memory

00:04:16,720 --> 00:04:20,799
in runtime

00:04:17,919 --> 00:04:24,240
and can migrate faster using concurrent

00:04:20,799 --> 00:04:24,240
pulling and pushing off data

00:04:25,199 --> 00:04:29,520
for more details we have a links to our

00:04:27,440 --> 00:04:33,759
demo video

00:04:29,520 --> 00:04:36,479
design doc github repository and readme

00:04:33,759 --> 00:04:39,520
if you want to try out the tool please

00:04:36,479 --> 00:04:43,040
please visit the download page

00:04:39,520 --> 00:04:46,800
over for here information

00:04:43,040 --> 00:04:46,800
you can read me off from migrator

00:04:48,960 --> 00:04:53,840
thank you very much for your kind

00:04:50,840 --> 00:04:53,840

YouTube URL: https://www.youtube.com/watch?v=svlozCM1pT4


