Title: Monitoring the monitor - David Leadbeater, G-Research
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Monitoring the monitor - David Leadbeater, G-Research

There are various ways to make sure Prometheus is working. We’ll cover these from cloud based services to Prometheus instances monitoring each other. Then we’ll explain why we developed a component to help with this.
Captions: 
	00:00:00,320 --> 00:00:03,520
and i'm here to talk about montreal i'm

00:00:02,960 --> 00:00:05,120
david

00:00:03,520 --> 00:00:06,879
and i'm here to talk about monitoring

00:00:05,120 --> 00:00:09,040
the monitor or

00:00:06,879 --> 00:00:11,840
if a prometheus falls does it make a

00:00:09,040 --> 00:00:15,679
sound prometheus to monitor your service

00:00:11,840 --> 00:00:15,679
but what monitors your prometheus

00:00:15,759 --> 00:00:21,439
so probably the first query you come

00:00:19,520 --> 00:00:23,680
across when you learn prometheus is

00:00:21,439 --> 00:00:27,359
something like this

00:00:23,680 --> 00:00:30,000
is prometheus up or actually is it down

00:00:27,359 --> 00:00:30,640
so this is an alert that if the metric

00:00:30,000 --> 00:00:34,079
up

00:00:30,640 --> 00:00:37,600
with a job label matching prometheus is

00:00:34,079 --> 00:00:40,399
zero then the query will return results

00:00:37,600 --> 00:00:41,680
um so you can use this in an alert with

00:00:40,399 --> 00:00:44,719
something like

00:00:41,680 --> 00:00:47,600
job down with an expression of up

00:00:44,719 --> 00:00:48,879
job prometheus is zero and then it will

00:00:47,600 --> 00:00:52,960
raise another

00:00:48,879 --> 00:00:56,160
annotation prometheus down for example

00:00:52,960 --> 00:00:57,760
so that's quite simple um

00:00:56,160 --> 00:00:59,600
but that's not enough to actually

00:00:57,760 --> 00:01:02,079
monitor prometheus itself

00:00:59,600 --> 00:01:03,359
because well it's not a cartoon it's not

00:01:02,079 --> 00:01:06,000
that simple

00:01:03,359 --> 00:01:07,119
and you can't monitor yourself with

00:01:06,000 --> 00:01:10,640
yourself

00:01:07,119 --> 00:01:12,400
so going back to basics again

00:01:10,640 --> 00:01:14,240
the architecture of a normal previous

00:01:12,400 --> 00:01:16,240
setup is something like this

00:01:14,240 --> 00:01:18,080
we have prometheus talking to an alert

00:01:16,240 --> 00:01:22,000
manager sending alerts to

00:01:18,080 --> 00:01:24,240
some kind of alert receiver so

00:01:22,000 --> 00:01:25,600
we only have one of each of these now

00:01:24,240 --> 00:01:27,119
the receiver is maybe something like

00:01:25,600 --> 00:01:28,560
page due to where someone else takes

00:01:27,119 --> 00:01:31,040
responsibility for

00:01:28,560 --> 00:01:32,240
actually making that reliable for you

00:01:31,040 --> 00:01:34,079
but prometheus

00:01:32,240 --> 00:01:35,439
and alert manager unless you're using a

00:01:34,079 --> 00:01:36,799
managed service are probably your

00:01:35,439 --> 00:01:39,759
responsibility

00:01:36,799 --> 00:01:40,479
so a common setup is to run multiple

00:01:39,759 --> 00:01:42,799
often

00:01:40,479 --> 00:01:44,640
so a pair of previous instances

00:01:42,799 --> 00:01:47,920
monitoring the same target

00:01:44,640 --> 00:01:49,520
and then also alert manager in a cluster

00:01:47,920 --> 00:01:52,840
mode of some kind

00:01:49,520 --> 00:01:54,159
and these ideally running on different

00:01:52,840 --> 00:01:56,719
machines

00:01:54,159 --> 00:01:57,360
so there's now some level of resiliency

00:01:56,719 --> 00:02:00,399
there

00:01:57,360 --> 00:02:03,439
which is good but

00:02:00,399 --> 00:02:05,759
what happens if the receiver is down

00:02:03,439 --> 00:02:08,640
or unreachable well alert manager tries

00:02:05,759 --> 00:02:08,640
to raise an alert

00:02:08,959 --> 00:02:15,680
but it can't go anywhere so

00:02:12,959 --> 00:02:15,680
for example

00:02:16,720 --> 00:02:20,480
premiums raises an alert like this

00:02:18,560 --> 00:02:25,599
saying jobs down

00:02:20,480 --> 00:02:27,680
and then can't go to the receiver

00:02:25,599 --> 00:02:29,280
so a common approach in the past was to

00:02:27,680 --> 00:02:31,680
have some kind of backup device

00:02:29,280 --> 00:02:33,840
connected to your server directly

00:02:31,680 --> 00:02:34,720
which meant you could use the internet

00:02:33,840 --> 00:02:37,599
and also

00:02:34,720 --> 00:02:39,920
sms for example um obviously it's a bit

00:02:37,599 --> 00:02:43,360
difficult to connect a phone to a

00:02:39,920 --> 00:02:45,280
server in the cloud so

00:02:43,360 --> 00:02:47,440
how people often deal with this is

00:02:45,280 --> 00:02:48,160
rather than alerting when something is

00:02:47,440 --> 00:02:50,640
down

00:02:48,160 --> 00:02:51,840
have a particular alert that exists as a

00:02:50,640 --> 00:02:55,360
heartbeat

00:02:51,840 --> 00:02:58,480
that is always expected which then

00:02:55,360 --> 00:02:59,360
is always sent to the receiver and the

00:02:58,480 --> 00:03:01,760
receiver

00:02:59,360 --> 00:03:04,480
somewhere on the internet knows that it

00:03:01,760 --> 00:03:07,840
should expect to receive an alert and

00:03:04,480 --> 00:03:10,319
if it doesn't then it raises an alert

00:03:07,840 --> 00:03:11,519
so it inverts alerting essentially

00:03:10,319 --> 00:03:13,599
saying

00:03:11,519 --> 00:03:16,080
if there isn't another then start

00:03:13,599 --> 00:03:18,640
raising the load

00:03:16,080 --> 00:03:20,640
so there are many ways of doing this uh

00:03:18,640 --> 00:03:23,440
healthchecks.i o provide a

00:03:20,640 --> 00:03:24,319
service that does this which is written

00:03:23,440 --> 00:03:25,920
in python

00:03:24,319 --> 00:03:28,319
you can run it yourself or there's a

00:03:25,920 --> 00:03:30,239
crowd-hosted version of it

00:03:28,319 --> 00:03:31,599
dead man snitch integrates with

00:03:30,239 --> 00:03:35,200
pagerduty

00:03:31,599 --> 00:03:37,680
and is cloud hosted karma which is

00:03:35,200 --> 00:03:39,040
a web-based ui for alert manager can

00:03:37,680 --> 00:03:42,000
also display an alert

00:03:39,040 --> 00:03:44,159
when a particular alert isn't present so

00:03:42,000 --> 00:03:45,920
that obviously doesn't page anyone but

00:03:44,159 --> 00:03:47,519
can show on a screen or something that

00:03:45,920 --> 00:03:49,200
there's a problem which if

00:03:47,519 --> 00:03:50,799
you have a knock or something could

00:03:49,200 --> 00:03:53,200
potentially be useful

00:03:50,799 --> 00:03:56,000
um alternatively to do something

00:03:53,200 --> 00:03:56,000
entirely custom

00:03:56,239 --> 00:04:00,080
so let's look at how we actually set up

00:03:58,080 --> 00:04:01,360
alert manager to talk about heartbeat

00:04:00,080 --> 00:04:04,080
receiver

00:04:01,360 --> 00:04:05,360
in the alert manager config we have a

00:04:04,080 --> 00:04:08,080
route that matches

00:04:05,360 --> 00:04:09,360
a label of severity heartbeat and then

00:04:08,080 --> 00:04:11,439
sends that to

00:04:09,360 --> 00:04:14,000
a particular heartbeat receiver and

00:04:11,439 --> 00:04:17,600
you'll see in this example the url has a

00:04:14,000 --> 00:04:19,919
id in it which would be team specific or

00:04:17,600 --> 00:04:21,519
um specific to each prometheus instance

00:04:19,919 --> 00:04:24,800
that is monitored by

00:04:21,519 --> 00:04:26,800
the receiver at the other end which

00:04:24,800 --> 00:04:28,080
unfortunately then means that this alert

00:04:26,800 --> 00:04:31,120
manager file

00:04:28,080 --> 00:04:32,320
needs to have every id for everything

00:04:31,120 --> 00:04:34,800
that is monitored

00:04:32,320 --> 00:04:36,720
in it um obviously that's not too

00:04:34,800 --> 00:04:40,560
difficult so it can be templated

00:04:36,720 --> 00:04:42,240
or various other approaches but

00:04:40,560 --> 00:04:43,680
it still means that this is yet another

00:04:42,240 --> 00:04:46,800
thing to configure

00:04:43,680 --> 00:04:47,840
and the configuration needs to be

00:04:46,800 --> 00:04:51,040
managed and

00:04:47,840 --> 00:04:53,759
so on um

00:04:51,040 --> 00:04:54,320
it's yet another moving part essentially

00:04:53,759 --> 00:04:57,600
so

00:04:54,320 --> 00:04:59,440
instead with prom msd we have the same

00:04:57,600 --> 00:05:01,280
alert that we had before

00:04:59,440 --> 00:05:02,639
but in this alert you'll see that there

00:05:01,280 --> 00:05:06,320
are some

00:05:02,639 --> 00:05:08,880
annotations that have msd

00:05:06,320 --> 00:05:10,000
at the start of them which essentially

00:05:08,880 --> 00:05:12,800
tell prom msd

00:05:10,000 --> 00:05:14,639
how it should behave um the activation

00:05:12,800 --> 00:05:17,440
is the activation time

00:05:14,639 --> 00:05:19,600
um some labels to override and then the

00:05:17,440 --> 00:05:21,840
alert managers to send the alert to

00:05:19,600 --> 00:05:24,320
which is unfortunately the one thing

00:05:21,840 --> 00:05:27,680
that primosd

00:05:24,320 --> 00:05:29,840
compromises on it can't support dynamic

00:05:27,680 --> 00:05:31,360
alert manager discovery because the

00:05:29,840 --> 00:05:34,479
allowed managers have to be

00:05:31,360 --> 00:05:36,960
actually specified in the alert itself

00:05:34,479 --> 00:05:37,759
although potentially we could fix that

00:05:36,960 --> 00:05:41,039
with some

00:05:37,759 --> 00:05:42,240
changes elsewhere in the future um but

00:05:41,039 --> 00:05:44,720
this does mean that all the

00:05:42,240 --> 00:05:46,720
configuration for a team's

00:05:44,720 --> 00:05:48,000
alert is actually contained in the alert

00:05:46,720 --> 00:05:50,320
itself

00:05:48,000 --> 00:05:51,520
and nothing special is needed for

00:05:50,320 --> 00:05:52,720
heartbeat events

00:05:51,520 --> 00:05:54,720
obviously they probably would have

00:05:52,720 --> 00:05:56,960
team-specific routing in the

00:05:54,720 --> 00:05:57,840
central alert manager but they don't

00:05:56,960 --> 00:06:00,000
need

00:05:57,840 --> 00:06:02,000
separate configuration for heartbeats

00:06:00,000 --> 00:06:03,199
which you know might get forgotten or so

00:06:02,000 --> 00:06:06,800
on because it's not

00:06:03,199 --> 00:06:09,280
used all the time and so on

00:06:06,800 --> 00:06:11,600
so what then happens is this in this

00:06:09,280 --> 00:06:15,039
case raises two alerts for each of our

00:06:11,600 --> 00:06:18,479
um availability pair and those go to

00:06:15,039 --> 00:06:21,680
problem sd so let's actually see how

00:06:18,479 --> 00:06:24,720
this works

00:06:21,680 --> 00:06:29,680
so over here i have

00:06:24,720 --> 00:06:31,840
um some of the example configs that come

00:06:29,680 --> 00:06:34,240
with prominence d so it's just a

00:06:31,840 --> 00:06:35,120
conflict directory and i'm running four

00:06:34,240 --> 00:06:37,919
terminals

00:06:35,120 --> 00:06:39,520
here um first of all i'm just running a

00:06:37,919 --> 00:06:42,080
netcat listening on a

00:06:39,520 --> 00:06:44,240
random port this is going to be the

00:06:42,080 --> 00:06:45,440
normal alert receiver so we'll just see

00:06:44,240 --> 00:06:47,850
the http requests

00:06:45,440 --> 00:06:49,199
sent to that so

00:06:47,850 --> 00:06:52,240
[Music]

00:06:49,199 --> 00:06:54,319
inside prime msd this configs directory

00:06:52,240 --> 00:06:57,599
has an alert manager config

00:06:54,319 --> 00:06:59,360
an alert and a prometheus config

00:06:57,599 --> 00:07:01,520
so what i'm going to do is i'm just

00:06:59,360 --> 00:07:04,800
going to run alert manager

00:07:01,520 --> 00:07:07,280
using that pre-provided config um

00:07:04,800 --> 00:07:08,080
i'm also just going to run prometheus

00:07:07,280 --> 00:07:12,000
and

00:07:08,080 --> 00:07:13,280
prometheus will then be running so

00:07:12,000 --> 00:07:17,440
you'll notice i haven't yet started

00:07:13,280 --> 00:07:20,880
prominence d so i also just need to do

00:07:17,440 --> 00:07:23,759
that so we now have previous alert

00:07:20,880 --> 00:07:26,240
manager and prime msd all running

00:07:23,759 --> 00:07:27,759
and let's just first of all go to

00:07:26,240 --> 00:07:30,240
prometheus here

00:07:27,759 --> 00:07:30,960
and if we look at the alerts ui you now

00:07:30,240 --> 00:07:34,479
see

00:07:30,960 --> 00:07:37,440
this expected on that heartbeat is

00:07:34,479 --> 00:07:39,039
active and we can see as i discussed all

00:07:37,440 --> 00:07:40,639
the activation things

00:07:39,039 --> 00:07:43,039
you'll see in this case though that i've

00:07:40,639 --> 00:07:45,440
put the activation at one minute

00:07:43,039 --> 00:07:46,960
you also notice this alert for now is

00:07:45,440 --> 00:07:48,800
not actually active

00:07:46,960 --> 00:07:50,720
because there's a full threshold of 30

00:07:48,800 --> 00:07:53,840
seconds just to make sure that

00:07:50,720 --> 00:07:56,000
the previous instance isn't flapping

00:07:53,840 --> 00:07:57,840
so this amount is still pending

00:07:56,000 --> 00:07:58,560
hopefully i've spoken for long enough i

00:07:57,840 --> 00:08:01,680
have

00:07:58,560 --> 00:08:03,360
and that alert is now firing so that now

00:08:01,680 --> 00:08:04,560
means that we have an expectation of

00:08:03,360 --> 00:08:07,360
that heartbeat

00:08:04,560 --> 00:08:08,479
that is fight rank so what's happening

00:08:07,360 --> 00:08:10,879
to that

00:08:08,479 --> 00:08:12,000
well that is going to alert manager

00:08:10,879 --> 00:08:15,280
which

00:08:12,000 --> 00:08:17,199
conveniently i have running here um

00:08:15,280 --> 00:08:19,840
[Music]

00:08:17,199 --> 00:08:22,240
and we now see there's an alert for

00:08:19,840 --> 00:08:25,919
expected alright heartbeat over here

00:08:22,240 --> 00:08:27,280
that we have the relevant annotations on

00:08:25,919 --> 00:08:30,560
um

00:08:27,280 --> 00:08:34,159
and if we check where that's going

00:08:30,560 --> 00:08:36,959
that's going to rob a misd and actually

00:08:34,159 --> 00:08:37,599
our pager has no one that's going to

00:08:36,959 --> 00:08:40,880
okay

00:08:37,599 --> 00:08:44,720
so then if we go over to prominence d

00:08:40,880 --> 00:08:47,920
over here we'll see we just have

00:08:44,720 --> 00:08:49,360
a prometheus um and in this case

00:08:47,920 --> 00:08:51,440
it's not running in kubernetes so

00:08:49,360 --> 00:08:51,839
there's no namespace or anything it's

00:08:51,440 --> 00:08:54,000
just

00:08:51,839 --> 00:08:55,040
prometheus which obviously in a real

00:08:54,000 --> 00:08:57,760
setup you would have

00:08:55,040 --> 00:08:59,360
a few more labels there but this for a

00:08:57,760 --> 00:09:02,640
demo this works

00:08:59,360 --> 00:09:05,360
so you'll see that that is

00:09:02,640 --> 00:09:07,120
saying it will activate in a few seconds

00:09:05,360 --> 00:09:09,600
and i've actually got this

00:09:07,120 --> 00:09:10,480
set i think to repeat every five seconds

00:09:09,600 --> 00:09:12,880
so

00:09:10,480 --> 00:09:14,080
if i just sit reloading this page you'll

00:09:12,880 --> 00:09:17,440
see

00:09:14,080 --> 00:09:21,120
it actually never gets below

00:09:17,440 --> 00:09:24,000
about 55 seconds so

00:09:21,120 --> 00:09:24,640
now let's just go to where we are

00:09:24,000 --> 00:09:27,760
running

00:09:24,640 --> 00:09:30,880
uh prometheus and i'll just kill it

00:09:27,760 --> 00:09:35,440
okay so that was pretty fierce yes

00:09:30,880 --> 00:09:40,800
so i've now stopped prometheus so

00:09:35,440 --> 00:09:40,800
actually that's interesting because

00:09:40,880 --> 00:09:47,120
you'll see this alert is now still

00:09:44,320 --> 00:09:48,240
active and that's because alert manager

00:09:47,120 --> 00:09:51,519
over here

00:09:48,240 --> 00:09:54,560
still knows about this alert for now

00:09:51,519 --> 00:09:57,600
um i've actually set the

00:09:54,560 --> 00:09:58,560
in the prometheus config the evaluation

00:09:57,600 --> 00:10:02,480
interval to

00:09:58,560 --> 00:10:03,040
15 seconds so if i carry on talking for

00:10:02,480 --> 00:10:07,279
about

00:10:03,040 --> 00:10:09,600
four times 15 seconds um

00:10:07,279 --> 00:10:11,360
we should eventually find that that

00:10:09,600 --> 00:10:13,440
alert stops being sent

00:10:11,360 --> 00:10:19,839
uh luckily this stuff isn't live so if

00:10:13,440 --> 00:10:19,839
this fails i'll just edit it

00:10:23,600 --> 00:10:27,760
okay so it's now about to activate i'm

00:10:26,000 --> 00:10:30,399
just hitting refresh here so you can see

00:10:27,760 --> 00:10:30,399
what's happening

00:10:30,880 --> 00:10:35,839
there we go so it's now gone red and

00:10:33,760 --> 00:10:37,440
says it's sent in alert

00:10:35,839 --> 00:10:39,040
so if we go back to this alert measure

00:10:37,440 --> 00:10:41,200
and have a look

00:10:39,040 --> 00:10:44,399
yep the heartbeat's disappeared and we

00:10:41,200 --> 00:10:47,839
now have a no alert connectivity alert

00:10:44,399 --> 00:10:51,120
and in theory

00:10:47,839 --> 00:10:54,959
if we go here

00:10:51,120 --> 00:11:01,839
we should actually get

00:10:54,959 --> 00:11:01,839
that delivered to us um

00:11:02,480 --> 00:11:08,800
there we go so we've now

00:11:05,600 --> 00:11:15,839
been told we have no relay connectivity

00:11:08,800 --> 00:11:15,839
so that's how problematic works

00:11:17,519 --> 00:11:20,640
okay so obviously that's a very simple

00:11:20,079 --> 00:11:22,720
setup

00:11:20,640 --> 00:11:24,240
and in reality you'd have a few more

00:11:22,720 --> 00:11:27,600
components involved

00:11:24,240 --> 00:11:29,440
so um a full architecture of deploying

00:11:27,600 --> 00:11:30,240
it might look something like this you

00:11:29,440 --> 00:11:32,800
have

00:11:30,240 --> 00:11:33,360
three teams running prometheus instances

00:11:32,800 --> 00:11:35,440
for

00:11:33,360 --> 00:11:36,720
applications which talk to an alert

00:11:35,440 --> 00:11:38,399
manager cluster

00:11:36,720 --> 00:11:40,320
the alert manager cluster routes to

00:11:38,399 --> 00:11:43,519
prominence d as well as

00:11:40,320 --> 00:11:46,320
things in the cloud for other alerting

00:11:43,519 --> 00:11:47,200
um as well as an infrastructure

00:11:46,320 --> 00:11:50,399
prometheus

00:11:47,200 --> 00:11:52,560
for example that rather than

00:11:50,399 --> 00:11:54,399
using the prominence d running locally

00:11:52,560 --> 00:11:57,120
in the cluster

00:11:54,399 --> 00:11:57,760
uses something in the cloud which could

00:11:57,120 --> 00:12:00,480
also be

00:11:57,760 --> 00:12:01,360
a another instance of promo sd running

00:12:00,480 --> 00:12:03,360
elsewhere

00:12:01,360 --> 00:12:04,720
or it could be one of the mentioned

00:12:03,360 --> 00:12:08,160
cloud monitoring

00:12:04,720 --> 00:12:09,360
services um and you'll also notice if

00:12:08,160 --> 00:12:12,880
you follow the red line

00:12:09,360 --> 00:12:15,279
that if prime msd here

00:12:12,880 --> 00:12:16,160
detects that there's a problem it sends

00:12:15,279 --> 00:12:18,720
it to

00:12:16,160 --> 00:12:20,240
alert manager but it also sends it to a

00:12:18,720 --> 00:12:23,360
webhook receiver which

00:12:20,240 --> 00:12:26,560
goes straight to something elsewhere

00:12:23,360 --> 00:12:27,760
um which means promise d doesn't need to

00:12:26,560 --> 00:12:30,800
depend on anything

00:12:27,760 --> 00:12:32,399
other than a weapon receiver which could

00:12:30,800 --> 00:12:35,519
run on the same machine or

00:12:32,399 --> 00:12:38,079
you know even in the same pod as a as

00:12:35,519 --> 00:12:41,360
prominent d in kubernetes

00:12:38,079 --> 00:12:42,720
um and yeah as as mentioned the

00:12:41,360 --> 00:12:46,000
infrastructure prometheus has

00:12:42,720 --> 00:12:47,600
a separate monitoring that is

00:12:46,000 --> 00:12:49,200
potentially in a different cluster or

00:12:47,600 --> 00:12:51,200
elsewhere so

00:12:49,200 --> 00:12:53,760
the infrastructure team can be notified

00:12:51,200 --> 00:12:57,519
if everything is broken

00:12:53,760 --> 00:12:59,040
um application teams can be notified if

00:12:57,519 --> 00:13:00,639
their prometheus is broken by an

00:12:59,040 --> 00:13:03,279
explicit alert

00:13:00,639 --> 00:13:04,959
but if they actually are running in

00:13:03,279 --> 00:13:06,959
multiple clusters maybe they don't need

00:13:04,959 --> 00:13:09,760
to be told about

00:13:06,959 --> 00:13:11,680
their actual application being down if

00:13:09,760 --> 00:13:14,959
it's an infrastructure problem because

00:13:11,680 --> 00:13:16,880
the probers don't fail um and

00:13:14,959 --> 00:13:18,800
it means that you know you don't get a

00:13:16,880 --> 00:13:19,600
critical alert everything is broken when

00:13:18,800 --> 00:13:21,519
actually

00:13:19,600 --> 00:13:22,800
it's not all broken so there's

00:13:21,519 --> 00:13:25,200
flexibility in

00:13:22,800 --> 00:13:26,160
how you how you set this up that means

00:13:25,200 --> 00:13:29,040
you can make sure

00:13:26,160 --> 00:13:29,920
that the alerts are actually actionable

00:13:29,040 --> 00:13:33,279
and

00:13:29,920 --> 00:13:35,680
so on so

00:13:33,279 --> 00:13:36,480
prime msd is now open source and it's

00:13:35,680 --> 00:13:39,440
available on

00:13:36,480 --> 00:13:40,639
our github there so thank you to do

00:13:39,440 --> 00:13:42,480
research for

00:13:40,639 --> 00:13:44,399
supporting my work and open sourcing

00:13:42,480 --> 00:13:49,279
this and

00:13:44,399 --> 00:13:49,279

YouTube URL: https://www.youtube.com/watch?v=uklzgLPUpzY


