Title: Ups, let's revert! ðŸ”¥ Automating Kubernetes Rollouts with Argo a... Bartlomiej PÅ‚otka & Anais Urlichs
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Donâ€™t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Ups, let's revert! ðŸ”¥ Automating Kubernetes Rollouts with Argo and Prometheus for HA - Bartlomiej PÅ‚otka & Anais Urlichs, Codefresh

"Thanks to Kubernetes, deploying and running applications in the cloud was never easier. Unfortunately, with the capabilities, desire for highly reliable services with strict minimal downtime SLO grows too. And we all know what increases risk of incidents: Upgrades!
Vanilla rollouts functionality works great for basic needs, but it lacks many advanced deployment strategies which can drastically reduce distruption time during critical production promotions, especially those failed ones. Thatâ€™s were Argo Rollouts comes in, solving this need by introducing tiny shim over Kuberentes Deployment object. But with the power, comes the complexity and without proper observability and monitoring it can become unmanagable pretty quickly.

In this talk you will see demo showing an efficient way of setting up critical Prometheus monitoring and alerting for advanced Kubernetes deployment operations Argo allows. You will learn how to observe the progress of your rollouts, Blue-Green or Canary deployments, how to perform rollbacks without human intervention and how to only alert when humans have to be called!"
Captions: 
	00:00:00,080 --> 00:00:04,000
hello hello everyone uh welcome to prom

00:00:02,399 --> 00:00:05,920
con 2021

00:00:04,000 --> 00:00:07,680
um i hope you have a great time in this

00:00:05,920 --> 00:00:11,440
conference soon we have kubecon 2

00:00:07,680 --> 00:00:13,120
kubecon um two and yeah we are excited

00:00:11,440 --> 00:00:15,920
to be here and show you

00:00:13,120 --> 00:00:16,960
some amazing scenarios uh related to

00:00:15,920 --> 00:00:20,320
promote use

00:00:16,960 --> 00:00:21,600
and the ecosystem hello everybody really

00:00:20,320 --> 00:00:23,680
excited to be here

00:00:21,600 --> 00:00:25,840
my name is anes orlese i'm a site

00:00:23,680 --> 00:00:27,760
reliability engineer at sivo

00:00:25,840 --> 00:00:28,880
sivo is a cloud computing company based

00:00:27,760 --> 00:00:30,640
on kubernetes

00:00:28,880 --> 00:00:32,399
before that i was working at code fresh

00:00:30,640 --> 00:00:34,719
which is a devops automation platform

00:00:32,399 --> 00:00:36,239
i'm also a cncf ambassador and i run a

00:00:34,719 --> 00:00:36,640
youtube channel around a challenge

00:00:36,239 --> 00:00:38,800
called

00:00:36,640 --> 00:00:40,399
100 days of kubernetes if any of this

00:00:38,800 --> 00:00:43,040
got you curious you can check out my

00:00:40,399 --> 00:00:46,960
twitter at holy sonny's also linked

00:00:43,040 --> 00:00:48,640
there yeah my name is bartek

00:00:46,960 --> 00:00:50,160
platka and i'm principal software

00:00:48,640 --> 00:00:53,600
engineer at red hat

00:00:50,160 --> 00:00:54,719
i'm part of the team which is uh

00:00:53,600 --> 00:00:56,640
you know responsible for the

00:00:54,719 --> 00:00:58,480
observability and monitoring for red

00:00:56,640 --> 00:01:01,760
card including openshift as well

00:00:58,480 --> 00:01:04,320
and uh yeah we are maintaining

00:01:01,760 --> 00:01:05,280
lots of projects around that part of the

00:01:04,320 --> 00:01:07,760
infrastructure

00:01:05,280 --> 00:01:09,600
i'm maintaining prometheus um i'm

00:01:07,760 --> 00:01:11,520
co-author of the thanos project

00:01:09,600 --> 00:01:12,960
and i'm maintaining other you know go

00:01:11,520 --> 00:01:15,759
like repositories all around

00:01:12,960 --> 00:01:16,400
around the github i don't have youtube

00:01:15,759 --> 00:01:18,640
channel

00:01:16,400 --> 00:01:20,000
i sometimes blog posting i'm writing my

00:01:18,640 --> 00:01:23,040
book with aurelie

00:01:20,000 --> 00:01:24,479
and i'm uh cncf seek observability

00:01:23,040 --> 00:01:27,920
technique as well

00:01:24,479 --> 00:01:29,759
so yeah welcome awesome so what do we

00:01:27,920 --> 00:01:30,960
have prepared for you for the next 20

00:01:29,759 --> 00:01:34,240
minutes or so

00:01:30,960 --> 00:01:35,119
we have a little ping-pong application

00:01:34,240 --> 00:01:37,439
of which we're going to show you

00:01:35,119 --> 00:01:39,360
different scenarios and in addition to

00:01:37,439 --> 00:01:42,640
that we have a client that's gonna

00:01:39,360 --> 00:01:43,759
request um requests information from

00:01:42,640 --> 00:01:44,960
that application

00:01:43,759 --> 00:01:46,479
and those are both micro service

00:01:44,960 --> 00:01:48,320
application and then in addition to that

00:01:46,479 --> 00:01:49,920
we are going to use prometheus and argo

00:01:48,320 --> 00:01:52,159
rollouts now aggro rollouts are going to

00:01:49,920 --> 00:01:54,159
be used to automate our deployments

00:01:52,159 --> 00:01:55,920
and prometheus is going to be used to

00:01:54,159 --> 00:01:57,759
out like basically to help

00:01:55,920 --> 00:01:58,960
our worlds to know what's happening

00:01:57,759 --> 00:02:01,119
within the rollouts

00:01:58,960 --> 00:02:02,719
to in case something happens wrong to

00:02:01,119 --> 00:02:04,479
revert our wallets as well

00:02:02,719 --> 00:02:07,119
so let's jump right into our demo and

00:02:04,479 --> 00:02:08,959
show you what we have set up

00:02:07,119 --> 00:02:10,800
as you can see here we are in katakota

00:02:08,959 --> 00:02:12,720
katakura is an online platform

00:02:10,800 --> 00:02:14,879
that allows you to spin up demo

00:02:12,720 --> 00:02:16,400
environments such as this one you can

00:02:14,879 --> 00:02:17,280
also try out the scenario at the end

00:02:16,400 --> 00:02:18,720
we're going to give you all of the

00:02:17,280 --> 00:02:20,319
information so you can check out the

00:02:18,720 --> 00:02:21,200
repository and the code and also the

00:02:20,319 --> 00:02:22,879
scenario

00:02:21,200 --> 00:02:24,640
now we have our cuban news cluster set

00:02:22,879 --> 00:02:25,520
up here we have nothing on it right now

00:02:24,640 --> 00:02:28,640
and we're going to just

00:02:25,520 --> 00:02:30,720
go ahead and create a namespace

00:02:28,640 --> 00:02:31,920
and now to start with every

00:02:30,720 --> 00:02:33,360
infrastructure has to have some

00:02:31,920 --> 00:02:35,840
monitoring so

00:02:33,360 --> 00:02:37,440
we start installing our resources from

00:02:35,840 --> 00:02:39,360
the observability

00:02:37,440 --> 00:02:41,040
components so first we start on the

00:02:39,360 --> 00:02:43,120
right you can see prometheus

00:02:41,040 --> 00:02:44,080
um components so we install promote use

00:02:43,120 --> 00:02:46,239
one binary

00:02:44,080 --> 00:02:48,239
and it's supposed to gather metrics and

00:02:46,239 --> 00:02:49,440
exemplars and i will tell later what

00:02:48,239 --> 00:02:52,879
example means and

00:02:49,440 --> 00:02:53,519
why we need that we also can do alerting

00:02:52,879 --> 00:02:55,760
on this

00:02:53,519 --> 00:02:56,720
on this node so it's kind of super super

00:02:55,760 --> 00:02:58,560
useful and

00:02:56,720 --> 00:03:00,480
promote use will gather those metrics

00:02:58,560 --> 00:03:02,000
and examples from any components

00:03:00,480 --> 00:03:03,120
including other components you can see

00:03:02,000 --> 00:03:05,040
on this diagram

00:03:03,120 --> 00:03:07,120
so we have also graphana which is will

00:03:05,040 --> 00:03:09,040
be our single point of access so it will

00:03:07,120 --> 00:03:11,360
be our ui our front end

00:03:09,040 --> 00:03:13,040
and uh we can you know we statically

00:03:11,360 --> 00:03:15,280
configure some dashboards

00:03:13,040 --> 00:03:16,560
and that will allow us to see the

00:03:15,280 --> 00:03:18,640
progress on our rollout

00:03:16,560 --> 00:03:20,480
and you know the health of our system

00:03:18,640 --> 00:03:22,800
and how it impacts or not

00:03:20,480 --> 00:03:23,519
our customers and we have our secret

00:03:22,800 --> 00:03:26,560
agent here

00:03:23,519 --> 00:03:27,599
temple on the left and it's uh it's like

00:03:26,560 --> 00:03:30,959
a cool project

00:03:27,599 --> 00:03:33,840
um really with similar architecture than

00:03:30,959 --> 00:03:34,480
than other systems we see like thanos

00:03:33,840 --> 00:03:37,680
cortex

00:03:34,480 --> 00:03:39,200
or or loki and prometheus with labeling

00:03:37,680 --> 00:03:41,040
mechanics similar to that

00:03:39,200 --> 00:03:43,280
and it's about to it's this

00:03:41,040 --> 00:03:45,680
responsibility is to gather traces

00:03:43,280 --> 00:03:47,120
which we will use for the later part of

00:03:45,680 --> 00:03:48,799
this demo

00:03:47,120 --> 00:03:51,200
now we went ahead and we applied those

00:03:48,799 --> 00:03:51,760
resources now those resources will take

00:03:51,200 --> 00:03:54,159
a few

00:03:51,760 --> 00:03:55,599
seconds to spin up completely in the

00:03:54,159 --> 00:03:56,080
meantime we're going to take a look at

00:03:55,599 --> 00:03:58,319
our

00:03:56,080 --> 00:03:59,120
rollouts and going to apply our wallets

00:03:58,319 --> 00:04:01,200
resources

00:03:59,120 --> 00:04:04,159
now what is aggro rollouts exactly it's

00:04:01,200 --> 00:04:06,640
an operator and custom visas definitions

00:04:04,159 --> 00:04:08,400
and both are used to then deploy other

00:04:06,640 --> 00:04:09,280
resources for our rollouts to make our

00:04:08,400 --> 00:04:11,840
wallets happen

00:04:09,280 --> 00:04:12,879
and basically use advanced deployment

00:04:11,840 --> 00:04:14,879
strategies

00:04:12,879 --> 00:04:16,400
such as progressive delivery blue green

00:04:14,879 --> 00:04:17,919
deployments as well as canary

00:04:16,400 --> 00:04:20,239
deployments now in our case we're going

00:04:17,919 --> 00:04:22,160
to focus on canary deployments

00:04:20,239 --> 00:04:23,759
so let's take a look at the aggro

00:04:22,160 --> 00:04:24,400
resources at the yamafest that we're

00:04:23,759 --> 00:04:27,360
actually

00:04:24,400 --> 00:04:28,800
deploying and using agro rollouts with

00:04:27,360 --> 00:04:30,880
at the beginning we have our rollout

00:04:28,800 --> 00:04:32,720
yammer file now the roller demo file is

00:04:30,880 --> 00:04:33,360
similar to a deployment resource that

00:04:32,720 --> 00:04:34,960
you might be

00:04:33,360 --> 00:04:37,280
familiar with if you're at this

00:04:34,960 --> 00:04:39,360
conference and

00:04:37,280 --> 00:04:41,040
it basically adds additional information

00:04:39,360 --> 00:04:43,360
to our deployment resource

00:04:41,040 --> 00:04:45,520
so we can see over here that we also

00:04:43,360 --> 00:04:48,320
specify the container that we want to

00:04:45,520 --> 00:04:48,639
that we want to use and the replica set

00:04:48,320 --> 00:04:50,080
of

00:04:48,639 --> 00:04:52,240
our deployment so in this case we're

00:04:50,080 --> 00:04:54,560
going to use a replica set of five

00:04:52,240 --> 00:04:55,440
if you're not using a service mesh you

00:04:54,560 --> 00:04:57,759
will have to

00:04:55,440 --> 00:04:59,520
use a minimum replica set of five so for

00:04:57,759 --> 00:05:00,400
aggro rollers to be able to distribute

00:04:59,520 --> 00:05:02,880
the traffic

00:05:00,400 --> 00:05:03,919
and move traffic between your deployment

00:05:02,880 --> 00:05:06,000
resources

00:05:03,919 --> 00:05:07,120
so that is specified in the steps

00:05:06,000 --> 00:05:08,400
section

00:05:07,120 --> 00:05:10,320
we're going to start off with a new

00:05:08,400 --> 00:05:13,360
deployment uh with a

00:05:10,320 --> 00:05:15,600
rate of traffic rate of 20 so that means

00:05:13,360 --> 00:05:18,320
whenever we roll out a new deployment

00:05:15,600 --> 00:05:20,400
of our application it's gonna first just

00:05:18,320 --> 00:05:22,320
spin up one part and scale down

00:05:20,400 --> 00:05:23,440
one of the existing five parts of our

00:05:22,320 --> 00:05:25,759
initial deployment

00:05:23,440 --> 00:05:27,440
and then with an interval of first 60

00:05:25,759 --> 00:05:30,080
seconds and then 30 seconds

00:05:27,440 --> 00:05:32,160
it's going to gradually roll out the new

00:05:30,080 --> 00:05:33,440
deployment with this resource based on

00:05:32,160 --> 00:05:34,960
this resource

00:05:33,440 --> 00:05:37,039
another interesting section is our

00:05:34,960 --> 00:05:39,520
analysis now this

00:05:37,039 --> 00:05:40,560
refers to an analysis template that is

00:05:39,520 --> 00:05:42,479
used to check

00:05:40,560 --> 00:05:44,960
whether or not our role is actually

00:05:42,479 --> 00:05:46,960
successful if we don't have the section

00:05:44,960 --> 00:05:48,000
agar bullets will just gradually shift

00:05:46,960 --> 00:05:50,560
the traffic

00:05:48,000 --> 00:05:52,400
and roll it like it is and as soon as

00:05:50,560 --> 00:05:53,840
long as the pods are running correctly

00:05:52,400 --> 00:05:54,880
within our cluster we'll just go ahead

00:05:53,840 --> 00:05:56,800
and roll it out

00:05:54,880 --> 00:05:58,160
now that's obviously not what we want

00:05:56,800 --> 00:05:59,039
because that would require us to

00:05:58,160 --> 00:06:01,360
manually

00:05:59,039 --> 00:06:03,680
check whether or not our roller is okay

00:06:01,360 --> 00:06:04,400
instead we want to utilize the analysis

00:06:03,680 --> 00:06:08,560
template

00:06:04,400 --> 00:06:08,560
and prometheus to automate this process

00:06:09,600 --> 00:06:15,280
right so on a rollout section

00:06:12,800 --> 00:06:16,080
above we specify certain template that

00:06:15,280 --> 00:06:19,120
we used

00:06:16,080 --> 00:06:21,199
and our template is called low error

00:06:19,120 --> 00:06:24,639
load latency

00:06:21,199 --> 00:06:26,479
template and we specify here the exact

00:06:24,639 --> 00:06:28,479
you know data what it means for rollout

00:06:26,479 --> 00:06:31,600
to be successful or not

00:06:28,479 --> 00:06:33,680
and we kind of base those criteria right

00:06:31,600 --> 00:06:34,400
now in our demo based on the two metrics

00:06:33,680 --> 00:06:37,120
we have

00:06:34,400 --> 00:06:38,319
one metric uh is lower than 20p error

00:06:37,120 --> 00:06:41,199
rate and second thing

00:06:38,319 --> 00:06:42,000
is latency 90p lower than one second

00:06:41,199 --> 00:06:44,319
what that means

00:06:42,000 --> 00:06:45,520
well essentially first of all our error

00:06:44,319 --> 00:06:48,080
rate metric

00:06:45,520 --> 00:06:49,360
is supposed to give us uh you know the

00:06:48,080 --> 00:06:52,000
overview of the error rate

00:06:49,360 --> 00:06:53,120
of of uh pings that are happening in the

00:06:52,000 --> 00:06:56,400
overall system

00:06:53,120 --> 00:07:00,479
and we expect the air rate to be lower

00:06:56,400 --> 00:07:02,639
um than uh 20 so essentially success 80

00:07:00,479 --> 00:07:04,080
of the request overall in our system has

00:07:02,639 --> 00:07:06,800
to be successful

00:07:04,080 --> 00:07:07,759
now uh we do that much very often like

00:07:06,800 --> 00:07:10,080
we just we do

00:07:07,759 --> 00:07:11,919
this check every 20 seconds normally you

00:07:10,080 --> 00:07:13,599
probably you you like to do this every

00:07:11,919 --> 00:07:15,360
minute or every five minutes

00:07:13,599 --> 00:07:17,280
and but we wanted to kind of you know

00:07:15,360 --> 00:07:19,599
speed up this demo and we have a

00:07:17,280 --> 00:07:20,319
very aggressive scrape interval on pro

00:07:19,599 --> 00:07:22,479
materials

00:07:20,319 --> 00:07:23,520
five seconds uh normally you should have

00:07:22,479 --> 00:07:25,520
probably 15.

00:07:23,520 --> 00:07:27,199
now this is only one metric second

00:07:25,520 --> 00:07:30,400
metric called latency 90p

00:07:27,199 --> 00:07:30,800
lower than one second we are observing

00:07:30,400 --> 00:07:33,599
the

00:07:30,800 --> 00:07:35,039
tail latency of our application again

00:07:33,599 --> 00:07:38,880
for all the customers

00:07:35,039 --> 00:07:42,240
and um we expect the tail latency to be

00:07:38,880 --> 00:07:43,120
uh lower than one second so most of the

00:07:42,240 --> 00:07:46,319
users

00:07:43,120 --> 00:07:48,960
90 of the users should have that latency

00:07:46,319 --> 00:07:49,440
if not then we claim this operation to

00:07:48,960 --> 00:07:51,759
be

00:07:49,440 --> 00:07:53,759
uh to be failed and rollout to be

00:07:51,759 --> 00:07:54,560
reverted and we can see also the failure

00:07:53,759 --> 00:07:57,280
limit

00:07:54,560 --> 00:07:58,960
um this is kind of important uh you know

00:07:57,280 --> 00:07:59,759
additional info that you can provide

00:07:58,960 --> 00:08:03,440
which uh

00:07:59,759 --> 00:08:06,240
tells argo rollout to wait for free

00:08:03,440 --> 00:08:07,120
failures for this metric and only three

00:08:06,240 --> 00:08:09,680
failures

00:08:07,120 --> 00:08:11,520
actually trigger um you know the failure

00:08:09,680 --> 00:08:12,080
of the whole deployment this is to avoid

00:08:11,520 --> 00:08:15,039
some

00:08:12,080 --> 00:08:15,840
um spikes or some fluckinesses as well

00:08:15,039 --> 00:08:18,479
and now

00:08:15,840 --> 00:08:18,879
you can see our diagram expanded to

00:08:18,479 --> 00:08:20,560
those

00:08:18,879 --> 00:08:22,000
new two new resources we are just

00:08:20,560 --> 00:08:23,840
rolling out

00:08:22,000 --> 00:08:25,840
uh we have argo rollat which is our

00:08:23,840 --> 00:08:26,879
rollout you know operator actually

00:08:25,840 --> 00:08:29,520
controller

00:08:26,879 --> 00:08:31,039
in in details and it is you know

00:08:29,520 --> 00:08:33,440
managing those applications

00:08:31,039 --> 00:08:35,360
and distributing traffic and and as and

00:08:33,440 --> 00:08:36,159
i said we don't need to have service

00:08:35,360 --> 00:08:37,839
match for that

00:08:36,159 --> 00:08:40,399
we don't need to have a special ingress

00:08:37,839 --> 00:08:41,919
although those are supported as well

00:08:40,399 --> 00:08:43,599
right now we are just using plain

00:08:41,919 --> 00:08:45,360
rollout controller which allows us to

00:08:43,599 --> 00:08:47,600
distribute this is that the load

00:08:45,360 --> 00:08:48,720
and you can see our grow up is also

00:08:47,600 --> 00:08:50,880
checking prematures

00:08:48,720 --> 00:08:53,360
from time to time to check latency and

00:08:50,880 --> 00:08:53,360
error rate

00:08:53,440 --> 00:08:56,560
now that we've deployed those resources

00:08:55,760 --> 00:08:59,279
we can check

00:08:56,560 --> 00:09:00,480
our rollout client whether or not our

00:08:59,279 --> 00:09:01,360
initial rollout has happened

00:09:00,480 --> 00:09:03,120
successfully

00:09:01,360 --> 00:09:04,800
so in this case we can see that five

00:09:03,120 --> 00:09:06,640
parts are currently up and running of

00:09:04,800 --> 00:09:08,560
our initial deployment of our initial

00:09:06,640 --> 00:09:10,640
image and it's called stable

00:09:08,560 --> 00:09:12,959
and we can see that any further rollouts

00:09:10,640 --> 00:09:14,800
will use the canary deployment strategy

00:09:12,959 --> 00:09:16,959
now once all of our resources up and

00:09:14,800 --> 00:09:18,720
running we can go ahead and open our

00:09:16,959 --> 00:09:20,000
grafana dashboard that provides our

00:09:18,720 --> 00:09:21,920
universal ui

00:09:20,000 --> 00:09:23,360
and shows us exactly what's happening

00:09:21,920 --> 00:09:27,120
within our application

00:09:23,360 --> 00:09:28,880
and the requests to our application

00:09:27,120 --> 00:09:30,640
so in this case we have several

00:09:28,880 --> 00:09:32,800
different sections we could be precise

00:09:30,640 --> 00:09:35,680
and different panels within each section

00:09:32,800 --> 00:09:36,880
now let's reduce the time for the past

00:09:35,680 --> 00:09:38,640
five minutes

00:09:36,880 --> 00:09:40,240
and have a look at the first section or

00:09:38,640 --> 00:09:41,040
user experience what does it actually

00:09:40,240 --> 00:09:43,200
tell us

00:09:41,040 --> 00:09:45,279
so this checks our ping application

00:09:43,200 --> 00:09:46,720
makes sure what is the percentage of our

00:09:45,279 --> 00:09:49,120
successful pinks

00:09:46,720 --> 00:09:50,160
um from that application and in this

00:09:49,120 --> 00:09:52,800
case we are right now at

00:09:50,160 --> 00:09:54,320
95 percent now additionally we can see

00:09:52,800 --> 00:09:57,440
in the next panel

00:09:54,320 --> 00:09:59,839
the overall latency per pinx from

00:09:57,440 --> 00:10:00,800
our paying application so we can see

00:09:59,839 --> 00:10:04,399
that this is the

00:10:00,800 --> 00:10:05,040
50 percent like how many users are

00:10:04,399 --> 00:10:08,480
seeing

00:10:05,040 --> 00:10:12,720
that latency below 0.4 seconds

00:10:08,480 --> 00:10:14,800
and how many are seeing about around 0.5

00:10:12,720 --> 00:10:16,560
seconds latency so basically how long

00:10:14,800 --> 00:10:19,519
does it take for a ping

00:10:16,560 --> 00:10:20,079
to be received by the fighter pinger

00:10:19,519 --> 00:10:22,079
from the

00:10:20,079 --> 00:10:23,760
punk to be received by the clinger from

00:10:22,079 --> 00:10:25,600
the pawn application

00:10:23,760 --> 00:10:27,200
additionally we can see here the overall

00:10:25,600 --> 00:10:30,480
success and error rate

00:10:27,200 --> 00:10:32,399
of our application so how many errors do

00:10:30,480 --> 00:10:33,760
do our customers actually receive when

00:10:32,399 --> 00:10:35,120
they open our application what is the

00:10:33,760 --> 00:10:38,720
percentage here

00:10:35,120 --> 00:10:40,880
yes so as mentioned the very first

00:10:38,720 --> 00:10:42,480
row is for the client experience so we

00:10:40,880 --> 00:10:43,360
take those metrics directly from the

00:10:42,480 --> 00:10:46,240
client

00:10:43,360 --> 00:10:47,600
uh to have like you know the closest

00:10:46,240 --> 00:10:49,279
experience they have

00:10:47,600 --> 00:10:50,640
but now there are kind of useful

00:10:49,279 --> 00:10:51,920
information we can get from our

00:10:50,640 --> 00:10:53,680
infrastructure as well

00:10:51,920 --> 00:10:55,839
so the below metrics are from our

00:10:53,680 --> 00:10:57,920
servers so first of all we have a

00:10:55,839 --> 00:10:58,640
certain metrics from the server app

00:10:57,920 --> 00:11:00,959
directly

00:10:58,640 --> 00:11:02,560
the same error rate to make sure we see

00:11:00,959 --> 00:11:04,640
the similar error

00:11:02,560 --> 00:11:07,279
success rates between the client and the

00:11:04,640 --> 00:11:09,040
server we see the latency as well

00:11:07,279 --> 00:11:11,040
and the difference is that we now

00:11:09,040 --> 00:11:12,880
visualize that in a second type of

00:11:11,040 --> 00:11:15,600
visualization you can do for

00:11:12,880 --> 00:11:16,640
for latency uh histogram metric that

00:11:15,600 --> 00:11:18,800
promotes

00:11:16,640 --> 00:11:19,680
allows so you can do percentiles but you

00:11:18,800 --> 00:11:22,000
can actually

00:11:19,680 --> 00:11:24,720
show this with the heat map and the heat

00:11:22,000 --> 00:11:27,040
map is kind of very natural and easy to

00:11:24,720 --> 00:11:28,560
to read from essentially the brighter

00:11:27,040 --> 00:11:32,000
the color the

00:11:28,560 --> 00:11:34,320
more amount of users are

00:11:32,000 --> 00:11:35,600
experiencing the latency within certain

00:11:34,320 --> 00:11:37,600
bucket so we can see

00:11:35,600 --> 00:11:40,640
most of the users are experiencing

00:11:37,600 --> 00:11:41,360
latency between 300 milliseconds and 600

00:11:40,640 --> 00:11:43,040
milliseconds

00:11:41,360 --> 00:11:44,720
so it's a kind of you know good

00:11:43,040 --> 00:11:45,760
experience overall there are some

00:11:44,720 --> 00:11:48,160
requests you can see

00:11:45,760 --> 00:11:49,040
which are even faster but but small

00:11:48,160 --> 00:11:51,680
amount of those

00:11:49,040 --> 00:11:53,680
and then on the right it's kind of the

00:11:51,680 --> 00:11:54,399
similar thing we can see on the panel

00:11:53,680 --> 00:11:58,000
above

00:11:54,399 --> 00:12:00,399
uh so it's an error error rate just we

00:11:58,000 --> 00:12:01,360
hide the success rate because uh from

00:12:00,399 --> 00:12:03,200
the you know

00:12:01,360 --> 00:12:04,399
sre perspective self-reliability

00:12:03,200 --> 00:12:06,079
engineer or devops

00:12:04,399 --> 00:12:07,920
you really want to focus on what is

00:12:06,079 --> 00:12:08,480
important for you and right now we want

00:12:07,920 --> 00:12:10,720
to care

00:12:08,480 --> 00:12:11,680
you know how many errors each version

00:12:10,720 --> 00:12:14,959
you know kind of

00:12:11,680 --> 00:12:15,600
exposes so we have some separations on

00:12:14,959 --> 00:12:18,480
the

00:12:15,600 --> 00:12:19,279
version which is in our case like a

00:12:18,480 --> 00:12:22,800
docker

00:12:19,279 --> 00:12:24,160
uh docker image tag really and then last

00:12:22,800 --> 00:12:24,639
but not the least we have a rollout

00:12:24,160 --> 00:12:27,760
state

00:12:24,639 --> 00:12:28,639
so we to take various metrics from argo

00:12:27,760 --> 00:12:31,680
rollout

00:12:28,639 --> 00:12:33,920
controller directly first of all we see

00:12:31,680 --> 00:12:34,880
uh what are the rollouts in progress

00:12:33,920 --> 00:12:37,360
then we can see

00:12:34,880 --> 00:12:38,000
the rollouts per our version so there

00:12:37,360 --> 00:12:39,600
will be number

00:12:38,000 --> 00:12:41,360
of replicas essentially sorry it's not

00:12:39,600 --> 00:12:44,720
rollouts but number of replicas

00:12:41,360 --> 00:12:47,600
per each version where we have been

00:12:44,720 --> 00:12:48,399
rolled and uh and deployed and then on

00:12:47,600 --> 00:12:52,079
the right

00:12:48,399 --> 00:12:54,240
uh last panel is really about the you

00:12:52,079 --> 00:12:56,480
know our rollout analysis runs

00:12:54,240 --> 00:12:57,760
if they were successful or not based on

00:12:56,480 --> 00:13:00,720
our argo

00:12:57,760 --> 00:13:01,519
um analysis template which you remember

00:13:00,720 --> 00:13:04,079
was

00:13:01,519 --> 00:13:05,440
measuring the latency and the error rate

00:13:04,079 --> 00:13:08,000
and you know what are those

00:13:05,440 --> 00:13:08,720
checks uh observations are over the our

00:13:08,000 --> 00:13:11,839
time so

00:13:08,720 --> 00:13:13,200
will this will come handy

00:13:11,839 --> 00:13:14,880
awesome so now that we have a general

00:13:13,200 --> 00:13:16,800
understanding of what our grifana

00:13:14,880 --> 00:13:17,920
dashboard will tell us over time from

00:13:16,800 --> 00:13:19,600
our deployments

00:13:17,920 --> 00:13:22,320
we can go ahead and we can trigger a new

00:13:19,600 --> 00:13:23,680
deployment through aggro rollouts

00:13:22,320 --> 00:13:25,360
so we're just going to deploy a new

00:13:23,680 --> 00:13:26,399
version and we named in this case errors

00:13:25,360 --> 00:13:28,320
really fancy

00:13:26,399 --> 00:13:30,240
now first off we can already see within

00:13:28,320 --> 00:13:33,200
our argo rollout section

00:13:30,240 --> 00:13:34,560
that one part is spin up and our initial

00:13:33,200 --> 00:13:37,600
deployment is down to

00:13:34,560 --> 00:13:40,639
four parts then we can also see the

00:13:37,600 --> 00:13:42,399
analysis that is being performed and

00:13:40,639 --> 00:13:43,199
basically whether or not it's hitting an

00:13:42,399 --> 00:13:46,079
error rate

00:13:43,199 --> 00:13:46,959
can see here in our errors dashboard per

00:13:46,079 --> 00:13:50,240
version

00:13:46,959 --> 00:13:51,839
that right now here our new deployment

00:13:50,240 --> 00:13:54,880
is actually is already showing

00:13:51,839 --> 00:13:56,880
some arrows but at this point it's it

00:13:54,880 --> 00:13:58,160
was really just now up to just now it

00:13:56,880 --> 00:14:00,079
was around the same

00:13:58,160 --> 00:14:01,600
as with our initial deployment so there

00:14:00,079 --> 00:14:03,519
was nothing worrying yet

00:14:01,600 --> 00:14:05,440
but then we can see that our overall

00:14:03,519 --> 00:14:08,320
success rate that we want to have

00:14:05,440 --> 00:14:10,399
above 80 is slowly going down now it's

00:14:08,320 --> 00:14:11,680
below 90. so let's see how this is going

00:14:10,399 --> 00:14:14,000
to develop

00:14:11,680 --> 00:14:15,839
now in terms of latency we can actually

00:14:14,000 --> 00:14:16,959
see that the latency of both deployments

00:14:15,839 --> 00:14:19,199
are fairly similar

00:14:16,959 --> 00:14:20,800
nothing has really changed there now now

00:14:19,199 --> 00:14:21,760
we can see that actually something

00:14:20,800 --> 00:14:24,560
happened

00:14:21,760 --> 00:14:26,160
ago rollouts realize that something is

00:14:24,560 --> 00:14:26,720
something is going wrong something is

00:14:26,160 --> 00:14:30,000
off

00:14:26,720 --> 00:14:31,600
and has terminated our rollout so we can

00:14:30,000 --> 00:14:34,399
see it just basically stop

00:14:31,600 --> 00:14:34,800
and right at instead of slowly scaling

00:14:34,399 --> 00:14:36,880
down

00:14:34,800 --> 00:14:38,240
our new deployment it just cut it off

00:14:36,880 --> 00:14:41,440
and scaled back up

00:14:38,240 --> 00:14:43,199
our initial deployment to five replicas

00:14:41,440 --> 00:14:44,560
now we can have a look here of why this

00:14:43,199 --> 00:14:48,079
has happened

00:14:44,560 --> 00:14:51,600
we can see that our lower than 20

00:14:48,079 --> 00:14:54,320
error rate has actually caused this

00:14:51,600 --> 00:14:55,360
this has failed and has caused this

00:14:54,320 --> 00:14:58,320
rollback to be

00:14:55,360 --> 00:14:58,959
this volatile to be terminated so the

00:14:58,320 --> 00:15:01,199
good thing

00:14:58,959 --> 00:15:03,360
is that we are now back in business

00:15:01,199 --> 00:15:05,600
right everything recovered and

00:15:03,360 --> 00:15:07,440
we were able to mitigate uh the

00:15:05,600 --> 00:15:10,240
problematic rollout without

00:15:07,440 --> 00:15:12,240
our interaction of the system right our

00:15:10,240 --> 00:15:14,720
hands are here we didn't touch anything

00:15:12,240 --> 00:15:18,079
it was automatically uh done and

00:15:14,720 --> 00:15:19,680
recovered so it's it's pretty awesome

00:15:18,079 --> 00:15:21,440
awesome so now that we know that

00:15:19,680 --> 00:15:23,199
something is really off of our error

00:15:21,440 --> 00:15:24,480
rate we want to fix that right we want

00:15:23,199 --> 00:15:26,959
to deploy a new version

00:15:24,480 --> 00:15:27,839
which does not cause as many errors for

00:15:26,959 --> 00:15:30,160
our users

00:15:27,839 --> 00:15:31,839
and overall that the error rate here is

00:15:30,160 --> 00:15:34,320
staying low for everybody

00:15:31,839 --> 00:15:35,519
from our client side right so we're

00:15:34,320 --> 00:15:36,959
going to go ahead and we're going to

00:15:35,519 --> 00:15:37,839
deploy a new version in this case we're

00:15:36,959 --> 00:15:40,560
going to call

00:15:37,839 --> 00:15:42,000
our update or fix slow and we're going

00:15:40,560 --> 00:15:44,399
to deploy this image and

00:15:42,000 --> 00:15:46,320
going to observe our rollouts again now

00:15:44,399 --> 00:15:48,240
it will show us our previous revisions

00:15:46,320 --> 00:15:50,240
so we can see here revision 2

00:15:48,240 --> 00:15:51,440
which was our error deployment has

00:15:50,240 --> 00:15:55,279
actually failed

00:15:51,440 --> 00:15:57,120
we can see that that fail of our

00:15:55,279 --> 00:15:58,959
of our the checks our errors how many

00:15:57,120 --> 00:16:01,440
errors we have

00:15:58,959 --> 00:16:03,199
has caused the immediate termination of

00:16:01,440 --> 00:16:03,759
this rollout we can see additionally

00:16:03,199 --> 00:16:06,079
that

00:16:03,759 --> 00:16:07,199
our new deployment is spun up there's

00:16:06,079 --> 00:16:09,759
one port running

00:16:07,199 --> 00:16:10,560
and it has already passed two checks

00:16:09,759 --> 00:16:13,519
from our

00:16:10,560 --> 00:16:15,360
analysis template now let's go back to

00:16:13,519 --> 00:16:17,600
our grafana dashboard and see how

00:16:15,360 --> 00:16:19,839
everything is developing

00:16:17,600 --> 00:16:21,199
overall here's our error rate that's

00:16:19,839 --> 00:16:24,480
slowly

00:16:21,199 --> 00:16:26,800
showing on in the dashboard in the panel

00:16:24,480 --> 00:16:28,560
but what's happening over here so it's

00:16:26,800 --> 00:16:30,000
like a tail latency so

00:16:28,560 --> 00:16:32,160
you know most of the users are not

00:16:30,000 --> 00:16:34,880
impacted at all it's like

00:16:32,160 --> 00:16:36,560
you know 50 percentiles it stays stable

00:16:34,880 --> 00:16:37,839
but you know the tail latency so the

00:16:36,560 --> 00:16:39,920
unlucky users

00:16:37,839 --> 00:16:41,440
are really unlucky now because it's not

00:16:39,920 --> 00:16:42,800
like only a couple of milliseconds

00:16:41,440 --> 00:16:45,920
hundred milliseconds more

00:16:42,800 --> 00:16:49,040
is suddenly two seconds or more uh

00:16:45,920 --> 00:16:50,480
to res to retrieve the punk uh when you

00:16:49,040 --> 00:16:52,399
are pinging right so

00:16:50,480 --> 00:16:53,920
something is really wrong with this uh

00:16:52,399 --> 00:16:56,000
deployment hopefully our gorillas will

00:16:53,920 --> 00:16:58,000
notice that

00:16:56,000 --> 00:17:00,000
at the same time we don't see many like

00:16:58,000 --> 00:17:01,279
much of change within our error section

00:17:00,000 --> 00:17:02,320
it's about the same for both of our

00:17:01,279 --> 00:17:05,679
deployments

00:17:02,320 --> 00:17:07,600
and now we can see right away life that

00:17:05,679 --> 00:17:09,199
aggro wallets actually was not happy

00:17:07,600 --> 00:17:11,760
with that deployment

00:17:09,199 --> 00:17:12,400
and it didn't pass our checks so it

00:17:11,760 --> 00:17:15,600
scaled

00:17:12,400 --> 00:17:17,199
down right after two uh parts being spun

00:17:15,600 --> 00:17:19,120
up from our new deployment it

00:17:17,199 --> 00:17:20,799
con reverted the rollout and it's been

00:17:19,120 --> 00:17:23,199
back our initial deployment

00:17:20,799 --> 00:17:24,959
we can see now that our uh rollout

00:17:23,199 --> 00:17:26,959
reverted to our initial deployment

00:17:24,959 --> 00:17:28,720
the latency is back to what it has been

00:17:26,959 --> 00:17:32,160
before what has been prior

00:17:28,720 --> 00:17:34,000
so we can relax again everything is back

00:17:32,160 --> 00:17:36,960
to normal again but we still have we're

00:17:34,000 --> 00:17:38,720
unable to deploy our update

00:17:36,960 --> 00:17:40,400
yeah and we are super frustrated like

00:17:38,720 --> 00:17:42,640
what's going on with our

00:17:40,400 --> 00:17:44,000
coding i don't know we are we are bad

00:17:42,640 --> 00:17:46,559
programmers or what like

00:17:44,000 --> 00:17:48,000
how how all those rollers are failing

00:17:46,559 --> 00:17:49,679
and to know more

00:17:48,000 --> 00:17:51,280
i would love to know what what exactly

00:17:49,679 --> 00:17:51,919
wrong with this application and with the

00:17:51,280 --> 00:17:55,200
errors

00:17:51,919 --> 00:17:56,160
it's usually easier to uh find the root

00:17:55,200 --> 00:17:58,160
cause of the error

00:17:56,160 --> 00:18:00,160
like on the previous deployment but now

00:17:58,160 --> 00:18:00,559
we have kind of tricky situation we have

00:18:00,160 --> 00:18:02,960
a

00:18:00,559 --> 00:18:05,120
low latency normally for most of the

00:18:02,960 --> 00:18:07,360
users but some users are experiencing

00:18:05,120 --> 00:18:08,240
enormous you know wait time for the

00:18:07,360 --> 00:18:10,400
request

00:18:08,240 --> 00:18:11,520
and you know imagine that you we cannot

00:18:10,400 --> 00:18:13,840
really locally

00:18:11,520 --> 00:18:15,120
test this out and uh and and find the

00:18:13,840 --> 00:18:16,480
root cause we cannot reproduce this

00:18:15,120 --> 00:18:18,880
locally so what we do

00:18:16,480 --> 00:18:20,400
this is where exemplars in pro for uh

00:18:18,880 --> 00:18:24,000
promote use exemplars

00:18:20,400 --> 00:18:26,160
and uh actually um the whole story is

00:18:24,000 --> 00:18:28,160
that it's open metrics exemplars because

00:18:26,160 --> 00:18:28,720
prometheus is using open metrics data

00:18:28,160 --> 00:18:31,039
model

00:18:28,720 --> 00:18:31,919
which designs the exemplars and the

00:18:31,039 --> 00:18:35,120
metrics

00:18:31,919 --> 00:18:35,760
of different types and we kind of scrape

00:18:35,120 --> 00:18:37,039
those

00:18:35,760 --> 00:18:38,799
information directly from the

00:18:37,039 --> 00:18:41,520
applications from both pink

00:18:38,799 --> 00:18:44,000
and finger and the application now this

00:18:41,520 --> 00:18:46,160
exemplar allows us to specify additional

00:18:44,000 --> 00:18:49,120
information that happened during this

00:18:46,160 --> 00:18:50,799
uh for example latency observation so

00:18:49,120 --> 00:18:51,440
with the latency observation for every

00:18:50,799 --> 00:18:54,080
request

00:18:51,440 --> 00:18:56,400
we can put something else what we put we

00:18:54,080 --> 00:18:58,640
essentially put in our case the trace id

00:18:56,400 --> 00:19:00,240
because i didn't mention that before our

00:18:58,640 --> 00:19:01,360
applications are instrumented with

00:19:00,240 --> 00:19:04,240
tracing as well

00:19:01,360 --> 00:19:05,760
so every ping is actually traceable and

00:19:04,240 --> 00:19:08,640
the trace is essentially

00:19:05,760 --> 00:19:10,880
you know a certain um information bound

00:19:08,640 --> 00:19:13,039
into the bound to the request flow

00:19:10,880 --> 00:19:14,320
across different multiple you know

00:19:13,039 --> 00:19:17,280
microservices

00:19:14,320 --> 00:19:18,160
so now we have the link between the

00:19:17,280 --> 00:19:20,640
trace

00:19:18,160 --> 00:19:22,080
to and we put between the metrics and

00:19:20,640 --> 00:19:23,679
the trace within the form of the

00:19:22,080 --> 00:19:27,120
exemplar so we can see

00:19:23,679 --> 00:19:28,960
a different metadata accompanied by the

00:19:27,120 --> 00:19:30,720
by the observation so we can see that

00:19:28,960 --> 00:19:34,799
this direct

00:19:30,720 --> 00:19:36,720
unlucky user has the you know um

00:19:34,799 --> 00:19:38,240
uh that the request where we're kind of

00:19:36,720 --> 00:19:40,000
above two seconds

00:19:38,240 --> 00:19:42,240
but we would like to know the root cause

00:19:40,000 --> 00:19:42,880
so let's go and and kind of go to the

00:19:42,240 --> 00:19:45,039
tempo

00:19:42,880 --> 00:19:47,120
which is our tracing solution to see

00:19:45,039 --> 00:19:49,919
what's going on so now we have a tracing

00:19:47,120 --> 00:19:50,960
ui visible and we can see the trace so

00:19:49,919 --> 00:19:53,679
essentially the

00:19:50,960 --> 00:19:54,640
the the flow of the request um between

00:19:53,679 --> 00:19:56,880
different components

00:19:54,640 --> 00:19:58,400
and the co and the code itself right so

00:19:56,880 --> 00:20:01,440
we can see that finger

00:19:58,400 --> 00:20:03,120
is there is the green line it is uh you

00:20:01,440 --> 00:20:06,159
know our client that

00:20:03,120 --> 00:20:09,600
seen um what two uh zero

00:20:06,159 --> 00:20:12,960
um 2.01 seconds

00:20:09,600 --> 00:20:15,039
of latency and it experienced this

00:20:12,960 --> 00:20:16,720
latency based on some other things that

00:20:15,039 --> 00:20:19,120
happened so let's go to the child

00:20:16,720 --> 00:20:19,760
process which is essentially it hits you

00:20:19,120 --> 00:20:21,360
you can see

00:20:19,760 --> 00:20:23,600
down now we have a different service

00:20:21,360 --> 00:20:25,120
it's called up it's our service which is

00:20:23,600 --> 00:20:27,840
responding to our request

00:20:25,120 --> 00:20:28,880
and we can see it also uh reported that

00:20:27,840 --> 00:20:30,960
long duration

00:20:28,880 --> 00:20:31,919
uh which is two seconds something within

00:20:30,960 --> 00:20:33,440
the server

00:20:31,919 --> 00:20:36,000
maybe introduced this so let's go

00:20:33,440 --> 00:20:39,039
further our pink handler also see

00:20:36,000 --> 00:20:39,600
two seconds so it's like a span a

00:20:39,039 --> 00:20:41,840
different

00:20:39,600 --> 00:20:42,960
function that happens in the code itself

00:20:41,840 --> 00:20:46,960
different phrase

00:20:42,960 --> 00:20:46,960
and now when we go

00:20:47,039 --> 00:20:51,360
below we can see that all of those are

00:20:49,360 --> 00:20:53,200
reporting two seconds including the

00:20:51,360 --> 00:20:55,440
adding latency based on probability

00:20:53,200 --> 00:20:57,760
that's that's a

00:20:55,440 --> 00:20:58,880
suspicious name by the way and then but

00:20:57,760 --> 00:21:00,320
you know maybe that's not

00:20:58,880 --> 00:21:02,640
the thing that actually introduced the

00:21:00,320 --> 00:21:04,080
latency let's go and see other requests

00:21:02,640 --> 00:21:05,679
in this trace we can see that the last

00:21:04,080 --> 00:21:06,480
request right status which is

00:21:05,679 --> 00:21:08,480
essentially

00:21:06,480 --> 00:21:09,679
just uh responding with some kind of

00:21:08,480 --> 00:21:11,600
bytes to the user

00:21:09,679 --> 00:21:14,120
actually happened you know pretty fast

00:21:11,600 --> 00:21:17,280
like almost immediate it was only like

00:21:14,120 --> 00:21:18,080
0.02 milliseconds so we know now based

00:21:17,280 --> 00:21:20,159
on this view

00:21:18,080 --> 00:21:21,760
that aiding adding latency based on

00:21:20,159 --> 00:21:23,760
probability function

00:21:21,760 --> 00:21:25,600
was the one which introduced the latency

00:21:23,760 --> 00:21:27,679
and we know about that because well we

00:21:25,600 --> 00:21:29,679
implemented this so when you look on the

00:21:27,679 --> 00:21:31,280
tags we have other channel information

00:21:29,679 --> 00:21:33,679
that we could provide

00:21:31,280 --> 00:21:35,200
so what we did we we essentially

00:21:33,679 --> 00:21:37,760
simulated some

00:21:35,200 --> 00:21:39,440
latency based on probability and we can

00:21:37,760 --> 00:21:42,080
see the unlucky one was

00:21:39,440 --> 00:21:42,720
uh was this and it reduced exactly two

00:21:42,080 --> 00:21:45,679
seconds

00:21:42,720 --> 00:21:47,200
so we literally sleep for two seconds

00:21:45,679 --> 00:21:49,360
and we kind of you know

00:21:47,200 --> 00:21:50,960
uh we choose some random number in this

00:21:49,360 --> 00:21:53,200
case it was like 30

00:21:50,960 --> 00:21:55,039
33 and he was within some probability

00:21:53,200 --> 00:21:55,520
that introduced this latency so now we

00:21:55,039 --> 00:21:57,520
know

00:21:55,520 --> 00:21:59,200
where to go in the code where exactly

00:21:57,520 --> 00:22:01,679
the code we need to go

00:21:59,200 --> 00:22:04,159
and and fix it so that's pretty powerful

00:22:01,679 --> 00:22:07,360
technique of finding the root cause

00:22:04,159 --> 00:22:10,159
of the problem and um just maybe to

00:22:07,360 --> 00:22:10,960
you know show you exactly how we did

00:22:10,159 --> 00:22:13,600
that we can

00:22:10,960 --> 00:22:15,039
jump super quickly to the code uh it's a

00:22:13,600 --> 00:22:16,480
glock application

00:22:15,039 --> 00:22:18,720
but you really do the same thing in

00:22:16,480 --> 00:22:21,520
other languages this is what

00:22:18,720 --> 00:22:21,840
is super powerful with instrumentations

00:22:21,520 --> 00:22:23,760
and

00:22:21,840 --> 00:22:25,679
and i use like from to use the most

00:22:23,760 --> 00:22:27,840
popular promoters client

00:22:25,679 --> 00:22:29,679
and then i use open telemetry go

00:22:27,840 --> 00:22:30,320
instrumentation to combine those two

00:22:29,679 --> 00:22:32,799
together

00:22:30,320 --> 00:22:35,280
so what we have seen with the kfc right

00:22:32,799 --> 00:22:36,320
now is a client instrumentation kind of

00:22:35,280 --> 00:22:39,440
abstraction

00:22:36,320 --> 00:22:41,360
where we instrument http client with

00:22:39,440 --> 00:22:42,880
additional context so when the request

00:22:41,360 --> 00:22:46,000
is sent

00:22:42,880 --> 00:22:47,600
you can see that we are before actually

00:22:46,000 --> 00:22:52,000
after the round trip

00:22:47,600 --> 00:22:55,600
we are checking if we have online

00:22:52,000 --> 00:22:58,320
line 95 if we check if we have any trace

00:22:55,600 --> 00:22:59,200
uh combined within this request if we

00:22:58,320 --> 00:23:01,120
have a trace

00:22:59,200 --> 00:23:02,720
and we need to check if it doesn't have

00:23:01,120 --> 00:23:05,520
any trace id

00:23:02,720 --> 00:23:07,200
and also we check is the sampled because

00:23:05,520 --> 00:23:08,000
it might be that the trace is kind of

00:23:07,200 --> 00:23:09,280
omitted because

00:23:08,000 --> 00:23:11,280
we have some kind of probability

00:23:09,280 --> 00:23:13,280
sampling or whatever in our case

00:23:11,280 --> 00:23:15,039
everything is sampled but we check

00:23:13,280 --> 00:23:17,440
anyway and then only then

00:23:15,039 --> 00:23:18,799
we put a special trace id into the

00:23:17,440 --> 00:23:21,440
exemplar

00:23:18,799 --> 00:23:22,000
and then this is kind of observing the

00:23:21,440 --> 00:23:23,919
the

00:23:22,000 --> 00:23:25,039
the the kind of duration of the request

00:23:23,919 --> 00:23:27,760
in the histogram of

00:23:25,039 --> 00:23:29,360
of from to use with the trace id and

00:23:27,760 --> 00:23:29,919
then if we don't have the stress id then

00:23:29,360 --> 00:23:32,159
we just

00:23:29,919 --> 00:23:34,159
observe without trace id so this is the

00:23:32,159 --> 00:23:36,880
whole code that has to be

00:23:34,159 --> 00:23:37,840
uh created like not not like very

00:23:36,880 --> 00:23:40,720
complex thing

00:23:37,840 --> 00:23:41,600
to have uh exemplar being provided in

00:23:40,720 --> 00:23:43,840
the metric

00:23:41,600 --> 00:23:45,360
and then um it is automatically chosen

00:23:43,840 --> 00:23:46,320
by the graffana if you go to back to

00:23:45,360 --> 00:23:48,960
grafana

00:23:46,320 --> 00:23:50,000
it is automatically uh discovered that

00:23:48,960 --> 00:23:52,000
this metric

00:23:50,000 --> 00:23:53,200
has some traces has some sorry has some

00:23:52,000 --> 00:23:54,960
examples

00:23:53,200 --> 00:23:56,720
so now you need to go i guess to the

00:23:54,960 --> 00:24:00,080
back to our dashboard

00:23:56,720 --> 00:24:03,120
yeah and you can see that once i

00:24:00,080 --> 00:24:04,880
once we kind of published uh the metric

00:24:03,120 --> 00:24:07,279
once we set the metric you can actually

00:24:04,880 --> 00:24:09,840
uh click on the client request latency

00:24:07,279 --> 00:24:11,520
per second edit

00:24:09,840 --> 00:24:13,440
you can see that once i put those

00:24:11,520 --> 00:24:14,880
metrics uh there is this kind of

00:24:13,440 --> 00:24:17,360
exemplar

00:24:14,880 --> 00:24:19,279
icon and it automatically detects that

00:24:17,360 --> 00:24:20,240
for those metrics there are examples

00:24:19,279 --> 00:24:23,520
provided

00:24:20,240 --> 00:24:26,480
via you know promotes api which is like

00:24:23,520 --> 00:24:28,720
just now in the new version 2.26

00:24:26,480 --> 00:24:30,799
available under feature flag

00:24:28,720 --> 00:24:33,279
it is seeing examples and then

00:24:30,799 --> 00:24:35,360
visualizating that in very interactive

00:24:33,279 --> 00:24:36,720
you know thing that we can click on so

00:24:35,360 --> 00:24:38,159
it's a very powerful technique that i

00:24:36,720 --> 00:24:40,080
really recommend for everyone to use

00:24:38,159 --> 00:24:40,480
it's it's it's kind of amazing it allows

00:24:40,080 --> 00:24:42,080
us

00:24:40,480 --> 00:24:44,400
a super nice correlation between those

00:24:42,080 --> 00:24:45,840
signals

00:24:44,400 --> 00:24:48,000
awesome so now that we know what's

00:24:45,840 --> 00:24:50,240
actually going wrong with an application

00:24:48,000 --> 00:24:51,200
we can deploy another update and make

00:24:50,240 --> 00:24:52,720
sure that we

00:24:51,200 --> 00:24:55,120
that we actually fixed it so in this

00:24:52,720 --> 00:24:57,760
case we're going to deploy our

00:24:55,120 --> 00:25:00,000
our update called best now it's

00:24:57,760 --> 00:25:03,120
currently spinning up our new deployment

00:25:00,000 --> 00:25:03,919
of our best application and scaling now

00:25:03,120 --> 00:25:05,760
on the other

00:25:03,919 --> 00:25:07,440
let's see how this translates into our

00:25:05,760 --> 00:25:09,360
dashboard and whether or not it's

00:25:07,440 --> 00:25:11,440
actually improving things

00:25:09,360 --> 00:25:12,880
we can already see that our latency is

00:25:11,440 --> 00:25:16,240
actually going down

00:25:12,880 --> 00:25:18,880
let's see if it's also staying down so

00:25:16,240 --> 00:25:20,080
while we wait for this rollout to

00:25:18,880 --> 00:25:22,480
succeed or not

00:25:20,080 --> 00:25:24,000
maybe we can go to our repository and

00:25:22,480 --> 00:25:24,480
show you how you can leverage this

00:25:24,000 --> 00:25:27,600
knowledge

00:25:24,480 --> 00:25:30,480
and moreover you can use the katakoda

00:25:27,600 --> 00:25:30,799
url you can see on the right um just go

00:25:30,480 --> 00:25:32,480
there

00:25:30,799 --> 00:25:34,320
click there and then you can experience

00:25:32,480 --> 00:25:35,919
this demo uh on your own

00:25:34,320 --> 00:25:38,000
awesome so let's jump back into our

00:25:35,919 --> 00:25:39,440
dashboard and see what's going on here

00:25:38,000 --> 00:25:42,240
yeah there are no additional failures

00:25:39,440 --> 00:25:44,880
yet there are one progressing still

00:25:42,240 --> 00:25:46,799
so we are not sure we are not yet

00:25:44,880 --> 00:25:49,200
finished with these deployments

00:25:46,799 --> 00:25:51,039
but we are super close and we can see

00:25:49,200 --> 00:25:52,000
that our latency is further decreasing

00:25:51,039 --> 00:25:54,080
also for the

00:25:52,000 --> 00:25:55,360
higher end of you like for the for the

00:25:54,080 --> 00:25:57,200
rest of users who

00:25:55,360 --> 00:25:58,720
were quite unlucky before in terms of

00:25:57,200 --> 00:26:01,520
latency

00:25:58,720 --> 00:26:01,919
and our overall error rate is staying

00:26:01,520 --> 00:26:05,039
low

00:26:01,919 --> 00:26:06,080
as low as our previous or initial

00:26:05,039 --> 00:26:07,279
deployment

00:26:06,080 --> 00:26:09,200
and now we can see our initial

00:26:07,279 --> 00:26:10,240
deployment has actually completely

00:26:09,200 --> 00:26:12,480
scaled down

00:26:10,240 --> 00:26:14,480
and our new deployment is up by our

00:26:12,480 --> 00:26:16,799
entire replica set by five

00:26:14,480 --> 00:26:17,679
so it seems that our entire robot has

00:26:16,799 --> 00:26:19,120
actually passed

00:26:17,679 --> 00:26:20,720
and is now up and running our new

00:26:19,120 --> 00:26:21,679
deployment is now running and has passed

00:26:20,720 --> 00:26:24,880
our checks

00:26:21,679 --> 00:26:26,799
and we can see that there's no new

00:26:24,880 --> 00:26:28,080
new errors from our analysis template

00:26:26,799 --> 00:26:30,720
being introduced this

00:26:28,080 --> 00:26:31,440
this red block is just basically showing

00:26:30,720 --> 00:26:32,799
our or

00:26:31,440 --> 00:26:35,440
the previous errors from previous

00:26:32,799 --> 00:26:38,640
rollouts but our new analysis has

00:26:35,440 --> 00:26:39,279
passed successfully so hope you like it

00:26:38,640 --> 00:26:41,840
and please

00:26:39,279 --> 00:26:42,720
check out our demo uh on yours on your

00:26:41,840 --> 00:26:44,640
own and and

00:26:42,720 --> 00:26:46,159
and play with our resources as well and

00:26:44,640 --> 00:26:49,440
give us feedback on the

00:26:46,159 --> 00:26:52,960
on the github repo ask questions and

00:26:49,440 --> 00:26:54,880
uh yeah feel free to reach us on twitter

00:26:52,960 --> 00:26:56,880
or anywhere literally

00:26:54,880 --> 00:26:58,240
and hopefully we have some time for

00:26:56,880 --> 00:27:00,919
questions

00:26:58,240 --> 00:27:03,919
awesome thank you everybody for

00:27:00,919 --> 00:27:03,919

YouTube URL: https://www.youtube.com/watch?v=Z8hfs_CN1EY


