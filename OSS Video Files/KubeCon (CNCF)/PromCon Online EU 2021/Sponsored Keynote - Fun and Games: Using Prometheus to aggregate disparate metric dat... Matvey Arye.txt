Title: Sponsored Keynote - Fun and Games: Using Prometheus to aggregate disparate metric dat... Matvey Arye
Publication date: 2021-05-21
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Sponsored Keynote - Fun and Games: Using Prometheus to aggregate disparate metric data formats at a major gaming company - Matvey Arye, Timescale

In this session, you’ll learn how and why a major gaming company used Prometheus to simplify querying, dashboarding, and reporting on their load testing data, by aggregating disparate metric formats. You’ll learn about the challenge of centralizing monitoring data from disparate sources, why it's becoming increasingly common, and an example of how to overcome it by using Prometheus in an innovative way. You’ll also walk away with a nuanced understanding of the importance of flexibility in observability systems, as well as frameworks to advocate for more Prometheus adoption inside your organization.
We’ll focus on one story, but the problem of aggregating disparate metric sources is an all too common one. 
In this case, a major gaming company uses Prometheus to conduct load testing before releasing its games into production, so that every game runs reliably and performantly. Each online game is composed of many services, run by independent engineering teams. Given the mammoth scale of the company, it was not possible to standardize on one tool to monitor metrics. Each team used its favorite metrics monitoring tool, which includes all the usual suspects, like Prometheus, DataDog, Graphite, Zabbix, New Relic, StatsD, etc. 
 
This gave rise to an interesting technical challenge: how to efficiently centralize load testing metrics and monitoring data from disparate metrics sources into a consistent format and data store? 
The gaming company solved this problem by using Prometheus and Promscale -a versatile long-term store for observability - to centralize, store and analyze the disparate load testing metrics. The result is a simple, unified system that allows the team to run queries, deliver useful dashboards, and generate reports to analyze load test results in an on-demand and self-serve fashion.
We’ll also discuss some other dimensions of storage flexibility we believe are beneficial to support complex, hybrid environments, often found in the real world.
Captions: 
	00:00:00,240 --> 00:00:06,160
hi today i'm going to be talking about

00:00:03,120 --> 00:00:08,559
using prometheus to centralize desperate

00:00:06,160 --> 00:00:12,160
metric data formats

00:00:08,559 --> 00:00:15,599
my name is matt and i've been working

00:00:12,160 --> 00:00:19,439
at a company called time scale since

00:00:15,599 --> 00:00:21,039
i finished my phd actually the cto of

00:00:19,439 --> 00:00:24,640
time scale was my

00:00:21,039 --> 00:00:28,080
phd css advisor and so i didn't

00:00:24,640 --> 00:00:29,279
fly far from the nest so to speak i was

00:00:28,080 --> 00:00:32,399
one of the core

00:00:29,279 --> 00:00:32,960
architects of time scale db and for the

00:00:32,399 --> 00:00:35,600
past

00:00:32,960 --> 00:00:36,399
year and a half have been leading prom

00:00:35,600 --> 00:00:39,280
skill

00:00:36,399 --> 00:00:39,920
which is a project to allow users to

00:00:39,280 --> 00:00:43,360
easily

00:00:39,920 --> 00:00:46,399
ingest prometheus data into time scale

00:00:43,360 --> 00:00:48,879
in my old time i like going skiing

00:00:46,399 --> 00:00:50,399
going to see the shows traveling and

00:00:48,879 --> 00:00:53,680
taking photos

00:00:50,399 --> 00:00:56,719
but during the past year covet has made

00:00:53,680 --> 00:00:59,840
most of those things impossible so

00:00:56,719 --> 00:01:00,879
i really love to geek out on those

00:00:59,840 --> 00:01:04,000
topics

00:01:00,879 --> 00:01:07,119
with you over twitter slack

00:01:04,000 --> 00:01:10,320
or email today i'm gonna tell you about

00:01:07,119 --> 00:01:12,960
an interesting use case we had from

00:01:10,320 --> 00:01:14,799
one of our users and later on in the

00:01:12,960 --> 00:01:17,759
silica i'll tell you why i

00:01:14,799 --> 00:01:18,799
think this use case really illustrates

00:01:17,759 --> 00:01:22,000
the power

00:01:18,799 --> 00:01:22,479
of prometheus and prompt scale this use

00:01:22,000 --> 00:01:25,520
case

00:01:22,479 --> 00:01:28,080
involves a gaming company which

00:01:25,520 --> 00:01:29,600
produces many games and the way the

00:01:28,080 --> 00:01:33,119
company is structured

00:01:29,600 --> 00:01:36,479
is that each game has a team

00:01:33,119 --> 00:01:39,520
which controls its infrastructure

00:01:36,479 --> 00:01:42,320
and observability system but the team

00:01:39,520 --> 00:01:43,360
we were actually working with the load

00:01:42,320 --> 00:01:46,799
testing team

00:01:43,360 --> 00:01:49,040
this team is responsible for making sure

00:01:46,799 --> 00:01:52,240
that each of these games can

00:01:49,040 --> 00:01:55,119
scale to handle a large number of

00:01:52,240 --> 00:01:55,840
users that play these games over pcs

00:01:55,119 --> 00:01:59,040
consoles

00:01:55,840 --> 00:02:02,399
etc they do this by load testing

00:01:59,040 --> 00:02:05,360
each of these games regularly via a

00:02:02,399 --> 00:02:06,640
ci pipeline and generating a report

00:02:05,360 --> 00:02:10,000
based on the results

00:02:06,640 --> 00:02:13,360
the problem however that this team has

00:02:10,000 --> 00:02:14,040
is that each of these games has a

00:02:13,360 --> 00:02:16,640
different

00:02:14,040 --> 00:02:18,319
observability system and it's a

00:02:16,640 --> 00:02:20,800
challenge to bring

00:02:18,319 --> 00:02:21,440
all of the data together in the one

00:02:20,800 --> 00:02:24,160
place

00:02:21,440 --> 00:02:24,879
in order to combine with the test

00:02:24,160 --> 00:02:28,160
results

00:02:24,879 --> 00:02:30,400
and generate the report the architecture

00:02:28,160 --> 00:02:33,599
looks something like this

00:02:30,400 --> 00:02:36,720
the load testing team has a low

00:02:33,599 --> 00:02:39,599
generator which generates load

00:02:36,720 --> 00:02:41,280
onto game services running in the

00:02:39,599 --> 00:02:44,800
staging environment

00:02:41,280 --> 00:02:48,080
of a game's infrastructure

00:02:44,800 --> 00:02:51,120
and there are many such games remember

00:02:48,080 --> 00:02:52,080
the load testing infrastructure is

00:02:51,120 --> 00:02:54,879
monitored by

00:02:52,080 --> 00:02:55,519
prometheus pushing data into prompt

00:02:54,879 --> 00:02:58,560
scale

00:02:55,519 --> 00:02:59,920
and storing it in time scale db and the

00:02:58,560 --> 00:03:03,200
query engine

00:02:59,920 --> 00:03:06,640
is a homegrown dashboarding

00:03:03,200 --> 00:03:07,920
system developed by the load testing

00:03:06,640 --> 00:03:10,720
team

00:03:07,920 --> 00:03:11,760
now the results of the load test are

00:03:10,720 --> 00:03:15,200
apparent

00:03:11,760 --> 00:03:17,599
in the results of the observability

00:03:15,200 --> 00:03:18,879
system inside the games and as we

00:03:17,599 --> 00:03:21,360
mentioned before

00:03:18,879 --> 00:03:23,120
each game might use a different

00:03:21,360 --> 00:03:25,760
observability system

00:03:23,120 --> 00:03:27,599
the load testing team wrote data

00:03:25,760 --> 00:03:31,280
collected to collect

00:03:27,599 --> 00:03:34,799
the key observability metrics from each

00:03:31,280 --> 00:03:38,000
team's observability stack and push

00:03:34,799 --> 00:03:41,519
those metrics into prompt scale

00:03:38,000 --> 00:03:44,080
this allows all of the load testing data

00:03:41,519 --> 00:03:44,799
to be centralized in the one place

00:03:44,080 --> 00:03:47,599
including

00:03:44,799 --> 00:03:50,080
both machine metrics and game metrics

00:03:47,599 --> 00:03:50,640
the benefits of this to the load testing

00:03:50,080 --> 00:03:53,519
team

00:03:50,640 --> 00:03:54,560
is that they're able to save metrics

00:03:53,519 --> 00:03:57,840
into their own

00:03:54,560 --> 00:03:59,280
system to analyze it over time which

00:03:57,840 --> 00:04:02,080
makes it easy to run

00:03:59,280 --> 00:04:02,640
reports and to build visualizations

00:04:02,080 --> 00:04:05,680
using

00:04:02,640 --> 00:04:08,640
sql and promptql queries i

00:04:05,680 --> 00:04:09,680
asked this user why did they use

00:04:08,640 --> 00:04:12,239
prometheus

00:04:09,680 --> 00:04:12,959
they mentioned that prometheus is an

00:04:12,239 --> 00:04:15,519
industry

00:04:12,959 --> 00:04:16,400
standard with an easy to understand data

00:04:15,519 --> 00:04:19,359
layout

00:04:16,400 --> 00:04:20,160
and it's easy to use from ql as well as

00:04:19,359 --> 00:04:23,120
a gaining

00:04:20,160 --> 00:04:24,240
wide adoption inside the company and the

00:04:23,120 --> 00:04:28,160
team wants to be

00:04:24,240 --> 00:04:31,199
on that kind of bleeding edge and

00:04:28,160 --> 00:04:31,840
the team runs kubernetes and prometheus

00:04:31,199 --> 00:04:34,400
is very

00:04:31,840 --> 00:04:35,840
cloud friendly and so it seemed a

00:04:34,400 --> 00:04:39,040
natural fit

00:04:35,840 --> 00:04:40,880
now everybody in this conference knows

00:04:39,040 --> 00:04:43,520
all of these reasons

00:04:40,880 --> 00:04:45,440
for using prometheus but i think it's

00:04:43,520 --> 00:04:49,120
good to be reminded of them

00:04:45,440 --> 00:04:52,560
so that we could advocate for prometheus

00:04:49,120 --> 00:04:54,800
usage inside our organizations more

00:04:52,560 --> 00:04:57,919
effectively

00:04:54,800 --> 00:04:59,280
i also asked the user why they chose

00:04:57,919 --> 00:05:01,680
prom scale

00:04:59,280 --> 00:05:02,320
one of the main reasons was backfill

00:05:01,680 --> 00:05:05,199
turns out

00:05:02,320 --> 00:05:06,080
that this team had legacy data they

00:05:05,199 --> 00:05:08,880
wanted

00:05:06,080 --> 00:05:10,320
to be in the same centralized data

00:05:08,880 --> 00:05:13,440
repository

00:05:10,320 --> 00:05:14,479
as the new data they wanted the flexible

00:05:13,440 --> 00:05:17,600
querying

00:05:14,479 --> 00:05:20,320
they wanted both push and pull

00:05:17,600 --> 00:05:22,639
as i showed in the architecture slide

00:05:20,320 --> 00:05:25,280
they found it easy to

00:05:22,639 --> 00:05:26,000
use and deploy they mentioned the time

00:05:25,280 --> 00:05:29,039
scale to be

00:05:26,000 --> 00:05:31,360
slack channel as giving good community

00:05:29,039 --> 00:05:32,080
support and they needed long term

00:05:31,360 --> 00:05:35,280
storage

00:05:32,080 --> 00:05:38,560
for prometheus anyway so this checked

00:05:35,280 --> 00:05:41,840
multiple boxes for them

00:05:38,560 --> 00:05:42,240
now i'd like to stop and listen to why i

00:05:41,840 --> 00:05:45,600
think

00:05:42,240 --> 00:05:49,039
this is a good use case

00:05:45,600 --> 00:05:52,880
to think about and i think the main

00:05:49,039 --> 00:05:56,000
reason is that it shows that

00:05:52,880 --> 00:05:57,039
prometheus and his data format and

00:05:56,000 --> 00:06:00,080
promptql

00:05:57,039 --> 00:06:03,280
form a good basis for these hybrid

00:06:00,080 --> 00:06:06,720
systems which combine different

00:06:03,280 --> 00:06:10,160
metrics together for analysis

00:06:06,720 --> 00:06:10,479
and reporting and this ties in very well

00:06:10,160 --> 00:06:13,280
with

00:06:10,479 --> 00:06:15,919
promiscuous version which is to be the

00:06:13,280 --> 00:06:19,120
flexible database for prometheus

00:06:15,919 --> 00:06:20,639
and other observability data what do i

00:06:19,120 --> 00:06:23,440
mean by flexibility

00:06:20,639 --> 00:06:24,080
i mean to both for both push and pull

00:06:23,440 --> 00:06:26,639
for

00:06:24,080 --> 00:06:28,800
different data formats obviously

00:06:26,639 --> 00:06:32,960
promises remote rate

00:06:28,800 --> 00:06:36,479
the prometheus exposition text format

00:06:32,960 --> 00:06:39,280
and json so booth for promql

00:06:36,479 --> 00:06:40,880
for dashboarding queries and sql for

00:06:39,280 --> 00:06:43,280
longer term analytics

00:06:40,880 --> 00:06:44,000
support for backfill different data

00:06:43,280 --> 00:06:47,280
types

00:06:44,000 --> 00:06:48,319
including not only promising flows but

00:06:47,280 --> 00:06:50,400
also strings

00:06:48,319 --> 00:06:52,560
and symbols as well as different

00:06:50,400 --> 00:06:55,520
observability modalities

00:06:52,560 --> 00:06:58,080
right now we support metrics thanks to

00:06:55,520 --> 00:07:02,000
prometheus but we want to add

00:06:58,080 --> 00:07:05,039
support for logs and traces as well the

00:07:02,000 --> 00:07:06,960
flexibility allows you to do is to put a

00:07:05,039 --> 00:07:09,680
lot of your observability

00:07:06,960 --> 00:07:11,199
data into a single database but why

00:07:09,680 --> 00:07:13,440
would you want to do that

00:07:11,199 --> 00:07:15,520
i think there are two main reasons one

00:07:13,440 --> 00:07:18,240
is operational and

00:07:15,520 --> 00:07:18,960
one is analytical on the operational

00:07:18,240 --> 00:07:21,199
side

00:07:18,960 --> 00:07:22,560
running any kind of database or data

00:07:21,199 --> 00:07:25,360
store is hard

00:07:22,560 --> 00:07:26,319
because it is a state for service so you

00:07:25,360 --> 00:07:29,520
have to deal with

00:07:26,319 --> 00:07:31,039
pvcs or hard disks you have to think

00:07:29,520 --> 00:07:34,000
about backups

00:07:31,039 --> 00:07:36,160
high availability fail over security

00:07:34,000 --> 00:07:39,520
scaling and tuning

00:07:36,160 --> 00:07:42,639
these are complex systems and so

00:07:39,520 --> 00:07:44,000
why would you want to run multiple types

00:07:42,639 --> 00:07:46,879
of these systems

00:07:44,000 --> 00:07:48,000
if you could decide on just one the

00:07:46,879 --> 00:07:51,360
other reason

00:07:48,000 --> 00:07:53,680
is that having more data

00:07:51,360 --> 00:07:55,039
inside the single database allows you to

00:07:53,680 --> 00:07:58,879
use joints

00:07:55,039 --> 00:08:02,240
to do analysis across these data types

00:07:58,879 --> 00:08:05,199
this allows for correlations

00:08:02,240 --> 00:08:06,160
more complex analytics as well as

00:08:05,199 --> 00:08:09,360
predictions

00:08:06,160 --> 00:08:13,199
and ml if you want to learn more about

00:08:09,360 --> 00:08:15,759
this you can go to the following website

00:08:13,199 --> 00:08:16,560
to get slides and the resources from

00:08:15,759 --> 00:08:19,599
this talk

00:08:16,560 --> 00:08:19,919
i encourage you to check out tops which

00:08:19,599 --> 00:08:23,520
is

00:08:19,919 --> 00:08:27,280
the observability stack for kubernetes

00:08:23,520 --> 00:08:30,639
which is a cli tool that allows you to

00:08:27,280 --> 00:08:33,680
deploy a observability

00:08:30,639 --> 00:08:36,800
suite inside of kubernetes in

00:08:33,680 --> 00:08:40,000
under five minutes that sweet includes

00:08:36,800 --> 00:08:43,200
prometheus grafana prom scale and

00:08:40,000 --> 00:08:45,839
other components and of course check

00:08:43,200 --> 00:08:47,680
us out on github it's worth mentioning

00:08:45,839 --> 00:08:50,800
that we are hosting

00:08:47,680 --> 00:08:51,519
a bit of a further discussion later on

00:08:50,800 --> 00:08:54,000
in this

00:08:51,519 --> 00:08:56,160
conference about dealing with this kind

00:08:54,000 --> 00:08:57,600
of messy world where you have multiple

00:08:56,160 --> 00:09:00,959
metric systems

00:08:57,600 --> 00:09:04,000
or legacy and cloud deployments and

00:09:00,959 --> 00:09:04,399
this plethora of complexity and i hope

00:09:04,000 --> 00:09:07,680
to

00:09:04,399 --> 00:09:08,240
see you at that discussion thank you and

00:09:07,680 --> 00:09:12,160
you can

00:09:08,240 --> 00:09:16,920
find me on twitter at 7ny

00:09:12,160 --> 00:09:19,920
or by email at matt timescare.com

00:09:16,920 --> 00:09:19,920

YouTube URL: https://www.youtube.com/watch?v=AuQSf_bunMI


