Title: Auto-suggesting and generating recording rules for Prometheus - Shreyas Srivatsan & Gibbs Cullen
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Auto-suggesting and generating recording rules for Prometheus - Shreyas Srivatsan & Gibbs Cullen, Chronosphere

"High cardinality metrics often cause alerts and dashboards to time out when they try to fetch too much data. Prometheus provides recording rules to speed up queries by pre-generating the queries, however, they have to be configured manually and require reconfiguring alerts and dashboards to point to the recorded series. The performance degradation often happens as new metrics are introduced with more instances or deploys and a working query may break all of a sudden.

In this talk, we will show you how slow queries can be preemptively detected and automatically sped up without any manual reconfiguration. This is done by automatically analyzing the widely available inbuilt Prometheus query log and generating suggested recording rules for frequently queried metrics that take considerable time to execute.

We’ll walk through a concrete demo of the tool which can also use parameters min-query-time and min-query-count to help suggest the most impactful recording rules: https://github.com/chronosphereio/high-cardinality-analyzer."
Captions: 
	00:00:00,000 --> 00:00:03,840
hi everybody my name is shreyas and

00:00:02,240 --> 00:00:05,759
today we're here to

00:00:03,840 --> 00:00:08,160
talk about how we can auto suggest and

00:00:05,759 --> 00:00:10,559
auto generate recording rules

00:00:08,160 --> 00:00:12,480
uh with the goal of kind of speeding up

00:00:10,559 --> 00:00:16,160
dashboards and kind of in general the

00:00:12,480 --> 00:00:18,560
speeding up query performance

00:00:16,160 --> 00:00:19,920
my name is uh i'm a technical leader

00:00:18,560 --> 00:00:22,560
chronosphere where we're building a

00:00:19,920 --> 00:00:25,119
hosted metrics and monitoring platform

00:00:22,560 --> 00:00:26,640
uh targeting large-scale high-throughput

00:00:25,119 --> 00:00:30,000
use cases

00:00:26,640 --> 00:00:32,960
uh the platform's built on m3 which is

00:00:30,000 --> 00:00:34,160
open source kind of remote storage back

00:00:32,960 --> 00:00:37,520
end

00:00:34,160 --> 00:00:38,079
for prometheus prior to this i was an

00:00:37,520 --> 00:00:40,800
uber

00:00:38,079 --> 00:00:42,960
on the observation primarily working on

00:00:40,800 --> 00:00:46,559
alerting and dashboarding

00:00:42,960 --> 00:00:48,640
and yeah the agenda for today

00:00:46,559 --> 00:00:49,600
is first kind of want to lay out the

00:00:48,640 --> 00:00:52,719
problem statement

00:00:49,600 --> 00:00:53,199
talking about high cardinality use cases

00:00:52,719 --> 00:00:56,239
where

00:00:53,199 --> 00:00:58,000
aggregation of metrics are useful uh

00:00:56,239 --> 00:00:59,280
then we kind of talk about a couple of

00:00:58,000 --> 00:01:00,879
different ways where we can actually

00:00:59,280 --> 00:01:03,920
aggregate metrics

00:01:00,879 --> 00:01:05,920
uh to speed dashboards up um

00:01:03,920 --> 00:01:07,280
and speed querying up in general uh we

00:01:05,920 --> 00:01:10,320
follow that up with a demo

00:01:07,280 --> 00:01:12,240
of uh how we can make how how we can

00:01:10,320 --> 00:01:14,799
make this possible

00:01:12,240 --> 00:01:16,080
and finally kind of the goal is to just

00:01:14,799 --> 00:01:18,720
show how we can make this

00:01:16,080 --> 00:01:18,720
easy to use

00:01:19,119 --> 00:01:22,640
so first kind of just setting up the

00:01:20,720 --> 00:01:25,680
problem statement

00:01:22,640 --> 00:01:26,159
high cardinality metrics is basically

00:01:25,680 --> 00:01:29,759
kind of

00:01:26,159 --> 00:01:30,479
uh like is is basically the area where

00:01:29,759 --> 00:01:33,280
we

00:01:30,479 --> 00:01:34,880
probably want aggregation uh c advisor

00:01:33,280 --> 00:01:38,000
metrics are kind of a very

00:01:34,880 --> 00:01:40,000
classic case of high quality metrics uh

00:01:38,000 --> 00:01:42,399
c advisor basically

00:01:40,000 --> 00:01:43,840
uh is a way to get resource usage and

00:01:42,399 --> 00:01:45,280
performance metrics of running

00:01:43,840 --> 00:01:49,200
containers

00:01:45,280 --> 00:01:51,680
um so it's essentially like cpu

00:01:49,200 --> 00:01:54,159
memory disk network traffic and kind of

00:01:51,680 --> 00:01:56,880
all kind of the infrastructure level

00:01:54,159 --> 00:01:58,079
uh things for containers uh this is kind

00:01:56,880 --> 00:02:00,399
of a very simple

00:01:58,079 --> 00:02:01,759
like c advisor dashboard which is kind

00:02:00,399 --> 00:02:05,119
of monitoring like

00:02:01,759 --> 00:02:07,920
5000 dot containers and has

00:02:05,119 --> 00:02:09,440
various metrics on it if you kind of

00:02:07,920 --> 00:02:11,360
look at the dashboard there's some

00:02:09,440 --> 00:02:12,160
aggregate metrics and you also kind of

00:02:11,360 --> 00:02:14,400
have like per

00:02:12,160 --> 00:02:15,520
pod metrics uh where you kind of want to

00:02:14,400 --> 00:02:18,560
look at like

00:02:15,520 --> 00:02:21,840
the top usage information

00:02:18,560 --> 00:02:23,360
so such a dashboard like just kind of

00:02:21,840 --> 00:02:25,360
looking at the scale of the

00:02:23,360 --> 00:02:29,200
carnality of data you realize that they

00:02:25,360 --> 00:02:31,519
can come really slow really quickly

00:02:29,200 --> 00:02:33,840
and this is where kind of like you can

00:02:31,519 --> 00:02:36,959
make portions of the dashboard faster by

00:02:33,840 --> 00:02:38,400
using kind of pre-aggregations an

00:02:36,959 --> 00:02:42,400
example of kind of

00:02:38,400 --> 00:02:45,040
how c like c advisor metrics are slow

00:02:42,400 --> 00:02:48,080
is just looking at something simple like

00:02:45,040 --> 00:02:50,720
container cpu usage

00:02:48,080 --> 00:02:51,360
container cpu said just with kind of all

00:02:50,720 --> 00:02:54,720
the

00:02:51,360 --> 00:02:57,920
tags that get added by c advisor like

00:02:54,720 --> 00:03:01,040
as the metrics are script uh

00:02:57,920 --> 00:03:03,599
for the same for the same environment

00:03:01,040 --> 00:03:04,879
you kind of have 16 000 series

00:03:03,599 --> 00:03:06,640
and if you just want to kind of just

00:03:04,879 --> 00:03:08,000
display all of these things it's going

00:03:06,640 --> 00:03:11,280
to take 20 seconds to

00:03:08,000 --> 00:03:14,800
just display them so any

00:03:11,280 --> 00:03:15,920
simple kind of operation like doing a

00:03:14,800 --> 00:03:18,239
sum on these or

00:03:15,920 --> 00:03:19,920
doing a max of these is basically going

00:03:18,239 --> 00:03:21,599
to be really reasonable

00:03:19,920 --> 00:03:23,519
but you probably don't really care about

00:03:21,599 --> 00:03:25,040
kind of the whole combination of all of

00:03:23,519 --> 00:03:27,840
these metrics

00:03:25,040 --> 00:03:29,120
so what you can do is this is an example

00:03:27,840 --> 00:03:30,560
where you're going to just

00:03:29,120 --> 00:03:32,319
taken the same metric and you've

00:03:30,560 --> 00:03:33,760
aggregated them to just like a couple of

00:03:32,319 --> 00:03:35,280
different labels

00:03:33,760 --> 00:03:37,599
in this case a couple of labels we've

00:03:35,280 --> 00:03:39,519
aggregated them are kind of the cluster

00:03:37,599 --> 00:03:40,720
and kind of the name space the container

00:03:39,519 --> 00:03:42,879
is in

00:03:40,720 --> 00:03:44,480
uh if you're kind of using name spaces

00:03:42,879 --> 00:03:46,720
as the place

00:03:44,480 --> 00:03:47,680
like as a division of your services or

00:03:46,720 --> 00:03:50,799
have like some

00:03:47,680 --> 00:03:53,439
form of some form of division kind of

00:03:50,799 --> 00:03:56,959
this gives you

00:03:53,439 --> 00:03:58,319
like your cpu usage per customer or

00:03:56,959 --> 00:04:01,439
tenant or

00:03:58,319 --> 00:04:03,040
some yeah some notion of that

00:04:01,439 --> 00:04:05,519
so this is probably good enough to tell

00:04:03,040 --> 00:04:07,840
you how a particular like

00:04:05,519 --> 00:04:08,640
portion of your uh code is actually

00:04:07,840 --> 00:04:09,920
working

00:04:08,640 --> 00:04:11,760
now this doesn't mean that we don't

00:04:09,920 --> 00:04:14,000
actually want the underlying series

00:04:11,760 --> 00:04:15,599
like when we actually have an issue uh

00:04:14,000 --> 00:04:17,120
within a particular namespace and a

00:04:15,599 --> 00:04:19,040
cluster you probably want those

00:04:17,120 --> 00:04:20,160
underlying like individual series to

00:04:19,040 --> 00:04:22,000
actually

00:04:20,160 --> 00:04:24,560
uh go and like dig deep into that

00:04:22,000 --> 00:04:26,160
namespace to see what's wrong

00:04:24,560 --> 00:04:28,240
but just from a perspective kind of what

00:04:26,160 --> 00:04:30,320
you want from the overview dashboard or

00:04:28,240 --> 00:04:31,840
something you want to alert on this is

00:04:30,320 --> 00:04:35,840
kind of what

00:04:31,840 --> 00:04:40,720
you would want to see normally

00:04:35,840 --> 00:04:43,680
so high quality metrics like as we see

00:04:40,720 --> 00:04:45,040
it's just like have many dimensions and

00:04:43,680 --> 00:04:48,160
can kind of be really

00:04:45,040 --> 00:04:49,440
slow to query so really high quality

00:04:48,160 --> 00:04:50,960
metrics are

00:04:49,440 --> 00:04:53,199
something that just leads to slowing

00:04:50,960 --> 00:04:55,759
dashboards uh the cardinality of

00:04:53,199 --> 00:04:57,919
dimensions can keep increasing

00:04:55,759 --> 00:04:58,800
as we add new instances roll out new

00:04:57,919 --> 00:05:01,039
images

00:04:58,800 --> 00:05:02,479
new instance tags show up new image tags

00:05:01,039 --> 00:05:04,560
show up

00:05:02,479 --> 00:05:06,720
these basically result in more number of

00:05:04,560 --> 00:05:08,960
unique series

00:05:06,720 --> 00:05:10,639
and as these new tags come up you just

00:05:08,960 --> 00:05:12,240
keep having like an explosion of the

00:05:10,639 --> 00:05:15,680
number of series

00:05:12,240 --> 00:05:18,000
um if you want kind of craze which

00:05:15,680 --> 00:05:19,360
are like spanning across time like

00:05:18,000 --> 00:05:21,919
across days or

00:05:19,360 --> 00:05:23,840
even like hours when like new rollouts

00:05:21,919 --> 00:05:25,680
have happened

00:05:23,840 --> 00:05:27,039
we just end up like crazy end up

00:05:25,680 --> 00:05:27,840
capturing more and more underlying

00:05:27,039 --> 00:05:29,680
series

00:05:27,840 --> 00:05:31,840
eventually just leading to slowing

00:05:29,680 --> 00:05:34,560
dashboards

00:05:31,840 --> 00:05:36,639
so what we see as users is that this

00:05:34,560 --> 00:05:39,199
dashboard which is really fast today

00:05:36,639 --> 00:05:41,120
like over the next few weeks just keeps

00:05:39,199 --> 00:05:44,639
on getting slower

00:05:41,120 --> 00:05:48,400
eventually it leads to kind of a browser

00:05:44,639 --> 00:05:49,680
like getting locked up uh and then

00:05:48,400 --> 00:05:52,080
when you actually want to use these

00:05:49,680 --> 00:05:53,600
dashboards and engineers then notices

00:05:52,080 --> 00:05:56,960
this and then realizes

00:05:53,600 --> 00:05:59,280
okay the dashboard needs to be optimized

00:05:56,960 --> 00:06:01,120
so how do you actually kind of debug and

00:05:59,280 --> 00:06:02,720
like approach this problem

00:06:01,120 --> 00:06:05,199
uh the first step is to kind of figure

00:06:02,720 --> 00:06:08,880
out which queries are the culprit

00:06:05,199 --> 00:06:10,800
inspecting the requests from a dashboard

00:06:08,880 --> 00:06:13,039
to look for these slow queries is a good

00:06:10,800 --> 00:06:15,199
start you can do that in one of the

00:06:13,039 --> 00:06:17,039
browser inspectors like chrome inspector

00:06:15,199 --> 00:06:20,240
or firefox inspect

00:06:17,039 --> 00:06:21,680
panel but even if you kind of know what

00:06:20,240 --> 00:06:24,000
these queries are then kind of you have

00:06:21,680 --> 00:06:25,840
to associate that back to the panels uh

00:06:24,000 --> 00:06:27,600
on a last a large dashboard that's not

00:06:25,840 --> 00:06:29,680
always easy

00:06:27,600 --> 00:06:31,759
another option is to use the prometheus

00:06:29,680 --> 00:06:33,520
query log

00:06:31,759 --> 00:06:35,440
which kind of has information of all the

00:06:33,520 --> 00:06:38,240
queries running in the system

00:06:35,440 --> 00:06:39,039
uh if you're able to kind of parse that

00:06:38,240 --> 00:06:41,520
information

00:06:39,039 --> 00:06:43,440
then you can kind of associate that back

00:06:41,520 --> 00:06:46,479
to a dashboard

00:06:43,440 --> 00:06:48,160
but that again is kind of difficult

00:06:46,479 --> 00:06:50,000
but once we figure out kind of which are

00:06:48,160 --> 00:06:50,720
these which queries are actually the

00:06:50,000 --> 00:06:52,800
culprit

00:06:50,720 --> 00:06:54,560
uh we actually need to have a second

00:06:52,800 --> 00:06:56,080
step and we need to decide that

00:06:54,560 --> 00:06:57,919
we can actually pre-aggregate these

00:06:56,080 --> 00:06:59,440
metrics uh to actually speed up the

00:06:57,919 --> 00:07:02,479
queries

00:06:59,440 --> 00:07:03,919
uh so let's kind of talk about like the

00:07:02,479 --> 00:07:06,400
different options we have to kind of

00:07:03,919 --> 00:07:09,599
aggregate metrics

00:07:06,400 --> 00:07:10,900
so recording rules are probably the most

00:07:09,599 --> 00:07:12,240
obvious option to

00:07:10,900 --> 00:07:15,199
[Music]

00:07:12,240 --> 00:07:16,720
aggregate metrics they basically are

00:07:15,199 --> 00:07:18,960
supported by prometheus and allow

00:07:16,720 --> 00:07:22,560
pre-computing like frequently needed

00:07:18,960 --> 00:07:24,960
uh or like expensive queries

00:07:22,560 --> 00:07:26,479
and they basically store back that great

00:07:24,960 --> 00:07:29,440
time to restore the tstb

00:07:26,479 --> 00:07:30,720
so if you actually want to like query

00:07:29,440 --> 00:07:32,800
the information

00:07:30,720 --> 00:07:34,400
you just make like simple queries which

00:07:32,800 --> 00:07:36,080
just like pulls back a single time

00:07:34,400 --> 00:07:37,919
series and so the queries get really

00:07:36,080 --> 00:07:39,919
become really fast

00:07:37,919 --> 00:07:41,919
uh once we do that you can just have

00:07:39,919 --> 00:07:43,599
dashboards point back to kind of

00:07:41,919 --> 00:07:45,199
pre-computer time series

00:07:43,599 --> 00:07:48,080
uh the really cool thing about recording

00:07:45,199 --> 00:07:51,120
rules is you basically have

00:07:48,080 --> 00:07:54,560
the your you basically have the all of

00:07:51,120 --> 00:07:55,840
like uh all of comql like functions

00:07:54,560 --> 00:07:58,319
available to you

00:07:55,840 --> 00:08:00,400
so you can do like anything as complex

00:07:58,319 --> 00:08:03,440
as you want

00:08:00,400 --> 00:08:04,639
to actually do your pre-computation an

00:08:03,440 --> 00:08:06,560
example we give here

00:08:04,639 --> 00:08:08,879
is we have these two metrics which is

00:08:06,560 --> 00:08:10,560
kind of total and available bytes

00:08:08,879 --> 00:08:12,960
but if you want a dashboard which is

00:08:10,560 --> 00:08:14,479
just going to show you percentage used

00:08:12,960 --> 00:08:16,240
then you can actually write a recording

00:08:14,479 --> 00:08:18,800
rule which has does that

00:08:16,240 --> 00:08:20,639
which basically has uh which is

00:08:18,800 --> 00:08:22,560
basically doing a computation of like

00:08:20,639 --> 00:08:23,120
total minus available divided by total

00:08:22,560 --> 00:08:25,520
and then

00:08:23,120 --> 00:08:26,319
kind of stores that back in this new

00:08:25,520 --> 00:08:27,840
metric

00:08:26,319 --> 00:08:30,160
and then the dashboard can just query

00:08:27,840 --> 00:08:32,240
this individual metric

00:08:30,160 --> 00:08:33,200
so recording rules basically they

00:08:32,240 --> 00:08:34,479
execute and like

00:08:33,200 --> 00:08:36,399
pre-compute the query at regular

00:08:34,479 --> 00:08:39,680
intervals like as you define it

00:08:36,399 --> 00:08:40,240
every minute 30 seconds you can probably

00:08:39,680 --> 00:08:42,240
do it

00:08:40,240 --> 00:08:43,279
at the same interval that things get

00:08:42,240 --> 00:08:45,040
scraped so

00:08:43,279 --> 00:08:47,839
all your metrics kind of can have the

00:08:45,040 --> 00:08:52,240
same uh resolution

00:08:47,839 --> 00:08:54,480
um the one kind of a couple of downsides

00:08:52,240 --> 00:08:56,480
are like one probably downside with

00:08:54,480 --> 00:08:58,080
recording rules is that

00:08:56,480 --> 00:08:59,839
the underlying time series still need to

00:08:58,080 --> 00:09:01,680
be stored in the tsdb

00:08:59,839 --> 00:09:03,279
and the queries accessing many time

00:09:01,680 --> 00:09:06,480
series can get expensive

00:09:03,279 --> 00:09:08,480
so if we are actually aggregating across

00:09:06,480 --> 00:09:10,160
many many time series then

00:09:08,480 --> 00:09:12,000
the query itself may take tens of

00:09:10,160 --> 00:09:13,600
seconds so

00:09:12,000 --> 00:09:16,480
that kind of determines like how

00:09:13,600 --> 00:09:19,440
frequently you can actually like

00:09:16,480 --> 00:09:21,600
run your recording vocalize but there

00:09:19,440 --> 00:09:23,519
are certain situations where

00:09:21,600 --> 00:09:25,040
we do not necessarily need the

00:09:23,519 --> 00:09:27,600
underlying metrics

00:09:25,040 --> 00:09:29,200
uh and like the raw metrics can be

00:09:27,600 --> 00:09:31,519
dropped and not stored

00:09:29,200 --> 00:09:33,519
uh and we know we may not also care

00:09:31,519 --> 00:09:34,720
about kind of the more complex queries

00:09:33,519 --> 00:09:35,360
where recording rules are really

00:09:34,720 --> 00:09:38,480
powerful

00:09:35,360 --> 00:09:39,839
so for that kind of we want to go

00:09:38,480 --> 00:09:41,920
i want to go over kind of the m3

00:09:39,839 --> 00:09:44,399
aggregation tier

00:09:41,920 --> 00:09:45,279
which does not support kind of all the

00:09:44,399 --> 00:09:47,120
functionality

00:09:45,279 --> 00:09:48,320
that recording rules does but kind of

00:09:47,120 --> 00:09:49,920
does it in a more

00:09:48,320 --> 00:09:51,519
like does the aggregations in a

00:09:49,920 --> 00:09:54,000
different style

00:09:51,519 --> 00:09:55,680
uh so m3 basically is a remote storage

00:09:54,000 --> 00:09:58,720
for prometheus

00:09:55,680 --> 00:10:00,720
uh it basically kind of moves the

00:09:58,720 --> 00:10:04,000
expensive recording rule computation

00:10:00,720 --> 00:10:06,480
to a streaming aggregation tier so it's

00:10:04,000 --> 00:10:07,040
not running the query engine itself but

00:10:06,480 --> 00:10:08,880
rather

00:10:07,040 --> 00:10:11,440
happens in a streaming aggregation tier

00:10:08,880 --> 00:10:13,360
before metrics get persisted

00:10:11,440 --> 00:10:15,760
uh the aggregator basically allows down

00:10:13,360 --> 00:10:17,120
sampling dropping or aggregating metrics

00:10:15,760 --> 00:10:21,120
prior to persisting them

00:10:17,120 --> 00:10:24,560
to have remote storage in this case m3db

00:10:21,120 --> 00:10:27,839
um the aggregation tier allows like

00:10:24,560 --> 00:10:28,720
two types of uh aggregations uh one is

00:10:27,839 --> 00:10:31,040
roll-up rules

00:10:28,720 --> 00:10:32,800
which allow you to kind of aggregate

00:10:31,040 --> 00:10:36,000
like a cross metrics

00:10:32,800 --> 00:10:38,399
apply functions like sum max etc

00:10:36,000 --> 00:10:39,760
and then mapping rules which have a

00:10:38,399 --> 00:10:40,880
couple of different purposes for the

00:10:39,760 --> 00:10:43,440
purpose of

00:10:40,880 --> 00:10:44,720
this talk we're just going to talk about

00:10:43,440 --> 00:10:48,320
them as

00:10:44,720 --> 00:10:48,320
having the ability to drop metrics

00:10:48,640 --> 00:10:52,480
so roll-up rules essentially they

00:10:50,560 --> 00:10:55,440
contain a series of transforms

00:10:52,480 --> 00:10:56,800
uh which are applied in order uh metrics

00:10:55,440 --> 00:10:59,839
applied to them

00:10:56,800 --> 00:11:00,959
like the metrics that that these roller

00:10:59,839 --> 00:11:04,240
pools get applied on

00:11:00,959 --> 00:11:05,680
kind of depends on a filter match uh now

00:11:04,240 --> 00:11:07,440
roll-up rules kind of you have like

00:11:05,680 --> 00:11:08,480
three different steps the first one is

00:11:07,440 --> 00:11:11,040
to take a delta

00:11:08,480 --> 00:11:14,160
the second one is kind of sum by like

00:11:11,040 --> 00:11:16,560
the dimensions that you're interested in

00:11:14,160 --> 00:11:18,800
and then finally kind of create a

00:11:16,560 --> 00:11:22,720
monotonic uh

00:11:18,800 --> 00:11:22,720
monotonic cumulative counter

00:11:23,279 --> 00:11:26,800
so let's look at kind of what each of

00:11:24,720 --> 00:11:29,360
these steps actually entail

00:11:26,800 --> 00:11:30,560
the first step here which is called an

00:11:29,360 --> 00:11:32,640
increased transform

00:11:30,560 --> 00:11:34,160
is basically kind of taking the delta of

00:11:32,640 --> 00:11:36,000
the underlying series

00:11:34,160 --> 00:11:37,279
so the metrics which come in are

00:11:36,000 --> 00:11:40,320
actually

00:11:37,279 --> 00:11:44,160
monotonically increasing metrics as uh

00:11:40,320 --> 00:11:46,480
all um as prometheus counters are

00:11:44,160 --> 00:11:48,880
um so before they can actually be rolled

00:11:46,480 --> 00:11:50,079
up we kind of need to get the deltas to

00:11:48,880 --> 00:11:52,959
figure out kind of like

00:11:50,079 --> 00:11:53,680
what the value is at the at the specific

00:11:52,959 --> 00:11:55,360
times

00:11:53,680 --> 00:11:57,279
so first step kind of like gives you

00:11:55,360 --> 00:11:59,040
delta is for each of these

00:11:57,279 --> 00:12:00,639
and these deltas are kind of sent over

00:11:59,040 --> 00:12:02,639
to the roll-up

00:12:00,639 --> 00:12:03,760
the roll-up basically now takes these

00:12:02,639 --> 00:12:06,000
deltas and then

00:12:03,760 --> 00:12:09,120
they sum by the unique dimensions

00:12:06,000 --> 00:12:11,680
specified in the group by

00:12:09,120 --> 00:12:12,399
for the container cpu usage we basically

00:12:11,680 --> 00:12:14,000
want to

00:12:12,399 --> 00:12:16,320
sum them up by container name and

00:12:14,000 --> 00:12:18,399
namespace so we're going to just

00:12:16,320 --> 00:12:20,079
like sum them up back into our name and

00:12:18,399 --> 00:12:22,959
namespace and

00:12:20,079 --> 00:12:24,880
we want to do an activation is just some

00:12:22,959 --> 00:12:25,839
so that's what kind of this roll-up step

00:12:24,880 --> 00:12:28,160
is

00:12:25,839 --> 00:12:30,320
so once we have these kind of delta sums

00:12:28,160 --> 00:12:32,160
or sums of deltas

00:12:30,320 --> 00:12:36,320
the next step is kind of transform them

00:12:32,160 --> 00:12:36,320
back into the monotonically increasing

00:12:37,120 --> 00:12:43,200
format that to actually store them to

00:12:39,760 --> 00:12:45,200
make them compatible prometheus again

00:12:43,200 --> 00:12:46,959
so the last one does basically a

00:12:45,200 --> 00:12:49,839
cumulative add of these metrics

00:12:46,959 --> 00:12:51,040
uh to end up with like an aggregated

00:12:49,839 --> 00:12:54,800
time series

00:12:51,040 --> 00:12:58,240
uh this is basically sent to m3db um

00:12:54,800 --> 00:12:58,639
and the namespace essentially identifies

00:12:58,240 --> 00:13:00,880
like

00:12:58,639 --> 00:13:03,680
where to actually store this with an m3b

00:13:00,880 --> 00:13:03,680
it's a yeah

00:13:03,920 --> 00:13:08,639
so the second thing is now we've been

00:13:06,639 --> 00:13:10,000
able to roll these up

00:13:08,639 --> 00:13:12,240
and we've actually been able to roll

00:13:10,000 --> 00:13:15,040
these up before uh

00:13:12,240 --> 00:13:15,360
persisting the raw metrics for storage

00:13:15,040 --> 00:13:17,440
so

00:13:15,360 --> 00:13:19,519
mapping rules now come in and they kind

00:13:17,440 --> 00:13:22,720
of allow us to drop these metrics

00:13:19,519 --> 00:13:23,760
based on a particular filter you don't

00:13:22,720 --> 00:13:25,200
have to do this

00:13:23,760 --> 00:13:28,320
but in the case where you kind of don't

00:13:25,200 --> 00:13:30,720
want the original unaggregated series

00:13:28,320 --> 00:13:32,320
this kind of can save a lot of storage

00:13:30,720 --> 00:13:35,120
space so if you actually don't want the

00:13:32,320 --> 00:13:38,639
per container

00:13:35,120 --> 00:13:40,399
cpu information then like or

00:13:38,639 --> 00:13:43,040
rather you don't want kind of the other

00:13:40,399 --> 00:13:45,440
dimensions of the cpu information here

00:13:43,040 --> 00:13:47,600
then you can kind of sum that metric has

00:13:45,440 --> 00:13:49,839
already been stored

00:13:47,600 --> 00:13:51,360
and you can say like with a similar

00:13:49,839 --> 00:13:53,760
filter that i just want

00:13:51,360 --> 00:13:56,079
all the original series to be dropped so

00:13:53,760 --> 00:13:58,720
it's just a space-saving thing

00:13:56,079 --> 00:13:59,199
the other kind of neat thing about doing

00:13:58,720 --> 00:14:01,839
this

00:13:59,199 --> 00:14:02,560
is that if we do something like this we

00:14:01,839 --> 00:14:05,360
can actually

00:14:02,560 --> 00:14:07,120
store the aggregate series with the same

00:14:05,360 --> 00:14:09,440
name as the original metric

00:14:07,120 --> 00:14:11,279
so depending on how the dashboard

00:14:09,440 --> 00:14:13,199
queries are set up

00:14:11,279 --> 00:14:14,639
you potentially can speed up dashboards

00:14:13,199 --> 00:14:18,079
without even

00:14:14,639 --> 00:14:20,079
making any changes to them

00:14:18,079 --> 00:14:22,079
so quick summary of aggregation tier

00:14:20,079 --> 00:14:24,880
essentially allows for ingestion time

00:14:22,079 --> 00:14:26,800
streaming aggregation uh the metrics can

00:14:24,880 --> 00:14:28,639
be aggregated rolled up based on defined

00:14:26,800 --> 00:14:31,760
rules

00:14:28,639 --> 00:14:34,320
and raw metrics can basically be dropped

00:14:31,760 --> 00:14:37,199
based on like whatever matching filters

00:14:34,320 --> 00:14:40,480
uh by using mapping rules

00:14:37,199 --> 00:14:42,240
uh just a quick kind of uh comparison

00:14:40,480 --> 00:14:44,560
between both of these

00:14:42,240 --> 00:14:46,560
recording rules obviously the biggest

00:14:44,560 --> 00:14:49,680
advantage really is that their general

00:14:46,560 --> 00:14:52,240
purpose and kind of support full prom ql

00:14:49,680 --> 00:14:54,160
uh they're a bit expensive um because

00:14:52,240 --> 00:14:56,240
they run against the query engine so

00:14:54,160 --> 00:14:58,399
they may affect other queries role pools

00:14:56,240 --> 00:15:02,000
are also expensive but they just

00:14:58,399 --> 00:15:03,680
can't happen to happen happen to

00:15:02,000 --> 00:15:05,120
occur kind of at a different stage of

00:15:03,680 --> 00:15:06,560
the pipeline so

00:15:05,120 --> 00:15:08,800
they don't affect kind of queries

00:15:06,560 --> 00:15:11,360
themselves

00:15:08,800 --> 00:15:12,000
the one kind of thing is like all the

00:15:11,360 --> 00:15:14,320
data

00:15:12,000 --> 00:15:15,680
has to be stored so there's a higher

00:15:14,320 --> 00:15:18,320
storage cost

00:15:15,680 --> 00:15:19,600
um as against kind of roll-up rules

00:15:18,320 --> 00:15:21,600
where

00:15:19,600 --> 00:15:23,440
you don't necessarily need to store all

00:15:21,600 --> 00:15:25,680
the data

00:15:23,440 --> 00:15:27,440
scroll up rules in general are kind of

00:15:25,680 --> 00:15:28,560
more efficient to run as an injection

00:15:27,440 --> 00:15:30,560
time aggregation

00:15:28,560 --> 00:15:32,320
or at least kind of it separates it out

00:15:30,560 --> 00:15:35,360
into a separate

00:15:32,320 --> 00:15:36,639
separate piece of software

00:15:35,360 --> 00:15:38,639
it gives them really only strong

00:15:36,639 --> 00:15:40,880
aggregates

00:15:38,639 --> 00:15:42,800
if we only store the aggregates then you

00:15:40,880 --> 00:15:43,920
can potentially speed up dashboards and

00:15:42,800 --> 00:15:45,440
queries

00:15:43,920 --> 00:15:47,440
uh because they can have the same metric

00:15:45,440 --> 00:15:49,040
name but really the biggest downside is

00:15:47,440 --> 00:15:51,360
it does not support full prompt ql

00:15:49,040 --> 00:15:53,040
uh but only specific rate so if you have

00:15:51,360 --> 00:15:56,160
a sam or a max then

00:15:53,040 --> 00:15:58,399
yes this works really well uh

00:15:56,160 --> 00:15:59,839
with kind of more complicated like

00:15:58,399 --> 00:16:01,440
functions in prom ql

00:15:59,839 --> 00:16:02,959
like you want to divide like two

00:16:01,440 --> 00:16:06,240
different series

00:16:02,959 --> 00:16:08,320
uh less in our example earlier then yeah

00:16:06,240 --> 00:16:12,399
these don't really work

00:16:08,320 --> 00:16:14,320
now but either of these cases um

00:16:12,399 --> 00:16:15,680
there are challenges using them you kind

00:16:14,320 --> 00:16:17,199
of need to figure out kind of what to

00:16:15,680 --> 00:16:19,040
pre-compute

00:16:17,199 --> 00:16:22,079
uh and for that you need to figure out

00:16:19,040 --> 00:16:23,519
bad queries by analyzing dashboards

00:16:22,079 --> 00:16:25,519
then you need to kind of configure the

00:16:23,519 --> 00:16:26,880
aggregation rules

00:16:25,519 --> 00:16:29,600
change the dashboard to query for

00:16:26,880 --> 00:16:31,200
aggregated metrics

00:16:29,600 --> 00:16:32,720
after you do all that

00:16:31,200 --> 00:16:34,320
[Music]

00:16:32,720 --> 00:16:36,079
you may have not gone and changed every

00:16:34,320 --> 00:16:38,480
panel so what happens when metric

00:16:36,079 --> 00:16:40,079
changes or a second panel becomes slow

00:16:38,480 --> 00:16:42,160
you end up having to repeat the process

00:16:40,079 --> 00:16:44,480
all over again

00:16:42,160 --> 00:16:45,360
so really is it possible to do this

00:16:44,480 --> 00:16:47,839
automatically

00:16:45,360 --> 00:16:49,839
and we're going to show you a way where

00:16:47,839 --> 00:16:52,880
we can kind of do something

00:16:49,839 --> 00:16:55,199
reasonably automatically

00:16:52,880 --> 00:16:57,839
so to start with we have a couple of

00:16:55,199 --> 00:16:57,839
graphs here

00:16:58,399 --> 00:17:03,199
which which are continuously updating

00:17:01,120 --> 00:17:05,120
now

00:17:03,199 --> 00:17:06,559
they're generated using this tool or

00:17:05,120 --> 00:17:09,520
prominent workbench which

00:17:06,559 --> 00:17:11,600
just generates some synthetic metrics uh

00:17:09,520 --> 00:17:13,520
one is kind of disk usage and the other

00:17:11,600 --> 00:17:16,959
is a cpu usage metric

00:17:13,520 --> 00:17:18,720
uh and uh have like two queries which

00:17:16,959 --> 00:17:21,919
are kind of running

00:17:18,720 --> 00:17:24,799
uh max of the disk

00:17:21,919 --> 00:17:26,400
by a measurement type below we have two

00:17:24,799 --> 00:17:27,839
panels recording rules and we'll show

00:17:26,400 --> 00:17:29,200
you kind of how these recording rules

00:17:27,839 --> 00:17:33,120
can get populated

00:17:29,200 --> 00:17:34,640
automatically um it's coming here we

00:17:33,120 --> 00:17:35,919
have prom remote bench running which is

00:17:34,640 --> 00:17:37,760
like continuously

00:17:35,919 --> 00:17:40,320
like emitting metrics which are scraped

00:17:37,760 --> 00:17:42,000
by a prometheus instance we have here

00:17:40,320 --> 00:17:44,080
and in this tab we kind of have a high

00:17:42,000 --> 00:17:48,240
power analogy analyzer that

00:17:44,080 --> 00:17:51,520
uh we can actually run uh so we have the

00:17:48,240 --> 00:17:54,559
analyzer kind of set up to run in a loop

00:17:51,520 --> 00:17:55,440
um it's gonna run the high chrono

00:17:54,559 --> 00:17:56,880
analyzer

00:17:55,440 --> 00:17:59,440
it's pointing to the query log that

00:17:56,880 --> 00:18:01,280
prometheus is running with

00:17:59,440 --> 00:18:02,960
uh has a couple of different parameters

00:18:01,280 --> 00:18:03,280
here and it's indicating that you have

00:18:02,960 --> 00:18:06,400
to

00:18:03,280 --> 00:18:06,799
generate recording rules and push it

00:18:06,400 --> 00:18:08,720
into

00:18:06,799 --> 00:18:10,160
kind of the primitive cycle analogy

00:18:08,720 --> 00:18:11,679
rules

00:18:10,160 --> 00:18:13,520
and then it curls and kind of reloads

00:18:11,679 --> 00:18:17,760
the prometheus config and

00:18:13,520 --> 00:18:19,360
this kind of keeps doing it so

00:18:17,760 --> 00:18:20,720
if you're on the analyzer what this is

00:18:19,360 --> 00:18:22,240
going to do is it's going to look at the

00:18:20,720 --> 00:18:23,440
parameters query log it's going to go

00:18:22,240 --> 00:18:25,679
figure out kind of

00:18:23,440 --> 00:18:27,600
which queries are things that it can

00:18:25,679 --> 00:18:29,120
actually change

00:18:27,600 --> 00:18:31,360
and then based on that it's going to go

00:18:29,120 --> 00:18:33,600
and like generate recording rules

00:18:31,360 --> 00:18:35,120
uh so if you can see like what this is

00:18:33,600 --> 00:18:36,320
actually generated it's basically the

00:18:35,120 --> 00:18:39,280
indirect recording

00:18:36,320 --> 00:18:42,720
through python analyzer group uh

00:18:39,280 --> 00:18:44,400
generated a couple of different rules

00:18:42,720 --> 00:18:46,400
for kind of both the queries that we

00:18:44,400 --> 00:18:47,280
have so if we kind of go back to the

00:18:46,400 --> 00:18:51,039
dashboard

00:18:47,280 --> 00:18:54,240
now we'll actually

00:18:51,039 --> 00:18:56,559
very soon maybe let's kind of look at

00:18:54,240 --> 00:18:58,400
the last five minutes you kind of see

00:18:56,559 --> 00:19:00,480
these recording rules show up

00:18:58,400 --> 00:19:02,000
uh so what this is basically done is

00:19:00,480 --> 00:19:03,840
kind of taking the prometheus query log

00:19:02,000 --> 00:19:05,440
figured out what to do and just

00:19:03,840 --> 00:19:06,960
automatically kind of added these

00:19:05,440 --> 00:19:09,840
recording rules

00:19:06,960 --> 00:19:11,280
um so let's talk a bit about kind of how

00:19:09,840 --> 00:19:13,840
this is actually possible

00:19:11,280 --> 00:19:15,200
so let's kind of jump back uh to our

00:19:13,840 --> 00:19:18,960
slides

00:19:15,200 --> 00:19:20,799
um so fundamentally what we're doing is

00:19:18,960 --> 00:19:21,520
we're basically analyzing the parameters

00:19:20,799 --> 00:19:25,039
query log

00:19:21,520 --> 00:19:28,400
um and making decisions on like

00:19:25,039 --> 00:19:30,559
how to actually uh

00:19:28,400 --> 00:19:32,080
speak like on making decisions and what

00:19:30,559 --> 00:19:34,559
to actually speed up

00:19:32,080 --> 00:19:36,000
uh so we want to log so premier's query

00:19:34,559 --> 00:19:39,280
log basically logs

00:19:36,000 --> 00:19:42,559
all queries run by the promptql engine

00:19:39,280 --> 00:19:44,320
uh it provides like a bunch of times

00:19:42,559 --> 00:19:45,280
like the total evaluation time the

00:19:44,320 --> 00:19:47,520
amount of time it's

00:19:45,280 --> 00:19:49,679
spent in the queue like the preparation

00:19:47,520 --> 00:19:51,840
time for the query and kind of just the

00:19:49,679 --> 00:19:53,200
evaluation of the inner kind of

00:19:51,840 --> 00:19:54,880
expression

00:19:53,200 --> 00:19:57,440
so it has detailed information about

00:19:54,880 --> 00:19:58,080
kind of all steps of the actual curve

00:19:57,440 --> 00:19:59,600
itself

00:19:58,080 --> 00:20:01,520
on the right side is kind of a simple

00:19:59,600 --> 00:20:02,240
example for a particular query kind of

00:20:01,520 --> 00:20:05,280
like

00:20:02,240 --> 00:20:07,440
how these timings end up showing up

00:20:05,280 --> 00:20:09,120
so what the analyzer does is it's

00:20:07,440 --> 00:20:10,880
basically an offline process

00:20:09,120 --> 00:20:12,240
which helps us generate recording or

00:20:10,880 --> 00:20:15,039
roll-up rules

00:20:12,240 --> 00:20:17,760
uh uses the prometheus query log to find

00:20:15,039 --> 00:20:19,520
different candidates for aggregation

00:20:17,760 --> 00:20:21,280
and then it provides recommendations for

00:20:19,520 --> 00:20:23,440
recording rules or

00:20:21,280 --> 00:20:26,320
m3 aggregated roll up or mapping rules

00:20:23,440 --> 00:20:29,520
to kind of speed up expensive queries

00:20:26,320 --> 00:20:32,480
um how does it actually do this

00:20:29,520 --> 00:20:34,000
uh it goes over like days of prometheus

00:20:32,480 --> 00:20:37,600
query logs

00:20:34,000 --> 00:20:39,760
um it kind of finds the most commonly

00:20:37,600 --> 00:20:42,000
hit expensive queries

00:20:39,760 --> 00:20:42,799
how it determines expensive queries is

00:20:42,000 --> 00:20:44,720
basically

00:20:42,799 --> 00:20:46,400
kind of evaluation time like how much

00:20:44,720 --> 00:20:48,799
time it took to kind of evaluate the

00:20:46,400 --> 00:20:48,799
query

00:20:49,120 --> 00:20:53,679
if the query is expensive

00:20:52,159 --> 00:20:56,159
then you kind of need to figure out if

00:20:53,679 --> 00:20:59,520
it's a candidate for aggregation

00:20:56,159 --> 00:21:00,080
that basically depends on the cost of

00:20:59,520 --> 00:21:03,520
the query

00:21:00,080 --> 00:21:05,679
is it due to the like so cost over is

00:21:03,520 --> 00:21:08,240
probably due to the number of series

00:21:05,679 --> 00:21:09,840
but if we're occurring like 10 000

00:21:08,240 --> 00:21:12,240
series from the tsdb

00:21:09,840 --> 00:21:14,400
and we're returning 10 000 series then

00:21:12,240 --> 00:21:16,720
there's no aggregation really like

00:21:14,400 --> 00:21:17,440
worthwhile aggregation to do there on

00:21:16,720 --> 00:21:20,799
the other hand

00:21:17,440 --> 00:21:23,200
if the query has a bunch of

00:21:20,799 --> 00:21:25,200
sum max or different aggregation

00:21:23,200 --> 00:21:28,320
functions or it's doing operations like

00:21:25,200 --> 00:21:30,000
divides and multiplies

00:21:28,320 --> 00:21:31,520
then there's kind of an additional cost

00:21:30,000 --> 00:21:33,760
to this kind of like

00:21:31,520 --> 00:21:34,559
other than just just retrieving the

00:21:33,760 --> 00:21:38,320
series so

00:21:34,559 --> 00:21:40,080
it kind of goes and looks at um

00:21:38,320 --> 00:21:41,840
what gets returned from the query and

00:21:40,080 --> 00:21:43,919
like how much how many are there

00:21:41,840 --> 00:21:45,760
like how many series underlying series

00:21:43,919 --> 00:21:48,080
is the query actually capturing

00:21:45,760 --> 00:21:49,919
and based on that it determines if like

00:21:48,080 --> 00:21:52,960
it's worthwhile to actually make this

00:21:49,919 --> 00:21:55,120
and the application also

00:21:52,960 --> 00:21:56,799
allows you to like put in a couple of

00:21:55,120 --> 00:21:58,320
different parameters there

00:21:56,799 --> 00:22:00,480
for example i think in the example we

00:21:58,320 --> 00:22:02,400
had was we're looking for

00:22:00,480 --> 00:22:04,320
at least like a couple of queries which

00:22:02,400 --> 00:22:07,840
have taken more than a second to run

00:22:04,320 --> 00:22:10,080
and that's when it like kicks off the

00:22:07,840 --> 00:22:12,000
optimization

00:22:10,080 --> 00:22:14,720
so goal for the tool is to kind of

00:22:12,000 --> 00:22:16,000
provide proposals for recording a rollup

00:22:14,720 --> 00:22:17,679
rules

00:22:16,000 --> 00:22:20,320
and then users are kind of free to

00:22:17,679 --> 00:22:23,440
configure these rules as necessary

00:22:20,320 --> 00:22:26,000
uh if we do end up using recording rules

00:22:23,440 --> 00:22:28,720
or roller crews and dashboards and other

00:22:26,000 --> 00:22:31,840
places kind of need to be changed

00:22:28,720 --> 00:22:33,679
they need to be changed after that but

00:22:31,840 --> 00:22:35,600
the tool is kind of a baseline thing and

00:22:33,679 --> 00:22:37,520
then like you're kind of free to go and

00:22:35,600 --> 00:22:39,520
like extend it and kind of incorporate

00:22:37,520 --> 00:22:42,799
it into kind of whatever workflows

00:22:39,520 --> 00:22:44,559
that make sense to you um yeah so that's

00:22:42,799 --> 00:22:47,440
kind of what the tool is

00:22:44,559 --> 00:22:49,440
some resources this is a link to where

00:22:47,440 --> 00:22:51,360
tool is hosted

00:22:49,440 --> 00:22:53,360
like feel free to kind of take a look

00:22:51,360 --> 00:22:54,559
and give us suggestions and help us

00:22:53,360 --> 00:22:56,640
improve it

00:22:54,559 --> 00:22:57,760
a couple of other resources are kind of

00:22:56,640 --> 00:22:59,600
around like

00:22:57,760 --> 00:23:01,120
what recording rules are and why they're

00:22:59,600 --> 00:23:03,440
useful and

00:23:01,120 --> 00:23:06,240
uh there's also a help here on kind of

00:23:03,440 --> 00:23:09,280
the mt aggregator

00:23:06,240 --> 00:23:10,559
and yeah thank you uh thanks a lot for

00:23:09,280 --> 00:23:14,159
your time

00:23:10,559 --> 00:23:18,960
and yeah uh thanks for being here and

00:23:14,159 --> 00:23:18,960

YouTube URL: https://www.youtube.com/watch?v=Xxo_gNRd5T4


