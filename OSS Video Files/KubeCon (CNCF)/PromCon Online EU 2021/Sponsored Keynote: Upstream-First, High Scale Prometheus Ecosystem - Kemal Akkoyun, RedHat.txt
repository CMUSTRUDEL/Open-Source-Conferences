Title: Sponsored Keynote: Upstream-First, High Scale Prometheus Ecosystem - Kemal Akkoyun, RedHat
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Sponsored Keynote: Upstream-First, High Scale Prometheus Ecosystem - Kemal Akkoyun, Software Engineer, RedHat

Welcome to the beautiful world of cloud-native observability. Where things that your application is doing are understandable, clear, and non-surprising. Where you get notified only when the user is impacted or will be soon. Where you can in minutes drill down to what’s wrong, where the impact is and why. Where running the observability does not mean deploying 4 separate big, expensive systems with different UI and endpoints.

This is what is achievable right now with open source projects, we help maintain at Red Hat. Prometheus is no longer just one binary. It’s the way of thinking, a way of simplifying observability on all fronts. Join us to learn more about how Red Hat runs Prometheus and other open-source projects that share the same principles and patterns.
Captions: 
	00:00:00,160 --> 00:00:04,319
hello everyone i hope you are enjoying

00:00:02,240 --> 00:00:06,160
the conference so far

00:00:04,319 --> 00:00:08,639
today i am going to talk about how we

00:00:06,160 --> 00:00:11,679
use prometheus and its friends for

00:00:08,639 --> 00:00:13,440
openshift offering in reddit and i am

00:00:11,679 --> 00:00:15,519
going to tell you about the system that

00:00:13,440 --> 00:00:17,680
we built for remote health monitoring

00:00:15,519 --> 00:00:20,160
for the openshift clusters

00:00:17,680 --> 00:00:22,960
also how we gather telemetry data to

00:00:20,160 --> 00:00:26,000
make data-driven decisions in reddit

00:00:22,960 --> 00:00:27,680
without further ado let's start i work

00:00:26,000 --> 00:00:28,800
for openshift observability and

00:00:27,680 --> 00:00:30,960
monitoring team

00:00:28,800 --> 00:00:32,079
as a software engineer also i am a

00:00:30,960 --> 00:00:35,280
thanos maintainer

00:00:32,079 --> 00:00:37,200
and a prometheus contributor as a team

00:00:35,280 --> 00:00:39,520
we are building a platform to collect

00:00:37,200 --> 00:00:42,320
and store observability signals

00:00:39,520 --> 00:00:44,399
also we have sre responsibilities and we

00:00:42,320 --> 00:00:45,520
are on call for the internal platform

00:00:44,399 --> 00:00:47,680
that we are building

00:00:45,520 --> 00:00:48,879
the system that we build is called

00:00:47,680 --> 00:00:51,840
telemeter

00:00:48,879 --> 00:00:53,920
so what is the limiter in simple terms

00:00:51,840 --> 00:00:55,760
telemeter is an open source remote

00:00:53,920 --> 00:00:57,680
health monitoring system

00:00:55,760 --> 00:00:59,359
remote health monitoring works by

00:00:57,680 --> 00:01:01,359
sending a carefully chosen

00:00:59,359 --> 00:01:02,480
subset of data back to the telemeter

00:01:01,359 --> 00:01:04,559
service

00:01:02,480 --> 00:01:05,680
the data is anonymized to maintain

00:01:04,559 --> 00:01:08,159
privacy

00:01:05,680 --> 00:01:10,000
we do not collect any identifying

00:01:08,159 --> 00:01:12,080
information such as user names and

00:01:10,000 --> 00:01:14,000
password or resource names

00:01:12,080 --> 00:01:16,159
and the full list of data collection is

00:01:14,000 --> 00:01:18,400
available publicly

00:01:16,159 --> 00:01:20,560
so the the primary information collected

00:01:18,400 --> 00:01:23,360
by telemetry includes the number of

00:01:20,560 --> 00:01:24,240
updates available per cluster the number

00:01:23,360 --> 00:01:26,720
of errors

00:01:24,240 --> 00:01:28,880
that occur during an update the progress

00:01:26,720 --> 00:01:32,159
information of running updates

00:01:28,880 --> 00:01:33,680
health condition and status of openshift

00:01:32,159 --> 00:01:34,720
component that is installed on the

00:01:33,680 --> 00:01:36,560
clusters

00:01:34,720 --> 00:01:38,960
the name of the platform that openshift

00:01:36,560 --> 00:01:40,960
deployed on such an aws

00:01:38,960 --> 00:01:42,960
the data we collect enables us to

00:01:40,960 --> 00:01:45,439
provide a lot of benefits to the end

00:01:42,960 --> 00:01:47,759
user otherwise would be impossible

00:01:45,439 --> 00:01:49,759
with the help of the telemetry data we

00:01:47,759 --> 00:01:51,040
can observe events that may seem normal

00:01:49,759 --> 00:01:53,200
to a single and

00:01:51,040 --> 00:01:55,360
a single end user but with the

00:01:53,200 --> 00:01:57,520
perspective of seeing those events

00:01:55,360 --> 00:01:58,799
across the fleet of users we can provide

00:01:57,520 --> 00:02:01,200
more insights

00:01:58,799 --> 00:02:02,799
with the connected information we can

00:02:01,200 --> 00:02:05,759
improve the quality of the

00:02:02,799 --> 00:02:07,600
releases and react more quickly to the

00:02:05,759 --> 00:02:10,879
issues found in the clusters

00:02:07,600 --> 00:02:13,040
as a result we provide better support

00:02:10,879 --> 00:02:15,280
and the information allows openshift to

00:02:13,040 --> 00:02:17,360
more rapidly release new features and

00:02:15,280 --> 00:02:20,000
focus on engineering resources

00:02:17,360 --> 00:02:22,239
where they can be most impactful to the

00:02:20,000 --> 00:02:25,840
end users

00:02:22,239 --> 00:02:27,760
so how we build telemetry in red hat

00:02:25,840 --> 00:02:30,640
we have an absolute upstream first

00:02:27,760 --> 00:02:32,959
mentality so telemetry is based on

00:02:30,640 --> 00:02:34,160
the open source tools on the promet use

00:02:32,959 --> 00:02:36,480
ecosystem

00:02:34,160 --> 00:02:37,280
we deploy and maintain highly available

00:02:36,480 --> 00:02:39,280
prometheus

00:02:37,280 --> 00:02:40,319
alert manager and several thanos

00:02:39,280 --> 00:02:43,519
components

00:02:40,319 --> 00:02:46,080
in each and every cluster we run

00:02:43,519 --> 00:02:48,400
how we how do we deploy we use

00:02:46,080 --> 00:02:49,440
prometheus operator to deploy prometheus

00:02:48,400 --> 00:02:51,840
alert manager and

00:02:49,440 --> 00:02:53,280
several thanos components into openshift

00:02:51,840 --> 00:02:56,400
clusters

00:02:53,280 --> 00:02:58,000
so let me briefly explain what it is for

00:02:56,400 --> 00:02:59,760
the ones who aren't familiar with the

00:02:58,000 --> 00:03:01,840
prometeus operator

00:02:59,760 --> 00:03:03,920
the prometeus operator provides

00:03:01,840 --> 00:03:05,760
kubernetes native deployment and

00:03:03,920 --> 00:03:07,200
management of prometus and related

00:03:05,760 --> 00:03:09,200
monitoring components

00:03:07,200 --> 00:03:11,519
the purpose of this project is to

00:03:09,200 --> 00:03:13,840
simplify and automate the configuration

00:03:11,519 --> 00:03:15,920
of a prometeuse-based monitoring stack

00:03:13,840 --> 00:03:18,400
for the kubernetes clusters

00:03:15,920 --> 00:03:20,480
by using prometeos operator out of the

00:03:18,400 --> 00:03:22,480
box we monitor critical cluster

00:03:20,480 --> 00:03:24,080
components and alert on the metrics we

00:03:22,480 --> 00:03:26,879
collect

00:03:24,080 --> 00:03:28,400
moreover we also let the users define

00:03:26,879 --> 00:03:29,599
configure and deploy their own

00:03:28,400 --> 00:03:32,799
monitoring stack

00:03:29,599 --> 00:03:35,200
to monitor their own workloads

00:03:32,799 --> 00:03:36,720
in this stack we have prometheus aha

00:03:35,200 --> 00:03:38,640
payer alert manager

00:03:36,720 --> 00:03:40,400
thanos ruler and tunnel square to

00:03:38,640 --> 00:03:43,920
provide global overview

00:03:40,400 --> 00:03:46,879
for the prometus ha payer

00:03:43,920 --> 00:03:48,560
so how does it look like as i already

00:03:46,879 --> 00:03:50,080
told you we deployed prometheus using

00:03:48,560 --> 00:03:52,560
prometheus operator

00:03:50,080 --> 00:03:54,319
and several other tunnels components for

00:03:52,560 --> 00:03:56,879
each cluster we collect and send

00:03:54,319 --> 00:03:59,280
critical metrics critical alerts

00:03:56,879 --> 00:04:01,280
and the information about upgrades to

00:03:59,280 --> 00:04:03,360
our telemeter service

00:04:01,280 --> 00:04:05,439
in the first version of the system the

00:04:03,360 --> 00:04:07,120
incluster pro meteors was collecting the

00:04:05,439 --> 00:04:09,760
data from the workflows in

00:04:07,120 --> 00:04:11,680
prometus format and then a dedicated

00:04:09,760 --> 00:04:13,439
component was scraping the federated

00:04:11,680 --> 00:04:14,560
endpoint of prometheus every four and a

00:04:13,439 --> 00:04:16,239
half minutes

00:04:14,560 --> 00:04:18,479
then the component was cleaning the

00:04:16,239 --> 00:04:20,320
metrics and anonymizing the data and

00:04:18,479 --> 00:04:22,479
then sending the metrics in prometus

00:04:20,320 --> 00:04:25,040
federate data format to the server-side

00:04:22,479 --> 00:04:27,199
telemeter server

00:04:25,040 --> 00:04:28,320
on the server side we had the telemeter

00:04:27,199 --> 00:04:30,320
server component

00:04:28,320 --> 00:04:31,759
it was receiving the data on the

00:04:30,320 --> 00:04:33,759
federate data format and

00:04:31,759 --> 00:04:35,440
store the data into its hashing of

00:04:33,759 --> 00:04:37,600
numerous replicas

00:04:35,440 --> 00:04:39,360
and then the telemetry servers had been

00:04:37,600 --> 00:04:41,040
scraped by the two replicas of

00:04:39,360 --> 00:04:43,040
prometheuses

00:04:41,040 --> 00:04:44,320
ingesting all of the all of the data

00:04:43,040 --> 00:04:46,880
twice

00:04:44,320 --> 00:04:48,240
the hashing was super primitive all the

00:04:46,880 --> 00:04:51,360
data was in memory

00:04:48,240 --> 00:04:53,360
and nothing was persisted and we use

00:04:51,360 --> 00:04:55,360
prometheus to directly provide access to

00:04:53,360 --> 00:04:58,080
the data we collect

00:04:55,360 --> 00:05:00,000
ultimately they were bottlenecks and we

00:04:58,080 --> 00:05:01,759
failed due to the high volumes of data

00:05:00,000 --> 00:05:03,840
to ingest and queries

00:05:01,759 --> 00:05:06,240
moreover the queries of that would bring

00:05:03,840 --> 00:05:08,160
down one of the prometheuses eventually

00:05:06,240 --> 00:05:09,440
and that would prevent us from ingesting

00:05:08,160 --> 00:05:12,400
more data

00:05:09,440 --> 00:05:14,880
so when we hit these scalability issues

00:05:12,400 --> 00:05:17,199
we decided to invest a redesign of the

00:05:14,880 --> 00:05:19,039
system

00:05:17,199 --> 00:05:21,360
and we chose tunnels to build a more

00:05:19,039 --> 00:05:23,680
scalable system upon

00:05:21,360 --> 00:05:25,759
thanos helped us to compose a highly

00:05:23,680 --> 00:05:28,000
available metric system with unlimited

00:05:25,759 --> 00:05:30,160
storage capacity

00:05:28,000 --> 00:05:32,479
which can be added seamlessly on top of

00:05:30,160 --> 00:05:34,240
the existing prometheus deployments

00:05:32,479 --> 00:05:36,080
tunnels help us to build a cost

00:05:34,240 --> 00:05:36,560
efficient store for historical metric

00:05:36,080 --> 00:05:39,039
data

00:05:36,560 --> 00:05:41,120
while retaining fast query latencies we

00:05:39,039 --> 00:05:43,120
introduced a new custom central metrics

00:05:41,120 --> 00:05:44,000
collection pipeline for telemeter using

00:05:43,120 --> 00:05:46,560
tunnels

00:05:44,000 --> 00:05:48,479
to make it even more scalable red

00:05:46,560 --> 00:05:49,039
invested in adding a new component to

00:05:48,479 --> 00:05:52,160
thanos

00:05:49,039 --> 00:05:53,680
called receipt at that point this was a

00:05:52,160 --> 00:05:55,520
novel idea for thanos

00:05:53,680 --> 00:05:57,280
because it had changed thanos model from

00:05:55,520 --> 00:05:59,840
a pool based solution to a

00:05:57,280 --> 00:06:00,960
push-based one this effort was started

00:05:59,840 --> 00:06:03,440
in june

00:06:00,960 --> 00:06:05,680
2019 and we ended up building a sas

00:06:03,440 --> 00:06:07,440
offering for reddit's internal usage for

00:06:05,680 --> 00:06:10,400
internal customers

00:06:07,440 --> 00:06:10,800
so how does the niv stack look like all

00:06:10,400 --> 00:06:13,120
the

00:06:10,800 --> 00:06:15,280
all the ink cluster bits remain the same

00:06:13,120 --> 00:06:17,520
and we converted the telemeter server to

00:06:15,280 --> 00:06:18,800
a mere authentication proxy and data

00:06:17,520 --> 00:06:21,199
transformer

00:06:18,800 --> 00:06:22,800
for the legacy endpoint all the uploaded

00:06:21,199 --> 00:06:24,800
data now being converted to the

00:06:22,800 --> 00:06:27,520
prometeus remote data format

00:06:24,800 --> 00:06:29,199
and send send it over to the thanos

00:06:27,520 --> 00:06:31,600
receiver stream

00:06:29,199 --> 00:06:33,360
we added a new endpoint though with the

00:06:31,600 --> 00:06:34,160
new endpoint we added the ability to

00:06:33,360 --> 00:06:35,759
support

00:06:34,160 --> 00:06:38,000
direct descending metrics from the

00:06:35,759 --> 00:06:41,039
prometus through telemeter server

00:06:38,000 --> 00:06:43,199
using a remote remote drive api

00:06:41,039 --> 00:06:45,520
in the upcoming openshift versions we

00:06:43,199 --> 00:06:47,520
are planning to move completely

00:06:45,520 --> 00:06:49,360
to writing directly from prometheuses to

00:06:47,520 --> 00:06:51,360
telemetry servers

00:06:49,360 --> 00:06:53,759
also we created a new controller for

00:06:51,360 --> 00:06:55,520
tunnels received to coordinate updates

00:06:53,759 --> 00:06:57,919
adding and removing nodes from the

00:06:55,520 --> 00:07:00,800
tunnel's receive hashtag

00:06:57,919 --> 00:07:03,039
and we utilize thanos store and querier

00:07:00,800 --> 00:07:05,280
to provide access to the data to our

00:07:03,039 --> 00:07:07,680
internal customers

00:07:05,280 --> 00:07:10,080
but how about the other signals i

00:07:07,680 --> 00:07:12,960
mentioned that we collect them right

00:07:10,080 --> 00:07:14,319
so this diagram shows what we have now

00:07:12,960 --> 00:07:16,400
metrics and logs

00:07:14,319 --> 00:07:18,000
and what are we planning to build soon

00:07:16,400 --> 00:07:20,080
tracing and profiling

00:07:18,000 --> 00:07:21,919
to extend the functionality of telemetry

00:07:20,080 --> 00:07:23,840
we decided to build yet another open

00:07:21,919 --> 00:07:26,800
source observability system

00:07:23,840 --> 00:07:28,800
our main goals were to provide multiple

00:07:26,800 --> 00:07:31,039
observability signal support

00:07:28,800 --> 00:07:32,080
correlation between signals seamless

00:07:31,039 --> 00:07:33,840
multi-tenancy

00:07:32,080 --> 00:07:35,599
authentication and authorization and

00:07:33,840 --> 00:07:37,759
improved security

00:07:35,599 --> 00:07:40,080
thanks to prometheus ecosystem and the

00:07:37,759 --> 00:07:42,880
other systems that were built similarly

00:07:40,080 --> 00:07:44,080
this was a relatively easy task so we

00:07:42,880 --> 00:07:47,280
based our design

00:07:44,080 --> 00:07:49,759
on two major points schema-less labels

00:07:47,280 --> 00:07:51,840
and object storage support we recently

00:07:49,759 --> 00:07:54,160
started to provide logging solution

00:07:51,840 --> 00:07:56,639
using grafana to our internal customers

00:07:54,160 --> 00:07:59,919
at reddit

00:07:56,639 --> 00:08:01,599
because this system observatorium

00:07:59,919 --> 00:08:04,000
you can think observatorium as a

00:08:01,599 --> 00:08:05,840
distribution we have packed a reference

00:08:04,000 --> 00:08:08,400
architecture of thanos loki

00:08:05,840 --> 00:08:10,639
and soon others that allows easier

00:08:08,400 --> 00:08:12,879
installation configuration and operating

00:08:10,639 --> 00:08:15,360
of several observability systems that we

00:08:12,879 --> 00:08:17,919
found the most useful and practical

00:08:15,360 --> 00:08:18,720
and of course all of these efforts are

00:08:17,919 --> 00:08:20,240
open source

00:08:18,720 --> 00:08:22,240
you can check out everything we have

00:08:20,240 --> 00:08:25,680
done so far by visiting our

00:08:22,240 --> 00:08:28,000
website please do so so

00:08:25,680 --> 00:08:30,000
what we learned while doing while

00:08:28,000 --> 00:08:32,479
building this platform

00:08:30,000 --> 00:08:34,320
in such a big organization like ours

00:08:32,479 --> 00:08:36,399
being able to offer an internal sauce

00:08:34,320 --> 00:08:38,719
for the less experienced teams

00:08:36,399 --> 00:08:41,200
extremely useful moreover the

00:08:38,719 --> 00:08:42,080
flexibility of deployment and design is

00:08:41,200 --> 00:08:44,480
must have

00:08:42,080 --> 00:08:46,320
requirements and priorities change

00:08:44,480 --> 00:08:48,399
overnight and we had the support from

00:08:46,320 --> 00:08:49,680
one cpu cluster to many many huge

00:08:48,399 --> 00:08:51,920
clusters

00:08:49,680 --> 00:08:53,519
using schema-less labels to correlate

00:08:51,920 --> 00:08:55,760
signals was paid off

00:08:53,519 --> 00:08:57,760
api driven model helped us to extend the

00:08:55,760 --> 00:09:00,399
system easily

00:08:57,760 --> 00:09:02,240
and cost matters and on an even medium

00:09:00,399 --> 00:09:03,600
scale it takes millions of dollars to

00:09:02,240 --> 00:09:06,240
gather all the log lines

00:09:03,600 --> 00:09:08,240
traces or events so focusing on

00:09:06,240 --> 00:09:09,680
actionable metrics helped us to scale

00:09:08,240 --> 00:09:12,240
seamlessly

00:09:09,680 --> 00:09:13,360
and also relying on the object storages

00:09:12,240 --> 00:09:15,200
for long-term retain

00:09:13,360 --> 00:09:17,600
retention helped us to control all the

00:09:15,200 --> 00:09:17,600
costs

00:09:17,920 --> 00:09:21,279
last but not the least we are currently

00:09:20,560 --> 00:09:22,880
hiring

00:09:21,279 --> 00:09:24,640
if you want to work with us and

00:09:22,880 --> 00:09:27,040
contribute to all these cool

00:09:24,640 --> 00:09:27,760
open source upstream projects please let

00:09:27,040 --> 00:09:29,600
us know

00:09:27,760 --> 00:09:31,120
you can apply by using the link in the

00:09:29,600 --> 00:09:33,600
slides

00:09:31,120 --> 00:09:35,200
if you have any questions please reach

00:09:33,600 --> 00:09:36,959
out to us you can find us on the

00:09:35,200 --> 00:09:40,240
upstream projects cncf

00:09:36,959 --> 00:09:44,240
selections thanks everyone for listening

00:09:40,240 --> 00:09:44,240

YouTube URL: https://www.youtube.com/watch?v=r0fRFH_921E


