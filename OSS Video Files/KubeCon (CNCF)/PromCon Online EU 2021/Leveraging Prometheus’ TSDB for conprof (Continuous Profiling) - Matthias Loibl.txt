Title: Leveraging Prometheus’ TSDB for conprof (Continuous Profiling) - Matthias Loibl
Publication date: 2021-05-03
Playlist: PromCon Online EU 2021
Description: 
	Don’t miss out! Join us at our upcoming event: KubeCon + CloudNativeCon North America 2021 in Los Angeles, CA from October 12-15. Learn more at https://kubecon.io The conference features presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.

Leveraging Prometheus’ TSDB for conprof (Continuous Profiling) - Matthias Loibl

"conprof - github.com/conprof/conprof - is an open-source project that collects profile samples of CPU and memory over time, which can be used for performance analysis amongst other things. Conprof has its roots in Prometheus, this talk elaborates on the similarities that have prevailed as well as the optimizations that had to be made to make it more suitable for continuous profiling. We will first demonstrate how conprof works by analyzing an application with a memory leak.

Prometheus’ time-series database (TSDB) stores tuples of timestamp and float64 value, for each time series and thus, is optimized for storing these as efficiently as possible. conprof, on the other hand, needs to store tuples of timestamp and the raw, recorded profile that can be arbitrarily large. We needed to adapt the TSDB for a completely new use case.

Every conprof workflow starts by looking at the timestamps of each series and only afterwards individual profiles are requested. We explored optimizing for timestamp-reads to ignore the profiling data to save CPU and memory. Separating timestamps and profiles would make it easier and maybe we could improve the compression for profiles?
This talk will take us on a journey to see how we've answered these questions so far!"
Captions: 
	00:00:00,000 --> 00:00:03,840
hey everybody i'm matthias loigel and

00:00:02,240 --> 00:00:05,279
today i want to talk to you about

00:00:03,840 --> 00:00:08,639
leveraging prometheus

00:00:05,279 --> 00:00:10,559
tsdb time series database for controv

00:00:08,639 --> 00:00:12,240
for continuous profiling

00:00:10,559 --> 00:00:13,679
i hope you're staying safe i hope you're

00:00:12,240 --> 00:00:17,520
doing well and

00:00:13,679 --> 00:00:20,000
yeah let's get into it

00:00:17,520 --> 00:00:21,680
quickly about me i'm matthias logger as

00:00:20,000 --> 00:00:23,920
i said i'm a senior software engineer at

00:00:21,680 --> 00:00:24,320
polar signals i previously worked at red

00:00:23,920 --> 00:00:27,840
hat

00:00:24,320 --> 00:00:29,679
and grubermetic i am open source

00:00:27,840 --> 00:00:30,560
maintainer with others working on

00:00:29,679 --> 00:00:33,280
comprov

00:00:30,560 --> 00:00:34,640
and thanos and prometheus operator and

00:00:33,280 --> 00:00:37,520
your prometheus

00:00:34,640 --> 00:00:38,800
among very other things and i'm also

00:00:37,520 --> 00:00:41,760
organizing the

00:00:38,800 --> 00:00:44,640
berlin prometheus meetup you can find me

00:00:41,760 --> 00:00:48,800
as i wrote down on the left hand side

00:00:44,640 --> 00:00:48,800
on social media always add metamata

00:00:49,440 --> 00:00:56,079
cool so let's talk about profiling

00:00:52,640 --> 00:00:56,800
profiling if you may be asking what it

00:00:56,079 --> 00:00:59,280
is

00:00:56,800 --> 00:01:00,320
is a form of dynamic programming

00:00:59,280 --> 00:01:03,440
analysis that

00:01:00,320 --> 00:01:06,240
measures for example the space

00:01:03,440 --> 00:01:08,400
so the memory a program uses the time

00:01:06,240 --> 00:01:10,400
complexity so the cpu

00:01:08,400 --> 00:01:12,960
and the usage of instruction and the

00:01:10,400 --> 00:01:15,119
frequency and duration of function calls

00:01:12,960 --> 00:01:18,320
so it's fairly low level but it's not

00:01:15,119 --> 00:01:18,320
all too complicated

00:01:19,520 --> 00:01:23,360
being at prom con you might be asking

00:01:21,520 --> 00:01:24,640
what's the difference of profiling and

00:01:23,360 --> 00:01:27,119
metrics

00:01:24,640 --> 00:01:28,159
so i tend to think that metrics surface

00:01:27,119 --> 00:01:31,360
problems

00:01:28,159 --> 00:01:33,840
so if we're thinking about the cpu

00:01:31,360 --> 00:01:34,960
and we look at a graphana dashboard we

00:01:33,840 --> 00:01:37,920
can see that

00:01:34,960 --> 00:01:39,680
for some reason this cpu usage is higher

00:01:37,920 --> 00:01:41,439
than expected

00:01:39,680 --> 00:01:43,600
same for the memory we might be looking

00:01:41,439 --> 00:01:45,920
at the memory of a profile

00:01:43,600 --> 00:01:47,360
of a program and see that for some

00:01:45,920 --> 00:01:50,880
reason

00:01:47,360 --> 00:01:52,960
the usage is a lot higher than expected

00:01:50,880 --> 00:01:54,640
now what we want to do instead of

00:01:52,960 --> 00:01:56,479
looking at dashboards all the time

00:01:54,640 --> 00:01:59,600
because sometimes we need to sleep

00:01:56,479 --> 00:02:01,439
we want to alert um on on things right

00:01:59,600 --> 00:02:03,280
so for the cpu we have the cpu

00:02:01,439 --> 00:02:04,799
throttling high alert

00:02:03,280 --> 00:02:06,479
and that actually tells you when the

00:02:04,799 --> 00:02:09,840
kernel is kind of like

00:02:06,479 --> 00:02:12,400
throttling a process too much and

00:02:09,840 --> 00:02:13,920
it could be faster but it kind of

00:02:12,400 --> 00:02:16,959
reached the limit of

00:02:13,920 --> 00:02:20,160
the cpu time the process gets

00:02:16,959 --> 00:02:22,720
um and then for kind of like for a more

00:02:20,160 --> 00:02:25,200
symptom-based approach um

00:02:22,720 --> 00:02:26,959
you might have for your service a p90

00:02:25,200 --> 00:02:30,160
latency

00:02:26,959 --> 00:02:32,000
requirement that like 90 of requests

00:02:30,160 --> 00:02:33,200
are answered within like one second or

00:02:32,000 --> 00:02:36,319
something like that

00:02:33,200 --> 00:02:39,519
and if the requests overall are too slow

00:02:36,319 --> 00:02:40,640
um something maybe with the cpu might be

00:02:39,519 --> 00:02:42,560
happening so we

00:02:40,640 --> 00:02:44,720
want to like take a look at that but we

00:02:42,560 --> 00:02:47,519
can't really see what's happening

00:02:44,720 --> 00:02:49,440
and then for the memory um most of the

00:02:47,519 --> 00:02:51,840
time what we kind of like see

00:02:49,440 --> 00:02:53,040
is that there might be a cube part crash

00:02:51,840 --> 00:02:56,000
looping alert

00:02:53,040 --> 00:02:57,360
and that sometimes indicates that there

00:02:56,000 --> 00:02:59,599
are out of memory

00:02:57,360 --> 00:03:00,959
kills happening so that kind of

00:02:59,599 --> 00:03:04,159
terminates the process

00:03:00,959 --> 00:03:07,280
because it has reached full memory

00:03:04,159 --> 00:03:09,280
limits and yeah it tries

00:03:07,280 --> 00:03:11,200
like the kernel tries to maintain the

00:03:09,280 --> 00:03:14,640
overall system stability

00:03:11,200 --> 00:03:15,280
and kills the process kubernetes is just

00:03:14,640 --> 00:03:18,800
an example

00:03:15,280 --> 00:03:19,920
same for systemd or thus so looking at

00:03:18,800 --> 00:03:23,519
this metric

00:03:19,920 --> 00:03:27,599
we can see that right at the time

00:03:23,519 --> 00:03:30,959
roughly 21 minutes or something

00:03:27,599 --> 00:03:32,560
the the process got killed and went from

00:03:30,959 --> 00:03:35,280
one gigabyte of memory

00:03:32,560 --> 00:03:37,040
all the way down to zero and we don't

00:03:35,280 --> 00:03:38,080
really know what happened we just see

00:03:37,040 --> 00:03:40,480
that something happened

00:03:38,080 --> 00:03:41,680
from the metrics but what we really want

00:03:40,480 --> 00:03:44,159
to know is like

00:03:41,680 --> 00:03:46,159
what did the programs state look like at

00:03:44,159 --> 00:03:48,480
20 minutes for example right

00:03:46,159 --> 00:03:49,200
so this is what we're trying to answer

00:03:48,480 --> 00:03:52,000
and

00:03:49,200 --> 00:03:52,799
this in the end really is what profiles

00:03:52,000 --> 00:03:56,000
give you they

00:03:52,799 --> 00:03:56,640
show why something is happening like in

00:03:56,000 --> 00:03:59,599
the

00:03:56,640 --> 00:04:00,000
lower levels of the pro of the program

00:03:59,599 --> 00:04:02,879
so

00:04:00,000 --> 00:04:04,640
for cpu it shows where the process has

00:04:02,879 --> 00:04:07,680
spent a lot of time

00:04:04,640 --> 00:04:11,439
and kind of like shows the function um

00:04:07,680 --> 00:04:15,200
oftentimes quite clearly that has spent

00:04:11,439 --> 00:04:15,760
the most cpu time so most of the time is

00:04:15,200 --> 00:04:18,880
like

00:04:15,760 --> 00:04:20,799
a good indicator that spending some some

00:04:18,880 --> 00:04:22,960
some time on that function and trying to

00:04:20,799 --> 00:04:25,680
improve it is worthwhile

00:04:22,960 --> 00:04:27,680
and then for the memory we have two

00:04:25,680 --> 00:04:30,560
different profiles

00:04:27,680 --> 00:04:31,360
so one for example is the logs profile

00:04:30,560 --> 00:04:34,240
and that shows

00:04:31,360 --> 00:04:36,160
how much each function overall like over

00:04:34,240 --> 00:04:38,639
the entire time of the

00:04:36,160 --> 00:04:40,080
of the process running has allocated so

00:04:38,639 --> 00:04:43,199
which are like constantly

00:04:40,080 --> 00:04:44,479
allocating memory was others that don't

00:04:43,199 --> 00:04:46,960
really do that much of

00:04:44,479 --> 00:04:49,440
of memory locations and then there's the

00:04:46,960 --> 00:04:51,919
heap profiles that show

00:04:49,440 --> 00:04:53,440
what the current what currently every

00:04:51,919 --> 00:04:56,000
function

00:04:53,440 --> 00:04:56,960
has allocated right so it kind of shows

00:04:56,000 --> 00:05:00,960
you

00:04:56,960 --> 00:05:04,080
um if you use like one gigabit

00:05:00,960 --> 00:05:07,840
uh one gigabyte of memory which um

00:05:04,080 --> 00:05:08,800
functions um you know have what part of

00:05:07,840 --> 00:05:12,080
the memory of that

00:05:08,800 --> 00:05:16,000
gigabyte allocated so that's

00:05:12,080 --> 00:05:18,320
often quite useful for troubleshooting

00:05:16,000 --> 00:05:19,280
so one of the projects for profiling is

00:05:18,320 --> 00:05:21,360
pprof

00:05:19,280 --> 00:05:23,360
came from google and it reads a

00:05:21,360 --> 00:05:26,400
collection of profiling samples

00:05:23,360 --> 00:05:28,720
so only only every

00:05:26,400 --> 00:05:30,080
few seconds it takes snapshots of the

00:05:28,720 --> 00:05:33,120
program state

00:05:30,080 --> 00:05:35,520
and stores them or like we do that

00:05:33,120 --> 00:05:37,880
manually right

00:05:35,520 --> 00:05:39,680
and the format is described in a

00:05:37,880 --> 00:05:42,639
profile.proto

00:05:39,680 --> 00:05:43,360
file and i will show you that in just a

00:05:42,639 --> 00:05:45,840
second

00:05:43,360 --> 00:05:46,880
it uses the dot visualization tool from

00:05:45,840 --> 00:05:50,160
graph wiz

00:05:46,880 --> 00:05:53,600
to make beautiful graphs

00:05:50,160 --> 00:05:55,840
and then we can read profiles from disk

00:05:53,600 --> 00:05:58,160
from a local disk or we can pull them

00:05:55,840 --> 00:05:59,600
via http and that's quite helpful if

00:05:58,160 --> 00:06:01,919
the process is running somewhere else on

00:05:59,600 --> 00:06:04,639
the server in the kubernetes cluster

00:06:01,919 --> 00:06:08,080
whatever you have um and here we can

00:06:04,639 --> 00:06:11,280
kind of do the remote troubleshooting

00:06:08,080 --> 00:06:13,120
so for the proto of prof just want to

00:06:11,280 --> 00:06:14,720
quickly like demystify a couple of

00:06:13,120 --> 00:06:18,080
things

00:06:14,720 --> 00:06:21,199
the pro 5 at the very top

00:06:18,080 --> 00:06:22,880
has multiple samples and every sample

00:06:21,199 --> 00:06:25,840
has a location

00:06:22,880 --> 00:06:26,560
and these locations have lines and each

00:06:25,840 --> 00:06:29,440
line

00:06:26,560 --> 00:06:29,440
has a function

00:06:29,840 --> 00:06:33,120
and to make it a bit more clear i

00:06:32,560 --> 00:06:35,600
actually

00:06:33,120 --> 00:06:37,199
ran a debugger and looked at a profile

00:06:35,600 --> 00:06:40,720
in memory and you can see

00:06:37,199 --> 00:06:42,639
this the 17th sample of the profile it

00:06:40,720 --> 00:06:45,280
has a location it has a line it has a

00:06:42,639 --> 00:06:47,199
function and if we look at the function

00:06:45,280 --> 00:06:48,560
it has a couple of strings and one of

00:06:47,199 --> 00:06:52,160
the strings is the name

00:06:48,560 --> 00:06:55,280
and that says that the current sample

00:06:52,160 --> 00:06:56,800
and function line etc was the bytes.make

00:06:55,280 --> 00:07:00,960
slice function call

00:06:56,800 --> 00:07:03,280
and further down we see that on line 229

00:07:00,960 --> 00:07:04,240
this function is being called in the

00:07:03,280 --> 00:07:07,360
file

00:07:04,240 --> 00:07:09,199
buffer.go so that's really all there is

00:07:07,360 --> 00:07:12,000
to it

00:07:09,199 --> 00:07:14,880
a bit more metadata but that's like the

00:07:12,000 --> 00:07:18,319
underlying format

00:07:14,880 --> 00:07:20,319
um prof ships as part of each go release

00:07:18,319 --> 00:07:21,360
so if you already have go it comes with

00:07:20,319 --> 00:07:23,680
a go tool p

00:07:21,360 --> 00:07:24,800
prof sub command and you can just start

00:07:23,680 --> 00:07:26,880
using it

00:07:24,800 --> 00:07:28,000
and actually many applications already

00:07:26,880 --> 00:07:31,360
exposed preprof

00:07:28,000 --> 00:07:34,479
one of which is prometheus um

00:07:31,360 --> 00:07:37,440
for other languages um we

00:07:34,479 --> 00:07:37,919
basically get cpu profiling which is

00:07:37,440 --> 00:07:40,479
great

00:07:37,919 --> 00:07:41,360
and some support heap profiles and not

00:07:40,479 --> 00:07:44,800
so much for

00:07:41,360 --> 00:07:47,840
everything else and for go we even get

00:07:44,800 --> 00:07:49,759
go routine profiles and fg prof

00:07:47,840 --> 00:07:52,160
is a new one outside the standard

00:07:49,759 --> 00:07:54,000
library um started by someone which is

00:07:52,160 --> 00:07:56,800
really cool as well i will not go into

00:07:54,000 --> 00:07:56,800
this one right now

00:07:56,960 --> 00:08:00,400
and then if you don't have p prof if you

00:07:58,879 --> 00:08:02,479
cannot change

00:08:00,400 --> 00:08:04,960
the actual program the process that you

00:08:02,479 --> 00:08:08,400
want to instrument

00:08:04,960 --> 00:08:11,360
there's perth and that works um without

00:08:08,400 --> 00:08:13,120
like any code changes and then there's a

00:08:11,360 --> 00:08:15,759
perf data converter

00:08:13,120 --> 00:08:17,599
and we actually recently open sourced a

00:08:15,759 --> 00:08:20,000
project called professor

00:08:17,599 --> 00:08:20,800
and that converts the profiles from

00:08:20,000 --> 00:08:24,800
perth

00:08:20,800 --> 00:08:27,440
and into prof and then ships it to a

00:08:24,800 --> 00:08:29,599
profiling back end and we still

00:08:27,440 --> 00:08:31,520
recommend if you can we still recommend

00:08:29,599 --> 00:08:32,479
using the native instrumentation via

00:08:31,520 --> 00:08:34,800
prof

00:08:32,479 --> 00:08:36,479
whenever possible this just kind of like

00:08:34,800 --> 00:08:39,519
as a last resort and

00:08:36,479 --> 00:08:41,120
um it was already quite quite useful and

00:08:39,519 --> 00:08:44,800
quite cool to see that

00:08:41,120 --> 00:08:47,440
it still works right as i said

00:08:44,800 --> 00:08:48,560
prometheus is actually instrumented with

00:08:47,440 --> 00:08:51,360
p prof so

00:08:48,560 --> 00:08:52,399
what does it look like p prof um exposes

00:08:51,360 --> 00:08:55,760
the p

00:08:52,399 --> 00:08:57,839
prometheus exposes a p prof endpoint um

00:08:55,760 --> 00:08:59,839
and the handler is located at this given

00:08:57,839 --> 00:09:01,760
address and if you take a look at this

00:08:59,839 --> 00:09:03,600
it just gives you like a super simple

00:09:01,760 --> 00:09:06,000
page and you can click on these

00:09:03,600 --> 00:09:08,560
um profiles but it's just a bunch of

00:09:06,000 --> 00:09:10,800
text and you can't really do anything

00:09:08,560 --> 00:09:12,720
really useful with it right so what we

00:09:10,800 --> 00:09:15,360
actually want to do is to use the go

00:09:12,720 --> 00:09:18,000
to people sub command give it that

00:09:15,360 --> 00:09:20,959
address and we can pull down the profile

00:09:18,000 --> 00:09:21,600
and then type for example web or svg and

00:09:20,959 --> 00:09:24,640
we get

00:09:21,600 --> 00:09:27,120
a graph um as i shown in the

00:09:24,640 --> 00:09:28,320
in the screenshot and as you can see in

00:09:27,120 --> 00:09:31,680
the profile um

00:09:28,320 --> 00:09:33,440
the prom ql extrapolate rate function

00:09:31,680 --> 00:09:36,080
call is quite

00:09:33,440 --> 00:09:38,240
quite big and that's due to me running

00:09:36,080 --> 00:09:41,360
some queries

00:09:38,240 --> 00:09:44,480
in my prometheus while by pulling this

00:09:41,360 --> 00:09:47,440
cpu profile and then

00:09:44,480 --> 00:09:50,080
just shortly after i ran the go to a

00:09:47,440 --> 00:09:53,200
prof profile with a heap endpoint

00:09:50,080 --> 00:09:55,279
and unsurprisingly we can see that right

00:09:53,200 --> 00:09:58,080
in the center there's the prom ql

00:09:55,279 --> 00:09:59,519
evaluator taking a lot of memory and

00:09:58,080 --> 00:10:02,000
that's also due to me

00:09:59,519 --> 00:10:03,519
running a couple of queries i guess um

00:10:02,000 --> 00:10:05,600
so just to give you a high level

00:10:03,519 --> 00:10:07,760
overview

00:10:05,600 --> 00:10:09,600
so after all we're here from co for

00:10:07,760 --> 00:10:10,880
continuous profiling and you might be

00:10:09,600 --> 00:10:12,959
asking what it is

00:10:10,880 --> 00:10:14,399
so p prof creates these samples right

00:10:12,959 --> 00:10:16,959
it's just these

00:10:14,399 --> 00:10:17,600
small snapshots of what the state looks

00:10:16,959 --> 00:10:19,680
like

00:10:17,600 --> 00:10:21,519
and what we actually want to do is we

00:10:19,680 --> 00:10:23,839
want to sample every so often

00:10:21,519 --> 00:10:25,680
and that's what the continuous profiling

00:10:23,839 --> 00:10:28,480
part really is

00:10:25,680 --> 00:10:30,800
um and we want and we can do that

00:10:28,480 --> 00:10:33,680
because it comes with a little overhead

00:10:30,800 --> 00:10:35,040
um with because it's sampling and we

00:10:33,680 --> 00:10:37,279
we've seen from like

00:10:35,040 --> 00:10:39,200
point two point three percent to three

00:10:37,279 --> 00:10:42,720
percent depends on the process

00:10:39,200 --> 00:10:43,440
that you're profiling um what we really

00:10:42,720 --> 00:10:45,920
try to do

00:10:43,440 --> 00:10:46,800
is we hope to get profits right before

00:10:45,920 --> 00:10:49,519
um kills

00:10:46,800 --> 00:10:50,880
so out of memory kills happen so that we

00:10:49,519 --> 00:10:53,360
get like the very

00:10:50,880 --> 00:10:54,079
last bad state before the process was

00:10:53,360 --> 00:10:56,000
killed

00:10:54,079 --> 00:10:57,440
and we want to do this automatically

00:10:56,000 --> 00:10:59,600
rather than by hand because we might

00:10:57,440 --> 00:11:01,600
forget being in an incident etc

00:10:59,600 --> 00:11:04,800
we want to be sure that we have these

00:11:01,600 --> 00:11:06,800
profiles when we really need the most

00:11:04,800 --> 00:11:08,640
so yeah enter continuous profiling let's

00:11:06,800 --> 00:11:12,000
take a deep dive

00:11:08,640 --> 00:11:13,360
as i said we want these heap logs and

00:11:12,000 --> 00:11:14,720
profiles and some others like go

00:11:13,360 --> 00:11:17,680
routines etc

00:11:14,720 --> 00:11:19,040
and we want to like every 10 seconds in

00:11:17,680 --> 00:11:22,240
this example for

00:11:19,040 --> 00:11:24,320
uh for for instance um take these

00:11:22,240 --> 00:11:26,079
profiles and snapshots and then we want

00:11:24,320 --> 00:11:27,760
to store them and if this looks like

00:11:26,079 --> 00:11:30,160
anything

00:11:27,760 --> 00:11:32,959
similar that you've seen before it yeah

00:11:30,160 --> 00:11:35,440
it totally looks like 10 series right

00:11:32,959 --> 00:11:37,839
so yeah we create a comprov and control

00:11:35,440 --> 00:11:40,560
stands on the shoulders of giants so it

00:11:37,839 --> 00:11:43,040
stands on the shoulders of prometheus

00:11:40,560 --> 00:11:46,160
and thanos and cortex

00:11:43,040 --> 00:11:48,320
and it's really cool to see like

00:11:46,160 --> 00:11:49,600
code from all these three projects come

00:11:48,320 --> 00:11:52,160
together and

00:11:49,600 --> 00:11:53,680
we heavily use the prometheus time

00:11:52,160 --> 00:11:56,079
series database

00:11:53,680 --> 00:11:56,720
um which will even dive deeper into

00:11:56,079 --> 00:11:59,440
later

00:11:56,720 --> 00:12:01,519
uh we use the prometheus so discovery

00:11:59,440 --> 00:12:02,320
and we use the scrape manager as well so

00:12:01,519 --> 00:12:04,399
all the like

00:12:02,320 --> 00:12:06,720
i'm scraping samples from each process

00:12:04,399 --> 00:12:09,760
is exactly the same code almost

00:12:06,720 --> 00:12:12,800
as prometheus has it and then

00:12:09,760 --> 00:12:16,160
we also use the prometheus remote right

00:12:12,800 --> 00:12:19,040
um mechanism to whenever we

00:12:16,160 --> 00:12:19,920
scrape samples we can ship them off uh

00:12:19,040 --> 00:12:24,399
remotely

00:12:19,920 --> 00:12:27,440
so yeah like lots of reusability

00:12:24,399 --> 00:12:29,760
that we gain from from relying on on

00:12:27,440 --> 00:12:31,839
these projects

00:12:29,760 --> 00:12:33,920
what does controv look like it's quite a

00:12:31,839 --> 00:12:34,399
simplistic ui at the moment and you can

00:12:33,920 --> 00:12:37,200
see

00:12:34,399 --> 00:12:38,320
you have this like query interface on on

00:12:37,200 --> 00:12:41,279
the top

00:12:38,320 --> 00:12:42,880
um top left where you type what kind of

00:12:41,279 --> 00:12:44,560
profile you want in this case a heap

00:12:42,880 --> 00:12:48,000
profile and then the job

00:12:44,560 --> 00:12:50,880
um selector and then yeah you get like

00:12:48,000 --> 00:12:52,959
a series of of timestamps and you can

00:12:50,880 --> 00:12:56,000
click on on the individual ones and

00:12:52,959 --> 00:12:59,279
you will get a profile

00:12:56,000 --> 00:13:01,440
and to query um these profiles it's

00:12:59,279 --> 00:13:02,399
because we use the prometheus service

00:13:01,440 --> 00:13:04,959
discovery

00:13:02,399 --> 00:13:06,639
it looks almost exactly the same as as

00:13:04,959 --> 00:13:07,040
prometheus so on the left hand side you

00:13:06,639 --> 00:13:09,920
see

00:13:07,040 --> 00:13:10,480
the go mems that's heap in used bytes

00:13:09,920 --> 00:13:12,959
metric

00:13:10,480 --> 00:13:13,760
and we can literally copy these label

00:13:12,959 --> 00:13:16,880
selectors

00:13:13,760 --> 00:13:17,440
and put them in control and change the

00:13:16,880 --> 00:13:19,920
metric

00:13:17,440 --> 00:13:21,519
to heap so we get the heat profile and

00:13:19,920 --> 00:13:22,000
then with the same label selectors we

00:13:21,519 --> 00:13:23,760
get

00:13:22,000 --> 00:13:25,920
a heap profile for that process and

00:13:23,760 --> 00:13:27,360
that's that's really powerful and super

00:13:25,920 --> 00:13:30,240
cool

00:13:27,360 --> 00:13:34,079
now i want to quickly give you a demo um

00:13:30,240 --> 00:13:34,639
to get a real feeling for comprov all

00:13:34,079 --> 00:13:37,600
right

00:13:34,639 --> 00:13:39,920
demo time so i have this example

00:13:37,600 --> 00:13:40,800
application that exposes prof and does a

00:13:39,920 --> 00:13:44,079
bunch of bad

00:13:40,800 --> 00:13:44,560
stuff and we want to be able to profile

00:13:44,079 --> 00:13:47,680
this

00:13:44,560 --> 00:13:49,040
application with comprof so first of all

00:13:47,680 --> 00:13:52,480
let me start our

00:13:49,040 --> 00:13:55,199
application by simply running the binary

00:13:52,480 --> 00:13:56,639
and that will start up yeah binds to

00:13:55,199 --> 00:14:00,720
port 8080

00:13:56,639 --> 00:14:03,199
and i can show you the

00:14:00,720 --> 00:14:04,639
scrape conflict that i have for this

00:14:03,199 --> 00:14:07,760
application for comprof

00:14:04,639 --> 00:14:10,079
um it is a static config so we literally

00:14:07,760 --> 00:14:12,399
just give it the localhost 8080

00:14:10,079 --> 00:14:13,760
address and we want to scrape it every

00:14:12,399 --> 00:14:16,880
single second

00:14:13,760 --> 00:14:18,560
um and it's called app so that's pretty

00:14:16,880 --> 00:14:21,600
much all there is to it

00:14:18,560 --> 00:14:25,199
so let's start con prof um

00:14:21,600 --> 00:14:27,360
with this config file and we want to run

00:14:25,199 --> 00:14:30,000
all the components of con prof

00:14:27,360 --> 00:14:30,880
at the same time so now we're starting

00:14:30,000 --> 00:14:34,560
con prof

00:14:30,880 --> 00:14:37,519
and this will take a second to

00:14:34,560 --> 00:14:38,959
start scraping um but we can already go

00:14:37,519 --> 00:14:42,959
to

00:14:38,959 --> 00:14:46,160
the compromise interface on

00:14:42,959 --> 00:14:47,680
ten thousand nine hundred two and yeah

00:14:46,160 --> 00:14:51,440
let's just

00:14:47,680 --> 00:14:54,560
search for job equals app and

00:14:51,440 --> 00:14:56,800
query it and sure enough we get um

00:14:54,560 --> 00:14:58,000
profiles for logs go routine heap and

00:14:56,800 --> 00:15:00,480
thread create

00:14:58,000 --> 00:15:02,480
so maybe let's look at heap first and we

00:15:00,480 --> 00:15:05,279
can even

00:15:02,480 --> 00:15:06,480
go and only show the heap metric or

00:15:05,279 --> 00:15:09,680
profile

00:15:06,480 --> 00:15:11,040
so yeah now we can let's click at the

00:15:09,680 --> 00:15:14,320
latest one

00:15:11,040 --> 00:15:18,000
and awesome we get this

00:15:14,320 --> 00:15:18,560
we get the profile and we can see that

00:15:18,000 --> 00:15:20,880
there

00:15:18,560 --> 00:15:23,360
in the main file there's a function

00:15:20,880 --> 00:15:27,440
called logmem and that has currently

00:15:23,360 --> 00:15:30,720
98 allocated um

00:15:27,440 --> 00:15:32,560
we can also look at a flame graph

00:15:30,720 --> 00:15:34,639
which is kind of a different way of

00:15:32,560 --> 00:15:38,160
looking at this so we see that

00:15:34,639 --> 00:15:39,600
at the root we've allocated 88 megabytes

00:15:38,160 --> 00:15:42,639
and all of these we have

00:15:39,600 --> 00:15:45,120
87 for the alloc

00:15:42,639 --> 00:15:47,199
mem function we can drill into different

00:15:45,120 --> 00:15:49,920
ones and we can click around

00:15:47,199 --> 00:15:51,680
we can go up again and that's just a

00:15:49,920 --> 00:15:52,480
different way of exploring these

00:15:51,680 --> 00:15:55,920
profiles

00:15:52,480 --> 00:15:57,440
which oftentimes is quite quite nice as

00:15:55,920 --> 00:16:00,560
well

00:15:57,440 --> 00:16:04,160
so let's look at all profiles again

00:16:00,560 --> 00:16:07,040
and yeah we also got cpu profiles so

00:16:04,160 --> 00:16:09,120
let's take a look at this one and sure

00:16:07,040 --> 00:16:12,320
enough we can see that

00:16:09,120 --> 00:16:13,920
um the calculate fib

00:16:12,320 --> 00:16:16,000
maybe it has something to do with

00:16:13,920 --> 00:16:19,040
fibonacci um

00:16:16,000 --> 00:16:22,079
took 99.63

00:16:19,040 --> 00:16:24,959
of the cpu time so

00:16:22,079 --> 00:16:27,120
yeah something is clearly happening here

00:16:24,959 --> 00:16:29,839
and

00:16:27,120 --> 00:16:31,839
we can also for this one take a look at

00:16:29,839 --> 00:16:34,959
the flame graph and we can see

00:16:31,839 --> 00:16:38,720
that the calculate fib is kind of like

00:16:34,959 --> 00:16:41,839
um or using cpu time like this

00:16:38,720 --> 00:16:46,320
and look into different sub

00:16:41,839 --> 00:16:49,519
graphs of of the cpu profile

00:16:46,320 --> 00:16:52,240
which is pretty cool yeah so

00:16:49,519 --> 00:16:53,040
let's take a look at the actual program

00:16:52,240 --> 00:16:56,000
that we were

00:16:53,040 --> 00:16:56,560
um instrumenting open up the main go

00:16:56,000 --> 00:16:59,199
file

00:16:56,560 --> 00:16:59,600
and yeah like right in the main go we

00:16:59,199 --> 00:17:02,320
call

00:16:59,600 --> 00:17:03,440
calculate fib so fibonacci and analog

00:17:02,320 --> 00:17:06,000
memory

00:17:03,440 --> 00:17:07,839
um so we calculate the fibonacc

00:17:06,000 --> 00:17:09,919
fibonacci number of

00:17:07,839 --> 00:17:11,360
very big number and that obviously takes

00:17:09,919 --> 00:17:13,839
a lot of cpu time

00:17:11,360 --> 00:17:14,880
and then we also allocate lots and lots

00:17:13,839 --> 00:17:16,559
of memory

00:17:14,880 --> 00:17:18,559
so these were the two functions that

00:17:16,559 --> 00:17:21,919
were quite

00:17:18,559 --> 00:17:25,120
prominently shown in our profiles

00:17:21,919 --> 00:17:26,640
very cool all right cool so let's talk

00:17:25,120 --> 00:17:28,160
about the control of time series

00:17:26,640 --> 00:17:30,720
database

00:17:28,160 --> 00:17:34,080
so because prometheus stores everything

00:17:30,720 --> 00:17:35,120
as tuples of n64 and float64 n64 for

00:17:34,080 --> 00:17:38,480
timestamps

00:17:35,120 --> 00:17:40,240
and float64 for all values um

00:17:38,480 --> 00:17:41,600
we needed to change that we needed

00:17:40,240 --> 00:17:43,840
controv to store

00:17:41,600 --> 00:17:45,760
in 64 and byte slice because profiles

00:17:43,840 --> 00:17:48,960
are bite slices

00:17:45,760 --> 00:17:51,520
and we ended up needing to change a lot

00:17:48,960 --> 00:17:52,799
of little code in very many places and

00:17:51,520 --> 00:17:56,880
that's what we did

00:17:52,799 --> 00:17:59,440
to give you an example here is the tsdb

00:17:56,880 --> 00:18:00,799
appender interface and we had to change

00:17:59,440 --> 00:18:04,640
the append interface

00:18:00,799 --> 00:18:08,000
um yeah to to accept n64 and float

00:18:04,640 --> 00:18:10,720
64 to n64 and byte slice um

00:18:08,000 --> 00:18:11,919
and super similar the tsdb iterator

00:18:10,720 --> 00:18:15,600
interface

00:18:11,919 --> 00:18:17,679
um whenever we like uh iterate through

00:18:15,600 --> 00:18:20,000
all these samples

00:18:17,679 --> 00:18:20,880
um once we actually want to retrieve a

00:18:20,000 --> 00:18:23,600
sample

00:18:20,880 --> 00:18:26,720
we uh return a byte slice instead of a

00:18:23,600 --> 00:18:26,720
float64 now

00:18:27,120 --> 00:18:34,080
all right so whenever controv scrapes

00:18:31,120 --> 00:18:35,200
a pre-prof endpoint we get back a gzip

00:18:34,080 --> 00:18:38,080
profile

00:18:35,200 --> 00:18:40,000
and we need to uncompress that profile

00:18:38,080 --> 00:18:42,480
we need to

00:18:40,000 --> 00:18:43,679
validate and parse that profile and once

00:18:42,480 --> 00:18:45,679
we've done that we

00:18:43,679 --> 00:18:48,000
g-zip it again and then we store these

00:18:45,679 --> 00:18:51,280
individual samples right

00:18:48,000 --> 00:18:53,600
um and that's what we did and that's how

00:18:51,280 --> 00:18:55,120
we could quickly reuse prometheus time

00:18:53,600 --> 00:18:57,600
series database

00:18:55,120 --> 00:18:59,280
but we wanted to improve things so as a

00:18:57,600 --> 00:19:02,559
first step we started

00:18:59,280 --> 00:19:03,039
um storing uncompressed profiles so we

00:19:02,559 --> 00:19:05,840
had to

00:19:03,039 --> 00:19:06,240
just change this one little snippet of

00:19:05,840 --> 00:19:09,520
code

00:19:06,240 --> 00:19:12,000
to store uncompressed profiles and

00:19:09,520 --> 00:19:13,039
what we ended up having was all the

00:19:12,000 --> 00:19:17,200
profiles

00:19:13,039 --> 00:19:19,280
uh in raw and we were storing them

00:19:17,200 --> 00:19:20,799
and it wasn't that great but it allowed

00:19:19,280 --> 00:19:22,799
us to actually

00:19:20,799 --> 00:19:25,360
do a next step and that is improve the

00:19:22,799 --> 00:19:26,240
compression by grouping samples together

00:19:25,360 --> 00:19:28,799
in

00:19:26,240 --> 00:19:30,080
in in groups and we i think we are right

00:19:28,799 --> 00:19:32,160
now 12

00:19:30,080 --> 00:19:34,640
samples per group and then we take all

00:19:32,160 --> 00:19:37,840
of these compress them and if you think

00:19:34,640 --> 00:19:38,559
back to the format a lot of the data is

00:19:37,840 --> 00:19:41,840
actually just

00:19:38,559 --> 00:19:44,160
strings right um so what we can do by

00:19:41,840 --> 00:19:46,559
grouping these samples together we can

00:19:44,160 --> 00:19:48,960
compress these strings a lot more

00:19:46,559 --> 00:19:51,039
efficient

00:19:48,960 --> 00:19:53,360
and then to make this better compression

00:19:51,039 --> 00:19:54,320
actually happen we needed to change the

00:19:53,360 --> 00:19:56,799
underlying

00:19:54,320 --> 00:19:57,760
time series database chunks so what we

00:19:56,799 --> 00:20:00,799
currently have

00:19:57,760 --> 00:20:02,960
is a bytes chunk that was always

00:20:00,799 --> 00:20:05,280
appending tuples of timestamps and

00:20:02,960 --> 00:20:07,679
values the timestamps

00:20:05,280 --> 00:20:09,360
were always a double delta so super

00:20:07,679 --> 00:20:10,559
similar to prometheus

00:20:09,360 --> 00:20:12,559
and then the values were these

00:20:10,559 --> 00:20:15,840
individual samples right

00:20:12,559 --> 00:20:17,679
and if we think think how to

00:20:15,840 --> 00:20:19,440
like kind of like iterate over these we

00:20:17,679 --> 00:20:22,799
are always like times

00:20:19,440 --> 00:20:27,120
0 value 0 times m1 times the

00:20:22,799 --> 00:20:29,280
value 1 and so on and looking at our ui

00:20:27,120 --> 00:20:30,159
oftentimes we only want to see the

00:20:29,280 --> 00:20:31,919
timestamps

00:20:30,159 --> 00:20:34,799
in this case we don't care about the

00:20:31,919 --> 00:20:38,000
individual profile just yet

00:20:34,799 --> 00:20:38,960
so like like iterating over all these

00:20:38,000 --> 00:20:42,000
samples

00:20:38,960 --> 00:20:44,640
is kind of a waste and to

00:20:42,000 --> 00:20:45,679
improve things we split the bytes chunk

00:20:44,640 --> 00:20:48,000
into a timestamp

00:20:45,679 --> 00:20:48,880
chunk and a values chunk essentially

00:20:48,000 --> 00:20:52,240
storing them

00:20:48,880 --> 00:20:53,440
separately but we kept track so we

00:20:52,240 --> 00:20:55,760
always append

00:20:53,440 --> 00:20:57,600
together to both chunks and we always

00:20:55,760 --> 00:21:00,720
iterate over both

00:20:57,600 --> 00:21:03,440
if needed but now we can only

00:21:00,720 --> 00:21:04,080
iterate over time stems and ignore the

00:21:03,440 --> 00:21:07,600
values

00:21:04,080 --> 00:21:10,799
the samples entirely

00:21:07,600 --> 00:21:12,960
if we have to though we can

00:21:10,799 --> 00:21:14,799
at the same time iterate over both

00:21:12,960 --> 00:21:18,159
timestamps and values

00:21:14,799 --> 00:21:22,080
to in the end get to the same place

00:21:18,159 --> 00:21:24,400
and then return that profile

00:21:22,080 --> 00:21:25,919
what this allows us to do is the

00:21:24,400 --> 00:21:28,080
timestamps can be

00:21:25,919 --> 00:21:30,400
iterated over individually they are

00:21:28,080 --> 00:21:33,840
still using double data

00:21:30,400 --> 00:21:37,120
to to store them and the values instead

00:21:33,840 --> 00:21:39,039
are now grouped without the timestamps

00:21:37,120 --> 00:21:39,919
in between and we can compress these

00:21:39,039 --> 00:21:43,679
groups of

00:21:39,919 --> 00:21:44,159
of profiles together and get a lot more

00:21:43,679 --> 00:21:47,440
um

00:21:44,159 --> 00:21:49,600
compression out of this after lots of

00:21:47,440 --> 00:21:52,640
benchmarking we actually chose

00:21:49,600 --> 00:21:54,559
um ted std for compression

00:21:52,640 --> 00:21:55,760
um i don't know if that's not the right

00:21:54,559 --> 00:21:58,000
pronunciation

00:21:55,760 --> 00:21:59,360
but yeah like we're using ted std for

00:21:58,000 --> 00:22:02,080
for compression now

00:21:59,360 --> 00:22:03,360
and in the benchmarks i'm i'm seeing

00:22:02,080 --> 00:22:06,799
like up to 50

00:22:03,360 --> 00:22:08,880
disc savings and in other benchmarks i

00:22:06,799 --> 00:22:11,120
think i've seen up to 75

00:22:08,880 --> 00:22:13,520
so it's quite significant with a little

00:22:11,120 --> 00:22:16,000
bit of overhead in memory

00:22:13,520 --> 00:22:18,000
um but yeah like i think it's totally

00:22:16,000 --> 00:22:21,039
worth it

00:22:18,000 --> 00:22:23,600
so for compreh road map

00:22:21,039 --> 00:22:24,559
obviously the ui is very basic and we

00:22:23,600 --> 00:22:27,360
will want to

00:22:24,559 --> 00:22:29,440
re-ramp the ui we might take ui

00:22:27,360 --> 00:22:32,400
components from our product and

00:22:29,440 --> 00:22:35,120
and get them into con prof um so that's

00:22:32,400 --> 00:22:37,520
definitely something we want to do

00:22:35,120 --> 00:22:39,520
we also think that there's more room for

00:22:37,520 --> 00:22:40,720
improvements for the storage we can have

00:22:39,520 --> 00:22:43,760
better compression

00:22:40,720 --> 00:22:46,240
if you think back a lot of the actual

00:22:43,760 --> 00:22:49,440
data are just strings that are

00:22:46,240 --> 00:22:52,159
repeated and we already made

00:22:49,440 --> 00:22:52,799
some significant improvements by

00:22:52,159 --> 00:22:54,880
grouping

00:22:52,799 --> 00:22:56,400
samples together but i think we can

00:22:54,880 --> 00:22:59,919
still get more

00:22:56,400 --> 00:23:01,760
um by kind of like improving the storage

00:22:59,919 --> 00:23:03,679
and we also can be more efficient with

00:23:01,760 --> 00:23:06,080
querying

00:23:03,679 --> 00:23:07,280
all right that's everything um i hope

00:23:06,080 --> 00:23:10,080
you enjoyed the

00:23:07,280 --> 00:23:11,120
rest of prom con and yeah we have some

00:23:10,080 --> 00:23:12,960
some nice

00:23:11,120 --> 00:23:15,760
sauce product out which is called the

00:23:12,960 --> 00:23:19,039
polar signals continuous profiler and

00:23:15,760 --> 00:23:20,799
we actually take profiles and get

00:23:19,039 --> 00:23:21,760
metrics out of them so as you can see in

00:23:20,799 --> 00:23:24,640
the screenshot

00:23:21,760 --> 00:23:25,679
you get like a metric for each profile

00:23:24,640 --> 00:23:27,360
and you can see

00:23:25,679 --> 00:23:29,120
if something is zooming you click on the

00:23:27,360 --> 00:23:30,000
on the metric and you get a profile so

00:23:29,120 --> 00:23:32,159
that's pretty

00:23:30,000 --> 00:23:33,760
exciting as well so check that out let

00:23:32,159 --> 00:23:36,159
us know if you have

00:23:33,760 --> 00:23:37,039
any other feedback and if you want to

00:23:36,159 --> 00:23:39,600
become

00:23:37,039 --> 00:23:40,159
part of the comprov community reach out

00:23:39,600 --> 00:23:46,400
as well

00:23:40,159 --> 00:23:46,400

YouTube URL: https://www.youtube.com/watch?v=GwQZSS8tQH0


