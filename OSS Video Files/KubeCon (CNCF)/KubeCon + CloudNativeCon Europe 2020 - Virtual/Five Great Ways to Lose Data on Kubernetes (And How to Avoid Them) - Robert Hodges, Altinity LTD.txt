Title: Five Great Ways to Lose Data on Kubernetes (And How to Avoid Them) - Robert Hodges, Altinity LTD
Publication date: 2020-08-27
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Five Great Ways to Lose Data on Kubernetes (And How to Avoid Them) - Robert Hodges, Altinity LTD 

Databases and stateful apps are gravitating rapidly to Kubernetes, so the sorrows of accidental data loss cannot be far behind. As long-time database engineers and authors of the ClickHouse Kubernetes operator, our team has seen lots of imaginative ways to lose data. We also learned how to prevent them.  The talk starts with standard high availability/disaster recovery models used in DBMS and demonstrates that classic ways to lose data are still alive and well on Kubernetes. We'll then pivot to Kubernetes-specific disasters-in-waiting, such as the PV that wasn't, affinity afflictions, and the dreaded fat fingers of fate. The talk will help instill a healthy sense of paranoia and give listeners tools to ensure their experiences with cloud-native data will be happy ones.

https://sched.co/Zet6
Captions: 
	00:00:00,080 --> 00:00:04,480
hello everyone this is robert hodges and

00:00:02,399 --> 00:00:06,240
i'm happy to welcome you to my talk on

00:00:04,480 --> 00:00:08,960
five great ways to lose data on

00:00:06,240 --> 00:00:11,360
kubernetes and how to avoid them

00:00:08,960 --> 00:00:12,480
it's an honor to be presenting at

00:00:11,360 --> 00:00:15,920
kubecon europe

00:00:12,480 --> 00:00:17,840
2020 where this is a great conference a

00:00:15,920 --> 00:00:19,600
lot of really wonderful talks so we're

00:00:17,840 --> 00:00:21,039
just totally happy that uh to be

00:00:19,600 --> 00:00:22,400
selected

00:00:21,039 --> 00:00:24,400
the topics that i'll be talking about

00:00:22,400 --> 00:00:26,320
are things that my company and

00:00:24,400 --> 00:00:28,800
and my colleagues have dealt with from

00:00:26,320 --> 00:00:30,080
day to day over the last couple years so

00:00:28,800 --> 00:00:31,439
we hope that you will enjoy the

00:00:30,080 --> 00:00:32,480
information we're about to share with

00:00:31,439 --> 00:00:34,559
you

00:00:32,480 --> 00:00:36,160
speaking of my company let's do some

00:00:34,559 --> 00:00:38,239
introductions

00:00:36,160 --> 00:00:40,320
so my name again is robert hodges i am

00:00:38,239 --> 00:00:44,399
the alternative ceo

00:00:40,320 --> 00:00:46,719
um and but more relevantly for this talk

00:00:44,399 --> 00:00:48,640
i have a background long background in

00:00:46,719 --> 00:00:50,879
data i started with my first database

00:00:48,640 --> 00:00:53,280
management system in 1983.

00:00:50,879 --> 00:00:54,239
i've been working on them uh databases

00:00:53,280 --> 00:00:55,840
of one kind or another almost

00:00:54,239 --> 00:00:57,600
continuously since then

00:00:55,840 --> 00:00:59,359
with some short jogs into things like

00:00:57,600 --> 00:01:02,559
virtualization and security

00:00:59,359 --> 00:01:04,799
i've been using kubernetes since 2018.

00:01:02,559 --> 00:01:06,720
my company altenity is an enterprise

00:01:04,799 --> 00:01:09,920
provider for click house

00:01:06,720 --> 00:01:11,360
so we provide services and software for

00:01:09,920 --> 00:01:12,960
the click house data warehouse it's a

00:01:11,360 --> 00:01:16,240
very popular very fast

00:01:12,960 --> 00:01:18,799
open source sql data warehouse

00:01:16,240 --> 00:01:20,159
again most relevantly for this talk we

00:01:18,799 --> 00:01:21,840
are the implementers of the click house

00:01:20,159 --> 00:01:23,600
kubernetes operator and have made a

00:01:21,840 --> 00:01:25,759
major commitment to

00:01:23,600 --> 00:01:26,640
operating data warehouses in a cloud

00:01:25,759 --> 00:01:29,600
native fashion

00:01:26,640 --> 00:01:30,960
on kubernetes and that work is the

00:01:29,600 --> 00:01:31,759
source of a lot of the information we're

00:01:30,960 --> 00:01:34,640
going to provide

00:01:31,759 --> 00:01:34,640
in this talk today

00:01:37,119 --> 00:01:41,040
before i get started i'd like to just

00:01:39,040 --> 00:01:43,040
define a couple of things so that

00:01:41,040 --> 00:01:45,119
we're we have the right frame of

00:01:43,040 --> 00:01:48,880
reference for thinking about

00:01:45,119 --> 00:01:49,360
the problems of data loss so the first

00:01:48,880 --> 00:01:51,600
thing

00:01:49,360 --> 00:01:52,960
is just to define what do we mean by

00:01:51,600 --> 00:01:56,479
data

00:01:52,960 --> 00:01:58,560
in a nutshell data is any information

00:01:56,479 --> 00:02:00,640
and well for the purposes of this talk

00:01:58,560 --> 00:02:03,119
is any information that is

00:02:00,640 --> 00:02:04,240
used to either operate or guide a

00:02:03,119 --> 00:02:06,159
business

00:02:04,240 --> 00:02:08,399
in the example i'm showing you right now

00:02:06,159 --> 00:02:10,640
we see a typical table from a from a

00:02:08,399 --> 00:02:13,120
data warehouse containing sales data

00:02:10,640 --> 00:02:14,959
so data warehouses are used to answer

00:02:13,120 --> 00:02:16,480
questions that often go to company

00:02:14,959 --> 00:02:18,800
strategy and tactics such

00:02:16,480 --> 00:02:20,720
as which products have the best gross

00:02:18,800 --> 00:02:21,360
margins over time if you can figure that

00:02:20,720 --> 00:02:23,120
out

00:02:21,360 --> 00:02:24,160
you know which products make the most

00:02:23,120 --> 00:02:25,599
money and the ones that you want to

00:02:24,160 --> 00:02:27,599
focus on selling

00:02:25,599 --> 00:02:29,440
similarly if you can answer the question

00:02:27,599 --> 00:02:32,480
which kinds of companies are most likely

00:02:29,440 --> 00:02:34,720
to buy sku556 that's a product

00:02:32,480 --> 00:02:36,160
then you can direct your sales people to

00:02:34,720 --> 00:02:40,080
the places where they're most likely to

00:02:36,160 --> 00:02:40,080
make sales and bring home the bacon

00:02:40,560 --> 00:02:43,599
similarly we want to define what we mean

00:02:42,239 --> 00:02:46,319
by the word lose

00:02:43,599 --> 00:02:48,239
well in the context of data lose is

00:02:46,319 --> 00:02:49,280
actually a spectrum it's not a single

00:02:48,239 --> 00:02:50,720
thing so

00:02:49,280 --> 00:02:52,400
when we lose data we can talk about

00:02:50,720 --> 00:02:54,879
losing it temporarily so

00:02:52,400 --> 00:02:56,400
it can be just gone for a while but can

00:02:54,879 --> 00:02:59,360
come back completely

00:02:56,400 --> 00:03:01,360
all the way to a state where it is

00:02:59,360 --> 00:03:02,080
completely gone forever you will never

00:03:01,360 --> 00:03:05,360
see it again

00:03:02,080 --> 00:03:07,760
you will feel awful and it's there's

00:03:05,360 --> 00:03:09,200
it's just vaporized so i call this the

00:03:07,760 --> 00:03:11,599
data loss arrow of

00:03:09,200 --> 00:03:13,519
evil and what's kind of interesting in

00:03:11,599 --> 00:03:15,760
in when dealing with data

00:03:13,519 --> 00:03:17,040
is the value depending on what kind of

00:03:15,760 --> 00:03:19,120
data you have

00:03:17,040 --> 00:03:20,720
the loss may be more or less severe at

00:03:19,120 --> 00:03:22,640
different points in the spectrum

00:03:20,720 --> 00:03:23,760
so for example if you're running my

00:03:22,640 --> 00:03:27,440
sequel and

00:03:23,760 --> 00:03:30,959
using it to operate a an e-commerce site

00:03:27,440 --> 00:03:32,799
if my sequel is even avail unavailable

00:03:30,959 --> 00:03:34,239
and the data is not not accessible it

00:03:32,799 --> 00:03:36,400
means you can't sell anything

00:03:34,239 --> 00:03:38,000
so you're basically at a standstill for

00:03:36,400 --> 00:03:39,760
data warehouses that's not such a big

00:03:38,000 --> 00:03:40,959
deal they often offer

00:03:39,760 --> 00:03:42,879
many of them particularly if they're

00:03:40,959 --> 00:03:45,200
batch oriented or doing uh

00:03:42,879 --> 00:03:47,120
answering strategic questions may run

00:03:45,200 --> 00:03:48,560
sort of on a nine to five basis

00:03:47,120 --> 00:03:50,640
so having it down for a while is not

00:03:48,560 --> 00:03:53,280
such a big deal on the other hand

00:03:50,640 --> 00:03:54,879
losing some big chunk of data means that

00:03:53,280 --> 00:03:56,159
you lose vision on what's the state of

00:03:54,879 --> 00:03:58,560
the business so that can be a very

00:03:56,159 --> 00:04:01,280
serious problem

00:03:58,560 --> 00:04:03,599
in both cases um and and for other

00:04:01,280 --> 00:04:05,519
databases as well there is a point

00:04:03,599 --> 00:04:07,519
where it's all gone and the result may

00:04:05,519 --> 00:04:09,120
be in in many cases that the business

00:04:07,519 --> 00:04:11,280
simply stops functioning so this is a

00:04:09,120 --> 00:04:13,840
very serious problem

00:04:11,280 --> 00:04:15,040
so with these two definitions in mind we

00:04:13,840 --> 00:04:16,959
can now step in

00:04:15,040 --> 00:04:18,959
and look at some of the different ways

00:04:16,959 --> 00:04:22,079
that we can lose data on kubernetes

00:04:18,959 --> 00:04:24,560
and how we avoid them so

00:04:22,079 --> 00:04:25,199
the first problem that i want to get

00:04:24,560 --> 00:04:27,040
discuss

00:04:25,199 --> 00:04:29,040
is what i call the single copy

00:04:27,040 --> 00:04:32,080
catastrophe

00:04:29,040 --> 00:04:34,479
this is pretty obvious from the name but

00:04:32,080 --> 00:04:36,000
first let's start by praising kubernetes

00:04:34,479 --> 00:04:38,720
one of the things that is wonderful

00:04:36,000 --> 00:04:40,960
about kubernetes is how easy it is to

00:04:38,720 --> 00:04:42,800
set up complex applications

00:04:40,960 --> 00:04:45,520
in this particular example i'm going to

00:04:42,800 --> 00:04:49,919
use helm so helm is a popular tool

00:04:45,520 --> 00:04:51,440
to create deployments in kubernetes

00:04:49,919 --> 00:04:53,440
bring them up you can upgrade them you

00:04:51,440 --> 00:04:55,199
can take them away in this particular

00:04:53,440 --> 00:04:56,160
example i'm going to create a namespace

00:04:55,199 --> 00:04:58,080
my sql

00:04:56,160 --> 00:05:00,160
using cubecuddle i'm then going to run a

00:04:58,080 --> 00:05:03,600
single line helm install command

00:05:00,160 --> 00:05:06,720
to install the um

00:05:03,600 --> 00:05:07,759
a my sql server which will then pop up

00:05:06,720 --> 00:05:09,840
and run

00:05:07,759 --> 00:05:12,080
and a few minutes after even a few

00:05:09,840 --> 00:05:15,360
seconds after i run this command

00:05:12,080 --> 00:05:17,280
i will see a database running in a pod

00:05:15,360 --> 00:05:19,440
on kubernetes with an attached

00:05:17,280 --> 00:05:21,199
persistent volume

00:05:19,440 --> 00:05:22,960
unfortunately this database is kind of a

00:05:21,199 --> 00:05:24,240
delicate flower and

00:05:22,960 --> 00:05:26,880
particularly when we're thinking about

00:05:24,240 --> 00:05:28,400
data loss so there's a number of bad

00:05:26,880 --> 00:05:30,560
things that could possibly happen

00:05:28,400 --> 00:05:32,240
you could delete the pod that's usually

00:05:30,560 --> 00:05:34,560
not a big deal

00:05:32,240 --> 00:05:36,639
when you install from helm but it's

00:05:34,560 --> 00:05:38,479
possible that if you delete the node

00:05:36,639 --> 00:05:40,800
you might lose the storage because in

00:05:38,479 --> 00:05:42,560
fact the storage could

00:05:40,800 --> 00:05:44,479
you don't know for sure unless you check

00:05:42,560 --> 00:05:46,880
but the storage could be uh

00:05:44,479 --> 00:05:48,720
allocated locally and for sure if you

00:05:46,880 --> 00:05:50,400
delete the persistent volume

00:05:48,720 --> 00:05:52,240
for example if you allocate block

00:05:50,400 --> 00:05:55,280
storage your date is gone

00:05:52,240 --> 00:05:57,120
there's there's no replacement and

00:05:55,280 --> 00:05:59,360
this is kind of the same situation that

00:05:57,120 --> 00:05:59,919
you have if you have a laptop and you

00:05:59,360 --> 00:06:03,039
drop it

00:05:59,919 --> 00:06:07,360
lose it or something just uh

00:06:03,039 --> 00:06:08,960
something the storage dies so it's gone

00:06:07,360 --> 00:06:10,720
so databases have dealt with this

00:06:08,960 --> 00:06:12,720
problem for a really long time

00:06:10,720 --> 00:06:15,440
and the solution in just about every

00:06:12,720 --> 00:06:17,520
single case is to have replicas

00:06:15,440 --> 00:06:19,440
so you have different kinds of copies of

00:06:17,520 --> 00:06:21,280
data which mean that if you lose your

00:06:19,440 --> 00:06:22,880
main copy there's another copy you can

00:06:21,280 --> 00:06:24,639
use instead

00:06:22,880 --> 00:06:26,960
and my sql is kind of instructive

00:06:24,639 --> 00:06:29,440
because it illustrates very well

00:06:26,960 --> 00:06:31,120
the two main styles of replicas that we

00:06:29,440 --> 00:06:34,000
get with databases

00:06:31,120 --> 00:06:35,919
so my sql as you know if you've used

00:06:34,000 --> 00:06:36,880
have experience directly with mysql

00:06:35,919 --> 00:06:39,360
yourself

00:06:36,880 --> 00:06:41,520
has very good built-in replication and

00:06:39,360 --> 00:06:44,319
so replication enables you to have live

00:06:41,520 --> 00:06:44,960
copies you have a primary my sequel

00:06:44,319 --> 00:06:46,560
server

00:06:44,960 --> 00:06:48,880
that would for your e-commerce site that

00:06:46,560 --> 00:06:50,880
would be where your transactions run

00:06:48,880 --> 00:06:52,960
it has storage attached to it and then

00:06:50,880 --> 00:06:53,919
you have my sql replication enabled and

00:06:52,960 --> 00:06:56,479
it goes

00:06:53,919 --> 00:06:57,520
it as soon as a transaction commits in

00:06:56,479 --> 00:06:58,880
the primary it

00:06:57,520 --> 00:07:00,800
it immediately transfers it to the

00:06:58,880 --> 00:07:02,240
replica it's very fast

00:07:00,800 --> 00:07:03,840
often a second or less to move

00:07:02,240 --> 00:07:05,919
transactions across

00:07:03,840 --> 00:07:07,599
so what that means with this live copy

00:07:05,919 --> 00:07:08,960
is if you lose the primary you can just

00:07:07,599 --> 00:07:11,520
promote one of the replicas

00:07:08,960 --> 00:07:13,520
and you're right back in business again

00:07:11,520 --> 00:07:16,080
the other kind of replica is a static

00:07:13,520 --> 00:07:18,720
copy which is represented by a backup

00:07:16,080 --> 00:07:20,000
so you can take those replicas and this

00:07:18,720 --> 00:07:22,160
is again a very common

00:07:20,000 --> 00:07:23,919
way that this is handled in my sql from

00:07:22,160 --> 00:07:27,280
time to time you'll run a backup

00:07:23,919 --> 00:07:28,639
of the of the my of the mysql replica

00:07:27,280 --> 00:07:30,240
and then you'll store that back up in

00:07:28,639 --> 00:07:32,319
another location in case you need to

00:07:30,240 --> 00:07:34,720
make more replicas at a later time

00:07:32,319 --> 00:07:35,919
so static copies in live and live

00:07:34,720 --> 00:07:38,240
replicas

00:07:35,919 --> 00:07:39,759
these solve the problem of what to do

00:07:38,240 --> 00:07:42,800
when you lose your data in your

00:07:39,759 --> 00:07:44,000
in your primary database so

00:07:42,800 --> 00:07:46,160
the issue though when you come to

00:07:44,000 --> 00:07:48,319
kubernetes is how do you

00:07:46,160 --> 00:07:49,759
actually implement this in kubernetes

00:07:48,319 --> 00:07:52,319
because we're talking

00:07:49,759 --> 00:07:53,759
at the very least a complex deployment

00:07:52,319 --> 00:07:55,599
with a couple of different types of

00:07:53,759 --> 00:07:58,960
servers running in separate pods

00:07:55,599 --> 00:08:01,039
we have backups and in fact

00:07:58,960 --> 00:08:02,000
as we get to other types of databases

00:08:01,039 --> 00:08:04,720
these these may be

00:08:02,000 --> 00:08:05,759
far more complex modern databases are

00:08:04,720 --> 00:08:08,960
typically

00:08:05,759 --> 00:08:11,039
very complex distributed applications so

00:08:08,960 --> 00:08:12,160
the question is how do we set it set

00:08:11,039 --> 00:08:15,199
this up in

00:08:12,160 --> 00:08:17,360
in kubernetes especially in cases where

00:08:15,199 --> 00:08:19,840
we might have to

00:08:17,360 --> 00:08:23,360
create dozens of resources to implement

00:08:19,840 --> 00:08:23,360
the database in kubernetes

00:08:24,639 --> 00:08:28,160
the answer that's evolved over the last

00:08:26,400 --> 00:08:29,599
few years is something called kubernetes

00:08:28,160 --> 00:08:31,520
operators

00:08:29,599 --> 00:08:33,279
so the way that a kubernete what

00:08:31,520 --> 00:08:35,039
kubernetes operator does is it allows

00:08:33,279 --> 00:08:36,800
you to create what are called custom

00:08:35,039 --> 00:08:38,560
resource definitions

00:08:36,800 --> 00:08:40,479
so instead of thinking of the database

00:08:38,560 --> 00:08:41,919
as a collection of deployments stateful

00:08:40,479 --> 00:08:45,360
sets

00:08:41,919 --> 00:08:47,839
services pods you instead have a single

00:08:45,360 --> 00:08:50,000
a new resource which represents the

00:08:47,839 --> 00:08:52,640
entire structure of the database

00:08:50,000 --> 00:08:54,640
and and so what happens is you load this

00:08:52,640 --> 00:08:57,680
custom resource definition

00:08:54,640 --> 00:08:59,440
into kubernetes it is kubernetes api

00:08:57,680 --> 00:09:01,279
recognized that it's handled by a

00:08:59,440 --> 00:09:02,320
particular operator type it dispatches

00:09:01,279 --> 00:09:04,959
it to the operator

00:09:02,320 --> 00:09:07,120
which then looks at it and decides what

00:09:04,959 --> 00:09:08,720
changes it needs to make in kubernetes

00:09:07,120 --> 00:09:11,200
to implement the database

00:09:08,720 --> 00:09:13,279
so what it then does is from there it

00:09:11,200 --> 00:09:16,160
will set up things like stateful sets or

00:09:13,279 --> 00:09:17,839
services or pods or persistent storage

00:09:16,160 --> 00:09:19,920
define them in fcd and then the native

00:09:17,839 --> 00:09:23,120
controllers will take over

00:09:19,920 --> 00:09:25,279
actually implementing the um

00:09:23,120 --> 00:09:26,320
the database out in the kubernetes

00:09:25,279 --> 00:09:29,279
cluster itself

00:09:26,320 --> 00:09:30,240
and the result is a best practice uh

00:09:29,279 --> 00:09:32,640
deployment

00:09:30,240 --> 00:09:36,000
which contains all the pieces including

00:09:32,640 --> 00:09:37,760
things like replication possibly backups

00:09:36,000 --> 00:09:41,120
that you need to ensure that you have

00:09:37,760 --> 00:09:43,600
adequate replicas to protect your data

00:09:41,120 --> 00:09:44,720
so and just to illustrate the what we

00:09:43,600 --> 00:09:47,200
show on the right here

00:09:44,720 --> 00:09:48,959
is a data warehouse setup these are

00:09:47,200 --> 00:09:51,120
relatively complicated compared

00:09:48,959 --> 00:09:53,680
especially compared to my sequel let's

00:09:51,120 --> 00:09:56,640
just have a look at the crd

00:09:53,680 --> 00:09:57,839
you can see from this that in fact what

00:09:56,640 --> 00:09:59,920
this does is it

00:09:57,839 --> 00:10:02,720
it is much simpler than for example

00:09:59,920 --> 00:10:04,880
having to do the deployments directly

00:10:02,720 --> 00:10:05,920
and do all the low-level resources we

00:10:04,880 --> 00:10:07,519
have a single

00:10:05,920 --> 00:10:08,959
very simple resource file this is

00:10:07,519 --> 00:10:09,760
actually a real resource file that you

00:10:08,959 --> 00:10:12,480
could load

00:10:09,760 --> 00:10:14,800
and it says let's have a cluster named

00:10:12,480 --> 00:10:16,320
ch01

00:10:14,800 --> 00:10:18,640
actually with a configuration called

00:10:16,320 --> 00:10:20,160
replicated two shards two replicas

00:10:18,640 --> 00:10:22,160
and then point to zookeeper which is

00:10:20,160 --> 00:10:25,040
used to keep uh to keep track of

00:10:22,160 --> 00:10:25,920
of state between the replicas that's it

00:10:25,040 --> 00:10:28,320
so

00:10:25,920 --> 00:10:30,560
this illustrates how how much the

00:10:28,320 --> 00:10:33,360
operators simplify this and then

00:10:30,560 --> 00:10:35,279
also handle the issues of ensuring

00:10:33,360 --> 00:10:37,040
proper replicas as well as many other

00:10:35,279 --> 00:10:38,240
things that are necessary for databases

00:10:37,040 --> 00:10:40,959
to function

00:10:38,240 --> 00:10:43,200
so this is a huge step forward and is

00:10:40,959 --> 00:10:46,560
really one of the ways that we can

00:10:43,200 --> 00:10:48,959
implement replication and backups in

00:10:46,560 --> 00:10:51,839
databases on kubernetes in a relatively

00:10:48,959 --> 00:10:54,640
straightforward manner

00:10:51,839 --> 00:10:56,160
so that was the first so replicas are

00:10:54,640 --> 00:10:58,000
one of the first solutions to

00:10:56,160 --> 00:10:59,200
to losing data let's look at some of the

00:10:58,000 --> 00:11:01,920
other ways that we can

00:10:59,200 --> 00:11:04,640
we can lose data so the next one is what

00:11:01,920 --> 00:11:07,279
i call blast radius blues

00:11:04,640 --> 00:11:08,880
and let's talk about this term it's one

00:11:07,279 --> 00:11:11,200
of my favorite terms from

00:11:08,880 --> 00:11:12,079
from the high availability world the

00:11:11,200 --> 00:11:14,320
notion is

00:11:12,079 --> 00:11:15,680
a blast radius is how far away do you

00:11:14,320 --> 00:11:18,560
have to be before you're not

00:11:15,680 --> 00:11:19,360
affected by a failure and there are

00:11:18,560 --> 00:11:20,600
various

00:11:19,360 --> 00:11:22,560
you can think of this as a bunch of

00:11:20,600 --> 00:11:25,120
concentric circles

00:11:22,560 --> 00:11:27,360
for example if a host fails or something

00:11:25,120 --> 00:11:28,640
on the host ceases to work

00:11:27,360 --> 00:11:30,399
anything running on that host and

00:11:28,640 --> 00:11:30,800
potentially any data attached to the

00:11:30,399 --> 00:11:34,640
host

00:11:30,800 --> 00:11:36,480
may become available may just disappear

00:11:34,640 --> 00:11:38,079
so that's a that's a relatively

00:11:36,480 --> 00:11:38,399
constrained failure but they can of

00:11:38,079 --> 00:11:41,680
course

00:11:38,399 --> 00:11:42,079
extend out in to be to cover much more

00:11:41,680 --> 00:11:44,800
ground

00:11:42,079 --> 00:11:45,839
so for example uh hosts and racks racks

00:11:44,800 --> 00:11:47,680
are in data centers

00:11:45,839 --> 00:11:49,360
there it is possible to have failures

00:11:47,680 --> 00:11:51,040
which make data

00:11:49,360 --> 00:11:52,800
completely inaccessible across a data

00:11:51,040 --> 00:11:54,880
center so for example the failure of

00:11:52,800 --> 00:11:56,880
network attached storage

00:11:54,880 --> 00:11:59,120
or a failure of the network itself which

00:11:56,880 --> 00:12:02,000
can either cause data loss or

00:11:59,120 --> 00:12:02,240
unavailability beyond that you can think

00:12:02,000 --> 00:12:04,480
of

00:12:02,240 --> 00:12:05,440
a failure at the level of a kubernetes

00:12:04,480 --> 00:12:07,120
cluster

00:12:05,440 --> 00:12:10,079
so that things within that cluster

00:12:07,120 --> 00:12:11,519
become unavailable or unusable

00:12:10,079 --> 00:12:13,600
and then of course a failure within an

00:12:11,519 --> 00:12:16,320
entire region so for example

00:12:13,600 --> 00:12:17,839
you can have failures in you there have

00:12:16,320 --> 00:12:18,800
been historically failures in things

00:12:17,839 --> 00:12:21,600
like amazon

00:12:18,800 --> 00:12:22,880
that cost entire regions to lose access

00:12:21,600 --> 00:12:25,440
to critical services

00:12:22,880 --> 00:12:26,880
and even data across across a number of

00:12:25,440 --> 00:12:30,880
data centers

00:12:26,880 --> 00:12:33,680
so the really key thing with with

00:12:30,880 --> 00:12:34,399
to avoid blast radius problems is to get

00:12:33,680 --> 00:12:36,800
distance

00:12:34,399 --> 00:12:38,320
between your replicas and for that

00:12:36,800 --> 00:12:42,880
there's an incredibly helpful

00:12:38,320 --> 00:12:45,360
concept in in kubernetes called affinity

00:12:42,880 --> 00:12:46,240
which basically gives us the ability to

00:12:45,360 --> 00:12:48,480
move

00:12:46,240 --> 00:12:50,320
pods around as well as associated

00:12:48,480 --> 00:12:52,560
resources like storage

00:12:50,320 --> 00:12:53,360
into different locations so that they

00:12:52,560 --> 00:12:55,360
are

00:12:53,360 --> 00:12:57,200
far enough away that if if one thing

00:12:55,360 --> 00:12:58,079
fails it doesn't take all your data down

00:12:57,200 --> 00:13:00,959
with it

00:12:58,079 --> 00:13:02,079
so here's here's a simple example a very

00:13:00,959 --> 00:13:05,360
simple example that

00:13:02,079 --> 00:13:07,920
illustrates affinity as well as it's

00:13:05,360 --> 00:13:09,040
it's um it's opposite which is

00:13:07,920 --> 00:13:11,360
anti-affinity

00:13:09,040 --> 00:13:13,360
so in this particular picture we have a

00:13:11,360 --> 00:13:15,839
couple of zookeeper pods

00:13:13,360 --> 00:13:17,760
zookeeper is a very popular distributed

00:13:15,839 --> 00:13:20,399
directory service

00:13:17,760 --> 00:13:21,360
that maintains consensus across a number

00:13:20,399 --> 00:13:22,639
of nodes

00:13:21,360 --> 00:13:24,399
it's used for things like leader

00:13:22,639 --> 00:13:26,240
election as well as

00:13:24,399 --> 00:13:28,000
uh you know holding distributed logs

00:13:26,240 --> 00:13:29,120
anything where a series of processes

00:13:28,000 --> 00:13:31,200
need to agree

00:13:29,120 --> 00:13:32,639
on what the state of things is it's

00:13:31,200 --> 00:13:35,920
really important with zookeeper

00:13:32,639 --> 00:13:37,839
to move the to have the nodes separated

00:13:35,920 --> 00:13:38,959
so that a failure will not take all of

00:13:37,839 --> 00:13:41,120
them down

00:13:38,959 --> 00:13:43,279
in this particular case what we can do

00:13:41,120 --> 00:13:45,760
is we can use anti-affinity

00:13:43,279 --> 00:13:48,959
to keep two zookeepers from showing up

00:13:45,760 --> 00:13:52,240
on the same node in this case host 116

00:13:48,959 --> 00:13:53,279
on the left similarly we can use

00:13:52,240 --> 00:13:57,279
affinity

00:13:53,279 --> 00:13:58,160
to drive the um that that zookeeper one

00:13:57,279 --> 00:14:00,880
instance

00:13:58,160 --> 00:14:03,040
to a host that has a particular that has

00:14:00,880 --> 00:14:06,399
particular characteristics that interest

00:14:03,040 --> 00:14:07,199
us one common use of affinity is to

00:14:06,399 --> 00:14:09,760
drive

00:14:07,199 --> 00:14:12,160
the pods basically to separate the pods

00:14:09,760 --> 00:14:13,760
across availability zones

00:14:12,160 --> 00:14:15,920
what i'd like to do is show you the code

00:14:13,760 --> 00:14:17,680
that that implements this

00:14:15,920 --> 00:14:20,240
because it's a very powerful feature

00:14:17,680 --> 00:14:22,560
inside kubernetes

00:14:20,240 --> 00:14:23,600
so let's first look at anti-affinity so

00:14:22,560 --> 00:14:26,240
this is the

00:14:23,600 --> 00:14:28,000
this is pod anti-affinity which drives

00:14:26,240 --> 00:14:29,199
the basically it's going to drive the

00:14:28,000 --> 00:14:30,880
the pods apart

00:14:29,199 --> 00:14:32,720
and keep them from landing on this being

00:14:30,880 --> 00:14:35,279
scheduled on the same hosts

00:14:32,720 --> 00:14:36,320
so what you see here is a part of a pod

00:14:35,279 --> 00:14:38,720
definition

00:14:36,320 --> 00:14:40,639
and under the spec we have affinity

00:14:38,720 --> 00:14:41,839
rules and we have a rule for pod

00:14:40,639 --> 00:14:43,680
anti-affinity

00:14:41,839 --> 00:14:45,120
and we say that this is a rule that is

00:14:43,680 --> 00:14:46,880
going to be

00:14:45,120 --> 00:14:48,320
used during scheduling so when we're

00:14:46,880 --> 00:14:50,240
we're starting the pod but then we don't

00:14:48,320 --> 00:14:51,199
care later on while the pod is actually

00:14:50,240 --> 00:14:54,800
running

00:14:51,199 --> 00:14:58,000
and what this syntax says is that

00:14:54,800 --> 00:15:01,440
there's a key hostname and we don't

00:14:58,000 --> 00:15:05,040
want to have two people basically two

00:15:01,440 --> 00:15:07,360
pods in this case from a

00:15:05,040 --> 00:15:08,639
from one of our our data warehouses we

00:15:07,360 --> 00:15:09,920
don't want to have two pods from the

00:15:08,639 --> 00:15:12,800
same data warehouse

00:15:09,920 --> 00:15:14,560
on the same host as you can see from

00:15:12,800 --> 00:15:15,839
looking at this syntax and even hearing

00:15:14,560 --> 00:15:17,199
me read it it's a little bit

00:15:15,839 --> 00:15:18,320
non-intuitive

00:15:17,199 --> 00:15:20,480
and we'll get to that because that

00:15:18,320 --> 00:15:21,360
actually turns out to be an issue but if

00:15:20,480 --> 00:15:24,000
you get this right

00:15:21,360 --> 00:15:25,519
what this will do is is drive the pods

00:15:24,000 --> 00:15:28,160
off the you will never get two pods

00:15:25,519 --> 00:15:31,360
scheduled on the same host

00:15:28,160 --> 00:15:33,440
now what we can also do is is is move

00:15:31,360 --> 00:15:34,800
pods to make them schedule in particular

00:15:33,440 --> 00:15:37,680
locations and for this

00:15:34,800 --> 00:15:39,440
node affinity is very helpful what we

00:15:37,680 --> 00:15:42,399
see here

00:15:39,440 --> 00:15:45,600
is node affinity which is going to

00:15:42,399 --> 00:15:48,639
assign a pod to a particular

00:15:45,600 --> 00:15:51,920
availability zone here us west 2.

00:15:48,639 --> 00:15:53,759
so the and in this case

00:15:51,920 --> 00:15:56,240
what we see is the key is the failure

00:15:53,759 --> 00:15:58,079
domain blah blah blah zone

00:15:56,240 --> 00:16:00,320
that is a property that's automatically

00:15:58,079 --> 00:16:02,000
set in most kubernetes clusters

00:16:00,320 --> 00:16:04,639
and we see that the value that we're

00:16:02,000 --> 00:16:06,399
looking for is us west 2a

00:16:04,639 --> 00:16:08,320
so what this is going to do is any pod

00:16:06,399 --> 00:16:08,800
that contains this is going to want to

00:16:08,320 --> 00:16:12,639
land

00:16:08,800 --> 00:16:13,680
in in us west 2a when it gets scheduled

00:16:12,639 --> 00:16:15,519
one of the things that's kind of

00:16:13,680 --> 00:16:16,880
interesting i should note here about

00:16:15,519 --> 00:16:19,920
about affinity

00:16:16,880 --> 00:16:20,800
is particularly availability zones is we

00:16:19,920 --> 00:16:23,120
have found in

00:16:20,800 --> 00:16:24,800
our work that it's best if you're really

00:16:23,120 --> 00:16:27,199
concerned about distribution across

00:16:24,800 --> 00:16:28,560
azs it's best to be very explicit about

00:16:27,199 --> 00:16:30,320
it as we show here

00:16:28,560 --> 00:16:31,920
there are sort of tricks you can use

00:16:30,320 --> 00:16:34,720
that for example we'll

00:16:31,920 --> 00:16:36,000
we'll get node affinity perhaps across a

00:16:34,720 --> 00:16:38,880
stateful set but we

00:16:36,000 --> 00:16:40,480
find them difficult to configure and

00:16:38,880 --> 00:16:42,240
it's just better to be totally explicit

00:16:40,480 --> 00:16:44,639
about where you want things to land

00:16:42,240 --> 00:16:46,000
which means that different pods within a

00:16:44,639 --> 00:16:47,839
database cluster

00:16:46,000 --> 00:16:50,079
will actually have different different

00:16:47,839 --> 00:16:52,880
affinity rules

00:16:50,079 --> 00:16:53,360
the result when you get it right is that

00:16:52,880 --> 00:16:56,560
you get

00:16:53,360 --> 00:16:58,880
distance between your replicas so

00:16:56,560 --> 00:17:00,800
in this illustration right here we're

00:16:58,880 --> 00:17:02,560
showing a couple of shards on the data

00:17:00,800 --> 00:17:05,039
warehouse that's click house

00:17:02,560 --> 00:17:06,880
and then we have three zookeeper pods so

00:17:05,039 --> 00:17:09,039
that's a zookeeper ensemble

00:17:06,880 --> 00:17:10,079
we have the replicas neatly spread

00:17:09,039 --> 00:17:13,760
across the

00:17:10,079 --> 00:17:18,079
availability zones us west 2 a b and c

00:17:13,760 --> 00:17:19,280
and um and and so that that spreads the

00:17:18,079 --> 00:17:21,199
data

00:17:19,280 --> 00:17:23,120
the uh other thing that i want to draw

00:17:21,199 --> 00:17:25,199
your attention to is the backups

00:17:23,120 --> 00:17:26,319
so if you take backups a really smart

00:17:25,199 --> 00:17:28,559
thing to do is stick them in

00:17:26,319 --> 00:17:30,160
object storage this is common just

00:17:28,559 --> 00:17:32,720
because object storage is

00:17:30,160 --> 00:17:34,480
convenient and relatively inexpensive

00:17:32,720 --> 00:17:37,840
but the other thing about object storage

00:17:34,480 --> 00:17:40,400
is that it is itself replicated and

00:17:37,840 --> 00:17:41,840
can be quite distant from the systems

00:17:40,400 --> 00:17:43,919
that you're backing up so for example

00:17:41,840 --> 00:17:45,360
you can set up replication automatically

00:17:43,919 --> 00:17:46,960
across regions

00:17:45,360 --> 00:17:49,120
and what will happen is that means you

00:17:46,960 --> 00:17:50,559
have a backup that's even outside the

00:17:49,120 --> 00:17:54,320
region that you're currently

00:17:50,559 --> 00:17:55,520
currently working in so the topology

00:17:54,320 --> 00:17:57,360
that we're showing right here with the

00:17:55,520 --> 00:18:00,000
distance between replicas is actually

00:17:57,360 --> 00:18:03,200
quite common in kubernetes

00:18:00,000 --> 00:18:06,799
and and is very easy to implement

00:18:03,200 --> 00:18:09,440
with things like cops with aussie ks

00:18:06,799 --> 00:18:10,880
and other types of managed kubernetes so

00:18:09,440 --> 00:18:14,000
so definitely something that you should

00:18:10,880 --> 00:18:15,200
use to protect data

00:18:14,000 --> 00:18:18,000
when we're talking about blast

00:18:15,200 --> 00:18:19,440
protection across regions or kubernetes

00:18:18,000 --> 00:18:21,440
in other words sort of

00:18:19,440 --> 00:18:22,880
wider out in these concentric circles

00:18:21,440 --> 00:18:24,640
dealing with things like the loss of a

00:18:22,880 --> 00:18:27,200
kubernetes cluster

00:18:24,640 --> 00:18:29,280
the loss of a region this is also this

00:18:27,200 --> 00:18:30,480
is no longer a data problem but more of

00:18:29,280 --> 00:18:33,760
a question of losing

00:18:30,480 --> 00:18:36,000
it resources entirely so

00:18:33,760 --> 00:18:38,160
it's much more of an exercise for the

00:18:36,000 --> 00:18:40,160
reader that doesn't just include data

00:18:38,160 --> 00:18:44,160
but includes things like hey how do we

00:18:40,160 --> 00:18:46,960
handle networking how do we handle dns

00:18:44,160 --> 00:18:48,480
things like that so that's really beyond

00:18:46,960 --> 00:18:49,679
the scope of this talk

00:18:48,480 --> 00:18:51,280
although it's something that people

00:18:49,679 --> 00:18:52,640
commonly do so what we'll do is we'll

00:18:51,280 --> 00:18:55,440
leave that here

00:18:52,640 --> 00:18:56,000
and let you work on that yourself

00:18:55,440 --> 00:18:57,440
meanwhile

00:18:56,000 --> 00:18:59,200
what i'm going to do is proceed to

00:18:57,440 --> 00:19:02,400
another kind of data loss

00:18:59,200 --> 00:19:03,919
which is directly related to kubernetes

00:19:02,400 --> 00:19:05,280
itself

00:19:03,919 --> 00:19:07,760
and that's what i call affinity

00:19:05,280 --> 00:19:10,880
afflictions so

00:19:07,760 --> 00:19:12,160
affinity rules are great and uh but one

00:19:10,880 --> 00:19:13,440
thing you should do is you should

00:19:12,160 --> 00:19:15,440
definitely check that they're really

00:19:13,440 --> 00:19:18,320
doing what you think they're doing

00:19:15,440 --> 00:19:19,520
here's an example so we have a zookeeper

00:19:18,320 --> 00:19:22,640
ensemble

00:19:19,520 --> 00:19:24,160
which i set up as an experiment and

00:19:22,640 --> 00:19:25,760
what i've run is a cube cuddle command

00:19:24,160 --> 00:19:27,440
to show where the pods are running and

00:19:25,760 --> 00:19:28,160
the first thing that i notice is even

00:19:27,440 --> 00:19:31,440
though this

00:19:28,160 --> 00:19:32,640
this uh this zookeeper was supposedly

00:19:31,440 --> 00:19:34,080
properly set up and

00:19:32,640 --> 00:19:36,160
and is something i expect to run in

00:19:34,080 --> 00:19:37,679
production i notice that two of the pods

00:19:36,160 --> 00:19:40,240
are running on the same host so that's a

00:19:37,679 --> 00:19:42,240
little bit spooky

00:19:40,240 --> 00:19:43,679
the other thing that i notice is if i

00:19:42,240 --> 00:19:46,720
want to go check

00:19:43,679 --> 00:19:49,679
where those hosts are in in which azs i

00:19:46,720 --> 00:19:53,120
can see that they're spread out across

00:19:49,679 --> 00:19:54,640
two az's so that's good what we can do

00:19:53,120 --> 00:19:55,600
is take these two things together and

00:19:54,640 --> 00:19:58,960
draw a picture

00:19:55,600 --> 00:20:02,000
of how the uh how the zookeepers

00:19:58,960 --> 00:20:03,919
are actually distributed and what we see

00:20:02,000 --> 00:20:07,120
is we've got two on one host

00:20:03,919 --> 00:20:09,200
one on another and the fact is that

00:20:07,120 --> 00:20:11,280
the the distribution here is essentially

00:20:09,200 --> 00:20:12,960
random it's sort of luck of the draw

00:20:11,280 --> 00:20:15,760
that we got two on one machine

00:20:12,960 --> 00:20:16,559
and and one on another the fact that

00:20:15,760 --> 00:20:18,799
they were just

00:20:16,559 --> 00:20:20,720
if they had come up as they could have

00:20:18,799 --> 00:20:22,799
come up all on different machines

00:20:20,720 --> 00:20:24,960
or we could have even have had all pods

00:20:22,799 --> 00:20:27,280
on on a single machine

00:20:24,960 --> 00:20:28,640
and what was actually happening here in

00:20:27,280 --> 00:20:31,600
this particular case

00:20:28,640 --> 00:20:32,400
is it just weren't any affinity rules um

00:20:31,600 --> 00:20:34,000
and this is

00:20:32,400 --> 00:20:35,440
so so one of the things that you do want

00:20:34,000 --> 00:20:36,960
to check for is that

00:20:35,440 --> 00:20:38,480
is that if you think you have affinity

00:20:36,960 --> 00:20:39,440
rules you want to make sure they really

00:20:38,480 --> 00:20:42,400
are there

00:20:39,440 --> 00:20:43,039
and in this particular example uh what i

00:20:42,400 --> 00:20:45,280
did was

00:20:43,039 --> 00:20:46,559
run a variation in the cube cuddle get

00:20:45,280 --> 00:20:49,120
pods command

00:20:46,559 --> 00:20:51,760
and this shortened version with jq

00:20:49,120 --> 00:20:52,960
allows me to pull out the affinity rules

00:20:51,760 --> 00:20:55,440
so that i can look at them

00:20:52,960 --> 00:20:56,240
quickly and what i can see here is that

00:20:55,440 --> 00:20:57,440
they are

00:20:56,240 --> 00:20:59,679
that they're null there's simply no

00:20:57,440 --> 00:21:00,559
rules at all so obviously affinity

00:20:59,679 --> 00:21:02,960
doesn't work

00:21:00,559 --> 00:21:04,320
if you don't use it but the other thing

00:21:02,960 --> 00:21:05,679
that that you need to do when you're

00:21:04,320 --> 00:21:08,080
checking these rules is to make sure

00:21:05,679 --> 00:21:10,000
they're really doing what you expect

00:21:08,080 --> 00:21:11,440
as i mentioned before the syntax for

00:21:10,000 --> 00:21:13,520
affinity rules is

00:21:11,440 --> 00:21:15,120
non-intuitive i think that's a kind of a

00:21:13,520 --> 00:21:17,520
kind word

00:21:15,120 --> 00:21:18,960
so it's pretty easy to to set things up

00:21:17,520 --> 00:21:20,880
that look correct

00:21:18,960 --> 00:21:22,720
but then actually behave surprisingly

00:21:20,880 --> 00:21:25,039
when they're deployed so again

00:21:22,720 --> 00:21:26,159
double checking the affinity rules and

00:21:25,039 --> 00:21:28,320
as well as the

00:21:26,159 --> 00:21:30,159
the actual implementation where where

00:21:28,320 --> 00:21:31,840
things where the pods actually land

00:21:30,159 --> 00:21:35,679
is super important for using them

00:21:31,840 --> 00:21:35,679
correctly and protecting your data

00:21:36,400 --> 00:21:40,159
let's move off affinity rules to another

00:21:38,640 --> 00:21:42,000
one of my favorites and this is a

00:21:40,159 --> 00:21:45,200
problem that i call the persistent

00:21:42,000 --> 00:21:47,919
volume that wasn't and uh this

00:21:45,200 --> 00:21:48,480
is surprisingly common i've probably hit

00:21:47,919 --> 00:21:50,240
this

00:21:48,480 --> 00:21:52,559
at least a dozen times in different

00:21:50,240 --> 00:21:55,200
forms let me talk about a couple of

00:21:52,559 --> 00:21:57,919
common ways that this can occur

00:21:55,200 --> 00:21:58,640
before we go anywhere it's important to

00:21:57,919 --> 00:22:01,679
recognize

00:21:58,640 --> 00:22:04,799
that that ins in kubernetes

00:22:01,679 --> 00:22:05,760
ephemeral storage is not a bad thing it

00:22:04,799 --> 00:22:08,080
is a feature

00:22:05,760 --> 00:22:09,919
not a bug that's in fact one of the

00:22:08,080 --> 00:22:11,360
things that makes kubernetes so powerful

00:22:09,919 --> 00:22:12,880
is you can have different kinds of

00:22:11,360 --> 00:22:14,320
storage and we're going to take

00:22:12,880 --> 00:22:17,679
advantage of that

00:22:14,320 --> 00:22:20,080
in in in just a couple of minutes

00:22:17,679 --> 00:22:21,840
but this is a picture of two kubernetes

00:22:20,080 --> 00:22:24,320
nodes and one i call the bad pod

00:22:21,840 --> 00:22:25,600
i has a bad pod running on it and it has

00:22:24,320 --> 00:22:27,919
no it has

00:22:25,600 --> 00:22:29,919
no block storage it's just writing to

00:22:27,919 --> 00:22:32,559
the ephemeral file system in the pod

00:22:29,919 --> 00:22:33,919
itself like overlay fs the other one is

00:22:32,559 --> 00:22:36,000
a good pod and that's actually got

00:22:33,919 --> 00:22:37,760
nicely allocated block storage

00:22:36,000 --> 00:22:39,679
now what's interesting is as far as the

00:22:37,760 --> 00:22:42,000
applications are concerned

00:22:39,679 --> 00:22:42,720
they can't tell the difference uh what

00:22:42,000 --> 00:22:44,640
kind of

00:22:42,720 --> 00:22:46,480
you know what kind of storage you have

00:22:44,640 --> 00:22:47,600
the pods work in both cases the answer

00:22:46,480 --> 00:22:49,840
api calls

00:22:47,600 --> 00:22:51,200
so on and so forth everything looks good

00:22:49,840 --> 00:22:53,200
what's even more interested

00:22:51,200 --> 00:22:54,720
is the pods themselves cannot

00:22:53,200 --> 00:22:56,640
necessarily tell

00:22:54,720 --> 00:22:58,559
whether they have truly persistent

00:22:56,640 --> 00:23:01,520
storage because they just see it as a

00:22:58,559 --> 00:23:02,960
mounted in the file system so in

00:23:01,520 --> 00:23:04,480
kubernetes

00:23:02,960 --> 00:23:05,760
to say something is bad and to say

00:23:04,480 --> 00:23:07,120
something is good well this is in the

00:23:05,760 --> 00:23:09,919
eye of the beholder

00:23:07,120 --> 00:23:10,400
if it's a web server of course it's it's

00:23:09,919 --> 00:23:12,320
uh

00:23:10,400 --> 00:23:14,080
it's fine we don't really care whether

00:23:12,320 --> 00:23:15,679
the storage is ephemeral or not

00:23:14,080 --> 00:23:17,440
if it's a database of course that could

00:23:15,679 --> 00:23:19,120
be quite a that could be quite a serious

00:23:17,440 --> 00:23:21,840
problem

00:23:19,120 --> 00:23:22,240
so one of the things that you want to do

00:23:21,840 --> 00:23:24,720
is

00:23:22,240 --> 00:23:26,320
particularly when you're dependent on

00:23:24,720 --> 00:23:27,360
assuming that persistent volumes have

00:23:26,320 --> 00:23:29,600
been allocated

00:23:27,360 --> 00:23:31,760
is to make sure they're really there so

00:23:29,600 --> 00:23:34,159
here's an example

00:23:31,760 --> 00:23:35,039
we can we run a cube cuddle get pvz

00:23:34,159 --> 00:23:39,840
command and i've

00:23:35,039 --> 00:23:39,840
i've changed the output a little bit to

00:23:41,200 --> 00:23:47,279
so that i can make sure that

00:23:45,520 --> 00:23:48,960
make sure that we really see what we

00:23:47,279 --> 00:23:50,960
expect and when we

00:23:48,960 --> 00:23:54,799
when we look at the pvc we see that it

00:23:50,960 --> 00:23:57,840
has a storage class of cops ssd 117

00:23:54,799 --> 00:23:59,679
that's the the storage class for cops

00:23:57,840 --> 00:24:03,279
running uh kubernetes

00:23:59,679 --> 00:24:05,120
uh uh 1.17 that looks pretty good

00:24:03,279 --> 00:24:07,200
moreover under volume we see there's a

00:24:05,120 --> 00:24:08,480
real pv out there so persistent volume

00:24:07,200 --> 00:24:11,279
has been allocated

00:24:08,480 --> 00:24:12,559
we could actually check that using a get

00:24:11,279 --> 00:24:14,799
pv command

00:24:12,559 --> 00:24:15,760
and and look at that another thing

00:24:14,799 --> 00:24:18,799
that's important

00:24:15,760 --> 00:24:20,080
is to look at the storage class itself

00:24:18,799 --> 00:24:22,480
and make sure that it's using a

00:24:20,080 --> 00:24:24,080
provisioner we expect so the provisioner

00:24:22,480 --> 00:24:26,400
is the thing that actually goes out and

00:24:24,080 --> 00:24:29,760
allocates the storage and again

00:24:26,400 --> 00:24:33,520
our our class here is um

00:24:29,760 --> 00:24:34,240
is cops ssd117 its provisioner is the oz

00:24:33,520 --> 00:24:36,000
ebs

00:24:34,240 --> 00:24:38,159
storage provisioner which goes out and

00:24:36,000 --> 00:24:40,320
allocates block storage

00:24:38,159 --> 00:24:41,840
so so far so good this actually sounds

00:24:40,320 --> 00:24:43,760
really good this is allocating block

00:24:41,840 --> 00:24:45,039
storage it should stick around when pods

00:24:43,760 --> 00:24:48,640
get restarted

00:24:45,039 --> 00:24:50,720
everything looks copacetic however

00:24:48,640 --> 00:24:52,720
that's not necessarily that's not to say

00:24:50,720 --> 00:24:55,360
that we're fully protected

00:24:52,720 --> 00:24:55,760
so because persistent volumes are only

00:24:55,360 --> 00:24:57,840
good

00:24:55,760 --> 00:25:00,000
if you actually use them here's an

00:24:57,840 --> 00:25:02,240
example of a bug we hit with zookeeper

00:25:00,000 --> 00:25:04,080
it fortunately did not lead to a down

00:25:02,240 --> 00:25:05,279
system because we detected it in time

00:25:04,080 --> 00:25:07,919
and were able to

00:25:05,279 --> 00:25:08,960
to correct it but what happened was due

00:25:07,919 --> 00:25:10,880
to

00:25:08,960 --> 00:25:12,400
a misconfiguration problem in our

00:25:10,880 --> 00:25:14,799
zookeepers

00:25:12,400 --> 00:25:15,600
we had the ebs storage mounted as var

00:25:14,799 --> 00:25:18,480
lib zk

00:25:15,600 --> 00:25:20,640
data but we accidentally set the

00:25:18,480 --> 00:25:22,640
configuration file so that zookeeper

00:25:20,640 --> 00:25:24,159
wrote its snapshots to data and it's

00:25:22,640 --> 00:25:27,120
logged to data log

00:25:24,159 --> 00:25:28,720
in ephemeral storage and the problem is

00:25:27,120 --> 00:25:29,279
you of course can't tell that this is

00:25:28,720 --> 00:25:31,440
happening

00:25:29,279 --> 00:25:32,720
until you actually restart the zookeeper

00:25:31,440 --> 00:25:35,120
node

00:25:32,720 --> 00:25:36,640
now with zookeeper there's another

00:25:35,120 --> 00:25:38,320
interesting variation that

00:25:36,640 --> 00:25:40,960
it's not even enough to restart

00:25:38,320 --> 00:25:44,159
zookeeper to see this because zookeeper

00:25:40,960 --> 00:25:45,919
replicates automatically so in this

00:25:44,159 --> 00:25:47,120
particular case we had an ensemble of

00:25:45,919 --> 00:25:49,760
three zookeepers

00:25:47,120 --> 00:25:50,559
when individual pods restarted they

00:25:49,760 --> 00:25:52,960
would then

00:25:50,559 --> 00:25:54,640
connect back to the other two pods get

00:25:52,960 --> 00:25:56,240
the data back and so it would be

00:25:54,640 --> 00:25:58,640
transparent that we had lost

00:25:56,240 --> 00:26:01,039
data the the zookeeper replication

00:25:58,640 --> 00:26:03,919
behavior was hiding this it was only

00:26:01,039 --> 00:26:05,600
at a certain point that we um that we

00:26:03,919 --> 00:26:07,520
actually accidentally noticed

00:26:05,600 --> 00:26:09,520
that the storage was not there and

00:26:07,520 --> 00:26:12,720
realized we had a real problem

00:26:09,520 --> 00:26:14,559
so so you really want to be

00:26:12,720 --> 00:26:17,039
very careful about this and this is an

00:26:14,559 --> 00:26:19,440
example of an area where the answer

00:26:17,039 --> 00:26:21,039
that you know to solving this kind of

00:26:19,440 --> 00:26:24,559
problem is testing

00:26:21,039 --> 00:26:26,559
and a lot of testing so for example just

00:26:24,559 --> 00:26:28,799
be super paranoid check that kubernetes

00:26:26,559 --> 00:26:31,039
resource definitions are out there as i

00:26:28,799 --> 00:26:33,120
as i showed before look at file system

00:26:31,039 --> 00:26:34,640
mounts uh make sure that the mounted

00:26:33,120 --> 00:26:38,320
file system

00:26:34,640 --> 00:26:39,760
matches where you exp is is that the

00:26:38,320 --> 00:26:41,120
you know that the pvs are mounted

00:26:39,760 --> 00:26:42,240
exactly where you expect them to and

00:26:41,120 --> 00:26:44,960
you're writing to them

00:26:42,240 --> 00:26:46,720
kill pods that's the best way of if if

00:26:44,960 --> 00:26:49,520
they're just single databases they

00:26:46,720 --> 00:26:51,039
if they lose their data then uh then

00:26:49,520 --> 00:26:52,880
you'll detect that you can kill nodes

00:26:51,039 --> 00:26:54,720
you can kill and restart all pods in a

00:26:52,880 --> 00:26:57,919
replicated database

00:26:54,720 --> 00:27:00,080
deleting volumes is a good thing so

00:26:57,919 --> 00:27:01,440
take the volume away if the database is

00:27:00,080 --> 00:27:02,880
truly dependent on it

00:27:01,440 --> 00:27:04,480
and it's pointing to it and writing to

00:27:02,880 --> 00:27:07,440
it the database should stop working and

00:27:04,480 --> 00:27:09,919
finally test with huge amounts of data

00:27:07,440 --> 00:27:11,679
many database problems are revealed only

00:27:09,919 --> 00:27:13,760
when you have a large amount of data

00:27:11,679 --> 00:27:15,440
in this particular case if you add a lot

00:27:13,760 --> 00:27:16,320
of data it will fill the local file

00:27:15,440 --> 00:27:17,840
system

00:27:16,320 --> 00:27:19,200
and then you'll quickly see that you've

00:27:17,840 --> 00:27:20,159
got a problem because you're not even

00:27:19,200 --> 00:27:23,600
writing to the

00:27:20,159 --> 00:27:23,600
to the to the block storage

00:27:23,919 --> 00:27:29,679
so with that we're

00:27:26,960 --> 00:27:31,360
we've shown four ways you can lose data

00:27:29,679 --> 00:27:33,200
here's the last one which i call fat

00:27:31,360 --> 00:27:35,200
fingers of fate

00:27:33,200 --> 00:27:36,799
and this is probably the saddest one

00:27:35,200 --> 00:27:39,039
because

00:27:36,799 --> 00:27:39,840
really if you think about it the best

00:27:39,039 --> 00:27:42,880
way

00:27:39,840 --> 00:27:45,039
to lose data is to do it yourself and

00:27:42,880 --> 00:27:47,919
the virtue of kubernetes

00:27:45,039 --> 00:27:49,120
of course that we can create things has

00:27:47,919 --> 00:27:52,559
this

00:27:49,120 --> 00:27:54,720
mirror image that we can or sort of

00:27:52,559 --> 00:27:56,559
as equal and opposite effect that we can

00:27:54,720 --> 00:27:58,880
take them away just as quickly as we can

00:27:56,559 --> 00:28:01,120
create them so this single line command

00:27:58,880 --> 00:28:03,840
will wipe out that mysql database that i

00:28:01,120 --> 00:28:07,039
created as an example at the beginning

00:28:03,840 --> 00:28:09,360
so that's it data's gone so

00:28:07,039 --> 00:28:11,520
one of the things that given how easy it

00:28:09,360 --> 00:28:13,279
is to destroy things accidentally one of

00:28:11,520 --> 00:28:15,200
the things that's worth thinking about

00:28:13,279 --> 00:28:16,559
is how to prevent is how to protect your

00:28:15,200 --> 00:28:19,679
persistent volumes

00:28:16,559 --> 00:28:20,640
so they don't just evaporate and it

00:28:19,679 --> 00:28:23,120
turns out

00:28:20,640 --> 00:28:24,720
that we can do this through what are

00:28:23,120 --> 00:28:27,840
called um

00:28:24,720 --> 00:28:29,840
reclaim policies so we we

00:28:27,840 --> 00:28:32,240
list the pvs here and i show this

00:28:29,840 --> 00:28:33,919
example you can see the policy is delete

00:28:32,240 --> 00:28:37,279
that means that when the

00:28:33,919 --> 00:28:39,440
the persistent volume claim that caused

00:28:37,279 --> 00:28:43,039
the storage to be created goes away

00:28:39,440 --> 00:28:45,200
the storage just vaporizes so that's bad

00:28:43,039 --> 00:28:46,080
what we can do if we have existing pvs

00:28:45,200 --> 00:28:49,120
is we can fix

00:28:46,080 --> 00:28:51,200
this by changing the reclaim policy to

00:28:49,120 --> 00:28:52,720
be retained and this patch command shows

00:28:51,200 --> 00:28:54,799
how to do that

00:28:52,720 --> 00:28:56,399
another possibility if we want to do

00:28:54,799 --> 00:28:59,600
this in a general way

00:28:56,399 --> 00:29:02,799
is to create a new storage class so

00:28:59,600 --> 00:29:04,320
the kubernetes is really flexible and

00:29:02,799 --> 00:29:06,480
this is a great example of where that

00:29:04,320 --> 00:29:08,159
flexibility really works for you

00:29:06,480 --> 00:29:10,480
i've created in this definition a

00:29:08,159 --> 00:29:11,279
storage class which is exactly the same

00:29:10,480 --> 00:29:13,520
as the normal

00:29:11,279 --> 00:29:14,960
cop storage class except that it by

00:29:13,520 --> 00:29:18,159
default uses retain

00:29:14,960 --> 00:29:19,520
as the as the reclaim policy so any any

00:29:18,159 --> 00:29:21,520
persistent volumes

00:29:19,520 --> 00:29:23,120
allocated with this will have retain as

00:29:21,520 --> 00:29:25,520
their policy

00:29:23,120 --> 00:29:26,240
at that point if i destroy my sql by

00:29:25,520 --> 00:29:28,880
accident

00:29:26,240 --> 00:29:30,720
i can reclaim the storage just by making

00:29:28,880 --> 00:29:33,760
a persistent volume claim

00:29:30,720 --> 00:29:35,279
that matches the uh the volume that

00:29:33,760 --> 00:29:36,320
we're trying to attach to in every

00:29:35,279 --> 00:29:38,480
respect

00:29:36,320 --> 00:29:40,720
um and this is something where you have

00:29:38,480 --> 00:29:42,960
to be quite careful about it so for

00:29:40,720 --> 00:29:44,799
example you must have the volume name

00:29:42,960 --> 00:29:46,159
stuck in your persistent volume

00:29:44,799 --> 00:29:47,760
otherwise it won't match and it'll just

00:29:46,159 --> 00:29:48,559
stay in a pending state but if you do

00:29:47,760 --> 00:29:51,279
this right

00:29:48,559 --> 00:29:53,200
you can recover storage quite nicely and

00:29:51,279 --> 00:29:54,240
in fact here's the process that you go

00:29:53,200 --> 00:29:56,559
through to reclaim

00:29:54,240 --> 00:29:58,320
is you've blown it away by accident just

00:29:56,559 --> 00:30:00,240
go in remove the claim ref

00:29:58,320 --> 00:30:01,919
from the pv that says hey it's it's now

00:30:00,240 --> 00:30:03,600
free to be reallocated

00:30:01,919 --> 00:30:05,760
go ahead and run that definition again

00:30:03,600 --> 00:30:08,000
to create a new persistent volume

00:30:05,760 --> 00:30:10,000
and then our persistent volume claim and

00:30:08,000 --> 00:30:11,840
then with helm it actually has a feature

00:30:10,000 --> 00:30:13,520
which allows it to reattach

00:30:11,840 --> 00:30:14,880
to existing storage and you get your

00:30:13,520 --> 00:30:18,320
database back

00:30:14,880 --> 00:30:21,600
and that solves the problem

00:30:18,320 --> 00:30:24,000
so with this we've covered

00:30:21,600 --> 00:30:26,399
five ways to lose data talked about

00:30:24,000 --> 00:30:28,399
solutions i just like to summarize

00:30:26,399 --> 00:30:30,159
by thinking not so much about the ways

00:30:28,399 --> 00:30:31,120
to lose data but about the things you

00:30:30,159 --> 00:30:34,960
want to do

00:30:31,120 --> 00:30:38,159
to solve those problems so the core

00:30:34,960 --> 00:30:40,399
of of solving problems with data loss in

00:30:38,159 --> 00:30:41,279
kubernetes is just to be exceedingly

00:30:40,399 --> 00:30:43,120
paranoid

00:30:41,279 --> 00:30:45,279
this is something that's true of data in

00:30:43,120 --> 00:30:47,200
general and then beyond that

00:30:45,279 --> 00:30:49,360
there are five specific things that you

00:30:47,200 --> 00:30:51,120
want to do first of all have replicas of

00:30:49,360 --> 00:30:53,760
data

00:30:51,120 --> 00:30:55,840
always extra copies are always good

00:30:53,760 --> 00:30:58,880
separate them by distance

00:30:55,840 --> 00:31:00,720
use affinity rules so that you can lean

00:30:58,880 --> 00:31:02,640
on the kubernetes capabilities that

00:31:00,720 --> 00:31:05,120
allow you to spread

00:31:02,640 --> 00:31:06,880
pods across uh you know sort of a

00:31:05,120 --> 00:31:07,840
process across availability zones this

00:31:06,880 --> 00:31:09,440
is a great feature

00:31:07,840 --> 00:31:12,000
and keep them from clustering on single

00:31:09,440 --> 00:31:13,039
nodes use reclaim policies to protect

00:31:12,000 --> 00:31:15,120
your storage

00:31:13,039 --> 00:31:16,320
and then finally just test the daylights

00:31:15,120 --> 00:31:18,080
out of everything

00:31:16,320 --> 00:31:19,679
if you're in data this is something you

00:31:18,080 --> 00:31:22,880
just becomes part of you that

00:31:19,679 --> 00:31:24,880
anytime anytime you have data just

00:31:22,880 --> 00:31:26,000
do all kinds of tests on it because this

00:31:24,880 --> 00:31:27,600
is the best way to

00:31:26,000 --> 00:31:29,440
define problems before they actually

00:31:27,600 --> 00:31:32,240
occur and then finally

00:31:29,440 --> 00:31:33,679
if you have operators available check

00:31:32,240 --> 00:31:35,360
them out and use them they're a

00:31:33,679 --> 00:31:39,279
wonderful feature of kubernetes

00:31:35,360 --> 00:31:42,559
and and very uh very useful

00:31:39,279 --> 00:31:44,240
so that's it um i hope you've enjoyed

00:31:42,559 --> 00:31:45,039
the talk it's been really fun putting

00:31:44,240 --> 00:31:47,600
this together

00:31:45,039 --> 00:31:48,159
and and uh and presenting it i just want

00:31:47,600 --> 00:31:51,200
to say

00:31:48,159 --> 00:31:52,960
as a final uh note we are hiring like

00:31:51,200 --> 00:31:55,039
any self-respecting startup if you like

00:31:52,960 --> 00:31:56,640
this talk or think you can do it better

00:31:55,039 --> 00:31:58,559
please give us a call we'd love to talk

00:31:56,640 --> 00:32:03,200
to you our own operator

00:31:58,559 --> 00:32:05,360
is is shown here in the github reference

00:32:03,200 --> 00:32:07,120
so check it out there's many other great

00:32:05,360 --> 00:32:09,760
operators percona has a great one that's

00:32:07,120 --> 00:32:11,440
good operators for zookeeper

00:32:09,760 --> 00:32:13,679
it's a it's an increasingly powerful

00:32:11,440 --> 00:32:14,000
paradigm and definitely something worth

00:32:13,679 --> 00:32:16,240
learning

00:32:14,000 --> 00:32:19,679
about thanks and i look forward to

00:32:16,240 --> 00:32:19,679

YouTube URL: https://www.youtube.com/watch?v=zW7gLyPln3w


