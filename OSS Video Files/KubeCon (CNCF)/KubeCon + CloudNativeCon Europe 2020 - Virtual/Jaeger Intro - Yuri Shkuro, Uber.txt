Title: Jaeger Intro - Yuri Shkuro, Uber
Publication date: 2020-08-28
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Jaeger Intro - Yuri Shkuro, Uber 

This session is an introduction to Jaeger and distributed tracing. We will do a demo of the current Jaeger features, talk about the roadmap, and finish with a Q&A. After this session the attendees should better understand how Jaeger fits in the observability space for cloud native applications. For more information on the project everybody is welcome to attend the Jaeger Deep Dive Session.

https://sched.co/Zewm
Captions: 
	00:00:00,160 --> 00:00:04,319
hello and welcome to uh project

00:00:02,480 --> 00:00:07,759
introduction for jaeger

00:00:04,319 --> 00:00:09,280
my name is yurish crowe um this month is

00:00:07,759 --> 00:00:11,519
the fifth anniversary of

00:00:09,280 --> 00:00:13,759
jaeger starting october uh from the

00:00:11,519 --> 00:00:17,520
first commute it's in august

00:00:13,759 --> 00:00:19,840
and so congrats to the project um

00:00:17,520 --> 00:00:21,680
as far as the agenda so i will start

00:00:19,840 --> 00:00:24,960
with a brief introduction of

00:00:21,680 --> 00:00:26,800
uh why tracing is an important um part

00:00:24,960 --> 00:00:30,000
of the observability for your

00:00:26,800 --> 00:00:32,160
systems i will do a short

00:00:30,000 --> 00:00:33,760
live demo of jaeger features we'll talk

00:00:32,160 --> 00:00:36,079
about jaeger architecture

00:00:33,760 --> 00:00:38,399
we'll talk about sampling and i want to

00:00:36,079 --> 00:00:40,160
finish with discussion about

00:00:38,399 --> 00:00:43,680
the relationship between jaeger and open

00:00:40,160 --> 00:00:46,480
telemetry and how these two projects are

00:00:43,680 --> 00:00:48,879
um to say a few words about myself i'm a

00:00:46,480 --> 00:00:52,480
software engineer at uber

00:00:48,879 --> 00:00:53,120
i started jaeger as an internal project

00:00:52,480 --> 00:00:56,559
and then

00:00:53,120 --> 00:00:58,960
we donated it to cloud native foundation

00:00:56,559 --> 00:01:00,559
i'm also a co-founder of open tracing

00:00:58,960 --> 00:01:01,840
and open telemetry projects which are

00:01:00,559 --> 00:01:04,559
the instrumentation

00:01:01,840 --> 00:01:06,240
uh projects for tracing and i also

00:01:04,559 --> 00:01:08,320
published a book last year

00:01:06,240 --> 00:01:10,400
about tracing and my experiences at uber

00:01:08,320 --> 00:01:11,200
as well as experiences with jaeger and

00:01:10,400 --> 00:01:12,720
how to

00:01:11,200 --> 00:01:14,400
deploy it and configure and things like

00:01:12,720 --> 00:01:18,400
that you can find

00:01:14,400 --> 00:01:19,040
uh my contact informations on on my

00:01:18,400 --> 00:01:22,159
website

00:01:19,040 --> 00:01:24,960
uh with a blog and another information

00:01:22,159 --> 00:01:26,479
so to start with with the search

00:01:24,960 --> 00:01:30,000
question of observability

00:01:26,479 --> 00:01:31,200
um why tracing is important in

00:01:30,000 --> 00:01:32,320
microservices

00:01:31,200 --> 00:01:34,240
especially in microservices

00:01:32,320 --> 00:01:37,280
architectures and

00:01:34,240 --> 00:01:39,520
why it becomes popular today so

00:01:37,280 --> 00:01:40,479
modern systems have to deal with a very

00:01:39,520 --> 00:01:43,119
large scale

00:01:40,479 --> 00:01:44,000
and the traditional wisdom is that we

00:01:43,119 --> 00:01:46,159
cannot scale

00:01:44,000 --> 00:01:47,119
up our servers because there's a limit

00:01:46,159 --> 00:01:49,840
to how much

00:01:47,119 --> 00:01:50,720
any given server can process and so

00:01:49,840 --> 00:01:53,119
usually people

00:01:50,720 --> 00:01:54,000
say okay we need to scale out and build

00:01:53,119 --> 00:01:55,840
architecture in that

00:01:54,000 --> 00:01:58,479
way but what does scale out actually

00:01:55,840 --> 00:02:00,320
mean the simplest uh

00:01:58,479 --> 00:02:01,920
approach to scaling out is that if you

00:02:00,320 --> 00:02:04,240
have a monolith service

00:02:01,920 --> 00:02:05,280
you can just replicate it multiple times

00:02:04,240 --> 00:02:07,520
in many hosts and

00:02:05,280 --> 00:02:09,520
put a load balance in front of it and if

00:02:07,520 --> 00:02:12,400
that's the whole of your architecture

00:02:09,520 --> 00:02:12,640
then you have a pretty uh clear picture

00:02:12,400 --> 00:02:14,480
of

00:02:12,640 --> 00:02:16,400
uh how to debug these things and what

00:02:14,480 --> 00:02:17,840
type durability story is it because you

00:02:16,400 --> 00:02:18,560
could probably get away with just

00:02:17,840 --> 00:02:20,319
metrics

00:02:18,560 --> 00:02:21,760
for every single instance tagged with

00:02:20,319 --> 00:02:23,520
the instance name and then you can

00:02:21,760 --> 00:02:26,400
investigate problems with that

00:02:23,520 --> 00:02:28,879
architecture but in reality this is not

00:02:26,400 --> 00:02:31,599
how the systems are built today

00:02:28,879 --> 00:02:32,879
and this is another type of scaling that

00:02:31,599 --> 00:02:35,519
we can talk about is it's

00:02:32,879 --> 00:02:36,800
scaling the systems in in depth uh

00:02:35,519 --> 00:02:38,400
meaning that

00:02:36,800 --> 00:02:40,319
we don't have a single monarchs we

00:02:38,400 --> 00:02:41,680
actually break monoliths into many many

00:02:40,319 --> 00:02:42,239
different pieces and many different

00:02:41,680 --> 00:02:44,480
layers

00:02:42,239 --> 00:02:45,680
and so any individual request that that

00:02:44,480 --> 00:02:48,080
hits the architecture

00:02:45,680 --> 00:02:49,120
has to go through all these many layers

00:02:48,080 --> 00:02:52,000
so modern

00:02:49,120 --> 00:02:53,440
architectures are really deep systems uh

00:02:52,000 --> 00:02:55,680
if you look at any of them

00:02:53,440 --> 00:02:58,319
and deep systems present unique

00:02:55,680 --> 00:03:00,560
challenges in terms of observability

00:02:58,319 --> 00:03:01,360
if we look at uh this picture which is

00:03:00,560 --> 00:03:05,040
just a

00:03:01,360 --> 00:03:08,239
screenshot of uh microservices layout

00:03:05,040 --> 00:03:09,200
two birds few years ago so imagine that

00:03:08,239 --> 00:03:11,280
there is a request

00:03:09,200 --> 00:03:12,959
coming from the mobile application that

00:03:11,280 --> 00:03:14,640
request might look like this when it

00:03:12,959 --> 00:03:16,400
hits this architecture right it

00:03:14,640 --> 00:03:19,120
it goes through a large number of

00:03:16,400 --> 00:03:22,319
services all participating together and

00:03:19,120 --> 00:03:23,599
inserting one single request and

00:03:22,319 --> 00:03:25,440
we can look at it in a different way

00:03:23,599 --> 00:03:27,440
this is the actual representation of the

00:03:25,440 --> 00:03:27,920
real production request from from uber

00:03:27,440 --> 00:03:30,720
right

00:03:27,920 --> 00:03:32,239
might be an application start and we can

00:03:30,720 --> 00:03:34,720
see here there's like over 100

00:03:32,239 --> 00:03:35,680
uh remote procedure calls within that

00:03:34,720 --> 00:03:38,799
execution

00:03:35,680 --> 00:03:40,159
uh 14 levels of depth so why is that a

00:03:38,799 --> 00:03:41,120
problem for observability well the

00:03:40,159 --> 00:03:44,560
problem is that

00:03:41,120 --> 00:03:48,319
now we have to answer questions uh

00:03:44,560 --> 00:03:50,959
about uh what's wrong with the system

00:03:48,319 --> 00:03:52,400
uh given so many different nodes in the

00:03:50,959 --> 00:03:53,040
system that where the things can go

00:03:52,400 --> 00:03:55,120
wrong right

00:03:53,040 --> 00:03:56,480
and how do we start with that we we

00:03:55,120 --> 00:03:58,319
approach it usually with

00:03:56,480 --> 00:04:00,319
oh we have some business metrics maybe

00:03:58,319 --> 00:04:03,280
the number of trips in a particular city

00:04:00,319 --> 00:04:04,640
and suddenly that metrics dipped and so

00:04:03,280 --> 00:04:05,840
we have an alert and we try to

00:04:04,640 --> 00:04:07,200
investigate but then

00:04:05,840 --> 00:04:09,120
then what where do you go from that

00:04:07,200 --> 00:04:09,920
business metric saying something is

00:04:09,120 --> 00:04:12,239
wrong to

00:04:09,920 --> 00:04:13,760
where in this massive architecture a

00:04:12,239 --> 00:04:15,040
particular change happened that is

00:04:13,760 --> 00:04:16,799
causing this problem

00:04:15,040 --> 00:04:18,479
and this is where tracing really comes

00:04:16,799 --> 00:04:21,600
in because tracing

00:04:18,479 --> 00:04:24,160
allows us to look at individual requests

00:04:21,600 --> 00:04:25,360
in full depth end to end and if there

00:04:24,160 --> 00:04:27,520
are errors happening they will be

00:04:25,360 --> 00:04:30,240
captured in the trace

00:04:27,520 --> 00:04:30,639
um and so the the conclusion here is

00:04:30,240 --> 00:04:32,720
that

00:04:30,639 --> 00:04:35,199
this is why tracing is important because

00:04:32,720 --> 00:04:37,919
uh in in the modern deep systems

00:04:35,199 --> 00:04:40,000
um sometimes it's much more difficult to

00:04:37,919 --> 00:04:42,080
find where the problem is given the how

00:04:40,000 --> 00:04:44,080
distributed the system is

00:04:42,080 --> 00:04:46,080
rather than what is specifically wrong

00:04:44,080 --> 00:04:48,080
because once you narrow down the problem

00:04:46,080 --> 00:04:49,759
there are a lot of variety of different

00:04:48,080 --> 00:04:51,280
tools that you can use to troubleshoot

00:04:49,759 --> 00:04:52,400
it within let's say a single instance of

00:04:51,280 --> 00:04:53,919
a service

00:04:52,400 --> 00:04:55,840
but even to get there is very

00:04:53,919 --> 00:04:57,280
challenging especially if you're

00:04:55,840 --> 00:04:58,880
starting from an alert on a business

00:04:57,280 --> 00:05:01,919
metric

00:04:58,880 --> 00:05:04,000
um i just want to give a brief kind of

00:05:01,919 --> 00:05:05,280
crash course into tracing if you don't

00:05:04,000 --> 00:05:07,520
know how that works

00:05:05,280 --> 00:05:09,440
uh the the tracing is built on the

00:05:07,520 --> 00:05:11,840
concept of a context propagation

00:05:09,440 --> 00:05:13,440
meaning that when the request comes to

00:05:11,840 --> 00:05:14,080
our architecture in this case let's say

00:05:13,440 --> 00:05:16,639
service a

00:05:14,080 --> 00:05:17,759
at the top is the kind of gateway

00:05:16,639 --> 00:05:20,560
service maybe

00:05:17,759 --> 00:05:22,479
or api service and as soon as the

00:05:20,560 --> 00:05:23,360
request comes in we assign a unique id

00:05:22,479 --> 00:05:25,600
to that request

00:05:23,360 --> 00:05:27,440
let's call it a trace id and we store it

00:05:25,600 --> 00:05:29,280
in a thing which we call a context or a

00:05:27,440 --> 00:05:32,000
metadata and that metadata

00:05:29,280 --> 00:05:33,520
is attached to every single other rpc

00:05:32,000 --> 00:05:35,680
call or request that

00:05:33,520 --> 00:05:37,360
the execution of this top level request

00:05:35,680 --> 00:05:39,520
involves right and so as

00:05:37,360 --> 00:05:40,720
service or a call service b we will pass

00:05:39,520 --> 00:05:43,039
that metadata in

00:05:40,720 --> 00:05:44,080
and so on and so on uh and what that

00:05:43,039 --> 00:05:45,840
does is basically

00:05:44,080 --> 00:05:49,039
that allows us to tag the a single

00:05:45,840 --> 00:05:51,360
execution across multiple services

00:05:49,039 --> 00:05:53,680
and then assemble this data on the back

00:05:51,360 --> 00:05:54,800
end into coherent representation of the

00:05:53,680 --> 00:05:56,639
of the execution and

00:05:54,800 --> 00:05:58,080
this is kind of a typical representation

00:05:56,639 --> 00:06:00,960
of a trace

00:05:58,080 --> 00:06:03,280
in the gantt chart where each service is

00:06:00,960 --> 00:06:05,840
represented by a certain span of time

00:06:03,280 --> 00:06:07,360
uh where like from start to end of the

00:06:05,840 --> 00:06:09,039
operation within that service

00:06:07,360 --> 00:06:10,560
and then there's a hierarchical

00:06:09,039 --> 00:06:13,199
relationship between uh

00:06:10,560 --> 00:06:15,199
between services um like a causality

00:06:13,199 --> 00:06:17,199
which is being tracked by this metadata

00:06:15,199 --> 00:06:19,680
being propagated

00:06:17,199 --> 00:06:21,919
now let's look at the demo of jaeger

00:06:19,680 --> 00:06:25,280
features so i have here

00:06:21,919 --> 00:06:27,440
um a clone of uh main jaeger repository

00:06:25,280 --> 00:06:30,319
from github yaga tracing slash jager

00:06:27,440 --> 00:06:32,240
and there is a directory for examples

00:06:30,319 --> 00:06:33,199
with a single sample application called

00:06:32,240 --> 00:06:35,520
hot rod

00:06:33,199 --> 00:06:38,960
so in that application we can see there

00:06:35,520 --> 00:06:38,960
is a docker compose file

00:06:40,160 --> 00:06:44,720
which loads uh so-called jaeger

00:06:42,800 --> 00:06:48,000
all-in-one this is a

00:06:44,720 --> 00:06:49,680
binary of single binary that combines

00:06:48,000 --> 00:06:51,759
all of the jaeger components in just one

00:06:49,680 --> 00:06:52,080
executable so you can kind of easily run

00:06:51,759 --> 00:06:54,479
it

00:06:52,080 --> 00:06:55,599
and then there is another uh docker

00:06:54,479 --> 00:06:59,199
image for the actual

00:06:55,599 --> 00:06:59,199
demo applications right so

00:07:01,440 --> 00:07:04,160
let's start it

00:07:05,520 --> 00:07:09,280
uh we can see in the logs uh there are

00:07:08,479 --> 00:07:11,360
two things so

00:07:09,280 --> 00:07:12,639
jaeger is the the main jaeger back end

00:07:11,360 --> 00:07:16,000
so it starts on

00:07:12,639 --> 00:07:17,759
on this uh port 16686 this is the query

00:07:16,000 --> 00:07:19,360
series which also shows the front end

00:07:17,759 --> 00:07:21,120
the ui and then the

00:07:19,360 --> 00:07:22,960
the gem application started a whole

00:07:21,120 --> 00:07:25,599
bunch of services uh

00:07:22,960 --> 00:07:27,120
the the one that's at 8080 is the one

00:07:25,599 --> 00:07:28,400
that we actually want to look at so

00:07:27,120 --> 00:07:31,120
let's go there

00:07:28,400 --> 00:07:33,039
so we can see this is a kind of a mock

00:07:31,120 --> 00:07:34,720
rides on demand application where you

00:07:33,039 --> 00:07:36,960
have customers and you can click a

00:07:34,720 --> 00:07:39,520
button and get a car right so let's

00:07:36,960 --> 00:07:41,440
order uh a few cars and we can see here

00:07:39,520 --> 00:07:44,800
there's like license plate

00:07:41,440 --> 00:07:45,919
eta of arrival then some tracking

00:07:44,800 --> 00:07:48,639
information for

00:07:45,919 --> 00:07:50,639
for debugging purposes uh and and the

00:07:48,639 --> 00:07:52,879
latency and you can see that the latency

00:07:50,639 --> 00:07:54,720
kind of the more requests we put in the

00:07:52,879 --> 00:07:56,960
the higher the latency becomes right

00:07:54,720 --> 00:07:58,240
so like if i if i do another request it

00:07:56,960 --> 00:08:01,039
will be short

00:07:58,240 --> 00:08:02,560
but if i send many of them then we'll

00:08:01,039 --> 00:08:03,199
see that the latency keeps climbing

00:08:02,560 --> 00:08:05,440
right

00:08:03,199 --> 00:08:06,400
let's look at how these requests look in

00:08:05,440 --> 00:08:09,919
jager

00:08:06,400 --> 00:08:12,479
so we'll go to localhost at this point

00:08:09,919 --> 00:08:14,560
that i mentioned previously

00:08:12,479 --> 00:08:16,800
and we can see here in the services drop

00:08:14,560 --> 00:08:19,440
down that

00:08:16,800 --> 00:08:20,080
jaeger already detected all of our

00:08:19,440 --> 00:08:22,319
services

00:08:20,080 --> 00:08:24,400
in that mock application it also

00:08:22,319 --> 00:08:26,400
detected itself because

00:08:24,400 --> 00:08:28,160
jaeger query itself is instrumented and

00:08:26,400 --> 00:08:30,879
so we can get traces from it

00:08:28,160 --> 00:08:32,719
um so but we're interested in in in in

00:08:30,879 --> 00:08:34,080
the hot rods uh application but before

00:08:32,719 --> 00:08:36,399
we jump into traces

00:08:34,080 --> 00:08:38,000
would it be nice if we knew what the

00:08:36,399 --> 00:08:39,680
architecture of that application is

00:08:38,000 --> 00:08:41,039
because all we've seen so far is the

00:08:39,680 --> 00:08:42,719
front-end

00:08:41,039 --> 00:08:44,080
and so jaeger has the system

00:08:42,719 --> 00:08:46,240
architecture tab for this

00:08:44,080 --> 00:08:47,200
there's two views one is not like useful

00:08:46,240 --> 00:08:50,000
for this case but

00:08:47,200 --> 00:08:52,240
the dag view is is is good uh because it

00:08:50,000 --> 00:08:53,040
gives us very clear picture of what all

00:08:52,240 --> 00:08:54,800
the services that

00:08:53,040 --> 00:08:56,959
are included in this application how

00:08:54,800 --> 00:08:57,920
they're connected uh this is a number of

00:08:56,959 --> 00:09:00,880
requests

00:08:57,920 --> 00:09:01,760
that were executed during my run of the

00:09:00,880 --> 00:09:04,560
demo

00:09:01,760 --> 00:09:06,160
let's go to see what what actual traces

00:09:04,560 --> 00:09:08,080
look like right so there's a few ways

00:09:06,160 --> 00:09:10,480
that you can search for traces

00:09:08,080 --> 00:09:11,360
one is we can just go and do a blind

00:09:10,480 --> 00:09:13,920
search

00:09:11,360 --> 00:09:16,000
once we selected the service we can just

00:09:13,920 --> 00:09:19,279
see and find all of them right

00:09:16,000 --> 00:09:20,480
the other option we can do search by

00:09:19,279 --> 00:09:21,760
specific values

00:09:20,480 --> 00:09:23,600
if they are captured in an

00:09:21,760 --> 00:09:26,560
instrumentation and in this case

00:09:23,600 --> 00:09:27,519
the instrumentation records these

00:09:26,560 --> 00:09:30,720
license plates

00:09:27,519 --> 00:09:33,040
as the tag driver on the span of the

00:09:30,720 --> 00:09:37,120
of the service and so we can put it in a

00:09:33,040 --> 00:09:39,360
search query

00:09:37,120 --> 00:09:40,800
we get just this one trace right and if

00:09:39,360 --> 00:09:42,720
we go in that trace

00:09:40,800 --> 00:09:44,240
uh so let's let's just go through that

00:09:42,720 --> 00:09:46,800
view which is kind of the main view

00:09:44,240 --> 00:09:48,320
of most racing systems uh at least for

00:09:46,800 --> 00:09:50,959
for a given trace right so

00:09:48,320 --> 00:09:51,680
here on we have a timeline so this is

00:09:50,959 --> 00:09:54,160
also known as

00:09:51,680 --> 00:09:56,000
a gantt chart view um and at the top we

00:09:54,160 --> 00:09:56,800
have uh what we call a mini map it's

00:09:56,000 --> 00:09:58,880
kind of the

00:09:56,800 --> 00:10:00,720
small presentation of the trace it's

00:09:58,880 --> 00:10:02,000
useful when you need to navigate very

00:10:00,720 --> 00:10:05,440
large traces because

00:10:02,000 --> 00:10:06,560
you can zoom in here and uh jump into

00:10:05,440 --> 00:10:08,640
various places

00:10:06,560 --> 00:10:10,079
but in this case uh almost all of the

00:10:08,640 --> 00:10:11,360
trays well actually half of it still

00:10:10,079 --> 00:10:13,680
fits only on the screen

00:10:11,360 --> 00:10:14,480
so uh the mini map might be useful as

00:10:13,680 --> 00:10:17,120
well

00:10:14,480 --> 00:10:18,160
then on the left we have um all these

00:10:17,120 --> 00:10:20,240
services uh

00:10:18,160 --> 00:10:21,519
laid out in the hierarchical manner

00:10:20,240 --> 00:10:23,680
showing what

00:10:21,519 --> 00:10:24,800
uh how how which service called which

00:10:23,680 --> 00:10:28,399
are the service rights

00:10:24,800 --> 00:10:31,440
um so we can if we collapse everything

00:10:28,399 --> 00:10:34,320
and then open only uh one level

00:10:31,440 --> 00:10:34,720
we will see that uh actually one more

00:10:34,320 --> 00:10:36,399
level

00:10:34,720 --> 00:10:38,000
we can see here that the front-end

00:10:36,399 --> 00:10:40,000
service called the customer then it's

00:10:38,000 --> 00:10:42,560
called the root service multiple times

00:10:40,000 --> 00:10:43,200
uh and also called the driver service

00:10:42,560 --> 00:10:45,920
here

00:10:43,200 --> 00:10:47,680
right so which is what what we've seen

00:10:45,920 --> 00:10:50,720
in the in architecture diagram

00:10:47,680 --> 00:10:52,320
um and uh and

00:10:50,720 --> 00:10:54,000
basically every every bar here

00:10:52,320 --> 00:10:54,640
represents a single span within the

00:10:54,000 --> 00:10:57,680
service

00:10:54,640 --> 00:10:59,680
uh you can see that let's say if we pick

00:10:57,680 --> 00:11:00,480
a front end there is more than one span

00:10:59,680 --> 00:11:02,079
here even though

00:11:00,480 --> 00:11:03,839
logically there's only one operation

00:11:02,079 --> 00:11:05,120
because uh the way the tracing

00:11:03,839 --> 00:11:07,600
instrumentation is done

00:11:05,120 --> 00:11:09,519
it creates a span not only for every

00:11:07,600 --> 00:11:10,720
incoming request but also for every

00:11:09,519 --> 00:11:14,399
outgoing request

00:11:10,720 --> 00:11:16,160
so this http get dispatched

00:11:14,399 --> 00:11:18,000
that was the entry point into the

00:11:16,160 --> 00:11:18,720
front-end service but then when it made

00:11:18,000 --> 00:11:21,360
a call

00:11:18,720 --> 00:11:21,920
another http call to the to the customer

00:11:21,360 --> 00:11:23,760
service

00:11:21,920 --> 00:11:25,680
there was another child spend so-called

00:11:23,760 --> 00:11:28,000
client spam created there

00:11:25,680 --> 00:11:29,760
and if we click on any span we can get

00:11:28,000 --> 00:11:32,320
uh additional details so

00:11:29,760 --> 00:11:34,480
let's look at the at the root span so

00:11:32,320 --> 00:11:37,279
the one thing that we have in spans are

00:11:34,480 --> 00:11:39,120
things called tags tags are just some uh

00:11:37,279 --> 00:11:41,120
metadata that you can attach to the

00:11:39,120 --> 00:11:43,279
spans and it gets recorded and sent

00:11:41,120 --> 00:11:45,040
in in the background to the yeah tracing

00:11:43,279 --> 00:11:47,760
back and so we can see there is

00:11:45,040 --> 00:11:48,720
useful things specifically like the the

00:11:47,760 --> 00:11:50,399
exact url

00:11:48,720 --> 00:11:52,560
uh you don't want to put the url in the

00:11:50,399 --> 00:11:54,000
span name because that will make it very

00:11:52,560 --> 00:11:54,959
high cardinality and difficult to

00:11:54,000 --> 00:11:56,639
aggregate

00:11:54,959 --> 00:11:58,000
but in the tags you can put any kind of

00:11:56,639 --> 00:12:01,120
cardinality and

00:11:58,000 --> 00:12:03,839
this is just i think nonsense for the ui

00:12:01,120 --> 00:12:04,720
cache investing and then another

00:12:03,839 --> 00:12:08,079
interesting thing

00:12:04,720 --> 00:12:09,040
here is a span kind which says this is a

00:12:08,079 --> 00:12:11,040
server span

00:12:09,040 --> 00:12:13,519
right so if we look at the next one as i

00:12:11,040 --> 00:12:16,560
said this is an outgoing outgoing call

00:12:13,519 --> 00:12:20,399
uh or sorry this one doesn't have this

00:12:16,560 --> 00:12:20,880
this one it will have a span kind of

00:12:20,399 --> 00:12:23,920
client

00:12:20,880 --> 00:12:26,000
so which says this is the uh the call

00:12:23,920 --> 00:12:26,639
i'm making out of the process to to the

00:12:26,000 --> 00:12:28,560
next

00:12:26,639 --> 00:12:31,279
server and we can see that the url is

00:12:28,560 --> 00:12:34,079
it's hitting the customer service

00:12:31,279 --> 00:12:34,720
going back to the top span um another

00:12:34,079 --> 00:12:38,160
thing that

00:12:34,720 --> 00:12:42,000
we can see here is that

00:12:38,160 --> 00:12:45,760
the locks section right so uh

00:12:42,000 --> 00:12:46,560
in in the output here we can see that

00:12:45,760 --> 00:12:48,480
there's like

00:12:46,560 --> 00:12:50,399
as i was executing this request there is

00:12:48,480 --> 00:12:52,480
a bunch of logs written to the standard

00:12:50,399 --> 00:12:55,040
out right and this is normal

00:12:52,480 --> 00:12:57,680
you can pipe them into some central uh

00:12:55,040 --> 00:13:00,000
look aggregator aggregator

00:12:57,680 --> 00:13:01,120
but they're kind of very difficult to

00:13:00,000 --> 00:13:02,560
use because again

00:13:01,120 --> 00:13:04,480
if if there are multiple concurrent

00:13:02,560 --> 00:13:06,399
requests you get a single jumble of

00:13:04,480 --> 00:13:07,519
of these log lines mixed together

00:13:06,399 --> 00:13:09,519
whereas here

00:13:07,519 --> 00:13:11,279
these logs belong just to a single

00:13:09,519 --> 00:13:12,880
request right and this the way they get

00:13:11,279 --> 00:13:14,480
into the trace is that

00:13:12,880 --> 00:13:16,560
if you look at the implementation of hot

00:13:14,480 --> 00:13:19,200
rod application it has a special

00:13:16,560 --> 00:13:20,959
wrapper for the logger so that logger

00:13:19,200 --> 00:13:22,320
not only logs to send it out but also

00:13:20,959 --> 00:13:24,800
writes to the current span

00:13:22,320 --> 00:13:26,160
uh and that allows you to kind of get

00:13:24,800 --> 00:13:29,360
the exact same information

00:13:26,160 --> 00:13:30,880
uh but uh like filtered out from the

00:13:29,360 --> 00:13:32,560
from the rest of the noise that you can

00:13:30,880 --> 00:13:34,160
see in the logs so this is a very

00:13:32,560 --> 00:13:37,120
powerful feature of tracing that

00:13:34,160 --> 00:13:38,240
allows you to investigate problems um

00:13:37,120 --> 00:13:41,040
within like very

00:13:38,240 --> 00:13:42,240
specific uh execution of your of your

00:13:41,040 --> 00:13:44,639
total trace

00:13:42,240 --> 00:13:45,440
now let's uh expand everything again and

00:13:44,639 --> 00:13:48,160
look at uh

00:13:45,440 --> 00:13:48,639
what what the trace can also tell us uh

00:13:48,160 --> 00:13:50,399
so

00:13:48,639 --> 00:13:53,040
one thing that's immediately obvious is

00:13:50,399 --> 00:13:55,279
that there is this mysql select

00:13:53,040 --> 00:13:57,199
which takes uh a majority like two

00:13:55,279 --> 00:13:59,120
thirds of the total time of this request

00:13:57,199 --> 00:14:00,959
right so if we were

00:13:59,120 --> 00:14:03,040
investigating the latency problem then

00:14:00,959 --> 00:14:03,600
this is clearly the path where we would

00:14:03,040 --> 00:14:06,639
look

00:14:03,600 --> 00:14:07,440
for for why this this particular places

00:14:06,639 --> 00:14:09,120
is very slow

00:14:07,440 --> 00:14:10,959
it also has some interesting logs i'm

00:14:09,120 --> 00:14:12,000
not going to go into into this much too

00:14:10,959 --> 00:14:13,360
much but it says

00:14:12,000 --> 00:14:15,199
waiting for log behind three

00:14:13,360 --> 00:14:16,480
transactions so it's probably not the

00:14:15,199 --> 00:14:19,680
actual query that was

00:14:16,480 --> 00:14:20,720
uh taking so long but the the operation

00:14:19,680 --> 00:14:22,480
was simply stuck

00:14:20,720 --> 00:14:24,160
in india service right so there might be

00:14:22,480 --> 00:14:26,959
some resource pool contention

00:14:24,160 --> 00:14:28,880
here um the the other thing we can see

00:14:26,959 --> 00:14:31,839
in the trace is that uh these multiple

00:14:28,880 --> 00:14:33,600
calls from the driver to radius service

00:14:31,839 --> 00:14:34,959
some of them failed as as can be

00:14:33,600 --> 00:14:37,920
indicated by this

00:14:34,959 --> 00:14:39,920
red exclamation point and the way that

00:14:37,920 --> 00:14:41,920
that is detected is that if you look at

00:14:39,920 --> 00:14:42,720
the tags you will see error equals true

00:14:41,920 --> 00:14:44,560
right so again

00:14:42,720 --> 00:14:46,000
this is uh instrumentation setting that

00:14:44,560 --> 00:14:47,839
up you can always do it for your own

00:14:46,000 --> 00:14:50,800
spans or your own operations

00:14:47,839 --> 00:14:52,320
uh how you see fit uh jager simply just

00:14:50,800 --> 00:14:54,560
displays this

00:14:52,320 --> 00:14:55,680
these errors have the ability to a

00:14:54,560 --> 00:14:58,720
bubble up the

00:14:55,680 --> 00:15:00,480
tree so if i start uh collapsing

00:14:58,720 --> 00:15:02,320
uh things and we can see here that the

00:15:00,480 --> 00:15:04,000
driver is shown with an error even

00:15:02,320 --> 00:15:04,800
though there was no specific error in

00:15:04,000 --> 00:15:06,800
the driver

00:15:04,800 --> 00:15:07,920
uh but then if you expand it then it's

00:15:06,800 --> 00:15:09,600
actually below right

00:15:07,920 --> 00:15:11,199
so sometimes it's useful when you're

00:15:09,600 --> 00:15:12,720
looking at the very top of the trace to

00:15:11,199 --> 00:15:13,920
know if there are any errors and then

00:15:12,720 --> 00:15:16,560
you can drill down into

00:15:13,920 --> 00:15:17,199
where the exactly they happened and

00:15:16,560 --> 00:15:19,680
finally

00:15:17,199 --> 00:15:20,880
uh another thing that we can easily tell

00:15:19,680 --> 00:15:21,920
from the trades in terms of like

00:15:20,880 --> 00:15:22,880
analyzing performance of the

00:15:21,920 --> 00:15:24,959
applications that

00:15:22,880 --> 00:15:26,560
these calls to radius seem to be

00:15:24,959 --> 00:15:28,320
happening all in uh

00:15:26,560 --> 00:15:30,079
in a sort of a staircase pattern right

00:15:28,320 --> 00:15:31,759
one after another so that would clearly

00:15:30,079 --> 00:15:34,399
be an indication of something that

00:15:31,759 --> 00:15:36,399
potentially can be improved uh because

00:15:34,399 --> 00:15:37,600
if we know the the business logic of the

00:15:36,399 --> 00:15:39,199
application here

00:15:37,600 --> 00:15:40,560
uh it looks like it's getting like

00:15:39,199 --> 00:15:41,440
driver informations for different

00:15:40,560 --> 00:15:43,360
drivers and so

00:15:41,440 --> 00:15:45,120
why couldn't the task radius for that in

00:15:43,360 --> 00:15:46,639
parallel or in a single bulk request

00:15:45,120 --> 00:15:48,560
right so that would have saved

00:15:46,639 --> 00:15:50,160
uh this this much time basically by

00:15:48,560 --> 00:15:52,480
doing this all in one

00:15:50,160 --> 00:15:54,320
and finally the last segment where the

00:15:52,480 --> 00:15:56,480
front application calls into the root

00:15:54,320 --> 00:15:58,079
service asking what is the closest route

00:15:56,480 --> 00:15:59,360
for the driver to get to us so that we

00:15:58,079 --> 00:16:02,480
can compute the ata

00:15:59,360 --> 00:16:04,480
then we can see here that uh this

00:16:02,480 --> 00:16:06,399
execution is not as bad because there

00:16:04,480 --> 00:16:08,000
are some concurrent requests going on

00:16:06,399 --> 00:16:09,519
but there is no more than three

00:16:08,000 --> 00:16:10,160
concurrent requests for some reason

00:16:09,519 --> 00:16:13,199
right so

00:16:10,160 --> 00:16:15,680
this stop uh one two three and then

00:16:13,199 --> 00:16:16,880
once as soon as one star stops then the

00:16:15,680 --> 00:16:18,720
other one starts so

00:16:16,880 --> 00:16:20,399
this is again an indication of some sort

00:16:18,720 --> 00:16:22,000
of resourceful contention this is like

00:16:20,399 --> 00:16:23,680
an executor pool uh

00:16:22,000 --> 00:16:25,279
that's uh limited by three and you

00:16:23,680 --> 00:16:27,279
cannot execute more than that and so

00:16:25,279 --> 00:16:28,959
if you run multiple concurrent requests

00:16:27,279 --> 00:16:29,680
then they're all gonna be blocking on

00:16:28,959 --> 00:16:31,440
this thing

00:16:29,680 --> 00:16:33,199
but in this case we don't see it because

00:16:31,440 --> 00:16:34,639
the contention the main contention is

00:16:33,199 --> 00:16:37,680
really the mysql

00:16:34,639 --> 00:16:39,279
uh span if you if i go back to search

00:16:37,680 --> 00:16:39,680
screen the one thing i didn't talk about

00:16:39,279 --> 00:16:42,160
it is

00:16:39,680 --> 00:16:42,800
uh is uh kind of what what you get on

00:16:42,160 --> 00:16:45,920
the search screen

00:16:42,800 --> 00:16:48,000
so um we you can also search for traces

00:16:45,920 --> 00:16:49,600
uh by other attributes specifically by

00:16:48,000 --> 00:16:50,000
duration right which could be very

00:16:49,600 --> 00:16:52,000
useful

00:16:50,000 --> 00:16:53,519
because if you are capturing a lot of

00:16:52,000 --> 00:16:54,560
traces in the system some of them are

00:16:53,519 --> 00:16:56,480
maybe

00:16:54,560 --> 00:16:58,000
very short and quick and you are not

00:16:56,480 --> 00:16:59,920
really interested in investigating them

00:16:58,000 --> 00:17:00,880
you really want to look at what your p99

00:16:59,920 --> 00:17:02,720
latency

00:17:00,880 --> 00:17:04,319
traces and presumably you have some

00:17:02,720 --> 00:17:05,120
metric which says oh your p90 lite

00:17:04,319 --> 00:17:06,720
entity is like

00:17:05,120 --> 00:17:08,880
two seconds right and you can put a

00:17:06,720 --> 00:17:09,919
query saying can you show me traces

00:17:08,880 --> 00:17:12,480
which are longer than

00:17:09,919 --> 00:17:13,760
1.5 seconds right and so then boom we

00:17:12,480 --> 00:17:16,959
only have one trace here

00:17:13,760 --> 00:17:20,319
in this case if i make it maybe like

00:17:16,959 --> 00:17:22,959
one second i get a few more right um

00:17:20,319 --> 00:17:25,120
so uh that allows you to narrow down the

00:17:22,959 --> 00:17:27,280
search and then investigate uh the

00:17:25,120 --> 00:17:28,400
what the differences are uh in in this

00:17:27,280 --> 00:17:31,120
traces but

00:17:28,400 --> 00:17:32,080
sometimes um looking at one single trace

00:17:31,120 --> 00:17:34,559
doesn't necessarily

00:17:32,080 --> 00:17:35,360
reveal all the problems that might be

00:17:34,559 --> 00:17:37,919
happening

00:17:35,360 --> 00:17:39,200
in terms of performance and uh typically

00:17:37,919 --> 00:17:41,280
when you use normal

00:17:39,200 --> 00:17:42,640
uh performance profiling tools like

00:17:41,280 --> 00:17:44,160
memory allocations

00:17:42,640 --> 00:17:45,200
you would take a snapshot before and

00:17:44,160 --> 00:17:46,080
after and then you compare those

00:17:45,200 --> 00:17:47,840
snapshots right

00:17:46,080 --> 00:17:49,919
and so jaeger also has this ability to

00:17:47,840 --> 00:17:50,640
do the comparison uh you can select two

00:17:49,919 --> 00:17:53,039
traces

00:17:50,640 --> 00:17:54,960
uh and then click compare and what it

00:17:53,039 --> 00:17:56,880
does is that combines them into into the

00:17:54,960 --> 00:17:58,880
graph representation and uses green and

00:17:56,880 --> 00:18:01,440
red color coding to indicate where

00:17:58,880 --> 00:18:03,200
there are missing or extra nodes in this

00:18:01,440 --> 00:18:05,760
case because there's only one

00:18:03,200 --> 00:18:06,880
uh extra node so the the picture is not

00:18:05,760 --> 00:18:09,520
that interesting but

00:18:06,880 --> 00:18:11,120
if we had a very large trace then that

00:18:09,520 --> 00:18:13,520
picture got a lot more interesting and i

00:18:11,120 --> 00:18:16,640
can show you later in the slides

00:18:13,520 --> 00:18:18,559
example from real production traces and

00:18:16,640 --> 00:18:22,640
finally the last piece i want to show

00:18:18,559 --> 00:18:23,520
here is we've looked at the system

00:18:22,640 --> 00:18:26,799
architecture

00:18:23,520 --> 00:18:28,880
before this one right

00:18:26,799 --> 00:18:30,480
there is another view that jaeger has

00:18:28,880 --> 00:18:32,080
which is called deep dependency graph

00:18:30,480 --> 00:18:35,120
it's built from the search results

00:18:32,080 --> 00:18:36,240
let me remove this uh duration so that i

00:18:35,120 --> 00:18:39,200
can get more traces

00:18:36,240 --> 00:18:40,000
so 12 traces uh and then deep dependency

00:18:39,200 --> 00:18:43,200
graph

00:18:40,000 --> 00:18:44,080
builds uh uh an execution um like a

00:18:43,200 --> 00:18:46,559
rigid

00:18:44,080 --> 00:18:48,080
representation of that of those traces

00:18:46,559 --> 00:18:49,200
however it looks very similar to the

00:18:48,080 --> 00:18:51,039
system architecture

00:18:49,200 --> 00:18:53,600
but there is a significant difference in

00:18:51,039 --> 00:18:56,160
that this this graph is transitive so

00:18:53,600 --> 00:18:57,520
when you see an error going through the

00:18:56,160 --> 00:19:00,799
like front-end service

00:18:57,520 --> 00:19:01,280
to driver and then to radius we know

00:19:00,799 --> 00:19:03,600
that

00:19:01,280 --> 00:19:05,280
there is actually a path in some of the

00:19:03,600 --> 00:19:06,080
traces that goes through these two

00:19:05,280 --> 00:19:08,799
services

00:19:06,080 --> 00:19:10,320
whereas in the system architecture

00:19:08,799 --> 00:19:13,520
that's not guaranteed so

00:19:10,320 --> 00:19:15,120
um just because there is a kind of an

00:19:13,520 --> 00:19:16,799
error from front end to customer doesn't

00:19:15,120 --> 00:19:18,640
mean that there is any request from

00:19:16,799 --> 00:19:18,960
front end which will reach the mysql

00:19:18,640 --> 00:19:21,120
right

00:19:18,960 --> 00:19:22,320
because this graph is based on just the

00:19:21,120 --> 00:19:24,320
pairwise connections

00:19:22,320 --> 00:19:25,520
between services and so why is that

00:19:24,320 --> 00:19:28,480
important is because

00:19:25,520 --> 00:19:30,720
uh sometimes sorry you need to search

00:19:28,480 --> 00:19:30,720
again

00:19:31,039 --> 00:19:34,480
when you're investigating like

00:19:32,320 --> 00:19:36,240
dependencies of the services you

00:19:34,480 --> 00:19:38,000
not only want to see what are my

00:19:36,240 --> 00:19:39,600
immediate dependencies under on the

00:19:38,000 --> 00:19:41,520
front-end service but you want to

00:19:39,600 --> 00:19:43,520
like look for deeper dependencies but

00:19:41,520 --> 00:19:45,360
only those which actually matter because

00:19:43,520 --> 00:19:46,799
if there is a dependency of root service

00:19:45,360 --> 00:19:47,440
let's say it's doing some background

00:19:46,799 --> 00:19:49,840
caching

00:19:47,440 --> 00:19:51,280
and hit some some storage here which

00:19:49,840 --> 00:19:52,640
never affects any of the front-end

00:19:51,280 --> 00:19:53,840
requests then we don't show it here

00:19:52,640 --> 00:19:54,480
because it does not come up in the

00:19:53,840 --> 00:19:56,480
traces

00:19:54,480 --> 00:19:58,480
right and another feature of this graph

00:19:56,480 --> 00:20:00,640
is that uh it's it is not going to show

00:19:58,480 --> 00:20:02,400
you the whole architecture but only pass

00:20:00,640 --> 00:20:05,200
going through the focal servers which is

00:20:02,400 --> 00:20:05,600
why it's in pink here so we can refocus

00:20:05,200 --> 00:20:07,440
this

00:20:05,600 --> 00:20:08,799
view to another service and then it will

00:20:07,440 --> 00:20:10,240
become even smaller

00:20:08,799 --> 00:20:12,880
uh for example in drive right because

00:20:10,240 --> 00:20:14,559
there's no paths that go through driver

00:20:12,880 --> 00:20:16,480
and also through the customer service so

00:20:14,559 --> 00:20:20,000
they're not going to be shown here

00:20:16,480 --> 00:20:23,120
and finally this graph is also uh

00:20:20,000 --> 00:20:25,440
can show you uh operation level

00:20:23,120 --> 00:20:27,039
view not just a service level view right

00:20:25,440 --> 00:20:27,600
and so we can see here that there are

00:20:27,039 --> 00:20:29,200
actually

00:20:27,600 --> 00:20:31,280
two different operations from the regis

00:20:29,200 --> 00:20:33,200
that are being called uh and so they can

00:20:31,280 --> 00:20:33,679
you can switch to this in the layout

00:20:33,200 --> 00:20:35,600
view

00:20:33,679 --> 00:20:37,039
you can you can change the graph to to

00:20:35,600 --> 00:20:39,520
show you different information

00:20:37,039 --> 00:20:41,120
um so this graph we we built it as a

00:20:39,520 --> 00:20:43,280
sort of a platform to

00:20:41,120 --> 00:20:45,280
to do more stuff with it specifically

00:20:43,280 --> 00:20:47,120
overlay and real-time information like

00:20:45,280 --> 00:20:49,200
what's your current latency or

00:20:47,120 --> 00:20:50,159
error rates on this graph right now it's

00:20:49,200 --> 00:20:51,679
not hooked up

00:20:50,159 --> 00:20:54,240
in any way but this is kind of the

00:20:51,679 --> 00:20:57,600
future direction that we're going with

00:20:54,240 --> 00:21:00,559
so i think this uh this is uh

00:20:57,600 --> 00:21:02,559
all i wanted to show you in the demo so

00:21:00,559 --> 00:21:05,840
and then i will switch to my

00:21:02,559 --> 00:21:09,919
slide deck um and

00:21:05,840 --> 00:21:12,559
i'll start with this sorry

00:21:09,919 --> 00:21:14,240
with this uh view that i mentioned over

00:21:12,559 --> 00:21:16,799
production trace where you compare it to

00:21:14,240 --> 00:21:20,240
traces and we can see here that because

00:21:16,799 --> 00:21:20,799
um this this these two traces are much

00:21:20,240 --> 00:21:22,960
larger

00:21:20,799 --> 00:21:24,559
uh there also they have a lot more

00:21:22,960 --> 00:21:26,799
interesting differences right and so

00:21:24,559 --> 00:21:28,480
at the red at the bottom shows that a

00:21:26,799 --> 00:21:30,480
whole number of calls

00:21:28,480 --> 00:21:31,760
uh did not happen in the in the round

00:21:30,480 --> 00:21:34,000
right hand side trace

00:21:31,760 --> 00:21:36,320
uh and so for example if we look at the

00:21:34,000 --> 00:21:36,799
duration the left one was 2.7 seconds

00:21:36,320 --> 00:21:39,200
the

00:21:36,799 --> 00:21:40,720
right one is 1.4 and so well because

00:21:39,200 --> 00:21:42,320
this whole section is missing that kind

00:21:40,720 --> 00:21:44,159
of explains why it was faster

00:21:42,320 --> 00:21:46,720
but it also potentially there is an

00:21:44,159 --> 00:21:48,720
error there right and so

00:21:46,720 --> 00:21:50,720
yeah i will skip this detail there's

00:21:48,720 --> 00:21:52,720
another way to to look at this

00:21:50,720 --> 00:21:54,960
uh comparison of the traces so this

00:21:52,720 --> 00:21:57,360
comparison is currently structural so

00:21:54,960 --> 00:21:59,120
we're just comparing do the nodes exist

00:21:57,360 --> 00:22:01,919
in one versus another graph right

00:21:59,120 --> 00:22:02,240
but uh we also may look sometimes at the

00:22:01,919 --> 00:22:04,240
uh

00:22:02,240 --> 00:22:05,520
the latency differences within each

00:22:04,240 --> 00:22:07,919
individual span right

00:22:05,520 --> 00:22:08,640
um and so the latency difference is is

00:22:07,919 --> 00:22:10,799
kind of uh

00:22:08,640 --> 00:22:12,720
gives you a very different picture uh

00:22:10,799 --> 00:22:15,760
but also

00:22:12,720 --> 00:22:17,360
drives your um sight

00:22:15,760 --> 00:22:19,200
immediately to the problems where you

00:22:17,360 --> 00:22:20,880
want to investigate right so here we can

00:22:19,200 --> 00:22:21,600
see we're using like a heat map color

00:22:20,880 --> 00:22:24,080
coding

00:22:21,600 --> 00:22:26,240
for for like the darker red the darker

00:22:24,080 --> 00:22:27,760
the red the the more latency differences

00:22:26,240 --> 00:22:30,000
in is within these notes

00:22:27,760 --> 00:22:31,679
uh and then the quiet notes means that

00:22:30,000 --> 00:22:34,159
some of the notes were not even present

00:22:31,679 --> 00:22:35,520
in in the right hand side trace whereas

00:22:34,159 --> 00:22:36,000
gray means it's like there are no

00:22:35,520 --> 00:22:37,760
difference

00:22:36,000 --> 00:22:39,600
and so we can see for example that kind

00:22:37,760 --> 00:22:40,000
of the the overall latency difference

00:22:39,600 --> 00:22:42,720
came

00:22:40,000 --> 00:22:44,159
into in this path right so it got

00:22:42,720 --> 00:22:45,360
lighter light lighter but this is kind

00:22:44,159 --> 00:22:47,200
of one of the

00:22:45,360 --> 00:22:49,360
suspect spans that you might want to

00:22:47,200 --> 00:22:50,240
drill down and and look at what was

00:22:49,360 --> 00:22:54,320
happening there

00:22:50,240 --> 00:22:56,480
uh and if you um mouse over uh

00:22:54,320 --> 00:22:57,440
these uh these nodes that actually shows

00:22:56,480 --> 00:22:59,200
you how much

00:22:57,440 --> 00:23:01,280
extra time was spent in this specific

00:22:59,200 --> 00:23:02,480
span and how what the percentage of time

00:23:01,280 --> 00:23:05,520
over all of the trace

00:23:02,480 --> 00:23:07,280
was wasted there um so i believe this

00:23:05,520 --> 00:23:09,360
this view is actually still

00:23:07,280 --> 00:23:11,039
not in the main branch it's like in the

00:23:09,360 --> 00:23:13,600
pull request unfortunately so

00:23:11,039 --> 00:23:14,799
you can probably try it right away but

00:23:13,600 --> 00:23:17,840
um

00:23:14,799 --> 00:23:19,039
so uh in conclusion you can use tracing

00:23:17,840 --> 00:23:20,960
tool to

00:23:19,039 --> 00:23:22,080
monitor transactions in your distributed

00:23:20,960 --> 00:23:23,520
architecture and see

00:23:22,080 --> 00:23:25,200
where the requests are executing which

00:23:23,520 --> 00:23:25,840
services has been hit and what happens

00:23:25,200 --> 00:23:28,400
in every

00:23:25,840 --> 00:23:30,159
step of the way right uh we can also do

00:23:28,400 --> 00:23:32,480
root cause analysis by looking at

00:23:30,159 --> 00:23:33,440
individual details like what tags exist

00:23:32,480 --> 00:23:36,720
in each span

00:23:33,440 --> 00:23:38,159
uh like maybe sql query or a url uh or

00:23:36,720 --> 00:23:40,159
some errors where they happen

00:23:38,159 --> 00:23:41,440
and we can drill down into those uh we

00:23:40,159 --> 00:23:43,919
can also look at

00:23:41,440 --> 00:23:45,200
various patterns of how the timeline

00:23:43,919 --> 00:23:47,679
time layout

00:23:45,200 --> 00:23:49,200
of the trace look and so we can detect

00:23:47,679 --> 00:23:50,880
immediately patterns like what's the

00:23:49,200 --> 00:23:52,559
longest critical path what's the

00:23:50,880 --> 00:23:54,640
staircase pattern why it's happening

00:23:52,559 --> 00:23:56,320
sequentially and things like that these

00:23:54,640 --> 00:23:58,960
visualizations make it very easy to

00:23:56,320 --> 00:24:01,200
troubleshoot um and finally we can do

00:23:58,960 --> 00:24:03,279
um various service dependency analysis

00:24:01,200 --> 00:24:03,600
on the traces by using this system graph

00:24:03,279 --> 00:24:06,559
and

00:24:03,600 --> 00:24:08,159
the transitive dependency graph and as i

00:24:06,559 --> 00:24:09,760
mentioned all of that is based on the

00:24:08,159 --> 00:24:12,960
distributed context propagation which is

00:24:09,760 --> 00:24:15,279
provided by jager sdks

00:24:12,960 --> 00:24:16,320
and so now let's talk about jaeger

00:24:15,279 --> 00:24:19,600
architecture and

00:24:16,320 --> 00:24:20,480
overall jaeger project so uh jaeger as a

00:24:19,600 --> 00:24:23,279
platform

00:24:20,480 --> 00:24:24,640
uh consists of these four components uh

00:24:23,279 --> 00:24:27,120
it has a a

00:24:24,640 --> 00:24:28,799
number of client libraries also called

00:24:27,120 --> 00:24:30,159
sdks or tracers

00:24:28,799 --> 00:24:32,159
in different languages you can see it on

00:24:30,159 --> 00:24:33,039
the left they all implement open tracing

00:24:32,159 --> 00:24:35,840
api

00:24:33,039 --> 00:24:37,600
um and so those are the things that you

00:24:35,840 --> 00:24:38,799
put inside your application or inside

00:24:37,600 --> 00:24:39,679
the framework that you're using within

00:24:38,799 --> 00:24:41,120
the application

00:24:39,679 --> 00:24:43,440
right and they collect data and they

00:24:41,120 --> 00:24:44,000
send it out uh to the jager backend and

00:24:43,440 --> 00:24:45,600
that's the

00:24:44,000 --> 00:24:47,039
the middle piece here trace collection

00:24:45,600 --> 00:24:50,080
back-end uh which

00:24:47,039 --> 00:24:51,840
includes storage uh some pre-processing

00:24:50,080 --> 00:24:53,279
some potential aggregations and things

00:24:51,840 --> 00:24:55,120
like that uh and

00:24:53,279 --> 00:24:56,720
back-end also feeds into the data mining

00:24:55,120 --> 00:24:59,360
platform where you can run

00:24:56,720 --> 00:25:01,279
big job analysis big data analysis like

00:24:59,360 --> 00:25:02,240
flink jobs for real-time streaming or

00:25:01,279 --> 00:25:05,679
spark

00:25:02,240 --> 00:25:08,000
and create aggregates or views of your

00:25:05,679 --> 00:25:10,159
traces for example the system graph

00:25:08,000 --> 00:25:12,159
uh in the demo it was like all in memory

00:25:10,159 --> 00:25:12,720
but you can deploy it in such a way that

00:25:12,159 --> 00:25:15,039
it will

00:25:12,720 --> 00:25:16,640
actually compute uh from from a large

00:25:15,039 --> 00:25:19,279
amount of traces and give you the

00:25:16,640 --> 00:25:21,200
visualizations and finally the the front

00:25:19,279 --> 00:25:22,080
end is embedded in the jaeger query as i

00:25:21,200 --> 00:25:24,080
mentioned

00:25:22,080 --> 00:25:25,679
and provides the different views of the

00:25:24,080 --> 00:25:28,720
traces

00:25:25,679 --> 00:25:29,200
um one thing that is worth mentioning is

00:25:28,720 --> 00:25:30,799
that

00:25:29,200 --> 00:25:32,559
jaeger project by itself does not

00:25:30,799 --> 00:25:34,159
provide instrumentation right so if you

00:25:32,559 --> 00:25:36,000
have no instrumentation in application

00:25:34,159 --> 00:25:37,760
you're not going to get traces

00:25:36,000 --> 00:25:39,120
and we don't have auto instrumentation

00:25:37,760 --> 00:25:42,000
agents either uh

00:25:39,120 --> 00:25:43,120
and this was a country's decision uh

00:25:42,000 --> 00:25:46,880
because

00:25:43,120 --> 00:25:48,640
those aspects of of distributed tracing

00:25:46,880 --> 00:25:50,320
are taken care of by projects like open

00:25:48,640 --> 00:25:52,640
tracing and open telemetry and i'll

00:25:50,320 --> 00:25:54,799
later speak about what that means uh and

00:25:52,640 --> 00:25:55,679
so you need to get instrumentation

00:25:54,799 --> 00:25:57,360
somehow

00:25:55,679 --> 00:25:59,120
so that you can start exporting data and

00:25:57,360 --> 00:26:00,799
yeah deals with

00:25:59,120 --> 00:26:02,559
collecting that data and presenting and

00:26:00,799 --> 00:26:05,840
analyzing it

00:26:02,559 --> 00:26:07,200
so as far as history so by the way jager

00:26:05,840 --> 00:26:09,600
means hunter

00:26:07,200 --> 00:26:11,679
and don't spell it please with jagger

00:26:09,600 --> 00:26:14,559
that's not the official name

00:26:11,679 --> 00:26:15,760
it was inspired by dapper from google

00:26:14,559 --> 00:26:17,679
and and open zipkin

00:26:15,760 --> 00:26:19,760
as i mentioned we created the tuber and

00:26:17,679 --> 00:26:21,600
then uh donated to cloud native

00:26:19,760 --> 00:26:23,840
foundation and now it's a top-level

00:26:21,600 --> 00:26:25,200
uh graduated project at cloud native

00:26:23,840 --> 00:26:28,880
foundation

00:26:25,200 --> 00:26:31,760
um so how jaeger fits in

00:26:28,880 --> 00:26:33,360
in your architecture is this is the

00:26:31,760 --> 00:26:34,799
slide tries to explain that so let's say

00:26:33,360 --> 00:26:38,080
you have two services a and b

00:26:34,799 --> 00:26:39,520
right as i mentioned uh you need to have

00:26:38,080 --> 00:26:41,360
some form of instrumentation

00:26:39,520 --> 00:26:43,440
in those services and there are various

00:26:41,360 --> 00:26:45,120
options you have you you can have

00:26:43,440 --> 00:26:46,640
open tracing instrumentation with lots

00:26:45,120 --> 00:26:47,200
of libraries are supported by open

00:26:46,640 --> 00:26:49,440
tracing

00:26:47,200 --> 00:26:51,520
that you can just plug in and you don't

00:26:49,440 --> 00:26:54,880
have to do much in your code really

00:26:51,520 --> 00:26:58,240
just initialize some things and uh

00:26:54,880 --> 00:26:59,919
and then we also include a jaeger sdk

00:26:58,240 --> 00:27:00,559
which simply implements that open

00:26:59,919 --> 00:27:02,559
tracing

00:27:00,559 --> 00:27:04,559
binding so that when instrumentation

00:27:02,559 --> 00:27:06,720
captures the data it just gives it

00:27:04,559 --> 00:27:08,240
into the jager tracer and then tracer is

00:27:06,720 --> 00:27:08,640
possible to send you into jaeger back

00:27:08,240 --> 00:27:11,200
end

00:27:08,640 --> 00:27:12,799
right however there are two data paths

00:27:11,200 --> 00:27:15,600
that you can see here on the screen

00:27:12,799 --> 00:27:17,919
the the the top one in in a solid line

00:27:15,600 --> 00:27:20,320
is the so-called in-band

00:27:17,919 --> 00:27:21,600
data which is when service a makes a

00:27:20,320 --> 00:27:23,919
request to service b

00:27:21,600 --> 00:27:24,799
it includes a certain metadata about the

00:27:23,919 --> 00:27:26,320
trace

00:27:24,799 --> 00:27:28,240
in that request that's a very small

00:27:26,320 --> 00:27:30,000
piece of data like usually trace id span

00:27:28,240 --> 00:27:31,679
id and sampling flag

00:27:30,000 --> 00:27:33,520
and there are different formats that are

00:27:31,679 --> 00:27:34,000
supported so jaeger has its own native

00:27:33,520 --> 00:27:35,840
format

00:27:34,000 --> 00:27:37,760
that was originally developed a tuber

00:27:35,840 --> 00:27:40,000
but there is also now a w3c

00:27:37,760 --> 00:27:41,360
standard format called trace context

00:27:40,000 --> 00:27:43,919
that you can also configure

00:27:41,360 --> 00:27:44,640
jager sdks to use to communicate between

00:27:43,919 --> 00:27:46,559
services and

00:27:44,640 --> 00:27:49,120
we also support zip code b3 format which

00:27:46,559 --> 00:27:51,840
is another alternative of that

00:27:49,120 --> 00:27:53,039
and so that's how the trace information

00:27:51,840 --> 00:27:54,720
gets into the service b

00:27:53,039 --> 00:27:56,559
which it reads that metadata and then

00:27:54,720 --> 00:27:58,880
creates uh again tracing data

00:27:56,559 --> 00:28:00,880
to send it out of band and so the trace

00:27:58,880 --> 00:28:02,159
data that goes to jager back end is

00:28:00,880 --> 00:28:04,240
really

00:28:02,159 --> 00:28:05,279
is sent in the background uh by

00:28:04,240 --> 00:28:08,000
background threads

00:28:05,279 --> 00:28:09,760
um and it does not like happen on the

00:28:08,000 --> 00:28:11,840
critical path of the application where

00:28:09,760 --> 00:28:13,360
this part happens the top part on the

00:28:11,840 --> 00:28:15,279
critical path right it's part of your

00:28:13,360 --> 00:28:17,279
request execution flow

00:28:15,279 --> 00:28:19,679
and as i mentioned you don't actually

00:28:17,279 --> 00:28:21,039
have to have necessarily jaeger tracers

00:28:19,679 --> 00:28:22,960
in your service because

00:28:21,039 --> 00:28:24,640
there is other ways you can instrument

00:28:22,960 --> 00:28:25,919
you can instrument it with zipkin like a

00:28:24,640 --> 00:28:27,679
brave sur

00:28:25,919 --> 00:28:29,520
library or you can instrument with

00:28:27,679 --> 00:28:32,640
various open telemetry sdks

00:28:29,520 --> 00:28:35,120
uh they all kind of support jaeger as

00:28:32,640 --> 00:28:36,960
as a data format except that if you zip

00:28:35,120 --> 00:28:37,600
can down jager itself supports zip code

00:28:36,960 --> 00:28:39,520
format

00:28:37,600 --> 00:28:40,640
but jager backhand can combine all the

00:28:39,520 --> 00:28:44,080
data and present you

00:28:40,640 --> 00:28:45,120
in for a form of traces so this is the

00:28:44,080 --> 00:28:47,840
architecture so

00:28:45,120 --> 00:28:48,480
um of jager itself right so on the left

00:28:47,840 --> 00:28:50,240
again

00:28:48,480 --> 00:28:52,240
imagine that you have your application

00:28:50,240 --> 00:28:54,240
and you have jager client or jager sdk

00:28:52,240 --> 00:28:56,080
iranian inside the application and the

00:28:54,240 --> 00:28:58,559
typical deployment that we recommend is

00:28:56,080 --> 00:28:59,360
that iran jaeger agent which is a small

00:28:58,559 --> 00:29:01,919
process

00:28:59,360 --> 00:29:02,960
uh as a host agent so that you don't

00:29:01,919 --> 00:29:05,039
have to like run

00:29:02,960 --> 00:29:07,039
many of them on one host although if you

00:29:05,039 --> 00:29:09,520
do want you can run it as a sidecar

00:29:07,039 --> 00:29:10,640
in the classic like pod uh kubernetes

00:29:09,520 --> 00:29:12,080
pod so that

00:29:10,640 --> 00:29:13,840
every application will have its own

00:29:12,080 --> 00:29:16,320
eager agent is really it just

00:29:13,840 --> 00:29:16,960
is a proxy it knows how to find jaeger

00:29:16,320 --> 00:29:19,520
back end

00:29:16,960 --> 00:29:20,960
and and send the data there um so you

00:29:19,520 --> 00:29:22,880
don't actually have to use it you can

00:29:20,960 --> 00:29:24,480
configure jager clients to go directly

00:29:22,880 --> 00:29:26,880
to collector but then you have to deal

00:29:24,480 --> 00:29:30,640
with some discovery maybe like uns

00:29:26,880 --> 00:29:32,399
uh sir dns uh name uh so that uh like

00:29:30,640 --> 00:29:34,000
collectors can be located whereas when

00:29:32,399 --> 00:29:35,600
the agent is local to the host you just

00:29:34,000 --> 00:29:37,919
send it to a local port

00:29:35,600 --> 00:29:39,360
and also uh with the agent you can use

00:29:37,919 --> 00:29:41,919
udp so that

00:29:39,360 --> 00:29:43,679
it's sort of like it's a telemetry data

00:29:41,919 --> 00:29:45,440
it's not a big deal if you lose it

00:29:43,679 --> 00:29:47,039
whereas when you send to jager collector

00:29:45,440 --> 00:29:47,600
you can't really use udp so you have to

00:29:47,039 --> 00:29:50,480
use

00:29:47,600 --> 00:29:51,360
uh some http uh protocol to send the

00:29:50,480 --> 00:29:53,600
data

00:29:51,360 --> 00:29:55,520
and then jager collector receives all

00:29:53,600 --> 00:29:57,360
these traces from multiple applications

00:29:55,520 --> 00:29:59,440
and saves them to the database you can

00:29:57,360 --> 00:30:00,480
also have spark of link jobs running off

00:29:59,440 --> 00:30:02,799
of that database

00:30:00,480 --> 00:30:03,840
uh and then the jager query visualizes

00:30:02,799 --> 00:30:05,840
that thing

00:30:03,840 --> 00:30:08,159
and the last piece which is shown in the

00:30:05,840 --> 00:30:09,279
rats here is is a control flow this is

00:30:08,159 --> 00:30:10,399
um

00:30:09,279 --> 00:30:12,559
something that we've built from the

00:30:10,399 --> 00:30:14,000
beginning into jager architecture uh

00:30:12,559 --> 00:30:16,399
which i'll speak to

00:30:14,000 --> 00:30:18,320
uh when we talk about sampling it allows

00:30:16,399 --> 00:30:20,640
you to push configuration back into the

00:30:18,320 --> 00:30:20,960
jager sdks to affect how the sampling is

00:30:20,640 --> 00:30:23,760
done

00:30:20,960 --> 00:30:25,600
in the application however this was the

00:30:23,760 --> 00:30:28,720
architecture that we initially uh

00:30:25,600 --> 00:30:29,120
run a tour and later on we switched to

00:30:28,720 --> 00:30:31,039
to

00:30:29,120 --> 00:30:32,799
to slightly different architecture where

00:30:31,039 --> 00:30:34,159
after jager collector we introduced

00:30:32,799 --> 00:30:37,200
kafka

00:30:34,159 --> 00:30:38,159
before a component which writes spends

00:30:37,200 --> 00:30:40,399
in into storage

00:30:38,159 --> 00:30:41,440
right so the jager collector got split

00:30:40,399 --> 00:30:43,360
into collector and

00:30:41,440 --> 00:30:44,799
in in gesture and indexer and the reason

00:30:43,360 --> 00:30:47,120
we did that is because

00:30:44,799 --> 00:30:48,000
um when you sometimes have a traffic

00:30:47,120 --> 00:30:50,799
spikes

00:30:48,000 --> 00:30:52,799
uh or some application uh is deployed

00:30:50,799 --> 00:30:54,720
with like a sampling of hundred percent

00:30:52,799 --> 00:30:56,320
it's very easy to send too much data

00:30:54,720 --> 00:30:58,559
that jager collector is simply not able

00:30:56,320 --> 00:31:00,240
to say fast enough into this database

00:30:58,559 --> 00:31:01,600
uh because database have like a

00:31:00,240 --> 00:31:04,480
throughput limit um

00:31:01,600 --> 00:31:05,440
and whereas kafka is usually a more

00:31:04,480 --> 00:31:08,960
elastic

00:31:05,440 --> 00:31:11,279
uh storage you can think of it uh

00:31:08,960 --> 00:31:12,240
that that can accommodate uh huge

00:31:11,279 --> 00:31:14,720
traffic spikes

00:31:12,240 --> 00:31:15,440
and so this is one reason we introduced

00:31:14,720 --> 00:31:16,960
it so like

00:31:15,440 --> 00:31:18,559
we don't lose data when there is a

00:31:16,960 --> 00:31:20,720
traffic spike we just write more to

00:31:18,559 --> 00:31:23,519
kafka and then we experience a certain

00:31:20,720 --> 00:31:25,120
ingestion delay or lag as a result

00:31:23,519 --> 00:31:26,320
because it takes a bit more time to save

00:31:25,120 --> 00:31:28,159
it to the database and so

00:31:26,320 --> 00:31:29,679
the traces are not immediately available

00:31:28,159 --> 00:31:31,919
in to the query

00:31:29,679 --> 00:31:33,519
but another serious reason why we

00:31:31,919 --> 00:31:35,440
introduced kafka is because it allows us

00:31:33,519 --> 00:31:37,039
to to start building flink jobs

00:31:35,440 --> 00:31:38,880
uh to do aggregations like the

00:31:37,039 --> 00:31:41,200
dependency graphs um

00:31:38,880 --> 00:31:42,960
in real time rather than iranian spark

00:31:41,200 --> 00:31:43,919
job which has to read the whole database

00:31:42,960 --> 00:31:46,080
and and like

00:31:43,919 --> 00:31:47,120
this is uh doing it in real time is much

00:31:46,080 --> 00:31:49,440
more uh

00:31:47,120 --> 00:31:51,600
interactive when you get uh data faster

00:31:49,440 --> 00:31:53,279
into into a system and it reacts faster

00:31:51,600 --> 00:31:55,279
so this is what we're currently running

00:31:53,279 --> 00:31:56,880
at uber and uh again you don't have to

00:31:55,279 --> 00:31:58,559
use kafka you can go directly to the

00:31:56,880 --> 00:31:59,519
database it's really it depends on what

00:31:58,559 --> 00:32:03,519
you want to get

00:31:59,519 --> 00:32:06,000
from this and the technology stack

00:32:03,519 --> 00:32:07,679
uh to mention for jaeger is it's uh

00:32:06,000 --> 00:32:09,760
backhand is all written in go

00:32:07,679 --> 00:32:12,080
uh we support pluggable storage there

00:32:09,760 --> 00:32:14,000
are two ways there's a

00:32:12,080 --> 00:32:15,600
several back-ends which are natively

00:32:14,000 --> 00:32:17,600
supported directly by the

00:32:15,600 --> 00:32:19,360
binaries that we uh distribute

00:32:17,600 --> 00:32:20,559
specifically cassandra elasticsearch

00:32:19,360 --> 00:32:23,679
badger which is a

00:32:20,559 --> 00:32:26,320
sort of single node storage on disk and

00:32:23,679 --> 00:32:27,679
also users like a toy in memory

00:32:26,320 --> 00:32:30,000
implementation that is used by

00:32:27,679 --> 00:32:32,720
all-in-one uh binary

00:32:30,000 --> 00:32:34,960
uh however there is also another uh

00:32:32,720 --> 00:32:36,559
plugable solution called jrpc plugins

00:32:34,960 --> 00:32:37,919
where you can implement any kind of

00:32:36,559 --> 00:32:41,120
storage backend

00:32:37,919 --> 00:32:43,360
uh communicating over jpc with the jager

00:32:41,120 --> 00:32:45,440
end with the jager collector and that

00:32:43,360 --> 00:32:48,000
allows uh sort of

00:32:45,440 --> 00:32:49,200
us to to extend the capabilities to

00:32:48,000 --> 00:32:51,360
other storage engines

00:32:49,200 --> 00:32:53,519
uh without kind of bringing all of that

00:32:51,360 --> 00:32:54,799
maintenance overhead into jager main

00:32:53,519 --> 00:32:56,640
jaeger repository

00:32:54,799 --> 00:32:58,399
uh the frontend is built in javascript

00:32:56,640 --> 00:33:01,919
and react is pretty standard

00:32:58,399 --> 00:33:04,080
uh instrumentation libraries or like

00:33:01,919 --> 00:33:06,320
sdks are all implemented in open tracing

00:33:04,080 --> 00:33:08,840
and we integrated with

00:33:06,320 --> 00:33:10,159
various like kafka and apache flink

00:33:08,840 --> 00:33:11,440
frameworks

00:33:10,159 --> 00:33:13,360
and as i mentioned zip code

00:33:11,440 --> 00:33:16,559
compatibility involves

00:33:13,360 --> 00:33:17,360
two things we can uh jager clients

00:33:16,559 --> 00:33:20,480
understand

00:33:17,360 --> 00:33:23,200
zipkin headers format on the wire

00:33:20,480 --> 00:33:24,960
and it also zip can collect sorry jager

00:33:23,200 --> 00:33:27,279
collector can also receive

00:33:24,960 --> 00:33:28,159
data from zipkin in various formats that

00:33:27,279 --> 00:33:30,640
you can support

00:33:28,159 --> 00:33:32,000
um it can even read from kafka like in

00:33:30,640 --> 00:33:33,039
in the zip code street format for

00:33:32,000 --> 00:33:35,760
example

00:33:33,039 --> 00:33:37,679
um now as i promised let's talk about

00:33:35,760 --> 00:33:40,000
quickly about sampling so first of all

00:33:37,679 --> 00:33:40,960
why do we sample right why why it's even

00:33:40,000 --> 00:33:43,440
a topic

00:33:40,960 --> 00:33:45,360
and the problem is that trace tracing

00:33:43,440 --> 00:33:48,399
information is very very rich

00:33:45,360 --> 00:33:50,000
uh you imagine like as i explained in

00:33:48,399 --> 00:33:51,840
the demo for every rpc

00:33:50,000 --> 00:33:53,360
call you have two spans on the client

00:33:51,840 --> 00:33:55,519
and on the server so each

00:33:53,360 --> 00:33:56,480
each span can have all kinds of tags and

00:33:55,519 --> 00:33:58,880
like attributes

00:33:56,480 --> 00:34:00,960
like with urls it can have logs so this

00:33:58,880 --> 00:34:01,360
is pretty bulky objects that we have to

00:34:00,960 --> 00:34:03,440
ship

00:34:01,360 --> 00:34:05,519
and that happens for every single rpc

00:34:03,440 --> 00:34:06,480
request or actually hundreds of the rpc

00:34:05,519 --> 00:34:08,240
requests right so

00:34:06,480 --> 00:34:09,679
the the volume of data accumulates

00:34:08,240 --> 00:34:13,200
pretty fast um

00:34:09,679 --> 00:34:13,839
and uh so if your service is doing like

00:34:13,200 --> 00:34:16,800
i don't know

00:34:13,839 --> 00:34:18,480
ten ten thousand rpcs uh imagine how

00:34:16,800 --> 00:34:19,359
much data you're gonna accumulate per

00:34:18,480 --> 00:34:22,000
second

00:34:19,359 --> 00:34:23,440
so storing all of that can incur pretty

00:34:22,000 --> 00:34:25,280
large storage costs

00:34:23,440 --> 00:34:26,560
that's one reason for sampling but

00:34:25,280 --> 00:34:28,320
another reason is that

00:34:26,560 --> 00:34:30,079
uh just collecting all this data from

00:34:28,320 --> 00:34:31,760
the application also has an impact on

00:34:30,079 --> 00:34:33,440
the performance of your application so

00:34:31,760 --> 00:34:35,119
you may introduce latency because you

00:34:33,440 --> 00:34:37,440
waste in cpu cycles

00:34:35,119 --> 00:34:39,040
on processing all this data and so

00:34:37,440 --> 00:34:40,639
sampling is usually the technique to

00:34:39,040 --> 00:34:41,440
deal with that overhead and with the

00:34:40,639 --> 00:34:44,560
with a

00:34:41,440 --> 00:34:47,599
large storage cost to avoid them

00:34:44,560 --> 00:34:50,000
so there are two types of sampling that

00:34:47,599 --> 00:34:51,679
typically used in the tracing system one

00:34:50,000 --> 00:34:53,359
is head by sampling where

00:34:51,679 --> 00:34:54,960
the sampling decision is made at the

00:34:53,359 --> 00:34:56,720
very beginning when the trace is just

00:34:54,960 --> 00:34:57,520
starting so when you create a new random

00:34:56,720 --> 00:34:59,280
trace id

00:34:57,520 --> 00:35:01,359
we flip a coin and say okay we're gonna

00:34:59,280 --> 00:35:03,040
sample it or not and once we make the

00:35:01,359 --> 00:35:04,800
decision that decision is fixed

00:35:03,040 --> 00:35:06,800
for the life of the trace and it's

00:35:04,800 --> 00:35:08,320
propagated as part of the trace context

00:35:06,800 --> 00:35:09,920
so that every other service which

00:35:08,320 --> 00:35:10,960
participates in the trace it will use

00:35:09,920 --> 00:35:13,200
the same decision

00:35:10,960 --> 00:35:14,320
so that you don't get like uh partial

00:35:13,200 --> 00:35:17,680
traces somewhere

00:35:14,320 --> 00:35:19,839
here uh and uh because uh

00:35:17,680 --> 00:35:21,760
that is pretty cheap way of doing the

00:35:19,839 --> 00:35:22,400
sampling it has very minimal performance

00:35:21,760 --> 00:35:24,320
overhead

00:35:22,400 --> 00:35:26,640
especially when the trace is not sampled

00:35:24,320 --> 00:35:29,200
all your uh instrumentation is really

00:35:26,640 --> 00:35:30,160
effectively a no-op um you you get very

00:35:29,200 --> 00:35:33,119
little overhead

00:35:30,160 --> 00:35:34,720
um and this is the default mode that's

00:35:33,119 --> 00:35:37,680
supported by jager sdk

00:35:34,720 --> 00:35:38,160
the downside of upfront sampling is that

00:35:37,680 --> 00:35:40,640
uh

00:35:38,160 --> 00:35:41,440
your 99 of your requests in the system

00:35:40,640 --> 00:35:43,359
are gonna be

00:35:41,440 --> 00:35:45,359
normal and not particularly interesting

00:35:43,359 --> 00:35:47,359
right you really want to look at

00:35:45,359 --> 00:35:48,480
outliers in terms of latency maybe

00:35:47,359 --> 00:35:51,599
errors uh

00:35:48,480 --> 00:35:52,240
and those happen much more rarely and as

00:35:51,599 --> 00:35:54,160
a result

00:35:52,240 --> 00:35:56,560
if you also in like let's say they

00:35:54,160 --> 00:35:58,240
happened 100 000 times and plus you also

00:35:56,560 --> 00:35:58,640
have a sampling rate and one thousand

00:35:58,240 --> 00:36:00,960
then

00:35:58,640 --> 00:36:02,640
really your chance to get an outlier or

00:36:00,960 --> 00:36:05,200
anomaly is one in a million

00:36:02,640 --> 00:36:06,480
uh so that's kind of a bad thing about

00:36:05,200 --> 00:36:08,640
head based sampling and

00:36:06,480 --> 00:36:10,160
unfortunately there's not much that can

00:36:08,640 --> 00:36:12,560
be improved about it because

00:36:10,160 --> 00:36:14,000
uh it simply doesn't know anything about

00:36:12,560 --> 00:36:15,520
what will happen to the trace when it

00:36:14,000 --> 00:36:16,800
makes a sampling decision it's like done

00:36:15,520 --> 00:36:20,240
at the beginning

00:36:16,800 --> 00:36:22,079
and so um the way the

00:36:20,240 --> 00:36:23,359
head-based sampling is done in jaeger is

00:36:22,079 --> 00:36:24,560
that each sdk

00:36:23,359 --> 00:36:26,640
can be configured with different

00:36:24,560 --> 00:36:28,240
samplers you can say use probabilistic

00:36:26,640 --> 00:36:29,920
sample like a coin flip with a certain

00:36:28,240 --> 00:36:31,599
rates or you can use rate limit and say

00:36:29,920 --> 00:36:34,079
in like this many per second

00:36:31,599 --> 00:36:36,079
uh but uh the interesting part is that

00:36:34,079 --> 00:36:37,599
is that all sdk supports so-called

00:36:36,079 --> 00:36:39,280
remote sampling where

00:36:37,599 --> 00:36:41,200
as i mentioned in the architecture

00:36:39,280 --> 00:36:42,880
diagram the configuration actually comes

00:36:41,200 --> 00:36:45,200
from the back end

00:36:42,880 --> 00:36:46,720
and that's very powerful because when

00:36:45,200 --> 00:36:48,320
you have very many services in your

00:36:46,720 --> 00:36:49,440
architecture and many different teams

00:36:48,320 --> 00:36:51,040
running those services

00:36:49,440 --> 00:36:53,040
those teams don't necessarily know what

00:36:51,040 --> 00:36:55,760
kind of sampling is good for

00:36:53,040 --> 00:36:57,440
for that system uh they also don't know

00:36:55,760 --> 00:36:58,480
when the traffic patterns changes and

00:36:57,440 --> 00:37:00,160
how do you need to

00:36:58,480 --> 00:37:01,920
reconfigure the sample or redeployed

00:37:00,160 --> 00:37:03,200
application uh whereas when the

00:37:01,920 --> 00:37:05,040
configuration comes from the

00:37:03,200 --> 00:37:06,560
from the center location you could do

00:37:05,040 --> 00:37:07,280
all of that in a much more intelligent

00:37:06,560 --> 00:37:09,520
way

00:37:07,280 --> 00:37:10,400
at minimum you you give control to the

00:37:09,520 --> 00:37:11,680
sampling to the

00:37:10,400 --> 00:37:13,440
team that runs the tracing

00:37:11,680 --> 00:37:13,760
infrastructure so that they have sort of

00:37:13,440 --> 00:37:15,599
like

00:37:13,760 --> 00:37:17,440
levers in terms of how much traffic they

00:37:15,599 --> 00:37:20,160
want to ingest um

00:37:17,440 --> 00:37:21,680
and uh but you can also make something

00:37:20,160 --> 00:37:22,079
more intelligent like adaptive sampling

00:37:21,680 --> 00:37:24,960
which

00:37:22,079 --> 00:37:25,680
calculates things on a sort of a control

00:37:24,960 --> 00:37:27,359
loop

00:37:25,680 --> 00:37:29,200
and reacts to traffic spikes this is

00:37:27,359 --> 00:37:31,280
what we use at uber um

00:37:29,200 --> 00:37:32,880
and the one thing is is also interesting

00:37:31,280 --> 00:37:35,040
is the configuration can be done

00:37:32,880 --> 00:37:36,160
per service and tournament points so

00:37:35,040 --> 00:37:37,920
it's very often

00:37:36,160 --> 00:37:40,000
the case that a given service may have

00:37:37,920 --> 00:37:42,800
multiple endpoints with very different

00:37:40,000 --> 00:37:43,359
uh rates of of queries to them or

00:37:42,800 --> 00:37:44,800
requests

00:37:43,359 --> 00:37:46,800
and so you don't want to sample

00:37:44,800 --> 00:37:49,040
everything at let's say one percent

00:37:46,800 --> 00:37:50,320
if the difference if you have multiple

00:37:49,040 --> 00:37:52,320
like orders of magnitude

00:37:50,320 --> 00:37:54,079
in the qps of the end points so this

00:37:52,320 --> 00:37:56,160
configuration allows you to do at the

00:37:54,079 --> 00:37:58,000
individual level and

00:37:56,160 --> 00:37:59,200
so you can read the documentation of how

00:37:58,000 --> 00:38:01,119
to configure it

00:37:59,200 --> 00:38:03,440
now let's talk about tail based sounding

00:38:01,119 --> 00:38:04,000
so tail by sampling is is different uh

00:38:03,440 --> 00:38:05,599
completely

00:38:04,000 --> 00:38:07,599
different mode where the sampling

00:38:05,599 --> 00:38:09,359
decision is made at the end of the trace

00:38:07,599 --> 00:38:11,040
and because of that it can be much more

00:38:09,359 --> 00:38:12,800
intelligent we can look at the latencies

00:38:11,040 --> 00:38:13,680
we can look at the errors or some logs

00:38:12,800 --> 00:38:15,760
whatever

00:38:13,680 --> 00:38:16,800
anything that looks interesting in the

00:38:15,760 --> 00:38:20,160
trace we can we can

00:38:16,800 --> 00:38:23,040
affect how we sample those traces uh um

00:38:20,160 --> 00:38:23,520
in in an interesting ways right however

00:38:23,040 --> 00:38:26,320
that

00:38:23,520 --> 00:38:28,320
uh requires that all these traces still

00:38:26,320 --> 00:38:31,280
need to be stored somewhere because

00:38:28,320 --> 00:38:32,880
um like traces are distributed you have

00:38:31,280 --> 00:38:34,320
all these spans cramming from all these

00:38:32,880 --> 00:38:34,640
different applications you kind of need

00:38:34,320 --> 00:38:36,880
to

00:38:34,640 --> 00:38:38,400
assemble them all in one place first and

00:38:36,880 --> 00:38:40,240
you don't want to store them on disk

00:38:38,400 --> 00:38:42,800
during that assembly time because

00:38:40,240 --> 00:38:43,599
uh then we defeat the purpose of

00:38:42,800 --> 00:38:45,359
sampling we

00:38:43,599 --> 00:38:47,359
we want not to hit the disk because it's

00:38:45,359 --> 00:38:48,960
very expensive so it really means you

00:38:47,359 --> 00:38:50,079
have to allocate a lot of memory to

00:38:48,960 --> 00:38:51,680
store all these traces

00:38:50,079 --> 00:38:53,200
until they're done and you can make a

00:38:51,680 --> 00:38:55,040
sampling decision fortunately they're

00:38:53,200 --> 00:38:56,079
all short usually traces like last no

00:38:55,040 --> 00:38:57,760
more than a second so

00:38:56,079 --> 00:39:00,400
most of them can be expired from memory

00:38:57,760 --> 00:39:03,200
very quickly so it doesn't necessarily

00:39:00,400 --> 00:39:05,200
introduce a lot of memory overhead um uh

00:39:03,200 --> 00:39:07,520
but there are some like architectural

00:39:05,200 --> 00:39:08,480
things you need to do to support that

00:39:07,520 --> 00:39:10,320
and another

00:39:08,480 --> 00:39:12,160
another kind of downside of tail-based

00:39:10,320 --> 00:39:14,320
sampling is that because we need to

00:39:12,160 --> 00:39:16,960
collect all the data from from all this

00:39:14,320 --> 00:39:18,640
every single request that means that it

00:39:16,960 --> 00:39:19,599
has the maximum performance overhead on

00:39:18,640 --> 00:39:21,680
your application

00:39:19,599 --> 00:39:23,440
right and so it's a trade-off whether

00:39:21,680 --> 00:39:25,920
you want to afford that and maybe

00:39:23,440 --> 00:39:27,680
in increase in latency you can sometimes

00:39:25,920 --> 00:39:28,800
combine the tail-based and head-based

00:39:27,680 --> 00:39:30,720
sampling so let's say

00:39:28,800 --> 00:39:32,560
instead of sampling one in a thousand

00:39:30,720 --> 00:39:33,359
you say well let's sample one and ten

00:39:32,560 --> 00:39:35,680
but then do

00:39:33,359 --> 00:39:36,720
like for every tenth we kind of do the

00:39:35,680 --> 00:39:38,560
tail-based sampling

00:39:36,720 --> 00:39:41,760
so that that allows you to control the

00:39:38,560 --> 00:39:44,160
costs and performance

00:39:41,760 --> 00:39:45,040
uh so how sampling tail based sampling

00:39:44,160 --> 00:39:46,480
works in the aeger

00:39:45,040 --> 00:39:48,160
so there's nothing that needs to be done

00:39:46,480 --> 00:39:49,359
on the jaeger sdks because you simply

00:39:48,160 --> 00:39:53,599
configure them with

00:39:49,359 --> 00:39:56,000
either 100 or like a fixed percentage uh

00:39:53,599 --> 00:39:57,520
where the the what the magic happens is

00:39:56,000 --> 00:39:58,960
really in on the back end but

00:39:57,520 --> 00:40:00,720
jager components themselves don't

00:39:58,960 --> 00:40:03,839
support tail by sampling but

00:40:00,720 --> 00:40:05,520
we now release uh new components called

00:40:03,839 --> 00:40:07,920
open telemetry collectors

00:40:05,520 --> 00:40:09,040
uh those are jaeger binaries

00:40:07,920 --> 00:40:11,119
specifically

00:40:09,040 --> 00:40:12,640
built with from the open telemetry

00:40:11,119 --> 00:40:13,520
collector we just support tail by

00:40:12,640 --> 00:40:15,280
sampling and

00:40:13,520 --> 00:40:17,119
you can configure those collectors with

00:40:15,280 --> 00:40:18,880
various sampling rules by like latency

00:40:17,119 --> 00:40:21,920
or certain tags like error tags

00:40:18,880 --> 00:40:23,599
um unfortunately at this point uh open

00:40:21,920 --> 00:40:25,440
telemeter collector only supports a

00:40:23,599 --> 00:40:28,160
single node mode which means

00:40:25,440 --> 00:40:30,000
if you can fit all your traces uh in one

00:40:28,160 --> 00:40:32,240
node memory then you're okay but

00:40:30,000 --> 00:40:33,040
if you need a kind of a scaled out

00:40:32,240 --> 00:40:34,960
solution

00:40:33,040 --> 00:40:37,280
uh then that's not currently available

00:40:34,960 --> 00:40:39,040
but there is work already happening

00:40:37,280 --> 00:40:40,480
and there's a blog post by grafana how

00:40:39,040 --> 00:40:42,800
they did it so

00:40:40,480 --> 00:40:44,400
it will it will be available in the near

00:40:42,800 --> 00:40:46,800
future

00:40:44,400 --> 00:40:47,440
uh and finally uh to close this i

00:40:46,800 --> 00:40:48,960
mentioned

00:40:47,440 --> 00:40:50,640
that i want to talk about uh open

00:40:48,960 --> 00:40:51,359
telemetry so open telemetry is a new

00:40:50,640 --> 00:40:54,960
project

00:40:51,359 --> 00:40:57,839
in cncf which is a it

00:40:54,960 --> 00:41:00,160
it's a descendant of open uh tracing and

00:40:57,839 --> 00:41:02,400
open sensors these two projects merge

00:41:00,160 --> 00:41:04,319
and it deals with again establishing a

00:41:02,400 --> 00:41:05,920
unified instrumentation framework so

00:41:04,319 --> 00:41:07,200
that you can reuse the instrumentation

00:41:05,920 --> 00:41:09,280
in multiple applications

00:41:07,200 --> 00:41:10,960
right it does not deal with the back end

00:41:09,280 --> 00:41:12,160
collection of traces except for the

00:41:10,960 --> 00:41:13,839
collector which is kind of an

00:41:12,160 --> 00:41:17,200
intermediate piece

00:41:13,839 --> 00:41:19,359
so to to illustrate that let's say uh

00:41:17,200 --> 00:41:20,720
we're we're dealing normally with jaeger

00:41:19,359 --> 00:41:21,839
and open tracing this is how jaeger

00:41:20,720 --> 00:41:23,920
historically evolved

00:41:21,839 --> 00:41:24,960
and so you have application your

00:41:23,920 --> 00:41:26,560
application at the top

00:41:24,960 --> 00:41:28,240
there's like three types of applications

00:41:26,560 --> 00:41:28,800
that typically can be instrumented for

00:41:28,240 --> 00:41:30,880
tracing

00:41:28,800 --> 00:41:32,319
some of them are explicitly instrumented

00:41:30,880 --> 00:41:33,520
directly in your code you can start

00:41:32,319 --> 00:41:36,720
spans and write

00:41:33,520 --> 00:41:38,960
where is metadata into it or more often

00:41:36,720 --> 00:41:40,720
you're in the middle uh box here where

00:41:38,960 --> 00:41:42,400
you use some rpc framework which comes

00:41:40,720 --> 00:41:44,480
with the middleware for tracing

00:41:42,400 --> 00:41:45,440
um or there's another option where you

00:41:44,480 --> 00:41:47,680
can

00:41:45,440 --> 00:41:48,960
sometimes there are in certain languages

00:41:47,680 --> 00:41:50,640
there are auto instrumentation

00:41:48,960 --> 00:41:52,400
facilities where you can just attach a

00:41:50,640 --> 00:41:54,400
library to your binary and then

00:41:52,400 --> 00:41:55,839
you magically get traces like in open

00:41:54,400 --> 00:41:56,560
tracing that's available for example in

00:41:55,839 --> 00:41:59,119
python and

00:41:56,560 --> 00:42:00,560
in java and then all these

00:41:59,119 --> 00:42:02,079
instrumentations they talk through open

00:42:00,560 --> 00:42:04,000
tracing api which

00:42:02,079 --> 00:42:05,920
as i mentioned calls back into the jager

00:42:04,000 --> 00:42:07,599
sdk and then jaeger sdk sends the data

00:42:05,920 --> 00:42:08,000
back to the jager back end right so it's

00:42:07,599 --> 00:42:10,240
very

00:42:08,000 --> 00:42:11,920
kind of all clear here so now what

00:42:10,240 --> 00:42:14,079
happens with open telemetry

00:42:11,920 --> 00:42:15,440
uh so open telemetry presents a

00:42:14,079 --> 00:42:16,960
different slightly different api than

00:42:15,440 --> 00:42:18,880
open tracing it's conceptually still

00:42:16,960 --> 00:42:19,440
very similar traces and spans and

00:42:18,880 --> 00:42:21,440
everything

00:42:19,440 --> 00:42:22,480
uh but the method names are slightly

00:42:21,440 --> 00:42:24,160
different so

00:42:22,480 --> 00:42:26,079
but you can just use the different types

00:42:24,160 --> 00:42:29,200
of instrumentations for open telemetry

00:42:26,079 --> 00:42:29,680
and then you don't have to have uh or

00:42:29,200 --> 00:42:31,760
run

00:42:29,680 --> 00:42:33,680
jaeger sdk you can just run standard

00:42:31,760 --> 00:42:35,040
open telemetry sdk which is included in

00:42:33,680 --> 00:42:38,800
the project in all languages

00:42:35,040 --> 00:42:40,800
right and then that sdk has the ability

00:42:38,800 --> 00:42:42,880
to export data directly in the jager

00:42:40,800 --> 00:42:44,880
format so you can still have jager agent

00:42:42,880 --> 00:42:47,520
or jaeger collector except in jaeger

00:42:44,880 --> 00:42:48,800
data spans uh but you can also

00:42:47,520 --> 00:42:51,440
alternatively run

00:42:48,800 --> 00:42:53,119
open telemetry collector directly which

00:42:51,440 --> 00:42:54,720
and then export data in the open

00:42:53,119 --> 00:42:57,440
telemetry format which

00:42:54,720 --> 00:42:59,359
is kind of a standardized way now to

00:42:57,440 --> 00:43:00,480
represent traces and jager is gradually

00:42:59,359 --> 00:43:03,040
migrating to that

00:43:00,480 --> 00:43:05,119
uh and then open telemetry collector

00:43:03,040 --> 00:43:06,160
will still be able to forward data to

00:43:05,119 --> 00:43:09,680
jager back and

00:43:06,160 --> 00:43:13,359
in the format and uh so as i mentioned

00:43:09,680 --> 00:43:15,359
jaeger uh components now exist that

00:43:13,359 --> 00:43:16,960
extend open telemetry because collector

00:43:15,359 --> 00:43:18,400
in open telemetry is written on go we

00:43:16,960 --> 00:43:20,480
can just use it as a library

00:43:18,400 --> 00:43:21,520
so we've built our own versions of those

00:43:20,480 --> 00:43:24,160
binaries

00:43:21,520 --> 00:43:25,920
which have the same capabilities as a

00:43:24,160 --> 00:43:26,480
open telemetry drop stream collectors

00:43:25,920 --> 00:43:28,400
but

00:43:26,480 --> 00:43:30,319
with additional jaeger extensions which

00:43:28,400 --> 00:43:30,800
are kind of uh specific to jaeger for

00:43:30,319 --> 00:43:32,880
example

00:43:30,800 --> 00:43:34,720
we can plug in directly our storage

00:43:32,880 --> 00:43:35,839
implementations into a collector so that

00:43:34,720 --> 00:43:39,040
you don't have to run

00:43:35,839 --> 00:43:40,000
like multiple services to uh to push the

00:43:39,040 --> 00:43:43,040
data through

00:43:40,000 --> 00:43:43,440
um and we're also converting uh jaeger

00:43:43,040 --> 00:43:46,000
uh

00:43:43,440 --> 00:43:47,680
implementations for storage now to work

00:43:46,000 --> 00:43:48,160
directly with the open telemeter data

00:43:47,680 --> 00:43:49,760
model

00:43:48,160 --> 00:43:51,440
uh which is slightly richer than open

00:43:49,760 --> 00:43:53,440
tracing because it will

00:43:51,440 --> 00:43:54,880
kind of provide us better compatibility

00:43:53,440 --> 00:43:57,440
and uh path forward

00:43:54,880 --> 00:43:58,800
uh and so we're trying to uh kind of

00:43:57,440 --> 00:44:00,880
reuse all the good thing that open

00:43:58,800 --> 00:44:02,720
telemetry is building so that we can

00:44:00,880 --> 00:44:04,560
reduce the efforts we need to maintain

00:44:02,720 --> 00:44:05,119
for example all the jager sdks which was

00:44:04,560 --> 00:44:08,400
pretty

00:44:05,119 --> 00:44:11,839
expensive um work in the past um

00:44:08,400 --> 00:44:14,960
so if you want to learn more about uh

00:44:11,839 --> 00:44:16,480
uh jager uh you can you can attend a

00:44:14,960 --> 00:44:18,960
deep dive which will happen

00:44:16,480 --> 00:44:20,400
uh on the next day uh and so pavel will

00:44:18,960 --> 00:44:22,319
be talking about more about jaeger

00:44:20,400 --> 00:44:24,560
architecture and other things and

00:44:22,319 --> 00:44:26,000
finally these are uh the ways that you

00:44:24,560 --> 00:44:28,560
can get in touch with uh

00:44:26,000 --> 00:44:30,000
with the community of jaeger project

00:44:28,560 --> 00:44:31,280
there is a blog post there's a twitter

00:44:30,000 --> 00:44:33,440
account that you can follow

00:44:31,280 --> 00:44:34,480
and there's online chat where you can go

00:44:33,440 --> 00:44:37,839
and ask questions

00:44:34,480 --> 00:44:42,000
things like that so that's the end of my

00:44:37,839 --> 00:44:42,000

YouTube URL: https://www.youtube.com/watch?v=UNqilb9_zwY


