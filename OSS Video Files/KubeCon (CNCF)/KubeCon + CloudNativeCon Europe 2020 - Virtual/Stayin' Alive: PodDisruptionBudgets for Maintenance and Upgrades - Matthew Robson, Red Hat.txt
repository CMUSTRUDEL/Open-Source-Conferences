Title: Stayin' Alive: PodDisruptionBudgets for Maintenance and Upgrades - Matthew Robson, Red Hat
Publication date: 2020-08-27
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Stayin' Alive: PodDisruptionBudgets for Maintenance and Upgrades - Matthew Robson, Red Hat 

As we work harder to automate our clusters, it becomes more and more difficult to guarantee the availability requirements of our applications. In large clusters, operations teams may not have the insights to ensure an applications minimum capacity requirements are maintained. Without that understanding, you may inadvertently bring down or inhibit applications through routine maintenance activities.  Enter the Pod Disruption Budget (PDB). Simply put, PDBs allows application owners to define the minimum requirement for a service to operate in a stable manner.  In this Lightning Talk, let me walk you through the benefits, usage and implementation of PDB’s. As an attendee, you will walk away with the necessary knowledge on how to use PDBs to define enforceable operating requirements of your applications.

https://sched.co/ZemK
Captions: 
	00:00:00,080 --> 00:00:03,760
hi everyone welcome to my lightning talk

00:00:02,399 --> 00:00:05,920
staying alive

00:00:03,760 --> 00:00:07,120
pod disruption budgets for maintenance

00:00:05,920 --> 00:00:09,679
and upgrades

00:00:07,120 --> 00:00:11,599
my name is matthew robson and i'm a

00:00:09,679 --> 00:00:13,920
principal technical account manager

00:00:11,599 --> 00:00:15,200
at red hat working on openshift and

00:00:13,920 --> 00:00:17,119
kubernetes

00:00:15,200 --> 00:00:18,960
if you have any questions following this

00:00:17,119 --> 00:00:20,640
lightning talk feel free to reach out to

00:00:18,960 --> 00:00:24,160
me on twitter at matt

00:00:20,640 --> 00:00:27,920
j robson so what exactly

00:00:24,160 --> 00:00:28,480
is a pod disruption budget a pdb for

00:00:27,920 --> 00:00:31,199
short

00:00:28,480 --> 00:00:33,040
is an application owner created object

00:00:31,199 --> 00:00:33,920
that defines the minimum number of

00:00:33,040 --> 00:00:36,000
replicas

00:00:33,920 --> 00:00:37,680
that must be available for an

00:00:36,000 --> 00:00:40,000
application to operate

00:00:37,680 --> 00:00:41,920
in a stable manner during voluntary

00:00:40,000 --> 00:00:44,160
disruptions

00:00:41,920 --> 00:00:46,480
with that said why would i want to use

00:00:44,160 --> 00:00:48,239
pod disruption budgets

00:00:46,480 --> 00:00:49,760
first off they're owned by the

00:00:48,239 --> 00:00:52,160
application team

00:00:49,760 --> 00:00:53,360
application owners best understand the

00:00:52,160 --> 00:00:55,520
requirements and

00:00:53,360 --> 00:00:56,719
performance characteristics of their

00:00:55,520 --> 00:00:58,640
services

00:00:56,719 --> 00:01:00,000
this is also supportive of the

00:00:58,640 --> 00:01:02,000
operations team

00:01:00,000 --> 00:01:03,760
because it bridges the gap of

00:01:02,000 --> 00:01:07,119
application knowledge

00:01:03,760 --> 00:01:10,080
to operational execution

00:01:07,119 --> 00:01:11,200
pdbs also define your availability

00:01:10,080 --> 00:01:13,840
requirements

00:01:11,200 --> 00:01:14,240
this could refer to maintaining things

00:01:13,840 --> 00:01:17,439
like

00:01:14,240 --> 00:01:20,159
quorum requirements for an sla

00:01:17,439 --> 00:01:21,119
or the minimum number of pods to support

00:01:20,159 --> 00:01:25,360
a specific

00:01:21,119 --> 00:01:26,720
workload pdbs are also respected by the

00:01:25,360 --> 00:01:29,759
eviction api

00:01:26,720 --> 00:01:32,479
this means anything like drain auto

00:01:29,759 --> 00:01:35,280
scaling or the descheduler can leverage

00:01:32,479 --> 00:01:36,560
disruption budgets finally all your

00:01:35,280 --> 00:01:39,280
favorite controllers

00:01:36,560 --> 00:01:41,520
like deployments replication controllers

00:01:39,280 --> 00:01:45,360
stateful sets and replica sets

00:01:41,520 --> 00:01:47,680
are easily integrated with anything

00:01:45,360 --> 00:01:48,640
there are some caveats involuntary

00:01:47,680 --> 00:01:51,600
disruptions

00:01:48,640 --> 00:01:52,799
like node crashes hardware failures and

00:01:51,600 --> 00:01:54,960
network outages

00:01:52,799 --> 00:01:56,560
cannot be prevented with disruption

00:01:54,960 --> 00:01:58,960
budgets

00:01:56,560 --> 00:02:01,119
equally if you explicitly delete your

00:01:58,960 --> 00:02:02,000
pods or you scale your deployment to

00:02:01,119 --> 00:02:05,200
zero

00:02:02,000 --> 00:02:06,079
pdbs won't help when dealing with single

00:02:05,200 --> 00:02:08,319
replicas

00:02:06,079 --> 00:02:10,160
i would recommend not using disruption

00:02:08,319 --> 00:02:11,920
budgets because they're burdensome on

00:02:10,160 --> 00:02:14,560
the operations team

00:02:11,920 --> 00:02:16,000
for any drain to occur the pdb would

00:02:14,560 --> 00:02:18,160
need to be deleted

00:02:16,000 --> 00:02:20,480
this can lead to things like indefinite

00:02:18,160 --> 00:02:23,360
hanging with drains

00:02:20,480 --> 00:02:24,879
finally do not overlap your selectors if

00:02:23,360 --> 00:02:27,680
you're creating multiple

00:02:24,879 --> 00:02:28,560
disruption budgets this can confuse

00:02:27,680 --> 00:02:33,200
things

00:02:28,560 --> 00:02:35,280
and lead to drains hanging as well

00:02:33,200 --> 00:02:36,480
the requirements are quite simple for

00:02:35,280 --> 00:02:39,200
disruption budgets

00:02:36,480 --> 00:02:40,319
first of all a meaningful name so that

00:02:39,200 --> 00:02:43,760
it's attributable

00:02:40,319 --> 00:02:46,319
to the pods that it oversees secondly

00:02:43,760 --> 00:02:48,879
a match label corresponding to your

00:02:46,319 --> 00:02:51,760
particular controller selector

00:02:48,879 --> 00:02:52,640
the third piece either a min available

00:02:51,760 --> 00:02:55,760
or a max

00:02:52,640 --> 00:02:58,000
unavailable setting min available refers

00:02:55,760 --> 00:02:59,040
to the minimum amount of pods that must

00:02:58,000 --> 00:03:02,080
be available

00:02:59,040 --> 00:03:02,959
where max unavailable refers to the

00:03:02,080 --> 00:03:05,680
maximum

00:03:02,959 --> 00:03:06,400
number of pods that can be deleted at

00:03:05,680 --> 00:03:10,400
any point

00:03:06,400 --> 00:03:13,200
in time the definition looks like this

00:03:10,400 --> 00:03:14,080
as follows first we have our spec

00:03:13,200 --> 00:03:17,680
selector

00:03:14,080 --> 00:03:19,840
match label which is app django ws

00:03:17,680 --> 00:03:22,000
you can see how this corresponds to our

00:03:19,840 --> 00:03:25,200
deployment selector of app

00:03:22,000 --> 00:03:27,920
django ws then we have our

00:03:25,200 --> 00:03:29,360
min available or conversely our max

00:03:27,920 --> 00:03:32,480
unavailable setting

00:03:29,360 --> 00:03:34,640
this can be an integer or a percentage

00:03:32,480 --> 00:03:35,680
when dealing with percentages if the

00:03:34,640 --> 00:03:38,080
number of pods

00:03:35,680 --> 00:03:40,239
is not an even number it automatically

00:03:38,080 --> 00:03:44,480
gets rounded up to the nearest whole

00:03:40,239 --> 00:03:47,840
integer in practice pod distribution

00:03:44,480 --> 00:03:51,040
budgets look as follows you can see

00:03:47,840 --> 00:03:52,159
currently we have three pods our desired

00:03:51,040 --> 00:03:54,720
or min available

00:03:52,159 --> 00:03:55,599
is two meaning we have an allowed

00:03:54,720 --> 00:03:58,799
disruption

00:03:55,599 --> 00:04:01,439
of one if we go ahead and drain

00:03:58,799 --> 00:04:02,239
our first worker you can see the node is

00:04:01,439 --> 00:04:05,760
cordoned

00:04:02,239 --> 00:04:08,239
our pod django ws is evicted

00:04:05,760 --> 00:04:10,720
and then the drain is completed looking

00:04:08,239 --> 00:04:13,760
back at our disruption budget object

00:04:10,720 --> 00:04:14,560
you can see our current is now two our

00:04:13,760 --> 00:04:17,280
desired

00:04:14,560 --> 00:04:17,840
is two meaning our allowed disruptions

00:04:17,280 --> 00:04:20,560
is now

00:04:17,840 --> 00:04:22,960
zero we can no longer tolerate any more

00:04:20,560 --> 00:04:25,520
pods being deleted

00:04:22,960 --> 00:04:27,440
if we try to drain a second worker where

00:04:25,520 --> 00:04:28,000
another one of those django pods is

00:04:27,440 --> 00:04:30,560
running

00:04:28,000 --> 00:04:32,320
you can see that it's going to fail

00:04:30,560 --> 00:04:34,000
because it would violate the pods

00:04:32,320 --> 00:04:36,639
disruption budget

00:04:34,000 --> 00:04:38,560
using a timeout of 10 seconds we can see

00:04:36,639 --> 00:04:41,680
that the drain did not complete

00:04:38,560 --> 00:04:44,000
and we can reevaluate and try again

00:04:41,680 --> 00:04:45,759
to recap encourage your application

00:04:44,000 --> 00:04:48,639
owners to define their operating

00:04:45,759 --> 00:04:51,680
requirements with pod disruption budgets

00:04:48,639 --> 00:04:54,000
leverage your voluntary eviction tools

00:04:51,680 --> 00:04:55,360
such as food control drain for all of

00:04:54,000 --> 00:04:56,400
your maintenance and upgrade

00:04:55,360 --> 00:04:58,639
requirements

00:04:56,400 --> 00:05:01,600
and remember the caveats and watch out

00:04:58,639 --> 00:05:02,960
for the bad practices that we discussed

00:05:01,600 --> 00:05:05,199
thanks for listening to my lightning

00:05:02,960 --> 00:05:08,680
talk and enjoy the rest of kubecon

00:05:05,199 --> 00:05:11,680
cloud native con virtual europe 2020.

00:05:08,680 --> 00:05:11,680

YouTube URL: https://www.youtube.com/watch?v=0AGZ5no6-yo


