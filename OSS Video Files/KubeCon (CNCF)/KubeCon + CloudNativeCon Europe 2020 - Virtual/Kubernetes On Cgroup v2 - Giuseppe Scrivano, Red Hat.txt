Title: Kubernetes On Cgroup v2 - Giuseppe Scrivano, Red Hat
Publication date: 2020-08-28
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Kubernetes On Cgroup v2 - Giuseppe Scrivano, Red Hat 

This talk will go over the current status of cgroups v2 in the Kubernetes and container ecosystem.  Efforts are underway to enable cgroups v2 in containers runtimes and up the stack in Kubernetes so users can benefit from new kernel features such as PSI and have better OOM handling through using projects such as oomd.  Particular focus will be placed on the changes required in the OCI (Open Container Initiative) specifications and how the containers runtime must be adapted to use the new version.

https://sched.co/ZeoS
Captions: 
	00:00:00,080 --> 00:00:04,640
hi everyone i'm joseph esquivel i'm

00:00:01,680 --> 00:00:06,000
working reddit in the containers team

00:00:04,640 --> 00:00:07,759
during this talk i will go through the

00:00:06,000 --> 00:00:09,760
secret linux kernel feature

00:00:07,759 --> 00:00:11,519
give some basic introduction with a

00:00:09,760 --> 00:00:12,559
particular focus on how it's used by

00:00:11,519 --> 00:00:14,480
kubernetes

00:00:12,559 --> 00:00:16,160
i'll go through about c group version

00:00:14,480 --> 00:00:17,680
and what it took to get support for c

00:00:16,160 --> 00:00:20,400
group of v2

00:00:17,680 --> 00:00:20,960
so what are c groups c group is a kernel

00:00:20,400 --> 00:00:23,199
feature

00:00:20,960 --> 00:00:24,800
that allows to set research constraints

00:00:23,199 --> 00:00:26,560
to a group of processes

00:00:24,800 --> 00:00:27,840
it also allows to monitor how much of

00:00:26,560 --> 00:00:30,640
these resources are

00:00:27,840 --> 00:00:31,920
really used its structure has a set of

00:00:30,640 --> 00:00:35,120
different controllers

00:00:31,920 --> 00:00:37,280
each of them specialized for a

00:00:35,120 --> 00:00:40,399
kind of resource so we have controllers

00:00:37,280 --> 00:00:42,719
like cpu to limit the monitor cpu usage

00:00:40,399 --> 00:00:44,000
or the memory controller that is

00:00:42,719 --> 00:00:46,399
specialized with memory

00:00:44,000 --> 00:00:48,399
usage c groups are structured in a

00:00:46,399 --> 00:00:48,960
hierarchical way each c group has a

00:00:48,399 --> 00:00:52,559
parent

00:00:48,960 --> 00:00:54,559
except for the root c group

00:00:52,559 --> 00:00:56,000
they are managed from user space to a

00:00:54,559 --> 00:00:58,239
file system

00:00:56,000 --> 00:00:59,680
so how do we use them whenever you have

00:00:58,239 --> 00:01:01,840
a problem like limiting

00:00:59,680 --> 00:01:03,680
resource usage for a pod or a container

00:01:01,840 --> 00:01:05,760
c groups can help us

00:01:03,680 --> 00:01:07,360
we can solve problems like having

00:01:05,760 --> 00:01:08,000
constraints on how much memory can be

00:01:07,360 --> 00:01:09,920
used

00:01:08,000 --> 00:01:12,000
how much cpu and what cpu cores are

00:01:09,920 --> 00:01:15,600
allowed for some

00:01:12,000 --> 00:01:18,000
containers we can set also weights

00:01:15,600 --> 00:01:20,320
on each c group saying how much time in

00:01:18,000 --> 00:01:22,159
proportion to the other c groups

00:01:20,320 --> 00:01:23,520
it should get when there is cpu

00:01:22,159 --> 00:01:25,840
contention

00:01:23,520 --> 00:01:27,200
we can also limit assets to the os

00:01:25,840 --> 00:01:30,320
devices

00:01:27,200 --> 00:01:33,040
also pits are a finite resource

00:01:30,320 --> 00:01:34,640
so through c groups we can also limit

00:01:33,040 --> 00:01:39,439
how many of them are

00:01:34,640 --> 00:01:43,280
allowed to each container

00:01:39,439 --> 00:01:46,960
uh to give some background sigrpu v1

00:01:43,280 --> 00:01:50,159
the one currently used by kubernetes was

00:01:46,960 --> 00:01:52,479
first developed at google in 2006 and

00:01:50,159 --> 00:01:55,119
uh force finally merged in the upstream

00:01:52,479 --> 00:01:58,479
linux kernel in 2008

00:01:55,119 --> 00:02:03,360
secret v2 followed up

00:01:58,479 --> 00:02:05,360
a decade later and was released in 2016.

00:02:03,360 --> 00:02:07,040
so it's uh it's not really such a novel

00:02:05,360 --> 00:02:09,200
technology it has been around already

00:02:07,040 --> 00:02:11,520
for some years

00:02:09,200 --> 00:02:13,360
the proposal for adding uh secret v2

00:02:11,520 --> 00:02:15,520
supports

00:02:13,360 --> 00:02:16,480
was accepted just a few months ago last

00:02:15,520 --> 00:02:20,000
february

00:02:16,480 --> 00:02:20,000
and the secret pv2 support

00:02:20,319 --> 00:02:26,480
was added for the release 119.

00:02:24,480 --> 00:02:29,520
c groups are not a monolithic feature

00:02:26,480 --> 00:02:32,160
controllers were that the later

00:02:29,520 --> 00:02:33,120
after the initial support for example

00:02:32,160 --> 00:02:36,720
ldma

00:02:33,120 --> 00:02:40,480
was added to c group week 1 even after

00:02:36,720 --> 00:02:44,239
zero pv2 was already released

00:02:40,480 --> 00:02:46,000
so let's start with segreg one

00:02:44,239 --> 00:02:47,840
as i have already stressed it it's a

00:02:46,000 --> 00:02:50,720
group of different controllers each of

00:02:47,840 --> 00:02:54,080
them specialized for a single resource

00:02:50,720 --> 00:02:56,720
configuration separately for each of

00:02:54,080 --> 00:02:56,720
controller

00:02:59,040 --> 00:03:02,400
they have a hierarchical structure in

00:03:01,040 --> 00:03:04,080
the picture we see a few

00:03:02,400 --> 00:03:05,440
c groups created for the memory

00:03:04,080 --> 00:03:07,760
controller a and b

00:03:05,440 --> 00:03:08,640
are at the same level while c is a child

00:03:07,760 --> 00:03:11,599
of a

00:03:08,640 --> 00:03:12,879
so it inherits all the limitations we we

00:03:11,599 --> 00:03:16,000
set for a

00:03:12,879 --> 00:03:17,519
and the likewise d inherits all the

00:03:16,000 --> 00:03:21,200
limitations from

00:03:17,519 --> 00:03:22,400
from c child c groups can restrict

00:03:21,200 --> 00:03:27,200
further the

00:03:22,400 --> 00:03:27,200
limits that were set for the parent

00:03:27,599 --> 00:03:31,040
other controllers might or may not have

00:03:29,760 --> 00:03:32,799
the same urge

00:03:31,040 --> 00:03:34,480
since they are configured separately

00:03:32,799 --> 00:03:37,760
they could have a completely different

00:03:34,480 --> 00:03:40,879
uh tree in practic in practice this

00:03:37,760 --> 00:03:42,400
freedom is never used and and the same

00:03:40,879 --> 00:03:44,640
hierarchy

00:03:42,400 --> 00:03:46,319
with the same naming is mirrored across

00:03:44,640 --> 00:03:50,560
all the

00:03:46,319 --> 00:03:52,400
controllers later we will see how this

00:03:50,560 --> 00:03:56,159
experience led some decisions

00:03:52,400 --> 00:03:58,959
in the c group of v2 design

00:03:56,159 --> 00:03:59,840
since the freedom of having controllers

00:03:58,959 --> 00:04:02,080
configured

00:03:59,840 --> 00:04:03,760
separately bring some difficulties in

00:04:02,080 --> 00:04:06,480
managing them

00:04:03,760 --> 00:04:08,400
while a container is being configured

00:04:06,480 --> 00:04:09,519
it's part of some c groups but not of

00:04:08,400 --> 00:04:12,959
the others

00:04:09,519 --> 00:04:16,239
so setting c groups for a process is not

00:04:12,959 --> 00:04:16,239
an atomic operation

00:04:16,720 --> 00:04:21,919
so cdc group

00:04:20,160 --> 00:04:23,919
there are there are no syscalls or io

00:04:21,919 --> 00:04:24,639
ctrls to configure c groups everything

00:04:23,919 --> 00:04:28,880
is managed

00:04:24,639 --> 00:04:31,040
through the file system

00:04:28,880 --> 00:04:32,800
the c group file system is the api for

00:04:31,040 --> 00:04:35,120
managing c groups

00:04:32,800 --> 00:04:36,400
each controller in secret v1 is a

00:04:35,120 --> 00:04:39,199
different file system

00:04:36,400 --> 00:04:41,120
it must be mounted separately well this

00:04:39,199 --> 00:04:42,960
is not something users must do manually

00:04:41,120 --> 00:04:44,479
usually the operating system takes care

00:04:42,960 --> 00:04:47,520
of setting up all the

00:04:44,479 --> 00:04:50,479
all these mounts and

00:04:47,520 --> 00:04:51,680
we we see how how many mounts there are

00:04:50,479 --> 00:04:54,320
typically for a

00:04:51,680 --> 00:04:55,360
on a c group of one host like each

00:04:54,320 --> 00:04:59,600
controller

00:04:55,360 --> 00:04:59,600
has a different mount

00:05:02,160 --> 00:05:05,440
some useful information is also exposed

00:05:04,240 --> 00:05:08,960
to the

00:05:05,440 --> 00:05:11,440
procfs file system if we want to know

00:05:08,960 --> 00:05:12,320
what uh c group process belongs we can

00:05:11,440 --> 00:05:17,039
look that

00:05:12,320 --> 00:05:18,639
through the uh proc b2c group file

00:05:17,039 --> 00:05:20,639
there is also another file that gives

00:05:18,639 --> 00:05:25,440
more general stats

00:05:20,639 --> 00:05:28,320
about c groups usage

00:05:25,440 --> 00:05:29,759
so how to interact with c groups regular

00:05:28,320 --> 00:05:33,360
file system operations like

00:05:29,759 --> 00:05:35,360
mkdir rename write rmd are used to

00:05:33,360 --> 00:05:36,960
manage them

00:05:35,360 --> 00:05:38,400
to give an id in this table we can see

00:05:36,960 --> 00:05:42,160
what operations are

00:05:38,400 --> 00:05:43,680
allowed to use let's say we are

00:05:42,160 --> 00:05:45,360
running a process and we would like to

00:05:43,680 --> 00:05:48,560
limit its memory usage

00:05:45,360 --> 00:05:52,000
and we want to do this manually

00:05:48,560 --> 00:05:53,840
using directly the c group api

00:05:52,000 --> 00:05:55,600
so the first thing we we have to do is

00:05:53,840 --> 00:05:57,600
to make sure the memory controller is

00:05:55,600 --> 00:06:00,319
mounted

00:05:57,600 --> 00:06:00,800
so under the memory controller we create

00:06:00,319 --> 00:06:03,360
a new

00:06:00,800 --> 00:06:05,120
c group uh for simplicity we'll just

00:06:03,360 --> 00:06:07,840
create it under the

00:06:05,120 --> 00:06:07,840
the root

00:06:09,919 --> 00:06:13,440
then we will move the process we want to

00:06:12,960 --> 00:06:16,240
limit

00:06:13,440 --> 00:06:17,759
under the secret we just created and we

00:06:16,240 --> 00:06:21,919
do that by writing the

00:06:17,759 --> 00:06:23,919
process speed to the cgroup.prox file

00:06:21,919 --> 00:06:25,520
and finally we can set the limits on the

00:06:23,919 --> 00:06:29,600
new c group

00:06:25,520 --> 00:06:31,600
by writing to the memory.limitingbytes

00:06:29,600 --> 00:06:34,880
file

00:06:31,600 --> 00:06:37,600
once the is terminated

00:06:34,880 --> 00:06:38,240
the signal from must be cleaned manually

00:06:37,600 --> 00:06:42,080
through

00:06:38,240 --> 00:06:42,880
rmd so no one is going to do this

00:06:42,080 --> 00:06:45,360
manually but

00:06:42,880 --> 00:06:47,520
these are exactly the steps the

00:06:45,360 --> 00:06:50,560
container runtime

00:06:47,520 --> 00:06:52,400
takes for managing c groups

00:06:50,560 --> 00:06:54,479
i need to quickly introduce quality of

00:06:52,400 --> 00:06:56,720
service classes in kubernetes as it is

00:06:54,479 --> 00:06:58,800
important to understand the next slides

00:06:56,720 --> 00:07:00,479
the cubelet assigns a qs class to each

00:06:58,800 --> 00:07:03,360
bot depending on how

00:07:00,479 --> 00:07:04,960
its limits and requests are set if each

00:07:03,360 --> 00:07:06,000
container in the pod has bought limits

00:07:04,960 --> 00:07:08,479
and requests

00:07:06,000 --> 00:07:09,440
set and they are equal then it gets what

00:07:08,479 --> 00:07:11,520
it asks for

00:07:09,440 --> 00:07:12,639
so it gets assigned the guaranteed qs

00:07:11,520 --> 00:07:14,479
class

00:07:12,639 --> 00:07:16,240
if it's not guaranteed but there is at

00:07:14,479 --> 00:07:19,280
least one limit set

00:07:16,240 --> 00:07:21,440
deposits assigned the boostable qs class

00:07:19,280 --> 00:07:22,960
everything else that is neither

00:07:21,440 --> 00:07:27,360
guaranteed

00:07:22,960 --> 00:07:27,360
nor boostable it's a best effort

00:07:28,880 --> 00:07:32,960
so the human killer strictly speaking

00:07:31,440 --> 00:07:34,880
it's not a c group feature

00:07:32,960 --> 00:07:36,160
but it's used by kubernetes to

00:07:34,880 --> 00:07:39,360
complement

00:07:36,160 --> 00:07:41,840
how it's used c groups actually each

00:07:39,360 --> 00:07:44,400
linux process has a home score

00:07:41,840 --> 00:07:45,919
in out of memory situations the kernel

00:07:44,400 --> 00:07:47,759
will take it into account

00:07:45,919 --> 00:07:50,240
and will prefer terminating processes

00:07:47,759 --> 00:07:52,720
with a higher room score

00:07:50,240 --> 00:07:56,479
it's possible to modify its volume part

00:07:52,720 --> 00:07:56,479
to a configurable adjustment

00:07:57,199 --> 00:08:05,280
uh so uh um scoring kubernetes each

00:08:02,240 --> 00:08:06,960
qs class is assigned a different home

00:08:05,280 --> 00:08:09,199
score

00:08:06,960 --> 00:08:10,800
as we saw earlier the higher the score

00:08:09,199 --> 00:08:12,639
is the most likely the

00:08:10,800 --> 00:08:15,840
processors are going to be terminated by

00:08:12,639 --> 00:08:18,479
the kernel if there is not enough memory

00:08:15,840 --> 00:08:21,360
best effort pods are the first ones to

00:08:18,479 --> 00:08:21,360
be terminated

00:08:21,440 --> 00:08:26,479
guaranteed pods instead have almost the

00:08:23,759 --> 00:08:29,440
same home score as cubelet

00:08:26,479 --> 00:08:31,039
the boostable pods instead gets assigned

00:08:29,440 --> 00:08:35,279
a dynamic

00:08:31,039 --> 00:08:37,760
value that it's calculated

00:08:35,279 --> 00:08:39,440
considering how much memories are

00:08:37,760 --> 00:08:41,599
located to them in proportion to the

00:08:39,440 --> 00:08:44,719
entire

00:08:41,599 --> 00:08:44,719
available memory

00:08:45,839 --> 00:08:49,360
so c groups are used by the cubelet

00:08:48,640 --> 00:08:51,279
together with

00:08:49,360 --> 00:08:53,920
home score to manage and organize the

00:08:51,279 --> 00:08:55,519
pods the hierarchy created by kubernetes

00:08:53,920 --> 00:08:59,519
is quite complex

00:08:55,519 --> 00:09:03,040
there is a cubot c group the dot

00:08:59,519 --> 00:09:06,160
slice suffix is used

00:09:03,040 --> 00:09:09,200
when using the systemd driver

00:09:06,160 --> 00:09:10,880
all pods are under this three group

00:09:09,200 --> 00:09:14,080
it's possible to settle limits for all

00:09:10,880 --> 00:09:14,080
the pods at this level

00:09:14,480 --> 00:09:18,000
there is a mechanism in the cubelet

00:09:16,160 --> 00:09:19,920
called the system reservation

00:09:18,000 --> 00:09:21,519
where some resources are allocated for

00:09:19,920 --> 00:09:24,320
the rest of the system

00:09:21,519 --> 00:09:27,120
when such reservation is in place limits

00:09:24,320 --> 00:09:30,000
for all the pods are configured

00:09:27,120 --> 00:09:31,440
at this level at the cubot c-group so

00:09:30,000 --> 00:09:34,480
that it will affect

00:09:31,440 --> 00:09:38,560
every every bot

00:09:34,480 --> 00:09:42,399
the guaranteed pods are a

00:09:38,560 --> 00:09:45,040
children of the cubs c group

00:09:42,399 --> 00:09:46,320
at the same level like uh as each

00:09:45,040 --> 00:09:49,440
guaranteed pod

00:09:46,320 --> 00:09:52,160
there are two sibling c groups cubs

00:09:49,440 --> 00:09:53,519
best effort and keyboards burstable each

00:09:52,160 --> 00:09:56,640
of them

00:09:53,519 --> 00:09:57,519
contain all the pods for the for the qos

00:09:56,640 --> 00:10:00,399
class

00:09:57,519 --> 00:10:00,399
they refer to

00:10:01,920 --> 00:10:05,120
the the the difference at the c group

00:10:03,839 --> 00:10:07,440
level between uh

00:10:05,120 --> 00:10:08,959
burstable and best effort is that best

00:10:07,440 --> 00:10:12,800
effort gets the lowest

00:10:08,959 --> 00:10:15,600
cpu weight so that on cpu contention

00:10:12,800 --> 00:10:15,600
the best effort

00:10:16,240 --> 00:10:22,560
pods gets the lowest

00:10:19,360 --> 00:10:22,560
amount of cpu

00:10:23,839 --> 00:10:27,360
when we specify a limit in the yam spec

00:10:26,079 --> 00:10:29,760
file

00:10:27,360 --> 00:10:31,600
they will be ultimately written to the c

00:10:29,760 --> 00:10:34,800
group file system as we saw

00:10:31,600 --> 00:10:38,240
earlier until it gets there though

00:10:34,800 --> 00:10:41,360
there are quite a few steps involved

00:10:38,240 --> 00:10:42,959
once the pod is scheduled to unload the

00:10:41,360 --> 00:10:45,360
cube blood handles the request of

00:10:42,959 --> 00:10:48,560
creating the pods

00:10:45,360 --> 00:10:49,120
the limits that are configured for the

00:10:48,560 --> 00:10:52,240
pod

00:10:49,120 --> 00:10:56,720
are passed down to the cri runtime using

00:10:52,240 --> 00:11:01,839
grpc and together with a home score

00:10:56,720 --> 00:11:01,839
value that the cubelet calculated

00:11:03,920 --> 00:11:07,760
the container runtime after it sets up

00:11:06,079 --> 00:11:08,880
the storage and the network for the

00:11:07,760 --> 00:11:11,839
container

00:11:08,880 --> 00:11:13,760
it passed down the configuration to the

00:11:11,839 --> 00:11:17,519
oc i run time

00:11:13,760 --> 00:11:20,160
that it responsible for

00:11:17,519 --> 00:11:20,959
for running and configuring the

00:11:20,160 --> 00:11:25,120
container

00:11:20,959 --> 00:11:25,120
c group so we have

00:11:26,240 --> 00:11:32,399
we have the communication between

00:11:27,839 --> 00:11:34,560
cubelet and cri runtime using gerpis

00:11:32,399 --> 00:11:36,800
then we have the container runtime

00:11:34,560 --> 00:11:38,320
passing the configuration to the ocran

00:11:36,800 --> 00:11:43,360
time

00:11:38,320 --> 00:11:45,839
through a json file

00:11:43,360 --> 00:11:47,040
the ocean run time in the in the steps

00:11:45,839 --> 00:11:50,480
to create

00:11:47,040 --> 00:11:51,120
the container will uh will write this

00:11:50,480 --> 00:11:54,800
values

00:11:51,120 --> 00:11:57,360
to the to the c group file system

00:11:54,800 --> 00:12:00,720
and set also the room score adjustment

00:11:57,360 --> 00:12:00,720
to the proc file system

00:12:01,200 --> 00:12:05,040
the cubelet uses c groups also for

00:12:03,120 --> 00:12:07,120
monitoring resource usages

00:12:05,040 --> 00:12:08,880
and that's done through the advisor the

00:12:07,120 --> 00:12:11,279
advisor is a package embedded

00:12:08,880 --> 00:12:13,200
directly into the cubelet so while the

00:12:11,279 --> 00:12:14,720
container runtime is responsible

00:12:13,200 --> 00:12:17,200
for creating the c groups for the

00:12:14,720 --> 00:12:19,519
various containers c advisor

00:12:17,200 --> 00:12:20,880
uses the same c groups to read

00:12:19,519 --> 00:12:23,360
statistics on what

00:12:20,880 --> 00:12:24,959
resources are used by each of them i

00:12:23,360 --> 00:12:27,360
won't go into the details

00:12:24,959 --> 00:12:28,320
but obviously c advisor also needs to

00:12:27,360 --> 00:12:32,639
understand the c

00:12:28,320 --> 00:12:34,480
groups version used by the host shortly

00:12:32,639 --> 00:12:35,839
it's just another component to take care

00:12:34,480 --> 00:12:39,120
of for the c group

00:12:35,839 --> 00:12:42,160
v to support

00:12:39,120 --> 00:12:44,079
so why do we care about c group we do

00:12:42,160 --> 00:12:45,279
if we can do everything already with a

00:12:44,079 --> 00:12:47,680
single moving one

00:12:45,279 --> 00:12:49,600
well there are some issues with the

00:12:47,680 --> 00:12:52,880
sigrupui one that

00:12:49,600 --> 00:12:53,360
ultimately led to having a new version

00:12:52,880 --> 00:12:57,040
of

00:12:53,360 --> 00:12:59,120
for the for c groups

00:12:57,040 --> 00:13:00,800
as we saw each controller must be

00:12:59,120 --> 00:13:03,200
handled separately

00:13:00,800 --> 00:13:04,160
even if this freedom is never used in

00:13:03,200 --> 00:13:07,680
practice

00:13:04,160 --> 00:13:08,959
it is just another uh additional

00:13:07,680 --> 00:13:12,480
complexity for uh

00:13:08,959 --> 00:13:14,880
no real gain also

00:13:12,480 --> 00:13:16,800
integration with some subsystems is not

00:13:14,880 --> 00:13:20,079
ideal

00:13:16,800 --> 00:13:24,560
and fixing uh some of the ps is not

00:13:20,079 --> 00:13:24,560
possible as it will be a breaking change

00:13:25,120 --> 00:13:30,399
one big issue that doesn't affect

00:13:27,440 --> 00:13:33,360
directly our kubernetes groups today

00:13:30,399 --> 00:13:35,920
is that delegation is not safe it's not

00:13:33,360 --> 00:13:38,240
possible with c group v1 to delegate a

00:13:35,920 --> 00:13:41,600
subtree to a less privileged

00:13:38,240 --> 00:13:44,720
process in a safe way it's also not

00:13:41,600 --> 00:13:47,519
possible to pre-allocate resources

00:13:44,720 --> 00:13:48,000
if we want to pre-allocate for a c group

00:13:47,519 --> 00:13:51,040
a certain

00:13:48,000 --> 00:13:53,760
amount of resources like we like if we

00:13:51,040 --> 00:13:56,480
want to allocate uh

00:13:53,760 --> 00:13:59,600
some memory that's not possible today

00:13:56,480 --> 00:13:59,600
with zero v1

00:14:00,959 --> 00:14:04,720
as we saw the setup is not an atomic

00:14:03,680 --> 00:14:07,920
operation

00:14:04,720 --> 00:14:09,920
since a process must be

00:14:07,920 --> 00:14:12,480
configured for each controller

00:14:09,920 --> 00:14:15,760
separately

00:14:12,480 --> 00:14:17,920
uh another big issue that it's

00:14:15,760 --> 00:14:21,600
affecting kubernetes is that the um

00:14:17,920 --> 00:14:21,600
killer is not c group aware

00:14:22,160 --> 00:14:30,000
if there is a out of memory situation

00:14:26,639 --> 00:14:30,480
the and given different process with the

00:14:30,000 --> 00:14:34,160
same

00:14:30,480 --> 00:14:37,519
um score the kernel will terminate these

00:14:34,160 --> 00:14:40,160
processes without taking into account

00:14:37,519 --> 00:14:44,480
to what c group they belong so what

00:14:40,160 --> 00:14:46,320
happens is that uh

00:14:44,480 --> 00:14:48,399
processes from different containers can

00:14:46,320 --> 00:14:51,120
be terminated

00:14:48,399 --> 00:14:52,399
so leaving the these containers in a

00:14:51,120 --> 00:14:56,000
broken state

00:14:52,399 --> 00:14:58,560
since some process are terminated but

00:14:56,000 --> 00:15:02,639
some others are left running

00:14:58,560 --> 00:15:06,320
ideally what we would uh want is that

00:15:02,639 --> 00:15:09,440
a container so that it's under a

00:15:06,320 --> 00:15:13,040
single c group is terminated like a

00:15:09,440 --> 00:15:14,000
an atomic unit so that all the processes

00:15:13,040 --> 00:15:17,279
in the c group

00:15:14,000 --> 00:15:20,079
are terminated and

00:15:17,279 --> 00:15:22,639
and also we would like to first try

00:15:20,079 --> 00:15:27,839
terminating a single container

00:15:22,639 --> 00:15:27,839
before uh affecting uh more of them

00:15:28,639 --> 00:15:33,199
so let's get to singapore v2 now

00:15:34,000 --> 00:15:39,759
so how we got here uh so fedora

00:15:37,040 --> 00:15:41,120
31 was the first distro to enable secret

00:15:39,759 --> 00:15:43,839
v2 by default

00:15:41,120 --> 00:15:46,480
we took a bet with it but it turned out

00:15:43,839 --> 00:15:49,839
to be a good decision

00:15:46,480 --> 00:15:50,639
this was the turning point since once we

00:15:49,839 --> 00:15:53,040
switched to

00:15:50,639 --> 00:15:55,360
the default for fedora the rest of the

00:15:53,040 --> 00:16:00,000
ecosystem followed

00:15:55,360 --> 00:16:00,000
the cost was uh was high since uh

00:16:02,560 --> 00:16:06,320
since programs missing support for

00:16:04,240 --> 00:16:11,040
secret we do wouldn't work

00:16:06,320 --> 00:16:11,040
on a default fedora 31 installation

00:16:11,360 --> 00:16:15,279
cyrano initially was my side project i

00:16:14,079 --> 00:16:18,240
started working on it

00:16:15,279 --> 00:16:19,519
as a replacement for run c while playing

00:16:18,240 --> 00:16:23,040
with it

00:16:19,519 --> 00:16:26,399
i added support for c group of v2

00:16:23,040 --> 00:16:29,120
and and it gained features

00:16:26,399 --> 00:16:31,759
until we decided to use it by default on

00:16:29,120 --> 00:16:33,680
fedora with podman

00:16:31,759 --> 00:16:37,120
we got a good feed the feedbacks with

00:16:33,680 --> 00:16:41,279
that and it's still used as the default

00:16:37,120 --> 00:16:43,040
oci runtime at the time of the fedora 31

00:16:41,279 --> 00:16:46,000
switch

00:16:43,040 --> 00:16:48,079
kubernetes also gained support for huge

00:16:46,000 --> 00:16:50,320
tlb

00:16:48,079 --> 00:16:52,160
that controller will still missing in

00:16:50,320 --> 00:16:57,279
the in the kernel for c group

00:16:52,160 --> 00:17:00,800
v2 so by the time i got to work on the

00:16:57,279 --> 00:17:03,040
enhancement proposal for the kubernetes

00:17:00,800 --> 00:17:04,160
i also needed to make sure that huge dlp

00:17:03,040 --> 00:17:07,280
was supported in secret

00:17:04,160 --> 00:17:10,880
v2 soi to to add support for

00:17:07,280 --> 00:17:10,880
uh tlb in the kernel

00:17:12,160 --> 00:17:18,000
so why do we need secret v2

00:17:15,760 --> 00:17:19,120
well there are various reasons as we saw

00:17:18,000 --> 00:17:22,799
before with the

00:17:19,120 --> 00:17:26,079
issues related to singapore v1

00:17:22,799 --> 00:17:28,559
the most important uh one

00:17:26,079 --> 00:17:29,760
for uh having the the switch to secret

00:17:28,559 --> 00:17:34,080
v2 is that

00:17:29,760 --> 00:17:37,200
secret one is considered legacy and

00:17:34,080 --> 00:17:39,840
as such is named in the kernel also

00:17:37,200 --> 00:17:40,880
no new features will be added in the

00:17:39,840 --> 00:17:44,160
future so

00:17:40,880 --> 00:17:47,840
this uh this migration had to happen

00:17:44,160 --> 00:17:47,840
at some point

00:17:48,559 --> 00:17:52,559
uh also all the issues we saw before

00:17:51,600 --> 00:17:55,679
with the c group

00:17:52,559 --> 00:17:57,679
v1 are somehow fixed with the group of

00:17:55,679 --> 00:18:01,679
v2

00:17:57,679 --> 00:18:05,130
we have a c group aware whom killer

00:18:01,679 --> 00:18:07,200
so that containers can be terminated as

00:18:05,130 --> 00:18:10,720
[Music]

00:18:07,200 --> 00:18:13,679
atomically and

00:18:10,720 --> 00:18:15,039
my favorite feature that we gained with

00:18:13,679 --> 00:18:17,280
c group of v2

00:18:15,039 --> 00:18:18,160
is that delegation to less privileged

00:18:17,280 --> 00:18:21,360
process

00:18:18,160 --> 00:18:22,880
it's safe what does what does it mean it

00:18:21,360 --> 00:18:26,400
means that

00:18:22,880 --> 00:18:29,280
uh we can create a sub c group

00:18:26,400 --> 00:18:32,000
and well literally shown it to a

00:18:29,280 --> 00:18:36,080
different user

00:18:32,000 --> 00:18:36,080
and let the user fully manage it

00:18:37,440 --> 00:18:44,080
so that an user that it's not root on

00:18:40,880 --> 00:18:47,360
the system can fully manage

00:18:44,080 --> 00:18:50,160
a c group and that's

00:18:47,360 --> 00:18:50,480
and that's safe a different return c

00:18:50,160 --> 00:18:54,960
group

00:18:50,480 --> 00:18:57,440
we want another difference

00:18:54,960 --> 00:18:58,320
uh moving to c group v2 is that on

00:18:57,440 --> 00:19:00,960
companies

00:18:58,320 --> 00:19:02,000
by default we also enable the c group

00:19:00,960 --> 00:19:05,360
namespace

00:19:02,000 --> 00:19:08,160
i will talk more about it later

00:19:05,360 --> 00:19:09,440
so c group v2 there is a single

00:19:08,160 --> 00:19:13,039
hierarchy different

00:19:09,440 --> 00:19:14,480
than v1 and there is also a single mount

00:19:13,039 --> 00:19:16,960
point

00:19:14,480 --> 00:19:19,200
and the process is part of a single c

00:19:16,960 --> 00:19:19,200
group

00:19:20,559 --> 00:19:24,240
some controllers are not entered by c

00:19:22,880 --> 00:19:27,760
group itself

00:19:24,240 --> 00:19:30,559
anymore like the devices controller

00:19:27,760 --> 00:19:33,200
now that it's and it's entered through

00:19:30,559 --> 00:19:33,200
eppf

00:19:33,840 --> 00:19:38,799
the controllers are appropriate property

00:19:36,480 --> 00:19:41,919
of the c group itself

00:19:38,799 --> 00:19:44,000
so now we don't create a c group

00:19:41,919 --> 00:19:45,200
under a controller but we enable a

00:19:44,000 --> 00:19:48,880
controller for the c

00:19:45,200 --> 00:19:51,200
group a controller can be used in a

00:19:48,880 --> 00:19:52,559
c group only fits already enabled in the

00:19:51,200 --> 00:19:57,840
parent

00:19:52,559 --> 00:20:01,440
c group this is done through a new file

00:19:57,840 --> 00:20:01,440
cgroup subtree control

00:20:03,039 --> 00:20:06,880
not everything that was exposed by

00:20:04,720 --> 00:20:09,840
sigrpup v1 is available with c group

00:20:06,880 --> 00:20:12,400
v2 for instance there are some stat

00:20:09,840 --> 00:20:13,440
files like usage per cpu but it's not

00:20:12,400 --> 00:20:16,799
present in c group

00:20:13,440 --> 00:20:19,200
v2 there are also some

00:20:16,799 --> 00:20:21,440
new features that are not available in c

00:20:19,200 --> 00:20:24,320
group v1

00:20:21,440 --> 00:20:25,440
some configuration changed semantics in

00:20:24,320 --> 00:20:27,600
v2

00:20:25,440 --> 00:20:30,320
for example the memory swap limit is

00:20:27,600 --> 00:20:32,320
configured in a different way

00:20:30,320 --> 00:20:33,679
some other configuration kept the same

00:20:32,320 --> 00:20:37,039
semantics as in

00:20:33,679 --> 00:20:39,120
v1 but are using either different names

00:20:37,039 --> 00:20:42,159
or different ranges

00:20:39,120 --> 00:20:42,960
an example is cpu shares that changed

00:20:42,159 --> 00:20:49,440
both the name

00:20:42,960 --> 00:20:53,200
and the range in c group v2

00:20:49,440 --> 00:20:56,880
um so other uh difference with

00:20:53,200 --> 00:20:59,360
single proof v2 the first one it's a

00:20:56,880 --> 00:21:00,080
huge difference to always keep in mind

00:20:59,360 --> 00:21:02,720
with c group

00:21:00,080 --> 00:21:03,200
v2 processor can be added only to leaf

00:21:02,720 --> 00:21:06,400
nodes

00:21:03,200 --> 00:21:06,400
in the hierarchy

00:21:07,280 --> 00:21:12,559
each node in the c group

00:21:10,320 --> 00:21:14,000
hierarchy can either have children c

00:21:12,559 --> 00:21:17,200
groups or

00:21:14,000 --> 00:21:19,600
f processes with c group v1 instead

00:21:17,200 --> 00:21:20,640
process could be moved at any level in

00:21:19,600 --> 00:21:25,840
the tree

00:21:20,640 --> 00:21:25,840
so we we had a c group that was

00:21:26,240 --> 00:21:29,760
having a process to manage but also

00:21:28,720 --> 00:21:32,320
having sub

00:21:29,760 --> 00:21:32,320
c groups

00:21:33,679 --> 00:21:37,600
as i already said before a controller

00:21:35,919 --> 00:21:38,400
must be enabled in all the parent c

00:21:37,600 --> 00:21:42,880
groups before

00:21:38,400 --> 00:21:44,559
it can be used in a in a c group

00:21:42,880 --> 00:21:46,720
originally all the trades had to be in

00:21:44,559 --> 00:21:50,080
the same c group

00:21:46,720 --> 00:21:52,320
the this limitation was relaxed at the

00:21:50,080 --> 00:21:55,679
newer kernels and now it's possible to

00:21:52,320 --> 00:21:59,280
migrate single threats to different

00:21:55,679 --> 00:22:02,320
c groups there is also an

00:21:59,280 --> 00:22:05,600
another cool feature

00:22:02,320 --> 00:22:07,440
ns delegate that was added later

00:22:05,600 --> 00:22:09,520
and that makes the allegation to

00:22:07,440 --> 00:22:14,159
privileged process even safer

00:22:09,520 --> 00:22:14,159
as it enables additional restrictions

00:22:15,280 --> 00:22:19,919
pressure style information is a new c

00:22:17,600 --> 00:22:22,320
group v2 feature

00:22:19,919 --> 00:22:22,960
for each c group it says how much time

00:22:22,320 --> 00:22:25,039
was

00:22:22,960 --> 00:22:26,080
wasted waiting for resources to be

00:22:25,039 --> 00:22:27,919
available

00:22:26,080 --> 00:22:31,120
it gives a good indication when the

00:22:27,919 --> 00:22:35,200
system is not doing enough useful work

00:22:31,120 --> 00:22:35,200
and if a pot must be terminated

00:22:35,919 --> 00:22:41,679
this new feature enables umd then killer

00:22:39,280 --> 00:22:43,520
in user space that it's developed by

00:22:41,679 --> 00:22:47,280
facebook and

00:22:43,520 --> 00:22:51,120
being added to systemd so that

00:22:47,280 --> 00:22:55,840
the hume killer management is moved

00:22:51,120 --> 00:22:55,840
to user space from the from the kernel

00:22:58,960 --> 00:23:02,320
in secret v2 memory limits are

00:23:01,360 --> 00:23:04,480
configured

00:23:02,320 --> 00:23:06,480
through four different files that gives

00:23:04,480 --> 00:23:09,840
a lot of control on

00:23:06,480 --> 00:23:09,840
on memory

00:23:10,320 --> 00:23:17,840
still this configuration is not

00:23:14,240 --> 00:23:19,840
exposed to kubernetes as we

00:23:17,840 --> 00:23:21,120
as we are exposing at the moment just

00:23:19,840 --> 00:23:25,360
the features that were

00:23:21,120 --> 00:23:27,679
also available in c group v1 but

00:23:25,360 --> 00:23:30,880
this this is something to well to

00:23:27,679 --> 00:23:33,760
improve in in the future

00:23:30,880 --> 00:23:34,400
now with c group v2 it's possible to

00:23:33,760 --> 00:23:37,440
specify

00:23:34,400 --> 00:23:39,200
a portion of memory that's never going

00:23:37,440 --> 00:23:44,080
to be reclaimed

00:23:39,200 --> 00:23:44,080
that's configured to the memory.min file

00:23:44,480 --> 00:23:52,240
and there are also other levels

00:23:48,240 --> 00:23:54,640
configurable memory low it's a

00:23:52,240 --> 00:23:55,360
it's a memory that it's reclaimed only

00:23:54,640 --> 00:23:58,480
if there is

00:23:55,360 --> 00:24:01,760
nothing uh reclaimable in other c groups

00:23:58,480 --> 00:24:01,760
then there is memory high

00:24:02,799 --> 00:24:07,200
and the kernel on the wrong run tries to

00:24:05,279 --> 00:24:10,240
keep a memory usage for the c group

00:24:07,200 --> 00:24:13,440
below this limit and then there is uh

00:24:10,240 --> 00:24:15,679
well there is still a heart limit and

00:24:13,440 --> 00:24:18,960
once that is reached the

00:24:15,679 --> 00:24:19,440
process is it's terminated immediately

00:24:18,960 --> 00:24:22,000
by

00:24:19,440 --> 00:24:22,000
the kernel

00:24:23,520 --> 00:24:30,480
this is not a secret feature added

00:24:26,559 --> 00:24:31,520
by secret v2 but this was not used the

00:24:30,480 --> 00:24:35,840
before with cgroup

00:24:31,520 --> 00:24:38,400
v1 as only

00:24:35,840 --> 00:24:40,080
well only newer kernels had support for

00:24:38,400 --> 00:24:42,559
it

00:24:40,080 --> 00:24:43,360
with c group v2 instead we are sure that

00:24:42,559 --> 00:24:45,840
the kernel

00:24:43,360 --> 00:24:46,720
it's new enough to have also the c group

00:24:45,840 --> 00:24:48,400
namespace

00:24:46,720 --> 00:24:50,159
so it makes sense to enable it by

00:24:48,400 --> 00:24:52,240
default

00:24:50,159 --> 00:24:53,200
also this is the right timing for such

00:24:52,240 --> 00:24:58,480
breaking changes

00:24:53,200 --> 00:25:02,480
because anyway we are changing all the

00:24:58,480 --> 00:25:06,720
c group api is exposed to the

00:25:02,480 --> 00:25:06,720
user space and to the containers

00:25:10,320 --> 00:25:14,000
when the container runs in a new c group

00:25:12,159 --> 00:25:17,600
namespace it won't be able to read its

00:25:14,000 --> 00:25:19,360
full c group from the host perspective

00:25:17,600 --> 00:25:21,919
anymore

00:25:19,360 --> 00:25:23,360
it will be limited to the namespace that

00:25:21,919 --> 00:25:26,640
we are just creating

00:25:23,360 --> 00:25:27,679
when a space is created the current c

00:25:26,640 --> 00:25:30,159
group becomes the

00:25:27,679 --> 00:25:30,159
the root

00:25:33,440 --> 00:25:37,520
the ocean time is the component

00:25:35,360 --> 00:25:38,640
responsible for ultimately setting up

00:25:37,520 --> 00:25:41,200
the container

00:25:38,640 --> 00:25:42,720
it's the lowest level in the stack and

00:25:41,200 --> 00:25:44,320
as we saw earlier it gets the

00:25:42,720 --> 00:25:46,159
configuration directly from a

00:25:44,320 --> 00:25:50,400
container's runtime engine such as

00:25:46,159 --> 00:25:50,400
cryo container d as a json file

00:25:51,279 --> 00:25:54,799
as part of the configuration that it

00:25:53,760 --> 00:25:57,440
gets there is a

00:25:54,799 --> 00:26:00,559
resources block that specifies the c

00:25:57,440 --> 00:26:00,559
group limits to use

00:26:01,200 --> 00:26:05,279
on a c group v1 system as we can see on

00:26:03,840 --> 00:26:08,400
the slide

00:26:05,279 --> 00:26:09,520
this configuration maps exactly to the c

00:26:08,400 --> 00:26:11,440
group file system

00:26:09,520 --> 00:26:12,880
there isn't any creative step in the oc

00:26:11,440 --> 00:26:13,760
iron time it just moves the

00:26:12,880 --> 00:26:17,679
configuration

00:26:13,760 --> 00:26:17,679
from the json file to the file system

00:26:17,919 --> 00:26:21,039
there are time specs are designed for c

00:26:20,080 --> 00:26:24,720
group v1

00:26:21,039 --> 00:26:27,440
they don't map nicely to c group v2

00:26:24,720 --> 00:26:28,240
different file names in different ranges

00:26:27,440 --> 00:26:31,039
and some

00:26:28,240 --> 00:26:31,840
times the different semantics are not

00:26:31,039 --> 00:26:35,679
managed

00:26:31,840 --> 00:26:38,720
through this configuration

00:26:35,679 --> 00:26:40,640
the only mechanism for extending the

00:26:38,720 --> 00:26:41,679
ucrm time configuration is true

00:26:40,640 --> 00:26:43,200
annotations

00:26:41,679 --> 00:26:45,520
but that's not really nice for

00:26:43,200 --> 00:26:48,559
structured data and it's not a

00:26:45,520 --> 00:26:50,960
standard it's not it's meant

00:26:48,559 --> 00:26:51,679
only for custom extensions not for

00:26:50,960 --> 00:26:55,200
something like

00:26:51,679 --> 00:26:57,200
sigrupue to support the first two

00:26:55,200 --> 00:26:59,679
problems

00:26:57,200 --> 00:27:00,320
are somehow solved today and i'll show

00:26:59,679 --> 00:27:02,640
you

00:27:00,320 --> 00:27:04,640
how in the next slide for the tiered one

00:27:02,640 --> 00:27:06,559
i opened a proposal for extending the

00:27:04,640 --> 00:27:09,840
runtime specifications

00:27:06,559 --> 00:27:12,240
and adding native support for c group

00:27:09,840 --> 00:27:12,240
v2

00:27:14,720 --> 00:27:20,720
so the problems i introduced before

00:27:18,799 --> 00:27:22,240
are solved today with a mechanism of

00:27:20,720 --> 00:27:25,600
conversion between the

00:27:22,240 --> 00:27:25,919
two versions firstly i had implemented

00:27:25,600 --> 00:27:28,960
it

00:27:25,919 --> 00:27:30,799
for siran and they thought it was a hack

00:27:28,960 --> 00:27:32,799
but well it was good enough to get

00:27:30,799 --> 00:27:35,840
started without worrying

00:27:32,799 --> 00:27:36,559
about changing specifications and the

00:27:35,840 --> 00:27:39,919
rest of the

00:27:36,559 --> 00:27:43,120
stack an example is the

00:27:39,919 --> 00:27:46,080
cpu shares file that

00:27:43,120 --> 00:27:46,880
if we get the configuration from c group

00:27:46,080 --> 00:27:50,480
week one

00:27:46,880 --> 00:27:53,440
that so the range

00:27:50,480 --> 00:27:54,320
as the c group v1 valid range is

00:27:53,440 --> 00:27:57,360
converted to

00:27:54,320 --> 00:28:00,000
cgroup v2

00:27:57,360 --> 00:28:01,360
so it's written to a different file and

00:28:00,000 --> 00:28:03,840
using a different

00:28:01,360 --> 00:28:03,840
range

00:28:07,200 --> 00:28:11,679
advantages of such mechanism well is

00:28:10,080 --> 00:28:13,919
that a container engine

00:28:11,679 --> 00:28:15,360
requires minimal changes as it can

00:28:13,919 --> 00:28:19,440
generate exactly the same

00:28:15,360 --> 00:28:21,679
configuration for the oci runtime

00:28:19,440 --> 00:28:23,600
same thing for the cubelet that can pass

00:28:21,679 --> 00:28:26,960
down the the same limits

00:28:23,600 --> 00:28:30,240
as before as they will be converted

00:28:26,960 --> 00:28:33,600
later anyway

00:28:30,240 --> 00:28:36,880
uh run c now performs the

00:28:33,600 --> 00:28:40,799
the same conversions so there are

00:28:36,880 --> 00:28:45,840
there are two oc iron times uh using the

00:28:40,799 --> 00:28:45,840
this mechanism now

00:28:46,080 --> 00:28:53,440
so what's next well we reached the

00:28:49,520 --> 00:28:56,640
features parrot with cgroup v1 now

00:28:53,440 --> 00:28:57,440
uh probably the first next step is to

00:28:56,640 --> 00:29:00,720
get

00:28:57,440 --> 00:29:03,919
the runtime specs changes

00:29:00,720 --> 00:29:07,120
and once we have that we can add

00:29:03,919 --> 00:29:10,720
native support through the stack

00:29:07,120 --> 00:29:11,440
starting from the starting from the oca

00:29:10,720 --> 00:29:14,799
runtime

00:29:11,440 --> 00:29:16,640
and and extending

00:29:14,799 --> 00:29:18,080
containers runtime and kubernetes to

00:29:16,640 --> 00:29:21,600
take advantage of the

00:29:18,080 --> 00:29:24,000
new features we have with c group

00:29:21,600 --> 00:29:24,000
v2

00:29:25,360 --> 00:29:33,120
c group v2 enables some useful

00:29:28,880 --> 00:29:37,919
new use cases for kubernetes

00:29:33,120 --> 00:29:40,720
as i was saying before delegation to

00:29:37,919 --> 00:29:40,720
less privileged

00:29:41,120 --> 00:29:46,960
processes it's it's safe now

00:29:44,159 --> 00:29:48,320
so what is possible for kubernetes that

00:29:46,960 --> 00:29:50,720
it it will be possible to have

00:29:48,320 --> 00:29:52,840
nested the kubernetes with a full c

00:29:50,720 --> 00:29:55,440
group

00:29:52,840 --> 00:29:57,760
support and that's

00:29:55,440 --> 00:29:59,440
and that can be done in a safe way since

00:29:57,760 --> 00:30:02,559
the

00:29:59,440 --> 00:30:08,320
nested environment cannot affect

00:30:02,559 --> 00:30:12,960
the outside limits

00:30:08,320 --> 00:30:12,960

YouTube URL: https://www.youtube.com/watch?v=u8h0e84HxcE


