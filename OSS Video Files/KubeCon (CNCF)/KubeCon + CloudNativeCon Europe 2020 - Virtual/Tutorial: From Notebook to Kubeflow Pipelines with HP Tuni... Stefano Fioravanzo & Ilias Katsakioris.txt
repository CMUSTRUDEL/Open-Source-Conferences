Title: Tutorial: From Notebook to Kubeflow Pipelines with HP Tuni... Stefano Fioravanzo & Ilias Katsakioris
Publication date: 2020-08-27
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Tutorial: From Notebook to Kubeflow Pipelines with HP Tuning: A Data Science Journey - Stefano Fioravanzo & Ilias Katsakioris, Arrikto 

An introduction to Kubeflow, the ML toolkit for K8s and the workflows you can use as a data scientist to scale up your ML code effortlessly.  Ever thought how hard it is to convert your Jupyter Notebooks into deployable and composable pipelines, scale up computation and run hyperparameter tuning? With Kubeflow, this process becomes extremely easy as you make use of the many components of this ML toolkit: Pipelines, Kale, Katib, Snapshot Store.  You will learn how to deploy Kubeflow in minutes, explore your ML code inside a Jupyter Notebook, convert it to a composable and scalable workflow with the click of a button, make the pipeline reproducible using immutable snapshots, go back in history and debug it, run hyperparameter tuning and distribute your computation.  Did we mention you won’t need any specific SDK or CLI command to do this? Sounds like magic? Come and see for yourself!

https://sched.co/ZerG
Captions: 
	00:00:00,000 --> 00:00:03,120
hello everyone and welcome to our

00:00:01,680 --> 00:00:05,680
session from notebook tiki flow

00:00:03,120 --> 00:00:08,720
pipelines with hyperparameters unit

00:00:05,680 --> 00:00:10,400
in this session we will be showing a

00:00:08,720 --> 00:00:12,160
complete data science workflow

00:00:10,400 --> 00:00:13,599
for optimizing your models and we will

00:00:12,160 --> 00:00:17,039
be using jupter notebooks

00:00:13,599 --> 00:00:19,760
jail cardip and ufo pipelines

00:00:17,039 --> 00:00:20,400
this is us i'm miles kazakiewis software

00:00:19,760 --> 00:00:24,480
engineer at

00:00:20,400 --> 00:00:27,840
rito hi everyone i'm stefano filabanzo

00:00:24,480 --> 00:00:29,519
software engineer at eric as well

00:00:27,840 --> 00:00:31,439
at this point we would like to give some

00:00:29,519 --> 00:00:36,000
credit to sarah maddox from google

00:00:31,439 --> 00:00:36,000
who helped us dance with this tutorial

00:00:36,640 --> 00:00:42,079
what's in it for you in this session

00:00:39,760 --> 00:00:43,360
what you learned by the end of the

00:00:42,079 --> 00:00:44,800
session you will have converted the

00:00:43,360 --> 00:00:46,000
jupiter notebook to you flow pipeline

00:00:44,800 --> 00:00:47,840
with scale

00:00:46,000 --> 00:00:49,200
this makes your workflow a lot easier

00:00:47,840 --> 00:00:51,840
and faster

00:00:49,200 --> 00:00:52,320
you will have performed by pipeline

00:00:51,840 --> 00:00:55,440
based

00:00:52,320 --> 00:00:57,520
hyperparameter tuning with caching which

00:00:55,440 --> 00:00:59,199
makes your pipelines run faster

00:00:57,520 --> 00:01:02,239
everything will be simple since we will

00:00:59,199 --> 00:01:03,840
be using intuitive uis for everything

00:01:02,239 --> 00:01:05,519
you will also accelerate your machine

00:01:03,840 --> 00:01:07,040
learning life cycle with j as an

00:01:05,519 --> 00:01:08,799
orchestration tool

00:01:07,040 --> 00:01:11,040
and finally you will gain complete

00:01:08,799 --> 00:01:12,880
visibility of your training

00:01:11,040 --> 00:01:14,560
you can find a slide deck following this

00:01:12,880 --> 00:01:17,680
link here where you are also

00:01:14,560 --> 00:01:19,200
able to win some cool prizes although

00:01:17,680 --> 00:01:21,520
this is a recorded session

00:01:19,200 --> 00:01:24,640
we have live dna on twitter and linkedin

00:01:21,520 --> 00:01:24,640
using these hashtags

00:01:25,119 --> 00:01:28,720
the machine learning platform we are

00:01:26,479 --> 00:01:30,320
going to use is 2flow

00:01:28,720 --> 00:01:32,159
for those who are not familiar with it

00:01:30,320 --> 00:01:33,759
qflo is an open source project dedicated

00:01:32,159 --> 00:01:34,799
to making the deployment of machine

00:01:33,759 --> 00:01:36,640
learning workflows

00:01:34,799 --> 00:01:38,320
on kubernetes simple portable and

00:01:36,640 --> 00:01:41,040
scalable

00:01:38,320 --> 00:01:42,000
and how can you use it you can deploy

00:01:41,040 --> 00:01:43,840
and manage

00:01:42,000 --> 00:01:46,240
complex machine learning systems at

00:01:43,840 --> 00:01:49,040
scale you can experiment rapidly

00:01:46,240 --> 00:01:50,079
you can perform hyper parameter tuning

00:01:49,040 --> 00:01:52,880
you can deploy

00:01:50,079 --> 00:01:54,240
hybrid and multi-cloud workloads and you

00:01:52,880 --> 00:01:56,479
can have the show coveted

00:01:54,240 --> 00:01:59,040
feature of cicd continuous integration

00:01:56,479 --> 00:02:00,880
and deployment

00:01:59,040 --> 00:02:02,399
this is a ten thousand feet overview of

00:02:00,880 --> 00:02:05,520
machine learning as an

00:02:02,399 --> 00:02:07,280
as an ml engineer would look at it

00:02:05,520 --> 00:02:09,840
machine learning requires some tools and

00:02:07,280 --> 00:02:11,760
frameworks for data scientists

00:02:09,840 --> 00:02:13,680
tubeflow encapsulates applications and

00:02:11,760 --> 00:02:15,760
services to run those tools on top of

00:02:13,680 --> 00:02:18,319
kubernetes clusters

00:02:15,760 --> 00:02:20,640
and kubernetes runs on prem or on any

00:02:18,319 --> 00:02:20,640
cloud

00:02:21,360 --> 00:02:26,000
and this is a machine learning workflow

00:02:23,120 --> 00:02:28,160
as a data scientist would experience it

00:02:26,000 --> 00:02:29,680
data science begins with identifying the

00:02:28,160 --> 00:02:31,200
problem and collecting and analyzing

00:02:29,680 --> 00:02:33,440
data

00:02:31,200 --> 00:02:35,040
then the data scientist has to choose a

00:02:33,440 --> 00:02:37,840
machine learning algorithm and code

00:02:35,040 --> 00:02:37,840
their models

00:02:38,080 --> 00:02:41,120
subsequently they have to experiment

00:02:40,800 --> 00:02:44,319
with

00:02:41,120 --> 00:02:46,000
data and model training then they

00:02:44,319 --> 00:02:46,800
optimize their model with hyperparameter

00:02:46,000 --> 00:02:48,480
zooming

00:02:46,800 --> 00:02:50,800
and of course in the very end they need

00:02:48,480 --> 00:02:52,400
to serve the model with the best result

00:02:50,800 --> 00:02:55,040
tube flow has various components to

00:02:52,400 --> 00:02:57,120
achieve the all the aforementioned

00:02:55,040 --> 00:02:58,239
in this session we will focus on jupiter

00:02:57,120 --> 00:03:01,440
notebooks jail

00:02:58,239 --> 00:03:04,000
cardip and zip flow pipelines

00:03:01,440 --> 00:03:05,920
how can someone use duplo you have a

00:03:04,000 --> 00:03:08,400
graphical user interface a gui

00:03:05,920 --> 00:03:10,640
to manage notebooks volumes snapshots

00:03:08,400 --> 00:03:11,920
pipelines etc

00:03:10,640 --> 00:03:14,400
this is an image of the central

00:03:11,920 --> 00:03:14,400
dashboard

00:03:14,640 --> 00:03:17,680
also you have two command line

00:03:16,800 --> 00:03:21,440
interfaces

00:03:17,680 --> 00:03:24,159
jf cattle and cubecad and of course

00:03:21,440 --> 00:03:25,360
tubeflow has its own apis and sdks and

00:03:24,159 --> 00:03:27,920
here are some examples

00:03:25,360 --> 00:03:30,159
the pipeline sdk cadib api and metadata

00:03:27,920 --> 00:03:31,360
sdk

00:03:30,159 --> 00:03:33,200
there's a perception that machine

00:03:31,360 --> 00:03:35,360
learning is all about model code and

00:03:33,200 --> 00:03:36,720
complex math but in reality machine

00:03:35,360 --> 00:03:38,400
learning applications are complex

00:03:36,720 --> 00:03:41,599
distributed systems

00:03:38,400 --> 00:03:44,799
they acquire lots of devops and various

00:03:41,599 --> 00:03:47,920
components for monitoring serving

00:03:44,799 --> 00:03:49,760
managing the resources etc ubflo

00:03:47,920 --> 00:03:52,080
containerizes all these components

00:03:49,760 --> 00:03:53,599
so that you can run them on kubernetes

00:03:52,080 --> 00:03:56,480
thus you have a uniform way

00:03:53,599 --> 00:03:57,200
to run your workloads everywhere and at

00:03:56,480 --> 00:04:01,840
this point

00:03:57,200 --> 00:04:01,840
i'll hand over to stefano

00:04:04,239 --> 00:04:11,920
thank you alias so now you've seen

00:04:08,480 --> 00:04:13,680
how kubeflow can accelerate and simplify

00:04:11,920 --> 00:04:15,280
the whole orchestration process of

00:04:13,680 --> 00:04:19,120
running machine learning

00:04:15,280 --> 00:04:20,959
workflows at scale um in this tutorial

00:04:19,120 --> 00:04:22,800
we're going to specifically focus on the

00:04:20,959 --> 00:04:25,120
data science experience

00:04:22,800 --> 00:04:25,919
so how keyflow and the tools we're going

00:04:25,120 --> 00:04:29,440
to present

00:04:25,919 --> 00:04:32,160
will enable to dramatically simplify

00:04:29,440 --> 00:04:33,280
and accelerate the data science workflow

00:04:32,160 --> 00:04:35,840
starting from

00:04:33,280 --> 00:04:36,800
your development environment that is

00:04:35,840 --> 00:04:39,919
most often

00:04:36,800 --> 00:04:42,960
jupiter jupiter notebooks then

00:04:39,919 --> 00:04:43,520
how kale as a workflow tool will enable

00:04:42,960 --> 00:04:45,520
you

00:04:43,520 --> 00:04:47,600
to create pipelines seamlessly

00:04:45,520 --> 00:04:48,560
seamlessly and run them in cubesat

00:04:47,600 --> 00:04:51,360
pipelines

00:04:48,560 --> 00:04:52,240
and then while and then while doing all

00:04:51,360 --> 00:04:56,240
of these

00:04:52,240 --> 00:04:58,560
rock will enable you to close the circle

00:04:56,240 --> 00:05:00,880
go all the way back from running to

00:04:58,560 --> 00:05:02,440
developing

00:05:00,880 --> 00:05:04,800
essentially with complete

00:05:02,440 --> 00:05:11,840
reproducibility enabling you

00:05:04,800 --> 00:05:11,840
to explore your pipelines back in time

00:05:13,039 --> 00:05:19,919
so we we're talking about

00:05:16,800 --> 00:05:22,960
um pipelines uh as the

00:05:19,919 --> 00:05:26,479
as the core um workflow

00:05:22,960 --> 00:05:29,840
tool in kubeflow this is because um

00:05:26,479 --> 00:05:30,960
by data science is inherently a pipeline

00:05:29,840 --> 00:05:33,919
process

00:05:30,960 --> 00:05:36,320
usually when you do data science your

00:05:33,919 --> 00:05:38,880
job your work consists of

00:05:36,320 --> 00:05:40,560
several uh independent steps that come

00:05:38,880 --> 00:05:42,880
one after the other

00:05:40,560 --> 00:05:45,039
this may be data ingestion and then

00:05:42,880 --> 00:05:48,720
transformation validation

00:05:45,039 --> 00:05:51,919
afterwards you do model trading um

00:05:48,720 --> 00:05:53,759
either locally or at scale and you end

00:05:51,919 --> 00:05:56,080
up serving and monitoring

00:05:53,759 --> 00:05:57,039
so all of these steps can be combined

00:05:56,080 --> 00:06:00,720
together

00:05:57,039 --> 00:06:03,919
in pipelines now this workshop

00:06:00,720 --> 00:06:07,280
will focus on how to simplify

00:06:03,919 --> 00:06:08,400
as much as possible the building process

00:06:07,280 --> 00:06:11,440
of a pipeline

00:06:08,400 --> 00:06:14,720
directly from your local environment

00:06:11,440 --> 00:06:17,919
via a graphical based approach

00:06:14,720 --> 00:06:20,960
and how to completely

00:06:17,919 --> 00:06:21,600
automate data versioning to enable to

00:06:20,960 --> 00:06:24,240
enable

00:06:21,600 --> 00:06:24,960
reproducibility and collaboration of

00:06:24,240 --> 00:06:27,120
course

00:06:24,960 --> 00:06:28,080
the tools that will make this possible

00:06:27,120 --> 00:06:31,199
are kale

00:06:28,080 --> 00:06:31,199
and erictosrock

00:06:31,840 --> 00:06:38,880
now i've been talking about

00:06:35,039 --> 00:06:42,160
converting a notebook to a pipeline so

00:06:38,880 --> 00:06:45,280
this is this makes a lot of sense

00:06:42,160 --> 00:06:47,600
because if you think about it when you

00:06:45,280 --> 00:06:50,240
work in a jupiter notebook

00:06:47,600 --> 00:06:51,280
you are already defining a sort of

00:06:50,240 --> 00:06:54,400
workflow

00:06:51,280 --> 00:06:55,360
that is pipeline based all of your cells

00:06:54,400 --> 00:06:59,720
in the notebook

00:06:55,360 --> 00:07:02,720
can be thought as as um

00:06:59,720 --> 00:07:05,680
isolated steps

00:07:02,720 --> 00:07:07,199
that can be even combined together and

00:07:05,680 --> 00:07:08,880
attached to one another with

00:07:07,199 --> 00:07:11,840
dependencies

00:07:08,880 --> 00:07:12,880
this process makes it very easy to

00:07:11,840 --> 00:07:16,080
parallelize

00:07:12,880 --> 00:07:18,880
and isolate your code at scale so

00:07:16,080 --> 00:07:20,160
you can you could even easily run hyper

00:07:18,880 --> 00:07:23,919
parameter tuning

00:07:20,160 --> 00:07:27,199
over a notebook and its steps

00:07:23,919 --> 00:07:30,479
and then also it becomes

00:07:27,199 --> 00:07:31,120
very easy when running pipelines in the

00:07:30,479 --> 00:07:34,039
cube

00:07:31,120 --> 00:07:35,280
pipelines to apply data versioning and

00:07:34,039 --> 00:07:37,360
reproducibility

00:07:35,280 --> 00:07:38,400
to all of these independent steps and

00:07:37,360 --> 00:07:40,479
you'll see how

00:07:38,400 --> 00:07:41,840
and you'll see later how this can be

00:07:40,479 --> 00:07:44,879
achieved

00:07:41,840 --> 00:07:47,120
and the various parts of your notebook

00:07:44,879 --> 00:07:50,479
that become independent steps might

00:07:47,120 --> 00:07:54,240
might have different requirements so a

00:07:50,479 --> 00:07:57,199
data processing step might be cpu bound

00:07:54,240 --> 00:07:58,639
so it will need a certain kind of

00:07:57,199 --> 00:08:01,840
hardware resources

00:07:58,639 --> 00:08:02,479
while another part of your code that

00:08:01,840 --> 00:08:05,360
might be

00:08:02,479 --> 00:08:06,720
doing deep learning training would

00:08:05,360 --> 00:08:09,360
benefit from

00:08:06,720 --> 00:08:10,639
running in a node with a gpu and all of

00:08:09,360 --> 00:08:14,400
this becomes

00:08:10,639 --> 00:08:14,400
feasible with kale and rock

00:08:14,960 --> 00:08:20,319
let's take a look at how these

00:08:18,000 --> 00:08:22,319
tools combined together with kubeflow

00:08:20,319 --> 00:08:25,360
dramatically improve

00:08:22,319 --> 00:08:28,000
your development workflow and lifecycle

00:08:25,360 --> 00:08:29,360
so traditionally you would write some

00:08:28,000 --> 00:08:32,159
machine learning code

00:08:29,360 --> 00:08:33,599
maybe in your notebook but then once

00:08:32,159 --> 00:08:35,839
you're ready to

00:08:33,599 --> 00:08:37,360
build a pipeline out of it and scale it

00:08:35,839 --> 00:08:39,760
up you would need to

00:08:37,360 --> 00:08:41,760
create a new docker image where you

00:08:39,760 --> 00:08:45,600
package all of your dependencies

00:08:41,760 --> 00:08:48,399
and your code then you need to take your

00:08:45,600 --> 00:08:51,519
original code your machine learning code

00:08:48,399 --> 00:08:54,160
and actually write a pipeline with some

00:08:51,519 --> 00:08:56,240
specific sdk for example the q flow

00:08:54,160 --> 00:08:58,640
pipelines dsl

00:08:56,240 --> 00:08:59,360
combine this pipeline upload it and run

00:08:58,640 --> 00:09:02,320
it

00:08:59,360 --> 00:09:03,760
whenever you need to go back because

00:09:02,320 --> 00:09:06,240
maybe you made a mistake

00:09:03,760 --> 00:09:07,120
or you need to improve your own code you

00:09:06,240 --> 00:09:09,519
need to go

00:09:07,120 --> 00:09:11,839
through all of these cycles this cycle

00:09:09,519 --> 00:09:14,240
again you need to write again your code

00:09:11,839 --> 00:09:15,200
repackage it into a docker image again

00:09:14,240 --> 00:09:18,000
and again

00:09:15,200 --> 00:09:19,360
but now with cadence rock all of these

00:09:18,000 --> 00:09:21,600
processes

00:09:19,360 --> 00:09:23,120
process becomes much simpler because you

00:09:21,600 --> 00:09:24,320
just need to write the code in the

00:09:23,120 --> 00:09:28,000
notebook

00:09:24,320 --> 00:09:30,399
tag the notebook with the ui driven way

00:09:28,000 --> 00:09:31,440
you'll see later how and with the click

00:09:30,399 --> 00:09:35,040
of a button

00:09:31,440 --> 00:09:37,279
just run it whenever you need to comment

00:09:35,040 --> 00:09:39,200
your code

00:09:37,279 --> 00:09:41,920
it's seamless just write it in the

00:09:39,200 --> 00:09:45,120
notebook and click the button again

00:09:41,920 --> 00:09:48,080
you can already think how this workflow

00:09:45,120 --> 00:09:48,800
dramatically improves your iteration

00:09:48,080 --> 00:09:51,920
time and

00:09:48,800 --> 00:09:54,959
how much faster allows you

00:09:51,920 --> 00:09:54,959
to be productive

00:09:55,200 --> 00:10:00,640
okay so

00:09:58,240 --> 00:10:01,519
besides converting notebooks to single

00:10:00,640 --> 00:10:03,360
pipelines

00:10:01,519 --> 00:10:06,160
we look also at how to run hyper

00:10:03,360 --> 00:10:08,959
parameter tuning optimizations

00:10:06,160 --> 00:10:09,760
in a data science world it is usually

00:10:08,959 --> 00:10:12,480
the case

00:10:09,760 --> 00:10:13,279
that you are running machine learning

00:10:12,480 --> 00:10:17,440
algorithms

00:10:13,279 --> 00:10:20,480
that are dependent on some parameters

00:10:17,440 --> 00:10:23,120
so what you end up doing after writing

00:10:20,480 --> 00:10:24,480
your first iteration of an algorithm is

00:10:23,120 --> 00:10:25,600
starting to tinkering with the

00:10:24,480 --> 00:10:29,279
parameters

00:10:25,600 --> 00:10:31,760
and analyzing by yourself

00:10:29,279 --> 00:10:33,920
the produced metrics this is of course a

00:10:31,760 --> 00:10:37,120
very time consuming process

00:10:33,920 --> 00:10:37,920
and also our problem but then there are

00:10:37,120 --> 00:10:42,959
ways

00:10:37,920 --> 00:10:46,640
to automate all of these using some

00:10:42,959 --> 00:10:50,079
automatic tools that basically

00:10:46,640 --> 00:10:53,120
use your algorithm and search

00:10:50,079 --> 00:10:55,440
through a given search space the

00:10:53,120 --> 00:10:58,240
parameters that you give in

00:10:55,440 --> 00:10:59,279
automatically with some specific

00:10:58,240 --> 00:11:02,399
mathematical

00:10:59,279 --> 00:11:04,720
optimizations techniques and

00:11:02,399 --> 00:11:06,240
optimize over the metrics that you want

00:11:04,720 --> 00:11:08,320
to optimize

00:11:06,240 --> 00:11:10,480
in the case of qflo katip is the

00:11:08,320 --> 00:11:13,200
official hyperparameter tuner

00:11:10,480 --> 00:11:14,720
it supports several machine learning

00:11:13,200 --> 00:11:15,920
frameworks including including

00:11:14,720 --> 00:11:20,000
tensorflow

00:11:15,920 --> 00:11:23,839
mx and mxnet and others and

00:11:20,000 --> 00:11:27,040
today what we are going to see is how

00:11:23,839 --> 00:11:30,399
you can go from a notebook

00:11:27,040 --> 00:11:33,760
to kale to katib so

00:11:30,399 --> 00:11:36,320
scaling up your notebook as pipeline

00:11:33,760 --> 00:11:38,160
with hyperparameter tuning with just a

00:11:36,320 --> 00:11:40,800
click of a button

00:11:38,160 --> 00:11:42,160
and with this i will hand it over back

00:11:40,800 --> 00:11:45,040
to ilias who will

00:11:42,160 --> 00:11:46,880
start walking you through the actual

00:11:45,040 --> 00:11:50,240
tutorial

00:11:46,880 --> 00:11:51,760
thank you stefano to follow the tutorial

00:11:50,240 --> 00:11:53,600
please go to this link to find the

00:11:51,760 --> 00:11:56,320
corresponding code lab that contains

00:11:53,600 --> 00:11:58,160
step by step instructions this is the

00:11:56,320 --> 00:12:00,399
agenda of the tutorial

00:11:58,160 --> 00:12:02,079
we will install them in af then we will

00:12:00,399 --> 00:12:02,720
get some already cooked machine learning

00:12:02,079 --> 00:12:05,279
code

00:12:02,720 --> 00:12:07,120
explore it convert it through pipeline

00:12:05,279 --> 00:12:09,120
and then perform hyper parameter tuning

00:12:07,120 --> 00:12:11,920
on a specific model

00:12:09,120 --> 00:12:13,360
the first step is to install minik af

00:12:11,920 --> 00:12:16,800
but let's see what mini af

00:12:13,360 --> 00:12:19,519
is mini af is the easiest way

00:12:16,800 --> 00:12:21,040
to deploy an experiment with tube flow

00:12:19,519 --> 00:12:23,680
in just a few minutes

00:12:21,040 --> 00:12:25,760
you can have it on gcp on your laptop or

00:12:23,680 --> 00:12:27,760
any on-prem infrastructure

00:12:25,760 --> 00:12:29,680
it is an all-in-one single node heap

00:12:27,760 --> 00:12:32,399
flow distribution

00:12:29,680 --> 00:12:33,120
and mini af mini clip flow is actually

00:12:32,399 --> 00:12:35,920
mini cube

00:12:33,120 --> 00:12:39,200
with kubeflow and our software rok a

00:12:35,920 --> 00:12:42,160
data management platform

00:12:39,200 --> 00:12:42,959
this image is taken from the tfx paper

00:12:42,160 --> 00:12:44,240
tube flow

00:12:42,959 --> 00:12:45,920
brings all these parts together in a

00:12:44,240 --> 00:12:47,440
container of the way and gives you a

00:12:45,920 --> 00:12:48,720
nice way to perform hyper parameter

00:12:47,440 --> 00:12:50,560
streaming

00:12:48,720 --> 00:12:52,399
moreover cubeshop provides you with an

00:12:50,560 --> 00:12:53,839
intuitive ui to manage all your

00:12:52,399 --> 00:12:55,519
components

00:12:53,839 --> 00:12:57,040
as we have already mentioned to your

00:12:55,519 --> 00:12:59,040
ransomware kubernetes

00:12:57,040 --> 00:13:00,639
which takes care of duplicates of

00:12:59,040 --> 00:13:02,639
orchestration

00:13:00,639 --> 00:13:03,839
but then you also need to manage your

00:13:02,639 --> 00:13:05,839
storage

00:13:03,839 --> 00:13:08,000
you can use many open source tools for

00:13:05,839 --> 00:13:10,320
your cloud provider's object store

00:13:08,000 --> 00:13:13,200
but mini-af comes with rok our data

00:13:10,320 --> 00:13:13,200
management platform

00:13:13,360 --> 00:13:16,880
we have made a lot of contributions to

00:13:14,800 --> 00:13:18,399
kubeflow because of our expertise in

00:13:16,880 --> 00:13:20,160
storage and data management

00:13:18,399 --> 00:13:21,440
we have extended tube flow making it

00:13:20,160 --> 00:13:23,200
data aware

00:13:21,440 --> 00:13:24,720
you contributed code so that your flow

00:13:23,200 --> 00:13:26,560
can use kubernetes pvcs

00:13:24,720 --> 00:13:28,000
no matter where where is deployed on

00:13:26,560 --> 00:13:30,880
your laptop on prem

00:13:28,000 --> 00:13:31,839
or a public cloud rock takes care of

00:13:30,880 --> 00:13:33,920
data versioning

00:13:31,839 --> 00:13:36,320
and syncs your data across all different

00:13:33,920 --> 00:13:38,079
infrastructures

00:13:36,320 --> 00:13:40,079
let's see why data versioning is

00:13:38,079 --> 00:13:41,680
important machine learning

00:13:40,079 --> 00:13:43,519
let's say we have pipeline where the

00:13:41,680 --> 00:13:46,160
first step is data validation

00:13:43,519 --> 00:13:47,920
when the step is done we take a snapshot

00:13:46,160 --> 00:13:49,760
and we do the same for the next step

00:13:47,920 --> 00:13:52,880
data pre-processing

00:13:49,760 --> 00:13:55,040
then another step runs but it fails how

00:13:52,880 --> 00:13:57,519
are we going to debug

00:13:55,040 --> 00:13:59,519
we can clone the latest snapshot explore

00:13:57,519 --> 00:14:01,920
the data and code

00:13:59,519 --> 00:14:04,399
and then fix the error and continue the

00:14:01,920 --> 00:14:04,399
pipeline

00:14:06,399 --> 00:14:09,920
let's now set up our mini kf

00:14:10,000 --> 00:14:16,959
this is the code lab you found before

00:14:14,399 --> 00:14:20,000
here's introduction related to the

00:14:16,959 --> 00:14:20,000
workload we're going to follow

00:14:20,639 --> 00:14:30,639
here are various information for those

00:14:24,000 --> 00:14:33,120
who are not familiar with google cloud

00:14:30,639 --> 00:14:33,120
finally

00:14:33,680 --> 00:14:37,199
let's head to the marketplace

00:14:35,040 --> 00:14:45,839
installment jf

00:14:37,199 --> 00:14:45,839
minkief is a marketplace solution

00:14:48,160 --> 00:14:50,959
who search for it

00:14:56,000 --> 00:15:02,399
click on launch

00:14:59,760 --> 00:15:05,040
choose the the project we're going to

00:15:02,399 --> 00:15:05,040
deploy it in

00:15:05,440 --> 00:15:09,680
and here are various configurations we

00:15:07,040 --> 00:15:13,279
can choose for our mini kf

00:15:09,680 --> 00:15:16,000
go to the zone can choose

00:15:13,279 --> 00:15:18,079
its disks we can select gpus if they are

00:15:16,000 --> 00:15:19,440
available in the zone

00:15:18,079 --> 00:15:21,839
and once we're ready we can click on

00:15:19,440 --> 00:15:21,839
deploy

00:15:21,920 --> 00:15:27,279
so now the instance is going to come up

00:15:25,120 --> 00:15:29,759
and after that it will take about 15

00:15:27,279 --> 00:15:33,839
minutes for mink to be up and running

00:15:29,759 --> 00:15:33,839
and for you to be ready to experiment

00:15:34,639 --> 00:15:38,399
at this point i'll hand over back to

00:15:38,839 --> 00:15:43,920
stephanie

00:15:40,639 --> 00:15:46,720
thank you idiots so uh

00:15:43,920 --> 00:15:48,240
in no time you should have a ready mini

00:15:46,720 --> 00:15:51,600
kf and you will

00:15:48,240 --> 00:15:53,279
be able to start uh with uh with the

00:15:51,600 --> 00:15:56,480
tutorial yourself

00:15:53,279 --> 00:15:58,959
now i will walk you through

00:15:56,480 --> 00:16:00,000
the rest of the codelab so we'll see how

00:15:58,959 --> 00:16:02,959
you can

00:16:00,000 --> 00:16:05,519
go from notebook to cutie to cueva

00:16:02,959 --> 00:16:05,519
pipelines

00:16:06,079 --> 00:16:12,720
so i will head over to my mini kf

00:16:09,519 --> 00:16:14,560
as you can see here i have uh the queue

00:16:12,720 --> 00:16:18,480
flow sensor dashboard

00:16:14,560 --> 00:16:19,920
um if you are already used to qflo if

00:16:18,480 --> 00:16:23,199
you've already used it

00:16:19,920 --> 00:16:25,279
uh you might notice that the

00:16:23,199 --> 00:16:27,040
the sidebar here is a little bit

00:16:25,279 --> 00:16:30,480
different this is because

00:16:27,040 --> 00:16:33,279
um we've been working on the kubeflow

00:16:30,480 --> 00:16:34,480
center dashboard to unify uh in a

00:16:33,279 --> 00:16:36,880
consistent way

00:16:34,480 --> 00:16:38,639
all of the key flow components together

00:16:36,880 --> 00:16:41,759
in a single place

00:16:38,639 --> 00:16:44,399
and you will see how i will be using

00:16:41,759 --> 00:16:45,440
these throughout the tutorial of course

00:16:44,399 --> 00:16:47,600
this is what work

00:16:45,440 --> 00:16:50,480
that we are going to contribute upstream

00:16:47,600 --> 00:16:54,160
very soon so you will be able to see it

00:16:50,480 --> 00:16:55,199
in the github discussions so now i'm

00:16:54,160 --> 00:16:59,440
heading over

00:16:55,199 --> 00:17:03,120
to notebooks and i'll create

00:16:59,440 --> 00:17:06,959
a new notebook just for this demo

00:17:03,120 --> 00:17:11,280
and they need kubecon i'm selecting

00:17:06,959 --> 00:17:11,280
a jupiter image that includes scale

00:17:11,600 --> 00:17:22,240
then i'm also adding a data volume

00:17:19,600 --> 00:17:23,760
and i press launch and that's it this is

00:17:22,240 --> 00:17:26,959
very simple in kubeflow

00:17:23,760 --> 00:17:30,000
to spin up new jupiter servers

00:17:26,959 --> 00:17:33,840
where you can work on your own code

00:17:30,000 --> 00:17:33,840
and start new pipelines

00:17:46,880 --> 00:17:50,880
okay the notebook server has been

00:17:49,840 --> 00:17:54,080
provisioned

00:17:50,880 --> 00:17:54,080
so you can just connect

00:17:54,400 --> 00:17:58,799
and in no time we have provisioned in a

00:17:57,360 --> 00:18:02,880
service of way

00:17:58,799 --> 00:18:02,880
a full jupiter lab environment

00:18:05,760 --> 00:18:10,080
now i want to clone some code where i

00:18:07,760 --> 00:18:12,240
can work on

00:18:10,080 --> 00:18:13,600
i can do that from the code lab as you

00:18:12,240 --> 00:18:15,919
can see here there are all of the

00:18:13,600 --> 00:18:19,360
instructions that you can follow

00:18:15,919 --> 00:18:24,080
metadata stage now i'll just scroll

00:18:19,360 --> 00:18:28,000
here to the code that allows me to clone

00:18:24,080 --> 00:18:30,960
the k repository so i'll head over back

00:18:28,000 --> 00:18:30,960
to my notebook

00:18:31,760 --> 00:18:37,440
so we are cloning the official k

00:18:33,520 --> 00:18:37,440
repository that is open source

00:18:38,240 --> 00:18:42,000
as you can see there is an examples

00:18:40,960 --> 00:18:45,360
folder

00:18:42,000 --> 00:18:48,480
we're going to open the top read

00:18:45,360 --> 00:18:52,559
classification example

00:18:48,480 --> 00:18:54,960
that we prepared just for this tutorial

00:18:52,559 --> 00:18:56,160
there are two notebooks we will start

00:18:54,960 --> 00:18:58,880
with the dog brick

00:18:56,160 --> 00:19:01,360
one and then switch over to the drug dog

00:18:58,880 --> 00:19:05,760
breed cathedral

00:19:01,360 --> 00:19:09,200
now i want to go over into the details

00:19:05,760 --> 00:19:12,720
of what is implemented

00:19:09,200 --> 00:19:15,520
in this notebook just keep in mind

00:19:12,720 --> 00:19:16,480
that this code is based on a utah city

00:19:15,520 --> 00:19:20,080
project

00:19:16,480 --> 00:19:23,120
where we are basically processing

00:19:20,080 --> 00:19:26,720
some dog images and building

00:19:23,120 --> 00:19:29,679
a deep learning classifier to recognize

00:19:26,720 --> 00:19:29,679
dog breeds

00:19:30,000 --> 00:19:33,760
the first thing i want to do is to

00:19:31,840 --> 00:19:36,000
verify that my notebook

00:19:33,760 --> 00:19:36,799
has the dependencies it needs to run the

00:19:36,000 --> 00:19:38,400
code

00:19:36,799 --> 00:19:40,160
so i'll just run the first cell here

00:19:38,400 --> 00:19:42,000
with the import and we

00:19:40,160 --> 00:19:43,440
notice that we are missing some

00:19:42,000 --> 00:19:46,480
dependencies

00:19:43,440 --> 00:19:49,120
so i have a helper here that runs

00:19:46,480 --> 00:19:51,679
clip install to install the requirements

00:19:49,120 --> 00:19:54,480
notice how i am installing this library

00:19:51,679 --> 00:19:55,200
here now on the fly this will be

00:19:54,480 --> 00:19:57,200
important

00:19:55,200 --> 00:20:00,240
at a later stage when we will convert

00:19:57,200 --> 00:20:00,240
this for a pipeline

00:20:01,280 --> 00:20:04,320
you can think of this process as i could

00:20:03,440 --> 00:20:07,039
be

00:20:04,320 --> 00:20:08,480
tinkering with the code solving bugs

00:20:07,039 --> 00:20:11,280
adding new features

00:20:08,480 --> 00:20:12,799
um and then installing the planet

00:20:11,280 --> 00:20:16,080
dependencies on the fly

00:20:12,799 --> 00:20:16,080
as i add new code

00:20:17,039 --> 00:20:23,520
okay so now we should have all the

00:20:20,400 --> 00:20:23,520
dependencies we need

00:20:24,240 --> 00:20:27,440
i'll restart my kernel

00:20:30,559 --> 00:20:38,240
okay so every import succeeded

00:20:35,200 --> 00:20:39,280
so let's say i'm done with the code i'm

00:20:38,240 --> 00:20:42,320
done with the code

00:20:39,280 --> 00:20:43,679
so i have implemented all my data

00:20:42,320 --> 00:20:46,159
preparation

00:20:43,679 --> 00:20:48,960
and then i'm detecting my dogs i'm

00:20:46,159 --> 00:20:52,640
writing some detectors

00:20:48,960 --> 00:20:53,039
we brought the cnn and deep learning

00:20:52,640 --> 00:20:56,000
code

00:20:53,039 --> 00:20:56,880
to recognize them again i won't go into

00:20:56,000 --> 00:21:00,000
details

00:20:56,880 --> 00:21:03,120
but you see there's some involved code

00:21:00,000 --> 00:21:03,760
in this um this notebook so i tested it

00:21:03,120 --> 00:21:05,919
locally

00:21:03,760 --> 00:21:07,679
i have all my dependencies and now i

00:21:05,919 --> 00:21:08,320
want to scale it up and convert it to a

00:21:07,679 --> 00:21:11,360
pipeline

00:21:08,320 --> 00:21:12,240
how do i do that so i can go over here

00:21:11,360 --> 00:21:15,679
on the left

00:21:12,240 --> 00:21:17,600
click on the k panel enable it

00:21:15,679 --> 00:21:19,440
and now you see a whole bunch of things

00:21:17,600 --> 00:21:22,559
happening

00:21:19,440 --> 00:21:25,679
so kale is a

00:21:22,559 --> 00:21:28,960
is a way for you to

00:21:25,679 --> 00:21:32,799
tag notebook cells in a ui driven way

00:21:28,960 --> 00:21:36,159
and assign these cells to pipeline steps

00:21:32,799 --> 00:21:39,520
let's let's take an example here

00:21:36,159 --> 00:21:43,600
here you see this cell is tagged as

00:21:39,520 --> 00:21:47,600
step load data by pressing on

00:21:43,600 --> 00:21:51,120
the top right here uck provides

00:21:47,600 --> 00:21:54,000
a very simple way for you to target

00:21:51,120 --> 00:21:54,480
this cell with this specific name and

00:21:54,000 --> 00:21:56,559
what you're

00:21:54,480 --> 00:21:59,039
doing here is basically assigning this

00:21:56,559 --> 00:22:01,120
cell to a pipeline step

00:21:59,039 --> 00:22:02,240
multiple cells as you can see can be

00:22:01,120 --> 00:22:05,600
merged together

00:22:02,240 --> 00:22:06,320
in a single one and then it is that

00:22:05,600 --> 00:22:10,840
simple

00:22:06,320 --> 00:22:12,480
to create new steps and

00:22:10,840 --> 00:22:15,280
dependencies

00:22:12,480 --> 00:22:17,679
once you do this of course we've already

00:22:15,280 --> 00:22:19,440
tagged this notebook for simplicity

00:22:17,679 --> 00:22:23,840
you can just select an eq flip

00:22:19,440 --> 00:22:23,840
experiment give the notebook a name

00:22:24,480 --> 00:22:27,760
and then click the compile and run

00:22:26,400 --> 00:22:30,559
button

00:22:27,760 --> 00:22:32,640
so what is happening now is that caleb

00:22:30,559 --> 00:22:35,200
has validated the notebook

00:22:32,640 --> 00:22:36,000
and now brock our richto's rock data

00:22:35,200 --> 00:22:38,240
platform

00:22:36,000 --> 00:22:39,200
is taking a snapshot of the current

00:22:38,240 --> 00:22:42,559
notebook

00:22:39,200 --> 00:22:45,120
all of its volume volumes

00:22:42,559 --> 00:22:46,240
so that your environment is completely

00:22:45,120 --> 00:22:51,039
reproduced

00:22:46,240 --> 00:22:51,039
in the pipeline steps this is why

00:22:51,360 --> 00:22:55,679
even though i just installed on the fly

00:22:54,320 --> 00:22:58,640
my libraries

00:22:55,679 --> 00:22:59,840
the code will run seamlessly in the

00:22:58,640 --> 00:23:02,880
pipeline steps

00:22:59,840 --> 00:23:04,720
without you having to build new docker

00:23:02,880 --> 00:23:06,960
images

00:23:04,720 --> 00:23:08,159
so chaos then compiled the notebook

00:23:06,960 --> 00:23:11,120
created a python

00:23:08,159 --> 00:23:12,000
uploaded it for five times and it starts

00:23:11,120 --> 00:23:15,360
with a new run

00:23:12,000 --> 00:23:23,840
let's go to q4 pipelines using this link

00:23:15,360 --> 00:23:23,840
and see the pipeline running

00:23:25,039 --> 00:23:29,120
okay so as you can see here i have my

00:23:28,159 --> 00:23:32,320
two volumes

00:23:29,120 --> 00:23:34,640
that are basically um new

00:23:32,320 --> 00:23:36,240
volumes that start from snapshot that

00:23:34,640 --> 00:23:39,200
were taken by a rock

00:23:36,240 --> 00:23:40,240
when i clicked the button and my python

00:23:39,200 --> 00:23:42,080
is starting

00:23:40,240 --> 00:23:43,279
let's give it a little bit of time to

00:23:42,080 --> 00:23:45,919
run while i

00:23:43,279 --> 00:23:47,279
go back to the slides and talk to you

00:23:45,919 --> 00:23:51,679
and walk you through how

00:23:47,279 --> 00:23:55,039
kale is actually doing all of this

00:23:51,679 --> 00:23:58,559
so scale is basically a python package

00:23:55,039 --> 00:24:01,200
and a jupyter lab extension

00:23:58,559 --> 00:24:02,080
um it allows you to convert a jupiter

00:24:01,200 --> 00:24:05,520
notebook

00:24:02,080 --> 00:24:06,559
to cubesat pipelines in a completely ui

00:24:05,520 --> 00:24:09,760
driven way

00:24:06,559 --> 00:24:14,080
without the need of writing any kind

00:24:09,760 --> 00:24:16,880
of external sdk how does it do that

00:24:14,080 --> 00:24:18,799
well in the point you click the compile

00:24:16,880 --> 00:24:21,760
compiler button

00:24:18,799 --> 00:24:22,320
running compile button ko takes the

00:24:21,760 --> 00:24:24,799
notebook

00:24:22,320 --> 00:24:25,679
and parses it analyzing all of the

00:24:24,799 --> 00:24:29,120
annotations

00:24:25,679 --> 00:24:31,520
you've added via the key ui this

00:24:29,120 --> 00:24:33,120
turns into an internal graph

00:24:31,520 --> 00:24:36,159
representation that is

00:24:33,120 --> 00:24:39,039
parsed by ko so that it can look

00:24:36,159 --> 00:24:40,559
at all the data dependencies between the

00:24:39,039 --> 00:24:43,679
notebook cells

00:24:40,559 --> 00:24:44,559
so cable detects these dependencies and

00:24:43,679 --> 00:24:47,120
then

00:24:44,559 --> 00:24:49,440
it takes care of marshalling the data

00:24:47,120 --> 00:24:52,400
between the pipeline steps for you

00:24:49,440 --> 00:24:53,200
so that the code execution is completely

00:24:52,400 --> 00:24:56,880
seamless

00:24:53,200 --> 00:24:59,520
just like it happens in the notebook

00:24:56,880 --> 00:25:00,000
and all of this is possible because

00:24:59,520 --> 00:25:02,400
scale

00:25:00,000 --> 00:25:03,520
also takes care of generating a whole

00:25:02,400 --> 00:25:06,080
bunch of code

00:25:03,520 --> 00:25:07,200
that you would have that you don't need

00:25:06,080 --> 00:25:09,120
to write

00:25:07,200 --> 00:25:11,279
anymore because katie is doing it for

00:25:09,120 --> 00:25:11,279
you

00:25:12,880 --> 00:25:17,440
note that cable is an open source

00:25:15,039 --> 00:25:19,520
project that is hosted

00:25:17,440 --> 00:25:21,440
in the kubeflow cable github github

00:25:19,520 --> 00:25:23,840
organization

00:25:21,440 --> 00:25:24,960
and we would be very happy to receive

00:25:23,840 --> 00:25:28,000
feedback

00:25:24,960 --> 00:25:30,720
and questions and also

00:25:28,000 --> 00:25:30,720
contributions

00:25:31,279 --> 00:25:37,279
now let's go over

00:25:34,320 --> 00:25:39,760
back over to the pipeline that it is

00:25:37,279 --> 00:25:42,880
still running

00:25:39,760 --> 00:25:46,080
it should be done very quickly

00:25:42,880 --> 00:25:48,400
so notice how

00:25:46,080 --> 00:25:49,279
we already have two steps you might

00:25:48,400 --> 00:25:52,880
remember

00:25:49,279 --> 00:25:55,360
that we tagged some cells with the load

00:25:52,880 --> 00:25:58,000
data

00:25:55,360 --> 00:25:58,799
the load data notation and then we

00:25:58,000 --> 00:26:01,039
tagged

00:25:58,799 --> 00:26:01,840
some other cells with the detected token

00:26:01,039 --> 00:26:05,440
notation

00:26:01,840 --> 00:26:07,520
specifying a dependency so kate was able

00:26:05,440 --> 00:26:08,880
to separate this code into multiple

00:26:07,520 --> 00:26:12,320
steps and then

00:26:08,880 --> 00:26:14,720
even branch out

00:26:12,320 --> 00:26:16,400
the training of the several different

00:26:14,720 --> 00:26:18,960
deep learning algorithms

00:26:16,400 --> 00:26:21,520
because the dependencies were all

00:26:18,960 --> 00:26:24,400
referring to a common step

00:26:21,520 --> 00:26:26,000
so you now have code that would have run

00:26:24,400 --> 00:26:28,320
serial in the notebook

00:26:26,000 --> 00:26:30,480
running concurrently in the queue for

00:26:28,320 --> 00:26:34,880
pipeline

00:26:30,480 --> 00:26:34,880
all of the logs are preserved

00:26:36,080 --> 00:26:43,520
and also let me go to a pipeline

00:26:40,240 --> 00:26:46,960
that was already concluded

00:26:43,520 --> 00:26:48,390
or rather i take this one since it has

00:26:46,960 --> 00:26:50,840
already

00:26:48,390 --> 00:26:52,880
[Music]

00:26:50,840 --> 00:26:56,400
completed

00:26:52,880 --> 00:26:57,760
as you can see i have a whole bunch of

00:26:56,400 --> 00:27:00,320
logs here

00:26:57,760 --> 00:27:01,679
that are taking uh that are basically

00:27:00,320 --> 00:27:03,919
reproducing

00:27:01,679 --> 00:27:05,679
the exact visualization that you would

00:27:03,919 --> 00:27:09,200
see in jupiter

00:27:05,679 --> 00:27:12,480
for example here the cnn resonant 15

00:27:09,200 --> 00:27:14,559
in the notebook produces

00:27:12,480 --> 00:27:16,960
a visualization at the end of its

00:27:14,559 --> 00:27:20,320
computation to show

00:27:16,960 --> 00:27:22,880
one of the detected images one of the

00:27:20,320 --> 00:27:26,080
dog breeds detective

00:27:22,880 --> 00:27:27,200
so as you can see uh kale is able to

00:27:26,080 --> 00:27:29,520
completely reproduce

00:27:27,200 --> 00:27:31,039
even the notebook visualizations so

00:27:29,520 --> 00:27:32,000
whether you are plotting with matte

00:27:31,039 --> 00:27:34,799
blocking

00:27:32,000 --> 00:27:35,919
or interactive with a visualization with

00:27:34,799 --> 00:27:39,120
blockly

00:27:35,919 --> 00:27:42,240
all of this is preserved and reproduced

00:27:39,120 --> 00:27:42,240
in qfl pipelines

00:27:42,399 --> 00:27:48,640
okay so now that we've run

00:27:45,840 --> 00:27:49,919
a single pipeline we would want to scale

00:27:48,640 --> 00:27:53,440
it up

00:27:49,919 --> 00:27:55,679
and run it as a hyper parameter tuning

00:27:53,440 --> 00:27:55,679
job

00:27:56,159 --> 00:28:03,760
let's go to the catal notebook

00:28:00,960 --> 00:28:05,760
in this specific notebook we are

00:28:03,760 --> 00:28:08,320
skipping

00:28:05,760 --> 00:28:09,919
two of the learning algorithms and we

00:28:08,320 --> 00:28:12,960
are running just over

00:28:09,919 --> 00:28:12,960
the resonant 50.

00:28:14,799 --> 00:28:18,240
what we want to do is to parameterize

00:28:17,120 --> 00:28:22,799
the notebook

00:28:18,240 --> 00:28:25,600
with some input parameters and

00:28:22,799 --> 00:28:27,520
run the generated pipeline with cutie to

00:28:25,600 --> 00:28:31,840
optimize over a certain metric

00:28:27,520 --> 00:28:31,840
so that we can optimize automatically

00:28:32,159 --> 00:28:36,640
the training algorithm so we need two

00:28:34,960 --> 00:28:41,120
important things to do this

00:28:36,640 --> 00:28:44,720
input parameters and an output metric

00:28:41,120 --> 00:28:47,520
so with ko it is very simple to create

00:28:44,720 --> 00:28:49,200
pipeline parameters from the notebook

00:28:47,520 --> 00:28:52,320
all you need to do

00:28:49,200 --> 00:28:54,880
is to define an um

00:28:52,320 --> 00:28:55,679
notebook cell with some variable

00:28:54,880 --> 00:29:00,240
assignments

00:28:55,679 --> 00:29:03,919
and target with the kui

00:29:00,240 --> 00:29:03,919
with the pipeline parameter tag

00:29:04,559 --> 00:29:09,600
and then i'm scrolling down all the way

00:29:07,360 --> 00:29:12,960
to the bottom of the notebook

00:29:09,600 --> 00:29:16,480
what you want to do is to have your code

00:29:12,960 --> 00:29:19,760
produce a metric produce a metric

00:29:16,480 --> 00:29:20,399
that then will be exported by the

00:29:19,760 --> 00:29:24,000
pipeline

00:29:20,399 --> 00:29:25,279
and analyzed by cutting so what you need

00:29:24,000 --> 00:29:28,240
to do

00:29:25,279 --> 00:29:30,159
is to print whatever variable your

00:29:28,240 --> 00:29:32,480
algorithm is producing

00:29:30,159 --> 00:29:33,679
and that you want to be used as an

00:29:32,480 --> 00:29:37,520
optimization

00:29:33,679 --> 00:29:40,320
metric print it and then tag

00:29:37,520 --> 00:29:41,919
the cell with the pipeline metric stock

00:29:40,320 --> 00:29:46,240
this is all you need

00:29:41,919 --> 00:29:49,760
once you have this you can go over

00:29:46,240 --> 00:29:52,880
the kale sidebar

00:29:49,760 --> 00:29:54,799
enable the the copy toggle

00:29:52,880 --> 00:29:57,200
and click on the setup cutting job

00:29:54,799 --> 00:29:57,200
button

00:29:57,600 --> 00:30:03,120
now as you can see okay it provides you

00:30:00,480 --> 00:30:06,399
with this very simple to use ui

00:30:03,120 --> 00:30:08,399
that provides the input parameters

00:30:06,399 --> 00:30:10,240
these are exactly the names of the

00:30:08,399 --> 00:30:14,320
variables i defined

00:30:10,240 --> 00:30:16,960
in the notebook cell we have already

00:30:14,320 --> 00:30:17,440
filled the values we want to use but you

00:30:16,960 --> 00:30:20,080
could do

00:30:17,440 --> 00:30:20,880
you could insert here whatever values

00:30:20,080 --> 00:30:24,080
you want

00:30:20,880 --> 00:30:28,880
from ranges lists integer

00:30:24,080 --> 00:30:28,880
float streaming values what have you

00:30:28,960 --> 00:30:33,279
then i can select a search optimization

00:30:31,039 --> 00:30:36,320
algorithm

00:30:33,279 --> 00:30:39,120
and my search objective again

00:30:36,320 --> 00:30:41,200
this is exactly the variable name we are

00:30:39,120 --> 00:30:45,279
printing in the pipeline metrics

00:30:41,200 --> 00:30:48,399
cell that's all you need to do

00:30:45,279 --> 00:30:49,120
i'm clicking close and again the

00:30:48,399 --> 00:30:52,799
compiler

00:30:49,120 --> 00:30:53,919
run button so now a similar thing

00:30:52,799 --> 00:30:57,039
happens

00:30:53,919 --> 00:31:00,080
as before okay validates the notebook

00:30:57,039 --> 00:31:00,640
rock takes an immutable snapshot at this

00:31:00,080 --> 00:31:04,000
specific

00:31:00,640 --> 00:31:04,960
point in time then kl converts the

00:31:04,000 --> 00:31:07,919
notebook

00:31:04,960 --> 00:31:09,840
uploads a new pipeline and now here's

00:31:07,919 --> 00:31:13,840
what's happening

00:31:09,840 --> 00:31:15,519
kale is actually starting a new cutting

00:31:13,840 --> 00:31:18,559
job

00:31:15,519 --> 00:31:19,840
nucleotide experiment where each

00:31:18,559 --> 00:31:23,279
caterpillar

00:31:19,840 --> 00:31:26,080
is actually a q flow pipeline run

00:31:23,279 --> 00:31:26,880
so this was not possible before it is

00:31:26,080 --> 00:31:29,360
scale

00:31:26,880 --> 00:31:30,559
that is that it is acting as a bridge

00:31:29,360 --> 00:31:33,600
between khatib

00:31:30,559 --> 00:31:37,519
thank you for pipelines let's go

00:31:33,600 --> 00:31:37,519
see these experiments

00:31:38,080 --> 00:31:45,440
so here i'm opening

00:31:41,279 --> 00:31:49,279
the cutie experiment page let's give it

00:31:45,440 --> 00:31:49,279
a few seconds to load up

00:31:53,919 --> 00:31:59,440
as you can see i have a new cutting

00:31:56,720 --> 00:32:02,559
experiment

00:31:59,440 --> 00:32:05,200
and here a new cubeflow experiment

00:32:02,559 --> 00:32:06,000
they have the same names they have one

00:32:05,200 --> 00:32:09,120
single run

00:32:06,000 --> 00:32:11,519
for the moment again with the same name

00:32:09,120 --> 00:32:12,399
so you have a one-to-one correspondence

00:32:11,519 --> 00:32:15,440
here

00:32:12,399 --> 00:32:16,159
you see which are the input parameters

00:32:15,440 --> 00:32:18,880
that can be

00:32:16,159 --> 00:32:20,240
decided to give as input to this

00:32:18,880 --> 00:32:23,440
pipeline

00:32:20,240 --> 00:32:24,480
so i'm heading back over to pipelines i

00:32:23,440 --> 00:32:30,399
see the pipeline

00:32:24,480 --> 00:32:30,399
generated it is starting to run

00:32:30,720 --> 00:32:38,559
i can see them here in the config tab

00:32:34,799 --> 00:32:38,559
what are its input parameters

00:32:40,880 --> 00:32:44,399
so this pipeline is running just like

00:32:43,279 --> 00:32:47,360
before

00:32:44,399 --> 00:32:49,840
but with specific parameter parameters

00:32:47,360 --> 00:32:53,360
that were given by cathedral

00:32:49,840 --> 00:32:57,200
now it would take a lot of time for us

00:32:53,360 --> 00:33:00,399
to monitor uh tens of pipelines running

00:32:57,200 --> 00:33:02,799
in this experiment so let me head

00:33:00,399 --> 00:33:10,399
over to an experiment that we already

00:33:02,799 --> 00:33:13,519
run some time ago

00:33:10,399 --> 00:33:15,840
okay so this was an experiment created

00:33:13,519 --> 00:33:18,000
in the same way we did now from the

00:33:15,840 --> 00:33:20,480
notebook clicking a button on kl

00:33:18,000 --> 00:33:21,440
as you can see i have several pipelines

00:33:20,480 --> 00:33:24,720
here

00:33:21,440 --> 00:33:27,039
that produced a specific

00:33:24,720 --> 00:33:28,880
metric so this is the metric that we

00:33:27,039 --> 00:33:31,360
were bringing from the notebook

00:33:28,880 --> 00:33:32,159
here was able to interpret that and

00:33:31,360 --> 00:33:36,159
output

00:33:32,159 --> 00:33:40,559
this metric as an artifact let's go over

00:33:36,159 --> 00:33:40,559
also to the katip experiment

00:33:44,240 --> 00:33:47,519
this is the corresponding cutif

00:33:45,760 --> 00:33:50,399
experiment that was

00:33:47,519 --> 00:33:50,720
running as you can see we have many runs

00:33:50,399 --> 00:33:54,480
and

00:33:50,720 --> 00:33:57,519
how their performance varied over time

00:33:54,480 --> 00:33:59,200
we have all of the runs even here with

00:33:57,519 --> 00:34:07,840
the corresponding metrics

00:33:59,200 --> 00:34:07,840
and input parameters

00:34:08,960 --> 00:34:13,679
in the meanwhile our experiment is still

00:34:12,720 --> 00:34:16,159
running

00:34:13,679 --> 00:34:18,839
and you can see here you can have a live

00:34:16,159 --> 00:34:22,480
view directly from the notebook

00:34:18,839 --> 00:34:22,480
about how many

00:34:22,960 --> 00:34:30,800
runs are are running in this moment

00:34:27,119 --> 00:34:34,240
and as soon as one at least one

00:34:30,800 --> 00:34:34,879
would finish you would see here a live

00:34:34,240 --> 00:34:37,760
view

00:34:34,879 --> 00:34:38,240
over what is the current best result as

00:34:37,760 --> 00:34:41,280
well

00:34:38,240 --> 00:34:44,879
all from your notebook

00:34:41,280 --> 00:34:47,760
okay so that was it for the hands-on

00:34:44,879 --> 00:34:48,159
tutorial i hope you will have time to go

00:34:47,760 --> 00:34:51,040
through

00:34:48,159 --> 00:34:51,520
your code lab play around with the code

00:34:51,040 --> 00:34:55,919
stop

00:34:51,520 --> 00:34:58,480
start your own pipelines and have fun

00:34:55,919 --> 00:34:59,200
so let me go back to the slides to

00:34:58,480 --> 00:35:03,839
summarize

00:34:59,200 --> 00:35:03,839
what we saw together today

00:35:08,000 --> 00:35:11,599
so what have you learned today today you

00:35:10,480 --> 00:35:15,680
have learned

00:35:11,599 --> 00:35:18,240
how you can run a pipeline based hyper

00:35:15,680 --> 00:35:19,839
tuning workflow starting directly from

00:35:18,240 --> 00:35:23,119
your jupiter notebook

00:35:19,839 --> 00:35:26,960
in a super simple ui driven way

00:35:23,119 --> 00:35:30,000
kale is acting as the workflow tool

00:35:26,960 --> 00:35:30,880
that brings together notebooks q5

00:35:30,000 --> 00:35:34,079
pipelines

00:35:30,880 --> 00:35:35,280
and cocktail experiments this is a great

00:35:34,079 --> 00:35:37,760
simplification

00:35:35,280 --> 00:35:38,560
of how you can run machine learning

00:35:37,760 --> 00:35:43,119
workflows

00:35:38,560 --> 00:35:46,240
in a ui driven way also

00:35:43,119 --> 00:35:49,599
um all of these um

00:35:46,240 --> 00:35:52,800
exploit caching and i will show you

00:35:49,599 --> 00:35:55,040
in a second how

00:35:52,800 --> 00:35:57,040
that i actually forgot to mention before

00:35:55,040 --> 00:35:59,280
so we'll go back to the

00:35:57,040 --> 00:36:01,119
to the queue for pipelines ui so that

00:35:59,280 --> 00:36:04,240
you can see that

00:36:01,119 --> 00:36:06,640
all of this workflow thanks to rock

00:36:04,240 --> 00:36:09,440
is exploiting our our caching feature

00:36:06,640 --> 00:36:11,839
that is built on top of kubeflow

00:36:09,440 --> 00:36:12,560
and also this allows you to collaborate

00:36:11,839 --> 00:36:17,760
faster

00:36:12,560 --> 00:36:21,359
and more easily so let's head over back

00:36:17,760 --> 00:36:23,440
to the flow dashboard this is actually

00:36:21,359 --> 00:36:28,000
something i wanted to show you

00:36:23,440 --> 00:36:32,079
but forgot so

00:36:28,000 --> 00:36:34,000
as you can see here i am looking

00:36:32,079 --> 00:36:35,760
at the experiment that we had already

00:36:34,000 --> 00:36:38,160
run in the past

00:36:35,760 --> 00:36:41,040
so i have several runs i took one of

00:36:38,160 --> 00:36:44,160
them at random and

00:36:41,040 --> 00:36:44,640
there is this little icon here so this

00:36:44,160 --> 00:36:47,520
means

00:36:44,640 --> 00:36:48,400
that this pipeline was cached actually

00:36:47,520 --> 00:36:51,200
these steps

00:36:48,400 --> 00:36:51,200
were cached

00:36:51,599 --> 00:36:58,320
so why is this happening

00:36:55,119 --> 00:37:00,720
the cnn resonant 50 step

00:36:58,320 --> 00:37:01,680
is the only one in the pipeline that is

00:37:00,720 --> 00:37:04,720
dependent

00:37:01,680 --> 00:37:05,599
on some input parameters it is the only

00:37:04,720 --> 00:37:09,200
part of the

00:37:05,599 --> 00:37:13,280
node on the original notebook code that

00:37:09,200 --> 00:37:17,359
reads that acts upon those parameters

00:37:13,280 --> 00:37:20,839
since k knows this it also knows

00:37:17,359 --> 00:37:24,240
that the previous steps won't change

00:37:20,839 --> 00:37:27,440
um and it that will never change

00:37:24,240 --> 00:37:28,480
across all of your runs this means that

00:37:27,440 --> 00:37:31,119
running load data

00:37:28,480 --> 00:37:33,200
and the tech talks several times would

00:37:31,119 --> 00:37:36,880
be a waste of resources

00:37:33,200 --> 00:37:39,359
so what we've done is we've extended

00:37:36,880 --> 00:37:40,160
the current caching implementation of

00:37:39,359 --> 00:37:44,079
qflo

00:37:40,160 --> 00:37:47,920
that works over artifacts

00:37:44,079 --> 00:37:50,079
to work also over snapshots and pvcs

00:37:47,920 --> 00:37:52,480
in this way the running time and the

00:37:50,079 --> 00:37:55,920
performance of all of these pipelines

00:37:52,480 --> 00:37:55,920
is greatly improved

00:37:58,000 --> 00:38:05,119
okay so back again to us

00:38:02,000 --> 00:38:06,160
our summary running pipelines from

00:38:05,119 --> 00:38:09,599
notebook

00:38:06,160 --> 00:38:10,240
to queue flow in a completely ui driven

00:38:09,599 --> 00:38:13,599
way

00:38:10,240 --> 00:38:16,240
with caching in order to accelerate your

00:38:13,599 --> 00:38:16,240
workflow

00:38:16,839 --> 00:38:22,640
dramatically

00:38:18,640 --> 00:38:25,359
i would like to conclude by saying that

00:38:22,640 --> 00:38:27,280
in a ricktalk we are very passionate

00:38:25,359 --> 00:38:29,920
open source contributors

00:38:27,280 --> 00:38:31,119
we've contributed to many components in

00:38:29,920 --> 00:38:34,880
kubeflow

00:38:31,119 --> 00:38:37,839
from jupiter manager ui to pipelines

00:38:34,880 --> 00:38:38,800
to building our opinionated and super

00:38:37,839 --> 00:38:41,839
portable

00:38:38,800 --> 00:38:44,400
q flow distribution mini kf

00:38:41,839 --> 00:38:45,520
we've done lots of work to the

00:38:44,400 --> 00:38:49,680
authentication and

00:38:45,520 --> 00:38:52,800
authorization architecture of keyflow

00:38:49,680 --> 00:38:53,839
and also many contributions to the linux

00:38:52,800 --> 00:38:57,920
kernel

00:38:53,839 --> 00:38:57,920
to support our data platform

00:38:58,839 --> 00:39:06,800
qflow is a great community

00:39:01,760 --> 00:39:09,040
made up of many many uh big companies

00:39:06,800 --> 00:39:11,040
and as a community we are always

00:39:09,040 --> 00:39:14,640
striving to find new contribution

00:39:11,040 --> 00:39:17,760
contributors and new passionate users so

00:39:14,640 --> 00:39:20,400
please get involved you can find here

00:39:17,760 --> 00:39:21,680
lots of pointers from the github

00:39:20,400 --> 00:39:24,880
organization

00:39:21,680 --> 00:39:28,560
to our slack twitter handle

00:39:24,880 --> 00:39:29,920
or write us an email to the qflo discuss

00:39:28,560 --> 00:39:32,560
google group

00:39:29,920 --> 00:39:35,040
will be happy to support you and on

00:39:32,560 --> 00:39:35,040
board you

00:39:35,280 --> 00:39:40,240
so thank you for staying with us

00:39:38,320 --> 00:39:42,640
we really hope you'll have fun playing

00:39:40,240 --> 00:39:45,839
around with our tutorial

00:39:42,640 --> 00:39:47,520
make sure to head over our website and

00:39:45,839 --> 00:39:51,119
there and this quick link

00:39:47,520 --> 00:39:54,880
where you will find the slides and um

00:39:51,119 --> 00:39:59,839
and you could win some cool prizes

00:39:54,880 --> 00:39:59,839

YouTube URL: https://www.youtube.com/watch?v=QK0NxhyADpM


