Title: Scaling Prometheus: How We Got Some Thanos Into Cortex - Thor Hansen, HashiCorp & Marco Pracucci
Publication date: 2020-08-28
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Scaling Prometheus: How We Got Some Thanos Into Cortex - Thor Hansen, HashiCorp & Marco Pracucci, Grafana Labs 

Cortex is a long term storage for Prometheus, designed for scalability, multi-tenancy and high-availability. It can reliably ingest and query millions of time series per second with sub-second latency. The current storage design uses a NoSQL store to index series and an object store for compressed time series data - two dependencies, and one with significant cost implications. In this talk we will show the new experimental Cortex blocks storage, based on Thanos and Prometheus TSDB, aiming to reduce the Cortex operational cost without compromising scalability and performances. We’ll cover the trade-off between the standard chunks storage and the new blocks storage, and share lessons learned running Cortex at scale. Cortex is a CNCF sandbox project.

https://sched.co/Zeuw
Captions: 
	00:00:00,160 --> 00:00:04,560
hello welcome to the talk scaling

00:00:02,240 --> 00:00:05,759
prometheus how we got some tunnels into

00:00:04,560 --> 00:00:08,080
cortex

00:00:05,759 --> 00:00:09,920
uh in this talk we are going to show

00:00:08,080 --> 00:00:13,040
cortex and specifically

00:00:09,920 --> 00:00:13,920
a new storage engine we have built into

00:00:13,040 --> 00:00:16,800
cortex

00:00:13,920 --> 00:00:17,199
leveraging containers before that let me

00:00:16,800 --> 00:00:20,480
do

00:00:17,199 --> 00:00:22,640
a quick introduction about cortex

00:00:20,480 --> 00:00:23,600
cortex is a distributed time series

00:00:22,640 --> 00:00:26,800
database built

00:00:23,600 --> 00:00:28,400
on top of prometheus cortex is

00:00:26,800 --> 00:00:29,279
horizontally scalable and highly

00:00:28,400 --> 00:00:31,679
available

00:00:29,279 --> 00:00:34,559
and offer a durable long-term storage

00:00:31,679 --> 00:00:37,600
for your time series data

00:00:34,559 --> 00:00:40,879
cortex supports multi-tenancy

00:00:37,600 --> 00:00:43,840
and the typical use case is to have

00:00:40,879 --> 00:00:45,440
a global view across multiple prometheus

00:00:43,840 --> 00:00:48,879
servers

00:00:45,440 --> 00:00:51,280
over the years cortex also

00:00:48,879 --> 00:00:53,360
spent a lot of effort in order to

00:00:51,280 --> 00:00:57,199
optimize the read path

00:00:53,360 --> 00:01:01,039
and today cortex offers a very good

00:00:57,199 --> 00:01:03,920
query performances cortex is a cncf

00:01:01,039 --> 00:01:04,879
sandbar project and we are currently in

00:01:03,920 --> 00:01:08,560
the process

00:01:04,879 --> 00:01:11,280
of moving to incubation

00:01:08,560 --> 00:01:12,159
the typical use case of cortex is that

00:01:11,280 --> 00:01:15,439
you have

00:01:12,159 --> 00:01:18,560
multiple prometheus servers usually in

00:01:15,439 --> 00:01:21,439
configured in a shapers and

00:01:18,560 --> 00:01:21,840
you configure prometheus to remote write

00:01:21,439 --> 00:01:24,960
to

00:01:21,840 --> 00:01:27,520
a central cortex cluster where

00:01:24,960 --> 00:01:29,200
all your time series are written and

00:01:27,520 --> 00:01:32,320
then you configure grafana

00:01:29,200 --> 00:01:35,920
or the querying tool of your choice to

00:01:32,320 --> 00:01:39,680
query back this metrics from cortex

00:01:35,920 --> 00:01:43,119
cortex internally vendor prometheus and

00:01:39,680 --> 00:01:46,240
we use the same exact prometheus promql

00:01:43,119 --> 00:01:50,320
engine this guarantees 100

00:01:46,240 --> 00:01:50,320
compatibility on your queries

00:01:51,040 --> 00:01:54,640
if you look at the microservices

00:01:53,759 --> 00:01:57,280
architecture

00:01:54,640 --> 00:01:59,119
of cortex cortex is composed by

00:01:57,280 --> 00:02:02,320
different services

00:01:59,119 --> 00:02:02,719
interacting together different services

00:02:02,320 --> 00:02:05,759
which

00:02:02,719 --> 00:02:07,200
you can independently horizontally scale

00:02:05,759 --> 00:02:10,160
up and down

00:02:07,200 --> 00:02:12,239
on the right path you have a prometheus

00:02:10,160 --> 00:02:15,040
configured to remote right

00:02:12,239 --> 00:02:15,599
to the distributor and the distributor

00:02:15,040 --> 00:02:18,160
is

00:02:15,599 --> 00:02:19,360
the ingress for the right path and is

00:02:18,160 --> 00:02:22,720
responsible to

00:02:19,360 --> 00:02:24,800
shard and replicate your series across a

00:02:22,720 --> 00:02:27,520
pool of ingestors

00:02:24,800 --> 00:02:28,160
injesters keep they receive the serious

00:02:27,520 --> 00:02:31,519
samples

00:02:28,160 --> 00:02:34,160
in memory and periodically flush

00:02:31,519 --> 00:02:35,599
there's a series to the long term

00:02:34,160 --> 00:02:37,760
storage

00:02:35,599 --> 00:02:39,040
under the foot the long term storage is

00:02:37,760 --> 00:02:41,280
actually composed by

00:02:39,040 --> 00:02:42,400
two different data stores an object

00:02:41,280 --> 00:02:46,080
store and an

00:02:42,400 --> 00:02:50,480
index store the object store is like gcs

00:02:46,080 --> 00:02:54,160
or s3 is used to store chunks

00:02:50,480 --> 00:02:56,959
of compressed timestamp value pairs

00:02:54,160 --> 00:03:00,400
which we call chunks and the index store

00:02:56,959 --> 00:03:03,599
like big table or dynamodb or cassandra

00:03:00,400 --> 00:03:06,640
is used to store an inverted index

00:03:03,599 --> 00:03:10,720
which we use to look up the chunks by

00:03:06,640 --> 00:03:14,000
the label the query label matches

00:03:10,720 --> 00:03:16,640
and the query time time range

00:03:14,000 --> 00:03:18,800
on the read path you configure graffana

00:03:16,640 --> 00:03:22,239
or your queering tool

00:03:18,800 --> 00:03:24,720
to send the query request

00:03:22,239 --> 00:03:25,519
to the query frontend which is the

00:03:24,720 --> 00:03:28,239
ingress

00:03:25,519 --> 00:03:29,920
for the read path in a cortex cluster

00:03:28,239 --> 00:03:32,400
the query frontend

00:03:29,920 --> 00:03:33,519
the main purpose of the query frontend

00:03:32,400 --> 00:03:37,280
is to offer

00:03:33,519 --> 00:03:40,000
result caching and also

00:03:37,280 --> 00:03:40,640
employs some optimization techniques

00:03:40,000 --> 00:03:43,680
which

00:03:40,640 --> 00:03:44,720
we will see in details later on which

00:03:43,680 --> 00:03:47,920
allows to

00:03:44,720 --> 00:03:48,560
parallelize the query execution across

00:03:47,920 --> 00:03:51,680
multiple

00:03:48,560 --> 00:03:54,480
query nodes so the query

00:03:51,680 --> 00:03:56,319
or a splitted query that can be splitted

00:03:54,480 --> 00:03:59,599
by the query front-end is actually

00:03:56,319 --> 00:04:00,000
executed by a pool of queries which will

00:03:59,599 --> 00:04:01,680
fetch

00:04:00,000 --> 00:04:03,840
the most recent samples from the

00:04:01,680 --> 00:04:04,959
ingestors and all their data from the

00:04:03,840 --> 00:04:07,519
long term storage

00:04:04,959 --> 00:04:09,760
and then we'll run the promql engine on

00:04:07,519 --> 00:04:13,760
top of this

00:04:09,760 --> 00:04:16,400
now this slide shows you

00:04:13,760 --> 00:04:17,040
the microservices architecture but you

00:04:16,400 --> 00:04:19,199
are not

00:04:17,040 --> 00:04:20,799
required to deploy cortex in

00:04:19,199 --> 00:04:24,240
microservices architecture

00:04:20,799 --> 00:04:26,000
because cortex actually support a second

00:04:24,240 --> 00:04:28,000
operational mode which is the single

00:04:26,000 --> 00:04:30,720
binary mode and it's the easiest

00:04:28,000 --> 00:04:31,600
way to deploy a cortex cluster today so

00:04:30,720 --> 00:04:35,520
when you deploy

00:04:31,600 --> 00:04:37,680
cortex in single binary mode cortex

00:04:35,520 --> 00:04:39,120
is running as a single binary with a

00:04:37,680 --> 00:04:42,240
single configuration

00:04:39,120 --> 00:04:45,840
if you deploy inside kubernetes

00:04:42,240 --> 00:04:48,320
it will just be a single deployment

00:04:45,840 --> 00:04:49,120
and what we actually do is that

00:04:48,320 --> 00:04:52,240
internally

00:04:49,120 --> 00:04:54,080
within one single cortex process

00:04:52,240 --> 00:04:55,600
we run all the microservices all the

00:04:54,080 --> 00:04:57,840
cortex microservices

00:04:55,600 --> 00:04:59,759
so the microservices are hidden to you

00:04:57,840 --> 00:05:00,320
when you deploy cortex in single binary

00:04:59,759 --> 00:05:04,400
mode

00:05:00,320 --> 00:05:07,680
but you can still horizontally scale

00:05:04,400 --> 00:05:10,400
cortex running multiple replicas

00:05:07,680 --> 00:05:11,520
of the single binary and all the cortex

00:05:10,400 --> 00:05:13,759
properties

00:05:11,520 --> 00:05:14,639
like horizontal scalability and high

00:05:13,759 --> 00:05:17,759
availability

00:05:14,639 --> 00:05:21,039
or query performances are preserved in

00:05:17,759 --> 00:05:21,039
single binary mode as well

00:05:22,320 --> 00:05:27,680
this architecture um over the time

00:05:24,960 --> 00:05:31,199
proved to work and scale very well

00:05:27,680 --> 00:05:35,680
we have seen cortex clusters

00:05:31,199 --> 00:05:39,360
ranging from few tens to 100 million

00:05:35,680 --> 00:05:40,080
active series and uh in the typical use

00:05:39,360 --> 00:05:43,360
case

00:05:40,080 --> 00:05:44,880
um we see the 99.5 percentile query

00:05:43,360 --> 00:05:48,800
latency below

00:05:44,880 --> 00:05:51,680
2.5 second however

00:05:48,800 --> 00:05:52,240
for many users requiring both an object

00:05:51,680 --> 00:05:55,600
store

00:05:52,240 --> 00:05:57,440
and an index store introduce extra

00:05:55,600 --> 00:05:59,360
operational complexity

00:05:57,440 --> 00:06:00,800
and if you run in the cloud like big

00:05:59,360 --> 00:06:03,840
table or dynamodb

00:06:00,800 --> 00:06:06,080
also extra costs to run a cortex cluster

00:06:03,840 --> 00:06:07,919
so almost an year ago we started

00:06:06,080 --> 00:06:10,960
brainstorming on the idea to

00:06:07,919 --> 00:06:12,240
remove the index store at all the idea

00:06:10,960 --> 00:06:15,360
was well

00:06:12,240 --> 00:06:18,400
are we able to remove the index store

00:06:15,360 --> 00:06:21,680
dependency at all and store all the data

00:06:18,400 --> 00:06:25,199
only in the object store

00:06:21,680 --> 00:06:28,639
and that's how the cortex blocks storage

00:06:25,199 --> 00:06:29,520
started and specifically the cortex

00:06:28,639 --> 00:06:33,120
block storage

00:06:29,520 --> 00:06:36,479
is an alternative storage engine

00:06:33,120 --> 00:06:38,960
currently in the experimental phase

00:06:36,479 --> 00:06:39,759
we supporting cortex so you can deploy

00:06:38,960 --> 00:06:42,639
cortex

00:06:39,759 --> 00:06:43,440
with the chunk storage which is the

00:06:42,639 --> 00:06:46,560
architecture

00:06:43,440 --> 00:06:47,120
i just showed you or you can deploy

00:06:46,560 --> 00:06:50,000
cortex

00:06:47,120 --> 00:06:51,039
using the new and experimental block

00:06:50,000 --> 00:06:52,960
storage

00:06:51,039 --> 00:06:54,160
in this talk we are going to cover how

00:06:52,960 --> 00:06:57,120
the block storage work

00:06:54,160 --> 00:06:57,680
under the food hello everyone my name is

00:06:57,120 --> 00:06:59,919
marco

00:06:57,680 --> 00:07:01,199
i'm a software engineer at grafana labs

00:06:59,919 --> 00:07:03,840
i'm a cortex maintainer

00:07:01,199 --> 00:07:06,160
and i recently joined tino spontaneous

00:07:03,840 --> 00:07:06,160
as well

00:07:06,880 --> 00:07:10,880
hi i'm thor hansen i'm a software

00:07:08,880 --> 00:07:12,319
engineer at hashicorp

00:07:10,880 --> 00:07:14,240
i got involved with cortex in my

00:07:12,319 --> 00:07:16,240
previous role where we serve metrics and

00:07:14,240 --> 00:07:16,639
alerts for many customers we were a

00:07:16,240 --> 00:07:18,560
small

00:07:16,639 --> 00:07:19,680
team and needed the scalability of

00:07:18,560 --> 00:07:21,680
cortex

00:07:19,680 --> 00:07:23,360
as well as the multi-tenancy but didn't

00:07:21,680 --> 00:07:24,240
want to have to manage a separate index

00:07:23,360 --> 00:07:27,759
store

00:07:24,240 --> 00:07:27,759
which is where this all started for me

00:07:28,800 --> 00:07:32,240
so the main idea behind the block

00:07:30,400 --> 00:07:33,919
storage solution is to open up a

00:07:32,240 --> 00:07:36,560
prometheus tsdb

00:07:33,919 --> 00:07:38,880
per tenant per ingestor and upload these

00:07:36,560 --> 00:07:41,360
blocks to long-term storage

00:07:38,880 --> 00:07:42,720
a tsdb block both contains the chunks of

00:07:41,360 --> 00:07:45,520
compressed data points

00:07:42,720 --> 00:07:47,199
and importantly also contains the index

00:07:45,520 --> 00:07:48,879
and the entire block can easily be

00:07:47,199 --> 00:07:50,560
stored in an object store

00:07:48,879 --> 00:07:53,759
basically removing the need to run a

00:07:50,560 --> 00:07:53,759
dedicated index store

00:07:55,840 --> 00:07:59,840
but isn't this what thanos was already

00:07:57,440 --> 00:07:59,840
doing

00:07:59,919 --> 00:08:03,280
so instead of trying to reinvent the

00:08:01,440 --> 00:08:04,479
wheel we can leverage the lessons

00:08:03,280 --> 00:08:07,280
learned from thanos

00:08:04,479 --> 00:08:07,280
and grow on that

00:08:08,400 --> 00:08:12,160
so i built the initial version of this

00:08:09,840 --> 00:08:12,639
idea it would open a tsdb for each

00:08:12,160 --> 00:08:14,160
tenant

00:08:12,639 --> 00:08:17,360
on every ingestor to store incoming

00:08:14,160 --> 00:08:19,039
rights these tsdbs would periodically

00:08:17,360 --> 00:08:22,319
write a block to the ingestor's local

00:08:19,039 --> 00:08:24,160
disk under a tenant prefix directory

00:08:22,319 --> 00:08:26,080
ingestor started a thanos shipper per

00:08:24,160 --> 00:08:27,120
tenant to scan each of these tsdb's

00:08:26,080 --> 00:08:28,720
directories

00:08:27,120 --> 00:08:30,400
and upload newly written blocks to

00:08:28,720 --> 00:08:32,000
long-term storage

00:08:30,400 --> 00:08:34,000
ingestors would now serve queries

00:08:32,000 --> 00:08:36,000
straight from the local tsdb

00:08:34,000 --> 00:08:37,279
injector tsubs had a given retention

00:08:36,000 --> 00:08:38,640
period where blocks that had been

00:08:37,279 --> 00:08:39,519
shipped to storage but exceeded the

00:08:38,640 --> 00:08:42,560
retention period

00:08:39,519 --> 00:08:44,720
were deleted from local storage it also

00:08:42,560 --> 00:08:46,640
modified the query to use thanos querier

00:08:44,720 --> 00:08:48,800
which would download and cache the index

00:08:46,640 --> 00:08:51,519
and blocks found in long-term storage

00:08:48,800 --> 00:08:52,399
for each tenant prefix query could then

00:08:51,519 --> 00:08:54,320
serve requests

00:08:52,399 --> 00:08:56,160
by either using the cash blocks or

00:08:54,320 --> 00:08:58,560
downloading blocks on demand from block

00:08:56,160 --> 00:08:58,560
storage

00:08:59,760 --> 00:09:03,040
i'd like to give special thanks to peter

00:09:02,160 --> 00:09:04,399
and ganesh

00:09:03,040 --> 00:09:06,480
for their contributions to this

00:09:04,399 --> 00:09:08,160
community effort as well as others who

00:09:06,480 --> 00:09:10,720
put in time and effort into making this

00:09:08,160 --> 00:09:12,480
idea a real solution it took us nine

00:09:10,720 --> 00:09:14,480
more months of hard work to stabilize

00:09:12,480 --> 00:09:17,040
and scale out the block storage

00:09:14,480 --> 00:09:18,000
we introduced two new cortex services

00:09:17,040 --> 00:09:20,560
the compactor

00:09:18,000 --> 00:09:21,760
and store gateway we added three layers

00:09:20,560 --> 00:09:25,279
of caching

00:09:21,760 --> 00:09:26,959
index chunks and metadata we implemented

00:09:25,279 --> 00:09:29,200
most of the existing cortex features

00:09:26,959 --> 00:09:31,200
like rate limits and operational tooling

00:09:29,200 --> 00:09:32,720
and we've made many optimizations and

00:09:31,200 --> 00:09:34,399
bug fixes

00:09:32,720 --> 00:09:36,640
today the cortex block storage is still

00:09:34,399 --> 00:09:37,920
marked experimental but at kerfano labs

00:09:36,640 --> 00:09:39,040
they're already running it at scale in a

00:09:37,920 --> 00:09:40,560
few of their clusters

00:09:39,040 --> 00:09:43,200
and we expect to mark this feature as

00:09:40,560 --> 00:09:43,200
stable soon

00:09:44,320 --> 00:09:48,080
so let me show you the current state the

00:09:46,880 --> 00:09:49,760
cortex architecture

00:09:48,080 --> 00:09:51,680
doesn't change much between the original

00:09:49,760 --> 00:09:52,480
trunk storage which we'll still continue

00:09:51,680 --> 00:09:54,880
to support

00:09:52,480 --> 00:09:56,240
and the new black storage writes still

00:09:54,880 --> 00:09:57,120
are distributed and replicated to

00:09:56,240 --> 00:09:59,200
ingestors

00:09:57,120 --> 00:10:00,800
and then uploaded to object storage

00:09:59,200 --> 00:10:02,320
reads still go through the queries which

00:10:00,800 --> 00:10:03,600
read from the ingestors to get recent

00:10:02,320 --> 00:10:04,959
metrics

00:10:03,600 --> 00:10:06,480
the bigger changes come with two

00:10:04,959 --> 00:10:07,040
additions to querying from long-term

00:10:06,480 --> 00:10:08,560
storage

00:10:07,040 --> 00:10:11,120
we added the store gateway and the

00:10:08,560 --> 00:10:11,120
compactor

00:10:11,920 --> 00:10:15,680
store gateway is responsible for the

00:10:13,360 --> 00:10:17,760
discovery of newly uploaded blocks

00:10:15,680 --> 00:10:19,440
it will scan for newly uploaded blocks

00:10:17,760 --> 00:10:20,959
and download the index in each block and

00:10:19,440 --> 00:10:22,800
cache them locally

00:10:20,959 --> 00:10:24,320
with these stored indices it acts as a

00:10:22,800 --> 00:10:24,800
queryable interface from long-term

00:10:24,320 --> 00:10:28,480
storage

00:10:24,800 --> 00:10:28,480
and can fetch query blocks on demand

00:10:29,040 --> 00:10:32,160
the compactor is responsible for

00:10:30,880 --> 00:10:33,920
reducing the number of blocks in

00:10:32,160 --> 00:10:35,360
long-term storage by combining and

00:10:33,920 --> 00:10:37,040
de-duplicating blocks

00:10:35,360 --> 00:10:38,800
it will periodically scan attendance

00:10:37,040 --> 00:10:41,120
prefix and object storage to locate

00:10:38,800 --> 00:10:42,800
blocks that are eligible for compaction

00:10:41,120 --> 00:10:44,640
using the thanos file name format to

00:10:42,800 --> 00:10:45,839
determine overlap

00:10:44,640 --> 00:10:47,920
it will download the compaction

00:10:45,839 --> 00:10:49,279
candidate blocks and create a new larger

00:10:47,920 --> 00:10:51,040
block out of the data

00:10:49,279 --> 00:10:54,880
we will then upload that new block and

00:10:51,040 --> 00:10:54,880
mark the compacted blocks for deletion

00:10:56,560 --> 00:11:00,320
the compactor and store gateway leverage

00:10:58,480 --> 00:11:02,800
the same sharding ring code that cortex

00:11:00,320 --> 00:11:04,560
and gestures use to distribute tenants

00:11:02,800 --> 00:11:06,240
so as their scale increases the number

00:11:04,560 --> 00:11:08,800
of tenants they need to perform work on

00:11:06,240 --> 00:11:10,480
decreases however at this time there's

00:11:08,800 --> 00:11:13,120
still no way to scale compaction for a

00:11:10,480 --> 00:11:13,120
single tenant

00:11:13,600 --> 00:11:16,399
all of these new courts are new to

00:11:15,040 --> 00:11:17,200
cortex but are actually borrowed from

00:11:16,399 --> 00:11:18,480
thanos

00:11:17,200 --> 00:11:19,920
they have been updated to support

00:11:18,480 --> 00:11:22,079
multi-tenancy and sharding for

00:11:19,920 --> 00:11:23,440
horizontal scalability

00:11:22,079 --> 00:11:27,040
the ingestor shipping to long-term

00:11:23,440 --> 00:11:28,959
storage is based on thanos shipper

00:11:27,040 --> 00:11:30,560
the store gateway querying blocks from

00:11:28,959 --> 00:11:32,000
long-term storage is based on thanos

00:11:30,560 --> 00:11:34,000
bucket store

00:11:32,000 --> 00:11:36,480
and the compactor is based on thanos

00:11:34,000 --> 00:11:36,480
compactor

00:11:37,360 --> 00:11:40,880
so here's a closer look at the right

00:11:38,640 --> 00:11:42,959
path a given metric is replicated to

00:11:40,880 --> 00:11:45,200
multiple ingestors for each user

00:11:42,959 --> 00:11:46,720
so you can see here the same metric is

00:11:45,200 --> 00:11:48,560
written to three separate blocks and

00:11:46,720 --> 00:11:49,680
three separate ingestors

00:11:48,560 --> 00:11:51,839
these blocks are written to an

00:11:49,680 --> 00:11:53,760
adjuster's local storage every two hours

00:11:51,839 --> 00:11:55,200
and a long running process called the

00:11:53,760 --> 00:11:56,959
shipper will discover

00:11:55,200 --> 00:11:59,760
newly written blocks and upload them to

00:11:56,959 --> 00:11:59,760
long-term storage

00:12:00,720 --> 00:12:04,000
this poses an interesting scaling

00:12:02,399 --> 00:12:05,279
problem in regards to how many blocks

00:12:04,000 --> 00:12:06,959
are created

00:12:05,279 --> 00:12:08,800
the number of blocks grows quickly with

00:12:06,959 --> 00:12:10,240
the number of tenants a cluster has

00:12:08,800 --> 00:12:12,320
with a thousand tenants and a small

00:12:10,240 --> 00:12:14,079
scale of 50 ingestors we'll be creating

00:12:12,320 --> 00:12:16,000
600 000 blocks a day

00:12:14,079 --> 00:12:18,240
which equates to 200 million blocks a

00:12:16,000 --> 00:12:18,240
year

00:12:19,040 --> 00:12:22,320
since we replicate data and ingestors

00:12:21,040 --> 00:12:24,959
typically recommended to be

00:12:22,320 --> 00:12:26,399
3x the issue is we're storing a large

00:12:24,959 --> 00:12:28,079
number of identical metrics

00:12:26,399 --> 00:12:29,920
in storage when we really only need to

00:12:28,079 --> 00:12:32,800
be storing a single copy of each data

00:12:29,920 --> 00:12:32,800
point in storage

00:12:33,600 --> 00:12:36,880
the solution is the compactor that was

00:12:35,600 --> 00:12:38,720
mentioned earlier

00:12:36,880 --> 00:12:40,480
which performs both horizontal

00:12:38,720 --> 00:12:42,160
compaction which creates fewer larger

00:12:40,480 --> 00:12:44,000
blocks over a greater time period than

00:12:42,160 --> 00:12:45,839
just the two hours that was uploaded

00:12:44,000 --> 00:12:47,760
as well as vertical compaction which

00:12:45,839 --> 00:12:52,240
deduplicates the overlapping blocks

00:12:47,760 --> 00:12:54,639
from the ingested replication

00:12:52,240 --> 00:12:55,279
this results in one block per day per

00:12:54,639 --> 00:12:57,200
tenant

00:12:55,279 --> 00:12:59,200
when we originally had 12 times the

00:12:57,200 --> 00:13:00,720
number of ingestors pretended

00:12:59,200 --> 00:13:02,639
and importantly the compactor can

00:13:00,720 --> 00:13:06,800
horizontally scale to support a large

00:13:02,639 --> 00:13:08,720
number of tenants

00:13:06,800 --> 00:13:10,399
now the previous solution reduces the

00:13:08,720 --> 00:13:11,440
footprint after compaction in the object

00:13:10,399 --> 00:13:13,600
storage

00:13:11,440 --> 00:13:14,959
but we are still shipping one block per

00:13:13,600 --> 00:13:16,959
adjuster per tenant

00:13:14,959 --> 00:13:19,360
every two hours as well as opening a

00:13:16,959 --> 00:13:20,959
tsdb on every ingester for every tenon

00:13:19,360 --> 00:13:24,079
which meant the memory footprint did not

00:13:20,959 --> 00:13:24,079
scale with the number of nodes

00:13:25,040 --> 00:13:28,320
the solution was shuffle sharding where

00:13:27,120 --> 00:13:30,079
for any given tenant

00:13:28,320 --> 00:13:32,160
that user's blocks would only end up on

00:13:30,079 --> 00:13:34,079
a subset of ingestors

00:13:32,160 --> 00:13:36,079
so for example tenants may be

00:13:34,079 --> 00:13:38,079
shuffle-sharded to foreign gestures

00:13:36,079 --> 00:13:39,360
so for every user only four blocks per

00:13:38,079 --> 00:13:41,760
tenant per two hours

00:13:39,360 --> 00:13:43,440
are uploaded to object storage and each

00:13:41,760 --> 00:13:45,680
tenant uses a static amount of memory

00:13:43,440 --> 00:13:47,199
overhead for the tstbs

00:13:45,680 --> 00:13:49,360
this also has the benefit that the

00:13:47,199 --> 00:13:50,959
compactor no longer needs to perform as

00:13:49,360 --> 00:13:52,399
much work compacting blocks

00:13:50,959 --> 00:13:55,040
as we've reduced the number of blocks

00:13:52,399 --> 00:13:55,040
that are uploaded

00:13:56,160 --> 00:13:59,120
here's an example of write performance

00:13:57,760 --> 00:14:00,639
courtesy of grafana labs in the

00:13:59,120 --> 00:14:02,800
environment they're running

00:14:00,639 --> 00:14:04,800
we can see there's still a very low 99th

00:14:02,800 --> 00:14:08,800
percentile latencies with two and a half

00:14:04,800 --> 00:14:08,800
million samples per second ingested

00:14:09,920 --> 00:14:16,320
so we have seen that we can

00:14:13,199 --> 00:14:18,079
efficiently he ingests a large amount of

00:14:16,320 --> 00:14:19,040
samples per second with a pretty low

00:14:18,079 --> 00:14:21,680
latency

00:14:19,040 --> 00:14:23,199
but the real question is how do we query

00:14:21,680 --> 00:14:26,000
back this data

00:14:23,199 --> 00:14:27,680
um one in one of our clusters we are

00:14:26,000 --> 00:14:30,880
running at grifon labs

00:14:27,680 --> 00:14:33,120
we have a tenant with 30 million active

00:14:30,880 --> 00:14:36,160
series running on the block storage

00:14:33,120 --> 00:14:39,440
um which means that we store

00:14:36,160 --> 00:14:42,480
about 200 gigabyte of blocks per day

00:14:39,440 --> 00:14:46,240
after compaction if you project this

00:14:42,480 --> 00:14:48,399
uh to one ear retention it means that

00:14:46,240 --> 00:14:49,279
we need to be ready to query back this

00:14:48,399 --> 00:14:52,320
data

00:14:49,279 --> 00:14:54,480
about across a storage of about 70

00:14:52,320 --> 00:14:57,199
terabytes

00:14:54,480 --> 00:14:57,839
uh to understand how the the read path

00:14:57,199 --> 00:14:59,760
work

00:14:57,839 --> 00:15:01,440
for the block storage uh we have to do a

00:14:59,760 --> 00:15:03,360
little step back

00:15:01,440 --> 00:15:05,199
and focusing on the query front-end

00:15:03,360 --> 00:15:07,839
which is the ingress

00:15:05,199 --> 00:15:08,800
on the read path now as previously

00:15:07,839 --> 00:15:11,360
mentioned

00:15:08,800 --> 00:15:12,160
the query frontend provide two main

00:15:11,360 --> 00:15:14,560
features

00:15:12,160 --> 00:15:16,639
query execution parallelization and

00:15:14,560 --> 00:15:19,920
result caching

00:15:16,639 --> 00:15:21,360
the basic form of the query execution

00:15:19,920 --> 00:15:24,639
parallelization

00:15:21,360 --> 00:15:28,240
is based on time splitting

00:15:24,639 --> 00:15:31,360
when the query frontend receive a query

00:15:28,240 --> 00:15:32,399
spanning over a large time range a time

00:15:31,360 --> 00:15:35,120
range which cover

00:15:32,399 --> 00:15:36,079
more than one day the query frontend

00:15:35,120 --> 00:15:40,160
split

00:15:36,079 --> 00:15:40,560
this query into multiple queries each

00:15:40,160 --> 00:15:43,680
one

00:15:40,560 --> 00:15:45,680
covering one single day

00:15:43,680 --> 00:15:48,160
aligning the timestamp of the splitted

00:15:45,680 --> 00:15:51,360
query to midnight utc

00:15:48,160 --> 00:15:54,480
so if we receive a query spanning over

00:15:51,360 --> 00:15:57,680
the last three days this query will be

00:15:54,480 --> 00:16:00,240
actually executed will be actually split

00:15:57,680 --> 00:16:02,079
into three different queries each query

00:16:00,240 --> 00:16:04,560
will cover a different day

00:16:02,079 --> 00:16:06,399
and there's three splitted queries will

00:16:04,560 --> 00:16:09,600
be executed concurrently

00:16:06,399 --> 00:16:10,000
by the queries and their results will be

00:16:09,600 --> 00:16:12,079
then

00:16:10,000 --> 00:16:14,160
merged by the query frontend before

00:16:12,079 --> 00:16:17,360
sending back the response to grafana

00:16:14,160 --> 00:16:17,360
or your quilling tool

00:16:18,240 --> 00:16:24,800
now this means that in most of the cases

00:16:22,079 --> 00:16:25,519
internally a single query executed by

00:16:24,800 --> 00:16:28,320
the query

00:16:25,519 --> 00:16:29,600
will cover only one day and that's the

00:16:28,320 --> 00:16:33,120
primary reason

00:16:29,600 --> 00:16:36,639
why by default we compact blocks up to

00:16:33,120 --> 00:16:40,160
one day period and doing

00:16:36,639 --> 00:16:40,800
tests uh in our clusters we have seen

00:16:40,160 --> 00:16:43,839
that

00:16:40,800 --> 00:16:48,480
this allows for a better parallelization

00:16:43,839 --> 00:16:48,480
when you run queries over a large time

00:16:48,839 --> 00:16:53,440
range

00:16:50,079 --> 00:16:57,759
the querier then execute this

00:16:53,440 --> 00:17:00,560
one day query now the query

00:16:57,759 --> 00:17:02,079
periodically discover new blocks

00:17:00,560 --> 00:17:04,959
uploaded to the storage

00:17:02,079 --> 00:17:06,480
running a scan over the bucket and it

00:17:04,959 --> 00:17:08,880
keeps in memory a map

00:17:06,480 --> 00:17:09,839
of all the known blocks in the storage

00:17:08,880 --> 00:17:11,919
for each block

00:17:09,839 --> 00:17:13,439
we just need few information which is

00:17:11,919 --> 00:17:16,480
the tenant id

00:17:13,439 --> 00:17:19,199
the block id and the minimum and maximum

00:17:16,480 --> 00:17:20,319
timestamp of samples within the specific

00:17:19,199 --> 00:17:23,280
block

00:17:20,319 --> 00:17:24,160
and then when the query receive a query

00:17:23,280 --> 00:17:26,559
it look for

00:17:24,160 --> 00:17:27,760
all the block ids containing at least

00:17:26,559 --> 00:17:31,200
one sample

00:17:27,760 --> 00:17:34,880
within the query start and end time

00:17:31,200 --> 00:17:37,919
and based on the filtered block ideas

00:17:34,880 --> 00:17:39,200
it computes the set of store gateways

00:17:37,919 --> 00:17:41,360
that should be queried

00:17:39,200 --> 00:17:43,440
in order to query back the data from

00:17:41,360 --> 00:17:45,840
desk blocks

00:17:43,440 --> 00:17:47,760
the query will fetch the most recent

00:17:45,840 --> 00:17:49,679
data which has not been flushed to the

00:17:47,760 --> 00:17:52,960
storage yet from the ingestors

00:17:49,679 --> 00:17:55,039
and we will query the blocks stored

00:17:52,960 --> 00:17:57,520
in the object store through the store

00:17:55,039 --> 00:17:57,520
gateways

00:17:58,480 --> 00:18:02,720
blocks are sharded and optionally

00:18:01,760 --> 00:18:06,080
replicated

00:18:02,720 --> 00:18:06,720
across the store gateways this means

00:18:06,080 --> 00:18:09,919
that

00:18:06,720 --> 00:18:12,400
we can horizontally shard the blocks

00:18:09,919 --> 00:18:13,520
across the pool of store gateways and

00:18:12,400 --> 00:18:16,000
for each block

00:18:13,520 --> 00:18:17,280
belonging to the chart of one specific

00:18:16,000 --> 00:18:20,559
store gateway

00:18:17,280 --> 00:18:23,760
the store gateway loads the index editor

00:18:20,559 --> 00:18:27,039
the index adder is a small subset of the

00:18:23,760 --> 00:18:28,799
entire block indexer in in our clusters

00:18:27,039 --> 00:18:30,640
we see that the index editor is in the

00:18:28,799 --> 00:18:31,280
order of two percent of the index so

00:18:30,640 --> 00:18:34,799
it's a

00:18:31,280 --> 00:18:37,919
little part of the index

00:18:34,799 --> 00:18:43,520
and it's used to speed up um

00:18:37,919 --> 00:18:46,720
the index lookup at query time

00:18:43,520 --> 00:18:49,600
the query query the blocks through

00:18:46,720 --> 00:18:51,280
the minimum set of store gateways

00:18:49,600 --> 00:18:53,520
holding the required blocks

00:18:51,280 --> 00:18:54,799
so when the query receive a query the

00:18:53,520 --> 00:18:57,440
query computer

00:18:54,799 --> 00:18:58,640
the list of block ideas which should be

00:18:57,440 --> 00:19:01,600
queried

00:18:58,640 --> 00:19:03,120
and then look up the ash ring which we

00:19:01,600 --> 00:19:06,480
use

00:19:03,120 --> 00:19:07,840
to as the baseline technology to

00:19:06,480 --> 00:19:10,000
implement the sharding and the

00:19:07,840 --> 00:19:11,919
replication to find

00:19:10,000 --> 00:19:13,039
the minimum set of store gateways

00:19:11,919 --> 00:19:16,640
holding

00:19:13,039 --> 00:19:19,600
the blocks that needs to be queried

00:19:16,640 --> 00:19:22,080
and concurrently query does blocks

00:19:19,600 --> 00:19:25,440
through the store gateways

00:19:22,080 --> 00:19:28,240
if we look inside a single store gateway

00:19:25,440 --> 00:19:30,480
the store gateway always download the

00:19:28,240 --> 00:19:34,000
fully download to the local disk

00:19:30,480 --> 00:19:34,480
and then hem up in memory the index

00:19:34,000 --> 00:19:37,520
setter

00:19:34,480 --> 00:19:39,520
of each block belonging

00:19:37,520 --> 00:19:42,080
to the shard of the specific store

00:19:39,520 --> 00:19:44,080
gateway but the entire index

00:19:42,080 --> 00:19:45,520
or the chunks files which are even

00:19:44,080 --> 00:19:48,640
bigger have never

00:19:45,520 --> 00:19:50,400
entirely downloaded to the to the store

00:19:48,640 --> 00:19:52,320
gateway

00:19:50,400 --> 00:19:53,919
how the system work which has been

00:19:52,320 --> 00:19:57,280
inherited by thanos

00:19:53,919 --> 00:20:00,480
is that uh the full index and the chunks

00:19:57,280 --> 00:20:03,440
are lazily fetched at query time uh

00:20:00,480 --> 00:20:04,240
through multiple get byte range request

00:20:03,440 --> 00:20:08,159
so we

00:20:04,240 --> 00:20:11,520
load the minimum data as possible

00:20:08,159 --> 00:20:14,640
when we add query time if we look

00:20:11,520 --> 00:20:15,360
at a single query received in the store

00:20:14,640 --> 00:20:17,120
gateway

00:20:15,360 --> 00:20:18,480
the typical query contain four

00:20:17,120 --> 00:20:22,159
information

00:20:18,480 --> 00:20:25,120
the setup series label matches

00:20:22,159 --> 00:20:26,159
for which the samples should be fetched

00:20:25,120 --> 00:20:28,960
a start

00:20:26,159 --> 00:20:30,080
and an end timestamp and the list of

00:20:28,960 --> 00:20:31,919
block ids

00:20:30,080 --> 00:20:33,600
which has been previously computed by

00:20:31,919 --> 00:20:36,080
the query

00:20:33,600 --> 00:20:37,360
and then the store gateway will run a

00:20:36,080 --> 00:20:40,159
local lookup

00:20:37,360 --> 00:20:40,720
on the symbols and posting of the table

00:20:40,159 --> 00:20:42,960
which are

00:20:40,720 --> 00:20:44,799
done through the previously downloaded

00:20:42,960 --> 00:20:48,159
index editor

00:20:44,799 --> 00:20:49,600
and the result of this lookup is

00:20:48,159 --> 00:20:53,039
actually the input for the

00:20:49,600 --> 00:20:55,440
remote lookup and so the store gateway

00:20:53,039 --> 00:20:56,880
will fetch the postings and the series

00:20:55,440 --> 00:20:59,840
and then from there

00:20:56,880 --> 00:21:00,559
the chunks which are the segments of

00:20:59,840 --> 00:21:02,880
compressed

00:21:00,559 --> 00:21:05,039
timestamp value person of the samples

00:21:02,880 --> 00:21:07,679
for the matching series

00:21:05,039 --> 00:21:10,240
this remote lookup is done through get

00:21:07,679 --> 00:21:13,919
byte range request

00:21:10,240 --> 00:21:16,960
now we actually want to

00:21:13,919 --> 00:21:17,360
lower as much as possible the calls to

00:21:16,960 --> 00:21:20,720
the

00:21:17,360 --> 00:21:21,440
object store both because of performance

00:21:20,720 --> 00:21:25,039
reasons

00:21:21,440 --> 00:21:28,640
and also because of costs given

00:21:25,039 --> 00:21:31,679
most of the object store pricing

00:21:28,640 --> 00:21:35,280
is based on a mix of data stored

00:21:31,679 --> 00:21:36,720
in terms of gigabyte and number of api

00:21:35,280 --> 00:21:39,840
calls you run

00:21:36,720 --> 00:21:42,000
and we have introduced uh three layers

00:21:39,840 --> 00:21:44,000
of caching the metadata cache the index

00:21:42,000 --> 00:21:47,520
cache and the chunks cache

00:21:44,000 --> 00:21:49,840
the metadata cache is used by the

00:21:47,520 --> 00:21:50,880
blocks discovery mechanism which is

00:21:49,840 --> 00:21:54,240
running both

00:21:50,880 --> 00:21:55,919
inside the query and the store gateway

00:21:54,240 --> 00:21:58,720
while the index cache and the chunks

00:21:55,919 --> 00:22:01,039
cache is only used by the store gateway

00:21:58,720 --> 00:22:01,840
the index cache is a caching layer in

00:22:01,039 --> 00:22:04,159
front of the

00:22:01,840 --> 00:22:05,360
posting and serious lookup and the

00:22:04,159 --> 00:22:09,760
chunks cache

00:22:05,360 --> 00:22:12,400
is a cache in front of

00:22:09,760 --> 00:22:12,880
the fetching of the chunks containing

00:22:12,400 --> 00:22:15,520
the

00:22:12,880 --> 00:22:16,240
compressed samples since the chunks

00:22:15,520 --> 00:22:19,280
files

00:22:16,240 --> 00:22:21,840
are up to 512 megabytes each

00:22:19,280 --> 00:22:23,280
we don't nev we never download we never

00:22:21,840 --> 00:22:26,000
fully download

00:22:23,280 --> 00:22:27,440
the entire object or the entire file and

00:22:26,000 --> 00:22:32,240
we never fully cache

00:22:27,440 --> 00:22:35,440
uh a single entry with 1 512 megabyte

00:22:32,240 --> 00:22:36,400
but what we do is we do a sub-object

00:22:35,440 --> 00:22:41,760
caching

00:22:36,400 --> 00:22:45,200
uh aligning the offset to 16 kilobyte

00:22:41,760 --> 00:22:47,919
caching is not mandatory

00:22:45,200 --> 00:22:50,480
so it's an optional component but it's

00:22:47,919 --> 00:22:53,600
recommended in production

00:22:50,480 --> 00:22:57,200
last but not least we have done

00:22:53,600 --> 00:22:58,480
this work as a necessity we had in

00:22:57,200 --> 00:23:00,720
cortex but we

00:22:58,480 --> 00:23:03,440
back ported all those improvements to

00:23:00,720 --> 00:23:03,440
thanos as well

00:23:04,720 --> 00:23:08,000
at grafana labs we are currently running

00:23:07,280 --> 00:23:11,760
a

00:23:08,000 --> 00:23:15,120
few clusters and in one of test clusters

00:23:11,760 --> 00:23:19,280
which is a staging cluster we have

00:23:15,120 --> 00:23:22,080
done we have done an atypical setup

00:23:19,280 --> 00:23:23,679
so we are running two identical clusters

00:23:22,080 --> 00:23:26,240
since few months

00:23:23,679 --> 00:23:27,200
by identical i mean identical in terms

00:23:26,240 --> 00:23:30,640
of

00:23:27,200 --> 00:23:33,679
version of cortex that we run and

00:23:30,640 --> 00:23:36,720
scale same number of

00:23:33,679 --> 00:23:37,280
cortex nodes ingesting the same exact

00:23:36,720 --> 00:23:41,440
series

00:23:37,280 --> 00:23:45,600
roughly 10 mega 10 million active series

00:23:41,440 --> 00:23:47,840
and then we have introduced a proxy

00:23:45,600 --> 00:23:49,200
uh which is called query t and we open

00:23:47,840 --> 00:23:51,440
source as well

00:23:49,200 --> 00:23:53,520
which mirror every single query we do

00:23:51,440 --> 00:23:56,240
receive to both clusters

00:23:53,520 --> 00:23:57,600
and we make the two backend clusters one

00:23:56,240 --> 00:23:59,200
running the block storage

00:23:57,600 --> 00:24:03,520
and another one running the chunk

00:23:59,200 --> 00:24:05,919
storage compete in terms of performances

00:24:03,520 --> 00:24:07,120
what we have seen uh is that the block

00:24:05,919 --> 00:24:09,679
storage performances

00:24:07,120 --> 00:24:10,799
are pretty good and comparable with the

00:24:09,679 --> 00:24:14,080
chunk storage

00:24:10,799 --> 00:24:18,159
for most of the use cases

00:24:14,080 --> 00:24:21,440
we still have some spikes in the p99

00:24:18,159 --> 00:24:24,880
in the block storage mainly due

00:24:21,440 --> 00:24:28,400
to cold cold caches

00:24:24,880 --> 00:24:31,760
but the the progress we have made

00:24:28,400 --> 00:24:34,880
and measured this way over the past

00:24:31,760 --> 00:24:37,039
few months is pretty good and

00:24:34,880 --> 00:24:38,000
i personally believe we are on a good

00:24:37,039 --> 00:24:40,559
track

00:24:38,000 --> 00:24:41,840
to run the block storage with comparable

00:24:40,559 --> 00:24:46,080
performances

00:24:41,840 --> 00:24:47,200
compared to the to the chunk storage now

00:24:46,080 --> 00:24:50,400
the question here is

00:24:47,200 --> 00:24:50,960
what's next um so if you if you're

00:24:50,400 --> 00:24:52,799
running

00:24:50,960 --> 00:24:54,559
the block storage the experimental block

00:24:52,799 --> 00:24:56,799
storage today um

00:24:54,559 --> 00:24:57,679
or you are interested into giving it a

00:24:56,799 --> 00:25:00,640
try

00:24:57,679 --> 00:25:01,760
please be aware we are very close to

00:25:00,640 --> 00:25:04,799
market stable

00:25:01,760 --> 00:25:07,760
um it's something we expect will happen

00:25:04,799 --> 00:25:07,760
uh this quarter

00:25:07,840 --> 00:25:14,159
we are currently working on many things

00:25:10,960 --> 00:25:16,400
but to mention um the

00:25:14,159 --> 00:25:18,080
most important probably we are

00:25:16,400 --> 00:25:20,400
continuously working to

00:25:18,080 --> 00:25:21,679
improve the query performances this is

00:25:20,400 --> 00:25:25,039
an endless work

00:25:21,679 --> 00:25:25,679
um and we are doing many high iterations

00:25:25,039 --> 00:25:28,480
over it

00:25:25,679 --> 00:25:29,679
we have several ideas we still want to

00:25:28,480 --> 00:25:32,880
to experiment

00:25:29,679 --> 00:25:34,559
and that they may lend into upstream

00:25:32,880 --> 00:25:37,440
changes

00:25:34,559 --> 00:25:38,240
but yeah keep in mind we are spending a

00:25:37,440 --> 00:25:42,960
lot of time

00:25:38,240 --> 00:25:45,200
on continuously improving performances

00:25:42,960 --> 00:25:47,039
we also want to productionize the

00:25:45,200 --> 00:25:49,760
shuffle shutting

00:25:47,039 --> 00:25:50,400
and basically we want to be able to

00:25:49,760 --> 00:25:53,679
scale

00:25:50,400 --> 00:25:57,279
a cortex cluster to

00:25:53,679 --> 00:26:01,440
any size of tenants under tenants

00:25:57,279 --> 00:26:04,400
thousand tenants tens thousand tenants

00:26:01,440 --> 00:26:05,360
and we believe that shuffle sharding is

00:26:04,400 --> 00:26:08,320
the way to go

00:26:05,360 --> 00:26:09,840
in this direction and also in the thanos

00:26:08,320 --> 00:26:12,559
community

00:26:09,840 --> 00:26:15,200
we recently started the work to

00:26:12,559 --> 00:26:19,760
introduce the support for deletions

00:26:15,200 --> 00:26:24,400
being able to selectively delete

00:26:19,760 --> 00:26:27,679
serious and time series data from thanos

00:26:24,400 --> 00:26:28,159
we are very interested uh in deletions

00:26:27,679 --> 00:26:31,360
as well

00:26:28,159 --> 00:26:33,600
uh in the cortex community and

00:26:31,360 --> 00:26:35,039
we will work closely uh with the thanos

00:26:33,600 --> 00:26:37,919
community to make it happen

00:26:35,039 --> 00:26:41,440
and introduce the support for deletions

00:26:37,919 --> 00:26:41,440
in the cortex block storage as well

00:26:42,400 --> 00:26:46,799
so thank you very much for joining this

00:26:45,120 --> 00:26:49,919
talk

00:26:46,799 --> 00:26:50,400
now there will be a qa session but

00:26:49,919 --> 00:26:52,320
please

00:26:50,400 --> 00:26:53,520
don't forget to check out the cncf

00:26:52,320 --> 00:26:56,799
schedule

00:26:53,520 --> 00:27:00,000
there are a couple of cortex rooms

00:26:56,799 --> 00:27:02,080
and booth hours

00:27:00,000 --> 00:27:03,360
so please check out the schedule and

00:27:02,080 --> 00:27:07,039
join us if you have

00:27:03,360 --> 00:27:07,039

YouTube URL: https://www.youtube.com/watch?v=Z5OJzRogAS4


