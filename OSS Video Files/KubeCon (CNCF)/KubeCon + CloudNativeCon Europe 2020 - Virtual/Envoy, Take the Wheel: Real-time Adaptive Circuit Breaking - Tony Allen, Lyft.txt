Title: Envoy, Take the Wheel: Real-time Adaptive Circuit Breaking - Tony Allen, Lyft
Publication date: 2020-08-27
Playlist: KubeCon + CloudNativeCon Europe 2020 - Virtual
Description: 
	Don’t miss out! Join us at our upcoming events: EnvoyCon Virtual on October 15 and KubeCon + CloudNativeCon North America 2020 Virtual from November 17-20. Learn more at https://kubecon.io. The conferences feature presentations from developers and end users of Kubernetes, Prometheus, Envoy, and all of the other CNCF-hosted projects.  

Envoy, Take the Wheel: Real-time Adaptive Circuit Breaking - Tony Allen, Lyft 

Modern service mesh deployments are equipped with hundreds of tunables, such as timeouts and circuit breakers. Finding ideal initial values requires deep technical expertise. Workloads change over time, requiring regular effort to re-tune stale parameters. As a consequence, configuration errors have become a source of operational toil and one of the major causes of system failures across the industry. The service mesh should aim to expose a minimal configuration surface by dynamically adjusting parameters based on observations.  Tony Allen will provide a deep-dive into how Envoy’s Adaptive Concurrency Control feature dynamically tunes circuit breaker thresholds using real-time sampling of request latencies, removing the need for periodic adjustment. He will also discuss lessons learned deploying the feature to Lyft’s production service mesh.

https://sched.co/Zes8
Captions: 
	00:00:00,080 --> 00:00:04,400
hello everybody my name is tony allen

00:00:02,080 --> 00:00:07,680
i'm a software engineer at lyft

00:00:04,400 --> 00:00:09,200
i work in the resilience group on envoy

00:00:07,680 --> 00:00:11,120
related things and i'm going to talk a

00:00:09,200 --> 00:00:13,120
little bit about what we've been working

00:00:11,120 --> 00:00:15,120
on for the past year which is

00:00:13,120 --> 00:00:17,119
the adaptive concurrency control filter

00:00:15,120 --> 00:00:20,560
and envoy

00:00:17,119 --> 00:00:22,000
so the adaptive concurrency filter is an

00:00:20,560 --> 00:00:23,680
http filter

00:00:22,000 --> 00:00:25,359
that was implemented in envoy it

00:00:23,680 --> 00:00:28,840
measures

00:00:25,359 --> 00:00:32,880
request latency baselines

00:00:28,840 --> 00:00:34,880
and samples requests that are coming in

00:00:32,880 --> 00:00:36,800
after that and compares it to the

00:00:34,880 --> 00:00:40,160
baseline so

00:00:36,800 --> 00:00:42,320
if sampled latencies are increasing

00:00:40,160 --> 00:00:44,559
from what the baseline would be then

00:00:42,320 --> 00:00:46,960
we'll allow less requests

00:00:44,559 --> 00:00:49,680
through the filter so this was inspired

00:00:46,960 --> 00:00:53,680
by a 2018 blog post

00:00:49,680 --> 00:00:55,840
from netflix engineering so we took

00:00:53,680 --> 00:00:58,480
those ideas and implemented them inside

00:00:55,840 --> 00:01:01,760
of envoy with a few changes

00:00:58,480 --> 00:01:03,840
that i'll talk about later on

00:01:01,760 --> 00:01:04,960
the way we're going to do things here is

00:01:03,840 --> 00:01:06,320
we're going to talk a little bit about

00:01:04,960 --> 00:01:10,000
simulating traffic

00:01:06,320 --> 00:01:11,280
so that's how i was able to test various

00:01:10,000 --> 00:01:14,720
workloads and

00:01:11,280 --> 00:01:16,960
develop the filter so i'm going to show

00:01:14,720 --> 00:01:19,040
various concepts using simulations and

00:01:16,960 --> 00:01:20,240
i'll describe what the graphs look like

00:01:19,040 --> 00:01:21,759
first off

00:01:20,240 --> 00:01:23,680
and then we'll talk about concurrency

00:01:21,759 --> 00:01:26,080
and circuit breaking

00:01:23,680 --> 00:01:27,360
so why would one want to limit

00:01:26,080 --> 00:01:29,119
concurrency requests

00:01:27,360 --> 00:01:30,720
where latency is coming from that sort

00:01:29,119 --> 00:01:33,759
of stuff

00:01:30,720 --> 00:01:34,799
kind of detour go off into a little bit

00:01:33,759 --> 00:01:37,840
about envoy

00:01:34,799 --> 00:01:40,000
and envoy filters which leads into

00:01:37,840 --> 00:01:41,119
adaptive concurrency control how that

00:01:40,000 --> 00:01:44,320
was implemented

00:01:41,119 --> 00:01:47,280
and then tails from lift infrastructure

00:01:44,320 --> 00:01:48,960
so the simulation framework that i'm

00:01:47,280 --> 00:01:50,159
using is something called buffer bloater

00:01:48,960 --> 00:01:54,079
i wrote it to

00:01:50,159 --> 00:01:57,280
test the adaptive concurrency filter

00:01:54,079 --> 00:01:58,560
while developing it the way it works is

00:01:57,280 --> 00:02:01,920
it spins up a client

00:01:58,560 --> 00:02:05,360
and a server locally on the machine and

00:02:01,920 --> 00:02:07,600
i'd spin up a envoy process the client

00:02:05,360 --> 00:02:09,200
would send requests to the envoy process

00:02:07,600 --> 00:02:12,239
which would route it

00:02:09,200 --> 00:02:13,920
to the server the server would

00:02:12,239 --> 00:02:16,080
take the request sleep for some

00:02:13,920 --> 00:02:19,120
configurable amount of time

00:02:16,080 --> 00:02:22,480
and then send a http 200 reply back

00:02:19,120 --> 00:02:25,920
through the envoy to the client so

00:02:22,480 --> 00:02:29,280
you'll have control over the

00:02:25,920 --> 00:02:31,120
rps coming from the client and how long

00:02:29,280 --> 00:02:33,840
so you can do this in stages

00:02:31,120 --> 00:02:36,160
you can say i want 100 rps and then bump

00:02:33,840 --> 00:02:39,200
up to 200 rps for some amount of time

00:02:36,160 --> 00:02:42,720
the server also will have a configurable

00:02:39,200 --> 00:02:45,200
latency distribution so you can specify

00:02:42,720 --> 00:02:47,200
you know what percentile you want to

00:02:45,200 --> 00:02:48,959
have what latency

00:02:47,200 --> 00:02:51,519
it'll gather snaps and plot them which

00:02:48,959 --> 00:02:54,879
we'll see later on and it allows

00:02:51,519 --> 00:02:59,360
for uh cool simulations like serger

00:02:54,879 --> 00:03:01,040
server degradations uh ramp ups and rps

00:02:59,360 --> 00:03:03,920
and that sort of stuff the adaptive

00:03:01,040 --> 00:03:06,239
currency filter really relies on or to

00:03:03,920 --> 00:03:08,159
understand it you need to look at

00:03:06,239 --> 00:03:09,440
temporal behaviors so this really helps

00:03:08,159 --> 00:03:11,519
out with that

00:03:09,440 --> 00:03:13,360
the source code can be found at the link

00:03:11,519 --> 00:03:17,120
in the bottom left there

00:03:13,360 --> 00:03:19,519
so this is a configuration file

00:03:17,120 --> 00:03:20,319
for this thing the only things to note

00:03:19,519 --> 00:03:22,560
are

00:03:20,319 --> 00:03:24,400
that the client has an rps and a

00:03:22,560 --> 00:03:27,440
duration that you can specify

00:03:24,400 --> 00:03:30,640
and this can be staged so you can

00:03:27,440 --> 00:03:34,080
simulate burst in traffic or ramp down

00:03:30,640 --> 00:03:37,360
that sort of stuff the server itself

00:03:34,080 --> 00:03:40,720
will have a latency profile

00:03:37,360 --> 00:03:40,720
and that also is staged

00:03:41,680 --> 00:03:46,799
the output from the simulations looks a

00:03:44,879 --> 00:03:48,319
little bit like this so at the very top

00:03:46,799 --> 00:03:50,000
you're going to have request latencies

00:03:48,319 --> 00:03:53,760
so each blue point

00:03:50,000 --> 00:03:54,400
represents a request and the y-axis

00:03:53,760 --> 00:03:56,799
would be

00:03:54,400 --> 00:03:58,799
request latency and some unit of time

00:03:56,799 --> 00:03:59,120
the unit of time is really important for

00:03:58,799 --> 00:04:02,239
the

00:03:59,120 --> 00:04:05,280
stuff i'm trying to show the rps

00:04:02,239 --> 00:04:06,640
is the second line there the rps is

00:04:05,280 --> 00:04:08,720
static it doesn't change for the

00:04:06,640 --> 00:04:10,959
duration of that simulation and then

00:04:08,720 --> 00:04:13,439
timeouts below that there were no

00:04:10,959 --> 00:04:15,680
timeouts in this one

00:04:13,439 --> 00:04:16,639
so you can see that there's a p99 that's

00:04:15,680 --> 00:04:18,560
very obvious

00:04:16,639 --> 00:04:20,560
in the request latencies you can see the

00:04:18,560 --> 00:04:23,680
p95 and the bulk of the requests

00:04:20,560 --> 00:04:28,960
after that that profile didn't change

00:04:23,680 --> 00:04:32,000
through the simulation

00:04:28,960 --> 00:04:33,919
and then furthermore there's 503s that

00:04:32,000 --> 00:04:35,440
would show up below the request timeouts

00:04:33,919 --> 00:04:37,120
the number of requests that are in

00:04:35,440 --> 00:04:38,240
flight from the perspective of the

00:04:37,120 --> 00:04:41,120
client

00:04:38,240 --> 00:04:42,560
and then the request success rate so

00:04:41,120 --> 00:04:44,400
this is all the information

00:04:42,560 --> 00:04:46,400
i'm going to try and point out what the

00:04:44,400 --> 00:04:50,240
relevant pieces of info

00:04:46,400 --> 00:04:52,880
are for this as

00:04:50,240 --> 00:04:52,880
we move on

00:04:53,360 --> 00:04:59,600
so in the netflix blog post in 2018

00:04:56,880 --> 00:05:00,000
there was this interesting diagram so on

00:04:59,600 --> 00:05:02,400
the

00:05:00,000 --> 00:05:03,680
y-axis you have rps and on the x-axis

00:05:02,400 --> 00:05:05,840
you have time

00:05:03,680 --> 00:05:07,039
and what it's trying to show is that

00:05:05,840 --> 00:05:09,680
there's some

00:05:07,039 --> 00:05:11,039
capacity that let's say a server would

00:05:09,680 --> 00:05:13,520
have

00:05:11,039 --> 00:05:14,400
and that's you know unknowable for the

00:05:13,520 --> 00:05:17,600
purposes of

00:05:14,400 --> 00:05:21,039
you know any configurations

00:05:17,600 --> 00:05:22,400
so the rps is stable in the beginning

00:05:21,039 --> 00:05:25,680
and it's below what the capacity the

00:05:22,400 --> 00:05:27,840
server is and then about halfway through

00:05:25,680 --> 00:05:29,440
it bumps up beyond what this capacity is

00:05:27,840 --> 00:05:31,919
and you'll notice that this latency in

00:05:29,440 --> 00:05:33,600
the blue line begins to increase

00:05:31,919 --> 00:05:36,800
at that halfway point and as it

00:05:33,600 --> 00:05:38,960
increases it's going to go into

00:05:36,800 --> 00:05:40,479
timeout town and then at some point it's

00:05:38,960 --> 00:05:42,639
just gonna

00:05:40,479 --> 00:05:44,320
have this comic book explosion so i

00:05:42,639 --> 00:05:46,240
wanted to kind of understand

00:05:44,320 --> 00:05:48,720
what's going on there can the simulation

00:05:46,240 --> 00:05:51,520
framework reproduce this

00:05:48,720 --> 00:05:52,000
uh long story short it can so the

00:05:51,520 --> 00:05:55,039
latency

00:05:52,000 --> 00:05:57,600
at the top is steadily increasing from

00:05:55,039 --> 00:05:59,280
that midway point that the rps

00:05:57,600 --> 00:06:00,639
increases beyond what the capacity that

00:05:59,280 --> 00:06:03,039
server would be

00:06:00,639 --> 00:06:04,720
about three quarters the way through

00:06:03,039 --> 00:06:06,479
we're seeing timeouts

00:06:04,720 --> 00:06:07,919
which will cause a success rate to

00:06:06,479 --> 00:06:09,840
plummet to zero

00:06:07,919 --> 00:06:13,199
and that's what we're seeing when

00:06:09,840 --> 00:06:13,199
there's that comic book explosion

00:06:13,680 --> 00:06:20,160
nice so let's talk about

00:06:16,880 --> 00:06:22,010
cues and concurrency uh now that

00:06:20,160 --> 00:06:23,280
the simulation has been explained

00:06:22,010 --> 00:06:27,440
[Music]

00:06:23,280 --> 00:06:30,400
so there's some fixed number of requests

00:06:27,440 --> 00:06:31,919
that a system can handle at any time and

00:06:30,400 --> 00:06:33,840
that's dictated by

00:06:31,919 --> 00:06:36,000
various resources available to whatever

00:06:33,840 --> 00:06:38,400
computer is running on right so cpu

00:06:36,000 --> 00:06:39,520
storage network that sort of stuff

00:06:38,400 --> 00:06:42,560
anything

00:06:39,520 --> 00:06:46,319
beyond that as in like if

00:06:42,560 --> 00:06:48,160
requests come in and need more

00:06:46,319 --> 00:06:49,840
resources that are available to that

00:06:48,160 --> 00:06:50,880
machine a queue is going to start to

00:06:49,840 --> 00:06:53,520
form

00:06:50,880 --> 00:06:55,759
if the incoming rate of request doesn't

00:06:53,520 --> 00:06:55,759
change

00:06:56,479 --> 00:07:00,000
so that's what we're seeing here about

00:06:57,919 --> 00:07:02,000
the halfway point we're seeing

00:07:00,000 --> 00:07:03,680
the outstanding request count just

00:07:02,000 --> 00:07:06,000
steadily increase

00:07:03,680 --> 00:07:06,960
and as it steadily increases we see the

00:07:06,000 --> 00:07:10,240
latencies

00:07:06,960 --> 00:07:13,759
steadily increase until some point

00:07:10,240 --> 00:07:13,759
all the requests just start timing out

00:07:14,319 --> 00:07:18,960
so not all this cueing is bad uh

00:07:17,360 --> 00:07:21,120
temporary increases in

00:07:18,960 --> 00:07:22,800
queuing delays are totally okay this

00:07:21,120 --> 00:07:26,720
happens in real life all the time

00:07:22,800 --> 00:07:29,199
right uh there's unexpected bursts

00:07:26,720 --> 00:07:30,240
we recover from them so what you see

00:07:29,199 --> 00:07:32,880
here

00:07:30,240 --> 00:07:34,960
is a simulation where i bumped up the

00:07:32,880 --> 00:07:36,800
rps to the same amount that we saw on

00:07:34,960 --> 00:07:39,199
the first simulation but then quickly

00:07:36,800 --> 00:07:40,880
dropped it back down to normal levels

00:07:39,199 --> 00:07:42,720
and you'll see that

00:07:40,880 --> 00:07:44,479
at the bottom there the request queue

00:07:42,720 --> 00:07:46,479
begins to form

00:07:44,479 --> 00:07:48,479
but then just as the rps goes back down

00:07:46,479 --> 00:07:50,639
to normal levels

00:07:48,479 --> 00:07:52,400
the queues burn down and then latencies

00:07:50,639 --> 00:07:55,360
return back to normal the success rate

00:07:52,400 --> 00:07:55,360
was unaffected there

00:07:56,479 --> 00:08:01,039
now trouble kind of starts to form when

00:07:59,280 --> 00:08:02,879
latencies get too high

00:08:01,039 --> 00:08:05,680
the latencies increase due to cueing

00:08:02,879 --> 00:08:06,720
delays and if we have a burst as we saw

00:08:05,680 --> 00:08:09,759
earlier that just

00:08:06,720 --> 00:08:12,160
doesn't go away it'll begin to

00:08:09,759 --> 00:08:13,840
negatively affect the callers which lead

00:08:12,160 --> 00:08:15,919
to cascading failures and what we mean

00:08:13,840 --> 00:08:18,319
by this is

00:08:15,919 --> 00:08:19,840
your server is going to be inundated

00:08:18,319 --> 00:08:20,960
with requests a queue is going to start

00:08:19,840 --> 00:08:23,680
to form

00:08:20,960 --> 00:08:26,240
latencies are going to increase all the

00:08:23,680 --> 00:08:27,680
callers to that service

00:08:26,240 --> 00:08:29,199
are also going to start to form queues

00:08:27,680 --> 00:08:30,240
because they're dependent on that

00:08:29,199 --> 00:08:31,440
service

00:08:30,240 --> 00:08:33,360
so that they can service their own

00:08:31,440 --> 00:08:34,880
requests and then all the callers

00:08:33,360 --> 00:08:37,120
depend on that and then next thing you

00:08:34,880 --> 00:08:39,760
know everyone has cues

00:08:37,120 --> 00:08:42,479
all the latencies have increased timeout

00:08:39,760 --> 00:08:45,920
town comic book explosion

00:08:42,479 --> 00:08:49,120
so how do we fix this circuit breaking

00:08:45,920 --> 00:08:50,480
is the answer in envoy today and the way

00:08:49,120 --> 00:08:52,160
those work there are five different

00:08:50,480 --> 00:08:52,880
circuit breakers but the one we really

00:08:52,160 --> 00:08:54,720
care about

00:08:52,880 --> 00:08:56,480
is this max request circuit breaker

00:08:54,720 --> 00:08:57,600
which will limit the number of

00:08:56,480 --> 00:09:00,959
outstanding requests

00:08:57,600 --> 00:09:03,519
allowed to a particular cluster so

00:09:00,959 --> 00:09:06,080
if i'm an envoy i receive a request and

00:09:03,519 --> 00:09:07,920
i need to route it to a cluster

00:09:06,080 --> 00:09:09,440
if that cluster has an upper bound on

00:09:07,920 --> 00:09:12,080
the number of outstanding requests

00:09:09,440 --> 00:09:13,519
and i have that many outstanding

00:09:12,080 --> 00:09:16,080
requests

00:09:13,519 --> 00:09:16,880
to that cluster i'm just going to return

00:09:16,080 --> 00:09:19,360
a 503

00:09:16,880 --> 00:09:22,320
to whoever sent the request that's

00:09:19,360 --> 00:09:24,560
routed there

00:09:22,320 --> 00:09:26,080
so let's revisit that traffic overload

00:09:24,560 --> 00:09:27,440
scenario and see how

00:09:26,080 --> 00:09:30,640
circuit breaking would have affected

00:09:27,440 --> 00:09:30,640
this simulation

00:09:30,720 --> 00:09:34,080
you'll notice that with a

00:09:31,760 --> 00:09:36,720
well-configured circuit breaker

00:09:34,080 --> 00:09:38,160
the latencies are under control so if

00:09:36,720 --> 00:09:39,600
you pay attention to scales there at the

00:09:38,160 --> 00:09:42,480
top if you can see it

00:09:39,600 --> 00:09:43,760
uh the p99 latency doesn't really

00:09:42,480 --> 00:09:46,399
increase half of

00:09:43,760 --> 00:09:48,839
what the request latency in the

00:09:46,399 --> 00:09:51,839
non-circuit breaker simulation

00:09:48,839 --> 00:09:53,519
shows now you can look at the timeout

00:09:51,839 --> 00:09:55,839
frequencies there's no timeouts with a

00:09:53,519 --> 00:09:59,279
well-configured circuit breaker scenario

00:09:55,839 --> 00:10:00,240
there are 503 responses and that's due

00:09:59,279 --> 00:10:02,240
to

00:10:00,240 --> 00:10:04,079
the circuit breaker rejecting requests

00:10:02,240 --> 00:10:05,440
that would have otherwise

00:10:04,079 --> 00:10:07,600
contributed to the queue that would have

00:10:05,440 --> 00:10:09,839
formed

00:10:07,600 --> 00:10:11,519
and you'll see that the active requests

00:10:09,839 --> 00:10:13,519
count for the well configured circuit

00:10:11,519 --> 00:10:15,600
breaker doesn't exceed whatever

00:10:13,519 --> 00:10:17,200
the max request setting was for this

00:10:15,600 --> 00:10:20,000
particular simulation

00:10:17,200 --> 00:10:21,440
so the success rate doesn't doesn't uh

00:10:20,000 --> 00:10:25,200
plummet down to

00:10:21,440 --> 00:10:25,200
zero at about three quarters of the way

00:10:26,320 --> 00:10:32,240
now not all circuit breakers are

00:10:29,360 --> 00:10:34,240
configured well in real life they're

00:10:32,240 --> 00:10:36,560
notoriously hard to configure

00:10:34,240 --> 00:10:38,399
so if we have a poorly configured

00:10:36,560 --> 00:10:40,959
circuit breaker that's still better than

00:10:38,399 --> 00:10:42,079
having no circuit breakers at all you'll

00:10:40,959 --> 00:10:44,160
see that

00:10:42,079 --> 00:10:46,000
in the poorly configured case the

00:10:44,160 --> 00:10:49,279
request latencies are much higher

00:10:46,000 --> 00:10:51,600
we're still getting some timeouts

00:10:49,279 --> 00:10:53,120
or rather we're getting timeouts that we

00:10:51,600 --> 00:10:54,240
weren't seeing in the well-configured

00:10:53,120 --> 00:10:56,000
case

00:10:54,240 --> 00:10:57,600
and the queue size is going to be much

00:10:56,000 --> 00:11:00,240
larger right because we

00:10:57,600 --> 00:11:01,519
overshot what the circuit breaker

00:11:00,240 --> 00:11:03,360
setting should have been

00:11:01,519 --> 00:11:04,959
and the success rate suffers because of

00:11:03,360 --> 00:11:06,160
the timeouts in conjunction with the

00:11:04,959 --> 00:11:08,480
503s

00:11:06,160 --> 00:11:09,760
but it's still better than what we saw

00:11:08,480 --> 00:11:12,160
originally when there was no circuit

00:11:09,760 --> 00:11:12,160
breaking

00:11:12,640 --> 00:11:16,560
so how do we configure these things you

00:11:15,519 --> 00:11:18,240
need to understand the service

00:11:16,560 --> 00:11:21,200
limitations and this is done by

00:11:18,240 --> 00:11:23,360
performance testing profiling of a

00:11:21,200 --> 00:11:24,959
particular application or service

00:11:23,360 --> 00:11:26,399
you can ramp up the traffic you can pay

00:11:24,959 --> 00:11:28,720
attention to the latencies

00:11:26,399 --> 00:11:30,240
see when it tips over see what the q was

00:11:28,720 --> 00:11:33,120
at that particular time

00:11:30,240 --> 00:11:35,200
and then set the limit to that you'll

00:11:33,120 --> 00:11:38,320
also want to allow headroom for births

00:11:35,200 --> 00:11:39,440
as we saw earlier we have bursts they're

00:11:38,320 --> 00:11:42,399
okay if they go away

00:11:39,440 --> 00:11:43,519
quickly and we want to keep the currency

00:11:42,399 --> 00:11:46,800
values up to date

00:11:43,519 --> 00:11:48,000
now this is pretty hard uh system

00:11:46,800 --> 00:11:49,920
topologies change

00:11:48,000 --> 00:11:51,519
people push code that affects the

00:11:49,920 --> 00:11:53,600
performance characteristics of an

00:11:51,519 --> 00:11:57,440
application

00:11:53,600 --> 00:11:59,680
this is very hard so service owners

00:11:57,440 --> 00:12:01,040
really shouldn't have to profile a

00:11:59,680 --> 00:12:03,440
concurrency their application

00:12:01,040 --> 00:12:04,639
ideally they would just focus on the

00:12:03,440 --> 00:12:06,560
application

00:12:04,639 --> 00:12:08,079
and not have to think about

00:12:06,560 --> 00:12:09,120
network-related things the whole point

00:12:08,079 --> 00:12:12,800
in envoy is to

00:12:09,120 --> 00:12:14,880
abstract the network so

00:12:12,800 --> 00:12:17,360
it's hard also to account for bursts

00:12:14,880 --> 00:12:19,440
it's unclear

00:12:17,360 --> 00:12:20,959
what is acceptable like what is the time

00:12:19,440 --> 00:12:23,839
length for a burst

00:12:20,959 --> 00:12:24,560
uh how big can these cues get how do we

00:12:23,839 --> 00:12:26,160
measure

00:12:24,560 --> 00:12:27,600
when it burns down the queue if things

00:12:26,160 --> 00:12:29,519
return back to normal

00:12:27,600 --> 00:12:31,760
and manually keeping the concurrency

00:12:29,519 --> 00:12:34,000
value or manually keeping the

00:12:31,760 --> 00:12:36,480
circuit breaker values up to date is

00:12:34,000 --> 00:12:36,480
very hard

00:12:37,040 --> 00:12:42,959
so in an ideal world we wouldn't have to

00:12:40,639 --> 00:12:45,360
do this manual configuration

00:12:42,959 --> 00:12:47,360
uh the system would figure out its own

00:12:45,360 --> 00:12:49,279
limits there's no need to

00:12:47,360 --> 00:12:50,720
know about system topology or hardware

00:12:49,279 --> 00:12:51,200
and then a lot of cases we can't know

00:12:50,720 --> 00:12:53,120
this

00:12:51,200 --> 00:12:54,800
if you're in a cloud vendor who knows

00:12:53,120 --> 00:12:57,440
what these things are running on

00:12:54,800 --> 00:12:59,519
and what abstractions they put in place

00:12:57,440 --> 00:13:03,440
we would also want it to

00:12:59,519 --> 00:13:04,880
adapt to changes so if i push a commit

00:13:03,440 --> 00:13:06,480
that

00:13:04,880 --> 00:13:08,480
makes the performance characteristics of

00:13:06,480 --> 00:13:11,519
an application suffer or

00:13:08,480 --> 00:13:13,360
uh dramatically benefits the performance

00:13:11,519 --> 00:13:15,279
characteristics such that i can handle

00:13:13,360 --> 00:13:16,800
more requests simultaneously we want

00:13:15,279 --> 00:13:18,320
this thing to adapt to it

00:13:16,800 --> 00:13:20,399
and we want it to be cheap to compute

00:13:18,320 --> 00:13:21,440
because the purpose is to run an

00:13:20,399 --> 00:13:24,959
application

00:13:21,440 --> 00:13:24,959
not just to run envoy

00:13:25,680 --> 00:13:29,600
we can do this in an envoy filter but

00:13:28,800 --> 00:13:31,200
first

00:13:29,600 --> 00:13:32,880
we must talk about envoy filters and how

00:13:31,200 --> 00:13:36,560
they work so

00:13:32,880 --> 00:13:39,279
an analogy i like to use here is uh

00:13:36,560 --> 00:13:40,320
pasta sauce is the pasta noodle what

00:13:39,279 --> 00:13:42,800
envoy filters

00:13:40,320 --> 00:13:45,440
are to envoy so the pasta noodle is just

00:13:42,800 --> 00:13:47,279
a vessel for the pasta sauce

00:13:45,440 --> 00:13:49,040
an envoy is just a vessel for envoy

00:13:47,279 --> 00:13:52,160
filters

00:13:49,040 --> 00:13:53,040
so here's a basic envoy config okay we

00:13:52,160 --> 00:13:56,079
don't have

00:13:53,040 --> 00:13:57,519
any filters configured here and i'm just

00:13:56,079 --> 00:13:59,920
going to spin up an envoy

00:13:57,519 --> 00:14:02,639
with this configuration and run our

00:13:59,920 --> 00:14:05,040
quest through it and see what it does

00:14:02,639 --> 00:14:06,639
and what ends up happening is that envoy

00:14:05,040 --> 00:14:08,079
closes the connection because it has no

00:14:06,639 --> 00:14:11,120
filters

00:14:08,079 --> 00:14:11,120
so it has nothing to do

00:14:14,160 --> 00:14:19,040
so let's zoom in on an envoy process and

00:14:16,160 --> 00:14:21,920
see what's going on in there

00:14:19,040 --> 00:14:22,800
so you'll see the greatest hits of envoy

00:14:21,920 --> 00:14:25,760
they're the listener

00:14:22,800 --> 00:14:26,399
the listener filters network filters the

00:14:25,760 --> 00:14:29,440
listener

00:14:26,399 --> 00:14:31,760
is a construct inside of envoy that

00:14:29,440 --> 00:14:33,040
you just tell it i'd like to listen on

00:14:31,760 --> 00:14:34,480
some port

00:14:33,040 --> 00:14:36,240
and all the requests that come in that

00:14:34,480 --> 00:14:36,959
are destined to that port hit that

00:14:36,240 --> 00:14:39,120
listener

00:14:36,959 --> 00:14:40,320
and then they kick off the chain of

00:14:39,120 --> 00:14:43,360
events that

00:14:40,320 --> 00:14:44,720
send things through the filters the

00:14:43,360 --> 00:14:48,079
listener filters

00:14:44,720 --> 00:14:49,440
are something that allows code to be

00:14:48,079 --> 00:14:52,560
executed upon

00:14:49,440 --> 00:14:56,320
accepting a connection

00:14:52,560 --> 00:14:59,199
so upon the first except

00:14:56,320 --> 00:15:01,120
on that socket we can run some code in

00:14:59,199 --> 00:15:03,680
these listener filters so something that

00:15:01,120 --> 00:15:05,920
would change connection metadata

00:15:03,680 --> 00:15:07,120
if i wanted to change the remote ip or

00:15:05,920 --> 00:15:10,399
determine if something

00:15:07,120 --> 00:15:11,519
is a tls connection or not

00:15:10,399 --> 00:15:14,959
this is where that kind of stuff would

00:15:11,519 --> 00:15:17,199
happen you can also

00:15:14,959 --> 00:15:19,360
rate limit connection so the local rate

00:15:17,199 --> 00:15:22,240
limit filter in envoy is implemented as

00:15:19,360 --> 00:15:22,240
a listener filter

00:15:23,839 --> 00:15:28,880
now the network filters are envoy

00:15:27,120 --> 00:15:30,240
filters that operate on

00:15:28,880 --> 00:15:32,000
bytes that are coming in over a

00:15:30,240 --> 00:15:34,800
connection so

00:15:32,000 --> 00:15:36,399
there's read and write filters and the

00:15:34,800 --> 00:15:40,480
read filters

00:15:36,399 --> 00:15:43,920
execute code when data is received

00:15:40,480 --> 00:15:48,639
over a connection and the right filter

00:15:43,920 --> 00:15:48,639
just does something when data is written

00:15:50,240 --> 00:15:54,000
now a really important network filter is

00:15:52,399 --> 00:15:57,759
the http connection manager

00:15:54,000 --> 00:16:00,240
filter and this is what has

00:15:57,759 --> 00:16:01,360
the http filters that everyone knows and

00:16:00,240 --> 00:16:03,440
loves

00:16:01,360 --> 00:16:05,120
so it'll parse raw bytes over the

00:16:03,440 --> 00:16:07,440
connection like any

00:16:05,120 --> 00:16:08,720
network filter but those bytes are

00:16:07,440 --> 00:16:10,959
converted into

00:16:08,720 --> 00:16:12,959
http message objects and this is going

00:16:10,959 --> 00:16:14,880
to allow envoy to

00:16:12,959 --> 00:16:16,959
operate at a higher level of abstraction

00:16:14,880 --> 00:16:19,040
so we can do things on a per request

00:16:16,959 --> 00:16:23,120
basis instead of a per

00:16:19,040 --> 00:16:26,160
series of bytes basis

00:16:23,120 --> 00:16:28,639
inside the http connection manager we

00:16:26,160 --> 00:16:30,959
have an http filter chain

00:16:28,639 --> 00:16:32,720
and that's going to be agnostic to

00:16:30,959 --> 00:16:34,000
whatever the underlying request protocol

00:16:32,720 --> 00:16:38,240
is whether it be

00:16:34,000 --> 00:16:40,320
h1h2 grpc quick whatever

00:16:38,240 --> 00:16:41,440
and these http filters are going to have

00:16:40,320 --> 00:16:44,480
a decode step

00:16:41,440 --> 00:16:46,560
which is what which is uh what's going

00:16:44,480 --> 00:16:49,680
to operate on inbound requests so

00:16:46,560 --> 00:16:51,040
imagine i'm an envoy i receive an http

00:16:49,680 --> 00:16:54,240
request

00:16:51,040 --> 00:16:55,680
then i'm going to decode it and then

00:16:54,240 --> 00:16:56,959
send it along it'll go somewhere i'm

00:16:55,680 --> 00:16:58,320
going to get a reply back and then i'm

00:16:56,959 --> 00:16:58,639
going to perform the encode step and

00:16:58,320 --> 00:17:00,639
then

00:16:58,639 --> 00:17:02,399
return it where it needs to go not all

00:17:00,639 --> 00:17:04,559
http filters need to do

00:17:02,399 --> 00:17:05,919
things on the d code or the end code

00:17:04,559 --> 00:17:08,160
right they can do one or the other or

00:17:05,919 --> 00:17:08,160
both

00:17:09,360 --> 00:17:13,199
so now that we're all experts in envoy

00:17:11,839 --> 00:17:14,400
filters let's talk a little bit about

00:17:13,199 --> 00:17:18,160
adaptive concurrency

00:17:14,400 --> 00:17:18,480
i'm going to keep this as simple as i

00:17:18,160 --> 00:17:20,880
can

00:17:18,480 --> 00:17:22,319
it's it's a complicated concept but i'm

00:17:20,880 --> 00:17:24,240
gonna talk a little bit of a high level

00:17:22,319 --> 00:17:25,039
and gloss over some of the nitty gritty

00:17:24,240 --> 00:17:27,120
details

00:17:25,039 --> 00:17:29,039
perhaps at a future time i can talk

00:17:27,120 --> 00:17:33,200
about envoy for

00:17:29,039 --> 00:17:35,280
evil wizards or something but not today

00:17:33,200 --> 00:17:37,120
so the adaptive currency filter is

00:17:35,280 --> 00:17:40,240
implemented as an http filter

00:17:37,120 --> 00:17:40,960
since we're operating on a per request

00:17:40,240 --> 00:17:42,400
basis

00:17:40,960 --> 00:17:43,840
right we don't really care about bytes

00:17:42,400 --> 00:17:44,960
over the connection we care about what

00:17:43,840 --> 00:17:48,080
those bytes

00:17:44,960 --> 00:17:51,120
represent so it protects

00:17:48,080 --> 00:17:53,280
a upstream server by

00:17:51,120 --> 00:17:54,640
enforcing an upper bound on outstanding

00:17:53,280 --> 00:17:55,840
requests similar to what a circuit

00:17:54,640 --> 00:17:58,320
breaker is going to do

00:17:55,840 --> 00:18:00,880
so we can turn away excessive traffic by

00:17:58,320 --> 00:18:02,400
returning 503s and not

00:18:00,880 --> 00:18:04,400
forwarding it along to the application

00:18:02,400 --> 00:18:06,080
we're trying to protect and this

00:18:04,400 --> 00:18:08,080
filter can sit in front of the circuit

00:18:06,080 --> 00:18:08,960
breaker and be used in conjunction with

00:18:08,080 --> 00:18:11,039
it

00:18:08,960 --> 00:18:12,400
so one reason you'd want to do this is

00:18:11,039 --> 00:18:14,480
if you want to toggle the adaptive

00:18:12,400 --> 00:18:15,919
concurrency filter on or off

00:18:14,480 --> 00:18:17,840
then you still have circuit breakers

00:18:15,919 --> 00:18:20,559
protecting you in that case or you can

00:18:17,840 --> 00:18:20,559
compare the two

00:18:22,240 --> 00:18:26,160
so one important concept with the

00:18:24,080 --> 00:18:27,840
adaptive concurrency filter is this

00:18:26,160 --> 00:18:31,840
gradient so this was

00:18:27,840 --> 00:18:34,559
discussed in that 2018 netflix blog post

00:18:31,840 --> 00:18:36,559
it's inspired from latency based tcp

00:18:34,559 --> 00:18:38,480
congestion control algorithms

00:18:36,559 --> 00:18:39,919
so you know everyone's first

00:18:38,480 --> 00:18:43,120
introduction to tcp

00:18:39,919 --> 00:18:44,080
is that we'll you know adjust our uh our

00:18:43,120 --> 00:18:45,600
tcp window

00:18:44,080 --> 00:18:47,520
if we drop packets or something like

00:18:45,600 --> 00:18:49,760
that but there's a drop base that means

00:18:47,520 --> 00:18:51,919
some nick buffer somewhere has filled up

00:18:49,760 --> 00:18:54,880
full of packets right and then

00:18:51,919 --> 00:18:55,440
a request was dropped so we're adjusting

00:18:54,880 --> 00:18:57,280
it then

00:18:55,440 --> 00:19:00,400
the latency based algorithms are going

00:18:57,280 --> 00:19:03,280
to prevent that from happening or try to

00:19:00,400 --> 00:19:03,919
by observing whatever latency was

00:19:03,280 --> 00:19:08,000
measured

00:19:03,919 --> 00:19:12,080
during the handshake and adjusting

00:19:08,000 --> 00:19:14,640
the tcp window based on

00:19:12,080 --> 00:19:15,840
how long it takes requests to go over

00:19:14,640 --> 00:19:17,120
the network

00:19:15,840 --> 00:19:19,360
so we're kind of doing the same thing

00:19:17,120 --> 00:19:21,200
here we're going to measure some kind of

00:19:19,360 --> 00:19:22,720
ideal latency and use that as our

00:19:21,200 --> 00:19:24,400
baseline and that's what we'll call the

00:19:22,720 --> 00:19:26,640
rtt ideal or the

00:19:24,400 --> 00:19:28,559
ideal round trip time and then we're

00:19:26,640 --> 00:19:31,760
also going to measure

00:19:28,559 --> 00:19:33,200
just sampled requests right so once we

00:19:31,760 --> 00:19:35,280
have this ideal

00:19:33,200 --> 00:19:36,960
as we move forward sampling requests we

00:19:35,280 --> 00:19:38,960
can slice up

00:19:36,960 --> 00:19:41,120
various time windows and then we can

00:19:38,960 --> 00:19:43,440
summarize them and call this an rtt

00:19:41,120 --> 00:19:43,440
sample

00:19:44,240 --> 00:19:50,160
now this gradient value which is

00:19:48,160 --> 00:19:52,000
related to these round trip times is

00:19:50,160 --> 00:19:54,400
nice because it informs what

00:19:52,000 --> 00:19:56,480
direction we want to take our

00:19:54,400 --> 00:19:58,720
concurrency limit in this filter

00:19:56,480 --> 00:20:00,559
so if the ideal round trip time is less

00:19:58,720 --> 00:20:01,919
than the sampled round trip time we know

00:20:00,559 --> 00:20:04,240
a q is formed

00:20:01,919 --> 00:20:06,400
right the sampled latencies are

00:20:04,240 --> 00:20:09,520
increasing from what is ideal

00:20:06,400 --> 00:20:12,960
so we want to lower the concurrency

00:20:09,520 --> 00:20:14,240
limit if the ideal is roughly equal to

00:20:12,960 --> 00:20:16,640
the sampled or

00:20:14,240 --> 00:20:17,440
the ideal is greater than the sampled

00:20:16,640 --> 00:20:20,159
latencies

00:20:17,440 --> 00:20:21,200
right we're doing pretty good so we can

00:20:20,159 --> 00:20:23,760
increase

00:20:21,200 --> 00:20:26,080
the concurrency limit and see what

00:20:23,760 --> 00:20:26,080
happens

00:20:26,720 --> 00:20:31,200
so we can represent the new limit like

00:20:29,840 --> 00:20:32,880
this we'll have the current limit you

00:20:31,200 --> 00:20:34,400
just multiply it by the gradient and you

00:20:32,880 --> 00:20:36,480
get the behavior i mentioned a moment

00:20:34,400 --> 00:20:38,640
ago

00:20:36,480 --> 00:20:39,679
uh you also need to think what about

00:20:38,640 --> 00:20:42,880
headroom for bursts

00:20:39,679 --> 00:20:44,799
and also what if the rtt ideal is

00:20:42,880 --> 00:20:46,480
roughly equal to the rtt sampled your

00:20:44,799 --> 00:20:47,760
gradient's going to be about one

00:20:46,480 --> 00:20:49,120
we're not really going to be changing

00:20:47,760 --> 00:20:51,200
the concurrency limit so we need a

00:20:49,120 --> 00:20:54,240
mechanism to

00:20:51,200 --> 00:20:56,799
push the concurrency limit wider

00:20:54,240 --> 00:20:57,760
and we can do that by just adding some

00:20:56,799 --> 00:20:59,520
headroom value

00:20:57,760 --> 00:21:02,640
right which i think by default is going

00:20:59,520 --> 00:21:04,080
to be the square root of what the

00:21:02,640 --> 00:21:06,960
concurrency limit is or what the new

00:21:04,080 --> 00:21:06,960
concurrency limit is

00:21:08,720 --> 00:21:12,240
so this behavior of having a currency

00:21:11,679 --> 00:21:15,200
value

00:21:12,240 --> 00:21:18,000
and then probing into or basically

00:21:15,200 --> 00:21:20,080
widening the concurrency limit

00:21:18,000 --> 00:21:22,400
and allowing more requests through until

00:21:20,080 --> 00:21:24,640
the latency begins to deviate

00:21:22,400 --> 00:21:25,520
in which case we close it as shown in

00:21:24,640 --> 00:21:28,159
this

00:21:25,520 --> 00:21:30,960
netflix blog post figure and a

00:21:28,159 --> 00:21:33,280
simulation i performed

00:21:30,960 --> 00:21:34,720
in python i just scripted this up so

00:21:33,280 --> 00:21:37,840
you'll see an actual limit

00:21:34,720 --> 00:21:40,240
in blue on the left side and then the

00:21:37,840 --> 00:21:43,520
same thing in orange on the right side

00:21:40,240 --> 00:21:46,960
uh the way i simulated this was if

00:21:43,520 --> 00:21:48,720
uh the concurrency limit was

00:21:46,960 --> 00:21:50,080
larger than what the actual limits

00:21:48,720 --> 00:21:53,200
should be right

00:21:50,080 --> 00:21:54,880
then inject latency into this and then

00:21:53,200 --> 00:21:57,360
see what happens so what you end up

00:21:54,880 --> 00:21:59,679
having is the scenario where we

00:21:57,360 --> 00:22:02,720
widen the concurrency limit wait for

00:21:59,679 --> 00:22:05,840
some kind of latency deviation to occur

00:22:02,720 --> 00:22:07,360
close the limit and then wait for things

00:22:05,840 --> 00:22:09,039
to return back to normal so you'll see

00:22:07,360 --> 00:22:11,280
this bobbing motion

00:22:09,039 --> 00:22:14,480
and it tends to hover around what the

00:22:11,280 --> 00:22:14,480
actual limit should be

00:22:14,720 --> 00:22:19,679
this also adjusts itself uh to service

00:22:18,080 --> 00:22:21,679
degradations so

00:22:19,679 --> 00:22:24,240
you can imagine a scenario where the

00:22:21,679 --> 00:22:27,360
ideal concurrency

00:22:24,240 --> 00:22:28,080
will be lower or higher in the case of a

00:22:27,360 --> 00:22:29,600
patch that

00:22:28,080 --> 00:22:31,679
changes the performance characteristics

00:22:29,600 --> 00:22:34,400
for the better this

00:22:31,679 --> 00:22:36,000
methodology still ends up finding what

00:22:34,400 --> 00:22:38,960
the ideal concurrency is

00:22:36,000 --> 00:22:38,960
in these simulations

00:22:39,360 --> 00:22:43,120
it's a little more complicated as i said

00:22:41,039 --> 00:22:45,840
earlier right there's this buffer value

00:22:43,120 --> 00:22:46,240
and there's all kinds of stuff in there

00:22:45,840 --> 00:22:49,280
but

00:22:46,240 --> 00:22:50,960
i'm not going to talk about that maybe

00:22:49,280 --> 00:22:52,840
folks have questions after i can answer

00:22:50,960 --> 00:22:55,760
them

00:22:52,840 --> 00:22:57,840
so let's run through

00:22:55,760 --> 00:22:59,600
a life of a packet or a life of a

00:22:57,840 --> 00:23:00,640
request for the adaptive concurrency

00:22:59,600 --> 00:23:03,520
filter

00:23:00,640 --> 00:23:04,799
you remember our fellow from earlier

00:23:03,520 --> 00:23:06,720
very smart

00:23:04,799 --> 00:23:09,200
he has an adapt currency filter a

00:23:06,720 --> 00:23:12,480
circuit breaker which we'll ignore

00:23:09,200 --> 00:23:15,039
and we'll call this the lime surface so

00:23:12,480 --> 00:23:17,200
this lime service can handle one request

00:23:15,039 --> 00:23:18,720
at a time

00:23:17,200 --> 00:23:20,799
you can see the concurrency limits one

00:23:18,720 --> 00:23:23,760
we have no outstanding requests

00:23:20,799 --> 00:23:24,320
so when a request is coming in through

00:23:23,760 --> 00:23:27,039
the filter

00:23:24,320 --> 00:23:28,400
it's going to hit the decode step in the

00:23:27,039 --> 00:23:30,000
filter

00:23:28,400 --> 00:23:32,000
and in there we're going to mark a time

00:23:30,000 --> 00:23:33,120
stamp we're going to say what time is it

00:23:32,000 --> 00:23:35,280
right now

00:23:33,120 --> 00:23:37,120
and then we're going to check against

00:23:35,280 --> 00:23:38,720
how many outstanding requests we have

00:23:37,120 --> 00:23:40,559
right and if the outstanding requests

00:23:38,720 --> 00:23:41,840
are less than the concurrency limit

00:23:40,559 --> 00:23:44,320
we can go ahead and just forward that

00:23:41,840 --> 00:23:44,720
request along so while the lime service

00:23:44,320 --> 00:23:47,440
is

00:23:44,720 --> 00:23:49,679
inspecting this line or whatever it does

00:23:47,440 --> 00:23:51,919
another request is going to come in

00:23:49,679 --> 00:23:54,159
now our concurrency limits one our

00:23:51,919 --> 00:23:56,080
outstanding requests are won

00:23:54,159 --> 00:24:00,000
so we can't let this through we've

00:23:56,080 --> 00:24:00,000
already hit our concurrency limit

00:24:00,080 --> 00:24:05,679
so we just reject it with a 503

00:24:04,159 --> 00:24:07,760
now the line service is done doing

00:24:05,679 --> 00:24:10,559
whatever it does and then

00:24:07,760 --> 00:24:11,760
it's going to send a reply back and

00:24:10,559 --> 00:24:13,279
that's going to go through the encode

00:24:11,760 --> 00:24:16,000
step

00:24:13,279 --> 00:24:17,200
and we can derive the request latency at

00:24:16,000 --> 00:24:19,520
this point

00:24:17,200 --> 00:24:21,039
so we know when the request went through

00:24:19,520 --> 00:24:22,400
the filter and was forwarded along to

00:24:21,039 --> 00:24:24,240
the line service and then we know when

00:24:22,400 --> 00:24:25,600
the reply is coming back so we can just

00:24:24,240 --> 00:24:28,799
subtract the two

00:24:25,600 --> 00:24:33,840
and then get what the request latency is

00:24:28,799 --> 00:24:37,360
sample the value

00:24:33,840 --> 00:24:39,120
now our rtt ideal

00:24:37,360 --> 00:24:40,799
uh we're going to want to periodically

00:24:39,120 --> 00:24:43,520
recalculate this

00:24:40,799 --> 00:24:45,120
and the way that this works is we're

00:24:43,520 --> 00:24:45,520
going to fix the concurrency limit to

00:24:45,120 --> 00:24:48,640
some

00:24:45,520 --> 00:24:49,440
low value that we can either specify

00:24:48,640 --> 00:24:51,360
because we know

00:24:49,440 --> 00:24:52,720
what our application should be able to

00:24:51,360 --> 00:24:55,120
handle at

00:24:52,720 --> 00:24:55,840
uh in the worst case like there's no

00:24:55,120 --> 00:24:57,760
reason

00:24:55,840 --> 00:25:00,480
that our application cannot handle

00:24:57,760 --> 00:25:02,400
whatever this low value is

00:25:00,480 --> 00:25:04,960
and then we'll aggregate all the samples

00:25:02,400 --> 00:25:06,480
and summarize it

00:25:04,960 --> 00:25:09,440
or you can just leave it at the default

00:25:06,480 --> 00:25:09,440
which i believe is three

00:25:10,400 --> 00:25:13,520
and then it's going to periodically

00:25:11,760 --> 00:25:16,240
recalculate the concurrency limit at

00:25:13,520 --> 00:25:18,080
another interval

00:25:16,240 --> 00:25:21,279
and that's also sampled or that's also

00:25:18,080 --> 00:25:24,960
summarized via percentiles

00:25:21,279 --> 00:25:26,720
so here's a basic config we'll have our

00:25:24,960 --> 00:25:27,679
concurrency limit update interval which

00:25:26,720 --> 00:25:30,080
in this case would be

00:25:27,679 --> 00:25:34,400
500 milliseconds and then our min rtt

00:25:30,080 --> 00:25:34,400
recalculation which is that rtt ideal

00:25:34,720 --> 00:25:42,240
and that's at about 300 seconds there

00:25:39,520 --> 00:25:44,159
so given these two things we've only

00:25:42,240 --> 00:25:44,559
specified two pieces of data how often

00:25:44,159 --> 00:25:46,880
am i

00:25:44,559 --> 00:25:48,960
updating my concurrency limit how often

00:25:46,880 --> 00:25:50,320
am i recalculating the ideal around trip

00:25:48,960 --> 00:25:52,000
time just in case

00:25:50,320 --> 00:25:55,919
service characteristic change or i have

00:25:52,000 --> 00:25:57,679
a noisy neighbor or something like that

00:25:55,919 --> 00:25:59,600
so here's the adaptive currency filter

00:25:57,679 --> 00:26:01,200
in action compared to a well configured

00:25:59,600 --> 00:26:03,919
circuit breaker

00:26:01,200 --> 00:26:04,240
you'll see that in the middle there in

00:26:03,919 --> 00:26:07,120
our

00:26:04,240 --> 00:26:09,440
scenario the rps is going to bump up and

00:26:07,120 --> 00:26:12,159
then a q is going to begin to form

00:26:09,440 --> 00:26:14,640
and then the filter is going to react to

00:26:12,159 --> 00:26:16,960
this increased latency due that q

00:26:14,640 --> 00:26:18,000
lower the concurrency limit and you can

00:26:16,960 --> 00:26:20,720
see there that

00:26:18,000 --> 00:26:22,080
there's increased 503s there's a higher

00:26:20,720 --> 00:26:23,679
rate than there would be for the rest of

00:26:22,080 --> 00:26:26,400
the simulation

00:26:23,679 --> 00:26:28,480
the cue size burns down because we're

00:26:26,400 --> 00:26:30,320
not letting as many requests through

00:26:28,480 --> 00:26:32,159
and then latencies return back to normal

00:26:30,320 --> 00:26:34,880
and then we're periodically just

00:26:32,159 --> 00:26:36,400
uh returning 503s in scenarios where the

00:26:34,880 --> 00:26:39,520
latency gets

00:26:36,400 --> 00:26:41,200
higher than we would like now compare

00:26:39,520 --> 00:26:41,919
this with the well-configured circuit

00:26:41,200 --> 00:26:44,080
breaker

00:26:41,919 --> 00:26:46,640
uh the the latencies are tighter in the

00:26:44,080 --> 00:26:50,559
adaptive concurrent currency filter

00:26:46,640 --> 00:26:53,200
the success rate's mostly the same

00:26:50,559 --> 00:26:54,000
and the cue sizes are roughly equivalent

00:26:53,200 --> 00:26:55,279
as well

00:26:54,000 --> 00:26:57,279
but what's interesting is that the

00:26:55,279 --> 00:26:59,679
adaptive currency filter only needed

00:26:57,279 --> 00:27:00,880
two pieces of information it didn't need

00:26:59,679 --> 00:27:03,679
to know

00:27:00,880 --> 00:27:04,880
how many requests the service can handle

00:27:03,679 --> 00:27:06,960
we didn't have to do any kind of

00:27:04,880 --> 00:27:09,760
measurements of that kind

00:27:06,960 --> 00:27:12,159
we just had to know how often should it

00:27:09,760 --> 00:27:13,760
calculate what the ideal latency is

00:27:12,159 --> 00:27:16,559
and how often should it update the

00:27:13,760 --> 00:27:19,360
concurrency limit

00:27:16,559 --> 00:27:19,360
i think that's awesome

00:27:19,679 --> 00:27:23,120
forget about circuit breakers i'm just

00:27:21,200 --> 00:27:24,159
kidding we should probably use both if

00:27:23,120 --> 00:27:26,320
you want to toggle the haptic

00:27:24,159 --> 00:27:28,159
concurrency on and off

00:27:26,320 --> 00:27:29,840
uh the configuration for adaptive

00:27:28,159 --> 00:27:30,960
currency can get more complicated if you

00:27:29,840 --> 00:27:32,399
want it to

00:27:30,960 --> 00:27:34,000
right there's all kinds of values like a

00:27:32,399 --> 00:27:35,120
jitter a buffer value

00:27:34,000 --> 00:27:37,919
all that kind of stuff that you can

00:27:35,120 --> 00:27:39,600
specify but one thing i want to zoom in

00:27:37,919 --> 00:27:41,760
on is this min concurrency

00:27:39,600 --> 00:27:45,039
setting and that's just going to specify

00:27:41,760 --> 00:27:47,360
the minimum allowed concurrency limit

00:27:45,039 --> 00:27:48,240
okay so when you're recalculating the

00:27:47,360 --> 00:27:51,279
min rtt

00:27:48,240 --> 00:27:53,679
or the ideal round trip time

00:27:51,279 --> 00:27:55,200
the filter will pin the concurrency

00:27:53,679 --> 00:27:58,480
limit to this value

00:27:55,200 --> 00:27:58,480
and then make its measurements

00:27:59,200 --> 00:28:03,279
if that value is too low what you can

00:28:01,440 --> 00:28:04,000
end up with the scenario you can end up

00:28:03,279 --> 00:28:06,480
with

00:28:04,000 --> 00:28:07,600
is that the success rate is going to

00:28:06,480 --> 00:28:09,919
drop because we're

00:28:07,600 --> 00:28:11,360
rejecting lots of requests during the

00:28:09,919 --> 00:28:14,080
measurement window

00:28:11,360 --> 00:28:15,200
so you'll see this periodic drop there

00:28:14,080 --> 00:28:17,520
now this can be

00:28:15,200 --> 00:28:19,360
mitigated with retries okay so if i have

00:28:17,520 --> 00:28:22,000
a fleet of servers

00:28:19,360 --> 00:28:23,039
that are all going through this min rtt

00:28:22,000 --> 00:28:24,799
calculation

00:28:23,039 --> 00:28:27,200
i'll send a request to one it's going to

00:28:24,799 --> 00:28:28,799
return a 503 and then you can just retry

00:28:27,200 --> 00:28:31,360
somewhere else and hopefully they're not

00:28:28,799 --> 00:28:34,080
in a measurement window

00:28:31,360 --> 00:28:35,440
we can help that along by introducing

00:28:34,080 --> 00:28:37,200
this jitter value

00:28:35,440 --> 00:28:38,720
which is going to randomly delay the

00:28:37,200 --> 00:28:40,399
calculations to prevent

00:28:38,720 --> 00:28:43,120
an entire fleet from sinking you can

00:28:40,399 --> 00:28:45,360
imagine some scenario where you

00:28:43,120 --> 00:28:47,679
scale up and you have lots of servers

00:28:45,360 --> 00:28:48,720
that came up at the exact same time and

00:28:47,679 --> 00:28:50,960
started their

00:28:48,720 --> 00:28:52,399
min rtt measurements at the exact same

00:28:50,960 --> 00:28:53,919
time

00:28:52,399 --> 00:28:56,240
so what this jitter value is going to do

00:28:53,919 --> 00:28:57,600
is introduce a random timer

00:28:56,240 --> 00:28:59,919
and then make sure that things don't

00:28:57,600 --> 00:29:03,679
line up so you can see here over

00:28:59,919 --> 00:29:06,480
multiple simulations the

00:29:03,679 --> 00:29:08,320
bursts and uh 503s during the min rtt

00:29:06,480 --> 00:29:10,399
measurement window they don't line up

00:29:08,320 --> 00:29:12,320
right so if you hit one and you're

00:29:10,399 --> 00:29:14,080
rejected and you retry on another

00:29:12,320 --> 00:29:17,919
the chances are very high that you're

00:29:14,080 --> 00:29:19,760
not going to hit a mid-rtt window

00:29:17,919 --> 00:29:21,600
so i'll talk about our experiences at

00:29:19,760 --> 00:29:25,520
lift and

00:29:21,600 --> 00:29:29,600
default settings so we track the p95

00:29:25,520 --> 00:29:32,399
latencies and we find that that's

00:29:29,600 --> 00:29:33,039
uh that's what we want but that's for

00:29:32,399 --> 00:29:36,640
the

00:29:33,039 --> 00:29:38,640
rtt ideal and the sampled latencies um

00:29:36,640 --> 00:29:40,080
and i didn't put this here but we by

00:29:38,640 --> 00:29:42,320
default uh

00:29:40,080 --> 00:29:45,279
take a measurement of 500 requests to

00:29:42,320 --> 00:29:46,960
calculate that p95 for the min rtt

00:29:45,279 --> 00:29:49,039
we have a 500 millisecond sampling

00:29:46,960 --> 00:29:51,679
window so that's every 500 milliseconds

00:29:49,039 --> 00:29:54,799
we're updating the concurrency limit

00:29:51,679 --> 00:29:56,480
there's a 50 jitter and a three minute

00:29:54,799 --> 00:29:59,760
min rtt window

00:29:56,480 --> 00:30:01,360
so that means every three to

00:29:59,760 --> 00:30:04,720
four and a half minutes there's going to

00:30:01,360 --> 00:30:08,000
be a in rtt measurement window

00:30:04,720 --> 00:30:12,000
we have 100 buffer which means we'll

00:30:08,000 --> 00:30:15,440
allow a doubling of what the

00:30:12,000 --> 00:30:17,200
ideal latency is in our samples

00:30:15,440 --> 00:30:19,840
anything beyond that then we'll start to

00:30:17,200 --> 00:30:21,919
clamp down on the concurrency limit

00:30:19,840 --> 00:30:23,919
and we have a minimum currency of 25 by

00:30:21,919 --> 00:30:27,120
default

00:30:23,919 --> 00:30:28,799
some services change many of these

00:30:27,120 --> 00:30:30,799
values but the thing we find that

00:30:28,799 --> 00:30:32,240
people tend to change the most is the

00:30:30,799 --> 00:30:34,720
minimum concurrency

00:30:32,240 --> 00:30:36,159
this is actually pretty easy to change

00:30:34,720 --> 00:30:38,000
folks will just

00:30:36,159 --> 00:30:39,679
look in their dashboards and look at

00:30:38,000 --> 00:30:40,880
their service at steady state when it's

00:30:39,679 --> 00:30:43,440
not on fire

00:30:40,880 --> 00:30:43,919
see what the number of active requests

00:30:43,440 --> 00:30:46,399
is

00:30:43,919 --> 00:30:47,840
over you know seven days or so and then

00:30:46,399 --> 00:30:49,279
just pick something that's about that

00:30:47,840 --> 00:30:50,080
value because you know that things are

00:30:49,279 --> 00:30:52,640
fine there

00:30:50,080 --> 00:30:54,480
and we want to keep the latencies about

00:30:52,640 --> 00:30:56,640
where they would be there

00:30:54,480 --> 00:30:57,600
before adopting the feature for a new

00:30:56,640 --> 00:30:59,840
service

00:30:57,600 --> 00:31:01,679
we always verify that the downstream

00:30:59,840 --> 00:31:05,120
callers to that service

00:31:01,679 --> 00:31:05,919
are retrying on 503s it'd be unfortunate

00:31:05,120 --> 00:31:07,440
if they

00:31:05,919 --> 00:31:10,080
sent to requests were rejected by the

00:31:07,440 --> 00:31:11,919
filter because of some mnrt calculation

00:31:10,080 --> 00:31:12,640
window and then never retried somewhere

00:31:11,919 --> 00:31:14,640
else where it

00:31:12,640 --> 00:31:17,760
most likely would have succeeded so we

00:31:14,640 --> 00:31:20,159
have an increased success rate with that

00:31:17,760 --> 00:31:23,440
because of this we use retry budgets in

00:31:20,159 --> 00:31:25,039
lieu of retry circuit breakers

00:31:23,440 --> 00:31:27,200
you can imagine a scenario where you

00:31:25,039 --> 00:31:29,600
have very high rps service

00:31:27,200 --> 00:31:30,559
and you enter a min rtt measurement

00:31:29,600 --> 00:31:32,880
window

00:31:30,559 --> 00:31:34,880
and lots of requests get rejected so you

00:31:32,880 --> 00:31:38,159
have lots of retries and by default

00:31:34,880 --> 00:31:41,519
envoys retry circuit breaker uh

00:31:38,159 --> 00:31:44,320
setting is is three active retries

00:31:41,519 --> 00:31:45,679
that are allowed the retry budgets do

00:31:44,320 --> 00:31:48,799
this as a percentage

00:31:45,679 --> 00:31:50,640
so you can say 25 of my outstanding

00:31:48,799 --> 00:31:52,559
requests are allowed to be retries and

00:31:50,640 --> 00:31:53,840
you still get the protection from retry

00:31:52,559 --> 00:31:56,399
storms this way

00:31:53,840 --> 00:31:58,120
but it will scale with the number of

00:31:56,399 --> 00:32:00,000
outstanding requests you have for a

00:31:58,120 --> 00:32:03,120
service um

00:32:00,000 --> 00:32:04,799
we then set the min concurrency to

00:32:03,120 --> 00:32:08,000
roughly what the steady state number of

00:32:04,799 --> 00:32:11,519
active requests was as i said earlier

00:32:08,000 --> 00:32:13,600
some general observations almost all

00:32:11,519 --> 00:32:15,360
adapted concurrency events that i've

00:32:13,600 --> 00:32:19,679
witnessed are due to

00:32:15,360 --> 00:32:23,039
a service degradation not spikes in rps

00:32:19,679 --> 00:32:26,559
so a bad deploy or an upstream

00:32:23,039 --> 00:32:29,519
dependency or like a third party

00:32:26,559 --> 00:32:31,679
that's having a latency or an outage

00:32:29,519 --> 00:32:33,279
that's where it's coming from it's never

00:32:31,679 --> 00:32:35,039
too much traffic is coming into an

00:32:33,279 --> 00:32:37,440
instance causing the latencies to

00:32:35,039 --> 00:32:37,440
increase

00:32:38,640 --> 00:32:42,080
so let's take a look at that bad deploy

00:32:40,240 --> 00:32:45,120
scenario this was

00:32:42,080 --> 00:32:49,279
a situation where a service at lyft had

00:32:45,120 --> 00:32:51,679
done a deployment and it had caused

00:32:49,279 --> 00:32:53,840
cpu utilization to increase on the

00:32:51,679 --> 00:32:56,720
majority of the

00:32:53,840 --> 00:32:58,320
nodes in that cluster and it was

00:32:56,720 --> 00:32:59,760
independent of the number of requests

00:32:58,320 --> 00:33:00,799
that were coming in so if you just sent

00:32:59,760 --> 00:33:04,720
it no requests

00:33:00,799 --> 00:33:04,720
the cpu would still be redlining

00:33:05,600 --> 00:33:09,440
so adaptive currency in this scenario

00:33:07,440 --> 00:33:11,519
notices that oh okay so

00:33:09,440 --> 00:33:13,360
it doesn't know that the cpu utilization

00:33:11,519 --> 00:33:17,200
is increasing but it is seeing that

00:33:13,360 --> 00:33:20,240
the latencies are beginning to increase

00:33:17,200 --> 00:33:23,760
so it starts shedding tons of load

00:33:20,240 --> 00:33:28,000
and what it's doing is it it's keeping

00:33:23,760 --> 00:33:31,679
the sampled round trip times within

00:33:28,000 --> 00:33:33,120
2x of the min rtt because we have 100

00:33:31,679 --> 00:33:35,840
buffer value

00:33:33,120 --> 00:33:37,600
so the average min rtt is in yellow

00:33:35,840 --> 00:33:38,880
there and in green those are our sampled

00:33:37,600 --> 00:33:43,519
round trip times

00:33:38,880 --> 00:33:47,200
and it stays roughly within uh

00:33:43,519 --> 00:33:48,000
well within 100 of what the min rtt is

00:33:47,200 --> 00:33:50,320
there

00:33:48,000 --> 00:33:51,519
so all that load shedding was just for

00:33:50,320 --> 00:33:54,480
the purposes of

00:33:51,519 --> 00:33:56,399
keeping those two lines close together

00:33:54,480 --> 00:33:57,279
but as you can see you know 100 cpu

00:33:56,399 --> 00:33:59,279
utilization

00:33:57,279 --> 00:34:02,000
isn't doing any favors to the request

00:33:59,279 --> 00:34:07,519
latencies there

00:34:02,000 --> 00:34:07,519

YouTube URL: https://www.youtube.com/watch?v=CQvmSXlnyeQ


